# Max-Sliced Mutual Information

 Dor Tsur

Ben-Gurion University

&Ziv Goldfeld

Cornell University

&Kristjan Greenewald

MIT-IBM Watson AI Lab

###### Abstract

Quantifying dependence between high-dimensional random variables is central to statistical learning and inference. Two classical methods are canonical correlation analysis (CCA), which identifies maximally correlated projected versions of the original variables, and Shannon's mutual information, which is a universal dependence measure that also captures high-order dependencies. However, CCA only accounts for linear dependence, which may be insufficient for certain applications, while mutual information is often infeasible to compute/estimate in high dimensions. This work proposes a middle ground in the form of a scalable information-theoretic generalization of CCA, termed max-sliced mutual information (mSMI). mSMI equals the maximal mutual information between low-dimensional projections of the high-dimensional variables, which reduces back to CCA in the Gaussian case. It enjoys the best of both worlds: capturing intricate dependencies in the data while being amenable to fast computation and scalable estimation from samples. We show that mSMI retains favorable structural properties of Shannon's mutual information, like variational forms and identification of independence. We then study statistical estimation of mSMI, propose an efficiently computable neural estimator, and couple it with formal non-asymptotic error bounds. We present experiments that demonstrate the utility of mSMI for several tasks, encompassing independence testing, multi-view representation learning, algorithmic fairness, and generative modeling. We observe that mSMI consistently outperforms competing methods with little-to-no computational overhead.

## 1 Introduction

Dependence measures between random variables are fundamental in statistics and machine learning for tasks spanning independence testing [1; 2; 3], clustering [4; 5], representation learning [6; 7], and self-supervised learning [8; 9; 10]. There is a myriad of measures quantifying different notions of dependence, with varying statistical and computational complexities. The simplest is the Pearson correlation coefficient [11], which only captures linear dependencies. At the other extreme is Shannon's mutual information [12], which is a universal dependence measure that is able to identify arbitrarily intricate dependencies. Despite its universality and favorable properties, accurately estimating mutual information from data is infeasible in high-dimensional settings. First, mutual information estimation rates suffers from the curse of dimensionality, whereby convergence rates deteriorate exponentially with dimension [13]. Additionally, computing mutual information requires integrating a log-likelihood ratio over a high-dimensional space, which is generally intractable.

Between these two extremes is the popular canonical correlation analysis (CCA) [14], which identifies maximally correlated linear projections of variables. Nevertheless, classical CCA still only captures linear dependence, which has inspired nonlinear extensions such as Hirschfeld-Gebelein-Renyi (HGR) maximum correlation [15; 16; 17], kernel CCA [18; 19], deep CCA [20; 7], and various other generalizations [21; 22; 23; 24]. However, HGR is computationally infeasible, while kernel and deep CCA can be burdensome in high dimensions, as they require optimization over reproducing kernel Hilbert spaces or deep neural networks, respectively. To overcome these shortcomings, this work proposesmax-sliced mutual information (mSMI)--a scalable information-theoretic extension of CCA that captures the full dependence structure while only requiring optimization over linear projections.

### Contributions

The mSMI is defined as the maximal mutual information between linear projections of the variables. Namely, the \(k\)-dimensional mSMI between \(X\) and \(Y\) with values in \(\mathbb{R}^{d_{x}}\) and \(\mathbb{R}^{d_{y}}\), respectively, is1

Footnote 1: The parameter \(k\) is fixed and small compared to the ambient dimensions \(d_{x},d_{y}\), often simply set as \(k=1\).

\[\overline{\mathsf{Sl}}_{k}(X;Y)\coloneqq\sup_{(\mathrm{A},\mathrm{B})\in \mathrm{St}(k,d_{x})\times\mathrm{St}(k,d_{y})}\mathsf{I}(\mathrm{A}^{\intercal }X;\mathrm{B}^{\intercal}Y),\]

where \(\mathrm{St}(k,d)\) is the Stiefel manifold of \(d\times k\) matrices with orthonormal columns. Unlike the nonlinear CCA variants that use nonlinear feature extractors in the high-dimensional ambient spaces, mSMI retains the linear projections of CCA and captures nonlinear structures in the _low-dimensional_ feature space. This is done by using the mutual information between the projected variables, rather than correlation, as the optimization objective. Beyond being considerably simpler from a computational standpoint, this crucial difference allows mSMI to identify the full dependence structure, akin to classical mutual information. mSMI can also be viewed as the maximized version of the average-sliced mutual information (aSMI) [25, 26], which averages \(\mathsf{I}(\mathrm{A}^{\intercal}X;\mathrm{B}^{\intercal}Y)\) with respect to (w.r.t.) the Haar measure over \(\mathrm{St}(k,d_{x})\times\mathrm{St}(k,d_{y})\). However, we demonstrate that compared to aSMI, mSMI benefits from improved neural estimation error bounds and a clearer interpretation.

We show that mSMI inherits important properties of mutual information, including identification of independence, tensorization, and variational forms. For jointly Gaussian \((X,Y)\), the optimal mSMI projections coincide with those of \(k\)-dimensional CCA [27], posing mSMI as a natural information-theoretic generalization. Beyond the Gaussian case, the solutions differ and mSMI may yield more effective representations for downstream tasks due to the intricate dependencies captured by mutual information. We demonstrate this superiority empirically for multi-view representation learning.

For efficient computation, we propose an mSMI neural estimator based on the Donsker-Varadhan (DV) variational form [28]. Neural estimators have seen a surge in interest due to their scalability and compatibility with gradient-based optimization [29, 30, 31, 32, 33, 34, 35, 36]. Our estimator employs a single model that composes the projections with the neural network approximation of the DV critic, and then jointly optimizes them. This results in both the estimated mSMI value and the optimal projection matrices. Building on recent analysis of neural estimation of \(f\)-divergences [37, 38], we establish non-asymptotic error bounds that scale as \(O\big{(}k^{1/2}(\ell^{-1/2}+kn^{-1/2})\big{)}\), where \(\ell\) and \(n\) are the numbers of neurons and \((X,Y)\) samples, respectively. Equating \(\ell\) and \(n\) results in the (minimax optimal) parametric estimation rate, which highlights the scalability of mSMI and its compatibility to modern learning settings.

In our empirical investigation, we first demonstrate that our mSMI neural estimator converges orders of magnitude faster than that of aSMI [26]. This is because the latter requires (parallel) training of many neural estimators corresponding to different projection directions, while the mSMI estimator optimizes a single combined model. Notwithstanding the reduction in computational overhead, we show that mSMI outperforms average-slicing for independence testing. Next, we compare mSMI with deep CCA [20, 7] by examining downstream classification accuracy based on representations obtained from both methods in a multi-view learning setting. Remarkably, we observe that even the linear mSMI projections outperform nonlinear representations obtained from deep CCA. We also consider an application to algorithmic fairness under the informin framework [39]. Replacing their generalized Pearson correlation objective with mSMI, we again observe superior performance in the form of more fair representations whose utility remains on par with the fairness-agnostic model. Lastly, we devise a max-sliced version of the InfoGAN by replacing the classic mutual information regularizer with its max-sliced analog. We show that despite the low-dimensional projections, the max-sliced InfoGAN successfully learns to disentangle the latent space and generates quality samples.

## 2 Background and Preliminaries

**Notation.** For \(a,b\in\mathbb{R}\), we use the notation \(a\wedge b=\min\{a,b\}\) and \(a\lor b=\max\{a,b\}\). For \(d\geq 1\), \(\|\cdot\|\) is the Euclidean norm in \(\mathbb{R}^{d}\). The Stiefel manifold of \(d\times k\) matrices with orthonormal columnsis denoted by \(\mathrm{St}(k,d)\). For a \(d\times k\) matrix \(\mathrm{A}\), we use \(\mathfrak{p}^{\mathrm{A}}:\mathbb{R}^{d}\to\mathbb{R}^{k}\) for the orthogonal projection onto the row space of \(\mathrm{A}\). For \(\mathrm{A}\in\mathbb{R}^{d\times k}\) with \(\mathrm{rank}(\mathrm{A})=r\leq k\wedge d\), we write \(\sigma_{1}(\mathrm{A}),\ldots,\sigma_{r}(\mathrm{A})\) for its non-zero singular values, and assume without loss of generality (w.l.o.g.) that they are arranged in descending order. Similarly, the eigenvalues of a square matrix \(\Sigma\in\mathbb{R}^{d\times d}\) are denoted by \(\lambda_{1}(\Sigma),\ldots,\lambda_{d}(\Sigma)\). Let \(\mathcal{P}(\mathbb{R}^{d})\) denote the space of Borel probability measures on \(\mathbb{R}^{d}\). For \(\mu,\nu\in\mathcal{P}(\mathbb{R}^{d})\), we use \(\mu\otimes\nu\) to denote a product measure, while \(\mathrm{spt}(\mu)\) designates the support of \(\mu\). All random variables throughout are assumed to be continuous w.r.t. the Lebesgue measure. For a measurable map \(f\), the pushforward of \(\mu\) under \(f\) is denoted by \(f_{\sharp}\mu=\mu\circ f^{-1}\), i.e., if \(X\sim\mu\) then \(f(X)\sim f_{\sharp}\mu\). For a jointly distributed pair \((X,Y)\sim\mu_{XY}\in\mathcal{P}(\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}})\), we write \(\Sigma_{X}\) and \(\Sigma_{XY}\) for covariance matrix of \(X\) and cross-covariance matrix of \((X,Y)\), respectively.

Canonical correlation analysis.CCA is a classical method for devising maximally correlated linear projections of a pair of random variables \((X,Y)\sim\mu_{XY}\in\mathcal{P}(\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}})\) via [14]

\[(\theta_{\mathsf{CCA}},\phi_{\mathsf{CCA}})=\operatorname*{ argmax}_{(\phi,\theta)\in\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}}}\frac{ \theta\Upsilon_{XY}\phi^{\mathsf{T}}}{\sqrt{\theta^{\mathsf{T}}\Sigma_{XX} \theta\phi^{\mathsf{T}}\Sigma_{YY}\phi}}=\operatorname*{argmax}_{\begin{subarray} {c}(\theta,\phi)\in\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}}:\\ \theta^{\mathsf{T}}\Sigma_{X}\theta=\phi^{\mathsf{T}}\Sigma_{Y}\phi=1\end{subarray}} \theta^{\mathsf{T}}\Sigma_{XY}\phi,\] (1)

where the former objective is the correlation coefficient \(\rho(\theta^{\mathsf{T}}X,\phi^{\mathsf{T}}Y)\) between the projected variables and the equality follows from invariance of \(\rho\) to scaling. The global optimum has an analytic form as \((\theta_{\mathsf{CCA}},\phi_{\mathsf{CCA}})=(\Sigma_{X}^{-1/2}\theta_{1}, \Sigma_{Y}^{-1/2}\phi_{1})\), where \((\theta_{1},\phi_{1})\) is the (unit-length) top left- and right-singular vector pair associated with the largest singular value of \(\mathrm{T}_{XY}\coloneqq\Sigma_{X}^{-1/2}\Sigma_{XY}\Sigma_{Y}^{-1/2}\in \mathbb{R}^{d_{x}\times d_{y}}\). This solution is efficiently computable in \(O((d_{x}\lor d_{y})^{3})\) time, given that the population correlation matrices are known. CCA extends to \(k\)-dimensional projections via the optimization [27]

\[\max_{\begin{subarray}{c}(\mathrm{A},\mathrm{B})\in\mathbb{R}^{d_{x}\times k} \times\mathbb{R}^{d_{y}\times k}:\\ \mathrm{A}^{\mathsf{T}}\Sigma_{X}\mathrm{A}=\mathrm{B}^{\mathsf{T}}\Sigma_{Y} \mathrm{B}=\mathrm{I}_{k}\end{subarray}}\mathrm{tr}(\mathrm{A}^{\mathsf{T}} \Sigma_{XY}\mathrm{B}),\] (2)

with the optimal CCA matrices being \((\mathrm{A}_{\mathsf{CCA}},\mathrm{B}_{\mathsf{CCA}})=(\Sigma_{X}^{-1/2} \mathrm{U}_{k},\Sigma_{Y}^{-1/2}\mathrm{V}_{k})\), where \(\mathrm{U}_{k}\) and \(\mathrm{V}_{k}\) are the matrices of the first \(k\) left- and right-singular vectors of \(\mathrm{T}_{XY}\). The optimal objective value then becomes the sum of the top \(k\) singular values of \(\mathrm{T}_{XY}\) (namely, its Ky Fan \(k\)-norm).

Divergences and information measures.Let \(\mu,\nu\in\mathcal{P}(\mathbb{R}^{d})\) satisfy \(\mu\ll\nu\), i.e., \(\mu\) is absolutely continuous w.r.t. \(\nu\). The Kullback-Leibler (KL) divergence is defined as \(\mathsf{D}(\mu\|\nu)\coloneqq\int_{\mathbb{R}^{d}}\log(d\mu/d\nu)d\mu\). We have \(\mathsf{D}(\mu\|\nu)\geq 0\), with equality if and only if (iff) \(\mu=\nu\). Mutual information and differential entropy are defined from the KL divergence as follows. Let \((X,Y)\sim\mu_{XY}\in\mathcal{P}(\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}})\) and denote the corresponding marginal distributions by \(\mu_{X}\) and \(\mu_{Y}\). The mutual information between \(X\) and \(Y\) is given by \(\mathsf{I}(X;Y)\coloneqq\mathsf{D}(\mu_{XY}\|\mu_{X}\otimes\mu_{Y})\) and serves as a measure of dependence between those random variables. The differential entropy of \(X\) is defined as \(\mathsf{h}(X)=\mathsf{h}(\mu_{X})\coloneqq-\mathsf{D}(\mu_{X}\|\mathrm{Leb})\). Mutual information between (jointly) continuous variables and differential entropy are related via \(\mathsf{I}(X;Y)=\mathsf{h}(X)+\mathsf{h}(Y)-\mathsf{h}(X,Y)\); decompositions in terms of conditional entropies are also available [40].

## 3 Max-Sliced Mutual Information

We now define the \(k\)-dimensional mSMI, establish structural properties thereof, and explore the Gaussian setting and its connections to CCA. We focus here on the case of (linear) \(k\)-dimensional projections and discuss extensions to nonlinear slicing in Section 3.3.

**Definition 1** (Max-sliced mutual information).: _For \(1\leq k\leq d_{x}\wedge d_{y}\), the \(k\)-dimensional mSMI between \((X,Y)\sim\mu_{XY}\in\mathcal{P}(\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}})\) is_

\[\overline{\mathsf{Sl}}_{k}(X;Y)\coloneqq\sup_{(\mathrm{A},\mathrm{B})\in \mathrm{St}(k,d_{x})\times\mathrm{St}(k,d_{y})}\mathsf{I}(\mathrm{A}^{\mathsf{T }}X;\mathrm{B}^{\mathsf{T}}Y),\] (3)

_where \(\mathrm{St}(k,d)\) is the Stiefel manifold of \(d\times k\) matrices with orthonormal columns._

The mSMI measures Shannon's mutual information between the most informative \(k\)-dimensional projections of \(X\) and \(Y\). It can be viewed as a maximized version of the aSMI \(\underline{\mathsf{Sl}}_{k}(X;Y)\) from [25; 26], defined as the integral of \(\mathrm{I}(\mathrm{A}^{\intercal}X;\mathrm{B}^{\intercal}Y)\) w.r.t. the Haar measure over \(\mathrm{St}(k,d_{x})\times\mathrm{St}(k,d_{y})\). For \(d=d_{x}=d_{y}\), we have \(\underline{\mathsf{Sl}}_{d}(X;Y)=\overline{\mathsf{Sl}}_{d}(X;Y)=\mathsf{I}(X;Y)\) due to invariance of mutual information to bijections. The supremum in mSMI is achieved since the Stiefel manifold is compact and the function \((\mathrm{A},\mathrm{B})\mapsto\mathsf{I}(\mathrm{A}^{\intercal}X;\mathrm{B}^{ \intercal}Y)\) is Lipschitz and thus continuous (Lemma 2 of [26]).

**Remark 1** (Multivariate and conditional mSMI).: _The mSMI definition above extends to the multivariate and conditional cases as follows. Let \((X,Y,Z)\sim\mu_{XYZ}\in\mathcal{P}(\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}} \times\mathbb{R}^{d_{z}})\). The \(k\)-dimensional multivariate and conditional mSMI functionals are, respectively, \(\overline{\mathsf{Sl}}_{k}(X,Y;Z)\coloneqq\max_{\mathrm{A},\mathrm{B},\mathrm{ C}}\mathsf{I}(\mathrm{A}^{\intercal}X,\mathrm{B}^{\intercal}Y;\mathrm{C}^{ \intercal}Z)\) and \(\overline{\mathsf{Sl}}_{k}(X;Y|Z)\coloneqq\max_{\mathrm{A},\mathrm{B}, \mathrm{C}}\mathsf{I}(\mathrm{A}^{\intercal}X;\mathrm{B}^{\intercal}Y| \mathrm{C}^{\intercal}Z)\). Connections between \(\overline{\mathsf{Sl}}_{k}(X;Y)\) and its multivariate and conditional versions are given in the proposition to follow. We also note that one may generalize the definition of \(\overline{\mathsf{Sl}}_{k}(X;Y)\) to allow for projections into feature spaces of different dimensions, i.e., \(\mathrm{A}\in\mathrm{St}(k_{x},d_{x})\) and \(\mathrm{B}\in\mathrm{St}(k_{y},d_{y})\), for \(k_{x}\neq k_{y}\). We expect our theory to extend to that case, but leave further exploration for future work._

In the spirit of mSMI, we define the max-sliced differential entropy.

**Definition 2** (Max-sliced entropy).: _The \(k\)-dimensional max-sliced (differential) entropy of \(X\sim\mu_{X}\in\mathcal{P}(\mathbb{R}^{d})\) is \(\overline{\mathsf{sh}}_{k}(X)\coloneqq\overline{\mathsf{sh}}_{k}(\mu)\coloneqq \sup_{\mathrm{A}\in\mathrm{St}(k,d)}\mathsf{h}(\mathrm{A}^{\intercal}X)\)._

An important property of classical differential entropy is the maximum entropy principle [40], which finds the highest entropy distribution within given class. In Appendix B, we study the max-sliced entropy maximizing distribution in several common scenarios. For instance, we show that \(\overline{\mathsf{sh}}_{k}\) is maximized by the Gaussian distribution under a fixed (mean and) covariance constraint. Namely, letting \(\mathcal{P}_{1}(m,\Sigma)\coloneqq\big{\{}\mu\in\mathcal{P}(\mathbb{R}^{d}) :\,\mathrm{spt}(\mu)=\mathbb{R}^{d}\,,\,\mathbb{E}_{\mu}[X]=m\,,\,\mathbb{E}_{ \mu}\big{[}(X-m)(X-m)^{\intercal}\big{]}=\Sigma\big{\}}\), we have \(\operatorname*{argmax}_{\mu\in\mathcal{P}_{1}(\mu,\Sigma)}\overline{\mathsf{sh }}_{k}(\mu)=\mathcal{N}(m,\Sigma)\). An intimate connection between max-sliced entropy and PCA is established in the sequel, under the Gaussian setting.

**Remark 2** (Sliced divergences).: _The slicing technique has originated as a means to address scalability issues concerning statistical divergences. Significant attention was devoted to sliced Wasserstein distances as discrepancy measures between probability distributions [41; 42; 43; 44; 45; 46; 47]. As such, the sliced Wasserstein distance differs from mutual information and its sliced variants, which quantify dependence between random variables, rather than discrepancy per se. Additionally, as Wasserstein distances are rooted in optimal transport theory, they heavily depend on the geometry of the underlying data space. Mutual information, on the other hand, is induced by the KL divergence, which only depends on the log-likelihood of the considered distributions and overlooks geometry._

### Structural Properties

The following proposition lists useful properties of the mSMI, which are similar to those of the average-sliced variant (cf. [26, Proposition 1]) as well as Shannon's mutual information itself.

**Proposition 1** (Structural properties).: _The following properties hold:_

1. _Bounds:_ _For any integers_ \(k_{1}<k_{2}\)_:_ \(\underline{\mathsf{Sl}}_{k_{1}}(X;Y)\leq\overline{\mathsf{Sl}}_{k_{1}}(X;Y) \leq\overline{\mathsf{Sl}}_{k_{2}}(X;Y)\leq\mathsf{I}(X;Y)\)_._
2. _Identification of independence:_ \(\overline{\mathsf{Sl}}_{k}(X;Y)\geq 0\) _with equality iff_ \((X,Y)\) _are independent._
3. _KL divergence representation:_ _We have_ \[\overline{\mathsf{Sl}}_{k}(X;Y)=\sup_{(\mathrm{A},\mathrm{B})\in\mathrm{St}(k, d_{x})\times\mathrm{St}(k,d_{y})}\mathsf{D}\big{(}(\mathfrak{p}^{\mathrm{A}}, \mathfrak{p}^{\mathrm{B}})_{\#}\mu_{XY}\big{\|}(\mathfrak{p}^{\mathrm{A}}, \mathfrak{p}^{\mathrm{B}})_{\#}\mu_{X}\otimes\mu_{Y}\big{)},\]
4. _Sub-chain rule:_ _For any random variables_ \(X_{1},\ldots,X_{n},Y\)_, we have_ \[\overline{\mathsf{Sl}}_{k}(X_{1},\ldots,X_{n};Y)\leq\overline{\mathsf{Sl}}_{k }(X_{1};Y)+\sum_{i=2}^{n}\overline{\mathsf{Sl}}_{k}(X_{i};Y|X_{1},\ldots,X_{i-1 }).\]
5. _Tensorization:_ _For mutually independent_ \(\{(X_{i},Y_{i})\}_{i=1}^{n}\)_,_ \(\overline{\mathsf{Sl}}_{k}\big{(}\{X_{i}\}_{i=1}^{n};\{Y_{i}\}_{i=1}^{n} \big{)}=\sum\limits_{i=1}^{n}\overline{\mathsf{Sl}}_{k}(X_{i};Y_{i})\)_._

The proof follows by similar arguments to those in the average-sliced case, but is given for completeness in Supplement A.1. Of particular importance are Properties 2 and 3. The former renders mSMI sufficient for independence testing despite being significantly less complex than the classical mutual information between the high-dimensional variables. The latter, which represent mSMI as a supremized KL divergence, is the basis for neural estimation techniques explored in Section 4.

**Remark 3** (Relation to average-SMI).: _Beyond the inequality relationship in Property I above, Proposition 4 in [25] (paraphrased) shows that for matrices \(\mathrm{W}_{x},\mathrm{W}_{y}\) and vectors \(b_{x},b_{y}\) of appropriate dimensions, we have \(\mathrm{sup}_{\mathrm{W}_{x},\mathrm{W}_{y},b_{x},b_{y}}\underline{\mathsf{Sl} }_{1}(\mathrm{W}_{x}^{\intercal}X+b_{x};\mathrm{W}_{y}^{\intercal}Y+b_{y})= \overline{\mathsf{Sl}}_{1}(X;Y)\), and the relation readily extends to projection dimension \(k>1\). In words, optimizing the aSMI over linear transformations of the high-dimensional data vectors coincides with the max-sliced version. This further justifies the interpretation of \(\overline{\mathsf{Sl}}_{k}(X;Y)\) as the information between the two most informative representations of \(X,Y\) in a \(k\)-dimensional feature space. It also suggests that mSMI is compatible for feature extraction tasks, as explored in Section 5.3 ahead._

### Gaussian Max-SMI versus CCA

The mSMI is an information-theoretic extension of the CCA coefficient \(\rho_{\mathsf{CCA}}(X,Y)\), which is able to capture higher order dependencies. Interestingly, when \((X,Y)\) are jointly Gaussian, the two notions coincide. We next state this relation and provide a closed-form expression for the Gaussian mSMI.

**Proposition 2** (Gaussian mSMI).: _Let \(X\sim\mathcal{N}(m_{X},\Sigma_{X})\) and \(Y\sim\mathcal{N}(m_{Y},\Sigma_{Y})\) be \(d_{x}\)- and \(d_{y}\)-dimensional jointly Gaussian vectors with nonsingular covariance matrices and cross-covariance \(\Sigma_{XY}\). For any \(k\leq d_{x}\wedge d_{y}\), we have_

\[\overline{\mathsf{Sl}}_{k}(X;Y)=\mathsf{I}(\Lambda_{\mathsf{CCA}}^{\intercal }X;\mathrm{B}_{\mathsf{CCA}}^{\intercal}Y)=-\frac{1}{2}\sum_{i=1}^{k}\log \big{(}1-\sigma_{i}(\mathrm{T}_{XY})^{2}\big{)},\] (4)

_where \((\Lambda_{\mathsf{CCA}},\mathrm{B}_{\mathsf{CCA}})\) are the CCA solutions from (2), \(\mathrm{T}_{XY}=\Sigma_{X}^{-1/2}\Sigma_{XY}\Sigma_{Y}^{-1/2}\in\mathbb{R}^{d_ {x}\times d_{y}}\), and \(\sigma_{k}(\mathrm{T}_{XY})\leq\ldots\leq\sigma_{1}(\mathrm{T}_{XY})\leq 1\) are the top \(k\) singular values of \(\mathrm{T}_{XY}\) (ordered)._

This proposition is proven in Supplement A.2. We first show that the optimization domain of \(\overline{\mathsf{Sl}}_{k}(X;Y)\) can be switched from the product of Stiefel manifolds to the space of all matrices subject to a unit variance constraint (akin to (2)), without changing the mSMI value. This implies that the CCA solutions \((\Lambda_{\mathsf{CCA}},\mathrm{B}_{\mathsf{CCA}})\) from (2) are feasible for mSMI and we establish their optimality using a generalization of the Poincare separation theorem [48, Theorem 2.2]. Specializing Proposition 2 to one-dimensional projections, i.e., when \(k=1\), the mSMI is given in terms of the canonical correlation coefficient \(\rho_{\mathsf{CCA}}(X,Y)\coloneqq\mathrm{sup}_{(\phi,\theta)\in\mathbb{R}^{d_ {x}}\times\mathbb{R}^{d_{y}}}\)\(\rho(\theta^{\intercal}X,\phi^{\intercal}Y)\). Namely,

\[\overline{\mathsf{Sl}}_{1}(X;Y)=\mathsf{I}(\theta_{\mathsf{CCA}}^{\intercal} X;\phi_{\mathsf{CCA}}^{\intercal}Y)=-0.5\log\big{(}1-\rho_{\mathsf{CCA}}(X,Y)^{2} \big{)},\]

where \((\theta_{\mathsf{CCA}},\phi_{\mathsf{CCA}})\) are the global optimizers of \(\rho_{\mathsf{CCA}}(X,Y)\).

**Remark 4** (Beyond Gaussian data).: _While the mSMI solution coincides with that of CCA in the Gaussian case, this is no longer expected to hold for non-Gaussian distributions. CCA is designed to maximize correlation, while mSMI has Shannon's mutual information between the projected variables as the optimization objective. Unlike correlation, mutual information captures higher order dependencies between the variables, and hence the optimal mSMI matrices will not generally coincide with \((\Lambda_{\mathsf{CCA}},\mathrm{B}_{\mathsf{CCA}})\). Furthermore, the intricate dependencies captured by mutual information suggest that the optimal mSMI projections may yield representations that are more effective for downstream tasks. We empirically verify this observation in Section 5 on several tasks, including classification, multi-view representation learning, and algorithmic fairness._

Similarly to the above, the Gaussian max-sliced entropy is related to PCA [49, 14]. In Supplement A.3, we prove the following.

**Proposition 3** (Gaussian max-sliced entropy).: _For a \(d\)-dimensional Gaussian variable \(X\sim\mathcal{N}(m,\Sigma)\), we have \(\overline{\mathsf{sh}}_{k}(X)=\mathrm{sup}_{\mathrm{A}\in\mathrm{St}(k,d)} \operatorname{h}(A^{\intercal}X)=\mathsf{h}(\mathrm{A}_{\mathsf{pCA}}^{ \intercal}X)=0.5\sum_{i=1}^{k}\log\big{(}2\pi e\lambda_{i}(\Sigma)\big{)}\), where \(\Lambda_{\mathsf{PCA}}\) is optimal PCA matrix and \(\lambda_{1}(\Sigma),\ldots\lambda_{k}(\Sigma)\) are the top \(k\) eigenvalues of \(\Sigma\)._

Note that the eigenvalues \(\lambda_{1}(\Sigma),\ldots\lambda_{k}(\Sigma)\) are non-negative since \(\Sigma\) is a covariance matrix. Extrapolating beyond the Gaussian case, this poses max-sliced entropy as an information-theoretic generalization of PCA for unsupervised dimensionality reduction. An analogous extension using the Renyi entropy of order \(2\) was previously considered in [50] for the purpose of binary classification. In that regard, \(\overline{\mathsf{sh}}_{k}(X)\) can be viewed as the \(\alpha\)-Renyi variant when \(\alpha\to 1\).

### Generalizations Beyond Linear Slicing

The notion of mSMI readily generalizes beyond linear slicing. Fix \(d_{x},d_{y}\geq 1\), \(k\leq d_{x}\wedge d_{y}\), and consider two (nonempty) function classes \(\mathcal{G}\subseteq\{g:\mathbb{R}^{d_{x}}\to\mathbb{R}^{k}\}\) and \(\mathcal{H}\subseteq\{h:\mathbb{R}^{d_{y}}\to\mathbb{R}^{k}\}\).

**Definition 3** (Generalized mSMI).: _The generalized mSMI between \((X,Y)\sim\mu_{XY}\in\mathcal{P}(\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}})\) w.r.t. the classes \(\mathcal{G}\) and \(\mathcal{H}\) is \(\overline{\mathsf{Sl}}_{\mathcal{G},\mathcal{H}}(X;Y)\coloneqq\sup_{(g,h)\in \mathcal{G}\times\mathcal{H}}\mathsf{I}\big{(}g(X);h(Y)\big{)}\)._

The generalized variant reduces back to \(\overline{\mathsf{Sl}}_{k}(X;Y)\) by taking \(\mathcal{G}=\mathcal{G}_{\mathsf{proj}}\coloneqq\{\mathsf{p}^{\mathsf{A}}: \,\mathrm{A}\in\mathrm{St}(k,d_{x})\}\) and \(\mathcal{H}=\mathcal{H}_{\mathsf{proj}}\coloneqq\{\mathsf{p}^{\mathsf{B}}: \,\mathrm{B}\in\mathrm{St}(k,d_{y})\}\), but otherwise allows more flexibility in the way \((X,Y)\) are mapped into \(\mathbb{R}^{k}\). We also have that if \(\mathcal{G}\subseteq\mathcal{G}^{\prime}\) and \(\mathcal{H}\subseteq\mathcal{H}^{\prime}\), then \(\overline{\mathsf{Sl}}_{\mathcal{G},\mathcal{H}}(X;Y)\leq\overline{\mathsf{ Sl}}_{\mathcal{G}^{\prime},\mathcal{H}}(X;Y)\leq\mathsf{I}(X;Y)\), which corresponds to Property 1 from Proposition 1. Further observations are as follows.

**Proposition 4** (Properties).: _For any classes \(\mathcal{G},\mathcal{H}\), we have that \(\overline{\mathsf{Sl}}_{\mathcal{G},\mathcal{H}}\) always satisfies Properties 3-5 from Proposition 1. If further \(\mathcal{G}_{\mathsf{proj}}\subseteq\mathcal{G}\) and \(\mathcal{H}_{\mathsf{proj}}\subseteq\mathcal{H}\), then \(\overline{\mathsf{Sl}}_{\mathcal{G},\mathcal{H}}\) also satisfies Property 2._

We omit the proof as it follows by the same argument as Proposition 1, up to replacing the linear projections with the functions \((g,h)\in\mathcal{G}\times\mathcal{H}\). In practice, the classes \(\mathcal{G}\) and \(\mathcal{H}\) are chosen to be parametric, typically realized by artificial neural networks. As discussed in Remark 5 ahead, this is well-suited to the neural estimation framework for mSMI (both standard and generalized). Lastly, note that \(\overline{\mathsf{Sl}}_{\mathcal{G},\mathcal{H}}(X;Y)\) corresponds to the objective of multi-view representation learning [51], which considers the maximization of the mutual information between NN-based representation of the considered variables. We further investigate this relation in Section 5.3.

## 4 Neural Estimation of Max-SMI

We study estimation of mSMI from data, seeking an efficiently computable and scalable approach subject to formal performance guarantees. Towards that end, we observe that the mSMI is compatible with neural estimation [29; 38] due to its convenient variational form. In what follows we derive the neural estimator, describe the algorithm to compute it, and provide non-asymptotic error bounds.

### Estimator and Algorithm

Fix \(d_{x},d_{y}\geq 1\), \(k\leq d_{x}\wedge d_{y}\), and \(\mu_{XY}\in\mathcal{P}(\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}})\); we suppress \(k,d_{x},d_{y}\) from our notation of the considered function classes. Neural estimation is based on the DV variational form:2

Footnote 2: One may instead use the form that stems from convex duality: \(\mathsf{I}(U;V)=\sup_{f}\mathbb{E}[f(U,V)]-\mathbb{E}\big{[}e^{f(\tilde{U}, \tilde{V})}-1\big{]}\).

\[\mathsf{I}(X;Y)=\sup_{f\in\mathcal{F}}\mathcal{L}_{\mathsf{DV}}(f;\,\mu_{XY}), \quad\mathcal{L}_{\mathsf{DV}}(f;\,\mu_{XY})\coloneqq\mathbb{E}[f(X,Y)]-\log \big{(}e^{\mathbb{E}[f(\tilde{X},\tilde{Y})]}\big{)},\]

where \((X,Y)\sim\mu_{XY}\), \((\tilde{X},\tilde{Y})\sim\mu_{X}\otimes\mu_{Y}\), and \(\mathcal{F}\) is the class of all measurable functions \(f:\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{y}}\to\mathbb{R}\) (often referred to as DV potentials) for which the expectations above are finite. As mSMI is the maximal mutual information between projections of \(X,Y\), we have

\[\overline{\mathsf{Sl}}_{k}(X;Y)=\sup_{(\mathrm{A},\mathrm{B})\in\mathrm{St}(k,d _{x})\times\mathrm{St}(k,d_{y})}\sup_{f\in\mathcal{F}}\mathcal{L}_{\mathsf{DV}} \big{(}f;\,(\mathsf{p}^{\mathsf{A}},\mathsf{p}^{\mathsf{B}})_{\sharp}\mu_{XY} \big{)}=\sup_{f\in\mathcal{F}^{\mathsf{proj}}}\mathcal{L}_{\mathsf{DV}}(f;\mu_{ XY}),\]

where \(\mathcal{F}^{\mathsf{proj}}\coloneqq\big{\{}f\circ(\mathsf{p}^{\mathsf{A}}, \mathsf{p}^{\mathsf{B}})\,:\,f\in\mathcal{F},\,(\mathrm{A},\mathrm{B})\in \mathrm{St}(k,d_{x})\times\mathrm{St}(k,d_{y})\big{\}}\). The RHS above is given by optimizing the DV objective \(\mathcal{L}_{\mathsf{DV}}\) over the _composed_ class \(\mathcal{F}^{\mathsf{proj}}\), which first projects \((X,Y)\mapsto(\Lambda^{\intercal}X,\mathrm{B}^{\intercal}Y)\) and then applies a DV potential \(f:\mathbb{R}^{k}\times\mathbb{R}^{k}\to\mathbb{R}\) to the projected variables.

Neural estimator.Neural estimators parametrize the DV potential by neural nets, approximate expectations by sample means, and optimize the resulting empirical objective over parameter space. Let \(\mathcal{F}_{\mathsf{nn}}\) be a class of feedforward networks with input space \(\mathbb{R}^{k}\times\mathbb{R}^{k}\) and real-valued outputs.3 Given i.i.d. samples \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\) from \(\mu_{XY}\), we first generate negative samples (i.e., from \(\mu_{X}\otimes\mu_{Y}\)) by taking \((X_{1},Y_{\sigma(1)}),\ldots,(X_{n},Y_{\sigma(n)})\), where \(\sigma\in S_{n}\) is a permutation such that\(\sigma(i)\neq i\), for all \(i=1,\ldots,n\). The neural estimator of \(\mathsf{SI}_{k}(X;Y)\) is now given by

\[\mathsf{SI}_{k}^{\mathcal{F}_{\mathsf{nn}}}(X^{n},Y^{n})\coloneqq \sup_{f\in\mathcal{F}_{\mathsf{nn}}^{\mathsf{proj}}}\frac{1}{n}\sum_{i=1}^{n}f (X_{i},Y_{i})-\log\left(\frac{1}{n}\sum_{i=1}^{n}e^{f(X_{i},Y_{\sigma(i)})} \right),\] (5)

where \(\mathcal{F}_{\mathsf{nn}}^{\mathsf{proj}}\coloneqq\left\{f\circ(\mathsf{p}^{ \mathsf{A}},\mathsf{p}^{\mathsf{B}})\,:\,f\in\mathcal{F}_{\mathsf{nn}},\,( \mathrm{A},\mathrm{B})\in\mathrm{St}(k,d_{x})\times\mathrm{St}(k,d_{y})\right\}\) is the composition of the neural network class with the projection maps. The projection maps can be absorbed into the network architecture as a first linear layer that maps the \((d_{x}+d_{y})\)-dimensional input to a \(2k\)-dimensional feature vector, which is then further processed by the original \(f\in\mathcal{F}_{\mathsf{nn}}\) network. Note that projection onto the Stiefel manifold can be efficiently and differentially computed via QR decomposition. Hence, the Stiefel manifold constraint can be easily enforced by setting \(A,B\) to be the projections of unconstrained \(d\times k\) matrices. Further details on the implementation are given in Supplement C.

**Remark 5** (Nonlinear slicing).: _For learning tasks that may need more expressive representations of \((X,Y)\), one may employ the nonlinear mSMI variant from Section 3.3. In practice, the classes \(\mathcal{G}=\{g_{\theta}\}\) and \(\mathcal{H}=\{h_{\phi}\}\) are taken to be parametric, realized by neural networks. These networks naturally compose with the DV critic \(f_{\psi}\), resulting in a single compound model \(f_{\psi}\circ(g_{\theta},h_{\phi})\)._

### Performance Guarantees

Neural estimation involves three sources of error: (i) function approximation of the DV potential; (ii) empirical estimation of the means; and (iii) optimization, which comes from employing suboptimal (e.g., gradient-based) routines. Our analysis provides sharp non-asymptotic bounds for errors of type (i) and (ii), leaving the account of the optimization error for future work. We focus on a class of \(\ell\)-neuron shallow ReLU networks, although the ideas extend to other nonlinearities and deep architectures. Define \(\mathcal{F}_{\mathsf{nn}}^{\ell}\) as the class of all \(f:\mathbb{R}^{k}\times\mathbb{R}^{k}\to\mathbb{R}\), \(f(z)=\sum_{i=1}^{\ell}\beta_{i}\phi\left(\langle w_{i},z\rangle+b_{i}\right)+ \langle w_{0},z\rangle+b_{0}\), whose parameters satisfy \(\max_{1\leqslant i\leqslant\ell}\|w_{i}\|_{1}\lor|b_{i}|\leq 1\), \(\max_{1\leqslant i\leqslant\ell}|\beta_{i}|\leq\frac{a_{\ell}}{2\ell}\), and \(|b_{0}|,\|w_{0}\|_{1}\leq a_{\ell}\), where \(\phi(z)=z\lor 0\) is the ReLU activation and \(a_{\ell}=\log\log\ell\lor 1\).

Consider the neural mSMI estimator \(\mathsf{SI}_{k}^{n,\ell}\coloneqq\mathsf{SI}_{k}^{\mathcal{F}_{\mathsf{nn}}^{ \ell}}(X^{n},Y^{n})\) (see (5)). We provide convergence rates for it over an appropriate distribution class, drawing upon the results of [37] for neural estimation of \(f\)-divergences. For compact \(\mathcal{X}\subset\mathbb{R}^{d_{x}}\) and \(\mathcal{Y}\subset\mathbb{R}^{d_{y}}\), let \(\mathcal{P}_{\mathsf{ac}}(\mathcal{X}\times\mathcal{Y})\) be the set of all Lebesgue absolutely continuous joint distribution \(\mu_{XY}\) with \(\mathrm{spt}(\mu_{XY})\subseteq\mathcal{X}\times\mathcal{Y}\). Denote the Lebesgue density of \(\mu_{XY}\) by \(f_{XY}\). The distribution class of interest is4

Footnote 4: Here, \(\mathcal{C}_{b}^{s}(\mathcal{U})\coloneqq\left\{f\in\mathcal{C}^{s}(\mathcal{U }):\max_{\alpha:\|\alpha\|_{1}\leq s}\|D^{\alpha}f\|_{\infty\mathcal{U}}\leq b\right\}\), where \(D^{\alpha}\), \(\alpha=(\alpha_{1},\ldots,\alpha_{d})\in\mathbb{Z}_{\geq 0}^{d}\), is the partial derivative operator of order \(\sum_{i=1}^{d}\alpha_{i}\). The restriction of \(f:\mathbb{R}^{d}\to\mathbb{R}\) to \(\mathcal{X}\subseteq\mathbb{R}^{d}\) is \(f|_{\mathcal{X}}\).

\[\mathcal{P}_{k}(M,b)\coloneqq\left\{\mu_{XY}\in\mathcal{P}_{\mathsf{ac}}( \mathcal{X}\times\mathcal{Y}):\begin{aligned} \exists\,r\in\mathcal{C}_{b}^{k+3}( \mathcal{U})&\text{ for some open set }\mathcal{U}\supset\mathcal{X}\times\mathcal{Y}\\ \text{ s.t.}&\log f_{XY}=r|_{\mathcal{X}\times \mathcal{Y}},\;\mathsf{l}(X;Y)\leq M\end{aligned}\right\},\] (6)

which, in particular, contains distributions whose densities are bounded from above and below on \(\mathcal{X}\times\mathcal{Y}\) with a smooth extension to an open set covering \(\mathcal{X}\times\mathcal{Y}\). This includes uniform distributions, truncated Gaussians, truncated Cauchy distributions, etc. The following theorem provides the convergence rate for the mSMI neural estimator, uniformly over \(\mathcal{P}_{k}(M,b)\).

**Theorem 1** (Neural estimation error).: _For any \(M,b\geq 0\), we have_

\[\sup_{\mu_{X,Y}\in\mathcal{P}_{k}(M,b)}\mathbb{E}\left[\left|\overline{\mathsf{ SI}}_{k}(X;Y)-\widehat{\mathsf{SI}}_{k}^{n,\ell}\right|\right]\leq Ck^{\frac{1}{2}} \big{(}\ell^{-\frac{1}{2}}+kn^{-\frac{1}{2}}\big{)}.\]

_where \(C\) depends on \(M\), \(b\), \(k\), and the radius of the ambient space \(\|\mathcal{X}\times\mathcal{Y}\|\coloneqq\sup_{(x,y)\in\mathcal{X}\times \mathcal{Y}}\|(x,y)\|\)._

The theorem is proven in Supplement A.4 by adapting the error bound from [38, Proposition 2] to hold for \(\mathsf{I}(\mathrm{A}^{\intercal}X;\mathrm{B}^{\intercal}Y)\), uniformly over \((\mathrm{A},\mathrm{B})\in\mathrm{St}(k,d_{x})\times\mathrm{St}(k,d_{y})\). To that end, we show that for any \(\mu_{XY}\in\mathcal{P}_{k}(b,M)\), the log-density of \((\mathrm{A}^{\intercal}X,\mathrm{B}^{\intercal}Y)\sim(\mathfrak{p}^{\mathsf{A}},\mathfrak{p}^{\mathsf{B}})_{\sharp}\mu_{XY}\) admits an extension (to an open set containing the support) with \(k+3\) continuous and uniformly bounded derivatives.

**Remark 6** (Parametric rate and optimality).: _Taking \(\ell\asymp n\), the resulting rate in Theorem 1 is parametric, and hence minimax optimal. This result implicitly assumes that \(M\) is known when picking the neural net parameters. This assumption can be relaxed to mere existence of (an unknown) \(M\), resulting in an extra \(\mathrm{polylog}(\ell)\) factor multiplying the \(n^{-1/2}\) term._

**Remark 7** (Comparison to average-SMI).: _Neural estimation of classic mutual information under the framework of [38] requires the density to have Holder smoothness \(s\geq\lfloor(d_{x}+d_{y})/2\rfloor+3\). For \(\overline{\mathsf{Sl}}_{k}(X;Y)\), smoothness of \(k+3\) is sufficient (even though the ambient dimension is the same), which means it can be estimated over a larger distribution class. Similar gains in terms of smoothness levels were observed for asMI in [26]. Nevertheless, we note that mSMI is more compatible with neural estimation than average-slicing [25; 26]. The mSMI neural estimator integrates the max-slicing into the neural network architecture and optimizes a single objective. The asMI neural estimator from [26] requires an additional Monte Carlo integration step to approximate the integral over the Steifel manifolds. This results in an extra \(k^{1/2}m^{-1/2}\) term in the error bound, where \(m\) is the number of Monte Carlo samples, introducing a burdensome computational overhead (see Section 5.1)._

**Remark 8** (Non-ReLU networks).: _Theorem 1 employs the neural estimation bound from [38], which relies on [52] to control the approximation error. As noted in [38], their bound extends to any other sigmoidal bounded activation with \(\lim_{z\to-\infty}\sigma(z)=0\) and \(\lim_{z\to\infty}\sigma(z)=1\) by appealing to the approximation bound from [53] instead. Doing so would allow relaxing the smoothness requirement on the extension to \(r\in\mathcal{C}_{b}^{k+2}\) in (6), but at the expense of scaling the hidden layer parameters as \(\ell^{1/2}\log\ell\) (as opposed to the ReLU-based bound, where the parameter scale is independent of \(\ell\))._

## 5 Experiments

### Neural Estimation

We compare the performance of neural estimation methods for mSMI and aSMI on a synthetic dataset of correlated Gaussians. Let \(X,Z\sim\mathcal{N}(0,1)\) be i.i.d. and set \(Y=\rho X+\sqrt{1-\rho^{2}}Z\), for \(\rho\in(0,1)\). The goal is to estimate the \(k\)-dimensional mSMI and aSMI between \((X,Y)\). We train our mSMI neural estimator and the aSMI neural estimator from [26, Section 4.2] based on \(n\) i.i.d. samples, and compare their performance as a function of \(n\). Both average and max-sliced algorithms converge at similar rates; however, aSMI has significantly higher time complexity due to the need to train multiple neural estimators (one for each projection direction). This is shown in Figure 1, where we compare the average epoch time for each algorithm against the dataset size. Implementation details are given in Supplement C.

### Independence Testing

In this experiment, we compare mSMI and aSMI for independence testing. We follow the setting from [26, Section 5], generating \(d\)-dimensional samples correlated in a latent \(d^{\prime}\)-dimensional subspace and estimating the information measure to determine dependence. We estimate the aSMI with the method from [26], using \(m=1000\) Monte Carlo samples and the Kozachenko-Leonenko estimator for the mutual information between the projected variables [54]. We then compute AUC-ROC over 100 trials,

Figure 1: Neural estimation performance with \(\rho=0.5\). Convergence vs. \(n\) in upper figures and average epoch time vs. \(n\) in lower figure.

Figure 2: ROC-AUC comparison. Dashed and solid lines show results for aSMI [26] and mSMI (ours), respectively. Blue plots correspond to \((d,d^{\prime})=(10,4)\), while red correspond to \((d,d^{\prime})=(20,6)\).

considering various ambient and projected dimensions. For mSMI, as we cannot differentiate through the Kozachenko-Leonenko estimator, we resort to gradient-free methods. We employ the LIPO algorithm from [55] with a stopping criterion of 1000 samples. This choice is motivated by the Lipschitzness of \((\mathrm{A},\mathrm{B})\mapsto\left|\{\mathrm{A}^{\intercal}X;\mathrm{B}^{ \intercal}Y\}\right.\) w.r.t. the Frobenius norm on \(\mathrm{St}(k,d_{x})\times\mathrm{St}(k,d_{y})\) (cf. [26, Lemma 2]). Figure 2 shows that when \(k>2\), mSMI captures independence better than asMI, particularly in the lower sample regime. We hypothesize that this is due to the fact that the shared signal lies in a low-dimensional subspace, which mSMI can isolate and perhaps better exploit than asMI, which averages over all subspaces. When \(k\) is much smaller than the shared signal dimension \(d^{\prime}\), mSMI fails to capture all the information and asMI, which takes all slices into account, may be preferable. Results are averaged over 10 seeds. Further implementation details are in Supplement C.

### Multi-View Representation Learning

We next explore mSMI as an information-theoretic generalization of CCA by examining its utility in multi-view representation learning--a popular CCA application. Without using class labels, we obtain mSMI-based \(k\)-dimensional representations of the top and bottom halves of MNIST images (considered as two separate views of the digit image). This is done by computing the \(k\)-dimensional mSMI between the views and using the maximizing projected variables as the representations. We compare to similarly obtained CCA-based representations, following the method of [20]. Both linear and nonlinear (parameterized by an MLP neural network) slicing models are optimized with similar initialization and data but different loss functions. Performance is evaluated via downstream 10-class classification accuracy, utilizing the learned top-half representations. Results are averaged over 10 seeds. As shown in Table 1, mSMI outperforms CCA for learning meaningful representations. Interestingly, linear representations learned by mSMI outperform nonlinear representations from the CCA methodology, demonstrating the potency of mSMI. Full implementation details and additional results are given in Supplements C and D, respectively.

The asMI is not considered for this experiment since it does not provide a concrete latent space representation (as it is an averaged quantity). Moreover, if one were to maximize asMI as an objective to derive such representations, this would simply lead back to computing mSMI; cf. Remark 3.

### Learning Fair Representations

Another common application of dependence measures is learning fair representations of data. We seek a data transformation \(Z=f(X)\) that is useful for predicting some outcome or label \(Y\), while being statistically independent of some sensitive attribute \(A\) (e.g., gender, race, or religion of the subject). In other words, a fair representation is one that is not affected by the subjects' protected attributes so that downstream predictions are not biased against protected groups, even if the training data may have been biased. Following the setup of [39], we measure utility and fairness using the HGR maximal correlation \(\rho_{\mathsf{HGR}}(\cdot,\cdot)=\sup_{h,g}\rho\big{(}h(\cdot),g(\cdot)\big{)}\), seeking large \(\rho_{\mathsf{HGR}}(Z,Y)\) and small \(\rho_{\mathsf{HGR}}(Z,A)\) where \(h\) and \(g\) are parameterized by neural networks. As solving this minimax problem directly is difficult in practice, following [39] we learn \(Z\) by optimizing the bottleneck equation \(\rho_{\mathsf{HGR}}(Z,Y)-\beta\mathsf{SI}_{k}(Z,A)\), where we use a neural estimator for the mSMI and \(\beta\), \(k\) are hyperparameters.

Table 2 shows results on the US Census Demographic dataset extracted from the 2015 American Community Survey, which has 37 features collected over 74,000 census tracts. Here \(Y\) is the fraction of children below the poverty line in a tract, and \(A\) is the fraction of women in the tract. Following the same experimental setup as [39], the learned \(Z\) is 80-dimensional. As [39] showed that their "Slice" approach significantly outperformed all other baselines on this experiments under a computational

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \(k\) & **Linear CCA** & **Linear mSMI** & **MLP DCCA** & **MLP mSMI** \\ \hline
1 & 0.261\(\pm\)0.03 & **0.274\(\pm\)**0.02 & 0.284\(\pm\)0.03 & **0.291\(\pm\)**0.02** \\
2 & 0.32\(\pm\)0.02 & **0.346\(\pm\)**0.02 & 0.314\(\pm\)0.03 & **0.417\(\pm\)**0.02 \\
4 & 0.42\(\pm\)0.01 & **0.478\(\pm\)**0.02 & 0.441\(\pm\)0.04 & **0.546\(\pm\)**0.01 \\
8 & 0.553\(\pm\)0.03 & **0.666\(\pm\)**0.01 & 0.655\(\pm\)0.02 & **0.665\(\pm\)**0.01 \\
12 & 0.614\(\pm\)0.02 & **0.751\(\pm\)**0.01 & 0.697\(\pm\)**0.01 & **0.753\(\pm\)**0.01 \\
16 & 0.673\(\pm\)0.02 & **0.775\(\pm\)**0.01 & 0.730\(\pm\)0.02 & **0.779\(\pm\)**0.01 \\
20 & 0.704\(\pm\)0.007 & **0.791\(\pm\)**0.006 & 0.774\(\pm\)0.01 & **0.798\(\pm\)**0.01 \\ \hline \end{tabular}
\end{table}
Table 1: Downstream classification accuracy from MNIST representations by CCA and mSMI.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline  & **N/A** & **Slice**[39] & \multicolumn{6}{c|}{**mSMI (ours)**} \\ \hline \hline  & & \(k=1\) & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) & \(\mathbf{k=6}\) & \(k=7\) \\ \hline \(\rho_{\mathsf{HGR}}(Z,Y)\) & 0.949 & 0.967 & 0.955 & 0.958 & 0.952 & 0.942 & 0.940 & 0.957 & 0.933 \\ \hline \(\rho_{\mathsf{HGR}}(Z,A)\) & 0.795 & 0.116 & 0.220 & 0.099 & 0.067 & 0.048 & 0.029 & **0.026** & 0.047 \\ \hline \end{tabular}
\end{table}
Table 2: Learning a fair representation of the US Census Demographic dataset, following the setup of [39]. Results are shown as the median over 10 runs with random data splits. The fairest result is \(k=6\).

constraint5, we apply the same computational constraint to our approach and compare only to Slice and to the "N/A" fairness-agnostic model trained on the bottleneck objective with \(\beta=0\). Note that for \(k>1\), mSMI learns a more fair representation \(Z\) (lower \(\rho_{\mathsf{HRG}}(Z,A)\)) than Slice, while retaining a utility \(\rho_{\mathsf{HRG}}(Z,Y)\) on par with the fairness agnostic N/A model. We emphasize that due to the reasons outlined in Section 5.3, aSMI is not suitable for the considered task and is thus not included in the comparison. Results on the Adult dataset are shown in Supplement E.

Footnote 5: Runtime per iteration not to exceed the runtime of Slice per iteration. We used an NVIDIA V100 GPU.

### Max-Sliced InfoGAN

We present an application of max-slicing to generative modeling under the InfoGAN framework [56]. The InfoGAN learns a disentangled latent space by maximizing the mutual information between a latent code variable and the generated data. We revisit this architecture but replace the classical mutual information regularizer in the InfoGAN objective with mSMI. Our max-sliced InfoGAN is tested on the MNIST and Fashion-MNIST datasets. Figure 3 presents the generated samples for several projection dimensions. We consider \(3\) latent codes \((C_{1},C_{2},C_{3})\), which automatically learn to encode different features of the data. We vary the values of \(C_{1}\), which is a \(10\)-state discrete variable, along the column (and consider random values of \((C_{2},C_{3})\) along the rows). Evidently, \(C_{1}\) successfully disentangles the 10 class labels and the quality of generated samples is on par with past implementations [56; 26]. We stress that since mSMI relies on low-dimensional projections, the resulting InfoGAN mutual information estimator uses a reduced number of parameters (at the negligible cost of optimizing over linear projections). Additional details are given in Supplement C.

## 6 Conclusion

This paper proposed mSMI, an information theoretic generalization of CCA. mSMI captures the full dependence structure between two high dimensional random variables, while only requiring an optimized linear projection of the data. We showed that mSMI inherits important properties of Shannon's mutual information and that when the random variables are Gaussian, the mSMI optimal solutions coincide with classic \(k\)-dimensional CCA. Moving beyond Gaussian distributions, we present a neural estimator of mSMI and establish non-asymptotic error bounds.

Through several experiments we demonstrate the utility of mSMI for tasks spanning independence testing, multi-view representation learning, algorithmic fairness and generative modeling, showing it outperforms popular methodologies. Possible future directions include an investigation of an operational meaning of mSMI, either in information theoretic or physical terms, extension of the proposed formal guarantees to the nonlinear setting, and the extension of the neural estimation convergence guarantees to deeper networks. Additionally, mSMI can provide a mathematical foundation to mutual information-based representation learning, a popular area of self-supervised learning [10; 57].

In addition to the above, we plan to develop a rigorous theory for the choice of \(k\), which is currently devised empirically and is treated as a hyperparameter. When the support of the distributions lies in some \(d^{\prime}<d\) dimensional subspace, the choice of \(k=d^{\prime}\) is sufficient to recover the classical mutual information, and therefore it characterizes the full dependence structure. Extrapolating from this point, we conjecture that the optimal value of \(k\) is related to the intrinsic dimension of the data distribution, even when it is not strictly supported on a low-dimensional subset.

Figure 3: MNIST images generated via the max-sliced InfoGAN.

## References

* [1] Sidney Siegel. Nonparametric statistics. _The American Statistician_, 11(3):13-19, 1957.
* [2] Larry D Haugh. Checking the independence of two covariance-stationary time series: a univariate residual cross-correlation approach. _Journal of the American Statistical Association_, 71(354):378-385, 1976.
* [3] Thomas B Berrett and Richard J Samworth. Nonparametric independence testing via mutual information. _Biometrika_, 106(3):547-566, 2019.
* [4] Atul J Butte and Isaac S Kohane. Mutual information relevance networks: functional genomic clustering using pairwise entropy measurements. In _Biocomputing 2000_, pages 418-429. World Scientific, 1999.
* [5] Alexander Kraskov, Harald Stogbauer, Ralph G Andrzejak, and Peter Grassberger. Hierarchical clustering using mutual information. _Europhysics Letters_, 70(2):278, 2005.
* [6] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* [7] Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On deep multi-view representation learning. In _International conference on machine learning_, pages 1083-1092. PMLR, 2015.
* [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [9] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In _International Conference on Machine Learning_, pages 12310-12320. PMLR, 2021.
* [10] Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, et al. A cookbook of self-supervised learning. _arXiv preprint arXiv:2304.12210_, 2023.
* [11] Karl Pearson. Vii. note on regression and inheritance in the case of two parents. _proceedings of the royal society of London_, 58(347-352):240-242, 1895.
* [12] Claude Elwood Shannon. A mathematical theory of communication. _The Bell system technical journal_, 27(3):379-423, 1948.
* [13] Liam Paninski. Estimation of entropy and mutual information. _Neural Computation_, 15:1191-1253, June 2003.
* [14] Harold Hotelling. Analysis of a complex of statistical variables into principal components. _Journal of educational psychology_, 24(6):417, 1933.
* [15] Hermann O Hirschfeld. A connection between correlation and contingency. _Mathematical Proceedings of the Cambridge Philosophical Society_, 31(4):520-524, 1935.
* [16] Hans Gebelein. Das statistische problem der korrelation als variations-und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung. _ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift fur Angewandte Mathematik und Mechanik_, 21(6):364-379, 1941.
* [17] Alfred Renyi. On measures of dependence. _Acta Mathematica Academiae Scientiarum Hungarica_, 10(3-4):441-451, 1959.
* [18] Shotaro Akaho. A kernel method for canonical correlation analysis. _arXiv preprint cs/0609071_, 2006.
* [19] David R Hardoon, Sandor Szedmak, and John Shawe-Taylor. Canonical correlation analysis: An overview with application to learning methods. _Neural computation_, 16(12):2639-2664, 2004.

* Andrew et al. [2013] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation analysis. In _International conference on machine learning_, pages 1247-1255. PMLR, 2013.
* Bach and Jordan [2005] Francis R Bach and Michael I Jordan. A probabilistic interpretation of canonical correlation analysis. _Technical Report_, 2005.
* Kim et al. [2006] Tae-Kyun Kim, Josef Kittler, and Roberto Cipolla. Learning discriminative canonical correlations for object recognition with image sets. In _Computer Vision-ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006, Proceedings, Part III 9_, pages 251-262. Springer, 2006.
* Parkhomenko et al. [2009] Elena Parkhomenko, David Tritchler, and Joseph Beyene. Sparse canonical correlation analysis with application to genomic data integration. _Statistical applications in genetics and molecular biology_, 8(1), 2009.
* Painsky et al. [2020] Amichai Painsky, Meir Feder, and Naftali Tishby. Nonlinear canonical correlation analysis: A compressed representation approach. _Entropy_, 22(2):208, 2020.
* Goldfeld and Greenewald [2021] Ziv Goldfeld and Kristjan Greenewald. Sliced mutual information: A scalable measure of statistical dependence. _Advances in Neural Information Processing Systems_, 34:17567-17578, 2021.
* Goldfeld et al. [2022] Ziv Goldfeld, Kristjan Greenewald, Theshani Nuradha, and Galen Reeves. k-sliced mutual information: A quantitative study of scalability with dimension. _Advances in Neural Information Processing Systems_, 35:15982-15995, 2022.
* Mardia et al. [1979] Kantial Varichand Mardia, John T Kent, and John M Bibby. Multivariate analysis. _Probability and mathematical statistics_, 1979.
* Donsker and Varadhan [1983] Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain Markov process expectations for large time. iv. _Communications on Pure and Applied Mathematics_, 36(2):183-212, 1983.
* Belghazi et al. [2018] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In _International Conference on Machine Learning_, pages 531-540. PMLR, 2018.
* Poole et al. [2018] Ben Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On variational lower bounds of mutual information. In _NeurIPS Workshop on Bayesian Deep Learning_, 2018.
* Chan et al. [2019] Chung Chan, Ali Al-Bashabsheh, Hing Pang Huang, Michael Lim, Da Sun Handason Tam, and Chao Zhao. Neural entropic estimation: A faster path to mutual information estimation. _arXiv preprint arXiv:1905.12957_, 2019.
* Song and Ermon [2020] Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information estimators. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2020.
* Zhang et al. [2019] Jingjing Zhang, Osvaldo Simeone, Zoran Cvetkovic, Eugenio Abela, and Mark Richardson. Itene: Intrinsic transfer entropy neural estimator. _arXiv preprint arXiv:1912.07277_, 2019.
* Mukherjee et al. [2020] Sudipto Mukherjee, Himanshu Asnani, and Sreeram Kannan. Ccmi: Classifier based conditional mutual information estimation. In _Uncertainty in artificial intelligence_, pages 1083-1093. PMLR, 2020.
* Tsur et al. [2023] Dor Tsur, Ziv Aharoni, Ziv Goldfeld, and Haim Permuter. Neural estimation and optimization of directed information over continuous spaces. _IEEE Transactions on Information Theory_, 2023.
* Guo et al. [2022] Qing Guo, Junya Chen, Dong Wang, Yuewei Yang, Xinwei Deng, Jing Huang, Larry Carin, Fan Li, and Chenyang Tao. Tight mutual information estimation with contrastive fenchel-legendre optimization. _Advances in Neural Information Processing Systems_, 35:28319-28334, 2022.

* [37] Sreejith Sreekumar, Zhengxin Zhang, and Ziv Goldfeld. Non-asymptotic performance guarantees for neural estimation of f-divergences. In _International Conference on Artificial Intelligence and Statistics_, pages 3322-3330. PMLR, 2021.
* [38] Sreejith Sreekumar and Ziv Goldfeld. Neural estimation of statistical divergences. _Journal of Machine Learning Research_, 23(126):1-75, 2022.
* [39] Yanzhi Chen, Weihao Sun, Yingzhen Li, and Adrian Weller. Scalable infomin learning. _Advances in Neural Information Processing Systems_, 35:2226-2239, 2022.
* [40] Thomas M Cover and A Joy Thomas. _Elements of Information Theory_. Wiley, New-York, 2nd edition, 2006.
* [41] Julien Rabin, Gabriel Peyre, Julie Delon, and Marc Bernot. Wasserstein barycenter and its application to texture mixing. In _Scale Space and Variational Methods in Computer Vision: Third International Conference, SSVM 2011, Ein-Gedi, Israel, May 29-June 2, 2011, Revised Selected Papers 3_, pages 435-446. Springer, 2012.
* [42] Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David Forsyth, and Alexander G Schwing. Max-sliced wasserstein distance and its use for gans. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10648-10656, 2019.
* [43] Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized sliced wasserstein distances. _Advances in neural information processing systems_, 32, 2019.
* [44] Tianyi Lin, Chenyou Fan, Nhat Ho, Marco Cuturi, and Michael Jordan. Projection robust wasserstein distance and riemannian optimization. _Advances in neural information processing systems_, 33:9383-9397, 2020.
* [45] Minhui Huang, Shiqian Ma, and Lifeng Lai. A riemannian block coordinate descent method for computing the projection robust wasserstein distance. In _International Conference on Machine Learning_, pages 4446-4455. PMLR, 2021.
* [46] Tianyi Lin, Zeyu Zheng, Elynn Chen, Marco Cuturi, and Michael I Jordan. On projection robust optimal transport: Sample complexity and model misspecification. In _International Conference on Artificial Intelligence and Statistics_, pages 262-270. PMLR, 2021.
* [47] Sloan Nietert, Ziv Goldfeld, Ritwik Sadhu, and Kengo Kato. Statistical, robustness, and computational guarantees for sliced wasserstein distances. _Advances in Neural Information Processing Systems_, 35:28179-28193, 2022.
* [48] C Radhakrishna Rao. Separation theorems for singular values of matrices and their applications in multivariate analysis. _Journal of Multivariate Analysis_, 9(3):362-377, 1979.
* [49] Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. _The London, Edinburgh, and Dublin philosophical magazine and journal of science_, 2(11):559-572, 1901.
* [50] Wojciech Marian Czarnecki, Rafal Jozefowicz, and Jacek Tabor. Maximum entropy linear manifold for learning discriminative low-dimensional representation. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part I 15_, pages 52-67. Springer, 2015.
* [51] Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2020.
* [52] Jason M Klusowski and Andrew R Barron. Approximation by combinations of relu and squared relu ridge functions with \(\ell^{1}\) and \(\ell^{0}\) controls. _IEEE Transactions on Information Theory_, 64(12):7649-7656, 2018.
* [53] Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. _IEEE Transactions on Information theory_, 39(3):930-945, 1993.

* [54] Alexander Kraskov, Harald Stogbauer, and Peter Grassberger. Estimating mutual information. _Physical review E_, 69(6):066138, 2004.
* [55] Cedric Malherbe and Nicolas Vayatis. Global optimization of lipschitz functions. In _International Conference on Machine Learning_, pages 2314-2323. PMLR, 2017.
* [56] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. _Advances in neural information processing systems_, 29, 2016.
* [57] Ravid Shwartz-Ziv and Yann LeCun. To compress or not to compress-self-supervised learning and information theory: A review. _arXiv preprint arXiv:2304.09355_, 2023.
* [58] Yury Polyanskiy and Yihong Wu. _Information Theory: From Coding to Learning_. Cambridge University Press, 2022.
* [59] Jan R Magnus and Heinz Neudecker. _Matrix differential calculus with applications in statistics and econometrics_. John Wiley & Sons, 2019.
* [60] Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). _arXiv preprint arXiv:1511.07289_, 2015.
* [61] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [62] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. _Advances in neural information processing systems_, 27, 2014.
* [63] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.