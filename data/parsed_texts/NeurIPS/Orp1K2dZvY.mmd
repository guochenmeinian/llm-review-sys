# Weakly Supervised 3D Open-vocabulary Segmentation

 Kunhao Liu\({}^{1}\)  Fangneng Zhan\({}^{2}\)  Jiahui Zhang\({}^{1}\)  Muyu Xu\({}^{1}\)  Yingchen Yu\({}^{1}\)

**Abdulmotaleb El Saddik\({}^{3,5}\)  Christian Theobalt\({}^{2}\)  Eric Xing\({}^{4,5}\)  Shijian Lu\({}^{1}\)\({}^{*}\)**

\({}^{1}\)Nanyang Technological University \({}^{2}\)Max Planck Institute for Informatics

\({}^{3}\)University of Ottawa \({}^{4}\)Carnegie Mellon University \({}^{5}\)MBZUAI

Corresponding author

###### Abstract

Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at https://github.com/Kunhao-Liu/3D-OVS.

## 1 Introduction

Semantic segmentation of 3D scenes holds significant research value due to its broad range of applications such as robot navigation [1], object localization [2], autonomous driving [3], 3D scene editing [4], augmented/virtual reality, etc. Given the super-rich semantics in 3D scenes, a crucial aspect of this task is achieving open-vocabulary segmentation that can handle regions and objects of various semantics including those with long-tail distributions. This is a grand challenge as it necessitates a comprehensive understanding of natural language and the corresponding objects in the 3D world.

The main challenge in open-vocabulary 3D scene segmentation is the lack of large-scale and diverse 3D segmentation datasets. Existing 3D segmentation datasets like ScanNet [5] primarily focus on restricted scenes with limited object classes, making them unsuitable for training open-vocabulary models. An alternative is to distill knowledge from pre-trained 2D open-vocabulary segmentation models to 3D representations as learned with NeRF [6] or point clouds, by fitting the feature maps or segmentation probability outputs from the 2D models [4, 7]. Though this approach circumvents the need for the 3D datasets, it inherits the limitations of the 2D models which are usually finetuned with close-vocabulary datasets of limited text labels [8, 9], thereby compromising the open-vocabulary property, especially for text labels with long-tail distributions [2, 3].

We achieve precise and annotation-free 3D open-vocabulary segmentation by distilling knowledge from two pre-trained foundation models into NeRF in a weakly supervised manner, supervised only by the open-vocabulary text descriptions of the objects in a scene, as illustrated in Fig. 1. One foundation model is CLIP [10] which is trained with Internet-scale text-image pairs [11] capturing extensive open-vocabulary multimodal knowledge. The other is DINO [12, 13] which is trained with large-scale unlabelled images capturing superb scene layout and object boundary information. However, CLIP yields image-level features which are not suitable for pixel-level semantic segmentation. Thus certain mechanisms should be designed to extract pixel-level CLIP features without fine-tuning. Additionally, the image patches' CLIP features may have ambiguities for segmentation, which need to be regularized for accurate open-vocabulary segmentation. At the other end, DINO produces feature maps instead of explicit segmentation maps. Certain distillation techniques should be designed to extract the necessary information from DINO features to facilitate precise segmentation.

We construct a hierarchical set of image patches to extract pixel-level features from image-level CLIP features and design a 3D _Selection Volume_ to identify the appropriate hierarchical level for each 3D point, effectively aligning CLIP features with pixel-level features without fine-tuning. In addition, we introduce a _Relevancy-Distribution Alignment (RDA)_ loss to address CLIP feature ambiguities, aligning segmentation probability distribution with class relevancies that capture similarities between class text features and corresponding CLIP features. Moreover, we propose a novel _Feature-Distribution Alignment (FDA)_ loss to distill object boundary information from DINO features. The FDA loss encourages close segmentation probability distributions for points with similar DINO features and distant distributions for dissimilar features. To address the training instability due to diverse distribution shapes, we further re-balance weights associated with similar and dissimilar DINO features.

Our method enables weakly supervised open-vocabulary segmentation of 3D scenes with accurate object boundaries. By distilling knowledge from CLIP without fine-tuning, our approach preserves its open-vocabulary knowledge and effectively handles text labels with long-tail distributions. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Remarkably, our experiments demonstrate that our method surpasses fully supervised models trained with segmentation annotations in certain scenes, highlighting the possibility that 3D open-vocabulary segmentation can be effectively learned from large amounts of 2D images and text-image pairs.

In summary, the contributions of this work are three-fold. _Firstly_, we propose an innovative pipeline for weakly supervised 3D open-vocabulary segmentation by distilling knowledge from pre-trained foundation models into NeRF without requiring any annotations in training. _Secondly_, we introduce a Selection Volume to align image-level CLIP features with pixel-level features, supplemented by novel Relevancy-Distribution Alignment and Feature-Distribution Alignment losses that respectively resolve CLIP features' ambiguities and effectively distill DINO features for 3D scene segmentation. _Lastly_, extensive experiments demonstrate that our method effectively recognizes long-tail classes and produces accurate segmentation maps, even with limited input data.

Figure 1: **Weakly Supervised 3D Open-vocabulary Segmentation. Given the multi-view images of a 3D scene and the open-vocabulary text descriptions, our method distills open-vocabulary multimodal knowledge from CLIP and object reasoning ability from DINO into the reconstructed NeRF, producing accurate object boundaries for the 3D scene without requiring any segmentation annotations during training.**

Related Work

Open-vocabulary Segmentation.In recent years, the field of 2D open-vocabulary segmentation has garnered significant attention, driven by the availability of extensive text-image datasets and vast computational resources. Predominant approaches [8; 14; 15; 16; 17; 18; 19] typically distill knowledge from large-scale pre-trained models, such as image-text contrastive learning models [20; 21; 10; 22] and diffusion models [23]. However, the distillation process requires fine-tuning on close-vocabulary datasets, contrasting with massive datasets used for large-scale pre-trained models [11]. This leads to limited performance in recalling infrequent classes with long-tail distributions [2; 3], compromising the open-vocabulary property. OpenSeg [24] is not finetuned on a closed set of classes but is weakly supervised via image captions. However, OpenSeg has a smaller vocabulary and knowledge than CLIP as it is trained on a much smaller dataset. Our method, without fine-tuning CLIP, effectively handles such classes.

3D Scenes Segmentation.3D scene segmentation has been a long-standing challenge in computer vision. Traditional approaches focus on point clouds or voxels with limited class variety in datasets, restricting generalizability to unseen classes [25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35; 36]. Recently, numerous point-cloud-based techniques have emerged to explore open-vocabulary 3D scene segmentation by encoding 2D open-vocabulary models' features into 3D scene points [37; 38; 3; 39; 40]. However, these methods are also mostly evaluated on datasets with restricted scenes and limited class ranges [27; 28; 41; 5], not fully exhibiting the open-vocabulary property. Moreover, point clouds have compromised geometric details, making them less suitable for precise segmentation compared to NeRF representations [42; 6]. Consequently, there has been a surge in NeRF-based 3D segmentation techniques that mainly address interactive segmentation [43; 44; 4], panoptic segmentation [45; 46], moving part segmentation [47], object part segmentation [48], object co-segmentation [49], unsupervised object segmentation [50; 51], etc. FFD [4] attempts to segment unseen text labels during training by fitting LSeg's [8] feature maps to a NeRF, but inherits LSeg's limitations, hindering generalization to long-tail distribution classes. Our method overcomes these challenges by directly using CLIP image features and distilling them into a NeRF representation [42] without fine-tuning on close-vocabulary datasets.

Foundation Models.Pre-trained foundation models [52; 53] have become a powerful paradigm in computer science due to their ability to capture general knowledge and adapt to various downstream tasks [20; 23; 54; 55; 56; 57; 12; 58]. These models are trained using various paradigms in natural language processing, such as masked language modeling [57; 58], denoising autoencoder [59], replaced token detection [60], and sentence prediction tasks[61], as well as in computer vision, including data generation [62; 63; 63], data reconstruction [64], and data contrastive learning [20; 10; 12; 21; 13; 22]. Foundation models acquire emergent capabilities for exceptional performance on downstream tasks, either in a zero-shot manner or with fine-tuning. In this work, we harness the capabilities of two prominent foundation models, CLIP [10] and DINO [12; 13]. CLIP learns associations between images and texts by mapping them to a shared space, facilitating applications in tasks like image classification, object detection, visual question-answering, and image generation [65; 10; 12; 62; 66; 9]. DINO, trained in a self-supervised manner, extracts scene layout information, particularly object boundaries, and has been successfully employed in tasks such as classification, detection, segmentation, keypoint estimation, depth estimation, and image editing [12; 13; 49; 67; 68; 69].

## 3 Method

We propose a novel method for weakly supervised open-vocabulary segmentation of reconstructed NeRF. Given the multi-view images of a scene and the open-vocabulary text description for each class, we aim to segment the reconstructed NeRF such that every 3D point is assigned a corresponding class label.

To achieve this, we exploit the CLIP model's multimodal knowledge by mapping each 3D point to a CLIP feature representing its semantic meaning. As CLIP only generates image-level features, we extract a hierarchy of CLIP features from image patches and learn a 3D _Selection Volume_ for pixel-level feature extraction, as described in Sec. 3.1.

[MISSING_PAGE_FAIL:4]

\[S(\textbf{r})=\text{Softmax}\left(\sum_{i}T_{i}\alpha_{i}S_{i}\right)\in[0,1]^{N_{ x}},\] (2)

where \(C_{i},F_{i},S_{i}\) are the color, feature, and selection vector of each sampled point along the ray, \(T_{i}=\Pi_{j=0}^{i-1}(1-\alpha_{i})\) is the accumulated transmittance and \(\alpha_{i}=1-\text{exp}(-\delta_{i}\sigma_{i})\) is the opacity of the point. We apply a Softmax function to the selection vector of each ray such that the sum of the probability of each scale is equal to 1.

For a set of rays \(\mathcal{R}\) in each training batch, the supervision loss can then be formulated as the combination of the L2 distance between rendered and ground truth RGB values and the cosine similarities \(\cos\langle,\rangle\) between the rendered features and the selected multi-scale CLIP features:

\[\mathcal{L}_{supervision}=\sum_{\textbf{r}\in\mathcal{R}}\left(\left\|\hat{C} (\textbf{r})-C(\textbf{r})\right\|_{2}^{2}-\cos\langle\hat{F}(\textbf{r}),S( \textbf{r})F(\textbf{r})\rangle\right).\] (3)

Given a set of text descriptions \(\{\text{[CLASS]}_{i}\}_{i=1}^{C}\) of \(C\) classes and the CLIP text encoder \(E_{t}\), we can get the classes' text features \(T=E_{t}(\text{[CLASS]})\in\mathbb{R}^{C\times D}\). Then we can get the segmentation logits \(z(\textbf{r})\) of the ray **r** by computing the cosine similarities between the rendered CLIP feature and the classes' text features:

\[z(\textbf{r})=\cos\langle T,\hat{F}(\textbf{r})\rangle\in\mathbb{R}^{C}.\] (4)

We can then get the class label of the ray \(l(\textbf{r})=\text{argmax}(z(\textbf{r}))\).

### Relevancy-Distribution Alignment for Ambiguity Mitigation

To mitigate the ambiguities of the CLIP features, we propose to align the segmentation probability distribution with the spatially normalized relevancy maps of each class, enabling our method to identify specific image regions described by each class text, as illustrated in Fig. 2. The segmentation probability of each ray \(P(\textbf{r})\) can be derived from the segmentation logits with a Softmax function:

\[P(\textbf{r})=\text{Softmax}\left(z(\textbf{r})\right)\in[0,1]^{C}.\] (5)

The relevancy of a given class is determined by the similarity between the class's text feature and the selected feature from the hierarchy of image patches' CLIP features. Given an image \(I\), we can get its multi-scale pixel-level CLIP feature \(F_{I}\in\mathbb{R}^{N_{x}\times D\times H\times W}\) using Alg. 1 and selection vector \(S_{I}\in\mathbb{R}^{N_{s}\times H\times W}\) using Eq. (2). And then we can get the image's relevancy map \(R_{I}\in\mathbb{R}^{C\times H\times W}\) as:

\[R_{I_{hw}}=S_{I_{hw}}\cos\langle T,F_{I_{hw}}\rangle,\] (6)

where where \(h,w\) denotes the index in the \(H\) and \(W\) channel. We normalize each class's relevancy independently within an input view to \([0,1]\) to mitigate the ambiguities of CLIP features, making our method discern image regions described by each class text:

\[\bar{R}_{I}=\left(R_{I}-\text{min}(R_{I})\right)/\left(\text{max}(R_{I})- \text{min}(R_{I})\right)\in[0,1]^{C\times H\times W},\] (7)

Figure 2: **Mitigating CLIP features’ ambiguities with normalized relevancy maps.** For original relevancy maps \(r_{a},r_{b}\) of classes \(a\) and \(b\), we note a higher relevancy for class \(b\) in Region 2 than in other image regions. Despite this, the ambiguities of CLIP features lead to Region 2’s classification as \(a\) due to the higher absolute relevancy of \(a\) in Region 2, even as \(a\) is located in Region 1. To rectify this, we normalize each class’s relevancy maps to a fixed range. These normalized relevancy maps, \(\bar{r_{a}}\) and \(\bar{r_{b}}\), reduce such ambiguities, facilitating accurate region-class assignments.

where \(\text{min}()\) and \(\text{max}()\) are the functions getting the lowest and highest values across the spatial dimensions (i.e. \(H\) and \(W\)). We apply a Softmax function to \(\bar{R}_{I}\) to make it a probability vector. Then we can assign each ray **r** its normalized relevancy with all the classes \(\bar{R}(\textbf{r})\in[0,1]^{C}\). We employ the Jensen-Shannon (JS) divergence to measure the discrepancy between the normalized relevancy \(\bar{R}(\textbf{r})\) and the segmentation probability distribution \(P(\textbf{r})\) of each ray, formulating the Relevancy-Distribution Alignment (RDA) loss:

\[\mathcal{L}_{RDA}=\sum_{\textbf{r}\in\mathcal{R}}\sum_{c\in C}\left(P(\textbf{ r})_{c}\text{log}\left(\frac{P(\textbf{r})_{c}}{M_{P\bar{R}}(\textbf{r})_{c}} \right)+\bar{R}(\textbf{r})_{c}\text{log}\left(\frac{\bar{R}(\textbf{r})_{c}} {M_{P\bar{R}}(\textbf{r})_{c}}\right)\right)/2,\] (8)

where \(M_{P\bar{R}}(\textbf{r})=(P(\textbf{r})+\bar{R}(\textbf{r}))/2\) is the average of the two distributions, and the subscript \(c\) denotes the probability of the \(c\)th class. By aligning the normalized relevancies and the segmentation probability distributions, our method can effectively identify the specific region corresponding to the text description of each class.

### Feature-Distribution Alignment for Precise Object Boundary Segmentation

To ensure the segmentation exhibits precise object boundaries, we align the segmentation probability distribution with the images' DINO features, which have been shown to capture superb scene layouts and object boundary information [12; 13]. Following [49; 68], we extract the scene layout information with a DINO feature correlation tensor. Given a patch of size \(H_{p}\times W_{p}\), we can get the correlation tensor \(Corr\_F\in\mathbb{R}^{H_{p}W_{p}\times H_{p}W_{p}}\) as:

\[Corr\_F_{hwij}=\cos(f_{hw},f_{ij}),\] (9)

whose entries represent the cosine similarity between the DINO features \(f\) at spatial positions \((h,w)\) and \((i,j)\) of the patch. In order to construct the correlation tensor for the segmentation probability distribution, we propose utilizing the JS divergence to assess the similarity between segmentation probabilities at two distinct spatial positions. The choice of JS divergence offers several advantages, including its symmetric nature and a bounded range of \([0,1]\), which contribute to improved numerical stability. However, since we only care about the class label of each point, i.e. the entry with the highest probability, we use a low temperature \(\tau<1\) to get a sharper version of the segmentation probability distribution \(\dot{P}\) to let the model focus on the entry with the largest probability:

\[\dot{P}=\text{Softmax}\;(z/\tau)\in[0,1]^{C}.\] (10)

The distribution correlation tensor \(Corr\_D\in\mathbb{R}^{H_{p}W_{p}\times H_{p}W_{p}}\) can thus be computed with:

\[Corr\_D_{hwij}=\sum_{c\in C}\left(\dot{P}_{hwc}\text{log}\left(\frac{\dot{P} _{hwc}}{M_{\dot{P}\dot{P}c}}\right)+\dot{P}_{ijc}\text{log}\left(\frac{\dot{ P}_{ijc}}{M_{\dot{P}\dot{P}c}}\right)\right)/2,\] (11)

where \(\dot{P}_{hwc},\dot{P}_{ijc}\) are the segmentation probabilities of the \(c\)th class at spatial locations \((h,w)\) and \((i,j)\) of the patch, \(M_{\dot{P}\dot{P}}=(\dot{P}_{hw}+\dot{P}_{ij})/2\) is the average of the two distributions. Thus the correlation loss [68] can be expressed as:

\[\mathcal{L}_{corr}=\sum_{hwij}(Corr\_F_{hwij}-b)\times Corr\_D_{hwij},\] (12)

Figure 3: **Difference between similar and distant distributions.** Distributions having large divergence from the target distribution exhibit significantly diverse shapes, increasing the training instability (left). Conversely, distributions displaying low divergence with the target distribution consistently demonstrate a similar shape (right).

where \(b\) is a hyper-parameter denoting that we consider the segmentation probabilities of two spatial locations \((h,w)\) and \((i,j)\) to be similar if their DINO features' similarity is larger than \(b\) and distant if less than \(b\). Nonetheless, the correlation loss \(\mathcal{L}_{corr}\) introduces significant instability due to the diverse shapes of distributions with large divergence from a target distribution, making the loss assign wrong labels to the segmented parts. Conversely, when a distribution displays a low JS divergence with the target distribution, it consistently demonstrates a similar shape to the target distribution, as shown in Fig. 3. Based on this observation, we propose re-balancing the weights associated with similar and dissimilar DINO features. Specifically, we allocate a much greater weight to the correlation loss arising from similar DINO features and a smaller weight to that of dissimilar DINO features, thereby mitigating the instability caused by the correlation loss. Thus the Feature-Distribution Alignment (FDA) loss can be formulated with:

\[pos\_F=\text{clamp}(Corr\_F-b,\text{min}=0),\quad neg\_F=\text{clamp}(Corr \_F-b,\text{max}=0),\] (13)

\[\mathcal{L}_{FDA}=\lambda_{pos}\sum_{hwij}(pos\_F_{hwij}\times Corr\_D_{hwij} )/\text{count\_nonzero}(pos\_F)+\\ \lambda_{neg}\sum_{hwij}(neg\_F_{hwij}\times Corr\_D_{hwij})/ \text{count\_nonzero}(neg\_F),\] (14)

where \(\text{clamp}(,\text{min/max}=0)\) is to clamp all the elements smaller/greater than 0 to 0, thus making \(pos\_F\geq 0\) and \(neg\_F\leq 0\), \(\text{count\_nonzero}()\) is to count the number of non zero elements, and \(\lambda_{pos},\lambda_{neg}\) are the weights associated with similar and dissimilar DINO features, which are set as \(\lambda_{pos}=200\gg\lambda_{neg}=0.2\) by default.

The total training loss is: \(\mathcal{L}=\mathcal{L}_{supervision}+\mathcal{L}_{RDA}+\mathcal{L}_{FDA}\), where \(\mathcal{L}_{supervision},\mathcal{L}_{RDA}\) are calculated with randomly sampled rays and \(\mathcal{L}_{FDA}\) is calculated with randomly sampled patches.

## 4 Experiments

We evaluate our method on 3D open-vocabulary segmentation, showing that our method can recognize long-tail classes and produce highly accurate object boundaries even with limited input data. We employ TensoRF [42] as the backbone and extract 3 scales of pixel-level CLIP features. More implementation details and experiments are in the appendix.

Dataset.Existing 3D segmentation datasets predominantly focus on either restricted scenes with a narrow range of object classes [70; 5], or individual objects [71; 72; 73], thereby limiting their capacity to fully assess the task of 3D open-vocabulary segmentation. Thus following [2], we create a dataset comprising 10 distinct scenes. Each scene features a set of long-tail objects situated in various poses and backgrounds. Ground truth masks for the test views are manually annotated, enabling both qualitative and quantitative evaluation of our segmentation methods. We also evaluate our method on more diverse datasets which include human body, human head, indoor scenes with low-quality images [70; 74], and a complex scene from LERF datasets [2], as shown in the appendix.

Baselines.We benchmark our method with three NeRF-based methods capable of 3D open-vocabulary segmentation: FFD [4], Semantic-NeRF (abbreviated as Sem) [45], and LERF [2].

\begin{table}
\begin{tabular}{c|c|c c|c c|c c|c c|c c c}  & Methods & \multicolumn{2}{c|}{\(\begin{tabular}{c}_bed_\\ **mel** \\ \end{tabular} } & \multicolumn{2}{c|}{\(\begin{tabular}{c}_lown_ \\ **Accuracy** \\ \end{tabular} } & \multicolumn{2}{c|}{\(\begin{tabular}{c}_room_ \\ **mol** \\ \end{tabular} } & \multicolumn{2}{c|}{\(\begin{tabular}{c}_bench_ \\ **Accuracy** \\ \end{tabular} } & \multicolumn{2}{c|}{\(\begin{tabular}{c}_tableU.\end{tabular} } & \multicolumn{2}{c|}{\(\begin{tabular}{c}_techn_ \\ **accarray{}\end{tabular} } & \multicolumn{2}{c|}{\(\begin{tabular}{c}_tableU.\end{tabular} } & \multicolumn{2}{c|}{\(
\begin{tabular}{c}_techn_ \\ **accarray{}\end{tabular} } \\ \hline \hline \multirow{3}{*}{2D} & LSeg [8] & 56.0 & 87.6 & 04.5 & 16.5 & 17.5 & 77.5 & 19.2 & 46.1 & 06.0 & 42.7 & 07.6 & 29.9 \\  & ODISE [16] & 52.6 & 86.5 & 48.3 & 35.4 & 39.8 & 82.5 & 52.5 & 59.7 & 24.1 & 39.0 & 39.7 & 34.5 \\  & OV-Seg [19] & 79.8 & 40.4 & 66.1 & 69.6 & 81.2 & 92.1 & 72.4 & 49.1 & 88.9 & 89.2 & 80.6 & 65.3 \\ \hline \multirow{6}{*}{3D} & FFD [41] & 56.6 & 86.9 & 03.7 & 09.5 & 42.9 & 82.6 & 25.1 & 51.4 & 06.1 & 42.8 & 07.9 & 30.1 \\  & Sem(ODISE) [45] & 50.3 & 86.5 & 27.7 & 22.2 & 24.2 & 80.5 & 29.5 & 61.5 & 25.6 & 56.4 & 18.4 & 30.8 \\ \cline{1-1}  & Sem(OV-Seg) [45] & 59.3 & 96.7 & 66.5 & 89.0 & 87.6 & **95.4** & 53.8 & 81.9 & **94.2** & **98.5** & 83.8 & **94.6** \\ \cline{1-1}  & LERF [2] & 73.5 & 86.9 & 27.0 & 43.8 & 73.7 & 93.5 & 46.6 & 79.8 & 53.2 & 79.7 & 33.4 & 41.0 \\ \cline{1-1}  & **Ours** & 89.5 & 96.7 & 74.0 & 91.6 & 88.2 & **97.3** & 92.8 & 98.9 & 89.3 & 96.3 & 88.8 & **96.5** \\ \hline \end{tabular}
\end{table}
Table 1: **Quantitative comparisons. We report the mIoU(\(\uparrow\)) scores and the Accuracy(\(\uparrow\)) scores of the following methods in 6 scenes and highlight the best, second-best, and third-best scores. Our method outperforms both 2D and 3D methods without any segmentation annotations in training.**FFD, a pioneering effort in 3D open-vocabulary segmentation, applies the feature maps of the 2D open-vocabulary segmentation method, LSeg [8], to achieve its segmentation results. For Sem, we adapt it to the open-vocabulary segmentation context by distilling segmentation results from two state-of-the-art 2D open-vocabulary segmentation models: the diffusion-model-based ODISE [16], and the CLIP-based OV-Seg [19]. LERF closely aligns with our proposed approach due to its use of knowledge distillation from CLIP and DINO. However, its primary focus is on object localization rather than segmentation. We use the same scale level number and patch sizes in LERF for fair comparisons. We also include results obtained by independently segmenting each test view using the aforementioned 2D models. Note that under our settings, FFD, Sem are fully supervised methods using segmentation annotations.

### Results

We present the qualitative results in Fig. 4 and quantitative results in Tab. 1. Our proposed method outperforms all other techniques, including those which heavily rely on extensive segmentation annotations, such as LSeg, ODISE, OV-Seg. In particular, ODISE and FFD underperform in our evaluation, as they are unable to identify many long-tail classes, suggesting that the fine-tuning process of LSeg and ODISE may have led to a significant loss of the open-vocabulary knowledge originally encapsulated by CLIP and Stable Diffusion [23]. OV-Seg attempts to retain CLIP's knowledge by leveraging a mined dataset, however, it requires a mask proposal model which produces inconsistent segmentation across views, making Sem(OV-Seg) produce noisy and imprecise segmentation. LERF

Figure 4: **Qualitative comparisons. Visualization of the segmentation results in 3 scenes. Our method successfully recognizes long-tail classes and produces the most accurate segmentation maps.**

also fails to capture precise object boundaries due to its usage of a relatively naive regularization loss, which fails to fully exploit the object boundary information within the DINO features. In contrast, our method exhibits robust performance, successfully recognizing long-tail classes and generating accurate and well-defined boundaries for each class. However, LERF allows querying any object without the need for running optimization again, which is an advantage over our method.

### Studies

Ablations.We conduct ablation studies to evaluate the individual contributions of the RDA loss and the FDA loss to the overall performance of our proposed method. As shown in Tab. 2, both RDA loss and FDA loss are crucial to our method, without each of which can result in severe performance degradation. As illustrated in Fig. 5, without the RDA loss, the model does not resolve the ambiguities of the CLIP features, leading to misclassifications. For instance, it fails to distinguish between an _orange cat_ and a _Portuguese egg tart_, and confuses a _mini offroad car_ with _wood_. Without the FDA loss, although our method can correctly locate each class, it fails to segment precise object boundaries. When discarding the re-balancing in the FDA loss, i.e. using the correlation loss [68], the model produces accurate boundaries but assigns each cluster the wrong label due to the instability brought by diverse distribution shapes.

Limited input.Given the substantial computational and storage demands of extracting hierarchical CLIP features for each view (exceeding 1GB for 3 scales in the full model), we explore whether reducing input CLIP features would yield similar results, as shown in Tab. 2 and Fig. 5. We test two modifications: reducing views for feature extraction and using a single scale of CLIP features rather than three. Halving the input views for feature extraction leads to negligible performance degradation (\(<1\%\)) and minor visual differences compared to the full model. When reducing to only \(10\%\) of input views, equivalent to 2-3 views in our dataset, we observe a modest \(9\%\) drop in the mIoU score and a \(1\%\) decrease in the Accuracy score, while retaining accurate segmentation across most classes. Using a single scale of CLIP features also only incurs minimal degradation (\(<1\%\)). Even under extreme conditions, i.e., extracting a single scale of features from \(10\%\) of input views (total only \(\nicefrac{{1}}{{30}}\) input of the full model), performance degradation is limited to \(10\%\). This efficient approach even outperforms LERF [2] which utilizes all input views and scales, highlighting our method's robustness.

## 5 Limitations

The limitations of our method are twofold. First, unlike LERF [2], our method requires text labels before training. To perform segmentation with new text labels, our method needs to be retrained.

\begin{table}
\begin{tabular}{l|c c}  & **mIoU** & **Accuracy** \\ \hline \hline w/o RDA loss & 57.2 & 79.4 \\ w/o FDA loss & 58.2 & 82.7 \\ w/o re-balance & 44.9 & 74.3 \\ \hline
50\% views & 85.7 & **95.7** \\
10\% views & 79.1 & 94.6 \\ single scale & 85.2 & 95.5 \\ single \& 10\% & 77.1 & 94.6 \\ \hline
**full model** & 86.2 & 95.8 \\ \end{tabular}
\end{table}
Table 2: **Studies.** We report the mean mIoU(\(\uparrow\)) scores and the Accuracy(\(\uparrow\)) scores in our studies.

Figure 5: **Studies.** Visualization of the studies on ablations and limited input.

Inferring accurate boundaries with open vocabularies is challenging for implicit representations like NeRF, as NeRF learns a continuous representation rather than a discrete one. It is promising to learn object-level discrete representation using NeRF in future work.

Second, since our method has never seen any segmentation maps during training (only weakly supervised by the text labels), it fails to segment complex scenes like indoor datasets [70, 74] with high precision, as shown in the appendix. Our method distills pixel-level CLIP features in a patch-based fashion with a strong inductive bias for compact objects with an aspect ratio close to 1. For objects with large complex shapes and unobvious textures, our method would fail to recognize them.

## 6 Conclusion

In this study, we address the challenge of 3D open-vocabulary segmentation by distilling knowledge from the pre-trained foundation models CLIP and DINO into reconstructed NeRF in a weakly supervised manner. We distill the open-vocabulary multimodal knowledge from CLIP with a Selection Volume and a novel Relevancy-Distribution Alignment loss to mitigate the ambiguities of CLIP features. In addition, we introduce a novel Feature-Distribution Alignment loss to extract accurate object boundaries by leveraging the scene layout information within DINO features. Our method successfully recognizes long-tail classes and produces precise segmentation maps, even when supplied with limited input data, suggesting the possibility of learning 3D segmentation from 2D images and text-image pairs.

## Acknowledgement

We sincerely thank Zuhao Yang, Zeyu Wang, Weijing Tao, and Kunyang Li for collecting the dataset. This project is funded by the Ministry of Education Singapore, under the Tier-1 project scheme with project number RT18/22.

## References

* [1] Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, and Arthur Szlam. Clipfields: Weakly supervised semantic fields for robotic memory. _arXiv preprint arXiv:2210.05663_, 2022.
* [2] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. _arXiv preprint arXiv:2303.09553_, 2023.
* [3] Krishna Murthy Jatavallabula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al. Conceptfusion: Open-set multimodal 3d mapping. _arXiv preprint arXiv:2302.07241_, 2023.
* [4] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. _arXiv preprint arXiv:2205.15585_, 2022.
* [5] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5828-5839, 2017.
* [6] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.
* [7] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. _arXiv preprint arXiv:2211.15654_, 2022.
* [8] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. _arXiv preprint arXiv:2201.03546_, 2022.
* [9] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16793-16803, 2022.
* [10] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [11] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatuszaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.
* [12] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve'e Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9630-9640, 2021.
* [13] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.
* [14] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, and Tianrui Li. Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation. _arXiv preprint arXiv:2211.14813_, 2022.
* [15] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18134-18144, 2022.
* [16] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. _arXiv preprint arXiv:2303.04803_, 2023.
* [17] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. _arXiv preprint arXiv:2212.11270_, 2022.
* [18] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11583-11592, 2022.
* [19] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. _arXiv preprint arXiv:2210.04150_, 2022.
* [20] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _International Conference on Machine Learning_, pages 4904-4916. PMLR, 2021.
* [21] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. _arXiv preprint arXiv:2110.05208_, 2021.
* [22] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language pre-training. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXVI_, pages 529-544. Springer, 2022.
* [23] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [24] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVI_, pages 540-557. Springer, 2022.
* [25] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese. Joint 2d-3d-semantic data for indoor scene understanding. _arXiv preprint arXiv:1702.01105_, 2017.
* [26] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Semantickitti: A dataset for semantic scene understanding of lidar sequences. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9297-9307, 2019.
* [27] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Long, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11621-11631, 2020.
* [28] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. _arXiv preprint arXiv:1709.06158_, 2017.
* [29] Dave Zhenyu Chen, Angel X Chang, and Matthias Niessner. Scanrefer: 3d object localization in rgb-d scans using natural language. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XX_, pages 202-221. Springer, 2020.

* [30] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In _2012 IEEE conference on computer vision and pattern recognition_, pages 3354-3361. IEEE, 2012.
* [31] Binh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, Minh-Khoi Tran, Lap-Fai Craig Yu, and Sai-Kit Yeung. Scennn: A scene meshes dataset with annotations. _2016 Fourth International Conference on 3D Vision (3DV)_, pages 92-101, 2016.
* [32] Yiyi Liao, Jun Xie, and Andreas Geiger. Kitti-360: A novel dataset and benchmarks for urban scene understanding in 2d and 3d. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45:3292-3310, 2021.
* [33] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 909-918, 2019.
* [34] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2446-2454, 2020.
* [35] Lyne P. Tchapmi, Christopher Bongsoo Choy, Iro Armeni, JunYoung Gwak, and Silvio Savarese. Segcloud: Semantic segmentation of 3d point clouds. _2017 International Conference on 3D Vision (3DV)_, pages 537-547, 2017.
* [36] Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with superpoint graphs. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4558-4567, 2018.
* [37] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Language-driven open-vocabulary 3d scene understanding. _arXiv preprint arXiv:2211.16312_, 2022.
* [38] Kirill Mazur, Edgar Sucar, and Andrew J Davison. Feature-realistic neural fusion for real-time, open set scene understanding. _arXiv preprint arXiv:2210.03043_, 2022.
* [39] Huy Ha and Shuran Song. Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models. In _Conference on Robot Learning_, 2022.
* [40] Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wenping Wang. Clip2scene: Towards label-efficient 3d scene understanding by clip. _arXiv preprint arXiv:2301.04926_, 2023.
* [41] Iro Armeni, Ozan Sener, Amir Roshan Zamir, Helen Jiang, Ioannis K. Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1534-1543, 2016.
* [42] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorfial radiance fields. In _European Conference on Computer Vision_, 2022.
* [43] Rahul Goel, Dhawal Sirikonda, Saurabh Saini, and PJ Narayanan. Interactive segmentation of radiance fields. _arXiv preprint arXiv:2212.13545_, 2022.
* [44] Vadim Tscherezki, Iro Laina, Diane Larlus, and Andrea Vedaldi. Neural feature fusion fields: 3d distillation of self-supervised 2d image representations. _2022 International Conference on 3D Vision (3DV)_, pages 443-453, 2022.
* [45] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew J. Davison. In-place scene labelling and understanding with implicit scene representation. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 15818-15827, 2021.
* [46] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulo, Norman Muller, Matthias Niessner, Angela Dai, and Peter Kontschieder. Panoptic lifting for 3d scene understanding with neural fields. _arXiv preprint arXiv:2212.09802_, 2022.
* [47] Vadim Tscherezki, Diane Larlus, and Andrea Vedaldi. Neuraldiff: Segmenting 3d objects that move in egocentric videos. _2021 International Conference on 3D Vision (3DV)_, pages 910-919, 2021.
* [48] Jesus Zarzar, Sara Rojas, Silvio Giancola, and Bernard Ghanem. Segnerf: 3d part segmentation with neural radiance fields. _arXiv preprint arXiv:2211.11215_, 2022.
* [49] Zhiwen Fan, Peihao Wang, Yifan Jiang, Xinyu Gong, Dejia Xu, and Zhangyang Wang. Nerf-sos: Any-view self-supervised object segmentation on complex scenes. _arXiv preprint arXiv:2209.08776_, 2022.
* [50] Shengnan Liang, Yichen Liu, Shangzhe Wu, Yu-Wing Tai, and Chi-Keung Tang. Onerf: Unsupervised 3d object segmentation from multiple views. _arXiv preprint arXiv:2211.12038_, 2022.

* [51] Karl Stelzner, Kristian Kersting, and Adam R Kosiorek. Decomposing 3d scenes into objects via unsupervised volume segmentation. _arXiv preprint arXiv:2104.01148_, 2021.
* [52] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.
* [53] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. A comprehensive survey on pretrained foundation models: A history from bert to chatgrp. _arXiv preprint arXiv:2302.09419_, 2023.
* [54] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. _OpenAI blog_, 2019.
* [55] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [56] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [57] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [58] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. _Transactions of the Association for Computational Linguistics_, 8:64-77, 2020.
* [59] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. _arXiv preprint arXiv:1910.13461_, 2019.
* [60] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. _arXiv preprint arXiv:2003.10555_, 2020.
* [61] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. _arXiv preprint arXiv:1909.11942_, 2019.
* [62] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [63] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* [64] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16000-16009, 2022.
* [65] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu. Cris: Clip-driven referring image segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11686-11695, 2022.
* [66] Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, and Furu Wei. Clip models are few-shot learners: Empirical studies on vqa and visual entailment. In _Annual Meeting of the Association for Computational Linguistics_, 2022.
* [67] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic appearance transfer. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10738-10747, 2022.
* [68] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T Freeman. Unsupervised semantic segmentation by distilling feature correspondences. _arXiv preprint arXiv:2203.08414_, 2022.
* [69] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. _arXiv preprint arXiv:2112.05814_, 2(3):4, 2021.
* [70] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The replica dataset: A digital replica of indoor spaces. _arXiv preprint arXiv:1906.05797_, 2019.
* [71] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. _arXiv preprint arXiv:2212.08051_, 2022.

* [72] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omnioject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. _arXiv preprint arXiv:2301.07525_, 2023.
* [73] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10901-10911, 2021.
* [74] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M Susskind. Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10912-10922, 2021.
* [75] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.
* [76] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. _ACM Transactions on Graphics (TOG)_, 2019.

## Appendix A Implementation Details

### Model Architecture

We use TensoRF [42] as our base NeRF architecture for efficiency, the plane size is also the same as the default setting of TensoRF. The RGB and CLIP feature branches share the same volume and use the same intermediate features. The selection volume and density volume are two other independent volumes. We directly use the features extracted from the selection volume and density volume as the selection vector and the density value, as they have low dimensions and are view-independent. We use the original MLP architecture in TensoRF to extract the view-dependent RGB value and use another MLP which discards view direction input to extract the rendered CLIP feature. The architecture is illustrated in Fig. 6.

### Hyperparameters

We set \(\tau=0.2\) to get the shaper segmentation probability distribution \(\hat{P}\). The offset \(b\) is set to \(0.7\) to measure the similarities of the DINO features, meaning that two DINO features are considered similar if their cosine similarity is larger than \(0.7\), and different if less than \(0.7\). We use 3 scales of CLIP features, and the patch sizes of each scale are set as \(s/5,s/7,\) and \(s/10\), where \(s\) is the smaller value in the width and height of the input image \(I\). In the ablation studies, we use \(s/7\) as the patch size of the single-scale CLIP feature input. The weights associated with similar and dissimilar DINO features in \(\mathcal{L}_{FDA}\) are set as \(\lambda_{pos}=200\) and \(\lambda_{neg}=0.2\) by default. In certain scenes, we find that setting \(\lambda_{neg}\) to 0.22 or 0.18 can produce better results. We use ViT-B/16 CLIP model to extract the image and text features and use version 1 dino_vit8 model to extract the DINO features because it employs the smallest downsampling factor of 8 which is advantageous for high-precision segmentation.

### Training

To reconstruct a NeRF from multiview images of a scene, we follow the same training settings as TensoRF. For segmentation training, we train the model for 15k iterations. In the first 5k iterations,

Figure 6: **Model architecture.**we freeze the shared volume and density volume, and train the selection volume and the CLIP feature branch. For the rest 10k iterations, we further finetune the shared volume and the RGB branch. We use Adam optimizer with \(betas=(0.9,0.99)\). The learning rates for training the volume and MLP branch are respectively set to \(0.02\) and \(1e-4\). For finetuning the volume and the MLP, the learning rates are set to \(5e-3\) and \(5e-5\). We also employ a learning rate decay with a factor of \(0.1\).

The multi-scale pixel-level CLIP features of training views are pre-computed before training and the DINO features are computed with sampled patches on the fly during training. When computing \(\mathcal{L}_{supervision}\) and \(\mathcal{L}_{RDA}\), we randomly sample rays with a batch size of 4096. When computing \(\mathcal{L}_{FDA}\) we randomly sample patches of size \(256\times 256\) with a batch size of 8. We use a downsampling factor of 8 when sampling rays and a factor of 5 when sampling patches. The model is trained on an NVIDIA A5000 GPU with 24G memory for \(\sim\)1h30min for each scene.

### Dataset

We capture 10 scenes using smartphones and use Colmap [75] to extract camera parameters for each image. We capture \(20\sim 30\) images for each scene and the resolution of each image is \(4032\times 3024\). We follow the data structure of LLFF [76]. We manually annotate the segmentation maps of 5 views for each scene as the ground truth for evaluation.

We list the text labels used in our experiments in Tab. 3. Note that we sometimes choose to add a general word to describe the whole background, such as _wall_, _desktop_, following LERF [2]. The text labels in our dataset contain many long-tail classes, which can be used to fully assess the open-vocabulary capability.

\begin{table}
\begin{tabular}{c|c} \hline \hline
**Scene** & **Text Labels** \\ \hline \hline bed & red bag, black leather shoe, banana, \\  & hand, camera, _white sheet_ \\ \hline sofa & a stack of UNO cards, a red Nintendo Switch job-con controller, \\  & Pikachu, Gundam, Xbox wireless controller, _grey sofa_ \\ \hline lawn & red apple, New York Yankees cap, stapler, \\  & black headphone, hand soap, _green lawn_ \\ \hline room & shrilling chicken, weaving basket, rabbit, \\  & dinosaur, baseball, _wood wall_ \\ \hline bench & Portuguese egg tart, orange cat, green grape, \\  & mini offroad car, dressing doll, _pebbled concrete wall_, _wood_ \\ \hline table & a wooden ukulele, a beige mug, a GPU card with fans, \\  & a black Nike shoe, a Hatsune Miku statue, _lime wall_ \\ \hline office desk & the book of The Unbearable Lightness of Being, a can of red bull drink, \\  & a white keyboard, a pack of pocket tissues, _desktop_, _blue partition_ \\ \hline blue sofa & a bottle of perfume, sunglasses, a squirrel pig doll, \\  & a JBL bluetooth speaker, an aircon controller, _blue-grey sofa_ \\ \hline snacks & \begin{tabular}{c} Coke Cola, orange juice drink, calculator, \\ pitaya, Glico Pocky chocolate, biscuits sticks box, _desktop_ \\ \end{tabular} \\ \hline covered desk & 
\begin{tabular}{c} Winnie-the-Poon, Dove body wash, gerbera, \\ electric shaver, canned chili sauce, _desktop_ \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Dataset.** We list the collected 10 scenes and the corresponding text labels. The background labels are in _Italic font_.

## Appendix B More Ablations

We perform two more ablation studies on the Selection Volume and the FDA loss, as shown in Tab. 4 and Fig. 7. Without the Selection Volume, we simply average the multi-scale CLIP features rather than learning the appropriate scale. We can see that both the mIoU score and the Accuracy score are inferior to the full model. We could discard the dissimilar part \(neg\_F\) since dissimilar DINO features often impair the stability of the correlation loss. However, \(neg\_F\) encourages different segmentation probabilities for different semantic regions and it plays a crucial role for precise object boundary extraction.

## Appendix C More Evaluations

We additionally perform evaluations on human body, human head, indoor datasets with low-quality images [70, 74], and a complex scene from LERF datasets [2]. We compare with the concurrent work LERF qualitatively due to the lack of labels or the defective annotations as pointed out in [4]. We also perform experiments with different text prompts. We use the same scale level number and patch sizes in all comparisons.

Human body and head.As shown in Fig. 8, our method segments more precise parts than LERF. Specifically, LERF fails to segment the "head" in the human body and the "black T-shirt" in the human head. In contrast, our method can recognize and segment these parts correctly because our designed RDA loss addresses the ambiguity of the CLIP features effectively.

Indoor scenes with low-quality images.Fig. 9 shows experiments on the indoor datasets [70, 74], where many images are unrealistically rendered with less photorealistic appearances (as indicated in [4]) and have limited spatial resolution (\(640\times 480\) or \(1024\times 768\)). Due to these data constraints,

\begin{table}
\begin{tabular}{l|c c}  & **mIoU** & **Accuracy** \\ \hline \hline w/o \(neg\_F\) & 76.9 & 92.4 \\ w/o Selection & 84.8 & 95.3 \\ \hline
**full model** & **86.2** & **95.8** \\ \end{tabular}
\end{table}
Table 4: **More Ablations.**

Figure 8: Evaluation on human body dataset (left). Evaluation on human head dataset (right).

Figure 7: **More ablations.**our method sometimes confuses labels with similar appearances. However, we can see that our method still outperforms LERF by successfully segmenting more labels.

Complex scenes.Fig. 10 (left) shows the segmentation of one challenging sample from the LERF dataset, where the scene has complex geometry as well as many objects of varying sizes. It can be observed that LERF cannot segment most objects due to the ambiguities of CLIP features while our method can segment more objects correctly with more precise boundaries. Fig. 10 (right) shows a scene with multiple instances of a same class. Since instances of the same class often share similar appearance, texture, etc., they also have similar DINO features. As a result, FDA will not mistakenly segment them into different classes. The RDA loss will further help by assigning all these instances to the same text label. In the experiment, we observed that our method successfully segments all three apples into the same class with accurate boundaries.

Figure 11: Evaluation on rephrased texts.

Figure 10: Evaluation on a complex scene from LERF datasets (left). Evaluation on a scene with multiple instances of a same class (right).

Figure 9: Evaluation on indoor datasets with lower quality images.

[MISSING_PAGE_FAIL:19]

Figure 12: **bed.**

Figure 13: **sofa.**

Figure 14: **lawn.**

Figure 15: **room.**

Figure 16: **bench.**

Figure 17: **table.**

Figure 19: **blue sofa.**

Figure 18: **office desk.**

Figure 21: **covered desk.**

Figure 20: **snacks.**