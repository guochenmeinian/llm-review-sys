# What If the Input is Expanded in OOD Detection?

 Boxuan Zhang\({}^{1}\)  Jianing Zhu\({}^{2}\)  Zengmao Wang\({}^{1}\)  Tongliang Liu\({}^{3}\)  Bo Du\({}^{1}\)  Bo Han\({}^{2,4}\)

\({}^{1}\)School of Computer Science, Wuhan University

\({}^{2}\)TMLR Group, Department of Computer Science, Hong Kong Baptist University

\({}^{3}\)Sydney AI Center, The University of Sydney \({}^{4}\)RIKEN Center for Advanced Intelligence Project

{boxzhang1005, wangzengmao, dubo}@whu.edu.cn

{csjnzhu, bhanml}@comp.hkbu.edu.hk tongliang.liu@sydney.edu.cn

Equal ContributionCorrespondence to Zengmao Wang (wangzengmao@whu.edu.cn)

###### Abstract

Out-of-distribution (OOD) detection aims to identify OOD inputs from unknown classes, which is important for the reliable deployment of machine learning models in the open world. Various scoring functions are proposed to distinguish it from in-distribution (ID) data. However, existing methods generally focus on excavating the discriminative information from a single input, which implicitly limits its representation dimension. In this work, we introduce a novel perspective, i.e., employing different common corruptions on the input space, to expand that. We reveal an interesting phenomenon termed _confidence mutation_, where the confidence of OOD data can decrease significantly under the corruptions, while the ID data shows a higher confidence expectation considering the resistance of semantic features. Based on that, we formalize a new scoring method, namely, _Confidence a**Ver**age (CoVer), which can capture the dynamic differences by simply averaging the scores obtained from different corrupted inputs and the original ones, making the OOD and ID distributions more separable in detection tasks. Extensive experiments and analyses have been conducted to understand and verify the effectiveness of CoVer. The code is publicly available at: https://github.com/tmlr-group/CoVer.

## 1 Introduction

Out-of-distribution (OOD) detection [23; 28; 44] is important for reliable machine learning model deployment in open-world scenarios, where various samples from unknown classes, i.e., OOD data, are constantly emerging [4]. Deep neural networks [20] (DNNs) are demonstrated to be overconfident about these OOD data, which may result in disasters for some safety-critical applications [5; 21]. Traditional OOD detection methods [23; 29; 28; 30; 45; 46; 1; 61] design various scoring functions based on the outputs or representations extracted from well-trained models. Recently, some research also extended it into a zero-shot setting [31], which leverages the multi-modal information based on vision-language models (VLMs) and requires no further training on in-distribution (ID) data. A series of methods [48; 36; 52; 26] are proposed for improving OOD detection based on such advances.

Although promising progress has been achieved, existing methods mainly focus on excavating the discriminative information of a single input. For instance, ReAct [45], DICE [46], and ASH [1] integrates the activation regularization or reshaping to the forward path of a single input in single-modal DNNs; MCM [31] characterizes the confidence of a single input by the similarity between visual features and text representation of ID classes in VLMs. However, specializing in a single inputmay implicitly constrain the representation dimension for detection, leaving some hard-to-distinguish OOD samples with features similar to ID samples fail to be identified (refer to the distribution overlap in the left panel of Figure 1). Therefore, it naturally motivates the following critical research question: _Can we expand the dimension of the input space to explore OOD discriminative representations?_

In this work, we introduce a novel perspective to investigate that, i.e., employing the common corruptions [22] in the input space. Through a systematical comparison, we reveal an interesting phenomenon termed _confidence mutation_, where the confidence of OOD data can decrease significantly under the corruptions, while ID data shows higher confidence expectation considering different input dimensions. Specifically, as shown in Figure 1, corrupted inputs result in lower confidence in both OOD and ID data. However, one critical dynamic discovery is that its confidence about overconfident OOD data is changed more than the unconfident ID data under the same corruptions (refer to Figure 2), indicating a natural difference in feature-level resistance of the originally overlapped parts (refer to Figure 3). With the original inputs, we can find that the model is overall more confident in ID data.

Based on the above, we propose a new scoring framework, namely, _Confidence a**Verage**_ (CoVer), as illustrated in Figure 4. At the high level, we expand the original single representation dimension into multiple ones to excavate discriminative information. In detail, we introduce a simple but effective method for identifying OOD data with confidence mutation, which can be formalized as an average of OOD scores (e.g., Eq. (6)) obtained by different corrupted inputs and the original one. With the expectation among multiple input dimensions, CoVer can effectively reflect the knowledge of invariant semantic features that are discriminative from ID data to OOD data. It also matches an intuition that ID data can be more likely recognized as high confidence by models considering different input views, especially with the corruptions affecting the non-semantic high-frequency parts.

We conducted extensive experiments to verify the effectiveness of our proposed method. Since CoVer is an input-side design compatible with single-modal and multi-modal networks, we adpot various benchmarks for DNN-based and VLM-based OOD detection tasks. Under extensive evaluations, our CoVer can achieve the superior performance compared with different baselines. Moreover, CoVer exhibits excellent compatibility, as evidenced by the better performance of some methods combined with CoVer. Finally, a range of ablation studies of the scoring framework and further discussions from different perspectives are provided. In summary, our main contributions can be listed as follows,

* Conceptually, we introduce a novel perspective for identifying OOD inputs by considering the common corruptions to expand the representation dimensions. (in Section 3.1)
* We reveal an interesting phenomenon termed _confidence mutation_, where the confidences of OOD data can vary to significantly lower than ID data under corruptions (in Section 3.2).
* Technically, we formalize a novel scoring method, namely, _Confidence a**verage**_ (CoVer), a simple average of the confidence estimated from extended corrupted inputs and the original one. The corresponding empirical analysis is presented to understand it (in Section 3.3).

Figure 1: Comparison of scores distributions and detection results with different inputs for representation dimension expansion. _Left panel:_ results with a single original input; _Middle panel:_ results with a single corrupted input, which perform worse but have mutated scores for some OOD samples (see Figure 2); _Right panel:_ results with multiple inputs (CoVer), which achieve the variance reduction for the ID distribution and perform a better ID-OOD separability (see Figure 3 for more explanations).

* Empirically, extensive experiments on both traditional and zero-shot OOD detection benchmarks have verified the effectiveness and compatibility of our CoVer, and we conduct various ablations and further discussions to provide a comprehensive analysis (in Section 4).

## 2 Preliminaries

In this section, we briefly introduce the preliminaries of OOD detection on basic setups and the advanced zero-shot setting on VLMs. For related works, we leave detailed discussions in Appendix B.

Problem setups.Let \(\mathcal{X}\) be the input space and \(\mathcal{Y}=\{y_{1},...,y_{K}\}\) be the label space, where \(K\) is the number of ID classes. Given the ID random input \(x_{i}\in\mathcal{X}\) and OOD random input \(x_{o}\in\mathcal{X}\), we consider the ID marginal distribution \(\mathcal{D}_{\mathrm{ID}}\) and those with the OOD marginal distribution \(\mathcal{D}_{\mathrm{OOD}}\), where \(\mathcal{D}_{\mathrm{OOD}}\) is defined as an irrelevant distribution of which the label set has no intersection with \(\mathcal{Y}\). The goal of OOD detection is to figure out inputs with the OOD distribution \(\mathcal{D}_{\mathrm{ID}}\) from those with the ID distribution \(\mathcal{D}_{\mathrm{OOD}}\), which can be considered as a binary classification problem. For traditional OOD detection, given a model \(f\) trained on ID data with logit outputs, a score function \(S(\cdot)\) and a threshold \(\lambda\), the detection model \(g(\cdot)\) can be defined as,

\[g_{\lambda}(x)=\text{ID},\ \ \text{If }S(x;f)\geq\lambda;\ \text{ otherwise},\ \ g_{\lambda}(x)=\text{OOD}.\] (1)

where \(x\) is detected as ID data if and only if \(S(\mathbf{x})\geq\lambda\); otherwise, it is rejected as OOD data that should not be predicted by the model \(f\). Designing a practical \(S(x;f)\) is crucial for OOD detection.

CLIP-based vision-language modelsCLIP [42] has shown impressive performance in the zero-shot classification task by profiting from massive amounts of training data and large-size models. Here we briefly review the mechanism of CLIP-based VLMs. A CLIP-based model \(\mathbf{f}\) usually contains an image encoder \(\mathbf{f}^{\text{image}}\) and a text encoder \(\mathbf{f}^{\text{text}}\). Given a random input \(x\sim\mathcal{D}_{\mathrm{ID}}\) and a label \(y\sim\mathcal{Y}\), we use \(\mathbf{f}^{\text{image}}\) and \(\mathbf{f}^{\text{text}}\) to extract the image features \(h\in\mathbb{R}^{d}\) and the text features \(e_{j}\in\mathbb{R}^{d}\) as follows:

\[h=\mathbf{f}^{\text{image}}(x),\quad e_{j}=\mathbf{f}^{\text{ text}}(p(y_{j})),\quad\forall j=1,2,...,K,\] (2)

where \(p(\cdot)\) refers to the prompt template for the input label, \(d\) is the embedding dimension. The predictions are formulated as the cosine similarity between the image features \(h\) and text features \(e_{j}\),

\[\hat{y}=\operatorname*{arg\,max}_{y_{j}\in\mathcal{Y}}\{\text{ cos}(h,e_{j})\},\quad\text{where}\quad e_{j}=\mathbf{f}^{\text{text}}(p(y_{j})).\] (3)

Figure 2: Demonstration about detailed explanations for the discovery illustrated in Figure 1. The ID and OOD data here are divided into four groups, i.e., Confident ID, Unconfident ID, Overconfident OOD, and Unconfident OOD. _First Row_: the variation of confidence scores for ID and OOD data before and after being corrupted. The critical difference lies in the greater confidence declination for overconfident OOD data compared to unconfident ID data. (see Figure 3 for further discussion). _Second Row_: scatter maps of confidence scores sampled from the four groups under the same corruption, statistically supporting the findings of the first row. See Appendix C.2 for more details.

Zero-shot OOD detectionDifferent from traditional OOD detection methods based on a classifier \(f\) well-trained on single-modal, recent zero-shot OOD detection studies [31, 26] leverage a pre-trained VLM-based model (e.g. CLIP [42]) without any fine-tuning on ID training data. The text features from given ID label names (i.e. ID classes) as the class-wise weights functionally play the same role as the classifier. With guaranteed ID classification accuracy, the primary goal of zero-shot OOD detection in this paper is to distinguish OOD samples that do not belong to any known ID classes.

## 3 CoVer: Confidence Average

In this section, we formally present our proposed new scoring framework, i.e., _Confidence aVerage_ (CoVer). First, we introduce the motivation of representation dimension expansion and present the notable discovery (Section 3.1). Second, we conduct the exploration for the confidence mutation of overconfident OOD data considering the corrupted inputs (Section 3.2). Lastly, we provide the detailed implementation and analysis of our formalized CoVer score (Section 3.3).

### Representation Dimension Expansion

DNNs are demonstrated to be overconfident on those OOD samples, and a series of works [23, 45, 1, 54, 31, 26] are dedicated to eliminating the effects through the perspective of feature representation. However, achieving that is demonstrated to be hard as it generally requires careful optimization [54], or additional prior knowledge [36] on the single input. As illustrated in the middle panel of Figure 1, adopting some agnostic corruptions on the single input may result in worse separability between the ID and OOD distribution. Specializing in a single input seems to implicitly constrain the representation dimension for detection. In this work, we naturally raise the following question,

_What if we expand the dimension of representation for the original inputs_

_to enhance OOD discriminative representations?_

Using the same corruption adopted in Figure 1, we can conduct the dimension expansion by simultaneously considering both the corrupted variant and the original input. One notable discovery is that considering multiple inputs can achieve better performance on OOD detection, even though the single corruption transformation brings negative effects on identifying OOD samples. The surprising comparison results attract us to further explore the underlying mechanism of expanding the representation dimension for the original input, especially the dynamics before and after adopting the corruptions.

Figure 3: Visual exploration of random unconfident ID samples and the confidence mutation exemplified on random overconfident OOD samples under the same corruption. For each original input and its corrupted variant, we leverage the Fast Fourier Transformation to extract their low-frequency and high-frequency parts. _Left panel:_ visual investigation on unconfident ID samples with ID semantic features at low-frequency levels that are resistant to corruptions. _Right panel:_ an intuitive comparison of overconfident OOD samples, whose confidences show significant changes due to the elimination of non-semantic features at the high-frequency level. See Appendix C.4.2 for more detailed analyses.

### Confidence Mutation under Corruptions

Although employing corruption on the single inputs leads to worse ID-OOD separability, we can find obvious shifts toward less confidence in both OOD and ID distributions. It is expected for OOD data that corruption can help the model mitigate overconfidence, while the ID data are also affected severely and enlarge the overlap on the single dimension. In contrast, considering the multiple inputs by averaging the confidence scores shows variance reduction for ID distribution, which indicates the distinguishable dynamics between ID and OOD data. To elicit the underlying mechanism, we provide a definition to characterize the change of model confidence on the inputs under corruption.

**Definition 3.1** (Confidence Difference).: _Given a well-trained model \(f\) and a score function \(S(\cdot)\) measuring the confidence of \(f\) on an input \(x\), we have a basic static to characterize the differences between the original input and that under a corruption \(c(\cdot)\colon\operatorname{MU}_{c}(x,S,f)\triangleq(S(x;f)-S(c(x);f))\)._

Based on the comparison in Figure 1, we divide the ID and OOD data into four groups according to the model confidence on their original inputs, and present an overall comparison of the confidence differences in Figure 2. We reveal the critical dynamic differences in the corrupted variants of ID and OOD data, where both large \(\operatorname{MU}(x,s,f)\) exist in the data whose natural inputs own higher confidence in each part, demonstrating the model confidence on the overconfident OOD data decrease more than the unconfident ID data under the same corruption. We can get the empirical observation,

**Observation 3.2** (Confidence Mutation).: _Given the overconfidence OOD inputs \(x_{o}\in\mathcal{D}_{OOD}\), we can observe more significant differences in the change of confidences under the same corruption \(c(\cdot)\) than the ID samples \(x_{i}\in\mathcal{D}_{\text{ID}}\) with similar model confidence (constrained by \(\epsilon\)) on the natural inputs,_

\[\mathbb{E}_{x_{i}\sim\mathcal{D}_{\text{ID}}}(\operatorname{MU}_{c}(x_{i},S,f ))<\mathbb{E}_{x_{o}\sim\mathcal{D}_{\text{OOD}}}(\operatorname{MU}_{c}(x_{o},S,f)).\] (4)

In Figure 3, we further visualize the samples of unconfident ID data and overconfident OOD data. Under the comparison of saliency maps and the Fast Fourier Transformation, we find the confidence mutation reflects the feature level vulnerability of OOD data compared with ID data. Intuitively, the former can be severely affected by the common corruption to eliminate the non-semantic features, which generally exist at the high-frequency level. In contrast, the semantic feature of unconfident ID data can maintain confidence as the limited effects of corruption on the low-frequency part.

**Observation 3.3** (Resistance of ID features in frequency views).: _Assuming that ID data owns the ID semantic features existing at the low-frequency level (extract by \(\Gamma_{\xi}\)) while the OOD data has some non-semantic features at the high-frequency level for activating the high confidence of the model on its prediction, we can observe the following empirical relation on adopting the same corruptions,_

\[\mathbb{E}(\operatorname{MU}_{c}(x,S,f))\propto\operatorname{KL}((f(\Gamma_{ \xi}(c(x))))||f(\Gamma_{\xi}(x)).\] (5)

where \(\Gamma\) indicates the Fourier transformation. We suggest that common corruptions [22] might act as perturbations of high-frequency features within the input representation. For OOD samples, which inherently lack ID semantic features, altering high-frequency features could potentially lead to notable changes in model confidence, while the ID data shows relatively better resistance on it (see the left panel of Figure 3). This observation tentatively supports the notion that ID data maintains an overall higher confidence expectation under conditions of expanded representation dimension. To validate its generality, additional results involving various common corruptions are presented in Appendix C.2.2.

### Scoring Function Implementation and Analysis

Based on the previous understanding of confidence mutation, we formalize our CoVer, a new scoring framework for OOD detection. The procedure of CoVer mainly contains the following _four_ parts as illustrated in Figure 4, and the final averaged multi-dimensional scores can be provided as follows,

\[S_{\text{CoVer}}=\mathbb{E}_{x\sim d(\mathcal{X},\tilde{\mathcal{X}})}\max_{ i}\frac{e^{s_{i}(x)/\tau}}{\sum_{j=1}^{K}e^{s_{j}(x)/\tau}},\quad d(\mathcal{X}, \tilde{\mathcal{X}}):=\{x,c(x)|x\in\mathcal{X},c\in C\},\] (6)

where \(\mathbb{E}_{x\sim d(\mathcal{X},\tilde{\mathcal{X}})}\) is the confidence expecation over all scores dimensions, \(K\) is the number of ID classes, \(\tau\) is the temperature coefficient of the softmax function. In the following, we detailedly introduce the specific operations to obtain the final \(S_{\text{CoVer}}\) and the corresponding notations.

To enlarge the dimension of the original single input for confidence average, we introduce various corrupted inputs. In this work, we employ the corruption functions defined in [22], which consiststotal of 90 distinct corruptions. We provide the visualization of these different corruptions in Appendix C.4.1. Given the input space \(\mathcal{X}\) and a set of corruption functions \(C\), the corrupted inputs can be formulated as \(\{c(x)|x\in\mathcal{X},c\in C\}\rightarrow\tilde{\mathcal{X}}\), resulting in the multi-dimensional input spaces \(d(\mathcal{X},\tilde{\mathcal{X}})\).

Given an input image \(x\sim d(\mathcal{X},\tilde{\mathcal{X}})\), we adopt an image encoder with fixed parameters to extract the feature of the original dimension \(h_{O}\) and features of corrupted dimensions \(h_{1},...h_{N}\). Then we predict the logit output \(s(x)\) for each dimensional feature \(h_{d},\forall d=O,1,...,N\). For the DNN-based model \(f\), the outputs of these features are denoted as \(s(x)=f(x)=\mathrm{logits}_{d}\). For the VLM-based model, the outputs are label-wise matching scores based on the cosine similarity: \(s_{j}(x)=\frac{h_{d}\cdot e_{j}}{\|h_{d}\|\cdot\|e_{j}\|}\).

For the logit output \(s(x)\) predicted from a specific input dimension, we assign it with an OOD score to implement one dimension of the CoVer score. As shown in the right-middle panel of Figure 1, the OOD score can be formalized by some traditional scoring functions, like the softmax scoring function [23] (refer to Eq. (6)) and the free energy scoring function [30]. In addition, the OOD score can also be formulated by variants of some novel scoring functions, like those in CLIPN [52] and NegLabel [26]. The detailed implementations for alternative scoring functions can be found in Appendix C.3.

## 4 Experiments

In this section, we present the comprehensive verification of the proposed CoVer in the OOD detection benchmarks. First, we introduce several critical parts of experimental setups (in Section 4.1). Second, we provide the performance comparison and compatibility experiments of our CoVer with various DNN-based and VLM-based OOD detection methods (in Section 4.2). Third, we conduct extensive ablation studies and further discussions to understand the properties of our CoVer (in Section 4.3).

### Experimental Setups

In this part, we present the critical parts of experimental setups and leave more details in Appendix C.

Datasets.Following previous work [1; 31], we adopt the ImageNet-1K OOD benchmark [24], which uses the ImageNet-1K [14] as ID data and iNaturalist [49], SUN [55], Places [60], and Textures [7] as OOD data. For each of the OOD datasets, the classes do not overlap with the ID dataset. As the same as MCM [31], we also use subsets of ImageNet-1K for fine-grained analysis, like ImageNet-10 that mimics the class distribution of CIFAR-10 but with high-resolution images. For hard OOD evaluation, we exploit ImageNet-20 with 20 categories similar to ImageNet-10 in the semantic space

Figure 4: Overview of CoVer. _Left panel_: visualization of the raw input and inputs w.r.t different corruptions; _Left-middle panel_: procedures of logit outputs from single-modal and multi-modal networks; _Right-middle panel_: scoring functions that equip each dimensional output with an OOD score; _Right panel_: realization of CoVer by averaging OOD scores obtained from multiple dimensions.

(e.g., dog (ID) vs. wolf (OOD)). To have more experimental comparison, we also reproduce one setting from spurious OOD detection [35], whose hard OOD inputs are created to share the same background (i.e., water) as ID data but have different object labels (e.g., a boat rather than a bird). To select the most effective corruption types for each method, we use SVHN [37] as the validation set.

Model Setup.In this paper, we implement CoVer on various architectures, including DNN-like ResNet50, and VLM-like CLIP [42], AltCLIP [6], MetaCLIP [56], GroupViT [57]. Unless otherwise instructed, for VLM-based zero-shot OOD detection, we use CLIP-B/16 which consists of an image encoder based on ViT-B/16 Transformer [15] and a text encoder built with the masked self-attention Transformer [50]. We use the algorithmically generated corruptions defined in [22]. Each type of corruption has a severity level \(\epsilon\) from 1 to 5, with \(\epsilon=1\) being the least severe and increasing up to \(\epsilon=5\). By default, we use the CoVer score in the max-softmax form and set \(\tau=1\) as the temperature.

Baseline Methods and Evaluation Metrics.We compare the proposed method with various competitive methods. Specifically, we adopt Maximum Softmax Probability (MSP) [23], ODIN [29], Mahalanobis [28], Energy [30], ReAct [45], DICE [46] and ASH [1] as traditional OOD detection baseline methods. The VLM-based OOD detection methods we compared with include MCM, a method specifically designed for zero-shot OOD detection, as well as some traditional methods including MSP, ODIN, Energy, Mahalanobis, GradNorm [24], ViM [51], KNN [47], ZOC [18] that were re-implemented using a finetuned CLIP ViT-B/16 on the ImageNet-1K, see Appendix A for more details. For a fair comparison, we keep the original hyperparameter setups of the comparative methods and adopt the following metrics to evaluate the OOD detection performance: (1) the false positive rate (FPR95) of the OOD samples when the true positive rate (TPR) [29] of the in-distribution samples is at 95%, (2) the area under the receiver operating characteristic curve (AUROC) [13].

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{6}{c}{OOD Dataset} & \multirow{2}{*}{
\begin{tabular}{c} \\ iNaturalist \\ \end{tabular} } & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c}{Textures} & \multicolumn{2}{c}{Average} \\ \cline{2-10}  & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) \\ \hline MSP & 87.74 & 54.99 & 80.86 & 70.83 & 79.76 & 73.99 & 79.61 & 68.00 & 81.99 & 66.95 \\ ODIN & 91.37 & 41.57 & 86.89 & 53.97 & 84.44 & 62.15 & 87.57 & 45.53 & 87.57 & 50.80 \\ Mahalanobis & 52.65 & 97.00 & 42.41 & 98.50 & 41.79 & 98.40 & 85.01 & 55.80 & 55.47 & 87.43 \\ Energy score & 89.95 & 55.72 & 85.89 & 59.26 & 82.86 & 64.92 & 85.99 & 53.72 & 86.17 & 58.41 \\ ReAct & 96.22 & 20.38 & 94.20 & 24.20 & 91.58 & 33.85 & 89.80 & 47.30 & 92.95 & 31.43 \\ DICE & 94.49 & 25.63 & 90.83 & 35.15 & 87.48 & 46.49 & 90.30 & 31.72 & 90.77 & 34.75 \\ DICE+ReAct & 96.24 & 18.64 & 93.94 & 25.45 & 90.67 & 36.86 & 92.74 & 28.07 & 93.40 & 27.25 \\ ASH-B & 94.25 & 28.95 & 93.02 & 40.21 & 87.52 & 95.21 & 95.33 & 34.89 & 90.91 & 39.04 \\
**ASH-B + CoVer** & 97.14 & 14.04 & 94.12 & 25.77 & 91.05 & 35.93 & 91.93 & 30.39 & 93.56 & 26.53 \\ ASH-S & 97.88 & 11.38 & 94.04 & 27.96 & 91.03 & 39.74 & **97.62** & **11.88** & 95.14 & 22.74 \\
**ASH-S + CoVer** & **98.33** & **8.73** & **94.59** & **26.63** & **91.47** & **38.06** & 97.22 & 13.92 & **95.40** & **21.83** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with competitive OOD detection baselines based on ResNet-50. The ID data are ImageNet-1K. \(\uparrow\) indicates larger values are better and \(\downarrow\) indicates smaller values are better.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{6}{c}{OOD Dataset} & \multirow{2}{*}{
\begin{tabular}{c} \\ iNaturalist \\ \end{tabular} } & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c}{Textures} & \multicolumn{2}{c}{Average} \\ \cline{2-10}  & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) \\ \hline \multicolumn{10}{c}{**Requires training (or w. fine-tuning)**} \\ MSP & 87.44 & 58.36 & 79.73 & 73.72 & 79.67 & 74.41 & 79.69 & 71.93 & 81.63 & 69.61 \\ ODIN & 94.65 & 30.22 & 87.17 & 54.04 & 85.54 & 55.06 & 87.85 & 51.67 & 88.80 & 47.75 \\ Energy & 95.33 & 26.12 & 92.66 & 35.97 & 91.41 & 39.87 & 86.76 & 57.61 & 91.54 & 39.89 \\ GradNorm & 72.56 & 81.50 & 72.86 & 82.00 & 73.70 & 80.41 & 70.26 & 79.36 & 72.35 & 80.82 \\ ViM & 93.16 & 32.19 & 87.19 & 54.01 & 83.75 & 60.67 & 87.18 & 53.94 & 87.82 & 50.20 \\ KNN & 94.52 & 29.17 & 92.67 & 35.62 & 91.02 & 39.61 & 85.67 & 64.35 & 90.97 & 42.19 \\ \hline \multicolumn{10}{c}{**Zero-shot (to training required)**} \\ Mahalanobis & 56.22 & 99.22 & 60.89 & 99.28 & 68.96 & 98.31 & 65.36 & 98.15 & 62.86 & 98.74 \\ Energy & 85.54 & 80.49 & 84.21 & 78.75 & 84.81 & 72.29 & 66.63 & 92.89 & 80.30 & 81.11 \\ ZOC & 86.09 & 87.30 & 81.20 & 81.51 & 83.39 & 73.06 & 76.46 & 98.90 & 81.79 & 85.19 \\ MCM & 94.61 & 30.95 & 92.57 & 37.57 & 89.77 & 44.65 & 86.10 & 57.77 & 90.76 & 42.73 \\
**CoVer (ours)** & **95.98** & **22.55** & **93.42** & **32.85** & **90.27** & **40.71** & **90.14** & **43.39** & **92.45** & **34.88** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison with competitive OOD detection baselines based on CLIP-B/16. The ID data are ImageNet-1K. \(\uparrow\) indicates larger values are better and \(\downarrow\) indicates smaller values are better.

[MISSING_PAGE_FAIL:8]

of our CoVer on different backbones of CLIP, and the larger backbone boosts the performance of OOD detection. The second part shows that CoVer can generalize better to various VLM architectures compared to MCM. More fine-grained results on different OOD sets can be found in Appendix C.2.1.

Superiority of multi-dimensional scoring framework.To verify the superiority of our CoVer with a multi-dimensional scoring framework compared with the uni-dimensional one, we report the performance comparison using different corruption types in Figure 5(a). Here, the uni-dimensional framework is denoted as using a single corrupted input for confidence estimation. The results indicate that extended dimensions provide valuable clues for enhancing OOD discriminative representations, thus enlarging the separability between ID and OOD data. We leave the fine-grained results and further discussions in Appendix C.2.2, which provide a better understanding of the improvement.

Significance of the number of expanded measuring dimensions.As a critical aspect of our CoVer, the number of the extended confidence measuring dimensions will control the performance enhancement for OOD detection. In Figure 5(b), we present results for varying the number of expanded dimensions using various corruptions with the same severity level. The results demonstrate that an increasing number of expanded representation dimensions gradually improves the performance then probably declines, while consistently outperforming the baseline. This indicates that the addition of measuring dimensions prioritizes enhancing the distinction of OOD data with mutated confidence while ID data shows resistance. More detailed results and analyses are provided in Appendix C.2.3.

Comparison of the variation trends in different corruption severity levels.In Figure 5(c), we show the performance by varying the severity level \(\epsilon\) for each specific corruption style. We can observe that three types of trends emerge with an increasing level of severity, i.e., up, down, and up then down. This phenomenon indicates that an appropriate level of corruption is critical for the optimization of CoVer. One possible reason may be that the threshold maximizing the distinction between ID and OOD data varies from different types of corruption. To further explain this observation, we provide more results on the \(\epsilon\) among various corruption styles and more discussions in Appendix C.2.4.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline ID Dataset & OOD Dataset & Method & AUROC\(\uparrow\) & FPR95\(\downarrow\) \\ \hline \multirow{2}{*}{ImageNet-10} & \multirow{2}{*}{ImageNet-20} & MCM & 88.99 & 49.79 \\  & & **CoVer** & **89.98** & **46.18** \\ \hline \multirow{2}{*}{ImageNet-20} & \multirow{2}{*}{ImageNet-100} & MCM & **90.21** & **44.78** \\ \cline{2-4}  & & **CoVer** & 91.49 & 38.17 \\ \hline \multirow{2}{*}{AlcCLIP} & \multirow{2}{*}{VIT-L/14} & MCM & 91.54 & 40.74 \\  & & **CoVer** & **93.03** & **32.15** \\ \hline \multirow{2}{*}{MetaCLIP} & \multirow{2}{*}{VIT-B/16-quickgeh} & MCM & 87.57 & 85.97 \\  & & **CoVer** & **88.64** & **55.68** \\ \hline \multirow{2}{*}{GroupVIT} & \multirow{2}{*}{GroupVIT} & MCM & 85.10 & 57.85 \\  & & **CoVer** & **86.94** & **51.19** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Zero-shot OOD detection performance comparison on hard OOD detection tasks. All methods are based on CLIP-B/16.

Figure 5: Ablation Study. (a) superiority of the multi-dimensional scoring framework; (b) exploration of different quantity of expanded input dimensions; (c) using different severity levels of a specific corruption type; (d) comparison with different realizations for each dimensional confidence score.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Architecture & Backbone & Method & AUROC\(\uparrow\) & FPR95\(\downarrow\) \\ \hline \multirow{4}{*}{CLIP} & ResNet50 & MCM & 88.99 & 49.79 \\  & & **CoVer** & **89.98** & **46.18** \\ \cline{2-5}  & \multirow{2}{*}{VIT-B/32} & MCM & 89.82 & 45.75 \\  & & **CoVer** & **90.21** & **44.78** \\ \cline{2-5}  & & **VIT-L/14** & MCM & 91.49 & 38.17 \\ \cline{2-5}  & & **CoVer** & **92.61** & **32.97** \\ \hline \multirow{2}{*}{AlcCLIP} & \multirow{2}{*}{VIT-L/14} & MCM & 91.54 & 40.74 \\  & & **CoVer** & **93.03** & **32.15** \\ \hline \multirow{2}{*}{MetaCLIP} & \multirow{2}{*}{VIT-B/16-quickgeh} & MCM & 87.57 & 85.97 \\  & & **CoVer** & **88.64** & **55.68** \\ \hline \multirow{2}{*}{GroupVIT} & \multirow{2}{*}{GroupVIT} & MCM & 85.10 & 57.85 \\  & & **CoVer** & **86.94** & **51.19** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Zero-shot OOD detection performance with different VLM architectures’ representations on ImageNet-1K(ID).

Generality of integrating with different OOD detection schemes.Since the proposed CoVer is a general scoring framework with an average of confidence scores measured from multiple dimensions, the specific realization for each dimensional score have multiple choices. In Figure 5(d), we present the comparison with different realizations integrated w./w.o. CoVer (e.g., ResNet-50 based DICE [46] and ASH-B [1], CLIP-B/16 based MCM [31] and NegLabel [26]), where they have different performance improvements compared with the original baseline without constraints for the modality.

## 5 Discussion

### Broader Impact

OOD detection is crucial for deploying reliable deep learning systems in real-world applications [21], ensuring models can effectively identify data that differ significantly from the training distribution. This ability is vital in safety-critical areas [5], where incorrect predictions due to unexpected inputs can lead to severe consequences. For instance, in the field of autonomous driving, OOD detection helps the system recognize and react appropriately to novel scenarios not covered during training, such as new road signs or altered traffic conditions due to construction. This is imperative as it prevents autonomous vehicles from making potentially hazardous decisions based on learned but now irrelevant data, thereby enhancing their safety and robustness in dynamic environments.

Our research highlights a fundamental yet overlooked challenge in existing OOD detection methods, which often specialize in a single input type. This specialization may inadvertently limit the representational dimensions for detection, complicating the identification of subtle OOD samples. For effective OOD detection, it is crucial to not only improve empirical performance through enhanced OOD discriminative representations but also to address this pervasive issue within the general scoring framework. Our new scoring framework leverages expanded input dimensions and utilizes a confidence score expectation to address these concerns, which also shares similar intuitions with some related work [43] in adversarial defense via random transformation. Comprehensive experiments demonstrate its effectiveness and compatibility, suggesting that our method is potentially a new generalized framework and provides new insights into OOD detection from a different perspective.

### Limitation

While our method introduces a promising framework for OOD detection and provides unique insights through the use of corrupted images to enhance representational dimensions, it does face certain challenges. First, our method indeed faces several failure cases. When CoVer utilizes certain severe corruption types (e.g., Spatter, Elastic transform), its performance is worse than with single input. This is because these types are more severe compared to others, leading to excessive damages to semantic features. Effective corruption types are those only perturb non-semantic features, which generally exist at the high-frequency level, resulting in different confidence variations between ID and OOD data. Notably, except for leveraging the validation set, our approach lacks a standardized criterion for selecting the types and intensities of corruptions, which is essential for uniform effectiveness across various scenarios. Additionally, the expansion of input dimensions, though beneficial for detection accuracy, may lead to increased evaluation times. These limitations highlight areas for potential improvement, particularly in balancing detection capabilities with computational efficiency. Despite these challenges, the enhanced detection capabilities our method introduces mark a significant step forward in the reliability of machine learning models against OOD inputs.

## 6 Conclusion

In this paper, we introduce a novel perspective for OOD detection, i.e., expanding the representation dimensions. With the different common corruptions, we reveal an interesting phenomenon termed _confidence mutation_, where the confidence values of some overconfident OOD samples can vary significantly compared with the original inputs. To this end, we propose a new scoring framework, namely, _Confidence aVerge_ (CoVer), which simultaneously considers the original and expanded input dimensions. Adopting a simple but effective average operation, CoVer can capture the dynamical discrimination of OOD samples and better enhance the separability of ID and OOD distributions. We have conducted extensive experiments to present its effectiveness and compatibility with different methods. We hope our work can draw new insights from a different view on OOD detection.

## 7 Acknowledgement

BXZ, ZMW and BD were supported by the National Key Research and Development Program of China 2023YFC2705700, National Natural Science Foundation of China under Grants 62225113, 62271357, Natural Science Foundation of Hubei Province under Grants 2023BAB072, the Innovative Research Group Project of Hubei Province under Grants 2024AFA017, the Fundamental Research Funds for the Central Universities under Grants 2042023kf0134, Wuhan Natural Science Foundation 2024040801020236, and the numerical calculations in this paper have been done on the supercomputing system in the Supercomputing Center of Wuhan University. JNZ and BH were supported by NSFC General Program No. 62376235, Guangdong Basic and Applied Basic Research Foundation Nos. 2022A1515011652 and 2024A1515012399, RIKEN Collaborative Research Fund, HKBU Faculty Niche Research Areas No. RC-FNRA-IG/22-23/SCI/04, and HKBU CSD Departmental Incentive Scheme.

## References

* [1] Djurisic Andrija, Bozanic Nebojsa, Ashok Arjun, and Liu Rosanne. Extremely simple activation shaping for out-of-distribution detection. In _ICLR_, 2023.
* [2] Haoyue Bai, Gregory Canal, Xuefeng Du, Jeongyeol Kwon, Robert D Nowak, and Yixuan Li. Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection. In _ICML_, 2023.
* [3] Julian Bitterwolf, Alexander Meinke, Maximilian Augustin, and Matthias Hein. Breaking down out-of-distribution detection: Many methods based on OOD training data estimate a combination of the same core quantities. In _ICML_, 2022.
* [4] Julian Bitterwolf, Maximilian Mueller, and Matthias Hein. In or out? fixing imagenet out-of-distribution detection evaluation. In _ICML_, 2023.
* [5] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. In _arXiv_, 2021.
* [6] Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, and Ledell Wu. Altclip: Altering the language encoder in clip for extended language capabilities. In _arXiv_, 2022.
* [7] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In _CVPR_, 2014.
* [8] Pierre Colombo, Eduardo Dadalto, Guillaume Staerman, Nathan Noiry, and Pablo Piantanida. Beyond mahalanobis distance for textual ood detection. In _NeurIPS_, 2022.
* [9] Pierre Colombo, Marine Picot, Federica Granese, Marco Romanelli, Francisco Messina, and Pablo Piant. A halfspace-mass depth-based method for adversarial attack detection. _TMLR_, 2023.
* [10] Pierre Colombo, Marine Picot, Nathan Noiry, Guillaume Staerman, and Pablo Piantanida. Toward stronger textual attack detectors. In _arXiv_, 2023.
* [11] Maxime Darrin, Pablo Piantanida, and Pierre Colombo. Rainproof: An umbrella to shield text generators from out-of-distribution data. In _arXiv_, 2022.
* [12] Maxime Darrin, Guillaume Staerman, Eduardo Dadalto Camara Gomes, Jackie CK Cheung, Pablo Piantanida, and Pierre Colombo. Unsupervised layer-wise score aggregation for textual ood detection. In _AAAI_, 2024.
* [13] Jesse Davis and Mark Goadrich. The relationship between precision-recall and ROC curves. In _ICML_, 2006.
* [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.

* [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _arXiv_, 2020.
* [16] Xuefeng Du, Yiyou Sun, Jerry Zhu, and Yixuan Li. Dream the impossible: Outlier imagination with diffusion models. In _NeurIPS_, 2023.
* [17] Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. VOS: learning what you don't know by virtual outlier synthesis. In _ICLR_, 2022.
* [18] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detection based on the pre-trained model clip. In _AAAI_, 2022.
* [19] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. In _NeurIPS_, 2021.
* [20] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. _Deep learning_. MIT Press, 2016.
* [21] Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joseph Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. In _ICML_, 2022.
* [22] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _ICLR_, 2019.
* [23] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In _ICLR_, 2017.
* [24] Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional shifts in the wild. In _NeurIPS_, 2021.
* [25] Wenyu Jiang, Hao Cheng, Mingcai Chen, Chongjun Wang, and Hongxin Wei. Dos: Diverse outlier sampling for out-of-distribution detection. In _arXiv_, 2023.
* [26] Xue Jiang, Feng Liu, Zhen Fang, Hong Chen, Tongliang Liu, Feng Zheng, and Bo Han. Negative label guided ood detection with pretrained vision-language models. In _ICLR_, 2024.
* [27] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based learning. In _Predicting structured data_, 2006.
* [28] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In _NeurIPS_, 2018.
* [29] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In _ICLR_, 2018.
* [30] Weitang Liu, Xiaoyun Wang, John D Owens, and Yixuan Li. Energy-based out-of-distribution detection. In _NeurIPS_, 2020.
* [31] Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li, and Yixuan Li. Delving into out-of-distribution detection with vision-language representations. In _NeurIPS_, 2022.
* [32] Yifei Ming, Ying Fan, and Yixuan Li. Poem: Out-of-distribution detection with posterior sampling. In _ICML_, 2022.
* [33] Yifei Ming and Yixuan Li. How does fine-tuning impact out-of-distribution detection for vision-language models? _IJCV_, 132(2):596-609, 2024.
* [34] Yifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li. How to exploit hyperspherical embeddings for out-of-distribution detection? In _arXiv_, 2022.
* [35] Yifei Ming, Hang Yin, and Yixuan Li. On the impact of spurious correlation for out-of-distribution detection. In _AAAI_, 2022.

* [36] Atsuyuki Miyai, Qing Yu, Go Irie, and Kiyoharu Aizawa. Locoop: Few-shot out-of-distribution detection via prompt learning. In _NeurIPS_, 2023.
* [37] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In _NeurIPS Workshop_, 2011.
* [38] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In _CVPR_, 2015.
* [39] Jaewoo Park, Yoon Gyo Jung, and Andrew Beng Jin Teoh. Nearest neighbor guidance for out-of-distribution detection. In _CVPR_, 2023.
* [40] Marine Picot, Nathan Noiry, Pablo Piantanida, and Pierre Colombo. Adversarial attack detection under realistic constraints. 2022.
* [41] Marine Picot, Guillaume Staerman, Federica Granese, Nathan Noiry, Francisco Messina, Pablo Piantanida, and Pierre Colombo. A simple unsupervised data depth-based method to detect adversarial images. 2023.
* [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.
* [43] Edward Raff, Jared Sylvester, Steven Forsyth, and Mark McLean. Barrage of random transforms for adversarially robust defense. In _CVPR_, 2019.
* [44] Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark A. DePristo, Joshua V. Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In _NeurIPS_, 2019.
* [45] Yiyou Sun, Chuan Guo, and Yixuan Li. React: Out-of-distribution detection with rectified activations. In _NeurIPS_, 2021.
* [46] Yiyou Sun and Yixuan Li. Dice: Leveraging sparsification for out-of-distribution detection. In _ECCV_, 2022.
* [47] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In _ICML_, 2022.
* [48] Leitian Tao, Xuefeng Du, Xiaojin Zhu, and Yixuan Li. Non-parametric outlier synthesis. In _ICLR_, 2023.
* [49] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In _CVPR_, 2018.
* [50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.
* [51] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In _CVPR_, 2022.
* [52] Hualiang Wang, Yi Li, Huifeng Yao, and Xiaomeng Li. Clipn for zero-shot ood detection: Teaching clip to say no. In _ICCV_, 2023.
* [53] Qizhou Wang, Zhen Fang, Yonggang Zhang, Feng Liu, Yixuan Li, and Bo Han. Learning to augment distributions for out-of-distribution detection. In _NeurIPS_, 2023.
* [54] Qizhou Wang, Feng Liu, Yonggang Zhang, Jing Zhang, Chen Gong, Tongliang Liu, and Bo Han. Watermarking for out-of-distribution detection. In _NeurIPS_, 2022.
* [55] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _CVPR_, 2010.

* [56] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. In _arXiv_, 2023.
* [57] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In _CVPR_, 2022.
* [58] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, et al. Openood: Benchmarking generalized out-of-distribution detection. In _NeurIPS Datasets and Benchmarks Track_, 2022.
* [59] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. In _arXiv_, 2021.
* [60] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. _TPAMI_, 40(6):1452-1464, 2017.
* [61] Jianing Zhu, Yu Geng, Jiangchao Yao, Tongliang Liu, Gang Niu, Masashi Sugiyama, and Bo Han. Diversified outlier exposure for out-of-distribution detection via informative extrapolation. In _NeurIPS_, 2023.
* [62] Jianing Zhu, Hengzhuang Li, Jiangchao Yao, Tongliang Liu, Jianliang Xu, and Bo Han. Unleashing mask: Explore the intrinsic out-of-distribution detection capability. In _ICML_, 2023.

## Appendix for CoVer

The whole Appendix is organized as follows. In Appendix A, we present the detailed definitions and implementation of baseline methods that are considered in our exploration. In Appendix B, we provide detailed discussions about related works. In Appendix C, we provide extra experimental details and more comprehensive results with further discussion on the underlying implications. In Appendix D, we provide some preliminary statistical analysis about CoVer. Finally, in Appendix E, we provide the further analysis for a better understanding of our work.

### Reproducibility Statement

To ensure the reproducibility of experimental results, we provide the source code at https://github.com/tmlr-group/CoVer. Below we summarize several important aspects to facilitate reproducible results:

* **Datasets.** The datasets we used are all publicly accessible, which is introduced in Section 4. Following MCM [31], we also use subsets of ImageNet-1K for fine-grained analysis, like ImageNet-10. For hard OOD evaluation, we exploit ImageNet-20 with 20 categories similar to ImageNet-10. We also reproduce the spurious OOD detection [35] with \(r=0.9\), which determines relative size of majority vs. minority groups.
* **Assumption.** We set our main experiments to a zero-shot scenario where a well-trained CLIP-like model on the original classification task is available [42]. Under this assumption, CoVer detects OOD samples in parallel with the zero-shot classification task and has no impact on ID classification performance.
* **Open source.** The source code is available at https://github.com/tmlr-group/CoVer. We provide a backbone for our experiments as well as several auxiliary components, such as score estimation.
* **Environment.** All experiments are conducted on NVIDIA GeForce RTX 3090 GPUs with Python 3.10 and PyTorch 2.2.

## Appendix A Details about Considered Baselines and Metrics

In this section, we provide details about the baselines for the scoring functions, as well as the corresponding hyper-parameters and other related metrics that are considered in our work.

Maximum Softmax Probability (MSP).[23] proposes to use maximum softmax probability to discriminate ID and OOD samples. The score is defined as follows,

\[S_{\text{MSP}}(x;f)=\max_{c}P(y=c|x;f)=\max\texttt{ softmax}(f(x)),\] (7)

where \(f\) represents the given well-trained model and \(c\) is one of the classes \(\mathcal{Y}=\{1,\dots,C\}\). The larger softmax score indicates the larger probability for a sample to be ID data, reflecting the model's confidence on the sample.

Odin.[29] designed the ODIN score, leveraging the temperature scaling and tiny perturbations to widen the gap between the distributions of ID and OOD samples. The ODIN score is defined as follows,

\[S_{\text{ODIN}}(x;f)=\max_{c}P(y=c|\tilde{x};f)=\max\texttt{ softmax}(\frac{f(\tilde{x})}{T}),\] (8)

where \(\tilde{x}\) represents the perturbed samples (controled by \(\epsilon\)), \(T\) represents the temperature. For fair comparison, we adopt the suggested hyperparameters [29]: \(\epsilon=1.4\times 10^{-3}\), \(T=1.0\times 10^{4}\).

Mahalanobis.[28] introduces a Mahalanobis distance-based confidence score, exploiting the feature space of the neural networks by inspecting the class conditional Gaussian distributions. The Mahalanobis distance score is defined as follows,

\[S_{\text{Mahalanobis}}(x;f)=\max_{c}-(f(x)-\hat{\mu}_{c})^{T}\hat{\Sigma}^{-1} (f(x)-\hat{\mu}_{c}),\] (9)where \(\hat{\mu}_{c}\) represents the estimated mean of multivariate Gaussian distribution of class \(c\), \(\hat{\Sigma}\) represents the estimated tied covariance of the \(C\) class-conditional Gaussian distributions.

Energy.[30] proposes to use the Energy of the predicted logits to distinguish the ID and OOD samples. The Energy score is defined as follows,

\[S_{\text{Energy}}(x;f)=-T\log\sum_{c=1}^{C}e^{f(x)_{c}/T},\] (10)

where \(T\) represents the temperature parameter. As theoretically illustrated in [30], a lower Energy score indicates a higher probability for a sample to be ID. Following [30], we fix the \(T\) to \(1.0\) throughout all experiments.

Ash.[1] designs a extremely simple, post-hoc method called Activation SHaping for OOD detection. It removes a large portion of an input's activation at a late layer and adjusts the rest of the activation values by scaling them up or assigning them a constant value. The simplified representation is then passed throughout the rest of the network. The logit output is used to classify ID samples and calculate scores for OOD detection as usual. For ASH-B version, we adopt the MSP score and implement it with the hyperparameter \(p=65\); For ASH-S version, we apply it with energy score and the hyperparameter \(p=90\). Both settings are suggested by [1].

Mcm.[31] proposes Maximum Concept Matching (MCM), a simple yet effective zero-shot OOD detection method based on aligning visual features with textual concepts. Formally, the MCM score can be defined as:

\[S_{\text{MCM}}\left(\mathbf{x}^{\prime};\mathcal{Y}_{\text{in}},\mathcal{T}, \mathcal{I}\right)=\max_{i}\frac{e^{s_{i}\left(\mathbf{x}^{\prime}\right)/ \tau}}{\sum_{j=1}^{K}e^{s_{j}\left(\mathbf{x}^{\prime}\right)/\tau}}\] (11)

Clipn.[52] proposes a novel CLIP architecture, which equips CLIP with a "no" logic via the learnable "no" prompts and a "no" text encoder. Specifically, CLIPN proposes two novel inference algorithms to perform OOD detection via using negation semantics, where the algorithm named agreeing-to-differ (ATD) is more effective in experimental results. The ATD form of the CLIPN score can be formulated as follows,

\[S_{\text{CLIPN}}(x)=\sum_{j=1}^{C}\frac{e^{s_{i,j}(x)/\tau}}{e^{s_{i,j}(x)/ \tau}+e^{s_{i,j}^{no}(x)/\tau}}\cdot\frac{e^{s_{i,j}(x)/\tau}}{\sum_{k=1}^{C} e^{s_{i,k}(x)/\tau}}\] (12)

where C is the number of classes, \(s_{i,j}(x)\) and \(s_{i,j}^{no}(x)\) are denoted as the inner product of the image feature and the corresponding text feature

\[s_{i,j}(x)=<\textbf{f}^{\text{image}}(x),\textbf{f}^{\text{text}}(p(y_{j}))>,\quad s_{i,j}^{no}(x)=<\textbf{f}^{\text{image}}(x),\textbf{f}^{\text{text}} _{\text{no}}(p(y_{j}^{no}))>\] (13)

where \(\textbf{f}^{\text{text}}_{\text{no}}\) is the "no" text encoder and \(p(y_{j}^{no})\) the text with "no" logic.

NegLabel.[26] proposes a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases and designs a novel scheme for the OOD score collaborated with negative labels. NegLabel score can be formulated as

\[S_{\text{NegLabel}}(\mathbf{x})=S^{*}\left(\operatorname{sim}(\mathbf{x}, \mathcal{Y}),\operatorname{sim}\left(\mathbf{x},\mathcal{Y}^{-}\right)\right)\] (14)

where \(S^{*}\left(\cdot,\cdot\right)\) represents a fusion function that combines the similarity of a sample with ID labels \(\operatorname{sim}(\mathbf{x},\mathcal{Y})\) and the similarity of the sample with negative labels \(\operatorname{sim}(\mathbf{x},\mathcal{Y}^{-})\). The sum-softmax form of NegLabel score is defined as follows,

\[S_{\text{NegLabel}}(x)=\frac{\sum_{i=1}^{K}e^{s_{i}(x)/\tau}}{\sum_{i=1}^{K}e^ {s_{i}(x)/\tau}+\sum_{j=1}^{M}e^{s_{j}^{\text{seg}}(x)/\tau}}\] (15)

where \(K\) is the number of ID labels, \(\tau\) is the temperature coefficient of the softmax function and \(M\) is the number of negative labels, \(s_{i}(x)\) and \(s_{j}^{\text{seg}}(x)\) are formulated as the cosine similarity, defined as follows,

\[s_{i}(x)=\cos(\textbf{f}^{\text{image}}(x),\boldsymbol{e}_{i}),\quad s_{j}^{ \text{seg}}(x)=\cos(\textbf{f}^{\text{image}}(x),\widetilde{e}_{j})\] (16)Detailed Discussion with Related Works

In this section, we provide detailed discussions about related works.

Traditional OOD Detection.There has been an increasing interest in OOD detection since the phenomenon of overconfidence in OOD data was first discovered in [38]. As a formal benchmark for OOD detection, [23] proposed using softmax prediction probability as a conventional baseline method. Afterward, numerous approaches have been developed to address visual OOD detection, which can be classified into two categories, i.e., post hoc scoring mechanism and training-time regularization [59, 58]. Post hoc methods are dedicated to exploring a better OOD score by freezing the model's parameters. ODIN [29] improves the previous MSP [23] by scaling with the temperature and slightly perturbing the inputs. Mahalanobis introduces the Mahalanobis distance in the feature space to measure the confidence score. Energy [30] exploits the energy function [27] to distinguish ID and OOD data. Both ReAct [45] and DICE [46] are improved from Energy, ReAct by feature clipping, and DICE by discarding the most prominent weights in the fully connected layer. ASH [1] designs an extremely simple method that removes a large portion of an input's activation and adjusts the rest. On the other hand, training-time regularization methods exploit the potential access to partial OOD data during model training. MOS [24] groups all classes and introduces an extra class to each group to reformulate the loss function during training. VOS [17] enhances the quality of the energy score by creating synthetic virtual anomalies. CIDER [34] exploits KNN [47] to boost OOD detection performance through the optimization of contrastive loss. DAOL [53] alleviates the OOD distribution discrepancy by crafting an OOD distribution set that contains all distributions in a Wasserstein ball centered on the auxiliary OOD distribution. The presence of outliers leads to superior performance compared to training without outliers, as evidenced by numerous previous studies [3, 25, 2, 16, 62].

OOD Detection with vision-language representations.With the rapid development of multi-modal large language models (MLLMs), such as CLIP [42], much attention has been paid to OOD detection with vision-language representations [33]. MCM [31] proposed the first zero-shot OOD detection framework that combines the temperature scaling strategy and maximum softmax probability as the OOD score. Following MCM, some works fine-tuned CLIP's image encoder for visual OOD detection [48, 36]. NPOS [48] utilized generated OOD data to optimize the ID-OOD decision boundary. LoCoOp exploited the portions of CLIP's local features as OOD features to realize OOD regularization. Some latest methods [52, 26] boosted OOD detection by adding extra clues obtained from negative textual information. CLIPN [52] equipped CLIP with a "no" logic via a text encoder that can understand negative prompts. NegLabel [26] introduced numerous negative labels and distinguished OOD samples by examining their affinities between ID and negative labels.

Data depths and information projections.Computing OOD scores on the embedding output of the last layer of the encoder is not the best choice for textual OOD detection. To address this, [12] proposed aggregating OOD scores across all layers and introduced an extended text OOD classification benchmark, MILTOOD-C. In a similar vein, RainProf [11] introduced a relative information projection framework and a new benchmark called LOFTER on text generators, considering both OOD performance and task-specific metrics. Building on the idea of information projection, REFEREE [40] leveraged I-projection to extract relevant information from the softmax outputs of a network for black-box adversarial attack detection. On the other hand, APPROVED [41] proposed to compute a similarity score between an input sample and the training distribution using the statistical notion of data depth at the logit layer. HAMPER [9] introduced a method to detect adversarial examples by utilizing the concept of data depths, particularly the halfspace-mass (HM) depth, known for its attractive properties and non-differentiability. Furthermore, TRUSTED [8] relied on the information available across all hidden layers of a network, leveraging a novel similarity score based on the Integrate Rank-Weighted depth for textual OOD detection. LAROUSSE [10] employed a new anomaly score built on the HM depth to detect textual adversarial attacks in an unsupervised manner.

## Appendix C Additional Experimental Results and Further Discussion

In this section, we provide additional experimental results from different perspectives to verify the effectiveness our proposed CoVer. First, we introduce the additional experimental setups for the empirical verification in previous figures and implement details about our method. Second, we 

[MISSING_PAGE_FAIL:18]

performance under the single-dimensional scoring framework with a single corrupted image as the input sample. However, considering the multiple inputs, i.e., both original and corrupted inputs, averaging their confidence scores shows a huge performance enhancement. For example, comparing the uni-dimensional and multi-dimensional results when the corruption type is _Contrast_, the incorporation of the extended dimension provides +17.57% AUROC (from 74.20% to **91.77%**) and -52.02% FPR95 (from 88.61% to **36.95%**), indicating that corrupted inputs require the combination of original inputs to reveal its ability to enhance the distinguishability of OOD samples at the feature level. For more detailed experiments regarding CoVer expanded by corrupted inputs at other severity levels, please refer to Appendix C.2.4.

To have a more intuitive understanding on the superiority of multi-dimensional inputs, we provide a comparison with score distributions and detection results w/w.o. an expanded input dimension transformed by _Brightness_, _Fog_, _Motion blur_, and _Speckle noise_ in Figure 6. From the middle column, we notice that the OOD samples are more difficult to detect by the model confidence from single corrupted inputs. This is mainly because the confidence of the ID sample, which was originally high, drops drastically when it is corrupted thereby interfering with the model's judgment of the OOD sample. In contrast, through a simple but critical average operation, CoVer generally achieves better ID-OOD separability. This phenomenon can be attributed to two main reasons. First, the ID samples have an overall higher confidence expectation, eliminating the originally confident ID interference present under the single corrupted input. Secondly, as illustrated in Figure 2 and Figure 3, the data lies in the originally overlapped part of the ID and OOD distributions, i.e., unconfident ID data and overconfident OOD data, demonstrate significant differences in the variation of confidences under the same corruption. Specifically, the ID data shows resistance while the OOD data shows vulnerability, thus better exposing the OOD samples to be rejected.

#### c.2.3 Impact of the Number of Expanded Measuring Dimensions.

In Figure 5(b), we report the OOD detection performance variations evaluated on different numbers of expanded measuring dimensions. However, we have not specifically analyzed the impact of employing different types of corruptions under each number of extended representation dimensions. In Table 9, we further present the detailed results for different numbers of extended dimensions under corruption severity levels 1 and 2, with each number we enumerate two different kinds of combinations of corruption types. The experimental results demonstrate that considering confidence estimation on both original input and expanded variant dimension constantly enhances the OOD detection performance across four OOD datasets.

However, as the expanded dimension gives priority to the phenomenon of confidence mutation that is more discriminative in OOD data than that is in ID data, the newly added representation dimension sometimes leads to a slight decline in performance. For instance, the best performance is three expanded dimensions that grouped by _Defocus blur_, _Motion blur_ and _Fog_ inputs under the corruption severity level 1; the combination of five types of corruptions including _Defocus blur_, _Motion blur_, _Gaussion blur_, _Fog_, and _Saturate_ achieve a better OOD detection performance when the variants are generated by corruptions at severity level 2. This phenomenon suggests that, while

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Architecture} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{Standardization} & \multicolumn{3}{c}{SDU Dataset} & \multicolumn{3}{c}{Places} & \multicolumn{3}{c}{Textures} & \multicolumn{3}{c}{Average} \\ \cline{5-11}  & & & AUROC & FPR951 & AUROC & FPR951 & AUROC & FPR951 & AUROC & FPR951 & AUROC & FPR951 \\ \hline \multirow{6}{*}{CLIP} & ResNet50 & MCM & 93.86 & 32.16 & 90.74 & 46.21 & 85.66 & 60.68 & 85.71 & 60.11 & 88.99 & 49.79 \\  & **CoVer** & **94.57** & **30.26** & **90.75** & **45.51** & **86.52** & **54.84** & **87.66** & **54.11** & 89.97 & **46.18** \\ \cline{2-11}  & VT-B/03 & MCM & **93.61** & **33.82** & 91.42 & 41.79 & 89.56 & 45.61 & 84.67 & 61.63 & 89.82 & 45.75 \\  & **CoVer** & 93.52 & 34.51 & **91.46** & **40.65** & 88.89 & 46.92 & **86.77** & **57.06** & **90.21** & **44.78** \\ \cline{2-11}  & VT-B/016 & MCM & 94.61 & 30.95 & 92.57 & 37.57 & 89.77 & 44.65 & 86.10 & 57.77 & 90.76 & 42.73 \\  & **CoVer** & **95.62** & **24.35** & **93.48** & **31.94** & **90.67** & **39.74** & **86.61** & **90.44** & **92.10** & **36.62** \\ \cline{2-11}  & VT-L/14 & MCM & 94.95 & 23.88 & 94.14 & 29.0 & 92.0 & 35.42 & 84.88 & 59.88 & 91.49 & 38.17 \\  & **CoVer** & **96.16** & **20.84** & **94.91** & **24.85** & **92.37** & **32.42** & **87.0** & **54.04** & **92.61** & **32.96** \\ \hline \multirow{2}{*}{AnICLIP} & ViL/14 & MCM & 92.91 & 43.31 & 94.56 & 25.5 & 91.65 & 37.92 & 87.06 & 53.24 & 91.54 & 40.74 \\  & **CoVer** & **96.00** & **22.21** & **95.17** & **24.35** & **92.04** & **34.61** & **88.93** & **47.43** & **93.04** & **32.15** \\ \hline \multirow{2}{*}{MetaCLIP} & ViL/16-gnicking & MCM & 87.68 & 64.97 & 90.57 & 48.79 & 86.63 & 59.66 & 85.40 & 62.64 & 87.57 & 58.97 \\  & **CoVer** & **89.30** & **61.67** & **91.09** & **46.76** & **88.07** & **54.13** & **86.09** & **61.66** & **88.64** & **55.68** \\ \hline \multirow{2}{*}{GroupVT} & ViL/14 & MCM & 89.58 & 49.08 & 85.78 & 58.57 & 82.01 & 64.84 & 83.01 & 58.92 & 85.09 & 57.85 \\  & **CoVer** & **91.65** & **37.08** & **87.67** & **53.1** & **83.74** & **60.43** & **84.7** & **54.15** & **86.94** & **51.19** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Compared to MCM with different VLM architectures on ImageNet-1K(ID). All values are percentages. \(\uparrow\) indicates larger values are better and \(\downarrow\) indicates smaller values are better.

extended dimensions preferentially provide additional clues that make the OOD samples more salient, excessive extended dimensions or the inclusion of input dimensions transformed from some specific uncommon corruptions may result in a greater degree of interference in the ID samples, leading to a slight declination of the performance.

#### c.2.4 Full Results of CoVer with Variants Corrupted at Different Severity Levels.

In Figure 5(c), we report three representative variation trends of performance with an increasing level of the corruption severity. In Table 10 and Table 11, we present the full results of the proposed CoVer with 18 different types of corruptions at 5 severity levels. It is worth noting that the experimental results here are all based on the average of the model confidence scores measured on one original input and one extended corrupted input. Furthermore, we present the performance changing trends of our CoVer based on all 18 corruption styles in Figure 7. It can be seen that the performance of CoVer is more sensitive to the severity levels of corruptions compared to the number of extended representation dimensions analyzed in Appendix C.2.3. Specifically, as shown in Figure 7, some common corruption types from categories including weather (e.g., _Brightness_ and _Fog_) and blur (e.g., _Motion blur_ and _Defocus blur_) can achieve lower FPR95 values. We further observe that the commonality of these better-performing corruption types is that their perturbations to image features are more mild compared to other types (e.g., _Spatter_ and _Elastic transform_ from the digital class, _Impulse noise_ from the noise category), and they generally do not excessively corrupt the semantic features. In Appendix C.4, we will more intuitively demonstrate the distinctions between these various types of corruptions through the visualizations of corrupted ID and OOD samples, illustrating the varying degrees of enhancement or attenuation they impart on the performance of CoVer.

\begin{table}
\begin{tabular}{l|c|c c c c c c c c c} \hline \hline \multirow{2}{*}{Corruption Type} & \multirow{2}{*}{Mode} & \multicolumn{4}{c}{OOD Dataset} & \multicolumn{4}{c}{Features} & \multicolumn{4}{c}{Average} \\ \cline{3-11}  & & AUROC1 & FPR95\(\downarrow\) & AUROC & FPR95\(\downarrow\) & AUROC1 & FPR95\(\downarrow\) & AUROC1 & FPR95\(\downarrow\) & AUROC1 & FPR95\(\downarrow\) \\ \hline \multirow{2}{*}{Brightness} & Uni & 92.84 & 41.42 & 90.17 & 50.25 & 87.35 & 54.42 & 86.48 & 55.21 & 89.21 & 50.33 \\  & Multi & **95.11** & **28.29** & **92.76** & **36.84** & **90.05** & **43.08** & **88.08** & **52.3** & **91.5** & **40.13** \\ \hline \multirow{2}{*}{Fog} & Uni & 88.42 & 60.34 & 84.71 & 75.11 & 81.34 & 78.46 & 82.0 & 65.48 & 84.12 & 69.85 \\  & Multi & **95.61** & **24.19** & **93.13** & **33.77** & **90.15** & **40.96** & **88.11** & **81.42** & **91.77** & **36.91** \\ \hline \multirow{2}{*}{Contrast} & Uni & 82.0 & 87.94 & 91.36 & 89.42 & 67.68 & 91.01 & 75.77 & 86.06 & 74.24 & 88.61 \\  & Multi & **96.41** & **19.6** & **92.46** & **36.35** & **89.17** & **44.07** & **89.05** & **48.07** & **91.77** & **36.95** \\ \hline \multirow{2}{*}{Motion Blur} & Uni & 82.12 & 71.66 & 72.48 & 90.65 & 67.88 & 92.11 & 76.12 & 73.12 & 74.65 & 81.89 \\  & Multi & **94.98** & **26.13** & **91.44** & **42.02** & **87.78** & **50.18** & **87.24** & **50.92** & **90.35** & **42.31** \\ \hline \multirow{2}{*}{Defocus Blur} & Uni & 80.75 & 65.67 & 77.1 & 80.25 & 74.12 & 83.31 & 79.03 & 70.0 & 77.75 & 74.81 \\  & Multi & **94.04** & **30.8** & **92.12** & **39.58** & **89.09** & **45.25** & **88.32** & **49.8** & **90.89** & **41.36** \\ \hline \multirow{2}{*}{Gaussian Blur} & Uni & 77.49 & 73.47 & 73.84 & 83.89 & 71.13 & 86.05 & 77.61 & 69.86 & 75.02 & 78.34 \\  & Multi & **93.52** & **83.33** & **91.59** & **42.03** & **88.64** & **47.39** & **88.01** & **50.25** & **90.44** & **43.24** \\ \hline \multirow{2}{*}{Spatter} & Uni & 75.45 & 87.44 & 81.57 & 83.09 & 79.15 & 83.43 & 71.27 & 85.57 & 76.86 & 84.88 \\  & Multi & **92.06** & **44.98** & **92.3** & **40.42** & **89.46** & **46.1** & **83.09** & **62.36** & **89.43** & **48.47** \\ \hline \multirow{2}{*}{Saturate} & Uni & 88.48 & 57.4 & 86.37 & 64.72 & 84.76 & 66.84 & 83.03 & 64.63 & 85.66 & 63.4 \\  & Multi & **94.24** & **31.27** & **92.11** & **38.41** & **89.9** & **43.08** & **87.23** & **53.37** & **90.87** & **41.53** \\ \hline \multirow{2}{*}{Elastic Transform} & Uni & 54.9 & 98.67 & 65.13 & 95.65 & 65.08 & 94.5 & 47.27 & 97.09 & 58.1 & 96.47 \\  & Multi & **90.57** & **53.15** & **91.12** & **45.97** & **88.52** & **49.36** & **77.98** & **72.84** & **87.05** & **55.33** \\ \hline \multirow{2}{*}{JPEG Compression} & Uni & 79.43 & 86.23 & 83.88 & 75.1 & 80.99 & 75.62 & 70.58 & 81.49 & 80.34 & 79.61 \\  & Multi & **93.08** & **39.52** & **92.95** & **36.21** & **89.97** & **42.17** & **86.27** & **57.34** & **90.57** & **43.81** \\ \hline \multirow{2}{*}{Pixelate} & Uni & 82.88 & 74.39 & 78.18 & 85.36 & 75.9 & 88.07 & 73.27 & 83.65 & 77.56 & 82.87 \\  & Multi & **94.56** & **29.86** & **91.79** & **39.89** & **89.11** & **45.89** & **85.86** & **58.48** & **90.33** & **43.53** \\ \hline \multirow{2}{*}{Speckle Noise} & Uni & 85.34 & 80.59 & 96.20 & 95.65 & 67.38 & 94.56 & 86.17 & 92.07 & 72.19 & 90.79 \\  & Multi & **96.49** & **19.3** & **91.51** & **43.85** & **89.29** & **49.75** & **85.44** & **56.95** & **90.59** & **42.59** \\ \hline \multirow{2}{*}{Glass Blur} & Uni & 76.1 & 82.53 & 74.05 & 84.98 & 70.49 & 87.97 & 62.0 & 86.05 & 70.66 & 85.38 \\  & Multi & **94.51** & **30.81** & **92.32** & **37.47** & **89.12** & **45.22** & **84.66** & **51.88** & **90.58** & **43.77** \\ \hline \multirow{2}{*}{Gaussian Noise} & Uni & 73.7 & 87.88 & 56.11 & 97.37 & 55.99 & 96.32 & 59.85 & 95.55 & 61.41 & 94.28 \\  & Multi & **95.47** & **23.58** & **90.74** & **47.27** & **88.26** & **52.06** & **85.4** & **57.82** & **89.97** & **45.18** \\ \hline \multirow{2}{*}{Shot Noise} & Uni & 76.81 & 85.35 & 58.25 &

Figure 6: Comparison of scores distributions and detection results with different inputs for representation dimension expansion under various corruptions.

\begin{table}
\begin{tabular}{l|c|c c c c c c c c c c} \hline \hline \multirow{2}{*}{Corruption Type} & \multirow{2}{*}{Severity} & \multicolumn{6}{c}{OOD Dataset} & \multicolumn{6}{c}{Textures} & \multicolumn{6}{c}{Average} \\ \cline{3-13}  & & Level & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{SUM} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c}{Textures} & \multicolumn{2}{c}{Average} \\ \cline{3-13}  & & AUROC\({}^{\uparrow}\) & FPR95\({}^{\downarrow}\) & AUROC\({}^{\uparrow}\) & FPR95\({}^{\downarrow}\) & AUROC\({}^{\uparrow}\) & FPR95\({}^{\downarrow}\) & AUROC\({}^{\uparrow}\) & FPR95\({}^{\downarrow}\) & AUROC\({}^{\uparrow}\) & FPR95\({}^{\downarrow}\) \\ \hline \multirow{6}{*}{Brightness} & 1 & 94.62 & 30.09 & 92.71 & 36.78 & **90.16** & 43.53 & 86.72 & 55.57 & 91.05 & 41.49 \\  & 2 & 94.63 & 30.73 & 92.67 & 36.69 & 90.09 & 43.55 & 87.07 & 54.65 & 91.11 & 41.16 \\  & 3 & 94.67 & 30.24 & 92.63 & 37.17 & 90.00 & 43.53 & 87.51 & 53.67 & 91.20 & 41.15 \\  & 4 & 94.88 & 29.52 & 92.67 & 37.06 & 90.04 & 43.86 & 87.89 & 52.61 & 91.37 & 40.76 \\  & 5 & **95.11** & **28.29** & **92.76** & 36.84 & 90.05 & **43.08** & **88.08** & **52.30** & **91.50** & **40.13** \\ \hline \multirow{6}{*}{Fog} & 1 & 95.24 & 26.09 & 92.76 & 37.00 & 90.12 & 42.95 & 87.67 & 52.68 & 91.45 & 39.68 \\  & 2 & 95.42 & 25.20 & 92.80 & 36.71 & 90.13 & 43.54 & 88.00 & 51.28 & 91.59 & 39.18 \\  & 3 & 95.56 & **24.11** & 93.02 & 34.80 & **90.21** & 42.31 & **88.26** & 48.94 & 91.76 & 37.54 \\  & 4 & 95.49 & 24.68 & 90.34 & 34.60 & 90.15 & 41.64 & 88.17 & 49.17 & 91.71 & 37.52 \\  & 5 & **95.61** & 24.19 & **93.19** & **33.77** & 90.15 & **40.96** & 88.11 & **47.22** & **91.77** & **36.91** \\ \hline \multirow{6}{*}{Contrast} & 1 & 94.60 & 30.26 & 92.77 & 37.24 & 89.99 & 43.93 & 87.08 & 55.11 & 91.11 & 41.64 \\  & 2 & 94.88 & 28.37 & 92.88 & 36.71 & 90.02 & 43.87 & 87.54 & 53.78 & 91.33 & 40.68 \\  & 3 & 95.46 & 25.55 & **93.04** & 35.94 & **90.11** & 42.90 & 88.57 & 50.23 & 91.80 & 38.65 \\  & 4 & 96.37 & 20.10 & 92.95 & **35.18** & 89.85 & **42.13** & **90.13** & **43.90** & **92.32** & **35.33** \\  & 5 & **96.41** & **19.60** & 92.46 & 36.05 & 89.17 & 44.07 & 89.05 & 48.07 & 91.77 & 36.95 \\ \hline \multirow{6}{*}{Motion Blur} & 1 & 95.18 & 26.65 & 93.24 & 33.21 & **90.54** & 40.30 & 87.03 & 54.38 & 91.50 & 38.63 \\  & 2 & **95.34** & 26.04 & **93.41** & **32.15** & 90.44 & **40.28** & 87.17 & 53.35 & **91.99** & **37.95** \\  & 3 & 95.16 & 26.14 & 93.06 & 32.88 & 89.75 & 41.62 & 87.26 & 51.88 & 91.31 & 38.13 \\  & 4 & 94.97 & **26.03** & 92.14 & 37.97 & 89.58 & 47.20 & **87.26** & **51.12** & 90.74 & 40.58 \\  & 5 & 94.98 & 26.13 & 91.40 & 42.02 & 87.78 & 50.18 & 87.24 & 50.92 & 90.35 & 42.31 \\ \hline \multirow{6}{*}{Defocus Blur} & 1 & 94.80 & 29.26 & 93.57 & 31.87 & **90.68** & **39.19** & 87.74 & 52.22 & 91.70 & 38.13 \\  & 2 & **94.85** & **28.64** & **93.60** & **31.77** & 90.57 & 39.60 & 88.21 & 50.25 & **91.81** & **37.66** \\  & 3 & 94.65 & 29.04 & 93.14 & 33.75 & 90.13 & 41.39 & **88.52** & **80.16** & 91.61 & 38.59 \\  & 4 & 94.25 & 30.52 & 92.66 & 36.40 & 89.66 & 42.88 & 88.49 & 49.40 & 91.27 & 39.80 \\  & 5 & 94.04 & 30.80 & 92.12 & 39.58 & 88.09 & 45.25 & 88.32 & 49.80 & 90.89 & 41.36 \\ \hline \multirow{6}{*}{Gaussian Blur} & 1 & 94.66 & 30.49 & 92.97 & 35.25 & 90.39 & 41.49 & 86.90 & 55.05 & 91.23 & 40.57 \\  & 2 & **94.80** & **29.34** & **93.39** & **32.99** & **90.44** & **40.33** & 87.69 & 51.74 & **91.58** & **38.60** \\  & 3 & 94.41 & 30.95 & 93.16 & 33.84 & 90.08 & 41.50 & 87.99 & 51.38 & 91.41 & 39.42 \\  & 4 & 93.94 & 33.01 & 92.65 & 36.77 & 89.59 & 43.91 & **88.03** & 50.90 & 91.05 & 41.15 \\  & 5 & 93.52 & 33.3 & 91.59 & 42.03 & 88.64 & 47.39 & 88.01 & **50.25** & 90.44 & 43.24 \\ \hline \multirow{6}{*}{Spatter} & 1 & **94.48** & **30.85** & **92.89** & 35.54 & **90.30** & 42.06 & **86.45** & **56.44** & **61.03** & **41.22** \\  & 2 & 94.25 & 32.70 & **90.80** & **32.54** & 90.02 & **41.99** & 85.46 & 58.69 & 90.73 & 42.16 \\  & 3 & 94.18 & 34.14 & 93.01 & 36.66 & 90.01 & 42.80 & 84.68 & 61.05 & 90.47 & 43.51 \\  & 4 & 92.25 & 44.55 & 92.39 & 39.15 & 89.66 & 44.66 & 84.29 & 61.74 & 89.65 &

## Appendix A

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Corruption} & \multirow{2}{*}{Severity} & \multicolumn{5}{c}{OOD Dataset} & \multicolumn{2}{c}{} \\  & & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c}{Testures} & \multicolumn{2}{c}{Average} \\ \cline{3-11}  & & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) & AUROC\(\uparrow\) & FPR95\(\downarrow\) \\ \hline \multirow{5}{*}{Shot Noise} & 1 & 94.37 & 32.83 & **92.34** & 39.94 & **89.59** & 45.12 & 87.14 & 55.73 & 90.86 & 43.25 \\  & 2 & 94.82 & 29.23 & **38.92** & 89.49 & **44.90** & **87.18** & **58.21** & **90.95** & **42.07** \\  & 3 & 95.23 & 26.53 & 91.81 & 42.03 & 89.04 & 47.28 & 86.77 & 55.67 & 90.71 & 42.88 \\  & 4 & 95.80 & 22.83 & 91.08 & 46.01 & 88.56 & 50.64 & 85.74 & 57.64 & 90.30 & 44.28 \\  & 5 & **95.97** & **21.26** & 90.85 & 46.16 & 88.44 & 51.13 & 85.09 & 90.11 & 44.16 \\ \hline \multirow{5}{*}{Zoom Blur} & 1 & **94.63** & **29.75** & **93.16** & **32.44** & **90.20** & **40.27** & **87.18** & **52.57** & **91.29** & **38.76** \\  & 2 & 94.08 & 33.05 & 92.70 & 35.13 & 89.62 & 42.70 & 87.00 & 53.03 & 90.85 & 40.98 \\  & 3 & 93.50 & 35.87 & 92.15 & 37.36 & 89.01 & 45.09 & 85.95 & 55.66 & 90.15 & 43.49 \\  & 4 & 92.95 & 37.98 & 91.65 & 39.51 & 88.48 & 46.92 & 85.63 & 57.04 & 89.68 & 45.36 \\  & 5 & 92.35 & 40.41 & 90.93 & 42.52 & 87.93 & 49.05 & 84.46 & 59.27 & 88.92 & 47.81 \\ \hline \multirow{5}{*}{Snow} & 1 & 94.35 & 32.94 & **92.62** & **36.32** & **89.71** & **43.21** & 85.82 & 87.22 & 90.62 & 42.80 \\  & 2 & **94.63** & 30.66 & 92.23 & 37.93 & 89.18 & 44.87 & 86.15 & 56.47 & 90.55 & 42.48 \\  & 3 & 94.57 & **30.42** & 91.28 & 37.94 & 89.18 & 44.78 & 86.20 & 55.80 & 90.53 & 42.23 \\  & 4 & 94.59 & 30.74 & 91.80 & 36.99 & 88.79 & 46.04 & 86.35 & 55.16 & 90.38 & 42.91 \\  & 5 & 95.35 & 26.59 & 91.94 & 39.16 & 88.76 & 45.93 & **86.60** & **53.40** & **90.66** & **41.27** \\ \hline \multirow{5}{*}{Impulse Noise} & 1 & 93.33 & 41.46 & **92.61** & **38.58** & **89.70** & **44.57** & **86.37** & 57.32 & **90.50** & 45.48 \\  & 2 & 94.24 & 33.91 & 91.94 & 43.03 & 89.11 & 47.87 & 86.09 & 56.26 & 90.34 & **45.27** \\ \cline{1-1}  & 3 & 94.43 & 31.89 & 91.41 & 45.29 & 88.63 & 49.98 & 86.05 & 55.76 & 90.13 & 45.73 \\ \cline{1-1}  & 4 & 95.11 & 26.64 & 90.73 & 47.75 & 88.04 & 52.06 & 86.01 & **54.56** & 89.97 & 45.25 \\ \cline{1-1}  & 5 & **95.75** & **22.70** & 90.05 & 50.09 & 87.73 & 53.49 & 86.06 & 54.91 & 89.90 & 45.30 \\ \hline \hline \end{tabular}
\end{table}
Table 11: (Extension of Table 10) Full results of CoVer under one extended input with 18 alternative types of corruptions at 5 severity levels based on CLIP-B/16. The ID data are ImageNet-1K.

Figure 7: Performance variation trends of CoVer by varying severity levels under different types of corruptions.

[MISSING_PAGE_FAIL:25]

### Visualization Analysis

#### c.4.1 Visualization of Corrupted ID and OOD Samples

In Figure 8, we provide the visualized examples under 18 different types of algorithmically generated corruptions which are officially introduced in [22]. Furthermore, there are 5 severity levels for each corruption style, ranging from inconsequential to devastating corruption of the original clean image as shown in Figure 9. By synthesizing these 18 types and 5 severity levels, we generate a comprehensive and diverse set of 90 distinct forms of corrupted inputs, which can be leveraged to enhance the dimensionality of input representation.

Figure 8: Visualization of an original sample and its corrupted instances under each corruption type officially defined in [22]

Figure 9: Visualization of varying severity levels, with _Impulse Noise_, _Snow_, and _Glass Blur_ (all introduced in [22]) modestly to markedly corrupting the natural clean image.

#### c.4.2 Comparison with Salient Maps and Corresponding Confidence Variations.

To better illustrate the effectiveness of CoVer, which primarily stems from the differing confidence variations between ID and OOD samples under identical corruption conditions, we provide some visualization results of ID and OOD images, as shown in Figure 10 and Figure 11. All the images are picked from the datasets in the ImageNet-1K OOD detection benchmark. Each subfigure shows the feature maps of the original image and its corrupted variants, including _Contrast_ (Severity Level 4) and _Defocus Blur_ (Severity Level 2). For comparison, we also provide the corresponding confidence variations between the original (red bar), the corrupted (blue bar), and the averaged (yellow bar) one. The confidence scores are based on the form of maximum-softmax scoring function given by CLIP-B/16. We continue to divide the data into four categories, denoted as confident ID data (refer to row 1 to row 2 in Figure 10), unconfident ID data (refer to row 3 to row 6 in Figure 10), overconfident OOD data (refer to row 1 to row 4 in Figure 11), and unconfident OOD data (refer to row 5 to row 6 in Figure 11). Here we focus on the differences between unconfident ID data and overconfident OOD to verify the analysis claimed in Section 3.2.

In Figure 10, it is obvious that ID images have more significant ID-semantic feature activations (see the foreground salient responses) due to the knowledge of the ID label space. Firstly, for confident ID data, the changes in confidence post-corruption can manifest as either minor or abrupt. For example, in the image in row 1, column 1 (ILSVRC2012_val_00020025), the corruption results in only a negligible loss of the semantics of the ID category present in the foreground. Conversely, for the image in row 2, column 1 (ILSVRC2012_val_00044407), the corruption enhances disturbances from non-semantic areas, leading to a loss of model confidence. Nevertheless, by averaging the original and corrupted confidence scores, the confident ID data remains stable within a higher confidence interval, demonstrating the superiority of CoVer's mechanism. Secondly, for unconfident ID data, due to the presence of ID semantic features, the model exhibits resilience in its confidence when subjected to corruption. For instance, the image in row 4, column 1 (ILSVRC2012_val_00022503) shows the model's resilience, as it continues to focus on the foreground regions belonging to the ID category despite various degrees of corruption, thus maintaining its confidence score unaffected. The same results can also be seen in row 3, column 1 (ILSVRC2012_val_00048997) and row 4, column 2 (ILSVRC2012_val_00020119).

In Figure 11, particularly in the case of overconfident OOD images, there is an excessive reaction to the corruption. This phenomenon is apparent because, unlike ID data, OOD images inherently lack semantic information about ID categories, leading to the disappearance of areas with high feature activation under the same type and severity level of corruption. For instance, the image in row 2, column 1 (f_formal_garden_00003688) demonstrates that regions highly responsive in their original state become irrelevant after corruption, resulting in the confidence mutation. This shift further confirms that the model's overconfidence in them is primarily due to misleading non-semantic features, such as textures and styles. Similar results can also be found in other OOD datasets, such as the image in row 3, column 1 (sun_arsrkiznzlekfvg), and the image in row 4. column 2 (wrinkled_0070). However, the confidence of unconfident ID data, comparable to that of these overconfident OOD samples, remains stable. Consequently, by averaging the confidence scores before and after corruption, CoVer effectively captures the distinctions between OOD and ID data that initially overlap, thus enhancing the separability of ID and OOD distributions. Furthermore, for unconfident OOD data, due to their overall low relevance to ID semantics, the confidence scores consistently remain in a lower range (like the image in row 5, column 1, named 1b0ac86be7f53fd9058646315ed17269).

Figure 10: Case visualization of ID images. The left part of each subfigure contains the original image (with the dataset name and filename) and its corruptions with their feature maps. The right part shows the confidence variations corresponding to each corruption.

Figure 11: Case visualization of OOD images. The left part of each subfigure contains the original image (with the dataset name and filename) and its corruptions with their feature maps. The right part shows the confidence variations corresponding to each corruption.

Preliminary Statistical Analysis

In this section, we present the statistical implications with detailed definitions and assumptions. The primary objective is to show that CoVer can lead to an increase in the separability of the distributions of ID and OOD by introducing confidence expectations under the extended representation dimension. In the following parts, we first introduce the considering performance metric and some preliminary setups for the analyses.

Metric.The separability between ID and OOD data can be reflected by the \(\mathrm{FPR}_{\lambda}\), which is the performance metric of an OOD detector, defined as follows,

\[\mathrm{FPR}_{\lambda}=F_{\text{out}}\,\left(F_{\text{in}}^{-1}(\lambda)\right)\] (19)

where \(F_{\text{in}}\) and \(F_{\text{out}}\) represent the cumulative distribution functions (CDFs) corresponding to the confidence scores obtained by ID and OOD samples, respectively. \(\lambda\in[0,1]\) is denoted as the true positive rate (TPR), indicating the proportion of samples that are correctly classified as one of the ID categories. The metric \(\mathrm{FPR}_{\lambda}\) quantifies the overlap degree between the scores that the OOD detector assigns to ID and OOD samples, with lower values indicative of superior performance.

Preliminary setups.Following previous works [32, 31], owing to the robust representational capabilities of pre-trained DNNs and the consistent alignment between cosine similarity scores and labels observed in CLIP-like models, we assume that the features extracted from DNNs or the cosine similarity scores in CLIP-like models approximately conform to a Gaussian Mixture Model (GMM) with equal class priors: \(\left(\frac{1}{2}\mathcal{N}\left(\mu_{pi},\sigma_{pi}\right)+\frac{1}{2} \mathcal{N}\left(\mu_{po},\sigma_{po}\right)\right)\), where \(\mu_{pi}\) and \(\mu_{po}\) are the means of the ID and OOD distribution, while \(\sigma_{pi}\) and \(\sigma_{po}\) are the corresponding standard deviations.. Specifically, we use \(\mathcal{D}_{\text{ID}}=\mathcal{N}\left(\mu_{pi},\sigma_{pi}\right)\) and \(\mathcal{D}_{\text{OOD}}=\mathcal{N}\left(\mu_{po},\sigma_{po}\right)\) denote the ID marginal distribution and the OOD marginal distribution, respectively.

Assumptions.Refer to Figure 1 and Figure 6, the comparisons of score distributions obtained by different input modes, we can derive a series of assumptions about the variation relationships between \(\mu_{pi}\) and \(\mu_{po}\), and between \(\sigma_{pi}\) and \(\sigma_{po}\). Empirical exploration can be found in Figure 12.

**Assumption D.1** (Variation of \(\mu_{pi}\) and \(\mu_{po}\)).: _Let \(\Delta\mu_{pi}\) and \(\Delta\mu_{po}\) represent the changes in the means of ID and OOD distributions, respectively, after corruption and averaging. We assume that \(|\Delta\mu_{pi}|>|\Delta\mu_{po}|\), resulting in a narrowing gap between \(\mu_{pi}\) and \(\mu_{po}\)._

This assumption is predicated on the observation that the means of ID distributions, \(\mu_{pi}\), decrease more significantly under identical corruption levels compared to OOD distributions, \(\mu_{po}\). The generally higher initial confidence scores of ID samples make their means more susceptible to substantial decreases (refer to the left panel of Figure 12). This reduction is greater than that experienced by OOD samples, thereby significantly narrowing the gap between \(\mu_{pi}\) and \(\mu_{po}\). For an intuitive example, the gap between the means of confident ID data (left panel of Figure 12) and overconfident OOD data (right-middle panel of Figure 12) is closer. This illustrates the pronounced impact of the averaging operation on ID distributions compared to OOD distributions.

**Assumption D.2** (Variation of \(\sigma_{pi}\) and \(\sigma_{po}\)).: _We define the changes in the variances of ID and OOD distributions as \(\Delta\sigma_{pi}\) and \(\Delta\sigma_{po}\), respectively. We postulate that the reduction in variance for ID distributions, \(\Delta\sigma_{pi}\), is greater than that for OOD distributions, \(\Delta\sigma_{po}\): \(|\Delta\sigma_{pi}|>|\Delta\sigma_{po}|\)._

This assumption is supported by the observation that high-confidence ID samples, due to their higher initial confidence levels, experience larger and more abrupt drops in confidence upon corruption (see the left panel of Figure 12). Consequently, the averaging process post-corruption results in a significantly greater reduction in the variance \(\sigma_{pi}\) in ID distributions compared to \(\sigma_{po}\) in OOD distributions. This marked decrease in variability within ID confidence scores, relative to the OOD ones, underscores the efficacy of the averaging operation in dramatically stabilizing the ID confidence scores more than the adjustments observed in OOD confidence scores.

Given the preliminaries and assumptions above, we can derive the following extended lemma to demonstrate the superior performance of CoVer.

**Lemma D.3** (Declination of \(\mathrm{FPR}_{\lambda}\)).: _Assuming the variation relationships between \(\mu_{pi}\) and \(\mu_{po}\), and between \(\sigma_{pi}\) and \(\sigma_{po}\), CoVer enables a lower \(\mathrm{FPR}_{\lambda}\)._Proof of Lemma D.3.: We aim to investigate the relationship between the \(\mathrm{FPR}_{\lambda}\) metric and the variations in \(\mu_{pi}\), \(\mu_{po}\), \(\sigma_{pi}\), and \(\sigma_{po}\). The \(\mathrm{FPR}_{\lambda}\) metric can be reformulated as follows:

\[\mathrm{FPR}_{\lambda} =\Phi\left(\Phi^{-1}(\lambda;\mu_{pi},\sigma_{pi});\mu_{po},\sigma _{po}\right)\] \[=\Phi\left(\mu_{pi}+\sigma_{pi}\cdot\Phi^{-1}(\lambda);\mu_{po}, \sigma_{po}\right)\] \[=\Phi\left(\frac{\mu_{pi}+\sigma_{pi}\cdot\Phi^{-1}(\lambda)-\mu _{po}}{\sigma_{po}}\right)\] (20)

where \(\Phi\) is the cumulative distribution function of the Gaussian distribution, and \(\Phi^{-1}\) is its inverse function. Considering the differences before applying CoVer:

\[\Delta\mu =\mu_{pi}-\mu_{po}\] \[\Delta\sigma =\sigma_{pi}-\sigma_{po}\] (21)

With the assumptions that \(\Delta\mu\) and \(\Delta\sigma\) are affected by the averaging process in CoVer, we express these changes as:

\[\Delta\mu_{\text{new}} =\mu^{\prime}_{pi}-\mu^{\prime}_{po}\] \[\Delta\sigma_{\text{new}} =\sigma^{\prime}_{pi}-\sigma^{\prime}_{po}\] (22)

where \(\mu^{\prime}_{pi}\) and \(\mu^{\prime}_{po}\) are the new means post-CoVer, \(\sigma^{\prime}_{pi}\) and \(\sigma^{\prime}_{po}\) are the new variances post-CoVer. Given that \(\Delta\mu_{\text{new}}\) is reduced and \(\Delta\sigma_{\text{new}}\) indicates a significant contraction, particularly a reduction in \(\sigma^{\prime}_{pi}\) relative to \(\sigma^{\prime}_{po}\), the numerator in \((\mu_{pi}+\sigma_{pi}\cdot\Phi^{-1}(\lambda)-\mu_{po})/\sigma_{po}\), decreases while the denominator basically unchanged. This results in the argument of \(\Phi\) becoming smaller. Since \(\Phi\) is monotonically increasing, a decrease in the argument directly translates to a lower value of \(\mathrm{FPR}_{\lambda}\), thereby reducing the probability of falsely classifying OOD samples as ID.

This analysis underscores the benefit of CoVer, particularly through its influence on expanding representation dimensions and optimizing the detection framework. By narrowing the gap between the mean scores and significantly reducing the variance in ID distributions relative to OOD, CoVer enhances the model's discriminative capability and improves its robustness, ultimately leading to more reliable OOD detection.

## Appendix E Future Analysis

### Discussion about Extra Runtime

Considering the addition of corrupted inputs, CoVer would result in non-negligible additional runtime. If there are \(N\) expanded dimensions, it will take \(N\) times the duration of a single input to implement CoVer. However, our CoVer is only applied in the inference phase of OOD detection, and it is generally fast, as shown in Table 13. We believe that the performance improvements offered by CoVer are well-worth the extra few minutes of runtime.

Figure 12: Empirical exploration to evidence the proposed assumptions. Based on Figure 2, we further present scatter maps of averaged confidence scores for comparison.

[MISSING_PAGE_FAIL:32]

### Exploration on the harder OOD dataset

NINCO [4] has proposed three OOD datasets with no categorical contamination which include NINCO, OOD unit-tests, and NINCO popular OOD datasets subsamples, which are demonstrated to be harder than common OOD detection benchmarks. Here, we evaluate the effectiveness of CoVer on these datasets in 16. The results demonstrate that CoVer, when combined with ASH, consistently achieves better performance across the three NINCO OOD datasets.

### Comparison with other Competitive Methods

Comparison with NNGuide and MaxLogit.To provide a comprehensive comparisons, we have conducted comparison experiments with NNGuide [39] and MaxLogit [21] to enrich our analysis in Table 17. First, our experimental results show that CoVer outperforms these competitive post-hoc methods on the ResNet50 architecture. Second, the performance of these post-doc methods, especially NNGuide, encounter significant drop when conducted on CLIP-B/16 architecture. We believe the reason for the poor performance is the difference in training data. Many pos-hoc methods are designed on ImageNet pre-trained networks, where only ID data are used during training. In contrast, when training CLIP, both ID datas and OOD datas are used. This leads to different activations of OOD data. Another reason is that the pos-hoc method relies heavily on the choice of hyperparameters. The hyperparameters of NNGuide need to be re-selected on different models. Despite these issues, our CoVer can still perform better than these methods. Furthermore, we also combine our CoVer with MaxLogit and NNGuide and report the results in Table 18, which further demonstrates the effectiveness and compatibility of our method.

\begin{table}
\begin{tabular}{l|c|c c c c} \hline \hline Architecture & \(D_{in}\) & \(D_{out}\) & Method & AUROC\(\uparrow\) & FPR95\(\downarrow\) \\ \hline \multirow{4}{*}{ResNet50} & \multirow{4}{*}{ImageNet-1K} & NINCO & ASH & 82.26 & 69.22 \\  & & NINCO & AHS + CoVer & **82.80** & **68.59** \\  & & NINCO unit-tests & ASH & 99.13 & 4.85 \\  & & NINCO unit-tests & AHS + CoVer & **99.49** & **2.12** \\  & & NINCO subsamples & ASH & 82.07 & 56.10 \\  & & NINCO subsamples & AHS + CoVer & **82.67** & **54.44** \\ \hline \hline \end{tabular}
\end{table}
Table 16: The overall results of CoVer on three NINCO OOD datasets without leveraging VLMs/CLIP. The ID data are ImageNet-1K.

\begin{table}
\begin{tabular}{l|l c c c c c c c c c} \hline \hline \multirow{2}{*}{Architecture} & \multirow{2}{*}{Method} & \multicolumn{4}{c}{OOD Dataset} & \multicolumn{4}{c}{Average} \\ \cline{3-11}  & & AUROC & FPR95\(\downarrow\) & AUROC & FPR95\(\downarrow\) & AUROC & FPR95\(\downarrow\) & AUROC & FPR95\(\downarrow\) & AUROC & FPR95\(\downarrow\) \\ \hline \multirow{4}{*}{ResNet50} & MaxLogit & 91.93 & 50.91 & 86.59 & 59.87 & 84.18 & 65.68 & 86.40 & 54.36 & 87.07 & 57.70 \\  & & NNGuide(\(\downarrow\)=1) & 93.13 & 34.06 & 90.41 & 38.66 & 88.06 & 47.46 & 91.67 & 28.99 & 90.82 & 37.57 \\  & & NNGuide(\(\downarrow\)=1) & 94.33 & 29.27 & 91.23 & 36.4 & 88.71 & 46.2 & 92.93 & 26.31 & 91.80 & 34.55 \\  & & NNGuide(\(\downarrow\)=1) & 95.10 & 26.06 & 91.44 & 86.56 & 88.63 & 47.64 & **90.34** & **24.17** & 92.19 & 33.68 \\  & & CoVour (ours) & **97.14** & **14.04** & **94.12** & **25.57** & **91.05** & **35.93** & 91.53 & 90.39 & **93.56** & **26.53** \\ \hline \multirow{4}{*}{CLIP-B/16} & MaxLogit & 89.31 & 61.66 & 87.43 & 64.59 & 85.95 & 63.67 & 71.68 & 86.61 & 83.59 & 69.08 \\  & & NNGuide(\(\downarrow\)=1) & 65.06 & 99.38 & 68.56 & 97.27 & 72.19 & 93.51 & 64.06 & 96.94 & 67.97 & 97.16 \\  & & NNGuide(\(\downarrow\)=1) & 60.68 & 99.68 & 68.06 & 71.65 & 94.83 & 62.21 & 90.99 & 65.83 & 97.89 \\  & & NNGuide(\(\downarrow\)=1) & 51.34 & 99.85 & 64.84 & 98.83 & 68.74 & 96.49 & 53.26 & 96.63 & 59.54 & 98.70 \\  & & CoVour (ours) & **95.58** & **22.55** & **94.42** & **32.85** & **90.27** & **46.71** & **90.14** & **43.39** & **92.45** & **34.88** \\ \hline \hline \end{tabular}
\end{table}
Table 17: Comparison with NNGuide and MaxLogit based on ResNet50 and CLIP-B/16. The ID data are ImageNet-1K.

\begin{table}
\begin{tabular}{l|l|c c c c c c c c c} \hline \hline \multirow{2}{*}{Architecture} & \multirow{2}{*}{Method} & \multicolumn{4}{c}{Normalist} & \multicolumn{4}{c}{Sum} & \multicolumn{4}{c}{Projects} & \multicolumn{4}{c}{Textures} & \multicolumn{4}{c}{Average} \\ \cline{3-11}  & & AUROC & FPR95\(\downarrow\) & AUROC & FPR95\(\downarrow\) & AUROC & FPR95\(\downarrow\) & AUROC & FPR95\(\downarrow\) & AUROC & FPR95\(\downarrow\) \\ \hline \multirow{4}{*}{**CLIP-B/16**} & MaxLogit & 89.31 & 61.66 & 87.43 & 64.39 & 85.95 & 63.67 & 71.68 & 86.61 & 83.59 & 69.08 \\  & & MaxLogit & **91.78** & **93.93** & **89.20** & **59.64** & **87.99** & **59.15** & **74.01** & **84.50** & **85.72** & **63.31** \\ \hline \multirow{4}{*}{**ResNet50**} & MaxLogit & 91.13 & 50.91 & 86.59 & 95.87 & 84.18 & 65.68 & 86.40 & 54.36 & 87.07 & 57.20 \\  & & MaxLogit & 92.85 & 42.19 & 87.19 & 89.87 & 84.97 & 63.84 & 86.59 & 54.49 & 87.90 & 54.83 \\ \cline{1-1}  & & NNGuide(\(\downarrow\)=1) & 95.13 & 42.40 & 90.41 & 33.56 & 88.06 & 47.46 & 91.67 & 29.39 & 90.32 & 37.57 \\ \cline{1-1}  & & NNGuide(\(\downarrow\)=1) & **94.98** & **28.16** & **91.47** & **36.51** & **88.82** & **44.52** & 91.91 & 29.24 & **94.72** & **33.86** \\ \cline{1-1}  & NNGuide(\(\downarrow\)=1) & 94.33 & 29.27 & 91.23 & 36.4 & 85.71 & 46.20 & 92.93 & 26.31 & 91.80 & 34.55 \\ \cline{1-1}  & NNGuide(\(\downarrow\)=1) & **95.84** & **21.61** & **91.91** & **34.45** & **89.33** & **43.42** & 93.12 & 25.80 & **92.56** & **33.32** \\ \cline{1-1}  & NNGuide(\(\downarrow\)=1) & **95.10** & 26.09 & 91.44 & 36.56 & 85.63 & 47.64 & 91.61 & 24.17 & 92.19 & 33.08 \\ \cline{1-1} \cline{2-11}  & & NNGuide(\(\downarrow\)=1) & **96.42** & **92.20** & **34.43** & **89.39** & **44.34** & 93.79 & 23.58 & **92.95** & **30.48** \\ \hline \hline \end{tabular}
\end{table}
Table 18: Compatibility experiments of CoVer combined with NNGuide and MaxLogit based on ResNet50 and CLIP-B/16. The ID data are ImageNet-1K.

Comparison with Watermarking.In addition, Watermarking [54] is another competitive methods needed to be considered. We have added the analysis about the Watermarking method with our CoVer in the following two aspects.

Conceptually, we have noticed that Watermarking utilizes a well-trained mask to help the original images be distinguishable from the OOD data. However, Watermarking is still trying to excavate the useful feature representation in a single-input perspective. In contrast, the critical distinguishable point and also the advantage of our CoVer method lies in input dimension expansion with the corrupt variants, which instead provide a extra dimension to explore the confidence mutation to better identify the OOD samples.

Experimentally, we have conducted the comparison and report the results in Table 19. The results show that, on the one hand, training an optimized watermarking for effectively distinguishing between ID and OOD samples is a time-consuming process. On the other hand, CoVer achieves this by introducing corrupted inputs to capture the confidence variations between ID and OOD data during the test phase, which is simpler, faster, and more effective.

Comparison with Data Depth, Information Projection, and Isolation Forest.We have also conducted comparison experiments between our CoVer and baselines methods from data depths, information projections, and isolation forest, as detailed in Table 20.

Due to the large scale of the ImageNet training set, we sampled 50 samples per class to construct a subset from the training data to represent the training distribution, as recommended by the similar work named NNGuide [39]. For data depths, we reimplemented APPROVED [41] for comparison. For information projections, we reproduced REFEREE [40] for comparison. For Isolation Forest, we use logits as the input to detect the anomaly logits in ID and OOD samples.

The results indicate that AD and textual OOD detection methods, such as Data Depth and Information Projection, may not suit for visual OOD detection tasks, a view also mentioned in related surveys [8, 9]. Similarly, classical ML methods for AD, such as Isolation Forest, seem to be failed to excavate discriminative representations when applied to image OOD detection. However, since these methods are insightful in distinguishing the outliers, we believe it is worth further efforts in the future to adopt the critical intuition into the OOD detection problem.

### Compatibility with each DNN-based mehtods

It is worth to note that, in Table 1, we only reported the results of CoVer combined with ASH because it best demonstrates the excellence of CoVer. In Table 3, we also show the results of CoVer combined with DICE and ReAct, and CoVer can also provide performance gains for them. Here in Table 21, we further report the comparison of CoVer combined with each mentioned DNN-based methods (adding MSP, ODIN, and Energy score), which strongly demonstrates its superiority.

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c} \hline \hline \multirow{3}{*}{Architecture} & \multirow{3}{*}{Method} & \multicolumn{3}{c}{Normalist} & \multicolumn{3}{c}{SUN} & \multicolumn{3}{c}{Places} & \multicolumn{3}{c}{Textures} & \multicolumn{3}{c}{Average} \\ \cline{3-11}  & & AUROC & FPR951 & AUROC & FPR951 & AUROC & FPR951 & AUROC & FPR951 & AUROC & FPR951 \\ \hline \multirow{3}{*}{ResNet50} & APPROVED & 53.25 & 95.90 & 60.10 & 94.63 & 56.77 & 94.26 & 70.21 & 78.30 & 60.06 & 90.77 \\  & REFEREE & 79.77 & 94.76 & 73.28 & 96.91 & 72.9 & 96.80 & 74.01 & 94.08 & 74.99 & 95.56 \\  & IsLinear Forest & 90.36 & 85.94 & 93.55 & 94.78 & 62.27 & 93.91 & 65.89 & 81.37 & 64.12 & 89.00 \\  & **MSP+CoVer** & **90.81** & **44.90** & **82.51** & **66.38** & **81.57** & **69.34** & **81.00** & **65.43** & **83.97** & **61.41** \\ \hline \hline \end{tabular}
\end{table}
Table 20: Comparison with competitive anomaly detection and textual OOD detection baselines based on ResNet50. The selected methods’ types are Data Depth, Information Projection and Isolation Forest, respectively. The ID data are ImageNet-1K.

\begin{table}
\begin{tabular}{c|c|c c c c c c c c c c} \hline \hline \multirow{3}{*}{Architecture} & \multirow{3}{*}{Method} & \multicolumn{3}{c}{Normalist} & \multicolumn{3}{c}{SUN} & \multicolumn{3}{c}{Places} & \multicolumn{3}{c}{Textures} & \multicolumn{3}{c}{Average} \\ \cline{3-11}  & & & AUROC & FPR951 & AUROC & FPR951 & AUROC & FPR951 & AUROC & FPR951 & AUROC & FPR951 \\ \hline \multirow{3}{*}{ResNet50} & \multirow{3}{*}{Wierstrastra} & 80.31 & 74.45 & 79.21 & 73.27 & 79.78 & 71.43 & 80.44 & 67.53 & 79.94 & 71.67 & 3 \\  & & **MSP+CoVer** & **90.81** & **44.90** & **82.51** & **66.38** & **81.57** & **69.34** & **81.00** & **65.43** & **83.97** & **61.41** & **10 mins** \\ \hline \hline \end{tabular}
\end{table}
Table 19: Comparison with competitive OOD detection method Watermark based on ResNet50. The ID data are ImageNet-1K.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have clearly stated the main contributions made in the paper and important assumptions and limitations in both the Abstract and the Introduction (Section 1). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 5.2, we have discussed the limitations of this work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.

\begin{table}
\begin{tabular}{c|c|c c c c c c c c c} \hline \hline \multirow{2}{*}{Architecture} & \multirow{2}{*}{Method} & \multicolumn{4}{c}{OOD Dataset} & \multicolumn{4}{c}{Textures} & \multicolumn{4}{c}{Average} \\ \cline{3-11}  & & Alburocul & CPRR955 & \(\Delta\)IROC & CPRR955 & \(\Delta\)IROC & CPRR955 & \(\Delta\)IROC & CPRR955 & \(\Delta\)IROC & CPRR955 \\ \hline \multirow{9}{*}{**ResNet50**} & MSP & 87.74 & 54.99 & 80.86 & 70.83 & 79.76 & 73.99 & 79.61 & 68.00 & 81.99 & 66.95 \\  & **MSP-Cover** & **90.81** & **44.49** & **82.51** & **66.38** & **81.57** & **69.34** & **81.00** & **65.43** & **83.97** & **61.41** \\  & ODIN & 91.37 & 41.57 & 86.89 & 53.97 & 84.44 & 62.15 & 87.57 & 45.53 & 87.57 & 50.80 \\  & ODIN\&**C-Cover** & **93.66** & **31.56** & **84.14** & **84.57** & **85.98** & **86.97** & **87.47** & **44.77** & **85.84** & **46.62** \\  & Energy score & 89.95 & 55.72 & 85.89 & 59.87 & 82.86 & 64.92 & 85.99 & 53.72 & 86.17 & 58.41 \\  & Energy+Cover** & **92.13** & **46.67** & **87.42** & **86.50** & **84.98** & **63.16** & **86.99** & **51.70** & **87.91** & **54.51** \\  & Reactor & 96.22 & 20.38 & 94.20 & 24.20 & 91.53 & 33.35 & 83.00 & 47.30 & 92.95 & 31.43 \\  & ReCa+Cover** & **97.58** & **13.35** & **97.57** & **18.91** & **93.08** & **29.02** & **91.58** & **40.74** & **94.48** & **25.51** \\  & DeCa+Cover** & 94.49 & 25.63 & 90.83 & 35.15 & 87.48 & 46.90 & 90.30 & 31.72 & 90.77 & 34.75 \\  & DICE+Cover** & **96.83** & **18.66** & **93.53** & **28.52** & **90.00** & **40.54** & **91.14** & **31.15** & **92.87** & **29.19** \\  & ASH+B-Cover** & 94.25 & 28.95 & 90.32 & 40.21 & 87.52 & 94.92 & 91.53 & 33.48 & 90.91 & 39.04 \\  & ASH+B-Cover** & **97.14** & **14.04** & **94.12** & **25.77** & **91.05** & **35.93** & **91.93** & **30.39** & **93.56** & **26.53** \\ \hline \hline \end{tabular}
\end{table}
Table 21: Compatibility experiments of CoVer combined with each mentioned DNN-based OOD detection method. The ID data are ImageNet-1K.

* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not include new theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have presented a reproducibility statement at the beginning of the Appendix. In addition, we have provided the code and data in the supplementary material to reproduce the main experimental results of the paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have provided open access to the code and data with sufficient instructions in the supplementary material to faithfully reproduce the main experimental results of the paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In Section 4.1, we have briefly introduced the experimental setups about datasets, model setups, and evaluation metrics. Furthermore, we have provided full details of experimental settings in Appendix C.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.

* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In the third ablation study about the number of expanded measuring dimensions of our proposed method, the results are accompanied by error bars, and the factors of variability are clearly stated (refer to Section 4.3). In Appendix C.2.3, we have provided the full results of this experiment. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided sufficient information on the computer resources, such as the type, amount, and memory of compute workers GPU required for the experiments in the reproducibility statement at the beginning of the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes]Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics, and the authors remain anonymous. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: In Section 5.1, we have discussed the potential broader impact of this paper. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All used assets in the paper are properly credited, with explicit mentions of their licenses and terms of use, in full compliance with the recommended guidelines. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.