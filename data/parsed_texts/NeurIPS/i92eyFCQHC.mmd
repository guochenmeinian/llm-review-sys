# WildVision: Evaluating Vision-Language Models

in the Wild with Human Preferences

 Yujie Lu\({}^{\spadesuit}\) Dongfu Jiang\({}^{\diamond}\)

&Wenhu Chen\({}^{\diamond}\) William Yang Wang\({}^{\spadesuit}\) Yejin Choi\({}^{\diamond}\) Bill Yuchen Lin\({}^{\diamond}\)

\({}^{\diamond}\)Allen Institute of AI University of Washington

\({}^{\spadesuit}\)University of California, Santa Barbara \({}^{\diamond}\)University of Waterloo

yujielu@ucsb.edu, yuchenl@allenai.org

###### Abstract

Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects human preferences to evaluate VLMs. We curated WV-Bench by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-Arena Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar. Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.

Figure 1: WildVision-Arena (WV-Arena) supports multi-round multimodal chats with \(20+\) models, enabling the comparison of VLMs in real-world scenarios. We curate WildVision-Bench (WV-Bench) by selecting 500 samples from \(20k+\) in-the-wild chats and \(8k+\) user ratings. Automatic model scorings on WV-Bench closely correlate with the Elo ratings on WV-Arena.

Introduction

Vision-language models (VLMs) [68; 82; 69; 49; 14; 113; 3; 5] have shown groundbreaking performance across various applications, necessitating enhanced evaluation approaches [87; 24; 107; 106] to keep up with their rapid advancements. Current evaluation benchmarks, however, are constrained by simplicity [53; 102] and practicality [101; 50]. Meanwhile, evaluation metrics for vision and language tasks are predominantly reference-based, focusing on exact matches or model-based scores [87; 7]. The success of the CLIP model [73] has enabled reference-free evaluation [24], reducing the need for reference curation while maintaining alignment with human annotators. More recent evaluation methods [56; 107; 35] leverage the instruction-following capability of LLMs and the expertise of vision models [15; 91; 34], making the automatic evaluation of VLMs more fine-grained and interpretable. Despite these advancements, a gap remains between these metrics and human preferences when comparing a large number of models' capabilities in real-world multimodal interactions.

In this paper, we introduce WildVision-Arena and WildVision-Bench to address the need for tracking human preferences regarding models' capabilities in the wild. Our WildVision-Arena is a chatbot-style [110; 12] platform that facilitates easy comparison among VLMs, utilizing the Elo Rating system as the primary ranking metric. With the support of over \(20\) models (GPT-4o [69], GPT-4V [68], Gemini-Pro [82], Gemini-1.5 [81], Reka [83], Claude-3 [2], LLaVA-NEXT [48], etc), alongside a side-by-side chatting interface over images, we have crowdsourced over \(20,000\) multi-round human-AI chat interactions, including over \(8,000\) votes and fine-grained feedback. We then sample diversified and safe data as our WildVision-Bench and adapt AlpacalEval [44] to visual context. Specifically, we use the latest released GPT-4o [69] as a judge model to vote between each VLM and the reference model Claude-3-Sonnet [2]. The statistically estimated model scores on WV-Bench achieve a Spearman's Correlation of \(0.94\) with Elo ratings in WildVision-Arena.

Our comprehensive analysis of these in-the-wild chats identifies areas for improvement in recognizing visual context, spatial reasoning and imagination, and expert domain knowledge. Additionally, lower-performing VLMs struggle with discerning fine visual details in images, hindered by resolution and contextual limitations. Across the board, these models also face challenges with hallucination and safety concerns. Our main contributions can be summarized as:

* We develop WildVision-Arena, an interactive evaluation platform that hosts over 20 VLMs and a live leaderboard reflecting crowdsourced user preferences on real-world chats.
* We curate WildVision-Bench from WildVision-Arena, a fast-evaluation benchmark that closely aligned with human preferences at \(0.94\) Spearman's Correlation.
* We comprehensively analyze \(20,000+\) multimodal conversations and \(8,000+\) votes, and we will release this data to advance future research in VLMs.

## 2 WildVision-Arena: Ranking VLMs with Human Preference

In this section, we introduce WildVision-Arena and present statistics of in-the-wild chat data, along with a deep analysis of human preferences that formulate our online VLMs leaderboard.

### Overview Design of WildVision-Arena

Users conduct multi-round chats over uploaded images, during which two models from the pool or third-party APIs are sampled. Users vote for the better response, with the model's identity revealed afterward, and can provide reasons for their choices. Votes contribute to a live leaderboard, which is updated every few hours to rank the models. Appendix A shows a screenshot of our user interface. In WildVision-Arena, we currently support \(20+\) VLMs as shown in the leaderboard on the right part of Figure 1. The generation hyperparameters are set the same when comparing these models, and users can change the temperature, top-p and max output tokens per their use cases.

### Statistics of Chat Data with Votings

Each chat data point that has human voting is classified into a category-subcategory and domain-subdomain using GPT-4v. The prompt template details are provided in Appendix E.1. Key statistics of user voting in WildVision-Arena are presented in Table 1. The number of tokens is estimated with tiktoken tokenizer corresponding to model 'gpt-3.5-turbo'. Figure 2 and Figure 3 visualize the distribution of these voting data in terms of question categories and image domains, respectively. In addition to the three dominant question categories (Recognition, Descriptive, Analytical), the Interactive, Instructive, and Creative categories are also receiving increasing interest. Users are mostly interested in chat about images tagged with the _Entertainment_ domain (most of which are related to games and movies/TV shows), as well as the Urban, Expert, and People domains.

### Crowdsourced Human Preference on VLMs in the Wild

Pairwise ComparisonWe visualize the heatmap of battle counts and win fractions of seven models out of the 20+ models supported in the WildVision-Arena in Figure 4. The battle count heatmap highlights the frequency of direct comparisons, with models like GPT-4V vs. Gemini-Pro (\(252\) voted battles) being tested more rigorously. GPT-4o consistently outperforms the others by a large margin, winning \(77\%\) of its battles against the second-best model, GPT-4V, which ranks as the second best. Reka-Flash follows closely behind GPT-4V, winning \(42\%\) of its battles, while other models demonstrate lower winning rates. Among the open-source models, LLAVA-NEXT leads, though there remains a significant gap between it and both GPT-4V and GPT-4o.

Expert Agreement with User VotingTo assess the quality of crowdsourced user voting data on our platform, we evaluated inter-annotator agreement by comparing the annotations of our experts

Figure 4: Battle Count Heatmap (Left): the number of voted comparisons between models. Win Fraction Heatmap (Right): the winning rate of Model A over Model B in voted comparisons.

with those from users of the WildVision-Arena. This analysis was conducted on a set of 100 samples. Our findings indicate a substantial level of agreement with the two experts, with an average percentage agreement of \(72.5\%\). Furthermore, the calculated Cohen's Kappa coefficient was \(0.59\), suggesting a moderate to high degree of reliability in the annotations across different annotators.

### Model Ranking with Elo Rating in WildVision-Arena

Following Chatbot Arena [12], we adapt Elo Rating System [17] to provide a dynamic evaluation platform for ranking VLMs by statistical modeling based on our collected direct pairwise comparisons. We briefly introduce the Online Elo Rating and the statistical estimation method.

Online Elo RatingElo rating focuses on modeling the probability of player \(i\) winning against player \(j\) given their existing ratings \(R_{i}\) and \(R_{j}\) respectively, where \(i,j\in N\). We define a binary outcome \(Y_{ij}\) for each comparison between player \(i\) and player \(j\), where \(Y_{ij}=1\) if player \(i\) wins against player \(j\), and \(Y_{ij}=0\) otherwise. Then the logistic probability is formulated as:

\[P(Y_{ij}=1)=\frac{1}{1+10^{(R_{j}-R_{i})/\alpha}},\] (1)

where \(\alpha=400\) for Elo rating computation. After a match, each player's rating is updated by the formula: \(R^{\prime}_{i}=R_{i}+K\times(S(i|j)-E(i|j))\), where \(S(i|j)\) is the actual match outcome (1 for a win, 0.5 for a tie, and 0 for a loss), and \(E(i|j)=P(Y_{ij}=1)\). The higher-rated player will win fewer points if they win but lose more if they lose, while the lower-rated player will experience the opposite. The computation of the online Elo rating is correlated with the comparison order. Therefore, we follow Chatbot Arena to adopt the Bradley-Terry model [9] for a stable statistical estimation.

The probability of player \(i\) winning against player \(j\) given their existing ratings \(R_{i}\) and \(R_{j}\) respectively.

Statistical EstimationThe Bradley-Terry model [9] estimates the Elo rating using a logistic regression model and maximum likelihood estimation (MLE). Let's say there are \(N\) players, and we have a series of pairwise comparisons, where \(W_{ij}\) is the number of times player \(i\) wins against player \(j\). The log-likelihood function for all pairwise comparisons can be written as:

\[\mathcal{L}(\mathbf{R})=\sum_{i,j\in N,i\neq j}\left(W_{ij}Y_{ij}\log P(Y_{ij }=1)\right),\] (2)

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \multirow{2}{*}{**Models**} & \multirow{2}{*}{**Size**} & \multirow{2}{*}{**Elo**} & \multirow{2}{*}{**Battles**} & \multirow{2}{*}{**MMMU**} & \multicolumn{3}{c}{**Question Category**} & \multicolumn{3}{c}{**Image Domain**} \\ \cline{5-11}  & & & & & & & & Analy. & Descri. & Recogn. & Enter. & Objects & Expert \\ \hline GPT-40 [69] & – & **1235** & \(434\) & **62.8** & **1290** & **1250** & **1236** & **1362** & **1203** & **1293** \\ GPT-4-Vision [68] & – & 1132 & \(2288\) & 56.8 & 1154 & 1169 & 1099 & 1177 & 1109 & 1178 \\ Reka-Flash [83] & – & 1107 & 513 & 56.3 & 1093 & 1141 & 1067 & 1069 & 1101 & 1191 \\ Claude-3-OPUS [2] & – & 1100 & 908 & 59.4 & 1117 & 1096 & 1092 & 1111 & 1127 & 1128 \\ Gemini-Pro-Vision [82] & – & 1061 & 2229 & 47.9 & 1099 & 1041 & 1090 & 1088 & 1077 & 1041 \\ Yi-VL-PLUS [1] & – & 1061 & 283 & – & 1084 & 1040 & 1078 & 1001 & 1119 & 1101 \\ LLaVA-NEXT [48] & \(34B\) & 1059 & 1826 & 51.1 & 1068 & 1104 & 1021 & 1074 & 1015 & 1052 \\ Gemini-1-5-Flash [81] & – & 1055 & 132 & – & 1090 & 1018 & 1085 & 1190 & 990 & 1127 \\ Claude-3-Bonnet [2] & – & 1041 & 496 & 53.1 & 1063 & 1056 & 1041 & 1033 & 1023 & 1119 \\ CogVLM-Chat-HF [89] & \(13B\) & 1016 & 1024 & 32.1 & 950 & 947 & 1006 & 955 & 930 & 950 \\ Claude-3-Haiku [2] & – & 1002 & 419 & 50.2 & 964 & 1008 & 996 & 1033 & 1014 & 1005 \\ LLAVA-NEXT [48] & \(7B\) & 992 & 1367 & 35.1 & 963 & 1032 & 977 & 992 & 1023 & 1001 \\ DeepSeek-VL [51] & \(7B\) & 979 & 646 & 36.6 & 988 & 984 & 953 & 956 & 1026 & 962 \\ Idefies [37] & \(8B\) & 965 & 100 & 36.6 & 818 & 1003 & 1011 & 909 & 1071 & 1020 \\ LLAVA-NEXT [48] & \(13B\) & 956 & 201 & 35.9 & 965 & 974 & 1006 & 975 & 971 & 987 \\ Qwen-VL-Chat [5] & \(10B\) & 930 & 1328 & 35.9 & 898 & 937 & 940 & 923 & 942 & 902 \\ Bunny-V1 [23] & \(3B\) & 921 & 389 & 38.2 & 897 & 922 & 878 & 884 & 823 & 823 \\ MiniCPM-V [26] & \(3B\) & 910 & 1349 & 34.7 & 895 & 911 & 925 & 888 & 890 & 840 \\ LLaVA-V1-5 [47] & \(13B\) & 891 & 299 & 36.4 & 952 & 838 & 920 & 887 & 827 & 914 \\ Tiny-LLaVA-v1-HF [111] & \(3B\) & 879 & 288 & 33.1 & 901 & 828 & 821 & 808 & 853 & 894 \\ InstrueBLIP [14] & \(7B\) & 862 & 807 & 30.6 & 834 & 856 & 891 & 840 & 902 & 763 \\ UFORM-Gen2-Qwen [86] & \(500M\) & 827 & 452 & – & 911 & 785 & 853 & 768 & 937 & 830 \\ \hline \end{tabular}
\end{table}
Table 2: WildVision-Arena Leaderboard. We show the full elo score and within three question categories (Analytical, Descriptive, Recognition) and three image domains (Entertainment, Objects, Expert) of 22 models with a time cutoff at May 29, 2024. **Best** Second Best Best among proprietary models Best among open-source models.

where \(\mathbf{R}=\{R1,...,R_{N}\}\) is the Elo rating variable of each player. Since this modeling does not consider ties, in practice, we duplicate all the votes and force half of the tie votes to be counted as left model \(i\) winning (\(Y_{ij}=1\)) and the other half as right model \(j\) winning (\(Y_{ij}=0\)).

### WildVision-Arena Leaderboard

We report the leaderboard results in Table 2, including the full Elo ratings and the total number of battles for each model, with a time cutoff on May 29, 2024. Additionally, we provide the Elo ratings for three main question categories (Analytical, Descriptive, Recognition) and three main image domains (Entertainment, Natural, Expert) to better understand the specialties of each model. GPT-4o quickly dominates the leaderboard after its release, surpassing the previous state-of-the-art GPT-4V by a significant margin, followed by Reka-Flash, Claude-3-OPUS. Yi-VL-PLUS and LLaVA-NEXT-34B achieve the same rank, reflecting that both models are based on the Yi [1]. Among open-source models, LLaVA-NEXT-34B ranks first, even surpassing Gemini-1.5-Flash and Claude-3-Sonnet, Claude-3-Haiku, indicating a strong baseline for research purposes. To compare models under each question category and image domain, we present the top six models ranked in the WildVision-Arena leaderboard in terms of Elo ratings for each question category and image domain in Figure 5. GPT-4o consistently outperforms all other models except for the images tagged with Natural, where varying specialties are more commonly observed among the other models.

## 3 WildVision-Bench: In-the-Wild Testbed for VLMs

Recent VLMs reveal a closing gap with GPT-4V on various benchmarks[101, 102], but this improvement is not always reflected in users' daily experiences. This discrepancy arises from current models' limited generalizability compared to proprietary ones, which fixed benchmarks fail to capture. To address this, we propose creating WildVision-Bench, a challenging and natural benchmark for VLMs that reflects real-world human use cases, with models' rankings aligning closely with the WildVision-Arena leaderboard contributed by diverse crowdsourced user votes.

Figure 5: Elo ratings of six models across question categories (Top) and image domains (Bottom).

### Data Curation Pipeline

Starting with in-the-wild multimodal conversation data from WildVision-Arena's users, we apply the NSFW detector [36] on the images to filter out unsafe content. We then perform deduplication on the images and apply diversity sampling to formulate a public set of 500 data samples for WildVision-Bench. Our experts manually annotate 50 samples as a preview of a hidden set, which will be updated dynamically to avoid contamination. We showcase the model performance on two cases from expert annotations in Table 3.

### Automatic Evaluation on WildVision-Bench

VLMs as a Local EvaluatorPrevious work [107; 35] shows alignment between GPT-4V and humans when evaluating the performance of VLMs. We further validate the agreement of GPT-4V with crowdsourced human preferences in WildVision-Arena to ensure its efficacy in the wild. Specifically, we feed a pair of multimodal conversations along with the votes into GPT-4V to select among four choices: 1) left/right vote: the left/right model response is better, 2) tie/bad vote: both models are equally good/bad. In Appendix E.3, we provide the detailed prompt template for GPT-4V. We show the GPT-4V vs Arena Human alignment in Figure 6. We observe that GPT-4V has relatively low agreement with humans on tie votes but shows high agreement with humans when both models

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multirow{2}{*}{**Metric vs Human**} & \multicolumn{3}{c}{**GPT-4v**} \\ \cline{2-4}  & **4-way** & **3-way** & **Binary** \\ \hline F1 Score (Macro) & 0.4245 & 0.5143 & 0.7792 \\ F1 Score (Micro) & 0.5747 & 0.5842 & 0.7796 \\ F1 Score (Weighted) & 0.5407 & 0.5536 & 0.7798 \\ Cohen’s Kappa Score & 0.3404 & 0.3442 & 0.5585 \\ Pearson Correlation & 0.2906 & 0.2880 & 0.5587 \\ \hline \hline \end{tabular}
\end{table}
Table 3: VLMs’ responses on two cases from WildVision-Bench expert annotated samples. The example \(\#61\) is a hard case that all models fall short at.

Figure 6: Left: GPT-4V vs. Arena Human Voting. Right: Agreement; 4-way: left/right/tie/bad vote. 3-way: left/right/other. Binary: left/right voteexhibit distinguishable differences. However, predicting when both models are bad is challenging as GPT-4V sometimes falls short in these examples as well.

WildVision-Bench Alignment with Human Preferences in WildVision-ArenaInspired by Alpaca Eval [16], we adopt a similar approach to rank VLMs on our WildVision-Bench automatically. Specifically, we use GPT-4o as the judgment model and Claude-3-Sonnet as our reference model. We compare each model's answers on the WildVision-Bench public set with Claude-3-Sonnet and then use GPT-4o, which shows better alignment with humans in our cases, to give a vote. The template in Table E.3 is used for the prompt of the judge, where 5 levels of comparison results are defined, which are "Better+", "Better", "Tie", "Worse", and "Worse+" respectively. We report the score results of these models in Table 4. This achieves a \(0.94\) Spearman correlation with the WildVision-Arena leaderboard.

Benchmark Correlation HeatmapWe visualize the Spearman correlation heatmap among various multimodal benchmarks in Figure 7. The MMBench-series [50] (CCBench, MMBench EN, MMBench CN) considers fine-grained perception and reasoning tasks in multiple choice questions. MMVet [101] evaluates integrated capabilities in visual question answering. MMStar [10] alleviates misjudgment issues with high-quality multiple choice questions. HallucionBench [22] focus on investigating hallucination issues, while MMMU [102] and MathVista [53] focus on college-level subject knowledge and mathematical reasoning in visual contexts, respectively. WildVision Elo represents the arena leaderboard, reflecting human preferences using Elo ratings from pairwise comparisons. WildVision Bench represents ranking model using estimated model score on our WildVision-Bench. This achieves the highest correlation with WildVision Elo, indicating its crucial role in simulating human preferences on these VLMs in the real world. The runner-up in alignment with human preferences is MMVet, followed by MMMU and MMStar.

\begin{table}
\begin{tabular}{l|c c c c c c c c c} \hline \hline
**Model** & **Score** & **95\% CI** & **Win Rate** & **Reward** & **Much Better** & **Better** & **Tie** & **Worse** & **Much Worse** & **Avg Tokens** \\ \hline GPT-4o [99] & \(89.41\) & \((-1.7,2.0)\) & \(80.6\) & \(56.4\) & \(255.0\) & \(148.0\) & \(14.0\) & \(72.0\) & \(11.0\) & \(157\) \\ GPT-4Vision [68] & \(80.01\) & \((-19.28)\) & \(71.8\) & \(39.4\) & \(18.2\) & \(177.0\) & \(22.0\) & \(91.0\) & \(28.0\) & \(140\) \\ Relax-Flash [83] & \(64.79\) & \((-2.9,3.0)\) & \(58.8\) & \(18.9\) & \(135.0\) & \(15.0\) & \(15.0\) & \(28.0\) & \(116.0\) & \(62.0\) & \(181\) \\ Claude-3-Opus [2] & \(26.25\) & \((-2.8,3.4)\) & \(53.0\) & \(13.5\) & \(103.0\) & \(162.0\) & \(48.0\) & \(141.0\) & \(64.0\) & \(120\) \\ Yi-VL-PLUS [1] & \(55.09\) & \((-2.9,3.0)\) & \(52.8\) & \(7.2\) & \(98.0\) & \(160.0\) & \(29.0\) & \(124.0\) & \(83.0\) & \(150\) \\ LiLAv-NEXT-348 [48] & \(51.91\) & \((-31.2,4)\) & \(49.2\) & \(2.5\) & \(90.0\) & \(156.0\) & \(26.0\) & \(145.0\) & \(83.0\) & \(165\) \\ Claude-3-Sonnet [2] & \(50.00\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) & \((-2.9,3.0)\) \\ Claude-3-Biasu [2] & \(37.07\) & \((-32.4,2.2)\) & \(30.6\) & \(-12.6\) & \(5.0\) & \(99.0\) & \(47.0\) & \(-228.0\) & \(72.0\) & \(97\) \\ Gemini-P-vision [82] & \(35.45\) & \((-2.6,3.2)\) & \(32.6\) & \(-21.0\) & \(80.0\) & \(83.0\) & \(27.0\) & \(167.0\) & \(143.0\) & \(66\) \\ LLAv-NEXT-138 [48] & \(33.69\) & \((-3.8,2.7)\) & \(33.85\) & \(-21.4\) & \(62.0\) & \(107.0\) & \(25.0\) & \(167.0\) & \(139.0\) & \(138\) \\ DeepSF-VL-7B [51] & \(33.48\) & \((-2.2,3.0)\) & \(35.6\) & \(-21.2\) & \(59.0\) & \(119.0\) & \(17.0\) & \(161.0\) & \(144.0\) & \(119\) \\ CoSyT-ML-49N [81] & \(38.48\) & \((-2.7,2.4)\) & \(30.05\) & \(-26.4\) & \(75.0\) & \(78.0\) & \(15.0\) & \(172.0\) & \(160.0\) & \(63\) \\ LiLAv-NEXT-7B [48] & \(26.15\) & \((-2.7,2.3)\) & \(27.05\) & \(-31.4\) & \(45.0\) & \(90.0\) & \(36.0\) & \(164.0\) & \(165.0\) & \(139\) \\ Hefe(2-37) [37] & \(27.1\) & \((-2.4,2.5)\) & \(26.4\) & \(-35.8\) & \(44.0\) & \(88.0\) & \(19.0\) & \(164.0\) & \(185.0\) & \(128\) \\ Owen-VL-Chat [5] & \(17.87\) & \((-2.6,2.2)\) & \(19.6\) & \(45.7\) & \(47.9\) & \(42.0\) & \(56.0\) & \(15.0\) & \(155.0\) & \(232.0\) & \(70\) \\ LiLAv-NEXT-5138 [47] & \(14.15\) & \((-2.2,2.2)\) & \(16.8\) & \(52.5\) & \(28.0\) & \(56.0\) & \(19.0\) & \(157.0\) & \(240.0\) & \(87\) \\ Bumy-38 [23] & \(12.70\) & \((-1.8,1.9)\) & \(16.6\) & \(-54.4\) & \(23.0\) & \(60.0\) & \(10.0\) & \(164.0\) & \(243.0\) & \(76\) \\ MinTM-CV [26] & \(11.66\) & \((-1.8,1.2)\) & \(13.6\) & \(57.5\) & \(25.0\) & \(43.0\) & \(16.0\) & \(164.0\) & \(252.0\) & \(89\) \\ Tiny-LLVA [11] & \(8.01\) & \((-1.4,1.4)\) & \(11.0\) & \(<66.2\) & \(16.0\) & \(39.0\) & \(15.0\) & \(127.0\) & \(303.0\) & \(74\) \\ UFORM-Gen2-Owen [86] & \(7.55\) & \((-1.6,1.1)\) & \(10.8\) & \(<68.5\) & \(16.0\) & \(38.0\) & \(11.0\) & \(115.0\) & \(320.0\) & \(92\) \\ Intruc(BLP-TB) [14] & \(5.54\) & \((-1.3,1.5)\) & \(7.8\) & \(7\) & \(-72.5\) & \(11.0\) & \(28.0\) & \(15.0\) & \(117.0\) & \(329.0\) & \(47\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Estimated model scores of VLMs on WildVision-Bench

Figure 7: WildVision-Bench achieves the highest correlation with WildVision-Arena, with a Spearman’s correlation of 0.94.

## 4 Analysis

In-the-wild Multimodal ChatIn contrast to public benchmark, in-the-wild multimodal conversations involve images and instructions from a diverse range of sources and receive vote data from a varied group of users. This better helps us understand how current VLMs can benefit real-world scenarios and reveal improvement directions for researchers in the field. In Appendix B, we present more cases under each image domain and question category. We will release both multimodal chat and crowdsourced voting data for future research.

Failure CasesIn Table 5, we present two distinct failure instances that are documented in the WildVision-Arena platform. This analysis reveals that GPT-4V's limitations primarily stem from insufficient background knowledge, whereas Gemini-Pro-Vision often fails to discern and process subtle details crucial for deriving correct answers. Additional details on these failure cases are provided in Appendix Our categorization of common failures includes six types: Visual Recognition, Visual Reasoning, Spatial Imagination, Contextual Understanding, Expert Domain Knowledge, Hallucination, and Safety. Although not all failure cases can be included in this paper, we plan to periodically release additional cases on our live platform to aid ongoing research and development.

Model Comparison on WildVision-BenchTable 3 compares the responses of GPT-4V, LLaVA-NEXT-34B, and Gemini-Pro-Vision on a validation sample from WildVision-Bench. GPT-4V generally outperforms the other models, confirming expectations of its superior capabilities. Nevertheless, all models occasionally fail to deliver correct responses, notably in scenarios requiring compositional reasoning, regardless of the simplicity of the text or the image involved. We also observe that recognizing and interpreting subtle visual details within images is still challenging for less capable models.

Broader ImpactFor the first version of data release, we plan to release over 20,000 crowdsourced multi-turn conversation data and more than 8,000 human votings with reasons, providing a valuable resource for understanding human preferences in VLMs interactions and developing models that align more closely with human standards in real-world scenarios. We will also present a live leaderboard together with useful failure case analysis to keep track of recent advancements in this field. Additionally, by open-sourcing the WildVision-Arena code, we enable researchers and developers to adapt our methods to other domains. We will also support fast evaluation of our WildVision-Bench for quick and human-aligned evaluation, which aligns with the human preferences in VLMs in real-world scenarios.

\begin{table}
\begin{tabular}{p{142.3pt} p{142.3pt}} \hline \hline
**GPT-4V:** The image shows a screenshot of Windows 3x, which is a series of graphical operating environments produced by Microsoft for use on personal computers, prior to the Windows 95 operating system... \\ \hline \hline \end{tabular}
\end{table}
Table 5: Failure cases of GPT-4V and Gemini-Pro-Vision sampled from WildVision-Arena.

Modality, Resolution, Long Context, Resource-EfficientMany work have extended vision-language models (VLMs) beyond image-text modalities, including video [105; 57; 109], audio [13], and even applied to embodied agent [65]. Future work may consider improving all-in-one models [63; 92; 82; 112; 19] by discovering better methods to integrate these modality data. Recent works have enabled high-resolution [48; 96] and text reading [108; 25] capabilities in VLMs, although many failure cases are still induced by low resolution or poor OCR capability. Other work advances multi-image and long-context capabilities in VLMs [61; 37; 29; 79; 54]. We expect future research to discover the best mechanisms for balancing compact and effective approaches to convey multimodal information, such as recent progress of text representation in pixel space [75; 18; 55]. This is essential to closing the gap between open-source multimodal agents [99; 104] and proprietary ones [97; 69]. Although many works [26; 111] have made VLMs more compact, their performance is still not satisfying. Future work may further improve the performance of smaller models with less training data and higher throughput inference.

World Knowledge and Safety in VLMsThe challenge of embedding extensive world knowledge within VLMs is significant, particularly given their current limitations in understanding physical principles and interacting with real-world environments. These models' ability to dynamically expand their knowledge base through activities like browsing the internet, reading books, or watching videos is an exciting potential advancement. Key concerns in LLMs include security [94; 64; 90; 98], privacy [31; 38], and the propagation of truthfulness [30; 77; 45] and prevention of misinformation [80; 72; 103]. For VLMs, they face unique safety challenges: 1) incorrect alignment of multimodal data can lead to harmful outputs, 2) images may contain sensitive information, necessitating careful handling, and 3) VLMs are vulnerable to attacks manipulating both text and images.

## 5 Related Work

Live Benchmarking for vision-language modelsVision-and-language pre-training starts from models [42; 43] adapting objectives in BERT [33], to models [74] adopting contrastive learning, and to unified frameworks [52; 88; 41; 40] without task-specific head. With recent advancements of Large Language Models [67; 20; 4; 84; 85], their multi-modal counterparts [68; 82; 14; 49; 47; 5; 28; 37] are dominating vision and language tasks. Beyond previous task-specific caption [11; 78], visual question answer [62; 59; 27; 21; 60], grounding [46; 100; 66; 58; 71], more benchmarks [101; 50; 39; 32] are proposed to capture VLMs capabilities. When building such benchmarks, there is an urge need to consider alleviating data contamination [76; 6] during eval, assuring robustness [55] and difficulty [70], and incorporating real-world scenarios [8; 93]. We build WildVision-Arena to support diversified, difficult, in-the-wild, live benchmarking [12; 95] of VLMs.

Human-Aligned Evaluation for vision-language modelsEvaluation for open-ended vision and language tasks [8; 93; 70] are usually challenging, and recent techniques improve human alignment by mapping free-form predictions to pre-defined choices [50], using larger models as the evaluator [56; 107]. In the domain of evaluating LLMs, a certain approaches [110; 16] prove their effectiveness in aligning with real-world annotators on the Chatbot Arena [12]. This inspires our efforts in curating in-the-wild small-scale WildVision-Bench, that can support fast evaluation by pair-wise comparison with reference model (such as Claude-3-Sonnet [2]), and achieve alignment with crowdsourced human ratrors on WildVision-Arena.

## 6 Conclusion

We first introduce WildVision-Arena, a dynamic evaluation platform for comparing vision-language models (VLMs) in the wild. We conduct comparative insights across over 20 models by utilizing an extensive dataset of 20,000+ multimodal conversations and 8,000+ votes, allowing for continuous refinement of VLMs performance. From these in-the-wild chats, we then sample safe and diversified data for WildVision-Bench and apply automatic evaluation that closely aligns with crowdsourced human preferences from WildVision-Arena. Our comprehensive analysis on these in-the-wild chats indicates future directions for advancing VLMs.

## References

* [1] 01. AI, ;. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024.
* [2] Anthropic. The claude 3 model family: Opus, sonnet, haiku., 2024.
* [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. _arXiv preprint arXiv:2308.01390_, 2023.
* [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.
* [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.
* [6] Simone Balloccu, Patricia Schmidtova, Mateusz Lango, and Ondrej Dusek. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms, 2024.
* [7] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In _Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization_, pages 65-72, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics.
* [8] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schimdt. Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. _arXiv preprint arXiv:2308.06595_, 2023.
* [9] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952.
* [10] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? _arXiv preprint arXiv:2403.20330_, 2024.
* [11] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_, 2015.
* [12] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024.
* [13] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models, 2023.
* [14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instruct blight: Towards general-purpose vision-language models with instruction tuning. _arXiv preprint arXiv:2305.06500_, 2023.
* [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.
* [16] Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators, 2024.
* [17] Arpad E Elo. The proposed uscf rating system, its development, theory, and applications. _Chess life_, 22(8):242-247, 1967.
* [18] Tianyu Gao, Zirui Wang, Adithya Bhaskar, and Danqi Chen. Improving language understanding from screenshots, 2024.

* [19] Rohit Girdhar, Alaaldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all, 2023.
* chat based ai tool from google, powered by palm 2. _https://bard.google.com/?hl=en_, 2023.
* [21] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017.
* [22] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models, 2023.
* [23] Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, and Bo Zhao. Efficient multimodal learning from data-centric perspective, 2024.
* [24] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: A reference-free evaluation metric for image captioning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7514-7528, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
* [25] Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang. mplug-paperowl: Scientific diagram analysis with the multimodal large language model, 2024.
* [26] Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and Maosong Sun. Large multilingual models pivot zero-shot multimodal learning across languages, 2024.
* [27] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709, 2019.
* [28] IDEFICS. Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model, 2023.
* [29] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning, 2024.
* [30] Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, and He He. Personas as a way to model truthfulness in language models, 2024.
* [31] Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 10697-10707. PMLR, 17-23 Jul 2022.
* [32] Aniruddha Kembhavi, Michael Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. _ArXiv_, abs/1603.07396, 2016.
* [33] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _Proceedings of NAACL-HLT_, pages 4171-4186, 2019.
* [34] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023.
* [35] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation, 2023.
* [36] Gant Laborde. Deep nn for nsfw detection.
* [37] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024.

* [38] Katherine Lee, Daph Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deplotating training data makes language models better. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8424-8445, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* [39] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023.
* [40] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, pages 19730-19742, 2023.
* [41] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pages 12888-12900. PMLR, 2022.
* [42] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language, 2019.
* [43] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXX 16_, pages 121-137. Springer, 2020.
* [44] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.
* [45] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2022.
* [46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.
* [47] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.
* [48] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.
* [49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.
* [50] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024.
* [51] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024.
* [52] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks, 2022.
* [53] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024.
* [54] Yujie Lu, Xiujun Li, Tsu-Jui Fu, Miguel Eckstein, and William Yang Wang. From text to pixel: Advancing long-context understanding in mllms, 2024.
* [55] Yujie Lu, Xiujun Li, William Yang Wang, and Yejin Choi. Vim: Probing multimodal large language models for visual embedded instruction following, 2023.
* [56] Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation, 2023.

* [57] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models, 2023.
* [58] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 11-20, 2016.
* [59] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.
* [60] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pages 2200-2209, 2021.
* [61] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu He, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang. Mm1: Methods, analysis & insights from multimodal llm pre-training, 2024.
* [62] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In _2019 international conference on document analysis and recognition (ICDAR)_, pages 947-952. IEEE, 2019.
* [63] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, Kavya Srinet, Babak Damavandi, and Anuj Kumar. An efficient and scalable any-modality augmented language model, 2023.
* [64] Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D. Griffin. Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities, 2023.
* [65] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought, 2023.
* [66] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. Modeling context between objects for referring expression understanding. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 792-807. Springer, 2016.
* [67] OpenAI. Gpt-4: Technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [68] OpenAI. Gpt-4v(sion) system card. _https://openai.com/research/gpt-4v-system-card_, 2023.
* [69] OpenAI. Gpt-40. _https://openai.com/index/hello-gpt-40_, 2024.
* [70] Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, Ethan Yeo, Eugenie Lamprecht, Qi Liu, Yuqi Wang, Eric Chen, Deyu Fu, Lei Li, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Mikel Artetxe, and Yi Tay. Vibe-eval: A hard evaluation suite for measuring progress of multimodal language models, 2024.
* [71] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _Proceedings of the IEEE international conference on computer vision_, pages 2641-2649, 2015.
* multilingual misinformation and its evolution, 2023.
* [73] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [74] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.

* [75] Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, and Desmond Elliott. Language modelling with pixels, 2023.
* [76] Oscar Sainz, Jon Ander Campos, Iker Garcia-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark, 2023.
* except when they are convincingly wrong, 2024.
* [78] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pages 742-758. Springer, 2020.
* [79] Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. Milebench: Benchmarking mllms in long context, 2024.
* [80] Jinyan Su, Claire Cardie, and Preslav Nakov. Adapting fake news detection to the era of large language models, 2024.
* [81] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024.
* [82] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [83] Reka Team, Aitor Ormazabal, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Alekseiv, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, and Zhihui Xie. Reka core, flash, and edge: A series of powerful multimodal language models, 2024.
* [84] Hugo Touvron, Thibaut Lavril, Gautier Izcard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [85] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [86] UForm. Uform: Pocket-sized multimodal ai for content understanding and generation, 2024.
* [87] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation, 2015.
* [88] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In _International Conference on Machine Learning_, pages 23318-23340. PMLR, 2022.
* [89] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogylm: Visual expert for pretrained language models, 2024.
* [90] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?, 2023.
* [91] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding, 2022.
* [92] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm, 2023.
* [93] XAI. Realworldqa benchmark, 2024.
* [94] Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. Recipes for safety in open-domain chatbots, 2021.

* [95] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. LvIm-ehub: A comprehensive evaluation benchmark for large vision-language models, 2023.
* [96] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images, 2024.
* [97] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan Wang. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation, 2023.
* [98] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. _High-Confidence Computing_, 4(2):100211, June 2024.
* [99] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. Agent lumos: Unified and modular training for open-source language agents, 2024.
* [100] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14_, pages 69-85. Springer, 2016.
* [101] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. _arXiv preprint arXiv:2308.02490_, 2023.
* [102] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _arXiv preprint arXiv:2311.16502_, 2023.
* [103] Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang, Yang Zhang, and Dong Wang. Evidence-driven retrieval augmented response generation for online misinformation, 2024.
* [104] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Apparent: Multimodal agents as smartphone users, 2023.
* [105] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding, 2023.
* [106] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: A survey, 2024.
* [107] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold. Gpt-4v(ision) as a generalist evaluator for vision-language tasks, 2023.
* [108] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding, 2024.
* [109] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: A strong zero-shot video understanding model, April 2024.
* [110] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
* [111] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: A framework of small-scale large multimodal models, 2024.
* [112] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, and Li Yuan. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment, 2024.
* [113] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.

[MISSING_PAGE_EMPTY:16]

User Interface

In Figure 8, we show a screenshot of the user interface of our WildVision-Arena, which presents an interactive environment for evaluating multimodal large language models. This environment allows users to input questions and compare responses from multiple models simultaneously. Each model's answer is displayed side-by-side, enabling a straightforward comparison of their performance and capabilities based on user queries related to specific images or tasks. The interface also facilitates easy selection and voting to decide which model's response fits the user's criteria best, enhancing the user's ability to judge and refine the models' outputs effectively.

Figure 8: User Interface of WildVision-Arena.

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_FAIL:20]

Analysis of Failure Cases

We observe some common failure patterns of VLMs in the wild from WildVision-Arena chat data. In Tables 9-13, we present specific failure cases. Based on the types of errors, we have condensed six categories, detailed in the following paragraphs.

Visual RecognitionFailures in this category involve several types of recognition challenges. Complex text, such as artistic Chinese characters (Error #5 in Table 10), and small details, such as the text on a shop sign in a restaurant scene (Error _#3_ in Table 9), often elude accurate detection. Errors also occur with small text in screenshots (Error _#18_ in Table 13), recognizing objects against novel backgrounds (Error _#9_ in Table 11), and identifying new objects within known contexts, like a helmet next to a motorcycle (Error _#11_ in Table 11). Failures in interpreting charts (Error _#4_ in Table 9) and identifying reflections (Error _#20_ in Table 13) are also noted.

Visual ReasoningThis category covers the ability to discern visual differences and reason through visual data. Errors include overlooking changes in clothes of the character (Error _#6_ in Table 10), strategizing in chess (Error _#1_ in Table 9), and reading analog clocks (Error _#16_ in Table 11).

Spatial ImaginationChallenges in this area involve imagining the outcomes of spatial transformations. Failures are evident in cube folding tasks (Error _#10_ in Table 11) and visual riddles involving the folding of objects like magazines (Error _#12_ in Table 11).

Contextual UnderstandingErrors in this category relate to recognizing and understanding contexts specific to certain domains, such as movies (Error _#7_ in Table 10) and video games (Error _#8_ in Table 10). Failures also include responding inappropriately to marked images (Error _#17_ in Table 13).

Expert Domain KnowledgeThis involves specialized knowledge areas where VLMs fail to provide accurate information or analysis, such as verifying architectural compliance (Error _#2_ in Table 9) and analyzing programming code details (Error _#15_ in Table 11).

HallucinationErrors of hallucination occur when models generate incorrect or misleading content based on subtle cues or nonexistent details, such as fictitious characters (Error _#13_ in Table 12) or obscure brand names (Error _#18_ in Table 13).

SafetyIssues in this category are critical as they involve handling harmful, biased, or inappropriate content, and dealing with NSFW images in ways that either excessively censor or insufficiently filter content. These images are not presented and are filtered out in benchmark curation.

**GPT-4V:** This chess position is unconventional and humorous because it features a board filled with knights (horses) except for a single queen. In a standard game of chess, this situation would not occur, as each player starts with just two knights. In this specific position, there's not much strategy to discuss because the game doesn't reflect an actual playable scenario. However, if we want to engage with this as a fun exercise in imagination, you could consider moving one of the knights that isn't pinned or creating a threat to the queen. For example, you could move one of the knights on the third rank to attack the queen, such as Nc3-d5 or Nc3-c4. Remember, this is just a playful scenario and not representative of a legitimate chess position. In a real game, the best move would depend on a much broader context of the board and the strategic positions of all the pieces, not just an army of knights.

**Text Prompt:** What food will these people have very soon?

**LLaVA-NEXT-34B:** Based on the image, the people are lined up outside a building with a sign that reads "Salsa's Authentic Mexican Restaurant." It seems likely that they will be having Mexican food very soon, which could include items such as tacos, burritos, enchiladas, chiles rellenos, guacamole, and other similar dishes that are typically served at Mexican restaurants. The specific menu items will depend on the restaurant's offerings and the preferences of the customers.

**GPT-4V:** I'm sorry, but as an AI, I do not have the capacity to verify the compliance of architectural plans with specific building codes. Building code compliance typically involves a wide range of considerations, including but not limited to accessibility, fire safety, structural integrity, and mechanical systems. Professional architects and engineers, often alongside local code enforcement officials, ensure that designs and construction meet the appropriate codes and regulations. To determine compliance with the Ontario Building Code, the plans would need to be reviewed by a certified professional who has access to and understands the local regulations and requirements. They would check if the design meets all necessary stipulations for things such as room sizes, egress requirements, bathroom accessibility, fire ratings, and many other details. You should consult with a licensed Ontario architect or a building code consultant for an official assessment of compliance.

**LLaVA-NEXT-34B:** The MOHS hardness of Marble, as indicated on the chart, is 3.

\begin{table}
\begin{tabular}{p{284.5pt}}
**Text Prompt:** What food will these people have very soon? \\
**LLaVA-NEXT-34B:** Based on the image, the people are lined up outside a building with a sign that reads “Salsa’s Authentic Mexican Restaurant." It seems likely that they will be having Mexican food very soon, which could include items such as tacos, burritos, enchiladas, chiles rellenos, guacamole, and other similar dishes that are typically served at Mexican restaurants. The specific menu items will depend on the restaurant’s offerings and the preferences of the customers. \\ \end{tabular}
\end{table}
Table 9: Failure Cases.

[MISSING_PAGE_FAIL:23]

**GPT-4V:** The keyboard in the image features a non-standard layout, which appears to be a compact form factor lacking the numeric keypad commonly found on full-sized variants. Without being able to view the entire keyboard, it's impossible to provide an exact count of the keys...

**Text Prompt:** what is this

**GPT-4V:** This is an oversized, artistic representation of a classic hand-held manual egg beater or whisk. It's designed to mimic the appearance of a traditional egg beater with an exaggerated size, likely for decorative or exhibit purposes as suggested by the setting that appears to be a show or exhibition hall. The inclusion of this piece in a public space may indicate it being part of an art installation, event display, or perhaps a promotion related to culinary equipment or innovation.

**Text Prompt:** By examining the unfolded cube (net) at the top of the image, which one of the four cubes labeled A, B, C, and D can be created by folding this net along the edges?

**Gemini-Pro-Vision:** The correct answer is C. The unfolded cube (net) at the top of the image has two red squares and three orange circles. Cube C has two red squares and three orange circles in the same arrangement as the unfolded cube.

**Text Prompt:** Try to guess the solution to this Fold-in from Mad magazine. A Mad Fold-in is a visual riddle that a human can easily solve by physically folding the page as explained in the instructions in the top-right corner. I know the solution. I want to verify if you are able to solve it. What big car feature will soon be included in small economy cars?

**LLaVA-NEXT-3dB:** The Mad Fold-in features a car salesman in front of a sign that says "CONSERVE ENERGY DRIVE A SMALL CAR" with various "big car" features like a tire iron, a spare tire, a picnic basket, and a trunk full of items. When you fold the page as directed, it's revealed that the large "big car" features are actually inside the small car, showing that the small car also has these items, humorously implying that the small car will soon include all the features typically associated with big cars. The joke is that despite their size, small cars will also include all the amenities that were once exclusive to larger vehicles.

\begin{table}
\begin{tabular}{c c} _Image_ & Error _\#11_ \\ \end{tabular}
\end{table}
Table 11: Failure Cases.

**Text Prompt:** What is the name of the character with the sunglasses?

**GPT-4V:** You're looking at an image from the animated film Zootopia.The character with the sunglasses is named Nick Wilde; he's a red fox who is one of the main characters in the movie.

**Text Prompt:** Are the workers wearing safety devices?

**LLaVA-NEXT-34B:** In the image you've provided, the workers are wearing safety vests, which are commonly used in construction and industrial environments to increase visibility and reduce the risk of accidents. These vests are a standard safety device to help workers be more visible to operators of heavy machinery and other workers in bright colors that contrast with the surroundings. The workers also appear to be wearing hard hats, which are required on construction sites to protect the head from potential injuries.

_Image_ Error #16

**Text Prompt:** What time is it on that watch?

**Gemini-Pro-Vision:** This is a code snippet in Python. It seems to be a function that preprocesses an image for object detection. The function takes an image as input and returns a preprocessed image and a list of bounding boxes.

**LLaVA-NEXT-34B:** The watch in the image is displaying a time that appears to be 10:11. The hour hand is pointing to the number 10, and the minute hand is pointing to the number 11.

\begin{table}
\begin{tabular}{p{142.3pt} p{142.3pt} p{142.3pt}}
**Text Prompt:** What do you see in the image?

**Gemini-Pro-Vision:** This is a code snippet in Python. It seems to be a function that preprocesses an image for object detection. The function takes an image as input and returns a preprocessed image and a list of bounding boxes.

\begin{tabular}{p{142.3pt} p{142.3pt}}
**Text Prompt:** What time is it on that watch?

**LLaVA-NEXT-34B:** The watch in the image is displaying a time that appears to be 10:11. The hour hand is pointing to the number 10, and the minute hand is pointing to the number 11.

\end{tabular}
\end{table}
Table 12: Failure Cases.

[MISSING_PAGE_EMPTY:26]

Data Analysis

Position and Length Biased Human PreferencesHuman preferences on models are known to be biased to the length of model responses. Previous work [12] show not strong correlation found between length and rank at model-wisely. To further understand each voting point in WildVision-Arena. We plot Figure 9 to show the winner distribution over token length. The Blue line represents the density of Model B wins over Model A, and the x axis be the token length difference which substract length of Model B from Model A. This plot help decouple the cofounders of position bias and length bias. And when the token length difference is more obvious, the preferences are slightly biased to length output model. And this effect is marginal when both model have long output.

Battles TrendIn Figure 10, we plot the number of votes per day with a date cutoff at May 29, 2024. On average, WildVision-Arena got \(71\) votes from the users per day.

Model ChatsWe visualize number of conversations per model in Figure 11.

Figure 11: Number of Chats Per Model

Figure 10: Number of Votes Per Day

Figure 9: Winner Density Plot

Prompt Template

### Taxonomy Annotation

In Table E.1, we show the template fed into GPT-4v to annotate the question category and image domain for each data sample in WildVision-Arena as in Figure 2 and Figure 3.

[Image] <image>

[Question] What is the color of the main object in the image?

[System] Given the image and following text question, please classify the content according to the specified taxonomy:

_Question Categories_:

Descriptive - General Description (Provide a broad overview of what the image contains.)... Recognition - Object Recognition (What objects are present in the image?)... Instructive - How-to Guides (How do I obtain what's depicted in the image?)... Analytical - Data Analysis (Analyze the data presented in the image.)... Comprehensive - Cultural Analysis (Analyze the cultural significance of the image.)... Interactive - Bug Fixing (Fix the bug in the code depicted in the image.) Creative - Music and Composition (Compose a song inspired by the image.)

_Image Domains_:

Urban - Cityscapes, Infrastructure, Public Spaces, Buildings, Transportation, Street Scenes People - Portraits, Crowds, Faces, Selfies, Group Photos Event - Cultural Events, Historical Events, Social Gatherings, Performances, Sports, Fashion, Lifestyle Objects - Accessory, Vehicles, Sports Equipment, Kitchenware, Food, Furniture, Electronics, Appliances, Household Tools, Musical Instruments, Art Supplies, Office Supplies Entertainment - Games, Movies and TV Shows, Media and Communication, Web and Mobile Apps Screenshots Expert - Art and Design, Business, Science, Health and Medicine, Humanities and Social Science, Tech and Engineering Please analyze the text and image provided and classify them into the appropriate category and subcategory, as well as the main image domain and subdomain, based on the taxonomy above. Please only reply with four values 1. question category, 2. question subcategory, 3. image domain, 4. image subdomain) in a string separated by [&]. For example, "Descriptive[&]Object Description[&]Natural[&]Landscapes".

[Output] Analytical[&]Attribute-based Question Answer[&]Objects[&]Furniture

### VLM Voting

In Table E.2, we show the template used to generate the pairwise preference by utilizing GPT-4V as a local evaluator 3.2.

[frame=single,width=0.5cm]
**[Image]** <image>
**[Question]** What is the color of the main object in the image?
**[Model Assistant A's Response]** Blue.
**[Model Assistant B's Response]** Red.
**[System]** Please act as an impartial judge and evaluate the quality of the responses provided by two model assistants to the user question displayed in [Question]. You should choose the assistant that follows the user's instructions and answers the user's questions better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Avoid any positional biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. Reply with "leftvote" if you find assistant A better, "rightvote" if assistant B is better, "bothbad_vote" if both responses are wrong, and "tievote" if both assistants provide equally satisfactory answers. If you are unable to make a decision, please reply with "NA".
**[Evaluator Output]** leftvote

[MISSING_PAGE_FAIL:30]

Discussions

### Limitations

Although our platform integrates a variety of multimodal models for convenient comparison, it inevitably omits some recently released models, potentially limiting the breadth of insights available. Additionally, the platform's stress testing is inadequate; scaling up is imperative to handle the increasing volume of user queries each day. There is also a critical need to balance the protection of third-party models with ensuring that model responses remain unbiased and true to their design. Despite logging data for research purposes and informing users accordingly, ongoing efforts are required to enhance system security to prevent data leaks.

### Societal Impact

WildVision Arena serves as a dynamic benchmarking tool, embracing crowd-sourced input from a diverse range of users. However, biases persist, particularly among English-speaking users--a reflection of some models' linguistic limitations--and among those with a specific interest in multimodal research. Efforts are underway to refine the interface, aiming to broaden participation and reduce existing biases. By enhancing accessibility and user engagement, we strive to create a more inclusive platform that better represents global perspectives.

## Appendix G Accessiblity of Datasets

### Dataset Documentation and Intended Uses

To interact with models and submit votes, visit Hugging Face Vision Arena1. To view the live leaderboard, navigate to the leaderboard tab on the same page. Data can be accessed for downloading and viewing at WildVision Arena Data2.

Footnote 1: https://huggingface.co/spaces/WildVision/vision-arena

Footnote 2: https://huggingface.co/datasets/WildVision/wildvision-arena-data

### Maintenance Plan

The live leaderboard of WildVision-Arena is updated every three hours. The data will be continually updated at WildVision on Hugging Face3. The code for the platform will be open-sourced at WildVision-Bench Github repo4 and welcome community effort. The voting data and code for the evaluation will be provided to facilitate easy reproduction of the leaderboard.

Footnote 3: https://huggingface.co/WildVision

### Author Statement

We confirm that we bear all responsibility in case of violation of rights during the collection of data on WildVision-Arena and WildVision-Bench. We will take appropriate action when needed.

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Section??.
* Did you include the license to the code and datasets? [No] The code and the data are proprietary.
* Did you include the license to the code and datasets? [N/A]Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] Appendix 3. Did you discuss any potential negative societal impacts of your work? [Yes] Appendix 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] Appendix 2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [Yes] Section 3, 4 2. Did you include complete proofs of all theoretical results? [Yes] Section 3, 4. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? first page 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? Section 3,4 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Section 3,4 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? Section 3,4
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? Section 3,4 2. Did you mention the license of the assets? Section 4 3. Did you include any new assets either in the supplemental material or as a URL? Section 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? Section 4. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? Appendix 5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? User interface 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? The data annotation part of the project is classified as exempt by Human Subject Committee via IRB protocols. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? Appendix 6.