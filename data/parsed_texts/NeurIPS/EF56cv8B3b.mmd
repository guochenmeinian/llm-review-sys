# LayoutPromoter: Awaken the Design Ability of

Large Language Models

 Jiawei Lin

Xi'an Jiaotong University

kylelin@stu.xjtu.edu.cn &Jiaqi Guo

Microsoft

jiaqiguo@microsoft.com &Shizhao Sun

Microsoft

shizsu@microsoft.com &Zijiang James Yang

Xi'an Jiaotong University

zijiang@xjtu.edu.cn &Jian-Guang Lou

Microsoft

jou@microsoft.com &Dongmei Zhang

Microsoft

dongmeiz@microsoft.com

Work done during an internship at Microsoft Research Asia.

###### Abstract

Conditional graphic layout generation, which automatically maps user constraints to high-quality layouts, has attracted widespread attention today. Although recent works have achieved promising performance, the lack of _versatility_ and _data efficiency_ hinders their practical applications. In this work, we propose LayoutPromoter, which leverages large language models (LLMs) to address the above problems through in-context learning. LayoutPromoter is made up of three key components, namely input-output serialization, dynamic exemplar selection and layout ranking. Specifically, the input-output serialization component meticulously designs the input and output formats for each layout generation task. Dynamic exemplar selection is responsible for selecting the most helpful prompting exemplars for a given input. And a layout ranker is used to pick the highest quality layout from multiple outputs of LLMs. We conduct experiments on all existing layout generation tasks using four public datasets. Despite the simplicity of our approach, experimental results show that LayoutPromoter can compete with or even outperform state-of-the-art approaches on these tasks without any model training or fine-tuning. This demonstrates the effectiveness of this versatile and training-free approach. In addition, the ablation studies show that LayoutPromoter is significantly superior to the training-based baseline in a low-data regime, further indicating the data efficiency of LayoutPromoter. Our project is available here.

## 1 Introduction

_Layout_, which consists of a set of well-arranged graphic elements, plays a critical role in graphic design. To alleviate the workload of designers and allow non-expert users to engage in the design process, numerous studies have delved into the automatic layout generation for diverse user needs [7, 15, 18, 19, 21, 22, 39] (i.e., layout constraints). Based on input layout constraints, existing conditional layout generation tasks can be categorized into the following groups: _constraint-explicit layout generation_ (e.g., generating layouts conditioned on element types), _content-aware layout generation_, and _text-to-layout_ (see the left side of Figure 1 for constraint examples). Early works in this field [7, 19, 21, 22] primarily focus on individual tasks and develop task-specific model architectures and optimization methods. More recently, task-generic approaches [15, 12, 14] have emerged. Compared to task-specific methods, they achieve greater flexibility and controllability on more tasks, while maintaining the quality of the generated layouts.

Although state-of-the-art methods [15; 12; 14; 9; 24] have achieved promising results, they still suffer from some limitations that impede their applications in real-world scenarios. First, _the previous approaches struggle to simultaneously cope with all the layout generation tasks depicted in Figure 1_. They are typically tailored for specific tasks and cannot be applied to others. For instance, the state-of-the-art diffusion-based model LayoutDM [14] proposes to inject _explicit_ layout constraints through masking or logit adjustment during inference, but it fails to do so for implicit or vague constraints, e.g., constraints expressed in natural language (i.e., text-to-layout). Consequently, distinct models need to be deployed for different tasks, leading to inconvenience. This motivates us to explore a more versatile approach for layout generation. Second, _the existing methods are not data-efficient either_. They usually necessitate extensive constraint-layout pair data for model training. For example, LayoutFormer++ [15] relies on the publynet dataset [40] with a size of 300K for training to generate aesthetically pleasing document layouts. However, collecting such large datasets for some low-resource layout domains (e.g., poster layouts) is prohibitively expensive. Besides, even if such a dataset is available, training is time-consuming and costly. Hence, there is a pressing need to develop a data-efficient layout generation method.

In this work, we consider leveraging the powerful pre-trained large language models (LLMs) to address the above problems. The intuition behind is as follows. First, recent research has shown the versatility of LLMs in various tasks [28; 13; 1; 38]. By carefully designing input-output formats, these tasks can be converted into sequence-to-sequence generation problems and effectively addressed by LLMs. This emerging trend inspires us to utilize LLMs to tackle all conditional layout generation tasks in a unified manner. Second, since the training corpus contains layout source code [28; 4] (e.g., HTML code and XML code), LLMs have acquired some layout-related knowledge during pre-training. For example, they may inherently possess the ability to align graphic elements and avoid unnecessary overlap between them, which is beneficial for producing high-quality and visually appealing layouts. Consequently, an LLMs-based approach holds promise to enhance data efficiency compared to existing models that are trained from scratch. Third, an additional advantage of LLMs lies in their remarkable in-context learning performance [3; 28; 36; 35; 13]. It means that instead of fine-tuning LLMs individually for each layout generation task, we can simply prompt them to perform the desired task with a few input-output demonstrations. This characteristic further allows LLMs to generate layouts in a training-free manner without any parameter updates.

To this end, we propose _LayoutPrompter_ (see Figure 1). It formulates all conditional layout generation tasks as sequence-to-sequence transformation problems and leverages LLMs to tackle them through in-context learning. To unleash the full potential of LLMs for layout generation, two key issues need to be addressed. First, how to awaken the layout-related knowledge in LLMs for achieving decent performance? Second, how to facilitate LLMs understanding diverse user constraints and layout characteristics in distinct domains? LayoutPrompter tackles the two issues with the _input-output serialization_ module and the _dynamic exemplar selection_ module, respectively. **I. Input-Output Serialization.** Since prevalent LLMs can only read token sequences, this module is responsible for representing user constraints and layouts as sequences so that LLMs can su

Figure 1: LayoutPrompter is a versatile method for graphic layout generation, capable of solving various conditional layout generation tasks (as illustrated on the left side) across a range of layout domains (as illustrated on the right side) without any model training or fine-tuning.

related knowledge. To represent input layout constraints as sequences, we borrow the successful experience of LayoutFormer++ [15], where they present two simple but effective principles (i.e., constraint representation and constraint combination) to serialize constraints. We experimentally find that the serialization scheme is also effective for LLMs. To represent layouts as sequences, our principle is to convert them into a format resembling what LLMs have encountered during pre-training, thereby leveraging the existing layout-related knowledge within LLMs. Specifically, we serialize the layout into the corresponding source code (e.g., HTML) to obtain the output sequence. **II. Dynamic Exemplar Selection.** This module is used to select prompting exemplars that have similar layout constraints to the test samples. In contrast to random exemplars, dynamic exemplars ensure that LLMs receive the most relevant context, so they can better comprehend the desired constraints and produce plausible layouts accordingly. To support this technique, we develop an evaluation suite to measure the constraint similarities between a given test sample and all candidate exemplars from the training set. Then, we select those with the highest similarity scores as prompting exemplars. In addition, we introduce a layout ranker to further improve LayoutPrompter's performance. Considering that LLMs can produce distinct outputs through sampling, we generate multiple layouts with the same input constraints, and use the ranker to select the highest-quality one as the final output.

We conduct extensive experiments on various tasks and layout domains to evaluate LayoutPrompter. Experimental results show that LayoutPrompter can tackle all existing conditional layout generation tasks, demonstrating its versatility. Despite without any model training or fine-tuning, LayoutPrompter is on par or even better than the state-of-the-art approaches. Besides, our ablation studies exhibit that LayoutPrompter can still achieve good performance when there is only a small set of candidate exemplars, indicating that it is superior to existing training-based methods in terms of data efficiency. In summary, LayoutPrompter is a versatile, data-efficient and training-free layout generation method.

## 2 Related Work

**Graphic Layout Generation.** Automatic graphic layout generation is an emerging research topic in recent years. To meet diverse user requirements, existing methods have defined various layout generation tasks, including layout generation conditioned on element types [21; 19; 18], layout generation conditioned on element types and sizes [19], layout generation conditioned on element relationships [18; 21], layout completion [7; 23] and refinement [30]. In addition to these constraint-explicit tasks, some works consider more challenging but useful tasks, such as content-aware layout generation [39; 9] and text-to-layout [11; 24]. Content-aware layout generation aims at arranging spatial space for pre-defined elements on a given canvas. The generated layouts not only need to be visually pleasing, but also avoid salient areas of the canvas. Text-to-layout is to generate layouts according to human language descriptions.

Early works in this field primarily focus on an individual task and propose task-specific approaches based on Generative Adversarial Networks (GANs) [22; 9], Variational Autoencoders (VAEs) [21; 16] and Transformers [19; 7; 11; 24; 30]. Recently, some general approaches [15; 12; 14; 37] have appeared. LayoutFormer++ [15] proposes to represent various constraints as sequences and then leverages a Transformer [32] encoder-decoder architecture to generate layouts from constraint sequences. [12; 14] develop diffusion-based models for constraint-explicit layout generation. However, none of the existing methods can simultaneously handle all layout generation tasks. Furthermore, these methods are highly dependent on large amounts of training data, which hinders their practical applications. In this work, we introduce techniques such as dynamic exemplar selection, input-output serialization and layout ranking to effectively utilize LLMs to overcome the above limitations, making LayoutPrompter a versatile and data-efficient approach (see Table 1).

**Large Language Models.** Large language models (LLMs) with billions of parameters, such as GPT [3; 28], PaLM [6] and LLMa [31], have demonstrated excellent f

\begin{table}
\begin{tabular}{l c c c} \hline \hline Methods & Versatile & Data-Efficient & Training-Free \\ \hline LayoutTransformer [7], BLT [19], and so on [9; 24; 16; 30] & ✗ & ✗ & ✗ \\ LayoutFormer++ [15], LayoutDM [14], LGDM [12] & _partially_ & ✗ & ✗ \\ LayoutPrompter (ours) & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: A comparison between existing conditional layout generation methods and LayoutPrompter.

various natural language processing (NLP) tasks. Thanks to the emergent ability [33] brought by the scale of model and data, they largely outperform prior supervised approaches and even match human-level performance on some tasks, without any finetuning. The versatility and effectiveness of LLMs inspire us to develop a layout generation method based on them.

Recent studies show that the prompting strategy plays a crucial role in model performance. For example, chain-of-thought (CoT) prompting [34] is proposed to improve the reasoning ability of LLMs by incorporating intermediate reasoning steps in the exemplars. Least-to-most prompting [41; 17] (also known as decomposed prompting) is introduced to solve complex multi-step reasoning tasks. To enhance contextual knowledge, [26; 10] use a retrieval module to dynamically select in-context exemplars. They experimentally find that exemplars semantically similar to test samples can better unleash the model's knowledge. Specifically, they use a sentence encoder to convert model inputs to vector representations. Then, for each test sample, they retrieve the nearest neighbors in the encoded sentence embedding space to construct prompts. Motivated by them, we propose a similar prompting strategy in this work. Since the input of layout generation tasks is different from prior works, we introduce a customized evaluation suite to measure sample distances. Experimental results demonstrate its effectiveness in LayoutPromter.

## 3 LayoutPromter

In this section, we elaborate on LayoutPromter, a versatile, data-efficient and training-free layout generation method built upon LLMs. Our main contribution lies in proposing a set of useful techniques for applying LLMs to layout generation. Specifically, to support sequence-to-sequence transformation and make maximum use of the design knowledge within LLMs, we carefully consider the serialization scheme that represents task inputs and outputs as sequences (Section 3.2). Moreover, to enhance the comprehension of user-specified layout constraints, we propose a dynamic exemplar selection module to retrieve the most relevant exemplars from the training set to perform in-context learning (Section 3.3). Besides, a layout ranker is designed to evaluate layout quality and rank multiple layouts generated under the same constraints, further improving model performance (Section 3.4).

### Overview

Let's consider a conditional layout generation task. We denote its training set as \(\mathcal{D}=\{(x_{j},y_{j})\}_{j=1}^{M}\). Here, \((x_{j},y_{j})\) represents the \(j\)-th sample of \(\mathcal{D}\), which is an (input constraint, output layout) pair, and \(M\) is the total number of samples. As illustrated in Figure 2, for a given test query \(x_{\text{test}}\), the in-context learning prompt \(\mathbf{P}\) is composed by sequentially concatenating a task-specific preamble \(R\)

Figure 2: An overview of LayoutPromter. The complete prompt consists of a task-specific preamble, \(N\) in-context exemplars and a test input. The exemplars are dynamically retrieved from the training set according to the test input. Subsequently, the prompt is fed into an LLM to generate \(L\) distinct layouts. We employ a layout ranker to select the best one as the final output.

\(N\) exemplars and the query itself:

\[\mathbf{P}=[R;F_{X}(x_{k_{1}});F_{Y}(y_{k_{1}});\ldots;F_{X}(x_{k_{N}});F_{Y}(y_{ k_{N}});F_{X}(x_{\text{test}})],\quad\{k_{i}\}_{i=1}^{N}=G(x_{\text{test}},\mathcal{D}).\] (1)

To be more specific, the preamble \(R\) provides the essential information about the target task, such as the _task description_, _layout domain_ and _canvas size_. \(F_{X}(\cdot)\) and \(F_{Y}(\cdot)\) are serialization functions that transform task input \(x\) and output \(y\) into sequences, respectively. \(G(\cdot,\cdot)\) denotes an exemplar selection function, which retrieves the in-context exemplars from \(\mathcal{D}\) according to \(x_{\text{test}}\). The details of \(F_{X}\), \(F_{Y}\) and \(G\) will be elaborated in the following sections.

Notably, when the number of exemplars \(N\) is set to \(0\), few-shot in-context learning degenerates to zero-shot learning, where LLMs predict the test output \(y_{\text{test}}\) solely based on the preamble \(R\) and \(x_{\text{test}}\). In our experiments (see Section B.2 in Appendix), we find that additional exemplar guidance can help LLMs better comprehend the task and grasp the rough pattern of the required layouts. Hence, we opt for few-shot learning (\(N>0\)) instead of zero-shot learning (\(N=0\)) in this work.

### Input-Output Serialization

To begin, we first establish some notations. For each element \(e\) that constitutes a layout, we describe it by its element type \(c\), left coordinate \(l\), top coordinate \(t\), width \(w\) and height \(h\), i.e., \(e=(c,l,t,w,h)\). Here, \(c\) is a categorical attribute. The other four are numerical geometric attributes, which will be discretized in the implementation (see Section A in Appendix).

**Input Constraint Serialization.** For constraint-explicit layout generation, the input constraints are element-wise constraints on \(e\). We serialize such constraints in the same way as LayoutFormer++ [15], where they represent each constraint as a sequence and then combine different constraints through concatenation. For example, if \(x\) specifies the element types and sizes, \(F_{X}(x)\) takes the form of \(F_{X}(x)=\text{"}c_{1}w_{1}h_{1}|c_{2}w_{2}h_{2}|\ldots\text{"}\). In this work, we adopt these ready-made sequences for constraint-explicit layout generation tasks. Regarding content-aware layout generation, the image nature of the input canvas poses a unique challenge for serialization, i.e., enabling LLMs that can only read text to perceive image content. Inspired by DS-GAN [9], we recognize that the saliency map [8] can well capture the key content shape of a canvas while discarding other high-frequency, irrelevant details (see Figure 3). To facilitate serialization, we further convert it into a rectified saliency map \(m=(l_{m},t_{m},w_{m},h_{m})\) by detecting region boundaries with pixel values greater than a certain threshold. After preprocessing, the input canvas \(x\) can be represented in a format understandable by LLMs: \(F_{X}(x)=\text{"Content Constraint: left }l_{m}\text{px,top }t_{m}\text{px,width }w_{m}\text{px,height }h_{m}\text{px}\)". For the text-to-layout task, where natural language descriptions are used to generate layouts, the constraint sequence is simply the input text itself.

**Output Layout Serialization.** For the output \(y\), we propose to serialize it into the HTML format that LLMs are more familiar with and good at, rather than the plain sequence used in prior works [15; 7]. Following common HTML representation, we denote the complete output sequence as a concatenation of multiple HTML segments \(a\): \(F_{Y}(y)=[a_{1};a_{2};\ldots]\). Here, the \(i\)-th segment \(a_{i}\) represents the \(i\)-th graphic element \(e_{i}\) of \(y\). It specifies the element attributes in the following format:

\[\text{<div class="}c_{i}\text{" style="left:}l_{i}\text{px; top:}t_{i}\text{px; width:}w_{i}\text{px; height:}h_{i}\text{px}^{n}\text{>}\text{</div>}.\] (2)

Thanks to the powerful in-context learning ability of LLMs, the test output \(y_{\text{test}}\) will be predicted in the same HTML format, making it easy to extract the required element attributes from the output. More input-output examples can be found in Section D of the supplementary material.

### Dynamic Exemplar Selection

As mentioned above, \(G\) selects \(N\) in-context exemplars that have the most similar layout constraints to \(x_{\text{test}}\) from \(\mathcal{D}\). The selected exemplars are randomly shuffled and combined to construct \(\mathbf{P}\) (see Equation 1), thereby enhancing LLMs' understanding of various constraints. To achieve this, we design an evaluation suite \(s\) to measure the constraint similarity between the test query \(x_{\text{test}}\) and each

Figure 3: An input canvas is converted into a saliency map.

candidate exemplar \((x_{j},y_{j})\in\mathcal{D}\). Then, \(G\) can be further expressed as a Top-k selection function:

\[G(x_{\text{test}},\mathcal{D})\triangleq\text{Top-k}(\bigcup_{(x_{j},y_{j})\in \mathcal{D}}\{s(x_{\text{test}},x_{j})\},N).\] (3)

Since we divide existing layout generation tasks into three categories, each with distinct input constraints, their similarity measures have different representations. We'll elaborate below.

**Constraint-Explicit Layout Generation.** As constraint-explicit layout generation tasks only consider element-wise constraints, we define \(s(x_{\text{test}},x_{j})\) using inter-element constraint similarities. Specifically, we construct a bipartite graph between \(x_{\text{test}}=\{p_{\text{test}}^{u}\}_{u=1}^{U}\) and \(x_{j}=\{p_{j}^{v}\}_{v=1}^{v}\), where \(p\) denotes the element-wise constraint on \(e\). \(U,V\) are the constraint numbers of \(x_{\text{test}},x_{j}\). Then, the inter-element similarity \(W\) (i.e., the weight of bipartite graph) and the overall constraint similarity \(s\) are defined as:

\[s(x_{\text{test}},x_{j})\triangleq\frac{1}{|\mathbb{M}_{\text{max}}|}\sum_{(p _{\text{test}}^{u},p_{j}^{v})\in\mathbb{M}_{\text{max}}}W(p_{\text{test}}^{u},p_{j}^{v}),\quad W(p_{\text{test}}^{u},p_{j}^{v})=\mathds{1}(p_{\text{test}}^ {u},p_{j}^{v})2^{-\|\mathbf{g}_{\text{test}}^{u}-\mathbf{g}_{j}^{v}\|_{2}}.\] (4)

Here, \(\mathds{1}\) is a 0-1 function equal to 1 if \(p_{\text{test}}^{u}\) and \(p_{j}^{v}\) specify the same element type, and 0 otherwise. This ensures that constraint similarity is only considered between elements with the same type. \(\mathbf{g}_{\text{test}}^{u}\) and \(\mathbf{g}_{j}^{v}\) are specified geometric attributes of \(p_{\text{test}}^{u}\) and \(p_{j}^{v}\). Given the edge weight \(W\) of the bipartite graph, we adopt Hungarian method [20] to obtain the maximum matching \(\mathbb{M}_{\text{max}}\). And \(s(x_{\text{test}},x_{j})\) is calculated as the average weight of matched edges (as shown in Equation 4).

**Content-Aware Layout Generation.** The constraint of content-aware layout generation is the input canvas. The similarity of two canvases \(x_{\text{test}},x_{j}\) is defined as the IoU (Intersection over Union) of their rectified saliency maps (see Section 3.2) \(m_{\text{test}},m_{j}\):

\[s(x_{\text{test}},x_{j})\triangleq\text{IoU}(m_{\text{test}},m_{j})=\frac{|m_ {\text{test}}\cap m_{j}|}{|m_{\text{test}}\cup m_{j}|}.\] (5)

**Text-to-Layout.** We leverage the CLIP [29] text encoder to encode input texts into embeddings. The constraint similarity \(s(x_{\text{test}},x_{j})\) is defined as the cosine similarity of input text embeddings \(n_{\text{test}},n_{j}\):

\[s(x_{\text{test}},x_{j})\triangleq\frac{n_{\text{test}}\cdot n_{j}}{\|n_{ \text{test}}|\|n_{j}\|}.\] (6)

### Layout Ranker

People usually judge the quality of generated layouts from two perspectives: (1) whether they are visually pleasing; (2) whether they look like the real layouts. Therefore, our proposed layout ranker follows the same principles to evaluate layout quality. To be more specific, it measures the quality of an output layout using a combination of metrics:

\[q(y_{\text{test}})=\lambda_{1}\text{Alignment}(y_{\text{test}})+\lambda_{2} \text{Overlap}(y_{\text{test}})+\lambda_{3}(1-\text{mIoU}(y_{\text{test}})).\] (7)

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline  & & & \multicolumn{6}{c}{RICO} & \multicolumn{6}{c}{PubLayNet} \\ \cline{4-11} Tasks & Methods & mIoU \(\uparrow\) & FID \(\downarrow\) & Align. \(\downarrow\) & Overlap \(\downarrow\) & Vio. \(\downarrow\) & mIoU \(\uparrow\) & FID \(\downarrow\) & Align. \(\downarrow\) & Overlap \(\downarrow\) & Vio. \(\downarrow\) \\ \hline \multirow{3}{*}{Gen-T} & BLT & 0.216 & 25.633 & 0.150 & 0.983 & - & 0.140 & 38.684 & 0.036 & 0.196 & - \\  & LayoutForm++ & 0.432 & 1.096 & 0.230 & 0.530 & 0.3 & 0.348 & 8.411 & 0.020 & 0.008 & 0. \\  & LayoutFormer+ & 0.429 & 3.233 & **0.109** & **0.505** & 0.64 & **0.382** & **3.022** & 0.037 & 0.047 & 0.50 \\ \hline \multirow{3}{*}{Gen-TS} & BLT & 0.604 & 0.951 & 0.181 & 0.660 & 0.0 & 0.428 & 7.914 & 0.021 & 0.419 & 0. \\  & LayoutFormer++ & 0.620 & 0.757 & 0.202 & 0.542 & 0. & 0.471 & 0.720 & 0.024 & 0.037 & 0. \\  & LayoutFormer & 0.552 & 1.458 & **0.145** & 0.544 & 0.18 & 0.453 & 1.067 & 0.049 & 0.091 & 0. \\ \hline \multirow{3}{*}{Gen-R} & CLGLO & 0.286 & 8.989 & 0.311 & 0.615 & 3.66 & 0.277 & 19.738 & 0.123 & 0.200 & 6.66 \\  & LayoutFormer++ & 0.424 & 5.972 & 0.332 & 0.537 & 11.84 & 0.353 & 4.954 & 0.025 & 0.076 & 3.9 \\  & LayoutFormer & 0.400 & **5.178** & **0.101** & 0.564 & 10.58 & 0.347 & **3.620** & 0.037 & 0.161 & 12.29 \\ \hline \multirow{3}{*}{Completion} & LayoutTransformer & 0.363 & 6.679 & 0.194 & 0.478 & - & 0.077 & 14.769 & 0.019 & 0.0013 & - \\  & LayoutFormer++ & 0.732 & 4.574 & 0.077 & 0.487 & - & 0.471 & 10.251 & 0.020 & 0.0022 & - \\  & LayoutFormer & 0.667 & 7.318 & 0.084 & **0.428** & - & **0.476** & **2.132** & 0.023 & 0.017 & - \\ \hline \multirow{3}{*}{Refinement} & RUITE & 0.811 & 0.107 & 0.133 & 0.483 & - & 0.781 & 0.061 & 0.029 & 0.020 & - \\  & LayoutFormer++ & 0.816 & 0.032 & 0.123 & 0.489 & - & 0.785 & 0.086 & 0.024 & 0.006 & - \\ \cline{1-1}  & LayoutFormer & 0.745 & 0.978 & 0.159 & **0.478** & - & 0.647 & 0.278 & 0.072 & 0.048 & - \\ \hline \hline \end{tabular}
\end{table}
Table 2: Quantitative comparison with baselines on constraint-explicit layout generation tasks. \(\uparrow\) indicates larger values are better, \(\downarrow\) indicates smaller values are better.

Here, \(\lambda_{1}\), \(\lambda_{2}\) and \(\lambda_{3}\) are hyper-parameters to balance the importance of each metric. Alignment and Overlap reflect quality from the perspective (1), while mIoU mainly focuses on perspective (2). We will introduce them in Section 4.1. The output layout with the lowest \(q\) value (lower \(q\) indicates better quality) is returned as the final output.

## 4 Experiments

### Setups

**Datasets.** We conduct experiments on 4 datasets, including RICO [27], PubLayNet [40], PosterLayout [9] and WebUI [24]. Their statistics and usages are illustrated in Table 3. For RICO and PubLayNet, we adopt the same dataset splits as LayoutFormer++ [15]. While for PosterLayout, the training set includes 9,974 poster-layout pairs, and the remaining 905 posters are used for testing. Regarding the WebUI dataset, we adopt the dataset splits provided by parse-then-place [24]. In all cases, the in-context exemplars are retrieved from the full training set.

**Baselines.** Since constraint-explicit layout generation tasks have task-specific and task-generic methods, we compare LayoutPrompter against both kinds of state-of-the-art methods on these tasks. Concretely, we choose LayoutFormer++ [15] as the common task-generic baseline. The task-specific baselines are (1) BLT [19] for generation conditioned on types (Gen-T), (2) BLT [19] for generation conditioned on types with sizes (Gen-TS), (3) CLG-LO [18] for generation conditioned on relationships (Gen-R), (4) LayoutTransformer [7] for completion, and (5) RUITE [30] for refinement. Moreover, we compare LayoutPrompter with DS-GAN [9] and CGL-GAN [42] on content-aware layout generation. We compare with Mockup [11] and parse-then-place [24] on text-to-layout.

Figure 4: Qualitative comparison between LayoutPrompter and the state-of-the-art baseline LayoutFormer++ [15] on constraint-explicit layout generation tasks (better view in color and 2x zoom).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Dataset & Domain & Associated Task & \# Training Set & \# Test Set & \# Element Types \\ \hline RICO & Android & constraint-explicit layout generation & 31,694 & 3,729 & 25 \\ PubLayNet & document & constraint-explicit layout generation & 311,397 & 10,998 & 5 \\ PosterLayout & poster & content-aware layout generation & 9,974 & 905 & 3 \\ WebUI & web & text-to-layout & 3,835 & 487 & 10 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Dataset statistics. Note that these datasets are only used on specific tasks.

[MISSING_PAGE_FAIL:8]

that LayoutPrompter achieves as good controllability and generation quality as LayoutFormter++. First, the layouts generated by our approach satisfy various input constraints well, including type constraints, size constraints, relationship constraints, etc. Second, our approach can also produce visually pleasing layouts with well-aligned elements and small overlapping areas. Both qualitative and quantitative results demonstrate the effectiveness of LayoutPrompter.

**Content-Aware Layout Generation.** The quantitative and qualitative results are presented in Table 4 and Figure 5, respectively. Remarkably, LayoutPrompter surpasses the training-based baselines on almost all metrics. This indicates that LayoutPrompter is capable of producing higher-quality and more content-aware layouts compared to the baselines. The rendered results further validate the conclusion. For example, in columns (f) and (g) of Figure 5, the layouts from DS-GAN [9] contain serious misalignments and overlaps. And column (e) shows that DS-GAN sometimes fails to generate content-aware layouts. In contrast, our approach can not only produce aesthetic layouts but also avoid the most salient objects in the input canvas, such as the person, teddy bear, car, etc.

**Text-to-Layout.** The quantitative and qualitative comparisons are shown in Table 5 and Figure 6. Since text-to-layout is one of the most challenging layout generation tasks, LayoutPrompter slightly lags behind the current state-of-the-art method parse-then-place [24], especially on mIoU and FID metrics. However, on the other four metrics, LayoutPrompter is comparable to the baselines. Thanks to the excellent understanding capability of LLMs, our approach can better satisfy the constraints specified in textual descriptions in some cases. For example, in cases (d) and (e) of Figure 6, LayoutPrompter successfully generates _4-6 links_ and _four logos_, while parse-then-place makes wrong predictions about the number of elements.

### Ablation Studies

**Effect of Introduced Components.** LayoutPrompter has three key components, including input-output serialization, dynamic exemplar selection and layout ranking. To investigate their effects, we perform the following ablation studies (see Table 6). (1) Since LayoutFormer++ [15] has proven the effectiveness of constraint sequences relative to other formats, we only study the effect of HTML representation, which is not covered in previous works. Specifically, we replace HTML with a plain sequence proposed by LayoutFormer++ [15] (denoted as w/o HTML) to represent the output layout. This results in a significant drop in FID and overlap metrics on Gen-T. (2) To understand the contribution of dynamic exemplar selection, we compare against its variant (w/o dynamic selection) that adopts random sampling for exemplar retrieval. LayoutPrompter achieves significantly better FID and mIoU across the board. Though the variant has better Align. and Overlap scores in some tasks, its noticeably poor FID and mIoU scores indicate that it fails to acquire the layout patterns in specific domains (e.g., the generated layout does not look like a real UI layout). (3) To understand the

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & mIoU \(\uparrow\) & FID \(\downarrow\) & Align. \(\downarrow\) & Overlap \(\downarrow\) & Type Vio. \(\%\downarrow\) & Pos \& Size Vio. \(\%\downarrow\) \\ \hline Mockup & 0.1927 & 37.0123 & 0.0059 & 0.4348 & 31.49 & 44.92 \\ parse-then-place & 0.6841 & 2.9592 & 0.0008 & 0.1380 & 11.36 & 19.14 \\ LayoutPrompter (Ours) & 0.3190 & 10.7706 & 0.0009 & **0.0892** & 15.09 & 23.78 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Quantitative comparison with baselines on text-to-layout.

Figure 6: Qualitative results of Mockup, parse-then-place (short as PTP) and LayoutPrompter on text-to-layout (better view in color and 2\(\times\) zoom).

effect of the proposed layout ranker, we compare it against a variant (w/o layout ranker) that randomly picks a layout from model outputs. We find that the layout ranker consistently yields improvements on the mIoU and Align. metrics of all tasks.

**Effect of Training Set Size.** We switch training set sizes: 500, 2000, 10000 and full set (see Table 7). In our approach, the training set represents the exemplar retrieval pool. The results show that the performance of LayoutFormer++ drops rapidly as the training data decreases, but our method is much slightly affected. When training samples are limited (e.g., 500 and 2000), our approach significantly outperforms the training-based baseline on all metrics. These observations suggest that LayoutPromoter is a more data-efficient approach, which is effective in low-resource scenarios. Due to space limitations, more experimental results on stability, the effect of the number of examples, and generalization ability can be found in Section B of the supplementary material.

## 5 Conclusion and Limitation

In this work, we concentrate on leveraging Large Language Models (LLMs) for conditional layout generation to address issues present in existing methods. To enhance the performance of our approach, we introduce three crucial components: input-output serialization, dynamic exemplar selection, and layout ranking. We conduct experiments on 7 existing layout generation tasks using 4 public datasets. Both qualitative and quantitative results highlight that LayoutPromoter is a versatile, data-efficient, and training-free method capable of generating high-quality, constraint-compliant layouts. Despite these promising results, there are still some limitations. First, the performance of our approach is influenced by the number of elements in the layouts, with more elements leading to more failure cases. Notably, this is not a problem specific to our approach and has been observed in prior work [2] as well. Second, we have not studied whether LayoutPromoter is equally effective for other LLMs such as PaLM and LLaMa. Third, with the rapid development of large multimodal models such as GPT-4V, PaLI [5] and LLaVA [25], we get a promising chance to extend LayoutPromoter to supporting layout constraints specified in a wide range of modalities. We leave them for future research.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & & & \multicolumn{4}{c}{RICO} \\ \cline{3-7} Tasks & Methods & mIoU \(\uparrow\) & FID \(\downarrow\) & Align. \(\downarrow\) & Overlap \(\downarrow\) & Vio. \% \(\downarrow\) \\ \hline \multirow{3}{*}{Gen-T} & LayoutPromoter & 0.429 & 3.233 & 0.109 & 0.505 & 0.64 \\  & _w/o HTML_ & 0.460 & 7.009 & 0.106 & 0.663 & 0. \\  & _w/o dynamic selection_ & 0.251 & 8.154 & 0.053 & 0.399 & 0.24 \\  & _w/o layout ranker_ & 0.367 & 3.149 & 0.142 & 0.498 & 0.45 \\ \hline \multirow{3}{*}{Gen-TS} & LayoutPromoter & 0.552 & 1.458 & 0.145 & 0.544 & 0.18 \\  & _w/o dynamic selection_ & 0.337 & 8.107 & 0.199 & 0.400 & 0.24 \\  & _w/o layout ranker_ & 0.505 & 1.528 & 0.153 & 0.549 & 0.13 \\ \hline \multirow{3}{*}{Gen-R} & LayoutPromoter & 0.400 & 5.178 & 0.101 & 0.564 & 10.58 \\  & _w/o dynamic selection_ & 0.223 & 14.177 & 0.067 & 0.597 & 15.95 \\  & _w/o layout ranker_ & 0.341 & 5.282 & 0.137 & 0.545 & 6.54 \\ \hline \multirow{3}{*}{Completion} & LayoutPromoter & 0.667 & 7.318 & 0.084 & 0.428 & - \\  & _w/o dynamic selection_ & 0.449 & 17.409 & 0.062 & 0.422 & - \\  & _w/o layout ranker_ & 0.580 & 11.194 & 0.093 & 0.451 & - \\ \hline \multirow{3}{*}{Refinement} & LayoutPromoter & 0.745 & 0.978 & 0.159 & 0.478 & - \\  & _w/o dynamic selection_ & 0.662 & 1.718 & 0.208 & 0.468 & - \\ \cline{1-1}  & _w/o layout ranker_ & 0.705 & 1.161 & 0.188 & 0.478 & - \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation studies of the introduced components on RICO.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline  & & & \multicolumn{4}{c}{LayoutFormer++} & \multicolumn{4}{c}{LayoutPromoter} \\ \cline{3-13} Tasks & \# Training samples & mIoU \(\uparrow\) & FID \(\downarrow\) & Align. \(\downarrow\) & Overlap \(\downarrow\) & Vio. \% \(\downarrow\) & mIoU \(\uparrow\) & FID \(\downarrow\) & Align. \(\downarrow\) & Overlap \(\downarrow\) & Vio. \% \(\downarrow\) \\ \hline \multirow{3}{*}{Gen-T} & 500 & 0.176 & 92.643 & 0.272 & 0.668 & 69.27 & 0.343 & 7.201 & 0.105 & 0.539 & 0.11 \\  & 2,000 & 0.209 & 48.702 & 0.165 & 0.573 & 62.22 & 0.362 & 6.140 & 0.083 & 0.527 & 0.22 \\  & 10,000 & 0.368 & 3.370 & 0.132 & 0.572 & 11.02 & 0.389 & 4.658 & 0.097 & 0.527 & 0.11 \\  & Full Set & 0.432 & 1.096 & 0.230 & 0.530 & 0. & 0.429 & 3.233 & 0.109 & 0.505 & 0.64 \\ \hline \multirow{3}{*}{Gen-TS} & 500 & 0.171 & 79.641 & 0.301 & 0.808 & 74.66 & 0.405 & 4.068 & 0.130 & 0.596 & 0.13 \\  & 2,000 & 0.249 & 39.673 & 0.209 & 0.655 & 53.07 & 0.424 & 3.460 & 0.143 & 0.604 & 0.06 \\ \cline{1-1}  & 10,000 & 0.529 & 2.395 & 0.215 & 0.596 & 1.86 & 0.464 & 2.606 & 0.138 & 0.580 & 0.06 \\ \cline{1-1}  & Full Set & 0.620 & 0.757 & 0.202 & 0.542 & 0. & 0.552 & 1.458 & 0.145 & 0.544 & 0.18 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation studies of training set size on RICO.

## References

* [1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 35:23716-23736, 2022.
* [2] D. M. Arroyo, J. Postels, and F. Tombari. Variational transformer networks for layout generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13642-13652, 2021.
* [3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code, 2021.
* [5] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. V. Thapliyal, J. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. R. Ruiz, A. P. Steiner, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut. PaLI: A jointly-scaled multilingual language-image model. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=mWVoBz4W0u.
* [6] A. Chowdherey, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [7] K. Gupta, J. Lazarow, A. Achille, L. S. Davis, V. Mahadevan, and A. Shrivastava. Layouttransformer: Layout generation and completion with self-attention. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1004-1014, 2021.
* [8] X. Hou and L. Zhang. Saliency detection: A spectral residual approach. In _2007 IEEE Conference on computer vision and pattern recognition_, pages 1-8. Ieee, 2007.
* [9] H. Hsu, X. He, Y. Peng, H. Kong, and Q. Zhang. Posterlayout: A new benchmark and approach for content-aware visual-textual presentation layout, 2023.
* [10] Y. Hu, C.-H. Lee, T. Xie, T. Yu, N. A. Smith, and M. Ostendorf. In-context learning for few-shot dialogue state tracking. _arXiv preprint arXiv:2203.08568_, 2022.
* [11] F. Huang, G. Li, X. Zhou, J. F. Canny, and Y. Li. Creating user interface mock-ups from high-level text descriptions with deep-learning models. _arXiv preprint arXiv:2110.07775_, 2021.
* [12] M. Hui, Z. Zhang, X. Zhang, W. Xie, Y. Wang, and Y. Lu. Unifying layout generation with a decoupled diffusion model, 2023.
* [13] S. Imani, L. Du, and H. Shrivastava. Mathprompter: Mathematical reasoning using large language models. _arXiv preprint arXiv:2303.05398_, 2023.
* [14] N. Inoue, K. Kikuchi, E. Simo-Serra, M. Otani, and K. Yamaguchi. Layoutdm: Discrete diffusion model for controllable layout generation. _arXiv preprint arXiv:2303.08137_, 2023.
* [15] Z. Jiang, H. Deng, Z. Wu, J. Guo, S. Sun, V. Mijovic, Z. Yang, J.-G. Lou, and D. Zhang. Unilayout: Taming unified sequence-to-sequence transformers for graphic layout generation. _arXiv preprint arXiv:2208.08037_, 2022.

* [16] A. A. Jyothi, T. Durand, J. He, L. Sigal, and G. Mori. Layoutvae: Stochastic scene layout generation from a label set. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9895-9904, 2019.
* [17] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson, P. Clark, and A. Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. _arXiv preprint arXiv:2210.02406_, 2022.
* [18] K. Kikuchi, E. Simo-Serra, M. Otani, and K. Yamaguchi. Constrained graphic layout generation via latent optimization. In _Proceedings of the 29th ACM International Conference on Multimedia_, pages 88-96, 2021.
* [19] X. Kong, L. Jiang, H. Chang, H. Zhang, Y. Hao, H. Gong, and I. Essa. Blt: bidirectional layout transformer for controllable layout generation. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII_, pages 474-490. Springer, 2022.
* [20] H. W. Kuhn. The hungarian method for the assignment problem. _Naval research logistics quarterly_, 2(1-2):83-97, 1955.
* [21] H.-Y. Lee, L. Jiang, I. Essa, P. B. Le, H. Gong, M.-H. Yang, and W. Yang. Neural design network: Graphic layout generation with constraints. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16_, pages 491-506. Springer, 2020.
* [22] J. Li, J. Yang, J. Zhang, C. Liu, C. Wang, and T. Xu. Attribute-conditioned layout gan for automatic graphic design. _IEEE Transactions on Visualization and Computer Graphics_, 27(10):4039-4048, 2020.
* [23] Y. Li, J. Amelot, X. Zhou, S. Bengio, and S. Si. Auto completion of user interface layout design using transformer-based tree decoders. _arXiv preprint arXiv:2001.05308_, 2020.
* [24] J. Lin, J. Guo, S. Sun, W. Xu, T. Liu, J.-G. Lou, and D. Zhang. A parse-then-place approach for generating graphic layouts from textual descriptions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.
* [25] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In _NeurIPS_, 2023.
* [26] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What makes good in-context examples for gpt-3? _arXiv preprint arXiv:2101.06804_, 2021.
* [27] T. F. Liu, M. Craft, J. Situ, E. Yumer, R. Mech, and R. Kumar. Learning design semantics for mobile apps. In _Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology_, UIST '18, page 569-579, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450359481. doi: 10.1145/3242587.3242650. URL https://doi.org/10.1145/3242587.3242650.
* [28] OpenAI. Gpt-4 technical report, 2023.
* [29] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [30] S. Rahman, V. P. Sermuga Pandian, and M. Jarke. Ruite: Refining ui layout aesthetics using transformer encoder. In _26th International Conference on Intelligent User Interfaces-Companion_, pages 81-83, 2021.
* [31] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.

* [33] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.
* [34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.
* [35] K. Yang, D. Klein, N. Peng, and Y. Tian. Doc: Improving long story coherence with detailed outline control. _arXiv preprint arXiv:2212.10077_, 2022.
* [36] K. Yang, N. Peng, Y. Tian, and D. Klein. Re3: Generating longer stories with recursive repropting and revision. _arXiv preprint arXiv:2210.06774_, 2022.
* [37] J. Zhang, J. Guo, S. Sun, J.-G. Lou, and D. Zhang. Layoutdiffusion: Improving graphic layout generation by discrete diffusion probabilistic models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 7226-7236, October 2023.
* [38] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola. Multimodal chain-of-thought reasoning in language models. _arXiv preprint arXiv:2302.00923_, 2023.
* [39] X. Zheng, X. Qiao, Y. Cao, and R. W. Lau. Content-aware generative modeling of graphic design layouts. _ACM Transactions on Graphics (TOG)_, 38(4):1-15, 2019.
* [40] X. Zhong, J. Tang, and A. J. Yepes. Publaynet: largest dataset ever for document layout analysis, 2019.
* [41] D. Zhou, N. Scharli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, O. Bousquet, Q. Le, and E. Chi. Least-to-most prompting enables complex reasoning in large language models. _arXiv preprint arXiv:2205.10625_, 2022.
* [42] M. Zhou, C. Xu, Y. Ma, T. Ge, Y. Jiang, and W. Xu. Composition-aware graphic layout gan for visual-textual presentation designs. _arXiv preprint arXiv:2205.00303_, 2022.

Coordinate Discretization

In this work, element coordinates are scaled proportionally into a canvas of size \(C_{W}\times C_{H}\). We follow the baselines to choose these two parameters. Specifically, in RICO, \(C_{W}=90\)px, \(C_{H}=160\)px. In PubLayNet, \(C_{W}=120\)px, \(C_{H}=160\)px. In PosterLayout, \(C_{W}=102\)px, \(C_{H}=150\)px. In WebUI, \(C_{W}=120\)px, \(C_{H}=120\)px. Then, the coordinates are discretized to the nearest integers.

## Appendix B Additional Experimental Results and Analysis

### Stability of Generation Performance

The output of LLMs varies with the random seed and hyper-parameters (e.g., the temperature). That is, for the same input constraint, LLMs are able to generate many completely different layouts. Since the hyper-parameters are a trade-off between generation quality and diversity, we fix them to the default values of OpenAI API and study the impact of random seeds on model performance. Specifically, we run inference on the test set 10 times, each using a different random seed. Then, we calculate the mean and variance of each quantitative metric (see Table 8). The small variances indicate the stability of LayoutPromter's performance under different random seeds.

### Effect of Exemplar Number

We conduct ablation experiments on the number of prompting exemplars. Figure 7 shows the zero-shot (\(N=0\)) qualitative results on Gen-T. It is obvious that LLMs fail to generate reasonable layouts in a zero-shot scheme. Table 9 exhibits the quantitative comparison of the Gen-T task on RICO. The results indicate that the number of prompting exemplars mainly affects mIoU and FID. Specifically, as the number of prompting exemplars increases, mIoU and FID get improved. In summary, the number of exemplars has a positive effect on the performance of LayoutPromter.

### Generalization Ability

To investigate the generalization ability of LayoutPromter, we compute the DocSim similarity between the generated layouts and their prompting layouts (see Table 10). The DocSim of LayoutFormer++ is computed between the generated layouts and training layouts. The quantitative results show that LayoutPromter achieves competitive or even better scores compared to LayoutFormer++,

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{RICO} & \multicolumn{4}{c}{PubLayNet} \\ \cline{2-10} Tasks & mIoU\(\uparrow\) & FID \(\downarrow\) & Align. \(\downarrow\) & Overlap \(\downarrow\) & Vio. \(\downarrow\) & mIoU\(\uparrow\) & FID \(\downarrow\) & Align. \(\downarrow\) & Overlap \(\downarrow\) & Vio. \(\%\downarrow\) \\ \hline Gen-T & 0.3686±0.002 & 3.118±0.045 & 0.130±0.010 & 0.498±0.004 & 0.546±0.148 & 0.343±0.001 & 4.041±0.067 & 0.042±0.007 & 0.047±0.002 & 0.049±0.059 \\ Gen-TS & 0.594±0.001 & 1.489±0.037 & 0.155±0.003 & 0.550±0.005 & 0.134±0.025 & 0.393±0.001 & 2.016±0.024 & 0.054±0.008 & 0.098±0.002 & 0. \\ \hline \hline \end{tabular}
\end{table}
Table 8: Effect of random seeds. In this experiment, we disable the layout ranker to eliminate the impact of the ranking mechanism on model performance.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{4}{c}{RICO} \\ \cline{2-7} Tasks & \# exemplar & mIoU \(\uparrow\) & FID \(\downarrow\) & Align. \(\downarrow\) & Overlap \(\downarrow\) & Vio. \(\%\downarrow\) \\ \hline \multirow{4}{*}{Gen-T} & 1 & 0.381 & 5.007 & 0.115 & 0.491 & 0.85 \\  & 3 & 0.413 & 5.098 & 0.120 & 0.492 & 0.51 \\ \cline{1-1}  & 5 & 0.414 & 4.521 & 0.114 & 0.492 & 0.65 \\ \cline{1-1}  & 10 & 0.427 & 3.523 & 0.092 & 0.486 & 0.67 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Ablation studies on the number of prompting exemplars. We run experiments on 1,000 test samples.

Figure 7: Zero-shot results on the Gen-T task.

[MISSING_PAGE_EMPTY:15]

[MISSING_PAGE_EMPTY:16]

### Generation Conditioned on Element Types and Sizes

Figure 10: Qualitative results of Gen-TS on RICO and PubLayNet. The element type and size constraints are in the table.

### Generation Conditioned on Element Relationships

Figure 11: Qualitative results of Gen-R on RICO and PubLayNet. The element relationship constraints are in the table.

### Layout Completion

Figure 12: Qualitative results of completion on RICO and PubLayNet.

### Layout Refinement

Figure 13: Qualitative results of refinement on RICO and PubLayNet. Note that each group has two layouts. The left one is the noisy layout, and the right one is the refined layout.

### Content-Aware Layout Generation

### Text-to-Layout

Figure 14: Qualitative results of content-aware layout generation on PosterLayout.

Figure 15: Qualitative results of text-to-layout on WebUI. The textual descriptions are in the table.

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_FAIL:24]

**Preamble**

Please generate a layout based on the given information.

_Task Description_: layout completion

_Layout Domain_: android layout

_Camvas Size_: canvas width is 90px, canvas height is 160px

**Exemplar 1**

_Partial Layout_: image 21 5 47 40

<html>

<body>

<div class="image" style="left:21px; top:5px; width:47px; height:40px"></div>

<div class="text button" style="left:2px; top:53px; width:84px; height:15px"></div>

<div class="image" style="left:7px; top:74px; width:9px; height:5px"></div>

<div class="text" style="left:19px; top:74px; width:67px; height:5px"></div>

<div class="text button" style="left:2px; top:85px; width:84px; height:14px"></div>

<div class="text button" style="left:1px; top:104px; width:86px; height:12px"></div>

<div class="text button" style="left:1px; top:136px; width:86px; height:11px"></div>

</body>

</html>

**Exemplar 2**

_Partial Layout_: image 17 5 56 11

<html>

<body>

<div class="image" style="left:17px; top:5px; width:66px; height:11px"></div>

<div class="image" style="left:0px; top:17px; width:90px; height:48px"></div>

<div class="text" style="left:2px; top:65px; width:86px; height:48px"></div>

<div class="image" style="left:0px; top:108px; width:90px; height:5px"></div>

<div class="pager indicator" style="left:38px; top:114px; width:12px; height:8px"></div>

<div class="text button" style="left:3px; top:124px; width:82px; height:13px"></div>

<div class="text button" style="left:62px; top:137px; width:17px; height:10px"></div>

<div class="text" style="left:10px; top:140px; width:51px; height:6px"></div>

</body>

</html>

.....

**Test Sample**

_Partial Layout_: image 12 10 65 32

(Generated by LLMs)

\begin{table}
\begin{tabular}{l}
**Preamble** \\ _Layout_: image 12 10 65 32 \\ \end{tabular}
\end{table}
Table 14: A prompt example of layout completion on RICO.

**Preamble**

Please generate a layout based on the given information.

_Task Description_: layout refinement

_Layout Domain_: android layout

_Canvas Size_: canvas width is 90px, canvas height is 160px

**Exemplar 1**

_Noise Layout_: advertisement 11 18 70 11 | icon 76 5 11 11 | icon 0 6 12 10 | image 16 8 13 11 | text 30 3 21 5 | text 29 11 23 4 | toolbar 0 5 88 16 | web view 9 16 69 12 | web view 11 17 70 12 | web view 0 20 90 140

<html>

<body>

[MISSING_PAGE_POST]

</body>

[MISSING_PAGE_POST]

<div class="web view" style="left:10px; top:5px; width:70px; height:10px"></div>

</body>

</html>

....

**Test Sample**

_Noise Layout_: icon 68 5 10 12 | icon 1 5 9 12 | icon 80 5 12 13 | text 14 7 56 2 | toolbar 0 5 90 10 | web view 0 18 90 130 | web view 0 19 90 130

(Generated by LLMs)

\begin{table}
\begin{tabular}{l}
**Preamble**

Please generate a layout based on the given information. \\
_Task Description_: layout refinement \\
_Layout Domain_: android layout \\
**Canvas Size**: canvas width is 90px, canvas height is 160px \\
**Exemplar 1**

_Noise Layout_: advertisement 11 18 70 11 | icon 76 5 11 11 | icon 0 6 12 10 | image 16 8 13 11 | text 30 3 21 5 | text 29 11 23 4 | toolbar 0 5 88 16 | web view 9 16 69 12 | web view 11 17 70 12 | web view 0 20 90 140 \\

<html> \\ \textless{}body> \\ \textless{}div class="advertisment" style="left:10px; top:18px; width:70px; height:11px"></div> \\ \textless{}div class="icon" style="left:77px; top:6px; width:12px; height:11px"></div> \\ \textless{}div class="icon" style="left:0px; top:5px; width:12px; height:11px"></div> \\ \textless{}div class="icon" style="left:0px; top:5px; width:12px; height:13px"></div> \\ \textless{}div class="text" style="left:0px; top:5px; width:70px; height:11px"></div> \\ \textless{}div class="web view" style="left:10px; top:5px; width:70px; height:10px"></div> \\ \textless{}/body> \\ \textless{}/html> \\ \hline \end{tabular}
\end{table}
Table 15: A prompt example of layout refinement on RICO.

[MISSING_PAGE_FAIL:27]

[MISSING_PAGE_FAIL:28]