# Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper introduces a new method for minimizing matrix-smooth non-convex objectives through the use of novel Compressed Gradient Descent (CGD) algorithms enhanced with a matrix-valued stepsize. The proposed algorithms are theoretically analyzed first in the single-node and subsequently in the distributed settings. Our theoretical results reveal that the matrix stepsize in CGD can capture the objective's structure and lead to faster convergence compared to a scalar stepsize. As a byproduct of our general results, we emphasize the importance of selecting the compression mechanism and the matrix stepsize in a layer-wise manner, taking advantage of model structure. Moreover, we provide theoretical guarantees for free compression, by designing specific layer-wise compressors for the non-convex matrix smooth objectives. Our findings are supported with empirical evidence.

## 1 Introduction

The minimization of smooth and non-convex functions is a fundamental problem in various domains of applied mathematics. Most machine learning algorithms rely on solving optimization problems for training and inference, often with structural constraints or non-convex objectives to accurately capture the learning and prediction problems in high-dimensional or non-linear spaces. However, non-convex problems are typically NP-hard to solve, leading to the popular approach of relaxing them to convex problems and using traditional methods. Direct approaches to non-convex optimization have shown success but their convergence and properties are not well understood, making them challenging for large scale optimization. While its convex alternative has been extensively studied and is generally an easier problem, the non-convex setting is of greater practical interest often being the computational bottleneck in many applications.

In this paper, we consider the general minimization problem:

\[\min_{x\in\mathbb{R}^{d}}f(x),\] (1)

where \(f:\mathbb{R}^{d}\to\mathbb{R}\) is a differentiable function. In order for this problem to have a finite solution we will assume throughout the paper that \(f\) is bounded from below.

**Assumption 1**.: _There exists \(f^{\mathrm{inf}}\in\mathbb{R}\) such that \(f(x)\geq f^{\mathrm{inf}}\) for all \(x\in\mathbb{R}^{d}\)._

The stochastic gradient descent (SGD) algorithm [12, 13, 14] is one of the most common algorithms to solve this problem. In its most general form, it can be written as

\[x^{k+1}=x^{k}-\gamma g(x^{k}),\] (2)

where \(g(x^{k})\) is a stochastic estimator of \(\nabla f(x^{k})\) and \(\gamma>0\) is a positive scalar stepsize. A particular case of interest is the compressed gradient descent (CGD) algorithm [16], where the estimatoris taken as a compressed alternative of the initial gradient:

\[g(x^{k})=\mathcal{C}(\nabla f(x^{k})),\] (3)

and the compressor \(\mathcal{C}\) is chosen to be a "sparser" estimator that aims to reduce the communication overhead in distributed or federated settings. This is crucial, as highlighted in the seminal paper by [16], which showed that the bottleneck of distributed optimization algorithms is the communication complexity. In order to deal with the limited resources of current devices, there are various compression objectives that are practical to achieve. These include also compressing the model broadcasted from server to clients for local training, and reducing the computational burden of local training. These objectives are mostly complementary, but compressing gradients has the potential for the greatest practical impact due to slower upload speeds of client connections and the benefits of averaging [16]. In this paper we will focus on this latter problem.

An important subclass of compressors are the sketches. Sketches are linear operators defined on \(\mathbb{R}^{d}\), i.e., \(\mathcal{C}(y)=\bm{S}y\) for every \(y\in\mathbb{R}^{d}\), where \(\bm{S}\) is a random matrix. A standard example of such a compressor is the Rand-\(k\) compressor, which randomly chooses \(k\) entries of its argument and scales them with a scalar multiplier to make the estimator unbiased. Instead of communicating all \(d\) coordinates of the gradient, one communicates only a subset of size \(k\), thus reducing the number of communicated bits by a factor of \(d/k\). Formally, Rand-\(k\) is defined as follows: \(\bm{S}=\sum_{j=1}^{k}\frac{d}{k}e_{i_{j}}^{\top}e_{i_{j}}^{\top}\), where \(i_{j}\) are the selected coordinates of the input vector. We refer the reader to [17] for an overview on compressions.

Besides the assumption that function \(f\) is bounded from below, we also assume that it is \(\bm{L}\) matrix smooth, as we are trying to take advantage of the entire information contained in the smoothness matrix \(\bm{L}\) and the stepsize matrix \(\bm{D}\).

**Assumption 2** (Matrix smoothness).: _There exists \(\bm{L}\in\mathbb{S}_{+}^{d}\) such that_

\[f(x)\leq f(y)+\left\langle\nabla f(y),x-y\right\rangle+\frac{1}{2}\left\langle \bm{L}(x-y),x-y\right\rangle\] (4)

_holds for all \(x,y\in\mathbb{R}^{d}\)._

The assumption of matrix smoothness, which is a generalization of scalar smoothness, has been shown to be a more powerful tool for improving supervised model training. In [15], the authors proposed using smoothness matrices and suggested a novel communication sparsification strategy to reduce communication complexity in distributed optimization. The technique was adapted to three distributed optimization algorithms in the convex setting, resulting in significant communication complexity savings and consistently outperforming the baselines. The results of this study demonstrate the efficacy of the matrix smoothness assumption in improving distributed optimization algorithms.

The case of block-diagonal smoothness matrices is particularly relevant in various applications, such as neural networks (NN). In this setting, each block corresponds to a layer of the network, and we characterize the smoothness with respect to nodes in the \(i\)-th layer by a corresponding matrix \(\bm{L}_{i}\). Unlike in the scalar setting, we favor the similarity of certain entries of the argument over the others. This is because the information carried by the layers becomes more complex, while the nodes in the same layers are similar. This phenomenon has been observed visually in various studies, such as those by [14] and [15].

Another motivation for using a layer-dependent stepsize has its roots in physics. In nature, the propagation speed of light in media of different densities varies due to frequency variations. Similarly, different layers in neural networks carry different information, metric systems, and scaling. Thus, the stepsizes need to be picked accordingly to achieve optimal convergence.

We study two matrix stepsized CGD-type algorithms and analyze their convergence properties for non-convex matrix-smooth functions. As mentioned earlier, we put special emphasis on the block-diagonal case. We design our sketches and stepsizes in a way that leverages this structure, and we show that in certain cases, we can achieve compression without losing in the overall communication complexity.

### Related work

Many successful convex optimization techniques have been adapted for use in the non-convex setting. Here is a non-exhaustive list: adaptivity [17, 16], variance reduction [14, 15],LBZR21], and acceleration [14]. A paper of particular importance for our work is that of [13], which proposes a unified scheme for analyzing stochastic gradient descent in the non-convex regime. A comprehensive overview of non-convex optimization can be found in [15, 16, 17]. A classical example of a matrix stepsized method is Newton's method. This method has been popular in the optimization community for a long time [11, 12, 13]. However, computing the stepsize as the inverse Hessian of the current iteration results in significant computational complexity. Instead, quasi-Newton methods use an easily computable estimator to replace the inverse of the Hessian [1, 16, 17, 18]. An example is the Newton-Star algorithm [13], which we discuss in Section 2.

[14] analyzed sketched gradient descent by making the compressors unbiased with a sketch-and-project trick. They provided an analysis of the resulting algorithm for the linear feasibility problem. Later, [15] proposed a variance-reduced version of this method. Leveraging the layer-wise structure of neural networks has been widely studied for optimizing the training loss function. For example, [16] propose SGD with different scalar stepsizes for each layer, [17, 18] propose layer-wise normalization for Stochastic Normalized Gradient Descent, and [19, 20] propose layer-wise compression in the distributed setting.

DCGD, proposed by [13], has since been improved in various ways, such as in [15, 16]. There is also a large body of literature on other federated learning algorithms with unbiased compressors [1, 18, 19, 20, 21, 22].

### Contributions

Our paper contributes in the following ways:

* We propose two novel matrix stepsize sketch CGD algorithms in Section 2, which, to the best of our knowledge, are the first attempts to analyze a fixed matrix stepsize for non-convex optimization. We present a unified theorem in Section 3 that guarantees stationarity for minimizing matrix-smooth non-convex functions. The results shows that taking our algorithms improve on their scalar alternatives. The complexities are summarized in Table 1 for some particular cases.
* We design our algorithms' sketches and stepsize to take advantage of the layer-wise structure of neural networks, assuming that the smoothness matrix is block-diagonal. In Section 4, we prove that our algorithms achieve better convergence than classical methods.
* Assuming the that the server-to-client communication is less expensive [15, 16], we propose distributed versions of our algorithms in Section 5, following the standard FL scheme, and prove weighted stationarity guarantees. Our theorem recovers the result for DCGD in the scalar case and improves it in general.
* We validate our theoretical results with experiments. The plots and framework are provided in the Appendix.

### Preliminaries

The usual Euclidean norm on \(\mathbb{R}^{d}\) is defined as \(\left\|\cdot\right\|\). We use bold capital letters to denote matrices. By \(\bm{I}_{d}\) we denote the \(d\times d\) identity matrix, and by \(\bm{O}_{d}\) we denote the \(d\times d\) zero matrix. Let \(\mathbb{S}^{d}_{++}\) (resp. \(\mathbb{S}^{d}_{+}\)) be the set of \(d\times d\) symmetric positive definite (resp. semi-definite) matrices. Given \(\bm{Q}\in\mathbb{S}^{d}_{++}\) and \(x\in\mathbb{R}^{d}\), we write \(\left\|x\right\|_{\bm{Q}}:=\sqrt{\langle\bm{Q}x,x\rangle},\) where \(\langle\cdot,\cdot\rangle\) is the standard Euclidean inner product on \(\mathbb{R}^{d}\). For a matrix \(\bm{A}\in\mathbb{S}^{d}_{++}\), we define by \(\lambda_{\max}(\bm{A})\) (resp. \(\lambda_{\min}(\bm{A})\)) the largest (resp. smallest) eigenvalue of the matrix \(\bm{A}\). Let \(\bm{A}_{i}\in\mathbb{R}^{d_{i}\times d_{i}}\) and \(d=d_{1}+\ldots+d_{\ell}\). Then the matrix \(\bm{A}=\mathrm{Diag}(\bm{A}_{1},\ldots,\bm{A}_{\ell})\) is defined as a block diagonal \(d\times d\) matrix where the \(i\)-th block is equal to \(\bm{A}_{i}\). We will use \(\mathrm{diag}(\bm{A})\in\mathbb{R}^{d\times d}\) to denote the diagonal of any matrix \(\bm{A}\in\mathbb{R}^{d\times d}\). Given a function \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\), its gradient and its Hessian at point \(x\in\mathbb{R}^{d}\) are respectively denoted as \(\nabla f(x)\) and \(\nabla^{2}f(x)\).

The algorithms

Below we define our two main algorithms:

\[x^{k+1} =x^{k}-\bm{D}\bm{S}^{k}\nabla f(x^{k}),\] (det-CGD1)

and

\[x^{k+1} =x^{k}-\bm{T}^{k}\bm{D}\nabla f(x^{k}).\] (det-CGD2)

Here, \(\bm{D}\in\mathbb{S}^{d}_{++}\) is the fixed stepsize matrix. The sequences of random matrices \(\bm{S}^{k}\) and \(\bm{T}^{k}\) satisfy the next assumption.

**Assumption 3**.: _We will assume that the random sketches that appear in our algorithms are i.i.d., unbiased, symmetric and positive semi-definite for each algorithm. That is_

\[\bm{S}^{k},\bm{T}^{k} \in\mathbb{S}^{d}_{+},\quad\bm{S}^{k}\overset{iid}{\sim}\mathcal{ S}\quad\text{and}\quad\bm{T}^{k}\overset{iid}{\sim}\mathcal{T}\] \[\mathbb{E}\left[\bm{S}^{k}\right] =\mathbb{E}\left[\bm{T}^{k}\right]=\bm{I}_{d},\quad\text{for every}\quad k \in\mathbb{N}.\]

A simple instance of det-CGD1 and det-CGD2 is the vanilla GD. Indeed, if \(\bm{S}^{k}=\bm{T}^{k}=\bm{I}_{d}\) and \(\bm{D}=\gamma\bm{I}_{d}\), then \(x^{k+1}=x^{k}-\gamma\nabla f(x^{k})\). In general, one may view these algorithms as Newton-type methods. In particular, our setting includes the Newton Star (NS) algorithm by [14]:

\[x^{k+1}=x^{k}-\left(\nabla^{2}f(x^{\inf})\right)^{-1}\nabla f(x^{k}).\] (NS)

The authors prove that in the convex case it converges to the unique solution \(x^{\inf}\) locally quadratically, provided certain assumptions are met. However, it is not a practical method as it requires knowledge of the Hessian at the optimal point. This method, nevertheless, hints that constant matrix stepsize can yield fast convergence guarantees. Our results allow us to choose the \(\bm{D}\) depending on the smoothness matrix \(\bm{L}\). The latter can be seen as a uniform upper bound on the Hessian.

The difference between det-CGD1 and det-CGD2 is the update rule. In particular, the order of the sketch and the stepsize is interchanged. When the sketch \(\bm{S}\) and the stepsize \(\bm{D}\) are commutative w.r.t. matrix product, the algorithms become equivalent. In general, a simple calculation shows that if we take

\[\bm{T}^{k}=\bm{D}\bm{S}^{k}\bm{D}^{-1},\] (5)

then det-CGD1 and det-CGD2 are the same. Defining \(\bm{T}^{k}\) according to (5), we recover the unbiasedness condition:

\[\mathbb{E}\left[\bm{T}^{k}\right]=\bm{D}\mathbb{E}\left[\bm{S}^{k}\right] \bm{D}^{-1}=\bm{I}_{d}.\] (6)

However, in general \(\bm{D}\mathbb{E}\left[\bm{S}^{k}\right]\bm{D}^{-1}\) is not necessarily symmetric, which contradicts to Assumption 3. Thus, det-CGD1 and det-CGD2 are not equivalent for our purposes.

## 3 Main results

Before we state the main result, we present a stepsize condition for det-CGD1 and det-CGD2, respectively:

\[\mathbb{E}\left[\bm{S}^{k}\bm{D}\bm{L}\bm{D}\bm{S}^{k}\right]\preceq\bm{D},\] (7)

and

\[\mathbb{E}\left[\bm{D}\bm{T}^{k}\bm{L}\bm{T}^{k}\bm{D}\right]\preceq\bm{D}.\] (8)

In the case of vanilla GD (7) and (8) become \(\gamma<L^{-1}\), which is the standard condition for convergence.

Below is the main convergence theorem for both algorithms in the single-node regime.

**Theorem 1**.: _Suppose that Assumptions 1-3 are satisfied. Then, for each \(k\geq 0\)_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|_{ \bm{D}}^{2}\right]\leq\frac{2(f(x^{0})-f^{\inf})}{K},\] (9)

_if one of the below conditions is true:_1. _The vectors_ \(x^{k}\) _are the iterates of_ \(\text{det-CGD1}\) _and_ \(\bm{D}\) _satisfies (_7_);_
2. _The vectors_ \(x^{k}\) _are the iterates of_ \(\text{det-CGD2}\) _and_ \(\bm{D}\) _satisfies (_8_)._

It is important to note that Theorem 1 yields the same convergence rate for any \(\bm{D}\in\mathbb{S}_{++}^{d}\), despite the fact that the matrix norms on the left-hand side cannot be compared for different weight matrices. To ensure comparability of the right-hand side of (9), it is necessary to normalize the weight matrix \(\bm{D}\) that is used to measure the gradient norm. We propose using determinant normalization, which involves dividing both sides of (9) by \(\det(\bm{D})^{1/d}\), yielding the following:

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|_{ \frac{\bm{D}}{\det(\bm{D})^{1/d}}}^{2}\right]\leq\frac{2(f(x^{0})-f^{\inf})}{ \det(\bm{D})^{1/d}K}.\] (10)

This normalization is meaningful because adjusting the weight matrix to \(\frac{\bm{D}}{\det(\bm{D})^{1/d}}\) allows its determinant to be 1, making the norm on the left-hand side comparable to the standard Euclidean norm. It is important to note that the volume of the normalized ellipsoid \(\left\{x\in\mathbb{R}^{d}\ :\ \left\|x\right\|_{\bm{D}/\det(\bm{D})^{1/d}}^{2} \leq 1\right\}\) does not depend on the choice of \(\bm{D}\in\mathbb{S}_{++}^{d}\). Therefore, the results of (9) are comparable across different \(\bm{D}\) in the sense that the right-hand side of (9) measures the volume of the ellipsoid containing the gradient.

### Optimal matrix stepsize

In this section, we describe how to choose the optimal stepsize that minimizes the iteration complexity. The problem is easier for \(\text{det-CGD2}\). We notice that (8) can be explicitly solved. Specifically, it is equivalent to

\[\bm{D}\preceq\left(\mathbb{E}\left[\bm{T}^{k}\bm{L}\bm{T}^{k}\right]\right)^{ -1}.\] (11)

We want to emphasize that the RHS matrix is invertible despite the sketches not being so. Indeed. The map \(h:\bm{T}\to\bm{T}\bm{L}\bm{T}\) is convex on \(\mathbb{S}_{+}^{d}\). Therefore, Jensen's inequality implies

\[\mathbb{E}\left[\bm{T}^{k}\bm{L}\bm{T}^{k}\right]\succeq\mathbb{E}\left[\bm {T}^{k}\right]\bm{L}\mathbb{E}\left[\bm{T}^{k}\right]=\bm{L}\succ\bm{O}_{d}.\]

This explicit condition on \(\bm{D}\) can assist in determining the optimal stepsize. Since both \(\bm{D}\) and \((\bm{T}^{k}\bm{L}\bm{T}^{k})^{-1}\) are positive definite, then the right-hand side of (10) is minimized exactly when

\[\bm{D}=(\bm{T}^{k}\bm{L}\bm{T}^{k})^{-1}.\] (12)

The situation is different for \(\text{det-CGD1}\). According to (10), the optimal \(\bm{D}\) is defined as the solution of the following constrained optimization problem:

\[\begin{split}\text{minimize}&\quad\log\det(\bm{D}^{ -1})\\ \text{subject to}&\quad\mathbb{E}\left[\bm{S}^{k}\bm{D} \bm{L}\bm{D}\bm{S}^{k}\right]\preceq\bm{D}\\ &\quad\bm{D}\in\mathbb{S}_{++}^{d}.\end{split}\] (13)

**Proposition 1**.: _The optimization problem (13) with respect to stepsize matrix \(\bm{D}\in\mathbb{S}_{++}^{d}\), is a convex optimization problem with convex constraint._

The proof of this proposition can be found in the Appendix. It is based on the reformulation of the constraint to its equivalent quadratic form inequality. Using the trace trick, we can prove that for every vector chosen in the quadratic form, it is convex. Since the intersection of convex sets is convex, we conclude the proof.

One could consider using the CVXPY[1] package to solve (13), provided that it is first transformed into a Disciplined Convex Programming (DCP) form [1]. Nevertheless, (7) is not recognized as a DCP constraint in the general case. To make CVXPY applicable, additional steps tailored to the problem at hand must be taken.

## 4 Leveraging the layer-wise structure

In this section we focus on the block-diagonal case of \(\bm{L}\) for both \(\mathrm{det}\)-CGD1 and \(\mathrm{det}\)-CGD2. In particular, we propose hyper-parameters of \(\mathrm{det}\)-CGD1 designed specifically for training NNs. Let us assume that \(\bm{L}=\mathrm{Diag}(\bm{L}_{1},\dots,\bm{L}_{\ell})\), where \(\bm{L}_{i}\in\mathbb{S}_{++}^{d_{i}}\). This setting is a generalization of the classical smoothness condition, as in the latter case \(\bm{L}_{i}=L\bm{I}_{d_{i}}\) for all \(i=1,\dots,\ell\). Respectively, we choose both the sketches and the stepsize to be block diagonal: \(\bm{D}=\mathrm{Diag}(\bm{D}_{1},\dots,\bm{D}_{\ell})\) and \(\bm{S}^{k}=\mathrm{Diag}(\bm{S}_{1}^{k},\dots,\bm{S}_{\ell}^{k})\), where \(\bm{D}_{i},\bm{S}_{i}^{k}\in\mathbb{S}_{++}^{d_{i}}\).

Let us notice that the left hand side of the inequality constraint in (13) has quadratic dependence on \(\bm{D}\), while the right hand side is linear. Thus, for every matrix \(\bm{W}\in\mathbb{S}_{++}^{d}\), there exists \(\gamma>0\) such that

\[\gamma^{2}\lambda_{\max}\left(\mathbb{E}\left[\bm{S}^{k}\bm{W}\bm{L}\bm{W}\bm{S }^{k}\right]\right)\leq\gamma\lambda_{\min}(\bm{W}).\]

Therefore, for \(\gamma\bm{W}\) we deduce

\[\mathbb{E}\left[\bm{S}^{k}(\gamma\bm{W})\bm{L}(\gamma\bm{W})\bm{S}^{k}\right] \preceq\gamma^{2}\lambda_{\max}\left(\mathbb{E}\left[\bm{S}^{k}\bm{W}\bm{L}\bm{ W}\bm{S}^{k}\right]\right)\bm{I}_{d}\preceq\gamma\lambda_{\min}(\bm{W})\bm{I}_{d} \preceq\gamma\bm{W}.\] (14)

The following theorem is based on this simple fact applied to the corresponding blocks of the matrices \(\bm{D},\bm{L},\bm{S}^{k}\) for \(\mathrm{det}\)-CGD1.

**Theorem 2**.: _Let \(f:\mathbb{R}^{d}\to\mathbb{R}\) satisfy Assumptions 1 and 2, with \(\bm{L}\) admitting the layer-separable structure \(\bm{L}=\mathrm{Diag}(\bm{L}_{1},\dots,\bm{L}_{\ell})\), where \(\bm{L}_{1},\dots,\bm{L}_{\ell}\in\mathbb{S}_{++}^{d_{i}}\). Choose random matrices \(\bm{S}_{1}^{k},\dots,\bm{S}_{\ell}^{k}\in\mathbb{S}_{+}^{d}\) to satisfy Assumption 3 for all \(i\in[\ell]\), and let \(\bm{S}^{k}:=\mathrm{Diag}(\bm{S}_{1}^{k},\dots,\bm{S}_{\ell}^{k})\). Furthermore, choose matrices \(\bm{W}_{1},\dots,\bm{W}_{\ell}\in\mathbb{S}_{++}^{d}\) and scalars \(\gamma_{1},\dots,\gamma_{\ell}>0\) such that_

\[\gamma_{i}\leq\lambda_{\max}^{-1}\left(\mathbb{E}\left[\bm{W}_{i}^{-1/2}\bm{S }_{i}^{k}\bm{W}_{i}\bm{L}_{i}\bm{W}_{i}\bm{S}_{i}^{k}\bm{W}_{i}^{-1/2}\right] \right)\qquad\forall i\in[\ell].\] (15)

_Letting \(\bm{W}:=\mathrm{Diag}(\bm{W}_{1},\dots,\bm{W}_{\ell})\), \(\Gamma:=\mathrm{Diag}(\gamma_{1}\bm{I}_{d_{1}},\dots,\gamma_{\ell}\bm{I}_{d_ {\ell}})\) and \(\bm{D}:=\Gamma\bm{W}\), we get_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|_{ \frac{\Gamma\bm{W}}{\det(\Gamma\bm{W})^{1/d}}}^{2}\right]\leq\frac{2(f(x^{0} )-f^{\inf})}{\det\left(\Gamma\bm{W}\right)^{1/d}K}.\] (16)

\begin{table}
\begin{tabular}{l l l l} \hline \hline No. & The method & \(\left(\bm{S}_{i}^{k},\bm{D}_{i}\right)\) & \(l\geq 1,d_{i},k_{i},\sum_{i=1}^{d}k_{i}=k\), layer structure & \(l=1,k_{i}=k\), general structure \\ \hline
1. & \(\mathrm{det}\)-CGD1 & \(\left(\bm{I}_{d},\gamma\bm{L}_{i}^{-1}(\bm{L}_{i})\right)\) & \(d\cdot\det(\bm{LIn particular, if the scalars \(\{\gamma_{i}\}\) are chosen to be equal to their maximum allowed values from (15), then the convergence factor of (16) is equal to

\[\det\left(\Gamma\bm{W}\right)^{-\frac{1}{d}}=\left[\prod_{i=1}^{d}\lambda_{\max }^{d_{i}}\left(\mathbb{E}\left[\bm{W}_{i}^{-\frac{1}{2}}\bm{S}_{i}^{k}\bm{W}_{i }\bm{L}_{i}\bm{W}_{i}\bm{S}_{i}^{k}\bm{W}_{i}^{-\frac{1}{2}}\right]\right) \right]^{\frac{1}{d}}\det(\bm{W}^{-1})^{\frac{1}{d}}.\]

Table 1 contains the (expected) communication complexities of \(\text{det-CGD1}\), \(\text{det-CGD2}\) and GD for several choices of \(\bm{W},\bm{D}\) and \(\bm{S}^{k}\). Here are a few comments about the table. We deduce that taking a matrix stepsize without compression (row 1) we improve GD (row 13). A careful analysis reveals that the result in row 5 is always worse than row 7 in terms of both communication and iteration complexity. However, the results in row 6 and row 7 are not comparable in general, meaning that neither of them is universally better. More discussion on this table can be found in the Appendix.

Compression for free.Now, let us focus on row 12, which corresponds to a sampling scheme where the \(i\)-th layer is independently selected with probability \(q_{i}\). Mathematically, it goes as follows:

\[\bm{T}_{i}^{k}=\frac{\eta_{i}}{q_{i}}\bm{I}_{d_{i}},\quad\text{ where}\quad\eta_{i}\sim\text{Bernoulli}(q_{i}).\] (17)

Jensen's inequality implies that

\[\left(\sum_{i=1}^{l}q_{i}d_{i}\right)\cdot\prod_{i=1}^{l}\left(\frac{1}{q_{i}} \right)^{\frac{d_{i}}{d}}\geq d.\] (18)

The equality is attained when \(q_{i}=q\) for all \(i\in[\ell]\). The expected bits transferred per iteration of this algorithm is then equal to \(k_{\exp}=qd\) and the communication complexity equals \(d\det(\bm{L})^{1/d}\). Comparing with the results for \(\text{det-CGD2}\) with \(\text{rand-}k_{\exp}\) on row 11 and using the fact that \(\det(\bm{L})\leq\det\left(\operatorname{diag}(\bm{L})\right)\), we deduce that the Bernoulli scheme is better than the uniform sampling scheme. Notice also, the communication complexity matches the one for the uncompressed \(\text{det-CGD2}\) displayed on row 9. This, in particular means that using the Bern-\(q\) sketches we can compress the gradients for free. The latter means that we reduce the number of bits broadcasted at each iteration without losing in the total communication complexity. In particular, when all the layers have the same width \(d_{i}\), the number of broadcasted bits for each iteration is reduced by a factor of \(q\).

## 5 Distributed setting

In this section we describe the distributed versions of our algorithms and present convergence guarantees for them. Let us consider an objective function that is sum decomposable:

\[f(x):=\frac{1}{n}\sum_{i=1}^{n}f_{i}(x),\]

where each \(f_{i}:\mathbb{R}^{d}\to\mathbb{R}\) is a differentiable function. We assume that \(f\) satisfies Assumption 1 and the component functions satisfy the below condition.

**Assumption 4**.: _Each component function \(f_{i}\) is \(\bm{L}_{i}\)-smooth and is bounded from below: \(f_{i}(x)\geq f_{i}^{\inf}\) for all \(x\in\mathbb{R}^{d}\)._

This assumption also implies that \(f\) is of matrix smoothness with \(\bar{\bm{L}}\in\mathbb{S}_{++}^{d}\), where \(\bar{\bm{L}}=\frac{1}{n}\sum_{i=1}^{n}\bm{L}_{i}\). Following the standard FL framework [16, 17, 18], we assume that the \(i\)-th component function \(f_{i}\) is stored on the \(i\)-th client. At each iteration, the clients in parallel compute and compress the local gradient \(\nabla f_{i}\) and communicate it to the central server. The server, then aggregates the compressed gradients, computes the next iterate, and in parallel broadcasts it to the clients. See the algorithms below for the pseudo-codes.

**Theorem 3**.: _Let \(f_{i}:\mathbb{R}^{d}\to\mathbb{R}\) satisfy Assumption 4 and let \(f\) satisfy Assumption 1 and Assumption 2 with smoothness matrix \(\bm{L}\). If the stepsize satisfies_

\[\bm{D}\bm{L}\bm{D}\preceq\bm{D},\] (19)

_then the following convergence bound is true for the iteration of Algorithm 1:_

\[\min_{0\leq k\leq K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|_{\frac{ \bm{D}}{\det(\bm{D})^{1/d}}}^{2}\right]\leq\frac{2(1+\frac{\lambda_{\bm{D}}}{n })^{K}\left(f(x^{0})-f^{\inf}\right)}{\det(\bm{D})^{1/d}\,K}+\frac{2\lambda_{ \bm{D}}\Delta^{\inf}}{\det(\bm{D})^{1/d}\,n},\] (20)

_where \(\Delta^{\inf}:=f^{\inf}-\frac{1}{n}\sum_{i=1}^{n}f_{i}^{\inf}\) and_

\[\lambda_{\bm{D}}:=\max_{i}\left\{\lambda_{\max}\left(\mathbb{E}\left[\bm{L}_{ i}^{\frac{1}{d}}\left(\bm{S}_{i}^{k}-\bm{I}_{d}\right)\bm{D}\bm{L}\bm{D}\left( \bm{S}_{i}^{k}-\bm{I}_{d}\right)\bm{L}_{i}^{\frac{1}{d}}\right]\right)\right\}.\]

The same result is true for Algorithm 2 with a different constant \(\lambda_{\bm{D}}\). The proof of Theorem 3 and its analogue for Algorithm 2 are presented in the Appendix. The analysis is largely inspired by [13, Theorem 1]. Now, let us examine the right-hand side of (20). We start by observing that the first term has exponential dependence in \(K\). However, the term inside the brackets, \(1+\lambda_{\bm{D}}/n\), depends on the stepsize \(\bm{D}\). Furthermore, it has a second-order dependence on \(\bm{D}\), implying that \(\lambda_{\alpha\bm{D}}=\alpha^{2}\lambda_{\bm{D}}\), as opposed to \(\det(\alpha\bm{D})^{1/d}\), which is linear in \(\alpha\). Therefore, we can choose a small enough coefficient \(\alpha\) to ensure that \(\lambda_{\bm{D}}\) is of order \(n/K\). This means that for a fixed number of iterations \(K\), we choose the matrix stepsize to be "small enough" to guarantee that the denominator of the first term is bounded. The following corollary summarizes these arguments, and its proof can be found in the Appendix.

**Corollary 1**.: _We reach an error level of \(\varepsilon^{2}\) in (20) if the following conditions are satisfied:_

\[\bm{D}\bm{L}\bm{D}\preceq\bm{D},\quad\lambda_{\bm{D}}\leq\min\left\{\frac{n }{K},\frac{n\varepsilon^{2}}{4\Delta^{\inf}}\det(\bm{D})^{1/d}\right\},\quad K \geq\frac{12(f(x^{0})-f^{\inf})}{\det(\bm{D})^{1/d}\,\varepsilon^{2}}.\] (21)

Proposition 2 in the Appendix proves that these conditions with respect to \(\bm{D}\) are convex. In order to minimize the iteration complexity for getting \(\varepsilon^{2}\) error, one needs to solve the following optimization problem

\[\begin{array}{ll}\text{minimize}&\log\det(\bm{D}^{-1})\\ \text{subject to}&\bm{D}\quad\text{satisfies}\quad\eqref{eq:constraint}.\end{array}\]

Choosing the optimal stepsize for Algorithm 1 is analogous to solving (13). One can formulate the distributed counterpart of Theorem 2 and attempt to solve it for different sketches. Furthermore, this leads to a convex matrix minimization problem involving \(\bm{D}\). We provide a formal proof of this property in the Appendix. Similar to the single-node case, computational methods can be employed using the CVXPY package. However, some additional effort is required to transform (21) into the disciplined convex programming (DCP) format.

The second term in (20) corresponds to the convergence neighborhood of the algorithm. It does not depend on the number of iteration, thus it remains unchanged, after we choose the stepsize.

Nevertheless, it depends on the number of clients \(n\). In general, the term \(\Delta^{\mathrm{inf}}/n\) can be unbounded, when \(n\to+\infty\). However, per Corollary 1, we require \(\lambda_{\bm{D}}\) to be upper-bounded by \(n/K\). Thus, the neighborhood term will indeed converge to zero when \(K\to+\infty\), if we choose the stepsize accordingly.

We compare our results with the existing results for DCGD. In particular we use the technique from [10] for the scalar smooth DCGD with scalar stepsizes. This means that the parameters of algorithms are \(\bm{L}_{i}=L_{i}\bm{I}_{d},\bm{L}=L\bm{I}_{d},\bm{D}=\gamma\bm{I}_{d},\omega= \lambda_{\max}\left(\mathbb{E}\left[\left(\bm{S}_{i}^{k}\right)^{\top}\bm{S}_{ i}^{k}\right]\right)-1\). One may check that (21) reduces to

\[\gamma\leq\min\left\{\frac{1}{L},\sqrt{\frac{n}{KL_{\max}L\omega}},\frac{n \varepsilon^{2}}{4\Delta^{\mathrm{inf}}L_{\max}L\omega}\right\}\quad\text{and }\quad K\gamma\geq\frac{12(f(x^{0})-f^{\mathrm{inf}})}{\varepsilon^{2}}\] (22)

As expected, this coincides with the results from [10, Corollary 1]. See the Appendix for the details on the analysis of [10]. Finally, we back up our theoretical findings with experiments. See Figure 1 for a simple experiment confirming that Algorithms 1 and 2 have better iteration and communication complexity compared to scalar stepsized DCGD. For more details on the experiments we refer the reader to the corresponding section in the Appendix.

## 6 Conclusion

### Limitations

It is worth noting that every point in \(\mathbb{R}^{d}\) can be enclosed within some volume 1 ellipsoid. To see this, let \(0\neq v\in\mathbb{R}^{d}\) and define \(\bm{Q}:=\frac{\alpha}{\left\lVert v\right\rVert^{2}}vv^{\top}+\beta\sum_{i=1} ^{d}v_{i}v_{i}^{\top}\), where \(v_{1}=\frac{v}{\left\lVert v\right\rVert},v_{2},\ldots,v_{d}\) form an orthonormal basis. The eigenvalues of \(\bm{Q}\) are \(\beta\) (with multiplicity \(d-1\)) and \(\alpha\) (with multiplicity \(1\)), so we have \(\det(\bm{Q})=\beta^{d-1}\alpha\leq 1\). Furthermore, we have \(\left\lVert v\right\rVert_{\bm{Q}}^{2}=v^{\top}\bm{Q}v=\alpha\left\lVert v \right\rVert^{2}\). By choosing \(\alpha=\frac{1}{\left\lVert v\right\rVert^{2}}\) and \(\beta=\left\lVert v\right\rVert^{2/(d-1)}\), we can obtain \(\det(\bm{Q})=1\) while \(\left\lVert v\right\rVert_{\bm{Q}}^{2}\leq 1\). Therefore, having the average \(\bm{D}\)-norm of the gradient bounded by a small number does not guarantee that the average Euclidean norm is small. This implies that the theory does not guarantee stationarity in the Euclidean sense.

### Future work

Matrix stepsize gradient methods are still not well studied and require further analysis. Although many important algorithms have been proposed using scalar stepsizes and are known to have good performance, their matrix analogs have yet to be thoroughly examined. The distributed algorithms proposed in Section 5 follow the structure of DCGD by [10]. However, other federated learning mechanisms such as MARINA, which has variance reduction [1], or EF21 by [10], which has powerful practical performance, should also be explored.

Figure 1: Comparison of standard DCGD, DCGD with matrix smoothness, D-det-CGD1 and D-det-CGD2 with optimal diagonal stepsizes under rand-\(1\) sketch. The stepsize for standard DCGD is determined using [10, Proposition 4], the stepsize for DCGD with matrix smoothness along with \(\bm{D}_{1}\), \(\bm{D}_{2}\) is determined using Corollary 1, the error level is set to be \(\varepsilon^{2}=0.0001\). Here \(G_{K,\bm{D}}:=\frac{1}{K}\big{(}\sum_{k=0}^{K-1}\left\lVert\nabla f(x^{k}) \right\rVert_{\bm{D}/\det(\bm{D})^{1/d}}^{2}\big{)}\).

## References

* [ABK07] Mehiddin Al-Baali and H Khalfan. An overview of some practical quasi-newton methods for unconstrained optimization. _Sultan Qaboos University Journal for Science [SQUJS]_, 12(2):199-209, 2007.
* [ABSM14] Mehiddin Al-Baali, Emilio Spedicato, and Francesca Maggioni. Broyden's quasi-Newton methods for a nonlinear system of equations and unconstrained optimization: a review and open problems. _Optimization Methods and Software_, 29(5):937-954, 2014.
* [AGL\({}^{+}\)17] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-efficient SGD via gradient quantization and encoding. _Advances in neural information processing systems_, 30, 2017.
* [B\({}^{+}\)15] Sebastien Bubeck et al. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* [Bro65] Charles G Broyden. A class of methods for solving nonlinear simultaneous equations. _Mathematics of computation_, 19(92):577-593, 1965.
* [CL11] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. _ACM transactions on intelligent systems and technology (TIST)_, 2(3):1-27, 2011.
* [DB16] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex optimization. _The Journal of Machine Learning Research_, 17(1):2909-2913, 2016.
* [DBA\({}^{+}\)20] Aritra Dutta, El Houcine Bergou, Ahmed M Abdelmoniem, Chen-Yu Ho, Atal Narayan Sahu, Marco Canini, and Panos Kalnis. On the discrepancy between the theoretical analysis and practical implementations of compressed communication for distributed deep learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 3817-3824, 2020.
* [DDG\({}^{+}\)22] Marina Danilova, Pavel Dvurechensky, Alexander Gasnikov, Eduard Gorbunov, Sergey Guminov, Dmitry Kamzolov, and Innokentiy Shibaev. Recent theoretical advances in non-convex optimization. In _High-Dimensional Optimization and Probability: With a View Towards Data Science_, pages 79-163. Springer, 2022.
* [DM77] John E Dennis, Jr and Jorge J More. Quasi-Newton methods, motivation and theory. _SIAM review_, 19(1):46-89, 1977.
* [DOG\({}^{+}\)19] Darina Dvinskikh, Aleksandr Ogaltsov, Alexander Gasnikov, Pavel Dvurechensky, Alexander Tyurin, and Vladimir Spokoiny. Adaptive gradient descent for convex and non-convex stochastic optimization. _arXiv preprint arXiv:1911.08380_, 2019.
* [GBLR21] Eduard Gorbunov, Konstantin P Burlachenko, Zhize Li, and Peter Richtarik. MARINA: Faster non-convex distributed learning with compression. In _International Conference on Machine Learning_, pages 3788-3798. PMLR, 2021.
* [GBY06] Michael Grant, Stephen Boyd, and Yinyu Ye. Disciplined convex programming. _Global optimization: From theory to implementation_, pages 155-210, 2006.
* [GCH\({}^{+}\)19] Boris Ginsburg, Patrice Castonguay, Oleksii Hrinchuk, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, Huyen Nguyen, Yang Zhang, and Jonathan M Cohen. Stochastic gradient methods with layer-wise adaptive moments for training of deep networks. _arXiv preprint arXiv:1905.11286_, 2019.
* [GLQ\({}^{+}\)19] Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter Richtarik. SGD: General analysis and improved rates. In _International Conference on Machine Learning_, pages 5200-5209. PMLR, 2019.
* [GNDG19] SV Guminov, Yu E Nesterov, PE Dvurechensky, and AV Gasnikov. Accelerated primal-dual gradient descent with linesearch for convex, nonconvex, and nonsmooth optimization problems. In _Doklady Mathematics_, volume 99, pages 125-128. Springer, 2019.

* [GR15] Robert M Gower and Peter Richtarik. Randomized iterative methods for linear systems. _SIAM Journal on Matrix Analysis and Applications_, 36(4):1660-1690, 2015.
* [GT74] William B Bragg and Richard A Tapia. Optimal error bounds for the Newton-Kantorovich theorem. _SIAM Journal on Numerical Analysis_, 11(1):10-13, 1974.
* [HHH\({}^{+}\)19] Samuel Horvath, Chen-Yu Ho, Ludovit Horvath, Atal Narayan Sahu, Marco Canini, and Peter Richtarik. Natural compression for distributed deep learning. _CoRR_, abs/1905.10988, 2019.
* [HKM\({}^{+}\)23] Samuel Horvath, Dmitry Kovalev, Konstantin Mishchenko, Peter Richtarik, and Sebastian Stich. Stochastic distributed learning with gradient quantization and double-variance reduction. _Optimization Methods and Software_, 38(1):91-106, 2023.
* [HMR18] Filip Hanzely, Konstantin Mishchenko, and Peter Richtarik. SEGA: Variance reduction via gradient sketching. _Advances in Neural Information Processing Systems_, 31, 2018.
* [IQR21] Rustem Islamov, Xun Qian, and Peter Richtarik. Distributed second order methods with fast rates and compressed communication. In _International conference on machine learning_, pages 4617-4628. PMLR, 2021.
* [JK\({}^{+}\)17] Prateek Jain, Purushottam Kar, et al. Non-convex optimization for machine learning. _Foundations and Trends(r) in Machine Learning_, 10(3-4):142-363, 2017.
* [JRSPS16] Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization. _Advances in neural information processing systems_, 29, 2016.
* [KFJ18] Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning with compressed gradients. _arXiv preprint arXiv:1806.06573_, 2018.
* [KMA\({}^{+}\)21] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* [KMY\({}^{+}\)16] Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. _arXiv preprint arXiv:1610.05492_, 2016.
* [KR20] Ahmed Khaled and Peter Richtarik. Better theory for SGD in the nonconvex world. _arXiv preprint arXiv:2002.03329_, 2020.
* [LBZR21] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtarik. PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In _International conference on machine learning_, pages 6286-6295. PMLR, 2021.
* [LKQR20] Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richtarik. Acceleration for compressed gradient descent in distributed and federated optimization. _arXiv preprint arXiv:2002.11364_, 2020.
* [MB11] Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. _Advances in neural information processing systems_, 24, 2011.
* [MGTR19] Konstantin Mishchenko, Eduard Gorbunov, Martin Takac, and Peter Richtarik. Distributed learning with compressed gradient differences. _arXiv preprint arXiv:1901.09269_, 2019.
* [Mie80] George J Miel. Majorizing sequences and error bounds for iterative methods. _Mathematics of Computation_, 34(149):185-202, 1980.

* [MMR\({}^{+}\)17] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2017.
* [MMSR22] Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtarik. ProxSkip: Yes! Local gradient steps provably lead to communication acceleration! Finally! In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 15750-15769. PMLR, 17-23 Jul 2022.
* [MSR22] Artavazd Maranjyan, Mher Safaryan, and Peter Richtarik. GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity. _arXiv preprint arXiv:2210.16402_, 2022.
* [RSF21] Peter Richtarik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A new, simpler, theoretically better, and practically faster error feedback. _Advances in Neural Information Processing Systems_, 34:4384-4396, 2021.
* [SHR21] Mher Safaryan, Filip Hanzely, and Peter Richtarik. Smoothness matrices beat smoothness constants: Better communication compression techniques for distributed optimization. _Advances in Neural Information Processing Systems_, 34:25688-25702, 2021.
* [SSR22] Mher Safaryan, Egor Shulgin, and Peter Richtarik. Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor. _Information and Inference: A Journal of the IMA_, 11(2):557-580, 2022.
* [Sti19] Sebastian U Stich. Unified optimal analysis of the (stochastic) gradient method. _arXiv preprint arXiv:1907.04232_, 2019.
* [WSR22] Bokun Wang, Mher Safaryan, and Peter Richtarik. Theoretically better and numerically faster distributed optimization with smoothness-aware quantization techniques. _Advances in Neural Information Processing Systems_, 35:9841-9852, 2022.
* [Yam87] Tetsuro Yamamoto. A convergence theorem for newton-like methods in banach spaces. _Numerische Mathematik_, 51:545-557, 1987.
* [YCN\({}^{+}\)15] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. _arXiv preprint arXiv:1506.06579_, 2015.
* [YHL\({}^{+}\)17] Adams Wei Yu, Lei Huang, Qihang Lin, Ruslan Salakhutdinov, and Jaime Carbonell. Block-normalized gradient method: An empirical study for training deep neural network. _arXiv preprint arXiv:1707.04822_, 2017.
* [ZCAW17] Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network decisions: Prediction difference analysis. _arXiv preprint arXiv:1702.04595_, 2017.
* [ZKV\({}^{+}\)20] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? _Advances in Neural Information Processing Systems_, 33:15383-15393, 2020.
* [ZTJY19] Qinghe Zheng, Xinyu Tian, Nan Jiang, and Mingqiang Yang. Layer-wise learning based stochastic gradient descent method for the optimization of deep convolutional neural network. _Journal of Intelligent & Fuzzy Systems_, 37(4):5641-5654, 2019.