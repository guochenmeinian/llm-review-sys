# Optimality of Message-Passing Architectures for Sparse Graphs

Aseem Baranwal

David R. Cheriton School of Computer Science

University of Waterloo, Waterloo, Canada

aseem.baranwal@uwaterloo.ca

&Kimon Fountoulakis

David R. Cheriton School of Computer Science

University of Waterloo, Waterloo, Canada

kimon.fountoulakis@uwaterloo.ca

&Aukosh Jagannath

Department of Statistics and Actuarial Science,

Department of Applied Mathematics,

David R. Cheriton School of Computer Science

University of Waterloo, Waterloo, Canada

a.jagannath@uwaterloo.ca

###### Abstract

We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is \(O(1)\) in the number of nodes, in the fixed-dimensional asymptotic regime, i.e., the dimension of the feature data is fixed while the number of nodes is large. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical convolution in the regime of high graph signal. Furthermore, we prove a corresponding non-asymptotic result.

## 1 Introduction

Graph Neural Networks (GNNs) have rapidly emerged as a powerful tool for learning on graph-structured data, where along with features of the entities, there also exists a relational structure among them. They have found numerous applications to a wide range of domains such as social networks (Backstrom and Leskovec, 2011), recommendation systems (Ying et al., 2018; Hao et al., 2020), chip design (Mirhoseini et al., 2021), bioinformatics (Scarselli et al., 2009; Zhang et al., 2021), computer vision (Monti et al., 2017), quantum chemistry (Gilmer et al., 2017), statistical physics (Battaglia et al., 2016; Bapst et al., 2020), and financial forensics (Zhang et al., 2017; Weber et al., 2019). Most of the success with these applications has been possible due to the advent of themessage-passing paradigm in GNNs, however, designing optimal GNN architectures for such a wide variety of applications still remains a challenging task.

In this work, we are interested in the node classification problem on very sparse feature-decorated graphs that are locally tree-like. We focus on the regime where the dimension of the node features is fixed and the number of nodes is large. Our motivation for considering this regime is that many major benchmark datasets for node classification appear to scale in this fashion. For example, in the popular Open Graph Benchmark collection (Hu et al., 2020), the medium and large-scale node-property prediction datasets have roughly \(10^{6}\) nodes (ogbn-products, ogbn-mag) to about \(10^{8}\) nodes (ogbn-papers100M), each with roughly \(10^{2}\) features. Graphs with such properties exist naturally in social, informational and biological networks; for motivational examples, see Stelzl et al. (2005), Adcock et al. (2013). We present a precise definition of optimality for node classification tasks on locally tree-like graphs in this scaling regime and compute the optimal classifier according to this definition, for a multi-class statistical data model where node features can have arbitrary continuous or discrete distributions. Subsequently, we show that a message-passing GNN architecture is able to realize the optimal classifier. Furthermore, we provide a theoretical analysis, comparing the generalization error of the optimal classifier with other architectures like GCN and simple MLPs. Our results support a recent work (Velickovic, 2022) in the context of classification on sparse graphs. In particular, we show that when node features are accompanied by sparse graphical side information, message-passing graph neural networks are able to realize the optimal classification scheme, and as such, there does not exist a better architecture beyond the message-passing paradigm.

Related Work.There has been a tremendous amount of work on GNN architecture design, where the most popular designs are based on a convolutional architecture, with each layer of the neural network performing a weighted convolution (averaging) operation with immediate neighbours, e.g., graph convolutional networks (GCN) (Kipf and Welling, 2017; Chen et al., 2020) or graph attention networks (GAT) (Velickovic et al., 2018). These architectures are known to have several limitations regarding their expressive power (see, for e.g., Li et al. (2018), Oono and Suzuki (2020), Balcilar et al. (2021), Xu et al. (2021), Keriven (2022)).

An interesting line of research consists of both theoretical and empirical works that attempt to address these limitations by developing an understanding of GNN architectures within the scope of message-passing (Rong et al., 2020; Liu et al., 2022; Maskey et al., 2022), as well as beyond it (Maron et al., 2019; Murphy et al., 2019; Chen et al., 2019). For example, Xu et al. (2018) propose an architecture with a technique called skip-connections, that flexibly leverages different ranges of neighbourhoods for each node to enable structure-awareness in node representations, Chen et al. (2020) propose a modification of the vanilla GCN with an initial residual that effectively relieves the problem of oversmoothing (Oono and Suzuki, 2020), and Keriven et al. (2021) study the universality of structural GNNs in the large random graph limit. However, this area of research still lacks a clear understanding of optimality in the context of graph learning problems, making it hard to design architectures for which a well-defined notion of optimality can be theoretically justified.

Several works have studied traditional message-passing GNN architectures like GCN and GAT using the binary contextual stochastic block model, see for example, Baranwal et al. (2021), Chien et al. (2022), Fountoulakis et al. (2022, 2022), Javaloy et al. (2022), Baranwal et al. (2023). These analyses rely heavily on two assumptions: first, the graph is not too sparse, i.e., for a graph with \(n\) nodes, the expected degree of a node is \(\Omega_{n}(\log^{2}n/n)\), and second, the node features are modelled as a Gaussian mixture. The work by Wei et al. (2022) is of particular interest to us, where the authors take a Bayesian inference perspective to investigate the functions of non-linearity in GNNs for binary node classification. They characterize the max-a-posterior estimation of a node label given the features of itself and its immediate neighbours. A similar perspective to that of Wei et al. (2022) is discussed in Gosch et al. (2023), where the latter authors derive insights into the robustness-accuracy trade-off in GNNs for node classification. In contrast to these inspiring works, we study the highly sparse regime where the expected degree of a node is \(O_{n}(1)\) and consider nodes beyond the immediate neighbours, at any fixed distance. (In fact, our non-asymptotic results allow distances of order \(c\log n\) for small enough \(c>0\), see Section3.5 below.) Furthermore, our main result holds for a general multi-class statistical model with arbitrary continuous or discrete feature distributions and arbitrary edge-connectivity probabilities between all pairs of classes.

Our Contributions.In this paper, we use a multi-class statistical model with arbitrary node features and edge-connectivity profiles among all pairs of classes to study the node classification problem in the regime where the graph component of the data is very sparse, i.e., the expected degree is \(O(1)\). The data model is described in Section3.2. We state the following main results and findings:

1. We introduce a family of graph neural network architectures that are asymptotically (in the number of nodes \(n\to\infty\)) Bayes optimal in a local sense for a general multi-class data model with arbitrary feature distributions. The optimality is stated precisely in Theorem1.
2. We analyze the architecture in the simpler two-class setting with Gaussian features, explicitly characterizing the generalization error in terms of the natural signal-to-noise ratio (SNR) in the data, and perform a comparative study against other learning methods analyzed using the same statistical model (Theorems 2 and 3). We find two key insights: * When the graph SNR is very low, the architecture reduces to a simple MLP that does not consider the graph, while if it is very high, our architecture reduces to a typical convolutional network that averages information from all nodes in the local neighbourhood. In the regime between the low and high SNRs, the architecture interpolates and performs better than both a simple MLP and a typical GCN. * If the information in the graph is larger than a threshold, then a simple convolution is able to perform better than all methods that do not utilize the graph. Not surprisingly, this threshold aligns with the Kesten-Stigum weak-recovery threshold for community detection in sparse networks (Massoulie, 2014; Mossel et al., 2018).
3. In the non-asymptotic setting with a fixed number of nodes, we show that even for a logarithmic depth, the neighbourhoods of an overwhelming fraction of nodes are tree-like with high probability. Subsequently, we show that the optimal classifier in the non-asymptotic setting obtains an error that is close to that incurred by the optimal classifier in the asymptotic setting. This is formalized in Theorem4.

Let us end this section by reiterating that we work in the fixed-dimensional regime. It is natural to wonder as to the performance of this architecture as compared to optimal algorithms in the high-dimensional setting. We present a numerical comparison of our work to the AMP-BP algorithm from Deshpande et al. (2018) in low and high-dimensional settings in AppendixB.

## 2 Architecture

This section explains the design of our GNN architecture for node classification. We perform two modifications to existing message-passing architectures. First, we decouple the layers in the neural network from the neighbourhood radius in the message-passing framework. This style of decoupled architecture has previously been studied, see for example, Nikolentzos et al. (2020); Feng et al. (2022); Baranwal et al. (2023). Second, we introduce a learnable parameter that models edge connectivity between each pair of classes and helps construct the messages to propagate. In the following, for any matrix \(M\), we denote row \(i\) of \(M\) by \(M_{i,:}\) and column \(i\) of \(M\) by \(M_{:,i}\).

Before stating our architecture, we need the following additional notation and pre-processing. Let \(\ell\geq 0\) and \(L>0\) be fixed integers. Let \(C\geq 2\) be the number of classes. For given data \((\mathbf{A},\mathbf{X})\) where \(\mathbf{A}\in\mathds{R}^{n\times n}\) is the adjacency matrix of an unweighted undirected graph, and \(\mathbf{X}\in\mathds{R}^{n\times d}\) is the node feature matrix, we perform a pre-computation on the graph to construct a tensor \(\tilde{\mathbf{A}}\) as follows:

\[\tilde{\mathbf{A}}^{(k)}=f(\mathbf{A}^{k})\wedge\left(\neg f\left(\sum_{m=0} ^{k-1}\mathbf{A}^{m}\right)\right)\text{ for }k\in\{1,\ldots,\ell\},\]

where \(f(M)\) for a matrix \(M\) returns the entry-wise flattened matrix with \(f(M)_{ij}=\mathds{1}(M_{ij}>0)\), and \((\wedge,\neg)\) denote the entry-wise bit-wise operators ('and', 'negation') respectively. Note here that \(\tilde{\mathbf{A}}^{(k)}\) is an \(n\times n\) binary matrix with \(\tilde{\mathbf{A}}^{(k)}_{uv}=1\) if and only if \(v\) is present in the distance \(k\) neighbourhood of \(u\) but not within the distance \((k-1)\) neighbourhood. The idea behind this pre-processing step is the following: for each node \(u\) and each \(k\in[\ell]\), we want to divide the radius \(\ell\) neighbourhood of \(u\) into \(\ell\) groups of nodes, where each group \(k\in[\ell]\) consists of nodes that are within discovered the neighbourhood at each distance from a given node. \(\tilde{\mathbf{A}}^{(k)}_{u:}\) models a non-backtracking walk of length \(k\) that considers new nodes in the distance-\(k\) neighbourhood that were not discovered.

We can now define the graph neural network architecture as follows.

**Architecture 1**.: Given input data \((\mathbf{A},\mathbf{X})\) where \(\mathbf{A}\in\{0,1\}^{n\times n}\) is the adjacency matrix and \(\mathbf{X}\in\mathds{R}^{n\times d}\) is the node feature matrix, define:

\[\mathbf{H}^{(0)}=\mathbf{X}, \mathbf{H}^{(l)}=\sigma_{l}(\mathbf{H}^{(l-1)}\mathbf{W}^{(l)}+ \mathbf{1}_{n}\mathbf{b}^{(l)})\text{ for }l\in[L],\] \[\mathbf{Q}=\mathrm{sigmoid}(\mathbf{Z}), \mathbf{M}^{(k)}_{u,i}=\max_{j\in[C]}\left\{\mathbf{H}^{(L)}_{u,j }+\log(\mathbf{Q}^{k}_{i,j})\right\}\text{ for }k\in[\ell],u\in[n],i\in[C].\]

Then the predicted label is given by \(\mathbf{\hat{y}}=\{\hat{y}_{u}\}_{u\in[n]}\), where

\[\hat{y}_{u}=\operatorname*{argmax}_{i\in[C]}\left(\mathbf{H}^{(L)}_{u,c}+\sum _{k=1}^{\ell}\tilde{\mathbf{A}}^{(k)}_{u,:}\mathbf{M}^{(k)}_{:,i}\right).\]

Let us pause here to comment on the interpretation of the terms arising in this architecture. Here, \(\mathbf{H}^{(L)}\) is viewed as the output of a simple \(L\)-layer MLP with \(\{\sigma_{l}\}_{l\in[L]}\) being a set of non-linear functions. We have \((\mathbf{W}^{(l)},\mathbf{b}^{(l)})_{l\in[L]}\) as the learnable parameters of this MLP, with suitable dimensions so that \(\mathbf{H}^{(L)}\in\mathds{R}^{n\times C}\). In addition, we introduce the learnable parameter \(\mathbf{Z}\in\mathds{R}^{C\times C}\) which is used to model edge connectivity among all pairs of classes. The quantity \(\tilde{\mathbf{A}}^{(k)}_{u,:}\mathbf{M}^{(k)}_{:,i}=\sum_{v\in[n]}\tilde{ \mathbf{A}}^{(k)}_{u,v}\mathbf{M}^{(k)}_{v,i}\) is viewed as the sum of messages \(\mathbf{M}^{(k)}_{v,i}\) passed by all distance \(k\) neighbours of node \(u\).

Although Architecture 1 follows the style of convolutional architectures like GCN and GAT for collecting messages within a local neighbourhood, the novelty lies in the construction of the messages \(\mathbf{M}\). Intuitively, \(\mathbf{Q}=\mathrm{sigmoid}(\mathbf{Z})\) learns the probabilities of edge connectivity between all pairs of classes so that \(\mathbf{Q}^{k}\) models the probability of observing a distance \(k\) path between a pair of nodes in two classes. To predict the label of node \(u\), the messages from other nodes \(v\) are constructed based on their features \(\mathbf{X}_{v}\), their distance \(k\) from node \(u\), and the path probabilities \(\mathbf{Q}^{k}\). We show in Theorem1 that this architecture is in a sense (made precise in Definition3.2) universally optimal among all node-classification schemes for sparse graphs. Our result thus aligns with the observations in Velickovic (2022), showing that optimal neural network architectures for node classification on sparse graphs are implementable using the message-passing paradigm.

## 3 Theoretical Analysis and Discussion

In this section, we present a theoretical analysis of the message-passing GNN given in Architecture 1. We begin by defining a natural notion of optimality in our setting and show that among local learning methods on graphs, Architecture 1 is optimal according to this definition on a very general statistical model. We then compute the generalization error and compare the architecture to other well-studied methods like a simple MLP and a GCN.

### Asymptotic Local Bayes Optimality

For classification tasks, it is natural to use a notion of generalization error in a "per sample" or online sense. Without graphical side information, the natural choice is the Bayes risk. With graphical information, however, there is an important obstruction: the number of samples is equal to the size of the corresponding graph. As such, a naive extension of the Bayes risk does not have this property.

A natural approach would be to consider the Bayes risk for estimators that take in the node, the data set, and the graph, i.e., \(\hat{y}_{v}=\hat{y}(v,(X,G))\). In this case, however, the risk necessarily implicitly depends on the sample size, \(n\), through \(G\). One might try to remove this dependence by taking the infinite sample size limit, but for a class of estimators this general, it is not clear that such a limit is well defined. To circumvent this issue, we restrict attention to node classifiers that are only allowed "local" information around the node. The large graph limit of the generalization error is then naturally interpreted via _local weak convergence_. (For the convenience of the reader, we briefly recall the notion of local weak convergence of sparse graphs in AppendixA.1. See also Ramanan (2021, Chapter 1) or Bordenave (2016, Section 3) for more detailed expositions.) In this limit, one can then interpret the generalization error as a per-sample error for the randomly rooted graph \((G,u)\) where \(u\) is a uniform random vertex in \(V(G)\). (Here and in the following a _rooted graph_ is a pair of a graph and a distinguished vertex, \(u\), called _the root._) With these observations, we are led to a natural notion of Bayes optimality, namely _asymptotic local Bayes optimality_ which we define presently.1

Footnote 1: It is also desirable for the empirical misclassification error to converge to the generalization error. If one works with the stronger notion of local convergence in probability, then this will hold as well. This later mode will hold in our examples but we leave this to future work.

Before turning to this definition, we must first recall the notion of \(\ell\)-_local classifiers_. For a node \(v\) in a graph \(G\), let \(\eta_{k}(v)=\{u\in V(G):d(u,v)\leq k\}\) denote the ball of radius \(k\) for the canonical graph distance metric.

**Definition 3.1** (\(\ell\)-local classifier).: Let \(G=(\mathbf{A},\mathbf{X})\) be a feature-decorated graph of \(n\) vertices with \(d\)-dimensional features \(\mathbf{X}_{u}\) for each vertex \(u\). For a fixed radius \(\ell>0\), an \(\ell\)-local node-classifier is a function \(f\) that takes as input a root vertex \(u\in[n]\), the adjacency matrix \(\mathbf{A}\) and the features of all nodes within the \(\ell\)-neighbourhood of \(u\), i.e., \(\{\mathbf{X}_{v}\}_{v\in\eta_{\ell}(u)}\)), and outputs a classification label for \(u\).

Suppose now that we have a sequence of (random) feature decorated graphs \((X_{n},G_{n})\) with \(|V(G)|=n\). Let \(u_{n}\) denote a uniform at random vertex in \(G_{n}\). Suppose finally that the rooted feature-decorated graphs \((X_{n},G_{n},u_{n})\) locally weakly converge to \((X,G,u)\). We can then define the notion of asymptotically \(\ell\)-locally Bayes optimal classifiers for this sequence of problems.

**Definition 3.2**.: We say that a classifier \(h_{\ell}^{*}\in\mathcal{C}_{\ell}\) is asymptotically \(\ell\)-locally Bayes optimal classifier of the root for the sequence \(\{(X_{n},G_{n},u_{n})\}\) if it minimizes the probability of misclassification of the root of the local weak limit, \((X,G,u)\), over the class \(\mathcal{C}_{\ell}\), i.e.,

\[h_{\ell}^{*}=\operatorname*{argmin}_{h\in\mathcal{C}_{\ell}}\mathbf{Pr}\left[ h(u,\{\mathbf{X}_{v}\}_{v\in\eta_{\ell}(u,G)})\neq y_{u}\right].\]

Before turning to our data model, we note here that the reader may ask whether or not the asymptotically \(\ell\)-locally Bayes optimal classifier is in any sense the limit of optimal \(\ell\)-local classifier of the random root, \(u_{n}\). We show this in an appropriate sense in Theorem 4.

### Data Model

Let us now turn to the data model that we use for our analysis. We work with the general multi-class contextual stochastic block model (CSBM) where each node belongs to one of \(C\) different classes labelled \(1,\ldots,C\), and the node features have arbitrary continuous or discrete distributions. This model with \(C=2\), along with a specialization to Gaussian features has been extensively studied in several works on (semi)-supervised node classification and unsupervised community detection, see, for example, Deshpande et al. (2018); Lu and Sen (2020); Baranwal et al. (2021); Wei et al. (2022); Fountoulakis et al. (2022); Baranwal et al. (2023). Informally, a CSBM consists of a coupling of a stochastic block model (SBM) Holland et al. (1983) with a mixture model where the components of the mixture have arbitrary distributions and are associated with the blocks of the SBM.

More formally, let \(n,d\) be positive integers such that \(n\) denotes the number of nodes and \(d\) denotes the dimension of the node features. Define \(y_{1},\ldots,y_{n}\in\{1,\ldots,C\}\) as the latent variables (class labels) to be inferred. We will assume that the latent variables have a uniform prior, i.e., \(y_{u}\sim\operatorname{Unif}(\{[C]\})\) for all \(u\). For the relational part of the data, we have an undirected unweighted graph of \(n\) nodes, \(G=(V,E)\) with adjacency matrix \(\mathbf{A}=(a_{uv})_{u,v\in[n]}\sim\operatorname{SBM}(n,\mathbf{Q})\), where \(\mathbf{Q}=\{q_{ij}\}\in[0,1]^{C\times C}\) is the edge-probability matrix, meaning that

\[\mathbf{Pr}(a_{uv}=1\mid y_{u}=i,y_{v}=j)=q_{ij}.\]

The node attributes, \(\mathbf{X}\in\mathbb{R}^{n\times d}\) are sampled from a mixture of \(C\) arbitrary continuous or discrete distributions, \(\mathds{P}=\{\mathds{P}_{i}\}_{i\in[C]}\), where corresponding to the \(y_{u}\), we have \(\mathbf{X}_{u}\sim\mathds{P}_{y_{u}}\) for all \(u\in[n]\).

We will view \(n\) as large and study the setting where \(d\) is fixed (does not grow with \(n\)). We note here that in previous related works Baranwal et al. (2021); Wei et al. (2022); Baranwal et al. (2023), crucial assumptions have been made about the distribution of the node features and the sparsity of the graph, i.e., \(q_{ij}=\Omega_{n}(\log^{2}n/n)\). In contrast, we work in the extremely sparse setting where \(q_{ij}=b_{ij}/n\) for constants \(b_{ij}>1\), so we write \(\mathbf{Q}=\mathbf{B}/n\) where \(\mathbf{B}=\{b_{ij}\}_{i,j\in[C]}\). Furthermore, the only assumption we need about the distributions \(\mathds{P}_{i}\) is that \(\mathds{P}_{i}\) are absolutely continuous with respect to some base measure, in which case their densities exist, denoted by \(\rho_{i}\). For ease of reading, we encourage the reader to consider the case where \(\mathds{P}_{i}\) are continuous or discrete, therefore, the base measure is simply the Lebesgue measure on \(\mathds{R}\) or the counting measure on \(\mathds{Z}\) respectively.

For a feature-decorated graph \(G=(\mathbf{A},\mathbf{X})=(\{a_{uv}\}_{u,v\in[n]},\{\mathbf{X}_{u}\}_{u\in n})\) sampled from the model described above, we say that \(G\sim\mathrm{CSBM}(n,d,\mathds{P},\mathbf{Q})\) or \(G\sim\mathrm{CSBM}(n,d,\mathds{P},\mathbf{B}/n)\).

### Optimal Classifier

We are now ready to state our first main result that characterizes the asymptotically \(\ell\)-locally Bayes optimal classifier on the CSBM data described in Section 3.2.

**Theorem 1** (Bayes optimal message-passing).: _For any \(\ell\geq 1\), the asymptotically \(\ell\)-locally Bayes optimal classifier of the root for the sequence \((G_{n},u_{n})\sim\mathrm{CSBM}(n,d,\mathds{P},\mathbf{Q})\) is_

\[h_{\ell}^{*}(u,\{\mathbf{X}_{v}\}_{v\in\eta_{\ell}(u)})=\operatorname*{argmax }_{i\in[C]}\big{\{}\log\rho_{i}(\mathbf{X}_{u})+\sum_{v\in\eta_{\ell}(u)\setminus \{u\}}\mathcal{M}_{i\,d(u,v)}(\mathbf{X}_{v})\big{\}},\]

_where \(\{\rho_{i}\}_{i\in[C]}\) are the densities associated with the distributions \(\mathds{P}_{i}\in\mathds{P}\), and_

\[\mathcal{M}_{ik}(\mathbf{x})=\max_{j\in[C]}\big{\{}\log\rho_{j}(\mathbf{x})+ \log\mathbf{Q}_{ij}^{k}\big{\}}\,.\]

Let us briefly discuss the meaning of Theorem 1. It states that universally among all \(\ell\)-local classifiers, \(h_{\ell}^{*}\) is asymptotically Bayes optimal for the sparse CSBM data. We view \(\mathcal{M}_{ik}(\mathbf{X}_{v})\) as the message gathered from node \(v\) that is distance \(k\) away from node \(u\). In particular, \(\mathcal{M}_{ik}(\mathbf{X}_{v})\) naturally maximizes the likelihood of observing node \(v\) in class \(j\) at distance \(k\) from node \(u\) in class \(i\), over all \(j\in[C]\). Furthermore, this optimal classifier is realizable using Architecture 1 (see for example, Lu et al. (2017, Theorem 1), where it is shown that any Lebesgue measurable function can be approximated arbitrarily closely by standard neural networks). Consequently, this result shows that in the sparse setting, the message-passing paradigm can realize the optimal node classification scheme irrespective of the distributions of the node features or the inter-class edge probabilities.

For an intuitive understanding of Theorem 1, it helps to consider two extreme cases. First, if \(\mathbf{Q}=p\mathbf{I}\) for some \(p\in[0,1]\), then the classifier reduces to a simple convolution, \(h_{\ell}^{*}(u)=\operatorname*{argmax}_{i\in[C]}\big{\{}\sum_{v\in\eta_{k}(u) }\log\rho_{i}(\mathbf{X}_{v})\big{\}}\). Second, if \(\mathbf{Q}=p\mathbf{1}\mathbf{1}^{\top}\), then \(q_{ij}=p\) for all \(i,j\in[C]\), meaning that the graph component of the data is Erdos-Renyi, and hence, completely uninformative for the purposes of node classification. In this case, the classifier reduces to \(h_{\ell}^{*}(u)=\operatorname*{argmax}_{i\in[C]}\big{\{}\log\rho_{i}(\mathbf{ X}_{u})\big{\}}\), i.e., it is optimal to look at only the features of node \(u\) to predict its label since the neighbourhood does not provide any meaningful information. We formalize this intuition later for a simpler case (see Theorem 3).

### Comparative Study

In this section, we perform a theoretical analysis of the classifier in Theorem 1 using a well-studied specialization of the CSBM data model described in Section 3.2. For ease of discussion, let us restrict ourselves to the setting where there are two classes. Formally, we have \(C=2\), and without loss of generality, the class labels \(y_{u}\in\{\pm 1\}\) for all \(u\in[n]\). The distributions of the node features are given by \(\mathbf{X}_{u}\sim\mathds{P}_{y_{u}}\) with corresponding density \(\rho_{y_{u}}\). Furthermore, \(\mathbf{Q}=\{q_{ij}\}\) is a \(2\times 2\) matrix with \(q_{ii}=p=a/n\) and \(q_{ij}=q=b/n\) with constants \(a>1,b\geq 0\) for classes \(i\neq j\). For a data sample \(G=(\mathbf{A},\mathbf{X})\) from this model, we write \(G\sim\mathrm{CSBM}(n,d,\{\mathds{P}_{\pm}\},\mathbf{Q})\) or \(G\sim\mathrm{CSBM}(n,d,\{\mathds{P}_{\pm}\},\frac{a}{n},\frac{b}{n})\). We also recognize the quantity associated with the signal-to-noise ratio (SNR) in the graph structure for this case, which is given by

\[\Gamma=\frac{|p-q|}{p+q}=\frac{|a-b|}{a+b}.\] (1)

Note that the quantity \(\Gamma\) has been recognized as the meaningful SNR in several related works where the underlying random graph model is the binary symmetric stochastic block model, for example, Baranwal et al. (2021); Fountoulakis et al. (2022); Wei et al. (2022); Baranwal et al. (2023).

Let us now state Theorem 1 in the case of two classes. For given input \(x\in\mathds{R}\) and \(c>0\), let \(\varphi(x,c)=\min(\max(x,-c),c)\) denote the value of \(x\) clipped between the range \([-c,c]\).

**Corollary 1.1** (Optimal classifier for binary symmetric CSBM).: _For any \(\ell\geq 1\), the asymptotically \(\ell\)-locally Bayes optimal classifier of the root for the sequence \((G_{n},u_{n})\sim\mathrm{CSBM}(n,d,\mathds{P},\frac{a}{n},\frac{b}{n})\) is_

\[h_{\ell}^{*}(u,\{\mathbf{X}_{v}\}_{v\in\mathcal{H}_{l}(u)})=\mathrm{sgn}\left( \psi(\mathbf{X}_{u})+\sum_{v\in\mathcal{H}_{u}(u)\setminus\{u\}}\mathcal{M}_{ d(u,v)}(\mathbf{X}_{v})\right),\]

_where \(\mathcal{M}_{k}(\mathbf{x})=\mathrm{sgn}(a-b)\cdot\varphi(\psi(\mathbf{x}),c _{k})\) with \(c_{k}=\log\left(\frac{1+\Gamma^{k}}{1-\Gamma^{k}}\right)\), and \(\psi(\mathbf{x})=\log\frac{\rho_{+}(\mathbf{x})}{\rho_{-}(\mathbf{x})}\)._

In this simplified setting, we note that the messages propagated from nodes in the \(\ell\)-local neighbourhood of node \(u\) are clipped proportional to a function of their distance \(k\) from node \(u\). In particular, the clip threshold \(c_{k}\) can be expressed in terms of the graph SNR \(\Gamma\) from (1). It is interesting to observe in Corollary 1.1 that \(c_{k}\) decreases rapidly as \(k\) increases. Since \(\Gamma<1\), this means that to predict the label of node \(u\), the value of the message propagated from node \(v\) at distance \(k\) from \(u\) decreases exponentially in \(k\).

The above simplification helps us interpret the classifier in terms of the graph SNR \(\Gamma\). We will now impose an assumption on the distribution of node features. This will help us analyze the generalization error in terms of the SNR in both the features and the graph, and enable us to compare the performance with other learning methods that are well-studied in the same statistical settings. We will resort to the setting where the features of the CSBM follow a Gaussian mixture. Note that this specialized statistical model has been studied extensively in previous works for benchmarking existing GNN architectures, see for example, Baranwal et al. (2021); Fountoulakis et al. (2022); Wei et al. (2022); Baranwal et al. (2023).

In principle, one could compute the generalization error of \(h_{\ell}^{*}\) for arbitrary distributions on the node features (see Appendix A.3.2), however, we report the error for Gaussian features for expository reasons. The generalization error is defined for a classifier \(h\) to be the probability of disagreement between the true label \(y_{u}\) and the output of the classifier \(h_{u}\) for node \(u\). We characterize the error for \(h_{\ell}^{*}\) in the case where \(\mathds{P}_{-},\mathds{P}_{+}\) correspond to the Gaussian mixture with components \(\mathcal{N}(-\boldsymbol{\mu},\sigma^{2}\mathbf{I})\) and \(\mathcal{N}(\boldsymbol{\mu},\sigma^{2}\mathbf{I})\) for fixed \(\boldsymbol{\mu}\in\mathds{R}^{d}\) and \(\sigma>0\)2.

Footnote 2: In principle, it is easy to analyze arbitrarily fixed means, say \(\boldsymbol{\mu},\boldsymbol{\nu}\) instead of keeping them symmetric, i.e., \(\pm\boldsymbol{\mu}\), with only minor changes in calculations.

In this case, a notion of the signal-to-noise ratio of the features naturally exists, i.e., \(\gamma=\left\|\boldsymbol{\mu}\right\|_{2}/\sigma\), a quantity proportional to the ratio of the distance between the means of the mixture and the standard deviation. The log-likelihood ratio in this setting is \(\psi(\mathbf{x})=\log\frac{\rho_{+}(\mathbf{x})}{\rho_{-}(\mathbf{x})}=\frac {2}{\sigma^{2}}\langle\mathbf{x},\boldsymbol{\mu}\rangle\).

Consider a sequence \(\{(G_{n},u_{n})\}_{n\geq 1}\) with \(G_{n}=(V(G_{n}),E(G_{n}))\) from this model where \(u_{n}\sim\mathrm{Unif}(V(G_{n}))\). In this setting, in the absence of features, it is known that \((G_{n},u_{n})\) converges locally weakly to a Poisson Galton-Watson tree (see for example, Mossel et al. (2015, Section 4)). Here, for every node, we additionally have features that are independent of the graph, and hence, as a straightforward consequence of Mossel et al. (2015, Section 4), \((G_{n},u_{n})\) in our case converges to a feature-decorated Poisson Galton-Watson tree \((G,u)\).

For the root node \(u\), let \(\alpha_{k}\) and \(\beta_{k}\) denote the number of children at generation \(k\) in class \(y_{u}\) and \(-y_{u}\) respectively, where \(y_{u}\) denotes the label of node \(u\). Then \(\{\alpha_{k}\}_{k\geq 0}\) and \(\{\beta_{k}\}_{k\geq 0}\) are characterized by

\[\alpha_{0} =1,\beta_{0}=0,\] \[\alpha_{k} \sim\mathrm{Poi}\left(\frac{a\alpha_{k-1}+b\beta_{k-1}}{2}\right),\beta_{k}\sim\mathrm{Poi}\left(\frac{a\beta_{k-1}+b\alpha_{k-1}}{2}\right) \text{ for }k\in[\ell].\] (2)

For a classifier \(h\) acting on \(G\), let \(\mathcal{E}(h)\) denote the probability of misclassification of the root \(u\) in \(G\), i.e., \(\mathcal{E}(h)=\mathbf{Pr}(h_{u}y_{u}<0)\). Correspondingly, in the case of finite \(n\), we denote by \(\mathcal{E}_{n}(h)\) the probability of misclassification of a uniform random node \(u_{n}\) in \(G_{n}\). We are now ready to state the generalization error of \(h_{\ell}^{*}\).

**Theorem 2** (Generalization error).: _For any \(\ell\geq 1\), the generalization error of the asymptotically \(\ell\)-locally Bayes optimal classifier of the root for the sequence \((G_{n},u_{n})\sim\mathrm{CSBM}(n,d,\mathds{P},\mathbf{Q})\) with Gaussian features is given by_

\[\mathcal{E}(h_{\ell}^{*})=\mathbf{Pr}\left[g+\frac{1}{2\gamma}\sum_{k\in[ \ell]}\left(\sum_{i\in[\alpha_{k}]}Z_{k,i}^{(a)}+\sum_{i\in[\beta_{k}]}Z_{k,i }^{(b)}\right)>\gamma\right],\]_where \(\alpha_{k},\beta_{k}\) are as in (2), \(Z_{k,i}^{(a)}=\varphi(-2\gamma^{2}+2\gamma g_{k,i},c_{k})\), \(Z_{k,i}^{(b)}=\varphi(2\gamma^{2}+2\gamma g_{k,i},c_{k})\), and \(g,\{g_{k,i}\}\) are mutually independent standard Gaussian random variables._

Let us now understand how the error described in Theorem2 behaves in terms of the two SNRs \(\gamma\) (for the features), and \(\Gamma\) (for the graph). Note that \(\mathcal{E}(h_{\ell}^{*})\to 0\) as \(\gamma\to\infty\), and \(\mathcal{E}(h_{\ell}^{*})\to\nicefrac{{1}}{{2}}\) as \(\gamma\to 0\). This means that if the signal in the features is large, the number of mistakes made by the classifier vanishes, while if the signal is very small, then roughly half of the nodes are misclassified (equivalent to making a uniform random guess for each node).

To see how \(\Gamma\) affects the error, we begin by looking at two extreme settings: first, where the graph is complete noise, i.e., \(\Gamma=0\), and second, where the graph signal is very strong, i.e., \(\Gamma\to 1\), followed by a discussion on how \(h_{\ell}^{*}\) interpolates between these extremes. Let \(\phi_{\pm}\) denote the Gaussian density functions with means \(\pm\bm{\mu}\) and variance \(\sigma^{2}\mathbf{I}_{d}\). Define the random variable

\[\xi_{\ell}=\xi_{\ell}(a,b)=\frac{1+\sum_{k=1}^{\ell}|\alpha_{k}-\beta_{k}|}{ \sqrt{1+\sum_{k=1}^{\ell}(\alpha_{k}+\beta_{k})}},\] (3)

where \(\alpha_{k},\beta_{k}\) follow (2). In the following, we denote the vanilla GCN classifier from Kipf and Welling (2017) by \(h_{\mathrm{gcn}}\). We then have the following result.

**Theorem 3** (Extreme graph signals).: _Let \(h_{\ell}^{*}\) be the classifier from Corollary1.1, \(h_{0}^{*}(u)=\operatorname{sgn}(\langle\mathbf{X}_{u},\bm{\mu}\rangle)\) be the Bayes optimal classifier given only the feature information of the root node \(u\), and \(h_{\mathrm{gcn}}\) be the one-layer vanilla GCN classifier. Then we have that for any fixed \(\ell\):_

1. _If_ \(\Gamma=0\) _then_ \(\mathcal{E}(h_{\ell}^{*})=\mathcal{E}(h_{0}^{*})=\Phi(-\gamma)\)_, where_ \(\Phi\) _is the standard Gaussian CDF._
2. _If_ \(\Gamma\to 1\) _then_ \(\xi_{\ell}\geq 1\) _a.s. and_ \(\mathcal{E}(h_{\ell}^{*})\to\mathbf{Pr}(g>\gamma\xi_{\ell})\)_, where_ \(g\sim\mathcal{N}(0,1)\)_._
3. \(\mathcal{E}(h_{\mathrm{gcn}})=\mathbf{Pr}\left(g>\gamma\xi_{1}\right)\)_._

Theorem3 shows that in the regime of extremely low graph SNR, the optimal classifier \(h_{\ell}^{*}\) reduces to a linear classifier \(h_{0}^{*}(u)=\operatorname{sgn}(\langle\mathbf{X}_{u},\bm{\mu}\rangle)\), which can be realized by a simple MLP that does not use the graph component of the data at all. On the other hand, in the regime of extremely strong graph SNR, \(h_{\ell}^{*}\) reduces to a simple convolution over all nodes in the \(\ell\)-neighbourhood and is comparable to a typical GCN. Furthermore, we note that in the strong graph SNR regime \(\Gamma\to 1\), \(\mathcal{E}(h_{\ell}^{*})\to\mathbf{Pr}(g>\gamma\xi_{\ell})\leq\Phi(-\gamma)\) since \(\xi_{\ell}\geq 1\). The clip operation during the propagation of messages makes things interesting between these two extremes, where \(h_{\ell}^{*}\) interpolates between a simple MLP and an \(\ell\)-hop convolutional network. This interpolation is characterized by the graph signal \(\Gamma\), since the messages are clipped in the range \([-c_{k},c_{k}]\), where \(c_{k}=\log\left(\frac{\frac{1+\Gamma^{k}}{1-\Gamma^{k}}}{\Gamma-\Gamma^{k}}\right)\).

In addition, Theorem3 concludes that if \(\xi_{1}(a,b)>1\), then a GCN can perform better than every classifier that does not see the graph. On the other hand, if \(\xi_{1}(a,b)<1\), then a GCN incurs more errors on the data than the best methods that do not use the graph. Interestingly, but not surprisingly, this result aligns with the Kesten-Stigum weak recovery threshold for the community-detection problem on the sparse stochastic block model (Massoulie, 2014; Mossel et al., 2018), meaning that if weak recovery is possible on the graph component of the data, then a GCN is able to exploit it to perform better than methods that do not use the graph, e.g., a simple MLP.

We now demonstrate our results through experiments using pytorch and pytorch-geometric (Fey and Lenssen, 2019). The following simulations are for the setting \(n=10000\) and \(d=4\) for binary classification on the CSBM. We implement Architecture1 for the binary case, and perform full-batch training on a graph sampled from the CSBM with certain signals (mentioned in the figures), followed by an evaluation of the architecture on a new graph sampled from the same distribution.

In Fig.1, we show that the accuracy obtained by the optimal classifier is higher than both a simple MLP and a vanilla GCN (Kipf and Welling, 2017). We plot the test accuracy of Architecture1 against the SNR in the node features, \(\gamma=\left\lVert\bm{\mu}\right\rVert/\sigma\) in Fig.1a, and against the graph SNR \(\Gamma=|a-b|/(a+b)\) in Fig.1b. We fix \(\Gamma=0.42\) and \(\gamma=1\) for the two plots, respectively. We chose these specific values because they generate relatively clearer plots where the accuracy metrics for the three architectures are easily visible and distinguished from each other. The results are similar for other values for \(\Gamma\) and \(\gamma\), i.e., the Bayes optimal architecture is superior to both MLP and GCN.

Furthermore, Fig. 2 shows that as claimed in Theorem 3, when the graph signal is at the extremes, i.e., \(\Gamma=0\) and \(\Gamma=1\), Architecture 1 behaves like a simple MLP and performs a typical convolution (averaging) over all nodes in the \(\ell\)-neighbourhood, respectively. In the regime of poor graph SNR, i.e., \(\Gamma=0\), a GCN is worse than a simple MLP, as inferred from part three of Theorem 3.

Finally, we observe that for the binary setting when the parameters of the architecture are initialized uniformly at random, gradient descent converges and the neural network learns the right parameters such that Architecture 1 realizes the optimal classifier in Corollary 1. This convergence, along with a comparison of our architecture to the Approximate Message-passing Belief Propagation (AMP-BP) algorithm from Deshpande et al. (2018) is presented in Appendix B.

### Non-asymptotic Setting

We now turn to the non-asymptotic regime and argue that for fixed \(n\), the classifier in Corollary 1.1 is still in a formal sense, Bayes optimal for an overwhelming fraction of nodes. We begin by exploiting the fact that for up to logarithmic depth neighbourhoods, a sparse CSBM graph is tree-like.

**Proposition 3.1** (Tree neighbourhoods).: _Let \(G=(V,E)\sim\mathrm{CSBM}(n,d,\mathds{P},\frac{a}{n},\frac{b}{n})\) for constants \(a,b>1\). Then for any \(\ell=c\log n\) such that \(c\log((a+b)/2)<1/4\), with probability \(1-O(1/\log^{2}n)\), the number of nodes \(u\in V\) whose \(\ell\)-neighbourhood is cycle-free is \(n(1-o(\log^{4}(n)/\sqrt{n}))\)._

In particular, Proposition 3.1 states that for \(\ell=c\log n\) for a suitable constant \(c\), the \(\ell\)-neighbourhood of an overwhelming fraction of nodes is a tree. This implies that the classifier \(h_{\ell}^{*}\) is Bayes optimal for roughly all of the nodes. Moreover, since the diameter of a sparse graph (as in our setting) is \(O(\log n)\) almost surely (Chung and Lu, 2001, Theorem 6), any learning mechanism can only look as far as \(O(\log n)\)-hops away from a node to gather new information. This shows that for such graphs, GNNs that are not very deep and look at only up to logarithmic distance in the neighbourhood are sufficient.

Let us now turn to the misclassification error in the non-asymptotic setting. Recall that for a classifier \(h\in\mathcal{C}_{\ell}\), we denote by \(\mathcal{E}_{n}(h)\) and \(\mathcal{E}(h)\) the misclassification error of \(h\) on the data model with \(n\) nodes,

Figure 1: Comparison of Architecture 1 against an MLP and a vanilla GCN (Kipf and Welling, 2017).

Figure 2: Demonstration of Theorem 3 for extreme graph signals. In the case where \(\Gamma=0\), the architecture reduces to an MLP (Fig. 1(a)), while if \(\Gamma=1\), it behaves the same as a GCN (Fig. 1(b)).

and on the limiting data model with \(n\to\infty\), respectively. Furthermore, recall from Corollary1.1 that \(\min_{h\in\mathcal{C}_{\ell}}\mathcal{E}(h)=\mathcal{E}(h_{\ell}^{*})\). Our next result shows that the optimal misclassification error in the non-asymptotic setting across all \(\ell\)-local classifiers, i.e., \(\min_{h\in\mathcal{C}_{\ell}}\mathcal{E}_{n}(h)\), is close to the misclassification error obtained in the non-asymptotic setting by \(h_{\ell}^{*}\). Moreover, \(\min_{h\in\mathcal{C}_{\ell}}\mathcal{E}_{n}(h)\) is also close to \(\mathcal{E}(h_{\ell}^{*})\) which is explicitly computed in Theorem2.

**Theorem 4** (Misclassification error for fixed \(n\)).: _For any \(1\leq\ell\leq c\log n\) such that the positive constant \(c\) satisfies \(c\log(\frac{a+b}{2})<1/4\), we have that_

\[\left|\min_{h\in\mathcal{C}_{\ell}}\mathcal{E}_{n}(h)-\mathcal{E}_{n}(h_{\ell }^{*})\right|=O\left(\frac{1}{\log^{2}n}\right),\quad\left|\min_{h\in\mathcal{ C}_{\ell}}\mathcal{E}_{n}(h)-\mathcal{E}(h_{\ell}^{*})\right|=O\left(\frac{1}{ \log^{2}n}\right).\]

Recall that Corollary1.1 implies that \(h_{\ell}^{*}\) performs optimally on the limiting data model (asymptotic setting) among the class of \(\ell\)-local classifiers \(\mathcal{C}_{\ell}\), but it may not be optimal for the non-asymptotic data model where we have a finite feature-decorated graph with \(n\) nodes. However, Theorem4 helps us conclude that even in the non-asymptotic setting, \(h_{\ell}^{*}\) performs almost as well as the actual optimal classifier among \(\mathcal{C}_{\ell}\) in this case, as long as we compare with classifiers that can only look at moderate logarithmic depths in the local neighbourhood, i.e., \(\ell\leq c\log n\) for a suitable \(c\).

## 4 Conclusion and Future Work

In this work, we present a comprehensive theoretical characterization of the Bayes optimal node classification architecture for sparse feature-decorated graphs and show that it can be realized using the message-passing framework. Utilizing a well-established and well-studied statistical model, we interpret its performance in terms of the SNR in the data and validate our findings through empirical analysis of synthetic data. Additionally, we identify the following limitations as prospects for future work: (1) We consider neighbourhoods up to distance \(\ell=c\log n\) for a small enough \(c\). Extending \(\ell\) to the graph's diameter (known to be \(O(\log n)\) with high probability) by removing the restriction on \(c\) poses challenges due to the presence of cycles. (2) More insights can be provided through experiments on real data to benchmark the architecture in cases where we have a significant gap between the theoretical assumptions (sparse and locally tree-like graph) and the real-world data.

A.J. acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Canada Research Chairs programme. Cette recherche a ete enterprise grace, en partie, au soutien financier du Conseil de Recherches en Sciences Naturelles et en Genie du Canada (CRSNG), [RGPIN-2020-04597, DGECR-2020-00199], et du Programme des chaines de recherche du Canada.

K. Fountoulakis would like to acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC). Cette recherche a ete financee par le Conseil de recherches en sciences naturelles et en genie du Canada (CRSNG), [RGPIN-2019-04067, DGECR-2019-00147].

## References

* Adcock et al. (2013) A. B. Adcock, B. D. Sullivan, and M. W. Mahoney. Tree-like structure in large social and information networks. In _2013 IEEE 13th International Conference on Data Mining_, pages 1-10, 2013. doi: 10.1109/ICDM.2013.77.
* Backstrom and Leskovec (2011) L. Backstrom and J. Leskovec. Supervised random walks: predicting and recommending links in social networks. In _Proceedings of the fourth ACM international conference on Web search and data mining_, pages 635-644, 2011.
* Balcilar et al. (2021) M. Balcilar, G. Renton, P. Heroux, B. Gauzere, S. Adam, and P. Honeine. Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective. In _International Conference on Learning Representations_, 2021.
* Bapst et al. (2020) V. Bapst, T. Keck, A. Grabska-Barwinska, C. Donner, E. D. Cubuk, S. S. Schoenholz, A. Obika, A. W. Nelson, T. Back, D. Hassabis, et al. Unveiling the predictive power of static structure in glassy systems. _Nature Physics_, 16(4):448-454, 2020.
* Bettett et al. (2019)A. Baranwal, K. Fountoulakis, and A. Jagannath. Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proc. of Mach. Learn. Res._, pages 684-693. PMLR, 18-24 Jul 2021.
* Baranwal et al. (2023) A. Baranwal, K. Fountoulakis, and A. Jagannath. Effects of Graph Convolutions in Multi-layer Networks. In _The Eleventh International Conference on Learning Representations_, 2023.
* Barbour and Chen (2005) A. D. Barbour and L. H. Y. Chen. _An introduction to Stein's method_, volume 4. World Scientific, 2005.
* Battaglia et al. (2016) P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, and K. Kavukcuoglu. Interaction Networks for Learning about Objects, Relations and Physics. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2016.
* Bordenave (2016) C. Bordenave. Lecture notes on random graphs and probabilistic combinatorial optimization, 2016.
* 545, 1975.
* Chen et al. (2020) M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li. Simple and Deep Graph Convolutional Networks. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 1725-1735. PMLR, 13-18 Jul 2020.
* Chen et al. (2019) Z. Chen, S. Villar, L. Chen, and J. Bruna. On the equivalence between graph isomorphism testing and function approximation with GNNs. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Chien et al. (2022) E. Chien, W.-C. Chang, C.-J. Hsieh, H.-F. Yu, J. Zhang, O. Milenkovic, and I. S. Dhillon. Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction. In _International Conference on Learning Representations_, 2022.
* Chung and Lu (2001) F. Chung and L. Lu. The diameter of sparse random graphs. _Advances in Applied Mathematics_, 26(4):257-279, 2001.
* Deshpande et al. (2018) Y. Deshpande, S. Sen, A. Montanari, and E. Mossel. Contextual Stochastic Block Models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* Dreier et al. (2018) J. Dreier, P. Kuinke, B. Le Xuan, et al. Local Structure Theorems for Erdos-Renyi Graphs and Their Algorithmic Applications. _SOFSEM 2018: Theory and Practice of Computer Science LNCS 10706_, page 125, 2018.
* Feng et al. (2022) J. Feng, Y. Chen, F. Li, A. Sarkar, and M. Zhang. How powerful are k-hop message passing graph neural networks. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Fey and Lenssen (2019) M. Fey and J. E. Lenssen. Fast Graph Representation Learning with PyTorch Geometric. In _ICLR Workshop on Representation Learning on Graphs and Manifolds_, 2019.
* Fountoulakis et al. (2022) K. Fountoulakis, D. He, S. Lattanzi, B. Perozzi, A. Tsitsulin, and S. Yang. On classification thresholds for graph attention with edge features. _arXiv preprint arXiv:2210.10014_, 2022a.
* Fountoulakis et al. (2022) K. Fountoulakis, A. Levi, S. Yang, A. Baranwal, and A. Jagannath. Graph Attention Retrospective. _arXiv preprint arXiv:2202.13060_, 2022b.
* Gilmer et al. (2017) J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural Message Passing for Quantum Chemistry. In _Proceedings of the 34th International Conference on Machine Learning_, 2017.
* Gosch et al. (2023) L. Gosch, D. Sturm, S. Geisler, and S. Gunnemann. Revisiting robustness in graph machine learning. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=h1o7Ry9Zctm.

J. Hao, T. Zhao, J. Li, X. L. Dong, C. Faloutsos, Y. Sun, and W. Wang. P-companion: A principled framework for diversified complementary product recommendation. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 2517-2524, 2020.
* Holland et al. [1983] P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. _Social networks_, 5(2):109-137, 1983.
* Hu et al. [2020] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. In _Advances in Neural Information Processing Systems_, 2020.
* Javaloy et al. [2022] A. Javaloy, P. S. Martin, A. Levi, and I. Valera. Learnable graph convolutional attention networks. In _Has it Trained Yet? NeurIPS 2022 Workshop_, 2022.
* Keriven [2022] N. Keriven. Not too little, not too much: a theoretical analysis of graph (over)smoothing. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Keriven et al. [2021] N. Keriven, A. Bietti, and S. Vaiter. On the universality of graph neural networks on large random graphs. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 6960-6971. Curran Associates, Inc., 2021.
* Kipf and Welling [2017] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations (ICLR)_, 2017.
* Li et al. [2018] Q. Li, Z. Han, and X.-M. Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In _Thirty-Second AAAI conference on artificial intelligence_, 2018.
* Liu et al. [2022] S. Liu, S. Jing, T. Zhao, Z. Huang, and D. Wu. Enhancing Multi-hop Connectivity for Graph Convolutional Networks. In _First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022_, 2022.
* Lu and Sen [2020] C. Lu and S. Sen. Contextual Stochastic Block Model: Sharp Thresholds and Contiguity. _ArXiv_, 2020. arXiv:2011.09841.
* Lu et al. [2017] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of neural networks: A view from the width. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Maron et al. [2019] H. Maron, H. Ben-Hamu, N. Shamir, and Y. Lipman. Invariant and equivariant graph networks. In _International Conference on Learning Representations_, 2019.
* Maskey et al. [2022] S. Maskey, R. Levie, Y. Lee, and G. Kutyniok. Generalization analysis of message passing neural networks on large random graphs. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Massoulie [2014] L. Massoulie. Community Detection Thresholds and the Weak Ramanujan Property. In _Proceedings of the Forty-Sixth Annual ACM Symposium on Theory of Computing_, page 694-703, 2014.
* Mirhoseini et al. [2021] A. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori, S. Wang, Y.-J. Lee, E. Johnson, O. Pathak, A. Nazi, et al. A graph placement methodology for fast chip design. _Nature_, 594(7862):207-212, 2021.
* Monti et al. [2017] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, July 2017.
* Mossel et al. [2015] E. Mossel, J. Neeman, and A. Sly. Reconstruction and estimation in the planted partition model. _Probability Theory and Related Fields_, 162:431-461, 2015.
* Mossel et al. [2018] E. Mossel, J. Neeman, and A. Sly. A proof of the block model threshold conjecture. _Combinatorica_, 38(3):665-708, 2018.
* Mossel et al. [2019]R. Murphy, B. Srinivasan, V. Rao, and B. Ribeiro. Relational pooling for graph representations. In K. Chaudhuri and R. Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 4663-4673. PMLR, 09-15 Jun 2019.
* Nikolentzos et al. (2020) G. Nikolentzos, G. Dasoulas, and M. Vazirgiannis. k-hop graph neural networks. _Neural Networks_, 130:195-205, 2020.
* Oono and Suzuki (2020) K. Oono and T. Suzuki. Graph Neural Networks Exponentially Lose Expressive Power for Node Classification. In _International Conference on Learning Representations_, 2020.
* Ramanan (2021) K. Ramanan. CRM-PIMS Summer School 2021: Background Material For Mini Course on Asymptotics of Interacting Stochastic Processes on Sparse Graphs, 2021.
* Rong et al. (2020) Y. Rong, W. Huang, T. Xu, and J. Huang. DropEdge: Towards Deep Graph Convolutional Networks on Node Classification. In _International Conference on Learning Representations_, 2020.
* Scarselli et al. (2009) F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The Graph Neural Network Model. _IEEE Transactions on Neural Networks_, 20(1), 2009.
* Stein (1972) C. Stein. A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In _Proc. Sixth Berkeley Symp. Math. Stat. Prob._, pages 583-602, 1972.
* Stelzl et al. (2005) U. Stelzl, U. Worm, M. Lalowski, C. Haenig, F. H. Brembeck, H. Goehler, M. Stroedicke, M. Zenkner, A. Schoenherr, S. Koeppen, et al. A human protein-protein interaction network: a resource for annotating the proteome. _Cell_, 122(6):957-968, 2005.
* Velickovic (2022) P. Velickovic. Message passing all the way up. In _ICLR 2022 Workshop on Geometrical and Topological Representation Learning_, 2022.
* Velickovic et al. (2018) P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph Attention Networks. In _International Conference on Learning Representations_, 2018.
* Velickovic (2022) P. Velickovic. Message passing all the way up, 2022.
* Vershynin (2018) R. Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_, volume 47. Cambridge University Press, 2018.
* Weber et al. (2019) M. Weber, G. Domeniconi, J. Chen, D. K. I. Weidele, C. Bellei, T. Robinson, and C. E. Leiserson. Anti-money laundering in bitcoin: Experimenting with graph convolutional networks for financial forensics. _arXiv preprint arXiv:1908.02591_, 2019.
* Wei et al. (2022) R. Wei, H. Yin, J. Jia, A. R. Benson, and P. Li. Understanding Non-linearity in Graph Neural Networks from the Bayesian-Inference Perspective. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Xu et al. (2018) K. Xu, C. Li, Y. Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka. Representation Learning on Graphs with Jumping Knowledge Networks. In J. Dy and A. Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 5453-5462. PMLR, 10-15 Jul 2018.
* Xu et al. (2021) K. Xu, M. Zhang, J. Li, S. S. Du, K.-I. Kawarabayashi, and S. Jegelka. How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks. In _International Conference on Learning Representations_, 2021.
* Ying et al. (2018) R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec. Graph Convolutional Neural Networks for Web-Scale Recommender Systems. _KDD '18: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 974-983, 2018.
* Zhang et al. (2017) S. Zhang, D. Zhou, M. Y. Yildirim, S. Alcorn, J. He, H. Davulcu, and H. Tong. Hidden: hierarchical dense subgraph detection with application to financial fraud detection. In _Proceedings of the 2017 SIAM International Conference on Data Mining_, pages 570-578. SIAM, 2017.
* Zhang et al. (2021) X.-M. Zhang, L. Liang, L. Liu, and M.-J. Tang. Graph neural networks and their current applications in bioinformatics. _Frontiers in genetics_, 12:690049, 2021.
* Zhang et al. (2018)Supplementary Discussion and Proofs

### Local Weak Convergence

We briefly recall here the notion of local weak convergence of random rooted graphs. The notion of local weak convergence of random, feature-decorated, rooted graphs is defined analogously.

Let us begin first with the case of rooted graphs. A rooted graph \((G,u)=((E,V),u)\) is a graph \(G\) with a distinguished vertex \(u\) called the root. We say that two rooted graphs \((G_{1},u_{1})=((E_{1},V_{1}),u_{1})\), \((G_{2},u_{2})=((E_{2},V_{2}),u_{2})\) are isomorphic if there is a bijection \(\phi:V_{1}\to V_{2}\) such that \(\phi(u)=v\) and such that if \((x,y)\in E_{1}\) then \((\phi(x),\phi(y))\in E_{2}\). In this case we write \((G_{1},u_{1})\cong(G_{2},u_{2})\). For a rooted graph \((G,u)\), we denote its isomorphism class as \([(G,u)]\).

Let \(\mathcal{G}_{*}\) denote the set of isomorphism classes of (locally finite) rooted graphs. For a vertex \(v\in G\) we let \(\eta_{k}(v)\) denote the collection of neighbours of \(v\) of distance at most \(k\) in the canonical edge distance metric, and \(G[\eta_{k}(v)]\) denote the subgraph induced on this collection of vertices. We then have the notion of _local convergence_ in \(\mathcal{G}_{*}\).

**Definition A.1** (Local convergence on \(\mathcal{G}_{*}\)).: We say that a sequence \([(G_{n},u_{n})]\in\mathcal{G}_{*}\)_converges locally_ to \([(G,u)]\in\mathcal{G}_{*}\) if for each \(k>0\), we have that \(G[\eta_{k}(u_{n})]\cong G[\eta_{k}(u)]\) eventually.

It can be shown [1, Lemma 3.4] that \(\mathcal{G}_{*}\) equipped with the topology of local convergence is a Polish space. We are then in the position to define local weak convergence on \(\mathcal{G}_{*}\). In brief, the topology of local weak convergence of random graphs is the topology of weak convergence of measures on the space of probability measures on \(\mathcal{G}_{*}\), namely \(\mathcal{M}_{1}(G_{*})\).

**Definition A.2** (local weak convergence of rooted graphs).: A sequence \(\{[G_{n},u_{n}]\}\) of random rooted graphs with corresponding laws \(\{\mu_{n}\}\subseteq\mathcal{M}_{1}(\mathcal{G}_{*})\) is said to locally weakly converge to a random rooted graph \([G,u]\) with law \(\mu\in\mathcal{M}_{1}(\mathcal{G}_{*})\) if \(\mu_{n}\to\mu\) weakly.

We note here that it is common also to talk about the notion of local weak convergence of a sequence of (finite) random graphs \(G_{n}\). In this case \(G_{n}\to G\) locally weakly if \([(G_{n},u_{n})]\to[(G,u)]\) locally weakly where \(u_{n}\sim\mathrm{Unif}(V(G_{n}))\).

### Preliminary Results

In this section, we state two important preliminary results and a fact that are used to establish our results in the paper.

**Lemma A.1**.: _[_1_, Lemma 3]__. Let \(G\sim\mathrm{CSBM}(n,d,\{\mathbf{P}_{-},\mathbf{P}_{+}\},\frac{a}{n},\frac{b}{ n})\) and \(r\) be a fixed constant. Then the probability that there exists an \(r\)-neighbourhood in \(G\) with \(m\) more edges than the number of vertices is bounded as follows:_

\[\mathbf{Pr}(\exists\ G_{r}\subset G\ \text{s.t.}\ |E(G_{r})|\geq|V(G_{r})|+m) \leq\frac{(2(r(m+1))^{2}(a+b))^{2r(m+1)+m}}{n^{m}}.\]

**Lemma A.2**.: _[_1_, Lemma 4.2]__. Assume \(\ell=c\log(n)\) with \(c\log\Delta<1/4\), where \(\Delta=(a+b)/2\). Then with high probability, no node \(i\) has more than one cycle in its \(\ell\)-neighbourhood. Moreover, for any \(m>0\), with probability at least \(1-O(1/m^{2})\) the number of nodes \(i\) whose \(\ell\)-neighbourhood contains at least one cycle is bounded by \(O(m\log^{3}(n)\Delta^{2\ell})\)._

**Fact A.3**.: _For any non-negative \(u,v,x,y\) such that \(x,y\leq 1\),_

\[xu+yv-\frac{1}{2}\left(xu+yv\right)^{2}\leq 1-\left(1-x\right)^{u}\left(1-y \right)^{v}\leq xu+yv.\]

### Bayes Optimal Classifier

In this section, we compute the asymptotically \(\ell\)-locally Bayes optimal classifier for the general CSBM described in Section 3.2 and establish Theorem 1, followed by a proof of Corollary 1.1. Next, we compute the generalization error for the two-class case with arbitrary node features.

#### a.3.1 Computing the Classifier

For the proofs, we introduce the notation \(N_{k}(u)\) for a given graph to mean the set of vertices that are at a distance of exactly \(k\) from node \(u\) in the graph. Thus, \(\eta_{k}(u)=\{u\}\cup_{j=1}^{k}N_{k}(u)\).

**Theorem** (Restatement of Theorem 1).: _For any \(\ell\geq 1\), the asymptotically \(\ell\)-locally Bayes optimal classifier of the root for the sequence \((G_{n},u_{n})\sim\mathrm{CSBM}(n,d,\mathrm{P},\mathbf{Q})\) is_

\[h_{\ell}^{*}(u,\{\mathbf{X}_{v}\}_{v\in\mathcal{H}(u)})=\operatorname*{argmax} _{i\in[C]}\big{\{}\log\rho_{i}(\mathbf{X}_{u})+\sum_{v\in\mathcal{H}(u)\setminus \{u\}}\mathcal{M}_{i\,d(u,v)}(\mathbf{X}_{v})\big{\}},\]

_where \(\{\rho_{i}\}_{i\in[C]}\) are the densities associated with the distributions \(\mathrm{P}_{i}\in\mathrm{P}\), and_

\[\mathcal{M}_{ik}(\mathbf{x})=\max_{j\in[C]}\big{\{}\log\rho_{j}(\mathbf{x})+ \log\mathbf{Q}_{ij}^{k}\big{\}}\,.\]

_Furthermore, there exists a choice of parameters for Architecture 1 such that it realizes \(h_{\ell}^{*}\)._

Proof.: We begin by writing the MAP estimation for this problem. Note that the features \(\mathbf{X}_{v}\in\mathds{R}^{d}\) for every node \(v\in[n]\) follow the law \(\mathrm{P}_{i}\) if \(y_{v}=i\in[C]\). In addition, recall that we have the edge-probability matrix \(\mathbf{Q}=\{q_{ij}\}_{i,j\in[C]}\) with \(\mathbf{Pr}((u,v)\) is an edge \(\mid y_{u}=i,y_{v}=j)=q_{ij}\), where \(q_{ij}=b_{ij}/n\) for absolute constants \(b_{ij}\). Then we can write the likelihood of the \(\ell\)-neighbourhood of node \(u\) as the joint function:

\[f(u,\{(\mathbf{X}_{v},y_{v})\}_{v\in\mathcal{H}(u)},\eta_{\ell}(u))=\mathbf{P} _{\{y_{v}\}_{v\in\mathcal{H}(u)}}\rho_{y_{u}}(\mathbf{X}_{u})\prod_{k\in[ \ell]}\prod_{v\in N_{k}(u)}\rho_{y_{v}}(\mathbf{X}_{v})\mathbf{Q}_{y_{u}y_{v}} ^{k}.\] (4)

In the above, \(\mathbf{Q}_{ij}^{k}\) denotes the \(i,j\)-th entry of the matrix \(\mathbf{Q}^{k}\) and quantifies the probability that a node in class \(j\) is at a distance \(k\) from a node in class \(i\); while \(\mathbf{P}_{\{y_{v}\}_{v\in\mathcal{H}(u)}}\) denotes the prior distribution of the node labels, which by our assumption is uniform. Let us now compute the MAP estimator.

\[h_{\ell}^{*}(u,\{\mathbf{X}_{v}\}_{v\in\mathcal{H}(u)}) =\operatorname*{argmax}_{i\in[C]}\max_{\begin{subarray}{c}y_{v} \in[C]\\ v\in\mathcal{H}(u)\end{subarray}}f(u,\{(\mathbf{X}_{v},y_{v})\}_{v\in \mathcal{H}_{\ell}(u)},\eta_{\ell}(u))\] \[=\operatorname*{argmax}_{i\in[C]}\rho_{i}(\mathbf{X}_{u})\prod_{ k\in[\ell]}\prod_{v\in N_{k}(u)}\max_{j\in[C]}\rho_{j}(\mathbf{X}_{v}) \mathbf{Q}_{ij}^{k}\] \[=\operatorname*{argmax}_{i\in[C]}\log\rho_{i}(\mathbf{X}_{u})+ \sum_{k\in[\ell]}\sum_{v\in N_{k}(u)}\max_{j\in[C]}\big{\{}\log(\rho_{j}( \mathbf{X}_{v}))+\log(\mathbf{Q}_{ij}^{k})\big{\}}\] \[=\operatorname*{argmax}_{i\in[C]}\log\rho_{i}(\mathbf{X}_{u})+ \sum_{k\in[\ell]}\sum_{v\in N_{k}(u)}\max_{j\in[C]}\mathcal{M}_{ik}(\mathbf{ X}_{v}),\]

where \(\mathcal{M}_{ik}(\mathbf{X}_{v})=\log(\rho_{j}(\mathbf{X}_{v}))+\log(\mathbf{ Q}_{ij}^{k})\). Furthermore, note that an instance of Architecture 1 with \(L=1\), \(\sigma_{1}=\{\rho_{i}\}_{i\in[C]}\), and \(\mathbf{Q}\) for the edge-probabilities realizes the function \(h_{\ell}^{*}\) for a given root node \(u\) and its \(\ell\)-neighbourhood \(\eta_{\ell}(u)\), in the sense that \(\hat{y}_{u}=h_{\ell}^{*}(u,\{\mathbf{X}_{v}\}_{v\in\mathcal{H}_{\ell}(u)})\). 

Next, we obtain a simpler version of the classifier for the two-class symmetric CSBM with an arbitrary distribution of node features. Recall that \(\psi\) is the log of the likelihood ratio.

**Corollary** (Restatement of Corollary 1.1).: _For any \(\ell\geq 1\), the asymptotically \(\ell\)-locally Bayes optimal classifier of the root for the sequence \((G_{n},u_{n})\sim\mathrm{CSBM}(n,d,\mathrm{P},\frac{a}{n},\frac{b}{n})\) is_

\[h_{\ell}^{*}(u,\{\mathbf{X}_{v}\}_{v\in\mathcal{H}_{\ell}(u)})=\mathrm{sgn} \big{(}\psi(\mathbf{X}_{u})+\sum_{v\in\mathcal{H}_{\ell}(u)\setminus\{u\}} \mathcal{M}_{d(u,v)}(\mathbf{X}_{v})\big{)},\]

_where \(\mathcal{M}_{k}(\mathbf{x})=\mathrm{sgn}(a-b)\cdot\varphi(\psi(\mathbf{x}),c_ {k})\) with \(c_{k}=\log\Big{(}\frac{1+\Gamma^{k}}{1-\Gamma^{k}}\Big{)}\), and \(\psi(\mathbf{x})=\log\frac{\rho_{+}(\mathbf{x})}{\rho_{-}(\mathbf{x})}\)._

Proof.: The proof follows directly from Theorem 1, by taking \(C=2\). In this two-class case, the features \(\mathbf{X}_{v}\in\mathds{R}^{d}\) for every node follow the law \(\mathds{P}_{y}\), where the class labels \(y_{v}\in\{\mp 1\}\) are \(\mathrm{Unif}(\{-1,+1\})\). In addition, we have

\[\mathbf{Pr}((u,v)\text{ is an edge})=\begin{cases}p&y_{u}=y_{v}\\ q&y_{u}\neq y_{v}\end{cases},\]where \(p=a/n\) and \(q=b/n\) for absolute constants \(a>1,b\geq 0\). Define the quantities \(p_{k},q_{k}\) as follows for \(k\in[\ell]\):

\[p_{k}=\sum_{j=0}^{\lfloor k/2\rfloor}\binom{k}{2j}p^{k-2j}q^{2j}, q_{k}=\sum_{j=1}^{\lceil k/2\rceil}\binom{k}{2j-1}p^{k-2j+1}q^{2j-1}.\] (5)

Then we can simplify the likelihood of the \(\ell\)-neighbourhood of node \(u\) from (4) as follows:

\[f(\{(\mathbf{X}_{v},y_{v})\}_{v\in\eta_{\ell}(u)})=\rho_{y_{u}}( \mathbf{X}_{u})\prod_{k\in[\ell]}\prod_{v\in N_{k}(u)}\left(\rho_{y_{v}}( \mathbf{X}_{v})p_{k}^{\frac{1+y_{k}y_{v}}{2}}q_{k}^{\frac{1-y_{k}y_{v}}{2}} \right).\]

Then maximizing the likelihood over possible class labels, we have

\[h_{\ell}^{*}(u,\{\mathbf{X}_{v}\}_{v\in\eta_{\ell}(u)})=\operatorname*{ argmax}_{y_{u}\in\{\pm 1\}}\max_{\begin{subarray}{c}\{y_{v}\in\{\pm 1\}\}\\ v\in\eta_{\ell}(u)\end{subarray}}f(\{\{(\mathbf{X}_{v},y_{v})\}_{v\in\eta_{ \ell}(u)})\] \[=\operatorname*{argmax}_{y_{u}\in\{\pm 1\}}\max_{\begin{subarray}{c}\{y _{v}\in\{\pm 1\}\}\\ v\in\eta_{\ell}(u)\end{subarray}}\log f(\{(\mathbf{X}_{v},y_{v})\}_{v\in\eta_{ \ell}(u)})\] \[=\operatorname*{argmax}_{y_{u}\in\{\pm 1\}}\left\{\log\rho_{y_{u}}( \mathbf{X}_{u})+\max_{\begin{subarray}{c}\{y_{v}\in\{\pm 1\}\}\\ v\in\eta_{\ell}(u)\end{subarray}}\sum_{k\in[\ell]}\sum_{v\in N_{k}(u)}\log\left( \rho_{y_{v}}(\mathbf{X}_{v})p_{k}^{\frac{1+y_{k}y_{v}}{2}}q_{k}^{\frac{1-y_{k} y_{v}}{2}}\right)\right\}\] \[=\operatorname*{sgn}\left(\log\frac{\rho_{+}(\mathbf{X}_{u})}{ \rho_{-}(\mathbf{X}_{u})}+\sum_{k\in[\ell]}\sum_{v\in N_{k}(u)}\mathcal{M}_{k }(\mathbf{X}_{v})\right),\]

where \(\mathcal{M}_{k}(\mathbf{x})=\log(\max(p_{k}\rho_{+}(\mathbf{x}),q_{k}\rho_{- }(\mathbf{x})))-\log(\max(p_{k}\rho_{-}(\mathbf{x}),q_{k}\rho_{+}(\mathbf{x})))\). Next, we observe that for any \(w,x,y,z\in\mathds{R}\),

\[\log(\max(wy,xz))-\log(\max(wz,xy))=\operatorname*{sgn}(w-x)\cdot\min\left(\max \left(\log\frac{y}{z},-\left|\log\frac{w}{x}\right|\right),\left|\log\frac{w}{x }\right|\right).\]

Hence, \(\mathcal{M}_{k}(\mathbf{X}_{v})\) is simply \(\operatorname*{sgn}(a-b)\cdot\varphi(\log\frac{\rho_{+}(\mathbf{X}_{v})}{\rho_ {-}(\mathbf{X}_{v})},c_{k})\), i.e., the signed likelihood ratio clipped between \(-c_{k}\) and \(c_{k}\). 

#### a.3.2 Generalization Error

Let us now compute the generalization error of \(h_{\ell}^{*}\). Formally, given a data instance \((u,\{\mathbf{X}_{v}\}_{v\in\eta_{\ell}(u)})\) along with the neighbourhood \(\eta_{\ell}(u)\), \(h_{\ell}^{*}\) outputs a label \(\hat{y}_{u}\in\{\pm 1\}\), and the generalization error is defined as the probability \(\mathbf{Pr}(y_{u}\hat{y}_{u}<1)\). For a simple calculation, let us assume that the latent labels \(y_{i}\) are uniformly distributed, i.e., \(\mathbf{Pr}(y_{i}=-1)=\mathbf{Pr}(y_{i}=1)=\frac{1}{2}\). It is straightforward to generalize to unbalanced settings. Recall that the features in classes \(\pm 1\) follow the law \(\mathds{P}_{\pm}\), and denote by the log likelihood ratio by \(\psi(\mathbf{x})=\log(\rho_{+}(\mathbf{x})/\rho_{-}(\mathbf{x}))\). Then we have that for a fixed \(u\),

\[\mathcal{E}(h_{\ell}^{*}) =\mathbf{Pr}\left(y_{u}\left(\log\frac{\rho_{+}(\mathbf{X}_{u})}{ \rho_{-}(\mathbf{X}_{u})}+\sum_{k\in[\ell]}\sum_{v\in N_{k}(u)}\mathcal{M}_{ k}(\mathbf{X}_{v})\right)<0\right)\] \[=\frac{1}{2}\left[\mathbf{Pr}\left(\psi(\mathbf{Y}^{(1)})+\sum_{k \in[\ell]}\mathbf{Z}_{k}^{(1)}>0\right)+\mathbf{Pr}\left(\psi(\mathbf{Y}^{( 2)})+\sum_{k\in[\ell]}\mathbf{Z}_{k}^{(2)}<0\right)\right],\]

where \(\mathbf{Z}_{k}^{(1)}=\sum_{j\in[\alpha_{k}]}\mathcal{M}_{k}(\mathbf{Y}_{k,j}^ {(1)})+\sum_{j\in[\beta_{k}]}\mathcal{M}_{k}(\mathbf{Y}_{k,j}^{(2)})\) and \(\mathbf{Z}_{k}^{(2)}=\sum_{j\in[\alpha_{k}]}\mathcal{M}_{k}(\mathbf{Y}_{k,j}^ {(2)})+\sum_{j\in[\beta_{k}]}\mathcal{M}_{k}(\mathbf{Y}_{k,j}^{(1)})\) are independent random variables with \(\mathbf{Y}_{k,j}^{(1)}\sim\mathds{P}_{-}\) and \(\mathbf{Y}_{k,j}^{(2)}\sim\mathds{P}_{+}\).

The above expression is not particularly insightful. For this reason, we now specialize to the case of Gaussian features and interpret the error in terms of the natural SNRs associated with the Gaussian mixture and the graph, i.e., the quantities \(\gamma\) and \(\Gamma\).

### Specialization to Gaussian Features

In this section, we look at the specialized setting where the node features are sampled from a symmetric binary Gaussian mixture model. Let us begin with the generalization error in this case.

#### a.4.1 Generalization Error

**Theorem** (Restatement of Theorem 2).: _For any \(\ell\geq 1\), the generalization error of the asymptotically \(\ell\)-locally Bayes optimal classifier of the root for the sequence \((G_{n},u_{n})\sim\mathrm{CSBM}(n,d,\mathbb{P},\mathbf{Q})\) with Gaussian features is given by_

\[\mathcal{E}(h_{\ell}^{*})=\mathbf{Pr}\left[g+\frac{1}{2\gamma}\sum_{k\in[\ell ]}\left(\sum_{i\in[\alpha_{k}]}Z_{k,i}^{(a)}+\sum_{i\in[\beta_{k}]}Z_{k,i}^{(b )}\right)>\gamma\right],\]

_where \(\alpha_{k},\beta_{k}\) are as in (2), \(Z_{k,i}^{(a)}=\varphi(-2\gamma^{2}+2\gamma g_{k,i},c_{k})\), \(Z_{k,i}^{(b)}=\varphi(2\gamma^{2}+2\gamma g_{k,i},c_{k})\), and \(g,\{g_{k,i}\}\) are mutually independent standard Gaussian random variables._

Proof.: For the Gaussian mixture, the log of the likelihood ratio for a node \(u\) is given by \(\frac{2}{\sigma^{2}}\langle\mathbf{X}_{u},\boldsymbol{\mu}\rangle\stackrel{{ \mathcal{D}}}{{=}}2y_{u}\gamma^{2}+2\gamma g\), where \(g\sim\mathcal{N}(0,1)\). Replacing every \(\{\mathbf{X}_{i}\}_{i\in[n]}\) as \(\mathbf{X}_{i}=\mathrm{E}\mathbf{X}_{i}+\sigma\mathbf{g}_{i}=y_{i} \boldsymbol{\mu}+\sigma\mathbf{g}_{i}\), we obtain the expression in Theorem 2. 

#### a.4.2 Extreme Graph SNRs

Let us now turn to the next result, where we analyze the generalization error in the cases where the graph SNR \(\Gamma\) takes extreme values.

**Theorem** (Restatement of Theorem 3).: _Let \(h_{\ell}^{*}\) be the classifier from Corollary 1.1, \(h_{0}^{*}(u)=\mathrm{sgn}(\langle\mathbf{X}_{u},\boldsymbol{\mu}\rangle)\) be the Bayes optimal classifier given only the feature information of the root node \(u\), and \(h_{\mathrm{gcn}}\) be the one-layer vanilla GCN classifier. Then we have that for any fixed \(\ell\):_

1. _If_ \(\Gamma=0\) _then_ \(\mathcal{E}(h_{\ell}^{*})=\mathcal{E}(h_{0}^{*})=\Phi(-\gamma)\)_, where_ \(\Phi\) _is the standard Gaussian CDF._
2. _If_ \(\Gamma\to 1\) _then_ \(\xi_{\ell}\geq 1\) _a.s. and_ \(\mathcal{E}(h_{\ell}^{*})\to\mathbf{Pr}(g>\gamma\xi_{\ell})\)_, where_ \(g\sim\mathcal{N}(0,1)\)_._
3. \(\mathcal{E}(h_{\mathrm{gcn}})=\mathbf{Pr}\left(g>\gamma\xi_{1}\right)\)_._

Proof.: Note that when \(\Gamma=0\), i.e., when \(a=b\), then \(a_{k}=b_{k}\) for all \(k\in[\ell]\), hence, \(c_{k}=\log(a_{k}/b_{k})=0\). This implies that all information from the \(k\)-hop neighbours is truncated to \(0\) for all \(k\in[\ell]\). Thus, the classifier reduces to \(h_{u}^{*}=h_{\ell}^{*}(u,\{\mathbf{X}_{v}\}_{v\in\eta_{t}(u)})=\mathrm{sgn} \left(\psi(\mathbf{X}_{u})\right)=\mathrm{sgn}\left(g+y_{u}\gamma\right)=h_{0 }^{*}(u,\{\mathbf{X}_{u}\})\). Thus, the probability that \(h_{u}^{*}y_{u}<0\) is

\[\mathbf{Pr}(h_{u}^{*}y_{u}<0)=\mathbf{Pr}(y_{u}g+\gamma<0)=\Phi(-\gamma),\]

where \(\Phi(\cdot)\) is the standard Gaussian CDF.

For the other case where \(\Gamma\to 1\), we have two sub-cases: Either \(a\to 0\) with \(b\neq 0\), or \(a\neq 0\) with \(b\to 0\). In this case, \(c_{k}\to\infty\) for all \(k\), so the classifier takes the form

\[h_{\ell}^{*}(u,\{\mathbf{X}_{v}\}_{v\in\eta_{\ell}(u)})=\mathrm{sgn}\left(g+y_ {u}\gamma+\sum_{k\in[\ell]}\sum_{v\in N_{k}(u)}(g_{k,v}+y_{v}\gamma)\right).\]

Hence, the probability of making a mistake is

\[\mathbf{Pr}(h_{u}^{*}y_{u}<0) =\mathbf{Pr}\left(y_{u}g+\gamma+\sum_{k\in[\ell]}\sum_{v\in N_{ k}(u)}(y_{u}g_{k,v}+y_{u}y_{v}\gamma)<0\right)\] \[=\mathbf{Pr}\left(g>\gamma\frac{|\eta_{\ell}^{(a)}-\eta_{\ell}^{ (b)}|}{\sqrt{\eta_{\ell}^{(a)}+\eta_{\ell}^{(b)}}}\right),\]where \(\eta^{(a)}_{\ell},\eta^{(b)}_{\ell}\) denote the total number of nodes in the \(\ell\)-neighbourhood \(\eta_{\ell}(u)\) that are in the same class as \(u\) and different class as \(u\), respectively. The last equation is obtained by using the fact that \((g,\{g_{k,v}\})\) are i.i.d. standard Gaussians. Note that in this case since either \(b\to 0\) or \(a\to 0\) (but not both), we have \(\eta^{(b)}_{\ell}\to 0\) or \(\eta^{(a)}_{\ell}\to 0\) using (2) for any fixed \(\ell\). Thus, \(\xi_{\ell}(a,b)>1\) a.s. Following a similar analysis, one can find that \(\mathcal{E}(h_{\mathrm{gcn}})=\mathbf{Pr}(g>\gamma\cdot\xi_{1}(a,b))\). 

It is interesting to note that we may not have \(\xi_{1}(a,b)>1\) in general, meaning that a GCN is better than methods that do not use a graph only in the case where \(\xi_{1}(a,b)>1\).

### Non-asymptotic Analysis

First, consider the case where \(\ell\), the total depth of the neighbourhood is a constant independent of \(n\), the number of nodes.

Putting \(m=1\) in Lemma A.1, we see that the probability is bounded by \((8\ell^{2}(a+b))^{4\ell+1}/n=O(1/n)\). Hence, we conclude that in the limit \(n\to\infty\), there are no cycles in any constant-depth neighbourhoods in the graph. In particular, we obtain that the local weak limit \((G,u)\) is a tree.

We now turn to the case where the depth of the neighbourhood is logarithmic in \(n\).

**Proposition** (Restatement of Proposition 3.1).: _Let \(G=(V,E)\sim\mathrm{CSBM}(n,d,\mathds{P},\frac{a}{n},\frac{b}{n})\). Then for any \(\ell=c\log n\) such that \(c\log(\frac{a+b}{2})<1/4\), with probability \(1-O(1/\log^{2}n)\), the number of nodes \(u\in V\) whose \(\ell\)-neighbourhood is cycle-free is \(n\left(1-o(\frac{\log^{4}(n)}{\sqrt{n}})\right)\)._

Proof.: In Lemma A.2, observe that since \(c\log\Delta<1/4\), we have \(\ell<\frac{\log n}{4\log\Delta}=\frac{1}{4}\log_{\Delta}n\). Thus, putting \(m=\log n\), we find that with probability at least \(1-O(1/\log^{2}n)\), the number of nodes whose \(\ell\)-neighbourhood contains at least one cycle is bounded by \(O(\log^{4}(n)\Delta^{2\ell})=o(\log^{4}(n)\sqrt{n})\). Hence, the fraction of nodes whose \(\ell\)-neighbourhood is cycle-free is \(1-o(\frac{\log^{4}n}{\sqrt{n}})\). 

For a fixed node \(u\in[n]\), let us denote the number of nodes at distance \(k\) (respectively \(\leq k\)) from \(u\) with class label \(\pm y_{u}\) by \(U^{\pm}_{k}(u)\) (respectively, \(U^{\pm}_{\leq k}(u)\)). Also let \(n_{\pm}\) denote the number of nodes with class label \(\pm y_{u}\), so that \(n=n_{+}+n_{-}\). Note that \(U^{+}_{0}(u)=1\), \(U^{-}_{0}(u)=0\), and conditionally on the sigma-field \(\mathcal{F}_{k-1}=\sigma(U^{\pm}_{\leq k-1},1\leq k-1)\), we have

\[U^{+}_{k}(u) \sim\mathrm{Bin}\left(n_{+}-U^{+}_{\leq k-1},1-(1-a/n)^{U^{+}_{k- 1}}(1-b/n)^{U^{-}_{k-1}}\right),\] (6) \[U^{-}_{k}(u) \sim\mathrm{Bin}\left(n_{-}-U^{-}_{\leq k-1},1-(1-a/n)^{U^{-}_{k -1}}(1-b/n)^{U^{+}_{k-1}}\right).\] (7)

Define \(S_{k}(u)=U^{+}_{k}(u)+U^{-}_{k}(u)\) to be the number of nodes at distance exactly \(k\) from \(u\), and denote \(\Delta=\frac{a+b}{2}\) to be the expected degree of a node. Correspondingly, recall from (2) that we have

\[\alpha_{0} =1,\beta_{0}=0,\] \[\alpha_{k} \sim\mathrm{Poi}\left(\frac{a\alpha_{k-1}+b\beta_{k-1}}{2}\right),\beta_{k}\sim\mathrm{Poi}\left(\frac{a\beta_{k-1}+b\alpha_{k-1}}{2}\right) \text{ for }k\in[\ell].\] (8)

Let us now state a useful high-probability bound on \(S_{k}(u)=U^{+}_{k}(u)+U^{-}_{k}(u)\).

**Lemma A.4**.: _[_Massoulie, 2014_, Theorem 2.3]__. For any \(\ell=c\log n\) with \(c\log\Delta<1/4\), there exist constants \(C,\epsilon>0\) such that with probability at least \(1-O(n^{-\epsilon})\), \(S_{k}(u)\leq C\Delta^{k}\log(n)\) for all \(u\in[n]\) and all \(k\in[\ell]\)._

We now obtain a total variation bound between the sequences \(\{U^{\pm}_{k}\}_{k\geq 0}\) and \(\{\alpha_{k},\beta_{k}\}_{k\geq 0}\).

**Lemma A.5**.: _Let \(u\in[n]\) be fixed with label \(y_{u}\in\{\pm 1\}\). Let \(\ell=c\log n\) with \(c\log\Delta<1/4\). Then the total variation distance between the collections of variables \(\{U^{+}_{k}(u),U^{-}_{k}(u)\}_{k\leq\ell}\) and \(\{\alpha_{k}(u),\beta_{k}(u)\}_{k\leq\ell}\) is bounded by \(O(\log^{3}n/n^{1/4})\)._Proof.: Define the following events for \(C\) as in Lemma A.4:

\[\Omega_{k}=\{S_{k}\leq C\Delta^{k}\log n\},1\leq k\leq\ell.\] (9)

Conditionally on the sigma-field \(\mathcal{F}_{k-1}=\sigma(U_{t}^{\pm}(u),t\leq k-1)\) and the event \(\Omega_{k-1}\), we compute the total variation distance between the variables \((U_{k}^{+}(u),U_{k}^{-}(u))\) and \((\alpha_{k}(u),\beta_{k}(u))\). Since \(u\) is fixed, we omit it from the notation for brevity. Define the following random variables:

\[W_{k}^{+}\sim\mathrm{Poi}\left(\frac{aU_{k-1}^{+}+bU_{k-1}^{-}}{2}\right), \qquad\qquad W_{k}^{-}\sim\mathrm{Poi}\left(\frac{aU_{k-1}^{-}+bU_{k-1}^{+}}{2 }\right).\]

We now apply the Stein-Chen method to bound \(d_{\mathrm{TV}}(U_{k}^{\pm},W_{k}^{\pm})\). For more details on this technique, we refer to Stein (1972); Chen (1975); Barbour and Chen (2005). In particular, we use the fact that for \(X_{1}\sim\mathrm{Bin}(n,\lambda/n)\), \(X_{2}\sim\mathrm{Poi}(\lambda)\) and \(X_{3}\sim\mathrm{Poi}(\lambda^{\prime})\), \(d_{\mathrm{TV}}(X_{1},X_{2})\leq\lambda/n\) and \(d_{\mathrm{TV}}(X_{2},X_{3})\leq|\lambda-\lambda^{\prime}|\). Let us focus on \(d_{\mathrm{TV}}(U_{k}^{+},W_{k}^{+})\) as the other case for \(d_{\mathrm{TV}}(U_{k}^{-},W_{k}^{-})\) is similar. Construct an intermediate random variable based on the distributions of \(U_{k}^{\pm}\) as in Eqs. (6) and (7),

\[V_{k}\sim\mathrm{Poi}\left((n_{+}-U_{\leq k-1}^{+})\left(1-(1-a/n)^{U_{k-1}^{+} }(1-b/n)^{U_{k-1}^{-}}\right)\right).\]

Denote \(T_{t}=1-\left(1-\frac{a}{n}\right)^{U_{t}^{+}}\left(1-\frac{b}{n}\right)^{U_{t }^{-}}\) for brevity. Note that using triangle inequality,

\[d_{\mathrm{TV}}(V_{k},W_{k}^{+}) \leq\left|(n_{+}-U_{\leq k-1}^{+})T_{k-1}-\frac{aU_{k-1}^{+}+bU_{ k-1}^{-}}{2}\right|\] \[\leq\left|\left(n_{+}-U_{\leq k-1}^{+}-\frac{n}{2}\right)T_{k-1} \right|+\left|\frac{aU_{k-1}^{+}+bU_{k-1}^{-}-nT_{k-1}}{2}\right|\] \[=\left|\left(n_{+}-U_{\leq k-1}^{+}-\frac{n}{2}\right)T_{k-1} \right|+\frac{n}{2}\left|\frac{aU_{k-1}^{+}+bU_{k-1}^{-}}{n}-T_{k-1}\right|\] \[\leq\left|\left(n_{+}-U_{\leq k-1}^{+}-\frac{n}{2}\right)T_{k-1} \right|+\frac{1}{4n}\left(aU_{k-1}^{+}+bU_{k-1}^{-}\right)^{2},\]

where in the last inequality we used Fact A.3. Then we obtain the variation distance:

\[d_{\mathrm{TV}}(U_{k}^{+},W_{k}^{+}) \leq d_{\mathrm{TV}}(U_{k}^{+},V_{k})+d_{\mathrm{TV}}(V_{k},W_{k} ^{+})\] \[\leq T_{k-1}+\left|\left(n_{+}-U_{\leq k-1}^{+}-\frac{n}{2} \right)T_{k-1}\right|+\frac{1}{4n}\left(aU_{k-1}^{+}+bU_{k-1}^{-}\right)^{2}\] \[=T_{k-1}\left(1+\left|n_{+}-U_{\leq k-1}^{+}-\frac{n}{2}\right| \right)+\frac{1}{4n}\left(aU_{k-1}^{+}+bU_{k-1}^{-}\right)^{2}.\]

Consider now a choice of \(c\) such that \(c\log\Delta<1/4\). We have \(\ell=c\log n<\frac{1}{4}\log_{\Delta}n\), implying that \(\Delta^{\ell}\leq n^{1/4}\). Recalling (9) corresponding to Lemma A.4, we have that under the event \(\Omega_{k-1}\) for \(k\leq\ell\), the number of nodes at distance \(k-1\) is

\[S_{k-1}=U_{k-1}^{+}+U_{k-1}^{-}\leq C\Delta^{k-1}\log n\leq C\Delta^{\ell} \log n\leq Cn^{1/4}\log n.\] (10)

Observe now that from Fact A.3, \(T_{k-1}\leq\frac{aU_{k-1}^{+}+bU_{k-1}^{-}}{n}\). Recalling that \(y_{u}\) have a uniform prior, by the Chernoff bound (Vershynin, 2018, Theorem 2.3.1) on \(n_{+}\), we have \(|n_{+}-\frac{n}{2}|=O(\sqrt{n}\log n)\) with probability at least \(1-1/\mathrm{poly}(n)\). Thus, we obtain that under this event,

\[d_{\mathrm{TV}}(U_{k}^{+},W_{k}^{+}) \leq O\left(\frac{|U_{\leq k-1}^{+}|}{n}+\frac{\log n}{\sqrt{n}} \right)\cdot\left(aU_{k-1}^{+}+bU_{k-1}^{-}\right)+\frac{1}{4n}\left(aU_{k-1}^ {+}+bU_{k-1}^{-}\right)^{2}\] \[\leq O\left(\frac{\log n}{\sqrt{n}}\right)\cdot\max(a,b)S_{k-1}+ \frac{\max(a,b)^{2}}{4n}S_{k-1}^{2}=O\left(\frac{\log^{2}n}{n^{1/4}}\right),\]

where in the last step we used the bound from (10). Now recall that the variables \(\{U_{k}^{\pm},\alpha_{k},\beta_{k}\}_{k\in[\ell]}\) are defined as in Eqs. (6) to (8) for all \(k\leq\ell\). For a fixed \(u\in[n]\), we have the base casesand \(U_{0}^{-}=\beta_{0}=0\). Then following an induction argument with a union bound over all \(k\in\{1,\ldots,\ell\}\), we have that the variation distance between the sequences \(\{U_{k}^{+},U_{k}^{-}\}_{k\leq\ell}\) and \(\{\alpha_{k},\beta_{k}\}_{k\leq\ell}\) is upper bounded by \(O\left(\frac{\log^{3}n}{n^{1/4}}\right)\). 

We now obtain a relationship between the misclassification error on the data model with finite \(n\), i.e., \(\mathcal{E}_{n}\) and the error on the limit of the model with \(n\to\infty\), i.e., \(\mathcal{E}\).

**Theorem** (Restatement of Theorem 4).: _For any \(1\leq\ell\leq c\log n\) such that the positive constant \(c\) satisfies \(c\log(\frac{a+b}{2})<1/4\), we have that_

Proof.: Consider a random feature-decorated graph \(G_{n}\sim\mathrm{CSBM}(n,d,\{\mathbb{P}_{\pm}\},a/n,b/n)\), where \(\mathbb{P}_{\pm}\) correspond to the distributions \(\mathcal{N}(\pm\bm{\mu},\sigma^{2}\mathbf{I})\) for the node features given by \(\{\mathbf{X}_{u}\}_{u\in[n]}\). For a classifier \(h\in\mathcal{C}_{\ell}\), the class of all \(\ell\)-local classifiers, define \(\mathcal{E}_{n}(h)\) to be the probability of misclassification for a uniform at random node \(u\in[n]\), i.e., \(\mathcal{E}_{n}(h)=\mathbf{Pr}(y_{u}\cdot h(u,\{\mathbf{X}_{v}\}_{v\in\eta_{ \ell}(u)},\eta_{\ell}(u))<0)\). Since it is known that all classifiers in \(\mathcal{C}_{\ell}\) operate on \(u\) given the information in its \(\ell\)-neighbourhood \(\eta_{\ell}(u)\), we will omit \(\eta_{\ell}(u)\) from the notation and say \(h(u)\) instead of \(h(u,\{\mathbf{X}_{v}\}_{v\in\eta_{\ell}(u)},\eta_{\ell}(u))\) when it is understood. Let \(\mathbf{P}\) be the joint measure of the variables \(\{U_{k}^{\pm}\}_{k\leq\ell}\) from Eqs. (6) and (7), and \(\mathbf{P}^{\prime}\) be the joint measure of the variables \(\{\alpha_{k},\beta_{k}\}_{k\leq\ell}\) from Eq. (8). Then Lemma A.5 gives us that \(d_{\mathrm{TV}}(\mathbf{P},\mathbf{P}^{\prime})\leq O\left(\frac{\log^{3}n}{ n^{1/4}}\right)=o_{n}(1)\).

Recall \(\mathcal{E}(h_{\ell}^{*})\) computed in Theorem 2 for the limiting data model \((G,u)\).

\[\mathcal{E}(h_{\ell}^{*}) =\mathbf{Pr}\left[g+\frac{1}{2\gamma}\sum_{k\in[\ell]}\left(\sum _{i=1}^{\alpha_{k}}Z_{k,i}^{(a)}+\sum_{i=1}^{\beta_{k}}Z_{k,i}^{(b)}\right)>\gamma\right]\] \[=\int\mathbf{Pr}\left[g+\frac{1}{2\gamma}\sum_{k\in[\ell]}\left( \sum_{i=1}^{\alpha_{k}}Z_{k,i}^{(a)}+\sum_{i=1}^{\beta_{k}}Z_{k,i}^{(b)}\right) >\gamma\right|\{\alpha_{k},\beta_{k}\}_{k\leq\ell}\right]d\mathbf{P}^{\prime}.\]

Similarly, we have

\[\mathcal{E}_{n}(h_{\ell}^{*}) =\mathbf{Pr}\left[g+\frac{1}{2\gamma}\sum_{k\in[\ell]}\left(\sum _{i=1}^{U_{k}^{+}}Z_{k,i}^{(a)}+\sum_{i=1}^{U_{k}^{-}}Z_{k,i}^{(b)}\right)>\gamma\right]\] \[=\int\mathbf{Pr}\left[g+\frac{1}{2\gamma}\sum_{k\in[\ell]}\left( \sum_{i=1}^{U_{k}^{+}}Z_{k,i}^{(a)}+\sum_{i=1}^{U_{k}^{-}}Z_{k,i}^{(b)}\right)> \gamma\right|\{U_{k}^{\pm}\}_{k\leq\ell}\right]d\mathbf{P}.\]

Thus, we obtain that

\[|\mathcal{E}_{n}(h_{\ell}^{*})-\mathcal{E}(h_{\ell}^{*})|\leq d_{\mathrm{TV}}( \mathbf{P},\mathbf{P}^{\prime})\leq O\left(\frac{\log^{3}n}{n^{1/4}}\right)=o_ {n}(1),\] (11)

Let us now focus on the case with finite \(n\). Let \(A\) denote the event from Proposition 3.1 where the number of nodes with cycle-free \(\ell\)-neighbourhoods is \(1-o(\frac{\log^{4}n}{\sqrt{n}})\). For a node \(u\in G_{n}\), let \(E_{u}\) denote the event that the subgraph induced by the \(\ell\)-neighbourhood of \(u\), \(\eta_{\ell}(u)\) is a tree. Then observe that for a uniform random node \(u\in G_{n}\),

\[\min_{h\in\mathcal{C}_{\ell}}\mathcal{E}_{n}(h) =\mathbf{Pr}(y_{u}h_{\ell,n}^{*}(u)<0)\] \[=\mathbf{Pr}(E_{u})\mathbf{Pr}(y_{u}h_{\ell,n}^{*}(u)<0\mid E_{u}) +\mathbf{Pr}(E_{u}^{\mathsf{c}})\mathbf{Pr}(y_{u}h_{\ell,n}^{*}(u)<0\mid E_{u} ^{\mathsf{c}})\] \[=(1-o_{n}(1))\mathbf{Pr}(y_{u}h_{\ell}^{*}(u)<0)+o_{n}(1)\] \[=\mathcal{E}_{n}(h_{\ell}^{*})\pm o_{n}(1).\]In the above, we used from Proposition 3.1 that \(\mathbf{Pr}(E_{u})=\mathbf{Pr}(E_{u}\cap A)+\mathbf{Pr}(E_{u}\cap A^{\mathsf{c}})=1 -O(\frac{1}{\log^{2}n})\), and that \(\mathcal{E}_{n}(h_{\ell}^{*})=\min_{h\in\mathcal{C}_{\ell}}\mathbf{Pr}(y_{u}h(u )<0\mid E_{u})\). This establishes the first part:

\[|\min_{h\in\mathcal{C}_{\ell}}\mathcal{E}_{n}(h)-\mathcal{E}_{n}(h_{\ell}^{*})| =O\left(\frac{1}{\log^{2}n}\right).\]

Combining the above display with (11), we obtain the second part, i.e.,

\[\left|\min_{h\in\mathcal{C}_{\ell}}\mathcal{E}_{n}(h)-\min_{h\in \mathcal{C}_{\ell}}\mathcal{E}(h)\right| =\left|\min_{h\in\mathcal{C}_{\ell}}\mathcal{E}_{n}(h)-\mathcal{E }(h_{\ell}^{*})\right|\] \[=\left|\min_{h\in\mathcal{C}_{\ell}}\mathcal{E}_{n}(h)-\mathcal{E }_{n}(h_{\ell}^{*})+\mathcal{E}_{n}(h_{\ell}^{*})-\mathcal{E}(h_{\ell}^{*})\right|\] \[\leq\left|\min_{h\in\mathcal{C}_{\ell}}\mathcal{E}_{n}(h)- \mathcal{E}_{n}(h_{\ell}^{*})\right|+\left|\mathcal{E}_{n}(h_{\ell}^{*})- \mathcal{E}(h_{\ell}^{*})\right|\] \[=O\left(\frac{1}{\log^{2}n}\right)+O\left(\frac{\log^{3}n}{n^{1/4 }}\right)=O\left(\frac{1}{\log^{2}n}\right).\qed\]

## Appendix B Additional Empirical Observations

### Convergence of Parameters

We observe empirically that gradient descent (SGD and Adam implementations in the pytorch library) converges in the binary setting. In this case, the neural network learns the right parameters corresponding to the parameters of the CSBM, i.e., \(\rho\) and \(\mathbf{Q}\), such that Architecture 1 realizes the optimal classifier in Corollary 1.1.

In Fig. 3, the x-axis denotes the number of epochs elapsed since the beginning of the training process. For the first plot, the y-axis denotes the cosine similarity between the parameters \(\theta_{1}\in\mathds{R}^{d}\) learned by the MLP \(\mathbf{H}^{(L)}\) in Architecture 1 and the ansatz \(\bm{\mu}\) that realizes the optimal classifier; while in the second plot, the y-axis denotes the absolute difference between the clip parameter \(\theta_{2}\in\mathds{R}\) and the ansatz value \(\log(\frac{1+\Gamma}{1-\Gamma})\). These experiments are performed in the same setting as Fig. 0(b) with fixed \(\Gamma=0.2\). We see that the parameters converge as the number of training iterations increases. The reported metrics are averaged over \(10\) trials, and the standard deviation is shown at each iteration using the transl

Figure 3: Convergence of parameters of Architecture 1.

[MISSING_PAGE_EMPTY:22]