# Generative Modeling of

Molecular Dynamics Trajectories

 Bowen Jing11Hannes Stark11Tommi Jaakkola1Bonnie Berger12

1CSAIL, Massachusetts Institute of Technology

2Dept. of Mathematics, Massachusetts Institute of Technology

{bjing, hstark}@mit.edu, tommi@csail.mit.edu, bab@mit.edu

Equal contribution.

###### Abstract

Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show preliminary results on scaling to protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at https://github.com/bjing2016/mdgen.

## 1 Introduction

Numerical integration of Newton's equations of motion at atomic scales, known as _molecular dynamics_ (MD), is a widely-used technique for studying diverse molecular phenomena in chemistry, biology, and other molecular sciences (Alder and Wainwright, 1959; Rahman, 1964; Verlet, 1967; McCammon et al., 1977). While general and versatile, MD is computationally demanding due to the large separation in timescales between integration steps and relevant molecular phenomena. Thus, a vast body of literature spanning several decades aims to accelerate or enhance the sampling efficiency of MD simulation algorithms (Ryckaert et al., 1977; Darden et al., 1993; Sugita and Okamoto, 1999; Laio and Parrinello, 2002; Anderson et al., 2008; Shaw et al., 2009). More recently, learning surrogate models of MD has become an active area of research in deep generative modeling (Noe et al., 2019; Zheng et al., 2023; Klein et al., 2024; Schreiner et al., 2024; Jing et al., 2024). However, existing training paradigms fail to fully leverage the rich dynamical information in MD training data, restricting their applicability to a limited set of downstream problems.

In this work, we propose MDGen, a novel paradigm for fast, general-purpose surrogate modeling of MD based on _direct generative modeling of simulated trajectories_. Different from previous works, which learn the autoregressive transition density or equilibrium distribution of MD, we formulate end-to-end generative modeling of full trajectories viewed as _time-series_ of 3D molecular structures. Akin to how image generative models were extended to videos (Ho et al., 2022), our framing of the problem augments single-structure generative models with an additional time dimension, opening the door to a larger set of forward and inverse problems to which our model can be applied. Whenprovided (and conditioned on) the initial "frame" of a given system, such generative models serve as familiar surrogate forward simulators of the reference dynamics. However, by providing other kinds of conditioning, these "molecular video" generative models also enable highly flexible applications to a variety of inverse problems not possible with existing surrogate models. In sum, we formulate and showcase the following novel capabilities of MDGen:

* _Forward simulation_--given the initial frame of a trajectory, we sample a potential time evolution of the molecular system.
* _Interpolation_--given the frames at the _two endpoints_ of a trajectory, we sample a plausible path connecting the two. In chemistry, this is known as _transition path sampling_ and is important for studying reactions and conformational transitions.
* _Upsampling_--given a trajectory with timestep \(\Delta t\) between frames, we upsample the "frame" by a factor of \(M\) to obtain a trajectory with timestep \(\Delta t/M\). This infers fast motions from trajectories saved at less frequent intervals.
* _Inpainting_--given part of a molecule and its trajectory, we _generate the rest of the molecule_ (and its time evolution) to be consistent with the known part of the trajectory. This ability could be applied to design molecules to scaffold desired dynamics.

These tasks are conceptually illustrated in Figure 1. While the forward simulation task aligns with the typical modeling paradigm of approximating the data-generating process, the others represent novel capabilities on scientifically important inverse problems _not straightforward to address even with MD itself_. As such, our framework presents a new perspective on how to unlock value from MD simulation with machine learning towards diverse downstream objectives. We highlight further exciting possibilities opened up by our framework in Section 5.

We demonstrate our framework on MD simulations of tetrapeptides (i.e., length-4 peptides), with preliminary extensions to full-sized protein monomers. To do so, we parameterize molecular trajectories in terms of sidechain torsions and residue offsets with respect to conditioning _key frames_, obtaining a generative modeling task over a 2D array of \(SE(3)\)-invariant tokens rather than residue frames or point clouds. In this parameterization, we can then employ a Scalable Interpolant Transformer (SiT) (Ma et al., 2024) as our flow-based generative backbone, avoiding the more restrictive geometric architectures commonly used for molecular structure. Furthermore, by replacing the time-wise attention in SiT with the long-context architecture Hyena (Poli et al., 2023), we provide proof-of-concept of scaling up to trajectories of 100k frames, enabling a wide range of timescales and dynamical processes to be captured with a single model generation.

We evaluate MDGen on the forward simulation, interpolation, upsampling, and inpainting tasks on tetrapeptides in a _transferable_ setting (i.e., unseen test peptides). Our model accurately reproduces free energy surfaces and dynamical content such as torsional relaxation and Markov state fluxes, provides realistic transition paths between arbitrary pairs of metastable states, and recovers fast dynamical phenomena below the sampling threshold of coarse-timestep trajectories. In preliminary

Figure 1: (_Left_) Tasks: generative modeling of MD trajectories addresses several tasks by conditioning on different parts of a trajectory. (_Right_) Method: We tokenize trajectories of \(T\) frames and \(L\) residues into an \((T\times L)\)-array of SE(3)-invariant tokens encoding roto-translation offsets from _key frames_ and torsion angles. Using _stochastic interpolants_, we generate arrays of such tokens from Gaussian noise.

steps toward dynamics-scaffolded design, we show that molecular inpainting with MDGen obtains much higher sequence recovery than inverse folding methods based on one or two static frames. Finally, we evaluate MDGen on simulation of proteins and find that it outperforms MSA subsampling with AlphaFold (Del Alamo et al., 2022) in terms of recovering ensemble statistical properties.

## 2 Background

**Molecular dynamics.** At a high level, the aim of molecular dynamics is the integrate the equations of motion \(M_{i}\dot{\mathbf{x}}_{i}=-\nabla_{\mathbf{x}_{i}}U(\mathbf{x}_{1}\ldots \mathbf{x}_{N})\) for each particle \(i\) in a molecular configuration \((\mathbf{x}_{1}\ldots\mathbf{x}_{N})\in\mathbb{R}^{3N}\), where \(M_{i}\) is the mass and \(U\) is the potential energy function (or _force field_) \(U:\mathbb{R}^{3N}\to\mathbb{R}\). However, these equations of motion are often modified to include a _thermost_ in order to model contact with surroundings at a given temperature. For example, the widely-used _Langevin thermostat_ transforms the equations of motion into a stochastic diffusion process:

\[d\mathbf{x}_{i}=\mathbf{p}_{i}/M_{i}\,dt,\quad d\mathbf{p}_{i}=-\nabla_{ \mathbf{x}_{i}}U\,dt-\gamma\mathbf{p}_{i}\,dt+\sqrt{2M_{i}\gamma kT}\,d\mathbf{ w}\] (1)

where \(\mathbf{p}_{i}\) are the momenta. By design, this process converges to the _Boltzmann distribution_ of the system \(p(\mathbf{x}_{1}\ldots\mathbf{x}_{N})\propto e^{-U/kT}\). To incorporate interactions with solvent molecules--ubiquitous in biochemistry--one includes a box of surrounding solvent molecules as part of the molecular system (explicit solvent) or modifies the force field \(U\) to model their effects (implicit solvent). In either case, only the positions \(\mathbf{x}_{i}\) of non-solvent atoms are of interest, and their time evolution constitutes (for our purposes) the _MD trajectory_.

**Deep learning for MD.** An emerging body of work seeks to approximate the distributions over configurations \(\mathbf{X}=(\mathbf{x}_{1}\ldots\mathbf{x}_{N})\) arising from Equation 1 with deep generative models. Fu et al. (2023), Timewarp (Klein et al., 2024), and ITO (Schreiner et al., 2024) learn the _transition density_\(p(\mathbf{X}_{t+\Delta t}\mid\mathbf{X}_{t})\) and emulate MD trajectories via simulation rollouts of the learned model. On the other hand, _Boltzmann generators_(Noe et al., 2019; Kohler et al., 2021; Garcia Satorras et al., 2021; Midgley et al., 2022, 2024) directly approximate the stationary Boltzmann distribution, forgoing any explicit modeling of dynamics. In particular, Boltzmann-targeting diffusion models trained with frames from MD trajectories have demonstrated promising scalability and generalization to protein systems (Zheng et al., 2023; Jing et al., 2024). However, these works have focused exclusively on forward simulation and have not explored joint modeling of entire trajectories \((\mathbf{X}_{t}\ldots\mathbf{X}_{t+N\Delta t})\) or the inverse problems accessible under such a formulation.

**Stochastic interpolants.** We build our MD trajectory generative model under the _stochastic interpolants_ framework: Given a continuous distribution \(p_{1}\equiv p_{\text{data}}\) over \(\mathbb{R}^{n}\), stochastic interpolants (Albergo and Vanden-Eijnden, 2022; Albergo et al., 2023; Lipman et al., 2022; Liu et al., 2022), provide a method for learning continuous flow-based models \(d\mathbf{x}=v_{\theta}(\mathbf{x},t)\,dt\) transporting a prior distribution \(p_{0}\) (e.g., \(p_{0}\equiv\mathcal{N}(0,\mathbf{I})\)) to the data \(p_{1}\). To do so, one defines intermediate distributions \(\mathbf{x}_{t}\sim p_{t},t\in(0,1)\) via \(\mathbf{x}_{t}=\alpha_{t}\mathbf{x}_{1}+\sigma_{t}\mathbf{x}_{0}\) where \(\mathbf{x}_{0}\sim p_{0}\) and \(\mathbf{x}_{1}\sim p_{1}\) and the interpolation path satisfies \(\alpha_{0}=\sigma_{1}=0\) and \(\alpha_{1}=\sigma_{0}=1\). A neural network \(v_{\theta}:\mathbb{R}^{n}\times[0,1]\to\mathbb{R}^{n}\) is trained to approximate the time-evolving flow field

\[v_{\theta}(\mathbf{x}_{t},t)\approx v(\mathbf{x}_{t},t)\equiv\mathbb{E}_{ \mathbf{x}_{0},\mathbf{x}_{1}|\mathbf{x}_{t}}[\hat{\alpha}_{t}\mathbf{x}_{1} +\hat{\sigma}_{t}\mathbf{x}_{0}]\] (2)

which satisfies the transport equation \(\partial p_{t}/\partial t+\nabla\cdot(p_{t}v_{t})=0\). Hence, at convergence, noisy samples \(\mathbf{x}_{0}\sim p_{0}\) can be evolved under \(v_{\theta}\) to obtain data samples \(\mathbf{x}_{1}\sim p_{1}\). When parameterized with transformers (Vaswani et al., 2017), stochastic interpolants are state-of-the-art in image generation (Esser et al., 2024). In particular, we adopt the notation, architecture, and training framework of Scalable Interpolant Transformer (SiT) (Ma et al., 2024), to which we refer for further exposition.

## 3 Method

### Tokenizing Peptide Trajectories

Given a chemical specification of a molecular system with \(N\) atoms, our aim is to learn a generative model over time-series \(\bm{\chi}\equiv[\mathbf{X}_{1},\ldots\mathbf{X}_{T}]\) of corresponding molecular structures \(\mathbf{X}_{i}\in\mathbb{R}^{3N}\) for some trajectory length \(T\). In this work, we specialize to MD trajectories of short peptides (Sections 4.1-4.4) or single-chain proteins (4.4). Thus, our chemical specifications are amino acid sequences\(A=\{1\ldots 20\}^{L}\), and we adopt an \(SE(3)\)-based parameterization of peptide structures (Jumper et al., 2021; Yim et al., 2023). In this parameterization, the all-atom coordinates of each amino acid residue are _implicitly_ described by a roto-translation (i.e., element of \(SE(3)\)) corresponding to the rigid body motion of the residue, and seven torsion angles describing its conformation:

\[\bm{\chi}_{t}^{l}=[g,\tau_{1},\ldots\tau_{7}],\quad g\in SE(3),\tau\in\mathbb{T },\quad\bm{\chi}\in\left(\left[SE(3)\times\mathbb{T}^{7}\right]^{L}\right)^{T}\] (3)

Throughout, subscripts indicate time and superscripts residue indices. The undefined torsion angles can be randomized and are unsupervised for residues with fewer than seven torsion angles.

Traditionally, equivariant architectures have been required for geometry-aware processing of polypeptide structures. However, to learn a scalable generative model over this space of roto-translations and torsion angles, we seek to represent each \(\bm{\chi}_{t}^{l}\) in terms of an \(SE(3)\)-invariant feature vector--a _token_ suitable for processing by vanilla transformers. To obtain such a vector, we leverage the fact that we are concerned with _conditional trajectory generation_--meaning that there always exists at least one frame in the trajectory with un-noised roto-translations, which we do not need to generate and can reference in the modeling process. Inspired by analogy to video compression, we refer to such frames as _key frames_. We can then obtain \(SE(3)\)-invariant tokens by parameterizing the roto-translations of remaining structures _relative_ to the key frames.

In more detail, given \(K\) key frames at times \(t_{1}\ldots t_{K}\) we tokenize residue \(j\) in frame \(t\) as:

\[\bm{\chi}_{t}^{j}=\left[\phi\left([g_{t_{1}}^{j}]^{-1}g_{t}^{j}\right),\ldots, \phi\left([g_{t_{K}}^{j}]^{-1}g_{t}^{j}\right),\psi([\tau_{t}^{j}]_{1}),\ldots \psi([\tau_{t}^{j}]_{7})\right]\subset\mathbb{R}^{7K+14}\] (4)

where \(g_{t}^{j}\in SE(3)\) represents the roto-translation and \([\tau_{t}^{j}]_{i}\) the torsion angles of residue \(j\) at frame \(t\). Here, \(\phi:SE(3)\rightarrow\mathbb{R}^{7}\) parameterizes an element of \(SE(3)\) in terms of a unit quaternion and translation vector, and \(\psi:\mathbb{T}\rightarrow\mathbb{R}^{2}\) converts torsion angles to points on the unit circle. We thus obtain a (\(7K+14\))-dimensional array for each residue in every frame. Because the relative roto-translations and torsion angles are both \(SE(3)\)-invariant, in this manner we can represent a polypeptide molecular trajectory as an \((T\times L)\)-array of \(SE(3)\)-invariant tokens.

To _untokenize_ a generated trajectory of tokens to all-atom coordinates \(\mathbf{X}_{t}\in\mathbb{R}^{3N}\), we first convert each predicted quaternion and translation vector to a relative roto-translation and apply it to the key frame(s), obtaining absolute roto-translations. We then read off the torsion angles from the unit circle and assemble the all-atom coordinates as implemented in Jumper et al. (2021), averaging the reconstructions from different key frames if needed.

### Flow Model Architecture

Our base modeling task is to generate a distribution over \(\mathbb{R}^{T\times L\times(7K+14)}\) conditioned on roto-translations of one or more key frames \(g_{t_{1}}\ldots g_{t_{K}}\), and (in most settings) amino acid identities \(A\). To do so, we learn a flow-based model via the stochastic interpolant framework described in SiT (Ma et al., 2024) and parameterize a velocity network \(v_{\theta}(\cdot\mid g_{t_{1}}\ldots g_{t_{K}},A):\mathbb{R}^{T\times L\times (7K+14)}\times[0,1]\rightarrow\mathbb{R}^{T\times L\times(7K+14)}\). To condition on the key frames and amino acids, we first provide the sequence embedding to several IPA layers (Jumper et al., 2021) that embed the key frame roto-translations; these conditioning representations (which are \(SE(3)\)-invariant) are broadcast across the time axis and added to the input embeddings. The main trunk of the network consists of alternating attention blocks across the residue index and across time, with the construction of each block closely resembling DiT (Peebles and Xie, 2023). Sidechain torsions and roto-translation offsets, when available, are directly provided to the model as conditioning tokens. Further details are provided in Appendix A.1.

In the molecular inpainting setting where we also _generate_ the amino acid identities, we additionally require a generative framework over these discrete variables. While several formulations of discrete diffusion or flow-matching are available (Hoogeboom et al., 2021; Austin et al., 2021; Campbell et al., 2022, 2024), we select Dirichlet flow matching (Stark et al., 2024) as it is most compatible with the continuous-space, continuous-time stochastic interpolant framework used for the positions. Specifically, we place the amino acid identities on the 20-dimensional probability simplex (one per amino acid), augment the token representations with these variables, and regress against a \(T\times L\times(7K+14+20)\)-dimensional vector field. Further details are provided in Appendix A.2.

### Conditional Generation

We present the precise specifications of the various conditional generation settings in Table 1. Depending on the task, we choose the key frames to be the first frame \(g_{1}\) or the first and last frames \(g_{1},g_{T}\). Each conditional generation task is further characterized by providing the ground-truth tokens of known frames or residues as additional inputs to the velocity network. Meanwhile, mask tokens are provided for the unknown frames and residues that the model generates. For example, in the upsampling setting, we provide ground-truth tokens every \(M\) frames, while mask tokens are provided for all other frames. We note that in the inpainting setting, the model accesses the roto-translations \(g\) of designed residues at the trajectory endpoints via the key frames, constituting a slight departure from the full inpainting setting. However, these residues are not observed for intermediate frames, and their identities are never provided to the model.

## 4 Experiments

We evaluate MDGen on its ability to learn from MD simulations of training molecules and then sample trajectories for unseen molecules. We focus on _tetrapeptides_ as our main molecule class for evaluation as they provide nontrivial chemical diversity while remaining small enough to tractably simulate to equilibrium (Klein et al., 2024). Sections 4.1-4.3 thoroughly evaluate our model on forward simulations, interpolation / transition path sampling, and trajectory upsampling on test peptides. Section 4.4 provides proof-of-concept and preliminary exploration of additional tasks--namely, inpainting for dynamics-conditioned design, long trajectories with Hyena (Poli et al., 2023), and scaling to simulations of protein monomers. Separate models are trained for each setting.

To obtain tetrapeptide MD trajectories for training and evaluation, we run implicit- and explicit-solvent, all-atom simulations of \(\approx\)3000 training, 100 validation, and 100 test tetrapeptides for 100 ns. For proteins, we use explicit-solvent, all-atom simulations from the ATLAS dataset (Vander Meersche et al., 2024), which provides three 100 ns trajectories for each of 1390 structurally diverse proteins. Unless otherwise specified, models are trained with trajectory timesteps of \(\Delta t=10\) ps. Our default baselines consist of _replicate MD simulations_ ranging from 10 ps to 100 ns, with additional comparisons in each section as appropriate.

Our experiments make extensive use of Markov State Models (MSMs), a widely used coarse-grained representation of molecular dynamics (Prinz et al., 2011; Noe et al., 2013). We obtain an MSM to represent a system by discretizing its MD trajectory (parameterized with torsion angles) into 10 metastable states and estimating the transition probabilities between them. Appendix B provides further details on constructing MSMs and other experimental settings. Additional results, including structural validations and further comparisons with related methods, can be found in Appendix C.

### Forward Simulation

In the _forward simulation_ setting, we train a model to sample 10 ns trajectories conditioned on the first frame. By chaining together successive model rollouts at inference time, we obtain 100 ns trajectories for each peptide to compare with ground-truth simulations. We evaluate if these sampled trajectories (1) match the structural distribution of trajectories from MD, (2) accurately capture the dynamical content of MD, and (3) traverse the state space in less wall-clock time than MD.

**Distributional similarity.** We report the Jensen-Shannon divergence (JSD) between the ground-truth and emulated trajectories along various collective variables shown in Figure 2 and Table 2. The first

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Setting & Key frames & Generate & Conditioned on & Token dim. \\ \hline Forward simulation & \(g_{1}\) & \(g_{1\cdots T},\boldsymbol{\tau}_{1\cdots T}\) & \(g_{1},\boldsymbol{\tau}_{1},A\) & 21 \\ Interpolation & \(g_{1},g_{T}\) & \(g_{1\cdots T},\boldsymbol{\tau}_{1\cdots T}\) & \(g_{1,T},\boldsymbol{\tau}_{1\cdots T},A\) & 28 \\ Upsampling & \(g_{1}\) & \(g_{1\cdots T},\boldsymbol{\tau}_{1\cdots T}\) & \(g_{1+\{1,2,\cdots\}M},\boldsymbol{\tau}_{1+\{1,2,\cdots\}M},A\) & 21 \\ Inpainting & \(g_{1},g_{T}\) & \(g_{1\cdots T},A\) & \(g_{1\cdots T}^{\text{known}}\) & 7 (+20) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Conditional generation settings. \(g\): roto-translations, \(\boldsymbol{\tau}\): torsions, \(A\): residue identities \(M\): upsampling factor. Superscripts indicate residue index and subscripts indicate frame (time) index. For inpainting, we find that excluding identities and torsions reduces overfitting.

set of these are the individual torsion angles (backbone and sidechains) in each tetrapeptide. The second set of variables are the top independent components obtained from _time-lagged independent components analysis_ (TICA), representing the slowest dynamic modes of the peptide. By each of these collective variables, MDGen demonstrates excellent distributional similarity to the ground truth, approaching the accuracy of replicate 100-ns simulations. To more stringently assess the ability to locate and populate modes in the joint distribution over state space, we build Markov State Models (MSMs) for each test peptide using the MD trajectory, extract the corresponding metastable states, and compare the ground-truth and emulated distributions over metastable states. Our model captures the relative ranking of states reasonably well and rarely misses important states or places high mass on rare states (Figure 2D).

**Dynamical content.** We compute the dynamical properties of each tetrapeptide in terms of the _decorrelation time_ of each torsion angle from the MD simulation and from our sampled trajectory. Intuitively, this assesses if our model can discriminate between slow- and fast-relaxing torsional barriers. The correlation between true and predicted relaxation timescales is plotted in Figure 2F, showing excellent agreement for sidechain torsions and reasonable agreement for backbones. To assess coarser but higher-dimensional dynamical content, we compute the _flux matrix_ between all pairs of distinct metastable states using ground-truth and sampled trajectories and find substantial Spearman correlation between their entries (mean \(\rho=0.67\pm 0.01\); Figure 8). Thus, our simulation rollouts can accurately identify high-flux transitions in the peptide conformational landscape.

**Sampling speed.** Averaged across test peptides, our model samples 100 ns-equivalent trajectories in \(\approx\)60 GPU-seconds, compared to \(\approx\)3 GPU-hours for MD. To quantify the speedup more rigorously,

\begin{table}
\begin{tabular}{l c c c c|c} \hline \hline C.V. & Ours & 10 ns & 1 ns & 100 ps & 100 ns \\ \hline Torsions (bb) & **.130** &.145 &.212 &.311 &.103 \\ Torsions (sc) & **.093** &.111 &.261 &.403 &.055 \\ Torsions (all) & **.109** &.125 &.240 &.364 &.076 \\ \hline TICA-0 & **.230** &.323 &.432 &.477 &.201 \\ TICA-0,1 joint & **.316** &.424 &.568 &.643 &.268 \\ \hline MSM states & **.235** &.363 &.493 &.527 &.208 \\ \hline Runtime & 60s & 1067s & 107s & 11s & 3h \\ \hline \hline \end{tabular}
\end{table}
Table 2: JSD between sampled and ground-truth distributions, with replicate simulations as baselines. 100 ns represents oracle performance.

Figure 2: **Forward simulation evaluations on test peptides. (A) Torsion angle distributions for the six backbone torsion angles from MD trajectories (orange) and sampled trajectories (blue). (B, C) Free energy surfaces along the top two TICA components computed from backbone and sidechain torsion angles. (D) Markov State Model occupancies computed from MD trajectories versus sampled trajectories, pooled across all test peptides (\(n=1000\) states total). (E) Wall-clock decorrelation times of the first TICA component under MD versus our model rollouts. (F) Relaxation times of torsion angles computed from MD versus sampled trajectories, pooled across all test peptides—508 backbone (blue) and 722 sidechain (orange) torsions in total. (G) Torsion angles in the tetrapeptide AAAA colored by the decorrelation time computed from MD (top) and from rollout trajectories (bottom).**

we compute the decorrelation wall-clock times along the slowest independent component from TICA, capturing how quickly the simulation traverses the highest barriers in state space. These times are plotted in Figure 2E, showing that our model achieves a speedup of 10x-1000x over the MD simulation for 78 out of 100 peptides (the other 22 peptides did not fully decorrelate).

### Interpolation

In the _interpolation_ or _transition path sampling_ setting, we train a model to sample 1 ns trajectories conditioned on the first and last frames. For evaluation, we identify the two most well-separated states (i.e., with the least flux between them) for each test peptide and sample an ensemble of 1000 transition paths between them. Figure 3 shows an example of such a sampled path, which passes through several intermediate states on the free energy surface to connect the two endpoints.

To evaluate the accuracy of these sampled transitions, we cannot directly compare with MD trajectories since, in most cases, there are zero or very few 1-ns transitions between the two selected states (by design, the transition is a _rare event_). Thus, we instead discretize the trajectory over MSM metastable states and evaluate the _path likelihood_ under the transition path distribution from the reference MSM (details in Appendix B.3). We also report the fraction of valid paths (i.e., non-zero probability) and the JSD between the distribution of visited states from our path distribution versus the transition path distribution of the reference MSM. For baselines, we sample transition paths from MSMs constructed from replicate MD simulations of varying lengths and compute the same metrics for these (discrete) path ensembles under the reference MSM.

As shown in Figure 3, our paths have higher likelihoods than those sampled from any replicate MD MSM shorter than 100ns, which is the length of the reference MD simulation itself. Moreover, MDGen's ensembles have the best JSDs to the distribution of visited states of the reference MD MSM and the highest fraction on valid non-zero probability paths. Hence, our model enables zero-shot sampling of trajectories corresponding to arbitrary rare transitions for unseen peptides.

### Upsampling

Molecular dynamics trajectories are often saved at relatively long time intervals (10s-100s of picoseconds) to reduce disk storage; however, some molecular motions occur at faster timescales and

Figure 3: **Transition path sampling results. (Top) Intermediate states of one of the 1-nanosecond interpolated trajectories between two metastable states for the test peptide IPGD. (Bottom Left) The corresponding trajectory on the 2D free energy surface of the top two TICA components (more examples in Figure 9). (Bottom Right) Statistics averaged over 100 test peptides and 1000 paths for each of them. Shown are JSD, fraction of drawn paths that are valid transition paths, and average path likelihood of our discretized transitions under the reference MSM compared to discrete transitions drawn from the reference MSM or alternative MSMs built from replica simulations of varying lengths.**

would be missed by downstream analysis of the saved trajectory. In the _upsampling_ setting, we train MDGen to upsample trajectories saved with timestep 10 ps to a finer timestep of 100 fs, representing a 100x upsampling factor. To evaluate if the upsampled trajectories accurately capture the fastest dynamics, we compute the _autocorrelation function_\(\langle\cos(\theta_{t}-\theta_{t+\Delta t})\rangle\) of each torsion angle in the test peptides as a function of lag time \(\Delta t\) ranging from 100 fs to 100 ps.

Representative examples of ground truth, subsampled, and reconstructed autocorrelation functions for two test peptides are shown in Figure 4 (further examples in Figure 10). We further compute the _dynamical content_ as the negative derivative of the autocorrelation with respect to log-timescale, which captures the extent of dynamic relaxations occurring at that timescale (Shaw et al., 2009). These visualizations highlight the significant dynamical information absent from the subsampled trajectory and which are accurately recovered by our model. In particular, our model distinctly recovers the _oscillations_ of certain torsion angles as seen in the non-monotonicity of the autocorrelation function at sub-picosecond timescales; these features are completely missed at the original sampling frequency.

### Additional Tasks

**Inpainting Design.** We aim to sample trajectories conditioned on the dynamics of the two flanking residues of the tetrapeptide; in particular, the model determines the identities and dynamics of the two inner residues. We focus on _dynamics scaffolding_ as one possible higher-level objective of inpainting: given the conformational transition of the observed residues, we hope to design peptides that support flux between the corresponding Markov states. Thus, for each test peptide, we select a 100-ps transition between the two most well-connected Markov states, mask out the inner residue identities and dynamics, and inpaint them with our model. To evaluate the designs, we compute the fraction of

Figure 4: **Recovery of fast dynamics via trajectory upsampling for peptide GTLM. (_Left_) Autocorrelations of each torsion angle from (—) the original 100 fs-timestep trajectory, (\(\bullet\)) the subsampled 10 ns-timestep trajectory, and ( \(\cdots\)) the reconstructed 100 fs-timestep trajectory (all length 100 ns). (_Right_) Dynamical content as a function of timescale from the upsampled vs. ground truth trajectories, stacked for all torsion angles (same color scheme). The subsampled trajectory contains only the shaded region and our model recovers the unshaded region. Further examples in Figure 10.**

Figure 5: Autocorrelation functions of MDGEN sidechain torsion angles computed from a 10-ns MD trajectory (_left_) versus a single **100k-frame model sample** with Hyena (_right_), capturing dynamics spanning four orders of magnitude.

\begin{table}
\begin{tabular}{l c c} \hline \hline Method & High & Random \\  & Flux & Path \\ \hline MDGen & **52.1\%** & **62.0\%** \\ DynMPNN & 17.4\% & 24.5\% \\ S-MPNN & 16.3\% & 13.5\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: Sequence recovery for the inner two peptides when conditioning on the partial trajectory (MDGen), the two terminal frames (DynMPNN), or a single frame (S-MPNN).

generated residue types that are identical to the tetrapeptide in which the target transition is known to occur. We compare MDGen with a bespoke inverse folding baseline that is provided the two terminal states (i.e., two fully observed MD frames), and thus designs peptides that support the two modes (rather than additionally a partially-observed _transition_ between them). We call this baseline DynMPNN, and it otherwise has the same architecture and settings as MDGen (more details in Appendix B.3). We find (Table 3) that MDGen recovers the ground-truth peptide substantially more often than DynMPNN when conditioned on a high-flux path or (as a sanity check) a random path from the reference simulation.

**Scaling to Long Trajectories.** Although Section 4.1 showed that our model can emulate long trajectories, this was limited to rollouts of 1000 frames at a time with coarse 10 ps timesteps, potentially missing faster dynamics or disrupting slower dynamics. Thus, we investigate generating extremely long consistent trajectories that capture timescales spanning several orders of magnitude _within a single model sample_. To do so, we replace the time attention in our baseline SiT architecture with a non-causal Hyena operator (Poli et al., 2023), which has \(O(N\log N)\) rather than \(O(N^{2})\) overhead. We overfit on 100k-frame, 10-ns trajectories of the pentapeptide MDGEN and compare the torsional autocorrelation functions computed from a _single_ generated trajectory with a _single_ ground truth trajectory (Figure 5). Although not yet comparable to the main set of forward simulation experiments due to data availability and architectural expressivity reasons, these results demonstrate proof-of-concept for longer context lengths in future work.

**Protein Simulation.** To demonstrate the applicability of our method for larger systems such as proteins, we train a model to emulate all-atom simulations of proteins from the ATLAS dataset (Vander Meersche et al., 2024) conditioned on the first frame (i.e., forward simulation). We follow the same splits as Jing et al. (2024). Due to the much larger number of residues, we generate samples with 250 frames and 400 ps timestep, such that a single sample emulates the 100 ns ATLAS reference trajectory. The difficulty of running fully equilibrated trajectories for proteins prevents the construction of Markov state models used in our main evaluations. Instead, we compare statistical properties of forward simulation ensembles following Jing et al. (2024). Our ensembles successfully emulate the ground-truth ensembles at a level of accuracy between AlphaFlow and MSA subsampling while being orders of magnitude faster per generated structure than either (Table 4; Figure 6).

## 5 Discussion

**Limitations.** Our experiments have validated the model and architecture for peptide simulations; however, a few limitations provide opportunities for future improvement. Due to the reliance on key frames, the model is not capable of unconditional generation or inpainting of residue rototranslations. The weaker performance on protein monomers relative to peptides suggests that scaling to larger systems will likely require additional data or methodological innovations. Fine-tuning of single structure models for co-generation of the key frames and trajectory tokens, similar to the content-frame decomposition of video diffusion models (Yu et al., 2024), may provide improvement. Since our tokenization scheme is specific to polypeptides, alternative strategies will be needed to model all-atom trajectories of more general systems, such as organic ligands, materials, or explicit solvent. More ambitious applications (see below) may require the ability to model trajectories not of a predefined set of atoms but over a region of space in which atoms may enter and exit. As such, we anticipate advancements in tokenization and architecture to be a fruitful direction of future work.

**Opportunities.** Similar to the foundational role of video generative models for understanding the macroscopic world (Yang et al., 2024), MD trajectory generation could serve as a multitask, unifying paradigm for deep learning over the microscopic world. Interpolation can be more broadly framed as _hypothesis generation_ for mechanisms of arbitrary molecular phenomena, especially when only partial information about the end states is supplied. Molecular inpainting could be a general technique to design molecular machinery by scaffolding more fine-grained and complex dynamics, for example, redesigning proteins to enhance rare transitions observed only once in a simulation or (with _ab initio_ trajectories) _de novo_ design of enzymatic mechanisms and motifs. Other types of conditioning not explored in this work may lead to further applications, such as conditioning over textual or experimental descriptors of the trajectory. Future availability of significantly more ground truth MD trajectory data for diverse chemical systems could be a chief enabler of such work. Lastly, considerations unique to molecular trajectories, such as equilibrium vs non-equilibrium processes, Markovianity, and the reversibility of the microscopic world contrasted with the macroscopic world (e.g., the missing arrow of time), could provide ripe areas for theoretical exploration.

## Acknowledgments and Disclosure of Funding

We thank Felix Faltings, Jason Yim, Mateo Reveiz, Gabriele Corso, and anonymous NeurIPS reviewers for helpful feedback and discussions.

This work was supported by the National Institute of General Medical Sciences of the National Institutes of Health under award number 1R35GM141861; the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Department of Energy Computational Science Graduate Fellowship under Award Number DESC0022158; the National Science Foundation under Grant No. 1918839; the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium; the Abdul Latif Jameel Clinic for Machine Learning in Health; the DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE) threats program; and the DARPA Accelerated Molecular Discovery program. This research used resources of the National Energy Research Scientific Computing Center (NERSC), a Department of Energy Office of Science User Facility using NERSC awards ASCR-ERCAP0027302 and ASCRERECAP0027818

## References

* Albergo et al. [2023] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023.
* Albergo and Vanden-Eijnden [2022] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In _The Eleventh International Conference on Learning Representations_, 2022.
* Alder and Wainwright [1959] Berni J Alder and Thomas Everett Wainwright. Studies in molecular dynamics. i. general method. _The Journal of Chemical Physics_, 31(2):459-466, 1959.
* Anderson et al. [2008] Joshua A Anderson, Chris D Lorenz, and Alex Travesset. General purpose molecular dynamics simulations fully implemented on graphics processing units. _Journal of computational physics_, 227(10):5342-5359, 2008.
* Austin et al. [2021] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* Campbell et al. [2022] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. _Advances in Neural Information Processing Systems_, 35:28266-28279, 2022.
* Campbell et al. [2024] Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. _arXiv preprint arXiv:2402.04997_, 2024.
* Campbell et al. [2021]John D Chodera and Frank Noe. Markov state models of biomolecular conformational dynamics. _Current opinion in structural biology_, 25:135-144, 2014.
* Darden et al. [1993] Tom Darden, Darrin York, and Lee Pedersen. Particle mesh ewald: An n log (n) method for ewald sums in large systems. _The Journal of chemical physics_, 98(12):10089-10092, 1993.
* Del Alamo et al. [2022] Diego Del Alamo, Davide Sala, Hassane S Mchaourab, and Jens Meiler. Sampling alternative conformational states of transporters and receptors with alphafold2. _Elife_, 11:e75751, 2022.
* Eastman et al. [2017] Peter Eastman, Jason Swails, John D Chodera, Robert T McGibbon, Yutong Zhao, Kyle A Beauchamp, Lee-Ping Wang, Andrew C Simmonett, Matthew P Harrigan, Chaya D Stern, et al. Openmm 7: Rapid development of high performance algorithms for molecular dynamics. _PLoS computational biology_, 13(7):e1005659, 2017.
* Esser et al. [2024] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. _arXiv preprint arXiv:2403.03206_, 2024.
* Fu et al. [2023] Xiang Fu, Tian Xie, Nathan J Rebello, Bradley Olsen, and Tommi S Jaakkola. Simulate time-integrated coarse-grained molecular dynamics with multi-scale graph networks. _Transactions on Machine Learning Research_, 2023.
* Satorras et al. [2021] Victor Garcia Satorras, Emiel Hoogeboom, Fabian Fuchs, Ingmar Posner, and Max Welling. E (n) equivariant normalizing flows. _Advances in Neural Information Processing Systems_, 34:4181-4192, 2021.
* Ho et al. [2022] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _Advances in Neural Information Processing Systems_, 35:8633-8646, 2022.
* Hoogeboom et al. [2021] Emiel Hoogeboom, Alexey A Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. _arXiv preprint arXiv:2110.02037_, 2021.
* Husic and Pande [2018] Brooke E Husic and Vijay S Pande. Markov state models: From an art to a science. _Journal of the American Chemical Society_, 140(7):2386-2396, 2018.
* Jing et al. [2024] Bowen Jing, Bonnie Berger, and Tommi Jaakkola. Alphafold meets flow matching for generating protein ensembles. _arXiv preprint arXiv:2402.04845_, 2024.
* Jumper et al. [2021] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* Klein et al. [2024] Leon Klein, Andrew Foong, Tor Fjelde, Bruno Mlodozeniec, Marc Brockschmidt, Sebastian Nowozin, Frank Noe, and Ryota Tomioka. Timewarp: Transferable acceleration of molecular dynamics by learning time-coarsened dynamics. _Advances in Neural Information Processing Systems_, 36, 2024.
* Kohler et al. [2021] Jonas Kohler, Andreas Kramer, and Frank Noe. Smooth normalizing flows. _Advances in Neural Information Processing Systems_, 34:2796-2809, 2021.
* Laio and Parrinello [2002] Alessandro Laio and Michele Parrinello. Escaping free-energy minima. _Proceedings of the national academy of sciences_, 99(20):12562-12566, 2002.
* Lipman et al. [2022] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. _arXiv preprint arXiv:2210.02747_, 2022.
* Liu et al. [2022] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. _arXiv preprint arXiv:2209.03003_, 2022.
* Ma et al. [2024] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. _arXiv preprint arXiv:2401.08740_, 2024.
* McCammon et al. [1977] J Andrew McCammon, Bruce R Gelin, and Martin Karplus. Dynamics of folded proteins. _nature_, 267(5612):585-590, 1977.
* McCammon et al. [2018]Laurence Midgley, Vincent Stimper, Javier Antoran, Emile Mathieu, Bernhard Scholkopf, and Jose Miguel Hernandez-Lobato. Se (3) equivariant augmented coupling flows. _Advances in Neural Information Processing Systems_, 36, 2024.
* Midgley et al. (2022) Laurence Illing Midgley, Vincent Stimper, Gregor NC Simm, Bernhard Scholkopf, and Jose Miguel Hernandez-Lobato. Flow annealed importance sampling bootstrap. _arXiv preprint arXiv:2208.01893_, 2022.
* Noe et al. (2019) Frank Noe, Simon Olsson, Jonas Kohler, and Hao Wu. Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. _Science_, 365(6457):eaaw1147, 2019.
* Noe et al. (2013) Frank Noe, Hao Wu, Jan-Hendrik Prinz, and Nuria Plattner. Projected and hidden Markov models for calculating kinetics and metastable states of complex molecules. _The Journal of Chemical Physics_, 139(18):184114, 11 2013.
* Pande et al. (2010) Vijay S Pande, Kyle Beauchamp, and Gregory R Bowman. Everything you wanted to know about markov state models but were afraid to ask. _Methods_, 52(1):99-105, 2010.
* Peebles and Xie (2023) William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4195-4205, 2023.
* Perez-Hernandez et al. (2013) Guillermo Perez-Hernandez, Fabian Paul, Toni Giorgino, Gianni De Fabritiis, and Frank Noe. Identification of slow molecular order parameters for markov model construction. _The Journal of chemical physics_, 139(1), 2013.
* Poli et al. (2023) Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. In _International Conference on Machine Learning_, pages 28043-28078. PMLR, 2023.
* Prinz et al. (2011) Jan-Hendrik Prinz, Hao Wu, Marco Sarich, Bettina Keller, Martin Senne, Martin Held, John D. Chodera, Christof Schutte, and Frank Noe. Markov models of molecular kinetics: Generation and validation. _The Journal of Chemical Physics_, 134(17):174105, 2011.
* Rahman (1964) Aneesur Rahman. Correlations in the motion of atoms in liquid argon. _Physical review_, 136(2A):A405, 1964.
* Roblitz and Weber (2013) Susanna Roblitz and Marcus Weber. Fuzzy spectral clustering by pcca+: application to markov state models and data classification. _Advances in Data Analysis and Classification_, Jun 2013.
* Ryckaert et al. (1977) Jean-Paul Ryckaert, Giovanni Ciccotti, and Herman JC Berendsen. Numerical integration of the cartesian equations of motion of a system with constraints: molecular dynamics of n-alkanes. _Journal of computational physics_, 23(3):327-341, 1977.
* Scherer et al. (2015) Martin K. Scherer, Benjamin Trendelkamp-Schroer, Fabian Paul, Guillermo Perez-Hernandez, Moritz Hoffmann, Nuria Plattner, Christoph Wehmeyer, Jan-Hendrik Prinz, and Frank Noe. PyEMMA 2: A Software Package for Estimation, Validation, and Analysis of Markov Models. _Journal of Chemical Theory and Computation_, 11:5525-5542, 2015.
* Schreiner et al. (2024) Mathias Schreiner, Ole Winther, and Simon Olsson. Implicit transfer operator learning: Multiple time-resolution models for molecular dynamics. _Advances in Neural Information Processing Systems_, 36, 2024.
* Shaw et al. (2009) David E Shaw, Ron O Dror, John K Salmon, JP Grossman, Kenneth M Mackenzie, Joseph A Bank, Cliff Young, Martin M Deneroff, Brannon Batson, Kevin J Bowers, et al. Millisecond-scale molecular dynamics simulations on anton. In _Proceedings of the conference on high performance computing networking, storage and analysis_, pages 1-11, 2009.
* Stark et al. (2024) Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. _arXiv preprint arXiv:2402.05841_, 2024.
* Sugita and Okamoto (1999) Yuji Sugita and Yuko Okamoto. Replica-exchange molecular dynamics method for protein folding. _Chemical physics letters_, 314(1-2):141-151, 1999.
* Sugita et al. (2015)Yann Vander Meersche, Gabriel Cretin, Aria Gheeraert, Jean-Christophe Gelly, and Tatiana Galochkina. Atlas: protein flexibility description from atomistic molecular dynamics simulations. _Nucleic Acids Research_, 52(D1):D384-D392, 2024.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Verlet (1967) Loup Verlet. Computer" experiments" on classical fluids. i. thermodynamical properties of lennard-jones molecules. _Physical review_, 159(1):98, 1967.
* Wehmeyer et al. (2017) Christoph Wehmeyer, Martin K Scherer, Tim Hempel, Brooke E Husic, Simon Olsson, and Frank Noe. Introduction to markov state modeling with the pyemma software--v0. 3.
* Yang et al. (2024) Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Video as the new language for real-world decision making. _arXiv preprint arXiv:2402.17139_, 2024.
* Yim et al. (2023) Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. _arXiv preprint arXiv:2302.02277_, 2023.
* Yu et al. (2024) Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, and Anima Anandkumar. Efficient video diffusion models via content-frame motion-latent decomposition. _arXiv preprint arXiv:2403.14148_, 2024.
* Zheng et al. (2023) Shuxin Zheng, Jiyan He, Chang Liu, Yu Shi, Ziheng Lu, Weitao Feng, Fusong Ju, Jiaxi Wang, Jianwei Zhu, Yaosen Min, et al. Towards predicting equilibrium distributions for molecular systems with deep learning. _arXiv preprint arXiv:2306.05445_, 2023.

Method Details

### Flow Model Architecture

**Notation.** Here, as in the main text, we use the following notation:

* \(T\): number of trajectory frames
* \(L\): number of amino acids
* \(K\): number of key frames, with indicies \(t_{1}\dots t_{K}\)

In Algorithms 1-3 below, we modify the architectures of DiffusionTransformerAttentionLayer and DiffusionTransformerFinalLayer from DiT (Peebles and Xie, 2023). Elements from these layers are also then incorporated into our custom InvariantPointAttentionLayer.

``` Input: noisy tokens \(\bm{\chi}\in\mathbb{R}^{T\times L\times(7K+14)}\), conditioning tokens \(\bm{\chi}_{\text{cond}}\in\mathbb{R}^{T\times L\times(7K+14)}\), key frame roto-translations \(g_{t_{1}}\dots g_{t_{K}}\in\left(SE(3)^{L}\right)^{K}\), flow matching time \(t\), amino acid identities \(A\in\{1,\dots 20\}^{L}\), conditioning mask \(\mathbf{m}\in\{0,1\}^{T\times L\times(7K+14)}\) Output: flow velocity \(v\in\mathbb{R}^{T\times L\times(7K+14)}\)
1\(t\leftarrow\mathrm{Embed}(t)\);
2for\(k\gets 1\)to\(K\)do
3\(\bm{\chi}_{k}=\mathrm{Embed}(A)+\sum_{k^{\prime}}\mathrm{Linear}(g_{t_{k}}^{-1}g _{t_{k^{\prime}}})\) ;
4for\(l\gets 1\)to\(num\_ipa\_layers\)do
5\(\bm{\chi}_{k}=\mathrm{InvariantPointAttentionLayer}(\bm{\chi},g_{k},t)\)
6\(\bm{\chi}=\sum_{k}\bm{\chi}_{k}+\mathrm{Linear}(\bm{\chi})+\mathrm{Linear}(\bm{\chi}_{\text{cond}} \odot\mathbf{m})+\mathrm{Embed}(\mathbf{m})\);
7for\(l\gets 1\)to\(num\_transformer\_layers\)do
8\(\bm{\chi}=\mathrm{DiffusionTransformerAttentionLayer}(\bm{\chi},t)\)
9return DiffusionTransformerFinalLayer\((\bm{\chi},t)\) ```

**Algorithm 1**Velocity network

``` Input:\(\bm{\chi}\in\mathbb{R}^{T\times L\times C}\), time conditioning \(t\)
1\((\alpha,\beta,\gamma)_{t,\ell,f}=\mathrm{Linear}(t)\);
2\(\bm{\chi}\mathrel{+}=g_{\ell}\odot\mathrm{AttentionWithRoPE}(\gamma_{\ell} \odot\mathrm{LayerNorm}(\bm{\chi})+\beta_{\ell},\text{dim}=1)\);
3\(\bm{\chi}\mathrel{+}=g_{\ell}\odot\mathrm{AttentionWithRoPE}(\gamma_{\ell} \odot\mathrm{LayerNorm}(\bm{\chi})+\beta_{\ell},\text{dim}=0)\);
4\(\bm{\chi}\mathrel{+}=g_{m}\odot\mathrm{MLP}(\gamma_{m}\odot\mathrm{ LayerNorm}(\bm{\chi})+\beta_{m})\);
5return\(\bm{\chi}\) ```

**Algorithm 2**DiffusionTransformerAttentionLayer

### Integrating Dirichlet Flow Matching

To additionally generate amino acid identities along with the trajectory dynamics, we integrate our SiT flow matching framework with Dirichlet flow matching (Stark et al., 2024). Specifically, we now parameterize a velocity network \(v_{\theta}:\left(\mathbb{R}^{7K}\oplus\mathbb{R}^{20}\right)^{T\times L} \times\left[0,1\right]\rightarrow\left(\mathbb{R}^{7K}\oplus\mathbb{R}^{20} \right)^{T\times L}\). No architecture modifications are necessary other than augmenting the tokens with one-hot tokens of residue identity, broadcasted across time. At training time, we sample from the Dirichlet probability path (rather than the Gaussian path) for those token elements. However, the parameterization is subtle as Dirichlet FM trains with cross-entropy loss, contrary to the standard flow-matching MSE loss. Thus, during _training time_ we minimize the loss

\[\mathcal{L}=\mathbb{E}\left[\|v_{\theta}\llbracket\ldots,\texttt{;}\texttt{-}20 \rrbracket-u_{t}(\boldsymbol{\chi}_{t}\mid\boldsymbol{\chi}_{1})\|^{2}+ \mathrm{CrossEntropy}(\mathrm{Softmax}(v_{\theta}\llbracket\ldots,\texttt{-}20 \texttt{:}\texttt{1}),A)\right]\] (5)

That is, we interpret the last 20 outputs in the channel dimension as logits over the 20 residue types. At _inference time_, on the other hand, we convert these logits to the Dirichlet FM flow field:

\[v_{\theta}^{\prime}=\mathrm{Concat}\left(v_{\theta}\llbracket\ldots,\texttt{; }\texttt{-}20\rrbracket,\sum_{i}\mathrm{Softmax}(v_{\theta}\llbracket\ldots, \texttt{-}20\texttt{:}\texttt{1})_{i}\cdot u_{\text{DFM}}(\cdot\mid x_{1}=i)\right)\] (6)

where \(u_{\text{DFM}}\) is the appropriate Dirichlet vector field from Stark et al. (2024).

### Conditional Generation

We control the conditional generation settings by simply setting appropriate entries of the conditioning mask \(\mathbf{m}\) in Algorithm 1 to 1 or 0. Specifically,

* For the forward simulation setting, \(\mathbf{m}[t,\ell,c]=\begin{cases}1&t=1\\ 0&t\neq 1\end{cases}\)
* For the inpainting setting, \(\mathbf{m}[t,\ell,c]=\begin{cases}1&t\in\{1,T\}\\ 0&t\notin\{1,T\}\end{cases}\)
* For the upsampling setting, \(\mathbf{m}[t,\ell,c]=\begin{cases}1&t\,\%\,M=1\\ 0&t\,\%\,M\neq 1\end{cases}\) where \(M\) is the upsampling factor.
* For the inpainting setting, \(\mathbf{m}[t,\ell,c]=\begin{cases}1&\ell\in\mathcal{S}_{\text{known}}\\ 0&\ell\notin\mathcal{S}_{\text{known}}\end{cases}\) where \(\mathcal{S}_{\text{known}}\) is the set of residues in the known part of the trajectory.

We use 1 indexing to be consistent with the main text. In practice, in the inpainting setting we also mask out all torsion angles and withhold the amino acid identities for all residues. Further, we do not train the model to generate the torsions as all, such that the tokenization yields \(\boldsymbol{\chi}\in\mathbb{R}^{T\times L\times TK}\). These interventions were observed to be necessary to prevent overfitting.

## Appendix B Experimental Details

### Markov State Models

A Markov State Model (MSM) is a representation of a system's dynamics discretized into \(r\) states \(s\in\{1\ldots r\}\) and a discrete timesteps separated by _time lag_\(\tau\) such that the dynamics are approximately Markovian (Husic and Pande, 2018; Chodera and Noe, 2014; Pande et al., 2010). An MSM is parameterized with a vector \(\boldsymbol{\pi}\) that assigns each state a stationary probability and a matrix \(T\) containing the probabilities for transitioning from state \(s_{t}\) to \(s_{t+1}\) after one timestep, i.e., \(T_{i,j}=p(s_{t+1}=j\mid s_{t}=i)\).

To build a Markov state model, we use PyEMMA (Scherer et al., 2015; Wehmeyer et al.) and its accompanying tutorials. Briefly, we first featurize molecular trajectories with all torsion angles as points on the unit circle, obtaining a \(2m\)-dimensional invariant trajectory where \(m\) is the number of torsion angles. We run TICA on these trajectories with kinetic scaling and then run \(k\)-means clustering with \(k=100\) over the first few (5-10 chosen by PyEMMA) TICA coordinates. We then estimate an MSM over these 100 states and use PCCA+ spectral clustering (Roblitz and Weber, 2013) to further group these into 10 metastable states. Our final MSM is built from the discrete trajectory over these 10 metastable states. In all cases we use timelag \(\tau=100\) ps.

**Unconditionally sampling an MSM.** To unconditionally sample a trajectory of length \(N\) from an MSM, we first sample the start state from the stationary distribution, i.e., \(s_{1}\sim\boldsymbol{\pi}\). We then iteratively sample each subsequent state as \(s_{t+1}\sim T_{s_{t},:}\).

**Sampling an MSM conditioned on a start state.** To sample a trajectory of length \(N\) conditioned on a starting state \(s_{1}\), we iteratively sample each subsequent state as \(s_{t+1}\sim T_{s_{t},:}\).

**Sampling an MSM conditioned on a start and end state.** For our transition path sampling evaluations in Section 4.2, we employ replica transition paths sampled from an MSM by conditioning on a start state \(s_{1}\) and end state \(s_{N}\). To do so, we iteratively sample each state between the conditioning states by utilizing the probability

\[p(s_{t+1}=j\mid s_{t}=i,s_{N}=k)=\frac{p(s_{N}=k\mid s_{t+1}=j,s_{t}=i)p(s_{t+ 1}=j\mid s_{t}=i)}{p(s_{N}=k\mid s_{t}=i)}.\] (7)

Firstly, the term \(p(s_{t+1}=j\mid s_{t}=i)\) is available in out transition matrix as \(T_{i,j}\). Secondly, we obtain \(p(s_{N}=k\mid s_{t}=i)\) as an entry of the \((N-t)th\) matrix exponential of the transition matrix. Specifically \(p(s_{N}=k\mid s_{t}=i)=T_{i,k}^{(N-t)}\) where the superscript denotes a matrix exponential. Lastly, we obtain the term \(p(s_{N}=k\mid s_{t+1}=j,s_{t}=i)\) by realizing that under the Markov assumption \(p(s_{N}=k\mid s_{t+1}=j,s_{t}=i)=p(s_{N}=k\mid s_{t+1}=j)\). Further, \(p(s_{N}=k\mid s_{t+1}=j)=T_{j,k}^{(N-t)-1}\).

Replacing the terms in Equation 7 results in

\[p(s_{t+1}=j\mid s_{t}=i,s_{N}=k)=\frac{T_{j,k}^{(N-t-1)}T_{i,j}}{T_{i,k}^{(N-t) }}.\] (8)

Thus, we sample states \(s_{2}\dots s_{N-1}\) iteratively as

\[s_{t+1}\sim\frac{T_{i,s_{N}}^{(N-t-1)}T_{s_{t},:}}{T_{s_{t},s_{N}}^{(N-t)}}.\] (9)

### Tetrapeptide Molecular Dynamics

We run all-atom molecular dynamics simulations in OpenMM (Eastman et al., 2017) using the amber14 force field parameters with gbn2 implicit solvent or tip3pfb water model. Initial structures are generated with PyMOL, prepared with pdbfixer, and protonated at neutral pH. For explicit solvent, we prepare a solvent box with 10 A padding and neutralize the system with sodium or chloride ions. All simulations are integrated with Langevin thermostat at 350K with hydrogen bond constraints, timestep 2 fs, and friction coefficient \(0.3\) ps\({}^{-1}\) (explicit) or \(0.1\) ps\({}^{-1}\) (implicit). For explicit solvent, nonbonded interactions are cut off at 10 A with long-range particle-mesh Ewald. We first minimize the energy with L-BFGS and then equilibrate the system in the NVT ensemble for 20 ps. We then run 100 ns of production simulation in the NVT ensemble (implicit) or NPT ensemble with Monte Carlo barostat at 1 bar (explicit). We write heavy atom positions every 100 fs.

For explicit-solvent settings (forward simulation, interpolation, inpainting), we run simulations for 3109 training, 100 validation, and 100 test peptides. For implicit-solvent settings (upsampling), we run simulations for 2646 training, 100 validation, and 100 test peptides. All peptides are randomly chosen and split. Additionally, 5195 training and 100 validation implicit solvent simulations are run for the pentapeptide MDGEN.

### Evaluation Details

Trajectory FeaturizationWe featurize trajectories by selecting the sine and cosine of all torsion angles as the collective variables. Specifically, we featurize \(\psi,\phi\) backbone angles and all \(\chi\) sidechain torsion angles for each peptide. We then reduce dimensionality with Time-lagged Independent Components Analysis (TICA) (Perez-Hernandez et al., 2013) in PyEMMA (Scherer et al., 2015).

Jensen-Shannon DivergenceWe compute the JSD as implemented in scipy, i.e.,

\[\sqrt{\frac{D(p\mid m)+D(q\mid m)}{2}}\] (10)where \(m=(p+q)/2\). For the 1-dimenional JSD over torsion angles, we discretize the range \([-\pi,\pi]\) into 100 bins. For the 1-dimensional JSD over TIC-0, we discretize the range spanning the maximum and minimum values into 100 bins. For the 2-dimensional JSD over TIC-0,1 we discretize the space into \(50\times 50\) bins.

AutocorrelationThe autocorrelation of torsion angle \(\theta\) at time lag \(\Delta t\) is defined as \(\langle\cos(\theta_{t}-\theta_{t+\Delta t})\rangle\), corresponding to the inner product of \(\theta_{t},\theta_{t+\Delta t}\) on the unit circle. To compute the _decorrelation time_ of a torsion angle, we subtract the baseline inner product \(\langle\cos\theta\rangle^{2}+\langle\sin\theta\rangle^{2}\), this is analogous to removing the mean of a real-valued time series before computing the autocorrelation. The decorrelation time is then defined as the time required for the autocorrelation to fall below \(1/e\) of its initial value (which is always unity), with the subtracted baseline computed from the reference trajectory. In a small number of cases (21 torsions), the MDGen trajectory did not decorrelate within 1000 frames (10 ns), and we exclude the angle from Figure 2F.

To compute the decorrelation time for TIC-0, we now define the autocorrelation as

\[\mathbb{E}[(y_{t}-\mu)(y_{t+\Delta t}-\mu)]/\sigma^{2}\] (11)

where \(\mu,\sigma\) are computed from the _reference_ trajectory. Hence, when computed for a sampled MDGen trajectory, the autocorrelation may not start at unity and may not decay to zero. We report a _decorrelation time_ if starts above and falls below 0.5 within 1000 frames (10 ns), which happens in 74 out of 100 cases as shown in Figure 2E.

InterpolationIn our interpolation or transition path sampling experiments, we sample 1000 trajectories of length 1ns for each of our 100 test tetraepeptides. We first select a start state \(s_{1}\) and an end state \(s_{N}\) that exhibits non-trivial transitions. To do so, we consider a reference MD simulation of 100 ns for the tetrapeptide and obtain an MSM as described in Appendix B.1. From the MSM's transition matrix \(T\) and stationary distribution \(\bm{\pi}\), we compute the flux matrix \(F=T\odot Pi\) where \(Pi\) is the square matrix with \(\bm{\pi}\) in each column. The chosen start and end state is the row and column index of the smallest non-zero entry in \(F\).

With the start state \(s_{1}\) and end state \(s_{N}\) selected, we sample 1000 start frames \(\mathbf{x}_{1}\) and end frames \(\mathbf{x}_{N}\) from the states. The 1000 start frames are sampled from all frames in the reference MD simulation that belong to state \(s_{1}\). Analogously, the end frames are sampled from all frames belonging to state \(s_{N}\). Using the 1000 pairs of start and end frames, we condition MDGen on them and generate trajectories of 100 frames (1 ns). For evaluation, we discretize these trajectories under the 10-state clustering determined by the MSM of the reference MD simulation as described in Appendix B.1. Note that with the MSM lag time of 100 ps, these discrete trajectories are of length 10.

MD baselines. To sample transition paths of 1 ns between our selected start and end states, we employ MSMs built from replica MD simulations of varying lengths. For instance, for a replica MD simulation of 100ns, we first discretize its trajectory with the cluster assignments of the reference MD simulation (the same cluster assignments as we use to discretize the MDGen ensemble and that we use for evaluation). Next, we estimate an MSM from the discretized trajectory. We then proceed to sample 1000 transition paths from the MSM as described in Appendix B.1 where the path is conditioned on an end and start state. In the event that the replica MSM has zero transition probability for transitioning out of the start state or zero probability for transitioning into the end state (this occurs if the replica MD simulation never visited the start or end state), we treat all 1000 paths of the replica MD as having zero probability for our evaluation metrics which are further detailed in the following.

Computing TPS metrics. As described above, we obtain ensembles of 1000 discretized 1ns paths of 100 frames for both MDGen and the replica MD simulations. For these, in Figure 3, we show a JSD, the rate of valid paths, and the average path probability. These metrics are computed with respect to the MSM of the reference MD simulation of length 100 ns.

* To compute the JSD, we draw 1000 discrete transition paths from the reference MD simulation and compute the probability of visiting each state from the frequency with which each state is visited. We do the same for the transition path ensemble of MDGen (or the baseline) and compute the JSD between the categorical distributions as described above.
* The average path probability for an ensemble is the average of its paths' probabilities for transitioning from the start to the end state under the reference MSM. This probability can be computed as described in Appendix B.1.

* The valid path rate is the fraction of paths that have a non-zero probability.

InpaintingIn our inpainting experiments, we set out to design tetraptides that transition between two states. Considering the residue indices \(1,2,3\) and \(4\), we call the residues \(1,4\) the flanking residues which we condition on and \(2,3\) the inner residues which we aim to design. Specifically, we condition MDGen on the trajectory of the flanking residues' backbone coordinates and generate the residue identities of the inner two residues. To carry out this design for a single tetrapeptide, we draw 1000 samples from MDGen to estimate the mode of its joint distribution over the inner two residues.

The conditioning information (the trajectories of the outer two residues' backbone coordinates) is different for the two evaluation settings of designing transitions with _high flux_ or for designing arbitrary transitions. However, for both of them, the start and end frames are provided as conditioning information via the key frames. In the high flux setting, the conditioning information is obtained by sampling 1000 paths from the reference MD simulation of length 10 ps with 100 frames that start and end in the desired states. These states are determined as those with the maximum flux between them (see the paths about interpolation above for a description of flux). When designing residues that give rise to arbitrary random paths, the trajectories are randomly sampled from the reference simulation.

After sampling 1000 pairs of residues for the inner two residues, we select the most frequently occurring pair as the final design. For this design, we report the sequence recovery (the fraction of residues that match the original sequence of the MD simulations from which the conditioning information was sampled).

Inpainting Baselines. We aim to assess the benefit that is obtained by the trajectory-based inference of MDGen over a baseline that only takes the start frame or the start and end frame as input for designing residues that transition between two states. Thus, we construct DynMPNN and S-MPNN. These baselines use the same architecture as MDGen in the inpainting setting, but DynMPNN only obtains the start and end frames as key frames and via their roto-translation offsets for the first and last frames. S-MPNN is the analog with only the first frame.

Notably, in the inpainting setting, **MDGen** and the baselines do not treat torsion angles, and all torsion angle entries of the SE(3)-invariant tokens are set to 0. Furthermore, the model does not take the amino acids of the flanking residues as input. We make this choice of withholding all information about amino acid identities since otherwise, the models overfit on the arbitrary identities of the flanking residues and do not generalize to the test set.

Protein SimulationsFor training and evaluation on proteins, we use trajectories from the ATLAS dataset (Vander Meersche et al., 2024), which includes 3 replicates of 100 ns explicit-solvent, all-atom simulations for each of 1390 non-membrane protein monomers. The proteins are chosen from the PDB as representatives of all available ECOD domains and are thus structurally non-redundant. We split the dataset into 1265 training, 39 validation, and 82 test proteins by PDB release date following Jing et al. (2024). At training time, we randomly select a protein, select one of the three replicates, subsample every 40 frames, obtaining a training target with 250 frames. We train with random crops of up to 256 residues, but draw samples for the full protein at inference time. To compute statistical similarity of the MDGen ensembles with the ground truth MD ensembles, we compare the 250 frames with 30k pooled frames from all three trajectories. Baseline metrics and runtimes for AlphaFlow and MSA subsampling are taken directly from Jing et al. (2024). Analysis and visualization code for Table 4 and Figure 6 are provided courtesy of Jing et al. (2024).

RuntimeMD runtimes in Table 2 are tabulated on a NVIDIA T4 GPU. All MDGen experiments are carried out on NVIDIA A6000 GPUs. AlphaFlow and MSA subsampling runtimes in Table 4 are tabulated on NVIDIA A100 GPUs by Jing et al. (2024).

[MISSING_PAGE_EMPTY:19]

### Interpolation

Figure 9: Four of 1000 transition paths of MDGen for several tetrapeptides in the test set.

### Upsampling

Figure 10: **Recovery of fast dynamics via trajectory upsampling** for random test peptides. (_Left_) Autocorrelations of each torsion angle from (—) the original 100 fs-timestep trajectory, (\(\bullet\)) the subsampled 10 ns-timestep trajectory, and (\(\cdots\)) the reconstructed 100 fs-timestep trajectory (all length 100 ns). (_Right_) Dynamical content as a function of timescale from the upsampled vs. ground truth trajectories, stacked for all torsion angles (same color scheme). The subsampled trajectory contains only the shaded region and our model recovers the unshaded region.

### Inpainting

Figure 11: For six tetrapeptides, we show the states that we chose in our design experiments when designing transitions between the highest flux states. Column 1 shows the flux matrix with zeros on the diagonal. Column 2, the free energy surface of a 100 ns simulation and the selected start and end states based on the highest flux in the flux matrix. Column 3, the MSM that was built from the MD simulation.

### Structural Validation

In addition to distributional similarity and dynamical content, we also assess the frequency of clashes or high-energy structures in MDGen forward simulation rollouts. Specifically, we compute the distributions of:

* The closest distance between any pair of nonbonded atoms
* Nonbonded energy (Coulomb + Lennard-Jones)
* Torsional energy
* Heavy atom bond lengths
* Radius of gyration

These distributions are shown and compared to the ground truth in Figure 12. We find that the vast majority of MDGen structures are of high quality (i.e., clashes are rare) and adhere closely to the ground truth distributions.

### Additional Comparisons

In this section, we compare MDGen to Timewarp (Klein et al., 2024) and ITO (Schreiner et al., 2024), generative models for autoregressively rolling out surrogate MD trajectories. Note that these comparisons are limited to the forward simulation task as Timewarp and ITO are not capable of solving the other tasks.

* For **Timewarp**(Klein et al., 2024), we use the 4AA model with weights from the authors and sample 100 ns trajectories by running 2000 inference steps with timestep 50 ps. We do not use MH acceptance steps as the authors found exploration of the energy landscape to be much more effective without them.
* For **ITO**(Schreiner et al., 2024), transferable models across tetrapeptides are not available. We therefore re-train ITO on our tetrapeptide dataset with timesteps of 500 ps. We then run 200 inference steps to sample 100 ns trajectories.

For both methods, we observe that trajectories are unstable without further intervention. To bolster the baselines, we run OpenMM (Eastman et al., 2017) relaxation steps between each timestep to proceed with further analysis. We note that the Timewarp authors, in lieu of relaxation, rejected steps with an energy increase of 300 kJ / mol (Klein et al., 2024); however, we found that this strategy would reject the majority of proposed steps on generic tetrapeptides. In Figure 13, we visualize the free energy surfaces and torsion angle distributions for several peptides from Timewarp and ITO compared with MDGen. Table 5 shows the Jensen-Shannon divergences from the forward simulation rollouts across all test peptides (c.f. Table 2). Qualitatively and quantitatively, our model obtains better consistency with the ground-truth free energy surfaces.

Figure 12: Histograms of various structural validation metrics between MDGen forward simulation and reference trajectories, pooled across all test tetrapeptides.

## 6 Conclusion

Figure 13: Comparison of MDGen, Timewarp, and ITO in terms of forward simulation evaluations on test peptides. (_Left_) Torsion angle distributions for the six backbone torsion angles from MD trajectories (orange) and sampled trajectories (blue). (_Right_) Free energy surfaces along the top two TICA components computed from backbone and sidechain torsion angles.

\begin{table}
\begin{tabular}{l c c c} \hline \hline C.V. & MDGen & Timewarp & ITO \\ \hline Torsions (bb) & **.130** &.325 &.564 \\ Torsions (sc) & **.093** &.427 &.462 \\ Torsions (all) & **.109** &.383 &.505 \\ \hline TICA-0 & **.230** &.265 &.538 \\ TICA-0,1 joint & **.316** &.419 &.756 \\ \hline MSM states &.235 & **.222** &.414 \\ \hline Runtime & 60s & 599s & 2083s \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison of MDGen, Timewarp, and ITO in terms of the JSD between sampled and ground-truth distributions along various collective variables in the forward simulation setting.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We outline all contributions of the paper, emphasizing the ones with robust support and qualifying the ones for which the support is preliminary. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 5 for a discussion of limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide details sufficient to reproduce our model and experiments in Appendix A and Appendix B. In particular, the model architecture is detailed in Algorithm 1. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We have released our code and data under MIT license. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Training and test details are provided throughout the main text, figure and table captions, and Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Our results are reported across a large test set for the task (\(n=100\)) and focus more on the new qualitative capabilities that we introduce rather than quantitative improvements on benchmarks. Thus, we did not feel it was necessary to report error bars but would be happy to provide them upon request. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: Our models are small by modern standards and we did not feel it was necessary to explicitly highlight the compute time required for experiments. We provide some information on computational resources in Appendix B. Further, the runtimes reported in Tables 2 and 4 provide an idea of the efficiency of our models. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: All authors have read and adhered to, in every respect, the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This paper presents work whose goal is to advance the field of AI for Science. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not release data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We provide the original citations of major open-source software packages and datasets used in our work. We did not feel it was necessary to explicitly mention their open-source licenses as all are publicly available. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Model code and weights for reproducibility are provided via a public repository with instructions for replicating training and evaluation. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.