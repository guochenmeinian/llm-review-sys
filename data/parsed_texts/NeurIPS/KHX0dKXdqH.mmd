# Causal Imitation for Markov Decision Processes:

a Partial Identification Approach

 Kangrui Ruan

Columbia University

kr2910@columbia.edu

&Junzhe Zhang

Syracuse University

jzhan403@syr.edu

&Xuan Di

Columbia University

sharon.di@columbia.edu

Equal contribution; Kangrui: work performed while at Columbia University.

&Elias Bareinboim

Columbia University

eb@cs.columbia.edu

###### Abstract

Imitation learning enables an agent to learn from expert demonstrations when the performance measure is unknown and the reward signal is not specified. Standard imitation methods do not generally apply when the learner and the expert's sensory capabilities mismatch and demonstrations are contaminated with unobserved confounding bias. To address these challenges, recent advancements in causal imitation learning have been pursued. However, these methods often require access to underlying causal structures that might not always be available, posing practical challenges. In this paper, we investigate robust imitation learning within the framework of canonical Markov Decision Processes (MDPs) using partial identification, allowing the agent to achieve expert performance even when the system dynamics are not uniquely determined from the confounded expert demonstrations. Specifically, first, we theoretically demonstrate that when unobserved confounders (UCs) exist in an MDP, the learner is generally unable to imitate expert performance. We then explore imitation learning in partially identifiable settings -- either transition distribution or reward function is non-identifiable from the available data and knowledge. Augmenting the celebrated GAIL method (Ho & Ermon, 2016), our analysis leads to two novel causal imitation algorithms that can obtain effective policies guaranteed to achieve expert performance.

## 1 Introduction

Children often learn how to behave in an unfamiliar environment by imitating adults. Imitation learning (IL) enables a learning agent to behave in an unknown environment by observing expert demonstrations. It provides a viable approach for policy learning from demonstrations when the reward function is not fully known and reward signals are not specified [2, 30, 8, 20, 31]. Imitation learning has been widely applied across disciplines, such as autonomous driving [13, 39], robotics [18], natural language processing [10, 11, 40, 41], and chronic disease management [52, 44].

It has been acknowledged in the literature that imitation learning could face significant challenges when _unobserved confounding bias_ in expert demonstrations cannot be ruled out _a priori_[17, 57, 27, 42]. For illustration with simplicity, consider a Multi-Armed Bandit (MAB) model [28] described in Fig. 1; \(X\in\{0,1\}\) is a binary action, and

Figure 1: A multi-armed bandit model.

\(Y\) is the reward; \(U\) is a latent covariate (to the imitator) uniformly drawn over a binary domain \(\{0,1\}\). Values of the reward are decided by a reward function \(Y\gets X\oplus U\) where \(\oplus\) is a "\(xor\)" operator. An expert demonstrator, having access to covariate \(U\), selecting action based on an expert policy \(X\leftarrow\neg U\). Evaluating the expert's performance gives \(\mathbb{E}[Y]=\mathbb{E}[\neg U\oplus U]=1\). On the other hand, an imitator, mimicking the expert's behavior, will follow a policy \(\pi(X)=P(X)=0.5\), selecting action uniformly at random. Evaluating the imitator's performance gives \(\mathbb{E}_{\pi}\left[Y\right]=\sum_{x}\pi(x)\mathbb{E}[x\oplus U]=0.5\), far from the expert's performance \(\mathbb{E}[Y]=1\).

Causal Inference (CI) addresses the challenges of unobserved confounding bias within the observational data [32, 47, 53]. It leverages causal knowledge integral to the data generation process, typically represented as a causal diagram or potential outcomes [32, 43, 5]. More recently, incorporating causal inference methods into the imitation learning paradigm, _causal imitation learning_ has evolved into a critical area of research [16, 57, 27, 7, 49, 42]. To compensate for the presence of unobserved confounding bias, these methods rely on additional structural or parametric knowledge about causal relationships among variables in the environment. By utilizing such domain knowledge, the imitator is able to recover the underlying system dynamics (i.e., causal effect) from confounded expert demonstrations and, in turn, obtain an imitating policy that can achieve the expert's performance.

By and large, the combination of causal knowledge and observational data does not always allow one to point-identify the causal effect, called the _non-identifiable_. That is, more than one parametrization of the target effect is compatible with the same observational data and model assumptions [32, Def. 3.2.2]. For instance, in the MAB environment described previously, the imitator's performance \(\mathbb{E}_{\pi}\left[Y\right]\) is not identifiable from the confounded observational distribution \(P(X,Y)\)[32, Thm. 3.4.1]. _Partial identification_ methods concerned with inferring about target causal effects in non-identifiable settings, and has been a target of growing interest in the domains of causal inference [35, 12, 37, 14, 58], econometrics [21, 35, 38, 48], and more recently, in machine learning [25, 24, 23]. Among these works, two approaches are often employed: (1) bounds are derived for the target effect under minimal assumptions, or (2) additional untestable assumptions are invoked under which the causal effect is identifiable, and then sensitivity analysis is conducted to assess how the target causal effect varies as the untestable assumptions are changed. Despite their effectiveness in addressing data bias, partial identification has still been rarely explored in the context of imitation learning.

This paper studies the partial identification for imitation learning in a generalized sequential decision-making environment of Markov Decision Processes (MDPs, [36]). The imitator must determine a sequence of actions, while unobserved confounders cannot be ruled out a priori in expert demonstrations. We discuss the solutions case-by-case, depending on the identifiability of the underlying system dynamics from the confounded data, including the reward function \(\mathcal{R}\) and the transition distribution \(\mathcal{T}\). Specifically, our contributions can be summarized as follows. (1) We theoretically prove that when unobserved confounders generally exist, it is infeasible to learn a robust policy that is guaranteed to achieve expert performance from the demonstration data. (2) When only the transition distribution \(\mathcal{T}\) is identifiable, we propose a novel imitation algorithm that leverages the bounds over the non-identifiable reward \(\mathcal{R}\); by matching the weighted occupancy measure, the imitator is able to obtain a policy that can outperform the expert. (3) We propose an alternative algorithm when the reward \(\mathcal{R}\) is identifiable, but there is unobserved confounding affecting the transition \(\mathcal{T}\). Our proposed algorithms could be implemented by augmenting the celebrated generative adversarial imitation learning framework (GAIL, [19]). Table 1 briefly summarizes this paper's main contributions. Due to space constraints, all proofs are provided in Appendices A and B.

### Preliminaries

This section introduces the basic notations and definitions used throughout the paper. We use capital letters to denote random variables (\(X\)), small letters for their values (\(x\)), and \(\mathscr{D}_{X}\) for the domain

\begin{table}
\begin{tabular}{l|l|l} \hline \hline  & **Identifiable Reward** & **Non-Identifiable Reward** \\ \hline
**Identifiable Transition** & Standard IRL (e.g., GAIL[19]) & CAIL-\(\mathcal{R}\) (Alg. 1 in Sec. 3.1) \\ \hline
**Non-Identifiable Transition** & CAIL-\(\mathcal{T}\) (Alg. 2 in Sec. 3.2) & Inimitable (Thm. 1 in Sec. 2.1) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of main contributions in this paper, including the analysis and proposed algorithms.

[MISSING_PAGE_FAIL:3]

For analytical clarity, we define reward function \(\mathcal{R}(s,x)\) as the expected value \(\sum_{y}y\mathcal{R}(s,x,y)\). Fix the discounted factor \(\gamma\in[0,1]\). A common objective for an agent to optimize is the cumulative return \(R_{t}=Y_{t}+\gamma Y_{t+1}+\gamma^{2}Y_{t+2}+\cdots=\sum_{k=0}^{\infty}\gamma^{ k}Y_{t+k}\).

Imitation Learning.When a detailed parametrization of the transition distribution \(\mathcal{T}\) and the reward function \(\mathcal{R}\) is available, the agent can obtain an optimal policy using standard planning algorithms [36, 6]. However, in many practical applications, complete knowledge of these parametrizations is often unavailable, necessitating a learning process. In this paper, we consider the _imitation learning_ setting, where the agent has access to observed trajectories generated by the expert. More specifically, at each time step \(t\), the expert selects an action \(X_{t}\gets f_{X}(s_{t},\bm{u}_{t})\) based on the current state \(S_{t}=s_{t}\) and latent noise \(\bm{U}_{t}=\bm{u}_{t}\). Fig. 1(b) shows the graphical representation of the data-generating process of the expert; the highlighted bi-directed arrows, e.g., \(X_{t}\leftrightarrow Y_{t}\), indicate the presence of an unobserved confounder \(U\in\bm{U}_{t}\) affecting both the action \(X_{t}\) and outcome \(Y_{t}\). We summarize the expert trajectories using the observational distribution \(P(\bm{X},\bm{S},\bm{Y})\) over sequences of variables \(\bm{X}=\{X_{1},X_{2},\dots\}\), \(\bm{S}=\{S_{1},S_{2},\dots\}\), and \(\bm{Y}=\{Y_{1},Y_{2},\dots\}\). It is verifiable from Fig. 1(b) that Markov property holds with regard to distribution \(P(\bm{X},\bm{S},\bm{Y})\). For any horizon \(T\),

\[P\left(\bar{\bm{x}}_{1:T},\bar{\bm{s}}_{1:T},\bar{\bm{y}}_{1:T}\right)=P(s_{1} )\prod_{t=1}^{T}P(x_{t}\mid s_{t})\widetilde{\mathcal{T}}(s_{t},x_{t},s_{t+1} )\widetilde{\mathcal{R}}(s_{t},x_{t},y_{t})\] (4)

where \(\widetilde{\mathcal{T}}\) and \(\widetilde{\mathcal{R}}\) are the expert's nominal transition distribution and reward function computed from the _observational_ distribution as follows:

\[\widetilde{\mathcal{T}}\left(s,x,s^{\prime}\right)=P\left(S_{t+1}=s^{\prime} \mid S_{t}=s,X_{t}=x\right),\qquad\widetilde{\mathcal{R}}\left(s,x\right)= \mathbb{E}\left[Y_{t}\mid S_{t}=s,X_{t}=x\right]\] (5)

By convention in imitation learning, we assume the rewards \(Y_{t}\) are generally unobserved to the learner; instead, it has access to a parametric family \(\mathcal{R}\) containing the expert's nominal reward function \(\mathbb{E}\left[Y_{t}\mid s_{t},x_{t}\right]\). Given the expert demonstrations \(\mathcal{D}\) sampled from \(P(X_{1},X_{2},\dots,S_{1},S_{2},\dots)\) and the parametric reward family \(\mathcal{R}\), the imitator attempts to learn policy \(\pi\) that can achieve expert performance, i.e., \(\mathbb{E}_{\pi}\left[\sum_{t=1}^{\infty}\gamma^{t-1}Y_{t}\right]\geq\mathbb{E }\left[\sum_{t=1}^{\infty}\gamma^{t-1}Y_{t}\right]\). Standard imitation methods focus on the identifiable setting where the imitator's transition distribution \(\mathcal{T}\) and reward function \(\mathcal{R}\) is consistent with the expert's nominal transition \(\widetilde{\mathcal{T}}\) and reward \(\widetilde{\mathcal{R}}\). Formally,

**Definition 2** (Causal Consistency).: For an interventional distribution \(P_{\pi}\) and an observational distribution \(P\) satisfying the Markov property (Def. 1), Causal Consistency is said to hold with respect to \(P_{\pi}\) and \(P\) if the following statement is true, for every time step \(t=1,2,\dots\),

\[P_{x_{t}}\left(s_{t+1}\mid s_{t}\right)=P\left(s_{t+1}\mid s_{t},x_{t}\right), \quad\text{and}\quad P_{x_{t}}\left(y_{t}\mid s_{t}\right)=P\left(y_{t}\mid s _{t},x_{t}\right)\] (6)

When the invariances of Def. 2 hold, the learner could recover the parametrization of the transition distribution \(\mathcal{T}\) from observational data \(P(\bm{X},\bm{S})\) and infer about the reward function \(\mathcal{R}\) from the parametric family \(\mathcal{R}\). An imitating policy \(\pi\) is obtainable by solving the following minimax program,

\[\nu^{*}=\min_{\bm{\pi}}\max_{\mathcal{R}\in\mathcal{R}}\sum_{s,x}\mathcal{R}(s,x)\left(P(x\mid s)\rho(s)-\pi(x\mid s)\rho_{\pi}(s)\right)\] (7)

where the imitator's \(\rho_{\pi}\) and the expert's \(\rho\) occupancy measures are defined as, respectively, \(\rho_{\pi}(s)=\sum_{t=0}^{\infty}\gamma^{t}P_{\pi}\left(S_{t}=s\right)\) and \(\rho(s)=\sum_{t=0}^{\infty}\gamma^{t}P\left(S_{t}=s\right)\). The solution \(\pi\) is guaranteed to achieve expert

Figure 2: Causal diagrams where \(S_{t}\) represents the state, \(X_{t}\) represents the action (shaded blue) and \(Y_{t}\) represents the latent reward (shaded red). (a) MDP\({}_{\text{exp}}\) describes the imitator’s interaction with the environment; (b) MDP\({}_{\text{obs}}\) shows the data-generating process for the expert demonstrations.

performance when the gap \(\nu^{*}\leq 0\). This means that the imitator following policy \(\pi\) performs as well as the expert, even in the worst-case environment instance compatible with the demonstration data and model assumption. Several imitation learning algorithms have been proposed to solve the optimization problem in Eq. (7), including [1; 50; 19].

Graphical criteria exist [33; 46; 34] to examine whether causal consistency (Def. 2) holds from causal knowledge of the environment, including the celebrated _backdoor_ condition [32; Def. 3.3.1],[15]. In MDPs, this means that the causal links between the latent noise \(\bm{U}_{t}\) and action \(X_{t}\) are not effective - the graphical representation of the imitator's (Fig. 2a) and the expert's (Fig. 2b) data-generating process coincide. However, in practice, causal consistency could be fragile and does not necessarily hold due to the presence of unobserved confounders in the demonstration data [57; 39]. The remainder of this paper studies imitation learning when violations occur in the invariance relationships of Eq. (6).

### Imitation with Non-Identifiable Transition and Reward

We first consider the imitation setting described in Fig. 2b where unobserved confounders generally exist in the expert demonstrations; both the transition distribution \(\mathcal{T}\) and reward function \(\mathcal{R}\) are not identifiable from Eq. (6). Here, we will show that expert performance is not imitable by constructing worst-case MDP instances where the expert always outperforms the imitator.

The state value function \(V_{\pi}(s)\) is defined as the expected return given the imitator's starting state \(S_{t}=s\) following a policy \(\pi\), i.e., \(V_{\pi}(s)=\mathbb{E}_{\pi}\left[R_{t}\mid S_{t}=s\right]\). For any policy \(\pi\), the imitator's performance can be written as \(\mathbb{E}_{\pi}\left[R_{1}\right]=\sum_{s_{1}}P(s_{1})V_{\pi}(s_{1})\). The value function of any state \(s\) can thus be recursively defined using the celebrated _Bellman Equation_[6]:

\[V_{\pi}(s)=\sum_{x}\pi(x\mid s)\left(\mathcal{R}(s,x)+\gamma\sum_{s^{\prime} }\mathcal{T}(s,x,s^{\prime})V_{\pi}(s^{\prime})\right)\] (8)

where \(\gamma\) denotes the discount factor. While the transition distribution \(\mathcal{T}\) and the reward function \(\mathcal{R}\) are not uniquely discernible from the observational distribution due to the unobserved confounding, it is still possible to learn about them from demonstrations using partial identification. Without loss of generality, the reward \(Y_{t}\) is normalized in a real interval \([0,1]\). Through rigorous adaptation of the bounding strategies established in [29; 55], we successfully derive the bounds for the transition distribution \(\mathcal{T}\) and reward function \(\mathcal{R}\), for every realization \((s,x,s^{\prime})\in\mathcal{S}\times\mathcal{X}\times\mathcal{S}\),

\[\mathcal{T}\left(s,x,s^{\prime}\right)\in\left[\widetilde{\mathcal{ T}}\left(s,x,s^{\prime}\right)P(x\mid s),\;\widetilde{\mathcal{T}}\left(s,x,s^{ \prime}\right)P(x\mid s)+P\left(\neg x\mid s\right)\right]\] (9) \[\mathcal{R}\left(s,x\right)\in\left[\widetilde{\mathcal{R}}\left( s,x\right)P(x\mid s),\;\widetilde{\mathcal{R}}\left(s,x\right)P(x\mid s)+P(\neg x \mid s)\right]\] (10)

Among the above quantities, \(\widetilde{\mathcal{T}}\) and \(\widetilde{\mathcal{R}}\) are the expert's nominal transition distribution and reward function in Eq. (5); \(P(x\mid s)\) stands for the propensity score \(P\left(X_{t}=x\mid S_{t}=s\right)\) and \(P(\neg x\mid s)=1-P(x\mid s)\). We can then construct a worst-case MDP for any policy \(\pi\) at state \(s\) by solving the following optimization program: minimize the Bellman's equation in Eq. (8) as the objective function, subject to the observational constraints in Eqs. (9) and (10). Solving this program enables a valid MDP construction since the transition distribution \(\mathcal{T}\) and the reward function \(\mathcal{R}\) are independent components induced by the underlying model and can be optimized separately.

**Theorem 1**.: _Given any positive observational distribution \(P(\bm{X},\bm{S},\bm{Y})>0\), there exists an MDP model \(\hat{M}\) compatible with the causal graph of Fig. 2b such that \(P(\bm{X},\bm{S},\bm{Y};\hat{M})=P(\bm{X},\bm{S},\bm{Y})\) and for any policy \(\pi\), any time step \(t=1,2,\ldots\), any state \(s\in\mathcal{S}\),_

\[V_{\pi}\left(s;\hat{M}\right)<\mathbb{E}\left[R_{t}\mid S_{t}=s;\hat{M}\right].\] (11)

In other words, there always exists a candidate MDP instance \(\hat{M}\) compatible with the demonstration data such that an imitator is always unable to achieve expert performance (r.h.s. in Eq. (11)), regardless of the deployed policy \(\pi\). It follows from Thm. 1 that there is no policy \(\pi\) learnable from confounded demonstrations that is guaranteed to perform at least as the expert in all possible scenarios. This means that expert performance is not imitable when unobserved confounding generally exists. The following example demonstrates the challenges of unobserved confounding in a single-stage MDP.

**Example 1** (Single-Stage MDP).: Consider a 1-stage MDP model with horizon \(T=1\). For any policy \(\pi(X_{1}\mid S_{1})\), the imitator's expected return is \(\mathbb{E}_{\pi}\left[Y_{1}\right]=\sum_{s_{1},x_{1}}\mathcal{R}(s_{1},x_{1}) \pi(x_{1}\mid s_{1})P(s_{1})\). It followsfrom the tight lower bound in Eq. (10) that there exists an worst-case MDP model \(\hat{M}\) compatible with the observational distribution \(P(X_{1},S_{1},Y_{1})\) such that \(\mathcal{R}(s_{1},x_{1})=\mathbb{E}\left[Y_{1}\mid s_{1},x_{1}\right]P(x_{1} \mid s_{1})\). In this MDP instance \(\hat{M}\), the imitator's expected return can be further written as

\[\mathbb{E}_{\pi}\left[Y_{1}\right]=\sum_{s_{1},x_{1}}\mathbb{E}[Y_{1}\mid s_{1 },x_{1}]P(x_{1}|s_{1})\pi(x_{1}|s_{1})P(s_{1})<\sum_{s_{1},x_{1}}\mathbb{E}[Y_ {1}\mid s_{1},x_{1}]P(x_{1}|s_{1})P(s_{1})\] (12)

The last step holds since probabilities of the policy \(\pi(x_{1}\mid s_{1})\in[0,1]\) and \(\sum_{x_{1}}\pi(x_{1}\mid s_{1})=1\). Marginalizing the above equation gives \(\mathbb{E}_{\pi}\left[Y_{1}\right]<\mathbb{E}[Y_{1}]\) - the imitator is unable to achieve expert performance regardless of the deployed policy \(\pi\). This analysis applies analogously to the MAB model in Fig. 1, which can be thought of as a 1-stage MDP with no initial state \(S_{1}=\emptyset\). We refer the readers to Appendix F for more examples about the 2-stage MDP.

## 3 Partial Identification for Robust Imitation

The impossibility results in Thm. 1 imply that robust imitation cannot be guaranteed when unobserved confounders generally exist in the demonstration data. This means we must explore alternative assumptions to learn an imitating policy guaranteed to achieve expert performance. Meanwhile, standard imitation methods apply when causal consistency of Def. 2 holds, and no unobserved confounder affects the transition or reward function. A natural question at this point arises: whether robust imitation is feasible for settings between the unconfounded (Fig. 0(b)) and fully confounded cases (Fig. 0(a)), where unobserved confounding bias affects only either the transition distribution or reward function? This section aims to answer this question.

### Imitation with Identifiable Transition and Non-Identifiable Reward

We first examine the setting graphically described in Fig. 2(a) where the reward function is confounded, while the transition distribution is identifiable from the demonstration data. In this case, the first equation of Def. 2 holds while the second one fails. To initiate the discussion, we write the expected return of a candidate policy \(\pi\) in an MDP environment as follows [36],

\[\mathbb{E}_{\pi}\left[R_{1}\right]=\sum_{s,x}\mathcal{R}\left(s,x\right)\pi(x \mid s)\rho_{\pi}(s)\] (13)

Among quantities in the above equation, the state occupancy measure \(\rho_{\pi}(s)=\sum_{t=0}^{\infty}\gamma^{t}P_{\pi}\) (\(S_{t}=s\)) is a function of the initial state distribution \(P(s)=P(S_{1}=s)\) and the transition distribution \(\mathcal{T}\). Specifically, \(\rho_{\pi}(s)\) can be recursively written as \(\rho_{\pi}(s)=P\left(s\right)+\gamma\sum_{s^{\prime},x}\mathcal{T}\left(s^{ \prime},x,s\right)\pi(x\mid s^{\prime})\rho_{\pi}(s^{\prime})\). When the transition distribution is unconfounded (Fig. 2(a)), one could recover its parametrization \(\mathcal{T}(s,x,s^{\prime})\) following the first formula of Def. 1. Therefore, what remains undetermined in Eq. (13) is the non-identifiable reward function \(\mathcal{R}\). It follows from Eq. (10) that parametrization of \(\mathcal{R}(s,x)\) can be bounded from the observational distribution. The imitator's expected return could thus be lower bounded as \(\mathbb{E}_{\pi}\left[R_{1}\right]\geq\sum_{s,x}\widetilde{\mathcal{R}}\left( s,x\right)P(x\mid s)\pi(x\mid s)\rho_{\pi}(s)\), where \(\widetilde{\mathcal{R}}\) is the nominal reward function defined in Eq. (5). Similarly, the expert's expected return could be decomposed as \(\mathbb{E}\left[R_{1}\right]=\sum_{x,s}\widetilde{\mathcal{R}}(s,x)P(x\mid s )\rho(s)\), where \(\rho(s)=\sum_{t=0}^{\infty}\gamma^{t}P\left(S_{t}=s\right)\) is the expert's occupancy measure. Optimizing the worst-case gap between the imitator \(\mathbb{E}_{\pi}\left[R_{1}\right]\) and expert \(\mathbb{E}\left[R_{1}\right]\) leads to a minimax optimization problem, the solution of which leads to a possible imitating policy.

Figure 3: (a) MDP\({}_{\text{obs-Y}}\) shows a data-generating process for expert demonstrations where only the reward \(Y_{t}\) is confounded with the action \(X_{t}\); (b) MDP\({}_{\text{obs-S}}\) shows a data-generating process for expert demonstrations where only the next state \(S_{t+1}\) is confounded with the action \(X_{t}\).

**Theorem 2**.: _Given an MDP \(M\) compatible with the causal graph of Fig. 2(a), let \(\mathscr{R}\) be a parametric family containing the conditional reward \(\mathbb{E}[Y_{t}\mid s_{t},x_{t}]\). Consider the following optimization program,_

\[\nu^{*}=\min_{\pi}\max_{\mathscr{R}\in\mathscr{R}}\sum_{s,x}\widetilde{ \mathcal{R}}(s,x)P(x\mid s)\left(\rho(s)-\pi(x\mid s)\rho_{\bm{\pi}}(s)\right)\] (14)

_When the gap \(\nu^{*}\leq 0\), the solution \(\pi^{*}\) is an imitating policy satisfying \(\mathbb{E}_{\pi^{*}}\left[R_{1}\right]\geq\mathbb{E}[R_{1}]\)._

In other words, Thm. 2 computes an imitating policy within the environment depicted in Fig. 2(a) by finding a policy maximizing the worst-case reward function compatible with the demonstration data and the expert's nominal reward. Later in Sec. 4, we will demonstrate that such a solution exists and robust imitation learning is feasible in Fig. 2(a).

The optimization program in Thm. 2 could be solved by augmenting some standard imitation learning such as GAIL [19]. To make the argument more precise, let the parametric family \(\mathscr{R}\) be a set of reward function \(\mathcal{R}(s,x)\) taking values in the real space \(\mathbb{R}\). We penalize the complexity of a reward function \(\mathcal{R}\) by subtracting a convex regularization function \(\psi(\mathcal{R})\) from Eq. (14); the detailed definition of \(\psi(\mathcal{R})\) is given by [19, Eq. 13]. Solving the optimization program of Eq. (14) is equivalent to matching weighted occupancy measures between the imitator and the expert, shown in Appendix B,

\[\nu^{*} =\min_{\pi}\psi^{*}\left(P(x\mid s)\rho(s)-P(x\mid s)\pi(x\mid s )\rho_{\pi}(s)\right)\] (17) \[=\min_{\pi}\max_{D\in(0,1)^{g\times X}}\mathbb{E}[\log(D(S,X))]+ \mathbb{E}_{\pi}\left[P(x\mid s)\log(1-D(S,X))\right],\] (18)

where \(\psi^{*}=\max_{\mathcal{R}}a^{\top}\mathcal{R}-\psi(\mathcal{R})\) is a conjugate function of \(\psi\); function \(D\in\mathcal{S}\times\mathcal{X}\mapsto(0,1)\) is a discriminator classifier (e.g, a neural network). The above optimization problem is in the form of two neural networks competing against each other in a zero-sum game. The detailed implementation of our proposed algorithm, called CAIL-\(\mathcal{R}\), is provided in Alg. 1. Compared to the standard GAIL algorithm, Alg. 1 adds weight to the signal generated by the discriminator for the imitator and then attempts to match the distribution between the weighted samples and expert demonstrations.

### Imitation with Non-Identifiable Transition and Identifiable Reward

In this section, we examine the MDP\({}_{\text{obs-s}}\) environment as graphically depicted in Fig. 2(b), where the reward function is unconfounded, but UCs affect the action \(X_{t}\) and the next state \(S_{t+1}\) simultaneously. In this setting, the second equation of Causal Consistency (Def. 2) is satisfied, aligning the reward function \(\mathcal{R}\) with the expert's nominal reward function. However, the first equation of Def. 2 does not generally hold due to confounding bias, making the transition distribution \(\mathcal{T}\) not identifiable from demonstrations. Despite these challenges, we utilize partial identification techniques to bound the transition function \(\mathcal{T}\), and subsequently estimate the imitator's performance.

More precisely, consider again the expected return decomposition in Eq. (13). The identifiable reward function \(\mathcal{R}\) must be contained in the parametric space of the expert's nominal reward \(\mathcal{R}\). The transition distribution \(\mathcal{T}\) can be bounded from the demonstration data using Eq. (9). One could thus obtain a lower bound over the imitator's performance by reasoning about the worst-case occupancy measure compatible with demonstrations. Formally, with the fixed reward function \(\mathcal{R}\) and the fixed policy \(\pi\), the imitator's return \(\mathbb{E}_{\pi}\left[R_{1}\right]\) is bounded by:

\[\mathbb{E}_{\pi}\left[R_{1}\right]\geq\min_{\mathcal{T},\rho_{\pi}}\quad\sum _{s,x}\mathcal{R}(s,x)\pi(x\mid s)\rho_{\pi}(s)\] (19)

\[\text{s.t.:}\ \ \rho_{\pi}(s)\geq 0,\quad\sum_{s}\rho_{\pi}(s)=\frac{1}{1- \gamma},\quad\text{and}\ \rho_{\pi}\left(s\right)=P\left(s\right)+\gamma\sum_{s^{\prime},x}\mathcal{T} \left(s^{\prime},x,s\right)\pi(x\mid s^{\prime})\rho_{\pi}(s^{\prime})\]

\[\text{Obs. Constraints }\mathcal{T}:\quad\begin{cases}\sum_{s}\mathcal{T} \left(s^{\prime},x,s\right)=1,\quad\text{and}\ \mathcal{T}\left(s,x,s^{\prime}\right)\geq\widetilde{\mathcal{T}}\left(s,x,s^ {\prime}\right)P(x\mid s)\\ \mathcal{T}\left(s,x,s^{\prime}\right)\leq\widetilde{\mathcal{T}}\left(s,x,s^ {\prime}\right)P(x\mid s)+P\left(\neg x\mid s\right)\end{cases}\] (20)

The above optimization problem is similar to the classical linear program for planning in MDPs [36]. The main difference is that the transition distribution \(\mathcal{T}\) is no longer fixed but bounded in a convex space \(\mathcal{T}\) specified from the observational data. Therefore, we develop an imitating policy by minimizing the performance gap between the imitator and the expert in the worst-case environment compatible with the observational data and prior knowledge.

**Theorem 3**.: _Given an MDP \(M\) compatible with the causal graph of Fig. 2(b), let \(\mathcal{R}\) be a parametric family containing the conditional reward \(\mathbb{E}[Y_{t}\mid s_{t},x_{t}]\), and \(\mathcal{T}\) be a parametric family over conditional probabilities \(P\left(s_{t+1}\mid s_{t},x_{t}\right)\) defined in Eq. (20). Consider the following program,_

\[\nu^{*}=\min_{\pi}\ \max_{\mathcal{R}\in\mathcal{R}}\ \max_{\mathcal{T}\in \mathcal{T}}\ \sum_{s,x}\mathcal{R}(s,x)\left(P(x\mid s)\rho(s)-\pi(x\mid s)\rho_{\pi}\left(s ;\mathcal{T}\right)\right)\] (21)

_When the gap \(\nu^{*}\leq 0\), the solution \(\pi^{*}\) is an imitating policy satisfying \(\mathbb{E}_{\pi^{*}}\left[R_{1}\right]\geq\mathbb{E}[R_{1}]\)._

We solve the optimization program in Thm. 3 by augmenting GAIL, a standard imitation method [19]. By penalizing the complexity of a reward function \(\mathcal{R}\) using a convex regularization function \(\psi(\mathcal{R})\) from Eq. (14), Eq. (21) is reducible to the following distribution matching problem,

\[\nu^{*}=\min_{\pi}\ \max_{D\in(0,1)^{\mathcal{S}\times\mathcal{X}}}\ \max_{\mathcal{T}\in \mathcal{T}}\ \mathbb{E}[\log(D(S,X))]+\mathbb{E}_{\pi}\left[\log(1-D(S,X));\mathcal{T} \right],\] (22)

We present the step-by-step implementation of our imitation method, CAIL-\(\mathcal{T}\), in Alg. 2. It is similar to the standard GAIL [19]; however, a significant distinction arises at step 4, where the imitator collects trajectories from the worst-case occupancy measure as presented in Eq. (19) and Eq. (20), which is obtainable by iteratively solving a series of linear programs. We refer readers to Appendix C for a more detailed discussion, where we propose an iterative algorithm designed to find the worst-case occupancy measure efficiently.

## 4 Experiments

In this section, we validate the theoretical findings presented in Thm. 1 and illustrate the applications of the proposed CAIL algorithms (Alg. 1 and Alg. 2) on various causal imitation learning tasks. Such tasks range from synthetic causal models to real-world scenarios. To summarize, when both the transition and the reward are confounded, there always exists a worst-case MDP instance \(\dot{M}\) compatible with the expert demonstrations, but the imitator consistently fails to match expertperformance, aligning with the proof provided in Sec. 2.1. When either the transition or the reward is confounded, we systematically evaluate our algorithms against the standard BC and GAIL methods, highlighting the importance of optimizing within the worst-case SCM. Standard BC mimics the expert's nominal behavior policy \(P(X|S)\) via supervised learning; standard GAIL learns a policy by solving a min-max game [19]. We provide in Appendix D more details on the experiment setup.

MDPobs - Random Instances.This experiment aims to empirically validate the theoretical findings discussed in Thm. 1. Consider SCM instances compatible with Fig. 1(b) including binary observed variables \(S_{t},X_{t},Y_{t}\in\{0,1\}\). \(1000\) random discrete MDPs are sampled, in other words, the reward functions and the transition probabilities are generally different among these models. The expert is able to observe the state \(S_{t}\), the unobserved variable \(U_{t}\). However, the imitator, lacking access to both \(U_{t}\) or the reward \(\mathbb{E}_{\pi}[Y_{t}]\), makes decisions solely on \(S_{t}\). As shown in Fig. 3(a), imitators consistently failed to match expert performance. Specifically, prevalent negative performance gaps indicate that most of imitators were consistently worse than experts; only in rare cases did the performance gaps near \(-0.5\), supporting our theoretical insights in Thm. 1. In summary, imitators fail to achieve the expert's performance when both the reward and the transition are confounded.

MDPobs.Y - Driving.To demonstrate the proposed framework, as outlined in Alg. 1, we consider a scenario when an autonomous vehicle ('ego vehicle') aims to learn optimal driving strategies from expert demonstrations. The state \(S_{t}\) contains some critical driving information, e.g., the velocities of the ego vehicle and the leading vehicle and the spatial distance between them. The action \(X_{t}\) represents acceleration or deceleration decisions the ego vehicle makes. The unobserved variable \(U_{t}\) represents some information accessible to the expert but inaccessible to the imitator, e.g. slippery road conditions [26]. The reward \(Y_{t}\) is designed to reflect multiple realistic driving objectives, e.g., safety, comfort, efficiency, and so on. \(U_{t}\) has an effect on the reward \(Y_{t}\). Unlike the scenarios described in [39, 42], due to UCs between \(X_{t}\) and \(Y_{t}\) at each step \(t\), it is impossible to find a \(\pi\)-backdoor admissible set. BC, GAIL, and CAIL utilize the same policy space \(\pi(x\mid s)\). The major difference between CAIL and GAIL lies in that CAIL optimizes the imitator by the weighted reward generated from the discriminator - \(P(x\mid s)\log(1-D(s,x))\). As illustrated in Fig. 3(b), where means and standard deviations are computed over \(100\) trajectories, CAIL consistently outperforms BC and GAIL.

MDPobs.S - Medical Treatment.Consider the challenge of providing medical treatment to acutely ill patients, where the primary goal is to learn a policy so that the morality rate can be decreased. We utilize the real-world medical treatment dataset, i.e., Medical Information Mart for Intensive Care III (MIMIC-III) dataset [22]. MIMIC-III consists trajectories of clinical information (e.g., heart rate, oxygen saturation, and so on) recorded at various time intervals. However, due to privacy concerns, certain essential variables are masked or not properly recorded [45], e.g., socioeconomic status or the experience levels of caregivers [9, 56]. Specifically, the state \(S_{t}\) encapsulates the critical health information for the patients, e.g., prolonged elevated heart rate (peHR). The action \(X_{t}\) represents whether to treat the medicine or not. The reward \(Y_{t}\) is designed to represent the intent of the doctor as much as possible, e.g., avoiding the patient's mortality. The unobserved confounded

Figure 4: Simulation results for our experiments. Fig. 3(a) illustrates the performance gap histogram for the experiment MDPobs, where negative values indicate performance worse than expert performance. Fig. 3(b) shows the convergence plot for CAIL, GAIL, and BC performance. Fig. 3(c) shows the final performance, where y-axis represents the expected return.

\(U_{t}\) simultaneously affects the action \(X_{t}\) and the next state \(S_{t+1}\). Simulation results are illustrated in Fig. 4c, which shows that the proposed framework performs the best among all strategies. BC and IRL fail to obtain an imitating policy that could match expert performance.

## 5 Conclusion

This paper investigates imitation learning in Markov Decision Processes where the unobserved confounding bias cannot be ruled out _a priori_. We establish theoretically that when such unobserved confounders generally exist, it is infeasible to obtain a robust imitating policy that can perform at least as well as the expert across all possible environments compatible with the demonstration data and prior knowledge. Departing from this critical realization, our research diverges into two distinct problem settings - one where only the transition distribution is unconfounded, but the reward function is non-identifiable due to unobserved confounding; and the other where the reward function is unconfounded and the transition distribution is non-identifiable. We then propose novel imitation learning algorithms using partial identification techniques, which allow the imitator to obtain effective policies that can achieve expert performance for both problem settings. Through extensive experiments, we empirically validate the theoretical findings and systematically evaluate our algorithms on different scenarios, ranging from simulated causal models to real-world datasets.

## Acknowledgements

This research was supported in part by the NSF, ONR, AFOSR, DoE, Amazon, JP Morgan, and The Alfred P. Sloan Foundation.

## References

* [1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In _Proceedings of the twenty-first international conference on Machine learning_, page 1. ACM, 2004.
* [2] B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from demonstration. _Robotics and autonomous systems_, 57(5):469-483, 2009.
* [3] A. Balke and J. Pearl. Counterfactuals and policy analysis in structural models. In _Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence_, pages 11-18. Morgan Kaufmann Publishers Inc., 1995.
* [4] E. Bareinboim, J. D. Correa, D. Ibeling, and T. Icard. On pearl's hierarchy and the foundations of causal inference. In _Probabilistic and Causal Inference: The Works of Judea Pearl_, page 507-556. Association for Computing Machinery, New York, NY, USA, 1st edition, 2022.
* [5] E. Bareinboim and J. Pearl. Causal inference and the data-fusion problem. _Proceedings of the National Academy of Sciences_, 113:7345-7352, 2016.
* [6] R. Bellman. Dynamic programming. _Science_, 153(3731):34-37, 1966.
* [7] I. Bica, D. Jarrett, and M. van der Schaar. Invariant causal imitation learning for generalizable policies. _Advances in Neural Information Processing Systems_, 34:3952-3964, 2021.
* [8] A. Billard, S. Calinon, R. Dillmann, and S. Schaal. Survey: Robot programming by demonstration. _Handbook of robotics_, 59, 2008.
* [9] N. B. Carnegie, M. Harada, and J. L. Hill. Assessing sensitivity to unmeasured confounding using a simulated potential confounder. _Journal of Research on Educational Effectiveness_, 9(3):395-420, 2016.
* [10] K.-W. Chang, A. Krishnamurthy, A. Agarwal, H. Daume III, and J. Langford. Learning to search better than your teacher. In _International Conference on Machine Learning_, pages 2058-2066. PMLR, 2015.

* [11] A. Chen, J. Scheurer, J. A. Campos, T. Korbak, J. S. Chan, S. R. Bowman, K. Cho, and E. Perez. Learning from natural language feedback. _Transactions on Machine Learning Research_, 2024.
* [12] D. Chickering and J. Pearl. A clinician's apprentice for analyzing non-compliance. In _Proceedings of the Twelfth National Conference on Artificial Intelligence_, volume Volume II, pages 1269-1276. MIT Press, Menlo Park, CA, 1996.
* [13] S. Choi, J. Kim, and H. Yeo. Trajgail: Generating urban vehicle trajectories using generative adversarial imitation learning. _Transportation Research Part C: Emerging Technologies_, 128:103091, 2021.
* [14] C. Cinelli, D. Kumor, B. Chen, J. Pearl, and E. Bareinboim. Sensitivity analysis of linear structural causal models. In _International Conference on Machine Learning_, pages 1252-1261, 2019.
* [15] J. Correa and E. Bareinboim. A calculus for stochastic interventions: Causal effect identification and surrogate experiments. In _Proceedings of the 34th AAAI Conference on Artificial Intelligence_, New York, NY, 2020. AAAI Press.
* [16] P. de Haan, D. Jayaraman, and S. Levine. Causal confusion in imitation learning. In _Advances in Neural Information Processing Systems_, pages 11693-11704, 2019.
* [17] J. Etesami and P. Geiger. Causal transfer for imitation learning and decision making under sensor-shift. In _Proceedings of the 34th AAAI Conference on Artificial Intelligence_, New York, NY, 2020. AAAI Press.
* [18] B. Fang, S. Jia, D. Guo, M. Xu, S. Wen, and F. Sun. Survey of imitation learning for robotic manipulation. _International Journal of Intelligent Robotics and Applications_, 3:362-369, 2019.
* [19] J. Ho and S. Ermon. Generative adversarial imitation learning. _Advances in neural information processing systems_, 29, 2016.
* [20] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne. Imitation learning: A survey of learning methods. _ACM Computing Surveys (CSUR)_, 50(2):1-35, 2017.
* [21] G. W. Imbens and D. B. Rubin. Bayesian inference for causal effects in randomized experiments with noncompliance. _The annals of statistics_, pages 305-327, 1997.
* [22] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. Anthony Celi, and R. G. Mark. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9, 2016.
* [23] S. Joshi, J. Zhang, and E. Bareinboim. Towards safe policy learning under partial identifiability: A causal approach. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 13004-13012, 2024.
* [24] N. Kallus, A. M. Puli, and U. Shalit. Removing hidden confounding by experimental grounding. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems 31_, pages 10911-10920. Curran Associates, Inc., 2018.
* [25] N. Kallus and A. Zhou. Confounding-robust policy improvement. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, pages 9289-9299, 2018.
* [26] R. Krajewski, J. Bock, L. Kloeker, and L. Eckstein. The highd dataset: A drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems. In _2018 21st International Conference on Intelligent Transportation Systems (ITSC)_, pages 2118-2125, 2018.
* [27] D. Kumor, J. Zhang, and E. Bareinboim. Sequential causal imitation learning with unobserved confounders. _Advances in Neural Information Processing Systems_, 2021.
* 22, 1985.
* [29] C. Manski. Nonparametric bounds on treatment effects. _American Economic Review, Papers and Proceedings_, 80:319-323, 1990.
* [30] A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In _Icml_, volume 99, pages 278-287, 1999.
* [31] T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, J. Peters, et al. An algorithmic perspective on imitation learning. _Foundations and Trends in Robotics_, 7(1-2):1-179, 2018.
* [32] J. Pearl. _Causality: Models, Reasoning, and Inference_. Cambridge University Press, New York, 2000.
* [33] J. Pearl and J. Robins. Probabilistic evaluation of sequential plans from causal models with hidden variables. In P. Besnard and S. Hanks, editors, _Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI 1995)_, pages 444-453. Morgan Kaufmann, San Francisco, 1995.
* [34] E. Perkovic, J. Textor, M. Kalisch, and M. H. Maathuis. A complete generalized adjustment criterion. _arXiv preprint arXiv:1507.01524_, 2015.
* [35] D. J. Poirier. Revising beliefs in nonidentified models. _Econometric theory_, 14(4):483-509, 1998.
* [36] M. L. Puterman. _Markov Decision Processes: Discrete Stochastic Dynamic Programming_. John Wiley & Sons, Inc., 1994.
* [37] A. Richardson, M. G. Hudgens, P. B. Gilbert, and J. P. Fine. Nonparametric bounds and sensitivity analysis of treatment effects. _Statistical science: a review journal of the Institute of Mathematical Statistics_, 29(4):596, 2014.
* [38] J. P. Romano and A. M. Shaikh. Inference for identifiable parameters in partially identified econometric models. _Journal of Statistical Planning and Inference_, 138(9):2786-2807, 2008.
* [39] K. Ruan and X. Di. Learning human driving behaviors with sequential causal imitation learning. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(4):4583-4592, 2022.
* [40] K. Ruan, X. He, J. Wang, X. Zhou, H. Feng, and A. Kebarighotbi. S2e: Towards an end-to-end entity resolution solution from acoustic signal. In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 10441-10445. IEEE, 2024.
* [41] K. Ruan, X. Wang, and X. Di. From twitter to reasoner: Understand mobility travel modes and sentiment using large language models. In _2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC)_, 2024.
* [42] K. Ruan, J. Zhang, X. Di, and E. Bareinboim. Causal imitation learning via inverse reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023.
* [43] D. Rubin. Direct and indirect causal effects via potential outcomes. _Scandinavian Journal of Statistics_, 31:161-170, 2004.
* [44] S. I. H. Shah, A. Coronato, M. Naeem, and G. De Pietro. Learning and assessing optimal dynamic treatment regimes through cooperative imitation learning. _IEEE Access_, 10:78148-78158, 2022.
* [45] Z. Shahn, N. I. Shapiro, P. D. Tyler, D. Talmor, and L.-w. H. Lehman. Fluid-limiting treatment strategies among sepsis patients in the icu: a retrospective causal analysis. _Critical Care_, 24:1-9, 2020.
* [46] I. Shpitser, T. VanderWeele, and J. Robins. On the validity of covariate adjustment for estimating causal effects. In _Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence_, pages 527-536. AUAI, Corvallis, OR, 2010.
* [47] P. Spirtes, C. N. Glymour, and R. Scheines. _Causation, prediction, and search_. MIT press, 2000.

* [48] J. Stoye. More on confidence intervals for partially identified parameters. _Econometrica_, 77(4):1299-1315, 2009.
* [49] G. Swamy, S. Choudhury, D. Bagnell, and S. Wu. Causal imitation learning under temporally correlated noise. In _International Conference on Machine Learning_, pages 20877-20890. PMLR, 2022.
* [50] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In _Advances in neural information processing systems_, pages 1449-1456, 2008.
* [51] J. Tian. _Studies in Causal Reasoning and Learning_. PhD thesis, Computer Science Department, University of California, Los Angeles, CA, November 2002.
* [52] L. Wang, W. Yu, X. He, W. Cheng, M. R. Ren, W. Wang, B. Zong, H. Chen, and H. Zha. Adversarial cooperative imitation learning for dynamic treatment regimes. In _Proceedings of The Web Conference 2020_, pages 1785-1795, 2020.
* [53] T.-Z. Wang, T. Qin, and Z.-H. Zhou. Estimating possible causal effects with latent variables via adjustment. In _International Conference on Machine Learning_, pages 36308-36335. PMLR, 2023.
* [54] J. Zhang and E. Bareinboim. Markov decision processes with unobserved confounders: A causal approach. Technical Report R-23, Colummbia Causal AI Lab, 2016, https://causalai.net/mdp-causal.pdf.
* [55] J. Zhang and E. Bareinboim. Near-optimal reinforcement learning in dynamic treatment regimes. In _Advances in Neural Information Processing Systems_, pages 13401-13411, 2019.
* [56] J. Zhang and E. Bareinboim. Can humans be out of the loop? Technical Report R-64, Causal Artificial Intelligence Lab, Columbia University, 2020. Also, to appear: Proc. of the 1st Conference on Causal Learning and Reasoning (CLeaR), 2022.
* [57] J. Zhang, D. Kumor, and E. Bareinboim. Causal imitation learning with unobserved confounders. _Advances in Neural Information Processing Systems_, 33:12263-12274, 2020.
* [58] J. Zhang, J. Tian, and E. Bareinboim. Partial counterfactual identification from observational and experimental data. In _International Conference on Machine Learning_, pages 26548-26558. PMLR, 2022.

Proofs

In this section, we provide proofs for the theoretical claims delineated in the paper. Throughout this paper, it is important to note that detailed parametrizations of the underlying SCM are not known to the agent. Instead, the agent has access to the expert's demonstrations, which are summarized as the observational distribution \(P(\bm{X},\bm{S},\bm{Y})\).

We begin by revisiting the distribution of state visitation. Specifically, \(\rho_{\pi}(s)\) can be calculated by:

\[\rho_{\pi}\left(s\right)=P\left(s\right)+\gamma\sum_{s^{\prime},x}\mathcal{T} \left(s^{\prime},x,s\right)\pi(x\mid s^{\prime})\rho_{\pi}(s^{\prime})\] (23)

where \(P\left(s\right)\) represents the initial state distribution, \(\gamma\) represents the discount factor, \(\mathcal{T}\) represents the transition probabilities for the imitator. Subsequently, we are able to develop the occupancy measure for the policy \(\pi\):

\[\rho_{\pi}(s,x)=\rho_{\pi}(s)\pi(x\mid s)\] (24)

It is important to note that, although the format of the occupancy measure \(\rho_{\pi}(s,x)\) shares a formal resemblance to the one presented in GAIL [19], \(\rho_{\pi}(s,x)\) specifically represents an interventional distribution with policy \(\text{do}(\pi)\). The identifiability of the transition \(\mathcal{T}\left(s,x,s^{\prime}\right)\) directly impacts the identifiability of \(P_{\pi}\left(s_{t}\right)\). If \(P_{\pi}\left(s_{t}\right)\) is not identifiable, \(\rho_{\pi}(s)\) and \(\rho_{\pi}(s,x)\) are consequently not identifiable.

**Theorem 1**.: _Given any positive observational distribution \(P(\bm{X},\bm{S},\bm{Y})>0\), there exists an MDP model \(\hat{M}\) compatible with the causal graph of Fig. 2b such that \(P(\bm{X},\bm{S},\bm{Y};\hat{M})=P(\bm{X},\bm{S},\bm{Y})\) and for any policy \(\pi\), any time step \(t=1,2,\dots\), any state \(s\in\mathcal{S}\),_

\[V_{\pi}\left(s;\hat{M}\right)<\mathbb{E}\left[R_{t}\mid S_{t}=s;\hat{M}\right].\] (11)

Proof.: Without loss of generality, the reward \(Y\) is normalized so that it has a range of \([0,1]\) Based on the value function defined in Eq. (8), we first show how to expand it into a recursive version:

\[V_{\pi}(s_{t}) =\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^{k}Y_{t+k}\mid s_ {t}\right]\] (25) \[=\mathbb{E}_{\pi}[Y_{t}\mid s_{t}]+\mathbb{E}_{\pi}\left[\sum_{k=1 }^{\infty}\gamma^{k}Y_{t+k}\mid s_{t}\right]\] (26) \[=\mathbb{E}_{\pi}[Y_{t}\mid s_{t}]+\gamma\mathbb{E}_{\pi}\left[ \sum_{k=1}^{\infty}\gamma^{k-1}Y_{t+k}\mid s_{t}\right]\] (27) \[=\mathbb{E}_{\pi}[Y_{t}\mid s_{t}]+\gamma\sum_{s_{t+1}}P_{\pi}(s_ {t+1}\mid s_{t})\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^{k}Y_{t+1+k} \mid s_{t},s_{t+1}\right]\] (28) \[=\mathbb{E}_{\pi}[Y_{t}\mid s_{t}]+\gamma\sum_{s_{t+1}}P_{\pi}(s_ {t+1}\mid s_{t})V_{\pi}(s_{t+1}),\] (29)

where \(\gamma\) is the discount factor, \(P_{\pi}(s_{t+1}\mid s_{t})\) denotes the transition probability when executing policy \(\pi\).

From the second last line to the last line is justified by the experimental markovian property, as discussed in Sec. 2, following the graph Fig. 2a. More details could be found in [54]. \(\mathbb{E}_{\pi}[Y_{t}\mid s_{t}]=\mathbb{E}[Y_{t}\mid s_{t},\text{do}(\pi)]\) denotes the expected reward received by the agent when executing policy \(\pi\). Similarly, the transition probability

\[P_{\pi}(s_{t+1}\mid s_{t})=\sum_{x_{t}}P_{x_{t}}(s_{t+1}\mid s_{t})\pi(x_{t} \mid s_{t}),\] (30)

and \(P_{x_{t}}(s_{t+1}\mid s_{t})=P(s_{t+1}\mid s_{t},\text{do}(x_{t}))=\mathcal{T} (s_{t},x_{t},s_{t+1})\). Generally speaking, when any unobserved confounder exists between \(S_{t+1}\) and \(X_{t}\), the causal query \(P_{x_{t}}(s_{t+1}\mid s_{t})\) is not identifiable [32, 43, 5, 51]. Building on the previous derivations, we arrive at the recursive formulation of the value function under policy \(\pi\):

\[V_{\pi}(s_{t}) =\sum_{x_{t}}\mathcal{R}\left(s_{t},x_{t}\right)\pi(x_{t}\mid s_{t })+\gamma\sum_{s_{t+1}}\mathcal{T}(s_{t},x_{t},s_{t+1})\pi(x_{t}\mid s_{t})V_{ \pi}(s_{t+1})\] (31) \[=\sum_{x_{t}}\pi(x_{t}\mid s_{t})\left(\mathcal{R}\left(s_{t},x_{t }\right)+\gamma\sum_{s_{t+1}}\mathcal{T}(s_{t},x_{t},s_{t+1})V_{\pi}(s_{t+1}) \right).\] (32)

Next, to establish the validity of the preceding claim, we proceed by applying the technique of mathematical induction. Let \(|S|\) denote the number of distinct states for \(S\).

Base case \(t=T\).For the final timestep \(T\), for each state index \(j\) where \(\forall j\), \(1\leq j\leq|S|\), the value function \(V_{\pi}(s_{(T,j)})\) can be defined as follows:

\[\begin{split} V_{\pi}(s_{(T,j)})&=\mathbb{E}_{\pi} \left[Y_{T}\mid S_{T}=s_{(T,j)}\right]\\ &=\sum_{x_{t}}\mathbb{E}_{x_{t}}\left[Y_{T}\mid S_{T}=s_{(T,j)} \right)\right]\pi(x_{t}\mid s_{(T,j)})\end{split}\] (33)

where \(s_{(T,j)}\) refers to the scenario where the state at the final timestep \(S_{T}\) is equal the specific state \(j\).

In order to obtain the worst-case SCM \(\hat{M}\), we need to minimize \(V_{\pi}(s_{T})-V(s_{T})\) compatible with the observational distribution, by establishing its lower bound. To this end, we directly employ the natural bound [29], which has been discussed in Sec. 2.1:

\[\begin{split}\min_{M}& V_{\pi}(s_{(T,j)};M)-V(s_{(T,j)};M)\\ &=\sum_{x_{t}}\mathbb{E}_{x_{t}}\left[Y_{T}\mid S_{T}=s_{(T,j)}; M\right]\pi(x_{t}\mid s_{(T,j)})-V(s_{(T,j)};M)\\ &=\sum_{x_{t}}\mathbb{E}\left[Y_{T}\mid s_{(T,j)},x_{t}\right]P (x_{t}\mid s_{(T,j)})\pi(X_{T}=x_{t}\mid s_{(T,j)})-\sum_{x_{t}}\mathbb{E} \left[Y_{T}\mid s_{(T,j)},x_{t}\right]P(x_{t}\mid s_{(T,j)})\\ &<0\end{split}\] (34)

The last step is justified because \(P(\bm{X},\bm{S},\bm{Y})>0\) and \(0\leq\pi(X_{T}=x_{t}\mid s_{(T,j)})\leq 1\). Intuitive examples illustrating this conclusion are provided in Sec. 2.1 and Appendix F. Therefore, this confirms the validity of the inequality for the base case.

Specifically, in certain degenerate cases where there is only one possible action, the imitator has no choice but to follow that single option. Consequently, unobserved confounders are less likely to introduce significant effects in these scenarios. However, under such conditions, pursuing imitation learning is not meaningful, as there is no variability in choice for the imitator to learn from. Therefore, such cases are of limited relevance to the scope of this analysis.

Induction case.Suppose at \(t+1\), \(V_{\pi}(s_{t+1})<V(s_{t+1})\), we need to prove \(V_{\pi}(s_{t})<V(s_{t})\).

\[V_{\pi}(s_{t})=\mathbb{E}_{\pi}[Y_{t}\mid s_{t}]+\gamma\sum_{j}P_{\pi}(s_{(t+1,j)}\mid s_{t})\underbrace{V_{\pi}(s_{(t+1,j)})}_{<V(s_{(t+1,j)})}\] (35)

Without loss of generality, we assume that the state with the minimal value at \(t+1\) is denoted as \(s_{(t+1,|S|)}\). Our approach is founded on the premise that in obtaining the worst-case SCM \(\hat{M}\), it is strategic to allocate the lowest possible transition probabilities to the state with the highest value, while preferentially assigning higher probabilities to states demonstrating smaller values. Specifically, one starts with the estimate \(P(S_{t+1}=s_{(t+1,1)},x_{t}\mid S_{t})\) for \(P_{x_{t}}(S_{t+1}=s_{(t+1,1)}\mid S_{t})\). Following this logic, we systematically allocate probability masses for indices \(1\leq j\leq|S|-1\) as follows:

\[P_{x_{t}}(S_{t+1}=s_{(t+1,j)}\mid S_{t})\gets P(S_{t+1}=s_{(t+1,j)},x_{t} \mid S_{t})\]In accordance with the established properties of probability distributions, it follows that:

\[\sum_{j=1}^{|S|}P_{x_{t}}(S_{t+1}=s_{(t+1,j)}\mid S_{t})=1.\]

Considering the state \(s_{(t+1,|S|)}\), the corresponding probability can be assigned as:

\[P_{x_{t}}(S_{t+1}=s_{(t+1,|S|)}\mid S_{t})=1-\sum_{j=1}^{|S|-1}P_{x_{t}}(S_{t+1 }=s_{(t+1,j)}\mid S_{t}).\]

By substituting the assigned values, we are able to derive the following expression:

\(P_{x_{t}}(S_{t+1}=s_{(t+1,|S|)}\mid S_{t})\gets 1-P(s_{(t+1,1)},x_{t}\mid S _{t})-P(s_{(t+1,2)},x_{t}\mid S_{t})\cdots-P(s_{(t+1,|S|-1)},x_{t}\mid S_{t}),\)

where the right-hand side simplifies to:

\[\begin{split}\left(P(s_{(t+1,1)},x_{t}\mid S_{t})+P(s_{(t+1,2)}, x_{t}\mid S_{t})\cdots+P(s_{(t+1,|S|-1)},x_{t}\mid S_{t})\right)&= \left(\sum_{j=1}^{|S|-1}P\left(s_{(t+1,j)},x_{t}\mid S_{t}\right)\right)\\ &=P(x_{t}\mid S_{t})-P(s_{(t+1,|S|)},x_{t}\mid S_{t}).\end{split}\]

It is established that the expression \(0\leq 1-P(x_{t}\mid S_{t})+P(s_{(t+1,|S|)},x_{t}\mid S_{t})\leq 1\) holds true. This inequality is supported by the following equation:

\[\sum_{j=1}^{|S|}P(s_{(t+1,j)},x_{t}\mid S_{t})=P(x_{t}\mid S_{t}).\]

To further analyze the expert policy, the associated value function \(V(s_{t})\) can be expanded as follows:

\[\begin{split} V(s_{t})&=\mathbb{E}\left[\sum_{k=0} ^{\infty}\gamma^{k}Y_{t+k}\mid S_{t}=s_{t}\right]\\ &=\mathbb{E}[Y_{t}\mid s_{t}]+\gamma\sum_{j}P(s_{(t+1,j)}\mid s_ {t})V(s_{(t+1,j)}),\end{split}\] (36)

where \(\gamma\) is the discount factor, \(P(s_{(t+1,j)}\mid s_{t})\) denotes the observational transition probability. Notably, \(P(s_{(t+1,j)}\mid s_{t})\) and \(P_{\pi}(s_{(t+1,j)}\mid s_{t})\) are generally different, because they reflect two distinct probabilities: \(P(s_{(t+1,j)}\mid s_{t})\) corresponding to the observational distribution and the other, \(P_{\pi}(s_{(t+1,j)}\mid s_{t})\), representing the imitator's transition dynamics.

In accordance with the established properties of probability distributions, it follows that:

\[\sum_{j=1}^{|S|}P(S_{t+1}=s_{(t+1,j)}\mid S_{t})=1.\]

Without loss of generality, suppose the policy is a deterministic policy. Actually, the following proof holds true regardless of the choice of \(x_{t}\). Subsequently, we analyze the gap between \(V_{\pi}(s_{t})\) and \(V(s_{t})\) as follows:

\[\begin{split}& V_{\pi}(s_{t})-V(s_{t})\\ &=\left(\mathbb{E}_{\pi}[Y_{t}\mid s_{t}]+\gamma\sum_{j=1}^{|S|}P _{\pi}(s_{(t+1,j)}\mid s_{t})V_{\pi}(s_{(t+1,j)})\right)\\ &-\left(\mathbb{E}[Y_{t}\mid s_{t}]+\gamma\sum_{j=1}^{|S|}P(s_{(t +1,j)}\mid s_{t})V(s_{(t+1,j)})\right)\\ &=\left(\mathbb{E}_{\pi}[Y_{t}\mid s_{t}]+\gamma\sum_{j=1}^{|S|} \sum_{x_{t}}P_{x_{t}}(s_{(t+1,j)}\mid s_{t})\pi(x_{t}\mid s_{t})V_{\pi}(s_{(t +1,j)})\right)\\ &-\left(\mathbb{E}[Y_{t}\mid s_{t}]+\gamma\sum_{j=1}^{|S|}P(s_{(t +1,j)}\mid s_{t})V(s_{(t+1,j)})\right)\end{split}\] (37)\[\min_{M} V_{\pi}(s_{t};M)-V(s_{t};M)\] (38) \[=\mathbb{E}_{\pi}[Y_{t}\mid s_{t};M]-\mathbb{E}[Y_{t}\mid s_{t};M]+ \gamma\sum_{j=1}^{|S|-1}\left(P(s_{(t+1,j)},x_{t}\mid s_{t})-P(s_{(t+1,j)}\mid s _{t})\right)V(s_{(t+1,j)})\] \[+\gamma\left(1-\left(\sum_{j=1}^{|S|-1}P\left(s_{(t+1,j)},x_{t} \mid s_{t}\right)\right)-P(s_{(t+1,|S|)}\mid s_{t})\right)V(s_{(t+1,|S|)})\] \[=\underbrace{\mathbb{E}_{\pi}[Y_{t}\mid s_{t};M]-\mathbb{E}[Y_{t }\mid s_{t};M]}_{<0}\] \[+\gamma\sum_{j=1}^{|S|-1}\frac{\left(P(s_{(t+1,j)},x_{t}\mid s_{t })-P(s_{(t+1,j)}\mid s_{t})\right)}{<0}\underbrace{\left(V(s_{(t+1,j)})-V(s_{ (t+1,|S|)})\right)}_{>0}\] \[<0\]

where \(\min_{M}\mathbb{E}_{\pi}[Y_{t}\mid s_{t};M]-\mathbb{E}[Y_{t}\mid s_{t};M]<0\) follows a similar logic as previously introduced in the base case, and \(V(s_{(t+1,j)})-V(s_{(t+1,|S|)})>0\) is consistent with the ordering assumption, where the state \(s_{(t+1,|S|)}\) represents the minimal value.

In some degenerated cases when \(\mathbb{E}_{\pi}[Y_{t}\mid s_{t}]=0\) and \(\mathbb{E}[Y_{t}\mid s_{t}]=0\), it might coincidentally follow that \(V_{\pi}(s_{t})=0\), which is equal to \(V(s_{t})=0\). Another instance of degeneracy occurs when the value function \(V(s_{(t+1,j)})\) remains the same across all states. Such occurrences are extremely unlikely in practical scenarios, especially when \(P(\bm{X},\bm{S},\bm{Y})>0\). 

## Appendix B Derivations for Causal GAIL Algorithms

**Theorem 2**.: _Given an MDP \(M\) compatible with the causal graph of Fig. 2(a), let \(\mathscr{R}\) be a parametric family containing the conditional reward \(\mathbb{E}[Y_{t}\mid s_{t},x_{t}]\). Consider the following optimization program,_

\[\nu^{*}=\min_{\pi}\max_{\widetilde{\mathcal{R}}\in\mathscr{R}}\sum_{s,x} \widetilde{\mathcal{R}}(s,x)P(x\mid s)\left(\rho(s)-\pi(x\mid s)\rho_{\bm{\pi} }(s)\right)\] (14)

_When the gap \(\nu^{*}\leq 0\), the solution \(\pi^{*}\) is an imitating policy satisfying \(\mathbb{E}_{\pi^{*}}\left[R_{1}\right]\geq\mathbb{E}[R_{1}]\)._

Proof.: Based on Eq. (13), we have:

\[\mathbb{E}_{\pi}\left[R_{1}\right]=\sum_{s,x}\mathcal{R}\left(s,x\right)\pi(x \mid s)\rho_{\pi}(s)\]

It follows from Eq. (10) that parametrization of \(\mathcal{R}(s,x)\) can be bound from the observational distribution. The imitator's expected return could thus be lower bounded as

\[\mathbb{E}_{\pi}\left[R_{1}\right]\geq\sum_{s,x}\widetilde{\mathcal{R}}\left( s,x\right)P(x\mid s)\pi(x\mid s)\rho_{\pi}(s)\]

Note that the expert's expected return could be similarly decomposed as

\[\mathbb{E}\left[R_{1}\right]=\sum_{x,s}\widetilde{\mathcal{R}}(s,x)P(x\mid s) \rho(s),\]

where \(\rho(s)=\sum_{t=0}^{\infty}\gamma^{t}P\left(S_{t}=s\right)\) is the expert's occupancy measure.

\[\nu^{*} =\min_{\pi}\max_{M} \mathbb{E}\left[R_{1};M\right]-\mathbb{E}_{\pi}\left[R_{1};M\right]\] (39) \[=\min_{\pi}\max_{\widetilde{\mathcal{R}},\mathcal{R}} \sum_{x,s}\widetilde{\mathcal{R}}(s,x)P(x\mid s)\rho(s)-\sum_{s,x} \mathcal{R}\left(s,x\right)\pi(x\mid s)\rho_{\pi}(s)\] (40) \[=\min_{\pi}\max_{\widetilde{\mathcal{R}}} \sum_{s,x}\widetilde{\mathcal{R}}(s,x)P(x\mid s)\left(\rho(s)-\pi( x\mid s)\rho_{\bm{\pi}}(s)\right),\] (41)

which is the ultimate target expression.

Next, we will show the derivation details for matching weighted occupancy measures between the imitator and the expert. Suppose \(\psi^{*}=\max_{\mathcal{R}}a^{\top}\mathcal{R}-\psi(\mathcal{R})\) is a conjugate function of \(\psi\). Following a similar logic in [19], we utilize a smiliar cost regularizer \(\psi_{GA}\), leading to the formulation of Alg. 1. Basically, Alg. 1 minimizes Jensen-Shannon divergence between \(P(x\mid s)\rho(s)\) and \(P(x\mid s)\pi(x\mid s)\rho_{\pi}(s)\).

First, we reformulate the equation into state-action occupancy measures:

\[\psi^{*}\left(P(x\mid s)\rho(s)-P(x\mid s)\pi(x\mid s)\rho_{\pi}(s)\right)=\psi ^{*}\left(\rho(s,x)-P(x\mid s)\rho_{\pi}(s,x)\right)\] (42)

Based on the definition of \(\psi^{*}\), we have:

\[\psi^{*}\left(\rho(s,x)-P(x\mid s)\rho_{\pi}(s,x)\right)\] (43) \[=\max_{\mathcal{R}}\sum_{s,x}\left(\rho(s,x)-P(x\mid s)\rho_{\pi }(s,x)\right)\mathcal{R}(s,x)-\sum_{s,x}P(x\mid s)\rho_{\pi}(s,x)g_{\phi}( \mathcal{R}(s,x))\] (44) \[=\sum_{s,x}\max_{\mathcal{R}}\rho(s,x)\mathcal{R}-P(x\mid s)\rho _{\pi}(s,x)\phi\left(-\phi^{-1}(-\mathcal{R})\right)\] (45) \[=\sum_{s,x}\max_{\mathcal{R}^{\prime}}\rho(s,x)(-\phi(\mathcal{R} ^{\prime}))-P(x\mid s)\rho_{\pi}(s,x)\phi\left(-\phi^{-1}(\phi(\mathcal{R}^{ \prime}))\right)\] (46) \[=\sum_{s,x}\max_{\mathcal{R}^{\prime}}\rho(s,x)(-\phi(\mathcal{R} ^{\prime}))-P(x\mid s)\rho_{\pi}(s,x)\phi\left(-\mathcal{R}^{\prime}\right)\] (47)

where we make the change of variables \(\mathcal{R}\rightarrow-\phi(\mathcal{R}^{\prime})\). Suppose \(D\in\mathcal{S}\times\mathcal{X}\mapsto(0,1)\) is a discriminator classifier (e.g, a neural network). Using the logistic loss \(\phi(x)=\log\left(1+e^{-x}\right)\), we can get:

\[\psi^{*}\left(\rho(s,x)-P(x\mid s)\rho_{\pi}(s,x)\right)\] (48) \[=\sum_{s,x}\max_{\mathcal{R}^{\prime}}\rho(s,x)\log\left(\frac{1} {1+e^{-\mathcal{R}^{\prime}}}\right)+P(x\mid s)\rho_{\pi}(s,x)\log\left(1- \frac{1}{1+e^{-\mathcal{R}^{\prime}}}\right)\] (49) \[=\max_{D\in(0,1)^{s\times x}}\mathbb{E}[\log(D(S,X))]+\mathbb{E} _{\pi}\left[P(x\mid s)\log(1-D(S,X))\right],\] (50)

which is the ultimate target expression.

**Theorem 3**.: _Given an MDP \(M\) compatible with the causal graph of Fig. 2(b), let \(\mathcal{R}\) be a parametric family containing the conditional reward \(\mathbb{E}[Y_{t}\mid s_{t},x_{t}]\), and \(\mathcal{T}\) be a parametric family over conditional probabilities \(P\left(s_{t+1}\mid s_{t},x_{t}\right)\) defined in Eq. (20). Consider the following program,_

\[\nu^{*}=\min_{\pi}\,\max_{\mathcal{R}\in\mathcal{R}}\,\max_{\mathcal{T}\,\otimes }\,\sum_{s,x}\mathcal{R}(s,x)\left(P(x\mid s)\rho(s)-\pi(x\mid s)\rho_{\pi} \left(s;\mathcal{T}\right)\right)\] (21)

_When the gap \(\nu^{*}\leq 0\), the solution \(\pi^{*}\) is an imitating policy satisfying \(\mathbb{E}_{\pi^{*}}\left[R_{1}\right]\geq\mathbb{E}[R_{1}]\)._

Proof.: Based on Eq. (13), we have:

\[\mathbb{E}_{\pi}\left[R_{1}\right] =\sum_{s,x}\mathcal{R}\left(s,x\right)\pi(x\mid s)\rho_{\pi}(s)\] \[=\sum_{s,x}\mathcal{R}\left(s,x\right)\underbrace{\rho_{\pi}(s,x )}_{\text{Non-ID}}\]

The reward function \(\mathcal{R}\) is identifiable and must be contained in the parametric space of the expert's nominal reward \(\mathcal{R}\). In other words,

\[\mathcal{R}\left(s,x\right)=\widetilde{\mathcal{R}}\left(s,x\right).\] (51)The transition distribution \(\mathcal{T}\) can be bounded from the demonstration data using Eq. (9). Therefore, we get:

\[\nu^{*} =\min_{\pi}\max_{M}\quad\mathbb{E}\left[R_{1};M\right]-\mathbb{E}_{ \pi}\left[R_{1};M\right]\] (52) \[=\min_{\pi}\max_{\mathcal{T},\mathcal{R},\mathcal{R}}\quad\sum_{s,x}\widetilde{\mathcal{R}}(s,x)P(x\mid s)\rho(s)-\mathcal{R}\left(s,x\right) \rho_{\pi}(s,x;\mathcal{T})\] (53) \[=\min_{\pi}\max_{\mathcal{T},\mathcal{R}}\quad\sum_{s,x}\mathcal{ R}\left(s,x\right)\left(P(x\mid s)\rho(s)-\rho_{\pi}(s,x;\mathcal{T})\right)\] (54) \[=\min_{\pi}\max_{\mathcal{T}\in\mathcal{T},\mathcal{R}\in \mathcal{R}}\quad\sum_{s,x}\mathcal{R}(s,x)\left(P(x\mid s)\rho(s)-\pi(x\mid s )\rho_{\pi}\left(s;\mathcal{T}\right)\right)\] (55)

which is the ultimate desired expression. 

Consider again the expected return decomposition in Eq. (13). The reward function \(\mathcal{R}\) is identifiable and must be contained in the parametric space of the expert's nominal reward \(\mathcal{R}\). The transition distribution \(\mathcal{T}\) can be bounded from the demonstration data using Eq. (9). One could thus obtain a lower bound over the imitator's performance by reasoning about the worst-case occupancy measure compatible with demonstrations. Formally, with the fixed reward function \(\mathcal{R}\) and the fixed policy \(\pi\), the imitator's return is bounded by

\[\mathbb{E}_{\pi}\left[R_{1}\right] \geq\min_{\mathcal{T},\rho_{\pi}}\quad\sum_{s,x}\mathcal{R}(s,x) \pi(x\mid s)\rho_{\pi}(s)\] (56) subject to: \[\rho_{\pi}(s)\geq 0,\quad\text{and}\,\sum_{s}\rho_{\pi}(s)=\frac{1}{1 -\gamma}\] \[\rho_{\pi}\left(s\right)=P\left(s\right)+\gamma\sum_{s^{\prime}, x}\mathcal{T}\left(s^{\prime},x,s\right)\pi(x\mid s^{\prime})\rho_{\pi}(s^{ \prime})\] \[\text{Obs. Constraints }\mathcal{T}: \begin{cases}\sum_{s}\mathcal{T}\left(s^{\prime},x,s\right)=1, \quad\text{and}\,\mathcal{T}\left(s,x,s^{\prime}\right)\geq\widetilde{ \mathcal{T}}\left(s,x,s^{\prime}\right)P(x\mid s)\\ \mathcal{T}\left(s,x,s^{\prime}\right)\leq\widetilde{\mathcal{T}}\left(s,x,s^ {\prime}\right)P(x\mid s)+P\left(\neg x\mid s\right)\end{cases}\] (57)

The above optimization problem is similar to the classic linear program for planning in MDPs [36]. The main difference is that the transition distribution \(\mathcal{T}\) is no longer fixed but bounded in a convex space \(\mathcal{T}\) specified from the observational data. Similar to the previous setting, we could solve an imitating policy by minimizing the performance gap between the imitator and the expert in the worst-case environment compatible with the observational data and prior knowledge.

Next, we will provide a heuristic algorithm to solve the optimization program presented in Eq. (19) and Eq. (20). Specifically, as discussed in Eq. (9), we are able to bound the transition distribution \(\mathcal{T}\) by:

\[\mathcal{T}\left(s,x,s^{\prime}\right)\in\left[\widetilde{\mathcal{T}}\left(s,x,s^{\prime}\right)P(x\mid s),\widetilde{\mathcal{T}}\left(s,x,s^{\prime} \right)P(x\mid s)+P\left(\neg x\mid s\right)\right].\] (58)

The intuition for Alg. 3 is: in order to find the worst case, we need to put as less transition probability mass as possible to the state with maximal values, and allocate higher transition probabilities to states with smaller values. Without loss of generality, suppose \(V_{x_{t}}(s_{(t+1,|S|)})\) is found to have the smallest relative value. For all other states \(j\neq|S|\), we need to allocate as less transition probability mass as possible. Therefore, we take the lower bound:

\[P_{x_{t}}(S_{t+1}=s_{(t+1,j)}\mid s_{t}) :=P(S_{t+1}=s_{(t+1,j)},x_{t}\mid s_{t})\] (59) \[:=P(S_{t+1}=s_{(t+1,j)}\mid s_{t},x_{t})P(x_{t}\mid s_{t}),\] (60)

where \(P_{x_{t}}(s_{t+1}\mid s_{t})=P(s_{t+1}\mid s_{t},\text{do}(x_{t}))=\mathcal{T} (s_{t},x_{t},s_{t+1})\), and \(P(s_{(t+1)}\mid s_{t},x_{t})=\widetilde{\mathcal{T}}\left(s_{t},x_{t},s_{t+1}\right)\). For the state \(s_{(t+1,|S|)}\), we have:

\[P_{x_{t}}(S_{t+1}=s_{(t+1,|S|)}\mid s_{t}) :=1-\left(\sum_{j=1}^{|S|-1}P(S_{t+1}=s_{(t+1,j)},x_{t}\mid s_{t}) \right).\] (61)Following a similar logic in Alg. 1: we reformulate the equation into state-action occupancy measures:

\[\psi^{*}\left(P(x\mid s)\rho(s)-\pi(x\mid s)\rho_{\pi}(s;\mathcal{T})\right)= \psi^{*}\left(\rho(s,x)-\rho_{\pi}(s,x;\mathcal{T})\right)\] (62)

Based on the definition of \(\psi^{*}\), we have:

\[\psi^{*}\left(\rho(s,x)-\rho_{\pi}(s,x;\mathcal{T})\right)\] (63) \[=\max_{\mathcal{T},\mathcal{R}}\sum_{s,x}\left(\rho(s,x)-\rho_{ \pi}(s,x;\mathcal{T})\right)\mathcal{R}(s,x)-\sum_{s,x}\rho_{\pi}(s,x;\mathcal{ T})g_{\phi}(\mathcal{R}(s,x))\] (64) \[=\sum_{s,x}\max_{\mathcal{T},\mathcal{R}}\rho(s,x)\mathcal{R}- \rho_{\pi}(s,x;\mathcal{T})\phi\left(-\phi^{-1}(-\mathcal{R})\right)\] (65) \[=\sum_{s,x}\max_{\mathcal{T},\mathcal{R}^{\prime}}\rho(s,x)(- \phi(\mathcal{R}^{\prime}))-\rho_{\pi}(s,x;\mathcal{T})\phi\left(-\phi^{-1}( \phi(\mathcal{R}^{\prime}))\right)\] (66) \[=\sum_{s,x}\max_{\mathcal{T},\mathcal{R}^{\prime}}\rho(s,x)(- \phi(\mathcal{R}^{\prime}))-\rho_{\pi}(s,x;\mathcal{T})\phi\left(-\mathcal{R} ^{\prime}\right)\] (67)

Suppose \(D\in\mathcal{S}\times\mathcal{X}\mapsto(0,1)\) is a discriminator classifier (e.g, a neural network). Using the logistic loss \(\phi(x)=\log\left(1+e^{-x}\right)\), we can get:

\[\psi^{*}\left(\rho(s,x)-\rho_{\pi}(s,x;\mathcal{T})\right)\] (68) \[=\sum_{s,x}\max_{\mathcal{T},\mathcal{R}^{\prime}}\rho(s,x)\log \left(\frac{1}{1+e^{-\mathcal{R}^{\prime}}}\right)+\rho_{\pi}(s,x;\mathcal{T}) \log\left(1-\frac{1}{1+e^{-\mathcal{R}^{\prime}}}\right)\] (69) \[=\max_{\mathcal{T},D}\mathbb{E}[\log(D(S,X))]+\mathbb{E}_{\pi} \left[\log(1-D(S,X));\mathcal{T}\right].\] (70)

Therefore, we are able to obtain the ultimate target expression:

\[\nu^{*}=\min_{\pi}\max_{\mathcal{T}\in\mathcal{T},D\in(0,1)^{S\times X}} \mathbb{E}[\log(D(S,X))]+\mathbb{E}_{\pi}\left[\log(1-D(S,X));\mathcal{T} \right].\] (71)

## Appendix C Finding the Worst-Case Transition Distribution

In this section, we provide a practical algorithm, Alg. 3, designed to solve the optimization problem formulated in Eq. (19) and Eq. (20). The underlying rationale of Alg. 3 is to search for the worst-case scenario by allocating the minimal transition probability mass to the state with the highest value while assigning greater transition probabilities to states with lower values. The resulting solution should still adhere to a set of predefined observational constraints to ensure feasibility. This approach ensures that the most "adversarial" outcome is prioritized during the optimization process.

To further clarify the approach above, consider the following numerical example. Suppose there are only two states. The value function \(V_{x_{t}}(s_{t+1})\) takes on two values: \(V_{x_{t}}(s_{(t+1,1)})=0.8\) and \(V_{x_{t}}(s_{(t+1,2)})=0.2\). Because \(V_{x_{t}}(s_{(t+1,1)})>V_{x_{t}}(s_{(t+1,2)})\), the algorithm seeks the worst-case discounted future reward by allocating \(P_{x_{t}}(s_{(t+1,1)}\mid s_{t})\gets P(s_{(t+1,1)},x_{t}\mid s_{t})\) and \(P_{x_{t}}(s_{(t+1,2)}\mid s_{t})\gets 1-P(s_{(t+1,1)},x_{t}\mid s_{t})\)3. As such, we are able to collect trajectories from the imitator, even though \(P_{\pi}(s_{t+1}\mid s_{t})\) is not identifiable.

More Details for the Experiments

All experiments were conducted using Intel Cascade Lake processors, with \(30\) vCPUs and \(120\) GB memory on a system running Ubuntu 18.04. Upon acceptance of this manuscript, we intend to make the source code available in the camera-ready version of the paper.

MDPobsPreviously, 1000 random discrete causal models are sampled and all the performance gaps are less than \(0\). In other words, when both the reward and the transition are confounded, all imitators fail to match expert performance.

Specifically, let's take a look at one example instance of those randomly sampled SCM instances. Its detailed parameterization is provided as follows:

\[\begin{split}& P(s_{0})=0.5,\quad\quad\quad\quad\quad\quad \quad\quad\quad\quad P(s_{1})=0.5\\ & P(x_{0},y_{0},s^{\prime}_{0}\mid s_{0})=0.1888,\quad P(x_{0},y_{ 0},s^{\prime}_{1}\mid s_{0})=0.2099,\\ & P(x_{0},y_{1},s^{\prime}_{0}\mid s_{0})=0.0294,\quad P(x_{0},y_ {1},s^{\prime}_{1}\mid s_{0})=0.2116,\\ & P(x_{1},y_{0},s^{\prime}_{0}\mid s_{0})=0.1465,\quad P(x_{1},y_ {0},s^{\prime}_{1}\mid s_{0})=0.0226,\\ & P(x_{1},y_{1},s^{\prime}_{0}\mid s_{0})=0.0645,\quad P(x_{1},y_ {1},s^{\prime}_{1}\mid s_{0})=0.1267,\\ & P(x_{0},y_{0},s^{\prime}_{0}\mid s_{1})=0.1762,\quad P(x_{0},y_ {0},s^{\prime}_{1}\mid s_{1})=0.1775,\\ & P(x_{0},y_{1},s^{\prime}_{0}\mid s_{1})=0.0290,\quad P(x_{0},y_ {1},s^{\prime}_{1}\mid s_{1})=0.1786,\\ & P(x_{1},y_{0},s^{\prime}_{0}\mid s_{1})=0.1761,\quad P(x_{1},y_ {0},s^{\prime}_{1}\mid s_{1})=0.0893,\\ & P(x_{1},y_{1},s^{\prime}_{0}\mid s_{1})=0.1472,\quad P(x_{1},y_ {1},s^{\prime}_{1}\mid s_{1})=0.0261,\end{split}\] (72)

where \(s^{\prime}\) denotes the next state; \(P(x_{0},y_{0},s^{\prime}_{0}\mid s_{0})\) is the abbreviation format for \(P(X_{t}=x_{0},Y_{t}=y_{0},S_{t+1}=s^{\prime}_{0}\mid S_{t}=s_{0})\).

The expert is able to observe the state \(S_{t}\), the unobserved variable \(U_{t}\), and the reward \(Y_{t}\). However, the imitator, lacking access to both \(U_{t}\) or the reward \(\mathbb{E}_{\pi}[Y_{t}]\), makes decisions solely on \(S_{t}\). In other words, all methods utilize the same policy scope \(\pi(x\mid s)\). As shown in Fig. 3(a), imitators consistently failed to match expert performance. Prevalent negative performance gaps indicate that most of imitators were significantly worse than experts; only in rare cases did the performance gaps near \(-0.5\), supporting our theoretical insights presented in Thm. 1. Furthermore, as depicted in Fig. 4(a), CAIL does not achieve expert-level performance, specifically, \(\mathbb{E}_{\pi}\left[R_{t}\right]-\mathbb{E}\left[R_{t}\right]=-1.9019\). However, although CAIL performs worse than the expert, CAIL still consistently outperforms BC and GAIL by effectively learning from the constructed worst-case MDP instances.

MDPobs-Y: Additional Experiment.Consider an SCM instance compatible with Fig. 2(a) including binary observed variables \(S_{t},X_{t},Y_{t}\in\{0,1\}\). \(S_{t}\) represents the state at each time step. \(X_{t}\) denotes the action. The unobserved variable \(U_{t}\) represents some information accessible to the expert but

Figure 5: Simulation results for experiments that are not included in the main manuscript.

inaccessible to the imitator. Additionally, the imitator lacks access to the reward \(\mathbb{E}_{\pi}[Y_{t}]\). Its detailed parameterization is provided as follows:

\[P(s_{0})=0.5, P(s_{1})=0.5\] \[P(x_{0},y_{0},s^{\prime}_{0}\mid s_{0})=0.1775, P(x_{0},y_{0},s^{\prime}_{1}\mid s_{0})=0.2029,\] \[P(x_{0},y_{1},s^{\prime}_{0}\mid s_{0})=0.0001, P(x_{0},y_{1},s^{\prime}_{1}\mid s_{0})=0.0001,\] \[P(x_{1},y_{0},s^{\prime}_{0}\mid s_{0})=0.0993, P(x_{1},y_{0},s^{\prime}_{1}\mid s_{0})=0.0199,\] \[P(x_{1},y_{1},s^{\prime}_{0}\mid s_{0})=0.2001, P(x_{1},y_{1},s^{\prime}_{1}\mid s_{0})=0.3001,\] (73) \[P(x_{0},y_{0},s^{\prime}_{0}\mid s_{1})=0.2859, P(x_{0},y_{0},s^{\prime}_{1}\mid s_{1})=0.1359,\] \[P(x_{0},y_{1},s^{\prime}_{0}\mid s_{1})=0.0001, P(x_{0},y_{1},s^{\prime}_{1}\mid s_{1})=0.0001,\] \[P(x_{1},y_{0},s^{\prime}_{0}\mid s_{1})=0.2969, P(x_{1},y_{0},s^{\prime}_{1}\mid s_{1})=0.2809,\] \[P(x_{1},y_{1},s^{\prime}_{0}\mid s_{1})=0.0001, P(x_{1},y_{1},s^{\prime}_{1}\mid s_{1})=0.0001,\]

where \(s^{\prime}\) denotes the next state; \(P(x_{0},y_{0},s^{\prime}_{0}\mid s_{0})\) is the abbreviation format for \(P(X_{t}=x_{0},Y_{t}=y_{0},S_{t+1}=s^{\prime}_{0}\mid S_{t}=s_{0})\). As depicted in Fig. 4(b), CAIL performs the best among all strategies. Both BC and GAIL fail to match expert performance. Such result shows the effectiveness of Alg. 1.

## Appendix E Broader Impacts

This paper investigates the theoretical framework of causal imitation learning from confounded demonstrations. Our framework is versatile, applicable to various real-world domains such as autonomous driving, robotics, industrial automation, and medical decisions modeling. One of the positive impacts of this study is the exploration of the risks associated with training IRL algorithms when demonstrations are generally contaminated by unobserved confounders. We theoretically prove that when both the transition distribution \(\mathcal{T}\) and reward function \(\mathcal{R}\) are not identifiable, there is no policy \(\pi\) learnable from confounded demonstrations that is guaranteed to perform at least as the expert in all possible scenarios. Such theoretical findings have been validated through extensive randomly generated causal models. When either the reward function or the transition distribution is confounded, we augment the GAIL framework by utilizing partial identification techniques, so that the imitator is optimized within the worst-case scenarios. Specfically, the worst-case reward function in Alg. 1 and the worst-case occupancy measure in Alg. 2. By mitigating the risks associated with unobserved confounders in expert demonstrations, our framework supports the development of more transparent and accountable AI systems. This transparency is crucial in high-stakes areas such as healthcare and transportation, where decision-making errors can have significant repercussions. More broadly, our framework significantly enhances the reliability and safety of autonomous systems in various fields, which prioritize safety and robustness during their decision-making processes. They are increasingly important because black-box AI systems, - whose internal workings remain opaque - become more and more prevalent, and our understandings of their potential implications remain limited.

## Appendix F Impossibility Result in Two-Stage MDPs

In this extension of the MAB model introduced in Sec. 1, we explore a two-stage framework (see Fig. 1(b)). Our previous discussions demonstrated that in MAB settings affected by unobserved confounders, the expert consistently outperforms the imitator; that is, i.e., \(\mathbb{E}_{x}\left[Y\right]<\mathbb{E}[Y]\).

We now extend our analysis to the two-stage MDPs. Specifically, the agent first observes the state \(S_{1}\), selects an action \(X_{1}\), and subsequently, it receives a reward \(Y_{1}\). The process then progresses to the second stage, where the agent transitions to state \(S_{2}\). It chooses an action \(X_{2}\), and then it receives a further reward \(Y_{2}\). A pivotal distinction between this scenario and prior examples lies in the transition probability \(P_{\pi_{1}}(S_{2}\mid S_{1})\). Therefore, we investigate their cumulative reward:

\[\mathbb{E}_{\pi_{1},\pi_{2}}[Y_{1}+Y_{2}]\qquad\text{and}\qquad\mathbb{E}[Y_{1} +Y_{2}].\] (74)

As a motivating example, we assume that all variables are binary. Our analysis begins by comparing the performance at the final stage, specifically, \(\mathbb{E}_{\pi_{1},\pi_{2}}[Y_{2}]\).

Suppose \(f(S_{2})=\mathbb{E}[Y_{2}\mid S_{2},X_{2}]P(X_{2}\mid S_{2})\). Without loss of generality, we assume an ordering in the functional values associated with different states: \(f(S_{2}=0)>f(S_{2}=1)\). To address the non-identifiability issue caused by the transition distribution \(P_{\pi_{1}}(S_{2}\mid S_{1})\), as discussed in Eq. (9), we formulate the worst-case SCM by allocating \(f(S_{2}=0)\) with probability mass \(P(S_{2}=0,X_{1}\mid S_{1})\). In other words, we assign the lower bound \(P(S_{2}=0,x_{1}\mid Z_{1})\) to the non-identifiable query \(P_{x_{1}}(S_{2}=0\mid Z_{1})\). As such, we are able to rewrite the expert's rewards as follows:

\[\mathbb{E}[Y_{2}] =f(S_{2}=0)*P(S_{2}=0,X_{1}=0|Z_{1})P(Z_{1})\] (75) \[+f(S_{2}=0)*P(S_{2}=0,X_{1}=1|Z_{1})P(Z_{1})\] (76) \[+f(S_{2}=1)*P(S_{2}=1,X_{1}=0|Z_{1})P(Z_{1})\] (77) \[+f(S_{2}=1)*P(S_{2}=1,X_{1}=1|Z_{1})P(Z_{1})\] (78)

and the imitator's reward can be written as

\[\mathbb{E}_{\pi_{1},\pi_{2}}[Y_{2}] =\pi_{1}(X_{1}=0|Z_{1})\cdot A+\pi_{1}(X_{1}=1|Z_{1})\cdot B\] (79) \[A =f(S_{2}=0)*P(S_{2}=0,X_{1}=0|Z_{1})P(Z_{1})\] (80) \[+f(S_{2}=1)*(1-P(S_{2}=0,X_{1}=0|Z_{1}))P(Z_{1})\] (81) \[B =f(S_{2}=0)*P(S_{2}=0,X_{1}=1|Z_{1})P(Z_{1})\] (82) \[+f(S_{2}=1)*(1-P(S_{2}=0,X_{1}=1|Z_{1}))P(Z_{1})\] (83)

where is \(\mathbb{E}_{\pi_{1},\pi_{2}}[Y_{2}]\) a convex combination of the quantities \(A\) and \(B\). Therefore, \(\mathbb{E}_{\pi_{1},\pi_{2}}[Y_{2}]\leq\max\{A,B\}\). Given that \(f(S_{2}=0)>f(S_{2}=1)\), we are able to establish that \(A<\mathbb{E}[Y_{2}]\) and \(B<\mathbb{E}[Y_{2}]\). Therefore, \(\mathbb{E}_{\pi_{1},\pi_{2}}[Y_{2}]<\mathbb{E}[Y_{2}]\). Using a similar rationale introduced in Sec. 1, we get \(\mathbb{E}_{\pi_{1}}[Y_{1}]<\mathbb{E}[Y_{1}]\). Consequently,

\[\mathbb{E}_{\pi_{1},\pi_{2}}[Y_{1}+Y_{2}]<\mathbb{E}[Y_{1}+Y_{2}].\] (84)

In other words, the imitator is unable to learn a policy that can obtain the expert's performance in the worst-case 2-stage MDP compatible with the observational distribution \(P(X_{1},X_{2},S_{1},S_{2},Y_{1},Y_{2})\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The major claims made in the abstract and introduction has accurately reflect the paper's contributions and scope. Specifically, we summarized our contributions in Sec. 1, e.g., the theoretical findings and the propose innovative algorithms. Additionally, Table 1 provides a brief summary of this paper's main contributions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the requirements and limitations of the work in Sec. 1 and Sec. 2. To address infinite horizon decision-making challenges, we utilize the Markov Property, as outlined in Def. 1. However, our study generalizes standard imitation methods by focusing on scenarios where causal consistency does not universally hold true.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The assumptions and problem settings can be found in Sec. 2. Due to space constraints, all detailed proofs are provided in Appendices A and B. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All the important details to reproduce the major experimental results in this paper can be found Sec. 4 and Appendix D. Proposed algorithms are provided in Alg. 1 and Alg. 2. Guidelines:* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: If the paper is accepted, we intend to make the source code available in the camera-ready version of the paper. During the meantime, all the important details to reproduce the major experimental results in this paper can be found in Sec. 4 and Appendix D. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the important training and test details in this paper can be found Sec. 4 and Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bars and other appropriate information about the statistical significance the experiments could be found in Sec. 4 and Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Detailed information on the computer resources can be found in Sec. 4 and Appendix D.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This paper introduces novel causal imitation learning algorithms that adapt to confounded expert demonstrations within MDPs by using partial identification techniques. The research conducted in this conform with paper NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Societal impacts are discussed in Sec. 1 and Appendix E. Our framework can be applied to various fields in reality, including autonomous driving, robotics, industrial automation, medical decisions modeling and so on. One of positive impacts of this work is that we discuss the potential risk of training IRL algorithms when demonstrations are contaminated by unobserved confounders, and how to utilize partial identification techniques to make the imitator robust. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer:[Yes] Justification: Please check Sec. 4 and Appendix D. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Please check Sec. 4 and Appendix D. Guidelines: * The answer NA means that the paper does not release new assets.

* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.