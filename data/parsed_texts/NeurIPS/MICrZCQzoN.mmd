# Generalization bounds for mixing processes via delayed online-to-PAC conversions

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study the generalization error of statistical learning algorithms in a non-_i.i.d._ setting, where the training data is sampled from a stationary mixing process. We develop an analytic framework for this scenario based on a reduction to online learning with delayed feedback. In particular, we show that the existence of an online learning algorithm with bounded regret (against a fixed statistical learning algorithm in a specially constructed game of online learning with delayed feedback) implies low generalization error of said statistical learning method even if the data sequence is sampled from a mixing time series. The rates demonstrate a trade-off between the amount of delay in the online learning game and the degree of dependence between consecutive data points, with near-optimal rates recovered in a number of well-studied settings when the delay is tuned appropriately as a function of the mixing time of the process.

## 1 Introduction

In machine learning, generalization denotes the ability of a model to infer patterns from a dataset of training examples and apply them to analyze previously unseen data (Shalev-Shwartz and Ben-David, 2014). The gap in accuracy between the model's predictions on new data and those on the training set is usually referred to as _generalization error_. Providing upper bounds on this quantity is a central goal in statistical learning theory. Classically, bounds based on notions of complexity (_e.g._, VC dimension and Rademacher complexity) for the model's hypothesis space were used to provide uniform worst-case guarantees (see Bousquet et al., 2004; Vapnik, 2013; Shalev-Shwartz and Ben-David, 2014). However, results of this kind are often too loose to be applied to the most common machine learning over-parameterised models, such as deep neural networks (Zhang et al., 2021). As a consequence, several approaches have been proposed to obtain algorithm-dependent generalization bounds, which can adapt to the problem and be much tighter in practice than their uniform counterparts. Often, the underlying idea is that if the algorithm's output does not have a too strong dependence on the specific input dataset used for the training, then the model should not be prone to overfitting, and so generalize well. Examples of results that build onto these ideas are stability bounds, information-theoretic bounds, and PAC-Bayesian bounds (see, _e.g._, Bousquet and Elisseeff, 2002; Russo and Zou, 2020; Hellstrom et al., 2023; Alquier, 2024).

Most results in the literature focus on the _i.i.d._ setting, where the training dataset is made of independent draws from some underlying data distribution. However, for several applications, this assumption is far from realistic. For instance, it excludes the case where observations received by the learner have some inherent temporal dependence, as it is the case for stock prices, daily energy consumption, or sensor data from physical environments (Ariyo et al., 2014; Takeda et al., 2016). This calls for the development of theory for addressing non-_i.i.d._ data. A common approach in the extant literature is to consider a class of non-_i.i.d._ data-generating processes usually referred to as stationary \(\beta\)-mixingor \(\varphi\)-mixing processes. This assumption, together with a "blocking" trick introduced by Yu (1994), has led to a few results in the literature: Meir (2000), Mohri and Rostamizadeh (2008), Shalizi and Kontorovich (2013), and Wolfer and Kontorovich (2019) provided uniform worst-case generalization bounds, Steinwart and Christmann (2009) and Agarwal and Duchi (2012) discussed excess risk bound (comparing the algorithm's output with the best possible hypothesis), while Mohri and Rostamizadeh (2010) gave bounds based on a stability analysis (in the sense of Bousquet and Elisseeff, 2002).

Here, we propose propose results for the non-_i.i.d._ setting in the form of PAC-Bayesian bounds (Guedj, 2019; Alquier, 2024): high probability upper bounds on the expected generalization error of randomized learning algorithms. We achieve this by combining the "blocking" argument by Yu (1994) to manage the concentration of sums of correlated random variables, with the recent online-to-PAC conversion technique recently proposed by Lugosi and Neu (2023). Using their framework we show a new way to obtain generalization bounds for stationary dependent processes that satisfy a certain "short-memory" property (intuitively meaning that data points that are closer in time are more heavily dependent on each other). Our assumption slightly differs from \(\beta\)-mixing in the sense that we only need it to hold for a specific class of bounded loss functions. Among other results, this allows us to prove PAC-Bayesian generalization bounds for mixing processes. This complements previous work on such bounds that have only considered mild relaxations of the _i.i.d._ condition such as assuming that the data has a martingale structure (see, _e.g._, Seldin et al., 2012; Chugg et al., 2023; Haddouche and Guedj, 2023). Notable exceptions are the works of Alquier and Wintenberger (2012), Alquier et al. (2013), and Eringis et al. (2022, 2024), who provided generalization bounds for a sequential prediction setting where both the data-generating process and the hypothesis class used for prediction are stable dynamical systems. Their results are proved under some very specific conditions on these systems, and their guarantees involve unspecified problem-dependent constants that may be large. In contrast, our bounds hold under general, simple-to-verify conditions and feature explicit constants.

The rest of the paper is organized as follows. In Section 2 we properly define the generalization error of a statistical learning algorithm for both _i.i.d._ and non-_i.i.d._ cases, and state our main assumption on the data dependence. Our main contribution lies in Section 3, where after recalling the results for the _i.i.d._ setting we show how to adapt this to stationary mixing processes. In Section 4 we provide concrete results of the bounds we can obtain through the online-to-PAC conversion. Finally in Section 5 we extend our results to the setting where the hypothesis class itself may consist of dynamical systems.

**Notation.** For a distribution over hypotheses \(P\in\Delta_{W}\) and bounded function \(f:\mathcal{W}\to\mathbb{R}\) we write \(\langle P,f\rangle\) to refer to the expectation of \(\mathbb{E}_{W\sim P}[f(W)]\). We denote \(\mathcal{D}_{KL}(P||Q)=\mathbb{E}_{X\sim P}\left[\ln\left(\frac{P(X)}{Q(X)} \right)\right]\) to refer to the Kullback-Leibler divergence. We use \(||.||\) to denote a norm on the Banach space \(\mathcal{Q}\) of the finite signed measures, and \(||.||_{*}\) the corresponding dual norm on the dual space \(\mathcal{Q}^{*}\) of measurable functions \(f\) on \(\mathcal{W}\) such that \(||f||_{*}=\sup_{Q\in\mathcal{Q}:||Q||\leq 1}\langle Q,f\rangle\).

## 2 Preliminaries

The classical statistical learning framework usually considers a dataset \(S_{n}=(Z_{1},...,Z_{n})\), made of \(n\)_i.i.d._ elements drawn from a distribution \(\mu\) over a measurable instance space \(\mathcal{Z}\). Often, one can think of each \(Z_{i}\) as a feature-label pair \((X_{i},Y_{i})\). Furthermore, we are given a measurable class \(\mathcal{W}\) of hypotheses and a loss function \(\ell:\mathcal{W}\times\mathcal{Z}\to\mathbb{R}_{+}\), with \(\ell(w,z)\) measuring the quality of the hypothesis \(w\in\mathcal{W}\) on the data instance \(z\in\mathcal{Z}\). For any given hypothesis \(w\in\mathcal{W}\), two key objects of interest are the _training error_\(\widehat{\mathcal{L}}(w,S_{n})=\frac{1}{n}\sum_{i=1}^{n}\ell(w,Z_{i})\) and the _test error_\(\mathcal{L}(w)=\mathbb{E}_{Z^{\prime}\sim\mu}[\ell(w,Z^{\prime})]\), where the random element \(Z^{\prime}\) has the same distribution as \(Z_{i}\) and is independent of \(S_{n}\).

A learning algorithm \(\mathcal{A}:\mathcal{Z}^{n}\to\mathcal{W}\) maps the training sample to an hypothesis in \(\mathcal{W}\). More generally, we will focus on randomized learning algorithms, returning a probability distribution \(P_{W_{n}|S_{n}}\in\Delta_{\mathcal{W}}\) over \(\mathcal{W}\), conditionally on \(S_{n}\) (deterministic algorithms can be recovered as special cases, whose the outputs are Dirac distributions). The ultimate goal of the learner is to minimize the test error. Yet, this quantity cannot be computed without knowledge of the data generating distribution \(\mu\). In practice, one typically relies on the training error in order to gauge the quality of the algorithm. For an algorithm \(\mathcal{A}:S_{n}\mapsto P_{W_{n}|S_{n}}\), we define the _generalization error_ as the expected gap between training and test error:

\[\mathrm{Gen}(\mathcal{A},S_{n})=\mathbb{E}\left[\mathcal{L}(W_{n})-\widehat{ \mathcal{L}}(W_{n},S_{n})\Big{|}S_{n}\right].\]The expectation in the above expression integrates over the randomness in the output of the algorithm \(W_{n}\sim P_{W_{n}|S_{n}}\), conditionally on the sample \(S_{n}\). We remark that the test error is _not_ equal to the mean of the training error, due to the dependence of \(W_{n}\) on the training data.

We extend the previous setting by considering the case where the data have an intrinsic temporally ordered structure, and come in the form of a stationary process \((Z_{t})_{t\in\mathbb{N}^{*}}\sim\nu\). Formally, we assume that the joint marginal distribution of any block \((Z_{t},Z_{t-1},\ldots,Z_{t-i})\) is the same as the distribution of \((Z_{t+j},Z_{t+j-1},\ldots,Z_{t+j-i})\) for any \(t\), \(i\) and \(j\), but the data points are not necessarily independent of each other. In particular, the marginal distribution of \(Z_{t}\) is constant and is denoted by \(\mu\). Thus, it is natural to continue to use the definition of the test loss and generalization error given above, although with the understanding that \(\mu\) now refers to the marginal distribution of an independent copy of \(Z_{1}\), a sample point from a stationary non-_i.i.d._ process. We remark here that other notions of the test loss may also be considered, and the framework that we propose can be extended to most natural definitions with little work (but potentially large notational overhead). In Section 5, we provide such an extension for a more general setting where the hypotheses themselves are allowed to have memory and the process may not be as strongly stationary as our assumption above requires.

In order to obtain generalization results we need to have some control on how strong the dependencies between different datapoints are allowed to be. To this regard, we consider the following assumption.

**Assumption 1**.: _There exists a non-increasing sequence \((\phi_{d})_{d\in\mathbb{N}^{*}}\) of non-negative real numbers such that, for all \(w\in\mathcal{W}\) and all \(t\in\mathbb{N}^{*}\):_

\[\mathbb{E}\left[\mathcal{L}(w)-\ell(w,Z_{t})\Big{|}\mathcal{F}_{t-d}\right] \leq\phi_{d}\,,\]

_where \(\mathcal{L}(w)=\mathbb{E}_{Z^{\prime}\sim\mu}[\ell(w,Z^{\prime})]\), with \(Z^{\prime}\) being independent on the process \((Z_{t})_{t\in\mathbb{N}^{*}}\) and having as distribution the stationary marginal \(\mu\) of the \(Z_{t}\)._

The intuition behind this assumption is that the loss associated with the observations \(Z_{t}\) becomes almost independent of the past after \(d\) steps, enabling us to treat each sequence of the form \((Z_{t},Z_{t+d},\ldots,Z_{t+(n-t)d})\) as an approximately _i.i.d._ sequence. Note that this assumption differs from the usual \(\beta\)-mixing assumption which requires the distribution of \(Z_{t}|\mathcal{F}_{t-d}\) to be close to the marginal distribution \(\mu\) for all \(t\), in terms of total variation distance. Our assumption is somewhat weaker in the sense that it only requires the expected losses under these distributions to be close, and only a one-sided inequality is required. It is easy to verify that our assumption is satisfied if the process is \(\beta\)-mixing in the usual sense and the losses are bounded in \([0,1]\).

## 3 Proving generalization bounds via online learning

Online learning focuses on algorithms that aim to improve performance incrementally as new information becomes available, often without any underlying assumption on how data are generated. The online learner's performance is typically measured leveraging the idea of regret. This involves introducing a cost function for the problem and defining the regret as the difference between the cumulative cost of the online learner and that of a fixed comparator. We refer to the monographs Cesa-Bianchi and Lugosi, 2006 and Orabona, 2019 for comprehensive overviews on online learning and regret analysis. Recently, Lugosi and Neu (2023) established a connection between upper bounds on the regret and generalization bounds, showing that the existence of a strategy with a bounded regret in a specially designed online game translates into a generalization bound, via a technique dubbed _online-to-PAC conversion_. Their focus is on the _i.i.d._ setting, where the training dataset is made of independent draws. Here, we show that this framework can naturally be extended beyond the _i.i.d._ assumption.

In what follows, we briefly review the setup of Lugosi and Neu (2023) in Section 3.1 and then describe our new extension of their model to the non-_i.i.d._ case in Section 3.2. In particular, we prove a high-probability bound for the generalization error of any statistical learning algorithm learnt with a stationary mixing process verifying Assumption 1.

### Online-to-PAC conversions for _i.i.d._ data

Lugosi and Neu (2023) have recently established a framework to obtain generalization bounds via a reduction to online learning. Their technique allows to recover several classic PAC-Bayesianresults, and provide a range of generalizations thereof. The main idea of Lugosi and Neu (2023) is to introduce an online learning game called the _generalization game_, where the following steps are repeated for a sequence of rounds \(t=1,2,\ldots,n\):

* the online learner picks a distribution \(P_{t}\in\Delta_{\mathcal{W}}\);
* the adversary selects the cost function \(c_{t}:w\mapsto\ell(w,Z_{t})-\mathcal{L}(w)\);
* the online learner incurs the cost \(\langle P_{t},c_{t}\rangle=\mathbb{E}_{W\sim P_{t}}[c_{t}(W)]\);
* \(Z_{t}\) is revealed to the learner.

The learner can adopt any strategy to pick \(P_{t}\), but they can only rely on past knowledge to make their prediction. Explicitly, if \(\mathcal{F}_{t}\) denotes the sigma-algebra generated by \(Z_{1},...,Z_{t}\), then \(P_{t}\) has to be \(\mathcal{F}_{t-1}\)-measurable. We also emphasize that in this setup the online learner is allowed to know the loss function \(\ell\) and the distribution \(\mu\) of the data points \(Z_{t}\), and therefore by revealing the value of \(Z_{t}\), the online learner may compute the entire cost function \(c_{t}\).

We define the _regret_ of the online learner against the possibly data-dependent _comparator_\(P^{*}\in\Delta_{\mathcal{W}}\) as \(\operatorname{Regret}(P^{*})=\sum_{t=1}^{n}\langle P_{t}-P^{*},c_{t}\rangle\). Now, denote as \(P_{W_{n}|S_{n}}\) the distribution produced by the supervised learning algorithm. With this notation, the generalization error can be written as \(\operatorname{Gen}(\mathcal{A},S_{n})=-\frac{1}{n}\sum_{t=1}^{n}\langle P_{W_ {n}|S_{n}},c_{t}\rangle\). By adding and subtracting the quantity \(M_{n}=-\frac{1}{n}\sum_{t=1}^{n}\langle P_{t},c_{t}\rangle\) we get the following decomposition.

**Theorem 1** (Theorem 1 in Lugosi and Neu, 2023; see appendix A.1).: _With the notation introduced above,_

\[\operatorname{Gen}(\mathcal{A},S_{n})=\frac{\operatorname{Regret}_{n}(P_{W_{n }|S_{n}})}{n}+M_{n}\,.\] (1)

The first of these terms correspond to the _regret_ of the online learner against a fixed _comparator strategy_ that picks \(P_{W_{n}|S_{n}}\) at each step. The second term is a martingale and can be bounded in high probability with standard concentration tools. Indeed, since \(P_{t}\) is chosen before \(Z_{t}\) is revealed, one can easily check that \(\mathbb{E}[\langle P_{t},c_{t}\rangle|\mathcal{F}_{t-1}]=0\). Thus, to prove a bound on the generalization error of the statistical learning algorithm, it is enough to find an online learning algorithm with bounded regret against \(P_{W_{n}|S_{n}}\) in the generalization game.

As a concrete application of the above, the following generalization bound is obtained when picking the classic exponential weighted average (EWA) algorithm (Vovk, 1990; Littlestone and Warmuth, 1994; Freund and Schapire, 1997) as online strategy, and plugging its regret bound into (1).

**Theorem 2** (Corollary \(6\) in Lugosi and Neu, 2023).: _Suppose that \(\ell(w,z)\in[0,1]\) for all \(w,z\). Then, for any \(P_{1}\in\Delta_{\mathcal{W}}\) and \(\eta>0\), with probability at least \(1-\delta\) on the draw of \(S_{n}\), uniformly on every learning algorithm \(\mathcal{A}:S_{n}\mapsto P_{W_{n}|S_{n}}\), we have_

\[\operatorname{Gen}(\mathcal{A},S_{n})\leq\frac{\mathcal{D}_{KL}(P_{W_{n}|S_{n }}||P_{1})}{\eta n}+\frac{\eta}{2}+\sqrt{\frac{2\log\left(\frac{1}{\delta} \right)}{n}}\,.\]

Proof.: We can bound each term of (1) separately. A data-dependent bound for the regret term is obtained via a direct application of the regret analysis of EWA which brings the term \(\frac{\mathcal{D}_{KL}(P_{W_{n}|S_{n}}||P_{1})}{\eta n}+\frac{\eta}{2}\) (see Appendix B.1). The term \(\sqrt{\frac{2\log\left(\frac{1}{\delta}\right)}{n}}\) results from bounding the martingale \(M_{n}\) via an application of Hoeffding-Azuma inequality. 

Note that the first term in the above bound is data-dependent due to the presence of \(P_{W_{n}|S_{n}}\), and thus optimizing it requires a data-dependent choice of \(\eta\), which is not allowed by Theorem 2. However, via a union bound argument it is possible to get a bound in the form

\[\operatorname{Gen}(\mathcal{A},S_{n})=\mathcal{O}\left(\sqrt{\frac{\mathcal{D} _{KL}(P_{W_{n}|S_{n}}||P_{1})}{n}}+\sqrt{\frac{1}{n}\log\left(\frac{\log n}{ \delta}\right)}\right),\]

For the details, we refer to the proof of Corollary 5 of Lugosi and Neu (2023), which recovers a classical PAC-Bayes bound of McAllester (1998).

### Online-to-PAC conversions for non-_i.i.d._ data

In what follows, we will drop the _i.i.d._ assumption for the data, and instead consider non-_i.i.d._ sequences satisfying Assumption 1. For this setting we define the following variant of the generalization game.

**Definition 1** (Generalization game with delay).: _The generalization game with delay \(d\in\mathbb{N}^{*}\) is an online learning game where the following steps are repeated for a sequence of rounds \(t=1,...,n\):_

* _the online learner picks a distribution_ \(P_{t}\in\Delta_{\mathcal{W}}\)_;_
* _the adversary selects the cost function_ \(c_{t}:w\mapsto\ell(w,Z_{t})-\mathcal{L}(w)\)_;_
* _the online learner incurs the cost_ \(\langle P_{t},c_{t}\rangle=\mathbb{E}_{W\sim P_{t}}[c_{t}(W)]\)_;_
* _if_ \(t\geq d\)_,_ \(Z_{t-d+1}\) _(and thus_ \(c_{t-d+1}\)_) is revealed to the learner._

The main difference between our version of the generalization game and the standard one of Lugosi and Neu (2023) is the introduction of a _delay_ on the online learning algorithm's decisions. Specifically, we will force the online learner to only take information into account up to time \(t-d\) when picking their action \(P_{t}\). Clearly, setting \(d=1\) recovers the original version of the generalization game with no delay.

It is easy to see that the regret decomposition of Theorem 1 still remains valid in the current setting. The purpose of introducing the delay is to be able to make sure that the term \(M_{n}=-\frac{1}{n}\sum_{t=1}^{n}\left\langle P_{t},c_{t}\right\rangle\) is small. The lemma below states that the increments of \(M_{n}\) behave similarly to a martingale-difference sequence, thanks to the introduction of the delay.

**Lemma 1**.: _Fix \(d\in\llbracket 1,n\rrbracket\). Under assumption 1, it holds for all \(t\in\llbracket 1,n\rrbracket\):_

\[\mathbb{E}[\langle-P_{t},c_{t}\rangle|\mathcal{F}_{t-d}]\leq\phi_{d}\,.\]

_where \(P_{t}\) and \(c_{t}\) are defined as in 1._

Proof.: Since \(P_{t}\) is \(\mathcal{F}_{t-d}\)-measurable we have \(\mathbb{E}[\langle-P_{t},c_{t}\rangle|\mathcal{F}_{t-d}]=\langle P_{t}, \mathbb{E}[-c_{t}|\mathcal{F}_{t-d}]\rangle\leq\phi_{d}\), where the last step uses Assumption 1. 

Thus, by following the decomposition of Theorem 1, we are left with the problem of bounding the regret of the delayed online learning algorithm against \(P_{W_{n}|S_{n}}\), denoted as \(\mathrm{Regret}_{d,n}(P_{W_{n}|S_{n}})=\sum_{t=1}^{n}\left\langle P_{t}-P_{W_ {n}|S_{n}},c_{t}\right\rangle\). The following proposition states a simple and clean bound that one can immediately derive from these insights.

**Proposition 1** (Bound in expectation).: _Consider \((Z_{t})_{t\in\mathbb{N}^{*}}\) satisfying Assumption 1 and suppose there exists a \(d\)-delayed online learning algorithm with regret bounded by \(\mathrm{Regret}_{d,n}(P^{*})\) against any comparator \(P^{*}\). Then, the expected generalization of \(\mathcal{A}\) is bounded as_

\[\mathbb{E}\left[\mathrm{Gen}(\mathcal{A},S_{n})\right]\leq\frac{\mathbb{E} \left[\mathrm{Regret}_{d,n}(P_{W_{n}|S_{n}})\right]}{n}+\phi_{d}\,.\]

Proof.: By Theorem 1, it holds that \(\mathbb{E}[\mathrm{Gen}(\mathcal{A},S_{n})]=\frac{\mathbb{E}[\mathrm{Regret}_{d,n}(P_{W_{n}|S_{n}})]}{n}+\mathbb{E}[M_{n}]\), where the regret is for a strategy \(P_{t}\) in the delayed generalization game. Hence, by Lemma 1

\[\mathbb{E}[M_{n}]=\mathbb{E}\left[-\frac{1}{n}\sum_{t=1}^{n}\langle P_{t},c_{ t}\rangle\right]=\frac{1}{n}\sum_{t=1}^{n}\mathbb{E}[\langle-P_{t},c_{t} \rangle]=\frac{1}{n}\sum_{t=1}^{n}\mathbb{E}\left[\mathbb{E}[\langle-P_{t},c _{t}\rangle|\mathcal{F}_{t-d}]\right]\leq\phi_{d}\,,\]

which proves the claim. 

The above result holds in expectation over the training sample. We now provide a high-probability guarantee on the generalization error.

**Theorem 3** (Bound in probability).: _Assume that \((Z_{t})_{t\in\mathbb{N}^{*}}\) satisfies Assumption 1 and consider a \(d\)-delayed online learning algorithm with regret bounded by \(R_{d,n}(P^{*})\) against any comparator \(P^{*}\). Then, for any \(\delta>0\), it holds with probability \(1-\delta\) on the draw of \(S_{n}\), uniformly for all \(\mathcal{A}\),_

\[\mathrm{Gen}(\mathcal{A},S_{n})\leq\frac{R_{d,n}(P_{W_{n}|S_{n}})}{n}+\phi_{d }+\sqrt{\frac{2d\log\left(\frac{d}{\delta}\right)}{n}}.\]The proof of this claim follows directly from combining the decomposition of Theorem 1 with a standard concentration result for mixing processes that we state below.

**Lemma 2**.: _Fix \(d\in\llbracket 1,n\rrbracket\) and consider \((Z_{t})_{t\in\mathbb{N}^{*}}\) satisfying Assumption 1. Consider the generalization game of Definition 1. Then, for any \(\delta>0\), the following bound is satisfied with probability at least \(1-\delta\):_

\[M_{n}\leq\phi_{d}+\sqrt{\frac{2d\log\left(\frac{d}{\delta}\right)}{n}}.\]

The proof is based on a classic "blocking" technique due to Yu (1994). For the sake of completeness, we provide a proof in Appendix A.2.

## 4 New generalization bounds for non-_i.i.d._ data

The dependence on the delay \(d\) for the bounds that we presented in the previous section is non-trivial. Indeed, if on the one hand increasing the delay will reduce the magnitude of \(\phi_{d}\), on the other hand the regret of the online learner will grow with \(d\). There is hence a trade-off between these two terms appearing in our bounds. In what follows, we derive some concrete generalization bounds from Theorem 3, under a number of different choices of the online learning algorithm. For concreteness, we will consider two types of mixing assumptions, but stress that the approach can be applied to any process that satisfies Assumption 1.

### Regret bounds for delayed online learning

From Theorem 3, we can obtain a generalization bound using our framework if we have a regret bound for a delayed online algorithm. This is a well-known problem in the area of online learning (see, _e.g._, Weinberger and Ordentlich, 2002; Joulani et al., 2013). In the following, we will leverage the following simple trick that allows us to extend the regret bounds of any online learning algorithm to its delayed counterpart, provided that the regret bound respects some specific assumptions.

**Lemma 3** (Weinberger and Ordentlich, 2002).: _Consider any online algorithm whose regret satisfies \(\operatorname{Regret}_{n}(P^{*})\leq R(n)\) for any comparator \(P^{*}\), where \(R\) is a non-decreasing real-valued function such that \(y\mapsto yR(x/y)\) is a concave function of \(y\) for any fixed \(x\). Then, for any \(d\geq 1\) there exists an online learning algorithm with delay \(d\) such that, for any comparator \(P^{*}\),_

\[\operatorname{Regret}_{d,n}(P^{*})\leq dR\left(n/d\right)\,.\]

The proof idea is closely related to the blocking trick of Yu (1994), with an algorithmic construction that runs one instance of the base method for each index \(i=1,2,\dots,d\), with the \(i\)-th instance being responsible for the regret in rounds \(i,i+d,i+2d,\dots\) (more details are provided in Appendix B.3). For most of the regret bounds that we consider, the function \(R\) takes the form \(R(n)=O(\sqrt{n})\), so that the first term in the generalization bound is typically of order \(\sqrt{d/n}\). Since this term matches the bound on \(M_{n}\) in Lemma 2, in this case the final generalization bound behaves effectively as if the sample size was \(n/d\) instead of \(n\).

### Geometric and algebraic mixing

The following definition gives two concrete examples of mixing processes that satisfy Assumption 1 with different choices of \(\phi_{d}\), and are commonly considered in the related literature (see, e.g., Mohri and Rostamizadeh, 2010; Levin and Peres, 2017).

**Definition 2**.: _We say that a stationary process \((Z_{t})_{t\in\mathbb{N}^{*}}\) satisfying Assumption 1 is:_

* geometrically mixing_, if_ \(\phi_{d}=Ce^{-\frac{d}{t}}\)_, for some positive_ \(\tau\) _and_ \(C\)_;_
* algebraically mixing_, if_ \(\phi_{d}=Cd^{-r}\)_, for some positive_ \(r\) _and_ \(C\)_._

Instantiating the bound of Theorem 3 to these two cases yields the following two corollaries.

**Corollary 1**.: _Assume \((Z_{t})_{t\in\mathbb{N}^{*}}\) is a geometrically mixing process with constants \(\tau,C>0\). Consider a \(d\)-delayed online learning algorithm with regret bounded by \(R_{d,n}(P^{*})\) for all comparators \(P^{*}\).__Then, setting \(d=\lceil\tau\log n\rceil\), for any \(\delta>0\), with probability at least \(1-\delta\) we have that, uniformly for any algorithm \(\mathcal{A}\),_

\[\mathrm{Gen}(\mathcal{A},S_{n})\leq\frac{R_{d,n}(P_{W_{n}|S_{n}})}{n}+\frac{C}{n }+\sqrt{\frac{2\left(\tau\log n+1\right)\log\left(\frac{\tau\log n+1}{\delta} \right)}{n}}\,.\]

Up to a term linear in \(\tau\) and some logarithmic factors, the above states that under the geometric mixing the same rates are achievable as in the _i.i.d._ setting. Roughly speaking, this amounts to saying that the effective sample size is a factor \(\tau\) smaller than the original number of samples \(n\), as long as generalization is concerned.

**Corollary 2**.: _Assume \((Z_{t})_{t\in\mathbb{N}^{*}}\) is an algebraic mixing process with constants \(r,C>0\). Consider a \(d\)-delayed online learning algorithm with regret bounded by \(R_{d,n}(P^{*})\) against any comparator \(P^{*}\). Then, setting \(d=\left(C^{2}n\right)^{1/(1+2r)}\), for any \(\delta>0\), with probability at least \(1-\delta\) we have that, uniformly for any algorithm \(\mathcal{A}\),_

\[\mathrm{Gen}(\mathcal{A},S_{n})\leq\frac{R_{d,n}(P_{W_{n}|S_{n}})}{n}+C\left( 1+\sqrt{\log(d/\delta)}\right)n^{-\frac{2r}{2(1+2r)}}\,.\]

This result suggests that the rates achievable for algebraically mixing processes are qualitatively much slower than what one can get for _i.i.d._ or geometrically mixing data sequences (although the rates do eventually approach \(1/\sqrt{n}\) as \(r\) goes to infinity).

### Multiplicative weights with delay

We start our discussion on possible online strategies by focusing on the classic exponential weighted average (EWA) algorithm (Vovk, 1990; Littlestone and Warmuth, 1994; Freund and Schapire, 1997). We fix a data-free prior \(P_{1}\in\Delta_{\mathcal{W}}\) and a learning rate parameter \(\eta>0\). We consider the updates

\[P_{t+1}=\operatorname*{arg\,min}_{P\in\Delta_{\mathcal{W}}}\left\{\langle P, c_{t}\rangle+\frac{1}{\eta}\mathcal{D}_{KL}(P||P_{t})\right\},\]

Combining the standard regret bound of EWA (see Appendix B.1) with Lemma 3 and Corollary 1 yields the result that follows.

**Corollary 3**.: _Suppose that \((Z_{t})_{t\in\mathbb{N}^{*}}\) is a geometric mixing process with constants \(\tau,C>0\). Suppose that \(\ell(w,z)\in[0,1]\) for all \(w,z\). Then, for any \(P_{1}\in\Delta_{\mathcal{W}}\) and any \(\delta>0\), with probability at least \(1-\delta\), uniformly on any learning algorithm \(\mathcal{A}\) we have_

\[\mathrm{Gen}(\mathcal{A},S_{n})\leq\frac{\mathcal{D}_{KL}(P^{*}||P_{1})(\tau \log n+1)}{\eta n}+\frac{\eta}{2}+\frac{C}{n}+\sqrt{\frac{2\left(\tau\log n+1 \right)\log\left(\frac{\tau\log n+1}{\delta}\right)}{n}}\,.\]

This results suggests that when considering geometric mixing processes, by applying a union bound over a well-chosen range of \(\eta\) we recover the PAC-Bayes bound of McAllester (1998) up to a \(O(\sqrt{\tau\log n})\) factor. A similar result can be derived from Corollary 2 for algebraically mixing processes, leading to a bound typically scaling as \(n^{-2r/(2(1+2r))}\).

### Follow the regularized leader with delay

In this subsection we extend the common class of online learning algorithms known as follow the regularized leader (FTRL, see _e.g._, Abernethy and Rakhlin, 2009; Orabona, 2019) to the problem of learning with delay. FTRL algorithms are defined using a convex regularization function \(h:\Delta_{\mathcal{W}}\rightarrow\mathbb{R}\). We restrict ourselves to the set of proper, lower semi-continuous and \(\alpha\)-strongly convex functions with respect to a norm \(||.||\) (and its respective dual norm \(||.||_{*}\)) defined on the set of signed finite measures on \(\mathcal{W}\) (see Appendix B.2 for more details). The online procedure (without delay) of the FTRL algorithm is as follows:

\[P_{t+1}=\operatorname*{arg\,min}_{P\in\Delta_{\mathcal{W}}}\left\{\sum_{s=1} ^{t}\langle P,c_{s}\rangle+\frac{1}{\eta}h(P)\right\}.\]The existence of the minimum is guaranteed by the compactness of \(\Delta_{\mathcal{W}}\) under \(\left\|\cdot\right\|\), and its uniqueness is ensured by the strong convexity of \(h\). Combining the analysis of FTRL (see Appendix B.2) with Lemma 3 and Corollary 1 yields the following result.

**Corollary 4**.: _Suppose that \((Z_{t})_{t\in\mathbb{N}^{*}}\) is a geometric mixing process with constants \(\tau,C>0\). Suppose that \(\ell(w,z)\in[0,1]\) for all \(w,z\). Assume there exists \(B>0\) such that for all \(t,\left\|c_{t}\right\|_{*}\leq B\). Then, for any \(P_{1}\in\Delta_{\mathcal{W}}\), for any \(\delta>0\) with probability at least \(1-\delta\) on the draw of \(S_{n}\), uniformly for all \(\mathcal{A}\),_

\[\mathrm{Gen}(\mathcal{A},S_{n})\leq\frac{\left(h(P^{*})-h(P_{1})\right)(\tau \log n+1)}{\eta n}+\frac{\eta B^{2}}{2\alpha}+\frac{C}{n}+\sqrt{\frac{2\left( \tau\log n+1\right)\log\left(\frac{\tau\log n+1}{\delta}\right)}{n}}.\]

This generalization bound is similar to the bound of Theorem 9 of Lugosi and Neu (2023) up to a \(O(\sqrt{\tau\log n})\) factor, when applying a union-bound argument over an appropriate grid of learning-rates \(\eta\). In particular, this result recovers PAC-Bayesian bounds like those of Corollary 3 when choosing \(h=\mathcal{D}_{\mathrm{KL}}\left(\cdot\middle\|P_{1}\right)\). We refer to Section 3.2 in Lugosi and Neu (2023) for more discussion on such bounds. As before, a similar result can be stated for algebraically mixing processes, with the leading terms approaching zero at rate of \(n^{-2r/2(1+2r)}\) instead of \(n^{-1/2}\).

## 5 Generalization bounds for dynamic hypotheses

Finally, inspired by the works of Eringis et al. (2022, 2024), we extend our framework to accommodate loss functions \(\ell\) that rely not only on the last data point \(Z_{t}\), but on the entire data sequence \(\overline{Z}_{t}=(Z_{t},Z_{t-1},\dots,Z_{1})\). Formally, we will consider loss functions of the form \(\ell:\mathcal{W}\times\mathcal{Z}^{*}\rightarrow\mathbb{R}_{+}\)1 and write \(\ell(w,\overline{z}_{t})\) to denote the loss associated with hypothesis \(w\in\mathcal{W}\) on sequence \(\overline{z}_{t}\in\mathcal{Z}^{t}\). This consideration extends the learning problem to class of dynamical predictors such as Kalman filters, autoregressive models, or recurrent neural networks (RNNs), broadly used in time-series forecasting (Ariyo et al., 2014; Takeda et al., 2016). Specifically, if we think of \(z_{t}=(x_{t},y_{t})\) as a data-pair of context and observation, in time-series prediction we usually not only rely on the context \(x_{t}\) but also on the past sequence of contexts and observations \((x_{t-1},y_{t-1},\dots,x_{1},y_{1})\). As an example, consider \(\ell(w,z_{t},\dots,z_{1})=\frac{1}{2}(y_{t}-h_{w}(x_{t},z_{t-1},\dots,z_{1}))^ {2}\) where \(h\in\mathcal{H}\) is a function class parameterized by \(\mathcal{W}\). For this type of loss function a natural definition of the test error is:

Footnote 1: Here, \(\mathcal{Z}^{*}\) denotes the disjoint union \(\mathcal{Z}^{*}=\sqcup_{t\in\mathbb{N}}\mathcal{Z}^{t}\).

\[\widetilde{\mathcal{L}}(w)=\lim_{n\rightarrow\infty}\mathbb{E}[\ell(w,Z_{t}^{ \prime},Z_{t-1}^{\prime},...,Z_{t-n}^{\prime})],\]

where \(\overline{Z}_{t}^{\prime}=(Z_{t}^{\prime},Z_{t-1}^{\prime},\dots)\) is a semi-infinite random sequence drawn from the same stationary process that has generated the data \(\overline{Z}_{t}\). We consider the following assumption.

**Assumption 2**.: _For a given process \((Z_{t})_{t\in\mathbb{Z}}\) with joint-distribution \(\nu\) over \(\mathcal{Z}^{\mathbb{Z}}\) and same marginals \(\mu\) over \(\mathcal{Z}\), there exists a non-increasing sequence \((\phi_{d})_{d\in\mathbb{N}^{*}}\) of non-negative real numbers such that the following holds for all \(w\in\mathcal{W}\), for all \(t\in\mathbb{N}^{*}\):_

\[\mathbb{E}\left[\ell(w,Z_{t},\dots,Z_{1})-\widetilde{\mathcal{L}}(w)\middle| \mathcal{F}_{t-d}\right]\leq\phi_{d}.\]

This is a generalization of Assumption 1 in the sense that taking \(\ell(w,Z_{t},\dots,Z_{1})=\ell(w,Z_{t})\) simply amounts to requiring the same mixing condition as before. For our online-to-PAC conversion we consider the same framework as in Definition 1, except that now the cost function is defined as

\[c_{t}:w\mapsto\ell(w,Z_{t},\dots,Z_{1})-\widetilde{\mathcal{L}}(w)\,.\]

Then it easy to check that result of Lemma 2 still holds for this specific cost, and we can thus extend all the results of Section 4. For concreteness, we state the following adaptation of Theorem 3 below.

**Theorem 4**.: _Assume \((Z_{t})_{t\in\mathbb{Z}}\) which satisfies Assumption 2 and consider a \(d\)-delayed online learning algorithm with regret bounded by \(R_{d,n}(P^{*})\) against any comparator \(P^{*}\). Then, for any \(\delta>0\), it holds with probability \(1-\delta\):_

\[\mathrm{Gen}(\mathcal{A},S_{n})\leq\frac{R_{d,n}(P_{W_{n}\mid S_{n}})}{n}+\phi_ {d}+\sqrt{\frac{2d\log\left(\frac{d}{\delta}\right)}{n}}.\]To see that Assumption 2 can be verified and the resulting bounds can be meaningfully applied, consider the following concrete assumptions about the hypothesis class, the loss function, and the data generating process. The first assumption says that for any given hypothesis, the influence of past data points on the associated loss vanishes with time (_i.e._, the hypothesis forgets the old data points at a controlled rate).

**Assumption 3**.: _There exists a decreasing sequence \((B_{d})_{d\in\mathbb{N}^{*}}\) of non-negative real numbers such that for any two sequences \(\overline{z}_{t}=(z_{t},\ldots,z_{i})\) and \(\overline{z}^{\prime}_{t}=(z^{\prime}_{t},\ldots,z^{\prime}_{j})\) of possibly different lengths that satisfy \(z_{k}=z^{\prime}_{k}\) for all \(k\in t,\ldots,t-d+1\), we have \(|\ell(w,\overline{z}_{t})-\ell(w,\overline{z}^{\prime}_{t})|\leq B_{d},\) for all \(w\in\mathcal{W}\)._

This condition can be verified for stable dynamical systems like autoregressive models, certain classes of RNNs, or sequential predictors that have bounded memory by design (see Eringis et al., 2022, 2024). The next assumption is a refinement of Assumption 1, adapted to the case where the loss function acts on blocks of \(d\) data points \(\overline{z}_{t-d+1:t}=(z_{t},z_{t-1},\ldots,z_{t-d+1})\).

**Assumption 4**.: _Let \(\overline{Z}_{t}=(Z_{t},\ldots,Z_{1})\) be a sequence of data points and let \(\overline{Z}^{\prime}_{t}=(Z^{\prime}_{t},\ldots,Z^{\prime}_{0},\ldots)\) be an independent copy of the same process. Then, there exists a decreasing sequence \((\beta_{d})_{d\in\mathbb{N}^{*}}\) non-negative real numbers such that the following is satisfied for all hypotheses \(w\in\mathcal{W}\) and all \(d\in\mathbb{N}^{*}\):_

\[\mathbb{E}\left[\left.\ell(w,\overline{Z}^{\prime}_{t-d+1:t})-\ell(w,\overline {Z}_{t-d+1:t})\right|\mathcal{F}_{t-2d}\right]\leq\beta_{d}\,.\]

This assumption can be verified whenever the loss function is bounded and the joint distribution of the data block \(\overline{Z}_{t-d+1:t}\) satisfies a \(\beta\)-mixing assumption. In more detail, this latter condition amounts to requiring that the conditional distribution of each data block given a block that trails \(d\) steps behind is close to the marginal distribution in total variation distance, up to an additive term of \(\beta_{d}\). The following proposition shows that these two simple conditions together imply that Assumption 2 holds, and that thus the bound of Theorem 4 can be meaningfully instantiated for bounded-memory hypothesis classes deployed on mixing processes.

**Proposition 2**.: _Suppose that the loss function satisfies Assumption 3 and the data distribution satisfies Assumption 4. Then Assumption 2 is satisfied with \(\phi_{d}=2B_{d/2}+\beta_{d/2}\)._

## 6 Conclusion

We have developed a general framework for deriving generalization bounds for non-i.i.d. processes under a general mixing assumption, via an extension of the online-to-PAC-conversion framework of Lugosi and Neu (2023). Among other results, this approach has allowed us to prove PAC-Bayesian generalization bounds for such data in a clean and transparent way, and even study classes of dynamic hypotheses under a simple bounded-memory condition. These results provide a clean and tight alternative to the results of (Alquier and Wintenberger, 2012; Eringis et al., 2022). The generality of our approach further demonstrates the power of the Online-to-PAC scheme of Lugosi and Neu (2023), and in particular our results provide further evidence that this framework is particularly promising for developing techniques for generalization in non-i.i.d. settings. We hope that flexibility of our framework will find further uses and enables more rapid progress in the area.

## References

* Abernethy and Rakhlin (2009) Abernethy, J. and Rakhlin, A. (2009). Beating the adaptive bandit with high probability. _IEEE Information Theory and Applications Workshop_.
* Agarwal and Duchi (2012) Agarwal, A. and Duchi, J. (2012). The generalization ability of online algorithms for dependent data. _IEEE Transactions on Information Theory_, 59(1).
* Alquier (2024) Alquier, P. (2024). User-friendly introduction to PAC-Bayes bounds. _Foundations and Trends in Machine Learning_, 17(2).
* Alquier et al. (2013) Alquier, P., Li, X., and Wintenberger, O. (2013). Prediction of time series by statistical learning: general losses and fast rates. _Dependence Modeling_, 1(1).
* Alquier and Wintenberger (2012) Alquier, P. and Wintenberger, O. (2012). Model selection for weakly dependent time series forecasting. _Bernoulli_, 18(3).
* Ariyo et al. (2014) Ariyo, A. A., Adewumi, A. O., and Ayo, C. K. (2014). Stock price prediction using the ARIMA model. _UKSim-AMSS International Conference on Computer Modelling and Simulation_.
* Bousquet et al. (2004) Bousquet, O., Boucheron, S., and Lugosi, G. (2004). _Introduction to Statistical Learning Theory_. Springer.
* Bousquet and Elisseeff (2002) Bousquet, O. and Elisseeff, A. (2002). Stability and generalization. _Journal of Machine Learning Research_, 2.
* Cesa-Bianchi and Lugosi (2006) Cesa-Bianchi, N. and Lugosi, G. (2006). _Prediction, learning, and games_. Cambridge university press.
* Chugg et al. (2023) Chugg, B., Wang, H., and Ramdas, A. (2023). A unified recipe for deriving (time-uniform) PAC-Bayes bounds. _Journal of Machine Learning Research_, 24(372).
* Eringis et al. (2022) Eringis, D., Leth, J., Tan, Z., Wisniewski, R., and Petreczky, M. (2022). PAC-Bayesian-like error bound for a class of linear time-invariant stochastic state-space models. _arXiv:2212.14838_.
* Eringis et al. (2024) Eringis, D., Leth, J., Tan, Z., Wisniewski, R., and Petreczky, M. (2024). PAC-Bayes generalisation bounds for dynamical systems including stable rnns. _AAAI Conference on Artificial Intelligence_.
* Freund and Schapire (1997) Freund, Y. and Schapire, R. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. _Journal of Computer and System Sciences_, 55.
* Guedj (2019) Guedj, B. (2019). A primer on PAC-Bayesian learning. _Second congress of the French Mathematical Society_.
* Haddouche and Guedj (2023) Haddouche, M. and Guedj, B. (2023). PAC-Bayes generalisation bounds for heavy-tailed losses through supermartingales. _Transactions on Machine Learning Research_, 2023(4).
* Hellstrom et al. (2023) Hellstrom, F., Durisi, G., Guedj, B., and Raginsky, M. (2023). Generalization bounds: Perspectives from information theory and PAC-Bayes. _arXiv:2309.04381_.
* Joulani et al. (2013) Joulani, P., Gyorgy, A., and Szepesvari, C. (2013). Online learning under delayed feedback. _ICML_.
* Levin and Peres (2017) Levin, D. and Peres, Y. (2017). _Markov chains and mixing times_. American Mathematical Soc.
* Littlestone and Warmuth (1994) Littlestone, N. and Warmuth, M. (1994). The weighted majority algorithm. _Information and computation_, 108(2).
* Lugosi and Neu (2023) Lugosi, G. and Neu, G. (2023). Online-to-PAC conversions: Generalization bounds via regret analysis. _arXiv:2305.19674_.
* McAllester (1998) McAllester, D. A. (1998). Some PAC-Bayesian theorems. _COLT_.
* Meir (2000) Meir, R. (2000). Nonparametric time series prediction through adaptive model selection. _Machine Learning_, 39.
* Meir and Rakhlin (2013)* Mohri and Rostamizadeh (2008) Mohri, M. and Rostamizadeh, A. (2008). Rademacher complexity bounds for non-i.i.d. processes. _NeurIPS_.
* Mohri and Rostamizadeh (2010) Mohri, M. and Rostamizadeh, A. (2010). Stability bounds for stationary \(\phi\)-mixing and \(\beta\)-mixing processes. _Journal of Machine Learning Research_, 11(26).
* Orabona (2019) Orabona, F. (2019). A modern introduction to online learning. _arXiv:1912.13213_.
* Russo and Zou (2020) Russo, D. and Zou, J. (2020). How much does your data exploration overfit? controlling bias via information usage. _IEEE Transactions on Information Theory_, 66(1).
* Seldin et al. (2012) Seldin, Y., Laviolette, F., Cesa-Bianchi, N., Shawe-Taylor, J., and Auer, P. (2012). PAC-Bayesian inequalities for martingales. _IEEE Transactions on Information Theory_, 58(12).
* From Theory to Algorithms_. Cambridge University Press.
* Shalizi and Kontorovich (2013) Shalizi, C. and Kontorovich, A. (2013). Predictive PAC learning and process decompositions. _NeurIPS_.
* Steinwart and Christmann (2009) Steinwart, I. and Christmann, A. (2009). Fast learning from non-i.i.d. observations. _NeurIPS_.
* Takeda et al. (2016) Takeda, H., Tamura, Y., and Sato, S. (2016). Using the ensemble Kalman filter for electricity load forecasting and analysis. _Energy_, 104.
* Vapnik (2013) Vapnik, V. (2013). _The nature of statistical learning theory_. Springer science & business media.
* Vovk (1990) Vovk, V. (1990). Aggregating strategies. _COLT_.
* Weinberger and Ordentlich (2002) Weinberger, M. and Ordentlich, E. (2002). On delayed prediction of individual sequences. _IEEE Transactions on Information Theory_, 48(7).
* Wolfer and Kontorovich (2019) Wolfer, G. and Kontorovich, A. (2019). Minimax learning of ergodic Markov chains. _ALT_.
* Yu (1994) Yu, B. (1994). Rates of convergence for empirical processes of stationary mixing sequences. _The Annals of Probability_, 22(1).
* Zhang et al. (2021) Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2021). Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3).

Omitted proofs

### The proof of Theorem 1

Let \((P_{t})_{t=1}^{n}\in\Delta_{\mathcal{W}}^{n}\) be the predictions of an online learner playing the _generalization game_. Then

\[\operatorname{Gen}(\mathcal{A},S_{n}) =\frac{1}{n}\sum_{t=1}^{n}\mathbb{E}[\ell_{t}(W_{n})-\mathcal{L}(W_ {n})|S_{n}]\] \[=-\frac{1}{n}\sum_{t=1}^{n}\mathbb{E}[c_{t}(W_{n})|S_{n}]\] \[=-\frac{1}{n}\sum_{t=1}^{n}\langle P_{W_{n}|S_{n}},c_{t}\rangle\] \[=\frac{1}{n}\sum_{t=1}^{n}\langle P_{t}-P_{W_{n}|S_{n}},c_{t} \rangle-\frac{1}{n}\sum_{t=1}^{n}\langle P_{t},c_{t}\rangle\] \[=\frac{\operatorname{Regret}_{n}(P_{W_{n}|S_{n}})}{n}+M_{n}.\]

### The proof of Lemma 2

Assume \(n=Kd\) for simplicity:

\[M_{n} =-\frac{1}{n}\sum_{t=1}^{n}\langle P_{t},c_{t}\rangle\] \[=\frac{1}{dK}\sum_{i=1}^{d}\sum_{t=1}^{K}\langle-P_{i+d(t-1)},c_{ i+d(t-1)}\rangle\]

We denote \(X_{t}^{(i)}=\langle-P_{i+d(t-1)},c_{i+d(t-1)}\rangle\) and we want to bound in high-probability the term \(\frac{1}{K}\sum_{t=1}^{K}X_{t}^{(i)}\). Let also denote \(\mathcal{F}_{t}^{(i)}=\mathcal{F}_{i+d(t-1)}\). Then for \(i\in\llbracket 1,d\rrbracket\), we can write using Chernoff's technique that for all \(\lambda>0\) it holds:

\[\mathbb{P}\left(\frac{1}{K}\sum_{t=1}^{K}X_{t}^{(i)}\geq u\right) \leq\frac{\mathbb{E}\left[e^{\frac{\lambda}{K}\sum_{t=1}^{K}X_{t} ^{(i)}}\right]}{e^{\lambda u}}\] \[\leq\mathbb{E}\left[e^{\frac{\lambda}{K}\sum_{t=1}^{K-1}X_{t}^{( i)}}\mathbb{E}\left[e^{\frac{\lambda}{K}X_{K}^{(i)}}\Big{|}\mathcal{F}_{K-1}^{(i)} \right]\right]e^{-\lambda u}.\]

Now remark that:

\[\mathbb{E}\left[e^{\frac{\lambda}{K}X_{K}^{(i)}}\Big{|}\mathcal{F}_{K-1}^{(i) }\right]=\mathbb{E}\left[e^{\frac{\lambda}{K}\langle X_{K}^{(i)}-\mathbb{E}[X _{K}^{(i)}|F_{K-1}^{(i)}]\rangle}\Big{|}F_{K-1}^{(i)}\right]e^{\frac{\lambda}{ K}\mathbb{E}[X_{K}^{(i)}|F_{K-1}^{(i)}]}.\]

If we denote \(Z=X_{K}^{(i)}-\mathbb{E}[X_{K}^{(i)}|F_{K-1}^{(i)}]\) then \(|Z|\leq 2\) and \(\mathbb{E}[Z|F_{K-1}^{(i)}]=0\) so via Hoeffding's lemma:

\[\mathbb{E}[e^{\frac{\lambda}{K}Z}]\leq e^{\frac{\lambda^{2}}{2K^{2}}}.\]

Now by construction of the \(P_{t}\) and because of Lemma 1 it follows that for all \(i\), \(\mathbb{E}[X_{K}^{(i)}|F_{K-1}^{(i)}]\leq\phi_{d}\). Repeating the same reasoning for each term of the sum yields:

\[\mathbb{P}\left(\frac{1}{K}\sum_{t=1}^{K}X_{t}^{(i)}\geq u\right)\leq e^{ \frac{\lambda^{2}}{2K}}e^{\lambda\phi_{d}}e^{-\lambda u}.\]

Optimzing with \(\lambda=K(u-\phi_{d})\) and taking \(\delta=e^{-\frac{K(u-\phi)^{2}}{2}}\) it finally holds for any \(\delta>0,\) with probability \(1-\frac{\delta}{d}\):

\[\frac{1}{K}\sum_{t=1}^{K}X_{t}^{(i)}\leq\phi_{d}+\sqrt{\frac{2\log\left(\frac {d}{\delta}\right)}{K}}\.\]Thus applying a union bound we have with probability \(1-\delta\):

\[M_{n}\leq\phi_{d}+\sqrt{\frac{2\log\left(\frac{d}{\delta}\right)}{K}}\,,\]

which concludes the proof. 

### Proof of Proposition 2

Suppose without loss of generality that \(d\) is even and define \(d^{\prime}=d/2\). For the proof, let \(\overline{Z}^{\prime}_{n}\) be a semi-infinite sequence drawn independently from the same process as \(\overline{Z}_{n}\). Then, we have

\[\tilde{\mathcal{L}}(w) =\lim_{n\to\infty}\mathbb{E}[\ell(w,Z^{\prime}_{t},Z^{\prime}_{t- 1},...,Z^{\prime}_{t-n})]\] \[\leq\mathbb{E}[\ell(w,Z^{\prime}_{t},Z^{\prime}_{t-1},\dots,Z^{ \prime}_{t-d^{\prime}})]+B_{d^{\prime}}\] \[\leq\mathbb{E}\left[\left.\ell(w,Z_{t},Z_{t-1},\dots,Z_{t-d^{ \prime}})\right|\mathcal{F}_{t-2d^{\prime}}\right]+B_{d^{\prime}}+\beta_{d^{ \prime}}\] \[\leq\mathbb{E}\left[\left.\ell(w,Z_{t},Z_{t-1},\dots,Z_{t-d^{ \prime}},\dots,Z_{1})\right|\mathcal{F}_{t-2d^{\prime}}\right]+2B_{d^{\prime}} +\beta_{d^{\prime}}\] \[\leq\mathbb{E}\left[\left.\ell(w,Z_{t},Z_{t-1},\dots,Z_{1}) \right|\mathcal{F}_{t-2d^{\prime}}\right]+2B_{d^{\prime}}+\beta_{d^{\prime}}\,,\]

where we used Assumption 3 in the first inequality, Assumption 4 in the second one, and Assumption 3 again in the last step. This proves the statement. 

## Appendix B Online Learning Tools and Results

### Regret Bound for EWA

Recalling EWA updates we have:

\[P_{t+1}=\operatorname*{arg\,min}_{P\in\Delta_{\mathcal{W}}}\left\{\langle P, c_{t}\rangle+\frac{1}{\eta}\mathcal{D}_{KL}(P||P_{t})\right\},\]

where \(\eta>0\) is a learning-rate parameter. The minimizer can be shown to exist and satisfies:

\[\frac{\mathrm{d}P_{t+1}}{\mathrm{d}P_{t}}(w)=\frac{e^{-\eta c_{t}(w)}}{\int_{ \mathcal{W}}e^{-\eta c_{t}(w^{\prime})}\mathrm{d}P_{t}(w^{\prime})},\]

and the following result holds.

**Proposition 3**.: _For any prior \(P_{1}\in\Delta_{\mathcal{W}}\) and any comparator \(P^{*}\in\Delta_{\mathcal{W}}\) the regret of EWA simultaneously satisfies for \(\eta>0\):_

\[\mathrm{Regret}(P^{*})\leq\frac{\mathcal{D}_{\text{KL}}(P^{*}||P_{1})}{\eta} +\frac{\eta}{2}\sum_{t=1}^{n}||c_{t}||_{\infty}^{2}.\]

We refer the reader to Appendix A.1 of Lugosi and Neu (2023) for a complete proof of the result above.

### Regret Bound for FTRL

We say that \(h\) is \(\alpha-\)strongly convex if the following inequality is satisfied for all \(P,P^{\prime}\in\Delta_{\mathcal{W}}\) and all \(\lambda\in[0,1]\):

\[h(\lambda P+(1-\lambda)P^{\prime})\leq\lambda h(P)+(1-\lambda)h(P^{\prime})- \frac{\alpha\lambda(1-\lambda)}{2}||P-P^{\prime}||^{2}.\]

Recalling the FTRL updates:

\[P_{t+1}=\operatorname*{arg\,min}_{P\in\Delta_{\mathcal{W}}}\left\{\sum_{s=1}^ {t}\langle P,c_{s}\rangle+\frac{1}{\eta}h(P)\right\},\]

the following results holds.

**Proposition 4**.: _For any prior \(P_{1}\in\Delta_{\mathcal{W}}\) and any comparator \(P^{*}\in\Delta_{\mathcal{W}}\) the regret of FTRL simultaneously satisfies for \(\eta>0\):_

\[\mathrm{Regret}_{n}(P^{*})\leq\frac{h(P^{*})-h(P_{1})}{\eta}+\frac{\eta}{2 \alpha}\sum_{t=1}^{n}||c_{t}||_{*}^{2}.\]

We refer the reader to Appendix A.3 of Lugosi and Neu (2023) for a complete proof of the results above.

### Details about the reduction of Weinberger and Ordentlich (2002)

For concreteness we formally present how to turn any online learning algorithm into its delayed version. For sake of convenience, assume \(n=Kd\). We denote \(\tilde{c}_{t}^{(i)}=c_{i+d(t-1)}\) (for instance \(\tilde{c}_{1}^{(1)}=c_{1}\) is the cost revealed at time \(d+1\)). Then we create \(d\) instances of horizon time \(K\) of the online learning as follows, for \(i=1,\ldots,d\):

* We initialize \(\tilde{P}_{1}^{(i)}=P_{0}\),
* for each block \(i\) of length \(K\) we update for \(t=1,\ldots,K\): \[\tilde{P}_{t+1}^{(i)}=\text{OL}_{\text{update}}\left((\tilde{c}_{s}^{(i)})_{s =1}^{t}\right).\]

Here \(\text{OL}_{\text{update}}\) refers to the update function of the online learning algorithm we consider which can possibly depend of the whole history of cost functions (e.g., in the case of the FTRL update).

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We claim that we present a new framework adapted from Lugosi and Neu, 2023 to prove generalization bounds in non-_i.i.d_ setting. We present it in Section 3and we provide PAC-Bayesian bounds in Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.

* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The main result of the paper lies in Section 3.2 and is carefully explained. Regarding Section 4 where most of the results are presented we give all the technical results and references in the AppendixB.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: paper does not include experiments requiring code. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.

* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments requiring code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details**Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?

Answer: [NA]

Justification: The paper does not include experiments requiring code.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments requiring code. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments requiring code. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The contribution is mainly theoretical so we do not discuss these issues in the paper. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.

* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We do not use existing assets. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.