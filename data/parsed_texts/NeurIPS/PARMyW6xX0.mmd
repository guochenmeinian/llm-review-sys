# Type-to-Track: Retrieve Any Object

via Prompt-based Tracking

Pha Nguyen\({}^{1}\), Kha Gia Quach\({}^{2}\), Kris Kitani\({}^{3}\), Khoa Luu\({}^{1}\)

\({}^{1}\) CVIU Lab, University of Arkansas \({}^{2}\) pdActive Inc. \({}^{3}\) Robotics Institute, Carnegie Mellon University

\({}^{1}\){panguyen, khoaluu}@uark.edu \({}^{2}\)kquach@ieee.org \({}^{3}\)kkitani@cs.cmu.edu

uark-cviu.github.io/Type-to-Track

###### Abstract

One of the recent trends in vision problems is to use natural language captions to describe the objects of interest. This approach can overcome some limitations of traditional methods that rely on bounding boxes or category annotations. This paper introduces a novel paradigm for Multiple Object Tracking called _Type-to-Track_, which allows users to track objects in videos by typing natural language descriptions. We present a new dataset for that Grounded Multiple Object Tracking task, called _GroOT_, that contains videos with various types of objects and their corresponding textual captions describing their appearance and action in detail. Additionally, we introduce two new evaluation protocols and formulate evaluation metrics specifically for this task. We develop a new efficient method that models a transformer-based eMbed-ENcoDE-extRact framework (_MENDER_) using the third-order tensor decomposition. The experiments in five scenarios show that our _MENDER_ approach outperforms another two-stage design in terms of accuracy and efficiency, up to 14.7% accuracy and \(4\times\) speed faster.

## 1 Introduction

Tracking the movement of objects in videos is a challenging task that has received significant attention in recent years. Various methods have been proposed to tackle this problem, including deep learning techniques. However, despite these advances, there is still room for improvement in intuitiveness and responsiveness. One potential way to improve object tracking in videos is to incorporate user input into the tracking process. Traditional Visual Object Tracking (VOT) methods typically require

Figure 1: An example of the responsive _Type-to-Track_. The user provides a video sequence and a prompting request. During tracking, the system is able to discriminate appearance attributes to track the target subjects accordingly and iteratively responds to the user’s tracking request. Each box color represents a unique identity.

users to manually select objects in the video by points [1], bounding boxes [2; 3], or trained object detectors [4; 5]. Thus, in this paper, we introduce a new paradigm, called _Type-to-Track_, to this task that combines responsive typing input to guide the tracking of objects in videos. It allows for more intuitive and conversational tracking, as users can simply type in the name or description of the object they wish to track, as illustrated in Fig. 1. Our intuitive and user-friendly _Type-to-Track_ approach has numerous potential applications, such as surveillance and object retrieval in videos.

We present a new Grounded Multiple Object Tracking dataset named _GroOT_ that is more advanced than existing tracking datasets [6; 7]. _GroOT_ contains videos with various types of multiple objects and detailed textual descriptions. It is \(2\times\) larger and more diverse than any existing datasets, and it can construct many different evaluation settings. In addition to three easy-to-construct experimental settings, we propose two new settings for prompt-based visual tracking. It brings the total number of settings to five, which will be presented in Section 5. These new experimental settings challenge existing designs and highlight the potential for further advancements in our proposed research topic.

In summary, this work addresses the use of natural language to guide and assist the Multiple Object Tracking (MOT) tasks with the following contributions. First, a novel paradigm named _Type-to-Track_ is proposed, which involves responsive and conversational typing to track any objects in videos. Second, a new _GroOT_ dataset is introduced. It contains videos with various types of objects and their corresponding textual descriptions of 256K words describing definition, appearance, and action. Next, two new evaluation protocols that are tracking by _retrieval prompts_ and _caption prompts_, and three class-agnostic tracking metrics are formulated for this problem. Finally, a new transformer-based eMbed-ENcobe-extRact framework (_MENDER_) is introduced with third-order tensor decomposition as the first efficient approach for this task. Our contributions in this paper include a novel paradigm, a rich semantic dataset, an efficient methodology, and challenging benchmarking protocols with new evaluation metrics. These contributions will be advantageous for the field of Grounded MOT by providing a valuable foundation for the development of future algorithms.

## 2 Related Work

### Visual Object Tracking Datasets and Benchmarks

**Datasets.** To develop and train VOT models for the computer vision task of tracking objects in videos, various datasets have been created and widely used. Some of the most popular datasets for VOT are OTB [19; 8], VOT [9], GOT [10], MOT challenges [12; 14] and BDD100K [15]. Visual object tracking has two sub-tasks: _Single Object Tracking_ (SOT) and _Multiple Object Tracking_ (MOT). Table 1 shows that there is a wide variety of object tracking datasets in both types available, each with its own strengths and weaknesses. Existing datasets with NLP [6; 7] only support the SOT task, while our _GroOT_ dataset supports MOT with approximately \(2\times\) larger in description size.

**Benchmarks.** Current benchmarks for tracking can be broadly classified into two main categories: _Tracking by Bounding Box_ and _Tracking by Natural Language_, depending on the type of initialization.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline
**Datasets** & **Task** & **NLP** & **\#Videos** & **\#Frames** & **\#Tracks** & **\#AnnBoxes** & **\#Words** & **\#Settings** \\ \hline
**OTB100**[8] & SOT & ✗ & 100 & 59K & 100 & 59K & - & - \\
**VOT-2017**[9] & SOT & ✗ & 60 & 21K & 60 & 21K & - & - \\
**GOT-10k**[10] & SOT & ✗ & 10K & 1.5M & 10K & 1.5M & - & - \\
**TrackingNet**[11] & SOT & ✗ & **30K** & **14.43M** & **30K** & **14.43M** & - & - \\ \hline
**MOT17**[12] & **MOT** & ✗ & 14 & 11.2K & 1.3K & 0.3M & - & - \\
**TAO**[13] & **MOT** & ✗ & 1.5K & **2.2M** & 8.1K & 0.17M & - & - \\
**MOT20**[14] & **MOT** & ✗ & 8 & 13.41K & 3.83K & 2.1M & - & - \\
**BDD100K**[15] & **MOT** & ✗ & **2K** & 318K & **130.6K** & **3.3M** & - & - \\ \hline
**LaSOT**[6] & SOT & ✓ & 1.4K & **3.52M** & 1.4K & **3.52M** & 9.8K & 1 \\
**TNL2K**[7] & SOT & ✓ & 2K & 1.24M & 2K & 1.24M & 10.8K & 1 \\
**Ref-DAVIS**[16] & VOS & ✓ & 150 & 94K & 400+ & - & 10.3K & **2** \\
**Refer-YTOS**[17] & VOS & ✓ & **4K** & 1.24M & **7.4K** & 131K & **158K** & **2** \\ \hline
**Ref-KITTI**[18] & **MOT** & ✓ & 18 & 6.65K & - & - & 3.7K & 1 \\
**GroOT (Ours)** & **MOT** & ✓ & **1,515** & **2.25M** & **13.3K** & **2.57M** & **256K** & **5** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of current datasets. # denotes the number of the corresponding item. **Bold** numbers are the best number in each sub-block, while **highlighted** numbers are the best across all sub-blocks.

Previous benchmarks [20; 19; 8; 9; 21; 22; 22; 23] were limited to test videos before the emergence of deep trackers. The first publicly available benchmarks for visual tracking were OTB-2013 [19] and OTB-2015 [8], consisting of 50 and 100 video sequences, respectively. GOT-10k [10] is a benchmark featuring 10K videos classified into 563 classes and 87 motions. TrackingNet [11], a subset of the object detection benchmark YT-BB [24], includes 31K sequences. Furthermore, there are long-term tracking benchmarks such as OxUvA [25] and LaSOT [6]. OxUvA spans 14 hours of video in 337 videos, comprising 366 object tracks. On the other hand, LaSOT [6] is a language-assisted dataset consisting of 1.4K sequences with 9.8K words in their captions. In addition to these benchmarks, TNL2K [7] includes 2K video sequences for natural language-based tracking and focuses on expressing the attributes. LaSOT [6] and TNL2K [7] support one benchmarking setting with their provided prompts, while our _GroOT_ dataset supports five settings. Ref-KITTI [18] is built upon the KITTI [26] dataset and contains only two categories, including car and pedestrian, while our _GroOT_ dataset focuses on category-agnostic tracking, and outnumbers the frames and settings.

A similar task with a different nomenclature to the Grounded MOT task is Referring Video Object Segmentation (Ref-VOS) [16; 17], which primarily measures the overlapping area between the ground truth and prediction for a single foreground object in each caption, with less emphasis on densely tracking multiple objects over time. In contrast, our proposed _Type-to-Track_ paradigm is distinct in its focus on _responsively_ and _conversationally_ typing to track any objects in videos, requiring maintaining the temporal motions of multiple objects of interest.

### Grounded Object Tracking

**Grounded Vision-Language Models** accurately map language concepts onto visual observations by understanding both vision content and natural language. For instance, visual grounding [29] seeks to identify the location of nouns or short phrases (such as a black hat or a blue bird) within an image. Grounded captioning [30; 31; 32] can generate text descriptions and align predicted words with object regions in an image. Visual dialog [33] enables meaningful dialogues with humans about visual content using natural, conversational language. Some visual dialog systems may incorporate referring expression recognition [34] to resolve expressions in questions or answers.

**Grounded Single Object Tracking** is limited to tracking a single object with box-initialized and language-assisted methods. The GTI [27] framework decomposes the tracking by language task into three sub-tasks: Grounding, Tracking, and Integration, and generates tubelet predictions frame-by-frame. AdaSwitcher [7] module identifies tracking failure and switches to visual grounding for better tracking. [35] introduce a unified system using attention memory and cross-attention modules with learnable semantic prototypes. Another transformer-based approach [28] is presented including a cross-modal fusion module, task-specific heads, and a proxy token-guided fusion module.

### Discussion

Most existing datasets and benchmarks for object tracking are limited in their coverage and diversity of language and visual concepts. Additionally, the prompts in the existing Grounded SOT benchmarks do not contain variations in covering many objects in a single prompt, which limits the application of existing trackers in practical scenarios. To address this, we present a new dataset and benchmarking

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline
**Approach** & **Task** & **NLP** & **Clb-agn** & **Feat** & **Stages** \\ \hline GT1 [27] & SOT & assist & ✗ & concat & **single** \\ TransVLT [28] & SOT & assist & ✗ & **attn** & **single** \\ \hline TrackFormer [4] & **MOT** & - & ✗ & - & - \\ \hline MDETFR-**T-rin** & **MOT** & **init** & ✓ & **attn** & two \\ TransRMOT [18] & **MOT** & **init** & ✓ & **attn** & two \\
**MENDER** & **MOT** & **init** & ✓ & **attn** & **single** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of key features of tracking methods. **Cls-agn** is for class-agnostic, while **Feat** is for the approach of feature fusion and **Stages** indicates the number of stages in the model design incorporating NLP into the tracking task. **NLP** indicates how text is utilized for the tracker: _assist_ (w/ box) or can _initialize_ (w/o box).

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline
**Datasets** & **Followations** & **Frames** & **Friends** & **Aandpasses** & **Friends** & **Parts** \\ \hline \multirow{3}{*}{**MTTI-**} & Train & 7 & 5,316 & 546 & 12,297 & **3729** & (1) \\  & Test & 7 & 5,519 & 785 & 185,000 & **5.751** & (2) \\ \hline \multirow{3}{*}{**TADF-T**} & **Total** & 14 & 11,125 & 1,337 & 300,373 & **5.949** & (2) \\ \cline{2-7}  & Train & 500 & 764,526 & 2,645 & 5,459 & 11,125 & **10.800** & (4) \\ \cline{2-7}  & Val & 914 & 4,666,58 & 5,485 & 11,112 & **10.800** & (4) \\ \cline{2-7}  & Test & 914 & 221,284,86 & 7,921 & 16,450 & & & \\ \hline \multirow{3}{*}{**MTOT-2**} & Train & 4 & 8,911 & 2,337 & 136,927 & - & (5) \\  & Test & 4 & 4,479 & 1,504 & 7,465,465 & - & (6) \\ \cline{2-7}  & **Total** & 8 & 13,410 & 3,837 & 2,102,387 & & \\ \hline \multirow{3}{*}{**GroOT-2**} & **Total** & 8 & 1,314 & 6,337 & 2,102,387 & & \\ \cline{2-7}  & **Total** & 8 & 1,329,287 & 1,329,287 & 2,102,387 & & \\ \cline{2-7}  & **Total** & 13 & 2,487 & 1,329,287 & 2,102,387 & & \\ \hline \multirow{3}{*}{**GroOT-3**} & **Total** & 13 & 2,487,17 & 13,294 & 2,250,509 & 21,546 & _all_ \\ \cline{2-7}  & **Total** & 13 & 2,129,87 & 1,329,294 & 2,509,509 & 21,546 & _all_ \\ \cline{2-7}  & **Total** & 13 & 2,139,287 & 1,329,294 & 2,509,509 & 21,546 & _all_ \\ \cline{2-7}  & **Total** & 13 & 2,149,87 & 1,329,294 & 2,309,509 & 21,546 & _all_ \\metrics to support the emerging trend of the Grounded MOT, where the goal is to align language descriptions with fine-grained regions or objects in videos.

As shown in Table 2, most of the recent methods for the Grounded SOT task are not class-agnostic, meaning they require prior knowledge of the object. GTI [27] and TransVLT [28] need to input the initial bounding box, while TrackFormer [4] need the pre-defined category. The operation used in [27] to fuse visual and textual features is _concatenation_ which can only support prompts describing a single object. A Grounded MOT can be constructed by integrating a grounded object detector, i.e. MDETR [36], and an object tracker, i.e. TrackFormer [4]. However, this approach is low-efficient because the visual features have to be extracted multiple times. In contrast, our proposed MOT approach _MENDER_ formulates third-order _attention_ to adaptively focus on many targets, and it is an efficient _single-stage_ and _class-agnostic_ framework. The scope of _class-agnostic_ in our approach is constructing a large vocabulary of concepts via a visual-textual corpus, following [37; 38; 39].

## 3 Dataset Overview

### Data Collection and Annotation

Existing object tracking datasets are typically designed for specific types of video scenes [40; 41; 42; 43; 44; 2]. To cover a diverse range of scenes, _GroOT_ was created using official videos and bounding box annotations from the MOT17 [12], TAO [13], and MOT20 [14]. The MOT17 dataset comprises 14 sequences with diverse environmental conditions such as crowded scenes, varying viewpoints, and camera motion. The TAO dataset is composed of videos from seven different datasets, such as the ArgoVerse [45] and BDD [15] datasets containing outdoor driving scenes, while LaSOT [6] and YFCC100M [46] datasets include in-the-wild internet videos. Additionally, the AVA [47], Charades [48], and HACS [49] datasets include videos depicting human-human and human-object interactions. By combining these datasets, _GroOT_ covers multiple types of scenes and encompasses a wide range of 833 objects. This diversity allows for a wide range of object classes with captions to be included, making it an invaluable resource for training and evaluating visual grounding algorithms.

We release our textual description annotations in COCO format [50]. Specifically, a new key 'captions' which is a list of strings is attached to each 'annotations' item in the official annotation. In the MOT17 subset, we attempt to maintain two types of caption for well-visible objects: one describes the _appearance_ and the other describes the _action_. For example, the caption for a well-visible person might be ['a man wearing a gray shirt', 'person walking on the street'] as shown in Fig. 2. However, 10% of tracklets only have one caption type, and 3% do not have any captions due to their low visibility. The physical characteristics of a person or their personal accessories, such as their clothing, bag color, and hair color are considered to be part of their appearance. Therefore, the appearance captions include verbs 'carrying' or 'holding' to describe personal accessories. In the TAO subset, objects other than humans have one caption describing appearance, for instance, ['a red and black scooter']. Objects that are human have the same two types of captions as the MOT17 subset. An example is shown in Fig. 1(b). These captions are consistently annotated throughout the tracklets. Fig. 3 is the word-cloud visualization of our annotations.

### _Type-to-Track_ Benchmarking Protocols

Let \(\mathbf{V}\) be a video sample lasts \(t\) frames, where \(\mathbf{V}=\left\{\mathbf{I}_{t}\ |\ t<|\mathbf{V}|\right\}\) and \(\mathbf{I}_{t}\) be the image sample at a particular time step \(t\). We define a request prompt \(\mathbf{P}\) that describes the objects of interest, and \(\mathbf{T}_{t}\) is the set of tracklets of interest up to time step \(t\). The _Type-to-Track_ paradigm requires a tracker network \(\mathcal{T}(\mathbf{I}_{t},\mathbf{T}_{t-1},\mathbf{P})\) that efficiently take into account \(\mathbf{I}_{t}\), \(\mathbf{T}_{t-1}\), and \(\mathbf{P}\) to produce \(\mathbf{T}_{t}=\mathcal{T}(\mathbf{I}_{t},\mathbf{T}_{t-1},\mathbf{P})\). To advance the task of multiple object retrieval, another benchmarking set is created in addition to the _GroOT_ dataset. While training and testing sets follow a _One-to-One_ scenario, where each caption describes a single tracklet, the new retrieval set contains prompts that follow a _One-to-Many_ scenario, where a short prompt describes multiple objects. This scenario highlights the need for diverse methods to improve the task of multiple object retrieval. The retrieval set is provided with a subset of tracklets in the TAO validation set and three custom _retrieval prompts_ that change throughout the tracking process in a video \(\{\mathbf{P}_{t_{1}=0},\mathbf{P}_{t_{2}},\mathbf{P}_{t_{3}}\}\), as depicted in Fig. 1(a). The _retrieval prompts_ are generated through a semi-automatic process that involves: (i) selecting the most commonly occurring category in the video, and (ii) cascadingly filtering to the object that appears for the longest duration. In contrast, the _caption prompts_ are created by joining tracklet captions in the scene and keeping it consistent throughout the tracking period. We name these two evaluation scenarios as _tracklet captions_ **cap** and _object retrieval_ **refer**. With three more easy-to-construct scenarios, five scenarios in total will be studied for the experiments in Section 5. Table 3 presents the statistics of the five settings, and the data portions are highlighted in the corresponding colors.

### Class-agnostic Evaluation Metrics

As indicated in [51], long-tailed classification is a very challenging task in imbalanced and large-scale datasets such as TAO. This is because it is difficult to distinguish between similar fine-grained classes, such as bus and van, due to the class hierarchy. Additionally, it is even more challenging to treat every class independently. The traditional method of evaluating tracking performance leads to inadequate benchmarking and undesired tracking results. In our _Type-to-Track_ paradigm, the main task is not to classify objects to their correct categories but to retrieve and track the object of interest. Therefore, to alleviate the negative effect, we reformulate the original per-category metrics of MOTA [52], IDF1 [53], HOTA [54] into class-agnostic metrics:

\[\text{MOTA}=\frac{1}{|CLS^{n}|}\sum_{cls}^{CLS^{n}}\left(1-\frac{\sum_{t} \left(\text{FN}_{t}+\text{FP}_{t}+\text{IDS}_{t}\right)}{\sum_{t}\text{GT}_{ t}}\right)_{cls},\text{CA-MOTA}=1-\frac{\sum_{t}\left(\text{FN}_{t}+\text{FP}_{t}+ \text{IDS}_{t}\right)_{CLS^{1}}}{\sum_{t}\left(\text{GT}_{CLS^{1}}\right)_{t}}\] (1)

\[\text{IDF1}=\frac{1}{|CLS^{n}|}\sum_{cls}^{CLS^{n}}\left(\frac{2\times\text{ IDTP}}{2\times\text{IDTP}+\text{IDFP}+\text{IDFN}}\right)_{cls},\text{CA- IDF1}=\frac{\left(2\times\text{IDTP}\right)_{CLS^{1}}}{\left(2\times\text{ IDTP}+\text{IDFP}+\text{IDFN}\right)_{CLS^{1}}}\] (2)

\[\text{HOTA}=\frac{1}{|CLS^{n}|}\sum_{cls}^{CLS^{n}}\left(\sqrt{\text{DetA} \cdot\text{AssA}}\right)_{cls},\text{CA-HOTA}=\sqrt{\left(\text{DetA}_{CLS^{1 }}\right)\cdot\left(\text{AssA}_{CLS^{1}}\right)}\] (3)

where \(CLS^{n}\) is the category, set size \(n\) is reduced to \(1\) by combining all elements: \(CLS^{n}\to CLS^{1}\).

## 4 Methodology

### Problem Formulation

Given the image \(\mathbf{I}_{t}\) and the request prompt \(\mathbf{P}\) describing the objects of interest, which can adaptively change between \(\{\mathbf{P}_{t_{1}}\), \(\mathbf{P}_{t_{2}}\), \(\mathbf{P}_{t_{3}}\}\) in the **ret** setting, and \(K\) is the prompt's length \(|\mathbf{P}|=K\), let \(enc(\cdot)\) and \(emb(\cdot)\) be the visual encoder and the word embedding model to extract features of image tokens and prompt tokens, respectively. The resulting outputs, \(enc(\mathbf{I}_{t})\in\mathbb{R}^{M\times D}\) and \(emb(\mathbf{P})\in\mathbb{R}^{K\times D}\), where \(D\) is the length of feature dimensions. A list of region-prompt associations \(\mathbf{C}_{t}\), which contains objects' bounding boxes and their confident scores, can be produced by Eqn. (4):

\[\mathbf{C}_{t}=\underset{\gamma}{dec}\Big{(}enc(\mathbf{I}_{t})\bar{\times}emb( \mathbf{P})^{\intercal},enc(\mathbf{I}_{t})\Big{)}=\Big{\{}\mathbf{c}_{i}=(c_{ x},c_{y},c_{w},c_{h},c_{conf})_{i}\mid i<M\Big{\}}_{t}\] (4)

where \((\bar{\times})\) is an operation representing the region-prompt correlation, that will be elaborated in the next section, \(\underset{\gamma}{dec}(\cdot,\cdot)\) is an object decoder taking the similarity and the image features to decode to object locations, thresholded by a scoring parameter \(\gamma\) (i.e. \(c_{conf}\geq\gamma\)). For simplicity, the cardinality of the set of objects \(|\mathbf{C}_{t}|=M\), implying each image token produces one region-text correlation.

We define \(\mathbf{T}_{t}=\Big{\{}\mathbf{tr}_{j}=(tr_{x},tr_{y},tr_{w},tr_{h},tr_{conf}, tr_{id})_{j}\mid j<N\Big{\}}_{t}\) produced by the tracker \(\mathcal{T}\), where \(N=|\mathbf{T}_{t}|\) is the cardinality of current tracklets. \(i\), \(j\), \(k\), and \(t\) are consistently denoted as indexers for objects, tracklets, prompt tokens, and time steps for the rest of the paper.

**Remark 1**_Third-order Tensor Modeling.: Since the Type-to-Track paradigm requires three input components \(\mathbf{I}_{t}\), \(\mathbf{T}_{t-1}\), and \(\mathbf{P}\), an **auto-regressive single-stage end-to-end framework** can be formulated via third-order tensor modeling._

To achieve this objective, a combination of initialization, object decoding, visual encoding, feature extraction, word embedding, and aggregation can be formulated as in Eqn. (5):

\[\mathbf{T}_{t}=\begin{cases}initialize(\mathbf{C}_{t})&t=0\\ \underset{\gamma}{dec}\Big{(}\mathbf{1}_{D\times D\times D}\times_{1}enc( \mathbf{I}_{t})\times_{2}ext(\mathbf{T}_{t-1})\times_{3}emb(\mathbf{P}),enc( \mathbf{I}_{t})\Big{)}&\forall t>0\end{cases}\] (5)

where \(ext(\cdot)\) denotes the visual feature extractor of the set of tracklets, \(ext(\mathbf{T}_{t-1})\in\mathbb{R}^{N\times D}\), \(\mathbf{1}_{D\times D\times D}\) is an all-ones tensor has size \(D\times D\times D\), ( \(\times_{n}\) ) is the \(n\)-mode product of the third-order tensor [55] to aggregate many types of token1, and \(initialize(\cdot)\) is the function to ascendingly assign unique identities to tracklets for the first time those tracklets appear.

Footnote 1: implemented by a single Python code with Numpy: np.einsum(‘ai, bj, ck -> abc’, P, I, T).

Let \(T\in\mathbb{R}^{M\times N\times K}\) be the resulting tensor \(T=\mathbf{1}_{D\times D\times D}\times_{1}enc(\mathbf{I}_{t})\times_{2}ext( \mathbf{T}_{t-1})\times_{3}emb(\mathbf{P})\). The objective function can be expressed as the log softmax of the positive region-tracklet-prompt triplet over all possible triplets, defined in Eqn. (6):

\[\theta^{\star}_{enc,ext,emb}=\arg\max_{\theta_{enc,ext,emb}}\left(\log\Big{(} \frac{\exp(T_{ijk})}{\sum_{l}^{K}\sum_{n}^{N}\sum_{m}^{M}\exp(T_{lnm})}\Big{)}\right)\] (6)

where \(\theta\) denotes the network's parameters, the combination of the \(i^{th}\) image token, the \(j^{th}\) tracklet, and the \(k^{th}\) prompt token is the correlated triplet.

In the next subsection, we elaborate our model design for the tracking function \(\mathcal{T}(\mathbf{I}_{t},\mathbf{T}_{t-1},\mathbf{P})\), named _MENDER_, as defined in Eqn. (5), and loss functions for the problem objective in Eqn. (6).

### MENDER for Multiple Object Tracking by Prompts

The correlation in Eqn. (5) has the cubic time and space complexity \(\mathcal{O}(n^{3})\), which can be intractable as the input length grows and hinder the model scalability.

**Remark 2**_Correlation Simplification.: Since both \(enc(\cdot)\) and \(ext(\cdot)\) are visual encoders, the region-prompt correlation can be equivalent to the tracklet-prompt correlation. Therefore, the region-tracklet-prompt correlation tensor \(T\) can be simplified to lower the computation footprint._

To design that goal, the extractor and encoder share network weights for computational efficiency:

\[ext(\mathbf{T}_{t-1})_{j}=ext\Big{(}\{\mathbf{tr}_{j}\}_{t-1}\Big{)}=\Big{\{} enc(\mathbf{I}_{t-1})_{i}\colon\mathbf{c}_{i}\mapsto\mathbf{tr}_{j}\Big{\}}\text{, therefore }\Big{(}(T_{ij:)_{t-1}=(T_{ii:)_{t}}\Big{)}\colon\mathbf{c}_{i}\mapsto\mathbf{tr}_{j }\lx@note{footnote}{If $\mathbf{P}$ changes, the equivalence still holds true, see Appendix for the full algorithm.}\] (7)

where \(T_{ij:}\) and \(T_{ii:}\) are lateral and horizontal slices. In layman's terms, the region-prompt correlation at the time step \(t-1\) is equivalent to the tracklet-prompt correlation at the time step \(t\), as visualized in Fig. 4(a). Therefore, one practically needs to model the region-tracklet and tracklet-promptcorrelations which reduces time and space complexity from \(\mathcal{O}(n^{3})\) to \(\mathcal{O}(n^{2})\), significantly lowering computation footprint. We alternatively rewrite the decoding step in Eqn. (5) as follows:

\[\mathbf{T}_{t}=\underset{\gamma}{dec}\Bigg{(}\Big{(}enc(\mathbf{I}_{t})\bar{ \times}ext(\mathbf{T}_{t-1})^{\intercal}\Big{)}\times\Big{(}ext(\mathbf{T}_{t -1})\bar{\times}emb(\mathbf{P})^{\intercal}\Big{)},enc(\mathbf{I}_{t})\Bigg{)} \quad\forall t>0\] (8)

**Correlation Representations.** In our approach, the correlation operation \((\bar{\times})\) is modelled by the _multi-head cross-attention_ mechanism [57], as depicted in Fig. 4(b). The attention matrix can be computed as:

\[\sigma(\mathbf{X})\bar{\times}\sigma(\mathbf{Y})=\mathcal{A}_{\mathbf{X}| \mathbf{Y}}=\mathrm{softmax}\Bigg{(}\frac{\Big{(}\sigma(\mathbf{X})\times W _{\mathcal{Q}}^{\mathbf{X}}\Big{)}\times\Big{(}\sigma(\mathbf{Y})\times W_{K} ^{\mathbf{Y}}\Big{)}^{\intercal}}{\sqrt{D}}\Bigg{)}\] (9)

where \(\mathbf{X}\) and \(\mathbf{Y}\) tokens are one of these types: region, tracklet, prompt. \(\sigma(\cdot)\) is one of the operations \(enc(\cdot)\), \(emb(\cdot)\), \(ext(\cdot)\) as the corresponding operation to \(\mathbf{X}\) or \(\mathbf{Y}\). Superscript \(W_{Q}\), \(W_{K}\), and \(W_{V}\) are the projection matrices corresponding to \(\mathbf{X}\) or \(\mathbf{Y}\) as in the attention mechanism.

Then, the attention weight from the image \(\mathbf{I}_{t}\) to the prompt \(\mathbf{P}\) are computed by the matrix multiplication for \(\mathcal{A}_{\mathbf{I}|\mathbf{T}}\) and \(\mathcal{A}_{\mathbf{T}|\mathbf{P}}\) to aggregate the information from two matrices as in Eqn. (8). The result is the matrix \(\mathcal{A}_{\mathbf{I}|\mathbf{T}\times\mathbf{T}|\mathbf{P}}=\mathcal{A}_{ \mathbf{I}|\mathbf{T}}\times\mathcal{A}_{\mathbf{T}|\mathbf{P}}\) that shows the correlation between each input or output. Then, the resulting attention matrix \(\mathcal{A}_{\mathbf{I}|\mathbf{T}\times\mathbf{T}|\mathbf{P}}\) is used to produce the object representations at time \(t\):

\[\mathbf{Z}_{t}=\mathcal{A}_{\mathbf{I}|\mathbf{T}\times\mathbf{T}|\mathbf{P}} \times\Big{(}emb(\mathbf{P})\times W_{V}^{\mathbf{P}}\Big{)}+\mathcal{A}_{ \mathbf{I}|\mathbf{T}}\times\Big{(}ext(\mathbf{T}_{t-1})\times W_{V}^{ \mathbf{T}}\Big{)}\] (10)

**Object Decoder \(dec(\cdot)\)** utilizes context-aware features \(\mathbf{Z}_{t}\) that are capable of preserving identity information while adapting to changes in position. The tracklet set \(\mathbf{T}_{t}\) is defined in the _auto-regressive_ manner to adjust to the movements of the object being tracked as in Eqn. (8). For decoding the final output at any frame, the decoder transforms the object representation by a 3-layer \(\mathrm{FFN}\) to predict bounding boxes and confidence scores for frame \(t\):

\[\mathbf{T}_{t}=\Big{\{}\mathbf{tr}_{j}=(tr_{x},tr_{y},tr_{w},tr_{h},tr_{conf})_ {j}\Big{\}}_{t}\overset{tr_{conf}\geq\gamma}{=}\mathrm{FFN}\Big{(}\mathbf{Z}_ {t}+enc(\mathbf{I}_{t})\Big{)}\] (11)

where the identification information of tracklets, represented by \(tr_{id}\), is not determined directly by the \(\mathrm{FFN}\) model. Instead, the \(tr_{id}\) value is set when the tracklet is first initialized and maintained till its end, similar to _tracking-by-attention_ approaches [4, 58, 59, 60].

Figure 4: The _auto-regressive_ manner takes advantage of the equivalent components. Simplifying the correlation in (a) turns the solution to _MENDER_ in (b), and reduces complexity to \(\mathcal{O}(n^{2})\) where \(n\) denotes the size of tokens.

### Training Losses

To achieve the training objective function as in Eqn. (6), we formulate the objective function into two loss functions \(L_{\mathbf{I}|\mathbf{T}}\) and \(L_{\mathbf{T}|\mathbf{P}}\) for correlation training and one loss \(L_{GIOU}\) for decoder training:

\[\mathcal{L}=\gamma_{\mathbf{T}|\mathbf{P}}L_{\mathbf{T}|\mathbf{P}}+\gamma_{ \mathbf{I}|\mathbf{T}}L_{\mathbf{I}|\mathbf{T}}+\gamma_{GIOU}L_{GIOU}\] (12)

where \(\gamma_{\mathbf{T}|\mathbf{P}}\), \(\gamma_{\mathbf{I}|\mathbf{T}}\), and \(\gamma_{GIOU}\) are corresponding coefficients, which are set to \(0.3\) by default.

**Alignment Loss \(L_{\mathbf{T}|\mathbf{P}}\)** is a contrastive loss, which is used to assure the alignment of the ground-truth object feature and caption pairs \((\mathbf{T},\mathbf{P})\) which can be obtained in our dataset. There are two alignment losses used, one for all objects normalized by the number of positive prompt tokens and the other for all prompt tokens normalized by the number of positive objects. The total loss can be expressed as:

\[\begin{split}& L_{\mathbf{T}|\mathbf{P}}=\\ -\frac{1}{|\mathbf{P}^{+}|}\sum_{k}^{|\mathbf{P}^{+}|}\log(\frac{ \exp\left(ext(\mathbf{T})_{j}^{\intercal}\times emb(\mathbf{P})_{k}\right)}{ \sum\limits_{l}^{K}\exp\left(ext(\mathbf{T})_{j}^{\intercal}\times emb( \mathbf{P})_{l}\right)})-\frac{1}{|\mathbf{T}^{+}|}\sum_{j}^{|\mathbf{T}^{+}|} \log(\frac{\exp\left(emb(\mathbf{P})_{k}^{\intercal}\times ext(\mathbf{T})_{j }\right)}{\sum\limits_{l}^{N}\exp\left(emb(\mathbf{P})_{k}^{\intercal}\times ext (\mathbf{T})_{l}\right)})\end{split}\] (13)

where \(\mathbf{P}^{+}\) and \(\mathbf{I}^{+}\) are the sets of positive prompts and image tokens corresponding to the selected \(enc(\mathbf{I})_{i}\) and \(emb(\mathbf{P})_{k}\), respectively.

**Objectness Losses.** To model the track's temporal changes, our network learns from training samples that capture both appearance and motion generated by two adjacent frames:

\[L_{\mathbf{I}|\mathbf{T}}=-\sum_{j}^{N}\log(\frac{\exp\left(ext(\mathbf{T})_{ j}^{\intercal}\times enc(\mathbf{I})_{i}\right)}{\sum_{l}^{N}\exp\left(ext( \mathbf{T})_{j}^{\intercal}\times enc(\mathbf{I})_{l}\right)})\quad\text{, and}\quad L_{GIOU}=\sum_{j}^{N}\ell_{GIOU}(\mathbf{tr}_{j},\mathbf{ obj}_{i})\] (14)

\(L_{\mathbf{I}|\mathbf{T}}\) is the log-softmax loss to guide the tokens' alignment as similar to Eqn. (13). In the \(L_{GIOU}\) loss, \(\mathbf{obj}_{i}\) is the ground truth object corresponding to \(\mathbf{tr}_{j}\). The optimal assignment between \(\mathbf{tr}_{j}\) or \(\mathbf{obj}_{i}\) to the ground truth object is computed efficiently by the Hungarian algorithm, following DETR [56]. \(\ell_{GIOU}\) is the Generalized IoU loss [61].

## 5 Experimental Results

### Implementation Details

**Experimental Scenarios.** We create three types of prompt: _category name_\(\mathbf{\overline{num}}\), _category synonyms_\(\mathbf{syn},\)_category definition_\(\mathbf{def}\). One _tracklet captions_\(\mathbf{cap}\) scenario is constructed by our detailed annotations and one more _objects retrieval_\(\mathbf{ref}\) scenario is given in our custom request prompts as described in Subsec. 3.2. The dataset contains 833 classes, each has a name and a corresponding set of synonyms that are different names for the same category, such as [man, woman, human, pedestrian, boy, girl, child] for person. Additionally, each category is described by a _category definition_ sentence. This definition makes the model deal with the variations in the text prompts. We join the names, synonyms, definitions, or captions and filter duplicates to construct the prompt. Trained models use as the same type as testing. We annotated the raw tracking data of the best-performant tracker (i.e., BoT-SORT [62] at 80.5% MOTA and 80.2% IDF1) at the time we constructed experiments and used it as the sub-optimal ground truth of MOT17 and MOT20 (parts _(2, 4)_ in Table 3). That is also the raw data we used to evaluate all our ablation studies.

**Datasets and Metrics.** RefCOCO+ [63] and Flickr30k [64] serve as pre-trained datasets for acquiring a vocabulary of visual-textual concepts [37]. The \(ext(\cdot)\) operation is not involved in this training step. After obtaining a pre-trained model from RefCOCO+ and Flickr30k, we train and evaluate our model for the proposed _Type-to-Track_ task on all five scenarios on our _GroOT_ dataset and the first-three scenarios for MOT20 [14]. The tracking performance is reported in class-agnostic metrics CA-MOTA, CA-IDF1, and CA-HOTA as in Subsec. 3.3 and mAP50 as defined in [13].

**Tokens Production.**\(emb(\cdot)\) utilizes RoBERTa [65] to convert the text input into a sequence of numerical tokens. The tokens are fed into the RoBERTa-base model for text encoding using a12-layer transformer network with 768 hidden units and 12 self-attention heads per layer. \(enc(\cdot)\) is implemented using a ResNet-101 [66] as the backbone to extract visual features from the input image. The output of the ResNet is processed by a Deformable DETR encoder [67] to generate visual tokens. For each dimension, we use sine and cosine functions with different frequencies as positional encodings, similar to [68]. A feature resizer combining a list of \((\mathrm{Linear},\mathrm{LayerNorm},\mathrm{Dropout})\) is used to map to size \(D=512\) for all token producers.

### Ablation Study

**Comparisons in Different Scenarios.** Table 4 shows comparisons in the performance of different prompt inputs. For MOT17 and MOT20, the _category name_ is 'person', while _category definition_ is 'a human being'. Since the prompt by _category definition_ is short, it does not differ much from the \(\overline{\mathbf{nm}}\) setting. However, the \(\overline{\mathbf{syn}}\) setting shuffles between some words, resulting in a slight decrease in CA-MOTA and CA-IDF1. The \(\overline{\mathbf{cap}}\) setting results in prompts that contain more diverse and complex vocabulary, and more context-specific information. It is more difficult for the model to accurately localize the objects and identify their identity within the image, as it needs to take into account a wider range of linguistic cues, resulting in a decrease in performance compared to **def** (59.5% CA-MOTA and 54.8% CA-IDF1 vs 67.3% CA-MOTA and 72.4% CA-IDF1 on MOT17).

For TAO, the **def** setting has a significant number of variations and many tenuous connections in the scene context, for example, 'an aircraft that has a fixed wing and is powered by propellers or jets' for the airplane category. Therefore, it results in a decrease in performance (16.8% CA-MOTA and 27.7% CA-IDF1) compared to \(\overline{\mathbf{cap}}\) (20.7% CA-MOTA and 32.0% CA-IDF1), because the \(\overline{\mathbf{cap}}\) setting is more specific on the object level than category level. The best performant setting is \(\overline{\mathbf{nm}}\) (27.3% CA-MOTA and 37.2% CA-IDF1), where names are combined.

**Simplied Attention Representations.** Table 4 also presents the effectiveness of different attention representations of the full tensor \(T\) (denoted by \(\mathcal{K}\)) and the simplified correlation (denoted by \(\mathcal{V}\)). The performance is reported with frame per second (FPS), which is self-measured on one GPU NVIDIA RTX 3060 12GB. Overall, the performance of simplified correlation is witnessed with a superior speed of up to 2\(\times\) (7.8 FPS vs 3.4 FPS of \(\overline{\mathbf{cap}}\) on MOT17 and 11.5 FPS vs 7.6 FPS of \(\overline{\mathbf{retr}}\) on TAO), resulting in and a slight increase in accuracy due to attention stability and precision gain.

\begin{table}
\begin{tabular}{l|c|c c c c c c} \hline \hline
**P** & **sin** & **CA-MOTA** & **CA-IDF1** & **MT** & **IDs** & **mAP** & **FPS** \\ \hline \hline
**min** & \(\mathcal{K}\)/ & 67.0 & 71.20 & 544 & 1352 & 0.876 & 10.3 \\ \hline
**syn** & \(\mathcal{K}\)/ & 65.10 & 71.10 & 554 & 1348 & 0.874 & 10.3 \\ \hline
**def** & \(\mathcal{K}\) & 67.00 & 72.10 & 556 & 1343 & 0.876 & 5.8 \\ \hline
**def** & \(\mathcal{K}\) & **67.30** & **72.40** & **568** & **1322** & **0.877** & **10.3** \\ \hline
**sign** & \(\mathcal{K}\) & 58.20 & 53.20 & **289** & **171** & 0.644 & 3.4 \\  & \(\mathcal{V}\) & **59.50** & **54.80** & 201 & **1734** & **0.688** & **7.8** \\ \hline \hline
**content** & \(\mathcal{K}\)/ & 72.00 & 3596 & & & & \\ \hline
**sign** & \(\mathcal{K}\)/ & 27.30 & 32.05 & 3253 & 4284 & 0.212 & 11.2 \\ \hline
**sign** & \(\mathcal{K}\)/ & 25.70 & 36.10 & 3122 & 5048 & 0.189 & 11.2 \\ \hline
**def** & \(\mathcal{K}\) & 15.20 & 27.30 & 2452 & 6235 & 0.154 & 6.2 \\  & \(\mathcal{K}\) & **27.70** & **2547** & **6118** & **0.188** & **10.5** \\ \hline
**sign** & \(\mathcal{K}\)/ & 20.30 & 31.80 & 2043 & 522 & 0.188 & 4.3 \\  & \(\mathcal{V}\) & **20.70** & **32.00** & **3103** & **5192** & **0.184** & **8.7** \\ \hline
**br

### Comparisons with A Baseline Design

Due to the new proposed topic, no current work has the same scope or directly solves our problem. Therefore, we compare our proposed _MENDER_ against a two-stage baseline tracker in Table 5. We use current SOTA methods to develop this approach, i.e., MDETR [36] for the grounded detector, while TrackFormer [4] for the object tracker. It is worth noting that our _MENDER_ relies on direct regression to locate and track the object of interest, without the need for an explicit grounded object detection stage. Table 5 shows our proposed _MENDER_ outperforms the baseline on both CA-MOTA and CA-IDF1 metrics in all four settings _category synonyms_, _category definition_, _tracklet captions_ and _object retrieval_ (25.7% vs. 21.3%, 16.8% vs. 14.6%, 20.7% vs. 15.3% and 32.9% vs. 25.7% CA-MOTA on TAO), while can maintain up to \(4\times\) run-time speed (10.3 FPS vs 2.2 FPS). The results indicate that training a single-stage network enhances efficiency and reduces errors by avoiding separate feature extractions for both detection and tracking steps.

### Comparisons with State-of-the-Art Approaches

The _category name_\(\overline{\text{mm}}\) setting is also the official MOT benchmark. Table 6 is the comparison of our result on the _category name_ setting on the official leaderboard of MOT17, compared with other state-of-the-art approaches, including ByteTrack [69] and TrackFormer [4]. Note that our proposed _MENDER_ is one of the first attempts at the Grounded MOT task, not to achieve the top rankings on the general MOT leaderboard. In contrast, other SOTA approaches benefit from the efficient single-category design in their separate object detectors, while our single-stage design is agnostic to the category and for flexible textual input. Compared to TrackFormer [4], our proposed _MENDER_ only demonstrates a marginal decrease in identity assignment (67.1% vs 68.0% CA-IDF1). The decrease in the CA-MOTA stems from our detector's design which integrates flexible input.

## 6 Conclusion

We have presented a novel problem of _Type-to-Track_, which aims to track objects using natural language descriptions instead of bounding boxes or categories, and a large-scale dataset to advance this task. Our proposed _MENDER_ model reduces the computational complexity of third-order correlations by designing an efficient attention method that scales quadratically w.r.t the input sizes. Our experiments on three datasets and five scenarios demonstrate that our model achieves state-of-the-art accuracy and speed for class-agnostic tracking.

**Limitations.** While our proposed metrics effectively evaluate the proposed _Type-to-Track_ problem, they may not be ideal for measuring precision-recall characteristics in retrieval tasks. Additionally, the lack of the question-answering task in data and problem formulation may limit the algorithm to not being able to provide language feedback such as clarification or alternative suggestions. Additional benchmarks incorporating question-answering are excellent research avenues for future work. While the performance of our proposed _MENDER_ may not be optimal for well-defined categories, it paves the way for exploring new avenues in open vocabulary and open-world scenarios [74].

**Broader Impacts.** The _Type-to-Track_ problem and the proposed _MENDER_ model have the potential to impact various fields, such as surveillance and robotics, where recognizing object interactions is a crucial task. By reformulating the problem with text support, the proposed methodology can improve the intuitiveness and responsiveness of tracking, making it more practical for video input support in large-language models [75] and real-world applications similar to ChatGPT. However, it could bring potential negative impacts related to human rights by providing a video retrieval system via text.

\begin{table}
\begin{tabular}{l|c|c c c c c c c c c} \hline \hline
**Approach** & **Cls-agn** & **CA-IDF1** & **CA-MOTA** & **CA-HOTA** & **MT** & **ML** & **AssA** & **DetA** & **LocA** & **IDs** \\ \hline ByteTrack [69] & ✗ & 77.3 & 80.3 & 63.1 & 957 & 516 & 52.7 & 55.6 & 81.8 & 3,378 \\ TrackFormer [4] & ✗ & 68.0 & 74.1 & 57.3 & 1,113 & 246 & 54.1 & 60.9 & 82.8 & 2,829 \\ QuasiDense [70] & ✗ & 66.3 & 68.7 & 53.9 & 957 & 516 & 52.7 & 55.6 & 81.8 & 3,378 \\ CenterTrack [71] & ✗ & 64.7 & 67.8 & 52.2 & 816 & 579 & 51.0 & 53.8 & 81.5 & 3,039 \\ TraDeS [72] & ✗ & 63.9 & 69.1 & 52.7 & 858 & 507 & 50.8 & 55.2 & 81.8 & 3,555 \\ CTracker [73] & ✗ & 57.4 & 66.6 & 49.0 & 759 & 570 & 45.2 & 53.6 & 81.3 & 5,529 \\ \hline
**MENDER** & ✓ & 67.1 & 65.0 & 53.9 & 678 & 648 & 54.4 & 53.6 & 83.4 & 3,266 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparisons to the state-of-the-art approaches on the _category name_

**Acknowledgment.** This work is partly supported by NSF Data Science, Data Analytics that are Robust and Trusted (DART), and Google Initiated Research Grant. We also thank Utsav Prabhu and Chi-Nhan Duong for their invaluable discussions and suggestions and acknowledge the Arkansas High-Performance Computing Center for providing GPUs.

## References

* [1]P. Nguyen, T. Truong, M. Huang, Y. Liang, N. Le, and K. Luu (2022) Self-supervised domain adaptation in crowd counting. In 2022 IEEE International Conference on Image Processing (ICIP), pp. 2786-2790. Cited by: SS1.
* [2]K. G. Quach, P. Nguyen, H. Le, T. Truong, C. N. Duong, M. Tran, and K. Luu (2021) Dyglip: a dynamic graph model with link prediction for accurate multi-camera multiple object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13784-13793. Cited by: SS1.
* [3]K. G. Quach, H. Le, P. Nguyen, C. N. Duong, T. Dai Bui, and K. Luu (2022) Depth perspective-aware multiple object tracking. arXiv preprint arXiv:2207.04551. Cited by: SS1.
* [4]T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer (2022) Trackformer: multi-object tracking with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8844-8854. Cited by: SS1.
* [5]P. Nguyen, K. G. Quach, J. Gauch, S. U. Khan, B. Raj, and K. Luu (2023) Utopia: unconstrained tracking objects without preliminary examination via cross-domain adaptation. arXiv preprint arXiv:2306.09613. Cited by: SS1.
* [6]H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai, Y. Xu, C. Liao, and H. Ling (2019) Lasot: a high-quality benchmark for large-scale single object tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5374-5383. Cited by: SS1.
* [7]X. Wang, X. Shu, Z. Zhang, B. Jiang, Y. Wang, Y. Tian, and F. Wu (2021-06) Towards more flexible and accurate object tracking with natural language: algorithms and benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13763-13773. Cited by: SS1.
* [8]Y. Wu, J. Lim, and M. Yang (2015) Object tracking benchmark. IEEE Transactions on Pattern Analysis and Machine Intelligence37 (9), pp. 1834-1848. Cited by: SS1.
* [9]M. Kristan, J. Matas, A. Leonardis, T. Vojir, R. Pflugfelder, G. Fernandez, G. Nebehay, F. Porikli, and L. Cehovin (2016-Nov) A novel performance evaluation methodology for single-target trackers. IEEE Transactions on Pattern Analysis and Machine Intelligence38 (11), pp. 2137-2155. Cited by: SS1.
* [10]L. Huang, X. Zhao, and K. Huang (2019) Got-10k: a large high-diversity benchmark for generic object tracking in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence. Cited by: SS1.
* [11]M. Muller, A. Bibi, S. Giancola, S. Alsubahi, and B. Ghanem (2018) TrackingNet: a large-scale dataset and benchmark for object tracking in the wild. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 300-317. Cited by: SS1.
* [12]A. Milan, L. Leal-Taixe, I. Reid, S. Roth, and K. Schindler (2016-05) MOT16: a benchmark for multi-object tracking. arXiv:1603.00831 [cs]. Cited by: SS1.
* [13]A. Dave, T. Khurana, P. Tokmakov, C. Schmid, and D. Ramanan (2020) Tao: a large-scale benchmark for tracking any object. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V 16, pp. 436-454. Cited by: SS1.
* [14]P. Dendorfer, H. Rezatofighi, A. Milan, J. Shi, D. Cremers, I. Reid, S. Roth, K. Schindler, and L. Leal-Taixe (2020) Mot20: a benchmark for multi object tracking in crowded scenes. arXiv preprint arXiv:2003.09003. Cited by: SS1.
* [15]F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell (2020-06) BDD100K: a diverse driving dataset for heterogeneous multitask learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1.
* [16]A. Vaswani, A. Shazeer, J. Uszkoreit, A. Kavukcuoglu, and A. Courville (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1020. Cited by: SS1.
* [17]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [18]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [19]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [20]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [21]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [22]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [23]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [24]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [25]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [26]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [27]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [28]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [29]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1119. Cited by: SS1.
* [30]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1038. Cited by: SS1.
* [31]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1190. Cited by: SS1.
* [32]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1190. Cited by: SS1.
* [33]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1119. Cited by: SS1.
* [34]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1119. Cited by: SS1.
* [35]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1190. Cited by: SS1.
* [36]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1119. Cited by: SS1.
* [37]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1119. Cited by: SS1.
* [38]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1119. Cited by: SS1.
* [39]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1119. Cited by: SS1.
* [40]A. Vaswani, A. Shazeer, and J. Uszkoreit (2017) Attention-based deep neural networks for large-scale object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1018-1119. Cited by* [16] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In _Computer Vision-ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2-6, 2018, Revised Selected Papers, Part IV 14_, pages 123-141. Springer, 2019.
* [17] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with a large-scale benchmark. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XV 16_, pages 208-223. Springer, 2020.
* [18] Dongming Wu, Wencheng Han, Tiancai Wang, Xingping Dong, Xiangyu Zhang, and Jianbing Shen. Referring multi-object tracking. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14633-14642, 2023.
* [19] Yi Wu, Jongwoo Lim, and Ming Hsuan Yang. Object tracking benchmark. _IEEE Transactions on Pattern Analysis & Machine Intelligence_, 37(9):1834, 2015.
* [20] Pengpeng Liang, Erik Blasch, and Haibin Ling. Encoding color information for visual tracking: Algorithms and benchmark. _IEEE Transactions on Image Processing_, 24(12):5630-5644, 2015.
* [21] A Li, M Lin, Y Wu, MH Yang, and S Yan. NUS-PRO: A New Visual Tracking Challenge. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 38(2):335-349, 2016.
* [22] Siyi Li and Dit-Yan Yeung. Visual object tracking for unmanned aerial vehicles: A benchmark and new motion models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31, 2017.
* [23] Hamed Kiani Galoogahi, Ashton Fagg, Chen Huang, Deva Ramanan, and Simon Lucey. Need for speed: A benchmark for higher frame rate object tracking. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 1125-1134, 2017.
* [24] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, and Vincent Vanhoucke. Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in video. In _proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5296-5305, 2017.
* [25] Jack Valmadre, Luca Bertinetto, Joao F Henriques, Ran Tao, Andrea Vedaldi, Arnold WM Smeulders, Philip HS Torr, and Efstratios Gavves. Long-term tracking in the wild: A benchmark. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 670-685, 2018.
* [26] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. _The International Journal of Robotics Research_, 32(11):1231-1237, 2013.
* [27] Zhengyuan Yang, Tushar Kumar, Tianlang Chen, Jingsong Su, and Jiebo Luo. Grounding-tracking-integration. _IEEE Transactions on Circuits and Systems for Video Technology_, 31(9):3433-3443, 2020.
* [28] Haojie Zhao, Xiao Wang, Dong Wang, Huchuan Lu, and Xiang Ruan. Transformer vision-language tracking via proxy token guided cross-modal fusion. _Pattern Recognition Letters_, 2023.
* [29] Fenglin Liu, Xian Wu, Shen Ge, Xuancheng Ren, Wei Fan, Xu Sun, and Yuexin Zou. Dimbert: learning vision-language grounded representations with disentangled multimodal-attention. _ACM Transactions on Knowledge Discovery from Data (TKDD)_, 16(1):1-19, 2021.
* [30] Wenqiao Zhang, Haochen Shi, Siliang Tang, Jun Xiao, Qiang Yu, and Yueting Zhuang. Consensus graph representation learning for better grounded image captioning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 3394-3402, 2021.
* [31] Wenhui Jiang, Minwei Zhu, Yuming Fang, Guangming Shi, Xiaowei Zhao, and Yang Liu. Visual cluster grounding for image captioning. _IEEE Transactions on Image Processing_, 31:3920-3934, 2022.
* [32] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. _arXiv preprint arXiv:2212.00280_, 2022.
* [33] Haonan Yu, Haichao Zhang, and Wei Xu. Interactive grounded language acquisition and generalization in a 2d world. In _International Conference on Learning Representations_, 2018.
* [34] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_, pages 787-798, 2014.

* [35] Yihao Li, Jun Yu, Zhongpeng Cai, and Yuwen Pan. Cross-modal target retrieval for tracking by natural language. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4931-4940, 2022.
* [36] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mcdert-modulated detection for end-to-end multi-modal understanding. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1780-1790, 2021.
* [37] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14393-14402, 2021.
* [38] Muhammad Maaz, Hanoona Bangalath Rasheed, Salman Hameed Khan, Fahad Shahbaz Khan, Rao Muhammad Anwer, and Ming-Hsuan Yang. Multi-modal transformers excel at class-agnostic object detection. _arXiv_, 2021.
* [39] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and Derek Hoiem. Towards general purpose vision systems: An end-to-end task-agnostic vision-language architecture. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16399-16409, 2022.
* [40] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 DAVIS Challenge on Video Object Segmentation. _arXi_, 2017.
* [41] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In _ICCV_, 2019.
* [42] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark. _arXiv_, 2018.
* [43] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, and Song Bai. Occluded video instance segmentation: A benchmark. _International Journal of Computer Vision_, 130(8):2022-2039, 2022.
* [44] Namdar Homayounfar, Justin Liang, Wei-Chiu Ma, and Raquel Urtasun. Videoclick: Video object segmentation with a single click. _arXiv preprint arXiv:2101.06545_, 2021.
* [45] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d tracking and forecasting with rich maps. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8748-8757, 2019.
* [46] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. _Communications of the ACM_, 59(2):64-73, 2016.
* [47] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A video dataset of spatio-temporally localized atomic visual actions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6047-6056, 2018.
* [48] Gunnar A Sigurdsson, Gul Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood noes: Crowdsourcing data collection for activity understanding. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I 14_, pages 510-526. Springer, 2016.
* [49] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips and segments dataset for recognition and temporal localization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8668-8678, 2019.
* [50] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.
* [51] Siyuan Li, Martin Danelljan, Henghui Ding, Thomas E Huang, and Fisher Yu. Tracking every thing in the wild. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022. Proceedings, Part XXII_, pages 498-515. Springer, 2022.
* [52] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear mot metrics. _EURASIP Journal on Image and Video Processing_, 2008:1-10, 2008.

* [53] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In _European conference on computer vision_, pages 17-35. Springer, 2016.
* [54] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taixe, and Bastian Leibe. Hota: A higher order metric for evaluating multi-object tracking. _International journal of computer vision_, 129(2):548-578, 2021.
* [55] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. _SIAM review_, 51(3):455-500, 2009.
* [56] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I 16_, pages 213-229. Springer, 2020.
* [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [58] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. In _European Conference on Computer Vision (ECCV)_, 2022.
* [59] Pha Nguyen, Kha Gia Quach, Chi Nhan Duong, Son Lam Phung, Ngan Le, and Khoa Luu. Multi-camera multi-object tracking on the move via single-stage global association approach. _arXiv preprint arXiv:2211.09663_, 2022.
* [60] Pha Nguyen, Kha Gia Quach, Chi Nhan Duong, Ngan Le, Xuan-Bac Nguyen, and Khoa Luu. Multi-camera multiple 3d object tracking on the move for autonomous vehicles. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2569-2578, 2022.
* [61] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union. In _The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2019.
* [62] Nir Aharon, Roy Orfag, and Ben-Zion Bobrovsky. Bot-sort: Robust associations multi-pedestrian tracking. _arXiv preprint arXiv:2206.14651_, 2022.
* [63] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14_, pages 69-85. Springer, 2016.
* [64] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. _IJCV_, 123(1):74-93, 2017.
* [65] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [66] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [67] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable {detr}: Deformable transformers for end-to-end object detection. In _International Conference on Learning Representations_, 2021.
* [68] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8741-8750, 2021.
* [69] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. In _Proceedings of the European Conference on Computer Vision (ECCV)_, 2022.
* [70] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasi-dense similarity learning for multiple object tracking. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 164-173, 2021.

* [71] Xingyi Zhou, Vladlen Koltun, and Philipp Krahenbuhl. Tracking objects as points. In _European Conference on Computer Vision_, pages 474-490. Springer, 2020.
* [72] Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and Junsong Yuan. Track to detect and segment: An online multi-object tracker. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12352-12361, 2021.
* [73] Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu. Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV 16_, pages 145-161. Springer, 2020.
* [74] Siyuan Li, Tobias Fischer, Lei Ke, Henghui Ding, Martin Danelljan, and Fisher Yu. Ovtrack: Open-vocabulary multiple object tracking. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5567-5577, 2023.
* [75] OpenAI. Gpt-4 technical report. _arXiv_, 2023.