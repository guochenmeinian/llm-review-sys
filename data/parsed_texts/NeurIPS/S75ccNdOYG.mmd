# Posterior Sampling for Competitive RL: Function Approximation and Partial Observation

 Shuang Qiu

HKUST

masqiu@ust.hk

&Ziyu Dai

New York University

zd2365@cims.nyu.edu

&Han Zhong

Peking University

hanzhong@stu.pku.edu.cn

&Zhaoran Wang

Northwestern University

zhaoranwang@gmail.com

&Zhuoran Yang

Yale University

zhuoran.yang@yale.edu

&Tong Zhang

HKUST

tongzhang@ust.hk

Equal Contribution.

###### Abstract

This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms that can scale sublinearly with the proposed GEC and the number of episodes \(T\). To the best of our knowledge, we for the first time develop generic model-based posterior sampling algorithms for competitive RL that can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning.

## 1 Introduction

Multi-agent reinforcement learning (MARL) tackles sequential decision-making problems where multiple players simultaneously interact with the shared environment, affecting each other's behavior in a coupled manner. Under a competitive reinforcement learning (RL) setting, the goal of each player is to maximize (_resp._ minimize) her own cumulative gains (_resp._ losses) in the presence of other agents. Recent years have been tremendous practical successes of MARL in a variety of application domains, such as autonomous driving [56], Go [58], StarCraft [65], Dota2 [8] and Poker [10]. These successes are attributed to advanced MARL algorithms that can coordinate multiple players by exploiting potentially partial observations of the latent states and employ powerful function approximators (neural networks in particular), which empower us to tackle practical problems with large state spaces.

Apart from the empirical success, there is a growing body of literature on establishing theoretical guarantees for Markov games (MGs) [57] - a standard framework for describing the dynamics of competitive RL. In particular, [71, 16, 33, 28, 77] extend the works in single-agent reinforcementlearning (RL) with function approximation [29; 60; 67; 34; 5; 11; 19; 31; 21] by developing sample-efficient algorithms that are capable to solve two-player zero-sum MGs with function approximation. In addition, as opposed to the aforementioned literature on MGs assuming the state of players is fully observable, the recent work [41] analyze Markov games under partial observability [41], i.e., the complete information about underlying states is lacking. However, most of the existing works are built upon the principle of "optimism in the face of uncertainty" (OFU) [38] for exploration. Furthermore, from a practical perspective, achieving optimism often requires explicit construction of bonus functions, which are often designed in a model-specific fashion and computationally challenging to implement.

Another promising strand of exploration techniques is based on posterior sampling, which is shown by previous works on bandits [12] and RL [48] to perform better than OFU-based algorithms. Meanwhile, posterior sampling methods, unlike OFU-based algorithms [31; 21] that need to solve complex optimization problems to achieve optimism, can be efficiently implemented by ensemble approximations [48; 44; 17; 46] and stochastic gradient Langevin dynamics (SGLD) [70]. Despite the superiority of posterior sampling, its theoretical understanding in MARL remains limited. The only exception is [73], which proposes a model-free posterior sampling algorithm for zero-sum MGs with general function approximation. However, [73] cannot capture some common tractable competitive RL models with a model-based nature, such as linear mixture MGs [16] and low witness rank MGs [28]. Moreover, their result is restricted to the fully observable MGs without handling the partial observability of the players' states. Therefore, we raise the following question:

_Can we design provably sample-efficient posterior sampling algorithms for competitive RL with even partial observations under general function approximation?_

Concretely, the above question poses three major challenges. First, despite the success of the OFU principle in partially observable Markov games (POMGs), it remains elusive how to incorporate the partial observations into the posterior sampling framework under a MARL setting with provably efficient exploration. Second, it is also unclear whether there is a generic function approximation condition that can cover more known classes in both full and partial observable MARL and is meanwhile compatible with the posterior sampling framework. Third, with the partial observation and function approximation, it is challenging to explore how we can solve MGs under the setups of self-play, where all players can be coordinated together, and adversarial learning, where the opponents' policies are adversarial and uncontrollable by the learner. Our work takes an initial step towards tackling such challenges by concentrating on the typical competitive RL scenario, the two-player zero-sum MG, and proposing statistically efficient posterior sampling algorithms under function approximation that can solve both self-play and adversarial MGs with full and partial observations.

**Contributions.** Our contributions are four-fold: **(1)** We first propose the two generalized eluder coefficient (GEC) as the complexity measure for the competitive RL with function approximation, namely self-play GEC and adversarial GEC, that captures the exploration-exploitation tradeoff in many existing MGs, including linear MGs, linear mixture MGs, weakly revealing POMGs, decodable POMGs. The proposed measures also generalize the recently developed GEC condition [83] from single-agent RL to MARL, suitably adjusting the exploration policy particularly for the adversarial setting. **(2)** Incorporating the proposed self-play GEC for general function approximation, we propose a model-based posterior sampling algorithm with self-play to learn the Nash equilibrium (NE), which successfully handles the partial observability of states along with a full observable setup by carefully designed likelihood functions. **(3)** We identify POMG models aligned with the form of the adversarial GEC, which fit MG learning with adversarially-varying policies of the opponent. We further propose a model-based posterior sampling algorithm for adversarial learning with general function approximation. **(4)** We prove regret bounds for our proposed algorithms that scale sublinearly with the number of episodes \(T\), the corresponding GEC \(d_{\mathrm{GEC}}\), and a quantity measuring the coverage of the optimal model by the initial model sampling distribution. To the best of our knowledge, we present the first model-based posterior sampling approaches to sample-efficiently learn MGs with function approximation, handling partial observability in both self-play and adversarial settings.

**Related Works.** There is a large body of literature studying MGs, especially zero-sum MGs. In the self-play setting, many papers have focused on solving approximate NE in tabular zero-sum MGs [6; 7; 71; 51; 43], zero-sum MGs with linear function approximation [71; 16], zero-sum MGs with low-rank structures [50; 79; 47], and zero-sum MGs with general function approximation [33; 28; 73]. On the other hand, there are also several recent papers focusing on the adversarial setting [71; 61; 33; 28] aim to learn the Nash value under the setting of unrevealed opponent's policies, where the adversarial policies of the opponent are unobservable. In addition, another line of adversarial MGs concentrates on a revealed policy setting, where the opponent's full policy can be observed, leading to efficiently learning a sublinear regret comparing against the best policy in hindsight. Particularly, [42] and [77] develop efficient algorithms in tabular and function approximation settings, respectively. Our approach focuses on the unrevealed policy setting, which is considered to be a more practical setup. There are also works studying MGs from various aspects [59; 32; 45; 84; 35; 20; 52; 9; 82; 18; 72; 74], such as multi-player general-sum MGs, reward-free MGs, MGs with delayed feedback, and offline MGs, which are beyond the scope of our work. Most of the aforementioned works follow the OFU principle and differ from our posterior sampling methods. The recent work [73] proposes a model-free posterior sampling algorithm for two-player zero-sum MGs but is limited to the self-play setting with fully observable states. Moreover, their work requires a strong Bellman completeness assumption that is restrictive compared to only requiring realizability in our work, mainly due to the monotonicity, meaning that adding a new function to the function class may violate it. Many model-based models like linear mixture MGs [16] and low witness rank MGs [28] are not Bellman-complete, so they cannot be captured by [73]. Without the completeness assumption, our model-based posterior sampling approaches can solve a rich class of tractable MGs, including linear mixture MGs, low witness rank MGs [28], and even POMGs, tackling both self-play and adversarial learning settings.

Our work is related to a line of research on posterior sampling methods in RL. For single-agent RL, most existing works such as [54] analyze the Bayesian regret bound. There are also some works [4; 53; 76] focusing on the frequentist (worst-case) regret bound. Our work is more closely related to the recently developed feel-good Thompson sampling technique proposed by [81] for the frequentist regret bound, and its extension to single-agent RL [19; 2; 3] and two-player zero-sum MGs [73].

Our work is also closely related to the line of research on function approximation in RL. Such a line of works proposes algorithms for efficient policy learning under diverse function approximation classes, spanning from linear Markov decision processes (MDPs) [34], linear mixture MDPs [5; 85] to nonlinear and general function classes, including, for instance, generalized linear MDPs [69], kernel and neural function classes [75], bounded eluder dimension [49; 68], Bellman rank [29], witness rank [60], bellman eluder dimension [31], bilinear [21], decision-estimation coefficient [24; 14], decoupling coefficient [19], admissible Bellman characterization [15], and GEC [83] classes.

The research on partial observability in RL [27] is closely related to our work. The works [36; 30] show that learning history-dependent policies generally can cause an exponential sample complexity. Thus, many recent works focus on analyzing tractable subclasses of partially observable Markov decision processes (POMDPs), which includes weakly revealing POMDPs [30; 39], observable POMDPs [26; 25], decodable POMDPs [22; 23], low-rank POMDPs[66], regular PSR [78], PO-bilinear class [63], latent MDP with sufficient tests [37], B-stable PSR [13], well-conditioned PSR [40], POMDPs with deterministic transition kernels [30; 62], and GEC [83]. Nevertheless, in contrast to our work which focuses on the two-player competitive setting with partial observation, these papers merely consider the single-agent setting. The recent research [41] further generalizes weakly revealing POMDPs to its multi-agent counterpart, weakly revealing POMGs, in a general-sum multi-player setting based on the OFU principle. But when specialized to the two-player case, our work proposes a general function class that can subsume the class of weakly revealing POMGs as a special case. It would be intriguing to generalize our framework to the general-sum settings in the future.

**Notations.** We denote by \(\mathrm{KL}(P||Q)=\mathbb{E}_{x\sim P}[\log(\mathrm{d}P(x)/\mathrm{d}Q(x))]\) the KL divergence and \(D^{2}_{\mathrm{He}}(P,Q)=1/2\cdot\mathbb{E}_{x\sim P}(\sqrt{\mathrm{d}Q(x)/ \mathrm{d}P(x)}-1)^{2}\) the Hellinger distance. We denote by \(\Delta_{\mathcal{X}}\) the set of all distributions over \(\mathcal{X}\) and \(\mathrm{Unif}(\mathcal{X})\) the uniform distribution over \(\mathcal{X}\). We let \(x\wedge y\) be \(\min\{x,y\}\).

## 2 Problem Setup

We introduce the basic concept of the two-player zero-sum Markov game (MG), function approximation, and the new complexity conditions for function approximation. Concretely, we study two typical classes of MGs, i.e., fully observable MGs and partially observable MGs, as defined below.

**Fully Observable Markov Game.** We consider an episodic two-player zero-sum fully observable Markov game (FOMG2) specified by a tuple \((\mathcal{S},\mathcal{A},\mathcal{B},\mathbb{P},r,H)\), where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) and \(\mathcal{B}\) are the action spaces of Players 1 and 2 respectively, \(H\) is the length of the episode. We denote by \(\mathbb{P}:=\{\mathbb{P}_{h}\}_{h=1}^{H}\) the transition kernel with \(\mathbb{P}_{h}(s^{\prime}|s,a,b)\) specifying the probability (density) of transitioning from state \(s\) to state \(s^{\prime}\) given Players 1 and 2's actions \(a\in\mathcal{A}\) and \(b\in\mathcal{B}\) at step \(h\). We denote the reward function as \(x=\{r_{h}\}_{h=1}^{H}\) with \(r_{h}:\mathcal{S}\times\mathcal{A}\times\mathcal{B}\mapsto[0,1]\) being the reward received by players at step \(h\). We define \(\pi=\{\pi_{h}\}_{h=1}^{H}\) and \(\nu=\{\nu_{h}\}_{h=1}^{H}\) as _Markovian_ policies for Players 1 and 2, i.e., \(\pi_{h}(a|s)\) and \(\nu_{h}(b|s)\) are the probability of taking action \(a\) and \(b\) conditioned on the current state \(s\) at step \(h\). Without loss of generality, we assume the initial state \(s_{1}\) is fixed for each episode. We consider a realistic setting where the transition kernel \(\mathbb{P}\) is _unknown_ and thereby needs to be approximated using the collected data.

**Partially Observable Markov Game.** This paper further studies an episodic zero-sum partially observable Markov game (POMG), which is distinct from the FOMG setup in that the state \(s\) is not directly observable. In particular, a POMG is represented by a tuple \((\mathcal{S},\mathcal{A},\mathcal{B},\mathcal{O},\mathbb{P},\mathbb{O},\mu, r,H)\), where \(\mathcal{S}\), \(\mathcal{A}\), \(\mathcal{B}\), \(H\), and \(\mathbb{P}\) are similarly the state and action spaces, the episode length, and the transition kernel. Here \(\mu_{1}(\cdot)\) denotes the initial state distribution. We denote by \(\mathbb{O}:=\{\mathbb{O}_{h}\}_{h=1}^{H}\) the emission kernel so that \(\mathbb{O}_{h}(o|s)\) is the probability of having a partial observation \(o\in\mathcal{O}\) at state \(s\) with \(\mathcal{O}\) being the observation space. Since we only have an observation \(o\) of a state, the reward function is defined as \(r:=\{r_{h}\}_{h=1}^{H}\) with \(r_{h}(o,a,b)\in[0,1]\) depending on actions \(a,b\) and the observation \(o\), and the policies for players are defined as \(\pi=\{\pi_{h}\}_{h=1}^{H}\) and \(\nu=\{\nu_{h}\}_{h=1}^{H}\), where \(\pi_{h}(a_{h}|\tau_{h-1},o_{h})\) and \(\nu_{h}(b_{h}|\tau_{h-1},o_{h})\) is viewed as the probability of taking actions \(a_{h}\) and \(b_{h}\) depending on all histories \((\tau_{h-1},o_{h})\). Here we let \(\tau_{h}:=(o_{1},a_{1},b_{1}\ldots,o_{h},a_{h},b_{h})\). Then, in contrast to FOMGs, the policies in POMGs are _history-dependent_, defined on all prior observations and actions rather than the current state \(s\). We define \(\mathbf{P}_{h}^{\pi,\nu}(\tau_{h}):=\int_{\mathcal{S}^{h}}\mu_{1}(s_{1})\prod _{h^{\prime}=1}^{h-1}[\mathbb{O}_{h^{\prime}}(o_{h^{\prime}}|s_{h^{\prime}}) \pi_{h^{\prime}}(b_{h^{\prime}}|\tau_{h^{\prime}-1},o_{h^{\prime}})\nu_{h^{ \prime}}(b_{h^{\prime}}|\tau_{h^{\prime}-1},o_{h^{\prime}})\mathbb{P}_{h^{ \prime}}(s_{h^{\prime}+1}|s_{h^{\prime}},a_{h^{\prime}},b_{h^{\prime}})] \mathbb{O}_{h}(o_{h}|s_{h})\mathrm{d}s_{1:h}\), which is the joint distribution of \(\tau_{h}\) under the policy pair \((\pi,\nu)\). Removing policies in \(\mathbf{P}_{h}^{\pi,\nu}\), we define the function \(\mathbf{P}_{h}(\tau_{h}):=\int_{\mathcal{S}^{h}}\mu_{1}(s_{1})\prod_{h^{\prime }=1}^{h-1}[\mathbb{O}_{h^{\prime}}(o_{h^{\prime}}|s_{h^{\prime}})\mathbb{P}_{ h^{\prime}}(s_{h^{\prime}+1}|s_{h^{\prime}},a_{h^{\prime}},b_{h^{\prime}})] \mathbb{O}_{h}(o_{h}|s_{h})\mathrm{d}s_{1:h}\). We assume that the parameters \(\theta:=(\mu_{1},\mathbb{P},\mathbb{O})\) are _unknown_ and thus \(\mathbf{P}_{h}\) is _unknown_ as well, which should be approximated in algorithms via online interactions.

**Online Interaction with the Environment.** In POMGs, at step \(h\) of episode \(t\) of the interaction, players take actions \(a_{h}^{t}\sim\pi_{h}^{t}(\cdot|\tau_{h-1}^{t},o_{h}^{t})\) and \(b_{h}^{t}\sim\nu_{h}^{t}(\cdot|\tau_{h-1}^{t},b_{h-1}^{t},o_{h}^{t})\) depending on their action and observation histories, receiving a reward \(r_{h}(o_{h}^{t},a_{h}^{t},b_{h}^{t})\) and transitions from the latent state \(s_{h}^{t}\) to \(s_{h+1}^{t}\sim\mathbb{P}_{h}(\cdot\mid s_{h}^{t},a_{h}^{t},b_{h}^{t})\) with an observation \(o_{h+1}^{t}\sim\mathbb{O}_{h}(\cdot|s_{h}^{t})\) generated. When the underlying state \(s_{h}^{t}\) is observable and the policies become Markovian, we have actions \(a_{h}^{t}\sim\pi_{h}^{t}(s_{h}^{t})\) and \(b_{h}^{t}\sim\nu_{h}^{t}(s_{h}^{t})\) and the reward \(r_{h}(s_{h}^{t},a_{h}^{t},b_{h}^{t})\). Then, it reduces to the interaction process under the FOMG setting.

**Value Function, Best Response, and Nash Equilibrium.** To characterize the learning objective and the performance of the algorithms, we define the value function as the expected cumulative rewards under the policy pair \((\pi,\nu)\) starting from the initial step \(h=1\). For FOMGs, we define the value function as \(V^{\pi,\nu}:=\mathbb{E}[\sum_{h=1}^{H}r_{h}(s_{h},a_{h},b_{h})\mid s_{1},\pi, \nu,\mathbb{P}]\), where the expectation is taken over all the randomness induced by \(\pi\), \(\nu\), and \(\mathbb{P}\). For POMG, we define the value function as \(V^{\pi,\nu}:=\mathbb{E}[\sum_{h=1}^{H}r_{h}(o_{h},a_{h},b_{h})\mid\pi,\nu,\theta]\), with the expectation taken for \(\pi\), \(\nu\), and \(\theta\).

Our work studies the competitive setting of RL, where Player 1 (_max-player_) aims to maximize the value function \(V^{\pi,\nu}\) while Player 2 (_min-player_) aims to minimize it. With the defined value function, given a policy pair \((\pi,\nu)\), we define their _best responses_ respectively as \(\texttt{br}(\pi)\in\arg\min_{\nu}V^{\pi,\nu}\) and \(\texttt{br}(\nu)\in\arg\max_{\pi}V^{\pi,\nu}\). Then, we say a policy pair \((\pi^{*},\nu^{*})\) is a _Nash equilibrium_ (NE) if

\[V^{\pi^{*},\nu^{*}}=\max_{\pi}\min_{\nu}V^{\pi,\nu}=\min_{\nu}\max_{\pi}V^{\pi, \nu}.\]

Thus, it always holds that \(\pi^{*}=\texttt{br}(\nu^{*})\) and \(\nu^{*}=\texttt{br}(\pi^{*})\). For abbreviation, we denote \(V^{*}=V^{\pi^{*},\nu^{*}}\), \(V^{\pi,*}=\min_{\nu}V^{\pi,\nu}\), and \(V^{*,\nu}=\max_{\pi}V^{\pi,\nu}\), which implies \(V^{*}=V^{\pi^{*},*}=V^{*,\nu^{*}}\) for NE \((\pi^{*},\nu^{*})\). Moreover, we define the policy pair \((\pi,\nu)\) as an \(\varepsilon\)-approximate NE if it satisfies \(V^{*,\nu}-V^{\pi,*}\leq\varepsilon\).

**Function Approximation.** Since the environment is unknown to players, the model-based RL setting requires us to learn the true model of the environment, \(f^{*}\), via (general) function approximation. We use the functions \(f\) lying in a general model function class \(\mathcal{F}\) to approximate the environment. We make a standard realizability assumption on the relationship between the model class and the true model.

**Assumption 1** (Realizability).: _For a model class \(\mathcal{F}\), the true model \(f^{*}\) satisfies \(f^{*}\in\mathcal{F}\)._In our work, the true model \(f^{*}\) represents the transition kernel \(\mathbb{P}\) for the FOMG and \(\theta\) for the POMG. For any \(f\in\mathcal{F}\), we let \(\mathbb{P}_{f}\) and \(\theta_{f}=(\mu_{f},\mathbb{P}_{f},\mathbb{O}_{f})\) be the models under the approximation function \(f\) and \(V_{f}^{\pi,\nu}\) the value function associated with \(f\). For POMGs, we denote \(\mathbf{P}_{f,h}^{\pi,\nu}\) and \(\mathbf{P}_{f,h}\) as \(\mathbf{P}_{h}^{\pi,\nu}\) and \(\mathbf{P}_{h}\) under the model \(f\).

**MGs with Self-Play and Adversarial Learning.** Our work investigates two important MG setups for competitive RL, which are the self-play setting and the adversarial setting. In the self-play setting, the learner can control _both_ players together to execute the proposed algorithms to learn an approximate NE. Therefore, our objective is to design sample-efficient algorithms to generate a sequence of policy pairs \(\{(\pi^{t},\nu^{t})\}_{t=1}^{T}\) in \(T\) episodes such that the following regret can be minimized,

\[\mathrm{Reg}^{\mathrm{sp}}(T):=\sum_{t=1}^{T}\big{[}V_{f^{*}}^{*,\nu^{t}}-V_{f ^{*}}^{\pi^{t},*}\big{]}.\]

In the adversarial setting, we can no longer coordinate both players, and only _single_ player is controllable. Under such a circumstance, the opponent plays arbitrary and even adversarial policies. Wlog, suppose that the main player is the max-player with the policies \(\{\pi^{t}\}_{t=1}^{T}\) generated by a carefully designed algorithm and the opponent is min-player with arbitrary policies \(\{\nu^{t}\}_{t=1}^{T}\). The objective of the algorithm is to learn policies \(\{\pi^{t}\}_{t=1}^{T}\) to maximize the overall cumulative rewards in the presence of an adversary. To measure the performance of algorithms, we define the following regret for the adversarial setting by comparing the learned value against the Nash value, i.e.,

\[\mathrm{Reg}^{\mathrm{adv}}(T)=\sum_{t=1}^{T}\big{[}V_{f^{*}}^{*}-V_{f^{*}}^{ \pi^{t},\nu^{t}}\big{]}.\]

## 3 Model-Based Posterior Sampling for the Self-Play Setting

We propose algorithms aiming to generate a sequence of policy pairs \(\{(\pi^{t},\nu^{t})\}_{t=1}^{T}\) by controlling the learning process of both players such that the regret \(\mathrm{Reg}^{\mathrm{sp}}(T)\) is small. Such a regret can be decomposed into two parts, namely \(\sum_{t=1}^{T}[V_{f^{*}}^{*}-V_{f^{*}}^{\pi^{t},*}]\) and \(\sum_{t=1}^{T}[V_{f^{*}}^{*,\nu^{t}}-V_{f^{*}}^{*}]\), which inspires our to design algorithms for learning \(\{\pi^{t}\}_{t=1}^{T}\) and \(\{\nu^{t}\}_{t=1}^{T}\) separately by targeting at minimizing these two parts respectively. Due to the symmetric structure of such a game learning problem, we propose the algorithm to learn \(\{\pi^{t}\}_{t=1}^{T}\) as summarized in Algorithm 1. The algorithm for learning \(\{\nu^{t}\}_{t=1}^{T}\) can be proposed in a symmetric way in Algorithm 3, which is deferred to Appendix A. Our proposed algorithm features an integration of the model-based posterior sampling and the exploiter-guided self-play in a multi-agent learning scenario. In Algorithm 1, Player 1 is the main player, while Player 2 is called the exploiter, who assists the learning of the main player by exploiting her weakness.

**Posterior Sampling for the Main Player.** The posterior sampling constructs a posterior distribution \(p^{t}(\cdot|Z^{t-1})\) over the function class \(\mathcal{F}\) each round based on collected data and a pre-specified prior distribution \(p^{0}(\cdot)\), where \(Z^{t-1}\) denotes the random history up to the end of the \((t-1)\)-th episode. For ease of notation, hereafter, we omit \(Z^{t-1}\) in the posterior distribution. Most recent literature shows that adding an optimism term in the posterior distribution can lead to sample-efficient RL algorithms. Thereby, we define the distribution \(p^{t}(\cdot)\) over the function class \(\mathcal{F}\) for the main player as in Line 3 of Algorithm 1, which is proportional to \(p^{0}(f)\exp[\gamma V_{f}^{*}+\sum_{\tau=1}^{t-1}\sum_{h=1}^{H}L_{h}^{\tau}(f)]\). Here, \(\gamma V_{f}^{*}\) serves as the optimism term, and \(L_{h}^{\tau}(f)\) is the likelihood function built upon the pre-collected data. Such a construction of \(p^{t}(\cdot)\) indicates that we will assign a higher probability (density) to a function \(f\), which results in higher values of the combination of the optimism term and the likelihood function. We sample a model \(\overline{f}^{t}\) from the distribution \(p^{t}(\cdot)\) over the model class and learn the policy \(\pi^{t}\) for the main player such that \((\pi^{t},\overline{\nu}^{t})\) is the NE of the value function under the model \(\overline{f}^{t}\) in Line 4, where \(\overline{\nu}^{t}\) is a dummy policy and only used in our theoretical analysis.

**Posterior Sampling for the Exploiter.** The exploiter aims to track the best response of \(\pi^{t}\) to assist learning a low regret. The best response of \(\pi^{t}\) generated by the exploiter is nevertheless based on a value function under a different model than \(\overline{f}^{t}\). Specifically, for the exploiter, we define the posterior sampling distribution \(q^{t}(\cdot)\) using an optimism term \(-\gamma V_{f}^{\pi^{t},*}\) and the summation of likelihood functions, i.e., \(\sum_{\tau=1}^{t-1}\sum_{h=1}^{H}L_{h}^{\tau}(f)\), along with a prior distribution \(q^{0}(\cdot)\), in Line 5 of Algorithm 1. The negative term \(-\gamma V_{f}^{\pi^{t},*}\) favors a model with a low value and is thus optimistic from the exploiter's perspective but pessimistic for the main player. We then sample a model \(\underline{f}^{t}\) from \(q^{t}(\cdot)\) and compute the best response of \(\pi^{t}\), denoted as \(\underline{\nu}^{t}\), under the model \(\underline{f}^{t}\) as in Line 7.

**Data Sampling and Likelihood Function.** With the learned \(\pi^{t}\) and \(\nu^{t}\), we define a joint exploration policy \(\sigma^{t}\) in Line 7 of Algorithm 1, by executing which we can collect a dataset \(\mathcal{D}^{t}\). We are able to further construct the likelihood functions \(\{L_{h}^{t}(f)\}_{h=1}^{H}\) in Line 8 using \(\mathcal{D}^{t}\). Different game settings require specifying diverse exploration policies \(\sigma^{t}\) and likelihood functions \(\{L_{h}^{t}(f)\}_{h=1}^{H}\). Particularly, for the game classes mainly discussed in this work, we set \(\sigma^{t}=(\pi^{t},\underline{\nu}^{t})\) for both FOMGs and POMGs. In FOMGs, we let \(\mathcal{D}^{t}=\{(s_{h}^{t},a_{h}^{t},b_{h}^{t},s_{h+1}^{t})\}_{h=1}^{H}\), where for each \(h\in[H]\), the data point \((s_{h}^{t},a_{h}^{t},b_{h}^{t},s_{h+1}^{t})\) is collected by executing \(\sigma^{t}\) to the \(h\)-th step of the game. The corresponding likelihood function is defined using the transition kernel as

\[L_{h}^{t}(f)=\eta\log\mathbb{P}_{f,h}(s_{h+1}^{t}\,|\,s_{h}^{t},a_{h}^{t},b_{h} ^{t}).\] (1)

Furthermore, under the POMG setting, we let the dataset be \(\mathcal{D}^{t}=\{\tau_{h}^{t}\}_{h=1}^{H}\), where the data point \(\tau_{h}^{t}=(o_{1}^{t},a_{1}^{t},b_{1}^{t}\,\dots,o_{h}^{t},a_{h}^{t},b_{h}^{ t})\) is collected by executing \(\sigma^{t}\) to the \(h\)-th step of the game for each \(h\in[H]\). We further define the associated likelihood function as

\[L_{h}^{t}(f)=\eta\log\mathbb{P}_{f,h}(\tau_{h}^{t}).\] (2)

Such a construction of the likelihood function in a log-likelihood form can result in learning a model \(f\) well approximating the true model \(f^{*}\) measured via the Hellinger distance.

### Regret Analysis for the Self-play Setting

Our regret analysis is based on a novel structural complexity condition for multi-agent RL and a quantity to measure how the well the prior distributions cover the optimal model \(f^{*}\). We first define the following condition for the self-play setting.

**Definition 1** (Self-Play GEC).: _For any sequences of functions \(f^{t},g^{t}\in\mathcal{F}\), suppose that a pair of policies \((\pi^{t},\nu^{t})\) satisfies: **(a)**\(\pi^{t}=\operatorname*{argmax}_{\pi}\min_{\nu}V_{f^{t}}^{\pi,\nu}\) and \(\nu^{t}=\operatorname*{argmin}_{\nu}V_{g^{t}}^{\pi^{t},\nu}\), or **(b)**\(\nu^{t}=\operatorname*{argmin}_{\nu}V_{f^{t}}^{\pi,\nu}\) and \(\pi^{t}=\operatorname*{argmax}_{\pi}V_{g^{t}}^{\pi,\nu^{t}}\). Denoting the joint exploration policy as \(\sigma^{t}\) depending on \(f^{t}\) and \(g^{t}\), for any \(\rho\in\{f,g\}\) and \((\pi^{t},\nu^{t})\) following **(a)** and **(b)**, the self-play GEC \(d_{\mathrm{GEC}}\) is defined as the minimal constant \(d\) satisfying_

\[\left|\sum_{t=1}^{T}\left(V_{\rho^{t}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{\pi^{t}, \nu^{t}}\right)\right|\leq\left[d\sum_{h=1}^{H}\sum_{t=1}^{T}\left(\sum_{\tau= 1}^{t-1}\mathbb{E}_{(\sigma^{\tau},h)}\ell(\rho^{t},\xi_{h}^{\tau})\right) \right]^{\frac{1}{2}}+2H(dHT)^{\frac{1}{2}}+\epsilon HT.\]

Our definition of self-play GEC is inspired by [83] for the single-agent RL. Then, it shares an analogous meaning to the single-agent GEC. Here \((\sigma^{\tau},h)\) implies running the joint exploration policy \(\sigma^{\tau}\) to step \(h\) to collect a data point \(\xi_{h}^{\tau}\). The LHS of the inequality is viewed as the prediction error and the RHS is the training error defined on a loss function \(\ell\) plus a burn-in error \(2H(dHT)^{\frac{1}{2}}+\epsilon HT\) that is non-dominating when \(\epsilon\) is small. The loss function \(\ell\) and \(\epsilon\) can be problem-specific. We determine \(\ell(f,\xi_{h})\) for FOMGs with \(\xi_{h}=(s_{h},a_{h})\) and POMGs with \(\xi_{h}=\tau_{h}\) respectively as

\[\text{FOMG: }D_{\mathrm{He}}^{2}(\mathbb{P}_{f,h}(\cdot|\xi_{h}),\mathbb{P }_{f^{*},h}(\cdot|\xi_{h})),\quad\text{POMG: }1/2\cdot\left(\sqrt{\mathbb{P}_{f,h}(\xi_{h})/\mathbb{P}_{f^{*},h}(\xi_{h})}-1 \right)^{2},\] (3)such that \(\mathbb{E}_{(\sigma,h)}[\ell(f,\xi_{h})]=D^{2}_{\mathrm{He}}(\mathbf{P}^{\sigma}_{ \cdot,h},\mathbf{P}^{\sigma}_{f,h})\) for POMGs. The intuition for GEC is that if hypotheses have a small training error on a well-explored dataset, then the out-of-sample prediction error is also small, which characterizes the hardness of environment exploration.

Since the posterior sampling steps in our algorithms depend on the initial distributions \(p^{0}\) and \(q^{0}\), we define the following quantity to measure how well the prior distributions \(p^{0}\) and \(q^{0}\) cover the optimal model \(f^{*}\in\mathcal{F}\), which is also a multi-agent generalization of its single-agent version [2, 83].

**Definition 2** (Prior around True Model).: _Given \(\beta>0\) and any distribution \(p^{0}\in\Delta_{\mathcal{F}}\), we define_

\[\omega(\beta,p^{0})=\inf_{\varepsilon>0}\{\beta\varepsilon-\ln p^{0}[ \mathcal{F}(\varepsilon)]\},\]

_where \(\mathcal{F}(\varepsilon):=\{f\in\mathcal{F}\ :\ \sup_{h,s,a,b}\mathrm{KL}^{\frac{1}{2}}( \mathbb{P}_{\mathbf{f}^{\ast},h}(\cdot\,|\,s,a,b)\|\mathbb{P}_{f,h}(\cdot\,| \,s,a,b))\leq\varepsilon\}\) for FOMGs and \(\mathcal{F}(\varepsilon):=\{f\in\mathcal{F}\ :\ \sup_{\pi,\nu}\mathrm{KL}^{\frac{1}{2}}( \mathbf{P}^{\pi,\nu}_{f^{\ast},H}\|\mathbf{P}^{\pi,\nu}_{f,H})\leq\varepsilon\}\) for POMGs._

When the model class \(\mathcal{F}\) is a finite space, if let \(p^{0}=\mathrm{Unif}(\mathcal{F})\), we simply know that \(\omega(\beta,p^{0})\leq\log|\mathcal{F}|\) where \(|\mathcal{F}|\) is the cardinality of \(\mathcal{F}\). Furthermore, for an infinite function class \(\mathcal{F}\), the term \(\log|\mathcal{F}|\) can be substituted by a quantity having logarithmic dependence on the covering number of the function class \(\mathcal{F}\). With the multi-agent GEC condition and the definition of \(\omega\), we have the following regret bound for both FOMGs and POMGs.

**Proposition 1**.: _Letting \(\eta=1/2\), \(\gamma_{1}=2\sqrt{\omega(4HT,p^{0})T/d_{\mathrm{GEC}}}\), \(\gamma_{2}=2\sqrt{\omega(4HT,q^{0})T/d_{\mathrm{GEC}}}\), \(\epsilon=1/\sqrt{HT}\) in Definition 1, when \(T\geq\max\{4H^{2}\omega(4HT,p^{0})/d_{\mathrm{GEC}},4H^{2}\omega(4HT,q^{0})/d_ {\mathrm{GEC}},\)\(d_{\mathrm{GEC}}/H\}\), under both FOMG and POMG settings, Algorithm 1 admits the following regret bound,_

\[\mathbb{E}[\mathrm{Reg}_{1}^{\mathrm{sp}}(T)]:=\mathbb{E}[\sum_{t=1}^{T}(V_{f^ {\ast}}^{\ast}-V_{f^{\ast}}^{\ast^{\prime},\ast})]\leq 6\sqrt{d_{\mathrm{GEC}}HT \cdot[\omega(4HT,p^{0})+\omega(4HT,q^{0})]}.\]

This proposition gives the upper bound \(\mathbb{E}[\mathrm{Reg}_{1}^{\mathrm{sp}}(T)]\) following the updating rules in Algorithm 1 when the max-player is the main player. As Algorithm 3 is symmetric to Algorithm 1, we obtain the following regret bound of \(\mathbb{E}[\mathrm{Reg}_{2}^{\mathrm{sp}}(T)]\) for Algorithm 3 when the min-player is the main player.

**Proposition 2**.: _Under the same parameter settings as Proposition 1, Algorithm 3 admits the following regret bound,_

\[\mathbb{E}[\mathrm{Reg}_{2}^{\mathrm{sp}}(T)]:=\mathbb{E}[\sum_{t=1}^{T}(V_{f^ {\ast}}^{\ast,\nu^{t}}-V_{f^{\ast}}^{\ast})]\leq 6\sqrt{d_{\mathrm{GEC}}HT \cdot[\omega(4HT,p^{0})+\omega(4HT,q^{0})]}.\]

Combining the results of Propositions 1 and 2, due to \(\mathrm{Reg}^{\mathrm{sp}}(T)=\mathrm{Reg}_{1}^{\mathrm{sp}}(T)+\mathrm{Reg}_{2 }^{\mathrm{sp}}(T)\), we obtain the following overall regret when running Algorithms 1 and 3 together.

**Theorem 1**.: _Under the settings of Propositions 1 and 2, executing both Algorithms 1 and 3 leads to_

\[\mathbb{E}[\mathrm{Reg}^{\mathrm{sp}}(T)]\leq 12\sqrt{d_{\mathrm{GEC}}HT \cdot[\omega(4HT,p^{0})+\omega(4HT,q^{0})]}.\]

The above results indicate that the proposed posterior sampling self-play algorithms (Algorithms 1 and 3) separately admit a sublinear dependence on GEC \(d_{\mathrm{GEC}}\), the number of learning episodes \(T\), as well as \(\omega(4HT,p^{0})\) and \(\omega(4HT,q^{0})\) for both FOMG and POMG settings. They lead to the same overall regret bound combining Propositions 1 and 2. In particular, when \(\mathcal{F}\) is finite with \(p^{0}=q^{0}=\mathrm{Unif}(\mathcal{F})\), Algorithms 1 and 3 admit regrets of \(O(\sqrt{d_{\mathrm{GEC}}HT\cdot\log|\mathcal{F}|})\). The quantity \(\omega\) can be associated with the log-covering number if \(\mathcal{F}\) is infinite. Please see Appendix C for analysis.

## 4 Posterior Sampling for the Adversarial Setting

Without loss of generality, we assume that the max-player is the main agent and the min-player is the opponent. Under this setting, the goal of the main player is to maximize her cumulative rewards as much as possible, comparing against the value under the NE, i.e., \(V_{f^{\ast}}^{\ast}\). We develop a novel algorithm for this setting as summarized in Algorithm 2. In our algorithm, the opponent's policy is assumed to be arbitrary and is also _not revealed_ to the main player. The only information about the opponent is the current state or the partial observation of her state as well as the actions taken.

We adopt the optimistic posterior sampling approach for the main player with defining an optimism term as \(\gamma V_{f}^{\ast}\) motivated by the above learning target, and the likelihood function \(L_{h}^{t}(f)\) with \(L_{h}^{t}(f):=\eta\log\mathbb{P}_{f,h}(s_{h+1}^{t}\,|\,s_{h}^{t},a_{h}^{t},b_{h}^ {t})\) in (1) for FOMGs and \(L_{h}^{t}(f)=\eta\log\mathbf{P}_{f,h}(\tau_{h}^{t})\) in (2) for POMGsrespectively. The policy \(\pi^{t}\) learned by the main player is from computing the NE of the value function under the current model \(f^{t}\) sampled from the posterior distribution \(p^{t}\). In addition, the joint exploration policy is set to be \(\sigma^{t}=(\pi^{t},\nu^{t})\) where \(\nu^{t}\) is the potentially adversarial policy played by the opponent. Thus, we can collect the data defined as \(\mathcal{D}^{t}=\{(s^{t}_{h},a^{t}_{h},b^{t}_{h},s^{t}_{h+1})\}_{h=1}^{H}\) and \(\mathcal{D}^{t}=\{\tau^{t}_{h}\}_{h=1}^{H}\) with \(\tau^{t}_{h}=(o^{t}_{1},a^{t}_{1},b^{t}_{1}\dots,o^{t}_{h},a^{t}_{h},b^{t}_{h})\) for FOMGs and POMGs respectively, collected by executing \(\sigma^{t}\) to the \(h\)-th step of the game for each \(h\in[H]\).

**Remark 1**.: _In Algorithm 2, we define the joint exploration policy \(\sigma^{t}=(\pi^{t},\nu^{t})\), which is the key to the success of the algorithm design under the adversarial setting, especially for POMGs. Under the single-agent setting, the prior work [83] sets the exploration policy for a range of partially observable models subsumed by the PSR model as \(\pi^{t}_{1:h-1}\circ_{h}\operatorname{Unif}(\mathcal{A})\), i.e., running \(\pi^{t}\) for steps \(1\) to \(h-1\) and then sampling the data at step \(h\) by enforcing a uniform policy. Such an exploration scheme fails to work when facing an uncontrollable opponent who does not play a uniform policy at step \(h\). Theoretically, we prove that employing policies \((\pi^{t}_{1:h},\nu^{t}_{1:h})\) for exploration without the uniform policy, the self-play and adversarial GEC conditions in Definitions 1 and 3 are still satisfied for a class of POMGs including weakly revealing and decodable POMGs. This eventually leads to a unified adversarial learning algorithm for both FOMGs and POMGs._

### Regret Analysis for the Adversarial Setting

Before demonstrating our regret analysis, we first define a multi-agent GEC fitting the adversarial learning scenario. Considering that the opponent's policy is uncontrollable during the learning, we let \(\{\nu^{t}\}_{t=1}^{T}\) be arbitrary, which is clearly distinguished from self-play GEC defined in Definition 1.

**Definition 3** (Adversarial GEC).: _For any sequence of functions \(\{f^{t}\}_{t=1}^{T}\) with \(f^{t}\in\mathcal{F}\) and any sequence of the opponent's policies \(\{\nu^{t}\}_{t=1}^{T}\), suppose that the main player's policies \(\{\mu^{t}\}_{t=1}^{T}\) are generated via \(\mu^{t}=\operatorname*{argmax}_{\pi}\min_{\nu}V_{f^{t}}^{\nu^{t}}\). Denoting the joint exploration policy as \(\{\sigma^{t}\}_{t=1}^{T}\) depending on \(\{f^{t}\}_{t=1}^{T}\), the adversarial GEC \(d_{\mathrm{GEC}}\) is defined as the minimal constant \(d\) satisfying_

\[\sum_{t=1}^{T}\left(V_{f^{t}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{\pi^{t},\nu^{t}} \right)\leq\left[d\sum_{h=1}^{H}\sum_{t=1}^{T}\left(\sum_{\tau=1}^{t-1}\mathbb{ E}_{(\sigma^{\tau},h)}\ell(f^{t},\xi^{\tau}_{h})\right)\right]^{\frac{1}{2}}+2H(dHT)^{ \frac{1}{2}}+\epsilon HT.\]

Our regret analysis for Algorithm 2 also depends on the quantity \(\omega(\beta,p^{0})\) that characterizes the coverage of the prior distribution \(p^{0}\) on the true model \(f^{*}\). Then, we have the following regret bound.

**Theorem 2**.: _Letting \(\eta=\frac{1}{2}\), \(\gamma=2\sqrt{\omega(4HT,p^{0})T/d_{\mathrm{GEC}}}\), \(\epsilon=1/\sqrt{HT}\) in Definition 3, when \(T\geq\max\{4H^{2}\omega(4HT,p^{0})/d_{\mathrm{GEC}},d_{\mathrm{GEC}}/H\}\), under both FOMG and POMG settings, Algorithm 2 admits the following regret bound,_

\[\mathbb{E}[\mathrm{Reg}^{\mathrm{adv}}(T)]\leq 4\sqrt{d_{\mathrm{GEC}}HT\cdot \omega(4HT,p^{0})}.\]

The above result indicates that we can achieve a meaningful regret bound by a posterior sampling algorithm with general function approximation, even when the opponent's policy is adversarial and her full policies \(\nu^{t}\) are not revealed. This regret has a sublinear dependence on \(d_{\mathrm{GEC}}\), the number of episodes \(T\), as well as \(\omega(4HT,p^{0})\). Similarly, when \(\mathcal{F}\) is finite, Algorithm 2 admits a regret of \(O(\sqrt{d_{\mathrm{GEC}}HT\cdot\log|\mathcal{F}|})\). The term \(\log|\mathcal{F}|\) can be the log-covering number of \(\mathcal{F}\) if it is infinite.

[MISSING_PAGE_FAIL:9]

where \(\Delta V^{*}_{\vec{f}^{t}}:=V^{\pi^{t},\vec{p}^{t}}_{\vec{f}^{t}}-V^{*}_{f^{*}}\) and \(\Delta V^{\pi^{t},*}_{\vec{f}^{t}}=V^{\pi^{t},\vec{p}^{t}}_{\vec{f}^{t}}-V^{\pi^ {t},*}_{f^{*}}\) are associated with the optimism terms in posterior distributions. The inequality above for Term(i) is due to Line 4 such that \(V^{\pi^{t},\vec{p}^{t}}_{\vec{f}^{t}}=\min_{\nu}V^{\pi^{t},\nu^{t}}_{\vec{f}^{t} }\leq V^{\pi^{t},\vec{p}^{t}}_{\vec{f}^{t}}\). By Definition 1 for self-play GEC, we obtain that \(\sum_{t=1}^{T}\left(V^{\pi^{t},\vec{p}^{t}}_{\vec{f}^{t},1}-V^{\pi^{t},\vec{p} ^{t}}_{f^{*}}\right)\) and \(\sum_{t=1}^{T}\left(V^{\pi^{t},\vec{p}^{t}}_{f^{*}}-V^{\pi^{t},\vec{p}^{t}}_{ \vec{f}^{t},1}\right)\) can be bounded by

\[\left[d_{\mathrm{GEC}}\sum_{h=1}^{H}\sum_{t=1}^{T}\left(\sum_{t=1}^{t-1}\mathbb{ E}_{(\sigma^{t}_{\mathrm{exp}},h)}\ell(\rho^{t},\xi^{t}_{h})\right)\right]^{1/2}+2H(d_{ \mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT,\]

where \(\rho^{t}\) is chosen as \(\overline{f}^{t}\) or \(\underline{f}^{t}\) respectively. By Lemma 11 and Lemma 12, we prove that for both FOMGs and POMGs, the accumulation of the losses \(\ell(\overline{f}^{t},\xi^{t}_{h})\) in (3) connects to the likelihood function \(L^{t}_{h}\) defined in (1) and (2). Thus, we obtain \(\mathbb{E}[\mathrm{Term(i)}]\leq\sum_{t=1}^{T}\mathbb{E}_{Z^{t-1}}\mathbb{E}_ {\overline{f}^{t}\sim p^{t}}\{-\gamma_{1}\Delta V^{*}_{\overline{f}^{t}}- \sum_{h=1}^{H}\sum_{t=1}^{t-1}[L^{t}_{h}(\overline{f}^{t})-L^{t}_{h}(f^{*})]+ \log\frac{p^{t}(\overline{f}^{t})}{p^{t}(\overline{f}^{t})}\}+2H(d_{\mathrm{ GEC}}HT)^{\frac{1}{2}}+\epsilon HT\) and \(\mathbb{E}[\mathrm{Term(ii)}]\) has a similar bound based on \(q^{t}\), where \(Z^{t-1}\) is the randomness history. By Lemma 10, the posterior distributions \(p^{t}\) and \(q^{t}\) following Lines 3 and 5 of Algorithm 1 can minimize the above upper bounds for \(\mathbb{E}[\mathrm{Term(i)}]\) and \(\mathbb{E}[\mathrm{Term(ii)}]\). Therefore, we can relax \(p^{t}\) and \(q^{t}\) to be distributions defined around the true model \(f^{*}\) to enlarge above bounds. When \(T\) is sufficiently large and \(\eta=1/2\), we have

\[\mathbb{E}[\mathrm{Term(i)}] \leq\omega(HT,p^{0})T/\gamma_{1}+\gamma_{1}d_{\mathrm{GEC}}H/4+2H( d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT,\] \[\mathbb{E}[\mathrm{Term(ii)}] \leq\omega(HT,q^{0})T/\gamma_{2}+\gamma_{2}d_{\mathrm{GEC}}H/4+2H( d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT.\]

Choosing proper values for \(\epsilon\),\(\gamma_{1}\), and \(\gamma_{2}\), we obtain the bound for \(\mathbb{E}[\mathrm{Reg}^{\mathrm{sp}}_{\mathrm{2}}(T)]\) in Theorem 1 via \(\mathrm{Reg}^{\mathrm{sp}}_{1}(T)=\mathrm{Term(i)}\) + Term(ii). In addition, we can prove the bound of \(\mathbb{E}[\mathrm{Reg}^{\mathrm{sp}}_{\mathrm{2}}(T)]\) in a symmetric manner. Finally, combining \(\mathbb{E}[\mathrm{Reg}^{\mathrm{sp}}_{\mathrm{1}}(T)]\) and \(\mathbb{E}[\mathrm{Reg}^{\mathrm{sp}}_{\mathrm{2}}(T)]\) gives the result in Theorem 1.

**Proof Sketch of Theorem 2.** Under the adversarial setting, the policy of the opponent \(\nu^{t}\) is not generated by the algorithm, which could be arbitrarily time-varying. We decompose \(\mathrm{Reg}^{\mathrm{adv}}(T)=\sum_{t=1}^{T}\Delta V^{*}_{f^{t}}+\sum_{t=1}^{T }[V^{*}_{f^{t}}-V^{\pi^{t},\nu^{t}}_{f^{*}}]\) where \(\Delta V^{*}_{f^{t}}:=V^{*}_{f^{*}}-V^{*}_{f^{t}}\) relates to optimism. Since \((\pi^{t},\overline{\nu}^{t})\) is NE of \(V^{\pi,\nu}_{f^{t}}\) as in Line 3 of Algorithm 2, we have \(V^{*}_{f^{t}}=\min_{\nu}V^{\pi^{t},\nu}_{f^{t}}\leq V^{\pi^{t},\nu^{t}}_{f^{t}}\), which leads to

\[\mathrm{Reg}^{\mathrm{adv}}(T)\leq\sum_{t=1}^{T}\Delta V^{*}_{f^{t}}+\sum_{t=1} ^{T}\big{[}V^{\pi^{t},\nu^{t}}_{f^{t}}-V^{\pi^{t},\nu^{t}}_{f^{*}}\big{]}.\]

We can bound \(\sum_{t=1}^{T}[V^{\pi^{t},\nu^{t}}_{f^{t}}-V^{\pi^{t},\nu^{t}}_{f^{*}}]\) via adversarial GEC in Definition 3 by

\[\left[d_{\mathrm{GEC}}\sum_{h=1}^{H}\sum_{t=1}^{T}\Big{(}\sum_{t=1}^{t-1} \mathbb{E}_{(\sigma^{t}_{\mathrm{exp}},h)}\ell(f^{t},\xi^{t}_{h})\Big{)}\Big{]} ^{\frac{1}{2}}+2H(d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT.\]

Connecting the loss \(\ell(\overline{f}^{t},\xi^{t}_{h})\) to the likelihood function \(L^{t}_{h}\) defined in (1) and (2) via Lemmas 11 and 12, we obtain \(\mathbb{E}[\mathrm{Reg}^{\mathrm{adv}}(T)]\leq\sum_{t=1}^{T}\mathbb{E}_{Z^{t-1}} \mathbb{E}_{f^{t}\sim p^{t}}\{\gamma\sum_{t=1}^{T}\Delta V^{*}_{f^{t}}-\sum_{h=1 }^{H}\sum_{t=1}^{t-1}[L^{t}_{h}(f^{t})-L^{t}_{h}(f^{*})]+\log\frac{p^{t}(f^{t})} {p^{t}(f^{t})}\}+2H(d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT\). Lemma 10 shows \(p^{t}\) in Line 3 of Algorithm 2 can minimize this bound. Thus, relaxing \(p^{t}\) to be distribution defined around the true model \(f^{*}\), with sufficiently large \(T\) and \(\eta=1/2\), we have

\[\mathbb{E}[\mathrm{Reg}^{\mathrm{adv}}(T)]\leq\omega(4HT,p^{0})T/\gamma+\gamma d_{ \mathrm{GEC}}H/4+2H(d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT.\]

Choosing proper values for \(\epsilon\) and \(\gamma\), we eventually obtain the bound for \(\mathbb{E}[\mathrm{Reg}^{\mathrm{adv}}(T)]\) in Theorem 2.

**Discussion of Limitations.** Our work has studied several but a limited number of tractable MG classes in both FOMGs and POMGs. It is interesting to further define new MG classes by generalizing their single-agent counterparts and explore the relation between these MG classes and low self-play/adversarial GEC classes. It is also intriguing to generalize our method to general-sum settings.

## Acknowledgments and Disclosure of Funding

The authors would like to thank the anonymous reviewers for their valuable comments. The authors would like to thank Wei Xiong for the helpful discussions. Shuang Qiu and Tong Zhang acknowledge the funding supported by the GRF 16310222.

## References

* [1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* [2] Alekh Agarwal and Tong Zhang. Model-based rl with optimistic posterior sampling: Structural conditions and sample complexity. _arXiv preprint arXiv:2206.07659_, 2022.
* [3] Alekh Agarwal and Tong Zhang. Non-linear reinforcement learning in large action spaces: Structural conditions and sample-efficiency of posterior sampling. _arXiv preprint arXiv:2203.08248_, 2022.
* [4] Shipra Agrawal and Randy Jia. Posterior sampling for reinforcement learning: worst-case regret bounds. _arXiv preprint arXiv:1705.07041_, 2017.
* [5] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_, pages 463-474. PMLR, 2020.
* [6] Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In _International conference on machine learning_, pages 551-560. PMLR, 2020.
* [7] Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. _Advances in neural information processing systems_, 33:2159-2170, 2020.
* [8] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_, 2019.
* [9] Jose Blanchet, Miao Lu, Tong Zhang, and Han Zhong. Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage. _arXiv preprint arXiv:2305.09659_, 2023.
* [10] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. _Science_, 365(6456):885-890, 2019.
* [11] Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimization. In _International Conference on Machine Learning_, pages 1283-1294. PMLR, 2020.
* [12] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. _Advances in neural information processing systems_, 24, 2011.
* [13] Fan Chen, Yu Bai, and Song Mei. Partially observable rl with b-stability: Unified structural condition and sharp sample-efficient algorithms, 2022.
* [14] Fan Chen, Song Mei, and Yu Bai. Unified algorithms for rl with decision-estimation coefficients: No-regret, pac, and reward-free learning. _arXiv preprint arXiv:2209.11745_, 2022.
* [15] Zixiang Chen, Chris Junchi Li, Angela Yuan, Quanquan Gu, and Michael I Jordan. A general framework for sample-efficient function approximation in reinforcement learning. _arXiv preprint arXiv:2209.15634_, 2022.
* [16] Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player zero-sum linear mixture markov games. In _International Conference on Algorithmic Learning Theory_, pages 227-261. PMLR, 2022.
* [17] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. _Advances in neural information processing systems_, 31, 2018.
* [18] Qiwen Cui and Simon S Du. When is offline two-player zero-sum markov game solvable? _arXiv preprint arXiv:2201.03522_, 2022.

* [19] Christoph Dann, Mehryar Mohri, Tong Zhang, and Julian Zimmert. A provably efficient model-free posterior sampling method for episodic reinforcement learning. _Advances in Neural Information Processing Systems_, 34:12040-12051, 2021.
* [20] Constantinos Daskalakis, Noah Golowich, and Kaiqing Zhang. The complexity of markov equilibrium in stochastic games. _arXiv preprint arXiv:2204.03991_, 2022.
* [21] Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. In _International Conference on Machine Learning_, pages 2826-2836. PMLR, 2021.
* [22] Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably efficient rl with rich observations via latent state decoding. In _International Conference on Machine Learning_, pages 1665-1674. PMLR, 2019.
* [23] Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosefi. Provable reinforcement learning with a short-term memory. In _International Conference on Machine Learning_, pages 5832-5850. PMLR, 2022.
* [24] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* [25] Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. Learning in observable pomdps, without computationally intractable oracles. _arXiv preprint arXiv:2206.03446_, 2022.
* [26] Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. Planning in observable pomdps in quasipolynomial time. _arXiv preprint arXiv:2201.04735_, 2022.
* [27] Zhaohan Daniel Guo, Shayan Doroudi, and Emma Brunskill. A pac rl algorithm for episodic pomdps. In _Artificial Intelligence and Statistics_, pages 510-518. PMLR, 2016.
* [28] Baihe Huang, Jason D Lee, Zhaoran Wang, and Zhuoran Yang. Towards general function approximation in zero-sum markov games. _arXiv preprint arXiv:2107.14702_, 2021.
* [29] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_, pages 1704-1713. PMLR, 2017.
* [30] Chi Jin, Sham Kakade, Akshay Krishnamurthy, and Qinghua Liu. Sample-efficient reinforcement learning of undercomplete pomdps. _Advances in Neural Information Processing Systems_, 33:18530-18539, 2020.
* [31] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. _Advances in neural information processing systems_, 34:13406-13418, 2021.
* [32] Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning-a simple, efficient, decentralized algorithm for multiagent rl. _arXiv preprint arXiv:2110.14555_, 2021.
* [33] Chi Jin, Qinghua Liu, and Tiancheng Yu. The power of exploiter: Provable multi-agent rl in large state spaces. _arXiv preprint arXiv:2106.03352_, 2021.
* [34] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* [35] Yujia Jin, Vidya Muthukumar, and Aaron Sidford. The complexity of infinite-horizon general-sum stochastic games. _arXiv preprint arXiv:2204.04186_, 2022.
* [36] Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Pac reinforcement learning with rich observations. _Advances in Neural Information Processing Systems_, 29, 2016.
* [37] Jeongyeol Kwon, Yonathan Efroni, Constantine Caramanis, and Shie Mannor. Rl for latent mdps: Regret guarantees and a lower bound. _Advances in Neural Information Processing Systems_, 34:24523-24534, 2021.

* [38] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [39] Qinghua Liu, Alan Chung, Csaba Szepesvari, and Chi Jin. When is partially observable reinforcement learning not scary? In _Conference on Learning Theory_, pages 5175-5220. PMLR, 2022.
* [40] Qinghua Liu, Praneeth Netrapalli, Csaba Szepesvari, and Chi Jin. Optimistic mle-a generic model-based algorithm for partially observable sequential decision making. _arXiv preprint arXiv:2209.14997_, 2022.
* [41] Qinghua Liu, Csaba Szepesvari, and Chi Jin. Sample-efficient reinforcement learning of partially observable markov games. _arXiv preprint arXiv:2206.01315_, 2022.
* [42] Qinghua Liu, Yuanhao Wang, and Chi Jin. Learning markov games with adversarial opponents: Efficient algorithms and fundamental limits. _arXiv preprint arXiv:2203.06803_, 2022.
* [43] Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning with self-play. In _International Conference on Machine Learning_, pages 7001-7010. PMLR, 2021.
* [44] Xiuyuan Lu and Benjamin Van Roy. Ensemble sampling. _Advances in neural information processing systems_, 30, 2017.
* [45] Weichao Mao and Tamer Basar. Provably efficient reinforcement learning in decentralized general-sum Markov games. _arXiv preprint arXiv:2110.05682_, 2021.
* [46] Anusha Nagabandi, Kurt Konolige, Sergey Levine, and Vikash Kumar. Deep dynamics models for learning dexterous manipulation. In _Conference on Robot Learning_, pages 1101-1112. PMLR, 2020.
* [47] Chengzhuo Ni, Yuda Song, Xuezhou Zhang, Chi Jin, and Mengdi Wang. Representation learning for general-sum low-rank markov games. _arXiv preprint arXiv:2210.16976_, 2022.
* [48] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. _Advances in neural information processing systems_, 29, 2016.
* [49] Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. _Advances in Neural Information Processing Systems_, 27, 2014.
* [50] Shuang Qiu, Lingxiao Wang, Chenjia Bai, Zhuoran Yang, and Zhaoran Wang. Contrastive ucb: Provably efficient contrastive self-supervised learning in online reinforcement learning. In _International Conference on Machine Learning_, pages 18168-18210. PMLR, 2022.
* [51] Shuang Qiu, Xiaohan Wei, Jieping Ye, Zhaoran Wang, and Zhuoran Yang. Provably efficient fictitious play policy optimization for zero-sum markov games with structured transitions. In _International Conference on Machine Learning_, pages 8715-8725. PMLR, 2021.
* [52] Shuang Qiu, Jieping Ye, Zhaoran Wang, and Zhuoran Yang. On reward-free rl with kernel and neural function approximations: Single-agent mdp and markov game. In _International Conference on Machine Learning_, pages 8737-8747. PMLR, 2021.
* [53] Daniel Russo. Worst-case regret bounds for exploration via randomized value functions. _Advances in Neural Information Processing Systems_, 32, 2019.
* [54] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. _Mathematics of Operations Research_, 39(4):1221-1243, 2014.
* [55] Igal Sason and Sergio Verdu. \(f\)-divergence inequalities. _IEEE Transactions on Information Theory_, 62(11):5973-6006, 2016.
* [56] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. _arXiv preprint arXiv:1610.03295_, 2016.
* [57] Lloyd S Shapley. Stochastic games. _Proceedings of the national academy of sciences_, 39(10):1095-1100, 1953.

* Silver et al. [2016] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot. Mastering the game of Go with deep neural networks and tree search. _Nature_, 529(7587):484-489, 2016.
* Song et al. [2021] Ziang Song, Song Mei, and Yu Bai. When can we learn general-sum Markov games with a large number of players sample-efficiently? _arXiv preprint arXiv:2110.04184_, 2021.
* Sun et al. [2019] Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches. In _Conference on learning theory_, pages 2898-2933. PMLR, 2019.
* Tian et al. [2021] Yi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Online learning in unknown markov games. In _International conference on machine learning_, pages 10279-10288. PMLR, 2021.
* Uehara et al. [2022] Masatoshi Uehara, Ayush Sekhari, Jason D Lee, Nathan Kallus, and Wen Sun. Computationally efficient pac rl in pomdps with latent determinism and conditional embeddings. _arXiv preprint arXiv:2206.12081_, 2022.
* Uehara et al. [2022] Masatoshi Uehara, Ayush Sekhari, Jason D Lee, Nathan Kallus, and Wen Sun. Provably efficient reinforcement learning in partially observable dynamical systems. _arXiv preprint arXiv:2206.12020_, 2022.
* Van Handel [2014] Ramon Van Handel. Probability in high dimension. Technical report, PRINCETON UNIV NJ, 2014.
* Vinyals et al. [2017] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Kuttler, John Agapiou, Julian Schrittwieser, et al. StarCraft II: A new challenge for reinforcement learning. _arXiv preprint arXiv:1708.04782_, 2017.
* Wang et al. [2020] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Represent to control partially observed systems: Representation learning with provable sample efficiency. In _The Eleventh International Conference on Learning Representations_.
* Wang et al. [2020] Ruosong Wang, Ruslan Salakhutdinov, and Lin F Yang. Provably efficient reinforcement learning with general value function approximation. _arXiv preprint arXiv:2005.10804_, 2020.
* Wang et al. [2020] Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. _Advances in Neural Information Processing Systems_, 33:6123-6135, 2020.
* Wang et al. [2019] Yining Wang, Ruosong Wang, Simon S Du, and Akshay Krishnamurthy. Optimism in reinforcement learning with generalized linear function approximation. _arXiv preprint arXiv:1912.04136_, 2019.
* Welling and Teh [2011] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 681-688. Citeseer, 2011.
* Xie et al. [2020] Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium. In _Conference on learning theory_, pages 3674-3682. PMLR, 2020.
* Xiong et al. [2022] Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, Liwei Wang, and Tong Zhang. Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent mdp and markov game. _arXiv preprint arXiv:2205.15512_, 2022.
* Xiong et al. [2022] Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, and Tong Zhang. A self-play posterior sampling algorithm for zero-sum markov games. In _International Conference on Machine Learning_, pages 24496-24523. PMLR, 2022.
* Yang et al. [2023] Yunchang Yang, Han Zhong, Tianhao Wu, Bin Liu, Liwei Wang, and Simon S Du. A reduction-based framework for sequential decision making with delayed feedback. _arXiv preprint arXiv:2302.01477_, 2023.

* [75] Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael I Jordan. On function approximation in reinforcement learning: Optimism in the face of large state spaces. _arXiv preprint arXiv:2011.04622_, 2020.
* [76] Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric. Frequentist regret bounds for randomized least-squares value iteration. In _International Conference on Artificial Intelligence and Statistics_, pages 1954-1964. PMLR, 2020.
* [77] Wenhao Zhan, Jason D Lee, and Zhuoran Yang. Decentralized optimistic hyperpolicy mirror descent: Provably no-regret learning in markov games. _arXiv preprint arXiv:2206.01588_, 2022.
* [78] Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. Pac reinforcement learning for predictive state representations. _arXiv preprint arXiv:2207.05738_, 2022.
* [79] Tianjun Zhang, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans, and Bo Dai. Making linear mdps practical via contrastive representation learning. In _International Conference on Machine Learning_, pages 26447-26466. PMLR, 2022.
* [80] Tong Zhang. From \(\varepsilon\)-entropy to kl-entropy: Analysis of minimum information complexity density estimation. 2006.
* [81] Tong Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. _SIAM Journal on Mathematics of Data Science_, 4(2):834-857, 2022.
* [82] Han Zhong, Wei Xiong, Jiyuan Tan, Liwei Wang, Tong Zhang, Zhaoran Wang, and Zhuoran Yang. Pessimistic minimax value iteration: Provably efficient equilibrium learning from offline datasets. _arXiv preprint arXiv:2202.07511_, 2022.
* [83] Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond. _arXiv preprint arXiv:2211.01962_, 2022.
* [84] Han Zhong, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Can reinforcement learning find stackelberg-nash equilibria in general-sum markov games with myopically rational followers? _Journal of Machine Learning Research_, 24(35):1-52, 2023.
* [85] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_, pages 4532-4576. PMLR, 2021.

Appendix

###### Contents

* A Omitted Algorithm in the Main Text
* B Computation of Self-Play and Adversarial GEC
* B.1 Linear MG
* B.2 Linear Mixture MG
* B.3 Low-Witness-Rank MG
* B.4 Weakly Revealing POMG
* B.5 Decodable POMG
* C Computation of \(\omega(\beta,p^{0})\)
* C.1 Fully Observable Markov Game
* C.2 Partially Observable Markov Game
* D Technical Lemmas for Main Theorems
* D.1 Lemmas
* D.2 Proof of Lemma 11
* D.3 Proof of Lemma 12
* D.4 Proof of Lemma 13
* D.5 Proof of Lemma 14
* E Proofs for Algorithms 1 and 3
* E.1 Proof of Proposition 1
* E.2 Proof of Proposition 2
* E.3 Proof of Theorem 1
* F Proofs for Algorithm 2
* F.1 Proof of Theorem 2
* G Other Supporting Lemmas

## Appendix A Omitted Algorithm in the Main Text

```
1:Input: Model class \(\mathcal{F}\), prior distributions \(p^{0}\) and \(q^{0}\), \(\gamma_{1}\), and \(\gamma_{2}\).
2:for\(t=1,\dots,T\)do
3: Draw a model \(\overline{f}^{t}\sim p^{t}(\cdot)\) with defining \(p^{t}(f)\propto p^{0}(f)\exp[-\gamma_{1}V_{f}^{*}+\sum_{\iota=1}^{t-1}\sum_{h=1 }^{H}L_{h}^{\tau}(f)]\)
4: Compute \(\nu^{t}\) by letting \((\overline{\pi}^{t},\nu^{t})\) be NE of \(V_{\overline{f}^{t}}^{\pi,\nu}\)
5: Draw a model \(\underline{f}^{t}\sim q^{t}(\cdot)\) with defining \(q^{t}(f)\propto q^{0}(f)\exp[\gamma_{2}V_{f}^{*,\nu^{t}}+\sum_{\iota=1}^{t-1} \sum_{h=1}^{H}L_{h}^{\tau}(f)]\)
6: Compute \(\underline{\pi}^{t}\) by letting \(\underline{\pi}^{t}\) be the best response of \(\nu^{t}\) w.r.t. \(V_{\underline{f}^{t}}^{\pi,\nu}\)
7: Collect data \(\mathcal{D}^{t}\) by executing the joint exploration policy \(\sigma^{t}\)
8: Define the likelihood functions \(\{L_{h}^{t}(f)\}_{h=1}^{H}\) using the collected data \(\mathcal{D}^{t}\)
9:endfor
10:Return:\((\nu^{1},\dots,\nu^{T})\). ```

**Algorithm 3** Model-Based Posterior Sampling for Self-Play (Min-Player)

## Appendix B Computation of Self-Play and Adversarial GEC

In this section, we present the computation of the self-play and adversarial GEC for different FOMG and POMG classes, including linear MGs, linear mixture MGs, low-witness-rank MGs, weakly revealing POMGs, and a novel POMG class, dubbed decodable POMGs, newly proposed by this work.

Note that the GEC \(d_{\mathrm{GEC}}\) essentially depends on \(\epsilon\) as shown in Definition 1 and Definition 3 such that it should be expressed as \(d_{\mathrm{GEC}}^{\epsilon}\). For convenience, we omit such a dependence in the notation. In our main theorems, we set \(\epsilon=1/\sqrt{HT}\), and according to our derivation in this section, \(d_{\mathrm{GEC}}\) has an \(O(\log(1/\epsilon))=O(\cdot\log(HT))\) factor which only scales logarithmically in \(T\).

### Linear MG

In this subsection, we show that the linear MG is in the class of MGs with low self-play and adversarial GEC and further calculate \(d_{\mathrm{GEC}}\) for the linear MG.

**Definition 4** (Linear Markov Game [71]).: _The linear MG is a FOMG admitting the following linear structures on the reward function and transition kernel,_

\[r_{h}(s,a,b)=\mathbf{w}_{h}^{\top}\phi(s,a,b),\quad\mathbb{P}_{h}(s^{\prime} \,|\,s,a,b)=\boldsymbol{\theta}_{h}(s^{\prime})^{\top}\boldsymbol{\phi}(s,a, b),\]

_where there exist a known feature map \(\boldsymbol{\phi}:\mathcal{S}\times\mathcal{A}\times\mathcal{B}\mapsto \mathbb{R}^{d}\) and unknown coefficients \(\mathbf{w}_{h}\in\mathbb{R}^{d}\) and \(\boldsymbol{\theta}_{h}(s^{\prime})\in\mathbb{R}^{d}\) that should be learned in algorithms. We assume that the feature map and the coefficients satisfy \(\|\boldsymbol{\phi}(s,a,b)\|_{2}\leq 1\), \(\int_{\mathcal{S}}\|\boldsymbol{\theta}_{h}(s)\|_{2}\mathrm{d}s\leq\sqrt{d}\), and \(\|\mathbf{w}_{h}\|_{2}\leq\sqrt{d}\)._

Then, \(d_{\mathrm{GEC}}\) for the linear MG can be calculated in the following proposition. We can show that both low self-play and adversarial GEC classes subsume the class of linear MGs.

**Proposition 3** (Linear MG \(\subset\) Low Self-Play/Adversarial GEC).: _For linear MGs with \(\phi(s,a,b)\in\mathbb{R}^{d}\) as defined in Definition 4, and for \(\epsilon>0\), when \(T\) is sufficiently large, choosing \(\ell(f,\xi_{h})=D_{\mathrm{He}}^{2}\big{(}\mathbb{P}_{f,h}(\cdot\,|\,\xi_{h}), \mathbb{P}_{f^{*},h}(\cdot\,|\,\xi_{h})\big{)}\) as in (3) with \(\xi_{h}=(s_{h},a_{h},b_{h})\), we have_

_linear MG \(\subset\) MG with low self-play and adversarial GEC_

_with \(d_{\mathrm{GEC}}\) satisfying_

\[d_{\mathrm{GEC}}=16H^{3}d\log\left(1+\frac{HT}{\epsilon}\right).\]

Proof.: Since the value function is defined as \(V_{f}^{\pi,\nu}(s_{h}):=\mathbb{E}_{\pi,\nu,\mathbb{P}_{f}}[\sum_{h^{\prime}= h}^{H}r_{h^{\prime}}(s_{h^{\prime}},a_{h^{\prime}},b_{h^{\prime}})|s_{h}]\) at step \(h\) with denoting \(V_{f}^{\pi,\nu}=V_{f,1}^{\pi,\nu}(s_{1})\) and we also have a relation

[MISSING_PAGE_FAIL:18]

where \(\mathbf{1}_{\{\cdot\}}\) denotes an indicator function.

Plugging (5) into (7) and taking a summation from \(t=1\) to \(T\), we obtain

\[\sum_{h=1}^{H}\sum_{t=1}^{T} \|\bm{q}_{h}^{t}\|_{\Phi_{h}^{t-1}}\min\{\varphi_{h}^{t},1\}\mathbf{ 1}_{\{\varphi_{h}^{t}<1\}}+H\cdot\mathbf{1}_{\{\varphi_{h}^{t}\geq 1\}}\] \[\leq\sum_{h=1}^{H}\sum_{t=1}^{T}\sqrt{\sum_{t=1}^{t-1}[\langle \bm{\phi}_{h}^{t},\bm{q}_{h}^{t}\rangle]^{2}+4\lambda dH^{2}\cdot\min\{\varphi_ {h}^{t},1\}\mathbf{1}_{\{\varphi_{h}^{t}<1\}}}+H\cdot\mathbf{1}_{\{\varphi_{h}^ {t}\geq 1\}}\] \[\leq\sum_{h=1}^{H}\sum_{t=1}^{T}\left(\sqrt{\sum_{t=1}^{t-1}[ \langle\bm{\phi}_{h}^{t},\bm{q}_{h}^{t}\rangle]^{2}+\sqrt{4\lambda dH^{2}}} \right)\cdot\min\{\varphi_{h}^{t},1\}\mathbf{1}_{\{\varphi_{h}^{t}<1\}}+H \cdot\mathbf{1}_{\{\varphi_{h}^{t}\geq 1\}}\] \[\leq\sqrt{\sum_{h=1}^{H}\sum_{t=1}^{T}\sum_{t=1}^{t-1}[\langle \bm{\phi}_{h}^{t},\bm{q}_{h}^{t}\rangle]^{2}}\sqrt{\sum_{h=1}^{H}\sum_{t=1}^{T }\min\{(\varphi_{h}^{t})^{2},1\}\mathbf{1}_{\{\varphi_{h}^{t}<1\}}}\] \[\quad+\sqrt{4\lambda dH^{3}T\sum_{h=1}^{H}\sum_{t=1}^{T}\min\{( \varphi_{h}^{t})^{2},1\}\mathbf{1}_{\{\varphi_{h}^{t}<1\}}}+H\sum_{h=1}^{H} \sum_{t=1}^{T}\mathbf{1}_{\{\varphi_{h}^{t}\geq 1\}},\] (8)

where the second inequality uses \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\) and the last inequality is by Cauchy-Schwarz. According to the elliptical potential lemma (Lemma 15), we have the following bound,

\[\sum_{h=1}^{H}\sum_{t=1}^{T}\min\{(\varphi_{h}^{t})^{2},1\}\leq \sum_{h=1}^{H}2\log\left(\frac{\det\Phi_{h}^{T}}{\det\Phi_{h}^{0}}\right)\leq 2Hd \log\left(1+\frac{T}{d\lambda}\right),\] (9)

where the first inequality uses Lemma 15, and the last inequality uses \(\log\det\Phi_{h}^{t}\leq d\log\frac{\operatorname{tr}(\Phi_{h}^{T})}{d}\leq d \log\frac{\lambda d+T}{d}\). We also note that \(\phi_{h}^{t}\) cannot exceed \(1\) too many times, as detailed in Lemma 16. By Lemma 16 and the fact that \(\mathbf{1}_{\{\varphi_{h}^{t}\geq 1\}}\leq 1\), we obtain

\[\sum_{h=1}^{H}\sum_{t=1}^{T}\mathbf{1}_{\{\varphi_{h}^{t}\geq 1\}} \leq\min\left\{\frac{3Hd}{\log 2}\log\left(1+\frac{1}{\lambda\log 2} \right),HT\right\}.\] (10)

Combining (6), (7), (8),(9), and (10), we obtain

\[\left|\sum_{t=1}^{T}\left(V_{f^{t}}^{\pi^{t},\nu^{t}}-V_{f^{\ast} }^{\pi^{t},\nu^{t}}\right)\right|\] \[\quad\leq\sqrt{2Hd\log\left(1+\frac{T}{d\lambda}\right)\sum_{h=1} ^{H}\sum_{t=1}^{T}\sum_{i=1}^{t-1}[\langle\bm{\phi}_{h}^{t},\omega_{h}^{t} \rangle]^{2}}\] \[\quad\quad+\sqrt{4\lambda dH^{3}T\min\left\{2Hd\log\left(1+\frac{ T}{d\lambda}\right),HT\right\}}+\min\left\{\frac{3H^{2}d}{\log 2}\log\left(1+\frac{1}{ \lambda\log 2}\right),H^{2}T\right\}\] \[\quad\leq H\sqrt{16Hd\log\left(1+\frac{T}{d\lambda}\right)\sum_{t =1}^{T}\sum_{h=1}^{H}\sum_{t=1}^{t-1}\mathbb{E}_{(\pi^{t},\nu^{t},h)}D_{\rm He }^{2}\big{(}\mathbb{P}_{f^{t},h}(\cdot\,|\,\xi_{h}^{t}),\mathbb{P}_{f^{\ast},h}(\cdot\,|\,\xi_{h}^{t})\big{)}}\] \[\quad\quad+\lambda dH^{2}T+\min\left\{2H^{2}d\log\left(1+\frac{T}{ d\lambda}\right),H^{2}T\right\}+\min\left\{\frac{3H^{2}d}{\log 2}\log\left(1+\frac{1}{ \lambda\log 2}\right),H^{2}T\right\}\] \[\quad\leq\sqrt{16H^{3}d\log\left(1+\frac{T}{d\lambda}\right)\sum_{t =1}^{T}\sum_{h=1}^{H}\sum_{t=1}^{t-1}\mathbb{E}_{(\pi^{t},\nu^{t},h)}D_{\rm He }^{2}\big{(}\mathbb{P}_{f^{t},h}(\cdot\,|\,\xi_{h}^{t}),\mathbb{P}_{f^{\ast},h}(\cdot\,|\,\xi_{h}^{t})\big{)}}\] \[\quad\quad+\lambda dH^{2}T+2\left[2H^{2}d\log\left(1+\frac{T}{d \lambda}\right)\wedge H^{2}T\right],\] (11)which further leads to

\[\left|\sum_{t=1}^{T}\left(V_{f^{*}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{\pi^ {t},\nu^{t}}\right)\right| \leq\sqrt{16H^{3}d\log\left(1+\frac{HT}{\epsilon}\right)\sum_{t=1} ^{T}\sum_{h=1}^{H}\sum_{t=1}^{t-1}\mathbb{E}_{(\pi^{*},\nu^{t},h)}\ell(f^{t}, \xi_{h}^{t})}\] \[\quad+\epsilon HT+2\sqrt{2H^{4}Td\log\left(1+\frac{HT}{\epsilon} \right)}\]

by \(x\wedge y\leq\sqrt{xy}\) and setting \(\epsilon=\lambda Hd\) where \(\lambda>0\) and \(\ell(f,\xi_{h}):=D_{\mathrm{He}}^{2}(\mathbb{P}_{f,h}(\cdot\,|\,\xi_{h}), \mathbb{P}_{f^{*},h}(\cdot\,|\,\xi_{h}))\) with \(\xi_{h}=(s_{h},a_{h},b_{h})\). Thus, we obtain that when \(T\) is sufficiently large, we have

\[d_{\mathrm{GEC}}=16H^{3}d\log\left(1+\frac{HT}{\epsilon}\right).\]

Note that the above results hold for arbitrary policy pair \((\nu^{t},\pi^{t})\). This implies that linear MGs with the dimension \(d\) is subsumed by the class of MGs with self-play and adversarial GEC \(d_{\mathrm{GEC}}=16H^{3}d\log(1+\frac{HT}{\epsilon})\). This completes the proof. 

### Linear Mixture MG

In this subsection, we discuss the linear mixture MG setting, where the transition kernel is a linear combination of some basis transition kernels and thus admits a linear structure. Specifically, we have the following definition of the linear mixture MG.

**Definition 5** (Linear Mixture Markov Game [16]).: _There exists a \(d\)-dimensional feature map \(\bm{\phi}:\mathcal{S}\times\mathcal{A}\times\mathcal{B}\times\mathcal{S} \mapsto\mathbb{R}^{d}\) and an unknown coefficient \(\bm{\theta}_{h}\in\mathbb{R}^{d}\), such that the transition of the Markov game can be represented by_

\[\mathbb{P}_{h}(s_{h+1}\,|\,s_{h},a_{h},b_{h})=\bm{\theta}_{h}^{\top}\bm{\phi}( s_{h+1}\,|\,s_{h},a_{h},b_{h}),\]

_where \(\|\bm{\theta}_{h}\|_{2}\leq B\) and \(\|\int_{\mathcal{S}}\bm{\phi}(s,a,b,s^{\prime})V(s^{\prime})\mathrm{d}s^{ \prime}\|_{2}\leq H\) for any bounded function \(V:\mathcal{S}\mapsto[0,H]\)._

In the context of linear mixture MGs, both self-play and adversarial GEC can be calculated.

**Proposition 4** (Linear Mixture MG \(\subset\) Low Self-play/Adversarial GEC).: _For linear mixture MGs with \(\bm{\phi}(s,a,b,s^{\prime})\in\mathbb{R}^{d}\) defined as in Definition 4, and for \(\epsilon>0\), when \(T\) is sufficiently large, choosing \(\ell(f,\xi_{h})=D_{\mathrm{He}}^{2}\big{(}\mathbb{P}_{f,h}(\cdot\,|\,\xi_{h}), \mathbb{P}_{f^{*},h}(\cdot\,|\,\xi_{h})\big{)}\) as in (3) with \(\xi_{h}=(s_{h},a_{h},b_{h})\), we have_

_linear mixture MG \(\subset\) MG with low self-play and adversarial GEC_

_with \(d_{\mathrm{GEC}}\) satisfying_

\[d_{\mathrm{GEC}}=16H^{3}d\log\left(1+\frac{TB^{2}}{d\epsilon H}\right).\]

Proof.: The proof of GEC bound in a linear mixture Markov game is similar to the one in the linear Markov game case. We first define the Bellman residual in expectation as

\[\mathcal{E}_{f,h}^{\pi,\nu}:=\mathbb{E}_{\pi,\nu,\mathbb{P}_{f^{*}}}\left[Q_{f, h}^{\pi,\nu}(s_{h},a_{h},b_{h})-\mathbb{E}_{s_{h+1}\sim\mathbb{P}_{f^{*},h}( \cdot|s_{h},a_{h},b_{h})}\left(r_{h}(s_{h},a_{h},b_{h})+V_{f,h+1}^{\pi,\nu}(s_{ h+1})\right)\right].\]

One can derive the following property of \(\mathcal{E}_{f,h}^{\pi,\nu}\) by precisely the same derivation as in (4), which is

\[V_{f^{t}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{\pi^{t},\nu^{t}}=\sum_{h=1}^{H}\mathcal{ E}_{f^{t},h}^{\pi^{t},\nu^{t}}.\]

We next show that in a linear mixture Markov game, the Bellman residual \(\mathcal{E}_{f,h}^{\pi,\nu}\) can be linearly represented, whose form is

\[\mathcal{E}_{f,h}^{\pi,\nu} =\mathbb{E}_{\pi,\nu,\mathbb{P}_{f^{*}}}\int_{\mathcal{S}}[\mathbb{ P}_{f,h}(s|s_{h},a_{h},b_{h})-\mathbb{P}_{f^{*},h}(s|s_{h},a_{h},b_{h})]V_{f,h+1}^{ \pi,\nu}(s)\mathrm{d}s\] \[=\mathbb{E}_{\pi,\nu,\mathbb{P}_{f^{*}}}\int_{\mathcal{S}}[\bm{ \phi}(s_{h},a_{h},b_{h},s)^{\top}(\bm{\theta}_{f,h}-\bm{\theta}_{f^{*},h})]V_{f,h+1}^{\pi,\nu}(s)\mathrm{d}s\] \[=\langle\bm{\phi}(s_{h},a_{h},b_{h},f,\pi,\nu),\bm{q}_{h}(f)\rangle,\]where we define \(\bm{\phi}(s_{h},a_{h},b_{h},f,\pi,\nu):=\mathbb{E}_{\pi,\nu,\mathbb{P}_{f}}\), \(\int_{\mathcal{S}}\bm{\phi}(s_{h},a_{h},b_{h},s^{\prime})V^{\pi,\nu}_{f,h+1}(s^{ \prime})\mathrm{d}s^{\prime}\) and \(\bm{q}_{h}(f):=\bm{\theta}_{f,h}-\bm{\theta}_{f^{*},h}\). Note that \(\|\bm{\phi}(s_{h},a_{h},b_{h},f,\pi,\nu)\|\leq H\) and \(\|\bm{q}_{h}(f)\|\leq 2B\) by Definition 5.

We next define \(\bm{\phi}_{h}^{t}:=\bm{\phi}(s_{h}^{t},a_{h}^{t},b_{h}^{t},f^{\prime},\pi^{t},\nu^{t})\), \(\bm{q}_{h}^{t}:=\bm{q}_{h}(f^{t})\), \(\Phi_{h}^{t}:=\lambda\mathrm{I}_{d}+\frac{1}{H^{2}}\sum_{t=1}^{t}\bm{\phi}_{h }^{t}(\bm{\phi}_{h}^{t})^{\top}\) and \(\varphi_{h}^{t}:=\|\bm{\phi}_{h}^{t}\|_{(\Phi_{h}^{t-1})^{-1}}\). Since we have

\[|\bm{\langle\phi}_{h}^{t},\bm{q}_{h}^{t}\rangle|^{2} =\bigg{|}\mathbb{E}_{\pi^{t},\nu^{t},\mathbb{P}_{f^{*}}}\int_{ \mathcal{S}}\big{(}\mathbb{P}_{f^{t},h}(s\,|\,s_{h}^{t},a_{h}^{t},b_{h}^{t})- \mathbb{P}_{f^{*},h}(s\,|\,s_{h}^{t},a_{h}^{t},b_{h}^{t})\big{)}V^{\pi^{+},\nu ^{t}}_{f^{*},h+1}(s)\mathrm{d}s\bigg{|}^{2}\] \[\leq H^{2}\mathbb{E}_{\pi^{t},\nu^{t},\mathbb{P}_{f^{*}}}\| \mathbb{P}_{f^{t},h}(\cdot\,|\,s_{h}^{t},a_{h}^{t},b_{h}^{t})-\mathbb{P}_{f^{* },h}(\cdot\,|\,s_{h}^{t},a_{h}^{t},b_{h}^{t})\|_{1}^{2}\] \[\leq 8H^{2}\mathbb{E}_{\pi^{t},\nu^{t},\mathbb{P}_{f^{*}}}D_{ \mathrm{He}}^{2}\big{(}\mathbb{P}_{f^{t},h}(\cdot\,|\,s_{h}^{t},a_{h}^{t},b_{h }^{t}),\mathbb{P}_{f^{*},h}(\cdot\,|\,s_{h}^{t},a_{h}^{t},b_{h}^{t})\big{)},\] (12)

where the first inequality is due to \(V^{\pi^{t},\nu^{t}}_{f^{*},h+1}(s)\leq H\) and the second inequality is due to \(\|P-Q\|_{2}^{2}\leq 8D_{\mathrm{He}}^{2}(P,Q)\). By utilizing \(\|\bm{q}_{h}(f)\|\leq 2B\) and (12), we further obtain an upper bound of \(\|\bm{q}_{h}^{t}\|_{\Phi_{h}^{t-1}}\) as follows,

\[\|\bm{q}_{h}^{t}\|_{\Phi_{h}^{t-1}} \leq\sqrt{\sum_{t=1}^{t-1}[\langle\bm{\phi}_{h}^{t},\bm{q}_{h}^{ t}\rangle]^{2}+\lambda\|\bm{q}_{h}^{t}\|_{2}^{2}}\] \[\leq\sqrt{\sum_{t=1}^{t-1}8H^{2}\mathbb{E}_{\pi^{t},\nu^{t}, \mathbb{P}_{f^{*}}}D_{\mathrm{He}}^{2}\left(\mathbb{P}_{f^{t},h}(\cdot\,|\,s_ {h}^{t},a_{h}^{t},b_{h}^{t}),\mathbb{P}_{f^{*},h}(\cdot\,|\,s_{h}^{t},a_{h}^{t },b_{h}^{t})\right)+4\lambda B^{2}}.\]

The rest of the proof is the same as the proof in the linear Markov game, from (5) to (11), except with a scaling factor. We provide the final result here as follows,

\[\left|\sum_{t=1}^{T}\left(V^{\pi^{t},\nu^{t}}_{f^{t}}-V^{\pi^{t}, \nu^{t}}_{f^{*}}\right)\right|\] \[\quad\leq\sqrt{16H^{3}d\log\left(1+\frac{T}{d\lambda}\right)\sum_{t =1}^{T}\sum_{h=1}^{H}\sum_{t=1}^{t-1}\mathbb{E}_{\pi^{t},\nu^{t},\mathbb{P}_{f ^{*}}}D_{\mathrm{He}}^{2}\left(\mathbb{P}_{f^{t},h}(\cdot\,|\,s_{h}^{t},a_{h}^ {t},b_{h}^{t}),\mathbb{P}_{f^{*},h}(\cdot\,|\,s_{h}^{t},a_{h}^{t},b_{h}^{t})\right)}\] \[\quad\quad+\lambda B^{2}T+\min\left\{2H^{2}d\log\left(1+\frac{T}{ d\lambda}\right),H^{2}T\right\}+\min\left\{\frac{3H^{2}d}{\log 2}\log\left(1+\frac{1}{\lambda\log 2} \right),H^{2}T\right\}\] \[\quad\leq\sqrt{16H^{3}d\log\left(1+\frac{T}{d\lambda}\right)\sum_{t =1}^{T}\sum_{h=1}^{H}\sum_{t=1}^{t-1}\mathbb{E}_{\pi^{t},\nu^{t},\mathbb{P}_{f ^{*}}}D_{\mathrm{He}}^{2}\left(\mathbb{P}_{f^{t},h}(\cdot\,|\,s_{h}^{t},a_{h}^ {t},b_{h}^{t}),\mathbb{P}_{f^{*},h}(\cdot\,|\,s_{h}^{t},a_{h}^{t},b_{h}^{t})\right)}\] \[\quad\quad+\lambda B^{2}T+2\left[2H^{2}d\log\left(1+\frac{T}{d \lambda}\right)\wedge H^{2}T\right],\]

Note that here \((\pi^{t},\nu^{t})\) can be any arbitrary policy pair, including the ones required as in Definitions 1 and 3. Moreover, we will see that the above result satisfies the definitions of both self-play and adversarial GEC when using the inequality \(x\wedge y\leq\sqrt{xy}\) and choosing \(\lambda=\epsilon H/B^{2}\) and \(\ell(f,\xi_{h})=D_{\mathrm{He}}^{2}\big{(}\mathbb{P}_{f,h}(\cdot\,|\,\xi_{h}), \mathbb{P}_{f^{*},h}(\cdot\,|\,\xi_{h})\big{)}\). Therefore, we have

\[d_{\mathrm{GEC}}=16H^{3}d\log\left(1+\frac{TB^{2}}{d\epsilon H}\right)\]

for \(T\) sufficiently large, which shows that linear mixture MGs with the dimension \(d\) is subsumed by MGs with self-play and adversarial GEC \(d_{\mathrm{GEC}}=16H^{3}d\log(1+\frac{TB^{2}}{d\epsilon H})\). This completes the proof. 

### Low-Witness-Rank MG

Witness rank is a complexity measure that absorbs a wide range of settings in single-agent RL[60]. The work [28] further studies the witness rank under the MG setting with self-play. In this subsection, we show that MGs with low self-play GEC subsumes the class of MGs with a low witness rank in the self-play setting. Since the witness rank under the adversarial setting was not investigated in prior works, we here only analyze the self-play setting.

Before presenting our main proposition, we first show the definitions of witnessed model misfit and witness rank in MGs with self-play [28].

**Definition 6** (Witnessed model misfit in Markov game).: _Suppose that there exists a discriminator class \(\mathcal{U}\) with \(u\in\mathcal{U}:\mathcal{S}\times\mathcal{A}\times\mathcal{B}\times\mathcal{S }\mapsto[0,H]\). The witnessed model misfit in Markov games \(\mathcal{W}(f,g,\rho,h)\) for the models \(f,g,\rho\in\mathcal{F}\) at step \(h\) is characterized by_

\[\mathcal{W}(f,g,\rho,h)= \sup_{u\in\mathcal{U}}\Big{|}\mathbb{E}_{(s_{h},a_{h},b_{h})\sim \mathbb{P}_{f^{*},\tilde{\pi},\tilde{\nu}}}\Big{[}\mathbb{E}_{s^{\prime}\sim \mathbb{P}_{\rho,h}(\cdot\,|\,s_{h},a_{h},b_{h})}u(s_{h},a_{h},b_{h},s^{\prime})\] \[\qquad-\mathbb{E}_{s_{h+1}\sim\mathbb{P}_{f^{*},h}(\cdot\,|\,s_{h},a_{h},b_{h})}u(s_{h},a_{h},b_{h},s_{h+1})\Big{]}\Big{|},\]

_where \(\tilde{\pi}:=\operatorname*{argmax}_{\pi}\min_{\nu}V_{f}^{\pi,\nu}\) and \(\tilde{\nu}:=\operatorname*{argmin}_{\nu}V_{g}^{\tilde{\pi},\nu}\) with \(f,g\in\mathcal{F}\)._

The definition of \((\tilde{\pi},\tilde{\nu})\) matches the requirement for the policy pair in the self-play GEC, i.e., condition **(1)** in Definition 1. We note that condition **(2)** in our definition of self-play GEC is designed for Algorithm 3, which is thus a symmetric condition to **(1)**. We define the Bellman residual as

\[\mathcal{E}_{f,h}^{\pi,\nu}:=\mathbb{E}_{\pi,\nu,\tilde{\nu}_{f^{*}}}\left[Q_ {f,h}^{\pi,\nu}(s_{h},a_{h},b_{h})-\mathbb{E}_{s_{h+1}\sim\mathbb{P}_{f^{*},h }(\cdot\,|s_{h},a_{h},b_{h})}\left(r_{h}(s_{h},a_{h},b_{h})+V_{f,h+1}^{\pi,\nu }(s_{h+1})\right)\right].\]

Then, we have the following definition of witness rank for MGs with self-play.

**Definition 7** (Witness Rank for Self-Play).: _There exist two functions \(\bm{W}_{h}:\mathcal{F}\times\mathcal{F}\mapsto\mathbb{R}^{d}\) and \(\bm{X}_{h}:\mathcal{F}\mapsto\mathbb{R}^{d}\) and constant \(\kappa>0\) such that_

\[\kappa_{\mathrm{wit}}|\mathcal{E}_{\rho,h}^{\tilde{\pi},\tilde{\nu}}|\leq \langle\bm{W}_{h}(f,g),\bm{X}_{h}(\rho)\rangle,\quad\mathcal{W}(f,g,\rho,h) \geq\langle\bm{W}_{h}(f,g),\bm{X}_{h}(\rho)\rangle,\]

_where \(\tilde{\pi}:=\operatorname*{argmax}_{\pi}\min_{\nu}V_{f}^{\pi,\nu}\) and \(\tilde{\nu}:=\operatorname*{argmin}_{\nu}V_{g}^{\tilde{\pi},\nu}\) with \(f,g\in\mathcal{F}\), \(\|\bm{W}_{h}(\cdot)\|_{2}\leq HB\) for a constant \(B\), and \(\|\bm{X}_{h}(\cdot)\|_{2}\leq 1\) for any step \(h\). Then, the witness rank of MGs with self-play is defined as the dimension \(d\) of the range spaces for functions \(\bm{W}\) and \(\bm{X}\)._

We show that in the self-play setting, \(d_{\mathrm{GEC}}\) is bounded for MGs with a low witness rank. It is worth noting that the proof of GEC bound for MGs with a low witness rank generalizes the proof for linear MGs in Appendix B.1.

**Proposition 5** (Low Self-Play Witness Rank \(\subset\) Low Self-play GEC).: _For a Markov game with witness rank \(d\) as defined in Definition 7, when \(T\) sufficiently large, choosing \(\ell(f,\xi_{h})=D_{\mathrm{He}}^{2}\big{(}\mathbb{P}_{f,h}(\cdot\,|\,\xi_{h}), \mathbb{P}_{f^{*},h}(\cdot\,|\,\xi_{h})\big{)}\) as in (3) with \(\xi_{h}=(s_{h},a_{h},b_{h})\), we have_

_MG with low self-play witness rank \(\subset\) MG with low self-play GEC with \(d_{\mathrm{GEC}}\) satisfying_

\[d_{\mathrm{GEC}}=\frac{4H^{3}d}{\kappa_{\mathrm{wit}}^{2}}\log\left(1+\frac{ THB^{2}}{4d\epsilon\kappa_{\mathrm{wit}}^{2}}\right).\]

Proof.: We first give a bound of the model misfit in the Hellinger distance. By \(0\leq u\leq H\), we have

\[\Big{[}\mathbb{E}_{s^{\prime}\sim\mathbb{P}_{\rho,h}(\cdot\,|\,s _{h},a_{h},b_{h})}u(s_{h},a_{h},b_{h},s^{\prime})-\mathbb{E}_{s_{h+1}\sim \mathbb{P}_{f^{*},h}(\cdot\,|\,s_{h},a_{h},b_{h})}u(s_{h},a_{h},b_{h},s_{h+1}) \Big{]}^{2}\] \[\qquad\leq H^{2}\big{\|}\mathbb{P}_{\rho,h}(\cdot\,|\,s_{h},a_{h},b_{h})-\mathbb{P}_{f^{*},h}(\cdot\,|\,s_{h},a_{h},b_{h})\big{\|}_{1}^{2}\] \[\qquad\leq 2H^{2}D_{\mathrm{He}}^{2}\big{(}\mathbb{P}_{\rho,h}( \cdot\,|\,s_{h},a_{h},b_{h}),\mathbb{P}_{f^{*},h}(\cdot\,|\,s_{h},a_{h},b_{h}) \big{)},\]

where \(\rho\) can be \(f\) or \(g\) as in the definition of self-play GEC. By the definition of \(\mathcal{W}(f,g,\rho,h)\) and Jensen's inequality, we further get

\[\mathcal{W}(f,g,\rho,h)^{2}\leq 2H^{2}\mathbb{E}_{(s_{h},a_{h},b_{h})\sim \mathbb{P}_{f^{*},\tilde{\pi},\tilde{\nu}}}D_{\mathrm{He}}^{2}\big{(}\mathbb{P}_ {\rho,h}(\cdot\,|\,s_{h},a_{h},b_{h}),\mathbb{P}_{f^{*},h}(\cdot\,|\,s_{h},a_{h },b_{h})\big{)},\] (13)

where we use \(\sup_{v}\mathbb{E}_{x}f(x,v)\leq\mathbb{E}_{x}\sup_{v}f(x,v)\) and the definition of \((\tilde{\pi},\tilde{\nu})\). Note that following our definition of self-play GEC, if we choose \(f=f^{t}\) and \(g=g^{t}\) in the model misfit, then correspondingly we have \(\tilde{\pi}=\pi^{t}\) and \(\tilde{\nu}=\nu^{t}\) as in the condition **(1)** of Definition 1.

By the definition of \(\mathcal{E}_{\rho,h}^{\pi^{t},\nu^{t}}\), we can decompose the value difference as

\[\left|\sum_{t=1}^{T}\left(V_{\rho^{t}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{\pi^{t},\nu^ {t}}\right)\right|=\left|\sum_{t=1}^{T}\sum_{h=1}^{H}\mathcal{E}_{\rho^{t},h}^ {\pi^{t},\nu^{t}}\right|.\]

Hereafter we define \(\bm{W}_{h}^{t}:=\bm{W}_{h}(f^{t},g^{t})\), \(\bm{X}_{h}^{t}:=\bm{X}_{h}(\rho^{t})\), \(\Sigma_{h}^{t}:=\lambda\mathrm{I}+\sum_{i=1}^{t}\bm{X}_{h}^{i}(\bm{X}_{h}^{i})^ {\top}\), and \(\varphi_{h}^{t}:=\|\bm{X}_{h}^{t}\|_{(\Sigma_{h}^{t})^{-1}}^{2}\). Using the definition of \(\bm{W}_{h}\) and \(\bm{X}_{h}\) and \(\mathcal{E}_{\rho^{t},h}^{\pi^{t},\nu^{t}}\leq H\), we obtain

\[\left|\sum_{t=1}^{T}\sum_{h=1}^{H}\mathcal{E}_{\rho^{t},h}^{\pi^{t },\nu^{t}}\right| \leq\sum_{t=1}^{T}\sum_{h=1}^{H}H\min\left\{\frac{1}{H\kappa_{\rm{ wit}}}\langle\bm{W}_{h}(f^{t},g^{t}),\bm{X}_{h}(\rho^{t})\rangle,1\right\}\] \[=\sum_{t=1}^{T}\sum_{h=1}^{H}H\min\left\{\frac{1}{H\kappa_{\rm{ wit}}}\langle\bm{W}_{h}(f^{t},g^{t}),\bm{X}_{h}(\rho^{t})\rangle,1\right\} \left(\bm{1}_{\{\varphi_{h}^{t-1}\leq 1\}}+\bm{1}_{\{\varphi_{h}^{t-1}>1\}}\right)\] \[\leq\underbrace{\sum_{t=1}^{T}\sum_{h=1}^{H}\frac{1}{\kappa_{\rm {wit}}}\|\bm{W}_{h}^{t}\|_{\Sigma_{h}^{t-1}}\min\left\{\|\bm{X}_{h}^{t}\|_{( \Sigma_{h}^{t-1})^{-1}},1\right\}\bm{1}_{\{\varphi_{h}^{t-1}\leq 1\}}}_{\rm Term(i)}\] \[\quad+\underbrace{H\sum_{t=1}^{T}\sum_{h=1}^{H}\bm{1}_{\{\varphi_{ h}^{t-1}>1\}}}_{\rm Term(ii)},\]

where \(\bm{1}_{\{\cdot\}}\) denotes the indicator function. For Term(ii), we can apply Lemma 16 to obtain

\[H\sum_{t=1}^{T}\sum_{h=1}^{H}\bm{1}_{\{\varphi_{h}^{t-1}>1\}}\leq\min\left\{ \frac{3H^{2}d}{\log 2}(1+\frac{1}{\lambda\log 2}),H^{2}T\right\}.\]

It remains to bound Term(i). We first calculate the sum of \(\|\bm{W}_{h}^{t}\|_{\Sigma_{h}^{t}}^{2}\), which is

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\|\bm{W}_{h}^{t-1}\|_{\Sigma_{h}^{t}}^ {2}\] \[\qquad=\sum_{t=1}^{T}\sum_{h=1}^{H}\left(\lambda\|\bm{W}_{h}^{t} \|_{2}^{2}+\sum_{t=1}^{t-1}\langle\bm{W}_{h}^{t},\bm{X}_{h}^{i}\rangle^{2}\right)\] \[\qquad\leq H^{3}TB^{2}\lambda+\sum_{t=1}^{T}\sum_{h=1}^{H}\sum_{ \iota=1}^{t-1}\langle\bm{W}_{h}^{t},\bm{X}_{h}^{i}\rangle^{2}\] \[\qquad\leq H^{3}TB^{2}\lambda+2H^{2}\sum_{t=1}^{T}\sum_{h=1}^{H} \sum_{\iota=1}^{t-1}\mathbb{E}_{(\pi^{*},\nu^{t},h)}D_{\rm{He}}^{2}\big{(} \mathbb{P}_{\rho^{\prime},h}(\cdot\,|\,s_{h},a_{h},b_{h}),\mathbb{P}_{f^{*},h}( \cdot\,|\,s_{h},a_{h},b_{h})\big{)},\]

where the first inequality uses \(\|\bm{W}_{h}^{t}\|\leq HB\), and the second inequality uses the definition of witness rank and (13). We also let \(\xi_{h}^{t}:=(s_{h}^{*},a_{h}^{t},b_{h}^{t})\). We next use Lemma 15 to obtain

\[\sum_{h=1}^{H}\sum_{t=1}^{T}\min\{\varphi_{h}^{t},1\}\leq\min\left\{2Hd\log \left(1+\frac{T}{d\lambda}\right),HT\right\}.\]

[MISSING_PAGE_EMPTY:24]

Here we consider undercomplete POMGs where there are more observations than hidden states, \(|\mathcal{O}|\geq|\mathcal{S}|\). Formally, it can be defined as an \(\alpha\)-revealing POMG as follows:

**Definition 8** (\(\alpha\)-Revealing POMG [41]).: _A POMG is \(\alpha\)-revealing if it satisfies that there exists \(\alpha>0\) such that \(\min_{h}\sigma_{S}(\mathbb{O}_{h})\geq\alpha\), where \(\sigma_{S}(\cdot)\) denotes the \(S\)-th singular value of a matrix with \(S=|\mathcal{S}|\)._

The \(\alpha\)-revealing condition measures how hard it is to recover states from observations. Before presenting the GEC bound for an \(\alpha\)-revealing POMG, we first define some notations. We define \(\tau_{h:h^{\prime}}:=(o_{i},a_{i},b_{i})_{i=h}^{h^{\prime}}\) to be a set of observation-action tuples from step \(h\) to \(h^{\prime}\). For simplicity, we denote \(\tau_{1:h}\) as \(\tau_{h}\). We define \(\bm{q}_{0}:=\mathbb{O}_{1}\bm{\mu}_{1}\in\mathbb{R}^{|\mathcal{O}|}\), where \(\bm{\mu}_{1}\in\mathbb{R}^{|\mathcal{S}|}\) is a vector formed by the distribution of the initial state. We define \(\bm{q}(\tau_{h}):=[\Pr\big{(}o_{1:h},o_{h+1}\,|\,\mathbf{do}(a_{1:h},b_{1:h}) \big{)}]_{o\in\mathcal{O}}\in\mathbb{R}^{|\mathcal{O}|}\) when \(\tau_{h}=(o_{i},a_{i},b_{i})_{i=1}^{h}\). Here \(\mathbf{do}(a_{1:h},b_{1:h})\) means executing actions \(a_{h^{\prime}}\) and \(b_{h^{\prime}}\) at step \(h^{\prime}\). In other words, \(\Pr\big{(}o_{1:h},o_{h+1}\,|\,\mathbf{do}(a_{1:h},b_{1:h})\big{)}\) describes the probability of receiving observations \(o_{1:h}\) given the actions fixed as \((a_{1:h-1},b_{1:h-1})\). We define \(\bar{\bm{q}}(\tau_{h}):=\bm{q}(\tau_{h})/\mathbf{P}(\tau_{h})\), which is the concatenation of the probabilities under condition \(\tau_{h}\). We have \(\bar{\bm{q}}(\tau_{h})=[\Pr(o\,|\,\tau_{1:h})]_{o\in\mathcal{O}}\), where \(\Pr(o\,|\,\tau_{1:h})\) means the probability of receiving \(o_{h+1}=o\) under the condition that the previous trajectory is \(\tau_{h}\).

Based on the above definitions, we then derive the GEC bound for an \(\alpha\)-revealing POMG in the following proposition.

**Proposition 6** (\(\alpha\)-Revealing POMG \(\subset\) Low Self-Play/Adversarial GEC).: _For an \(\alpha\)-revealing POMG defined in Definition 8, with the loss \(\ell(f,\xi_{h})\) defined in (3) for POMGs, we have_

\[\alpha\]

_-revealing POMG \(\subset\) MG with low self-play and adversarial GEC with \[d_{\mathrm{GEC}}=O\left(\frac{H^{3}|\mathcal{O}|^{3}|\mathcal{A}|^{2}| \mathcal{B}|^{2}|\mathcal{S}|^{2}\varsigma}{\alpha^{4}}\right),\]_

_where \(\varsigma=2\log\Big{(}1+\frac{4|\mathcal{A}|^{2}|\mathcal{B}|^{2}|\mathcal{O} |^{2}|\mathcal{S}|^{2}}{\alpha^{2}}\Big{)}\)._

For our proof of this proposition, we prove several supporting lemmas presented in Appendix B.4.1.

Proof.: For ease of notation, we let \(S=|\mathcal{S}|\), \(A=|\mathcal{A}|\), \(B=|\mathcal{B}|\), and \(O=|\mathcal{O}|\) in our proof. We define the following observed operator representation:

\[\bm{M}_{h}(o,a,b):=\mathbb{O}_{h+1}\mathbb{P}_{h,a,b}\mathrm{diag}\big{(} \mathbb{O}_{h}(o\,|\,\cdot)\big{)}\mathbb{O}_{h}^{\dagger}\quad\in\mathbb{R}^{ O\times O}\]

for \(1\leq h\leq H\). One can verify that the previously defined \(\bm{M}_{h}(o,a,b)\) has the following property,

\[\bm{q}(\tau_{h})=\bm{M}_{h}(o_{h},a_{h},b_{h})\bm{M}_{h-1}(o_{h-1},a_{h-1},b_{h -1})\ldots\bm{M}_{1}(o_{1},a_{1},b_{1})\bm{q}_{0}.\]

Specifically, \(\mathbf{P}(\tau_{H})=\bm{M}_{H}(o_{H},a_{H},b_{H})\ldots\bm{M}_{1}(o_{1},a_{1}, b_{1})\bm{q}_{0}\). For simplicity, we define

\[\bm{M}_{h^{\prime}:h}(o_{h:h^{\prime}},a_{h:h^{\prime}},b_{h:h^{\prime}}):=\bm{ M}_{h^{\prime}}(o_{h^{\prime}},a_{h^{\prime}},b_{h^{\prime}})\bm{M}_{h^{ \prime}-1}(o_{h^{\prime}-1},a_{h^{\prime}-1},b_{h^{\prime}-1})\ldots\bm{M}_{h} (o_{h},a_{h},b_{h}).\]

We also rewrite \(\bm{M}_{H:h+1}(o_{h+1:H},a_{h+1:H},b_{h+1:H})\) as \(\bm{m}(o_{h+1:H},a_{h+1:H},b_{h+1:H})\) to emphasize that \(\bm{M}_{H:h+1}(o_{h+1:H},a_{h+1:H},b_{h+1:H})\) is a vector since \(\bm{M}_{H}(o_{H},a_{H},b_{H})\) is a vector.

We start by using Lemma 5 to decompose the regret as follows

\[\left|\sum_{t=1}^{T}\Big{(}V_{f^{t}}^{\pi^{t},\nu^{*}}-V_{f^{*}}^ {\pi^{t},\nu^{*}}\Big{)}\right|\] \[\leq\sum_{t=1}^{T}\frac{H\sqrt{S}}{\alpha}\Big{[}\sum_{h=1}^{H} \sum_{\tau_{h}}\|(\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{h}) )\bm{q}(\tau_{h-1})\|_{1}\sigma^{t}(\tau_{h})+\|\bm{q}_{0}^{t}-\bm{q}_{0}\|_{1} \Big{]}.\]Combine this with the fact that \(V_{f^{t}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{\pi^{t},\nu^{t}}\leq H\),we have

\[\left|\sum_{t=1}^{T}\Big{(}V_{f^{t}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{ \pi^{t},\nu^{t}}\Big{)}\right|\] \[\leq\sum_{t=1}^{T}\frac{H\sqrt{S}}{\alpha}\Big{[}\sum_{h=1}^{H} \min\Big{\{}\frac{\alpha}{\sqrt{S}},\sum_{\tau_{h}}\|(\bm{M}_{h}^{t}(o_{h},a_{ h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{h}))\bm{q}(\tau_{h-1})\|_{1}\sigma^{t}( \tau_{h})\Big{\}}\] \[\qquad\qquad\qquad\qquad+\|\bm{q}_{0}^{t}-\bm{q}_{0}\|_{1}\Big{]}.\]

where we define

\[\pi(\tau_{h})=\prod_{h^{\prime}=1}^{h}\pi_{h^{\prime}}(a_{h^{\prime}}\,|\,\tau _{h^{\prime}-1},o_{h^{\prime}}),\quad\nu(\tau_{h})=\prod_{h^{\prime}=1}^{h} \nu_{h^{\prime}}(b_{h^{\prime}}\,|\,\tau_{h^{\prime}-1},o_{h^{\prime}}),\quad \sigma^{t}(\tau_{h})=\pi^{t}(\tau_{h})\nu^{t}(\tau_{h}),\]

and \(M_{h}^{t}\) and \(q_{0}^{t}\) correspond to the model \(f^{t}\) sampled in time step \(t\).

We next focus on \(\sum_{\tau_{h}}\|(\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{ h}))\bm{q}(\tau_{h-1})\|_{1}\sigma^{t}(\tau_{h})\). By the definition of \(\bar{\bm{q}}\) and \(\bm{q}\), we obtain

\[\sum_{\tau_{h}}\|\big{(}\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})-\bm{M}_ {h}(o_{h},a_{h},b_{h})\big{)}\bm{q}(\tau_{h-1})\|_{1}\sigma(\tau_{h})\] \[=\operatorname*{\mathbb{E}}_{\tau_{h-1}\sim\bm{\mathsf{P}}_{f^{* },h-1}^{\pi^{t},\nu^{t}}}\sum_{o_{h},a_{h},b_{h}}\sum_{j=1}^{O}|\widetilde{ \bm{e}}_{j}^{\top}(\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{ h}))\bar{\bm{q}}(\tau_{h-1})|\sigma^{t}(o_{h},a_{h},b_{h};\tau_{h-1})\] \[=\operatorname*{\mathbb{E}}_{\tau_{h-1}\sim\bm{\mathsf{P}}_{f^{* },h-1}^{\pi^{t},\nu^{t}}}\sum_{o_{h},a_{h},b_{h}}\sum_{j=1}^{O}|\widetilde{\bm {e}}_{j}^{\top}(\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{h}) )\mathbb{O}_{h-1}\mathbb{O}_{h-1}^{\dagger}\] \[\qquad\qquad\qquad\qquad\qquad\cdot\bar{\bm{q}}(\tau_{h-1})\sigma ^{t}(o_{h},a_{h},b_{h};\tau_{h-1})\Big{|},\]

where \(\widetilde{\bm{e}}_{j}\) denotes the \(j\)-th standard basis vector in \(\mathbb{R}^{O}\), and \(\sigma^{t}(\tau_{h:h^{\prime}};\tau_{h-1}):=\pi^{t}(\tau_{h:h^{\prime}};\tau_{ h-1})\cdot\nu^{t}(\tau_{h:h^{\prime}};\tau_{h-1})\), where \(\pi^{t}(\tau_{h:h^{\prime}};\tau_{h-1})=\prod_{h^{\prime}=h}^{h^{\prime}}\pi_{h ^{\prime}}^{t}(a_{h^{\prime\prime}}\,|\,\tau_{h^{\prime\prime}-1},o_{h^{\prime \prime}})\), \(\nu^{t}(\tau_{h:h^{\prime}};\tau_{h-1})=\prod_{h^{\prime\prime}=h}^{h^{\prime}} \nu_{h^{\prime\prime}}^{t}(b_{h^{\prime\prime}}\,|\,\tau_{h^{\prime\prime}-1}, o_{h^{\prime\prime}})\). For simplicity, we define \(w_{h,t,j,o,a,b}:=(\widetilde{\bm{e}}_{j}^{\top}(\bm{M}_{h}^{t}(o,a,b)-\bm{M}_{h}(o,a,b)) \mathbb{O}_{h-1})^{\top}\) and \(x_{\tau_{h-1},\sigma,o,a,b}:=\mathbb{O}_{h-1}^{\dagger}\bar{\bm{q}}(\tau_{h-1}) \sigma(o,a,b;\tau_{h-1})\). Then, we have

\[\operatorname*{\mathbb{E}}_{\tau_{h-1}\sim\bm{\mathsf{P}}_{f^{* },h-1}^{\pi^{t},\nu^{t}}}\sum_{o_{h},a_{h},b_{h}}\sum_{j=1}^{O}|\widetilde{\bm {e}}_{j}^{\top}(\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{h}) )\mathbb{O}_{h-1}\mathbb{O}_{h-1}^{\dagger}\bar{\bm{q}}(\tau_{h-1})\sigma^{t}(o _{h},a_{h},b_{h};\tau_{h-1})\] \[=\sum_{o,a,b}\operatorname*{\mathbb{E}}_{\tau_{h-1}\sim\bm{ \mathsf{P}}_{f^{*},h-1}^{\pi^{t},\nu^{t}}}\sum_{j=1}^{O}|w_{h,t,j,o,a,b}^{\top} x_{\tau_{h-1},\sigma^{t},o,a,b}|.\]

Next, we will apply the \(\ell_{2}\) eluder technique [13, 83] in Lemma 17 to derive the upper bound. We first analyze the upper bounds of \(\sum_{j=1}^{O}\|w_{h,t,j,o,a,b}\|_{2}\) and \(\operatorname*{\mathbb{E}}_{\tau_{h-1}\sim\bm{\mathsf{P}}_{f^{*},h-1}^{\pi^{t}, \nu^{t}}}\|x_{\tau_{h-1},\sigma^{t},o,a,b}\|_{2}^{2}\) for the usage of the \(\ell_{2}\) eluder technique. According to Lemma 7, for any \((o,a,b)\in\mathcal{O}\times\mathcal{A}\times\mathcal{B}\), we have

\[\operatorname*{\mathbb{E}}_{\tau_{h-1}\sim\bm{\mathsf{P}}_{f^{*},h-1}^{\pi^{t}, \nu^{t}}}\|x_{\tau_{h-1},\sigma^{t},o,a,b}\|_{2}^{2}\leq S,\qquad\sum_{j=1}^{O} \|w_{h,t,j,o,a,b}\|_{2}\leq\frac{2ABd\sqrt{S}}{\alpha}.\]According to Lemma 17, setting \(R=\frac{\alpha}{\sqrt{S}}\) and \(R_{x}^{2}R_{w}^{2}\leq\frac{4A^{2}B^{2}O^{2}S^{2}}{\alpha^{2}}\) in the new \(\ell_{2}\) eluder technique, we can obtain

\[\sum_{t=1}^{T}\min\Big{\{}\frac{\alpha}{\sqrt{S}},\mathop{\mathbb{ E}}_{\tau_{h-1}\sim\mathbf{P}_{f^{*},h-1}^{t,t^{t}}}\sum_{j=1}^{O}\sum_{\alpha,a,b}|w_{h,t,j,o,a,b}^{\top}x_{\tau_{h-1},\sigma^{t},o,a,b}|\Big{\}}\] \[\leq\sum_{o_{h},a_{h},b_{h}}\Bigg{\{}O\varsigma\Big{[}\frac{ \alpha^{2}}{S}T+\sum_{t=1}^{T}\sum_{t=1}^{t-1}\mathop{\mathbb{E}}_{\tau_{h-1} \sim\mathbf{P}_{f^{*},h-1}^{\pi^{t},\mu^{t}}}\big{(}\|\big{(}\boldsymbol{M}_{h }^{t}(o_{h},a_{h},b_{h})-\boldsymbol{M}_{h}(o_{h},a_{h},b_{h})\big{)}\bar{ \boldsymbol{q}}(\tau_{h-1})\|_{1}\] \[\qquad\qquad\cdot\sigma^{t}(o_{h},a_{h},b_{h};\tau_{h-1})\big{)} ^{2}\Big{]}\Bigg{\}}^{\frac{1}{2}},\]

where \(\varsigma=2\log\left(1+\frac{4A^{2}B^{2}O^{2}S^{2}}{\alpha^{2}}\right)\). By Jensen's inequality, the last term above is further relaxed as

\[\Bigg{\{}O^{2}AB\varsigma\Big{[}\frac{OAB\alpha^{2}}{S}T+\sum_{t=1 }^{T}\sum_{i=1}^{t-1}\mathop{\mathbb{E}}_{\tau_{h-1}\sim\mathbf{P}_{f^{*},h-1 }^{\pi^{t},\mu^{t}}}\big{(}\sum_{o_{h},a_{h},b_{h}}\|\big{(}\boldsymbol{M}_{h }^{t}(o_{h},a_{h},b_{h})-\boldsymbol{M}_{h}(o_{h},a_{h},b_{h})\big{)}\bar{ \boldsymbol{q}}(\tau_{h-1})\|_{1}\] \[\quad\cdot\sigma^{t}(o_{h},a_{h},b_{h};\tau_{h-1})\big{)}^{2} \Big{]}\Bigg{\}}^{\frac{1}{2}}.\]

Then, invoking Lemma 6, we obtain

\[\sum_{t=1}^{T}\min\Big{\{}\frac{\alpha}{\sqrt{S}},\mathop{ \mathbb{E}}_{\tau_{h-1}\sim\mathbf{P}_{f^{*},h-1}^{\pi^{t},\mu^{t}}}\sum_{j=1 }^{O}\sum_{o,a,b}|w_{h,t,j,o,a,b}^{\top}x_{\tau_{h-1},\sigma^{t},o,a,b}|\Big{\}}\] \[\qquad\lesssim\Bigg{[}O^{2}AB\varsigma\Big{(}\frac{OAB\alpha^{2} }{S}T+\frac{S}{\alpha^{2}}\sum_{t=1}^{T}\sum_{\iota=1}^{t-1}D_{\mathrm{He}}^{2 }\big{(}\mathbf{P}_{f^{\prime},h}^{\pi^{t},\mu^{t}},\mathbf{P}_{f^{*},h}^{\pi^ {t},\nu^{t}}\big{)}\Big{)}\Bigg{]}^{\frac{1}{2}},\] (14)

We next focus on \(\|\boldsymbol{q}_{0}^{t}-\boldsymbol{q}_{0}\|_{1}\). Using that \(\|\boldsymbol{q}_{0}^{t}-\boldsymbol{q}_{0}\|_{1}\leq 1\) and \(2(t-1)\geq t\) when \(t\geq 2\), we obtain

\[\sum_{t=1}^{T}\|\boldsymbol{q}_{0}^{t}-\boldsymbol{q}_{0}\|_{1} \leq 1+\sum_{t=2}^{T}\Big{[}\frac{2(t-1)}{t}\|\boldsymbol{q}_{0}^{ t}-\boldsymbol{q}_{0}\|_{1}^{2}\Big{]}^{\frac{1}{2}}\] \[\lesssim 1+\Big{[}\sum_{t=2}^{T}\frac{2}{t}\Big{]}^{\frac{1}{2}} \Big{[}\sum_{t=2}^{T}(t-1)D_{\mathrm{He}}^{2}(\boldsymbol{q}_{0}^{t}, \boldsymbol{q}_{0})\Big{]}^{\frac{1}{2}}\] \[\lesssim 1+\Big{[}\log T\sum_{t=1}^{T}\sum_{\iota=1}^{t-1}D_{ \mathrm{He}}^{2}(\boldsymbol{q}_{0}^{t},\boldsymbol{q}_{0})\Big{]}^{\frac{1}{2}}.\] (15)

Thus, by invoking (14) and (15), we get the regret bound by

\[\left|\sum_{t=1}^{T}\Big{(}V_{f^{*}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^ {\pi^{t},\nu^{t}}\Big{)}\right|\] \[\qquad\lesssim H\Bigg{(}OABH\sqrt{O\varsigma T}+\sqrt{\frac{O^{2} AB\varsigma S^{2}H}{\alpha^{4}}\sum_{h=0}^{H}\sum_{t=1}^{T}\sum_{\iota=1}^{t-1}D_{ \mathrm{He}}^{2}(\mathbf{P}_{f^{*},h}^{\pi^{t},\nu^{t}},\mathbf{P}_{f^{*},h}^ {\pi^{t},\nu^{t}})}\Bigg{)}\] \[\qquad\lesssim\Bigg{[}\frac{O^{2}AB\varsigma S^{2}H^{3}}{\alpha^ {4}}\sum_{h=0}^{H}\sum_{t=1}^{T}\sum_{\iota=1}^{t-1}D_{\mathrm{He}}^{2}( \mathbf{P}_{f^{*},h}^{\pi^{t},\nu^{t}},\mathbf{P}_{f^{*},h}^{\pi^{t},\nu^{t}}) \Bigg{]}^{\frac{1}{2}}+(O^{3}A^{2}B^{2}H^{3}\varsigma\cdot HT)^{\frac{1}{2}}.\]

Since the above derivation is for any policy pair \((\pi^{t},\nu^{t})\), we can show that \(\alpha\)-revealing POMG is subsumed by the classes of both self-play and adversarial GEC class with a bounded \(d_{\mathrm{GEC}}\). We finally prove

\[d_{\mathrm{GEC}}=O\left(\frac{O^{3}A^{2}B^{2}H^{3}S^{2}\varsigma}{\alpha^{4}} \right),\]

where \(\varsigma=2\log\left(1+\frac{4A^{2}B^{2}O^{2}S^{2}}{\alpha^{2}}\right)\). This completes the proof.

#### b.4.1 Lemmas for Proof of Proposition 6

Here, we present and prove all the lemmas used for the proof of Proposition 6. All the lemmas presented here follow the notations of Proposition 6.

**Lemma 3**.: _In an \(\alpha\)-revealing POMG, for any policy pair \((\pi,\nu)\), any vector \(x\in\mathbb{R}^{O}\), and any \(\tau_{h-1}\), we have_

\[\sum_{\tau_{h:H}}|\bm{m}(\tau_{h:H})x|\sigma(\tau_{h:H};\tau_{h-1})\leq\frac{ \sqrt{S}}{\alpha}\|x\|_{1}.\]

Proof.: We first obtain

\[\sum_{\tau_{h:H}}|\bm{m}(\tau_{h:H})x|\sigma(\tau_{h:H};\tau_{h-1})\] \[=\sum_{\tau_{h:H}}|\bm{m}(\tau_{h:H})\mathbb{O}_{h}\mathbb{O}_{h}^ {\dagger}x|\sigma(\tau_{h:H};\tau_{h-1})\] \[\leq\sum_{i=1}^{S}\sum_{\tau_{h:H}}|\bm{m}(\tau_{h:H})\mathbb{O} _{h}\bm{e}_{i}|\cdot|\bm{e}_{i}^{\top}\mathbb{O}_{h}^{\dagger}x|\sigma(\tau_ {h:H};\tau_{h-1}),\]

where \(\bm{e}_{i}\in\mathbb{R}^{S}\) denotes the standard basis vector whose \(i\)-th element is \(1\) and \(0\) for others. Note that \(\sum_{\tau_{h:H}}|\bm{m}(\tau_{h:H})\mathbb{O}_{h}\bm{e}_{i}|\sigma(\tau_{h:H };\tau_{h-1})=\sum_{\tau_{h:H}}\Pr^{\sigma}(\tau_{h:H}\,|\,\tau_{1:h-1},s_{i} )=1\). Thus we obtain

\[\sum_{i=1}^{S}\sum_{\tau_{h:H}}|\bm{m}(\tau_{h:H})\mathbb{O}_{h}\bm{e}_{i}| \cdot|\bm{e}_{i}^{\top}\mathbb{O}_{h}^{\dagger}x|\sigma(\tau_{h:H};\tau_{h-1})\]

\[=\|\mathbb{O}_{h}^{\dagger}x\|_{1}\leq\|\mathbb{O}_{h}^{\dagger}\|_{1}\cdot\|x \|_{1}\leq\sqrt{S}\|\mathbb{O}_{h}^{\dagger}\|_{2}\cdot\|x\|_{1}\leq\frac{ \sqrt{S}}{\alpha}\|x\|_{1},\]

which completes the proof. 

**Lemma 4**.: _In an \(\alpha\)-revealing POMG, for any policy pair \((\pi,\nu)\) and any \(x\in\mathbb{R}^{O}\), we have_

\[\sum_{(o_{h},a_{h},b_{h})\in\mathcal{O}\times\mathcal{A}\times\mathcal{B}}\| \bm{M}_{h}(o_{h},a_{h},b_{h})x\|_{1}\sigma(o_{h},a_{h},b_{h})\leq\frac{\sqrt{S }}{\alpha}\|x\|_{1}.\]

Proof.: We first obtain

\[\sum_{(o_{h},a_{h},b_{h})\in\mathcal{O}\times\mathcal{A}\times \mathcal{B}}\|\bm{M}_{h}(o_{h},a_{h},b_{h})x\|_{1}\sigma(o_{h},a_{h},b_{h})\] \[=\sum_{o_{h},a_{h},b_{h}}\sum_{j=1}^{O}|\widetilde{\bm{e}}_{j}^{ \top}\bm{M}_{h}(o_{h},a_{h},b_{h})x|\sigma(o_{h},a_{h},b_{h})\] \[\leq\sum_{o_{h},a_{h},b_{h}}\sum_{j=1}^{O}\sum_{i=1}^{S}| \widetilde{\bm{e}}_{j}^{\top}\bm{M}_{h}(o_{h},a_{h},b_{h})\mathbb{O}_{h}\bm{ e}_{i}|\cdot|\bm{e}_{i}^{\top}\mathbb{O}_{h}^{\dagger}x|\sigma(o_{h},a_{h},b_{h}),\]

where \(\widetilde{\bm{e}}_{j}\) is the \(j\)-th standard basis vector in \(\mathbb{R}^{O}\) and \(\bm{e}_{i}\) is the \(i\)-th standard basis vector in \(\mathbb{R}^{S}\). Note that we have

\[\sum_{o_{h},a_{h},b_{h}}\sum_{j=1}^{O}|\widetilde{\bm{e}}_{j}^{ \top}\bm{M}_{h}(o_{h},a_{h},b_{h})\mathbb{O}_{h}\bm{e}_{i}|\sigma(o_{h},a_{h},b _{h})\] \[=\sum_{o_{h},a_{h},b_{h}}\sum_{j=1}^{O}\Pr(o_{j},o_{h},a_{h},b_{h} \,|\,s_{i})\sigma(o_{h},a_{h},b_{h})\] \[=\sum_{o_{h},a_{h},b_{h}}\Pr(o_{h},a_{h},b_{h}\,|\,s_{i})\sigma(o_ {h},a_{h},b_{h})\] \[\leq\sum_{o_{h},a_{h},b_{h}}\sigma(o_{h},a_{h},b_{h})=1.\]Combining the above result, we obtain

\[\sum_{o_{h},a_{h},b_{h}}\sum_{j=1}^{O}\sum_{i=1}^{S}|\widetilde{ \bm{e}}_{j}^{\top}\bm{M}_{h}(o_{h},a_{h},b_{h})\mathbb{O}_{h}\bm{e}_{i}|\cdot| \bm{e}_{i}^{\top}\mathbb{O}_{h}^{\dagger}x|\sigma(o_{h},a_{h},b_{h})\] \[\qquad\leq\sum_{i=1}^{S}|\bm{e}_{i}^{\top}\mathbb{O}_{h}^{\dagger }x|\] \[\qquad=\|\mathbb{O}_{h}^{\dagger}x\|_{1}\leq\|\mathbb{O}_{h}^{ \dagger}\|_{1}\cdot\|x\|_{1}\leq\sqrt{S}\|\mathbb{O}_{h}^{\dagger}\|_{2}\cdot \|x\|_{1}\leq\frac{\sqrt{S}}{\alpha}\|x\|_{1},\]

which completes the proof. 

**Lemma 5**.: _The value difference in a POMG can be decomposed as_

\[V_{f^{t}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{\pi^{t},\nu^{t}}\leq\frac{ H\sqrt{S}}{\alpha}\sum_{h=1}^{H}\Big{[}\sum_{\tau_{h}}\|(\bm{M}_{h}^{t}-\bm{M}_{h}) \bm{q}(\tau_{h-1})\|_{1}\sigma^{t}(\tau_{h})+\|q_{0}^{t}-q_{0}\|_{1}\Big{]}.\]

Proof.: Denoting \(r(\tau_{H})\) as the sum of rewards from \(h=1\) to \(H\) on \(\tau_{H}\), we obtain

\[V_{f^{t}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{\pi^{t},\nu^{t}}\] \[\qquad=\sum_{\tau_{H}}\Big{(}\mathbf{P}_{f^{t}}^{\pi^{t},\nu^{t}} (\tau_{H})-\mathbf{P}_{f^{*}}^{\pi^{t},\nu^{t}}(\tau_{H})\Big{)}r(\tau_{H})\] \[\qquad\leq H\sum_{\tau_{H}}|\mathbf{P}_{f^{t}}^{\pi^{t},\nu^{t}} (\tau_{H})-\mathbf{P}_{f^{*}}^{\pi^{t},\nu^{t}}(\tau_{H})|\] \[\qquad=H\sum_{\tau_{H}}|\bm{M}_{H:1}^{t}(o_{1:H},a_{1:H},b_{1:H} )\bm{q}_{0}^{t}-\bm{M}_{H:1}(o_{1:H},a_{1:H},b_{1:H})\bm{q}_{0}|\sigma^{t}(\tau _{H}).\]

Using the triangle inequality, we further obtain

\[H\sum_{\tau_{H}}|\bm{M}_{H:1}^{t}(o_{1:H},a_{1:H},b_{1:H})\bm{q }_{0}^{t}-\bm{M}_{H:1}(o_{1:H},a_{1:H},b_{1:H})\bm{q}_{0}|\sigma^{t}(\tau_{H})\] \[\qquad\leq H\sum_{\tau_{H}}\Big{[}\sum_{h=1}^{H}|\bm{m}^{t}(\tau_{ h+1:H})\big{(}\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{h}) \big{)}\bm{q}(\tau_{h-1})|\sigma^{t}(\tau_{H})\] \[\qquad\quad+|\bm{m}^{t}(\tau_{H})(\bm{q}_{0}^{t}-\bm{q}_{0})| \sigma^{t}(\tau_{H})\Big{]},\]

where \(\bm{m}(\tau_{h+1:H})=\bm{M}_{H:h+1}(\tau_{h+1:H})\) and \(\bm{q}(\tau_{h-1})=\bm{M}_{h-1:1}(\tau_{h-1})\bm{q}_{0}\). Note that Lemma 3 shows that

\[\sum_{\tau_{h+1:H}}|\bm{m}^{t}(\tau_{h+1:H})\big{(}\bm{M}_{h}^{t}( o_{h},a_{h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{h})\big{)}\bm{q}(\tau_{h-1})| \sigma^{t}(\tau_{h+1:H};\tau_{1:h})\] \[\qquad\leq\frac{\sqrt{S}}{\alpha}\|\big{(}\bm{M}_{h}^{t}(o_{h},a_ {h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{h})\big{)}\bm{q}(\tau_{h-1})\|_{1}\]

and

\[\sum_{\tau_{H}}|\bm{m}^{t}(\tau_{H})(\bm{q}_{0}^{t}-\bm{q}_{0})| \sigma^{t}(\tau_{H})\leq\frac{\sqrt{S}}{\alpha}\|\bm{q}_{0}^{t}-\bm{q}_{0}\|_{1}.\]Thus we obtain

\[H \sum_{\tau_{H}}\Big{[}\sum_{h=1}^{H}|\bm{m}^{t}(\tau_{h+1:H})\big{(} \bm{M}_{h}^{t}(o_{h},a_{h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{h})\big{)}\bm{q}( \tau_{h-1})|\sigma^{t}(\tau_{H})\] \[+|\bm{m}^{t}(\tau_{H})(\bm{q}_{0}^{t}-\bm{q}_{0})|\sigma^{t}(\tau_ {H})\Big{]}\] \[=H\Big{[}\sum_{h=1}^{H}\sum_{\tau_{H}}|\bm{m}^{t}(\tau_{h+1:H}) \big{(}\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{h})\big{)} \bm{q}(\tau_{h-1})|\sigma^{t}(\tau_{H})\] \[\quad+\sum_{\tau_{H}}|\bm{m}^{t}(\tau_{H})(\bm{q}_{0}^{t}-\bm{q}_ {0})|\sigma^{t}(\tau_{H})\Big{]}\] \[\leq\frac{H\sqrt{S}}{\alpha}\sum_{h=1}^{H}\Big{[}\sum_{\tau_{h}} \|(\bm{M}_{h}^{t}-\bm{M}_{h})\bm{q}(\tau_{h-1})\|_{1}\sigma(\tau_{h})+\|\bm{q}_ {0}^{t}-\bm{q}_{0}\|_{1}\Big{]},\]

which completes the proof. 

The next lemma gives the bound of training error by Hellinger distances.

**Lemma 6**.: _For any model \(f^{t}\) and policy \(\sigma^{t}\), we have_

\[\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\mathbb{E}_{\tau_{h-1}\sim\mathbf{P}_{f^{*}, h-1}^{\pi^{*},\omega^{t}}}\Big{[}\sum_{o_{h},a_{h},b_{h}}\|\big{(}\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{h})\big{)}\bar{\bm{q}}(\tau_{h-1})\|_{ 1}\sigma^{t}(o_{h},a_{h},b_{h};\tau_{h-1})\Big{]}^{2}\] \[\qquad\lesssim\frac{S}{\alpha^{2}}\sum_{\iota=1}^{t-1}\sum_{h=1}^ {H}D_{\mathrm{He}}^{2}(\mathbf{P}_{f^{\prime},h}^{\pi^{*},\nu^{t}},\mathbf{P} _{f^{\prime},h}^{\pi^{*},\nu^{t}}).\]

_where \(\lesssim\) omits absolute constants._

Proof.: By using \((a+b)^{2}\leq 2a^{2}+2b^{2}\) and the triangle inequality, we decompose the LHS into two parts to bound them separately. We have

\[\sum_{\iota=1}^{t-1}\mathbb{E}_{\tau_{h-1}\sim\mathbf{P}_{f^{*}, h-1}^{\pi^{*},\omega^{t}}}\Big{[}\sum_{o_{h},a_{h},b_{h}}\|\big{(}\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})-\bm{M}_{h}(o_{h},a_{h},b_{h})\big{)}\bar{\bm{q}}(\tau_{h-1})\|_{ 1}\sigma^{t}(o_{h},a_{h},b_{h};\tau_{h-1})\Big{]}^{2}\] \[\qquad\leq 2\sum_{\iota=1}^{t-1}\underbrace{\Big{[}\sum_{\tau_{h}} \|\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})\bar{\bm{q}}^{t}(\tau_{h-1})-\bm{M}_{h}(o_{h },a_{h},b_{h})\bar{\bm{q}}(\tau_{h-1})\|_{1}\mathbf{P}(\tau_{h-1})\sigma^{t}( \tau_{h})\Big{]}^{2}}_{\mathrm{Term(i)}}\] \[\qquad+2\sum_{\iota=1}^{t-1}\underbrace{\Big{[}\sum_{\tau_{h}}\| \bm{M}_{h}^{t}(o_{h},a_{h},b_{h})\big{(}\bar{\bm{q}}^{t}(\tau_{h-1})-\bar{\bm{q }}(\tau_{h-1})\big{)}\|_{1}\mathbf{P}(\tau_{h-1})\sigma^{t}(\tau_{h})\Big{]}^{2} }_{\mathrm{Term(ii)}}.\]

For Term(i), we equivalently rewrite this term as

\[\Big{[}\sum_{\tau_{h}}\|\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})\bar{\bm {q}}^{t}(\tau_{h-1})-\bm{M}_{h}(o_{h},a_{h},b_{h})\bar{\bm{q}}(\tau_{h-1})\|_{ 1}\mathbf{P}(\tau_{h-1})\sigma^{t}(\tau_{h})\Big{]}^{2}\] \[\qquad=\Big{[}\operatorname*{\mathbb{E}}_{\tau_{h-1}\sim\mathbf{P}_ {f^{*},h-1}^{\pi^{*},\omega^{t}}}\sum_{o_{h}}\sum_{a_{h},b_{h}}\|\bm{M}_{h}^{t} (o_{h},a_{h},b_{h})\bar{\bm{q}}^{t}(\tau_{h-1})-\bm{M}_{h}(o_{h},a_{h},b_{h}) \bar{\bm{q}}(\tau_{h-1})\|_{1}\] \[\qquad\qquad\cdot\sigma^{t}(o_{h},a_{h},b_{h};\tau_{h-1})\Big{]}^{2}.\]We first consider the case when \(h<H\). According to the definitions of \(\bar{\bm{q}}_{h}^{t}\) and \(\bar{\bm{q}}_{h}\), we obtain

\[\text{Term(i)}= \Big{[}\operatorname*{\mathbb{E}}_{\tau_{h-1}\sim\operatorname*{ \mathbb{P}}_{f^{*},h-1}^{\pi^{i},\nu^{i}}}\sum_{o_{h}}\sum_{a_{h},b_{h}}\|\bm{M }_{h}^{t}(o_{h},a_{h},b_{h})\bar{\bm{q}}^{t}(\tau_{h-1})-\bm{M}_{h}(o_{h},a_{h}, b_{h})\bar{\bm{q}}(\tau_{h-1})\|_{1}\] \[\qquad\qquad\cdot\sigma^{t}(o_{h},a_{h},b_{h};\tau_{h-1})\Big{]}^ {2}\] \[= \Big{[}\operatorname*{\mathbb{E}}_{\tau_{h-1}\sim\operatorname*{ \mathbb{P}}_{f^{*},h-1}^{\pi^{i},\nu^{i}}}\sum_{o_{h},a_{h},b_{h}}\sum_{i=1}^{O }\Big{|}\mathrm{Pr}_{f^{t}}(o_{h},o_{i}\,|\,\tau_{1:h-1},\mathbf{do}(a_{h},b_{ h}))\] \[\qquad\qquad-\mathrm{Pr}_{f^{*}}(o_{h},o_{i}\,|\,\tau_{h-1}, \mathbf{do}(a_{h},b_{h}))\Big{|}\sigma^{t}(o_{h},a_{h},b_{h};\tau_{h-1})\Big{]} ^{2}\] \[= \Big{[}\sum_{\tau_{h-1},o_{h}}|\mathrm{Pr}_{f^{t}}^{\pi^{i},\nu^ {i}}(\tau_{h-1},o_{h})-\mathrm{Pr}_{f^{*}}^{\pi^{i},\nu^{i}}(\tau_{h-1},o_{h}) |\Big{]}^{2}\] \[\leq \|\mathbb{P}_{f^{t},h}^{\pi^{i},\nu^{i}}(\cdot)-\mathbb{P}_{f^{*},h}^{\pi^{i},\nu^{i}}(\cdot)\|_{1}^{2}\leq 8D_{\mathrm{He}}^{2}\big{(}\mathbb{P}_{f^{ t},h}^{\pi^{i},\nu^{i}}(\cdot),\mathbb{P}_{f^{*},h}^{\pi^{i},\nu^{i}}(\cdot) \big{)},\]

where \(\mathrm{Pr}_{f^{t}}^{\pi^{i},\nu^{i}}(\tau_{h-1},o_{h})\) denotes the probability of \((\tau_{h-1},o_{h})\) if the actions are taken following \((\pi^{i},\nu^{i})\) and the observations follow the omission and transition process in the model \(f^{t}\). The first inequality is by the fact that the \(L_{1}\) difference of two marginal distributions is no more than the \(L_{1}\) difference of two uniformed distributions, and the second inequality is due to \(\|P-Q\|_{1}^{2}\leq 8D_{\mathrm{He}}^{2}(P,Q)\). Similarly, when \(h=H\), Term(i) can be bounded as

\[\text{Term(i)}= \Big{[}\operatorname*{\mathbb{E}}_{\tau_{h-1}\sim\operatorname*{ \mathbb{P}}_{f^{*},h-1}^{\pi^{i},\nu^{i}}}\sum_{o_{h},a_{h},b_{h}}|\mathrm{Pr}_ {f^{t}}(o_{H}\,|\,\tau_{H-1},\mathbf{do}(a_{H},b_{H}))-\mathrm{Pr}_{f^{*}}(o_{ H}\,|\,\tau_{H-1},\mathbf{do}(a_{H},b_{H}))\] \[\quad\cdot\sigma^{t}(o_{H},a_{H},b_{H};\tau_{H-1})\Big{]}^{2}\] \[= \Big{[}\sum_{\tau_{H-1},o_{H}}|\mathrm{Pr}_{f^{t}}^{\pi^{i},\nu^ {i}}(\tau_{H-1},o_{H})-\mathrm{Pr}_{f^{*}}^{\pi^{i},\nu^{i}}(\tau_{H-1},o_{H}) |\Big{]}^{2}\] \[\leq \|\mathbb{P}_{f^{t},H}^{\pi^{i},\nu^{i}}(\cdot)-\mathbb{P}_{f^{*},H}^{\pi^{i},\nu^{i}}(\cdot)\|_{1}^{2}\leq 8D_{\mathrm{He}}^{2}\big{(}\mathbb{P}_{f^{ *},H}^{\pi^{i},\nu^{i}}(\cdot),\mathbb{P}_{f^{*},H}^{\pi^{i},\nu^{i}}(\cdot) \big{)}.\]

For Term(ii), Lemma 4 is used to give an upper bound. Specifically, we obtain that

\[\text{Term(i)}= \Big{[}\sum_{\tau_{h}}\|\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})\big{(} \bar{\bm{q}}^{t}(\tau_{h-1})-\bar{\bm{q}}(\tau_{h-1})\big{)}\|_{1}\mathbb{P}( \tau_{h-1})\sigma^{t}(\tau_{h})\Big{]}^{2}\] \[\leq \frac{S}{\alpha^{2}}\Big{[}\sum_{\tau_{h-1}}\|\bar{\bm{q}}^{t}( \tau_{h-1})-\bar{\bm{q}}(\tau_{h-1})\|_{1}\mathbb{P}(\tau_{h-1})\sigma^{t}( \tau_{h-1})\Big{]}^{2}\] \[\leq \frac{S}{\alpha^{2}}\|\mathbb{P}_{f^{t},h}^{\pi^{i},\nu^{i}}- \mathbb{P}_{f^{t},h}^{\pi^{i},\nu^{i}}\|_{1}^{2}\leq\frac{8S}{\alpha^{2}}D_{ \mathrm{He}}^{2}(\mathbb{P}_{f^{t},h}^{\pi^{i},\nu^{i}},\mathbb{P}_{f^{t},h}^{ \pi^{i},\nu^{i}}).\]

Combining the above results completes the proof of this lemma. 

**Lemma 7**.: _For any \((o,a,b)\in\mathcal{O}\times\mathcal{A}\times\mathcal{B}\), we have_

\[\operatorname*{\mathbb{E}}_{\tau_{h-1}\sim\operatorname*{\mathbb{P}}^{\pi^{i}, \nu^{i}}}\|x_{\tau_{h-1},\sigma^{t},o,a,b}\|_{2}^{2}\leq S,\qquad\sum_{j=1}^{O} \|w_{h,t,j,o,a,b}\|_{2}\leq\frac{2ABd\sqrt{S}}{\alpha}.\]

Proof.: We prove the former statement first. By noting that the \(\mathbb{O}_{h-1}^{\dagger}\bar{\bm{q}}(\tau_{h-1})=[\mathrm{Pr}(s_{i}\,|\,\tau_ {h-1})]_{i=1}^{S}\) and \(\sigma^{t}(o_{h},a_{h},b_{h};\tau_{h-1})\leq 1\), we obtain

\[\|x_{\tau_{h-1},\sigma^{t},o_{h},a_{h},b_{h}}\|_{2}^{2}\] \[\qquad=\|\mathbb{O}_{h-1}^{\dagger}\bar{\bm{q}}(\tau_{h-1})\sigma^{t }(o_{h},a_{h},b_{h};\tau_{h-1})\|_{2}^{2}\] \[\qquad\leq\sum_{i=1}^{S}\mathrm{Pr}(s_{i}\,|\,\tau_{h-1})^{2}\leq S.\]Therefore the first statement in Lemma 7 is proven. To prove the second statement, we write

\[\sum_{j=1}^{O}\|w_{h,t,j,o,a,b}\|_{2}\] \[\leq\sum_{o,a,b}\sum_{j=1}^{O}\|w_{h,t,j,o,a,b}\|_{2}\] \[\leq\sum_{o,a,b}\sum_{j=1}^{O}\|w_{h,t,j,o,a,b}\|_{1}\] \[=\sum_{o,a,b}\sum_{i=1}^{O}\|(\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})- \bm{M}_{h}(o_{h},a_{h},b_{h}))\mathbb{O}_{h-1}\widetilde{\bm{e}}_{i}\|_{1},\]

where the second inequality is due to \(\|\cdot\|_{2}\leq\|\cdot\|_{1}\), and \(\widetilde{\bm{e}}_{i}\) stands for the \(i\)-th standard basis vector in \(\mathbb{R}^{O}\). Then we apply Lemma 4 to obtain

\[\sum_{o,a,b}\sum_{i=1}^{O}\|(\bm{M}_{h}^{t}(o_{h},a_{h},b_{h})- \bm{M}_{h}(o_{h},a_{h},b_{h}))\mathbb{O}_{h-1}\widetilde{\bm{e}}_{i}\|_{1}\] \[\leq\frac{2AB\sqrt{S}}{\alpha}\sum_{i=1}^{O}\|\mathbb{O}_{h-1} \widetilde{\bm{e}}_{i}\|_{1}\leq\frac{2ABO\sqrt{S}}{\alpha}\|\mathbb{O}_{h-1 }\|_{1}=\frac{2ABO\sqrt{S}}{\alpha},\]

where the second inequality is by \(\|Ax\|_{1}\leq\|A\|_{1}\|x\|_{1}\). The proof is completed. 

### Decodable POMG

In this section, we propose a new class of POMGs, dubbed decodable POMG, by generalizing decodable POMDPs [23, 22] from the single-agent setting to the multi-agent setting.

**Definition 9** (Decodable POMG).: _We say a POMG is a decodable POMG if an unknown decoder function \(\phi_{h}\) exists, which recovers the state at step \(h\) from the observation at step \(h\). We have for any \(1\leq h\leq H\) that_

\[\phi_{h}(o_{h})=s_{h}.\]

Given the decoder, we can define the transition from observation to observation as follows,

\[\mathbb{P}_{h}(o_{h+1}\,|\,o_{h},a_{h},b_{h})\] \[\quad=\sum_{s_{h+1}\in\mathcal{S}}\mathbb{O}(o_{h+1}\,|\,s_{h+1}) \mathbb{P}(s_{h+1}\,|\,s_{h}=\phi_{h}(o_{h}),a_{h},b_{h}).\]

Our next proposition will show that the class of decodable POMGs is subsumed by the class of MGs with low self-play and adversarial GEC.

**Proposition 7** (Decodable POMG \(\subset\) Low Self-Play/Adversarial GEC).: _For a decodable POMG as defined in Definition 9, with the loss \(\ell(f,\xi_{h})\) defined in Definition 3 for POMGs, we have_

\[\text{Decodable POMG}\subset\text{POMG}\text{ with low self-play and adversarial GEC}\]

_with \(d_{\mathrm{GEC}}\) satisfying_

\[d_{\mathrm{GEC}}=O(H^{3}|\mathcal{O}|^{3}|\mathcal{A}|^{2}|\mathcal{B}|^{2} \varsigma),\]

_where \(\varsigma=2\log\Big{(}1+4|\mathcal{A}|^{2}|\mathcal{B}|^{2}|\mathcal{O}|^{2}| \mathcal{S}|\Big{)}\)._

Proof.: The proof of this proposition is largely the same as the proof of Proposition 6, except that the coefficient before \(\|x\|_{1}\) in Lemma 3 and 4 is changed, from \(\frac{\sqrt{S}}{\alpha}\) to \(1\), as is proved in Lemma 8 and Lemma 9. Following the proof of Proposition 6 and using Lemmas 8 and 9 yields our result.

#### b.5.1 Lemmas for Pool of Proposition 7

We present the lemmas for Proposition 7. These lemmas are analogous to Lemma 3 and Lemma 4. For convenience in analysis, we define some notations as follows.

Similar to the case of \(\alpha\)-revealing POMG in Appendix B.4, we define \(\bm{q}_{0}\), \(\tau_{h:h^{\prime}}\), \(\bm{q}(\tau_{h})\) and \(\bar{\bm{q}}(\tau_{h})\). We now define a different observable operator as follows,

\[\bm{M}_{h}(o,a,b):=\mathbb{P}_{h,a,b}\mathrm{diag}(\bm{e}_{o}),\]

where \(\bm{e}_{o}\in\mathbb{R}^{O}\) is the basis vector with only the \(o\)-th entry being \(1\). With these definitions, one can show that

\[\bm{q}(\tau_{h})=\bm{M}_{h}(o_{h},a_{h},b_{h})\bm{M}_{h-1}(o_{h-1},a_{h-1},b_{ h-1})\ldots\bm{M}_{1}(o_{1},a_{1},b_{1})\bm{q}_{0}.\]

We define \(\bm{M}_{h^{\prime}:h}(o_{h:h^{\prime}},a_{h:h^{\prime}},b_{h:h^{\prime}}):= \bm{M}_{h^{\prime}}(o_{h^{\prime}},a_{h^{\prime}},b_{h^{\prime}})\bm{M}_{h^{ \prime}}(o_{h^{\prime}-1},a_{h^{\prime}-1},b_{h^{\prime}-1})\ldots\bm{M}_{h}(o _{h},a_{h},b_{h})\) and rewrite \(\bm{M}_{H:h+1}(o_{h+1:H},a_{h+1:H},b_{h+1:H})\) as \(\bm{m}(o_{h+1:H},a_{h+1:H},b_{h+1:H})\) to emphasize that \(\bm{m}(o_{h+1:H},a_{h+1:H},b_{h+1:H})\) is a vector since \(\bm{M}_{H}(o_{H},a_{H},b_{H})\) is a vector.

**Lemma 8**.: _For any \(x\in\mathbb{R}^{O}\), any policy pair \((\pi,\nu)\), and any \(\tau_{h-1}\), we have_

\[\sum_{\tau_{h:H}}|\bm{m}(\tau_{h:H})x|\sigma(\tau_{h:H};\tau_{h-1})\leq\|x\|_ {1}.\]

Proof.: We can first bound the LHS as

\[\sum_{\tau_{h:H}}| \bm{m}(\tau_{h:H})x|\sigma(\tau_{h:H};\tau_{h-1})\] \[=\sum_{\tau_{h:H}}|\sum_{i=1}^{O}\widetilde{\bm{e}}_{i}^{\top}x \Pr(\tau_{h+1:H}\,|\,o_{i})|\sigma(\tau_{h:H};\tau_{h-1})\] \[\leq\sum_{\tau_{h:H}}\sum_{i=1}^{O}|\widetilde{\bm{e}}_{i}^{\top }x|\Pr(\tau_{h+1:H}\,|\,o_{i})\sigma(\tau_{h:H};\tau_{h-1}),\]

where \(\widetilde{\bm{e}}_{i}\) is the \(i\)-th basis vector of the space \(\mathbb{R}^{O}\). Since \(\sum_{\tau_{h+1:H}}\Pr(\tau_{h+1:H}\,|\,o_{i})\sigma(\tau_{h:H};\tau_{h-1})\leq 1\), we further obtain

\[\sum_{\tau_{h:H}}\sum_{i=1}^{O}|\widetilde{\bm{e}}_{i}^{\top}x| \Pr(\tau_{h+1:H}\,|\,o_{i})\sigma(\tau_{h:H};\tau_{h-1})\] \[\leq\sum_{i=1}^{O}|\widetilde{\bm{e}}_{i}^{\top}x|=\|x\|_{1}.\]

This completes the proof. 

**Lemma 9**.: _For any \(x\in\mathbb{R}^{O}\), policy pair \((\pi,\nu)\) and \(\tau_{h-1}\), we have_

\[\sum_{o\in\mathcal{O},a\in\mathcal{A},b\in\mathcal{B}}\|\bm{M}_{h}(o,a,b)x\|_ {1}\sigma(o,a,b;\tau_{h-1})\leq\|x\|_{1}.\]

Proof.: We can first show that

\[\sum_{o\in\mathcal{O},a\in\mathcal{A},b\in\mathcal{B}}\|\bm{M}_{h}(o,a,b)x\|_ {1}\sigma(o,a,b;\tau_{h-1})\] \[=\sum_{o\in\mathcal{O},a\in\mathcal{A},b\in\mathcal{B}}\sum_{i=1} ^{O}|\widetilde{\bm{e}}_{i}^{\top}\bm{M}_{h}(o,a,b)x|\sigma(o,a,b;\tau_{h-1})\] \[\leq\sum_{o\in\mathcal{O},a\in\mathcal{A},b\in\mathcal{B}}\sum_{i=1 }^{O}|\widetilde{\bm{e}}_{i}^{\top}x|\Pr(o_{i}\,|\,o,a,b)\sigma(o,a,b;\tau_{h-1 }).\]Since \(\sum_{o\in\mathcal{O},a\in\mathcal{A},b\in\mathcal{B}}\Pr(o_{i}\,|\,o,a,b) \sigma(o,a,b;\tau_{h-1})\leq 1\), we further obtain

\[\sum_{o\in\mathcal{O},a\in\mathcal{A},b\in\mathcal{B}}\sum_{i=1}^{O}|\widetilde {\bm{e}}_{i}^{\top}x|\mathbb{P}(o_{i}\,|\,o,a,b)\sigma(o,a,b;\tau_{h-1})\]

\[\leq\sum_{i=1}^{O}|\bm{e}_{i}^{\top}x|=\|x\|_{1}.\]

This concludes the proof. 

## Appendix C Computation of \(\omega(\beta,p^{0})\)

In this section, we discuss the upper bound of the quantity \(\omega(\beta,p^{0})\) for both FOMGs and POMGs. For FOMGs, the analysis can be adopted from Lemma 2 in [2]. For POMGs, we give the detailed analysis to show the bound of \(\omega(\beta,p^{0})\), which is the first proof for the partially observable setting.

### Fully Observable Markov Game

We can adopt the result from Lemma 2 in [2] to give a bound for \(\omega(\beta,p^{0})\) in the context of fully observable Markov games.

**Proposition 8** (Lemma 2 of [2]).: _When \(\mathcal{F}\) is finite, \(p^{0}\) is a uniform distribution over \(\mathcal{F}\), then \(\omega(\beta,p^{0})\leq\log|\mathcal{F}|\). When \(\mathcal{F}\) is infinite, suppose a transition kernel \(\mathbb{P}_{0}\) satisfies \(\mathbb{P}_{f^{*}}\ll\mathbb{P}_{0}\) and \(\left\|\frac{\mathrm{d}\mathbb{P}_{f^{*}}}{\mathrm{d}\mathbb{P}_{0}}\right\|_ {\infty}\leq B\), then for \(\epsilon\leq 2/3\) and \(B\geq\log(6B^{2}/\epsilon)\), there exists a prior \(p^{0}\) on \(\mathcal{F}\) such that_

\[\omega(\beta,p^{0})\leq\beta\epsilon+\log\left(\mathcal{N}\left(\frac{\epsilon }{6\log(B/\nu)}\right)\right),\]

_where \(\nu=\epsilon/(6\log(6B^{2}/\epsilon))\) and \(\mathcal{N}(\epsilon)\) stands for the \(\epsilon\)-covering number w.r.t. the distance_

\[d(f,f^{\prime}):=\sup_{s,a,b,h}\big{|}D_{\mathrm{He}}^{2}\big{(}\mathbb{P}_{f, h}(\cdot\,|\,s,a,b),\mathbb{P}_{f^{*},h}(\cdot\,|\,s,a,b)\big{)}-D_{ \mathrm{He}}^{2}\big{(}\mathbb{P}_{f^{\prime},h}(\cdot\,|\,s,a,b),\mathbb{P}_ {f^{*},h}(\cdot\,|\,s,a,b)\big{)}\big{|}.\]

To apply Proposition 8 in FOMGs, we can further show that \(|D_{\mathrm{He}}^{2}(P,R)-D_{\mathrm{He}}^{2}(Q,R)|\leq\sqrt{2}D_{\mathrm{He}} ^{2}(P,R)|\leq\frac{\sqrt{2}}{2}\|P-Q\|_{1}\) for distributions \(P,Q,R\), so that the covering number under the distance \(d\) can be bounded by the covering number w.r.t. the \(\ell_{1}\) distance denoted as \(\mathcal{N}_{1}(\epsilon)\), i.e.,

\[\mathcal{N}(\epsilon)\leq\mathcal{N}_{1}(\sqrt{2}\epsilon),\]

where \(\mathcal{N}_{1}(\epsilon)\) is defined w.r.t. the distance

\[d_{1}(f,f^{\prime})=\sup_{s,a,b,h}\|\mathbb{P}_{f,h}(\cdot\,|\,s,a,b)- \mathbb{P}_{f^{\prime},h}(\cdot\,|\,s,a,b)\|_{1}.\]

The covering number under the \(\ell_{1}\) distance is more common and is readily applicable to many problems.

Taking linear mixture MGs for an instance (defined in Definition 5), we calculate \(\omega(4HT,p^{0})\), which is the term quantifying the coverage of the initial sampling distribution in our theorems. We first obtain that

\[\sup_{s,a,b,h}\|P_{f,h}(\cdot\,|\,s,a,b)-P_{f^{\prime},h}(\cdot \,|\,s,a,b)\|_{1}\] \[\qquad=\sup_{s,a,b,h}\int_{s^{\prime}\in\mathcal{S}}\bm{\phi}(s^ {\prime},s,a,b)^{\top}\Big{(}\bm{\theta}_{f,h}-\bm{\theta}_{f^{\prime},h} \Big{)}\mathrm{d}s^{\prime}\] \[\qquad\leq\sup_{s,a,b,h}\int_{s^{\prime}\in\mathcal{S}}\|\bm{ \phi}(s^{\prime},s,a,b)\|_{2}\mathrm{d}s^{\prime}\Big{\|}\bm{\theta}_{f,h}- \bm{\theta}_{f^{\prime},h}\Big{\|}_{2}\] \[\qquad\leq\sup_{h}\Big{\|}\bm{\theta}_{f,h}-\bm{\theta}_{f^{\prime },h}\Big{\|}_{2},\] (16)where the first inequality is by Cauchy-Schwarz, and the second inequality is due to \(\int_{s^{\prime}\in\mathcal{S}}\|\bm{\phi}(s^{\prime},s,a,b)\|_{2}\mathrm{d}s^{ \prime}\leq 1\). Since the model space is \(\big{\{}\bm{\theta}:\sup_{h}\|\bm{\theta}_{h}\|_{2}\leq B\big{\}}\), under the \(\ell_{2}\) distance measure of

\[d_{2}(f,f^{\prime})=\sup_{h}\left\|\bm{\theta}_{f,h}-\bm{\theta}_{f^{\prime},h} \right\|_{2},\]

we have

\[\mathcal{N}(\epsilon)\leq\mathcal{N}_{1}(\sqrt{2}\epsilon)\leq \left(1+\frac{2\sqrt{2}B}{\varepsilon}\right)^{Hd}.\] (17)

according to (16) and the covering number for an Euclidean ball. Combining this result (17) with Proposition 8 together, we can show that there exists a prior distribution \(p^{0}\) with a sufficiently large \(T\) such that

\[\omega(4HT,p^{0})\lesssim Hd\log\Big{(}1+BT\log\big{(}T\log(T) \big{)}\Big{)}.\]

### Partially Observable Markov Game

In this subsection, we prove the bound of \(\omega(\beta,p^{0})\) for the partially observable setting, inspired by the proof of Proposition 8 (Lemma 2 in [2]).

**Proposition 9**.: _When \(\mathcal{F}\) is finite, \(p^{0}\) is a uniform distribution over \(\mathcal{F}\), then \(\omega(\beta,p^{0})\leq\log|\mathcal{F}|\). When \(\mathcal{F}\) is infinite, suppose a distribution \(\mathbf{P}_{0}\) satisfies that for any policy pair \((\pi,\nu)\), \(\mathbf{P}_{f^{\prime},H}^{\pi,\nu}\ll\mathbf{P}_{0}^{\pi,\nu}\) and \(\|\mathrm{d}\mathbf{P}_{f^{\prime},H}^{\pi,\nu}/\mathrm{d}\mathbf{P}_{0}^{\pi,\nu}\|_{\infty}\leq B\), where \(B\geq 1\). Then for \(\epsilon\leq 2/3\) and \(B\geq\log(6B^{2}/\epsilon)\), there exists a prior \(p^{0}\) on \(\mathcal{F}\) such that_

\[\omega(\beta,p^{0})\leq\beta\epsilon+\log\left(\mathcal{N}\left( \frac{\epsilon}{6\log(B/\nu)}\right)\right),\]

_where \(\nu=\epsilon/(6\log(6B^{2}/\epsilon))\) and \(\mathcal{N}(\cdot)\) is the covering number w.r.t. the distance_

\[d(f,f^{\prime})=\sup_{\pi,\nu}\left|D_{\mathrm{He}}^{2}(\mathbf{P }_{f,H}^{\pi,\nu},\mathbf{P}_{f^{\prime},H}^{\pi,\nu})-D_{\mathrm{He}}^{2}( \mathbf{P}_{f^{\prime},H}^{\pi,\nu},\mathbf{P}_{f^{\prime},H}^{\pi,\nu}) \right|.\]

The assumption in the theorem above covers the case when \(\mathcal{S}\) is finite, where we choose \(P_{0}\) to be uniform on \(\mathcal{S}\) regardless of the policy. We also note that

\[\|\mathrm{d}\mathbf{P}_{f^{\prime},H}^{\pi,\nu}/\mathrm{d}\mathbf{P}_{0}^{\pi,\nu}\|_{\infty}=\sup_{\tau_{H}}\left|\frac{\mathrm{d}\mathbf{P}_{f^{\prime},H }^{\pi,\nu}(\tau_{H})}{\mathrm{d}\mathbf{P}_{0}^{\pi,\nu}(\tau_{H})}\right|= \sup_{\tau_{H}}\left|\frac{\mathrm{d}\mathbf{P}_{f^{\star},H}(\tau_{H})\sigma (\tau_{H})}{\mathrm{d}\mathbf{P}_{0}(\tau_{H})\sigma(\tau_{H})}\right|=\sup_{ \tau_{H}}\left|\frac{\mathrm{d}\mathbf{P}_{f^{\star},H}(\tau_{H})}{\mathrm{d }\mathbf{P}_{0}(\tau_{H})}\right|,\]

which does not depend on the joint policy \(\sigma=(\pi,\nu)\).

To apply the above proposition in POMGs, by the relation between different distances: \(|D_{\mathrm{He}}^{2}(P,R)-D_{\mathrm{He}}^{2}(Q,R)|\leq\sqrt{2}D_{\mathrm{He}}^ {2}(P,R)|\leq\frac{\sqrt{2}}{2}\|P-Q\|_{1}\) for distributions \(P,Q,R\), we can show the covering number under the distance \(d\) can be bounded by the covering number w.r.t. the \(\ell_{1}\) distance denoted as \(\mathcal{N}_{1}(\epsilon)\), i.e.,

\[\mathcal{N}(\epsilon)\leq\mathcal{N}_{1}(\sqrt{2}\epsilon),\]

where \(\mathcal{N}_{1}(\epsilon)\) is defined w.r.t. the distance

\[d_{1}(f,f^{\prime}):=\sup_{\pi,\nu}\left\|\mathbf{P}_{f,H}^{\pi,\nu}-\mathbf{ P}_{f^{\prime},H}^{\pi,\nu}\right\|_{1}.\]

Such a covering number under \(\ell_{1}\) distance is analyzed in the work [78], generalizing whose results gives that POMGs with different structures admit a log-covering number \(\log\mathcal{N}_{1}(\epsilon)=\mathrm{ploy}(|\mathcal{O}|,|\mathcal{A}|,| \mathcal{B}|,|\mathcal{S}|,H,\log(1/\epsilon))\). We refer readers to [78] for detailed calculation of log-covering numbers under \(d_{1}(f,f^{\prime})\). Therefore, we can eventually show that \(\omega(4HT,p^{0})=\mathrm{ploy}(|\mathcal{O}|,|\mathcal{A}|,|\mathcal{B}|,| \mathcal{S}|,H,\log(HT))\). Next, we show the detailed proof for Proposition 9.

Proof.: When \(\mathcal{F}\) is finite and \(p^{0}\) is the uniform distribution on \(\mathcal{F}\), the proof is straightforward as we have \(\omega(\beta,p^{0})\leq\beta\varepsilon+\log|\mathcal{F}|\) for any \(\varepsilon\geq 0\) and a uniform distribution \(p^{0}\). Setting \(\varepsilon\) to approach \(0^{+}\) completes the proof.

When \(\mathcal{F}\) is infinite, we start by setting up a \(\gamma\)-covering \(\mathcal{C}(\gamma)\subset\mathcal{F}\) w.r.t. the distance \(d(f,f^{\prime})=\sup_{\pi,\nu}\left|D^{2}_{\mathrm{He}}(\mathbf{P}^{\pi,\nu}_ {f,H},\mathbf{P}^{\pi,\nu}_{f^{*},H})-D^{2}_{\mathrm{He}}(\mathbf{P}^{\pi,\nu }_{f^{*},H},\mathbf{P}^{\pi,\nu}_{f^{*},H})\right|\), where \(\gamma>0\) is a variable to be specified. Since \(f^{*}\) is covered, an \(\widetilde{f}\in\mathcal{C}(\gamma)\) satisfies \(d(\widetilde{f},f^{*})=\sup_{\pi,\nu}D^{2}_{\mathrm{He}}(\mathbf{P}^{\pi,\nu }_{\widetilde{f},H},\mathbf{P}^{\pi,\nu}_{f^{*},H})\leq\gamma\). We further define \(\mathcal{C}_{\nu}(\gamma):=\{\nu\mathbf{P}_{0}+(1-\nu)\mathbf{P}_{f}|f\in \mathcal{C}(\gamma)\}\) and also \(\mathbf{P}_{f^{\prime}}:=\nu\mathbf{P}_{0}+(1-\nu)\mathbf{P}_{\widetilde{f}}\). We note that \(\sup_{\pi,\nu}\|\frac{\mathrm{d}\mathbf{P}^{\pi,\nu}_{f^{*},H}}{\mathrm{d} \mathbf{P}^{\pi,\nu}_{f^{*},H}}\|_{\infty}\leq\frac{B}{\nu}\). Then, we obtain

\[D^{2}_{\mathrm{He}} (\mathbf{P}^{\pi,\nu}_{f^{\prime},H},\mathbf{P}^{\pi,\nu}_{f^{*},H})\] \[=1-\int\sqrt{\mathrm{d}(\nu\mathbf{P}^{\pi,\nu}_{0}+(1-\nu) \mathbf{P}^{\pi,\nu}_{\widetilde{f},H})\mathrm{d}\mathbf{P}^{\pi,\nu}_{f^{*},H}}\] \[\leq 1-\left(\nu\int\sqrt{\mathrm{d}\mathbf{P}^{\pi,\nu}_{0} \mathrm{d}\mathbf{P}^{\pi,\nu}_{f^{*},H}}+(1-\nu)\int\sqrt{\mathrm{d} \mathbf{P}^{\pi,\nu}_{\widetilde{f},H}\mathrm{d}\mathbf{P}^{\pi,\nu}_{f^{*},H }}\right)\] \[=D^{2}_{\mathrm{He}}(\mathbf{P}^{\pi,\nu}_{\widetilde{f},H}, \mathbf{P}^{\pi,\nu}_{f^{*},H})+\nu\int\sqrt{\mathrm{d}\mathbf{P}^{\pi,\nu}_{ \widetilde{f},H}\mathrm{d}\mathbf{P}^{\pi,\nu}_{f^{*},H}}-\nu\int\sqrt{ \mathrm{d}\mathbf{P}^{\pi,\nu}_{0}\mathrm{d}\mathbf{P}^{\pi,\nu}_{f^{*},H}}\] \[\leq\gamma+\nu,\]

where the first inequality uses Jensen's inequality and the second inequality is by \(\sup_{\pi,\nu}D^{2}_{\mathrm{He}}(\mathbf{P}^{\pi,\nu}_{\widetilde{f},H}, \mathbf{P}^{\pi,\nu}_{f^{*},H})\leq\gamma\) and \(0\leq\int\sqrt{\mathrm{d}PdQ}\leq 1\). To connect to the definition of \(\mathcal{F}(\varepsilon)\), we further invoke Theorem 9 from [55] and obtain

\[\mathrm{KL}(\mathbf{P}^{\pi,\nu}_{\widetilde{f}^{*},H}||\mathbf{P}^{\pi,\nu}_ {f^{*},H})\leq\zeta(B/\nu)D^{2}_{\mathrm{He}}(\mathbf{P}^{\pi,\nu}_{f^{*},H}, \mathbf{P}^{\pi,\nu}_{\widetilde{f}^{*},H})\]

for any policy pair \((\pi,\nu)\), where \(\zeta(b)\leq\max\left\{1,\frac{b\log b}{(1-\sqrt{b})^{2}}\right\}\) for \(b>1\). Plugging the above inequalities together, we obtain

\[\mathrm{KL}(\mathbf{P}^{\pi,\nu}_{f^{*},H}||\mathbf{P}^{\pi,\nu}_{f^{*},H}) \leq\zeta(B/\nu)(\gamma+\nu).\] (18)

It remains to find a proper choice of \(\gamma\) and \(\nu\) to obtain \(\zeta(B/\nu)(\gamma+\nu)\leq\varepsilon\). We choose \(\nu=\varepsilon/\big{(}6\log(6B^{2}/\varepsilon)\big{)}\) and \(\gamma=\varepsilon/\big{(}6\log(B/\nu)\big{)}\). Given the condition \(B\geq\log(6B^{2}/\varepsilon)\), we have

\[\nu=\frac{\varepsilon}{6\log(\frac{6B^{2}}{\varepsilon})}\leq\frac{\varepsilon} {6B},\]

so that

\[\nu=\frac{\varepsilon}{6\log(\frac{6B^{2}}{\varepsilon})}\leq\frac{\varepsilon} {6\log(\frac{B}{\nu})}=\gamma.\] (19)

Given \(\varepsilon\leq 2/3\) and \(B\geq 1\), we obtain

\[\nu\leq\frac{\varepsilon}{6B}\leq\frac{1}{9B}\leq\frac{B}{9},\]

namely \(B/\nu\geq 9\). We note that when \(b>9\), it holds that

\[\zeta(b)\leq\frac{b\log b}{(1-\sqrt{b})^{2}}\leq\frac{b\log b}{b-2\sqrt{b}} \leq 3\log b.\]

Thus we have \(\zeta(B/\nu)\leq 3\log(B/\nu)\). Finally, we obtain

\[\zeta(B/\nu)(\gamma+\nu)\leq 2\gamma\zeta(B/\nu)=\frac{\varepsilon}{3\log( \frac{B}{\nu})}\zeta(B/\nu)\leq\varepsilon,\]

where the first inequality uses (19), the first equation uses the choice of \(\gamma\), and the second inequality uses \(\zeta(B/\nu)\leq 3\log(B/\nu)\). Choosing \(p^{0}\) to be a uniform distribution on \(\mathcal{C}_{\nu}(\gamma)\) completes the proof.

Technical Lemmas for Main Theorems

In this section, we first provide several important supporting lemmas used in the proofs of Theorems 1 and 2. We then present detailed proofs for these lemmas.

### Lemmas

**Lemma 10**.: _Let \(\upsilon\) be any probability distribution over \(f\in\mathcal{F}\) where \(\mathcal{F}\) is an arbitrary set. Then, \(\mathbb{E}_{f\sim\upsilon(\cdot)}[G(f)+\log\upsilon(f)]\) is minimized at \(\upsilon(f)\propto\exp(-G(f))\)._

Proof.: This lemma is a corollary of Gibbs variational principle. For the detailed proof of this lemma, we refer the readers to the proof of Lemma 4.10 in [64]. This completes the proof. 

The above lemma states that \(\upsilon(f)\) in the above-described form solves the minimization problem \(\min_{\upsilon\in\Delta(\mathcal{F})}\mathbb{E}_{f\sim\upsilon}[G(f)+\log \upsilon(f)]\), which helps to understand the design of the posterior sampling steps in our proposed algorithms. This lemma is also used in the proofs of the following two lemmas.

The following two lemmas provide the upper bounds for the expectation of the Hellinger distance by the likelihood functions defined in (1) and (2) for FOMGs and POMGs respectively.

**Lemma 11**.: _Under the FOMG setting, for any \(t\geq 1\), let \(Z^{t}\) be the system randomness history up to the \(t\)-th episode, \(p^{t}(\cdot|Z^{t-1})\) be any posterior distribution over the function class \(\mathcal{F}\) with \(p^{0}(\cdot)\) denoting an initial distribution, and \((\pi^{t},\nu^{t})\) be any Markovian policy pair for Player 1 and Player 2 depending on \(f^{t}\sim p^{t}\). Suppose that \((s^{t}_{h},a^{t}_{h},b^{t}_{h},s^{t}_{h+1})\) is a data point sampled independently by executing the policy pair \((\pi^{t},\nu^{t})\) to the \(h\)-th step of the \(t\)-th episode. If we define \(L^{t}_{h}(f):=\eta\log\mathbb{P}_{f,h}(s^{t}_{h+1}\,|\,s^{t}_{h},a^{t}_{h},b^{ t}_{h})\) with \(\eta=1/2\) as in (1), we have the following relation_

\[\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\mathbb{E}_{Z^{t-1}}\mathbb{E}_ {f^{t}\sim p^{t}}\mathbb{E}_{(\pi^{t},\nu^{t},h)}[D^{2}_{\mathrm{He}}(\mathbb{ P}_{f^{t},h}(\cdot|s^{t}_{h},a^{t}_{h},b^{t}_{h}),\mathbb{P}_{f^{\prime},h}( \cdot|s^{t}_{h},a^{t}_{h},b^{t}_{h}))]\] \[\qquad\leq\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f^{t}\sim p^{t}}\left[ -\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\left(L^{t}_{h}(f^{t})-L^{t}_{h}(f^{*}) \right)+\log\frac{p^{t}(f^{t})}{p^{0}(f^{t})}\right],\]

_where \(\mathbb{E}_{(\pi^{t},\nu^{t},h)}\) denotes taking an expectation over the data \((s^{t}_{h},a^{t}_{h},b^{t}_{h})\) sampled following the policy pair \((\pi^{t},\nu^{t})\) and the true model \(\mathbb{P}_{f^{*}}\) up to the \(h\)-th step at any \(t\)._

Proof.: Please see Appendix D.2 for a detailed proof. 

**Lemma 12**.: _Under the POMG setting, for any \(t\geq 1\), let \(Z^{t}\) be the system randomness history up to the \(t\)-th episode, \(p^{t}(\cdot|Z^{t-1})\) be any posterior distribution over the function class \(\mathcal{F}\) with \(p^{0}(\cdot)\) denoting an initial distribution, and \((\pi^{t},\nu^{t})\) be any general history-dependent policy pair for Player 1 and Player 2 depending on \(f^{t}\sim p^{t}\). Suppose that \(\tau^{t}_{h}=(o^{t}_{1},a^{t}_{1},b^{t}_{1}\ldots,o^{t}_{h},a^{t}_{h},b^{t}_{h})\) is a data point sampled independently by executing the policy pair \((\pi^{t},\nu^{t})\) to the \(h\)-th step of the \(t\)-th episode. If we define \(L^{t}_{h}(f):=\eta\log\mathbb{P}_{f}(\tau^{t}_{h})\) with \(\eta=1/2\) as in (2), we have the following relation_

\[\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\mathbb{E}_{Z^{t-1}}\mathbb{E}_ {f^{t}\sim p^{t}}[D^{2}_{\mathrm{He}}(\mathbb{P}^{\pi^{t},\nu^{t}}_{ft,h}, \mathbb{P}^{\pi^{t},\nu^{t}}_{f^{*},h})]\] \[\qquad\leq\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f^{t}\sim p^{t}}\left[ -\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\left(L^{t}_{h}(f^{t})-L^{t}_{h}(f^{*}) \right)+\log\frac{p^{t}(f^{t})}{p^{0}(f^{t})}\right],\]

_where \(\mathbb{P}^{\pi,\nu}_{f,h}\) denotes the distribution for \(\tau_{h}=(o_{1},a_{1},b_{1},\ldots,o_{h},a_{h},b_{h})\) under the model \(\theta_{f}\) and the policy pair \((\pi,\nu)\) up to the \(h\)-th step._

Proof.: Please see Appendix D.3 for a detailed proof. 

The next lemma shows that when the model is sufficiently close to \(f^{*}\) with the distances employed in Definition 2, the following value differences are small enough under both FOMG and POMG settings.

**Lemma 13**.: _If the model \(f\) satisfies \(\sup_{h,s,a,b}\mathrm{KL}^{\frac{1}{2}}(\mathbb{P}_{f^{*},h}(\cdot\,|\,s,a,b)\| \mathbb{P}_{f,h}(\cdot\,|\,s,a,b))\leq\varepsilon\) for FOMGs and \(\sup_{\pi,\nu}\mathrm{KL}^{\frac{1}{2}}(\mathbf{P}_{f^{*},h}^{\pi,\nu}\|\mathbf{ P}_{f,H}^{\pi,\nu})\leq\varepsilon\) for POMGs, we have that their corresponding value function satisfies_

\[V_{f^{*}}^{*}-V_{f}^{*}\leq 3H\varepsilon,\qquad\sup_{\pi}(V_{f^{*}}^{\pi, \nu}-V_{f}^{\pi,\nu})\leq 3H\varepsilon,\qquad\sup_{\nu}(V_{f^{*}}^{*,\nu}-V_{f}^{ *,\nu})\leq 3H\varepsilon.\]

Proof.: Please see Appendix D.4 for detailed proof. 

Finally, we show that when the model is sufficiently close to \(f^{*}\), we will obtain that the following likelihood function difference is small under both FOMG and POMG settings.

**Lemma 14**.: _If the model \(f\) satisfies \(\sup_{h,s,a,b}\mathrm{KL}^{\frac{1}{2}}(\mathbb{P}_{f^{*},h}(\cdot\,|\,s,a,b) \|\mathbb{P}_{f,h}(\cdot\,|\,s,a,b))\leq\varepsilon\) for FOMGs and \(\sup_{\pi,\nu}\mathrm{KL}^{\frac{1}{2}}(\mathbf{P}_{f^{*},H}^{\pi,\nu}\| \mathbf{P}_{f,H}^{\pi,\nu})\leq\varepsilon\) for POMGs, we have that their corresponding likelihood function defined in (1) and (2) satisfies_

\[|\mathbb{E}(L_{h}^{t}(f)-L_{h}^{t}(f^{*}))|\leq\eta\varepsilon^{2},\]

_where the expectation is taken with respect to the randomness in \(L_{h}^{t}\)._

Proof.: Please see Appendix D.5 for detailed proof. 

### Proof of Lemma 11

Proof.: The proof of Lemma 11 can be viewed as a multi-agent generalization of the proof for Lemma E.5 in [83]. We start our proof by first considering the following equality

\[\mathbb{E}_{Z^{t-1}} \mathbb{E}_{f^{t}\sim p^{t}}\left[-\sum_{h=1}^{H}\sum_{\iota=1}^{t -1}\left(L_{h}^{t}(f^{t})-L_{h}^{t}(f^{*})\right)+\log\frac{p^{t}(f^{t})}{p^{ 0}(f^{t})}\right]\] \[=\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f^{t}\sim p^{t}}\left[\sum_{h=1} ^{H}\sum_{\iota=1}^{t-1}\eta\log\frac{\mathbb{P}_{f^{*},h}(s_{h+1}^{t}|s_{h}^ {t},a_{h}^{t},b_{h}^{t})}{\mathbb{P}_{f^{t},h}(s_{h+1}^{t}|s_{h}^{t},a_{h}^{t},b_{h}^{t})}+\log\frac{p^{t}(f^{t})}{p^{0}(f^{t})}\right].\]

Next, we lower bound RHS of the above equality. We define

\[\overline{L}_{h}^{t}(f):=\eta\log\frac{\mathbb{P}_{f,h}(s_{h+1}^{t}|s_{h}^{t},a_{h}^{t},b_{h}^{t})}{\mathbb{P}_{f^{*},h}(s_{h+1}^{t}|s_{h}^{t},a_{h}^{t},b_ {h}^{t})},\qquad\widetilde{L}_{h}^{t}(f):=\overline{L}_{h}^{t}(f)-\log\mathbb{ E}_{(\pi^{*},\nu^{*},\mathbb{P}_{f^{*},h})}[\exp(\overline{L}_{h}^{t}(f))],\]

where \(\mathbb{E}_{(\pi^{*},\nu^{*},\mathbb{P}_{f^{*},h})}\) denotes taking an expectation over the data \((s_{h}^{t},a_{h}^{t},b_{h}^{t},s_{h+1}^{t})\) sampled following the policy pair \((\pi^{*},\nu^{*})\) and the true model \(\mathbb{P}_{f^{*}}\) to the \(h\)-th step with \(s_{h+1}^{t}\sim\mathbb{P}_{f^{*},h}(\cdot|s_{h}^{t},a_{h}^{t},b_{h}^{t})\) at round \(\iota\). Then, we will show that \(\mathbb{E}_{Z^{t-1}}[\exp(\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\widetilde{L}_{h} ^{t}(f))]=1\) by induction, following from [80]. Suppose that for any \(k\), we have at \(k-1\) that \(\mathbb{E}_{Z^{k-1}}[\exp(\sum_{h=1}^{H}\sum_{\iota=1}^{k-1}\widetilde{L}_{h} ^{t}(f))]=1\). Then, at \(k\), we have

\[\mathbb{E}_{Z^{k}}\left[\exp\left(\sum_{h=1}^{H}\sum_{\iota=1}^{k }\widetilde{L}_{h}^{t}(f)\right)\right] =\mathbb{E}_{Z^{k-1}}\left[\exp\left(\sum_{h=1}^{H}\sum_{\iota=1}^ {k-1}\widetilde{L}_{h}^{t}(f)\right)\exp\left(\sum_{h=1}^{H}\widetilde{L}_{h}^{ k}(f)\right)\right]\] \[=\mathbb{E}_{Z^{k-1}}\left[\exp\left(\sum_{h=1}^{H}\sum_{\iota=1} ^{k-1}\widetilde{L}_{h}^{t}(f)\right)\mathbb{E}_{f^{k}\sim p^{k}}\prod_{h=1}^{H }\mathbb{E}_{(\pi^{k},\nu^{k},\mathbb{P}_{f^{*},h})}\exp\left(\widetilde{L}_{h }^{k}(f)\right)\right]\] \[=\mathbb{E}_{Z^{k-1}}\left[\exp\left(\sum_{h=1}^{H}\sum_{\iota=1} ^{k-1}\widetilde{L}_{h}^{t}(f)\right)\right]=1,\]

where the second equality uses the fact that the data is sampled independently, the third equality is due to \(\mathbb{E}_{(\pi^{k},\nu^{k},\mathbb{P}_{f^{*},h})}\exp(\widetilde{L}_{h}^{k}( f))=\mathbb{E}_{(\pi^{k},\nu^{k},\mathbb{P}_{f^{*},h})}\exp(\overline{L}_{h}^{k}( f))/\mathbb{E}_{(\pi^{k},\nu^{k},\mathbb{P}_{f^{*},h})}[\exp( \overline{L}_{h}^{k}(f))]=1\) by the definition of \(\widetilde{L}_{h}^{k}(f)\), and the last equality is by \(\mathbb{E}_{Z^{k-1}}[\exp(\sum_{h=1}^{H}\sum_{\iota=1}^{k-1}\widetilde{L}_{h} ^{t}(f))]=1\) in the above assumption. Moreover, for \(k=1\), we have a trivial result that\(\mathbb{E}_{f^{1}\sim p^{1}}\prod_{h=1}^{H}\mathbb{E}_{(\pi^{1},\nu^{1},\mathbb{P}_ {f^{*}},h)}\exp(\widetilde{L}_{h}^{1}(f))=1\). Consequently, we conclude that for any \(k\), the above equality holds. Then, when \(k=t-1\), we have

\[\mathbb{E}_{Z^{t-1}}\left[\exp\left(\sum_{h=1}^{H}\sum_{\iota=1}^{t-1} \widetilde{L}_{h}^{\iota}(f)\right)\right]=1.\] (20)

Furthermore, we have

\[\mathbb{E}_{Z^{t-1}} \mathbb{E}_{f^{t}\sim p^{t}}\left[-\sum_{h=1}^{H}\sum_{\iota=1}^{t -1}\widetilde{L}_{h}^{\iota}(f^{t})+\log\frac{p^{t}(f^{t})}{p^{0}(f^{t})}\right]\] \[\geq\mathbb{E}_{Z^{t-1}}\inf_{p}\mathbb{E}_{f\sim p}\left[-\sum_{ h=1}^{H}\sum_{\iota=1}^{t-1}\widetilde{L}_{h}^{\iota}(f)+\log\frac{p(f)}{p^{0}(f)}\right]\] \[=-\mathbb{E}_{Z^{t-1}}\log\mathbb{E}_{f\sim p^{0}}\exp\left[\sum _{h=1}^{H}\sum_{\iota=1}^{t-1}\widetilde{L}_{h}^{\iota}(f)\right]\] \[\geq-\log\mathbb{E}_{f\sim p^{0}}\mathbb{E}_{Z^{t-1}}\exp\left[ \sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\widetilde{L}_{h}^{\iota}(f)\right]=0,\]

where the last inequality is by Jensen's inequality and the last equality is due to (20). For the first equality, we use the fact that the following distribution is the minimizer

\[p(f) \propto\exp\left(\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\widetilde{L}_ {h}^{\iota}(f)+\log p^{0}(f)\right)=p^{0}(f)\exp\left(\sum_{h=1}^{H}\sum_{ \iota=1}^{t-1}\widetilde{L}_{h}^{\iota}(f)\right)\] \[\iff p(f)=\frac{p^{0}(f)\exp\left(\sum_{h=1}^{H}\sum_{\iota=1}^{t-1} \widetilde{L}_{h}^{\iota}(f)\right)}{\int_{\mathcal{F}}p^{0}(f)\exp\left(\sum _{h=1}^{H}\sum_{\iota=1}^{t-1}\widetilde{L}_{h}^{\iota}(f)\right)\mathrm{d}f} =\frac{p^{0}(f)\exp\left(\sum_{h=1}^{H}\sum_{\iota=1}^{t-1} \widetilde{L}_{h}^{\iota}(f)\right)}{\mathbb{E}_{f\sim p^{0}}\left[\exp\left( \sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\widetilde{L}_{h}^{\iota}(f)\right)\right]}\]

according to Lemma 10, such that plugging in the above distribution leads to the first equality. Thus, according to the definitions of \(\widetilde{L}_{h}^{t}\) and \(\overline{L}_{h}^{t}\), we have

\[\mathbb{E}_{Z^{t-1}} \mathbb{E}_{f^{t}\sim p^{t}}\left[-\sum_{h=1}^{H}\sum_{\iota=1}^{t -1}\eta\log\frac{\mathbb{P}_{f,h}(s_{h+1}^{i}|s_{h}^{i},a_{h}^{i},b_{h}^{i})}{ \mathbb{P}_{f^{*},h}(s_{h+1}^{i}|s_{h}^{i},a_{h}^{i},b_{h}^{i})}+\log\frac{p^{t }(f^{t})}{p^{0}(f^{t})}\right]\] \[\geq\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f^{t}\sim p^{t}}\left[-\sum_{ h=1}^{H}\sum_{\iota=1}^{t-1}\log\mathbb{E}_{(\pi^{t},\nu^{t},\mathbb{P}_{f^{*}},h)} \exp\left(\eta\log\frac{\mathbb{P}_{f,h}(s_{h+1}^{i}|s_{h}^{i},a_{h}^{i},b_{h}^ {i})}{\mathbb{P}_{f^{*},h}(s_{h+1}^{i}|s_{h}^{i},a_{h}^{i},b_{h}^{i})}\right) \right].\]

Moreover, to further lower bound the RHS of the above inequality, by the inequality that \(\log x\leq x-1\) and the setting \(\eta=\frac{1}{2}\), we have

\[-\log\mathbb{E}_{(\pi^{t},\nu^{t},\mathbb{P}_{f^{*}},h)}\exp\left( \log\frac{\sqrt{\mathbb{P}_{f,h}(s_{h+1}^{i}|s_{h}^{i},a_{h}^{i},b_{h}^{i})}}{ \sqrt{\mathbb{P}_{f^{*},h}(s_{h+1}^{i}|s_{h}^{i},a_{h}^{i},b_{h}^{i})}}\right)\] \[=-\log\mathbb{E}_{(\pi^{t},\nu^{t},\mathbb{P}_{f^{*}},h)}\frac{ \sqrt{\mathbb{P}_{f,h}(s_{h+1}^{i}|s_{h}^{i},a_{h}^{i},b_{h}^{i})}}{\sqrt{ \mathbb{P}_{f^{*},h}(s_{h+1}^{i}|s_{h}^{i},a_{h}^{i},b_{h}^{i})}}\] \[\geq 1-\mathbb{E}_{(\pi^{t},\nu^{t},\mathbb{P}_{f^{*}},h)}\frac{ \sqrt{\mathbb{P}_{f,h}(s_{h+1}^{i}|s_{h}^{i},a_{h}^{i},b_{h}^{i})}}{\sqrt{ \mathbb{P}_{f^{*},h}(s_{h+1}^{i}|s_{h}^{i},a_{h}^{i},b_{h}^{i})}}\] \[=1-\mathbb{E}_{(\pi^{t},\nu^{t},h)}\int_{S}\sqrt{\mathbb{P}_{f,h}(s |s_{h}^{t},a_{h}^{i},b_{h}^{i})\mathbb{P}_{f^{*},h}(s|s_{h}^{t},a_{h}^{i},b_{h} ^{i})}\mathrm{d}s\] \[=\mathbb{E}_{(\pi^{t},\nu^{t},h)}[D_{\mathrm{He}}^{2}(\mathbb{P} _{f,h}(\cdot|s_{h}^{t},a_{h}^{i},b_{h}^{i}),\mathbb{P}_{f^{*},h}(\cdot|s_{h}^{t},a_{h}^{t},b_{h}^{i}))],\]where the last equality is by \(D^{2}_{\mathrm{He}}(P,Q)=\frac{1}{2}\int(\sqrt{\mathrm{d}P(x)}-\sqrt{\mathrm{d}Q( x)})^{2}=1-\int\sqrt{\mathrm{d}P(x)\mathrm{d}Q(x)}\). Here \(\mathbb{E}_{(\pi^{t},\nu^{t},h)}\) denotes taking expectation over \((s^{t}_{h},a^{t}_{h},b^{t}_{h})\) sampled following the policy pair \((\pi^{t},\nu^{t})\) and the true model \(\mathbb{P}_{f^{\ast}}\) of the \(h\)-th step but without the next state \(s^{t}_{h+1}\) generated by \(\mathbb{P}_{f^{\ast},h}(\cdot|s^{t}_{h},a^{t}_{h},b^{t}_{h})\) at round \(\iota\). In the sequel, combining the above results, we have

\[\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f^{t}\sim p^{t}}\left[-\sum_{h=1} ^{H}\sum_{\iota=1}^{t-1}\left(L^{\iota}_{h}(f^{t})-L^{\iota}_{h}(f^{\ast}) \right)+\log\frac{p^{t}(f^{t})}{p^{0}(f^{t})}\right]\] \[\geq\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f^{t}\sim p^{t}}\left[\sum_{ h=1}^{H}\sum_{\iota=1}^{t-1}\mathbb{E}_{(\pi^{t},\nu^{t},h)}[D^{2}_{\mathrm{He}}( \mathbb{P}_{f,h}(\cdot|s^{t}_{h},a^{t}_{h},b^{t}_{h}),\mathbb{P}_{f^{\ast},h}( \cdot|s^{t}_{h},a^{t}_{h},b^{t}_{h}))]\right].\]

This completes the proof. 

### Proof of Lemma 12

Proof.: The proof of Lemma 12 is similar to the proof of Lemma 11. We will give a brief description of the main steps for our proof of Lemma 12. We start our proof by considering the following equality

\[\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f^{t}\sim p^{t}}\left[-\sum_{h=1} ^{H}\sum_{\iota=1}^{t-1}\left(L^{\iota}_{h}(f^{t})-L^{\iota}_{h}(f^{\ast}) \right)+\log\frac{p^{t}(f^{t})}{p^{0}(f^{t})}\right]\] \[=\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f^{t}\sim p^{t}}\left[\sum_{h=1} ^{H}\sum_{\iota=1}^{t-1}\eta\log\frac{\mathbb{P}_{f^{\ast},h}(\tau^{\iota}_{h} )}{\mathbb{P}_{f^{\ast},h}(\tau^{\iota}_{h})}+\log\frac{p^{t}(f^{t})}{p^{0}(f^ {t})}\right].\]

Next, we lower bound RHS of the above equality. We define

\[\overline{L}^{\iota}_{h}(f):=\eta\log\frac{\mathbb{P}_{f,h}(\tau^{\iota}_{h} )}{\mathbb{P}_{f^{\ast},h}(\tau^{\iota}_{h})},\qquad\widetilde{L}^{\iota}_{h} (f):=\overline{L}^{\iota}_{h}(f)-\log\mathbb{E}_{(\pi^{t},\nu^{\nu},h)}[\exp( \overline{L}^{\iota}_{h}(f))],\]

where \(\mathbb{E}_{(\pi^{t},\nu^{t},h)}\) denotes taking an expectation over the data \(\tau^{\iota}_{h}\) sampled following the policy pair \((\pi^{t},\nu^{t})\) and the true model \(\theta_{f^{\ast}}\) to the \(h\)-th step at round \(\iota\). Then, we can show that

\[\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f^{t}\sim p^{t}}\left[-\sum_{h=1}^{H}\sum_{ \iota=1}^{t-1}\widetilde{L}^{\iota}_{h}(f^{t})+\log\frac{p^{t}(f^{t})}{p^{0}(f ^{t})}\right]\geq 0,\]

following a similar derivation as (20) in the proof of Lemma 11. With setting \(\eta=\frac{1}{2}\), this result further leads to

\[-\log\mathbb{E}_{(\pi^{t},\nu^{t},h)}\exp\left(\log\frac{\sqrt{ \mathbb{P}_{f,h}(\tau^{\iota}_{h})}}{\sqrt{\mathbb{P}_{f^{\ast},h}(\tau^{\iota }_{h})}}\right)\] \[\geq 1-\mathbb{E}_{(\pi^{t},\nu^{t},h)}\frac{\sqrt{\mathbb{P}_{f,h} (\tau^{\iota}_{h})}}{\sqrt{\mathbb{P}_{f^{\ast},h}(\tau^{\iota}_{h})}}=1- \mathbb{E}_{(\pi^{t},\nu^{t},h)}\frac{\sqrt{\mathbb{P}_{f,h}^{\pi^{t},\nu^{t}} (\tau^{\iota}_{h})}}{\sqrt{\mathbb{P}_{f^{\ast},h}^{\pi^{t},\nu^{t}}(\tau^{ \iota}_{h})}}\] \[=1-\int_{(\mathcal{O}\times\mathcal{A})^{h}}\sqrt{\mathbb{P}_{f,h} ^{\pi^{t},\nu^{t}}(\tau_{h})\mathbb{P}_{f^{\ast},h}^{\pi^{t},\nu^{t}}(\tau_{h })}\mathrm{d}\tau_{h}\] \[=D^{2}_{\mathrm{He}}(\mathbb{P}_{f,h}^{\pi^{t},\nu^{t}},\mathbb{P }_{f^{\ast},h}^{\pi^{t},\nu^{t}}).\]

In the sequel, combining the above results, we have

\[\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\mathbb{E}_{Z^{t-1}}\mathbb{E}_ {f^{t}\sim p^{t}}[D^{2}_{\mathrm{He}}(\mathbb{P}_{f^{t},h}^{\pi^{t},\nu^{t}}, \mathbb{P}_{f^{\ast},h}^{\pi^{t},\nu^{t}})]\] \[\qquad\leq\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f^{t}\sim p^{t}}\left[- \sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\left(L^{\iota}_{h}(f^{t})-L^{\iota}_{h}(f^{ \ast})\right)+\log\frac{p^{t}(f^{t})}{p^{0}(f^{t})}\right].\]

This completes the proof.

### Proof of Lemma 13

Proof.: We first prove the upper bound of \(|V_{f^{*}}^{\pi,\nu}-V_{f}^{\pi,\nu}|\) for any \((\pi,\nu)\) and \(f\) under FOMG and POMG settings separately.

For FOMGs, we let \(V_{f^{*}}^{\pi,\nu}=V_{f^{*}}^{\pi,\nu}\) and \(V_{f}^{\pi,\nu}=V_{f}^{\pi,\nu}\). Then, according to the Bellman equation that \(Q_{f,h}^{\pi,\nu}(s,a,b)=r_{h}(s,a,b)+\langle\mathbb{P}_{f,h}(\cdot|s,a,b),V_{f, h+1}^{\pi,\nu}(\cdot)\rangle\) and also \(V_{f,h}^{\pi,\nu}(s,a,b)=\mathbb{E}_{a\sim\pi_{h}(\cdot|s),b\sim\nu_{h}(\cdot| s)}[Q_{f,h}^{\pi,\nu}(s,a,b)]\), we have

\[\left|V_{f^{*}}^{\pi,\nu}-V_{f}^{\pi,\nu}\right|\] \[\qquad\leq\mathbb{E}_{\pi,\nu}\left|\langle\mathbb{P}_{f^{*}}( \cdot|s_{1},a_{1},b_{1}),V_{f^{*},2}^{\pi,\nu}(\cdot)\rangle-\langle\mathbb{ P}_{f,1}(\cdot|s_{1},a_{1},b_{1}),V_{f,2}^{\pi,\nu}(\cdot)\rangle\right|\] \[\qquad\leq H\mathbb{E}_{\pi,\nu}\left\|\mathbb{P}_{f^{*}}(\cdot |s_{1},a_{1},b_{1})-\mathbb{P}_{f,1}(\cdot|s_{1},a_{1},b_{1})\right\|_{1}+ \mathbb{E}_{\pi,\nu,\mathbb{P}_{f^{*}}}\left|V_{f^{*},2}^{\pi,\nu}(s_{2})-V_{ f,2}^{\pi,\nu}(s_{2})\right|\] \[\qquad\vdots\qquad(\text{recursively applying the above derivation})\] \[\qquad\leq H\mathbb{E}_{\pi,\nu,\mathbb{P}_{f^{*}}}\sum_{h=1}^{H} \left\|\mathbb{P}_{f^{*},h}(\cdot|s_{h},a_{h},b_{h})-\mathbb{P}_{f,h}(\cdot|s _{h},a_{h},b_{h})\right\|_{1},\]

which further leads to

\[\left|V_{f^{*}}^{\pi,\nu}-V_{f}^{\pi,\nu}\right|\] \[\qquad\leq H^{2}\sup_{h,s,a,b}\left\|\mathbb{P}_{f^{*},h}(\cdot |s,a,b)-\mathbb{P}_{f,h}(\cdot|s,a,b)\right\|_{1}\] \[\qquad\leq 3H^{2}\sup_{h,s,a,b}\mathrm{KL}^{\frac{1}{2}}( \mathbb{P}_{f^{*},h}(\cdot|s,a,b)\|\mathbb{P}_{f,h}(\cdot|s,a,b))\leq 3H^{2}\varepsilon,\]

where the third inequality is by Pinsker's inequality and the last inequality is by the assumption of this lemma. On the other hand, we can show that the above result also holds for POMGs. Then, for this setting, we have

\[\left|V_{f^{*}}^{\pi,\nu}-V_{f}^{\pi,\nu}\right| =\int_{(\mathcal{O}\times\mathcal{A}\times\mathcal{B})^{H}}( \mathbf{P}_{f^{*},H}^{\pi,\nu}(\tau_{H})-\mathbf{P}_{f,H}^{\pi,\nu}(\tau_{H}) )\bigg{(}\sum_{h=1}^{H}r_{h}(o_{h},a_{h},b_{h})\bigg{)}\mathrm{d}\tau_{H}\] \[=H\int_{(\mathcal{O}\times\mathcal{A}\times\mathcal{B})^{H}} \left|\mathbf{P}_{f^{*},H}^{\pi,\nu}(\tau_{H})-\mathbf{P}_{f,H}^{\pi,\nu}(\tau _{H})\right|\mathrm{d}\tau_{H}\] \[=H\|\mathbf{P}_{f^{*},H}^{\pi,\nu}(\tau_{H})-\mathbf{P}_{f,H}^{ \pi,\nu}(\tau_{H})\|_{1}\] \[=3H\sup_{\pi,\nu}\mathrm{KL}^{\frac{1}{2}}(\mathbf{P}_{f^{*},H}^{ \pi,\nu}\|\mathbf{P}_{f,H}^{\pi,\nu})\leq 3H\varepsilon.\]

To unify our results, we enlarge the above bound by a factor of \(H\) and eventually obtain that

\[\left|V_{f^{*}}^{\pi,\nu}-V_{f}^{\pi,\nu}\right|\leq 3H^{2}\varepsilon,\]

for both FOMGs and POMGs. Moreover, by the properties of the operators \(\min\), \(\max\), and \(\max\min\), we have

\[V_{f^{*}}^{*}-V_{f}^{*}=\max_{\pi}\min_{\nu}V_{f^{*}}^{\pi,\nu}-\max_{\pi}\min_ {\nu}V_{f}^{\pi,\nu}\leq\sup_{\pi,\nu}|V_{f^{*}}^{\pi,\nu}-V_{f}^{\pi,\nu}| \leq 3H^{2}\varepsilon.\]

\[\sup_{\pi}(V_{f^{*}}^{\pi,*}-V_{f}^{\pi,*}) =\sup_{\pi}(\min_{\nu}V_{f^{*}}^{\pi,\nu}-\min_{\nu}V_{f}^{\pi,\nu })\leq\sup_{\pi,\nu}|V_{f^{*}}^{\pi,\nu}-V_{f}^{\pi,\nu}|\leq 3H^{2}\varepsilon.\]

\[\sup_{\nu}(V_{f^{*}}^{\pi,\nu}-V_{f}^{*,\nu}) =\sup_{\nu}(\sum_{\pi}V_{f^{*}}^{\pi,\nu}-\sum_{\pi}V_{f}^{\pi,\nu })\leq\sup_{\pi,\nu}|V_{f^{*}}^{\pi,\nu}-V_{f}^{\pi,\nu}|\leq 3H^{2}\varepsilon.\]

This concludes the proof of this lemma.

### Proof of Lemma 14

Proof.: We first prove the upper bound under the FOMG setting. By (1), we know that

\[L_{h}^{t}(f^{*})-L_{h}^{t}(f) =\eta\log\mathbb{P}_{f^{*},h}(s_{h+1}^{t}\,|\,s_{h}^{t},a_{h}^{t}, b_{h}^{t})-\eta\log\mathbb{P}_{f,h}(s_{h+1}^{t}\,|\,s_{h}^{t},a_{h}^{t},b_{h}^{t})\] \[=\eta\log\frac{\mathbb{P}_{f^{*},h}(s_{h+1}^{t}\,|\,s_{h}^{t},a_{h }^{t},b_{h}^{t})}{\mathbb{P}_{f,h}(s_{h+1}^{t}\,|\,s_{h}^{t},a_{h}^{t},b_{h}^{t })},\]

which further leads to

\[|\mathbb{E}(L_{h}^{t}(f^{*})-L_{h}^{t}(f))| =\eta\left|\mathbb{E}\log\frac{\mathbb{P}_{f^{*},h}(s_{h+1}^{t}\, |\,s_{h}^{t},a_{h}^{t},b_{h}^{t})}{\mathbb{P}_{f,h}(s_{h+1}^{t}\,|\,s_{h}^{t}, a_{h}^{t},b_{h}^{t})}\right|\] \[=\eta\left|\mathbb{E}_{(s_{h}^{t},a_{h}^{t},b_{h}^{t})}\mathbb{E} _{s_{h+1}^{t}\sim\mathbb{P}_{f^{*},h}(\cdot|s_{h}^{t},a_{h}^{t},b_{h}^{t})} \log\frac{\mathbb{P}_{f^{*},h}(s_{h+1}^{t}\,|\,s_{h}^{t},a_{h}^{t},b_{h}^{t})} {\mathbb{P}_{f,h}(s_{h+1}^{t}\,|\,s_{h}^{t},a_{h}^{t},b_{h}^{t})}\right|\] \[\leq\eta\sup_{s,a,b}\mathrm{KL}(\mathbb{P}_{f^{*},h}(\cdot\,|\,s, a,b)\|\mathbb{P}_{f,h}(\cdot\,|\,s,a,b))\] \[\leq\eta\varepsilon^{2},\]

where we use the definition of KL divergence in the first inequality and the last inequality is by the condition that \(\sup_{h,s,a,b}\mathrm{KL}^{\frac{1}{2}}(\mathbb{P}_{f^{*},h}(\cdot\,|\,s,a,b) \|\mathbb{P}_{f,h}(\cdot\,|\,s,a,b))\leq\varepsilon\).

We next prove the upper bound under the POMG setting. According to (2), we have

\[L_{h}^{t}(f^{*})-L_{h}^{t}(f)=\eta\log\mathbf{P}_{f^{*},h}(\tau_{h}^{t})-\eta \log\mathbf{P}_{f,h}(\tau_{h}^{t})=\eta\log\frac{\mathbf{P}_{f^{*},h}(\tau_{h }^{t})}{\mathbf{P}_{f,h}(\tau_{h}^{t})},\]

which further leads to

\[|\mathbb{E}(L_{h}^{t}(f^{*})-L_{h}^{t}(f))| =\eta\left|\mathbb{E}_{\tau_{h}^{t}\sim\mathbf{P}_{f^{*},h}^{\pi ^{t},\nu^{t}}(\cdot)}\log\frac{\mathbf{P}_{f^{*},h}(\tau_{h}^{t})}{\mathbf{P} _{f,h}(\tau_{h}^{t})}\right|\] \[=\eta\left|\mathbb{E}_{\tau_{h}^{t}\sim\mathbf{P}_{f^{*},h}^{\pi ^{t},\nu^{t}}(\cdot)}\log\frac{\mathbf{P}_{f^{*},h}^{\pi^{t},\nu^{t}}(\tau_{h }^{t})}{\mathbf{P}_{f,h}^{\pi^{t},\nu^{t}}(\tau_{h}^{t})}\right|\] \[=\eta\mathrm{KL}(\mathbf{P}_{f^{*},h}^{\pi^{t},\nu^{t}}\|\mathbf{P} _{f,h}^{\pi^{t},\nu^{t}}),\]

where we use the definition of KL divergence and the relation between \(\mathbf{P}_{f,h}\) and \(\mathbf{P}_{f,h}^{\pi,\nu}\). Now we consider to lower bound \(\mathrm{KL}(\mathbf{P}_{f^{*},H}^{\pi^{t},\nu^{t}}\|\mathbf{P}_{f,H}^{\pi^{t}, \nu^{t}})\). We have

\[\mathrm{KL}(\mathbf{P}_{f^{*},H}^{\pi^{t},\nu^{t}}\|\mathbf{P}_{f,H}^{\pi^{t},\nu^{t}}) =\mathbb{E}_{\tau_{h}^{t}\sim\mathbf{P}_{f^{*},h}^{\pi^{t},\nu^{ t}}(\cdot)}\log\frac{\mathbf{P}_{f^{*},\nu^{t}}^{\pi^{t},\nu^{t}}(\tau_{h}^{t})}{ \mathbf{P}_{f,H}^{\pi^{t},\nu^{t}}(\tau_{h}^{t})}\] \[=\mathbb{E}_{\tau_{h}^{t}\sim\mathbf{P}_{f^{*},H}^{\pi^{t},\nu^{ t}}(\cdot)}\left[\log\frac{\mathbf{P}_{f^{*},h}^{\pi^{t},\nu^{t}}(\tau_{h}^{t})}{ \mathbf{P}_{f,h}^{\pi^{t},\nu^{t}}(\tau_{h}^{t})}+\log\frac{\mathrm{Pr}_{f^{*} }^{\pi^{t},\nu^{t}}(\tau_{h+1:H}^{t}|\tau_{h}^{t})}{\mathrm{Pr}_{f^{*},\nu^{t}} (\tau_{h+1:H}^{t}|\tau_{h}^{t})}\right]\] \[=\mathrm{KL}(\mathbf{P}_{f^{*},h}^{\pi^{t},\nu^{t}}\|\mathbf{P}_{f,h}^{\pi^{t},\nu^{t}})+\mathbb{E}_{\tau_{h}^{t}\sim\mathbf{P}_{f^{*},h}^{\pi^{t},\nu^{t}}(\cdot)}\mathrm{KL}(\mathrm{Pr}_{f^{*}}^{\pi^{t},\nu^{t}}(\cdot|\tau_{ h}^{t})\|\,\mathrm{Pr}_{f}^{\pi^{t},\nu^{t}}(\cdot|\tau_{h}^{t}))\] \[\geq\mathrm{KL}(\mathbf{P}_{f^{*},h^{*}}^{\pi^{t},\nu^{t}}\|\mathbf{ P}_{f,h}^{\pi^{t},\nu^{t}}),\]

where \(\mathrm{Pr}_{f}^{\pi^{t},\nu^{t}}\) denote the probability under the model \(f\) and the policy pair \(\pi^{t},\nu^{t}\), the third equality is by the definition of KL divergence, and the inequality is by the non-negativity of KL divergence. Therefore, combining the above results and the condition \(\sup_{\pi,\nu}\mathrm{KL}^{\frac{1}{2}}(\mathbf{P}_{f^{*},H}^{\pi,\nu}\| \mathbf{P}_{f,H}^{\pi,\nu})\leq\varepsilon\), we obtain

\[|\mathbb{E}(L_{h}^{t}(f^{*})-L_{h}^{t}(f))|\leq\eta\mathrm{KL}(\mathbf{P}_{f^{*},H}^{\pi^{t},\nu^{t}}\|\mathbf{P}_{f,H}^{\pi^{t},\nu^{t}})\leq\eta\varepsilon^{2}.\]

This concludes the proof of this lemma.

Proofs for Algorithms 1 and 3

In this section, we provide the detailed proof for Theorem 1. In particular, our proof is compatible with both the FOMG and the POMG settings. Thus, we unify both setups in a single theorem and present their proof together in this section.

To characterize the value difference under models \(f\) and \(f^{*}\) we define the following terms, which are

\[\Delta V_{f}^{\pi,*}(s):=V_{f}^{\pi,*}(s)-V_{f^{*}}^{\pi,*}(s),\] (21)

and

\[\Delta V_{f}^{*}(s):=V_{f}^{*}(s)-V_{f^{*}}^{*}(s).\] (22)

In addition, we define the difference of likelihood functions at step \(h\) of time \(t\) as

\[\Delta L_{h}^{t}(f)=L_{h}^{t}(f)-L_{h}^{t}(f^{*}).\] (23)

Then, the updating rules of the posterior distribution in Algorithm 1 have the following equivalent forms

\[p^{t}(f)\propto p^{0}(f)\exp\Big{[}\gamma_{1}V_{f}^{*}+\sum_{i= 1}^{t-1}\sum_{h=1}^{H}L_{h}^{\tau}(f)\Big{]}\] \[\iff p^{t}(f)\propto p^{0}(f)\exp\Big{[}\gamma_{1}\Delta V_{f}^{*}+\sum_ {i=1}^{t-1}\sum_{h=1}^{H}\Delta L_{h}^{\tau}(f)\Big{]},\] (24)

and

\[q^{t}(f)\propto q^{0}(f)\exp[-\gamma_{2}V_{f}^{\pi^{t},*}+\sum_{i =1}^{t-1}\sum_{h=1}^{H}L_{h}^{\tau}(f)]\] \[\iff q^{t}(f)\propto q^{0}(f)\exp[-\gamma_{2}\Delta V_{f}^{\pi^{t},*}+ \sum_{i=1}^{t-1}\sum_{h=1}^{H}\Delta L_{h}^{\tau}(f)].\] (25)

since here adding or subtracting terms irrelevant to \(f\), i.e., \(V_{f^{*}}^{\pi,*}\), \(V_{f^{*}}^{*}\), and \(L_{h}^{t}(f^{*})\), within the power of all exponential terms will not change the posterior distribution of \(f\).

To learning a sublinear upper bound for the expected value of the total regret, i.e., \(\mathbb{E}[\mathrm{Reg}^{\mathrm{sp}}(T)]=\mathbb{E}\sum_{t=1}^{T}[V_{f^{*}}^{ *,\nu^{t}}-V_{f^{*}}^{\pi^{t},*}]\) for the self-play setting, we need to execute both Algorithm 1 and 3, which are two symmetric algorithms. We can decompose \(\mathrm{Reg}^{\mathrm{sp}}(T)\) as

\[\mathrm{Reg}^{\mathrm{sp}}(T)=\mathrm{Reg}^{\mathrm{sp}}_{1}(T)+ \mathrm{Reg}^{\mathrm{sp}}_{2}(T),\]

where we define

\[\mathrm{Reg}^{\mathrm{sp}}_{1}(T):=\sum_{t=1}^{T}[V_{f^{*}}^{*}-V_{f^{*}}^{ \pi^{t},*}],\qquad\mathrm{Reg}^{\mathrm{sp}}_{2}(T):=\sum_{t=1}^{T}[V_{f^{*}}^{ *,\nu^{t}}-V_{f^{*}}^{*}].\]

In fact, executing Algorithm 1 leads to a low regret upper bound for \(\mathbb{E}[\mathrm{Reg}^{\mathrm{sp}}_{1}(T)]\) while running Algorithm 3 incurs a low regret upper bound for \(\mathbb{E}[\mathrm{Reg}^{\mathrm{sp}}_{2}(T)]\). Since the two algorithms are symmetric, the derivation of their respective regret bounds is thus similar. In the following subsections, we present the proofs of Proposition 1, Proposition 2, and Theorem 1 sequentially.

### Proof of Proposition 1

Proof.: We start our proof by first decomposing the regret as follows

\[\mathrm{Reg}^{\mathrm{sp}}_{1}(T)=\underbrace{\sum_{t=1}^{T}[V_{f^{*}}^{*}-V_ {f^{*}}^{\pi^{t},\underline{\nu}^{t}}]}_{\text{Term(i)}}+\underbrace{\sum_{t= 1}^{T}[V_{f^{*}}^{\pi^{t},\underline{\nu}^{t}}-V_{f^{*}}^{\pi^{t},*}]}_{ \text{Term(ii)}}.\]Our goal is to give the upper bound for the expected value of the total regret, which is. Thus, we need to derive the upper bounds of and separately. Intuitively, according to our analysis below, can be viewed as the regret incurred by the updating rules for the main player in Line 3 and Line 4 of Algorithm 1, and is associated with the exploiter's updating rules in Line 5 and Line 6 of Algorithm 1,

**Bound.** To bound, we give the following decomposition

(i) (ii)

where according to Algorithm 1, we have, i.e., is the NE of. Thus, the second equality is by (ii), and the inequality is due to

(i)

where the second inequality is due to. Combining the above results, we have

We need to further bound the RHS of the above equality. Note that for FOMGs and POMGs, we have different definitions of. Specifically, according to our definition of, for FOMGs, we define

and for POMGs, we have the relation

On the other hand, for the two MG settings, we also define as in (1) and (2). According to Lemmas 11 and 12, unifying their results, we can obtain which further gives

\[\mathbb{E}_{Z^{t-1}} \mathbb{E}_{\overline{f}^{t}\sim p^{t}}\left[-\gamma_{1}\Delta V_{ \overline{f}^{t}}^{*}+\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\mathbb{E}_{(\pi^{*}, \omega^{\iota},h)}\ell(\overline{f}^{t},\xi_{h}^{t})\right]\] \[\leq\mathbb{E}_{Z^{t-1}}\mathbb{E}_{\overline{f}^{t}\sim p^{t}} \left[-\gamma_{1}\Delta V_{\overline{f}^{t}}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^ {t-1}\Delta L_{h}^{\iota}(\overline{f}^{t})+\log\frac{p^{t}(\overline{f}^{t})} {p^{0}(\overline{f}^{t})}\right]\] \[=\mathbb{E}_{Z^{t-1}}\mathbb{E}_{\overline{f}^{t}\sim p^{t}} \left[-\gamma_{1}\Delta V_{\overline{f}^{t}}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^ {t-1}\Delta L_{h}^{\iota}(\overline{f}^{t})-\log p^{0}(\overline{f}^{t})+\log p ^{t}(\overline{f}^{t})\right].\]

where \(\Delta L_{h}^{t}(f):=L_{h}^{t}(f)-L_{h}^{t}(f^{*})\) is defined in (23). Note that in the above analysis, we slightly abuse the notation of \(\pi^{t}\) and \(\underline{\nu}^{t}\), which denote the Markovian policies for FOMG and history-dependent policies for POMGs. Therefore, according to Lemma 10, we obtain that the distribution

\[p^{t}(f) \propto\exp\left(\gamma_{1}\Delta V_{f}^{*}+\sum_{h=1}^{H}\sum_{ \iota=1}^{t-1}\Delta L_{h}^{\iota}(f)+\log p^{0}(f)\right)\] \[=p^{0}(f)\exp\left(\gamma_{1}\Delta V_{f}^{*}+\sum_{h=1}^{H}\sum_ {\iota=1}^{t-1}\Delta L_{h}^{\iota}(f)\right)\]

minimizes the last term in the above inequality. As discussed in (24), this distribution is an equivalent form of the posterior distribution updating rule for \(p^{t}\) adopted in Line 3 of Algorithm 1. This result implies that

\[\mathbb{E}_{Z^{t-1}} \mathbb{E}_{\overline{f}^{t}\sim p^{t}}\left[-\gamma_{1}\Delta V _{\overline{f}^{t}}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota }(\overline{f}^{t})+\log\frac{p^{t}(\overline{f}^{t})}{p^{0}(\overline{f}^{t})}\right]\] \[=\mathbb{E}_{Z^{t-1}}\inf_{p}\mathbb{E}_{f\sim p}\left[-\gamma_{1 }\Delta V_{f}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota}(f)+ \log\frac{p(f)}{p^{0}(f)}\right].\]

By Definition 2, further letting \(\widetilde{p}:=p^{0}(f)\cdot\mathbf{1}(f\in\mathcal{F}(\varepsilon))/p^{0}( \mathcal{F}(\varepsilon))\), we have

\[\mathbb{E}_{Z^{t-1}}\inf_{p}\mathbb{E}_{f\sim p}\left[-\gamma_{1 }\Delta V_{f}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota}(f)+ \log\frac{p(f)}{p^{0}(f)}\right]\] \[\leq\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f\sim\widetilde{p}}\left[- \gamma_{1}\Delta V_{f}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{ \iota}(f)+\log\frac{p(f)}{p^{0}(f)}\right]\] \[\leq(3H^{2}\gamma_{1}+\eta HT)\varepsilon-\log p^{0}(\mathcal{F}( \varepsilon))\] \[\leq\omega(4HT,p^{0}),\]

where the second inequality is by Lemma 13 and Lemma 14 as well as \(\varepsilon\leq 1\), and the last inequality is by our setting that \(\eta=\frac{1}{2}\) and \(H\gamma_{1}\leq T\). Therefore, by combining the above results, we obtain

\[\mathbb{E}[\text{Term(i)}] \leq\frac{\omega(HT,p^{0})T}{\gamma_{1}}+\frac{\gamma_{1}d_{ \mathrm{GEC}}H}{4}+2H(d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT\] \[\leq 4\sqrt{d_{\mathrm{GEC}}HT\omega(4HT,p^{0})},\] (26)

where we set \(HT>d_{\mathrm{GEC}}\), \(\epsilon=1/\sqrt{HT}\) for self-play GEC in Definition 1, and \(\gamma_{1}=2\sqrt{\omega(4HT,p^{0})T/d_{\mathrm{GEC}}}\). Since we set \(\epsilon=1/\sqrt{HT}\), the self-play GEC \(d_{\mathrm{GEC}}\) is defined associated with \(\epsilon=1/\sqrt{HT}\) by Definition 1. However, our analysis in Appendix B shows that this setting only introduces \(\log T\) factors in \(d_{\mathrm{GEC}}\) in the worst case.

**Bound \(\mathbb{E}[\text{Term(ii)}]\).** Next, we consider to bound \(\mathbb{E}[\text{Term(ii)}]\). We start by decomposing Term(ii) as follows,

\[\text{Term(ii)} =\sum_{t=1}^{T}[V_{f^{*}}^{\pi^{t},\underline{v}^{t}}-V_{\underline {f}^{t}}^{\pi^{t},\underline{v}^{t}}+V_{\underline{f}^{t}}^{\pi^{t}, \underline{v}^{t}}-V_{f^{*}}^{\pi^{t},*}]\] \[=\sum_{t=1}^{T}[V_{f^{*}}^{\pi^{t},\underline{v}^{t}}-V_{ \underline{f}^{t}}^{\pi^{t},\underline{v}^{t}}]+\sum_{t=1}^{T}\Delta V_{ \underline{f}^{t}}^{\pi^{t},*},\]

where the second equality is by (21) and the fact that \(\underline{\nu}^{t}=\operatorname*{argmin}_{\nu}V_{\underline{f}^{t}}^{\pi^{t },\nu}\).

In addition, according to Definition 1 and the updating rules in Algorithm 1, setting \(\sigma^{t}=(\pi^{t},\underline{\nu}^{t})\), \(\rho^{t}=\underline{f}^{t}\), we have

\[\left|\sum_{t=1}^{T}\left(V_{\underline{f}^{t}}^{\pi^{t}, \underline{v}^{t}}-V_{f^{*}}^{\pi^{t},\underline{v}^{t}}\right)\right|\] \[\quad\leq\left[d_{\mathrm{GEC}}\sum_{h=1}^{H}\sum_{t=1}^{T}\Big{(} \sum_{t=1}^{t-1}\mathbb{E}_{(\sigma^{t},h)}\ell(\underline{f}^{t},\xi_{h}^{t}) \Big{)}\right]^{1/2}+2H(d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT\] \[\quad\leq\frac{1}{\gamma_{2}}\sum_{h=1}^{H}\sum_{t=1}^{T}\Big{(} \sum_{t=1}^{t-1}\mathbb{E}_{(\pi^{t},\underline{\nu}^{t},h)}\ell(\underline{f} ^{t},\xi_{h}^{t})\Big{)}+\frac{\gamma_{2}d_{\mathrm{GEC}}}{4}+2H(d_{\mathrm{ GEC}}HT)^{\frac{1}{2}}+\epsilon HT,\]

where the second inequality is due to \(\sqrt{xy}\leq\frac{1}{\gamma_{2}}x^{2}+\frac{\gamma_{2}}{4}y^{2}\). Thus, we obtain that

\[\text{Term(ii)} =\frac{1}{\gamma_{2}}\sum_{t=1}^{T}\sum_{h=1}^{H}\Big{(}\sum_{t=1} ^{t-1}\mathbb{E}_{(\pi^{t},\underline{\nu}^{t},h)}\ell(\underline{f}^{t},\xi_ {h}^{t})\Big{)}\] \[\quad+\sum_{t=1}^{T}\Delta V_{\underline{f}^{t}}^{\pi^{t},*}+ \frac{\gamma_{2}d_{\mathrm{GEC}}}{4}+2H(d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+ \epsilon HT.\]

We further bound the RHS of the above equality. By the definitions of \(\ell(f,\xi_{h})\) for FOMGs and POMGs and also the definitions of \(L_{h}^{t}(f)\) as in (1) and (2), according to Lemmas 11 and 12, we can obtain

\[\mathbb{E}_{Z^{t-1},\overline{f}^{t}\sim p^{t}}\mathbb{E}_{ \underline{f}^{t}\sim q^{t}}\left[\gamma_{2}\Delta V_{\underline{f}^{t}}^{\pi^ {t},*}+\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\mathbb{E}_{(\pi^{t},\underline{\nu }^{t},h)}\ell(\underline{f}^{t},\xi_{h}^{t})\right]\] \[\quad\leq\mathbb{E}_{Z^{t-1},\overline{f}^{t}\sim p^{t}}\mathbb{ E}_{\underline{f}^{t}\sim q^{t}}\left[\gamma_{2}\Delta V_{\underline{f}^{t}}^{ \pi^{t},*}-\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{t}(\underline{f}^{t} )+\log\frac{q^{t}(\underline{f}^{t})}{q^{0}(\underline{f}^{t})}\right]\] \[\quad=\mathbb{E}_{Z^{t-1},\overline{f}^{t}\sim p^{t}}\inf_{q} \mathbb{E}_{f\sim q}\left[\gamma_{2}\Delta V_{f}^{\pi^{t},*}-\sum_{h=1}^{H} \sum_{\iota=1}^{t-1}\Delta L_{h}^{t}(f)+\log\frac{q(f)}{q^{0}(f)}\right],\]

where the expectation for \(\overline{f}^{t}\sim p^{t}\) exists due to that \(\pi^{t}\) is computed based on \(\overline{f}^{t}\), and also according to Lemma 10 and (25), the last equality can be achieved by that the updating rule for the distribution \(q^{t}\) in Algorithm 1, i.e., equivalently

\[q^{t}(f) \propto\exp\left(-\gamma_{2}\Delta V_{f}^{\pi^{t},*}+\sum_{h=1}^{ H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{t}(f)+\log q^{0}(f)\right)\] \[\quad=q^{0}(f)\exp\left(-\gamma_{2}\Delta V_{f}^{\pi^{t},*}+\sum_ {h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{t}(f)\right),\]

can minimize \(\mathbb{E}_{f\sim q}\left[\gamma_{2}\Delta V_{f}^{\pi^{t},*}-\sum_{h=1}^{H} \sum_{\iota=1}^{t-1}\Delta L_{h}^{t}(f)+\log\frac{q(f)}{q^{0}(f)}\right]\).

By Definition 2, further letting \(\widetilde{q}:=q^{0}(f)\cdot\mathbf{1}(f\in\mathcal{F}(\varepsilon))/q^{0}( \mathcal{F}(\varepsilon))\) with \(\varepsilon\leq 1\), we have

\[\mathbb{E}_{Z^{t-1},\overline{f}^{t}\sim p^{t}}\inf_{q}\mathbb{E}_ {f\sim q}\left[\gamma_{2}\Delta V_{f}^{\pi^{t},*}-\sum_{h=1}^{H}\sum_{\iota=1}^ {t-1}\Delta L_{h}^{\iota}(f)+\log\frac{q(f)}{q^{0}(f)}\right]\] \[\qquad\leq\mathbb{E}_{Z^{t-1},\overline{f}^{t}\sim p^{t}}\mathbb{ E}_{f\sim\widetilde{q}}\left[\gamma_{2}\Delta V_{f}^{\pi^{t},*}-\sum_{h=1}^{H} \sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota}(f)+\log\frac{q(f)}{q^{0}(f)}\right]\] \[\qquad\leq(3H^{2}\gamma_{2}+\eta HT)\varepsilon-\log q^{0}( \mathcal{F}(\varepsilon))\] \[\qquad\leq\omega(4HT,q^{0}),\]

where the second inequality is by Lemma 13 and Lemma 14 as well as \(\varepsilon\leq 1\), and the last inequality is by our setting that \(\eta=\frac{1}{2}\) and \(H\gamma_{2}\leq T\). Therefore, combining the above results, we have

\[\mathbb{E}[\text{Term(ii)}] \leq\frac{\omega(HT,q^{0})T}{\gamma_{2}}+\frac{\gamma_{2}d_{ \mathrm{GEC}}H}{4}+2H(d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT\] \[\leq 4\sqrt{d_{\mathrm{GEC}}HT\cdot\omega(4HT,q^{0})},\] (27)

where we set \(HT>d_{\mathrm{GEC}}\), \(\epsilon=1/\sqrt{HT}\) for self-play GEC in Definition 1, and \(\gamma_{2}=2\sqrt{\omega(4HT,q^{0})T/d_{\mathrm{GEC}}}\).

**Combining Results.** Finally, combining the results in (26) and (27), we eventually obtain

\[\mathbb{E}[\mathrm{Reg}_{1}^{\mathrm{sp}}(T)] =\mathbb{E}[\text{Term(i)}]+\mathbb{E}[\text{Term(ii)}]\leq 6 \sqrt{d_{\mathrm{GEC}}HT(\omega(4HT,p^{0})+\omega(4HT,q^{0}))}.\]

This completes the proof. 

### Proof of Proposition 2

Proof.: Due to the symmetry of Algorithms 1 and 3, we can similarly bound \(\mathbb{E}[\mathrm{Reg}_{2}^{\mathrm{sp}}(T)]\) in the way of bounding \(\mathbb{E}[\mathrm{Reg}_{2}^{\mathrm{sp}}(T)]\). Therefore, in this subsection, we only present the main steps for the proof. Specifically, by Algorithm 3, we have a decomposition as

\[\mathrm{Reg}_{2}^{\mathrm{sp}}(T)=\underbrace{\sum_{t=1}^{T}[V_{f^{*}}^{*,\nu^ {t}}-V_{\overline{f}^{t}}^{\pi^{t},\nu^{t}}]}_{\text{Term(iv)}}+\underbrace{ \sum_{t=1}^{T}[V_{\overline{f}^{t}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{*}]}_{\text{ Term(iii)}}.\]

**Bound \(\mathbb{E}[\text{Term(iii)}]\).** To bound \(\mathbb{E}[\text{Term(iii)}]\), we give the following decomposition

\[\text{Term(iii)} =\sum_{t=1}^{T}[V_{f^{*}}^{\pi^{t},\nu^{t}}-V_{\overline{f}^{t}}^ {\pi^{t},\nu^{t}}+V_{\overline{f}^{t}}^{\pi^{t},\nu^{t}}-V_{\overline{f}^{t}}^ {\pi^{t},\nu^{t}}+V_{\overline{f}^{t}}^{\overline{\pi}^{t},\nu^{t}}-V_{ \overline{f}^{*}}^{*}]\] \[\leq\sum_{t=1}^{T}[V_{\overline{f}^{t}}^{\pi^{t},\nu^{t}}-V_{ \overline{f}^{t}}^{\pi^{t},\nu^{t}}+V_{\overline{f}^{t}}^{\pi^{t},\nu^{t}}-V_ {f^{*}}^{*}]\] \[=\sum_{t=1}^{T}\Delta V_{\overline{f}^{t}}^{*}+\sum_{t=1}^{T}[V_{ \overline{f}^{*}}^{\pi^{t},\nu^{t}}-V_{\overline{f}^{t}}^{\pi^{t},\nu^{t}}],\]

where according to Algorithm 3, we have \((\overline{\pi}^{t},\nu^{t})=\operatorname*{argmax}_{\pi}\operatorname*{ argmin}_{\nu}V_{\overline{f}^{t}}^{\pi,\nu}\), which thus leads to

\[V_{\overline{f}^{t}}^{\overline{\pi}^{t},\nu^{t}}=\max_{\pi}V_{\overline{f}^{t }}^{\pi,\nu^{t}}\geq V_{\overline{f}^{t}}^{\underline{\pi}^{t},\nu^{t}}.\]

According to the condition **(2)** in Definition 1 for self-play GEC and the updating rules in Algorithm 3, setting the exploration policy pair as \(\sigma^{t}=(\underline{\pi}^{t},\nu^{t})\), we have

\[\left|\sum_{t=1}^{T}\left(V_{\overline{f}^{t},1}^{\pi^{t},\nu^{t }}-V_{\overline{f}^{*}}^{\pi^{t},\nu^{t}}\right)\right|\] \[\qquad\leq\frac{1}{\gamma_{1}}\sum_{h=1}^{H}\sum_{t=1}^{T}\Big{(} \sum_{t=1}^{t-1}\mathbb{E}_{(\underline{\pi}^{t},\nu^{t},h)}\ell(\overline{f}^ {t},\xi_{h}^{t})\Big{)}+\frac{\gamma_{1}d_{\mathrm{GEC}}}{4}+2H(d_{\mathrm{GEC }}HT)^{\frac{1}{2}}+\epsilon HT.\]We need to further bound the RHS of the above equality. For FOMGs and POMGs, by their definitions of \(\ell(f,\xi_{h})\) as well as \(L_{h}^{t}(f)\), according to Lemmas 11 and 12, we obtain

\[\mathbb{E}_{Z^{t-1}} \mathbb{E}_{\overline{f}^{t}\sim p^{t}}\left[\gamma_{1}\Delta V_{ \overline{f}^{t}}^{*}+\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\mathbb{E}_{(\pi^{t}, \omega^{t},h)}\ell(\overline{f}^{t},\xi_{h}^{t})\right]\] \[\leq\mathbb{E}_{Z^{t-1}}\mathbb{E}_{\overline{f}^{t}\sim p^{t}} \left[\gamma_{1}\Delta V_{\overline{f}^{t}}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^{t -1}\Delta L_{h}^{\iota}(\overline{f}^{t})+\log\frac{p^{t}(\overline{f}^{t})}{p ^{0}(\overline{f}^{t})}\right],\]

where \(\Delta L_{h}^{t}(f):=L_{h}^{t}(f)-L_{h}^{t}(f^{*})\) is defined in (23). According to Lemma 10, we obtain that the distribution \(p^{t}\) defined in Algorithm 3, equivalently

\[p^{t}(f)\propto p^{0}(f)\exp\left(-\gamma_{1}\Delta V_{f}^{*}+\sum_{h=1}^{H} \sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota}(f)\right)\]

minimizes the last term in the above inequality. This result implies that

\[\mathbb{E}_{Z^{t-1}} \mathbb{E}_{\overline{f}^{t}\sim p^{t}}\left[\gamma_{1}\Delta V_{ \overline{f}^{t}}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota}( \overline{f}^{t})+\log\frac{p^{t}(\overline{f}^{t})}{p^{0}(\overline{f}^{t})}\right]\] \[=\mathbb{E}_{Z^{t-1}}\inf_{p}\mathbb{E}_{f\sim p}\left[\gamma_{1} \Delta V_{f}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota}(f)+ \log\frac{p(f)}{p^{0}(f)}\right].\]

By Definition 2, further letting \(\widetilde{p}:=p^{0}(f)\cdot\mathbf{1}(f\in\mathcal{F}(\varepsilon))/p^{0}( \mathcal{F}(\varepsilon))\), we have

\[\mathbb{E}_{Z^{t-1}} \inf_{p}\mathbb{E}_{f\sim p}\left[\gamma_{1}\Delta V_{f}^{*}- \sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota}(f)+\log\frac{p(f)}{p^{ 0}(f)}\right]\] \[\leq\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f\sim\widetilde{p}}\left[ \gamma_{1}\Delta V_{f}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{ \iota}(f)+\log\frac{p(f)}{p^{0}(f)}\right]\] \[\leq(3H^{2}\gamma_{1}+\eta HT)\varepsilon-\log p^{0}(\mathcal{F}( \varepsilon))\] \[\leq\omega(4HT,p^{0}),\]

where the second inequality is by Lemma 13 and Lemma 14 as well as \(\varepsilon\leq 1\), and the last inequality is by our setting that \(\eta=\frac{1}{2}\) and \(H\gamma_{1}\leq T\). Therefore, by combining the above results, we obtain

\[\mathbb{E}[\text{Term(iii)}] \leq\frac{\omega(HT,p^{0})T}{\gamma_{1}}+\frac{\gamma_{1}d_{ \mathrm{GEC}}H}{4}+2H(d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT\] \[\leq 4\sqrt{d_{\mathrm{GEC}}HT\omega(4HT,p^{0})},\] (28)

where we set \(HT>d_{\mathrm{GEC}}\), \(\epsilon=1/\sqrt{HT}\) for self-play GEC in Definition 1, and \(\gamma_{1}=2\sqrt{\omega(4HT,p^{0})T/d_{\mathrm{GEC}}}\).

**Bound \(\mathbb{E}[\text{Term(iv)}]\).** Next, we consider to bound \(\mathbb{E}[\text{Term(iv)}]\) as follows,

\[\text{Term(iv)} =\sum_{t=1}^{T}[V_{f^{*}}^{*,\nu^{t}}-V_{\underline{f}^{t}}^{ \underline{\pi}^{t},\nu^{t}}+V_{\underline{f}^{t}}^{\underline{\pi}^{t},\nu^ {t}}-V_{\overline{f}^{t}}^{\underline{\pi}^{t},\nu^{t}}]\] \[=-\sum_{t=1}^{T}\Delta V_{\underline{f}^{t}}^{*,\nu^{t}}+\sum_{t =1}^{T}[V_{\underline{f}^{t}}^{\underline{\pi}^{t},\nu^{t}}-V_{f^{*}}^{ \underline{\pi}^{t},\nu^{t}}],\]

where the second equality is by the fact that \(\underline{\pi}^{t}=\operatorname*{argmax}_{\pi}V_{\underline{f}^{t}}^{\pi,\nu^ {t}}\) according to the updating rule.

By Definition 1 and the updating rules in Algorithm 3, we have

\[\left|\sum_{t=1}^{T} \left(V_{\underline{f}^{t}}^{\underline{\pi}^{t},\nu^{t}}-V_{ \overline{f}^{t}}^{\underline{\pi}^{t},\nu^{t}}\right)\right|\] \[\leq\frac{1}{\gamma_{2}}\sum_{h=1}^{H}\sum_{t=1}^{T}\Big{(}\sum_{ \iota=1}^{t-1}\mathbb{E}_{(\underline{\pi}^{t},\nu^{t},h)}\ell(\underline{f}^ {t},\xi_{h}^{t})\Big{)}+\frac{\gamma_{2}d_{\mathrm{GEC}}}{4}+2H(d_{\mathrm{GEC} }HT)^{\frac{1}{2}}+\epsilon HT.\]By the definitions of \(\ell(f,\xi_{h})\) for FOMGs and POMGs and also the definitions of \(L^{t}_{h}(f)\) as in (1) and (2), we have

\[\mathbb{E}_{Z^{t-1},\overline{f}^{t}\sim p^{t}}\mathbb{E}_{\underline {f}^{t}\sim q^{t}}\left[-\gamma_{2}\Delta V^{*,\nu^{t}}_{f}+\sum_{h=1}^{H}\sum_ {\iota=1}^{t-1}\mathbb{E}_{(\overline{\iota}^{t},\omega^{t},h)}\ell(\underline {f}^{t},\xi_{h}^{t})\right]\] \[\qquad\leq\mathbb{E}_{Z^{t-1},\overline{f}^{t}\sim p^{t}}\mathbb{ E}_{\underline{f}^{t}\sim q^{t}}\left[-\gamma_{2}\Delta V^{*,\nu^{t}}_{f}-\sum_{h=1}^{H} \sum_{\iota=1}^{t-1}\Delta L^{\iota}_{h}(\underline{f}^{t})+\log\frac{q^{t}( \underline{f}^{t})}{q^{0}(\underline{f}^{t})}\right]\] \[\qquad=\mathbb{E}_{Z^{t-1},\overline{f}^{t}\sim p^{t}}\inf_{q} \mathbb{E}_{f\sim q}\left[-\gamma_{2}\Delta V^{\pi^{t},*}_{f}-\sum_{h=1}^{H} \sum_{\iota=1}^{t-1}\Delta L^{\iota}_{h}(f)+\log\frac{q(f)}{q^{0}(f)}\right],\]

where the expectation for \(\overline{f}^{t}\sim p^{t}\) exists due to that \(\nu^{t}\) is computed based on \(\overline{f}^{t}\), and also according to Lemma 10, the last equality can be achieved by that the updating rule for the distribution \(q^{t}\) in Algorithm 3, i.e., equivalently

\[q^{t}(f)\propto q^{0}(f)\exp\left(\gamma_{2}\Delta V^{\pi^{t},*}_{f}+\sum_{h=1 }^{H}\sum_{\iota=1}^{t-1}\Delta L^{\iota}_{h}(f)\right),\]

can minimize \(\mathbb{E}_{f\sim q}\left[-\gamma_{2}\Delta V^{*\pi^{t},\nu^{t}}_{f}-\sum_{h=1 }^{H}\sum_{\iota=1}^{t-1}\Delta L^{\iota}_{h}(f)+\log\frac{q(f)}{q^{0}(f)}\right]\). By Definition 2, further letting \(\widetilde{q}:=q^{0}(f)\cdot\mathbf{1}(f\in\mathcal{F}(\varepsilon))/q^{0}( \mathcal{F}(\varepsilon))\) with \(\varepsilon\leq 1\), we have

\[\mathbb{E}_{Z^{t-1},\overline{f}^{t}\sim p^{t}}\inf_{q}\mathbb{E} _{f\sim q}\left[\gamma_{2}\Delta V^{\pi^{t},*}_{f}-\sum_{h=1}^{H}\sum_{\iota =1}^{t-1}\Delta L^{\iota}_{h}(f)+\log\frac{q(f)}{q^{0}(f)}\right]\] \[\qquad\leq\mathbb{E}_{Z^{t-1},\overline{f}^{t}\sim p^{t}}\mathbb{ E}_{f\sim\widetilde{q}}\left[\gamma_{2}\Delta V^{\pi^{t},*}_{f}-\sum_{h=1}^{H} \sum_{\iota=1}^{t-1}\Delta L^{\iota}_{h}(f)+\log\frac{q(f)}{q^{0}(f)}\right]\] \[\qquad\leq(3H^{2}\gamma_{2}+\eta HT)\varepsilon-\log q^{0}( \mathcal{F}(\varepsilon))\] \[\qquad\leq\omega(4HT,q^{0}),\]

where the second inequality is by Lemma 13 and Lemma 14 as well as \(\varepsilon\leq 1\), and the last inequality is by our setting that \(\eta=\frac{1}{2}\) and \(H\gamma_{2}\leq T\). Therefore, combining the above results, we have

\[\mathbb{E}[\text{Term(iv)}] \leq\frac{\omega(HT,q^{0})T}{\gamma_{2}}+\frac{\gamma_{2}d_{ \mathrm{GEC}}H}{4}+2H(d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT\] \[\leq 4\sqrt{d_{\mathrm{GEC}}HT\cdot\omega(4HT,q^{0})},\] (29)

where we set \(HT>d_{\mathrm{GEC}}\), \(\epsilon=1/\sqrt{HT}\) for self-play GEC in Definition 1, and \(\gamma_{2}=2\sqrt{\omega(4HT,q^{0})T/d_{\mathrm{GEC}}}\).

**Combining Results.** Finally, combining the results in (28) and (29), we eventually obtain

\[\mathbb{E}[\mathrm{Reg}_{2}^{\mathrm{sp}}(T)]=\mathbb{E}[\text{Term(iii)}]+ \mathbb{E}[\text{Term(iv)}]\leq 6\sqrt{d_{\mathrm{GEC}}HT(\omega(4HT,p^{0})+ \omega(4HT,q^{0}))}.\]

This completes the proof. 

### Proof of Theorem 1

Proof.: The proof of Theorem 1 is immediate by the relation that \(\mathrm{Reg}^{\mathrm{sp}}(T)=\mathrm{Reg}_{1}^{\mathrm{sp}}(T)+\mathrm{Reg}_{2 }^{\mathrm{sp}}(T)\). Thus, according to the above proofs of Proposition 1 and Proposition 2, under the conditions in both propositions, we have that

\[\mathbb{E}[\mathrm{Reg}^{\mathrm{sp}}(T)] =\mathbb{E}[\mathrm{Reg}_{1}^{\mathrm{sp}}(T)]+\mathbb{E}[\mathrm{ Reg}_{2}^{\mathrm{sp}}(T)]\] \[\leq 12\sqrt{d_{\mathrm{GEC}}HT\cdot(\omega(4HT,p^{0})+\omega(4HT,q^{0}))}.\]

This completes the proof.

Proofs for Algorithm 2

In this section, we provide the detailed proof for Theorem 2 and unify the proofs for both the FOMG and POMG settings together.

We recall that the value difference and the likelihood function difference are already defined in (22) and (23), which are

\[\Delta V_{f}^{*}:=V_{f}^{*}-V_{f^{*}}^{*},\] \[\Delta L_{h}^{t}(f)=L_{h}^{t}(f)-L_{h}^{t}(f^{*}).\]

Then, similar to (24), the updating rules of posterior distribution in Algorithm 2 have the following equivalent form as

\[p^{t}(f)\propto p^{0}(f)\exp\Big{[}\gamma V_{f}^{*}+\sum_{\iota =1}^{t-1}\sum_{h=1}^{H}L_{h}^{\tau}(f)\Big{]}\] \[\iff p^{t}(f)\propto p^{0}(f)\exp\Big{[}\gamma\Delta V_{f}^{*}+\sum_{ \iota=1}^{t-1}\sum_{h=1}^{H}\Delta L_{h}^{\tau}(f)\Big{]},\] (30)

since adding or subtracting terms irrelevant to \(f\), i.e., \(V_{f^{*}}^{*}\) and \(L_{h}^{t}(f^{*})\), within the power of all exponential terms will not change the posterior distribution of \(f\).

### Proof of Theorem 2

Proof.: To bound the expected value of the adversarial regret \(\operatorname{Reg}^{\mathrm{adv}}(T)\), i.e., \(\mathbb{E}[\operatorname{Reg}^{\mathrm{adv}}(T)]\), we first decompose the regret \(\operatorname{Reg}^{\mathrm{adv}}_{w}\) as follows

\[\operatorname{Reg}^{\mathrm{adv}}(T) =\sum_{t=1}^{T}[V_{f^{*}}^{*}-V_{f^{t}}^{*}]+\sum_{t=1}^{T}[V_{f ^{*}}^{*}-V_{f^{*}}^{\pi^{t},\nu^{t}}]\] \[=-\sum_{t=1}^{T}\Delta V_{f^{t}}^{*}+\sum_{t=1}^{T}[V_{f^{*}}^{*}- V_{f^{*}}^{\pi^{t},\nu^{t}}].\]

Furthermore, for the term \(\sum_{t=1}^{T}[V_{f^{t}}^{*}-V_{f^{*}}^{\pi^{t},\nu^{t}}]\), we have

\[\sum_{t=1}^{T}[V_{f^{t}}^{*}-V_{f^{*}}^{\pi^{t},\nu^{t}}] =\sum_{t=1}^{T}[V_{f^{t}}^{\pi^{t},\overline{\nu}^{t}}-V_{f^{*}}^ {\pi^{t},\nu^{t}}]\] \[\leq\sum_{t=1}^{T}[V_{f^{t}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{\pi^{t},\nu^{t}}],\]

where the above result is by \((\pi^{t},\overline{\nu}^{t})=\operatorname{argmax}_{\pi}\operatorname{argmin} _{\nu}V_{f^{t}}^{\pi,\nu}\), i.e., \((\pi^{t},\overline{\nu}^{t})\) is the NE of \(V_{f^{t}}^{\pi,\nu}\), according to Algorithm 2, and also due the relation that

\[V_{f^{t}}^{\pi^{t},\overline{\nu}^{t}}=\min_{\nu}V_{f^{t}}^{\pi^{t},\nu}\leq V _{f^{t}}^{\pi^{t},\nu^{t}}.\]

Thus, we have

\[\operatorname{Reg}^{\mathrm{adv}}(T)=-\sum_{t=1}^{T}\Delta V_{f^{t}}^{*}+\sum _{t=1}^{T}[V_{f^{t}}^{\pi^{t},\nu^{t}}-V_{f^{*}}^{\pi^{t},\nu^{t}}].\]

[MISSING_PAGE_FAIL:51]

minimizes the last term in the above inequality, which is the posterior distribution updating rule for \(p^{t}\) adopted in Algorithm 2. This result implies that

\[\mathbb{E}_{Z^{t-1}} \mathbb{E}_{f^{t}\sim p^{t}}\left[-\gamma\Delta V_{f^{t}}^{*}-\sum_ {h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota}(f^{t})+\log\frac{p^{t}(f^{t}) }{p^{0}(f^{t})}\right]\] \[=\mathbb{E}_{Z^{t-1}}\inf_{p}\mathbb{E}_{f\sim p}\left[-\gamma \Delta V_{f}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota}(f^{t}) +\log\frac{p(f)}{p^{0}(f)}\right].\]

For the adversarial setting, we define \(\mathcal{F}(\varepsilon)\) as in Definition 2 similar to the definition of \(\mathcal{F}(\varepsilon)\) in the self-play setting. Further letting \(\widetilde{p}:=p^{0}(f)\cdot\mathbf{1}(f\in\mathcal{F}(\varepsilon))/p^{0}( \mathcal{F}(\varepsilon))\) with \(\varepsilon\leq 1\) and \(\omega\) be associated with \(\mathcal{F}(\varepsilon)\), we have

\[\mathbb{E}_{Z^{t-1}} \inf_{p}\mathbb{E}_{f\sim p}\left[-\gamma\Delta V_{f}^{*}-\sum_ {h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota}(f)+\log\frac{p(f)}{p^{0}(f) }\right]\] \[\leq\mathbb{E}_{Z^{t-1}}\mathbb{E}_{f\sim\widetilde{p}}\left[- \gamma\Delta V_{f}^{*}-\sum_{h=1}^{H}\sum_{\iota=1}^{t-1}\Delta L_{h}^{\iota} (f)+\log\frac{p(f)}{p^{0}(f)}\right]\] \[\leq(3H^{2}\gamma+\eta HT)\varepsilon-\log p^{0}(\mathcal{F}( \varepsilon))\] \[\leq\omega(4HT,p^{0}),\]

where the second inequality is by Lemma 13 and Lemma 14 as well as \(\varepsilon\leq 1\), and the last inequality is by our setting that \(\eta=\frac{f}{2}\) and \(\gamma\leq T\). Therefore, by combining the above results, we obtain

\[\mathbb{E}[\mathrm{Reg}^{\mathrm{adv}}(T)] \leq\frac{\omega(4HT,p^{0})T}{\gamma}+\frac{\gamma d_{\mathrm{GEC }}H}{4}+2H(d_{\mathrm{GEC}}HT)^{\frac{1}{2}}+\epsilon HT\] \[\leq 4\sqrt{d_{\mathrm{GEC}}HT\cdot\omega(4HT,p^{0})},\]

where we set \(HT>d_{\mathrm{GEC}}\), \(\epsilon=1/\sqrt{HT}\) for adversarial GEC in Definition 3, and \(\gamma=2\sqrt{\omega(4HT,p^{0})T/d_{\mathrm{GEC}}}\). Note that since we set \(\epsilon=1/\sqrt{HT}\), the adversarial GEC \(d_{\mathrm{GEC}}\) is now calculated associated with \(\epsilon=1/\sqrt{HT}\) by Definition 3. Further by our analysis in Appendix B, we know that this setting only introduces \(\log T\) factors in \(d_{\mathrm{GEC}}\) in the worst case. This concludes the proof. 

## Appendix G Other Supporting Lemmas

**Lemma 15** (Elliptical Potential Lemma in [1], Lemma 11).: _Suppose \(\{\phi_{t}\}_{t\geq 0}\) is a sequence in \(\mathbb{R}^{d}\) satisfying \(\|\phi_{t}\|_{2}\leq 1\), and \(\Lambda_{0}\) is a positive definite \(d\times d\) matrix. Let \(\Lambda_{t}=\Lambda_{0}+\sum_{\iota=1}^{t}\phi_{\iota}\phi_{\iota}^{\top}\). Then, the following inequalities hold_

\[\log\left(\frac{\det\Lambda_{t}}{\det\Lambda_{0}}\right)\leq\sum_{\iota=1}^{t }\min\left\{\phi_{\iota}^{\top}(\Lambda_{\iota-1})^{-1}\phi_{\iota},1\right\} \leq 2\log\left(\frac{\det\Lambda_{t}}{\det\Lambda_{0}}\right).\]

**Lemma 16** (Estimation of Elliptical Potential [38]).: _Given \(\lambda>0\) and \(\{\phi_{t}\}_{t\geq 0}\) with \(\|\phi_{t}\|_{2}\leq 1\), denoting \(\Lambda_{t}=\lambda\mathrm{I}+\sum_{\iota=1}^{t}\phi_{\iota}\phi_{\iota}^{\top}\), then \(\phi_{\iota}^{\top}(\Lambda_{\iota-1})^{-1}\phi_{\iota}\) is upper bounded by_

\[\frac{3d}{\log 2}\log\left(1+\frac{1}{\lambda\log 2}\right).\]

**Lemma 17** (\(\ell_{2}\) Eluder Technique [13, 83]).: _Suppose that \(\{w_{t,j}\in\mathbb{R}^{d}\}_{(t,j)\in[T]\times[J]}\), \(\{x_{t,i}\in\mathbb{R}^{d}_{\{t,i\}\in[T]\times[I]}\}_{(t,i)}\), and distributions \(\{p_{t}\in\Delta_{[I]}\}_{[T]}\) satisfy_

* \(\sum_{s=1}^{t-1}\mathbb{E}_{i\sim p_{s}}(\sum_{j=1}^{J}|w_{t,j}^{\top}x_{s,i}|) ^{2}\leq\gamma_{t}\)_,_
* \(\mathbb{E}_{i\sim p_{t}}\|x_{t,i}\|_{2}^{2}\leq R_{x}^{2}\)_,_* \(\sum_{j=1}^{J}\|w_{t,j}\|_{2}\leq R_{w}\)_,_

_for any \(1\leq t\leq T\). Then, for \(R>0\), we have_

\[\sum_{t=1}^{T}\min\{R,\mathbb{E}_{i\sim p_{t}}\Big{(}\sum_{j=1}^{J}|w_{t,j}^{ \top}x_{t,i}|\Big{)}\}\leq\Bigg{[}2d\Big{(}R^{2}T+\sum_{t=1}^{T}\gamma_{t} \Big{)}\cdot\log\Big{(}1+\frac{TR_{x}^{2}R_{w}^{2}}{R^{2}}\Big{)}\Bigg{]}^{1/2}.\]