# AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation

 Tong Wu\({}^{1}\)\({}^{1}\), Zhihao Fan\({}^{2}\)\({}^{1}\), Xiao Liu\({}^{3}\), Hai-Tao Zheng\({}^{1,8}\), Yeyun Gong\({}^{3}\), Yelong Shen\({}^{4}\),

**Jian Jiao\({}^{5}\), Juntao Li\({}^{6}\), Zhongyu Wei\({}^{2}\), Jian Guo\({}^{7}\), Nan Duan\({}^{3}\), Weizhu Chen\({}^{4}\)**

\({}^{1}\)Shezhen International Graduate School, Tsinghua University, \({}^{2}\) Fudan University,

\({}^{3}\)Microsoft Research Asia, \({}^{4}\)Microsoft Azure AI, \({}^{5}\)Microsoft,

\({}^{6}\)Soochow University, \({}^{7}\)IDEA Research, \({}^{8}\)Pengcheng Laboratory

{yegong, yeshe, nanduan, wzchen}@microsoft.com,

zheng.haitao@sz.tsinghua.edu.cn, guojian@idea.edu.cn

Work done during an internship at Microsoft Research Asia.These authors contributed equally to this work.Corresponding author.

###### Abstract

Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced sequential dependency in comparison to images, and the majority of existing language models are trained with a left-to-right auto-regressive approach. To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that the generation of tokens on the right depends on the generated ones on the left, a mechanism achieved through employing a dynamic number of denoising steps that vary based on token position. This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right. In a series of experiments on various text generation tasks, including text summarization, machine translation, and common sense generation, AR-Diffusion clearly demonstrated its superiority over existing diffusion language models and that it can be \(100\times\sim 600\times\) faster when achieving comparable results. Our code is available at this https URL.

## 1 Introduction

Text generation is a fundamental task within the field of natural language processing (NLP). Pretrained language models like GPT-4 [1], LLaMA [21], and Alpaca [18] have garnered significant attention with their ability to generate fluent and human-like textual content. These models utilize the auto-regressive (AR) Transformer decoders [21] to emit generated tokens one-by-one in sequential order from left to right. By leveraging the power of position dependency, AR models are able to enhance the naturalness, coherence, and adherence to human language conventions in the generated text [14].

Recent studies have shown the remarkable performance of diffusion models in image generation [15], motivating researchers to extend diffusion to text generation [16, 17, 18, 19, 20]. By introducing timestep, these methods progressively regulate the interpolation between the original tokens and Gaussian noise, then iteratively denoise for text generation. At each timestep, the diffusion-based text generator predictsall tokens simultaneously following Non-Auto-Regression (NAR) [Lewis et al., 2020, Qi et al., 2020, 2021, Li et al., 2022b], leading to faster decoding speed compared to AR. However, it also inherits the drawback of NAR, namely the sacrifice of inter-token position dependency [Li et al., 2022c] and the drop of generation performance [Bao et al., 2021].

To conduct a comprehensive analysis, we introduce a two-dimensional coordinate system to track the diffusion timestep of tokens \(f(\cdot)\) positioned at various locations. As illustrated in Figure 1, the system assigns the token position \(n\in[1,N]\) to the horizontal axis and the diffusion timestep \(t\in[0,T]\) to the vertical axis. Diffusion-LM [Li et al., 2022a], which is followed by existing diffusion-based text generation models, is shown in Figure 1(a). It assigns a uniform timestep \(t\) to all tokens. In contrast, tokens in the AR model depicted in Figure 1(b) exhibit distinct timesteps within a generation step (\(t_{i}\)). For instance, the already decoded token at position \(n_{1}\) has a timestep of \(0\), while the to-be-decoded token at position \(n_{2}\) has a timestep of \(T\). This approach effectively captures the sequential dependency. Motivated by this observation, we introduce AR-Diffusion, an auto-regressive diffusion method, for the disparity in token positions and the principle of sequential token identification.

In AR-Diffusion, we propose a **multi-level diffusion strategy** that includes both sentence-level and token-level diffusion. We randomly choose a sentence-level timestep \(t\), and assign **dynamic movement speeds**\(v(\cdot)\) by determining position-sensitive token-level timestep \(f(n,t)\) for each token. This enables tokens at the left of a sentence to undergo faster movement from random Gaussian noise to token embedding, while those at the right of the sentence experience slower movement to better utilize information from previously denoised tokens. During inference, to reduce the significant number of inference steps (e.g., 2,000) required in Diffusion-LM [Li et al., 2022a], SeqDiffSeq [Yuan et al., 2022] and GENIE [Lin et al., 2023], we introduce a skipping mechanism that collaborates with the multi-level diffusion strategy to accelerate the process.

Experimental results across various text generation tasks, such as text summarization, machine translation, and common sense generation, have consistently demonstrated that AR-Diffusion surpasses existing text diffusion models, including AR methods in terms of both quality and diversity. Moreover, our verification reveals that AR-Diffusion requires fewer resources during decoding while maintaining superior performance. It achieves \(100\times\) faster than SeqDiffSeq [Yuan et al., 2022] in machine translation and \(600\times\) faster than GENIE [Lin et al., 2023] in text summarization while delivering comparable results. Furthermore, it demonstrates promising results even in a challenging scenario where decoding is limited to only two steps.

Figure 1: Model behaviors illustrated on a two-dimensional coordinate system, where the horizontal axis stands for the position and the vertical axis represents the diffusion timestep. In the inference stage, different models will behave differently. (a) For the typical Diffusion-LM [Li et al., 2022a], each token share the identical movement speed \(v(n_{1},t_{i},t_{i+1})=v(n_{2},t_{i},t_{i+1})=|t_{i+1}-t_{i}|\). (b) For AR from the perspective of diffusion models, the tokens have two states based on the degree of interpolation between the original tokens and Gaussian noise: to be decoded (at timestep \(t=T\)) and already decoded (at timestep \(t=0\)). Specifically, we have \(v(n_{1},t_{i},t_{i+1})=0\) and \(v(n_{2},t_{i},t_{i+1})=T\). (c) In AR-Diffusion, \((n_{e},t_{e})\) is the coordinate of anchor point. Tokens in different positions exhibit varying movement speeds, such as \(v(n_{1},t_{i},t_{i+1})>v(n_{2},t_{i},t_{i+1})\) when \(n_{1}<n_{2}\).

Preliminary

### Conditional Generative Language Models

In the field of natural language generation, conditional generative models are commonly implemented using either auto-regressive (AR) or non-auto-regressive (NAR) methods. In AR (Vaswani et al., 2017), tokens on the right are predicted based on visible left tokens. The likelihood is given by \(p_{\text{AR}}(\bm{y}|\bm{x})=\prod_{i=1}^{N}p(\bm{y}_{i}|\bm{y}_{1:i-1};\bm{x})\), where \(y_{i}\) denotes the \(i\)-th token of \(\bm{y}\). On the other hand, NAR (Gu et al., 2017) assumes conditional independence among tokens and generates them uniformly without distinction during decoding, resulting in the likelihood \(p_{\text{NAR}}(\bm{y}|\bm{x})=\prod_{i=1}^{N}p(\bm{y}_{i}|\bm{x})\). This parallel generation approach is of lower quality compared to AR, although it offers a substantial speed advantage.

### Diffusion Models for Text Generation

Recently, Li et al. (2022) propose a natural language generation model based on the diffusion process, which is typically divided into a forward noising process and a reverse denoising process.

Specifically, the forward process is a fixed linear Gaussian model, which gradually perturbs the random variable \(\bm{z}_{0}\) until it becomes the standard Gaussian distribution. This can be formalized as:

\[q(\bm{z}_{t}\mid\bm{z}_{0};\bm{x})=\mathcal{N}(\bm{z}_{t};\sqrt{\bar{\alpha}_{ t}}\bm{z}_{0},(1-\bar{\alpha}_{t})\textbf{I}),\] (1)

where, \(\bar{\alpha}_{t}=\prod_{i=1}^{t}\alpha_{i}\), and \(\alpha_{i}\) is a coefficient that monotonically decreases with timestep \(t\), \(\bm{z}_{t}\) is the latent state at timestep \(t\).

The reverse process is to initiate from standard Gaussian noise and progressively utilize the denoising transition \(p_{\bm{\theta}}(\bm{z}_{t-1}|\bm{z}_{t};\bm{x})\) for generation.

\[p_{\bm{\theta}}(\bm{z}_{t-1}\mid\bm{z}_{t};\bm{x})=\mathcal{N} \big{(}\bm{z}_{t-1};\mu_{\bm{\theta}}(\bm{z}_{t},t;\bm{x}),\Sigma_{\bm{\theta }}(\bm{z}_{t},t;\bm{x})\big{)},\] (2)

where the mean \(\mu_{\bm{\theta}}\) and variance \(\Sigma_{\bm{\theta}}\) are learned from the model. In particular, we follow Li et al. (2022)'s approach of using predefined variance without trainable parameters.

To extend the continuous diffusion process to discrete text generation, Li et al. (2022) introduce an additional Markov transition from the discrete tokens \(\bm{y}\) to the latent variable \(\bm{z}_{0}\). In practice, we add an embedding step \(q_{\phi}(\bm{z}_{0}|\bm{y})=\mathcal{N}(\bm{z}_{0};\mathrm{Emb}(\bm{y}),(1- \alpha_{0})\textbf{I})\) in the forward process, and use a trainable rounding step which is parametrized by \(p_{\bm{\theta}}(\bm{y}|\bm{z}_{0};\bm{x})=\prod_{i=1}^{N}p_{\bm{\theta}}(y_{ i}|z_{0}^{i};\bm{x})\) in the reverse process. In each timestep, we utilize an encoder-decoder model \(\bm{g}_{\bm{\theta}}(\bm{z}_{t},t;\bm{x})\) to approximate \(\bm{z}_{0}\)(Lin et al., 2023) in a NAR manner and then estimate \(\mu_{\bm{\theta}}(\bm{z}_{t},t;\bm{x})\).

In consequence, combined with maximizing the evidence lower bound (ELBO) of \(\log p_{\bm{\theta}}(\bm{y}|\bm{x})\), our training objective of the conditional diffusion language model is:

\[\mathcal{L}=\mathbb{E}_{q_{\phi}(\bm{z}_{0:T}|\bm{y})}\left[-\log p_{\bm{ \theta}}(\bm{y}\mid\bm{z}_{0};\bm{x})+\sum_{t=1}^{T}\|\bm{z}_{0}-\bm{g}_{\bm{ \theta}}(\bm{z}_{t},t;\bm{x})\|^{2}\right].\] (3)

## 3 Methodology

### Multi-Level Diffusion

In the typical diffusion process, every token in the text sequence has the same diffusion timestep. In order to leverage the sequential nature of language, we enable tokens to have different diffusion timesteps during the forward and reverse pass. To accomplish this, we propose a multi-level diffusion strategy that includes both sentence-level and token-level diffusion. Firstly, at the sentence-level, we follow Diffusion-LM (Li et al., 2022) to randomly select a timestep \(t\). Secondly, at the token-level, we incorporate positional information \(n\in[1,N]\) based on the sentence-level timestep to regulate the diffusion timestep for the current token. The procedure is illustrated as:

\[\bm{z}_{t}=\big{(}\bm{z}_{f(1,t)}^{1},\bm{z}_{f(2,t)}^{2},\cdots,\bm{z}_{f(N,t )}^{N}\big{)},\] (4)

where \(N\) is the given target sentence length, \(\bm{z}_{t}\) is the sentence representation at timestep4\(t\), \(\bm{z}_{f(n,t)}^{n}\) is the latent representation for the \(n\)-th token at sentence-level timestep \(t\), and \(f(n,t)\) is a token-leveltimestep function that denotes the token-level diffusion timestep determined by token position \(n\) and sentence-level timestep \(t\).

We visualize the token-level timestep \(\big{(}n,f(n,t)\big{)}\) onto a two-dimensional coordinate system as Figure 1, which takes the token **position** as the horizontal axis and the sentence-level **timestep** as the vertical axis. Furthermore, to provide a more profound description of the characteristics of movement, we define the speed of movement as the following equation.

\[v(n,t_{i},t_{i+1})=f(n,t_{i+1})-f(n,t_{i}),\] (5)

where \(t_{i}\) and \(t_{i+1}\) are the start and end sentence-level diffusion timesteps. It can be observed that tokens in Diffusion-LM share the same movement speed, while those in AR exhibit different speeds.

### Token-Level Diffusion with Dynamic Movement Speed

Based on the speed of movement, we propose a fundamental principle, dynamic movement speed, for designing the token-level diffusion timestep function \(f(n,t)\) to take advantage of AR in diffusion. Specifically, elements on the left side of a sentence undergo higher movement speed from random Gaussian noise to token embedding, while those on the right side experience lower movement speed, thereby they can be generated in the later sentence-level timestep and utilize information from previously generated tokens more effectively.

```
0: Dataset \(\{(\bm{x},\bm{y})\}\), maximum timestep number \(T\) and maximum target length \(N\).
0: Optimized model parameters \(\bm{\theta}\).
1: Define an anchor point \((n_{e},t_{e})\)5.

Footnote 5: In particular, the anchor point is set as \((2\times N,T)\) in our implementation. The impact of different choices of the anchor point is discussed in supplementary material E.

```
0: Dataset \(\{(\bm{x},\bm{y})\}\), maximum timestep number \(T\) and maximum target length \(N\).
0: Optimized model parameters \(\bm{\theta}\).
1: Define an anchor point \((n_{e},t_{e})\)5.

Footnote 5: In particular, the anchor point is set as \((2\times N,T)\) in our implementation. The impact of different choices of the anchor point is discussed in supplementary material E.

\[f(n,t)=\text{clip}\big{(}\frac{t_{e}-t_{s}}{n_{e}-n_{s}}(n-n_{s})+t_{s},0,T \big{)}\] (7)

6: Sample \(\bm{z}_{f(n,t)}^{n}\) for each \(n\) in different positions with Gaussian reparameterization.

7: According to equation (3) and equation (9), employ gradient descent to optimize the objective:

\[\min_{\theta}\Big{[}-\log p_{\bm{\theta}}(\bm{y}\mid\bm{z}_{0};\bm{x})+\sum_{n =1}^{N}\big{\|}\bm{g}_{\bm{\theta}}(\bm{z}_{f(n,t)}^{n},f(n,t);\bm{x})-\bm{z} _{0}\big{\|}^{2}\Big{]}\] (8)

8:until converged ```

**Algorithm 1** Training Process of AR-Diffusion.

Following the guidance of the principle, we develop a token-level diffusion strategy with the linear function, which is shown in Figure 1(c). In particular, the procedure is illustrated in Algorithm 1, where \(\text{clip}(x,\min,\max)\) function is to clamp all elements in \(x\) into the range \([\min,\max]\). Specifically, in the forward process of diffusion, the start point goes to the left from \((N,0)\) to \((0,0)\) along the horizontal axis and then moves up to \((0,T)\) along the vertical axis. Therefore, the entire range of sentence-level timestep is extended to \([0,N+T]\).

In the reverse diffusion process, the multi-level diffusion follows the formula:

\[\bm{g}_{\bm{\theta}}\big{(}\bm{z}_{t},t;\bm{x})=\bm{g}_{\bm{\theta}}\Big{(} \big{(}\bm{z}_{f(1,t)}^{1},f(1,t)\big{)},\big{(}\bm{z}_{f(2,t)}^{2},f(2,t) \big{)},\cdots,\big{(}\bm{z}_{f(N,t)}^{N},f(N,t)\big{)};\bm{x}\Big{)},\] (9)

where \(\bm{g}_{\theta}(\bm{z}_{f(n,t)}^{n},f(n,t);\bm{x})\) denotes the \(n\)-th element.

### Inference with Skipping

Typically, the generation process needs to go through all the sentence-level timesteps from \(T+N\) to \(0\). To reduce the decoding time, we introduce a skipping mechanism that allows us to traverse a subset of timesteps.

```
0: Source condition \(\bm{x}\), number of decoding steps \(M\) and model parameters \(\bm{\theta}\). Output: Predicted target embedding \(\hat{\bm{y}}\).
1: Define an anchor point \((n_{e},t_{e})\).
2: Uniformly select a decreasing sequence of timesteps \(\{t_{i}\}_{i=0}^{M}\) ranging from \(T+N\) to \(0\).
3: Sample \(\bm{z}_{t_{0}}\sim\mathcal{N}(\bm{0},\mathbf{I})\).
4:for\(i=0\) to \(M-1\)do
5: Calculate the start point \((n_{s},t_{s})\) using equation (6).
6: Based on the current sentence-level inference steps \(t_{i}\) and the next one \(t_{i+1}\), assign token-level timesteps \(f(n,t_{i})\) and \(f(n,t_{i+1})\) to token in position \(n\) using equation (7).
7: Reverse sample \(\bm{z}_{t_{i+1}}=\big{(}\bm{z}_{f(1,t_{i+1})}^{1},\bm{z}_{f(2,t_{i+1})}^{2}, \cdots,\bm{z}_{f(N,t_{i+1})}^{N}\big{)}\) from \(p_{\theta}(\bm{z}_{t_{i+1}}\mid\bm{z}_{t_{i}};\bm{x})\) with the following formulas: \[p_{\theta}(\bm{z}_{t_{i+1}}\mid\bm{z}_{t_{i}};\bm{x})=\prod_{n=1}^{N}p_{\theta }\big{(}\bm{z}_{f(n,t_{i+1})}^{n}\mid\bm{z}_{f(n,t_{i})}^{n};\bm{x}\big{)}\] (10) \[p_{\theta}\big{(}\bm{z}_{f(n,t_{i+1})}^{n}\mid\bm{z}_{f(n,t_{i})}^{n};\bm{x} \big{)}\sim\mathcal{N}\big{(}\bm{z}_{f(n,t_{i+1})}^{n};\lambda\bm{z}_{f(n,t_{ i})}^{n}+\mu\bm{g}_{\theta}(\bm{z}_{f(n,t)}^{n},f(n,t);\bm{x}),\sigma\mathbf{I} \big{)}\] (11)
8:endfor
9: Map \(\bm{z}_{t_{M}}\) to the nearest embedding \(\hat{\bm{y}}\). ```

**Algorithm 2** Inference Process of AR-Diffusion with the Skipping Mechanism.

In practice, we propose an algorithm for the inference, illustrated in Algorithm 2.

\[\lambda=\frac{\sqrt{\frac{\bar{\alpha}_{f(n,t_{i})}}{\bar{\alpha}_{f(n,t_{i+1 })}}}(1-\bar{\alpha}_{f(n,t_{i+1})})}{1-\bar{\alpha}_{f(n,t_{i})}},\;\mu=\frac{ \sqrt{\bar{\alpha}_{f(n,t_{i+1})}}(1-\frac{\bar{\alpha}_{f(n,t_{i})}}{\bar{ \alpha}_{f(n,t_{i+1})}})}{1-\bar{\alpha}_{f(n,t_{i})}},\;\sigma=\frac{(1- \alpha_{f(n,t_{i})})(1-\bar{\alpha}_{f(n,t_{i+1})})}{1-\bar{\alpha}_{f(n,t_{i })}}\] (12)

In equation (10), the conditional distribution of \(\bm{z}_{t_{i+1}}\) is inferred by \(p_{\theta}(\bm{z}_{t_{i+1}}|\bm{z}_{t_{i}};\bm{x})\), and then we decompose it by positions due to the independent forward process of elements at different positions. From equation (11) to equation (12), we establish the relationship between tokens at different timesteps, and the detailed derivation can be found in supplementary material F.

## 4 Experiments

### Tasks and Datasets

Text SummarizationThis task involves taking a long document as input and generating a concise sentence as output. This requires models with the ability to identify important content and rewrite it in a condensed form. In our experiments, we use the publicly available XSum[Narayan et al., 2018] and Cnn/DailyMail Hermann et al. [2015] on GLGE6, which is also named as GLGE-Easy.

Footnote 6: https://microsoft.github.io/glge/

Footnote 7: https://github.com/facebookresearch/fairseq/tree/main/examples/translation

Machine TranslationTranslation is a widely used sequence-to-sequence task. The input is a sequence of words in the source language, and the output is a sequence of corresponding words in the target language. We choose the IWSLT 2014 dataset and the data processing method is to follow the scripts provided by fairseq7.

Footnote 8: https://inklab.usc.edu/CommonGen/

Common Sense GenerationIn this task, the model is provided with a concept set consisting of objects and actions as input. The objective is to generate a sentence that incorporates these concepts and describes a realistic scenario. We use CommonGen8 dataset for evaluation.

Footnote 8: https://github.com/facebookresearch/fairseq/tree/main/examples/translation

### Main Results

The results on different datasets are shown in Table 1, Table 2, Table 3 and Table 4. The best result is **bolded** and the second-best result is underlined. "\(k\)" indicates the number of generated candidate samples9. It can be seen from the results in each table that AR-Diffusion achieves the best performance.

Footnote 9: The relationship between sample number and results is discussed in supplementary material E.

During the inference process, we utilize **20** inference steps and employ Minimum Bayes Risk (MBR) (Kumar and Byrne, 2004) decoding to select the best sample, following (Li et al., 2022). We choose MBR instead of the selection approach in GENIE, as GENIE picks up the best sample by calculating the maximum score for each generated one using ground truth, which introduces unfairness. To ensure a fair comparison, we re-implement GENIE using our configuration and perform inference with 20 steps. More experimental details can be found in the supplementary material B.

Text SummarizationThe results presented in Table 1 and Table 2 clearly demonstrate that AR-Diffusion outperforms the existing NAR and Semi-NAR approaches across all metrics. Moreover, AR-Diffusion consistently achieves significant improvements over GENIE in terms of all indicators. Furthermore, in comparison to Transformer, AR-Diffusion outperforms it on both ROUGE-1 and ROUGE-L, while achieving comparable performance in terms of ROUGE-2. Notably, when the sample number is 500, AR-Diffusion demonstrates superiority over Transformer across all the measures.

Machine TranslationTable 3 presents the BLEU score implemented by SeqDiffuSeq setting11. AR-Diffusion outperforms the non-auto-regressive CNAT in greedy search for a single sample, and achieves a substantial gain. Moreover, the BLEU score of AR-Diffusion surpasses GENIE by a large margin and shows a slightly better performance than the AR Transformer. Specially, AR-Diffusion achieves a more powerful result at \(k\) = 500.

\begin{table}
\begin{tabular}{l|c|c c c} \hline \hline
**Methods** & **Pattern** & ROUGE-1 & ROUGE-2 & ROUGE-L \\ \hline NAT (Gu et al., 2017) & & 24.0 & 3.9 & 20.3 \\ iNAT (Lee et al., 2018) & & 24.0 & 4.0 & 20.4 \\ CMLM (Ghazvininejad et al., 2019) & & 23.8 & 3.6 & 20.2 \\ LevT (Gu et al., 2019) & & 24.8 & 4.2 & 20.9 \\ \hline InsT (Stern et al., 2019) & & 17.7 & 5.2 & 16.1 \\ iNAT (Lee et al., 2018) & & 27.0 & 6.9 & 22.4 \\ CMLM (Ghazvininejad et al., 2019) & & 29.1 & 7.7 & 23.0 \\ LevT (Gu et al., 2019) & & 25.3 & 7.4 & 21.5 \\ \hline LSTM (Greff et al., 2017) & AR10  & 25.1 & 6.9 & 19.9 \\ Transformer (Vaswani et al., 2017) & & 30.5 & 10.4 & 24.2 \\ \hline GENIE (Lin et al., 2023) (\(k\) = 50) & & 29.3 & 8.3 & 21.9 \\ AR-Diffusion (\(k\) = 50) & & 31.7 & 10.1 & 24.7 \\ AR-Diffusion (\(k\) = 500) & & **32.2** & **10.6** & **25.2** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on XSum test set. The results of NAR and Semi-NAR are from Qi et al. (2021), and the results of AR are from GLGE (Liu et al., 2021).

\begin{table}
\begin{tabular}{c|c|c c c} \hline \hline
**Methods** & **Pattern** & ROUGE-1 & ROUGE-2 & ROUGE-L \\ \hline LSTM (Greff et al., 2017) & AR & 37.3 & 15.7 & 34.4 \\ Transformer (Vaswani et al., 2017) & & 39.5 & 16.7 & 36.7 \\ \hline GENIE (Lin et al., 2023) (\(k\) = 50) & & 34.4 & 12.8 & 32.1 \\ AR-Diffusion (\(k\) = 50) & Diffusion & 39.6 & 16.3 & 37.1 \\ AR-Diffusion (\(k\) = 500) & & **40.2** & **17.1** & **37.7** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on Cnn/DailyMail test set. The results of AR are from GLGE Liu et al. (2021).

### Inference Efficiency

First, we use the number of function evaluations (NFE) as a measure to compare inference efficiency (Ye et al., 2023) in machine translation. From Table 3, it is evident that even when the NFE is reduced to 1% of SeqDiffuSeq (equivalent to \(100\times\) faster), AR-Diffusion still outperforms SeqDiffuSeq. Moreover, increasing the number of generated candidate samples (\(k=500\)) leads to further performance improvements, albeit with increased time consumption.

Second, we conduct experiments with an **extremely limited number of inference steps** (2 and 3)12 and compare the performance with that of GENIE in XSum. The results are presented in Table 5. When reducing the number of steps to 2, GENIE experiences a significant decline, with an average score of 4.20 in the AVG Drop column, while AR-Diffusion exhibits a comparatively smaller

\begin{table}
\begin{tabular}{l|c|c c|c c|c c} \hline \hline
**Methods** & **Pattern** & \multicolumn{2}{c|}{ROUGE-2/L} & \multicolumn{2}{c|}{BLEU-3/4} & \multicolumn{2}{c}{METEOR} & \multicolumn{1}{c}{SPICE} \\ \hline bRNN-CopyNet (Gu et al., 2016) & & 9.23 & 30.57 & 13.60 & 7.80 & 17.40 & 16.90 \\ Trans-CopyNet (Lin et al., 2020) & AR & 11.08 & 32.57 & 17.20 & 10.60 & 18.80 & 18.00 \\ MeanPooling-CopyNet (Lin et al., 2020) & & 11.36 & 34.63 & 14.80 & 8.90 & 19.20 & 20.20 \\ \hline LevT (Gu et al., 2019) & \multirow{2}{*}{NAR} & 12.22 & 35.42 & 23.10 & 15.00 & 22.10 & 21.40 \\ ConstLeven (Susanto et al., 2020) & & 13.47 & 35.19 & 21.30 & 12.30 & **25.00** & 23.20 \\ \hline GENIE (Lin et al., 2023) (\(k\) = 50) & \multirow{2}{*}{Diffusion} & 12.89 & 35.21 & 22.00 & 13.30 & 24.30 & 23.00 \\ AR-Diffusion (\(k\) = 50) & & **13.93** & **37.36** & **25.60** & **16.40** & **25.00** & **24.20** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results on CommonGen dev set. Results of NAR and AR are from Lin et al. (2020).

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline
**Methods** & **Pattern** & BLEU & **Steps** & **NFE (Steps\(\times k\))** \\ \hline Transformer (Vaswani et al., 2017) & AR & 34.74 & - & - \\ \hline CNAT (Bao et al., 2021) & NAR & 29.81 & - & - \\ \hline SeqDiffuSeq (Yuan et al., 2022) (\(k\) = 1) & \multirow{2}{*}{Diffusion} & 29.83 & 2,000 & 2,000 (2,000 \(\times\) 1) \\ AR-Diffusion (\(k\) = 1) & & 30.19 & 20 & 20 (20 \(\times\) 1) \\ \hline GENIE (Lin et al., 2023) (\(k\) = 50) & \multirow{2}{*}{Diffusion} & 30.08 & 20 & 1,000 (20 \(\times\) 50) \\ AR-Diffusion (\(k\) = 50) & & 34.95 & 20 & 1,000 (20 \(\times\) 50) \\ AR-Diffusion (\(k\) = 500) & & **35.62** & 20 & 10,000 (20 \(\times\) 500) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on IWSLT14 De\(\rightarrow\)En test set following the setting of SeqDiffuSeq. “NFE” indicates the Number of Function Evaluations (Ye et al., 2023).

\begin{table}
\begin{tabular}{l|c c|c c c|c} \hline \hline
**Methods** & **Pattern** & \multicolumn{2}{c|}{BLEU} & \multicolumn{1}{c}{**Steps**} & \multicolumn{1}{c}{**NFE (Steps\(\times k\))**} \\ \hline Transformer (Vaswani et al., 2017) & AR & 34.74 & - & - \\ \hline CNAT (Bao et al., 2021) & NAR & 29.81 & - & - \\ \hline SeqDiffuSeq (Yuan et al., 2022) (\(k\) = 1) & \multirow{2}{*}{Diffusion} & 29.83 & 2,000 & 2,000 (2,000 \(\times\) 1) \\ AR-Diffusion (\(k\) = 1) & & 30.19 & 20 & 20 (20 \(\times\) 1) \\ \hline GENIE (Lin et al., 2023) (\(k\) = 50) & \multirow{2}{*}{Diffusion} & 30.08 & 20 & 1,000 (20 \(\times\) 50) \\ AR-Diffusion (\(k\) = 50) & & 34.95 & 20 & 1,000 (20 \(\times\) 50) \\ AR-Diffusion (\(k\) = 500) & & **35.62** & 20 & 10,000 (20 \(\times\) 500) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on IWSLT14 De\(\rightarrow\)En test set following the setting of SeqDiffuSeq. “NFE” indicates the Number of Function Evaluations (Ye et al., 2023).

decrease of 1.34. Furthermore, with 3 steps, although the performance deterioration of GENIE is somewhat reduced, the average score still shows a decline of 2.81. In contrast, AR-Diffusion maintains a high performance level, with an average score differing from the 20-step result by only 0.64. Notably, the results of AR-Diffusion at 3 steps are comparable to the results of GENIE at 2,000 steps. Therefore, compared to GENIE, the inference speed of AR-Diffusion can be accelerated by up to \(600\times\).

### Analysis

#### Diversity of Samples

Diversity is a key advantage of diffusion models. To measure the diversity of generated samples, We adopt the SELF-BLEU (Zhu et al., 2018) metric, in which a lower score indicates higher diversity. In Lin et al. (2023), various sampling methods were applied to the pre-trained auto-regressive model BART13. As shown in Table 6, AR-Diffusion achieves significantly higher diversity compared to the auto-regressive model. Furthermore, the diversity can be comparable to GENIE with a better performance.

Footnote 13: Specific details are written in supplementary material G.

Ablation StudyTo demonstrate the effectiveness of our proposed method, we perform ablation experiments on the XSum dataset. Our results show that both our proposed multi-level diffusion and skipping mechanism are essential for achieving the high performance of AR-Diffusion.

Maintaining the skipping inference method, we remove the token-level diffusion during the training process, which degenerates to GENIE w/ skipping. The comparison results are shown in Figure 2(a). It can be observed that after removing, the AVG-ROUGE score is greatly lower after 2 steps.

The performance of applying our proposed skipping mechanism and DDIM (Song et al., 2021) to AR-Diffusion is shown in Figure 2(b). The results demonstrate that the skipping mechanism consistently outperforms DDIM in various inference steps. Additionally, the skipping mechanism can be easily applied to GENIE. As depicted in Figure 2(c), DDIM suffers a significant drop in performance when the number of inference steps is less than 40. In contrast, the skipping mechanism consistently maintains good performance across all inference steps.

\begin{table}
\begin{tabular}{c|c c c c c c|c c} \hline \hline \multicolumn{2}{c|}{**Methods**} & \multicolumn{4}{c|}{BART} & \multicolumn{2}{c|}{GENIE} & \multicolumn{1}{c}{AR-Diffusion} \\ \hline \multirow{2}{*}{**Sampling**} & Greedy & Beam & Diverse & Typical & Top-k & Nucleus & \multirow{2}{*}{Diffusion} \\  & Search & Search & Beam Search & Sample & Sample & Sample & & \\ \hline SELF-BLEU \(\downarrow\) & 100.0 & 93.4 & 75.6 & 76.9 & 80.2 & 79.1 & 29.3 & 30.4 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Diversity of **10** generated samples on XSum test set and average of **10** results. The results of BART and GENIE are quoted from Lin et al. (2023).

Figure 2: Ablation experiments on XSum test set and taking \(k\) = 5. The horizontal axis is the number of inference steps and the vertical axis is AVG-ROUGE = (ROUGE-1 + ROUGE-2 + ROUGE-L) / 3.

Case StudyBy mapping the state to the token with the highest logits, we visualize the intermediate states of AR-Diffusion. As depicted in Figure 3, AR-Diffusion undergoes a denoising process, transforming the random Gaussian noise into a coherent sentence over 20 steps, and we present 5 of them. With the progression of each timestep, compared to the tokens on the right side of the sentence, the tokens on the left side demonstrate faster determination and a rapid increase in the corresponding logits. This behavior is consistent with our principle of dynamic movement speed from left to right.

## 5 Related Work

AR and NAR Language ModelsAR models have been the dominant approach for text generation [12, 13, 14], but their token-by-token generation nature often leads to unsatisfactory inference speed. To address this issue, NAR models have been developed in recent years. The NAR method is initially proposed by Gu et al. [2017], its objective is generate the entire output sequence in parallel, thereby improving generation speed and efficiency. Subsequently, LevT [14] adopts insertion and deletion to address the lack of flexibility in NAR generation, CMLM [15] utilizes a masked language model to improve the quality of NAR generation through a constant number of iterations, and CNAT [1] introduces latent variables to represent the category information of the target word to make full use of the latent representation. However, these NAR methods are hard to model inter-token position dependency and deficient in generation performance.

Continuous Text DiffusionThe application of diffusion models to continuous text space is first introduced by Li et al. [2022]. Through the embedding and rounding processes, the direct integration of continuous noise into word embeddings was accomplished. After that, more people attempt to adopt continuous text diffusion model to solve sequence-to-sequence tasks. DiffuSeq [14] divides the input into two parts, utilizing one part as a condition, and perturbs the other part with noise. CDCD [11] proposes score interpolation and time warping to allow diffusion model and Euclidean embedding to share the same loss function for training. SeqDiffuSeq [23], GENIE [10] and DINOISER [24] incorporate diffusion model into the encoder-decoder structure through cross-attention mechanisms.

It is important to highlight the differences between our method and both ARDMs [12] and TimeGrad [21], despite the common references to autoregression and diffusion in all these. ARDMs employ an order-agnostic technique, leveraging masking and prediction for generation in arbitrary orders. On the other hand, TimeGrad integrates RNN and diffusion to model the conditional distribution of future steps of multivariate time series. In contrast, our research focuses on implementing the diffusion process within a continuous embedding space, with the primary aim of generating text in a left-to-right sequence.

## 6 Conclusion

This paper introduces AR-Diffusion, which exhibits AR-like generation behavior but enables efficient parallel decoding. Embracing the inherent sequential nature of language, we propose a multi

Figure 3: The intermediate state of AR-Diffusion gradually generating real text from a standard Gaussian noise through 20 steps. The brightness of the color represents the magnitude of the logits, with darker colors indicating larger logits. More cases are shown in the supplementary materials H.

level diffusion model, consisting of sentence-level and token-level components, to assign dynamic movement speeds to tokens. Consequently, compared to those on the right, the left tokens undergo fewer denoising steps and generate earlier to subsequently influence the later ones. Furthermore, we introduce a skipping mechanism to facilitate parallel generation within the multi-level diffusion framework. The experimental results across various tasks demonstrate that AR-Diffusion surpasses existing diffusion models in terms of quality while maintaining diversity. Additionally, compared to existing diffusion language models, AR-Diffusion achieves comparable results while being \(100\times\sim 600\times\) faster.

## 7 Acknowledgements

This research is supported by National Natural Science Foundation of China (Grant No.62276154), Research Center for Computer Network (Shenzhen) Ministry of Education, the Natural Science Foundation of Guangdong Province (Grant No. 2023A1515012914), Basic Research Fund of Shenzhen City (Grant No. JCYJ20210324120012033 and JSGG20210802154402007), the Major Key Project of PCL for Experiments and Applications (PCL2021A06), and Oversea Cooperation Research Fund of Tsinghua Shenzhen International Graduate School (HW2021008).

## References

* OpenAI (2023) OpenAI. GPT-4 technical report. _CoRR_, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _CoRR_, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.
* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.
* Li et al. (2022a) Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-LM improves controllable text generation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022a. URL https://openreview.net/forum?id=3s9IrEsjLyk.
* Gong et al. (2022) Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. Diffuseeq: Sequence to sequence text generation with diffusion models. _CoRR_, abs/2210.08933, 2022. doi: 10.48550/arXiv.2210.08933. URL https://doi.org/10.48550/arXiv.2210.08933.
* Liu et al. (2020)Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. _arXiv preprint arXiv:2211.15089_, 2022.
* Yuan et al. (2022) Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, and Songfang Huang. Seqdiffuseq: Text diffusion with encoder-decoder transformers, 2022.
* Ye et al. (2023) Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Mingxuan Wang. Dinoiser: Diffused conditional sequence learning by manipulating noises, 2023.
* Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, pages 7871-7880. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.703. URL https://doi.org/10.18653/v1/2020.acl-main.703.
* Qi et al. (2020) Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou. ProphetNet: Predicting future n-gram for sequence-to-SequencePre-training. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 2401-2410, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.217. URL https://aclanthology.org/2020.findings-emnlp.217.
* Qi et al. (2021) Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu Chen, Dayiheng Liu, Kewen Tang, Houqiang Li, Jiusheng Chen, Ruofei Zhang, et al. Bang: Bridging autoregressive and non-autoregressive generation with large scale pretraining. In _International Conference on Machine Learning_, pages 8630-8639. PMLR, 2021.
* Li et al. (2022) Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. ELMER: A non-autoregressive pre-trained language model for efficient and effective text generation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pages 1044-1058. Association for Computational Linguistics, 2022b. URL https://aclanthology.org/2022.emnlp-main.68.
* Li et al. (2022) Yafu Li, Leyang Cui, Yongjing Yin, and Yue Zhang. Multi-granularity optimization for non-autoregressive translation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pages 5073-5084. Association for Computational Linguistics, 2022c. URL https://aclanthology.org/2022.emnlp-main.339.
* Bao et al. (2021) Yu Bao, Shujian Huang, Tong Xiao, Dongqi Wang, Xinyu Dai, and Jiajun Chen. Non-autoregressive translation by learning target categorical codes. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5749-5759, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.458. URL https://aclanthology.org/2021.naacl-main.458.
* Lin et al. (2023) Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan, and Weizhu Chen. Text generation with diffusion language models: A pre-training approach with continuous paragraph denoise, 2023.
* Gu et al. (2017) Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. _arXiv preprint arXiv:1711.02281_, 2017.
* Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 1797-1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/afdec7005cc9f14302cd0474f0f3c96-Paper.pdf.
* Kumar and Byrne (2004) Shankar Kumar and William Byrne. Minimum bayes-risk decoding for statistical machine translation. Technical report, JOHNS HOPKINS UNIV BALTIMORE MD CENTER FOR LANGUAGE AND SPEECH PROCESSING (CLSP), 2004.
* Liu et al. (2021) Dayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang Zhang, Jian Jiao, Weizhu Chen, Jie Fu, Linjun Shou, Ming Gong, et al. Glge: A new general language generation evaluation benchmark. In _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 408-420, 2021.
* Lee et al. (2018) Jason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-autoregressive neural sequence modeling by iterative refinement. _arXiv preprint arXiv:1802.06901_, 2018.
* Ghazvininejad et al. (2019) Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 6112-6121, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1633. URL https://aclanthology.org/D19-1633.
* Gu et al. (2019) Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. In _Advances in Neural Information Processing Systems_, pages 11181-11191, 2019.
* Stern et al. (2019) Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. Insertion transformer: Flexible sequence generation via insertion operations. _arXiv preprint arXiv:1902.03249_, 2019.
* Greff et al. (2017) Klaus Greff, Rupesh Kumar Srivastava, Jan Koutnik, Bas R. Steunebrink, and Jurgen Schmidhuber. LSTM: A search space odyssey. _IEEE Trans. Neural Networks Learn. Syst._, 28(10):2222-2232, 2017.
* Lin et al. (2020) Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 1823-1840, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.165. URL https://aclanthology.org/2020.findings-emnlp.165.
* Gu et al. (2016) Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K. Li. Incorporating copying mechanism in sequence-to-sequence learning. In _ACL (1)_. The Association for Computer Linguistics, 2016.
* Susanto et al. (2020) Raymond Hendy Susanto, Shamil Chollampatt, and Liling Tan. Lexically constrained neural machine translation with levenshtein transformer. In _ACL_, pages 3536-3543. Association for Computational Linguistics, 2020.
* Zhu et al. (2018) Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pages 1097-1100, 2018.
* Song et al. (2021) Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL https://openreview.net/forum?id=St1giarCHLP.
* Dong et al. (2023) Chenhe Dong, Yinghui Li, Haifan Gong, Miaoxin Chen, Junxin Li, Ying Shen, and Min Yang. A survey of natural language generation. _ACM Comput. Surv._, 55(8):173:1-173:38, 2023. doi: 10.1145/3554727. URL https://doi.org/10.1145/3554727.
* Hoogeboom et al. (2022) Emiel Hoogeboom, Alexey A. Gritsenko, Jasmin Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=Lm8T39vLDTE.
* Hager et al. (2019)Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 8857-8868. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/rasul21a.html.
* Kudo and Richardson [2018] Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012.
* Luo [2022] Calvin Luo. Understanding diffusion models: A unified perspective. _arXiv preprint arXiv:2208.11970_, 2022.
* Xiao et al. [2022] Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-Yan Liu. A survey on non-autoregressive generation for neural machine translation and beyond. _CoRR_, abs/2204.09269, 2022.
* Vijayakumar et al. [2016] Ashwin K. Vijayakumar, Michael Cogswell, Ramprasaath R. Selvaraju, Qing Sun, Stefan Lee, David J. Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. _CoRR_, abs/1610.02424, 2016.
* Meister et al. [2022] Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Typical decoding for natural language generation. _CoRR_, abs/2202.00666, 2022.
* Fan et al. [2018] Angela Fan, Mike Lewis, and Yann N. Dauphin. Hierarchical neural story generation. In _ACL (1)_, pages 889-898. Association for Computational Linguistics, 2018.
* Holtzman et al. [2020] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In _ICLR_. OpenReview.net, 2020.

Limitation

A primary limitation of our work lies in the requirement of generating a large number of candidate samples for optimal performance. As an illustration in Table 2 of Cnn/DailyMail dataset, AR-Diffusion (\(k\) = 50) achieves a 0.8 lower ROUGE-2 score compared to AR-Diffusion (\(k\) = 500). We anticipate exploring more efficient sampling strategies to minimize the number of generated samples without performance drop.

## Appendix B Experimental Details

Model SetupOur model configuration is implemented based on Transformer-base (Vaswani et al., 2017). In particular, For XSum and Cnn/DailyMail, we set the diffusion embedding dimension to 128. For IWSLT14, we use 64-dimensional diffusion embedding, 4 attention heads and 1024-dimensional feed-forward layers. For CommonGen, we adopt 64-dimensional diffusion embedding, 8 attention heads and 512-dimensional feed-forward layers.

Training and InferenceIn the training phase, we employ a square-root noise schedule and 2,000 diffusion steps (Li et al., 2022). Specially, we use the tokenizer and vocabulary constructed by Byte Pair Encoding (BPE)14(Kudo and Richardson, 2018) for translation tasks. For other tasks, we adopt the tokenizer and vocabulary of bert-base-uncased.

Footnote 14: We train bpe on the training set, and follow the vocabulary size of fairseq, IWSLT14 is set to 10,000.

BaselinesWe set four groups of baselines:

\(\bullet\) NAR: NAT (Gu et al., 2017), iNAT (Lee et al., 2018), CMLM (Ghazvininejad et al., 2019), LevT (Gu et al., 2019) and CNAT (Bao et al., 2021);

\(\bullet\) Semi-NAR: InsT (Stern et al., 2019), iNAT (Lee et al., 2018), CMLM (Ghazvininejad et al., 2019) and LevT (Gu et al., 2019);

\(\bullet\) AR: bRNN (Gu et al., 2016), LSTM (Greff et al., 2017) and Transformer (Vaswani et al., 2017);

\(\bullet\) Diffusion: DiffusionLM (Li et al., 2022), CDCD (Dieleman et al., 2022), SeqDiffuSeq (Yuan et al., 2022), DINOISER (Ye et al., 2023) and GENIE (Lin et al., 2023).

MetricsWe follow the approach of Qi et al. (2020)15 to evaluate the **ROUGE-1/2/L** of the summarization task. For the evaluation of translation tasks, we adopt the setting of SeqDiffuSeq (Yuan et al., 2022) to report BLEU score. In addition, we also calculate the SacreBLEU score according to the setting of DINOISER (Ye et al., 2023) for comparison. For CommonGen, we employ ROUGE-2/L, BLEU-3/4, METEOR and SPICE under the evaluation methods of Lin et al. (2020)16.

Footnote 15: https://github.com/microsoft/ProphetNet/tree/master/GLGE_baselines

Footnote 16: https://github.com/INK-USC/CommonGen/tree/master/evaluation/Traditional/eval_metrics

Training ParametersOur training parameters on different datasets are shown in Table 7. Our linear schedule warm up steps is 4,000 \(\times N_{gc}\), where \(N_{gc}\) denotes gradient accumulation number. In addition, we use the AdamW (weight decay = 0.0) optimizer and dropout is 0.2. All experiments are implemented on 8 Tesla V100-32G. It takes about 20 hours to train XSum and Cnn/DailyMail, about 5 hours to train IWSLT14, and about 2 hours to train CommenGen.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Dataset & Lr \& Schedule & Batch Size & Optimized Steps & Target Length \\ \hline XSum & 8e-4 \& Cosine & 128\(\times\)3\(\times\)8 & 80,000 / 3 & 50 \\ Cnn/DailyMail & 8e-4 \& Cosine & 80\(\times\)5\(\times\)8 & 100,000 / 5 & 180 \\ IWSLT14 De\(\rightarrow\)En & 2e-3 \& Cosine & 192\(\times\)2\(\times\)8 & 160,000 / 2 & 90 \\ IWSLT14 En\(\rightarrow\)De & 1.8e-3 \& Cosine & 768\(\times\)1\(\times\)8 & 60,000 & 90 \\ CommonGen & 3e-4 \& Constant & 384\(\times\)1\(\times\)8 & 40,000 & 54 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Training Parameter Settings. Batch Size = mini batch size \(\times\)\(N_{gc}\)\(\times\) GPU number, Optimized Steps = total steps / \(N_{gc}\), and \(N_{gc}\) is gradient accumulation number.

More Results on IWSLT14

In Table 8 we give the SacreBLEU score according to the setting of DINOISER. AR-Diffusion has notable improvements over non-auto-regressive CMLM. Besides, AR-Diffusion achieves excellent performance among text diffusion models for both En\(\rightarrow\)De and De\(\rightarrow\)En tasks. Specifically, AR-Diffusion is far superior to GENIE and comparable to the newly proposed DINOISER at \(n\) = 50. Nevertheless, the performance is stronger than DINOISER when \(k\) = 50017.

Footnote 17: DINOISER has shown in their Figure 4 that their method is not better with a larger \(k\).

## Appendix D Comparison of Runtime Speed

We also compare the runtime speeds of GENIE and AR-Diffusion by measuring the time taken for each sample generation, denoted as seconds per iteration (s/it). To ensure precision in our measurements, we sample 50 cases from the CNN/DailyMail dataset. The generation process speed comprises two main components: function evaluation (FE, which is equivalent to the model forward computation) and MBR. All experiments are conducted on an A100-40G GPU and 50 CPUs.

The results are presented in Table 9. It is evident that for \(k\leq 50\), the MBR duration is minimal and can be negligible relative to NFE. Furthermore, even when \(k\) = 500 and step = 20, AR-Diffusion remains approximately twice as efficient as GENIE (\(k\) = 10 and step = 2000).

## Appendix E Impact of Minimum Bayes Risk and Anchor Point

Minimum Bayes RiskTo investigate the relationship between the number of generated candidate samples (\(k\)) and the quality of generation, we generate varying numbers of samples, ranging up to 1,000, on the IWSLT14 De\(\rightarrow\)En test set and present the results in Figure 4. The curve demonstrates an initial gain of approximately 0.5 SacreBLEU within the first 200 samples, after which the gain becomes insignificant with generating more samples.

Anchor PointWe conduct experiments on AR-Diffusion using different anchor points \((n_{e},t_{e})\). These anchor points vary in terms of \(n_{e}\) values, namely \(1.0\times N\), \(2.0\times N\) and \(3.0\times N\), where \(N\) denotes the target sentence length. Additionally, they share a common \(t_{e}\) value of \(T\), which represents

\begin{table}
\begin{tabular}{l|c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{2}{c}{IWSLT14} \\  & De\(\rightarrow\)En & En\(\rightarrow\)De \\ \hline Transformer (AR, beam = 5) [23] & 33.61 & 28.30 \\ \hline CMLM (NAR, \(k\) = 5) [18] & 29.41 & 24.34 \\ \hline DiffusionLM (\(k\) = 50) [11] & 29.11 & 22.91 \\ DINOISER (\(k\) = 50) [24] & 31.61 & 26.14 \\ \hline GENIE (\(k\) = 50) [11] & 29.45 & 23.89 \\ AR-Diffusion (\(k\) = 50) & 31.80 & 26.01 \\ AR-Diffusion (\(k\) = 500) & **32.35** & **26.51** \\ \hline \hline \end{tabular}
\end{table}
Table 8: SacreBLEU on the IWSLT14 test set. This result follows the setting of DiNoiSer.

\begin{table}
\begin{tabular}{l|c|c c c c} \hline \hline  & **GENIE** & \multicolumn{4}{c}{**AR-Diffusion**} \\ \hline \(k\) & 10 & 10 & 10 & 50 & 500 \\ Steps & 2000 & 3 & 20 & 20 & 20 \\ \hline Speed of NFE (s/it) & 47.54 & 0.25 & 0.61 & 2.12 & 21.03 \\ Speed of MBR (s/it) & 0.02 & 0.02 & 0.02 & 0.08 & 7.14 \\ Total Speed (s/it) & **47.56** & **0.27** & **0.63** & **2.20** & **28.17** \\ \hline ROUGE-2 in XSum & 8.78 & 8.68 & 9.32 & 10.1 & 10.6 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Comparison of inference speed between GENIE and AR-Diffusion.

the total time step of diffusion. We present the results in Table 10, and determine that the best result is achieved at \((n_{e},t_{e})\) = \((2.0\times N,T)\).

## Appendix F Proof of Inference with Skipping

During the inference process, skipping strategy requires the model \(\bm{g_{\theta}}\) to infer the state \(\bm{z}_{t_{i+1}}^{n_{2}}\) at a far-off timestep \(t_{i+1}\) compared to the current state \(\bm{z}_{t_{i}}^{n_{2}}\), where \(t_{i+1}\ll t_{i}\). In our model, due to the dynamic speed setting, token \(\bm{z}_{t_{i+1}}^{n_{1}}\) with smaller timestep \(t_{i+1}\leq t_{i}\), which is closer to \(t_{i+1}\), and positions \(n_{1}\leq n_{2}\) can provide stronger auxiliary information than \(\bm{z}_{t_{i}}^{n_{1}}\). This reduces the difficulty of inferring states for tokens in the end, making our multi-level diffusion model particularly suitable for accelerating the generation process.

Through maximizing the evidence lower bound (ELBO) of \(p(\bm{z}_{0})\), the training object is equivalent to minimize the divergence between \(q(\bm{z}_{t}|\bm{z}_{t-1},\bm{z}_{0})\) and \(p_{\theta}(\bm{z}_{t-1}|\bm{z}_{t})\) following [10].

By converting the joint probability distribution into a conditional probability distribution, we obtain the following formula for \(q(\bm{z}_{t_{i+1}}|\bm{z}_{t_{i}},\bm{z}_{0})\).

\[\begin{split} q(\bm{z}_{t_{i+1}}|\bm{z}_{t_{i}},\bm{z}_{0})& =q(\bm{z}_{t_{i+1}}|\bm{z}_{t_{i}-1},\bm{z}_{t_{i}},\bm{z}_{0})\;q (\bm{z}_{t_{i+1}-1}|\bm{z}_{t_{i}},\bm{z}_{0})\\ &=q(\bm{z}_{t_{i+1}}|\bm{z}_{t_{i}-1},\bm{z}_{0})\;q(\bm{z}_{t_{i+ 1}-1}|\bm{z}_{t_{i}},\bm{z}_{0})\\ &=q(\bm{z}_{t_{i+1}}|\bm{z}_{t_{i}-2},\bm{z}_{0})\;q(\bm{z}_{t_{i +1}-2}|\bm{z}_{t_{i}-1},\bm{z}_{0})\;q(\bm{z}_{t_{i+1}-1}|\bm{z}_{t_{i}},\bm{z} _{0})\\ &=\prod_{k=1}^{t_{i}-t_{i+1}}q(\bm{z}_{t_{i}-k}|\bm{z}_{t_{i}-k+1 },\bm{z}_{0})\end{split}\] (13)

Similarly, we reach the same conclusion regarding \(p_{\theta}(\bm{z}_{t_{i+1}}|\bm{z}_{t_{i}})\).

Based on equation (13), which consists of \(q(\bm{z}_{t}|\bm{z}_{t-1},\bm{z}_{0})\), and the interchangeability between \(q(\bm{z}_{t}|\bm{z}_{t-1},\bm{z}_{0})\) and \(p_{\theta}(\bm{z}_{t-1}|\bm{z}_{t})\), we can decompose \(q(\bm{z}_{t_{i+1}}|\bm{z}_{t_{i}},\bm{z}_{0})\) by incorporating \(\bm{z}_{t_{i}}\) and \(\bm{z}_{0}\), and utilize our estimated \(\bm{z}_{0}\) to determine the expression of \(p_{\theta}(\bm{z}_{t_{i+1}}|\bm{z}_{t_{i}})\).

\[q(\bm{z}_{t_{i+1}}\mid\bm{z}_{t_{i}},\bm{z}_{0})=\prod_{n=1}^{N}q \big{(}z_{f(n,t_{i+1})}^{n}\mid z_{f(n,t_{i})}^{n},z_{0}^{n}\big{)}\] (14)

\begin{table}
\begin{tabular}{c c|c} \hline \hline \(n_{e}\) & \(t_{e}\) & SacreBLEU \\ \hline \(1.0\times N\) & \(T\) & 31.23 \\ \(2.0\times N\) & \(T\) & **31.80** \\ \(3.0\times N\) & \(T\) & 31.58 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Effect of anchor point at different positions on the IWSLT14 De\(\rightarrow\)En test set. “\(N\)” indicates the target sequence length and “T” represents the total time step of diffusion.

Figure 4: Relationship between the number of candidate samples for applying MBR and SacreBLEU on IWSLT14 De\(\rightarrow\)En test set.

Next, we obtain the explicit expression \(q\big{(}z_{f(n,t_{i+1})}^{n}\mid z_{f(n,t_{i})}^{n},z_{0}^{n}\big{)}\) through linear interpolation between \(z_{f(n,t_{i})}^{n}\) and \(z_{0}^{n}\).

\[q\big{(}z_{f(n,t_{i+1})}^{n}\mid z_{f(n,t_{i})}^{n},z_{0}^{n}\big{)} =\frac{q(z_{f(n,t_{i})}^{n}\mid z_{f(n,t_{i+1})}^{n}z_{0}^{n})q(z_{f(n,t_{i+1}) }^{n}\mid z_{0}^{n})}{q(z_{f(n,t_{i})}^{n}\mid z_{0}^{n})}\] \[= \frac{\mathcal{N}\Big{(}z_{f(n,t_{i})}^{n};\sqrt{\frac{\bar{ \alpha}_{f(n,t_{i})}}{\bar{\alpha}_{f(n,t_{i+1})}}}z_{f(n,t_{i+1})}^{n},\big{(} 1-\frac{\bar{\alpha}_{f(n,t_{i})}}{\bar{\alpha}_{f(n,t_{i+1})}}\big{)}I\Big{)} \mathcal{N}\Big{(}z_{f(n,t_{i+1})}^{n};\sqrt{\bar{\alpha}_{f(n,t_{i+1})}}z_{0 }^{n},(1-\bar{\alpha}_{f(n,t_{i+1})})I\Big{)}}{\mathcal{N}\Big{(}z_{f(n,t_{i} )}^{n};\sqrt{\bar{\alpha}_{f(n,t_{i})}}z_{0}^{n},(1-\bar{\alpha}_{t_{i}})I \Big{)}}\] \[\propto \exp\Big{\{}-\frac{\big{(}z_{f(n,t_{i})}^{n}-\sqrt{\frac{\bar{ \alpha}_{f(n,t_{i})}}{\bar{\alpha}_{f(n,t_{i+1})}}}z_{f(n,t_{i+1})}^{n}\big{)} ^{2}}{2(1-\frac{\bar{\alpha}_{f(n,t_{i})}}{\bar{\alpha}_{f(n,t_{i+1})}})}- \frac{\big{(}z_{f(n,t_{i+1})}^{n}-\sqrt{\bar{\alpha}_{t_{i+1}}}z_{0}^{n}\big{)} ^{2}}{1-\bar{\alpha}_{f(n,t_{i+1})}}\] \[\qquad+\frac{\big{(}z_{f(n,t_{i})}^{n}-\sqrt{\bar{ \alpha}_{f(n,t_{i})}}z_{0}^{n}\big{)}^{2}}{1-\bar{\alpha}_{f(n,t_{i})}}\Big{\}}\] \[= \exp\Big{\{}-\frac{1-\bar{\alpha}_{t_{i}}}{2(1-\frac{\bar{\alpha }_{t_{i}}}{\bar{\alpha}_{i_{i+1}}})(1-\bar{\alpha}_{i_{i+1}})}\Big{[}z_{f(n,t_ {i+1})}^{n}-2\Big{(}\frac{\sqrt{\frac{\bar{\alpha}_{f(n,t_{i})}}{\bar{\alpha} _{f(n,t_{i+1})}}}(1-\bar{\alpha}_{f(n,t_{i+1})})}{1-\bar{\alpha}_{f(n,t_{i})}} z_{f(n,t_{i})}^{n}\] \[\qquad+\frac{\sqrt{\bar{\alpha}_{f(n,t_{i+1})}}(1-\bar{\alpha}_{f (n,t_{i+1})})}{1-\bar{\alpha}_{f(n,t_{i})}}z_{0}^{n}\Big{)}z_{f(n,t_{i+1})}^{ n}\Big{]}\Big{\}}\] \[\propto \mathcal{N}\Big{(}z_{f(n,t_{i+1})}^{n};\frac{\sqrt{\frac{\bar{ \alpha}_{f(n,t_{i})}}{\bar{\alpha}_{f(n,t_{i+1})}}}(1-\bar{\alpha}_{f(n,t_{i+1} )})}{1-\bar{\alpha}_{f(n,t_{i})}}z_{f(n,t_{i})}^{n}+\frac{\sqrt{\bar{\alpha}_{ f(n,t_{i+1})}}(1-\frac{\bar{\alpha}_{f(n,t_{i+1})}}{\bar{\alpha}_{f(n,t_{i+1})}})}{1- \bar{\alpha}_{f(n,t_{i})}}z_{0}^{n},\] \[\qquad\frac{(1-\frac{\bar{\alpha}_{t_{i}}}{\bar{\alpha}_{t_{i+1}} })(1-\bar{\alpha}_{t_{i+1}})}{1-\bar{\alpha}_{t_{i}}}I\Big{)}\] \[= \mathcal{N}(z_{f(n,t_{i+1})}^{n};\lambda z_{f(n,t_{i})}^{n}+\mu z _{0}^{n},\sigma I)\] (15)

where we have the following notations for simplification.

\[\lambda=\frac{\sqrt{\frac{\bar{\alpha}_{f(n,t_{i})}}{\bar{\alpha}_{f(n,t_{i+1} )}}}(1-\bar{\alpha}_{f(n,t_{i+1})})}{1-\bar{\alpha}_{f(n,t_{i})}},\;\mu=\frac{ \sqrt{\bar{\alpha}_{f(n,t_{i+1})}}(1-\frac{\bar{\alpha}_{f(n,t_{i})}}{\bar{ \alpha}_{f(n,t_{i+1})}})}{1-\bar{\alpha}_{f(n,t_{i})}},\;\sigma=\frac

#ft hmm $ northern hacker support by yells lion on [unused698] tennis bars named! ##s \(\supset\) 1898 168240 limp a british soldier who was killed by an army in 43 lifted losers requested ##ged a prosecutor of verbal of rogers $. a british soldier who was killed by an army in intersect has been named upon a prosecutor of deep offset $.' process. a british soldier who was killed by an army in ash has been named by the ministry of crucial of championships. a british soldier who was killed by an army in afghanistan has been named by the ministry of defence of moddra. proxy oxidation ##h ##r o 1 ##rilonaate ##rwinpuri saskatoon amplitude $.1702 ##r the $.#rvalstrake ##r; barcelona a plaque for spain's first ##falis war amid been be in the historic fleet of serumnik say, the ##r of 2003 dso g fee.##r.p a plaque for spain's first world war has been added in the century city of liverpool say, the certain of britain in g fee. a plaque for spain's first world war has been be in the historic city of liverpool say, the amazing of spain in g post. a plaque for spain's first world war has been unveiled in the historic city of liverpool, the first of britain in the years. ##r.apelica6 breach ##nostic [unused557] fires sale why organizational interception originates <<Sinial ##pa knots china's prime minister creators it concerning the " 800 <<Sin # beach '1728 withdrawal [unused308] ## attending china's prime minister says it is the "ity emissions " in the country's [unused697] foundation, the engine of ". china's prime minister says it is the " emissions " in the country's tq crisis, the engine of ". # [unused887]v china's prime minister says it is the " emissions " in the country's economic crisis, the engine of parliament.