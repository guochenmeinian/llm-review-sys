# Learning Large-Scale MTP\({}_{2}\) Gaussian Graphical Models via Bridge-Block Decomposition

Xiwen Wang\({}^{1}\), Jiaxi Ying\({}^{1,2}\), Daniel P. Palomar\({}^{1}\)

The Hong Kong University of Science and Technology\({}^{1}\)

HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute\({}^{2}\)

{xwangew, jx.ying}@connect.ust.hk, palomar@ust.hk

Corresponding author.

###### Abstract

This paper studies the problem of learning the large-scale Gaussian graphical models that are multivariate totally positive of order two (MTP\({}_{2}\)). By introducing the concept of bridge, which commonly exists in large-scale sparse graphs, we show that the entire problem can be equivalently optimized through (1) several smaller-scaled sub-problems induced by a _bridge-block decomposition_ on the thresholded sample covariance graph and (2) a set of explicit solutions on entries corresponding to _bridges_. From practical aspect, this simple and provable discipline can be applied to break down a large problem into small tractable ones, leading to enormous reduction on the computational complexity and substantial improvements for all existing algorithms. The synthetic and real-world experiments demonstrate that our proposed method presents a significant speed-up compared to the state-of-the-art benchmarks.

## 1 Introduction

In recent decades, the surge in data availability has posed challenges for analyzing and understanding large-scale datasets. A key challenge is examining pairwise relationships among numerous variables, which can be represented using Gaussian graphical models (GGMs) [1] that depict variable connections as graphs. The precision matrix, which is the inverse of the covariance matrix, helps determine the non-zero patterns of GGMs [2; 3]. A traditional approach to estimate the precision matrix is the graphical lasso [4; 5], which is formulated as a regularized log-determinant program. The solutions of graphical lasso possess an appealing property known as sparsity, which severs as a common assumption particularly in large graphical models and has been shown to offer numerous benefits by a significant amount of research. For example, the sparsity can reduce the model size and improve the interpretability of the model by highlighting the most important variables and their relationships, allowing better understanding the underlying causal structure [6; 7; 8].

In this paper, we solve the large-scale sparse precision matrix estimation problem for GGMs that follow a multivariate totally positive of order two (MTP\({}_{2}\)) Gaussian distribution [9; 10], or equivalently, possess a precision matrix that is a symmetric \(M\)-matrix [11], where all off-diagonal elements are non-positive. The so-called MTP\({}_{2}\) property is a special form of positive dependence and plays an essential role in applications where all interaction potentials are considered non-negative or the focus is on emphasizing positive associations between variables [12; 13; 14; 15]. The MTP\({}_{2}\) property has been applied in various fields. Here are some examples. In finance, MTP\({}_{2}\) structures are often exploited as the asset returns are often considered positively correlated [16; 17; 18]; In machine learning, MTP\({}_{2}\) GGMs are recognized as attractive Markov random field and have been used in applications such as taxonomic reasoning [19] and psychometrics [13]. The MTP\({}_{2}\) precision matrix, also referred toas the generalized graph Laplacian [20; 21; 22], along with the combinatorial graph Laplacian [23; 24; 25; 26; 27], have found broad applications across a variety of fields due to their unique characteristics. These applications include, but are not limited to, graph Fourier transform [28], electrical circuits analysis [29], image and video coding [30], financial time-series analysis [31; 32], and structured graph learning [33; 34].

The objective of this paper is to build a theoretical foundation and devise effective approaches for learning large-scale, sparse \(\text{MTP}_{2}\) GGMs. The contributions of this paper are threefold.

* This is the first work in the literature that introduces the notion of bridges in the context of learning \(\text{MTP}_{2}\) GGMs to the best of our knowledge. Building upon this notion, we prove that the presence of explicit solutions for some entries depends on whether an edge functions as a bridge. Meanwhile, we demonstrate that the optimal solution exhibits a decomposed structure through a vertex-partition known as bridge-block decomposition.
* Based on theoretical findings, we propose an efficient framework that decomposes a large problem into several small tractable ones, each of which can be solved using any existing algorithm. With some negligible extra cost for bridge-block decomposition, the proposed method results in a dimension reduction that significantly cuts down the computational complexity.
* The synthetic and real-world experiments demonstrate that our proposed methods prompt a considerable speed-up for all existing methods and enable the solving of large-scale \(\text{MTP}_{2}\) GGMs that were previously considered infeasible.

### Notation and Organization

Vectors and matrices are written as lower and upper case bold letters, respectively. An undirected graph is denoted as \(G=(\mathcal{V},\mathcal{E})\), where \(\mathcal{V}\) is the set of nodes with size \(\left|\mathcal{V}\right|=p\) and \(\mathcal{E}\subset\mathcal{V}\times\mathcal{V}\) is the set of edges. Note that we shall not distinguish between \((i,j)\in G\) and \((i,j)\in\mathcal{E}\). Some graph terminology frequently used throughout the paper are introduced as follows.

* **Support graph** Given a symmetric matrix \(\bm{A}\in\mathbb{S}^{p}\), the support graph of \(\bm{A}\), denoted as \(\text{supp}\left(\bm{A}\right)\), is defined as an undirected graph with the vertex set \(\mathcal{V}=\left\{1,\dots,p\right\}\) and the edge set \(\mathcal{E}\in\mathcal{V}\times\mathcal{V}\) such that \((i,j)\in\mathcal{E}\) if and only if \(A_{ij}\neq 0\) for every two different vertices \(i,j\in\mathcal{V}\).
* **Neighbors** The neighbors of a node \(i\) refer to the subset of vertices that are connected to this node, i.e., \(\mathcal{N}\left(i\right)=\left\{j\left|\left(i,j\right)\in\mathcal{E}\right.\right\}\).
* **Path and Cycle** A path from node \(i_{1}\in\mathcal{V}\) to node \(i_{T}\in\mathcal{V}\) is a sequence (i.e., an ordered set) of edges denoted as \(d_{i_{1},i_{T}}\), each one incident to the next, i.e., \(\left\{(i_{t},i_{t+1})\right\}_{t=1}^{T-1}\subseteq\mathcal{E}\). A cycle is a path from a node to itself when it does not duplicately include the same edge.
* **Partition** A partition \(\mathcal{P}=\left\{\mathcal{V}_{1},\dots,\mathcal{V}_{K}\right\}\) is a collection of non-empty, disjoint vertex sets \(\mathcal{V}_{1},\dots,\mathcal{V}_{K}\) called cluster such that \(\bigcup_{i=1}^{K}\mathcal{V}_{k}=\mathcal{V}\).
* **Component** A component of \(G\) refers to a subset of vertices that are connected to each other by edges, but are not connected to any other vertices in the graph.

The paper is organized as follows. In Section 2, we introduce the problem formulation and related works. In Section 3, we present our main result along with its potential impact. Section 4 presents the numerical experiments to show that the proposed methods can facilitate the performance of existing algorithms and solve large-scale \(\text{MTP}_{2}\) GGMs problems 2. In Section 5 we draw the conclusions.

Footnote 2: Codes are available in https://github.com/Xiwen1997/mtp2-bbd.

## 2 Problem Formulation and Related Works

### Problem Formulation

Let \(\bm{y}\) be a random vector that follows a Gaussian distribution with zero mean and a precision matrix \(\bm{\Theta}\), a.k.a. the inverse of covariance matrix \(\bm{\Sigma}\). A Gaussian graphical model represents the conditional dependency between random variables via a graph \(G\), in which nodes correspond to \(\bm{y}\) and edges between these variables represent conditional dependencies, which can be equivalently characterized by the non-zero patterns of the precision matrix \(\bm{\Theta}\)[2].

This paper considers estimating the precision matrix \(\bm{\Theta}\) given \(n\) independent and identically distributed observations \(\{\bm{y}_{1},\ldots,\bm{y}_{n}\}\) that follow an \(\text{MTP}_{2}\) Gaussian distribution. Equivalently, \(\bm{\Theta}\) is assumed to be a symmetric \(M\)-matrix, i.e., \(\Theta_{ij}\leq 0\) for any \(i\neq j\). The problem is formulated as

\[\underset{\bm{\Theta}\in\mathcal{M}^{p}}{\text{minimize}} -\log\det\left(\bm{\Theta}\right)+\left\langle\bm{\Theta},\bm{S} \right\rangle+\sum_{i\neq j}\Lambda_{ij}\left|\Theta_{ij}\right|,\] (1)

where \(\bm{S}\) is the sample covariance matrix, \(\Lambda_{ij}\geq 0\) are the regularization coefficients, the objective is to minimize the negative log-likelihood of the data subject to a weighted \(\ell_{1}\)-norm penalty on the precision matrix, and \(\mathcal{M}^{p}\) refers to a set of \(M\)-matrices with dimension \(p\), i.e.,

\[\mathcal{M}^{p}=\left\{\bm{\Theta}\in\mathbb{S}^{p}\left|\bm{\Theta}\succ \bm{0}\text{ and }\Theta_{ij}\leq 0,\forall i\neq j\right.\right\}.\] (2)

We refer to \(\bm{\Theta}^{\star}\) as the optimal solution of (1) and to the support graph of \(\bm{\Theta}^{\star}\), i.e., \(\text{supp}\left(\bm{\Theta}^{\star}\right)\), as the _optimal graph_. Particularly, we define the _thresholded sample covariance matrix_\(\bm{T}\) as

\[T_{ij}=\begin{cases}S_{ij}-\Lambda_{ij}&\text{if }i\neq j\text{ and }S_{ij}>\Lambda_{ij},\\ 0&\text{otherwise},\end{cases}\] (3)

and the support graph of \(\bm{T}\) as the _thresholded sample covariance graph_ (short for _thresholded graph_). Clearly, entries corresponding to \(S_{ij}\leq\lambda_{ij}\) will be thresholded to zero. Since the thresholded graph holds immense importance throughout the paper, unless explicitly mentioned otherwise, we denote \(G=\text{supp}\left(\bm{T}\right)\).

The involvement of \(\text{MTP}_{2}\) constraints, which require that \(\Theta_{ij}\leq 0\) for all \(i\neq j\), confers several advantages [11]. For example, all \(M\)-matrices are inverse-positive, i.e., \(\bm{\Theta}^{-1}\geq\bm{0}\), and the non-smoothness from \(\ell_{1}\) norms can be eliminated via \(\sum_{i\neq j}\Lambda_{ij}|\Theta_{ij}|=-\langle\bm{\Lambda},\bm{\Theta}\rangle,\) where \(\bm{\Lambda}=(\Lambda_{ij})\) and \(\text{diag}\left(\bm{\Lambda}\right)=\bm{0}\). Meanwhile, the \(\text{MTP}_{2}\) constraints maintain the sparsity in the estimated precision matrix as an implicit regularizer [19]. Throughout the paper, we require the following assumption.

**Assumption 2.1**.: For any \(i\neq j\), we have \(S_{ij}<\sqrt{S_{ii}S_{jj}}\).

Assumption 2.1 holds with probability 1 if the number of observations \(n\geq 2\)[13; 19]. Under that assumption, the minimizer of Problem (1) exists and is unique according to [19, Theorem 1].

### Related Works

In literature, devising algorithms for learning \(\text{MTP}_{2}\) GGMs has garnered considerable attentions. For instance, block coordinate descent (BCD) methods [19; 26; 35] update a single row/column of the precision matrix in a cyclic manner by solving non-negative least squares problems. Projection-based methods, such as projected gradient descent [36] and projected quasi-Newton algorithms [37], iteratively take steps along the descent direction and then project the solutions back to the feasible region. Despite these efforts, existing research has not fully explored the potential of exploiting \(\text{MTP}_{2}\) properties to reduce computational complexity. With complexities of \(\mathcal{O}(p^{4})\) for BCD methods and \(\mathcal{O}(p^{3})\) for projection-based techniques, addressing large-scale problems continues to be challenging, particularly when manipulating full-dimensional matrices without problem reduction.

Instead of devising efficient algorithms, recently, leveraging the properties of the thresholded sample covariance graph has emerged as a popular approach for learning GGMs. Specifically, the existence of closed-form solution has been established for graphical lasso when the thresholded sample covariance graph is acyclic (i.e., contains no cycles) [38; 39]. However, the applicability of their main results is limited by certain conditions that are difficult to verify. Interestingly, those conditions are unnecessary for the existence of closed-from solutions in \(\text{MTP}_{2}\) GGMs [20]. Despite the theoretical establishment on closed-form solutions, an exact acyclic structure is rarely observed in practice. Therefore, more research dives into the relationship between \(\text{supp}\left(\bm{T}\right)\) and \(\text{supp}\left(\bm{\Theta}^{\star}\right)\). Research found that \(\text{supp}\left(\bm{\Theta}^{\star}\right)\) is a subset of \(\text{supp}\left(\bm{T}\right)\)[13], the number of connected components in \(\text{supp}\left(\bm{T}\right)\) is identical to that in \(\text{supp}\left(\bm{\Theta}^{\star}\right)\)[20], and there exist necessary conditions for the presence of edges in \(\text{supp}\left(\bm{\Theta}^{\star}\right)\) via analyzing the path products on thresholded matrix [13].

This paper advances prior research in two ways. Firstly, unlike previous studies that only provided an explicit solution for \(\Theta_{ij}\) in the case of acyclic thresholded graphs, we reveal that this explicit solution for \(\Theta_{ij}\) consistently applies to all \((i,j)\) pairs acting as bridges, regardless of whether the thresholded graph is acyclic or non-acyclic. Secondly, we highlight that the optimal graph can be represented in an equivalent decomposed form through a vertex partition, termed as the bridge-block decomposition of the thresholded graph by leveraging \(\text{MTP}_{2}\) properties.

## 3 Proposed Methods

The goal of this paper is not about developing an algorithm but to shed light on the remarkable properties concealed in the thresholded graph. This section is organized as follows. In Section 3.1, we introduce bridge and bridge-block decomposition. Section 3.2 presents our main result. Then in Section 3.3, we elaborate on the contributions and connections to existing research.

### Bridge and Bridge-Block Decomposition

Bridge is one of the important concepts in graph theory. Technically, an edge is called a _bridge_ if and only if its deletion increases the number of graph components. Therefore, an edge is a bridge only when it is not contained in any cycles [40]. Notably, when a graph consists solely of bridges, it is referred to as _acyclic_. In this paper, we denote \(\mathcal{B}\) as the set of all bridges, i.e,

\[\mathcal{B}:=\left\{\left.\left(i,j\right)\right|\left(i,j\right)\text{is a bridge in }G\right\}.\] (4)

_Remark 3.1_.: Bridges are frequently observed, particularly in large-scale sparse graphs [41; 42; 43]. This is attributed to the fact that in sparse graphs, only the most significant relationships between variables are retained, and the removal of edges can create additional connected components, giving rise to the presence of bridges.

_Remark 3.2_.: In practice, the set \(\mathcal{B}\) can be efficiently obtained via various bridge-finding algorithms [40; 44]. These algorithms employ a depth-first search approach, resulting in a computational complexity of \(\mathcal{O}\left(\left|\mathcal{V}\right|+\left|\mathcal{E}\right|\right)\). In the sparse graphs that are of interest to us, the number of edges \(\left|\mathcal{E}\right|\) typically scales similarly to the number of nodes \(\left|\mathcal{V}\right|\). As a result, the computational cost of identifying bridges in large-scale sparse graphs remains low.

Using Figure 1 as an illustration, we perform the _bridge-block decomposition_[45] as follows. By removing all the bridges, we compute the clusters \(\mathcal{V}_{k}\) corresponding to the components of \(G=\left(\mathcal{V},\mathcal{E}\setminus\mathcal{B}\right)\). This process results in a vertex-partition, known as the bridge-block decomposition:

\[\mathcal{P}^{\text{bbd}}=\left\{\mathcal{V}_{1},\mathcal{V}_{2},\ldots, \mathcal{V}_{K}\right\},\] (5)

where \(K\) refers to the number of clusters, also the number of components in the graph \(\left(\mathcal{V},\mathcal{E}\setminus\mathcal{B}\right)\).

Without loss of generality, we define the operator \(\psi\) as \(\psi\left(i\right)=k\) if node \(i\) belongs to the \(k\)-th cluster. In the literature, the bridge-block decomposition is also known as the _2-edge-connected decomposition_[46; 47], a fundamental concept in graph theory with numerous applications such as community search [48], social network mining [49], and transmission networks [50].

In practice, the cost of obtaining the bridge-block decomposition \(\mathcal{P}^{\text{bbd}}\), which involves calculating the thresholded graph, bridges, and clusters, is negligible. With the aforementioned preliminary knowledge, we formally introduce our main result as follows.

### Main Results

Considering a square matrix \(\bm{A}\in\mathbb{S}^{p}\), we define \(\bm{A}_{\mathcal{V}_{k}}\in\mathbb{S}^{p_{k}}\) as the principal sub-matrix of \(\bm{A}\) keeping the rows and columns indexed by \(\mathcal{V}_{k}\), in which \(p_{k}=\left|\mathcal{V}_{k}\right|\) is the number of nodes in \(k\)-th cluster and

Figure 1: An illustration of the bridge-block decomposition. Edges \(\left(5,6\right)\) and \(\left(9,10\right)\) are identified as bridges since removing either of them increases the number of connected components. After removing the bridge edges, the resulting decomposition \(\mathcal{P}^{\text{bbd}}=\left\{\left\{1,2,3,4,5\right\},\left\{7,8,9,10\right\}, \left\{11,12,13,14,15,16\right\}\right\}\).

we have \(p=\sum_{k}p_{k}\). For each \(i\in\mathcal{V}_{k}\), we mark \(\pi\left(i\right)\) as its corresponding index in \(\mathcal{V}_{k}\). Hence, we define \(\widehat{\bm{\Theta}}_{k}\) as the optimal solution of \(k\)-th sub-problem, i.e.,

\[\widehat{\bm{\Theta}}_{k}=\arg\min_{\bm{\Theta}_{k}\in\mathcal{M}^{p_{k}}}- \log\det\left(\bm{\Theta}_{k}\right)+\left\langle\bm{\Theta}_{k},\bm{S}_{ \mathcal{V}_{k}}-\bm{\Lambda}_{\mathcal{V}_{k}}\right\rangle.\] (6)

The main result is then given as follows.

**Theorem 3.3**.: _Under Assumption 2.1, given the bridge-block decomposition of the thresholded graph \(\text{supp}\left(\bm{T}\right)\) as \(\mathcal{P}^{\text{bbd}}\), and the optimal solution of each sub-problem (46) as \(\widehat{\bm{\Theta}}_{k}\), the optimal solution of Problem (1), i.e., \(\bm{\Theta^{*}}\), can be obtained as_

\[\bm{\Theta}_{i,j}^{*}=\begin{cases}[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(i) }+\zeta_{i}&\text{if }i=j\in\mathcal{V}_{k},\\ \left[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}&\text{if }i\neq j\text{ and }i,j\in \mathcal{V}_{k},\\ -T_{ij}/(S_{ii}S_{jj}-T_{ij}^{2})&\text{if }(i,j)\in\mathcal{B},\\ 0&\text{otherwise}.\end{cases}\] (7)

_in which \(\zeta_{i}=\frac{1}{S_{ii}}\sum_{(i,m)\in\mathcal{B}}\frac{T_{im}^{2}}{S_{ii}S_ {mm}-T_{im}^{2}}\) and \(\zeta_{i}=0\) if \(\forall m:(i,m)\notin\mathcal{B}\)._

Figure 2 depicts an example of how to apply Theorem 3.3 to obtain the optimal solution more efficiently. Theoretically, the key to show the optimality of (7) is via an explicit expression of the inverse of \(\bm{\Theta}\), i.e., \(\bm{R}=\bm{\Theta}^{-1}\) using following theorem.

**Theorem 3.4**.: _Given \(\bm{S},\bm{T}\in\mathbb{S}^{p}\), the bridge-block decomposition \(\mathcal{P}^{\text{bbd}}=\{\mathcal{V}_{1},\ldots,\mathcal{V}_{K}\}\) of \(\text{supp}\left(\bm{T}\right)\), and a set of matrices \(\{\widehat{\bm{\Theta}}_{k}\in\mathbb{S}_{++}^{|\mathcal{V}_{k}|}\}_{k=1}^{K}\), the inverse of \(\bm{\Theta}\) in the form of (7) is derived as_

\[R_{ij}=\begin{cases}[\widehat{\bm{R}}_{k}]_{\pi(i),\pi(j)}&\text{if }i,j\in\mathcal{V}_{k},\\ T_{ij}&\text{if }(i,j)\in\mathcal{B},\\ \sqrt{S_{ii}S_{jj}}\cdot g_{ij}\left(\bm{R}\right)&\text{otherwise},\end{cases}\] (8)

_where \(\widehat{\bm{R}}_{k}=[\widehat{\bm{\Theta}}_{k}]^{-1}\) and for each \(i,\,j\) in different clusters, \(g_{ij}\) is given as_

\[g_{ij}\left(\bm{R}\right)=\prod_{t=0}^{2T}R_{u_{t},u_{t+1}}\big{/}\sqrt{S_{u_ {t},u_{t}}S_{u_{t+1},u_{t+1}}},\ \ \text{where }u_{0}\overset{\Delta}{=}i,\ u_{2T+1}\overset{ \Delta}{=}j,\] (9)

_in which \(T-1\) is the number of bridges in \(d_{ij}\), \(g_{ij}=0\) if \(d_{ij}=\emptyset\), and \(u\) denotes a sequence of incident' bridges, i.e., \(d_{ij}\cap\mathcal{B}=\{\left(u_{1},u_{2}\right),\ldots,\left(u_{2T-1},u_{2T} \right)\}\) following the orders of \(d_{ij}\) while preserving the elements that are bridges._

We note that all terms in (9) can be computed via (8) and Theorem 3.4 itself generally holds for arbitrary \(\bm{S},\bm{T}\), and \(\{\widehat{\bm{\Theta}}_{k}\}_{k=1}^{K}\). Detailed proofs and discussions are deferred to Appendix A.

Theorem 3.4 is theoretically critical as it provides an explicit form of the inverse of \(\bm{\Theta}\), which was previously difficult to compute. It is vital for demonstrating the optimality of \(\bm{\Theta}\) by connecting the KKT system of the original large-scale problem to the KKT systems of smaller sub-problems through the bridge-block decomposition. Consequently, together with \(\text{MTP}_{2}\) properties, we show that \(\bm{\Theta}\) in the form of (7) satisfies the KKT condition of Problem (1) in Appendix B.

**The role of \(\text{MTP}_{2}\) constraints:** Although Theorem 3.4 can be applied without requiring \(\text{MTP}_{2}\) properties, it is important to highlight that these properties serve as sufficient conditions to demonstrate the optimality of \(\bm{\Theta}=\bm{R}^{-1}\). By incorporating the MTP\({}_{2}\) constraints, the non-smoothness in graphical lasso is eliminated, resulting in simplified optimality conditions as shown below:

\[\forall i: -R_{ii}+S_{ii}=0 -R_{ii}+S_{ii}=0\] (10a) \[\forall\Theta_{ij}\neq 0: -R_{ij}+S_{ij}+\lambda_{ij}\cdot\text{sign}(\Theta_{ij})=0 \implies -R_{ij}+S_{ij}-\lambda_{ij}=0\] (10b) \[\forall\Theta_{ij}=0: |-R_{ij}+S_{ij}|\leq\lambda_{ij} -R_{ij}+S_{ij}-\lambda_{ij}\leq 0\] (10c)

Simultaneously, \(\bm{R}\) becomes non-negative (\(\geq\bm{0}\)). As a result, The most challenging part of the KKT conditions \(-R_{ij}+S_{ij}-\lambda_{ij}\leq 0\) holds under MTP\({}_{2}\) properties. In Appendix C, we will elaborate the details and explore additional conditions under which we can extend the applicability of Theorem 3.3 to the traditional graphical lasso.

**Proposed solving framework:** In practical applications, it is often more efficient to employ the bridge-block decomposition technique instead of directly manipulating full-sized matrices. This approach involves breaking down the problem into smaller isolated sub-problems. By addressing these sub-problems individually, we can compute the optimal solution by utilizing the solutions obtained from each sub-problem, as outlined in Theorem 3.3. This approach offers several advantages:

* **Significant reduction in computational cost**. Foremost, the cost of bridge-block decomposition is cheap. Suppose that we use a BCD solver of complexity \(\mathcal{O}\left(p^{4}\right)\), then the total cost is reduced to \(\sum_{k=1}^{K}\mathcal{O}(|\mathcal{V}_{k}|^{4})\ll\mathcal{O}(p^{4})\), where \(\sum_{k}|\mathcal{V}_{k}|=p\). This can prompt an enormous difference.
* **Considerable reduction in memory cost**. The memory cost is typically troublesome for large-scale problems as each full-dimensional matrix contains \(p^{2}\) elements. Theorem 3.3 can avoid generating a number of full-dimensional intermediate variables during computation.
* **Potential speed-up via parallel computing**. The sub-problems can be optimized independently, which allows parallel computing for significant speed-up.

### Connection to Existing Research

From Theorem 3.3, we obtain a very interesting result that the \((i,j)\)-th entries of \(\bm{\Theta}\) admits an explicit solution, i.e., \(\Theta_{ij}=-T_{ij}/\left(S_{ii}S_{jj}-T_{ij}^{2}\right)\) if edge \((i,j)\) is a bridge in the thresholded graph \(\text{supp}\left(\bm{T}\right)\). To the best of our knowledge, this result, shown in Corollary 3.5, is the first sufficient condition for an edge belonging to optimal graph \(\text{supp}\left(\bm{\Theta}^{\star}\right)\).

**Corollary 3.5**.: _If edge \((i,j)\) is a bridge in the thresholded graph \(\text{supp}\left(\bm{T}\right)\), then \((i,j)\) is also a bridge in the optimal graph \(\text{supp}\left(\bm{\Theta}^{\star}\right)\)._

Corollary 3.5 can be obtained as follows. By the definition of a bridge, from nodes \(i\) to \(j\) the path \(d_{ij}=\{(i,j)\}\) is the only path in the thresholded graph, and removing it would result in an increase in the number of components. Given that \(\text{supp}(\bm{\Theta}^{\star})\subseteq\text{supp}(\bm{T})\) and \(\Theta_{ij}\neq 0\), it follows that \(d_{ij}=\{(i,j)\}\) remains the unique path from nodes \(i\) to \(j\) in the optimal graph. This indicates that the edge \((i,j)\) continues to function as a bridge.

In the literature, only some necessary conditions for edges in the thresholded graph \(\text{supp}\left(\bm{T}\right)\) to be retained in the optimal graph \(\text{supp}\left(\bm{\Theta}^{\star}\right)\) have been established. For instance, [13] and [20] indicate that \(\text{supp}\left(\bm{\Theta}^{\star}\right)\subseteq\text{supp}\left(\bm{T}\right)\), which implies that \((i,j)\notin\text{supp}\left(\bm{\Theta}^{\star}\right)\) if \((i,j)\notin\text{supp}\left(\bm{T}\right)\).

Another explicit solution is also applicable when \(k\)-th cluster only contains one node, i.e., \(\mathcal{V}_{k}=\{i\}\), then \(\widehat{\bm{\Theta}}_{k}=1/S_{ii}\). As a consequence, Theorem 3.3 generalizes to the following corollary for acyclic graph which admits a bridge-block decomposition into \(p\) clusters.

**Corollary 3.6**.: _Suppose that the thresholded graph \(\text{supp}\left(\bm{T}\right)\) is acyclic, then the optimal graph \(\text{supp}\left(\bm{\Theta}^{\star}\right)\) is also acyclic and \(\bm{\Theta}^{\star}\) admits a closed-form solution as_

\[\Theta_{ij}^{\star}=\begin{cases}\frac{1}{S_{ii}}\left(1+\sum_{m\in\mathcal{N} (i)}\frac{T_{im}^{2}}{S_{ii}S_{mm}-T_{im}^{2}}\right)&\text{if }i=j,\\ -\frac{T_{ij}}{S_{ii}S_{jj}-T_{ij}^{2}}&\text{if }(i,j)\in\mathcal{B},\\ 0&\text{otherwise}.\end{cases}\] (11)

The proof directly follows Theorem 3.3. Corollary 3.6 coincides with [20, Theorem 3] and [38]. Compared to our results in Theorem 3.3, explicit solutions in literature heavily depend on the graph structure. Hence, our theory appears as the first to reveal that, fundamentally, it is the edge property, i.e., whether the edge is a bridge, that decides the existence of explicit solutions.

In conclusion, our proposed methods generalize the existing results with more profound understandings, offering significant potential for solving large-scale precision matrix estimation problems.

## 4 Numerical Simulations

We conduct experiments on synthetic and real-world data to evaluate how the proposed method accelerates the convergence of existing algorithms compared to algorithms that do not exploit it. All experiments were conducted on 2.60GHZ Xeon Gold 6348 machines and Linux OS. All methods are implemented in MATLAB and the state-of-the-art methods we consider includes

* **BCD**: Block Coordinate Descent [19] of complexity \(\mathcal{O}(p^{4})\).
* **PGD**: Projected Gradient Descent [36] of complexity \(\mathcal{O}(p^{3})\).
* **PQN-LBFGS**: A projected quasi-Newton method with limited memory BFGS [51]. The complexity is \(\mathcal{O}((m+p)p^{2})\), where \(m\) is the number of iterations stored for approximating the Hessian.
* **FPN**: Fast projected Newton-like method [37] of complexity \(\mathcal{O}(p^{3})\).

Note that all methods converge to the optimal solution. The target here is not to compare these methods but to evaluate how our proposed framework promotes the efficiency of these methods.

### Synthetic Data Experiments

We use the processes described in [19] to generate the data. We begin with an underlying graph that has an adjacency matrix \(\bm{A}\in\mathbb{S}^{p}\), and define \(\bm{\Theta}=\delta\bm{I}-\bm{A}\), where \(\delta=1.05\cdot\lambda_{\max}\left(\bm{A}\right)\) and \(\lambda_{\max}\left(\bm{A}\right)\) represents the largest eigenvalue of \(\bm{A}\). his ensures that \(\bm{\Theta}\) is a positive definite matrix with off-diagonal elements being negative, making \(\bm{\Theta}\) a randomly generated \(M\)-matrix. Next, we normalize \(\bm{\Theta}\) by substituting it with \(\bm{D}\bm{\Theta}\bm{D}\), where \(\bm{D}\) is a suitably chosen diagonal matrix, ensuring that \(\text{diag}(\bm{\Theta}^{-1})=\bm{1}\). We then sample \(n=10p\) data points from a Gaussian distribution \(\mathcal{N}(\bm{0},\bm{\Theta}^{-1})\) and calculate the sample covariance matrix as \(\bm{S}\).

Following [25], we construct the regularization matrix \(\bm{\Lambda}\). Briefly, given an initial estimate \(\bm{\Theta}^{(0)}\), we set \(\Lambda_{ij}=\chi\big{/}(\Theta_{ij}^{(0)}+\epsilon)\) when \(i\neq j\) and \(\Lambda_{ij}=0\) when \(i=j\). Here, \(\chi>0\) determines the sparsity level and \(\varepsilon\) is a small positive constant, such as \(10^{-3}\). Generally, a large penalty is expected on \(\Theta_{ij}\) when \(\Theta_{ij}^{(0)}\) is small. We efficiently compute \(\bm{\Theta}^{(0)}\) using the following equation:

\[\Theta_{ij}^{(0)}=-T_{ij}\backslash\left(S_{ii}S_{jj}-T_{ij}^{2}\right),\quad \forall i\neq j,\] (12)

and then appropriately adjust the value of \(\chi\) so that \(\text{supp}\left(\bm{T}\right)\) has only one connected component, while \(\text{supp}\left(\bm{\Theta}^{\star}\right)\) can roughly recover the underlying structure.

We consider two scale-free random graphs as underlying with their typical structures in Figure 4.

**1. Barabasi-Albert (BA)** graph [52; 53] of order one. BA models generate random scale-free networks via preferential attachment, suitable for modeling various networks like the Internet, world wide web, protein interactions, citations, and social/online networks.

**2. Stochastic Block Model (SBM)** of networks [54], a.k.a. the community graph [55]. Stochastic block models serve as fundamental tools in network science, creating random networks based on community structures, where nodes within the same group are more likely to form connections.

In the experiments, we consider \(p=5000\) and \(p=20000\) and calculate the relative errors as follows:

\[\text{RE}\left(\bm{\Theta}\right)=\left|f\left(\bm{\Theta}\right)-f\left(\bm {\Theta}^{\star}\right)\right|/\left|f\left(\bm{\Theta}^{\star}\right)\right|,\] (13)

where we compare the relative errors at each iteration against the computational time. Here, \(\bm{\Theta}^{\star}\) represents the optimal solution, and \(f\) signifies the objective function of (1). For methods employing bridge-block decomposition (bbd), we first calculate \(\widehat{\bm{\Theta}}_{k}^{(s)}\) for each cluster at the \(s\)-th iteration, followed by calculating the intermediate solution \(\bm{\Theta}^{(s)}\) using (7). The computational time at the \(s\)-th iteration is then the sum of the costs to obtain \(\mathcal{P}^{\text{bbd}}\), calculate all \(\widehat{\bm{\Theta}}_{k}^{(s)}\), and compute \(\bm{\Theta}^{(s)}\) via (7).

The results for \(p=5000\) and \(p=20000\) are displayed in Figure 3, with all results averaged over five realizations. We highlight that the extra computational cost compared to methods without acceleration is negligible as shown in Table 1. From the figures, we observe that:

* **Significant Speed-up:** Our proposed framework substantially accelerates state-of-the-art methods by one to four orders of magnitude.
* **Solving Otherwise Infeasible Problems:** When \(p=20000\), existing methods cannot optimize the problem within \(10^{4}\) seconds. In contrast, our framework greatly speeds up convergence, making it possible to solve large-scale problems.
* **Higher Speed-up for Sparser Graph:** Proposed method achieves greater acceleration on the BA graph than the SBM graph, as the former exhibits a stronger sparsity pattern.
* **Higher Speed-up for Methods of Higher-complexity:** Figure 3 demonstrates that our method has a more significant impact on improving the efficiency of the BCD method, which has higher computational complexity and thus benefits more from dimension reduction.

To further shed light on the factors that determine the magnitudes of improvement, in the next sub-section, we conduct additional experiments that dive into how bridge-block decomposition boosts the performance of existing methods subject to different structures of the thresholded graph.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{2}{c}{Conducting Bridge-block decomposition} & \multicolumn{2}{c}{Computing \(\mathbf{\Theta}\) from \(\{\widehat{\mathbf{\Theta}}_{k}\}_{k=1}^{K}\)} \\ \hline  & \(p=5k\) & \(p=20k\) & \(p=5k\) & \(p=20k\) \\ \hline BA & \(0.2\) & \(2.2\) & \(0.0\) & \(0.0\) \\ SBM & \(0.3\) & \(2.3\) & \(0.1\) & \(0.2\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Average computational time (seconds) of extra cost.

Figure 3: Relative errors of the objective values versus the computational time. Colors are used for distinguishing the state-of-the-art methods. Solid circle symbols stand for methods accelerated by bridge-block decomposition (bbd) and square symbols stand for methods without acceleration.

### Further Experiments on Ratios of Improvement

We investigate a scenario where the underlying graph is a community graph with a block-tridiagonal adjacency matrix, and each cluster contains an equal number of nodes (\(p_{1}=|\mathcal{V}_{k}|,\forall k\)). Without loss of generality, we assume that the internal edges within clusters form a cycle. To generate \(\bm{S}\) and \(\bm{\Lambda}\), we follow the procedures in Section 4.1. We then modify \(\bm{\Lambda}\) to \(\alpha\left(\bm{1}\bm{1}^{T}-\bm{A}\right)+\left(1-\alpha\right)\bm{\Lambda}\), ensuring that \(\text{supp}\left(\bm{T}\right)=\text{supp}\left(\bm{A}\right)\), where \(\alpha\in[0,1]\) and \(\bm{A}\) is the adjacency matrix of the considered graph.

We experiment with various values of \(K\) (the number of clusters) and \(p_{1}\) (the number of nodes in each cluster). For each configuration, we calculate the ratio of improvement, defined as the quotient of the time required to achieve \(\text{RE}\left(\bm{\Theta}\right)<10^{-6}\) without using bridge-block decomposition to the time when applying the decomposition. We conduct multiple trials of the BCD method and display the results in Figure 5. The results in Figure 5 suggest that the number of sub-graphs \(K\) is a primary factor influencing the ratio values. The improvement is deemed tremendous, especially for large-scale settings. The involvement of bridge-block decomposition enables solving an MTP\({}_{2}\) GGMs with hundreds of millions of variables as long as we can conduce bridge-block decomposition. Consequently, many previously unattainable real-life applications can now be optimized.

### Real-word Data Experiment

We consider learning the MTP\({}_{2}\) GGM for the Crop Image dataset available from the UCR Time Series Archive [56]. This dataset comprises \(p=24000\) pixels, where each pixel corresponds to a time series record capturing spectral information in \(n=46\) instances. These instances represent geometrically and radiometrically corrected satellite images. By examining this dataset, we can observe the temporal changes in the observed areas through the recorded measurements. To facilitate our analysis, the data has been pre-processed by [57] using an indexing technique. This preprocessing step enables us to work with compressed informative indexes instead of handling large image files.

Our goal is to perform graph-based clustering to the time series data that contains 46 observations, denoted as \(\{\bm{y}_{1},\dots,\bm{y}_{46}\}\) using MTP\({}_{2}\) GGMs, where \(\bm{y}_{i}\in\mathbb{R}^{24000}\). The data includes \(24\) classes corresponding to different indexed land covers, such as corn, barley, or lake. It is important to note that these labels are not involved in the clustering process but help evaluate the quality of the final graph-based clusters by computing the graph modularity [58]. Via analyzing the data, in Appendix D, we discuss that why learning MTP\({}_{2}\) GGMs is statistically meaningful in the context of clustering and show that the MTP\({}_{2}\) assumptions approximately hold for this dataset.

The initial estimate \(\bm{\Theta}^{(0)}\) is computed using the method proposed in [59]. The regularization matrix \(\bm{\Lambda}\) is determined using the approach in Section 4.1 with \(\epsilon=0.01\) and \(\chi=0.2\). We then compute the sample correlation matrix as \(\bm{S}\in\mathbb{S}^{24000}\). This results in a precision matrix estimation problem involving \(5.76\times 10^{8}\) variables to optimize, which is quite challenging for state-of-the-art methods without bridge-block decomposition.

Figure 5(b) shows the results of empirical convergence, and Figure 5(a) visualizes a local structure of \(\text{supp}\left(\bm{\Theta}\right)^{*}\) (modularity = \(0.6849\)). In line with previous findings, our suggested approach greatly speeds up the convergence of all current algorithms by a minimum of three orders of magnitude. This demonstrates immense potential for managing large-scale sparse graphical models.

While our paper's primary focus is not to uncover insights into the estimated \(\text{MTP}_{2}\) GGMs for a deeper understanding of the underlying data, interesting phenomena can still be observed in the structures presented in Figure 5(a). Notably, we find that the majority of edges exist within the same crop type, while connections between nodes associated with different crops are relatively sparse. This observation is valuable for clustering processes and aligns with our expectations, as stronger positive dependencies are often observed within the same class, while dependencies between different classes tend to be weaker.

Moreover, our graphical representation reveals more nuanced patterns. For instance, in Figure 5(a) of our manuscript, we observe a dense network of edges between two crop types, 'temporary meadow' and 'pasture', indicating a significantly stronger conditional dependency compared to other categories. This observation further supports our understanding. Hence, the inferred conditional dependency structure encapsulates the inherent interrelations among different crops. As a result, our proposed framework based on bridge-block decomposition effectively facilitates learning in large-scale \(\text{MTP}_{2}\) graphical models.

## 5 Conclusions and Discussions

Real-world sparse Gaussian graphical models often comprise subsets of variables that are densely connected to one another, while variables in different clusters maintain weak connections. However, standard estimation algorithms do not account for this property. In this paper, by introducing the concept of bridge, we leverage these characteristics to reveal an interesting finding: the optimal solution for Problem (1) equivalently admits a decomposed form via the bridge-block decomposition of the thresholded graph. We provide theoretical insights into the separability of optimal solutions and the existence of explicit expressions based on edge properties, specifically whether an edge is a bridge. This method surpasses conventional approaches that depend on unique graph structures, such as determining if a graph is acyclic. From the practical aspect, our proposed method offers a handy architect for accelerating any existing algorithms by handling a large-scale learning problem via a number of small and tractable sub-problems. Although our method is mainly developed for sparse large-scale graphs and therefore seems inapplicable for dense graph, as elaborated in the appendix C, it can also provide a quick way to obtain a solution, which can serve as either a starting point (warm-start) for numerical algorithms or an alternative for acquiring approximate solutions. Overall, our simple and provable approach demonstrates exceptional performance and paves a novel way for more extensive designs of large-scale \(\text{MTP}_{2}\) Gaussian graphical models.

Figure 6: Visualizations and experimental results of sparse \(\text{MTP}_{2}\) GGMs for Crop data set. (a) A local structure of optimal graph with \(2008\) nodes. Nodes with matching labels are assigned the same color and connected by a matching edge color, while different groups of nodes are connected by gray edges. (b) Results of convergence to learn sparse \(\text{MTP}_{2}\) GGMs for Crop data set.

Acknowledgements

This work was supported by the Hong Kong Research Grants Council GRF 16207820, 16310620, and 16306821, the Hong Kong Innovation and Technology Fund (ITF) MHP/009/20, and the Project of Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone under Grant HZQB-KCZYB-2020083. We would also like to thank the anonymous reviewers for their valuable feedback on the manuscript.

## References

* [1] H. Rue and L. Held, "Gaussian Markov random fields: theory and applications," _Monographs on Statistics and Applied Probability_, vol. 104, 2005.
* [2] S. L. Lauritzen, _Graphical models_. Clarendon Press, 1996, vol. 17.
* [3] J. Whittaker, _Graphical models in applied multivariate statistics_. Wiley Publishing, 2009.
* [4] O. Banerjee, L. El Ghaoui, and A. d'Aspremont, "Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data," _The Journal of Machine Learning Research_, vol. 9, pp. 485-516, 2008.
* [5] J. Friedman, T. Hastie, and R. Tibshirani, "Sparse inverse covariance estimation with the graphical lasso," _Biostatistics_, vol. 9, no. 3, pp. 432-441, 2008.
* [6] M. Yuan and Y. Lin, "Model selection and estimation in the Gaussian graphical model," _Biometrika_, vol. 94, no. 1, pp. 19-35, 2007.
* [7] H. Liu, K. Roeder, and L. Wasserman, "Stability approach to regularization selection for high dimensional graphical models," _Advances in Neural Information Processing Systems_, vol. 23, 2010.
* [8] P. Danaher, P. Wang, and D. M. Witten, "The joint graphical lasso for inverse covariance estimation across multiple classes," _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, vol. 76, no. 2, pp. 373-397, 2014.
* [9] E. Bolviken, "Probability inequalities for the multivariate normal with non-negative partial correlations," _Scandinavian Journal of Statistics_, pp. 49-58, 1982.
* [10] D. M. Malioutov, J. K. Johnson, and A. S. Willsky, "Walk-sums and belief propagation in Gaussian graphical models," _The Journal of Machine Learning Research_, vol. 7, pp. 2031-2064, 2006.
* [11] R. J. Plemmons, "M-matrix characterizations. I-nonsingular M-matrices," _Linear Algebra and its Applications_, vol. 18, no. 2, pp. 175-188, 1977.
* [12] S. Fallat, S. Lauritzen, K. Sadeghi, C. Uhler, N. Wermuth, and P. Zwiernik, "Total positivity in Markov structures," _The Annals of Statistics_, pp. 1152-1184, 2017.
* [13] S. Lauritzen, C. Uhler, and P. Zwiernik, "Maximum likelihood estimation in Gaussian models under total positivity," _The Annals of Statistics_, vol. 47, no. 4, pp. 1835-1863, 2019.
* [14] F. Rottger, S. Engelke, and P. Zwiernik, "Total positivity in multivariate extremes," _The Annals of Statistics_, vol. 51, no. 3, pp. 962-1004, 2023.
* [15] D. Rossell and P. Zwiernik, "Dependence in elliptical partial correlation graphs," _Electronic Journal of Statistics_, vol. 15, no. 2, pp. 4236-4263, 2021.
* [16] A. Muller and M. Scarsini, "Archimedean copulae and positive dependence," _Journal of Multivariate Analysis_, vol. 93, no. 2, pp. 434-445, 2005.
* [17] R. Agrawal, U. Roy, and C. Uhler, "Covariance matrix estimation under total positivity for portfolio selection," _Journal of Financial Econometrics_, vol. 20, no. 2, pp. 367-389, 2022.
* [18] R. Zhou, J. Ying, and D. P. Palomar, "Covariance matrix estimation under low-rank factor model with nonnegative correlations," _IEEE Transactions on Signal Processing_, vol. 70, pp. 4020-4030, 2022.
* [19] M. Slawski and M. Hein, "Estimation of positive definite M-matrices and structure learning for attractive Gaussian markov random fields," _Linear Algebra and its Applications_, vol. 473, pp. 145-179, 2015.

* [20] E. Pavez, H. E. Egilmez, and A. Ortega, "Learning graphs with monotone topology properties and multiple connected components," _IEEE Transactions on Signal Processing_, vol. 66, no. 9, pp. 2399-2413, 2018.
* [21] J. Ying, J. V. d. M. Cardoso, and D. P. Palomar, "A fast algorithm for graph learning under attractive Gaussian Markov random fields," in _2021 55th Asilomar Conference on Signals, Systems, and Computers_, 2021, pp. 1520-1524.
* [22] Z. Deng and A. M.-C. So, "A fast proximal point algorithm for generalized graph Laplacian learning," in _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE, 2020, pp. 5425-5429.
* [23] J. Ying, J. V. d. M. Cardoso, and D. P. Palomar, "Does the \(\ell_{1}\)-norm learn a sparse graph under Laplacian constrained graphical models?" _arXiv preprint arXiv:2006.14925_, 2020.
* [24] ----, "Nonconvex sparse graph learning under Laplacian constrained graphical model," in _Advances in Neural Information Processing Systems_, vol. 33, 2020, pp. 7101-7113.
* [25] ----, "Minimax estimation of Laplacian constrained precision matrices," in _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, vol. 130, 2021, pp. 3736-3744.
* [26] H. E. Egilmez, E. Pavez, and A. Ortega, "Graph learning from data under Laplacian and structural constraints," _IEEE Journal of Selected Topics in Signal Processing_, vol. 11, no. 6, pp. 825-841, 2017.
* [27] J. Ying, X. Han, R. Zhou, X. Wang, and H. C. So, "Network topology inference with sparsity and Laplacian constraints," _arXiv preprint arXiv:2309.00960_, 2023.
* [28] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains," _IEEE signal processing magazine_, vol. 30, no. 3, pp. 83-98, 2013.
* [29] F. Dorfler and F. Bullo, "Kron reduction of graphs with applications to electrical networks," _IEEE Transactions on Circuits and Systems I: Regular Papers_, vol. 60, no. 1, pp. 150-163, 2012.
* [30] H. E. Egilmez, Y.-H. Chao, A. Ortega, B. Lee, and S. Yea, "GBST: Separable transforms based on line graphs for predictive video coding," in _2016 IEEE International Conference on Image Processing (ICIP)_. IEEE, 2016, pp. 2375-2379.
* [31] J. V. D. M. Cardoso, J. Ying, and D. P. Palomar, "Graphical models in heavy-tailed markets," in _Advances in Neural Information Processing Systems_, vol. 34, 2021, pp. 19 989-20 001.
* [32] ----, "Learning bipartite graphs: Heavy tails and multiple components," in _Advances in Neural Information Processing Systems_, vol. 35, 2022, pp. 14 044-14 057.
* [33] S. Kumar, J. Ying, J. V. d. M. Cardoso, and D. P. Palomar, "A unified framework for structured graph learning via spectral constraints," _Journal of Machine Learning Research_, vol. 21, no. 22, pp. 1-60, 2020.
* [34] ----, "Structured graph learning via Laplacian spectral constraints," in _Advances in Neural Information Processing Systems_, vol. 32, 2019, pp. 11 647-11 658.
* [35] E. Pavez and A. Ortega, "Generalized Laplacian precision matrix estimation for graph signal processing," in _2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE, 2016, pp. 6350-6354.
* [36] J. Ying, J. V. D. M. Cardoso, and D. P. Palomar, "Adaptive estimation of graphical models under total positivity," in _International Conference on Machine Learning_, 2023, pp. 40 054-40 074.
* [37] J.-F. Cai, J. V. d. M. Cardoso, D. P. Palomar, and J. Ying, "Fast projected Newton-like method for precision matrix estimation under total positivity," _arXiv preprint arXiv:2112.01939_, 2021.
* [38] S. Sojoudi, "Equivalence of graphical lasso and thresholding for sparse graphs," _The Journal of Machine Learning Research_, vol. 17, no. 1, pp. 3943-3963, 2016.
* [39] S. Fattahi and S. Sojoudi, "Graphical lasso and thresholding: Equivalence and closed-form solutions," _Journal of Machine Learning Research_, 2019.
* [40] J. M. Schmidt, "A simple test on 2-vertex-and 2-edge-connectivity," _Information Processing Letters_, vol. 113, no. 7, pp. 241-244, 2013.

* [41] S. Hojsgaard and S. L. Lauritzen, "Graphical Gaussian models with edge and vertex symmetries," _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, vol. 70, no. 5, pp. 1005-1027, 2008.
* [42] M. L. Stein, "Space-time covariance functions," _Journal of the American Statistical Association_, vol. 100, no. 469, pp. 310-321, 2005.
* [43] C. Li and H. Li, "Network-constrained regularization and variable selection for analysis of genomic data," _Bioinformatics_, vol. 24, no. 9, pp. 1175-1182, 2008.
* [44] R. E. Tarjan, "A note on finding the bridges of a graph," _Information Processing Letters_, vol. 2, no. 6, pp. 160-161, 1974.
* [45] J. Westbrook and R. E. Tarjan, "Maintaining bridge-connected and biconnected components on-line," _Algorithmica_, vol. 7, no. 1, pp. 433-464, 1992.
* [46] L. Chang, J. X. Yu, L. Qin, X. Lin, C. Liu, and W. Liang, "Efficiently computing k-edge connected components via graph decomposition," in _Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data_, 2013, pp. 205-216.
* [47] R. Zhou, C. Liu, J. X. Yu, W. Liang, B. Chen, and J. Li, "Finding maximal k-edge-connected subgraphs from a large graph," in _Proceedings of the 15th International Conference on Extending Database Technology_, 2012, pp. 480-491.
* [48] Y. Fang, X. Huang, L. Qin, Y. Zhang, W. Zhang, R. Cheng, and X. Lin, "A survey of community search over big graphs," _The VLDB Journal_, vol. 29, no. 1, pp. 353-392, 2020.
* [49] R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu, "Mining newsgroups using networks arising from social behavior," in _Proceedings of the 12th International Conference on World Wide Web_, 2003, pp. 529-535.
* [50] L. Lan and A. Zocca, "Refining bridge-block decompositions through two-stage and recursive tree partitioning," _arXiv preprint arXiv:2110.06998_, 2021.
* [51] D. Kim, S. Sra, and I. S. Dhillon, "Tackling box-constrained optimization via a new projected quasi-Newton approach," _SIAM Journal on Scientific Computing_, vol. 32, no. 6, pp. 3548-3563, 2010.
* [52] A.-L. Barabasi and R. Albert, "Emergence of scaling in random networks," _Science_, vol. 286, no. 5439, pp. 509-512, 1999.
* [53] R. Albert and A.-L. Barabasi, "Statistical mechanics of complex networks," _Reviews of Modern Physics_, vol. 74, no. 1, p. 47, 2002.
* [54] P. W. Holland, K. B. Laskey, and S. Leinhardt, "Stochastic blockmodels: First steps," _Social Networks_, vol. 5, no. 2, pp. 109-137, 1983.
* [55] S. Fortunato, "Community detection in graphs," _Physics Reports_, vol. 486, no. 3-5, pp. 75-174, 2010.
* [56] H. A. Dau, A. Bagnall, K. Kamgar, C.-C. M. Yeh, Y. Zhu, S. Gharghabi, C. A. Ratanamahatana, and E. Keogh, "The UCR time series archive," _IEEE/CAA Journal of Automatica Sinica_, vol. 6, no. 6, pp. 1293-1305, 2019.
* [57] C. W. Tan, G. I. Webb, and F. Petitjean, "Indexing and classifying gigabytes of time series under time warping," in _SIAM International Conference on Data Mining_, 2017, pp. 1-10.
* [58] M. E. Newman, "Modularity and community structure in networks," _Proceedings of the National Academy of Sciences_, vol. 103, no. 23, pp. 8577-8582, 2006.
* [59] F. Nie, X. Wang, M. Jordan, and H. Huang, "The constrained Laplacian rank algorithm for graph-based clustering," in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 30, no. 1, 2016.

## Appendix

In what follows, we present technical proofs omitted in the main content. In Section A, we introduce important properties of matrix \(\bm{R}\) and prove Theorem 3.4. In Section B, we prove the main result, i.e., Theorem 3.3. In Section C, we introduce some extension of the proposed framework to broaden its applicability. In Section D, we will comment on real-world experiments and explain why \(\text{MTP}_{2}\) assumption approximately holds for Crop Image dataset.

## Appendix A Proof of Theorem 3.4

This section contains a detailed description of how we construct the matrix \(\bm{R}\) and prove Theorem 3.4 based on the properties of \(\bm{R}\). The proof relies on the concept of a 'generalized path product', which involves a set of edges labeled as a 'bridge path'. The matrix \(\bm{R}\) has several notable properties that are essential in demonstrating Theorem 3.3.

### Definitions and Important Properties

To begin, let us define the 'bridge path' \(\mathcal{B}_{ij}\) for nodes \(i\) and \(j\) that belong to different clusters, i.e., \(\psi(i)\neq\psi(j)\) based on the bridge set \(\mathcal{B}\) in the thresholded graph \(G\). The bridge path \(\mathcal{B}_{ij}\) is defined as the set of bridges in any path \(d_{ij}\) from node \(i\) to node \(j\), where each element in \(\mathcal{B}_{ij}\) is a bridge in \(\mathcal{B}\) and the elements in \(\mathcal{B}_{ij}\) follow the order of \(d_{ij}\) while preserving the elements in \(\mathcal{B}\). Symbolically, we can express this as:

\[\mathcal{B}_{ij}=d_{ij}\cap\mathcal{B}=\left\{\left(u_{1},u_{2}\right),\left( u_{3},u_{4}\right),\ldots,\left(u_{2T-1},u_{2T}\right)\right\},\] (14)

where \(T\) denotes the number of clusters in any path from nodes \(i\) to \(j\). It is important to note the following:

* Each element in \(\mathcal{B}_{ij}\) is a bridge in the thresholded graph.
* The elements in \(\mathcal{B}_{ij}\) follow the orders of \(d_{ij}\). This means that for any path from nodes \(i\) to \(j\), the path will visit \(\left(u_{2i-1},u_{2i}\right)\), then visit \(\left(u_{2j-1},u_{2j}\right)\) whenever we have \(i<j\). consequently, we have \[\psi\left(i\right)=\psi\left(u_{1}\right),\quad\psi\left(u_{2}\right)=\psi \left(u_{3}\right),\ldots,\psi\left(u_{2T-2}\right)=\psi\left(u_{2T-1}\right), \quad\psi\left(u_{2T}\right)=\psi\left(j\right).\] (15)
* For any path \(d_{ij}\), the bridge path \(\mathcal{B}_{ij}=d_{ij}\cap\mathcal{B}\) is unique.
* \(\mathcal{B}_{ij}\) is defined as an empty set if there is no path from nodes \(i\) to \(j\).

It is important to mention that if the thresholded graph is acyclic, the bridge path can be reduced to a traditional path. In this scenario, all edges in the path \(d_{ij}\) become bridges, and the bridge path \(\mathcal{B}_{ij}\) is equivalent to the path \(d_{ij}\), which can be expressed as \(\mathcal{B}_{ij}=d_{ij}\).

We define the matrix \(\bm{R}\in\mathbb{S}^{p}\) based on the definition of bridge-path, as shown below:

\[R_{ij}=\begin{cases}\left[\widehat{\bm{R}}_{k}\right]_{\pi(i),\pi(j)}&\text{ if }i,j\in\mathcal{V}_{k},\\ T_{ij}&\text{ if }(i,j)\in\mathcal{B},\\ \sqrt{S_{ii}S_{jj}}\cdot g_{ij}\left(\bm{R}\right)&\text{ otherwise},\end{cases}\] (16)

where \(\widehat{\bm{R}}_{k}=[\widehat{\bm{\Theta}}_{k}]^{-1}\) and for each \(i\), \(j\) in different clusters, \(g_{ij}\) is given as

\[g_{ij}\left(\bm{R}\right)=\prod_{t=0}^{2T}R_{u_{t},u_{t+1}}\big{/}\sqrt{S_{u_ {t},u_{t}}S_{u_{t+1},u_{t+1}}},\quad\text{where }u_{0}\overset{\Delta}{=}i,\ u_{2T+1}\overset{ \Delta}{=}j,\] (17)

where \(u\) denotes the bridge path \(\mathcal{B}_{ij}\) and \(g_{ij}\left(\bm{R}\right)=0\) if \(\mathcal{B}_{ij}=\emptyset\).

_Remark A.1_.: Clearly, we have \(R_{ij}=R_{ji}\) for any \(i\) and \(j\).

**Example**: For instance, using Figure 7 as an example, we consider a path from node \(1\) to \(10\) as

\[d_{1,10}=\{\left(1,4\right),\left(4,5\right),\underbrace{\left(5,6\right)}_{ \text{bridge}},\left(6,7\right),\left(7,8\right),\left(8,9\right),\underbrace{ \left(9,10\right)}_{\text{bridge}}\},\] (18)

the bridge path is then given as

\[\mathcal{B}_{1,10}=\left\{\left(5,6\right),\left(9,10\right)\right\},\] (19)and \(R_{1,10}\) is computed as

\[R_{1,10}=\sqrt{S_{1,1}}\cdot g_{1,10}\left(\bm{R}\right)\cdot\sqrt{S _{10,10}}\] \[\quad=\sqrt{S_{1,1}}\cdot\frac{R_{1,5}}{\sqrt{S_{1,1}S_{5,5}}}\cdot \frac{R_{5,6}}{\sqrt{S_{5,5}S_{6,6}}}\cdot\frac{R_{6,9}}{\sqrt{S_{6,6}S_{9,9}}} \cdot\frac{R_{9,10}}{\sqrt{S_{9,9}S_{10,10}}}\cdot\frac{R_{10,10}}{\sqrt{S_{1 0,10}S_{10,10}}}\cdot\sqrt{S_{10,10}}.\] (20)

Particularly, we note that

* The bridge path \(\mathcal{B}_{1,10}\) can not be written as \(\{(9,10),(5,6)\}\) as \(d_{1,10}\) would always visit the bridge \((5,6)\) before visiting \((9,10)\).
* The bridge path \(\mathcal{B}_{1,10}\) can not be written as \(\{(6,5),(9,10)\}\) as \(d_{1,10}\) leave first cluster at node \(5\), then enter second cluster at node \(6\). Therefore, we have \(\psi(1)=\psi(5)\).
* It is easy to observe that \(\mathcal{B}_{10,1}=\{(10,9),(6,5)\}\). Based on definitions, we can obtain \(R_{10,1}=R_{1,10}\).
* Specially, if the matrix \(\bm{S}\) is given as a correlation matrix, i.e., \(\text{diag}(\bm{S})=\bm{1}\), then \(R_{1,10}=R_{1,5}\cdot R_{5,6}\cdot R_{6,9}\cdot R_{9,10}\cdot R_{10,10}\), which can be seen as a continued produce on a path, known as path product.

The matrix \(\bm{R}\) shares some important properties introduced as follows.

**Lemma A.2**.: _For any \(i,j\in\mathcal{V}_{k}\), \(\widehat{\bm{R}}_{k}\) together with \(\widehat{\bm{\Theta}}_{k}\) would satisfy the following equations_

\[-[\widehat{\bm{R}}_{k}]_{\pi(i),\pi(j)}+S_{i,j} =0 \text{if }i=j,\] (21a) \[-[\widehat{\bm{R}}_{k}]_{\pi(i),\pi(j)}+S_{i,j}-\Lambda_{i,j} =0 \text{if }i\neq j,[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}\neq 0,\] (21b) \[-[\widehat{\bm{R}}_{k}]_{\pi(i),\pi(j)}+S_{i,j}-\Lambda_{i,j} \leq 0 \text{if }i\neq j,[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}=0,\] (21c)

Proof.: Let's denote \(\Gamma_{\pi(i),\pi(j)}\) as the dual variables associated with the constraints \([\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}\leq 0\). With \(\widehat{\bm{R}}_{k}=\widehat{\bm{\Theta}}_{k}^{-1}\), the KKT conditions include

\[\text{stationarity:} -\widehat{\bm{R}}_{k}+\bm{S}_{\mathcal{V}_{k}}-\bm{\Lambda}_{ \mathcal{V}_{k}}+\bm{\Gamma}_{\mathcal{V}_{k}}=\bm{0},\] (22a) \[\text{primal feasibility:} [\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}\leq 0,\;\forall\pi(i)\neq\pi(j)\] (22b) \[\text{dual feasibility:} \Gamma_{\pi(i),\pi(j)}\geq 0,\;\forall\pi(i)\neq\pi(j)\] (22c) \[\text{complementary slackness:} [\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}\cdot\Gamma_{\pi(i), \pi(j)}=0,\;\forall\pi(i)\neq\pi(j)\] (22d)

We can eliminate the dual variables as follows:

* For \([\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}<0\), the complementary slackness leads to \(\Gamma_{\pi(i),\pi(j)}=0\), which implies \(-[\widehat{\bm{R}}_{k}]_{\pi(i),\pi(j)}+S_{i,j}-\Lambda_{i,j}=0\).
* For \([\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}=0\), we have \(\Gamma_{\pi(i),\pi(j)}\geq 0\), which implies \(-[\widehat{\bm{R}}_{k}]_{\pi(i),\pi(j)}+S_{i,j}-\Lambda_{i,j}\leq 0\).

Hence, Lemma A.2 is obtained.

Figure 7: An example of computing bridge path. For example, from node \(1\) to \(10\), all the paths will cover the bridges \((5,6)\) and \((9,10)\). Hence, we have \(\mathcal{B}_{1,10}=\{(5,6),(9,10)\}\).

In particular, we note that \([\widehat{\bm{R}}_{k}]_{\pi(i),\pi(i)}=S_{i,i}\) holds for any \(i\in\mathcal{V}_{k}\) according to (21a). As a result, the following property can be easily verified via the definitions.

**Corollary A.3**.: _Suppose that two nodes \(i\) and \(j\) are in different clusters, i.e., \(\psi\left(i\right)\neq\psi\left(j\right)\), and the bridge path is not empty and denoted by \(\mathcal{B}_{ij}\) with form of (14), then we have_

\[R_{ij}=\frac{R_{i,u_{i}}\cdot R_{u_{i},j}}{S_{u_{i},u_{i}}},\qquad\forall t=0, \ldots,2T+1,\] (23)

Proof.: Corollary A.3 can be easily verified via definitions. If \(t=0\) or \(t=2T+1\), Corollary A.3 holds as \(R_{i,i}=S_{i,i}\) and \(R_{j,j}=S_{j,j}\). Let \(s\) be a number that satisfies \(0\leq s\leq 2T\). For any \((u_{s},u_{s+1})\in\mathcal{B}_{ij}\), we have

\[R_{i,j}\cdot S_{u_{s},u_{s}} =\underbrace{\sqrt{S_{i,i}}\cdot\frac{R_{u_{0},u_{1}}}{\sqrt{S_{ u_{0},u_{0}}S_{u_{1},u_{1}}}}\cdot\ldots\cdot\frac{R_{u_{s-1},u_{s}}}{ \sqrt{S_{u_{s-1},u_{s-1}}S_{u_{s}}}}\cdot\sqrt{S_{u_{s},u_{s}}}}_{R_{i,u_{s}}}\] \[\qquad\cdot\underbrace{\sqrt{S_{u_{s},u_{s}}}\cdot\frac{R_{u_{s}, u_{s+1}}}{\sqrt{S_{u_{s},u_{s}}S_{u_{s+1},u_{s+1}}}}\cdot\ldots\cdot\frac{R_{u_{2T-1},u_{2T}}} {\sqrt{S_{u_{2T-1},u_{2T-1}}S_{u_{2T}}}}\cdot\sqrt{S_{j,j}}}_{R_{u_{s},j}}.\] (24)

Clearly, Corollary A.3 is obtained. 

The aforementioned properties of matrix \(\bm{R}\) are important for establishing the proof of Theorem 3.4 and Theorem 3.3.

### Proof of Theorem 3.4

Proof.: Let \(\bm{F}=\bm{\Theta}\bm{R}\), where \(\bm{\Theta}\) and \(\bm{R}\) have the forms of (7) and (8), respectively. The target is to show \(F_{ii}=1,\forall i\) and \(F_{ij}=0,\;\forall i\neq j\) using the following results we have derived.

**(a)** For any \(i,j\in\mathcal{V}_{k}\), we have

\[\bm{\Theta}_{ij}=\begin{cases}[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}+ \frac{1}{S_{ii}}\cdot\sum_{(i,m)\in\mathcal{B}}\frac{T_{im}^{2}}{S_{ii}S_{mm}- T_{im}^{2}}&\text{if $i=j$},\\ _{\blacksquare}[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}&\text{if $i\neq j$},\end{cases}\] (25)

and \(R_{ij}=[\widehat{\bm{R}}_{k}]_{\pi(i),\pi(j)}\) according to the definitions.

**(b)** For any nodes \(i\) and \(j\) in different clusters, suppose that \(\mathcal{B}_{ij}\) is not empty and edge \((u,v)\) is a bridge in \(\mathcal{B}_{ij}\), then \(R_{ij}S_{uu}=R_{iu}R_{uj}\) and \(R_{ij}S_{vv}=R_{iv}R_{vj}\) hold according to Corollary A.3.

**(c)** The matrix \(\widehat{\bm{F}}_{k}\), which is defined as \(\widehat{\bm{F}}_{k}=\widehat{\bm{R}}_{k}\widehat{\bm{\Theta}}_{k}\) is equal to the identity matrix, i.e, \(\widehat{\bm{F}}_{k}=\bm{I}\). In other words, we have

\[[\widehat{\bm{F}}_{k}]_{\pi(i),\pi(i)}=1,\;\text{and}\;[\widehat{\bm{F}}_{k}] _{\pi(i),\pi(j)}=0,\quad\forall i\neq j\;\text{and}\;i,j\in\mathcal{V}_{k}.\] (26)

Based on (a), (b), and (c), we first show that \(F_{ii}=1\) holds for any \(i\in\{1,\ldots,p\}\). We suppose that node \(i\) is in \(k\)-the cluster, i.e., \(k=\psi\left(i\right)\). Then, by using \(\forall m\notin(\mathcal{V}_{k}\cup\mathcal{N}\left(i\right)):\Theta_{im}=0\) from definitions in (7), we have

\[F_{ii} =\sum_{m=1}^{p}R_{im}\Theta_{im}=\sum_{m\in\mathcal{V}_{k}\backslash \{i\}}R_{im}\Theta_{im}+R_{ii}\Theta_{ii}+\sum_{(i,m)\in\mathcal{B}}R_{im} \Theta_{im}\] \[\overset{(a)}{=}\sum_{m\in\mathcal{V}_{k}\backslash\{i\}}[ \widehat{\bm{R}}_{k}]_{\pi(i),\pi(m)}[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(m) }+[\widehat{\bm{R}}_{k}]_{\pi(i),\pi(i)}[\widehat{\bm{\Theta}}_{k}]_{\pi(i), \pi(i)}\] \[\qquad+S_{ii}\cdot\frac{1}{S_{ii}}\sum_{(i,m)\in\mathcal{B}}\frac {T_{im}^{2}}{S_{ii}S_{mm}-T_{im}^{2}}+\sum_{(i,m)\in\mathcal{B}}T_{im}\left(- \frac{T_{im}}{S_{ii}S_{mm}-T_{im}^{2}}\right)\] \[=\sum_{m\in\mathcal{V}_{k}\backslash\{i\}}[\widehat{\bm{R}}_{k}]_ {\pi(i),\pi(m)}[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(m)}+[\widehat{\bm{R}}_{k}] _{\pi(i),\pi(i)}[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(i)}\overset{(c)}{=}[ \widehat{\bm{F}}_{k}]_{\pi_{k}(i),\pi_{k}(i)}=1.\] (27)Therefore, \(T_{ii}=1\) holds for any \(i\in\{1,\ldots,p\}\).

Let us proceed with the proof that \(F_{ij}=\sum_{m=1}^{p}R_{im}\Theta_{mj}=0\) for any \(i\neq j\). Without loss of generality, we suppose that node \(j\) is in the \(k\)-th cluster, i.e., \(k=\psi\left(j\right)\). According to the definitions, we have

\[\forall m\notin\left(\mathcal{V}_{k}\cup\mathcal{N}\left(j\right)\right): \Theta_{mj}=0.\] (28)

Here, there would be three cases to analyze.

The first case happens where \(i\in\mathcal{V}_{k}\), which means that nodes \(i\) and \(j\) are in the same cluster. In this case, we have

\[F_{ij} =\sum_{m=1}^{p}R_{im}\Theta_{mj}=\sum_{m\in\mathcal{V}_{k}\setminus \left\{j\right\}}R_{im}\Theta_{mj}+R_{ij}\Theta_{jj}+\sum_{(m,j)\in\mathcal{B} }R_{im}\Theta_{mj}\] \[\overset{(a)}{=} \sum_{m\in\mathcal{V}_{k}\setminus\left\{j\right\}}[\widehat{ \boldsymbol{R}}_{k}]_{\pi(i),\pi(m)}[\widehat{\boldsymbol{\Theta}}_{k}]_{\pi( m),\pi(j)}\] \[\quad+[\widehat{\boldsymbol{R}}_{k}]_{\pi(i),\pi(j)}\left([ \widehat{\boldsymbol{\Theta}}_{k}]_{\pi(j),\pi(j)}+\frac{1}{S_{jj}}\sum_{(m,j) \in\mathcal{B}}\frac{T_{mj}^{2}}{S_{mm}S_{jj}-T_{mj}^{2}}\right)+\sum_{(m,j) \in\mathcal{B}}R_{im}\Theta_{mj}\] \[\overset{(c)}{=} [\widehat{\boldsymbol{F}}_{k}]_{\pi_{k}(i),\pi_{k}(j)}+R_{ij} \left(\frac{1}{S_{jj}}\sum_{(m,j)\in\mathcal{B}}\frac{T_{mj}^{2}}{S_{mm}S_{jj }-T_{mj}^{2}}\right)+\sum_{(m,j)\in\mathcal{B}}R_{im}\Theta_{mj}\] \[\overset{(b,c)}{=} 0+R_{ij}\left(\frac{1}{S_{jj}}\sum_{(m,j)\in\mathcal{B}}\frac{T_{ mj}^{2}}{S_{mm}S_{jj}-T_{mj}^{2}}\right)+\sum_{(m,j)\in\mathcal{B}}\frac{R_{ij}R_{jm}}{S_ {jj}}\left(-\frac{T_{mj}}{S_{mm}S_{jj}-T_{mj}^{2}}\right)\] \[=0.\] (29)

The second case happens where \(i\notin\mathcal{V}_{k}\), but there is no path between nodes \(i\) and \(j\). In such a case, for any node \(m\) in \(\mathcal{V}_{k}\cup\mathcal{N}\left(j\right)\), there is also no path from nodes \(i\) to \(m\). Consequently, based on the definition of \(\boldsymbol{R}\), we have

\[\forall m\in\mathcal{V}_{k}\cup\mathcal{N}\left(j\right):\quad R_{im}=0.\] (30)

As a result, we obtain

\[F_{ij}=\sum_{m=1}^{p}R_{im}\Theta_{mj}=\sum_{m\in\mathcal{V}_{k}\cup\mathcal{ N}\left(j\right)}R_{im}\Theta_{mj}=0.\] (31)

The third case happens where \(i\notin\mathcal{V}_{k}\) and there exists a path from nodes \(i\) to \(j\). Then we have \(\psi\left(i\right)\neq\psi\left(j\right)\) and the bridge path \(\widehat{\mathcal{B}}_{ij}\) is denoted as

\[\mathcal{B}_{ij}=\left\{\left(u_{1},u_{2}\right),\left(u_{3},u_{4}\right), \ldots,\left(u_{2T-1},u_{2T}\right)\right\},\] (32)

where \(\psi\left(u_{1}\right)=\psi\left(i\right)\) and \(\psi\left(u_{2T}\right)=\psi\left(j\right)=k\). In particular, we emphasize that \(u_{2T}\in\mathcal{V}_{k}\) as \(\left(u_{2T-1},u_{2T}\right)\) is a bridge to enter the cluster that contains \(j\). Therefore, we have

\[F_{ij}=\sum_{m=1}^{p}R_{im}\Theta_{mj}\overset{(b)}{=}\frac{R_{i,u_{2T}}}{S_{ u_{2T},u_{2T}}}\cdot\sum_{m=1}^{p}R_{u_{2T},m}\Theta_{mj}=0,\] (33)

where the last equality is from (29) by using \(u_{2T}\in\mathcal{V}_{k}\). In conclusion, via enumerating all the possible cases, we have \(\boldsymbol{F}=\boldsymbol{I}\), which means that \(\boldsymbol{R}\boldsymbol{\Theta}=\boldsymbol{I}\). 

_Remark A.4_.: Theorem 3.4 generally holds for any \(\boldsymbol{T}\), \(\boldsymbol{S}\) and \(\{\widehat{\boldsymbol{\Theta}}_{k}\in\mathbb{S}_{++}^{|\mathcal{V}_{k}|}\}_{ k=1}^{K}\). It does not utilize \(\text{MTP}_{2}\) properties from \(\boldsymbol{\Theta}\). Hence, Theorem 3.4 itself may be extended to graphical models beyond \(\text{MTP}_{2}\) assumption.

Proof of Main Results

To prove Theorem 3.3, we require some preliminaries on \(\text{MTP}_{2}\) properties.

**Lemma B.1**.: _[_11_, Theorem 1]_ _A real symmetric matrix \(\bm{X}\in\mathbb{S}^{p}\), which admits the form:_

\[\forall i\neq j:\quad X_{ij}\leq 0,\] (34)

_is a non-singular \(M\)-matrix if and only if one of the following statements holds: (1) Every eigenvalue of \(\bm{X}\) is positive; (2) \(\bm{X}\) is inverse-positive. That is, \(\bm{X}^{-1}\) exists and \(\bm{X}^{-1}\geq\bm{0}\)._

A direct outcome of Lemma B.1 is that the matrix \(\bm{R}\) is a non-negative matrix.

**Corollary B.2**.: _The matrix \(\bm{R}\) given by (8) satisfies \(\bm{R}\geq\bm{0}\)._

Corollary B.2 holds according to the fact that

\[\forall k\in\{1,\dots,K\}:\quad\widehat{\bm{\Theta}}_{k}\in\mathcal{M}^{p_{k} }\quad\Rightarrow\quad\widehat{\bm{R}}_{k}\geq\bm{0}.\] (35)

Now, we begin proving Theorem 3.3.

**Proof of Theorem 3.3:** First of all, we show that the matrix \(\bm{\Theta}\), given as

\[\bm{\Theta}_{i,j}=\begin{cases}[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(i)}+ \zeta_{i}&\text{if }i=j\in\mathcal{V}_{k},\\ &\hskip 1.422638pt[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}&\text{if }i\neq j \text{ and }i,j\in\mathcal{V}_{k},\\ -T_{ij}/\big{(}S_{ii}S_{jj}-T_{ij}^{2}\big{)}&\text{if }(i,j)\in\mathcal{B},\\ 0&\text{otherwise}.\end{cases}\] (36)

is primal feasible and positive definite. Based on Assumption 2.1, which states that \(S_{ij}<\sqrt{S_{ii}S_{jj}}\), we know that \(T_{ij}^{2}\leq(S_{ij}-\Lambda_{ij})^{2}\leq S_{ij}^{2}<S_{ii}S_{jj}\). Hence, for any \(i\neq j\), we have \(\Theta_{ij}\leq 0\) due to \(-T_{ij}/\big{(}S_{ii}S_{jj}-T_{ij}^{2}\big{)}\leq 0\) and the primal feasibility of each \(\widehat{\bm{\Theta}}_{k}\). Additionally, since we have \(\bm{R}\geq\bm{0}\) according to Corollary B.2, we can conclude that \(\bm{\Theta}\) is inverse-positive according to Theorem 3.4, which implies that it is a positive-definite matrix. In other words, \(\bm{\Theta}\) is both primal feasible and inverse-positive, and we can express this as \(\bm{\Theta}\succ\bm{0}\) based on Lemma B.1.

In second step, we show that \(\bm{\Theta}=\bm{R}^{-1}\) satisfies the optimality condition listed as follows, i.e.,

\[-\big{[}\bm{\Theta}^{-1}\big{]}_{ij}+S_{ij} =0 \text{if }i=j,\] (37a) \[-\big{[}\bm{\Theta}^{-1}\big{]}_{ij}+S_{ij}-\Lambda_{ij} =0 \text{if }i\neq j,\Theta_{ij}\neq 0,\] (37b) \[-\big{[}\bm{\Theta}^{-1}\big{]}_{ij}+S_{ij}-\Lambda_{ij} \leq 0 \text{if }i=j,\Theta_{ij}=0.\] (37c)

For (37a), we let \(k=\psi\left(i\right)\). Then, according to (21a) in Lemma A.2, we have

\[-[\bm{\Theta}^{-1}]_{ij}+S_{ij}=-[\widehat{\bm{R}}_{k}]_{\pi(i),\pi(i)}+S_{ij }=0.\] (38)

For (37b), as \(\Theta_{ij}\neq 0\), the pair \((i,j)\) either satisfies \((i,j)\in\mathcal{B}\) or \(\psi\left(i\right)=\psi\left(j\right)\stackrel{{\Delta}}{{=}}k\). If \((i,j)\in\mathcal{B}\), then by using \(R_{ij}=T_{ij}\), we have

\[-[\bm{\Theta}^{-1}]_{ij}+S_{ij}-\Lambda_{ij}=-R_{ij}+T_{ij}=0.\] (39)

If nodes \(i\) and \(j\) are in the same cluster, then according to (21b) in Lemma A.2, we have \(\Theta_{ij}=[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}\neq 0\) such that

\[-[\bm{\Theta}^{-1}]_{ij}+S_{ij}-\Lambda_{ij}=-[\widehat{\bm{R}}_{k}]_{\pi(i), \pi(j)}+S_{ij}-\Lambda_{ij}=0.\] (40)

For (37c), the pair \((i,j)\) either satisfies \(\psi\left(i\right)\neq\psi\left(j\right)\) or \(\psi\left(i\right)=\psi\left(j\right)\). For first case, we have \(S_{ij}-\Lambda_{ij}\leq 0\) as \((i,j)\notin\text{supp}\left(\bm{T}\right)\). Consequently, we obtain

\[-[\bm{\Theta}^{-1}]_{ij}+S_{ij}-\Lambda_{ij}\leq-R_{ij}\leq 0,\] (41)

as \(\bm{R}\geq\bm{0}\). For second case where \(\psi\left(i\right)=\psi\left(j\right)\), we have \(\Theta_{ij}=[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}=0\). Then, applying (21c) in Lemma A.2, we have

\[-[\bm{\Theta}^{-1}]_{ij}+S_{ij}-\Lambda_{ij}=-[\widehat{\bm{R}}_{k}]_{\pi(i), \pi(j)}+S_{ij}-\Lambda_{ij}\leq 0.\] (42)

Hence, together with the fact that \(\bm{\Theta}\) is primal feasible and positive definite, we conclude that \(\bm{\Theta}\) satisfies the KKT conditions of convex Problem (1). Therefore, \(\bm{\Theta}\) is the optimal solution of Problem (1) and Theorem 3.3 is obtained.

Extensions and Discussions

### Extensions to traditional graphical lasso

The traditional graphical lasso is formulated as

\[\underset{\mathbf{\Theta}\succ\mathbf{0}}{\text{minimize}} -\log\det\left(\mathbf{\Theta}\right)+\left\langle\mathbf{\Theta}, \boldsymbol{S}\right\rangle+\sum_{i\neq j}\Lambda_{ij}\left|\Theta_{ij}\right|.\] (43)

The KKT conditions of graphical lasso include:

\[-R_{ij}+S_{ij}=0 \text{if }i=j,\] (44a) \[-R_{ij}+S_{ij}-\Lambda_{ij}\cdot\text{sign}\left(\Theta_{ij} \right)=0 \text{if }i\neq j,\Theta_{ij}\neq 0,\] (44b) \[-\Lambda_{ij}\leq-R_{ij}+S_{ij}-\leq\Lambda_{ij} \text{if }i=j,\Theta_{ij}=0.\] (44c)

In the realm of graphical lasso, the thresholded sample covariance matrix is usually defined as

\[T_{ij}=\begin{cases}0&\text{if }|S_{ij}|\leq\Lambda_{ij},\\ S_{ij}-\Lambda_{ij}\cdot\text{sign}\left(S_{ij}\right)&\text{if }|S_{ij}|>\Lambda_{ij}. \end{cases}\] (45)

Our main results Theorem 3.3 can be generalized to graphical lasso with some modifications.

**Theorem C.1**.: _Suppose that \(\mathcal{P}\) is the bridge-block decomposition of the thresholded graph, \(\widehat{\mathbf{\Theta}}_{k}\) is the optimal solution of \(k\)-th sub-problem, i.e.,_

\[\widehat{\mathbf{\Theta}}_{k}=\arg\min_{\mathbf{\Theta}_{k}\succ\mathbf{0}}- \log\det\left(\mathbf{\Theta}_{k}\right)+\left\langle\mathbf{\Theta}_{k}, \boldsymbol{S}_{\mathcal{V}_{k}}\right\rangle+\sum_{i\neq j}\left[\mathbf{ \Lambda}_{\mathcal{V}_{k}}\odot\left|\mathbf{\Theta}_{k}\right|\right]_{ij}.\] (46)

_The decomposed form, which is given as_

\[\mathbf{\Theta}_{i,j}^{*}=\begin{cases}[\widehat{\mathbf{\Theta}}_{k}]_{\pi(i ),\pi(i)}+\zeta_{i}&\text{if }i=j\in\mathcal{V}_{k},\\ [\widehat{\mathbf{\Theta}}_{k}]_{\pi(i),\pi(j)}&\text{if }i\neq j\text{ and }i,j\in \mathcal{V}_{k},\\ -T_{ij}/(S_{ii}S_{jj}-T_{ij}^{2})&\text{if }(i,j)\in\mathcal{B},\\ 0&\text{otherwise},\end{cases}\] (47)

_in which \(\zeta_{i}=\frac{1}{S_{ii}}\sum_{(i,m)\in\mathcal{B}}\frac{T_{im}^{2}}{S_{ii}S_ {mm}-T_{im}^{2}}\) and \(\zeta_{i}=0\) if \(\forall m:(i,m)\notin\mathcal{B}\), achieves optimality if \(\mathbf{\Theta}\succ\mathbf{0}\) and the following inequality is satisfied:_

\[|-R_{ij}+S_{ij}|\leq\Lambda_{ij},\quad\forall i\neq j\text{ and }i,j\text{ belong to different clusters}.\] (48)

It is important to note that the decomposed form (47) aligns with the structure of (7), with the key distinction lying in the computation of the thresholded matrix \(\boldsymbol{T}\). The proofs of Theorem C.1 can be conducted similar to Section B.

The condition (48) can be guaranteed under some circumstances. For example, we can set large values for \(\Lambda_{ij}\) whenever \(i\neq j\) and \(i\), \(j\) are in different clusters. In this case, the thresholded graph is sparse and the condition (48) is satisfied. Nevertheless, in the majority of practical situations, verifying this condition poses a substantial challenge. Conversely, these conditions are inherently satisfied when \(\text{MTP}_{2}\) constraints are applied.

### Extensions to dense graphs

The proposed method operates under an implicit assumption of sparsity, which is a common characteristic of high-dimensional applications where the focus is on identifying only the most significant relationships among variables. However, it is possible to encounter scenarios where the thresholded graph is dense, making it difficult to identify any bridges for conducting bridge-block decomposition. In such cases, the proposed framework can still be useful, and this section will primarily focus on exploring its potential applications under these circumstances. We will discuss various strategies and techniques for adapting the proposed method to deal with dense graphs, enabling a broader applicability of the proposed bridge-block decomposition approach.

One of typical approaches to make the proposed method applicable is to tune the values of regularization coefficients. As the thresholded matrix is computed as

\[\bm{T}=\max\left(\bm{0},\bm{S}-\bm{\Lambda}\right),\] (49)

we can always increase the value of \(\Lambda_{ij}\) to make the thresholded graph \(\text{supp}(\bm{T})\) sufficiently sparse. When tuning the hyper-parameters is not recommended, a more general strategy is to use the solution of proposed framework as an initial point of numerical algorithms.

**Warm-start algorithm**: One of the main possible strengths of the proposed decomposed-form solution is that it can be treated as an initial point (warm-start) for the numerical algorithms specialized for solving Problem 1. To apply the decomposition approach, we first need to derive a suitable vertex partition of the graph based on the given data matrix \(\bm{S}\) and regularization matrix \(\bm{\Lambda}\). There are several possible vertex partitioning techniques that can be used, including:

* \(k\)-edge-connected decomposition: This method divides a connected graph into its \(k\)-edge-connected sub-graphs, which can be useful for identifying bridges or bottlenecks in the graph structure.
* \(k\)-vertex-connected decomposition: This method decomposes a graph into its \(k\)-vertex-connected sub-graphs, which can be useful for identifying clusters or communities of vertices that are tightly connected to each other.
* \(k\)-core decomposition: This method identifies the maximal subgraphs of the graph in which each vertex has a degree of at least \(k\), which can be useful for identifying the most densely connected regions of the graph.
* Community detection: This method identifies groups of vertices that are densely connected within the group and sparsely connected to vertices outside the group, which can be useful for identifying clusters or communities of vertices with similar properties or behavior.

Once we obtain a vertex partition \(\mathcal{P}=\left\{\mathcal{V}_{1},\ldots,\mathcal{V}_{K}\right\}\), where \(K\) refers to the number of clusters, we can compute the initial point using the following form:

\[\Theta_{i,j}=\begin{cases}[\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(i)}+\zeta_{ i}&\text{if }i=j\in\mathcal{V}_{k},\\ [\widehat{\bm{\Theta}}_{k}]_{\pi(i),\pi(j)}&\text{if }i\neq j\text{ and }i,j\in \mathcal{V}_{k},\\ -T_{ij}/(S_{ii}S_{jj}-T_{ij}^{2})&\text{if }(i,j)\in\mathcal{E}_{e},\\ 0&\text{otherwise},\end{cases}\] (50)

in which \(\mathcal{E}_{e}\) refers to a set of external edges among clusters, i.e.,

\[\mathcal{E}_{e}=\left\{(i,j)\left|(i,j)\in\text{supp}\left(\bm{T}\right), \psi\left(i\right)\neq\psi\left(j\right)\right.\right\},\] (51)

\(\zeta_{i}\) is computed as

\[\zeta_{i}=\frac{1}{S_{ii}}\sum_{(i,m)\in\mathcal{E}_{e}}\frac{T_{im}^{2}}{S_{ ii}S_{mm}-T_{im}^{2}},\] (52)

and \(\widehat{\bm{\Theta}}_{k}\) is computed as the optimal solution of \(k\)-th sub-problem:

\[\widehat{\bm{\Theta}}_{k}=\arg\min_{\bm{\Theta}_{k}\in\mathcal{M}^{p_{k}}}- \log\det\left(\bm{\Theta}_{k}\right)+\left\langle\bm{\Theta}_{k},\bm{S}_{ \mathcal{V}_{k}}-\bm{\Lambda}_{\mathcal{V}_{k}}\right\rangle.\] (53)

_Remark C.2_.: The initial point is the optimal solution if the vertex-partition \(\mathcal{P}\) is a bridge-block decomposition of the optimal graph.

_Remark C.3_.: When the vertex partition \(\mathcal{P}\) is given as \(\mathcal{P}=\left\{\left\{1\right\},\left\{2\right\},\ldots,\left\{p\right\}\right\}\), i.e., \(\mathcal{V}_{k}=\left\{k\right\}\) for \(k=1,\ldots,p\), the initial point (50) reduces to a closed-form expression:

\[\Theta_{i,j}=\begin{cases}\frac{1}{S_{ii}}\left(1+\sum_{(i,m)\in\text{supp} \left(\bm{T}\right)}\frac{T_{im}^{2}}{S_{ii}S_{mm}-T_{im}^{2}}\right)&\text{if }i=j,\\ -T_{ij}/(S_{ii}S_{jj}-T_{ij}^{2})&\text{if }(i,j)\in\text{supp}\left(\bm{T} \right),\\ 0&\text{otherwise}.\end{cases}\] (54)

In practice, utilizing the warm-start strategy can also enhance the performance of certain numerical algorithms.

## Appendix D Additional Comments on Real-World Experiments

To show that MTP\({}_{2}\) assumption approximately holds for Crop dataset, we selected \(20\) random subsets from the Crop dataset. For each subset, we computed the Graphical Lasso and the MTP\({}_{2}\) graphical model for different values of using the first \(10\) observations. The remaining \(36\) observations were used to calculate the out-of-sample log-likelihood, which was then averaged across all datasets. This process allows us to evaluate how well these models generalize to unseen data.

As depicted in Figure 8 of the attached PDF, the MTP\({}_{2}\) graphical model outperforms the Graphical Lasso, providing a higher test log-likelihood. We present one instance of the estimated graphical lasso model in Figure 9. It reveals that most conditional correlations are positive (red edges, 90%), with a few being negative (blue edges, 10%). This pattern implies strong positive dependence in the Crop data, aligning with the characteristics of MTP\({}_{2}\).

These results are not unexpected, given that the Crop dataset comprises multiple clusters. Within the same cluster, we expect data points to exhibit greater similarity compared to those in different clusters. This situation signifies a form of positive dependence, thereby justifying the plausibility of the MTP\({}_{2}\) assumption in this problem.

Figure 8: Average log-likelihood in out-of-sample data for different precision matrix estimation models as a function of the sparsity promoting hyperparameter \(\lambda\).

Figure 9: Estimated Graphical Lasso of Crop data set at their highest average likelihood. Red edges represent positive conditional correlations, while blue edges represent negative ones.