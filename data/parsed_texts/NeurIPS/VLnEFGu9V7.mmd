# Regret Minimization via Saddle Point Optimization

 Johannes Kirschner

Department of Computer Science

University of Alberta

jkirschn@ualberta.ca &Seyed Alireza Bakhtiari

Department of Computer Science

University of Alberta

sbakhtia@ualberta.ca &Kushagra Chandak

Department of Computer Science

University of Alberta

kchandak@ualberta.ca &Volodymyr Tkachuk

Department of Computer Science

University of Alberta

vtkachuk@ualberta.ca &Csaba Szepesvari

Department of Computer Science

University of Alberta

szepesva@ualberta.ca

###### Abstract

A long line of works characterizes the sample complexity of regret minimization in sequential decision-making by min-max programs. In the corresponding saddle-point game, the min-player optimizes the sampling distribution against an adversarial max-player that chooses confusing models leading to large regret. The most recent instantiation of this idea is the decision-estimation coefficient (DEC), which was shown to provide nearly tight lower and upper bounds on the worst-case expected regret in structured bandits and reinforcement learning. By reparametrizing the offset DEC with the confidence radius and solving the corresponding min-max program, we derive an anytime variant of the Estimation-To-Decisions algorithm (Anytime-E2D). Importantly, the algorithm optimizes the exploration-exploitation trade-off online instead of via the analysis. Our formulation leads to a practical algorithm for finite model classes and linear feedback models. We further point out connections to the information ratio, decoupling coefficient and PAC-DEC, and numerically evaluate the performance of E2D on simple examples.

## 1 Introduction

Regret minimization is a widely studied objective in bandits and reinforcement learning theory [Lattimore and Szepesvari, 2020a] that has inspired practical algorithms, for example, in noisy zero-order optimization[e.g., Srinivas et al., 2010] and deep reinforcement learning [e.g., Osband et al., 2016]. Cumulative regret measures the online performance of the algorithm by the total loss suffered due to choosing suboptimal decisions. Regret is unavoidable to a certain extent as the learner collects information to reduce uncertainty about the environment. In other words, a learner will inevitably face the exploration-exploitation trade-off where it must balance collecting rewards and collecting information. Finding the right balance is the central challenge of sequential decision-making under uncertainty.

More formally, denote by \(\Pi\) a decision space and \(\mathcal{O}\) an observation space. Let \(\mathcal{M}\) be a class of models, where \(f=(r_{f},M_{f})\in\mathcal{M}\) associated with a reward function \(r_{f}:\Pi\rightarrow\mathbb{R}\) and observationmap \(M_{f}:\Pi\to\mathscr{P}(\mathcal{O})\), where \(\mathscr{P}(\mathcal{O})\) is the set of all probability distributions over \(\mathcal{O}\).1 The learner's objective is to collect as much reward as possible in \(n\) steps when facing a model \(f^{*}\in\mathcal{M}\). The learner's prior information is \(\mathcal{M}\) and the associated reward and observation maps, but does not know the true instance \(f^{*}\in\mathcal{M}\). The learner constructs a stochastic sequence \(\pi_{1},\ldots,\pi_{n}\) of decisions taking values in \(\Pi\) and adapted to the history of observations \(y_{t}\sim M_{f^{*}}(\pi_{t})\). The policy of the learner is the sequence of probability kernels \(\mu_{1:n}=(\mu_{t})_{t=1}^{n}\) that are used to take decisions. The expected regret of a policy \(\mu_{1:n}\) and model \(f^{*}\) after \(n\in\mathbb{N}\) steps is

Footnote 1: To simplify the presentation, we ignore tedious measure-theoretic details in this paper. The reader could either fill out the missing details, or just assume that all sets, unless otherwise stated, are discrete.

\[R_{n}(\mu_{1:n},f^{*})=\max_{\pi\in\Pi}\mathbb{E}\left[\sum_{t=1}^{n}r_{f^{*}}( \pi)-r_{f^{*}}(\pi_{t})\right]\]

The literature studies regret minimization for various objectives, including worst-case and instance-dependent frequentist regret (Lattimore and Szepesvari, 2020), Bayesian regret (Russo and Van Roy, 2014) and robust variants (Garcelon et al., 2020; Kirschner et al., 2020). For the frequentist analysis, all prior knowledge is encoded in the model class \(\mathcal{M}\). The worst-case regret of policy \(\mu_{1:n}\) on \(\mathcal{M}\) is \(\sup_{f\in\mathcal{M}}R_{n}(\mu_{1:n},f)\), and therefore the optimal minimax regret \(\inf_{\mu}\sup_{f\in\mathcal{M}}R_{n}(\mu_{1:n},f)\) only depends on \(\mathcal{M}\) and the horizon \(n\). The Bayesian, in addition, assumes access to a prior \(\nu\in\mathscr{P}(\mathcal{M})\), which leads to the Bayesian regret \(\mathbb{E}_{f\sim\nu}[R_{n}(\mu_{1:n},f)]\). Interestingly, the worst-case frequentist regret and Bayesian regret are dual in the following sense (Lattimore and Szepesvari, 2019):2

Footnote 2: The result by Lattimore and Szepesvari (2019) was only shown for finite action, reward and observation spaces, but can likely be extended to the infinite case under suitable continuity assumptions.

\[\inf_{\mu_{1:n}}\sup_{f\in\mathcal{M}}R_{n}(\mu_{1:n},f)=\sup_{\nu\in\mathscr{P }(\mathcal{M})}\inf_{\mu_{1:n}}\mathbb{E}_{f\sim\nu}[R_{n}(\mu_{1:n},f)]\] (1)

Unfortunately, directly solving for the minimax policy (or the worst-case prior) is intractable, except in superficially simple problems. This is because the optimization is over the exponentially large space of adaptive policies. However, the relationship in Eq. (1) has been directly exploited in prior works, for example, to derive non-constructive upper bounds on the worst-case regret via a Bayesian analysis (Bubeck et al., 2015). Moreover, it can be seen as inspiration underlying "optimization-based" algorithms for regret minimization: The crucial step is to carefully relax the saddle point problem in a way that preserves the statistical complexity, but can be analyzed and computed more easily. This idea manifests in several closely related algorithms, including information-directed sampling (Russo and Van Roy, 2014; Kirschner and Krause, 2018), ExpByOpt (Lattimore and Szepesvari, 2020, 2021), and most recently, the Estimation-To-Decisions (E2D) framework (Foster et al., 2021, 2023). These algorithms have in common that they optimize the information trade-off directly, which in structured settings leads to large improvements compared to standard optimistic exploration approaches and Thompson sampling. On the other hand, algorithms that directly optimize the information trade-off can be computationally more demanding and, consequently, are often not the first choice of practitioners. This is partly due to the literature primarily focusing on statistical aspects, leaving computational and practical considerations underexplored.

ContributionsBuilding on the results by Foster et al. (2021), we introduce the _average-constrained decision-estimation coefficient_ (\(\text{dec}_{\epsilon}^{ac}\)), a saddle-point objective that characterizes the frequentist worst-case regret in sequential decision-making with structured observations. Compared to the decision-estimation coefficient of (Foster et al., 2021), the \(\text{dec}_{\epsilon}^{ac}\) is parametrized via the confidence radius \(\epsilon\), instead of the Lagrangian offset multiplier. This allows optimization of the information trade-off online by the algorithm, instead of via the derived regret upper bound. Moreover, optimizing the \(\text{dec}_{\epsilon}^{ac}\) leads to an anytime version of the E2D algorithm (Anytime-E2D) with a straightforward analysis. We also point out relations between the \(\text{dec}_{\epsilon}^{ac}\), the information ratio (Russo and Van Roy, 2016), the decoupling coefficient (Zhang, 2022) and a PAC version of the DEC (Foster et al., 2023). We further detail how to implement the algorithm for finite model classes and linear feedback models, and demonstrate the advantage of the approach by providing improved bounds for linear bandits with side-observations. Lastly, we report the first empirical results of the E2D algorithm on simple examples.

### Related Work

There is a broad literature on regret minimization in bandits (Lattimore and Szepesvari, 2020) and reinforcement learning (Jin et al., 2018; Azar et al., 2017; Zhou et al., 2021; Du et al., 2021;Zanette et al., 2020). Arguably the most popular approaches are based on optimism, leading to the widely analysed upper confidence bound (UCB) algorithms (Lattimore and Szepesvari, 2020a), and Thompson sampling (TS) (Thompson, 1933; Russo and Van Roy, 2016).

A long line of work approaches regret minimization as a saddle point problem. Degenne et al. (2020) showed that in the structured bandit setting, an algorithm based on solving a saddle point equation achieves asymptotically optimal regret bounds, while explicitly controlling the finite-order terms. Lattimore and Szepesvari (2020) propose an algorithm based on exponential weights in the partial monitoring setting (Rustichini, 1999) that finds a distribution for exploration by solving a saddle-point problem. The saddle-point problem balances the trade-off between the exponential weights distribution and an information or stability term. The same approach was further refined by Lattimore and Gyorgy (2021). In stochastic linear bandits, Kirschner et al. (2021) demonstrated that information-directed sampling can be understood as a primal-dual method solving the asymptotic lower bound, which leads to an algorithm that is both worst-case and asymptotically optimal. The saddle-point approach has been further explored in the PAC setting (e.g., Degenne et al., 2020, 2020).

Our work is closely related to recent work by Foster et al. (2021, 2023). They consider _decision making with structured observations_ (DMSO), which generalizes the bandit and RL setting. They introduce a complexity measure, the _offset decision-estimation coefficient_ (offset DEC), defined as a min-max game between a learner and an environment, and provide lower bounds in terms of the offset DEC. Further, they provide an algorithm, _Estimation-to-Decisions_ (E2D) with corresponding worst-case upper bounds in terms of the offset DEC. Notably, the lower and upper bound nearly match and recover many known results in bandits and RL. More recently, Foster et al. (2023) refined the previous bounds by introducing the _constrained_ DEC and a corresponding algorithm E2D\({}^{+}\).

There are various other results related to the DEC and the E2D algorithm. Foster et al. (2022) show that the E2D achieves improved bounds in model-free RL when combined with optimistic estimation (as introduced by Zhang (2022)). Chen et al. (2022) introduced two new complexity measures based on the DEC that are necessary and sufficient for reward-free learning and PAC learning. They also introduced new algorithms based on the E2D algorithm for the above two settings and various other improvements. Foster et al. (2022) have shown that the DEC is necessary and sufficient to obtain low regret for _adversarial_ decision-making. An asymptotically instance-optimal algorithm for DMSO has been proposed by Dong and Ma (2022), extending a similar approach for the linear bandit setting (Lattimore and Szepesvari, 2017).

The decision-estimation coefficient is also related to the information ratio (Russo and Van Roy, 2014) and the decoupling coefficient (Zhang, 2022). The information ratio has been studied under both the Bayesian (Russo and Van Roy, 2014) and the frequentist regret (Kirschner and Krause, 2018; Kirschner et al., 2020, 2021, 2023) in various settings including bandits, reinforcement learning, and partial monitoring. The decoupling coefficient was studied for the Thompson sampling algorithm in contextual bandits (Zhang, 2022), and RL (Dann et al., 2021; Agarwal and Zhang, 2022).

## 2 Setting

We consider the sequential decision-making problem already introduced in the preface. Recall that \(\Pi\) is a compact decision space and \(\mathcal{O}\) is an observation space. The model class \(\mathcal{M}\) is a set of tuples \(f=(r_{f},M_{f})\) containing a reward function \(r_{f}:\Pi\rightarrow\mathbb{R}\) and an observation distribution \(M_{f}:\Pi\rightarrow\mathscr{P}(\mathcal{O})\). We define the gap function

\[\Delta(\pi,g)=r_{g}(\pi_{g}^{*})-r_{g}(\pi)\,,\]

where \(\pi_{g}^{*}=\arg\max_{\pi\in\Pi}r_{g}(\pi)\) is an optimal decision for model \(g\), chosen arbitrarily if not unique. A randomized policy is a sequence of kernels \(\mu_{1:n}=(\mu_{t})_{t=1}^{n}\) from histories \(h_{t-1}=(\pi_{1},y_{1},\ldots,\pi_{t-1},y_{t-1})\in(\Pi\times\mathcal{O})^{t-1}\) to sampling distributions \(\mathscr{P}(\Pi)\). The filtration generated by the history \(h_{t}\) is \(\mathcal{F}_{t}\). The learner's decisions \(\pi_{1},\ldots,\pi_{n}\) are sampled from the policy \(\pi_{t}\sim\mu_{t}\) and observations \(y_{t}\sim M_{f^{*}}(\pi_{t})\) are generated by an unknown true model \(f^{*}\in\mathcal{M}\). The expected regret under model \(f^{*}\) is formally defined as follows:

\[R_{n}(\mu_{1:n},f^{*})=\mathbb{E}\Bigg{[}\sum_{t=1}^{n}\mathbb{E}_{\pi_{t} \sim\mu_{t}(h_{t})}[\Delta(\pi_{t},f^{*})]\Bigg{]}\]For now, we do not make any assumption about the reward being observed. This provides additional flexibility to model a wide range of scenarios, including for example, duelling and ranking feedback (Yue and Joachims, 2009; Radlinski et al., 2008; Combes et al., 2015; Lattimore et al., 2018; Kirschner and Krause, 2021) (e.g. used in reinforcement learning with human feedback, RLHF) or dynamic pricing (Jean Boer, 2015). The setting is more widely known as partial monitoring Rustichini (1999). The special case where the reward is part of the observation distribution is called _decision-making with structured observations_(DMSO, Foster et al., 2021). Earlier work studies the closely related _structured bandit_ setting (Combes et al., 2017).

A variety of examples across bandit models and reinforcement learning are discussed in (Combes et al., 2017; Foster et al., 2021, 2023; Kirschner et al., 2023). For the purpose of this paper, we focus on simple cases for which we can provide tractable implementations. Besides the finite setting where \(\mathcal{M}\) can be enumerated, these are the following linearly parametrized feedback models.

**Example 2.1** (Linear Bandits, Abe and Long (1999)).: The model class is identified with a subset of \(\mathbb{R}^{d}\) and features \(\phi_{\pi}\in\mathbb{R}^{d}\) for each \(\pi\in\Pi\). The reward function is \(r_{f}(\pi)=\langle\phi(\pi),f\rangle\) and the observation distribution is \(M_{f}(\pi)=\mathcal{N}(\langle\phi_{\pi},f\rangle,1)\).

The linear bandit setting can be generalized by separating reward and feedback maps, which leads to the _linear partial monitoring_ framework (Lin et al., 2014; Kirschner et al., 2020). Here we restrict our attention to the special case of _linear bandits with side-observations_(c.f. Kirschner et al., 2023), which, for example, generalizes the classical semi-bandit setting Mannor and Shamir (2011)

**Example 2.2** (Linear Bandits with Side-Observations).: As in the linear bandit setting, we have \(\mathcal{M}\subset\mathbb{R}^{d}\), and features \(\phi_{\pi}\in\mathbb{R}^{d}\) that define the reward functions \(r_{f}(\pi)=\langle\phi_{\pi},f\rangle\). Observation matrices \(M_{\pi}\in\mathbb{R}^{m_{\pi}\times d}\) for each \(\pi\in\Pi\) define \(m_{\pi}\)-dimensional observation distributions \(M_{f}(\pi)=\mathcal{N}(M_{\pi}f,\sigma^{2}\mathbf{1}_{m_{\pi}})\). In addition, we assume that \(\phi_{\pi}\phi_{\pi}^{\top}\preceq M_{\pi}^{\top}M_{\pi}\), which is automatically satisfied if \(\phi_{\pi}^{\top}\) is included in the rows of \(M_{\pi}\), i.e. when the reward is part of the observations.

## 3 Regret Minimization via Saddle-Point Optimization

The goal of the learner is to choose decisions \(\pi\in\Pi\) that achieve a small gap \(\Delta(\pi,f^{*})\) under the true model \(f^{*}\in\mathcal{M}\). Since the true model is unknown, the learner has to collect data that provides statistical evidence to reject models \(g\neq f^{*}\) for which the regret \(\Delta(\pi,g)\) is large. To quantify the information-regret trade-off, we use a divergence \(D(\cdot\|\cdot)\) defined for distributions in \(\mathscr{P}(\mathcal{O})\). For a reference model \(f\), the information (or divergence) function is defined by:

\[I_{f}(\pi,g)=D_{\text{KL}}(M_{g}(\pi)\|M_{f}(\pi))\,,\]

where \(D_{\text{KL}}(\cdot\|\cdot)\) is the KL divergence. Intuitively, \(I_{f}(\pi,g)\) is the rate at which the learner collects statistical information to reject \(g\in\mathcal{M}\) when choosing \(\pi\in\Pi\) and data is generated under the reference model \(f\). Note that \(I_{f}(\pi,f)=0\) for all \(f\in\mathcal{M}\) and \(\pi\in\Pi\). As we will see shortly, the regret-information trade-off can be written precisely as a combination of the gap function, \(\Delta\), and the information function, \(I_{f}\). We remark in passing that other choices such as the Hellinger distance are also possible, and the KL divergence is mostly for concreteness and practical reasons.

To simplify the notation and emphasize the bilinear nature of the saddle point problem that we study, we will view \(\Delta,I_{f}\in\mathbb{R}^{\Pi\times\mathcal{M}}_{+}\) as \(|\Pi|\times|\mathcal{M}|\) matrices (by fixing a canonical ordering on \(\Pi\) and \(\mathcal{M}\)). For vectors \(\mu\in\mathbb{R}^{\Pi}\) and \(\nu\in\mathbb{R}^{\mathcal{M}}\), we will frequently write bilinear forms \(\mu\Delta_{f}\nu\) and \(\mu I_{f}\nu\). This also means that by convention, \(\mu\) will always denote a row vector, while \(\nu\) will always denote a column vector. The standard basis for \(\mathbb{R}^{\Pi}\) and \(\mathbb{R}^{\mathcal{M}}\) is \((e_{\pi})_{\pi\in\Pi}\) and \((e_{g})_{g\in\mathcal{M}}\).

### The Decision-Estimation Coefficient

To motivate our approach, we recall the _decision-estimation coefficient_ (DEC) introduced by Foster et al. (2021, 2023), before introducing the main quantity of interest, the _average-constrained DEC_. First, the _offset decision-estimation coefficient_ (without localization) (Foster et al., 2021) is

\[\text{dec}^{o}_{\lambda}(f)=\min_{\mu\in\mathscr{P}(\Pi)}\max_{g\in\mathcal{M} }\mu\Delta e_{g}-\lambda\mu I_{f}e_{g}\]

The tuning parameter \(\lambda>0\) controls the weight of the information matrix relative to the gaps: Viewing the above as a two-player zero-sum game, we see that increasing \(\lambda\) forces the max-playerto avoid models that differ significantly from \(f\) under the min-player's sampling distribution. The advantage of this formulation is that the information term \(\mu I_{f}e_{g}\) can be telescoped in the analysis, which directly leads to regret bounds in terms of the estimation error (introduced below in Eq. (8)). The disadvantage of the \(\lambda\)-parametrization is that the trade-off parameter is chosen by optimizing the final regret upper bound. This is inconvenient because the optimal choice requires knowledge of the horizon and a bound on \(\max_{f\in\mathcal{M}}\text{dec}^{o}_{\lambda}(f)\). Moreover, any choice informed by the upper bound may be conservative, leading to sub-optimal performance.

The _constrained decision-estimation coefficient_(Foster et al., 2023) is

\[\text{dec}^{c}_{\epsilon}(f)=\min_{\mu\in\mathscr{P}(\Pi)}\max_{g\in\mathcal{M }}\mu\Delta e_{g}\qquad\text{s.t.}\qquad\mu I_{f}e_{g}\leq\epsilon^{2}\] (2)

In this formulation, the max player is restricted to choose models \(g\) that differ from \(f\) at most by \(\epsilon^{2}\) in terms of the observed divergence under the min-player's sampling distribution. Note that because \(e_{\pi}I_{f}e_{f}=0\) for all \(e_{\pi}\in\Pi\), there always exists a feasible solution. For horizon \(n\), the radius can be set to \(\epsilon^{2}\approx\frac{\beta_{\mathcal{M}}}{n}\), where \(\beta_{\mathcal{M}}\) is a model estimation complexity parameter, thereby essentially eliminating the trade-off parameter from the algorithm. However, because of the hard constraint, strong duality of the Lagrangian saddle point problem (for fixed \(\mu\)) fails, and consequently, telescoping the information gain in the analysis is no longer easily possible (or at least, with the existing analysis). To achieve sample complexity \(\text{dec}^{c}_{\epsilon}(f)\), Foster et al. (2023) propose a sophisticated scheme that combines phased exploration with a refinement procedure (E2D\({}^{+}\)).

As the main quantity of interest in the current work, we now introduce the _average-constrained decision-estimation coefficient_, defined as follows:

\[\text{dec}^{ac}_{\epsilon}(f)=\min_{\mu\in\mathscr{P}(\Pi)}\max_{\nu\in \mathscr{P}(\mathcal{M})}\mu\Delta\nu\qquad\text{s.t.}\qquad\mu I_{f}\nu\leq \epsilon^{2}\] (3)

Similar to the \(\text{dec}^{c}_{\epsilon}\), the parameterization of the \(\text{dec}^{ac}_{\epsilon}\) is via the confidence radius \(\epsilon^{2}\), making the choice of the hyperparameter straightforward in many cases (more details in Section 3.2). By convexifying the domain \(\mathscr{P}(\mathcal{M})\) of the max-player, we recover strong duality of the Lagrangian (for fixed \(\mu\)). Thereby, the formulation inherits the ease of choosing the \(\epsilon\)-parameter from the \(\text{dec}^{c}_{\epsilon}\), while, at the same time, admitting a telescoping argument in the analysis and a much simpler algorithm.

Specifically, Sion's theorem implies three equivalent Lagrangian representations for Eq. (3):

\[\text{dec}^{ac}_{\epsilon}(f) =\min_{\mu\in\mathscr{P}(\Pi)}\max_{\nu\in\mathscr{P}(\mathcal{M })}\min_{\lambda\geq 0}\mu\Delta\nu-\lambda(\mu I_{f}\nu-\epsilon^{2})\] (4) \[=\min_{\lambda\geq 0,\mu\in\mathscr{P}(\Pi)}\max_{\nu\in \mathscr{P}(\mathcal{M})}\mu\Delta\nu-\lambda(\mu I_{f}\nu-\epsilon^{2})\] (5) \[=\min_{\lambda\geq 0}\max_{\nu\in\mathscr{P}(\mathcal{M})}\min_{ \mu\in\mathscr{P}(\Pi)}\mu\Delta\nu-\lambda(\mu I_{f}\nu-\epsilon^{2})\] (6)

When fixing the outer problem, strong duality holds for the inner saddle-point problem in each line, however, the joint program in Eq. (5) is not convex-concave. An immediate consequence of relaxing the domain of the max player and Eq. (5) is that

\[\text{dec}^{c}_{\epsilon}(f)\leq\text{dec}^{ac}_{\epsilon}(f)=\min_{\lambda \geq 0}\{\text{dec}^{o}_{\lambda}(f)+\lambda\epsilon^{2}\}\] (7)

The \(\text{dec}^{ac}_{\epsilon}\) can therefore be understood as setting the \(\lambda\) parameter of the \(\text{dec}^{o}_{\lambda}\) optimally for the given confidence radius \(\epsilon^{2}\). On the other hand, the cost paid for relaxing the program is that there exist model classes \(\mathcal{M}\) where the inequality in Eq. (7) is strict, and \(\text{dec}^{ac}_{\epsilon}\) does not lead to a tight characterization of the regret (Foster et al., 2023, Proposition 4.4). The remedy is that under a stronger regularity condition and localization, the two notions are essentially equivalent (Foster et al., 2023, Proposition 4.8).

### Anytime Estimation-To-Decisions (Anytime-E2D)

Estimations-To-Decisions (E2D) is an algorithmic framework that directly leverages the decision-estimation coefficient for choosing a decision in each round. The key idea is to compute a sampling distribution \(\mu_{t}\in\mathscr{P}(\Pi)\) attaining the minimal DEC for an estimate \(\hat{f}_{t}\) of the underlying model, and then define the policy to sample \(\pi_{t}\sim\mu_{t}\). The E2D approach, using the \(\text{dec}^{ac}_{\epsilon}\) formulation, is summarized in Algorithm 1. To compute the estimate \(\hat{f}_{t}\), the E2D algorithm takes an abstract estimation oracle EST as input, that, given the collected data, returns \(\hat{f}_{t}\in\mathcal{M}\). The final guarantee depends on the _estimation error_ (or estimation regret), defined as the sum over divergences of the observation distributions under the estimate \(\hat{f}_{t}\) and the true model \(f^{*}\):

\[\text{Est}_{n}=\mathbb{E}\Bigg{[}\sum_{t=1}^{n}\mu_{t}I_{\hat{f}_{t}}e^{f_{*}} \Bigg{]}\] (8)

Intuitively, the estimation error is well-behaved if \(\hat{f}_{t}\approx f^{*}\), since \(\mu_{t}I_{f^{*}}e_{f^{*}}=0\). Equation (8) is closely related to the _total information gain_ used in the literature on information-directed sampling (Russo and Van Roy, 2014) and kernel bandits (Srinivas et al., 2010).

To bound the estimation error, Foster et al. (2021) rely on _online density estimation_ (also, _online regression_ or _online aggregation_) (Cesa-Bianchi and Lugosi, 2006, Chapter 9). For finite \(\mathcal{M}\), the default approach is the _exponential weights algorithm_ (EWA), which we provide for reference in Appendix A. When using this algorithm, the estimation error always satisfies \(\text{Est}_{n}\leq\log(|\mathcal{M}|)\), see (Cesa-Bianchi and Lugosi, 2006, Proposition 3.1). While these bounds extend to continuous model classes via standard covering arguments, the resulting algorithm is often not tractable without additional assumptions. For linear feedback models (Examples 2.1 and 2.2), one can rely on the more familiar ridge regression estimator, which, we show, achieves bounded estimation regret \(\text{Est}_{n}\leq\mathcal{O}(d\log(n))\). For further discussion, see Appendix A.1.

With this in mind, we state our main result.

**Theorem 1**.: _Let \(\lambda_{t}\geq 0\) be any sequence adapted to the filtration \(\mathcal{F}_{t}\). Then the regret of Anytime-E2D (Algorithm 1) with input sequence \(\lambda_{t}\) satisfies for all \(n\geq 1\):_

\[R_{n}\leq\underset{t\in[n]}{\text{\rm ess}\sup}\Bigg{\{}\frac{\text{\rm dec} ^{ac}_{\epsilon_{t},\lambda_{t}}(\hat{f}_{t})}{\epsilon_{t}^{2}}\Bigg{\}}\Bigg{(} \sum_{t=1}^{n}\epsilon_{t}^{2}+\text{\rm Est}_{n}\Bigg{)}\]

_where we defined \(\text{\rm dec}^{ac}_{\epsilon,\lambda}(f)=\min_{\mu\in\mathscr{P}(\Pi)}\max_{ \nu\in\mathscr{P}(\mathcal{M})}\mu\Delta\nu-\lambda(\mu I_{f}\nu-\epsilon^{2})\)._

As an immediate corollary, we obtain a regret bound for Algorithm 1 where the sampling distribution \(\mu_{t}\) is chosen to optimize \(\text{\rm dec}^{ac}_{\epsilon_{t}}\) for any sequence \(\epsilon_{t}\).

**Corollary 1**.: _The regret of Anytime-E2D (Algorithm 1) with input \(\epsilon_{t}\geq 0\) satisfies for all \(n\geq 1\):_

\[R_{n}\leq\max_{t\in[n],f\in\mathcal{M}}\bigg{\{}\frac{\text{\rm dec}^{ac}_{ \epsilon_{t}}(f)}{\epsilon_{t}^{2}}\Bigg{\}}\Bigg{(}\sum_{t=1}^{n}\epsilon_{t} ^{2}+\text{\rm Est}_{n}\Bigg{)}\]

Importantly, the regret of Algorithm 1 is directly controlled by the worst-case DEC, \(\max_{f\in\mathcal{M}}\text{\rm dec}^{ac}_{\epsilon}(f)\), and the estimation error \(\text{Est}_{n}\). It remains to set \(\epsilon_{t}^{2}\) (respectively \(\lambda_{t}\)) appropriately. For a fixed horizon \(n\), we let \(\epsilon_{t}^{2}=\frac{\text{\rm Est}_{n}}{n}\). With the reasonable assumption that \(\max_{f\in\mathcal{M}}\big{\{}\epsilon^{-2}\text{\rm dec}^{ac}_{\epsilon}(f) \big{\}}\) is non-decreasing in \(\epsilon\), Corollary 1 reads

\[R_{n}\leq 2n\max_{f\in\mathcal{M}}\bigg{\{}\text{\rm dec}^{ac}_{\sqrt{\text{ \rm Est}_{n}/n}}(f)\bigg{\}}\,.\] (9)

This almost matches the lower bound \(R_{n}\geq\Omega(\text{\rm dec}^{c}_{1/\sqrt{n}}(\mathcal{F}))\)3(Foster et al., 2023, Theorem 2.2), up to the estimation error and the beforehand mentioned gap between \(\text{\rm dec}^{c}_{\epsilon}\) and \(\text{\rm dec}^{ac}_{\epsilon}\).

Footnote 3: Here, \(\text{\rm dec}^{c}_{\epsilon}(\mathcal{F})=\max_{f\in\text{\rm o}(\mathcal{M}) }\min_{\mu\in\mathscr{P}(\Pi)}\max_{g\in\mathcal{M}\cup\{f\}}\{\mu\Delta\nu:\mu I _{f}e_{g}\leq\epsilon^{2}\}\).

To get an anytime algorithm with essentially the same scaling as in Eq. (9), we set \(\epsilon_{t}^{2}=\log(|\mathcal{M}|)/t\) for finite model classes, and \(\epsilon_{t}^{2}=\frac{\beta_{\mathcal{M}}}{t}\) if \(\text{Est}_{t}\leq\beta_{\mathcal{M}}\log(t)\) for \(\beta_{\mathcal{M}}>0\). For linear bandits, \(\text{dec}_{\epsilon}^{ac}\leq\epsilon\sqrt{d}\) (see Section 3.3), and \(\text{Est}_{n}\leq d\log(n)\). Choosing \(\epsilon_{t}^{2}=d/t\) recovers the optimal regret bound \(R_{n}\leq\tilde{\mathcal{O}}(d\sqrt{n})\)(Lattimore and Szepesvari, 2020). Alternatively, one can also choose \(\lambda_{t}\) by minimizing an upper bound on \(\max_{t\in[n],f\in\mathcal{M}}\{\text{dec}_{\epsilon_{t},\lambda_{t}}^{ac}(f) /\epsilon_{t}^{2}\}\). For example, in linear bandits, \(\text{dec}_{\epsilon_{t},\lambda}^{ac}\leq\frac{d}{4\lambda}+\lambda\epsilon_{ t}^{2}\) (see Table 1); hence, for \(\epsilon_{t}^{2}=d/t\), we can set \(\lambda_{t}=t/4\). Further discussion and refined upper bound for linear feedback models are in Section 3.3.

Proof of Theorem 1.: Let \(\mu_{t}^{*}\) and \(\nu_{t}^{*}\) be a saddle-point solution to the offset dec,

\[\text{dec}_{\lambda_{t}}^{o}(\hat{f}_{t})=\min_{\mu\in\mathscr{P}(\Pi)}\max_{ \nu\in\mathscr{P}(\mathcal{M})}\mu\Delta\nu-\lambda_{t}\mu I_{\hat{f}_{t}}\nu\]

Note that \(\mu_{t}^{*}\Delta\nu_{t}^{*}-\lambda_{t}\mu_{t}^{*}I_{f}\nu_{t}^{*}\geq\mu_{t} ^{*}\Delta e_{f}-\lambda_{t}\mu_{t}^{*}I_{f}e_{f}\geq 0\), which implies that \(\lambda_{t}\epsilon_{t}^{2}\leq\text{dec}_{\epsilon_{t},\lambda_{t}}^{ac}\). Next,

\[R_{n}=\mathbb{E}\bigg{[}\sum_{t=1}^{n}\mu_{t}\Delta e_{f^{*}} \bigg{]} =\sum_{t=1}^{n}\mathbb{E}\Big{[}\mu_{t}\Delta e_{f^{*}}-\lambda_{ t}(\mu_{t}I_{\hat{f}_{t}}e_{f^{*}}-\epsilon_{t}^{2})+\lambda_{t}(\mu_{t}I_{ \hat{f}_{t}}e_{f^{*}}-\epsilon_{t}^{2})\Big{]}\] \[\leq\sum_{t=1}^{n}\mathbb{E}\bigg{[}\max_{g\in\mathcal{M}}\mu_{t} \Delta_{\hat{f}_{t}}e_{g}-\lambda_{t}(\mu_{t}I_{\hat{f}_{t}}e_{g}-\epsilon_{t} ^{2})+\lambda_{t}(\mu_{t}I_{\hat{f}_{t}}e_{f^{*}}-\epsilon_{t}^{2})\bigg{]}\] \[=\sum_{t=1}^{n}\mathbb{E}\bigg{[}\min_{\mu\in\mathscr{P}(\Pi)}\max _{\nu\in\mathscr{P}(\mathcal{M})}\mu\Delta\nu-\lambda_{t}(\mu I_{\hat{f}_{t}} \nu-\epsilon_{t}^{2})+\lambda_{t}(\mu_{t}I_{\hat{f}_{t}}e_{f^{*}}-\epsilon_{t} ^{2})\bigg{]}\]

So far, we only introduced the saddle point problem by maximizing over \(f^{*}\). The last equality is by our choice of \(\lambda_{t}\) and \(\mu_{t}\), and noting that \(\nu\in\mathscr{P}(\mathcal{M})\) can always be realized as a Dirac. Continuing,

\[R_{n} \leq\sum_{t=1}^{n}\mathbb{E}\Big{[}\text{dec}_{\epsilon_{t}, \lambda_{t}}^{ac}(\hat{f}_{t})+\lambda_{t}(\mu_{t}I_{\hat{f}_{t}}e_{f^{*}}- \epsilon_{t}^{2})\Big{]}\] \[\overset{(i)}{\leq}\sum_{t=1}^{n}\mathbb{E}\bigg{[}\text{dec}_{ \epsilon_{t},\lambda_{t}}^{ac}(\hat{f}_{t})+\frac{1}{\epsilon_{t}^{2}}\text{ dec}_{\epsilon_{t},\lambda_{t}}^{ac}(\hat{f}_{t})\mu_{t}I_{\hat{f}_{t}}e_{f^{*}} \bigg{]}\] \[\overset{(ii)}{\leq}\underset{t\in[n]}{\operatorname{ess}\sup} \max_{f\in\mathcal{M}}\bigg{\{}\frac{1}{\epsilon_{t}^{2}}\text{dec}_{ \epsilon_{t},\lambda_{t}}^{ac}(f)\bigg{\}}\sum_{t=1}^{n}\big{(}\epsilon_{t}^{2} +\mathbb{E}\Big{[}\mu_{t}I_{\hat{f}_{t}}e_{f^{*}}\Big{]}\big{)}\]

We first drop the negative term in \((i)\) and use the beforehand stated fact that \(\lambda_{t}\epsilon_{t}^{2}\leq\text{dec}_{\epsilon_{t},\lambda_{t}}^{ac}( \hat{f}_{t})\). The last step, \((ii)\), is taking the maximum out of the sum. 

### Certifying Upper Bounds

As shown by Corollary 1, the regret of Algorithm 1 scales directly with the \(\text{dec}_{\epsilon}^{ac}\). For analysis purposes, it is however useful to compute upper bounds on the \(\text{dec}_{\epsilon}^{ac}\) to verify the scaling w.r.t. parameters of interest. Via the equivalence Eq. (7), bounds on the \(\text{dec}_{\lambda}^{o}\) directly translate to the \(\text{dec}_{\epsilon}^{ac}\) (see Table 1). For a detailed discussion of upper bounds in various models, we refer to Foster et al. (2021). Below, we highlight three connections that are directly facilitated by the \(\text{dec}_{\epsilon}^{ac}\).

To this end, we first introduce a variant of the \(\text{dec}^{ac}_{\epsilon}\) where the gap function depends on \(f\):

\[\text{dec}^{ac,f}_{\epsilon}(f)=\min_{\mu\in\mathscr{P}(\Pi)}\max_{\nu\in \mathscr{P}(\mathcal{M})}\mu\Delta_{f}\nu\qquad\text{s.t.}\qquad\mu I_{f}\nu \leq\epsilon^{2}\,,\] (10)

where \(\Delta_{f}(\pi,g)=r_{g}(\pi_{g}^{*})-r_{f}(\pi)\). We remark that for distributions \(\nu\in\mathscr{P}(\mathcal{M})\) and \(\mu\in\mathscr{P}(\Pi)\), the gap \(\Delta_{f}\) can be decoupled, \(\mu\Delta_{f}\nu=\delta_{f}\nu+\mu\Delta_{f}e_{f}\), where we defined \(\delta_{f}(g)=r_{g}(\pi_{g}^{*})-r_{f}(\pi_{f}^{*})\). The following assumption implies that the observations for a decision \(\pi\) are at least as informative as observing the rewards.

**Assumption 1** (Reward Data Processing).: _The rewards and information matrices are related via the following data-processing inequality that holds for any \(\mu\in\mathscr{P}(\Pi)\):_

\[|\mathbb{E}_{\pi\sim\mu}[r_{f}(\pi)-r_{g}(\pi)]|\leq\sqrt{\mathbb{E}_{\pi\sim \mu}[D(M_{f}(\pi)\|M_{g}(\pi))]}\]

The next lemma shows that under Assumption 1, \(\text{dec}^{ac}_{\epsilon}(f)\) and \(\text{dec}^{ac,f}_{\epsilon}(f)\) are essentially equivalent, at least for the typical worst-case bounds where \(\max_{f\in\mathcal{M}}\text{dec}^{ac}_{\epsilon}(f)\geq\Omega(\epsilon)\).

**Lemma 1**.: _If Assumption 1 holds, then_

\[\text{dec}^{ac,f}_{\epsilon}(f)-\epsilon\leq\text{dec}^{ac}_{\epsilon}(f) \leq\text{dec}^{ac,f}_{\epsilon}(f)+\epsilon\]

The proof is in Appendix C.1. We remark that Algorithm 1 where the sampling distribution is computed for \(\text{dec}^{ac,f}_{\epsilon}(\hat{f}_{t})\) and \(\Delta_{f}\) achieves a bound analogous to Theorem 1, as long as Assumption 1 holds. For details see Lemma 8 in Appendix C.

Upper Bounds via DecouplingFirst, we introduce the _information ratio_,

\[\Psi_{f}(\mu,\nu)=\frac{(\mu\Delta_{f}\nu)^{2}}{\mu I_{f}\nu}\]

The definition is closely related to the Bayesian information ratio (Russo and Van Roy, 2016), where \(\nu\) takes the role of a prior over \(\mathcal{M}\). The Thompson sampling distribution is \(\mu_{\nu}^{\text{TS}}=\sum_{h\in\mathcal{M}}\nu_{h}e_{\pi_{h}^{*}}\). The decoupling coefficient, \(\text{dc}(f)\), (Zhang, 2022, Definition 1) is defined as the smallest number \(K\geq 0\), such that for all distributions \(\nu\in\mathscr{P}(\mathcal{M})\),

\[\mu_{\nu}^{\text{TS}}\Delta_{f}\nu\leq\inf_{\eta\geq 0}\bigg{\{}\eta\sum_{g,h \in\mathcal{M}}\nu_{g}\nu_{h}e_{\pi_{h}^{*}}(r_{g}-r_{f})^{2}+\frac{K}{4\eta} \bigg{\}}=\sqrt{K\sum_{g,h\in\mathcal{M}}\nu_{g}\nu_{h}e_{\pi_{h}^{*}}(r_{g}-r _{f})^{2}}\] (11)

The next lemma provides upper bounds on the \(\text{dec}^{ac}_{\epsilon}(f)\) in terms of the information ratio, which is further upper-bounded by the decoupling coefficient.

**Lemma 2**.: _With \(\Psi(f)=\max_{\nu\in\mathcal{M}}\min_{\mu\in\mathscr{P}(\Pi)}\Psi_{f}(\mu,\nu)\) and Assumption 1 satisfied, we have_

\[\text{dec}^{ac,f}_{\epsilon}(f)\leq\epsilon\sqrt{\Psi(f)}\leq\epsilon\sqrt{ \text{dc}(f)}\]

The proof follows directly using the AM-GM inequality, see Appendix C.2. By (Zhang, 2022, Lemma 2), this further implies \(\text{dec}^{ac,f}_{\epsilon}\leq\epsilon\sqrt{d}\). An analogous result for the generalized information ratio (Lattimore and Gyorgy, 2021) that recovers rates \(\epsilon^{\rho}\) for \(\rho\leq 1\) is given in Appendix C.4.

PAC to RegretAnother useful way to upper bound the \(\text{dec}^{ac,f}_{\epsilon}\) is via an analogous definition for the PAC setting (c.f. Eq. (10), Foster et al., 2023):

\[\text{pac-dec}^{ac,f}_{\epsilon}(f)=\min_{\mu\in\mathscr{P}(\Pi)}\max_{\nu\in \mathcal{M}}\delta_{f}\nu\qquad\text{s.t.}\qquad\mu I_{f}\nu\leq\epsilon^{2}\] (12)

**Lemma 3**.: _Under Assumption 1,_

\[\text{dec}^{ac,f}_{\epsilon}(f)\leq\min_{p\in[0,1]}\Bigl{\{}\text{pac-dec}^{ ac,f}_{\epsilon p^{-1/2}}(f)+p\Delta_{\max}\Bigr{\}}\]

The proof is given in Appendix B.1. Lemma 3 combined with Theorem 1 leads to \(\mathcal{O}(n^{2/3})\) upper bounds on the regret that are reminiscent of so-called globally observable games in linear partial monitoring (Kirschner et al., 2023).

Application to Linear Feedback ModelsTo illustrate the techniques introduced, we compute a regret bound for Algorithm 1 for linear bandits with side-observations (Examples 2.1 and 2.2).

**Lemma 4**.: _For linear bandits with side-observations and divergence \(I_{f}(\pi,g)=\|M_{\pi}(g-f)\|^{2}\),_

\[\textnormal{pac-dec}_{\epsilon}^{ac,f}(f)\leq\min_{\mu\in\mathscr{P}(\Pi)} \max_{b\in\Pi}\epsilon\|\phi_{b}\|_{V(\mu)^{-1}}\leq\epsilon\sqrt{d}\]

_where \(V(\mu)=\sum_{\pi\in\Pi}\mu_{\pi}M_{\pi}M_{\pi}^{\top}\). Moreover, denoting \(\Omega=\min_{\mu\in\mathscr{P}(\Pi)}\max_{b\in\Pi}\|\phi_{b}\|_{V(\mu)^{-1}}\),_

\[\textnormal{dec}_{\epsilon}^{ac,f}(f)\leq\min\Bigl{(}\epsilon\sqrt{\Psi(f)},2\epsilon^{2/3}\Omega^{1/3}\Delta_{\max}^{1/3}\Bigr{)}\]

The proof is given in Appendix B.2. While in the worst-case for linear bandits, there is no improvement over the standard \(\mathcal{O}(d\sqrt{n})\) without further refinement or specification of the upper bounds, in the case of linear side-observations there is an improvement whenever \(\Omega\leq\max_{f\in\mathcal{M}}\Psi(f)\). To exemplify the improvement, consider a semi-bandit with a "revealing" action \(\hat{\pi}\), e.g. \(M_{\hat{\pi}}=\mathbf{1}_{d}\). Here, the regret bound improves to \(R_{n}\leq\min\{d\sqrt{n},d^{1/3}n^{2/3}\}\), since then \(\textnormal{pac-dec}_{\epsilon}^{ac,f}(f)\leq\epsilon\). The corresponding improvement in the regime \(n\leq d^{4}\) might seem modest, but is relevant in high-dimensional and non-parametric models. Moreover, in (deep) reinforcement learning, high-dimensional models are commonly used and the learner obtains side information in the form of state observations. Therefore, it is plausible that the \(n^{2/3}\) rate is dominant even for a moderate horizon. Exploring this effect in reinforcement learning is therefore an important direction for future work.

Notably, this improvement is _not_ observed by upper confidence bound algorithms and Thompson sampling, because both approaches discard informative but suboptimal actions early on [c.f. Lattimore and Szepesvari, 2017], including the action \(\hat{\pi}\) in the example above. E2D for a constant offset parameter \(\lambda>0\), in principle, attains the better rate, but only if one pre-commits to a fixed horizon. Lastly, we note that a similar effect was observed for information-directed sampling in sparse high-dimensional linear bandits [Hao et al., 2020].

### Computational Aspects

For finite model classes, Algorithm 1 can be readily implemented. Since almost no structure is imposed on the gap and information matrices of size \(|\Pi|\times|\mathcal{M}|\), avoiding scaling with \(|\Pi|\cdot|\mathcal{M}|\) seems hardly possible without introducing additional assumptions. Even in the finite case, solving Eq. (3) is not immediate because the corresponding Lagrangian is not convex-concave. A practical approach is to solve the inner saddle point for Eq. (5) as a function of \(\lambda\). Strong duality holds for the inner problem, and one can obtain a solution efficiently by solving the corresponding linear program using standard solvers. It then remains to optimize over \(\lambda\geq 0\). This can be done, for example, via a grid search over the range \([0,\max_{f\in\mathcal{M}}\epsilon^{-2}\textnormal{dec}_{\epsilon}^{ac}(f)]\).

In the linear setting, the above is not satisfactory because most commonly \(\mathcal{M}\) is identified with parameters in \(\mathbb{R}^{d}\). As noted before, ridge regression can be used instead of online aggregation while preserving the optimal scaling of the estimation error (see Appendix A.1). The next lemma further shows that the saddle point problem Eq. (3) can be rewritten to only scale with the size of the decision set \(|\Pi|\).

**Lemma 5**.: _Consider linear bandits with side observations, \(\mathcal{M}=\mathbb{R}^{d}\) and quadratic divergence, \(I_{f}(\pi,g)=\|M_{\pi}(g-f)\|^{2}\), and denote \(\phi_{\mu}=\sum_{\pi\in\Pi}\mu_{\pi}\phi_{\pi}\) and \(V(\mu)=\sum_{\pi\in\Pi}\mu_{\pi}M_{\pi}^{\top}M_{\pi}\). Then_

\[\textnormal{dec}_{\epsilon}^{ac,f}(\hat{f}_{t})=\min_{\lambda\geq 0}\min_{ \mu\in\mathscr{P}(\Pi)}\max_{b\in\Pi}(\phi_{b}-\phi_{\mu},\hat{f}_{t})+\frac {1}{4\lambda}\|\phi_{b}\|_{V(\mu)^{-1}}^{2}+\lambda\epsilon^{2}\]

_Moreover, the objective is convex in \(\mu\in\mathscr{P}(\Pi)\)._

The proof is a straightforward calculation provided in Appendix C.5. Note that the saddle point expression is analogous to Eq. (5), and in fact, one can linearize the inner maximization over \(\mathscr{P}(\Pi)\), such that the inner saddle point becomes convex-concave. This leads to expressions equivalent to Eqs. (4) and (6), albeit the objective is no longer linear in \(\mu\in\mathscr{P}(\Pi)\). We use Lemma 5 to employ the same strategy as before: As a function of \(\lambda\geq 0\), solve the inner problem of the expression in Lemma 5, for example, as a convex program with \(|\Pi|\) variables and \(|\Pi|\) constraints (Appendix D). Then all that remains is to solve a one-dimensional optimization problem over \(\lambda\in[0,\max_{f\in\mathcal{M}}\epsilon^{-2}\textnormal{dec}_{\epsilon}^ {ac}(f)]\). We demonstrate this approach in Appendix E to showcase the performance of E2D on simple examples.

Conclusion

We introduced Anytime-E2D, an algorithm based on the estimation-to-decisions framework for sequential decision-making with structured observations. The algorithm optimizes the _average-constrained_ decision-making coefficient, which can be understood as a reparametrization of the corresponding offset version. The reparametrization facilitates an elegant anytime analysis and makes setting all remaining hyperparameters immediate. We demonstrate the improvement with a novel bound for linear bandits with side-observations, that is not attained by previous approaches. Lastly, we discuss how the algorithm can be implemented for finite and linear model classes. Nevertheless, much remains to be done. For example, one can expect the reference model to change very little from round to round, and therefore, it seems wasteful to solve Eq. (3) from scratch repetitively. Preferable instead would be an incremental scheme that iteratively computes updates to the sampling distribution.

## Acknowledgments and Disclosure of Funding

Johannes Kirschner gratefully acknowledges funding from the SNSF Early Postdoc.Mobility fellowship P2EZP2_199781.

## References

* Abbasi-Yadkori et al. (2011) Y. Abbasi-Yadkori, D. Pal, and C. Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* Abe and Long (1999) N. Abe and P. M. Long. Associative reinforcement learning using linear probabilistic concepts. In _Proceedings of the Sixteenth International Conference on Machine Learning_, ICML '99, pages 3-11, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. ISBN 1-55860-612-2.
* Agarwal and Zhang (2022) A. Agarwal and T. Zhang. Model-based rl with optimistic posterior sampling: Structural conditions and sample complexity. _arXiv preprint arXiv:2206.07659_, 2022.
* Azar et al. (2017) M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272. PMLR, 2017.
* Bubeck et al. (2015) S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization:\(\backslash\)sqrt regret in one dimension. In _Conference on Learning Theory_, pages 266-278. PMLR, 2015.
* Cesa-Bianchi and Lugosi (2006) N. Cesa-Bianchi and G. Lugosi. _Prediction, learning, and games_. Cambridge university press, 2006.
* Chen et al. (2022) F. Chen, S. Mei, and Y. Bai. Unified algorithms for rl with decision-estimation coefficients: No-regret, pac, and reward-free learning. _arXiv preprint arXiv:2209.11745_, 2022.
* Combes et al. (2015) R. Combes, S. Magureanu, A. Proutiere, and C. Laroche. Learning to rank: Regret lower bounds and efficient algorithms. In _Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems_, pages 231-244. ACM, 2015. ISBN 978-1-4503-3486-0.
* Combes et al. (2017) R. Combes, S. Magureanu, and A. Proutiere. Minimal exploration in structured stochastic bandits. _Advances in Neural Information Processing Systems_, 30, 2017.
* Dann et al. (2021) C. Dann, M. Mohri, T. Zhang, and J. Zimmert. A provably efficient model-free posterior sampling method for episodic reinforcement learning. _Advances in Neural Information Processing Systems_, 34:12040-12051, 2021.
* Degenne et al. (2020a) R. Degenne, P. Menard, X. Shang, and M. Valko. Gamification of pure exploration for linear bandits. In _International Conference on Machine Learning_, pages 2432-2442. PMLR, 2020a.
* Degenne et al. (2020b) R. Degenne, H. Shao, and W. Koolen. Structure adaptive algorithms for stochastic bandits. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 2443-2452. PMLR, 13-18 Jul 2020b. URL https://proceedings.mlr.press/v119/degenne20b.html.
* Dann et al. (2017)A. V. den Boer. Dynamic pricing and learning: Historical origins, current research, and new directions. _Surveys in Operations Research and Management Science_, 20(1):1-18, 2015.
* Dong and Ma (2022) K. Dong and T. Ma. Asymptotic instance-optimal algorithms for interactive decision making. _arXiv preprint arXiv:2206.02326_, 2022.
* Du et al. (2021) S. Du, S. Kakade, J. Lee, S. Lovett, G. Mahajan, W. Sun, and R. Wang. Bilinear classes: A structural framework for provable generalization in rl. In _International Conference on Machine Learning_, pages 2826-2836. PMLR, 2021.
* Dunn and Harshbarger (1978) J. C. Dunn and S. Harshbarger. Conditional gradient algorithms with open loop step size rules. _Journal of Mathematical Analysis and Applications_, 62(2):432-444, 1978.
* Foster et al. (2021) D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* Foster et al. (2022a) D. J. Foster, N. Golowich, J. Qian, A. Rakhlin, and A. Sekhari. A note on model-free reinforcement learning with the decision-estimation coefficient. _arXiv preprint arXiv:2211.14250_, 2022a.
* Foster et al. (2022b) D. J. Foster, A. Rakhlin, A. Sekhari, and K. Sridharan. On the complexity of adversarial decision making. _arXiv preprint arXiv:2206.13063_, 2022b.
* Foster et al. (2023) D. J. Foster, N. Golowich, and Y. Han. Tight guarantees for interactive decision making with the decision-estimation coefficient. _arXiv preprint arXiv:2301.08215_, 2023.
* Frank and Wolfe (1956) M. Frank and P. Wolfe. An algorithm for quadratic programming. _Naval research logistics quarterly_, 3(1-2):95-110, 1956.
* Garcelon et al. (2020) E. Garcelon, B. Roziere, L. Meunier, J. Tarbouriech, O. Teytaud, A. Lazaric, and M. Pirotta. Adversarial attacks on linear contextual bandits. _Advances in Neural Information Processing Systems_, 33:14362-14373, 2020.
* Gyorgy et al. (2013) A. Gyorgy, D. Pal, and C. Szepesvari. Online learning: Algorithms for big data. _Lecture Notes_, 2013.
* Hao et al. (2020) B. Hao, T. Lattimore, and M. Wang. High-dimensional sparse linear bandits. _Advances in Neural Information Processing Systems_, 33:10753-10763, 2020.
* Jaggi (2013) M. Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In _International conference on machine learning_, pages 427-435. PMLR, 2013.
* Jin et al. (2018) C. Jin, Z. Allen-Zhu, S. Bubeck, and M. I. Jordan. Is q-learning provably efficient? _Advances in neural information processing systems_, 31, 2018.
* Kirschner and Krause (2018) J. Kirschner and A. Krause. Information directed sampling and bandits with heteroscedastic noise. In _Conference On Learning Theory_, pages 358-384. PMLR, 2018.
* Kirschner and Krause (2021) J. Kirschner and A. Krause. Bias-robust bayesian optimization via dueling bandits. In _International Conference on Machine Learning_, pages 5595-5605. PMLR, 2021.
* Kirschner et al. (2020a) J. Kirschner, I. Bogunovic, S. Jegelka, and A. Krause. Distributionally Robust Bayesian Optimization. In _Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)_, August 2020a. URL http://proceedings.mlr.press/v108/kirschner20a/kirschner20a.pdf.
* Kirschner et al. (2020b) J. Kirschner, T. Lattimore, and A. Krause. Information directed sampling for linear partial monitoring. In _Conference on Learning Theory_, pages 2328-2369. PMLR, 2020b.
* Kirschner et al. (2021) J. Kirschner, T. Lattimore, C. Vernade, and C. Szepesvari. Asymptotically optimal information-directed sampling. In _Conference on Learning Theory_, pages 2777-2821. PMLR, 2021.
* Kirschner et al. (2021) J. Kirschner, T. Lattimore, and A. Krause. Linear partial monitoring for sequential decision making: Algorithms, regret bounds and applications. _Journal of Machine Learning Research_, August 2023.
* Lattimore and Gyorgy (2021) T. Lattimore and A. Gyorgy. Mirror descent and the information ratio. In _Conference on Learning Theory_, pages 2965-2992. PMLR, 2021.
* Liu et al. (2019)T. Lattimore and C. Szepesvari. The end of optimism? an asymptotic analysis of finite-armed linear bandits. In _Artificial Intelligence and Statistics_, pages 728-737. PMLR, 2017.
* Lattimore and Szepesvari (2019) T. Lattimore and C. Szepesvari. An information-theoretic approach to minimax regret in partial monitoring. In _Conference on Learning Theory_, pages 2111-2139. PMLR, 2019.
* Lattimore and Szepesvari (2020a) T. Lattimore and C. Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020a.
* Lattimore and Szepesvari (2020b) T. Lattimore and C. Szepesvari. Exploration by optimisation in partial monitoring. In _Conference on Learning Theory_, pages 2488-2515. PMLR, 2020b.
* Lattimore et al. (2018) T. Lattimore, B. Kveton, S. Li, and C. Szepesvari. Toprank: A practical algorithm for online stochastic ranking. In _Advances in Neural Information Processing Systems_, pages 3949-3958. Curran Associates, Inc., 2018.
* Lin et al. (2014) T. Lin, B. Abrahao, R. Kleinberg, J. Lui, and W. Chen. Combinatorial partial monitoring game with linear feedback and its applications. In _International Conference on Machine Learning_, pages 901-909, 2014.
* Mannor and Shamir (2011) S. Mannor and O. Shamir. From bandits to experts: On the value of side-observations. _Advances in Neural Information Processing Systems_, 24, 2011.
* Osband et al. (2016) I. Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via bootstrapped dqn. _Advances in neural information processing systems_, 29, 2016.
* Radlinski et al. (2008) F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse rankings with multi-armed bandits. In _Proceedings of the 25th International Conference on Machine Learning_, pages 784-791. ACM, 2008.
* Russo and Van Roy (2014) D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. _Advances in Neural Information Processing Systems_, 27, 2014.
* Russo and Van Roy (2016) D. Russo and B. Van Roy. An information-theoretic analysis of thompson sampling. _The Journal of Machine Learning Research_, 17(1):2442-2471, 2016.
* Rustichini (1999) A. Rustichini. Minimizing regret: The general case. _Games and Economic Behavior_, 29(1-2):224-243, 1999.
* Shalev-Shwartz et al. (2012) S. Shalev-Shwartz et al. Online learning and online convex optimization. _Foundations and Trends(r) in Machine Learning_, 4(2):107-194, 2012.
* Srinivas et al. (2010) N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In _Proc. International Conference on Machine Learning (ICML)_, 2010.
* Thompson (1933) W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 25(3-4):285-294, 1933.
* Yue and Joachims (2009) Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In _Proceedings of the 26th International Conference on Machine Learning_, pages 1201-1208. ACM, 2009.
* Zanette et al. (2020) A. Zanette, A. Lazaric, M. Kochenderfer, and E. Brunskill. Learning near optimal policies with low inherent bellman error. In _International Conference on Machine Learning_, pages 10978-10989. PMLR, 2020.
* Zhang (2022) T. Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. _SIAM Journal on Mathematics of Data Science_, 4(2):834-857, 2022.
* Zhou et al. (2021) D. Zhou, Q. Gu, and C. Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_, pages 4532-4576. PMLR, 2021.

## Appendix A Online Density Estimation

For any \(f\in\mathcal{M}\) and \(\pi\in\Pi\), we denote by \(p(\cdot|\pi,f)\) the the density function of the observation distribution \(M_{f}(\pi)\) w.r.t. a reference measure over the observation space \(\mathcal{O}\). Consider a finite model class \(\mathcal{M}\) and the KL divergence,

\[e_{\pi}I_{f}e_{g}=\mathbb{E}_{y\sim M_{g}(\pi)}\bigg{[}\log\bigg{(}\frac{p(y| \pi,g)}{p(y|\pi,f)}\bigg{)}\bigg{]}\] (13)

In this case, the estimation error can be written as follows:

\[\text{Est}_{n}=\mathbb{E}\bigg{[}\sum_{t=1}^{n}e_{\pi_{t}}I_{\hat {f}_{t}}e_{f}\bigg{]} =\mathbb{E}\bigg{[}\sum_{t=1}^{n}\log(p(y_{t}|\pi_{t},f^{*})/p(y_ {t}|\pi_{t},\hat{f}_{t})\bigg{]}\] \[=\mathbb{E}\bigg{[}\sum_{t=1}^{n}\log\Bigg{(}\frac{1}{p(y_{t}|\pi _{t},\hat{f}_{t})}\bigg{)}-\sum_{t=1}^{n}\log\bigg{(}\frac{1}{p(y_{t}|\pi_{t}, f^{*})}\bigg{)}\bigg{]}\]

The last line can be understood as the _estimation regret_ of the estimates \(\hat{f}_{1},\dots,\hat{f}_{n}\) under the logarithmic loss. A classical approach to control this term is the _exponential weights algorithm_ (EWA) given in Algorithm 2. For the EWA algorithm, we have the following bound.

**Lemma 6** (EWA for Online Density Estimation).: _For any data stream \(\{y_{1},\pi_{1},\dots,y_{n},\pi_{n}\}\) the predictions \(\hat{f}_{1},\dots\hat{f}_{n}\) obtained via Algorithm 2 with \(\eta=1\) satisfy_

\[\text{Est}_{n}\leq\mathbb{E}\bigg{[}\sum_{t=1}^{n}\log\bigg{(}\frac{1}{p(y_{t }|\pi_{t},\hat{f}_{t})}\bigg{)}-\inf_{g\in\mathcal{M}}\sum_{t=1}^{n}\log\bigg{(} \frac{1}{p(y_{t}|\pi_{t},g)}\bigg{)}\bigg{]}\leq\log(|\mathcal{M}|)\] (14)

For a proof, see (Cesa-Bianchi and Lugosi, 2006, Proposition 3.1).

### Bounding the Estimation Error of Projected Regularized Least-Squares

In this section, we consider the linear model from Example 2.2. We denote by \(\|\cdot\|\) the Euclidean norm. For simplicity, the observation maps \(M_{\pi}\in\mathbb{R}^{m\times d}\) are assumed to have the same output dimension \(m\in\mathbb{N}\). The observation distribution is such that \(y_{t}=M_{\pi_{t}}f^{*}+\xi_{t}\), where \(\xi\in\mathbb{R}^{m}\) is random noise such that \(\mathbb{E}_{t}[\xi]=0\) and \(\mathbb{E}_{t}\big{[}\|\xi\|^{2}\big{]}\leq\sigma^{2}\). Here, \(\mathbb{E}_{t}[\cdot]=\mathbb{E}[\cdot|\pi_{1},y_{1},\dots,\pi_{t-1},y_{t-1}, \pi_{t}]\) is the conditional observation in round \(t\) including the decision \(\pi_{t}\) chosen in round \(t\).

We will use the quadratic divergence4, \(e_{\pi}I_{f}e_{g}=\frac{1}{2}\|M_{\pi}(g-f)\|^{2}\) This choice corresponds to the Gaussian KL, but we do not require that the noise distribution is Gaussian is the following. In the linear bandit model, this choice reduces to \(e_{\pi}I_{f}e_{g}=\frac{1}{2}\langle\phi_{\pi},g-f\rangle^{2}\).

Footnote 4: We added a factor of \(\frac{1}{2}\) for convenience.

Let \(K\subset\mathbb{R}^{d}\) be a closed convex set. Our goal is to control the estimation regret for the projected regularized least-squares estimator,

\[\hat{f}_{t}=\operatorname*{arg\,min}_{f\in K}\sum_{s=1}^{t-1}\|M_{\pi_{s}}f-y _{s}\|^{2}+\|f\|_{V_{0}}^{2}=\text{Proj}_{V_{t}}\bigg{(}V_{t}^{-1}\sum_{s=1}^ {t-1}M_{\pi_{s}}^{\top}y_{s}\bigg{)}\] (15)

where \(V_{0}\) is a positive definite matrix, \(V_{t}=\sum_{s=1}^{t-1}M_{\pi_{s}}^{\top}M_{\pi_{s}}+V_{0}\) and \(\text{Proj}_{V_{t}}(\cdot)\) is the orthogonal projection w.r.t. the \(\|\cdot\|_{V_{t}}\) norm. For \(K=\mathbb{R}^{d}\) and \(V_{0}=\eta\mathbf{1}_{d}\), this recovers the standard ridgeregression. The projection is necessary to bound the magnitude of the squared loss, and the result will depend on an almost-surely bound on the 'observed' diameter,

\[\max_{f,g\in K}\max_{\pi\in\Pi}\|M_{\pi}(f-g)\|\leq B\]

Recall that our goal is to bound the estimation error,

\[\text{Est}_{n}=\mathbb{E}\!\left[\sum_{t=1}^{n}e_{\pi_{t}}I_{f_{t}}e_{f^{*}} \right]=\mathbb{E}\!\left[\sum_{t=1}^{n}\tfrac{1}{2}\|M_{\pi_{t}}(f^{*}-\hat{f} _{t})\|^{2}\right]\] (16)

We remark that one can get the following naive bound by applying Cauchy-Schwarz:

\[\sum_{t=1}^{n}\|M_{\pi_{t}}(f^{*}-\hat{f}_{t})\|^{2}\leq\sum_{t=1}^{n}\|M_{\pi_ {t}}\|_{V_{t}^{-1}}^{2}\|f^{*}-\hat{f}_{t}\|_{V_{t}}^{2}\leq\mathcal{O}(d^{2} \log(n)^{2})\] (17)

The last inequality follows from the elliptic potential lemma and standard concentration inequalities (Lattimore and Szepesvari, 2020a, Lemma 19.4 and Theorem 20.5). However, this will lead to an additional \(d\)-factor in the regret that can be avoided, as we see next.

For \(K=\mathbb{R}^{d}\), one-dimensional observations and noise bounded in the range \([-\bar{B},\bar{B}]\), one can also directly apply (Cesa-Bianchi and Lugosi, 2006, Theorem 11.7) to get \(\text{Est}_{n}\leq\mathcal{O}(\bar{B}^{2}d\log(n))\), thereby improving the naive bound by a factor \(d\log(n)\). This result is obtained in a more general setting, where no assumptions, other than boundedness, are placed on the observation sequence \(y_{1},\ldots,y_{n}\). Here we refine and generalize this result in two directions: First, we allow for the more general feedback model in with multi-dimensional observations (Example 2.2). Second, we directly exploit the stochastic observation model to obtain a stronger result that does not require the observation noise to be bounded.

**Theorem 2**.: _Consider the linear observation setting with additive noise and quadratic divergence \(e_{\pi}I_{f}e_{g}=\frac{1}{2}\|M_{\pi}(g-f)\|^{2}\), as described at the beginning of this section. Assume that \(\max_{f,g\in\mathcal{M},\pi\in\Pi}\|M_{\pi}(f-g)\|\leq B\) and \(\mathbb{E}\!\left[\|\xi_{t}\|^{2}\right]\leq\sigma^{2}\). Then_

\[\text{Est}_{n}\leq(\sigma^{2}+B^{2})\mathbb{E}\!\left[\log\!\left(\frac{\det V _{n}}{\det V_{0}}\right)\right]\]

_If in addition \(\|M_{\pi}\|\leq L\) and \(V_{0}=\eta\mathbf{1}_{d}\), then \(\text{Est}_{n}\leq(\sigma^{2}+B^{2})\log\left(1+\frac{nL^{2}}{\eta d}\right)\)._

**Remark 1**.: _Note that by the (Lattimore and Szepesvari, 2020a, Theorem 19.4), \(\log\!\left(\frac{\det V_{n}}{\det V_{0}}\right)\) can further be upper bounded by \(d\log\left(\frac{\text{mecv}_{0}+nL^{2}}{d\det(V_{0})^{1/d}}\right)\), which effectively results the desired bound._

Proof.: The proof adapts (Gyorgy et al., 2013, Theorem 19.8) to multi-dimensional observations and takes advantage of the stochastic loss function by taking the expectation.

First, define \(l_{t}(f)=\frac{1}{2}\|M_{\pi_{t}}f-y_{t}\|^{2}\). Then, using that \(\mathbb{E}_{t}[y_{t}]=M_{\pi_{t}}f^{*}\),

\[\text{Est}_{n}=\mathbb{E}\!\left[\sum_{t=1}^{n}\tfrac{1}{2}\|M_{ \pi_{t}}(f^{*}-\hat{f}_{t})\|^{2}\right] =\mathbb{E}\!\left[\sum_{t=1}^{n}\tfrac{1}{2}\|M_{\pi_{t}}\hat{f }_{t}-y_{t}\|^{2}-\tfrac{1}{2}\|M_{\pi_{t}}f^{*}-y_{t}\|^{2}\right]\] \[=\mathbb{E}\!\left[\sum_{t=1}^{n}l_{t}(\hat{f}_{t})-l_{t}(f^{*})\right]\]

Further, by directly generalizing (Gyorgy et al., 2013, Lemma 19.7), we have that

\[l_{t}(\hat{f}_{t+1})-l_{t}(\hat{f}_{t})\leq\nabla l_{t}(\hat{f}_{t})V_{t}^{-1 }\nabla l_{t}(\hat{f}_{t})=(M_{\pi_{t}}\hat{f}_{t}-y_{t})^{\top}M_{\pi_{t}}^{ \top}V_{t}^{-1}M_{\pi_{t}}(M_{\pi_{t}}\hat{f}_{t}-y_{t})\] (18)We now start upper bounding the estimation error,

\[\text{Est}_{n} \stackrel{{(i)}}{{\leq}}\|f^{*}\|^{2}+\mathbb{E}\Bigg{[} \sum_{t=1}^{n}\big{(}l_{t}(w_{t})-l_{t}(w_{t+1})\big{)}\Bigg{]}\] \[\stackrel{{(ii)}}{{\leq}}\|f^{*}\|^{2}+\mathbb{E} \Bigg{[}\sum_{t=1}^{n}\big{(}\xi_{t}+M_{\pi_{t}}(f^{*}-\hat{f}_{t-1})\big{)}M_{ \pi_{t}}V_{t}^{-1}M_{\pi_{t}}\big{(}\xi_{t}+M_{\pi_{t}}(f^{*}-\hat{f}_{t-1}) \big{)}\Bigg{]}\] \[\stackrel{{(iii)}}{{=}}\|f^{*}\|^{2}+\mathbb{E} \Bigg{[}\sum_{t=1}^{n}\xi_{t}M_{\pi_{t}}V_{t}^{-1}M_{\pi_{t}}\xi_{t}\Bigg{)}+ \mathbb{E}\Bigg{[}\sum_{t=1}^{n}\bar{x}_{t}M_{\pi_{t}}V_{t}^{-1}M_{\pi_{t}} \bar{x}_{t}\Big{)}\Bigg{]}\] \[\stackrel{{(iv)}}{{\leq}}\|f^{*}\|^{2}+\mathbb{E} \Bigg{[}\sum_{t=1}^{n}\lambda_{\max}(M_{\pi_{t}}V_{t}^{-1}M_{\pi_{t}})\|\xi_{t }\|^{2}\Bigg{]}+\mathbb{E}\Bigg{[}\sum_{t=1}^{n}\lambda_{\max}(M_{\pi_{t}}V_{t }^{-1}M_{\pi_{t}})\|\bar{x}_{t}\|^{2})\Bigg{]}\] \[\stackrel{{(v)}}{{\leq}}\|f^{*}\|^{2}+(\sigma^{2}+B ^{2})\mathbb{E}\Bigg{[}\sum_{t=1}^{n}\lambda_{\max}(M_{\pi_{t}}V_{t}^{-1}M_{\pi _{t}})\Bigg{]}\] (19)

The inequality \((i)\) follows from [1, Lemma 2.3]. For \((ii)\) we used Eq. (18). For \((iii)\) we used that \(\mathbb{E}_{t}[\xi_{t}]=0\). In \((iv)\), we introduce the maximum eigenvalue \(\lambda_{\max}(A)\) for \(A\in\mathbb{R}^{m\times m}\) and denote \(\bar{x}_{t}=M_{\pi_{t}}(f^{*}-f_{t-1})\). Lastly, in \((v)\) we used that \(\|\bar{x}_{t}\|^{2}\leq B\) and \(\mathbb{E}_{t}\big{[}\|\xi_{t}\|^{2}\big{]}\leq\sigma^{2}\).

We conclude the proof with basic linear algebra. Denote by \(\lambda_{i}(A)\) the \(i\)-th eigenvalue of a matrix \(M\in\mathbb{R}^{m\times m}\). Using the generalized matrix determinant lemma, we get

\[\det(V_{t-1}) =\det(V_{t}-M_{\pi_{t}}^{\top}M_{\pi_{t}})\] \[=\det(V_{t})\det(I-M_{\pi_{t}}^{\top}V_{t}^{-1}M_{\pi_{t}})\] \[=\det(V_{t})\prod_{i=1}^{m}(1-\lambda_{i}(M_{\pi_{t}}^{\top}V_{t}^ {-1}M_{\pi_{t}}))\]

Note that \(\lambda_{i}(M_{\pi_{t}}^{\top}V_{t}^{-1}M_{\pi_{t}})\in(0,1]\). Next, using that \(\log(1-x)\leq-x\) for all \(x<1\), we get that

\[\log\!\left(\frac{\det(V_{t-1})}{\det(V_{t})}\right)=\sum_{i=1}^{m}\log(1- \lambda_{i}(M_{\pi_{t}}^{\top}V_{t}^{-1}M_{\pi_{t}}))\leq-\sum_{i=1}^{m}\lambda _{i}(M_{\pi_{t}}^{\top}V_{t}^{-1}M_{\pi_{t}})\]

Rearranging the last display, and bounding the sum by its maximum element, we get

\[\lambda_{\max}(M_{\pi_{t}}^{\top}V_{t}^{-1}M_{\pi_{t}})\leq\sum_{i=1}^{m} \lambda_{i}(M_{\pi_{t}}^{\top}V_{t}^{-1}M_{\pi_{t}})\leq\log\!\left(\frac{\det( V_{t})}{\det(V_{t-1})}\right)\] (20)

The proof is concluded by combining Eqs. (19) and (20). 

**Remark 2** (Expected Regret).: _The beauty of Theorem 1 is that the proof uses only in-expectation arguments. This is unlike most previous analysis, that controls the regret via controlling tail-events, and bounds on the expected regret are then derived a-posteriori from high-probability bounds. In the context of linear bandits, Theorem 2 leads to bound on the expected regret that only requires the noise variance to be bounded, whereas most previous work relies on the stronger sub-Gaussian noise assumption [e.g. Abbasi-Yadkori et al., 2011]._

**Remark 3** (Kernel Bandits / Bayesian Optimization).: _Using the standard 'kernel-trick', the analysis can further be extended to the non-parametric setting where \(\mathcal{M}\) is an infinite-dimensional reproducing kernel Hilbert space (RKHS)._

## Appendix B PAC to Regret Bounds

### Proof of Lemma 3

Proof.: The Lagrangian for Eq. (12) is

\[\text{pac-dec}_{\epsilon}^{ac,f}(f)=\min_{\lambda\geq 0}\min_{\mu\in\mathscr{P}( \Pi)}\max_{\nu\in\mathscr{P}(\mathcal{M})}\delta_{f}\nu-\lambda(\mu I_{f}\nu- \epsilon^{2}).\]Reparametrize any \(\mu\in\mathscr{P}(\Pi)\) as \(\bar{\mu}(p)=(1-p)e_{\pi^{*}_{f}}+p\mu_{2}\). We bound \(\text{dec}^{ac,f}_{\epsilon}\) by a function of \(\text{pac-dec}^{ac,f}_{\epsilon}\). Starting from Eq. (5), we have

\[\text{dec}^{ac,f}_{\epsilon}(f) =\min_{\lambda\geq 0}\min_{\mu\in\mathscr{P}(\Pi)}\max_{\nu\in \mathscr{P}(\mathcal{M})}\mu\Delta_{f}\nu-\lambda(\mu I_{f}\nu-\epsilon^{2})\] \[=\min_{\lambda\geq 0}\min_{\bar{\mu}\in\mathscr{P}(\Pi)}\max_{ \nu\in\mathscr{P}(\mathcal{M})}\bar{\mu}\Delta_{f}\nu-\lambda(\bar{\mu}I_{f} \nu-\epsilon^{2})\] \[=\min_{\lambda\geq 0}\min_{0\leq p\leq 1}\min_{\mu_{2}\in\mathscr{P}( \Pi)}\max_{\nu\in\mathscr{P}(\mathcal{M})}\delta_{f}\nu+p\mu_{2}\Delta_{f}e_{ f}-\lambda\bar{\mu}I_{f}\nu-\lambda\epsilon^{2}\] \[\leq\min_{\lambda\geq 0}\min_{0\leq p\leq 1}\min_{\mu_{2}\in \mathscr{P}(\Pi)}\max_{\nu\in\mathscr{P}(\mathcal{M})}\delta_{f}\nu+p\mu_{2} \Delta_{f}e_{f}-\lambda p\mu_{2}I_{f}\nu-\lambda\epsilon^{2}\] \[\leq\min_{0\leq p\leq 1}\text{pac-dec}^{ac,f}_{\frac{\lambda}{ \sqrt{p}}}(f)+p\Delta_{\max}\,.\]

### Proof of Lemma 4

Proof of Lemma 4.: For the first part, note that

\[\text{pac-dec}^{ac,f}_{\epsilon}(f) =\min_{\mu\in\mathscr{P}(\Pi)}\min_{\lambda\geq 0}\max_{\nu\in \mathscr{P}(\mathcal{M})}\delta_{f}\nu-\lambda\mu I_{f}\nu+\lambda\epsilon^{2}\] \[=\min_{\mu\in\mathscr{P}(\Pi)}\min_{\lambda\geq 0}\max_{b\in\Pi} \max_{g\in\mathcal{M}}\langle\phi_{b},g\rangle-\langle\phi_{\pi^{*}_{f}},f \rangle-\lambda\|g-f\|_{V(\mu)}^{2}+\lambda\epsilon^{2}\] \[\stackrel{{(i)}}{{=}}\min_{\mu\in\mathscr{P}(\Pi)} \min_{\lambda\geq 0}\max_{b\in\Pi}\langle\phi_{b}-\phi_{\pi^{*}_{f}},f \rangle+\frac{1}{4\lambda}\|\phi_{b}\|_{V(\mu)^{-1}}^{2}+\lambda\epsilon^{2}\] \[\stackrel{{(ii)}}{{\leq}}\min_{\mu\in\mathscr{P}(\Pi )}\min_{\lambda\geq 0}\max_{b\in\Pi}\frac{1}{4\lambda}\|\phi_{b}\|_{V(\mu)^{-1}}^{2}+ \lambda\epsilon^{2}\] \[=\min_{\mu\in\mathscr{P}(\Pi)}\max_{b\in\Pi}\epsilon\|\phi_{b}\| _{V(\mu)^{-1}}\] \[\stackrel{{(iii)}}{{\leq}}\epsilon\sqrt{d}\,.\]

Equation \((i)\) follows by computing the maximizer attaining the quadratic form over \(\mathcal{M}=\mathbb{R}^{d}\). The inequality \((ii)\) is by definition of \(\pi^{*}_{f}\) and the last inequality \((iii)\) by the assumption that the reward is observed, respectively, \(\phi_{\pi}\phi_{\pi}^{\top}\preceq M_{\pi}^{\top}M_{\pi}\), and the Kiefer-Wolfowitz theorem.

The second part of the statement follows by combining Lemmas 2 and 3. 

## Appendix C Coefficient Relations Results and Proofs

**Lemma 7**.: _Assume Assumption 1 holds, i.e._

\[(\mu(r_{f}-r_{g}))^{2}\leq\mu I_{f}e_{g}\,.\] (21)

_Then_

\[\left(\sum_{g}\mu(r_{f}-r_{g})\nu_{g}\right)^{2}\leq\sum_{g} \left(\mu(r_{f}-r_{g})\right)^{2}\nu_{g}\leq\mu I_{f}\nu\,.\]

Proof.: First Jensen's inequality, then Eq. (21).

### Proof of Lemma 1

Proof of Lemma 1.: Note that

\[\text{dec}^{ac}_{\epsilon}(f) =\min_{\mu\in\mathcal{P}(\Pi)}\max_{\nu\in\mathcal{P}(\mathcal{M})} \mu\Delta\nu\qquad\text{s.t.}\qquad\mu I_{f}\nu\leq\epsilon^{2}\] \[=\min_{\mu\in\mathcal{P}(\Pi)}\max_{\nu\in\mathcal{P}(\mathcal{M} )}\mu\Delta_{f}\nu+\sum_{g\in\mathcal{M}}\sum_{\pi\in\Pi}\nu_{g}\mu_{\pi}(r_{f }(\pi)-r_{g}(\pi))\qquad\text{s.t.}\qquad\mu I_{f}\nu\leq\epsilon^{2}\] \[\leq\min_{\mu\in\mathcal{P}(\Pi)}\max_{\nu\in\mathcal{P}(\mathcal{ M})}\mu\Delta_{f}\nu+\sqrt{\mu I_{f}\nu}\qquad\text{s.t.}\qquad\mu I_{f}\nu \leq\epsilon^{2}\] \[\leq\epsilon+\min_{\mu\in\mathcal{P}(\Pi)}\max_{\nu\in\mathcal{ P}(\mathcal{M})}\mu\Delta_{f}\nu\qquad\text{s.t.}\qquad\mu I_{f}\nu\leq \epsilon^{2}\] \[\leq\epsilon+\text{dec}^{ac,f}_{\epsilon}(f)\qquad\text{s.t.} \qquad\mu I_{f}\nu\leq\epsilon^{2},\]

where the first inequality is by Lemma 7. Also, by lower bounding the sum in the second inequality by \(-\sqrt{\mu I_{f}\nu}\) we get left inequality. 

### Proof of Lemma 2

Proof of Lemma 2.: For the first inequality, using the definition of \(\text{dec}^{ac}_{\epsilon}(f,\Delta_{f})\) and the AM-GM inequality:

\[\text{dec}^{ac,f}_{\epsilon}(f) =\min_{\lambda\geq 0}\max_{\nu\in\mathscr{P}(\mathcal{M})}\min_{ \mu\in\mathscr{P}(\Pi)}\mu\Delta_{f}\nu-\lambda\mu I_{f}\nu+\lambda\epsilon^{2}\] \[\leq\min_{\lambda>0}\max_{\nu\in\mathscr{P}(\mathcal{M})}\min_{ \mu\in\mathscr{P}(\Pi)}\frac{(\mu\Delta_{f}\nu)^{2}}{4\lambda\mu I_{f}\nu}+ \lambda\epsilon^{2}\] (22) \[=\min_{\lambda>0}\frac{\Psi(f)}{4\lambda}+\lambda\epsilon^{2}= \epsilon\sqrt{\Psi(f)}\,.\] (23)

Further, by Eq. (11) and Assumption 1 we have \(\mu^{\text{TS}}_{\nu}\Delta_{f}\nu\leq\sqrt{K\sum_{g,h\in\mathcal{M}}\nu_{g} \nu_{h}e_{\pi^{*}_{h}}(r_{g}-r_{f})^{2}}\leq\sqrt{K\mu^{\text{TS}}_{f}I_{f}\nu}\), which gives \(\Psi(f)\leq K\). Plugging this into Eq. (23) gives the second inequality. 

### Regret bound for Algorithm 1 defined for \(\Delta_{f}\) and \(\text{dec}^{ac,f}_{\epsilon}\)

**Lemma 8**.: _If Assumption 1 holds, then the regret of Anytime-E2D (Algorithm 1) with \(\Delta\) replaced with \(\Delta_{f}\) is bounded as follows:_

\[R_{n}\leq\max_{t\in[n],f\in\mathcal{M}}\Biggl{\{}\frac{\text{ dec}^{ac,f}_{\epsilon_{t}}(f)}{\epsilon_{t}^{2}}\Biggr{\}}\Biggl{(}\sum_{t=1}^{n} \epsilon_{t}^{2}+\text{Est}_{n}\Biggr{)}+\sqrt{n\text{Est}_{n}}\]

Proof.: The proof follows along the lines of the proof of Theorem 1. The main difference is that when introducing \(\Delta_{f}\), we get a term that captures the reward estimation error:

\[R_{n} \leq\sum_{t=1}^{n}\mathbb{E}\Bigl{[}\text{dec}^{ac}_{\epsilon_{t }}(\hat{f}_{t})+\lambda_{t}(\mu_{t}I_{\hat{f}_{t}}e_{f^{*}}-\epsilon_{t}^{2}) \Bigr{]}+\sum_{t=1}^{n}\mathbb{E}\Bigl{[}\mu_{t}(r_{\hat{f}_{t}}-r_{f^{*}}) \Bigr{]}\] (24) \[\leq\max_{t\in[n]}\max_{f\in\mathcal{M}}\Biggl{\{}\frac{1}{ \epsilon_{t}^{2}}\text{dec}^{ac}_{\epsilon_{t}}(f)\Biggr{\}}\sum_{t=1}^{n} \left(\epsilon_{t}^{2}+\mathbb{E}\Bigl{[}\mu_{t}I_{\hat{f}_{t}}e_{f^{*}} \Bigr{]}\right)+\sqrt{n\text{Est}_{n}}\] (25)

For the last inequality, we used Cauchy-Schwarz and Assumption 1 to bound the error term,

\[\sum_{t=1}^{n}\mathbb{E}\Bigl{[}\mu_{t}(r_{\hat{f}_{t}}-r_{f^{*}})\Bigr{]} \leq\sqrt{n\sum_{t=1}^{n}\mathbb{E}\Bigl{[}(\mu_{t}(r_{\hat{f}_{t}}-r_{f^{*}})) ^{2}\Bigr{]}}\leq\sqrt{n\sum_{t=1}^{n}\mathbb{E}\Bigl{[}\mu_{t}I_{\hat{f}_{t}} e_{f}\Bigr{]}}=\sqrt{n\text{Est}_{n}}\]

### Generalized Information Ratio

The generalized information ratio (Lattimore and Gyorgy, 2021) for \(\mu\in\mathscr{P}(\Pi)\), \(\nu\in\mathscr{P}(\mathcal{M})\), and \(\alpha>1\) is defined as

\[\Psi_{\alpha,f}(\mu,\nu)=\frac{(\mu\Delta_{f}\nu)^{\alpha}}{\mu I_{f}\nu}\] (26)

For \(\alpha=2\), we get the standard information ratio introduced by Russo and Van Roy (2014) with \(\nu\) as a prior over the model class \(\mathcal{M}\). Define \(\Psi_{\alpha}(f)=\max_{\nu\in\mathcal{M}}\min_{\mu\in\mathscr{P}(\Pi)}\Psi_{ \alpha,f}(\mu,\nu)\). To upper bound \(\text{dec}^{ac}_{\epsilon}\), we have the following lemma.

**Lemma 9**.: _For the reference model \(f\), the ac-dec can be upper bounded as_

\[\text{dec}^{ac}_{\epsilon}(f)\leq\min_{\lambda>0}\left\{\lambda^{\frac{1}{1- \alpha}}\alpha^{\frac{\alpha}{1-\alpha}}(\alpha-1)\Psi_{\alpha}(f)^{\frac{1} {\alpha-1}}+\lambda\epsilon^{2}\right\}\] (27)

_for \(\alpha>1\)._

Proof.: We start by noting that for \(x_{1},\dots,x_{\alpha}\geq 0\), from AM-GM we have that \(\alpha(x_{1}\cdot x_{2}\cdots x_{\alpha})^{1/\alpha}\leq x_{1}+\cdots+x_{\alpha}\). Substituting \(x_{2}=x_{3}=\cdots x_{\alpha}\), we get \(\alpha\cdot x_{1}^{\frac{1}{\alpha}}\cdot x_{2}^{\frac{\alpha-1}{\alpha}}-x_{ 1}\leq(\alpha-1)x_{2}\). Writing \(x_{1}=\lambda\mu I_{f}\nu\) and \(x_{2}=\alpha^{\frac{\alpha}{1-\alpha}}\left(\frac{(\mu\Delta\nu)^{\alpha}}{ \lambda\mu I_{f}\nu}\right)^{\frac{1}{\alpha-1}}\) and using the previous inequality with the \(\text{dec}^{ac}_{\epsilon}\) program gives the result. 

The information ratio \(\Psi_{\alpha,f}(\mu,\nu)\) can be thought of as the Bayesian information ratio in (Russo and Van Roy, 2014) where the expectation is taken over the distribution \(\nu\) of possible environments. However, for information gain \(I_{f}\), (Russo and Van Roy, 2014) use entropy difference in the posterior distribution of \(\pi_{f}^{*}\) before and after the observation is revealed.

### Proof of Lemma 5

_Proof of Lemma 5._

\[\text{dec}^{ac,f}_{\epsilon}(\hat{f}_{t}) =\min_{\mu\in\mathscr{P}(\Pi)}\min_{\lambda\geq 0}\max_{b\in \Pi}\max_{g\in\mathcal{M}}\langle\phi_{b},g\rangle-\langle\phi_{\mu},\hat{f}_ {t}\rangle-\lambda\|g-\hat{f}_{t}\|_{V(\mu)}^{2}+\lambda\epsilon^{2}\] \[=\min_{\lambda\geq 0}\min_{\mu\in\mathscr{P}(\Pi)}\max_{b\in \Pi}\langle\phi_{b}-\phi_{\mu},\hat{f}_{t}\rangle+\frac{1}{4\lambda}\|\phi_{b }\|_{V(\mu)^{-1}}^{2}+\lambda\epsilon^{2}\,.\] (28)

The first equality is by definition, and the second equality follows from solving the quadratic maximization over \(g\in\mathcal{M}=\mathbb{R}^{d}\). To show that the problem is convex in \(\mu\), note that taking inverses of positive semi-definite matrices \(X,Y\) is a convex function, i.e. \(((1-\eta)X+\eta Y)^{-1}\preceq(1-\eta)X^{-1}+\eta Y^{-1}\). In particular, \(V((1-\eta)\mu_{1}+\eta\mu_{2})^{-1}\preceq(1-\eta)V(\mu_{1})^{-1}+\eta V(\mu_{ 2})^{-1}\). With this the claim follows. 

## Appendix D Convex Program for Fixed \(\lambda\)

Take Eq. (28) and fix \(\lambda>0\). Then we have the following saddle-point problem:

\[\min_{\mu\in\mathscr{P}(\Pi)}\max_{b\in\Pi}\langle\phi_{b}-\phi_ {\mu},\hat{f}_{t}\rangle+\frac{1}{4\lambda}\|\phi_{b}\|_{V(\mu)^{-1}}^{2}+ \lambda\epsilon^{2}\] \[=\lambda\epsilon^{2}+\min_{\mu\in\mathscr{P}(\Pi)}\max_{b\in\Pi} \langle\phi_{b}-\phi_{\mu},\hat{f}_{t}\rangle+\frac{1}{4\lambda}\|\phi_{b}\|_{V (\mu)^{-1}}^{2}\]

Up to the constant additive term, this saddle point problem is equivalent to the following convex program

\[\min_{y\in\mathbb{R},\mu\in\mathbb{R}^{\Pi}}y\] s.t. \[y \geq\langle\phi_{b}-\phi_{\mu},\hat{f}_{t}\rangle+\frac{1}{4 \lambda}\|\phi_{b}\|_{V(\mu)^{-1}}^{2}\quad\forall b\in\Pi\] \[\mathbf{1}\mu =1\] \[\mu_{\pi} \geq 0\quad\forall\pi\]Experiments

All experiments below were run on a semi-bandit problem with a "revealing action", as alluded to in the paragraph below Lemma 4. Specifically, we assume a semi-bandit model where \(\mathcal{M}=\mathbb{R}^{d}\) and the features are \(\phi_{\pi}\in\mathbb{R}^{d}\). For an instance \(f^{*}\in\mathcal{M}\), the reward function is \(r_{f^{*}}=\langle\phi_{\pi},f^{*}\rangle\) for all \(\pi\in\Pi\). There is one revealing (sub-optimal) action \(\hat{\pi}\neq\pi_{f^{*}}^{*}\). The observation for any action \(\pi\neq\hat{\pi}\) is

\[M_{f^{*}}(\pi)=\mathcal{N}(\langle\phi_{\pi},f^{*}\rangle,1)\] (29)

Define \(M_{\hat{\pi}}=[\phi_{\pi_{1}},\ldots,\phi_{\pi_{|\Pi|}}]^{\top}\). Then the observation for action \(\hat{\pi}\) is

\[M_{f^{*}}(\hat{\pi})=\mathcal{N}(M_{\hat{\pi}}f^{*},\mathbf{1}_{d})\] (30)

Thus, the information for any action \(\pi\neq\hat{\pi}\) is

\[I_{f}(g,\pi)=\frac{\sigma^{2}}{2}\langle\phi_{\pi},g-f\rangle^{2}\] (31)

while the information for action \(\hat{\pi}\) is

\[I_{f}(g,\hat{\pi})=\frac{\sigma^{2}}{2}\|M_{\hat{\pi}}(g-f)\|^{2}=\frac{\sigma ^{2}}{2}\sum_{\pi}\langle\phi_{\pi},g-f\rangle^{2}\] (32)

For this setting \(\text{Est}_{n}\leq\mathcal{O}(d\log(n))\) (see Appendix A.1).

### Experimental Setup

Our main objective is to compare our algorithm Anytime-E2D to the fixed-horizon E2D algorithm by Foster et al. (2021). Anytime-E2D and E2D were implemented by using the procedure described in Section 3.4. Both Anytime-E2D and E2D need to solve the inner convex problem in Lemma 5. To do so we use Frank-Wolfe (Frank and Wolfe, 1956; Dunn and Harshbarger, 1978; Jaggi, 2013) for 100 steps and warm-starting the optimization at the solution from the previous round, \(\mu_{t-1}\). For Anytime-E2D we further perform a grid search over \(\lambda\in[0,\max_{g\in\mathcal{M}}\epsilon^{-2}\text{dec}^{ac,f}(g)]\) (with a discretization of \(50\) points) to optimize over lambda within each iteration of Frank-Wolfe. For both the E2D and Anytime-E2D algorithm we used the version with the gaps \(\Delta\) replaced with \(\Delta_{f}\), since we noticed that both algorithms performed better with \(\Delta_{f}\). For E2D, the scale hyperparameter \(\lambda\) was set using \(\lambda=\sqrt{\frac{n}{4\log(n)}}\) as mentioned in Foster et al. (2021, Section 6.1.1). While for Anytime-E2D we set the hyper-parameter \(\epsilon_{t}^{2}=d/t\). Further, we compare to standard bandit algorithms: Upper Confidence Bound (UCB) and Thompson Sampling (TS) (Lattimore and Szepesvari, 2020a).

### Experiment 1

In this experiment, we aim to demonstrate the advantage of having an anytime algorithm. Specifically, we tune \(\lambda\) in the E2D algorithm for different horizons \(n=200,500,1000,2000\), but run it for a fixed horizon of \(n=2000\). As such, we expect our algorithm Anytime-E2D to perform better than E2D when \(\lambda\) was tuned for the incorrect horizons (i.e. \(n=200,500,1000\)). The feature dimension is \(d=3\). The number of decisions is \(|\Pi|=10\). We generated the features \(\phi_{\pi}\) for each \(\pi\in\Pi\) and parameter \(f^{*}\in\mathbb{R}^{d}\) randomly at the beginning and then kept them fixed throughout the experimentation. 100 independent runs were performed for each algorithm.

The results of the experiment can be seen as the left plot in Fig. 1. As expected, our algorithm Anytime-E2D performs better than E2D (for \(n=200,500,1000\)). This indicates that the E2D algorithm is sensitive to different settings of \(\lambda\), which is problematic when the horizon is not known beforehand. Whereas our Anytime-E2D algorithm performs well even when the horizon is not known.

### Experiment 2

In this experiment, we investigate the case when \(n<d^{4}\). As pointed out below Lemma 3, we expect improvement in this regime as the regret bound of our algorithm is \(R_{n}\leq\min\{d\sqrt{n},d^{1/3}n^{2/3}\}\)while the default, fixed-horizon E2D algorithm cannot achieve these bounds simultaneously and one has to pick one of \(d\sqrt{n}\) or \(d^{1/3}n^{2/3}\) beforehand for setting the scale hyperparameter \(\lambda\). It is standard that the choice of \(\lambda\) is made according to the \(d\sqrt{n}\) regret bound for E2D Foster et al. (2021)(which is not optimal when \(n<\!\!<d^{4}\)), especially, if the horizon is not known beforehand. Thus, we set the horizon to \(n=1000\) and the dimension of the feature space to \(d=30\), which gives us that \(n=1000\ll 810000=d^{4}\). The rest of the setup and parameters are the same as in the previous experiment except for the features \(\phi_{\pi}\) and \(f^{*}\) which are again chosen randomly in the beginning and then kept fixed throughout the experiment.

The results of the experiment can be seen as the right plot in Fig. 1. As expected, our algorithm Antime-E2D performs better than E2D, UCB, and TS. This indicates that indeed, Anytime-E2D is likely setting \(\lambda\) appropriately to achieve the preferred \(d^{1/3}n^{2/3}\) regret rate for small horizons. The poor performance of the other algorithms can be justified, since E2D is optimized based on the worse \(d\sqrt{n}\) regret rate (for small horizons), while the UCB and TS algorithms are not known to get regret better than \(d\sqrt{n}\).

Figure 1: Running Anytime-E2D, TS, UCB, and E2D optimized for different horizons \(n\in\{200,500,1000,2000\}\). Left: The result for horizon \(n=2000\), and the feature space dimension \(d=3\). Right: The result for horizon \(n=1000\), and the feature space dimension \(d=30\).