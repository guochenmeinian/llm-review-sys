# Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift

 Saurabh Garg*

Carnegie Mellon University

sgarg2@andrew.cmu.edu

&Amrith Setlur*

Carnegie Mellon University

asetlur@andrew.cmu.edu

&Zachary C. Lipton

Carnegie Mellon University

zlipton@andrew.cmu.edu

&Sivaraman Balakrishnan

Carnegie Mellon University

sbalakri@andrew.cmu.edu

&Virginia Smith

Carnegie Mellon University

smithv@andrew.cmu.edu

&Aditi Raghunathan

Carnegie Mellon University

aditirag@andrew.cmu.edu

Equal contribution.

###### Abstract

Self-training and contrastive learning have emerged as leading techniques for incorporating unlabeled data, both under distribution shift (unsupervised domain adaptation) and when it is absent (semi-supervised learning). However, despite the popularity and compatibility of these techniques, their efficacy in combination remains surprisingly unexplored. In this paper, we first undertake a systematic empirical investigation of this combination, finding (i) that in domain adaptation settings, self-training and contrastive learning offer significant complementary gains; and (ii) that in semi-supervised learning settings, surprisingly, the benefits are not synergistic. Across eight distribution shift datasets (_e.g._, BREEDs, WILDS), we demonstrate that the combined method obtains 3-8% higher accuracy than either approach independently. Finally, we theoretically analyze these techniques in a simplified model of distribution shift demonstrating scenarios under which the features produced by contrastive learning can yield a good initialization for self-training to further amplify gains and achieve optimal performance, even when either method alone would fail.

## 1 Introduction

Even under natural, non-adversarial distribution shifts, the performance of machine learning models often drops [65, 83, 47, 29]. While we might hope to retrain our models on labeled samples from the new distribution, this option is often unavailable due to the expense or impracticality of collecting new labels. Consequently, researchers have investigated the Unsupervised Domain Adaptation (UDA) setting. Here, given labeled source data and unlabeled out-of-distribution (OOD) target data, the goal is to produce a classifier that performs well on the target. Because UDA is generally underspecified [7], researchers have focused on two main paths: (i) domain adaptation papers that explore heuristics for incorporating the unlabeled target data, relying on benchmark datasets ostensibly representative of "real-world shifts" to adjudicate progress [72, 85, 47]; and (ii) theoretically motivated papers that explore structural assumptions under which UDA problems are well posed [77, 74]. This work engages with the former focusing on two popular methods: self-training and contrastive pretraining.

Self-training [75, 52, 79, 90, 86] and contrastive pretraining [13, 16, 93] were both proposed, initially, for traditional Semi-Supervised Learning (SSL) problems, where the labeled and unlabeled data are drawn from the same distribution. Here the central challenge is statistical: to exploit the unlabeled data to learn a better predictor than one would get by training on the (small) labeled data alone. Morerecently, these methods have emerged as favored empirical approaches for UDA, demonstrating efficacy on many popular benchmarks [70; 30; 12; 76]. In self-training, one first learns a predictor using source labeled data. The predictor then produces pseudolabels for the unlabeled target data, and a new predictor is trained on the pseudolabeled data. Contrastive pretraining learns representations from unlabeled data by enforcing invariance to specified augmentations. These representations are subsequently used to learn a classifier. In UDA, the representations are trained on the union of the source and target data. In attempts to explain their strong empirical performance, several researchers have attempted analyses under various assumptions on the data, task, and inductive biases of the function class[87; 39; 73; 76; 12; 40; 38; 11]. Despite the strong strong results, there has been surprisingly little work (both empirically and theoretically) exploring when either might be expected to perform best and whether the benefits might be complementary.

In this paper, we investigate the complementary benefits of self-training and contrastive pretraining. Interestingly, we find that the combination yields significant gains in UDA despite producing negligible gains in SSL. In experiments across eight distribution shift benchmarks (_e.g._ BREEDs [72], FMoW [47], Visda [63]), we observe that re-using unlabeled data for self-training (with FixMatch [79]) after learning contrastive representations (with SwAV [13]), yields \(>5\%\) average improvement on OOD accuracy in UDA as compared to \(<0.8\%\) average improvement in SSL (Fig. 1).

Next, we address the question _why the combination of self-training and contrastive learning_ proves synergistic in distribution shift scenarios. To facilitate our analysis, we consider a simplified distribution shift setting that includes two types of features: (i) invariant features that perfectly predict the label; and (ii) domain-dependent features that are predictive of the label in just source. Our theoretical analysis reveals that self-training can achieve optimal target performance but requires a "good" enough classifier to start with. We observe that source-only ERM fails to provide a "good" initialization. On the other hand, contrastive pretraining on unlabeled data performs better than ERM but is still sub-optimal. This implies that contrastive pretraining ends up decreasing reliance on domain-dependent features (as compared to ERM) but doesn't completely eliminate them. Nevertheless, contrastive pretraining does provide a "good" initialization for self-training, _i.e._, "good" initial pseudolabels on the target unlabeled data. As a result, self-training on top of contrastive learned features effectively unlearns the reliance on domain-dependent features and generalizes perfectly OOD. In contrast, for SSL settings (_i.e._, in distribution), our analysis highlights that contrastive pretraining already acquires sufficient predictive features such that linear probing with (a small amount of) labeled data picks up those features and attains near-optimal ID generalization.

Finally, we connect our theoretical understanding of "good" representations from contrastive learning and improved linear transferability from self-training back to observed empirical gains. We linearly probe representations (fix representations and train only the linear head) learned by contrastive pretraining vs. no pretraining and find: (i) contrastive pretraining substantially improves the ceiling on

Figure 1: _Self-training over Contrastive learning (STOC) improves over Contrastive Learning (CL) under distribution shift._**(a)** We observe that in SSL settings, where labeled and unlabeled data are drawn from the same distribution, STOC offers negligible improvements over CL. In contrast, in UDA settings where there is distribution shift between labeled and unlabeled data, STOC offers gains over CL. Results aggregated across 8 benchmarks. Results on individual data in Table 1 and 2. **(b)** 2-D illustration of our simplified distribution setup, depicting decision boundaries learned by ERM and CL and how Self-Training (ST) updates those. 1, 2, and 3 summarize our theoretical results in Sec. 4.

the target accuracy (performance of optimal linear probe) compared to ERM; (ii) self-training mainly improves linear transfer, _i.e_. OOD performance for the linear probe trained with source labeled data.

## 2 Setup and Preliminaries

**Task.** Our goal is to learn a predictor that maps inputs \(x\in\mathcal{X}\subseteq\mathbb{R}^{d}\) to outputs \(y\in\mathcal{Y}\). We parameterize predictors \(f=h\circ\Phi:\mathbb{R}^{d}\mapsto\mathcal{Y}\), where \(\Phi:\mathbb{R}^{d}\mapsto\mathbb{R}^{k}\) is a feature map and \(h\in\mathbb{R}^{k}\) is a classifier that maps the representation to the final scores or logits. Let \(\mathrm{P_{S}},\mathrm{P_{T}}\) be the source and target joint probability measures over \(\mathcal{X}\times\mathcal{Y}\) with \(\mathrm{p_{S}}\) and \(\mathrm{p_{T}}\) as the corresponding probability density (or mass) functions. The distribution over unlabeled samples from both the union of source and target is denoted as \(\mathrm{P_{U}}=(1/2)\cdot\mathrm{P_{S}}(x)+(1/2)\cdot\mathrm{P_{T}}(x)\).

We study two particular scenarios: (i) Unsupervised Domain Adaptation (UDA); and (ii) Semi-Supervised Learning (SSL). In UDA, we assume that the source and target distributions have the same label marginals \(\mathrm{P_{S}}(y)=\mathrm{P_{T}}(y)\) (_i.e_., no label proportion shift) and the same Bayes optimal predictor, _i.e_., \(\operatorname*{arg\,max}_{y}\mathrm{p_{S}}(y\mid x)=\operatorname*{arg\,max} _{y}\mathrm{p_{T}}(y\mid x)\). We are given labeled samples from the source, and unlabeled pool from the target. In contrast in SSL, there is no distribution shift, _i.e_., \(\mathrm{P_{S}}=\mathrm{P_{T}}=\mathrm{P_{U}}\). Here, we are given a small number of labeled examples and a comparatively large amount of unlabeled examples, both drawn from the same distribution, which we denote as \(\mathrm{P_{T}}\).

Unlabeled data is typically much cheaper to obtain, and our goal in both these settings is to leverage this along with labeled data to achieve good performance on the target distribution. In the UDA scenario, the challenge lies in generalizing out-of-distribution, while in SSL, the challenge is to generalize in-distribution despite the paucity of labeled examples. A predictor \(f\) is evaluated on distribution \(\mathrm{P}\) via its accuracy, _i.e_., \(A(f,\mathrm{P})=\operatorname*{\mathbb{E}_{P}}(\operatorname*{arg\,max}f(x)=y)\).

**Methods.** We now introduce the algorithms used for learning from labeled and unlabeled data.

1. _Source-only ERM (ERM):_ A standard approach is to simply perform supervised learning on the labeled data by minimizing the empirical risk \(\sum_{i=1}^{n}\ell(h\circ\Phi(x),y)\), for some classification loss \(\ell:\mathbb{R}\times\mathcal{Y}\mapsto\mathbb{R}\) (_e.g_., softmax cross-entropy) and labeled points \(\{(x_{i},y_{i})\}_{i=1}^{n}\).
2. _Contrastive Learning (CL):_ We first use the unlabeled data to learn a feature extractor. In particular, the objective is to learn a feature extractor \(\Phi_{\mathrm{cl}}\) that maps augmentations (for e.g. crops or rotations) of the same input close to each other and far from augmentations of random other inputs [13; 16; 93]. Once we have \(\Phi_{\mathrm{cl}}\), we learn a linear classifier \(h\) on top to minimize a classification loss on the labeled source data. We could either keep \(\Phi_{\mathrm{cl}}\) fixed or propagate gradients through. When clear from context, we also use CL to refer to just the contrastively pretrained backbone without training for downstream classification.
3. _Self-training (ST):_ This is a two-stage procedure, where the first stage performs source-only ERM by just looking at source-labeled data. In the second stage, we iteratively apply the current classifier on the unlabeled data to generate "pseudo-labels" and then update the classifier by minimizing a classification loss on the pseudolabeled data [52].

## 3 Self-Training Improves Contrastive Pretraining Under Distribution Shift

**Self-Training Over Contrastive learning (STOC).** Finally, rather than starting with a source-only ERM classifier, we propose to initialize self-training with a CL classifier, that was pretrained on unlabeled source and target data. ST uses that same unlabeled data again for pseudolabeling. As we demonstrate experimentally and theoretically, this combination of methods improves substantially over each independently.

**Datasets.** For both UDA and SSL, we conduct experiments across eight benchmark datasets: four BREEDs datasets [72]--Entity13, Entity30, Nonliving26, Living17; FMoW [47; 18] from WillDS benchmark; Officehome [85]; Visda [64; 63]; and CIFAR-10 [48]. Each of these datasets consists of domains, enabling us to construct source-target pairs (e.g., CIFAR10, we consider CIFAR10\(\rightarrow\)CINIC shift [22]). In the UDA setup, we adopt the source and target domains standard to previous studies (details in App. C.2). Because the SSL setting lacks distribution shift, we do not need to worry about domain designations and default to using source alone. To simulate limited supervision in SSL, we sub-sample the original labeled training set to 10%.

Experimental Setup and Protocols.SwAV [13] is the specific algorithm that we use for contrastive pretraining. In all UDA settings, unless otherwise specified, we pool all the (unlabeled) data from the source and target to perform SwAV. For self-training, we apply FixMatch [79], where the loss on source labeled data and on pseudolabeled target data are minimized simultaneously. For both methods, we fix the algorithm-specific hyperparameters to the original recommendations. For SSL settings, we perform SwAV and FixMatch on in-distribution unlabeled data. We experiment with Resnet18, Resnet50 [42] trained from scratch (random initialization). We do not consider off-the-shelf pretrained models (_e.g._, on Imagenet [68]) to avoid confounding our conclusions about contrastive pretraining. However, we note that our results on most datasets tend to be comparable to and sometimes exceed those obtained with ImageNet-pretrained models. For source-only ERM, as with other methods (FixMatch, SwAV), we default to using strong augmentation techniques: random horizontal flips, random crops, augmentation with Cutout [24], and RandAugment [21]. Moreover, unless otherwise specified, we default to full finetuning with source-only ERM, both from scratch and after contrastive pretraining, and for ST with FixMatch. For UDA, given that the setup precludes access to labeled data from the target distribution, we use source hold-out performance to pick the best hyperparameters. During pretraining, early stopping is done according to lower values of pretraining loss. For more details on datasets, model architectures, and experimental protocols, see App. C.

Results on UDA setup.Both ST and CL individually improve over ERM across all datasets, with CL significantly performing better than ST on 5 out of 8 benchmarks (see Table 1). Even on datasets where ST is better than CL, their performance remains close. Combining ST and CL with STOC shows an \(3\)-\(8\%\) improvement over the best alternative, yielding an absolute improvement in average accuracy of \(5.2\%\).

Note that by default, we train with CL on the combined unlabeled data from source and target. However, to better understand the significance of unlabeled target data in contrastive pretraining, we perform an ablation where the CL model was trained solely on unlabeled source data (refer to this as CL (source only); see App. C.4). We observe that ST on top of CL (source only) improves over ST (from scratch). However, the average performance of ST over CL (source only) is similar to that of standalone CL, maintaining an approximate 6% performance gap observed between CL and ST. This brings two key insights to the fore: (i) the observed benefit is not merely a result of the contrastive pretraining objective alone, but specifically CL with unlabeled target data helps; and (ii) both CL and ST leverage using target unlabeled data in a complementary nature.

Results on SSL setup.While CL improves over ST (as in UDA), unlike UDA, STOC doesn't offer any significant improvements over CL (see Table 2; ERM and ST results (refer to App. C.5). We conduct ablation studies with varying proportions of labeled data used for SSL, illustrating that there's considerable potential for improvement (see App. C.5). These findings highlight that the complementary nature of STOC over CL and ST individually is an artifact of distribution shift.

\begin{table}
\begin{tabular}{c c c c c c c c|c} \hline \hline Method & Living17 & Nonliv26 & Entity13 & Entity30 & FMoW & Visda & OH & CIFAR\(\rightarrow\) & Avg \\ \hline CL & \(91.15\) & \(84.58\) & \(90.73\) & \(85.47\) & \(43.05\) & \(97.67\) & \(49.73\) & \(91.78\) & \(79.27\) \\ STOC (ours) & \(92.00\) & \(85.95\) & \(91.27\) & \(86.14\) & \(44.43\) & \(97.70\) & \(49.95\) & \(93.06\) & \(80.06\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: _Results in the SSL setup._ We report accuracy on hold-out ID data. Recall that SSL uses labeled and unlabeled data from the same distribution during training. Refer to App. C.5 for ERM and ST.

\begin{table}
\begin{tabular}{c c c c c c c c|c} \hline \hline Method & Living17 & Nonliv26 & Entity13 & Entity30 & FMoW & Visda & OH & CIFAR & Avg \\ \hline CL & \(91.15\) & \(84.58\) & \(90.73\) & \(85.47\) & \(43.05\) & \(97.67\) & \(49.73\) & \(91.78\) & \(79.27\) \\ STOC (ours) & \(92.00\) & \(85.95\) & \(91.27\) & \(86.14\) & \(44.43\) & \(97.70\) & \(49.95\) & \(93.06\) & \(80.06\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: _Results in the UDA setup._ We report accuracy on target (OOD) data from which we only observe unlabeled examples during training. For benchmarks with multiple target distributions (_e.g._, OH, Visda), we report avg accuracy on those targets. Results with source performance, individual target performance, and standard deviation numbers are in App. C.4.

Theoretical Analysis and Intuitions

Our results on real-world datasets suggest that although self-training may offer little to no improvement over contrastive pretraining for in-distribution (_i.e._, SSL) settings, it leads to substantial improvements when facing distribution shifts in UDA (Sec. 3). Why do these methods offer complementary gains, but only under distribution shifts? In this section, we seek to answer this question by first replicating all the empirical trends of interest in a simple data distribution with an intuitive story (Sec. 4.1). In this toy model, we formally characterize the gains afforded by contrastive pretraining and self-training both individually (Secs. 4.2, 4.3) and when used together (Sec. 4.4).

Data distributionWe consider binary classification and model the inputs as consisting of two kinds of features: \(x=[x_{\mathrm{in}},x_{\mathrm{sp}}]\), where \(x_{\mathrm{in}}\in\mathbb{R}^{d_{\mathrm{in}}}\) is the invariant feature that is predictive of the label across both source \(\mathrm{P_{S}}\) and target \(\mathrm{P_{T}}\) and \(x_{\mathrm{sp}}\in\mathbb{R}^{d_{\mathrm{sp}}}\) is the spurious feature that is correlated with the label \(y\) only on the source domain \(\mathrm{P_{S}}\) but uncorrelated with label \(y\) in \(\mathrm{P_{T}}\). Formally, we sample \(\mathrm{y}\sim\mathrm{Unif}\{-1,1\}\) and generate inputs \(x\) conditioned on \(\mathrm{y}\) as follows:

\[\mathrm{P_{S}}\,:\,\,x_{\mathrm{in}} \sim\mathcal{N}(\gamma\cdot yw^{\star},\Sigma_{\mathrm{in}})\,\, \,x_{\mathrm{sp}}=\mathbf{y}\mathbf{1}_{d_{\mathrm{sp}}}\] \[\mathrm{P_{T}}\,:\,\,x_{\mathrm{in}} \sim\mathcal{N}(\gamma\cdot yw^{\star},\Sigma_{\mathrm{in}})\,\, \,x_{\mathrm{sp}}\sim\mathcal{N}(\mathbf{0},\Sigma_{\mathrm{sp}}),\] (1)

where \(\gamma\) is the margin afforded by the invariant feature2. We set the covariance of the invariant features \(\Sigma_{\mathrm{in}}=\sigma_{\mathrm{in}}^{2}\cdot(\mathbf{I}_{d_{\mathrm{in}} }-w^{\star}w^{\star\top})\). This makes the variance along the latent predictive direction \(w^{\star}\) to be zero. Note that the spurious feature is also completely predictive of the label in the source data. In fact, when \(d_{\mathrm{sp}}\) is sufficiently large, \(x_{\mathrm{sp}}\) is more predictive (than \(x_{\mathrm{in}}\)) of \(\mathrm{y}\) in the source. In the target, \(x_{\mathrm{sp}}\) is distributed as a Gaussian with \(\Sigma_{\mathrm{sp}}=\sigma_{\mathrm{sp}}^{2}\mathbf{I}_{d_{\mathrm{sp}}}\). We use \(w_{\mathrm{in}}\!\!=\!\![w^{\star},0,...,0]^{\top}\) to refer to the invariant direction/feature, and \(w_{\mathrm{sp}}=[0,...,0,1_{4\epsilon\epsilon\epsilon}\!/\!\sqrt{d_{\mathrm{sp }}}]^{\top}\) for the spurious direction.

Footnote 2: See App. D.1 for similarities and differences of our setup with prior works.

Data for UDA vs. SSLFor convenience, we assume access to infinite unlabeled data and replace their empirical quantities with population counterparts. For SSL, we sample both finite labeled and infinite unlabeled data from the same distribution \(\mathrm{P_{T}}\), where spurious features are absent (to exclude easy-to-generalize features). For UDA, we assume infinite labeled data from \(\mathrm{P_{S}}\) and infinite unlabeled from \(\mathrm{P_{T}}\). Importantly, note that due to distribution shift, population access of \(\mathrm{P_{S}}\) doesn't trivialize the problem as "ERM" on infinite labeled source data _does not_ achieve optimal performance on target.

Methods and objectivesRecall from Section 2 that we learn linear classifiers \(h\) over feature extractor \(\Phi\). For our toy setup, we consider linear feature extractors i.e. \(\Phi\) is a matrix in \(\mathbb{R}^{d\times k}\) and the prediction is given by \(\mathrm{sgn}(h^{\top}\Phi x)\). We use the exponential loss \(\ell(f(x),y)=\exp{(-yf(x))}\).

Self-trainingST performs ERM in the first stage using labeled data from the source, and then subsequently updates the head \(h\) by iteratively generating pseudolabels on the unlabeled target:

\[\mathcal{L}_{\mathrm{st}}(h;\Phi) := \mathbb{E}_{\mathrm{P_{T}}(x)}\ell(h^{\top}\Phi x,\mathrm{sgn}(h^ {\top}\Phi(x)))\] (2) \[\text{Update:}\,\,\,h^{t+1} = \frac{h^{t}-\eta\nabla_{h}\mathcal{L}_{\mathrm{st}}(h^{t};\Phi)}{ \left\lVert h^{t}-\eta\nabla_{h}\mathcal{L}_{\mathrm{st}}(h^{t};\Phi)\right\rVert _{2}}\]

For ERM and ST, we train both \(h\) and \(\Phi\) (equivalent to \(\Phi\) being identity and training a linear head).

Contrastive pretrainingWe obtain \(\Phi_{\mathrm{cl}}:=\arg\min_{\Phi}\mathcal{L}_{\mathrm{cl}}(\Phi)\) by minimizing the Barlow Twins objective [93], which prior works have shown is also equivalent to spectral contrastive and non-contrastive objectives [33, 11]. Given probability distribution \(\mathrm{P_{A}}(a\mid x)\) for input \(x\), and marginal \(\mathrm{P_{A}}\), we consider a constrained form of Barlow Twins in (3) which enforces features of "positive pairs" \(a_{1},a_{2}\) to be close while ensuring feature diversity. We assume a strict regularization \((\rho=0)\) for the theory arguments in the rest of the paper, and in App. D.2 we prove that all our claims hold for small \(\rho\) as well. For augmentations, we scale the magnitude of each co-ordinate uniformly by an independent amount, i.e., \(a\sim\mathrm{P_{A}}(\,\cdot\mid x)=\mathbf{c}\odot x\), where \(\mathbf{c}\sim\mathrm{Unif}[0,1]^{d}\). We try to mirror practical settings where the augmentations are fairly "generic", not encoding information about which features are invariant or spurious, and hence perturb all features symmetrically.

\[\mathcal{L}_{\mathrm{cl}}(\Phi) :=\mathbb{E}_{x\sim\mathrm{P_{0}}}\mathbb{E}_{a_{1},a_{2}\sim \mathrm{P_{A}}(\,\cdot\mid x)}\,\,\left\lVert\Phi(a_{1})-\Phi(a_{2})\right\rVert _{2}^{2}\] s.t. \[\left\lVert\mathbb{E}_{a\sim\mathrm{P_{A}}}\left[\Phi(a)\Phi(a)^ {\top}\right]-\mathbf{I}_{\mathbf{k}}\right\rVert_{F}^{2}\leqslant\rho\] (3)Keeping the \(\Phi_{\mathrm{cl}}\) fixed, we then learn a linear classifier \(h_{\mathrm{cl}}\) over \(\Phi_{\mathrm{cl}}\) to minimize the exponential loss on labeled source data (refer to as _linear probing_). For STOC, keeping the \(\Phi_{\mathrm{cl}}\) fixed and initializing the linear head with the CL linear probe (instead of source only ERM), we perform ST with (2).

**Example 1**.: _For the setup in (1), we choose \(\gamma=0.5\), \(\sigma_{\mathrm{sp}}^{2}=1\)., and \(\sigma_{\mathrm{in}}^{2}=0.05\) with \(d_{\mathrm{in}}=5\) and \(d_{\mathrm{sp}}=20\) for our running example. \(\nicefrac{{\gamma}}{{\sqrt{d_{\mathrm{sp}}}}}\) controls signal to noise ratio in the source such that spurious feature is easy-to-learn and the invariant feature is harder-to-learn. \(\sigma_{2}\) controls the noise in target which we show later is critical in unlearning the spurious feature with CL._

### Simulations and Intuitive Story: A Comparative Study Between SSL and DA

Our setup captures real-world trends in UDA setting.Our toy setup (in Example 1) accentuates the behaviors observed on real-world datasets (Fig. 2(a)): (i) both ERM and ST yield close to random performance (though ST performs slightly worse than ERM); (ii) CL improves over ERM but still yields sub-optimal target performance; (iii) STOC then further improves over CL, achieving near-optimal target performance. Note that, a linear predictor can improve target performance only by reducing its dependence on spurious feature \(x_{\mathrm{sp}}\), and increasing it on invariant feature \(x_{\mathrm{in}}\) (along \(w^{\star}\)). Given this, we can explain our trends if we understand the following: (i) how ST reduces dependence on spurious feature when done after CL; (ii) why CL helps reduce but not completely eliminate the reliance of linear head on spurious features. Before we present intuitions, we ablate over a key problem parameter that affects both the target performance and conditions for ST to work.

Effect of \(\gamma/\sigma_{\mathrm{sp}}\) on success of ST.By increasing the ratio of margin \(\gamma\) and variance of spurious feature on target \(\sigma_{\mathrm{sp}}\) (keeping others constant), the problem becomes easier because \(\gamma\) directly affects the signal on \(x_{\mathrm{in}}\) and reducing \(\sigma_{\mathrm{sp}}\) helps ST to unlearn \(x_{\mathrm{sp}}\) (see App. D.3). In Fig. 2(c), we see that a phase transition occurs for ST, _i.e._, after a certain threshold of \(\gamma/\sigma_{\mathrm{sp}}\), ST successfully recovers the optimal target predictor. This hints that ST has a binary effect, where beyond a certain magnitude of \(\gamma/\sigma_{\mathrm{sp}}\), ST can amplify the signal on domain invariant feature to obtain optimal target predictor. On the other hand, the performance of CL and ERM improve gradually where CL achieves high performance even at small ratios of \(\gamma/\sigma_{\mathrm{sp}}\). One way of viewing this trend with CL is that it magnifies the effective \(\gamma/\sigma_{\mathrm{sp}}\) in its representation space, because of which a linear head trained these representations have a good performance at low values of the ratio. Consequently, the _phase transition_ of STOC occurs much sooner than that of ST. Finally, we note that for CL the rate of performance increase diminishes at high values of \(\gamma/\sigma_{\mathrm{sp}}\) because CL fails to reduce dependency along \(x_{\mathrm{sp}}\) beyond a certain point.

An intuitive story.We return to the question of why self-training improves over contrastive learning under distribution shift in our Example 1. When the classifier at initialization of ST relies more on spurious features, ST aggravates this dependency. However, as the problem becomes easier (with increasing \(\gamma/\sigma_{\mathrm{sp}}\)), the source-only ERM classifier will start relying more on invariant rather than spurious feature. Once this ERM classifier is sufficiently accurate on the target, ST unlearns any dependency on spurious features achieving optimal target performance. In contrast, we observe that CL performs better than ERM but is still sub-optimal. This implies that CL ends up decreasing reliance on spurious features (as compared to ERM) but doesn't completely eliminate them. Combining ST and CL, a natural hypothesis explaining our trends is that CL provides a "favorable" initialization

Figure 2: _Our simplified model of shift captures real-world trends and theoretical behaviors:_ **(a)** Target (OOD) accuracy separation in the UDA setup (for problem parameters in Example 1). **(b)** Comparison of the benefits of STOC (ST over CL) over just CL in UDA and SSL settings, done across training iterations for contrastive pretraining. **(c)** Comparison between different methods in UDA setting, as we vary problem parameters \(\gamma\) and \(\sigma_{\mathrm{sp}}\), connecting our theory results in Sec. 4.

for ST by sufficiently increasing signal on invariant features. Our empirical findings emphasize an intriguing contrast suggesting that ST and CL improve target performance in complementary ways.

Why disparate behaviors for out-of-distribution vs. in distribution?In the SSL setup, recall, there is no distribution shift. In Example 1, we sample \(50k\) unlabeled data and \(100\) labeled data from the same (target) distribution to simulate SSL setup. Substantiating our findings on real-world data, we observe that STOC provides a small to negligible improvement over CL (refer to App. D). To understand why such disparate behaviors emerge, recall that in the UDA setting, the main benefit of STOC lies in picking up reliance on "good" features for OOD data, facilitated by CL initialization. While contrastive pretraining uncovers features that are "good" for OOD data, it also learns more predictive source-only features (which are not predictive at all on target). As a result, linear probing with source-labeled data picks up these source-only features, leaving considerable room for improvement on OOD data with further self-training. On the other hand, in the SSL setting, the limited ID labeled data might provide enough signal to pick up features predictive on ID data, leaving little to no room for improvement for further self-training. Corroborating our intuitions, throughout the CL training in the toy setup, when CL doesn't achieve near-perfect generalization, the improvements provided by STOC for each checkpoint remain minimal. On the other hand, for UDA setup, after reaching a certain training checkpoint in CL, STOC yields significant improvement (Fig. 2(b)).

In the next sections, we formalize our intuitions and analyze why ST and CL offer complementary benefits when dealing with distribution shifts. Formal statements and proofs are in App. E.

### Conditions for Success and Failure of Self-training over ERM from Scratch

In our results on Example 1, we observe that performing ST after ERM yields a classifier with near-random target accuracy. In Theorem 2, we characterize conditions under which ST fails and succeeds.

**Theorem 2** (Informal; Conditions for success and failure of ST over ERM).: _The target accuracy of ERM classifier, is given by \(0.5\cdot\operatorname{erfc}\left({-\nicefrac{{\gamma}}{{2}}}/{(\sqrt{2d_{ \operatorname{sp}}},\sigma_{\operatorname{sp}})}\right)\). Then ST performed in the second stage yields: (i) a classifier with \(\approx 0.5\) target accuracy when \(\gamma<\nicefrac{{1}}{{2}}\sigma_{\operatorname{sp}}\) and \(\sigma_{\operatorname{sp}}\geq 1\); and (ii) a classifier with near-perfect target accuracy when \(\gamma\geq\sigma_{\operatorname{sp}}\)._

The informal theorem above abstracts the exact dependency of \(\gamma,\sigma_{\operatorname{sp}}\), and \(d_{\operatorname{sp}}\) for the success and failure of ST over ERM. Our analysis highlights that while ERM learns a perfect predictor along \(w_{\operatorname{in}}\) (with norm \(\gamma\)), it also learns to depend on \(w_{\operatorname{sp}}\) (with norm \(\sqrt{d_{\operatorname{sp}}}\)) because of the perfect correlation of \(x_{\operatorname{sp}}\) with labels on the source. Our conditions depict that when the \(\gamma/\sigma_{\operatorname{sp}}\) is sufficiently small, then ST continues to erroneously enhance its reliance on the \(x_{\operatorname{sp}}\) feature for target prediction, resulting in near-random target performance. Conversely, when \(\gamma/\sigma_{\operatorname{sp}}\) is larger than 1, the signal in \(x_{\operatorname{in}}\) is correctly used for predictor on the majority of target points, and ST eliminates the \(x_{\operatorname{sp}}\) dependency, converging to an optimal target classifier.

Our proof analysis shows that if the ratio of the norm of the classifier along in the direction of \(w^{\star}\) is smaller than \(w_{\operatorname{sp}}\) by a certain ratio then the generated pseudolabels (incorrectly) use \(x_{\operatorname{sp}}\) for its prediction further increasing the component along \(w_{\operatorname{sp}}\). Moreover, normalization further diminishes the reliance along \(w^{\star}\), culminating in a near-random performance. The opposite occurs when the ERM classifier achieves a signal along \(w^{\star}\) that is sufficiently stronger than along \(w_{\operatorname{sp}}\). Upon substituting the parameters used in Example 1, the ERM and ST performances as determined by Theorem 2 align with our empirical results, notably, ST performance on target being near-random.

### CL Captures Both Features But Amplifies Invariant Over Spurious Features

Here we show that minimizing the contrastive loss (3) on unlabeled data from both \(\operatorname{P_{\mathsf{S}}}\) and \(\operatorname{P_{\mathsf{T}}}\) gives us a feature extractor \(\Phi_{\operatorname{cl}}\) that has a higher inner product with the invariant feature over the spurious feature. First, we derive a closed form expression for \(\Phi_{\operatorname{cl}}\) that holds for any linear backbone and augmentation distribution. Then, we introduce assumptions on the augmentation distribution (or equivalently on \(w^{\star}\)) and other problem parameters, that are sufficient to prove amplification.

**Proposition 3** (Barlow Twins solution).: _The solution for (3) is \(U_{k}^{\top}\Sigma_{\mathsf{A}}^{-1/2}\) where \(U_{k}\) are the top \(k\) eigenvectors of \(\Sigma_{\mathsf{A}}^{-1/2}\operatorname{\widetilde{\Sigma}}\Sigma_{\mathsf{A}} ^{-1/2}\). Here, \(\Sigma_{\mathsf{A}}\coloneqq\mathbb{E}_{a\sim\operatorname{P_{\mathsf{A}}}}[aa ^{\top}]\) is the covariance over augmentations, and \(\operatorname{\widetilde{\Sigma}}\coloneqq\mathbb{E}_{x\sim\operatorname{P_{ \mathsf{U}}}}[\widetilde{a}(x)\widetilde{a}(x)^{\top}]\) is the covariance matrix of mean augmentations \(\widetilde{a}(x)\coloneqq\mathbb{E}_{\operatorname{P_{\mathsf{A}}}(a|x)}[a]\)._

The above result captures the effect of augmentations through the matrix \(U_{k}\). If there were no augmentations, then \(\Sigma_{\mathsf{A}}=\operatorname{\widetilde{\Sigma}}\), implying that \(U_{k}\) could then be any random orthonormal matrix. Onthe other hand if augmentation distributions change prevalent covariances in the data, _i.e._, \(\Sigma_{\text{A}}\) is very different from \(\widehat{\Sigma}\), the matrix \(U_{k}\) would bias the CL solution towards directions that capture significant variance in marginal distribution on augmented data, but have low conditional variance, when conditioned on original point \(x\)--precisely the directions with low invariance loss. Hence, we can expect that CL would learn components along both invariant \(w_{\text{in}}\) and spurious \(w_{\text{sp}}\) because: (i) these directions explain a large fraction of variance in the raw data; (ii) augmentations that randomly scale down dimensions would add little variance along \(w_{\text{sp}}\) and \(w_{\text{in}}\) compared to noise directions in their null space. On the other hand it is unclear which of these directions is amplified more in \(\Phi_{\text{cl}}\). The following assumption and amplification result conveys that when the noise in target \((\sigma_{\text{sp}})\) is sufficiently large, the CL solution amplifies the invariant feature over the spurious feature.

**Assumption 4** (Informal; Alignment of \(w^{\star}\) with augmentations).: _We assume that \(w^{\star}\) aligns with \(\operatorname{P_{A}}(\cdot\mid x)\), i.e., \(\forall x\), \(\mathbb{E}_{a\mid x}[a^{\top}w^{\star}]=\nicefrac{{1}}{{2}}\cdot x^{\top} \mathrm{diag}(\mathbf{1}_{d})w^{\star}\) is high. Hence, we assume \(w^{\star}=\nicefrac{{1}}{{4_{\text{in}}}}\big{/}\sqrt{d_{\text{in}}}\)._

One implication of Assumption 4 is that when \(w^{\star}=\nicefrac{{1}}{{4_{\text{in}}}}\big{/}\sqrt{d_{\text{in}}}\), only the top two eigenvectors lie in the space spanned by \(w_{\text{in}}\) and \(w_{\text{sp}}\). To analyze our amplification with fewer eigenvectors from Proposition 3 while retaining all relevant phenomena, we assume \(w^{\star}=\nicefrac{{1}}{{4_{\text{in}}}}\big{/}\sqrt{d_{\text{in}}}\) for mathematical convenience. While Assumption 4 permits a tighter theoretical analysis, our empirical results in Sec. 4.1 hold more generally for \(w^{\star}\sim\mathcal{N}(0,\mathbf{I}_{d_{\text{in}}})\).

**Theorem 5** (Informal; CL recovers both \(w_{\text{in}}\) and \(w_{\text{sp}}\) but amplifies \(w_{\text{in}}\)).: _Under Assumption 4, the CL solution \(\Phi_{\text{cl}}\)=\([\phi_{1},\phi_{2},...,\phi_{k}]\) satisfies \(\phi_{j}^{\top}w_{\text{in}}=\phi_{j}^{\top}w_{\text{sp}}=0\)\(\forall j\geqslant 3\), \(\phi_{1}=c_{1}w_{\text{in}}+c_{3}w_{\text{sp}}\) and \(\phi_{2}=c_{2}w_{\text{in}}+c_{4}w_{\text{sp}}\). For constants \(K_{1},K_{2}>0\), \(\gamma=\nicefrac{{K_{1}K_{2}}}{{\sigma_{\text{sp}}}}\), \(d_{\text{sp}}=\nicefrac{{\sigma_{\text{sp}}^{2}}}{{\kappa^{2}}}\), \(\forall\epsilon>0\), \(\exists\sigma_{\text{sp}_{0}}\), such that for \(\sigma_{\text{sp}}\geqslant\sigma_{\text{sp}_{0}}\), \(\nicefrac{{\left|c_{1}/c_{3}\right.}}{{c_{3}}}-\nicefrac{{K_{1}K_{2}^{2}d_{ \text{in}}}}{{2L\sigma_{\text{in}}}}(d_{\text{in}}-1)\big{|}\leqslant\epsilon\), and \(\big{|}\nicefrac{{\left|c_{2}/c_{4}\right|}}{{-L\sqrt{d_{\text{sp}}/\gamma}}} \big{|}\leqslant\epsilon\), where \(L=1+K_{2}^{2}\)._

We analyze the amplification of \(w_{\text{in}}/w_{\text{sp}}\) with contrastive learning in the regime where \(\sigma_{\text{sp}}\) is large enough. In other words, if the target distribution has sufficient noise along the spurious feature, the augmentations prevent the CL solution from extracting components along \(w_{\text{sp}}\). Thus, in our analysis, we first analyze the amplification factors asymptotically \((\sigma_{\text{sp}}\to\infty)\), and then use the asymptotic behavior to draw conclusions for the regime where \(\sigma_{\text{sp}}\) is large but finite.

Theorem 5 conveys two results: (i) CL recovers components along both \(w_{\text{in}}\) and \(w_{\text{sp}}\) through \(\phi_{1},\phi_{2}\); and (ii) it increases the norm along \(w_{\text{in}}\) more than \(w_{\text{sp}}\). The latter is evident because the margin separating labeled points along \(w_{\text{in}}\) is now amplified by a factor of \(\nicefrac{{\left|c_{2}/c_{4}\right|}}{{\Omega(L\sqrt{d_{\text{sp}}}/\gamma)}}\) in \(\phi_{2}\). Naturally, this will improve the target performance of a linear predictor trained over CL representations. At the same time, we also see that in \(\phi_{1}\), the component along \(w_{\text{sp}}\) is still significant (\(\nicefrac{{c_{1}/c_{3}}}{{c_{3}}}=\mathcal{O}(\nicefrac{{1}}{{L\sigma_{\text {in}}^{2}}})\)). Intuitively, CL prefers the invariant feature since augmentations amplify the noise along \(w_{\text{sp}}\) in the target domain. At the same time, the variance induced by augmentations along \(w_{\text{sp}}\) in source is still very small due to which the dependence on \(w_{\text{sp}}\) is not completely alleviated. Due to the remaining components along \(w_{\text{sp}}\), the target performance for CL can remain less than ideal. Both the above arguments on target performance are captured in Corollary 6.

**Corollary 6** (Informal; CL improves OOD error over ERM but is still imperfect).: _For \(\gamma,\sigma_{\text{sp}},d_{\text{sp}}\) defined as in Theorem 5, \(\exists\sigma_{\text{sp}1}\) such that for all \(\sigma_{\text{sp}}\geqslant\sigma_{\text{sp}1}\), the target accuracy of CL (linear predictor on \(\Phi_{\text{cl}}\)) is \(\geqslant 0.5\operatorname{erfc}\left({-L^{\prime}\cdot\nicefrac{{\gamma}}{{ \sqrt{2}\sigma_{\text{sp}}}}}\right)\) and \(\leqslant 0.5\operatorname{erfc}\left({-L^{\prime}\cdot\nicefrac{{\gamma}}{{ \sqrt{2}\sigma_{\text{sp}}}}}\right)\), where \(L^{\prime}=\nicefrac{{K_{1}^{2}K_{1}}}{{\gamma_{\text{in}}^{2}(1-\nicefrac{{ 1}}{{4_{\text{in}}}})}}\). When \(\sigma_{\text{sp}1}>\sigma_{\text{in}}\sqrt{1-\nicefrac{{1}}{{d_{\text{in}}}}}\), the lower bound on accuracy is strictly better than ERM from scratch._

While \(\Phi_{\text{cl}}\) is still not ideal for linear probing, in the next part we will see how \(\Phi_{\text{cl}}\) can instead be sufficient for subsequent self-training to unlearn the remaining components along spurious features.

### Improvements with Self-training Over Contrastive Learning

The result in the previous section highlights that while CL may improve over ERM, the linear probe continues to depend on the spurious feature. Next, we characterize the behavior STOC. Recall, in the ST stage, we iteratively update the linear head with (2) starting with the CL backbone and head.

**Theorem 7** (Informal; ST improves over CL).: _Under the conditions of Theorem 5 and \(d_{\text{sp}}\leqslant K_{1}^{2}\cdot K_{2}^{2/3}\), the target accuracy of ST over CL is lower bounded by \(0.5\cdot\operatorname{erfc}\left({-\left|{c2/c4}\right|\cdot\nicefrac{{\gamma}}{{ \sqrt{(\sqrt{2}\sigma_{\text{sp}})}}}}\right)\approx 0.5\cdot\operatorname{erfc}\left({-L\sqrt{d_{\text{in}}}} \big{/}\sqrt{\nicefrac{{\gamma}}{{\sqrt{2}\sigma_{\text{sp}}}}}\right)\) where \(c_{2}\) and \(c_{4}\) are the coefficients of feature \(\phi_{2}\) along \(w_{\text{in}}\) and \(w_{\text{sp}}\) learned by BT._The above theorem states that when \(\sqrt{d_{w}}/{\sigma_{\mathrm{sp}}}\gg 1\) the target accuracy of ST over CL is close to 1. In Example 1, the lower bound of the accuracy of ST over CL is \(\mathrm{erfc}\left(-\sqrt{10}\right)\approx 2\) showing near-perfect target generalization. Recall that Theorem 6 shows that CL yields a linear head that mainly depends on both the invariant direction \(w_{\mathrm{in}}\) and the spurious direction \(w_{\mathrm{sp}}\). At initialization, the linear head trained on the CL backbone has negligible dependence on \(\phi_{2}\) (under conditions in Theorem 6). Building on that, the analysis in Theorem 7 captures that ST gradually reduces the dependence on \(w_{\mathrm{sp}}\) by learning a linear head that has a larger reliance on \(\phi_{2}\), which has a higher "effective" margin on the target, thus increasing overall dependency on \(w_{\mathrm{in}}\).

Theoretical comparison with SSL.Our analysis until now shows that linear probing with source labeled data during CL picks up features that are more predictive of source label under distribution shift, leaving a significant room for improvement on OOD data when self-trained further. In UDA, the primary benefit of ST lies in picking up the features with a high "effective" margin on target data that are not picked up by linear head trained during CL. In contrast, in the SSL setting, the limited ID labeled data may provide enough signal in picking up high-margin features that are predictive on ID data, leaving little to no room for improvement for further ST. We formalize this intuition in App. E.

### Reconciling Practice: Implications for Deep Non-Linear Networks

In this section, we experiment with deep non-linear backbone (_i.e_., \(\Phi_{\mathrm{cl}}\)). When we continue to fix \(\Phi_{\mathrm{cl}}\) during CL and STOC, the trends we observed with linear networks in Sec. 4.1 continue to hold. We then perform full fine-tuning with CL and STOC, i.e., propagate gradients even to \(\Phi_{\mathrm{cl}}\), as commonly done in practice. We present key takeaways here but detailed experiments are in App. D.4.

Benefits of augmentation for self-training.ST while updating \(\Phi_{\mathrm{cl}}\) can hurt due to overfitting issues when training with the finite sample of labeled and unlabeled data (drop by >10% over CL). This is due to the ability of deep networks to overfit on confident but incorrect pseudolabels on target data [94]. This exacerbates components along \(w_{\mathrm{sp}}\) and we find that augmentations (and other heuristics) typically used in practice (_e.g_. in FixMatch [79]) help avoid overfitting on incorrect pseudolabels.

Can ERM and ST over contrastive pretraining improve features?We find that self-training can also slightly improve features when we update the backbone with the second stage of STOC and when the CL backbone is early stopped sub-optimally (_i.e_. at an earlier checkpoint in Fig. 2(b)). This feature finetuning can now widen the gap between STOC and CL in SSL settings, as compared to the linear probing gap (as in 2). This is because STOC can now improve performance beyond just recovering the generalization gap for the linear head (which is typically small). However, STOC benefits are negligible when CL is not early stopped sub-optimally, _i.e_., trained till convergence. Thus, it remains unclear if STOC and CL have complementary benefits for feature learning in UDA or SSL settings. Investigating this is an interesting avenue for future work.

## 5 Connecting Experimental Gains with Theoretical Insights

Our theory emphasizes that under distribution shift contrastive pretraining improves the representations for target data, while self-training primarily improves linear classifiers learned on top. To investigate different methods in our UDA setup, we study the representations learned by each of them. We fix the representations and train linear heads over them to answer two questions: (i) How good are the representations in terms of their _ceiling_ of target accuracy (performance of the optimal linear probe)?--we evaluate this by training the classifier head on target labeled data (_i.e_., target linear probe); and (ii) How well do heads trained on source generalize to target?--we assess this by training a head on source labeled data (source linear probe) and evaluate its difference with target linear probe. For both, we plot target accuracy. We make two _intriguing_ observations Fig. 3):

**Does CL improve representations over ERM features?** Yes. We observe a substantial difference in accuracy (\(\approx 14\%\) gap) of target linear probes on backbones trained

Figure 3: _Target accuracy with source and target linear probes, which freezes backbones trained with various objectives and trains only the head in UDA setup. Avg. accuracy across all datasets. We observe that: (i) ST improves the linear transferability of source probes, and (ii) CL improves representations._

with contrastive pretraining (_i.e_. CL, STOC) and without it (_i.e_., ERM, ST) highlighting that CL significantly pushes the performance ceiling over non-contrastive features. As a side, our findings also stand in contrast to recent studies suggesting that ERM features might be "good enough" for OOD generalization [67; 46]. Instead, the observed gains with contrastively pretrained backbones (_i.e_. CL, STOC) demonstrate that target unlabeled data can be leveraged to further improve over ERM features.

**Do CL features yield _perfect_ linear transferability from source to target?** Recent works [40; 76] conjecture that under certain conditions CL representations, linear probes learned with source labeled data may transfer perfectly from source to target. However, we observe that this doesn't hold strictly in practice, and in fact, the linear transferability can be further improved with ST. We first note a significant gap between the performance of source linear probes and target linear probes illustrating that linear transferability is not perfect in practice. Moreover, while the accuracy of target linear probes doesn't change substantially between CL and STOC, the accuracy of the source linear probe improves significantly. Similar observations hold for ERM and ST, methods trained without contrastive pretraining. This highlights that ST performs "feature refinement" to improve source to target linear transfer (with relatively small improvements in their respective target probe performance). _The findings highlight the complementary nature of benefits on real-world data: ST improves linear transferability while CL improves representations._

## 6 Connections to Prior Work

Our empirical results and our analyses offer a perspective that contrasts with the prior literature that argues for the individual optimality of contrastive pretraining and self-training. We outline the key differences from existing studies here, and delve into other related works in App. A.

**Limitations of prior work analyzing contrastive learning** Prior works [40; 76] analyzing CL first make assumptions on the consistency of augmentations with labels [39; 11; 73; 44], and specifically for UDA make stronger ones on the augmentation graph connecting examples from same domain or class more than cross-class/cross-domain ones. While this is sufficient to prove linear transferability, it is unclear if this holds in practice when augmentations are imperfect, _i.e_. if they fail to mask the spurious features completely--as corroborated by our findings in Sec. 5. We show why this also fails in our simplified setup in App. F.1.

**Limitations of prior work analyzing self-training** Prior research views self-training as consistency regularization, ensuring pseudolabels for original samples align with their augmentations [12; 87; 79]. This approach abstracts the role played by the optimization algorithm and instead evaluates the global minimizer of a population objective promoting pseudolabel consistency. It also relies on specific assumptions about class-conditional distributions to guarantee pseudolabel accuracy across domains. However, this framework doesn't address issues in iterative label propagation. For example, when augmentation distribution has long tails, the consistency of pseudolabels depends on the sampling frequency of "favorable" augmentations (for more discussion see App. F.2). Our analysis thus follows the iterative examination of self-training [17].

## 7 Conclusion

In this study, we highlight the synergistic behavior of self-training and contrastive pretraining under distribution shift. Shifts in distribution are commonplace in real-world applications of machine learning, and even under natural, non-adversarial distribution shifts, the performance of machine learning models often drops. By simply combining existing techniques in self-training and constrastive learning, we find that we can improve accuracy by 3-8% rather than using either approach independently. Despite these significant improvements, we note that one limitation of this combined approach is that performing self-training sequentially after contrastive pretraining increases the computation cost for UDA. The potential for integrating these benefits into one unified training paradigm is yet unclear, presenting an interesting direction for future exploration.

Beyond this, we note that our theoretical framework primarily confines the analysis to training the backbone and linear network independently during the pretraining and fine-tuning/self-training phases. Although our empirical observations apply to deep networks with full fine-tuning, we leave a more rigorous theoretical study of full fine-tuning for future work. Our theory also relies on a covariate shift assumption (where we assume that label distribution also doesn't shift). Investigating the complementary nature of self-training and contrastive pretraining beyond the covariate shift assumption would be another interesting direction for future work.

## Acknowledgements

SG acknowledges the JP Morgan AI Ph.D. Fellowship and Bloomberg Ph.D. Fellowship for their support.

## References

* Alexandari et al. [2021] Alexandari, A., Kundaje, A., and Shrikumar, A. (2021). Adapting to label shift with bias-corrected calibration. In _International Conference on Machine Learning (ICML)_.
* Arora et al. [2019] Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N. (2019). A theoretical analysis of contrastive unsupervised representation learning. _arXiv preprint arXiv:1902.09229_.
* Azizzadenesheli et al. [2019] Azizzadenesheli, K., Liu, A., Yang, F., and Anandkumar, A. (2019). Regularized learning for domain adaptation under label shifts. In _International Conference on Learning Representations (ICLR)_.
* Bardes et al. [2021] Bardes, A., Ponce, J., and LeCun, Y. (2021). Vicreg: Variance-invariance-covariance regularization for self-supervised learning. _arXiv preprint arXiv:2105.04906_.
* Baricz [2008] Baricz, A. (2008). Mills' ratio: Monotonicity patterns and functional inequalities. _Journal of Mathematical Analysis and Applications_, 340(2):1362-1370.
* Bekker and Davis [2020] Bekker, J. and Davis, J. (2020). Learning from positive and unlabeled data: a survey. _Machine Learning_.
* Ben-David et al. [2010] Ben-David, S., Lu, T., Luu, T., and Pal, D. (2010). Impossibility Theorems for Domain Adaptation. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_.
* Berthelot et al. [2019] Berthelot, D., Carlini, N., Cubuk, E. D., Kurakin, A., Sohn, K., Zhang, H., and Raffel, C. (2019). Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. _arXiv preprint arXiv:1911.09785_.
* Bishop [2006] Bishop, C. M. (2006). _Pattern Recognition and Machine Learning_. Springer.
* Blanchard et al. [2011] Blanchard, G., Lee, G., and Scott, C. (2011). Generalizing from several related classification tasks to a new unlabeled sample. _Advances in neural information processing systems_, 24.
* Cabannes et al. [2023] Cabannes, V., Kiani, B. T., Balestriero, R., LeCun, Y., and Bietti, A. (2023). The ssl interplay: Augmentations, inductive bias, and generalization. _arXiv preprint arXiv:2302.02774_.
* Cai et al. [2021] Cai, T., Gao, R., Lee, J., and Lei, Q. (2021). A theory of label propagation for subpopulation shift. In _International Conference on Machine Learning_, pages 1170-1182. PMLR.
* Caron et al. [2020] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A. (2020). Unsupervised learning of visual features by contrasting cluster assignments. _Advances in Neural Information Processing Systems_, 33:9912-9924.
* Caron et al. [2021] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660.
* Chapelle et al. [2006] Chapelle, O., Scholkopf, B., and Zien, A. (2006). Semi-supervised learning. 2006. _Cambridge, Massachusetts: The MIT Press View Article_, 2.
* Chen et al. [2020a] Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020a). A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR.
* Chen et al. [2020b] Chen, X., Chen, W., Chen, T., Yuan, Y., Gong, C., Chen, K., and Wang, Z. (2020b). Self-pu: Self boosted and calibrated positive-unlabeled training. In _International Conference on Machine Learning_, pages 1510-1519. PMLR.

* Christie et al. (2018) Christie, G., Fendley, N., Wilson, J., and Mukherjee, R. (2018). Functional map of the world. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_.
* Cortes et al. (2010) Cortes, C., Mansour, Y., and Mohri, M. (2010). Learning Bounds for Importance Weighting. In _Advances in Neural Information Processing Systems (NIPS)_.
* Cortes and Mohri (2014) Cortes, C. and Mohri, M. (2014). Domain adaptation and sample bias correction theory and algorithm for regression. _Theoretical Computer Science_, 519.
* Cubuk et al. (2020) Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. (2020). Randaugment: Practical automated data augmentation with a reduced search space. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_, pages 702-703.
* Darlow et al. (2018) Darlow, L. N., Crowley, E. J., Antoniou, A., and Storkey, A. J. (2018). Cinic-10 is not imagenet or cifar-10. _arXiv preprint arXiv:1810.03505_.
* Deledalle et al. (2017) Deledalle, C.-A., Denis, L., Tabti, S., and Tupin, F. (2017). _Closed-form expressions of the eigen decomposition of 2 x 2 and 3 x 3 Hermitian matrices_. PhD thesis, Universite de Lyon.
* DeVries and Taylor (2017) DeVries, T. and Taylor, G. W. (2017). Improved regularization of convolutional neural networks with cutout. _arXiv preprint arXiv:1708.04552_.
* Elkan and Noto (2008) Elkan, C. and Noto, K. (2008). Learning classifiers from only positive and unlabeled data. In _International Conference Knowledge Discovery and Data Mining (KDD)_, pages 213-220.
* Ganin et al. (2016) Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lempitsky, V. (2016). Domain-adversarial training of neural networks. _The journal of machine learning research_.
* Gardner et al. (2018) Gardner, J., Pleiss, G., Weinberger, K. Q., Bindel, D., and Wilson, A. G. (2018). Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Garg et al. (2022a) Garg, S., Balakrishnan, S., and Lipton, Z. (2022a). Domain adaptation under open set label shift. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Garg et al. (2022b) Garg, S., Balakrishnan, S., Lipton, Z., Neyshabur, B., and Sedghi, H. (2022b). Leveraging unlabeled data to predict out-of-distribution performance. In _International Conference on Learning Representations (ICLR)_.
* Garg et al. (2023) Garg, S., Erickson, N., Sharpnack, J., Smola, A., Balakrishnan, S., and Lipton, Z. (2023). Rlsbench: A large-scale empirical study of domain adaptation under relaxed label shift. In _International Conference on Machine Learning (ICML)_.
* Garg et al. (2020) Garg, S., Wu, Y., Balakrishnan, S., and Lipton, Z. (2020). A unified view of label shift estimation. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Garg et al. (2021) Garg, S., Wu, Y., Smola, A., Balakrishnan, S., and Lipton, Z. (2021). Mixture proportion estimation and PU learning: A modern approach. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Garrido et al. (2022) Garrido, Q., Chen, Y., Bardes, A., Najman, L., and Lecun, Y. (2022). On the duality between contrastive and non-contrastive self-supervised learning. _arXiv preprint arXiv:2206.02574_.
* Grandvalet and Bengio (2006) Grandvalet, Y. and Bengio, Y. (2006). Entropy regularization.
* Gretton et al. (2009) Gretton, A., Smola, A. J., Huang, J., Schmittfull, M., Borgwardt, K. M., and Scholkopf, B. (2009). Covariate Shift by Kernel Mean Matching. _Journal of Machine Learning Research (JMLR)_.
* Grill et al. (2020) Grill, J.-B., Strub, F., Altche, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. (2020). Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284.

* [37] Gulrajani, I. and Lopez-Paz, D. (2020). In search of lost domain generalization. _arXiv preprint arXiv:2007.01434_.
* [38] HaoChen, J. Z. and Ma, T. (2022). A theoretical study of inductive biases in contrastive learning. _arXiv preprint arXiv:2211.14699_.
* [39] HaoChen, J. Z., Wei, C., Gaidon, A., and Ma, T. (2021). Provable guarantees for self-supervised deep learning with spectral contrastive loss. _Advances in Neural Information Processing Systems_, 34:5000-5011.
* [40] HaoChen, J. Z., Wei, C., Kumar, A., and Ma, T. (2022). Beyond separability: Analyzing the linear transferability of contrastive representations to related subpopulations. _arXiv preprint arXiv:2204.02683_.
* [41] He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9729-9738.
* [42] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep Residual Learning for Image Recognition. In _Computer Vision and Pattern Recognition (CVPR)_.
* [43] Joachims, T. et al. (1999). Transductive inference for text classification using support vector machines. In _Icml_, volume 99, pages 200-209.
* [44] Johnson, D. D., Hanchi, A. E., and Maddison, C. J. (2022). Contrastive learning can find an optimal basis for approximately view-invariant functions. _arXiv preprint arXiv:2210.01883_.
* [45] Kakade, S. M., Sridharan, K., and Tewari, A. (2008). On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. _Advances in neural information processing systems_, 21.
* [46] Kirichenko, P., Izmailov, P., and Wilson, A. G. (2022). Last layer re-training is sufficient for robustness to spurious correlations. _arXiv preprint arXiv:2204.02937_.
* [47] Koh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R. L., Gao, I., Lee, T., David, E., Stavness, I., Guo, W., Earnshaw, B. A., Haque, I. S., Beery, S., Leskovec, J., Kundaje, A., Pierson, E., Levine, S., Finn, C., and Liang, P. (2021). WILDS: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning (ICML)_.
* [48] Krizhevsky, A. and Hinton, G. (2009). Learning Multiple Layers of Features from Tiny Images. Technical report, Citeseer.
* [49] Kschischang, F. R. (2017). The complementary error function. _Online, April_.
* [50] Kumar, A., Ma, T., and Liang, P. (2020). Understanding self-training for gradual domain adaptation. In _International Conference on Machine Learning_, pages 5468-5479. PMLR.
* [51] Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., and Liang, P. (2022). Fine-tuning can distort pretrained features and underperform out-of-distribution. In _International Conference on Learning Representations_.
* [52] Lee, D.-H. et al. (2013). Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In _Workshop on challenges in representation learning, ICML_, volume 3, page 896.
* [53] Lipton, Z. C., Wang, Y.-X., and Smola, A. (2018). Detecting and Correcting for Label Shift with Black Box Predictors. In _International Conference on Machine Learning (ICML)_.
* [54] Long, M., Cao, Y., Wang, J., and Jordan, M. (2015). Learning transferable features with deep adaptation networks. In _International conference on machine learning_, pages 97-105. PMLR.
* [55] Long, M., Zhu, H., Wang, J., and Jordan, M. I. (2017). Deep transfer learning with joint adaptation networks. In _International conference on machine learning_. PMLR.

* Loshchilov and Hutter [2016] Loshchilov, I. and Hutter, F. (2016). Sgdr: Stochastic gradient descent with warm restarts. _arXiv preprint arXiv:1608.03983_.
* Ma et al. [2021] Ma, M. Q., Tsai, Y.-H. H., Liang, P. P., Zhao, H., Zhang, K., Salakhutdinov, R., and Morency, L.-P. (2021). Conditional contrastive learning for improving fairness in self-supervised learning. _arXiv preprint arXiv:2106.02866_.
* Mishra et al. [2021] Mishra, S., Saenko, K., and Saligrama, V. (2021). Surprisingly simple semi-supervised domain adaptation with pretraining and consistency. _arXiv preprint arXiv:2101.12727_.
* Muandet et al. [2013] Muandet, K., Balduzzi, D., and Scholkopf, B. (2013). Domain generalization via invariant feature representation. In _International Conference on Machine Learning_, pages 10-18. PMLR.
* Nagarajan et al. [2020] Nagarajan, V., Andreassen, A., and Neyshabur, B. (2020). Understanding the failure modes of out-of-distribution generalization. _arXiv preprint arXiv:2010.15775_.
* Oord et al. [2018] Oord, A. v. d., Li, Y., and Vinyals, O. (2018). Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_.
* Peng et al. [2019] Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B. (2019). Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1406-1415.
* Peng et al. [2017] Peng, X., Usman, B., Kaushik, N., Hoffman, J., Wang, D., and Saenko, K. (2017). Visda: The visual domain adaptation challenge.
* Peng et al. [2018] Peng, X., Usman, B., Saito, K., Kaushik, N., Hoffman, J., and Saenko, K. (2018). Syn2real: A new benchmark forsynthetic-to-real visual domain adaptation. _arXiv preprint arXiv:1806.09755_.
* Quinonero-Candela et al. [2008] Quinonero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, N. D. (2008). _Dataset shift in machine learning_. Mit Press.
* Roberts et al. [2022] Roberts, M., Mani, P., Garg, S., and Lipton, Z. (2022). Unsupervised learning under latent label shift. In _Advances in Neural Information Processing Systems (NeurIPS)_.
* Rosenfeld et al. [2022] Rosenfeld, E., Ravikumar, P., and Risteski, A. (2022). Domain-adjusted regression or: Erm may already learn features sufficient for out-of-distribution generalization. _arXiv preprint arXiv:2202.06856_.
* Russakovsky et al. [2015] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. (2015). Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115(3):211-252.
* Saerens et al. [2002] Saerens, M., Latinne, P., and Decaestecker, C. (2002). Adjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure. _Neural Computation_.
* Sagawa et al. [2021] Sagawa, S., Koh, P. W., Lee, T., Gao, I., Xie, S. M., Shen, K., Kumar, A., Hu, W., Yasunaga, M., Marklund, H., Beery, S., David, E., Stavness, I., Guo, W., Leskovec, J., Saenko, K., Hashimoto, T., Levine, S., Finn, C., and Liang, P. (2021). Extending the wild's benchmark for unsupervised adaptation. In _NeurIPS Workshop on Distribution Shifts_.
* Sagawa et al. [2020] Sagawa, S., Raghunathan, A., Koh, P. W., and Liang, P. (2020). An investigation of why overparameterization exacerbates spurious correlations. In _International Conference on Machine Learning_, pages 8346-8356. PMLR.
* Santurkar et al. [2021] Santurkar, S., Tsipras, D., and Madry, A. (2021). Breeds: Benchmarks for subpopulation shift. In _International Conference on Learning Representations (ICLR)_.
* Saunshi et al. [2022] Saunshi, N., Ash, J., Goel, S., Misra, D., Zhang, C., Arora, S., Kakade, S., and Krishnamurthy, A. (2022). Understanding contrastive learning requires incorporating inductive biases. In _International Conference on Machine Learning_, pages 19250-19286. PMLR.
* Scholkopf et al. [2012] Scholkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., and Mooij, J. (2012). On Causal and Anticausal Learning. In _International Conference on Machine Learning (ICML)_.

* [75] Scudder, H. (1965). Probability of error of some adaptive pattern-recognition machines. _IEEE Transactions on Information Theory_, 11(3):363-371.
* [76] Shen, K., Jones, R. M., Kumar, A., Xie, S. M., HaoChen, J. Z., Ma, T., and Liang, P. (2022). Connect, not collapse: Explaining contrastive learning for unsupervised domain adaptation. In _International Conference on Machine Learning_, pages 19847-19878. PMLR.
* [77] Shimodaira, H. (2000). Improving Predictive Inference Under Covariate Shift by Weighting the Log-Likelihood Function. _Journal of Statistical Planning and Inference_.
* [78] Shu, R., Bui, H. H., Narui, H., and Ermon, S. (2018). A dirt-t approach to unsupervised domain adaptation. _arXiv preprint arXiv:1802.08735_.
* [79] Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C. A., Cubuk, E. D., Kurakin, A., and Li, C.-L. (2020). Fixmatch: Simplifying semi-supervised learning with consistency and confidence. _Advances in Neural Information Processing Systems_, 33.
* [80] Stewart, G. W. (1993). On the early history of the singular value decomposition. _SIAM review_, 35(4):551-566.
* [81] Sun, B., Feng, J., and Saenko, K. (2017). Correlation alignment for unsupervised domain adaptation. In _Domain Adaptation in Computer Vision Applications_. Springer.
* [82] Sun, B. and Saenko, K. (2016). Deep coral: Correlation alignment for deep domain adaptation. In _European conference on computer vision_. Springer.
* [83] Torralba, A. and Efros, A. A. (2011). Unbiased look at dataset bias. In _CVPR 2011_, pages 1521-1528. IEEE.
* [84] Van Engelen, J. E. and Hoos, H. H. (2020). A survey on semi-supervised learning. _Machine learning_, 109(2):373-440.
* [85] Venkateswara, H., Eusebio, J., Chakraborty, S., and Panchanathan, S. (2017). Deep hashing network for unsupervised domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5018-5027.
* [86] Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. (2021). Tent: Fully test-time adaptation by entropy minimization. In _International Conference on Learning Representations_.
* [87] Wei, C., Shen, K., Chen, Y., and Ma, T. (2020). Theoretical analysis of self-training with deep networks on unlabeled data. _arXiv preprint arXiv:2010.03622_.
* [88] Wu, Z., Xiong, Y., Yu, S. X., and Lin, D. (2018). Unsupervised feature learning via non-parametric instance discrimination. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3733-3742.
* [89] Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. (2020a). Self-training with noisy student improves imagenet classification. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10687-10698.
* [90] Xie, X., Chen, J., Li, Y., Shen, L., Ma, K., and Zheng, Y. (2020b). Self-supervised cyclegan for object-preserving image-to-image domain adaptation. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XX 16_, pages 498-513. Springer.
* [91] Yang, X., Song, Z., King, I., and Xu, Z. (2022). A survey on deep semi-supervised learning. _IEEE Transactions on Knowledge and Data Engineering_.
* [92] Zadrozny, B. (2004). Learning and Evaluating Classifiers Under Sample Selection Bias. In _International Conference on Machine Learning (ICML)_.
* [93] Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. (2021). Barlow twins: Self-supervised learning via redundancy reduction. In _International Conference on Machine Learning_, pages 12310-12320. PMLR.

* Zhang et al. [2017] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. In _International Conference on Learning Representations (ICLR)_.
* Zhang et al. [2021] Zhang, J., Menon, A., Veit, A., Bhojanapalli, S., Kumar, S., and Sra, S. (2021). Coping with label shift via distributionally robust optimisation. In _International Conference on Learning Representations (ICLR)_.
* Zhang et al. [2013] Zhang, K., Scholkopf, B., Muandet, K., and Wang, Z. (2013). Domain Adaptation Under Target and Conditional Shift. In _International Conference on Machine Learning (ICML)_.
* Zhang [2019] Zhang, R. (2019). Making convolutional networks shift-invariant again. In _ICML_.
* Zhang et al. [2018] Zhang, W., Ouyang, W., Li, W., and Xu, D. (2018). Collaborative and adversarial network for unsupervised domain adaptation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_.
* Zhang et al. [2019] Zhang, Y., Liu, T., Long, M., and Jordan, M. (2019). Bridging theory and algorithm for domain adaptation. In _International Conference on Machine Learning_. PMLR.
* Zhu and Ghahramani [2003] Zhu, X. and Ghahramani, Z. (2003). Learning from labeled and unlabeled data with label propagation. _CMU CALD tech report CMU-CALD-02-107, 2002_.

## Appendix

### Appendix Table of Contents

* A Other Related Works
* B More Details on Problem Setup
* C Additional Experiments and Details
* C.1 Additional setup and notation
* C.2 Dataset details
* C.3 Method details
* C.4 Additional UDA experimemts
* C.5 Additional SSL experimemts
* C.6 Other experimental details
* D Additional Results in Toy Setup
* D.1 Detailed description of our simplified setup
* D.2 Discussion on self-training and contrastive learning objectives
* D.3 Additional empirical results in our simplified setup
* D.4 Reconciling Practice: Experiments with deep networks in toy setup
* E Formal Statements from Sec. 4
* E.1 Analysis of ERM and ST: Formal Statement of Theorem 2
* E.1.1 Proof of Proposition 3
* E.1.2 Analysis with \(\rho>0\) in Contrastive Pretraining Objective (3)
* E.1.3 Proof of Theorem 5
* E.1.4 Proof of Corollary 6
* E.2 Analysis of STOC: Formal Statement of Theorem 7
* E.3 Analysis for SSL
* F Limitations of Prior Work
* F.1 Contrastive learning analysis
* F.2 Self-training analysis
* G Additional Lemmas

## Appendix A Other Related Works

Unsupervised domain adaption.Without assumption on the nature of shift, UDA is underspecified [7]. This challenge has been addressed in various ways by researchers. One approach is to investigate additional structural assumptions under which UDA problems are well posed [77, 74]. Popular settings for which DA is well-posed include (i) _covariate shift_[96, 92, 19, 20, 35] where \(p(x)\) can change from source to target but \(p(y|x)\) remains invariant; and (ii) _label shift_[95, 53, 3, 1, 31, 95, 66, 30] where the label marginal \(p(y)\) can change but \(p(x|y)\) is shared across source and target. Principled methods with strong theoretical guarantees exists for adaptation under these settings when targetdistribution's support is a subset of the source support. Other works [25; 6; 32; 28] extend the label shift setting to scenarios where previously unseen classes may appear in the target and \(p(x|y)\) remains invariant among seen classes. A complementary line of research focuses on constructing benchmarks to develop heuristics for incorporating the unlabeled target data, relying on benchmark datasets ostensibly representative of "real-world shifts" to adjudicate progress [72; 85; 70; 62; 63]. As a result, various benchmark-driven heuristics have been proposed [54; 55; 82; 81; 99; 98; 26; 79]. Our work engages with the latter, focusing on two popular methods: self-training and contrastive pretraining.

Domain generalization.In domain generalization, the model is given access to data from multiple different domains and the goal is to generalize to a previously unseen domain at test time [10; 59]. For a survey of different algorithms for domain generalization, we refer the reader to Gulrajani and Lopez-Paz [37]. A crucial distinction here is that unlike the domain generalization setting, in DA problems, we have access to unlabeled examples from the test domain.

Semi-supervised learning.To learn from a small amount of labeled supervision, semi-supervised learning methods leverage unlabeled data alongside to improve learning models. One of the seminal works in SSL is the pseudolabeling method [75], where a classifier is trained on the labeled data and then used to classify the unlabeled data, which are then added to the training set. The work of Zhu and Ghahramani [100] built on this by introducing graph-based methods, and the transductive SVMs [43] presented an SVM-based approach. More recent works have focused on deep learning techniques, and similar to UDA, self-training and contrastive pretraining have emerged as two prominent choices. We delve into these methods in greater detail in the following paragraphs. For a discussion on other SSL methods, we refer interested readers to [15; 84; 91].

Self-training.Two popular forms of self-training are pseudolabeling [52] and conditional entropy minimization [34], which have been observed to be closely connected [8; 52; 79; 78]. Motivated by its strong performance in SSL and UDA settings [79; 89; 30; 78], several theoretical works have made attempts to understand its behavior [50; 87; 17]. [87; 12] aims to understand the behavior of the global minimizer of self-training objective by studying input consistency regularization, which enforces stability of the prediction for different augmentations of the unlabeled data. Our analysis of self-training is motivated by the work of Chen et al. [17] which explores the iterative behavior of self-training to unlearn spurious features. The setting of spurious features is of particular interest, since prior works have specifically analyzed the failures of out-of-distribution generalization in the presence of spurious features [60; 71].

Contrastive learning.An alternate line of work that uses unlabeled data for learning representations in the pretraining stage is contrastive learning [36; 61; 13; 16; 88]. Given an augmentation distribution, the main goal of contrastive objectives is to map augmentations drawn from the same input (positive pairs) to similar features, and force apart features corresponding to augmentations of different inputs (negative pairs) [13; 14; 41]. Prior works [11; 44; 38] have also shown a close relationship between contrastive [16; 39] and non-contrastive objectives [4; 93]. Consequently, in our analysis pertaining to the toy setup we focus on the mathematically non-contrastive objective Barlow Twins [93]. Using this pretrained backbone (either as an initialization or as a fixed feature extractor) a downstream predictor is learned using labeled examples. Several works [39; 73; 38; 2; 44] have analyzed the in-distribution generalization of the downstream predictor via label consistency arguments on the graph of positive pairs (augmentation graph). In contrast, we study the impact of contrastive learning under distribution shifts in the UDA setup. Other works [76; 40] that examine contrastive learning for UDA also conform to the augmentation graph view point, making additional assumptions that guarantee linear transferability. In our simplified setup involving spurious correlations, these abstract assumptions break easily when the augmentations are of a generic nature, akin to practice. Finally, some empirical works [58; 57] have found self-supervised objectives like contrastive pretraining to reduce dependence on spurious correlations. Corroborating their findings, we extensively evaluate the complementary benefits of contrastive learning and self-training on real-world datasets. Finding differing results in SSL and UDA settings, we further examine their behavior theoretically in our toy setup.

## Appendix B More Details on Problem Setup

In this section, we elaborate on our setup and methods studied in our work.

Unsupervised Domain Adaptation (UDA).We assume that we are given labeled data from the _source_ distribution and unlabeled data from a shifted, _target_ distribution, with the goal of performing well on target data. We assume that the source and target distributions have the same label marginals \(\mathrm{P_{S}}(y)=\mathrm{P_{T}}(y)\) (_i.e._, no label proportion shift) and the same Bayes optimal predictor, _i.e._, \(\operatorname*{arg\,max}_{y}p_{\mathrm{S}}(y\mid x)=\operatorname*{arg\,max}_{y }p_{\mathrm{T}}(y\mid x)\). Here, even with infinite labeled source data, the challenge lies in generalizing out-of-distribution. In experiments, we assume access to finite data but in theory, we assume population access to labeled source and unlabeled target.

Semi-Supervised Learning (SSL).Here, there is no distribution shift, _i.e._, \(\mathrm{P_{S}}=\mathrm{P_{T}}=\mathrm{P_{U}}\). We are given a small number of labeled examples and a comparatively large amount of unlabeled examples, both drawn from the same distribution. Without loss of generality, we denote this distribution with \(\mathrm{P_{T}}\). The goal in SSL is to generalize in-distribution. The challenge is primarily due to limited access to labeled data. Here, in experiments, we assume limited access to labeled data but a comparatively larger amount of unlabeled in-distribution data. In theory, we assume population access to unlabeled data but limited labeled examples.

Methods.As discussed in the main paper, we compare four methods for learning with labeled and unlabeled data. Table 8 summarizes the main methods and key differences between those methods in UDA and SSL setup. For exact implementation in our experiments, we refer reader to App. C.3.

## Appendix C Additional Experiments and Details

### Additional setup and notation

Recall, our goal is to learn a predictor that maps inputs \(x\in\mathcal{X}\subseteq\mathbb{R}^{d}\) to outputs \(y\in\mathcal{Y}\). We parameterize predictors \(f=h\circ\Phi:\mathbb{R}^{d}\mapsto\mathcal{Y}\), where \(\Phi:\mathbb{R}^{d}\mapsto\mathbb{R}^{k}\) is a feature map and \(h\in\mathbb{R}^{k}\) is a classifier that maps the representation to the final scores or logits. With \(A:\mathcal{X}\to\mathcal{A}\), we denote the augmentation function that takes in an input \(x\) and outputs an augmented view of the input \(A(x)\). Unless specified otherwise, we perform full-finetuning in all of our experiments on real-world data. That is, we backpropagate gradients in both the linear head \(h\) and the backbone \(\phi\). For UDA, we denote source labeled points as \(\{(x_{i},y_{i})\}_{i=1}^{n}\) and target unlabeled points as \(\{(x^{\prime}_{i})\}_{i=1}^{m}\). For SSL, we use the same notation for labeled and unlabeled in-distribution data.

### Dataset details

For both UDA and SSL, we conduct experiments across eight benchmark datasets. Each of these datasets consists of domains, enabling us to construct source-target pairs for UDA. The adopted source and target domains are standard to previous studies [76; 30; 70]. Because the SSL setting lacks distribution shift, we do not need to worry about domain designations and default to using source alone. To simulate limited supervision in SSL, we sub-sample the original labeled training set to 10%. Below provide exact details about the datasets used in our benchmark study.

* **CIFAR10** We use the original CIFAR10 dataset [48] as the source dataset. For target domains, we consider CINIC10 [22] which is a subset of Imagenet restricted to CIFAR10 classes and downsampled to 32\(\times\)32.
* **FMoW** In order to consider distribution shifts faced in the wild, we consider FMoW-WILDs [47; 18] from Wilds benchmark, which contains satellite images taken in different geographical regions and at different times. We use the original train as source and OOD val and OOD test splits as target domains as they are collected over different time-period. Overall, we obtain 3 different domains (1 source and 2 targets).
* **BREEDs** We also consider BREEDs benchmark [72] in our setup to assess robustness to subpopulation shifts. BREEDs leverage class hierarchy in ImageNet [68] to re-purpose original classes to be the subpopulations and defines a classification task on superclasses. We consider distribution shift due to subpopulation shift which is induced by directly making the subpopulations present in the training and test distributions disjoint. BREEDs benchmark contains 4 datasets **Entity-13**, **Entity-30**, **Living-17**, and **Non-living-26**, each focusing on different subtrees and levels in the hierarchy. Overall, for each of the 4 BREEDs datasets (i.e., Entity-13, Entity-30, Living17, and Non-living-26), we obtain one different domain which we consider as target. We refer to source and target as follows: BREEDs sub-population 1, BREEDs sub-population 2.
* **OfficeHome** We use four domains (art, clipart, product and real) from OfficeHome dataset [85]. We use the product domain as source and the other domains as target.
* **Visda** We use three domains (train, val and test) from the Visda dataset [64, 63]. While 'train' domain contains synthetic renditions of the objects, 'val' and 'test' domains contain real world images. To avoid confusing, the domain names with their roles as splits, we rename them as'synthetic', 'Real-1' and 'Real-2'. We use the synthetic (original train set) as the source domain and use the other domains as target.

We summarize the information about source and target domains in Table 3.

Train-test splitsWe partition each source and target dataset into \(80\%\) and \(20\%\) i.i.d. splits. We use \(80\%\) splits for training and \(20\%\) splits for evaluation (or validation). We throw away labels for the \(80\%\) target split and only use labels in the \(20\%\) target split for final evaluation. The rationale behind splitting the target data is to use a completely unseen batch of data for evaluation. This avoids evaluating on examples where a model potentially could have overfit. over-fitting to unlabeled examples for evaluation. In practice, if the aim is to make predictions on all the target data (i.e., transduction), we can simply use the (full) target set for training and evaluation.

**Simulating SSL settings and limited supervision.** For SSL settings, we choose the in-distribution domain as the source domain. To simulate limited supervision in SSL, we sub-sample the original labeled training set to 10% and use all the original dataset as unlabeled data. For evaluation, we further split the original holdout set into two partitions (one for validation and the other to report final accuracy numbers).

### Method details

For implementation, we build on top of WILDs [70] and RLSbench [30] open source libraries.

Figure 4: Examples from all the domains in each dataset.

ERM (Source only) training.We consider Empirical Risk Minimization (ERM) on the labeled source data as a baseline. Since this simply ignores the unlabeled target data, we call this as source only training. As mentioned in the main paper, we perform source only training with data augmentations. Formally, we minimize the following ERM loss:

\[L_{\text{source only}}(f)=\frac{1}{n}\sum_{i=1}^{n}\ell(f(A(x_{i}),y_{i}))\,,\] (4)

where \(A\) is the stochastic data augmentation operation and \(\ell\) is a loss function. For SSL, the ERM baseline only uses the small of labeled data available.

Contrastive Learning (CL).We perform contrastive pretraining on the unlabeled dataset to obtain the backbone \(\phi_{\text{cl}}\). And then we perform full fine-tuning with source labeled data by initializing the backbone with \(\phi_{\text{cl}}\). We use SwAV [13] for contrastive pretraining. The main idea behind SwAV is to train a model to identify different views of the same image as similar, while also ensuring that it finds different images to be distinct. This is accomplished through a _swapped_ prediction mechanism, where the goal is to compute a code from an augmented version of the image and predict this code from other augmented versions of the same image. In particular, given two image features \(\phi(x^{\prime}_{a1})\) and \(\phi(x^{\prime}_{a2})\) from two different augmentations of the same image \(x^{\prime}\), i.e., \(x^{\prime}_{a1},x^{\prime}_{a2}\sim A(x^{\prime})\), SwAV computes their codes \(z_{a1}\) and \(z_{a2}\) by matching the features to a set of \(K\) prototypes \(\{c_{1},\cdots,c_{K}\}\). Then SwAV minimizes the following loss such that \(\phi(x^{\prime}_{a1})\) can compute codes \(z_{a2}\) and \(\phi(x^{\prime}_{a2})\) can compute codes \(z_{a1}\):

\[L_{\text{SwAV}}(\phi)=\sum_{i=1}^{m}\sum_{x^{\prime}_{i,a1},x^{\prime}_{i,a2} \sim A(x^{\prime}_{i})}\ell^{\prime}(\phi(x^{\prime}_{i,a1}),z_{i,a2})+\ell^{ \prime}(\phi(x^{\prime}_{i,a2}),z_{i,a1})\,,\] (5)

where \(\ell^{\prime}\) computes KL-divergence between codes computed with features (e.g. \(\phi(x_{a1})\)) and the code computed by another view (e.g. \(z_{a2}\)). For more details about the algorithm, we refer the reader to Caron et al. [13]. In all UDA settings, unless otherwise specified, we pool all the (unlabeled) data from the source and target to perform SwAV. For SSL, we leverage in-distribution unlabeled data.

We employ SimCLR [16] for the CIFAR10 dataset, aligning with previous studies that have utilized contrastive pretraining on the same dataset [51, 76]. The reason for this choice is that SwAV relies on augmentations that involve cropping images to a smaller resolution, making it more suitable for datasets with larger resolutions beyond \(32\times 32\).

Self-Training (ST).For self-training, we apply FixMatch [79], where the loss on labeled data and on pseudolabeled unlabeled data are minimized simultaneously. Sohn et al. [79] proposed FixMatch as a variant of the simpler Pseudo-label method [52]. This algorithm dynamically generates psuedolabels and overfits on them in each batch. FixMatch employs consistency regularization on the unlabeled data. In particular, while pseudolabels are generated on a weakly augmented view of

\begin{table}
\begin{tabular}{l c c} \hline \hline Dataset & Source & Target \\ \hline CIFAR10 & CIFAR10v1 & CINIC10 \\ FMoW & FMoW (2002–’13) & FMoW (2013–’16), FMoW (2016–’18) \\ Entity13 & Entity13 (sub-population 1) & Entity13 (sub-population 2) \\ Entity30 & Entity30 (sub-population 1) & Entity30 (sub-population 2), \\ Living17 & Living17 (sub-population 1) & Living17 (sub-population 2), \\ Nonliving26 & Nonliving26 (sub-population 1) & Nonliving26 (sub-population 2), \\ Officehome & Product & Product, Art, ClipArt, Real \\  & Synthetic & Synthetic, Real-1 (originally referred to as val), \\ Visda & (originally referred to as train) & Real-2 (originally referred to as test) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Details of source and target sets in each dataset considered in our testbed.

[MISSING_PAGE_FAIL:22]

### Additional SSL experiments

### Other experimental details

Augmentations.For weak augmentation, we leverage random horizontal flips and random crops of pre-defined size. For SwAV, we also perform multicrop augmentation as proposed in Caron et al. [13]. For strong augmentation, we apply the following transformations sequentially: random horizontal flips, random crops of pre-defined size, augmentation with Cutout [24], and RandAugment [21]. For the exact implementation of RandAugment, we directly use the implementation of Sohn et al. [79]. Unless specified otherwise, for all methods, we default to using strong augmentation techniques.

Architectures.In our work, we experiment with Resnet18, Resnet50 [42] trained from scratch (_i.e_. random initialization). We do not consider off-the-shelf pretrained models (_e.g_., on Imagenet [68]) to avoid confounding our conclusions about contrastive pretraining. However, we note that our results on most datasets tend to be comparable to and sometimes exceed those obtained with ImageNet pretrained models. For BREEDs datasets, we employ Resnet18 architecture. For other datasets, we train a Resnet50 architecture.

Except for Resnets on CIFAR dataset, we used the standard pytorch implementation [27]. For Resnet on Cifar, we refer to the implementation here: https://github.com/kuangliu/pytorch-cifar. For all the architectures, whenever applicable, we add antialising [97]. We use the official library released with the paper.

Hyperparameters.For all the methods, we fix the algorithm-specific hyperparameters to the original recommendations. For UDA, given that the setup precludes access to labeled data from the target distribution, we use source hold-out performance to pick the best hyperparameters. During pretraining, early stopping is done according to lower values of pretraining loss.

We tune the learning rate and \(\ell_{2}\) regularization parameter by fixing the batch size for each dataset that corresponds to the maximum we can fit to 15GB GPU memory. We default to using cosine learning rate schedule [56]. We set the number of epochs for training as per the suggestions of the authors of respective benchmarks. For SSL, we run both ERM and FixMatch for approximately \(2000\) epochs. Note that we define the number of epochs as a full pass over the labeled training source data. We summarize the learning rate, batch size, number of epochs, and \(\ell_{2}\) regularization parameter used in our study in Table 7.

Compute infrastructure.Our experiments were performed across a combination of Nvidia T4, A6000, and V100 GPUs.

## Appendix D Additional Results in Toy Setup

In this section we will first give more details on our simplified setup that captures both contrastive pretraining and self-training in the same framework. Then, we provide some additional empirical results that are not captured theoretically but mimic behaviors observed in real world settings, highlighting the richness of our setup.

### Detailed description of our simplified setup

In this subsection, we will first re-iterate the problem setup in Sec. 4 and provide some comparisons between our setup and those in closely related works. We will then describe the four methods: ERM,

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Method & Living17 & Nonliv26 & Entity13 & Entity30 & FMoW & Visda & OH & CIFAR \\ \hline ERM & \(76.8_{\pm 0.1}\) & \(64.9_{\pm 0.2}\) & \(80.1_{\pm 0.0}\) & \(70.4_{\pm 0.3}\) & \(33.6_{\pm 0.4}\) & \(99.2_{\pm 0.0}\) & \(32.0_{\pm 0.2}\) & \(85.5_{\pm 0.1}\) \\ ST & \(85.4_{\pm 0.1}\) & \(75.7_{\pm 0.2}\) & \(85.4_{\pm 0.2}\) & \(77.3_{\pm 0.1}\) & \(33.6_{\pm 0.3}\) & \(99.2_{\pm 0.1}\) & \(32.0_{\pm 0.1}\) & \(93.1_{\pm 0.1}\) \\ CL & \(91.1_{\pm 0.5}\) & \(84.6_{\pm 0.6}\) & \(90.7_{\pm 0.4}\) & \(85.5_{\pm 0.3}\) & \(43.1_{\pm 0.2}\) & \(97.6_{\pm 0.3}\) & \(49.7_{\pm 0.2}\) & \(91.7_{\pm 0.2}\) \\ STOC (ours) & \(92.0_{\pm 0.1}\) & \(85.8_{\pm 0.2}\) & \(91.3_{\pm 0.3}\) & \(86.1_{\pm 0.2}\) & \(44.4_{\pm 0.1}\) & \(97.7_{\pm 0.2}\) & \(49.9_{\pm 0.2}\) & \(93.06_{\pm 0.3}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: _Results in the SSL setup_. We report accuracy on hold-out ID data. Recall that SSL uses labeled and unlabeled data from the same distribution during training.

ST, CL, and STOC, providing details on the exact estimates returned by these algorithms in the SSL and UDA settings.

Data distribution.We consider binary classification and model the inputs as consisting of two kinds of features: \(x=[x_{\mathrm{in}},x_{\mathrm{sp}}]\) where \(x_{\mathrm{in}}\in\mathbb{R}^{d_{\mathrm{in}}}\) is the invariant feature that is predictive of the label across both source \(\mathrm{P_{S}}\) and target \(\mathrm{P_{T}}\) and \(x_{\mathrm{sp}}\in\mathbb{R}^{d_{\mathrm{sp}}}\) is the spurious feature that is correlated with the label \(y\) only on the source domain \(\mathrm{P_{S}}\) but uncorrelated with label \(y\) in \(\mathrm{P_{T}}\). Here, \(x_{\mathrm{in}}\in\mathbb{R}^{d_{\mathrm{in}}}\) determines the label using the ground truth classifier \(w^{\star}\sim\mathrm{Unif}(\mathbb{S}^{d_{\mathrm{in}}-1})\), and \(x_{\mathrm{sp}}\in\mathbb{R}^{d_{\mathrm{sp}}}\) is strongly correlated with the label on source but random noise on target. Formally, we sample \(\mathsf{y}\sim\mathrm{Unif}\{-1,1\}\) and generate inputs \(x\) conditioned on \(\mathsf{y}\) as follows

\[\mathrm{P_{S}}:\;\;x_{\mathrm{in}}\sim\mathcal{N}(\gamma\cdot y \mathsf{w}^{\star},\Sigma_{\mathrm{in}})\;\;\;x_{\mathrm{sp}}=\mathsf{y} \mathds{1}_{d_{\mathrm{sp}}}\] \[\mathrm{P_{T}}:\;\;x_{\mathrm{in}}\sim\mathcal{N}(\gamma\cdot y \mathsf{w}^{\star},\Sigma_{\mathrm{in}})\;\;\;x_{\mathrm{sp}}\sim\mathcal{N} (\mathbf{0},\Sigma_{\mathrm{sp}}),\] (6)

where \(\gamma\) is the margin afforded by the invariant feature. We set covariance of the invariant features \(\Sigma_{\mathrm{in}}=\sigma_{\mathrm{in}}^{2}\cdot(\mathbf{I}_{d_{\mathrm{in}} }-w^{\star}{w^{\star}}^{\top})\) to capture structure in the invariant feature that the variance is less along the latent predictive direction \(w^{\star}\). Note that the spurious feature is completely predictive of the label in the source data, and is distributed as spherical Gaussian in the target data with \(\Sigma_{\mathrm{sp}}=\sigma_{\mathrm{sp}}^{2}\mathbf{I}_{d_{\mathrm{sp}}}\).

Why is our simplified setup interesting?In our setup, \(x_{\mathrm{in}}\) is the hard to learn feature that generalizes from source to target. The hardness of learning this feature is determined by the value of the margin \(\gamma\) and how it compares with size of the spurious feature (\(\sqrt{d_{\mathrm{sp}}}\)). Since, \(\gamma/\sqrt{d_{\mathrm{sp}}}\) is small in our setup, \(x_{\mathrm{in}}\) is much harder to learn on source data (even with population access) compared to the spurious feature \(x_{\mathrm{sp}}\) which generalizes poorly from source to target. These two types of features have been captured in similar analysis on spurious correlations [71, 60] since it imitates pitfalls emanating from the presence of spurious features in real world datasets (_e_.\(g\)., the easy to learn background feature in image classification problems). While this setup is simple, it is also expressive enough to elucidate both self-training and contrastive learning behaviors we observe in real world settings. Specifically, it captures the separation results we observe in Sec. 3.

Differences of our setup with prior works.While our distribution shift settings bears the above similarities it also has important differences with works analyzing self-training and contrastive pretraining individually. Chen et al. [17] analyze the iterative nature of self-training algorithm, where the premise is that we are given a classifier that not only has good performance on source data but in addition does not rely too much on the spurious feature. Under the strong condition of small norms along the spurious feature, they show that self-training can provably unlearn this small dependence when the target data along the spurious feature is random noise. This assumption is clearly violated in setups where the spurious correlation is strong (as in our toy setup), \(i\)._e_., the dependence on the spurious feature is rather large (much larger than that on the invariant feature) for any classifier that is trained directly on source data. Consequently, we show the need for "good" pretrained representations from contrastive pretraining over which if we train a linear predictor (using source labeled data), it will provably have a reduced "effective" dependence on the spurious feature.

Using an augmentation distribution similar to ours, Saunshi et al. [73] carried out contrastive pretraining analysis with the backbone belonging to a capacity constrained function class (similar analysis also in [40]). Our setup differs from this in two key ways: (i) we specifically consider a distribution

\begin{table}
\begin{tabular}{l c c c} \hline \hline Dataset & Batch size & \(\ell_{2}\) regularization set & Learning rate set \\ \hline CIFAR10 & 200 & \(\{0.001,0.0001,10^{-5},0.0\}\) & \(\{0.2,0.1,0.05,0.01,0.003,0.001\}\) \\ FMoW & 64 & \(\{0.001,0.0001,10^{-5},0.0\}\) & \(\{0.01,0.003,0.001,0.0003,0.0001\}\) \\ Entity13 & 256 & \(\{0.001,0.0001,10^{-5},0.0\}\) & \(\{0.4,0.2,0.1,0.05,0.02,0.01,0.005\}\) \\ Entity30 & 256 & \(\{0.001,0.0001,10^{-5},0.0\}\) & \(\{0.4,0.2,0.1,0.05,0.02,0.01,0.005\}\) \\ Entity30 & 256 & \(\{0.001,0.0001,10^{-5},0.0\}\) & \(\{0.4,0.2,0.1,0.05,0.02,0.01,0.005\}\) \\ Nonliving26 & 256 & \(\{0.001,0.0001,10^{-5},0.0\}\) & \(\{0.4,0.2,0.1,0.05,0.02,0.01,0.005\}\) \\ Officehome & 96 & \(\{0.001,0.0001,10^{-5},0.0\}\) & \(\{0.01,0.003,0.001,0.0003,0.0001\}\) \\ Visda & 96 & \(\{0.001,0.0001,10^{-5},0.0\}\) & \(\{0.03,0.01,0.003,0.001,0.0003\}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Details of the batch size, learning rate set and \(\ell_{2}\) regularization set considered in our testbed.

shift from source to target. Unlike their setting, it is not sufficient to make augmentations consistent with ground truth labels, since the predictor that uses just the spurious feature also assigns labels consistent with both ground truth predictions and augmentations on the source data; and (ii) our augmentation distribution assumes no knowledge of the invariant feature, which is why we augment all dimensions uniformly, as opposed to selectively augmenting a set of dimensions. In other words, we assume no knowledge of the structure of the optimal target predictor. For _e.g._, if we had knowledge of the spurious dimensions we could have just selectively augmented those. Assuming knowledge of these perfect augmentations is not ideal for two reasons: (a) it makes the problem so easy that just training an ERM model on source data with these augmentations would already yield a good target predictor (which rarely happens in practice); and (b) in real-world datasets perfect augmentations for the downstream task are not known. Hence, we stick to generic augmentations in our setup.

### Discussion on self-training and contrastive learning objectives

In text we will describe our objectives and methods for the UDA setup. In Table 8 we constrast the differences in the methods and objectives for SSL and UDA setups. Recall from Section 2 that we learn linear classifiers \(h\) over features extractors \(\Phi\). We consider linear feature extractor i.e. \(\Phi\) is a matrix in \(\mathbb{R}^{k\times d}\). For mathematical convenience, we assume access to infinite unlabeled data and hence replace the empirical quantities over unlabeled data with their population counterpart. In the UDA setting, we further assume access to infinite labeled data from the source. Note that due to distribution shift between source and target, "ERM" on infinite labeled data from the source does not necessarily achieve optimal performance on the target. For binary classification, we assume that the linear layer \(h\) maps features to a scalar in \(\mathbb{R}\) such that the prediction is \(\operatorname{sgn}(h^{\top}\Phi x)\). We use the exponential loss \(\ell(f(x),y)=\exp\left(-yf(x)\right)\) as the classification loss.

_Contrastive pretraining._ We obtain \(\Phi_{\text{cl}}\coloneqq\operatorname*{arg\,min}_{\Phi}\mathcal{L}_{\text{ cl}}(\Phi)\) by minimizing the Barlow Twins objective [93], which prior works have shown is also equivalent to spectral contrastive and non-contrastive objectives [33, 11]. In Sec. 4, we consider a constrained form of Barlow Twins in (3) which enforces representations of different augmentations \(a_{1},a_{2}\) of the same input \(x\) to be close in representation space, while ensuring feature diversity by staying in the constraint set. We assume a strict constraint on regularization \((\rho=0)\) for the theoretical arguments in the rest of the main paper. In App. E.1.2 we prove that all our claims hold for small \(\rho\) as well. In (7), we redefine the pretraining objective with a regularization term (instead of a constraint set) where \(\kappa\) controls the strength of the regularization term, with higher values of \(\kappa\) corresponding to stronger constraints on feature diversity. We then learn a linear classifier \(h_{\text{cl}}\) over \(\Phi_{\text{cl}}\) to minimize the exponential loss on labeled source data.

\[\mathcal{L}_{\text{cl}}(\Phi)\ \coloneqq\ \mathbb{E}_{x\sim\text{P}_{\text{U}}} \mathbb{E}_{a_{1},a_{2}\sim\text{P}_{\text{A}}(\cdot|x)}\left\|\Phi(a_{1})- \Phi(a_{2})\right\|_{2}^{2}\ \ +\ \ \kappa\cdot\left\|\mathbb{E}_{a\sim\text{P}_{\text{A}}}\left[\Phi(a)\Phi(a)^ {\top}\right]-\mathbf{I}_{k}\right\|_{F}^{2}\] (7)

\begin{table}
\begin{tabular}{l c c} Method & **UDA Setup** & **SSL Setup** \\ \hline
**ERM**: & \(h_{\text{erm}}=\operatorname*{arg\,min}_{h}\mathbb{E}_{\text{P}_{\text{S}}}\ell(h (x),y)\) & \(h_{\text{erm}}=\operatorname*{arg\,min}_{h}\frac{1}{h}\sum_{i=1}^{n}\ell(h(x_{i}),y_{i})\) \\  & & \(\{(x_{i},y_{i})\}_{i=1}^{n}\sim\text{P}_{\text{T}}{}^{n}\) \\ \hline
**ST**: & Starting from \(h_{\text{erm}}\) optimize over \(h\) (to get \(h_{\text{st}}\)): & Starting from \(h_{\text{erm}}\) optimize over \(h\) (to get \(h_{\text{st}}\)): \\  & \(\mathbb{E}_{\text{P}_{\text{T}}(x)}\ell(h(x),\operatorname{sgn}(h(x)))\) & \(\mathbb{E}_{\text{P}_{\text{T}}(x)}\ell(h(x),\operatorname{sgn}(h(x)))\) \\ \hline
**CL**: & \(\Phi_{\text{cl}}=\operatorname*{arg\,min}_{\phi}\mathcal{L}_{\text{cl}}(\Phi)\) & \(\Phi_{\text{cl}}=\operatorname*{arg\,min}_{\phi}\mathcal{L}_{\text{cl}}(\Phi)\) \\  & Use \((\text{P}_{\text{S}}(x)+\text{P}_{\text{T}}(x))/2\) for \(\mathcal{L}_{\text{cl}}(\Phi)\) & Use \(\text{P}_{\text{T}}(x)\) for \(\mathcal{L}_{\text{cl}}(\Phi)\) \\  & \(h_{\text{cl}}=\operatorname*{arg\,min}_{h}\mathbb{E}_{\text{P}_{\text{S}}}\ell(h \circ\Phi_{\text{cl}}(x),y)\) & \(h_{\text{cl}}=\operatorname*{arg\,min}_{h}\frac{1}{n}\sum_{i=1}^{n}\ell(h\circ \Phi_{\text{cl}}(x_{i}),y_{i})\) \\ \hline \end{tabular} 
\begin{tabular}{l c} \hline \multirow{2}{*}{**STOC**:} & Starting from \(h_{\text{cl}}\) optimize over \(h\) (to get \(h_{\text{stoc}}\)): & Starting from \(h_{\text{cl}}\) optimize over \(h\) (to get \(h_{\text{stoc}}\)): \\  & \(\mathbb{E}_{\text{P}_{\text{T}}(x)}\ell(h\circ\Phi_{\text{cl}}(x), \operatorname{sgn}(h\circ\Phi_{\text{cl}}(x)))\) & \(\mathbb{E}_{\text{P}_{\text{T}}(x)}\ell(h\circ\Phi_{\text{cl}}(x), \operatorname{sgn}(h\circ\Phi_{\text{cl}}(x)))\) \\ \hline \end{tabular}
\end{table}
Table 8: **Description of methods for SSL vs. UDA**: For each method we provide exact objectives used for experiments and analysis in the SSL and UDA setups (pertaining to Sec. 4).

Augmentations.Data augmentations play a key role in contrastive pre-training (and also as we see later, state-of-the-art self-training variants like FixMatch). Given input \(x\in\mathcal{X}\), let \(\operatorname{P_{A}}(a\mid x)\) denote the distribution over its augmentations, and \(\operatorname{P_{A}}\) denote the marginal distribution over all possible augmentations. We use the following simple augmentations where we scale the magnitude of each co-ordinate by a uniformly independent amount, _i.e._,

\[a\sim\operatorname{P_{A}}(\cdot\mid x)\equiv c\odot x\ \ \ \text{where,}\ \ c\sim \operatorname{Unif}[0,1]^{d}.\] (8)

The performance of different methods heavily depends on the assumptions we make on augmentations. We try to mirror practical settings where the augmentations are fairly "generic", not encoding any information about which features are invariant or spurious, and hence perturb all features symmetrically.

Self-training.ST performs ERM in the first stage using labeled data from the source, and then subsequently updates the head \(h\) by iteratively generating pseudolabels on the unlabeled target:

\[\mathcal{L}_{\operatorname{st}}(h;\Phi)\ :=\mathbb{E}_{\operatorname{P_{T}}(x)}\ell(h^{ \top}\Phi x,\operatorname{sgn}(h^{\top}\Phi(x)))\qquad\text{Update:}\ h^{t+1}= \frac{h^{t}-\eta\nabla_{h}\mathcal{L}_{\operatorname{st}}(h^{t};\Phi)}{\left\| h^{t}-\eta\nabla_{h}\mathcal{L}_{\operatorname{st}}(h^{t};\Phi)\right\|_{2}}\] (9)

For convenience, we keep the feature backbone \(\Phi\) fixed across the self-training iterations and only update the linear head on the pseudolabels.

STOC(Self-training after contrastive learning).Finally, we can combine the two unsupervised objectives where we do the self-training updates( 2) with \(h_{0}=h_{\mathrm{cl}}\) and \(\Phi_{0}=\Phi_{\mathrm{cl}}\) starting with the contrastive learning model rather than just source-only ERM. Here, we only update \(h\) and fix \(\Phi_{\mathrm{cl}}\).

### Additional empirical results in our simplified setup

We conduct two ablations on the hyperparameters for contrastive pretraining. First, we vary the dimensionality \(k\) of the linear feature extractor \(\Phi\in\mathbb{R}^{k\times d}\). Second, we vary the regularization strength \(\kappa\) that enforces feature diversity in the Barlow Twins objective (7). In Figure 5 we plot these ablations in the UDA setup.

Varying feature dimension.We find that CL recovers the full set of predictive features (_i.e._ both spurious and invariant) only when \(k\) is large enough (Figure 5_(left)_). Since the dimensionality of the true feature is \(5\) in our Example 1, reducing \(k\) below the true feature dimension hurts CL. Once \(k\) crosses a certain threshold, CL features completely capture the projection of the invariant feature \(w_{\mathrm{in}}\). After this point, it amplifies the component along \(w_{\mathrm{in}}\). It retains the amplification over the spurious feature \(w_{\mathrm{sp}}\) even as we increase \(k\). This is confirmed by our finding that further increasing \(k\) does not hurt CL performance. This is also inline with our theoretical observations, where we find that for

Figure 5: **Ablations on pretraining hyperparameters:** In the UDA setup we plot the performance of CL and STOC as we vary two pretraining hyper-parameters: _(left)_ the output dimension \((k)\) of the feature extractor \(\Phi\); and _(right)_ the strength \((\kappa)\) of the regularizer in the Barlow Twins objective in (7). While ablating on \(k\) we fix \(\kappa=0.5\), and while ablating on \(\kappa\) we fix \(k=10\). Other problem parameters are taken from Example 1.

suitable \(w^{\star}\), the subspace spanned by \(w_{\mathrm{in}}\) and \(w_{\mathrm{sp}}\) are contained in a low rank space (as low as rank \(2\)) of the contrastive representations (Theorem 5). Once CL has amplified the dependence along \(w_{\mathrm{in}}\) STOC improves over CL by unlearning any remaining dependence on the spurious \(w_{\mathrm{sp}}\). The above arguments for the CL trend also explain why the performance of STOC continues to remain \(\approx 100\%\) as we vary \(k\).

**Varying regularization strength.** In our main theoretical arguments we consider the constrained form of the Barlow Twins objective (3) with a strict constraint of \(\rho=0\) (we relax this theoretically as well, see E.1.2). For our experiments, we optimize the regularized version of this objective (7), where the constraint term now appears as a regularizer which enforces feature diversity, _i.e_. the features learned through contrastive pretraining span orthogonal parts of the input space (as governed under the metric defined by augmentation covariance matrix \(\Sigma_{A}\)). If \(\kappa\) is very low, then trivial solutions exist for the Barlow Twins objective. For _e.g_., \(\phi\approx\mathbf{0}\) (zero vector) achieves very low invariance loss. When \(\kappa<0.05\), we find that CL recovers these trivial solutions (Figure 5_(right)_). Hence, both CL and STOC perform poorly. As we increase \(\kappa\) the performance of both CL and STOC improve, mainly because the features returned by \(\Phi_{\mathrm{cl}}\) now comprise of the predictive directions \(w_{\mathrm{in}}\) and \(w_{\mathrm{sp}}\), as predictive by our theoretical arguments for \(\rho=0\) (which corresponds to large \(\kappa\)). On the other hand, when \(\kappa\) is too high optimization becomes hard since \(\kappa\) directly effects the Lipschitz constant of the loss function. Hence, the performance of CL drops by some value. Note that this does not effect the performance of STOC since CL continues to amplify \(w_{\mathrm{in}}\) over \(w_{\mathrm{sp}}\) even if it is returning suboptimal solutions with respect to the optimization loss of the pretraining objective.

### Reconciling Practice: Experiments with deep networks in toy setup

In this section we delve into the details of Sec. 4.5, _i.e_., we analyze performance of different methods when we make some design choices that imitate practice. First, we look at experiments involving a deep non-linear backbone \(\Phi\). Here, the non-linear \(\Phi\) is learned during contrastive pretraining and

Figure 6: **Results with linear backbone: We plot the OOD accuracy for ERM, CL, ST and STOC in the UDA setup and ID accuracy in the SSL setup when the feature extractor \(\Phi\) is a linear network. Note, that the feature extractor is still fixed during CL and STOC.**

Figure 7: **Results with non-linear backbone: We plot the OOD accuracy for ERM, CL, ST and STOC in the UDA setup and ID accuracy in the SSL setup when the feature extractor \(\Phi\) is a non-linear one-hidden layer network with ReLU activations. Note, that the feature extractor is still fixed during CL and STOC.**

fixed for CL and STOC. Then, we investigate trends when we continue to propagate gradients onto \(\Phi\) during STOC (we call this full-finetuning). Unlike previous cases, this allows features to be updated.

**Results with non-linear feature extractor \(\Phi\).** In Fig. 7 we plot the performance of the four methods when we use a non-linear feature extractor during contrastive pretraining. This feature extractor is a one-hidden layer neural network (hidden dimension is 500) with ReLU activations. We find that the trends observed with linear backbones in Fig. 6 are also replicated with the non-linear one. Specifically, we note that STOC improves over CL under distribution shifts, whereas CL is already close to optimal when there are no distribution shifts. We also see that CL and ST individually are subpar. In SSL, we see a huge drop in the performance of ST (over ERM) mainly because we only fit on pseudolabels during ST. This is different from practice where we continue to optimize loss on labeled data points while fitting the pseudolabels. Consequently, when we continue to optimize performance on source labeled data the performance of ST in SSL setup is improves from \(51.1\%\to 72.6\%\).

**Results with full fine-tuning.** Up till this point, we have only considered the case (for both SSL and UDA) where we fix the contrastive learned features when running CL and STOC, _i.e_., we only optimized the linear head \(h\). Now, we shall consider the setting where gradients are propagated to \(\Phi\) during STOC. Note that we still fix the representations for training the linear head during CL. Results for this setting are in Figure 8. We show two interesting trends that imitate real world behaviors.

_STOC benefits from augmentations during full-finetuning:_ In the UDA setup we find that ST while updating \(\Phi_{\text{cl}}\) can hurt due to overfitting issues when training with the finite sample of labeled and unlabeled data (drop by \(>7\%\) over CL). This is due to overfitting on confident but incorrect pseudolabels on target data. This can exacerbate components along spurious feature \(w_{\text{sp}}\) from source. One reasoning behind this is that deep neural networks can perfectly memorize them on finite unlabeled target data [94]. Heuristics typically used in practice (_e.g_. in FixMatch [79]) help avoid overfitting on incorrect pseudolabels: (i) confidence thresholding; to pick confident pseudolabel examples; (ii) pseudolabel a different augmented input than the one on which the self-training loss is optimized; and (iii) optimize source loss with labeled data simultaneously when fitting pseudolabels. Intuitively, thresholding introduces a curriculum where we only learn confident examples in the beginning whose pseudolabels are mainly determined by component along the invariant feature \(w_{\text{in}}\). Augmentations prevent the neural network from memorizing incorrect pseudolabels and optimizing source loss prevents forgetting of features learned during CL. When we implement these during full-finetuning in STOC we see that STOC now improves over CL (by \(>20\%\)).

Figure 8: **Finetuning the contrastive representations during STOC:** We propagate gradients to the feature backbone \(\Phi\) when running STOC algorithm. Note that CL still fixes the contrastive representations when learning a fixed linear head over it. On the _(left)_ we show results in UDA setup where we compare the performance of STOC with and without augmentations (along with other practical design choices like confidence thresholds and continuing to optimize source loss as done in FixMatch) when the feature backbone is non-linear. On the _(right)_ we show results for STOC and CL in the SSL setup when the feature backbone is linear.

_Can we improve contrastive pretraining features during STOC?_ We find that self-training can also improve features learned during contrastive pretraining when we update the full backbone during STOC (see Figure 8(_right_)). Specifically, in the SSL setup we find that STOC can now improve substantially over CL. Recall, that when we fixed \(\Phi_{\mathrm{cl}}\) this was not possible (see E.3 and Fig. 2(b)). This is mainly because STOC can now improve performance beyond just recovering the generalization gap for the linear head (which is typically small). This feature improvement is observed even when we fully finetune a linear feature extractor. Similar trends are also observed with the non-linear backbone. But, it becomes harder to identify a good stopping criterion for CL training. Thus, it remains unclear if STOC and CL have complementary benefits for feature learning in UDA or SSL settings. Investigating this is an interesting avenue for future work.

## Appendix E Formal Statements from Sec. 4

Recall from Section 2 that we learn linear classifiers \(h\) over features extractors \(\Phi\). We consider linear feature extractor i.e. \(\Phi\) is a matrix in \(\mathbb{R}^{d\times k}\) and the linear layer \(h:\mathbb{R}^{k}\rightarrow\mathbb{R}\) with a prediction as \(\mathrm{sgn}(h^{\top}\Phi x)\). We use the exponential loss \(\ell(f(x),y)=\exp\left(-yf(x)\right)\).

### Analysis of ERM and ST: Formal Statement of Theorem 2

For ERM and ST, we train both \(h\) and \(\Phi\). This is equivalent to \(\Phi=I_{d\times d}\) being identity and training a linear head \(h\). Recall that the ERM classifier is obtained by minimizing the population loss on labeled source data:

\[h_{\text{ERM}}=\operatorname*{arg\,min}_{h}\mathbb{E}_{(x,y) \sim\mathrm{P_{S}}}\left[\ell(x,y)\right]\,.\] (10)

We split Theorem 2 into Theorem 8 and Theorem 9. Before we characterize the ERM solution, we recall some additional notation. Define \(w_{\mathrm{in}}{=}[w^{\star},0,...,0]^{\top}\), and \(w_{\mathrm{sp}}=[0,...,0,\nicefrac{{1_{d_{\mathrm{sp}}}}}{{\sqrt{d_{\mathrm{ sp}}}}}]^{\top}\). The following proposition characterizes \(h_{\text{ERM}}\) and 0-1 error of the classifier on target:

**Theorem 8** (ERM classifier and its error on target).: _ERM classifier obtained as in (10) is given by_

\[\frac{h_{\text{ERM}}}{\left\|h_{\text{ERM}}\right\|_{2}}=\frac{ \gamma\cdot w_{\mathrm{in}}+\sqrt{d_{\mathrm{sp}}}\cdot w_{\mathrm{sp}}}{ \sqrt{\gamma^{2}+d_{\mathrm{sp}}}}\,.\]

_The target accuracy of \(h_{\text{ERM}}\) is given by \(0.5\cdot\mathrm{erfc}\left(-\nicefrac{{\gamma^{2}}}{{\left(\nicefrac{{ \gamma^{2}}}{{\sqrt{2d_{\mathrm{sp}}}}\cdot\sigma_{\mathrm{sp}}}\right)}}\right)\)._

Proof.: To prove this theorem, we first derive a closed-form expression for the ERM classifier and then use Lemma 29 to derive its 0-1 error on target. For Gaussian data with the same covariance matrices for class conditional \(\mathrm{P_{S}}(x|y=1)\) and \(\mathrm{P_{S}}(x|y=0)\), Bayes decision rule is given by the Fisher's linear discriminant direction (Chapter 4; Bishop [9]):

\[h(x)=\begin{cases}1,&\text{if }h^{\top}x>0\\ 0,&\text{otherwise}\end{cases}\]

where \(h=2\cdot\gamma(w_{\mathrm{in}})+2\cdot\sqrt{d_{\mathrm{sp}}}(w_{\mathrm{sp}})\). Plugging \(h\) in Lemma 29 we get the desired result. 

ST performs ERM in the first stage using labeled data from the source, and then subsequently updates the head \(h\) by iteratively generating pseudolabels on the unlabeled target:

\[\mathcal{L}_{\mathrm{st}}(h)\ \coloneqq\mathbb{E}_{\mathrm{P_{T}}(x)} \ell(h^{\top}x,\mathrm{sgn}(h^{\top}x))\,.\] (11)

Starting with \(h^{0}_{\text{ST}}=\nicefrac{{h_{\text{ERM}}}}{{\left\|h_{\text{ERM}}\right\|_ {2}}}\) (the classifier obtained with ERM) we perform the following iterative procedure for self-training:

\[h^{t+1}_{\text{ST}}=\frac{h^{t}_{\text{ST}}-\eta\nabla_{h}\mathcal{L}_{\mathrm{ st}}(h^{t}_{\text{ST}})}{\left|h^{t}_{\text{ST}}-\eta\nabla_{h}\mathcal{L}_{ \mathrm{st}}(h^{t}_{\text{ST}})\right|_{2}}\] (12)

Next, we characterize ST solution:

**Theorem 9** (ST classifier and its error on target).: _Starting with ERM solution, ST will lead to:_

[MISSING_PAGE_FAIL:30]

Now we will individually argue about the update of \(\widetilde{h}^{t+1}\) along the first \(d_{\rm in}\) dimensions and the last \(d_{\rm sp}\) dimensions. First, we have:

\[\widetilde{h}^{t+1}_{\rm in} = h^{t}_{\rm in}-\eta\cdot\frac{\partial{\cal L}_{\rm st}(h^{t})}{ \partial\widetilde{h}_{\rm in}}\] (23) \[= h^{t}_{\rm in}-\frac{\eta}{2}\left(-\mathrm{exp}\left(\frac{ \sigma_{t}^{2}}{2}-\mu_{t}\right)\cdot\mathrm{erfc}\left(-\frac{\mu_{t}}{ \sqrt{2}\sigma_{t}}+\frac{\sigma_{t}}{\sqrt{2}}\right)\right.\] \[\qquad\left.+\mathrm{exp}\left(\frac{\sigma_{t}^{2}}{2}+\mu_{t} \right)\cdot\mathrm{erfc}\left(\frac{\mu_{t}}{\sqrt{2}\sigma_{t}}+\frac{ \sigma_{t}}{\sqrt{2}}\right)\right)\cdot\gamma\cdot w^{\star}\] \[\qquad-\frac{\eta}{2}\left(\mathrm{exp}\left(\frac{\sigma_{t}^{2 }}{2}-\mu_{t}\right)\cdot\mathrm{erfc}\left(-\frac{\mu_{t}}{\sqrt{2}\sigma_{t }}+\frac{\sigma_{t}}{\sqrt{2}}\right)\right.\] \[\qquad\left.+\mathrm{exp}\left(\frac{\sigma_{t}^{2}}{2}+\mu_{t} \right)\cdot\mathrm{erfc}\left(\frac{\mu_{t}}{\sqrt{2}\sigma_{t}}+\frac{ \sigma_{t}}{\sqrt{2}}\right)\right.\] \[\qquad\left.-\frac{2\sqrt{2}}{\sigma_{t}\sqrt{\pi}}\mathrm{exp} \left(-\frac{\mu_{t}^{2}}{2\sigma_{t}^{2}}\right)\right)\cdot(2h^{t}_{\rm in} -2(h^{t}_{\rm in}\,{}^{\top}w^{\star})w^{\star})\cdot\sigma_{\rm in}^{2}\] \[= h^{t}_{\rm in}-\frac{\eta}{2}\cdot\alpha_{1}(\mu_{t},\sigma_{t}) \cdot\gamma\cdot w^{\star}-\frac{\eta}{2}\cdot\alpha_{2}(\mu_{t},\sigma_{t}) \cdot(2h^{t}_{\rm in}-2(h^{t}_{\rm in}\,{}^{\top}w^{\star})w^{\star})\cdot \sigma_{\rm in}^{2}\,.\]

Notice that the update of \(h^{t+1}_{\rm in}\) is split into two components, one along \(w^{\star}\) and the other along the orthogonal component \(2h^{t}_{\rm in}-2(h^{t}_{\rm in}\,{}^{\top}w^{\star})w^{\star}\). We will now argue that since at initialization, the component along \((I-w^{\star}w^{\star})\) is zero then it will remain zero. In particular, we have:

\[h^{0}_{\rm in}\,{}^{\top}(I-w^{\star}w^{\star}{}^{\top})\circ w^{\star}{}^{ \top}(I-w^{\star}w^{\star}{}^{\top})=0\,.\] (24)

With (23), we can argue that if \((I-w^{\star}w^{\star}{}^{\top})h^{t}_{\rm in}=0\), then \((I-w^{\star}w^{\star}{}^{\top})\widetilde{h}^{t+1}_{\rm inv}=0\) implying that \((I-w^{\star}w^{\star}{}^{\top})\widetilde{h}^{t}_{\rm in}=0\) for all \(t>0\). Hence, we have:

\[\widetilde{h}^{t+1}_{\rm inv} = h^{t}_{\rm in}-\eta\cdot\frac{\partial{\cal L}_{\rm st}(h^{t})}{ \partial\widetilde{h}_{\rm in}}\] (25) \[= h^{t}_{\rm in}-\frac{\eta}{2}\cdot\alpha_{1}(\mu_{t},\sigma_{t}) \cdot\gamma\cdot w^{\star}\,.\]

Second, we have the update \(\widetilde{h}^{t+1}_{\rm sp}\) given by:

\[\widetilde{h}^{t+1}_{\rm sp} = h^{t}_{\rm sp}-\eta\cdot\frac{\partial{\cal L}_{\rm st}(h^{t})}{ \partial\widetilde{h}_{\rm sp}}\] (26) \[= h^{t}_{\rm sp}-\frac{\eta}{2}\left(\mathrm{exp}\left(\frac{ \sigma_{t}^{2}}{2}-\mu_{t}\right)\cdot\mathrm{erfc}\left(-\frac{\mu_{t}}{\sqrt{ 2}\sigma_{t}}+\frac{\sigma_{t}}{\sqrt{2}}\right)\right.\] \[\qquad\qquad\left.+\mathrm{exp}\left(\frac{\sigma_{t}^{2}}{2}+\mu_ {t}\right)\cdot\mathrm{erfc}\left(\frac{\mu_{t}}{\sqrt{2}\sigma_{t}}+\frac{ \sigma_{t}}{\sqrt{2}}\right)-\frac{2\sqrt{2}}{\sigma_{t}\sqrt{\pi}}\mathrm{ exp}\left(-\frac{\mu_{t}^{2}}{2\sigma_{t}^{2}}\right)\right)\cdot h ^{t}_{\rm sp}\cdot\sigma_{\rm sp}^{2}\] \[= h^{t}_{\rm sp}-\frac{\eta}{2}\cdot\alpha_{2}(\mu_{t},\sigma_{t}) \cdot h^{t}_{\rm sp}\cdot\sigma_{\rm sp}^{2}\,.\]

Re-writing the expressions (25) and (26) for the update of \(\widetilde{h}^{t+1}\), we have:

\[\widetilde{h}^{t+1}_{\rm in} = h^{t}_{\rm in}(1-\frac{\eta}{2}\cdot\alpha_{1}(\mu_{t},\sigma_{t })\cdot\gamma^{2}/\mu_{t})\,.\] (27) \[\widetilde{h}^{t+1}_{\rm sp} = h^{t}_{\rm sp}(1-\frac{\eta}{2}\cdot\alpha_{2}(\mu_{t},\sigma_{t })\cdot\sigma_{\rm sp}^{2})\,.\] (28)

Here, we replace \(h^{t}_{\rm sp}=\mu_{t}\cdot w^{\star}/\gamma\) in (25) to get (27). Updates in (27) and (28) show that \(\widetilde{h}^{t+1}_{\rm inv}\) remains in the direction of \(h^{t}_{\rm in}\) and \(\widetilde{h}^{t+1}_{\rm sp}\) remains in the direction of \(h^{t}_{\rm sp}\).

Part-2.Now we will derive conditions under which \(h_{\mathrm{im}}^{t}\) and \(h_{\mathrm{gp}}^{t}\) will show monotonic behavior for necessary and sufficient conditions. We will first argue the condition under which ST will provably fail and converge to a classifier with a random target performance. For this, at every \(t\), if we have:

\[\frac{\left\|\widetilde{h}_{\mathrm{sp}}^{t+1}\right\|_{2}}{\left\| \widetilde{h}^{t+1}\right\|_{2}}>\left\|h_{\mathrm{sp}}^{t}\right\|_{2}\,,\] (29)

then we can argue that as \(t\to\infty\), we have \(\left\|h_{\mathrm{sp}}^{t}\right\|_{2}=1\) and hence, the ST classifier will have random target performance. Thus, we will focus on conditions, under which the norm on \(\left\|h_{\mathrm{sp}}^{t}\right\|_{2}\) increases with \(t\). Re-writing (29), we have:

\[\left|\widetilde{h}_{\mathrm{sp}}^{t+1}\right|_{2} >\left|\widetilde{h}^{t+1}\right|_{2}\cdot\left\|h_{\mathrm{sp}}^ {t}\right\|_{2}\] (30) \[\left|\widetilde{h}_{\mathrm{sp}}^{t+1}\right|_{2} >\left(\left\|\widetilde{h}_{\mathrm{sp}}^{t+1}\right\|_{2}+ \left|\widetilde{h}_{\mathrm{in}}^{t+1}\right|_{2}\right)\cdot\left\|h_{ \mathrm{sp}}^{t}\right\|_{2}\] (31) \[\left|\widetilde{h}_{\mathrm{sp}}^{t+1}\right|_{2}\cdot\left(1- \left\|h_{\mathrm{sp}}^{t}\right\|_{2}\right) >\left|\widetilde{h}_{\mathrm{in}}^{t+1}\right|_{2}\cdot\left\|h_{ \mathrm{sp}}^{t}\right\|_{2}\] (32) \[\frac{\left\|\widetilde{h}_{\mathrm{sp}}^{t+1}\right\|_{2}}{ \left\|h_{\mathrm{sp}}^{t}\right\|_{2}} >\frac{\left\|\widetilde{h}_{\mathrm{in}}^{t+1}\right\|_{2}}{ \left\|h_{\mathrm{in}}^{t}\right\|_{2}}\,.\] (33)

Plugging in (27) and (28) into (33), we get:

\[\left|1-\frac{\eta}{2}\cdot\alpha_{2}(\mu_{t},\sigma_{t})\cdot \sigma_{\mathrm{sp}}^{2}\right|>\left|1-\frac{\eta}{2}\cdot\alpha_{1}(\mu_{t}, \sigma_{t})\cdot\gamma^{2}/\mu_{t}\right|\,.\] (34)

For small enough \(\eta\), we have the necessary condition for the failure of ST as:

\[\alpha_{2}(\mu_{t},\sigma_{t})\cdot\sigma_{\mathrm{sp}}^{2}<\alpha_{1}(\mu_{ t},\sigma_{t})\cdot\gamma^{2}/\mu_{t}\,.\] (35)

Now we show in Lemma 11 and Lemma 10 that if the conditions assumed in the theorem continue to hold, then we can success and failure respectively.

**Lemma 10** (Necessary conditions for ST).: _Define \(\alpha_{1}\) and \(\alpha_{2}\) as in (20) and (21) respectively. If \(\sigma_{\mathrm{sp}}\geq 1\) and \(\gamma\leq\frac{1}{2\sqrt{\sigma_{\mathrm{sp}}}}\), then we have for all \(t\):_

\[\alpha_{2}(\mu_{t},\sigma_{t})\cdot\frac{\sigma_{\mathrm{sp}}^{2}\cdot\mu_{t} }{\gamma^{2}}\leq\alpha_{1}(\mu_{t},\sigma_{t})\,.\] (36)

Proof.: We upper bound and lower bound \(\alpha_{1}\) and \(\alpha_{2}\) by using the properties of \(\mathrm{r}\left(\cdot\right)\). Recall:

\[\alpha_{1}(\mu_{t},\sigma_{t})=\sqrt{\frac{2}{\pi}}\mathrm{exp} \left(-\frac{\mu_{t}^{2}}{2\sigma_{t}^{2}}\right)\left[\mathrm{r}\left(\sigma _{t}+\frac{\mu_{t}}{\sigma_{t}}\right)-\mathrm{r}\left(\sigma_{t}-\frac{\mu_{ t}}{\sigma_{t}}\right)\right]\,.\] (37)

and

\[\alpha_{2}(\mu_{t},\sigma_{t})=\sqrt{\frac{2}{\pi}}\mathrm{exp} \left(-\frac{\mu_{t}^{2}}{2\sigma_{t}^{2}}\right)\left[\mathrm{r}\left(\sigma _{t}+\frac{\mu_{t}}{\sigma_{t}}\right)+\mathrm{r}\left(\sigma_{t}-\frac{\mu_{ t}}{\sigma_{t}}\right)-\frac{2}{\sigma_{t}}\right]\,.\] (38)

We now use Taylor's expansion on \(\mathrm{r}\left(\cdot\right)\) and we get:

\[\mathrm{r}\left(\sigma_{t}\right)+\mathrm{r}^{\prime}\left(\sigma_{t}\right) \cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\leq\mathrm{r}\left(\sigma_{t}+ \frac{\mu_{t}}{\sigma_{t}}\right)\leq\mathrm{r}\left(\sigma_{t}\right)+ \mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t} }\right)+R^{\prime\prime}\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}\] (39)

and similarly, we get:

\[\mathrm{r}\left(\sigma_{t}\right)-\mathrm{r}^{\prime}\left(\sigma_{t}\right) \cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\leq\mathrm{r}\left(\sigma_{t}- \frac{\mu_{t}}{\sigma_{t}}\right)\leq\mathrm{r}\left(\sigma_{t}\right)- \mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t} }\right)+R^{\prime\prime}\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}\] (40)where \(R^{\prime\prime}=\mathrm{r}^{\prime\prime}\left(\sigma_{0}\right)\). This is because \(\mathrm{r}^{\prime\prime}\left(\cdot\right)\) takes positive values and is a decreasing function in \(\sigma_{t}\) (refer to Lemma 21). We now lower bound \(\alpha_{1}(\mu_{t},\sigma_{t})\) and upper bound \(\alpha_{2}(\mu_{t},\sigma_{t})\):

\[\frac{\alpha_{1}(\mu_{t},\sigma_{t})}{\sqrt{\frac{2}{\pi}\mathrm{exp}\left(- \frac{\mu_{t}^{2}}{2\sigma_{t}^{2}}\right)}}\geq 2\mathrm{r}^{\prime}\left( \sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)-R^{\prime\prime }\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}\] (41)

\[\frac{\alpha_{2}(\mu_{t},\sigma_{t})}{\sqrt{\frac{2}{\pi}\mathrm{exp}\left(- \frac{\mu_{t}^{2}}{2\sigma_{t}^{2}}\right)}}\leq 2\mathrm{r}\left(\sigma_{t} \right)+2\cdot R^{\prime\prime}\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}\] (42)

Substituting the lower bound and upper bound in (36) gives us the following as stricter a necessary condition (i.e., (43) implies (36)):

\[\left[2\mathrm{r}\left(\sigma_{t}\right)+2\cdot R^{\prime\prime }\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}-\frac{2}{\sigma_{t}}\right]\cdot \frac{\sigma_{\mathrm{sp}}^{2}}{\gamma^{2}}\leq 2\mathrm{r}^{\prime}\left( \sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)-R^{\prime\prime }\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}\] (43) \[\Longleftrightarrow\left[2\mathrm{r}\left(\sigma_{t}\right)+2 \cdot R^{\prime\prime}\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}-\frac{2}{ \sigma_{t}}\right]\cdot\frac{\sigma_{\mathrm{sp}}^{2}}{\gamma^{2}}\leq 2 \mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\left(\frac{1}{\sigma_{t}} \right)-R^{\prime\prime}\left(\frac{\mu_{t}}{\sigma_{t}^{2}}\right)\] (44) \[\Longleftrightarrow\left[\mathrm{r}\left(\sigma_{t}\right)+R^{ \prime\prime}\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}-\frac{1}{\sigma_{t}} \right]\cdot\frac{\sigma_{\mathrm{sp}}^{2}}{\gamma^{2}}\leq\mathrm{r}\left( \sigma_{t}\right)-\frac{1}{\sigma_{t}}-\frac{R^{\prime\prime}}{2}\left(\frac{ \mu_{t}}{\sigma_{t}^{2}}\right)\] (45) \[\Longleftrightarrow\left[R^{\prime\prime}\left(\frac{\mu_{t}}{ \sigma_{t}}\right)^{2}\right]\cdot\frac{\sigma_{\mathrm{sp}}^{2}}{\gamma^{2}}+ \frac{R^{\prime\prime}}{2}\left(\frac{\mu_{t}}{\sigma_{t}^{2}}\right)\leq \left(\mathrm{r}\left(\sigma_{t}\right)-\frac{1}{\sigma_{t}}\right)\cdot \left(1-\frac{\sigma_{\mathrm{sp}}^{2}}{\gamma^{2}}\right)\] (46) \[\Longleftrightarrow\left[R^{\prime\prime}\left(\frac{\mu_{t}^{2} }{\sigma_{t}}\right)\right]\cdot\frac{\sigma_{\mathrm{sp}}^{2}}{\gamma^{2}}+ \frac{R^{\prime\prime}}{2}\left(\frac{\mu_{t}}{\sigma_{t}}\right)\leq\left( \sigma_{t}\mathrm{r}\left(\sigma_{t}\right)-1\right)\cdot\left(1-\frac{\sigma_{ \mathrm{sp}}^{2}}{\gamma^{2}}\right)\] (47)

Now, we will argue the monotonicity of LHS and RHS in (47). Observe that LHS is increasing in \(\mu_{t}\) and decreasing in \(\sigma_{t}\) and RHS is decreasing in \(\sigma_{t}\) as \(\left(\sigma_{t}\mathrm{r}\left(\sigma_{t}\right)-1\right)\) is increasing (and the multiplier is negative). Moreover, if (47) holds true for maximum value of RHS and minimum of LHS, then we would have (36). Thus substituting \(\mu_{t}=\gamma\) and \(\sigma_{t}=\sigma_{0}\) in LHS and \(\sigma_{t}=\sigma_{\mathrm{sp}}\) in RHS, we get:

\[\left[R^{\prime\prime}\left(\frac{\gamma^{2}}{\sigma_{0}}\right) \right]\cdot\frac{\sigma_{\mathrm{sp}}^{2}}{\gamma^{2}}+\frac{R^{\prime \prime}}{2}\left(\frac{\gamma}{\sigma_{0}}\right)\leq\left(\sigma_{\mathrm{ sp}}\mathrm{r}\left(\sigma_{\mathrm{sp}}\right)-1\right)\cdot\left(1-\frac{ \sigma_{\mathrm{sp}}^{2}}{\gamma^{2}}\right)\] (48) \[\Longleftrightarrow R^{\prime\prime}\cdot\frac{\sigma_{\mathrm{ sp}}^{2}}{\sigma_{0}}+\frac{R^{\prime\prime}}{2}\left(\frac{\gamma}{\sigma_{0}} \right)\leq\left(\sigma_{\mathrm{sp}}\mathrm{r}\left(\sigma_{\mathrm{sp}}\right)-1 \right)\cdot\left(1-\frac{\sigma_{\mathrm{sp}}^{2}}{\gamma^{2}}\right)\] (49)

Taking \(\gamma\leq\frac{1}{2\sqrt{\sigma_{\mathrm{sp}}}}\) and substituting \(R^{\prime\prime}=\mathrm{r}^{\prime\prime}\left(\sigma_{0}\right)\):

\[\left(5/4\right)\cdot\mathrm{r}^{\prime\prime}\left(\sigma_{0}\right)\cdot \sigma_{\mathrm{sp}}\leq\left(\sigma_{\mathrm{sp}}\mathrm{r}\left(\sigma_{ \mathrm{sp}}\right)-1\right)\cdot\left(1-4\cdot\sigma_{\mathrm{sp}}^{3}\right)\] (51)

Analytically solving the above expression, we get that (51) is satisfied for all values of \(\sigma_{\mathrm{sp}}\geq 1\) when \(d_{\mathrm{sp}}\geq 1\). For example, the expression in (51) is also satisfied for the problem parameter used in the running example of the main paper.

As a remark, we note that in the proof of Lemma 10, the conditions derived are loose because of the relaxations made to simply the proof. In principle, the proof (and hence the conditions) can be tightened by carefully propagating second-order terms (which depend on \(\sigma_{t}\)) in (40).

**Lemma 11** (Sufficiency conditions for ST).: _Define \(\alpha_{1}\) and \(\alpha_{2}\) as in (20) and (21) respectively. If \(\sigma_{\mathrm{sp}}\leq\gamma\), then we have for all \(t\):_

\[\alpha_{2}(\mu_{t},\sigma_{t})\cdot\frac{\sigma_{\mathrm{sp}}^{2}\cdot\mu_{t}}{ \gamma^{2}}\geq\alpha_{1}(\mu_{t},\sigma_{t})\,.\] (52)Proof.: We upper bound and lower bound \(\alpha_{1}\) and \(\alpha_{2}\) by using the properties of \(\mathrm{r}\left(\cdot\right)\). Recall:

\[\alpha_{1}(\mu_{t},\sigma_{t})=\sqrt{\frac{2}{\pi}}\mathrm{exp}\left(-\frac{\mu _{t}^{2}}{2\sigma_{t}^{2}}\right)\left[\mathrm{r}\left(\sigma_{t}+\frac{\mu_{t }}{\sigma_{t}}\right)-\mathrm{r}\left(\sigma_{t}-\frac{\mu_{t}}{\sigma_{t}} \right)\right]\,.\] (53)

and

\[\alpha_{2}(\mu_{t},\sigma_{t})=\sqrt{\frac{2}{\pi}}\mathrm{exp}\left(-\frac{ \mu_{t}^{2}}{2\sigma_{t}^{2}}\right)\left[\mathrm{r}\left(\sigma_{t}+\frac{\mu _{t}}{\sigma_{t}}\right)+\mathrm{r}\left(\sigma_{t}-\frac{\mu_{t}}{\sigma_{t} }\right)-\frac{2}{\sigma_{t}}\right]\,.\] (54)

We now use Taylor's expansion on \(\mathrm{r}\left(\cdot\right)\) and we get:

\[\mathrm{r}\left(\sigma_{t}\right)+\mathrm{r}^{\prime}\left(\sigma_{t}\right) \cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\leq\mathrm{r}\left(\sigma_{t}+ \frac{\mu_{t}}{\sigma_{t}}\right)\leq\mathrm{r}\left(\sigma_{t}\right)+ \mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t }}\right)+\mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu _{t}}{\sigma_{t}}\right)^{2}\] (55)

and similarly, we get:

\[\mathrm{r}\left(\sigma_{t}\right)-\mathrm{r}^{\prime}\left(\sigma_{t}\right) \cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)+\mathrm{r}^{\prime\prime}\left( \sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}\leq\mathrm{ r}\left(\sigma_{t}-\frac{\mu_{t}}{\sigma_{t}}\right)\leq\mathrm{r}\left(\sigma_{t} \right)-\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{ \sigma_{t}}\right)+R^{\prime\prime}\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}\] (56)

where \(R^{\prime\prime}=\mathrm{r}^{\prime\prime}\left(\sigma_{0}\right)\). This is because \(\mathrm{r}^{\prime\prime}\left(\cdot\right)\) takes positive values and is a decreasing function in \(\sigma_{t}\) (refer to Lemma 21). We now lower bound \(\alpha_{1}(\mu_{t},\sigma_{t})\) and upper bound \(\alpha_{2}(\mu_{t},\sigma_{t})\):

\[\frac{\alpha_{1}(\mu_{t},\sigma_{t})}{\sqrt{\frac{2}{\pi}}\mathrm{exp}\left( -\frac{\mu_{t}^{2}}{2\sigma_{t}^{2}}\right)}\leq 2\mathrm{r}^{\prime}\left( \sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\] (57)

\[\frac{\alpha_{2}(\mu_{t},\sigma_{t})}{\sqrt{\frac{2}{\pi}}\mathrm{exp}\left( -\frac{\mu_{t}^{2}}{2\sigma_{t}^{2}}\right)}\geq 2\mathrm{r}\left(\sigma_{t} \right)+\mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu _{t}}{\sigma_{t}}\right)^{2}-\frac{2}{\sigma_{t}}\] (58)

Substituting the lower bound and upper bound in (52) gives us the following as stricter a sufficient condition (i.e., (59) implies (52)):

\[\left[2\mathrm{r}\left(\sigma_{t}\right)+\mathrm{r}^{\prime\prime }\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}- \frac{2}{\sigma_{t}}\right]\cdot\frac{\sigma_{\mathrm{sp}}^{2}\cdot\mu_{t}}{ \gamma^{2}}\geq 2\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t }}{\sigma_{t}}\right)\] (59) \[\Longleftrightarrow\left[2\mathrm{r}\left(\sigma_{t}\right)+ \mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{ \sigma_{t}}\right)^{2}-\frac{2}{\sigma_{t}}\right]\geq 2\mathrm{r}^{\prime}\left( \sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\cdot\frac{ \gamma^{2}}{\sigma_{\mathrm{sp}}^{2}\cdot\mu_{t}}\] (60) \[\Longleftrightarrow 2\mathrm{r}\left(\sigma_{t}\right)+\mathrm{r}^{\prime \prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}- \frac{2}{\sigma_{t}}-2\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\left( \frac{\mu_{t}}{\sigma_{t}}\right)\cdot\frac{\gamma^{2}}{\sigma_{\mathrm{sp}}^{ 2}\cdot\mu_{t}}\geq 0\] (61) \[\Longleftrightarrow 2\mathrm{r}\left(\sigma_{t}\right)\cdot\sigma_{t}+ \mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\frac{\mu_{t}^{2}}{\sigma_ {t}}-2-2\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\frac{\gamma^{2}}{ \sigma_{\mathrm{sp}}^{2}}\geq 0\] (62) \[\Longleftrightarrow 2\mathrm{r}^{\prime}\left(\sigma_{t}\right)+ \mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\frac{\mu_{t}^{2}}{\sigma_ {t}}-2\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\frac{\gamma^{2}}{\sigma_{ \mathrm{sp}}^{2}}\geq 0\] (63) \[\Longleftrightarrow \mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\frac{\mu_{t}^{2 }}{\sigma_{t}}+2\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\left[1-\frac{ \gamma^{2}}{\sigma_{\mathrm{sp}}^{2}}\right]\geq 0\] (64)

Hence, when \(\left[1-\frac{\gamma^{2}}{\sigma_{\mathrm{sp}}^{2}}\right]\leq 0\), we have condition in (64) hold true as \(\mathrm{r}^{\prime}\left(\sigma_{t}\right)\) is always negative. Hence, the condition \(\gamma\geq\sigma_{\mathrm{sp}}\) gives us the necessary condition. 

#### e.1.1 Proof of Proposition 3

For convenience, we first restate the Proposition 3 which gives us a closed form solution for (3) when \(\rho=0\). Then, we provide the proof, focusing first on the case of \(k=1\), and then showing that extension to \(k>1\) is straightforward and renders the final form in the proposition that follows.

**Proposition 12** (Barlow Twins solution).: _The solution for (3) is \(U_{k}^{\top}\Sigma_{\mathsf{A}}^{-1/2}\) where \(U_{k}\) are the \(top\)\(k\) eigenvectors of \(\Sigma_{\mathsf{A}}^{-1/2}\,\widetilde{\Sigma}\,\Sigma_{\mathsf{A}}^{-1/2}\). Here, \(\Sigma_{\mathsf{A}}\coloneqq\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}[aa^{\top}]\) is the covariance over augmentations, and \(\widetilde{\Sigma}\coloneqq\mathbb{E}_{x\sim\mathrm{P}_{\mathsf{U}}}[\widetilde {a}(x)\widetilde{a}(x)^{\top}]\) is the covariance matrix of mean augmentations \(\widetilde{a}(x)\coloneqq\mathbb{E}_{\mathrm{P}_{\mathsf{A}}(a|x)}[a]\)._

Proof.: We will use \(\phi(x)\) to denote \(\phi^{\top}x\) where \(\phi\in\mathbb{R}^{d}\). Throughout the proof, we use \(a\) to denote augmentation and \(a\) to denote the input. We will use \(\mathrm{P}_{\mathsf{A}}(a\mid x)\) as the probability measure over the space of augmentations \(\mathcal{A}\), given some input \(x\in\mathcal{X}\) (with corresponding density) \(p_{\mathsf{A}}(\cdot\mid x)\). Next, we use \(p_{\mathsf{A}}(\cdot)\) to denote the density associate with the marginal probability measure over augmentations: \(\mathrm{P}_{\mathsf{A}}=\int_{\mathcal{X}}\mathrm{P}_{\mathsf{A}}(a\mid x) \mathrm{dP}_{\mathsf{U}}\). Finally, the joint distribution over positive pairs \(A_{+}(a_{1},a_{2})=\int_{\mathcal{X}}\mathrm{P}_{\mathsf{A}}(a_{1}\mid x) \mathrm{P}_{\mathsf{A}}(a_{2}\mid x)\mathrm{dP}_{\mathsf{U}}\), gives us the positive pair graph over augmentations.

Before we solve the optimization problem in (3) for \(\Phi\in\mathbb{R}^{k\times d}\) for any general \(k\), let us first consider the case where \(k=1\), _i.e._ we only want to find a single linear projection \(\phi\). The constraint \(\rho=0\), transfers onto \(\phi\) in the following way:

\[\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}[\phi(a)^{2}]=1\quad=\quad\phi^{\top} \Sigma_{A}\phi=1\] (65)

Under the above constraint we want to minimize the invariance loss, which according to Lemma 22 is given by \(2\cdot\int_{\mathcal{A}}\phi(a)L(\phi)(a)\ \mathrm{dP}_{\mathsf{A}}\), where \(L(\phi)(\cdot)\) is the following linear operator.

\[L(\phi)(a)=\phi(a)-\int_{\mathcal{A}}\frac{A_{+}(a,a^{\prime})}{p_{\mathsf{A} }(a)}\cdot\phi(a^{\prime})\ \mathrm{d}a^{\prime}.\] (66)

Based on the definition of the operator, we can reformulate the constrained optimization for contrastive pretraining as:

\[\operatorname*{arg\,min}_{\phi:\phi^{\top}\Sigma_{A}\phi=1}\ \int_{\mathcal{A}}\phi(a)\cdot L(\phi)(a)\ \mathrm{dP}_{\mathsf{A}}\] (67) \[\implies\operatorname*{arg\,min}_{\phi:\phi^{\top}\Sigma_{A}\phi=1} \ \mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}[\phi(a)^{2}]-\int_{\mathcal{A}}\int_ {\mathcal{A}}\phi(a)\cdot\phi(a^{\prime})\cdot A_{+}(a,a^{\prime})\ \mathrm{d}a \mathrm{d}a^{\prime}\] (68) \[\implies\operatorname*{arg\,min}_{\phi:\phi^{\top}\Sigma_{A}\phi=1} \ \mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}[\phi(a)^{2}]-\int_{\mathcal{X}}\int_ {\mathcal{A}}\int_{\mathcal{A}}p_{\mathsf{A}}(a\mid x)p_{\mathsf{A}}(a^{ \prime}\mid x)\cdot\phi(a)\phi(a^{\prime})\ \mathrm{dP}_{\mathsf{U}}\] (69) \[\implies\operatorname*{arg\,min}_{\phi:\phi^{\top}\Sigma_{A}\phi=1} \ \mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}[\phi(a)^{2}]-\int_{\mathcal{X}}[ \widetilde{\phi}(x)]^{2}\ \mathrm{dP}_{\mathsf{U}},\] (70)

where \(\widetilde{\phi}(x)=\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}(\cdot\mid x)} \phi(x)=\mathbb{E}_{c\sim\mathrm{Unif}[0,1]^{d}}[\phi^{\top}(c\odot x)]\). Note that,

\[\widetilde{\phi}(x)^{2} =\left(\mathbb{E}_{c\sim\mathrm{Unif}[0,1]^{d}}[\phi^{\top}(c \odot x)]\right)^{2}\] (71) \[=\phi^{\top}(\mathbb{E}_{c\sim\mathrm{Unif}[0,1]^{d}}[c\odot x])( \mathbb{E}_{c\sim\mathrm{Unif}[0,1]^{d}}[c\odot x])^{\top}\phi\] (72) \[\implies\int_{\mathcal{X}}[\widetilde{\phi}(x)]^{2}\ \mathrm{dP}_{ \mathsf{U}} =\phi^{\top}\widetilde{\Sigma}\phi\] (73)

Further, since \(\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}[\phi(a)^{2}]=\phi^{\top}\Sigma\phi\) we can now rewrite our main optimization problem for \(k=1\) as:

\[\operatorname*{arg\,min}_{\phi:\phi^{\top}\Sigma_{A}\phi=1}\quad \phi^{\top}\Sigma_{A}\phi-\phi^{\top}\widetilde{\Sigma}\phi\] (74) \[=\operatorname*{arg\,max}_{\phi:\phi^{\top}\Sigma_{A}\phi=1}\ \phi^{\top}\widetilde{\Sigma}\phi\] (75)

Recall that in our setup both \(\widetilde{\Sigma}\) and \(\Sigma_{A}\) are positive definite and invertible matrices. To solve the above problem, let's consider a re-parameterization: \(\phi^{\prime}=\Sigma_{A}^{1/2}\phi\), thus \(\phi^{\top}\Sigma_{A}\phi=1\), is equivalent to the constraint \(\|\phi^{\prime}\|_{2}^{2}=1\). Based on this re-parameterization we are now solving:\[\arg\max_{\left|\phi^{\prime}\right|_{2}^{2}=1}\ \ \phi^{\prime\top}\Sigma_{A}^{-1/2} \cdot\tilde{\Sigma}\cdot\Sigma_{A}^{-1/2}\phi^{\prime},\] (76)

which is nothing but the top eigenvector for \(\Sigma_{A}^{-1/2}\cdot\tilde{\Sigma}\cdot\Sigma_{A}^{-1/2}\).

Now, to extend the above argument from \(k=1\) to \(k>1\), we need to care of one additional form of constraint in the form of feature diversity: \(\phi_{i}^{\top}\Sigma_{A}\phi_{j}=0\) when \(i\neq j\). But, we can easily redo the reformulations above and arrive at the following optimization problem:

\[\arg\max_{\left\|\phi^{\prime}_{i}\right\|_{2}^{2}=1,\ \forall i} \left[\phi^{\prime}_{1},\phi^{\prime}_{2},\ldots,\phi^{\prime}_{ k}\right]^{\top}\Sigma_{A}^{-1/2}\cdot\tilde{\Sigma}\cdot\Sigma_{A}^{-1/2} \left[\phi^{\prime}_{1},\phi^{\prime}_{2},\ldots,\phi^{\prime}_{k}\right],\] (77) \[\phi^{\prime\top}_{i}\phi^{\prime}_{j}=0,\ \ \forall i\neq j\]

where \(\phi^{\prime}_{i}=\Sigma_{A}^{1/2}\phi_{i}\). The above is nothing but the top \(k\) eigenvectors for the matrix \(\Sigma_{A}^{-1/2}\cdot\tilde{\Sigma}\cdot\Sigma_{A}^{-1/2}\). This completes the proof of Proposition 12. 

#### e.1.2 Analysis with \(\rho>0\) in Contrastive Pretraining Objective (3)

In (3) we considered the strict version of the optimization problem where \(\rho=0\). Here, we will consider the following optimization problem that we optimize for our experiments in the simplified setup:

\[\mathcal{L}_{\mathrm{cl}}(\Phi,\kappa)\ \coloneqq\ \mathbb{E}_{x\sim\mathrm{P}_{0}} \mathbb{E}_{a_{1},a_{2}\sim\mathrm{P}_{A}(\cdot|x)}\ \|\Phi(a_{1})-\Phi(a_{2})\|_{2}^{2}+\kappa\cdot\left| \mathbb{E}_{a\sim\mathrm{P}_{A}}\left[\Phi(a)\Phi(a)^{\top}\right]-\mathbf{I} _{k}\right|_{F}^{2},\] (78)

where \(\kappa>0\) is some finite constant (note that every \(\rho\) corresponds to some \(\kappa\) and particularly \(\rho=0\), corresponds to \(\kappa=\infty\)). Let \(\Phi^{\star}\) be the solution for (3) with \(\rho=0\), _i.e._ the solution described in Proposition 3. Now, we will show that in practice we can provably recover something close to \(\Phi^{\star}\) when \(\kappa\) is large enough.

**Theorem 13** (Solution for (78) is approximately equal to \(\Phi^{\star}\)).: _If \(\widehat{\Phi}\) is some solution that achieves low values of the objective \(\mathcal{L}_{\mathrm{cl}}(\Phi,\kappa)\) in (78), i.e., \(\mathcal{L}_{\mathrm{cl}}(\widehat{\Phi},\kappa)\leq\epsilon\), then there exists matrix \(W\in\mathbb{R}^{k\times k}\) such that:_

\[\mathbb{E}_{a\sim\mathrm{P}_{A}}\|W\cdot\Phi^{\star}(a)-\widehat {\Phi}(a)\|_{2}^{2}\leq\frac{k\epsilon}{2\gamma_{k+1}},\] \[\text{where, }\ \gamma_{k+1}\geq\frac{2\gamma_{1}^{2}}{k \epsilon}\cdot\left(1-\sqrt{\frac{\epsilon}{\kappa}}\right)-\frac{\gamma_{1}} {k},\]

_where \(\gamma_{k+1}\) is the the \(k+1^{th}\) eigenvalue for \(\mathbf{I}_{d}-\Sigma_{A}^{-1/2}\ \tilde{\Sigma}\ \Sigma_{A}^{-1/2}\). Here, \(\lambda_{1}\leq\lambda_{2}\leq\ldots\leq\lambda_{d}\)._

Proof.: Since we know that \(\mathcal{L}_{\mathrm{cl}}(\widehat{\Phi},\kappa)\leq\epsilon\), we can individually bound the invariance loss and the regularization term:

\[\mathbb{E}_{x\sim\mathrm{P}_{0}}\mathbb{E}_{a_{1},a_{2}\sim \mathrm{P}_{A}(\cdot|x)}\ \|\widehat{\Phi}(a_{1})-\widehat{\Phi}(a_{2})\|_{2}^{2}\leq\epsilon\] (79) \[\left|\mathbb{E}_{a\sim\mathrm{P}_{A}}\left[\widehat{\Phi}(a) \widehat{\Phi}(a)^{\top}\right]-\mathbf{I}_{k}\right|_{F}^{2}\leq\frac{ \epsilon}{\kappa}\] (80)

Thus,

\[\forall i\in[k]:\ \ 1-\sqrt{\frac{\epsilon}{\kappa}}\leq \widehat{\phi}_{i}^{\top}\ \Sigma_{A}\ \widehat{\phi}_{i}\leq 1+\sqrt{\frac{ \epsilon}{\kappa}}\] (81) \[\forall i\in[k]:\ \ \mathbb{E}_{x\sim\mathrm{P}_{0}}\mathbb{E}_{a_{1},a_{2} \sim\mathrm{P}_{A}(\cdot|x)}(\widehat{\phi}_{i}^{\top}a_{1}-\widehat{\phi}_{i} ^{\top}a_{2})^{2}\leq\epsilon\] (82)Let \(\phi_{1}^{\star},\phi_{2}^{\star},\phi_{3}^{\star},\ldots,\phi_{d}^{\star}\) be the solution returned by the analytical solution for \(\rho=0\), _i.e._ the solution in Proposition 3. Now, since \(\Phi^{\star}\) would span \(\mathbb{R}^{d}\) when \(\Sigma_{A}\) is full rank, we can denote:

\[\widehat{\phi}_{i}=\sum_{j=1}^{d}\eta_{i}^{(j)}\phi_{j}^{\star}\] (83)

Now from Lemma 22, the invariance loss for \(\widehat{\phi}_{i}\) can be written using the operator \(L(\phi)(a)=\phi(a)-\int_{\mathcal{A}}\frac{A_{+}(a,a^{\prime})}{p_{A}(a)}\phi (a^{\prime})\ \mathrm{d}a^{\prime}\):

\[\text{Invariance Loss}(\widehat{\phi}_{i}) :=\mathbb{E}_{x\sim\mathrm{P}_{\mathrm{U}}}\mathbb{E}_{a_{1},a_{2 }\sim\mathrm{P}_{\mathrm{A}}(\cdot\mid x)}(\widehat{\phi}_{i}^{\top}a_{1}- \widehat{\phi}_{i}^{\top}a_{2})^{2}\] (84) \[=2\cdot\mathbb{E}_{a\sim\mathrm{P}_{\mathrm{A}}}[\widehat{\phi} _{i}(a)L(\widehat{\phi}_{i})(a)]\] (85) \[=2\cdot\mathbb{E}_{a\sim\mathrm{P}_{\mathrm{A}}}\left[\left(\sum_ {j=1}^{d}\eta_{i}^{(j)}\phi_{i}^{\star}\right)L\left(\sum_{j=1}^{d}\eta_{i}^{( j)}\phi_{j}^{\star}\right)(a)\right]\] (86) \[=2\cdot\mathbb{E}_{a\sim\mathrm{P}_{\mathrm{A}}}\left[\left(\sum_ {j=1}^{d}\eta_{i}^{(j)}\phi_{j}^{\star}\right)\left(\sum_{j=1}^{d}\eta_{i}^{( j)}L(\phi_{j}^{\star})(a)\right)\right]\] (87) \[=2\cdot\sum_{j=1}^{d}\left(\eta_{i}^{(j)}\right)^{2}\mathbb{E}_{a \sim\mathrm{P}_{\mathrm{A}}}\left[\phi_{j}^{\star}(a)L(\phi_{j}^{\star})(a)\right]\] (88) \[\quad+2\cdot\sum_{m=1,n=1,m\neq n}^{d}\eta_{i}^{(m)}\eta_{i}^{(n) }\mathbb{E}_{a\sim\mathrm{P}_{\mathrm{A}}}\left[\phi_{m}^{\star}(a)L(\phi_{n}^ {\star})(a)\right]\] (89)

Since, \(\phi_{i}^{\star}(\cdot)\) are eigenfunctions of the operator \(L\)[38], we can conclude that:

\[\sum_{m=1,n=1,m\neq n}^{d}\eta_{i}^{(m)}\eta_{i}^{(n)}\mathbb{E}_{a\sim \mathrm{P}_{\mathrm{A}}}\left[\phi_{m}^{\star}(a)L(\phi_{n}^{\star})(a)\right] =0,\]

and if \(\gamma_{1}\leq\gamma_{2}\leq\gamma_{3}\ldots\leq\gamma_{d}\) are the eigenvalues for \(\phi_{1}^{\star},\phi_{2}^{\star},\phi_{3}^{\star},\ldots,\phi_{d}^{\star}\) under the decomposition of \(L(\phi)(\cdot)\) then:

\[\mathbb{E}_{x\sim\mathrm{P}_{\mathrm{U}}}\mathbb{E}_{a_{1},a_{2}\sim\mathrm{P }_{\mathrm{A}}(\cdot\mid x)}(\widehat{\phi}_{i}^{\top}a_{1}-\widehat{\phi}_{i }^{\top}a_{2})^{2}=2\cdot\sum_{j=1}^{d}\gamma_{j}\left(\eta_{i}^{(j)}\right)^{2}\] (90)

Recall, we are also aware of a condition on the regularization term: \(1-\sqrt{\frac{\epsilon}{\kappa}}\leq\widehat{\phi}_{i}^{\top}\ \Sigma_{A}\ \widehat{\phi}_{i}\leq 1+ \sqrt{\frac{\epsilon}{\kappa}}\).

\[\widehat{\phi}_{i}^{\top}\ \Sigma_{A}\ \widehat{\phi}_{i} =\left(\sum_{j=1}^{d}\eta_{i}^{(j)}\phi_{j}^{\star}\right)^{\top} \ \Sigma_{A}\ \left(\sum_{j=1}^{d}\eta_{i}^{(j)}\phi_{j}^{\star}\right)= \sum_{j=1}^{d}\left(\eta_{i}^{(j)}\right)^{2}\] (91) \[\implies 1-\sqrt{\frac{\epsilon}{\kappa}}\leq\sum_{j=1}^{d}\left(\eta_{i}^{( j)}\right)^{2}\leq 1+\sqrt{\frac{\epsilon}{\kappa}}\ \ \forall i.\] (92)

In order to show that the projection of \(\widehat{\phi}_{i}\) on \(\Phi^{\star}\) is significant, we need to argue that the term \(\sum_{j=k+1}^{d}\left(\eta_{i}^{(j)}\right)^{2}\) is small. The argument for this begins with the condition on invariance loss, and the fact that \(\gamma_{1}\leq\gamma_{2}\leq\ldots\leq\gamma_{k}\leq\gamma_{k+1}\leq\ldots\leq \gamma_{d}\):

\[\frac{\epsilon}{2}\geq\sum_{j=k+1}^{d}\left(\eta_{i}^{(j)}\right)^{2} \gamma_{j}\geq\gamma_{k+1}\cdot\left(\sum_{j=k+1}^{d}\left(\eta_{i}^{(j)} \right)^{2}\right)\] (93) \[\implies\sum_{j=k+1}^{d}\left(\eta_{i}^{(j)}\right)^{2}\leq\frac {\epsilon}{2\gamma_{k+1}}\] (94)Extending the above result \(\forall i\) by simply adding the bounds completes the claim of our first result in Theorem 13. Next, we will lower bound the eigenvalue \(\gamma_{k+1}\). Recall that, \(\sum_{j=1}^{k}\left(\eta_{i}^{(j)}\right)^{2}\geq 1-\sqrt{\frac{\varepsilon}{ \kappa}}-\frac{\epsilon}{2\gamma_{k+1}}\). Thus,

\[\gamma_{1}\cdot\left(1-\sqrt{\frac{\epsilon}{\kappa}}-\frac{ \epsilon}{2\gamma_{k+1}}\right)\leq\sum_{j=1}^{k}\gamma_{j}\left(\eta_{i}^{(j )}\right)^{2}\leq k\gamma_{k+1}\cdot\frac{\epsilon}{2\gamma_{1}}\] (95)

We assume that all eigenvalues are strictly positive, which is true under our augmentation distribution. Given, \(\gamma_{k+1}\geq\gamma_{1}\), we can rearrange the above to get:

\[\gamma_{k+1}\geq\frac{2\gamma_{1}^{2}}{k\epsilon}\cdot\left(1- \sqrt{\frac{\epsilon}{\kappa}}\right)-\frac{\gamma_{1}}{k}\] (96)

This completes the claim of our second result in Theorem 13. 

#### e.1.3 Proof of Theorem 5

In this section, we prove our main theorem about the recovery of both spurious \(w_{\mathrm{sp}}\), invariant \(w_{\mathrm{in}}\) features by the contrastive learning feature backbone, and also the amplification of the invariant over the spurious feature (where amplification is defined relatively with respect to what is observed in the data distribution alone). We begin by defining some quantities needed for analysis, that are fully determined by the choice of problem parameters for the model in (6).

From Section 4, we recall the definitions of \(w_{\mathrm{in}}\coloneqq[w^{\star},0,\ldots,0]\) and \(w_{\mathrm{sp}}\coloneqq[0,\ldots 0,w^{\prime}]\) where \(w^{\prime}=\mathbf{1}_{d_{\mathrm{sp}}}/\sqrt{d_{\mathrm{sp}}}\). Let us now define \(u_{1},u_{2}\) as the top two eigenvectors of \(\Sigma_{A}\) with eigenvalues \(\lambda_{1},\lambda_{2}>0\), (note that in our problem setup both \(\Sigma_{A}\) and \(\widetilde{\Sigma}\) are full rank positive definite matrices), and \(\tau:=\sqrt{\lambda_{1}/\lambda_{2}}\). Next we define \(\alpha\) as the angle between \(u_{1}\) and \(w_{\mathrm{in}}\), _i.e._, \(\cos(\alpha)=u_{1}^{\top}w_{\mathrm{in}}\). Based on the definitions of \(\alpha\) and \(\tau\), both of which are fully determined by the eigen decomposition of the post-augmentation feature covariance matrix \(\Sigma_{A}\), we now restate Theorem 5:

**Theorem 14** (Formal; CL recovers both invariant \(w_{\mathrm{in}}\) and spurious \(w_{\mathrm{sp}}\) but amplifies \(w_{\mathrm{in}}\)).: _Under Assumption 4\((w^{\star}=\nicefrac{{1}}{{4}}_{\mathrm{in}}/\sqrt{d_{\mathrm{in}}})\), the CL solution \(\Phi_{\mathrm{cl}}{=}[\phi_{1},\phi_{2},...,\phi_{k}]\) satisfies \(\phi_{j}^{\top}w_{\mathrm{in}}=\phi_{j}^{\top}w_{\mathrm{sp}}=0\)\(\forall j\geq 3\). For \(\tau,\alpha\) as defined above, the solution for \(\phi_{1},\phi_{2}\) is:_

\[\begin{bmatrix}w^{\star}\cdot\cot(\alpha)/\tau,&w^{\star}\\ w^{\prime}\cdot 1/\tau,&w^{\prime}\cdot\cot(\alpha)\end{bmatrix}\ \cdot\ \begin{bmatrix}\cos \theta,&\sin\theta\\ \sin\theta,&-\cos\theta\end{bmatrix},\]

_where \(0\leq\alpha,\theta\leq\pi/2\). Let us redefine \(\phi_{1}=c_{1}w_{\mathrm{in}}+c_{3}w_{\mathrm{sp}}\) and \(\phi_{2}=c_{2}w_{\mathrm{in}}+c_{4}w_{\mathrm{sp}}\). For constants \(K_{1},K_{2}>0\), \(\gamma=\nicefrac{{K_{1}K_{2}}}{{\sigma_{\mathrm{sp}}}}\), \(d_{\mathrm{sp}}=\nicefrac{{\sigma_{\mathrm{sp}}^{2}}}{{K_{2}^{2}}}\), \(\forall\epsilon>0\), \(\exists\sigma_{\mathrm{sp}_{0}}\), such that for \(\sigma_{\mathrm{sp}}\geq\sigma_{\mathrm{sp}_{0}}\):_

\[\frac{K_{1}K_{2}^{2}d_{\mathrm{in}}}{2L\sigma_{\mathrm{in}}^{2 }(d_{\mathrm{in}}-1)}+\epsilon\ \geq\ \frac{c_{1}}{c_{3}}\ \geq\ \frac{K_{1}K_{2}^{2}d_{\mathrm{in}}}{2L\sigma_{\mathrm{in}}^{2 }(d_{\mathrm{in}}-1)}-\epsilon\] \[\frac{L\sqrt{d_{\mathrm{sp}}}}{\gamma}+\epsilon\ \geq\ \left|\frac{c_{2}}{c_{4}}\right|\ \geq\ \frac{L\sqrt{d_{\mathrm{sp}}}}{\gamma}-\epsilon,\]

_where \(L=1+K_{2}^{2}\)._

Proof.: We will first show that the only components of interest are \(\phi_{1},\phi_{2}\). Then, we will prove conditions on the amplification of \(w_{\mathrm{in}}\) over \(w_{\mathrm{sp}}\) in \(\phi_{1},\phi_{2}\). Following is the proof overview:

1. When \(w^{\star}=\mathbf{1}_{d_{\mathrm{in}}}/\sqrt{d_{\mathrm{in}}^{-}}\), from the closed form expressions for \(\Sigma_{A}\) and \(\widetilde{\Sigma}\), show that the solution returned by solving the Barlow Twins objective depends on \(w_{\mathrm{in}}\) and \(w_{\mathrm{sp}}\) only through the first two components \(\phi_{1},\phi_{2}\).

2. For the components \(\phi_{1},\phi_{2}\), we will show that the dependence along \(w_{\rm in}\) is amplified compared to \(w_{\rm sp}\) when the target data sufficiently denoises the spurious feature (_i.e._, \(\sigma_{\rm sp}\) is sufficiently large).

#### Part-I:

We can divide the space \(\mathbb{R}^{d}\) into two subspaces that are perpendicular to each other. The first subspace is \(\mathcal{W}=\{b_{1}\cdot w_{\rm in}+b_{2}\cdot w_{\rm sp}:b_{1},b_{2}\in \mathbb{R}\}\), _i.e._ the rank \(2\) subspace spanned by \(w_{\rm in}\) and \(w_{\rm sp}\). The second subspace is \(\mathcal{W}_{\perp}\) where \(\mathcal{W}_{\perp}=\{u\in\mathbb{R}^{d}:u^{\top}w_{\rm in}=0,u^{\top}w_{\rm sp }=0\}\). Then, from Lemma 23 we can conclude that the matrix \(\Sigma_{A}\) can be written as:

\[\Sigma_{A} =\Sigma_{A_{\mathcal{W}}}+\Sigma_{A_{\mathcal{W}_{\perp}}}\] \[\Sigma_{A_{\mathcal{W}}} =\frac{1}{4}\begin{bmatrix}\left(\gamma^{2}(1+\nicefrac{{1}}{{3d _{\rm in}}})+\nicefrac{{\sigma_{\rm in}^{2}}}{{3}}(1-\nicefrac{{1}}{{d_{\rm in} }})\right)\cdot w^{\star}{w^{\star}}^{\top},&\nicefrac{{\gamma\sqrt{d_{\rm sp }}}}{{2}}\cdot w^{\star}{w^{\prime}}^{\top}\\ \gamma\nicefrac{{d_{\rm sp}}}{{2}}\cdot w^{\prime}{w^{\star}}^{\top},&\left( \nicefrac{{d_{\rm sp}}}{{2}}+\nicefrac{{4}}{{3}}\cdot\sigma_{\rm sp}^{2}+ \nicefrac{{1}}{{6}}\right)\cdot w^{\prime}{w^{\prime}}^{\top}\end{bmatrix},\] (97)

where \(\Sigma_{A_{\mathcal{W}_{\perp}}}:=\mathbb{E}_{a\sim\mathrm{P}_{A}}\left[\Pi_{ \mathcal{W}_{\perp}}(a)(\Pi_{\mathcal{W}_{\perp}}(a))^{\top}\right]\) is the covariance matrix in the null space of \(\mathcal{W}\), and \(\Pi_{\mathcal{W}_{\perp}}(a)\) is the projection of augmentation \(a\) into the null space of \(\mathcal{W}\), _i.e._ the covariance matrix in the space of non-predictive (noise) features. Similarly we can define:

\[\widetilde{\Sigma} = \widetilde{\Sigma}_{\mathcal{W}}\ \ +\ \ \widetilde{\Sigma}_{\mathcal{W}_{\perp}}\] \[\widetilde{\Sigma}_{\mathcal{W}} = \frac{1}{4}\begin{bmatrix}\gamma^{2}\cdot{w^{\star}}{w^{\star}}^{ \top},&\nicefrac{{\gamma\sqrt{d_{\rm sp}}}}{{2}}\cdot{w^{\star}}{w^{\prime}}^ {\top}\\ \gamma\nicefrac{{\sqrt{d_{\rm sp}}}}{{2}}\cdot{w^{\prime}}{w^{\star}}^{\top}, &\left(\nicefrac{{d_{\rm sp}}}{{2}}+\nicefrac{{\sigma_{\rm sp}^{2}}}{{2}} \right)\cdot{w^{\prime}}{w^{\prime}}^{\top}\end{bmatrix}\] (98)

Here again \(\widetilde{\Sigma}_{\mathcal{W}_{\perp}}:=\mathbb{E}_{x\sim\mathrm{P}_{0}} \left[\Pi_{\mathcal{W}_{\perp}}(\mathbb{E}_{c\sim\mathrm{Unif}[0,1]^{d}}(c \,\odot\,x))(\Pi_{\mathcal{W}_{\perp}}(\mathbb{E}_{c\sim\mathrm{Unif}[0,1]^{d }}(c\,\odot\,x)))^{\top}\right]\) is the covariance matrix of mean augmentations after they are projected onto the null space of predictive features. The above decomposition also follows from result in Lemma 23.

From Proposition 3, the closed form expression for the solution returned by optimizing the Barlow Twins objective in (3) is \(U^{\top}\Sigma_{A}^{-1/2}\) where \(U\) are the top-k eigenvectors of:

\[\Sigma_{A}^{-1/2}\cdot\widetilde{\Sigma}\cdot\Sigma_{A}^{-1/2}\] (99)

When \(w^{\star}=\mathbbm{1}_{d_{\rm in}}/\sqrt{d_{\rm in}}\), then \(\Sigma_{A_{\mathcal{W}_{\perp}}}=\widetilde{\Sigma}_{\mathcal{W}_{\perp}}+B\) where \(B\) is a diagonal matrix with diagonal given by \(\frac{1}{3}\cdot\mathrm{diag}(\widetilde{\Sigma}_{\mathcal{W}_{\perp}})\). Further, since \(\mathrm{diag}(\widetilde{\Sigma}_{\mathcal{W}_{\perp}})=p\cdot\mathbbm{1}_{d}\) for some constant \(p>0\), the eigenvectors of \(\widetilde{\Sigma}_{\mathcal{W}_{\perp}}\) and \(\Sigma_{A_{\mathcal{W}_{\perp}}}\) are exactly the same. Hence, when we consider the SVD of the expression \(\Sigma_{A}^{-1/2}\widetilde{\Sigma}\Sigma_{A}^{-1/2}\), the matrices \(\Sigma_{A_{\mathcal{W}_{\perp}}}\) and \(\widetilde{\Sigma}_{\mathcal{W}_{\perp}}\) have no effect on the SVD components that lie along the span of the predictive features. In fact, we only need to consider two rank 2 matrices (first terms in (98), (97)) and only do the SVD of \(\Sigma_{A_{\mathcal{W}}}^{-1/2}\cdot\widetilde{\Sigma}_{\mathcal{W}}\cdot \Sigma_{A_{\mathcal{W}}}^{-1/2}\).

There are only two eigenvectors of \(\Sigma_{A_{\mathcal{W}}}^{-1/2}\cdot\widetilde{\Sigma}_{\mathcal{W}}\cdot \Sigma_{A_{\mathcal{W}}}^{-1/2}\). We use \(\lambda_{1},\lambda_{2}\) to denote the eigenvalues of \(\Sigma_{A_{\mathcal{W}}}\), and \([\cos(\alpha)w^{\star},\sin(\alpha)w^{\prime}]^{\top}\), \([\sin(\alpha)w^{\star},-\cos(\alpha)w^{\prime}]^{\top}\) for the corresponding eigenvectors. Similarly, we use \(\widetilde{\lambda}_{1},\widetilde{\lambda}_{2}\) to denote the eigenvalues of \(\widetilde{\Sigma}_{\mathcal{W}}\), and \([\cos(\beta)w^{\star},\sin(\beta)w^{\prime}]^{\top}\), \([\sin(\beta)w^{\star},-\cos(\beta)w^{\prime}]^{\top}\) for the corresponding eigenvectors. Let \(\mathrm{SVD}_{U}(\cdot)\) denote the operation of obtaining the singular vectors of a matrix. Then, to compute the components of the final expression: \(\mathrm{SVD}_{U}(\Sigma_{A}^{-1/2}\widetilde{\Sigma}\Sigma_{A}^{-1/2})^{\top} \Sigma_{A}^{-1/2}\) that lies along the span of predictive features (in \(\mathcal{W}\)), we need only look at the decomposition of the following matrix:

\[\begin{bmatrix}\cos\theta&,&\sin(\theta)\\ \sin\theta&,&-\cos(\theta)\end{bmatrix}=\mathrm{SVD}_{U}\left(\begin{bmatrix} \nicefrac{{1}}{{\sqrt{\lambda_{1}}}},&0\\ 0,&\nicefrac{{1}}{{\sqrt{\lambda_{2}}}}\end{bmatrix}\cdot\begin{bmatrix}\cos( \alpha-\beta),&\sin(\alpha-\beta)\\ \sin(\alpha-\beta),&-\cos(\alpha-\beta)\end{bmatrix}\cdot\begin{bmatrix}\sqrt{ \widetilde{\lambda}_{1}},&0\\ 0,&\sqrt{\widetilde{\lambda}_{2}}\end{bmatrix}\right)\] (100)

Based on the above definitions of \(\theta,\alpha,\lambda_{1},\lambda_{2}\), we can then formulate \(\phi_{1}\) and \(\phi_{2}\) in the following way:\[[\phi_{1},\phi_{2}]=\begin{bmatrix}w^{\star}\cdot\frac{\cos(\alpha)}{\sqrt{\lambda_{1 }}},&w^{\star}\cdot\frac{\sin(\alpha)}{\sqrt{\lambda_{2}}}\\ w^{\prime}\cdot\frac{\sin(\alpha)}{\sqrt{\lambda_{1}}},&w^{\prime}\cdot\frac{- \cos(\alpha)}{\sqrt{\lambda_{2}}}\end{bmatrix}\cdot\begin{bmatrix}\cos\theta \ \,&\sin(\theta)\\ \sin\theta\ \,&-\cos(\theta)\end{bmatrix}\] (101)

To summarize, using arguments in Lemma 23 and the fact that \(w^{\star}=\mathbf{1}_{d_{\mathrm{in}}}/\sqrt{d_{\mathrm{in}}}\), we can afford to focus on just two rank two matrices \(\Sigma_{A_{\mathcal{W}}},\widetilde{\Sigma}_{\mathcal{W}}\) in the operation: \(\mathrm{SVD}_{U}(\Sigma_{A}^{-1/2})\widetilde{\Sigma}\Sigma_{A}^{-1/2}\). The other singular vectors from the SVD only impact directions that span \(\mathcal{W}_{\perp}\), and the singular vectors obtained by considering only the rank 2 matrices lie only in the space of \(\mathcal{W}\).

**Part-II:**

From the previous part we obtained forms of \(\phi_{1},\phi_{2}\) in terms of: \(\lambda_{1},\lambda_{2},\alpha,\theta\), all of which are fully specified by the SVD of \(\Sigma_{A_{\mathcal{W}}}\) and \(\widetilde{\Sigma}_{\mathcal{W}}\). If we define \(\tau:=\frac{\sqrt{\lambda_{1}}}{\sqrt{\lambda_{2}}}\), we can evaluate \(c_{1},c_{2},c_{3},c_{4}\) as:

\[c_{1} =\frac{\cot(\alpha)}{\tau}+\tan(\theta)\] (102) \[c_{2} =-1+\frac{\cot(\alpha)\tan(\theta)}{\tau}\] (103) \[c_{3} =\frac{1}{\tau}-\cot(\alpha)\tan(\theta)\] (104) \[c_{4} =\frac{\tan(\theta)}{\tau}+\cot(\alpha)\] (105)

Now, we are ready to begin proofs for our claims on the amplification factors, _i.e._ on the ratios \(c_{1}/c_{3}\), \(|c_{2}/c_{4}|\).

We will first prove some limiting conditions for \(c_{1}/c_{3}\), followed by those on \(|c_{2}/c_{4}|\). For each of these conditions we will rely on the forms for \(c_{1},c_{2},c_{3},c_{4}\) derived in the previous part, in terms of \(\alpha,\theta,\tau\) (where \(0\leqslant\alpha,\theta\leqslant\pi/2\)). We will also rely on some lemmas that characterize the asymptotic behavior of \(\alpha,\theta\) and \(\tau\) as we increase \(\sigma_{\mathrm{sp}}\). We defer the full proof of these helper lemmas to later sections.

**Asymptotic behavior of \(c_{1}/c_{3}\).**

From Lemma 25 and Lemma 26, when \(\gamma=\nicefrac{{K_{1}}}{{\sqrt{z}}}\) and \(\sigma_{\mathrm{sp}}=K_{2}\sqrt{z}\), then:

\[\lim_{z\to\infty}\frac{c_{1}}{c_{3}}=\frac{\cot\alpha+\tau\tan\theta}{1-\tau \cot\alpha\tan\theta}=\lim_{z\to\infty}\tau\tan\theta=\frac{K_{1}K_{2}^{2}}{( 1+K_{2}^{2})2\sigma_{\mathrm{in}}^{2}(1-\nicefrac{{1}}{{d_{\mathrm{in}}}})},\] (106)

where we apply Moore-Osgood when applying limits on intermediate forms. We can do this since \(\tau\tan\theta\) approaches a constant, and each of \(\cot\alpha,\tau\) and \(\tan\theta\) are continuous and smooth functions of \(z\) (see Lemma 24).

**Asymptotic behavior of \(\nicefrac{{|c_{2}/c_{4}|}}{{\tau}}\).**

When we consider the limiting behavior of \(\nicefrac{{c_{2}/c_{4}z}}{{\tau}}\), as we increase \(z\) or equivalently \(\sigma_{\mathrm{sp}}\) when \(\gamma=\nicefrac{{K_{1}}}{{\sqrt{z}}}\) and \(\sigma_{\mathrm{sp}}=K_{2}\sqrt{z}\), then we get:

\[\lim_{z\to\infty}\left|\frac{c_{2}}{c_{4}z}\right|=\left|\frac{-1+\cot(\alpha) \tan(\theta)}{\frac{\tan(\theta)z}{\tau}+\cot(\alpha)z}\right|.\] (107)

From Lemma 26, \(\cot\alpha\tan\theta\to 0\). Next, if we consider \(\lim_{z\to\infty}z\tan\nicefrac{{\theta}}{{\tau}}=\lim_{z\to\infty}\tau\tan \theta\cdot\nicefrac{{z}}{{\tau}^{2}}\). For \(\nicefrac{{z}}{{\tau}^{2}}\), we invoke Lemma 28, which states that when \(\gamma=\nicefrac{{K_{1}}}{{\sqrt{z}}}\) and \(\sigma_{\mathrm{sp}}=K_{2}\sqrt{z}\), then:

\[\lim_{z\to\infty}\frac{z}{\tau^{2}}=\frac{2\sigma_{\mathrm{in}}^{2}\nicefrac{{ 1}}{{3}}(1-\nicefrac{{1}}{{d_{\mathrm{in}}}})}{1+\nicefrac{{4}}{{3}}K_{2}^{2 }}.\] (108)

Further, in our bound on \(c_{1}/c_{3}\), we derived that \(\tau\tan\theta\to\nicefrac{{K_{1}K_{2}^{2}}(1+K_{2}^{2})2\sigma_{\mathrm{in}}^{ 2}(1-\nicefrac{{1}}{{d_{\mathrm{in}}}})}\). Once again using Moore-Osgood we can plug this along with (108) to get:

[MISSING_PAGE_FAIL:41]

\[0.5\operatorname{erfc}\left(-\frac{c_{1}+\beta c_{2}}{c_{3}+\beta c_{4}}\cdot \frac{\gamma}{\sqrt{2\sigma_{\mathrm{sp}}}}\right)\] (117)

where \(\beta=\nicefrac{{(c_{2}\gamma+c_{4}\sqrt{d_{\mathrm{sp}}})}}{{(c_{1}\gamma+c_{3} \sqrt{d_{\mathrm{sp}}})}}\).

Substituting \(\beta\) into the expression \(\frac{c_{1}+\beta c_{2}}{c_{3}+\beta c_{4}}\) we get:

\[\frac{c_{1}^{2}\gamma+c_{1}c_{3}\sqrt{d_{\mathrm{sp}}}+c_{2}^{2}\gamma+c_{2}c _{4}\sqrt{d_{\mathrm{sp}}}}{c_{1}c_{3}\gamma+c_{3}^{2}\sqrt{d_{\mathrm{sp}}}+ c_{2}c_{4}\gamma+c_{4}^{2}\sqrt{d_{\mathrm{sp}}}}\] (118)

We first substitute expressions for \(c_{1},c_{2},c_{3},c_{4}\) from (102), (103), (104) and (105) in the above expression. Then for \(\gamma=K_{1}/\sqrt{z},\sigma_{\mathrm{sp}}=K_{2}\sqrt{z}\), we substitute the expressions for \(\cot\alpha\), \(\tan\theta\), and \(\tau=\nicefrac{{\lambda_{1}}}{{\lambda_{2}}}\) with their corresponding closed form expressions (as functions of \(z\)) from Lemma 24. On the resulting expression we apply do repeated applications of L'Hopital's rule to get the following result:

\[\lim_{z\to\infty}\frac{c_{1}^{2}\gamma+c_{1}c_{3}\sqrt{d_{\mathrm{sp}}}+c_{2} ^{2}\gamma+c_{2}c_{4}\sqrt{d_{\mathrm{sp}}}}{c_{1}c_{3}\gamma+c_{3}^{2}\sqrt{ d_{\mathrm{sp}}}+c_{2}c_{4}\gamma+c_{4}^{2}\sqrt{d_{\mathrm{sp}}}}=\frac{2K_{2}^ {2}K_{1}}{\sigma_{\mathrm{in}}^{2}(1-\nicefrac{{1}}{{d_{\mathrm{in}}}})}\] (119)

Based on \(\gamma,d_{\mathrm{sp}},\sigma_{\mathrm{sp}}\) defined in Theorem 5, and (119) we can conclude that \(\exists\sigma_{\mathrm{sp}_{1}}\) such that for all \(\sigma_{\mathrm{sp}}\geq\sigma_{\mathrm{sp}_{1}}\):

\[\frac{4K_{2}^{2}K_{1}}{\sigma_{\mathrm{in}}^{2}(1-\nicefrac{{1}}{{d_{\mathrm{ in}}}})}\;\geq\;\frac{c_{1}^{2}\gamma+c_{1}c_{3}\sqrt{d_{\mathrm{sp}}}+c_{2}^{2} \gamma+c_{2}c_{4}\sqrt{d_{\mathrm{sp}}}}{c_{1}c_{3}\gamma+c_{3}^{2}\sqrt{d_{ \mathrm{sp}}}+c_{2}c_{4}\gamma+c_{4}^{2}\sqrt{d_{\mathrm{sp}}}}\;\geq\;\frac{ K_{2}^{2}K_{1}}{\sigma_{\mathrm{in}}^{2}(1-\nicefrac{{1}}{{d_{\mathrm{in}}}})}\] (120)

Finally, applying (120) to Lemma 29, we conclude the following: When \(\gamma=\nicefrac{{K_{1}K_{2}}}{{\sigma_{\mathrm{sp}}}},d_{\mathrm{sp}}= \nicefrac{{\sigma_{\mathrm{sp}}^{2}}}{{K_{2}^{2}}}\), there exists \(\sigma_{\mathrm{sp}_{1}}\), such that for any \(\sigma_{\mathrm{sp}}\geq\sigma_{\mathrm{sp}_{1}}\), target accuracy of CL is at least \(0.5\operatorname{erfc}\left(-L^{\prime}\cdot\frac{\gamma}{\sqrt{2\sigma_{ \mathrm{sp}}}}\right)\) and at most \(0.5\operatorname{erfc}\left(-4L^{\prime}\cdot\frac{\gamma}{\sqrt{2\sigma_{ \mathrm{sp}}}}\right)\), where \(L^{\prime}=\frac{K_{2}^{2}K_{1}}{\sigma_{\mathrm{in}}^{2}(1-\nicefrac{{1}}{{d_ {\mathrm{in}}}})}\).

Comparison with ERM.Recall from Theorem 8 the performance of ERM classifier (trained from scratch) is \(0.5\operatorname{erfc}\left(-\nicefrac{{\gamma}}{{\sqrt{2d_{\mathrm{sp}}}}} \sigma_{\mathrm{sp}}\right)\). The lower bound on the performance of classifier over CL representations is strictly better than ERM when:

\[\frac{\gamma}{\sqrt{d_{\mathrm{sp}}}}<L^{\prime}\] \[\iff\frac{K_{2}^{2}K_{1}}{\sigma_{\mathrm{in}}^{2}(1-\nicefrac{{ 1}}{{d_{\mathrm{in}}}})}>\frac{\gamma}{\sqrt{d_{\mathrm{sp}}}}\iff\frac{K_{2}^ {2}K_{1}}{\sigma_{\mathrm{in}}^{2}(1-\nicefrac{{1}}{{d_{\mathrm{in}}}})}> \frac{K_{1}K_{2}^{2}}{\sigma_{\mathrm{sp}}^{2}}\] \[\iff\sigma_{\mathrm{sp}}>\sigma_{\mathrm{in}}\sqrt{1-\nicefrac{{1}} {{d_{\mathrm{in}}}}}\iff\sigma_{\mathrm{sp}_{1}}>\sigma_{\mathrm{in}}\sqrt{1- \nicefrac{{1}}{{d_{\mathrm{in}}}}}.\]

This completes our proof of Corollary 6.

### Analysis of STOC: Formal Statement of Theorem 7

Recall ERM solution over contrastive pretraining. We showed that without loss of generality when \(k\) (the output dimensionality of \(\Phi\)) is greater than 2, we can restrict \(k\) to 2 and the \(\Phi\) can be denoted as \([\phi_{1},\phi_{2}]^{\top}\) where \(\phi_{1}=c_{1}w^{\star}+c_{3}w_{\mathrm{sp}}\) and \(\phi_{2}=c_{2}w^{\star}+c_{4}w_{\mathrm{sp}}\). The ERM solution of the linear head is then given by \(h_{1},h_{2}\in\mathbb{R}\):

\[h_{1}=c_{1}\cdot\gamma+c_{3}\cdot\sqrt{d_{\mathrm{sp}}}\,,\;\;\mathrm{and}\;\;h_ {2}=c_{2}\cdot\gamma+c_{4}\cdot\sqrt{d_{\mathrm{sp}}}\,.\] (121)

STOC performs self-training of the linear head over the CL solution. Before introducing the result, we need some additional notation. Let \(h^{t}\) denote the solution of the linear head at iterate \(t\). Without loss of generality, assume that the coefficients in \(\phi_{1}=c_{1}w_{\mathrm{in}}+c_{3}w_{\mathrm{sp}}\) and \(\phi_{2}=c_{2}w_{\mathrm{in}}+c_{4}w_{\mathrm{sp}}\) are such that \(c_{2}\) is positive and \(c_{1},c_{3},\) and \(c_{4}\) are negative. Moreover, for simplicity of exposition, assume that \(|c_{4}|>|c_{3}|\).

**Theorem 16**.: _Under the conditions of Corollary 15 and when \(\frac{\gamma^{2}}{\sigma_{\mathrm{sp}}}\geqslant\left[\frac{-c_{3}-c_{4}}{(c_{2}+c _{1})\cdot|c_{1}|}\right]\vee\left[\frac{c_{4}}{c_{1}\cdot c_{2}}\right]\), the target accuracy of ST over CL is lower bounded by \(0.5\cdot\mathrm{erfc}\left(-\lfloor c_{2}^{2}/c_{4}\rfloor\cdot\gamma/( \sqrt{2}\sigma_{\mathrm{sp}})\right)\geqslant 0.5\cdot\mathrm{erfc}\left(-L\cdot\sqrt{d_{ \mathrm{sp}}/(\sqrt{2}\sigma_{\mathrm{sp}})}\right)\) with \(L\geqslant 1\)._

Before proving Theorem 16, we first connect the condition \(\frac{\gamma^{2}}{\sigma_{\mathrm{sp}}}\geqslant\left[\frac{-c_{3}-c_{4}}{(c_ {2}+c_{1})\cdot|c_{1}|}\right]\vee\left[\frac{c_{4}}{c_{1}\cdot c_{2}}\right]\) with the result obtained with contrastive learning.

**Remark 1**.: We first argue that \(\left[\frac{-c_{3}-c_{4}}{(c_{2}+c_{1})\cdot|c_{1}|}\right]\) term dominates and hence, if we have \(\frac{\gamma^{2}}{\sigma_{\mathrm{sp}}}\geqslant\left[\frac{-c_{3}-c_{4}}{(c_ {2}+c_{1})\cdot|c_{1}|}\right]\), then we get the result in Theorem 16. First, recall that as \(\sigma_{\mathrm{sp}}\) increases, we have \(\left|\frac{c_{3}}{c_{1}}\right|\) converge to \(\frac{2L\sigma_{\mathrm{sp}}^{2}(d_{\mathrm{in}}-1)}{K_{1}K_{2}^{2}d_{\mathrm{ in}}}\), \(c_{2}\to 1\) and \(\frac{c_{1}}{c_{2}}\to 0\). Using these limits, we get:

\[\frac{\gamma^{2}}{\sigma_{\mathrm{sp}}}=\frac{K_{1}^{2}}{K_{2}\cdot z^{3/2}} \geqslant\frac{2L\sigma_{\mathrm{in}}^{2}(d_{\mathrm{in}}-1)}{K_{1}K_{2}^{2}d _{\mathrm{in}}}\;.\] (122)

which reduces the following condition: \(d_{\mathrm{sp}}\leqslant K_{1}^{2}K_{2}^{2/3}\cdot\left(\frac{d_{\mathrm{in}} }{2L\sigma_{\mathrm{in}}^{2}(d_{\mathrm{in}}-1)}\right)^{2/3}\).

Proof.: First, we create an outline of the proof. We argue about the updates of \(h^{t}\) showing that both \(h^{t}_{1}\) and \(h^{t}_{2}\) increase with \(|h^{t}_{2}|\) becoming greater than \(|h^{t}_{1}|\) for some large \(t\). Then we show that \(|h^{t}_{2}|\geqslant|h^{t}_{1}|\) is sufficient to obtain near-perfect target generalization.

Part 1.Recall the loss of used for self-training of \(h\):

\[\mathcal{L}_{\mathrm{st}}(h) =\mathbb{E}_{\mathrm{P}_{\mathrm{T}}(x)}\left[\ell(h^{\top} \Phi x,\mathrm{sgn}(h^{\top}\Phi x))\right]\] (123) \[=\mathbb{E}_{\mathrm{P}_{\mathrm{T}}(x)}\left[\exp\left(-\left|h^ {\top}\Phi x\right|\right)\right]\] (124) \[=\mathbb{E}_{z\sim\mathcal{N}(0,1)}\left[\exp\left(-\left|c_{1} \gamma h_{1}+c_{2}\gamma h_{2}+(c_{3}\sigma_{\mathrm{sp}}h_{1}+c_{4}\sigma_{ \mathrm{sp}}h_{2})\cdot z\right|\right)\right]\,.\] (125)

Define \(\mu_{t}=c_{1}\gamma h^{t}_{1}+c_{2}\gamma h^{t}_{2}\) and \(\sigma_{t}=c_{3}\sigma_{\mathrm{sp}}h^{t}_{1}+c_{4}\sigma_{\mathrm{sp}}h^{t}_ {2}\). With this notation, we can re-write the loss in (125) as \(\mathcal{L}_{\mathrm{st}}(h^{t})=\mathbb{E}_{z\sim\mathcal{N}(0,\sigma_{t}^{2} )}\left[\exp\left(-\left|\mu_{t}+z\right|\right)\right]\).

Similar to the the treatment in Theorem 9, we now derive a closed-form expression of \(\mathcal{L}_{\mathrm{st}}(h^{t})\) in Lemma 30:

\[\mathcal{L}_{\mathrm{st}}(h^{t})=\frac{1}{2}\left(\exp\left(\frac{\sigma_{t}^{ 2}}{2}-\mu_{t}\right)\cdot\mathrm{erfc}\left(-\frac{\mu_{t}}{\sqrt{2}\sigma_{ t}}+\frac{\sigma_{t}}{\sqrt{2}}\right)+\exp\left(\frac{\sigma_{t}^{2}}{2}+\mu_{t} \right)\cdot\mathrm{erfc}\left(\frac{\mu_{t}}{\sqrt{2}\sigma_{t}}+\frac{ \sigma_{t}}{\sqrt{2}}\right)\right)\,.\] (126)

Define:

\[A_{1}(\mu_{t},\sigma_{t}) =\exp\left(\frac{\sigma_{t}^{2}}{2}-\mu_{t}\right)\cdot\mathrm{ erfc}\left(-\frac{\mu_{t}}{\sqrt{2}\sigma_{t}}+\frac{\sigma_{t}}{\sqrt{2}}\right)\] \[=\sqrt{\frac{2}{\pi}}\mathrm{exp}\left(-\frac{\mu_{t}^{2}}{2 \sigma_{t}^{2}}\right)\mathrm{r}\left(\sigma_{t}-\frac{\mu_{t}}{\sigma_{t}} \right)\,,\] (127) \[A_{2}(\mu_{t},\sigma_{t}) =\exp\left(\frac{\sigma_{t}^{2}}{2}+\mu_{t}\right)\cdot\mathrm{ erfc}\left(\frac{\mu_{t}}{\sqrt{2}\sigma_{t}}+\frac{\sigma_{t}}{\sqrt{2}}\right)\] \[=\sqrt{\frac{2}{\pi}}\mathrm{exp}\left(-\frac{\mu_{t}^{2}}{2 \sigma_{t}^{2}}\right)\mathrm{r}\left(\sigma_{t}+\frac{\mu_{t}}{\sigma_{t}} \right)\,,\] (128) \[A_{3}(\mu_{t},\sigma_{t}) =\frac{2\sqrt{2}}{\sqrt{\pi}}\mathrm{exp}\left(-\frac{\mu_{t}^{2} }{2\sigma_{t}^{2}}\right)\,.\] (129)Let \(\widetilde{h}^{t+1}\) denote the un-normalized gradient descent update at iterate \(t+1\). We have:

\[\widetilde{h}^{t+1}=h^{t}-\eta\cdot\frac{\partial\mathcal{L}_{\text{st}}(h^{t})}{ \partial h}\,.\] (130)

Now we will individually argue about the update of \(\widetilde{h}^{t+1}\). First, we have:

\[\widetilde{h}^{t+1}_{1} =h^{t}_{1}-\eta\cdot\frac{\partial\mathcal{L}_{\text{st}}(h^{t}) }{\partial h_{1}}\] \[\widetilde{h}^{t+1}_{1} =h^{t}_{1}-\eta\cdot\underbrace{\left[A_{1}\cdot(\sigma_{t}c_{3 }\sigma_{\text{sp}}-c_{1}\gamma)+A_{2}\cdot(\sigma_{t}c_{3}\sigma_{\text{sp}}+ c_{1}\gamma)-A_{3}c_{3}\sigma_{\text{sp}}\right]}_{\delta_{1}}\,.\] (131)

and second, we have:

\[\widetilde{h}^{t+1}_{2} =h^{t}_{2}-\eta\cdot\frac{\partial\mathcal{L}_{\text{st}}(h^{t}) }{\partial h_{2}}\] \[\widetilde{h}^{t+1}_{2} =h^{t}_{2}-\eta\cdot\underbrace{\left[A_{1}\cdot(\sigma_{t}c_{4 }\sigma_{\text{sp}}-c_{2}\gamma)+A_{2}\cdot(\sigma_{t}c_{4}\sigma_{\text{sp}} +c_{2}\gamma)-A_{3}c_{4}\sigma_{\text{sp}}\right]}_{\delta_{2}}\,.\] (132)

We will now argue the conditions under which \(h^{t+1}_{2}\) increases till its value reaches \(1/\sqrt{2}\). In particular, we will argue that when \(h^{t}_{2}\) is negative, the norm \(|h^{t}_{2}|\) decreases and when \(h^{t}_{2}\) becomes positive, then its norm increases. We show that the following three conditions are sufficient to argue the increasing value of \(h^{t}_{2}\): for all \(t\), we have (i) \(\mu_{t}\geq\mu_{c}\) and \(|\sigma_{t}|<\sigma_{c}\) for constant \(\mu_{c}=|c_{1}\cdot\gamma|/2\) and \(\sigma_{c}=|c_{4}\sigma_{\text{sp}}|\); (ii) \(\delta_{2}<0\); (iii) \(|\delta_{2}|\geq\delta_{1}\). In Lemma 18, we argue that our assumption on the initialization of the backbone learned with BT implies the previous three conditions.

**Case-1.** When \(h^{t}_{2}\) is negative (and after the update, it remains negative). Then we want to argue the following:

\[\frac{(h^{t}_{2}-\eta\delta_{2})^{2}}{(h^{t}_{2}-\eta\delta_{2})^ {2}+(h^{t}_{1}-\eta\delta_{1})^{2}} \leq(h^{t}_{2})^{2}\] (133) \[\Rightarrow \frac{(h^{t}_{2}-\eta\delta_{2})^{2}}{(h^{t}_{2})^{2}} \leq(h^{t}_{2}-\eta\delta_{2})^{2}+(h^{t}_{1}-\eta\delta_{1})^{2}\] (134) \[\Rightarrow \frac{h^{t\,2}_{2}+\eta^{2}\delta_{2}^{2}-2\eta\delta_{2}h^{t}_{ 2}}{(h^{t}_{2})^{2}} \leq{h^{t\,2}_{2}}^{2}+\eta^{2}\delta_{2}^{2}-2\eta h^{t}_{2} \delta_{2}+{h^{t\,2}_{1}}^{2}+\eta^{2}\delta_{1}^{2}-2\eta h^{t}_{1}\delta_{1}\] (135) \[\Rightarrow 1+\frac{\eta^{2}\delta_{2}^{2}-2\eta\delta_{2}h^{t}_{2}}{(h^{t}_ {2})^{2}} \leq 1+\eta^{2}\delta_{2}^{2}-2\eta h^{t}_{2}\delta_{2}+\eta^{2} \delta_{1}^{2}-2\eta h^{t}_{1}\delta_{1}\] (136) \[\Rightarrow \eta^{2}\delta_{2}^{2}-2\eta\delta_{2}h^{t}_{2} \leq\left[\eta^{2}\delta_{2}^{2}-2\eta h^{t}_{2}\delta_{2}+\eta^{2} \delta_{1}^{2}-2\eta h^{t}_{1}\delta_{1}\right](h^{t}_{2})^{2}\] (137) \[\Rightarrow \eta^{2}\delta_{2}^{2}(h^{t}_{1})^{2}-2\eta\delta_{2}h^{t}_{2}(h^{ t}_{2})^{2} \leq\eta^{2}\delta_{1}^{2}(h^{t}_{2})^{2}-2\eta h^{t}_{1}\delta_{1}(h^{t}_{2})^{2}\] (138) \[\Rightarrow \eta^{2}\delta_{2}^{2}(h^{t}_{1})^{2}-\eta^{2}\delta_{1}^{2}(h^{ t}_{2})^{2} \leq 2\eta\delta_{2}h^{t}_{2}(h^{t}_{1})^{2}-2\eta h^{t}_{1}\delta_{1}(h^{t}_{2})^{2}\] (139) \[\Rightarrow \left[\eta\delta_{2}(h^{t}_{1})-\eta\delta_{1}(h^{t}_{2})\right] \left[\eta\delta_{2}(h^{t}_{1})+\eta\delta_{1}(h^{t}_{2})\right] \leq 2h^{t}_{2}h^{t}_{1}\left[\eta\delta_{2}(h^{t}_{1})-\eta\delta_{1 }(h^{t}_{2})\right]\] (140) \[\Rightarrow \left[\eta\delta_{2}(h^{t}_{1})+\eta\delta_{1}(h^{t}_{2})\right] \leq 2h^{t}_{2}h^{t}_{1}\] (141)

Since \(\delta_{2}<0\), \(|\delta_{2}|\geq|\delta_{1}|\) and \(h^{t}_{2}<h^{t}_{1}<0\), we have \([\eta\delta_{2}(h^{t}_{1})-\eta\delta_{1}(h^{t}_{2})]\) as positive. This implies inequality (140) to (141) and for small enough \(\eta\), (141) will continue to hold true.

**Case-2.** When \(h_{2}^{t}\) is positive but less than \(1/\sqrt{2}\). Then we want to argue the following:

\[\frac{(h_{2}^{t}-\eta\delta_{2})^{2}}{(h_{2}^{t}-\eta\delta_{2})^{2 }+(h_{1}^{t}-\eta\delta_{1})^{2}} \geqslant(h_{2}^{t})^{2}\] (142) \[\Rightarrow \frac{(h_{2}^{t}-\eta\delta_{2})^{2}}{(h_{2}^{t})^{2}} \geqslant(h_{2}^{t}-\eta\delta_{2})^{2}+(h_{1}^{t}-\eta\delta_{1})^{2}\] (143) \[\Rightarrow \frac{{h_{2}^{t}}^{2}+\eta^{2}\delta_{2}^{2}-2\eta\delta_{2}h_{2 }^{t}}{(h_{2}^{t})^{2}} \geqslant{h_{2}^{t}}^{2}+\eta^{2}\delta_{2}^{2}-2\eta h_{2}^{t} \delta_{2}+{h_{1}^{t}}^{2}+\eta^{2}\delta_{1}^{2}-2\eta h_{1}^{t}\delta_{1}\] (144) \[\Rightarrow 1+\frac{\eta^{2}\delta_{2}^{2}-2\eta\delta_{2}h_{2}^{t}}{(h_{2 }^{t})^{2}} \geqslant 1+\eta^{2}\delta_{2}^{2}-2\eta h_{2}^{t}\delta_{2}+\eta^{2 }\delta_{1}^{2}-2\eta h_{1}^{t}\delta_{1}\] (145) \[\Rightarrow \eta^{2}\delta_{2}^{2}-2\eta\delta_{2}h_{2}^{t} \geqslant\left[\eta^{2}\delta_{2}^{2}-2\eta h_{2}^{t}\delta_{2}+ \eta^{2}\delta_{1}^{2}-2\eta h_{1}^{t}\delta_{1}\right](h_{2}^{t})^{2}\] (146) \[\Rightarrow \eta^{2}\delta_{2}^{2}(h_{1}^{t})^{2}-2\eta\delta_{2}h_{2}^{t}(h_ {1}^{t})^{2} \geqslant\eta^{2}\delta_{1}^{2}(h_{2}^{t})^{2}-2\eta h_{1}^{t} \delta_{1}(h_{2}^{t})^{2}\] (147) \[\Rightarrow \eta^{2}\delta_{2}^{2}(h_{1}^{t})^{2}-\eta^{2}\delta_{1}^{2}(h_{ 2}^{t})^{2} \geqslant 2\eta\delta_{2}h_{2}^{t}(h_{1}^{t})^{2}-2\eta h_{1}^{t} \delta_{1}(h_{2}^{t})^{2}\] (148) \[\Rightarrow \left[\eta\delta_{2}(h_{1}^{t})-\eta\delta_{1}(h_{2}^{t})\right] \left[\eta\delta_{2}(h_{1}^{t})+\eta\delta_{1}(h_{2}^{t})\right] \geqslant 2h_{2}^{t}h_{1}^{t}\left[\eta\delta_{2}(h_{1}^{t})-\eta \delta_{1}(h_{2}^{t})\right]\] (149) \[\Rightarrow \left[\eta\delta_{2}(h_{1}^{t})+\eta\delta_{1}(h_{2}^{t})\right] \geqslant 2h_{2}^{t}h_{1}^{t}\] (150)

Since \(\delta_{2}<0\), \(|\delta_{2}|\geqslant|\delta_{1}|\), \(h_{1}^{t}\leqslant-1/\sqrt{2}\) and \(0<h_{2}^{t}<1/\sqrt{2}\), we have \(\left[\eta\delta_{2}(h_{1}^{t})-\eta\delta_{1}(h_{2}^{t})\right]\) as positive. This implies inequality (149) to (150). Focusing on (150), we note that \(h_{1}^{t}\cdot\delta_{2}\) is positive and greater in magnitude than \(h_{2}^{t}\cdot\delta_{1}\). Moreover, since \(h_{2}^{t}h_{1}^{t}\) is negative, (150) will continue to hold true.

Now, when \(h_{2}^{t}\) is positive and greater than \(1/\sqrt{2}\), then \(h_{2}^{t}\) will stay in that region. Convergence of STOC together with conditions of convergence as in Lemma 17 will imply that the at convergence \(h_{2}^{t}\) will remain greater than \(1/\sqrt{2}\), such that \(\frac{h_{2}^{t}}{h_{2}^{t}}=\frac{\delta_{1}}{\delta_{2}}\). Now we bound the target error of STOC.

**Part 2.** To bound the accuracy at any iterate \(t\) when \(h_{2}^{t}\geqslant 1/\sqrt{2}\), we have from Lemma 29:

\[\mathbb{E}_{\mathrm{P_{T}}}\left[y\cdot\left({h^{t}}^{\top}\phi_{ \mathrm{cl}}x\right)>0\right]=\mathbb{E}_{z\sim\mathcal{N}(0,1)}\left[z>- \frac{c_{1}\gamma h_{1}^{t}+c_{2}\gamma h_{2}^{t}}{|c_{3}\sigma_{\mathrm{sp}}h _{1}^{t}+c_{4}\sigma_{\mathrm{sp}}h_{2}^{t}|}\right]\,.\] (151)

We now upper bound and lower bound the fraction \(\frac{c_{1}\gamma h_{1}^{t}+c_{2}\gamma h_{2}^{t}}{|c_{3}\sigma_{\mathrm{sp}}h _{1}^{t}+c_{4}\sigma_{\mathrm{sp}}h_{2}^{t}|}\) in RHS in (151): (i) \(c_{1}\gamma h_{1}^{t}+c_{2}\gamma h_{2}^{t}\geqslant c_{2}\gamma h_{2}^{t}\) since both \(c_{1}\gamma h_{1}^{t}\) and \(c_{2}\gamma h_{2}^{t}\) have same sign; (ii) \(|c_{3}\sigma_{\mathrm{sp}}h_{1}^{t}+c_{4}\sigma_{\mathrm{sp}}h_{2}^{t}| \leqslant|c_{4}\sigma_{\mathrm{sp}}h_{2}^{t}|\) because \(|c_{4}\sigma_{\mathrm{sp}}h_{2}^{t}|\geqslant|c_{3}\sigma_{\mathrm{sp}}h_{1}^{t}|\) and they have opposite signs. Hence, from (151), we have:

\[\mathbb{E}_{\mathrm{P_{T}}}\left[y\cdot\left({h^{t}}^{\top}\phi_{ \mathrm{cl}}x\right)>0\right]=\mathbb{E}_{z\sim\mathcal{N}(0,1)}\left[z>- \frac{c_{2}\gamma h_{2}^{t}}{|c_{4}\sigma_{\mathrm{sp}}h_{2}^{t}|}\right]= \mathbb{E}_{z\sim\mathcal{N}(0,1)}\left[z>-\frac{c_{2}\gamma}{|c_{4}\sigma_{ \mathrm{sp}}|}\right].\] (152)

Substituting the definition of \(\mathrm{erfc}\), the expression (152) gives us the required lower bound on the target accuracy.

**Lemma 17** (Convergence of STOC).: _Assume the gradient updates as in (131) and (132). Then STOC converges at \(t=t_{c}\) when \(\frac{h_{1}^{t_{c}}}{h_{2}^{t_{c}}}=\frac{\delta_{1}}{\delta_{2}}\). For \(t>t_{c}\), (131) and (132) make no updates to the linear \(h\)._

Proof.: When the gradient updates \(\delta_{1}\) and \(\delta_{2}\) are such that \(h_{1}^{t+1}\) matches \(h_{1}^{t}\), we have convergence of STOC.

\[\frac{(h_{2}^{t}-\eta\delta_{2})^{2}}{(h_{2}^{t}-\eta\delta_{2})^{2}+(h_{1}^{t}- \eta\delta_{1})^{2}} =(h_{2}^{t})^{2}\] (153) \[\Rightarrow \frac{(h_{2}^{t}-\eta\delta_{2})^{2}}{(h_{2}^{t})^{2}} =(h_{2}^{t}-\eta\delta_{2})^{2}+(h_{1}^{t}-\eta\delta_{1})^{2}\] (154) \[\Rightarrow \frac{{h_{2}^{t}}^{2}+\eta^{2}\delta_{2}^{2}-2\eta\delta_{2}h_{2 }^{t}}{(h_{2}^{t})^{2}} ={h_{2}^{t}}^{2}+\eta^{2}\delta_{2}^{2}-2\eta h_{2}^{t}\delta_{2} +{h_{1}^{t}}^{2}+\eta^{2}\delta_{1}^{2}-2\eta h_{1}^{t}\delta_{1}\] (155) \[\Rightarrow 1+\frac{\eta^{2}\delta_{2}^{2}-2\eta\delta_{2}h_{2}^{t}}{(h_{2} ^{t})^{2}} =1+\eta^{2}\delta_{2}^{2}-2\eta h_{2}^{t}\delta_{2}+\eta^{2}\delta _{1}^{2}-2\eta h_{1}^{t}\delta_{1}\] (156) \[\Rightarrow \eta^{2}\delta_{2}^{2}-2\eta\delta_{2}h_{2}^{t} =\left[\eta^{2}\delta_{2}^{2}-2\eta h_{2}^{t}\delta_{2}+\eta^{2} \delta_{1}^{2}-2\eta h_{1}^{t}\delta_{1}\right](h_{2}^{t})^{2}\] (157) \[\Rightarrow \eta^{2}\delta_{2}^{2}(h_{1}^{t})^{2}-2\eta\delta_{2}h_{2}^{t}(h_ {1}^{t})^{2} =\eta^{2}\delta_{1}^{2}(h_{2}^{t})^{2}-2\eta h_{1}^{t}\delta_{1}(h_ {2}^{t})^{2}\] (158) \[\Rightarrow \eta^{2}\delta_{2}^{2}(h_{1}^{t})^{2}-\eta^{2}\delta_{1}^{2}(h_ {2}^{t})^{2} =2\eta\delta_{2}h_{2}^{t}(h_{1}^{t})^{2}-2\eta h_{1}^{t}\delta_{1}(h_ {2}^{t})^{2}\] (159) \[\Rightarrow \left[\eta\delta_{2}(h_{1}^{t})-\eta\delta_{1}(h_{2}^{t}) \right]\left[\eta\delta_{2}(h_{1}^{t})+\eta\delta_{1}(h_{2}^{t})\right]=2h_{2 }^{t}h_{1}^{t}\left[\eta\delta_{2}(h_{1}^{t})-\eta\delta_{1}(h_{2}^{t})\right]\] (160)

Thus either \(\left[\eta\delta_{2}(h_{1}^{t})-\eta\delta_{1}(h_{2}^{t})\right]=0\) or \(\left[\eta\delta_{2}(h_{1}^{t})+\eta\delta_{1}(h_{2}^{t})\right]=2h_{2}^{t}h_ {1}^{t}\). Since \(\eta\) is such that \(h_{1}-\eta\delta_{1}<0\), \(\left[\eta\delta_{2}(h_{1}^{t})+\eta\delta_{1}(h_{2}^{t})\right]\neq 2h_{2}^{t}h_{1}^{t}\) implying that \(\left[\eta\delta_{2}(h_{1}^{t})-\eta\delta_{1}(h_{2}^{t})\right]=0\) giving us the required condition. 

**Lemma 18**.: _Under the initialization conditions assumed in Theorem 16, for all \(t\), we have: (i) \(\mu_{t}\geqslant\mu_{c}\) and \(\left|\sigma_{t}\right|\leqslant\sigma_{c}\) for constant \(\mu_{c}=\left|c_{1}\cdot\gamma\right|/2\) and \(\sigma_{c}=\left|c_{4}\sigma_{\mathrm{sp}}\right|\); (ii) \(\delta_{2}<0\); (iii) \(\left|\delta_{2}\right|\geqslant\delta_{1}\), where \(\delta_{1}=A_{1}\cdot(\sigma_{t}c_{3}\sigma_{\mathrm{sp}}-c_{1}\gamma)+A_{2} \cdot(\sigma_{t}c_{3}\sigma_{\mathrm{sp}}+c_{1}\gamma)-A_{3}c_{3}\sigma_{ \mathrm{sp}}\) and \(\delta_{2}=A_{1}\cdot(\sigma_{t}c_{4}\sigma_{\mathrm{sp}}-c_{2}\gamma)+A_{2} \cdot(\sigma_{t}c_{4}\sigma_{\mathrm{sp}}+c_{2}\gamma)-A_{3}c_{4}\sigma_{ \mathrm{sp}}\) for \(A_{1},A_{2}\) and \(A_{3}\) defined in (127), (128), and (129)._

Proof.: Recall, \(\mu_{t}=c_{1}\gamma h_{1}^{t}+c_{2}\gamma h_{2}^{t}\) and \(\sigma_{t}=c_{3}\sigma_{\mathrm{sp}}h_{1}^{t}+c_{4}\sigma_{\mathrm{sp}}h_{2}^{t}\). First, we argue that \(\mu_{t}\) increases from the initialization value. Notice that \(\mu_{0}=c_{1}\gamma h_{1}^{t}+c_{2}\gamma h_{2}^{0}\). Due to Corollary 15, we have \(h_{2}^{0}\geqslant 0\). And since \(\left|c_{2}\right|>\left|c_{1}\right|\), we get \(\mu_{0}\geqslant\left|c_{1}\gamma\right|\) as both \(c_{1}\) and \(h_{1}^{0}\) are of same sign. Moreover, as training progresses with \(h_{1}^{t}\) remaining negative and \(h_{2}^{t}\) remaining positive, we have \(\mu_{t}\) stays greater than \(\mu_{0}\).

Recall the definition of \(A_{1},A_{2}\), and \(A_{3}\) in (127), (128), and (129). Moreover, recall the definition of \(\alpha_{1}(\mu_{t},\sigma_{t})\) and \(\alpha_{2}(\mu_{t},\sigma_{t})\):

\[\alpha_{1}(\mu_{t},\sigma_{t})=\sqrt{\frac{2}{\pi}}\mathrm{exp}\left(-\frac{\mu_ {t}^{2}}{2\sigma_{t}^{2}}\right)\left[\mathrm{r}\left(\sigma_{t}+\frac{\mu_{t} }{\sigma_{t}}\right)-\mathrm{r}\left(\sigma_{t}-\frac{\mu_{t}}{\sigma_{t}}\right) \right]\,.\] (161)

and

\[\alpha_{2}(\mu_{t},\sigma_{t})=\sqrt{\frac{2}{\pi}}\mathrm{exp}\left(-\frac{\mu_ {t}^{2}}{2\sigma_{t}^{2}}\right)\left[\mathrm{r}\left(\sigma_{t}+\frac{\mu_{t}}{ \sigma_{t}}\right)+\mathrm{r}\left(\sigma_{t}-\frac{\mu_{t}}{\sigma_{t}}\right)- \frac{2}{\sigma_{t}}\right]\,.\] (162)

Thus, we have \(\alpha_{1}(\mu_{t},\sigma_{t})\cdot A_{3}=A_{1}\cdot\sigma_{t}\) and \(\alpha_{2}(\mu_{t},\sigma_{t})\cdot A_{3}=\sigma_{t}\cdot\left(A_{2}\cdot-\frac{2 }{\sigma_{t}}A_{3}\right)\). Replacing the definition of \(A_{1}\), \(A_{2}\), and \(A_{3}\) in \(\delta_{1}\) and \(\delta_{2}\), we get:

\[\delta_{1}=\sigma_{t}c_{3}\sigma_{\mathrm{sp}}\cdot\alpha_{2}(\mu_{t},\sigma_{t })+c_{1}\gamma\alpha_{1}(\mu_{t},\sigma_{t})\quad\text{and}\quad\delta_{2}= \sigma_{t}c_{4}\sigma_{\mathrm{sp}}\cdot\alpha_{2}(\mu_{t},\sigma_{t})+c_{2} \gamma\alpha_{1}(\mu_{t},\sigma_{t})\] (163)

We now upper bound and lower bound \(\alpha_{1}\) and \(\alpha_{2}\) by using the properties of \(\mathrm{r}\left(\cdot\right)\). We use Taylor's expansion on \(\mathrm{r}\left(\cdot\right)\) and we get:

\[\mathrm{r}\left(\sigma_{t}\right)+\mathrm{r}^{\prime}\left(\sigma_{t}\right) \cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\leqslant\mathrm{r}\left( \sigma_{t}+\frac{\mu_{t}}{\sigma_{t}}\right)\leqslant\mathrm{r}\left(\sigma_{t }\right)+\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{ \sigma_{t}}\right)+\mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\left( \frac{\mu_{t}}{\sigma_{t}}\right)^{2}\] (164)

and similarly, we get:

\[\mathrm{r}\left(\sigma_{t}\right)-\mathrm{r}^{\prime}\left(\sigma_{t}\right) \cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)+\mathrm{rwhere \(R^{\prime\prime}=\mathrm{r}^{\prime\prime}\left(\sigma_{0}\right)\). This is because \(\mathrm{r}^{\prime\prime}\left(\cdot\right)\) takes positive values and is a decreasing function in \(\sigma_{t}\) (refer to Lemma 21). We now lower bound \(\alpha_{1}(\mu_{t},\sigma_{t})\) and upper bound \(\alpha_{2}(\mu_{t},\sigma_{t})\):

\[\frac{\alpha_{1}(\mu_{t},\sigma_{t})}{\sqrt{\frac{2}{\pi}}\mathrm{ exp}\left(-\frac{\mu_{t}^{2}}{2\sigma_{t}^{2}}\right)}\leq 2\mathrm{r}^{\prime}\left( \sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\] (166)

\[\frac{\alpha_{2}(\mu_{t},\sigma_{t})}{\sqrt{\frac{2}{\pi}}\mathrm{ exp}\left(-\frac{\mu_{t}^{2}}{2\sigma_{t}^{2}}\right)}\geq 2\mathrm{r}\left( \sigma_{t}\right)+\mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot \left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}-\frac{2}{\sigma_{t}}\] (167)

Part-1.We first prove that \(\delta_{2}\leq 0\). Substituting the lower bound and upper bound in (163) gives us the following as stricter a sufficient condition (i.e., (168) implies \(\delta_{2}\leq 0\)):

\[\left[2\mathrm{r}\left(\sigma_{t}\right)+\mathrm{r}^{\prime\prime }\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}- \frac{2}{\sigma_{t}}\right]\cdot\frac{\sigma_{\mathrm{sp}}\cdot\left(-c_{4} \right)}{\gamma\cdot c_{2}}\geq 2\mathrm{r}^{\prime}\left(\sigma_{t}\right) \cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\] (168) \[\Longleftrightarrow\left[2\mathrm{r}\left(\sigma_{t}\right)+ \mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{ \sigma_{t}}\right)^{2}-\frac{2}{\sigma_{t}}\right]\geq 2\mathrm{r}^{\prime} \left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\cdot \frac{\gamma\cdot c_{2}}{\sigma_{\mathrm{sp}}\cdot\left(-c_{4}\right)}\] (169) \[\Longleftrightarrow 2\mathrm{r}\left(\sigma_{t}\right)+\mathrm{r}^{ \prime\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}} \right)^{2}-\frac{2}{\sigma_{t}}-2\mathrm{r}^{\prime}\left(\sigma_{t}\right) \cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\cdot\frac{\gamma\cdot c_{2}}{ \sigma_{\mathrm{sp}}\cdot\left(-c_{4}\right)}\geq 0\] (170) \[\Longleftrightarrow 2\mathrm{r}\left(\sigma_{t}\right)\cdot\sigma_{t}+ \mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\frac{\mu_{t}^{2}}{\sigma _{t}}-2\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\mu_{t}\cdot\frac{ \gamma\cdot c_{2}}{\sigma_{\mathrm{sp}}\cdot\left(-c_{4}\right)}\geq 0\] (171) \[\Longleftrightarrow \mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\frac{\mu_{t}^{2} }{\sigma_{t}}+2\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\left[1-\mu_{t} \cdot\frac{\gamma\cdot c_{2}}{\sigma_{\mathrm{sp}}\cdot\left(-c_{4}\right)} \right]\geq 0\] (173)

Thus, if we have \(\mu_{t}\geq\frac{\sigma_{\mathrm{sp}}\cdot\left(-c_{4}\right)}{\gamma\cdot c _{2}}\), then (168) holds true.

Part-2.Next, we prove that \(|\delta_{2}|\geq\delta_{1}\). Substituting the lower bound and upper bound in (163) gives us the following as stricter a sufficient condition (i.e., (174) implies \(|\delta_{2}|\geq\delta_{1}\)):

\[\left[2\mathrm{r}\left(\sigma_{t}\right)+\mathrm{r}^{\prime\prime }\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)^{2}- \frac{2}{\sigma_{t}}\right]\cdot\frac{\sigma_{\mathrm{sp}}\cdot\left(-c_{4}-c _{3}\right)}{\gamma\cdot\left(c_{2}+c_{1}\right)}\geq 2\mathrm{r}^{\prime}\left( \sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\] (174) \[\Longleftrightarrow\left[2\mathrm{r}\left(\sigma_{t}\right)+ \mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{ \sigma_{t}}\right)^{2}-\frac{2}{\sigma_{t}}\right]\geq 2\mathrm{r}^{\prime}\left(\sigma_{t} \right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\cdot\frac{\gamma\cdot \left(c_{2}+c_{1}\right)}{\sigma_{\mathrm{sp}}\cdot\left(-c_{4}-c_{3}\right)}\] (175) \[\Longleftrightarrow 2\mathrm{r}\left(\sigma_{t}\right)+\mathrm{r}^{ \prime\prime}\left(\sigma_{t}\right)\cdot\left(\frac{\mu_{t}}{\sigma_{t}} \right)^{2}-\frac{2}{\sigma_{t}}-2\mathrm{r}^{\prime}\left(\sigma_{t}\right) \cdot\left(\frac{\mu_{t}}{\sigma_{t}}\right)\cdot\frac{\gamma\cdot\left(c_{2}+c _{1}\right)}{\sigma_{\mathrm{sp}}\cdot\left(-c_{4}-c_{3}\right)}\geq 0\] (176) \[\Longleftrightarrow 2\mathrm{r}\left(\sigma_{t}\right)\cdot\sigma_{t}+ \mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\frac{\mu_{t}^{2}}{\sigma _{t}}-2-2\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\mu_{t}\cdot\frac{ \gamma\cdot\left(c_{2}+c_{1}\right)}{\sigma_{\mathrm{sp}}\cdot\left(-c_{4}-c_{3 }\right)}\geq 0\] (177) \[\Longleftrightarrow 2\mathrm{r}^{\prime}\left(\sigma_{t}\right)+ \mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\frac{\mu_{t}^{2}}{\sigma _{t}}-2\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\mu_{t}\cdot\frac{ \gamma\cdot\left(c_{2}+c_{1}\right)}{\sigma_{\mathrm{sp}}\cdot\left(-c_{4}-c_ {3}\right)}\geq 0\] (178) \[\Longleftrightarrow \mathrm{r}^{\prime\prime}\left(\sigma_{t}\right)\cdot\frac{\mu_{t}^{2} }{\sigma_{t}}+2\mathrm{r}^{\prime}\left(\sigma_{t}\right)\cdot\left[1-\mu_{t} \cdot\frac{\gamma\cdot\left(c_{2}+c_{1}\right)}{\sigma_{\mathrm{sp}}\cdot \left(-c_{4}-c_{3}\right)}\right]\geq 0\] (179)

Thus, if we have \(\mu_{t}\geq\frac{\sigma_{\mathrm{sp}}\cdot\left(-c_{4}-c_{3}\right)}{\gamma \cdot\left(c_{2}+c_{1}\right)}\), then (174) holds true which in-turn implies \(|\delta_{2}|\geq\delta_{1}\). Plugging in \(\mu_{t}\geq\mu_{0}\), we get the required condition.

### Analysis for SSL

For SSL analysis, we argue that the projection learned by contrastive pretraining can significantly improve the generalization of the linear head learned on top, leaving little to no room for improvement for self-training. Our analysis leverages the margin-based bound for linear models from Kakade et al. [45]. Before introducing the result, we present some additional notation. Let \(\operatorname{Err}_{D}(w)\) denote 0-1 error of a classifier on a distribution \(D\). Define 0-1 error with margin \(\gamma\) as \(\widehat{\operatorname{Err}}^{\gamma}(w)=\sum_{i=1}^{n}\frac{[y_{i,w}{}^{ \top}x_{i}\leq\gamma]}{n}\).

**Theorem 19** (Corollary 6 in Kakade et al. [45]).: _For all classifiers \(w\) and margin \(\gamma\), we have with probability at least \(1-\delta\):_

\[\operatorname{Err}_{T}(w)\leq\widehat{\operatorname{Err}}^{\gamma}(w)+4\frac {B}{\gamma}\sqrt{\frac{1}{n}}+\sqrt{\frac{\log(1/\delta)}{n}}+\sqrt{\frac{ \log(\log_{2}(4B/\gamma))}{n}}\,,\] (180)

_where \(B\) is an upper bound on the \(\ell_{2}\) norm of the input points \(x\)._

When \(\widehat{\operatorname{Err}}^{\gamma}(w)\) is close to zero, the denominating term in RHS of (180) is \(4\frac{B}{\gamma}\sqrt{\frac{1}{n}}\). With Proposition 3, CL solution \(\phi\) obtained on the target domain alone (for SSL setup) is \(w_{\mathrm{in}}\) when \(k=1\). For larger \(k\)'s the CL solution is dominated by the \(\phi_{1}=w_{\mathrm{in}}\). Thus, SSL mainly reduces the B on the projected data by reducing the dependency from order \(\sqrt{d}\) to \(1\) where \(1\) is the dimensionality of the output of \(\phi\) without altering the margin. Thus, we get a tighter upper bound for linear probing performed on top CL features when compared with linear probing done on inputs directly.

Intuitively, since the target data has only one predictive feature (along \(w_{\mathrm{in}}\)), CL directly recovers this predictive feature as it is the predominant direction that minimizes invariance loss.

## Appendix F Limitations of Prior Work

### Contrastive learning analysis

Prior works that analyze contrastive learning show that minimizers of the CL objective recover clusters in the augmentation graph, which weights pairs of augmentations with their probability of being sampled as a positive pair [39, 11, 73, 44]. When there is no distribution shift in the downstream task, assumptions made on the graph in the form of consistency of augmentations with downstream labels, is sufficient to ensure that a linear probed head has good ID generalization. Under distribution shift, these assumptions are not sufficient and stronger ones are needed. _E.g._, some works assume that same-domain/class examples are weighted higher that cross-class cross-domain pairs [40, 76].

Using notation defined in [76], the assumption on the augmentation graph requires cross-class and same-domain weights (\(\beta\)) to be higher than cross-class and cross-domain weights (\(\gamma\)). It is unclear if examples from different classes in the same domain will be "connected" if strong spurious features exist in the source domain and augmentations fail to mask them completely (_e.g._, image background may not be completely masked by augmentations but it maybe perfectly predictive of the label on source domain). In such cases, the linear predictor learnt over CL would fail to generalize OOD. In our toy setup as well, the connectivity assumption fails since on source \(x_{\mathrm{sp}}\) is perfectly predictive of the label and the augmentations are imperfect, _i.e._, augmentations do not mask \(x_{\mathrm{sp}}\) and examples of different classes do not overlap in source (_i.e._, \(\beta=0\)). On the other hand, since \(x_{\mathrm{sp}}\) is now random on target, augmentations of different classes may overlap, _i.e._, \(\gamma>0\), thus breaking the connectivity assumption. This is also highlighted in our empirical findings of CL furnishing representations that do not fully enable linear transferability from source to target (see Sec. 5). These empirical findings also call into question existing assumptions on data augmentations, highlighting that perfect linear transferability may not typically hold in practice. It is in this setting that we believe self-training can improve over contrastive learning by unlearning source-only features and improving linear transferability.

### Self-training analysis

Some prior works on self-training view it as consistency regularization that constrain pseudolabels of original samples to be consistent with all their augmentations [79, 12, 87]. This frameworkabstracts the role played by the optimization algorithm and instead evaluates the global minimizer of a population objective that enforces consistency of pseudolabels. In addition, certain expansion assumptions on class-conditional distributions are needed to ensure that pseudolabels have good accuracy on source and target domains. This framework does not account for challenges involved in propagating labels iteratively. For _e.g._, when augmentation distribution has long tails, the consistency of pseudolabels depends on the sampling frequency of "favorable" augmentations. As an illustration, consider our augmentation distribution in the toy setup in Sec. 4. If it were not uniform over dimensions, but instead something that was highly skewed, then a large number of augmentations need to be sampled for every data point to propagate pseudolabels successfully from source labeled samples to target unlabeled samples during self-training. This might hurt the performance of ST when we are optimizing for only finitely many iterations and over finitely many datapoints. This is why in our analysis we instead adopt the iterative analysis of self-training [17].

## Appendix G Additional Lemmas

In this section we define some additional lemmas that we use in our theoretical analysis in E.

**Lemma 20** (Upper bound and lower bounds on \(\mathrm{erfc}\); Kschischang [49]).: _Define \(\mathrm{erfc}(x)=\frac{2}{\sqrt{\pi}}\cdot\int_{x}^{\infty}\exp(-z^{2})\cdot dz\). Then we have:_

\[\frac{2}{\sqrt{\pi}}\cdot\frac{\exp(-x^{2})}{x+\sqrt{x^{2}+2}}<\mathrm{erfc}( x)\leq\frac{2}{\sqrt{\pi}}\cdot\frac{\exp(-x^{2})}{x+\sqrt{x^{2}+4/\pi}}\]

**Lemma 21** (Properties of Mill's ratio [5]).: _Define the Mill's ratio as \(\mathrm{r}\left(x\right)=\exp\left(x^{2}/2\right)\cdot\mathrm{erfc}\left(x/ \sqrt{2}\right)\cdot\sqrt{\pi/2}\). Then following assertions are true: (i) \(\mathrm{r}\left(x\right)\) is a strictly decreasing log-convex function; (ii) \(\mathrm{r}^{\prime}(x)=x\cdot\mathrm{r}\left(x\right)-1\) is an increasing function with \(\mathrm{r}^{\prime}(x)<0\) for all x; (iii) \(\mathrm{r}^{\prime\prime}(x)=\mathrm{r}\left(x\right)+x^{2}\cdot\mathrm{r} \left(x\right)-x\) is a decreasing function with \(\mathrm{r}^{\prime\prime}(x)>0\) for all x; (iv) \(x^{2}\cdot\mathrm{r}^{\prime}(x)\) is a decreasing function of \(x\)._

**Lemma 22** (invariance loss as product with operator \(L\)).: _The invariance loss for some \(\phi\in\mathbb{R}^{d}\) is given as: \(2\cdot\int_{\mathcal{A}}\phi(a)\cdot L(\phi)(a)\ \mathrm{dP}_{\mathsf{A}}\) where the operator \(L\) is defined as:_

\[L(\phi)(a)=\phi(a)-\int_{\mathcal{A}}\frac{A_{+}(a,a^{\prime})}{ \rho_{\mathsf{A}}(a)}\cdot\phi(a^{\prime})\ \mathrm{d}a^{\prime}\]

Proof.: The invariance loss for \(\phi\) is given by:

\[\mathbb{E}_{x\sim\mathrm{P}_{0}}\mathbb{E}_{a_{1},a_{2}\sim\mathrm{ P}_{\mathsf{A}}\left(\cdot\left|x\right)}(a_{1}^{\top}\phi-a_{2}^{\top}\phi)^{2} =2\mathbb{E}_{x\sim\mathrm{P}_{0}}\mathbb{E}_{a\sim\mathrm{P}_{ \mathsf{A}}\left(\cdot\left|x\right)}\left[\phi(a)^{2}\right]\] \[\qquad\qquad\qquad\qquad-2\mathbb{E}_{a_{1},a_{2}\sim A_{+}(\cdot,\cdot)}\left[\phi(a_{1})\phi(a_{2})\right]\] (181) \[=2\cdot\int_{\mathcal{A}}\phi(a)\cdot L(\phi)(a)\ \mathrm{dP}_{\mathsf{A}}\] (183)

**Lemma 23**.: _If \(\mathcal{W}\) is the space spanned by \(w_{\mathrm{in}}\) and \(w_{\mathrm{sp}}\), and \(\mathcal{W}_{\perp}\) is the null space for \(\mathcal{W}\), then for any \(u\in\mathcal{W}\) and any \(v\in\mathcal{W}_{\perp}\), the covariance along these directions \(\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}[a^{\top}uv^{\top}a]=0\)._

Proof.: We can write the covariance over augmentations after we break down the augmentation \(a\) into two projections: \(a=\Pi_{\mathcal{W}}(a)+\Pi_{\mathcal{W}_{\perp}}(a)\)

\[\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}[a^{\top}uv^{\top}a] =\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}\left[\left(u^{\top}( \Pi_{\mathcal{W}}(a)+\Pi_{\mathcal{W}_{\perp}}(a))\right)\left(v^{\top}(\Pi_{ \mathcal{W}}(a)+\Pi_{\mathcal{W}_{\perp}}(a))\right)\right]\] (184) \[=\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}\left[\left(u^{\top}\Pi _{\mathcal{W}}(a)\right)\left(v^{\top}\Pi_{\mathcal{W}_{\perp}}(a)\right)\right]\] (185) \[=u^{\top}\left(\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}\left[\Pi _{\mathcal{W}}(a)\Pi_{\mathcal{W}_{\perp}}(a)^{\top}\right]\right)v=0\] (186)

where the last inequality follows from the fact that \(\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}}\left[\Pi_{\mathcal{W}}(a)\Pi_{ \mathcal{W}_{\perp}}(a)^{\top}\right]=\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}} \left[\Pi_{\mathcal{W}}(a)\right]\mathbb{E}_{a\sim\mathrm{P}_{\mathsf{A}}} \left[\Pi_{\mathcal{W}_{\perp}}(a)\right]^{\top}\), since the noise in the null space of \(\mathcal{W}\) is drawn independent of the component along \(\mathcal{W}\), and furthermore the individual expectations evaluate to zero.

**Lemma 24** (closed-form expressions for eigenvalues and eigenvectors of \(\Sigma_{A},\widetilde{\Sigma}\)).: _For a \(2\times 2\) real symmetric matrix \(\begin{bmatrix}a,&b\\ c,&d\end{bmatrix}\) the eigenvalues \(\lambda_{1},\lambda_{2}\) are given by the following expressions:_

\[\lambda_{1}=\frac{(a+b+\delta)}{2},\ \ \lambda_{2}=\frac{(a+b-\delta)}{2},\]

_where \(\delta=\sqrt{4c^{2}+(a-b)^{2}}\). Further, the eigenvectors are given by \(U=\begin{bmatrix}\cos(\theta),&\sin(\theta)\\ \sin(\theta),-cos(\theta)\end{bmatrix}\), where:_

\[\tan(\theta)=\frac{b-a+\delta}{2c}.\]

_For full proof of these statements see [23]. Here, we will use these statements to arrive at closed form expressions for the eigenvalues and eigenvectors of \(\Sigma_{A}\), \(\widetilde{\Sigma}\)._

Proof.: We can now substitute the above formulae with \(a,b,c,d\) taken from the expressions of \(\Sigma_{A}\) and \(\widetilde{\Sigma}\), to get the following values: \(\lambda_{1},\lambda_{2}\) are the eigenvalues of \(\Sigma_{A}\), with \(\alpha\) determining the corresponding eigenvectors \([\cos(\alpha),\sin(\alpha)],[\sin(\alpha),-\cos(\alpha)]\); and \(\widetilde{\lambda}_{1},\widetilde{\lambda}_{2}\) are the eigenvalues of \(\widetilde{\Sigma}\), with \(\beta\) determining the corresponding eigenvectors: \([\cos(\beta),\sin(\beta)],[\sin(\beta),-\cos(\beta)]\).

\[\lambda_{1} =\frac{1}{8}\Bigg{(}\gamma^{2}\left(1+\frac{1}{3d_{\rm in}} \right)+\frac{\sigma_{\rm in}^{2}}{3}\left(1-\frac{1}{d_{\rm in}}\right)+\frac {d_{\rm sp}}{2}+\frac{2\sigma_{\rm sp}^{2}}{3}+\frac{1}{6}\] \[+\sqrt{\gamma^{2}d_{\rm sp}+\left(\left(\gamma^{2}\left(1+\frac{ 1}{3d_{\rm in}}\right)+\frac{\sigma_{\rm in}^{2}}{3}\left(1-\frac{1}{d_{\rm in} }\right)\right)-\left(\frac{d_{\rm sp}}{2}+\frac{2\sigma_{\rm sp}^{2}}{3}+ \frac{1}{6}\right)\right)^{2}}\Bigg{)}\] (187)

\[\lambda_{2} =\frac{1}{8}\Bigg{(}\gamma^{2}\left(1+\frac{1}{3d_{\rm in}} \right)+\frac{\sigma_{\rm in}^{2}}{3}\left(1-\frac{1}{d_{\rm in}}\right)+\frac {d_{\rm sp}}{2}+\frac{2\sigma_{\rm sp}^{2}}{3}+\frac{1}{6}\] \[-\sqrt{\gamma^{2}d_{\rm sp}+\left(\left(\gamma^{2}\left(1+\frac{ 1}{3d_{\rm in}}\right)+\frac{\sigma_{\rm in}^{2}}{3}\left(1-\frac{1}{d_{\rm in }}\right)\right)-\left(\frac{d_{\rm sp}}{2}+\frac{2\sigma_{\rm sp}^{2}}{3}+ \frac{1}{6}\right)\right)^{2}}\Bigg{)}\] (188) \[\widetilde{\lambda}_{1} =\frac{1}{8}\Bigg{(}\gamma^{2}+\frac{d_{\rm sp}}{2}+\frac{\sigma_ {\rm sp}^{2}}{2}+\sqrt{\gamma^{2}d_{\rm sp}+\left(\gamma^{2}-\left(\frac{d_{ \rm sp}}{2}+\frac{\sigma_{\rm sp}^{2}}{2}\right)\right)^{2}}\Bigg{)}\] (189) \[\widetilde{\lambda}_{2} =\frac{1}{8}\Bigg{(}\gamma^{2}+\frac{d_{\rm sp}}{2}+\frac{\sigma_ {\rm sp}^{2}}{2}-\sqrt{\gamma^{2}d_{\rm sp}+\left(\gamma^{2}-\left(\frac{d_{ \rm sp}}{2}+\frac{\sigma_{\rm sp}^{2}}{2}\right)\right)^{2}}\Bigg{)}\] (190) \[\tan(\alpha) =\frac{1}{\gamma\sqrt{d_{\rm sp}}}\Bigg{(}\frac{d_{\rm sp}}{2}+ \frac{2\sigma_{\rm sp}^{2}}{3}+\frac{1}{6}-\left(\gamma^{2}\left(1+\frac{1}{3d _{\rm in}}\right)+\frac{\sigma_{\rm in}^{2}}{3}\left(1-\frac{1}{d_{\rm in}} \right)\right)\] \[+\sqrt{\gamma^{2}d_{\rm sp}+\left(\left(\gamma^{2}\left(1+\frac{ 1}{3d_{\rm in}}\right)+\frac{\sigma_{\rm in}^{2}}{3}\left(1-\frac{1}{d_{\rm in }}\right)\right)-\left(\frac{d_{\rm sp}}{2}+\frac{2\sigma_{\rm sp}^{2}}{3}+ \frac{1}{6}\right)\right)^{2}}\Bigg{)}\] (191) \[\tan(\beta) =\frac{1}{\gamma\sqrt{d_{\rm sp}}}\Bigg{(}\frac{d_{\rm sp}}{2}+ \frac{\sigma_{\rm sp}^{2}}{2}-\gamma^{2}+\sqrt{\gamma^{2}d_{\rm sp}+\left( \gamma^{2}-\left(\frac{d_{\rm sp}}{2}+\frac{\sigma_{\rm sp}^{2}}{2}\right) \right)^{2}}\Bigg{)}\] (192)

Consider the subclass of problem parameters, \(d_{\rm sp}=z,\gamma=\nicefrac{{K_{1}}}{{\sqrt{z}}}\) and \(\sigma_{\rm sp}=K_{2}\sqrt{z}\) for fixed constants \(K_{1},K_{2}>0\) and some variable \(z>0\), which we can vary to give us different problem instances for our toy model in (6).

\[\lambda_{1}=\frac{1}{8}\Bigg{(}\frac{K_{1}^{2}}{z}\left(1+\frac{1}{3d_{ \mathrm{in}}}\right)+\frac{\sigma_{\mathrm{in}}^{2}}{3}\left(1-\frac{1}{d_{ \mathrm{in}}}\right)+\frac{z}{2}+\frac{2K_{2}^{2}z}{3}+\frac{1}{6}\] \[\qquad\qquad+\sqrt{K_{1}^{2}+\left(\left(\frac{K_{1}^{2}}{z} \left(1+\frac{1}{3d_{\mathrm{in}}}\right)+\frac{\sigma_{\mathrm{in}}^{2}}{3} \left(1-\frac{1}{d_{\mathrm{in}}}\right)\right)-\left(\frac{z}{2}+\frac{2K_{2}^ {2}z}{3}+\frac{1}{6}\right)\right)^{2}}\Bigg{)}\] (193) \[\lambda_{2}=\frac{1}{8}\Bigg{(}\frac{K_{1}^{2}}{z}\left(1+\frac{1 }{3d_{\mathrm{in}}}\right)+\frac{\sigma_{\mathrm{in}}^{2}}{3}\left(1-\frac{1}{ d_{\mathrm{in}}}\right)+\frac{z}{2}+\frac{2K_{2}^{2}z}{3}+\frac{1}{6}\] \[\qquad\qquad-\sqrt{K_{1}^{2}+\left(\left(\frac{K_{1}^{2}}{z} \left(1+\frac{1}{3d_{\mathrm{in}}}\right)+\frac{\sigma_{\mathrm{in}}^{2}}{3} \left(1-\frac{1}{d_{\mathrm{in}}}\right)\right)-\left(\frac{z}{2}+\frac{2K_{2}^ {2}z}{3}+\frac{1}{6}\right)\right)^{2}}\Bigg{)}\] (194)

\[\widetilde{\lambda}_{1}=\frac{1}{8}\Bigg{(}\frac{K_{1}^{2}}{z}+ \frac{z}{2}+\frac{K_{2}^{2}z}{2}+\sqrt{K_{1}^{2}+\left(\frac{K_{1}^{2}}{z}- \left(\frac{z}{2}+\frac{K_{2}^{2}z}{2}\right)\right)^{2}}\Bigg{)}\] (195) \[\widetilde{\lambda}_{2}=\frac{1}{8}\left(\frac{K_{1}^{2}}{z}+ \frac{z}{2}+\frac{K_{2}^{2}z}{2}-\sqrt{K_{1}^{2}+\left(\frac{K_{1}^{2}}{z}- \left(\frac{z}{2}+\frac{K_{2}^{2}z}{2}\right)\right)^{2}}\right)\] (196)

\[\tan(\alpha)=\frac{1}{K_{1}}\Bigg{(}\frac{z}{2}+\frac{2K_{2}^{2}z }{3}+\frac{1}{6}-\left(\frac{K_{1}^{2}}{z}\left(1+\frac{1}{3d_{\mathrm{in}}} \right)+\frac{\sigma_{\mathrm{in}}^{2}}{3}\left(1-\frac{1}{d_{\mathrm{in}}} \right)\right)\] \[\qquad\qquad+\sqrt{K_{1}^{2}+\left(\frac{K_{1}^{2}}{z}\left(1+ \frac{1}{3d_{\mathrm{in}}}\right)+\frac{\sigma_{\mathrm{in}}^{2}}{3}\left(1- \frac{1}{d_{\mathrm{in}}}\right)-\left(\frac{z}{2}+\frac{2K_{2}^{2}z}{3}+ \frac{1}{6}\right)\right)^{2}}\Bigg{)}\] (197) \[\tan(\beta)=\frac{1}{K_{1}}\left(\frac{z}{2}+\frac{K_{2}^{2}z}{2} -\frac{K_{1}^{2}}{z}+\sqrt{K_{1}^{2}+\left(\frac{K_{1}^{2}}{z}-\left(\frac{z} {2}+\frac{K_{2}^{2}z}{2}\right)\right)^{2}}\right)\] (198)

From Stewart [80], we can use the closed form expression for the singular vectors of a \(2\times 2\) full rank asymmetric matrix \(\begin{bmatrix}a,&b\\ c,&d\end{bmatrix}\). The singular vectors are given by

\[\begin{bmatrix}\cos\theta,&\sin\theta\\ \sin\theta,&-\cos\theta\end{bmatrix},\]

where, \(\tan(2\theta)\) is given by:

\[\tan(2\theta)=\frac{2ac+2bd}{a^{2}+b^{2}-c^{2}-d^{2}}.\]

Now, substituting the values in the expression from (100), we get singular vectors of the above form where \(\theta\in[0,\nicefrac{{\pi}}{{2}}]\) satisfies:

\[\theta=\frac{1}{2}\tan^{-1}\Bigg{(}\frac{2\tan(\beta-\alpha);( \widetilde{\lambda}_{1}-\widetilde{\lambda}_{2})\cdot\sqrt{\lambda_{1}\lambda_ {2}}}{(\widetilde{\lambda}_{2}\widetilde{\lambda}_{1}-\lambda_{1}\widetilde{ \lambda}_{2})-(\widetilde{\lambda}_{1}\widetilde{\lambda}_{1}-\widetilde{ \lambda}_{2}\widetilde{\lambda}_{2})\cdot\tan^{2}(\alpha-\beta)}\Bigg{)}\] (199)

**Lemma 25** (asymptotic behavior of \(\tau\tan\theta\)).: _For \(\gamma=\nicefrac{{K_{1}}}{{\sqrt{z}}}\), \(\sigma_{\mathrm{sp}}=K_{2}\sqrt{z}\),_

\[\lim_{z\to\infty}\tau\tan\theta=\frac{K_{1}K_{2}^{2}}{(1+K_{2}^{2})2\sigma_{ \mathrm{in}}^{2}(1-\nicefrac{{1}}{{d_{\mathrm{in}}}})}\]

Proof.: In order to determine the asymptotic nature of \(\tan(\theta)\) as \(z\to\infty\), we take the limit of a slightly different term first, since we have the closed form expression of \(\tan(2\theta)\).

[MISSING_PAGE_EMPTY:52]

where \(p=\nicefrac{{\nicefrac{{\nicefrac{{\nicefrac{{\nicefrac{{\gamma}}{2}}}}}{{2}}}}}{{ \nicefrac{{\nicefrac{{\gamma}}{2}}}{{\nicefrac{{\gamma}}{2}}}}}+\nicefrac{{{\nicefrac{{ \gamma}}{2}}}}{{\nicefrac{{\gamma}}{2}}}+\nicefrac{{{\nicefrac{{\gamma}}{2}}}}{{ \nicefrac{{\gamma}}{2}}}\). Applying L'Hopital's (relevant expressions are continuous in \(z\)) rule we get: \(\lim_{z\to\infty}\nicefrac{{\nicefrac{{\gamma}}{{\gamma}}}}{{\nicefrac{{\gamma }}{{\gamma}}}}=\frac{2\nicefrac{{\sigma_{\mathrm{in}}^{2}}}{{\nicefrac{{ \gamma}}{{\nicefrac{{\gamma}}{2}}}}}(1-\nicefrac{{\nicefrac{{\gamma}}{2}}}{{ \nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{2}}}}})}{1+\nicefrac{{\nicefrac{{ \gamma}}{2}}}{{\nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{2}}}}}}\). 

**Lemma 29** (0-1 error of a classifier on target).: _Assume a classifier of the form \(w=l_{1}\cdot w_{\mathrm{in}}+l_{2}\cdot w_{\mathrm{sp}}\) where \(l_{1},l_{2}\in\mathbb{R}\) and \(w_{\mathrm{in}}{=}[w^{\star},0,...,0]^{\top}\), and \(w_{\mathrm{sp}}=[0,...,0,\nicefrac{{1}_{\mathrm{dep}}}{{\nicefrac{{\gamma}}{{ \nicefrac{{\gamma}}{2}}}}}]^{\top}\). Then the target accuracy of this classifier is given by \(0.5\cdot\mathrm{erfc}\left(-\frac{l_{1}\cdot\gamma}{\sqrt{2}\cdot l_{2}\cdot \sigma_{\mathrm{sp}}}\right)\)._

Proof.: Assume \((x,y)\sim\mathrm{P}_{\mathsf{T}}\). Accuracy of \(w\) is given by \(\mathbb{E}_{\mathrm{P}_{\mathsf{T}}}\left[\left(\mathrm{sign}\left(w^{\top}x \right)=y\right)\right]\).

\[\mathbb{E}_{\mathrm{P}_{\mathsf{T}}}\left[\mathrm{sign}\left(w^{ \top}x\right)=y\right] =\mathbb{E}_{\mathrm{P}_{\mathsf{T}}}\left[y\cdot\mathrm{sign}\left(w^ {\top}x\right)=1\right]\] \[=\mathbb{E}_{\mathrm{P}_{\mathsf{T}}}\left[y\cdot\left(w^{\top}x \right)>0\right]\] \[=\mathbb{E}_{\mathrm{P}_{\mathsf{T}}}\left[y\cdot\left(x^{\top}(l_ {1}\cdot w_{\mathrm{in}}+l_{2}\cdot w_{\mathrm{sp}})\right)>0\right]\] \[=\mathbb{E}_{\mathrm{P}_{\mathsf{T}}}\left[y\cdot\left(\gamma\cdot l _{1}\cdot y+l_{2}\cdot\sigma_{\mathrm{sp}}\right)>0\right]\] \[=\mathbb{E}_{z\sim\mathcal{N}(0,1)}\left[\left(\gamma\cdot l_{1}+y \cdot l_{2}\cdot\sigma_{\mathrm{sp}}\cdot z\right)>0\right]\] \[=\mathbb{E}_{z\sim\mathcal{N}(0,1)}\left[y\cdot l_{2}\cdot\sigma_{ \mathrm{sp}}\cdot z>-\gamma\cdot l_{1}\right]\] \[=\mathbb{E}_{z\sim\mathcal{N}(0,1)}\left[l_{2}\cdot\sigma_{ \mathrm{sp}}\cdot z>-\gamma\cdot l_{1}\right]\] \[=\mathbb{E}_{z\sim\mathcal{N}(0,1)}\left[z>-\frac{\gamma\cdot l_{1 }}{l_{2}\cdot\sigma_{\mathrm{sp}}}\right]\]

Using the definition of \(\mathrm{erfc}\) function, we get the aforementioned accuracy expression. 

**Lemma 30**.: _For \(\sigma>0\) and \(\mu\in\mathbb{R}\), we have_

\[g(\mu,\sigma) :=\mathbb{E}_{z\sim\mathcal{N}(0,\sigma)}\left[\exp\left(-\left|\mu +z\right|\right)\right]\] (200) \[=\frac{1}{2}\left(\exp\left(\nicefrac{{\sigma^{2}}}{{\nicefrac{{ \gamma}}{{2}}}}-\mu\right)\cdot\mathrm{erfc}\left(-\nicefrac{{\mu}}{{ \nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{2}}}}}\sigma+\nicefrac{{\sigma}}{{ \nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{2}}}}}\right)+\exp\left(\nicefrac{{ \sigma^{2}}}{{\nicefrac{{\gamma}}{{2}}}}+\mu\right)\cdot\mathrm{erfc}\left( \nicefrac{{\mu}}{{\nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{2}}}}}\sigma+ \nicefrac{{\sigma}}{{\nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{2}}}}}\right)\right)\] (201)

Proof.: The proof uses simple algebra and the definition of \(\mathrm{erfc}\) function.

\[g(\mu,\sigma) :=\mathbb{E}_{z\sim\mathcal{N}(0,\sigma)}\left[\exp\left(-\left|\mu +z\right|\right)\right]\] \[=\frac{1}{\sqrt{2\pi}}\int_{z}^{\infty}\exp\left(-\left|\mu+z \right|\right)\cdot\exp\left(-\frac{z^{2}}{2\sigma^{2}}\right)dz\] \[=\frac{1}{\sqrt{2\pi}}\int_{-\mu}^{\infty}\exp\left(-\mu+z\right) \cdot\exp\left(-\frac{z^{2}}{2\sigma^{2}}\right)dz+\frac{1}{\sqrt{2\pi}}\int_{- \infty}^{-\mu}\exp\left(\mu+z\right)\cdot\exp\left(-\frac{z^{2}}{2\sigma^{2}} \right)dz\] \[=\exp\left(\sigma^{2}/2-\mu\right)\int_{-\frac{\mu}{\sqrt{2\sigma} }+\frac{\sqrt{2\sigma}}{2}}^{\infty}\exp(-z^{2})dz+\exp\left(\sigma^{2}/2+\mu \right)\int_{-\infty}^{\frac{\mu}{\sqrt{2\sigma}}-\frac{\sqrt{2\sigma}}{2}}\exp( -z^{2})dz\] \[=\frac{1}{2}\left(\exp\left(\nicefrac{{\sigma^{2}}}{{\nicefrac{{ \gamma}}{{2}}}}-\mu\right)\cdot\mathrm{erfc}\left(-\nicefrac{{\mu}}{{\nicefrac{{ \gamma}}{{\nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{2}}}}}}\sigma+\nicefrac{{ \sigma}}{{\nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{2}}}}}}} \right)+\exp\left(\nicefrac{{\sigma^{2}}}{{\nicefrac{{\gamma}}{{\nicefrac{{ \gamma}}{{\nicefrac{{\gamma}}{2}}}}}}}+\mu\right)\cdot\mathrm{erfc}\left( \nicefrac{{\mu}}{{\nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{{ \nicefrac{{\gamma}}{2}}}}}}}}\sigma+\nicefrac{{\sigma}}{{\nicefrac{{\gamma}}{{ \nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{{\nicefrac{{\gamma}}{2}}}}}}}}}\right)\right)\]