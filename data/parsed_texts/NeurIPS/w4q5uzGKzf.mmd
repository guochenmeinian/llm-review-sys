# Preference-based Multi-Objective Bayesian

Optimization with Gradients

Joshua Hang Sai Ip\({}^{1}\) Ankush Chakrabarty\({}^{2}\) Hideyuki Masui\({}^{3}\) Ali Mesbah\({}^{1}\) Diego Romeres\({}^{2}\)

\({}^{1}\)University of California, Berkeley \({}^{2}\)Mitsubishi Electric Research Laboratories

\({}^{3}\)Mitsubishi Electric

\({}^{1}\){ipjoshua, mesbah}@berkeley.edu \({}^{2}\){chakrabarty,romeres}@merl.com

\({}^{3}\)masui.hideyuki@bc.mitsubishiielectric.co.jp

###### Abstract

We propose PUB-MOBO for personalized multi-objective Bayesian Optimization. PUB-MOBO combines utility-based MOBO with local multi-gradient descent to refine user-preferred solutions to be near-Pareto-optimal. Unlike traditional methods, PUB-MOBO does not require estimating the entire Pareto-front, making it more efficient. Experimental results on synthetic and real-world benchmarks show that PUB-MOBO consistently outperforms existing methods in terms of proximity to the Pareto-front and utility regret.

## 1 Introduction

Multi-objective Bayesian optimization (MOBO) is a particularly useful multi-objective optimization (MOO) strategy when the objectives are black-box functions constructed from noisy observations. Traditional MOBO methods such as \(q\)-EHVI [1] assume that all Pareto-optimal solutions are equally desirable to the user, which might not be the case in practice. There has been a growing interest in preference-based MOBO (e.g., [2, 3]) that leverages user preferences to guide the optimization process towards regions of interest within the Pareto-front, typically in the form of pairwise comparisons between solutions generated by the optimization algorithm. These comparisons are used to estimate an underlying utility function that describes user preferences. In [4], the authors propose the EUBO and qEIUU acquisition functions respectively, which take advantage of user-preference when querying new points. However, while preference-based MOBO can effectively identify solutions with high utility as informed by user feedback, the resulting solutions may not be Pareto-optimal.

We present Preference-Utility-Balanced MOBO (PUB-MOBO), that systematically determines the user-informed regions of interest within the Pareto-front by synergizing global and local search strategies. PUB-MOBO begins with a global search driven by utility maximization to identify regions in the solution space that align with user preferences. Subsequently, a local search is conducted in the vicinity of these solutions to discover dominating solutions that are closer to Pareto-optimality. Additionally, a new utility function, the Preference-Dominated Utility Function (PDUF), is proposed that encapsulates the concept of dominance within a single function. PDUF allows for consistently identifying dominating solutions, while providing a straightforward means for expressing all possible user preferences. This differs from existing utility functions for preference-based MOBO such as the negative \(\ell_{1}\) distance from an ideal solution irrespective of the solution being on the Pareto-front or an infeasible ideal solution [5], or the weighted sum where not all Pareto-optimal points can be assigned with the highest utility value from any choice of weights [6]. PDUF is then used in conjunction with gradient descent (GD) to seamlessly combine user preferences with the notion of dominance to identify user-preferred solutions that are approximately Pareto-optimal. Empirical demonstrations onseveral synthetic benchmark and real-world problems show that PUB-MOBO not only enhances the utility of the optimization solutions, but also yields near Pareto-optimal solutions.

## 2 Problem Formulation

We _minimize_\(n_{f}\) expensive-to-evaluate objective functions, denoted by \(f_{i}(\bm{x})\) for \(i\in\{1,\cdots,n_{f}\}\). Consequently, the objective function vector is denoted \(\bm{f}(\bm{x})\), where \(\bm{x}\in\mathbb{R}^{n_{x}}\) denote the decision variables. We assume that for a candidate \(\bm{x}\), the function \(\bm{f}(\bm{x})\) can be evaluated, but no first- or higher-order information about any component of \(\bm{f}\) is available. No analytical form of \(\bm{f}\) is known.

For MOO problems without user-preferences, the objective is to attain Pareto-optimality, which is defined as follows [7]. Note that accurately computing the set of Pareto-optimal points, referred to as the Pareto-front \(\mathbb{X}_{\mathsf{pareto}}\), can often be computationally prohibitive, even for small \(n_{f}\). In the presence of a user, estimating the entire Pareto-front may become unnecessary, especially when only specific sub-regions of the feasible set \(\mathbb{X}\) is of interest. Mathematically, such user-preferences are often abstracted in the MOBO literature via _utility functions_. Specifically, the MOO problem is recast as a (scalar) utility maximization problem

\[\max_{\bm{x}\in\mathbb{X}}\quad u\left(\bm{f}(\bm{x})\right),\] (1)

where \(u:\mathbb{R}^{n_{y}}\rightarrow\mathbb{R}\) is the unknown utility function that dictates the behavior of the user. Note that the input to the utility is a noise-corrupted outcome vector \(\bm{y}=\bm{f}(\bm{x})+\varepsilon\), where \(\varepsilon\) is zero-mean noise with variance \(\sigma_{\varepsilon}^{2}\bm{I}_{n_{y}}\) where \(\bm{I}_{n_{y}}\) is the \(n_{y}\times n_{y}\) identity matrix. Let the highest utility Pareto-point be defined as

\[\bm{x}^{*}\in\operatorname*{arg\,max}_{\bm{x}\in\mathbb{X}_{\mathsf{pareto}}}u( \bm{f}(\bm{x})).\] (2)

Following the preference BO literature, we assume the utility function is not available to evaluate and its functional form is unknown. Additionally, it is well-established that user preferences are difficult to be assigned to continuous numerical values; instead we suppose that users are more inclined to provide weak supervision in the form of pairwise comparisons [8; 4]. The following assumption is made to assert that a typical user will select dominating solutions when possible: _If \(\bm{y}_{1}\) and \(\bm{y}_{2}\) are candidate outcomes presented to the user and \(\bm{y}_{1}\succ\bm{y}_{2}\), then the user will always select \(\bm{y}_{1}\); that is, \(u(\bm{y}_{1})>u(\bm{y}_{2})\)_. This assumption should be enforced when modeling preference-based MOBO problems to accurately reflect real user behavior.

## 3 Preference-Utility-Balanced (PUB) MOBO

Users often require some assurance that the suggested candidates are not only high in utility, but also near-(Pareto)-optimal. PUB-MOBO relies on utility maximization to ascertain candidate solutions that are preferred by the user while promoting a local search towards the Pareto-front using estimated gradients. We observe that the local search finds solutions near Pareto points, which subsequently accelerates the search for high-utility solutions.

### PUB-MOBO Algorithm

The proposed PUB-MOBO method operates in three stages. We extend the two stages (PE: preference exploration, and EXP: outcome evaluation via experiments) in [4] with an additional stage based on local multi-gradient descent, denominated the GD stage. In each PUB-MOBO iteration, these three stages are executed, and the process is repeated _ad infinitum_, or (more practically) until a pre-decided budget for total number of outcome evaluations is attained; see Algorithm 1 in Appendix C.

**Preference Exploration**: Here, the user expresses their preferences over a query of two candidate solutions in a form of pairwise comparisons. The comparison is used to update the estimate \(\hat{u}\) of the utility, obtained implicitly with a pairwise GP and the EUBO acquisition function proposed in [4]; see Appendix B.1 for the closed-form expression. Note that no evaluation of \(\bm{f}\) is required for PE.

**Outcome evaluation via Experiments**: Here, we compute the optimal decision variables and evaluate true outcomes to update the outcome model \(\hat{\bm{f}}\) using the expected improvement under utility uncertainty (qEIUU) [9] acquisition; see Appendix B.2 for the closed-form expression. Maximizing qEIUU involves taking Monte Carlo samples [10; 11] and yields the optimal decision variables, \(\bm{x}_{\text{EXP}}\)After \(\bm{x}_{\text{EXP}}\) is obtained, we append it along with its true outcome value \(\bm{f}(\bm{x}_{\text{EXP}})\) to the current dataset.

**Multi-gradient descent**: This GD stage is motivated by the fact that \(\bm{x}_{\text{EXP}}\), while expected to be high in utility, is not specifically designed to be near the Pareto-front. Analogous to single-objective optimization, we will pursue local gradients that are expected to generate a trajectory of \(\bm{x}\) candidates that evolves towards a nearby Pareto-optimal point. We will refer to these gradient-following decision variables as '\(\bm{x}_{\text{GD}}\)'. We set the initial \(\bm{x}_{\text{GD}}\) to be \(\bm{x}_{\text{EXP}}\).

For a MOO problem, gradient descent must be adapted for multiple objectives. We propose the use of multiple gradient descent algorithm (MGDA) [12], which was designed for smooth multi-outcome objective functions. MGDA exhibits some theoretical properties that, we hypothesize, and demonstrate via experiments, are beneficial in the MOBO context.

MGDA exploits the KKT conditions [13] as a quadratic cost constrained on the probability simplex:

\[\min_{\bm{\alpha}\geq\bm{0}}\ \left\|\bm{\alpha}^{\top}\nabla\bm{f}(\bm{x}) \right\|^{2}\ \text{subject to:}\ \bm{1}^{\top}\bm{\alpha}=1.\] (3)

It is well-known, c.f. [12], that a solution to (3) is either: \(\bm{\alpha}^{\top}\nabla\bm{f}(\bm{x})=0\), in which case the current parameters \(\bm{x}\) are Pareto-optimal, or \(\bm{\alpha}^{\top}\nabla\bm{f}(\bm{x})\neq 0\), and \(\bm{\alpha}^{\top}\nabla\bm{f}(\bm{x})\) is a feasible descent direction. Given that (3) is a quadratic cost over linear constraints, we can use the Frank-Wolfe algorithm [14; 15] to efficiently compute optimal solutions; see pseudocode in Algorithm 3 in Appendix C.

Solving (3) yields an optimal \(\bm{\alpha}\) with which we can take a gradient step \(\bm{x}_{\text{GD}}\leftarrow\bm{x}_{\text{GD}}-\eta\bm{\alpha}^{\top}\bm{ \nabla}f(\bm{x}_{\text{GD}})\). However, there are two clear difficulties at this juncture. The first is that this update may yield an \(\bm{x}_{\text{GD}}\not\in\mathbb{X}\). To counter this, we stop updating when this happens, and stop the local gradient search phase, moving on to the next PUB-MOBO iterations with an updated dataset \(D\) that contains all the \(\bm{x}_{\text{GD}}\) and correspond \(\bm{y}_{\text{GD}}\) observed so far. The second and more debilitating problem is that we do not have access to gradients of \(\bm{f}\). Thankfully, we do have a surrogate model \(\bm{\hat{f}}\) with which we can obtain an estimate of the gradient at any \(\bm{x}\) with \(\bm{\mu}^{\nabla}:=\mathbb{E}[\bm{\nabla}\bm{\hat{f}}(\bm{x})]\) through (7a). The gradient step is then \(\bm{x}_{\text{GD}}\leftarrow\bm{x}_{\text{GD}}-\eta\bm{\alpha}^{\top}\bm{\mu} ^{\nabla}(\bm{x}_{\text{GD}})\). Unfortunately, there is no clear correlation between the uncertainties in \(\bm{f}\) and \(\bm{\nabla}\bm{f}\), so \(\bm{\mu}^{\nabla}\) could have large uncertainties even near previously observed points. Therefore, it is imperative to incorporate techniques that can reduce uncertainty in the posterior of the gradient estimate. To this end, we propose to use the gradient information (GI) acquisition function [16].

**Multi-gradient descent with GI acquisition**: We briefly explain the mechanism of the GI acquisition. Suppose we select the best candidate from the EXP stage, \(\bm{x}_{\text{EXP}}\), and set it as the initial candidate for the local gradient search: \(\bm{x}_{\text{GD}}\). The GI acquisition tries to select a subsequent point \(\bm{x}^{\prime}\) that will minimize the uncertainty of the gradient at \(\bm{x}_{\text{GD}}\) if \(\bm{x}^{\prime}\) and its corresponding \(\bm{y}^{\prime}\) were known. By considering all \(n_{f}\) objective independently distributed, we assess the uncertainty can formulate the uncertainty information using an A-optimal design criterion [17], which, for Gaussian distributions, involves maximizing:

\[\text{GI}(\bm{x}^{\prime})=\sum_{i=1}^{n_{f}}\text{Tr}\left(\nabla k_{i}(\bm{x }_{\text{GD}},\bm{X}^{\prime})\mathcal{K}_{\sigma}^{-1}(\bm{X}^{\prime}) \nabla k_{i}^{\top}(\bm{x}_{\text{GD}},\bm{X}^{\prime})\right)\] (4)

where \(\bm{X}^{\prime}=\{\bm{X}\cup\bm{x}^{\prime}\}\). For each gradient-step in \(n_{\text{GD}}\), the GI acquisition function is optimized \(n_{\text{GI}}\) times to reduce gradient uncertainty. Upon each optimization, we evaluate the outcome function to obtain a corresponding \(\bm{y}_{\text{GD}}\), which is appended to the dataset \(D\) for subsequent PUB-MOBO iterations. We provide the derivation of the GI acquisition function in Appendix B.3 and pseudocode of multi-gradient descent in Algorithm 2, in Appendix C.

### Preference-Dominated Utility Function

We propose the PDUF, which merges the concept of dominance with user preferences to help locate high utility points that are close to Pareto-optimality. The utility function, which represents user preferences, is employed to respond to user queries, such as providing pairwise comparisons between two outcomes [4]. It should satisfy two key properties:

1. _Dominance Preservation_: When evaluating a query, the true utility function should satisfy Assumption 1.
2. _Preference Integration_: The utility function should have parameters \(\theta_{u}\) that allow unique strictly maximal-utility Pareto-optimal solutions. That is, for any \(\bm{x}\in\mathbb{X}_{\text{pareto}}\), there exists an easily computable \(\theta_{u}\in\mathbb{R}^{n_{u}}\) such that \(u(\bm{f}(\bm{x})|\theta_{u})>u(\bm{f}(\{\mathbb{X}_{\text{pareto}}\setminus\bm{x} \}|\theta_{u})\).

For instance, the commonly used \(\ell_{1}\) distance (a) fails to satisfy the _Preference Integration_ property when calculated from the utopia point, and violates _Dominance Preservation_ when calculated from any other point. This is illustrated in Fig. 0(a), where the contours of an \(\ell_{1}\) distance utility function is shown with an example Pareto-front. Here, the two red points are indistinguishable according to the utility function, demonstrating the limitations of \(\ell_{1}\) distance in distinguishing between Pareto-optimal solutions.

Therefore, we propose the preference-dominated utility function (PDUF) which merges the concept of dominance with user preferences. An illustration of the contours in a 2D case is shown in Fig. 0(b). The PDUF integrates the concept of dominance with user preferences by combining multiple logistic functions centered around different points in the objective function space and is expressed as:

\[u(\bm{y})=\frac{1}{n_{c}}\sum_{i=1}^{n_{c}}\prod_{j=1}^{n_{y}}L_{\beta}(y_{j},c _{i,j})\] (5)

where \(L_{\beta}(y_{j},c_{i,j})=\frac{1}{1+\exp(\beta^{\prime}(y_{j}-c_{i,j}))}\). \(c_{i}=(c_{i,1},c_{i,2},\dots,c_{i,n_{y}})\) denotes the \(i^{\text{th}}\) center for one logistic function, \(\beta\) denotes a parameter that controls the steepness of the logistic function, and \(n_{c}\) denotes the number of centers. The logistic function \(L_{\beta}(y_{j},c_{i,j})\) approximates the step function and enforces dominance for each objective \(y_{j}\), as seen in the red dashed lines in Fig. 0(b), and the product aggregates this approximation for all objectives. Furthermore, the sum of logistic function products preserve dominance in the objective space. Indeed, for every \(\bm{\bar{y}}\) that dominates user query \(\bm{c}_{i}\), PDUF will express user preference with \(u(\bm{\bar{y}})>u(\bm{c}_{i})\). Finally, the centers define the parameters \(\theta_{u}\) that ensure the utility function adheres to the _preference integration_ property by aligning them along an arbitrary line (the grey line in Fig. 0(b)).

## 4 Experiments

We validate the proposed PUB-MOBO method on benchmarks commonly found in MOO literature: DTLZ1 (\(n_{x}=9,n_{f}=2\)) [18], DH1 (\(n_{x}=10,n_{f}=2\)) [7], Conceptual Marine Design (\(n_{x}=6,n_{f}=4\)) [19], Car Side Impact (\(n_{x}=7,n_{f}=4\)) [20]. The baselines and ablations that we compare are (i) EUBO+qEIUU baseline which contains only the PE and EXP stages; (ii) PUB-MOBO-PG which uses the predicted gradients (PG) without any outcome evaluations or GI optimizations in the GD stage. This makes it relatively inexpensive, but it ignores the fact that additional samples can yield useful derivative information; (iii) PUB-MOBO-PG+OE which is a PUB-MOBO ablation that uses the predicted gradients as in PUB-MOBO-PG, but an Outcome Evaluation (OE) is performed at every gradient descent step in an effort to lower gradient uncertainty around observed points; (iv)

Figure 1: Contour plots of (a) the commonly used negative \(l_{1}\) distance Utility function (b) the proposed PDUF.

PUB-MOBO which is the proposed method. Figure 2 illustrates the performance of the experiments in terms of utility regret and distance to the Pareto front w.r.t. outcome evaluations and user queries. EUBO+qEIUU is the poorest-performing algorithm for all the metrics affirming the effectiveness of the additional stage based on local gradient search. However, PUB-MOBO-PG performs equally poorly, largely due to inaccurate gradient estimation obtained with the surrogate model \(\hat{\bm{f}}\) in (7). We frequently observe that the evolution of \(\bm{x}_{\mathrm{GD}}\) in the GD stage is prematurely terminated either due to infeasibility in \(\bm{x}\) or because of incorrect solutions to MGDA due to erroneous \(\bm{\mu}^{\nabla}(\bm{x}_{\mathrm{GD}})\). The PG+OE variant significantly outperforms the PG variant due to its enhanced gradient estimation accuracy, which justifies the additional computational cost of updating the outcome model. PUB-MOBO further improves on the PG+OE variant by using the GI acquisition function to reduce gradient uncertainty, leading to even more accurate gradient estimates.

## 5 Conclusion

In this work we presented PUB-MOBO, a sample efficient Multi-Objective Bayesian Optimization algorithm that combine user-preference with a gradient-based search to compute near Pareto-optimal solutions. We verify that our proposed method yields high utility and reduced distance to Pareto-front solutions, and also demonstrate the importance of gradient uncertainty reduction in the gradient-based search. Finally, the proposed utility function respects dominance while modeling different user preferences.

Figure 2: Performance comparison on benchmarks DTLZ1, DH1, Conceptual Marine Design, Car Side Impact. Continuous lines show median over 100 runs, and shading indicates 25-75 percentiles.

## References

* [1] Samuel Daulton, Maximilian Balandat, and Eytan Bakshy. Differentiable expected hypervolume improvement for parallel multi-objective bayesian optimization. _Advances in Neural Information Processing Systems_, 33:9851-9864, 2020.
* [2] Ketong Shao, Diego Romeres, Ankush Chakrabarty, and Ali Mesbah. Preference-guided Bayesian optimization for control policy learning: Application to personalized plasma medicine. In _NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World_, 2023.
* [3] Ryota Ozaki, Kazuki Ishikawa, Youhei Kanzaki, Shion Takeno, Ichiro Takeuchi, and Masayuki Karasuyama. Multi-objective Bayesian optimization with active preference learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 14490-14498, 2024.
* [4] Zhiyuan Jerry Lin, Raul Astudillo, Peter Frazier, and Eytan Bakshy. Preference exploration for efficient bayesian optimization with multiple outcomes. In _International Conference on Artificial Intelligence and Statistics_, pages 4235-4258. PMLR, 2022.
* [5] Kaisa Miettinen. _Nonlinear multiobjective optimization_, volume 12. Springer Science & Business Media, 1999.
* [6] Giorgio Chiandussi, Marco Codegone, Simone Ferrero, and Federico Erminio Varesio. Comparison of multi-objective optimization methodologies for engineering applications. _Computers & Mathematics with Applications_, 63(5):912-942, 2012.
* [7] Kalyanmoy Deb and Himanshu Gupta. Searching for robust Pareto-optimal solutions in multi-objective optimization. In _International conference on evolutionary multi-criterion optimization_, pages 150-164. Springer, 2005.
* [8] Wei Chu and Zoubin Ghahramani. Preference learning with gaussian processes. In _Proceedings of the 22nd international conference on Machine learning_, pages 137-144, 2005.
* [9] Raul Astudillo and Peter Frazier. Multi-attribute bayesian optimization with interactive preference learning. In Silvia Chiappa and Roberto Calandra, editors, _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108 of _Proceedings of Machine Learning Research_, pages 4496-4507. PMLR, 26-28 Aug 2020.
* [10] James Wilson, Frank Hutter, and Marc Deisenroth. Maximizing acquisition functions for bayesian optimization. _Advances in neural information processing systems_, 31, 2018.
* [11] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. _Advances in neural information processing systems_, 33:21524-21538, 2020.
* [12] Jean-Antoine Desideri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization. _Comptes Rendus Mathematique_, 350(5-6):313-318, 2012.
* [13] Stefan Schaffler, Reinhart Schultz, and Klaus Weinzierl. Stochastic method for the solution of unconstrained vector optimization problems. _Journal of Optimization Theory and Applications_, 114:209-222, 2002.
* [14] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. _Advances in neural information processing systems_, 31, 2018.
* [15] Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In _International conference on machine learning_, pages 427-435. PMLR, 2013.
* [16] Sarah Muller, Alexander von Rohr, and Sebastian Trimpe. Local policy search with bayesian optimization. _Advances in Neural Information Processing Systems_, 34:20708-20720, 2021.
* [17] Ankush Chakrabarty, Gregery T Buzzard, and Ann E Rundell. Model-based design of experiments for cellular processes. _Wiley Interdisciplinary Reviews: Systems Biology and Medicine_, 5(2):181-203, 2013.

* [18] Kalyanmoy Deb, Lothar Thiele, Marco Laumanns, and Eckart Zitzler. Scalable test problems for evolutionary multiobjective optimization. In _Evolutionary multiobjective optimization: theoretical advances and applications_, pages 105-145. Springer, 2005.
* [19] Michael G Parsons and Randall L Scott. Formulation of multicriterion design optimization problems for solution with scalar numerical optimization methods. _Journal of Ship Research_, 48(01):61-76, 2004.
* [20] Himanshu Jain and Kalyanmoy Deb. An evolutionary many-objective optimization algorithm using reference-point based nondominated sorting approach, part ii: Handling constraints and extending to an adaptive approach. _IEEE Transactions on evolutionary computation_, 18(4):602-622, 2013.
* [21] Mauricio A Alvarez, Lorenzo Rosasco, Neil D Lawrence, et al. Kernels for vector-valued functions: A review. _Foundations and Trends(r) in Machine Learning_, 4(3):195-266, 2012.
* [22] Christopher KI Williams and Carl Edward Rasmussen. _Gaussian processes for machine learning_, volume 2. MIT press Cambridge, MA, 2006.
* [23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.

## Appendix A Preliminaries

### Modeling with Gaussian processes

We first discuss the modeling choices considered to learn the outcome function \(\bm{f}\) and the utility function \(u\): their respective approximations are denoted \(\hat{\bm{f}}\) and \(\hat{u}\).

#### a.1.1 Modeling outcomes

Gaussian process (GP) regression is a popular choice for constructing the surrogate \(\hat{\bm{f}}\) for the true outcome function \(\bm{f}\). We train an independent GP for each objective \(\bm{f}_{i}\), though a multi-output GP that models correlations between the objectives could also be considered [21]. Each GP is defined _a priori_ by a mean function \(m(\bm{x})\) and covariance function \(k_{i}(x,x^{\prime})\) called kernel. For this work, any \(\mathcal{C}^{2}\) kernel is admissible.

Let \(\bm{X}_{T}=[\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{T}]\); we drop the subscript for brevity. Given a dataset \(D:=(\bm{X},\bm{Y})\), comprising input-outcome pairs, the mean and variance of the posterior are given by

\[\mu_{i}(\bm{x}) =m(\bm{x})+k_{i}(\bm{x},\bm{X})\mathcal{K}_{\sigma}^{-1}(\bm{X}) (Y_{i}-m(\bm{X})),\] (6a) \[\Sigma_{i}(\bm{x}) =k_{i}(\bm{x},\bm{x})-k_{i}(\bm{x},\bm{X})\mathcal{K}_{\sigma}^{ -1}(\bm{X})k_{i}(\bm{X},\bm{x}),\] (6b)

where \(\mathcal{K}_{\sigma}(\bm{X}):=k_{i}(\bm{X},\bm{X})+\sigma^{2}\,\bm{I}\) and \(m(\cdot)\) is the prior mean. Since the derivative is a linear operator, the derivative GP is another GP [22] characterized fully by the mean and covariance functions

\[\mu_{i}^{\nabla}(\bm{x}) =\nabla m(\bm{x})+\nabla k_{i}(\bm{x},\bm{X})\mathcal{K}_{\sigma }^{-1}(\bm{X})(Y_{i}-m(\bm{X})),\] (7a) \[\Sigma_{i}^{\nabla}(\bm{x}) =\nabla^{2}k_{i}(\bm{x},\bm{x})-\nabla k_{i}(\bm{x},\bm{X}) \mathcal{K}_{\sigma}^{-1}(\bm{X})\nabla k_{i}(\bm{X},\bm{x}),\] (7b)

In the implementation, each GP is designed with a Matern 5/2 kernel with ARD, a lengthscale prior defined by Gamma(\(\alpha\) = 3, \(\beta\) = 6), and an outputscale prior defined by Gamma(\(\alpha\) = 2, \(\beta\) = 0.15). To infer the gradient mean (7a) and covariance (7b) of the posterior, automatic different [23] is used. The inputs \(\bm{X}\) are normalized from [0, 1] and the outcomes \(\bm{Y}\) are standardized to zero mean and unit variance during GP fitting. We initialize the model with 6 outcomes.

#### a.1.2 Modeling preferences

We assume the user is only capable of weak supervisions in the form of pairwise comparisons (PC). That is, if the user prefers \(\bm{y}:=\bm{f}\) over \(\bm{y}:=\bm{f}^{\prime}\), the pairwise comparison function \(r(\bm{y},\bm{y}^{\prime})=0\). In the event that the user prefers \(\bm{y}^{\prime}\) instead, \(r(\bm{y},\bm{y}^{\prime})=1\). Pairwise GPs c.f. [8] allow us to learn a latent functional representation \(\hat{u}\) of the true user utility based on this preference feedback. The latent function satisfies \(\hat{u}(\bm{y})>\hat{u}(\bm{y}^{\prime})\) if the user prefers \(\bm{y}\), and vice versa. In the implementation, we use the RBF kernel with ARD, a lengthscale prior defined by Gamma(\(\alpha\) = 2.4, \(\beta\) = 2.7), and an outputscale prior defined by a smoothed box prior from [0.01, 100]. The outcomes \(\bm{Y}\) are normalized from [0, 1] during GP fitting. We initialize with 12 Sobol points and form pairwise comparisons with every consecutive pair of outcomes to yield 6 user comparisons.

## Appendix B Acquisition functions

### EUBO

The EUBO acquisition is given by:

\[\mathsf{EUBO}(\bm{x_{1}},\bm{x_{2}})=\mathbb{E}[\max(\hat{u}(\bm{\hat{f}}(\bm {x_{1}}),\hat{u}(\bm{\hat{f}}(\bm{x_{2}}))],\] (8)

where the hat notation denotes surrogate models of the corresponding functions.

### qEIUU

The expected improvement under utility uncertainty is given by

\[\texttt{qEIUU}(\bm{x})=\mathbb{E}\left[\max(\hat{u}(\bm{\hat{f}}(\bm{x}))-\hat{ u}(\bm{f}(\bm{x}_{\text{best}})),0)\right],\] (9)

where \(\bm{x}_{\text{best}}=\arg\max_{x\in\bm{X}}\hat{u}(\bm{\hat{f}}(\bm{x}))\), and \(\bm{x}_{\text{EXP}}:=\arg\max_{\mathbb{X}}\texttt{qEIUU}(\bm{x})\). Since the expectation in (9) is with respect to the outcome and utility models, the analytical expression is challenging.

### Gi

We derive the GI acquisition function described in (4). The derivation is an adaptation of the Gradient Information acquisition function in [16] to the case of independent multi-objectives. We begin by expressing the difference in the trace of the gradient posterior covariance before and after the addition of the new datapoint \((\bm{x}^{\prime},\bm{y}^{\prime})\) to the dataset \(D\)

\[\text{GI}=\sum_{i=1}^{n_{f}}\mathbb{E}\left[\mathsf{Tr}(\Sigma_{i}^{\nabla}( \bm{x}_{\text{GD}}|D))-\mathsf{Tr}\left(\Sigma_{i}^{\nabla}\left(\bm{x}_{ \text{GD}}|D,(\bm{x}^{\prime},\bm{y}^{\prime})\right))\right].\]

This can be expressed as the Lebesgue-Stieltjes integral

\[\text{GI}=\sum_{i=1}^{n_{f}}\int\left[\mathsf{Tr}(\Sigma_{i}^{\nabla}(\bm{x}_{ \text{GD}}|D))-\mathsf{Tr}\left(\Sigma_{i}^{\nabla}\left(\bm{x}_{\text{GD}}|D, (\bm{x}^{\prime},\bm{y}^{\prime})\right))\right]dF(\bm{x}^{\prime}),\]

with \(F\) denoting the distribution of \(\bm{x}^{\prime}\). For optimization purposes maximizing GI is equivalent to maximizing

\[\underset{\bm{x}^{\prime}}{\text{argmax GI}}\equiv\underset{\bm{x}^{\prime}} {\text{argmax}}\sum_{i=1}^{n_{f}}\int-\mathsf{Tr}\left(\Sigma_{i}^{\nabla} \left(\bm{x}_{\text{GD}}|D,(\bm{x}^{\prime},\bm{y}^{\prime})\right)\right)dF( \bm{x}^{\prime}).\]

since the first term does not depend on the optimization variable \(\bm{x}^{\prime}\). Rewriting this formulation as a Reinmann integral will yield

\[\underset{\bm{x}^{\prime}}{\text{argmax GI}}=\underset{\bm{x}^{\prime}}{ \text{argmin}}\sum_{i=1}^{n_{f}}\int_{\mathbb{R}}\mathsf{Tr}\left(\Sigma_{i}^ {\nabla}\left(\bm{x}_{\text{GD}}|D,(\bm{x}^{\prime},\bm{y}^{\prime})\right) \right)\cdot p(\bm{f}(\bm{x}^{\prime})=\bm{y}^{\prime}|D)d\bm{y}^{\prime}.\]

As seen from (7b), the covariance in Gaussian distributions is independent of the observed outcomes, so the acquisition function can be further reduced to

\[\underset{\bm{x}^{\prime}}{\text{argmax GI}} =\underset{\bm{x}^{\prime}}{\text{argmin}}\sum_{i=1}^{n_{f}} \mathsf{Tr}\left(\Sigma_{i}^{\nabla}\left(\bm{x}_{\text{GD}}|D,(\bm{x}^{\prime },\bm{y}^{\prime})\right)\right)\underbrace{\int_{\mathbb{R}}p(\bm{f}(\bm{x}^ {\prime})=\bm{y}^{\prime}|D)d\bm{y}^{\prime}}_{=1},\] \[=\underset{\bm{x}^{\prime}}{\text{argmin}}\sum_{i=1}^{n_{f}} \mathsf{Tr}\left(\Sigma_{i}^{\nabla}\left(\bm{x}_{\text{GD}}|D,(\bm{x}^{\prime },\bm{y}^{\prime})\right)\right),\] \[=\underset{\bm{x}^{\prime}}{\text{argmax}}\sum_{i=1}^{n_{f}} \mathsf{Tr}\left(\nabla k_{i}(\bm{x}_{\text{GD}},\bm{X}^{\prime})\mathcal{K}_ {\sigma}^{-1}(\bm{X}^{\prime})\nabla k_{i}^{\top}(\bm{x}_{\text{GD}},\bm{X}^{ \prime})\right),\]

where \(\bm{X}^{\prime}=\{\bm{X}\cup\bm{x}^{\prime}\}\).

## Appendix C PUB-MOBO Algorithms

```
1: Generate initial data: \(\bm{x}_{\text{INIT}},\bm{y}_{\text{INIT}},r(\bm{y}_{\text{INIT}})\)
2:\(D=(\bm{x}_{\text{INIT}},\bm{y}_{\text{INIT}})\)
3:\(P=(\bm{y}_{\text{INIT}},r(\bm{y}_{\text{INIT}}))\)
4: Update outcome model \(\bm{\hat{f}}\) with \((\bm{x}_{\text{INIT}},\bm{y}_{\text{INIT}})\)
5: Update preference model \(\hat{u}\) with \((\bm{y}_{\text{INIT}},r(\bm{y}_{\text{INIT}}))\)
6:while# outcome evaluations \(\leq\) budget do
7:PE stage
8:\(\bm{x}_{\bm{1}},\bm{x}_{\bm{2}}\leftarrow\text{argmax}_{\bm{x}_{\bm{1}},\bm{x }_{\bm{2}}}\) EUBO
9:\(\bm{y}_{\bm{1}},\ \bm{y}_{\bm{2}}=\bm{\hat{f}}(\bm{x}_{\bm{1}}),\bm{\hat{f}}(\bm{x}_{\bm{2}})\)
10:\(r(\bm{y}_{\bm{1}},\ \bm{y}_{\bm{2}})\leftarrow\) user provides a comparison
11: Append \(P\) with \((\bm{y}_{\bm{1}},\ \bm{y}_{\bm{2}},r(\bm{y}_{\bm{1}},\ \bm{y}_{\bm{2}}))\)
12: Update pref. model \(\hat{u}\) with \((\bm{y}_{\bm{1}},\ \bm{y}_{\bm{2}},r(\bm{y}_{\bm{1}},\ \bm{y}_{\bm{2}}))\)
13:EXP stage
14:\(\bm{x}_{\text{EXP}}\leftarrow\text{argmax}_{\bm{x}}\) qEIUU
15:\(\bm{y}_{\text{EXP}}=\bm{f}(\bm{x}_{\text{EXP}})\)
16: Append \(D\) with \((\bm{x}_{\text{EXP}},\bm{y}_{\text{EXP}})\)
17: Update outcome model \(\bm{\hat{f}}\) with \((\bm{x}_{\text{EXP}},\bm{y}_{\text{EXP}})\)
18:GD stage
19:\((\bm{X}_{\text{GD}},\bm{Y}_{\text{GD}})\leftarrow\) Local Gradient Descent(\(\bm{x}_{\text{EXP}}\))
20: Append \(D\) with \((\bm{X}_{\text{GD}},\bm{Y}_{\text{GD}})\)
21:endwhile ```

**Algorithm 1** PUB-MOBO

```
1: Initialize \(\bm{x}_{\text{GD}}\leftarrow\bm{x}_{\text{EXP}}\)
2:\((\bm{X}_{\text{GD}},\bm{Y}_{\text{GD}})=(\emptyset,\emptyset)\)
3:# of multi-gradient steps, \(n_{\text{GD}}\)\(\triangleright\) default:10
4:# of GI optimizations, \(n_{\text{GI}}\)\(\triangleright\) default:1
5:Early stopping threshold, \(\varepsilon_{\text{GD}}\)\(\triangleright\) default:0.1
6:for\(i\leq n_{\text{GD}}\)do
7: Compute \(\bm{\mu}^{\nabla}(\bm{x}_{\text{GD}})\) using (7a)
8: Compute \(\bm{M}=\bm{\mu}^{\nabla}(\bm{x}_{\text{GD}})^{\top}\bm{\mu}^{\nabla}(\bm{x}_{ \text{GD}})\)
9:\(\bm{\alpha}\leftarrow\text{Frank-Wolfe}(\bm{M})\)
10:\(\bm{x}_{\text{GD}}\leftarrow\bm{x}_{\text{GD}}-\eta\bm{\alpha}^{\top}\bm{\mu}^ {\nabla}(\bm{x}_{\text{GD}})\)
11:if\(\bm{x}_{\text{GD}}\)\(\in\)X and \(\left\|\left|\bm{\alpha}^{\top}\bm{\mu}^{\nabla}(\bm{x}_{\text{GD}})\right|\right|_{ 2}^{2}>\varepsilon_{\text{GD}}\)then
12: Evaluate the true objective: \(\bm{y}_{\text{GD}}=\bm{f}(\bm{x}_{\text{GD}})\)
13: Append \((\bm{X}_{\text{GD}},\bm{Y}_{\text{GD}})\) with \((\bm{x}_{\text{GD}},\bm{y}_{\text{GD}})\)
14: Update outcome model \(\bm{\hat{f}}\) with \((\bm{x}_{\text{GD}},\bm{y}_{\text{GD}})\)
15:for\(j\leq n_{\text{GI}}\)do
16:\(\bm{x}_{\text{GI}}\leftarrow\arg\max_{\bm{x}^{\prime}}\)GI
17: Evaluate the true objective: \(\bm{y}_{\text{GI}}=\bm{f}(\bm{x}_{\text{GI}})\)
18: Append \((\bm{X}_{\text{GD}},\bm{Y}_{\text{GD}})\) with \((\bm{x}_{\text{GI}},\bm{y}_{\text{GI}})\)
19: Update outcome model \(\bm{\hat{f}}\) with \((\bm{x}_{\text{GI}},\bm{y}_{\text{GI}})\)
20:endfor
21:else
22:break
23:endif
24:endfor
25:return\((\bm{X}_{\text{GD}},\bm{Y}_{\text{GD}})\) ```

**Algorithm 2** Multi-Gradient Descent```
1:input M
2:initialize\(\boldsymbol{\alpha}=[\frac{1}{n_{f}},...,\frac{1}{n_{f}}]\) s.t. \(\boldsymbol{1}^{\top}\boldsymbol{\alpha}=1\)
3:for\(j\leq\#\) max no. of Frank-Wolfe steps do
4:\(\hat{t}=\arg\min_{r}\sum_{t}\epsilon_{t}\mathbf{M}_{rt}\)
5:\(\hat{\gamma}=\arg\min_{\gamma}\left((1-\gamma)\boldsymbol{\alpha}+\gamma \mathbf{e}_{\hat{t}}\right)^{\top}\mathbf{M}\left((1-\gamma)\boldsymbol{\alpha }+\gamma\mathbf{e}_{\hat{t}}\right)\)
6:\(\boldsymbol{\alpha}=(1-\hat{\gamma})\boldsymbol{\alpha}+\hat{\gamma}\mathbf{e}_ {\hat{t}}\)
7:if\(\hat{\gamma}\sim 0\) then
8:break
9:endif
10:endfor
11:return\(\boldsymbol{\alpha}\) ```

**Algorithm 3** Frank-Wolfe Algorithm