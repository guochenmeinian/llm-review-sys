# Fitting trees to \(\ell_{1}\)-hyperbolic distances

Joon-Hyeok Yim

Anna C. Gilbert

###### Abstract

Building trees to represent or to fit distances is a critical component of phylogenetic analysis, metric embeddings, approximation algorithms, geometric graph neural nets, and the analysis of hierarchical data. Much of the previous algorithmic work, however, has focused on generic metric spaces (i.e., those with no _a priori_ constraints). Leveraging several ideas from the mathematical analysis of hyperbolic geometry and geometric group theory, we study the tree fitting problem as finding the relation between the hyperbolicity (ultrametricity) vector and the error of tree (ultrametric) embedding. That is, we define a vector of hyperbolicity (ultrametric) values over all triples of points and compare the \(\ell_{p}\) norms of this vector with the \(\ell_{q}\) norm of the distortion of the best tree fit to the distances. This formulation allows us to define the average hyperbolicity (ultrametricity) in terms of a normalized \(\ell_{1}\) norm of the hyperbolicity vector. Furthermore, we can interpret the classical tree fitting result of Gromov as a \(p=q=\infty\) result. We present an algorithm HCCRootedTreeFit such that the \(\ell_{1}\) error of the output embedding is analytically bounded in terms of the \(\ell_{1}\) norm of the hyperbolicity vector (i.e., \(p=q=1\)) and that this result is tight. Furthermore, this algorithm has significantly different theoretical and empirical performance as compared to Gromov's result and related algorithms. Finally, we show using HCCRootedTreeFit and related tree fitting algorithms, that supposedly standard data sets for hierarchical data analysis and geometric graph neural networks have radically different tree fits than those of synthetic, truly tree-like data sets, suggesting that a much more refined analysis of these standard data sets is called for.

## 1 Introduction

Constructing trees or ultrametrics to fit given data are both problems of great interest in scientific applications (e.g., phylogeny), algorithmic applications (optimal transport and Wasserstein distances are easier, for example, to compute quickly on trees), data visualization and analysis, and geometric machine learning. An ultrametric space is one in which the usual triangle inequality has been strengthened to \(\tilde{d}(x,y)\leq\max\{d(x,z),d(y,z)\}\). A hyperbolic metric space is one in which the metric relations amongst any four points are the same as they would be in a tree, up to the additive constant \(\delta\). More generally, any finite subset of a hyperbolic space "looks like" a finite tree.

There has been a concerted effort to solve both of these problems in the algorithmic and machine learning communities, including [1, 2, 3, 4, 5] among many others. Indeed, the motivation for embedding into hyperbolic space or into trees was at the heart of the recent explosion in geometric graph neural networks [6].

As an optimization problem, finding the tree metric that minimizes the \(\ell_{p}\) norm of the difference between the original distances and those on the tree (i.e., the distortion) is known to be NP-hard for most formulations. [7] showed that it is APX-hard under the \(\ell_{\infty}\) norm. The first positive result also came from [7], which provided a \(3\)-approximation algorithm and introduced a reduction technique from a tree metric to an ultrametric (a now widely used technique). The current best known result is an \(O((\log n\log\log n)^{1/p})\) approximation for \(1<p<\infty\)[2], and \(O(1)\) approximation for \(p=1\)recently shown by [1]. Both papers exploit the hierarchical correlation clustering reduction method and its LP relaxation to derive an approximation algorithm.

While these approximation results are fruitful, they are not so practical (the LP result uses an enormous number of variables and constraints). On the more practical side, [8] provided a robust method, commonly known as _Neighbor Join_ (which can be computed in \(O(n^{3})\) time), for constructing a tree. Recently, [9] proposed an \(O(n^{2})\) method known as _TreeRep_ for constructing a tree. Unfortunately, neither algorithm provides a guaranteed bound on the distortion.

The main drawback with all of these results is that they assume almost nothing about the underlying discrete point set, when, in fact, many real application data sets are close to hierarchical or nearly so. After all, why fit a tree to generic data only to get a bad approximation? In fact, perhaps with some geometric assumptions on our data set, we can fit a better tree metric or ultrametric, perhaps even more efficiently than for a general data set.

Motivated by both Gromov's \(\delta\)-hyperbolicity [4] and the work of Chatterjee and Slonam [10] on average hyperbolicity, we define proxy measures of how _tree-like_ a data set is. We note that [4], [11] provide a simple algorithm and analysis to find a tree approximation for which the maximum distortion (\(\ell_{\infty}\) norm) is bounded by \(O(\delta\log n)\), where \(\delta\) is the hyperbolicity constant. Moreover, this bound turns out to be the best order we can have. In this paper, we go beyond the simple notion of \(\delta\)-hyperbolicity to define a vector of hyperbolicity values \(\bm{\Delta}(d)\) for a set of distance values. The various \(\ell_{p}\) norms of this vector capture how tree-like a data set is. Then, we show that the \(\ell_{q}\) norm of the distortion of the best fit tree and ultrametrics can be bounded in terms of this tree proxy. Thus, we give a new perspective on the tree fitting problem, use the geometric nature of the data set, and arrive at, hopefully, better and more efficient tree representations. The table below captures the relationship between the different hyperbolicity measures and the tree fit distortion. We note the striking symmetry in the tradeoffs.

Our main theoretical result can be summarized as

There is an algorithm which runs in time \(O(n^{3}\log n)\) which returns a tree metric \(d_{T}\) with distortion bounded by the average (or \(\ell_{1}\)) hyperbolicity of the distances; i.e.,

\[\|d-d_{T}\|_{1}\leq 8\|\bm{\Delta}_{x}(d)\|_{1}\leq 8{n-1\choose 3}\operatorname{ AvgHyp}(d).\]

Additionally, the performance of HCCRottedTreeFit and other standard tree fitting algorithms on commonly used data sets (especially in geometric graph neural nets) shows that they are quite far from tree-like and are not well-represented by trees, especially when compared with synthetic data sets. This suggests that we need considerably more refined geometric notions for learning tasks with these data sets.

## 2 Preliminaries

### Basic definitions

To set the stage, we work with a finite metric space \((X,d)\) and we set \(|X|=n\), the number of triples in \(X\) as \({n\choose 3}=\ell\), and \(r={n\choose 4}\) the number of quadruples of points in \(X\). In somewhat an abuse of notation, we let \({X\choose 3}\) denote the set of all triples chosen from \(X\) (and, similarly, for all quadruples of points). Next, we recall the notion of hyperbolicity, which is defined via the Gromov product [4]. Given a metric space \((X,d)\), the _Gromov product_ of two points \(x,y\in X\) with respect to a base point

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Hyperbolicity measure** & Value & Distortion \\ \hline Gromovâ€™s \(\delta\)-hyperbolicity & \(\delta=\|\bm{\Delta}_{x}\|_{\infty}\) & \(\|d-d_{T}\|_{\infty}=O(\delta\log n)=O(\|\bm{\Delta}_{x}\|_{\infty}\log n)\) \\ Average hyperbolicity & \(\delta=\frac{1}{{n-1\choose 3}}\|\bm{\Delta}_{x}\|_{1}\) & \(\|d-d_{T}\|_{1}=O(\delta n^{3})=O(\|\bm{\Delta}_{x}\|_{1})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Connection between \(\delta\)-hyperbolicity and average hyperbolicity and how these quantities determine the distortion of the resulting tree metric.

\(w\in X\) is defined as

\[gp_{w}(x,y):=\frac{1}{2}\left(d(x,w)+d(y,w)-d(x,y)\right).\]

We use the definition of the Gromov product on two points with respect to a third to establish the _four point condition_ and define

\[fp_{w}(d;x,y,z):=\max_{\pi\,\text{perm}}[\min(gp_{w}(\pi x,\pi z),gp_{w}(\pi y, \pi z))-gp_{w}(\pi x,\pi y)]\]

where the maximum is taken over all permutations \(\pi\) of the labels of the four points. Since \(fp_{w}(d;x,y,z)=fp_{x}(d;y,z,w)=fp_{y}(d;x,z,w)=fp_{z}(d;x,y,w)\), we sometimes denote the four point condition as \(fp(d;x,y,z,w)\). Similarly, we define the _three point condition_ as

\[tp(d;x,y,z):=\max_{\pi\,\text{perm}}[d(\pi x,\pi z)-\max(d(\pi x,\pi y),d(\pi y,\pi z))]\]

which we use to define ultrametricity.

Following a standard definition of Gromov, a metric space \((X,d)\) is said to be \(\delta\)-hyperbolic with respect to the base point \(w\in X\), if for any \(x,y,z\in X\), the following holds:

\[gp_{w}(x,y)\geq\min(gp_{w}(y,z),gp_{w}(x,z))-\delta.\]

We denote \(\operatorname{Hyp}(d)=\delta\), the usual hyperbolicity constant (similarly, \(\operatorname{UM}(d)\), is the usual ultrametricity constant). We note that this measure of hyperbolicity is a worst case measure and, as such, it may give a distorted sense of the geometry of the space. A graph which consists of a tree and a single cycle, for instance, is quite different from a single cycle alone but with a worst case measure, we will not be able to distinguish between those two spaces.

In order to disambiguate different spaces, we define the _hyperbolicity_ vector as the \(\ell\)-dimensional vector of all four point conditions with respect to \(d\):

\[\boldsymbol{\Delta}_{w}(d)=[fp(d;x,y,z,w)]\text{ for all }x,y,z\in\binom{X}{3}.\]

Similarly, we define the _ultrametricity_ vector as the \(\ell\)-dimensional vector of all three point conditions with respect to \(d\):

\[\boldsymbol{\Delta}(d)=[tp(d;x,y,z)]\text{ for all }x,y,z\in\binom{X}{3}.\]

We use the hyperbolicity and ultrametricity vectors to express more refined geometric notions.

We define _\(p\)-average hyperbolicity_ and _\(p\)-average ultrametricity_.

\[\operatorname{AvgHyp}_{p}(d)=\left(\frac{1}{r}\sum_{x,y,z,w\in \binom{X}{4}}fp(d;x,y,z,w)^{p}\right)^{1/p}\text{ and }\] \[\operatorname{AvgUM}_{p}(d)=\left(\frac{1}{\ell}\sum_{x,y,z\in \binom{X}{3}}tp(d;x,y,z)^{p}\right)^{1/p}\]

If \(p=1\), then the notions are simply the _average_ (and we will call them so). Also, for clarity, note the usual hyperbolicity and ultrametricity constants \(\operatorname{Hyp}(d)\) and \(\operatorname{UM}(d)\) are the \(p=\infty\) case.

**Proposition 2.1**.: We have the simple relations:

1. \(\operatorname{Hyp}(d)=\max_{x\in X}\|\boldsymbol{\Delta}_{x}(d)\|_{\infty} \geq\|\boldsymbol{\Delta}_{x}(d)\|_{\infty}\) for any \(x\in X\).
2. \(\operatorname{UM}(d)=\|\boldsymbol{\Delta}(d)\|_{\infty}\).

In the discussion of hierarchical correlation clustering in Section 2.4 and in the analysis of our algorithms in Section 3, we construct multiple graphs using the points of \(X\) as vertices and derived edges. Of importance to our analysis is the following combinatorial object which consists of the set of bad triangles in a graph (i.e., those triples of vertices in the graph for which exactly two edges, rather than three, are in the edge set). Given a graph \(G=(V,E)\), denote \(B(G)\), the set of _bad triangles_ in \(G\), as

\[B(G):=\left\{(x,y,z)\in\binom{V}{3}|\,|\{(x,y),(y,z),(z,x)\}\cap E|=2\right\}.\]

### Problem formulation

First, we formulate the tree fitting problem. Given a finite, discrete metric space \((X,d)\) and the distance \(d(x_{i},x_{j})\) between any two points \(x_{i},x_{j}\in X\), find a tree metric \((T,d_{T})\) in which the points in \(X\) are among the nodes of the tree \(T\) and the tree distance \(d_{T}(x_{i},x_{j})\) is "close" to the original distance \(d(x_{i},x_{j})\).

While there are many choices to measure how close \(d_{T}\) and \(d\) are, in this paper, we focus on the \(\ell_{p}\) error; i.e., \(\|d_{T}-d\|_{p}\), for \(1\leq p\leq\infty\). This definition is itself a shorthand notation for the following. Order the pairs of points \((x_{i},x_{j}),i<j\) lexicographically and write \(d\) (overloading the symbol \(d\)) for the vector of pairwise distances \(d(x_{i},x_{j})\). Then, we seek a tree distance function \(d_{T}\) whose vector of pairwise tree distances is close in \(\ell_{p}\) norm to the original vector of distances. For example, if \(p=\infty\), we wish to bound the _maximum_ distortion between any pairs on \(X\). If \(p=1\), we wish to bound the total error over all pairs. Similarly, we define the ultrametric fitting problem.

We also introduce the _rooted_ tree fitting problem. Given a finite, discrete metric space \((X,d)\) and the distance \(d(x_{i},x_{j})\) between any two points \(x_{i},x_{j}\in X\), and a distinguished point \(w\in X\), find a tree metric \((T,d_{T})\) such that \(\|d-d_{T}\|_{p}\) is small and \(d_{T}(w,x)=d(w,x)\) for all \(x\in X\). Although the rooted tree fitting problem has more constraints, previous work (such as [7]) shows that by choosing the base point \(w\) appropriately, the optimal error of the rooted tree embedding is bounded by a constant times the optimal error of the tree embedding. Also, the rooted tree fitting problem is closely connected to the ultrametric fitting problem.

Putting these pieces together, we observe that while considerable attention has been paid to the \(\ell_{q}\) tree fitting problem for generic distances with only mild attention paid to the assumptions on the input distances. No one has considered _both_ restrictions on the distances and more sophisticated measures of distortion. We define the \(\ell_{p}/\ell_{q}\) tree (ultrametric) fitting problem as follows.

**Definition 2.2** (\(\ell_{p}/\ell_{q}\) tree (ultrametric) fitting problem).: Given \((X,d)\) with hyperbolicity vector \(\boldsymbol{\Delta}_{x}(d)\) and \(\operatorname{AvgHyp}_{q}(d)\) (ultrametricity vector \(\boldsymbol{\Delta}(d)\) and \(\operatorname{AvgUM}_{q}(d)\)), find the tree metric \((T,d_{T})\) (ultrametric \((X,d_{U})\)) with distortion

\[\|d-d_{T}\|_{p}\leq\operatorname{AvgHyp}_{q}(d)\cdot f(n)\text{ or }\|d-d_{U}\|_{p}\leq \operatorname{AvgUM}_{q}(d)\cdot f(n)\]

for a growth function \(f(n)\) that is as small as possible. (Indeed, \(f(n)\) might be simply a constant.)

### Previous results

Next, we detail Gromov's classic theorem on tree fitting, using our notation above.

**Theorem 2.3**.: _[_4_]_ _Given a \(\delta\)-hyperbolic metric space \((X,d)\) and a reference point \(x\in X\), there exists a tree structure \(T\) and its metric \(d_{T}\) such that_

1. \(T\) _is_ \(x\)_-restricted, i.e.,_ \(d(x,y)=d_{T}(x,y)\) _for all_ \(y\in X\)_._
2. \(\|d-d_{T}\|_{\infty}\leq 2\|\boldsymbol{\Delta}_{x}(d)\|_{\infty}\lceil\log_{2 }(n-2)\rceil\)_._

In other words, we can bound the _maximum distortion_ of tree metric in terms of the hyperbolicity constant \(\operatorname{Hyp}(d)=\delta\) and the size of our input space \(X\).

### (Hierarchical) Correlation clustering and ultrametric fitting

Several earlier works ([2], [12]) connected the correlation clustering problem to that of tree and ultrametric fitting and, in order to achieve our results, we do the same. In the correlation clustering problem, we are given a graph \(G=(V,E)\) whose edges are labeled "+" (similar) or "-" (different) and we seek a clustering of the vertices into two clusters so as to minimize the number of pairs incorrectly classified with respect to the input labeling. In other words, minimize the number of "-" edges within clusters plus the number of "+" edges between clusters. When the graph \(G\) is complete, correlation clustering is equivalent to the problem of finding an optimal ultrametric fit under the \(\ell_{1}\) norm when the input distances are restricted to the values of 1 and 2.

Hierarchical correlation clustering is a generalization of correlation clustering that is also implicitly connected to ultrametric and tree fitting (see [1], [2], [12], [13]). In this problem, we are given a set of non-negatiweweights and a set of edge sets. We seek a partition of vertices that is both hierarchical andminimizes the weighted sum of incorrectly classified pairs of vertices. It is a (weighted) combination of correlation clustering problems.

More precisely, given a graph \(G=(V,E)\) with \(k+1\) edge sets \(G_{t}=(V,E_{t})\), and \(k+1\) weights \(\delta_{t}\geq 0\) for \(0\leq t\leq k\), we seek a _hierarchical_ partition \(P_{t}\) that minimizes the \(\ell_{1}\) objective function, \(\sum\delta_{t}|E_{t}\Delta E(P_{t})|\). It is _hierarchical_ in that for each \(t\), \(P_{t}\) subdivides \(P_{t+1}\).

Chowdury, et al. [13] observed that the Single Linkage Hierarchical Clustering algorithm (SLHC) whose output can be modified to produce an ultrametric that is designed to fit a given metric satisfies a similar property to that of Gromov's tree fitting result. In this case, the distortion bound between the ultrametric and the input distances is a function of the ultrametricity of the metric space.

**Theorem 2.4**.: _Given \((X,d)\) and the output of SLHC in the form of an ultrametric \(d_{U}\), we have_

\[\|d-d_{U}\|_{\infty}\leq\|\mathbf{\Delta}(d)\|_{\infty}\lceil\log_{2}(n-1)\rceil.\]

### Reductions and equivalent bounds

Finally, we articulate precisely how the tree and ultrametric fitting problems are related through the following reductions. We note that the proof of this theorem uses known techniques from [2] and [1] although the specific results are novel. First, an ultrametric fitting algorithm yields a tree fitting algorithm.

**Theorem 2.5**.: _Given \(1\leq p<\infty\) and \(1\leq q\leq\infty\). Suppose we have an ultrametric fitting algorithm such that for any distance function \(d\) on \(X\) (with \(|X|=n\)), the output \(d_{U}\) satisfies_

\[\|d-d_{U}\|_{p}\leq\mathrm{AvgUM}_{q}(d)\cdot f(n)\text{ for some growth function }f(n).\]

_Then there exists a tree fitting algorithm (using the above) such that given an input \(d\) on \(X\) (with \(|X|=n\)), the output \(d_{T}\) satisfies_

\[\|d-d_{T}\|_{p}\leq 2\left(\frac{n-3}{n}\right)^{1/q}\mathrm{AvgHyp}_{q}(d) \cdot f(n)\text{ for some growth function }f(n).\]

Conversely, a tree fitting algorithm yields an ultrametric fitting algorithm. From which we conclude that both problems should have the same asymptotic bound, which justifies our problem formulation.

**Theorem 2.6**.: _Given \(1\leq p<\infty\) and \(1\leq q\leq\infty\). Suppose that we have a tree fitting algorithm such that for any distance function \(d\) on \(X\) (with \(|X|=n\)), the output \(d_{T}\) satisfies_

\[\|d-d_{T}\|_{p}\leq\mathrm{AvgHyp}_{q}(d)\cdot f(n)\text{ for some growth function }f(n).\]

_Then there exists an ultrametric fitting algorithm (using the above) such that given an input \(d\) on \(X\) (with \(|X|=n\)), the output \(d_{T}\) satisfies_

\[\|d-d_{U}\|_{p}\leq 3^{\frac{p-1}{p}}\left(\frac{1}{4}+2^{q-4}\right)^{1/q} \cdot\mathrm{AvgUM}_{q}(d)\cdot f(2n)\text{ for some growth function }f(n).\]

Proofs of both theorems can be found in the Appendix 6.16.2.

## 3 Tree and ultrametric fitting: Algorithm and analysis

In this section, we present an algorithm for the \(p=q=1\) ultrametric fitting problem. We also present the proof of upper bound and present an example that shows this bound is asymptotically tight (despite our empirical evidence to the contrary). Then, using our reduction from Section 2, we produce a tree fitting result.

### HCC Problem

As we detailed in Section 2, correlation clustering is connected with tree and ultrametric fitting and in this section, we present a hierarchical correlation clustering (HCC) algorithm which _bounds_ the number of disagreement edges by constant factor of number of bad triangles. We follow with a proposition that shows the connection between bad triangle objectives and the \(\ell_{1}\) ultrametricity vector.

**Definition 3.1** (HCC with triangle objectives).: Given a vertex set \(V\) and \(k+1\) edge sets, set \(G_{t}=(V,E_{t})\) for \(0\leq t\leq k\). The edge sets are hierarchical so that \(E_{t}\subseteq E_{t+1}\) for each \(t\). We seek a _hierarchical_ partition \(P_{t}\) so that for each \(t\), \(P_{t}\) subdivides \(P_{t+1}\) and the number of disagreement edges \(|E_{t}\Delta E(P_{t})|\) is bounded by \(C\cdot|B(G_{t})|\) (where \(B\) denotes the set of bad triangles) for some constant \(C>0\).

Note that this problem does not include the weight sequence \(\{\delta_{t}\}\), as the desired output will also guarantee an upper bound on \(\sum\delta_{t}|E_{t}\Delta E(P_{t})|\), the usual \(\ell_{1}\) objective.

This proposition relates the \(\ell_{1}\) vector norm of \(\mathbf{\Delta}(d)\), the average ultrametricity of the distances, and the bad triangles in the derived graph. This result is why we adapt the hierarchical correlation clustering problem to include triangle objectives.

**Proposition 3.2**.: Given distance function \(d\) on \(\binom{X}{2}\) (with \(|X|=n\)) and \(s>0\), consider the \(s\)-neighbor graph \(G_{s}=(X,E_{s})\) where \(E_{s}\) denotes \(\{(x,y)\in\binom{X}{2}|d(x,y)\leq s\}\). Then we have

\[\|\mathbf{\Delta}(d)\|_{1}=\ell\cdot\mathrm{AvgUM}(d)=\int_{0}^{\infty}|B(G_{s })|ds.\]

### Main results

Our main contribution is that the HCCTriangle algorithm detailed in Section 3.3 solves our modified HCC problem and its output partition behaves "reasonably" on every level. From this partition, we can construct a good ultrametric fit which we then leverage for a good (rooted) tree fit (using the reduction from Section 2.

**Theorem 3.3**.: HCCTriangle _outputs a hierarchical partition \(P_{t}\) where \(|E_{t}\Delta E(P_{t})|\leq 4\cdot|B(G_{t})|\) holds for every \(0\leq t\leq\binom{n}{2}=k\). Furthermore, the algorithm runs in time \(O(n^{2})\)._

By Proposition 3.2 and Theorem 3.3, we can find an ultrametric fit using the HCCTriangle subroutine to cluster our points. This algorithm we refer to as HCCUltraFit. Using Theorem 3.3, we have following \(\ell_{1}\) bound.

**Theorem 3.4**.: _Given \(d\), HCCUltraFit outputs an ultrametric \(d_{U}\) with \(\|d-d_{U}\|_{1}\leq 4\|\mathbf{\Delta}\|_{1}=4\ell\cdot\mathrm{AvgUM}_{1}(d)\). The algorithm runs in time \(O(n^{2}\log n)\)._

In other words, HCCUltraFit solves the HCC Problem 3.1 with constant \(C=4\). The following proof shows why adapting HCCTriangle as a subroutine is successful.

**Proof of Theorem 3.4 from Theorem 3.3:** Suppose the outputs of HCCTriangle and HCCUltraFit are \(\{P_{t}\}\) and \(d_{U}\), respectively. Denote \(d_{i}:=d(e_{i})\) (\(d_{0}=0,d_{k+1}=\infty\)) and, for any \(s>0\), set

\[G_{s}=(X,E_{t}),\ P_{s}=P_{t}\ \text{for}\ d_{t}\leq s<d_{t+1}.\]

Then, for any \(x,y\in\binom{X}{2}\), we see that

\[(x,y)\in E_{s}\Delta E(P_{s})\quad\iff\quad d(x,y)\leq s<d_{U}(x,y)\ \text{or}\ \ d_{U}(x,y)\leq s<d(x,y).\]

Again, we use the integral notion from Proposition 3.2. Every edge \((x,y)\) will contribute \(|E_{s}\Delta E(P_{s})|\) with amount exactly \(|d_{U}(x,y)-d(x,y)|\). Then, by Theorem 3.3,

\[\|d-d_{U}\|_{1} =\int_{0}^{\infty}|E_{s}\Delta E(P_{s})|ds\] \[\leq 4\int_{0}^{\infty}|B(G_{s})|ds=4\|\mathbf{\Delta}(d)\|_{1},\]

as desired. Assuming that HCCTriangle runs in time \(O(n^{2})\), HCCUltraFit runs in time \(O(n^{2}\log n)\) as the initial step of sorting over all pairs is needed. Thus ends the proof.

By the reduction argument we discussed in Section 2, we can put all of these pieces together to conclude the following:

**Theorem 3.5**.: _Given \((X,d)\), we can find two tree fits with the following guarantees:_

* _Given a base point point_ \(x\in X\)_, HCCRootedTreeFit outputs a tree fit_ \(d_{T}\) _with_ \(\|d-d_{T}\|_{1}\leq 8\|\mathbf{\Delta}_{x}(d)\|_{1}\)_. The algorithm runs in_ \(O(n^{2}\log n)\)* _There exists_ \(x\in X\) _where_ \(\|\bm{\Delta}_{x}(d)\|_{1}\leq\binom{n-1}{3}\operatorname{AvgHyp}_{1}(d)\)_. Therefore, given_ \((X,d)\)_, one can find a (rooted) tree metric_ \(d_{T}\) _with_ \(\|d-d_{T}\|_{1}\leq 8\binom{n-1}{3}\operatorname{AvgHyp}_{1}(d)\) _in time_ \(O(n^{3}\log n)\)_._

### Algorithm and analysis

``` functionisHighlyConnected Input: vertex set \(X,Y\) and edge set \(E\)  For \(x\in X\):  If \(|\{y\in Y|(x,y)\in E\}|<\frac{|Y|}{2}\):  return False  For \(y\in Y\):  If \(|\{x\in X|(x,y)\in E\}|<\frac{|X|}{2}\):  return False return True ```

**Algorithm 1**isHighlyConnected: tests if two clusters are highly connected.

``` functionHCCTriangle Input: \(V=\{v_{1},\cdots,v_{n}\}\) and an ordering of all pairs \(\{e_{1},e_{2},\cdots,e_{\binom{n}{2}}\}\) on \(V\) so that \(E_{t}=\{e_{1},e_{2},\cdots,e_{t}\}\). Desired Output: _hierarchical_ partition \(\{P_{t}\}\) for \(0\leq t\leq\binom{n}{2}=k\) so that \(|E_{t}\Delta E(P_{t})|\leq 4\cdot|B(G_{t})|\) holds for every \(t\). Init: \(\mathcal{P}=\{\{v_{1}\},\{v_{2}\},\cdots,\{v_{n}\}\}\) \(P_{0}\leftarrow\mathcal{P}\)  For \(t\in\{1,2,\cdots,\binom{n}{2}\}\):  Take \(e_{t}=(x,y)\) with \(x\in C_{x}\) and \(y\in C_{y}\) (\(C_{x},C_{y}\in\mathcal{P}\))  If \(C_{x}\neq C_{y}\):  If \(\textsc{HighlyConnected}(C_{x},C_{y},E_{t})\) is true:  add \(C=C_{x}\cup C_{y}\) and remove \(C_{x},C_{y}\) in \(\mathcal{P}\). \(P_{t}\leftarrow\mathcal{P}\) return\(\{P_{t}\}\) ```

**Algorithm 2**HCCTriangle: Find HCC which respects triangle objectives

In the rest of this subsection, we provide a sketch of the proof that HCCTriangle provides the desired correlation clustering (the detailed proof can be found in Appendix 6.4). We denote the output of HCCTriangle by \(\{P_{t}\}\) and also assume \(E=E_{t}\) and \(P_{t}=\{C_{1},C_{2},\cdots,C_{k}\}\). The algorithm HCCTriangle agglomerates two clusters if they are highly connected, adds the cluster to the partition, and iterates. The key to the ordering of the edges input to HCCTriangle is that they are ordered by increasing distance so that the number of edges that are in "disagreement" in

Figure 1: Illustration of highly connectedness condition

the correlation clustering is upper bounded by the number of edges whose distances "violate" a triangle relationship. The proof proceeds in a bottom up fashion. (It is clear that the output partition is naturally hierarchical.) We count the number of bad triangles "within" and "between" clusters, which are lower bounded by the number of disagreement edges "within" and "between" clusters, respectively. The proof uses several combinatorial properties which follow from the highly connected condition. This is the key point of our proof.

**From an ultrametricity fit to a rooted tree fit:** Following a procedure from Cohen-Addad, et al. [1], we can also obtain a tree fit with the \(\ell_{1}/\ell_{1}\) objective. Note that the algorithm HCCRootedTreeFit takes the generic reduction method from ultrametric fitting algorithm but we have instantiated it with HCCUltraFit. For a self-contained proof, see the Appendix 6.3.

**Proof of Theorem 3.5:** This proof can be easily shown simply by applying Theorem 2.5 with \(p=q=1\) and \(f(n)=4\binom{n}{3}\).

```
1:procedureHCCRootedTreeFit
2:Input: distance function \(d\) on \(\binom{X}{2}\) and base point \(w\in X\)
3:Output: (rooted) tree metric \(d_{T}\) which fits \(d\) and \(d(x,w)=d_{T}(x,w)\) for all \(x\in X\)
4:\(M\leftarrow\max_{x\in X}d(x,w)\)
5:\(c_{w}(x,y)\gets 2M-d(x,w)-d(y,w)\) for all \(x,y\in\binom{X}{2}\)
6:\(d_{U}\leftarrow\textsc{HCCUltraFit}(d+c_{w})\)
7:\(d_{T}\gets d_{U}-c_{w}\)
8:return\(d_{T}\) ```

**Algorithm 4** Find a tree fitting given an ultrametric fitting procedure

**Running Time** Although isHighlyConnected is seemingly expensive, there is a way to implement HCCTriangle so that all procedures run in \(O(n^{2})\) time. Thus, HCCUltraFit can be implemented so as to run in \(O(n^{2}\log n)\) time. The detailed algorithm can be found in Appendix 6.5.

**Asymptotic Tightness** Consider \((d,X)\) with \(X=\{x_{1},\cdots,x_{n}\}\) and \(d(x_{1},x_{2})=d(x_{1},x_{3})=1\) and 2 otherwise. Then we see that \(tp(d;x_{1},x_{2},x_{3})=1\) and 0 otherwise, so that \(\|\Delta\|_{p}=1\). One can easily check that for any ultrametric \(d_{U}\), \(|\epsilon(x_{1},x_{2})|^{p}+|\epsilon(x_{1},x_{3})|^{p}+|\epsilon(x_{2},x_{3} )|^{p}\geq 2^{1-p}\) for \(\epsilon:=d_{U}-d\). When \(p=1\), \(\|d-d_{U}\|_{1}\geq\|\boldsymbol{\Delta}(d)\|_{1}=\binom{n}{3}\operatorname{ AvgUM}(d)\) holds for any ultrametric \(d_{U}\). While HCCUltraFit guarantees \(\|d-d_{U}\|_{1}\leq 4\|\boldsymbol{\Delta}(d)\|_{1}=4\binom{n}{3}\operatorname{ AvgUM}(d)\); this shows that our theoretical bound is asymptotically tight.

Examples demonstrating _how_ HCCUltraFit works and related discussion can be found in Appendix 6.7.1.

## 4 Experiments

In this section, we run HCCRootedTreeFit on several different type of data sets, those that are standard for geometric graph neural nets and those that are synthetic. We also compare our results with other known algorithms. We conclude that HCCRootedTreeFit (HCC) is optimal when the data sets are close to tree-like and when we measure with respect to distortion in the \(\ell_{1}\) sense and running time. It is, however, suboptimal in terms of the \(\ell_{\infty}\) measure of distortion (as to be expected). We also conclude that purportedly hierarchical data sets do not, in fact, embed into trees with low distortion, suggesting that geometric graph neural nets should be configured with different geometric considerations. Appendix 6.9 contains further details regarding the experiments.

### Common data sets

We used common unweighted graph data sets which are known to be hyperbolic or close to tree-like and often used in graph neural nets, especially those with geometric considerations. The data sets we used are C-elegan, CS Phd from [14], and CORA, Airport from [15]. (For those which contain multiple components, we chose the largest connected component.) Given an unweighted graph, we computed its shortest-path distance matrix and used that input to obtain a tree metric. We compared these results with the following other tree fitting algorithms TreeRep (TR) [9], NeighborJoin (NJ) [8], and the classical Gromov algorithm. As TreeRep is a randomized algorithm and HCCRootedTreeFit and Gromov's algorithm depends on the choice of a pivot vertex, we run all of these algorithms 100 times and report the average error with standard deviation. All edges with negative weights have been modified with weight 0, as TreeRep and NeighborJoin both occasionally produce edges with negative weights. Recall, both TreeRep and NeighborJoin enjoy _no_ theoretical guarantees on distortion.

First, we examine the results in Table 2. We note that although the guaranteed bound (of average distortion), \(8{n-1\choose 3}\operatorname{AvgHyp}(d)/{n\choose 2}\) is asymptotically tight even in worst case analysis, this bound is quite loose in practice; most fitting algorithms perform much better than that. We also see that the \(\ell_{1}\) error of HCCRootedTreeFit is comparable to that of TreeRep, while NeighborJoin performs much better than those. It tends to perform better when the graph data set is known to be _more_ hyperbolic (or tree-like), _despite no theoretical guarantees_. It is, however, quite slow.

Also, we note from Table 3 that Gromov's algorithm, which solves our \(\ell_{\infty}/\ell_{\infty}\) hyperbolicity problem tends to return better output in terms of \(\ell_{\infty}\) error. On the other hand, its result on \(\ell_{1}\) error is not as good as the other algorithms. In contrast, our HCCRootedTreeFit performs better on the \(\ell_{1}\) objective, which suggests that our approach to this problem is on target.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multicolumn{2}{c}{**Data set**} & C-elegan & CS Phd & CORA & Airport \\ \hline \(O(n^{3}\log n)\) & HCC & 0.648\(\pm\)0.013 & 3.114\(\pm\)0.029 & 18.125\(\pm\)0.330 & 28.821\(\pm\)0.345 \\ \(O(n^{3})\) & Gromov & 0.055\(\pm\)0.005 & 0.296\(\pm\)0.004 & 2.063\(\pm\)0.033 & 3.251\(\pm\)0.033 \\ \(O(n^{3})\) & TR & 0.068\(\pm\)0.009 & 0.223\(\pm\)0.046 & 0.610\(\pm\)0.009 & 0.764\(\pm\)0.151 \\ \(O(n^{3})\) & NJ & 0.336 & 4.659 & 268.45 & 804.67 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Running Time(s). For NJ, we implemented the naive algorithm, which is \(O(n^{3})\). We note that TreeRep produces a tree and not a set of distances; its running times excluded a step which computes the distance matrix, which takes \(O(n^{2})\) time.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Data set** & C-elegan & CS Phd & CORA & Airport \\ \hline HCC & 4.3\(\pm\)0.64 & 23.37\(\pm\)3.20 & 19.30\(\pm\)1.11 & 7.63\(\pm\)0.54 \\ Gromov & 3.32\(\pm\)0.47 & **13.24\(\pm\)**0.67 & **9.23\(\pm\)**0.53 & **4.04\(\pm\)**0.20 \\ TR & 5.90\(\pm\)0.72 & 21.01\(\pm\)3.34 & 16.86\(\pm\)2.11 & 10.00\(\pm\)1.02 \\ NJ & **2.97** & 16.81 & 13.42 & 4.18 \\ \hline \hline \end{tabular}
\end{table}
Table 3: \(\ell_{\infty}\) error (i.e., max distortion)In analyzing the running times in Table 4, we notice that HCCRootedTreeFit runs in truly \(O(n^{2}\log n)\) time. Also, its dominant part is the subroutine HCCTriangle, which runs in \(O(n^{2})\).

### Synthetic data sets

In order to analyze the performance of our algorithm in a more thorough and rigorous fashion, we generate random synthetic distances with low hyperbolicity. More precisely, we construct a synthetic weighted graph from fixed balanced trees. We use \(\mathrm{BT}(r,h)\) to indicate a balanced tree with branching factor \(r\) and height \(h\) and Disease from [6], an unweighted tree data set. For each tree, we add edges randomly, until we reach 500 additional edges. Each added edge is given a distance designed empirically to keep the \(\delta\)-hyperbolicity bounded by a value of 0.2.

Then we measured the \(\ell_{1}\) error of each tree fitting algorithm (and averaged over 50 trials). Note that all rooted tree fitting algorithms use a root node (for balanced trees, we used the apex; for Disease, we used node 0).

For these experiments, we see quite different results in Table 5. All of these data sets are truly _tree-like_. Clearly, NeighborJoin performs considerably worse on these data than on the common data sets above, especially when the input comes from a tree with _high branching factor_ (note that the branching factor of Disease is recorded as 6.224, which is also high). We also note that Gromov's method behaves much worse than all the other algorithms. This is possibly because Gromov's method is known to produce a _non-stretching_ tree fit, while it is a better idea to _stretch_ the metric in this case. The theoretical bound is still quite loose, but not as much as with the common data sets.

## 5 Discussion

All of our experiments show that it is critical to quantify how "tree-like" a data set is in order to understand how well different tree fitting algorithms will perform on that data set. In other words, we cannot simply assume that a data set is generic when fitting a tree to it. Furthermore, we develop both a measure of how tree-like a data set is and an algorithm HCCRootedTreeFit that leverages this behavior so as to minimize the appropriate distortion of this fit. The performance of HCCRootedTreeFit and other standard tree fitting algorithms shows that commonly used data sets (especially in geometric graph neural nets) are quite far from tree-like and are not well-represented by trees, especially when compared with synthetic data sets. This suggests that we need considerably more refined geometric notions for learning tasks with these data sets.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Initial Tree** & \(\mathrm{BT}(8,3)\) & \(\mathrm{BT}(5,4)\) & \(\mathrm{BT}(3,5)\) & \(\mathrm{BT}(2,8)\) & Disease \\ \hline \(n\) & 585 & 776 & 364 & 511 & 2665 \\ \(\|\bm{\Delta}_{r}\|_{1}/\binom{n-1}{3}\) & 0.01887\(\pm\)0.00378 & 0.01876\(\pm\)0.00375 & 0.00150\(\pm\)0.0036 & 0.00098\(\pm\)0.0020 & 0.00013\(\pm\)0.00310 \\ \hline HCC & **0.00443\(\pm\)**0.00098 & 0.01538\(\pm\)0.00733 & **0.04153\(\pm\)**0.01111 & 0.07426\(\pm\)0.02027 & **0.00061\(\pm\)**0.00588 \\ Gromov & 0.18225\(\pm\)0.00237 & 0.44015\(\pm\)0.00248 & 0.17085\(\pm\)0.00975 & 0.16898\(\pm\)0.00175 & 0.18977\(\pm\)0.00196 \\ TR & 0.01360\(\pm\)0.00366 & **0.01103\(\pm\)**0.00365 & 0.06080\(\pm\)0.0874 & 0.0959\(\pm\)0.01857 & 0.00081\(\pm\)0.00059 \\ NJ & 0.03180\(\pm\)0.00767 & 0.06092\(\pm\)0.01951 & 0.04309\(\pm\)0.00321 & **0.04360\(\pm\)**0.00648 & 0.00601\(\pm\)0.00281 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Average \(\ell_{1}\) error when \(n_{c}=500\)

## References

* [1] V. Cohen-Addad, D. Das, E. Kipouridis, N. Parotsidis, and M. Thorup, "Fitting distances by tree metrics minimizing the total error within a constant factor," in _2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)_, IEEE, 2022, pp. 468-479.
* [2] N. Ailon and M. Charikar, "Fitting tree metrics: Hierarchical clustering and phylogeny," in _46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05)_, IEEE, 2005, pp. 73-82.
* [3] M. Farach, S. Kannan, and T. Warnow, "A robust model for finding optimal evolutionary trees," _Algorithmica_, vol. 13, no. 1, pp. 155-179, 1995.
* [4] M. Gromov, "Hyperbolic groups," in _Essays in group theory_, Springer, 1987, pp. 75-263.
* [5] L. L. Cavalli-Sforza and A. W. Edwards, "Phylogenetic analysis. models and estimation procedures," _American journal of human genetics_, vol. 19, no. 3 Pt 1, p. 233, 1967.
* [6] I. Chami, Z. Ying, C. Re, and J. Leskovec, "Hyperbolic graph convolutional neural networks," _Advances in neural information processing systems_, vol. 32, 2019.
* [7] R. Agarwala, V. Bafna, M. Farach, M. Paterson, and M. Thorup, "On the approximability of numerical taxonomy (fitting distances by tree metrics)," _SIAM Journal on Computing_, vol. 28, no. 3, pp. 1073-1085, 1998.
* [8] N. Saitou and M. Nei, "The neighbor-joining method: A new method for reconstructing phylogenetic trees.," _Molecular biology and evolution_, vol. 4, no. 4, pp. 406-425, 1987.
* [9] R. Sonthalia and A. C. Gilbert, "Tree! i am no tree! i am a low dimensional hyperbolic embedding," _arXiv preprint arXiv:2005.03847_, 2020.
* [10] S. Chatterjee and L. Sloman, "Average gromov hyperbolicity and the parisi ansatz," _Advances in Mathematics_, vol. 376, p. 107 417, 2021.
* [11] E. Ghys and P. De La Harpe, "Espaces metriques hyperboliques," in _Sur les groupes hyperboliques d'apres Mikhael Gromov_, Springer, 1990, pp. 27-45.
* [12] B. Harb, S. Kannan, and A. McGregor, "Approximating the best-fit tree under l p norms," in _Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques: 8th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems, APPROX 2005 and 9th International Workshop on Randomization and Computation, RANDOM 2005, Berkeley, CA, USA, August 22-24, 2005. Proceedings_, Springer, 2005, pp. 123-133.
* [13] S. Chowdhury, F. Memoli, and Z. T. Smith, "Improved error bounds for tree representations of metric spaces," _Advances in Neural Information Processing Systems_, vol. 29, 2016.
* [14] R. A. Rossi and N. K. Ahmed, "The network data repository with interactive graph analytics and visualization," in _AAAI_, 2015. [Online]. Available: https://networkrepository.com.
* [15] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad, "Collective classification in network data," _AI magazine_, vol. 29, no. 3, pp. 93-93, 2008.
* [16] W. H. Day, "Computational complexity of inferring phylogenies from dissimilarity matrices," _Bulletin of mathematical biology_, vol. 49, no. 4, pp. 461-467, 1987.
* [17] B. H. Bowditch, "A course on geometric group theory.," 2006.
* [18] H. Samet, "The quadtree and related hierarchical data structures," _ACM Computing Surveys (CSUR)_, vol. 16, no. 2, pp. 187-260, 1984.
* [19] M. Elkin, Y. Emek, D. A. Spielman, and S.-H. Teng, "Lower-stretch spanning trees," _SIAM Journal on Computing_, vol. 38, no. 2, pp. 608-628, 2008.
* [20] N. Ailon, M. Charikar, and A. Newman, "Aggregating inconsistent information: Ranking and clustering," _Journal of the ACM (JACM)_, vol. 55, no. 5, pp. 1-27, 2008.
* [21] V. Chepoi, F. Dragan, B. Estellon, M. Habib, and Y. Vaxes, "Diameters, centers, and approximating trees of delta-hyperbolic geodesic spaces and graphs," in _Proceedings of the twenty-fourth annual symposium on Computational geometry_, 2008, pp. 59-68.
* [22] W. Chen, W. Fang, G. Hu, and M. W. Mahoney, "On the hyperbolicity of small-world and treelike random graphs," _Internet Mathematics_, vol. 9, no. 4, pp. 434-491, 2013.
* [23] D. Coudert, A. Nusser, and L. Viennot, "Computing graph hyperbolicity using dominating sets*," in _2022 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX)_, SIAM, 2022, pp. 78-90.

* [24] R. Sarkar, "Low distortion delaunay embedding of trees in hyperbolic plane," in _International Symposium on Graph Drawing_, Springer, 2011, pp. 355-366.
* [25] F. Sala, C. De Sa, A. Gu, and C. Re, "Representation tradeoffs for hyperbolic embeddings," in _International conference on machine learning_, PMLR, 2018, pp. 4460-4469.
* [26] I. Chami, A. Gu, V. Chatziafratis, and C. Re, "From trees to continuous embeddings and back: Hyperbolic hierarchical clustering," _Advances in Neural Information Processing Systems_, vol. 33, pp. 15 065-15 076, 2020.
* [27] N. Monath, M. Zaheer, D. Silva, A. McCallum, and A. Ahmed, "Gradient-based hierarchical clustering using continuous representations of trees in hyperbolic space," in _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, 2019, pp. 714-722.
* [28] M. Nickel and D. Kiela, "Poincare embeddings for learning hierarchical representations," _Advances in neural information processing systems_, vol. 30, 2017.
* [29] N. Linial, E. London, and Y. Rabinovich, "The geometry of graphs and some of its algorithmic applications," _Combinatorica_, vol. 15, no. 2, pp. 215-245, 1995.
* [30] M. Nickel and D. Kiela, "Learning continuous hierarchies in the lorentz model of hyperbolic geometry," in _International Conference on Machine Learning_, PMLR, 2018, pp. 3779-3788.
* [31] H. Fournier, A. Ismail, and A. Vigneron, "Computing the gromov hyperbolicity of a discrete metric space," _Information Processing Letters_, vol. 115, no. 6-8, pp. 576-579, 2015.
* [32] P. K. Agarwal, K. Fox, A. Nath, A. Sidiropoulos, and Y. Wang, "Computing the gromov-hausdorff distance for metric trees," _ACM Transactions on Algorithms (TALG)_, vol. 14, no. 2, pp. 1-20, 2018.
* [33] B. Nica and J. Spakula, "Strong hyperbolicity," _Groups, Geometry, and Dynamics_, vol. 10, no. 3, pp. 951-964, 2016.
* [34] T. Das, D. Simmons, and M. Urbanski, _Geometry and dynamics in Gromov hyperbolic metric spaces_. American Mathematical Soc., 2017, vol. 218.
* [35] I. Abraham, M. Balakrishnan, F. Kuhn, D. Malkhi, V. Ramasubramanian, and K. Talwar, "Reconstructing approximate tree metrics," in _Proceedings of the twenty-sixth annual ACM symposium on Principles of distributed computing_, 2007, pp. 43-52.

Appendix

### Proof of Theorem 2.5

We will follow the details from [7] (and [1] to address minor issues). This is, in fact, the generic reduction method from ultrametric fitting.

```
1:procedureUltraFit
2:Input: distance function \(d\)
3:Output: ultrametric \(d_{U}\) which fits \(d\)
4:procedureRootedTreeFit
5:Input: distance function \(d\) on \(\binom{X}{2}\) and base point \(w\in X\)
6:Output: (rooted) tree metric \(d_{T}\) which fits \(d\) and \(d(w,x)=d_{T}(w,x)\) for all \(x\in X\)
7: Define \(m=\max_{x\in X}d(w,x)\), \(c_{w}(x,y)=2m-d(w,x)-d(w,y)\), and \(\beta_{x}=2(m-d(w,x))(x\in X)\)
8:\(d_{U^{\prime}}=\textsc{UltraFit}(d+c_{w})\)
9: Restrict \(d_{U}(x,y)=\min(\max(\beta_{x},\beta_{y},d_{U^{\prime}}(x,y)),2m)\)
10:\(d_{T}=d_{U}-c_{w}\)
11:return\(d_{T}\) ```

**Algorithm 5** Find a tree fitting given an ultrametric fitting procedure

**Claim:** For any \(x,y\in X\), \(|d_{U}(x,y)-(d+c_{w})(x,y)|\leq|d_{U^{\prime}}(x,y)-(d+c_{w})(x,y)|\) holds. In other words, the restriction will reduce the error.

**Proof:** It is enough to check the two cases when \(d_{U}\) differs.

1. \(\max(\beta_{x},\beta_{y})=d_{U}(x,y)>d_{U^{\prime}}(x,y)\): without loss of generality, we assume that \(d_{U}(x,y)=\beta_{x}\geq\beta_{y}\). We have \((d+c_{w})(x,y)=d(x,y)+c_{w}(x,y)\geq(d(w,y)-d(w,x))+(2m-d(w,x)-d(w,y))=2m-2d(w,x)=\beta_{x}\), which shows \((d+c_{w})(x,y)\geq d_{U}(x,y)>d_{U^{\prime}}(x,y)\). Therefore the claim holds.
2. \(2m=d_{U}(x,y)<d_{U^{\prime}}(x,y)\): since \((d+c_{w})(x,y)=2m-2gp_{w}(x,y)\leq 2m\), we have \((d+c_{w})(x,y)\leq d_{U}(x,y)<d_{U^{\prime}}(x,y)\). Again, the claim holds.

This completes the proof.

**Claim:** The restriction \(d_{U}\) is also an ultrametric. 

**Proof:** We need to prove \(d_{U}(x,y)\leq\max(d_{U}(x,z),d_{U}(y,z))\) for all \(x,y,z\in X\). As \(U^{\prime}\) is ultrametric by assumption, it is enough to check only if

\[d_{U}(x,y)>d_{U^{\prime}}(x,y)\text{ or }\max(d_{U}(x,z),d_{U}(y,z))>\max(d_{U ^{\prime}}(x,z),d_{U^{\prime}}(y,z))\text{ holds.}\]

1. \(d_{U}(x,y)>d_{U^{\prime}}(x,y)\): we have \(\max(\beta_{x},\beta_{y})=d_{U}(x,y)>d_{U^{\prime}}(x,y)\). Without loss of generality, we assume that \(d_{U}(x,y)=\beta_{x}\geq\beta_{y}\). Then we have \(d_{U}(x,z)\geq\max(\beta_{x},\beta_{z})\geq\beta_{x}=d_{U}(x,y)\), which shows \(d_{U}(x,y)\leq\max(d_{U}(x,z),d_{U}(y,z))\).
2. \(\max(d_{U}(x,z),d_{U}(y,z))<\max(d_{U^{\prime}}(x,z),d_{U^{\prime}}(y,z))\): we have \(d_{U}(x,z)<d_{U^{\prime}}(x,z)\) or \(d_{U}(y,z)<d_{U^{\prime}}(y,z)\) holds. In either case, we have the value is clipped by the maximum value \(2m\) so that \(d_{U}(x,y)\leq\max(d_{U}(x,z),d_{U}(y,z))=2m\).

Therefore, we conclude that the stronger triangle inequality still holds, which completes the proof.

**Claim:** By the restriction, \(d_{T}\) satisfies tree metric.

**Proof:** First, we need to verify that \(d_{T}\) is a metric. First, we have

\[d_{T}(x,y)=d_{U}(x,y)-c_{w}(x,y)\geq\max(\beta_{x},\beta_{y})-c_{ w}(x,y) =\max(\beta_{x},\beta_{y})-\frac{1}{2}(\beta_{x}+\beta_{y})\] \[=\frac{|\beta_{x}-\beta_{y}|}{2}=|d(w,x)-d(w,y)|\geq 0,\]so that \(d_{T}\) is non-negative. Next, we will prove the triangle inequality: \(d_{T}(x,z)\leq d_{T}(x,y)+d_{T}(y,z)\). Without loss of generality, we assume that \(d_{U}(x,y)\geq d_{U}(y,z)\). Then we have

\[d_{T}(x,z)=d_{U}(x,z)-c_{w}(x,z) \leq\max(d_{U}(x,y),d_{U}(y,z))-c_{w}(x,z)\] \[=d_{U}(x,y)-c_{w}(x,y)+c_{w}(x,y)-c_{w}(x,z)\] \[=d_{T}(x,y)+(d(w,z)-d(w,y))\] \[\leq d_{T}(x,y)+|d(w,z)-d(w,y)|\leq d_{T}(x,y)+d_{T}(y,z).\]

Therefore, \(d_{T}\) is (non-negative) metric. To show it is a tree metric, we examine the four point condition of \(d_{T}\). For any \(x,y,z,t\in X\),

\[d_{T}(x,y)+d_{T}(z,t) =(d_{U}(x,y)-c_{w}(x,y))+(d_{U}(z,t)-c_{w}(z,t))\] \[=(d_{U}(x,y)+d_{U}(z,t))-(c_{w}(x,t)+c_{w}(z,t))\] \[=(d_{U}(x,y)+d_{U}(z,t))-(4m-d(w,x)-d(w,y)-d(w,z)-d(w,t)),\]

so that any pair sum of \(d_{T}\) differs from \(d_{U}\) by \(4m-d(w,x)-d(w,y)-d(w,z)-d(w,t)\). As \(d_{U}\) is ultrametric and thus 0-hyperbolic, \(d_{T}\) is also 0-hyperbolic, as desired. Thus, \(d_{T}\) is a tree metric. 

Finally, we prove that \(\ell_{p}\) error of \(d_{T}\) is bounded as desired. This can be done by

\[\|d-d_{T}\|_{p}=\|(d+c_{w})-(d_{T}+c_{w})\|_{p} =\|d_{U}-(d+c_{w})\|_{p}\] \[\leq\|d_{U^{\prime}}-(d+c_{w})\|_{p}\leq\operatorname{AvgUM}_{q}( d+c_{w})\cdot f(n),\]

by assumption. For \(1\leq q<\infty\), we have

\[\frac{1}{n}\sum_{w\in X}(\operatorname{AvgUM}_{q}(d+c_{w}))^{q} =\frac{1}{n}\sum_{w\in X}\frac{1}{{n\choose 3}}\sum_{x,y,z\in \binom{X\setminus\{w\}}{3}}tp(d+c_{w};x,y,z)^{q}\] \[=\frac{n-3}{n^{2}}\sum_{w\in X}\frac{1}{{n-1\choose 3}}\sum_{x,y,z \in\binom{X\setminus\{w\}}{3}}2^{q}fp_{w}(d;x,y,z)^{q}\] \[=2^{q}\frac{n-3}{n}\cdot\frac{1}{{n\choose 4}}\sum_{x,y,z,w\in \binom{X}{4}}fp^{q}(d;x,y,z,w)\] \[=2^{q}\frac{n-3}{n}(\operatorname{AvgHyp}_{q}(d))^{q}.\]

Therefore, there exists \(w\in X\) so that \(\operatorname{AvgUM}_{q}(d+c_{w})\leq 2\left(\frac{n-3}{n}\right)^{1/q} \operatorname{AvgHyp}_{q}(d)\). The \(q=\infty\) case can be separately checked.

Hence, there exists \(w\in X\) where such reduction yields a tree fitting with \(\ell_{p}\) error bounded by \(2\left(\frac{n-3}{n}\right)^{1/q}\operatorname{AvgHyp}_{q}(d)\cdot f(n)\), as desired.

Note that when we use HCCUltraFit, then \(\max(\beta_{x},\beta_{y})\leq d_{U^{\prime}}(x,y)\leq 2m\) always hold so that the clipping is not necessary.

### Proof of Theorem 2.6

Our reduction is based on the technique from [16] and Section 9 of [1]. Given \(d\) on \(X\), we will construct a distance \(d^{\prime}\) on \(Z=X\cup Y\) such that \(|X|=|Y|=n\) and

\[d^{\prime}(z,w)=\begin{cases}d(z,w)&\text{if }z,w\in X\\ M&\text{if }z\in X,w\in Y\text{ or }z\in Y,w\in X\text{ \ for all }z\neq w\in Z,\\ c&\text{if }z,w\in Y\end{cases}\]

for \(M\) large enough and \(c\) small enough. Then first we have

**Claim:** If \(1\leq q<\infty\), then \(\operatorname{AvgHyp}_{q}(d^{\prime})\leq\left(\frac{1}{4}+2^{q-4}\right)^{1 /q}\cdot\operatorname{AvgUM}_{q}(d)\). For \(q=\infty\), we have \(\operatorname{Hyp}(d^{\prime})\leq 2\operatorname{UM}(d)\).

**Proof:** It can be checked that if at least two of \(x,y,z,w\) is from \(Y\), then we immediately have \(fp(d^{\prime};x,y,z,w)=0\). Therefore, we get

\[\sum_{x,y,z,w\in\binom{X}{4}}fp(d^{\prime};x,y,z,w)^{q} =\sum_{x,y,z,w\in\binom{X}{4}}fp(d^{\prime};x,y,z,w)^{q}+\sum_{w\in Y }\sum_{x,y,z\in\binom{X}{3}}fp(d^{\prime};x,y,z,w)^{q}\] \[=\sum_{x,y,z,w\in\binom{X}{4}}fp(d;x,y,z,w)^{q}+\sum_{w\in Y}\sum_{ x,y,z\in\binom{X}{3}}tp(d;x,y,z)^{q}\] \[=\binom{n}{4}\operatorname{AvgHyp}_{q}(d)^{q}+n\binom{n}{3} \operatorname{AvgUM}_{q}(d)^{q}.\]

To bound \(\operatorname{AvgHyp}_{q}\), we will use the fact that

\[fp(d;x,y,z,w)\leq\frac{1}{2}[tp(d;x,y,z)+tp(d;x,y,w)+tp(d;x,z,w)+tp(d;y,z,w)].\]

Therefore,

\[\operatorname{AvgHyp}_{q}(d)^{q} =\frac{1}{\binom{n}{4}}\sum_{x,y,z,w\in\binom{X}{4}}fp(d;x,y,z,w)^ {q}\] \[\leq\frac{2^{q-2}}{\binom{n}{4}}\sum_{x,y,z,w\in\binom{X}{4}}[tp( d;x,y,z)^{q}+tp(d;x,y,w)^{q}+tp(d;x,z,w)^{q}+tp(d;y,z,w)^{q}]\] \[=\frac{2^{q-2}}{\binom{n}{4}}\sum_{x,y,z\in\binom{X}{3}}(n-3)tp(d; x,y,z)^{q}=\frac{2^{q}}{\binom{n}{3}}\sum_{x,y,z\in\binom{X}{3}}tp(d;x,y,z)^{q}=2 ^{q}\operatorname{AvgUM}_{q}(d)^{q}.\]

To sum up, we get

\[\operatorname{AvgHyp}_{q}(d^{\prime})^{q} =\frac{1}{\binom{2n}{4}}\sum_{x,y,z,w\in\binom{Z}{4}}fp(d^{\prime };x,y,z,w)^{q}\] \[=\frac{\binom{n}{4}}{\binom{2n}{4}}\operatorname{AvgHyp}_{q}(d)^ {q}+\frac{n\binom{n}{3}}{\binom{2n}{4}}\operatorname{AvgUM}_{q}(d)^{q}\] \[\leq\frac{1}{16}\operatorname{AvgHyp}_{q}(d)^{q}+\frac{1}{4} \operatorname{AvgUM}_{q}(d)^{q}\] \[\leq\frac{2^{q}}{16}\operatorname{AvgUM}_{q}(d)^{q}+\frac{1}{4} \operatorname{AvgUM}_{q}(d)^{q}=\left(\frac{1}{4}+2^{q-4}\right)\operatorname {AvgHyp}_{q}(d)^{q}.\]

The \(q=\infty\) case can be checked separately. 

Therefore, by the claim above and the assumption, we have a tree fitting \(d_{T}\) which fits \(d^{\prime}\) with \(\|d_{T}-d^{\prime}\|_{p}\leq\operatorname{AvgHyp}_{q}(d^{\prime})\cdot f(2n)\). Our goal is to construct a reduction to deduce an ultrametric fit \(d_{U}\) on \(X\), by utilizing \(d_{T}\). First,

**Claim:** If \(M\) and \(c\) were large and small enough respectively, then \(d_{T}(x_{1},y)+d_{T}(x_{2},y)-d_{T}(x_{1},x_{2})\geq 2c\) holds for every \(x_{1},x_{2}\in X\) and \(y\in Y\).

**Proof:** Suppose the claim fails. As \(d(x_{1},y)+d(x_{2},y)-d(x_{1},x_{2})=M+M-d(x_{1},x_{2})\), it suggests that one of three pair distances should have distortion at least \(\frac{1}{3}(2M-2c-d(x_{1},x_{2}))\). This should not happen if \(M-c\gg\operatorname{AvgHyp}_{q}(d^{\prime})\cdot f(2n)\). 

We will interpret the above claim as a structural property. Given a tree \(T\) which realizes the tree metric \(d_{T}\), one can find a Steiner node \(w\) on \(x_{1},x_{2},y\). Then we have \(d_{T}(w,y)=\frac{1}{2}[d_{T}(x_{1},y)+d_{T}(x_{2},y)-d_{T}(x_{1},x_{2})]\geq c\). Since it holds for any \(x_{1},x_{2}\in X\), it suggests that every geodesic segment \([x,y]\) for fixed \(y\in Y\) should share their paths at least \(c\). This observation allows a following _uniformization_ on \(Y\).

**Claim:** One can refine a tree metric so that \(d_{T}(y_{1},y_{2})=c\) for all \(y_{1},y_{2}\in Y\) with \(\ell_{p}\) error non-increasing.

**Proof:** Pick \(y_{0}=\operatorname*{argmin}_{y\in Y}\sum_{x\in X}|d_{T}(x,y)-d(x,y)|^{p}\). Then we will keep \(X\cup\{y_{0}\}\) with its tree structure and relocate all other \(y\neq y_{0}\in Y\). As every geodesic segments will share their paths at least \(c\), we will pick a point \(z\) with \(d_{T}(y,z)=c/2\) on the segment, and draw edge \((z,y)\) for all \(y\in Y\) with length \(c/2\). Then \(d_{T}(y_{1},y_{2})=c\) as desired. Furthermore, as \(d_{T^{\prime}}(x,y)=d_{T}(x,y_{0})\) for any \(x\in X\) and \(y\in Y\),

\[\|d_{T^{\prime}}-d\|_{p}^{p}\] \[= \sum_{z,w\in\binom{X}{2}}|d_{T^{\prime}}(z,w)-d(z,w)|^{p}\] \[= \sum_{x,x^{\prime}\in\binom{X}{2}}|d_{T^{\prime}}(x,x^{\prime})- d(x,x^{\prime})|^{p}+\sum_{x\in X,y\in Y}|d_{T^{\prime}}(x,y)-M|^{p}+\sum_{y,y^{ \prime}\in\binom{Y}{2}}|d_{T^{\prime}}(y,y^{\prime})-c|^{p}\] \[\leq \sum_{x,x^{\prime}\in\binom{X}{2}}|d_{T}(x,x^{\prime})-d(x,x^{ \prime})|^{p}+\sum_{x\in X,y\in Y}|d_{T}(x,y_{0})-M|^{p}+\sum_{y,y^{\prime}\in \binom{Y}{2}}0\] \[= \sum_{x,x^{\prime}\in\binom{X}{2}}|d_{T}(x,x^{\prime})-d(x,x^{ \prime})|^{p}+\sum_{y\in Y}\sum_{x\in X}|d_{T}(x,y_{0})-M|^{p}\] \[\leq \sum_{x,x^{\prime}\in\binom{X}{2}}|d_{T}(x,x^{\prime})-d(x,x^{ \prime})|^{p}+\sum_{y\in Y}\sum_{x\in X}|d_{T}(x,y)-M|^{p}\leq\|d_{T}-d\|_{p}^ {p},\]

as desired. 

**Algorithm 7** Construction of a restricted tree with respect to given root \(r\)

``` procedureRestrictTree
2:Input: Tree fitting \((T,d_{T})\) on \(Z\) Output: Tree fitting \(d_{T}\) such that \(d_{T}(x,y)=M\) for all \(x\in X\) and \(y\in Y\)
4:for each \(x\in X\): if\(d_{T}(x,y_{0})>M\):
6: Relocate \(x\) to point on geodesic \([x,y_{0}]\) so that \(d_{T}(x,y_{0})=M\) elseif\(d_{T}(x,y_{0})<M\):
8: Add edge \((x,x^{\prime})\) with length \(M-d_{T}(x,y_{0})\) and relocate \(x\) to \(x^{\prime}\) return\((T,d_{T})\) ```

**Algorithm 8** Refine a tree fit on \(Z=X\cup Y\)

**Claim:** One can refine a tree metric so that \(d_{T^{\prime}}(x,y)=M\) for all \(x\in X\) and \(y\in Y\) with \(\ell_{p}\) error not greater than \(3^{(p-1)/p}\) times the original error. Furthermore, if we restrict \(d_{T^{\prime}}\) on \(X\), then it is an ultrametric.

**Proof:** We will use the algorithm RestrictTree to obtain the restricted tree as described. It is clear that such procedure will successfully return a tree metric \(d_{T}\) with \(d_{T}(x,y)=M\) for all \(x\in X\) and \(y\in Y\). Furthermore, as Denote \(\epsilon:=d_{T}-d\). Then for any pair \(x_{1},x_{2}\in X\), its distortion does not increase greater than \(|\epsilon(x_{1},y_{0})|+|\epsilon(x_{2},y_{0})|\), which is the amount of relocation. We can utilize thisfact by

\[\|d_{T^{\prime}}-d\|_{p}^{p}= \sum_{z,w\in\binom{X}{2}}|d_{T^{\prime}}(z,w)-d(z,w)|^{p}\] \[= \sum_{x,x^{\prime}\in\binom{X}{2}}|d_{T^{\prime}}(x,x^{\prime})-d(x,x^{\prime})|^{p}+\sum_{x\in X,y\in Y}|d_{T^{\prime}}(x,y)-M|^{p}+\sum_{y,y^{ \prime}\in\binom{Y}{2}}|d_{T^{\prime}}(y,y^{\prime})-c|^{p}\] \[= \sum_{x,x^{\prime}\in\binom{X}{2}}|d_{T^{\prime}}(x,x^{\prime})- d(x,x^{\prime})|^{p}\] \[\leq \sum_{x,x^{\prime}\in\binom{X}{2}}(|\epsilon(x,y_{0})|+|\epsilon( x,x^{\prime})|+|\epsilon(x^{\prime},y_{0})|)^{p}\] \[\leq 3^{p-1}\sum_{x,x^{\prime}\in\binom{X}{2}}(|\epsilon(x,y_{0})|^ {p}+|\epsilon(x,x^{\prime})|^{p}+|\epsilon(x^{\prime},y_{0})|^{p})\] \[\leq 3^{p-1}\left(\sum_{x,x^{\prime}\in\binom{X}{2}}|\epsilon(x,x^{ \prime})|^{p}+(n-1)\sum_{x\in X}|\epsilon(x,y_{0})|^{p}\right)\] \[\leq 3^{p-1}\left(\sum_{x,x^{\prime}\in\binom{X}{2}}|\epsilon(x,x^{ \prime})|^{p}+n\sum_{x\in X}|\epsilon(x,y_{0})|^{p}\right)=3^{p-1}\|d_{T}-d\| _{p}^{p},\]

as desired. Then by the restriction, \(d_{T}(x,y_{0})=M\) holds for all \(x\in X\). Therefore,

\[tp(d_{T^{\prime}};x_{1},x_{2},x_{3})=fp(d_{T^{\prime}};x_{1},x_{2},x_{3},y_{0} )=0\]

holds for all \(x_{1},x_{2},x_{3}\in\binom{X}{3}\), which shows that \(d_{T^{\prime}}\) on \(X\) is an ultrametric. 

Denote the restricted metric as \(d_{U}\) (on \(\binom{X}{2}\))). Then we have

\[\|d_{U}-d\|_{p}=\|d_{T^{\prime\prime}}-d^{\prime}\|_{p}\leq 3^{ \frac{p-1}{p}}\|d_{T^{\prime}}-d^{\prime}\|_{p} \leq 3^{\frac{p-1}{p}}\|d_{T}-d^{\prime}\|_{p}\] \[\leq 3^{\frac{p-1}{p}}\left(\frac{1}{4}+2^{q-4}\right)^{1/q}\cdot \mathrm{AvgUM}_{q}(d)\cdot f(2n),\]

as desired.

### Self-contained proof of ultrametric fit to rooted tree fit

Choose any base point \(w\in X\) and run HCCRootedTreeFit. Note that \(d+c_{w}=2(M-gp_{w})\). Then by 3.4, we see that the output \(d_{T}\) satisfies

\[\|d-d_{T}\|_{1}\leq 8\|\bm{\Delta}(d+c_{w})\|_{1}=8\|\bm{\Delta}(M-gp_{w})\|_{1} =8\|\bm{\Delta}_{w}(d)\|_{1}.\]

Figure 2: This figure depicts how RestrictTree works. The idea is we can relocate every vertices so that \(d(x,y_{0})=d_{T}(x,y_{0})\) holds for every \(x\in X\).

Although \(\|\bm{\Delta}_{w}(d)\|_{1}\) itself does not satisfy the guaranteed bound, we know that

\[\sum_{w\in X}\|\bm{\Delta}_{w}(d)\|_{1} =\sum_{w\in X}\sum_{x,y,z\in\binom{X\setminus\{w\}}{3}}fp_{w}(d;x, y,z)\] \[=4\sum_{x,y,z,w\in\binom{X}{4}}fp(d;x,y,z,w)=4\binom{n}{4}\operatorname{ AvgHyp}(d)\]

so there exists an output \(d_{T}\) (with appropriately chosen \(w\)) with its \(\ell_{1}\) error bounded by \(8\cdot\frac{4}{n}\binom{n}{4}\operatorname{AvgHyp}(d)=8\binom{n-1}{3} \operatorname{AvgHyp}(d)\). As we need to check every base point \(w\in X\), this procedure runs in time \(O(n^{3}\log n)\).

### Proof of Theorem 3.3

We begin by recalling the definition of highly connectedness.

**Definition 6.1**.: Given a graph \(G=(V,E)\) and two disjoint vertex sets \(X\) and \(Y\), the pair \((X,Y)\) is said to be _highly connected_ if both

\[\text{for all }x\in X,\;|\{y\in Y|(x,y)\in E\}|\geq\frac{|Y|}{2},\text{and}\]

\[\text{for all }y\in Y,\;|\{x\in X|(y,x)\in E\}|\geq\frac{|X|}{2}\]

hold.

Our first lemma shows that every clusters should contain reasonably many edges "within", so that the number of "false positive" edges can be bounded. Because highly connectedness determines whether at least half of the edges have been connected, we can expect that the degree of every node should be at least half of the size of clusters.

**Lemma 6.2**.: For any \(C\in P_{t}\) with \(|C|\geq 2\) and \(x\in C\), \(|\{x^{\prime}\in C|(x,x^{\prime})\in E_{t}\}|\geq\frac{|C|}{2}\).

Proof.: We use induction on \(t\). The lemma is obviously true when \(t=0\). Next, suppose we pick \(C\in P_{t}\) with \(|C|\geq 2\) and the cluster \(C\) is formed at step \(t_{0}\leq t\). If \(t_{0}<t\), since \(P_{t}\) is increasing, it is clear that

\[|\{x^{\prime}\in C|(x,x^{\prime})\in E_{t}\}|\geq|\{x^{\prime}\in C|(x,x^{ \prime})\in E_{t_{0}}\}|\geq\frac{|C|}{2}\quad\text{for all }x\in C.\]

Therefore, it is enough to check when \(t_{0}=t\), i.e., \(C\) is the newly added cluster at step \(t\).

Suppose \(C\) is achieved by merging \(C=C_{a}\cup C_{b}\). We then have two cases to check, depending on the sizes of \(C_{a}\) and \(C_{b}\).

1. \(|C_{a}|=1\) or \(|C_{b}|=1\): Without loss of generality, assume that \(|C_{a}|=1\) and let \(C_{a}=\{a\}\). If \(C_{b}\) is also a singleton, then there is nothing to prove. If not, then, we must have \((a,y)\in E_{t}\) for all \(y\in C_{b}\) in order to be _highly connected_. And by the induction hypothesis, \[|\{y^{\prime}\in C_{b}|(y,y^{\prime})\in E_{t}\}|\geq|\{y^{\prime}\in C_{b}|(y, y^{\prime})\in E_{t-1}\}|\geq\frac{|C_{b}|}{2}\quad\text{for all }y\in C_{b}.\] Hence, we have \[|\{y^{\prime}\in C|(a,y^{\prime})\in E_{t}\}|=|C_{b}|=|C|-1\geq\frac{|C|}{2}\] and for all \(y\in C_{b}\) \[|\{y^{\prime}\in C|(y,y^{\prime})\in E_{t}\}|=|\{y^{\prime}\in C_{b}|(y,y^{ \prime})\in E_{t}\}|+1\geq\frac{|C_{b}|}{2}+1>\frac{|C|}{2},\] which completes the proof of Case 1.

* \(|C_{a}|>1\) and \(|C_{b}|>1\): By the induction hypothesis, we have \[|\{x^{\prime}\in C_{a}|(x,x^{\prime})\in E_{t}\}|\geq|\{x^{\prime}\in C_{a}|(x,x^{ \prime})\in E_{t-1}\}|\geq\frac{|C_{a}|}{2}\quad\text{for all }x\in C_{a},\] and similarly for \(C_{b}\). As \(C_{a}\) and \(C_{b}\) are both _highly connected_, we have \[|\{y^{\prime}\in C_{b}|(x,y^{\prime})\in E_{t}\}|\geq\frac{|C_{b}|}{2}\quad \text{for all }x\in C_{a},\] \[|\{x^{\prime}\in C_{b}|(y,x^{\prime})\in E_{t}\}|\geq\frac{|C_{a}|}{2}\quad \text{for all }y\in C_{b}.\] Therefore, for any \(x\in C_{a}\), \[|\{z^{\prime}\in C|(x,z^{\prime})\in E_{t}\}| =|\{x^{\prime}\in C_{a}|(x,x^{\prime})\in E_{t}\}|+|\{y^{ \prime}\in C_{b}|(x,y^{\prime})\in E_{t}\}|\] \[\geq\frac{|C_{a}|}{2}+\frac{|C_{b}|}{2}=\frac{|C|}{2},\] while each inequality comes from the induction hypothesis and the connectivity condition. We can similarly show the property for any \(y\in C_{b}\), which completes the proof of Case 2.

This completes the proof. 

Next, we prove the following isoperimetric property:

**Lemma 6.3**.: For \(C\in P_{t}\) denote \(\partial X:=\{(x,y)\in E_{t}|x\in X,y\in C\setminus X\}\) for \(X\subset C\). Then for any proper \(X\),

\[|\partial X|\geq\frac{|C|}{2}.\]

Proof.: Without loss of generality, assume that \(1\leq|X|\leq\frac{|C|}{2}\). Then for any \(x\in X\), by 6.2, there are at least \(\frac{|C|}{2}-|X|+1\) edges which connects \(x\) and other vertices not in \(X\). Therefore, we have

\[|\partial X|\geq|X|\left(\frac{|C|}{2}-|X|+1\right)\geq\frac{|C|}{2}.\]

As our version of the HCC problem bounds the number of edges in disagreement in terms of the bad triangles, with the next lemmas, we count the number of bad triangles \(|B(G_{t})|\) and bound the number of edges in disagreement. Recall that a bad triangle is defined as a triplet of edges in which only two of the three possible edges belong to \(E_{t}\). For each cluster \(C\in P_{t}\), denote the number of bad triangles _inside_\(C\) as \(T_{C}\). And for each cluster pair \(C,C^{\prime}\in P_{t}\), we count _some_ of the bad triangles _between_\(C\) and \(C^{\prime}\) as \(T_{C,C^{\prime}}\). More precisely,

**Definition 6.4**.: Given a partition \(P_{t}\), denote

\[T_{C}:=\{(x,y,z)|x,y,z\in C\text{ and }(x,y,z)\in B(G_{t})\}\quad\text{for }C\in P _{t},\]

\[T_{(C,C^{\prime})}:=\{(x,x^{\prime},y)|x,x^{\prime}\in C,y\in C^{\prime}\text { and }(x,x^{\prime}),(x,y)\in E_{t},(x^{\prime},y)\notin E_{t}\}\]

\[\cup\{(x,y,y^{\prime})|x\in C,y,y^{\prime}\in C^{\prime}\text{ and }(y,y^{\prime}),(x,y)\in E_{t},(x,y^{\prime})\notin E_{t}\}\quad\text{for }(C,C^{\prime})\in\binom{P_{t}}{2}.\]

Note that \(T_{C,C^{\prime}}\) does not count _every_ bad triangle between \(C\) and \(C^{\prime}\), as there might be bad triangles in which the missing edge is _inside_ the cluster. With these definitions, we have

\[\sum_{C\in P_{t}}|T_{C}|+\sum_{(C,C^{\prime})\in\binom{P_{t}}{2}}|T_{(C,C^{ \prime})}|\leq|B(G_{t})|.\]

**Proposition 6.5**.: For any cluster \(C\in P_{t}\), the number of edges _not_ in \(C\) is bounded by \(|T_{C}|\). That is,

\[|\{(x,y)\notin E|x,y\in C\}|\leq|T_{C}|.\]

(In fact, it is bounded by \(|T_{C}|/2\).)Proof.: We will prove that for any \(e=(x,y)\notin E_{t}\) with \(x,y\in C\), there exist at least two elements in \(T_{C}\) that contain \(e\), which implies our result. If \(|C|=1\), then there is nothing to prove. For any \(|C|\geq 2\) and \((x,y)\notin E_{t}\), by Lemma 6.2, there are at least \(\frac{|C|}{2}\) neighbors of \(x\) and \(y\) (we will denote each as \(N_{x}\) and \(N_{y}\), respectively). Since \(N_{x},N_{y}\subset C\setminus\{x,y\}\), we have

\[|N_{x}\cap N_{y}|=|N_{x}|+|N_{y}|-|N_{x}\cup N_{y}|\geq\frac{|C|}{2}+\frac{|C|} {2}-(|C|-2)=2.\]

For any \(z\in N_{x}\cap N_{y}\), \((x,y,z)\in T_{C}\) by definition, which proves the assertion. 

**Lemma 6.6**.: For \(G=(V,E)\) and two disjoint subsets \(X,Y\subset V\), suppose

\[|\{x^{\prime}\in X|(x,x^{\prime})\in E\}|\geq\frac{|X|}{2}\text{ for all }x\in X \text{ and }|\{y^{\prime}\in Y|(y,y^{\prime})\in E\}|\geq\frac{|Y|}{2}\text{ for all }y\in Y.\]

We will further assume that \(X\) and \(Y\) are not highly connected. Then we have

\[|\{(x,y)\in E|x\in X,y\in Y\}|\leq 4\cdot(|\{(x,x^{\prime},y)|x,x^{ \prime}\in X,y\in Y\text{ and }(x,x^{\prime}),(x,y)\in E,(x^{\prime},y)\notin E\}|\] \[+|\{(x,y,y^{\prime})|x\in X,y,y^{\prime}\in Y\text{ and }(y,y^{\prime}),(x,y)\in E,(x,y^{\prime})\notin E\}|)\]

Proof.: The key argument here is to use Lemma 6.3 and count the boundary sets. Define \(N_{Y}(x):=\{y\in Y|(x,y)\in E\}\) for \(x\in X\) (and \(N_{X}(y)\) for \(y\in Y\) respectively.) Then for any \((y,y^{\prime})\in\partial N_{Y}(x)\), \((x,y,y^{\prime})\) is a bad triangle and it contributes the right hand side. In other words, the right hand side of the above equation is upper bounded by

\[(\star)=4\cdot\left(\sum_{x\in X}|\partial N_{Y}(x)|+\sum_{y\in Y}|\partial N _{X}(y)|\right).\]

By 6.3, we know that for \(N_{Y}(x)\neq\emptyset\) or \(Y\), \(|\partial N_{Y}(x)|\geq\frac{|Y|}{2}\). We will count such \(x\in X\) and \(y\in Y\) which its neighbor set is _proper_, in order to make a lower bound. We divide the rest of the analysis into three cases:

1. There exists \(x_{0}\in X\) such that \(N_{Y}(x_{0})=\emptyset\): then for any \(y\in Y\), \(x_{0}\notin N_{X}(y)\) so that \(N_{X}(y)\) cannot be \(X\). Thus, \(N_{X}(y)\neq\emptyset\) immediately leads that \(|\partial N_{X}(y)|\geq\frac{|X|}{2}\). We have \[|\{(x,y)\in E|x\in X,y\in Y\}| \leq|X|\cdot|\{y\in Y|N_{X}(y)\neq\emptyset\}|\] \[\leq\sum_{y\in Y,N_{X}(y)\neq\emptyset}|X|\] \[\leq 2\sum_{y\in Y,N_{X}(y)\neq\emptyset}|\partial N_{X}(y)|\] \[\leq(\star)\leq\text{RHS},\] which proves the lemma.
2. There exists \(y_{0}\in Y\) such that \(N_{X}(y_{0})=\emptyset\): this case is proven as we did in Case 1.
3. For any \(x\in X\) and \(y\in Y\), \(N_{Y}(x)\neq\emptyset\) and \(N_{X}(y)\neq\emptyset\), we will use the assumption that \(X\) and \(Y\) are _not_ highly connected. With this assumption, we can also assume without loss of generality that there is an \(x_{0}\in X\) such that \(|N_{Y}(x_{0})|<\frac{|Y|}{2}\). Then for any \(y\notin N_{Y}(x_{0})\), \(N_{X}(y)\neq\emptyset\) and \(N_{X}(y)\neq X\) (as \(x_{0}\notin N_{X}(y)\)). Thus, the boundary set exists and has at least \(\frac{|X|}{2}\) elements. Therefore,

\[\text{RHS}\geq(\star) \geq 4\sum_{y\in Y}|\partial N_{X}(y)|\] \[\geq 4\sum_{y\in Y,y\notin N_{Y}(x_{0})}|\partial N_{X}(y)|\] \[\geq 4\sum_{y\in Y,y\notin N_{Y}(x_{0})}\frac{|X|}{2}\] \[=4\cdot\frac{|X|}{2}\cdot(|Y|-|N_{Y}(x_{0})|)\] \[>4\cdot\frac{|X|}{2}\cdot\frac{|Y|}{2}=|X|\cdot|Y|\geq\text{LHS}.\]

This completes the proof. 

**Proposition 6.7**.: For any cluster pair \((C,C^{\prime})\in P_{t}\), the number of edges _between_\(C\) and \(C^{\prime}\) is bounded by \(4|T_{(C,C^{\prime})}|\). (i.e., \(|\{(x,y)\in E|x\in C,y\in C^{\prime}\}|\leq 4|T_{(C,C^{\prime})}|\).)

Proof.: Here, we will use an induction on \(t\). When \(t=0\), then there is nothing to prove. Suppose the induction hypothesis holds for \(t_{0}<t\).

1. [label=Case 0: \(e_{t}\) does not connect two vertices between \(C\) and \(C^{\prime}\) and \(C,C^{\prime}\) are not added at step \(t\): then by induction hypothesis, the proposition holds at step \(t-1\), which is also true at step \(t\) (As the left hand side is invariant and \(|T_{(C,C^{\prime})}|\) is increasing).
2. \(e_{t}=(x,y)\) for \(x\in C\) and \(y\in C^{\prime}\): then it follows that our algorithm decided not to merge two clusters, which means \(C\) and \(C^{\prime}\) are _not_ highly connected. Then by Lemma 6.6, the number of edges is bounded by \(4|T_{(C,C^{\prime})}|\) as desired.
3. \(e_{t}\) does not connect two vertices between \(C\) and \(C^{\prime}\), but \(C\) or \(C^{\prime}\) is newly formed at step \(t\): note that both clusters cannot be generated at the same time, so we assume without loss of generality that \(C\) is the new cluster and assume it is generated by \(C=C_{a}\cup C_{b}\). Then by the induction hypothesis, we have \[\{(x,y)\in E|x\in C,y\in C^{\prime}\}| =\{(x,y)\in E|x\in C_{a},y\in C^{\prime}\}|+\{(x,y)\in E|x\in C_ {b},y\in C^{\prime}\}\] \[\leq 4(|T_{(C_{a},C^{\prime})}|+|T_{(C_{b},C^{\prime})}|)\quad \text{(by the induction hypothesis)}\] \[\leq 4|T_{(C,C^{\prime})}|.\]

This completes the proof. 

We are now ready to prove Theorem 3.3.

**Proof of Theorem 3.3:** For \(P_{t}=\{C_{1},C_{2},\cdots,C_{k}\}\), denote

\[e_{i} :=|\{(x,x^{\prime})\notin E|x,x^{\prime}\in C_{i}\}|,\] \[e_{i,j} :=|\{(x,y)\in E|x\in C_{i},y\in C_{j}\}|.\]

Then by the above propositions, we have

\[|E_{t}\Delta E(P_{t})| =\sum_{i}e_{i}+\sum_{i<j}e_{i,j}\] \[\leq\sum_{i}T_{C_{i}}+\sum_{i<j}4T_{(C_{i},C_{j})}\] \[\leq 4\left(\sum_{i}T_{C_{i}}+\sum_{i<j}T_{(C_{i},C_{j})}\right) \leq 4\cdot|B(G_{t})|.\]

### Implementation details

To detail our implementation, we will introduce the following variables.

For \(x\in X\) and cluster \(C\), denote \(D(x,C):=|\{y\in C|(x,y)\in E\}|\). We further denote the _connectivity condition_\(H(x,C)\) as \(\mathrm{IF}(D(x,C)\geq\frac{|C|}{2})\).

Also for given two clusters \(C_{a}\) and \(C_{b}\), we will denote \(M(C_{a},C_{b}):=|\{x\in C_{a}|H(x,C_{b})=1\}|\).

We can easily check that two disjoint clusters \(C_{a}\) and \(C_{b}\) are highly connected if and only if

\[M(C_{a},C_{b})+M(C_{b},C_{a})=|C_{a}|+|C_{b}|.\]

``` function HCCTriangle Input:\(X=\{x_{1},\cdots,x_{n}\}\), \(\{e_{t}\}\)  Initialize \(D,H,M\) as 0 for vertex set \(X\)  Initialize \(\mathcal{P}=\{\{x_{1}\},\{x_{2}\},\cdots,\{x_{n}\}\},\mathcal{E}=\emptyset\)  For \(t\in\{1,2,\cdots,\binom{n}{2}\}\):  Take \(e_{t}=(x,y)\) with \(x\in C_{a}\) and \(y\in C_{b}\) (\(C_{a},C_{b}\in\mathcal{P}\))  Add \(D(x,C_{b}),D(y,C_{a})\) by 1  Update \(H(x,C_{b})\) and \(H(y,C_{a})\)  If \(H\) has been updated, update \(M(C_{a},C_{b})\) and \(M(C_{b},C_{a})\) accordingly  If \(C_{a}\neq C_{b}\) and \(M(C_{a},C_{b})+M(C_{b},C_{a})=|C_{a}|+|C_{b}|\):  Remove \(C_{a},C_{b}\) and add \(C_{new}:=C_{a}\cup C_{b}\) in \(\mathcal{P}\)  \(D(:,C_{new})\gets D(:,C_{a})+D(:,C_{b})\)  Compute \(H(:,C_{new})\) and \(M(:,C_{new})\) (by using \(D(:,C_{new})\))  \(M(C_{new},:)\gets M(C_{a},:)+M(C_{b},:)\)  \(P_{t}\gets P\) return\(\{P_{t}\}\) ```

**Algorithm 8** HCCTriangle: Actual Implementation

One can observe that an iteration only takes \(O(1)\) when \(C_{a}\) and \(C_{b}\) are not merged. When we merge \(C_{new}\), we need time \(O(n)\) to compute new information of \(C_{new}\). Since such events occur exactly \(n-1\) times, we can conclude that HCCTriangle overall runs in time \(O(n^{2})\).

### Asymptotic tightness of Gromov's distortion bound

While it is well-known that Gromov's result is asymptotically tight, we give a self-contained sketch of the proof and provide an example that witnesses the bound. We detail the construction given by [17]. Consider the Poincare disk \(\mathbb{H}^{2}\) and its hyperbolic \(n=4m\)-gon \(H_{n,r}\) as a set of equally spaced \(n\) points around the circle of radius \(r\) in \(\mathbb{H}^{2}\). Label all the points as \(x_{1},x_{2},\ldots,x_{4m}\), in the counterclockwise order, and define \(y=x_{3m}\). Suppose \(d_{T}\) is a tree metric fit to \(d\) and denote the Gromov product with respect to \(y\) as \(gp_{y}\). Then

\[gp_{y}(x_{0},x_{2m})\geq\min(gp_{y}(x_{0},x_{1}),gp_{y}(x_{1},x_ {2m})) \geq\min(gp_{y}(x_{0},x_{1}),\min(gp_{y}(x_{1},x_{2}),gp_{y}(x_{2},x_ {2m}))\] \[\geq\quad\cdots\] \[\geq\min_{i\in\{0,1,\ldots,2m-1\}}gp_{y}(x_{i},x_{i+1}).\]

Therefore there exists an \(i\) such that

\[d_{T}(x_{i},y)+d_{T}(x_{i+1},y)-d_{T}(x_{i},x_{i+1})\leq d_{T}(x_{0},y)+d_{T}( x_{2m},y)-d_{T}(x_{0},x_{2m}).\]

On the other hand, for \(r,n\) large enough,

\[[d(x_{i},y)+d(x_{i+1},y)-d(x_{i},x_{i+1})]-[d(x_{0},y)+d(x_{2m},y)-d(x_{0},x_{ 2m})]\approx 2\log(\sin(\pi/m))\approx 2\log n,\]

shows one of six pair distances should have distortion at least \(\approx 1/3\log n=\Omega(\log n)\).

### Examples

#### 6.7.1 Example for HCCUltraFit

For the following distances \(d\) on \(X=\{a,b,c,d,e,f\}\), we will compute the ultrametric fit using HCC. We first sort all the pairs in increasing distance order. We merge \(\{a\}\) and \(\{b\}\) first; this edgecorresponds to \(e_{1}=(a,b)\). Then we merge \(\{d\}\) and \(\{e\}\) for \(e_{2}=(d,e)\). Next, when we explore \((e,f)\) (regardless of whether \((a,d)\) comes first or not), we merge \(\{d,e\}\) and \(\{f\}\) with corresponding distance 6. Then we merge \(\{a,b\}\) and \(\{c\}\) when we explore \((b,c)\). Finally, we merge \(\{a,b,c\}\) and \(\{d,e,f\}\) when both clusters are _highly connected_, and we check its corresponding fit is 8.

#### 6.7.2 Example for HCCRootedTreeFit

Next, given \(d\), we want to find a tree fitting \(d_{T}\) which _restricts_\(r\in X\). We first compute \(c_{r}\) and run HCCUltraFit on \(d+c_{r}\), denote the output \(d_{U}\). Then our tree fit \(d_{T}\) is obtained by \(d_{T}:=d_{U}-c_{r}\), which is a tree metric. Furthermore, it restricts \(r\in X\), so that \(d_{T}(x,r)=d(x,r)\) for all \(x\in X\).

Figure 4: This figure depicts the example output \(d_{U}\) by drawing a dendrogram.

Figure 3: This figure depicts the example that proves Gromovâ€™s distortion bound is asymptotically tight. By symmetry, we can conclude that Gromovâ€™s algorithm will always return the same \(\ell_{\infty}\) error regardless of the choice of base point \(w\).

### Rooted tree structure

We discuss how the tree structure of \(d_{T}\), the output of HCCRootedTreeFit, is related to that of \(d_{U}\). As fitting \(d_{U}\) is an _agglomerative_ clustering procedure, one may consider its linkage matrix \(Z\) with specified format in scipy library (See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html). At each step, we utilize the data to construct desired rooted tree by adding Steiner node. Detailed algorithm is as follows (For the simplicity, we will describe the procedure when \(X=[n]=\{0,1,\cdots,n-1\}\)).

### Experiment details

#### 6.9.1 TreeRep

We adapted code from TreeRep, which is available at https://github.com/rsonthal/TreeRep. We removed its dependency upon PyTorch and used the numpy library for two reasons. First, we have found that using the PyTorch library makes the implementation unnecessarily slower (especially for small input). Second, to achieve a fair comparison, we fixed the randomized seed using numpy. All edges with negative weight have been set to 0 after we have outputted the tree.

#### 6.9.2 Gromov

We used the duality of Gromov's algorithm and SLHC (observed by [13]) and simply used reduction method (from ultrametric fitting to rooted tree fitting). scipy library was used to run SLHC. This procedure only takes \(O(n^{2})\) time to compute.

Figure 5: This figure depicts how the output \(d_{T}\) looks like. This tree structure in fact can easily be obtained by utilizing the structure of dendrogram we computed.

#### 6.9.3 NeighborJoin

For NeighborJoin, we implemented simple code. Note that it does not contain any heuristics on faster implementation. All edges with negative weight have been set to 0 after we have outputted the tree.

#### 6.9.4 Hardware setup

All experiments have used shared-use computing resource. There are 4 CPU cores each with 5GiB memory. We executed all the code using a Jupyter notebook interface running Python 3.11.0 with numpy 1.24.2, scipy 1.10.0, and networkx 3.0. The operating system we used is Red Hat Enterprise Linux Server 7.9.

#### 6.9.5 Common data set

For C-elegan and CS phd, we used pre-computed distance matrix from https://github.com/rsonthal/TreeRep. For CORA, Airport, and Disease, we used https://github.com/HazyResearch/hgcn and computed the shortest-path distance matrix of its largest connected component. The supplementary material includes the data input that we utilized.

#### 6.9.6 Synthetic data set

To produce random synthetic tree-like hyperbolic metric from a given tree, we do the following.

1. Pick two random vertices \(v,w\) with \(d_{T}(v,w)>2\) (If not, then run 1 again.)
2. Add edge \((v,w)\) with weight \(d_{T}(v,w)-2\delta\) (for \(\delta=0.1\)).
3. We repeat until new \(n_{e}=500\) edges have been added.
4. We compute shortest-path metric of the outputted sparse graph.

We excluded pair with \(d_{T}(v,w)\leq 2\) because: if it is 1, then the procedure simply shrinks the edge. if it is 2, then one can consider the procedure as just adding an Steiner node (of \(v,w\) and their common neighbor), which does not pose _hyperbolicity_.

#### 6.9.7 Comparison

For the comparison, we have fixed the randomized seed using numpy library (from 0 to \(n_{seed}\) - 1). For common data set experiments, we run \(n_{seed}=100\) times. For synthetic data set experiments, we run \(n_{seed}=50\) times.

### Detailed Comparisons

**In comparison with [1]** As [1] presented an \(O(1)\) approximation that minimizes the \(\ell_{1}\) error, it can be deduced that the total error of its output is also bounded by \(O(\mathrm{AvgHyp}(d)n^{3})\), while the leading coefficient is not known. It would be interesting to analyze the performance of LP based algorithms including [1] if we have some _tree-like_ assumptions on input, such as \(\mathrm{AvgHyp}(d)\) or the \(\delta\)-hyperbolicity.

**In comparison with [10]** We bounded the _total error_ of the tree metric in terms of the average hyperbolicity \(\mathrm{AvgHyp}(d)\) and the size of our input space \(X\). As the growth function we found is \(O(n^{3})\), it can be seen that the _average error_ bound would be \(O(\mathrm{AvgHyp}(d)|X|)\), which is asymptotically tight (In other words, the dependency on \(|X|\) is necessary).

While the setup [10] used is quite different, the result can be interpreted as follows: they bounded the _expectation_ of the distortion in terms of the average hyperbolicity \(\mathrm{AvgHyp}\)_and_ the maximum bound on the Gromov product \(b\) (or the diameter \(D\)). The specific growth function in terms of \(\mathrm{AvgHyp}\) and \(b\) is not known, or is hard to track. By slightly tweaking our asymptotic tightness example, it can also be seen that the dependency on \(b\) should be necessary. It would also be interesting to find how tight the dependency in terms of \(b\) is.

**In comparison with QuadTree** There are a number of tree fitting algorithms in many applications, with various kinds of inputs and outputs. One notable method is QuadTree (referred in, for example, [18]) which outputs a tree structure where each node reperesents a rectangular area of the domain. There are two major differences between QuadTree and ours: first, QuadTree needs to input data points (Euclidean), while ours only requires a distance matrix. Also, the main output of QuadTree is an utilized tree data structure, while ours (and other comparisons) focus on _fitting_ the metric.

We conducted a simple experiment on comparing QuadTree and others including ours. First, we uniformly sampled 500 points in \([0,1]^{2}\subset\mathbb{R}^{2}\) and ran QuadTree algorithm as a baseline algorithm. While defining edge weights on the output of QuadTree is not clear, we used \(2^{-d}\) for depth \(d\) edges. Note that the input we sampled is _nowhere_ hyperbolic so that it may not enjoy the advantages of fitting algorithms which use such geometric assumptions.

There is a simple reason why QuadTree behaves worse when it comes to the metric fitting problem. QuadTree may distinguish two very close points across borders. Then its fitting distance between such points is very large; it in fact can be the diameter of the output tree, which is nearly 2 in this experiment. That kind of pairs pose a huge error.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Error** & Average \(\ell_{1}\) error & Max \(\ell_{\infty}\) error \\ \hline HCC & 0.260\(\pm_{0.058}\) & 1.478\(\pm_{0.213}\) \\ Gromov & 0.255\(\pm_{0.041}\) & **1.071\(\pm_{0.092}\)** \\ TR & 0.282\(\pm_{0.067}\) & 1.648\(\pm_{0.316}\) \\ NJ & **0.224** & 1.430 \\ QT & 1.123 & 1.943 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Experiments on unit cube in \(\mathbb{R}^{2}\). The data set _does not_ have some hyperbolicity feature so that the result kind may be different from the main experiments.