# Stepping on the Edge:

Curvature Aware Learning Rate Tuners

 Vincent Roulet

Google DeepMind

vroulet@google.com

&Atish Agarwala

Google DeepMind

thetish@google.com

&Jean-Bastien Grill

Google DeepMind

jbgrill@google.com

&Grzegorz Swirszcz

Google DeepMind

swirszcz@google.com

&Mathieu Blondel

Google DeepMind

mblondel@google.com

&Fabian Pedregosa

Google DeepMind

pedregosa@google.com

Equal contribution.

###### Abstract

Curvature information - particularly, the largest eigenvalue of the loss Hessian, known as the sharpness - often forms the basis for learning rate tuners. However, recent work has shown that the curvature information undergoes complex dynamics during training, going from a phase of increasing sharpness to eventual stabilization. We analyze the closed-loop feedback effect between learning rate tuning and curvature. We find that classical learning rate tuners may yield greater one-step loss reduction, yet they ultimately underperform in the long term when compared to constant learning rates in the full batch regime. These models break the stabilization of the sharpness, which we explain using a simplified model of the joint dynamics of the learning rate and the curvature. To further investigate these effects, we introduce a new learning rate tuning method, Curvature Dynamics Aware Tuning (CDAT), which prioritizes long term curvature stabilization over instantaneous progress on the objective. In the full batch regime, CDAT shows behavior akin to prefixed warm-up schedules on deep learning objectives, outperforming tuned constant learning rates. In the mini batch regime, we observe that stochasticity introduces confounding effects that explain the previous success of some learning rate tuners at appropriate batch sizes. Our findings highlight the critical role of understanding the joint dynamics of the learning rate and curvature, beyond greedy minimization, to diagnose failures and design effective adaptive learning rate tuners.

## 1 Introduction

The learning rate, a.k.a. stepsize, is the main hyperparameter controlling the efficiency and stability of gradient-based training of deep neural networks. The learning rate is typically adjusted through a predetermined schedule - often consisting of a warm-up phase, where the learning rate is gradually increased to a peak, followed by an annealing phase, where it is decreased to zero (Goyal et al., 2017; Loshchilov and Hutter, 2016). Tuning the shape of the schedule (warm-up time, peak learning rate, decay scale and shape) is essential for good performance. Despite recent efforts to understand their effectiveness, the optimal shape of these schedules remains an area of active research (Liu et al., 2020; Shi et al., 2023). The cost of tuning these schedules has led to interest in automatic selection of these hyperparameters with _learning rate tuners_ - methods which aim to automatically adjust the learning rate through training.

These methods have roots in traditional optimization theory, including inexact linesearch with Armijo-Goldstein criterion (Armijo, 1966; Nocedal and Wright, 1999) and Polyak stepsizes (Polyak, 1964), which select the learning rate via estimates of the gap to optimality of the objective. The Armijo-Goldstein criterion is a crucial component of popular full-batch convex optimizers, such as L-BFGS (Liu and Nocedal, 1989). Recent efforts have adapted linesearches to stochastic optimization, with some partial empirical successes and with some approaches offering convergence guarantees (Galli et al., 2023; Mutschler and Zell, 2020; Vaswani et al., 2019). Similar efforts have been made for Polyak stepsizes (Berrada et al., 2020; Loizou et al., 2021), in addition to new methods which combine distance to optimality with online learning convergence bounds (Cutkosky et al., 2023; Defazio and Mishchenko, 2023; Ivyi et al., 2023; Mishchenko and Defazio, 2023).

Classically-inspired methods, however, have generally struggled to gain traction in deep learning. This is partly due to their design, which prioritizes convex, Lipschitz-continuous, and/or smooth (Lipschitz-continuous gradients) objectives. In contrast, the loss landscape of deep networks is known to be non-convex (Li et al., 2018), and non-Lipschitz continuous (Hochreiter et al., 2001). Moreover, non-linear models, especially neural networks, will commonly undergo dramatic changes in geometry during training (Arora et al., 2022; Jastrzebski et al., 2019; Jastrzebski et al., 2020; Kalra et al., 2023; Kopitkov and Indelman, 2020; Lewkowycz et al., 2020; Wu et al., 2018). In particular, most models undergo a phase of _progressive sharpening_ - where the sharpness, the largest eigenvalue of the Hessian, increases during training (Cohen et al., 2021). These potentially detrimental effects are mitigated by non-linear stabilization arising from the discreteness of the dynamics - namely, the _edge of stability_ (EOS) phenomenon (Cohen et al., 2021). This causes large Hessian eigenvalues to stabilize at the critical value for a given learning rate in an equivalent smooth setting (for example, max Hessian eigenvalue stabilizes at \(\lambda_{\max}=2/\eta\) for learning rate \(\eta\)) (Cohen et al., 2023, 2021). The early training time behavior corresponds to the regime where there is the most feature learning (Cohen et al., 2023, 2021), and is the main focus of this work; at late times, the large eigenvalues of the Hessian usually drop below the edge of stability. Gilmer et al. (2022) considered EOS stabilization as a leading candidate for the necessity of the warm-up procedure; as the learning rate \(\eta\) increases, \(\lambda_{\max}\) is effectively annealed.

This raises some natural questions. _How do these sharpness dynamics affect the performance of learning rate tuners? What insights can we gain to design better tuners for deep learning?_ Our work takes a first step at answering these questions, starting with a study of some classical learning rate tuners: a linesearch ensuring sufficient decrease and an approximately greedy method that minimizes a quadratic approximation of the objective. Specifically, we find the following.

* We empirically observe that classical learning rate tuners qualitatively underperform their constant learning rate counterparts across several deep learning benchmarks, in the full batch regime, for which these methods were originally designed.
* Our empirical analysis of curvature dynamics reveals that classical learning rate tuners generally undershoot the edge of stability. This undershooting creates a snowball effect of ever-increasing sharpness and ever-decreasing learning rates.
* We propose a theoretical model that effectively captures these empirically observed failures.

Our analysis suggests that stabilizing the sharpness may be a more important goal for the long-term success of training, compared to greedily optimizing the objective. To explore this idea, we propose the Curvature Dynamics Aware Tuning (CDAT) method, which dynamically drives the learning rate to the EOS. In our exploration, we find the following.

* We observe empirically that the proposed learning rate tuner can outperform fine-tuned constant learning rate counterparts in a full batch regime.
* We analyze the sharpness dynamics induced by CDAT in these examples and observe that the progressive sharpening is mitigated by the tuner, increasing learning rates at early times before stabilizing, akin to an automatic warm-up schedule.
* We propose a theoretical model that clarifies the dynamical mechanisms by which CDAT maintains proximity to the EOS, while highlighting the limitations of existing models of curvature dynamics.

Our work suggests that the design of learning rate tuners benefits from exploiting curvature stabilization rather than focusing on loss decrease. The introduction of simple learning rate tuners can also refine our understanding of sharpness dynamics through feedback loop effects. Additional experiments and experimental details are presented in Appendix B and Appendix C respectively.

## 2 The Interplay Between Learning Rate Tuners and Curvature Dynamics

A leitmotif in the design of learning rate tuners has been to select the learning rate to ensure a maximal or sufficient decrease of the objective at each iteration. We focus here on two canonical examples. Polyak stepsizes and hyper-gradient descent are also briefly examined in Appendix B, Fig. 13.

### Canonical learning rate tuners failures in deep learning

The first classical approach we consider is a **linesearch** (ls) method that selects the learning rate \(\eta\) such that the objective \(f\) satisfies a certain decrease criterion (Armijo, 1966; Nocedal and Wright, 1999). Formally, given current parameters \(w_{t}\) and an update direction \(u_{t}\), the learning rate \(\eta_{t}^{\text{ls}}\) is chosen such that

\[f(w_{t}+\eta_{t}^{\text{ls}}u_{t})\leq f(w_{t})+c\,\eta_{t}^{\text{ls}}u_{t}^{ \top}\nabla f(w_{t})\,.\] (1)

This rule assumes that \(u_{t}\) is a descent direction (\(\nabla f(w_{t})^{\top}u_{t}<0\)), which ensures the existence of a learning rate satisfying (1). This holds true for simple Gradient Descent (GD) or preconditioned variants like RMSProp (Hinton et al., 2012). In the criterion (1), \(c\) is usually a small constant set to \(10^{-4}\) or \(0\). A valid learning rate is searched with a usual backtracking linesearch (Appendix C).

The second method we consider involves selecting the learning rate at each iteration to minimize a quadratic approximation of the objective. Formally, the objective \(f\) at parameters \(w_{t}\) can be approximated along an update direction \(u_{t}\) by a quadratic approximation \(q_{f}\) as

\[f(w_{t}+\eta u_{t})\approx q_{f}(\eta;w_{t},u_{t})\coloneqq f(w_{t})+\eta \nabla f(w_{t})^{\top}u_{t}+\frac{1}{2}\eta^{2}u_{t}^{\top}\nabla^{2}f(w_{t}) u_{t}.\] (2)

Provided that this quadratic approximation is strongly convex in \(\eta\) (\(u_{t}^{\top}\nabla^{2}f(w_{t})u_{t}>0\)), the minimum of the quadratic approximation \(q_{f}(\eta;w_{t},u_{t})\) is reached for the **quadratically greedy** (qg) learning rate \(\eta^{\text{qg}}\) given by

\[\eta_{t}^{\text{qg}}=\frac{-\nabla f(w_{t})^{\top}u_{t}}{u_{t}^{\top}\nabla^{2 }f(w_{t})u_{t}}\,.\] (3)

Setting the learning rate by minimizing the quadratic approximations (3) is a simple intuitive idea studied for example by Schaul et al. (2013), Martens and Grosse (2015, Section 6.4). This approach as well as linesearches are effective on simple linear problems (Fig. 2). While their rationale originates in non-stochastic optimization, they have been analyzed in the context of stochastic optimization for deep learning (Schaul et al., 2013; Vaswani et al., 2019).

Figure 1: **Simple learning rates tuners qualitatively underperform their constant learning rate counterparts.** Gradient descent or RMSProp with a tuned constant learning rate versus self-tuned gradient descent by a linesearch method (1), or a quadratically greedy rule (3) on various datasets, architectures and losses in a full batch regime. The linesearch may perform better at early times but stalls in the long term.

Figure 2: Classical learning rate tuners can be effective on linear models.

### Analyzing learning rate tuners through curvature dynamics

Full batch regime.We revisit the performance of the learning tuners presented in Section 2.1 in the full batch regime on deep learning problems in Fig. 1. As demonstrated in Fig. 1, a linesearch (1) or the quadratically greedy rule (3) qualitatively underperform their constant learning rate counterpart in the deep learning benchmarks considered. Notably, all these results are obtained despite being in a full batch regime, for which these methods are originally designed. To understand the failures of these approaches, we consider several measures presented in Fig. 3 (see also Fig. 12).

First, we observe a consistent decrease in the chosen learning rate over time, spanning several orders of magnitude (\(1^{\text{st}}\) panel of Fig. 3). This is surprising, as none of these approaches explicitly encode a decreasing learning rate mechanism. Specifically, the linesearch always initiates its search with a guess larger than the previously selected learning rate (see Appendix C for implementation details). Decreasing learning rates are theoretically optimal for non-smooth objectives (Nesterov et al., 2018), such as the ones induced by using the ReLU activation; however in our example, the gradient norm does not increase beyond one order of magnitude (\(4^{\text{th}}\) panel of Fig. 3). This suggests both that an increase in gradient norm is not the primary cause of learning rate decrease, and also explains why the learning rate decrease is correlated with slower progress on the training loss.

Following the work of Cohen et al. (2021), we analyze the dynamics of the sharpness, that is the largest eigenvalue of the Hessian, \(\lambda_{\max}(\nabla^{2}f(w_{t}))\). In the \(2^{\text{nd}}\) panel of Fig. 3, we observe that while sharpness stabilizes for gradient descent, it does not exhibit the same behavior for the considered learning rate tuners. By plotting the product of the learning rate \(\eta_{t}\) and the sharpness (\(3^{\text{rd}}\) panel of Fig. 3), we find that this product can exceed the stability threshold of \(2\), eventually stabilizing below this threshold for constant learning rate gradient descent. In contrast, for the learning rate tuners, this product neither surpasses the stability threshold nor stabilizes around \(2\) in the long run. Therefore, these classical learning rate tuners do not operate at the edge of stability.

From a theoretical perspective, objectives are typically classified as either smooth or non-smooth. Smooth objectives have gradients that are Lipschitz-continuous, at least locally around any point. Non-smooth objectives, on the other hand, may contain points with kinks (non-differentiable points). However, this taxonomy might not fully capture the curvature dynamics observed by Cohen et al. (2023, 2021) for constant learning rates, and in Fig. 1 for the classical learning rate tuners. In particular, the concept of smoothness might not be entirely relevant in the context of deep learning, where its local estimate (the spectral norm of the Hessian, also known as sharpness) can continue to increase throughout training. To push the limits of classical smoothness assumptions, we consider in Section 3 a learning rate tuner that propels the optimizer at the edge of stability or above, a regime that usual smoothness assumptions would theoretically prohibit.

Mini-batch regime.The results presented in Fig. 1 in the full batch regime _do not contradict_ the success of linesearches at medium batch size observed by Vaswani et al. (2019) in the stochastic regime. This observation is illustrated in Fig. 14, and was previously reported by Roulet et al. (2023). We simply point out that the success of linesearches observed by Vaswani et al. (2019) may not be entirely attributable to the method's original rationale.

Figure 3: **Classical learning rate tuners can undershoot the edge of stability. Learning rate, sharpness, their product, and the gradient norm evolution of a constant learning rate and learning rate tuners, full batch gradient descent. Learning rate decreases by \(3\) orders of magnitude for tuners (\(1^{\text{st}}\) panel) while sharpness increases (\(2^{\text{nd}}\) panel). Their product remains relatively steady, just below the edge of stability (\(3^{\text{rd}}\) panel). The gradient norm increases by less than a factor of \(10\), consistent with slow training at late times (\(4^{\text{th}}\) panel).**The actual success of linesearches in a stochastic regime may instead be explained by the attenuated progressive sharpening observed in such a regime (Agarwala and Pennington, 2024; Cohen et al., 2021; Jastrzebski et al., 2017). Moreover, linesearches applied to mini-batches tend to select larger learning rates than they would in a full-batch regime (Mutschler and Zell, 2020) potentially allowing them to avoid undershooting the full objective's edge of stability.

### Theoretical analysis

The sharpening effects can be understood theoretically. Previous work has shown that the stabilization provided by EOS is due to non-linear interaction between the component of the gradient in the largest eigendirection, and the dynamics of the largest eigenvalues themselves (Agarwala et al., 2023; Damian et al., 2023). We can use these analyses to understand why there is no stabilization for some classical learning rate tuners.

We start with the model from Damian et al. (2023), which focuses on the dynamics in the largest eigendirection of the Hessian. We consider a unique eigenvector for simplicity; we don't observe degeneracy in the eigenspace of the largest eigenvalue in any practical models. Given an objective \(f\) parameterized by parameters \(w_{t}\), let \(\lambda_{t}\) be the largest eigenvalue of the Hessian \(\nabla^{2}f(w_{t})\), i.e., \(\lambda_{t}\coloneqq\lambda(w_{t})\coloneqq\lambda_{\max}(\nabla^{2}f(w_{t}))\). Let \(v\) be its normalized eigenvector; the model assumes slow eigenvector change, so it is treated as a fixed direction. The joint dynamics of \(\lambda_{t}\) and the projection \(x_{t}\coloneqq v^{\top}w_{t}\) can then be written as

\[x_{t+1}=(1-\eta_{t}\lambda_{t})x_{t},\ \lambda_{t+1}=\eta_{t}(a-bx_{t}^{2})+ \lambda_{t}\,.\] (4)

Here, \(a\coloneqq-\nabla\lambda(w)^{\top}\nabla f(w)\) corresponds to the instantaneous change of \(\lambda\) along the negative gradient (the update direction), and \(b\coloneqq\|\nabla\lambda(w)\|^{2}\) encodes the non-linear negative feedback between \(x_{t}\) and \(\lambda_{t}\). Both \(a\) and \(b\) are considered constant along iterations. These equations are derived by Damian et al. (2023) using a Taylor expansion of the iterates combined with a coupling argument. We provide intuition for the model in Appendix A.1.

In the original model, the learning rate \(\eta_{t}\) is also fixed to \(\eta\). This leads to the following dynamics: while \(\eta\lambda_{t}<2\), the magnitude of \(x_{t}\) decreases. This, in turn, leads to an increase in \(\lambda_{t}\). Eventually, \(\eta\lambda_{t}>2\) and \(|x_{t}|\) increases. This eventually leads to the \(bx_{t}^{2}\) term becoming large, which decreases \(\lambda_{t}\). There is a range of learning rates over which this dynamic leads to quasi-stable oscillations of \(\lambda_{t}\) around the edge of stability value \(2/\eta\) (Fig. 4, blue curves).

When using a learning rate tuner, \(\eta_{t}\) is also a dynamical variable. This introduces the additional complication of a shifting edge of stability. Therefore, it is advantageous to analyze the dynamical system using normalized variables (Agarwala et al., 2023). We define \(y_{t}\coloneqq\eta_{t}\lambda_{t}-2\), where \(y=0\) corresponds to the EOS, and \(p_{t}\coloneqq x_{t}^{2}\). This gives us the dynamical equations (Appendix A.2)

\[p_{t+1}=(1+y_{t})^{2}p_{t},\ y_{t+1}=\eta_{t+1}\left[\eta_{t}\left(a-bp_{t} \right)\right]+\left(\frac{\eta_{t+1}}{\eta_{t}}\right)y_{t}+2\left[\frac{ \eta_{t+1}}{\eta_{t}}-1\right].\] (5)

We must then supply a rule for \(\eta_{t+1}\). In Fig. 3, we observed that in the full batch setting, the learning rate multiplied by the sharpness appears to quickly approach a threshold of \(2-\epsilon\) (corresponding to \(y=-\epsilon\)), and then varies slowly below the EOS threshold.

Figure 4: **The poor performance of classical learning rate tuners, understood in a simplified model.** The dynamics of learning rate \(\eta\), sharpness \(\lambda_{\max}\), and normalized centered sharpness \(y=\eta\lambda_{\max}-2\) are examined in the simplified model (5). With a constant \(\eta\), \(\lambda_{\max}\) stabilizes and \(y\) oscillates around \(0\) (blue). Classical learning rate tuners often quickly equilibrate around \(y_{t}=-\epsilon\), which we model using \(\eta=1.9\lambda_{\max}\) (orange). This equilibration of \(y\) away from zero prevents stabilization in \(\lambda_{\max}\), leading to an increase in \(\lambda_{\max}\), and a corresponding decrease in \(\eta\).

We model the varying learning rate as

\[\eta_{t}\coloneqq 2(1-\epsilon)/\lambda_{t}\,.\] (6)

This maintains \(y_{t}=-\epsilon\). Notably, this schedule was explicitly proposed by Cohen et al. (2021) (see also Fig. 15). In this regime, \(p_{t}\) decreases monotonically, aligning with the original goal of these methods to decrease the loss (Fig. 10). However, this eliminates feedback for controlling the increase in \(\lambda_{t}\), resulting in significant progressive sharpening (Fig. 4, orange curve).

Consequently, when attempting to enforce monotonicity, learning rate tuners may inadvertently disrupt the non-linear stabilization that makes gradient descent robust and effective for training deep neural networks. Continually undershooting the EOS triggers a snowball effect of decreasing learning rate and increasing sharpness. If there is no corresponding increase in gradient norms, this causes optimization to slow down.

The poor performance of the classical learning rate tuners in Fig. 1 therefore appear strongly correlated with their tendency to _undershoot_ the edge of stability in the normalized sharpness coordinate \(y\). In the following, we focus on understanding tuners that prioritize training at or near the edge of stability.

## 3 Optimizing on the Edge of Stability

Based on our observations in Section 2, we design learning rate tuners that position the underlying optimizer **on the edge** of stability (\(y=0\)). We analyze a tuner capable of operating both slightly below and slightly above the EOS in order to exploit nonlinear stabilization.

Formally, we investigate a generalization of the quadratically greedy rule from Section 2, which sought \(\eta_{t}\) to minimize the quadratic approximation \(q_{f}\) in (2). We instead choose the learning rate to be _on edge_ by seeking the largest value of \(\eta\) such that \(q_{f}\) is smaller or equal to the original value of \(f\),

\[\eta_{t}^{\text{oe}}\coloneqq\max\{\eta\geq 0:q_{f}(\eta;w_{t},u_{t})\leq f(w_{t })\}=-2\frac{\nabla f(w_{t})^{\top}u_{t}}{u_{t}^{\top}\nabla^{2}f(w_{t})u_{t} }\,,\] (7)

where the last formula holds provided that \(u_{t}^{\top}\nabla^{2}f(w_{t})u_{t}>0\) (convex quadratic) and \(\nabla f(w_{t})^{\top}u_{t}<0\) (\(u_{t}\) is a descent direction).

Figure 5: **Enforcing optimizers to stay on edge (\(\sigma=2.0\)) improves performance over greedy approximation (\(\sigma=1.0\)). Train loss and learning rate behaviors for fine-tuned optimizers vs self-tuned counterparts with CDAT on various datasets, architectures, losses in a full batch regime. Tuning the learning rate “on edge” (\(\sigma\approx 2\)) improves performance over greedy tuning (\(\sigma=1\)) as well as constant learning rate.**

For \(u_{t}=-\nabla f(w_{t})\), and if \(-\nabla f(w_{t})\) is aligned with the eigendirection \(v_{\max}\) associated with the largest eigenvalue \(\lambda_{\max}\) of \(H\), we recover the familiar \(\eta_{t}^{\text{oe}}=2/\lambda_{\max}\). Note however, that contrarily to using directly \(\eta_{t}=2/\lambda_{\max}\), the on-edge rule can naturally take into account the alignment with \(v_{\max}\) (see Fig. 15). We note that we recover the edge of stability even when the updates are given by the gradient multiplied by a preconditioner, e.g. \(u_{t}=-P^{-1}\nabla f(w_{t})\) for a matrix \(P\). In this case, we have \(-u^{\top}Hu/u^{\top}g=g^{\top}P^{-1}HP^{-1}g/g^{\top}P^{-1}g\) for \(H=\nabla^{2}f(w_{t})\), \(g=\nabla f(w_{t})\). This is maximized when \(P^{-1/2}g\) lies in the largest eigendirection of the PSD matrix \(\tilde{H}\equiv P^{-1/2}HP^{-1/2}\), which for \(\sigma=2\) gives us the learning rate \(\eta_{t}=2/\tilde{\lambda}_{\max}\), where \(\tilde{\lambda}_{\max}\) is the maximum eigenvalue of \(\tilde{H}\). This is exactly the edge of stability for adaptive methods (Cohen et al., 2023).

We note that the only difference between this and the quadratically greedy rule is a factor of \(2\) in the numerator. Inspired by this observation, and with an eye towards robustness, we define our _Curvature Dynamics Aware Tuning_ (CDAT) rule by:

\[\eta_{t}^{\text{cdt}}=\sigma\frac{n_{t}}{d_{t}},\quad\text{for }n_{t}=\max\{- \nabla f(w_{t})^{\top}u_{t},0\},\;d_{t}=|u_{t}^{\top}\nabla^{2}f(w_{t})u_{t}| +\varepsilon.\] (8)

The scaling factor \(\sigma\) lets us interpolate between greedy (\(\sigma=1\)) and on-edge (\(\sigma=2\)). We are most interested in the behavior near \(\sigma=2\), (also studied in Rosca et al. (2023)). In (8), the \(\max\) function takes care of the case where \(u_{t}\) is an ascent direction (\(\nabla f(w_{t})^{\top}u_{t}>0\)), the absolute value takes care of cases where the objective has negative curvature in the update directions (see Appendix C for additional justification), and we simply set \(\varepsilon=0\) as we always observed non-negligible positive curvature. The definitions of the numerator \(n_{t}\) and the denominator \(d_{t}\) allow for the possibility of exponential moving averages (EMA) of each quantity such as \(\tilde{n}_{t+1}=(1-\beta_{\text{cdt}})n_{t}+\beta_{\text{cdt}}\tilde{n}_{t}\) for \(\beta_{\text{cdt}}\) referred to as the CDAT EMA parameter thereafter. We observed that smoothing the estimates of \(n_{t}\) and \(d_{t}\) by an EMA is particularly relevant when the updates are themselves defined through an exponential moving average as in Adam, or when using the proposed rule in a stochastic setting.

CDAT has two major advantages: it is sensitive to information from all eigenvalues of \(\nabla^{2}f(w_{t})\), and it depends on updates \(u_{t}\) coming from any base optimizer. We will take advantage of these properties to explore the behavior of "on edge" optimization in a variety of settings.

### On edge optimizers in practice

Full batch regime.Fig. 5 presents results for training with CDAT across various optimizers, architectures, datasets, and losses. Overall, selecting the learning rate to be on edge (\(\sigma=2\)) is on par with or better than a fine-tuned constant learning rate and is always better than a quadratically greedy approach (\(\sigma=1\)). This observation holds even though the quadratically greedy rule ensures larger instantaneous decrease (Fig. 16). One notes that targeting slightly above the edge (\(\sigma=2.0625\)) provides even better performance than the on edge rule (\(\sigma=2\)) on all examples except the MLP Mixer on CIFAR10. However, targeting higher above the edge (\(\sigma=2.5\)) generally gives diverging results in the short or long terms. To integrate the proposed rule with the Adam optimizer, we also observed that the estimation of the curvatures through \(n_{t}\), \(d_{t}\) in (8) was necessary.

Figure 6: **Optimizing on edge induces different curvature dynamics.** Sharpness, product between learning rate and sharpness, and gradient norm evolutions for gradient descent with CDAT. By putting the learning rate on edge (\(\sigma\approx 2\)), the sharpness does not ever increase and actually decreases slightly over time. GD with CDAT operates slightly above the edge constantly during training. Its gradient norm evolution is akin to a fine-tuned constant learning rate baseline.

Remarkably, all choices around the edge (\(1.9375,2.0,20625\)) show a progressive increase of the learning rate that results generally in a better performance than the constant learning rate counterparts, except for RMSProp on the NanoLM experiment. The increasing learning rate behavior is akin to the warm-up phase generally hard-coded by a scheduler. In Fig. 19, we observe that the CDAT rule displays similar behavior as warm-up schedules, yet it may not fully capture the benefits of prefixed schedules.

In Fig. 6, we analyze the dynamics of the curvature when optimizing on edge. We observe that the sharpness can be pushed to reduce over the iterations (\(1^{\text{st}}\) panel of Fig. 6). The CDAT rule may operate constantly slightly above the edge (\(2^{\text{nd}}\) panel of Fig. 6). By reducing the sharpness, the algorithm may be able to take larger stepsizes and converge faster. Sensitivity to architecure's width and depth, as well as weight decay, are also analyzed in Fig. 18.

Mini batch regime.The CDAT rule can be used in a stochastic regime by replacing \(f\) in (8) by its stochastic counterpart \(f^{(m_{t})}\) on a mini-batch \(m_{t}\). However, two difficulties may arise.

First, the on edge rule is motivated by the sharpening effects of the overall objective, which can be overestimated or underestimated by a single mini-batch. Previous work shows that the trace of the Hessian may best capture the sharpening and stabilization effects in a stochastic regime (Agarwala and Pennington, 2024; Wu and Su, 2023); it is unclear what function of the Hessian spectrum, the CDAT rule captures in the stochastic regime. As a result the optimal scaling factor may vary with the mini-batch. In Fig. 7, we observe that the optimal scaling of the on-edge rule is proportional to the batch size up to some size. In particular, at specific batch sizes, we observe that the greedy rule (\(\sigma=1\)) outperforms the on-edge rule. This result is consistent with the good performance of linesearches or greedy rules in a mini-batch regime previously mentioned and observed in Fig. 14. We also observe in Fig. 7, that integrating an EMA into the estimation of the edge in (8) smooth out the selection of the optimal scaling factor.

Finally, the sharpening effects are known to be generally mitigated in the stochastic regime (Agarwala and Pennington, 2024; Cohen et al., 2021; Jastrzebski et al., 2017). The benefits of the on edge rule appear also subdued in this regime (Fig. 8, Fig. 20, Fig. 21).

### Modeling CDAT dynamics

The classical optimization framework is insufficient to fully explain the benefits of CDAT. For example, on a convex quadratic objective, \(\sigma=1\) is the optimal choice, and \(\sigma>2\) results (in the worst case) in a divergent algorithm. However, we can use a simplified model to begin understanding the joint dynamics of the learning rate and sharpness under CDAT.

We approximate the gradients around a stationary point \(w_{\star}\), where \(\nabla f(w_{\star})=0\), as \(\nabla f(w_{t})\approx H\bar{w}_{t}\) for \(\bar{w}_{t}\coloneqq w_{t}-w_{\star}\), and \(H\) being a symmetric matrix. In this scenario, the learning rate given by CDAT is \(\eta_{t}^{\text{data}}=\sigma(\bar{w}_{t}^{\top}\,H^{2}\bar{w}_{t})/(\bar{w}_{ t}^{\top}\,H^{3}\bar{w}_{t})\). Consider the case where \(H\) has two eigenvalues \(\lambda\) and \(\nu\), with \(\lambda>\nu\geq 0\). In this case the CDAT learning rate can be written as

\[\eta_{t}^{\text{cdt}}=\sigma\frac{\lambda^{2}p_{t}+\nu^{2}g_{t}}{\lambda^{3}p_ {t}+\nu^{3}g_{t}}=\sigma\frac{\lambda^{2}p_{t}/g_{t}+\nu^{2}}{\lambda^{3}p_{t }/g_{t}+\nu^{3}}\,.\] (9)

Figure 7: **Stochasticity shifts the optimal scaling.** Normalized performance of gradient descent with momentum equipped with CDAT in a stochastic regime with varying batch sizes. In a mini-batch regime, the optimal scale decreases as the batch size decreases. Using an exponential moving average smooths out the performance of the CDAT rule over batch sizes.

Here \(p_{t},g_{t}\) are the projections \(p_{t}:=(\bar{u}_{t}^{\top}v)^{2},g_{t}:=(\bar{w}_{t}^{\top}v_{\perp})^{2}\), respectively onto the eigendirections \(v\) and \(v_{\perp}\) associated with \(\lambda\), \(\nu\). Therefore, \(\eta_{t}^{\text{cal}}\) interpolates between its minimum value \(\sigma/\lambda\) to the larger value \(\sigma/\nu\), depending on the alignment ratio \(p_{t}/g_{t}\). For \(2\nu/\lambda<\sigma<2\), this rule can achieve learning rates both above and below the EOS.

We can gain additional insight by modeling a dynamical \(\lambda_{t}\), extending the model of Section 2.3. While model (5) captures the dynamics in the largest eigendirection \(v\), here we aim to model the dynamics in the orthogonal subspace. To simplify, we consider the eigendirections \(v,v_{\perp}\), and small eigenvalue \(\nu\) fixed. We then model the gradients as \(\nabla f(w_{t})\approx H_{t}\bar{w}_{t}\) with \(H_{t}=\lambda_{t}vv^{\top}+\nu v_{\perp}v_{\perp}^{\top}\). If we update \(w\) in the direction \(v_{\perp}\) using gradient descent on \(\nu g_{t}\), we obtain the following dynamical system describing the CDAT learning rate tuner:

\[\eta_{t+1}=\sigma\frac{\lambda_{t}^{2}p_{t}+\nu^{2}g_{t}}{\lambda_{t}^{3}p_{t} +\nu^{3}g_{t}},\quad g_{t+1}=(1-\eta_{t}\nu_{t})^{2}g_{t},\quad p_{t+1}=(1+y_{ t})^{2}p_{t}\,.\] (10)

Combining this with the update rule for \(y_{t}\) given in (5) completes the model.

There are two important regimes of behavior in this model. First, if \(y_{t}>0\), \(p_{t}\) will increase and eventually \(y_{t}\) will decrease as in the normal EOS case. If \(y_{t}<0\), the key threshold is \(y_{t}<-\eta_{t}\nu_{t}\). In this case, the ratio \(p_{t}/g_{t}\)_decreases_ - leading to an increase in \(\eta_{t}\) according to the on edge rule. If \(a-bp_{t}>0\) (as it is if \(p_{t}\) has become small due to \(y_{t}<0\)), then we see from (5) that this leads to an _increase_ in \(y_{t}\). This suggests that CDAT has a tendency to push \(y_{t}\) closer to the EOS - sending \(y\) towards \(0\) if the learning rate is driven by the eigendirections corresponding to smaller eigenvalues.

Numerical simulations on this model (Fig. 9) suggest that this effect can indeed cause remarkably small values of \(y\) (\(3^{\text{rd}}\) panel of Fig. 9). We emphasize that this is due to the _joint dynamics_ of \(\eta_{t}\) (induced by the learning rate tuner), and \(\lambda_{t}\), \(p_{t}\), and \(g_{t}\) (induced by GD). There are also important limitations in this model's ability to fully explain CDAT's behavior. For example, the model predicts runaway sharpening for \(\sigma<2\) (\(2^{\text{nd}}\) panel of Fig. 9), and divergence for \(\sigma>2\). In practice, we saw a range of stable and useful settings for scale centered around \(2\). This modeling limitation likely stems from neglecting the dynamics orthogonal to \(v\) as well as higher-order terms, which empirically tend to stabilize EOS dynamics (Agarwala et al., 2023).

Figure 8: **The performance of CDAT is subdued in the stochastic regime.** Fine-tuned constant, scheduled, and self-tuned with CDAT learning rates in a stochastic regime. In a stochastic regime, CDAT can also exhibit a form of learning rate warm-up (top figure). However, the interplay between sharpening and learning rate are known to be mitigated in a stochastic regime which may explain the underperformance of CDAT in this regime (bottom figure).

## 4 Conclusion and Future Directions

Summary.Our empirical results showed that simple linesearches and approximate greedy learning rate tuners underperform constant learning rate approaches in the full batch regime - despite being better on individual steps. The idea that "locally greedy" methods perform poorly on long time scales has been shown in other settings as well, including evolutionary dynamics Agarwal and Fisher (2019). Our experiments and theoretical work suggest the failure of these classical tuners is due to the fact that they suppress the feedback which stabilizes sharpness in the fixed learning rate setting. As the sharpness increases, tuners are forced to take smaller steps, which ends up leading to slower learning.

We find, in contrast, that prioritizing stability of the sharpness yields tangible benefits. Our CDAT method pushes the network towards the edge of stability via a dynamically driven process. It also naturally displays some form of progressive increase of the learning rate akin to prefixed warm-up schedules. CDAT also sheds light on the more complicated dynamics in small mini batch regime, where estimation of a locally greedy rule may actually place the optimizer on the edge of stability of the full batch objective.

Limitations and future directions.We explored some limitations of the current modeling framework in Section 2.3 - in particular, the failure to capture stabilization due to higher order terms. Developing improved models (either analytically or numerically) would allow for powerful tools from other disciplines to aid algorithm design - particularly, methods from control theory. For example, state feedback schemes can be designed through the analysis of nonlinear dynamical systems to ensure asymptotic stabilization (Isidori, 1995, Chapter 7). We believe a cross disciplinary approach will be useful for designing the next generation of learning rate tuners.

The proposed CDAT rule may also help to understand and refine the design of learning rate schedules through scaling ladders (Wortsman et al., 2024). Recent work has shown that transfer of learning rates over different scales is related to consistency of curvature dynamics (Noci et al., 2024); this suggests that approaches like ours may be useful to increase predictability of optimal learning rates across scale.

Acknowledgements.We thank James Martens and Mihaela Rosca for fruitful discussions on related ideas. We also thank the reviewers for their insightful comments that helped us refine the manuscript.

Figure 9: **A simple model partially captures the benefits induced by the proposed CDAT rule.** Dynamics of theoretical model of CDAT (10). For \(\sigma=2\), feedback stabilizes \(y\) close to the EOS (\(y=0\)), which stabilizes \(\lambda_{\max}\) (orange). For \(\sigma=2-\epsilon\) and small \(\epsilon\) (blue, \(\epsilon=0.1\)), model predicts that \(\lambda_{\max}\) slowly grows (middle), but predicts that \(y\) stabilizes to a value \(-\epsilon\ll y_{t}<0\) (right).

## References

* Agarwal and Fisher (2019) Agarwal, A. and Fisher, D. S. (2019), 'Adaptive walks on high-dimensional fitness landscapes and seascapes with distance-dependent statistics', _Theoretical population biology_**130**, 13-49.
* Agarwal et al. (2023) Agarwal, A., Pedregosa, F. and Pennington, J. (2023), Second-order regression models exhibit progressive sharpening to the edge of stability, _in_ 'International Conference on Machine Learning', PMLR, pp. 169-195.
* Agarwal and Pennington (2024) Agarwal, A. and Pennington, J. (2024), 'High dimensional analysis reveals conservative sharpening and a stochastic edge of stability', _arXiv preprint arXiv:2404.19261_.
* Almeida et al. (1999) Almeida, L. B., Langlois, T., Amaral, J. D. and Plakhov, A. (1999), Parameter adaptation in stochastic optimization, _in_ 'On-line learning in neural networks', pp. 111-134.
* Armijo (1966) Armijo, L. (1966), 'Minimization of functions having Lipschitz continuous first partial derivatives', _Pacific Journal of mathematics_**16**(1), 1-3.
* Arora et al. (2022) Arora, S., Li, Z. and Panigrahi, A. (2022), Understanding gradient descent on the edge of stability in deep learning, _in_ 'International Conference on Machine Learning', PMLR, pp. 948-1024.
* Ba et al. (2016) Ba, J. L., Kiros, J. R. and Hinton, G. E. (2016), 'Layer normalization', _arXiv preprint arXiv:1607.06450_.
* Baydin et al. (2018) Baydin, A. G., Cornish, R., Rubio, D. M., Schmidt, M. and Wood, F. (2018), Online learning rate adaptation with hypergradient descent, _in_ 'International Conference on Learning Representations'.
* Berrada et al. (2020) Berrada, L., Zisserman, A. and Kumar, M. P. (2020), Training neural networks for and by interpolation, _in_ 'International conference on machine learning', PMLR.
* Blondel and Roulet (2024) Blondel, M. and Roulet, V. (2024), 'The Elements of Differentiable Programming', _arXiv preprint arXiv:2403.14606_.
* Cohen et al. (2023) Cohen, J., Ghorbani, B., Krishnan, S., Agarwal, N., Medapati, S., Badura, M., Suo, D., Cardoze, D., Nado, Z., Dahl, G. E. and Gilmer, J. (2023), Adaptive gradient methods at the edge of stability, _in_ 'NeurIPS 2023 Workshop Heavy Tails in Machine Learning'.
* Cohen et al. (2021) Cohen, J., Kaur, S., Li, Y., Kolter, J. Z. and Talwalkar, A. (2021), Gradient descent on neural networks typically occurs at the edge of stability, _in_ 'International Conference on Learning Representations'.
* Cutkosky et al. (2023) Cutkosky, A., Defazio, A. and Mehta, H. (2023), Mechanic: A learning rate tuner, _in_ 'Advances in Neural Information Processing Systems'.
* Dagreou et al. (2024) Dagreou, M., Ablin, P., Vaiter, S. and Moreau, T. (2024), How to compute hessian-vector products?, _in_ 'ICLR Blogposts 2024'. https://iclr-blogposts.github.io/2024/blog/bench-hvp/.
* Damian et al. (2023) Damian, A., Nichani, E. and Lee, J. D. (2023), Self-stabilization: The implicit bias of gradient descent at the edge of stability, _in_ 'International Conference on Learning Representations'.
* DeepMind et al. (2020) DeepMind, Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky, P., Budden, D., Cai, T., Clark, A., Danihelka, I., Dedieu, A., Fantacci, C., Godwin, J., Jones, C., Hemsley, R., Hennigan, T., Hessel, M., Hou, S., Kapturowski, S., Keck, T., Kemaev, I., King, M., Kunesch, M., Martens, L., Merzic, H., Mikulik, V., Norman, T., Papamakarios, G., Quan, J., Ring, R., Ruiz, F., Sanchez, A., Sartran, L., Schneider, R., Sezener, E., Spencer, S., Srinivasan, S., Stanojevic, M., Stokowiec, W., Wang, L., Zhou, G. and Viola, F. (2020), 'The DeepMind JAX Ecosystem'. **URL:**_http://github.com/google-deepmind_
* Defazio and Mishchenko (2023) Defazio, A. and Mishchenko, K. (2023), Learning-rate-free learning by d-adaptation, _in_ 'International Conference on Machine Learning', PMLR, pp. 7449-7479.
* Dehghani et al. (2022) Dehghani, M., Gritsenko, A., Arnab, A., Minderer, M. and Tay, Y. (2022), Scenic: A jax library for computer vision research and beyond, _in_ 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)', pp. 21393-21398.
* Dehghani et al. (2020)Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K. and Fei-Fei, L. (2009), Imagenet: A large-scale hierarchical image database, _in_ '2009 IEEE Conference on Computer Vision and Pattern Recognition', IEEE, pp. 248-255.
* Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J. and Houlsby, N. (2021), An image is worth 16x16 words: Transformers for image recognition at scale, _in_ 'International Conference on Learning Representations'.
* Galli et al. (2023) Galli, L., Rauhut, H. and Schmidt, M. (2023), Don't be so monotone: Relaxing stochastic line search in over-parameterized models, _in_ 'Advances in Neural Information Processing Systems'.
* Ghorbani et al. (2019) Ghorbani, B., Krishnan, S. and Xiao, Y. (2019), An investigation into neural net optimization via Hessian eigenvalue density, _in_ 'International Conference on Machine Learning', PMLR, pp. 2232-2241.
* Gilmer et al. (2022) Gilmer, J., Ghorbani, B., Garg, A., Kudugunta, S., Neyshabur, B., Cardoze, D., Dahl, G. E., Nado, Z. and Firat, O. (2022), A loss curvature perspective on training instabilities of deep learning models, _in_ 'International Conference on Learning Representations'.
* Goyal et al. (2017) Goyal, P., Dollar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y. and He, K. (2017), 'Accurate, large minibatch SGD: Training imagenet in 1 hour', _arXiv preprint arXiv:1706.02677_.
* He et al. (2016) He, K., Zhang, X., Ren, S. and Sun, J. (2016), Deep residual learning for image recognition, _in_ 'Proceedings of the IEEE conference on computer vision and pattern recognition', pp. 770-778.
* Hinton et al. (2012) Hinton, G., Nitish, S. and Swersky, K. (2012), 'Divide the gradient by a running average of its recent magnitude', _COURSERA: Neural Networks for Machine Learning_.
* Hochreiter et al. (2001) Hochreiter, S., Bengio, Y., Frasconi, P. and Schmidhuber, J. (2001), 'Gradient flow in recurrent nets: the difficulty of learning long-term dependencies', _A Field Guide to Dynamical Recurrent Neural Networks_ pp. 237-244.
* Howard (2019) Howard, J. (2019), 'imagenette'.
* **URL:**_https://github.com/fastai/imagenette/_
* Ioffe and Szegedy (2015) Ioffe, S. and Szegedy, C. (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift, _in_ 'International conference on machine learning', pmlr, pp. 448-456.
* Isidori (1995) Isidori, A. (1995), _Nonlinear control systems_, third edn, Springer.
* Ivgi et al. (2023) Ivgi, M., Hinder, O. and Carmon, Y. (2023), DoG is SGD's best friend: A parameter-free dynamic step size schedule, _in_ 'International Conference on Machine Learning', PMLR, pp. 14465-14499.
* Jastrzebski et al. (2017) Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y. and Storkey, A. (2017), 'Three factors influencing minima in SGD', _arXiv preprint arXiv:1711.04623_.
* Jastrzebski et al. (2019) Jastrzebski, S., Kenton, Z., Ballas, N., Fischer, A., Bengio, Y. and Storkey, A. (2019), On the relation between the sharpest directions of DNN loss and the SGD step length, _in_ 'International Conference on Learning Representations'.
* Jastrzebski et al. (2020) Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho, K. and Geras, K. (2020), The break-even point on optimization trajectories of deep neural networks, _in_ 'International Conference on Learning Representations'.
* Kalra et al. (2023) Kalra, D. S., He, T. and Barkeshli, M. (2023), 'Universal sharpness dynamics in neural network training: Fixed point analysis, edge of stability, and route to chaos', _arXiv preprint arXiv:2311.02076_.
* Karpathy (2015) Karpathy, A. (2015), 'The unreasonable effectiveness of recurrent neural networks', http://karpathy.github.io/2015/05/21/rnn-effectiveness/.
* Kopitkov and Indelman (2020) Kopitkov, D. and Indelman, V. (2020), Neural spectrum alignment: Empirical study, _in_ 'International Conference on Artificial Neural Networks', pp. 168-179.
* Krizhevsky et al. (2014)Krizhevsky, A., Hinton, G. et al. (2009), Learning multiple layers of features from tiny images, Technical report, University of Toronto, ON, Canada.
* LeCun et al. (2010) LeCun, Y., Cortes, C. and Burges, C. (2010), 'Mnist handwritten digit database', _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_**2**.
* Lewkowycz et al. (2020) Lewkowycz, A., Bahri, Y., Dyer, E., Sohl-Dickstein, J. and Gur-Ari, G. (2020), 'The large learning rate phase of deep learning: the catapult mechanism', _arXiv preprint arXiv:2003.02218_.
* Li et al. (2018) Li, H., Xu, Z., Taylor, G., Studer, C. and Goldstein, T. (2018), 'Visualizing the loss landscape of neural nets', _Advances in neural information processing systems_**31**.
* Liu and Nocedal (1989) Liu, D. C. and Nocedal, J. (1989), 'On the limited memory bfgs method for large scale optimization', _Mathematical programming_.
* Liu et al. (2024) Liu, H., Li, Z., Hall, D. L. W., Liang, P. and Ma, T. (2024), Sophia: A scalable stochastic second-order optimizer for language model pre-training, _in_ 'International Conference on Learning Representations'.
* Liu et al. (2020) Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J. and Han, J. (2020), On the variance of the adaptive learning rate and beyond, _in_ 'International Conference on Learning Representations'.
* Loizou et al. (2021) Loizou, N., Vaswani, S., Laradji, I. H. and Lacoste-Julien, S. (2021), Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence, _in_ 'International Conference on Artificial Intelligence and Statistics', PMLR.
* Loshchilov and Hutter (2016) Loshchilov, I. and Hutter, F. (2016), SGDR: Stochastic gradient descent with warm restarts, _in_ 'International Conference on Learning Representations'.
* Martens and Grosse (2015) Martens, J. and Grosse, R. (2015), Optimizing neural networks with Kronecker-factored approximate curvature, _in_ 'International conference on machine learning', PMLR, pp. 2408-2417.
* Mishchenko and Defazio (2023) Mishchenko, K. and Defazio, A. (2023), 'Prodigy: An expeditiously adaptive parameter-free learner', _arXiv preprint arXiv:2306.06101_.
* Mutschler and Zell (2020) Mutschler, M. and Zell, A. (2020), 'Parabolic approximation line search for DNNs', _Advances in Neural Information Processing Systems_**33**, 5405-5416.
* Nesterov et al. (2018) Nesterov, Y. et al. (2018), _Lectures on convex optimization_, Vol. 137, Springer.
* Nocedal and Wright (1999) Nocedal, J. and Wright, S. J. (1999), _Numerical optimization_, Springer.
* Noci et al. (2024) Noci, L., Meterez, A., Hofmann, T. and Orvieto, A. (2024), 'Why do learning rates transfer? reconciling optimization and scaling limits for deep learning', _arXiv preprint arXiv:2402.17457_.
* Polyak (1964) Polyak, B. T. (1964), 'Some methods of speeding up the convergence of iteration methods', _USSR computational mathematics and mathematical physics_**4**(5), 1-17.
* Riedmiller and Braun (1992) Riedmiller, M. and Braun, H. (1992), Rprop: a fast adaptive learning algorithm, _in_ 'Proc. of the Int. Symposium on Computer and Information Science VII'.
* Rosca et al. (2023) Rosca, M., Wu, Y., Qin, C. and Dherin, B. (2023), 'On a continuous time model of gradient descent dynamics and instability in deep learning', _Transactions on Machine Learning Research_.
* Roulet et al. (2023) Roulet, V., Agarwala, A. and Pedregosa, F. (2023), On the interplay between stepsize tuning and progressive sharpening, _in_ 'OPT 2023: Optimization for Machine Learning'.
* Schaul et al. (2013) Schaul, T., Zhang, S. and LeCun, Y. (2013), No more pesky learning rates, _in_ 'International conference on machine learning', PMLR, pp. 343-351.
* Shi et al. (2023) Shi, B., Su, W. and Jordan, M. I. (2023), 'On learning rates and Schrodinger operators', _Journal of Machine Learning Research_**24**(379), 1-53.
* Tolstikhin et al. (2021) Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J. et al. (2021), 'MLP-mixer: An all-MLP architecture for vision', _Advances in neural information processing systems_**34**, 24261-24272.
* Tolstikhin et al. (2020)Vaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G. and Lacoste-Julien, S. (2019), 'Painless stochastic gradient: Interpolation, line-search, and convergence rates', _Advances in neural information processing systems_.
* Wortsman et al. (2024) Wortsman, M., Liu, P. J., Xiao, L., Everett, K. E., Alemi, A. A., Adlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-Dickstein, J., Xu, K., Lee, J., Gilmer, J. and Kornblith, S. (2024), Small-scale proxies for large-scale transformer training instabilities, _in_ 'International Conference on Learning Representations'.
* Wu et al. (2018) Wu, L., Ma, C. et al. (2018), 'How SGD selects the global minima in over-parameterized learning: A dynamical stability perspective', _Advances in Neural Information Processing Systems_**31**.
* Wu and Su (2023) Wu, L. and Su, W. J. (2023), The implicit regularization of dynamical stability in stochastic gradient descent, _in_ 'International Conference on Machine Learning', PMLR, pp. 37656-37684.

Theoretical Model

### Intuition for model of curvature dynamics

In Section 2.3, we extend the model from Damian et al. (2023) in order to understand the curvature dynamics. Here we provide some intuition for the basic structure of the model.

The analysis in Damian et al. (2023) is based on a \(3\)rd order expansion of the loss function. A third order approximation of \(f\) around some \(w_{0}\) reads

\[f(w) \approx f(w_{0})+\nabla f(w_{0})\cdot(w-w_{0})+\frac{1}{2}(w-w_{0})^{ \top}\nabla^{2}f(w_{0})(w-w_{0})\] \[+\frac{1}{6}\nabla^{3}f(w_{0})[w-w_{0},w-w_{0},w-w_{0}].\]

Assuming without loss of generality that \(w_{0}=0\), \(f(w_{0})=0\), we can write:

\[f(w)\approx\nabla f(w_{0})\cdot w+\frac{1}{2}w^{\top}\nabla^{2}f(w_{0})w+\frac {1}{6}\nabla^{3}f(w_{0})[w,w,w]\] (11)

Consider the gradient descent dynamics with learning rate \(\eta\) under this model. We can write:

\[w_{t+1}-w_{t}\approx-\eta(\nabla f(w_{0})+H(w_{t})w_{t})\] (12)

where \(H(w_{t})\equiv\nabla^{2}f(w_{0})+\frac{1}{2}\nabla^{3}f(w_{0})[w_{t},\cdot,\cdot]\) is the Hessian at the current parameter \(w_{t}\).

Let \(v\) be the direction of the largest eigenvalue of \(H(w_{0})\). For small enough \(w_{t}-w_{0}\), this is a good approximation of the largest eigendirection of \(H(w_{t})\). Consider the dynamics of the projection \(x_{t}\equiv v\cdot w_{t}\). We make the additional assumption that \(v\cdot\nabla f(w_{0})=0\) (which can be achieved with a coordinate transformation). We then get

\[x_{t+1}\approx(1-\eta\lambda_{t})x_{t}\] (13)

where \(\lambda_{t}\coloneqq\lambda(w_{t})\coloneqq\lambda_{\max}H(w_{t})\) is the largest eigenvalue of the current Hessian \(H(w_{t})\). The dynamics of \(\lambda_{t}\) are generally slower than the dynamics of \(w_{t}\) (and \(x_{t}\)), and are governed approximately by

\[\lambda_{t+1}-\lambda_{t}\approx(\nabla\lambda(w_{t}))\cdot(w_{t+1}-w_{t})=- \eta(\nabla\lambda(w_{t}))\cdot\nabla f(w_{t})\] (14)

This gradient is given by

\[\nabla\lambda(w_{t})=\nabla(v^{\top}H(w_{t})v)\approx\nabla^{3}f(w_{0})[v,v, \cdot].\] (15)

For very small \(x_{t}\), it has been observed (see e.g. Cohen et al. (2021)) that \(\lambda_{t+1}-\lambda_{t}\) is increasing during most of training. Early in this regime \(x_{t}\) is often very close to \(0\) (that is, the gradient has small component in the \(v\) direction). Therefore we are interested in the contribution of the gradient orthogonal to \(v\), \(\nabla f(w_{t})_{\perp}\equiv(I-vv^{\mathrm{T}})\nabla f(w_{t})\), to the dynamics. We define \(a_{t}\equiv-(\nabla\lambda(w_{t})\cdot\nabla f(w_{t})_{\perp})\) as the instantaneous change in the top eigenvalue, due to gradient contributions orthogonal to \(v\). We assume that this contribution is positive and, to simplify, independent of \(t\), that is, \(a_{t}\coloneqq a>0\).

From (13), while \(\lambda_{t}<2/\eta\), \(x_{t}\) is decreasing and from (14) \(\lambda_{t}\) is increasing.

Once \(\lambda_{t}>2/\eta\), the dynamics of \(x_{t}\) changes from convergence towards \(0\) to increasing values with alternating sign. We can write down the contribution to the gradient from \(x_{t}\) using our third order expansion of the loss. We have:

\[\nabla_{w}f(w_{0}+x_{t}v)\approx\nabla f(w_{0})+x_{t}H(w_{t})v+\frac{1}{2}x_{t }^{2}\nabla^{3}f(w_{0})[v,v,\cdot]\] (16)

We can now understand how these terms contribute to the dynamics of \(\lambda_{t}\). The first term contributes to sharpening (increasing \(\lambda_{t}\)) via the constant \(a\). The second term has oscillating sign and its long term contribution to \(\lambda_{t}\) is small. From Equation 13 we know that

\[x_{t+1}+x_{t}\approx(2-\eta\lambda_{t})x_{t}\] (17)

Near the edge of stability, \(\eta\lambda_{t}\) is close to \(2\), and the sum of successive \(x_{t}\) are small; therefore we neglect the linear \(x_{t}\) term in Equation 16.

The sign of the \(x_{t}^{2}\) term is constant and there are no such issues. Therefore, in the non-linear regime the dynamics of \(\lambda_{t}\) can be approximated by:

\[\lambda_{t+1}-\lambda_{t}\approx\eta a-\frac{\eta}{2}x_{t}^{2}\left(\nabla^{3}f( w_{0})[v,v,\cdot]\cdot\nabla^{3}f(w_{0})[v,v,\cdot]\right)\] (18)

If we define \(b\equiv\frac{1}{2}\left(\nabla^{3}f(w_{0})[v,v,\cdot]\cdot\nabla^{3}f(w_{0})[v,v,\cdot]\right)\) (guaranteed to be positive), then we arrive at the dynamical equations presented in Equation 4:

\[x_{t+1}-x_{t}=(2-\eta\lambda_{t})x_{t},\ \lambda_{t+1}-\lambda_{t}=\eta(a-bx_{t} ^{2})\] (19)

This system leads to a quasi-stable oscillation of \(\lambda_{t}\) around \(2/\eta\); when \(\lambda_{t}<2/\eta\), \(x_{t}\) goes to \(0\), and eventually \(\lambda_{t}\) increases; when \(\lambda_{t}>2/\eta\), \(x_{t}\) increases in magnitude, eventually leading to a decrease in \(\lambda_{t}\).

This heuristic argument is made rigorous in the appropriate regimes in Damian et al. (2023); there they compute the difference between the full dynamics and the dynamics with the gradient projected away from the largest eigendirection using a coupling argument. The key point is that higher order contributions to the gradient are guaranteed to be anti-aligned with the gradient of the large Hessian eigenvalues. This leads to a _restoring force_ which counteracts eigenvalue growth (progressive sharpening) if the largest eigenmode becomes unstable.

### Dynamics of rescaled variables

In Section 2.3, we used Equation 4 to derive equations in \(p\coloneqq x^{2}\) and \(y\coloneqq\eta\lambda-2\). We arrived at

\[p_{t+1}=(1+y_{t})^{2}p_{t},\ y_{t+1}=\eta_{t+1}\left[\eta_{t}\left(a-bp_{t} \right)\right]+\left(\frac{\eta_{t+1}}{\eta_{t}}\right)y_{t}+2\left[\frac{ \eta_{t+1}}{\eta_{t}}-1\right].\] (20)

The dynamical equation for \(p\) is obtained by squaring the equation for \(x_{t}\) in Equation 4. To derive the equation for \(y_{t}\), we first derive the dynamics of \(\eta_{t}\lambda_{t+1}\) as

\[\eta_{t}\lambda_{t+1}=\eta_{t}\left[\eta_{t}\left(a-bp_{t}\right)\right]+\eta _{t}\lambda_{t}.\] (21)

We then have

\[y_{t+1}=\frac{\eta_{t+1}}{\eta_{t}}[\eta_{t}\lambda_{t+1}-2]+2.\] (22)

Evaluating completes Equation 5.

### Dynamics of the projection on the largest eigenvector

We present results on the dynamics of \(p\) in the various models; these were omitted from the main text in order to simplify the presentation. For constant learning rate, \(p\) initially decreases until EOS is crossed, after which it enters into a cycle of increase and decrease (Figure 10, blue). For our model of linesearches, where \(\eta_{t}=2(1-\epsilon)/\lambda_{\max,t}\), \(p\) decays to \(0\) quickly and there is no mediation of sharpening (orange, \(\epsilon=0.1\)).

For our model of CDAT presented in Section 3.2, \(p\) stabilizes for \(\sigma=2\) (Figure 11, orange). For \(\sigma<2\), the model still predicts decay of \(p\), but the ratio of \(p_{t}\) to the orthgonal component \(g_{t}\) remains constant (Figure 11, blue). This fixed ratio stabilizes \(y\) to a value near \(0\).

In practice, the higher order terms in the dynamics provide additional stability, in the on-edge model, which allows \(p\) to stabilize as well, see Fig. 12, Fig. 17. The key is that these terms can operate when \(y\) is close to \(0\) for long periods of time. These results suggest that additional model development is required to understand the behavior of learning rate tuners which target the EOS.

## Appendix B Additional Experiments

### Further analyzes of base learning rate tuners

Fig. 12 completes Fig. 3 with measures of sharpening and learning rates on the settings considered in Fig. 1. For RMSProp we considered the preconditioned Hessian following the observations done by Cohen et al. (2023) that for adaptive gradient methods such as RMSProp or Adam, the sharpness of the preconditioned Hessian, rather than the sharpness of the Hessian, defines the edge of stability. Namely, recall that RMSProp takes updates of the form

\[w_{t+1}=w_{t}-P_{t}^{-1}\nabla f(w_{t}),\quad\text{for}P_{t}=\mathrm{diag}( \sqrt{\nu_{t}+\varepsilon})\]

for

\[\nu_{t}=(1-\beta_{2})g_{t}^{2}+\beta_{2}\nu_{t-1},\quad\nu_{-1}=0,\ g_{t}= \nabla f(w_{t}),\]

with \(\beta_{2}\) an exponential moving average parameter. The preconditioned Hessian takes then the form

\[\tilde{H}_{t}=P_{t}^{-1/2}\nabla^{2}f(w_{t})P_{t}^{-1/2},\]

and we report \(\lambda_{\max}(\tilde{H}_{t})\).

We observe similar behaviors in these regimes as in Fig. 3. Namely, the sharpness or preconditioned sharpness ever increase (\(2^{\text{nd}}\) panels of Fig. 12), while the learning rates ever decrease (\(1^{\text{st}}\) panels of Fig. 12). The constant learning counterpart can operate above the edge of stability while the self-tuned methods generally avoid crossing the edge of stability (\(3^{\text{rd}}\) panels of Fig. 12).

### Analyzing additional learning rate tuners

We consider the performance of two additional classical learning rate tuners, Polyak stepsize (Berrada et al., 2020; Loizou et al., 2021; Polyak, 1964) and hyper-gradient descent (Almeida et al., 1999; Baydin et al., 2018) akin to the resilient backpropagation scheme (Riedmiller and Braun, 1992).

Briefly, Polyak stepsizes consider setting the learning rate as

\[\eta_{t}=\min\left\{\frac{f(w_{t})-f^{\star}}{\|\nabla f(w_{t})\|^{2}},\eta_ {\max}\right\},\] (23)

where \(f^{\star}=\min_{w}f(w)\) is the minimum of the objective set to \(0\) by assuming that a neural network can overfit the data, and \(\eta_{\max}\) is a maximal stepsize selected as \(1\) or \(100\) in our experiments (we take the best instance).

Hyper-gradient descent considers updating the stepsize towards maximal decrease of the objective. Namely, defining the objective obtained after one step \(h_{t}(\eta)=f(w_{t}+\eta u_{t})\), the algorithm updates \(\eta_{t}\) by a gradient step on \(h_{t}\) resulting a priori in \(\eta_{t+1}=\eta_{t}-\alpha\nabla f(w_{t}+\eta_{t}u_{t})^{\top}u_{t}\) for a given hyper-learning rate \(\alpha\). Almeida et al. (1999); Baydin et al. (2018) argued for using multiplicative updates of the form

\[\eta_{t+1}=\eta_{t}\left(1-\beta\frac{\nabla f(w_{t}+\eta_{t}u_{t})^{\top}u_{ t}}{\|\nabla f(w_{t}+\eta_{t}u_{t})\|_{2}\|u_{t}\|_{2}}\right).\] (24)

Intuitively, the learning rate increases if the update is aligned with the negative gradient direction and decreases otherwise. Resilient backpropagation (Riedmiller and Braun, 1992) adopts a similar logic componentwise. In our experiments we vary \(\beta\) and select the best instance, see Appendix C.5 for more details.

We observe that Polyak stepsizes (top figure of Fig. 13) generally select larger learning rates than the constant learning rate counterpart. The efficiency of Polyak stepsizes is not reached by the CDAT rule with \(\sigma=2\) but with a slightly larger scale \(\sigma=2.06\). The efficiency of the Polyak stepsize method, in particular compared to a simple linesearch, in a full batch regime, has also been reported by Roulet et al. (2023). The proposed CDAT rule may capture the benefits of aggressive learning rates taken by Polyak stepsizes in a smoother way by allowing various scales.

On the other hand, the hyper-gradient descent performs just on par with the fine-tuned constant learning rate counterpart (bottom figure of Fig. 13). We also observe a slow, yet steady, progressive sharpening when using the hyper-gradient descent. As with the linesearch method or the quadratically greedy rule, the hyper-gradient descent focuses on selecting a learning rate that decreases the loss, which appears, across those tuners, to potentially suppress effective stabilization effects naturally appearing with constant learning rate.

### Base learning rate tuners in a stochastic regime

In Fig. 14, we report the performance of classical learning rate tuners (linesearch or quadratically greedy method) in a stochastic regime for varying batch-sizes. As observed previously by Vaswani et al. (2019) or Roulet et al. (2023), a linesearch for example can perform well in a stochastic regime. Note that the two approaches (linesearch and quadratically greedy method) display similar behaviors (just as they displayed similar behaviors in the full batch regime). This hints that, rather than playing with the numerous hyperparameters of a linesearch we may focus simply on an additional scaling factor for the quadratically greedy rule, which motivated the proposed CDAT rule.

### Targeting the edge of stability using the exact sharpness

Cohen et al. (2021, Appendix F) reported bad performance of adaptive learning rate tuners selecting the stepsize as

\[\eta=2/\lambda_{\max}(\nabla^{2}f(w))\,,\]

which may fix the learning rate just at the edge of stability. Note that such a definition does not take into account the additional alignment of the update with the largest eigenvector. Our proposed diagnostic rule CDAT rather considers the edge of stability given by a local approximation of the objective along the update so to take into account the alignment of the update with the largest eigenvector of the Hessian. We ran experiments with a rule

\[\eta=\sigma/\lambda_{\max}(\nabla^{2}f(w))\,,\] (25)

that lets the scaling factor vary just as done with CDAT. The only difference is in the estimation of the base estimate of the edge of stability (CDAT does it with the help of a quadratic approximation of the objective, while the rule (25) uses an exact computation of the sharpness). In Fig. 15, we observe that setting the scale \(\sigma\approx 2\) leads to poor performance as previously observed by Cohen et al. (2021). Note however that by setting the scaling much above \(2\) (like \(\sigma=3\)) such a rule may outperform a constant learning rate. This hints that the rule (25) misses the alignment of the update with the largest eigenvector, which motivated the CDAT rule.

### Analyzing instantaneous gains versus long-term gains

In Fig. 16, we investigate the difference of instantaneous decrease using the quadratically greedy rule (CDAT with \(\sigma=1\)) compared to the on edge rule (CDAT with \(\sigma=2\)). Throughout a training, the quadratically rule ensures a larger instantaneous decrease as intended through its definition as a learning rate that minimizes the loss. Yet, in the long term, the quadratically greedy rule underperforms the on edge rule (Fig. 5).

### Additional metrics for the CDAT rule

In Fig. 17, we additionally measure the alignment of the updates with the largest eigenvector and the angle between successive updates. We observe that the CDAT rule for \(\sigma\approx 2\) behaves similarly as the constant learning rate counterpart. In particular, the updates tend to quickly be in opposed directions. The quadratically greedy rule does not demonstrate such a behavior.

### Sensibility analysis to architecture hyperparameters

In Fig. 18, we study CDAT for simple MLPs in a full batch regime on the MNIST dataset. Our goal is to understand the benefits of the proposed CDAT rule for varying hyperparameters. First, we analyze the sensibility to width and depth of an MLP in a similar fashion as Cohen et al. (2021, Appendix D) did to analyze progressive sharpening.

We observe that accrued gains can be obtained with the CDAT rule for larger widths (top left panel of Fig. 18). Note that Cohen et al. (2021, Appendix D) found less sharpening at higher widths. In terms of depth (top right panel of Fig. 18), the CDAT rule works best with larger depths while we note a slight shift of optimal scaling factors from \(2\) to slightly below \(2\).

The CDAT rule appears to work best with small or no weight decay (bottom left panel of Fig. 18) while its benefits fade with larger weight decay (no difference between greedy \(\sigma=1\) and on edge \(\sigma=2\)). Finally, while the method naturally finds smaller train losses with larger subsets of data, we do not observe a significant shift of relative performance between scales as the size of the data increases (bottom right panel of Fig. 18).

### CDAT rule versus prefixed schedule in full batch regime

In Fig. 19, we compare the proposed CDAT rule with prefixed schedules in a full batch regime. We observe that while placing the optimizer on edge could improve on constant learning rate counterparts, prefixed schedules can outperform the CDAT rule. This points out that the feedback loop exploited by CDAT may miss some additional nonlinear effects that could further enhance self-tuning rules.

### Detailed performances of CDAT in stochastic regime

In Fig. 20 and Fig. 21, we detail the performances of the CDAT rule in the stochastic regime for varying batch sizes. In the stochastic regime, recall that the heatmap of the performance of CDAT in terms of batch-size heavily depended on the appropriate scaling factor Fig. 7 (in comparison a scaling factor of approximately \(\sigma=2\) appeared generally good in the full batch regime). In both Fig. 20 and Fig. 21, we observe that the method may generally work better at larger batch sizes. Understanding better the right statistics to estimate as well as appropriate estimators of the edge of stability in a stochastic regime is a future direction.

Figure 12: **Learning rates and sharpness dynamics of baseline learning rate tuners.** Learning rate, Hessian or preconditioned Hessian sharpness, their product, and the alignment between the update and the largest eigenvector of the Hessian or the preconditioned Hessian. As in Fig. 12, we observe that a linesearch (1) or a quadratically greedy (3) learning rate tuner display decreasing learning rates along training. The sharpness of the Hessian (for GD) or preconditioned Hessian (for RMSProp) keep increasing for the self-tuned baselines while they stabilize for the constant learning rate counterparts. The self-tuned methods perform generally below the edge of stability or at least much less above than the constant learning rate counterpart.

Figure 13: **Analyzing additional learning rate tuners.** Train loss, learning rate, sharpness, and their product along training with gradient descent using a constant or self-tuned learning rate with various tuners. Polyak stepizes (23) are effective in a full batch regime (top figure) outperforming CDAT on edge (\(\sigma=2\)). The effectiveness of the Polyak stepsizes are partially captured by an aggressive CDAT rule placing the optimizer on edge \(\sigma=2.06\). On the other hand, a hyper-gradient descent (24) performs just on par with the constant learning rate counterpart in this regime. It also displays an ever-increasing sharpening akin to the one observed for a linesearch or the quadratically greedy rule Fig. 3.

Figure 14: **Classical learning rate tuners can work well in stochastic regime.** In a stochastic regime, we observe that, e.g., a linesearch can perform on par or even better than the constant learning counterparts. However, this good performance is not explained by the common belief that linesearches work well in a full batch regime (Fig. 1). The linesearch and quadratically greedy rule perform similarly in this setting.

Figure 15: **On edge with exact sharpness.** We consider a rule similar to CDAT (8) but using the exact sharpness, that is the largest eigenvalue of the Hessian, as a base learning rate while varying an additional scale factor (see (25)). By using the exact sharpness, a scaling factor of \(\sigma=2\) leads now to poor performance, while a scaling factor of \(\sigma=3\) is performant. By using the exact sharpness (25) we do not take into account the actual alignment of the update with the largest eigenvector which may explain the shift of optimal scaling factors in this case.

Figure 16: **The quadratically greedy rule ensures larger instantaneous decrease of the loss.** Train losses and difference in loss between one step using the on-edge rule (CDAT, scale=2) and one step using quadratically greedy rule (CDAT, scale=1). Results averaged over 10 initializations and disjoint 4096-sample subsets of CIFAR100. MLP architecture: single hidden layer of size 1024, ReLU activations, trained with GD and cross-entropy loss in a full batch setting. We plot \(f(w_{t}+\eta^{\text{oe}}u_{t})-f(w_{t}+\eta^{\text{oe}}u_{t})\) along a training performed either with the quadratically greedy rule (\(\sigma=1\)) or the on-edge rule (\(\sigma=2\)). In both cases, this difference is positive meaning that the quadratically greedy rule ensures a larger instantaneous decrease of the loss. Yet the quadratically greedy rule underperforms in the long term (see Fig. 5, also holds in this full batch setting).

Figure 17: **Further analysis of the CDAT rule.** This is the same setting as in Fig. 6 except that we plot the alignment between the largest eigenvector of the Hessian and the update, and the angle between successive updates. We observe that the CDAT rule for \(\sigma\approx 2\) behaves similarly as the constant learning rate counterpart. In particular, the updates tend to quickly be in opposed directions. The quadratically greedy rule does not demonstrate such a behavior.

Figure 19: **CDAT rule may not fully capture the benefits of pre-fixed schedules**. Train loss and learning rate behaviors for fine-tuned optimizers with or without schedules vs self-tuned counterparts with CDAT on various architecture, datasets, losses in a full batch regime. While the CDAT rule displays a behavior to warmup schedules, it does not completely catch the benefits of pre-fixed schedules. Note for the top-left part that the performance of the pre-fixed schedule is akin to the performance reported with CDAT \(\sigma=2.5\) at early times suggesting that a varying scaling factor, or taking higher order dynamics may be important to fully capture the benefits of warm-up schedules.

Figure 18: **Improvements of CDAT rule on edge for varying hyperparameters. Varying width, depth, weight decay and size of the subset considered when using the CDAT rule with varying scaling factors. We observe that accrued gains can be obtained with the CDAT rule for larger widths (top left panel). In terms of depth (top right panel), the CDAT rule works best with larger depths, while we note there a slight shift of optimal scaling factors from \(2\) to slightly below \(2\). The CDAT rule appears to work best with small or no weight decay (bottom left panel) while its benefits fade with larger weight decay (no difference between greedy \(\sigma=1\) and on edge \(\sigma=2\)). Finally, while the method naturally finds smaller train losses with larger subsets of data, we do not observe a significant shift of relative performance between scales as the size of the data increases (bottom right panel).**

Figure 20: **Performance of fine-tuned algorithms in stochastic regime with SGD Momentum.** In the stochastic regime with SGD momentum, we observe that the CDAT rule may outperform the constant learning rate counterpart (particularly for large batch sizes) while performing on par or underperforming the scheduled learning rate counterparts. Interestingly, a warm-up phase appears naturally induced by the CDAT rule (right plots).

Figure 21: **Performance of fine-tuned algorithms in stochastic regime with Adam.** In the stochastic regime with Adam we observe varying performance of the CDAT rule, generally on par or underperforming the baselines. Several factors can explain the underperformance. The scaling factor is not well understood, and a finer grid search could improve the scheme. Similarly, a better estimate or a better understanding of the edge of stability in a stochastic regime could improve the approach. Tackling curvature dynamics by analyzing feedback effects as in the CDAT rule may help such design.

Experimental Details

### Datasets

Mnist.MNIST is an image classification dataset of handwritten digits (LeCun et al., 2010). We normalized the pictures so that each pixel is between \(0\) and \(1\). We did not standardize the data. We only used this dataset to test varying MLP architectures in Fig. 18. See Appendix C.5 for any additional relevant details.

Cifar10.CIFAR10 is an image classification dataset of colored images of size \(32\times 32\) with \(10\) classes (Krizhevsky et al., 2009). We normalized the pictures so that each pixel is between \(0\) and \(1\). We did not standardize the data. In the full batch regime, we considered a subset of \(4096\) samples. In the mini batch regime, we considered the full dataset of \(50,000\) samples for training (dropping out the remainder batch) and tested on the \(10,000\) validation samples.

Tiny Shakespeare.This consists in \(40,000\) lines of Shakespeare from a variety of Shakespeare's plays (Karpathy, 2015). The task consists in a character-level prediction task among 64 characters. In the full batch regime, we considered a subset of \(2048\) samples consisting of blocks of \(64\) characters.

Imagenet.Imagenet is an image classification dataset of various images from the web (Deng et al., 2009). The images have various sizes. The original Imagenet-1K dataset contains \(1000\) classes. For the full batch experiments, we consider the Imagenette (Howard, 2019) subset that consists in only \(10\) classes and took \(1024\) samples out of it. We consider the usual preprocessing for Imagenet as detailed in the Scenic library (Dehghani et al., 2022). Namely, for training we consider random cropping and random flip at training. For testing, we center crop the images. Each time the cropping reduces the colored images to a \(224\times 224\) size. In the mini-batch regime we consider the complete training dataset of \(1.2\) million images (Imagenet-1K), dropped the remainder batch, and reported test error on the \(50,000\) validation images.

### Architectures

Residual Network (ResNet).We considered the standard ResNet architectures (ResNet34, ResNet50) of He et al. (2016) as implemented in the Scenic library (Dehghani et al., 2022). For the examples with ResNet34 we removed the batch normalization layers (Ioffe and Szegedy, 2015). For the examples with ResNet50 we replace the batch normalization layers with layer normalization layers (Ba et al., 2016).

Multi-Layer Perceptron (MLP) Mixer.We consider the standard MLP Mixer architectures (Tolstikhin et al., 2021) as implemented in the Scenic library (Dehghani et al., 2022). By Mixer Ti/8, we mean the tiny model of Mixer provided in the Scenic library (see https://github.com/google-research/scenic/blob/main/scenic/projects/baselines/configs/imagenet/imagenet_augreg_mixer_config.py) with patches of size \(8\times 8\). We removed dropout (both layer and depth wise).

Nano Language Model (NanoLM).We consider a simpel sequence-to-sequence Transformer model implemented in Optax (https://github.com/google-deepmind/optax/blob/main/examples/nanolm.ipynb). The model consists of 6 stacked transformer blocks, each of which contains a multi-head attention layer followed by a feed-forward layer. Layer normalization is used used within the transformer blocks to improve training stability. Finally, a dense layer maps the model's output to the vocabulary size, producing probabilities for each character as the next potential character. The only difference with respect to the previous code is that we removed dropout (for deterministic training) and, as mentioned in the previous subsection, reduced the size of the dataset to be able to estimate the sharpness.

Vision Transformer (ViT).We consider the standard Vision Transformer architecture (Dosovitskiy et al., 2021) as implemented in the Scenic library (Dehghani et al., 2022). By ViT Ti/32, we mean the tiny model of Vision Transformer provided in the Scenic library (see https://github.com/google-research/scenic/blob/main/scenic/projects/baselines/configs/imagenet/imagenet_augreg_vit_config.py) with patches of size \(32\times 32\). We removed dropout (both layer and depth wise).

Weight decay.In all examples, except mentioned otherwise, we consider a fixed weight decay of \(10^{-5}\). See Fig. 18 (bottom left panel) for an analysis of sensitivity of the proposed CDAT rule with varying weight decay.

### Algorithms

In all experiments, when selecting the "best" tuner we considered the average train loss over the last \(5\) iterates.

Fine-tuning base optimizers.The implementation of all optimizers are taken from the Optax library (DeepMind et al., 2020). In all experiments, we fix all hyperparameters of the base optimizer ((S)GD, (S)GD with Momentum, RMSProp, Adam) to their default values: \(0.9\) for the momentum of (S)GD with Momentum, \(0.999\) for the EMA parameter of the second moment of RMSProp and Adam, \(0.9\) for the EMA parameter of the first moment of Adam. We fine-tune the learning rate on a logarithmic base of \(2\) around a base learning rate such as \(10^{-3}\) or \(10^{-4}\) (depending on the algorithm, the architecture and the mini-batch size in the stochastic regime as detailed below in Appendix C.5), while making sure that the grid is sufficiently large such that the best learning rate is found inside the grid and not as the smallest or the largest.

For the scheduled versions of the base optimizers, we consider three shapes: linear warm-up followed by constant learning rate, linear warm-up followed by linear decay, linear warm-up followed by cosine decay. The number of iterations for the warm-up period is chosen as a fraction of the overall number of steps detailed in Appendix C.5. We also varied the horizon for the decaying schedules, see again Appendix C.5.

Implementation of the linesearch procedure.To implement the linesearch procedure described in Section 2, we consider the following criterion

\[f(w_{t}+\eta_{t}^{\text{ls}}u_{t})\leq(1+\delta)f(w_{t})+c\eta_{t}^{\text{ls}} \nabla f(w_{t})^{\top}u_{t}.\]

Compared to (1), we added a relative decrease hyperparmeter \(\delta\) as we observed that the linesearch can sometimes stay stuck at vanishing learning rates otherwise.

To find a valid criterion we consider a usual backtracking linesearch that starts from a guess \(\eta_{t,0}=\min\{c_{+}\eta_{t-1}^{\text{ls}},1\}\). Choosing \(c_{+}=+\infty\) means that we start with an initial guess of \(1\) at each iteration. The learning rate is then decreased by a factor \(c_{-}\) until the criterion is satisfied. Formally, the selected stepsize is then

\[\eta_{t}^{\text{ls}}=\max\{\eta_{t,k}=c_{-}^{k}\eta_{t,0}:f(w_{t}+\eta_{t,k}u_ {t})\leq(1+\delta)f(w_{t})+c\eta_{t,k}\nabla f(w_{t})^{\top}u_{t}\}.\]

We run the search until the criterion is satisfied in the full batch regime and for a maximum of \(30\) iterations in the mini-batch regime. In the experiments, we consider the following variations.

* \(c\in[0,10^{-4},0.5]\),
* \(c_{+}\in[4,+\infty]\),
* \(c_{-}\in[0.8,0.9]\),
* \(\delta\in[0,1e-3]\) for \(c=0\) and \(\delta=0\), for \(c\in[10^{-4},0.5]\).

Implementation of quadratically greedy tuner and CDAT.To implement the quadratically greedy tuner or CDAT, we compute the denominator \(u^{\top}\nabla^{2}f(w)u\) as the second partial derivative of \(f\) along \(u\), that is,

\[u^{\top}\nabla^{2}f(w)u=\partial^{2}f(w)[u,u]=\partial(\partial f(\cdot)[u])( w)[u],\]

where \(\partial g(w)[u]\) amounts to a Jacobian vector product (jvp) computed with forward mode auto-diff in differentiable programming languages such as JAX (DeepMind et al., 2020).

Computing the denominator in the CDAT rule by forward mode automatic differentiation enables a much lower memory consumption than using Hessian vector products (see, e.g., (Blondel and Roulet, 2024, Chapter 8), (Dagreou et al., 2024) for more details). The computation of the denominator by applying twice forward mode automatic differentiation still incurs approximately three times the 

[MISSING_PAGE_FAIL:29]

We considered the full dataset of CIFAR10 with ResNet50 with layer normalization in place of batch normalization.

See the details given for Fig. 20 and Fig. 21.

For both values of \(\sigma\), \(a=5\cdot 10^{-2}\), \(b=10^{-1}\), \(\nu=0.1\), \(\lambda_{0}=18\), \(\eta_{0}=0.05\), \(g_{0}=p_{0}=4\).

Same settings as Figure Fig. 4.

Same settings as Figure Fig. 9.

This is the same setting as in Fig. 1.

We consider the same setting as in Fig. 3. For the Polyak stepsizes (23), we let \(\eta_{\max}\) vary between \(1\) and \(100\) and select the best. For the hypergradient descent, we let the hyper learning rate vary in \(\beta\in\{10^{i},i\in\{-3,\dots,0\}\}\).

We considered again the ResNet50 with layer normalization instead of batch normalization. For the constant learning rate baseline we searched over a grid of \(\{\eta_{m}\cdot 2^{i},i\in\{-1,\dots,5\}\}\), for \(\eta_{m}=10^{-4}\cdot\sqrt{m/4096}\) for \(m\) the batch size.

We considered a simple MLP with hidden sizes \((256,256,256)\), ReLU activations. We tuned the constant learning rate baseline on \(\{10^{-3}\cdot 2^{i},i\in\{-1,7\}]\}\).

Details are provided in the legend.

This is the same setting as in Fig. 6.

In this figure, the MLPs considered use ReLu activations. If not detailed, the weight decay is set to \(10^{-5}\) and the subset considered is of size \(8192\).

The settings are the same as in Fig. 5. For the constant learning rate baselines we searched on a grid \(\{\eta_{\text{base}}\cdot 2^{i},i\in\{-3,\dots,9\}\}\). The base learning rate \(\eta_{\text{base}}\) was chosen to be \(10^{-3}\) for ResNet, \(10^{-2}\) for the Mixer, \(10^{-4}\) for the NanoLM and ViT. For the schedules' shapes, we searched over linear warm-up, linear warm-up with linear decay, linear warm-up with cosine decay. The initial and end learning rate were set to \(0\). The horizons for the schedules were chosen in \([N,N/2,N/4]\) for \(N=8192\) for the NanoLM, \(N=16384\) for the ViT, Mixer and ResNet. The fraction of warm-up steps was searched in \(\{0.05,0.1,0.2\}\).

For the constant learning rate baseline, we consider searching the best constant learning rate on a grid \(\{\eta_{m}\cdot 2^{i},i\in\{-1,\dots,7\}\}\) for \(\eta_{m}=10^{-4}\cdot\sqrt{m/4096}\) where \(m\) denotes the varying batch size.

For the scheduled baseline, we consider the variants presented above (linear warm-up followed by constant, linear warm-up followed by cosine decay, linear warm-up followed by linear decay) with varying fraction of warm-up steps (\(0.05,0.1,0.2\)) and an initial learning rate of \(0\), a final learning rate of \(0\) for a fixed horizon of \(512\) epochs, and a peak learning rate searched over \(\{\eta_{m}\cdot 4^{i},i\in\{4,\dots,9\}\}\).

The scaling factor \(\sigma\) of CDAT was searched on a grid \(\{0.4,0.6,\dots,2.8\}\), and we also tuned the EMA parameter \(\beta_{\text{cda}}\) in the computation of the numerators and denominators of the edge in \(\{0,0.9,0.99\}\). The best parameters found for CDAT can be inferred from Fig. 7. Namely, we found that non-zero EMA parameter for the estimation of the edge decay was essential for good performance and that the best scaling factor varied with the batch size. For example, at batch size \(256\) the best scaling factor is \(\sigma=1.8\) with \(\beta_{\text{cda}}=0.9\).

For the constant learning rate baseline, we consider searching the best constant learning rate on a grid \(\{\eta_{m}\cdot 2^{i},i\in\{-1,\dots,7\}\}\) for \(\eta_{m}=10^{-4}\cdot\sqrt{m/1024}\) where \(m\) denotes the varying batch size.

Figure 10: Same settings as Figure Fig. 4.

Figure 21: For the constant learning rate baseline, we consider searching the best constant learning rate on a grid \(\{\eta_{m}\cdot 2^{i},i\in\{-1,\dots,7\}\}\) for \(\eta_{m}=10^{-4}\cdot\sqrt{m/1024}\) where \(m\) denotes the varying batch size.

Figure 7: We considered the full dataset of CIFAR10 with ResNet50 with layer normalization in place of batch normalization.

Figure 11: Same settings as Figure Fig. 4.

Figure 12: This is the same setting as in Fig. 1.

For the scheduled baseline, we consider a linear warm-up followed by cosine decay, with a fraction of warm-up steps of \(0.1\) and an initial learning rate of \(0\), a final learning rate of \(0\) for a fixed horizon of \(128\) epochs, and a peak learning rate searched over \(\{\eta_{m}\cdot 4^{i},i\in\{1,\dots,5\}\}\).

The scaling factor \(\sigma\) of CDAT was searched on a grid \(\{0.4,0.6,\dots,2.6\}\), and we also tuned the EMA parameter \(\beta_{\text{cda}}\) in the computation of the numerators and denominators of the edge in \(\{0,0.9,0.99\}\).

### Assets license and computing ressources

Assets.All experiments are done in the open-source JAX ecosystem (DeepMind et al., 2020): architectures are taken from Scenic (Dehghani et al., 2022), datasets from TensorFlow Dataset, algorithms from Optax. The datasets are MNIST (LeCun et al., 2010), (Creative Commons Attribution-Share Alike 3.0 license) CIFAR10 (Krizhevsky et al., 2009) (no available license), Imagenet (Deng et al., 2009) (ImageNet explicitly permits the use of the dataset for non-commercial research purposes, however there is no single license since the images are scrapped from different sources with different licenses), TinyShakespeare (Karpathy, 2015) (Apache 2.0 license in TensorFlow dataset, though the works of William Shakespeare are in the public domain).

Computing resources.Experiments have mostly been run on Tensor Processing Units (TPUs) v2 (180 Tera Floating-Point Operations per Second (TFLOPS), 64 GB High Bandwidth Memory (HBM)). Experiments on MLP Mixers required TPUs v3 (420 TFLOPS 128 GB HBM). Very small scale experiments on MNIST with MLPs were run on CPUs. In terms of wall time, as discussed in Appendix C.3, we observed that the CDAT rule can be twice slower than the constant or scheduled learning rate counterparts. We consider CDAT as a diagnostic tool and leave as future work efficient implementations. Preliminary experiments and additional attempts to further adapt the momentum parameter on edge are not reported.

Authors contributions.* [leftmargin=*]
* Vincent Roulet conducted the experimental work from the failures of linesearches to the analysis of the CDAT rule in various settings.
* Atish Agarwala developed the theoretical model, with associated figures, interpretations and comments. He also helped to guide the experimental study with the insights gathered by the model.
* Jean Bastien Grill did an initial empirical study that gathered first intuitions on the method. He participated in the discussions and contributed to the writing.
* Grzegorz Swirszcz participated in the discussions.
* Mathieu Blondel participated in the discussions, contributed to the writing and proposed an alternative rule using a Gauss-Newton approximation of the objective.
* Fabian Pedregosa initiated the project, performed a larger scale empirical study of the CDAT rule on the MLCommons benchmark, participated in the discussions, and contributed to the writing.

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: This is an exploratory work that poses questions and attempts at answers from experiments and simplified models.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We clearly point out the subdued effects of the proposed CDAT rule in the stochastic framework. In addition, we discuss the limitations of the current simplified models of the sharpness dynamics to explain the performance of CDAT in a full batch regime.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Paper includes numerical experiments on models, derived from previous work (Damian et al., 2023). Modeling assumptions are clearly labelled, and accompanied with simulations. We also discuss the limitations of the modeling assumptions.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We present detailed experimental setups in Appendix C.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We are not able to open source the code yet. We hope to release it with the arxiv version of the manuscript later. Note that the data is in open access, and the algorithms are based on open source libraries such as JAX (DeepMind et al., 2020), Optax, and Scenic (Dehghani et al., 2022).
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We present detailed experimental setups in Appendix C.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We privileged varying settings (datasets, architectures, losses, base optimizers) over varying the seeds to provide a global overview of the CDAT rule as a diagnostic tool. We also provided additional experiments with varying mini-batch sizes (Fig. 20, Fig. 21), varying depths, widths, weight decays in a MLP setting (Fig. 18). Our goal has been to extract qualitative conclusions rather than quantitative conclusions. We also show in all transparency that the proposed rule does not outperform prefixed schedules (Fig. 19), and highlight limitations in the stochastic regime (Fig. 8).
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: This is detailed in Appendix C.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes]
10. **Broader Impacts**Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: No societal impact of the work is foreseen.
* **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks
* **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: This is detailed in Appendix C.6.
* **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects