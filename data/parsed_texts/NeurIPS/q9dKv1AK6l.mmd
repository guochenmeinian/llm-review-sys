# Small steps no more: Global convergence of stochastic gradient bandits for arbitrary learning rates

Jincheng Mei

Google DeepMind

Google Research

Georgia Institute of Technology

University of Alberta

Simon Fraser University

Indian Institute of Science

Bo Dai

Google Research

Georgia Institute of Technology

Georgia Institute of Technology

Alekh Agarwal

Google Research

Georgia Institute of Technology

Sharan Vaswani

Simon Fraser University

Indian Institute of Science

Anant Raj

Google Research

Georgia Institute of Technology

Georgia Institute of Technology

Simon Fraser University

Indian Institute of Science

###### Abstract

We provide a new understanding of the stochastic gradient bandit algorithm by showing that it converges to a globally optimal policy almost surely using _any_ constant learning rate. This result demonstrates that the stochastic gradient algorithm continues to balance exploration and exploitation appropriately even in scenarios where standard smoothness and noise control assumptions break down. The proofs are based on novel findings about action sampling rates and the relationship between cumulative progress and noise, and extend the current understanding of how simple stochastic gradient methods behave in bandit settings.

## 1 Introduction

The stochastic gradient method has been ubiquitous in the field of machine learning for decades [4]. When applied to reinforcement learning (RL), a representative instantiation of stochastic gradient is the well known policy gradient [32] (or REINFORCE [34]) algorithm, where in each iteration an online sample is gathered using the current policy, from which a gradient estimate is obtained to conduct parameter updates. In the simplest setting of a stochastic bandit problem [16], where decisions matter only for one step, the REINFORCE policy gradient method becomes equivalent to the stochastic gradient bandit algorithm [31, Section 2.8]. Compared to other statistical methods, such as the upper confidence bound algorithm (UCB, [14, 3]), and Thompson sampling (TS, [33, 2]), the stochastic gradient bandit algorithm is conceptually simpler and more computationally efficient, as it does not calculate exploration bonuses nor posterior distributions. Moreover, the stochastic gradient method is highly scalable and naturally applicable to large scale neural networks [29, 30].

However, unlike UCB or TS, the stochastic gradient bandit algorithm does not have an equivalently well established and comprehensive theoretical footing. Given its pervasive success and widespread application in RL [30] and fine-tuning for large language models [26, 27]), it remains an important question to understand the success of stochastic gradient based algorithms in bandit-like settings, not only to bridge the gap between theory and practice, but also to identify more effective and robust variants. In this paper, we make a significant contribution to the theoretical understanding of the stochastic gradient bandit algorithm, bringing its justification closer to that of other less scalable but theoretically well established methods. In particular, we establish the surprising result that:

_For any constant learning rate \(\eta>0\), the stochastic gradient bandit algorithm is guaranteed to converge to the globally optimal policy almost surely._Since learning rate is the only tuning parameter in the stochastic gradient bandit algorithm, this result offers a remarkable robustness for the method, that it converges to a near optimal policy, irrespective of the value of this hyperparameter! Analysis of this algorithm is challenging because it requires techniques for simultaneously handling non-convex optimization, stochastic approximation, and the exploration-exploitation trade-off. Prior theoretical work on the stochastic gradient algorithm has primarily focused on non-convex optimization and stochastic approximation, but understanding the simultaneous effect on exploration has been largely lacking.

Recently, significant progress has been made in establishing global convergence results for policy gradient (PG) methods. For example, it has been shown that using exact gradients, Softmax PG converges to a globally optimal policy asymptotically as the number of iterations \(t\) goes to infinity [1]. Subsequent work has demonstrated that the asymptotic rate of convergence is \(O(1/t)\)[23], albeit with problem and initialization dependent constants [22; 17]. The rate and constant dependence in the true gradient setting have been improved via several techniques, including entropy regularization [23], normalization [21], and using natural gradient (mirror descent) [1; 6; 15].

Unfortunately, in the online stochastic setting, where the policy gradient has to be estimated using the current policy to collect samples, these accelerated methods all obtain worse asymptotic results than the standard Softmax PG [20], failing to converge to a global optimum without careful design choices [19]. Yet in the same setting, standard Softmax PG has been shown to succeed in its simplest form, provided only that a sufficiently small learning constant rate \(\eta\in\Theta(1)\) is used [24].

For stochastic gradient based methods, decaying or sufficiently small learning rates are used by almost all current approaches, motivated by classical convergence analyses from stochastic optimization [28; 12; 39; 38; 9; 37; 36; 24; 8]. Stationary point convergence is guaranteed for learning rates sufficiently small with respect to the smoothness of the objective function, while also decaying to zero at a precise rate if noise in the gradient estimator persists. In addition to appropriate learning rate control, many other techniques have been developed to control the effects of gradient noise, including regularization [38; 9], variance reduction [37], and carefully considering growth conditions [36; 24].

The technical challenges we face in the current study can be understood in the following aspects: **(1)** Using an arbitrarily large constant learning rate for online stochastic gradient optimization immediately renders the smoothness and noise control techniques mentioned above inapplicable. **(2)** With any constant learning rate \(\eta>0\), the question of whether oscillation or convergence will ultimately occur needs to be addressed before even considering whether any convergence is to a global optimum. This additional level of complexity arises because the optimization objective is not necessarily improved monotonically in expectation. Finally, **(3)** The gradient bandit algorithm does not use any exploration bonus, which means that new techniques are required to demonstrate that it adequately balances the exploration-exploitation trade-off.

In this paper, we resolve the above difficulties by uncovering intriguing exploration properties of stochastic gradient when using any constant learning rate. In particular, we establish the following.

**(i)**: In the stochastic online setting, with probability \(1\), the stochastic gradient bandit algorithm will not keep sampling any single action forever, implying that it will exhibit a minimal form of exploration without any further modification. This asymptotic event (as \(t\to\infty\)) happens with probability \(1\) and holds for any constant learning rate \(\eta>0\).
**(ii)**: This result can then be leveraged to show that, as a consequence, given any constant learning rate, the stochastic gradient bandit algorithms will converge to the globally optimal policy as \(t\to\infty\), with probability \(1\). That is, the probability of sub-optimal actions decays to \(0\), even though some of them are taken infinitely often asymptotically.

## 2 Setting and Background

We consider the stochastic multi-armed bandit problem [16], specified by \(K\) actions and a true mean reward vector \(r\in\mathbb{R}^{K}\), where for each action \(a\in[K]\coloneqq\{1,2,\ldots,K\}\),

\[r(a)=\int_{-R_{\max}}^{R_{\max}}x\cdot P_{a}(x)\mu(dx),\] (1)

where \(R_{\max}>0\) is the reward range, \(\mu\) is a finite measure over \([-R_{\max},R_{\max}]\), and \(P_{a}(x)\geq 0\) is the probability density function with respect to \(\mu\). We use \(R_{a}\) to denote the reward distribution for action \(a\) defined by the density \(P_{a}\) and base measure \(\mu\). The goal is to find a policy \(\pi_{\theta}\in[0,1]^{K}\) to achieve high expected reward,

\[\max_{\theta\in\mathbb{R}^{K}}\pi_{\theta}^{\top}r,\] (2)

where \(\pi_{\theta}\) is parameterized by \(\theta\in\mathbb{R}^{K}\).

**The gradient bandit algorithm.** A natural idea to optimize Eq. (2) is to use stochastic gradient ascent, which is shown in Algorithm 1 and known as the gradient bandit algorithm [31, Section 2.8]. In Algorithm 1, in each iteration \(t\geq 1\), the probability of pulling arm \(a\in[K]\) is given as

\[\pi_{\theta_{t}}(a)=[\mathrm{softmax}(\theta_{t})](a)\coloneqq \frac{\exp\{\theta_{t}(a)\}}{\sum_{a^{\prime}\in[K]}\exp\{\theta_{t}(a^{ \prime})\}},\quad\text{ for all }a\in[K],\] (3)

where \(\theta_{t}\in\mathbb{R}^{K}\) is the parameter vector to be updated. The following proposition shows that Algorithm 1 is an instance of stochastic gradient ascent with an unbiased gradient estimator [31, 24].

**Proposition 1** (Proposition 2.3 of [24]).: _Algorithm 1 is equivalent to the following update,_

\[\theta_{t+1}\leftarrow\theta_{t}+\eta\cdot\frac{d\,\pi_{\theta_{t}}^{\top} \hat{r}_{t}}{d\theta_{t}}=\theta_{t}+\eta\cdot\left(\text{diag}(\pi_{\theta_ {t}})-\pi_{\theta_{t}}\pi_{\theta_{t}}^{\top}\right)\hat{r}_{t},\] (4)

_where \(\mathbb{E}_{t}\Big{[}\frac{d\,\pi_{\theta_{t}}^{\top}\hat{r}_{t}}{d\theta_{t} }\Big{]}=\frac{d\,\pi_{\theta_{t}}^{\top}r}{d\theta_{t}}\), and \(\mathbb{E}_{t}[\cdot]\) is defined with respect to randomness from on-policy sampling \(a_{t}\sim\pi_{\theta_{t}}(\cdot)\) and reward sampling \(R_{t}(a_{t})\sim P_{a_{t}}\). The Jacobian of \(\theta\mapsto\pi_{\theta}\coloneqq\mathrm{softmax}(\theta)\) is \(\big{(}\frac{d\,\pi_{\theta}}{d\theta}\big{)}^{\top}=\text{diag}(\pi_{\theta} )-\pi_{\theta}\pi_{\theta}^{\top}\in\mathbb{R}^{K\times K}\), and \(\hat{r}_{t}(a)\coloneqq\frac{\mathbb{I}[a_{t}=a)}{\pi_{\theta_{t}}(a)}\cdot R _{t}(a)\) for all \(a\in[K]\) is the importance sampling (IS) estimator, and we set \(R_{t}(a)=0\) for all \(a\neq a_{t}\)._

``` Input: initial parameters \(\theta_{1}\in\mathbb{R}^{K}\), learning rate \(\eta>0\). Output: policies \(\pi_{\theta_{t}}=\mathrm{softmax}(\theta_{t})\). while\(t\geq 1\)do  Sample an action \(a_{t}\sim\pi_{\theta_{t}}(\cdot)\) and observe reward \(R_{t}(a_{t})\sim P_{a_{t}}\). for all \(a\in[K]\)do if\(a=a_{t}\)then \(\theta_{t+1}(a)\leftarrow\theta_{t}(a)+\eta\cdot(1-\pi_{\theta_{t}}(a))\cdot R _{t}(a_{t})\). else \(\theta_{t+1}(a)\leftarrow\theta_{t}(a)-\eta\cdot\pi_{\theta_{t}}(a)\cdot R_{t} (a_{t})\). endif endfor endwhile ```

**Algorithm 1** Gradient bandit algorithm (without baselines)

**Known results on the convergence of the gradient bandit algorithm.** Since Eq. (2) corresponds to a smooth non-concave maximization problem over \(\theta\in\mathbb{R}^{K}\)[23], using Algorithm 1 with decaying learning rates is sufficient to guarantee convergence to a stationary point [28, 25, 12, 39]. However, this is insufficient to ensure the globally optimal solution of Eq. (2) is reached, since there exist multiple stationary points. More recently, guarantees of convergence to a globally optimal policy have been developed for PG methods in the true gradient setting [1, 23], where the algorithm has access to exact mean rewards. These results were later extended to achieve global convergence guarantees (almost surely) in the stochastic setting [38, 9, 37, 36, 24, 18]. However, these extended results have required decaying or sufficiently small learning rates, motivated by exploiting smoothness and combating the inherent noise in stochastic gradients.

Despite these previous assumptions, there exists empirical and theoretical evidence that using a large learning rate in the stochastic gradient bandit algorithm is a viable option. For example, it has been observed that softmax policies learn even with extremely large learning rates such as \(2^{14}\)[11]. For logistic regression on linearly separable data, the objective has an exponential tail and the minimizer is unbounded, yet it has been shown that gradient descent with iteration dependent learning rate \(\eta\in\Theta(t)\) achieves accelerated \(O(1/t^{2})\) convergence [35]. Though the objective in Eq. (2) hassimilar properties, unlike logistic regression, the problem we are considering is non-concave, so the same techniques cannot be directly applied. The most related results are from [24], which proved that with a small problem specific constant learning rate, Algorithm 1 achieves convergence to a globally optimal policy almost surely. However, as mentioned, the learning rate choices in [24] rely on assumptions of (non-uniform) smoothness and noise growth conditions (their Lemmas 4.2, 4.3, and 4.6), which cannot be directly applied here for a large learning rate.

Consequently, the use of large learning rates appear to render existing results and techniques inapplicable. Furthermore, with a large constant learning rate, it is unclear whether Algorithm 1 will converge to any stationary point, or the iterates will keep oscillating. If the algorithm does converge, it is also not clear what effect large step-sizes have on exploration, and whether the algorithm will converge to the optimal arm in such cases. Resolving these questions requires new results that characterize the behavior of Algorithm 1, since the classical optimization and stochastic approximation convergence theories are no longer applicable, as explained.

## 3 Asymptotic Global Convergence of Gradient Bandit Algorithm

We have seen that solving the non-concave maximization problem Eq. (2) using Algorithm 1 with any constant (potentially large) learning rate requires ideas beyond classical optimization theory. Here, we take a different perspective to investigate how Algorithm 1 samples actions. For analysis, we make the following assumption about the reward distribution.

**Assumption 1** (True mean reward has no ties).: _For all \(i,j\in[K]\), if \(i\neq j\), then \(r(i)\neq r(j)\)._

**Remark 1**.: _Removing Assumption 1 remains an open question for future work, while we believe that Algorithm 1 works without Assumption 1. One piece of evidence to support this conjecture is that even in the exact gradient setting, the set of initializations where Softmax PG approaches non-strict one-hot policies has zero measure._

### Failure Mode of Aggressive Updates

It has been observed that several accelerated PG methods in the true gradient setting, including natural PG [13, 1] and normalized PG [21], obtain worse results than standard softmax PG if combined with online sampling \(a_{t}\sim\pi_{\theta_{t}}(\cdot)\) using constant learning rates [20]. The failure mode in these cases is that the update is too aggressive and commits to a sub-optimal arm without sufficiently exploring all arms. This results in a non-trivial probability of sampling one action forever, i.e., there exists a potentially sub-optimal action \(a\in[K]\), such that with some constant probability, \(a_{t}=a\) for all \(t\geq 1\). Such an outcome implies that \(\pi_{\theta_{t}}(a)\to 1\) as \(t\to\infty\)[20, Theorem 3]. Since \(a\in[K]\) could be a sub-optimal action with \(r(a)<r(a^{*})=\max_{a\in[K]}r(a)\), this results in a lack of exploration, and consequently, methods such as natural PG and normalized PG are not guaranteed to converge to the optimal action \(a^{*}\coloneqq\arg\max_{a\in[K]}r(a)\) with probability \(1\).

### Stochastic Gradient Automatically Avoids Lack of Exploration

Our first key finding is that Algorithm 1 does not keep sampling one action forever, no matter how large the constant learning rate is. This property avoids the problem of a lack of exploration, in the sense that Algorithm 1 will at least explore more than one action infinitely often. At first glance, this might not seem like a strong property, since the algorithm might somehow explore only sub-optimal actions forever. However, we will argue below that this property coupled with additional arguments is sufficient to guarantee convergence to the globally optimal policy.

Let us now formally prove the above property. By Algorithm 1, for all \(a\in[K]\), for all \(t\geq 1\),

\[\theta_{t+1}(a)\leftarrow\theta_{t}(a)+\left\{\begin{array}{rl}\eta\cdot(1- \pi_{\theta_{t}}(a))\cdot R_{t}(a),&\text{ if }a_{t}=a,\\ -\eta\cdot\pi_{\theta_{t}}(a)\cdot R_{t}(a_{t}),&\text{ otherwise.}\end{array}\right.\] (5)

We define \(N_{t}(a)\) as the number of times action \(a\in[K]\) is sampled up to iteration \(t\geq 1\), i.e.,

\[N_{t}(a)\coloneqq\sum_{s=1}^{t}\mathbb{I}\,\{a_{s}=a\},\] (6)and its asymptotic limit \(N_{\infty}(a)\coloneqq\lim_{t\to\infty}N_{t}(a)\), which could possibly be infinity. For all \(a\in[K]\), we have either \(N_{\infty}(a)=\infty\) or \(N_{\infty}(a)<\infty\), meaning that \(a\in[K]\) is sampled infinitely often or only finitely many times asymptotically. First, we prove the following Lemma 1, which shows that if an action \(a\in[K]\) is sampled only finitely many times as \(t\to\infty\), then the parameter corresponding to action \(a\) is also finite, i.e., \(\sup_{t\geq 1}|\theta_{t}(a)|<\infty\).

**Lemma 1**.: _Using Algorithm 1 with any constant \(\eta\in\Theta(1)\), if \(N_{\infty}(a)<\infty\) for an action \(a\in[K]\), then we have, almost surely,_

\[\sup_{t\geq 1}\theta_{t}(a)<\infty,\text{ and }\inf_{t\geq 1}\theta_{t}(a)>-\infty.\] (8)

Lemma 1 will be used multiple times in the subsequent convergence arguments.

Proof sketch.Since we assume action \(a\in[K]\) is sampled finitely many times, the update given in the case depicted by Eq. (5) happens finitely many times. Each update is bounded since the sampled reward is in \([-R_{\max},R_{\max}]\) by Eq. (1), and the learning rate is a constant, i.e., \(\eta\in\Theta(1)\). In Algorithm 1, \(\theta_{t}(a)\) is still updated even when \(a_{t}\neq a\), with the corresponding update given by the case depicted by Eq. (6). Therefore, whether \(\theta_{t}(a)\) is bounded depends on the cumulative probability \(\sum_{s=1}^{t}\pi_{\theta_{s}}(a)\) being summable as \(t\to\infty\). According to the extended Borel-Cantelli lemma (Lemma 3), we have, almost surely,

\[\Big{\{}\sum_{t\geq 1}\pi_{\theta_{t}}(a)=\infty\Big{\}}=\left\{N_{\infty}(a)= \infty\right\},\] (9)

which implies (by taking complements) that \(\sum_{t\geq 1}\pi_{\theta_{t}}(a)<\infty\) if and only if \(N_{\infty}(a)<\infty\). Therefore, if \(a\in[K]\) is sampled finitely often, \(\theta_{t}(a)\) will be updated in a bounded manner (using Eqs. (5) and (6)) as \(t\to\infty\), hence establishing Lemma 1. Detailed proofs for this lemma, as well as for all other results in this paper can be found in the appendix.

Given Lemma 1, we can then establish the above-mentioned finding about the exploration effect of Algorithm 1 in Lemma 2.

**Lemma 2** (Avoiding a lack of exploration).: _Using Algorithm 1 with any \(\eta\in\Theta(1)\), there exists at least a pair of distinct actions \(i,j\in[K]\) and \(i\neq j\), such that, almost surely,_

\[N_{\infty}(i)=\infty,\text{ and }N_{\infty}(j)=\infty.\] (10)

Proof sketch.The argument for the existence of one such action is straightforward, since by the pigeonhole principle, if there are finitely many actions, i.e., \(K<\infty\), there must be at least one action \(i\in[K]\) that is sampled infinitely often as \(t\to\infty\).

The argument for the existence of a second such action is by contradiction. Suppose that all the other actions \(j\in[K]\) with \(j\neq i\) are sampled only finitely many times as \(t\to\infty\). According to Lemma 1, their corresponding parameters must remain finite, i.e., \(\sup_{t\geq 1}|\theta_{t}(j)|<\infty\) for all \(j\in[K]\) with \(j\neq i\). Now consider \(\theta_{t}(i)\). By assumption, the second update case for this parameter, Eq. (6), happens only finitely often, since Eq. (6) can only occur when \(a_{t}\neq i\). Therefore, the key question is whether the cumulative probability \(\sum_{s=1}^{t}\left(1-\pi_{\theta_{s}}(i)\right)\) involved in the first case of the update, Eq. (5), is summable as \(t\to\infty\). Note that \(\sum_{s=1}^{t}\left(1-\pi_{\theta_{s}}(i)\right)=\sum_{s=1}^{t}\sum_{j\neq i }\pi_{\theta_{s}}(j)\), which is indeed summable as \(t\to\infty\), by the assumption and Eq. (9). This implies that action \(i\), which is sampled infinitely often, achieves a parameter magnitude, \(\sup_{t\geq 1}|\theta_{t}(i)|<\infty\), that remains bounded as \(t\to\infty\). Using the softmax parameterization Eq. (3) in the above argument, we conclude that for all \(a\in[K]\), \(\inf_{t\geq 1}\pi_{\theta_{t}}(a)>0\), i.e., every action's probability remains bounded away from zero, and hence is not summable. Using Eq. (9), this implies that every action is sampled infinitely often, which contradicts the assumption that only action \(i\) is sampled infinitely often as \(t\to\infty\).

Discussion.Lemma 2 implies that Algorithm 1 is not an aggressive method in the sense of [20], no matter how large the learning rate is, as long as it is constant, i.e., \(\eta\in\Theta(1)\). According to [20, Theorem 7], even if we fix the sampling in Algorithm 1 to a sub-optimal action \(a\in[K]\) forever, i.e., \(a_{t}=a\) for all \(t\geq 1\), its probability will not approach \(1\) faster than \(O(1/t)\), i.e., \(1-\pi_{\theta_{t}}(a)\in\Omega(1/t)\). This means that there must be at least one another action \(a^{\prime}\in[K]\) with \(a^{\prime}\neq a\), such that \(a^{\prime}\) will also be sampled infinitely often. A more intuitive explanation is that the \((1-\pi_{\theta_{t}}(a))\) term in Eq. (5) will be near \(0\), which slows the speed of committing to a deterministic policy on \(a\) whenever \(\pi_{\theta_{t}}(a)\) is close to \(1\), which encourages exploration. Such natural exploratory behavior arises in Algorithm 1 because of the softmax Jacobian \(\text{diag}(\pi_{\theta})-\pi_{\theta}\pi_{\theta}^{\top}\) in the update shown in Proposition 1, which determines the growth order of \(\theta_{t}(a)\) for all \(a\in[K]\) as \(t\to\infty\), making the effect of a constant learning rate \(\eta\in\Theta(1)\) asymptotically inconsequential.

### Warm up: Global Asymptotic Convergence when \(K=2\)

We now consider the simplest case, where we have only two possible actions. According to Lemma 2, each of the two actions must be sampled infinitely often as \(t\to\infty\). We now illustrate the second key result, that for both actions \(a\in[K]\), the random sequence \(\{\theta_{t}(a)\}_{t\geq 1}\) follows the direction of the expected gradient for sufficiently large \(t\geq 1\) almost surely. The proof uses a technique that has been previously used in [19; 24] for small learning rates, but here we observe that the same technique continues to work for Algorithm 1 no matter how large the learning rate is, as long as \(\eta\in\Theta(1)\).

**Theorem 1**.: _Let \(K=2\) and \(r(1)>r(2)\). Using Algorithm 1 with any \(\eta\in\Theta(1)\), we have, almost surely, \(\pi_{\theta_{t}}(a^{*})\to 1\) as \(t\to\infty\), where \(a^{*}\coloneqq\arg\max_{a\in[K]}r(a)\) (equal to Action \(1\) in this case)._

Proof sketch.According to Lemma 2, \(N_{\infty}(1)=N_{\infty}(2)=\infty\). Denote the the reward gap as \(\Delta\coloneqq r(a^{*})-\max_{a\neq a^{*}}r(a)>0\), which becomes \(\Delta=r(1)-r(2)\) for two actions. Since the stochastic gradient is unbiased (Proposition 1), we have, for all \(t\geq 1\) (detailed calculations omitted),

\[\mathbb{E}_{t}[\theta_{t+1}(a^{*})] =\theta_{t}(a^{*})+\eta\cdot\pi_{\theta_{t}}(a^{*})\cdot\big{(}r( a^{*})-\pi_{\theta_{t}}^{\top}r\big{)}\] (11) \[=\theta_{t}(a^{*})+\eta\cdot\pi_{\theta_{t}}(a^{*})\cdot\Delta \cdot(1-\pi_{\theta_{t}}(a^{*}))>\theta_{t}(a^{*}).\] (12)

A similar calculation shows that,

\[\mathbb{E}_{t}[\theta_{t+1}(2)]=\theta_{t}(2)-\eta\cdot\pi_{\theta_{t}}(2) \cdot\Delta\cdot(1-\pi_{\theta_{t}}(2))<\theta_{t}(2),\] (13)

which means that \(\theta_{t}(a^{*})\) is monotonically increasing in expectation and \(\theta_{t}(2)\) is monotonically decreasing in expectation. In other words, \(\{\theta_{t}(a^{*})\}_{t\geq 1}\) is a sub-martingale, while \(\{\theta_{t}(2)\}_{t\geq 1}\) is a super-martingale. However, since \(\theta_{t}\in\mathbb{R}^{K}\) is unbounded, Doob's martingale convergence results cannot be directly applied, so we pursue a different argument. Following [19; 24], given an action \(a\in[K]\), we define \(P_{t}(a)\coloneqq\mathbb{E}_{t}[\theta_{t+1}(a)]-\theta_{t}(a)\) as the "progress", and define \(W_{t}(a)\coloneqq\theta_{t}(a)-\mathbb{E}_{t-1}[\theta_{t}(a)]\) as the "noise", where \(\theta_{t}(a)=W_{t}(a)+P_{t-1}(a)+\theta_{t-1}(a)\). By recursion we can determine that,

\[\theta_{t}(a)=\mathbb{E}[\theta_{1}(a)]+\sum_{s=1}^{t}W_{s}(a)+\sum_{s=1}^{t- 1}P_{s}(a),\] (14)

i.e., \(\theta_{t}(a)\) is the result of "cumulative progress" and "cumulative noise". According to [24, Theorem C.3], the cumulative noise term can be bounded by using martingale concentration, where the order of the corresponding confidence interval is smaller than the order of the cumulative progress. Therefore, the summation will always be determined by the cumulative progress as \(t\to\infty\). According to the calculations in Eqs. (12) and (13), we have \(P_{t}(a^{*})>0\) and \(P_{t}(2)<0\), both of which are not summable. As a result, \(\theta_{t}(a^{*})\to\infty\) and \(\theta_{t}(2)\to-\infty\) as \(t\to\infty\), which implies that \(\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(2)}=\exp\{\theta_{t}(a^{*})- \theta_{t}(a)\}\to\infty\), hence \(\pi_{\theta_{t}}(a^{*})\to 1\) as \(t\to\infty\).

### Global Asymptotic Convergence for all \(K\geq 2\)

The illustrative two-action case shows that if \(\pi_{\theta_{t}}^{\top}r\in(r(2),r(a^{*}))\) and if both actions are sampled infinitely often, then we have, almost surely \(\theta_{t}(a^{*})\to\infty\) and \(\theta_{t}(2)\to-\infty\) as \(t\to\infty\). However, the question at the beginning of Section 3.2 remains: when \(K>2\), if the two actions sampled infinitely often in Lemma 2 are both sub-optimal, will that result in a similar failure mode to the one described in Section 3.1? The answer is no, which follows from our third key finding, which is based on another contradiction-based argument that establishes almost sure convergence to a globally optimal policy in the general \(K>2\) case.

**Theorem 2**.: _Given \(K\geq 2\), using Algorithm 1 with any \(\eta\in\Theta(1)\), we have, almost surely, \(\pi_{\theta_{t}}(a^{*})\to 1\) as \(t\to\infty\), where \(a^{*}=\arg\max_{a\in[K]}r(a)\) is the optimal action._Proof sketch.We consider two cases: \(N_{\infty}(a^{*})<\infty\) and \(N_{\infty}(a^{*})=\infty\), corresponding to whether the optimal action is sampled finitely or infinitely often as \(t\to\infty\). We argue that the first case (\(N_{\infty}(a^{*})<\infty\)) is impossible, while for the second case (\(N_{\infty}(a^{*})=\infty\)) we prove that \(\theta_{t}(a^{*})-\theta_{t}(a)\to\infty\) for all \(a\in[K]\) with \(r(a)<r(a^{*})\), which implies \(\pi_{\theta_{t}}(a^{*})\to 1\) as \(t\to\infty\).

_First case._ Suppose that \(N_{\infty}(a^{*})<\infty\). We argue that this is impossible via contradiction. Given the assumption and Lemma 2 we know there must be at least two other sub-optimal actions \(i_{1},i_{2}\in[K]\), \(i_{1}\neq i_{2}\), such that \(N_{\infty}(i_{1})=N_{\infty}(i_{2})=\infty\). In particular, let \(i_{1}=\arg\min_{a\in[K],N_{\infty}(a)=\infty}r(a)\) and \(i_{2}=\arg\max_{a\in[K],N_{\infty}(a)=\infty}r(a)\), hence \(r(i_{1})<r(i_{2})<r(a^{*})\). By Lemma 6 (see Appendix) we will also have \(r(i_{1})<\pi_{\theta_{t}}^{\top}r<r(i_{2})\) for sufficiently large \(t\geq 1\), which implies for action \(i_{1}\),

\[\mathbb{E}_{t}[\theta_{t+1}(i_{1})]=\theta_{t}(i_{1})+\eta\cdot\pi_{\theta_{ t}}(i_{1})\cdot\big{(}r(i_{1})-\pi_{\theta_{t}}^{\top}r)<\theta_{t}(i_{1}),\] (15)

for sufficiently large \(t\geq 1\), which further implies that \(\sup_{t\geq 1}\theta_{t}(i_{1})<\infty\). Meanwhile, for the optimal action \(a^{*}\), the assumption and Lemma 1 imply that \(\inf_{t\geq 1}\theta_{t}(a^{*})>-\infty\). Combining these two observations gives,

\[\sup_{t\geq 1}\frac{\pi_{\theta_{t}}(i_{1})}{\pi_{\theta_{t}}(a^{*})}=\sup_{t \geq 1}\,\exp\{\theta_{t}(i_{1})-\theta_{t}(a^{*})\}<\infty\,.\] (16)

On the other hand, since \(N_{\infty}(i_{1})=\infty\) and \(N_{\infty}(a^{*})<\infty\) (by assumption), we then have \(\sup_{t\geq 1}\exp\{\theta_{t}(i_{1})-\theta_{t}(a^{*})\}=\infty\) by Lemma 5 (see Appendix), which contradicts Eq. (16).

_Second case._ Suppose that \(N_{\infty}(a^{*})=\infty\). We will argue that \(\pi_{\theta_{t}}(a^{*})\to 1\) as \(t\to\infty\) almost surely. First, according to Lemma 2, there exists at least one sub-optimal action \(i_{1}\in[K]\), \(i_{1}\neq a^{*}\), such that \(N_{\infty}(i_{1})=\infty\). Let \(i_{1}=\arg\min_{a\in[K],N_{\infty}(a)=\infty}r(a)\). By Lemma 6 and the definition of \(a^{*}\), we have \(r(i_{1})<\pi_{\theta_{t}}^{\top}r<r(a^{*})\) for all sufficiently large \(t\geq 1\). Since \(N_{\infty}(i_{1})=\infty\), using similar calculations to Eqs. (13) and (14) in Theorem 1, we have, \(\theta_{t}(i_{1})\to-\infty\) as \(t\to\infty\). We also have, \(\inf_{t\geq 1}\theta_{t}(a^{*})>-\infty\) as \(t\to\infty\). Hence, \(\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(i_{1})}=\exp\{\theta_{t}(a^{* })-\theta_{t}(i_{1})\}\to\infty\) as \(t\to\infty\).

Define \(\mathcal{A}_{\infty}:=\{a\in[K]\mid N_{\infty}(a)=\infty\}\) as the set of actions that are sampled infinitely often, and note that \(|\mathcal{A}_{\infty}|\geq 2\) by Lemma 2. Sort the action indices in \(\mathcal{A}_{\infty}\) according to their expected reward values in descending order, i.e.,

\[r(a^{*})>r(i_{|\mathcal{A}_{\infty}|-1})>r(i_{|\mathcal{A}_{\infty}|-2})>\cdots >r(i_{2})>r(i_{1}).\] (17)

Assumption 1 is used here to prevent two arms from having the same reward and thus guarantee the inequalities are strict in Eq. (17). Next, using similar calculations as in Lemma 6, we have,

\[\pi_{\theta_{t}}^{\top}r-r(i_{2})>\pi_{\theta_{t}}(a^{*})\cdot\bigg{[}r(a^{*} )-r(i_{2})-\sum_{a^{-}\in\mathcal{A}^{-}(i_{2})}\frac{\pi_{\theta_{t}}(a^{-} )}{\pi_{\theta_{t}}(a^{*})}\cdot(r(i_{2})-r(a^{-}))\bigg{]},\] (18)

where \(\mathcal{A}^{-}(i_{2})\coloneqq\{a^{-}\in[K]:r(a^{-})<r(i_{2})\}\) is the set of actions that have lower mean reward than \(i_{2}\in[K]\), and note that \(i_{1}\in\mathcal{A}^{-}(i_{2})\). Using the above definitions, we can conclude that \(i_{1}\) is the only arm in \(\mathcal{A}^{-}(i_{2})\) that has been sampled infinitely often. According to Lemma 5, for all \(a^{-}\in\mathcal{A}^{-}(i_{2})\) with \(a^{-}\neq i_{1}\), we have, \(\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(a^{-})}\to\infty\) as \(t\to\infty\), since \(N_{\infty}(a^{*})=\infty\) (by assumption) and \(N_{\infty}(a^{-})<\infty\) (by Eq. (17)). Therefore, for all sufficiently large \(t\), the probability ratio in Eq. (18) \(\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(a^{-})}\to\infty\) for all \(a^{-}\in\mathcal{A}^{-}(i_{2})\), which implies that, for all sufficiently large \(t\geq 1\),

\[\pi_{\theta_{t}}^{\top}r-r(i_{2})>0.5\cdot\pi_{\theta_{t}}(a^{*})\cdot\big{(}r( a^{*})-r(i_{2})\big{)}>0.\] (19)

We have thus shown that \(\pi_{\theta_{t}}^{\top}r>r(i_{2})\). Recall that we had previously proved that \(\pi_{\theta_{t}}^{\top}r>r(i_{1})\). Hence, we will apply this argument recursively: after this point, \(i_{2}\in[K]\) will become the new "\(i_{1}\in[K]\)", and a similar inequality to Eq. (13) will then hold for \(i_{2}\in[K]\) from similar calculations to Eqs. (13) and (14) in Theorem 1, establishing \(\theta_{t}(i_{2})\to-\infty\) as \(t\to\infty\). This will imply that for all sufficiently large \(t\geq 1\),

\[\pi_{\theta_{t}}^{\top}r-r(i_{3})>0.5\cdot\pi_{\theta_{t}}(a^{*})\cdot\big{(}r( a^{*})-r(i_{3})\big{)}>0.\] (20)

Continuing the recursive argument, we can conclude for all actions \(a\in\mathcal{A}_{\infty}\) with \(a\neq a^{*}\) that \(\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(a)}\to\infty\) as \(t\to\infty\). Meanwhile, for all actions \(a\not\in\mathcal{A}_{\infty}\), Lemma 5 also shows that \(\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(a)}\to\infty\) as \(t\to\infty\). Combining these two results yields the conclusion that for all sub-optimal actions \(a\in[K]\) with \(r(a)<r(a^{*})\) we have \(\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(a)}\to\infty\) as \(t\to\infty\), which implies \(\pi_{\theta_{t}}(a^{*})\to 1\) as \(t\to\infty\). Thus, we have established almost sure convergence to the globally optimal policy.

Discussion.Lemma 2 is important to prove that the optimal arm will be sampled infinitely often. In particular, Lemma 2 guarantees \(|\mathcal{A}_{\infty}|\geq 2\) and the existence of \(i_{1}\) in Eq. (15), which can then be used to construct the contradiction in Eq. (16). Without Lemma 2, \(|\mathcal{A}_{\infty}|\) might be equal to \(1\) and the failure mode in Section 3.1 can occur, resulting in Algorithm 1 not sampling the optimal action infinitely often as \(t\to\infty\).

### Asymptotic Rate of Convergence

According to Theorem 2, almost surely, \(\pi_{\theta_{t}}(a^{*})\to 1\) as \(t\to\infty\). Therefore, after a large enough time \(\tau<\infty\), we have \(\pi_{\theta_{t}}(a^{*})\geq 1/2\), which implies that the "progress" term in Eq. (14) can be lower bounded. With this, an asymptotic rate of convergence can be proved as follows.

**Theorem 3**.: _For a large enough \(\tau>0\), for all \(T>\tau\), the average sub-optimality decreases at an \(O\left(\frac{\ln(T)}{T}\right)\) rate. Formally, if \(a^{*}\) is the optimal arm, then, for a constant \(c\),_

\[\frac{\sum_{s=\tau}^{T}r(a^{*})-\langle\pi_{s},r\rangle}{T}\leq\frac{c\,\ln(T )}{T-\tau}.\]

## 4 Simulation Study

To validate and enhance the theoretical findings, we ran experiments with a four action bandit environment (\(K=4\)) with a true mean reward vector of \(r=(0.2,0.05,-0.1,-0.4)^{\top}\in\mathbb{R}^{4}\). The reward distribution \(P_{a}\) for arm \(1\leq a\leq 4\) is Gaussian, centered at \(r(a)\) and with a standard deviation of \(0.1\). The environment is chosen to illustrate various phenomenon, which we discuss after presenting the results. The algorithm is Algorithm 1 with \(\theta_{1}=\bm{0}\in\mathbb{R}^{K}\).

For comparison, assuming that the random rewards belong to the \([-1,1]\) interval, the only result for the stochastic gradient bandit algorithm (Zu and others, 2014, Lemma 4.6) that allowed a constant learning rate required that the learning rate be less than \(\eta_{c}=\frac{\Delta^{2}}{40\cdot K^{3/2}\cdot R_{\max}^{3}}=\frac{9}{128000 }\approx 0.00007\), where we used \(\Delta=0.15\), \(K=4\), and \(R_{\max}=1\). While technically, the result does not apply to our case where the reward distributions have unbounded support, the probability of the reward landing outside of \([-1,1]\) is in the order of \(10^{-9}\). Choosing \(R_{\max}\) to be larger, this probability falls extremely quickly, which suggests that the above threshold is generous. For the experiments we use the learning rates \(\eta\in\{1,10,100,1000\}\), that are several orders of magnitudes larger than \(\eta_{c}\). For each learning rate, we plot the outcome of \(10\) runs, corresponding to different random seeds. Each run lasts \(10^{6}(\approx e^{14})\) iterations. The log-suboptimality gaps for the \(4\times 10\) cases are shown on Figures (a)a-(d)d, where they are plotted against the logarithm of time. Additional results for \(K=2\) arms are shown in Appendix D. Note that in this example small sub-optimality implies that the optimal arm is chosen with high probability. In what follows, we discuss the results in the plots.

Asymptotic convergence.For the smaller learning rates of \(\eta=1\) and \(10\), all \(10\) seeds rapidly and steadily converge, reaching a sub-optimality of \(e^{-14}\) or less. For \(\eta=100\) and \(1000\), most of the runs reach even small error even faster, but some runs are "stuck" even after \(10^{6}\) steps. Note that this does not contradict the theoretical result; nor do we suspect numerical issues. As seen for the case of \(\eta=100\), even after a long phase with little to no progress, a run can "recover" (see the grey curve). In fact, it is reasonable to expect that the price of increasing the learning rate is larger variance; as seen in these plots (subplots (a) and (b) are also attesting to this). Differences between learning rates are further discussed below.

Non-monotone objective value.Using a very small learning rate guarantees monotonic improvement (in expectation) in the policy's expected reward [24]. Conversely, a large learning rate results in non-monotonic evolution of the expected rewards \(\{\pi_{\theta_{t}}^{\top}r\}_{t\geq 1}\), even in the final stages of convergence, as can be seen clearly for the learning rates of \(\eta=1\) and \(10\) in Figures (a)a and (b)b. For larger \(\eta\), the non-monotone behavior happens over longer periods and is less visible in the plots. This is because using large learning rates causes the policy to rapidly increase the parameters for some action, after which the gradient becomes small, limiting further progress.

Rate of convergence.Figures (a)a and (b)b, where the log-log plot has a slope of nearly \(-1\), give some evidence that an \(O(1/t)\) asymptotic rate is achieved. In general, such a rate cannot be improvedin terms of \(t\)[14]. Theorem 3 gives a weaker version of convergence rate over averaged iterates (not last iterate), which is slightly worse than \(O(1/t)\). More work is needed to verify if the asymptotic convergence rate in Theorem 3 is improvable or not.

Different learning rates.Two observations can be made from Figure 1 regarding the effect of using different \(\eta\) values: **First**, during the final stage of convergence when \(r(a^{*})-\pi_{\theta_{t}}^{\top}r\approx 0\), using larger \(\eta\) results in faster convergence on average. As \(\eta\) increases, the order of \(\log\left(r(a^{*})-\pi_{\theta_{t}}^{\top}r\right)\) also changes from \(e^{-14}\) (\(\eta=1\)), to \(e^{-20}\) (\(\eta=100\)), and \(e^{-200}\) (\(\eta=1000\)). We conjecture that the asymptotic rate of convergence has an \(O(1/\eta)\) dependence. **Second**, using larger learning rates can take a longer time to enter the final stage of convergence. When \(\eta=1\) or \(10\), all curves quickly enter the final stage of \(r(a^{*})-\pi_{\theta_{t}}^{\top}r\approx 0\). However, for larger \(\eta\) values, \(1/10\) runs (\(\eta=100\)) and \(3/10\) runs (\(\eta=1000\)) result in \(r(a^{*})-\pi_{\theta_{t}}^{\top}r\) values far from \(0\) even after \(10^{6}\) iterations. These runs take orders of magnitude more iterations to eventually achieve \(r(a^{*})-\pi_{\theta_{t}}^{\top}r\approx 0\). These situations correspond to the policy \(\pi_{\theta_{t}}\) getting stuck near sub-optimal corners of the simplex, meaning that \(\pi_{\theta_{t}}(i)\approx 1\) for a sub-optimal action \(i\in[K]\) with \(r(i)<r(a^{*})\). In such cases, even Softmax PG with the true gradient can remain stuck on a sub-optimal plateau for an extremely long time [23]. However, the reason why larger learning rates lead to longer plateaus in the stochastic setting remains unclear.

Trade-offs and multi-stage characterizations of convergence.Given the above observations, there appears to exist a trade-off for \(\eta\): larger \(\eta\) values result in faster convergence during the final stage where \(r(a^{*})-\pi_{\theta_{t}}^{\top}r\approx 0\), but at the the cost of taking far longer to enter this final stage of convergence. Since asymptotic convergence results are insufficient for explaining these subtleties in a satisfactory manner, a more refined analysis that considers the different stages of convergence is required.

Figure 1: Log sub-optimality gap, \(\log\left(r(a^{*})-\pi_{\theta_{t}}^{\top}r\right)\), plotted against the logarithm of time, \(\log t\), in a \(4\)-action problem with various learning rates, \(\eta\). Each subplot shows a run with a specific learning rate. The curves in a subplot correspond to 10 different random seeds. Theory predicts that essentially all seeds will lead to a curve converging to zero (\(-\infty\) in these plots). For a discussion of the results, see the text.,

Conclusions and Future Directions

This work refines our understanding of stochastic gradient bandit algorithms by proving that it converges to a globally optimal policy almost surely with _any_ constant learning rate. Our new proof strategy based on the asymptotics of sample counts opens new directions for better characterizing exploration effects of stochastic gradient methods, while also suggesting interesting new questions. Characterizing the multiple stages of convergence remains another interesting future direction. One interesting possibility is that there might exist an optimal time-dependent scheme for _increasing_ the learning rate (such as \(\eta\in O(\log t)\)) to accelerate convergence, rather than use a constant \(\eta\in O(1)\). This is corroborated by our experiments: As seen in Figure 1, small learning rates perform better during the early stages of optimization, while larger learning rates achieve faster convergence during the final stage. Other directions include extending our bandit results to the more general RL setting [34], as well as extending our results for the softmax tabular parameterization to handle function approximation [1].

**Limitations:** While this work establishes a surprising asymptotic convergence result for any constant learning rate, it does not shed light on the effect of different learning rates on the convergence. Moreover, our analysis is limited to multi-armed bandits, and does not immediately extend to the general RL setting. These aspects are the main limitations of this paper.

**Broader impact:** This is primarily theoretical work on a fundamental algorithm that is used broadly in RL applications. We expect these results to improve the research community's understanding of the basic stochastic gradient bandit method.

## Acknowledgments and Disclosure of Funding

Jincheng Mei would like to thank Ramki Gummadi for providing feedback on a draft of this manuscript. Csaba Szepesvari and Dale Schuurmans gratefully acknowledge funding from the Canada CIFAR AI Chairs Program, Amii and NSERC. Sharan Vaswani acknowledges the support from the NSERC Discovery Grant (RGPIN-2022-04816).

## References

* [1] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. _Journal of Machine Learning Research_, 22(98):1-76, 2021.
* [2] Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In _Conference on learning theory_, pages 39-1. JMLR Workshop and Conference Proceedings, 2012.
* [3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47(2):235-256, 2002.
* [4] Leon Bottou. Large-scale machine learning with stochastic gradient descent. In _Proceedings of COMPSTAT'2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers_, pages 177-186. Springer, 2010.
* [5] Leo Breiman. _Probability_. SIAM, 1992.
* [6] Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of natural policy gradient methods with entropy regularization. _Operations Research_, 70(4):2563-2578, 2022.
* [7] Nicolo Cesa-bianchi and Claudio Gentile. Improved risk tail bounds for on-line algorithms. In Y. Weiss, B. Scholkopf, and J. Platt, editors, _Advances in Neural Information Processing Systems_, volume 18. MIT Press, 2005.
* [8] Denis Denisov and Neil Walton. Regret analysis of a markov policy gradient algorithm for multi-arm bandits. _arXiv preprint arXiv:2007.10229_, 2020.
* [9] Yuhao Ding, Junzi Zhang, and Javad Lavaei. Beyond exact gradients: Convergence of stochastic soft-max policy gradient methods with entropy regularization. _arXiv preprint arXiv:2110.10117_, 2021.
* 118, 1975.
* Garg et al. [2021] Shivam Garg, Samuele Tosatto, Yangchen Pan, Martha White, and A Rupam Mahmood. An alternate policy gradient estimator for softmax policies. _arXiv preprint arXiv:2112.11622_, 2021.
* Ghadimi and Lan [2013] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368, 2013.
* Kakade [2002] Sham M Kakade. A natural policy gradient. In _Advances in neural information processing systems_, pages 1531-1538, 2002.
* Lai et al. [1985] Tze Leung Lai, Herbert Robbins, et al. Asymptotically efficient adaptive allocation rules. _Advances in applied mathematics_, 6(1):4-22, 1985.
* Lan [2023] Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. _Mathematical programming_, 198(1):1059-1106, 2023.
* Lattimore and Szepesvari [2020] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Li et al. [2021] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Softmax policy gradient methods can take exponential time to converge. In _Conference on Learning Theory_, pages 3107-3110. PMLR, 2021.
* Lu et al. [2024] Michael Lu, Matin Aghaei, Anant Raj, and Sharan Vaswani. Towards principled, practical policy gradient for bandits and tabular mdps. _arXiv preprint arXiv:2405.13136_, 2024.
* Mei et al. [2022] Jincheng Mei, Wesley Chung, Valentin Thomas, Bo Dai, Csaba Szepesvari, and Dale Schuurmans. The role of baselines in policy gradient optimization. _Advances in Neural Information Processing Systems_, 35:17818-17830, 2022.
* Mei et al. [2021] Jincheng Mei, Bo Dai, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. Understanding the effect of stochasticity in policy optimization. _Advances in Neural Information Processing Systems_, 34:19339-19351, 2021.
* Mei et al. [2021] Jincheng Mei, Yue Gao, Bo Dai, Csaba Szepesvari, and Dale Schuurmans. Leveraging non-uniformity in first-order non-convex optimization. In _International Conference on Machine Learning_, pages 7555-7564. PMLR, 2021.
* Mei et al. [2020] Jincheng Mei, Chenjun Xiao, Bo Dai, Lihong Li, Csaba Szepesvari, and Dale Schuurmans. Escaping the gravitational pull of softmax. _Advances in Neural Information Processing Systems_, 33:21130-21140, 2020.
* Mei et al. [2020] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. In _International Conference on Machine Learning_, pages 6820-6829. PMLR, 2020.
* Mei et al. [2024] Jincheng Mei, Zixin Zhong, Bo Dai, Alekh Agarwal, Csaba Szepesvari, and Dale Schuurmans. Stochastic gradient succeeds for bandits. _arXiv preprint arXiv:2402.17235_, 2024.
* Nemirovski et al. [2009] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on optimization_, 19(4):1574-1609, 2009.
* Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022.
* Rafailov et al. [2024] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36, 2024.
* Robbins and Monro [1951] Herbert Robbins and Sutton Monro. A stochastic approximation method. _The annals of mathematical statistics_, pages 400-407, 1951.
* Schulman et al. [2015] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897, 2015.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement Learning: An Introduction_. MIT Press, 2018.

* Sutton et al. [1999] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. _Advances in neural information processing systems_, 12, 1999.
* Thompson [1933] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 25(3-4):285-294, 1933.
* Williams [1992] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine learning_, 8:229-256, 1992.
* Wu et al. [2024] Jingfeng Wu, Peter L Bartlett, Matus Telgarsky, and Bin Yu. Large stepsize gradient descent for logistic loss: Non-monotonicity of the loss improves optimization efficiency. _arXiv preprint arXiv:2402.15926_, 2024.
* Yuan et al. [2022] Rui Yuan, Robert M Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policy gradient. In _International Conference on Artificial Intelligence and Statistics_, pages 3332-3380. PMLR, 2022.
* Zhang et al. [2021] Junyu Zhang, Chengzhuo Ni, Csaba Szepesvari, Mengdi Wang, et al. On the convergence and sample efficiency of variance-reduced policy gradient method. _Advances in Neural Information Processing Systems_, 34:2228-2240, 2021.
* Zhang et al. [2020] Junzi Zhang, Jongho Kim, Brendan O'Donoghue, and Stephen Boyd. Sample efficient reinforcement learning with reinforce. _arXiv preprint arXiv:2010.11364_, 2020.
* Zhang et al. [2020] Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar. Global convergence of policy gradient methods to (almost) locally optimal policies. _SIAM Journal on Control and Optimization_, 58(6):3586-3612, 2020.

## Appendix A Asymptotic Convergence

**Lemma 1**.: Using Algorithm 1 with any constant \(\eta\in\Theta(1)\), if \(N_{\infty}(a)<\infty\) for an action \(a\in[K]\), then we have, almost surely,

\[\sup_{t\geq 1}\theta_{t}(a)<\infty,\text{ and }\inf_{t\geq 1}\theta_{t}(a)>-\infty.\] (21)

Proof.: Suppose \(N_{\infty}(a)<\infty\) for an action \(a\in[K]\). According to Algorithm 1,

\[\theta_{t+1}(a)\leftarrow\theta_{t}(a)+\begin{cases}\eta\cdot(1- \pi_{\theta_{t}}(a))\cdot R_{t}(a),&\text{if }a_{t}=a\,,\\ -\eta\cdot\pi_{\theta_{t}}(a)\cdot R_{t}(a_{t}),&\text{otherwise}\,,\end{cases}\] (22)

and let

\[I_{t}(a)\coloneqq\begin{cases}1,&\text{if }a_{t}=a\,,\\ 0,&\text{otherwise}\,.\end{cases}\] (23)

According to Eq. (22), we have, for all \(t\geq 1\),

\[\theta_{t}(a)-\theta_{1}(a)=\sum_{s=1}^{t-1}I_{s}(a)\cdot\eta \cdot(1-\pi_{\theta_{s}}(a))\cdot R_{s}(a)+\sum_{s=1}^{t-1}\left(1-I_{s}(a) \right)\cdot(-\eta)\cdot\pi_{\theta_{s}}(a)\cdot R_{s}(a_{s}).\] (24)

Using triangle inequality, we have,

\[|\theta_{t}(a)-\theta_{1}(a)| \leq\sum_{s=1}^{t-1}\left|I_{s}(a)\cdot\eta\cdot(1-\pi_{\theta_{ s}}(a))\cdot R_{s}(a)\right|+\sum_{s=1}^{t-1}\left|\left(1-I_{s}(a)\right) \cdot(-\eta)\cdot\pi_{\theta_{s}}(a)\cdot R_{s}(a_{s})\right|\] (25) \[\leq\eta\cdot R_{\max}\cdot\sum_{s=1}^{t-1}I_{s}(a)+\eta\cdot R_ {\max}\cdot\sum_{s=1}^{t-1}\pi_{\theta_{s}}(a)\] (26) \[=\eta\cdot R_{\max}\cdot\Big{(}N_{t-1}(a)+\sum_{s=1}^{t-1}\pi_{ \theta_{s}}(a)\Big{)}.\] (27)

By assumption, we have,

\[N_{\infty}(a)\coloneqq\lim_{t\to\infty}N_{t}(a)<\infty.\] (28)

According to the extended Borel-Cantelli Lemma 3, we have, almost surely,

\[\sum_{t=1}^{\infty}\pi_{\theta_{t}}(a)\coloneqq\lim_{t\to\infty }\sum_{s=1}^{t}\pi_{\theta_{s}}(a)<\infty.\] (29)

Combining Eqs. (25), (28) and (29), we have, almost surely,

\[\sup_{t\geq 1}|\theta_{t}(a)-\theta_{1}(a)|<\infty,\] (30)

which implies that, almost surely,

\[\sup_{t\geq 1}|\theta_{t}(a)|\leq\sup_{t\geq 1}|\theta_{t}(a)-\theta_{1}(a)|+ |\theta_{1}(a)|<\infty.\qed\]

**Lemma 2** (Avoiding lack of exploration).: Using Algorithm 1 with any \(\eta\in\Theta(1)\), there exists at least a pair of distinct actions \(i,j\in[K]\) and \(i\neq j\), such that, almost surely,

\[N_{\infty}(i)=\infty,\text{ and }N_{\infty}(j)=\infty.\] (31)Proof.: First, we have, for all \(t\geq 1\),

\[t =\sum_{s=1}^{t}\sum_{a\in[K]}I_{s}(a)\qquad\Big{(}\sum_{a\in[K]}I_{t }(a)=1\text{ for all }t\geq 1\Big{)}\] (32) \[=\sum_{a\in[K]}\sum_{s=1}^{t}I_{s}(a)\] (33) \[=\sum_{a\in[K]}N_{t}(a).\] (34)

By pigeonhole principle, there exists at least one action \(i\in[K]\), such that, almost surely,

\[N_{\infty}(i)\coloneqq\lim_{t\to\infty}N_{t}(i)=\infty.\] (35)

We argue the existence of another action by contradiction. Suppose for all the other actions \(j\in[K]\) and \(j\neq i\), we have \(N_{\infty}(j)<\infty\). According to the extended Borel-Cantelli Lemma 3, we have, almost surely,

\[\sum_{t=1}^{\infty}\pi_{\theta_{t}}(j)\coloneqq\lim_{t\to\infty} \sum_{s=1}^{t}\pi_{\theta_{s}}(j)<\infty.\] (36)

According to the update Eq. (22), we have, for all \(t\geq 1\),

\[\theta_{t}(i)-\theta_{1}(i)=\sum_{s=1}^{t-1}I_{s}(i)\cdot\eta \cdot(1-\pi_{\theta_{s}}(i))\cdot R_{s}(i)+\sum_{s=1}^{t-1}\left(1-I_{s}(i) \right)\cdot(-\eta)\cdot\pi_{\theta_{s}}(i)\cdot R_{s}(a_{s}).\] (37)

By triangle inequality, we have,

\[|\theta_{t}(i)-\theta_{1}(i)| \leq\sum_{s=1}^{t-1}\Big{|}I_{s}(i)\cdot\eta\cdot(1-\pi_{\theta_ {s}}(i))\cdot R_{s}(i)\Big{|}+\sum_{s=1}^{t-1}\Big{|}\left(1-I_{s}(i)\right) \cdot(-\eta)\cdot\pi_{\theta_{s}}(i)\cdot R_{s}(a_{s})\Big{|}\] (38) \[\leq\eta\cdot R_{\max}\cdot\sum_{s=1}^{t-1}\left(1-\pi_{\theta_{ s}}(i)\right)+\eta\cdot R_{\max}\cdot\sum_{s=1}^{t-1}\left(1-I_{s}(i)\right)\] (39) \[=\eta\cdot R_{\max}\cdot\Big{(}\sum_{s=1}^{t-1}\sum_{j\neq i} \pi_{\theta_{s}}(j)+\sum_{s=1}^{t-1}\sum_{j\neq i}I_{s}(j)\Big{)}\] (40) \[=\eta\cdot R_{\max}\cdot\Big{(}\sum_{j\neq i}\sum_{s=1}^{t-1}\pi_ {\theta_{s}}(j)+\sum_{j\neq i}N_{t-1}(j)\Big{)}.\] (41)

Combing Eqs. (36) and (38) and the assumption of \(N_{\infty}(j)<\infty\) for all \(j\neq i\), we have, almost surely,

\[\sup_{t\geq 1}|\theta_{t}(i)|\leq\sup_{t\geq 1}|\theta_{t}(i)-\theta_{1}(i)|+| \theta_{1}(i)|<\infty.\] (42)

Since \(N_{\infty}(j)<\infty\) for all \(j\neq i\) by assumption, and according to Lemma 1, we have, almost surely,

\[\sup_{t\geq 1}|\theta_{t}(j)|<\infty.\] (43)

Combining Eqs. (42) and (43), we have, for all action \(a\in[K]\),

\[\sup_{t\geq 1}|\theta_{t}(a)|<\infty,\] (44)

which implies that, there exists \(c>0\) and \(c\in O(1)\), such that, for all \(a\in[K]\),

\[\inf_{t\geq 1}\pi_{\theta_{t}}(a)=\inf_{t\geq 1}\frac{\exp\{\theta_{t}(a)\}}{ \sum_{a^{\prime}\in[K]}\exp\{\theta_{t}(a^{\prime})\}}\geq c>0.\] (45)Therefore, for all action \(a\in[K]\),

\[\sum_{t=1}^{\infty}\pi_{\theta_{t}}(a) \coloneqq\lim_{t\to\infty}\sum_{s=1}^{t}\pi_{\theta_{s}}(a)\] (46) \[\geq\lim_{t\to\infty}\sum_{s=1}^{t}c\] (47) \[=\lim_{t\to\infty}t\cdot c\] (48) \[=\infty.\] (49)

According to the extended Borel-Cantelli Lemma 3, we have, almost surely, for all action \(a\in[K]\),

\[N_{\infty}(a)=\infty,\] (50)

which is a contradiction with the assumption of \(N_{\infty}(j)<\infty\) for all the other actions \(j\in[K]\) with \(j\neq i\). Therefore, there exists another action \(j\in[K]\) with \(j\neq i\), such that \(N_{\infty}(j)=\infty\). 

**Theorem 1**.: Let \(K=2\) and \(r(1)>r(2)\). Using Algorithm 1 with any \(\eta\in\Theta(1)\), we have, almost surely, \(\pi_{\theta_{t}}(a^{*})\to 1\) as \(t\to\infty\), where \(a^{*}\coloneqq\arg\max_{a\in[K]}r(a)\) (equal to Action \(1\) in this case).

Proof.: The proof uses the same notations as of [24, Theorem 5.1].

According to Lemma 2, we have, \(N_{\infty}(1)=\infty\) and \(N_{\infty}(2)=\infty\), i.e., both of the two actions are sampled for infinitely many times as \(t\to\infty\).

Let \(\mathcal{F}_{t}\) be the \(\sigma\)-algebra generated by \(a_{1}\), \(R_{1}(a_{1})\), \(\cdots\), \(a_{t-1}\), \(R_{t-1}(a_{t-1})\):

\[\mathcal{F}_{t}=\sigma(\{a_{1},R_{1}(a_{1}),\cdots,a_{t-1},R_{t-1}(a_{t-1})\})\,.\] (51)

Note that \(\theta_{t}\) and \(I_{t}\) (defined by Eq. (23)) are \(\mathcal{F}_{t}\)-measurable for all \(t\geq 1\). Let \(\mathbb{E}_{t}[\cdot]\) denote the conditional expectation with respect to \(\mathcal{F}_{t}\): \(\mathbb{E}_{t}[X]=\mathbb{E}[X|\mathcal{F}_{t}]\). Define the following notations,

\[W_{t}(a) \coloneqq\theta_{t}(a)-\mathbb{E}_{t-1}[\theta_{t}(a)],\qquad( \text{``noise''})\] (52) \[P_{t}(a) \coloneqq\mathbb{E}_{t}[\theta_{t+1}(a)]-\theta_{t}(a).\qquad( \text{``progress''})\] (53)

For each action \(a\in[K]\), for \(t\geq 2\), we have the following decomposition,

\[\theta_{t}(a)=W_{t}(a)+P_{t-1}(a)+\theta_{t-1}(a).\] (54)

By recursion we can determine that,

\[\theta_{t}(a)=\mathbb{E}[\theta_{1}(a)]+\sum_{s=1}^{t}W_{s}(a)+\sum_{s=1}^{t- 1}P_{s}(a),\] (55)

while we also have,

\[\theta_{1}(a)=\underbrace{\theta_{1}(a)-\mathbb{E}[\theta_{1}(a)]}_{W_{1}(a)}+ \mathbb{E}[\theta_{1}(a)],\] (56)

and \(\mathbb{E}[\theta_{1}(a)]\) accounts for potential randomness in initializing \(\theta_{1}\in\mathbb{R}^{K}\). According to Proposition 1, we have,

\[P_{t}(a)=\mathbb{E}_{t}[\theta_{t+1}(a)]-\theta_{t}(a)=\eta\cdot\pi_{\theta_ {t}}(a)\cdot\left(r(a)-\pi_{\theta_{t}}^{\top}r\right),\] (57)

which implies that, for all \(t\geq 1\),

\[P_{t}(a^{*})>0>P_{t}(2).\] (58)

Next, we show that \(\sum_{s=1}^{t}P_{s}(a^{*})\to\infty\) as \(t\to\infty\) by contradiction. Suppose that,

\[\sum_{t=1}^{\infty}P_{t}(a^{*})<\infty.\] (59)For the optimal action \(a^{*}=1\),

\[\sum_{s=1}^{t}P_{s}(a^{*}) =\sum_{s=1}^{t}\eta\cdot\pi_{\theta_{s}}(a^{*})\cdot(r(a^{*})-\pi_{ \theta_{s}}^{\top}r)\] (60) \[=\eta\cdot\Delta\cdot\sum_{s=1}^{t}\pi_{\theta_{s}}(a^{*})\cdot(1 -\pi_{\theta_{s}}(a^{*})),\] (61)

where \(\Delta\coloneqq r(a^{*})-\max_{a\neq a^{*}}r(a)=r(1)-r(2)>0\) is the reward gap. Denote that, for all \(t\geq 1\),

\[V_{t}(a^{*})\coloneqq\frac{5}{18}\cdot\sum_{s=1}^{t-1}\pi_{\theta_{s}}(a^{*}) \cdot(1-\pi_{\theta_{s}}(a^{*})).\] (62)

According to Lemma 8 (using Eqs. (58) to (60) and (62)), we have, almost surely,

\[\sup_{t\geq 1}|\theta_{t}(a^{*})|<\infty.\] (63)

For the sub-optimal action (Action \(2\)), we have,

\[\sum_{s=1}^{t}P_{s}(2) =\sum_{s=1}^{t}\eta\cdot\pi_{\theta_{s}}(2)\cdot(r(2)-\pi_{ \theta_{s}}^{\top}r)\] (64) \[=-\eta\cdot\Delta\cdot\sum_{s=1}^{t}\pi_{\theta_{s}}(2)\cdot(1- \pi_{\theta_{s}}(2))\] (65) \[=-\sum_{s=1}^{t}P_{s}(a^{*}),\] (66)

and also denote, for all \(t\geq 1\),

\[V_{t}(2)\coloneqq\frac{5}{18}\cdot\sum_{s=1}^{t-1}\pi_{\theta_{s}}(2)\cdot(1- \pi_{\theta_{s}}(2))=V_{t}(a^{*}).\] (67)

According to Lemma 8 (using Eqs. (58), (59), (64) and (67)), we have, almost surely,

\[\sup_{t\geq 1}\left|\theta_{t}(2)\right|<\infty.\] (68)

Combining Eqs. (63) and (68), there exists \(c>0\) and \(c\in O(1)\), such that, for all \(a\in\{a^{*},2\}\),

\[\inf_{t\geq 1}\pi_{\theta_{t}}(a)=\inf_{t\geq 1}\frac{\exp\{\theta_{t}(a)\}}{ \sum_{a^{\prime}\in[K]}\exp\{\theta_{t}(a^{\prime})\}}\geq c>0,\] (69)

which implies that,

\[\sum_{t=1}^{\infty}\pi_{\theta_{t}}(a^{*})\cdot(1-\pi_{\theta_{t }}(a^{*})) \coloneqq\lim_{t\to\infty}\sum_{s=1}^{t}\pi_{\theta_{s}}(a^{*}) \cdot(1-\pi_{\theta_{s}}(a^{*}))\] (70) \[\geq\lim_{t\to\infty}\sum_{s=1}^{t}c\cdot c\] (71) \[=\lim_{t\to\infty}t\cdot c^{2}\] (72) \[=\infty,\] (73)

which is a contradiction with the assumption of Eq. (59). Therefore, we have,

\[\sum_{s=1}^{t}P_{s}(a^{*})\to\infty,\text{ as }t\to\infty.\] (74)

According to Lemma 9 (using Eqs. (58), (60), (62) and (74)), we have, almost surely,

\[\theta_{t}(a^{*})\to\infty,\text{ as }t\to\infty.\] (75)Similarly, according to Lemma 10 (using Eqs. (58), (64), (67) and (74)), we have, almost surely,

\[\lim_{t\to\infty}\theta_{t}(2)=-\infty.\] (76)

Note that,

\[\pi_{\theta_{t}}(a^{*}) =\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(2)+\pi_{\theta_{t }}(a^{*})}\] (77) \[=\frac{1}{\exp\{\theta_{t}(2)-\theta_{t}(a^{*})\}+1}.\] (78)

Combining Eqs. (75) to (77), we have, almost surely,

\[\pi_{\theta_{t}}(a^{*})\to 1,\text{ as }t\to\infty.\qed\]

**Theorem 2**.: Under Assumption 1, given \(K\geq 2\), using Algorithm 1 with any \(\eta\in\Theta(1)\), we have, almost surely, \(\pi_{\theta_{t}}(a^{*})\to 1\) as \(t\to\infty\), where \(a^{*}=\arg\max_{a\in[K]}r(a)\) is the optimal action.

Proof.: Similar to the proof of Theorem 1, we define \(\mathcal{F}_{t}\) to be the \(\sigma\)-algebra generated by \(a_{1}\), \(R_{1}(a_{1})\), \(\cdots\), \(a_{t-1}\), \(R_{t-1}(a_{t-1})\) i.e.

\[\mathcal{F}_{t}=\sigma(\{a_{1},R_{1}(a_{1}),\cdots,a_{t-1},R_{t-1}(a_{t-1})\})\,.\] (79)

Note that \(\theta_{t}\) and \(I_{t}\) (defined by Eq. (23)) are \(\mathcal{F}_{t}\)-measurable for all \(t\geq 1\). Let \(\mathbb{E}_{t}[\cdot]\) denote the conditional expectation with respect to \(\mathcal{F}_{t}\): \(\mathbb{E}_{t}[X]=\mathbb{E}[X|\mathcal{F}_{t}]\). Define the following notations,

\[W_{t}(a) \coloneqq\theta_{t}(a)-\mathbb{E}_{t-1}[\theta_{t}(a)],\qquad( \text{``noise''})\] (80) \[P_{t}(a) \coloneqq\mathbb{E}_{t}[\theta_{t+1}(a)]-\theta_{t}(a).\qquad( \text{``progress''})\] (81)

For each action \(a\in[K]\), for \(t\geq 2\), we have the following decomposition,

\[\theta_{t}(a)=W_{t}(a)+P_{t-1}(a)+\theta_{t-1}(a).\] (82)

By recursion we can determine that,

\[\theta_{t}(a)=\mathbb{E}[\theta_{1}(a)]+\sum_{s=1}^{t}W_{s}(a)+\sum_{s=1}^{t- 1}P_{s}(a),\] (83)

while we also have,

\[\theta_{1}(a)=\underbrace{\theta_{1}(a)-\mathbb{E}[\theta_{1}(a)]}_{W_{1}(a) }+\mathbb{E}[\theta_{1}(a)],\] (84)

and \(\mathbb{E}[\theta_{1}(a)]\) accounts for potential randomness in initializing \(\theta_{1}\in\mathbb{R}^{K}\). For an action \(a\in[K]\),

\[\mathcal{A}^{+}(a) \coloneqq\left\{a^{+}\in[K]:r(a^{+})>r(a)\right\},\] (85) \[\mathcal{A}^{-}(a) \coloneqq\left\{a^{-}\in[K]:r(a^{-})<r(a)\right\}.\] (86)

Define \(\mathcal{A}_{\infty}\) as the set of actions which are sampled for infinitely many times as \(t\to\infty\), i.e.,

\[\mathcal{A}_{\infty} \coloneqq\left\{a\in[K]\mid N_{\infty}(a)=\infty\right\}.\] (87)

**First part.** We show that \(N_{\infty}(a^{*})=\infty\) by contradiction.

Suppose \(a^{*}\not\in\mathcal{A}_{\infty}\) i.e. \(N_{\infty}(a^{*})<\infty\). According to Lemma 2, we have, \(|\mathcal{A}_{\infty}|\geq 2\). Since \(a^{*}\not\in\mathcal{A}_{\infty}\) by assumption, there must be at least two other sub-optimal actions \(i_{1},i_{2}\in[K]\), \(i_{1}\neq i_{2}\), such that,

\[N_{\infty}(i_{1})=N_{\infty}(i_{2})=\infty.\] (88)

In particular, define

\[i_{1} \coloneqq\operatorname*{arg\,min}_{\begin{subarray}{c}a\in[K],\\ N_{\infty}(a)=\infty\end{subarray}}r(a),\] (89) \[i_{2} \coloneqq\operatorname*{arg\,max}_{\begin{subarray}{c}a\in[K],\\ N_{\infty}(a)=\infty\end{subarray}}r(a).\] (90)

[MISSING_PAGE_EMPTY:18]

where

\[\Delta\coloneqq\min_{i,j\in[K],\ i\neq j}|r(i)-r(j)|>0.\qquad(\text{by Assumption \ref{Assumption:1}})\] (106)

Bounding the variance for arm \(i_{1}\), we have that for all large enough \(\tau\geq 1\),

\[V_{t}(i_{1}) \coloneqq\frac{5}{18}\cdot\sum_{s=\tau}^{t-1}\pi_{\theta_{s}}(i_{1 })\cdot(1-\pi_{\theta_{s}}(i_{1}))\] (107) \[=\frac{5}{18}\cdot\sum_{s=\tau}^{t-1}\pi_{\theta_{s}}(i_{1})\cdot \left(\sum_{a^{-}\in\mathcal{A}^{-}(i_{1})}\pi_{\theta_{s}}(a^{-})+\sum_{a^{+} \in\mathcal{A}^{+}(i_{1})}\pi_{\theta_{s}}(a^{+})\right)\] (108)

Using similar calculations as for the progress term, we have,

\[\sum_{a^{-}\in\mathcal{A}^{-}(i_{1})}\frac{\pi_{\theta_{s}}(a^{-})}{\sum_{a^{ +}\in\mathcal{A}^{+}(i_{1})}\pi_{\theta_{s}}(a^{+})}<\sum_{a^{-}\in\mathcal{A }^{-}(i_{1})}\frac{\pi_{\theta_{s}}(a^{-})}{\pi_{\theta_{s}}(i_{2})}\qquad( \text{Fewer terms in the denominator})\]

By Lemma 5, for large enough \(\tau\geq 1\), for all \(a^{-}\), \(\frac{\pi_{\theta_{s}}(a^{-})}{\pi_{\theta_{s}}(i_{2})}\leq\frac{13}{5}\ \frac{1}{|\mathcal{A}^{-}(i_{1})|}\). Hence,

\[\leq\sum_{a^{-}\in\mathcal{A}^{-}(i_{1})}\frac{13}{5}\cdot\frac{1 }{|\mathcal{A}^{-}(i_{1})|}\] (109) \[=\frac{13}{5}\] (110) \[\implies\sum_{a^{-}\in\mathcal{A}^{-}(i_{1})}\pi_{\theta_{s}}(a^ {-}) \leq\frac{13}{5}\ \sum_{a^{+}\in\mathcal{A}^{+}(i_{1})}\pi_{\theta_{s}}(a^{+})\] (111)

Combining Eqs. (108) and (111), we have that for large enough \(\tau\geq 1\),

\[V_{t}(i_{1}) =\frac{5}{18}\cdot\sum_{s=\tau}^{t-1}\pi_{\theta_{s}}(i_{1}) \cdot(1-\pi_{\theta_{s}}(i_{1}))\] (112) \[\leq\sum_{s=\tau}^{t-1}\pi_{\theta_{s}}(i_{1})\sum_{a^{+}\in \mathcal{A}^{+}(i_{1})}\pi_{\theta_{s}}(a^{+}).\] (113)

According to Lemma 12 (using Eqs. (94), (105) and (113)), we have, almost surely,

\[\sup_{t\geq 1}\theta_{t}(i_{1})<\infty.\] (114)

Since \(N_{\infty}(a^{*})<\infty\) by assumption, and according to Lemma 1, we have, almost surely,

\[\inf_{t\geq 1}\theta_{t}(a^{*})>-\infty.\] (115)

Combining Eqs. (114) and (115), we have,

\[\sup_{t\geq 1}\frac{\pi_{\theta_{t}}(i_{1})}{\pi_{\theta_{t}}(a^{*})}=\sup_{t \geq 1}\,\exp\{\theta_{t}(i_{1})-\theta_{t}(a^{*})\}<\infty\,.\] (116)

On the other hand, by Lemma 5, we have,

\[\sup_{t\geq 1}\frac{\pi_{\theta_{t}}(i_{1})}{\pi_{\theta_{t}}(a^{*})}=\infty\] (117)

which contradicts Eq. (116). Hence, the assumption that \(N_{\infty}(a^{*})<\infty\) cannot hold. This completes the proof by contradiction, and implies that \(N_{\infty}(a^{*})=\infty\).

**Second part.** With \(a^{*}\in\mathcal{A}_{\infty}\) i.e. \(N_{\infty}(a^{*})=\infty\), we now argue that \(\pi_{\theta_{t}}(a^{*})\to 1\) as \(t\to\infty\) almost surely.

According to Lemma 2, we have, \(|\mathcal{A}_{\infty}|\geq 2\). Since \(a^{*}\in\mathcal{A}_{\infty}\) by assumption, there must be at least one sub-optimal action \(i_{1}\in[K]\) with \(r(i_{1})<r(a^{*})\), such that,

\[N_{\infty}(i_{1})=\infty.\] (118)

In particular, define

\[i_{1}\coloneqq\operatorname*{arg\,min}_{\begin{subarray}{c}a\in[K],\\ N_{\infty}(a)=\infty\end{subarray}}r(a).\] (119)

According to Lemma 6, we have, for all sufficiently large \(\tau\geq 1\),

\[r(i_{1})<\pi_{\theta_{t}}^{\top}r<r(a^{*}).\] (120)

Using the same arguments as in the first part (except that \(i_{2}\) in Eq. (98) is replaced with \(a^{*}\)), both Eqs. (105) and (113) hold.

Next, we argue that \(\sum_{s=\tau}^{t}\pi_{\theta_{s}}(i_{1})\sum_{a^{+}\in\mathcal{A}^{+}(i_{1})} \pi_{\theta_{s}}(a^{+})\to\infty\) as \(t\to\infty\) by contradiction.

Suppose

\[\sum_{t=\tau}^{\infty}\pi_{\theta_{t}}(i_{1})\sum_{a^{+}\in \mathcal{A}^{+}(i_{1})}\pi_{\theta_{t}}(a^{+})<\infty.\] (121)

According to Lemma 8 (using Eqs. (94), (105), (113) and (121)), we have,

\[\sup_{t\geq 1}|\theta_{t}(i_{1})|<\infty.\] (122)

Calculating the progress and variance for arm \(a^{*}\), for \(t\geq 1\),

\[P_{t}(a^{*}) =\eta\cdot\pi_{\theta_{t}}(a^{*})\cdot(r(a^{*})-\pi_{\theta_{t}} ^{\top}r)\] (123) \[\geq\eta\cdot\Delta\cdot\pi_{\theta_{t}}(a^{*})\cdot\big{(}1-\pi _{\theta_{t}}(a^{*})\big{)}.\qquad\big{(}\Delta\coloneqq r(a^{*})-\max_{a \neq a^{*}}r(a)\big{)}\] (124) \[\geq 0.\] (125)

Denote that, for all \(t\geq 1\),

\[V_{t}(a^{*})\coloneqq\frac{5}{18}\cdot\sum_{s=1}^{t-1}\pi_{ \theta_{s}}(a^{*})\cdot(1-\pi_{\theta_{s}}(a^{*})).\] (126)

According to Lemma 11 (using Eqs. (123) and (126)), we have,

\[\inf_{t\geq 1}\theta_{t}(a^{*})>-\infty.\] (127)

Combining Eqs. (122) and (127), we have,

\[\sup_{t\geq 1}\frac{\pi_{\theta_{t}}(i_{1})}{\pi_{\theta_{t}}(a^{*})} =\sup_{t\geq 1}\exp\{\theta_{t}(i_{1})-\theta_{t}(a^{*})\}<\infty,\] (128)

For all \(t\geq 1\), \(|\theta_{t}(i_{1})|<\infty\) and since there is at least one arm (\(a^{*}\)) s.t. \(\inf_{t\geq 1}\theta_{t}(a)>-\infty\), there exists \(\epsilon>0\) and \(\epsilon\in O(1)\), such that,

\[\sup_{t\geq 1}\pi_{\theta_{t}}(i_{1})<1-2\,\epsilon.\] (129)

According to Eq. (118), we know that \(N_{\infty}(i_{1})=\infty\) and for all \(a^{-}\in\mathcal{A}^{-}(i_{1})\), \(N_{\infty}(a^{-})<\infty\). Using Lemma 5, we have, for all large enough \(t\geq 1\), \(\frac{\pi_{\theta_{t}}(a^{-})}{\pi_{\theta_{t}}(i_{1})}<\frac{\epsilon}{| \mathcal{A}^{-}(i_{1})|}\).

\[\pi_{\theta_{t}}(i_{1})+\sum_{a^{+}\in\mathcal{A}^{+}(i_{1})}\pi_ {\theta_{t}}(a^{+}) =1-\pi_{\theta_{t}}(i_{1})\sum_{a^{-}\in\mathcal{A}^{-}(i_{1})} \frac{\pi_{\theta_{t}}(a^{-})}{\pi_{\theta_{t}}(i_{1})}>1-\pi_{\theta_{t}}(i_ {1})\,\epsilon\] (130) \[\implies\pi_{\theta_{t}}(i_{1})+\sum_{a^{+}\in\mathcal{A}^{+}(i_{1 })}\pi_{\theta_{t}}(a^{+}) \geq 1-\epsilon.\] (131)

[MISSING_PAGE_EMPTY:21]

Combining Eqs. (142) and (145), we have, for all sufficiently large \(t\geq 1\),

\[\pi_{\theta_{t}}^{\top}r-r(i_{2})>\pi_{\theta_{t}}(a^{*})\cdot\frac{r(a^{*})-r(i _{2})}{2}>0.\] (146)

Hence we have, for all sufficiently large \(\tau\geq 1\),

\[r(i_{2})<\pi_{\theta_{t}}^{\top}r<r(a^{*})\] (147)

Comparing Eqs. (120) and (147), we can use a similar argument for \(i_{2}\) and conclude that, for sufficiently large \(\tau\geq 1\), then, \(\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(i_{2})}\to\infty,\;\text{as}\;t \to\infty\). This further implies that

\[r(i_{3})<\pi_{\theta_{t}}^{\top}r<r(a^{*}).\] (148)

Continuing this recursive argument, we have, for all actions \(a\in\mathcal{A}_{\infty}\) with \(a\neq a^{*}\),

\[\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(a)}\to\infty,\;\text{as}\;t \to\infty.\] (149)

Meanwhile, according to Lemma 5, we have, for all actions \(a\not\in\mathcal{A}_{\infty}\),

\[\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(a)}\to\infty,\;\text{as}\;t \to\infty.\] (150)

Combining Eqs. (149) and (150), we have, for all sub-optimal actions \(a\in[K]\) with \(r(a)<r(a^{*})\),

\[\frac{\pi_{\theta_{t}}(a^{*})}{\pi_{\theta_{t}}(a)}\to\infty,\;\text{as}\;t \to\infty.\] (151)

Finally, note that,

\[\pi_{\theta_{t}}(a^{*}) =\frac{\pi_{\theta_{t}}(a^{*})}{\sum_{a\in[K]:\;r(a)<r(a^{*})} \pi_{\theta_{t}}(a)+\pi_{\theta_{t}}(a^{*})}\] (152) \[=\frac{1}{\sum_{a\in[K]:\;r(a)<r(a^{*})}\frac{\pi_{\theta_{t}}(a )}{\pi_{\theta_{t}}(a^{*})}+1}.\] (153)

Combining Eqs. (151) and (153), we have, almost surely,

\[\pi_{\theta_{t}}(a^{*})\to 1,\;\text{as}\;t\to\infty.\qed\]

## Appendix B Miscellaneous Extra Supporting Results

**Lemma 3** (Extended Borel-Cantelli Lemma, Corollary 5.29 of [5]).: _Let \((\mathcal{F}_{n})_{n\geq 1}\) be a filtration, \(A_{n}\in\mathcal{F}_{n}\). Then, almost surely,_

\[\left\{\omega\,:\,\omega\in A_{n}\text{ infinitely often }\right\}=\left\{ \omega\,:\,\sum_{n=1}^{\infty}\mathbb{P}(A_{n}|\mathcal{F}_{n})=\infty \right\}\,.\] (154)

**Lemma 4** (Freedman's inequality [10, 7], Theorem C.3 of [24]).: _Let \(X_{1},X_{2},\dots\) be a sequence of random variables, such that for all \(t\geq 1\), \(|X_{t}|\leq 1/2\). Define_

\[S_{n}\coloneqq\left|\sum_{t=1}^{n}\mathbb{E}[X_{t}|X_{1},\dots,X_{t-1}]-X_{t} \right|\quad\text{and}\quad V_{n}\coloneqq\sum_{t=1}^{n}\mathrm{Var}[X_{t}|X_{ 1},\dots,X_{t-1}].\] (155)

_Then, for all \(\delta>0\),_

\[\Pr\left(\exists\;n:\;S_{n}\geq 6\;\sqrt{\left(V_{n}+\frac{4}{3}\right)\; \log\left(\frac{V_{n}+1}{\delta}\right)}+2\log\left(\frac{1}{\delta}\right)+ \frac{4}{3}\log 3\;\right)\leq\delta.\] (156)

**Lemma 5**.: _Using Algorithm 1, for any two different actions \(i,j\in[K]\) with \(i\neq j\), if \(N_{\infty}(i)=\infty\) and \(N_{\infty}(j)<\infty\), then we have, almost surely,_

\[\sup_{t\geq 1}\frac{\pi_{\theta_{t}}(i)}{\pi_{\theta_{t}}(j)}=\infty.\] (157)

Proof.: We prove the result by contradiction. Suppose

\[c\coloneqq\sup_{t\geq 1}\frac{\pi_{\theta_{t}}(i)}{\pi_{\theta_{t}}(j)}<\infty.\] (158)

According to the extended Borel-Cantelli Lemma 3, we have, for all \(a\in[K]\), almost surely,

\[\Big{\{}\sum_{t\geq 1}\pi_{\theta_{t}}(j)=\infty\Big{\}}=\left\{N_{\infty}(j)= \infty\right\}.\] (159)

Hence, taking complements, we have,

\[\Big{\{}\sum_{t\geq 1}\pi_{\theta_{t}}(j)<\infty\Big{\}}=\left\{N_{\infty}(j)<\infty\right\}\] (160)

also holds almost surely, which implies that,

\[\sum_{t=1}^{\infty}\pi_{\theta_{t}}(i)=\infty,\text{ and }\] (161) \[\sum_{t=1}^{\infty}\pi_{\theta_{t}}(j)<\infty.\] (162)

Therefore, we have,

\[\sum_{t=1}^{\infty}\pi_{\theta_{t}}(i) =\sum_{t=1}^{\infty}\pi_{\theta_{t}}(j)\cdot\frac{\pi_{\theta_{t }}(i)}{\pi_{\theta_{t}}(j)}\] (163) \[\leq c\cdot\sum_{t=1}^{\infty}\pi_{\theta_{t}}(j)\] (164) \[<\infty,\] (165)

which is a contradiction with Eq. (161).

**Lemma 6**.: _Using Algorithm 1, we have, almost surely, for all large enough \(t\geq 1\),_

\[r(i_{1})<\pi_{\theta_{t}}^{\top}r<r(i_{2}),\] (166)

_where \(i_{1},i_{2}\in[K]\) and \(i_{1}\neq i_{2}\) are the action indices defined as,_

\[i_{1}\coloneqq\operatorname*{arg\,min}_{\begin{subarray}{c}a\in[K],\\ N_{\infty}(a)=\infty\end{subarray}}r(a),\] (167) \[i_{2}\coloneqq\operatorname*{arg\,max}_{\begin{subarray}{c}a \in[K],\\ N_{\infty}(a)=\infty\end{subarray}}r(a).\] (168)

Proof.: Define \(\mathcal{A}_{\infty}\) as the set of actions which are sampled for infinitely many times as \(t\to\infty\), i.e.,

\[\mathcal{A}_{\infty}\coloneqq\left\{a\in[K]\mid N_{\infty}(a)=\infty\right\}.\] (169)

According to Lemma 2, we have \(|\mathcal{A}_{\infty}|\geq 2\), which implies that \(i_{1}\neq i_{2}\).

Given any sub-optimal action \(i\in[K]\) with \(r(i)<r(a^{*})\), we partition the remaining actions into two parts using \(r(i)\).

\[\mathcal{A}^{+}(i) \coloneqq\left\{a^{+}\in[K]:r(a^{+})>r(i)\right\},\] (170) \[\mathcal{A}^{-}(i) \coloneqq\left\{a^{-}\in[K]:r(a^{-})<r(i)\right\}.\] (171)

By definition, we have,

\[\mathcal{A}_{\infty}\subseteq\mathcal{A}^{+}(i_{1})\cup\{i_{1}\}, \text{ and }\] (172) \[\mathcal{A}_{\infty}\subseteq\mathcal{A}^{-}(i_{2})\cup\{i_{2}\}.\] (173)

**First part.**\(r(i_{1})<\pi_{\theta_{t}}^{\top}r\).

If \(r(i_{1})=\min_{a\in[K]}r(a)\), i.e., \(i_{1}\) is the "worst action", then \(r(i_{1})<\pi_{\theta_{t}}^{\top}r\) holds trivially. Otherwise, suppose \(r(i_{1})\neq\min_{a\in[K]}r(a)\). We have,

\[\pi_{\theta_{t}}^{\top}r-r(i_{1})=\sum_{a^{+}\in\mathcal{A}^{+}(i_{1})}\pi_{ \theta_{t}}(a^{+})\cdot(r(a^{+})-r(i_{1}))-\sum_{a^{-}\in\mathcal{A}^{-}(i_{ 1})}\pi_{\theta_{t}}(a^{-})\cdot(r(i_{1})-r(a^{-})).\] (174)

Consider the non-empty set \(\mathcal{A}_{\infty}\cap\mathcal{A}^{+}(i_{1})\). Pick an action \(j_{1}\in\mathcal{A}_{\infty}\cap\mathcal{A}^{+}(i_{1})\), and ignore all the other actions \(a^{+}\in\mathcal{A}^{+}(i_{1})\) with \(a^{+}\neq j_{1}\) in the above equation. We have,

\[\pi_{\theta_{t}}^{\top}r-r(i_{1}) >\pi_{\theta_{t}}(j_{1})\cdot(r(j_{1})-r(i_{1}))-\sum_{a^{-} \in\mathcal{A}^{-}(i_{1})}\pi_{\theta_{t}}(a^{-})\cdot(r(i_{1})-r(a^{-}))\] (175) \[=\pi_{\theta_{t}}(j_{1})\cdot\bigg{[}r(j_{1})-r(i_{1})-\sum_{a^{ -}\in\mathcal{A}^{-}(i_{1})}\frac{\pi_{\theta_{t}}(a^{-})}{\pi_{\theta_{t}}(j _{1})}\cdot(r(i_{1})-r(a^{-}))\bigg{]},\] (176)

where the first inequality is because of \(r(a^{+})-r(i_{1})>0\) for all \(a^{+}\in\mathcal{A}^{+}(i_{1})\) by Eq. (170). Note that \(N_{\infty}(j_{1})=\infty\) and \(N_{\infty}(a^{-})<\infty\). According to Lemma 5, we have, for all large enough \(t\geq 1\),

\[\sum_{a^{-}\in\mathcal{A}^{-}(i_{1})}\frac{\pi_{\theta_{t}}(a^{- })}{\pi_{\theta_{t}}(j_{1})}\cdot(r(i_{1})-r(a^{-}))=\sum_{a^{-}\in\mathcal{A} ^{-}(i_{1})}(r(i_{1})-r(a^{-}))\Big{/}\frac{\pi_{\theta_{t}}(j_{1})}{\pi_{ \theta_{t}}(a^{-})}\] (177) \[\qquad<\sum_{a^{-}\in\mathcal{A}^{-}(i_{1})}(r(i_{1})-r(a^{-})) \cdot\frac{1}{\big{|}\mathcal{A}^{-}(i_{1})\big{|}}\cdot\frac{r(j_{1})-r(i_{1 })}{r(i_{1})-r(a^{-})}\cdot\frac{1}{2}\] (178) \[\qquad=\frac{r(j_{1})-r(i_{1})}{2}.\] (179)

Combining Eqs. (175) and (177), we have,

\[\pi_{\theta_{t}}^{\top}r-r(i_{1})>\pi_{\theta_{t}}(j_{1})\cdot\frac{r(j_{1})-r( i_{1})}{2}>0,\] (180)where the last inequality is because \(j_{1}\in\mathcal{A}^{+}(i_{1})\).

**Second part.**\(\pi_{\theta_{t}}^{\top}r<r(i_{2})\).

The arguments are similar to the first part. If \(r(i_{2})=r(a^{*})\), i.e., \(i_{2}\) is the optimal action, then \(\pi_{\theta_{t}}^{\top}r<r(i_{2})\) holds trivially. Otherwise, suppose \(r(i_{2})\neq r(a^{*})\). We have,

\[r(i_{2})-\pi_{\theta_{t}}^{\top}r=\sum_{a^{-}\in\mathcal{A}^{-}( i_{2})}\pi_{\theta_{t}}(a^{-})\cdot(r(i_{2})-r(a^{-}))-\sum_{a^{+}\in\mathcal{A}^{+} (i_{2})}\pi_{\theta_{t}}(a^{+})\cdot(r(a^{+})-r(i_{2})).\] (181)

Consider the non-empty set \(\mathcal{A}_{\infty}\cap\mathcal{A}^{-}(i_{2})\). Pick an action \(j_{2}\in\mathcal{A}_{\infty}\cap\mathcal{A}^{-}(i_{2})\), and ignore all the other actions \(a^{-}\in\mathcal{A}^{-}(i_{2})\) with \(a^{-}\neq j_{2}\) in the above equation. We have,

\[r(i_{2})-\pi_{\theta_{t}}^{\top}r \geq\pi_{\theta_{t}}(j_{2})\cdot(r(i_{2})-r(j_{2}))-\sum_{a^{+} \in\mathcal{A}^{+}(i_{2})}\pi_{\theta_{t}}(a^{+})\cdot(r(a^{+})-r(i_{2}))\] (182) \[=\pi_{\theta_{t}}(j_{2})\cdot\bigg{[}r(i_{2})-r(j_{2})-\sum_{a^{ +}\in\mathcal{A}^{+}(i_{2})}\frac{\pi_{\theta_{t}}(a^{+})}{\pi_{\theta_{t}}(j_ {2})}\cdot(r(a^{+})-r(i_{2}))\bigg{]},\] (183)

where the first inequality is because of \(r(i_{2})-r(a^{-})>0\) for all \(a^{-}\in\mathcal{A}^{-}(i_{2})\) by Eq. (170). Note that \(N_{\infty}(j_{2})=\infty\) and \(N_{\infty}(a^{+})<\infty\). According to Lemma 5, we have, for all large enough \(t\geq 1\),

\[\sum_{a^{+}\in\mathcal{A}^{+}(i_{2})}\frac{\pi_{\theta_{t}}(a^{+ })}{\pi_{\theta_{t}}(j_{2})}\cdot(r(a^{+})-r(i_{2}))=\sum_{a^{+}\in\mathcal{A}^ {+}(i_{2})}\big{(}r(a^{+})-r(i_{2})\big{)}\Big{/}\frac{\pi_{\theta_{t}}(j_{2} )}{\pi_{\theta_{t}}(a^{+})}\] (184) \[\qquad<\sum_{a^{+}\in\mathcal{A}^{+}(i_{2})}\big{(}r(a^{+})-r(i_ {2})\big{)}\cdot\frac{1}{\big{|}\mathcal{A}^{+}(i_{2})\big{|}}\cdot\frac{r(i_ {2})-r(j_{2})}{r(a^{+})-r(i_{2})}\cdot\frac{1}{2}\] (185) \[\qquad=\frac{r(i_{2})-r(j_{2})}{2}.\] (186)

Combining Eqs. (182) and (184), we have,

\[r(i_{2})-\pi_{\theta_{t}}^{\top}r\geq\pi_{\theta_{t}}(j_{2})\cdot \frac{r(i_{2})-r(j_{2})}{2}>0,\] (187)

where the last inequality is because \(j_{2}\in\mathcal{A}^{-}(i_{2})\). 

For the following lemmas, we will use the notation defined in the proofs for Theorems 1 and 2.

**Lemma 7** (Concentration of noise).: _Given an action \(a\in[K]\). We have, with probability at least \(1-\delta\),_

\[\forall t:\quad\left|\sum_{s=1}^{t}W_{s+1}(a)\right|\leq 36\;\eta\;R_{ \max}\;\sqrt{(V_{t}(a)+4/3)\log\left(\frac{V_{t}(a)+1}{\delta}\right)}+12\; \eta\;R_{\max}\;\log(1/\delta)+8\;\eta\;R_{\max}\log 3,\] (188)

_where_

\[V_{t}(a)\coloneqq\frac{5}{18}\cdot\sum_{s=1}^{t}\pi_{\theta_{s} }(a)\cdot(1-\pi_{\theta_{s}}(a)),\] (189)

_and_

\[W_{t}(a)\coloneqq\theta_{t}(a)-\mathbb{E}_{t-1}[\theta_{t}(a)]\] (190)

_is originally defined by Eq. (52)._

Proof.: First, note that,

\[\mathbb{E}_{t}[W_{t+1}(a)]=0,\;\text{for all}\;t\geq 0.\] (191)Using the update Eq. (22), we have,

\[W_{t+1}(a)=\theta_{t+1}(a)-\mathbb{E}_{t}[\theta_{t+1}(a)]\] (192) \[\quad=\theta_{t}(a)+\eta\cdot(I_{t}(a)-\pi_{\theta_{t}}(a))\cdot R_ {t}(a_{t})-\left(\theta_{t}(a)+\eta\cdot\pi_{\theta_{t}}(a)\cdot\left(r(a)-\pi _{\theta_{t}}^{\top}r\right)\right)\] (193) \[\quad=\eta\cdot(I_{t}(a)-\pi_{\theta_{t}}(a))\cdot R_{t}(a_{t})- \eta\cdot\pi_{\theta_{t}}(a)\cdot\left(r(a)-\pi_{\theta_{t}}^{\top}r\right).\] (194)

According to Eq. (1), we have,

\[|W_{t+1}(a)|\leq 3\:\eta\cdot R_{\max}.\] (195)

The conditional variance of noise is,

\[\mathrm{Var}[W_{t+1}(a)|\mathcal{F}_{t}]\coloneqq\mathbb{E}_{t}[(W _{t+1}(a))^{2}]\] (196) \[\qquad\leq 2\:\eta^{2}\cdot\mathbb{E}_{t}[(I_{t}(a)-\pi_{ \theta_{t}}(a))^{2}\cdot R_{t}(a_{t})^{2}]+2\:\eta^{2}\cdot\pi_{\theta_{t}}(a )^{2}\cdot\left(r(a)-\pi_{\theta_{t}}^{\top}r\right)^{2},\] (197)

where the inequality is by \((a+b)^{2}\leq 2a^{2}+2b^{2}\). Next, we have,

\[\mathbb{E}_{t}[\left(I_{t}(a)-\pi_{\theta_{t}}(a)\right)^{2}\cdot R _{t}(a_{t})^{2}] =\pi_{t}(a)\cdot(1-\pi_{t}(a))^{2}\cdot r(a)^{2}+\sum_{a^{\prime }\neq a}\pi_{\theta_{t}}(a^{\prime})\cdot\pi_{t}(a)^{2}\cdot r(a^{\prime})^{2}\] (198) \[\leq R_{\max}^{2}\cdot\left(\pi_{t}(a)\cdot(1-\pi_{t}(a))^{2}+(1 -\pi_{t}(a))\cdot\pi_{t}(a)^{2}\right)\] (199) \[=R_{\max}^{2}\cdot\pi_{\theta_{t}}(a)\cdot(1-\pi_{\theta_{t}}(a)),\] (200)

and,

\[\left|r(a)-\pi_{\theta_{t}}^{\top}r\right| =\left|\sum_{a^{\prime}\neq a}\pi_{\theta_{t}}(a^{\prime})\cdot \left(r(a)-r(a^{\prime})\right)\right|\] (201) \[\leq\sum_{a^{\prime}\neq a}\pi_{\theta_{t}}(a^{\prime})\cdot|r(a )-r(a^{\prime})|\] (202) \[\leq 2\:R_{\max}\cdot\sum_{a^{\prime}\neq a}\pi_{\theta_{t}}(a^{ \prime})\] (203) \[=2\:R_{\max}\cdot\left(1-\pi_{\theta_{t}}(a)\right).\] (204)

Combining Eqs. (196), (198) and (201), we have,

\[\mathrm{Var}[W_{t+1}(a)|\mathcal{F}_{t}] \leq 2\:\eta^{2}\cdot R_{\max}^{2}\cdot\pi_{\theta_{t}}(a)\cdot \left(1-\pi_{\theta_{t}}(a)\right)+8\:\eta^{2}\cdot R_{\max}^{2}\cdot\pi_{ \theta_{t}}(a)^{2}\cdot\left(1-\pi_{\theta_{t}}(a)\right)^{2}\] (205) \[\leq 10\:\eta^{2}\cdot R_{\max}^{2}\cdot\pi_{\theta_{t}}(a)\cdot \left(1-\pi_{\theta_{t}}(a)\right).\] (206)

Let \(X_{t+1}(a)\coloneqq\frac{W_{t+1}(a)}{6\:\eta\cdot R_{\max}}\). Then we have,

\[|X_{t+1}(a)| \leq 1/2,\text{ and }\] (207) \[\mathrm{Var}[X_{t+1}(a)|\mathcal{F}_{t}] \leq\frac{5}{18}\cdot\pi_{\theta_{t}}(a)\cdot(1-\pi_{\theta_{t}} (a)).\] (208)

According to Lemma 4, there exists an event \(\mathcal{E}_{1}\) such that \(\mathrm{Pr}(\mathcal{E}_{1})\geq 1-\delta\), and when \(\mathcal{E}_{1}\) holds,

\[\forall t:\quad\left|\sum_{s=1}^{t}X_{s+1}(a)\right|\leq 6\:\sqrt{(V_{t}(a)+4/3) \log\left(\frac{V_{t}(a)+1}{\delta}\right)}+2\:\log(1/\delta)+\frac{4}{3}\log 3,\] (209)

which implies that,

\[\forall t:\quad\left|\sum_{s=1}^{t}W_{s+1}(a)\right|\leq 36\:\eta\:R_{\max}\: \sqrt{(V_{t}(a)+4/3)\log\left(\frac{V_{t}(a)+1}{\delta}\right)}+12\:\eta\:R_{ \max}\:\log(1/\delta)+8\:\eta\:R_{\max}\log 3,\] (210)

where \(V_{t}(a)\coloneqq\frac{5}{18}\cdot\sum_{s=1}^{t}\pi_{\theta_{s}}(a)\cdot(1-\pi _{\theta_{s}}(a))\). 

**Lemma 8** (Bounded progress).: _For any action \(a\in[K]\) with \(N_{\infty}(a)=\infty\), if there exists \(c>0\) and \(\tau<\infty\), such that, for all \(t\geq\tau\),_

\[\sum_{s=1}^{t}\big{|}P_{s}(a)\big{|}\geq c\cdot V_{t}(a),\] (211)

_where \(V_{t}(a)\) is defined in Eq. (189), and if also_

\[\sum_{t=1}^{\infty}\big{|}P_{t}(a)\big{|}<\infty,\] (212)

_then we have, almost surely,_

\[\sup_{t\geq 1}|\theta_{t}(a)|<\infty.\] (213)

Proof.: According to Eq. (55) and triangle inequality, we have,

\[\big{|}\theta_{t}(a)\big{|}\leq\Big{|}\mathbb{E}[\theta_{1}(a)]\Big{|}+\bigg{|} \sum_{s=1}^{t}W_{s}(a)\bigg{|}+\bigg{|}\sum_{s=1}^{t-1}P_{s}(a)\bigg{|}.\] (214)

According to Lemma 7, there exists an event \(\mathcal{E}_{1}\), such that \(\Pr(\mathcal{E}_{1})\geq 1-\delta\), and when \(\mathcal{E}_{1}\) holds, we have, for all \(t\geq\tau+1\),

\[\bigg{|}\sum_{s=1}^{t}W_{s}(a)\bigg{|}\leq 36\;\eta\;R_{\max}\;\sqrt{(V_{t-1}(a )+4/3)\cdot\log\left(\frac{V_{t-1}(a)+1}{\delta}\right)}+12\;\eta\;R_{\max}\; \log(1/\delta)+8\;\eta\;R_{\max}\log 3.\] (215)

According to Eq. (212), as \(t\to\infty\),

\[\bigg{|}\sum_{s=1}^{t-1}P_{s}(a)\bigg{|}\leq\sum_{s=1}^{t-1}\big{|}P_{s}(a) \big{|}<\infty.\] (216)

According to Eqs. (211), (215) and (216), we have,

\[\bigg{|}\sum_{s=1}^{t}W_{s}(a)\bigg{|}<\infty.\] (217)

Combining Eqs. (214), (216) and (217), we have, as \(t\to\infty\),

\[|\theta_{t}(a)|<\infty.\] (218)

Take any \(\omega\in\mathcal{E}\coloneqq\{N_{\infty}(a)=\infty\}\). Because \(\mathbb{P}(\mathcal{E}\setminus(\mathcal{E}\cap\mathcal{E}_{1}))\leq\mathbb{P} (\Omega\setminus\mathcal{E}_{1})\leq\delta\to 0\) as \(\delta\to 0\), we have that \(\mathbb{P}\)-almost surely for all \(\omega\in\mathcal{E}\) there exists \(\delta>0\) such that \(\omega\in\mathcal{E}\cap\mathcal{E}_{1}\) while Eq. (215) also holds for this \(\delta\). Take such a \(\delta\). We have, almost surely,

\[\sup_{t\geq 1}|\theta_{t}(a)|<\infty.\qed\]

**Lemma 9** (Unbounded positive progress).: _For any action \(a\in[K]\) with \(N_{\infty}(a)=\infty\), if there exists \(c>0\) and \(\tau<\infty\), such that, for all \(t\geq\tau\),_

\[P_{t}(a)>0,\text{ and }\] (219)

\[\sum_{s=\tau}^{t}P_{s}(a)\geq c\cdot V_{t}(a),\] (220)

_where \(V_{t}(a)\) is defined in Eq. (189), and if also,_

\[\sum_{t=\tau}^{\infty}P_{t}(a)=\infty,\] (221)

_then we have, almost surely,_

\[\theta_{t}(a)\to\infty,\text{ as }t\to\infty.\] (222)Proof.: According to Eq. (55),

\[\theta_{t}(a)=\mathbb{E}[\theta_{1}(a)]+\sum_{s=1}^{t}W_{s}(a)+\sum_{s=1}^{t-1}P_{ s}(a).\] (223)

According to Lemma 7, there exists an event \(\mathcal{E}_{1}\), such that \(\Pr(\mathcal{E}_{1})\geq 1-\delta\), and when \(\mathcal{E}_{1}\) holds, we have, for all \(t\geq\tau+1\),

\[\sum_{s=1}^{t}W_{s}(a)\geq-36\;\eta\;R_{\max}\;\underbrace{\sqrt{(V_{t-1}(a)+4 /3)\cdot\log\left(\frac{V_{t-1}(a)+1}{\delta}\right)}}_{\Im}-12\;\eta\;R_{\max} \;\log(1/\delta)-8\;\eta\;R_{\max}\log 3.\] (224)

By Eq. (221), as \(t\to\infty\),

\[\sum_{s=1}^{t-1}P_{s}(a)\to\infty,\] (225)

and the speed of \(\sum_{s=1}^{t-1}P_{s}(a)\to\infty\) is strictly faster than \(\Im\to\infty\), according to Eq. (220). This implies that \(\theta_{t}(a)\to\infty\), as a result of "cumulative progress" dominates "cumulative noise".

Take any \(\omega\in\mathcal{E}\coloneqq\{N_{\infty}(a)=\infty\}\). Because \(\mathbb{P}(\mathcal{E}\setminus(\mathcal{E}\cap\mathcal{E}_{1}))\leq\mathbb{P} (\Omega\setminus\mathcal{E}_{1})\leq\delta\to 0\) as \(\delta\to 0\), we have that \(\mathbb{P}\)-almost surely for all \(\omega\in\mathcal{E}\) there exists \(\delta>0\) such that \(\omega\in\mathcal{E}\cap\mathcal{E}_{1}\) while Eq. (224) also holds for this \(\delta\). Take such a \(\delta\). We have, almost surely,

\[\theta_{t}(a)\to\infty,\;\text{as}\;t\to\infty.\qed\]

**Lemma 10** (Unbounded negative progress).: _For any action \(a\in[K]\) with \(N_{\infty}(a)=\infty\), if there exists \(c>0\) and \(\tau<\infty\), such that, for all \(t\geq\tau\),_

\[P_{t}(a)<0,\;\text{and}\] (226)

\[-\sum_{s=\tau}^{t}P_{s}(a)\geq c\cdot V_{t}(a),\] (227)

_where \(V_{t}(a)\) is defined in Eq. (189), and if also,_

\[\sum_{t=\tau}^{\infty}P_{t}(a)=-\infty,\] (228)

_then we have, almost surely,_

\[\theta_{t}(a)\to-\infty,\;\text{as}\;t\to\infty.\] (229)

Proof.: The proof follows almost the same arguments for Lemma 9. 

**Lemma 11** (Positive progress).: _For any action \(a\in[K]\) with \(N_{\infty}(a)=\infty\), if there exists \(c>0\) and \(\tau<\infty\), such that, for all \(t\geq\tau\),_

\[P_{t}(a)>0,\;\text{and}\] (230)

\[\sum_{s=\tau}^{t}P_{s}(a)\geq c\cdot V_{t}(a),\] (231)

_where \(V_{t}(a)\) is defined in Eq. (189), then we have, almost surely,_

\[\inf_{t\geq 1}\theta_{t}(a)>-\infty.\] (232)

Proof.: **First case:** if \(\sum_{t=1}^{\infty}P_{t}(a)<\infty\), then according to Lemma 8, we have, almost surely,

\[\sup_{t\geq 1}|\theta_{t}(a)|<\infty,\] (233)

which implies Eq. (232).

**Second case:** if \(\sum_{t=1}^{\infty}P_{t}(a)=\infty\), then according to Lemma 9, we have, almost surely,

\[\theta_{t}(a)\to\infty,\;\text{as}\;t\to\infty,\] (234)

which also implies Eq. (232).

**Lemma 12** (Negative progress).: _For any action \(a\in[K]\) with \(N_{\infty}(a)=\infty\), if there exists \(c>0\) and \(\tau<\infty\), such that, for all \(t\geq\tau\),_

\[P_{t}(a) <0,\text{ and }\] (235) \[-\sum_{s=\tau}^{t}P_{s}(a) \geq c\cdot V_{t}(a),\] (236)

_where \(V_{t}(a)\) is defined in Eq. (189), then we have, almost surely,_

\[\sup_{t\geq 1}\theta_{t}(a)<\infty.\] (237)

Proof.: **First case:** if \(-\sum_{t=1}^{\infty}P_{t}(a)<\infty\), then according to Lemma 8, we have, almost surely,

\[\sup_{t\geq 1}|\theta_{t}(a)|<\infty,\] (238)

which implies Eq. (237).

**Second case:** if \(-\sum_{t=1}^{\infty}P_{t}(a)=\infty\), then according to Lemma 10, we have, almost surely,

\[\theta_{t}(a)\rightarrow-\infty,\text{ as }t\rightarrow\infty,\] (239)

which also implies Eq. (237).

Rate of Convergence

**Theorem 3**.: For a large enough \(\tau>0\), for all \(T>\tau\), the average sub-optimality decreases at an \(O\left(\frac{\ln(T)}{T}\right)\) rate. Formally, if \(a^{*}\) is the optimal arm, then, for a constant \(c\)

\[\frac{\sum_{s=\tau}^{T}r(a^{*})-\langle\pi_{s},r\rangle}{T}\leq\frac{c\,\ln(T)} {T-\tau}\]

Proof.: The progress for the optimal action is,

\[P_{t}(a^{*}) =\eta\cdot\pi_{\theta_{t}}(a^{*})\cdot(r(a^{*})-\pi_{\theta_{t}} ^{\top}r)\] (240) \[\geq\eta\cdot\Delta\cdot\pi_{\theta_{t}}(a^{*})\cdot\big{(}1-\pi _{\theta_{t}}(a^{*})\big{)}.\qquad\big{(}\Delta\coloneqq r(a^{*})-\max_{a\neq a ^{*}}r(a)\big{)}\] (241) \[\geq 0.\] (242)

Since \(\lim_{t\to\infty}\pi_{\theta_{t}}(a^{*})=1\), we have for all large enough \(t\geq 1\),

\[\pi_{\theta_{t}}(a^{*})\geq 1/2.\] (243)

Since \(N_{\infty}(a^{*})=\infty\) (Theorem 2 ) and \(|\mathcal{A}_{\infty}|\geq 2\) (Lemma 2), we have,

\[\sum_{t=1}^{\infty}\left(1-\pi_{t}(a^{*})\right)\geq\sum_{t=1}^{\infty}\pi_{t }(i_{1})=\infty,\] (244)

where \(N_{\infty}(i_{1})=\infty\) and \(r(i_{1})<r(a^{*})\). Therefore, we have,

\[\sum_{t=1}^{\infty}P_{t}(a^{*})=\infty.\] (245)

The variance of noise is,

\[V_{t}(a^{*})\coloneqq\frac{5}{18}\cdot\sum_{s=1}^{t-1}\pi_{\theta_{s}}(a^{*}) \cdot(1-\pi_{\theta_{s}}(a^{*})),\] (246)

which will be dominated by sum of \(P_{t}(a^{*})\) since \(V_{t}(a^{*})\) appears under square root (Lemma 7). Therefore, for all large enough \(t\geq\tau\),

\[\theta_{t}(a^{*})\geq C\cdot\sum_{s=\tau}^{t}\left(1-\pi_{s}(a^{*})\right).\] (247)

On the other hand, we argued recursively (in the proofs for Theorem 2) that for all sub-optimal action \(a\in[K]\) with \(r(a)<r(a^{*})\),

\[\sup_{t\geq 1}\theta_{t}(a)<\infty.\] (248)

Therefore, we have, for all large enough \(t\geq\tau\),

\[\theta_{t}(a^{*})-\theta_{t}(a)\geq C\cdot\sum_{s=\tau}^{t}\left(1-\pi_{s}(a^ {*})\right),\] (249)

which implies that,

\[\sum_{a\neq a^{*}}\exp\{\theta_{t}(a)-\theta_{t}(a^{*})\}\leq(K-1)\cdot\exp \bigg{\{}-C\cdot\sum_{s=\tau}^{t}\left(1-\pi_{s}(a^{*})\right)\bigg{\}}.\] (250)

Therefore, we have,

\[1-\pi_{t}(a^{*})\leq\frac{1-\pi_{t}(a^{*})}{\pi_{t}(a^{*})}=\sum_{a\neq a^{*} }\frac{\pi_{t}(a)}{\pi_{t}(a^{*})}\leq(K-1)\cdot\exp\bigg{\{}-C\cdot\sum_{s= \tau}^{t-1}\left(1-\pi_{s}(a^{*})\right)\bigg{\}}.\] (251)

[MISSING_PAGE_FAIL:31]

and note that it is an increasing function, Moreover, \(\dot{y}(t)(=\frac{d}{dt}y(t))=e^{-cy(t)}\) for any \(t\geq 0\). Hence,

\[y(t)=y_{0}+\int_{0}^{t}e^{-cy(s)}\,ds\,,\qquad t\geq 0\]

Define the function:

\[g(x):=x+B\,e^{-cx}\,,\]

and note that \(g\) is increasing when \(y\geq\frac{1}{c}\ln(B\,c)\). Moreover, \(y(0)=y_{0}\) and \(y_{n+1}=g(y_{n})\).

Since \(y(0)=y_{0}\geq\frac{1}{c}\ln(B\,c))\) and both \(\{y_{n}\}\) and \(y(t)\) are increasing, \(y_{n}\geq\frac{1}{c}\ln(B\,c)\) for all \(n\) and, \(y(t)\geq\frac{1}{c}\ln(B\,c)\) for all \(t\geq 0\).

We first prove that

\[y(n)\leq y_{n}\,,\qquad n=0,1,\ldots\,.\] (254)

We prove Eq. (254) by induction. The claim holds for \(n=0\) by construction. Now assume that \(y(n)\leq y_{n}\) holds for some \(n\geq 0\). Let us show that that \(y(n+1)\leq y_{n+1}\) also holds. For this note that

\[y(n+1) =y(n)+\int_{n}^{n+1}e^{-cy(s)}ds\] \[\leq y(n)+\int_{n}^{n+1}e^{-cy(n)}ds\] ( \[t\mapsto y(t)\] is increasing) \[=y(n)+e^{-cy(n)}\] \[\leq y(n)+B\,e^{-cy(n)}\] (since \[B\geq 1\] ) \[=g(y(n))\] (definition of \[g\] ) \[\leq g(y_{n})\] (induction hypothesis, \[g\] is increasing for \[y\geq\frac{1}{c}\ln(B\,c)\] and \[y_{n},y(n)\geq\frac{1}{c}\,\ln(B\,c)\] ) \[=y_{n+1}\,.\] (By definition of \[y_{n+1}\] )

By the principle of mathematical induction, \(y(n)\leq y_{n}\) for all \(n\geq 0\).

Define \(\Delta_{n}:=y_{n}-y(n)\). From our previous inequality, we know that \(\Delta_{n}\geq 0\). We now show that \(\{\Delta_{n}\}_{n}\) is bounded, from which the desired statement follows immediately. To show that \(\{\Delta_{n}\}_{n}\) is bounded we will show that \(\Delta_{n+1}-\Delta_{n}\) is summable. To show this, we start by obtaining an expression for \(\Delta_{n+1}-\Delta_{n}\). Let \(A=e^{cy_{0}}\). Let \(n\geq 0\). Direct calculation gives

\[\Delta_{n+1}-\Delta_{n}=e^{-cy_{n}}-\frac{1}{c}\ln\left(1+\frac{c}{cn+A} \right)\,.\]

Using that for all \(x>0\), \(\ln(1+x)\geq\frac{x}{1+\frac{x}{2}}\), we get

\[\Delta_{n+1}-\Delta_{n} \leq e^{-cy_{n}}-\frac{1}{cn+A+\frac{c}{2}}\] \[\leq e^{-cy(n)}-\frac{1}{cn+A+\frac{c}{2}}\] (from Eq. ( 254 ) \[\leq\frac{1}{cn+A}-\frac{1}{cn+A+\frac{c}{2}}\] (definition of \[y(n)\] ) \[=\frac{1}{2c}\,\frac{1}{(n+\nicefrac{{A}}{{c}})\,(n+\nicefrac{{A} }{{c}}+\frac{1}{2})}\] \[\leq\frac{1}{2c}\frac{1}{(n+1)^{2}}\,.\] (Since \[y_{0}\geq\frac{1}{c}\ln(B\,c)\] ), \[A/c=B\geq 1\] )

For a fixed \(m>0\), summing up the above inequality from \(n=0\) to \(m-1\),

\[\Delta_{m}-\Delta_{0} \leq\frac{1}{2c}\sum_{n=0}^{m-1}\frac{1}{(n+1)^{2}}\leq\frac{\pi^ {2}}{12c}\] (Since \[\sum_{i=1}^{\infty}\frac{1}{i^{2}}=\frac{\pi^{2}}{6}\] ) \[\implies\Delta_{m} \leq\frac{\pi^{2}}{12c}.\] (Since \[\Delta_{0}=0\] )where \(\pi=3.14159\dots\). Hence, it follows that for any \(n\geq 0\),

\[y_{n}=y(n)+\Delta_{n}\leq y(n)+\frac{\pi^{2}}{12c}=\frac{1}{c}\ln(cn+e^{cy_{0}})+ \frac{\pi^{2}}{12c}.\qed\]

**Lemma 15**.: _Let \(\{x_{n}\}\) be a nonnegative valued sequence such that \(x_{n+1}\leq x_{n}+B\,e^{-cx_{n}}\) for all \(n\geq 0\) with \(B\geq 1\), \(c>0\). Then, for all \(n\geq 0\),_

\[x_{n}\leq\frac{1}{c}\ln(cn+e^{cM})+\frac{\pi^{2}}{12c}\,,\]

_where \(M=\max\{B,\frac{1}{c}\ln(B\,c)),x_{0}\}\)._

Proof.: Let \(\{y_{n}\}\) be the solution to the difference equation \(y_{n+1}=y_{n}+B\,e^{-cy_{n}}\) where \(y_{0}=\max\{B,\frac{1}{c}\ln(B\,c)),x_{0}\}\). Since \(y_{0}\geq x_{0}\) and \(y_{0}\geq\max(B,\frac{1}{c}\ln(B\,c))\), we can use Lemma 13 to conclude that for all \(n\geq 0\),

\[x_{n}\leq y_{n}\,.\]

Furthermore, using Lemma 14 we can conclude that,

\[y_{n}\leq\frac{1}{c}\ln(cn+e^{cy_{0}})+\frac{\pi^{2}}{12c}.\]

Combining the above inequalities completes the proof.

[MISSING_PAGE_EMPTY:34]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In our view, the abstract and introduction summarize the main results, as well as the technical novelty in obtaining these results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We describe the setting and assumptions clearly. The main results are backed by proof sketches in the main text and detailed proofs in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our experiments consist of simple simulations, for which all the details are included in Section 4. The algorithm studied here is remarkably simple and well-known, so the experiments should be straightforward to reproduce with these details, without any code. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: Given that we are doing simulations with a very classical algorithm, for which we give all simulation details and hyperparameters, we do not believe that code is necessary to reproduce the results. Our data is simulated, and hence easily reproduced, given the distribution which we explicitly describe. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please see the details in Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: There are no comparisons for which significance needs to be demonstrated, but we provide evidence for the convergence by repeating 10 independent runs, which are presented in the plots. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: Our experiments are simply run on a single laptop or Python notebook, without requiring any particular infrastructure. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research does not involve human subjects or introduces any new datasets. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Section 5. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No data or models are released. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: No existing assets are used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No human subjects or crowdsourcing is involved in this work. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No human subjects or crowdsourcing is involved in this work. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.