# Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension

Moritz Haas\({}^{1}\) David Holzmuller\({}^{2}\) Ulrike von Luxburg\({}^{1}\) Ingo Steinwart\({}^{2}\)

\({}^{1}\)University of Tubingen and Tubingen AI Center, Germany

\({}^{2}\)Institute for Stochastics and Applications, University of Stuttgart, Germany

{mo.haas,ulrike.luxburg}@uni-tuebingen.de

{david.holzmueller,ingo.steinwart}@mathematik.uni-stuttgart.de

Equal contribution.

###### Abstract

The success of over-parameterized neural networks trained to near-zero training error has caused great interest in the phenomenon of benign overfitting, where estimators are statistically consistent even though they interpolate noisy training data. While benign overfitting in fixed dimension has been established for some learning methods, current literature suggests that for regression with typical kernel methods and wide neural networks, benign overfitting requires a high-dimensional setting where the dimension grows with the sample size. In this paper, we show that the smoothness of the estimators, and not the dimension, is the key: benign overfitting is possible if and only if the estimator's derivatives are large enough. We generalize existing inconsistency results to non-interpolating models and more kernels to show that benign overfitting with moderate derivatives is impossible in fixed dimension. Conversely, we show that rate-optimal benign overfitting is possible for regression with a sequence of spiky-smooth kernels with large derivatives. Using neural tangent kernels, we translate our results to wide neural networks. We prove that while infinite-width networks do not overfit benignly with the ReLU activation, this can be fixed by adding small high-frequency fluctuations to the activation function. Our experiments verify that such neural networks, while overfitting, can indeed generalize well even on low-dimensional data sets.

## 1 Introduction

While neural networks have shown great practical success, our theoretical understanding of their generalization properties is still limited. A promising line of work considers the phenomenon of benign overfitting, where researchers try to understand when and how models that interpolate noisy training data can generalize (Zhang et al., 2021; Belkin et al., 2018, 2019). In the high-dimensional regime, where the dimension grows with the number of sample points, consistency of minimum-norm interpolants has been established for linear models and kernel regression (Hastie et al., 2022; Bartlett et al., 2020; Liang and Rakhlin, 2020; Bartlett et al., 2021). In fixed dimension, minimum-norm interpolation with standard kernels is inconsistent (Rakhlin and Zhai, 2019; Buchholz, 2022).

In this paper, we shed a differentiated light on benign overfitting with kernels and neural networks. We argue that the dimension-dependent perspective does not capture the full picture of benign overfitting. In particular, we show that harmless interpolation with kernel methods and neural networks is possible, even in small fixed dimension, with adequately designed kernels and activation functions. The key is to properly design estimators of the form'signal+spike'. While minimum-norm criteria have widely been considered a useful inductive bias, we demonstrate that designing unusual norms can resolve the shortcomings of standard norms. For wide neural networks, harmless interpolation can berealized by adding tiny fluctuations to the activation function. Such networks do not require explicit regularization and can simply be trained to overfit (Figure 1).

On a technical level, we additionally prove that overfitting in kernel regression can only be consistent if the estimators have large derivatives. Using neural tangent kernels or neural network Gaussian process kernels, we can translate our results from kernel regression to the world of neural networks (Neal, 1996; Jacot et al., 2018). In particular, our results enable the design of activation functions that induce benign overfitting in fixed dimension: the spikes in kernels can be translated into infinitesimal fluctuations that can be added to an activation function to achieve harmless interpolation with neural networks. Such small high frequency oscillations can fit noisy observations without affecting the smooth component too much. Training finite neural networks with gradient descent shows that spiky-smooth activation functions can indeed achieve good generalization even when interpolating small, low-dimensional data sets (Figure 1 b,c).

Thanks to new technical contributions, our inconsistency results significantly extend existing ones. We use a novel noise concentration argument (Lemma D.6) to generalize existing inconsistency results on minimum-norm interpolants to the much more realistic regime of overfitting estimators with comparable Sobolev norm scaling, which includes training via gradient flow and gradient descent with "late stopping" as well as low levels of ridge regularization. Moreover, a novel connection to eigenvalue concentration results for kernel matrices (Proposition 5) allows us to relax the smoothness assumption and to treat heteroscedastic noise in Theorem 6. Lastly, our Lemma E.1 translates inconsistency results from bounded open subsets of \(\mathbb{R}^{d}\) to the sphere \(\mathbb{S}^{d}\), which leads to results for the neural tangent kernel and neural network Gaussian processes.

## 2 Setup and prerequisites

**General approach.** We consider a general regression problem on \(\mathbb{R}^{d}\) with an arbitrary, fixed dimension \(d\) and analyze kernel-based approaches to solve this problem: kernel ridge regression, kernel gradient flow and gradient descent, minimum-norm interpolation, and more generally, overfitting norm-bounded estimators. We then translate our results to neural networks via the neural network Gaussian process and the neural tangent kernel. Let us now introduce the formal framework.

**Notation.** We denote scalars by lowercase letters \(x\), vectors by bold lowercase letters \(\bm{x}\) and matrices by bold uppercase letters \(\bm{X}\). We denote the eigenvalues of \(\bm{A}\) as \(\lambda_{1}(\bm{A})\geq\ldots\geq\lambda_{n}(\bm{A})\) and the Moore-Penrose pseudo-inverse by \(\bm{A}^{+}\). We say that a probability distribution \(P\) has lower and upper bounded density if its density \(p\) satisfies \(0<c<p(\bm{x})<C\) for suitable constants \(c,C\) and all \(\bm{x}\) on a given domain.

**Regression setup.** We consider a data set \(D=((\bm{x}_{1},y_{1}),\ldots,(\bm{x}_{n},y_{n}))\in(\mathbb{R}^{d}\times\mathbb{ R})^{n}\) with i.i.d. samples \((\bm{x}_{i},y_{i})\sim P\), written as \(D\sim P^{n}\), where \(P\) is a probability distribution on \(\mathbb{R}^{d}\times\mathbb{R}\). We define \(\bm{X}\coloneqq(\bm{x}_{1},\ldots,\bm{x}_{n})\) and \(\bm{y}\coloneqq(y_{1},\ldots,y_{n})^{\top}\in\mathbb{R}^{n}\). Random variables \((\bm{x},y)\sim P\) denote

Figure 1: **Spiky-smooth overfitting in 2 dimensions.****a.** We plot the predicted function for ridgeless kernel regression with the Laplace kernel (blue) versus our spiky-smooth kernel (4) with Laplace components (orange) on \(\mathbb{S}^{1}\). The dashed black line shows the true regression function, black ’x’ denote noisy training points. Further details can be found in Section 6.2. **b.** The predicted function of a trained 2-layer neural network with ReLU activation (blue) versus ReLU plus shifted high-frequency sin-function (8) (orange). Using the weights learned with the spiky-smooth activation function in a ReLU network (green) disentangles the spike component from the signal component. **c.** Training error (solid lines) and test error (dashed lines) over the course of training for b. evaluated on \(10^{4}\) test points. The dotted black line shows the optimal test error. The spiky-smooth activation function does not require regularization and can simply be trained to overfit.

test points independent of \(D\), and \(P_{X}\) denotes the probability distribution of \(\bm{x}\). The (least squares) _empirical risk_\(R_{D}\) and _population risk_\(R_{P}\) of a function \(f:\mathbb{R}^{d}\to\mathbb{R}\) are defined as

\[R_{D}(f)\coloneqq\frac{1}{n}\sum_{i=1}^{n}(y_{i}-f(x_{i}))^{2},\qquad R_{P}(f) \coloneqq\mathbb{E}_{\bm{x},y}[(y-f(\bm{x}))^{2}]\.\]

We assume \(\operatorname{Var}(y|\bm{x})<\infty\) for all \(\bm{x}\). Then, \(R_{P}\) is minimized by the target function \(f_{P}^{*}(\bm{x})=\mathbb{E}[y|\bm{x}]\), and the _excess risk_ of a function \(f\) is given by

\[R_{P}(f)-R_{P}(f_{P}^{*})=\mathbb{E}_{\bm{x}}(f_{P}^{*}(\bm{x})-f(\bm{x}))^{2}\.\]

We call a data-dependent estimator \(f_{D}\)_consistent for \(P\)_ if its excess risk converges to \(0\) in probability, that is, for all \(\varepsilon>0\), \(\lim_{n\to\infty}P^{n}\left(D\in(\mathbb{R}^{d}\times\mathbb{R})^{n}\mid R_{P }(f_{D})-R_{P}(f_{P}^{*})\geq\varepsilon\right)=0\). We call \(f_{D}\)_consistent in expectation for \(P\)_ if \(\lim_{n\to\infty}\mathbb{E}_{D}R_{P}(f_{D})-R_{P}(f_{P}^{*})=0\). We call \(f_{D}\)_universally consistent_ if is it consistent for all Borel probability measures \(P\) on \(\mathbb{R}^{d}\times\mathbb{R}\).

**Solutions by kernel regression.** Recall that a kernel \(k\) induces a reproducing kernel Hilbert space \(\mathcal{H}_{k}\), abbreviated RKHS (more details in Appendix B). For \(f\in\mathcal{H}_{k}\), we consider the objective

\[\mathcal{L}_{\rho}(f)\coloneqq\frac{1}{n}\sum_{i=1}^{n}(y_{i}-f(\bm{x}_{i}))^ {2}+\rho\|f\|_{\mathcal{H}_{k}}^{2}\]

with regularization parameter \(\rho\geq 0\). Denote by \(f_{t,\rho}\) the solution to this problem that is obtained by optimizing on \(\mathcal{L}_{\rho}\) in \(\mathcal{H}_{k}\) with gradient flow until time \(t\in[0,\infty]\), using fixed a regularization constant \(\rho>0\), and initializing at \(f=0\in\mathcal{H}_{k}\). We show in Appendix C.1 that it is given as

\[f_{t,\rho}(\bm{x})\coloneqq k(\bm{x},\bm{X})\left(\bm{I}_{n}-e^{-\frac{2}{n}t (k(\bm{X},\bm{X})+\rho n\bm{I}_{n})}\right)\left(k(\bm{X},\bm{X})+\rho n\bm{I }_{n}\right)^{-1}\bm{y}\,\] (1)

where \(k(\bm{x},\bm{X})\) denotes the row vector \(\left(k(\bm{x},\bm{x}_{i})\right)_{i\in[n]}\) and \(k(\bm{X},\bm{X})=\left(k(\bm{x}_{i},\bm{x}_{j})\right)_{i,j\in[n]}\) the kernel matrix. \(f_{t,\rho}\) elegantly subsumes several popular kernel regression estimators as special cases: (i) classical kernel ridge regression for \(t\to\infty\), (ii) gradient flow on the unregularized objective for \(\rho\searrow 0\), and (iii) kernel "ridgeless" regression \(f_{\infty,0}(\bm{x})=k(\bm{x},\bm{X})k(\bm{X},\bm{X})^{+}\bm{y}\) in the joint limit of \(\rho\to 0\) and \(t\to\infty\). If \(k(\bm{X},\bm{X})\) is invertible, \(f_{\infty,0}\) is the interpolating function \(f\in\mathcal{H}_{k}\) with the smallest \(\mathcal{H}_{k}\)-norm.

**From kernels to neural networks: the neural tangent kernel (NTK) and the neural network Gaussian process (NNGP).** Denote the output of a NN with parameters \(\bm{\theta}\) on input \(\bm{x}\) by \(f_{\bm{\theta}}(\bm{x})\). It is known that for suitable random initializations \(\bm{\theta}_{0}\), in the infinite-width limit the random initial function \(f_{\bm{\theta}_{0}}\) converges in distribution to a Gaussian Process with the so-called Neural Network Gaussian Process (NNGP) kernel (Neal, 1996; Lee et al., 2018; Matthews et al., 2018). In Bayesian inference, the posterior mean function is then of the form \(f_{\infty,\rho}\). With minor modifications (Arora et al., 2019; Zhang et al., 2020), training infinitely wide NNs with gradient flow corresponds to learning the function \(f_{t,0}\) with the neural tangent kernel (NTK) (Jacot et al., 2018; Lee et al., 2019). If only the last layer is trained, the NNGP kernel should be used instead (Daniely et al., 2016). For ReLU activation functions, the RKHS of the infinite-width NNGP and NTK on the sphere \(\mathbb{S}^{d}\) is typically a Sobolev space (Bietti and Bach, 2021; Chen and Xu, 2021), see Appendix B.4. Using other parametrizations induces feature learning infinite-width limits for neural networks (Yang and Hu, 2021); an analysis of such neural network algorithms is left for future work.

## 3 Related work

We here provide a short summary of related work. A more detailed account is provided in Appendix A.

**Kernel regression.** With appropriate regularization, kernel ridge regularization with typical universal kernels like the Gauss, Matern, and Laplace kernels is universally consistent (Steinwart and Christmann, 2008, Chapter 9). Optimal rates in Sobolev RKHS can also be achieved using cross-validation of the regularization \(\rho\)(Steinwart et al., 2009) or early stopping rules (Yao et al., 2007; Raskutti et al., 2014; Wei et al., 2017). The above kernels as well as NTKs and NNGPs of standard fully-connected neural networks are rotationally invariant. In the high-dimensional regime, the class of functions that is learnable with rotation-invariant kernels is quite limited (Donhauser et al., 2021; Ghorbani et al., 2021; Liang et al., 2020).

**Inconsistency results.** Besides Rakhlin and Zhai (2019) and Buchholz (2022), Beaglehole et al. (2023) derive inconsistency results for ridgeless kernel regression given assumptions on the spectral tail in the Fourier basis, and contemporaneously propose a special case of our spiky-smooth kernel sequence to mimic kernel ridge regression without providing any quantitative statements. Li et al. (2023) show that polynomial convergence is impossible for common kernels including ReLU NTKs. Mallinar et al. (2022) conjecture inconsistency for interpolation with ReLU NTKs based on their semi-rigorous result, which essentially assumes that the eigenfunctions can be replaced by structureless Gaussian random variables. Lai et al. (2023) show an inconsistency-type result for overfitting two-layer ReLU NNs with \(d=1\), but for fixed inputs \(\bm{X}\). They also note that an earlier inconsistency result by Hu et al. (2021) relies on an unproven result. Mucke and Steinwart (2019) show that global minima of NNs can overfit both benignly and harmfully, but their result does not apply to gradient descent training. Overfitting with typical linear models around the interpolation peak is inconsistent (Ghosh and Belkin, 2022, Holzmuller, 2021).

**Classification.** For binary classification, benign overfitting is a more generic phenomenon than for regression (Muthukumar et al., 2021, Shamir, 2022), and consistency has been shown under linear separability assumptions (Montanari et al., 2019, Chatterji and Long, 2021, Frei et al., 2022), through complexity bounds for reference classes (Cao and Gu, 2019, Chen et al., 2021) or as long as the total variation distance of the class conditionals is sufficiently large and \(f^{*}(\bm{x})=\mathbb{E}[y|\bm{x}]\) lies in the RKHS with bounded norm (Liang and Recht, 2023). Chapter 8 of Steinwart and Christmann (2008) discusses how the overlap of the two classes may influence learning rates under positive regularization.

## 4 Inconsistency of overfitting with common kernel estimators

We consider a regression problem on \(\mathbb{R}^{d}\) in arbitrary, fixed dimension \(d\) that is solved by kernel regression. In this section, we derive several new results, stating that overfitting estimators with moderate Sobolev norm are inconsistent, in a variety of settings. In the next section, we establish the other direction: overfitting estimators can be consistent when we adapt the norm that is minimized.

### Beyond minimum-norm interpolants: general overfitting estimators with bounded norm

Existing generalization bounds often consider the perfect minimum norm interpolant. This is a rather theoretical construction; estimators obtained by training with gradient descent algorithms merely overfit and, in the best case, approximate interpolants with small norm. In this section, we extend existing bounds to arbitrary overfitting estimators whose norm does not grow faster than the minimum norm that would be required to interpolate the training data. Before we can state the theorem, we need to establish some technical assumptions.

Assumptions on the data generating process.The following assumptions (as in Buchholz (2022)) allow for quite general domains and distributions. They are standard in nonparametric statistics.

* Let \(P_{X}\) be a distribution on a bounded open Lipschitz domain \(\Omega\subseteq\mathbb{R}^{d}\) with lower and upper bounded Lebesgue density. Consider data sets \(D=\{(\bm{x}_{1},y_{1}),\ldots,(\bm{x}_{n},y_{n})\}\), where \(\bm{x}_{i}\sim P_{X}\) i.i.d. and \(y_{i}=f^{*}(\bm{x}_{i})+\varepsilon_{i}\), where \(\varepsilon_{i}\) is i.i.d. Gaussian noise with positive variance \(\sigma^{2}>0\) and \(f^{*}\in C_{c}^{\infty}(\Omega)\backslash\{0\}\) denotes a smooth function with compact support.

Assumptions on the kernel.Our assumption on the kernel is that its RKHS is equivalent to a Sobolev space. For integers \(s\in\mathbb{N}\), the norm of a Sobolev space \(H^{s}(\Omega)\) can be defined as

\[\|f\|_{H^{s}(\Omega)}^{2}\coloneqq\sum_{0\leq|\alpha|\leq s}\|D^{\alpha}f\|_{ L_{2}(\Omega)}^{2},\]

where \(D^{\alpha}\) denotes partial derivatives in multi-index notation for \(\alpha\). It measures the magnitude of derivatives up to some order \(s\). For general \(s>0\), \(H^{s}(\Omega)\) is (equivalent to) an RKHS if and only if \(s>d/2\). For example, Laplace and Matern kernels (Kanagawa et al., 2018, Example 2.6) have Sobolev RKHSs. The RKHS of the Gaussian kernel \(\mathcal{H}^{\mathrm{Gauss}}\) is contained in every Sobolev space, \(\mathcal{H}^{\mathrm{Gauss}}\subsetneq H^{s}\) for all \(s\geq 0\)(Steinwart and Christmann, 2008, Corollary 4.36). Due to its smoothness, the Gaussian kernel is potentially even more prone to harmful overfitting than Sobolev kernels (Mallinar et al., 2022). We make the following assumption on the kernel:* Let \(k\) be a positive definite real function whose RKHS \(\mathcal{H}_{k}\) is equivalent to the Sobolev space \(H^{s}\) for some \(s\in(\frac{d}{2},\frac{3d}{4}]\).

Now we are ready to state the main result of this section:

**Theorem 1** (**Overfitting estimators with small norms are inconsistent**).: _Let assumptions (D1) and (K) hold. Let \(c_{\mathrm{fit}}\in(0,1]\) and \(C_{\mathrm{norm}}>0\). Then, there exist \(c>0\) and \(n_{0}\in\mathbb{N}\) such that the following holds for all \(n\geq n_{0}\) with probability \(1-O(1/n)\) over the draw of the data set \(D\) with \(n\) samples: Every function \(f\in\mathcal{H}_{k}\) that satisfies the following two conditions_

* \(\frac{1}{n}\sum_{i=1}^{n}(f(x_{i})-y_{i})^{2}\leq(1-c_{\mathrm{fit}})\cdot \sigma^{2}\) _(training error of_ \(f\) _is below Bayes risk_)__
* \(\|f\|_{\mathcal{H}_{k}}\leq C_{\mathrm{norm}}\|f_{\infty,0}\|_{\mathcal{H}_{ k}}\) _(norm comparable to minimum-norm interpolant (_1_)),_

_has an excess risk that satisfies_

\[R_{P}(f)-R_{P}(f^{*})\geq c\sigma^{2}>0\;.\] (2)

In words: In fixed dimension \(d\), every differentiable function \(f\) that overfits the training data and is not much "spikier" than the minimum RKHS-norm interpolant is inconsistent!

**Proof idea.** Our proof follows a similar approach as Rakhlin and Zhai (2019), Buchholz (2022), and also holds for kernels with adaptive bandwidths. For small bandwidths, \(\|f_{\infty,0}\|_{L_{2}(P_{X})}\ll\|f^{*}\|_{L_{2}(P_{X})}\) because \(f_{\infty,0}\) decays to \(0\) between the training points, which shows that purely'spiky' estimators are inconsistent. In this case, the lower bound is independent of \(\sigma^{2}\). For all other bandwidths, interpolating \(\Theta(n)\) many noisy labels \(y_{i}\) incurs \(\Theta(1)\) error in an area of volume \(\Omega(1/n)\) around \(\Theta(n)\) data points with high probability, which accumulates to a total error \(\Omega(1)\). Our observation is that the same logic holds when overfitting by a constant fraction. Formally, we show that \(f^{*}\) and \(f\) must then be separated by a constant on a constant fraction of training points, with high probability, by using the fact that a constant fraction of the total noise cannot concentrate on less than \(\Theta(n)\) noise variables, with high probability (Lemma D.6). The full proof can be found in Appendix D. 

Assumption (O) is necessary in Theorem 1, because optimally regularized kernel ridge regression fulfills all other assumptions of Theorem 1 while achieving consistency with minimax optimal convergence rates (see Section 3). The necessity of Assumption (N) is demonstrated by Section 5.

The following proposition establishes that Theorem 1 covers the entire overfitting regime of the popular (regularized) gradient flow estimators \(f_{t,\rho}\) for all times \(t\in[0,\infty]\) and any regularization \(\rho\geq 0\). The proof in Appendix C.2 also covers gradient descent.

**Proposition 2** (**Popular estimators fulfill the norm bound (N)**).: _For arbitrary \(t\in[0,\infty]\) and \(\rho\in[0,\infty)\), \(f_{t,\rho}\) as defined in (1) fulfills Assumption (N) with \(C_{\mathrm{norm}}=1\)._

**Remark 3** (**Dimension dependency**).: Some works argue that for specific sequences of kernels \((k_{d})_{d\in\mathbb{N}}\), the constant \(c\) in Theorem 1 decreases with increasing dimension \(d\)(Liang et al., 2020, Liang and Rakhlin, 2020, Mallinar et al., 2022). In Theorem 1, if the equivalence constants in Assumption (K) are uniformly bounded in \(d\), the behavior in \(d\) might still depend on the definition of the Sobolev norms. Overall, similar to Rakhlin and Zhai (2019) and Buchholz (2022), our proof techniques do not allow to easily obtain a dependence on \(d\). 

### Inconsistency of overfitting with neural kernels

We would now like to apply the above results to neural kernels, which would allow us to translate our inconsistency results from the kernel domain to neural networks. However, to achieve this, we need to take one more technical hurdle: the equivalence results for NTKs and NNGPs only hold for probability distributions on the sphere \(\mathbb{S}^{d}\) (detailed summary in Appendix B.4). Lemma E.1 provides the missing technical link: It establishes a smooth correspondence between the respective kernels, Sobolev spaces, and probability distributions. The inconsistency of overfitting with (deep) ReLU NTKs and NNGP kernels then immediately follows from adapting Theorem 1 via Lemma E.1.

**Theorem 4** (**Overfitting with neural network kernels in fixed dimension is inconsistent**).: _Let \(c\in(0,1)\), and let \(P\) be a probability distribution with lower and upper bounded Lebesgue density on an arbitrary spherical cap \(T\coloneqq\{\bm{x}\in\mathbb{S}^{d}\mid x_{d+1}<v\}\subseteq\mathbb{S}^{d}\), \(v\in(-1,1)\). Let \(k\) either be_

* _the fully-connected ReLU NTK with_ \(0\)_-initialized biases of any fixed depth_ \(L\geq 2\)_, and_ \(d\geq 2\)_, or_
* _the fully-connected ReLU NNGP kernel without biases of any fixed depth_ \(L\geq 3\)_, and_ \(d\geq 6\)_Then, if \(f_{t,\rho}\) fulfills Assumption \((O)\) with probability at least \(c\) over the draw of the data set \(D\), \(f_{t,\rho}\) is inconsistent for \(P\)._

Theorem 4 also holds for more general estimators as in Theorem 1, cf. the proof in Appendix E.

Mallinar et al. (2022) already observed empirically that overfitting common network architectures yields suboptimal generalization performance on large data sets in fixed dimension. Theorem 4 now provides a rigorous proof for this phenomenon since sufficiently wide trained neural networks and the corresponding NTKs have a similar generalization behavior (e.g. (Arora et al., 2019, Theorem 3.2)).

### Relaxing smoothness and noise assumptions via spectral concentration bounds

In this section, we consider a different approach to derive lower bounds for the generalization error of overfitting kernel regression: through concentration results for the eigenvalues of kernel matrices. On a high level, we obtain similar results as in the last section. The novelty of this section is on the technical side, and we suggest that non-technical readers skip this section in their first reading.

We define the convolution kernel of a given kernel \(k\) as \(k_{*}(\bm{x},\bm{x}^{\prime})\coloneqq\int k(\bm{x},\bm{x}^{\prime\prime})k( \bm{x}^{\prime\prime},\bm{x}^{\prime})\,\mathrm{d}P_{X}(\bm{x}^{\prime\prime})\), which is possible whenever \(k(\bm{x},\cdot)\in L_{2}(P_{X})\) for all \(\bm{x}\). The latter condition is satisfied for bounded kernels. Our starting point is the following new lower bound:

**Proposition 5** (**Spectral lower bound**).: _Assume that the kernel matrix \(k(\bm{X},\bm{X})\) is almost surely positive definite, and that \(\mathrm{Var}(y|\bm{x})\geq\sigma^{2}\) for \(P_{X}\)-almost all \(\bm{x}\). Then, the expected excess risk satisfies_

\[\mathbb{E}_{D}R_{P}(f_{t,\rho})-R_{P}^{*}\geq\frac{\sigma^{2}}{n}\sum_{i=1}^{ n}\mathbb{E}_{\bm{X}}\frac{\lambda_{i}(k_{*}(\bm{X},\bm{X})/n)\left(1-e^{-2t( \lambda_{i}(k(\bm{X},\bm{X})/n)+\rho)}\right)^{2}}{(\lambda_{i}(k(\bm{X},\bm{X })/n)+\rho)^{2}}\;.\] (3)

Using concentration inequalities for kernel matrices and the relation between the integral operators of \(k\) and \(k_{*}\), it can be seen that for \(t=\infty\) and \(\rho=0\), every term in the sum in Eq. (3) should converge to \(1\) as \(n\to\infty\). However, since the number of terms in the sum increases with \(n\) and the convergence may not be uniform, this is not sufficient to show inconsistency in expectation. Instead, relative concentration bounds that are even stronger than the ones by Valdivia (2018) would be required to show inconsistency in expectation. However, by combining multiple weaker bounds and further arguments on kernel equivalences, we can still show inconsistency in expectation for a class of dot-product kernels on the sphere, including certain NTK and NNGP kernels (Appendix B.4):

**Theorem 6** (**Inconsistency for Sobolev dot-product kernels on the sphere**).: _Let \(k\) be a dot-product kernel on \(\mathbb{S}^{d}\), i.e., a kernel of the form \(k(\bm{x},\bm{x}^{\prime})=\kappa(\langle\bm{x},\bm{x}^{\prime}\rangle)\), such that its RKHS \(\mathcal{H}_{k}\) is equivalent to a Sobolev space \(H^{*}(\mathbb{S}^{d})\), \(s>d/2\). Moreover, let \(P\) be a distribution on \(\mathbb{S}^{d}\times\mathbb{R}\) such that \(P_{X}\) has a lower and upper bounded density w.r.t. the uniform distribution \(\mathcal{U}(\mathbb{S}^{d})\), and such that \(\mathrm{Var}(y|\bm{x})\geq\sigma^{2}>0\) for \(P_{X}\)-almost all \(\bm{x}\in\mathbb{S}^{d}\). Then, for every \(C>0\), there exists \(c>0\) independent of \(\sigma^{2}\) such that for all \(n\geq 1\), \(t\in(C^{-1}n^{2s/d},\infty]\), and \(\rho\in[0,Cn^{-2s/d})\), the expected excess risk satisfies_

\[\mathbb{E}_{D}R_{P}(f_{t,\rho})-R_{P}^{*}\geq c\sigma^{2}>0\;.\]

The assumptions of Theorem 6 and Theorem 4 differ in several ways. Theorem 6 applies to arbitrarily high smoothness \(s\) and therefore to ReLU NTKs and NNGPs in arbitrary dimension \(d\). Moreover, it applies to distributions on the whole sphere and allows more general noise distributions. On the flip side, it only shows inconsistency in expectation, which we believe could be extended to inconsistency for Gaussian noise. Moreover, it only applies to functions of the form \(f_{t,\rho}\) but provides an explicit bound on \(t\) and \(\rho\) to get inconsistency. For \(t=\infty\), the bound \(\rho=O(n^{-2s/d})\) appears to be tight, as larger \(\rho\) yield consistency for comparable Sobolev kernels on \(\mathbb{R}^{d}\)(Steinwart et al., 2009, Corollary 3).

We only prove Theorem 6 for dot-product kernels on the sphere since we can show for these kernels that \(\mathcal{H}_{k_{*}}\) is equivalent to a Sobolev space (Lemma F.13), while this is not true for open domains \(\Omega\)(Schaback, 2018). However, an improved understanding of \(\mathcal{H}_{k_{*}}\) for such \(\Omega\) could potentially allow to extend our proof to the non-spherical case.

The spectral lower bounds in Theorem F.2 show that our approach can directly benefit from developing better kernel matrix concentration inequalities. Conversely, the investigation of consistent kernel interpolation might provide information about where such concentration inequalities do not hold.

Consistency via spiky-smooth estimators - even in fixed dimension

In Section 4, we have seen that when common kernel estimators overfit, they are inconsistent for many kernels and a wide variety of distributions. We now design consistent interpolating kernel estimators. The key is to violate Assumption (N) for every fixed Sobolev RKHS norm \(\|\cdot\|_{\mathcal{H}_{k}}\) and introduce an inductive bias towards learning spiky-smooth functions.

### Almost universal consistency of spiky-smooth ridgeless kernel regression

In high dimensional regimes (where the dimension \(d\) is supposed to grow with the number of data points), benign overfitting of linear and kernel regression has been understood by an additive decomposition of the minimum-norm interpolant into a smooth regularized component that is responsible for good generalization, and a spiky component that interpolates the noisy data points while not harming generalization (Bartlett et al., 2021). This inspires us to enforce such a decomposition in arbitrary fixed dimension by adding a sharp kernel spike \(\rho\tilde{k}_{\gamma_{n}}\) to a common kernel \(\tilde{k}\). In this way, we can still generate any Sobolev RKHS (see Appendix G.2).

**Definition 7** (Spiky-smooth kernel).: Let \(\tilde{k}\) denote any universal kernel function on \(\mathbb{R}^{d}\). We call it the smooth component. Consider a second, translation invariant kernel \(\tilde{k}_{\gamma}\) of the form \(k_{\gamma}(\bm{x},\bm{y})=q(\frac{\bm{x}-\bm{y}}{\gamma})\), for some function \(q:\mathbb{R}^{d}\to\mathbb{R}\). We call it the spiky component. Then we define the \(\rho\)_-regularized spiky-smooth kernel with spike bandwidth_\(\gamma\) as

\[k_{\rho,\gamma}(\bm{x},\bm{y})=\tilde{k}(\bm{x},\bm{y})+\rho\cdot\tilde{k}_{ \gamma}(\bm{x},\bm{y}),\qquad\bm{x},\bm{y}\in\mathbb{R}^{d}.\] (4)

We now show that the minimum-norm interpolant of the spiky-smooth kernel sequence with properly chosen \(\rho_{n},\gamma_{n}\to 0\) is consistent for a large class of distributions, on a space with fixed (possibly small) dimension \(d\). We establish our result under the following assumption (as in Mucke and Steinwart (2019)), which is weaker than our previous Assumption (D1).

* There exists a constant \(\beta_{X}>0\) and a continuous function \(\phi:[0,\infty)\to[0,1]\) with \(\phi(0)=0\) such that the data generating probability distribution satisfies \(P_{X}(B_{t}(\bm{x}))\leq\phi(t)=O(t^{\beta_{X}})\) for all \(\bm{x}\in\Omega\) and all \(t\geq 0\) (here \(B_{t}(\bm{x})\) denotes the Euclidean ball of radius \(t\) around \(\bm{x}\)).

**Theorem 8** (Consistency of spiky-smooth ridgeless kernel regression).: _Assume that the training set \(D\) consists of \(n\) i.i.d. pairs \((\bm{x},y)\sim P\) such that the marginal \(P_{X}\) fulfills (D2) and \(\mathbb{E}y^{2}<\infty\). Let the kernel components satisfy:_

* \(\tilde{k}\) _is a universal kernel, and_ \(\rho_{n}\to 0\) _and_ \(n\rho_{n}^{4}\to\infty\)_._
* \(\tilde{k}_{\gamma_{n}}\) _denotes the Laplace kernel with a sequence of positive bandwidths_ \((\gamma_{n})\) _fulfilling_ \(\gamma_{n}\leq n^{-\frac{2+\alpha}{d}}\left(\left(\frac{\alpha}{4}+\frac{ \alpha}{2}\right)\ln n\right)^{-1}\)_, where_ \(\alpha>0\) _arbitrary._

_Then the minimum-norm interpolant of the \(\rho_{n}\)-regularized spiky-smooth kernel sequence \(k_{n}\coloneqq k_{\rho_{n},\gamma_{n}}\) is consistent for \(P\)._

**Remark 9** (Benign overfitting with optimal convergence rates).: Suppose that we have a Sobolev target function \(f^{*}\in H^{s^{*}}(\Omega)\backslash\{0\}\), that the noise satisfies a moment condition and that \(P_{X}\) has an upper- and lower-bounded density on a Lipschitz domain or the sphere. Then, we show in Theorem G.5 that, by using smooth components \(\tilde{k}\) whose RKHS is equivalent to a Sobolev space \(H^{s}\), \(s>\max(s^{*},d/2)\), and choosing the spike components \(\tilde{k}_{\gamma_{n}}\) as in Theorem 8, the minimum-norm interpolant of \(k_{n}\coloneqq k_{\rho_{n},\gamma_{n}}\) achieves the convergence rate \(n^{-\frac{s^{*}}{(s^{*}+d/2)}}\log^{2}(n)\) when choosing the quasi-regularization \(\rho_{n}\) properly. Moreover, for \(s^{*}>d/2\), this rate is known to be optimal up to the factor \(\log^{2}(n)\) (Remark G.6). Since optimal rates can be achieved both with optimal regularization and with interpolation, our results show that in Sobolev RKHSs, overfitting is neither intrinsically helpful nor harmful for generalization with the right choice of kernel function.

Figure 2: The spiky-smooth kernel with Laplace components (orange) consists of a Laplace kernel (blue) plus a Laplace kernel of height \(\rho\) and small bandwidth \(\gamma\).

Proof idea.With sharp spikes \(\gamma\to 0\), it holds that \(\tilde{k}_{\gamma}(\bm{X},\bm{X})\approx\bm{I}_{n}\), with high probability. Hence, ridgeless kernel regression with the spiky-smooth kernel interpolates the training set while approximating kernel ridge regression with the smooth component \(\tilde{k}\) and regularization \(\rho\). 

The theorem even holds under much weaker assumptions on the decay behavior of the spike component \(\tilde{k}_{\gamma_{n}}\), including Gaussian and Matern kernels. The full version of the theorem and its proof can be found in Appendix G. It also applies to kernels and distributions on the sphere \(\mathbb{S}^{d}\).

**Remark 10** (**Interplay between smoothness and dimensionality**).: Irrespective of the dimension \(d\), we achieve benign overfitting with estimators in RKHS of arbitrary degrees of smoothness. With increasing \(d\), for the Laplace kernel the spike bandwidth is allowed to be chosen as \(\gamma_{n}=\Omega(n^{-(2+\alpha)/d})\), \(\alpha>0\), for covariate distributions with upper bounded Lebesgue density (see Remark G.2). Hence the magnitude of derivatives of the spikes is allowed to scale less aggressively with increasing dimension. 

### From spiky-smooth kernels to spiky-smooth activation functions

So far, our discussion revolved around the properties of kernels and whether they lead to estimators that are consistent. We now turn our attention to the neural network side. The big question is whether it is possible to specifically design activation functions that enable benign overfitting in fixed, possibly small dimension. We will see that the answer is yes: similarly to adding sharp spikes to a kernel, we add tiny fluctuations to the activation function. Concretely, we exploit (Simon et al., 2022, Theorem 3.1). It states that any dot-product kernel on the sphere that is a dot-product kernel in every dimension \(d\) can be written as an NNGP kernel or an NTK of two-layer fully-connected networks with a specifically chosen activation function. Further details can be found in Appendix H.

**Theorem 11** (**Connecting kernels and activation functions**(Simon et al., 2022)).: _Let \(\kappa:[-1,1]\to\mathbb{R}\) be a function such that \(k_{d}:\mathbb{S}^{d}\times\mathbb{S}^{d}\to\mathbb{R},k_{d}(\bm{x},\bm{x}^{ \prime})=\kappa(\langle\bm{x},\bm{x}^{\prime}\rangle)\) is a kernel for every \(d\geq 1\). Then, there exist \(b_{i}\geq 0\) with \(\sum_{i=0}^{\infty}b_{i}<\infty\) such that \(\kappa(t)=\sum_{i=0}^{\infty}b_{i}t\), and for any choice of signs \((s_{i})_{i\in\mathbb{N}_{0}}\subseteq\{-1,+1\}\), the kernel \(k_{d}\) can be realized as the NNGP kernel or NTK of a two-layer fully-connected network without biases and with activation function_

\[\phi^{k_{d}}_{NNGP}(x)=\sum_{i=0}^{\infty}s_{i}(b_{i})^{1/2}h_{i}(x),\qquad \phi^{k_{d}}_{NTK}(x)=\sum_{i=0}^{\infty}s_{i}\left(\frac{b_{i}}{i+1}\right)^ {1/2}h_{i}(x).\] (5)

_Here, \(h_{i}\) denotes the \(i\)-th Probabilist's Hermite polynomial normalized such that \(\|h_{i}\|_{L_{2}(\mathcal{N}(0,1))}=1\)._

The following proposition justifies the approach of adding spikes \(\rho^{1/2}\phi^{\tilde{k}_{\gamma}}\) to an activation function to enable harmless interpolation with wide neural networks. Here we state the result for the case of the NTK; an analogous result holds for induced NNGP activation functions.

**Proposition 12** (**Additive decomposition of spiky-smooth activation functions**).: _Fix \(\tilde{\gamma},\rho>0\) arbitrary. Let \(k=\tilde{k}+\rho\tilde{k}_{\gamma}\) denote the spiky-smooth kernel where \(\tilde{k}\) and \(\tilde{k}_{\gamma}\) are Gaussian kernels of bandwidth \(\tilde{\gamma}\) and \(\gamma\), respectively. Assume that we choose signs \(\{s_{i}\}_{i\in\mathbb{N}}\) and then the activation functions \(\phi^{k}_{NTK}\), \(\phi^{\tilde{k}}_{NTK}\) and \(\phi^{\tilde{k}_{\gamma}}_{NTK}\) as in Theorem 11. Then, for \(\gamma>0\) small enough, it holds that_

\[\|\phi^{k}_{NTK}-(\phi^{k}_{NTK}+\sqrt{\rho}\cdot\phi^{\tilde{k}_{\gamma}}_{NTK })\|^{2}_{L_{2}(\mathcal{N}(0,1))}\leq 2^{1/2}\rho\gamma^{3/2}\exp\left(- \frac{1}{\gamma}\right)+\frac{4\pi(1+\tilde{\gamma})\gamma}{\tilde{\gamma}}.\]

**Proof idea.** When the spikes are sharp enough (\(\gamma\) small enough), the smooth and the spiky component of the activation function are approximately orthogonal in \(L_{2}(\mathcal{N}(0,1))\) (Figure 2(c)), so that the spiky-smooth activation function can be approximately additively decomposed into the smooth activation component \(\phi^{\tilde{k}}\) and the spike component \(\phi^{\tilde{k}}\) responsible for interpolation. 

To motivate why the added spike functions \(\rho^{1/2}\phi^{\tilde{k}_{\gamma}}\) should have small amplitudes, observe that Gaussian activation components \(\phi^{\tilde{k}_{\gamma}}\) satisfy

\[\|\phi^{\tilde{k}_{\gamma}}_{NNGP}\|^{2}_{L_{2}(\mathcal{N}(0,1))}=1,\qquad\| \phi^{\tilde{k}_{\gamma}}_{NTK}\|^{2}_{L_{2}(\mathcal{N}(0,1))}=\frac{\gamma}{ 2}\left(1-\exp\left(-\frac{2}{\gamma}\right)\right).\] (6)

Hence, the average amplitude of NNGP spike activation components \(\rho^{1/2}\phi^{\tilde{k}_{\gamma}}\) does not depend on \(\gamma\), while the average amplitude of NTK spike components decays to \(0\) with \(\gamma\to 0\). Since consistency requires the quasi-regularization \(\rho\to 0\), the spiky component of induced NTK as well as NNGP activation functions should vanish for large data sets \(n\to\infty\) to achieve consistency.

## 6 Experiments

Now we explore how appropriate spiky-smooth activation functions might look like and whether they indeed enable harmless interpolation for trained networks of finite width on finite data sets. Further experimental results are reported in Appendix I.

### What do common activation functions lack in order to achieve harmless interpolation?

To understand which properties we have to introduce into activation functions to enable harmless interpolation, we plot NTK spike components \(\phi^{k_{\gamma}}\) induced by the Gaussian kernel (Figure 2(a),b) as well as their Hermite series coefficients (Figure 2(c)). Remarkably, the spike components \(\phi^{\underline{k}_{\gamma}}\) approximately correspond to a shifted, high-frequency \(\sin\)-curve, when choosing the signs \(s_{i}\) in (5) to alternate every second \(i\), that is \(s_{i}=+1\) iff \(\lfloor i/2\rfloor\) even (Figure 2(a)). Proposition H.1 shows that the NNGP activation functions correspond to the fluctuation function

\[\omega_{\mathrm{NNGP}}(x;\gamma)\coloneqq\sqrt{2}\cdot\sin\left(\sqrt{2/ \gamma}\cdot x+\pi/4\right)=\sin\left(\sqrt{2/\gamma}\cdot x\right)+\cos\left( \sqrt{2/\gamma}\cdot x\right),\] (7)

where the last equation follows from the trigonometric addition theorem. For small bandwidths \(\gamma\), the NTK activation functions are increasingly well approximated (Appendix I.6) by

\[\omega_{\mathrm{NTK}}(x;\gamma)\coloneqq\sqrt{\gamma}\cdot\sin\left(\sqrt{2/ \gamma}\cdot x+\pi/4\right)=\sqrt{\gamma/2}\left(\sin\left(\sqrt{2/\gamma} \cdot x\right)+\cos\left(\sqrt{2/\gamma}\cdot x\right)\right).\] (8)

With decreasing bandwidth \(\gamma\to 0\) the frequency increases, while the amplitude decreases for the NTK and remains constant for the NNGP (see Eq. (6)). Plotting equivalent spike components \(\phi^{\underline{k}_{\gamma}}\) with different choices of the signs \(s_{i}\) (Figure 2(b) and Appendix I.5) suggests that harmless interpolation requires activation functions that contain **small high-frequency oscillations** or that **explode at large \(|x|\)**, which only affects few neurons. The Hermite series expansion of suitable activation functions should contain **non-negligible weight spread across high-order coefficients** (Figure 2(c)). While Simon et al. (2022) already truncate the Hermite series of induced activation functions at order \(5\), Figure 2(c) shows that an accurate approximation of spiky-smooth activation functions requires the truncation index to be larger than \(2/\gamma\). Only a careful implementation allows us to capture the high-order fluctuations in the Hermite series of the spiky activation functions. Our implementation can be found at https://github.com/moritzhaas/mind-the-spikes.

### Training neural networks to achieve harmless interpolation in low dimension

In Figure 1, we plot the results of (a) ridgeless kernel regression and (b) trained 2-layer neural networks with standard choices of kernels and activation functions (blue) as well as our spiky-smooth alternatives (orange). We trained on 15 points sampled i.i.d. from \(x=(x_{1},x_{2})\sim\mathcal{U}(\mathbb{S}^{1})\) and \(y=x_{1}+\varepsilon\) with \(\varepsilon\sim\mathcal{N}(0,0.25)\). The figure shows that both the Laplace kernel and standard ReLU networks interpolate the training data too smoothly in low dimension, and do not generalize well. However, our spiky-smooth kernel and neural networks with spiky-smooth activation functions achieve close to optimal generalization while interpolating the training data with sharp spikes.

Figure 3: **a, b.** Gaussian NTK activation components \(\phi^{\underline{k}_{\gamma}}_{NTK}\) defined via (5) induced by the Gaussian kernel with varying bandwidth \(\gamma\in[0.2,0.1,0.05]\) (the darker, the smaller \(\gamma\)) for **a.** bi-alternating signs \(s_{i}=+1\) iff \(\lfloor i/2\rfloor\) even, and **b.** randomly iid chosen signs \(s_{i}\sim\mathcal{U}(\{-1,+1\})\). **c.** Coefficients of the Hermite series of a Gaussian NTK activation component with varying bandwidth \(\gamma\). Observe peaks at \(2/\gamma\). For reliable approximations of activation functions use a truncation \(\geq 4/\gamma\). The sum of squares of the coefficients follows Eq. (6). Figure I.8 visualizes NNGP activation components.

We achieve this by using the adjusted activation function with high-frequency oscillations \(x\mapsto\mathrm{ReLU}(x)+\omega_{\mathrm{NTK}}(x;\frac{1}{5000})\) as defined in Eq. (8). With this choice, we avoid activation functions with exploding behavior, which would induce exploding gradients. Other choices of amplitude and frequency in Eq. (8) perform worse. To bring our neural networks close to the kernel regime, we use the neural tangent parameterization (Jacot et al., 2018) and make the networks very wide (20000 hidden neurons). To ensure that the initial function is identically zero, we use the antisymmetric initialization trick (Zhang et al., 2020). Over the course of training (Figure 1c), the standard ReLU network exhibits harmful overfitting, whereas the NN with a spiky-smooth activation function quickly interpolates the training set with nearly optimal generalization. Training details and hyperparameter choices can be found in Appendix I.1. Although the high-frequency oscillations perturb the gradients, the NN with spiky smooth activation has a stable training trajectory using gradient descent with a large learning rate of \(0.4\) or stochastic gradient descent with a learning rate of \(0.04\). Since our activation function is the sum of two terms, we can additively decompose the network into its ReLU-component and its \(\omega_{\mathrm{NTK}}\)-component. Figure 1b and Appendix I.2 demonstrate that our interpretation of the \(\omega_{\mathrm{NTK}}\)-component as'spiky' is accurate: The oscillations in the hidden neurons induced by \(\omega_{\mathrm{NTK}}\) interfere constructively to interpolate the noise in the training points and regress to \(0\) between training points. This entails immediate access to the signal component of the trained neural network in form of its ReLU-component.

## 7 Conclusion

Conceptually, our work shows that inconsistency of overfitting is quite a generic phenomenon for regression in fixed dimension. However, particular spiky-smooth estimators enable benign overfitting, even in fixed dimension. We translate the spikes that lead to benign overfitting in kernel regression into infinitesimal fluctuations that can be added to activation functions to consistently interpolate with wide neural networks. Our experiments verify that neural networks with spiky-smooth activation functions can exhibit benign overfitting even on small, low-dimensional data sets.

Technically, our inconsistency results cover many distributions, Sobolev spaces of arbitrary order, and arbitrary RKHS-norm-bounded overfitting estimators. Lemma E.1 serves as a generic tool to extend generalization bounds to the sphere \(\mathbb{S}^{d}\), allowing us to cover (deep) ReLU NTKs and ReLU NNGPs.

Future work.While our experiments serve as a promising proof of concept, it remains unclear how to design activation functions that enable harmless interpolation of more complex neural network architectures and data sets. As another interesting insight, our consistent kernel sequence shows that although kernels may have equivalent RKHS (see Appendix G.2), their generalization error can differ arbitrarily much; the constants of the equivalence matter and the narrative that depth does not matter in the NTK regime as in Bietti and Bach (2021) is too simplified. More promisingly, analyses that extend our analysis in the infinite-width limit to a joint scaling of width and depth could help us to understand the influence of depth (Fort et al., 2020; Li et al., 2021; Seleznova and Kutyniok, 2022). Finite-sample analyses of moderate-width neural networks with feature learning parametrizations (Yang and Hu, 2021) and other initializations could enable to understand how to induce a spiky-smooth inductive bias in feature learning neural architectures.

Funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC 2075 - 390740016 and EXC 2064/1 - Project 390727645, as well as the DFG Priority Program 2298/1, project STE 1074/5-1. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Moritz Haas and David Holzmuller. We want to thank Tizian Wenzel and Vaclav Voracek for interesting discussions. We also thank Nadine Grosse, Jens Wirth, and Daniel Winkle for helpful comments on Sobolev spaces, and Nilotpal Sinha for pointing us to Laurent series.

## References

* Adams and Fournier (2003) Robert A. Adams and John J.F. Fournier. _Sobolev Spaces_. Elsevier Science, 2003.
* Adams et al. (2018)Michael Aerni, Marco Milanta, Konstantin Donhauser, and Fanny Yang. Strong inductive biases provably prevent harmless interpolation. In _International Conference on Learning Representations (ICLR)_, 2023.
* Agranovich (2015) Mikhail S Agranovich. _Sobolev spaces, their generalizations and elliptic problems in smooth and Lipschitz domains_. Springer, 2015.
* Arnould et al. (2023) Ludovic Arnould, Claire Boyer, and Erwan Scornet. Is interpolation benign for random forest regression? In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2023.
* Arora et al. (2019) Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Russ R. Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* Averyanov and Celisse (2020) Yaroslav Averyanov and Alain Celisse. Early stopping and polynomial smoothing in regression with reproducing kernels. _arXiv:2007.06827_, 2020.
* Bartlett et al. (2020) Peter L. Bartlett, Philip M. Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_, 117(48):30063-30070, 2020.
* Bartlett et al. (2021) Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. _Acta Numerica_, 30:87-201, 2021.
* Beaglehole et al. (2023) Daniel Beaglehole, Mikhail Belkin, and Parthe Pandit. On the inconsistency of kernel ridgeless regression in fixed dimensions. _SIAM Journal on Mathematics of Data Science_, 5(4):854-872, 2023.
* Belkin et al. (2018) Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to understand kernel learning. In _International Conference on Machine Learning (ICML)_, 2018.
* Belkin et al. (2019) Mikhail Belkin, Alexander Rakhlin, and Alexandre B. Tsybakov. Does data interpolation contradict statistical optimality? In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2019.
* Bell (2014) Jordan Bell. The singular value decomposition of compact operators on Hilbert spaces, 2014.
* Bietti and Bach (2021) Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. In _International Conference on Learning Representations (ICLR)_, 2021.
* Buchholz (2022) Simon Buchholz. Kernel interpolation in sobolev spaces is not consistent in low dimensions. In _Conference on Learning Theory (COLT)_, 2022.
* Cao and Gu (2019) Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* Carl and Stephani (1990) Bernd Carl and Irmtraud Stephani. _Entropy, Compactness and the Approximation of Operators_. Cambridge University Press, Cambridge, 1990.
* Chatterji and Long (2021) Niladri S. Chatterji and Philip M. Long. Finite-sample analysis of interpolating linear classifiers in the overparameterized regime. _Journal of Machine Learning Research (JMLR)_, 2021.
* Chen and Xu (2021) Lin Chen and Sheng Xu. Deep neural tangent kernel and laplace kernel have the same rkhs. In _International Conference on Learning Representations (ICLR)_, 2021.
* Chen et al. (2021) Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is sufficient to learn deep ReLU networks? In _International Conference on Learning Representations (ICLR)_, 2021.
* Chizat et al. (2019) Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* 127, 2006.
* Chizat et al. (2019)* Daniely et al. (2016) Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2016.
* De Vito et al. (2021) Ernesto De Vito, Nicole Mucke, and Lorenzo Rosasco. Reproducing kernel hilbert spaces on manifolds: Sobolev and diffusion spaces. _Analysis and Applications_, 19(03):363-396, 2021.
* DeVore and Sharpley (1993) Ronald A DeVore and Robert C Sharpley. Besov spaces on domains in \(\mathbb{R}^{d}\). _Transactions of the American Mathematical Society_, 335(2):843-864, 1993.
* Devroye et al. (1998) Luc Devroye, Laszlo Gyorfi, and Adam Krzyzak. The Hilbert kernel regression estimate. _Journal of Multivariate Analysis_, 65(2):209-227, 1998.
* Di Nezza et al. (2012) Eleonora Di Nezza, Giampiero Palatucci, and Enrico Valdinoci. Hitchhiker's guide to the fractional Sobolev spaces. _Bulletin des Sciences Mathematiques_, 136(5):521-573, 2012.
* Donhauser et al. (2021) Konstantin Donhauser, Mingqi Wu, and Fanny Yang. How rotational invariance of common kernels prevents generalization in high dimensions. In _International Conference on Machine Learning (ICML)_, 2021.
* Donhauser et al. (2022) Konstantin Donhauser, Nicolo Ruggeri, Stefan Stojanovic, and Fanny Yang. Fast rates for noisy interpolation require rethinking the effect of inductive bias. In _International Conference on Machine Learning (ICML)_, 2022.
* Duembgen (2010) Lutz Duembgen. Bounding standard gaussian tail probabilities. _arXiv:1012.2063_, 2010.
* Edmunds and Triebel (1996) David E. Edmunds and Hans Triebel. _Function Spaces, Entropy Numbers, Differential Operators_. Cambridge University Press, Cambridge, 1996.
* Fischer and Steinwart (2020) Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algorithms. _Journal of Machine Learning Research (JMLR)_, 2020.
* Fort et al. (2020) Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* Frei et al. (2022) Spencer Frei, Niladri S Chatterji, and Peter Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In _Conference on Learning Theory (COLT)_, 2022.
* Gerschgorin (1931) Semjon Gerschgorin. Uber die Abgrenzung der Eigenwerte einer Matrix. _Izv. Akad. Nauk. USSR Otd. Fiz.-Mat. Nauk_, 1931.
* 1054, 2021.
* Ghosh and Belkin (2022) Nikhil Ghosh and Mikhail Belkin. A universal trade-off between the model size, test loss, and training loss of linear predictors. _arXiv:2207.11621_, 2022.
* Gneiting (2013) Tilmann Gneiting. Strictly and non-strictly positive definite functions on spheres. _Bernoulli_, 19(4):1327-1349, 2013.
* 986, 2022.
* He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _IEEE international conference on computer vision (ICCV)_, 2015.
* Holzmuller (2021) David Holzmuller. On the universality of the double descent peak in ridgeless regression. In _International Conference on Learning Representations (ICLR)_, 2021.
* Holzmuller et al. (2021)David Holzmuller and Ingo Steinwart. Training two-layer relu networks with gradient descent is inconsistent. _Journal of Machine Learning Research (JMLR)_, 23(181):1-82, 2022.
* Horn and Johnson (2013) Roger A. Horn and Charles R. Johnson. _Matrix Analysis_. Cambridge University Press, 2013.
* Hu et al. (2021) Tianyang Hu, Wenjia Wang, Cong Lin, and Guang Cheng. Regularization matters: A nonparametric perspective on overparametrized neural network. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2021.
* Hubert et al. (2015) Simon Hubert, Quoc Le Gia, and Tanya Morton. _Spherical Radial Basis Functions, Theory and Applications_. Springer International Publishing, 2015.
* Hubert et al. (2023) Simon Hubert, Emilio Porcu, Chris J. Oates, and Mark Girolami. Sobolev spaces, kernels and discrepancies over hyperspheres. _Transactions on Machine Learning Research (TMLR)_, 2023.
* Jacot et al. (2018) Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and generalization in neural networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* Ji et al. (2021) Ziwei Ji, Justin D. Li, and Matus Telgarsky. Early-stopped neural networks are consistent. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* Johansson et al. (2023) Fredrik Johansson et al. _mpmath: a Python library for arbitrary-precision floating-point arithmetic (version 1.3.0)_, 2023. https://mpmath.org/.
* Kanagawa et al. (2018) Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K. Sriperumbudur. Gaussian processes and kernel methods: A review on connections and equivalences. _arXiv:1805.08845v1_, 2018.
* Lai et al. (2023) Jianfa Lai, Manyun Xu, Rui Chen, and Qian Lin. Generalization ability of wide neural networks on \(\mathbb{R}\). _arXiv:2302.05933_, 2023.
* 1338, 2000.
* Lee et al. (2018) Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as Gaussian processes. In _International Conference on Learning Representations (ICLR)_, 2018.
* Lee et al. (2019) Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* Li et al. (2021) Mufan Li, Mihai Nica, and Dan Roy. The future is log-gaussian: Resnets and their infinite-depth-and-width limit at initialization. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* Li et al. (2023) Yicheng Li, Haobo Zhang, and Qian Lin. Kernel interpolation generalizes poorly. _Biometrika_, 2023.
* Liang and Rakhlin (2020) Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel "ridgeless" regression can generalize. _Annals of Statistics_, 48(3):1329-1347, 2020.
* Liang and Recht (2023) Tengyuan Liang and Benjamin Recht. Interpolating classifiers make few mistakes. _Journal of Machine Learning Research (JMLR)_, 24(20):1-27, 2023.
* Liang et al. (2020) Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels. In _Conference on Learning Theory (COLT)_, 2020.
* Lions and Magenes (2012) Jacques Louis Lions and Enrico Magenes. _Non-homogeneous boundary value problems and applications: Vol. 1_. Springer Science & Business Media, 2012.
* Mallinar et al. (2022) Neil Rohit Mallinar, James B Simon, Amirhesam Abedsoltan, Parthe Pandit, Misha Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* Mensens et al. (2018)Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. In _International Conference on Learning Representations (ICLR)_, 2018.
* Mcrae et al. (2022) Andrew D. Mcrae, Santhosh Karnik, Mark Davenport, and Vidya K. Muthukumar. Harmless interpolation in regression and classification with structured features. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2022.
* Megginson (1998) Robert E. Megginson. _An Introduction to Banach Space Theory_. Springer-Verlag, New York, 1998.
* Mirsky (1959) Leon Mirsky. On the trace of matrix products. _Mathematische Nachrichten_, 20(3-6):171-174, 1959.
* Montanari et al. (2019) Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime. _arXiv:1911.01544_, 2019.
* Muthukumar et al. (2021) Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and Anant Sahai. Classification vs regression in overparameterized regimes: Does the loss function matter? _JMLR_, 22(1):10104-10172, 2021.
* Mucke and Steinwart (2019) Nicole Mucke and Ingo Steinwart. Global minima of DNNs: The plenty pantry. _arXiv:1905.10686_, 2019.
* Neal (1996) Radford M. Neal. _Priors for Infinite Networks_. Springer New York, 1996.
* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* Pietsch (1987) Albrecht Pietsch. _Eigenvalues and s-Numbers_. Geest & Portig K.-G., Leipzig, 1987.
* Pinelis (2020) Iosif Pinelis. Exact lower and upper bounds on the incomplete gamma function. _Mathematical Inequalities & Applications_, 23(4):1261-1278, 2020.
* Rakhlin and Zhai (2019) Alexander Rakhlin and Xiyu Zhai. Consistency of interpolation with Laplace kernels is a high-dimensional phenomenon. In _Conference on Learning Theory (COLT)_, 2019.
* 215, 2023.
* Raskutti et al. (2014) Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Early stopping and non-parametric regression: An optimal data-dependent stopping rule. _Journal of Machine Learning Research (JMLR)_, 15(11):335-366, 2014.
* Rasmussen and Williams (2005) Carl Edward Rasmussen and Christopher K. I. Williams. _Gaussian Processes for Machine Learning_. MIT Press, 2005.
* Richter (1958) Hans Richter. Zur Abschatzung von Matrizennormen. _Mathematische Nachrichten_, 18(1-6):178-187, 1958.
* Schaback (2018) Robert Schaback. Superconvergence of kernel-based interpolation. _Journal of Approximation Theory_, 235:1-19, 2018.
* Schneider and Grosse (2013) Cornelia Schneider and Nadine Grosse. Sobolev spaces on Riemannian manifolds with bounded geometry: General coordinates traces. _Mathematische Nachrichten_, 286(16), 2013.
* Seleznova and Kutyniok (2022) Mariia Seleznova and Gitta Kutyniok. Neural tangent kernel beyond the infinite-width limit: Effects of depth and initialization. In _International Conference on Machine Learning (ICML)_, 2022.
* Shamir (2022) Ohad Shamir. The implicit bias of benign overfitting. In _Conference on Learning Theory (COLT)_, 2022.
* Sohn et al. (2019)James Benjamin Simon, Sajant Anand, and Mike Deweese. Reverse engineering the neural tangent kernel. In _International Conference on Machine Learning (ICML)_, 2022.
* Steinwart (2001) Ingo Steinwart. Consistency of support vector machines and other regularized kernel machines. _IEEE Transactions on Information Theory_, 2001.
* Steinwart (2017) Ingo Steinwart. A short note on the comparison of interpolation widths, entropy numbers, and Kolmogorov widths. _J. Approx. Theory_, 215:13-27, 2017.
* Steinwart and Christmann (2008) Ingo Steinwart and Andreas Christmann. _Support Vector Machines_. Springer New York, 2008.
* Steinwart and Scovel (2012) Ingo Steinwart and Clint Scovel. Mercer's theorem on general domains: on the interaction between measures, kernels, and RKHSs. _Constr. Approx._, 35:363-417, 2012.
* Steinwart et al. (2009) Ingo Steinwart, Don R. Hush, and Clint Scovel. Optimal rates for regularized least squares regression. In _Conference on Learning Theory (COLT)_, 2009.
* Stroock et al. (2011) Daniel W. Stroock et al. _Essentials of integration theory for analysis_. Springer, 2011.
* Vakili et al. (2021) Sattar Vakili, Michael Bromberg, Jezabel Garcia, Da-shan Shiu, and Alberto Bernacchia. Uniform generalization bounds for overparameterized neural networks. _arXiv:2109.06099_, 2021.
* Valdivia (2018) Ernesto Araya Valdivia. Relative concentration bounds for the spectrum of kernel matrices. _arXiv:1812.02108_, 2018.
* Vershynin (2018) Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_. Cambridge University Press, 2018.
* Wang et al. (2018) Guorong Wang, Yimin Wei, Sanzheng Qiao, Peng Lin, and Yuzhuo Chen. _Generalized Inverses: Theory and Computations_. Springer, 2018.
* Wang and Scott (2022) Yutong Wang and Clayton Scott. Consistent interpolating ensembles via the manifold-Hilbert kernel. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* Wei et al. (2017) Yuting Wei, Fanny Yang, and Martin J Wainwright. Early stopping for kernel boosting algorithms: A general analysis with localized complexities. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* Wendland (2005) Holger Wendland. _Scattered Data Approximation_. Cambridge University Press, 2005.
* Xu and Gu (2023) Xingyu Xu and Yuantao Gu. Benign overfitting of non-smooth neural networks beyond lazy training. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2023.
* Yang and Hu (2021) Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In _International Conference on Machine Learning (ICML)_, 2021.
* Yao et al. (2007) Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. _Constructive Approximation_, 26:289-315, 2007.
* Zhang et al. (2021) Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* Zhang et al. (2020) Yaoyu Zhang, Zhi-Qin John Xu, Tao Luo, and Zheng Ma. A type of generalization error induced by initialization in deep neural networks. In _Proceedings of The First Mathematical and Scientific Machine Learning Conference_, 2020.

## Appendix Contents.

* A Detailed related work
* B Kernels and Sobolev spaces on the sphere
* B.1 Background on Sobolev spaces
* B.2 General kernel theory and notation
* B.3 Dot-product kernels on the sphere
* B.4 Neural kernels
* C Gradient flow and gradient descent with kernels
* C.1 Derivation of gradient flow and gradient descent
* C.2 Gradient flow and gradient descent initialized at \(0\) have monotonically growing \(\mathcal{H}\)-norm
* D Proof of Theorem 1
* D.1 Auxiliary results for the proof of Theorem 1
* E Translating between \(\mathbb{R}^{d}\) and \(\mathbb{S}^{d}\)
* F Spectral lower bound
* F.1 General lower bounds
* F.2 Equivalences of norms and eigenvalues
* F.3 Kernel matrix eigenvalue bounds
* F.4 Spectral lower bound for dot-product kernels on the sphere
* G Proof of Theorem 8
* G.1 Auxiliary results for the proof of Theorem 8
* G.2 RKHS norm bounds
* H Spiky-smooth activation functions induced by Gaussian components
* I Additional experimental results
	* I.1 Experimental details of Figure 1
	* I.2 Disentangling signal from noise in neural networks with spiky-smooth activation functions
	* I.3 Repeating the finite-sample experiments
	* I.4 Spiky-smooth activation functions
	* I.5 Isolated spike activation functions
	* I.6 Additive decomposition and \(\sin\)-fit
	* I.7 Spiky-smooth kernel hyper-parameter selection
* IDetailed related work

Motivated by Zhang et al. (2021) and Belkin et al. (2018), an abundance of papers have tried to grasp when and how benign overfitting occurs in different settings. Rigorous understanding is mainly restricted to linear (Bartlett et al., 2020), feature (Hastie et al., 2022) and kernel regression (Liang and Rakhlin, 2020) under restrictive distributional assumptions. In the well-specified linear setting under additional assumptions, the minimum-norm interpolant is consistent if and only if \(k\ll n\ll d\), the top-\(k\) eigendirections of the covariate covariance matrix align with the signal, followed by sufficiently many 'quasi-isotropic' directions with eigenvalues of similar magnitude (Bartlett et al., 2020).

Kernel methods.The analysis of kernel methods is more nuanced and depends on the interplay between the chosen kernel, the choice of regularization and the data distribution. \(L_{2}\)-generalization error bounds can be derived in the eigenbasis of the kernel's integral operator (Mcrae et al., 2022), where upper bounds of the form \(\sqrt{\bm{y}^{\top}k(\bm{X},\bm{X})^{-1}\bm{y}/n}\) promise good generalization when the regression function \(f^{*}\) is aligned with the dominant eigendirections of the kernel, or in other words, when \(\|f^{*}\|_{\mathcal{H}}\) is small. Most recent work focuses on high-dimensional limits, where the data dimensionality \(d\to\infty\). For \(d\to\infty\), the Hilbert space and its norm change, so that consistency results that demand bounded Hilbert norm of rotation-invariant kernels do not even include simple functions like sparse products (Donhauser et al., 2021, Lemma 2.1). In the regime \(d^{l+\delta}\leq n\leq d^{l+1-\delta}\), rotation-invariant (neural) kernel methods (Ghorbani et al., 2021, Donhauser et al., 2021) can in fact only learn the polynomial parts up to order \(l\) of the regression function \(f^{*}\), and fully-connected NTKs do so. Liang et al. (2020) uncover a related multiple descent phenomenon in kernel regression, where the risk vanishes for most \(n\to\infty\), but peaks at \(n=d^{l}\) for all \(i\in\mathbb{N}\). The slower \(d\) grows, the slower the optimal rate \(n^{-\frac{1}{2l+1}}\) between the peaks. Note, however, that these bounds are only upper bounds, and whether they are optimal remains an open question to the best of our knowledge. Another recent line of work analyzes how different inductive biases, measured in \(\|\cdot\|_{p}\)-norm minimization, \(p\in[1,2]\), (Donhauser et al., 2022) or in the filter size of convolutional kernels (Aerni et al., 2023), affects the generalization properties of minimum-norm interpolants. While the risk on noiseless training samples (bias) decreases with decreasing \(p\) or small filter size, the sensitivity to noise in the training data (variance) increases. Hence only 'weak inductive biases', that is large \(p\) or large filter sizes, enable harmless interpolation. Our results suggest that to achieve harmless interpolation in fixed dimension one has to construct and minimize more unusual norms than \(\|\cdot\|_{p}\)-norms.

Regularised kernel regression achieves optimal rates.With appropriate regularization, kernel ridge regularization with typical universal kernels like the Gauss, Matern, and Laplace kernels is universally consistent (Steinwart and Christmann, 2008, Chapter 9). Steinwart et al. (2009, Corollary 6) even implies minimax optimal nonparametric rates for clipped kernel ridge regression with Sobolev kernels and \(f^{*}\in H^{\beta}\) where \(d/2<\beta\leq s\) for the choice \(\rho_{n}=n^{-2s/(2\beta+d)}\). Although \(f^{*}\) is not necessarily in the RKHS, KRR is adaptive and can still achieve optimal learning rates. Lower smoothness \(\beta\) of \(f^{*}\) as well as higher smoothness of the kernel should be met with faster decay of \(\rho_{n}\). Optimal rates in Sobolev RKHS can also be achieved using cross-validation of the regularization \(\rho\)(Steinwart et al., 2009), early stopping rules based on empirical localized Rademacher (Raskutti et al., 2014) or Gaussian complexity (Wei et al., 2017) or smoothing of the empirical risk via kernel matrix eigenvalues (Averyanov and Celisse, 2020).

Lower bounds for kernel regression.Besides Rakhlin and Zhai (2019) and Buchholz (2022), Beaglehole et al. (2023) derive inconsistency results for kernel ridgeless regression given assumptions on the spectral tail in the Fourier basis. Mallinar et al. (2022) provide a characterization of kernel ridge regression into benign, tempered and catastrophic overfitting using a _heuristic_ approximation of the risk via the kernel's eigenspectrum, essentially assuming that the eigenfunctions can be replaced by structureless Gaussian random variables. A general lower bound for ridgeless linear regression Holzmuller (2021) predicts bad generalization near the "interpolation threshold", where the dimension of the feature space is close to \(n\), also known as the _double descent_ phenomenon. In this regime, Ghosh and Belkin (2022) also consider overfitting by a fraction beyond the noise level and derive a lower bound for linear models.

Benign overfitting in fixed dimension.Only few works have established consistency results for interpolating models in fixed dimension. The first statistical guarantees for Nadaraya-Watson kernel smoothing with singular kernels were given by Devroye et al. (1998). Optimal non-asymptotic results have only been established more recently. Belkin et al. (2019) show that Nadaraya-Watson kernel smoothing achieves minimax optimal convergence rates for \(a\in(0,d/2)\) under smoothness assumptions on \(f^{*}\), when using singular kernels such as truncated Hilbert kernels \(K(u)=\|u\|_{2}^{2}\mathds{1}_{\|u\|\leq 1}\), which do not induce RKHS that only contain weakly differentiable functions (as our results do). By thresholding the kernel they can adjust the amount of overfitting without affecting the generalization bound. To the best of our knowledge, rigorously proving or disproving analogous bounds for kernel ridge regression remains an open question. Arnould et al. (2023) show that median random forests are able to interpolate consistently in fixed dimension because of an averaging effect introduced through feature randomization. They conjecture consistent interpolation for Breiman random forests based on numerical experiments.

Classification.For binary classification tasks, benign overfitting is a more generic phenomenon than for regression tasks (Muthukumar et al., 2021, Shamir, 2022). Consistency has been shown under linear separability assumptions (Montanari et al., 2019, Chatterji and Long, 2021, Frei et al., 2022) and through complexity bounds with respect to reference classes like the Neural Tangent Random Feature' model (Cao and Gu, 2019, Chen et al., 2021). Most recently, Liang and Recht (2023) have shown that the 0-1-generalization error of minimum RKHS-norm interpolants \(\hat{f}_{0}\) is upper bounded by \(\frac{\|f_{0}\|_{2}^{2}}{n}\) and analogously that kernel ridge regression \(\hat{f}_{\rho}\) generalizes as \(\frac{\bm{y}^{\top}(k(\bm{X},\bm{X})+\rho I)^{-1}\bm{y}}{n}\), where the numerator upper bounds \(\|\hat{f}_{\rho}\|_{\mathcal{H}}^{2}\). Their bounds imply consistency as long as the total variation distance between the class conditionals is sufficiently large and the regression function has bounded RKHS-norm, and their Lemma 7 shows that the upper bound is rate optimal. Under a noise condition on the regression function \(f^{*}(\bm{x})=\mathbb{E}[y|\bm{x}]\) for binary classification and bounded \(\|f^{*}\|_{\mathcal{H}}\), our results together with Liang and Recht (2023) reiterate the distinction between benign overfitting for binary classification and inconsistent overfitting for least squares regression for a large class of distributions in kernel regression over Sobolev RKHS. Chapter 8 of Steinwart and Christmann (2008) discusses how the overlap of the two classes may influence learning rates under positive regularization. Using Nadaraya-Watson kernel smoothing, Wang and Scott (2022) offer the first consistency result for a simple interpolating ensemble method with data-independent base classifiers.

Connection to neural networks.It is known that neural networks can behave like kernel methods in certain infinite-width limits. For example, the function represented by a randomly initialized NN behaves like a Gaussian process with the NN Gaussian process (NNGP) kernel, which depends on details such as the activation function and depth of the NN (Neal, 1996, Lee et al., 2018, Matthews et al., 2018). Hence, Bayesian inference in infinitely wide NNs is GP regression, whose posterior predictive mean function is of the form \(f_{\infty,\rho}\), where \(\rho\) depends on the assumed noise variance. Moreover, gradient flow training of certain infinitely wide NNs is similar to gradient flow training with the so-called _neural tangent kernel_ (NTK) (Jacot et al., 2018, Lee et al., 2019, Arora et al., 2019), and the correspondence can be made exact using small modifications to the NN to remove the stochastic effect of the random initial function (Arora et al., 2019, Zhang et al., 2020). In other words, certain infinitely wide NNs trained with gradient flow learn functions of the form \(f_{t,0}\).

When considering the sphere \(\Omega=\mathbb{S}^{d}\), the NTK and NNGP kernels of fully-connected NNs are dot-product kernels, i.e., \(k(\bm{x},\bm{x}^{\prime})=\kappa(\langle\bm{x},\bm{x}^{\prime}\rangle)\) for some function \(\kappa:[-1,1]\to\mathbb{R}\). Moreover, from Bietti and Bach (2021) and Chen and Xu (2021) it follows that the RKHS of typical NTK and NNGP kernels for the ReLU activation function are equivalent to the Sobolev spaces \(H^{(d+1)/2}(\mathbb{S}^{d})\) and \(H^{(d+3)/2}(\mathbb{S}^{d})\), respectively, cf Appendix B.4.

Regarding consistency, Ji et al. (2021) use the NTK correspondence to show that early-stopped wide NNs for classification are universally consistent under some assumptions. On the other hand, Holzmuller and Steinwart (2022) show that zero-initialized biases can prevent certain two-layer ReLU NNs from being universally consistent. Lai et al. (2023) show an inconsistency-type result for overfitting two-layer ReLU NNs with \(d=1\), but for fixed inputs \(\bm{X}\). They also note that an earlier inconsistency result by Hu et al. (2021) relies on an unproven result. Li et al. (2023) show that consistency with polynomial convergence rates is impossible for minimum-norm interpolants of common kernels including ReLU NTKs. Mallinar et al. (2022) conjecture tempered overfitting and therefore inconsistency for interpolation with ReLU NTKs based on their semi-rigorous result and the results of Bietti and Bach (2021) and Chen and Xu (2021). Xu and Gu (2023) establish consistency of overfitting wide 2-layer neural networks beyond the NTK regime for binary classification in very high dimension \(d=\Omega(n^{2})\) and for a quite restricted class of distributions (the mean difference \(\mu\) of the class conditionals needs to fulfill \(\mu=\Omega((d/n)^{1/4}\log^{1/4}(md/n))\) and \(\mu=O((d/n)^{1/2})\)).

## Appendix B Kernels and Sobolev spaces on the sphere

### Background on Sobolev spaces

We say that two Hilbert spaces \(\mathcal{H}_{1},\mathcal{H}_{2}\) are equivalent, written as \(\mathcal{H}_{1}\cong\mathcal{H}_{2}\), if they are equal as sets and the corresponding norms \(\|\cdot\|_{\mathcal{H}_{1}}\) and \(\|\cdot\|_{\mathcal{H}_{2}}\) are equivalent.

Let \(\Omega\) be an open set with \(C^{\infty}\) boundary. In this paper, we will mainly consider \(\ell_{2}\)-balls for \(\Omega\). There are multiple equivalent ways to define a (fractional) Sobolev space \(H^{s}(\Omega)\), \(s\in\mathbb{R}_{\geq 0}\), these are equivalent in the sense that the resulting Hilbert spaces will be equivalent. For example, \(H^{s}(\Omega)\) can be defined through restrictions of functions from \(H^{s}(\mathbb{R}^{d})\), through interpolation spaces, or through Sobolev-Slobodetski norms (see e.g. Chapter 5 and 14 in Agranovich, 2015 and Chapters 7-10 in Lions and Magenes, 2012). Some requirements on \(\Omega\) can be relaxed, for example to Lipschitz domains, by using more general extension operators (e.g. DeVore and Sharpley, 1993). Since our results are based on equivalent norms and not specific norms, we do not care which of these definitions is used. Further background on Sobolev spaces can be found in Adams and Fournier (2003), Wendland (2005) and Di Nezza et al. (2012).

### General kernel theory and notation

There is a one-to-one correspondence between kernel functions \(k\) and the corresponding reproducing kernel Hilbert spaces (RKHS) \(\mathcal{H}_{k}\). Mercer's theorem (Steinwart and Christmann, 2008, Theorem 4.49) states that for compact \(\Omega\), continuous \(k\) and a Borel probability measure \(P_{X}\) on \(\Omega\) whose support is \(\Omega\), the integral operator \(T_{k,P_{X}}:L_{2}(P_{X})\to L_{2}(P_{X})\) given by

\[T_{k,P_{X}}f(\bm{x})=\int_{\Omega}f(\bm{x}^{\prime})k(\bm{x},\bm{x}^{\prime}) dP_{X}(\bm{x}^{\prime}),\]

can be decomposed into an orthonormal basis \((e_{i})_{i\in I}\) of \(L_{2}(P_{X})\) and corresponding eigenvalues \((\lambda_{i})_{i\in I}\geq 0\), \(\lambda_{l}\searrow 0\), such that

\[T_{k,P_{X}}f=\sum_{i\in I}\lambda_{i}\langle f,e_{i}\rangle e_{i},\qquad f\in L _{2}(P_{X}).\]

We write \(\lambda_{i}(T_{k,P_{X}})\coloneqq\lambda_{i}\). Moreover, \(k(\bm{x},\bm{x}^{\prime})=\sum_{i\in I}\lambda_{i}e_{i}(\bm{x})e_{i}(\bm{x}^{ \prime})\) converges absolutely and uniformly, and the RKHS is given by

\[\mathcal{H}_{k}=\left\{\sum_{i\in I}a_{i}\sqrt{\lambda_{i}}e_{i}\;\middle|\; \sum_{i\in I}a_{i}^{2}<\infty\right\}.\] (B.1)

The corresponding inner product between \(f=\sum_{i\in I}a_{i}\sqrt{\lambda_{i}}e_{i}\in\mathcal{H}\) and \(g=\sum_{i\in I}b_{i}\sqrt{\lambda_{i}}e_{i}\in\mathcal{H}\) can then be written as

\[\langle f,g\rangle_{\mathcal{H}}=\sum_{i\in I}a_{i}b_{i}.\] (B.2)

We use asymptotic notation \(O,\Omega,\Theta\) for integers \(n\) in the following way: We write

\[f(n) =O(g(n))\Leftrightarrow\exists C>0\forall n:f(n)\leq Cg(n)\] \[f(n) =\Omega(g(n))\Leftrightarrow g(n)=O(f(n))\] \[f(n) =\Theta(g(n))\Leftrightarrow f(n)=O(g(n))\text{ and }g(n)=O(f(n))\;.\]

Above, we require that the inequality \(f(n)\leq Cg(n)\) holds for all \(n\) and not only for \(n\geq n_{0}\). This implies that if \(f(n)=\Omega(g(n))\), then \(f\) must be nonzero whenever \(g\) is nonzero. This is an important detail when arguing about equivalence of RKHSs, since it allows the following statement: If we have two kernels \(k,k\) with Mercer representations

\[k(\bm{x},\bm{x}^{\prime})=\sum_{i\in I}\lambda_{i}e_{i}(\bm{x})e_{i}(\bm{x}^{ \prime})\]\[\tilde{k}(\bm{x},\bm{x}^{\prime})=\sum_{i\in I}\tilde{\lambda}_{i}e_{i}(\bm{x})e_ {i}(\bm{x}^{\prime})\]

with identical eigenfunctions \(e_{i}\) and eigenvalues satisfying \(\lambda_{i}=\Theta(\tilde{\lambda}_{i})\), then the associated RKHSs are equivalent by (B.1) and (B.2).

### Dot-product kernels on the sphere

A kernel of the form \(k(\bm{x},\bm{x}^{\prime})=\kappa(\langle\bm{x},\bm{x}^{\prime}\rangle)\) for some function \(\kappa\) is called _dot-product kernel_. Dot-product kernels are rotationally invariant. Especially, NTKs and NNGPs of fully-connected NNs restricted to the sphere \(\mathbb{S}^{d}\) are dot-product kernels. Moreover, kernels like the Laplace, Matern, and Gaussian kernels that only depend on the distance between their inputs are also dot-product kernels when restricted to the sphere \(\mathbb{S}^{d}\). Therefore, in this section, we will assume that \(k:\mathbb{S}^{d}\times\mathbb{S}^{d}\to\mathbb{R}\) is a dot-product kernel.

We can then leverage some convenient results from the theory of dot-product kernels on the sphere, which are summarized in more detail by Hubbert et al. (2023). Let \(\{Y_{l,1},\ldots,Y_{l,N_{l,l}}\}\) be a real orthonormal basis for the space of spherical harmonics of degree \(l\) within \(L_{2}(\mathbb{S}^{d})\). Moreover, let \(\omega_{d}\) be the surface area of \(\mathbb{S}^{d}\), then the \(\tilde{Y}_{l,i}\coloneqq\sqrt{\omega_{d}}Y_{l,i}\) form a corresponding orthonormal basis w.r.t. the uniform distribution \(\mathcal{U}(\mathbb{S}^{d})\). Then, a Mercer representation of \(k\) is given by

\[k(\bm{x},\bm{x}^{\prime})=\sum_{l=0}^{\infty}\mu_{l}\sum_{i=1}^{N_{l,d}}Y_{l,i }(\bm{x})Y_{l,i}(\bm{x}^{\prime})=\sum_{l=0}^{\infty}\tilde{\mu}_{l}\sum_{i=1} ^{N_{l,d}}\tilde{Y}_{l,i}(\bm{x})\tilde{Y}_{l,i}(\bm{x}^{\prime})\;,\]

with \(\tilde{\mu}_{l}=\mu_{l}/\omega_{d}\). Especially, the integral operator \(T_{k,\mathcal{U}(\mathbb{S}^{d})}\) for the uniform distribution \(\mathcal{U}(\mathbb{S}^{d})\) has eigenvalues \(\tilde{\mu}_{l}\) with multiplicity \(N_{l,d}\) and eigenfunctions \(\tilde{Y}_{l,i}\). The RKHS of \(k\) is then given by

\[\mathcal{H}_{k}=\left\{\sum_{l=0}^{\infty}\sqrt{\mu_{l}}\sum_{i=1}^{N_{l,d}}a _{l,i}Y_{l,i}\;\middle|\;\sum_{l=0}^{\infty}\sum_{i=1}^{N_{l,d}}a_{l,i}^{2}< \infty\right\}\;.\]

Since the index \(l\) can be zero, we will denote decay asymptotics for \(l\) in the form \(\Theta((l+1)^{-q})\) and not \(\Theta(l^{-q})\), cf. our definition of \(\Theta\) notation in Appendix B.2.

**Lemma B.1** (Sobolev dot-product kernels on the sphere).: _For a dot-product kernel \(k\) on \(\mathbb{S}^{d}\) as above, the RKHS \(\mathcal{H}_{k}\) is equivalent to the Sobolev space \(H^{s}(\mathbb{S}^{d}),s>d/2\), if and only if \(\mu_{l}=\Theta((l+1)^{-2s})\). In this case, we have_

\[\lambda_{i}(T_{k,\mathcal{U}(\mathbb{S}^{d})})=\Theta(i^{-2s/d})\;.\]

Proof.: **Step 0: Equivalence.** If \(\mu_{l}=\Theta((l+1)^{-2s})\), it is stated in Section 3 in Hubbert et al. (2023) that \(\mathcal{H}_{k}\cong H^{s}(\mathbb{S}^{d})\). On the other hand, if \(\mu_{l}\neq\Theta((l+1)^{-2s})\), it is easy to see that \(\mathcal{H}_{k}\) is not equivalent to the RKHS of a kernel with \(\mu_{l}=\Theta((l+1)^{-2s})\). It remains to show \(\lambda_{i}(T_{k,\mathcal{U}(\mathbb{S}^{d})})=\Theta(i^{-2s/d})\).

**Step 1: Ordering the eigenvalues.** Consider a permutation \(\pi:\mathbb{N}_{0}\to\mathbb{N}_{0}\) such that

\[\tilde{\mu}_{\pi(0)}\geq\tilde{\mu}_{\pi(1)}\geq\ldots\]

We can then define the partial sums

\[S_{l}\coloneqq\sum_{i=0}^{l}N_{\pi(i),d}\;.\]

For \(S_{l-1}<i\leq S_{l}\), we then have \(\lambda_{i}(T_{k,\mathcal{U}(\mathbb{S}^{d})})=\tilde{\mu}_{\pi(l)}\).

**Step 2: Show \(\pi(i)=\Theta(i)\).** Let \(c,C>0\) such that \(c(l+1)^{-2s}\leq\tilde{\mu}_{l}\leq C(l+1)^{-2s}\) for all \(l\in\mathbb{N}_{0}\). For indices \(i,j\in\mathbb{N}_{0}\), we have the implications

\[i>j \Rightarrow c(\pi(i)+1)^{-2s}\leq\tilde{\mu}_{\pi(i)}\leq\tilde{\mu}_{\pi(j)} \leq C(\pi(j)+1)^{-2s}\] \[\Rightarrow \pi(i)+1\geq\left(\frac{c}{C}\right)^{1/(2s)}(\pi(j)+1)\;.\]Therefore, for \(i\geq 1\) and \(j\geq 0\),

\[\pi(i)+1 \geq\left(\frac{c}{C}\right)^{1/(2s)}\max_{i^{\prime}<i}(\pi(i^{ \prime})+1)\geq\left(\frac{c}{C}\right)^{1/(2s)}((i-1)+1)\geq\Omega(i+1)\;,\] \[\pi(j)+1 \leq\left(\frac{C}{c}\right)^{1/(2s)}\min_{j^{\prime}>j}(\pi(j^{ \prime})+1)\leq\left(\frac{C}{c}\right)^{1/(2s)}((j+1)+1)\leq O(j+1)\;.\]

We can thus conclude that \(\pi(i)+1=\Theta(i+1)\).

**Step 3: Individual Eigenvalue decay.** As explained in Section 2.1 in Hubbert et al. (2023), we have \(N_{l,d}=\Theta((l+1)^{d-1})\). Therefore,

\[S_{l}=\sum_{i=0}^{l}\Theta((\pi(i)+1)^{d-1})=\sum_{i=0}^{l}\Theta((i+1)^{d-1}) =\Theta((l+1)^{d})\;.\]

Now, let \(i\geq 1\) and let \(l\in\mathbb{N}_{0}\) such that \(S_{l-1}<i\leq S_{l}\). We have \(i\geq\Omega(l^{d})\), and \(i\leq O((l+1)^{d})\), which implies \(i=\Theta((l+1)^{d})\) since \(i\geq 1\). Therefore,

\[\lambda_{i}=\tilde{\mu}_{\pi(l)}=\Theta((\pi(l)+1)^{-2s})=\Theta((l+1)^{-2s}) =\Theta\left(i^{-2s/d}\right)\;.\qed\]

### Neural kernels

Several NTK and NNGP kernels have RKHSs that are equivalent to Sobolev spaces on \(\mathbb{S}^{d}\). In the following cases, we can deduct this from known results:

* Consider fully-connected NNs with \(L\geq 3\) layers without biases and the activation function \(\varphi(x)=\max\{0,x\}^{m}\), \(m\in\mathbb{N}_{0}\). Especially, the case \(m=1\) corresponds to the ReLU activation. Vakili et al. (2021) generalize the result by Bietti and Bach (2021) from \(m=1\) to \(m\geq 1\), showing that the NTK-RKHS is equivalent to \(H^{s}(\mathbb{S}^{d})\) for \(s=(d+2m-1)/2\) and the NNGP-RKHS is equivalent to \(H^{s}(\mathbb{S}^{d})\) for \(s=(d+2m+1)/2\). For \(m=0\), Bietti and Bach (2021) essentially show that the NNGP-RKHS is equivalent to \(H^{s}(\mathbb{S}^{d})\) for \(s=(d+2^{2-L})/2\). However, all of the aforementioned result have the problem that the main theorem by Bietti and Bach (2021) allows for the possibility that finitely many \(\mu_{l}\) are zero, which can change the RKHS. Using our Lemma B.2 below, it follows that all \(\mu_{l}\) are in fact nonzero for NNGPs and NTKs since they are kernels in every dimension \(d\) using the same function \(\kappa\) independent of the dimension. Hence, the equivalences to Sobolev spaces stated before are correct.
* Chen and Xu (2021) prove that the RKHS of the NTK corresponding to fully-connected ReLU NNs with zero-initialized biases and \(L\geq 2\) (as opposed to no biases and \(L\geq 3\) above) layers is equivalent to the RKHS of the Laplace kernel on the sphere. Since the Laplace kernel is a Matern kernel of order \(\nu=1/2\) (see e.g. Section 4.2 in Rasmussen and Williams (2005)), we can use Proposition 5.2 of Hubbert et al. (2023) to obtain equivalence to \(H^{s}(\mathbb{S}^{d})\) with \(s=(d+1)/2\). Alternatively, we can obtain the RKHS of the Laplace kernel from Bietti and Bach (2021) combined with Lemma B.2.

Bietti and Bach (2021) also show that under an integrability condition on the derivatives, \(C^{\infty}\) activations induce NTK and NNGP kernels whose RKHSs are smaller than every Sobolev space.

**Lemma B.2** (Guaranteeing non-zero eigenvalues).: _Let \(\kappa:[-1,1]\to\mathbb{R}\), let \(d\geq 1\), and let_

\[k_{d} :\mathbb{S}^{d}\times\mathbb{S}^{d},k_{d}(\bm{x},\bm{x}^{\prime} )\coloneqq\kappa(\langle\bm{x},\bm{x}^{\prime}\rangle)\] \[k_{d+2} :\mathbb{S}^{d+2}\times\mathbb{S}^{d+2},k_{d+2}(\bm{x},\bm{x}^{ \prime})\coloneqq\kappa(\langle\bm{x},\bm{x}^{\prime}\rangle)\;.\]

_Suppose that \(k_{d+2}\) is a kernel. Then, \(k_{d}\) is a kernel. Moreover, if the corresponding eigenvalues \(\mu_{l}\) of \(k_{d}\) satisfy \(\mu_{l}>0\) for infinitely many \(l\), then they satisfy \(\mu_{l}>0\) for all \(l\in\mathbb{N}_{0}\)._

Proof.: The fact that \(k_{d}\) is a kernel follows directly from the inclusion \(\Phi_{d+2}\subseteq\Phi_{d}\) mentioned in Gneiting (2013). For \(D\in\{d,d+2\}\), let \(\mu_{l,d}\) be the sequence of eigenvalues \(\mu_{l}\) associated with \(k_{D}\). Then, as mentioned for example by Hubbert et al. (2023), the Schoenberg coefficients \(b_{l,d}\) satisfy

\[b_{l,d}=\frac{\Gamma\left(\frac{d+1}{2}\right)N_{m,d}\mu_{l,d}}{2\pi^{(d+1)/2}}\;.\]Especially, the Schoenberg coefficients \(b_{l,d}\) have the same sign as the eigenvalues \(\mu_{l,d}\). We use

\[0\leq b_{l,d+2}=\begin{cases}b_{l,d}-\frac{1}{2}b_{l+2,d}&,l=0\text{ and }d=1\\ \frac{1}{2}(l+1)(b_{l,d}-b_{l+2,d})&,l\geq 1\text{ and }d=1\\ \frac{(l+d-1)(l+d)}{d(2l+d-1)}b_{l,d}-\frac{(l+1)(l+2)}{d(2l+d+3)}b_{l+2,d}&,d \geq 2\,\end{cases}\]

where the inequality follows from the fact that \(k_{d+2}\) is a kernel and the equality is the statement of Corollary 3 by Gneiting (2013). In any of the three cases, \(b_{l+2,d}>0\) implies \(b_{l,d}>0\). Hence, if \(b_{l,d}>0\) for infinitely many \(l\), then \(b_{l,d}>0\) for all \(l\), which implies \(\mu_{l,d}>0\) for all \(l\). 

## Appendix C Gradient flow and gradient descent with kernels

### Derivation of gradient flow and gradient descent

Here, we derive expressions for gradient flow and gradient descent in the RKHS for the regularized loss

\[L(f)\coloneqq\frac{1}{n}\sum_{i=1}^{n}(y_{i}-f(\bm{x}_{i}))^{2}+\rho\|f\|_{ \mathcal{H}_{k}}^{2}=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\langle k(\bm{x}_{i}, \cdot),f\rangle_{\mathcal{H}_{k}})^{2}+\rho\langle f,f\rangle_{\mathcal{H}_{k }}^{2}\.\]

Note that we will take derivatives in the RKHS with respect to \(f\), which is different from taking derivatives w.r.t. the coefficients \(\bm{c}\) in a model \(f(\bm{x})=\bm{c}^{\top}k(\bm{X},\bm{x})\).

In the RKHS-Norm, the Frechet derivative of \(L\) is

\[\frac{\partial L(f)}{f}=\frac{2}{n}\sum_{i=1}^{n}(f(\bm{x}_{i})-y_{i})\langle k (\bm{x}_{i},\cdot),\cdot\rangle_{\mathcal{H}_{k}}+2\rho\langle f,\cdot\rangle _{\mathcal{H}_{k}}\,\]

which is represented in \(\mathcal{H}_{k}\) by

\[L^{\prime}(f)=\frac{2}{n}\sum_{i=1}^{n}(f(\bm{x}_{i})-y_{i})k(\bm{x}_{i}, \cdot)+2\rho f\.\]

Now assume that \(f=\sum_{i=1}^{n}a_{i}k(\bm{x}_{i},\cdot)=\bm{a}^{\top}k(\bm{X},\cdot)\). Then,

\[L^{\prime}(f) =\frac{2}{n}\sum_{i=1}^{n}(\bm{a}^{\top}k(\bm{X},\bm{x}_{i})-y_{i })k(\bm{x}_{i},\cdot)+2\rho\bm{a}^{\top}k(\bm{X},\cdot)\] \[=\frac{2}{n}\left((k(\bm{X},\bm{X})+\rho n\bm{I}_{n})\bm{a}-\bm{y }\right)^{\top}k(\bm{X},\cdot)\.\]

Especially, under gradient flow of \(f\), the coefficients \(\bm{a}\) follow the dynamics

\[\dot{\bm{a}}(t)=\frac{2}{n}\left(\bm{y}-(k(\bm{X},\bm{X})+\rho n\bm{I}_{n}) \bm{a}(t)\right)\,\]

which is solved for \(\bm{a}(0)=0\) by

\[\bm{a}(t)=\left(\bm{I}_{n}-e^{-\frac{2}{n}t(k(\bm{X},\bm{X})+\rho n\bm{I}_{n}) }\right)\left(k(\bm{X},\bm{X})+\rho n\bm{I}_{n}\right)^{-1}\bm{y}\,\]

which is the closed form expression (1) of \(f_{t,\rho}\).

For gradient descent, assuming that \(f_{t,\rho}^{\mathrm{GD}}=\bm{c}_{t,\rho}^{\top}k(\bm{X},\cdot)\), we have

\[f_{t+1,\rho}^{\mathrm{GD}} =f_{t,\rho}^{\mathrm{GD}}-\eta_{t}L^{\prime}(f_{t,\rho}^{\mathrm{ GD}})=\bm{c}_{t,\rho}^{\top}k(\bm{X},\cdot)-\eta_{t}\frac{2}{n}\left((k(\bm{X}, \bm{X})+\rho n\bm{I}_{n})\bm{c}_{t,\rho}-\bm{y}\right)^{\top}k(\bm{X},\cdot)\] \[=\left(\bm{c}_{t,\rho}+\eta_{t}\frac{2}{n}\left(\bm{y}-(k(\bm{X}, \bm{X})+\rho n\bm{I}_{n})\bm{c}_{t,\rho}\right)\right)^{\top}k(\bm{X},\cdot)\]

If \(f_{0,\rho}^{\mathrm{GD}}\equiv 0\), the coefficients evolve as \(\bm{c}_{0}=\bm{0}\) and

\[\bm{c}_{t+1,\rho}=\bm{c}_{t,\rho}+\eta_{t}\frac{2}{n}\left(\bm{y}-(k(\bm{X}, \bm{X})+\rho n\bm{I}_{n})\bm{c}_{t,\rho}\right)\.\]

For an analysis of gradient descent for kernel regression with \(\rho=0\), we refer to, e.g., Yao et al. (2007).

Gradient flow and gradient descent initialized at \(0\) have monotonically growing \(\mathcal{H}\)-norm

In the following proposition we show that under gradient flow and gradient descent with sufficiently small learning rates initialized at \(0\), the RKHS norm grows monotonically with time \(t\). This immediately implies that Assumption (N) with \(C_{\mathrm{norm}}=1\) holds for all estimators \(f_{t,\rho}\) from (1).

**Proposition C.1**.:
1. _For any_ \(t\in[0,\infty]\) _and any_ \(\rho\geq 0\)_,_ \(f_{t,\rho}\) _from (_1_) fulfills Assumption (N) with_ \(C_{\mathrm{norm}}=1\)_._
2. _For any_ \(t\in\mathbb{N}_{0}\cup\{\infty\}\) _and any_ \(\rho\geq 0\)_, with sufficiently small fixed learning rate_ \(0\leq\eta\leq\frac{1}{2(\rho+\lambda_{\max}(k(\bm{X},\bm{X}))/n)}\)_,_ \(f_{t,\rho}^{\mathrm{GD}}\) _fulfills Assumption (N) with_ \(C_{\mathrm{norm}}=1\)_._

Proof.: **Proof of (i):**

We write \(f_{t,\rho}(\bm{x})=k(\bm{x},\bm{X})\bm{c}_{t,\rho}\), where \(\bm{c}_{t,\rho}\coloneqq A_{t,\rho}(\bm{X})\bm{y}\). We now show that the RKHS-norm of \(f_{t,\rho}\) grows monotonically in \(t\), by using the eigendecomposition \(k(\bm{X},\bm{X})=\bm{U}\Lambda\bm{U}^{\top}\), where \(\Lambda=\text{diag}(\lambda_{1},\ldots,\lambda_{n})\in\mathbb{R}^{n\times n}\) is diagonal and \(\bm{U}\in\mathbb{R}^{n\times n}\) is orthonormal, and writing \(\tilde{\bm{y}}\coloneqq\bm{U}^{\top}\bm{y}\). Then it holds that

\[\|f_{t,\rho}\|_{\mathcal{H}}^{2} =(\bm{c}_{t,\rho})^{\top}k(\bm{X},\bm{X})\bm{c}_{t,\rho}=\tilde{ \bm{y}}^{\top}(\Lambda+\rho n\bm{I}_{n})^{-1}\left(\bm{I}_{n}-\exp\left(-\frac {2t}{n}(\Lambda+\rho n\bm{I}_{n})\right)\right)\Lambda\cdot\] \[\quad\left(\bm{I}_{n}-\exp\left(-\frac{2t}{n}(\Lambda+\rho n\bm{ I}_{n})\right)\right)(\Lambda+\rho n\bm{I}_{n})^{-1}\tilde{\bm{y}}\] \[=\sum_{k=1,\lambda_{k}+\rho n>0}^{n}\tilde{y}_{k}^{2}\underbrace{ \frac{\lambda_{k}}{(\lambda_{k}+\rho n)^{2}}}_{\leq 1/\lambda_{k}}\underbrace{\left(1- \exp\left(-\frac{2t}{n}(\lambda_{k}+\rho n)\right)\right)}_{\leq 1}\] \[\leq\sum_{k=1,\lambda_{k}>0}^{n}\tilde{y}_{k}^{2}\frac{1}{\lambda_ {k}}=\|f_{\infty,0}\|_{\mathcal{H}}^{2}.\]

**Proof of (ii):**

Expanding the iteration in the definition of \(\bm{c}_{t,\rho}\) yields

\[\bm{c}_{t+1,\rho}=\sum_{i=0}^{t}\prod_{j=0}^{t-i-1}\left(\bm{I}-\frac{2\eta_{t -j}}{n}(k(\bm{X},\bm{X})+\rho n\bm{I})\right)\frac{2\eta_{i}}{n}\bm{y}.\]

We again use the eigendecomposition \(k(\bm{X},\bm{X})=\bm{U}\Lambda\bm{U}^{\top}\), where \(\Lambda=\text{diag}(\lambda_{1},\ldots,\lambda_{n})\in\mathbb{R}^{n\times n}\) is diagonal and \(\bm{U}\in\mathbb{R}^{n\times n}\) is orthonormal, and write \(\tilde{\bm{y}}\coloneqq\bm{U}^{\top}\bm{y}\). Then, using sufficiently small learning rates \(0\leq\eta_{t}\leq\frac{1}{2(\rho+\lambda_{\max}(k(\bm{X},\bm{X}))/n)}\) in all time steps \(t\in\mathbb{N}\), it holds that

\[\|f_{t,\rho}^{\mathrm{GD}}\|_{\mathcal{H}}^{2}\] \[=(\bm{c}_{t,\rho})^{\top}k(\bm{X},\bm{X})\bm{c}_{t,\rho}\] \[=\tilde{\bm{y}}^{\top}\left(\sum_{i=0}^{t}\frac{2\eta_{i}}{n} \prod_{j=0}^{t-i-1}\left((1-2\eta_{t-j}\rho)\bm{I}-\frac{2\eta_{t-j}}{n}\Lambda \right)\right)\Lambda\left(\sum_{i=0}^{t}\frac{2\eta_{i}}{n}\prod_{j=0}^{t-i-1 }\left((1-2\eta_{t-j}\rho)\bm{I}-\frac{2\eta_{t-j}}{n}\Lambda\right)\right) \tilde{\bm{y}}\] \[=\sum_{k=1}^{n}\underbrace{\tilde{y}_{k}^{2}\lambda_{k}}_{\geq 0} \left(\sum_{i=0}^{t}\frac{2\eta_{i}}{n}\prod_{j=0}^{t-i-1}\underbrace{\left(1- 2\eta_{t-j}(\rho+\lambda_{k}/n)\right)}_{\in[0,1]}\right)^{2}.\] (C.1)

The last display shows that \(\|f_{t,\rho}^{\mathrm{GD}}\|_{\mathcal{H}}^{2}\) grows monotonically in \(t\), strictly monotonically if \(\eta_{t}\in(0,\frac{1}{2(\rho+\lambda_{\max}(k(\bm{X},\bm{X}))/n)})\) holds for all \(t\). It also shows that if \(\rho^{\prime}\geq\rho\) then \(\|f_{t,\rho}^{\mathrm{GD}}\|_{\mathcal{H}}\leq\|f_{t,\rho}^{\mathrm{GD}}\|_{ \mathcal{H}}\) for any \(t\in\mathbb{N}\cup\{\infty\}\). To see that \(\|f_{t,\rho}^{\mathrm{GD}}\|_{\mathcal{H}}^{2}\leq\|f_{\infty,0}\|_{\mathcal{ H}}^{2}\) for all \(t\in\mathbb{N}\cup\{\infty\}\) and all \(\rho\geq 0\), observe that with fixed learning rates \(\eta_{t}=\eta\in(0,\frac{1}{2(\rho+\lambda_{\max}(k(\bm{X},\bm{X}))/n)})\subseteq( 0,\frac{1}{2\lambda_{\max}(k(\bm{X},\bm{X}))/n})\), for all \(t\in\mathbb{N}\cup\{\infty\}\) it holds that

\[\sum_{i=0}^{t}\frac{2\eta_{i}}{n}\prod_{j=0}^{t-i-1}\left(1-2\eta_{t-j}\lambda_{ k}/n\right)=\frac{2\eta}{n}\sum_{i=0}^{t}(1-2\eta\lambda_{k}/n)^{t-i}\]\[=\frac{2\eta}{n}\sum_{i=0}^{t}(1-2\eta\lambda_{k}/n)^{i}=\frac{2\eta}{n}\frac{1-(1 -2\eta\lambda_{k}/n)^{t+1}}{2\eta\lambda_{k}/n}\leq\frac{1}{\lambda_{k}}.\]

Since it suffices to consider the case \(\rho\to 0\), using the above derivation in (C.1) yields \(\|f_{t,\rho}^{\mathrm{GD}}\|_{\mathcal{H}}^{2}\leq\|f_{\infty,0}\|_{\mathcal{H} }^{2}\) for all \(t\in\mathbb{N}\), which concludes the proof. 

## Appendix D Proof of Theorem 1

Our goal in this section is to prove Theorem D.1, which can be seen as a generalization of Theorem 1 to varying bandwidths. To be able to speak of bandwidths, we need to consider translation-invariant kernels. Although Theorem 1 is formulated for general kernels with Sobolev RKHS, it follows from Theorem D.1 since we can always find, for a fixed bandwidth, a translation-invariant kernel with equivalent RKHS, such that only the constant \(C_{\mathrm{norm}}\) changes in the theorem statement.

To generate the RKHS \(H^{s}\), Buchholz (2022) uses the translation-invariant kernel \(k^{B}(\bm{x},\bm{y})=u^{B}(\bm{x}-\bm{y})\) defined via its Fourier transform \(\hat{u}^{B}(\bm{\xi})=(1+|\bm{\xi}|^{2})^{-s}\). Adapting the bandwidth, the kernel is then normalized in the usual \(L_{1}\)-sense,

\[k^{B}_{\gamma}(\bm{x},\bm{y})=\gamma^{-d}u^{B}((\bm{x}-\bm{y})/\gamma).\] (D.1)

**Theorem D.1** (Inconsistency of overfitting estimators).: _Let assumptions (D1) and (K) hold. Let \(c_{\mathrm{fit}}\in(0,1]\) and \(C_{\mathrm{norm}}>0\). Then, there exist \(c>0\) and \(n_{0}\in\mathbb{N}\) such that the following holds for all \(n\geq n_{0}\) with probability \(1-O(1/n)\) over the draw of the data set \(D\) with \(n\) samples: For every function \(f\in\mathcal{H}_{k}\) with_

_(O) \(\frac{1}{\|f\|_{\mathcal{H}_{k}}}\sum_{i=1}^{n}(f(x_{i})-y_{i})^{2}\leq(1-c_{ \mathrm{fit}})\cdot\sigma^{2}\) (training error below Bayes risk) and_

_(N) \(\frac{1}{\|f\|_{\mathcal{H}_{k}}}\leq C_{\mathrm{norm}}\|f_{\infty,0}\|_{ \mathcal{H}_{k}}\) (norm comparable to minimum-norm interpolant (1)),_

_the excess risk satisfies_

\[R_{P}(f)-R_{P}(f^{*})\geq c>0\;.\] (D.2)

_If \(k_{\gamma}\) denotes a \(L_{1}\)-normalized translation-invariant kernel with bandwidth \(\gamma>0\), i.e. there exists a \(q:\mathbb{R}^{d}\to\mathbb{R}\) such that \(k_{\gamma}(x,y)=\gamma^{-d}q(\frac{x-y}{\gamma})\), then inequality (D.2) holds with \(c\) independent of the sequence of bandwidths \((\gamma_{n})_{n\in\mathbb{N}}\subseteq(0,1)\), as long as \(f_{D}\) fulfills (N) for the sequence \((\mathcal{H}_{\gamma_{n}})_{n\in\mathbb{N}}\) with constant \(C_{\mathrm{norm}}>0\)._

Proof.: By assumption, the RKHS norm \(\|\cdot\|_{\mathcal{H}_{k}}\) induced by the kernel \(k\) (or \(k_{\gamma}\) if we allow bandwidth adaptation) is equivalent to the RKHS norm \(\|\cdot\|_{\mathcal{H}_{\gamma}}\) induced by a kernel of the form (D.1) with an arbitrary but fixed choice of bandwidth \(\gamma\in(0,1)\), which means that there exists a constant \(C_{\gamma}>0\) such that \(\frac{1}{C_{\gamma}}\|f\|_{\mathcal{H}_{\gamma}}\leq\|f\|_{\mathcal{H}_{k}} \leq C_{\gamma}\|f\|_{\mathcal{H}_{\gamma}}\) for all \(f\in\mathcal{H}_{k}\). Hence the minimum-norm interpolant \(g_{D,\gamma}\) in \(\mathcal{H}_{\gamma}\) satisfies

\[\|f_{D}\|_{\mathcal{H}_{\gamma}}\leq C_{\gamma}\|f_{D}\|_{\mathcal{H}_{k}} \leq C_{\gamma}C_{\mathrm{norm}}\|g_{D}\|_{\mathcal{H}_{k}}\leq C_{\gamma}C_{ \mathrm{norm}}\|g_{D,\gamma}\|_{\mathcal{H}_{k}}\leq C_{\gamma}^{2}C_{\mathrm{ norm}}\|g_{D,\gamma}\|_{\mathcal{H}_{\gamma}},\]

where \(\|g_{D}\|_{\mathcal{H}_{k}}\leq\|g_{D,\gamma}\|_{\mathcal{H}_{k}}\) because, \(g_{D}\) is the minimum-norm interpolant in \(\mathcal{H}_{k}\).

Now consider the RKHS norm \(\|\cdot\|_{\mathcal{H}_{\gamma}}\) of a translation-invariant kernel \(k_{\gamma}\). Then the functions \(\left\{h_{p}(x)=e^{ip\cdot x}\right\}_{p\in\mathbb{R}^{d}}\) are eigenfunctions of the kernel's integral operator, so that the RKHS norm can be written as (Rakhlin and Zhai, 2019)

\[\|f\|_{\mathcal{H}_{\gamma}}^{2}=\frac{1}{(2\pi)^{d}}\int_{\mathbb{R}^{d}} \frac{|\hat{f}(\omega)|^{2}}{\hat{q}(\omega)}d\omega,\]

where \(\hat{f}\) denotes the Fourier transform of \(f\).

By assumption we know that there exists a \(C_{\gamma_{0}}>0\) such that \(\frac{1}{C_{\gamma_{0}}}\|f\|_{\mathcal{H}_{\gamma_{0}}}\leq\|f\|_{\mathcal{H} _{\gamma_{0}}}\leq C_{\gamma_{0}}\|f\|_{\mathcal{H}_{\gamma_{0}}}\) holds for some fixed bandwidth \(\gamma_{0}>0\), then substituting by \(\tilde{\omega}=\frac{\gamma}{\gamma_{0}}\omega\) yields

\[\|f\|_{\mathcal{H}_{\gamma}}=\frac{1}{(2\pi)^{d}}\int_{\mathbb{R}^{d}}\frac{| \hat{f}(\omega)|^{2}}{\hat{q}_{1}(\gamma\omega)}d\omega=\frac{1}{(2\pi)^{d}} \int_{\mathbb{R}^{d}}\frac{|\hat{f}(\frac{\gamma}{\gamma_{0}}\tilde{\omega})|^{2 }}{\hat{q}_{1}(\gamma_{0}\tilde{\omega})}\left(\frac{\gamma_{0}}{\gamma}\right) ^{d}d\tilde{\omega}=\|\hat{f}\|_{\mathcal{H}_{\gamma_{0}}}\]\[\leq C_{\gamma_{0}}\|\tilde{f}\|_{\mathcal{H}_{\gamma_{0}}}=\frac{C_{\gamma_{0}}}{(2 \pi)^{d}}\int_{R^{d}}\frac{|\tilde{f}(\frac{\gamma_{0}}{\gamma}\tilde{\omega})|^{ 2}}{\hat{q}_{2}(\gamma_{0}\tilde{\omega})}\left(\frac{\gamma_{0}}{\gamma} \right)^{d}d\tilde{\omega}=\frac{C_{\gamma_{0}}}{(2\pi)^{d}}\int_{R^{d}}\frac{ |\tilde{f}(\omega)|^{2}}{\hat{q}_{2}(\gamma\omega)}d\omega=C_{\gamma_{0}}\|f \|_{\mathcal{H}_{\gamma}}.\]

In the same way we get \(\frac{1}{C_{\gamma_{0}}}\|f\|_{\mathcal{H}_{\gamma}}\leq\|f\|_{\tilde{ \mathcal{H}}_{\gamma}}\) for arbitrary \(\gamma\in(0,1)\). This shows that the constant \(C_{\gamma_{0}}\), that quantifies the equivalence between \(\|\cdot\|_{\mathcal{H}_{\gamma}}\) and \(\|\cdot\|_{\tilde{\mathcal{H}}_{\gamma}}\) does not depend on the bandwidth \(\gamma\). Finally Proposition D.4, Proposition D.2 and Remark D.3 together yield the result. 

The following proposition generalizes the inconsistency result for large bandwidths, Proposition 4 in Buchholz (2022), beyond interpolating estimators to estimators that overfit at least an arbitrary constant fraction beyond the Bayes risk and whose RKHS norm is at most a constant factor larger than the RKHS norm of the minimum-norm interpolant. Compared to Rakhlin and Zhai (2019), Buchholz gets a statement in probability over the draw of a training set \(D\) and less restrictive assumptions on the domain \(\Omega\) and dimension \(d\).

**Proposition D.2** (**Inconsistency for large bandwidths**).: _Let \(c_{\mathrm{fit}}\in(0,1]\) and \(C_{\mathrm{norm}}>0\). Let the data set \(D=\{(\bm{x}_{1},y_{1}),\ldots,(\bm{x}_{n},y_{n})\}\) be drawn i.i.d. from a distribution \(P\) that fulfills Assumption (D1), let \(g_{D,\gamma}\) be the minimum-norm interpolant in \(\mathcal{H}:=\mathcal{H}_{\gamma}\) with respect to the kernel (D.1) for a bandwidth \(\gamma>0\). Then, for every \(A>0\), there exist \(c>0\) and \(n_{0}\in\mathbb{N}\) such that the following holds for all \(n\geq n_{0}\) with probability \(1-O(1/n)\) over the draw of the data set \(D\) with \(n\) samples:_

_For every function \(f\in\mathcal{H}\) that fulfills Assumption (O) with \(c_{\mathrm{fit}}\) and Assumption (N) with \(C_{\mathrm{norm}}\) the excess risk satisfies_

\[\mathbb{E}_{\bm{x}}(f(\bm{x})-f^{*}(\bm{x}))^{2}\geq c>0,\]

_where \(c\) depends neither on \(n\) nor on \(1>\gamma>An^{-1/d}>0\)._

**Remark D.3**.: Proposition D.2 holds for any kernel that fulfills Assumption (K). The reason is that any kernel \(k\) that fulfills assumption (K) and the kernel defined in (D.1) have the same RKHS and equivalent norms. Therefore every function \(f\in\mathcal{H}_{k}=\mathcal{H}_{\gamma}\) (equality as sets) that fulfills Assumptions (O) and (N) for the kernel \(k\) also fulfills Assumptions (O) and (N) with an adapted constant \(C_{\mathrm{norm}}\) for the kernel (D.1). 

Proof.: **Step 1: Generalizing the procedure in Buchholz (2022).**

We write \([n]=\{1,\ldots,n\}\) and follow the proof of Proposition 4 in Buchholz (2022). Define \(u(\bm{x})=f(\bm{x})-f^{*}(\bm{x})\). We need to show that with probability at least \(1-O(n^{-1})\) over the draw of \(D\) it holds that \(\|u\|_{L^{2}(P_{X})}\geq c>0\), where \(c\) depends neither on \(n\) nor on \(\gamma\).

For this purpose we show that with probability at least \(1-3n^{-1}\) over the draw of \(D\) there exist a constants \(c^{\prime\prime},\kappa^{\prime\prime}>0\) depending only on \(c_{\mathrm{fit}}\) and a subset \(\mathcal{P}^{\prime\prime}\subseteq[n]\) with \(|\mathcal{P}^{\prime\prime}|\geq\lfloor\kappa^{\prime\prime}\cdot n\rfloor\) such that

\[|f(\bm{x}_{i})-f^{*}(\bm{x}_{i})|\geq c^{\prime\prime}>0\text{ holds for all }i\in\mathcal{P}^{\prime\prime}.\] (D.3)

Then via Lemma 7 in Buchholz (2022) as well as Lemma D.7 we can choose a large subset \(\mathcal{P}^{\prime\prime\prime}\subseteq[n]\) of the training point indices with \(|\mathcal{P}^{\prime\prime\prime}|\geq n-|\mathcal{P}^{\prime\prime}|/2\), such that the \(\bm{x}_{i}\) for \(i\in\mathcal{P}^{\prime\prime\prime}\) are well-separated in the sense that \(\min_{\{i,j\in\mathcal{P}^{\prime\prime\prime},\,i\neq j\}}\|\bm{x}_{i}-\bm{x}_ {j}\|\geq d_{min}\) with \(d_{min}:=c^{\prime\prime\prime}n^{-1/d}\), where \(c^{\prime\prime\prime}\) depends on \(c_{\mathrm{fit}},d\), the upper bound on the Lebesgue density \(C_{u}\) and on the smoothness of the RKHS \(s\). Then the intersection \(\mathcal{P}^{\prime\prime}\cap\mathcal{P}^{\prime\prime\prime}\) contains at least \(\frac{|\mathcal{P}^{\prime\prime}|}{2}\) points. Now we can replace \(\mathcal{P}^{\prime}\) in the proof of Proposition 4 for \(s\in\mathbb{N}\) in Buchholz (2022) by the intersection \(\mathcal{P}^{\prime\prime}\cap\mathcal{P}^{\prime\prime\prime}\). The rest of the proof applies without modification, where (42) holds by our assumption \(\|f\|_{\mathcal{H}}\leq C_{\mathcal{H}}\|g_{D}\|_{\mathcal{H}}\). Our modifications do not affect Buchholz' arguments for the extension to \(s\notin\mathbb{N}\).

**Step 2: The existence of \(\mathcal{P}^{\prime\prime}\).**

Given a choice of \(\kappa^{\prime\prime},c^{\prime\prime}>0\), consider the event (over the draw of \(D\))

\[E \coloneqq\{\nexists\ \mathcal{P}^{\prime\prime}\subseteq[n] \text{ with }|\mathcal{P}^{\prime\prime}|\geq\lfloor\kappa^{\prime\prime}\cdot n \rfloor\text{ that fulfills (D.3)}\}\] \[=\{\exists\ \tilde{\mathcal{P}}\subseteq[n] \text{ with }|\tilde{\mathcal{P}}|\geq\lceil(1-\kappa^{\prime\prime})n\rceil \text{ such that }|f^{*}(\bm{x}_{i})-f(\bm{x}_{i})|<c^{\prime\prime}\quad\forall i\in\tilde{ \mathcal{P}}\}.\]

With the proper choices of \(c^{\prime\prime}\) and \(\kappa^{\prime\prime}\) independent of \(n\) and \(f\), we will show \(P(E)\leq 3n^{-1}\). We will find a small \(c^{\prime\prime}>0\) such that if \(f^{*}\) and \(f\) are closer than \(c^{\prime\prime}\) on too many training points \(\tilde{\mathcal{P}}\)and \(f\) overfits by at least the fraction \(c_{\tilde{\mathrm{ft}}}\), the noise variables \(\varepsilon_{i}\) on the complement \(\tilde{\mathcal{P}}^{c}\) would have to be unreasonably large, contradicting the event \(E_{6i}\) defined below, and implying (D.3) with high probability. We will use the notation \(\|\bm{f}\|_{\mathcal{P}}^{2}\coloneqq\sum_{i\in\mathcal{P}}f(\bm{x}_{i})^{2}\) and \(\|\bm{y}\|_{\mathcal{P}}^{2}\coloneqq\sum_{i\in\mathcal{P}}y_{i}^{2}\).

**Step 2b: Noise bounds.**

Lemma D.6\((i)\) states that there exists a \(\kappa^{\prime\prime}>0\) small enough such that the event (over the draw of \(D\))

\[E_{6i}\coloneqq\{\forall\mathcal{P}_{1}\subseteq[n]\text{ with }|\mathcal{P}_{1}|\leq\lfloor\kappa^{\prime\prime}\cdot n \rfloor\text{ it holds that }\frac{1}{n}\|\bm{f}^{*}-\bm{y}\|_{\mathcal{P}_{1}}^{2}=\frac{1}{n}\sum_{i \in\mathcal{P}_{1}}\varepsilon_{i}^{2}<\frac{c_{\tilde{\mathrm{ft}}}}{4} \sigma^{2}\},\]

fulfills, for \(n\) large enough, \(P(E_{6i})\geq 1-n^{-1}\).

Lemma D.6\((ii)\) implies that there exists a \(c_{\mathrm{lower}}>0\) such that the event (over the draw of \(D\))

\[E_{6ii}\coloneqq\{\forall\mathcal{P}_{2}\text{ with }|\mathcal{P}_{2}|\geq \lfloor(1-\kappa^{\prime\prime})n\rfloor\text{ it holds that }\frac{1}{n}\|\bm{f}^{*}-\bm{y}\|_{\mathcal{P}_{2}}^{2}\geq c_{\mathrm{lower}} \cdot\sigma^{2}\},\]

fulfills, for \(n\) large enough, \(P(E_{6ii})\geq 1-n^{-1}\).

Lemma D.5 states that the total amount of noise \(\|\bm{\varepsilon}\|_{[n]}^{2}\) concentrates around its mean \(n\sigma^{2}\). More precisely, we will use that for any \(c_{\varepsilon}\in(0,1)\) the event (over the draw of \(D\))

\[E_{5}\coloneqq\left\{\frac{1}{n}\|\bm{f}^{*}-\bm{y}\|_{[n]}^{2}\geq c_{ \varepsilon}\cdot\sigma^{2}\right\},\]

fulfills \(P(E_{5})\geq 1-\exp\left(-n\cdot\left(\frac{1-c_{\varepsilon}}{2}\right)^{2}\right)\).

**Step 2c: Lower bounding \(\|\bm{\varepsilon}\|_{\tilde{\mathcal{P}}^{c}}^{2}\).**

Given some function \(f\in\mathcal{H}\), assume in steps 2c and 2d that event \(E\) holds and that \(\tilde{\mathcal{P}}\subseteq[n]\) denotes a subset of the training set that fulfills \(|\tilde{\mathcal{P}}|\geq\lceil(1-\kappa^{\prime\prime})n\rceil\) and \(|f^{*}(\bm{x}_{i})-f(\bm{x}_{i})|<c^{\prime\prime}\quad\forall i\in\tilde{ \mathcal{P}}\).

In step 2c, assume we choose \(\tilde{c}_{\tilde{\mathrm{ft}}}>0\) such that \(\tilde{c}_{\tilde{\mathrm{ft}}}\|\bm{f}^{*}-\bm{y}\|_{\tilde{\mathcal{P}}}^{2 }\leq\|\bm{f}-\bm{y}\|_{\tilde{\mathcal{P}}}^{2}\). Then by the overfitting Assumption (O) it holds that

\[\frac{1}{n}\left(\tilde{c}_{\tilde{\mathrm{ft}}}\|\bm{f}^{*}-\bm{y}\|_{\tilde{ \mathcal{P}}}^{2}+\|\bm{f}-\bm{y}\|_{\tilde{\mathcal{P}}^{c}}^{2}\right) \leq\frac{1}{n}\left(\|\bm{f}-\bm{y}\|_{\tilde{\mathcal{P}}}^{2}+\|\bm{f}-\bm {y}\|_{\tilde{\mathcal{P}}^{c}}^{2}\right)\leq(1-c_{\tilde{\mathrm{ft}}}) \sigma^{2}.\] (D.4)

If we restrict ourselves to event \(E_{5}\), dropping the term \(\|\bm{f}-\bm{y}\|_{\tilde{\mathcal{P}}^{c}}^{2}\) in (D.4), then dividing by \(\tilde{c}_{\tilde{\mathrm{ft}}}\) and subtracting the result from the inequality in the definition of event \(E_{5}\) yields

\[\frac{1}{n}\|\bm{\varepsilon}\|_{\tilde{\mathcal{P}}^{c}}^{2}=\frac{1}{n}\|\bm{ f}^{*}-\bm{y}\|_{\tilde{\mathcal{P}}^{c}}^{2}\geq c_{\varepsilon}\sigma^{2}- \frac{1-c_{\tilde{\mathrm{ft}}}}{\tilde{c}_{\tilde{\mathrm{ft}}}}\sigma^{2}.\] (D.5)

**Step 2d: Choosing the constants.**

If we choose \(c_{\varepsilon}\coloneqq 1-\frac{c_{\tilde{\mathrm{ft}}}}{4}\) and \(\tilde{c}_{\tilde{\mathrm{ft}}}\coloneqq\frac{2-2c_{\tilde{\mathrm{ft}}}}{2- c_{\tilde{\mathrm{ft}}}}\in(0,1)\), then (D.5) becomes

\[\frac{1}{n}\|\bm{\varepsilon}\|_{\tilde{\mathcal{P}}^{c}}^{2}\geq\frac{c_{ \tilde{\mathrm{ft}}}}{4}\sigma^{2}.\]

Now it is left to show that the condition \(\tilde{c}_{\tilde{\mathrm{ft}}}\|\bm{f}^{*}-\bm{y}\|_{\tilde{\mathcal{P}}}^{2} \leq\|\bm{f}-\bm{y}\|_{\tilde{\mathcal{P}}}^{2}\), that is required for Step 2c, holds with high probability with our choice of \(\tilde{c}_{\tilde{\mathrm{ft}}}\).

With some arbitrary but fixed \(\varepsilon_{\mathrm{lower}}\in(0,\sqrt{c_{\mathrm{lower}}})\), choose \(c^{\prime\prime}\coloneqq(1-\sqrt{\tilde{c}_{\tilde{\mathrm{ft}}}})(\sqrt{ \frac{c_{\mathrm{lower}}}{1-\kappa^{\prime\prime}}}-\frac{c_{\mathrm{lower}}}{ \sqrt{1-\kappa^{\prime\prime}}})\sigma\). Then on event \(E_{6ii}\), for \(n\) large enough, it holds that

\[(1-\sqrt{\tilde{c}_{\tilde{\mathrm{ft}}}})\frac{1}{\sqrt{n}}\|\bm{f}^{*}-\bm{y} \|_{\tilde{\mathcal{P}}}\geq(1-\sqrt{\tilde{c}_{\tilde{\mathrm{ft}}}})\sqrt{c_{ \mathrm{lower}}}\sigma\geq\sqrt{1-\kappa^{\prime\prime}}\cdot c^{\prime\prime}+ \frac{c^{\prime\prime}}{\sqrt{n}}.\] (D.6)

By definition of \(\tilde{\mathcal{P}}\), it holds that

\[\|\bm{f}-\bm{f}^{*}\|_{\tilde{\mathcal{P}}}^{2}=\sum_{i\in\tilde{\mathcal{P}}}(f( \bm{x}_{i})-f^{*}(\bm{x}_{i}))^{2}<\lceil(1-\kappa^{\prime\prime})n\rceil(c^{ \prime\prime})^{2},\]so that

\[\frac{1}{\sqrt{n}}\|\bm{f}-\bm{f}^{*}\|_{\widehat{\mathcal{P}}}<\sqrt{1-\kappa^{ \prime\prime}}\cdot c^{\prime\prime}+\frac{c^{\prime\prime}}{\sqrt{n}}.\] (D.7)

Now, using the triangle inequality, (D.7) and (D.6) yields the condition required for Step 2c,

\[\frac{1}{\sqrt{n}}\|\bm{f}-\bm{y}\|_{\widehat{\mathcal{P}}}\] \[\geq \frac{1}{\sqrt{n}}\|\bm{f}^{*}-\bm{y}\|_{\widehat{\mathcal{P}}}- \frac{1}{\sqrt{n}}\|\bm{f}-\bm{f}^{*}\|_{\widehat{\mathcal{P}}}\] \[\geq \frac{1}{\sqrt{n}}\|\bm{f}^{*}-\bm{y}\|_{\widehat{\mathcal{P}}}- \sqrt{1-\kappa^{\prime\prime}}\cdot c^{\prime\prime}-\frac{c^{\prime\prime}}{ \sqrt{n}}\] \[\geq \sqrt{\widehat{c}_{\mathrm{fit}}}\frac{1}{\sqrt{n}}\|\bm{f}^{*}- \bm{y}\|_{\widehat{\mathcal{P}}}.\]

**Step 2e: Upper bounding the probability of \(E\).**

To conclude, we have seen in steps 2c and 2d that on \(E\cap E_{6ii}\cap E_{5}\), it holds that

\[\frac{1}{n}\|\bm{\varepsilon}\|_{\widehat{\mathcal{P}}_{c}}^{2}\geq\frac{c_{ \mathrm{fit}}}{4}\sigma^{2}.\]

On \(E_{6i}\), it holds that

\[\frac{1}{n}\|\bm{\varepsilon}\|_{\widehat{\mathcal{P}}_{c}}^{2}<\frac{c_{ \mathrm{fit}}}{4}\sigma^{2}.\]

Hence \(E_{6i}\cap E\cap E_{6ii}\cap E_{5}=\emptyset\). This implies \(E\subseteq(E_{5}\cap E_{6i}\cap E_{6ii})^{c}\), where the right hand side is independent of \(f\in\mathcal{H}\) and just depends on the training data \(D\). Since \(P(E_{6i})\geq 1-n^{-1}\) and \(P(E_{6ii}\cap E_{5})\geq 1-n^{-1}-\exp\left(-n\cdot\left(\frac{1-c_{c}}{2} \right)^{2}\right)\), it must hold that, for \(n\) large enough,

\[P(E)\leq P((E_{5}\cap E_{6i}\cap E_{6ii})^{c})\leq 2n^{-1}+\exp\left(-n \cdot\left(\frac{1-c_{\varepsilon}}{2}\right)^{2}\right)\leq 3n^{-1}.\qed\]

The following proposition generalizes the inconsistency result for small bandwidths, Proposition 5 in Buchholz (2022), beyond interpolating estimators to estimators whose RKHS norm is at most a constant factor larger than the RKHS norm of the minimum-norm interpolant. The intuition is that if the bandwidth is too small, then the minimum-norm interpolant \(g_{D,\gamma}\) returns to \(0\) between the training points. Then \(\|g_{D,\gamma}\|_{L_{2}(\rho)}\) is smaller and bounded away from \(\|f^{*}\|_{L_{2}(\rho)}\). We can replace \(g_{D,\gamma}\) by any other function \(f\in\mathcal{H}\) that fulfills Assumption (N).

**Proposition D.4** (**Inconsistency for small bandwidths**).: _Under the assumptions of Proposition D.2, there exist constants \(B,c>0\) such that, with probability \(1-O(n^{-1})\) over the draw of \(D\): For any function \(f\in\mathcal{H}\) that fulfills Assumption (N) but not necessarily Assumption (O), the excess risk satisfies_

\[\mathbb{E}_{\bm{x}}(f(\bm{x})-f^{*}(\bm{x}))^{2}\geq c>0,\]

_where \(c\) depends neither on \(n\) nor on \(\gamma<Bn^{-1/d}\)._

Proof.: Denote the upper bound on the Lebesgue density of \(P_{X}\) by \(C_{u}\). The triangle inequality implies

\[\|f^{*}-f\|_{L_{2}(P_{X})} \geq\|f^{*}\|_{L_{2}(P_{X})}-\|f\|_{L_{2}(P_{X})}\geq\|f^{*}\|_{L_ {2}(P_{X})}-\sqrt{C_{u}}\|f\|_{2}\] \[\geq\|f^{*}\|_{L_{2}(P_{X})}-\sqrt{C_{u}}\|f\|_{\mathcal{H}}\geq \|f^{*}\|_{L_{2}(P_{X})}-C_{\mathcal{H}}\sqrt{C_{u}}\|g_{D,\gamma}\|_{ \mathcal{H}},\]

where \(\|f\|_{2}\leq\|f\|_{\mathcal{H}}\) follows from the fact that the Fourier transform \(\hat{k}\) of the kernel satisfies \(\hat{k}(\xi)\leq 1\). Now in the proof of Lemma 17 in Buchholz (2022)\(a>0\) can be chosen smaller to generalize the statement to

\[\|g_{D,\gamma}\|_{\mathcal{H}}^{2}\leq\frac{1}{6C_{\mathcal{H}}^{2}C_{u}}\|f^{ *}\|_{L_{2}(P_{X})}^{2}+c_{9}(\gamma^{2}n^{2/d}+\gamma^{2s}n^{2s/d}),\]

where \(c_{9}\) depends on \(c_{u},f^{*},d,s\) and \(C_{\mathrm{norm}}\). Finally we can choose \(B\) small enough such that Eq. (32) in Buchholz (2022) can be replaced by \(C_{\mathcal{H}}\sqrt{C_{u}}\|g_{D,\gamma}\|_{\mathcal{H}}\leq\frac{2}{3}\|f^{*} \|_{L_{2}(P_{X})}\) so that we get

\[\|f^{*}-f\|_{L_{2}(P_{X})}\geq\frac{1}{3}\|f^{*}\|_{L_{2}(P_{X})}>0.\qed\]

### Auxiliary results for the proof of Theorem 1

**Lemma D.5** (Concentration of \(\chi^{2}_{n}\) variables).: _Let \(U\) be a chi-squared distributed random variable with \(n\) degrees of freedom. Then, for any \(c\in(0,1)\) it holds that_

\[P\left(\frac{U}{n}\leq c\right)\leq\exp\left(-n\cdot\left(\frac{1-c}{2}\right)^ {2}\right).\]

Proof.: Lemma 1 in Laurent and Massart (2000) implies for any \(x>0\),

\[P\left(\frac{U}{n}\leq 1-2\sqrt{\frac{x}{n}}\right)\leq\exp\left(-x\right).\]

Solving \(c=1-2\sqrt{\frac{x}{n}}\) for \(x\) yields \(x=n\cdot\left(\frac{1-c}{2}\right)^{2}\). 

**Lemma D.6**.: _Let \(\varepsilon_{1},\ldots,\varepsilon_{n}\) be i.i.d. \(\mathcal{N}(0,\sigma^{2})\) random variables, \(\sigma^{2}>0\). Let \((\varepsilon^{2})^{(i)}\) denote the \(i\)-th largest of \(\varepsilon_{1}^{2},\ldots,\varepsilon_{n}^{2}\)._

1. _[label=()]_
2. _A constant fraction of noise cannot concentrate on less than_ \(\Theta(n)\) _points:_ _For all constants_ \(\alpha,c>0\) _there exists a constant_ \(C\in(0,1)\) _such that with probability at least_ \(1-n^{-\alpha}\)_, for_ \(n\) _large enough,_ \[\frac{1}{n}\sum_{i=1}^{\lfloor Cn\rfloor}(\varepsilon^{2})^{(i)}<c\sigma^{2}\;.\]
3. \(\Theta(n)\) _points amount to a constant fraction of noise:_ _For all constants_ \(\alpha>0\) _and_ \(\kappa\in(0,1)\) _there exists a constant_ \(c>0\) _such that with probability at least_ \(1-n^{-\alpha}\)_, for_ \(n\) _large enough,_ \[\frac{1}{n}\sum_{i=1}^{\lfloor(1-\kappa)n\rfloor}(\varepsilon^{2})^{(n-i+1)} \geq c\sigma^{2}\;.\]

Proof.: Without loss of generality, we can assume \(\sigma^{2}=1\).

1. [label=()]
2. For a constant \(C\in(0,1)\) yet to be chosen, consider the sum \[S_{C,n}\coloneqq\frac{1}{n}\sum_{i=1}^{\lfloor Cn\rfloor}(\varepsilon^{2})^{(i )}\;.\] For \(T>0\) yet to be chosen, we consider the random set \(\mathcal{I}_{T}\coloneqq\{i\in[n]\mid\varepsilon_{i}^{2}>T\}\) and denote its size by \(K\coloneqq|\mathcal{I}_{T}|\). To bound \(K\), we note that \(K=\xi_{1}+\ldots+\xi_{n}\), where \(\xi_{i}=\mathds{1}_{\varepsilon_{i}^{2}>T}\). We first want to bound \(p_{T}\coloneqq\mathbb{E}\xi_{i}=P(\varepsilon_{i}^{2}>T)\). The random variables \(\varepsilon_{i}^{2}\) follow a \(\chi^{2}_{1}\)-distribution, whose CDF we denote by \(F(t)\) and whose PDF is \[f(t)=\mathds{1}_{(0,\infty)}(t)C_{1}t^{-1/2}\exp(-t/2)\] (D.8) for some absolute constant \(C_{1}\). Moreover, we use \(\Phi\) and \(\phi\) to denote the CDF and PDF of \(\mathcal{N}(0,1)\), respectively.
3. **Step 1: Tail bounds.** Following Duembgen (2010), we have for \(x>0\): \[1-\Phi(x) >\frac{2\phi(x)}{\sqrt{4+x^{2}}+x}\geq\frac{2\phi(x)}{2+x+x}=\frac {\phi(x)}{1+x}\] \[1-\Phi(x) <\frac{2\phi(x)}{\sqrt{2+x^{2}}+x}\leq\frac{2\phi(x)}{1+x}\;.\] Hence, for \(t>0\), we have \[1-F(t) =2(1-\Phi(\sqrt{t}))>\frac{2\phi(\sqrt{t})}{1+\sqrt{t}}=\sqrt{ \frac{2}{\pi}}\frac{\exp(-t/2)}{1+\sqrt{t}}\] \[1-F(t) =2(1-\Phi(\sqrt{t}))<\frac{4\phi(\sqrt{t})}{1+\sqrt{t}}=\sqrt{ \frac{8}{\pi}}\frac{\exp(-t/2)}{1+\sqrt{t}}\;.\]By choosing \(T\coloneqq-2\log(C\sqrt{\pi/32})>0\), we obtain

\[p_{T}=1-F(T)<\sqrt{\frac{8}{\pi}}\exp(-T/2)=C/2\;.\]

**Step 2: Bounding \(K\).** The random variables \(\xi_{i}\) from above satisfy \(\xi_{i}\in[0,1]\). By Hoeffding's inequality (Steinwart and Christmann, 2008, Theorem 6.10), we have for \(\tau>0\)

\[P\left(\frac{1}{n}\sum_{i=1}^{n}(\xi_{i}-\mathbb{E}\xi_{i})\geq(1-0)\sqrt{ \frac{\tau}{2n}}\right)\leq\exp(-\tau)\;.\]

We choose \(\tau\coloneqq C^{2}n/2\), such that with probability \(\geq 1-\exp(-C^{2}n/2)\), we have

\[K/n-p_{T}=\frac{1}{n}\sum_{i=1}^{n}(\xi_{i}-\mathbb{E}\xi_{i})\leq\sqrt{\frac {C^{2}n/2}{2n}}=C/2\;.\]

Suppose that this holds. Then, \(K\leq np_{T}+Cn/2<Cn\) and, since \(K\) is an integer, \(K\leq\lfloor Cn\rfloor\). This implies

\[S_{C,n}\leq\frac{1}{n}\left(\sum_{i=1}^{K}(\varepsilon^{2})^{(i)}+(\lfloor Cn \rfloor-K)T\right)\leq CT+\frac{1}{n}\sum_{i=1}^{K}(\varepsilon^{2})^{(i)}\;.\] (D.9)

We now want to bound \(\sum_{i=1}^{K}(\varepsilon^{2})^{(i)}\). To this end, we note that conditioned on \(K=k\) for some \(k\in[n]\), the \(k\) random variables \((\varepsilon_{i})_{i\in T_{T}}\) are i.i.d. drawn from the distribution of \(\varepsilon^{2}\) given \(\varepsilon^{2}>T\), for \(\varepsilon\sim\mathcal{N}(0,1)\). By \(X,X_{1},X_{2},\ldots\), we denote i.i.d. random variables drawn from the distribution of \(\varepsilon^{2}-T\mid\varepsilon^{2}>T\). This means that conditioned on \(K=k\),

\[\sum_{i=1}^{k}(\varepsilon^{2})^{(i)}=\sum_{i\in T_{T}}\varepsilon_{i}^{2}\quad \text{is distributed as}\quad kT+\sum_{i=1}^{k}X_{i}\;.\] (D.10)

**Step 3: Conditional expectation.** The density of \(X\) is given by

\[p_{X}(t) =\mathds{1}_{t>0}\frac{f(T+t)}{1-F(T)}\overset{\text{(\ref{eq: 1})}}{\leq}\mathds{1}_{t>0}\frac{C_{1}(T+t)^{-1/2}\exp(-(t+T)/2)}{\sqrt{2/\pi} \exp(-T/2)/(1+\sqrt{T})}\] \[\leq\mathds{1}_{t>0}C_{2}\exp(-t/2)\;,\]

where we have used that for \(t>0\),

\[\frac{1+\sqrt{T}}{\sqrt{T+t}}\leq\frac{1+\sqrt{T}}{\sqrt{T}}=1+\frac{1}{\sqrt {T}}\leq 2\]

since \(T=-2\log(C\sqrt{\pi/32})\geq-2\log(\sqrt{\pi/32})\approx 1.008\). We can now bound

\[\mathbb{E}[X] =\int_{0}^{\infty}tp_{X}(t)\,\mathrm{d}t\] \[\leq\int_{0}^{\infty}C_{2}t\exp(-t/2)\,\mathrm{d}t=4C_{2}\;.\] (D.11)

**Step 4: Conditional subgaussian norm.** For \(t\geq 0\),

\[P(|X|>t) =P(X>t)=\frac{1-F(T+t)}{1-F(T)}\leq 2\frac{1+\sqrt{T}}{1+\sqrt{T+t} }\frac{\exp(-(T+t)/2)}{\exp(-T/2)}\] \[\leq 2\exp(-t/2)\;.\]

Since the denominator \(2\) in \(2\exp(-t/2)\) is constant, by Proposition 2.7.1 and Definition 2.7.5 in Vershynin (2018), the subexponential norm \(\|X\|_{\psi_{1}}\) is therefore bounded by an absolute constant \(C_{3}\). Moreover, by Exceries 2.7.10 in Vershynin (2018), we have \(\|X-\mathbb{E}X\|_{\psi_{1}}\leq C_{4}\|X\|_{\psi_{1}}\leq C_{5}\) for absolute constants \(C_{4},C_{5}\).

**Step 5: Conditional Concentration.** Now, Bernstein's inequality for subexponential random variables (Vershynin, 2018, Corollary 2.8.1) yields for \(t\geq 0\) and some absolute constant \(C_{6}>0\):

\[P\left(\left|\sum_{i=1}^{k}X_{i}-\mathbb{E}X_{i}\right|\geq t\right)\leq 2 \exp\left(-C_{6}\min\left(\frac{t^{2}}{kC_{5}^{2}},\frac{t}{C_{5}}\right) \right)\;.\] (D.12)We choose \(t=C_{5}Cn\) and obtain for \(k\leq Cn\)

\[P\left(\sum_{i=1}^{k}(\varepsilon^{2})^{(i)}\geq kT+4C_{2}k+C_{5}Cn \;\middle|\;K=k\right)\] \[\stackrel{{\text{(D,\ref{eq:P1})}}}{{=}}P\left(\sum_{i =1}^{k}X_{i}\geq 4C_{2}k+C_{5}Cn\;\middle|\;K=k\right)\] \[\stackrel{{\text{(D,\ref{eq:P1})}}}{{\leq}}P\left( \left|\sum_{i=1}^{k}X_{i}-\mathbb{E}X_{i}\right|\geq t\right)\] \[\stackrel{{\text{(D,\ref{eq:P1})}}}{{\leq}}2\exp\left(- C_{6}Cn\right)\;.\]

**Step 6: Final bound.** From Step 2, we know that \(K\leq\lfloor Cn\rfloor\) with probability \(\geq 1-\exp(-C^{2}n/2)\). Moreover, in this case, Step 5 yields

\[\sum_{i=1}^{K}(\varepsilon^{2})^{(i)}<KT+4C_{2}K+C_{5}Cn\leq Cn(T+4C_{2}+C_{5})\]

with probability \(\geq 1-\exp(-C_{6}Cn)\). By Eq. (D.9), we therefore have

\[S_{C,n}<CT+C(T+4C_{2}+C_{5})=-4C\log(C\sqrt{\pi/32})+C_{7}C\;.\]

Since \(\lim_{C\searrow 0}-C\log(C)=0\), we can choose \(C\in(0,1)\) such that \(-4C\log(C\sqrt{\pi/32})+C_{7}C<c\) for the given constant \(c>0\) from the theorem statement, and obtain the desired bound with high probability in \(n\).
2. Since the \(\varepsilon_{i}^{2}\) are non-negative and their distribution has a density, there must exist \(T>0\) with \(P(\varepsilon_{i}^{2}<T)\leq(1-\kappa)/4\). Similar to the proof of (i), we then want to bound \(K\coloneqq|\{i\in[n]\;|\;\varepsilon_{i}^{2}<T\}|=\xi_{1}+\ldots+\xi_{i}\) with \(\xi_{i}=\mathds{1}_{\varepsilon_{i}^{2}<T}\). The \(\xi_{i}\in[0,1]\) are independent with \(\mathbb{E}\xi_{i}=P(\varepsilon_{i}^{2}<T)\leq(1-\kappa)/4\). As in Step 2 of (i), Hoeffding's inequality then yields for \(\tau>0\): \[P\left(\frac{1}{n}\sum_{i=1}^{n}(\xi_{i}-\mathbb{E}\xi_{i})\geq(1-0)\sqrt{ \frac{\tau}{2n}}\right)\leq\exp(-\tau)\;.\] We set \(\tau\coloneqq(1-\kappa)^{2}n/2\), such that with probability \(\geq 1-\exp((1-\kappa)^{2}n/2)\), we have \[K/n-(1-\kappa)/4 \leq K/n-P(\varepsilon_{i}^{2}<T)=\frac{1}{n}\sum_{i=1}^{n}(\xi_ {i}-\mathbb{E}\xi_{i})<\sqrt{\frac{(1-\kappa)^{2}n/2}{2n}}\] \[=\frac{1-\kappa}{2}\;.\] In this case, we have \[\frac{1}{n}\sum_{i=1}^{\lfloor(1-\kappa)n\rfloor}(\varepsilon^{2} )^{(n-i+1)} \geq\frac{1}{n}(\lfloor(1-\kappa)n\rfloor-K)T\geq\frac{1}{n}(((1- \kappa)n-1)-K)T\] \[\geq\left(\frac{1-\kappa}{4}-\frac{1}{n}\right)T\;,\] where the right-hand side is lower bounded by \(c\coloneqq(1-\kappa)T/8\) for \(n\) large enough. 

The next lemma is a generalization of Lemma 9 in Buchholz (2022) to arbitrary fractions \(\kappa\) of the training points. Therefore, for any \(\kappa\in(0,1)\) define

\[\delta_{\min}(\kappa)=n^{-1/d}\left(\frac{\kappa}{C_{\rho}\omega_{d}}\right)^ {1/d},\]

**Lemma D.7** (Generalization of Lemma 9 in Buchholz (2022)).: _Let \(\kappa,\nu\in(0,1)\), and let \(c_{\Omega}>0\) be a constant that satisfies \(P_{X}(\operatorname{dist}(\bm{x},\partial\Omega)<c_{\Omega})\leq\kappa\). Let \(\mathcal{P}=\{\bm{x}_{1},\ldots,\bm{x}_{n}\}\) be i.i.d. points distributed according to the measure \(P_{X}\), which has lower and upper bounded density on its entire bounded open Lipschitz domain \(\Omega\subseteq\mathbb{R}^{d}\), \(C_{l}\leq p_{X}(\bm{x})\leq C_{u}\). Then there exists a constant \(\Theta>0\) depending on \(d,C_{u},\nu\) such that with probability at least \(1-\exp\left(-\frac{3\kappa n}{\kappa}\right)\) there exists a good subset \(\mathcal{P}^{\prime}\subseteq\mathcal{P}\), \(|\mathcal{P}^{\prime}|\geq(1-7\kappa)n\), with the following properties: For \(\bm{x}\in\mathcal{P}^{\prime}\) we have \(\mathrm{dist}(\bm{x},\partial\Omega)\geq c_{\Omega}\), \(|\bm{x}-\bm{y}|>\delta_{\min}(\kappa)\) for \(\bm{x}\neq\bm{y}\in\mathcal{P}^{\prime}\), and for all \(\bm{x}\in\mathcal{P}^{\prime}\) we have_

\[\sum_{\bm{y}\in\mathcal{P}^{\prime}\setminus\{\bm{x}\}}|\bm{x}-\bm{y}|^{-d-2 \nu}\leq\frac{2\Theta\delta_{\min}(\kappa)^{-2\nu}n}{\kappa^{2}}.\]

Proof.: First by the definition of \(\delta_{\min}\), it holds that

\[P\left(\bm{x}_{j}\in\bigcup_{i<j}B\left(\bm{x}_{i},\delta_{\min}\right)\right) \leq C_{u}\omega_{d}\delta_{\min}^{d}n\leq\kappa\]

Also for all \(\bm{y}\in\Omega\)

\[\mathbb{E}_{\bm{x}}\left((\bm{x}-\bm{y})^{-d-2s}\bm{1}\left(|\bm {x}-\bm{y}|\geq\delta_{\min}\right)\right) =\int_{B\left(\bm{y},\delta_{\min}\right)^{c}}|\bm{x}-\bm{y}|^{-d -2\nu}p_{X}(\bm{x})\mathrm{d}\bm{x}\] \[\leq C_{u}\int_{B\left(\bm{x},\delta_{\min}\right)^{c}}|\bm{x}- \bm{y}|^{-d-2\nu}\mathrm{d}\bm{y}\leq\Theta\delta_{\min}^{-2\nu}\]

for some \(\Theta>0\) depending only on \(C_{u},d\) and \(\nu\). We conclude that for each \(j\)

\[P\left(\sum_{i<j}|\bm{x}_{i}-\bm{x}_{j}|^{-d-2\nu}\;\bm{1}\left(|\bm{x}_{i}- \bm{x}_{j}|>\delta_{\min}\right)>\frac{\Theta\delta_{\min}^{-2\nu}n}{\kappa} \right)\leq\kappa.\]

Also \(P\left(\mathrm{dist}\left(\bm{x}_{j},\partial\Omega\right)<c_{\Omega}\right)<\kappa\). The union bound implies that

\[P\left(\bm{x}_{j}\notin\bigcup_{i<j}B\left(\bm{x}_{i},\delta_{ \min}\right),\;\;\sum_{i<j}|\bm{x}_{i}-\bm{x}_{j}|^{-d-2\nu}\;\bm{1}_{|\bm{x}_ {i}-\bm{x}_{j}|>\delta_{\min}}<\frac{\Theta\delta_{\min}^{-2\nu}n}{\kappa},\; \;\mathrm{dist}\left(\bm{x}_{j},\partial\Omega\right)>c_{\Omega}\right)\] \[= P\left(\bm{x}_{j}\notin\bigcup_{i<j}B\left(\bm{x}_{i},\delta_{ \min}\right),\;\;\sum_{i<j}|\bm{x}_{i}-\bm{x}_{j}|^{-d-2\nu}<\frac{\Theta \delta_{\min}^{-2\nu}n}{\kappa},\;\;\mathrm{dist}\left(\bm{x}_{j},\partial \Omega\right)>c_{\Omega}\right)\geq 1-3\kappa.\]

We use a martingale construction similar to the one in Lemma 7 of Buchholz (2022) by defining

\[E_{j}:=\left\{\bm{x}_{j}\in\bigcup_{i<j}B\left(\bm{x}_{i},\delta_{\min}\right),\;\;\;\text{or}\;\;\;\sum_{i<j}|\bm{x}_{i}-\bm{x}_{j}|^{-d-2\nu}\geq\frac{ \Theta\delta_{\min}^{-2\nu}n}{\kappa},\;\;\;\text{or}\;\;\;\mathrm{dist}(\bm{ x}_{j},\partial\Omega)\leq c_{\Omega}\right\}.\]

Now define \(S_{n}\coloneqq\sum_{i=1}^{n}\bm{1}_{E_{i}}\). Using the filtration \(\mathcal{F}_{i}=\sigma(\bm{x}_{1},\ldots,\bm{x}_{i})\), \(S_{n}\) can be decomposed into \(S_{n}=M_{n}+A_{n}\), where \(M_{n}\) is a martingale and \(A_{n}\) is predictable with respect to \(\mathcal{F}_{n}\). We then get \(A_{n}\leq\sum_{i=1}^{n}P(E_{i}|\mathcal{F}_{i-1})\leq 3\kappa n\) as well as \(\mathrm{Var}(M_{i}|\mathcal{F}_{i-1})\leq 3\kappa\). Hence Freedman's inequality Theorem D.8 yields

\[P(S_{n}\geq 6\kappa n)\leq P(A_{n}\geq 3\kappa n)+P(M_{n}\geq 3\kappa n)\leq \exp\left(-\frac{3\kappa n}{7}\right).\]

This implies that with probability at least \(1-\exp\left(-\frac{3\kappa n}{7}\right)\) we can find a subset \(\mathcal{P}_{s}=\{\bm{z}_{1},\ldots,\bm{z}_{m}\}\) with \(|\mathcal{P}_{s}|\geq(1-6\kappa)n\) on which it holds that \(\min_{i\neq j}|\bm{z}_{i}-\bm{z}_{j}|\geq\delta_{\min},\mathrm{dist}\left(\bm{ z}_{j},\partial\Omega\right)\geq c_{\Omega}\) and

\[\sum_{i\neq j}|\bm{z}_{i}-\bm{z}_{j}|^{-d-2\nu}\leq\frac{2\Theta\delta_{\min}^{-2 \nu}n^{2}}{\kappa}.\]

Using Markov's inequality we see that there are at most \(\kappa n\) points in \(\mathcal{P}_{s}\) such that

\[\sum_{\bm{z}^{\prime}\in\mathcal{P}_{s},\bm{z}\neq\bm{z}^{\prime}}|\bm{z}-\bm{z} ^{\prime}|^{-d-2\nu}\geq\frac{2\Theta\delta_{\min}^{-2\nu}n}{\kappa^{2}}.\]Removing those points we find a subset \(\mathcal{P}^{\prime}\subset\mathcal{P}_{s}\) such that \(|\mathcal{P}^{\prime}|\geq(1-7\kappa)n\) and for each \(\bm{z}\in\mathcal{P}^{\prime}\)

\[\sum_{\bm{z}^{\prime}\in\mathcal{P}_{s},\bm{z}\neq\bm{z}^{\prime}}|\bm{z}-\bm{z }^{\prime}|^{-d-2\nu}\leq\frac{2\Theta\delta_{\min}^{-2\nu}n}{\kappa^{2}}.\qed\]

**Theorem D.8** (Freedman's inequality, Theorem \(6.1\) in Chung and Lu (2006)).: _Let \(M_{i}\) be a discrete martingale adapted to the filtration \(\mathcal{F}_{i}\) with \(M_{0}=0\) that satisfies for all \(i\geq 0\)_

\[|M_{i+1}-M_{i}| \leq K\] \[\operatorname{Var}\left(M_{i}\mid\mathcal{F}_{i-1}\right) \leq\sigma_{i}^{2}.\]

_Then_

\[P\left(M_{n}-\mathbb{E}\left(M_{n}\right)\geq\lambda\right)\leq e^{-\frac{ \lambda^{2}}{2\sum_{i=1}^{n}\sigma_{i}^{2}+K\lambda/3}}.\]

## Appendix E Translating between \(\mathbb{R}^{d}\) and \(\mathbb{S}^{d}\)

Since the RKHS of the ReLU NTK and NNGP kernels mentioned in Theorem 4 are equivalent to the Sobolev spaces \(H^{(d+1)/2}(\mathbb{S}^{d})\) and \(H^{(d+3)/2}(\mathbb{S}^{d})\), respectively (Chen and Xu, 2021, Bietti and Bach, 2021) (detailed summary in Appendix B.4). Inconsistency of functions in these RKHS that fulfill Assumptions (O) and (N), as in Theorem 1, follows immediately by adapting Theorem 1 via Lemma E.1. In particular, inconsistency holds for the gradient flow and gradient descent estimators \(f_{t,\rho}\) and \(f_{t,\rho}^{\mathrm{GD}}\) as soon as they overfit with lower bounded probability.

For arbitrary open sphere caps \(T\coloneqq\{\bm{x}\in\mathbb{S}^{d}\mid x_{d+1}<v\}\), \(v\in(-1,1)\), and the open unit ball \(B_{1}(0)\coloneqq\{\bm{y}\in\mathbb{R}^{d}\mid\|\bm{y}\|_{2}<1\}\), define the scaled stereographic projection \(\phi:T\to B_{1}(0)\subseteq\mathbb{R}^{d}\) as

\[\phi(x_{1},\ldots,x_{d+1})=\left(\frac{c_{v}x_{1}}{1-x_{d+1}},\ldots,\frac{c_{ v}x_{d}}{1-x_{d+1}}\right),\]

where the normalization constant \(c_{v}=\sqrt{\frac{1-v}{1+v}}\) ensures surjectivity.

Straightforward calculations show that \(\phi\) defines a diffeomorphism. Its inverse \(\phi^{-1}:B_{1}(0)\to T\) is given by

\[\phi^{-1}(y_{1},\ldots,y_{d})=\left(\frac{2c_{v}^{-1}y_{1}}{c_{v}^{-2}\|\bm{y }\|_{2}^{2}+1},\ldots,\frac{2c_{v}^{-1}y_{d}}{c_{v}^{-2}\|\bm{y}\|_{2}^{2}+1}, \frac{c_{v}^{-2}\|\bm{y}\|_{2}^{2}-1}{c_{v}^{-2}\|\bm{y}\|_{2}^{2}+1}\right).\]

We can translate kernel learning with the kernel \(k\) on \(\mathbb{S}^{d}\) and the probability distribution \(P\), where \(P_{X}\) is supported on \(T\), to kernel learning with a transformed kernel \(\tilde{k}\) and \(\tilde{P}\) using a sufficiently smooth diffeomorphism like \(\phi:T\to B_{1}(0)\subseteq\mathbb{R}^{d}\). If the RKHS of \(k\) is equivalent to \(H^{s}(\mathbb{S}^{d})\) then the RKHS of \(\tilde{k}\) is equivalent to \(H^{s}(B_{1}(0))\). We formalize this argument in the following lemma. As a consequence it suffices to prove all inconsistency results for Sobolev kernels on \(B_{1}(0)\).

**Lemma E.1** (**Transfer to sphere caps**).: _Let \(k\) be a kernel on \(\mathbb{S}^{d}\) whose RKHS is equivalent to a Sobolev space \(H^{s}(\mathbb{S}^{d})\). For fixed \(v\in(-1,1)\), consider an "open sphere cap" \(T\coloneqq\{\bm{x}\in\mathbb{S}^{d}\mid x_{d+1}<v\}\). Furthermore, consider a distribution \(P\) such that \(P_{X}\) is supported on \(T\) and has lower and upper bounded density \(p_{X}\) on \(T\), i.e. \(0<C_{l}\leq p_{X}(\bm{x})\leq C_{u}<\infty\) for all \(\bm{x}\in T\). Then_

* \(\tilde{k}(\bm{x},\bm{x}^{\prime})\coloneqq k(\phi^{-1}(\bm{x}),\phi^{-1}(\bm{x }^{\prime}))\) _defines a positive definite kernel on_ \(B_{1}(0)\subseteq\mathbb{R}^{d}\) _whose RKHS is equivalent to the Sobolev space_ \(H^{s}(B_{1}(0))\)_,_
* \(\tilde{P}\coloneqq P\circ\psi^{-1}\) _with_ \(\psi(\bm{x},y)\coloneqq(\phi(\bm{x}),y)\) _defines a probability distribution such that_ \(\tilde{P}_{\tilde{X}}\) _has lower and upper bounded density on_ \(B_{1}(0)\subseteq\mathbb{R}^{d}\)_,_

_and kernel learning with \((k,P)\) or with \((\tilde{k},\tilde{P})\) is equivalent in the following sense:_

_For every function \(f\in\mathcal{H}(k\big{|}_{T})\) the transformed function \(\tilde{f}=f\circ\phi^{-1}\in\mathcal{H}(\tilde{k})\) has the same RKHS norm, i.e. \(\left\|f\right\|_{\mathcal{H}(k\big{|}_{T})}=\left\|\tilde{f}\right\|_{\mathcal{ H}(\tilde{k})}\). Furthermore, the excess risks of \(f\) over \(P\) and of \(\tilde{f}\) over \(\tilde{P}\) coincide, i.e._

\[\mathbb{E}_{\bm{x}\sim P_{X}}(f(\bm{x})-f_{P}^{*}(\bm{x}))^{2}=\mathbb{E}_{\tilde {\bm{x}}\sim\tilde{P}_{X}}(\tilde{f}(\tilde{\bm{x}})-\tilde{f}_{\tilde{P}}^{*}( \tilde{\bm{x}}))^{2},\]

_where \(\tilde{f}_{\tilde{P}}^{*}(\tilde{\bm{x}})=\mathbb{E}_{(\tilde{X},\tilde{Y})\sim \tilde{P}}(\tilde{Y}|\tilde{X}=\tilde{\bm{x}})\) denotes the Bayes optimal predictor under \(\tilde{P}\)._

**Remark E.2**.: Many kernel regression estimators can be explicitly written as \(f_{D}^{k}(\bm{x})=\hat{f}_{n}(k(\bm{x},\bm{X}),k(\bm{X},\bm{X}),\bm{y})\) where \(\hat{f}_{n}:\mathbb{R}^{n}\times\mathbb{R}^{n\times n}\times\mathbb{R}^{n} \rightarrow\mathbb{R}\) denotes a measurable function for all \(n\in\mathbb{N}\). Then the explicit form is preserved under the transformation, i.e. \(f\circ\phi^{-1}=f_{D}^{k}\) with the transformed data set \(\tilde{D}=\{(\phi(\bm{x}_{i}),y_{i})\}_{i\in[n]}\). 

Proof of Lemma e.1.: **Step 1: Bounded density.** For \(i\in[d],j\in[d+1]\), the partial derivatives of \(\phi\) are given by

\[\partial_{x_{j}}\phi_{i}(\bm{x})=\begin{cases}\frac{c_{v}}{1-x_{d+1}},&\text{ for }\;i=j,\\ \frac{c_{v}k_{j}}{(1-x_{d+1})^{2}},&\text{ for }\;i\in[d],\;j=d+1,\\ 0,&\text{ otherwise.}\end{cases}\]

Given an arbitrary multi-index \(\alpha\), the partial derivatives \(\partial_{\alpha}\phi_{i}\in L^{2}(T)\), \(\partial_{\alpha}\phi_{j}^{-1}\in L^{2}(B_{1}(0))\) are bounded for all \(i\in[d],j\in[d+1]\), using \(x_{d+1}\leq v<1\) and the inverse function theorem.

Now define \(\tilde{k}(\bm{x},\bm{x}^{\prime})\coloneqq k(\phi^{-1}(\bm{x}),\phi^{-1}(\bm{ x}^{\prime}))\), \(\psi(\bm{x},y)\coloneqq(\phi(\bm{x}),y)\) and \(\tilde{P}\coloneqq P\circ\psi^{-1}\). Then using integration by substitution (Stroock et al., 2011, Theorem 5.2.16), the Lebesgue density of \(\tilde{P}_{X}\) is given by

\[p_{\tilde{X}}(\tilde{\bm{x}})=p_{X}(\phi^{-1}(\tilde{\bm{x}}))J\phi^{-1}( \tilde{\bm{x}}),\]

where

\[J\phi^{-1}(\tilde{\bm{x}})\coloneqq\left[\det\left(\left(\langle\partial_{i} \phi^{-1}(\tilde{\bm{x}}),\partial_{j}\phi^{-1}(\tilde{\bm{x}})\rangle_{ \mathbb{R}^{d+1}}\right)_{i,j\in\{1,\ldots,d\}}\right)\right]^{1/2}.\]

\(J\phi\) and \(J\phi^{-1}\) can be continuously extended to \(\bar{T}\) and \(\bar{B}_{1}(0)\), respectively. Then, since \(J\phi^{-1}\) is continuous on a compact set and because \(\phi\) with the extended domain remains a diffeomorphism so that \(J\phi^{-1}\) cannot attain the value \(0\), there exists a constant \(C_{\phi}>0\) such that \(\frac{1}{C_{\phi}}\leq J\phi^{-1}(\tilde{\bm{x}})\leq C_{\phi}\) for all \(\tilde{\bm{x}}\in B_{1}(0)\). Hence, \(p_{\tilde{X}}\) is lower and upper bounded.

**Step 2: Excess risks coincide.** If \((\tilde{X},\tilde{Y})\sim\tilde{P}\), the Bayes predictor of \(\tilde{Y}\) given \(\tilde{X}\) is given by \(\tilde{f}^{*}(\tilde{\bm{x}})=\mathbb{E}(\tilde{Y}|\tilde{X}=\tilde{\bm{x}})= f^{*}(\phi^{-1}(\tilde{\bm{x}}))\).

Let \(\pi_{1}(\bm{x},y)=\bm{x}\) be the projection onto the first component. Then, \(\phi(\pi_{1}(\bm{x},y))=\phi(\bm{x})=\pi_{1}(\phi(\bm{x}),y)=\pi_{1}(\psi(\bm{ x},y))\) and hence

\[\mathbb{E}_{\bm{x}\sim P_{X}}(f(\bm{x})-f^{*}(\bm{x}))^{2} =\mathbb{E}_{(\bm{x},y)\sim P}(f(\pi_{1}(\bm{x},y))-f^{*}(\pi_{1} (\bm{x},y)))^{2}\] \[=\mathbb{E}_{(\bm{x},y)\sim P}(f(\phi^{-1}(\phi(\pi_{1}(\bm{x},y ))))-f^{*}(\phi^{-1}(\phi(\pi_{1}(\bm{x},y))))^{2}\] \[=\mathbb{E}_{(\bm{x},y)\sim P}(\tilde{f}(\pi_{1}(\psi(\bm{x},y)) )-\tilde{f}^{*}(\pi_{1}(\psi(\bm{x},y))))^{2}\] \[=\mathbb{E}_{(\bm{x},y)\sim P}(\tilde{f}(\pi_{1}(\bm{x},y))- \tilde{f}^{*}(\pi_{1}(\bm{x},y)))^{2}\] \[=\mathbb{E}_{\bm{x}\sim\tilde{P}_{X}}(\tilde{f}(\bm{x})-\tilde{f} ^{*}(\bm{x}))^{2}\;.\]

**Step 3: Transformed RKHS.** We want to show that \(\mathcal{H}({k}{\left|{}_{T}\right.})\rightarrow\mathcal{H}(\tilde{k}),f \mapsto f\circ\phi^{-1}\) defines an isometric isomorphism, which especially shows the statement \(\left\|f\right\|_{\mathcal{H}({k}{\left|{}_{T}\right.})}=\left\|\tilde{f} \right\|_{\mathcal{H}(\tilde{k})}\) from the proposition. For this, we use the following theorem characterizing RKHSs:

**Theorem E.3** (Theorem 4.21 in Steinwart and Christmann (2008)).: _Let \(k:X\times X\rightarrow\mathbb{R}\) be a positive definite kernel function with feature space \(H_{0}\) and feature map \(\Phi_{0}:X\to H_{0}\). Then_

\[H=\{f:X\rightarrow\mathbb{R}\mid\exists w\in H_{0}:\ f=\langle w, \Phi_{0}(\cdot)\rangle_{H_{0}}\}\text{ with }\] \[\left\|f\right\|_{H}\coloneqq\inf\{\left\|w\right\|_{H_{0}}:\ f= \langle w,\Phi_{0}(\cdot)\rangle_{H_{0}}\},\]

_is the only RKHS for which \(k\) is a reproducing kernel._

A feature map for \({k}{\left|{}_{T}\right.}\) is given by \(\Phi:T\rightarrow\mathcal{H}({k}{\left|{}_{T}\right.})\), \(\Phi(\bm{x})=k(\bm{x},\cdot)\). Hence a feature map for \(\tilde{k}\) is given by \(\Phi\circ\phi^{-1}:B_{1}(0)\rightarrow\mathcal{H}({k}{\left|{}_{T}\right.})\). Theorem E.3 states that

\[\mathcal{H}({k}{\left|{}_{T}\right.})\] (E.1) \[\left\|f\right\|_{\mathcal{H}({k}{\left|{}_{T}\right.})}\coloneqq \inf\{\left\|w\right\|_{\mathcal{H}({k}{\left|{}_{T}\right.})}:\ f=\langle w, \Phi(\cdot)\rangle_{\mathcal{H}({k}{\left|{}_{T}\right.})}\},\]as well as

\[\mathcal{H}(\tilde{k}) =\Big{\{}\tilde{f}:B_{1}(0)\to\mathbb{R}\mid\exists w\in\mathcal{H}(k \big{|}_{T}):\ \tilde{f}=\langle w,\Phi\circ\phi^{-1}(\cdot)\rangle_{\mathcal{H}(k\mid_{T})} \Big{\}}\ \text{with}\] (E.2) \[\|\tilde{f}\|_{\mathcal{H}(\tilde{k})}\coloneqq\inf\{\|w\|_{ \mathcal{H}(k\mid_{T})}:\ \tilde{f}=\langle w,\Phi\circ\phi^{-1}(\cdot)\rangle_{\mathcal{H}(k\mid_{T})}\}.\]

As \(\phi^{-1}\) is bijective, this characterization induces an isometric isomorphism between \(\mathcal{H}(k\big{|}_{T})\) and \(\mathcal{H}(\tilde{k})\) by mapping \(f=\langle w,\Phi(\cdot)\rangle_{\mathcal{H}(k\mid_{T})}\in\mathcal{H}(k\big{|} _{T})\) to \(\tilde{f}=f\circ\phi^{-1}=\langle w,\Phi\circ\phi^{-1}(\cdot)\rangle_{ \mathcal{H}(k\mid_{T})}\in\mathcal{H}(\tilde{k})\). This shows \(\|f\|_{\mathcal{H}(k\mid_{T})}=\|\tilde{f}\|_{\mathcal{H}(k)}\).

**Step 4: RKHS of \(\tilde{k}\).** We now show that the RKHS of \(\tilde{k}\), denoted as \(\mathcal{H}(\tilde{k})\), is equivalent to \(H^{s}(B_{1}(0))\). To this end, denoting \(\mathcal{A}\circ\phi\coloneqq\{f\circ\phi\mid f\in\mathcal{A}\}\) and \(\mathcal{A}|_{T}\coloneqq\{f|_{T}\mid f\in\mathcal{A}\}\), we show the following equality of sets (ignoring the norms):

\[\mathcal{H}(\tilde{k})\circ\phi\stackrel{{\rm(I)}}{{=}}\mathcal{ H}(k\big{|}_{T})\stackrel{{\rm(II)}}{{=}}\mathcal{H}(k)\big{|}_{T} \stackrel{{\rm(III)}}{{=}}H^{s}(\mathbb{S}^{d})\big{|}_{T} \stackrel{{\rm(IV)}}{{=}}H^{s}(B_{1}(0))\circ\phi\.\]

Since \(\phi\) is bijective, this implies \(\mathcal{H}(\tilde{k})=H^{s}(B_{1}(0))\) as sets, and the norm equivalence then follows from Lemma F.7.

Equality (I) follows from Step 3. Equality (II) follows from Theorem E.3 by observing that if \(\Phi\) is a feature map for \(k\), then \(\Phi|_{T}\) is a feature map for \(k|_{T}\). Equality (III) holds by assumption. To show (IV), we need a characterization of \(H^{s}(\mathbb{S}^{d})\) that allows to work with charts like \(\phi\).

**Step 4.1: Chart-based characterization of \(H^{s}(\mathbb{S}^{d})\).** A trivialization of a Riemannian manifold \((M,g)\) with bounded geometry of dimension \(d\) consists of a locally finite open covering \(\{U_{\alpha}\}_{\alpha\in I}\) of \(M\), smooth diffeomorphisms \(\kappa_{\alpha}:V_{\alpha}\subset\mathbb{R}^{d}\to U_{\alpha}\), also called charts, and a partition of unity \(\{h_{\alpha}\}_{\alpha\in I}\) of \(M\) that fulfills \(\text{supp}(h_{\alpha})\subseteq U_{\alpha}\), \(0\leq h_{\alpha}\leq 1\) and \(\sum_{\alpha\in I}h_{\alpha}=1\). An admissible trivialization of \((M,g)\) is a uniformly locally finite trivialization of \(M\) that is compatible with geodesic coordinates, for details see (Schneider and Grosse, 2013, Definition 12).

In our case, define an open neighborhood of \(T\) by \(U_{1}\coloneqq\{\boldsymbol{x}\in\mathbb{S}^{d}\mid x_{d+1}<v+\varepsilon\}\) with some \(\varepsilon\in(0,1-v)\) arbitrary but fixed, and \(U_{2}\coloneqq\{\boldsymbol{x}\in\mathbb{S}^{d}\mid x_{d+1}>v+\varepsilon/2\}\). It holds that \(U_{1}\cup U_{2}=\mathbb{S}^{d}\). Moreover, there exists an appropriate partition of unity consisting of \(C^{\infty}\) functions \(h_{1},h_{2}:\mathbb{S}^{d}\to[0,1]\). Especially, we have \(h_{1}(T)\subseteq h_{1}(U_{2}^{c})=\{1\}\). Let \(\phi_{1}:U_{1}\to B_{r_{1}}(0)\) denote the stereographic projection with respect to \(\alpha_{0}=(0,\ldots,0,1)\) as above, scaled such that \(\phi_{1}|_{T}=\phi\) and hence \(\phi_{1}(T)=B_{1}(0)\). Similarly, let \(\phi_{2}:U_{2}\to B_{r_{2}}(0)\) denote an arbitrarily scaled stereographic projection with respect to \(\boldsymbol{x}_{0}=(0,\ldots,0,-1)\). Then \((\{U_{1},U_{2}\},\{\phi_{1}^{-1},\phi_{1}^{-1}\},\{h_{1},h_{2}\})\) yields an admissible trivialization of \(\mathbb{S}^{d}\) consisting of only two charts. A detailed derivation can be found in (Hubbert et al., 2015, Section 1.7). Therefore (Schneider and Grosse, 2013, Theorem 14) lets us define the Sobolev norm on \(\mathbb{S}^{d}\) (up to equivalence) as2

Footnote 2: Here, the norms are taken on \(H^{s}(\mathbb{R}^{d})\) since the respective functions can be extended to \(\mathbb{R}^{d}\) by zero outside of their domain of definition, thanks to the properties of the partition of unity.

\[\|g\|_{H^{s}(\mathbb{S}^{d})} \coloneqq\left(\sum_{\alpha\in I}\|(h_{\alpha}g)\circ\kappa_{ \alpha}\|_{H^{s}(\mathbb{R}^{d})}^{2}\right)^{1/2}\] \[=\left(\|(h_{1}g)\circ\phi_{1}^{-1}\|_{H^{s}(\mathbb{R}^{d})}^{2} +\|(h_{2}g)\circ\phi_{2}^{-1}\|_{H^{s}(\mathbb{R}^{d})}^{2}\right)^{1/2},\]

for any distribution \(g\in\mathcal{D}^{\prime}(\mathbb{S}^{d})\) (i.e. any continuous linear functional on \(C^{\infty}_{c}(\mathbb{S}^{d})\)). Then \(g\in H^{s}(\mathbb{S}^{d})\) if and only if \(\|g\|_{H^{s}(\mathbb{S}^{d})}<\infty\).

**Step 4.2: Showing (IV).** First, let \(g\in H^{s}(\mathbb{S}^{d})\). Then, as we saw in Step 4.1, we must have \(\|(h_{1}g)\circ\phi_{1}^{-1}\|_{H^{s}(\mathbb{R}^{d})}<\infty\) and thus \((h_{1}g)\circ\phi_{1}^{-1}\in H^{s}(\mathbb{R}^{d})\). By our discussion in Appendix B.1, we then have

\[(g|_{T})\circ\phi^{-1}=((h_{1}g)\circ\phi_{1}^{-1})|_{B_{1}(0)}\in H^{s}(B_{1}(0 ))\,\]

which shows \(g|_{T}\in H^{s}(B_{1}(0))\circ\phi\).

Now, let \(f\in H^{s}(B_{1}(0))\). Then, again following our discussion in Appendix B.1, there exists an extension \(\bar{f}\in H^{s}(\mathbb{R}^{d})\) with \(\bar{f}|_{B_{1}(0)}=f\). The set \(\mathcal{B}\coloneqq\phi_{1}(U_{1}\setminus U_{2})\) is a closed ball \(\overline{B_{r}(0)}\) of radius \(1<r<r_{1}\). Hence, we can find \(\varphi\in C^{\infty}(\mathbb{R}^{d})\) with \(\varphi(B_{1}(0))=\{1\}\) and \(\varphi((B_{r}(0))^{c})=\{0\}\). Since \(\varphi\) is smooth with compact support, we have \(\varphi\cdot\bar{f}\in H^{s}(\mathbb{R}^{d})\). Define

\[f_{\mathbb{S}^{d}}:\mathbb{S}^{d}\to\mathbb{R},\bm{x}\mapsto\begin{cases}( \varphi\cdot\bar{f})(\phi(\bm{x}))&,\bm{x}\in U_{1}\\ 0&,\bm{x}\not\in U_{1}\;.\end{cases}\]

By construction, we have \(f_{\mathbb{S}^{d}}(\bm{x})=0\) for all \(\bm{x}\in U_{2}\). Hence, the equivalent Sobolev norm from Step 4.1 is

\[\|f_{\mathbb{S}^{d}}\|_{H^{s}(\mathbb{S}^{d})} =\left(\|(h_{1}f_{\mathbb{S}^{d}})\circ\phi_{1}^{-1}\|_{H^{s}( \mathbb{R}^{d})}^{2}+\|(h_{2}f_{\mathbb{S}^{d}})\circ\phi_{2}^{-1}\|_{H^{s}( \mathbb{R}^{d})}^{2}\right)^{1/2}\] \[=\|(h_{1}\circ\phi_{1}^{-1})\cdot\varphi\cdot\bar{f}\|_{H^{s}( \mathbb{R}^{d})}<\infty\;,\]

which shows \(f_{\mathbb{S}^{d}}\in H^{s}(\mathbb{S}^{d})\). But then, \(f\circ\phi=f_{\mathbb{S}^{d}}|_{T}\in H^{s}(\mathbb{S}^{d})|_{T}\).

In total, we obtain \(H^{s}(\mathbb{S}^{d})|_{T}=H^{s}(B_{1}(0))\circ\phi\), which shows (IV). 

## Appendix F Spectral lower bound

### General lower bounds

A common first step to analyze the expected excess risk caused by label noise is to perform a bias-variance decomposition and integrate over \(\bm{y}\) first (see e.g. Liang and Rakhlin, 2020, Hastie et al., 2022, Holzmuller, 2021), which is also used in the following lemma.

**Lemma F.1**.: _Consider an estimator of the form \(f_{\bm{X},\bm{y}}(\bm{x})=(\bm{v}_{\bm{X},\bm{x}})^{\top}\bm{y}\). If \(\operatorname{Var}_{P}(y|\bm{x})\geq\sigma^{2}\) for \(P_{X}\)-almost all \(\bm{x}\), then the expected excess risk satisfies_

\[\mathbb{E}_{D}R_{P}(f_{\bm{X},\bm{y}})-R_{P}^{*}\geq\sigma^{2}\mathbb{E}_{\bm {X},\bm{x}}\operatorname{tr}(\bm{v}_{\bm{X},\bm{x}}(\bm{v}_{\bm{X},\bm{x}})^{ \top})\;.\]

Proof.: A standard bias-variance decomposition lets us lower-bound the expected excess risk by the estimator variance due to the label noise, which can then be further simplified:

\[\mathbb{E}_{D}R_{P}(f_{\bm{X},\bm{y}})-R_{P}^{*} \geq\mathbb{E}_{\bm{X},\bm{x}}\left(\mathbb{E}_{\bm{y}|\bm{X}} \left[f_{\bm{X},\bm{y}}(\bm{x})^{2}\right]-\left(\mathbb{E}_{\bm{y}|\bm{X}}[f _{\bm{X},\bm{y}}(\bm{x})]\right)^{2}\right)\;.\] \[=\mathbb{E}_{\bm{X},\bm{x}}\mathbb{E}_{\bm{y}|\bm{X}}\left(\bm{f }_{\bm{X},\bm{y}}(\bm{x})-\mathbb{E}_{\bm{y}|\bm{X}}f_{\bm{X},\bm{y}}(\bm{x}) \right)^{2}\] \[=\mathbb{E}_{\bm{X},\bm{x}}\mathbb{E}_{\bm{y}|\bm{X}}(\bm{v}_{\bm {X},\bm{x}})^{\top}(\bm{y}-\mathbb{E}_{\bm{y}|\bm{X}}\bm{y})(\bm{y}-\mathbb{E }_{\bm{y}|\bm{X}}\bm{y})^{\top}\bm{v}_{\bm{X},\bm{x}}\] \[=\mathbb{E}_{\bm{X},\bm{x}}(\bm{v}_{\bm{X},\bm{x}})^{\top} \left[\mathbb{E}_{\bm{y}|\bm{X}}(\bm{y}-\mathbb{E}_{\bm{y}|\bm{X}}\bm{y})( \bm{y}-\mathbb{E}_{\bm{y}|\bm{X}}\bm{y})^{\top}\right]\bm{v}_{\bm{X},\bm{x}}\] \[=\mathbb{E}_{\bm{X},\bm{x}}(\bm{v}_{\bm{X},\bm{x}})^{\top} \operatorname{Cov}(\bm{y}|\bm{X})\bm{v}_{\bm{X},\bm{x}}\;.\]

Here, the conditional covariance matrix can be lower bounded in terms of the Loewner order (which is defined as \(A\succeq B\Leftrightarrow B-A\) positive semi-definite):

\[\operatorname{Cov}(\bm{y}|\bm{X})=\begin{pmatrix}\operatorname{Var}(y_{1}| \bm{x}_{1})&&\\ &\ddots&\\ &&\operatorname{Var}(y_{n}|\bm{x}_{n})\end{pmatrix}\succeq\sigma^{2}\bm{I}_{n}\]

since the labels \(y_{i}\) are conditionally independent given \(\bm{X}\). We therefore obtain

\[\mathbb{E}_{D}R_{P}(f_{\bm{X},\bm{y}})-R_{P}^{*} \geq\mathbb{E}_{\bm{X},\bm{x}}(\bm{v}_{\bm{X},\bm{x}})^{\top} \operatorname{Cov}(\bm{y}|\bm{X})\bm{v}_{\bm{X},\bm{x}}\] \[\geq\sigma^{2}\mathbb{E}_{\bm{X},\bm{x}}\operatorname{tr}((\bm{v}_ {\bm{X},\bm{x}})^{\top}\bm{v}_{\bm{X},\bm{x}})\] \[=\sigma^{2}\mathbb{E}_{\bm{X},\bm{x}}\operatorname{tr}(\bm{v}_{ \bm{X},\bm{x}}(\bm{v}_{\bm{X},\bm{x}})^{\top})\;.\qed\]

**Proposition 5** (**Spectral lower bound**).: _Assume that the kernel matrix \(k(\bm{X},\bm{X})\) is almost surely positive definite, and that \(\operatorname{Var}(y|\bm{x})\geq\sigma^{2}\) for \(P_{X}\)-almost all \(\bm{x}\). Then, the expected excess risk satisfies_

\[\mathbb{E}_{D}R_{P}(f_{t,\rho})-R_{P}^{*}\geq\frac{\sigma^{2}}{n}\sum_{i=1}^{n} \mathbb{E}_{\bm{X}}\frac{\lambda_{i}(k_{*}(\bm{X},\bm{X})/n)\left(1-e^{-2t( \lambda_{i}(k(\bm{X},\bm{X})/n)+\rho)}\right)^{2}}{(\lambda_{i}(k(\bm{X},\bm{X} )/n)+\rho)^{2}}\;.\] (3)Proof.: Recall from Eq. (1) that

\[f_{t,\rho}(\bm{x}) =k(\bm{x},\bm{X})\bm{A}_{t,\rho}(\bm{X})\bm{y}\;,\] \[\bm{A}_{t,\rho}(\bm{X}) \coloneqq\left(\bm{I}_{n}-e^{-\frac{2\ell}{n}t(k(\bm{X},\bm{X})+ \rho n\bm{I}_{n})}\right)\left(k(\bm{X},\bm{X})+\rho n\bm{I}_{n}\right)^{-1}\;.\]

By setting \((\bm{v}_{\bm{X},\bm{x}})^{\top}\coloneqq k(\bm{x},\bm{X})\bm{A}_{t,\rho}(\bm{ X})\), we can write \(f_{\bm{X},\bm{y},t,\rho}(\bm{x})\coloneqq f_{t,\rho}(\bm{x})=(\bm{v}_{\bm{X},\bm{x }})^{\top}\bm{y}\). Using Lemma F.1, we then obtain

\[\mathbb{E}_{D}R_{P}(f_{\bm{X},\bm{y},t,\rho})-R_{P}^{*} \geq\sigma^{2}\mathbb{E}_{\bm{X},\bm{x}}\operatorname{tr}(\bm{v }_{\bm{X},\bm{x}}(\bm{v}_{\bm{X},\bm{x}})^{\top})\] \[=\sigma^{2}\mathbb{E}_{\bm{X},\bm{x}}\operatorname{tr}\left(\bm{ A}_{t,\rho}(\bm{X})^{\top}k(\bm{X},\bm{x})k(\bm{x},\bm{X})\bm{A}_{t,\rho}(\bm{X}) \right)\;.\]

Since

\[\left(\mathbb{E}_{\bm{x}}k(\bm{X},\bm{x})k(\bm{x},\bm{X})\right)_{ ij}=\mathbb{E}_{\bm{x}}k(\bm{x}_{i},\bm{x})k(\bm{x},\bm{x}_{j})=k_{*}(\bm{x} _{i},\bm{x}_{j})=k_{*}(\bm{X},\bm{X})_{ij}\;,\]

we conclude

\[\mathbb{E}_{D}R_{P}(f_{\bm{X},\bm{y},t,\rho})-R_{P}^{*} \geq\sigma^{2}\mathbb{E}_{\bm{X}}\operatorname{tr}(\bm{A}_{t,\rho }^{\top}k_{*}(\bm{X},\bm{X})\bm{A}_{t,\rho})\] \[=\sigma^{2}\mathbb{E}_{\bm{X}}\operatorname{tr}(k_{*}(\bm{X},\bm {X})\bm{A}_{t,\rho}(\bm{X})\bm{A}_{t,\rho}(\bm{X})^{\top})\;.\]

Richter (1958) showed (see also Mirsky, 1959) that for two symmetric matrices \(\bm{B},\bm{C}\), we have \(\operatorname{tr}(\bm{B}\bm{C})\geq\sum_{i=1}^{n}\lambda_{i}(\bm{B})\lambda_{n +1-i}(\bm{C})\). We can therefore conclude

\[\mathbb{E}_{D}R_{P}(f_{\bm{X},\bm{y},t,\rho})-R_{P}^{*} \geq\sigma^{2}\mathbb{E}_{\bm{X}}\sum_{i=1}^{n}\lambda_{i}(k_{*}( \bm{X},\bm{X}))\lambda_{n+1-i}(\bm{A}_{t,\rho}(\bm{X})\bm{A}_{t,\rho}(\bm{X})^ {\top})\;.\]

As \(\bm{A}_{t,\rho}(\bm{X})\bm{A}_{t,\rho}(\bm{X})^{\top}\) is built only out of the matrices \(k(\bm{X},\bm{X})\) and \(\bm{I}_{n}\), it is not hard to see that \(\bm{A}_{t,\rho}(\bm{X})\bm{A}_{t,\rho}(\bm{X})^{\top}\) has the same eigenbasis as \(k(\bm{X},\bm{X})\) with eigenvalues

\[\tilde{\lambda}_{i}\coloneqq\left(\frac{1-e^{-\frac{2\ell}{n}t( \lambda_{i}(k(\bm{X},\bm{X}))+\rho n)}}{\lambda_{i}(k(\bm{X},\bm{X}))+\rho n} \right)^{2}=\frac{1}{n^{2}}\left(\frac{1-e^{-2t(\lambda_{i}(k(\bm{X},\bm{X})/ n)+\rho)}}{\lambda_{i}(k(\bm{X},\bm{X})/n)+\rho}\right)^{2}\;.\]

It remains to order these eigenvalues correctly. To this end, we observe that for \(\lambda>0\), the function \(g(\lambda)\coloneqq\frac{1-e^{-2t\lambda}}{\lambda}\) satisfies

\[g^{\prime}(\lambda)=\frac{2t\lambda e^{-2t\lambda}-(1-e^{-2t \lambda})}{\lambda^{2}}=\frac{(2t\lambda+1)e^{-2t\lambda}-1}{\lambda^{2}}\leq \frac{e^{2t\lambda}e^{-2t\lambda}-1}{\lambda^{2}}=0\;.\]

Therefore, \(g\) is nonincreasing, hence the sequence \((\tilde{\lambda}_{i})\) is nondecreasing and thus

\[\lambda_{n+1-i}(\bm{A}_{t,\rho}\bm{A}_{t,\rho}^{\top})=\tilde{ \lambda}_{i}\;,\]

from which the claim follows. 

**Theorem F.2**.: _Let \(k\) be a kernel on a compact set \(\Omega\) and let \(P_{X}\) be supported on \(\Omega\). Suppose that \(k(\bm{X},\bm{X})\) is almost surely positive definite and that \(\operatorname{Var}(y|\bm{x})\geq\sigma^{2}\) for \(P_{X}\)-almost all \(\bm{x}\). Fix constants \(c>0\) and \(q,C\geq 1\). Suppose that \(\lambda_{i}\coloneqq\lambda_{i}(T_{k,P_{X}})\geq ci^{-q}\). Let \(\mathcal{I}(n)\) be the set of all \(i\in[n]\) for which_

\[\lambda_{i}/C \leq\lambda_{i}(k(\bm{X},\bm{X})/n)\leq C\lambda_{i}\] (F.1) \[\lambda_{i}^{2}/C \leq\lambda_{i}(k_{*}(\bm{X},\bm{X})/n)\]

_both hold at the same time with probability \(\geq 1/2\). Moreover, let \(I(n)\coloneqq\max\{m\in[n]\mid[m]\subseteq\mathcal{I}(n)\}\). Then, there exists a constant \(c^{\prime}>0\) depending only on \(c,C\) such that for all \(\rho\in[0,\infty)\) and \(t\in(0,\infty]\), the following two bounds hold:_

\[\mathbb{E}_{D}R_{P}(f_{\bm{X},\bm{y},t,\rho})-R_{P}^{*} \geq c^{\prime}\sigma^{2}\frac{1}{1+(\rho+t^{-1})n^{q}}\cdot \frac{|\mathcal{I}(n)|}{n}\;,\] \[\mathbb{E}_{D}R_{P}(f_{\bm{X},\bm{y},t,\rho})-R_{P}^{*} \geq c^{\prime}\sigma^{2}\min\left\{\frac{(\rho+t^{-1})^{-2}}{n}, \frac{(\rho+t^{-1})^{-1/q}}{n},\frac{I(n)}{n}\right\}\;.\]

[MISSING_PAGE_FAIL:37]

1. If \(\beta\geq 1\), we bound \[S(\beta)\geq\sum_{i=1}^{I(n)}\frac{1}{2(\beta i^{q})^{2}}\geq\frac{1}{2\beta^{2}}\;.\]
2. If \(\beta\in(I(n)^{-q},1)\), we observe that \[J(\beta)\coloneqq\lfloor\beta^{-1/q}\rfloor\geq\lceil\beta^{-1/q}\rceil-1\geq \frac{1}{2}\lceil\beta^{-1/q}\rceil\geq\frac{\beta^{-1/q}}{2}\] and therefore \[S(\beta)\geq\sum_{i=1}^{J(\beta)}\frac{1}{1+(\beta i^{q})^{2}}\geq\sum_{i=1}^{ J(\beta)}\frac{1}{1+1}=\frac{J(\beta)}{2}\geq\frac{\beta^{-1/q}}{4}\;.\]
3. If \(\beta\in(0,I(n)^{-q}]\), we similarly find that \[S(\beta)\geq\sum_{i=1}^{I(n)}\frac{1}{1+1}=\frac{I(n)}{2}\;.\]

Moreover, there is an absolute constant \(c_{1}>0\) such that for any \(\beta>0\),

\[S(\beta)\geq c_{1}\min\{\beta^{-2},\beta^{-1/q},I(n)\}\;,\] (F.6)

because

1. \(\beta^{-2}=\min\{\beta^{-2},\beta^{-1/q},I(n)\}\) for \(\beta\geq 1\),
2. \(\beta^{-1/q}=\min\{\beta^{-2},\beta^{-1/q},I(n)\}\) for \(\beta\in(I(n)^{-q},1)\), and
3. \(I(n)=\min\{\beta^{-2},\beta^{-1/q},I(n)\}\) for \(\beta\in(0,I(n)^{-q}]\).

**Step 4: Putting it together.** Combining the trivial bound in Step 3 with Eq. (F.4) and Eq. (F.5), we obtain

\[\mathbb{E}_{D}R_{P}(f_{\bm{X},\bm{y},t,\rho})-R_{P}^{*} \geq\frac{\sigma^{2}}{n}\sum_{i\in\mathcal{I}(n)}\mathbb{E}_{\bm {X}}S_{i}(\bm{X})\geq\frac{\sigma^{2}}{n}\cdot\frac{1}{4C^{5}}S(\beta)\] \[\geq c^{\prime}\sigma^{2}\frac{1}{1+(\rho+t^{-1})n^{q}}\cdot \frac{|\mathcal{I}(n)|}{n}\] (F.7)

for a suitable constant \(c^{\prime}>0\) depending only on \(c\) and \(C\).

Moreover, from Eq. (F.6), we obtain

\[S(\beta)\geq\tilde{c}_{1}\min\{\beta^{-2},\beta^{-1/q},I(n)\}\geq\tilde{c}^{ \prime\prime}\min\{(\rho+t^{-1})^{-2},(\rho+t^{-1})^{-1/q},I(n)\}\]

for a suitable constant \(\tilde{c}^{\prime\prime}>0\) depending only on \(c\). Again, (F.4) and (F.5) yield

\[\mathbb{E}_{D}R_{P}(f_{\bm{X},\bm{y},t,\rho})-R_{P}^{*} \geq\frac{\sigma^{2}}{n}\cdot\frac{1}{4C^{5}}S(\beta)\] \[\geq\frac{\tilde{c}^{\prime\prime}}{4C^{5}}\sigma^{2}\min\left\{ \frac{(\rho+t^{-1})^{-2}}{n},\frac{(\rho+t^{-1})^{-1/q}}{n},\frac{I(n)}{n} \right\}\;.\qed\]

### Equivalences of norms and eigenvalues

Later, we will use concentration inequalities for kernel matrix eigenvalues proved for specific kernels, which we then want to transfer to other kernels with equivalent RKHSs. In this subsection, we show that this is possible.

**Definition F.4** (\(C\)-equivalence of matrices and norms).: Let \(n\geq 1\) and let \(\bm{K},\tilde{\bm{K}}\in\mathbb{R}^{n\times n}\) be symmetric. For \(C\geq 1\), we say that \(\bm{K}\) and \(\tilde{\bm{K}}\) are \(C\)-equivalent if their ordered eigenvalues satisfy

\[C^{-1}\lambda_{i}(\bm{K})\leq\lambda_{i}(\tilde{\bm{K}})\leq C\lambda_{i}(\bm {K})\]

for all \(i\in[n]\). Moreover, we say that two norms \(\|\cdot\|_{A}\), \(\|\cdot\|_{B}\) on a vector space \(V\) are \(C\)-equivalent if

\[C^{-1}\|\bm{v}\|_{A}\leq\|\bm{v}\|_{B}\leq C\|\bm{v}\|_{A}\]

for all \(\bm{v}\in V\).

**Lemma F.5**.: _Let \(n\geq 1\) and let \(\bm{K},\tilde{\bm{K}}\in\mathbb{R}^{n\times n}\) be symmetric. Then, \(\bm{K}\) and \(\tilde{\bm{K}}\) are \(C\)-equivalent iff the Moore-Penrose pseudoinverses \(\bm{K}^{+}\) and \(\tilde{\bm{K}}^{+}\) are \(C\)-equivalent._

Proof.: This follows from the fact that if \(\bm{K}\) has eigenvalues \(\lambda_{1},\ldots,\lambda_{n}\), then \(\bm{K}^{+}\) has eigenvalues \(1/\lambda_{1},\ldots,1/\lambda_{n}\), where we define \(1/0\coloneqq 0\). (A detailed proof would be a bit technical due to the sorting of eigenvalues.) 

**Lemma F.6**.: _Let \(k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) be a kernel on a set \(\mathcal{X}\). Then, for any \(\bm{y}\in\mathbb{R}^{n}\),_

\[\bm{y}^{\top}k(\bm{X},\bm{X})^{+}\bm{y}=\|f^{*}_{k,\bm{y}}\|_{\mathcal{H}_{k}} ^{2}\;,\]

_where \(\mathcal{H}_{k}\) is the RKHS associated with \(k\) and \(f^{*}_{k,\bm{y}}\) is the minimum-norm regression solution_

\[f^{*}_{k,\bm{y}} \coloneqq\operatorname*{argmin}_{f\in B}\|f\|_{\mathcal{H}_{k}}^ {2},\] \[B \coloneqq\{f\in\mathcal{H}_{k}\;|\;\sum_{i=1}^{n}(f(\bm{x}_{i})-y _{i})^{2}=\inf_{\tilde{f}\in\mathcal{H}_{k}}\sum_{i=1}^{n}(\tilde{f}(\bm{x}_{i })-y_{i})^{2}\}\;.\]

Proof.: It is well-known that \(f^{*}_{k,\bm{y}}(\bm{x})=\sum_{i=1}^{n}\alpha_{i}k(\bm{x},\bm{x}_{i})\), where \(\bm{\alpha}\coloneqq\bm{K}^{+}\bm{y}\) (see e.g. Rangamani et al., 2023). We then have

\[\|f^{*}_{k,\bm{y}}\|_{\mathcal{H}_{k}}^{2} =\left\langle\sum_{i=1}^{n}\alpha_{i}k(\bm{x}_{i},\cdot),\sum_{j= 1}^{n}\alpha_{j}k(\bm{x}_{j},\cdot)\right\rangle_{\mathcal{H}_{k}}=\sum_{i=1}^ {n}\sum_{j=1}^{n}\alpha_{i}\bm{K}_{ij}\alpha_{j}\] \[=\bm{y}^{\top}\bm{K}^{+}\bm{K}\bm{K}^{+}\bm{y}=\bm{y}^{\top}\bm{K }^{+}\bm{y}\;,\]

where the last step follows from a standard identity for the Moore-Penrose pseudoinverse (see e.g. Section 1.1.1 in Wang et al., 2018). 

**Lemma F.7**.: _Let \(\mathcal{H}_{1}\) and \(\mathcal{H}_{2}\) be two RKHSs with \(\mathcal{H}_{1}\subset\mathcal{H}_{2}\). Then there exists a constant \(C>0\) such that \(\|f\|_{\mathcal{H}_{2}}\leq C\|f\|_{\mathcal{H}_{1}}\)._

Proof.: Let \(I:\mathcal{H}_{1}\to\mathcal{H}_{2}\) be the inclusion map, i.e. \(I_{h}\coloneqq h\) for all \(h\in\mathcal{H}_{1}\). Obviously, \(I\) is linear and we need to show that \(I\) is bounded. To this end, let \((h_{n})_{n\geq 1}\subset\mathcal{H}_{1}\) be a sequence such that there exist \(h\in\mathcal{H}_{1}\) and \(g\in\mathcal{H}_{2}\) with \(h_{n}\to h\) in \(\mathcal{H}_{1}\) and \(Ih_{n}\to g\) in \(\mathcal{H}_{2}\). This implies \(h_{n}\to h\) pointwise and \(h_{n}=Ih_{n}\to g\) pointwise, which in turn gives \(h=g\). The closed graph theorem, see e.g. (Megginson, 1998, Theorem 1.6.11), then shows that \(I\) is bounded. 

Applying Lemma F.7 twice shows that RKHSs \(\mathcal{H}_{1}\) and \(\mathcal{H}_{2}\) with \(\mathcal{H}_{1}=\mathcal{H}_{2}\) automatically have \(C\)-equivalent norms for a suitable constant \(C\geq 1\). The following result investigates the corresponding kernels.

**Proposition F.8** (Equivalent kernels have equivalent kernel matrices).: _Let \(k,\tilde{k}:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) be kernels such that their RKHSs are equal as sets and the corresponding RKHS-norms are \(C\)-equivalent as defined in Definition F.4. Then, for any \(n\geq 1\) and any \(\bm{x}_{1},\ldots,\bm{x}_{n}\in\mathcal{X}\), the corresponding kernel matrices \(k(\bm{X},\bm{X}),\tilde{k}(\bm{X},\bm{X})\) are \(C^{2}\)-equivalent._

Proof.: Let \(i\in[n]\). For \(\bm{y}\in\mathbb{R}^{n}\) we have, using the notation of Lemma F.6:

\[\bm{y}^{\top}k(\bm{X},\bm{X})^{+}\bm{y}=\|f^{*}_{k,\bm{y}}\|_{\mathcal{H}_{k}}^ {2} \geq C^{-2}\|f^{*}_{k,\bm{y}}\|_{\mathcal{H}_{\tilde{k}}}^{2}\geq C^{-2}\|f^{* }_{k,\bm{y}}\|_{\mathcal{H}_{\tilde{k}}}^{2}=C^{-2}\bm{y}^{\top}\tilde{k}(\bm{X },\bm{X})^{+}\bm{y}\;.\]

Now, by the Courant-Fischer-Weyl theorem,

\[\lambda_{i}(k(\bm{X},\bm{X})^{+}) =\sup_{V:\dim V=i}\inf_{y\in V:\|y\|_{2}=1}y^{\top}k(\bm{X},\bm{ X})^{+}y\] \[\geq C^{-2}\sup_{V:\dim V=i}\inf_{y\in V:\|y\|_{2}=1}y^{\top} \tilde{k}(\bm{X},\bm{X})^{+}y\] \[=C^{-2}\lambda_{i}(\tilde{k}(\bm{X},\bm{X})^{+})\;.\]

By switching the roles of \(k\) and \(\tilde{k}\), we obtain that \(k(\bm{X},\bm{X})^{+}\) and \(\tilde{k}(\bm{X},\bm{X})^{+}\) are \(C^{2}\)-equivalent. By Lemma F.5\(k(\bm{X},\bm{X})\) and \(\tilde{k}(\bm{X},\bm{X})\) are then also \(C^{2}\)-equivalent.

To prove Theorem 6 for arbitrary input distributions \(P_{X}\) with lower and upper bounded densities, we need the following theorem investigating the corresponding eigenvalues of the integral operator.

**Lemma F.9** (Integral operators for equivalent densities have equivalent eigenvalues).: _Let \(k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) be a kernel and let \(\mu,\nu\) be finite measures on \(\mathcal{X}\) whose support is \(\mathcal{X}\) such that \(\nu\) has an lower and upper bounded density w.r.t. \(\mu\). Then, \(\lambda_{i}(T_{k,\nu})=\Theta(\lambda_{i}(T_{k,\mu}))\)._

Proof.: Let \(p\) be such an upper bounded density, that is, \(\,\mathrm{d}\nu=p\,\mathrm{d}\mu\) and there exist \(c,C>0\) such that \(c\leq p(\bm{x})\leq C\) for all \(\bm{x}\in\mathcal{X}\). For \(f\in L_{2}(\nu)\), we have

\[\|p\cdot f\|_{L_{2}(\mu)}^{2}=\int f^{2}p^{2}\,\mathrm{d}\mu\leq C\int f^{2}p \,\mathrm{d}\mu=C\int f^{2}\,\mathrm{d}\nu=C\|f\|_{L_{2}(\nu)}^{2}\;.\]

Hence, the linear operator

\[A:L_{2}(\nu)\to L_{2}(\mu),f\mapsto p\cdot f\]

is well-defined and continuous. It is also easily verified that \(A\) is bijective. Moreover, we have

\[\langle Af,Af\rangle_{L_{2}(\mu)}=\int f^{2}p^{2}\,\mathrm{d}\mu\geq c\int f^{2 }p\,\mathrm{d}\mu=c\int f^{2}\,\mathrm{d}\nu=c\langle f,f\rangle_{L_{2}(\nu) }\;.\]

and

\[\langle f,T_{k,\nu}f\rangle_{L_{2}(\nu)}=\int\int p(\bm{x})f(\bm{x})k(\bm{x}, \bm{x}^{\prime})f(\bm{x}^{\prime})p(\bm{x}^{\prime})\,\mathrm{d}\mu(\bm{x})\, \mathrm{d}\mu(\bm{x}^{\prime})=\langle Af,T_{k,\mu}Af\rangle_{L_{2}(\mu)}\;.\]

Since \(T_{k,\mu}\) and \(T_{k,\nu}\) are compact, self-adjoint, and positive, we can use the Courant-Fischer minmax principle for operators (see e.g. Bell, 2014) to obtain

\[\lambda_{i}(T_{k,\nu}) =\max_{\begin{subarray}{c}V\subseteq L_{2}(\nu)\text{ subspace } \end{subarray}}\min_{f\in V\setminus\{0\}}\frac{\langle f,T_{k,\nu}f\rangle_{L_{2}( \nu)}}{\langle f,f\rangle_{L_{2}(\nu)}}\] \[\geq c_{\begin{subarray}{c}V\subseteq L_{2}(\nu)\text{ subspace } \end{subarray}}\min_{f\in V\setminus\{0\}}\frac{\langle Af,T_{k,\mu}Af\rangle_{L_{2} (\mu)}}{\langle Af,Af\rangle_{L_{2}(\mu)}}\] \[=c_{\begin{subarray}{c}V\subseteq L_{2}(\nu)\text{ subspace } \end{subarray}}\min_{g\in V\setminus\{0\}}\frac{\langle g,T_{k,\mu}g\rangle_{L_{2} (\mu)}}{\langle g,g\rangle_{L_{2}(\mu)}}\] \[=c\lambda_{i}(T_{k,\mu})\;.\]

Here, we have used that since \(A\) is bijective, the subspaces \(AV\) for \(\dim(V)=i\) are exactly the \(i\)-dimensional subspaces of \(L_{2}(\mu)\). Our calculation above shows that \(\lambda_{i}(T_{k,\mu})\leq O(\lambda_{i}(T_{k,\nu}))\). Since \(\,\mathrm{d}\mu=\frac{1}{p}\,\mathrm{d}\nu\) with the lower and upper bounded density \(1/p\), we can reverse the roles of \(\nu\) and \(\mu\) to also obtain \(\lambda_{i}(T_{k,\nu})\leq O(\lambda_{i}(T_{k,\mu}))\), which proves the claim. 

**Lemma F.10** (Integral operators of equivalent kernels have equivalent eigenvalues).: _Let \(k,\tilde{k}:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) be bounded kernels with RKHSs \(\mathcal{H}\) and \(\tilde{\mathcal{H}}\) satisfying \(\mathcal{H}=\tilde{\mathcal{H}}\) as sets. Moreover, let \(C\geq 1\) be a constant such that the corresponding RKHS-norms are \(C\)-equivalent and let \(\nu\) be a finite measure on \(\mathcal{X}\). If there exist constants \(q>0\) and \(c>0\) with_

\[\lambda_{i}(T_{k,\nu})\leq ci^{-q}\,,\qquad\qquad i\geq 1,\]

_then we also have_

\[\lambda_{i}(T_{\tilde{k},\nu})\leq c\cdot C^{2}\cdot K_{q}\cdot i^{-q}\,,\qquad \qquad i\geq 1,\]

_where \(K_{q}>0\) is a constant only depending on \(q\)._

Proof.: We follow the ideas outlined in (Steinwart, 2017, Section 3). To this end, let \(I_{k,\nu}:\mathcal{H}\to L_{2}(\nu)\) be the embedding \(h\mapsto[h]_{\sim}\), which is defined and compact since \(k\) is bounded and \(\nu\) is finite, see e.g. (Steinwart and Scovel, 2012, Lemma 2.3). We write \(S_{k,\nu}:=I_{k,\nu}^{*}\) for its adjoint, which in turn gives \(I_{k,\nu}=S_{k,\nu}^{*}\). Then (Steinwart and Scovel, 2012, Lemma 2.2) shows \(T_{k,\nu}=S_{k,\nu}^{*}\circ S_{k,\nu}\). We denote the \(i\)-th (dyadic) entropy number of \(I_{k,\nu}\) by \(\varepsilon_{i}(I_{k,\nu})\), see e.g. Carl and Stephani (1990), Edmunds and Triebel, 1996, Chapter 1.3.1), or (Steinwart and Christmann, 2008, Chapter 6) for a definition3. Moreover, we denote the \(i\)-approximation and singular numbers of \(I_{k,\nu}\) by \(a_{i}(I_{k,\nu})\), respectively \(s_{i}(I_{k,\nu})\). Since \(I_{k,\nu}\) compactly acts between Hilbert spaces, we then have \(a_{i}(I_{k,\nu})=s_{i}(I_{k,\nu})\), see e.g. (Pietsch, 1987, Chapter 2.11). This implies

Footnote 3: Usually, dyadic entropy numbers are denoted by \(e_{i}(\cdot)\), but since this symbol is already used for eigenfunctions, we use \(\varepsilon_{i}(\cdot)\) instead.

\[\lambda_{i}(T_{k,\nu})=a_{i}^{2}(I_{k,\nu})\] (F.8)

for all \(i\geq 1\) by the very definition of singular numbers. Finally, analogous definitions and considerations are made for the kernel \(\tilde{k}\). From \(C^{-1}\|\cdot\|_{\mathcal{H}}\leq\|\cdot\|_{\tilde{\mathcal{H}}}\leq C\|\cdot \|_{\mathcal{H}}\) we then conclude that

\[C^{-1}\varepsilon_{i}(I_{k,\nu})\leq\varepsilon_{i}(I_{\tilde{k},\nu})\leq C \varepsilon_{i}(I_{k,\nu})\,,\qquad\qquad i\geq 1,\] (F.9)

by the multiplicativity of entropy numbers, see e.g. (Edmunds and Triebel, 1996, Chapter 1.3.1).

Now, (F.8) and our eigenvalue assumption yield

\[a_{i}(I_{k,\nu})\leq\sqrt{c}\cdot i^{-q/2}\,,\qquad\qquad i\geq 1\,,\]

and Carl's inequality, see e.g. (Carl and Stephani, 1990, Theorem 3.1.1) then gives

\[\varepsilon_{i}(I_{k,\nu})\leq\sqrt{c}\cdot\tilde{K}_{q}\cdot i^{-q/2}\,, \qquad\qquad i\geq 1\,,\]

where \(\tilde{K}_{q}\) is a constant only depending on \(q\). By (Carl and Stephani, 1990, Inequality (3.0.9)) and (F.9) we then obtain

\[a_{i}(I_{\tilde{k},\nu})\leq 2\varepsilon_{i}(I_{\tilde{k},\nu})\leq 2C \varepsilon_{i}(I_{k,\nu})\leq 2C\sqrt{c}\cdot\tilde{K}_{q}\cdot i^{-q/2}\,, \qquad\qquad i\geq 1\,.\]

Another application of (F.8) then yields the assertion for \(K_{q}:=4\tilde{K}_{q}^{2}\). 

### Kernel matrix eigenvalue bounds

For upper bounds on the eigenvalues of kernel matrices, we use the following result:

**Proposition F.11** (Kernel matrix eigenvalue upper bound in expectation).: _For \(m\geq 1\), we have_

\[\mathbb{E}_{\bm{X}}\sum_{i=m}^{n}\lambda_{i}(k(\bm{X},\bm{X})/n)\leq\sum_{i=m }^{\infty}\lambda_{i}(T_{k})\;.\] (F.10)

Proof.: Theorem 7.29 in Steinwart and Christmann (2008) shows that

\[\mathbb{E}_{D\sim\mu^{n}}\sum_{i=m}^{\infty}\lambda_{i}(T_{k,D})\leq\sum_{i=m }^{\infty}\lambda_{i}(T_{k,\mu})\;,\] (F.11)

where \(T_{k,\mu}:L_{2}(\mu)\to L_{2}(\mu),f\mapsto\int k(x,\cdot)f(x)\,\mathrm{d}\mu (x)\) is the integral operator corresponding to the measure \(\mu\) and \(T_{k,D}\) is the corresponding discrete version thereof. We set \(\mu\coloneqq P_{X}\) and need to show that \(k(\bm{X},\bm{X})/n\) has the same eigenvalues as \(T_{k,D}\) if \(D\) and \(\mathcal{X}\) contain the same data points \(\bm{x}_{1},\dots,\bm{x}_{n}\). Consider a fixed \(D\). Then, we can write \(T_{k,D}(f)=n^{-1}ABf\), where

\[A:\mathbb{R}^{n}\to L_{2}(D),\bm{v} \mapsto\sum_{i=1}^{n}v_{i}k(\bm{x}_{i},\cdot)\] \[B:L_{2}(D)\to\mathbb{R}^{n},f \mapsto(f(\bm{x}_{1}),\dots,f(\bm{x}_{n}))^{\top}\;.\]

Then, \(k(\bm{X},\bm{X})/n\) is the matrix representation of \(n^{-1}BA\) with respect to the standard basis of \(\mathbb{R}^{n}\). But \(AB\) and \(BA\) have the same non-zero eigenvalues, which means that

\[\sum_{i=m}^{n}\lambda_{i}(k(\bm{X},\bm{X})/n)=\sum_{i=m}^{\infty}\lambda_{i}(T_ {k,D})\;,\]

from which the claim follows.

To obtain a lower bound, we want to leverage the lower bound by Buchholz (2022) for a certain radial basis function kernel with data generated from an open subset of \(\mathbb{R}^{d}\). However, we want to consider different kernels and distributions on the whole sphere. The following theorem bridges the gap by going to subsets of the data on a sphere cap, projecting them to \(\mathbb{R}^{d}\), and using the kernel equivalence results from Appendix F.2:

**Theorem F.12** (Kernel matrix eigenvalue lower bound for Sobolev kernels on the sphere).: _Let \(k\) be a kernel on \(\mathbb{S}^{d}\) such that its RKHS \(\mathcal{H}_{k}\) is equivalent to a Sobolev space \(H^{s}(\mathbb{S}^{d})\) with smoothness \(s>d/2\). Moreover, let \(P_{X}\) be a probability distribution on \(\mathbb{S}^{d}\) with lower and upper bounded density. Let the rows of \(\bm{X}\in\mathbb{R}^{n\times d}\) are drawn independently from \(P_{X}\). Then, for \(\varepsilon\in(0,1/20)\), there exists a constant \(c>0\) and \(n_{0}\in\mathbb{N}\) such that for all \(n\geq n_{0}\),_

\[\lambda_{m}(k(\bm{X},\bm{X})/n)\geq cn^{-2s/d}\]

_holds with probability \(\geq 4/5\) for all \(m\in\mathbb{N}\) with \(1\leq m\leq(1-11\varepsilon)n\)._

Proof.: We can choose a suitably large sphere cap \(T\) such that \(P_{X}(T)\geq 1-\varepsilon\). Define the conditional distribution \(P_{T}(\cdot)\coloneqq P_{X}(\cdot|T)\). Out of the points \(\bm{X}=(\bm{x}_{1},\dots,\bm{x}_{n})\), we can consider the submatrix \(\bm{X}_{T}=(\bm{x}_{i_{1}},\dots,\bm{x}_{i_{N}})^{\top}\) of the points lying in \(T\). Conditioned on \(N\), these points are i.i.d. samples from \(P_{T}\). Moreover, by applying Markov's inequality to a Bernoulli distribution, we obtain \(N\geq(1-10\varepsilon)n\) with probability \(\geq 9/10\). We fix a value of \(N\geq(1-10\varepsilon)n\) in the following and condition on it.

We denote the centered unit ball in \(\mathbb{R}^{d}\) by \(B_{1}(\mathbb{R}^{d})\). Using a construction as in Lemma E.1, we can transport \(k\) and \(P_{T}\) from \(T\) to the unit ball \(B_{1}(\mathbb{R}^{d})\) using a rescaled stereographic projection feature map \(\phi\), such that we obtain a kernel \(k_{\phi}\) and a distribution \(P_{\phi}=(P_{T})_{\phi}\) on \(B_{1}(\mathbb{R}^{d})\) that generate the same distribution of kernel matrices as \(k\) with \(P_{T}\), and such that \(\mathcal{H}_{k_{\phi}}\cong H^{s}(B_{1}(\mathbb{R}^{d}))\). The rows of \(\bm{X}_{\phi}\coloneqq\phi(\bm{X}_{T})\) are i.i.d. samples from \(P_{\phi}\). Moreover, we know that \(P_{\phi}\) has an lower and upper bounded density w.r.t. the Lebesgue measure on \(B_{1}(\mathbb{R}^{d})\).

In order to apply the results from Buchholz (2022), we define a translation-invariant reference kernel on \(\mathbb{R}^{d}\) through the Fourier transform

\[\hat{k}_{\mathrm{ref}}(\xi)=(1+|\xi|^{2})^{-2s}\;,\]

see Eq. (3) in Buchholz (2022). The RKHS of \(k_{\mathrm{ref}}\) on \(\mathbb{R}^{d}\) is equivalent to the Sobolev space \(H^{s}(\mathbb{R}^{d})\). Therefore, the RKHS of \(k_{\mathrm{ref}}|_{B_{1}(\mathbb{R}^{d}),B_{1}(\mathbb{R}^{d})}\) is \(H^{s}(B_{1}(\mathbb{R}^{d}))\), cf. the remarks in Appendix B.1 and Lemma F.7.

Now, let \(1\leq m\leq(1-11\varepsilon)n\), which implies

\[1\leq m\leq(1-11\varepsilon)n\leq(1-\varepsilon)(1-10\varepsilon)n\leq(1- \varepsilon)N\;.\]

We apply Theorem 12 by Buchholz (2022) with bandwidth \(\gamma=1\) and \(\alpha=2s\) to \(\lambda_{m}\) and obtain with probability at least \(1-2/N\):

\[\lambda_{m}(k_{\mathrm{ref}}(\bm{X}_{\phi},\bm{X}_{\phi}))^{-1} \leq c_{3}\left(\frac{N^{2(\alpha-d)/d}}{(N-m)^{(\alpha-d)/d}}+1 \right)\leq c_{3}\left(\frac{N^{2(\alpha-d)/d}}{(\varepsilon N)^{(\alpha-d)/d} }+1\right)\] \[\leq c_{4}(n^{\alpha/d-1}+1)\]

as long as \(N\) is large enough such that \((1-\varepsilon)N<N-32\ln(N)\), which is the case if \(n\) is large enough. Here, the constant \(c_{3}\) from Buchholz (2022) does not depend on \(N\) or \(m\), but only on \(\alpha\), \(d\), and the upper and lower bounds on the density, which in our case depend on \(\varepsilon\) through the choice of \(T\). Since \(\alpha=2s>d\), we have \(n^{\alpha/d-1}>1\) and therefore

\[\lambda_{m}(k_{\mathrm{ref}}(\bm{X}_{\phi},\bm{X}_{\phi})/n)\geq c_{5}n^{- \alpha/d}=c_{5}n^{-2s/d}\;.\]

Now, we want to translate this to the kernel \(k\). Since the RKHSs of \(k_{\phi}\) and \(k_{\mathrm{ref}}\) on \(B_{1}(\mathbb{R}^{d})\) are both equivalent to \(H^{s}(B_{1}(\mathbb{R}^{d}))\), the kernels themselves are \(C\)-equivalent for some constant \(C\geq 1\) as defined in Definition F.4. Therefore, Proposition F.8 shows that the corresponding kernel matrices are \(C^{2}\)-equivalent, which implies

\[\lambda_{m}(k_{*,\phi}(\bm{X}_{\phi},\bm{X}_{\phi})/n)\geq c_{5}C^{-2}n^{-2s/d }\;.\]By Cauchy's interlacing theorem, we therefore have

\[\lambda_{m}(k_{*}(\bm{X},\bm{X})/n)\geq\lambda_{m}(k_{*}(\bm{X}_{T},\bm{X}_{T})/n )=\lambda_{m}(k_{*,\phi}(\bm{X}_{\phi},\bm{X}_{\phi})/n)\geq c_{5}C^{-2}n^{-2s/ d}\;.\]

Denoting the event where \(\lambda_{m}(k_{*}(\bm{X},\bm{X})/n)\geq c_{5}C^{-2}n^{-2s/d}\) by \(A\), we thus have

\[P(A) =P(A|N\geq(1-10\varepsilon)n)P(N\geq(1-10\varepsilon)n)\geq\frac{ 9}{10}P(A|N\geq(1-10\varepsilon)n)\] \[=\frac{9}{10}\sum_{\hat{N}=\lceil(1-10\varepsilon)n\rceil}^{n}P(N =\hat{N}|N\geq(1-10\varepsilon)n)P(A|N=\hat{N})\] \[\geq\frac{9}{10}\sum_{\hat{N}=\lceil(1-10\varepsilon)n\rceil}^{n}P (N=\hat{N}|N\geq(1-10\varepsilon)n)(1-2/N)\] \[\geq\frac{9}{10}\left(1-\frac{2}{(1-10\varepsilon)n}\right)\sum_{ \hat{N}=\lceil(1-10\varepsilon)n\rceil}^{n}P(N=\hat{N}|N\geq(1-10\varepsilon)n)\] \[=\frac{9}{10}\left(1-\frac{2}{(1-10\varepsilon)n}\right)\geq\frac {4}{5}\;,\]

where the last step holds for sufficiently large \(n\). 

### Spectral lower bound for dot-product kernels on the sphere

An application of the spectral generalization bound in Proposition 5 requires a lower bound on eigenvalues of the kernel matrix \(k_{*}(\bm{X},\bm{X})\). To achieve this, we need to understand the properties of the convolution kernel \(k_{*}\). Since the eigenvalues of \(T_{k_{*},P_{X}}\) are the squared eigenvalues of \(T_{k_{*},P_{X}}\), one might hope that if \(\mathcal{H}_{k}\) is equivalent to a Sobolev space \(H^{s}\), then \(\mathcal{H}_{k_{*}}\) is equivalent to a Sobolev space \(H^{2s}\). Unfortunately, this is not the case in general, as \(\mathcal{H}_{k_{*}}\) might be a smaller space that involves additional boundary conditions (Schaback, 2018). However, perhaps since the sphere is a manifold without boundary, the desired characterization of \(\mathcal{H}_{k_{*}}\) holds for dot-product kernels on the sphere:

**Lemma F.13** (RKHS of convolution kernels).: _Let \(k\) be a dot-product kernel on \(\mathbb{S}^{d}\) such that its RKHS \(\mathcal{H}_{k}\) is equivalent to a Sobolev space \(H^{s}(\mathbb{S}^{d})\) with smoothness \(s>d/2\), and let \(P_{X}\) be a distribution on \(\mathbb{S}^{d}\) with lower and upper bounded density. Then, the RKHS \(\mathcal{H}_{k_{*}}\) of the kernel_

\[k_{*}:\mathbb{S}^{d}\times\mathbb{S}^{d}\to\mathbb{R},k_{*}(\bm{x},\bm{x}^{ \prime})\coloneqq\int k(\bm{x},\bm{x}^{\prime\prime})k(\bm{x}^{\prime\prime}, \bm{x}^{\prime})\,\mathrm{d}P_{X}(\bm{x}^{\prime\prime})\]

_is equivalent to the Sobolev space \(H^{2s}(\mathbb{S}^{d})\)._

Proof.: Define

\[k_{*,\mathrm{unif}}(\bm{x},\bm{x}^{\prime})=\int k(\bm{x},\bm{x}^{\prime\prime })k(\bm{x}^{\prime\prime},\bm{x}^{\prime})\,\mathrm{d}\mathcal{U}(\mathbb{S}^{ d})(\bm{x}^{\prime\prime})\;.\]

For the corresponding integral operator, we have

\[T_{k_{*,\mathrm{unif}}\,\mathcal{U}(\mathbb{S}^{d})}=T_{k,\mathcal{U}(\mathbb{ S}^{d})}^{2}\;.\]

This means that the corresponding eigenvalues are the squares of the eigenvalues of the corresponding integral operator of \(k\). Especially, we obtain the Mercer representations

\[k(\bm{x},\bm{x}^{\prime}) =\sum_{l=0}^{\infty}\mu_{l}\sum_{i=1}^{N_{l,d}}Y_{l,i}(\bm{x})Y_{l,i}(\bm{x}^{\prime})\;,\] \[k_{*,\mathrm{unif}}(\bm{x},\bm{x}^{\prime}) =\sum_{l=0}^{\infty}\mu_{l}^{2}\sum_{i=1}^{N_{l,d}}Y_{l,i}(\bm{x}) Y_{l,i}(\bm{x}^{\prime})\;,\]

where Lemma B.1 yields \(\mu_{l}=\Theta((l+1)^{-2s})\), hence \(\mu_{l}^{2}=\Theta((l+1)^{-4s})\) and hence \(\mathcal{H}_{k_{*,\mathrm{unif}}}\cong H^{2s}(\mathbb{S}^{d})\).

Next, we show the equality of the ranges of the integral operators:

\[R(T_{k,\mathcal{U}(\mathbb{S}^{d})})=R(T_{k,P_{X}})\.\]

Let \(p_{X}\) be a density of \(P_{X}\) w.r.t. the uniform distribution \(\mathcal{U}(\mathbb{S}^{d})\). If \(f\in R(T_{k,\mathcal{U}(\mathbb{S}^{d})})\), there exists \(g\in L_{2}(\mathcal{U}(\mathbb{S}^{d}))\) with \(f=T_{k,\mathcal{U}(\mathbb{S}^{d})}g\). But then, since \(p_{X}\) is lower bounded, we have \(g/p_{X}\in L_{2}(P_{X})\) and therefore

\[f=T_{k,P_{X}}(g/p_{X})\in R(T_{k,P_{X}})\.\]

An analogous argument shows that \(R(T_{k,P_{X}})\subseteq R(T_{k,\mathcal{U}(\mathbb{S}^{d})})\) since \(p_{X}\) is upper bounded.

The equality of the ranges yields for the RKHSs (as sets)

\[\mathcal{H}_{k_{*,\mathrm{unif}}}=R(T_{k,\mathcal{U}(\mathbb{S}^{d})})=R(T_{k,P_{X}})=\mathcal{H}_{k_{*}}\,\]

Applying Lemma F.7 twice then shows \(\mathcal{H}_{k_{*}}\cong H^{2s}(\mathbb{S}^{d})\). 

**Theorem 6** (**Inconsistency for Sobolev dot-product kernels on the sphere**).: _Let \(k\) be a dot-product kernel on \(\mathbb{S}^{d}\), i.e., a kernel of the form \(k(\bm{x},\bm{x}^{\prime})=\kappa(\langle\bm{x},\bm{x}^{\prime}\rangle)\), such that its RKHS \(\mathcal{H}_{k}\) is equivalent to a Sobolev space \(H^{s}(\mathbb{S}^{d})\), \(s>d/2\). Moreover, let \(P\) be a distribution on \(\mathbb{S}^{d}\times\mathbb{R}\) such that \(P_{X}\) has a lower and upper bounded density w.r.t. the uniform distribution \(\mathcal{U}(\mathbb{S}^{d})\), and such that \(\mathrm{Var}(y|\bm{x})\geq\sigma^{2}>0\) for \(P_{X}\)-almost all \(\bm{x}\in\mathbb{S}^{d}\). Then, for every \(C>0\), there exists \(c>0\) independent of \(\sigma^{2}\) such that for all \(n\geq 1\), \(t\in(C^{-1}n^{2s/d},\infty]\), and \(\rho\in[0,Cn^{-2s/d})\), the expected excess risk satisfies_

\[\mathbb{E}_{D}R_{P}(f_{t,\rho})-R_{P}^{*}\geq c\sigma^{2}>0\.\]

Proof.: **Step 0: Preparation.** Since the Sobelev space \(H^{2s}(\mathbb{S}^{d})\) is dense in the space of continuous functions \(\mathbb{S}^{d}\to\mathbb{R}\), the kernel \(k\) is universal. Applying (Steinwart and Christmann, 2008, Corollary 5.29 and Corollary 5.34) for the least squares loss thus shows that \(k\) is strictly positive definite. If we have mutually distinct \(\bm{x}_{1},\ldots,\bm{x}_{n}\), the corresponding Gram matrix \(k((\bm{x}_{i},\bm{x}_{j}))_{i,j=1}^{n}\) is therefore invertible. Now, our assumptions on \(P\) guarantee that \(\bm{X}\) consists almost surely of mutually distinct observations, and therefore \(k(\bm{X},\bm{X})\) is almost surely invertible.

By Proposition 5, we know that

\[\mathbb{E}_{D}\mathcal{R}_{P}(f_{\bm{X},\bm{y},t,\rho})-\mathcal{ R}_{P}^{*} \geq\frac{\sigma^{2}}{n}\sum_{i=1}^{n}\mathbb{E}_{\bm{X}}\frac{ \lambda_{i}(k_{*}(\bm{X},\bm{X})/n)\left(1-e^{-2t(\lambda_{i}(k(\bm{X},\bm{X})/ n)+\rho)}\right)^{2}}{(\lambda_{i}(k(\bm{X},\bm{X})/n)+\rho)^{2}}\] \[\geq\frac{\sigma^{2}}{n}\sum_{i=1}^{n}\mathbb{E}_{\bm{X}}\frac{ \lambda_{i}(k_{*}(\bm{X},\bm{X})/n)\left(1-e^{-2C^{-1}n^{2s/d}(\lambda_{i}(k( \bm{X},\bm{X})/n)+0)}\right)^{2}}{(\lambda_{i}(k(\bm{X},\bm{X})/n)+Cn^{-2s/d})^ {2}}\] \[\geq c_{n}\sigma^{2}\]

for a suitable constant \(c_{n}>0\) depending on \(n\) but not on \(\sigma^{2},t,\rho\), since the kernel matrix eigenvalues are nonzero almost surely. It is therefore sufficient to show the desired statement (with \(c\) independent of \(n,\sigma^{2},t,\rho\)) for sufficiently large \(n\).

In the following, we assume \(n\geq 40\) and set \(\varepsilon\coloneqq 1/100\).

**Step 1: Eigenvalue decay for the integral operator.** From Lemma B.1, we know that

\[\lambda_{i}(T_{k,\mathcal{U}(\mathbb{S}^{d})})=\Theta(i^{-2s/d})\.\]

Therefore, by Lemma F.9, we know that

\[\lambda_{i}(T_{k,P_{X}})=\Theta(i^{-2s/d})\.\]

**Step 2: Eigenvalue upper bound.** Next, we want to upper-bound suitable eigenvalues of the form \(\lambda_{i}(k(\bm{X},\bm{X})/n)\) using Proposition F.11. Using Step 1, we derive

\[\sum_{i=m}^{\infty}\lambda_{i}(T_{k,P_{X}})\leq C_{1}\sum_{i=m}^{\infty}i^{-2s/ d}\leq C_{2}\int_{m}^{\infty}x^{-2s/d}\,\mathrm{d}x=C_{3}m^{1-2s/d}\]with constants independent of \(m\geq 1\). For sufficiently large \(n\), we can choose \(m\in\mathbb{N}_{\geq 1}\) such that \(\varepsilon n\leq m\leq 2\varepsilon n\). Then, Proposition F.11 yields

\[\mathbb{E}_{\bm{X}}\sum_{i=m}^{n}\lambda_{i}(k(\bm{X},\bm{X})/n)\leq\sum_{i=m}^ {\infty}\lambda_{i}(T_{k})\leq C_{3}m^{1-2s/d}\leq C_{4}n^{1-2s/d}\;.\]

Since \(\mathbb{E}_{\bm{X}}\lambda_{i}(k(\bm{X},\bm{X})/n)\) is decreasing with \(i\), we have for \(i\geq 4\varepsilon n\geq 2m\):

\[\mathbb{E}_{\bm{X}}\lambda_{i}(k(\bm{X},\bm{X})/n)\leq C_{4}n^{1-2s/d}/m\leq C _{5}n^{-2s/d}\leq C_{6}\lambda_{i}(T_{k,P_{X}})\;.\]

**Step 3: Eigenvalue lower bounds.** From Lemma F.13, we know that \(\mathcal{H}_{k_{*}}\cong H^{2s}(\mathbb{S}^{d})\). Therefore, we can apply Lemma F.13 to both \(k\) and \(k_{*}\) and obtain for sufficiently large \(n\) and suitable constants \(c_{1},c_{2}>0\) that

\[\lambda_{i}(k(\bm{X},\bm{X})/n) \geq c_{1}n^{-2s/d}\] \[\lambda_{i}(k_{*}(\bm{X},\bm{X})/n) \geq c_{2}n^{-4s/d}\]

individually hold with probability \(\geq 4/5\) for all \(i\in\mathbb{N}\) with \(1\leq i\leq(1-11\varepsilon)n\). By the union bound, both bounds hold at the same time with probability \(\geq 3/5\).

**Step 4: Final result.** Now, using the value of \(m\) from Step 2, consider an index \(i\) with \(2m\leq i\leq(1-11\varepsilon)n\). Since \(2m\leq 4\varepsilon n\) and \(\varepsilon=1/100\), there are at least \(n/2\) such indices. By combining Step 3 and Step 1, we have

\[\lambda_{i}(k(\bm{X},\bm{X})/n) \geq c_{3}\lambda_{i}(T_{k,P_{X}})\] \[\lambda_{i}(k_{*}(\bm{X},\bm{X})/n) \geq c_{4}\lambda_{i}(T_{k,P_{X}})^{2}\]

with probability \(\geq 3/5\). By applying Markov's inequality to Step 2, we obtain

\[\lambda_{i}(k(\bm{X},\bm{X})/n)\leq 10C_{6}\lambda_{i}(T_{k,P_{X}})\]

with probability \(\geq 9/10\). Therefore, by the union bound, all three inequalities hold simultaneously with probability \(\geq 1/2\). Moreover, for \(q=2s/d\), we have \(\lambda_{i}(T_{k,P_{X}})\geq c_{5}i^{-q}\) by Step 1. We can thus apply the first lower bound from Theorem F.2 to obtain

\[\mathbb{E}_{D}R_{P}(f_{\bm{X},\bm{y},t,\rho})-R_{P}^{*} \geq c^{\prime}\sigma^{2}\frac{1}{1+(\rho+t^{-1})n^{2s/d}}\cdot \frac{|\mathcal{I}(n)|}{n}\] \[\geq c^{\prime}\sigma^{2}\frac{1}{1+(Cn^{-2s/d}+Cn^{-2s/d})n^{2s/ d}}\cdot\frac{n/2}{n}\] \[=\frac{c^{\prime}}{2+2C}\sigma^{2}\;.\qed\]

## Appendix G Proof of Theorem 8

Here we denote the solution of kernel ridge regression on \(D\) with the kernel function \(k\) and regularization parameter \(\rho>0\) as

\[\hat{f}_{\rho}^{k}(\bm{x})=k(\bm{x},\bm{X})\left(k(\bm{X},\bm{X})+\rho\bm{I} \right)^{-1}\bm{y},\]

and write \(\hat{f}_{0}^{k}(\bm{x})=k(\bm{x},\bm{X})k(\bm{X},\bm{X})^{+}\bm{y}\) for the minimum-norm interpolant in the RKHS of \(k\).

While Theorem 1 states that overfitting kernel ridge regression using Sobolev kernels is always inconsistent as long as the derivatives remain bounded by the derivatives of the minimum-norm interpolant of the fixed kernel (Assumption (N)), here we show that consistency over a large class of distributions is achievable by designing a kernel sequence, which can have Sobolev RKHS, that consists of a smooth component for generalization and a spiky component for interpolation.

Recall that \(\tilde{k}\) denotes any universal kernel function for the smooth component, and \(\breve{k}_{\gamma}\) denotes the kernel function of the spiky component with bandwidth \(\gamma\). Then we define the _\(\rho\)-regularized spiky-smooth kernel with spike bandwidth \(\gamma\)_ as

\[k_{\rho,\gamma}(\bm{x},\bm{x}^{\prime})=\tilde{k}(\bm{x},\bm{x}^{\prime})+\rho \cdot\breve{k}_{\gamma}(\bm{x},\bm{x}^{\prime}).\]

Let \(B_{t}(\bm{x})\coloneqq\{\bm{y}\in\mathbb{R}^{d}\mid|\bm{x}-\bm{y}|\leq t\}\) denote the Euclidean ball of radius \(t\geq 0\) around \(\bm{x}\in\mathbb{R}^{d}\).

* There exists a constant \(\beta_{X}>0\) and a continuous function \(\phi:[0,\infty)\to[0,1]\) with \(\phi(0)=0\) such that \(P_{X}(B_{t}(\bm{x}))\leq\phi(t)=O(t^{\beta_{X}})\) for all \(\bm{x}\in\Omega\) and all \(t\geq 0\).

The kernel \(\check{k}_{\gamma}\) of the spiky component should fulfill the following weak assumption on its decay behaviour. For example, Laplace, Matern, and Gaussian kernels all fulfill Assumption (SK).

* There exists a function \(\varepsilon:(0,\infty)\times[0,\infty)\to[0,1]\) such that for any bandwidth \(\gamma>0\) and any \(\delta>0\) it holds that
* \(\varepsilon(\gamma,\delta)\) is monotonically increasing in \(\gamma\),
* For all \(\bm{x},\bm{y}\in\Omega\), if \(|\bm{x}-\bm{y}|\geq\delta\) then \(|\check{k}_{\gamma}(\bm{x},\bm{y})|\leq\varepsilon(\gamma,\delta),\)
* For any rates \(\beta_{X},\beta_{k}>0\) there exists a rate \(\beta_{\gamma}>0\) such that, if \(\delta_{n}=\Omega(n^{-\beta_{X}})\) and \(\gamma_{n}=O(n^{-\beta_{\gamma}})\), then \(\varepsilon(\gamma_{n},\delta_{n})=O(n^{-\beta_{k}})\).

**Theorem G.1** (**Consistency of spiky-smooth ridgeless kernel regression)**.: _Assume that the training set \(D\) consists of \(n\) i.i.d. pairs \((\bm{x},y)\sim P\) such that the marginal \(P_{X}\) fulfills (D2) and \(\mathbb{E}y^{2}<\infty\). Let the kernel components satisfy:_

* \(\tilde{k}\) _denotes an arbitrary universal kernel, and_ \(\rho_{n}\to 0\) _and_ \(n\rho_{n}^{4}\to\infty\)_._
* \(\tilde{k}_{\gamma_{n}}\) _denotes a kernel function that fulfills Assumption (SK) with a sequence of positive bandwidths_ \((\gamma_{n})\) _fulfilling_ \(\gamma_{n}=O(\exp(-\beta n))\) _for some arbitrary_ \(\beta>0\)_._

_Then the minimum-norm interpolant of the \(\rho_{n}\)-regularized spiky-smooth kernel sequence \(k_{n}\coloneqq k_{\rho_{n},\gamma_{n}}\) is consistent for \(P\)._

**Remark G.2** (**Spike bandwidth scaling)**.: Under stronger assumptions on \(\phi\) and \(\varepsilon\) in assumptions (D2) and (SK), the spike bandwidths \(\gamma_{n}\) can be chosen to converge to \(0\) at a much slower rate. For example, if we choose \(\check{k}_{\gamma}\) to be the Laplace kernel, choosing bandwidths \(0<\gamma_{n}\leq\frac{\delta}{\beta\ln n}\) yields, for separated points \(|\bm{x}-\bm{y}|\geq\delta\),

\[\check{k}_{\gamma_{n}}(\bm{x},\bm{y})\leq\exp\left(-\frac{\delta}{\gamma_{n}} \right)\leq n^{-\beta}.\]

For probability measures with upper bounded Lebesgue density, we can choose \(\delta_{n}=n^{-\frac{2+\alpha}{d}}\) and \(\beta=\frac{9}{4}+\frac{\alpha}{2}\) for consistency or \(\beta=\frac{11}{4}+\frac{\alpha}{2}\) for optimal convergence rates, for any fixed \(\alpha>0\), in the proof of Theorem 8. Hence the Laplace kernel only requires a slow bandwidth decay rate of \(\gamma_{n}=\Omega\left(\frac{n^{-\frac{2+\alpha}{d}}}{\alpha\ln(n)}\right)\), where \(\alpha>0\) arbitrary. For the Gaussian kernel an analogous argument yields \(\gamma_{n}=\Omega\left(\frac{n^{-\frac{4+2\alpha}{d}}}{\alpha\ln(n)}\right)\). The larger the dimension \(d\), the slower the required bandwidth decay. 

**Remark G.3** (**Generalizations)**.: If one does not care about continuous kernels, one could simply take a Dirac kernel as the spike and then obtain consistency for all atom-free \(P_{X}\). However, we need a continuous kernel to be able to translate it to an activation function for the NTK. Beyond kernel regression, the spike component \(\check{k}_{\gamma}\) does not even need to be a kernel, it just needs to fulfill Assumption (SK) or a similar decay criterion. Then one could still use the 'quasi minimum-norm estimator' \(\bm{x}\mapsto(\tilde{k}+\rho_{n}\check{k}_{\gamma_{n}})(\bm{x},\bm{X})\cdot( \tilde{\bm{K}}+\rho_{n}\check{\bm{K}}_{\gamma_{n}})^{+}\bm{y}\). 

**Remark G.4** (**Consistency with a single kernel function)**.: Without resorting to kernel sequences as we do, there seems to be no rigorous proof showing that ridgeless kernel regression can be consistent in fixed dimension. In future work, can an analytical expression of such a kernel be found? According to the semi-rigorous results in Mallinar et al. (2022) a spectral decay like \(\lambda_{k}=\Theta(k^{-1}\cdot\log^{\alpha}(k))\), \(\alpha>1\) could lead to such a kernel. 

Proof of Theorem G.1.: Given any universal kernel, (Steinwart, 2001, Theorem 3.11 or Example 4.6) implies universal consistency of kernel ridge regression if \(\rho_{n}\to 0\) and \(n\rho_{n}^{4}\to\infty\). Hence, for any \(\varepsilon>0\) it holds that

\[\lim_{n\to\infty}P^{n}\left(D\in(\mathbb{R}^{d}\times\mathbb{R})^{n}\ |\ \ R_{P}(\check{f}_{\rho_{n}}^{\tilde{k}})-R_{P}(f_{P}^{*})=\mathbb{E}_{\bm{x}}( \check{f}_{\rho_{n}}^{\tilde{k}}(\bm{x})-f_{P}^{*}(\bm{x}))^{2}\geq(\varepsilon /2)^{2}\right)=0.\]

Due to the triangle inequality in \(L_{2}(P_{X})\), we know

\[R_{P}(\check{f}_{0}^{k_{n}})-R_{P}(f_{P}^{*})=\mathbb{E}_{\bm{x}}(\check{f}_{0 }^{k_{n}}(\bm{x})-f_{P}^{*}(\bm{x}))^{2}\]

[MISSING_PAGE_FAIL:47]

Using \(|\tilde{k}(\bm{x},\bm{X}_{i})|\leq 1\) for all \(i\in[n]\) yields the naive bound \(\|\tilde{k}(\bm{x},\bm{X})\|^{2}\leq n\).

Combining all terms in Eq. (G.1) yields its convergence to \(0\) as the product satifies the rate \(O(n^{4+\alpha}\rho_{n}^{-2}\varepsilon_{n}^{2})=o(1)\) with probability at least \(1-2n^{-\alpha}\).

**Bounding Eq. (G.2):**

The analysis below is restricted to the event of probability at least \(1-2n^{-\alpha}\), on which the bound on Eq. (G.1) holds.

Since \((n-1)\varepsilon_{n}\to 0\), for any \(C>1\) it holds for \(n\) large enough,

\[\rho_{n}\cdot\|(\tilde{\bm{K}}+\rho_{n}\tilde{\bm{K}}_{\gamma_{n}})^{-1}\| \leq\frac{\rho_{n}}{\lambda_{\min}(\tilde{\bm{K}})+\rho_{n}(1-(n-1) \varepsilon_{n})}\leq\frac{1}{(1-(n-1)\varepsilon_{n})}\leq C.\]

Finally we show \(\sup_{\bm{x}^{\prime}\in\mathbb{R}^{d}}\mathbb{E}_{\bm{x}}\tilde{k}_{\gamma_{n }}(\bm{x},\bm{x}^{\prime})^{2}\leq 2n^{-(2+\alpha)}\) for \(n\) large enough.

Fix an arbitrary \(\bm{x}^{\prime}\in\mathbb{R}^{d}\). Then by construction of \(\delta_{\alpha}(n)\) and \(\varepsilon_{n}\) it holds that

\[\mathbb{E}_{\bm{x}}\tilde{k}_{\gamma_{n}}(\bm{x},\bm{x}^{\prime })^{2} \leq 1\cdot P_{X}(\{\bm{x}\in\mathbb{R}^{d}:\;\tilde{k}_{\gamma_{n} }(\bm{x},\bm{x}^{\prime})^{2}\geq\varepsilon_{n}^{2}\})+\varepsilon_{n}^{2}\] \[\leq P_{X}(\{\bm{x}\in\mathbb{R}^{d}:\;|\bm{x}-\bm{x}^{\prime}|< \delta_{\alpha}(n)\})+\varepsilon_{n}^{2}\] \[\leq\phi(\delta_{\alpha}(n))+\varepsilon_{n}^{2}\leq n^{-(2+ \alpha)}+\varepsilon_{n}^{2}.\]

Since \(\varepsilon_{n}^{2}=o(\rho_{n}^{2}n^{-4-\alpha})\), we get \(\mathbb{E}_{\bm{x}}\tilde{k}_{\gamma_{n}}(\bm{x},\bm{x}^{\prime})^{2}\leq 2n^ {-(2+\alpha)}\) for \(n\) large enough.

Combining all terms in Eq. (G.2) yields its convergence to \(0\) with the rate \(O(n^{-(2+\alpha)}\cdot 1\cdot n^{1+\alpha})=O(n^{-1})\) with probability at least \(1-2n^{-\alpha}\), which concludes the proof. 

The following theorem shows that the minimum-norm interpolants of the spiky-smooth kernel sequence can achieve optimal convergence rates for Sobolev target functions, as long as \(\rho_{n}\) is properly chosen. We therefore introduce Assumption (D3), which resembles Assumption (D1) but allows more general target functions \(f^{*}\in H^{s^{*}}(\Omega)\backslash\{0\}\), \(s^{*}>0\), that may lie outside of the RKHS.

* Let \(\Omega=\mathbb{S}^{d}\) or let \(\Omega\subseteq\mathbb{R}^{d}\) be a bounded open Lipschitz domain. Let \(P_{X}\) be a distribution on \(\Omega\) with lower- and upper-bounded Lebesgue density. Consider i.i.d. data sets \(D=\{(\bm{x}_{1},y_{1}),\ldots,(\bm{x}_{n},y_{n})\}\subseteq\Omega\times\mathbb{R}\), where \(\bm{x}_{i}\sim P_{X}\), \(f^{*}(\bm{x})=\mathbb{E}[y|\bm{x}]\in H^{s^{*}}(\Omega)\backslash\{0\}\), \(s^{*}>0\), with \(\|f^{*}\|_{L^{\infty}(P_{X})}<B_{\infty}\) for some constant \(B_{\infty}>0\), \(\mathbb{E}y^{2}<\infty\) and there are constants \(\sigma,L>0\) such that \[\mathbb{E}\Big{[}|y-f^{*}(\bm{x})|^{m}\;\Big{|}\;\bm{x}\Big{]}\leq\frac{1}{2}m!\;\sigma^{2}\;L^{m-2},\] for \(P_{X}\)-almost all \(\bm{x}\in\Omega\) and all \(m\geq 2\).

The above moment condition holds for additive Gaussian noise with variance \(\sigma^{2}>0\). Hence Assumption (D1) is strictly stronger than Assumption (D3). The spike components \(\tilde{k}_{\gamma_{n}}\) can also be chosen as in Theorem G.1.

**Theorem G.5**.: _Assume Assumption (D3) holds and that the kernel components satisfy:_

* _the RKHS_ \(\tilde{\mathcal{H}}\) _of_ \(\tilde{k}\) _satisfies_ \(\tilde{\mathcal{H}}=H^{s}\) _as sets with_ \(s>\max(s^{*},d/2)\)_,_
* \(\tilde{k}_{\gamma_{n}}\) _denotes the Laplace kernel with a sequence of positive bandwidths_ \((\gamma_{n})\) _fulfilling_ \(\gamma_{n}\leq n^{-\frac{2+\alpha}{d}}\left((\frac{1}{4}+\frac{\alpha}{2})\ln n \right)^{-1}\)_, where_ \(\alpha>0\) _is arbitrary._

_Then there exists a constant \(C>0\) independent of \(n\) and there is a sequence \((\rho_{n})_{n\in\mathbb{N}}\) of order \(n^{-s/(s^{*}+d/2)}\) such that the minimum-norm interpolant \(f_{\rho_{n},\gamma_{n}}\) of the \(\rho_{n}\)-regularized spiky-smooth kernel sequence \(k_{n}\coloneqq k_{\rho_{n},\gamma_{n}}\) fulfills, with probability at least \(1-6n^{-(1\wedge\alpha)}\), for \(n\) large enough,_

\[R_{P}(f_{\rho_{n},\gamma_{n}})-R_{P}(f^{*})\leq C\;n^{-\frac{s^{*}}{(s^{*}+d/2)} }\log^{2}(n).\]

Proof.: **Step 1: Kernel ridge regression \(\hat{f}_{\rho_{n}}^{\tilde{k}}\) with optimal regularization achieves the desired convergence rate, with high probability.**

We slightly modify the proof of Theorem 8. Instead of using (Steinwart, 2001, Theorem 3.11 or Example 4.6), we use results of Fischer and Steinwart (2020). Here we first note that in the case \(\mathcal{H}=H^{s}(\Omega),\Omega\subseteq\mathbb{R}^{d}\) as RKHSs, we could directly use (Fischer and Steinwart, 2020, Corollary 5). Since we only have equivalent norms and also want to consider \(\Omega=\mathbb{S}^{d}\), see Lemma F.7, we need to resort to the underlying more general result (Fischer and Steinwart, 2020, Theorem 1). To this end, we first need to verify its Assumptions (EMB), (EVD), (SRC), and (MOM).

**Step 1.1: Verifying (MOM).** The moment condition (MOM) on the noise distributions holds since we assumed it in Assumption (D3).

**Step 1.2: Simpler equivalent spaces.** We verify the remaining conditions by analyzing them for a nicer equivalent RKHS \(\mathcal{H}\) and with uniform distribution \(\nu\) on \(\Omega\). For the non-spherical case \(\Omega\subseteq\mathbb{R}^{d}\), we choose \(\mathcal{H}\coloneqq H^{s}(\Omega)\). For the case \(\Omega=\mathbb{S}^{d}\), we choose \(\mathcal{H}\) as an RKHS associated to a dot-product kernel \(k\) with \(\mu_{l}=\Theta((l+1)^{-2s})\), such that \(\mathcal{H}\cong H^{s}\cong\tilde{\mathcal{H}}\) by Lemma B.1. In each case, \(\mathcal{H}=\tilde{\mathcal{H}}\) with equivalent norms, and \(L_{2}(\nu)=L_{2}(P_{X})\) with equivalent norms since we assumed in (D3) that \(P_{X}\) has an upper- and lower-bounded density.

**Step 1.3: Verifying (EVD+).** It suffices to verify the eigenvalue decay condition (EVD+) for \(\mathcal{H}\) and \(\nu\), since Lemma F.10 and Lemma F.9 then allow to transfer it to \(\tilde{\mathcal{H}}\) and \(P_{X}\).

For \(\Omega\subseteq\mathbb{R}^{d}\), as pointed out in front of (Fischer and Steinwart, 2020, Corollary 5), it is well-known that \(\mathcal{H}\) satisfies the polynomial eigenvalue decay assumption (EVD+) for \(p:=\frac{d}{2s}\).

For \(\Omega=\mathbb{S}^{d}\), our definition of \(\mathcal{H}\) together with Lemma B.1 directly yields (EVD+) for \(\mathcal{H}\) and \(\nu\) with \(p:=\frac{d}{2s}\).

**Step 1.4: Verifying (EMB) and (SRC).** The remaining two conditions of (Fischer and Steinwart, 2020, Theorem 1) are stated in terms of so-called power spaces, which in turn can be described by interpolation spaces of the real method. We therefore quickly recall these spaces. To this end, let us assume that we have two Banach spaces \(E\) and \(F\) such that \(F\subset E\) and the corresponding inclusion map is continuous. Then the so-called \(K\)-functional of an \(x\in E\) is defined by

\[K(x,t,E,F):=\inf_{y\in F}\bigl{(}t\|y\|_{F}+\|x-y\|_{E}\bigr{)}\,,\qquad\qquad t >0.\]

For \(q\in(0,1)\) and \(x\in E\) we then define

\[\|x\|_{q,2}^{2}:=\int_{0}^{\infty}t^{-2q-1}K^{2}(x,t,E,F)\,dt\]

and \([E,F]_{q,2}:=\{x\in E:\|x\|_{q,2}<\infty\}\). Let us now consider the cases \((E,F)=(L_{2}(\nu),\mathcal{H})\) and \((E,F)=(L_{2}(P_{X}),\tilde{\mathcal{H}})\). Now, for a suitable constant \(C\), \(\mathcal{H}\) and \(\tilde{\mathcal{H}}\) are \(C\)-equivalent, and \(L_{2}(\nu)\) and \(L_{2}(P_{x})\) are also \(C\)-equivalent. We then find

\[C^{-1}K(f,t,L_{2}(\Omega),\mathcal{H})\leq K(f,t,L_{2}(\Omega),\tilde{ \mathcal{H}})\leq CK(f,t,L_{2}(\Omega),\mathcal{H})\]

for all \(t>0\) and \(f\in L_{2}(\Omega)\), and consequently we have

\[[L_{2}(\nu),\mathcal{H}]_{q,2}=[L_{2}(P_{X}),\tilde{\mathcal{H}}]_{q,2}\]

for all \(q\in(0,1)\) with \(C\)-equivalent norms. Now, (Steinwart and Scovel, 2012, Theorem 4.6) shows that the power space \([\mathcal{H}]_{\nu}^{q}\) defined in (Steinwart and Scovel, 2012, Equation (36)) satisfies

\[[\mathcal{H}]_{\nu}^{q}=[L_{2}(\nu),\mathcal{H}]_{q,2}\]

with equivalent norms, and an analogous result is true for \(\tilde{\mathcal{H}}\) and \(P_{X}\). Moreover, \(\mathcal{H}\) is dense in \(L_{2}(\nu)\), and therefore (Steinwart and Scovel, 2012, Equations (36) and (18)) together with (Steinwart and Scovel, 2012, Lemma 2.2) show \([\mathcal{H}]_{\nu}^{0}=L_{2}(\nu)\) as spaces. Again, we analogously find \([\tilde{\mathcal{H}}]_{P_{X}}^{0}=L_{2}(P_{X})\) Consequently, for all \(0\leq q<1\) we have

\[[\mathcal{H}]_{\nu}^{q}=[\tilde{\mathcal{H}}]_{P_{X}}^{q}\]

with equivalent norms. From this we easily deduce that the Assumptions (EMB) and (SRC) are satisfied for \((\tilde{\mathcal{H}},P_{X})\) if and only if they are satisfied for \((\mathcal{H},\nu)\).

**Step 1.4.1: Non-spherical case.** Now, let \(\Omega\subseteq\mathbb{R}^{d}\). Then, (EMB) and (SRC) are satisfied for \((\mathcal{H},\nu)\) for \(\beta\coloneqq s^{*}/s\) and an arbitrary but fixed \(\alpha\in(p,\min\{1,p+\beta\})\) as outlined in front of (Fischer and Steinwart, 2020, Corollary 5). Applying Part ii) of (Fischer and Steinwart, 2020, Theorem 1) for \(\gamma=0\) then shows that there exists a sequence \((\rho_{n})_{n\in\mathbb{N}}\) of order \(n^{-s/(s^{*}+d/2)}\) and a constant \(K_{1}>0\) independent of \(n\) such that, for \(n\) large enough,

\[P^{n}\left(D\in(\mathbb{R}^{d}\times\mathbb{R})^{n}\ |\ \mathbb{E}_{\bm{x}}( \hat{f}_{\rho_{n}}^{\tilde{k}}(\bm{x})-f^{*}(\bm{x}))^{2}\geq K_{1}n^{-\frac{s ^{*}}{(s^{*}+d/2)}}\log^{2}(n)\right)\leq 4n^{-1}.\] (G.3)

**Step 1.4.2: Spherical case.** Suppose \(\Omega=\mathbb{S}^{d}\). Using the differently normalized spherical harmonics \(Y_{l,i}\) and \(\tilde{Y}_{l,i}\) from Appendix B.3, we obtain for the power spaces:

\[[\mathcal{H}]_{\nu}^{q} =\left\{\sum_{l=0}^{\infty}\sum_{i=1}^{N_{l,d}}a_{l}\tilde{\mu}_ {i}^{q/2}[\tilde{Y}_{l,i}]_{\sim}\mid(a_{li})\in\ell_{2}\right\}\] \[=\left\{\sum_{l=0}^{\infty}\sum_{i=1}^{N_{l,d}}a_{l,i}(l+1)^{-qs} [Y_{l,i}]_{\sim}\mid(a_{li})\in\ell_{2}\right\}\] \[=\left\{\sum_{l=0}^{\infty}\sum_{i=1}^{N_{l,d}}b_{l,i}[Y_{l,i}]_{ \sim}\mid\sum_{l=0}^{\infty}\sum_{i=1}^{N_{l,d}}b_{l,i}^{2}(l+1)^{2qs}<\infty\right\}\] \[=H^{qs}(\mathbb{S}^{d})\,\]

where the first equation follows from (Steinwart and Scovel, 2012, Eq. (36)), the second one from our definition of the \(\mu_{l}\) in Step 1.2, and the last one from (Hubbert et al., 2023, Section 3). Again, we can choose an arbitrary but fixed \(\alpha\in(p,\min\{1,p+\beta\})\). Then, \(\alpha s>ps=d/2\), which means that \([\mathcal{H}]_{\nu}^{\alpha}=H^{\alpha s}\) is an RKHS with bounded kernel (De Vito et al., 2021, Theorem 8), hence the embedding condition (EMB) holds. Similarly, (SRC) holds for \(\beta\coloneqq s^{*}/s\) and the result follows as above.

**Step 2: \(\hat{f}_{0}^{k_{n}}\) and \(\hat{f}_{\rho_{n}}^{\tilde{k}}\) are close in \(L_{2}(P_{X})\), with high probability.**

Since \(\frac{s^{*}}{(s^{*}+d/2)}<1\), it suffices to show that \(k_{n}\) fulfills, for some constant \(K_{2}>0\),

\[P^{n}\left(D\in(\mathbb{R}^{d}\times\mathbb{R})^{n}\ |\ \mathbb{E}_{\bm{x}}( \hat{f}_{0}^{k_{n}}(\bm{x})-\hat{f}_{\rho_{n}}^{\tilde{k}}(\bm{x}))^{2}\geq K_ {2}n^{-1}\right)\leq 2n^{-\alpha}.\] (G.4)

Since \(\gamma_{n}\leq n^{-\frac{2+\alpha}{d}}\left((\frac{11}{4}+\frac{\alpha}{2}) \ln n\right)^{-1}\), it holds that \(|\tilde{k}_{\gamma_{n}}(\bm{x},\bm{y})|\leq\varepsilon_{n}\coloneqq\rho_{n}n^{ -\frac{5+\alpha}{2}}\) (cf. Remark G.2). Then the product of all terms in Eq. (G.1) satisfies \(O(n^{4+\alpha}\rho_{n}^{-2}\varepsilon_{n}^{2})=o(n^{-1})\) with probability at least \(1-2n^{-\alpha}\). On the same event, the bound on Eq. (G.2) remains of order \(O(n^{-1})\), which shows Eq. (G.4). Combining (G.3) and (G.4) with the triangle inequality in \(L_{2}(P_{X})\) concludes the proof. 

**Remark G.6** (Optimality of the rates).: In the setting of Theorem G.5, we can apply (Fischer and Steinwart, 2020, Theorem 2) in order to obtain lower bounds on the achievable rates. We have already verified the conditions (MOM), (EVD+), (EMB), and (SRC) in the proof of Theorem G.5. In the case \(s^{*}>d/2\), we have \(\beta=s^{*}/s>d/(2s)=p\) and can therefore choose \(\alpha\in(p,\beta)\) such that \(\beta>\alpha\). Then, (Fischer and Steinwart, 2020, Theorem 2) yields a lower bound on the rate of the form (with constant probability)

\[n^{-\frac{\beta}{\beta+p}}=n^{-\frac{s^{*}}{s^{*}+d/2}}\,\]

which matches the rates in Theorem G.5 up to log terms. 

### Auxiliary results for the proof of Theorem 8

The distributional Assumption (D2) immediately implies that the training points are separated with high probability.

**Lemma G.7**.: _Assume (D2) is fulfilled with \(\beta_{X}>0\). Then with probability at least \(1-O(n^{-\alpha})\),_

\[\min_{i,j\in[n]:i\neq j}|\bm{x}_{i}-\bm{x}_{j}|\geq n^{-\frac{2+\alpha}{ \beta_{X}}}.\]

Proof.: For any \(i\in[n]\), the union bound implies

\[P(\min_{j\in[n]:i\neq j}|\bm{x}_{i}-\bm{x}_{j}|\leq\delta)=P\left(\bigcup_{j \in[n]:j\neq i}\{\bm{x}_{j}\in B_{\delta}(\bm{x}_{i})\}\right)\leq(n-1)\phi( \delta).\]Another union bound yields

\[P(\min_{i,j\in[n]:i\neq j}|\bm{x}_{i}-\bm{x}_{j}|\leq\delta)\leq n(n-1)\phi(\delta).\]

Choosing \(\delta_{\alpha}(n)=n^{-\frac{2+\alpha}{\beta_{X}}}\) yields \(\phi(\delta_{\alpha}(n))=O(\frac{1}{n^{2+\alpha}})\), which concludes the proof. 

The following lemma bounds \(\|\bm{A}^{-1}-\bm{B}^{-1}\|\) via \(\|\bm{A}^{-1}\|\) and \(\|\bm{A}-\bm{B}\|\). Similar results can for example be found in (Horn and Johnson, 2013, Section 5.8).

**Lemma G.8**.: _Let \(\bm{A},\bm{B}\in\mathbb{R}^{n\times n}\) be invertible matrices and let \(\|\cdot\|\) be a submultiplicative matrix norm with \(\|\bm{I}_{n}\|=1\). If \(\bm{A}\) and \(\bm{B}\) fulfill \(\|\bm{A}^{-1}\|\|\bm{A}-\bm{B}\|<1\), then it holds that_

\[\|\bm{B}^{-1}-\bm{A}^{-1}\|\leq\frac{\|\bm{A}^{-1}\|^{2}\cdot\|\bm{A}-\bm{B} \|}{1-\|\bm{A}^{-1}\|\cdot\|\bm{A}-\bm{B}\|}\,.\]

Proof.: Because of \(\|\bm{A}^{-1}(\bm{A}-\bm{B})\|\leq\|\bm{A}^{-1}\|\|\bm{A}-\bm{B}\|<1\) we get

\[\|\bm{I}-\bm{A}^{-1}(\bm{A}-\bm{B})\|\geq 1-\|\bm{A}^{-1}\|\|\bm{A}-\bm{B}\|.\]

Writing \(\bm{B}=\bm{A}(\bm{I}-\bm{A}^{-1}(\bm{A}-\bm{B}))\) yields \(\bm{B}^{-1}=(\bm{I}-\bm{A}^{-1}(\bm{A}-\bm{B}))^{-1}\bm{A}^{-1}\) which implies

\[\|\bm{B}^{-1}\|\leq\frac{\|\bm{A}^{-1}\|}{1-\|\bm{A}^{-1}\|\|\bm{A}-\bm{B}\|}.\]

Now write \(\bm{B}^{-1}-\bm{A}^{-1}=\bm{A}^{-1}(\bm{A}-\bm{B})\bm{B}^{-1}\) to get

\[\|\bm{B}^{-1}-\bm{A}^{-1}\|\leq\|\bm{A}^{-1}\|\|\bm{A}-\bm{B}\|\|\bm{B}^{-1}\|.\]

Combining the last two inequalities concludes the proof. 

### RKHS norm bounds

Here we show that if \(\tilde{k}\) and \(\tilde{k}_{\gamma}\) have RKHS equivalent to some Sobolev space \(H^{s}\), \(s>d/2\), then the RKHS of the spiky-smooth kernel \(k_{\rho,\gamma}\) is also equivalent to \(H^{s}\), for any fixed \(\rho,\gamma>0\). Hence all members of the spiky-smooth kernel sequence may have RKHS equivalent to a Sobolev space \(H^{s}\) and are individually inconsistent due to Theorem 1; yet the sequence is consistent. This shows that when arguing about generalization properties based on RKHS equivalence, the constants matter and the narrative that depth does not matter in the NTK regime as in Bietti and Bach (2021) is too simplified.

The following proposition states that the sum of kernels with equivalent RKHS yields an RKHS that is equivalent to the RKHS of the summands. For example, the spiky-smooth kernel with Laplace components possesses an RKHS equivalent to the RKHS of the Laplace kernel.

**Proposition G.9**.: _Let \(\mathcal{H}_{1}\) and \(\mathcal{H}_{2}\) denote the RKHS of \(k_{1}\) and \(k_{2}\) respectively. If \(\mathcal{H}_{1}=\mathcal{H}_{2}\) then the RKHS \(\mathcal{H}\) of \(k=k_{1}+k_{2}\) fulfills \(\mathcal{H}=\mathcal{H}_{1}\). Moreover, if \(C\geq 1\) is a constant with \(\frac{1}{C}\|f\|_{\mathcal{H}_{2}}\leq\|f\|_{\mathcal{H}_{1}}\leq C\|f\|_{ \mathcal{H}_{2}}\), then we have \(\frac{1}{\sqrt{2C}}\|f\|_{\mathcal{H}_{1}}\leq\|f\|_{\mathcal{H}}\leq\|f\|_{ \mathcal{H}_{1}}\)._

Proof.: The RKHS of \(k=k_{1}+k_{2}\) is given by \(\mathcal{H}=\mathcal{H}_{1}+\mathcal{H}_{2}\) with norm

\[\|f\|_{\mathcal{H}}^{2}=\min\{\|f\|_{\mathcal{H}_{1}}^{2}+\|f_{2}\|_{\mathcal{ H}_{2}}^{2}:\ f=f_{1}+f_{2},f_{1}\in\mathcal{H}_{1},f_{2}\in\mathcal{H}_{2}\}\,.\]

To see this we consider the map \(\Phi:X\to\mathcal{H}_{1}\times\mathcal{H}_{2}\) defined by \(\Phi(\bm{x}):=(\Phi_{1}(\bm{x},\cdot),\Phi_{2}(\bm{x},\cdot))\) for all \(\bm{x}\in X\), where \(X\) is the set, the spaces \(\mathcal{H}_{i}\) live on and \(\Phi_{i}(\bm{x}):=k_{i}(\bm{x},\cdot)\). The reproducing property of \(k_{1}\) and \(k_{2}\) immediately ensures that \(\Phi\) is a feature map of \(k_{1}+k_{2}\) and Theorem E.3 then shows

\[\mathcal{H} =\big{\{}\langle w,\Phi(\cdot)\rangle_{\mathcal{H}_{1}\times \mathcal{H}_{2}}:w\in\mathcal{H}_{1}\times\mathcal{H}_{2}\big{\}}\] \[=\big{\{}\langle w_{1},\Phi_{1}(\cdot)\rangle_{\mathcal{H}_{1}}+ \langle w_{2},\Phi_{2}(\cdot)\rangle_{\mathcal{H}_{2}}:w_{1}\in\mathcal{H}_{1}, w_{2}\in\mathcal{H}_{2}\big{\}}=\mathcal{H}_{1}+\mathcal{H}_{2}\]

as well as the formula for the norm on \(\mathcal{H}\). Now let \(f\in\mathcal{H}\). Considering the decomposition \(f=f_{1}+0\) then gives \(\|f\|_{\mathcal{H}}\leq\|f\|_{\mathcal{H}_{1}}\). Moreover, for \(f=f_{1}+f_{2}\) with \(f_{i}\in\mathcal{H}_{i}\) we have

\[\|f\|_{\mathcal{H}_{1}}\leq\|f_{1}\|_{\mathcal{H}_{1}}+\|f_{2}\|_{\mathcal{H}_{ 1}}\leq\|f_{1}\|_{\mathcal{H}_{1}}+C\|f_{2}\|_{\mathcal{H}_{2}}\leq\sqrt{2}C \big{(}\|f_{1}\|_{\mathcal{H}_{1}}^{2}+\|f_{2}\|_{\mathcal{H}_{1}}^{2}\big{)}^{1 /2}\,.\]

Taking the infimum over all decomposition then yields the estimate \(\|f\|_{\mathcal{H}_{1}}\leq\sqrt{2}C\|f\|_{\mathcal{H}}\).

Spiky-smooth activation functions induced by Gaussian components

Here we explore the properties of the NNGP and NTK activation functions induced by spiky-smooth kernels with Gaussian components.

To offer some more background, it is well-known that NNGPs and NTKs on the sphere \(\mathbb{S}^{d}\) are dot-product kernels, i.e., kernels of the form \(k_{d}(\bm{x},\bm{x}^{\prime})=\kappa(\langle\bm{x},\bm{x}^{\prime}\rangle)\), where the function \(\kappa\) has a series representation \(\kappa(t)=\sum_{i=0}^{\infty}b_{i}t^{i}\) with \(b_{i}\geq 0\) and \(\sum_{i=0}^{\infty}b_{i}<\infty\). The function \(\kappa\) is independent of the dimension \(d\) of the sphere. Conversely, all such kernels can be realized as NNGPs or NTKs (Simon et al., 2022, Theorem 3.1).

As dot-product kernel \(k(\bm{x},\bm{y})=\kappa(\langle\bm{x},\bm{y}\rangle)\) on the sphere, the Gaussian kernel has the simple analytic expression,

\[\kappa_{\gamma}^{Gauss}(z)=\exp\left(\frac{2(z-1)}{\gamma}\right),\]

with Taylor expansion

\[\kappa_{\gamma}^{Gauss}(z)=\sum_{i=0}^{\infty}\underbrace{\frac{2^{i}}{ \gamma^{i}i!}\exp(-2/\gamma)}_{b_{i}^{Gauss}}\ z^{i}.\]

For spiky-smooth kernels \(k=\tilde{k}+\rho\tilde{k}_{\gamma}\) with Gaussian components \(\tilde{k}\) and \(\tilde{k}_{\gamma}\) of width \(\tilde{\gamma}\) and \(\gamma\) respectively, we get Taylor series coefficients

\[b_{i}=\frac{\exp(-2/\tilde{\gamma})}{i!}\left(\frac{2}{\tilde{\gamma}}\right) ^{i}+\rho\frac{\exp(-2/\gamma)}{i!}\left(\frac{2}{\gamma}\right)^{i}.\] (H.1)

Now Theorem 11 states that as soon as \(\kappa\) induces a dot-product kernel for every input dimension \(d\), then the dot-product kernels can be written as the NNGP kernel of a 2-layer fully-connected network without biases and with the induced activation function

\[\phi_{NNGP}^{\kappa}(x)=\sum_{i=0}^{\infty}s_{i}b_{i}^{1/2}h_{i}(x),\]

or as the NTK of a 2-layer fully-connected network without biases and with the induced activation function

\[\phi_{NTK}^{\kappa}(x)=\sum_{i=0}^{\infty}s_{i}\left(\frac{b_{i}}{i+1}\right) ^{1/2}h_{i}(x),\]

where \(h_{i}\) denotes the \(i\)-th Probabilist's Hermite polynomial normalized such that \(\|h_{i}\|_{L_{2}(\mathcal{N}(0,1))}=1\) and \(s_{i}\in\{-1,+1\}\) are arbitrarily chosen for all \(i\in\mathbb{N}_{0}\).

Now we can study the induced activation functions if we know the kernel's Taylor coefficients \((b_{i})_{i\in\mathbb{N}_{0}}\). If infinitely many \(b_{i}>0\), then infinitely many activation functions induce the same dot-product kernel, with different choices of the signs \(s_{i}\). For alternating signs \(s_{i}=(-1)^{i}\), the symmetry property \(h_{i}(-x)=(-1)^{i}h_{i}(x)\) of the Hermite polynomials implies

\[\phi_{NNGP,+-}(x)=\phi_{NNGP,+}(-x),\qquad\phi_{NTK,+-}(x)=\phi_{NTK,+}(-x).\]

To form an orthonormal basis of \(L_{2}(\mathcal{N}(0,1))\) the unnormalized Probabilist's Hermite polynomials \(He_{i}\) have to be normalized by \(h_{i}(x)=\frac{1}{\sqrt{i!}}He_{i}(x)\). We can use the identity \(\exp(xt-\frac{t^{2}}{2})=\sum_{i=0}^{\infty}He_{i}(x)\frac{t^{i}}{i!}\) with \(t=\sqrt{2/\gamma}\) to analytically express the NNGP activation of the Gaussian kernel with all \(s_{i}=+1\) as the exponential function

\[\phi_{NNGP,+}^{Gauss}(x)=\exp(-1/\gamma)\sum_{i=0}^{\infty}\frac{1}{i!}\left( \frac{2}{\gamma}\right)^{\frac{i}{2}}h_{i}(x)=\exp\left(\left(\frac{2}{\gamma} \right)^{\frac{1}{2}}\cdot x-\frac{2}{\gamma}\right).\] (H.2)

Remarkably, the Gaussian kernel can not only be induced by an exponential activation function, but also by a single shifted sine activation function. This is shown in the following proposition.

**Proposition H.1** (Trigonometric Gaussian NNGP activation functions).: _For any \(\gamma>0\) and the bi-alternating choice of signs \(\{(-1)^{\lfloor i/2\rfloor}\}_{i=0,1,2,\ldots}\), the Gaussian kernel of bandwidth \(\gamma\) can be realized as the NNGP kernel of a two-layer fully-connected network without biases and with activation function_

\[\phi^{Gauss}_{NNGP,++--}(x)=\sin((2/\gamma)^{1/2}x)+\cos((2/\gamma)^{1/2}x).\]

Proof.: We write \(c=2/\gamma\). We need to show that

\[\sin(c^{1/2}x)+\cos(c^{1/2}x)=e^{-c/2}\sum_{i=0}^{\infty}(-1)^{\lfloor i/2 \rfloor}\frac{c^{i/2}}{i!}He_{i}(x).\]

We will use the fact that

\[e^{2xz-z^{2}}=\sum_{i=0}^{\infty}\frac{z^{i}}{i!}H_{i}(x),\]

with the choices \(z_{1}=i\sqrt{c/2}\) and \(z_{2}=-i\sqrt{c/2}\). Now, using \(e^{iax+b}=e^{b}(\cos(ax)+i\sin(ax))\), observe that

\[\sin(\sqrt{c}x) =\sin(\sqrt{2c}x/\sqrt{2})=\frac{1}{2ie^{c/2}}\left(e^{ix\sqrt{c }+c/2}-e^{ix\sqrt{c}+c/2}\right)\] \[=\frac{1}{2ie^{c/2}}\left(\sum_{i=0}^{\infty}\frac{(i\sqrt{c/2}) ^{i}}{i!}H_{i}(x/\sqrt{2})(1-(-1)^{i})\right)\] \[=e^{-c/2}\sum_{i=0}^{\infty}(-1)^{i}\frac{(\sqrt{c/2})^{2i+1}}{(2 i+1)!}H_{2i+1}(x/\sqrt{2}).\]

An analogous calculation yields

\[\cos(c^{1/2}x)=e^{-c/2}\sum_{i=0}^{\infty}(-1)^{i}\frac{(\sqrt{c/2})^{2i}}{(2 i)!}H_{2i}(x/\sqrt{2}).\]

Finally, using \(H_{i}(x/\sqrt{2})=2^{i/2}He_{i}(x)\), we get

\[\sin(c^{1/2}x)+\cos(c^{1/2}x) =e^{-c/2}\sum_{i=0}^{\infty}(-1)^{\lfloor i/2\rfloor}\frac{(c/2) ^{i/2}}{i!}H_{i}(x/\sqrt{2})\] \[=e^{-c/2}\sum_{i=0}^{\infty}(-1)^{\lfloor i/2\rfloor}\frac{c^{i/ 2}}{i!}He_{i}(x).\]

For \(\phi_{NNGP}(x)=\sum_{i=0}^{\infty}s_{i}\sqrt{b_{i}}h_{i}(x)\), we get \(\|\phi\|^{2}_{L_{2}(\mathcal{N}(0,1))}=\sum_{i=0}^{\infty}b_{i}\) invariant to the choice \(\{s_{i}\}_{i\in\mathbb{N}}\). For Gaussian NNGP activation components with bandwidth \(\gamma>0\) this yields

\[\|\phi^{Gauss}_{NNGP}\|^{2}_{L_{2}(\mathcal{N}(0,1))}=\exp(-2/\gamma)\sum_{i= 0}^{\infty}\frac{1}{i!}\left(\frac{2}{\gamma}\right)^{i}=1,\] (H.3)

because \(\{h_{i}\}_{i\in\mathbb{N}_{0}}\) is an ONB of \(L_{2}(\mathcal{N}(0,1))\). Analogously, for Gaussian NTK activation components, we get

\[\|\phi^{Gauss}_{NTK}\|^{2}_{L_{2}(\mathcal{N}(0,1))} =\exp(-2/\gamma)\sum_{i=0}^{\infty}\frac{1}{(i+1)!}\left(\frac{2} {\gamma}\right)^{i}\] \[=\exp(-2/\gamma)\frac{\gamma}{2}\sum_{i=1}^{\infty}\frac{1}{i!} \left(\frac{2}{\gamma}\right)^{i}=\frac{\gamma}{2}\left(1-\exp\left(-\frac{2} {\gamma}\right)\right).\] (H.4)

This implies that the average amplitude of NNGP activation functions does not depend on \(\gamma\), while the average amplitude of NTK activation functions decays with \(\gamma\to 0\).

By the fact \(h^{\prime}_{n}(x)=\sqrt{n}h_{n-1}(x)\), we know that any activation function \(\phi(x)=\sum_{i=0}^{\infty}s_{i}a_{i}h_{i}(x)\) has the derivative \(\phi^{\prime}(x)=\sum_{i=0}^{\infty}s_{i}a_{i+1}\sqrt{i+1}\cdot h_{i}(x)\) as long as \(\sum_{i=0}^{\infty}|a_{i+1}\sqrt{i+1}|<\infty\).

The following proposition formalizes the additive approximation \(\phi^{k}\approx\phi^{\tilde{k}}+\rho^{1/2}\phi^{\tilde{k}_{\gamma}}\), and quantifies the necessary scaling of \(\gamma\) for any demanded precision of the approximation.

**Proposition H.2**.: _Fix \(\tilde{\gamma},\rho>0\) arbitrary. Let \(k=\tilde{k}+\rho\tilde{k}_{\gamma}\) denote the spiky-smooth kernel where \(\tilde{k}\) and \(\tilde{k}_{\gamma}\) are Gaussian kernels of bandwidth \(\tilde{\gamma}\) and \(\gamma\), respectively. Assume that we choose the activation functions \(\phi^{k}_{NTK}\), \(\phi^{\tilde{k}}_{NTK}\) and \(\phi^{\tilde{k}}_{NTK}\) as in Theorem 11 with same signs \(\{s_{i}\}_{i\in\mathbb{N}}\). Then, for \(\gamma>0\) small enough, it holds that_

\[\|\phi^{k}_{NTK}-(\phi^{\tilde{k}}_{NTK}+\sqrt{\rho}\cdot\phi^{k _{\gamma}}_{NTK})\|_{L_{2}(\mathcal{N}(0,1))}^{2} \leq 2^{1/2}\rho\gamma^{3/2}\exp\left(-\frac{1}{\gamma}\right)+ \frac{4\pi\gamma(1+\tilde{\gamma})}{\tilde{\gamma}},\] \[\|\phi^{k}_{NNGP}-(\phi^{\tilde{k}}_{NNGP}+\sqrt{\rho}\cdot\phi^{ \tilde{k}_{\gamma}}_{NNGP})\|_{L_{2}(\mathcal{N}(0,1))}^{2} \leq 2^{3/2}\rho\gamma^{1/2}\exp\left(-\frac{1}{\gamma}\right)+ \frac{8\pi\gamma(1+\tilde{\gamma})}{\tilde{\gamma}^{2}}.\]

Proof.: Let \(b_{i,\gamma}=\frac{2^{i}}{\gamma^{i}i!}\exp(-2/\gamma)\) denote the Taylor coefficients of the Gaussian kernel. All considered infinite series converge absolutely.

\[\|\phi^{\tilde{\gamma},\rho}_{NNGP}-(\phi^{\tilde{\gamma}}_{NNGP}+ \sqrt{\rho}\cdot\phi^{\gamma}_{NNGP})\|_{L_{2}(\mathcal{N}(0,1))}^{2}\] \[= \|\sum_{i=0}^{\infty}s_{i}\sqrt{b_{i,\tilde{\gamma}}+\rho b_{i, \gamma}}h_{i}(x)-\sum_{i=0}^{\infty}s_{i}(\sqrt{b_{i,\tilde{\gamma}}}+\sqrt{ \rho b_{i,\gamma}})h_{i}(x)\|_{L_{2}(\mathcal{N}(0,1))}^{2}\] \[= \sum_{i=0}^{\infty}\left(\sqrt{b_{i,\tilde{\gamma}}+\rho b_{i, \gamma}}-(\sqrt{b_{i,\tilde{\gamma}}}+\sqrt{\rho b_{i,\gamma}})\right)^{2}\] \[\leq 2\underbrace{\sum_{i=0}^{I}(\sqrt{b_{i,\tilde{\gamma}}+\rho b_{ i,\gamma}}-b_{i,\tilde{\gamma}}^{1/2})^{2}}_{(I)}+2\underbrace{\rho\sum_{i=0}^{I}b_{i, \gamma}}_{(II)}+2\underbrace{\sum_{i=I+1}^{\infty}(\sqrt{b_{i,\tilde{\gamma}}+ \rho b_{i,\gamma}}-\rho^{1/2}b_{i,\gamma}^{1/2})^{2}}_{(III)}+2\underbrace{ \sum_{i=I+1}^{\infty}b_{i,\tilde{\gamma}}}_{(IV)},\]

for any \(I\in\mathbb{N}\). To bound \((I)\) observe

\[\sum_{i=0}^{I}(\sqrt{b_{i,\tilde{\gamma}}+\rho b_{i,\gamma}}-b_{i,\tilde{ \gamma}}^{1/2})^{2}=\sum_{i=0}^{I}\left(\rho b_{i,\gamma}+2b_{i,\tilde{\gamma}} \left(1-\sqrt{1+\frac{\rho b_{i,\gamma}}{b_{i,\tilde{\gamma}}}}\right)\right) \leq\rho\sum_{i=0}^{I}b_{i,\gamma}.\]

An analogous calculation for \((III)\) yields

\[\sum_{i=I+1}^{\infty}(\sqrt{b_{i,\tilde{\gamma}}+\rho b_{i,\gamma}}-\rho^{1/2}b _{i,\gamma}^{1/2})^{2}\leq\sum_{i=I+1}^{\infty}b_{i,\tilde{\gamma}}.\]

So overall we get the bound

\[\|\phi^{\tilde{\gamma},\gamma,\rho}_{NNGP}-(\phi^{\tilde{\gamma}}_{NNGP}+\sqrt{ \rho}\cdot\phi^{\gamma}_{NNGP})\|_{L_{2}(\mathcal{N}(0,1))}^{2}\leq 4\rho\sum_{i=0}^{I}b_{i, \gamma}+4\sum_{i=I+1}^{\infty}b_{i,\tilde{\gamma}}.\] (H.5)

Now, defining \(c\coloneqq 2/\gamma\),

\[\sum_{i=0}^{I}b_{i,\gamma}=\exp(-c)\sum_{i=0}^{I}\frac{1}{i!}c^{i}=\frac{ \Gamma(I+1,c)}{I!},\]

where \(\Gamma(k+1,c)\) denotes the upper incomplete Gamma function. Choosing \(I=\lfloor\frac{c}{2\pi}\rfloor\), (Pinelis, 2020, Theorem 1.1) yields, for \(c\geq 121\),

\[\frac{\Gamma(I+1,c)}{I!} \leq\exp(-c)\frac{(c+(I+1)!^{1!/I})^{I+1}}{(I+1)!\cdot(I+1)!^{1/I} }\leq\frac{\exp(-c)(c+I)^{I+1}}{(I+1)!^{(I+1)/I}}\] \[\leq\frac{\exp(-c)(c+I)^{I+1}}{(2\pi(I+1))^{1/2}\left(\frac{I+1}{e} \right)^{(I+1)^{2}/I}}\]\[\leq\frac{1}{\sqrt{2\pi(I+1)}}\exp\Big{(}-c+(I+1)\big{(}\ln(c+I)-\ln(I+1)+1 \big{)}\Big{)}\] \[\leq\frac{1}{\sqrt{c}}\exp\Big{(}-c+(\frac{c}{2\pi}+1)\big{(}\ln( \frac{2\pi+1}{2\pi}c)-\ln(\frac{c}{2\pi})+1\big{)}\Big{)}\] \[\leq\frac{1}{\sqrt{c}}\exp\Big{(}-c+(\frac{c}{2\pi}+1)\big{(}\ln( 2\pi+1)+1\big{)}\Big{)}\leq\frac{\exp\big{(}-\frac{c}{2}\big{)}}{\sqrt{c}},\] (H.6)

where we used \((I+1)!^{1/I}\leq I\) for \(I\geq 3\) in the first line, Stirling's approximation in the second line, and \((\frac{c}{2\pi}+1)\big{(}\ln(2\pi+1)+1\big{)}\leq c/2\) for \(c\geq 121\) in the last line.

It is obvious that

\[\sum_{i=I+1}^{\infty}b_{i,\tilde{\gamma}}\to 0,\quad\text{for }I=\lfloor\frac{c}{2 \pi}\rfloor\to\infty.\]

To quantify the rate of convergence, we use the bound \(\Gamma(I+1,c_{0})\geq e^{-c_{0}}I!(1+c_{0}/(I+1))^{I}\), which follows from applying Jensen's inequality to \(\Gamma(I+1,c_{0})=e^{-c_{0}}I!\mathbb{E}(1+c_{0}/G)^{I}\), where \(G\sim\Gamma(I+1,1)\) and \(\mathbb{E}G=I+1\). Defining \(c_{0}=2/\tilde{\gamma}\), it holds that

\[\sum_{i=I+1}^{\infty}b_{i,\tilde{\gamma}}\leq 1-\frac{\Gamma(I+1,c_{0})}{I!} \leq 1-e^{-c_{0}}\left(1+\frac{c_{0}}{I+1}\right)^{I}\leq 1-e^{-c_{0}}\left(1+ \frac{c_{0}}{I+1}\right)^{I+1}.\]

Taking the first two terms of the Laurent series expansion of \(n\mapsto\big{(}1+\frac{c_{0}}{n}\big{)}^{n}\) about \(n=\infty\) yields \(\Big{(}1+\frac{c_{0}}{I+1}\Big{)}^{I+1}>e^{c_{0}}(1-\frac{c_{0}^{2}}{2(I+1)})\) for \(I\) large enough (where we demand \(\gamma\in o(\tilde{\gamma}^{2})\)), thus

\[\sum_{i=I+1}^{\infty}b_{i,\tilde{\gamma}} \leq 1-e^{-c_{0}}\left(1+\frac{c_{0}}{I+1}\right)^{I+1}\cdot \left(1+\frac{c_{0}}{I+1}\right)^{-1}\] \[\leq\frac{c_{0}/(I+1)+c_{0}^{2}/(2(I+1))}{1+c_{0}/(I+1)}\leq \frac{c_{0}}{I+1}+\frac{c_{0}^{2}}{2(I+1)}\leq\frac{4\pi}{\tilde{\gamma}c}+ \frac{4\pi}{\tilde{\gamma}^{2}c}.\] (H.7)

Plugging (H.6) and (H.7) into (H.5) yields, for \(\gamma\leq 1/61\),

\[\|\phi_{NNGP}^{\tilde{\gamma},\gamma,\rho}-(\phi_{NNGP}^{\tilde{\gamma}}+ \sqrt{\rho}\cdot\phi_{NNGP}^{\gamma})\|_{L_{2}(\mathcal{N}(0,1))}^{2}\leq 2^{3/2} \rho\gamma^{1/2}\exp\left(-\frac{1}{\gamma}\right)+\frac{8\pi\gamma(1+\tilde{ \gamma})}{\tilde{\gamma}^{2}}.\]

For the NTK we get

\[\|\phi_{NNK}^{\tilde{\gamma},\gamma,\rho}-(\phi_{NNK}^{\tilde{ \gamma}}+\sqrt{\rho}\cdot\phi_{NTK}^{\gamma})\|_{L_{2}(\mathcal{N}(0,1))}^{2}\] \[= \|\sum_{i=0}^{\infty}s_{i}\sqrt{\frac{b_{i,\tilde{\gamma}}+\rho b _{i,\gamma}}{i+1}}h_{i}(x)-\sum_{i=0}^{\infty}s_{i}\left(\sqrt{\frac{b_{i, \tilde{\gamma}}}{i+1}}+\sqrt{\frac{\rho b_{i,\gamma}}{i+1}}\right)h_{i}(x)\|_ {L_{2}(\mathcal{N}(0,1))}^{2}\] \[=\,\sum_{i=0}^{\infty}\frac{1}{i+1}\left(\sqrt{b_{i,\tilde{\gamma }}+\rho b_{i,\gamma}}-(\sqrt{b_{i,\tilde{\gamma}}}+\sqrt{\rho b_{i,\gamma}}) \right)^{2}.\]

We can proceed exactly as for the NNGP, but choose \(I=\lfloor\frac{c}{2\pi}\rfloor-1\) to get

\[\sum_{i=0}^{I}\frac{b_{i,\gamma}}{i+1}=\exp(-c)\sum_{i=0}^{I}\frac{c^{i}}{(i+1 )!}=\frac{\exp(-c)}{c}\left(\sum_{i=0}^{I+1}\frac{c^{i}}{i!}-1\right)\leq \frac{\exp(-c/2)}{c^{3/2}}-\frac{\exp(-c)}{c},\]

and replace (H.7) with

\[\sum_{i=I+1}^{\infty}\frac{b_{i,\tilde{\gamma}}}{i+1}=\frac{\exp(-c_{0})}{c_{0 }}\sum_{i=I+2}^{\infty}\frac{c_{0}^{i}}{i!}\leq\frac{1}{I+2}+\frac{c_{0}}{2(I+ 2)}\leq\frac{\pi\gamma(1+\tilde{\gamma})}{\tilde{\gamma}}.\qed\]Additional experimental results

The code to reproduce all our experiments is provided in the supplementary material and under

https://github.com/moritzhaas/mind-the-spikes

Our implementations rely on PyTorch(Paszke et al., 2019) for neural networks and mpmath(Johansson et al., 2023) for high-precision calculations.

### Experimental details of Figure 1

For the kernel experiment (Figure 0(a)), we used the Laplace kernel with bandwidth \(0.4\) and the spiky-smooth kernel (4) with Laplace components with \(\rho=1,\tilde{\gamma}=1,\gamma=0.01\).

For the neural network experiment (Figure 0(b),c) we initialize 2-layer networks with NTK parametrization (Jacot et al., 2018) and He initialization (He et al., 2015). Using the antisymmetric initialization trick from Zhang et al. (2020) doubles the network width from \(10000\) to \(20000\) and helps to prevent errors induced by the random initialization function. It might also be helpful to increase the initialization variance (Chizat et al., 2019). We train the network with stochastic gradient descent of batch size \(1\) over the \(15\) training samples with learning rate \(0.04\) for \(2500\) epochs. Training with gradient descent and learning rate \(0.4\) produces similar results. We use the spiky-smooth activation function given by \(x\mapsto ReLU(x)+0.01\cdot(\sin(100x)+\cos(100x))\), which corresponds to \(x\mapsto ReLU(x)+\omega_{NTK}(x,\frac{1}{5000})\), including both even and uneven Hermite coefficients.

### Disentangling signal from noise in neural networks with spiky-smooth activation functions

Since our spiky-smooth activation function has the additive form \(\sigma_{spsm}(x)=ReLU(x)+\omega_{NTK}(x;\frac{1}{5000})\), we can dissect the learned neural network

\[f_{spsm}(\bm{x})=\bm{W}_{2}\cdot\sigma_{spsm}(\bm{W}_{1}\cdot\bm{x}+\bm{b}_{1 })+b_{2}=f_{ReLU}(\bm{x})+f_{spikes}(\bm{x})\] (I.1)

into its \(ReLU\)-component

\[f_{ReLU}(\bm{x})=\bm{W}_{2}\cdot ReLU(\bm{W}_{1}\cdot\bm{x}+\bm{b}_{1})+b_{2},\]

and its spike component

\[f_{spikes}(\bm{x})=\bm{W}_{2}\cdot\omega_{NTK}(\bm{W}_{1}\cdot\bm{x}+\bm{b}_{ 1};\frac{1}{5000}).\]

If the analogy to the spiky-smooth kernel holds and \(f_{spikes}\) fits the noise in the labels while having a small \(L_{2}\)-norm, then \(f_{ReLU}\) would have learned the signal in the data. Indeed Figure I.1 demonstrates that this simple decomposition is useful to disentangle the learned signal from the spike component in our setting. The figure also suggests that the oscillations in the activations of the hidden layer constructively interfere to interpolate the training points, while the differing frequencies and phases approximately destructively interfere on most of the remaining covariate support. Figure I.2 shows some of the functions generated by the hidden layer neurons of the spike component \(f_{spikes}\). Both the phases and frequencies vary. Destructive interference in sums of many oszillations occurs, for example, under a uniform phase distribution.

An exciting direction of future work will be to understand when and why the neural networks with spiky-smooth activation functions learn the target function well, and when the decomposition into \(ReLU\)- and spike component succeeds to disentangle the noise from the signal. Particular challenges will be to design architectures and learning algorithms that provably work on complex data sets and to determine their statistical convergence rates. A different line of work could evaluate whether there exist useful spike components for deep and narrow networks beyond the pure infinite-width limit. Maybe for deep architectures is suffices to apply spiky-smooth activation functions only between the penultimate and the last layer.

Of course an analogous additive decomposition exists for the minimum-norm interpolant \(\hat{f}_{0}^{k}\) of the spiky-smooth kernel,

\[\hat{f}_{0}^{k}(\bm{x})=(\tilde{k}+\rho_{n}\tilde{k}_{\gamma_{n}})(\bm{x},\bm{X} )\cdot(\tilde{\bm{K}}+\rho_{n}\tilde{\bm{K}}_{\gamma_{n}})^{-1}\bm{y}=f_{signal }(\bm{x})+f_{spikes}(\bm{x}),\] (I.2)

where

\[f_{signal}(\bm{x})=\tilde{k}(\bm{x},\bm{X})\cdot(\tilde{\bm{K}}+\rho_{n} \tilde{\bm{K}}_{\gamma_{n}})^{-1}\bm{y},\qquad f_{spikes}(\bm{x})=\rho_{n} \tilde{k}_{\gamma_{n}}(\bm{x},\bm{X})\cdot(\tilde{\bm{K}}+\rho_{n}\tilde{\bm{K }}_{\gamma_{n}})^{-1}\bm{y}.\]

We plot the results in Figure I.3. Observe that the spikes \(f_{spikes}\) regress to \(0\) more reliably than in the neural network.

Although spiky-smooth estimators can be consistent, any method that interpolates noise cannot be adversarially robust. The signal component \(f_{signal}\) may be a simple correction towards robust estimators. Figure I.4 suggests that the signal components of spiky-smooth estimators behave more robustly than ReLU networks or minimum-norm interpolants of Laplace kernels in terms of finite-sample variance.

Figure I.2: Here we plot the functions learned by \(12\) random hidden layer neurons of the spike component network \(f_{spikes}\) corresponding to Figure 1.

Figure I.1: **a. The \(ReLU\)-component \(f_{ReLU}\) (blue) and the full spiky-smooth network \(f_{spsm}\) (orange) of the learned neural network from Figure 1. b. The spike component \(f_{spikes}\) of the learned neural network from Figure 1 against the label noise in the training set, derived by subtracting the signal from the training points. Observe that the \(ReLU\)-component has learned the signal, while the spike component has fitted the noise in the data while regressing to \(0\) between data points.**

### Repeating the finite-sample experiments

We repeat the experiment from Figure 1 \(100\) times, both randomizing with respect to the training set and with respect to neural network initialization.

For the kernels (Figure I.4a), observe that all minimum-norm kernel interpolants are biased towards \(0\). While the Laplace kernel and the signal component (I.2) of the spiky-smooth kernel have similar averages, the spiky-smooth kernel has a slightly larger bias. However, both the spiky-smooth kernel as well as its signal component produces lower variance estimates than the Laplace kernel.

Considering the trained neural networks (Figure I.4b), the ReLU networks are approximately unbiased, but have large variance. The neural networks with spiky-smooth activation function as well as the extracted signal network (I.1) are similar on average: They are slighly biased towards \(0\), but have much smaller variance than the ReLU networks.

The training curves (Figure I.4c) offer similar conclusions as Figure 1: While the ReLU networks harmfully overfit over the course of training, the neural networks with spiky-smooth activation function quickly overfit to \(0\) training error with monotonically decreasing test error, which on average is almost optimal, already with only \(15\) training points. The spiky-smooth networks have smaller confidence bands, indicating increased robustness compared to the ReLU networks. If the ReLU networks would be early-stopped with perfect timing, they would generalize similarly well as the networks with spiky-smooth activation function.

### Spiky-smooth activation functions

In Figures I.5 and I.6 we plot the 2-layer NTK activation functions induced by spiky-smooth kernels with Gaussian components, where \(\tilde{k}\) has bandwidth \(1\), and in the first figure \(\rho=1\) while in the second figure \(\rho=0.1\).

Figure I.3: **a.** The signal component \(f_{signal}\) (blue) and the full minimum-norm interpolant \(\hat{f}_{0}^{k}\) (orange) of the spiky-smooth kernel from Figure 1. **b.** The spike component \(f_{spikes}\) of the spiky-smooth kernel from Figure 1 against the label noise in the training set, derived by subtracting the signal from the training points.

In Figure 1.7 we plot the corresponding 2-layer NNGP activation functions with \(\rho=1\). In contrast to the NTK activation functions the amplitudes of the fluctuations only depends on \(\rho\) and not on \(\gamma\). Our intuition is the following: Since the first layer weights are not learned in case of the NNGP, the first layer cannot learn constructive interference, so that the oscillations in the activation function need to be larger.

The additive approximation \(\phi^{k}\approx\phi^{\tilde{k}}+\rho^{1/2}\phi^{\tilde{k}_{\gamma}}\) remains accurate in all considered cases (Appendix I.6).

Figure 1.6: Same as above but \(\rho=0.1\). The high-frequency fluctuations are much smaller compared to Figure 1.5.

Figure 1.5: The 2-layer NTK activation functions for Gaussian-Gaussian spiky-smooth kernels with varying \(\gamma\) (columns) with \(k_{max}=1000\), \(\tilde{k}\) has bandwidth \(1\), \(\rho=1\). Top: all \(s_{i}=+1\), middle: \(+,+,-,-,+,+,...\), bottom: Random \(+1\) and \(-1\). Although the activation function induced by the spiky-smooth kernel is not exactly the sum of the activation functions induced by its components, this approximation is accurate because the spike components approximately live in a subspace of higher frequency in the Hermite basis orthogonal to the low-frequency subspace of the smooth component.

### Isolated spike activation functions

Figure I.8 is the equivalent of Figure 3 for the NNGP.

By plotting the NTK activation components corresponding to Gaussian spikes \(\phi^{\hat{k}_{\gamma}}\) with varying choices of the signs \(s_{i}\) in Figure I.9, we observe the following properties:

1. All \(s_{i}=+1\) leads to exponentially exploding activation functions, cf. Eq. (H.2).
2. If the signs \(s_{i}\) alternate every second \(i\), i.e. \(s_{i}=+1\) iff \(\lfloor\frac{i}{2}\rfloor\) even, \(\phi^{\hat{k}_{\gamma}}\) is approximately a single shifted \(\sin\)-curve with increasing frequency and decreasing/constant amplitude for NTK/NNGP activation functions, cf. Eq. (6).
3. If \(s_{i}\) is chosen uniformly at random, with high probability, \(\phi^{\hat{k}_{\gamma}}\) both oscillates at a high frequency around \(0\) and explodes for \(|x|\to\infty\).

Figure I.8: Same as Figure 3 but for the NNGP. In contrast to the NTK, the amplitudes of the oscillations in a. do not shrink with \(\gamma\to 0\). Otherwise the behaviour is analogous. For example, the Hermite coefficients peak at \(2/\gamma\). The squared coefficients sum to \(1\) (Eq. (6)).

Figure I.7: Same as above but NNGP and \(\rho=1\). As expected from the isolated spike plots: Spikes essentially add fluctuations that increase in frequency and stay constant in amplitude for \(\gamma\to 0\), \(\rho\) regulates the amplitude.

Figure I.10 visualizes NNGP activation functions induced by Gaussian spikes with varying bandwidth \(\gamma\). Observe similar behaviour as for the NTK but amplitudes invariant to \(\gamma\) as predicted by Eq. (6). For smaller \(\gamma\) the explosion of (all+) activation functions starts at larger \(x\), but appears sharper as can be seen in the analytic expression (H.2).

Figure I.11 resembles Figure I.9 but plotted on a larger range to visualize the exploding behaviour for \(|x|\to\infty\).

Figure I.10: Spike activation components as in Figure I.9, but for the NNGP with \(x\) between \([-4,4]\).

Figure I.9: The spike activation components of 2-layer NTK for Gaussian spikes with varying \(\gamma\) (columns), \(k_{max}=1000\), top: all \(s_{i}=+1\), middle: signs alternate every second index, bottom: 3 draws from uniformly random signs. With \(\gamma\to 0\), the amplitude shrinks, while the frequency increases.

### Additive decomposition and \(\sin\)-fit

Here we quantify the error of the \(\sin\)-approximation (8) of Gaussian NTK activation components. The additive decomposition \(\phi^{k}\approx\phi^{\tilde{k}}+\rho^{1/2}\phi^{\tilde{k}\gamma}\) quickly becomes accurate in the limit \(\gamma\to 0\) (Figures I.12 and I.13), the \(\sin\)-approximation seems to converge pointwise at rate \(\Theta(|x|\gamma)\), where a good approximation can be expected when \(|x|\ll 1/\gamma\). The error at large \(|x|\) arises because the spike component decays for \(|x|\to\infty\). For \(O(1)\) inputs, we conjecture that this inaccuracy does not dramatically affect the test error of neural networks when \(\gamma\) is chosen to be small.

Now we evaluate the numerical approximation quality of the \(\sin\)-fits (7) and (8) to the isolated spike activation components \(\phi^{\tilde{k}_{\gamma}}\). As expected by Proposition H.1, the NNGP oscillating activation function \(\phi^{\tilde{k}_{\gamma}}\) of a Gaussian spike component corresponds to Eq. (7) up to numerical errors. Both for the NNGP and for the NTK, the approximations become increasingly accurate with smaller bandwidths \(\gamma\to 0\) (Figure I.14). Again the approximation quality suffers for \(|x|\to\infty\), since \(\phi^{\tilde{k}_{\gamma}}_{NTK}\) slowly decay to \(0\) for \(|x|\to\infty\).

Figure I.12: The isolated NTK spike activation function (orange), the difference between spiky-smooth and smooth activation function (blue) and a fitted sin-curve (8) (green). All curves roughly align, in particular for \(\gamma\to 0\).

Figure I.13: The error of the additive decomposition \(\phi^{k}\approx\phi^{\tilde{k}}+\rho^{1/2}\phi^{\tilde{k}_{\gamma}}\) (blue) and the sin-fit (8) (orange) for the NTK. While the additive decomposition makes errors of order \(10^{-3},10^{-4},10^{-9},10^{-15}\) (from left to right) in the domain \([-4,4]\), the sin-fit is increasingly inaccurate for \(|x|\to\infty\), and increasingly accurate for \(\gamma\to 0\).

### Spiky-smooth kernel hyper-parameter selection

To understand the empirical performance of spiky-smooth kernels on finite data sets, we generate i.i.d. data where \(\bm{x}\sim\mathcal{U}(\mathbb{S}^{d})\) and

\[y=\bm{x}_{1}+\bm{x}_{2}^{2}+\sin(\bm{x}_{3})+\prod_{i=1}^{d+1}\bm{x}_{i}+ \varepsilon,\]

with \(\varepsilon\sim\mathcal{N}(0,\sigma^{2})\) independent of \(\bm{x}\) and evaluate the least squares excess risk of the minimum-norm interpolant. Figure I.15 shows that

* the smaller the spike bandwidth \(\gamma\), the better. At some point, the improvement saturates,
* \(\rho\) should be carefully tuned, it has large impact. As with \(\gamma\to 0\) ridgeless regression with the spiky-smooth kernel approximates ridge regression with \(\tilde{k}\) and regularization \(\rho\), simply choose the optimal regularization \(\rho^{opt}\) of ridge regression.
* The spiky-smooth kernel with Gaussian components exhibits catastrophic overfitting, when \(\gamma\) is too large (cf. Mallinar et al. (2022)), the Laplace kernel is more robust with respect to \(\gamma\).
* With sufficiently thin spikes and properly tuned \(\rho\), spiky-smooth kernels with Gaussian components outperform the Laplace counterparts.

We repeat the experiment in Figure I.16 with a slightly more complex generating function and come to the same conclusions.

Figure I.14: Absolute numerical error between the oscillating activation function \(\phi^{\tilde{k}_{\gamma}}\) of a Gaussian spike component and **a.** its analytical expression Eq. (7) for the NNGP and **b.** the approximation Eq. (8) for the NTK with varying bandwidth \(\gamma\).

Figure I.15: Least squares excess risk for spiky-smooth kernel ridgeless regression with Laplace components (_left_) and Gaussian components (_right_), with \(n=1000,d=2\), estimated on \(10000\) independent test points, \(\sigma^{2}=0.5,\tilde{\gamma}=1\). The smaller the spike bandwidth \(\gamma\), the better. Properly tuning \(\rho\) is important.

Figure I.16: Same as Figure I.15 but with the more complex generating function \(y=|\bm{x}_{1}|+\bm{x}_{2}^{2}+\sin(2\pi\bm{x}_{3})+\prod_{i=1}^{d+1}\bm{x}_{i}+\varepsilon\). The errors are larger compared to Figure I.15 and the optimal values of \(\rho\) are smaller, but the conceptual conclusions remain the same.