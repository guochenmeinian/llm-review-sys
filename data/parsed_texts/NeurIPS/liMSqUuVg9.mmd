# Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection

 Yu Bai

Salesforce Research

yu.bai@salesforce.com

&Fan Chen

Massachusetts Institute of Technology

fanchen@mit.edu

&Huan Wang

Salesforce Research

huan.wang@salesforce.com

&Caiming Xiong

Salesforce Research

cxiong@salesforce.com

&Song Mei

UC Berkeley

songmei@berkeley.edu

Equal technical and directional contributions.

Code is available at https://github.com/allenbai01/transformers-as-statisticians.

###### Abstract

Neural sequence models based on the transformer architecture have demonstrated remarkable _in-context learning_ (ICL) abilities, where they can perform new tasks when prompted with training and test examples, without any parameter update to the model. This work first provides a comprehensive statistical theory for transformers to perform ICL. Concretely, we show that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, Lasso, learning generalized linear models, and gradient descent on two-layer neural networks, with near-optimal predictive power on various in-context data distributions. Using an efficient implementation of in-context gradient descent as the underlying mechanism, our transformer constructions admit mild size bounds, and can be learned with polynomially many pretraining sequences.

Building on these "base" ICL algorithms, intriguingly, we show that transformers can implement more complex ICL procedures involving _in-context algorithm selection_, akin to what a statistician can do in real life--A _single_ transformer can adaptively select different base ICL algorithms--or even perform qualitatively different tasks--on different input sequences, without any explicit prompting of the right algorithm or task. We both establish this in theory by explicit constructions, and also observe this phenomenon experimentally. In theory, we construct two general mechanisms for algorithm selection with concrete examples: pre-ICL testing, and post-ICL validation. As an example, we use the post-ICL validation mechanism to construct a transformer that can perform nearly Bayes-optimal ICL on a challenging task--noisy linear models with mixed noise levels. Experimentally, we demonstrate the strong in-context algorithm selection capabilities of standard transformer architectures.

## 1 Introduction

Large neural sequence models have demonstrated remarkable _in-context learning_ (ICL) capabilities [12], where models can make accurate predictions on new tasks when prompted with training examples from the same task, in a zero-shot fashion without any parameter update to the model. A prevalent example is large language models based on the transformer architecture [84], which can perform a diverse range of tasks in context when trained on enormous text [12, 90]. Recent modelsin this paradigm such as GPT-4 achieve surprisingly impressive ICL performance that makes them akin to a general-purpose agent in many aspects [65; 14]. Such strong capabilities call for better understandings, which a recent line of work tackles from various aspects [49; 94; 28; 72; 15; 57; 64].

Recent pioneering work of Garg et al. [31] proposes an interpretable and theoretically amenable setting for understanding ICL in transformers. They perform ICL experiments where input tokens are real-valued (input, label) pairs generated from standard statistical models such as linear models (and the sparse version), neural networks, and decision trees. Garg et al. [31] find that transformers can learn to perform ICL with prediction power (and fitted functions) matching standard machine learning algorithms for these settings, such as least squares for linear models, and Lasso for sparse linear models. Subsequent work further studies the internal mechanisms [2; 86; 18], expressive power [2; 32], and generalization [47] of transformers in this setting. However, these works only showcase simple mechanisms such as regularized regression [31; 2; 47] or gradient descent [2; 86; 18], which are arguably only a small subset of what transformers are capable of in practice; or expressing universal function classes not specific to ICL [89; 32]. This motivates the following question:

_How do transformers learn in context beyond implementing simple algorithms?_

This paper makes steps on this question by making two main contributions: (1) We **unveil a general mechanism--_in-context algorithm selection_**--by which a _single_ transformer can adaptively _select different "base" ICL algorithms_ to use on _different ICL instances_, without any explicit prompting of the right algorithm to use in the input sequence. For example, a transformer may choose to perform ridge regression with regularization \(\lambda_{1}\) on ICL instance 1, and \(\lambda_{2}\) on ICL instance 2 (Figure 2); or perform regression on ICL instance 1 and classification on ICL instance 2 (Figure 5). This adaptivity allows transformers to achieve much stronger ICL performance than the base ICL algorithms. We both prove this in theory, and demonstrate this phenomenon empirically on standard transformer architectures. (2) Along the way, equally importantly, we present a comprehensive theory for ICL in transformers by establishing end-to-end quantitative guarantees for the **expressive power, in-context prediction performance, and sample complexity of pretraining**. These results add upon the recent line of work on the statistical learning theory of transformers [97; 89; 27; 39], and lay out a foundation for the intriguing special case where the _learning targets are themselves ICL algorithms_.

A detailed summary of our contributions is as follows.

* We prove that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, Lasso, convex risk minimization for learning generalized linear models (such as logistic regression), and gradient descent for two-layer neural networks (Section 3). Our constructions admit mild bounds on the number of layers, heads, and weight norms, and achieve near-optimal prediction power on many in-context data distributions.
* Technically, the above transformer constructions build on a new efficient implementation of in-context gradient descent (Appendix D), which could be broaderly applicable. For a broad class of smooth convex empirical risks over the in-context training data, we construct an \((L+1)\)-layer transformer that approximates \(L\) steps of gradient descent. Notably, the approximation error accumulates only _linearly_ in \(L\), utilizing a stability-like property of smooth convex optimization.
* We prove that transformers can perform in-context algorithm selection (Section 4). We construct two algorithm selection mechanisms: Post-ICL validation (Section 4.1), and Pre-ICL testing

Figure 1: **Illustration of in-context algorithm selection, and two mechanisms constructed in our theory.**_Left, middle-left_: A single transformer can perform ridge regression with different \(\lambda\)â€™s on input sequences with different observation noise; we prove this by the **post-ICL validation** mechanism (Section 4.1). _Middle-right, right_: A single transformer can perform linear regression on regression data and logistic regression on classification data; we prove this via the **pre-ICL testing** mechanism (Section 4.2).

(Section 4.2). For both mechanisms, we provide general constructions as well as concrete examples. Figure 1 provides a pictorial illustration of the two mechanisms.
* As a concrete application, using the post-ICL validation mechanism, we construct a transformer that can perform nearly Bayes-optimal ICL on noisy linear models with _mixed_ noise levels (Section 4.1.1), a more complex task than those considered in existing work.
* We provide the first line of results for _pretraining_ transformers to perform the various ICL tasks above, from polynomially many training sequences (Section 5 & Appendix K).
* Experimentally, we find that learned transformers indeed exhibit strong in-context algorithm selection capabilities in the settings considered in our theory (Section 6). For example, Figure 2 shows that a _single_ transformer can approach the individual Bayes risks (the optimal risk among all possible algorithms) simultaneously on two noisy linear models with different noise levels.

Transformers as statisticiansWe humbly remark that the typical toolkit of a statistician contains much more beyond those covered in this work, including and not limited to inference, uncertainty quantification, and theoretical analysis. This work merely aims to show the algorithm selection capability of transformers, akin to what a statistician _can_ do.

Related workOur work is intimately related to the lines of work on in-context learning, theoretical understandings of transformers, as well as other formulations for learning-to-learn such as meta-learning. Due to limited space, we discuss these related work in Appendix A.

## 2 Preliminaries

We consider a sequence of \(N\) input vectors \(\{\mathbf{h}_{i}\}_{i=1}^{N}\subset\mathbb{R}^{D}\), written compactly as an input matrix \(\mathbf{H}=[\mathbf{h}_{1},\dots,\mathbf{h}_{N}]\in\mathbb{R}^{D\times N}\), where each \(\mathbf{h}_{i}\) is a column of \(\mathbf{H}\) (also a _token_). Throughout this paper, we let \(\sigma(t):=\mathrm{ReLU}(t)=\max\left\{t,0\right\}\) denote the standard relu activation.

### Transformers

We consider transformer architectures that process any input sequence \(\mathbf{H}\in\mathbb{R}^{D\times N}\) by applying (encoder-mode2) attention layers and MLP layers formally defined as follows.

Footnote 2: Many of our results can be generalized to decoder-based architectures; see Appendix C for a discussion.

**Definition 1** (Attention layer).: _A (self-)attention layer with \(M\) heads is denoted as \(\mathrm{Attn}_{\boldsymbol{\theta}}(\cdot)\) with parameters \(\boldsymbol{\theta}=\left\{(\mathbf{V}_{m},\mathbf{Q}_{m},\mathbf{K}_{m}) \right\}_{m\in[M]}\subset\mathbb{R}^{D\times D}\). On any input sequence \(\mathbf{H}\in\mathbb{R}^{D\times N}\),_

\[\widetilde{\mathbf{H}}=\mathrm{Attn}_{\boldsymbol{\theta}}(\mathbf{H}):= \mathbf{H}+\tfrac{1}{N}\sum_{m=1}^{M}(\mathbf{V}_{m}\mathbf{H})\times\sigma \big{(}(\mathbf{Q}_{m}\mathbf{H})^{\top}(\mathbf{K}_{m}\mathbf{H})\big{)}\in \mathbb{R}^{D\times N},\] (1)

_where \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) is the ReLU function. In vector form,_

\[\widetilde{\mathbf{h}}_{i}=\left[\mathrm{Attn}_{\boldsymbol{\theta}}(\mathbf{ H})\right]_{i}=\mathbf{h}_{i}+\sum_{m=1}^{M}\tfrac{1}{N}\sum_{j=1}^{N}\sigma( (\mathbf{Q}_{m}\mathbf{h}_{i},\mathbf{K}_{m}\mathbf{h}_{j}))\cdot\mathbf{V}_ {m}\mathbf{h}_{j}.\]

Figure 2: In-context algorithm selection on two separate noisy linear regression tasks with noise \((\sigma_{1},\sigma_{2})=(0.1,0.5)\). _(a,b)_**A single transformer**TF_alg_select **simultaneously approaches the performance of the two individual Bayes predictors** ridge_lam_1 on task 1 and ridge_lam_2 on task 2. _(c)_ At token 20 (using example \(\{0,\dots,19\}\) for training), TF_alg_select approaches the Bayes error on two tasks simultaneously, and **outperforms ridge regression with any fixed \(\lambda\)**. _(a,b,c)_ Note that transformers pretrained on a single task (TF_noise_1, TF_noise_2) perform near-optimally on that task but suboptimally on the other task. More details about the setup and training method can be found in Appendix M.2.

Above, (1) uses a normalized ReLU3 activation \(t\mapsto\sigma(t)/N\) in place of the standard softmax activation; we remark this activation is also found to work well empirically in recent studies [78; 93].

Footnote 3: For each query index \(i\), the attention weights \(\{\sigma(\langle\mathbf{Q}_{m}\mathbf{h}_{i},\mathbf{K}_{m}\mathbf{h}_{j} \rangle)/N\}_{j\in[N]}\) is also a set of non-negative weights that sum to \(O(1)\) (similar as a softmax probability distribution) in typical scenarios. Also, our approximation results can potentially be generalized to softmax attention e.g. using the technique of [32].

**Definition 2** (MLP layer).: _A (token-wise) MLP layer with hidden dimension \(D^{\prime}\) is denoted as \(\mathrm{MLP}_{\boldsymbol{\theta}}(\cdot)\) with parameters \(\boldsymbol{\theta}=(\mathbf{W}_{1},\mathbf{W}_{2})\in\mathbb{R}^{D^{\prime} \times D}\times\mathbb{R}^{D\times D^{\prime}}\). On any input sequence \(\mathbf{H}\in\mathbb{R}^{D\times N}\),_

\[\widetilde{\mathbf{H}}=\mathrm{MLP}_{\boldsymbol{\theta}}(\mathbf{H}):= \mathbf{H}+\mathbf{W}_{2}\sigma(\mathbf{W}_{1}\mathbf{H}),\]

_where \(\sigma:\mathbb{R}\to\mathbb{R}\) is the ReLU function. In vector form, we have \(\widetilde{\mathbf{h}}_{i}=\mathbf{h}_{i}+\mathbf{W}_{2}\sigma(\mathbf{W}_{1} \mathbf{h}_{i})\)._

We consider a transformer architecture with \(L\geq 1\) transformer layers, each consisting of a self-attention layer followed by an MLP layer.

**Definition 3** (Transformer).: _An L-layer transformer, denoted as \(\mathrm{TF}_{\boldsymbol{\theta}}(\cdot)\), is a composition of L self-attention layers each followed by an MLP layer: \(\mathbf{H}^{(L)}=\mathrm{TF}_{\boldsymbol{\theta}}(\mathbf{H}^{(0)})\), where \(\mathbf{H}^{(0)}\in\mathbb{R}^{D\times N}\) is the input sequence, and_

\[\mathbf{H}^{(\ell)}=\mathrm{MLP}_{\boldsymbol{\theta}^{(\ell)}_{\mathrm{attr }}}\Big{(}\mathrm{Attn}_{\boldsymbol{\theta}^{(\ell)}_{\mathrm{attr}}}\left( \mathbf{H}^{(\ell-1)}\right)\Big{)},\ \ \ell\in\{1,\dots,L\}.\]

_Above, the parameter \(\boldsymbol{\theta}=(\boldsymbol{\theta}^{(1:L)}_{\mathrm{attr}},\boldsymbol{ \theta}^{(1:L)}_{\mathrm{attr}})\) consists of the attention layers \(\boldsymbol{\theta}^{(\ell)}_{\mathrm{attrn}}=\{(\mathbf{V}^{(\ell)}_{m}, \mathbf{Q}^{(\ell)}_{m},\mathbf{K}^{(\ell)}_{m})\}_{m\in[M^{(\ell)}]}\subset \mathbb{R}^{D\times D}\) and the MLP layers \(\boldsymbol{\theta}^{(\ell)}_{\mathrm{mlp}}=(\mathbf{W}^{(\ell)}_{1}, \mathbf{W}^{(\ell)}_{2})\in\mathbb{R}^{D^{(\ell)}\times D}\times\mathbb{R}^{ D\times D^{(\ell)}}\). We will frequently consider "attention-only" transformers with \(\mathbf{W}^{(\ell)}_{1},\mathbf{W}^{(\ell)}_{2}=\mathbf{0}\), which we denote as \(\mathrm{TF}^{\boldsymbol{\theta}}_{\boldsymbol{\theta}}(\cdot)\) for shorthand, with \(\boldsymbol{\theta}=\boldsymbol{\theta}^{(1:L)}:=\boldsymbol{\theta}^{(1:L)}_ {\mathrm{attr}}\)._

We additionally define the following norm of a transformer \(\mathrm{TF}_{\boldsymbol{\theta}}\):

\[\|\boldsymbol{\theta}\|:=\max_{\ell\in[L]}\Big{\{}\max_{m\in[M]}\Big{\{}\| \mathbf{Q}^{(\ell)}_{m}\|_{\mathrm{op}},\|\mathbf{K}^{(\ell)}_{m}\|_{\mathrm{ op}}\Big{\}}+\sum_{m=1}^{M}\|\mathbf{V}^{(\ell)}_{m}\|_{\mathrm{op}}+\| \mathbf{W}^{(\ell)}_{1}\|_{\mathrm{op}}+\|\mathbf{W}^{(\ell)}_{2}\|_{\mathrm{ op}}\Big{\}}.\] (2)

In (2), the choices of the operator norm and max/sums are for convenience only and not essential, as our results (e.g. for pretraining) depend only logarithmically on \(\|\boldsymbol{\theta}\|\).

### In-context learning

In an in-context learning (ICL) instance, the model is given a dataset \(\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\}_{i\in[N]}\stackrel{{ \mathrm{iid}}}{{\sim}}\mathsf{P}\) and a new test input \(\mathbf{x}_{N+1}\sim\mathsf{P}_{\mathbf{x}}\) for some data distribution \(\mathsf{P}\), where \(\{\mathbf{x}_{i}\}_{i\in[N]}\subseteq\mathbb{R}^{d}\) are the input vectors, \(\{y_{i}\}_{i\in[N]}\subseteq\mathbb{R}\) are the corresponding labels (e.g. real-valued for regression, or \(\{0,1\}\)-valued for binary classification), and \(\mathbf{x}_{N+1}\) is the test input on which the model is required to make a prediction. Different from standard supervised learning, in ICL, each instance \((\mathcal{D},\mathbf{x}_{N+1})\) is in general drawn from a different distribution \(\mathsf{P}_{j}\), such as a linear model with a new ground truth coefficient \(\mathbf{w}_{\star,j}\in\mathbb{R}^{d}\). Our goal is to construct _fixed_ transformer to perform ICL on a large set of \(\mathsf{P}_{j}\)'s.

We consider using transformers to perform ICL, in which we encode \((\mathcal{D},\mathbf{x}_{N+1})\) into an input sequence \(\mathbf{H}\in\mathbb{R}^{D\times(N+1)}\). In our theory, we use the following format, where the first two rows contain \((\mathcal{D},\mathbf{x}_{N+1})\) (zero at the location for \(y_{N+1}\)), and the third row contains fixed vectors \(\{\mathbf{p}_{i}\}_{i\in[N+1]}\) with ones, zeros, and indicator for being the train token (similar to a positional encoding vector):

\[\mathbf{H}=\begin{bmatrix}\mathbf{x}_{1}&\mathbf{x}_{2}&\dots&\mathbf{x}_{N}& \mathbf{x}_{N+1}\\ y_{1}&y_{2}&\dots&y_{N}&0\\ \mathbf{p}_{1}&\mathbf{p}_{2}&\dots&\mathbf{p}_{N}&\mathbf{p}_{N+1}\end{bmatrix} \in\mathbb{R}^{D\times(N+1)},\quad\mathbf{p}_{i}:=\begin{bmatrix}\mathbf{0}_{D -(d+3)}\\ 1\\ 1\{i<N+1\}\end{bmatrix}\in\mathbb{R}^{D-(d+1)}.\] (3)

We will choose \(D=\Theta(d)\), so that the hidden dimension of \(\mathbf{H}\) is at most a constant multiple of \(d\). We then feed \(\mathbf{H}\) into a transformer to obtain the output \(\widetilde{\mathbf{H}}=\mathrm{TF}_{\boldsymbol{\theta}}(\mathbf{H})\in\mathbb{R} ^{D\times(N+1)}\) with the same shape, and _read out_ the prediction \(\widehat{y}_{N+1}\) from the \((d+1,N+1)\)-th entry of \(\widetilde{\mathbf{H}}=[\widetilde{\mathbf{h}}_{i}]_{i\in[N+1]}\) (the entry corresponding to the missing test label): \(\widehat{y}_{N+1}=\mathsf{read}_{\boldsymbol{\gamma}}(\widetilde{\mathbf{H}}):= (\widetilde{\mathbf{h}}_{N+1})_{d+1}\). The goal is to predict \(\widehat{y}_{N+1}\) that is close to \(y_{N+1}\sim\mathsf{P}_{y|\mathbf{x}_{N+1}}\) measured by proper losses. We emphasize that we consider predicting only at the last token \(\mathbf{x}_{N+1}\), which is without much loss of generality.4

Footnote 4: Our constructions may be generalized to predicting at every token, by using a decoder architecture and potentially different input formats correspondingly (cf. Appendix C). Our theory focuses on predicting at the last token only, which simplifies the setting. Our experiments test both settings.

Miscellaneous setupsWe assume bounded features and labels throughout the paper (unless otherwise specified, e.g. when \(\mathbf{x}_{i}\) is Gaussian): \(\|\mathbf{x}_{i}\|_{2}\leq B_{x}\) and \(|y_{i}|\leq B_{y}\) with probability one. We use the standard notation \(\mathbf{X}=[\mathbf{x}_{1}^{\top};\ldots;\mathbf{x}_{N}^{\top}]\in\mathbb{R}^ {N\times d}\) and \(\mathbf{y}=[y_{1};\ldots;y_{N}]\in\mathbb{R}^{N}\) to denote the matrix of inputs and vector of labels, respectively. To prevent the transformer from blowing up on tail events, in all our results concerning (statistical) in-context prediction powers, we consider a clipped prediction \(\widehat{y}_{N+1}=\mathsf{read}_{\mathsf{V}}(\widehat{\mathbf{H}}):=\mathsf{ clip}_{R}(\widehat{\mathbf{h}}_{N+1})_{d+1}\), where \(\mathsf{clip}_{R}(t):=\mathrm{Proj}_{[-R,R]}(t)\) is the standard clipping operator with (a suitably large) radius \(R\geq 0\) that varies in different problems.

## 3 Basic in-context learning algorithms

We begin by constructing transformers that approximately implement a variety of standard machine learning algorithms in context, with mild size bounds and near-optimal prediction power on many standard in-context data distributions.

### In-context ridge regression and least squares

Consider the standard ridge regression estimator over the in-context training examples \(\mathcal{D}\) with regularization \(\lambda\geq 0\) (reducing to least squares at \(\lambda=0\) and \(N\geq d\)):

\[\mathbf{w}_{\mathrm{ridge}}^{\lambda}:=\arg\min_{\mathbf{w}\in\mathbb{R}^{d}} \tfrac{1}{2N}\sum_{i=1}^{N}\left(\langle\mathbf{w},\mathbf{x}_{i}\rangle-y_{i} \right)^{2}+\tfrac{\lambda}{2}\left\|\mathbf{w}\right\|_{2}^{2}.\] (ICRidge)

We show that transformers can approximately implement (ICRidge) (proof in Appendix F.1).

**Theorem 4** (Implementing in-context ridge regression).: _For any \(\lambda\geq 0\), \(0\leq\alpha\leq\beta\) with \(\kappa:=\tfrac{\beta+\lambda}{\alpha+\lambda}\), \(B_{w}>0\), and \(\varepsilon<B_{x}B_{w}/2\), there exists an \(L\)-layer attention-only transformer \(\mathrm{TF}_{\boldsymbol{\theta}}^{0}\) with_

\[L=\lceil 2\kappa\log(B_{x}B_{w}/(2\varepsilon))\rceil+1,\quad\max_{\ell\in[L]}M ^{(\ell)}\leq 3,\quad\left\|\boldsymbol{\theta}\right\|\leq 4R+8(\beta+ \lambda)^{-1}.\] (4)

_(with \(R:=\max\left\{B_{x}B_{w},B_{y},1\right\}\)) such that the following holds. On any input data \((\mathcal{D},\mathbf{x}_{N+1})\) such that the problem (ICRidge) is well-conditioned and has a bounded solution:_

\[\alpha\leq\lambda_{\min}(\mathbf{X}^{\top}\mathbf{X}/N)\leq\lambda_{\max}( \mathbf{X}^{\top}\mathbf{X}/N)\leq\beta,\quad\left\|\mathbf{w}_{\mathrm{ ridge}}^{\lambda}\right\|_{2}\leq B_{w}/2,\] (5)

\(\mathrm{TF}_{\boldsymbol{\theta}}^{0}\) _approximately implements (ICRidge): The prediction \(\widehat{y}_{N+1}=\mathsf{read}_{\mathsf{y}}(\mathrm{TF}_{\boldsymbol{\theta} }^{0}(\mathbf{H}))\) satisfies_

\[\left|\widehat{y}_{N+1}-\left\langle\mathbf{w}_{\mathrm{ridge}}^{\lambda}, \mathbf{x}_{N+1}\right\rangle\right|\leq\varepsilon.\] (6)

Theorem 4 presents the first quantitative construction for end-to-end in-context ridge regression up to arbitrary precision, and improves upon Akyurek et al. [2] whose construction does not give (or directly imply) an explicit error bound like (6). Further, the bounds on the number of layers and heads in (4) are mild (constant heads and logarithmically many layers).

Near-optimal in-context prediction power for linear problemsCombining Theorem 4 with standard analyses of linear regression yields the following corollaries (proofs in Appendix F.3 & F.4).

**Corollary 5** (Near-optimal linear regression with transformers by approximating least squares).: _For any \(N\geq\widetilde{\mathcal{O}}(d)\), there exists an \(\mathcal{O}(\kappa\log(\kappa N/\sigma))\)-layer transformer \(\boldsymbol{\theta}\), such that on any \(\mathsf{P}\) satisfying standard statistical assumptions for least squares (Assumption A), its ICL prediction \(\widehat{y}_{N+1}\) achieves_

\[\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})\sim\mathsf{P}}[(\widehat{y }_{N+1}-y_{N+1})^{2}]\leq\inf_{\mathbf{w}}\mathbb{E}_{(\mathbf{x},y)\sim \mathsf{P}}\big{[}(y-\langle\mathbf{w},\mathbf{x}\rangle)^{2}\big{]}+ \widetilde{\mathcal{O}}(d\sigma^{2}/N).\]

Assumption A requires only generic tail properties such as sub-Gaussianity, and _not_ realizability (i.e., \(\mathsf{P}\) follows a true linear model); \(\kappa,\sigma\) above denote the covariance condition number and the noise level therein. The \(\widetilde{\mathcal{O}}(d\sigma^{2}/N)\) excess risk is known to be rate-optimal for linear regression [38], and Corollary 5 achieves this in context with a transformer with only logarithmically many layers.

Next, consider Bayesian linear models where each in-context data distribution \(\mathsf{P}=\mathsf{P}_{\mathbf{w}_{\star}}^{\mathsf{lin}}\) is drawn from a Gaussian prior \(\pi:\mathbf{w}_{\star}\sim\mathsf{N}(0,\mathbf{I}_{d}/d)\), and \((\mathbf{x},y)\sim\mathsf{P}_{\mathbf{w}_{\star}}^{\mathsf{lin}}\) is sampled as \(\mathbf{x}\sim\mathsf{N}(\mathbf{0},\mathbf{I}_{d})\), \(y=\langle\mathbf{w}_{\star},\mathbf{x}\rangle+\mathsf{N}(0,\sigma^{2})\). It is a standard result that the Bayes estimator of \(y_{N+1}\) given \((\mathcal{D},\mathbf{x}_{N+1})\) is given by ridge regression (ICRidge): \(\widehat{y}_{N+1}^{\mathsf{Bayes}}:=\langle\mathbf{w}_{\mathrm{ridge}}^{ \lambda},\mathbf{x}_{N+1}\rangle\) with \(\lambda=d\sigma^{2}/N\). We show that transformers achieve nearly-Bayes risk for this problem, and we use

\[\mathsf{BayesRisk}_{\pi}:=\mathbb{E}_{\mathbf{w}_{\star}\sim\pi,(\mathcal{D}, \mathbf{x}_{N+1},y_{N+1})\sim\mathsf{P}_{\mathbf{w}_{\star}}^{\mathsf{lin}}} \left[\tfrac{1}{2}\big{(}\widehat{y}_{N+1}^{\mathsf{Bayes}}-y_{N+1}\big{)}^{2}\right]\]

to denote the Bayes risk of this problem under prior \(\pi\).

**Corollary 6** (Nearly-Bayes linear regression with transformers by approximating ridge regression).: _Under the Bayesian linear model above with \(N\geq\max\left\{d/10,\mathcal{O}\left(\log(1/\varepsilon)\right)\right\}\), there exists a \(L=\mathcal{O}\left(\log(1/\varepsilon)\right)\)-layer transformer such that \(\mathbb{E}_{\mathbf{w}_{\star},(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}/{\left[ \tfrac{1}{2}\big{(}\widehat{y}_{N+1}-y_{N+1}\big{)}^{2}\right]}\leq\mathsf{ BayesRisk}_{\pi}+\varepsilon\)._

Generalized linear modelsIn Appendix G, we extend the above results to generalized linear models [53] and show that transformers can approximate the corresponding convex risk minimization algorithm in context (which includes logistic regression for linear classification as an important special case), and achieve near-optimal excess risk under standard statistical assumptions.

### In-context Lasso

Consider the standard Lasso estimator [82] which minimizes an \(\ell_{1}\)-regularized linear regression loss \(\widehat{L}_{\mathrm{lasso}}\) over the in-context training examples \(\mathcal{D}\):

\[\mathbf{w}_{\mathrm{lasso}}:=\arg\min_{\mathbf{w}\in\mathbb{R}^{d}}\widehat{ L}_{\mathrm{lasso}}(\mathbf{w})=\tfrac{1}{2N}\sum_{i=1}^{N}\left(\langle \mathbf{w},\mathbf{x}_{i}\rangle-y_{i}\right)^{2}+\lambda_{N}\left\lVert \mathbf{w}\right\rVert_{1}.\] (IClasso)

We show that transformers can also approximate in-context Lasso with a mild number of layers, and can perform sparse linear regression in standard sparse linear models (proofs in Appendix H).

**Theorem 7** (Implementing in-context Lasso).: _For any \(\lambda_{N}\geq 0\), \(\beta>0\), \(B_{w}>0\), and \(\varepsilon>0\), there exists a \(L\)-layer transformer \(\mathrm{TF}_{\boldsymbol{\theta}}\) with_

\[L=\big{\lceil}\beta B_{w}^{2}/\varepsilon\big{\rceil}+1,\quad\max_{\ell\in[L]} M^{(\ell)}\leq 2,\quad\max_{\ell\in[L]}D^{(\ell)}\leq 2d,\quad\left\lVert \boldsymbol{\theta}\right\rVert\leq\mathcal{O}\big{(}R+(1+\lambda_{N})\beta^{ -1}\big{)}\]

_(where \(R:=\max\left\{B_{x}B_{w},B_{y},1\right\}\)) such that the following holds. On any input data \((\mathcal{D},\mathbf{x}_{N+1})\) such that \(\lambda_{\max}(\mathbf{X}^{\top}\mathbf{X}/N)\leq\beta\) and \(\left\lVert\mathbf{w}_{\mathrm{lasso}}\right\rVert_{2}\leq B_{w}/2,\,\mathrm{TF }_{\boldsymbol{\theta}}(\mathbf{H}^{(0)})\) approximately implements (IC Lasso), in that it outputs \(\widehat{y}_{N+1}=\langle\mathbf{x}_{N+1},\widehat{\mathbf{w}}\rangle\) with \(\widehat{L}_{\mathrm{lasso}}(\widehat{\mathbf{w}})-\widehat{L}_{\mathrm{ lasso}}(\mathbf{w}_{\mathrm{lasso}})\leq\varepsilon\)._

**Theorem 8** (Near-optimal sparse linear regression with transformers by approximating Lasso).: _For any \(d,N\geq 1,\delta>0,B_{w}^{*},\sigma>0\), there exists a \(\widetilde{\mathcal{O}}((B_{w}^{*})^{2}/\sigma^{2}\times(1+(d/N)))\)-layer transformer \(\boldsymbol{\theta}\) such that the following holds: For any \(s\) and \(N\geq\widetilde{\mathcal{O}}\left(s\log(d/\delta)\right)\), suppose that \(\mathsf{P}\) is an \(s\)-sparse linear model: \(\mathbf{x}_{i}\sim\mathsf{N}(0,\mathbf{I}_{d})\), \(y_{i}=\langle\mathbf{w}_{\star},\mathbf{x}_{i}\rangle+\mathsf{N}(0,\sigma^{2})\) for any \(\left\lVert\mathbf{w}_{\star}\right\rVert_{2}\leq B_{w}^{*}\) and \(\left\lVert\mathbf{w}_{\star}\right\rVert_{0}\leq s\), then with probability at least \(1-\delta\) (over the randomness of \(\mathcal{D}\)), the transformer output \(\widehat{y}_{N+1}\) achieves_

\[\mathbb{E}_{(\mathbf{x}_{N+1},y_{N+1})\sim\mathsf{P}}\big{[}(\widehat{y}_{N+1}- y_{N+1})^{2}\big{]}\leq\sigma^{2}[1+\mathcal{O}(s\log(d/\delta)/N)].\]

The \(\widetilde{\mathcal{O}}(s\log d/N)\) excess risk obtained in Theorem 8 is optimal up to log factors [62; 87]. We remark that Theorem 8 is not a direct corollary of Theorem 7; Rather, the bound on the number of layers in Theorem 8 requires a sharper convergence analysis of the (IClasso) problem under sparse linear models (Appendix H.2), similar to [1].

### Proof technique: In-context gradient descent

The constructions in Section 3.1 and 3.2 is built on the following result for approximating in-context (proximal) gradient descent on (regularized) convex losses.

**Theorem 9** (ICGD; Informal version of Theorem D.1 & D.2).: _For a broad class of convex losses of form \(\mathbf{w}\mapsto\frac{1}{N}\sum_{i=1}^{N}\ell(\mathbf{w}^{\top}\mathbf{x}_{i},y _{i})+R(\mathbf{w})\), there exists an \(L\)-layer transformer that takes in any \((\mathcal{D},\mathbf{w}^{0})\) and outputs \(\widehat{\mathbf{w}}^{L}\) such that \(\|\widehat{\mathbf{w}}^{L}-\mathbf{w}^{L}_{\{\mathrm{GD},\mathrm{PGD}\}}\|_{2} \leq\mathcal{O}(L\varepsilon)\), by composing \(L\) identical layers each \(\mathcal{O}(\varepsilon)\)-approximating a single step of GD (so that \(\mathcal{O}(L\varepsilon)\) is a linear error accumulation)._Theorem 9 is established in two main steps:

* Approximating one-step of ICGD using one attention layer (Proposition E.1), which substantially generalizes that of von Oswald et al. [86] (which only does GD on square losses with a _linear_ self-attention), and is simpler than the ones in Akyurek et al. [2] and Giannonu et al. [32].
* Stacking \(L\) of the above layer to approximate \(L\) steps of ICGD. Done naively, the error accumulation of this stacking operation is exponential in \(L\) in the worst case. We utilize the stability of _convex_ gradient descent (Lemma D.1) to obtain the _linear_ in \(L\) error accumulation in Theorem 9.

In Appendix D.3, we also give results for _non-convex_ GD on two-layer neural nets, though with a worse (exponential in \(L\)) error accumulation as expected.

## 4 In-context algorithm selection

We now show that transformers can perform various kinds of _in-context algorithm selection_, which allows them to implement more complex ICL procedures by adaptively selecting different "base" algorithms on different input sequences. We construct two general mechanisms: _Post-ICL validation_, and _Pre-ICL testing_; See Figure 1 for a pictorial illustration.

### Post-ICL validation mechanism

In our first mechanism, post-ICL validation, the transformer begins by implementing a _train-validation split_\(\mathcal{D}=(\mathcal{D}_{\text{train}},\mathcal{D}_{\text{val}})\), and running \(K\)_base_ ICL algorithms on \(\mathcal{D}_{\text{train}}\). Let \(\left\{f_{k}\right\}_{k\in[K]}\subset(\mathbb{R}^{d}\to\mathbb{R})\) denote the \(K\) learned predictors, and

\[\widehat{L}_{\text{val}}(f):=\tfrac{1}{|\mathcal{D}_{\text{val}}|}\sum_{( \mathbf{x}_{i},y_{i})\in\mathcal{D}_{\text{val}}}\ell(f(\mathbf{x}_{i}),y_{i})\] (7)

denote the validation loss of any predictor \(f\).

We show that (proof in Appendix I.1) a 3-layer transformer can output a predictor \(\widehat{f}\) that achieves nearly the smallest validation loss, and thus nearly optimal expected loss if \(\widehat{L}_{\text{val}}\) concentrates around the expected loss \(L\). Below, the input sequence \(\mathbf{H}\) uses a generalized positional encoding \(\mathbf{p}_{i}:=[\mathbf{0}_{D-(d+3)};1;t_{i}]\) in (3), where \(t_{i}:=1\) for \(i\in\mathcal{D}_{\text{train}}\), \(t_{i}:=-1\) for \(i\in\mathcal{D}_{\text{val}}\), and \(t_{N+1}:=0\).

**Proposition 10** (In-context algorithm selection via train-validation split).: _Suppose that \(\ell(\cdot,\cdot)\) in (7) is approximable by sum of relus (Definition D.1, which includes all \(C^{3}\)-smooth bivariate functions). Then there exists a 3-layer transformer \(\operatorname{TF}_{\boldsymbol{\theta}}\) that maps (defining \(y_{i}^{\prime}=y_{i}1\{i<N+1\}\))_

\[\mathbf{h}_{i}=[\mathbf{x}_{i};y_{i}^{\prime};*;f_{1}(\mathbf{x}_{i});\cdots \ ;f_{K}(\mathbf{x}_{i});\mathbf{0}_{K+1};1;t_{i}]\quad\to\quad\mathbf{h}_{i}^{ \prime}=[\mathbf{x}_{i};y_{i}^{\prime};*;\widehat{f}(\mathbf{x}_{i});1;t_{i} ],\ i\in[N+1],\]

_where the predictor \(\widehat{f}:\mathbb{R}^{d}\to\mathbb{R}\) is a convex combination of \(\left\{f_{k}:\widehat{L}_{\text{val}}(f_{k})\leq\min_{k_{*}\in[K]}\widehat{L} _{\text{val}}(f_{k_{*}})+\gamma\right\}\). As a corollary, for any convex risk \(L:(\mathbb{R}^{d}\to\mathbb{R})\to\mathbb{R}\), \(\widehat{f}\) satisfies_

\[L(\widehat{f})\leq\min_{k_{*}\in[K]}L(f_{k_{*}})+\max_{k\in[K]}\left|\widehat {L}_{\text{val}}(f_{k})-L(f_{k})\right|+\gamma.\]

Ridge regression with in-context regularization selectionAs an example, we use Proposition 10 to construct a transformer to perform in-context ridge regression with regularization selection according to the _unregularized_ validation loss \(\widehat{L}_{\text{val}}(\mathbf{w}):=\tfrac{1}{2|\mathcal{D}_{\text{val}}| }\sum_{(x_{i},y_{i})\in\mathcal{D}_{\text{val}}}\left(\langle\mathbf{w}, \mathbf{x}_{i}\rangle-y_{i}\right)^{2}\) (proof in Appendix I.2). Let \(\lambda_{1},\dots,\lambda_{K}\geq 0\) be \(K\) fixed regularization strengths.

**Theorem 11** (Ridge regression with in-context regularization selection).: _There exists a transformer with \(\mathcal{O}(\log(1/\varepsilon))\) layers and \(\mathcal{O}(K)\) heads such that the following holds: On any \((\mathcal{D},\mathbf{x}_{N+1})\) well-conditioned (cf. (5)) for all \(\left\{\lambda_{k}\right\}_{k\in[K]}\), it outputs \(\widehat{y}_{N+1}=\langle\widehat{\mathbf{w}},\mathbf{x}_{N+1}\rangle\), where_

\[\operatorname{dist}\!\left(\widehat{\mathbf{w}},\operatorname{conv}\{ \widehat{\mathbf{w}}_{\operatorname{ridge},\text{train}}^{\lambda_{k}}: \widehat{L}_{\text{val}}(\widehat{\mathbf{w}}_{\operatorname{ridge},\text{ train}}^{\lambda_{k}})\leq\min_{k_{*}\in[K]}\widehat{L}_{\text{val}}( \widehat{\mathbf{w}}_{\operatorname{ridge},\text{train}}^{\lambda_{k_{*}}})+ \gamma\}\right)\leq\varepsilon.\]

_Above, \(\widehat{\mathbf{w}}_{\operatorname{ridge},\text{train}}^{\lambda}\) denotes the solution to (ICRidge) on the training split \(\mathcal{D}_{\text{train}}\)._

#### 4.1.1 Nearly Bayes-optimal ICL on noisy linear models with mixed noise levels

We build on Theorem 11 to show that transformers can perform nearly Bayes-optimal ICL when data come from noisy linear models with a _mixture of \(K\) different noise levels_\(\sigma_{1},\ldots,\sigma_{K}>0\).

Concretely, consider the following data generating model, where we first sample \(\mathsf{P}=\mathsf{P}_{\mathbf{w}_{*},\sigma_{k}}\sim\pi\) from \(k\sim\Lambda\in\Delta([K])\), \(\mathbf{w}_{*}\sim\mathsf{N}(\mathbf{0},\mathbf{I}_{d}/d)\), and then sample data \(\left\{(\mathbf{x}_{i},y_{i})\right\}_{i\in[N+1]}\stackrel{{ \mathrm{iid}}}{{\sim}}\mathsf{P}_{\mathbf{w}_{*},\sigma_{k}}\) as

\[\mathsf{P}_{\mathbf{w}_{*},\sigma_{k}}:\mathbf{x}_{i}\sim\mathsf{N}(\mathbf{0 },\mathbf{I}_{d}),\quad y_{i}=\langle\mathbf{x}_{i},\mathbf{w}_{*}\rangle+ \mathsf{N}(0,\sigma_{k}^{2}).\]

For any fixed \((N,d)\), consider the Bayes risk for predicting \(y_{N+1}\) under this model:

\[\mathsf{BayesRisk}_{\pi}:=\inf_{\mathcal{A}}\mathbb{E}_{\pi}\big{[}\tfrac{1}{ 2}\big{(}\mathcal{A}(\mathcal{D})(\mathbf{x}_{N+1})-y_{N+1})^{2}\big{]}.\]

By standard Bayesian calculations, the above Bayes risk is attained when \(\mathcal{A}\) is a certain _mixture of \(K\) ridge regressions_ with regularization \(\lambda_{k}=d\sigma_{k}^{2}/N\); however, the mixing weights depend on \(\mathcal{D}\) in a highly non-trivial fashion (see Appendix J.2 for a derivation). By using the post-ICL validation mechanism in Theorem 11, we construct a transformer that achieves nearly the Bayes risk.

**Theorem 12** (Nearly Bayes-optimal ICL; Informal version of Theorem J.1).: _For sufficiently large \(N,d\), there exists a transformer with \(\mathcal{O}(\log N)\) layers and \(\mathcal{O}(K)\) heads such that on the above model, it outputs a prediction \(\widehat{y}_{N+1}\) that is nearly Bayes-optimal:_

\[\mathbb{E}_{\pi}\big{[}\tfrac{1}{2}(y_{N+1}-\widehat{y}_{N+1})^{2}\big{]} \leq\mathsf{BayesRisk}_{\pi}+\mathcal{O}\left((\log K/N)^{1/3}\right).\] (8)

In particular, Theorem 12 applies in the _proportional setting_ where \(N,d\) are large and \(N/d=\Theta(1)\)[22], in which case \(\mathsf{BayesRisk}_{\pi}=\Theta(1)\), and thus the transformer achieves vanishing excess risk relative to the Bayes risk at large \(N\).

This substantially strengthens the results of Akyurek et al. [2], who empirically find that transformers can achieve nearly Bayes risk under any _fixed_ noise level. By contrast, Theorem 12 shows that a _single_ transformer can achieve nearly Bayes risk even under a mixture of \(K\) noise levels, with quantitative guarantees. Also, our proof in fact gives a stronger guarantee: The transformer approaches the _individual Bayes risks on all \(K\) noise levels simultaneously_ (in addition to the overall Bayes risk for \(k\sim\Lambda\) as in Theorem 12). We demonstrate this empirically in Section 6 (cf. Figure 2(b) & 2).

Exact Bayes predictor vs. Post-ICL validation mechanismAs \(\mathsf{BayesRisk}_{\pi}\) is the theoretical lower bound for the risk of any possible ICL algorithm, Theorem 12 implies that our transformer performs similarly as the exact Bayes estimator5. Notice that our construction builds on the (generic) post-ICL validation mechanism, rather than a direct attempt of approximating the exact Bayes predictor, whose structure may vary significantly case-by-case. This highlights post-ICL validation as a promising mechanism for approximating the Bayes predictor on broader classes of problems beyond noisy linear models, which we leave as future work.

Footnote 5: By the Bayes risk decomposition for square loss, (8) implies that \(\mathbb{E}[(\widehat{y}_{N+1}-\widehat{y}_{N+1}^{\mathsf{Bayes}})^{2}]\leq \mathcal{O}((\log K/N)^{1/3})\).

Generalized linear models with adaptive link function selectionAs another example of the post-ICL validation mechanism, we construct a transformer that can learn a generalized linear model with adaptively chosen link function for the particular ICL instance; see Theorem J.2.

### Pre-ICL testing mechanism

In our second mechanism, pre-ICL testing, the transformer runs a _distribution testing_ procedure on the input sequence to determine the right ICL algorithm to use. While the test (and thus the mechanism itself) could in principle be general, we focus on cases where the test amounts to computing some simple summary statistics of the input sequence.

To showcase pre-ICL testing, we consider the toy problem of selecting between in-context regression and in-context classification, by running the following _binary type check_ on the input labels \(\left\{y_{i}\right\}_{i\in[N]}\):

\[\Psi^{\text{binary}}(\mathcal{D})=\frac{1}{N}\sum_{i=1}^{N}\psi(y_{i}),\quad \psi(y):=\begin{cases}1,&y\in\{0,1\},\\ 0,&y\not\in[-\varepsilon,\varepsilon]\cup[1-\varepsilon,1+\varepsilon],\\ \text{linear interpolation},&\text{otherwise}.\end{cases}\]

**Lemma 13**.: _There exists a single attention layer with 6 heads that implements \(\Psi^{\text{binary}}\) exactly._

Using this test, we construct a transformer that performs logistic regression when labels are binary, and linear regression with high probability if the label admits a continuous distribution.

**Proposition 14** (Adaptive regression or classification; Informal version of Proposition I.4).: _There exists a transformer with \(\mathcal{O}(\log(1/\varepsilon))\) layers such that the following holds: On any \(\mathcal{D}\) such that \(y_{i}\in\{0,1\}\), it outputs \(\widehat{y}_{N+1}\) that \(\varepsilon\)-approximates the prediction of in-context logistic regression._

_By contrast, for any distribution \(\mathsf{P}\) whose marginal distribution of \(y\) is not concentrated around \(\{0,1\}\), with high probability (over \(\mathcal{D}\)), \(\widehat{y}_{N+1}\)\(\varepsilon\)-approximates the prediction of in-context least squares._

The proofs can be found in Appendix I.3. We additionally show that transformers can implement more complex tests such as a _linear correlation test_, which can be useful in certain scenarios such as "confident linear regression" (predict only when the signal-to-noise ratio is high); see Appendix I.4.

## 5 Analysis of pretraining

Building on the expressivity results in Section 3 & 4, we provide the first line of polynomial sample complexity results for _pretraining_ transformers to perform ICL (including with in-context algorithm selection). We begin by providing a generic generalization guarantee for pretraining transformers.

Consider the pretraining ERM problem (TF-ERM), which minimizes the pretraining risk \(\widehat{L}_{\text{icl}}(\cdot)\) over \(n\) pretraining sequences. Let \(L_{\text{icl}}(\cdot)\) denote the corresponding population risk.

**Theorem 15** (Generalization of transformers; Informal version of Theorem K.1).: _The solution \(\widehat{\bm{\theta}}\) to (TF-ERM) over transformers with \(L\) layers, \(M\) heads per layer, and hidden dimension \(D^{\prime}\) satisfies_

\[L_{\text{icl}}(\widehat{\bm{\theta}})\leq\inf_{\bm{\theta}}L_{\text{icl}}( \bm{\theta})+\widetilde{\mathcal{O}}\Bigg{(}\sqrt{\frac{L^{2}(MD^{2}+DD^{ \prime})}{n}}\Bigg{)}.\]

Theorem 15 builds on standard uniform concentration analysis via chaining (Proposition B.4). Combining Theorem 15 with the in-context linear regression construction in Theorem 4 gives the following end-to-end result on the excess in-context prediction risk of trained transformers.

**Theorem 16** (Pretraining transformers for in-context linear regression; Informal version of Theorem K.2).: _Under Assumption \(A\) and \(N\geq\widetilde{\mathcal{O}}(d)\), the solution \(\widehat{\bm{\theta}}\) to (TF-ERM) with \(L=\mathcal{O}(\kappa\log(\kappa N/\sigma))\) layers, \(M=3\) heads, \(D^{\prime}=0\) (attention-only as in Theorem 4) achieves small excess ICL risk over the best linear predictor \(\mathbf{w}_{\mathsf{P}}^{\ast}:=\mathbb{E}_{\mathsf{P}}[\mathbf{xx}^{\top}]^{ -1}\mathbb{E}_{\mathsf{P}}[\mathbf{x}y]\) for each \(\mathsf{P}\):_

\[L_{\text{icl}}(\widehat{\bm{\theta}})-\mathbb{E}_{\mathsf{P}\sim\pi}\mathbb{ E}_{(\mathbf{x},y)\sim\mathsf{P}}\bigg{[}\frac{1}{2}(y-\langle\mathbf{w}_{ \mathsf{P}}^{\ast},\mathbf{x}\rangle)^{2}\bigg{]}\leq\widetilde{\mathcal{O}} \Bigg{(}\sqrt{\frac{\kappa^{2}d^{2}}{n}}+\frac{d\sigma^{2}}{N}\Bigg{)},\]

See Appendix K.2 for similar results in several additional settings.

## 6 Experiments

We test our theory by studying the ICL and in-context algorithm selection capabilities of transformers, using the encoder-based architecture in our theoretical constructions (Definition 3). Due to limited space, additional experimental details can be found in Appendix M.1. Results with a decoder architecture as in [31; 47] (including the setup of Figure 2) can be found in Appendix M.2.

Training data distributions and evaluationWe train a 12-layer transformer, with two modes for the training sequence (instance) distribution \(\pi\). In the "base" mode, similar to [31; 2; 86; 47], we sample the training instances from _one_ of the following base distributions (tasks), where we first sample \(\mathsf{P}=\mathsf{P}_{\mathbf{w}_{\star}}\sim\pi\) by sampling \(\mathbf{w}_{\star}\sim\mathsf{N}(\mathbf{0},\mathbf{I}_{d}/d)\), and then sample \(\{\langle\mathbf{x}_{i},y_{i}\rangle\}_{i\in[N+1]}\stackrel{{\text{ iid}}}{{\sim}}\mathsf{P}_{\mathbf{w}_{\star}}\) as \(\mathbf{x}_{i}\stackrel{{\text{iid}}}{{\sim}}\mathsf{N}(\mathbf{0 },\mathbf{I}_{d})\), and \(y_{i}\) from one of the following models studied in Section 3:

1. Linear model: \(y_{i}=\langle\mathbf{w}_{\star},\mathbf{x}_{i}\rangle\);
2. Noisy linear model: \(y_{i}=\langle\mathbf{w}_{\star},\mathbf{x}_{i}\rangle+\sigma z_{i}\), where \(\sigma>0\) is a fixed noise level, and \(z_{i}\sim\mathsf{N}(0,1)\)3. Sparse linear model: \(y_{i}=\left\langle\mathbf{w}_{\star},\mathbf{x}_{i}\right\rangle\) with \(\left\|\mathbf{w}_{\star}\right\|_{0}\leq s\), where \(s<d\) is a fixed sparsity level, and in this case we sample \(\mathbf{w}_{\star}\) from a special prior supported on \(s\)-sparse vectors;
4. Linear classification model: \(y_{i}=\operatorname{sign}(\left\langle\mathbf{w}_{\star},\mathbf{x}_{i} \right\rangle)\).

These base tasks have been empirically investigated by Garg et al. [31], though we remark that our architecture (used in our theory) differs from theirs in several aspects, such as encoder-based architecture instead of decoder-based, and ReLU activation instead of softmax. All experiments use \(d=20\). We choose \(\sigma\in\{\sigma_{1},\sigma_{2}\}=\{0.1,0.5\}\) and \(N=20\) for noisy linear regression, \(s=3\) and \(N=10\) for sparse linear regression, and \(N=40\) for linear regression and linear classification.

In the "mixture" mode, \(\pi\) is the uniform _mixture of two or more base distributions_. We consider two representative mixture modes studied in Section 4:

* Linear model + linear classification model;
* Noisy linear model with four noise levels \(\sigma\in\{0.1,0.25,0.5,1\}\).

Transformers trained with the mixture mode will be evaluated on _multiple_ base distributions simultaneously. When the base distributions are sufficiently diverse, a transformer performing well on all of them will _likely_ be performing some level of in-context algorithm selection. We evaluate transformers against standard machine learning algorithms in context (for each task respectively) as baselines.

ResultsFigure 2(a) shows the ICL performance of transformers on five base tasks, within each the transformer is trained on the same task. Transformers match the best baseline algorithm in four out of the five cases, except for the sparse regression task where the Transformer still outperforms least squares and matches Lasso with some choices of \(\lambda\) (thus utilizing sparsity to some extent). This demonstrates the strong ICL capability of the transformer architecture considered in our theory.

Figure 2(b) & 2(c) examine the in-context algorithm selection capability of transformers, on noisy linear regression with two different noise levels (Figure 2(b)), and regression + classification (Figure 2(c)). In both figures, the transformer trained in the mixture mode (TF_alg_select) approaches the best baseline algorithm on both tasks simultaneously. By contrast, transformers trained in the base mode for one of the tasks perform well on that task but behave suboptimally on the other task as expected. The existence of TF_alg_select showcases a single transformer that performs well on multiple tasks simultaneously (and thus has to perform in-context algorithm selection to some extent), supporting our theoretical results in Section 4.

## 7 Conclusion

This work shows that transformers can perform complex in-context learning procedures with strong in-context algorithm selection capabilties, by both explicit theoretical constructions and experiments. We believe our work opens up many exciting directions, such as (1) more mechanisms for in-context algorithm selection; (2) Bayes-optimal ICL on other problems by either the post-ICL validation mechanism or new approaches; (3) understanding the internal workings of transformers performing in-context algorithm selection; (4) other mechanisms for implementing complex ICL procedures beyond in-context algorithm selection; (5) further statistical analyses, e.g. of pretraining. Besides, this work focuses on the transformer architecture; alternative sequence-to-sequence architectures (such as RNNs) are beyond our scope but would be interesting directions for future work.

Figure 3: ICL capabilities of the transformer architecture used in our theoretical constructions. _(a)_ On five representative base tasks, transformers approximately match the best baseline algorithm for each task, when pretrained on the corresponding task. _(b,c)_ A **single transformer**TF_alg_select **simultaneously approaches the performance of the strongest baseline algorithm** on two separate tasks: _(b)_ noisy linear regression with two different noise levels \(\sigma\in\{0.1,0.5\}\), and _(c)_ adaptively selecting between regression and classification.

## Acknowledgment

The authors would like to thank Tengyu Ma and Jason D. Lee for the many insightful discussions. S. Mei is supported in part by NSF DMS-2210827 and NSF CCF-2315725.

## References

* Agarwal et al. [2010] A. Agarwal, S. Negahban, and M. J. Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. _Advances in Neural Information Processing Systems_, 23, 2010.
* Akyurek et al. [2022] E. Akyurek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-context learning? investigations with linear models. _arXiv preprint arXiv:2211.15661_, 2022.
* Ba et al. [2016] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* Bach [2017] F. Bach. Breaking the curse of dimensionality with convex neural networks. _The Journal of Machine Learning Research_, 18(1):629-681, 2017.
* Bai et al. [2021] Y. Bai, M. Chen, P. Zhou, T. Zhao, J. Lee, S. Kakade, H. Wang, and C. Xiong. How important is the train-validation split in meta-learning? In _International Conference on Machine Learning_, pages 543-553. PMLR, 2021.
* Bai et al. [2021] Y. Bai, S. Mei, H. Wang, and C. Xiong. Don't just blame over-parametrization for over-confidence: Theoretical analysis of calibration in binary classification. In _International Conference on Machine Learning_, pages 566-576. PMLR, 2021.
* Baxter [2000] J. Baxter. A model of inductive bias learning. _Journal of artificial intelligence research_, 12:149-198, 2000.
* Beck and Teboulle [2009] A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal recovery. _Convex optimization in signal processing and communications_, pages 42-88, 2009.
* Bengio et al. [2013] S. Bengio, Y. Bengio, J. Cloutier, and J. Gescei. On the optimization of a synaptic learning rule. In _Optimality in Biological and Artificial Networks'_, pages 281-303. Routledge, 2013.
* Bhattacharya et al. [2020] S. Bhattacharya, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to recognize formal languages. _arXiv preprint arXiv:2009.11264_, 2020.
* Bhattacharya et al. [2020] S. Bhattacharya, A. Patel, and N. Goyal. On the computational power of transformers and its implications in sequence modeling. _arXiv preprint arXiv:2006.09286_, 2020.
* Brown et al. [2020] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Bubeck [2015] S. Bubeck. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* Bubeck et al. [2023] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* Chan et al. [2022] S. Chan, A. Santoro, A. Lampinen, J. Wang, A. Singh, P. Richemond, J. McClelland, and F. Hill. Data distributional properties drive emergent in-context learning in transformers. _Advances in Neural Information Processing Systems_, 35:18878-18891, 2022.
* Chen et al. [2021] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.

* [17] K. Chua, Q. Lei, and J. D. Lee. How fine-tuning allows for effective meta-learning. _Advances in Neural Information Processing Systems_, 34:8871-8884, 2021.
* [18] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. _arXiv preprint arXiv:2212.10559_, 2022.
* [19] G. Denevi, C. Ciliberto, D. Stamos, and M. Pontil. Incremental learning-to-learn with statistical guarantees. _arXiv preprint arXiv:1803.08089_, 2018.
* [20] G. Denevi, C. Ciliberto, D. Stamos, and M. Pontil. Learning to learn around a common mean. _Advances in Neural Information Processing Systems_, 31, 2018.
* [21] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [22] E. Dobriban and S. Wager. High-dimensional asymptotics of prediction: Ridge regression and classification. _The Annals of Statistics_, 46(1):247-279, 2018.
* [23] L. Dong, S. Xu, and B. Xu. Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition. In _2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)_, pages 5884-5888. IEEE, 2018.
* [24] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui. A survey for in-context learning. _arXiv preprint arXiv:2301.00234_, 2022.
* [25] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [26] S. S. Du, W. Hu, S. M. Kakade, J. D. Lee, and Q. Lei. Few-shot learning via learning the representation, provably. _arXiv preprint arXiv:2002.09434_, 2020.
* [27] B. L. Edelman, S. Goel, S. Kakade, and C. Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_, pages 5793-5831. PMLR, 2022.
* [28] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021.
* [29] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.
* [30] C. Finn, A. Rajeswaran, S. Kakade, and S. Levine. Online meta-learning. In _International Conference on Machine Learning_, pages 1920-1930. PMLR, 2019.
* [31] S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022.
* [32] A. Giannou, S. Rajput, J.-y. Sohn, K. Lee, J. D. Lee, and D. Papailiopoulos. Looped transformers as programmable computers. _arXiv preprint arXiv:2301.13196_, 2023.
* [33] M. Hahn. Theoretical limitations of self-attention in neural sequence models. _Transactions of the Association for Computational Linguistics_, 8:156-171, 2020.
* [34] S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. In _Artificial Neural Networks--ICANN 2001: International Conference Vienna, Austria, August 21-25, 2001 Proceedings 11_, pages 87-94. Springer, 2001.
* [35] N. Hollmann, S. Muller, K. Eggensperger, and F. Hutter. Tabpfn: A transformer that solves small tabular classification problems in a second. _arXiv preprint arXiv:2207.01848_, 2022.

* [36] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural networks: A survey. _IEEE transactions on pattern analysis and machine intelligence_, 44(9):5149-5169, 2021.
* [37] J. Hron, Y. Bahri, J. Sohl-Dickstein, and R. Novak. Infinite attention: Nngp and ntk for deep attention networks. In _International Conference on Machine Learning_, pages 4376-4386. PMLR, 2020.
* [38] D. Hsu, S. M. Kakade, and T. Zhang. Random design analysis of ridge regression. In _Conference on learning theory_, pages 9-1. JMLR Workshop and Conference Proceedings, 2012.
* [39] S. Jelassi, M. E. Sander, and Y. Li. Vision transformers provably learn spatial structure. _arXiv preprint arXiv:2210.09221_, 2022.
* [40] K. Ji, J. D. Lee, Y. Liang, and H. V. Poor. Convergence of meta-learning with task-specific adaptation over partial parameters. _Advances in Neural Information Processing Systems_, 33:11490-11500, 2020.
* [41] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. vZidek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* [42] S. M. Kakade, V. Kanade, O. Shamir, and A. Kalai. Efficient learning of generalized linear and single index models with isotonic regression. _Advances in Neural Information Processing Systems_, 24, 2011.
* [43] M. Khodak, M.-F. F. Balcan, and A. S. Talwalkar. Adaptive gradient-based meta-learning methods. _Advances in Neural Information Processing Systems_, 32, 2019.
* [44] L. Kirsch and J. Schmidhuber. Meta learning backpropagation and improving it. _Advances in Neural Information Processing Systems_, 34:14122-14134, 2021.
* [45] L. Kirsch, J. Harrison, J. Sohl-Dickstein, and L. Metz. General-purpose in-context learning by meta-learning transformers. _arXiv preprint arXiv:2212.04458_, 2022.
* [46] K. Li and J. Malik. Learning to optimize. _arXiv preprint arXiv:1606.01885_, 2016.
* [47] Y. Li, M. E. Ildiz, D. Papailiopoulos, and S. Oymak. Transformers as algorithms: Generalization and implicit model selection in in-context learning. _arXiv preprint arXiv:2301.07067_, 2023.
* [48] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts to automata. _arXiv preprint arXiv:2210.10749_, 2022.
* [49] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What makes good in-context examples for gpt-\(3\)? _arXiv preprint arXiv:2101.06804_, 2021.
* [50] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. _arXiv preprint arXiv:2104.08786_, 2021.
* [51] A. Madani, B. McCann, N. Naik, N. S. Keskar, N. Anand, R. R. Eguchi, P.-S. Huang, and R. Socher. Progen: Language modeling for protein generation. _arXiv preprint arXiv:2004.03497_, 2020.
* [52] A. Maurer, M. Pontil, and B. Romera-Paredes. The benefit of multitask representation learning. _Journal of Machine Learning Research_, 17(81):1-32, 2016.
* [53] P. McCullagh. _Generalized linear models_. Routledge, 2019.
* [54] S. Mei, Y. Bai, and A. Montanari. The landscape of empirical risk for nonconvex losses. _The Annals of Statistics_, 46(6A):2747-2774, 2018.

* [55] S. Min, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Noisy channel language model prompting for few-shot text classification. _arXiv preprint arXiv:2108.04106_, 2021.
* [56] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context. _arXiv preprint arXiv:2110.15943_, 2021.
* [57] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? _arXiv preprint arXiv:2202.12837_, 2022.
* [58] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. _arXiv preprint arXiv:1707.03141_, 2017.
* [59] S. Muller, N. Hollmann, S. P. Arango, J. Grabocka, and F. Hutter. Transformers can do bayesian inference. _arXiv preprint arXiv:2112.10510_, 2021.
* [60] T. Nagler. Statistical foundations of prior-data fitted networks. _arXiv preprint arXiv:2305.11097_, 2023.
* [61] D. K. Naik and R. J. Mammone. Meta-neural networks that learn by learning. In _[Proceedings 1992] IJCNN International Joint Conference on Neural Networks_, volume 1, pages 437-442. IEEE, 1992.
* [62] S. N. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. 2012.
* [63] Y. Nesterov. _Lectures on convex optimization_, volume 137. Springer, 2018.
* [64] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022.
* [65] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [66] N. Parikh, S. Boyd, et al. Proximal algorithms. _Foundations and trends(r) in Optimization_, 1 (3):127-239, 2014.
* [67] J. Perez, J. Marinkovic, and P. Barcelo. On the turing completeness of modern neural network architectures. _arXiv preprint arXiv:1901.03429_, 2019.
* [68] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018.
* [69] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [70] A. Raventos, M. Paul, F. Chen, and S. Ganguli. The effects of pretraining task diversity on in-context learning of ridge regression. In _ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models_, 2023.
* [71] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In _International conference on learning representations_, 2017.
* [72] Y. Razeghi, R. L. Logan IV, M. Gardner, and S. Singh. Impact of pretraining term frequencies on few-shot reasoning. _arXiv preprint arXiv:2202.07206_, 2022.
* [73] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. _arXiv preprint arXiv:2205.06175_, 2022.
* [74] O. Rubin, J. Herzig, and J. Berant. Learning to retrieve prompts for in-context learning. _arXiv preprint arXiv:2112.08633_, 2021.

* [75] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with memory-augmented neural networks. In _International conference on machine learning_, pages 1842-1850. PMLR, 2016.
* [76] N. Saunshi, A. Gupta, and W. Hu. A representation learning perspective on the importance of train-validation splitting in meta-learning. In _International Conference on Machine Learning_, pages 9333-9343. PMLR, 2021.
* [77] J. Schmidhuber. _Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook_. PhD thesis, Technische Universitat Munchen, 1987.
* [78] K. Shen, J. Guo, X. Tan, S. Tang, R. Wang, and J. Bian. A study on relu and softmax in transformer. _arXiv preprint arXiv:2302.06461_, 2023.
* [79] C. Snell, R. Zhong, D. Klein, and J. Steinhardt. Approximating how single head attention learns. _arXiv preprint arXiv:2103.07601_, 2021.
* [80] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. _Advances in neural information processing systems_, 30, 2017.
* [81] S. Thrun and L. Pratt. _Learning to learn_. Springer Science & Business Media, 2012.
* [82] R. Tibshirani. Regression shrinkage and selection via the lasso. _Journal of the Royal Statistical Society: Series B (Methodological)_, 58(1):267-288, 1996.
* [83] N. Tripuraneni, M. Jordan, and C. Jin. On the theory of transfer learning: The importance of task diversity. _Advances in neural information processing systems_, 33:7852-7862, 2020.
* [84] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [85] R. Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [86] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent. _arXiv preprint arXiv:2212.07677_, 2022.
* [87] M. J. Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* [88] X. Wang, S. Yuan, C. Wu, and R. Ge. Guarantees for tuning the step size using a learning-to-learn approach. In _International Conference on Machine Learning_, pages 10981-10990. PMLR, 2021.
* [89] C. Wei, Y. Chen, and T. Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. _arXiv preprint arXiv:2107.13163_, 2021.
* [90] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.
* [91] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou, et al. Larger language models do in-context learning differently. _arXiv preprint arXiv:2303.03846_, 2023.
* [92] G. Weiss, Y. Goldberg, and E. Yahav. Thinking like transformers. In _International Conference on Machine Learning_, pages 11080-11090. PMLR, 2021.
* [93] M. Wortsman, J. Lee, J. Gilmer, and S. Kornblith. Replacing softmax with relu in vision transformers. _arXiv preprint arXiv:2309.08586_, 2023.
* [94] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian inference. _arXiv preprint arXiv:2111.02080_, 2021.

* [95] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process bounded hierarchical languages. _arXiv preprint arXiv:2105.11115_, 2021.
* [96] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu. Do transformers really perform badly for graph representation? _Advances in Neural Information Processing Systems_, 34:28877-28888, 2021.
* [97] C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal approximators of sequence-to-sequence functions? _arXiv preprint arXiv:1912.10077_, 2019.
* [98] Y. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner. Unveiling transformers with lego: a synthetic reasoning task. _arXiv preprint arXiv:2206.04301_, 2022.
* [99] Y. Zhang, B. Liu, Q. Cai, L. Wang, and Z. Wang. An analysis of attention via the lens of exchangeability and latent variable models. _arXiv preprint arXiv:2212.14852_, 2022.
* [100] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot performance of language models. In _International Conference on Machine Learning_, pages 12697-12706. PMLR, 2021.
* [101] X. Zuo, Z. Chen, H. Yao, Y. Cao, and Q. Gu. Understanding train-validation split in meta-learning with neural networks. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=JVlyfHEEm0k.

## Appendix A Related work

In-context learningThe in-context learning (ICL) capability of large language models (LLMs) has gained significant attention since demonstrated on GPT-3 Brown et al. [12]. A number of subsequent empirical studies have contributed to a better understanding of the capabilities and limitations of ICL in LLM systems, which include but are not limited to [49, 55, 56, 50, 100, 74, 72, 28, 45, 91]. For an overview of ICL, see the survey by Dong et al. [24] which highlights some key findings and advancements in this direction.

A line of recent work investigates why and how LLMs perform ICL [94, 31, 86, 2, 18, 32, 47, 70]. In particular, Xie et al. [94] propose a Bayesian inference framework explaining how ICL works despite formatting differences between training and inference distributions. Garg et al. [31] show empirically that transformers could be trained from scratch to perform ICL of linear models, sparse linear models, two-layer neural networks, and decision trees. Li et al. [47] analyze the generalization error of trained ICL transformers from a stability viewpoint. They also experimentally show that transformers could perform "in-context model selection" (conceptually similar to in-context algorithm selection considered in this work) in specific tasks and presented related theoretical hypotheses. However, they do not provide concrete mechanisms or constructions for in-context model selection. A recent work [99] shows that pretrained transformers can perform Bayesian inference in latent variable models, which may also be interpreted as a mechanism for ICL. Our experimental findings extend these results by unveiling and demonstrating the in-context algorithm selection capabilities of transformers.

Closely related to our theoretical results are [86, 2, 18, 32], which show (among many things) that transformers can perform ICL by simulating gradient descent. However, these results do not provide quantitative error bounds for simulating multi-step gradient descent, and only handle linear regression models or their simple variants. Among these works, Akyurek et al. [2] showed that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression; it also presented preliminary evidence that learned transformers perform ICL similar to Bayes-optimal ridge regression. Our work builds upon and substantially extends this line of work by (1) providing a more efficient construction for in-context gradient descent; (2) providing an end-to-end theory with additional results for pretraining and statistical power; (3) analyzing a broader spectrum of ICL algorithms, including least squares, ridge regression, Lasso, convex risk minimization for generalized linear models, and gradient descent on two-layer neural networks; and (4) constructing more complex ICL procedures using in-context algorithm selection.

When in-context data are generated from a prior, the Bayes risk is a theoretical lower bound for the risk of any possible ICL algorithm, including transformers. Xie et al. [94], Akyurek et al. [2] observe that learned transformers behave closely to the Bayes predictor on a variety of tasks such as hidden Markov models [94] and noisy linear regression with a fixed noise level [2, 47]. Using the in-context algorithm selection mechanism (more precisely the post-ICL validation mechanism), we show that transformers can perform nearly-Bayes optimal ICL in noisy linear models with mixed noise levels (a strictly more challenging task than considered in [2, 47]), with both concrete theoretical guarantees (Section 4.1.1) and empirical evidence (Figure 2 & 3b). Complementary to these works, a line of work on "prior-data fitted networks" [59, 60, 35] also empirically demonstrates the Bayesian optimality of transformers in various settings. Our expressivity results support these empirical findings and are applicable beyond the Bayesian setting, e.g. for providing frequentist in-context prediction guarantees for transformers.

Transformers and its theoryThe transformer architecture, introduced by [84], has revolutionized natural language processing and been adopted in most of the recently developed large language models such as BERT and GPT [68, 21, 12]. Broaderly, transformers have demonstrated remarkable performance in many other fields of artificial intelligence such as computer vision, speech, graph processing, reinforcement learning, and biological applications [23, 25, 51, 69, 96, 16, 41, 73, 65, 14]. Towards a better theoretical understanding, recent work has studied the capabilities [97, 67, 37, 95, 11, 98, 48], limitations [33, 10], and internal workings [28, 79, 92, 27, 64] of transformers.

We remark that the transformer architecture used in our theoretical constructions differs from the standard one by replacing the softmax activation (in the attention layers) with a (normalized) ReLU function. Transformers with ReLU activations is experimentally studied in the recent work of Shen et al. [78], who find that they perform as well as the standard softmax activation in many NLP tasks.

Meta-learningTraining models (such as transformers) to perform ICL can be viewed as an approach for the broader problem of learning-to-learn or meta-learning [77, 61, 81]. A number of other approaches has been studied extensively for this problem, including (and not limited to) training a meta-learner on how to update the parameters of a downstream learner [9, 46], learning parameter initializations that quickly adapt to downstream tasks [29, 71], learning latent embeddings that allow for effective similarity search [80]. Most relevant to the ICL setting are approaches that directly take as input examples from a downstream task and a query input and produce the corresponding output [34, 58, 75, 44]. For a comprehensive overview, see the survey [36].

Theoretical aspects of meta-learning have received significant recent interest [7, 52, 26, 83, 19, 30, 43, 40, 88, 20, 5, 76, 17, 101]. In particular, [52, 26, 83] analyzed the benefit of multi-task learning through a representation learning perspective, and [88, 20, 5, 76, 101] studied the statistical properties of learning the parameter initialization for downstream tasks.

TechniquesWe build on various existing techniques from the statistics and learning theory literature to establish our approximation and generalization guarantees for transformers. For the approximation component, we rely on a technical result of Bach [4] on the approximation power of ReLU networks. We use this result to show that transformers can approximate gradient descent (GD) on a broad range of loss functions, substantially extending the results of [86, 2, 18] who primarily consider the square loss. The recent work of Giannou et al. [32] also approximates GD with general loss functions by transformers, though using a different technique of forcing the softmax activations to act as sigmoids. Our analyses of Lasso and generalized linear models build on [87, 62, 1, 54]. Our generalization bound for transformers (used in our pretraining results) build on a chaining argument [87].

## Appendix B Technical tools

Additional notation for proofsWe say a random variable \(X\) is \(\sigma^{2}\)-sub-Gaussian (or \(\operatorname{SG}(\sigma)\) interchangeably) if \(\mathbb{E}[\exp(X^{2}/\sigma^{2})]\leq 2\). A random vector \(\mathbf{x}\in\mathbb{R}^{d}\) is \(\sigma^{2}\)-sub-Gaussian if \(\left\langle\mathbf{v},\mathbf{x}\right\rangle\) is \(\sigma^{2}\)-sub-Gaussian for all \(\left\|\mathbf{v}\right\|_{2}=1\). A random variable \(X\) is \(K\)-sub-Exponential (or \(\operatorname{SE}(K)\) interchangeably) if \(\mathbb{E}[\exp(\left|X\right|/K)]\leq 2\).

### Concentration inequalities

**Lemma B.1**.: _Let \(\boldsymbol{\beta}\sim\mathsf{N}(\mathbf{0},\mathbf{I}_{d}/d)\). Then we have_

\[\mathbb{P}\Big{(}\|\boldsymbol{\beta}\|_{2}^{2}\geq(1+\delta)^{2}\Big{)}\leq e ^{-d\delta^{2}/2}.\]

**Lemma B.2** (Theorem 6.1 of [87]).: _Let \(X=[X_{ij}]\in\mathbb{R}^{n\times d}\) be a Gaussian random matrix with \(X_{ij}\sim\mathsf{N}(0,1)\). Let \(\sigma_{\min}(X)\) and \(\sigma_{\min}(X)\) be the minimum and maximum singular value of \(X\), respectively. Then we have_

\[\mathbb{P}\Big{(}\sigma_{\max}(X)/\sqrt{n}\geq 1+\sqrt{d/n}+ \delta\Big{)} \leq e^{-n\delta^{2}/2},\] \[\mathbb{P}\Big{(}\sigma_{\min}(X)/\sqrt{n}\leq 1-\sqrt{d/n}- \delta\Big{)} \leq e^{-n\delta^{2}/2}.\]

The following lemma is a standard result of covariance concentration, see e.g. [85, Theorem 4.6.1].

**Lemma B.3**.: _Suppose that \(\mathbf{x}_{1},\cdots,\mathbf{x}_{N}\) are independent \(d\)-dimensional \(K\)-sub-Gaussian random vectors. Then as long as \(N\geq C_{0}d\), with probability at least \(1-\exp(-N/C_{0})\) we have_

\[\left\|\frac{1}{N}\sum_{i=1}^{N}\mathbf{x}_{i}\mathbf{x}_{i}^{\top}\right\|_ {\mathrm{op}}\leq 8K^{2},\]

_where \(C_{0}\) is a universal constant._

**Lemma B.4**.: _For random matrix \(\mathbf{X}=[x_{ij}]\in\mathbb{R}^{N\times d}\) with \(x_{ij}\stackrel{{\mathrm{iid}}}{{\sim}}\mathsf{N}(0,1)\) and \(\boldsymbol{\varepsilon}=[\varepsilon_{i}]\in\mathbb{R}^{N}\) with \(\varepsilon_{i}\stackrel{{\mathrm{iid}}}{{\sim}}\mathsf{N}(0, \sigma^{2})\), it holds that_

\[\mathbb{P}\Big{(}\big{\|}\mathbf{X}^{\top}\boldsymbol{\varepsilon}\big{\|}_{ \infty}\geq\sqrt{8N\sigma^{2}\log(2d/\delta)}\Big{)}\leq\delta+\exp(-N/2).\]

Proof.: We consider \(\mathbf{u}_{j}:=[x_{ij}]_{i}\in\mathbb{R}^{N}\), then \(\big{\|}\mathbf{X}^{\top}\boldsymbol{\varepsilon}\big{\|}_{\infty}=\max_{i\in[ d]}|\langle\mathbf{u}_{j},\boldsymbol{\varepsilon}\rangle|\). Notice that the random variables \(\langle\mathbf{u}_{1},\boldsymbol{\varepsilon}\rangle\,,\cdots\,,\langle \mathbf{u}_{d},\boldsymbol{\varepsilon}\rangle\) are independent \(\mathsf{N}(0,\|\boldsymbol{\varepsilon}\|_{2}^{2})\), and hence

\[\mathbb{P}\left(\max_{i\in[d]}|\langle\mathbf{u}_{j},\boldsymbol{\varepsilon }\rangle|\geq t\bigg{|}\,\boldsymbol{\varepsilon}\right)\leq 2d\exp\Bigg{(}-\frac{t^{2} }{2\,\|\boldsymbol{\varepsilon}\|_{2}^{2}}\Bigg{)}.\]

Further, by Lemma B.1, \(\mathbb{P}(\|\boldsymbol{\varepsilon}\|_{2}\geq 2\sigma\sqrt{N})\leq\exp(-N/2)\). Taking \(t=\sqrt{8N\sigma^{2}\log(2d/\delta)}\) completes the proof. 

### Approximation theory

For any signed measure \(\mu\) over a space \(\mathcal{W}\), let \(\mathrm{TV}(\mu):=\int_{\mathcal{W}}|d\mu(\mathbf{w})|\in[0,\infty]\) denote its total measure. Recall \(\sigma(\cdot)=\mathrm{ReLU}(\cdot)\) is the standard relu activation, and \(\mathsf{B}_{\infty}^{k}(R)=[-R,R]^{k}\) denotes the standard \(\ell_{\infty}\) ball in \(\mathbb{R}^{k}\) with radius \(R>0\).

**Definition B.1** (Sufficiently smooth \(k\)-variable function).: _We say a function \(g:\mathbb{R}^{k}\to\mathbb{R}\) is \((R,C_{\ell})\)-smooth, if for \(s=\lceil(k-1)/2\rceil+2\), \(g\) is a \(C^{s}\) function on \(\mathsf{B}_{\infty}^{k}(R)\), and_

\[\sup_{\mathbf{z}\in\mathsf{B}_{\infty}^{k}(R)}\big{\|}\nabla^{i}g(\mathbf{z} )\big{\|}_{\infty}=\sup_{\mathbf{z}\in\mathsf{B}_{\infty}^{k}(R)}\max_{j_{1}, \ldots,j_{i}\in[k]}|\partial_{x_{j_{1}}\ldots x_{j_{i}}}g(\mathbf{x})|\leq L _{i}\]

_for all \(i\in\{0,1,\ldots,s\}\), with \(\max_{0\leq i\leq s}L_{i}R^{i}\leq C_{\ell}\)._

The following result for expressing smooth functions as a random feature model with relu activation is adapted from Bach [4, Proposition 5].

**Lemma B.5** (Expressing sufficiently smooth functions by relu random features).: _Suppose function \(g:\mathcal{R}^{k}\to\mathbb{R}\) is \((R,C_{\ell})\) smooth. Then there exists a signed measure \(\mu\) over \(\mathcal{W}=\{\mathbf{w}\in\mathbb{R}^{k+1}:\|\mathbf{w}\|_{1}=1\}\) such that_

\[g(\mathbf{x})=\int_{\mathcal{W}}\frac{1}{R}\sigma(\mathbf{w}^{\top}[\mathbf{x };R])d\mu(\mathbf{w}),\qquad\forall\mathbf{x}\in\mathcal{X}\]

_and \(\mathrm{TV}(\mu)\leq C(k)C_{\ell}\), where \(C(k)<\infty\) is a constant that only depends on \(k\)._

**Lemma B.6** (Uniform finite-neuron approximation).: _Let \(\mathcal{X}\) be a space equipped with a distance function \(d_{\mathcal{X}}(\cdot,\cdot):\mathcal{X}\times\mathcal{X}\to\mathbb{R}_{\geq 0}\). Suppose function \(g:\mathcal{X}\to\mathbb{R}\) is given by_

\[g(\mathbf{x})=\int_{\mathcal{W}}\phi(\mathbf{x};\mathbf{w})d\mu(\mathbf{w}),\]_where \(\phi(\cdot;\cdot):\mathcal{X}\times\mathcal{W}\rightarrow[-B,B]\) is \(L\)-Lipschitz (in \(d_{\mathcal{X}}\)) in the first argument, and \(\mu\) is a signed measure over \(\mathcal{W}\) with finite total measure \(A=\mathrm{TV}(\mu)<\infty\). Then for any \(\varepsilon>0\), there exists \(\alpha_{1},\cdots,\alpha_{K}\in\{\pm 1\}\), \(\mathbf{w}_{1},\cdots,\mathbf{w}_{K}\in\mathcal{W}\) with \(K=\mathcal{O}(A^{2}B^{2}\log\mathcal{N}(\mathcal{X},d_{\mathcal{X}},\frac{ \varepsilon}{3AL})/\varepsilon^{2})\), such that_

\[\sup_{\mathbf{x}\in\mathcal{X}}\left|g(\mathbf{x})-\frac{A}{K}\sum_{i=1}^{K} \alpha_{i}\phi(\mathbf{x};\mathbf{w}_{i})\right|\leq\varepsilon,\]

_where \(\mathcal{N}(\mathcal{X},d_{\mathcal{X}},\frac{\varepsilon}{3AL})\) denotes the \((\frac{\varepsilon}{3AL})\)-covering number of \(\mathcal{X}\) in \(d_{\mathcal{X}}\)._

Proof.: Let \(\alpha(\mathbf{w}):=\mathrm{sign}(d\mu(\mathbf{w}))\in\{\pm 1\}\) denote the sign of the density \(d\mu(\mathbf{w})\). We have

\[g(\mathbf{x})=A\int_{\mathcal{W}}\alpha(\mathbf{w})\phi(\mathbf{x};\mathbf{w}) \times\frac{|d\mu(\mathbf{w})|}{A}.\] (9)

Note that \(|d\mu(\mathbf{w})|/A\) is the density of a probability distribution over \(\mathcal{W}\). Thus for any \(\mathbf{x}\in\mathcal{X}\), as long as \(K\geq\mathcal{O}(A^{2}B^{2}\log(1/\delta)/\varepsilon^{2})\), we can sample \(\mathbf{w}_{1},\ldots,\mathbf{w}_{K}\overset{\mathrm{lid}}{\sim}|d\mu(\cdot)|/A\), and obtain by Hoeffding's inequality that with probability at least \(1-\delta\),

\[\left|g(\mathbf{x})-\frac{A}{K}\sum_{i=1}^{K}\alpha(\mathbf{w}_{i})\phi( \mathbf{x};\mathbf{w}_{i})\right|\leq\varepsilon.\]

Let \(\mathcal{N}(\frac{\varepsilon}{3AL}):=\mathcal{N}(\mathcal{X},d_{\mathcal{X}}, \frac{\varepsilon}{3AL})\) for shorthand. By union bound, as long as \(K\geq\mathcal{O}(A^{2}B^{2}\log(\mathcal{N}(\frac{\varepsilon}{3AL})/\delta) /\varepsilon^{2})\), we have with probability at least \(1-\delta\) that for every \(\widehat{\mathbf{x}}\) in the covering set corresponding to \(\mathcal{N}(\frac{\varepsilon}{3AL})\),

\[\left|g(\widehat{\mathbf{x}})-\frac{A}{K}\sum_{i=1}^{K}\alpha(\mathbf{w}_{i}) \phi(\widehat{\mathbf{x}};\mathbf{w}_{i})\right|\leq\varepsilon/3.\]

Taking \(\delta=1/2\) (for which \(K=\mathcal{O}(A^{2}B^{2}\log\mathcal{N}(\frac{\varepsilon}{3AL})/\varepsilon^ {2})\)), by the probabilistic method, there exists a deterministic set \(\{\mathbf{w}_{i}\}_{i\in[K]}\subset\mathcal{W}\) and \(\left\{\alpha_{i}:=\alpha(\mathbf{w}_{i})\right\}_{i\in[K]}\in\{\pm 1\}\) such that the above holds.

Next, note that both \(g\) (by (9)) and the function \(\mathbf{x}\mapsto\frac{A}{K}\sum_{i=1}^{K}\alpha(\mathbf{w}_{i})\phi(\mathbf{ x};\mathbf{w}_{i})\) are \((AL)\)-Lipschitz. Therefore, for any \(\mathbf{x}\in\mathcal{X}\), taking \(\widehat{\mathbf{x}}\) to be the point in the covereing set with \(d_{\mathcal{X}}(\mathbf{x},\widehat{\mathbf{x}})\leq\frac{\varepsilon}{3AL}\), we have

\[\left|g(\mathbf{x})-\frac{A}{K}\sum_{i=1}^{K}\alpha(\mathbf{w}_{i })\phi(\mathbf{x};\mathbf{w}_{i})\right|\] \[\leq\left|g(\mathbf{x})-g(\widehat{\mathbf{x}})\right|+\left|g( \widehat{\mathbf{x}})-\frac{A}{K}\sum_{i=1}^{K}\alpha(\mathbf{w}_{i})\phi( \widehat{\mathbf{x}};\mathbf{w}_{i})\right|+\left|\frac{A}{K}\sum_{i=1}^{K} \alpha(\mathbf{w}_{i})\phi(\widehat{\mathbf{x}};\mathbf{w}_{i})-\frac{A}{K} \sum_{i=1}^{K}\alpha(\mathbf{w}_{i})\phi(\mathbf{x};\mathbf{w}_{i})\right|\] \[\leq AL\cdot\frac{\varepsilon}{3AL}+\frac{\varepsilon}{3}+AL \cdot\frac{\varepsilon}{3AL}=\varepsilon.\]

This proves the lemma. 

**Proposition B.1** (Approximating smooth \(k\)-variable functions).: _For any \(\varepsilon_{\mathrm{approx}}>0\), \(R\geq 1\), \(C_{\ell}>0\), we have the following: Any \((R,C_{\ell})\)-smooth function (Definition B.1) \(g:\mathbb{R}^{k}\rightarrow\mathbb{R}\) is \((\varepsilon_{\mathrm{approx}},R,M,C)\)-approximable by sum of relus (Definition D.1) with \(M\leq C(k)C_{\ell}^{2}\log(1+C_{\ell}/\varepsilon_{\mathrm{approx}})/ \varepsilon_{\mathrm{approx}}^{2})\) and \(C\leq C(k)C_{\ell}\), where \(C(k)>0\) is a constant that depends only on \(k\). In other words, there exists_

\[f(\mathbf{z})=\sum_{m=1}^{M}c_{m}\sigma(\mathbf{a}_{m}^{\top}[\mathbf{z};1]) \quad\mathrm{with}\quad\sum_{m=1}^{M}|c_{m}|\leq C,\qquad\max_{m\in[M]}\left\| \mathbf{a}_{m}\right\|_{1}\leq 1,\]

_such that \(\sup_{\mathbf{z}\in[-R,R]^{k}}|f(\mathbf{z})-g(\mathbf{z})|\leq\varepsilon_{ \mathrm{approx}}\)._Proof.: As function \(g:\mathcal{B}_{\infty}^{k}(R)\to\mathbb{R}\) is \((R,C_{\ell})\)-smooth, we can apply Lemma B.5 to obtain that there exists a signed measure \(\mu\) over \(\mathcal{W}:=\left\{\mathbf{w}\in\mathbb{R}^{k+1}:\left\|\mathbf{w}\right\|_{1} \leq 1\right\}\) such that

\[g(\mathbf{z})=\int_{\mathcal{W}}\frac{1}{R}\sigma(\mathbf{w}^{\top}[\mathbf{z} ;R])d\mu(\mathbf{w}),\qquad\forall\mathbf{z}\in[-R,R]^{k},\]

and \(A=\mathrm{TV}(\mu)\leq C(k)C_{\ell}\) where \(C(k)>0\) denotes a constant depending only on \(k\).

We now apply Lemma B.6 to approximate the above random feature by finitely many neurons. Let \(\mathbf{x}:=[\mathbf{z};R]\in\mathcal{X}:=[-R,R]^{k}\times\{R\}\). Then, the function \(\phi(\mathbf{x};\mathbf{w}):=\frac{1}{R}\sigma(\mathbf{w}^{\top}\mathbf{x})= \sigma(\frac{1}{R}\mathbf{w}^{\top}[\mathbf{z};R])\) is bounded by \(B=1\) and \((1/R)\)-Lipschitz in \(\mathbf{x}\) (in the standard \(\ell_{\infty}\)-distance). Further, we have \(\log\mathcal{N}(\mathcal{X},\left\|\cdot-\cdot\right\|_{\infty},\frac{\varepsilon _{\mathrm{approx}}}{3A/R})\leq\mathcal{O}(k\log(1+A/\varepsilon_{\mathrm{approx }}))\). We can thus apply Lemma B.6 to obtain that, for

\[M=\mathcal{O}\big{(}kA^{2}\log(1+A/\varepsilon_{\mathrm{approx}})/\varepsilon _{\mathrm{approx}}^{2}\big{)}=C(k)C_{\ell}^{2}\log(1+C_{\ell}/\varepsilon_{ \mathrm{approx}})/\varepsilon_{\mathrm{approx}}^{2},\]

there exists \(\boldsymbol{\alpha}=\left\{\alpha_{m}\right\}_{m\in[M]}\subset\{\pm 1\}\) and \(\mathbf{W}=\left\{\mathbf{w}_{m}\right\}_{m\in[M]}\subset\mathcal{W}=\left\{ \mathbf{w}\in\mathbb{R}^{k+1}:lone\mathbf{w}=1\right\}\) such that

\[\sup_{\mathbf{z}\in[-R,R]^{2}}\left|g(\mathbf{z})-f_{\boldsymbol{\alpha}, \mathbf{W}}(\mathbf{z})\right|\leq\varepsilon_{\mathrm{approx}},\]

where (recalling \(\mathbf{z}=[s;t]\))

\[f_{\boldsymbol{\alpha},\mathbf{W}}(\mathbf{z})=\frac{A}{M}\sum_{m=1}^{M} \alpha_{m}\sigma\bigg{(}\frac{1}{R}\mathbf{w}_{m}^{\top}[\mathbf{z};R]\bigg{)} =\sum_{m=1}^{M}\underbrace{\frac{A\alpha_{m}}{M}}_{c_{m}}\sigma \bigg{(}\underbrace{\Big{[}\frac{1}{R}\mathbf{w}_{m,1:k};w_{m,k+1}\Big{]}^{ \top}}_{\mathbf{a}_{m}^{\top}}[\mathbf{z};1]\big{)}.\]

Note that we have \(\sum_{m=1}^{M}\left|c_{m}\right|=A\leq C(k)C_{\ell}\), and \(\left\|\mathbf{a}_{m}\right\|_{1}\leq\left\|\mathbf{w}_{m}\right\|_{1}=1\). This is the desired result. 

### Optimization

The following convergence result for minimizing a smooth and strongly convex function is standard from the convex optimization literature, see e.g. Bubeck [13, Theorem 3.10].

**Proposition B.2** (Gradient descent for smooth and strongly convex functions).: _Suppose \(L:\mathbb{R}^{d}\to\mathbb{R}\) is \(\alpha\)-strongly convex and \(\beta\)-smooth for some \(0<\alpha\leq\beta\). Then, the gradient descent iterates \(\mathbf{w}_{\mathrm{GD}}^{t+1}:=\mathbf{w}_{\mathrm{GD}}^{t}-\eta\nabla L( \mathbf{w}_{\mathrm{GD}}^{t})\) with learning rate \(\eta=1/\beta\) and initialization \(\mathbf{w}_{\mathrm{GD}}^{0}\in\mathbb{R}^{d}\) satisfies for any \(t\geq 1\),_

\[\left\|\mathbf{w}_{\mathrm{GD}}^{t}-\mathbf{w}^{\star}\right\|_{2}^{2}\leq \exp\left(-t/\kappa\right)\cdot\left\|\mathbf{w}_{\mathrm{GD}}^{0}-\mathbf{w}^{ \star}\right\|_{2}^{2},\]

_where \(\kappa:=\beta/\alpha\) is the condition number of \(L\), and \(\mathbf{w}^{\star}:=\arg\min_{\mathbf{w}\in\mathbb{R}^{d}}L(\mathbf{w})\) is the minimizer of \(L\)._

The following convergence result of proximal gradient descent (PGD) on convex composite minimization problem is also standard, see e.g. [8].

**Proposition B.3** (Proximal gradient descent for convex function).: _Suppose \(L=f+h\), \(f:\mathbb{R}^{d}\to\mathbb{R}\) is convex and \(\beta\)-smooth for some \(\beta>0\), \(h:\mathbb{R}^{d}\to\mathbb{R}\) is a simple convex function. Then, the proximal gradient descent iterates \(\mathbf{w}_{\mathrm{PGD}}^{t+1}:=\mathbf{prox}_{\eta h}(\mathbf{w}_{\mathrm{ PGD}}^{\star}-\eta\nabla f(\mathbf{w}_{\mathrm{PGD}}^{t}))\) with learning rate \(\eta=1/\beta\) and initialization \(\mathbf{w}_{\mathrm{GD}}^{0}\in\mathbb{R}^{d}\) satisfies the following for any \(t\geq 1\):_

1. \(\{L(\mathbf{w}_{\mathrm{PGD}}^{t})\}\) _is a decreasing sequence._
2. _For any minimizer_ \(\mathbf{w}^{\star}\in\arg\min_{\mathbf{w}\in\mathbb{R}^{d}}L(\mathbf{w})\)_,_ \[L(\mathbf{w}_{\mathrm{GD}}^{t+1})-L(\mathbf{w}^{\star})\leq\frac{\beta}{2} \Big{(}\left\|\mathbf{w}_{\mathrm{PGD}}^{t}-\mathbf{w}^{\star}\right\|_{2}^{2}- \left\|\mathbf{w}_{\mathrm{PGD}}^{t+1}-\mathbf{w}^{\star}\right\|_{2}^{2}\Big{)},\] _and hence_ \(\Big{\{}\left\|\mathbf{w}_{\mathrm{PGD}}^{t}-\mathbf{w}^{\star}\right\|_{2}^{2} \Big{\}}\) _is also a decreasing sequence._
3. _For_ \(k\geq 1,t\geq 0\)_, it holds that_ \[L(\mathbf{w}_{\mathrm{GD}}^{t+k})-L(\mathbf{w}^{\star})\leq\frac{\beta}{2k} \left\|\mathbf{w}_{\mathrm{PGD}}^{t}-\mathbf{w}^{\star}\right\|_{2}^{2}.\]

### Uniform convergence

The following result is shown in [87, Section 5.6].

**Theorem B.1**.: _Suppose that \(\psi:[0,+\infty)\to[0,+\infty)\) is a convex, non-decreasing function that satisfies \(\psi(x+y)\geq\psi(x)\psi(y)\). For any random variable \(X\), we consider the Orlicz norm induced by \(\psi\): \(\left\|X\right\|_{\psi}:=\inf\left\{K>0:\mathbb{E}\psi(\left|X\right|/K)\right\}\leq 1\)._

_Suppose that \(\{X_{\theta}\}_{\theta}\) is a zero-mean random process indexed by \(\theta\in\Theta\) such that \(\left\|X_{\theta}-X_{\theta^{\prime}}\right\|_{\psi}\leq\rho(\theta,\theta^{ \prime})\) for some metric \(\rho\) on the space \(\Theta\). Then it holds that_

\[\mathbb{P}\Biggl{(}\sup_{\theta,\theta^{\prime}\in\Theta}\left|X_{\theta}-X_{ \theta^{\prime}}\right|\leq 8(J+t)\Biggr{)}\leq\frac{1}{\psi(t/D)}\ \forall t\geq 0,\]

_where \(D\) is the diameter of the metric space \((\Theta,\rho)\), and the generalized Dudley entropy integral \(J\) is given by_

\[J:=\int_{0}^{D}\psi^{-1}(N(\delta;\Theta,\rho))d\delta,\]

_where \(N(\delta;\Theta,\rho)\) is the \(\delta\)-covering number of \((\Theta,\rho)\)._

As a corollary of Theorem B.1, we have the following result.

**Proposition B.4** (Uniform concentration bound by chaining).: _Suppose that \(\{X_{\theta}\}_{\theta\in\Theta}\) is a zero-mean random process given by_

\[X_{\theta}:=\frac{1}{N}\sum_{i=1}^{N}f(z_{i};\theta)-\mathbb{E}_{z}[f(z;\theta)],\]

_where \(z_{1},\cdots,z_{N}\) are i.i.d samples from a distribution \(\mathbb{P}_{z}\) such that the following assumption holds:_

1. _The index set_ \(\Theta\) _is equipped with a distance_ \(\rho\) _and diameter_ \(D\)_. Further, assume that for some constant_ \(A\)_, for any ball_ \(\Theta^{\prime}\) _of radius_ \(r\) _in_ \(\Theta\)_, the covering number admits upper bound_ \(\log N(\delta;\Theta^{\prime},\rho)\leq d\log(2Ar/\delta)\) _for all_ \(0<\delta\leq 2r\)_._
2. _For any fixed_ \(\theta\in\Theta\) _and_ \(z\) _sampled from_ \(\mathbb{P}_{z}\)_, the random variable_ \(f(z;\theta)\) _is a_ \(\mathrm{SG}(B^{0})\)_-sub-Gaussian random variable._
3. _For any_ \(\theta,\theta^{\prime}\in\Theta\) _and_ \(z\) _sampled from_ \(\mathbb{P}_{z}\)_, the random variable_ \(f(z;\theta)-f(z;\theta^{\prime})\) _is a_ \(\mathrm{SG}(B^{1}\rho(\theta,\theta^{\prime}))\)_-sub-Gaussian random variable._

_Then with probability at least \(1-\delta\), it holds that_

\[\sup_{\theta\in\Theta}\left|X_{\theta}\right|\leq CB^{0}\sqrt{\frac{d\log(2A \kappa)+\log(1/\delta)}{N}},\]

_where \(C\) is a universal constant, and we denote \(\kappa=1+B^{1}D/B^{0}\)._

_Furthermore, if we replace the \(\mathrm{SG}\) in assumption (b) and (c) by \(\mathrm{SE}\), then with probability at least \(1-\delta\), it holds that_

\[\sup_{\theta\in\Theta}\left|X_{\theta}\right|\leq CB^{0}\Biggl{[}\sqrt{\frac{ d\log(2A\kappa)+\log(1/\delta)}{N}}+\frac{d\log(2A\kappa)+\log(1/\delta)}{N} \Biggr{]}.\]

Proof.: Fix a \(D_{0}\in(0,D]\) to be specified later. We pick a \((D_{0}/2)\)-covering \(\Theta_{0}\) of \(\Theta\) so that \(\log\left|\Theta_{0}\right|\leq d\log(2AD/D_{0})\). Then, by the standard uniform covering of independent sub-Gaussian random variables, we have with probability at least \(1-\delta/2\),

\[\sup_{\theta\in\Theta_{0}}\left|X_{\theta}\right|\leq CB^{0}\sqrt{\frac{d\log (2AD/D_{0})+\log(2/\delta)}{N}}.\]Assume that \(\Theta_{0}=\{\theta_{1},\cdots,\theta_{n}\}\). For each \(j\in[n]\), we consider \(\Theta_{j}\) is the ball centered at \(\theta_{j}\) of radius \(D_{0}\) in \((\Theta,\rho)\). Then \(\theta\in\Theta_{j}\) has diameter \(D_{0}\) and admits covering number bound \(\log\mathcal{N}(\Theta_{j},\delta)\leq d\log(AD_{0}/\delta)\). Hence, we can apply Theorem B.1 with the process \(\{X_{\theta}\}_{\theta\in\Theta_{j}}\), then

\[\psi=\psi_{2},\qquad\left\|X_{\theta}-X_{\theta^{\prime}}\right\|_{\psi}\leq \frac{B^{1}}{\sqrt{N}}\rho(\theta,\theta^{\prime}),\]

and a simple calculation yields

\[\mathbb{P}\!\left(\sup_{\theta,\theta^{\prime}\in\Theta_{j}}\left|X_{\theta}- X_{\theta^{\prime}}\right|\leq C^{\prime}B^{1}D_{0}\!\left(\sqrt{\frac{d\log(2A)}{N}}+t \right)\right)\leq 2\exp(-Nt^{2})\ \forall t\geq 0.\]

Therefore, we can let \(t\leq\sqrt{\log(2n/\delta)/N}\) in the above inequality and taking the union bound over \(j\in[n]\), and hence with probability at least \(1-\delta/2\), it holds that for all \(j\in[n]\),

\[\sup_{\theta,\theta^{\prime}\in\Theta_{j}}\left|X_{\theta}-X_{\theta^{\prime} }\right|\leq C^{\prime}B^{1}D_{0}\sqrt{\frac{2d\log(2AD/D_{0})+\log(4/\delta)} {N}}.\]

Notice that for each \(\theta\in\Theta\), there exists \(j\in[n]\) such that \(\theta\in\Theta_{j}\), and hence

\[\left|X_{\theta}\right|\leq\left|X_{\theta_{j}}\right|+\left|X_{\theta}-X_{ \theta_{j}}\right|.\]

Thus, with probability at least \(1-\delta\), it holds

\[\sup_{\theta\in\Theta}\left|X_{\theta}\right|\leq\sup_{\theta\in\Theta_{0}} \left|X_{\theta}\right|+\sup_{j}\sup_{\theta\in\Theta_{j}}\left|X_{\theta}-X_{ \theta_{j}}\right|\leq C^{\prime\prime}(B_{0}+B^{1}D_{0})\sqrt{\frac{d\log(2AD /D_{0})+\log(2/\delta)}{N}}.\]

Taking \(D_{0}=D/\kappa\) completes the proof of \(\mathrm{SG}\) case.

We next consider the \(\mathrm{SE}\) case. The idea is the same as the \(\mathrm{SG}\) case, but in this case we need to consider the following Orlicz-norm:

\[\psi_{N}(t)\coloneqq\exp\left(\frac{Nt^{2}}{t+1}\right)-1.\]

Then Bernstein's inequality of \(\mathrm{SE}\) random variables yields

\[\left\|X_{\theta}-X_{\theta^{\prime}}\right\|_{\psi_{N}}\leq C_{0}B^{1}\rho( \theta,\theta^{\prime})\]

for some universal constant \(C_{0}\). Therefore, we can repeat the argument above to deduce that with probability at least \(1-\delta\), it holds

\[\sup_{\theta\in\Theta}\left|X_{\theta}\right|\leq C^{\prime\prime}(B_{0}+B^{1} D_{0})\Bigg{[}\sqrt{\frac{d\log(2AD/D_{0})+\log(2/\delta)}{N}}+\frac{d\log(2AD/D_{0}) +\log(2/\delta)}{N}\Bigg{]}.\]

Taking \(D_{0}=D/\kappa\) completes the proof. 

### Useful properties of transformers

The following result can be obtained immediately by "joining" the attention heads and MLP layers of two single-layer transformers.

**Proposition B.5** (Joining parallel single-layer transformers).: _Suppose that \(P_{1}:\mathbb{R}^{(D_{0}+D_{1})\times N}\rightarrow\mathbb{R}^{D_{1}\times N },P_{2}:\mathbb{R}^{(D_{0}+D_{2})\times N}\rightarrow\mathbb{R}^{D_{2}\times N}\) are two sequence-to-sequence functions that are implemented by single-layer transformers, i.e. there exists \(\bm{\theta}_{1},\bm{\theta}_{2}\) such that_

\[\mathrm{TF}_{\bm{\theta}_{1}}: \mathbf{H}_{1}=\begin{bmatrix}\mathbf{h}_{1}^{(0)}\\ \mathbf{h}_{1}^{(1)}\end{bmatrix}_{1\leq i\leq N}\in\mathbb{R}^{(D_{0}+D_{1}) \times N}\mapsto\begin{bmatrix}\mathbf{H}^{(0)}\\ P_{1}(\mathbf{H}_{1})\end{bmatrix},\] \[\mathrm{TF}_{\bm{\theta}_{2}}: \mathbf{H}_{2}=\begin{bmatrix}\mathbf{h}_{1}^{(0)}\\ \mathbf{h}_{1}^{(2)}\end{bmatrix}_{1\leq i\leq N}\in\mathbb{R}^{(D_{0}+D_{2}) \times N}\mapsto\begin{bmatrix}\mathbf{H}^{(0)}\\ P_{2}(\mathbf{H}_{2})\end{bmatrix}.\]_Then, there exists \(\bm{\theta}\) such that for \(\mathbf{H}^{\prime}\) that takes form \(\mathbf{h}_{i}^{\prime}=[\mathbf{h}_{i}^{(0)};\mathbf{h}_{i}^{(1)};\mathbf{h}_{i} ^{(2)}]\), with \(\mathbf{h}_{i}^{(0)}\in\mathbb{R}^{D_{0}},\mathbf{h}_{i}^{(1)}\in\mathbb{R}^{D_ {1}},\mathbf{h}_{i}^{(2)}\in\mathbb{R}^{D_{2}}\), we have_

\[\mathrm{TF}_{\bm{\theta}}: \mathbf{H}^{\prime}=\begin{bmatrix}\mathbf{h}_{i}^{(0)}\\ \mathbf{h}_{i}^{(1)}\\ \mathbf{h}_{i}^{(2)}\end{bmatrix}_{1\leq i\leq N}\in\mathbb{R}^{(D_{0}+D_{1}+D _{2})\times N}\mapsto\begin{bmatrix}\mathbf{H}^{(0)}\\ P_{1}(\mathbf{H}_{1})\\ P_{2}(\mathbf{H}_{2})\end{bmatrix}.\]

_Further, \(\bm{\theta}\) has at most \(M\leq M_{1}+M_{2}\) heads, \(D^{\prime}\leq D_{1}^{\prime}+D_{2}^{\prime}\) hidden dimension in its MLP layer, and norm bound \(\llbracket\bm{\theta}\rrbracket\leq\llbracket\bm{\theta}_{1}\rrbracket+ \llbracket\bm{\theta}_{2}\rrbracket\)._

**Proposition B.6** (Joining parallel multi-layer transformers).: _Suppose that \(P_{1}:\mathbb{R}^{(D_{0}+D_{1})\times N}\to\mathbb{R}^{D_{1}\times N},P_{2}: \mathbb{R}^{(D_{0}+D_{2})\times N}\to\mathbb{R}^{D_{2}\times N}\) are two sequence-to-sequence functions that are implemented by multi-layer transformers, i.e. there exists \(\bm{\theta}_{1},\bm{\theta}_{2}\) such that_

\[\mathrm{TF}_{\bm{\theta}_{1}}: \mathbf{H}_{1}=\begin{bmatrix}\mathbf{h}_{i}^{(0)}\\ \mathbf{h}_{i}^{(2)}\end{bmatrix}_{1\leq i\leq N}\in\mathbb{R}^{(D_{0}+D_{1}) \times N}\mapsto\begin{bmatrix}\mathbf{H}^{(0)}\\ P_{1}(\mathbf{H}_{1})\end{bmatrix},\] \[\mathrm{TF}_{\bm{\theta}_{2}}: \mathbf{H}_{2}=\begin{bmatrix}\mathbf{h}_{i}^{(0)}\\ \mathbf{h}_{i}^{(2)}\end{bmatrix}_{1\leq i\leq N}\in\mathbb{R}^{(D_{0}+D_{2}) \times N}\mapsto\begin{bmatrix}\mathbf{H}^{(0)}\\ P_{2}(\mathbf{H}_{2})\end{bmatrix}.\]

_Then, there exists \(\bm{\theta}\) such that for \(\mathbf{H}^{\prime}\) that takes form \(\mathbf{h}_{i}^{\prime}=[\mathbf{h}_{i}^{(0)};\mathbf{h}_{i}^{(1)};\mathbf{h}_ {i}^{(2)}]\), with \(\mathbf{h}_{i}^{(0)}\in\mathbb{R}^{D_{0}},\mathbf{h}_{i}^{(1)}\in\mathbb{R}^{ D_{1}},\mathbf{h}_{i}^{(2)}\in\mathbb{R}^{D_{2}}\), we have_

\[\mathrm{TF}_{\bm{\theta}}: \mathbf{H}^{\prime}=\begin{bmatrix}\mathbf{h}_{i}^{(0)}\\ \mathbf{h}_{i}^{(1)}\\ \mathbf{h}_{i}^{(2)}\end{bmatrix}_{1\leq i\leq N}\in\mathbb{R}^{(D_{0}+D_{1}+D _{2})\times N}\mapsto\begin{bmatrix}\mathbf{H}^{(0)}\\ P_{1}(\mathbf{H}_{1})\\ P_{2}(\mathbf{H}_{2})\end{bmatrix}.\]

_Further, \(\bm{\theta}\) has at most \(L\leq\max\left\{L_{1},L_{2}\right\}\) layers, \(\max_{\ell\in[L]}M^{(\ell)}\leq\max_{\ell\in[L]}\left(M_{1}^{(\ell)}+M_{2}^{( \ell)}\right)\) heads, \(\max_{\ell\in[L]}D^{(\ell)}\leq\max_{\ell\in[L]}\left(D_{1}^{(\ell)}+D_{2}^{ (\ell)}\right)\) hidden dimension in its MLP layer (understanding the size of the empty layers as 0), and norm bound \(\llbracket\bm{\theta}\rrbracket\leq\llbracket\bm{\theta}_{1}\rrbracket+ \llbracket\bm{\theta}_{2}\rrbracket\)._

Proof.: When \(L_{1}=L_{2}\) (\(\bm{\theta}_{1}\) and \(\bm{\theta}_{2}\) have the same number of layers), the result follows directly by applying Proposition B.5 repeatedly for all \(L_{1}\) layers and the definition of the norm (2).

If (without loss of generality) \(L_{1}<L_{2}\), we can augment \(\bm{\theta}_{1}\) to \(L_{2}\) layers by adding \((L_{2}-L_{1})\) layers with zero attention heads, and zero MLP hidden dimension (note that this does not change \(M_{1}\), \(D_{1}^{\prime}\), and \(\llbracket\bm{\theta}_{1}\rrbracket\)). Due to the residual structure, the transformer maintains the output \(P_{1}(\mathbf{H}_{1})\) throughout layer \(L_{1}+1,\ldots,L_{2}\), and it reduces to the case \(L_{1}=L_{2}\). 

## Appendix C Extension to decoder-based architecture

Here we briefly discuss how our theoretical results can be adapted to decoder-based architectures (henceforth decoder TFs). Adopting the setting as in Section 2, we consider a sequence of \(N\) input vectors \(\left\{\mathbf{h}_{i}\right\}_{i=1}^{N}\subset\mathbb{R}^{D}\), written compactly as an input matrix \(\mathbf{H}=[\mathbf{h}_{1},\ldots,\mathbf{h}_{N}]\in\mathbb{R}^{D\times N}\). Recall that \(\sigma(t)\coloneqq\mathrm{ReLU}(t)=\max\left\{t,0\right\}\) denotes the standard relu activation.

### Decoder-based transformers

Decoder TFs are the same as encoder TFs, except that the attention layers are replaced by masked attention layers with a specific decoder-based (causal) attention mask.

**Definition C.1** (Masked attention layer).: _A masked attention layer with \(M\) heads is denoted as \(\mathrm{MAttn}_{\bm{\theta}}(\cdot)\) with parameters \(\bm{\theta}=\left\{(\mathbf{V}_{m},\mathbf{Q}_{m},\mathbf{K}_{m})\right\}_{m \in[M]}\subset\mathbb{R}^{D\times D}\). On any input sequence \(\mathbf{H}\in\mathbb{R}^{D\times N^{\prime}}\) with \(N^{\prime}\leq N\),_

\[\widetilde{\mathbf{H}}=\mathrm{MAttn}_{\bm{\theta}}(\mathbf{H}):=\mathbf{H}+\sum_ {m=1}^{M}(\mathbf{V}_{m}\mathbf{H})\times\left((\mathrm{MSK}_{1:N^{\prime},1:N^{ \prime}})\circ\sigma\big{(}(\mathbf{Q}_{m}\mathbf{H})^{\top}(\mathbf{K}_{m} \mathbf{H})\big{)}\right)\in\mathbb{R}^{D\times N^{\prime}},\] (10)_where \(\circ\) denotes the entry-wise (Hadamard) product of two matrices, and \(\mathrm{MSK}\in\mathbb{R}^{N\times N}\) is the mask matrix given by_

\[\mathrm{MSK}=\begin{bmatrix}1&1/2&1/3&\cdots&1/N\\ 0&1/2&1/3&\cdots&1/N\\ 0&0&1/3&\cdots&1/N\\ \cdots&\cdots&\cdots&\cdots&\cdots\\ 0&0&0&\cdots&1/N\end{bmatrix}.\]

_In vector form, we have_

\[\widetilde{\mathbf{h}}_{i}=\left[\mathrm{Attn}_{\boldsymbol{\theta}}(\mathbf{ H})\right]_{i}=\mathbf{h}_{i}+\sum_{m=1}^{M}\tfrac{1}{i}\sum_{j=1}^{i}\sigma( \langle\mathbf{Q}_{m}\mathbf{h}_{i},\mathbf{K}_{m}\mathbf{h}_{j}\rangle)\cdot \mathbf{V}_{m}\mathbf{h}_{j}.\]

Notice that standard masked attention definitions use the pre-activation additive masks (with mask value \(-\infty\)) [84]. The post-activation multiplicative masks we use is equivalent to the pre-activation additive masks, and the modified presentation is for notational convenience. We also use a normalized ReLU activation \(t\mapsto\sigma(t)/i\) in place of the standard softmax activation to be consistent with Definition 1. Note that the normalization \(1/i\) is to ensure that the attention weights \(\{\sigma(\langle\mathbf{Q}_{m}\mathbf{h}_{i},\mathbf{K}_{m}\mathbf{h}_{j} \rangle)/i\}_{j\in[i]}\) is a set of non-negative weights that sum to \(O(1)\). The motivation of masked attention layer is to ensure that, when processing a sequence of tokens, the computations at any token do not see any later token.

We next define the decoder-based transformers with \(L\geq 1\) transformer layers, each consisting of a masked attention layer (c.f. Definition C.1) followed by an MLP layer (c.f. Definition 2). This definition is similar to the definition of encoder-based transformers (c.f., Definition 3), except that we replace the attention layers by masked attention layers.

**Definition C.2** (Decoder-based Transformer).: _An \(L\)-layer decoder-based transformer, denoted as \(\mathrm{DTF}_{\boldsymbol{\theta}}(\cdot)\), is a composition of \(L\) self-attention layers each followed by an MLP layer: \(\mathbf{H}^{(L)}=\mathrm{DTF}_{\boldsymbol{\theta}}(\mathbf{H}^{(0)})\), where \(\mathbf{H}^{(0)}\in\mathbb{R}^{D\times N}\) is the input sequence, and_

\[\mathbf{H}^{(\ell)}=\mathrm{MLP}_{\boldsymbol{\theta}^{(\ell)}_{\mathrm{alg} }}\Big{(}\mathrm{MAttn}_{\boldsymbol{\theta}^{(\ell)}_{\mathrm{auto}}}\big{(} \mathbf{H}^{(\ell-1)}\big{)}\Big{)},\ \ \ell\in\{1,\ldots,L\}.\]

_Above, the parameter \(\boldsymbol{\theta}=(\boldsymbol{\theta}^{(1:L)}_{\mathrm{auto}},\boldsymbol{ \theta}^{(1:L)}_{\mathrm{alg}})\) is the parameter consisting of the attention layers \(\boldsymbol{\theta}^{(\ell)}_{\mathrm{auto}}=\{(\mathbf{V}^{(\ell)}_{m}, \mathbf{Q}^{(\ell)}_{m},\mathbf{K}^{(\ell)}_{m})\}_{m\in[M^{(\ell)}]}\subset \mathbb{R}^{D\times D}\) and the MLP layers \(\boldsymbol{\theta}^{(\ell)}_{\mathrm{alg}}=(\mathbf{W}^{(\ell)}_{1}, \mathbf{W}^{(\ell)}_{2})\in\mathbb{R}^{D^{(\ell)}\times D}\times\mathbb{R}^{D \times D^{(\ell)}}\). We will frequently consider "attention-only" decoder-based transformers with \(\mathbf{W}^{(\ell)}_{1},\mathbf{W}^{(\ell)}_{2}=\mathbf{0}\), which we denote as \(\mathrm{DTF}^{\boldsymbol{\theta}}_{\boldsymbol{\theta}}(\cdot)\) for shorthand, with \(\boldsymbol{\theta}=\boldsymbol{\theta}^{(1:L)}:=\boldsymbol{\theta}^{(1:L)}_ {\mathrm{auto}}\)._

We also use (2) to define the norm of \(\mathrm{DTF}_{\boldsymbol{\theta}}\).

### In-context learning with decoder-based transformers

We consider using decoder-based TFs to perform ICL. We encode \((\mathcal{D},\mathbf{x}_{N+1})\), which follows the generating rule as described in Section 2.2, into an input sequence \(\mathbf{H}\in\mathbb{R}^{D\times(2N+1)}\). In our theory, we use the following format, where the first two rows contain \((\mathcal{D},\mathbf{x}_{N+1})\) which alternating between \([\mathbf{x}_{i};0]\in\mathbb{R}^{d+1}\) and \([\mathbf{0}_{d\times 1};y_{i}]\in\mathbb{R}^{d+1}\) (the same setup as adopted in [31, 2]); The third row contains fixed vectors \(\{\mathbf{p}_{i}\}_{i\in[N+1]}\) with ones, zeros, the example index, and indicator for being the covariate token (similar to a positional encoding vector):

\[\mathbf{H}=\begin{bmatrix}\mathbf{x}_{1}&\mathbf{0}&\ldots&\mathbf{x}_{N}& \mathbf{0}&\mathbf{x}_{N+1}\\ 0&y_{1}&\ldots&0&y_{N}&0\\ \mathbf{p}_{1}&\mathbf{p}_{2}&\ldots&\mathbf{p}_{2N-1}&\mathbf{p}_{2N}& \mathbf{p}_{2N+1}\end{bmatrix},\ \ \ \mathbf{p}_{i}:=\begin{bmatrix}\mathbf{0}_{D-(d+4)}\\ \lceil i/2\rceil\\ 1\\ \mathrm{mod}(i+1,2)\end{bmatrix}\in\mathbb{R}^{D-(d+1)}.\] (11)

(11) is different from out input format (3) for encoder-based TFs. The main difference is that \((\mathbf{x}_{i},y_{i})\) are in different tokens in (11), whereas \((\mathbf{x}_{i},y_{i})\) are in the same token in (3). The reason for the former (i.e., different tokens in decoder) is that we want to avoid every \([\mathbf{x}_{i};0]\) token seeing the information of \(y_{i}\), since we will evaluate the loss at every token. The reason for the latter (i.e., the same token in encoder) is for presentation convenience: since we only evaluate the loss at the last token, it is not necessary to alternate between \([\mathbf{x}_{i};0]\) and \([\mathbf{0};y_{i}]\) to avoid information leakage.

We then feed \(\mathbf{H}\) into a decoder TF to obtain the output \(\widetilde{\mathbf{H}}=\operatorname{DTF}_{\boldsymbol{\theta}}(\mathbf{H})\in \mathbb{R}^{D\times(2N+1)}\) with the same shape, and _read out_ the prediction \(\widehat{y}_{N+1}\) from the \((d+1,2N+1)\)-th entry of \(\widetilde{\mathbf{H}}=[\widetilde{\mathbf{h}}_{i}]_{i\in[2N+1]}\) (the entry corresponding to the last missing test label): \(\widehat{y}_{N+1}=\mathsf{read}_{y}(\widetilde{\mathbf{H}}):=(\widetilde{ \mathbf{h}}_{2N+1})_{d+1}\). The goal is to predict \(\widehat{y}_{N+1}\) that is close to \(y_{N+1}\sim\mathsf{P}_{y|\mathbf{x}_{N+1}}\) measured by proper losses.

The benefit of using the decoder architecture is that, during the pre-training phase, one can construct the training loss function by using all the predictions \(\{\widehat{y}_{j}\}_{j\in[N+1]}\), where \(\widehat{y}_{j}\) gives the \((d+1,2j-1)\)-th entry of \(\widetilde{\mathbf{H}}=[\widetilde{\mathbf{h}}_{i}]_{i\in[2N+1]}\) for each \(j\in[N+1]\) (the entry corresponding to the missing test label of the \(2j-1\)'th token): \(\widehat{y}_{j}=\mathsf{read}_{y,j}(\widetilde{\mathbf{H}}):=(\widetilde{ \mathbf{h}}_{2j-1})_{d+1}\). Given a loss function \(\ell:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}\) associated to a single response, the training loss associated to the whole input sequence can be defined by \(\ell(\mathbf{H})=\sum_{j=1}^{N+1}\ell(y_{j},\widehat{y}_{j})\). This potentially enables less training sequences in the pre-training stage, and some generalization bound analysis justifying this benefit was provided in [47].

### Results

We discuss how our theoretical results upon encoder TFs can be converted to those of the decoder TFs. Taking the implementation of (ICGD) (a key mechanism that enables most basic ICL algorithms such as ridge regression; cf. Appendix D.1) as an example, this conversion is enabled by the following facts: (a) the input format (11) of decoders can be converted to the input format (3) of encoders by a 2-layer decoder TF; (b) the encoder TF that implements (ICGD) with input format (3), by a slight parameter modification, can be converted to a decoder TF that implements the (ICGD) algorithm with a converted input format.

Input format conversionDespite the difference between the input format (11) and (3), we show that there exists a 2-layer decoder TF that can convert the input format (11) to format (3). The proof can be found in Appendix C.4.

**Proposition C.1** (Input format conversion).: _There exists a 2-layer decoder TF \(\operatorname{DTF}\) with \(3\) heads per layer, hidden dimension \(2\) and \(\llbracket\boldsymbol{\theta}\rrbracket\leq 12\) such that upon taking input \(\mathbf{H}\) of format (11), it outputs \(\widetilde{\mathbf{H}}=\operatorname{DTF}(\mathbf{H})\) with_

\[\widetilde{\mathbf{H}}=\begin{bmatrix}\mathbf{x}_{1}&\mathbf{x}_{1}&\ldots& \mathbf{x}_{N}&\mathbf{x}_{N}&\mathbf{x}_{N+1}\\ 0&y_{1}&\ldots&0&y_{N}&0\\ \mathbf{p}_{1}&\mathbf{p}_{2}&\ldots&\mathbf{p}_{2N-1}&\mathbf{p}_{2N}& \mathbf{p}_{2N+1}\end{bmatrix}.\] (12)

_In particular, format (12) contains format (3) as a submatrix, by restricting to the \(\{1,2,\ldots,D-1,D-2,D\}\) rows and \(\{2,4,\ldots,2N-2,2N,2N+1\}\) columns._

Generalization TF constructions to decoder architectureThe construction in Theorem D.1 can be generalized to using the input format (12) along with a decoder TF, by using the scratch pad within the last token to record the gradient descent iterates. Further, if we slightly change the normalization in \(\operatorname{MSK}\) from \(1/i\) to \(1/((i-1)\lor 1)\), then the same construction performs (ICGD) (with training examples \(\{1,\ldots,j\}\)) at every token \(i=2j+1\) (corresponding to predicting at \(\mathbf{x}_{j+1}\)). Building on this extension, all our constructions in Section 3 and Section 4.2 can be generalized to decoder TFs.

### Proof of Proposition c.1

For the simplicity of presentation, we write \(c_{i}=\lceil i/2\rceil\), \(t_{i}=\operatorname{mod}(i+1,2)\), \(\mathbf{u}_{i}=\mathbf{h}_{i}[1:d]\in\mathbb{R}^{d+1}\) be the vector of first \(d\) entries of \(\mathbf{h}_{i}\)6, and let \(v_{i}=\mathbf{h}_{i}[d+1]\) be the \((d+1)\)-th entry of \(\mathbf{h}_{i}\). With such notations, the input sequence \(\mathbf{H}=[\mathbf{h}_{i}]_{i}\) can be compactly written as

Footnote 6: In other words, when \(2\nmid i\), \(\mathbf{u}_{i}=\mathbf{x}_{(i-1)/2}\); when \(2\mid i\), \(\mathbf{u}_{i}=\mathbf{0}_{d}\).

\[\mathbf{h}_{i}=[\mathbf{u}_{i};v_{i};\mathbf{0}_{D-d-4};c_{i};1;t_{i}].\]

In the following, we construct the desired \(\boldsymbol{\theta}=(\boldsymbol{\theta}^{(1)},\boldsymbol{\theta}^{(2)})\) as follows.

**Step 1:** construction of \(\boldsymbol{\theta}^{(1)}=(\boldsymbol{\theta}^{(1)}_{\mathsf{nattn}}, \boldsymbol{\theta}^{(1)}_{\mathsf{nlp}})\), so that \(\operatorname{MLP}_{\boldsymbol{\theta}^{(1)}_{\mathsf{nlp}}}\circ \operatorname{MAttn}_{\boldsymbol{\theta}^{(1)}_{\mathsf{nattn}}}\) maps

\[\mathbf{h}_{i}\quad\xrightarrow{\operatorname{MAttn}_{\boldsymbol{\theta}^{(1) }_{\mathsf{nattn}}}}\quad\mathbf{h}^{\prime}_{i}=[\mathbf{u}_{i};v_{i}; \mathbf{0}_{D-d-6};t_{i}(c_{i}^{2}+0.5);t_{i}c_{i};c_{i};1;t_{i}]\]\[\xrightarrow{\mathrm{MLP}_{\bm{\theta}^{(1)}_{\bm{\mathrm{alg}}}}} \mathbf{h}^{(1)}_{i}=[\mathbf{u}_{i};v_{i};\mathbf{0}_{D-d-6};t_{i}c_{i}^{2 };t_{i}c_{i};c_{i};1;t_{i}].\]

For \(m\in\{0,1\}\), we define matrices \(\mathbf{Q}^{(1)}_{m},\mathbf{K}^{(1)}_{m},\mathbf{V}^{(1)}_{m}\in\mathbb{R}^{D \times D}\) such that

\[\mathbf{Q}^{(1)}_{0}\mathbf{h}_{i}=\mathbf{Q}^{(1)}_{1}\mathbf{h}_{i}= \begin{bmatrix}t_{i}\\ \mathbf{0}\end{bmatrix},\qquad\mathbf{K}^{(1)}_{0}\mathbf{h}_{j}=\mathbf{K}^{( 1)}_{1}\mathbf{h}_{j}=\begin{bmatrix}c_{j}\\ \mathbf{0}\end{bmatrix},\qquad\mathbf{V}^{(1)}_{0}\mathbf{h}_{j}=\begin{bmatrix} \mathbf{0}_{D-4}\\ \mathbf{0}_{3}\end{bmatrix},\qquad\mathbf{V}^{(1)}_{1}\mathbf{h}_{j}=\begin{bmatrix} \mathbf{0}_{D-3}\\ \mathbf{0}_{2}\end{bmatrix},\]

for all \(i,j\). By the structure of \(\mathbf{h}_{i}\), these matrices indeed exist, and further it is straightforward to check that they have norm bounds

\[\max_{m}\left\|\mathbf{Q}^{(1)}_{m}\right\|_{\mathrm{op}}\leq 1,\quad\max_{m} \left\|\mathbf{K}^{(1)}_{m}\right\|_{\mathrm{op}}\leq 1,\quad\sum_{m}\left\| \mathbf{V}^{(1)}_{m}\right\|_{\mathrm{op}}\leq 5.\]

Now, for every \(i\),

\[\frac{1}{i}\sum_{j=1}^{i}\sum_{m\in\{0,1\}}\sigma\Big{(}\Big{\langle}\mathbf{Q }^{(1)}_{m}\mathbf{h}_{i},\mathbf{K}^{(1)}_{m}\mathbf{h}_{j}\Big{\rangle} \Big{)}\mathbf{V}^{(1)}_{m}\mathbf{h}_{j}=\frac{1}{i}\sum_{j=1}^{i}t_{i}\cdot[ \mathbf{0}_{D-4};3c_{j}^{2};2c_{j};0;0].\]

Notice that \(t_{i}\neq 0\) only when \(2\mid i\), we then compute for \(i=2k\) that

\[\sum_{j=1}^{i}3c_{j}^{2}=3\cdot\frac{k(k-1)(2k-1)}{3}+3k^{2}=2k^{3}+k,\qquad \sum_{j=1}^{i}2c_{j}=2\cdot k(k-1)+2k=2k^{2}.\]

Therefore, the \(\bm{\theta}^{(1)}_{\mathtt{matt}}=\{(\mathbf{Q}^{(1)}_{m},\mathbf{K}^{(1)}_{m},\mathbf{V}^{(1)}_{m}\in\mathbb{R}^{D\times D})\}_{m\in\{0,1\}}\) we construct above is indeed the desired attention layer. The existence of the desired \(\bm{\theta}^{(1)}_{\mathtt{alg}}\) is clear, and \(\bm{\theta}^{(1)}_{\mathtt{alg}}=(\mathbf{W}^{(1)}_{1},\mathbf{W}^{(1)}_{2})\) can further be chosen so that \(\|\mathbf{W}^{(1)}_{1}\|_{\mathrm{op}}\leq 1,\|\mathbf{W}^{(1)}_{2}\|_{ \mathrm{op}}\leq 1\).

**Step 2:** construction of \(\bm{\theta}^{(2)}\). For every \(m\in\{-1,0,1\}\), we define matrices \(\mathbf{Q}^{(2)}_{m},\mathbf{K}^{(2)}_{m},\mathbf{V}^{(2)}_{m}\in\mathbb{R}^{D \times D}\) such that

\[\mathbf{Q}^{(2)}_{0}\mathbf{h}^{(1)}_{i}=\mathbf{Q}^{(2)}_{1} \mathbf{h}^{(1)}_{i}=\mathbf{Q}^{(2)}_{-1}\mathbf{h}^{(1)}_{i}=\begin{bmatrix} t_{i}c_{i}^{2}\\ t_{i}c_{i}\\ \mathbf{0}\end{bmatrix},\] \[\mathbf{K}^{(2)}_{0}\mathbf{h}^{(1)}_{j}=\begin{bmatrix}1\\ -c_{j}\\ \mathbf{0}\end{bmatrix},\qquad\mathbf{K}^{(2)}_{1}\mathbf{h}^{(1)}_{j}= \begin{bmatrix}1\\ -(c_{j}+1)\\ \mathbf{0}\end{bmatrix},\qquad\mathbf{K}^{(2)}_{1}\mathbf{h}^{(1)}_{j}= \begin{bmatrix}1\\ -(c_{j}-1)\\ \mathbf{0}\end{bmatrix},\] \[\mathbf{V}^{(2)}_{0}\mathbf{h}^{(1)}_{j}=\begin{bmatrix}-4 \mathbf{u}_{j}\\ \mathbf{0}_{D-d}\end{bmatrix},\qquad\mathbf{V}^{(2)}_{1}\mathbf{h}^{(1)}_{j}= \mathbf{V}^{(2)}_{-1}\mathbf{h}^{(1)}_{j}=\begin{bmatrix}2\mathbf{u}_{j}\\ \mathbf{0}_{D-d}\end{bmatrix},\]

for all \(i,j\). By the structure of \(\mathbf{h}^{(1)}_{i}\), these matrices indeed exist, and further it is straightforward to check that they have norm bounds

\[\max_{m}\left\|\mathbf{Q}^{(2)}_{m}\right\|_{\mathrm{op}}\leq 1,\quad\max_{m} \left\|\mathbf{K}^{(2)}_{m}\right\|_{\mathrm{op}}\leq 2,\quad\sum_{m}\left\| \mathbf{V}^{(2)}_{m}\right\|_{\mathrm{op}}\leq 8.\]

Now, for every \(i,j\), we have

\[\sum_{m\in\{-1,0,1\}}\sigma\Big{(}\Big{\langle}\mathbf{Q}^{(2)}_{ m}\mathbf{h}^{(1)}_{i},\mathbf{K}^{(2)}_{m}\mathbf{h}^{(1)}_{j}\Big{\rangle} \Big{)}\Big{)}\mathbf{V}^{(2)}_{m}\mathbf{h}^{(1)}_{j}\] \[=\] \[= \ \mathbb{I}(c_{i}=c_{j})\cdot 2c_{i}t_{i}[\mathbf{u}_{j};\mathbf{0}_{D- d}],\]

where the last equality follows from the fact that

\[-2\sigma(x)+\sigma(x-1)+\sigma(x+1)=\begin{cases}0,&x\geq 1\;\text{or}\;x\leq-1,\\ x+1,&x\in[-1,0],\\ 1-x,&x\in[0,1].\end{cases}\]Therefore,

\[\frac{1}{i}\sum_{j=1}^{i}\sum_{m\in\{-1,0,1\}}\sigma\Big{(}\Big{\langle} \mathbf{Q}_{m}^{(2)}\mathbf{h}_{i}^{(1)},\mathbf{K}_{m}^{(2)}\mathbf{h}_{j}^{(1) }\Big{\rangle}\Big{)}\mathbf{V}_{m}^{(2)}\mathbf{h}_{j}^{(1)} =\frac{1}{i}\sum_{j=1}^{i}2\mathbb{I}(c_{i}=c_{j})c_{i}t_{i}[ \mathbf{u}_{j};\mathbf{0}_{D-d}]\] \[=\begin{cases}[\mathbf{x}_{k};\mathbf{0}_{D-d}],&i=2k\\ \mathbf{0}_{D},&\text{otherwise}\end{cases}.\]

Therefore, the \(\boldsymbol{\theta}_{\mathtt{mattn}}^{(2)}=\{(\mathbf{Q}_{m}^{(2)},\mathbf{K }_{m}^{(2)},\mathbf{V}_{m}^{(2)}\in\mathbb{R}^{D\times D})\}_{m\in\{-1,0,1\}}\) we construct above maps

\[\mathbf{h}_{i}^{(1)}\quad\to\quad\mathbf{h}_{i}^{\prime\prime}=[\mathbf{x}_{ \lceil i/2\rceil};v_{i};\mathbf{0}_{D-d-6};t_{i}c_{i}^{2};t_{c}c_{i};c_{i};1; t_{i}].\]

Finally, we only need to take a MLP layer \(\boldsymbol{\theta}_{\mathtt{alg}}^{(2)}=(\mathbf{W}_{1}^{(2)},\mathbf{W}_{2 }^{(2)})\) with hidden dimension 2 that maps

\[\mathbf{h}_{i}^{\prime\prime}\quad\to\quad\mathbf{h}_{i}^{(2)}=[\mathbf{x}_{ \lceil i/2\rceil};v_{i};\mathbf{0}_{D-d-6};0;0;c_{i};1;t_{i}],\]

which clearly exists and can be chosen so that \(\|\mathbf{W}_{1}^{(2)}\|_{\mathrm{op}}\leq 1,\|\mathbf{W}_{2}^{(2)}\|_{ \mathrm{op}}\leq 1\).

Combining the two steps above, we complete the proof of Proposition C.1. 

## Appendix D Mechanism: In-context gradient descent

Technically, the constructions in Section 3.1-3.2 rely on a new efficient construction for transformers to implement in-context gradient descent and its variants, which we present as follows. We begin by presenting the result for implementing (vanilla) gradient descent on convex empirical risks.

Compact notation of inputWe will often use shorthand \(y_{i}^{\prime}\in\mathbb{R}\) defined as \(y_{i}^{\prime}=y_{i}\) for \(i\in[N]\) and \(y_{N+1}^{\prime}=0\) to simplify our notation, with which the input sequence \(\mathbf{H}\in\mathbb{R}^{D\times(N+1)}\) can be compactly written as \(\mathbf{h}_{i}=[\mathbf{x}_{i};y_{i}^{\prime};\mathbf{p}_{i}]=[\mathbf{x}_{i} ;y_{i}^{\prime};\mathbf{0}_{D-d-3};1;t_{i}]\) for \(i\in[N+1]\), where \(t_{i}:=1\{i<N+1\}\) is the indicator for the training examples.

### Gradient descent on convex empirical risk

Let \(\ell(\cdot,\cdot):\mathbb{R}^{2}\to\mathbb{R}\) be a loss function. Let \(\widehat{L}_{N}(\mathbf{w}):=\frac{1}{N}\sum_{i=1}^{N}\ell(\mathbf{w}^{\top} \mathbf{x}_{i},y_{i})\) denote the empirical risk with loss function \(\ell\) on dataset \(\{(\mathbf{x}_{i},y_{i})\}_{i\in[N]}\), and

\[\mathbf{w}_{\mathrm{GD}}^{t+1}:=\mathbf{w}_{\mathrm{GD}}^{t}-\eta\nabla \widehat{L}_{N}(\mathbf{w}_{\mathrm{GD}}^{t})\] (ICGD)

denote the gradient descent trajectory on \(\widehat{L}_{N}\) with initialization \(\mathbf{w}_{\mathrm{GD}}^{0}\in\mathbb{R}^{d}\) and learning rate \(\eta>0\).

We require the partial derivative of the loss \(\partial_{s}\ell:(s,t)\mapsto\partial_{s}\ell(s,t)\) (as a bivariate function) to be approximable by a sum of relus, defined as follows.

**Definition D.1** (Approximability by sum of relus).: _A function \(g:\mathbb{R}^{k}\to\mathbb{R}\) is \((\varepsilon_{\mathrm{approx}},R,M,C)\)-approximable by sum of relus, if there exists a "\((M,C)\)-sum of relus" function_

\[f_{M,C}(\mathbf{z})=\sum_{m=1}^{M}c_{m}\sigma(\mathbf{a}_{m}^{\top}[\mathbf{z };1])\quad\mathrm{with}\quad\sum_{m=1}^{M}|c_{m}|\leq C,\ \max_{m\in[M]}\left\|\mathbf{a}_{m}\right\|_{1}\leq 1,\ \mathbf{a}_{m}\in\mathbb{R}^{k+1},\ c_{m}\in\mathbb{R},\]

_such that \(\sup_{\mathbf{z}\in[-R,R]^{k}}|g(\mathbf{z})-f_{M,C}(\mathbf{z})|\leq \varepsilon_{\mathrm{approx}}\)._

Definition D.1 is known to contain broad class of functions. For example, any mildly smooth \(k\)-variate function is approximable by a sum of relus for any \((\varepsilon_{\mathrm{approx}},R)\), with mild bounds on \((M,C)\) (Proposition B.1, building on results of Bach [4]). Also, any function that is a \((M,C)\)-sum of relus itself (which includes all piecewise linear functions) is by definition \((0,\infty,M,C)\)-approximable by sum of relus.

We show that \(L\) steps of (ICGD) can be approximately implemented by an \((L+1)\)-layer transformer.

**Theorem D.1** (Convex ICGD).: _Fix any \(B_{w}>0\), \(L>1\), \(\eta>0\), and \(\varepsilon\leq B_{w}/(2L)\). Suppose that_

1. _The loss_ \(\ell(\cdot,\cdot)\) _is convex in the first argument;_
2. \(\partial_{s}\ell\) _is_ \((\varepsilon,R,M,C)\)_-approximable by sum of relus with_ \(R=\max\left\{B_{x}B_{w},B_{y},1\right\}\)_.__Then, there exists an attention-only transformer \(\mathrm{TF}^{0}_{\bm{\theta}}\) with \((L+1)\) layers, \(\max_{\ell\in[L]}M^{(\ell)}\leq M\) heads within the first \(L\) layers, and \(M^{(L+1)}=2\) such that for any input data \((\mathcal{D},\mathbf{x}_{N+1})\) such that_

\[\sup_{\|\mathbf{w}\|_{2}\leq B_{w}}\lambda_{\max}(\nabla^{2}\widehat{L}_{N}( \mathbf{w}))\leq 2/\eta,\qquad\exists\mathbf{w}^{\star}\in\operatorname*{arg\, min}_{\mathbf{w}\in\mathbb{R}^{d}}\widehat{L}_{N}(\mathbf{w})\text{ such that }\|\mathbf{w}^{\star}\|_{2}\leq B_{w}/2,\]

\(\mathrm{TF}^{0}_{\bm{\theta}}(\mathbf{H}^{(0)})\) _approximately implements (ICGD) with initialization_ \(\mathbf{w}^{0}_{\mathrm{GD}}=\mathbf{0}\)_:_

1. _(Parameter space) For every_ \(\ell\in[L]\)_, the_ \(\ell\)_-th layer's output_ \(\mathbf{H}^{(\ell)}=\mathrm{TF}^{0}_{\bm{\theta}^{(L,\ell)}}(\mathbf{H}^{(0)})\) _approximates_ \(\ell\) _steps of (ICGD): We have_ \(\mathbf{h}^{(\ell)}_{i}=\left[\mathbf{x}_{i};y^{\prime}_{i};\widehat{\mathbf{w }}^{\ell};\mathbf{0}_{D-2d-3};1;t_{i}\right]\) _for every_ \(i\in[N+1]\)_, where_ \[\left\|\widehat{\mathbf{w}}^{\ell}-\mathbf{w}^{\ell}_{\mathrm{GD}}\right\|_{2} \leq\varepsilon\cdot(\ell\eta B_{x}).\] _Note that the bound scales as_ \(\mathcal{O}(\ell)\)_, a linear error accumulation._
2. _(Prediction space) The final output_ \(\mathbf{H}^{(L+1)}=\mathrm{TF}^{0}_{\bm{\theta}}(\mathbf{H}^{(0)})\) _approximates the prediction of_ \(L\) _steps of (ICGD): We have_ \(\mathbf{h}^{(L+1)}_{N+1}=\left[\mathbf{x}_{N+1};\widehat{y}_{N+1};\widehat{ \mathbf{w}}^{L};\mathbf{0}_{D-2d-3};1;t_{i}\right]\)_, where_ \(\widehat{y}_{N+1}=\left\langle\widehat{\mathbf{w}}^{L},\mathbf{x}_{N+1}\right\rangle\) _so that_ \[\left|\widehat{y}_{N+1}-\left\langle\mathbf{w}^{L}_{\mathrm{GD}},\mathbf{x}_{N +1}\right\rangle\right|\leq\varepsilon\cdot(L\eta B_{x}^{2}).\]

_Further, the transformer admits norm bound \(\left\|\bm{\theta}\right\|\leq 2+R+2\eta C\)._

The proof can be found in Appendix E.2. Theorem D.1 substantially generalizes that of von Oswald et al. [86] (which only does GD on square losses with a _linear_ self-attention), and is simpler than the ones in Akyurek et al. [2] and Giannou et al. [32]. See Figure 4 for a pictorial illustration of the basic component of the construction, which implements a single step of gradient descent using a single attention layer (Proposition E.1).

Technically, we utilize the stability of _convex_ gradient descent as in the following lemma (proof in Appendix E.3) to obtain the _linear_ error accumulation in Theorem D.1; the error accumulation will become _exponential_ in \(L\) in the non-convex case in general; see Lemma D.3(b).

**Lemma D.1** (Composition of error for approximating convex GD).: _Suppose \(f:\mathbb{R}^{d}\to\mathbb{R}\) is a convex function. Let \(\mathbf{w}^{\star}\in\operatorname*{arg\,min}_{\mathbf{w}\in\mathbb{R}^{d}}f (\mathbf{w})\), \(R\geq 2\|\mathbf{w}^{\star}\|_{2}\), and assume that \(\nabla f\) is \(L_{f}\)-smooth on \(\mathsf{B}^{d}_{2}(R)\). Let sequences \(\{\widehat{\mathbf{w}}^{\ell}\}_{\ell\geq 0}\subset\mathbb{R}^{d}\) and \(\{\mathbf{w}^{\ell}_{\mathrm{GD}}\}_{\ell\geq 0}\subset\mathbb{R}^{d}\) be given by \(\widehat{\mathbf{w}}^{0}=\mathbf{w}^{0}_{\mathrm{GD}}=\mathbf{0}\),_

\[\begin{cases}\widehat{\mathbf{w}}^{\ell+1}=\widehat{\mathbf{w}}^{\ell}-\eta \nabla f(\widehat{\mathbf{w}}^{\ell})+\bm{\varepsilon}^{\ell},\qquad\|\bm{ \varepsilon}^{\ell}\|_{2}\leq\varepsilon,\\ \mathbf{w}^{\ell+1}_{\mathrm{GD}}=\mathbf{w}^{\ell}_{\mathrm{GD}}-\eta\nabla f (\mathbf{w}^{\ell}_{\mathrm{GD}}),\end{cases}\]

_for all \(\ell\geq 0\). Then as long as \(\eta\leq 2/L_{f}\), for any \(0\leq L\leq R/(2\varepsilon)\), it holds that \(\left\|\widehat{\mathbf{w}}^{L}-\mathbf{w}^{L}_{\mathrm{GD}}\right\|_{2}\leq L\varepsilon\) and \(\|\widehat{\mathbf{w}}^{L}\|_{2}\leq\frac{R}{2}+L\varepsilon\leq R\)._

Figure 4: Illustration of our main mechanism for implementing basic ICL algorithms: One attention layer implements a single (ICGD) iterate (Proposition E.1 & Theorem D.1). Top: the attention mechanism as in Definition 1. Bottom: A single (ICGD) iterate. Middle: Linear algebraic illustration of the attention layer for implementing a GD update.

### Proximal gradient descent for regularized convex losses

Proximal gradient descent (PGD) is a variant of gradient descent that is suitable for minimizing regularized risks [66], in particular those with a non-smooth regularizer such as the \(\ell_{1}\) norm. In this section, we show that transformers can approximate PGD with similar quantitative guarantees as for GD in Appendix D.1.

Let \(\ell(\cdot,\cdot):\mathbb{R}^{2}\to\mathbb{R}\) be a loss function. Let \(\widehat{L}_{N}(\mathbf{w}):=\frac{1}{N}\sum_{i=1}^{N}\ell(\mathbf{w}^{\top} \mathbf{x}_{i},y_{i})+\mathcal{R}(\mathbf{w})\) denote the regularized empirical risk with loss function \(\ell\) on dataset \(\{(\mathbf{x}_{i},y_{i})\}_{i\in[N]}\) and regularizer \(\mathcal{R}\). To minimize \(\widehat{L}_{N}\), we consider the proximal gradient descent trajectory on \(\widehat{L}_{N}\) with initialization \(\mathbf{w}_{\mathrm{GD}}^{0}=\mathbf{0}\in\mathbb{R}^{d}\) and learning rate \(\eta>0\):

\[\mathbf{w}_{\mathrm{PGD}}^{+1}:=\mathbf{prox}_{\eta\mathcal{R}}\Big{(}\mathbf{ w}_{\mathrm{PGD}}^{\prime}-\eta\nabla\widehat{L}_{N}^{0}(\mathbf{w}_{\mathrm{PGD}}^ {\prime})\Big{)},\] (ICPGD)

where we denote \(\widehat{L}_{N}^{0}(\mathbf{w}):=\frac{1}{N}\sum_{i=1}^{N}\ell(\mathbf{w}^{ \top}\mathbf{x}_{i},y_{i})\).

To approximate (ICPGD) by transformers, in addition to the requirement on the loss \(\ell\) as in Theorem D.1, we additionally require the the proximal operator \(\mathbf{prox}_{\eta\mathcal{R}}(\cdot)\) to be approximable by an MLP layer (as a vector-valued analog of Definition D.1) defined as follows.

**Definition D.2** (Approximability by MLP).: _An operator \(P:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is \((\varepsilon,R,D,C)\)-approximable by MLP, if there exists a there exists a MLP \(\boldsymbol{\theta}_{\mathtt{nlp}}=(\mathbf{W}_{1},\mathbf{W}_{2})\in\mathbb{R }^{D\times d}\times\mathbb{R}^{d\times D}\) with hidden dimension \(D\), such that \(\sup_{\left\|\mathbf{w}\right\|_{2}\leq R}\left\|P(\mathbf{w})-\mathrm{MLP}_{ \boldsymbol{\theta}_{\mathtt{nlp}}}(\mathbf{w})\right\|_{2}\leq\varepsilon\)._

The definition above captures the proximal operator \(\mathbf{prox}_{\eta\mathcal{R}}\) for a broad class of regularizers, such as the (commonly-used) \(L_{1}\) and \(L_{2}\) regularizer listed in the following proposition, for all of which one can directly check that they can be exactly implemented by an MLP as stated below.

**Proposition D.1** (Proximal operators for commonly-used regularizers).: _For regularizer \(\mathcal{R}\) in \(\{\lambda\left\|\cdot\right\|_{1},\frac{\lambda}{2}\left\|\cdot\right\|_{2}^{2 },\mathbb{I}_{\mathbb{B}_{\infty}(B)}(\cdot)\}\), the operator \(\mathbf{prox}_{\eta\mathcal{R}}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is exactly approximable by MLP. More concretely, we have_

1. _For_ \(\mathcal{R}=\lambda\left\|\cdot\right\|_{1}\)_,_ \(\mathbf{prox}_{\eta\mathcal{R}}\) _is_ \((0,+\infty,4d,4+2\eta\lambda)\)_-approximable by MLP._
2. _For_ \(\mathcal{R}=\frac{\lambda}{2}\left\|\cdot\right\|_{2}^{2}\)_,_ \(\mathbf{prox}_{\eta\mathcal{R}}\) _is_ \((0,+\infty,2d,2+2\eta\lambda)\)_-approximable by MLP._
3. _For_ \(\mathcal{R}=\mathbb{I}_{\mathbb{B}_{\infty}(B)}(\cdot)\)_,_ \(\mathbf{prox}_{\eta\mathcal{R}}=\mathrm{Proj}_{\mathbb{B}_{\infty}(B)}\) _is_ \((0,+\infty,2d,2+2B)\)_-approximable by MLP._

**Theorem D.2** (Convex ICPGD).: _Fix any \(B_{w}>0\), \(L>1\), \(\eta>0\), and \(\varepsilon+\varepsilon^{\prime}\leq B_{w}/(2L)\). Suppose that_

1. _The loss_ \(\ell(\cdot,\cdot)\) _is convex in the first argument;_
2. \(\partial_{s}\ell\) _is_ \((\varepsilon,R,M,C)\)_-approximable by sum of relus with_ \(R=\max\left\{B_{x}B_{w},B_{y},1\right\}\)_._
3. \(\mathcal{R}\) _convex, and the proximal operator_ \(\mathbf{prox}_{\eta\mathcal{R}}(\mathbf{w})\) _is_ \((\eta\varepsilon^{\prime},R^{\prime},D^{\prime},C^{\prime})\)_-approximable by MLP with_ \(R^{\prime}=\sup_{\left\|\mathbf{w}\right\|_{2}\leq B_{w}}\left\|\mathbf{w}_{ \eta}^{+}\right\|_{2}+\eta\varepsilon\)_._

_Then there exists a transformer \(\mathrm{TF}_{\boldsymbol{\theta}}\) with \((L+1)\) layers, \(\max_{\ell\in[L]}M^{(\ell)}\leq M\) heads within the first \(L\) layers, \(M^{(L+1)}=2\), and hidden dimension \(D^{\prime}\) such that, for any input data \((\mathcal{D},\mathbf{x}_{N+1})\) such that \(\sup_{\left\|\mathbf{w}\right\|_{2}\leq B_{w}}\lambda_{\max}(\nabla^{2} \widehat{L}_{N}(\mathbf{w}))\leq 2/\eta,\qquad\exists\mathbf{w}^{\star}\in\underset{ \mathbf{w}\in\mathbb{R}^{d}}{\arg\min}\widehat{L}_{N}(\mathbf{w})\) such that \(\left\|\mathbf{w}^{\star}\right\|_{2}\leq B_{w}/2,\)_

\(\mathrm{TF}_{\boldsymbol{\theta}}(\mathbf{H}^{(0)})\) _approximately implements (ICGD):_

1. _(Parameter space) For every_ \(\ell\in[L]\)_, the_ \(\ell\)_-th layer's output_ \(\mathbf{H}^{(\ell)}=\mathrm{TF}_{\boldsymbol{\theta}^{(1,d)}}(\mathbf{H}^{(0)})\) _approximates_ \(\ell\) _steps of (ICGD): We have_ \(\mathbf{h}_{i}^{(\ell)}=[\mathbf{x}_{i};y_{i}^{\prime};\widehat{\mathbf{w}}^{ \ell};\mathbf{0}_{D-2d-3};1;t_{i}]\) _for every_ \(i\in[N+1]\)_, where_ \[\left\|\widehat{\mathbf{w}}^{\ell}-\mathbf{w}_{\mathrm{PGD}}^{\ell}\right\|_{2} \leq(\varepsilon+\varepsilon^{\prime})\cdot(L\eta B_{x}).\]
2. _(Prediction space) The final output_ \(\mathbf{H}^{(L+1)}=\mathrm{TF}_{\boldsymbol{\theta}}(\mathbf{H}^{(0)})\) _approximates the prediction of_ \(L\) _steps of (ICGD): We have_ \(\mathbf{h}_{N+1}^{(L+1)}=[\mathbf{x}_{N+1};\widehat{y}_{N+1};\widehat{\mathbf{w}} ^{L};\mathbf{0}_{D-2d-3};1;t_{i}]\)_, where_ \(\widehat{y}_{N+1}=\left\langle\widehat{\mathbf{w}}^{L},\mathbf{x}_{N+1}\right\rangle\) _so that_ \[\left|\widehat{y}_{N+1}-\left\langle\mathbf{w}_{\mathrm{PGD}}^{L},\mathbf{x}_{N+1 }\right\rangle\right|\leq(\varepsilon+\varepsilon^{\prime})\cdot(2L\eta B_{x}^{2}).\]_Further, the weight matrices have norm bounds \(\left\|\bm{\theta}\right\|\leq 3+R+2\eta C+C^{\prime}\)._

The proof of Theorem D.2 is essentially similar to the proof of Theorem D.1, using the following generalized version of Lemma D.1.

**Lemma D.2** (Composition of error for approximating convex PGD).: _Suppose \(f:\mathbb{R}^{d}\to\mathbb{R}\) is a convex function and \(\mathcal{R}\) is a convex regularizer. Let \(\mathbf{w}^{\star}\in\operatorname*{arg\,min}_{\mathbf{w}\in\mathbb{R}^{d}}f (\mathbf{w})+\mathcal{R}(\mathbf{w})\), \(R\geq 2\|\mathbf{w}^{\star}\|_{2}\), and assume that \(\nabla f\) is \(L_{f}\)-smooth on \(\mathsf{B}^{2}_{d}(R)\). Let sequences \(\{\widehat{\mathbf{w}}^{\ell}\}_{\ell\geq 0}\subset\mathbb{R}^{d}\) and \(\{\mathbf{w}^{\ell}_{\mathrm{GD}}\}_{\ell\geq 0}\subset\mathbb{R}^{d}\) be given by \(\widehat{\mathbf{w}}^{0}=\mathbf{w}^{0}_{\mathrm{GD}}=\mathbf{0}\),_

\[\begin{cases}\widehat{\mathbf{w}}^{\ell+1}=\mathbf{prox}_{\eta \mathcal{R}}\big{(}\widehat{\mathbf{w}}^{\ell}-\eta\nabla f(\widehat{\mathbf{ w}}^{\ell})\big{)}+\varepsilon^{\ell},\qquad\|\varepsilon^{\ell}\|_{2}\leq \varepsilon,\\ \mathbf{w}^{\ell+1}_{\mathrm{GD}}=\mathbf{prox}_{\eta\mathcal{R}}\big{(} \mathbf{w}^{\ell}_{\mathrm{GD}}-\eta\nabla f(\mathbf{w}^{\ell}_{\mathrm{GD}}) \big{)},\end{cases}\]

_for all \(\ell\geq 0\). Then as long as \(\eta\leq 2/L_{f}\), for any \(0\leq L\leq R/(2\varepsilon)\), it holds that \(\big{\|}\widehat{\mathbf{w}}^{L}-\mathbf{w}^{L}_{\mathrm{GD}}\big{\|}_{2}\leq L\varepsilon\) and \(\|\widehat{\mathbf{w}}^{L}\|_{2}\leq\frac{R}{2}+L\varepsilon\leq R\)._

The proof of the above lemma is done by utilizing the non-expansiveness of the PGD operator \(\mathbf{w}\mapsto\mathbf{prox}_{\eta\mathcal{R}}(\mathbf{w}-\eta\nabla f( \mathbf{w}))\) and otherwise following the same arguments as for Lemma D.1.

### Gradient descent on two-layer neural networks

We now move beyond the convex setting by showing that transformers can implement gradient descent on two-layer neural networks in context.

Suppose that the prediction function \(\mathrm{pred}(\mathbf{x};\mathbf{w}):=\sum_{k=1}^{K}u_{k}r(\mathbf{v}_{k}^{ \top}\mathbf{x})\) is given by a two-layer neural network, parameterized by \(\mathbf{w}=[\mathbf{v}_{k};u_{k}]_{k\in[K]}\in\mathbb{R}^{K(d+1)}\). Consider the empirical risk minimization problem:

\[\min_{\mathbf{w}\in\mathcal{W}}\widehat{L}_{N}(\mathbf{w}):=\frac{1}{2N}\sum_ {i=1}^{N}\ell(\mathrm{pred}(\mathbf{x}_{i};\mathbf{w}),y_{i})=\frac{1}{2N} \sum_{i=1}^{N}\ell\bigg{(}\sum_{k=1}^{K}u_{k}r(\mathbf{v}_{k}^{\top}\mathbf{x }_{i}),y_{i}\bigg{)},\] (13)

where \(\mathcal{W}\) is a bounded domain. For the sake of simplicity, in the following discussion we assume that \(\mathrm{Proj}_{\mathcal{W}}\) can be _exactly_ implemented by a MLP layer (e.g. \(\mathcal{W}=\mathsf{B}_{\infty}(R_{w})\) for some \(R_{w}>0\)).

**Theorem D.3** (Approximate ICGD on two-layer NNs).: _Fix any \(B_{v},B_{u}>0\), \(L\geq 1\), \(\eta>0\), and \(\varepsilon>0\). Suppose that_

1. _Both the activation function_ \(r\) _and the loss function_ \(\ell\) _is_ \(C^{4}\)_-smooth;_
2. \(\mathcal{W}\) _is a closed domain such that_ \(\mathcal{W}\subset\{\mathbf{w}=[\mathbf{v}_{k};u_{k}]_{k\in[K]}\in\mathbb{R}^{K (d+1)}:\left\|\mathbf{v}_{k}\right\|_{2}\leq B_{v},\left|u_{k}\right|\leq B_{u}\}\)_, and_ \(\mathrm{Proj}_{\mathcal{W}}=\mathrm{MLP}_{\bm{\theta}_{\mathtt{nlp}}}\) _for some MLP layer_ \(\bm{\theta}_{\mathtt{nlp}}\) _with hidden dimension_ \(D_{w}\) _and_ \(\big{\|}\bm{\theta}_{\mathtt{nlp}}\big{\|}\leq C_{w}\)_;_

_Then there exists a \((2L)\)-layer transformer \(\mathrm{TF}_{\bm{\theta}}\) with_

\[\max_{\ell\in[2L]}M^{(\ell)}\leq\widetilde{\mathcal{O}}\left(\varepsilon^{-2} \right),\qquad\max_{\ell\in[2L]}D^{(\ell)}\leq\widetilde{\mathcal{O}}\left( \varepsilon^{-2}\right)+D_{w},\qquad\left\|\bm{\theta}\right\|\leq\mathcal{O} \left(1+\eta\right)+C_{w},\]

_where \(\mathcal{O}\left(\cdot\right)\) hides the constants that depend on \(K\), the radius parameters \(B_{x},B_{y},B_{u},B_{v}\) and the smoothness of \(r\) and \(\ell\), such that for any input data \((\mathcal{D},\mathbf{x}_{N+1})\) such that input sequence \(\mathbf{H}^{(0)}\in\mathbb{R}^{D\times(N+1)}\) takes form (3), \(\mathrm{TF}_{\bm{\theta}}(\mathbf{H}^{(0)})\) approximately implements in-context gradient descent on risk (13): For every \(\ell\in[L]\), the \(2\ell\)-th layer's output \(\mathbf{h}^{(2\ell)}_{i}=[\mathbf{x}_{i};y_{i}^{\prime};\widehat{\mathbf{w}}^{ \ell};\mathbf{0};1;t_{i}]\) for every \(i\in[N+1]\), and_

\[\widehat{\mathbf{w}}^{\ell}=\mathrm{Proj}_{\mathcal{W}}\left(\widehat{\mathbf{ w}}^{\ell-1}-\eta(\nabla\widehat{L}_{N}(\widehat{\mathbf{w}}^{\ell-1})+ \varepsilon^{\ell-1})\right),\qquad\widehat{\mathbf{w}}^{0}=\mathbf{0},\] (14)

_where \(\big{\|}\bm{\varepsilon}^{\ell-1}\big{\|}_{2}\leq\varepsilon\) is an error term._

As a direct corollary, the transformer constructed above can approximate the true gradient descent trajectory \(\left\{\mathbf{w}^{\ell}_{\mathrm{GD}}\right\}_{\ell\geq 0}\) on (16), defined as \(\mathbf{w}^{0}_{\mathrm{GD}}=\mathbf{0}\) and \(\mathbf{w}^{\ell+1}_{\mathrm{GD}}=\mathbf{w}^{\ell}_{\mathrm{GD}}-\eta\nabla \widehat{L}_{N}(\mathbf{w}^{\ell}_{\mathrm{GD}})\) for all \(\ell\geq 0\).

**Corollary D.1** (Approximating multi-step ICGD on two-layer NNs).: _For any \(L\geq 1\), under the same setting as Theorem D.3, the \((2L)\)-layer transformer \(\mathrm{TF}_{\bm{\theta}}\) there approximates the true gradient descent trajectory \(\{\mathbf{w}_{\mathrm{GD}}^{\ell}\}_{\ell\geq 0}\): For the intermediate iterates \(\{\widehat{\mathbf{w}}^{\ell}\}_{\ell\in[L]}\) considered therein, we have_

\[\left\|\widehat{\mathbf{w}}^{\ell}-\mathbf{w}_{\mathrm{GD}}^{\ell}\right\|_{2} \leq L_{f}^{-1}(1+\eta L_{f})^{\ell}\varepsilon,\]

_where \(L_{f}=\sup_{\mathbf{w}\in\mathcal{W}}\left\|\nabla^{2}\widehat{L}_{N}(\mathbf{ w})\right\|_{\mathrm{op}}\) denotes the smoothness of \(\widehat{L}_{N}\) within \(\mathcal{W}\)._

Remark on error accumulationNote that in Corollary D.1, the error accumulates _exponentially_ in \(\ell\) rather than linearly as in Theorem D.1. This is as expected, since gradient descent on non-convex objectives is inherently unstable at a high level (a slight error added upon each step may result in a drastically different trajectories); technically, this happens as the stability-like property Lemma D.1 no longer holds for the non-convex case.

Corollary D.1 is a simple implication of Theorem D.3 and Part (b) of the following convergence and trajectory closeness result for inexact gradient descent. For any closed convex set \(\mathcal{W}\subset\mathbb{R}^{q}\), any function \(f:\mathcal{W}\to\mathbb{R}\), and any initial point \(\mathbf{w}\in\mathcal{W}\), let

\[\mathsf{G}_{\mathcal{W},\eta}^{f}(\mathbf{w}):=\frac{\mathbf{w}-\mathrm{Proj} _{\mathcal{W}}(\mathbf{w}-\eta\nabla f(\mathbf{w}))}{\eta}\]

denote the gradient mapping at \(\mathbf{w}\) with step size \(\eta\), a standard measure of stationarity in constrained optimization [63]. Note that \(\mathsf{G}_{\mathcal{W},\eta}^{f}(\mathbf{w})=\nabla f(\mathbf{w})\) when \(\mathbf{w}-\eta\nabla f(\mathbf{w})\in\mathcal{W}\) (so that the projection does not take effect).

**Lemma D.3** (Convergence and trajectory closeness of inexact GD).: _Suppose \(f:\mathcal{W}\to\mathbb{R}\), where \(\mathcal{W}\subset\mathbb{R}^{d}\) is a convex closed domain and \(\nabla f\) is \(L_{f}\)-Lipschitz on \(\mathcal{W}\). Let sequence \(\{\widehat{\mathbf{w}}^{\ell}\}_{\ell\geq 0}\subset\mathbb{R}^{d}\) be given by \(\widehat{\mathbf{w}}^{0}=\mathbf{w}^{0}\),_

\[\widehat{\mathbf{w}}^{\ell+1}=\mathrm{Proj}_{\mathcal{W}}\big{(}\widehat{ \mathbf{w}}^{\ell}-\eta(\nabla f(\widehat{\mathbf{w}}^{\ell})+\bm{\varepsilon }^{\ell})\big{)},\qquad\|\bm{\varepsilon}^{\ell}\|_{2}\leq\varepsilon,\]

_for all \(\ell\geq 0\). Then the following holds._

* _As long as_ \(\eta\leq 1/L_{f}\)_, for all_ \(L\geq 1\)_,_ \[\min_{\ell\in[L-1]}\left\|\mathsf{G}_{\mathcal{W},\eta}^{f}(\widehat{\mathbf{ w}}^{\ell})\right\|_{2}^{2}\leq\frac{1}{L}\sum_{\ell=0}^{L-1}\left\|\mathsf{G}_{ \mathcal{W},\eta}^{f}(\widehat{\mathbf{w}}^{\ell})\right\|_{2}^{2}\leq\frac{ 8(f(\mathbf{w}^{0})-\inf_{\mathbf{w}\in\mathcal{W}}f(\mathbf{w}))}{\eta L}+10 \varepsilon^{2}.\]
* _Let the sequences_ \(\{\mathbf{w}_{\mathrm{GD}}^{\ell}\}_{\ell\geq 0}\subset\mathbb{R}^{d}\) _and be given by_ \(\mathbf{w}_{\mathrm{GD}}^{0}=\mathbf{w}^{0}\) _and_ \(\mathbf{w}_{\mathrm{GD}}^{\ell+1}=\mathrm{Proj}_{\mathcal{W}}(\mathbf{w}_{ \mathrm{GD}}^{\ell}-\eta\nabla f(\mathbf{w}_{\mathrm{GD}}^{\ell}))\)_. Then it holds that_ \[\left\|\widehat{\mathbf{w}}^{\ell}-\mathbf{w}_{\mathrm{GD}}^{\ell}\right\|_{2} \leq L_{f}^{-1}(1+\eta L_{f})^{\ell}\varepsilon,\qquad\forall\ell\geq 0.\]

## Appendix E Proofs for Section D

### Approximating a single GD step

**Proposition E.1** (Approximating a single GD step by a single attention layer).: _Let \(\ell(\cdot,\cdot):\mathbb{R}^{2}\to\mathbb{R}\) be a loss function such that \(\partial_{1}\ell\) is \((\varepsilon,R,M,C)\)-approximable by sum of relus with \(R=\max\{B_{x}B_{w},B_{y},1\}\). Let \(\widehat{L}_{N}(\mathbf{w}):=\frac{1}{N}\sum_{i=1}^{N}\ell(\mathbf{w}^{\top} \mathbf{x}_{i},y_{i})\) denote the empirical risk with loss function \(\ell\) on dataset \(\{(\mathbf{x}_{i},y_{i})\}_{i\in[N]}\)._

_Then, for any \(\varepsilon>0\), there exists an attention layer \(\bm{\theta}=\left\{(\mathbf{Q}_{m},\mathbf{K}_{m},\mathbf{V}_{m})\right\}_{m \in[M]}\) with \(M\) heads such that, for any input sequence that takes form \(\mathbf{h}_{i}=[\mathbf{x}_{i};y_{i}^{\prime};\mathbf{w};\mathbf{0}_{D-2d-3}; 1;t_{i}]\) with \(\left\|\mathbf{w}\right\|_{2}\leq B_{w}\), it gives output \(\widehat{\mathbf{h}}_{i}=\left[\mathrm{Attn}_{\bm{\theta}}(\mathbf{H})\right]_ {i}=[\mathbf{x}_{i};y_{i}^{\prime};\widehat{\mathbf{w}};\mathbf{0}_{D-2d-3}; 1;t_{i}]\) for all \(i\in[N+1]\), where_

\[\left\|\widehat{\mathbf{w}}-(\mathbf{w}-\eta\nabla\widehat{L}_{N}(\mathbf{w}) )\right\|_{2}\leq\varepsilon\cdot(\eta B_{x}).\]

_Further, \(\left\|\bm{\theta}\right\|\leq 2+R+2\eta C\)._Proof of Proposition e.1.: As \(\partial_{s}\ell\) is \((\varepsilon,R,M,C)\)-approximable by sum of relus, there exists a function \(f:[-R,R]^{2}\rightarrow\mathbb{R}\) of form

\[f(s,t)=\sum_{m=1}^{M}c_{m}\sigma(a_{m}s+b_{m}t+d_{m})\ \ \text{with}\ \ \sum_{m=1}^{M}|c_{m}|\leq C,\ |a_{m}|+|b_{m}|+|d_{m}|\leq 1, \ \forall m\in[M],\]

such that \(\sup_{(s,t)\in[-R,R]^{2}}|f(s,t)-\partial_{s}\ell(s,t)|\leq\varepsilon\).

Next, for every \(m\in[M]\), we define matrices \(\mathbf{Q}_{m},\mathbf{K}_{m},\mathbf{V}_{m}\in\mathbb{R}^{D\times D}\) such that

\[\mathbf{Q}_{m}\mathbf{h}_{i}=\begin{bmatrix}a_{m}\mathbf{w}\\ b_{m}\\ d_{m}\\ -2\\ \mathbf{0}\end{bmatrix},\quad\mathbf{K}_{m}\mathbf{h}_{j}=\begin{bmatrix} \mathbf{x}_{j}\\ y_{j}\\ 1\\ R(1-t_{j})\\ \mathbf{0}\end{bmatrix},\quad\mathbf{V}_{m}\mathbf{h}_{j}=-\frac{(N+1)\eta c_{ m}}{N}\cdot\begin{bmatrix}\mathbf{0}_{d}\\ 0\\ \mathbf{x}_{j}\\ \mathbf{0}_{D-2d-1}\end{bmatrix}\]

for all \(i,j\in[N+1]\). As the input has structure \(\mathbf{h}_{i}=[\mathbf{x}_{i};y_{i}^{\prime};\mathbf{w};\mathbf{0}_{D-2d-3};1 ;t_{i}]\), these matrices indeed exist, and further it is straightforward to check that they have norm bounds

\[\max_{m\in[M]}\left\|\mathbf{Q}_{m}\right\|_{\text{op}}\leq 3,\quad\max_{m\in[M]} \left\|\mathbf{K}_{m}\right\|_{\text{op}}\leq 2+R,\quad\sum_{m\in[M]}\left\| \mathbf{V}_{m}\right\|_{\text{op}}\leq 2\eta C.\]

Consequently, \(\left\|\boldsymbol{\theta}\right\|\leq 2+R+2\eta C\).

Now, for every \(i,j\in[N+1]\), we have

\[\sigma(\langle\mathbf{Q}_{m}\mathbf{h}_{i},\mathbf{K}_{m}\mathbf{ h}_{j}\rangle) =\sigma\big{(}a_{m}\mathbf{w}^{\top}\mathbf{x}_{j}+b_{m}(1-t_{j})y_{j}+d_{m}-2 Rt_{j}\big{)}\] \[=\sigma\big{(}a_{m}\mathbf{w}^{\top}\mathbf{x}_{j}+b_{m}y_{j}+d_{ m}\big{)}1\{t_{j}=1\},\]

where the last equality follows from the bound

\[\big{|}a_{m}\mathbf{w}^{\top}\mathbf{x}_{j}+b_{m}(1-t_{j})y_{j}+d_{m}\big{|} \leq|a_{m}|B_{x}B_{w}+R\leq 2R,\] (15)

so that the above relu equals \(0\) if \(t_{j}\leq 0\). Therefore,

\[\sum_{m=1}^{M}\sigma(\langle\mathbf{Q}_{m}\mathbf{h}_{i},\mathbf{ K}_{m}\mathbf{h}_{j}\rangle)\mathbf{V}_{m}\mathbf{h}_{j}\] \[=\left(\sum_{m=1}^{M}c_{m}\sigma\big{(}a_{m}\mathbf{w}^{\top} \mathbf{x}_{j}+b_{m}y_{j}+d_{m}\big{)}\right)\cdot\frac{-(N+1)\eta}{N}1\{t_{j }=0\}[\mathbf{0}_{d+1};\mathbf{x}_{j};\mathbf{0}_{2}]\] \[=f(\mathbf{w}^{\top}\mathbf{x}_{j},y_{j})\cdot\frac{-(N+1)\eta}{ N}1\{t_{j}=0\}[\mathbf{0}_{d+1};\mathbf{x}_{j};\mathbf{0}_{D-2d-1}].\]

Thus letting the attention layer \(\boldsymbol{\theta}=\left\{(\mathbf{V}_{m},\mathbf{Q}_{m},\mathbf{K}_{m}) \right\}_{m\in[M]}\), we have

\[\widetilde{\mathbf{h}}_{i}=\left[\operatorname{Attn}_{\boldsymbol {\theta}}(\mathbf{H})\right]_{i}=\mathbf{h}_{i}+\frac{1}{N+1}\sum_{j=1}^{N+1} \sum_{m=1}^{M}\sigma(\langle\mathbf{Q}_{m}\mathbf{h}_{i},\mathbf{K}_{m}\mathbf{ h}_{j}\rangle)\mathbf{V}_{m}\mathbf{h}_{j}\] \[=\mathbf{h}_{i}-\frac{\eta}{N}\sum_{j=1}^{N}f(\mathbf{w}^{\top} \mathbf{x}_{j},y_{j})[\mathbf{0}_{d+1};\mathbf{x}_{j};\mathbf{0}_{2}]\] \[=[\mathbf{x}_{i};y_{i};\mathbf{w};1;t_{i}]\underbrace{-\frac{\eta} {N}\sum_{j=1}^{N}\partial_{s}\ell(\mathbf{w}^{\top}\mathbf{x}_{j},y_{j})[ \mathbf{0}_{d+1};\mathbf{x}_{j};\mathbf{0}_{D-2d-1}]}_{[\mathbf{0}_{d+1};- \eta\nabla\widetilde{L}_{N}(\mathbf{w});\mathbf{0}_{D-2d-1}]}+[\mathbf{0}_{d+ 1};\boldsymbol{\varepsilon};\mathbf{0}_{D-2d-1}]\] \[=[\mathbf{x}_{i};y_{i};\mathbf{w}_{\eta}^{+}+\boldsymbol{ \varepsilon};\mathbf{0}_{D-2d-3};1;t_{i}],\]

where the error vector \(\boldsymbol{\varepsilon}\in\mathbb{R}^{d}\) satisfies

\[\left\|\boldsymbol{\varepsilon}\right\|_{2}=\left\|-\frac{\eta}{N}\sum_{j=1}^{ N}\big{(}f(\mathbf{w}^{\top}\mathbf{x}_{j},y_{j})-\partial_{s}\ell(\mathbf{w}^{\top} \mathbf{x}_{j},y_{j})\big{)}\mathbf{x}_{j}\right\|_{2}\]\[\leq\frac{\eta}{N}\cdot N\cdot\varepsilon\cdot B_{x}=\varepsilon\cdot( \eta B_{x}).\]

This is the desired result. 

### Proof of Theorem d.1

We first prove part (a), which requires constructing the first \(L\) layers of \(\bm{\theta}\). Note that by our precondition \(L\leq B_{w}/(2\varepsilon)\).

By our precondition, the partial derivative of the loss \(\partial_{s}\ell\) is \((\varepsilon,R,M,C)\)-approximable by sum of relus. Therefore we can apply Proposition E.1 to obtain that, there exists a single attention layer \(\bm{\theta}^{(1)}=\left\{(\mathbf{Q}_{m},\mathbf{K}_{m},\mathbf{V}_{m})\right\} _{m\in[M]}\) with \(M\) heads (and norm bounds specified in Proposition E.1), such that for any \(\mathbf{w}\) with \(\left\|\mathbf{w}\right\|_{2}\leq B_{w}\), the attention layer \(\mathrm{Attn}_{\bm{\theta}^{(1)}}\) maps the input \(\mathbf{h}_{i}=[\mathbf{x}_{i};y^{\prime}_{i};\mathbf{w};\mathbf{0}_{D-2d-3}; 1;t_{i}]\) to output \(\mathbf{h}^{\prime}_{i}=[\mathbf{x}_{i};y^{\prime}_{i};\widehat{\mathbf{w}}; \mathbf{0}_{D-2d-3};1;t_{i}]\) for all \(i\in[N+1]\), where

\[\left\|\widehat{\mathbf{w}}-\left(\mathbf{w}-\eta\nabla\widehat{L}_{N}( \mathbf{w})\right)\right\|_{2}\leq\varepsilon\cdot(\eta B_{x})=:\varepsilon^{ \prime}.\]

Consider the \(L\)-layer transformer \(\bm{\theta}^{1:L}=(\bm{\theta}^{(1)},\dots,\bm{\theta}^{(1)})\) which stacks the same attention layer \(\bm{\theta}^{(1)}\) for \(L\) times, and for the given input \(\mathbf{h}^{(0)}_{i}=[\mathbf{x}_{i};y^{\prime}_{i};\mathbf{w}^{0};\mathbf{0} _{D-2d-3};1;t_{i}]\), its \(\ell\)-th layer's output \(\mathbf{h}^{(\ell)}_{i}=[\mathbf{x}_{i};y^{\prime}_{i};\widehat{\mathbf{w}}^{ \ell};\mathbf{0}_{D-2d-3};1;t_{i}]\).

We now inductively show that \(\left\|\widehat{\mathbf{w}}^{\ell}\right\|_{2}\leq B_{w}\) and \(\left\|\widehat{\mathbf{w}}^{\ell}-\mathbf{w}^{\ell}_{\mathrm{GD}}\right\|_{2 }\leq\ell\varepsilon\) for all \(\ell\in[L]\). The base case of \(\ell=0\) is trivial. Suppose the claim holds for \(\ell\). Then for \(\ell+1\leq L\leq B_{w}/(2\varepsilon)\), the sequence \(\left\{\widehat{\mathbf{w}}^{i}\right\}_{i\leq\ell+1}\) and \(\left\{\mathbf{w}^{i}_{\mathrm{GD}}\right\}_{i\leq\ell+1}\) satisfies the precondition of the error composition lemma (Lemma D.1) with error bound \(\varepsilon\), from which we obtain \(\left\|\widehat{\mathbf{w}}^{\ell+1}\right\|_{2}\leq B_{w}\) and

\[\left\|\widehat{\mathbf{w}}^{\ell+1}-\mathbf{w}^{\ell+1}_{\mathrm{GD}}\right\| _{2}\leq(\ell+1)\varepsilon^{\prime}.\]

This finishes the induction, and gives the following approximation guarantee for all \(\ell\in[L]\):

\[\left\|\widehat{\mathbf{w}}^{\ell}-\mathbf{w}^{\ell}_{\mathrm{GD}}\right\|_{2 }\leq\ell\varepsilon^{\prime}\leq\varepsilon\cdot(L\eta B_{x}),\]

which proves part (a).

We now prove part (b), which requires constructing the last attention layer \(\bm{\theta}^{(L+1)}\). Recall \(\mathbf{h}^{(L)}_{i}=[\mathbf{x}_{i};y^{\prime}_{i};\widehat{\mathbf{w}}^{L}; \mathbf{0}_{D-2d-3};1;t_{i}]\) for all \(i\in[N+1]\). We construct a 2-head attention layer \(\bm{\theta}^{(L+1)}=\{(\mathbf{Q}^{(L+1)}_{m},\mathbf{K}^{(L+1)}_{m},\mathbf{V }^{(L+1)}_{m})\}_{m=1,2}\) such that for every \(i,j\in[N+1]\),

\[\mathbf{Q}^{(L+1)}_{1}\mathbf{h}^{(L)}_{i}=[\mathbf{x}_{i}; \mathbf{0}_{D-d}],\ \ \mathbf{K}^{(L+1)}_{1}\mathbf{h}^{(L)}_{j}=[\widehat{\mathbf{w}}^{L}; \mathbf{0}_{D-d}],\ \ \mathbf{V}^{(L+1)}_{1}\mathbf{h}^{(L)}_{j}=[\mathbf{0}_{d};1; \mathbf{0}_{D-d-1}],\] \[\mathbf{Q}^{(L+1)}_{2}\mathbf{h}^{(L)}_{i}=[\mathbf{x}_{i}; \mathbf{0}_{D-d}],\ \ \mathbf{K}^{(L+1)}_{2}\mathbf{h}^{(L)}_{j}=[-\widehat{\mathbf{w}}^{L}; \mathbf{0}_{D-d}],\ \ \mathbf{V}^{(L+1)}_{2}\mathbf{h}^{(L)}_{j}=[\mathbf{0}_{d};-1; \mathbf{0}_{D-d-1}].\]

Note that the weight matrices have norm bound

\[\max_{i=1,2}\left\|\mathbf{Q}^{(L+1)}_{i}\right\|_{\mathrm{op}}\leq 1,\quad \max_{i=1,2}\left\|\mathbf{K}^{(L+1)}_{i}\right\|_{\mathrm{op}}\leq 1,\quad\sum_{i=1}^{2} \left\|\mathbf{V}^{(L+1)}_{i}\right\|_{\mathrm{op}}\leq 2.\]

Then we have

\[\mathbf{h}^{(L+1)}_{N+1}=\mathbf{h}^{(L)}_{N+1}+\frac{1}{N+1}\sum_ {j=1}^{N+1}\sum_{m=1}^{2}\sigma\Big{(}\Big{\langle}\mathbf{Q}^{(L+1)}\mathbf{h}^ {(L)}_{N+1},\mathbf{K}^{(L+1)}\mathbf{h}^{(L)}_{j}\Big{\rangle}\Big{)}\mathbf{ V}^{(L+1)}\mathbf{h}^{(L)}_{j}\] \[=[\mathbf{x}_{i};0;\widehat{\mathbf{w}}^{L};\mathbf{0}_{D-2d-3}; 1;1]+\big{(}\sigma(\big{\langle}\widehat{\mathbf{w}}^{L},\mathbf{x}_{N+1} \big{\rangle})-\sigma(-\big{\langle}\widehat{\mathbf{w}}^{L},\mathbf{x}_{N+1} \big{\rangle})\big{)}\cdot[\mathbf{0}_{d};1;\mathbf{0}_{D-d-1}]\] \[\stackrel{{(i)}}{{=}}[\mathbf{x}_{i};0;\widehat{ \mathbf{w}}^{L};\mathbf{0}_{D-2d-3};1;1]+[\mathbf{0}_{d};\big{\langle} \widehat{\mathbf{w}}^{L},\mathbf{x}_{N+1}\big{\rangle};\mathbf{0}_{D-d-1}]\] \[=[\mathbf{x}_{i};\underbrace{\langle\widehat{\mathbf{w}}^{L}, \mathbf{x}_{N+1}\rangle}_{\widehat{\mathbf{w}}^{N+1}};\widehat{\mathbf{w}}^{L}; \mathbf{0}_{D-2d-3};1;1],\]Above, (i) uses the identity \(t=\sigma(t)-\sigma(-t)\). Further by part (a) we have

\[\big{|}\widehat{y}_{N+1}-\big{\langle}\mathbf{w}_{\mathrm{GD}}^{L},\mathbf{x}_{N +1}\big{\rangle}\big{|}=\big{|}\big{\langle}\widehat{\mathbf{w}}^{L}-\mathbf{ w}_{\mathrm{GD}}^{L},\mathbf{x}_{N+1}\big{\rangle}\big{|}\leq\varepsilon\cdot(L \eta B_{x}^{2}).\]

This proves part (b), and also finishes the proof Theorem D.1 where the overall \((L+1)\)-layer attention-only transformer is given by \(\mathrm{TF}_{\bm{\theta}}^{0}\) with

\[\bm{\theta}=(\underbrace{\bm{\theta}^{(1)},\ldots,\bm{\theta}^{(1)}}_{L\ \mathrm{times}},\bm{\theta}^{(L+1)}).\]

### Proof of Lemma D.1

As \(f\) is a convex, \(L_{f}\) smooth function on \(\mathsf{B}_{2}^{d}(R)\), the mapping \(\mathcal{T}_{\eta}:\mathbf{w}\mapsto\mathbf{w}-\eta\nabla f(\mathbf{w})\) is non-expansive in \(\|\cdot\|_{2}\): Indeed, for any \(\mathbf{w},\mathbf{w}^{\prime}\in\mathsf{B}_{2}^{d}(R)\) we have

\[\left\|\mathcal{T}_{\eta}(\mathbf{w})-\mathcal{T}_{\eta}(\mathbf{ w}^{\prime})\right\|_{2} =\left\|\mathbf{w}-\eta\nabla f(\mathbf{w})-(\mathbf{w}^{\prime}- \eta\nabla f(\mathbf{w}^{\prime}))\right\|_{2}^{2}\] \[=\left\|\mathbf{w}-\mathbf{w}^{\prime}\right\|_{2}^{2}-2\eta \left\langle\mathbf{w}-\mathbf{w}^{\prime},\nabla f(\mathbf{w})-\nabla f( \mathbf{w}^{\prime})\right\rangle+\eta^{2}\left\|\nabla f(\mathbf{w})-\nabla f (\mathbf{w}^{\prime})\right\|_{2}^{2}\] \[\stackrel{{(i)}}{{\leq}}\left\|\mathbf{w}-\mathbf{w} ^{\prime}\right\|_{2}^{2}-\left(2\eta/L_{f}-\eta^{2}\right)\left\|\nabla f( \mathbf{w})-\nabla f(\mathbf{w}^{\prime})\right\|_{2}^{2}\stackrel{{ (ii)}}{{\leq}}\left\|\mathbf{w}-\mathbf{w}^{\prime}\right\|_{2}^{2}.\]

Above, (i) uses the property \(\langle\mathbf{w}-\mathbf{w}^{\prime},\nabla f(\mathbf{w})-\nabla f(\mathbf{ w}^{\prime})\rangle\geq\frac{1}{L_{f}}\|\nabla f(\mathbf{w})-\nabla f( \mathbf{w}^{\prime})\|_{2}^{2}\) for smooth convex functions [63, Theorem 2.1.5]; (ii) uses the precondition that \(\eta\leq 2/L_{f}\).

The lemma then follows directly by induction on \(L\). The base case of \(L=0\) follows directly by assumption that \(\widehat{\mathbf{w}}^{0}=\mathbf{w}_{\mathrm{GD}}^{0}\in\mathsf{B}_{2}^{d}(R/2)\). Suppose the claim holds for iterate \(L\). For iterate \(L+1\leq R/(2\varepsilon)\), we have

\[\left\|\widehat{\mathbf{w}}^{L+1}-\mathbf{w}_{\mathrm{GD}}^{L+1} \right\|_{2}=\left\|\mathcal{T}_{\eta}(\widehat{\mathbf{w}}^{L})+\varepsilon^ {L}-\mathcal{T}_{\eta}(\mathbf{w}_{\mathrm{GD}}^{L})\right\|_{2}\] \[\leq\left\|\mathcal{T}_{\eta}(\widehat{\mathbf{w}}^{L})-\mathcal{ T}_{\eta}(\mathbf{w}_{\mathrm{GD}}^{L})\right\|_{2}+\left\|\varepsilon^{L} \right\|_{2}\] \[\stackrel{{(i)}}{{\leq}}\left\|\widehat{\mathbf{w}}^ {L}-\mathbf{w}_{\mathrm{GD}}^{L}\right\|_{2}+\varepsilon\stackrel{{ (ii)}}{{\leq}}(L+1)\varepsilon.\]

Above, (i) uses the non-expansiveness, and (ii) uses the inductive hypothesis. Similarly, by our assumption \(\mathbf{w}^{\star}=\mathcal{T}_{\eta}(\mathbf{w}^{\star})\),

\[\left\|\widehat{\mathbf{w}}^{L+1}-\mathbf{w}^{\star}\right\|_{2}=\left\| \mathcal{T}_{\eta}(\widehat{\mathbf{w}}^{L})+\varepsilon^{L}-\mathcal{T}_{\eta }(\mathbf{w}^{\star})\right\|_{2}\leq\left\|\widehat{\mathbf{w}}^{L}- \mathbf{w}^{\star}\right\|_{2}+\left\|\varepsilon^{L}\right\|_{2}\leq\frac{R} {2}+(L+1)\varepsilon\leq R.\]

This finishes the induction. 

### Convex ICGD with \(\ell_{2}\) regularization

In the same setting as Theorem D.1, consider the ICGD dynamics over an \(\ell_{2}\)-regularized empirical risk:

\[\mathbf{w}_{\mathrm{GD}}^{t+1}:=\mathbf{w}_{\mathrm{GD}}^{t}-\eta\nabla \widehat{L}_{N}^{\lambda}(\mathbf{w}_{\mathrm{GD}}^{t})\] (ICGD-

\[\ell_{2}\]

)

with initialization \(\mathbf{w}_{\mathrm{GD}}^{0}\in\mathbb{R}^{d}\) and learning rate \(\eta>0\), where \(\widehat{L}_{N}^{\lambda}(\mathbf{w}):=\widehat{L}_{N}(\mathbf{w})+\frac{ \lambda}{2}\left\|\mathbf{w}\right\|_{2}^{2}\) denotes the \(\ell_{2}\)-regularized empirical risk.

**Corollary E.1** (Convex ICGD with \(\ell_{2}\) regularization).: _Fix any \(B_{w}>0\), \(L>1\), \(\eta>0\), and \(\varepsilon<B_{x}B_{w}\). Suppose the loss \(\ell(\cdot,\cdot)\) is convex in the first argument, and \(\partial_{s}\ell\) is \((\varepsilon,R,M,C)\)-approximable by sum of relus with \(R=\max\left\{B_{x}B_{w},B_{y},1\right\}\)._

_Then, there exists an attention-only transformer \(\mathrm{TF}_{\bm{\theta}}^{0}\) with \((L+1)\) layers, \(\max_{\ell\in[L]}M^{(\ell)}\leq M+1\) heads within the first \(L\) layers, and \(M^{(L+1)}=2\) such that for any input data \((\mathcal{D},\mathbf{x}_{N+1})\) with_

\[\sup_{\|\mathbf{w}\|_{2}\leq B_{w}}\lambda_{\max}(\nabla^{2}\widehat{L}_{N}^{ \lambda}(\mathbf{w}))\leq 2\eta^{-1},\qquad\exists\mathbf{w}^{\star}\in\operatorname*{arg\, min}_{\mathbf{w}\in\mathbb{R}^{d}}\widehat{L}_{N}^{\lambda}(\mathbf{w})\text{ such that }\|\mathbf{w}^{\star}\|_{2}\leq B_{w}/2,\]

\(\mathrm{TF}_{\bm{\theta}}^{0}(\mathbf{H}^{(0)})\) _approximately implements (ICGD-_\(\ell_{2}\)):_1. _(Parameter space) For every_ \(\ell\in[L]\)_, the_ \(\ell\)_-th layer's output_ \(\mathbf{H}^{(\ell)}=\operatorname{TF}_{\boldsymbol{\theta}^{(1:0)}}(\mathbf{H}^ {(0)})\) _approximates_ \(\ell\) _steps of (ICGD-_\(\ell_{2}\)_): We have_ \(\mathbf{h}_{i}^{(\ell)}=[\mathbf{x}_{i};y_{i}^{\prime};\widehat{\mathbf{w}}^{ \ell};\boldsymbol{0}_{D-2d-3};1;t_{i}]\) _for every_ \(i\in[N+1]\)_, where_ \[\big{\|}\widehat{\mathbf{w}}^{\ell}-\mathbf{w}_{\mathrm{GD}}^{\ell}\big{\|}_{ 2}\leq\varepsilon\cdot(2L\eta B_{x}).\]
2. _(Prediction space) The final output_ \(\mathbf{H}^{(L+1)}=\operatorname{TF}_{\boldsymbol{\theta}}(\mathbf{H}^{(0)})\) _approximates the prediction of_ \(L\) _steps of (ICGD-_\(\ell_{2}\)_): We have_ \(\mathbf{h}_{N+1}^{(L+1)}=[\mathbf{x}_{N+1};\widehat{y}_{N+1};\widehat{\mathbf{ w}}^{L};\mathbf{0}_{D-2d-3};1;0]\)_, where_ \[\big{|}\widehat{y}_{N+1}-\big{\langle}\mathbf{w}_{\mathrm{GD}}^{L},\mathbf{x }_{N+1}\big{\rangle}\big{|}\leq\varepsilon\cdot(2L\eta B_{x}^{2}).\]

_Further, the transformer admits norm bound \(\|\boldsymbol{\theta}\|\leq 2+R+(2C+\lambda)\eta\)._

Proof.: This construction is the same as in the proof of Theorem D.1, except that within each layer \(\ell\in[L]\), we add one more attention head \((\mathbf{Q}^{(\ell)},\mathbf{K}^{(\ell)},\mathbf{V}^{(\ell)})\subset\mathbb{ R}^{D\times D}\) which when acting on its input \(\mathbf{h}_{i}^{(\ell-1)}=[*;*;\widehat{\mathbf{w}}^{\ell-1};1;*]\) gives

\[\mathbf{Q}^{(\ell)}\mathbf{h}_{i}^{(\ell-1)}=\begin{bmatrix}1\\ \boldsymbol{0}_{D-1}\end{bmatrix},\quad\mathbf{K}^{(\ell)}\mathbf{h}_{j}^{( \ell-1)}=\begin{bmatrix}1\\ \boldsymbol{0}_{D-1}\end{bmatrix},\quad\mathbf{V}^{(\ell)}\mathbf{h}_{j}^{( \ell-1)}=\begin{bmatrix}\boldsymbol{0}_{d+1}\\ -\eta\lambda\widehat{\mathbf{w}}^{\ell-1}\\ \boldsymbol{0}_{2}\end{bmatrix}\]

for all \(i,j\in[N+1]\). Note that \(\big{\|}\mathbf{Q}^{(\ell)}\big{\|}_{\mathrm{op}}=\big{\|}\mathbf{K}^{(\ell) }\big{\|}_{\mathrm{op}}=1\), and \(\big{\|}\mathbf{V}^{(\ell)}\big{\|}_{\mathrm{op}}=\eta\lambda\). Further, it is straightforward to check that the output of this attention head on every \(\mathbf{h}_{i}^{(\ell)}\) is

Adding this onto the original output of the \(\ell\)-th layer exactly implements the gradient of the regularizer \(\mathbf{w}\mapsto\frac{\lambda}{2}\|\mathbf{w}\|_{2}^{2}\). The rest of the proof follows by repeating the argument of Theorem D.1, and combining the norm bound for the additional attention head here with the norm bound therein. 

### Proof of Theorem d.3

We only need to prove the following single-step version of Theorem D.3.

**Proposition E.2**.: _Under the assumptions of Theorem D.3, there exists a 2-layer transformer \(\operatorname{TF}_{\boldsymbol{\theta}}\) with the same bounds on the number of heads, hidden dimension and the norm, such that for any input data \((\mathcal{D},\mathbf{x}_{N+1})\) and any \(\mathbf{w}\in\mathbb{R}^{d}\), \(\operatorname{TF}_{\boldsymbol{\theta}}\) maps_

\[\mathbf{h}_{i}=[\mathbf{x}_{i};y_{i}^{\prime};\mathbf{w};\boldsymbol{0};1;t_ {i}]\quad\to\quad\mathbf{h}_{i}^{\prime}=[\mathbf{x}_{i};y_{i}^{\prime}; \mathbf{w}_{\eta}^{+};\boldsymbol{0};1;t_{i}],\]

_where_

\[\mathbf{w}_{\eta}^{+}=\mathrm{Proj}_{\mathcal{W}}\left(\mathbf{w}-\eta \nabla\widetilde{L}_{N}(\mathbf{w})+\boldsymbol{\varepsilon}(\mathbf{w}) \right),\qquad\|\boldsymbol{\varepsilon}(\mathbf{w})\|_{2}\leq\eta\varepsilon.\]

Before we present the formal (and technical) proof of Proposition E.2, we first provide some intuitions. To begin with, we first note that

\[\nabla_{\mathbf{w}}\widehat{L}_{N}(\mathbf{w})=\frac{1}{N}\sum_{i=1}^{N} \partial_{1}\ell(\mathrm{pred}(\mathbf{x}_{i};\mathbf{w}),y_{i})\cdot\nabla_ {\mathbf{w}}\mathrm{pred}(\mathbf{x}_{i};\mathbf{w}),\] (16)

where \(\partial_{1}\ell\) is the partial derivative of \(\ell\) with respect to the first component, and

\[\nabla_{\mathbf{w}}\mathrm{pred}(\mathbf{x}_{i};\mathbf{w})=\begin{bmatrix}u _{1}\cdot r^{\prime}(\langle\mathbf{v}_{1},\mathbf{x}_{i}\rangle)\cdot \mathbf{x}_{i}\\ r(\langle\mathbf{v}_{1},\mathbf{x}_{i}\rangle)\\ \vdots\\ u_{K}\cdot r^{\prime}(\langle\mathbf{v}_{K},\mathbf{x}_{i}\rangle)\cdot \mathbf{x}_{i}\\ r(\langle\mathbf{v}_{K},\mathbf{x}_{i}\rangle)\end{bmatrix}\in\mathbb{R}^{K(d+ 1)}.\] (17)Therefore, the basic idea is that we can use an attention layer to approximate \((\mathbf{x}_{i},\mathbf{w})\mapsto\mathrm{pred}(\mathbf{x}_{i};\mathbf{w})\), then use an MLP layer to implement \((\mathrm{pred}(\mathbf{x}_{i};\mathbf{w}),y_{i}^{\prime},t_{i})\mapsto 1\{i<N+1\} \cdot\partial_{1}\ell(\mathrm{pred}(\mathbf{x}_{i};\mathbf{w}),y_{i})\), and then use an attention layer to compute the gradient descent step \(\mathbf{w}\mapsto\mathbf{w}-\eta\nabla L_{N}(\mathbf{w})\), and finally use an MLP layer to implement the projection into \(\mathcal{W}\).

Based on the observations above, we now present the proof of Proposition E.2.

Proof of Proposition E.2.: We write \(D_{0}=d+1+K(d+1)\) be the length of the vector \([\mathbf{x}_{i};y_{i};\mathbf{w}]\). We also define

\[B_{r}:=\max_{|t|\leq B_{x}B_{u}}|r(t)|\,,\qquad B_{g}:=\max_{|t|\leq KB_{r},|y| \leq B_{y}}|\partial_{t}\ell(t,y)|\,.\]

Let us fix \(\varepsilon_{r},\varepsilon_{p},\varepsilon_{\ell}>0\) that will be specified later in proof (see (18)). By our assumption and Proposition B.1, the following facts hold.

1. The function \(r(t)\) is \((\varepsilon_{r},R_{1},M_{1},C_{1})\) for \(R_{1}=\max\left\{B_{x}B_{u},1\right\}\), \(M_{1}\leq\widetilde{\mathcal{O}}\left(C_{1}^{2}\varepsilon_{r}^{-2}\right)\), where \(C_{1}\) depends only on \(R_{1}\) and the \(C^{2}\)-smoothness of \(r\). Therefore, there exists \[\overline{r}(t)=\sum_{m=1}^{M}c_{m}^{1}\sigma(\left\langle\mathbf{a}_{m}^{1}, [t;1]\right\rangle)\ \ \mathrm{with}\ \ \ \sum_{m=1}^{M}\left|c_{m}^{1}\right|\leq C_{1},\ \left\|\mathbf{a}_{m}^{1}\right\|_{1}\leq 1,\ \forall m\in[M_{1}],\] such that \(\sup_{t\in[-R_{1},R_{1}]}|r(t)-\overline{r}(t)|\leq\varepsilon_{r}\).
2. The function \((t,y)\mapsto\partial_{1}\ell(t,y)\) is \((\varepsilon_{\ell},R_{2},M_{2},C_{2})\) for \(R_{2}=\max\left\{KB_{r},B_{y},1\right\}\)\(M_{2}\leq\widetilde{\mathcal{O}}\left(C_{2}^{2}\varepsilon_{\ell}^{-2}\right)\), where \(C_{2}\) depends only on \(R_{2}\) and the \(C^{3}\)-smoothness of \(\partial_{1}\ell\). Therefore, there exists \[g(t,y)=\sum_{m=1}^{M}c_{m}^{2}\sigma(\left\langle\mathbf{a}_{m}^{2},[t;y;1] \right\rangle)\ \ \mathrm{with}\ \ \ \sum_{m=1}^{M}\left|c_{m}^{2}\right|\leq C_{2},\ \left\|\mathbf{a}_{m}^{2}\right\|_{1}\leq 1,\ \forall m\in[M_{2}],\] such that \(\sup_{(t,y)\in[-R_{2},R_{2}]^{2}}|g(t,y)-\partial_{1}\ell(t,y)|\leq\varepsilon_ {\ell}\).
3. The function \((s,t)\mapsto s\cdot r^{\prime}(t)\) is \((\varepsilon_{p},R_{3},M_{3},C_{3})\) for \(R_{3}=\max\left\{B_{x}B_{u},B_{g}B_{u},1\right\}\), \(M_{3}\leq\widetilde{\mathcal{O}}\left(C_{3}^{2}\varepsilon_{p}^{-2}\right)\), where \(C_{3}\) depends only on \(R_{3}\) and the \(C^{3}\)-smoothness of \(r^{\prime}\). Therefore, there exists \[P(s,t)=\sum_{m=1}^{M}c_{m}^{3}\sigma(\left\langle\mathbf{a}_{m}^{3},[s;t;1] \right\rangle)\ \ \mathrm{with}\ \ \ \sum_{m=1}^{M}\left|c_{m}^{3}\right|\leq C_{3},\ \left\|\mathbf{a}_{m}^{3}\right\|_{1}\leq 1,\ \forall m\in[M_{3}],\] such that \(\sup_{(s,t)\in[-R_{3},R_{3}]^{2}}|P(s,t)-s\cdot r^{\prime}(t)|\leq\varepsilon_ {p}\).

In the following, we proceed to construct the desired transformer step by step.

Step 1: construction of \(\boldsymbol{\theta}_{\mathtt{attn}}^{(1)}\). We consider the matrices \(\{\mathbf{Q}_{k,m}^{(1)},\mathbf{K}_{k,m}^{(1)},\mathbf{V}_{k,m}^{(1)}\}_{k\in[ K],m\in[M_{1}]}\) so that for all \(i,j\in[N+1]\), we have

\[\mathbf{Q}_{k,m}^{(1)}\mathbf{h}_{i}=\begin{bmatrix}\mathbf{a}_{m}^{1}[1]\cdot \mathbf{x}_{i}\\ \mathbf{a}_{m}^{1}[2]\\ \mathbf{0}\end{bmatrix},\quad\mathbf{K}_{k,m}^{(1)}\mathbf{h}_{j}=\begin{bmatrix} \mathbf{v}_{k}\\ 1\\ \mathbf{0}\end{bmatrix},\quad\mathbf{V}_{k,m}^{(1)}\mathbf{h}_{j}=c_{m}^{1} \cdot u_{k}\mathbf{e}_{D_{0}+1}.\]

As the input has structure \(\mathbf{h}_{i}=[\mathbf{x}_{i};y_{i}^{\prime};\mathbf{w};\mathbf{0};1;t_{i}]\), these matrices indeed exist, and further it is straightforward to check that they have norm bounds

\[\max_{k,m}\left\|\mathbf{Q}_{k,m}^{(1)}\right\|_{\mathrm{op}}\leq 1,\quad\max_{k,m} \left\|\mathbf{K}_{k,m}^{(1)}\right\|_{\mathrm{op}}\leq 1,\quad\sum_{k,m}\left\| \mathbf{V}_{k,m}^{(1)}\right\|_{\mathrm{op}}\leq C_{1}.\]

A simple calculation shows that

\[\sum_{m\in[M_{1}],k\in[K]}\sigma\Big{(}\Big{\langle}\mathbf{Q}_{k,m}^{(1)} \mathbf{h}_{i},\mathbf{K}_{k,m}^{(1)}\mathbf{h}_{j}\Big{\rangle}\Big{)}\mathbf{V }_{k,m}^{(1)}\mathbf{h}_{j}=\sum_{k=1}^{K}u_{k}\overline{r}(\left\langle\mathbf{v }_{k},\mathbf{x}_{i}\right\rangle)\cdot\mathbf{e}_{D_{0}+1}.\]For simplicity, we denote \(\overline{\mathrm{pred}}(\mathbf{x};\mathbf{w}):=\sum_{k=1}^{K}u_{k}\overline{r}( \left\langle\mathbf{v}_{k},\mathbf{x}\right\rangle)\) in the following analysis. Thus, letting the attention layer \(\boldsymbol{\theta}_{\mathtt{attn}}^{(1)}=\left\{(\mathbf{V}_{k,m}^{(1)}, \mathbf{Q}_{k,m}^{(1)},\mathbf{K}_{k,m}^{(1)})\right\}_{(k,m)}\), we have

\[\mathrm{Attn}_{\boldsymbol{\theta}_{\mathtt{attn}}^{(1)}}:\mathbf{h}_{i} \mapsto\mathbf{h}_{i}^{(0.5)}=[\mathbf{x}_{i};y_{i}^{\prime};\mathbf{w}; \overline{\mathrm{pred}}(\mathbf{x}_{i};\mathbf{w});\mathbf{0};1;t_{i}].\]

Step 2: construction of \(\boldsymbol{\theta}_{\mathtt{nlp}}^{(1)}\). We pick matrices \(\mathbf{W}_{1},\mathbf{W}_{2}\) so that \(\mathbf{W}_{1}\) maps

\[\mathbf{W}_{1}\mathbf{h}_{i}^{(0.5)}=\left[\mathbf{a}_{m}^{2}[1]\cdot \overline{\mathrm{pred}}(\mathbf{x}_{i};\mathbf{w})+\mathbf{a}_{m}^{2}[2] \cdot y_{i}^{\prime}+\mathbf{a}_{m}^{2}[3]-R_{2}(1-t_{i})\right]_{m\in[M_{2}]} \in\mathbb{R}^{M_{2}},\]

and \(\mathbf{W}_{2}\in\mathbb{R}^{D\times M_{3}}\) with entries being \((\mathbf{W}_{2})_{(j,m)}=c_{m}^{2}1\{j=D_{0}+2\}\). It is clear that \(\left\|\mathbf{W}_{1}\right\|_{\mathrm{op}}\leq R_{2}+1\), \(\left\|\mathbf{W}_{2}\right\|_{\mathrm{op}}\leq C_{2}\). Then we have

\[\mathbf{W}_{2}\sigma(\mathbf{W}_{1}\mathbf{h}_{i}^{(0.5)}) =\sum_{m\in[M_{3}]}\sigma\big{(}\big{\langle}\mathbf{a}_{m}^{2}, [\overline{\mathrm{pred}}(\mathbf{x}_{i};\mathbf{w});y_{i}^{\prime};1]\big{\rangle} -R_{2}(1-t_{j})\big{)}\cdot c_{m}^{2}\mathbf{e}_{D_{0}+2}\] \[=1\{t_{j}=1\}\cdot g(\overline{\mathrm{pred}}(\mathbf{x}_{i}; \mathbf{w}),y_{i}^{\prime})\cdot\mathbf{e}_{D_{0}+2}.\]

In the following, we abbreviate \(g_{i}=1\{t_{j}=1\}\cdot g(\overline{\mathrm{pred}}(\mathbf{x}_{i};\mathbf{w}),y_{i}^{\prime})\). Hence, \(\boldsymbol{\theta}_{\mathtt{nlp}}\) maps

\[\mathrm{MLP}\boldsymbol{\theta}_{\mathtt{nlp}}:\mathbf{h}_{i}^{(0.5)}\mapsto \mathbf{h}_{i}^{(1)}=[\mathbf{x}_{i};y_{i}^{\prime};\mathbf{w};\overline{ \mathrm{pred}}(\mathbf{x}_{i};\mathbf{w});g_{i};\mathbf{0};1;t_{i}].\]

By the definition of the function \(g\), for each \(i\in[N]\),

\[|g_{i}-\partial_{1}\ell(\mathrm{pred}(\mathbf{x}_{i};\mathbf{w}),y_{i})|\leq \varepsilon_{\ell}+B_{u}L_{\ell}\varepsilon_{r},\]

where \(L_{\ell}:=\max_{|t|\leq KB_{r},|y|\leq B_{y}}\big{|}\partial_{t}^{2}\ell(\ell, y)\big{|}\) is the smoothness of \(\partial_{1}\ell\). Also, \(g_{N+1}=0\) by definition.

Step 3: construction of \(\boldsymbol{\theta}_{\mathtt{attn}}^{(2)}\). We consider the matrices \(\{\mathbf{Q}_{k,1,m}^{(2)},\mathbf{K}_{k,1,m}^{(2)},\mathbf{V}_{k,1,m}^{(2)}\}_ {k\in[K],m\in[M_{3}]}\) so that for all \(i,j\in[N+1]\), we have

\[\mathbf{Q}_{k,1,m}^{(2)}\mathbf{h}_{i}^{(1)}=\begin{bmatrix}\mathbf{a}_{m}^{3} [1]\cdot u_{k}\\ \mathbf{a}_{m}^{3}[2]\cdot\mathbf{v}_{k}\\ \mathbf{a}_{m}^{3}[3]\\ \mathbf{0}\end{bmatrix},\qquad\mathbf{K}_{k,1,m}^{(2)}\mathbf{h}_{j}^{(1)}= \begin{bmatrix}g_{j}\\ \mathbf{x}_{j}\\ 1\\ \mathbf{0}\end{bmatrix},\qquad\mathbf{V}_{k,1,m}^{(2)}\mathbf{h}_{j}^{(1)}=- \frac{(N+1)\eta c_{m}^{3}}{N}\cdot\begin{bmatrix}\mathbf{0}_{k(d+1)}\\ \mathbf{x}_{j}\\ \mathbf{0}\end{bmatrix}.\]

We further consider the matrices \(\{\mathbf{Q}_{k,2,m}^{(2)},\mathbf{K}_{k,2,m}^{(2)},\mathbf{V}_{k,2,m}^{(2)}\}_ {k\in[K],m\in[M_{1}]}\) so that for all \(i,j\in[N+1]\), we have

\[\mathbf{Q}_{k,2,m}^{(2)}\mathbf{h}_{i}^{(1)}=\begin{bmatrix}\mathbf{a}_{m}^{1} [1]\cdot\mathbf{v}_{k}\\ \mathbf{a}_{m}^{2}[2]\\ \mathbf{0}\end{bmatrix},\qquad\mathbf{K}_{k,2,m}^{(2)}\mathbf{h}_{j}^{(1)}= \begin{bmatrix}\mathbf{x}_{j}\\ 1\\ \mathbf{0}\end{bmatrix},\qquad\mathbf{V}_{k,2,m}^{(2)}\mathbf{h}_{j}^{(1)}=- \frac{(N+1)\eta c_{m}^{1}}{N}\cdot\begin{bmatrix}\mathbf{0}_{k(d+1)+d}\\ g_{j}\\ \mathbf{0}\end{bmatrix}.\]

By the structure of the input \(\mathbf{h}_{i}^{(1)}\), these matrices indeed exist, and further it is straightforward to check that they have norm bounds

\[\max_{(k,w,m)}\left\|\mathbf{Q}_{k,w,m}^{(2)}\right\|_{\mathrm{op}}\leq 1, \quad\max_{(k,w,m)}\left\|\mathbf{K}_{k,w,m}^{(2)}\right\|_{\mathrm{op}}\leq 1, \quad\sum_{(k,w,m)}\left\|\mathbf{V}_{k,w,m}^{(2)}\right\|_{\mathrm{op}}\leq 2\eta C_{1}+2 \eta C_{3}.\]

Furthermore, a simple calculation shows that

\[\mathbf{g}(\mathbf{w})=:\frac{1}{N+1}\sum_{i=1}^{N+1}\sum_{(k,w,m)}\sigma\Big{(} \Big{\langle}\mathbf{Q}_{k,w,m}^{(2)}\mathbf{h}_{i},\mathbf{K}_{k,w,m}^{(2)} \mathbf{h}_{j}\Big{\rangle}\Big{)}\mathbf{V}_{k,w,m}^{(2)}\mathbf{h}_{j}=-\frac{ \eta}{N}\sum_{j=1}^{N+1}\begin{bmatrix}P(u_{1}g_{j},\langle\mathbf{v}_{1}, \mathbf{x}_{j}\rangle)\cdot\mathbf{x}_{j}\\ \overline{r}(\langle\mathbf{v}_{1},\mathbf{x}_{j}\rangle)\cdot g_{j}\\ \vdots\\ P(u_{K}g_{j},\langle\mathbf{v}_{K},\mathbf{x}_{j}\rangle)\cdot\mathbf{x}_{j}\\ \overline{r}(\langle\mathbf{v}_{K},\mathbf{x}_{j}\rangle)\cdot g_{j}\\ \mathbf{0}\end{bmatrix},\]

where the summation is taken over all possibilities of the tuple \((k,w,m)\), i.e. over the union of

\([K]\times\{1\}\times[M_{3}]\) and \([K]\times\{2\}\times[M_{1}]\).

By our definition, we have \(|P(s,t)-sr^{\prime}(t)|\leq\varepsilon_{p}\) for all \(s,t\in[-R_{3},R_{3}]\). Therefore, for each \(i\in[N]\), \(k\in[K]\),

\[|P(u_{k}g_{j},\langle\mathbf{v}_{k},\mathbf{x}_{j}\rangle)-\partial _{1}\ell(\operatorname{pred}(\mathbf{x}_{j};\mathbf{w}),y_{j})\cdot u_{k}\cdot r ^{\prime}(\langle\mathbf{v}_{k},\mathbf{x}_{j}\rangle)| \leq\varepsilon_{p}+|g_{j}-\partial_{1}\ell(\operatorname{pred}( \mathbf{x}_{i};\mathbf{w}),y_{i})|\cdot|u_{k}|\cdot|r^{\prime}(\langle\mathbf{ v}_{k},\mathbf{x}_{j}\rangle)|\] \[\leq\varepsilon_{p}+B_{u}L_{r}(\varepsilon_{\ell}+B_{u}L_{\ell} \varepsilon_{r}),\]

where \(L_{r}:=\max_{|t|\leq B_{x}B_{u}}|r^{\prime}(t)|\) is the upper bound of \(r^{\prime}\). Similarly, for each \(i\in[N]\), \(k\in[K]\), we have

\[|\overline{r}(\langle\mathbf{v}_{k},\mathbf{x}_{j}\rangle)\cdot g_{j}-r( \langle\mathbf{v}_{k},\mathbf{x}_{j}\rangle)\cdot\partial_{1}\ell( \operatorname{pred}(\mathbf{x}_{j};\mathbf{w}),y_{j})|\leq 2B_{g} \varepsilon_{r}+2B_{r}(\varepsilon_{\ell}+B_{u}L_{\ell}^{2}\varepsilon_{r}).\]

As for the case \(i=N+1\), we have \(g_{N+1}=0\) and \(|P(u_{k}g_{N+1},\langle\mathbf{v}_{k},\mathbf{x}_{N+1}\rangle)|\leq\varepsilon _{p}\) for each \(k\in[K]\) by defintion. Combining these estimations and using (16) and (17), we can conclude that

\[\left\|\eta^{-1}\mathbf{g}(\mathbf{w})+\nabla\widehat{L}_{N}(\mathbf{w}) \right\|_{2}\leq\sqrt{K}B_{x}\cdot[\varepsilon_{p}+B_{u}L_{r}(\varepsilon_{ \ell}+B_{u}L_{\ell}\varepsilon_{r})]+2\sqrt{K}[B_{g}\varepsilon_{r}+B_{r}( \varepsilon_{\ell}+B_{u}L_{\ell}\varepsilon_{r})].\]

Thus, to ensure \(\left\|\eta^{-1}\mathbf{g}(\mathbf{w})+\nabla\widehat{L}_{N}(\mathbf{w}) \right\|_{2}\leq\varepsilon\), we only need to choose \(\varepsilon_{p},\varepsilon_{\ell},\varepsilon_{r}\) as

\[\varepsilon_{p}=\frac{\varepsilon}{3\sqrt{K}B_{x}},\qquad\varepsilon_{\ell}= \frac{\varepsilon}{9\sqrt{K}\max\left\{B_{r},L_{r}B_{x}B_{u}\right\}},\qquad \varepsilon_{r}=\frac{\varepsilon}{15\sqrt{K}\max\left\{B_{g},L_{\ell}B_{r}B_ {u},L_{r}L_{\ell}B_{x}B_{r}B_{u}^{2}\right\}}.\] (18)

Thus, letting the attention layer \(\boldsymbol{\theta}_{\mathtt{attn}}^{(2)}=\left\{(\mathbf{V}_{k,w,m}^{(2)}, \mathbf{Q}_{k,w,m}^{(2)},\mathbf{K}_{k,w,m}^{(2)})\right\}_{(k,w,m)}\), we have

\[\operatorname{Attn}_{\boldsymbol{\theta}_{\mathtt{attn}}^{(2)}}:\mathbf{h}_{i }^{(1)}\mapsto\mathbf{h}_{i}^{(1.5)}=[\mathbf{x}_{i};y_{i}^{\prime};\mathbf{w }+\eta\mathbf{g}(\mathbf{w});\overline{\operatorname{pred}}(\mathbf{x}_{i}; \mathbf{w});g_{i};\mathbf{0};1;t_{i}].\]

Step 4: construction of \(\boldsymbol{\theta}_{\mathtt{nlp}}^{(2)}\). We only need to pick \(\boldsymbol{\theta}_{\mathtt{nlp}}^{(2)}\) so that it maps

\[\mathbf{h}_{i}^{(1.5)}=[\mathbf{x}_{i};y_{i}^{\prime};\mathbf{w}+\eta\mathbf{ g}(\mathbf{w});\overline{\operatorname{pred}}(\mathbf{x}_{i};\mathbf{w});g_{i}; \mathbf{0};1;t_{i}]\xrightarrow{\operatorname{MLP}_{\boldsymbol{\theta}_{ \mathtt{nlp}}^{(2)}}^{(2)}}\mathbf{h}_{i}^{(2)}=[\mathbf{x}_{i};y_{i}^{\prime}; \operatorname{Proj}_{\mathcal{W}}(\mathbf{w}-\eta\mathbf{g}(\mathbf{w}));0;0; \mathbf{0};1;t_{i}].\]

By our assumption on the map \(\operatorname{Proj}_{\mathcal{W}}\), this is easy.

Combining the four steps above and taking \(\boldsymbol{\theta}=(\boldsymbol{\theta}_{\mathtt{attn}}^{(1)},\boldsymbol{ \theta}_{\mathtt{nlp}}^{(1)},\boldsymbol{\theta}_{\mathtt{attn}}^{(2)}, \boldsymbol{\theta}_{\mathtt{nlp}}^{(2)})\) completes the proof. \(\square\)

### Proof of Lemma d.3

For every \(\ell\geq 0\), define the intermediate iterates (before projection)

\[\widehat{\mathbf{w}}^{\ell+\frac{1}{2}}:=\widehat{\mathbf{w}}^{\ell}-\eta \big{(}\nabla f(\widehat{\mathbf{w}}^{\ell})+\varepsilon^{\ell}\big{)},\quad \mathbf{w}_{\mathrm{GD}}^{\ell+\frac{1}{2}}:=\mathbf{w}_{\mathrm{GD}}^{\ell}- \eta\nabla f(\mathbf{w}_{\mathrm{GD}}^{\ell}),\]

so that \(\widehat{\mathbf{w}}^{\ell+1}=\operatorname{Proj}_{\mathcal{W}}(\widehat{ \mathbf{w}}^{\ell+\frac{1}{2}})\) and \(\mathbf{w}_{\mathrm{GD}}^{\ell+1}=\operatorname{Proj}_{\mathcal{W}}(\mathbf{w}_ {\mathrm{GD}}^{\ell+\frac{1}{2}})\).

We first prove part (a). We begin by deriving a relation between \(\left\|\widehat{\mathbf{w}}^{\ell+1}-\widehat{\mathbf{w}}^{\ell}\right\|_{2}^{2}\) and \(\left\|\eta\mathbf{G}_{\mathcal{W},\eta}^{f}(\widehat{\mathbf{w}}^{\ell}) \right\|_{2}^{2}\).

Let \(\widehat{\mathbf{w}}^{\ell+\frac{1}{2}}:=\widehat{\mathbf{w}}^{\ell}-\eta \nabla f(\widehat{\mathbf{w}}^{\ell})\) and \(\widetilde{\mathbf{w}}^{\ell+1}:=\operatorname{Proj}_{\mathcal{W}}(\widehat{ \mathbf{w}}^{\ell+\frac{1}{2}})\) denote the _exact_ projected gradient iterate starting from \(\widehat{\mathbf{w}}^{\ell}\). We have

\[\left\|\widehat{\mathbf{w}}^{\ell+1}-\widehat{\mathbf{w}}^{\ell} \right\|_{2}^{2}\overset{(i)}{\geq}\frac{1}{2}\left\|\widetilde{\mathbf{w}}^{ \ell+1}-\widehat{\mathbf{w}}^{\ell}\right\|_{2}^{2}-\left\|\widehat{\mathbf{w}}^{ \ell+1}-\widetilde{\mathbf{w}}^{\ell+1}\right\|_{2}^{2}\overset{(ii)}{\geq} \frac{1}{2}\left\|\widetilde{\mathbf{w}}^{\ell+1}-\widehat{\mathbf{w}}^{\ell} \right\|_{2}^{2}-\left\|\widehat{\mathbf{w}}^{\ell+\frac{1}{2}}-\widetilde{\mathbf{w}}^{ \ell+\frac{1}{2}}\right\|_{2}^{2}\] (19) \[\overset{(iii)}{=}\frac{\eta^{2}}{2}\left\|\widehat{\mathbf{G}}_{ \mathcal{W},\eta}^{f}(\widehat{\mathbf{w}}^{\ell})\right\|_{2}^{2}-\left\|\eta \varepsilon\right\|_{2}^{2}\geq\frac{\eta^{2}}{2}\left\|\widehat{\mathbf{G}}_{ \mathcal{W},\eta}^{f}(\widehat{\mathbf{w}}^{\ell})\right\|_{2}^{2}-\eta^{2} \varepsilon^{2}.\]

Above, (i) uses the inequality \(\left\|a-b\right\|_{2}^{2}\geq\frac{1}{2}\left\|a\right\|_{2}^{2}-\left\|b \right\|_{2}^{2}\); (ii) uses the fact that projection to a convex set is a non-expansion; (iii) uses the definition of the gradient mapping.

By the \(L_{f}\)-smoothness of \(f\) within \(\mathcal{W}\), we have

\[f(\widehat{\mathbf{w}}^{\ell+1})-f(\widehat{\mathbf{w}}^{\ell})\leq\left\langle \nabla f(\widehat{\mathbf{w}}^{\ell}),\widehat{\mathbf{w}}^{\ell+1}-\widehat{ \mathbf{w}}^{\ell}\right\rangle+\frac{L_{f}}{2}\left\|\widehat{\mathbf{w}}^{ \ell+1}-\widehat{\mathbf{w}}^{\ell}\right\|_{2}^{2}\]\[\left\|\widehat{\mathbf{w}}^{\ell}-\mathbf{w}_{\mathrm{GD}}^{\ell} \right\|_{2}+\eta\varepsilon\overset{(iii)}{\leq}C\cdot\frac{C^{\ell}-1}{C-1} \cdot\eta\varepsilon=L_{f}^{\ell+1}-1\cdot\eta\varepsilon.\]

Above, (i) uses again the non-expansiveness of the convex projection \(\mathrm{Proj}_{\mathcal{W}}\); (ii) uses the fact that the operator \(\mathbf{w}\mapsto\mathbf{w}-\eta\nabla f(\mathbf{w})\) is \((1+\eta L_{f})=C\)-Lipschitz; and (iii) uses the inductive hypothesis. This proves the case for \(\ell+1\) and thus finishes the induction. We can further relax (20) into

\[\left\|\widehat{\mathbf{w}}^{\ell}-\mathbf{w}_{\mathrm{GD}}^{\ell}\right\|_{2 }\leq\frac{C^{\ell}}{1+\eta L_{f}-1}\cdot\eta\varepsilon=L_{f}^{-1}(1+\eta L _{f})^{\ell}\varepsilon.\]

This proves part (b). 

## Appendix F Proofs for Section 3.1

### Proof of Theorem 4

Fix \(\lambda\geq 0\), \(0\leq\alpha\leq\beta\) with \(\kappa:=\frac{\beta+\lambda}{\alpha+\lambda}\), and \(B_{w}>0\), and consider any in-context data \(\mathcal{D}\) such that the precondition of Theorem 4 holds. Let

\[L_{\mathrm{ridge}}(\mathbf{w}):=\frac{1}{2N}\sum_{i=1}^{N}\left(\left\langle \mathbf{w},\mathbf{x}_{i}\right\rangle-y_{i}\right)^{2}+\frac{\lambda}{2} \left\|\mathbf{w}\right\|_{2}^{2}\]denote the ridge regression loss in (lCRidge), so that \(\mathbf{w}^{\lambda}_{\mathrm{ridge}}=\arg\min_{\mathbf{w}\in\mathbb{R}^{d}}L_{ \mathrm{ridge}}(\mathbf{w})\). It is a standard result that \(\nabla^{2}L_{\mathrm{ridge}}(\mathbf{w})=\mathbf{X}^{\top}\mathbf{X}/N+\lambda \mathbf{I}_{d}\), so that \(L_{\mathrm{ridge}}\) is \((\alpha+\lambda)\)-strongly convex and \((\beta+\lambda)\)-smooth over \(\mathbb{R}^{d}\).

Consider the gradient descent algorithm on the ridge loss

\[\mathbf{w}^{t+1}_{\mathrm{GD}}=\mathbf{w}^{t}_{\mathrm{GD}}-\eta\nabla L_{ \mathrm{ridge}}(\mathbf{w}^{t}_{\mathrm{GD}})\]

with initialization, learning rate, and number of steps

\[\mathbf{w}^{0}_{\mathrm{GD}}:=\mathbf{0}_{d},\quad\eta:=\frac{1}{\beta+ \lambda},\quad T:=\left\lceil 2\kappa\log\left(\frac{B_{x}B_{w}}{2\varepsilon} \right)\right\rceil.\]

By standard convergence results for strongly convex and smooth functions (Proposition B.2), we have for all \(t\geq 1\) that

\[\left\|\mathbf{w}^{t}_{\mathrm{GD}}-\mathbf{w}^{\lambda}_{\mathrm{ridge}} \right\|_{2}^{2}\leq\exp\left(-\frac{t}{\kappa}\right)\left\|\mathbf{w}^{0}_{ \mathrm{GD}}-\mathbf{w}^{\lambda}_{\mathrm{ridge}}\right\|_{2}^{2}=\exp\left( -\frac{t}{\kappa}\right)\left\|\mathbf{w}^{\lambda}_{\mathrm{ridge}}\right\|_{2 }^{2}.\]

Further, we have

\[\left\|\mathbf{w}^{T}_{\mathrm{GD}}-\mathbf{w}^{\lambda}_{\mathrm{ridge}} \right\|_{2}\leq\exp\left(-\frac{T}{2\kappa}\right)\left\|\mathbf{w}^{\lambda }_{\mathrm{ridge}}\right\|_{2}\leq\frac{2\varepsilon}{B_{x}B_{w}}\cdot\frac{B_ {w}}{2}\leq\frac{\varepsilon}{B_{x}}.\] (21)

It remains to construct a transformer to approximate \(\mathbf{w}^{T}_{\mathrm{GD}}\). Notice that the problem (lCRidge) corresponds to an \(\ell_{2}\)-regularized ERM with the square loss \(\ell(s,t):=\frac{1}{2}(s-t)^{2}\), whose partial derivative \(\partial_{s}\ell(s,t)=s-t\) is exactly a sum of two relus:

\[\partial_{s}\ell(s,t)=2\sigma((s-t)/2)-2\sigma(-(s-t)/2).\]

In particular, this shows that \(\partial_{s}\ell(s,t)\) is \((0,R,2,4)\)-approximable for any \(R>0\), in particular for \(R=\max\left\{B_{x}B_{w},B_{y},1\right\}\).

Therefore, we can apply Corollary E.1 with the square loss \(\ell\), learning rate \(\eta\), regularization strength \(\lambda\) and accuracy parameter \(\varepsilon=0\) to obtain that there exists an attention-only transformer \(\mathrm{TF}^{0}_{\boldsymbol{\theta}}\) with \((T+1):=L\) layers such that the final output \(\mathbf{h}^{(L)}_{N+1}=[\mathbf{x}_{N+1};\widehat{y}_{N+1};*]\) with

\[\left|\widehat{y}_{N+1}-\left\langle\mathbf{w}^{T}_{\mathrm{GD}},\mathbf{x}_{ N+1}\right\rangle\right|=0,\] (22)

and number of heads \(M^{(\ell)}=3\) for all \(\ell\in[L-1]\) (can be taken as \(2\) in the unregularized case \(\lambda=0\) directly by Theorem D.1), and \(M^{(L)}=2\). Further, \(\boldsymbol{\theta}\) admits norm bound \(\left\|\boldsymbol{\theta}\right\|\leq 2+R+\frac{8+\lambda}{\beta+ \lambda}\leq 3R+8(\beta+\lambda)^{-1}+1\leq 4R+8(\beta+\lambda)^{-1}\).

Combining (21) and (22), we obtain that

\[\left|\widehat{y}_{N+1}-\left\langle\mathbf{w}^{\lambda}_{\mathrm{ridge}}, \mathbf{x}_{N+1}\right\rangle\right|=\left|\left\langle\mathbf{w}^{T}_{ \mathrm{GD}}-\mathbf{w}^{\lambda}_{\mathrm{ridge}},\mathbf{x}_{N+1}\right\rangle \right|\leq(\varepsilon/B_{x})\cdot B_{x}=\varepsilon.\]

Further, we have \(\mathsf{read}_{\mathbf{w}}(\mathbf{h}^{T}_{i})=\mathbf{w}^{T}_{\mathrm{GD}}\) for all \(i\in[N+1]\), where \(\mathsf{read}_{\mathbf{w}}(\mathbf{h}):=\mathbf{h}_{(d+2):(2d+1)}\) (cf. Corollary E.1), so that \(\|\mathsf{read}_{\mathbf{w}}(\mathbf{h}^{T}_{i})-\mathbf{w}^{\lambda}_{\mathrm{ ridge}}\|_{2}\leq\varepsilon/B_{x}\) as shown above. This finishes the proof. 

### Statistical analysis of in-context least squares

Consider the standard least-squares algorithm \(\mathcal{A}_{\mathrm{LS}}\) and least-squares estimator \(\widehat{\mathbf{w}}_{\mathrm{LS}}\in\mathbb{R}^{d}\) defined as

\[\mathcal{A}_{\mathrm{LS}}(\mathcal{D})(\mathbf{x}_{N+1}):=\left\langle\widehat{ \mathbf{w}}_{\mathrm{LS}},\mathbf{x}_{N+1}\right\rangle,\quad\widehat{\mathbf{ w}}_{\mathrm{LS}}=\left(\mathbf{X}^{\top}\mathbf{X}\right)^{-1}\mathbf{X}^{\top} \mathbf{y}\in\mathbb{R}^{d}.\] (ICLS)

For any distribution \(\mathsf{P}\) over \((\mathbf{x},y)\in\mathbb{R}^{d}\times\mathbb{R}\) and any estimator \(\mathbf{w}\in\mathbb{R}^{d}\), let

\[L_{\mathsf{P}}(\mathbf{w}):=\mathbb{E}_{(\mathbf{x}^{\prime},y)\sim\mathsf{P}} \Big{[}\tfrac{1}{2}(\left\langle\mathbf{w},\mathbf{x}^{\prime}\right\rangle-y^{ \prime})^{2}\Big{]}\]

denote the expected risk of \(\mathbf{w}\) over a new test example \((\mathbf{x}^{\prime},y^{\prime})\sim\mathsf{P}\).

**Assumption A** (Well-posedness for learning linear predictors).: _We say a distribution \(\mathsf{P}\) on \(\mathbb{R}^{d}\times\mathbb{R}\) is well-posed for learning linear predictors, if \((\mathbf{x},y)\sim\mathsf{P}\) satisfies_1. \(\|\mathbf{x}\|_{2}\leq B_{x}\) _and_ \(|y|\leq B_{y}\) _almost surely;_
2. _The covariance_ \(\mathbf{\Sigma}_{\mathsf{p}}:=\mathbb{E}_{\mathsf{p}}[\mathbf{x}\mathbf{x}^{ \top}]\) _satisfies_ \(\lambda_{\min}\mathbf{I}_{d}\preceq\mathbf{\Sigma}_{\mathsf{p}}\preceq\lambda_{ \max}\mathbf{I}_{d}\)_, with_ \(0<\lambda_{\min}\leq\lambda_{\max}\)_, and_ \(\kappa:=\lambda_{\max}/\lambda_{\min}\)_._
3. _The whitened vector_ \(\mathbf{\Sigma}_{\mathsf{p}}^{-1/2}\mathbf{x}\) _is_ \(K^{2}\)_-sub-Gaussian for some_ \(K\geq 1\)_._
4. _The best linear predictor_ \(\mathbf{w}_{\mathsf{p}}^{\star}:=\mathbb{E}_{\mathsf{p}}[\mathbf{x}\mathbf{x}^{ \top}]^{-1}\mathbb{E}_{\mathsf{p}}[\mathbf{x}y]\) _satisfies_ \(\|\mathbf{w}_{\mathsf{p}}^{\star}\|_{2}\leq B_{w}^{\star}\)_._
5. _We have_ \(\mathbb{E}[(y-\langle\mathbf{x},\mathbf{w}_{\mathsf{p}}^{\star}\rangle)^{2}| \mathbf{x}|\leq\sigma^{2}\) _with probability one (over_\mathbf{x}\)_)._

_Further, we say \(\mathsf{P}\) is well-posed with canonical parameters if_

\[B_{x}=\Theta(\sqrt{d}),\quad B_{y}=\Theta(1),\quad B_{w}^{\star}=\Theta(1), \quad\sigma\leq\mathcal{O}(1),\quad\lambda_{\max}=\Theta(1),\quad K=\Theta(1),\] (23)

_where \(\Theta(\cdot)\) and \(\mathcal{O}(\cdot)\) only hides absolute constants._

The following result bounds the excess risk of least squares under Assumption A with a clipping operation on the predictor; the clipping allows the result to only depend on the second moment of the noise (cf. Assumption A(5)) instead of e.g. its sub-Gaussianity, and also makes the result convenient to be directly translated to a result for transformers.

**Proposition F.1** (Guarantees for in-context least squares).: _Suppose distribution \(\mathsf{P}\) satisfies Assumption A. Then as long as \(N\geq\mathcal{O}(dK^{4}\log(1/\delta))\), we have the following:_

1. _The (clipped) least squares predictor achieves small expected excess risk (fast rate) over the best linear predictor: For any clipping radius_ \(R\geq B_{y}\)_,_ \[\mathbb{E}_{\mathcal{D},\mathbf{x}_{N+1},y_{N+1}\sim\mathsf{P}} \bigg{[}\frac{1}{2}(\mathsf{clip}_{R}(\langle\widehat{\mathbf{w}}_{\mathrm{ LS}},\mathbf{x}_{N+1}\rangle)-y_{N+1})^{2}\bigg{]}\leq\underbrace{\inf_{\mathbf{w} \in\mathbb{R}^{d}}L_{\mathsf{P}}(\mathbf{w})}_{L_{\mathsf{P}}(\mathbf{w}_{ \mathsf{p}}^{\star})}+\mathcal{O}\bigg{(}R^{2}\delta+\frac{d\sigma^{2}}{N} \bigg{)}.\] (24)
2. _We have_ \(\mathsf{P}(E_{\mathrm{cov}}\cap E_{w})\geq 1-\delta/10\)_, where_ \[E_{\mathrm{cov}}=E_{\mathrm{cov}}(\mathcal{D}):=\bigg{\{}\frac{1}{2} \mathbf{I}_{d}\preceq\mathbf{\Sigma}_{\mathsf{p}}^{-1/2}\widehat{\mathbf{ \Sigma}}\mathbf{\Sigma}_{\mathsf{p}}^{-1/2}\preceq 2\mathbf{I}_{d}\bigg{\}},\] (25) \[E_{w}=E_{w}(\mathcal{D}):=\Bigg{\{}\|\widehat{\mathbf{w}}_{ \mathrm{LS}}\|_{2}\leq B_{w}^{\star}+\sqrt{\frac{80d\sigma^{2}}{\delta N \lambda_{\min}}}\Bigg{\}}.\] (26)

Proof.: We first show \(\mathsf{P}(E_{\mathrm{cov}})\geq 1-\delta/20\). Let \(\widehat{\mathbf{\Sigma}}:=\frac{1}{N}\sum_{i=1}^{N}\mathbf{x}_{i}\mathbf{x} _{i}^{\top}\), and let the whitened covariance and noise variables be denoted as

\[\widetilde{\mathbf{x}}_{i}=\mathbf{\Sigma}_{\mathsf{p}}^{-1/2} \mathbf{x}_{i},\quad\widehat{\mathbf{\Sigma}}:=\frac{1}{N}\sum_{i=1}^{N} \widetilde{\mathbf{x}}_{i}\widetilde{\mathbf{x}}_{i}^{\top}=\mathbf{\Sigma}_ {\mathsf{p}}^{-1/2}\widehat{\mathbf{\Sigma}}\mathbf{\Sigma}_{\mathsf{p}}^{-1/2}.\]

Also let \(z_{i}:=y_{i}-\langle\mathbf{x}_{i},\mathbf{w}_{\mathsf{p}}^{\star}\rangle\) denote the "noise" variables. Note that

\[E_{\mathrm{cov}}=\bigg{\{}\frac{1}{2}\mathbf{I}_{d}\preceq \widetilde{\mathbf{\Sigma}}\preceq 2\mathbf{I}_{d}\bigg{\}}\]

is exactly a covariance concentration of the whitened vectors \(\{\widetilde{\mathbf{x}}_{i}\}_{i\in[N]}\). Recall that \(\mathbb{E}[\widetilde{\mathbf{x}}_{i}\widetilde{\mathbf{x}}_{i}^{\top}]= \mathbf{I}_{d}\), and \(\widetilde{\mathbf{x}}_{i}\) are \(K^{2}\)-sub-Gaussian by assumption. Therefore, we can apply [85, Theorem 4.6.1], we have with probability at least \(1-\delta/10\) that

\[\bigg{\|}\widehat{\mathbf{\Sigma}}-\mathbf{I}_{d}\bigg{\|}_{\mathrm{op}}\leq \mathcal{O}\Bigg{(}K^{2}\max\Bigg{\{}\sqrt{\frac{d+\log(1/\delta)}{N}},\frac{d +\log(1/\delta)}{N}\Bigg{\}}\Bigg{)}.\]

Setting \(N\geq\mathcal{O}(K^{4}(d+\log(1/\delta)))\) ensures that the right-hand side above is at most \(1/2\), on which event we have

\[\frac{1}{2}\mathbf{I}_{d}\preceq\widetilde{\mathbf{\Sigma}}\preceq\frac{3}{2 }\mathbf{I}_{d}\preceq 2\mathbf{I}_{d},\] (27)i.e. \(E_{\rm cov}\) holds. This shows that \({\sf P}(E_{\rm cov}^{c})\leq\delta/10\).

Next, we show (24). Using \(E_{\rm cov}\), we decompose the risk as

\[\begin{split}&\quad\mathbb{E}\bigg{[}\frac{1}{2}({\sf clip}_{R}( \langle\widehat{\mathbf{w}}_{\rm LS},\mathbf{x}_{N+1}\rangle)-y_{N+1})^{2} \bigg{]}\\ &=\mathbb{E}\bigg{[}\frac{1}{2}({\sf clip}_{R}(\langle\widehat{ \mathbf{w}}_{\rm LS},\mathbf{x}_{N+1}\rangle)-y_{N+1})^{2}1\{E_{\rm cov}\} \bigg{]}+\mathbb{E}\bigg{[}\frac{1}{2}({\sf clip}_{R}(\langle\widehat{ \mathbf{w}}_{\rm LS},\mathbf{x}_{N+1}\rangle)-y_{N+1})^{2}1\{E_{\rm cov}^{c} \}\bigg{]}\\ &\stackrel{{(i)}}{{\leq}}\mathbb{E}\bigg{[}\frac{1}{ 2}(\langle\widehat{\mathbf{w}}_{\rm LS},\mathbf{x}_{N+1}\rangle-y_{N+1})^{2}1 \{E_{\rm cov}\}\bigg{]}+2R^{2}\cdot(\delta/20)\\ &\stackrel{{(ii)}}{{=}}\mathbb{E}_{{\cal D}, \mathbf{x}_{N+1}}\bigg{[}\frac{1}{2}(\langle\widehat{\mathbf{w}}_{\rm LS}- \mathbf{w}_{\sf p}^{\star},\mathbf{x}_{N+1}\rangle)^{2}1\{E_{\rm cov}\} \bigg{]}+\mathbb{E}_{\mathbf{x}_{N+1},y_{N+1}}\bigg{[}\frac{1}{2}(\langle \mathbf{w}_{\sf p}^{\star},\mathbf{x}_{N+1}\rangle-y_{N+1})^{2}1\{E_{\rm cov} \}\bigg{]}+{\cal O}(R^{2}\delta)\\ &\leq\mathbb{E}_{{\cal D}}\bigg{[}\frac{1}{2}\left\|\widehat{ \mathbf{w}}_{\rm LS}-\mathbf{w}_{\sf p}^{\star}\right\|_{\mathbb{E}_{\sf p}}^{ 2}1\{E_{\rm cov}\}\bigg{]}+\underbrace{\mathbb{E}_{\mathbf{x}_{N+1},y_{N+1}} \bigg{[}\frac{1}{2}(\langle\mathbf{w}_{\sf p}^{\star},\mathbf{x}_{N+1}\rangle- y_{N+1})^{2}\bigg{]}}_{L_{\sf p}(\mathbf{w}_{\sf p}^{\star})}+{\cal O}(R^{2} \delta).\end{split}\] (28)

Above, (i) follows by assumption that \(|y_{N+1}|\leq B_{y}\leq R\) almost surely, so that removing the clipping can only potentially increase the distance in the first term, and the square loss is upper bounded by \(\frac{1}{2}\cdot(2R)^{2}\) almost surely in the second term; (ii) follows by the fact that \(\mathbb{E}_{\mathbf{x}_{N+1},y_{N+1}}[\langle\widehat{\mathbf{w}}_{\rm LS}- \mathbf{w}_{\sf p}^{\star},\mathbf{x}_{N+1}\rangle\,(\langle\mathbf{w}_{\sf p }^{\star},\mathbf{x}_{N+1}\rangle-y_{N+1})]=0\) by the definition of \(\mathbf{w}_{\sf p}^{\star}\), as well as the fact that \(1\{E_{\rm cov}\}\) is independent of \((\mathbf{x}_{N+1},y_{N+1})\).

It thus remains to bound \(\mathbb{E}_{{\cal D}}\Big{[}\frac{1}{2}\left\|\widehat{\mathbf{w}}_{\rm LS}- \mathbf{w}_{\sf p}^{\star}\right\|_{\mathbb{E}_{\sf p}}^{2}1\{E_{\rm cov}\} \Big{]}\). Note that on the event \(E_{\rm cov}\), we have

\[\mathbf{\Sigma}_{\sf p}^{1/2}\widehat{\mathbf{\Sigma}}^{-1}\mathbf{\Sigma}_{ \sf p}^{1/2}=\Big{(}\mathbf{\Sigma}_{\sf p}^{-1/2}\widehat{\mathbf{\Sigma}} \mathbf{\Sigma}_{\sf p}^{-1/2}\Big{)}^{-1}\preceq 2\mathbf{I}_{d}.\]

Therefore,

\[\begin{split}&\quad\frac{1}{2}\left\|\widehat{\mathbf{w}}_{\rm LS }-\mathbf{w}_{\sf p}^{\star}\right\|_{\mathbb{\Sigma}_{\sf p}}^{2}1\{E_{\rm cov }\}=\frac{1}{2}\big{(}(\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top} \mathbf{y}-\mathbf{w}_{\sf p}^{\star}\big{)}^{\top}\mathbf{\Sigma}_{\sf p}\big{(} (\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}\mathbf{y}-\mathbf{w}_{\sf p }^{\star}\big{)}1\{E_{\rm cov}\}\\ &=\frac{1}{2}\mathbf{z}^{\top}\mathbf{X}(\mathbf{X}^{\top} \mathbf{X})^{-1}\mathbf{\Sigma}_{\sf p}(\mathbf{X}^{\top}\mathbf{X})^{-1} \mathbf{X}^{\top}\mathbf{z}\cdot 1\{E_{\rm cov}\}\\ &=\frac{1}{2N^{2}}\mathbf{z}^{\top}\mathbf{X}\mathbf{\Sigma}_{\sf p }^{-1/2}\Big{(}\mathbf{\Sigma}_{\sf p}^{1/2}\widehat{\mathbf{\Sigma}}^{-1} \mathbf{\Sigma}_{\sf p}^{1/2}\Big{)}^{2}\mathbf{\Sigma}_{\sf p}^{-1/2}\mathbf{ X}^{\top}\mathbf{z}\cdot 1\{E_{\rm cov}\}\\ &\leq\frac{2}{N^{2}}\left\|\mathbf{\Sigma}_{\sf p}^{-1/2}\mathbf{ X}^{\top}\mathbf{z}\right\|_{2}^{2}1\{E_{\rm cov}\}=\frac{2}{N^{2}}\left\|\sum_{i=1}^{N} \widetilde{\mathbf{x}}_{i}z_{i}\right\|_{2}^{2}1\{E_{\rm cov}\}\leq\frac{2}{N^ {2}}\left\|\sum_{i=1}^{N}\widetilde{\mathbf{x}}_{i}z_{i}\right\|_{2}^{2}.\end{split}\]

Note that \(\mathbb{E}[\widetilde{\mathbf{x}}_{i}z_{i}]=\mathbf{\Sigma}_{\sf p}^{-1/2} \mathbb{E}[\mathbf{x}_{i}(y_{i}-\langle\mathbf{w}_{\sf p}^{\star},\mathbf{x}_{ i}\rangle)]=0\). Therefore, taking expectation on the above (over \({\cal D}\)), we get

\[\mathbb{E}_{{\cal D}}\bigg{[}\frac{1}{2}\left\|\widehat{\mathbf{w}}_{\rm LS}- \mathbf{w}_{\sf p}^{\star}\right\|_{\mathbb{\Sigma}_{\sf p}}^{2}1\{E_{\rm cov} \}\bigg{]}\leq\frac{2}{N^{2}}\mathbb{E}\Bigg{[}\left\|\sum_{i=1}^{N}\widetilde{ \mathbf{x}}_{i}z_{i}\right\|_{2}^{2}\Bigg{]}=\frac{2}{N}\mathbb{E}\Big{[}\| \widetilde{\mathbf{x}}_{1}z_{1}\|_{2}^{2}\Big{]}=\frac{2}{N}\mathbb{E}\big{[}z_{1}^{2 }\mathbf{x}_{1}^{\top}\mathbf{\Sigma}_{\sf p}^{-1}\mathbf{x}_{1}\big{]}\] (29)

\[\stackrel{{(i)}}{{\leq}}\frac{2\sigma^{2}}{N}\mathbb{E}\big{[} \mathbf{x}_{1}^{\top}\mathbf{\Sigma}_{\sf p}^{-1}\mathbf{x}_{1}\big{]}=\frac{2d \sigma^{2}}{N}.\] (30)

Above, (i) follows by conditioning on \(\mathbf{x}_{1}\) and using Assumption A(5). Combining with (28), we obtain

\[\mathbb{E}\bigg{[}\frac{1}{2}({\sf clip}_{R}(\langle\widehat{\mathbf{w}}_{\rm LS},\mathbf{x}_{N+1}\rangle)-y_{N+1})^{2}\bigg{]}\leq L_{\sf P}(\mathbf{w}_{\sf p }^{\star})+{\cal O}\bigg{(}R^{2}\delta+\frac{d\sigma^{2}}{N}\bigg{)}.\]

This proves (24).

Finally, we show \(\mathbb{P}(E_{\rm cov}\cap E_{w})\geq 1-\delta/10\). Using (29) and \(\mathbf{\Sigma}_{\sf P}\succeq\lambda_{\min}\mathbf{I}_{d}\) by assumption, we get

\[\mathbb{E}\Big{[}\|\widehat{\mathbf{w}}_{\rm LS}-\mathbf{w}_{\sf p}^{\star} \|_{2}^{2}1\{E_{\rm cov}\}\Big{]}\leq\frac{4d\sigma^{2}}{N\lambda_{\min}}.\]Therefore, using an argument similar to Chebyshev's inequality,

\[\mathsf{P}(E_{\mathrm{cov}}\cap E_{w}^{c})=\mathbb{E}\left[1\{E_{ \mathrm{cov}}\}\times 1\{\left\|\widehat{\mathbf{w}}_{\mathrm{LS}}\right\|_{2}> \sqrt{\frac{20}{\delta}\cdot\frac{4d\sigma^{2}}{N\lambda_{\min}}}+B_{w}^{*} \right]\right.\] \[\leq\mathbb{E}\Bigg{[}1\{E_{\mathrm{cov}}\}\times 1\{\left\| \widehat{\mathbf{w}}_{\mathrm{LS}}-\mathbf{w}_{\mathsf{P}}^{*}\right\|_{2}> \sqrt{\frac{20}{\delta}\cdot\frac{4d\sigma^{2}}{N\lambda_{\min}}}\}\Bigg{]}\] \[\leq\mathbb{E}\Bigg{[}1\{E_{\mathrm{cov}}\}\times\frac{\left\| \widehat{\mathbf{w}}_{\mathrm{LS}}-\mathbf{w}_{\mathsf{P}}^{*}\right\|_{2}^{2} }{\frac{20}{\delta}\cdot\frac{4d\sigma^{2}}{N\lambda_{\min}}}\Bigg{]}\leq \delta/20.\]

This implies that

\[\mathsf{P}(E_{\mathrm{cov}}\cap E_{w})=\mathsf{P}(E_{\mathrm{cov}})-\mathsf{ P}(E_{\mathrm{cov}}\cap E_{w}^{c})\geq 1-\delta/20-\delta/20\geq 1-\delta/10.\]

This is the desired result. 

### Proof of Corollary 5

The proof follows by first checking the well-conditionedness of the data \(\mathcal{D}\) (cf. (5)) with high probability, then invoking Theorem 4 (for approximation least squares) and Proposition F.1 (for the statistical power of least squares).

First, as \(\mathsf{P}\) satisfies Assumption A, by Proposition F.1, as long as \(N\geq\mathcal{O}(K^{4}(d+\log(1/\delta)))\), we have with probability at least \(1-\delta/10\) that event \(E_{\mathrm{cov}}\cap E_{w}\) holds. On this event, we have

\[\frac{1}{2}\lambda_{\min}\mathbf{I}_{d} \preceq\frac{1}{2}\mathbf{\Sigma}_{\mathsf{P}}\preceq\widehat{ \mathbf{\Sigma}}=\mathbf{X}^{\top}\mathbf{X}/N\preceq 2\mathbf{\Sigma}_{ \mathsf{P}}\preceq 2\lambda_{\max}\mathbf{I}_{d},\] \[\left\|\widehat{\mathbf{w}}_{\mathrm{LS}}\right\|_{2} \leq B_{w}/2:=\mathcal{O}\Bigg{(}B_{w}^{*}+\sqrt{\frac{d\sigma^{2} }{\delta N\lambda_{\min}}}\Bigg{)},\]

and thus the dataset \(\mathcal{D}\) is well-conditioned (in the sense of (5)) with parameters \(\alpha=\lambda_{\min}/2\), \(\beta=2\lambda_{\max}\), and \(B_{w}\) defined as above. Note that the condition number of \(\widehat{\mathbf{\Sigma}}\) is upper bounded by \(\beta/\alpha=4\lambda_{\max}/\lambda_{\min}\leq 4\kappa\), where \(\kappa\) is the upper bound on the condition number of \(\mathbf{\Sigma}_{\mathsf{P}}\) as in Assumption A(c).

Define parameters

\[\varepsilon=\sqrt{\frac{d\sigma^{2}}{N}},\quad\delta=\frac{d\sigma^{2}}{B_{y}^ {2}N}\wedge 1.\] (31)

Note that \(B_{w}\leq\mathcal{O}(B_{w}^{*}+\sqrt{B_{y}^{2}/\lambda_{\min}})\) by the above choice of \(\delta\).

We can thus apply Theorem 4 in the unregularized case (\(\lambda=0\)) to obtain that, there exists a transformer \(\boldsymbol{\theta}\) with \(\max_{\ell\in[L]}M^{(\ell)}\leq 3\), \(\left\|\boldsymbol{\theta}\right\|\leq 4R+4/\lambda_{\max}\) (with \(R=\max\left\{B_{x}B_{w},B_{y},1\right\}\)), and number of layers

\[L\leq\mathcal{O}\Bigg{(}\kappa\log\frac{B_{x}B_{w}}{\varepsilon}\Bigg{)}\leq \mathcal{O}\Bigg{(}\kappa\log\Bigg{(}B_{x}\sqrt{\frac{N}{d\sigma^{2}}}\Bigg{(} B_{w}^{*}+\frac{B_{y}^{2}}{\sqrt{\lambda_{\min}}}\Bigg{)}\Bigg{)}\Bigg{)},\]

such that on \(E_{\mathrm{cov}}\cap E_{w}\) (so that \(\mathcal{D}\) is well-conditioned), we have (choosing the clipping radius in \(\widetilde{\mathsf{ready}}_{\mathsf{y}}(\cdot)=\mathsf{clip}_{B_{y}}(\mathsf{ read}_{\mathsf{y}}(\cdot))\) to be \(B_{y}\)):

\[\left|\widetilde{\mathsf{ready}}_{\mathsf{y}}(\mathrm{TF}_{\boldsymbol{\theta}} ^{0}(\mathbf{H}))-\mathsf{clip}_{B_{y}}(\langle\widehat{\mathbf{w}}_{ \mathrm{LS}},\mathbf{x}_{N+1}\rangle)\right|\leq\left|\mathsf{read}_{\mathsf{ y}}(\mathrm{TF}_{\boldsymbol{\theta}}^{0}(\mathbf{H}))-\langle\widehat{\mathbf{w}}_{ \mathrm{LS}},\mathbf{x}_{N+1}\rangle\right|\leq\varepsilon=\sqrt{\frac{d \sigma^{2}}{N}}.\] (32)

We now bound the excess risk of the above transformer. Combining Proposition F.1 and (32), we have

\[\mathbb{E}\bigg{[}\left(\widetilde{\mathsf{ready}}_{\mathsf{y}}(\mathrm{TF}_{ \boldsymbol{\theta}}^{0}(\mathbf{H}))-y_{N+1}\right)^{2}\bigg{]}\]\[=\mathbb{E}\bigg{[}\bigg{(}\widetilde{\mathsf{read}_{y}}(\mathrm{TF}^{0}_ {\boldsymbol{\theta}}(\mathbf{H}))-y_{N+1}\bigg{)}^{2}1\{E_{\mathrm{cov}}\cap E_ {w}\}\bigg{]}+\mathbb{E}\bigg{[}\bigg{(}\widetilde{\mathsf{read}_{y}}( \mathrm{TF}^{0}_{\boldsymbol{\theta}}(\mathbf{H}))-y_{N+1}\bigg{)}^{2}1\{(E_{ \mathrm{cov}}\cap E_{w})^{c}\}\bigg{]}\] \[\leq 2\mathbb{E}\bigg{[}\bigg{(}\widetilde{\mathsf{read}_{y}}( \mathrm{TF}^{0}_{\boldsymbol{\theta}}(\mathbf{H}))-\mathsf{clip}_{B_{y}}( \langle\widehat{\mathbf{w}}_{\mathrm{LS}},\mathbf{x}_{N+1}\rangle)\bigg{)}^{2 }1\{E_{\mathrm{cov}}\cap E_{w}\}\bigg{]}\] \[\quad\quad+2\mathbb{E}\bigg{[}\bigg{(}\mathsf{clip}_{B_{y}}( \langle\widehat{\mathbf{w}}_{\mathrm{LS}},\mathbf{x}_{N+1}\rangle)-y_{N+1} \bigg{)}^{2}1\{E_{\mathrm{cov}}\cap E_{w}\}\bigg{]}+2B_{y}^{2}\cdot\delta/10\] \[\overset{(i)}{\leq}2\varepsilon^{2}+L_{\mathsf{P}}(\mathbf{w}_{ \mathsf{P}}^{\star})+\mathcal{O}\bigg{(}B_{y}^{2}\delta+\frac{d\sigma^{2}}{N} \bigg{)}+\mathcal{O}\big{(}B_{y}^{2}\delta\big{)}\] \[\leq L_{\mathsf{P}}(\mathbf{w}_{\mathsf{P}}^{\star})+\mathcal{O} \bigg{(}B_{y}^{2}\delta+\frac{d\sigma^{2}}{N}\bigg{)}\leq\mathcal{O}\bigg{(} \frac{d\sigma^{2}}{N}\bigg{)}.\]

Above, (i) uses the approximation guarantee (32) as well as Proposition F.1(a) (with clipping radius \(B_{y}\)). This proves the desired excess risk guarantee.

Finally, under the canonical choice of parameters (23), the bounds for \(L,M,\|\boldsymbol{\theta}\|\) simplify to

\[L\leq\mathcal{O}\bigg{(}\kappa\log\frac{N\kappa}{\sigma}\bigg{)},\quad\max_{ \ell\in[L]}M^{(\ell)}\leq 3,\quad\|\boldsymbol{\theta}\|\leq\mathcal{O}(\sqrt{ \kappa d}),\] (33)

and the requirement for \(N\) simplifies to \(N\geq\mathcal{O}(d+\log(1/\delta))=\widetilde{\mathcal{O}}(d)\) (as \(K=\Theta(1)\)). This proves the claim about the required \(N\) and \(L\). 

### Proof of Corollary 6

Fix parameters \(\delta,\underline{\varepsilon}>0\) to be specified later and a large universal constant \(C_{0}\). Let us set

\[\alpha=\max\Big{\{}0,1/2-\sqrt{d/N}\Big{\}}^{2},\qquad\beta=25,\] \[B_{w}^{\star}:=1+2\sqrt{\frac{\log(4/\delta)}{d}},\qquad B_{w}=C _{0}(B_{w}^{\star}+\sigma),\] \[B_{x}=C_{0}\sqrt{d\log(N/\delta)},\qquad B_{y}=C_{0}(B_{w}^{\star }+\sigma)\sqrt{\log(N/\delta)}.\]

Consider the following good events (below \(\boldsymbol{\varepsilon}=[\varepsilon_{i}]_{i\in[N]}\in\mathbb{R}^{N}\) is given by \(\varepsilon_{i}=y_{i}-\langle\mathbf{w}_{\star},\mathbf{x}_{i}\rangle\))

\[\mathcal{E}_{\pi} =\Big{\{}\|\mathbf{w}_{\star}\|_{2}\leq B_{w}^{\star},\| \boldsymbol{\varepsilon}\|_{2}\leq 2\sqrt{N}\sigma\Big{\}},\] \[\mathcal{E}_{w} =\big{\{}\alpha\leq\lambda_{\min}(\mathbf{X}^{\top}\mathbf{X}/N) \leq\lambda_{\max}(\mathbf{X}^{\top}\mathbf{X}/N)\leq\beta\big{\}},\] \[\mathcal{E}_{b} =\{\forall i\in[N],\ \|\mathbf{x}_{i}\|_{2}\leq B_{x},\ |y_{i}| \leq B_{y}\},\] \[\mathcal{E}_{b,N+1} =\{\|\mathbf{x}_{N+1}\|_{2}\leq B_{x},\ |y_{N+1}|\leq B_{y}\},\]

and we define \(\mathcal{E}:=\mathcal{E}_{\pi}\cap\mathcal{E}_{w}\cap\mathcal{E}_{b}\cap \mathcal{E}_{b,N+1}\). Under the event \(\mathcal{E}\), the problem (ICRidge) is well-conditioned and \(\|\mathbf{w}_{\mathrm{ridge}}^{\lambda}\|\leq B_{w}/2\) (by Lemma F.1).

Therefore, Theorem 4 implies that for \(\kappa=\frac{\sigma+\lambda}{\beta+\lambda}\), there exists a \(L=\lceil 2\kappa\log(B_{w}/\underline{\varepsilon})\rceil+1\)-layer transformer \(\boldsymbol{\theta}\) with prediction \(\widehat{y}_{N+1}:=\widetilde{\mathsf{read}_{y}}(\mathrm{TF}^{0}_{\boldsymbol{ \theta}}(\mathbf{H}))\) (clipped by \(B_{y}\)), such that under the good event \(\mathcal{E}\), we have \(\widehat{y}_{N+1}=\mathsf{clip}_{B_{y}}(\langle\mathbf{x}_{N+1},\widehat{ \mathbf{w}}\rangle)\) and \(\|\widehat{\mathbf{w}}-\mathbf{w}_{\mathrm{ridge}}^{\lambda}\|\leq\underline{\varepsilon}\).

In the following, we show that \(\boldsymbol{\theta}\) is indeed the desired transformer (when \(\underline{\varepsilon}\) and \(\delta\) is suitably chosen). Notice that we have

\[\mathbb{E}(\widehat{y}_{N+1}-y_{N+1})^{2}=\mathbb{E}\big{[}1\{\mathcal{E}\}( \widehat{y}_{N+1}-y_{N+1})^{2}\big{]}+\mathbb{E}\big{[}1\{\mathcal{E}^{c}\}( \widehat{y}_{N+1}-y_{N+1})^{2}\big{]},\]

and we analyze these two parts separately.

Prediction risk under good event \(\mathcal{E}\).We first note that

\[\mathbb{E}\big{[}1\{\mathcal{E}\}(\widehat{y}_{N+1}-y_{N+1})^{2} \big{]} =\mathbb{E}\Big{[}1\{\mathcal{E}\}(\mathsf{clip}_{B_{y}}(\langle \mathbf{x}_{N+1},\widehat{\mathbf{w}}\rangle)-y_{N+1})^{2}\Big{]}\] \[\leq\mathbb{E}\big{[}1\{\mathcal{E}\}(\langle\mathbf{x}_{N+1}, \widehat{\mathbf{w}}\rangle-y_{N+1})^{2}\big{]},\]where the inequality is because \(y_{N+1}\in[-B_{y},B_{y}]\) under the good event \(\mathcal{E}\). Notice that by our construction, under the good event \(\mathcal{E}\), \(\widehat{\mathbf{w}}=\widehat{\mathbf{w}}(\mathcal{D})\) depends only on the dataset \(\mathcal{D}\)7. Therefore, we have \(\|\widehat{\mathbf{w}}(\mathcal{D})-\mathbf{w}_{\mathrm{ridge}}^{\lambda}( \mathcal{D})\|\leq\underline{\varepsilon}\) as long as the event \(\mathcal{E}_{0}:=\mathcal{E}_{\pi}\cap\mathcal{E}_{w}\cap\mathcal{E}_{b}\) holds for \((\mathbf{w}_{\star},\mathcal{D})\). Thus, under \(\mathcal{E}_{0}\),

Footnote 7: We need this, as on \(\mathcal{E}^{c}\), the transformer output at this location could in principle depend additionally on \(\mathbf{x}_{N+1}\), as (15) may not hold due to the potential unboundedness of its input. A similar fact will also appear in later proofs (for generalized linear models and Lasso).

\[\mathbb{E}\left[\,1\{\mathcal{E}\}(\langle\mathbf{x}_{N+1}, \widehat{\mathbf{w}}\rangle-y_{N+1})^{2}\right]\mathbf{w}_{\star},\mathcal{D} \big{]} =\mathbb{E}\left[\,1\{\mathcal{E}\}(\langle\mathbf{x}_{N+1}, \widehat{\mathbf{w}}(\mathcal{D})\rangle-y_{N+1})^{2}\right]\mathbf{w}_{\star },\mathcal{D}\big{]}\] \[=\mathbb{E}\left[\,(\langle\mathbf{x}_{N+1},\widehat{\mathbf{w}}( \mathcal{D})\rangle-\langle\mathbf{x}_{N+1},\mathbf{w}_{\star}\rangle)^{2} \right]\mathbf{w}_{\star},\mathcal{D}\big{]}+\sigma^{2}\] \[=\,\|\widehat{\mathbf{w}}(\mathcal{D})-\mathbf{w}_{\star}\|_{2}^ {2}+\sigma^{2},\]

and we also have

\[\|\widehat{\mathbf{w}}(\mathcal{D})-\mathbf{w}_{\star}\|_{2}^{2} \leq\,\left\|\mathbf{w}_{\mathrm{ridge}}^{\lambda}-\mathbf{w}_{ \star}\right\|_{2}^{2}+2\left\|\mathbf{w}_{\mathrm{ridge}}^{\lambda}-\mathbf{w} _{\star}\right\|_{2}\left\|\widehat{\mathbf{w}}(\mathcal{D})-\mathbf{w}_{ \mathrm{ridge}}^{\lambda}\right\|_{2}+\left\|\widehat{\mathbf{w}}(\mathcal{D})- \mathbf{w}_{\mathrm{ridge}}^{\lambda}\right\|_{2}^{2}\] \[\leq\,\left\|\mathbf{w}_{\mathrm{ridge}}^{\lambda}-\mathbf{w}_{ \star}\right\|_{2}^{2}+2\underline{\varepsilon}\left\|\mathbf{w}_{\mathrm{ ridge}}^{\lambda}-\mathbf{w}_{\star}\right\|_{2}+\underline{\varepsilon}^{2}.\]

Recall that \(2\mathsf{BayesRisk}_{\pi}=\mathbb{E}_{\mathbf{w}_{\star},\mathcal{D}}\|\mathbf{ w}_{\mathrm{ridge}}^{\lambda}-\mathbf{w}_{\star}\|_{2}^{2}+\sigma^{2}\). Note that \(2\mathsf{BayesRisk}_{\pi}\leq 1+\sigma^{2}\) by definition. Therefore, we can conclude that

\[\mathbb{E}\big{[}1\{\mathcal{E}\}(\widehat{y}_{N+1}-y_{N+1})^{2}\big{]}\leq 2 \mathsf{BayesRisk}_{\pi}+2\underline{\varepsilon}+\underline{\varepsilon}^{2}.\]

Prediction risk under bad event \(\mathcal{E}^{c}\).Notice that

\[\mathbb{E}\big{[}1\{\mathcal{E}^{c}\}(\widehat{y}_{N+1}-y_{N+1})^{2}\big{]} \leq\sqrt{\mathbb{P}(\mathcal{E}^{c})\mathbb{E}[(\widehat{y}_{N+1}-y_{N+1})^{ 4}]}.\]

We can upper bound \(\mathbb{P}(\mathcal{E}^{c})=\mathbb{P}(\mathcal{E}^{c}_{\pi}\cup\mathcal{E}^{ c}_{w}\cup\mathcal{E}^{c}_{b}\cup\mathcal{E}^{c}_{b,N+1})\) by Lemma B.1, Lemma B.2 and the sub-Gaussian tail bound:

\[\mathbb{P}(\mathcal{E}^{c}_{\pi})\leq\frac{\delta}{2}+\exp(-N/8),\qquad \mathbb{P}(\mathcal{E}^{c}_{w})\leq 2\exp(-N/8),\qquad\mathbb{P}(\mathcal{E}^{c}_{b} \cup\mathcal{E}^{c}_{b,N+1})\leq\frac{\delta}{4}.\]

Thus, as long as \(N\geq 8\log(12/\delta)\), we have \(\mathbb{P}(\mathcal{E}^{c})\leq\delta\). Further, a simple calculation yields

\[\mathbb{E}(\widehat{y}_{N+1}-y_{N+1})^{4}\leq 8\mathbb{E}\widehat{y}_{N+1}^{4}+8 \mathbb{E}y_{N+1}^{4}\leq 8B_{y}^{2}+8\mathbb{E}y_{N+1}^{4}.\]

Notice that \(y_{N+1}|\mathbf{w}_{\star}\sim\mathsf{N}(0,\|\mathbf{w}_{\star}\|_{2}^{2}+ \sigma^{2})\), hence \(\mathbb{E}y_{N+1}^{4}=3\mathbb{E}(\|\mathbf{w}_{\star}\|_{2}^{2}+\sigma^{2})^ {2}\leq 3(3+2\sigma^{2}+\sigma^{4})\leq B_{y}^{4}\). Thus, we can conclude that

\[\mathbb{E}\big{[}1\{\mathcal{E}^{c}\}(\widehat{y}_{N+1}-y_{N+1})^{2}\big{]} \leq 4\sqrt{\delta}B_{y}.\]

Choosing \(\underline{\varepsilon}\) and \(\delta\).Combining the inequalities above, we have

\[\mathbb{E}(\widehat{y}_{N+1}-y_{N+1})^{2}\leq 2\mathsf{BayesRisk}_{\pi}+\Big{[}2 \underline{\varepsilon}\sqrt{2\mathsf{BayesRisk}_{\pi}}+\underline{\varepsilon}^{2}+ 4\sqrt{\delta}B_{y}\Big{]}.\]

To ensure \(\frac{1}{2}\mathbb{E}(\widehat{y}_{N+1}-y_{N+1})^{2}\leq\mathsf{BayesRisk}_{\pi}+\varepsilon\), we only need to take \((\underline{\varepsilon},\delta)\) so that the following constraints are satisfied:

\[\underline{\varepsilon}=\frac{1}{2}\min\big{\{}\varepsilon,\sqrt{\varepsilon} \big{\}},\qquad 4\sqrt{\delta}B_{y}\leq\frac{\varepsilon}{2},\qquad N\geq 8\log(12/\delta).\]

Therefore, it suffices to take \(\delta=\frac{c_{0}}{\log^{2}(N)}\Big{(}\frac{\varepsilon^{2}}{1+\sigma^{2}} \Big{)}^{2}\) for some small constant \(c_{0}\), then as long as

\[N\geq C\log\left(\frac{\sigma^{2}+1}{\varepsilon}\right)+C.\]

our choice of \(\underline{\varepsilon}\) and \(\delta\) is feasible. Note that \(\kappa\leq\mathcal{O}\left(1+\sigma^{-2}\right)\), and hence under such choice of \((\underline{\varepsilon},\delta)\), we have \(L=O(\log(1/\varepsilon))\) and \(\left\|\boldsymbol{\theta}\right\|=\widetilde{O}\Big{(}\sqrt{d}\Big{)}\). This is the desired result. 

**Lemma F.1**.: _Under the event \(\mathcal{E}_{\pi}\cap\mathcal{E}_{w}\), we have \(\left\|\mathbf{w}_{\mathrm{ridge}}^{\lambda}\right\|_{2}\leq\mathcal{O}\left(B_{ w}^{\star}+\sigma\right)\)._

Proof of Lemma F.1.: By the definition of \(\mathbf{w}_{\mathrm{ridge}}^{\lambda}\) and recall that \(\lambda=d\sigma^{2}/N\), we have \(\mathbf{w}_{\mathrm{ridge}}^{\lambda}=(\mathbf{X}^{\top}\mathbf{X}+d\sigma^{2 }\mathbf{I}_{d})^{-1}\mathbf{X}^{\top}\mathbf{y}\).

Therefore, we only need to prove the following fact: for any \(\gamma>0\) and \(\widehat{\boldsymbol{\beta}}=(\mathbf{X}^{\top}\mathbf{X}+d\gamma\mathbf{I}_ {d})^{-1}\mathbf{X}^{\top}\mathbf{y}\), we have

\[\|\widehat{\boldsymbol{\beta}}\|_{2}\leq B_{w}^{\star}+10\sigma(1+\gamma^{-1/ 2}).\] (34)

We now prove (34). Note that we have

\[\|\widehat{\boldsymbol{\beta}}\|_{2}=\|(\mathbf{X}^{\top}\mathbf{X}+d\gamma \mathbf{I}_{d})^{-1}\mathbf{X}^{\top}(\mathbf{X}\mathbf{w}_{\star}+\boldsymbol {\varepsilon})\|_{2}\leq\left\|\mathbf{B}_{1}\right\|_{\mathrm{op}}\|\mathbf{ w}_{\star}\|_{2}+\|\mathbf{B}_{2}\|_{\mathrm{op}}\left\|\boldsymbol{\varepsilon} \right\|_{2}\]

where \(\mathbf{B}_{1}=\mathbf{X}^{\top}\mathbf{X}(\mathbf{X}^{\top}\mathbf{X}+d \gamma\mathbf{I}_{d})^{-1}\), \(\mathbf{B}_{2}=(\mathbf{X}^{\top}\mathbf{X}+d\gamma\mathbf{I}_{d})^{-1} \mathbf{X}^{\top}\). Note that \(\left\|\mathbf{B}_{1}\right\|_{\mathrm{op}}\leq 1\) clearly holds, and under \(\mathcal{E}_{\pi}\) we also have \(\left\|\boldsymbol{\varepsilon}\right\|_{2}\leq 2\sqrt{N}\sigma\). Therefore, it remains to bound the term \(\left\|\mathbf{B}_{2}\right\|_{\mathrm{op}}\).

Consider the SVD decomposition of \(\mathbf{X}=U\Sigma V\), \(\Sigma=\mathrm{diag}(\lambda_{1},\cdots,\lambda_{d})\), and \(U\in\mathbb{R}^{N\times d},V\in\mathbb{R}^{d\times d}\) are orthonormal matrices. Then \(\mathbf{B}_{2}=V^{\top}(\Sigma^{2}+d\gamma\mathbf{I}_{d})^{-1}\Sigma U^{\top}\), and hence

\[\left\|\mathbf{B}_{2}\right\|_{\mathrm{op}}=\left\|(\Sigma^{2}+d\gamma \mathbf{I}_{d})^{-1}\Sigma\right\|_{\mathrm{op}}=\max_{i}\frac{\lambda_{i}}{ \lambda_{i}^{2}+d\gamma}.\]

When \(N\leq 36d\), we directly have \(\left\|\mathbf{B}_{2}\right\|_{\mathrm{op}}\leq\frac{1}{2}(d\gamma)^{-1/2}\leq 3 (N\gamma)^{-1/2}\). Otherwise, we have \(N\geq 36d\), and then for each \(i\in[d]\), \(\lambda_{i}\geq\sqrt{\lambda_{\min}(\mathbf{X}^{\top}\mathbf{X})}\geq\sqrt{ \alpha N}\geq\sqrt{N}/3\). Hence, in this case we also have \(\left\|\mathbf{B}_{2}\right\|_{\mathrm{op}}\leq\max_{i}\lambda_{i}^{-1}\leq 3 N^{-1/2}\). Combining the both cases completes the proof of (34). 

## Appendix G In-context learning of generalized linear models

As a natural generalization of linear regression, we now show that transformers can recover learn generalized linear models (GLMs) [53] (which includes logistic regression for linear classification as an important special case), by implementing the corresponding convex risk minimization algorithm in context, and achieve near-optimal excess risk under standard statistical assumptions.

Let \(g:\mathbb{R}\rightarrow\mathbb{R}\) be a link function that is non-decreasing and \(C^{2}\)-smooth. We consider the following convex empirical risk minimization (ERM) problem

\[\mathbf{w}_{\mathrm{GLM}}:=\operatorname*{arg\,min}_{\mathbf{w}\in\mathbb{R}^ {d}}\widehat{L}_{N}(\mathbf{w}):=\frac{1}{N}\sum_{i=1}^{N}\ell(\left\langle \mathbf{x}_{i},\mathbf{w}\right\rangle,y_{i}),\] (ICGLM)

where \(\ell(t,y):=-yt+\int_{0}^{t}g(s)ds\) is the convex (integral) loss associated with \(g\). A canonical example of (ICGLM) is logistic regression, in which \(g(t)=\sigma_{\mathrm{log}}(t):=(1+e^{-t})^{-1}\) is the sigmoid function, and the resulting \(\ell(t,y)=\ell_{\mathrm{log}}(t,y)=-yt+\log(1+e^{t})\) is the logistic loss.

The following result (proof in Appendix G.1) shows that, as long as the empirical risk \(\widehat{L}_{N}\) satisfies strong convexity and bounded solution conditions (similar as in Theorem 4), transformers can approximately implement the ERM predictor \(g(\left\langle\mathbf{x}_{N+1},\mathbf{w}_{\mathrm{GLM}}\right\rangle)\), with \(\mathbf{w}_{\mathrm{GLM}}\) given by (ICGLM).

**Theorem G.1** (Implementing convex risk minimization for GLMs).: _For any \(0<\alpha<\beta\) with \(\kappa:=\frac{\beta}{\alpha}\), \(B_{w}>0,B_{x}>0\), \(\kappa_{w}:=L_{g}B_{x}^{2}/\alpha+1\) and \(\varepsilon<B_{w}/2\), there exists an attention-only transformer \(\mathrm{TF}_{\boldsymbol{\theta}}^{0}\) with_

\[L=\lceil 2\kappa\log(L_{g}B_{w}B_{x}/\varepsilon)\rceil+1,\qquad\max_{\ell\in[L]}M ^{(\ell)}\leq\widetilde{\mathcal{O}}\left(C_{g}^{2}\kappa_{w}^{2}\varepsilon^{- 2}\right),\qquad\left\|\boldsymbol{\theta}\right\|\leq\mathcal{O}\left(R+ \beta^{-1}C_{g}\right),\]

_(where \(L_{g}:=\sup_{t}|g^{\prime}(t)|\), \(R:=\max\left\{B_{x}B_{w},B_{y},1\right\}\), and \(C_{g}>0\) is a constant that depends only on \(R\) and the \(C^{2}\)-smoothness of \(g\) within \([-R,R]\)), such that the following holds. On any input data \((\mathcal{D},\mathbf{x}_{N+1})\) such that_

\[\alpha\leq\lambda_{\min}(\nabla^{2}\widehat{L}_{N}(\mathbf{w}))\leq\lambda_{ \max}(\nabla^{2}\widehat{L}_{N}(\mathbf{w}))\leq\beta\,\text{ for all }\mathbf{w}\in\mathsf{B}_{2}(B_{w}),\qquad\left\|\mathbf{w}_{\mathrm{GLM}} \right\|_{2}\leq B_{w}/2,\] (35)\(\mathrm{TF}^{0}_{\bm{\theta}}(\mathbf{H}^{(0)})\) approximately implements (ICGLM): We have \(\mathbf{h}^{(L+1)}_{N+1}:=[\mathbf{x}_{N+1};\widehat{y}_{N+1};\widehat{\mathbf{w }};1;1]\), where_

\[|\widehat{y}_{N+1}-g(\langle\mathbf{x}_{N+1},\mathbf{w}_{\mathrm{GLM}}\rangle) |\leq\varepsilon.\]

In Theorem G.1, the number of heads scales as \(\widetilde{\mathcal{O}}(1/\varepsilon^{2})\) as opposed to \(\Theta(1)\) as in ridge regression (Theorem 4), due to the fact that the gradient of the loss is in general a smooth function that can be only _approximately_ expressed as a sum-of-relus (cf. Definition D.1 & Lemma B.5) rather than exactly expressed as in the case for the square loss.

In-context prediction powerWe next show that (proof in Appendix G.2) the transformer constructed in Theorem G.1 achieves desirable statistical power if the in-context data distribution satisfies standard statistical assumptions for learning GLMs. Let \(L_{\mathsf{P}}(\mathbf{w}):=\mathbb{E}_{[\mathbf{x},y)\sim\mathsf{P}}[\ell( \langle\mathbf{w},\mathbf{x}\rangle\,,y)]\) denote the corresponding population risk for any distribution \(\mathsf{P}\) of \((\mathbf{x},y)\). When \(\mathsf{P}\) is _realizable_ by a generalized linear model of link function \(g\) and parameter \(\bm{\beta}\) in the sense that \(\mathbb{E}_{\mathsf{P}}[y|\mathbf{x}]=g(\langle\bm{\beta},\mathbf{x}\rangle)\), it is a standard result that \(\bm{\beta}\) is indeed a minimizer of \(L_{\mathsf{P}}\)[42] (see also [6, Appendix A.3]).

**Theorem G.2** (Statistical guarantee for generalized linear models).: _For any fixed set of parameters defined in Assumption B, there exists a transformer \(\bm{\theta}\) with \(L\leq\mathcal{O}\left(\log(N)\right)\) layers and \(\max_{\ell\in[L]}M^{(\ell)}\leq\widetilde{\mathcal{O}}\left(d^{3}N\right)\), such that for any distribution \(\mathsf{P}\) satisfying Assumption B with those parameters, as long as \(N\geq\mathcal{O}\left(d\right)\), that outputs \(\widehat{y}_{N+1}=\widetilde{\mathsf{read}}_{\mathsf{y}}(\mathrm{TF}_{\bm{ \theta}}(\mathbf{H}))\) and \(\widehat{\mathbf{w}}=\widetilde{\mathsf{read}}_{\mathsf{w}}(\mathrm{TF}_{\bm{ \theta}}(\mathbf{H}))\in\mathbb{R}^{d}\) (for another read-out function \(\widetilde{\mathsf{read}}_{\mathsf{w}}\)) satisfying the following._

1. \(\widehat{\mathbf{w}}\) _achieves small excess risk under the population loss, i.e. for the linear prediction_ \(\widehat{y}_{N+1}^{\mathsf{lin}}:=\langle\mathbf{x}_{N+1},\widehat{\mathbf{w }}\rangle\)_,_ \[\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})\sim\mathsf{P}}\big{[}\ell( \widehat{y}_{N+1}^{\mathsf{lin}},y_{N+1})\big{]}-\min_{\bm{\beta}}L_{\mathsf{ P}}(\bm{\beta})\leq\mathcal{O}\left(d/N\right).\] (36)
2. _(Realizable setting) If there exists a_ \(\bm{\beta}\in\mathbb{R}^{d}\) _such that under_ \(\mathsf{P}\)_,_ \(\mathbb{E}[y|\mathbf{x}]=g(\langle\bm{\beta},\mathbf{x}\rangle)\) _almost surely, then_ \[\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})\sim\mathsf{P}}\big{[}( \widehat{y}_{N+1}-y_{N+1})^{2}\big{]}\leq\mathbb{E}_{(\mathbf{x}_{N+1},y_{N+1} )\sim\mathsf{P}}\big{[}\big{(}g(\langle\bm{\beta},\mathbf{x}_{N+1}\rangle)-y_ {N+1})^{2}\big{]}+\mathcal{O}\left(d/N\right),\] (37) _or equivalently,_ \(\mathbb{E}[(\widehat{y}_{N+1}-\mathbb{E}[y_{N+1}|\mathbf{x}_{N+1}])^{2}]\leq \mathcal{O}\left(d/N\right)\)_._

Above, \(\mathcal{O}\left(\cdot\right)\) hides constants that depend polynomially on the parameters in Assumption B. Similar as in Corollary 5, the \(\mathcal{O}(d/N)\) excess risk obtained here matches the optimal (fast) rate for typical learning problems with \(d\) parameters and \(N\) samples [87].

**Assumption B** (Well-posedness for learning GLMs).: _We assume that there is some \(B_{\mu}>0\) such that for any \(t\in[-B_{\mu},B_{\mu}]\), \(g^{\prime}(t)\geq\mu_{g}>0\)._

_We also assume that for each \(i\in[N+1]\), \((\mathbf{x}_{i},y_{i})\) is independently sampled from \(\mathsf{P}\) such that the following holds._

1. _Under the law_ \((\mathbf{x},y)\sim\mathsf{P}\)_, We have_ \(\mathbf{x}\sim\mathrm{SG}(K_{x})\)_,_ \(y\sim\mathrm{SG}(K_{y})\) _and_ \(g(\langle\mathbf{w},\mathbf{x}\rangle)\sim\mathrm{SG}(K_{y})\)__\(\forall\mathbf{w}\in\mathsf{B}_{2}(B_{w})\)_._
2. _For some_ \(\mu_{x}>0\)_, it holds that_ \[\mathbb{E}[1\{|\mathbf{x}^{\top}\mathbf{w}|\leq B_{\mu}/2\}\mathbf{x}\mathbf{x}^ {\top}]\succeq\mu_{x}\mathbf{I}_{d}\quad\forall\mathbf{w}\in\mathsf{B}_{2}(B_ {w}).\]
3. _For_ \(\bm{\beta}^{\star}=\arg\min L_{\mathsf{P}}\)_, it holds_ \(\left\|\bm{\beta}^{\star}\right\|_{2}\leq B_{w}/4\)_._

Applying Theorem G.2 to logistic regression, we have the following result as a direct corollary. Below, the Gaussian input assumption is for convenience only and can be generalized to e.g. sub-Gaussian input.

**Corollary G.1** (In-context logistic regression).: _Consider any in-context data distribution \(\mathsf{P}\) satisfying_

\[\mathbf{x}\sim\mathsf{N}(0,\mathbf{I}_{d}),\qquad y\in\{0,1\},\qquad\operatorname {arg\,min}_{\bm{\beta}\in\mathbb{R}^{d}}L_{\mathsf{P}}(\bm{\beta})\in\mathsf{B}_ {2}(B_{w}^{\star}).\]

_For the link function \(g=\sigma_{\log}\) and \(B_{w}^{\star}=\mathcal{O}\left(1\right)\), we can choose \(B_{w},B_{\mu},\mu_{g},L_{g},\mu_{x},K_{x},K_{y}=\Theta\left(1\right)\) so that Assumption B holds. In that case, when \(N\geq\mathcal{O}\left(d\right)\), there exists a transformer \(\bm{\theta}\) with \(L=\mathcal{O}\left(\log(N)\right)\) layers, such that for any \(\mathsf{P}\) considered above,_1. _The estimation_ \(\widehat{\mathbf{w}}=\widehat{\mathsf{read}}_{\mathbf{w}}(\mathrm{TF}_{\bm{\theta}} (\mathbf{H}))\) _output by_ \(\bm{\theta}\) _achieves excess risk bound (_36_)._
2. _(Realizable setting) Consider the logistic in-context data distribution_ \[\mathsf{P}_{\bm{\beta}}^{\mathsf{log}}:\qquad\mathbf{x}\sim\mathsf{N}(0, \mathbf{I}_{d}),\qquad y|\mathbf{x}\sim\mathrm{Bernoulli}(g(\langle\bm{\beta}, \mathbf{x}\rangle)).\] _Then, for any distribution_ \(\mathsf{P}=\mathsf{P}_{\bm{\beta}}^{\mathsf{log}}\) _with_ \(\left\lVert\bm{\beta}\right\rVert_{2}\leq B_{w}^{*}\)_, the prediction_ \(\widehat{y}_{N+1}=\widetilde{\mathsf{read}}_{\mathbf{y}}(\mathrm{TF}_{\bm{ \theta}}(\mathbf{H}))\) _of_ \(\bm{\theta}\) _additionally achieves the square loss excess risk (_37_)._

### Proof of Theorem g.1

Let us fix parameters \(\varepsilon_{g}>0\) and \(T>0\) (that we specify later in proof).

Define \(R=\max\{B_{x}B_{w},B_{y},1\}\) and

\[C_{g}:=\max_{i=0,1,2}\bigg{(}R^{i}\max_{s\in[-B,B]}\Big{|}g^{(i)}(s)\Big{|} \bigg{)}.\]

By Proposition B.1, \(g\) is \((\varepsilon_{g},M,R,C)\) with

\[C\leq\mathcal{O}\left(C_{g}\right),\qquad M\leq\mathcal{O}\left(C_{g}^{2} \varepsilon_{g}^{-2}\log(1+C_{g}\varepsilon_{g}^{-1})\right).\]

Therefore, we can invoke Theorem D.1 to obtain that, as long as \(2T\varepsilon_{g}\leq B_{w}\), there exists a \(T\)-layer attention-only transformer \(\bm{\theta}^{(1:T)}\) with \(M\) heads per layer, such that for any input \(\mathbf{H}\) of format (3) and satisfies (35), its last layer outputs \(\mathbf{h}_{i}^{(T)}=[\mathbf{x}_{i};y_{i}^{\prime};\widehat{\mathbf{w}}^{T}; \mathbf{0}_{D-2d-3};1;t_{i}]\), such that

\[\left\lVert\widehat{\mathbf{w}}^{T}-\mathbf{w}_{\mathrm{GD}}^{T}\right\rVert _{2}\leq\varepsilon_{g}\cdot(L\beta^{-1}B_{x}),\]

where \(\{\mathbf{w}_{\mathrm{GD}}^{\ell}\}_{\ell\in[L]}\) is the sequence of gradient descent iterates with stepsize \(\beta^{-1}\) and initialization \(\mathbf{w}_{\mathrm{GD}}^{0}=\mathbf{0}\). Notice that Proposition B.2 implies (with \(\kappa:=\beta/\alpha\))

\[\left\lVert\mathbf{w}_{\mathrm{GD}}^{T}-\mathbf{w}_{\mathrm{GLM}}\right\rVert _{2}\leq\exp(-T/(2\kappa))\left\lVert\mathbf{w}_{\mathrm{GLM}}\right\rVert_{2} \leq\exp(-T/(2\kappa))\cdot\frac{B_{w}}{2}:=\varepsilon_{o}.\]

Furthermore, we can show that (similar to the proof of Theorem D.1 (b)), there exists a single attention layer \(\bm{\theta}^{(T+1)}\) with \(M\) heads such that it outputs \(\mathbf{h}_{N+1}^{(T+1)}=[\mathbf{x}_{N+1};\widehat{y}_{N+1};\widehat{\mathbf{ w}}^{T};\mathbf{0}_{D-2d-3};1;0]\), where \(\left\lvert\widehat{y}_{N+1}-g(\langle\mathbf{x}_{N+1},\widehat{\mathbf{w}}^{T }\rangle)\right\rvert\leq\varepsilon_{g}\).

In the following, we show that for suitably chosen \((T,\varepsilon_{g})\), \(\bm{\theta}=(\bm{\theta}^{(1:T)},\bm{\theta}^{(T+1)})\) is the desired transformer. First notice that its output \(\mathbf{h}_{N+1}^{(T+1)}=[\mathbf{x}_{N+1};\widehat{y}_{N+1};\widehat{\mathbf{ w}}^{T};\mathbf{0}_{D-2d-3};1;0]\) satisfies

\[\left\lvert\widehat{y}_{N+1}-g(\langle\mathbf{x}_{N+1},\mathbf{w }_{\mathrm{GLM}}\rangle)\right\rvert \leq\left\lvert\widehat{y}_{N+1}-g(\langle\mathbf{x}_{N+1}, \widehat{\mathbf{w}}^{T}\rangle)\right\rvert+L_{g}\left\lvert\langle\mathbf{x} _{N+1},\widehat{\mathbf{w}}^{T}\rangle-\langle\mathbf{x}_{N+1},\mathbf{w}_{ \mathrm{GLM}}\rangle\right\rvert\] \[\leq\varepsilon_{g}+L_{g}B_{x}\left\lVert\widehat{\mathbf{w}}^{T}- \mathbf{w}_{\mathrm{GD}}^{T}\right\rVert_{2}+L_{g}B_{x}\left\lVert\mathbf{w}_{ \mathrm{GD}}^{T}-\mathbf{w}_{\mathrm{GLM}}\right\rVert_{2}\] \[\leq\varepsilon_{g}(1+L_{g}B_{x}\cdot T\beta^{-1}B_{x})+L_{g}B_{x }\varepsilon_{o}.\]

Therefore, for any fixed \(\varepsilon>0\), we can take

\[T=\left\lceil 2\kappa\log(L_{g}B_{x}B_{w}/\varepsilon)\right\rceil,\qquad \varepsilon_{g}=\frac{1}{2}\frac{\varepsilon}{1+T\cdot(L_{g}B_{x}^{2}\beta^{-1})},\]

so that the \(\bm{\theta}\) we construct above ensures \(\left\lvert\widehat{y}_{N+1}-g(\langle\mathbf{x}_{N+1},\mathbf{w}_{\mathrm{GLM}} \rangle)\right\rvert\leq\varepsilon\) for any input \(\mathbf{H}\) that satisfies (35). The upper bound on \(\left\lVert\bm{\theta}\right\rVert\) follows immediately from Theorem D.1. 

### Proof of Theorem g.2

We summarize some basic and useful facts about GLM in the following theorem. Its proof is presented in Appendix G.3 - G.6.

**Theorem G.3**.: _Under Assumption B, the following statements hold with universal constant \(C_{0}\) and constant \(C_{1},C_{2}\) that depend only on the parameters \((K_{x},K_{y},B_{\mu},B_{w},\mu_{x},L_{g},\mu_{g})\).__(a) As long as \(N\geq C_{1}\cdot d\), the following event happens with probability at least \(1-2e^{-N/C_{1}}\):_

\[\mathcal{E}_{w}:\qquad\frac{1}{8}\mu_{g}\mu_{x}\leq\lambda_{\min}(\nabla^{2} \widehat{L}_{N}(\mathbf{w}))\leq\lambda_{\max}(\nabla^{2}\widehat{L}_{N}( \mathbf{w}))\leq 8L_{g}K_{x}^{2},\quad\forall\mathbf{w}\in\mathsf{B}_{2}(B_{w}).\]

_(b) For any \(\delta>0\), we have with probability at least \(1-\delta\) that_

\[\varepsilon_{\mathrm{stat}}:=\sup_{\mathbf{w}\in\mathsf{B}_{2}(B_{w})}\Big{\|} \nabla_{\mathbf{w}}\widehat{L}_{N}(\mathbf{w})-\nabla_{\mathbf{w}}\mathbb{E}[ \widehat{L}_{N}(\mathbf{w})]\Big{\|}_{2}\leq C_{0}K_{x}K_{y}\max\Bigg{\{}\sqrt {\frac{d\iota+\log(1/\delta)}{N}},\frac{d\iota+\log(1/\delta)}{N}\Bigg{\}},\]

_where we denote \(\iota=\log(2+L_{g}K_{x}^{2}B_{w}/K_{y})\)._

_(c) Condition on (a) holds and \(N\geq C_{2}\cdot d\), the event \(\mathcal{E}_{r}:=\left\{\left\|\mathbf{w}_{\mathrm{GLM}}\right\|_{2}\leq B_{w} /2\right\}\) happens with probability at least \(1-e^{N/C_{2}}\)._

_(d) For any_ \(\mathbf{w}\in\mathsf{B}_{2}(B_{w})\)_, it holds that_

\[L_{p}(\mathbf{w})-L_{p}(\boldsymbol{\beta})\leq\frac{4}{\mu_{g}\mu_{x}}\Big{(} \varepsilon_{\mathrm{stat}}^{2}+\left\|\nabla\widehat{L}_{N}(\mathbf{w}) \right\|_{2}^{2}\Big{)}.\]

_(e) (Realizable setting) As long as_ \(\mathbf{w}_{\mathrm{GLM}}\in\mathsf{B}_{2}(B_{w})\)_, it holds that_

\[\mathbb{E}_{\mathbf{x}}(g(\left\langle\mathbf{x},\mathbf{w}_{\mathrm{GLM}} \right\rangle)-g(\left\langle\mathbf{x},\boldsymbol{\beta}\right\rangle))^{2} \leq\frac{L_{g}}{\mu_{x}\mu_{g}}\varepsilon_{\mathrm{stat}}^{2}.\]

Therefore, we can set

\[\alpha=\frac{\mu_{g}\mu_{x}}{8},\qquad\beta=8L_{g}K_{x}^{2},\] \[B_{x}=C_{0}K_{x}\sqrt{d\log(N/\delta)},\qquad B_{y}=C_{0}K_{y} \sqrt{\log(N/\delta)}.\]

Consider the following good events

\[\mathcal{E}_{b} =\{\forall i\in[N],\ \left\|\mathbf{x}_{i}\right\|_{2}\leq B_{x},\ \left|y_{i}\right|\leq B_{y}\},\] \[\mathcal{E}_{b,N+1} =\{\left\|\mathbf{x}_{N+1}\right\|_{2}\leq B_{x},\ \left|y_{N+1}\right|\leq B_{y}\},\] \[\mathcal{E} =\mathcal{E}_{r}\cap\mathcal{E}_{w}\cap\mathcal{E}_{b}\cap \mathcal{E}_{b,N+1}.\]

Under the event \(\mathcal{E}\) and our choice of \(\alpha,\beta\), the problem (ICGLM) is well-conditioned (i.e. (35) holds).

Theorem G.1 implies that there exists a transformer \(\boldsymbol{\theta}\) such that for any input \(\mathbf{H}\) of the form (3), \(\mathrm{TF}_{\boldsymbol{\theta}}\) outputs \(\mathbf{h}_{N+1}^{\prime}=[\mathbf{x}_{N+1};\widetilde{y}_{N+1};\widetilde{ \mathbf{w}};\mathbf{0}_{D-2d-3};1;0]\), such that the output is given by \(\widehat{y}_{N+1}=\overline{\mathsf{read}}_{\mathbf{y}}(\mathrm{TF}_{ \boldsymbol{\theta}}(\mathbf{H}))=\mathsf{clip}_{B_{y}}(\widetilde{y}_{N+1})\) and \(\widehat{\mathbf{w}}=\overline{\mathsf{read}}_{\mathbf{w}}(\mathrm{TF}_{ \boldsymbol{\theta}}(\mathbf{H})):=\mathrm{Proj}_{\mathsf{B}_{2}(B_{w})}( \widetilde{\mathbf{w}})\), and the following holds on the good event \(\mathcal{E}\):

1. \(\widetilde{y}_{N+1}=f_{\mathcal{D}}(\mathbf{x}_{N+1})\), where \(f_{\mathcal{D}}=\mathcal{A}(\mathcal{D})\) is a predictor such that \(|f_{\mathcal{D}}(\mathbf{x})-g(\left\langle\mathbf{x},\mathbf{w}_{\mathrm{GLM} }\right\rangle)|\leq\varepsilon\) for all \(\mathbf{x}\in\mathsf{B}_{2}(B_{x})\).
2. \(\widetilde{\mathbf{w}}=\widetilde{\mathbf{w}}(\mathcal{D})\in\mathsf{B}_{2}(B _{w})\) depends only on \(\mathcal{D}\) (by the proof of Theorem G.1 and Theorem D.1), such that \(\left\|\nabla\widehat{L}_{N}(\widetilde{\mathbf{w}})\right\|_{2}\leq\frac{ \beta\varepsilon}{L_{g}B_{w}}\).

In the following, we show that \(\boldsymbol{\theta}\) constructed above fulfills both (a) & (b) of Theorem G.2. The bounds on number of layers and heads and \(\left\|\boldsymbol{\theta}\right\|\) follows from plugging our choice of \(B_{x},B_{y}\) in our proof of Theorem G.1.

**Proof of Theorem G.2 (a).** Notice that under the good event \(\mathcal{E}\), we have \(\widehat{\mathbf{w}}=\widetilde{\mathbf{w}}=\widetilde{\mathbf{w}}(\mathcal{D})\) depends only on \(\mathcal{D}\). Then we have

\[\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}\big{[}\ell( \widehat{y}_{N+1}^{\mathsf{lin}},y_{N+1})\big{]}\] \[=\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}\big{[}1\{ \mathcal{E}\}\ell(\widehat{y}_{N+1}^{\mathsf{lin}},y_{N+1})\big{]}+\mathbb{E}_{( \mathcal{D},\mathbf{x}_{N+1},y_{N+1})}\big{[}1\{\mathcal{E}^{c}\}\ell( \widehat{y}_{N+1}^{\mathsf{lin}},y_{N+1})\big{]}\] \[=\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}[1\{ \mathcal{E}\}\ell(\left\langle\mathbf{x}_{N+1},\widetilde{\mathbf{w}}( \mathcal{D})\right\rangle,y_{N+1})]+\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}\big{[}1\{\mathcal{E}^{c}\}\ell(\widehat{y}_{N+1}^{\mathsf{lin}},y_{N+1 })\big{]}.\]

Thus, we can consider \(\mathcal{E}_{0}=\mathcal{E}_{r}\cap\mathcal{E}_{w}\cap\mathcal{E}_{b}\), and then

\[\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}[1\{\mathcal{E}\}\ell( \left\langle\mathbf{x}_{N+1},\widetilde{\mathbf{w}}(\mathcal{D})\right\rangle,y _{N+1})]\]\[=\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}[1\{\mathcal{E}_{ 0}\}\ell(\langle\mathbf{x}_{N+1},\widetilde{\mathbf{w}}(\mathcal{D})\rangle\,,y _{N+1})]-\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}[1\{\mathcal{E}_{ 0}-\mathcal{E}\}\ell(\langle\mathbf{x}_{N+1},\widetilde{\mathbf{w}}(\mathcal{ D})\rangle\,,y_{N+1})]\] \[=\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}[1\{\mathcal{E }_{0}\}L_{p}(\widetilde{\mathbf{w}}(\mathcal{D}))]-\mathbb{E}_{(\mathcal{D}, \mathbf{x}_{N+1},y_{N+1})}[1\{\mathcal{E}_{0}-\mathcal{E}\}\ell(\langle \mathbf{x}_{N+1},\widetilde{\mathbf{w}}(\mathcal{D})\rangle\,,y_{N+1})],\]

where the second equality follows from \(L_{p}(\widetilde{\mathbf{w}}(\mathcal{D}))=\mathbb{E}_{(\mathbf{x}_{N+1},y_{N+ 1})}[\mathcal{D}\ell(\langle\mathbf{x}_{N+1},\widetilde{\mathbf{w}}(\mathcal{ D})\rangle\,,y_{N+1}).\)

Therefore,

\[=\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}\big{[}1\{ \mathcal{E}^{c}\}\ell(\widetilde{y}_{N+1}^{\text{lin}},y_{N+1})\big{]}- \mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}[1\{\mathcal{E}_{0}- \mathcal{E}\}\ell(\langle\mathbf{x}_{N+1},\widetilde{\mathbf{w}}(\mathcal{D} )\rangle\,,y_{N+1})]\] \[\leq 2\sqrt{\mathbb{P}(\mathcal{E}^{c})\cdot\max\big{\{} \mathbb{E}\big{[}\ell(\widetilde{y}_{N+1}^{\text{lin}},y_{N+1})^{4}\big{]}, \mathbb{E}[\ell(\langle\mathbf{x}_{N+1},\widetilde{\mathbf{w}}(\mathcal{D}) \rangle\,,y_{N+1})^{4}]\big{\}}}=\mathcal{O}\left(\frac{B_{\ell}^{2}}{N^{5}} \right),\]

where the last line follows from Cauchy inequality and the fact \(\mathbb{P}(\mathcal{E}^{c})=\mathcal{O}\left(N^{-10}\right)\), and \(B_{\ell}\) is defined in Lemma G.1.

Notice that by Theorem G.3 (d), we have

\[\mathbb{E}_{\mathcal{D}}[1\{\mathcal{E}_{0}\}(L_{p}(\widetilde{\mathbf{w}})- \inf L_{p})]\leq\frac{4}{\mu_{g}\mu_{x}}\Big{(}\mathbb{E}[\varepsilon_{\text{ stat}}^{2}]+\mathbb{E}\Big{[}1\{\mathcal{E}_{0}\}\big{\|}\nabla\widehat{L}_{N}( \widetilde{\mathbf{w}})\big{\|}_{2}^{2}\Big{]}\Big{)},\]

and by Theorem G.3 (b) and taking integration over \(\delta>0\), we have

\[\mathbb{E}[\varepsilon_{\text{stat}}^{2}]\leq\mathcal{O}\left(1\right)\cdot K _{x}^{2}K_{y}^{2}\Bigg{(}\frac{d}{N}+\left(\frac{d}{N}\right)^{2}\Bigg{)}.\]

Also, we have \(\inf L_{p}=L_{p}(\boldsymbol{\beta}^{*})\leq B_{\ell}\) by Lemma G.1. Therefore, we can conclude that

\[\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})}\big{[}\ell(\widetilde{y }_{N+1}^{\text{lin}},y_{N+1})\big{]}\leq\inf L_{p}+\mathcal{O}\left(1\right) \cdot\Bigg{(}\frac{K_{x}^{2}K_{y}^{2}}{\mu_{g}\mu_{x}}\frac{d}{N}+\frac{K_{x} ^{4}}{\mu_{g}\mu_{x}B_{w}}\varepsilon^{2}+\frac{B_{\ell}^{2}}{N^{5}}\Bigg{)}.\]

Taking \(\varepsilon^{2}\leq\frac{K_{x}^{2}}{B_{w}K_{x}^{2}}\frac{d}{N}\) completes the proof. 

**Proof of Theorem G.2 (b).** Similar to the proof of Corollary 6, we have

\[\mathbb{E}(\widehat{y}_{N+1}-y_{N+1})^{2} =\mathbb{E}\big{[}1\{\mathcal{E}\}(\widehat{y}_{N+1}-y_{N+1})^{2 }\big{]}+\mathbb{E}\big{[}1\{\mathcal{E}^{c}\}(\widehat{y}_{N+1}-y_{N+1})^{2} \big{]}\] \[\leq\mathbb{E}\big{[}1\{\mathcal{E}\}(\widetilde{y}_{N+1}-y_{N+1 })^{2}\big{]}+\sqrt{\mathbb{P}(\mathcal{E}^{c})\mathbb{E}(\widehat{y}_{N+1}-y_{N +1})^{4}},\]

where the inequality follows from \(y_{N+1}\in[-B_{y},B_{y}]\) on event \(\mathcal{E}\). For the first part, we have

\[\mathbb{E}\Big{[}1\{\mathcal{E}\}(\widetilde{y}_{N+1}-y_{N+1})^{ 2}\Big{]} =\mathbb{E}\Big{[}1\{\mathcal{E}\}(f_{\mathcal{D}}(\mathbf{x}_{N+1} )-y_{N+1})^{2}\Big{]}\] \[\leq\mathbb{E}_{\mathcal{D}}\Big{[}1\{\mathcal{E}_{0}\}\cdot \mathbb{E}_{(\mathbf{x},y)\sim\mathsf{P}}\Big{[}1\{\|\mathbf{x}\|_{2}\leq B_{x} \}(f_{\mathcal{D}}(\mathbf{x})-y)^{2}\Big{]}\Big{]},\]

where we use the fact that the conditional distribution of \((\mathbf{x}_{N+1},y_{N+1})|\mathcal{D}\) agrees with \(\mathsf{P}\). Thus,

\[\mathbb{E}\Big{[}1\{\mathcal{E}\}(\widetilde{y}_{N+1}-y_{N+1})^{ 2}\Big{]}-\mathbb{E}_{(\mathbf{x},y)\sim\mathsf{P}}(g(\langle\mathbf{x}, \boldsymbol{\beta}\rangle)-y)^{2}\] \[\leq\mathbb{E}_{\mathcal{D}}\Big{[}1\{\mathcal{E}_{0}\}\cdot \mathbb{E}_{\mathbf{x}}1\{\|\mathbf{x}\|_{2}\leq B_{x}\}(f_{\mathcal{D}}( \mathbf{x})-g(\langle\mathbf{x},\boldsymbol{\beta}\rangle))^{2}\Big{]}\] \[\leq 2\mathbb{E}_{\mathcal{D}}\Big{[}1\{\mathcal{E}_{0}\}\cdot \mathbb{E}_{\mathbf{x}}1\{\|\mathbf{x}\|_{2}\leq B_{x}\}(f_{\mathcal{D}}( \mathbf{x})-g(\langle\mathbf{x},\boldsymbol{\beta}\rangle))^{2}\Big{]}\] \[\leq 2\varepsilon^{2}+\frac{2L_{g}}{\mu_{x}\mu_{g}}\mathbb{E}[ \varepsilon_{\text{stat}}^{2}]\leq 2\varepsilon^{2}+\mathcal{O}\left(1\right)\cdot\frac{L_{g}K_{x}^{2}K_{ y}^{2}}{\mu_{x}\mu_{g}}\frac{d}{N}.\]

For the second part, we know \(\mathbb{P}(\mathcal{E}^{c})=\mathcal{O}\left(N^{-10}\right)\) and

\[\mathbb{E}(\widehat{y}_{N+1}-y_{N+1})^{4}\leq 8\mathbb{E}\widehat{y}_{N+1}^{2}+8 \mathbb{E}y_{N+1}^{4}=\mathcal{O}\left(B_{y}^{4}\right).\]

In conclusion, we have

\[\mathbb{E}(\widehat{y}_{N+1}-y_{N+1})^{2}\leq\mathbb{E}(y_{N+1}-g(\langle\mathbf{x }_{N+1},\boldsymbol{\beta}\rangle))^{2}+2\varepsilon^{2}+\mathcal{O}\left(1 \right)\cdot\frac{L_{g}K_{x}^{2}K_{y}^{2}}{\mu_{x}\mu_{g}}\frac{d}{N}+\mathcal{O} \left(\frac{B_{y}^{2}}{N^{5}}\right).\]

Taking \(\varepsilon^{2}\leq\frac{L_{g}K_{x}^{2}K_{y}^{2}}{\mu_{x}\mu_{g}}\frac{d}{N}\) completes the proof.

**Lemma G.1**.: _Suppose that \(\mathbf{x}\sim\mathrm{SG}(K_{x})\), \(y\sim\mathrm{SG}(K_{y})\), and \(\mathbf{w}\) is a (possibly random) vector such that \(\left\|\mathbf{w}\right\|_{2}\leq B_{w}\). Then_

\[\mathbb{E}\big{[}\ell(\left\langle\mathbf{x},\mathbf{w}\right\rangle,y)^{4} \big{]}^{1/4}\leq\mathcal{O}\left(L_{g}K_{x}^{2}B_{w}^{2}d+K_{x}K_{y}B_{w}d \right)=:B_{\ell}.\]

Proof.: Notice that by our assumption, \(|g(0)|\leq 2K_{y}\). Therefore, by the definition of \(\ell\),

\[|\ell(t,y)|=\left|-yt+\int_{0}^{t}g(s)ds\right|\leq|t(g(0)-y)|+ \left|\int_{0}^{t}(g(s)-g(0))ds\right|\leq|t|\left(2K_{y}+|y|\right)+2L_{g}t^{ 2}.\]

The proof is then done by bounding the moment by \(\mathbb{E}\left|y\right|^{8}\leq\mathcal{O}\left(K_{y}^{8}\right)\) and \(\mathbb{E}\left|\left\langle\mathbf{x},\mathbf{w}\right\rangle\right|^{8}\leq B _{w}^{8}\mathbb{E}\left\|\mathbf{x}\right\|_{2}^{8}\leq\mathcal{O}\left(( \sqrt{d}B_{w}K_{x})^{8}\right)\), which is standard (by utilizing the tail bound of sub-Gaussian/sub-Exponential random variable). 

### Proof of Theorem G.3 (a)

We begin with the upper bound on \(\lambda_{\max}(\nabla^{2}\widehat{L}_{N}(\mathbf{w}))\). By Lemma B.3, as long as \(N\geq C_{0}\cdot d\), the following event

\[\mathcal{E}_{w,0}:\qquad\left\|\frac{1}{N}\sum_{i=1}^{N}\mathbf{x }_{i}\mathbf{x}_{i}^{\top}\right\|_{\mathrm{op}}\leq 8K^{2}.\]

happens with probability at least \(1-\exp(-N/C_{0})\). By the assumption that \(\sup|g^{\prime}|\leq L_{g}\), it is clear that when \(\mathcal{E}_{w,0}\) holds, we have \(\lambda_{\max}(\nabla^{2}\widehat{L}_{N}(\mathbf{w}))\leq 8L_{g}K_{x}^{2}\ \forall\mathbf{w}\in\mathbb{R}^{d}\).

In the following, we analyze the quantity \(\lambda_{\max}(\nabla^{2}\widehat{L}_{N}(\mathbf{w}))\). We have to invoke the following covering argument (see e.g. [85, Section 4.1.1]).

**Lemma G.2**.: _Suppose that \(\mathcal{V}\) is a \(\varepsilon\)-covering of \(\mathbb{S}^{d-1}\) with \(\varepsilon\in[0,1)\). Then the following holds:_

1. _For any_ \(d\times d\) _symmetric matrix_ \(A\)_,_ \(\left\|A\right\|_{\mathrm{op}}\leq\frac{1}{1-2\varepsilon}\max_{\mathbf{v} \in\mathcal{V}}\big{|}\mathbf{v}^{\top}A\mathbf{v}\big{|}\) _and_ \[\lambda_{\min}(A)\geq\min_{\mathbf{v}\in\mathcal{V}}\mathbf{v}^{ \top}A\mathbf{v}-2\varepsilon\left\|A\right\|_{\mathrm{op}}\]
2. _For any vector_ \(\mathbf{x}\in\mathbb{R}^{d}\)_,_ \(\left\|\mathbf{x}\right\|_{2}\leq\frac{1}{1-\varepsilon}\max_{\mathbf{v}\in \mathcal{V}}|\left\langle\mathbf{v},\mathbf{x}\right\rangle|\)_._

Notice that

\[\nabla^{2}\widehat{L}_{N}(\mathbf{w})=\frac{1}{N}\sum_{i=1}^{N}g^ {\prime}(\left\langle\mathbf{w},\mathbf{x}_{i}\right\rangle)\mathbf{x}_{i} \mathbf{x}_{i}^{\top} \succeq\frac{1}{N}\sum_{i=1}^{N}\mu_{g}\mathbb{I}(|\left\langle \mathbf{w},\mathbf{x}_{i}\right\rangle|\leq B_{\mu})\mathbf{x}_{i}\mathbf{x}_{i }^{\top}\] \[\succeq\frac{1}{N}\sum_{i=1}^{N}\mu_{g}\bigg{(}1-\frac{|\left\langle \mathbf{w},\mathbf{x}_{i}\right\rangle|}{B_{\mu}}\bigg{)}_{+}\mathbf{x}_{i} \mathbf{x}_{i}^{\top}.\]

Therefore, we can define \(h(t):=(B_{\mu}-|t|)_{+}\) (which is a \(1\)-Lipschitz function), and we have

\[\nabla^{2}\widehat{L}_{N}(\mathbf{w})\succeq\frac{\mu_{g}}{B_{ \mu}}\underbrace{\frac{1}{N}\sum_{i=1}^{N}h(\left\langle\mathbf{w},\mathbf{x} _{i}\right\rangle)\mathbf{x}_{i}\mathbf{x}_{i}^{\top}}_{=:A(\mathbf{w})}.\]

In the following, we pick a \(\varepsilon_{\mathbf{v}}\)-covering \(\mathcal{V}\) of \(\mathbb{S}^{d-1}\) such that \(|\mathcal{V}|\leq(3/\varepsilon_{\mathbf{v}})^{d}\) (we will specify \(\varepsilon_{\mathbf{v}}\) later in proof). Then for any \(\mathbf{w}\in\mathsf{B}_{2}(B_{w})\),

\[\lambda_{\min}(A(\mathbf{w}))\geq\min_{\mathbf{v}\in\mathcal{V}} \mathbf{v}^{\top}A(\mathbf{w})\mathbf{v}-2\varepsilon_{\mathbf{v}}\left\|A( \mathbf{w})\right\|_{\mathrm{op}}\]

By our definition of \(A(\mathbf{w})\), we have (for any fixed \(B_{xv}\))

\[\min_{\mathbf{v}\in\mathcal{V}}\mathbf{v}^{\top}A(\mathbf{w}) \mathbf{v}=\min_{\mathbf{v}\in\mathcal{V}}\frac{1}{N}\sum_{i=1}^{N}h(\left \langle\mathbf{w},\mathbf{x}_{i}\right\rangle)\left\langle\mathbf{v},\mathbf{x} _{i}\right\rangle^{2}\]\[\geq\min_{\mathbf{v}\in\mathcal{V}}\frac{1}{N}\sum_{i=1}^{N}h(\left\langle \mathbf{w},\mathbf{x}_{i}\right\rangle)\min\left\{\left\langle\mathbf{v}, \mathbf{x}_{i}\right\rangle^{2},B_{xv}^{2}\right\}\] \[\geq\min_{\mathbf{v}\in\mathcal{V}}\mathbb{E}[U_{\mathbf{v}}( \mathbf{w})]+\min_{\mathbf{v}\in\mathcal{V}}\left(U_{\mathbf{v}}(\mathbf{w}) -\mathbb{E}[U_{\mathbf{v}}(\mathbf{w})]\right).\]

By Lemma G.3, we can choose \(B_{xv}=K_{x}(15+\log(K_{x}^{2}/\mu_{x}))\), and then \(\mathbb{E}[U_{\mathbf{v}}(\mathbf{w})]\geq 3B_{\mu}\mu_{x}/8\). Thus, combining the inequalities above, we can take \(\varepsilon_{\mathbf{v}}=\frac{128K_{x}^{2}}{\mu_{x}}\) in the following, so that under event \(\mathcal{E}_{w,0}\),

\[\lambda_{\min}(\nabla^{2}\widehat{L}_{N}(\mathbf{w}))\geq\frac{\mu_{g}\mu_{x }}{8}+\frac{\mu_{g}}{B_{\mu}}\bigg{(}\frac{B_{\mu}\mu_{x}}{16}-\max_{\mathbf{v }\in\mathcal{V}}\left(\mathbb{E}[U_{\mathbf{v}}(\mathbf{w})]-U_{\mathbf{v}}( \mathbf{w})\right)\bigg{)}.\]

In the following, we consider the random process \(\left\{\overline{U}_{\mathbf{v}}(\mathbf{w}):=U_{\mathbf{v}}(\mathbf{w})- \mathbb{E}[U_{\mathbf{v}}(\mathbf{w})]\right\}_{\mathbf{w}}\), which is zero-mean and indexed by \(\mathbf{w}\in\mathsf{B}_{2}(B_{w})\). For any fixed \(\mathbf{v}\), consider applying Proposition B.4 to the random process \(\left\{\overline{U}_{\mathbf{v}}(\mathbf{w})\right\}_{\mathbf{w}}\). We need to verify the preconditions:

(a) With norm \(\rho(\mathbf{w},\mathbf{w}^{\prime})=\left\|\mathbf{w}-\mathbf{w}^{\prime} \right\|_{2},\log\mathcal{N}(\mathsf{B}_{\rho}(\mathbf{w},r),\delta)\leq d\log (2Ar/\delta)\) with constant \(A=2\);

(b) Let \(f(\mathbf{x};\mathbf{w}):=h(\left\langle\mathbf{w},\mathbf{x}_{i}\right\rangle )\min\left\{\left\langle\mathbf{v},\mathbf{x}_{i}\right\rangle^{2},B_{xv}^{2}\right\}\), then \(|f(\mathbf{x};\mathbf{w})|\leq B_{\mu}B_{xv}^{2}\) and hence in \(\mathrm{SG}(CB_{\mu}B_{xv}^{2})\) for any random \(\mathbf{x}\);

(c) For \(\mathbf{w},\mathbf{w}^{\prime}\in\mathcal{W}\), we have \(|h(\left\langle\mathbf{w},\mathbf{x}_{i}\right\rangle)-h(\left\langle\mathbf{ w}^{\prime},\mathbf{x}_{i}\right\rangle)|\leq|\left\langle\mathbf{w}-\mathbf{w}^{ \prime},\mathbf{x}_{i}\right\rangle|\). Hence, because \(\mathbf{x}\sim\mathrm{SG}(K_{x})\), the random variable \(h(\left\langle\mathbf{w},\mathbf{x}\right\rangle)-h(\left\langle\mathbf{w}^{ \prime},\mathbf{x}\right\rangle)\) is \(\mathrm{SG}(CK_{x}\|\mathbf{w}-\mathbf{w}^{\prime}\|_{2})\), and the random variable \(f(\mathbf{x};\mathbf{w})-f(\mathbf{x};\mathbf{w}^{\prime})\) is \(\mathrm{SG}(CK_{x}B_{xv}^{2}\|\mathbf{w}-\mathbf{w}^{\prime}\|_{2})\).

Therefore, we can apply Proposition B.4 to obtain that with probability \(1-\delta_{0}\), it holds

\[\sup_{\mathbf{w}}\big{|}\overline{U}_{\mathbf{v}}(\mathbf{w})\big{|}\leq C^{ \prime}B_{\mu}B_{xv}^{2}\Bigg{[}\sqrt{\frac{d\log(2\kappa_{g})+\log(1/\delta_ {0})}{N}}\Bigg{]},\]

where we denote \(\kappa_{g}=1+K_{x}B_{w}/B_{\mu}\). Setting \(\delta_{0}=\delta/\left|\mathcal{V}\right|\) and taking the union bound over \(\mathbf{v}\in\mathcal{V}\), we obtain that with probability at least \(1-\delta\),

\[\max_{\mathbf{v}\in\mathcal{V}}\sup_{\left\|\mathbf{w}\right\|_{2}\leq B_{w}} \big{|}\overline{U}_{\mathbf{v}}(\mathbf{w})\big{|}\leq C^{\prime}B_{\mu}B_{xv }^{2}\Bigg{[}\sqrt{\frac{d\log(8\kappa_{g}/\varepsilon_{\mathbf{v}})+\log(1/ \delta)}{N}}\Bigg{]},\]

where we use \(\log|\mathcal{V}|\leq d\log(4/\varepsilon_{\mathbf{v}})\). Therefore, we plug in the definition of \(\varepsilon_{\mathbf{v}}\) and \(B_{xv}\) to deduce that, if we set

\[C_{1}=\left(\frac{16C^{\prime}B_{xv}^{2}}{\mu_{x}}\right)^{2}\log(8\kappa_{g}/ \varepsilon_{\mathbf{v}}),\qquad\varepsilon_{\mathbf{v}}=\frac{128K_{x}^{2}}{ \mu_{x}},\qquad B_{xv}=K_{x}(15+\log(K_{x}^{2}/\mu_{x})),\]

then as long as \(N\geq C_{1}\cdot d\), it holds \(\max_{\mathbf{v}\in\mathcal{V}}\mathbb{E}[U_{\mathbf{v}}(\mathbf{w})]-U_{ \mathbf{v}}(\mathbf{w})\leq\frac{\mu_{x}B_{\mu}}{16}\) with probability at least \(1-\exp(-N/C_{1})\). This is the desired result. 

**Lemma G.3**.: _Under Assumption B, for \(B_{xv}=K_{x}(15+\log(K_{x}^{2}/\mu_{x}))\), it holds_

\[\inf_{\mathbf{w}\in\mathsf{B}_{2}(B_{w}),\mathbf{v}\in\mathbb{S}^{d-1}}\mathbb{E }[1\{|\mathbf{x}^{\top}\mathbf{w}|\leq B_{\mu}/2\}(\mathbf{x}^{\top}\mathbf{v} )^{2}1\{|\mathbf{x}^{\top}\mathbf{v}|\leq B_{xv}\}]\geq 3\mu_{x}/4.\]

Proof.: For any fixed \(\mathbf{w}\in\mathsf{B}_{2}(B_{w}),\mathbf{v}\in\mathbb{S}^{d-1}\),

\[\mathbb{E}[1\{|\mathbf{x}^{\top}\mathbf{w}|\leq B_{\mu}/2\}( \mathbf{x}^{\top}\mathbf{v})^{2}1\{|\mathbf{x}^{\top}\mathbf{v}|\leq B_{xv}\}]\] \[=\mathbb{E}[1\{|\mathbf{x}^{\top}\mathbf{w}|\leq B_{\mu}/2\}( \mathbf{x}^{\top}\mathbf{v})^{2}\}]-\mathbb{E}[1\{|\mathbf{x}^{\top}\mathbf{w}| \leq B_{\mu}/2\}(\mathbf{x}^{\top}\mathbf{v})^{2}1\{|\mathbf{x}^{\top}\mathbf{v}| >B_{xv}\}]\] \[\geq\mu_{x}-\mathbb{E}[(\mathbf{x}^{\top}\mathbf{v})^{2}1\{| \mathbf{x}^{\top}\mathbf{v}|>B_{xv}\}].\]

Because \(\mathbf{x}\sim\mathrm{SG}(K_{x})\), \(\mathbf{x}^{\top}\mathbf{v}\sim\mathrm{SG}(K_{x})\), and a simple calculation yields

\[\mathbb{E}[(\mathbf{x}^{\top}\mathbf{v})^{2}1\{|\mathbf{x}^{\top}\mathbf{v}|>tK_{x }\}]\leq 2K_{x}^{2}(t^{2}+1)\exp(-t^{2}).\]

Taking \(t=15+\log(K_{x}^{2}/\mu_{x})\) gives \(\mathbb{E}[(\mathbf{x}^{\top}\mathbf{v})^{2}1\{|\mathbf{x}^{\top}\mathbf{v}|>B_{ xv}\}]\leq\mu_{x}/4\), which completes the proof.

### Proof of Theorem G.3 (b)

Notice that

\[\nabla\widehat{L}_{N}(\mathbf{w})=\frac{1}{N}\sum_{i=1}^{N}{(g(\langle \mathbf{w},\mathbf{x}_{i}\rangle)-y_{i})\mathbf{x}_{i}}.\]

In the following, we pick a minimal \(1/2\)-covering of \(\mathbb{S}^{d-1}\) (so \(|\mathcal{V}|\leq 5^{d}\)). Then by Lemma G.2, it holds

\[\left\|\nabla\widehat{L}_{N}(\mathbf{w})-\mathbb{E}[\nabla\widehat{L}_{N}( \mathbf{w})]\right\|_{2}\leq 2\max_{\mathbf{v}\in\mathcal{V}}\left|\underbrace{ \langle\nabla\widehat{L}_{N}(\mathbf{w}),\mathbf{v}\rangle-\mathbb{E}[ \langle\nabla\widehat{L}_{N}(\mathbf{w}),\mathbf{v}\rangle]}_{=:X_{\mathbf{v} }(\mathbf{w})}\right.\]

Fix a \(\mathbf{v}\in\mathbb{S}^{d-1}\) and set \(\delta^{\prime}=\delta/|\mathcal{V}|\). We proceed to bound \(\sup_{\mathbf{w}}|X_{\mathbf{v}}(\mathbf{w})|\) by applying Proposition B.4 to the random process \(\left\{X_{\mathbf{v}}(\mathbf{w})\right\}_{\mathbf{w}}\). We need to verify the preconditions:

(a) With norm \(\rho(\mathbf{w},\mathbf{w}^{\prime})=\left\|\mathbf{w}-\mathbf{w}^{\prime} \right\|_{2}\), \(\log N(\delta;\mathsf{B}_{\rho}(r),\rho)\leq d\log(2Ar/\delta)\) with constant \(A=2\);

(b) For \(\mathbf{z}=[\mathbf{x};y]\), we let \(f(\mathbf{z};\mathbf{w}):=(g(\langle\mathbf{w},\mathbf{x}\rangle)-y)\left\langle \mathbf{x},\mathbf{v}\right\rangle\), then \(f(\mathbf{z};\mathbf{w})\sim\mathrm{SE}(CK_{x}K_{y})\) for any \(\mathbf{w}\) by our assumption on \((\mathbf{x},y)\);

(c) For \(\mathbf{w},\mathbf{w}^{\prime}\in\mathcal{W}\), we have \(|g(\langle\mathbf{w},\mathbf{x}\rangle)-g(\langle\mathbf{w}^{\prime},\mathbf{ x}\rangle)|\leq L_{g}\left|\langle\mathbf{w}-\mathbf{w}^{\prime},\mathbf{x} \rangle\right|\). Hence, because \(\mathbf{x}\sim\mathrm{SG}(K_{x})\), the random variable \(g(\langle\mathbf{w},\mathbf{x}_{i}\rangle)-g(\langle\mathbf{w}^{\prime}, \mathbf{x}_{i}\rangle)\) is sub-Gaussian in \(\mathrm{SG}(K_{x}L_{g}\|\mathbf{w}-\mathbf{w}^{\prime}\|_{2})\). Thus, \(f(\mathbf{z};\mathbf{w})-f(\mathbf{z};\mathbf{w}^{\prime})\) is sub-exponential in \(\mathrm{SE}(CK_{x}^{2}L_{g}\|\mathbf{w}-\mathbf{w}^{\prime}\|_{2})\).

Therefore, we can apply Proposition B.4 to obtain that with probability \(1-\delta_{0}\), it holds

\[\sup_{\mathbf{w}}|X_{\mathbf{v}}(\mathbf{w})|\leq C^{\prime}K_{x}K_{y}\Bigg{[} \sqrt{\frac{d\log(2\kappa_{y})+\log(1/\delta_{0})}{N}}+\frac{d\log(2\kappa_{y} )+\log(1/\delta_{0})}{N}\Bigg{]},\]

where we denote \(\kappa_{y}=1+L_{g}K_{x}^{2}B_{w}/K_{y}\). Setting \(\delta_{0}=\delta/\left|\mathcal{V}\right|\) and taking the union bound over \(\mathbf{v}\in\mathcal{V}\), we obtain that with probability at least \(1-\delta\),

\[\max_{\mathbf{v}\in\mathcal{V}}\sup_{\|\mathbf{w}\|_{2}\leq B_{w}}|X_{ \mathbf{v}}(\mathbf{w})|\leq C^{\prime}K_{x}K_{y}\Bigg{[}\sqrt{\frac{d\log(10 \kappa_{y})+\log(1/\delta)}{N}}+\frac{d\log(10\kappa_{y})+\log(1/\delta)}{N} \Bigg{]}.\]

This is the desired result. 

### Proof of Theorem G.3 (c)

In the following, we condition on (a) holds, i.e. \(\widehat{L}_{N}\) is \(\alpha\)-strongly-convex and \(\beta\) smooth over \(\mathsf{B}_{2}(B_{w})\) with \(\alpha=\mu_{x}\mu_{g}/8\) and \(\beta=8L_{g}K_{x}^{2}\). We define

\[\widetilde{\mathbf{w}}=\operatorname*{arg\,min}_{\mathbf{w}\in \mathsf{B}_{2}(B_{w})}\widehat{L}_{N}(\mathbf{w}).\]

Then by standard convex analysis, we have

\[\alpha\left\|\widetilde{\mathbf{w}}-\boldsymbol{\beta}^{\star} \right\|_{2}^{2}\leq\left\langle\nabla\widehat{L}_{N}(\widetilde{\mathbf{w}})- \nabla\widehat{L}_{N}(\boldsymbol{\beta}^{\star}),\widetilde{\mathbf{w}}- \boldsymbol{\beta}^{\star}\right\rangle\leq\left\langle-\nabla\widehat{L}_{N} (\boldsymbol{\beta}^{\star}),\widetilde{\mathbf{w}}-\boldsymbol{\beta}^{\star} \right\rangle\leq\left\|\nabla\widehat{L}_{N}(\boldsymbol{\beta}^{\star}) \right\|_{2}\left\|\widetilde{\mathbf{w}}-\boldsymbol{\beta}^{\star}\right\|_{2}.\]

Notice that \(\left\|\nabla\widehat{L}_{N}(\boldsymbol{\beta}^{\star})\right\|_{2}\leq \varepsilon_{\mathrm{stat}}\), we can conclude that

\[\left\|\widetilde{\mathbf{w}}\right\|_{2}\leq\left\|\boldsymbol{\beta}^{ \star}\right\|_{2}+\frac{\varepsilon_{\mathrm{stat}}}{\alpha}.\]

Recall that we assume \(\left\|\boldsymbol{\beta}^{\star}\right\|_{2}\leq B_{w}/4\), we can then consider \(\mathcal{E}_{s}:=\{\varepsilon_{\mathrm{stat}}<\alpha B_{w}/4\}\). Once \(\mathcal{E}_{s}\) holds, our argument above yields \(\left\|\widetilde{\mathbf{w}}\right\|_{2}<B_{w}\), which implies \(\nabla\widehat{L}_{N}(\widetilde{\mathbf{w}})=0\). Therefore, \(\widetilde{\mathbf{w}}=\operatorname*{arg\,min}_{\mathbf{w}\in\mathbb{R}^{d}} \widehat{L}_{N}(\mathbf{w})\). Further, by Theorem G.3, we can set

\[C_{2}:=\max\left\{2\iota\bigg{(}\frac{32\alpha K_{x}K_{y}}{B_{w}} \bigg{)}^{2},2\iota\cdot\frac{32\alpha K_{x}K_{y}}{B_{w}}\right\},\]

so that as long as \(N\geq C_{2}d\), the event \(\mathcal{E}_{s}\) holds with probability at least \(1-\exp(-N/C_{2})\). This is the desired result.

### Proof of Theorem G.3 (d) & (e)

We first prove Theorem G.3 (d). Notice that

\[\nabla^{2}L_{p}(\mathbf{w})=\mathbb{E}\big{[}g^{\prime}\big{(}\langle\mathbf{x}, \mathbf{w}\rangle\big{)}\mathbf{x}\mathbf{x}^{\top}\big{]}\succeq\mathbb{E} \big{[}\mu_{g}\mathbb{I}(|\langle\mathbf{x},\mathbf{w}\rangle|\leq B_{\mu}) \mathbf{x}\mathbf{x}^{\top}\big{]}\succeq\mu_{g}\mu_{x}\mathbf{I}_{d},\forall \mathbf{w}\in\mathsf{B}_{2}(B_{w}).\]

Therefore, \(L_{p}\) is \((\mu_{g}\mu_{x})\)-strongly-convex over \(\mathsf{B}_{2}(B_{w})\). Therefore, because \(\boldsymbol{\beta}^{\star}\in\mathsf{B}_{2}(B_{w})\) is the global minimum of \(L_{p}\), it holds that for all \(\mathbf{w}\in\mathsf{B}_{2}(B_{w})\),

\[L_{p}(\mathbf{w})-L_{p}(\boldsymbol{\beta}^{\star})\leq\frac{1}{2\mu_{g}\mu_{ x}}\left\|\nabla L_{p}(\mathbf{w})\right\|_{2}^{2}.\]

By the definition of \(\varepsilon_{\mathrm{stat}}\), \(\|\nabla L_{p}(\mathbf{w})\|_{2}\leq\varepsilon_{\mathrm{stat}}+\|\nabla \widehat{L}_{N}(\mathbf{w})\|_{2}\), and hence the proof of Theorem G.3 (d) is completed.

We next prove Theorem G.3 (e), where we assume that \(\mathbb{E}[y|\mathbf{x}]=g(\langle\mathbf{x},\boldsymbol{\beta}\rangle)\) (which implies \(\boldsymbol{\beta}^{\star}=\boldsymbol{\beta}\) directly) and \(\mathbf{w}_{\mathrm{GLM}}\in\mathsf{B}_{2}(B_{w})\). Notice that

\[\nabla L_{p}(\mathbf{w})=\mathbb{E}\Big{[}\nabla\widehat{L}_{N}(\mathbf{w}) \Big{]}=\mathbb{E}[(g(\langle\mathbf{x},\mathbf{w}\rangle)-y)\mathbf{x}]= \mathbb{E}[(g(\langle\mathbf{x},\mathbf{w}\rangle)-g(\langle\mathbf{w}, \boldsymbol{\beta}\rangle))\mathbf{x}],\]

and hence

\[\langle\nabla L_{p}(\mathbf{w}_{\mathrm{GLM}}),\mathbf{w}_{ \mathrm{GLM}}-\boldsymbol{\beta}\rangle =\mathbb{E}[(g(\langle\mathbf{x},\mathbf{w}_{\mathrm{GLM}}\rangle )-g(\langle\mathbf{w},\boldsymbol{\beta}\rangle))\cdot(\langle\mathbf{x}, \mathbf{w}_{\mathrm{GLM}}\rangle-\langle\mathbf{w},\boldsymbol{\beta}\rangle)]\] \[\geq\frac{1}{L_{g}}\mathbb{E}\big{[}(g(\langle\mathbf{x}, \mathbf{w}_{\mathrm{GLM}}\rangle)-g(\langle\mathbf{w},\boldsymbol{\beta} \rangle))^{2}\big{]}.\]

On the other hand, by the \((\mu_{g}\mu_{x})\)-strong-convexity of \(L_{p}\) over \(\mathsf{B}_{2}(B_{w})\), it holds that

\[\langle\nabla L_{p}(\mathbf{w}_{\mathrm{GLM}}),\mathbf{w}_{ \mathrm{GLM}}-\boldsymbol{\beta}\rangle\leq\frac{1}{\mu_{g}\mu_{x}}\left\| \nabla L_{p}(\mathbf{w}_{\mathrm{GLM}})\right\|_{2}^{2}.\]

Finally, using the definition of \(\mathbf{w}_{\mathrm{GLM}}\), we have \(\nabla\widehat{L}_{N}(\mathbf{w}_{\mathrm{GLM}})=0\), and hence \(\left\|\nabla L_{p}(\mathbf{w}_{\mathrm{GLM}})\right\|_{2}\leq\varepsilon_{ \mathrm{stat}}\), which completes the proof of Theorem G.3 (e). 

## Appendix H Proofs for Section 3.2

### Proof of Theorem 7

Fix \(\lambda_{N}\geq 0\), \(\beta>0\) and \(B_{w}>0\), and consider any in-context data \(\mathcal{D}\) such that the precondition of Theorem 7 holds. Recall that

\[L_{\mathrm{lasso}}(\mathbf{w}):=\frac{1}{2N}\sum_{i=1}^{N}\left(\langle \mathbf{w},\mathbf{x}_{i}\rangle-y_{i}\right)^{2}+\lambda_{N}\left\|\mathbf{w} \right\|_{1}\]

denotes the lasso regression loss in (IClasso), so that \(\mathbf{w}_{\mathrm{lasso}}=\arg\min_{\mathbf{w}\in\mathbb{R}^{d}}L_{\mathrm{ lasso}}(\mathbf{w})\). We further write

\[\widehat{L}_{N}^{0}(\mathbf{w}):=\frac{1}{2N}\sum_{i=1}^{N}\left(\langle \mathbf{w},\mathbf{x}_{i}\rangle-y_{i}\right)^{2},\qquad\mathcal{R}(\mathbf{w} ):=\lambda_{N}\left\|\mathbf{w}\right\|_{1}.\]

Note that \(\nabla^{2}\widehat{L}_{N}^{0}(\mathbf{w})=\mathbf{X}^{\top}\mathbf{X}/N\) and thus \(\widehat{L}_{N}^{0}\) is \(\beta\)-smooth over \(\mathbb{R}^{d}\).

Consider the proximal gradient descent algorithm on the ridge loss

\[\mathbf{w}_{\mathrm{PGD}}^{t+1}=\mathbf{prox}_{\eta\mathcal{R}}\Big{(} \mathbf{w}_{\mathrm{PGD}}^{t}-\eta\nabla\widehat{L}_{N}^{0}(\mathbf{w}_{ \mathrm{PGD}}^{t})\Big{)}\]

with initialization \(\mathbf{w}_{\mathrm{PGD}}^{0}:=\mathbf{0}_{d}\), learning rate \(\eta:=\beta^{-1}\), and number of steps \(T\) to be specified later. Similar to the proof of Theorem 4, we can construct a transformer to approximate \(\mathbf{w}_{\mathrm{GD}}^{T}\). Consider \(\ell(s,t)=\frac{1}{2}(s-t)^{2}\) and \(\mathcal{R}(\mathbf{w})=\lambda_{N}\left\|\mathbf{w}\right\|_{1}\), then \(\partial_{s}\ell(s,t)\) is \((0,+\infty,2,4)\)-approximable by sum of relus (cf. Definition D.1), and \(\mathbf{prox}_{\eta\mathcal{R}}\) is \((0,+\infty,4d,4+2\eta\lambda_{N})\)-approximable by sum of relus (Proposition D.1). Therefore, we can apply Theorem D.2 with the square loss \(\ell\), regularizer \(\mathcal{R}\), learning rate \(\eta\) and accuracy parameter \(0\) to obtain that there exists a transformer \(\mathrm{TF}_{\boldsymbol{\theta}}\) with \((T+1)\)layers, number of heads \(M^{(\ell)}=2\) for all \(\ell\in[L]\), and hidden dimension \(D^{\prime}=2d\), such that the final output \(\mathbf{h}_{N+1}^{(L)}=[\mathbf{x}_{N+1};\widehat{y}_{N+1};\mathbf{w}_{\mathrm{ PGD}}^{T};s]\) with \(\widehat{y}_{N+1}=\left\langle\mathbf{w}_{\mathrm{PGD}}^{T},\mathbf{x}_{N+1}\right\rangle\). Further, the weight matrices have norm bounds \(\left\|\boldsymbol{\theta}\right\|\leq 10R+(8+2\lambda_{N})\beta^{-1}\).

By the standard convergence result for proximal gradient descent (Proposition B.3), we have for all \(t\geq 1\) that

\[L_{\mathrm{lasso}}(\mathbf{w}_{\mathrm{PGD}}^{t})-L_{\mathrm{lasso}}( \mathbf{w}_{\mathrm{lasso}})\leq\frac{\beta}{2t}\left\|\mathbf{w}_{\mathrm{ lasso}}\right\|_{2}^{2}.\]

Plugging in \(\left\|\mathbf{w}_{\mathrm{lasso}}\right\|_{2}\leq B_{w}/2\) and \(T=L-1=\left\lceil\beta B_{w}^{2}/\varepsilon\right\rceil\) finishes the proof. 

### Sharper convergence analysis of proximal gradient descent for Lasso

Collection of parametersThroughout the rest of this section, we consider fixed \(N\geq 1\), \(\lambda_{N}=\sqrt{\frac{\rho\nu\log d}{N}}\) for \(\rho\geq 0\), \(\nu\geq 0\) fixed (and to be determined), fixed \(0<\alpha\leq\beta\), and fixed \(B_{w}^{\star}>0\). We write \(\kappa:=\beta/\alpha,\kappa_{s}:=\beta(B_{w}^{\star})^{2}/\nu^{2}\), and \(\omega_{N}:=\frac{\rho}{\alpha}\frac{s\log d}{N}\).

Here we present a sharper convergence analysis on the proximal gradient descent algorithm for \(L_{\mathrm{lasso}}\) under the following well-conditionedness assumption, which will be useful for proving Theorem 8 in the sequel.

**Assumption C** (Well-conditioned property for Lasso).: _We say the (IClasso) problem is well-conditioned with sparsity \(s\) if the following conditions hold:_

1. _The_ \((\alpha,\rho)\)_-RSC condition holds:_ \[\frac{\left\|\mathbf{X}\mathbf{w}\right\|_{2}^{2}}{N}\geq\alpha\left\| \mathbf{w}\right\|_{2}^{2}-\rho\frac{\log d}{N}\left\|\mathbf{w}\right\|_{1}^ {2},\qquad\forall\mathbf{w}\in\mathbb{R}^{d}.\] (38) _Further,_ \(\lambda_{\max}(\mathbf{X}^{\top}\mathbf{X}/N)\leq\beta\)_._
2. _The data_ \((\mathbf{X},\mathbf{y})\) _is "approximately generated from a_ \(s\)_-sparse linear model": There_ exists \(a\)__\(\mathbf{w}_{\star}\in\mathbb{R}^{d}\) _such that_ \(\left\|\mathbf{w}_{\star}\right\|_{2}\leq B_{w}^{\star},\left\|\mathbf{w}_{ \star}\right\|_{0}\leq s\) _and for the residue_ \(\boldsymbol{\varepsilon}=\mathbf{y}-\mathbf{X}\mathbf{w}_{\star}\)_,_ \[\left\|\mathbf{X}^{\top}\boldsymbol{\varepsilon}\right\|_{\infty}\leq\frac{1}{ 2}N\lambda_{N}.\]
3. _It holds that_ \(N\geq 32\frac{\rho}{\alpha}\cdot s\log d\) _(i.e._ \(32\omega_{N}\leq 1\)_)._

Assumption C1 imposes the standard restricted strong convexity (RSC) condition for the feature matrix \(\mathbf{X}\in\mathbb{R}^{N\times d}\), and Assumption C2 asserts that the data is approximately generated from a sparse linear model, with a bound on the \(L_{\infty}\) norm of the error vector \(\mathbf{X}^{\top}\boldsymbol{\varepsilon}\). Assumption C is entirely deterministic in nature, and suffices to imply the following convergence result. In the proof of Theorem 8, we show that Assumption C is satisfied with high probability when data is generated from the standard sparse linear model considered therein.

**Theorem H.1** (Sharper convergence guarantee for Lasso).: _Under Assumption C, for the PGD iterates \(\left\{\mathbf{w}^{t}\right\}_{t\geq 0}\) on loss function \(\widehat{L}_{\mathrm{lasso}}\) with stepsize \(\eta=1/\beta\) and starting point \(\mathbf{w}^{0}=\mathbf{0}\), we have \(\widehat{L}_{\mathrm{lasso}}(\mathbf{w}^{T})-\widehat{L}_{\mathrm{lasso}}( \mathbf{w}_{\mathrm{lasso}})\leq\varepsilon\) for all_

\[T\geq C\bigg{[}\frac{\beta(B_{w}^{\star})^{2}}{\nu}+\kappa\log\bigg{(}C\cdot \kappa\cdot\frac{\beta(B_{w}^{\star})^{2}}{\nu}\cdot\frac{\nu}{\varepsilon} \bigg{)}+\kappa\frac{\nu\omega_{N}^{2}}{\varepsilon}\bigg{]},\]

_where \(C\) is a universal constant._

The proof can be found in Appendix H.4. Combining Theorem H.1 with the construction in Theorem 7, we directly obtain the following result as a corollary.

**Theorem H.2** (In-context Lasso with transformers with sharper convergence).: _For any \(N,d,s\geq 1\), \(0<\alpha\leq\beta\), \(\nu\geq 0\), \(\rho\geq 0\), there exists a \(L\)-layer transformer \(\mathrm{TF}_{\boldsymbol{\theta}}\) with_

\[L=\left\lceil C\big{(}\kappa_{s}+\kappa(\log(C\kappa_{s}/\varepsilon)+\nu \omega_{N}^{2}/\varepsilon)\big{)}\right\rceil,\quad\max_{\ell\in[L]}M^{(\ell)} \leq 2,\quad\max_{\ell\in[L]}D^{(\ell)}\leq 2d,\]\[\left\|\boldsymbol{\theta}\right\|\leq 3+R+(8+2\lambda_{N})\beta^{-1},\]

_such that the following holds. On any input data \((\mathcal{D},\mathbf{x}_{N+1})\) such that the (ICLasso) problem satisfies Assumption C (which implies \(\left\|\mathbf{w}_{\mathrm{lasso}}\right\|_{2}\leq B_{w}/2\) with \(B_{w}=2B_{w}^{\star}+\sqrt{\nu/\alpha}\)), \(\mathrm{TF}_{\boldsymbol{\theta}}(\mathbf{H}^{(0)})\) approximately implements (IClasso), in that it outputs \(\widehat{y}_{N+1}=\mathsf{ready}_{\mathrm{y}}(\mathrm{TF}_{\boldsymbol{\theta} }(\mathbf{H}))=\langle\mathbf{x}_{N+1},\widehat{\mathbf{w}}\rangle\) with_

\[\widehat{L}_{\mathrm{lasso}}(\widehat{\mathbf{w}})-\widehat{L}_{\mathrm{ lasso}}(\mathbf{w}_{\mathrm{lasso}})\leq\varepsilon.\]

### Basic properties for Lasso

**Lemma H.1** (Relaxed basic inequality).: _Suppose that Assumption C2 holds. Then it holds that_

\[\left\|\mathbf{w}-\mathbf{w}_{\star}\right\|_{1}\leq 4\sqrt{s}\left\| \mathbf{w}-\mathbf{w}_{\star}\right\|_{2}+\frac{2}{\lambda_{N}}\Big{(} \widehat{L}_{\mathrm{lasso}}(\mathbf{w})-\widehat{L}_{\mathrm{lasso}}( \mathbf{w}_{\star})\Big{)},\qquad\forall\mathbf{w}\in\mathbb{R}^{d}.\]

_As a corollary, \(\left\|\mathbf{w}_{\mathrm{lasso}}-\mathbf{w}_{\star}\right\|_{1}\leq 4\sqrt{s} \left\|\mathbf{w}_{\mathrm{lasso}}-\mathbf{w}_{\star}\right\|_{2}\)._

Proof.: Let us first fix any \(\mathbf{w}\in\mathbb{R}^{d}\). Denote \(\boldsymbol{\Delta}=\mathbf{w}-\mathbf{w}_{\star}\), and let \(S=\mathrm{supp}(\mathbf{w}_{\star})\) be the set of indexes of nonzero entries of \(\mathbf{w}_{\star}\). Then by definition, \(\mathbf{y}=\mathbf{X}\mathbf{w}_{\star}+\boldsymbol{\varepsilon}\) and \(\left|S\right|\leq s\), and hence

\[\left\|\mathbf{X}\mathbf{w}-\mathbf{y}\right\|_{2}^{2}-\left\| \mathbf{X}\mathbf{w}_{\star}-\mathbf{y}\right\|_{2}^{2}=\left\|\mathbf{X} \boldsymbol{\Delta}-\boldsymbol{\varepsilon}\right\|_{2}^{2}-\left\| \boldsymbol{\varepsilon}\right\|_{2}^{2}=\left\|\mathbf{X}\boldsymbol{\Delta} \right\|_{2}^{2}-2\boldsymbol{\varepsilon}^{\top}\mathbf{X}\boldsymbol{\Delta},\] \[\left\|\mathbf{w}\right\|_{1}-\left\|\mathbf{w}_{\star}\right\|_{ 1}=\sum_{j\in S}(\left|\mathbf{w}[j]-\left|\mathbf{w}_{\star}[j]\right|)+\sum _{j\notin S}\left|\mathbf{w}[j]\right|\] \[\geq-\sum_{j\in S}\left|\mathbf{w}[j]-\mathbf{w}_{\star}[j] \right|+\sum_{j\notin S}\left|\mathbf{w}[j]\right|=\left\|\boldsymbol{\Delta} _{S^{c}}\right\|_{1}-\left\|\boldsymbol{\Delta}_{S}\right\|_{1}.\]

Combining these inequalities, we obtain

\[0\leq\frac{1}{2N}\left\|\mathbf{X}\boldsymbol{\Delta}\right\|_{ 2}^{2} \leq\frac{\boldsymbol{\varepsilon}^{\top}\mathbf{X}\boldsymbol{ \Delta}}{N}+\lambda_{N}(\left\|\boldsymbol{\Delta}_{S}\right\|_{1}-\left\| \boldsymbol{\Delta}_{S^{c}}\right\|_{1})+\widehat{L}_{\mathrm{lasso}}( \mathbf{w})-\widehat{L}_{\mathrm{lasso}}(\mathbf{w}_{\star})\] (39) \[\leq\frac{\lambda_{N}}{2}\left\|\boldsymbol{\Delta}\right\|_{1}+ \lambda_{N}(\left\|\boldsymbol{\Delta}_{S}\right\|_{1}-\left\|\boldsymbol{ \Delta}_{S^{c}}\right\|_{1})+\widehat{L}_{\mathrm{lasso}}(\mathbf{w})- \widehat{L}_{\mathrm{lasso}}(\mathbf{w}_{\star})\] \[=\frac{\lambda_{N}}{2}(3\left\|\boldsymbol{\Delta}_{S}\right\|_{1} -\left\|\boldsymbol{\Delta}_{S^{c}}\right\|_{1})+\widehat{L}_{\mathrm{lasso}}( \mathbf{w})-\widehat{L}_{\mathrm{lasso}}(\mathbf{w}_{\star}),\]

where the second inequality follows from \(\frac{\boldsymbol{\varepsilon}^{\top}\mathbf{X}\boldsymbol{\Delta}}{N}\leq \frac{\left\|\mathbf{X}^{\top}\boldsymbol{\varepsilon}\right\|_{\infty}}{N} \left\|\boldsymbol{\Delta}\right\|_{1}\) and our assumption that \(2\frac{\left\|\mathbf{X}^{\top}\boldsymbol{\varepsilon}\right\|_{\infty}}{N} \leq\lambda_{N}\), and the last inequality is due to \(\left\|\boldsymbol{\Delta}\right\|_{1}=\left\|\boldsymbol{\Delta}_{S}\right\| _{1}+\left\|\boldsymbol{\Delta}_{S^{c}}\right\|_{1}\). Therefore, we have

\[\left\|\boldsymbol{\Delta}\right\|_{1} =\left\|\boldsymbol{\Delta}_{S}\right\|_{1}+\left\|\boldsymbol{ \Delta}_{S^{c}}\right\|_{1}\leq 4\left\|\boldsymbol{\Delta}_{S}\right\|_{1}+\frac{2}{ \lambda_{N}}\Big{(}\widehat{L}_{\mathrm{lasso}}(\mathbf{w})-\widehat{L}_{ \mathrm{lasso}}(\mathbf{w}_{\star})\Big{)}\] \[\leq 4\sqrt{s}\left\|\boldsymbol{\Delta}\right\|_{2}+\frac{2}{ \lambda_{N}}\Big{(}\widehat{L}_{\mathrm{lasso}}(\mathbf{w})-\widehat{L}_{ \mathrm{lasso}}(\mathbf{w}_{\star})\Big{)},\]

where the last inequality follows from \(\left\|\boldsymbol{\Delta}_{S}\right\|_{1}\leq\sqrt{s}\left\|\boldsymbol{ \Delta}_{S}\right\|_{2}\leq\sqrt{s}\left\|\boldsymbol{\Delta}\right\|_{2}\). This completes the proof of our main inequality. As for the corollary, we only need to use the definition that \(\widehat{L}_{\mathrm{lasso}}(\mathbf{w}_{\mathrm{lasso}})\leq\widehat{L}_{ \mathrm{lasso}}(\mathbf{w}_{\star})\). 

**Proposition H.1** (Gap to parameter estimation error).: _Suppose that Assumption C holds. Then for all \(\mathbf{w}\in\mathbb{R}^{d}\),_

\[\left\|\mathbf{w}-\mathbf{w}_{\star}\right\|_{2}^{2}\leq C\bigg{[} \frac{s\lambda_{N}^{2}}{\alpha^{2}}+\nu^{-1}\mathsf{gap}^{2}+\mathsf{gap} \bigg{]},\]

_where we write \(\mathsf{gap}:=\widehat{L}_{\mathrm{lasso}}(\mathbf{w})-\widehat{L}_{\mathrm{ lasso}}(\mathbf{w}_{\mathrm{lasso}})\), and \(C=120\) is a universal constant. In particular, we have \(\left\|\mathbf{w}_{\mathrm{lasso}}-\mathbf{w}_{\star}\right\|_{2}^{2}\leq 10 \frac{\rho\nu}{\alpha^{2}}\frac{s\log d}{N}\)._Proof.: We follow the notation in the proof of Lemma H.1. By (39), we have

\[0\leq\frac{1}{2N}\left\|\mathbf{X}\bm{\Delta}\right\|_{2}^{2}\leq \frac{\lambda_{N}}{2}(3\left\|\bm{\Delta}_{S}\right\|_{1}-\left\|\bm{\Delta}_{S^ {c}}\right\|_{1})+\widehat{L}_{\mathrm{lasso}}(\mathbf{w})-\widehat{L}_{ \mathrm{lasso}}(\mathbf{w}_{\star}),\]

and hence \(\left\|\bm{\Delta}\right\|_{1}\leq 4\sqrt{s}\left\|\bm{\Delta}\right\|_{2}+ \frac{2\mathsf{gap}}{\lambda_{N}}\) due to \(\widehat{L}_{\mathrm{lasso}}(\mathbf{w})-\widehat{L}_{\mathrm{lasso}}( \mathbf{w}_{\star})\leq\mathsf{gap}\). On the other hand, by the RSC condition (38), it holds that

\[\frac{\left\|\mathbf{X}\bm{\Delta}\right\|_{2}^{2}}{N}\geq\alpha \left\|\bm{\Delta}\right\|_{2}^{2}-\rho\frac{\log d}{N}\left\|\bm{\Delta} \right\|_{1}^{2}.\]

Therefore, we have

\[\alpha\left\|\bm{\Delta}\right\|_{2}^{2} \leq 3\lambda_{N}\sqrt{s}\left\|\bm{\Delta}\right\|_{2}+\rho \frac{\log d}{N}\left\|\bm{\Delta}\right\|_{1}^{2}+2\mathsf{gap}\] \[\leq 3\lambda_{N}\sqrt{s}\left\|\bm{\Delta}\right\|_{2}+\rho \frac{\log d}{N}\bigg{(}4\sqrt{s}\left\|\bm{\Delta}\right\|_{2}+\frac{2\mathsf{ gap}}{\lambda_{N}}\bigg{)}^{2}+2\mathsf{gap}\] \[\leq\frac{5s\lambda_{N}^{2}}{\alpha}+\frac{\alpha}{6}\left\|\bm{ \Delta}\right\|_{2}^{2}+\rho\frac{20s\log d}{\lambda_{N}^{2}N}\left\|\bm{ \Delta}\right\|_{2}^{2}+\rho\frac{20\log d}{N}\mathsf{gap}^{2}+2\mathsf{gap},\]

where the last inequality uses AM-GM inequality and Cauchy inequality. Notice that \(\rho\frac{20s\log d}{N}\leq\frac{2}{3}\alpha\), we now derive that

\[\left\|\bm{\Delta}\right\|_{2}^{2}\leq\frac{30s\lambda_{N}^{2}}{ \alpha^{2}}+\rho\frac{120\log d}{\lambda_{N}^{2}N}\mathsf{gap}^{2}+12\mathsf{ gap}.\]

Plugging in \(\lambda_{N}=\sqrt{\frac{\rho\nu\log d}{N}}\) completes the proof. The corollary follows immediately by letting \(\mathbf{w}=\mathbf{w}_{\mathrm{lasso}}\) in above proof (hence \(\mathsf{gap}=0\)). 

**Lemma H.2** (Growth).: _It holds that_

\[\frac{1}{2N}\left\|\mathbf{X}(\mathbf{w}-\mathbf{w}_{\mathrm{lasso}})\right\| _{2}^{2}\leq\widehat{L}_{\mathrm{lasso}}(\mathbf{w})-\widehat{L}_{\mathrm{lasso }}(\mathbf{w}_{\mathrm{lasso}}),\qquad\forall\mathbf{w}.\]

Proof.: For simplicity we denote \(\mathbf{w}_{\mathrm{lasso}}:=\mathbf{w}_{\mathrm{lasso}}\). By the first order optimality condition, it holds that

\[0\in\frac{1}{N}\mathbf{X}^{\top}(\mathbf{X}\mathbf{w}_{\mathrm{lasso}}- \mathbf{y})+\partial R(\mathbf{w}_{\mathrm{lasso}}),\]

where we write \(R(\mathbf{w}):=\lambda_{N}\left\|\mathbf{w}\right\|_{1}\). Then by the convexity of \(R\), we have

\[R(\mathbf{w})-R(\mathbf{w}_{\mathrm{lasso}}) \geq\] \[= -\frac{1}{N}\left\langle\mathbf{X}\mathbf{w}_{\mathrm{lasso}}- \mathbf{y},(\mathbf{X}\mathbf{w}-\mathbf{y})-(\mathbf{X}\mathbf{w}_{\mathrm{ lasso}}-\mathbf{y})\right\rangle\] \[= -\frac{1}{2N}\left\|\mathbf{X}\mathbf{w}-\mathbf{y}\right\|_{2}^{2 }+\frac{1}{2N}\left\|\mathbf{X}\mathbf{w}_{\mathrm{lasso}}-\mathbf{y}\right\|_ {2}^{2}+\frac{1}{2N}\left\|\mathbf{X}(\mathbf{w}-\mathbf{w}_{\mathrm{lasso}}) \right\|_{2}^{2}.\]

Rearranging completes the proof. 

### Proof of Theorem H.1

For the simplicity of presentation, we write \(\mathbf{w}_{\mathrm{lasso}}=\mathbf{w}_{\mathrm{lasso}}\) and we denote \(\mathsf{gap}^{t}:=\widehat{L}_{\mathrm{lasso}}(\mathbf{w}^{t})-\widehat{L}_ {\mathrm{lasso}}(\mathbf{w}_{\mathrm{lasso}})\).

By Lemma H.1, we have \(\left\|\mathbf{w}^{t}-\mathbf{w}_{\star}\right\|_{1}\leq 4\sqrt{s}\left\|\mathbf{w}^{t}- \mathbf{w}_{\star}\right\|_{2}+\frac{2\mathsf{gap}^{t}}{\lambda_{N}}\), which implies

\[\left\|\mathbf{w}^{t}-\mathbf{w}_{\mathrm{lasso}}\right\|_{1} \leq\left\|\mathbf{w}^{t}-\mathbf{w}_{\star}\right\|_{1}+\left\|\mathbf{w}_{ \mathrm{lasso}}-\mathbf{w}_{\star}\right\|_{1}\leq 4\sqrt{s}\left\|\mathbf{w}^{t}- \mathbf{w}_{\mathrm{lasso}}\right\|_{2}+8\sqrt{s}\left\|\mathbf{w}_{\mathrm{lasso }}-\mathbf{w}_{\star}\right\|_{2}+\frac{2\mathsf{gap}^{t}}{\lambda_{N}}.\]We denote \(\mu_{N}=\rho^{2}\frac{\log d}{N}\). Using the assumption that \(\mathbf{X}\) is \((\alpha,\rho)\)-RSC, we obtain that

\[\frac{1}{N}\left\|X(\mathbf{w}^{t}-\mathbf{w}_{\mathrm{lasso}}) \right\|_{2}^{2} \geq\alpha\left\|\mathbf{w}^{t}-\mathbf{w}_{\mathrm{lasso}}\right\|_ {2}^{2}-\mu_{N}\left\|\mathbf{w}^{t}-\mathbf{w}_{\mathrm{lasso}}\right\|_{1}^{2}\] \[\geq\alpha\left\|\mathbf{w}^{t}-\mathbf{w}_{\mathrm{lasso}} \right\|_{2}^{2}-\mu_{N}\bigg{(}20s\left\|\mathbf{w}^{t}-\mathbf{w}_{\mathrm{lasso }}\right\|_{2}^{2}+640s\left\|\mathbf{w}_{\mathrm{lasso}}-\mathbf{w}_{\star} \right\|_{2}^{2}+\frac{40}{\lambda_{N}^{2}}(\mathsf{gap}^{t})^{2}\bigg{)}.\]

Thus, as long as \(N\geq\frac{30\rho^{2}s\log d}{\alpha}\), we have

\[\frac{\alpha}{3}\left\|\mathbf{w}^{t}-\mathbf{w}_{\mathrm{lasso} }\right\|_{2}^{2} \leq\frac{1}{N}\left\|\mathbf{X}(\mathbf{w}^{t}-\mathbf{w}_{ \mathrm{lasso}})\right\|_{2}^{2}+640s\mu_{N}\left\|\mathbf{w}_{\mathrm{lasso} }-\mathbf{w}_{\star}\right\|_{2}^{2}+\frac{40\mu_{N}}{\lambda_{N}^{2}}( \mathsf{gap}^{t})^{2}\] \[\leq 2\mathsf{gap}^{t}+40\nu^{-1}(\mathsf{gap}^{t})^{2}+640s\mu_{N }\left\|\mathbf{w}_{\mathrm{lasso}}-\mathbf{w}_{\star}\right\|_{2}^{2},\]

where the last inequality follows from Lemma H.2 and the definition of \(\lambda_{N},\mu_{N}\).

We define \(\varepsilon_{\mathrm{stat}}:=640s\mu_{N}\left\|\mathbf{w}_{\mathrm{lasso}}- \mathbf{w}_{\star}\right\|_{2}^{2}\), \(T_{0}:=10\beta\nu^{-1}\left\|\mathbf{w}_{\mathrm{lasso}}\right\|_{2}^{2}\). By Proposition B.3(3), it holds that for \(t\geq T_{0}\),

\[\mathsf{gap}^{t}\leq\frac{\beta}{2t}\left\|\mathbf{w}_{\mathrm{lasso}} \right\|_{2}^{2}\leq\frac{\beta}{2T_{0}}\left\|\mathbf{w}_{\mathrm{lasso}} \right\|_{2}^{2}=\frac{\nu}{20}.\]

Then for all \(t\geq T_{0}-1\), we have (the second \(\leq\) below uses Proposition B.3(2))

\[\frac{\alpha}{3}\left\|\mathbf{w}^{t+1}-\mathbf{w}_{\mathrm{lasso }}\right\|_{2}^{2}\leq 4\mathsf{gap}^{t+1}+\varepsilon_{\mathrm{stat}}\leq 2\beta \Big{(}\left\|\mathbf{w}^{t}-\mathbf{w}_{\mathrm{lasso}}\right\|_{2}^{2}- \left\|\mathbf{w}^{t+1}-\mathbf{w}_{\mathrm{lasso}}\right\|_{2}^{2}\Big{)}+ \varepsilon_{\mathrm{stat}},\] \[\Rightarrow \left\|\mathbf{w}^{t+1}-\mathbf{w}_{\mathrm{lasso}}\right\|_{2}^ {2}-\frac{3\varepsilon_{\mathrm{stat}}}{\alpha}\leq\bigg{(}1+\frac{\alpha}{6 \beta}\bigg{)}^{-1}\bigg{(}\left\|\mathbf{w}^{t}-\mathbf{w}_{\mathrm{lasso}} \right\|_{2}^{2}-\frac{3\varepsilon_{\mathrm{stat}}}{\alpha}\bigg{)}.\]

Therefore, for \(t\geq T_{0}-1\),

\[\left\|\mathbf{w}^{t}-\mathbf{w}_{\mathrm{lasso}}\right\|_{2}^ {2} \leq\] \[\leq\]

where the last inequality follows from Proposition B.3(2). Further, by Proposition B.3(3), we have

\[\mathsf{gap}^{t+k}\leq\frac{\beta}{2k}\left\|\mathbf{w}^{t}-\mathbf{w}_{ \mathrm{lasso}}\right\|_{2}^{2}\leq\frac{\beta}{2k}\bigg{[}\exp\left(-\frac{ \alpha}{8\beta}(t-T_{0})\right)\left\|\mathbf{w}_{\mathrm{lasso}}\right\|_{2}^ {2}+\frac{3\varepsilon_{\mathrm{stat}}}{\alpha}\bigg{]},\quad\forall t\geq T_{0} -1,k\geq 0.\]

Hence, we can conclude that \(\mathsf{gap}^{T}\leq\varepsilon\) for all \(T\) such that

\[T\geq 10\beta\nu^{-1}\left\|\mathbf{w}_{\mathrm{lasso}}\right\|_{2}^{2}+8 \kappa\log\left(\frac{\beta\left\|\mathbf{w}_{\mathrm{lasso}}\right\|_{2}^{2} }{\varepsilon}\right)+\frac{3\kappa\varepsilon_{\mathrm{stat}}}{\varepsilon}+1.\]

Now, by Proposition H.1, it holds that \(\left\|\mathbf{w}_{\mathrm{lasso}}-\mathbf{w}_{\star}\right\|_{2}^{2}\leq 10 \frac{\rho\nu}{\alpha^{2}}\frac{s\log d}{N}\), and hence

\[\left\|\mathbf{w}_{\mathrm{lasso}}\right\|_{2}^{2}\leq 2\left\|\mathbf{w}_{ \star}\right\|_{2}^{2}+2\left\|\mathbf{w}_{\mathrm{lasso}}-\mathbf{w}_{\star} \right\|_{2}^{2}\leq 2(B_{w}^{\star})^{2}+\frac{20\rho\nu s\log d}{\alpha^{2}N}.\]

Plugging in our definition of

\[\mu_{N}=\frac{\rho\log d}{N},\qquad\varepsilon_{\mathrm{stat}}:=400s\mu_{N} \left\|\mathbf{w}_{\mathrm{lasso}}-\mathbf{w}_{\star}\right\|_{2}^{2},\qquad \omega_{N}=\frac{\rho}{\alpha}\frac{s\log d}{N}\leq 1\]

completes the proof. 

### Proof of Theorem 8

In this section, we present the proof of Theorem 8 based on Theorem H.2. We begin by recalling the following RSC property of a Gaussian random matrix [87, Theorem 7.16], a classical result in the high-dimensional statistics literature.

**Proposition H.2** (RSC for Gaussian random design).: _Suppose that \(\mathbf{X}=[\mathbf{x}_{1};\cdots;\mathbf{x}_{N}]^{\top}\in\mathbb{R}^{N\times d}\) is a random matrix with each row \(\mathbf{x}_{i}\) being i.i.d. samples from \(\mathsf{N}(0,\bm{\Sigma})\). Then there are universal constants \(c_{1}=\frac{1}{8},c_{2}=50\) such that with probability at least \(1-\frac{e^{-N/32}}{1-e^{-N/32}}\),_

\[\frac{\left\|\mathbf{X}\mathbf{w}\right\|_{2}^{2}}{N}\geq c_{1}\left\|\mathbf{ w}\right\|_{\bm{\Sigma}}^{2}-c_{2}\rho(\bm{\Sigma})\frac{\log d}{N}\left\| \mathbf{w}\right\|_{1}^{2},\qquad\forall\mathbf{w}\in\mathbb{R}^{d},\] (40)

_where \(\rho(\bm{\Sigma})=\max_{i\in[d]}\Sigma_{ii}\) is the maximum of diagonal entries of \(\bm{\Sigma}\)._

Fix a parameter \(\delta_{1}\leq\delta\) (which we will specify in proof) and a large universal constant \(C_{0}\). Let us set

\[\alpha=c_{1}=\Theta\left(1\right),\qquad\beta=8(1+(d/N)),\qquad \rho=c_{2}=\Theta\left(1\right),\] \[B_{x}=C_{0}\sqrt{d\log(N/\delta_{1})},\qquad B_{y}=C_{0}(B_{w}^{ \star}+\sigma)\sqrt{\log(N/\delta_{1})}.\]

Similar to the proof of Corollary 6 (Appendix F.4), we consider the following good events (where \(\bm{\varepsilon}=\mathbf{X}\mathbf{w}_{\star}-\mathbf{y}\))

\[\mathcal{E}_{w} =\big{\{}\lambda_{\max}(\mathbf{X}^{\top}\mathbf{X}/N)\leq\beta \text{ and }\mathbf{X}\text{ is }(\alpha,\rho)\text{-RSC}\big{\}},\] \[\mathcal{E}_{r} =\Big{\{}\big{\|}\mathbf{X}^{\top}\bm{\varepsilon}\big{\|}_{ \infty}\geq 4\sigma\sqrt{N\log(4d/\delta)}\Big{\}},\] \[\mathcal{E}_{b} =\{\forall i\in[N],\ \left\|\mathbf{x}_{i}\right\|_{2}\leq B_{x},\ \left|y_{i}\right|\leq B_{y}\},\] \[\mathcal{E}_{b,N+1} =\{\left\|\mathbf{x}_{N+1}\right\|_{2}\leq B_{x},\ \left|y_{N+1}\right|\leq B_{y}\},\]

and we define \(\mathcal{E}:=\mathcal{E}_{w}\cap\mathcal{E}_{r}\cap\mathcal{E}_{b}\cap \mathcal{E}_{b,N+1}\).

Furthermore, we choose \(\nu>0\) that correspond to the choice \(\lambda_{N}=8\sigma\sqrt{\frac{\log(4d/\delta)}{N}}\), and we also assume \(N\geq\frac{32c_{2}}{c_{1}}\cdot s\log d\). Then, Assumption C holds on the event \(\mathcal{E}\).

Therefore, we can apply Theorem H.2 with \(\varepsilon=\nu\omega_{N}\), which implies that there exists a \(L\)-layer transformer \(\bm{\theta}\) such that its prediction \(\widehat{y}_{N+1}:=\mathsf{read}_{y}(\mathrm{TF}_{\bm{\theta}}^{0}(\mathbf{H}))\), so that under the good event \(\mathcal{E}\) we have \(\widehat{y}_{N+1}=\mathsf{clip}_{B_{y}}(\langle\mathbf{x}_{N+1},\widehat{ \mathbf{w}}\rangle)\), where

\[L_{\mathrm{lasso}}(\widehat{\mathbf{w}})-L_{\mathrm{lasso}}(\mathbf{w}_{ \mathrm{lasso}})\leq\nu\omega_{N}.\]

In the following, we show that \(\bm{\theta}\) is indeed the desired transformer (similarly to the proof in Appendix F.4). Consider the conditional prediction error

\[\mathbb{E}\left[\,(\widehat{y}_{N+1}-y_{N+1})^{2}\big{|}\,\mathcal{D}\right]= \mathbb{E}\left[\,1\{\mathcal{E}\}(\widehat{y}_{N+1}-y_{N+1})^{2}\big{|}\, \mathcal{D}\right]+\mathbb{E}\left[\,1\{\mathcal{E}^{c}\}(\widehat{y}_{N+1}-y _{N+1})^{2}\big{|}\,\mathcal{D}\right],\]

and we analyze these two parts separately under the good event \(\mathcal{E}_{0}:=\mathcal{E}_{w}\cap\mathcal{E}_{r}\cap\mathcal{E}_{b}\) of \(\mathcal{D}\).

Part I.We first note that

\[\mathbb{E}\left[\,1\{\mathcal{E}\}(\widehat{y}_{N+1}-y_{N+1})^{2} \big{|}\,\mathcal{D}\right] =\mathbb{E}\left[\,1\{\mathcal{E}\}(\mathsf{clip}_{B_{y}}( \langle\mathbf{x}_{N+1},\widehat{\mathbf{w}}\rangle)-y_{N+1})^{2}\Big{|}\, \mathcal{D}\right]\] \[\leq\mathbb{E}\left[\,1\{\mathcal{E}\}(\langle\mathbf{x}_{N+1}, \widehat{\mathbf{w}}\rangle-y_{N+1})^{2}\big{|}\,\mathcal{D}\right],\]

where the inequality is because \(y_{N+1}\in[-B_{y},B_{y}]\) under the good event \(\mathcal{E}\). Notice that by our construction, under the good event \(\mathcal{E}\), \(\widehat{\mathbf{w}}=\widehat{\mathbf{w}}(\mathcal{D})\) depends only on the dataset \(\mathcal{D}\) (because it is the \((L-1)\)-th iterate of PGD on (IClasso)) problem. Applying Proposition H.1 to \(\widehat{\mathbf{w}}(\mathcal{D})\) and using the definition of \(\omega_{N}\) and our choice of \(\lambda_{N}\), we obtain that (under \(\mathcal{E}_{0}\))

\[\left\|\widehat{\mathbf{w}}(\mathcal{D})-\mathbf{w}_{\star}\right\|_{2}^{2} \leq C\cdot\left[\frac{s\lambda_{N}^{2}}{\alpha^{2}}+\nu\omega_{N}^{2}+\nu \omega_{N}\right]=\mathcal{O}\left(\frac{\sigma^{2}s\log(d/\delta)}{N}\right).\]

Therefore, under \(\mathcal{E}_{0}\),

\[\mathbb{E}\left[\,1\{\mathcal{E}\}(\langle\mathbf{x}_{N+1}, \widehat{\mathbf{w}}\rangle-y_{N+1})^{2}\big{|}\,\mathcal{D}\right] =\mathbb{E}\left[\,1\{\mathcal{E}\}(\langle\mathbf{x}_{N+1}, \widehat{\mathbf{w}}(\mathcal{D})\rangle-y_{N+1})^{2}\big{|}\,\mathcal{D}\right]\] \[\leq\mathbb{E}\left[\,(\langle\mathbf{x}_{N+1},\widehat{ \mathbf{w}}(\mathcal{D})\rangle-y_{N+1})^{2}\big{|}\,\mathcal{D}\right]\] \[=\mathbb{E}\left[\,(\langle\mathbf{x}_{N+1},\widehat{\mathbf{w}}( \mathcal{D})\rangle-\langle\mathbf{x}_{N+1},\mathbf{w}_{\star}\rangle)^{2} \big{|}\,\mathcal{D}\right]+\sigma^{2}\] \[=\,\|\widehat{\mathbf{w}}(\mathcal{D})-\mathbf{w}_{\star}\|_{2}^{2} +\sigma^{2}\] \[=\sigma^{2}\bigg{[}1+\mathcal{O}\left(\frac{s\log(d/\delta)}{N} \right)\bigg{]}.\]Part II.Notice that under good event \(\mathcal{E}_{0}\), the bad event \(\mathcal{E}^{c}\) holds if and only if \(\mathcal{E}^{c}_{b,N+1}\) holds, and hence

\[\mathbb{E}\left[\left.1\{\mathcal{E}^{c}\}(\widehat{y}_{N+1}-y_{N+ 1})^{2}\right|\mathcal{D}\right] =\mathbb{E}\left[\left.1\{\mathcal{E}^{c}_{b,N+1}\}(\widehat{y}_ {N+1}-y_{N+1})^{2}\right|\mathcal{D}\right]\] \[\leq\sqrt{\mathbb{P}(\mathcal{E}^{c}_{b,N+1})\mathbb{E}[(\widehat {y}_{N+1}-y_{N+1})^{4}]}.\]

With a large enough constant \(C_{0}\), we clearly have \(\mathbb{P}(\mathcal{E}^{c}_{b,N+1})\leq(\delta_{1}/N)^{10}\). Further, a simple calculation yields

\[\mathbb{E}(\widehat{y}_{N+1}-y_{N+1})^{4}\leq 8\mathbb{E}(\widehat{y}_{N+1}^{ 4}+y_{N+1}^{4})\leq 8B_{y}^{4}+8\mathbb{E}y_{N+1}^{4}\leq 16B_{y}^{4},\]

where the last inequality is because the marginal distribution of \(y_{N+1}\) is simply \(\mathsf{N}(0,\sigma^{2}+\|\mathbf{w}_{*}\|_{2}^{2})\). Combining these yields

\[\mathbb{E}\left[\left.1\{\mathcal{E}^{c}\}(\widehat{y}_{N+1}-y_{N+ 1})^{2}\right|\mathcal{D}\right]\leq\mathcal{O}\left(\frac{\delta_{1}^{5}B_{ y}^{2}}{N^{5}}\right)\leq\mathcal{O}\left(\frac{\delta_{1}^{5}((B_{w}^{*})^{2}+ \sigma^{2})\log(1/\delta_{1})}{N^{4}}\right).\]

Therefore, choosing \(\delta_{1}=\min\{\delta,\frac{\sigma}{B_{w}^{*}}\}\) is enough for our purpose, and under such choice of \(\delta_{1}\),

\[\mathbb{E}\left[\left.1\{\mathcal{E}^{c}\}(\widehat{y}_{N+1}-y_{N+ 1})^{2}\right|\mathcal{D}\right]\leq\mathcal{O}\left(\frac{\sigma^{2}}{N^{4}} \right).\]

Conclusion.Combining the inequalities above, we can conclude that under \(\mathcal{E}_{0}\),

\[\mathbb{E}\left[\left.(\widehat{y}_{N+1}-y_{N+1})^{2}\right| \mathcal{D}\right]\leq\sigma^{2}\bigg{[}1+\mathcal{O}\left(\frac{s\log(d/ \delta)}{N}\right)\bigg{]}.\]

It remains to show that \(\mathbb{P}(\mathcal{E}_{0})\geq 1-\delta\). By Proposition H.2, Lemma B.2 and Lemma B.4, we have

\[\mathbb{P}(\mathcal{E}_{w})\leq 3\exp(-N/32),\qquad\mathbb{P}(\mathcal{E}_{r}) \leq\frac{\delta}{2},\qquad\mathbb{P}(\mathcal{E}_{b})\leq\frac{\delta}{4}.\]

Therefore, as long as \(N\geq 32\log(12/\delta)\), we have \(\mathbb{P}(\mathcal{E}_{0})\geq 1-\delta\). This completes the proof. 

We also remark that in the construction above,

\[R=\mathcal{O}\left((B_{w}^{\star}+\sigma)\sqrt{d}\log(N\cdot(1+B_{w}^{\star}/ \sigma))\right),\]

which would be useful for bounding \(\|\boldsymbol{\theta}\|\).

## Appendix I Proofs for Section 4

### Proof of Proposition 10

We begin by restating Proposition 10 into the following version, which contains additional size bounds on \(\boldsymbol{\theta}\).

**Theorem I.1** (Full statement of Proposition 10).: _Suppose that for_

\[\widehat{L}_{\mathsf{val}}(f):=\frac{1}{|\mathcal{D}_{\mathsf{val}}|}\sum_{( \mathbf{x}_{i},y_{i})\in\mathcal{D}_{\mathsf{val}}}\ell(f(\mathbf{x}_{i}),y_{i}),\]

\(\ell(\cdot,\cdot)\) _is \((\gamma/3,R,M,C)\)-approximable by sum of relus (Definition D.1). Then there exists a 3-layer transformer \(\mathrm{TF}_{\boldsymbol{\theta}}\) with_

\[\max_{\ell\in[3]}M^{(\ell)}\leq(M+3)K,\quad\max_{\ell\in[3]}D^{( \ell)}\leq K^{2}+K+1,\quad\|\boldsymbol{\theta}\|\leq\frac{2NKC}{|\mathcal{D} _{\mathsf{val}}|}+3\gamma^{-1}+7KR.\]

_that maps_

\[\mathbf{h}_{i}=[*;f_{1}(\mathbf{x}_{i});\cdots;f_{K}(\mathbf{x}_{i}); \mathbf{0}_{K+1};1;t_{i}]\quad\rightarrow\quad\mathbf{h}_{i}^{\prime}=[*; \widehat{f}(\mathbf{x}_{i});1;t_{i}],\ i\in[N+1],\]

_where the predictor \(\widehat{f}:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is a convex combination of \(\{f_{k}:\widehat{L}_{\mathsf{val}}(f_{k})\leq\min_{k_{*}\in[K]}\widehat{L}_{ \mathsf{val}}(f_{k_{*}})+\gamma\}\). As a corollary, for any convex risk \(L:(\mathbb{R}^{d}\rightarrow\mathbb{R})\rightarrow\mathbb{R}\), \(\widehat{f}\) satisfies_

\[L(\widehat{f})\leq\min_{k_{*}\in[K]}L(f_{k_{*}})+\max_{k\in[K]}\left|\widehat{L} _{\mathsf{val}}(f_{k})-L(f_{k})\right|+\gamma.\]To prove Theorem I.1, we first state and prove the following two propositions.

**Proposition I.1** (Evaluation layer).: _There exists a 1-layer transformer \(\mathrm{TF}_{\boldsymbol{\theta}}\) with \(MK\) heads and \(\left\|\boldsymbol{\theta}\right\|\leq 3R+2NKC/\left|\mathcal{D}_{\mathsf{val}}\right|\) such that for all \(\mathbf{H}\) such that \(\max_{i}\{|y^{\prime}_{i}|\}\leq R,\max_{i,k}\{|f_{k}(\mathbf{x}_{i})|\}\leq R\), \(\mathrm{TF}_{\boldsymbol{\theta}}\) maps_

\[\mathbf{h}_{i} =[\mathbf{x}_{i};y^{\prime}_{i};*;f_{1}(\mathbf{x}_{i});\cdots;f_ {K}(\mathbf{x}_{i});\mathbf{0}_{K+1};1;t_{i}]\] \[\rightarrow \mathbf{h}^{\prime}_{i} =[\mathbf{x}_{i};y^{\prime}_{i};*;f_{1}(\mathbf{x}_{i});\cdots;f_ {K}(\mathbf{x}_{i});\widetilde{L}_{\mathsf{val}}(f_{1});\cdots;\widetilde{L}_{ \mathsf{val}}(f_{K});0;1;t_{i}],\qquad i\in[N+1],\]

_where \(\widetilde{L}_{\mathsf{val}}(\cdot)\) is a functional such that \(\max_{k}\left|\widetilde{L}_{\mathsf{val}}(f_{k})-\widehat{L}_{\mathsf{val}}( f_{k})\right|\leq\varepsilon\)._

Proof of Proposition I.1.: As \(\ell\) is \((\varepsilon,R,M,C)\)-approximable by sum of relus, there exists a function \(g:\mathbb{R}^{2}\rightarrow\mathbb{R}\) of form

\[g(s,t)=\sum_{m=1}^{M}c_{m}\sigma(a_{m}s+b_{m}t+d_{m})\ \ \mathrm{with}\ \ \sum_{m=1}^{M}|c_{m}|\leq C,\ \ |a_{m}|+|b_{m}|+|d_{m}|\leq 1,\ \forall m\in[M],\]

such that \(\sup_{(s,t)\in[-R,R]^{2}}|g(s,t)-\ell(s,t)|\leq\varepsilon\). We define

\[\widetilde{L}_{\mathsf{val}}(f):=\frac{1}{|\mathcal{D}_{\mathsf{val}}|}\sum_{( \mathbf{x}_{i},y_{i})\in\mathcal{D}_{\mathsf{val}}}g(f(\mathbf{x}_{i}),y_{i}),\]

Next, for every \(m\in[M]\) and \(k\in[K]\), we define matrices \(\mathbf{Q}_{m,k},\mathbf{K}_{m,k},\mathbf{V}_{m,k}\in\mathbb{R}^{D\times D}\) such that

\[\mathbf{Q}_{m,k}\mathbf{h}_{i}=\begin{bmatrix}a_{m}\\ b_{m}\\ d_{m}\\ -2\\ \mathbf{0}\end{bmatrix},\quad\mathbf{K}_{m,k}\mathbf{h}_{j}=\begin{bmatrix}f_{ k}(\mathbf{x}_{j})\\ y_{j}\\ 1\\ R(1+t_{j})\\ \mathbf{0}\end{bmatrix},\quad\mathbf{V}_{m,k}\mathbf{h}_{j}=\frac{(N+1)c_{m}}{| \mathcal{D}_{\mathsf{val}}|}\cdot\mathbf{e}_{D-(K-k)-3}\]

where \(\mathbf{e}_{s}\in\mathbb{R}^{D}\) is the vector with \(s\)-th entry being \(1\) and others being \(0\). As the input has structure \(\mathbf{h}_{i}=[\mathbf{x}_{i};y^{\prime}_{i};*;f_{1}(\mathbf{x}_{i});\cdots; f_{K}(\mathbf{x}_{i});\mathbf{0}_{K+1};1;t_{i}]\), these matrices indeed exist, and further it is straightforward to check that they have norm bounds

\[\max_{m\in[M],k\in[K]}\left\|\mathbf{Q}_{m,k}\right\|_{\mathrm{op}}\leq 3, \quad\max_{m\in[M],k\in[K]}\left\|\mathbf{K}_{m,k}\right\|_{\mathrm{op}}\leq 2 +R,\quad\sum_{m\in[M],k\in[K]}\left\|\mathbf{V}_{m,k}\right\|_{\mathrm{op}} \leq\frac{K(N+1)C}{|\mathcal{D}_{\mathsf{val}}|}.\]

Now, for every \(i,j\in[N+1]\), we have

\[\sigma(\langle\mathbf{Q}_{m,k}\mathbf{h}_{i},\mathbf{K}_{m,k} \mathbf{h}_{j}\rangle) =\sigma(a_{m}f_{k}(\mathbf{x}_{j})+b_{m}y_{j}+d_{m}-2R(1+t_{j}))\] \[=\sigma\big{(}a_{m}\mathbf{w}^{\top}\mathbf{x}_{j}+b_{m}y_{j}+d_ {m}\big{)}1\{t_{j}=-1\},\]

where the last equality follows from the bound \(|a_{m}f_{k}(\mathbf{x}_{j})+b_{m}y_{j}+d_{m}|\leq R(|a_{m}|+|b_{m}|)+d_{m}\leq 2R\), so that the above relu equals \(0\) if \(t_{j}\leq 0\). Therefore, for each \(i\in[N+1]\) and \(k\in[K]\),

\[\sum_{m=1}^{M}\sigma(\langle\mathbf{Q}_{m,k}\mathbf{h}_{i},\mathbf{ K}_{m,k}\mathbf{h}_{j}\rangle)\mathbf{V}_{m,k}\mathbf{h}_{j}\] \[=\left(\sum_{m=1}^{M}c_{m}\sigma\big{(}a_{m}\mathbf{w}^{\top} \mathbf{x}_{j}+b_{m}y_{j}+d_{m}\big{)}\right)\cdot\frac{(N+1)}{|\mathcal{D}_{ \mathsf{val}}|}1\{t_{j}=-1\}\mathbf{e}_{D-(K-k)-3}\] \[=g(f_{k}(\mathbf{x}_{j}),y_{j})\cdot\frac{(N+1)}{|\mathcal{D}_{ \mathsf{val}}|}1\{t_{j}=-1\}\mathbf{e}_{D-(K-k)-3}.\]

Thus letting the attention layer \(\boldsymbol{\theta}=\{(\mathbf{V}_{m,k},\mathbf{Q}_{m,k},\mathbf{K}_{m,k})\}_{(m,k)\in[M]\times[K]}\), we have

\[\widetilde{\mathbf{h}}_{i}=\left[\mathrm{Attn}_{\boldsymbol{\theta}}(\mathbf{ H})\right]_{i}=\mathbf{h}_{i}+\frac{1}{N+1}\sum_{j=1}^{N+1}\sum_{m,k}\sigma( \langle\mathbf{Q}_{m,k}\mathbf{h}_{i},\mathbf{K}_{m,k}\mathbf{h}_{j}\rangle) \mathbf{V}_{m,k}\mathbf{h}_{j}\]\[=\mathbf{h}_{i}+\frac{1}{|\mathcal{D}_{\text{val}}|}\sum_{j=1}^{N+1} \sum_{k=1}^{K}g(f_{k}(\mathbf{x}_{j}),y_{j})\cdot 1\{t_{j}=-1\}\mathbf{e}_{D-(K-k)-3}\] \[=\mathbf{h}_{i}+\sum_{k=1}^{K}\Bigg{(}\frac{1}{|\mathcal{D}_{ \text{val}}|}\sum_{(\mathbf{x}_{j},y_{j})\in\mathcal{D}_{\text{val}}}g(f_{k}( \mathbf{x}_{j}),y_{j})\Bigg{)}\mathbf{e}_{D-(K-k)-3}\] \[=\mathbf{h}_{i}+\sum_{k=1}^{K}\widetilde{L}_{\text{val}}(f_{k}) \cdot\mathbf{e}_{D-(K-k)-3}\] \[=[\mathbf{x}_{i};y_{i}^{\prime};*;f_{1}(\mathbf{x}_{i});\cdots;f_ {K}(\mathbf{x}_{i});\mathbf{0}_{K+1};1;t_{i}]+[\mathbf{0}_{D-K-3};\widetilde{ L}_{\text{val}}(f_{1});\cdots;\widetilde{L}_{\text{val}}(f_{K});0;0;0]\] \[=[\mathbf{x}_{i};y_{i}^{\prime};*;f_{1}(\mathbf{x}_{i});\cdots;f_ {K}(\mathbf{x}_{i});\widetilde{L}_{\text{val}}(f_{1});\cdots;\widetilde{L}_{ \text{val}}(f_{K});0;1;t_{i}],\qquad i\in[N+1].\]

This is the desired result. 

**Proposition I.2** (Selection layer).: _There exists a 3-layer transformer \(\mathrm{TF}_{\boldsymbol{\theta}}\) with_

\[\max_{\ell\in[3]}M^{(\ell)}\leq 2K+2,\quad\max_{\ell\in[3]}D^{(\ell)}\leq K^{2}+ K+1,\quad\|\boldsymbol{\theta}\|\leq\gamma^{-1}+3KR+2.\]

_such that \(\mathrm{TF}_{\boldsymbol{\theta}}\) maps_

\[\mathbf{h}_{i} =[*;f_{1}(\mathbf{x}_{i});\cdots;f_{K}(\mathbf{x}_{i});\mathbb{L} _{1};\cdots;\mathbb{L}_{K};0;1;t_{i}]\] \[\rightarrow \mathbf{h}_{i}^{\prime} =[*;f_{1}(\mathbf{x}_{i});\cdots;f_{K}(\mathbf{x}_{i});*;\cdots; *;\widehat{f}(\mathbf{x}_{i});1;t_{i}],\qquad i\in[N+1],\]

_where \(\widehat{f}=\sum_{k=1}^{K}\lambda_{k}f_{k}\) is an aggregated predictor, where the weights \(\lambda_{1},\cdots,\lambda_{K}\geq 0\) are functions only on \(\mathbb{L}_{1},\cdots,\mathbb{L}_{k}\) such that_

\[\sum_{k=1}^{K}\lambda_{k}=1,\qquad\lambda_{k}>0\text{ only if }\mathbb{L}_{k}\leq\min_{k^{*}\in[K]}\mathbb{L}_{k^{*}}+\gamma.\]

Proof of Proposition I.2.: We construct a \(\boldsymbol{\theta}\) which is a composition of 2 MLP layers followed by an attention layer \((\boldsymbol{\theta}^{(1)}_{\text{nlp}},\boldsymbol{\theta}^{(2)}_{\text{nlp} },\boldsymbol{\theta}^{(3)}_{\text{ntn}})\).

Step 1: construction of \(\boldsymbol{\theta}^{(1)}_{\text{nlp}}\). We consider matrix \(\mathbf{W}^{(1)}_{1}\) that maps

\[\mathbf{h}=[*_{D-K-3};\mathbb{L}_{1};\cdots;\mathbb{L}_{K};*;*;*]\] \[\mapsto\mathbf{W}^{(1)}_{1}\mathbf{h}=[\mathbb{L}_{1}-\mathbb{L} _{2};\cdots;\mathbb{L}_{1}-\mathbb{L}_{K};\cdots;\mathbb{L}_{K}-\mathbb{L}_{K- 1};\mathbb{L}_{1};-\mathbb{L}_{1};\cdots;\mathbb{L}_{K};-\mathbb{L}_{K}],\]

i.e. \(\mathbf{W}^{(1)}_{1}\mathbf{h}\) is a \(K^{2}+K\) dimensional vector so that its entry contains \(\{\mathbb{L}_{k}-\mathbb{L}_{l}\}_{k,l\in[K]}\) and \(\{\mathbb{L}_{k},-\mathbb{L}_{k}\}_{k\in[K]}\). Clearly, such \(\mathbf{W}^{(1)}_{1}\) exists and can be chosen so that \(\left\|\mathbf{W}^{(1)}_{1}\right\|_{\text{op}}\leq 2K.\) We then consider a matrix \(\mathbf{W}^{(1)}_{2}\) that maps

\[\sigma(\mathbf{W}^{(1)}_{1}\mathbf{h})\mapsto\mathbf{W}^{(1)}_{2}\sigma( \mathbf{W}^{(1)}_{1}\mathbf{h})=[\mathbf{0}_{D-K-3};c_{1}-\mathbb{L}_{1}; \cdots;c_{K}-\mathbb{L}_{K};\mathbf{0}_{3}]\in\mathbb{R}^{D},\]

where \(c_{k}=c_{k}(\mathbb{L}):=\sum_{l\neq k}\sigma(\mathbb{L}_{k}-\mathbb{L}_{l})\). Notice that

\[c_{k}-\mathbb{L}_{k}=-\sigma(\mathbb{L}_{k})+\sigma(-\mathbb{L}_{k})+\sum_{l \neq k}\sigma(\mathbb{L}_{k}-\mathbb{L}_{l}),\]

and hence such \(\mathbf{W}^{(1)}_{2}\) exists and can be chosen so that \(\left\|\mathbf{W}^{(1)}_{2}\right\|_{\text{op}}\leq K+1\). We set \(\boldsymbol{\theta}^{(1)}_{\text{nlp}}=(\mathbf{W}^{(1)}_{1},\mathbf{W}^{(1)}_{2})\), then \(\mathrm{MLP}_{\boldsymbol{\theta}^{(1)}_{\text{nlp}}}\) maps \(\mathbf{h}_{i}\) to

\[\mathbf{h}^{(1)}_{i}=[*;f_{1}(\mathbf{x}_{i});\cdots;f_{K}(\mathbf{x}_{i});c_{ 1};\cdots;c_{K};0;1;t_{i}].\]

The basic property of \(\{c_{k}\}_{k\in[K]}\) is that, if \(c_{k}\leq\gamma\), then \(\mathbb{L}_{k}\leq\min_{k^{*}\in[K]}\mathbb{L}_{k^{*}}+\gamma\).

Step 2: construction of \(\boldsymbol{\theta}^{(2)}_{\text{nlp}}\). We consider matrix \(\mathbf{W}^{(2)}_{1}\) that maps

\[\mathbf{h}=[*_{D-K-3};c_{1};\cdots;c_{K};*;1;*]\]\[\begin{split}\mapsto\mathbf{W}_{1}^{(2)}\mathbf{h}=[1-\gamma^{-1}c_{1};c_ {1};-c_{1};\cdots;1-\gamma^{-1}c_{K};c_{K};-c_{K}]\in\mathbb{R}^{3K},\end{split}\]

and \(\mathbf{W}_{1}^{(2)}\) can be chosen so that \(\left\|\mathbf{W}_{1}^{(2)}\right\|_{\mathrm{op}}\leq K+1+\gamma^{-1}.\) We then consider a matrix \(\mathbf{W}_{2}^{(2)}\) that maps

\[\sigma(\mathbf{W}_{1}^{(2)}\mathbf{h})\mapsto\mathbf{W}_{2}^{(2)}\sigma( \mathbf{W}_{1}^{(1)}\mathbf{h})=[\mathbf{0}_{D-K-3};\sigma(1-\gamma^{-1}c_{1}) -c_{1};\cdots;\sigma(1-\gamma^{-1}c_{K})-c_{K};\mathbf{0}_{3}]\in\mathbb{R}^{D},\]

which exists and can be chosen so that \(\left\|\mathbf{W}_{2}^{(1)}\right\|_{\mathrm{op}}\leq 2\). We set \(\boldsymbol{\theta}_{\mathtt{nlp}}^{(2)}=(\mathbf{W}_{1}^{(2)},\mathbf{W}_{2}^ {(2)})\), then \(\mathrm{MLP}_{\boldsymbol{\theta}_{\mathtt{nlp}}^{(2)}}\) maps \(\mathbf{h}_{i}^{(1)}\) to

\[\mathbf{h}_{i}^{(2)}=[*;f_{1}(\mathbf{x}_{i});\cdots;f_{K}(\mathbf{x}_{i});u_{ 1};\cdots;u_{K};0;1;t_{i}],\]

where \(u_{k}=\sigma(1-\gamma^{-1}c_{k})\forall k\in[K]\). Clearly, \(u_{k}\in[0,1]\), and \(u_{k}>0\) if and only if \(c_{k}\leq\gamma\).

Step 3: construction of \(\boldsymbol{\theta}_{\mathtt{attn}}^{(3)}\). We define

\[\begin{split}\lambda_{1}=1-\sigma(1-u_{1}),\qquad\lambda_{k}= \sigma(1-u_{1}-\cdots-u_{k-1})-\sigma(1-u_{1}-\cdots-u_{k})\,\forall k\geq 2. \end{split}\]

Clearly, \(\lambda_{k}\geq 0\), and \(\sum_{k}\lambda_{k}=1\). Further,

\[\begin{split}\lambda_{k}>0\Rightarrow u_{k}>0\Rightarrow c_{k} \leq\gamma\Rightarrow\mathbb{L}_{k}\leq\min_{k^{*}\in[K]}\mathbb{L}_{k^{*}}+ \gamma.\end{split}\]

Therefore, it remains to construct \(\boldsymbol{\theta}_{\mathtt{attn}}^{(3)}\) that implements \(\widehat{f}=\sum_{k=1}^{K}\lambda_{k}f_{k}\) based on \([\mathbf{h}_{i}^{(2)}]_{i}\). Notice that

\[\begin{split}\widehat{f}(\mathbf{x}_{i})&=\sigma(1) \cdot f_{1}(\mathbf{x}_{i})+\sum_{k=1}^{K-1}\sigma(1-u_{1}-\cdots-u_{k-1}) \cdot(f_{k}(\mathbf{x}_{i})-f_{k-1}(\mathbf{x}_{i}))\\ &\quad-\sigma(1-u_{1}-\cdots-u_{K})\cdot f_{K}(\mathbf{x}_{i}), \end{split}\] (41)

and hence we construct \(\boldsymbol{\theta}_{\mathtt{attn}}^{(3)}\) as follows: for every \(k\in[K+1]\) and \(w\in\{0,1\}\), we define matrices \(\mathbf{Q}_{k,w},\mathbf{K}_{k,w},\mathbf{V}_{k,w}\in\mathbb{R}^{D\times D}\) such that for all \(k\in[K+1]\)

\[\begin{split}\mathbf{Q}_{k,0}\mathbf{h}_{i}^{(2)}&= \begin{bmatrix}(f_{k}(\mathbf{x}_{i})+R)\cdot\mathbf{1}_{k}\\ \mathbf{0}\end{bmatrix},\quad\mathbf{Q}_{k,1}\mathbf{h}_{i}^{(2)}=\begin{bmatrix} (f_{k-1}(\mathbf{x}_{i})+R)\cdot\mathbf{1}_{k}\\ \mathbf{0}\end{bmatrix},\\ \mathbf{K}_{k,0}\mathbf{h}_{j}^{(2)}&=\mathbf{K}_{k,1} \mathbf{h}_{j}^{(2)}=\begin{bmatrix}1\\ -u_{1}\\ \vdots\\ -u_{k-1}\\ \mathbf{0}\end{bmatrix},\qquad\mathbf{V}_{k,0}\mathbf{h}_{j}^{(2)}=\mathbf{e}_{ D-2}=-\mathbf{V}_{k,1}\mathbf{h}_{j}^{(2)},\end{split}\]

for all \(i,j\in[N+1]\), where we understand \(f_{0}=f_{K+1}=0\) and \(\mathbf{1}_{k}\) is the \(k\)-dimensional vector with all entries being 1. By the structure of \(\mathbf{h}_{i}^{(2)}\), these matrices indeed exist, and further it is straightforward to check that they have norm bounds

\[\max_{k\in[K+1],w\in\{0,1\}}\left\|\mathbf{Q}_{k,w}\right\|_{\mathrm{op}}\leq KR,\quad\max_{k\in[K+1],w\in\{0,1\}}\left\|\mathbf{K}_{k}\right\|_{\mathrm{op}} \leq 1,\quad\sum_{k\in[K+1],w\in\{0,1\}}\left\|\mathbf{V}_{k,w}\right\|_{\mathrm{op}} \leq 2K+2.\]

Now, for every \(i,j\in[N+1]\), \(k\in[K+1],w\in\{0,1\}\), we have

\[\begin{split}\sigma\Big{(}\Big{\langle}\mathbf{Q}_{k,w}\mathbf{h}_ {i}^{(2)},\mathbf{K}_{k,w}\mathbf{h}_{j}^{(2)}\Big{\rangle}\Big{)}&= \sigma((1-u_{1}-\cdots-u_{k-1})(f_{k-w}(\mathbf{x}_{i})+R))\\ &=\sigma(1-u_{1}-\cdots-u_{k-1})\cdot(f_{k-w}(\mathbf{x}_{i})+R), \end{split}\]

where the last equality follows from \(f_{k}(\mathbf{x}_{i})+R\geq 0\forall k\in[K]\). Therefore,

\[\begin{split}&\sum_{k\in[K+1],w\in\{0,1\}}\sigma\Big{(}\Big{\langle} \mathbf{Q}_{m,k}\mathbf{h}_{i}^{(2)},\mathbf{K}_{m,k}\mathbf{h}_{j}^{(2)} \Big{\rangle}\Big{)}\mathbf{V}_{m,k}\mathbf{h}_{j}^{(2)}\\ =&\sum_{k=1}^{K}\Big{[}\sigma(1-u_{1}-\cdots-u_{k-1})\cdot(f_{k}( \mathbf{x}_{i})+R)-\sigma(1-u_{1}-\cdots-u_{k-1})\cdot(f_{k-1}(\mathbf{x}_{i}) +R)\Big{]}\cdot\mathbf{e}_{D-2}\end{split}\]\(=\widehat{f}(\mathbf{x}_{i})\cdot\mathbf{e}_{D-2}\),

where the last equality is due to (41). Thus letting the attention layer \(\boldsymbol{\theta}^{(3)}_{\mathtt{attn}}=\{(\mathbf{V}_{k,w},\mathbf{Q}_{k,w}, \mathbf{K}_{k,w})\}_{(k,w)\in[K+1]\times\{0,1\}}\), we have

\[\mathbf{h}^{(3)}_{i} =\Big{[}\mathrm{Attn}_{\boldsymbol{\theta}}(\mathbf{H}^{(2)}) \Big{]}_{i}=\mathbf{h}_{i}+\frac{1}{N+1}\sum_{j=1}^{N+1}\sum_{k,w}\sigma\Big{(} \Big{\langle}\mathbf{Q}_{k,w}\mathbf{h}^{(2)}_{i},\mathbf{K}_{k,w}\mathbf{h}^{ (2)}_{j}\Big{\rangle}\Big{)}\mathbf{V}_{k,w}\mathbf{h}^{(2)}_{j}\] \[=\mathbf{h}^{(2)}_{i}+\widehat{f}(\mathbf{x}_{i})\cdot\mathbf{e} _{D-2}\] \[=[*;f_{1}(\mathbf{x}_{i});\cdots;f_{K}(\mathbf{x}_{i});u_{1}; \cdots;u_{K};\widehat{f}(\mathbf{x}_{i});1;t_{i}].\]

This is the desired result. 

Now, we are ready to prove Theorem I.1.

**Proof of Theorem I.1**  As \(\ell(\cdot,\cdot)\) is \((\gamma/3,R,M,C)\)-approximable by sum of relus, we can invoke Proposition I.1 to show that there exists a single attention layer \(\boldsymbol{\theta}^{(1)}_{\mathtt{attn}}\) so that \(\mathrm{Attn}_{\boldsymbol{\theta}^{(1)}_{\mathtt{attn}}}\) maps

\[\mathbf{h}_{i}\quad\rightarrow\quad\mathbf{h}^{\prime}_{i}=[\mathbf{x}_{i};y^{ \prime}_{i};*;f_{1}(\mathbf{x}_{i});\cdots;f_{K}(\mathbf{x}_{i});\widetilde{L} _{\mathsf{val}}(f_{1});\cdots;\widetilde{L}_{\mathsf{val}}(f_{K});0;1;t_{i}], \qquad i\in[N+1],\]

for any input \(\mathbf{H}=[\mathbf{h}_{i}]_{i}\) of the form described in Theorem I.1, and \(\widetilde{L}_{\mathsf{val}}(\cdot)\) is a functional such that \(\max_{k}\Big{|}\widetilde{L}_{\mathsf{val}}(f_{k})-\widehat{L}_{\mathsf{val}} (f_{k})\Big{|}\leq\gamma/3\).

Next, by the proof of Proposition I.2, there exists \((\boldsymbol{\theta}^{(1)}_{\mathtt{nlp}},\boldsymbol{\theta}^{(2)}_{ \mathtt{nlp}},\boldsymbol{\theta}^{(3)}_{\mathtt{attn}})\) that maps

\[\mathbf{h}^{\prime}_{i}\quad\rightarrow\quad\mathbf{h}^{(3)}_{i}=\Bigg{[} \mathbf{x}_{i};y^{\prime}_{i};*;f_{1}(\mathbf{x}_{i});\cdots;f_{K}(\mathbf{x}_ {i});*;\sum_{k=1}^{K}\lambda_{k}f_{k}(\mathbf{x}_{i});1;t_{i}\Bigg{]},\qquad i \in[N+1],\]

where \(\lambda=(\lambda_{1},\cdots,\lambda_{K})\in\Delta([K])\) and \(\lambda_{k}>0\) only when \(\widetilde{L}_{\mathsf{val}}(f_{k})\leq\min_{k^{*}}\widetilde{L}_{\mathsf{val }}(f_{k^{*}})+\gamma/3\). Using the fact that \(\max_{k}|\widetilde{L}_{\mathsf{val}}(f_{k})-\widehat{L}_{\mathsf{val}}(f_{k})| \leq\gamma/3\), we deduce that \(\lambda\) is supported on \(\{k:\widetilde{L}_{\mathsf{val}}(f_{k})\leq\min_{k_{*}\in[K]}\widehat{L}_{ \mathsf{val}}(f_{k_{*}})+\gamma\}\).

Therefore, \(\boldsymbol{\theta}=(\boldsymbol{\theta}^{(1)}_{\mathtt{attn}},\boldsymbol{ \theta}^{(1)}_{\mathtt{nlp}},\boldsymbol{\theta}^{(2)}_{\mathtt{nlp}}, \boldsymbol{\theta}^{(3)}_{\mathtt{attn}})\) is the desired transformer, with

\[\max_{\ell\in[3]}M^{(\ell)}\leq(M+3)K,\quad\max_{\ell\in[3]}D^{(\ell)}\leq K^ {2}+K+1,\]

and

\[\|\boldsymbol{\theta}\| \leq\max\left\{3R+\frac{2NKC}{|\mathcal{D}_{\mathsf{val}}|}+3K+1, K+3+\gamma^{-1},KR+2K+2\right\}\] \[\leq 7KR+\frac{2NKC}{|\mathcal{D}_{\mathsf{val}}|}+\gamma^{-1}.\]

This completes the proof. 

### Proof of Theorem I.1

We first restate Theorem I.1 into the following version which provides additional size bounds for \(\boldsymbol{\theta}\). For the simplicity of presentation, throughout this subsection and Appendix J, we denote \(\mathcal{I}_{t}=\{i:(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{\mathsf{train}}\}\), \(\mathcal{I}_{v}=\{i:(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{\mathsf{val}}\}\), \(\mathbf{X}_{\mathsf{train}}=[\mathbf{x}_{i}]_{i\in\mathcal{I}_{t}}\) to be the input matrix corresponding to the training split only, and \(N_{\mathsf{train}}=|\mathcal{D}_{\mathsf{train}}|\), \(N_{\mathsf{val}}=|\mathcal{D}_{\mathsf{val}}|\).

**Theorem I.2**.: _For any sequence of regularizations \(\{\lambda_{k}\}_{k\in[K]}\), \(0\leq\alpha\leq\beta\) with \(\kappa:=\max_{k}\frac{\beta+\lambda_{k}}{\alpha+\lambda_{k}}\), \(B_{w}>0\), \(\gamma>0\), and \(\varepsilon<B_{w}/2\), suppose in input format (3) we have \(D\geq\Theta(Kd)\). Then there exists an \(L\)-layer transformer \(\mathrm{TF}_{\boldsymbol{\theta}}\) with_

\[L=\lceil 2\kappa\log(B_{w}/(2\varepsilon))\rceil+4,\quad\max_{\ell\in[L]}M^{( \ell)}\leq 3K+1,\quad\max_{\ell\in[L]}D^{(\ell)}\leq K^{2}+K+1,\]\[\|\boldsymbol{\theta}\|\leq\mathcal{O}\bigg{(}KR+(\beta+\lambda)^{-1}+\frac{N}{N_{ \mathsf{val}}}+\gamma^{-1}\bigg{)},\qquad R:=\max\{B_{x}B_{w},B_{y},1\},\]

_such that the following holds. On any input data \((\mathcal{D},\mathbf{x}_{N+1})\) such that the problem (ICRidge) is well-conditioned and has a bounded solution:_

\[\alpha\leq\lambda_{\min}(\mathbf{X}_{\mathsf{train}}^{\top}\mathbf{X}_{ \mathsf{train}}/N_{\mathsf{train}})\leq\lambda_{\max}(\mathbf{X}_{\mathsf{ train}}^{\top}\mathbf{X}_{\mathsf{train}}/N_{\mathsf{train}})\leq\beta, \quad\max_{k\in[K]}\left\|\mathbf{w}_{\mathrm{ridge}}^{\lambda_{k}}(\mathcal{D}_ {\mathsf{train}})\right\|_{2}\leq B_{w}/2,\] (42)

\(\mathrm{TF}_{\boldsymbol{\theta}}^{0}\) _approximately implements ridge selection: its prediction_

\[\widehat{y}_{N+1}=\mathsf{read}_{y}(\mathrm{TF}_{\boldsymbol{\theta}}^{0}( \mathbf{H}))=\langle\widehat{\mathbf{w}},\mathbf{x}_{N+1}\rangle\,,\qquad \widehat{\mathbf{w}}=\sum_{k=1}^{K}\lambda_{k}\widehat{\mathbf{w}}_{k}\]

_satisfies the following._

1. _For each_ \(k\in[K]\)_,_ \(\widehat{\mathbf{w}}_{k}=\widehat{\mathbf{w}}_{k}(\mathcal{D}_{\mathsf{train}})\) _approximates the ridge estimator_ \(\mathbf{w}_{\mathrm{ridge}}^{\lambda_{k}}(\mathcal{D}_{\mathsf{train}})\)_, i.e._ \(\left\|\widehat{\mathbf{w}}_{k}-\mathbf{w}_{\mathrm{ridge}}^{\lambda_{k}}( \mathcal{D}_{\mathsf{train}})\right\|_{2}\leq\varepsilon\)_._
2. \(\lambda=(\lambda_{1},\cdots,\lambda_{K})\in\Delta([K])\) _so that_ \[\lambda_{k}>0\text{ only if }\widehat{L}_{\mathsf{val}}(\widehat{ \mathbf{w}}_{k})\leq\min_{k^{*}\in[K]}\widehat{L}_{\mathsf{val}}(\widehat{ \mathbf{w}}_{k^{*}})+\gamma.\]

In particular, if we set \(\gamma^{\prime}=2(B_{x}B_{w}+B_{y})B_{x}\varepsilon+\gamma\), then it holds that8

Footnote 8: This is because \(\widehat{L}_{\mathsf{val}}(\mathbf{w})\) is \((B_{x}B_{w}+B_{y})B_{x}\)-Lipschitz w.r.t. \(\mathbf{w}\in\mathsf{B}_{2}(B_{w})\).

\[\mathrm{dist}\bigg{(}\widehat{\mathbf{w}},\mathrm{conv}\{\widehat{\mathbf{w} }_{\mathrm{ridge},\mathsf{train}}^{\lambda_{k}}:\widehat{L}_{\mathsf{val}}( \widehat{\mathbf{w}}_{\mathrm{ridge},\mathsf{train}}^{\lambda_{k}})\leq\min_ {k_{*}\in[K]}\widehat{L}_{\mathsf{val}}(\widehat{\mathbf{w}}_{\mathrm{ridge},\mathsf{train}}^{\lambda_{k*}})+\gamma^{\prime}\}\bigg{)}\leq\varepsilon,\]

where we denote \(\widehat{\mathbf{w}}_{\mathrm{ridge},\mathsf{train}}^{\lambda_{k}}:=\mathbf{w }_{\mathrm{ridge}}^{\lambda_{k}}(\mathcal{D}_{\mathsf{train}})\).

To prove Theorem I.2, we first show that, for the squared validation loss, there exists a 3-layer transformer that performs predictor selection based on the _exactly_ evaluated \(\widehat{L}_{\mathsf{val}}(f_{k})\) for each \(k\in[K]\). (Proof in Appendix I.2.1.)

**Theorem I.3** (Square-loss version of Theorem I.1).: _Consider the squared validation loss_

\[\widehat{L}_{\mathsf{val}}(f):=\frac{1}{2|\mathcal{D}_{\mathsf{val}}|}\sum_{( x_{i},y_{i})\in\mathcal{D}_{\mathsf{val}}}(f(\mathbf{x}_{i})-y_{i})^{2}.\]

_Then there exists a 3-layer transformer \(\mathrm{TF}_{\boldsymbol{\theta}}\) with_

\[\max_{\ell\in[3]}M^{(\ell)}\leq 2K+2,\quad\max_{\ell\in[3]}D^{(\ell)}\leq K^{2 }+K+1,\quad\|\boldsymbol{\theta}\|\leq 7KR+\frac{2N}{|\mathcal{D}_{\mathsf{val}}|}+ \gamma^{-1},\]

_such that for any input \(\mathbf{H}\) that takes form_

\[\mathbf{h}_{i}=[\mathbf{x}_{i};y_{i}^{\prime};*;f_{1}(\mathbf{x}_{i});\cdots;f _{K}(\mathbf{x}_{i});\mathbf{0}_{K};*;1;t_{i}],\]

_where \(\mathrm{TF}_{\boldsymbol{\theta}}\) outputs \(\mathbf{h}_{N+1}=[\mathbf{x}_{N+1};\widehat{f}(\mathbf{x}_{N+1});*;1;0]\), where the predictor \(\widehat{f}:\mathbb{R}^{d}\to\mathbb{R}\) is a convex combination of \(\{f_{k}:\widehat{L}_{\mathsf{val}}(f_{k})\leq\min_{k_{*}\in[K]}\widehat{L}_{ \mathsf{val}}(f_{k_{*}})+\gamma\}\). As a corollary, for any convex risk \(L:(\mathbb{R}^{d}\to\mathbb{R})\to\mathbb{R}\), \(\widehat{f}\) satisfies_

\[L(\widehat{f})\leq\min_{k_{*}\in[K]}L(f_{k_{*}})+\max_{k\in[K]}\left|\widehat{L} _{\mathsf{val}}(f_{k})-L(f_{k})\right|+\gamma.\]

Proof of Theorem I.2 First, by the proof9 of Theorem 4 and Proposition B.6, for each \(k\in[K]\), there exists a \(T=L-3\) layer transformer \(\boldsymbol{\theta}^{(1:T)}\) such that \(\mathrm{TF}_{\boldsymbol{\theta}^{(1:T)}}\) maps

Footnote 9: Technically, an adapted version where the underlying ICGD mechanism operates on the training split (with \(t_{i}=1\)) with size \(N_{\mathsf{train}}\) instead of on all \(N\) training examples, which only changes \(\|\boldsymbol{\theta}\|\) by at most a constant factor, and does not change the number of layers and heads.

\[\mathbf{h}_{i}\quad\to\quad\mathbf{h}_{i}^{(T)}=[\mathbf{x}_{i};y_{i}^{\prime};* ;\langle\widehat{\mathbf{w}}_{1},\mathbf{x}_{i}\rangle\,;\cdots;\langle\widehat{ \mathbf{w}}_{K},\mathbf{x}_{i}\rangle\,;\mathbf{0}_{K};1;t_{i}],\]so that if (42) holds, we have \(\left\|\widehat{\mathbf{w}}_{k}-\mathbf{w}_{\mathrm{ridge}}^{\lambda_{k}}\right\|_ {2}\leq\varepsilon\) and \(\widehat{\mathbf{w}}_{k}\in\mathsf{B}_{2}(B_{w})\).

Next, by Theorem I.3, there exists a 3-layer transformer \(\boldsymbol{\theta}^{(T+1:T+3)}\) that outputs

\[\mathbf{h}_{N+1}^{(T+3)}=[\mathbf{x}_{N+1};\langle\widehat{\mathbf{w}}, \mathbf{x}_{N+1}\rangle\,;*;1;t_{i}],\]

where \(\widehat{\mathbf{w}}=\sum_{k=1}^{K}\lambda_{k}\widehat{\mathbf{w}}_{k}\), \(\lambda=(\lambda_{1},\cdots,\lambda_{K})\in\Delta([K])\) so that

\[\lambda_{k}>0\text{ only if }\widehat{L}_{\mathsf{val}}(\widehat{\mathbf{w}}_{k} )\leq\min_{k^{*}\in[K]}\widehat{L}_{\mathsf{val}}(\widehat{\mathbf{w}}_{k^{*} })+\gamma.\]

This is the desired result. 

#### i.2.1 Proof of Theorem I.3

Similar to the proof of Proposition 10, Theorem I.3 is a direct corollary by combining Proposition I.3 with Proposition I.2.

**Proposition I.3** (Evaluation layer for the squared loss).: _There exists an attention layer \(\mathrm{TF}_{\boldsymbol{\theta}}\) with \(2K\) heads and \(\left\|\boldsymbol{\theta}\right\|\leq 3R+2NK/\left|\mathcal{D}_{\mathsf{val}}\right|\) such that \(\mathrm{TF}_{\boldsymbol{\theta}}\) maps_

\[\mathbf{h}_{i}=[*;f_{1}(\mathbf{x}_{i});\cdots;f_{K}(\mathbf{x}_{ i});\mathbf{0}_{K};*;1;t_{i}]\] \[\rightarrow \mathbf{h}_{i}^{\prime}=[*;f_{1}(\mathbf{x}_{i});\cdots;f_{K}( \mathbf{x}_{i});\widehat{L}_{\mathsf{val}}(f_{1});\cdots;\widehat{L}_{\mathsf{ val}}(f_{K});*;1;t_{i}],\qquad i\in[N+1].\]

Proof of Proposition 1.3.: For every \(k\in[K]\), we define matrices \(\mathbf{Q}_{m,k},\mathbf{K}_{m,k},\mathbf{V}_{m,k}\in\mathbb{R}^{D\times D}\) such that for all \(i,j\in[N+1]\),

\[\mathbf{Q}_{k,0}\mathbf{h}_{i}=\begin{bmatrix}1\\ -1\\ -2\\ \mathbf{0}\end{bmatrix},\quad\mathbf{Q}_{k,1}\mathbf{h}_{i}=\begin{bmatrix} -1\\ 1\\ -2\\ \mathbf{0}\end{bmatrix},\quad\mathbf{K}_{k,0}\mathbf{h}_{j}=\mathbf{K}_{k,1} \mathbf{h}_{j}=\begin{bmatrix}f_{k}(\mathbf{x}_{j})\\ y_{j}\\ R(1+t_{j})\\ \mathbf{0}\end{bmatrix},\] \[\mathbf{V}_{k,0}\mathbf{h}_{j}=-\mathbf{V}_{k,1}\mathbf{h}_{j}= \frac{(N+1)}{2\left|\mathcal{D}_{\mathsf{val}}\right|}\cdot(f_{k}(\mathbf{x}_{ j})-y_{j})\mathbf{e}_{D-(K-k)-3}.\]

As the input has structure \(\mathbf{h}_{i}=[\mathbf{x}_{i};y_{i}^{\prime};*;f_{1}(\mathbf{x}_{i});\cdots;f _{K}(\mathbf{x}_{i});\mathbf{0}_{K+1};1;t_{i}]\), these matrices indeed exist, and further it is straightforward to check that they have norm bounds

\[\max_{k\in[K],w\in\{0,1\}}\left\|\mathbf{Q}_{k,w}\right\|_{\mathrm{op}}\leq 3,\quad\max_{k\in[K],w\in\{0,1\}}\left\|\mathbf{K}_{k,w}\right\|_{\mathrm{op}} \leq 1+R,\quad\sum_{k\in[K],w\in\{0,1\}}\left\|\mathbf{V}_{k,w}\right\|_{ \mathrm{op}}\leq\frac{K(N+1)}{\left|\mathcal{D}_{\mathsf{val}}\right|}.\]

Now, for every \(i,j\in[N+1]\), we have

\[\sum_{w\in\{0,1\}}\sigma(\langle\mathbf{Q}_{k,w}\mathbf{h}_{i}, \mathbf{K}_{k,w}\mathbf{h}_{j}\rangle)\mathbf{V}_{k,w}\mathbf{h}_{j}\] \[= [\sigma(f_{k}(\mathbf{x}_{j})-y_{j}-2R(1+t_{j}))-\sigma(y_{j}-f_{k }(\mathbf{x}_{j})-2R(1+t_{j}))]\cdot\frac{(N+1)}{2\left|\mathcal{D}_{\mathsf{ val}}\right|}(f_{k}(\mathbf{x}_{j})-y_{j})\mathbf{e}_{D-(K-k)-3}\] \[= 1\{t_{j}=-1\}\cdot[\sigma(f_{k}(\mathbf{x}_{j})-y_{j})-\sigma(y_{ j}-f_{k}(\mathbf{x}_{j}))]\cdot\frac{(N+1)}{2\left|\mathcal{D}_{\mathsf{val}}\right|}(f_{k}( \mathbf{x}_{j})-y_{j})\mathbf{e}_{D-(K-k)-3}\] \[= 1\{t_{j}=-1\}\cdot\frac{(N+1)}{2\left|\mathcal{D}_{\mathsf{val}} \right|}(f_{k}(\mathbf{x}_{j})-y_{j})^{2}\mathbf{e}_{D-(K-k)-3},\]

where the second equality follows from the bound \(\left|f_{k}(\mathbf{x}_{j})-y_{j}\right|\leq 2R\), so that the relus equals \(0\) if \(t_{j}\leq 0\). Thus letting the attention layer \(\boldsymbol{\theta}=\{(\mathbf{V}_{k,w},\mathbf{Q}_{k,w},\mathbf{K}_{k,w})\}_{(k, w)\in[K]\times\{0,1\}}\), we have

\[\widetilde{\mathbf{h}}_{i}=\left[\mathrm{Attn}_{\boldsymbol{\theta}}(\mathbf{H} )\right]_{i}=\mathbf{h}_{i}+\frac{1}{N+1}\sum_{j=1}^{N+1}\sum_{k,w}\sigma( \langle\mathbf{Q}_{k,w}\mathbf{h}_{i},\mathbf{K}_{k,w}\mathbf{h}_{j}\rangle) \mathbf{V}_{k,w}\mathbf{h}_{j}\]\[=\mathbf{h}_{i}+\frac{1}{2|\mathcal{D}_{\mathsf{val}}|}\sum_{j=1}^{N+1 }\sum_{k=1}^{K}(f_{k}(\mathbf{x}_{j})-y_{j})^{2}\cdot 1\{t_{j}=-1\}\mathbf{e}_{D-(K-k)-3}\] \[=\mathbf{h}_{i}+\sum_{k=1}^{K}\left(\frac{1}{2|\mathcal{D}_{ \mathsf{val}}|}\sum_{(\mathbf{x}_{j},y_{j})\in\mathcal{D}_{\mathsf{val}}}(f_{k} (\mathbf{x}_{j})-y_{j})^{2}\right)\mathbf{e}_{D-(K-k)-3}\] \[=\mathbf{h}_{i}+\sum_{k=1}^{K}\widehat{L}_{\mathsf{val}}(f_{k}) \cdot\mathbf{e}_{D-(K-k)-3}\] \[=[\mathbf{x}_{i};y^{\prime}_{i};\ast;f_{1}(\mathbf{x}_{i});\cdots ;f_{K}(\mathbf{x}_{i});\mathbf{0}_{K+1};1;t_{i}]+[\mathbf{0}_{D-K-3};\widehat {L}_{\mathsf{val}}(f_{1});\cdots;\widehat{L}_{\mathsf{val}}(f_{K});0;0;0]\] \[=[\mathbf{x}_{i};y^{\prime}_{i};\ast;f_{1}(\mathbf{x}_{i});\cdots ;f_{K}(\mathbf{x}_{i});\widehat{L}_{\mathsf{val}}(f_{1});\cdots;\widehat{L}_{ \mathsf{val}}(f_{K});0;1;t_{i}],\qquad i\in[N+1].\]

This is the desired result. 

### Proofs for Section 4.2

#### i.3.1 Proof of Lemma 13

It is straightforward to check that the binary type check \(\psi:\mathbb{R}\rightarrow\mathbb{R}\) can be expressed as a linear combination of \(6\) relu's (recalling \(\sigma(\cdot)=\mathrm{ReLU}(\cdot)\)):

\[\psi(y)=\sigma\bigg{(}\frac{y+\varepsilon}{\varepsilon}\bigg{)}-2 \sigma\Big{(}\frac{y}{\varepsilon}\Big{)}+\sigma\bigg{(}\frac{y-\varepsilon }{\varepsilon}\bigg{)}+\sigma\bigg{(}\frac{y-(1-\varepsilon)}{\varepsilon} \bigg{)}-2\sigma\bigg{(}\frac{y-1}{\varepsilon}\bigg{)}+\sigma\bigg{(}\frac{y- (1+\varepsilon)}{\varepsilon}\bigg{)}\] \[=:\sum_{m=1}^{6}a_{m}\sigma(b_{m}y+c_{m}),\]

with \(\sum_{m}|a_{m}|=8/\varepsilon\), \(\max_{m}\max\left\{|b_{m}|,|c_{m}|\right\}\leq 2\). We can thus construct an attention layer \(\bm{\theta}=\left\{(\mathbf{Q}_{m},\mathbf{K}_{m},\mathbf{V}_{m})\right\}_{m= 1}^{6}\) with \(6\) heads such that

\[\mathbf{Q}_{m}\mathbf{h}_{i}=[b_{m};c_{m};\mathbf{0}_{D-2}],\quad\mathbf{K}_{ m}\mathbf{h}_{j}=[y_{j};1;\mathbf{0}_{D-2}],\quad\mathbf{V}_{m}\mathbf{h}_{j}= \bigg{[}\frac{N+1}{N}a_{m}\cdot t_{j};\mathbf{0}_{D-1}\bigg{]},\]

which gives that for every \(i\in[N+1]\),

\[\sum_{m=1}^{6}\frac{1}{N+1}\sum_{j\in[N+1]}\sigma(\langle\mathbf{ Q}_{m}\mathbf{h}_{i},\mathbf{K}_{m}\mathbf{h}_{j}\rangle)[\mathbf{V}_{m} \mathbf{h}_{j}]_{1}\] \[=\sum_{m=1}^{6}\frac{1}{N}\sum_{j=1}^{N}\sigma(b_{m}y_{j}+c_{m})a_ {m}=\frac{1}{N}\sum_{j=1}^{N}\psi(y_{j})=\Psi^{\mathsf{binary}}(\mathcal{D}).\]

Further, we have \(\|\bm{\theta}\|\leq 18/\varepsilon=\mathcal{O}(1/\varepsilon)\). This is the desired result. 

By composing the above attention layer with one additional layer (with 2 heads) that implement the following function

\[\sigma(2(t-1/2))-\sigma(2(t-1)),\]

on the output \(\Psi^{\mathsf{binary}}(\mathcal{D})\), we directly obtain the following corollary.

**Corollary I.1** (Thresholded binary test).: _There exists a two-layer attention-only transformer with \(\max_{\ell\in[2]}M^{(\ell)}\leq 6\) and \(\|\bm{\theta}\|\leq\mathcal{O}(1/\varepsilon)\) that exactly implements the thresholded binary test_

\[\Psi^{\mathsf{binary}}_{\mathsf{thres}}(\mathcal{D}):=\begin{cases}1,&\text{ if }\Psi^{\mathsf{binary}}(\mathcal{D})\geq 1,\\ 0,&\text{ if }\Psi^{\mathsf{binary}}(\mathcal{D})\leq\frac{1}{2},\\ \text{linear interpolation},&\text{ o.w.}\end{cases}\] (43)

_at every token \(i\in[N+1]\), where we recall the definition of \(\Psi^{\mathsf{binary}}\) in Lemma 13._

[MISSING_PAGE_FAIL:68]

we obtain a single transformer \(\bm{\theta}\) with

\[L\leq\mathcal{O}\bigg{(}\kappa\log\frac{B_{x}B_{w}}{\varepsilon} \bigg{)},\quad\max_{t\in[L]}M^{(\ell)}\leq\mathcal{O}\bigg{(}\bigg{(}1+\frac{B_{ x}^{4}}{\alpha^{2}}\bigg{)}\varepsilon^{-2}\bigg{)},\quad\|\bm{\theta}_{\rm LS}\|\leq \mathcal{O}\bigg{(}R+\frac{1}{\beta}+\frac{1}{\varepsilon}\bigg{)},\]

which outputs (44) as its prediction (at the location for \(\widehat{y}_{N+1}\)).

It remains to show that (44) reduces to either one of \(\widehat{\mathfrak{I}}_{N+1}^{\rm log}\) or \(\widehat{\mathfrak{I}}_{N+1}^{\rm LS}\). When the data are binary (\(y_{i}\in\{0,1\}\)), we have \(\Psi^{\text{binary}}(\mathcal{D})=1\) and \(\Psi^{\text{binary}}_{\text{thres}}(\mathcal{D})=1\), in which case (44) becomes exactly \(\widehat{y}_{N+1}^{\rm log}\). By contrast, when data is sampled from a distribution that is \((C,\varepsilon_{0})\)-not-concentrated around \(\{0,1\}\), we have for any fixed \(\varepsilon\leq\varepsilon_{0}\wedge\frac{1}{4C}\) that, letting \(B_{\varepsilon}:=[-\varepsilon,\varepsilon]\cup[1-\varepsilon,1+\varepsilon]\) and \(\mathfrak{p}_{\varepsilon}:=\mathsf{P}_{y}(B_{\varepsilon})\leq C\varepsilon \leq\frac{1}{4}\), by Hoeffding's inequality,

\[\mathsf{P}(\Psi^{\text{binary}}_{\text{thres}}(\mathcal{D})\neq 0)= \mathsf{P}\bigg{(}\Psi^{\text{binary}}_{\text{thres}}(\mathcal{D})\geq\frac{1 }{2}\bigg{)}=\mathsf{P}\Bigg{(}\frac{1}{N}\sum_{i=1}^{N}1\{y_{j}\in B_{ \varepsilon}\}\geq\frac{1}{2}\Bigg{)}\] \[\leq\exp\Big{(}-c(1/2-\mathfrak{p}_{\varepsilon})^{2}N\Big{)} \leq\exp(-c^{\prime}N),\]

where \(c^{\prime}>0\) is an absolute constant. On the event \(\Psi^{\text{binary}}_{\text{thres}}(\mathcal{D})=0\) (which happens with probability at least \(1-\exp(-c^{\prime}N)\)), (44) becomes exactly \(\widehat{y}_{N+1}^{\rm LS}\). This finishes the proof. 

### Linear correlation test and application

In this section, we give another instantiation of the pre-ICL testing mechanism by showing that the transformer can implement a _linear correlation test_ that tests whether the correlation vector \(\mathbb{E}[\mathbf{x}y]\) has a large norm. We then use this test to construct a transformer to perform "confident linear regression", i.e. output a prediction from linear regression only when the signal-to-noise ratio is high.

For any fixed parameters \(\lambda_{\min},B_{w}^{\star}>0\), consider the linear correlation test over data \(\mathcal{D}\) defined as

\[\Psi^{\text{lin}}(\mathcal{D}) :=\frac{1}{\lambda_{\min}^{2}(B_{w}^{\star})^{2}/2}\cdot\Big{[} \sigma\Big{(}\|\widehat{\mathbf{t}}\|_{2}^{2}-(\lambda_{\min}B_{w}^{\star}/4) ^{2}\Big{)}-\sigma\Big{(}\|\widehat{\mathbf{t}}\|_{2}^{2}-(3\lambda_{\min}B_{ w}^{\star}/4)^{2}\Big{)}\Big{]}\] \[=\begin{cases}0,&\|\widehat{\mathbf{t}}\|_{2}^{2}\leq(\lambda_{ \min}B_{\star}^{\star}/4)^{2},\\ 1,&\|\widehat{\mathbf{t}}\|_{2}^{2}\geq(3\lambda_{\min}B_{w}^{\star}/4)^{2},\\ \text{linear interpolation},&\text{o.w.},\end{cases}\] (45) \[{\rm where}\ \widehat{\mathbf{t}} =\mathbf{T}(\mathcal{D}):=\frac{1}{N}\sum_{i=1}^{N}\mathbf{x}_{i }y_{i}.\]

Recall that \(\sigma(\cdot)={\rm ReLU}(\cdot)\) above denotes the relu activation.

We show that \(\Psi^{\text{lin}}\) can be exactly implemented by a 3-layer transformer.

**Lemma I.1** (Expressing \(\Psi^{\text{lin}}\) by transformer).: _There exists a 3-layer attention-only transformer \({\rm TF}_{\bm{\theta}}\) with at most \(2\) heads per layer and \(\|\bm{\theta}\|\leq\mathcal{O}(1+\lambda_{\min}^{2}(B_{w}^{\star})^{2})\) such that on input sequence \(\mathbf{H}\) of the form (3) with \(D\geq 2d+4\), the transformer exactly implements \(\Psi^{\text{lin}}\): it outputs \(\widetilde{\mathbf{H}}\) such that \(\widetilde{\mathbf{h}}_{i}=[\mathbf{x}_{i};y_{i}t_{i};*;\Psi^{\text{lin}}( \mathcal{D});1]\) for all \(i\in[N+1]\)._

Proof.: We begin by noting the following basic facts:

* Identity function can be implemented exactly by two ReLUs: \(t=\sigma(t)-\sigma(-t)\).
* Squared \(\ell_{2}\) norm can be implemented exactly by a single attention head (assuming every input \(\mathbf{h}_{i}\) contains the same vector \(\mathbf{g}\)): \(\|\mathbf{g}\|_{2}^{2}=\sigma(\langle\mathbf{g},\mathbf{g}\rangle)\).

We construct the transformer \(\bm{\theta}\) as follows.

Layer 1: Use 2 heads to implement \(\widehat{\mathbf{t}}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{x}_{i}y_{i}\), where \(\mathbf{V}_{\{1,2\}}^{(1)}\mathbf{h}_{j}=[\pm\mathbf{x}_{j};\mathbf{0}_{D-d}]\), \(\mathbf{Q}_{\{1,2\}}^{(1)}\mathbf{h}_{i}=[\frac{N+1}{N};\mathbf{0}_{D-1}]\), and \(\mathbf{K}_{\{1,2\}}^{(1)}\mathbf{h}_{j}=[\pm y_{j}t_{j};\mathbf{0}_{D-1}]=[ \pm y_{j}1\{j<N+1\};\mathbf{0}_{D-1}]\) (where we recall \(t_{j}=1\{j<N+1\}\) and note that \(y_{j}t_{j}\) corresponds exactly to the location for \(y_{j}\) in \(\mathbf{H}\), cf. (3)).

[MISSING_PAGE_FAIL:70]

On the above event, we have

\[\left\|\widehat{\mathbf{t}}\right\|_{2}=\left\|\frac{1}{N}\sum_{i=1}^{N}\mathbf{x }_{i}(\langle\mathbf{x}_{i},\mathbf{w}_{\mathbf{p}}^{\star}\rangle+\mathbf{z}_{ i})\right\|_{2}=\left\|\widehat{\mathbf{\Sigma}}\mathbf{w}_{\mathbf{p}}^{\star}+ \frac{1}{N}\sum_{i=1}^{N}\mathbf{x}_{i}z_{i}\right\|_{2}.\]

Therefore, in case 1, we have

\[\left\|\widehat{\mathbf{t}}\right\|_{2}\geq\left\|\widehat{\mathbf{\Sigma}} \mathbf{w}_{\mathbf{p}}^{\star}\right\|_{2}-\left\|\frac{1}{N}\sum_{i=1}^{N} \mathbf{x}_{i}z_{i}\right\|_{2}\geq 0.9\lambda_{\min}\left\|\mathbf{w}_{ \mathbf{p}}^{\star}\right\|_{2}-\frac{\lambda_{\min}B_{w}^{\star}}{8}\geq\frac {3\lambda_{\min}B_{w}^{\star}}{4}.\]

In case 2, we have

\[\left\|\widehat{\mathbf{t}}\right\|_{2}\leq\left\|\widehat{\mathbf{\Sigma}} \mathbf{w}_{\mathbf{p}}^{\star}\right\|_{2}+\left\|\frac{1}{N}\sum_{i=1}^{N} \mathbf{x}_{i}z_{i}\right\|_{2}\leq\lambda_{\max}\cdot\frac{\lambda_{\min}B_{ w}^{\star}}{10\lambda_{\max}}+\frac{\lambda_{\min}B_{w}^{\star}}{8}\leq\frac{ \lambda_{\min}B_{w}^{\star}}{4}.\]

The proof is finished by recalling the definition of \(\Psi^{\text{lin}}\) in (45), so that \(\Psi^{\text{lin}}(\mathcal{D})=1\) if \(\|\widehat{\mathbf{t}}\|_{2}\geq 3\lambda_{\min}B_{w}^{\star}/4\), and \(\Psi^{\text{lin}}(\mathcal{D})=0\) if \(\|\widehat{\mathbf{t}}\|_{2}\leq\lambda_{\min}B_{w}^{\star}/4\). 

Application: Confident linear regressionBy directly composing the linear correlation test in Lemma I.1 with the transformer construction in Corollary 5 (using an argument similar as the proof of Proposition I.4), and using the power of the linear correlation test Proposition I.5, we immediately obtain the following result, which outputs a prediction from (approximately) least squares if \(\widehat{\psi}:=\Psi^{\text{lin}}(\mathcal{D})=1\), and abstains from predicting if \(\widehat{\psi}=0\). This can be viewed as a form of "confident linear regression", where the model predicts only if it thinks the linear signal is strong enough.

**Proposition I.6** (Confident linear regression).: _For any \(B_{w}>0\), \(0<B_{w}^{\star}\leq\overline{B_{w}^{\star}}\), \(0\leq\lambda_{\min}\leq\lambda_{\max}\), \(\varepsilon\leq B_{x}B_{w}/10\), \(0<\alpha\leq\beta\) with \(\kappa:=\beta/\alpha\), there exists a \(L\)-layer attention-only transformer with_

\[L\leq\mathcal{O}\bigg{(}\kappa\log\frac{B_{x}B_{w}}{\varepsilon}\bigg{)}, \quad\max_{\ell\in[L]}M^{(\ell)}\leq\mathcal{O}(1),\quad\left\|\boldsymbol{ \theta}\right\|\leq\mathcal{O}\bigg{(}R+\frac{1}{\beta}+\lambda_{\min}^{2}(B_ {w}^{\star})^{2}\bigg{)}\]

_(with \(R:=\max\left\{B_{x}B_{w},B_{y},1\right\}\)) such that the following holds. Let \(N\geq\widetilde{\mathcal{O}}\Big{(}\max\left\{K^{4},\frac{\lambda_{\max}K^{2} \sigma^{2}}{(B_{w}^{\star})^{2}\lambda_{\min}^{2}}\right\}\cdot d\Big{)}\). Suppose the input format is (3) with dimension \(D\geq 2d+4\). Let ICL instance \((\mathcal{D},\mathbf{x}_{N+1})\) be drawn from any distribution \(\mathsf{P}\) satisfying Assumption D. Then the transformer outputs a 2-dimensional prediction (within the test token \(\widetilde{\mathbf{h}}_{N+1}\))_

\[(\widehat{y}_{N+1},\widehat{\psi})\in\mathbb{R}\times\{0,1\}\]

_such that the following holds:_

1. _If_ \(\|\mathbf{w}_{\mathbf{p}}^{\star}\|_{2}\geq B_{w}^{\star}\)_, then with probability at least_ \(1-\delta\) _over_ \(\mathcal{D}\)_, we have_ \(|\widehat{y}_{N+1}-\langle\widehat{\mathbf{w}}_{\mathrm{LS}},\mathbf{x}_{N+1 }\rangle\mid\leq\varepsilon\)_, and_ \(\widehat{\psi}=1\) _if_ \(\mathcal{D}\) _is in addition well-conditioned for least squares (in the sense of (_5_) with_ \(\lambda=0\)_)._
2. _If_ \(\|\mathbf{w}_{\mathbf{p}}^{\star}\|_{2}\leq\frac{\lambda_{\min}}{10\lambda_{ \max}}B_{w}^{\star}\)_, then with probability at least_ \(1-\delta\) _over_ \(\mathcal{D}\)_, we have_ \(\widehat{y}_{N+1}=0\) _and_ \(\widehat{\psi}=0\)_._

## Appendix J Proof of Theorem 12: Noisy linear model with mixed noise levels

For each fixed \(k\in[K]\), we consider the following data generating model \(\mathbb{P}_{k}\), where we first sample \(\mathsf{P}=\mathsf{P}_{\mathbf{w}_{\star},\sigma_{k}}\sim\pi\) from \(\mathbf{w}_{\star}\sim\mathsf{N}(\mathbf{0},\mathbf{I}_{d}/d)\), and then sample data \(\{(\mathbf{x}_{i},y_{i})\}_{i\in[N+1]}\stackrel{{\mathrm{iid}}}{{ \sim}}\mathsf{P}_{\mathbf{w}_{\star},\sigma_{k}}\) as

\[\mathsf{P}_{\mathbf{w}_{\star},\sigma_{k}}:\mathbf{x}_{i}\sim\mathsf{N}( \mathbf{0},\mathbf{I}_{d}),\quad y_{i}=\langle\mathbf{x}_{i},\mathbf{w}_{\star} \rangle+\varepsilon_{i},\quad\varepsilon_{i}\sim\mathsf{N}(0,\sigma_{k}^{2}).\]

Also, recall that the Bayes optimal estimator on \(\mathbb{P}_{k}\) is given by \(\widehat{y}_{N+1}^{\mathsf{Bayes}}=\left\langle\mathbf{w}_{\mathrm{ridge}}^{ \lambda_{k}}(\mathcal{D}),\mathbf{x}_{N+1}\right\rangle\) with ridge \(\lambda_{k}=\sigma_{k}^{2}d/N\), and the Bayes risk on \(\mathbb{P}_{k}\) is given by

\[\mathsf{BayesRisk}_{k}:=\inf_{\mathcal{A}}\mathbb{E}_{k}\big{[}\tfrac{1}{2}( \mathcal{A}(\mathcal{D})(\mathbf{x}_{N+1})-y_{N+1})^{2}\big{]}=\mathbb{E}_{k} \Big{[}\tfrac{1}{2}\big{(}\widehat{y}_{N+1}^{\mathsf{Bayes}}-y_{N+1}\big{)}^{2} \Big{]}.\]Recall that in Section 4.1.1, we consider a mixture law \(\mathbb{P}_{\pi}\) that generates data from \(\mathbb{P}_{k}\) with \(k\sim\Lambda\). It is clear that we have (pushing \(\inf_{\mathcal{A}}\) into \(\mathbb{E}_{k\sim\Lambda}\) does not increase the value) we have

\[\mathsf{BayesRisk}_{\pi}\geq\mathbb{E}_{k\sim\Lambda}[\mathsf{BayesRisk}_{k}],\]

i.e., the Bayes risk can only be greater if we consider a mixture of models. In other words, if a transformer can achieve near-Bayes ICL on each meta-task \(\mathbb{P}_{k}\), then it can perform near-Bayes ICL on any meta-task \(\pi\) which is a mixture of \(\mathbb{P}_{k}\) with \(k\sim\Lambda\). Therefore, to prove Theorem 12, it suffices to show the following (strengthened) result.

**Theorem J.1** (Formal version of Theorem 12).: _Suppose that \(N\geq 0.1d\) and we write \(\sigma_{\max}=\max_{k}\{\sigma_{k},1\},\sigma_{\min}=\min_{k}\{\sigma_{k},1\}\). Suppose in input format (3) we have \(D\geq\Theta(Kd)\). Then there exists a transformer \(\bm{\theta}\) with_

\[L\leq\mathcal{O}\left(\sigma_{\min}^{-2}\log(N/\sigma_{\min}) \right),\qquad\max_{\ell\in[L]}M^{(\ell)}\leq\mathcal{O}\left(K\right),\qquad \max_{\ell\in[L]}D^{(\ell)}\leq\mathcal{O}(K^{2}),\] \[\left\|\bm{\theta}\right\|\leq\mathcal{O}\left(\sigma_{\max}Kd \log(N)\right),\]

_such that for any \(k\in[K]\), it holds that_

\[\mathbb{E}_{k}\bigg{[}\frac{1}{2}(y_{N+1}-\widehat{y}_{N+1})^{2} \bigg{]}\leq\mathsf{BayesRisk}_{k}+\widetilde{\mathcal{O}}\left(\frac{\sigma_ {\max}^{2}}{\sigma_{\min}^{2/3}}\Big{(}\frac{\log K}{N}\Big{)}^{1/3}\right)\]

_if we choose \(N_{\mathsf{val}}:=|\mathcal{D}_{\mathsf{val}}|\asymp N^{2/3}[\log K]^{1/3}\)._

The core of the proof of Theorem J.1 is to show that any estimator \(\widehat{\mathbf{w}}\) that achieves small validation loss \(\widehat{L}_{\mathsf{val}}\) must achieve small population loss.

Throughout the rest of this section, recall that we define \(N_{\text{train}}=|\mathcal{D}_{\text{train}}|\,,\)\(N_{\mathsf{val}}=|\mathcal{D}_{\mathsf{val}}|\), \(\mathcal{I}_{\text{train}}=\{i:(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{\text{ train}}\}\), \(\mathcal{I}_{\text{val}}=\{i:(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{\mathsf{val}}\}\), and \(\mathbf{X}_{\text{train}}=[\mathbf{x}_{i}]_{i\in\mathcal{I}_{\text{train}}}\).

### Proof of Theorem j.1

Fix parameters \(\delta,\varepsilon,\gamma>0\) and a large universal constant \(C_{0}\). Let us set

\[\alpha=\max\Big{\{}0,1/2-\sqrt{d/N_{\text{train}}}\Big{\}}^{2}, \qquad\beta=25,\]

\[B_{w}^{\star}=1+C_{0}\sqrt{\frac{\log(N)}{d}},\qquad B_{w}=C_{0}(B_{w}^{\star }+\sigma_{\max}/\sigma_{\min}),\]

\[B_{x}=C_{0}\sqrt{d\log(N)},\qquad B_{y}=C_{0}(B_{w}^{\star}+\sigma_{\max}) \sqrt{\log(N)},\]

Then, we define good events similarly to the proof of Corollary 6 (Appendix F.4):

\[\mathcal{E}_{\pi} =\{\|\mathbf{w}_{\star}\|_{2}\leq B_{w}^{\star},\|\bm{\varepsilon }\|_{2}\leq 2\sigma_{\max}\sqrt{N}\},\] \[\mathcal{E}_{w} =\{\alpha\leq\lambda_{\min}(\mathbf{X}_{\text{train}}^{\top} \mathbf{X}_{\text{train}}/N_{\text{train}})\leq\lambda_{\max}(\mathbf{X}_{ \text{train}}^{\top}\mathbf{X}_{\text{train}}/N_{\text{train}})\leq\beta\},\] \[\mathcal{E}_{b,\text{train}} =\{\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{\text{train}}, \|\mathbf{x}_{i}\|_{2}\leq B_{x},|y_{i}|\leq B_{y}\},\] \[\mathcal{E}_{b,N+1} =\{\|\mathbf{x}_{N+1}\|_{2}\leq B_{x},|y_{N+1}|\leq B_{y}\}.\]

For the good event \(\mathcal{E}:=\mathcal{E}_{\pi}\cap\mathcal{E}_{w}\cap\mathcal{E}_{b,\text{ train}}\cap\mathcal{E}_{b,\text{test}}\cap\mathcal{E}_{b,N+1}\), we can show that \(\mathbb{P}(\mathcal{E}^{c})\leq\mathcal{O}\left(N^{-10}\right)\). Further, by the proof of Lemma F.1 (see e.g. (34)), we know that \(\max_{k\in[K]}\big{\|}\mathbf{w}_{\text{ridge}}^{\lambda_{k}}(\mathcal{D}_{ \text{train}})\big{\|}_{2}\leq B_{w}/2\) holds under the good event \(\mathcal{E}\).

For the ridge \(\lambda_{k}=\frac{d\sigma_{k}^{2}}{N_{\text{train}}}\) and parameters \((\alpha,\beta,\gamma,\varepsilon)\), we consider the transformer \(\bm{\theta}\) constructed in Theorem I.2, with a clipped prediction \(\widehat{y}_{N+1}=\widetilde{\mathsf{read}}_{\text{y}}(\operatorname{TF}_{ \bm{\theta}}(\mathbf{H}))\).

In the following, we upper bound the quantity \(\mathbb{E}_{k}(\widehat{y}_{N+1}-y_{N+1})^{2}\) for any fixed \(k\). Similar to the proof of Corollary 6 (Appendix F.4), we decompose

\[\mathbb{E}_{k}(\widehat{y}_{N+1}-y_{N+1})^{2}=\mathbb{E}_{k}\big{[}1\{\mathcal{E} \}(\widehat{y}_{N+1}-y_{N+1})^{2}\big{]}+\mathbb{E}_{k}\big{[}1\{\mathcal{E}^ {c}\}(\widehat{y}_{N+1}-y_{N+1})^{2}\big{]},\]

and we analyze these two parts separately.

Part I.Recall that by our construction, when \(\mathcal{E}\) holds, we have \(\widehat{y}_{N+1}=\mathsf{clip}_{B_{y}}(\langle\widehat{\mathbf{w}},\mathbf{x}_ {N+1}\rangle)\) and the statements of Theorem I.2 hold for \(\widehat{\mathbf{w}}\). Thus, we have

\[\mathbb{E}_{k}\big{[}1\{\mathcal{E}\}(\widehat{y}_{N+1}-y_{N+1})^ {2}\big{]} =\mathbb{E}_{k}\Big{[}1\{\mathcal{E}\}(\mathsf{clip}_{B_{y}}( \langle\mathbf{x}_{N+1},\widehat{\mathbf{w}}\rangle)-y_{N+1})^{2}\Big{]}\] \[\leq\mathbb{E}_{k}\big{[}1\{\mathcal{E}\}(\langle\mathbf{x}_{N+1},\widehat{\mathbf{w}}\rangle-y_{N+1})^{2}\big{]}.\]

Let us consider the following risk functional

\[L_{\mathsf{val},\mathbf{w}_{*}}(\mathbf{w})=\mathbb{E}_{(\mathbf{x},y)\sim \mathcal{P}_{\mathbf{w}_{*},\sigma_{k}}}\Big{[}\tfrac{1}{2}(\langle\mathbf{w},\mathbf{x}\rangle-y)^{2}\Big{]}=\tfrac{1}{2}\Big{(}\big{\|}\mathbf{w}- \mathbf{w}_{*}\big{\|}_{2}^{2}+\sigma_{k}^{2}\Big{)}.\]

Then, under the good event \(\mathcal{E}_{0}:=\mathcal{E}_{\pi}\cap\mathcal{E}_{w}\cap\mathcal{E}_{b, \text{train}}\cap\mathcal{E}_{b,\text{test}}\) of \((\mathbf{w}_{*},\mathcal{D})\),

\[\mathbb{E}_{k}\left[\left.1\{\mathcal{E}\}(\langle\mathbf{x}_{N+ 1},\widehat{\mathbf{w}}\rangle-y_{N+1})^{2}\right|\mathbf{w}_{*},\mathcal{D}\right] =\mathbb{E}_{k}\left[\left.1\{\mathcal{E}\}(\langle\mathbf{x}_{N +1},\widehat{\mathbf{w}}(\mathcal{D})\rangle-y_{N+1})^{2}\right|\mathbf{w}_{* },\mathcal{D}\right]\] \[=\mathbb{E}_{(\mathbf{x},y)\sim\mathcal{P}_{\mathbf{w}_{*}, \sigma_{k}}}\big{[}(\langle\mathbf{x}_{N+1},\widehat{\mathbf{w}}(\mathcal{D}) \rangle-y_{N+1})^{2}\big{]}\] \[=L_{\mathsf{val},\mathbf{w}_{*}}(\widehat{\mathbf{w}}(\mathcal{D})).\]

By our construction, under the good event \(\mathcal{E}_{0}\), we have

\[L_{\mathsf{val},\mathbf{w}_{*}}(\widehat{\mathbf{w}}(\mathcal{D}))\leq L_{ \mathsf{val},\mathbf{w}_{*}}(\widehat{\mathbf{w}}_{k}(\mathcal{D}_{\text{train }}))+\max_{l\in[K]}\left|\widehat{L}_{\mathsf{val}}(\widehat{\mathbf{w}}_{l}( \mathcal{D}_{\text{train}}))-L_{\mathsf{val},\mathbf{w}_{*}}(\widehat{ \mathbf{w}}_{l}(\mathcal{D}_{\text{train}}))\right|+\gamma,\]

where \(\big{\|}\widehat{\mathbf{w}}_{l}(\mathcal{D}_{\text{train}}))-\mathbf{w}_{ \mathrm{ridge}}^{\lambda_{l}}(\mathcal{D}_{\text{train}})\big{\|}_{2}\leq\varepsilon\) for each \(l\in[K]\). Clearly,

\[2\mathbb{E}_{k}[1\{\mathcal{E}_{0}\}L_{\mathsf{val},\mathbf{w}_{ *}}(\widehat{\mathbf{w}}_{k}(\mathcal{D}_{\text{train}}))]=\mathbb{E}_{k} \Big{[}1\{\mathcal{E}_{0}\}\Big{(}\|\widehat{\mathbf{w}}_{k}(\mathcal{D}_{ \text{train}})-\mathbf{w}_{*}\big{\|}_{2}^{2}+\sigma_{k}^{2}\Big{)}\Big{]}\] \[\leq\mathbb{E}_{k}\Big{[}1\{\mathcal{E}_{0}\}\Big{(}\big{\|} \mathbf{w}_{\mathrm{ridge}}^{\lambda_{k}}(\mathcal{D}_{\text{train}})-\mathbf{ w}_{*}\big{\|}_{2}^{2}+2\varepsilon\big{\|}\mathbf{w}_{\mathrm{ridge}}^{ \lambda_{k}}(\mathcal{D}_{\text{train}})-\mathbf{w}_{*}\big{\|}_{2}+ \varepsilon^{2}\Big{)}\Big{]}+\sigma_{k}^{2}\] \[\leq 2\mathsf{Risk}_{k,\text{train}}+2\varepsilon\sqrt{2\mathsf{ Risk}_{k,\text{train}}}+\varepsilon^{2},\]

where we denote \(2\mathsf{Risk}_{k,\text{train}}=\mathbb{E}_{k}\big{\|}\mathbf{w}_{\mathrm{ ridge}}^{\lambda_{k}}(\mathcal{D}_{\text{train}})-\mathbf{w}_{*}\big{\|}_{2}^{2}+ \sigma_{k}^{2}\), and we also note that \(\mathsf{Risk}_{k,\text{train}}\leq 1+\sigma_{k}^{2}\) by definition. By Lemma J.1, we have

\[\mathsf{Risk}_{k,\text{train}}\leq\mathsf{BayesRisk}_{k}+\mathcal{O}\left(( \sigma_{k}^{2}+1)\frac{N_{\mathsf{val}}}{N}\right).\]

We next deal with the term \(\varepsilon_{\mathsf{val}}:=\max_{l\in[K]}\big{|}\widehat{L}_{\mathsf{val}}( \widehat{\mathbf{w}}_{l}(\mathcal{D}_{\text{train}}))-L_{\mathsf{val},\mathbf{w} _{*}}(\widehat{\mathbf{w}}_{l}(\mathcal{D}_{\text{train}}))\big{|}\). Note that for the good event \(\mathcal{E}_{\text{train}}:=\mathcal{E}_{\pi}\cap\mathcal{E}_{w}\cap \mathcal{E}_{b,\text{train}}\) of \((\mathbf{w}_{*},\mathcal{D}_{\text{train}})\), we have

\[\mathbb{E}_{k}[1\{\mathcal{E}_{0}\}\varepsilon_{\mathsf{val}}]\leq\mathbb{E}_ {k}[1\{\mathcal{E}_{\text{train}}\}\varepsilon_{\mathsf{val}}]\leq\mathbb{E}_ {\mathbf{w}_{*},\mathcal{D}_{\text{train}}\cap\mathcal{E}_{b}}[1\{\mathcal{E}_{ \text{train}}\}\cdot\mathbb{E}_{\mathcal{D}_{\text{val}}}\left[\varepsilon_{ \mathsf{val}}\right]\mathbf{w}_{*},\mathcal{D}_{\text{train}}]].\]

Thus, Lemma J.2 yields

\[\mathbb{E}_{k}[1\{\mathcal{E}_{0}\}\varepsilon_{\mathsf{val}}]\leq\mathcal{O} \left(B_{w}^{2}\right)\cdot\Bigg{[}\sqrt{\frac{\log K}{N_{\mathsf{val}}}}+ \frac{\log K}{N_{\mathsf{val}}}\Bigg{]}.\]

Therefore, we can conclude that

\[\mathbb{E}_{k}\big{[}1\{\mathcal{E}\}(\widehat{y}_{N+1}-y_{N+1})^{2}\big{]}\leq 2 \mathsf{BayesRisk}_{k}+\mathcal{O}\left(\varepsilon\sigma_{\max}+\varepsilon^{2}+ \frac{\sigma_{\max}^{2}N_{\mathsf{val}}}{N}+B_{w}^{2}\sqrt{\frac{\log K}{N_{ \mathsf{val}}}}+\frac{B_{w}^{2}\log K}{N_{\mathsf{val}}}\right).\]

Therefore, we can choose \((\varepsilon,N_{\mathsf{val}})\) so that \(N_{\mathsf{val}}\leq N/2\) as

\[N_{\mathsf{val}}=\max\Bigg{\{}\bigg{(}\frac{B_{w}^{2}}{\sigma_{\max}^{2}}N\bigg{)} ^{2/3}\log^{1/3}(K),\log K\Bigg{\}},\qquad\varepsilon=\frac{\sigma_{\max}}{N}.\]

It is worth noting that such choice of \(N_{\mathsf{val}}\) is feasible as long as \(N\gtrsim\frac{B_{w}^{4}}{\sigma_{\max}^{4/3}}\log K\). Under such choice, we obtain

\[\frac{1}{2}\mathbb{E}_{k}\big{[}1\{\mathcal{E}\}(\widehat{y}_{N+1}-y_{N+1})^{2} \big{]}\leq\mathsf{BayesRisk}_{k}+\mathcal{O}\left(\sigma_{\max}^{4/3}B_{w}^{ 2/3}\bigg{(}\frac{\log K}{N}\bigg{)}^{1/3}\right).\]Part II.Similar to the proof of Corollary 6, we have

\[\mathbb{E}\big{[}1\{\mathcal{E}^{c}\}(\widehat{y}_{N+1}-y_{N+1})^{2}\big{]}\leq \mathcal{O}\left(\frac{B_{y}^{2}}{N^{5}}\right)\leq\mathcal{O}\left(\frac{\sigma _{\max}^{2}}{N^{4}}\right).\]

Conclusion.Combining the both cases, we obtain

\[\mathbb{E}_{k}\big{[}\tfrac{1}{2}(y_{N+1}-\widehat{y}_{N+1})^{2} \big{]} \leq\mathsf{BayesRisk}_{k}+\mathcal{O}\left(\sigma_{\max}^{4/3}B _{w}^{2/3}\Big{(}\tfrac{\log K}{N}\Big{)}^{1/3}\right)\] \[\leq\mathsf{BayesRisk}_{k}+\mathcal{O}\left(\tfrac{\sigma_{\max}^ {2}}{\sigma_{\min}^{2/3}}\Big{(}\tfrac{\log K}{N}\Big{)}^{1/3}+\sigma_{\max}^ {4/3}\tfrac{\log^{2/3}(N)\log^{1/3}(K)}{d^{2/3}N^{1/3}}\right)\] \[\leq\mathsf{BayesRisk}_{k}+\widetilde{\mathcal{O}}\left(\tfrac{ \sigma_{\max}^{2}}{\sigma_{\min}^{2/3}}\Big{(}\tfrac{\log K}{N}\Big{)}^{1/3} \right),\]

where we plug in our choice of \(B_{y}\). The bounds on \(M^{(\ell)},D^{(\ell)}\) and \(\left\lVert\boldsymbol{\theta}\right\rVert\) follows immediately from Theorem I.2. This completes the proof. 

### Derivation of the exact Bayes predictor

Let \((\mathcal{D},\mathbf{x}_{N+1},y_{N+1})\) be \((N+1)\) observations from the data generating model \(\pi\) considered in Section 4.1.1. On observing \((\mathcal{D},\mathbf{x}_{N+1})\), the Bayes predictor of \(y_{N+1}\) is given by its posterior mean:

\[\mathbb{E}_{\pi}[y_{N+1}|\mathcal{D},\mathbf{x}_{N+1}]=\mathbb{E}_{\pi}[ \left\langle\mathbf{x}_{N+1},\,\mathbf{w}_{\star}\right\rangle+\varepsilon_{N +1}|\mathcal{D},\mathbf{x}_{N+1}]=\left\langle\mathbf{x}_{N+1},\mathbb{E}_{\pi }[\left\langle\mathbf{w}_{\star}|\mathcal{D}\right\rangle]\right\rangle.\]

It thus remains to derive \(\mathbb{E}_{\pi}[\mathbf{w}_{\star}|\mathcal{D}]\). Recall that our data generating model is given by \(k\sim\Lambda\), By Bayes' rule, we have

\[\mathbb{E}_{\pi}[\mathbf{w}_{\star}|\mathcal{D}]=\sum_{k^{\prime}\in[K]} \mathbb{P}_{\pi}(k=k^{\prime}|\mathcal{D})\cdot\mathbb{E}_{\pi}[\mathbf{w}_{ \star}|\mathcal{D},k=k^{\prime}].\] (46)

On \(k=k^{\prime}\), the data is generated from the noisy linear model \(\mathbf{w}_{\star}\sim\mathsf{N}(\mathbf{0},\mathbf{I}_{d}/d)\), and \(\mathbf{y}=\mathbf{X}\mathbf{w}_{\star}+\boldsymbol{\varepsilon}\) where \(\varepsilon_{i}\stackrel{{\mathrm{iid}}}{{\sim}}\mathsf{N}(0,\sigma _{k^{\prime}}^{2})\). It is a standard result that \(\mathbb{E}_{\pi}[\mathbf{w}_{\star}|\mathcal{D},k=k^{\prime}]\) is given by the ridge estimator

\[\mathbb{E}_{\pi}[\mathbf{w}_{\star}|\mathcal{D},k=k^{\prime}]= \underbrace{\big{(}\mathbf{X}^{\top}\mathbf{X}+d\sigma_{k^{\prime}}^{2}\big{)} ^{-1}}_{\widehat{\mathbf{x}}_{k^{\prime}}^{\top}}\mathbf{X}^{\top}\mathbf{y}= :\widehat{\mathbf{w}}_{k^{\prime}}\] \[=\left(\frac{\mathbf{X}^{\top}\mathbf{X}}{N}+\frac{d\sigma_{k^{ \prime}}^{2}}{N}\right)^{-1}\frac{\mathbf{X}^{\top}\mathbf{y}}{N}.\]

(Note that the sample covariance within \(\widehat{\mathbf{\Sigma}}_{k^{\prime}}\) is not normalized by \(N\), which is not to be confused with remaining parts within the paper.) Therefore, the posterior mean (46) is exactly a weighted combination of \(K\) ridge regression estimators, each with regularization \(d\sigma_{k}^{2}/N\).

It remains to derive the mixing weights \(\mathbb{P}_{\pi}(k=k^{\prime}|\mathcal{D})\) for all \(k^{\prime}\in[K]\). By Bayes' rule, we have

\[\mathbb{P}_{\pi}(k=k^{\prime}|\mathcal{D})\propto_{k^{\prime}} \mathbb{P}_{\pi}(k=k^{\prime})\cdot\int_{\mathbf{w}_{\star}}p(\mathbf{w}_{ \star})\cdot\mathsf{p}_{k^{\prime},\mathbf{w}_{\star}}(\mathcal{D}|\mathbf{w}_ {\star})d\mathbf{w}_{\star}\] \[\propto\Lambda_{k^{\prime}}\cdot\int_{\mathbf{w}}\frac{1}{(2\pi d )^{d/2}(2\pi\sigma_{k^{\prime}}^{2})^{N/2}}\exp\left(-\frac{d\|\mathbf{w}\|_{2}^ {2}}{2}-\frac{\|\mathbf{X}\mathbf{w}-\mathbf{y}\|_{2}^{2}}{2\sigma_{k^{\prime} }^{2}}\right)d\mathbf{w}\] \[\propto\Lambda_{k^{\prime}}\cdot\int_{\mathbf{w}}\frac{1}{(2\pi \sigma_{k^{\prime}}^{2})^{N/2}}\exp\left(-\frac{1}{2}\mathbf{w}^{\top}\bigg{(} \frac{\mathbf{X}^{\top}\mathbf{X}}{\sigma_{k^{\prime}}^{2}}+d\mathbf{I}_{d} \bigg{)}\mathbf{w}+\left\langle\mathbf{w},\frac{\mathbf{X}^{\top}\mathbf{y}}{ \sigma_{k^{\prime}}^{2}}\right\rangle-\frac{\|\mathbf{y}\|_{2}^{2}}{2\sigma_{k^{ \prime}}^{2}}\right)d\mathbf{w}\] \[\propto\Lambda_{k^{\prime}}\cdot\int_{\mathbf{w}}\frac{1}{(2\pi \sigma_{k^{\prime}}^{2})^{N/2}}\exp\left(-\frac{1}{2\sigma_{k^{\prime}}^{2}}( \mathbf{w}-\widehat{\mathbf{w}}_{k^{\prime}})^{\top}\widehat{\mathbf{\Sigma}}_{k^{ \prime}}(\mathbf{w}-\widehat{\mathbf{w}}_{k^{\prime}})-\frac{1}{2\sigma_{k^{ \prime}}^{2}}\Big{(}\|\mathbf{y}\|_{2}^{2}-\mathbf{y}^{\top}\mathbf{X}\widehat{ \mathbf{\Sigma}}_{k^{\prime}}^{-1}\mathbf{X}^{\top}\mathbf{y}\Big{)}\right)d \mathbf{w}\] \[\propto\Lambda_{k^{\prime}}\cdot\frac{\det(\widehat{\mathbf{\Sigma}} _{k^{\prime}}/\sigma_{k^{\prime}}^{2})^{-1/2}}{\sigma_{k^{\prime}}^{N}}\exp\left(- \frac{1}{2\sigma_{k^{\prime}}^{2}}\Big{(}\|\mathbf{y}\|_{2}^{2}-\mathbf{y}^{ \top}\mathbf{X}\widehat{\mathbf{\Sigma}}_{k^{\prime}}^{-1}\mathbf{X}^{\top} \mathbf{y}\Big{)}\right)\]\[\propto\Lambda_{k^{\prime}}\cdot\frac{1}{\sigma_{k^{\prime}}^{N-d} \mathrm{det}(\mathbf{X}^{\top}\mathbf{X}+d\sigma_{k^{\prime}}^{2}\mathbf{I}_{d})^ {1/2}}\exp\left(-\frac{1}{2\sigma_{k^{\prime}}^{2}}\Big{(}\|\mathbf{y}\|_{2}^{2 }-\langle\mathbf{y},\mathbf{X}\hat{\mathbf{w}}_{k^{\prime}}\rangle\Big{)} \right).\]

Note that such mixing weights involve the determinant of the matrix \(\widehat{\boldsymbol{\Sigma}}_{k^{\prime}}=\mathbf{X}^{\top}\mathbf{X}+d\sigma _{k^{\prime}}^{2}\mathbf{I}_{d}\), which depends on the data \(\hat{\mathbf{X}}\) in a non-trivial fashion; Any transformer has to approximate these weights if their mechanism is to directly approximate the exact Bayesian predictor (46).

### Useful lemmas

**Lemma J.1**.: _For \(2\mathsf{Risk}_{k,\mathsf{train}}=\mathbb{E}_{k}\big{\|}\mathbf{w}_{\mathrm{ ridge}}^{\lambda_{k}}(\mathcal{D}_{\mathsf{train}})-\mathbf{w}_{\star}\big{\|}_{2}^ {2}+\sigma_{k}^{2}\), there exists universal constant \(C\) such that_

\[\mathsf{Risk}_{k,\mathsf{train}}\leq\mathsf{BayesRisk}_{k}+C( \sigma_{k}^{2}+1)\frac{N_{\mathsf{val}}}{N}.\]

Proof.: Recall that under \(\mathbb{P}_{k}\), we have

\[\mathbf{w}_{\star}\sim\mathsf{N}(0,\mathbf{I}_{d}/d),\qquad y_{ i}=\langle\mathbf{x}_{i},\mathbf{w}_{\star}\rangle+\varepsilon_{i},\qquad \boldsymbol{\varepsilon}_{i}\sim\mathsf{N}(0,\sigma^{2}).\]

We denote \(\mathbf{y}_{t}=[y_{i}]_{i\in\mathcal{I}_{\mathrm{c}us}}\), then by definition \(\mathbf{w}_{\mathrm{ridge}}^{\lambda_{k}}(\mathcal{D}_{\mathsf{train}})=( \mathbf{X}_{\mathsf{train}}^{\top}\mathbf{X}_{\mathsf{train}}+d\sigma_{k}^{2} )^{-1}\mathbf{X}_{\mathsf{train}}\mathbf{y}_{t}\) (with \(\lambda_{k}=d\sigma_{k}^{2}/N_{\mathsf{train}}\)). Thus, a simple calculation yields

\[2\mathsf{Risk}_{k,\mathsf{train}}=\mathbb{E}_{k}\big{\|}\mathbf{w}_{\mathrm{ ridge}}^{\lambda_{k}}(\mathcal{D}_{\mathsf{train}})-\mathbf{w}_{\star} \big{\|}_{2}^{2}+\sigma_{k}^{2}=\sigma_{k}^{2}\mathbb{E}\mathrm{tr}\big{(}( \mathbf{X}_{\mathsf{train}}^{\top}\mathbf{X}_{\mathsf{train}}+d\sigma_{k}^{2} )^{-1}\big{)}+\sigma_{k}^{2},\]

and analogously, \(2\mathsf{BayesRisk}_{k}=\sigma_{k}^{2}\mathbb{E}\mathrm{tr}\big{(}(\mathbf{X} ^{\top}\mathbf{X}+d\sigma_{k}^{2}\mathbf{I}_{d})^{-1}\big{)}+\sigma_{k}^{2}\). Therefore,

\[2\mathsf{Risk}_{k,\mathsf{train}}-2\mathsf{BayesRisk}_{k} =\sigma_{k}^{2}\mathbb{E}\mathrm{tr}\big{(}(\mathbf{X}_{\mathsf{ train}}^{\top}\mathbf{X}_{\mathsf{train}}+d\sigma_{k}^{2}\mathbf{I}_{d})^{-1} \big{)}-\sigma_{k}^{2}\mathbb{E}\mathrm{tr}\big{(}(\mathbf{X}^{\top}\mathbf{X }+d\sigma_{k}^{2}\mathbf{I}_{d})^{-1}\big{)}\] \[\leq\sigma_{k}^{2}N_{\mathsf{val}}\mathbb{E}_{k}[\lambda_{\min}( \boldsymbol{\Sigma})^{-1}],\]

where in the above inequality we denote \(\boldsymbol{\Sigma}:=\mathbf{X}_{\mathsf{train}}^{\top}\mathbf{X}_{\mathsf{ train}}+d\sigma_{k}^{2}\mathbf{I}_{d}\) and use the following fact:

\[\mathrm{tr}\big{(}\boldsymbol{\Sigma}^{-1}\big{)}-\mathrm{tr} \big{(}(\boldsymbol{\Sigma}+\mathbf{X}_{v}^{\top}\mathbf{X}_{v})^{-1}\big{)} =\mathrm{tr}\Big{(}\boldsymbol{\Sigma}^{-1/2}(\mathbf{I}_{d}-( \mathbf{I}_{d}+\boldsymbol{\Sigma}^{-1/2}\mathbf{X}_{v}^{\top}\mathbf{X}_{v} \boldsymbol{\Sigma}^{-1/2})^{-1})\boldsymbol{\Sigma}^{-1/2}\Big{)}\] \[=\mathrm{tr}\Big{(}\boldsymbol{\Sigma}^{-1/2}(\mathbf{I}_{d}+ \boldsymbol{\Sigma}^{-1/2}\mathbf{X}_{v}^{\top}\mathbf{X}_{v}\boldsymbol{ \Sigma}^{-1/2})^{-1}\boldsymbol{\Sigma}^{-1/2}\mathbf{X}_{v}^{\top}\mathbf{X} _{v}\boldsymbol{\Sigma}^{-1}\Big{)}\] \[=\Big{\langle}(\mathbf{I}_{d}+\boldsymbol{\Sigma}^{-1/2}\mathbf{ X}_{v}^{\top}\mathbf{X}_{v}\boldsymbol{\Sigma}^{-1/2})^{-1}\boldsymbol{\Sigma}^{-1/2} \mathbf{X}_{v}^{\top}\mathbf{X}_{v}\boldsymbol{\Sigma}^{-1/2},\boldsymbol{ \Sigma}^{-1}\Big{\rangle}\] \[\leq\mathrm{rank}(\boldsymbol{\Sigma}^{-1/2}\mathbf{X}_{v}^{\top} \mathbf{X}_{v}\boldsymbol{\Sigma}^{-1/2})\lambda_{\max}(\boldsymbol{\Sigma}^{-1 })\leq N_{\mathsf{val}}\lambda_{\min}(\boldsymbol{\Sigma})^{-1}.\]

**Case 1.** We first suppose that \(N_{\mathsf{train}}\leq 16d\). Then by definition \(\boldsymbol{\Sigma}\succeq d\sigma_{k}^{2}\mathbf{I}_{d}\), and hence

\[\sigma_{k}^{2}N_{\mathsf{val}}\mathbb{E}_{k}[\lambda_{\min}( \boldsymbol{\Sigma})^{-1}]\leq\frac{\sigma_{k}^{2}N_{\mathsf{val}}}{d\sigma_{k }^{2}}\leq\frac{16N_{\mathsf{val}}}{N_{\mathsf{train}}}\leq\frac{32N_{\mathsf{ val}}}{N}.\]

**Case 2.** When \(N_{\mathsf{train}}\geq 9d\), then we consider the event \(\mathcal{E}_{t}:=\{\lambda_{\min}(\mathbf{X}_{\mathsf{train}}^{\top}\mathbf{X}_{ \mathsf{train}}/N_{\mathsf{train}})\geq\frac{1}{16}\}\). By Lemma B.2 we have \(\mathbb{P}(\mathcal{E}_{t}^{c})\leq\exp(-N_{\mathsf{train}}/8)\). Therefore,

\[\sigma_{k}^{2}N_{\mathsf{val}}\mathbb{E}_{k}[\lambda_{\min}( \boldsymbol{\Sigma})^{-1}] =\sigma_{k}^{2}N_{\mathsf{val}}\mathbb{E}_{k}[1\{\mathcal{E}_{t}\} \lambda_{\min}(\boldsymbol{\Sigma})^{-1}]+\sigma_{k}^{2}N_{\mathsf{val}} \mathbb{E}_{k}[1\{\mathcal{E}_{t}^{c}\}\lambda_{\min}(\boldsymbol{\Sigma})^{-1}]\] \[\leq\frac{16\sigma_{k}^{2}N_{\mathsf{val}}}{N_{\mathsf{train}}} \cdot\mathbb{P}(\mathcal{E}_{t})+\frac{N_{\mathsf{val}}}{d}\cdot\mathbb{P}( \mathcal{E}_{t}^{c})\] \[\leq\frac{32\sigma_{k}^{2}N_{\mathsf{val}}}{N}+\frac{N_{\mathsf{val} }}{d}\cdot\exp(-N/16)=\mathcal{O}\left(\frac{(\sigma_{k}^{2}+1)N_{\mathsf{ val}}}{N}\right).\]

Combining these two cases finishes the proof. 

**Lemma J.2**.: _Condition on the event \(\mathcal{E}_{\mathsf{train}}\), we have_

\[\mathbb{E}_{\mathcal{D}_{\mathsf{val}}\sim\mathbb{P}_{k}|\mathbf{ w}_{\star},\mathcal{D}_{\mathsf{train}}}\!\left[\max_{l\in[K]}\Big{|}\widehat{L}_{ \mathsf{val}}(\widehat{\mathbf{w}}_{l})-L_{\mathsf{val},\mathbf{w}_{\star}}( \widehat{\mathbf{w}}_{l})\Big{|}\right]\leq CB_{w}^{2}\left[\frac{\log(2K)}{N_{ \mathsf{val}}}+\sqrt{\frac{\log(2K)}{N_{\mathsf{val}}}}\right],\]

_where we denote \(\widehat{\mathbf{w}}_{l}=\widehat{\mathbf{w}}_{l}(\mathcal{D}_{\mathsf{train}})\)._Proof.: We only need to work with a fixed pair of \((\mathbf{w}_{\star},\mathcal{D}_{\text{train}})\) such that \(\mathcal{E}_{\text{train}}\) holds. Hence, in the following we only consider the randomness of \(\mathcal{D}_{\text{val}}\) conditional on such a \((\mathbf{w}_{\star},\mathcal{D}_{\text{train}})\).

Recall that for any \(\mathbf{w}\),

\[\widehat{L}_{\text{val}}(\mathbf{w})=\frac{1}{2\left|\mathcal{D}_{\text{val}} \right|}\sum_{(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{\text{val}}}(\left\langle \mathbf{x}_{i},\mathbf{w}\right\rangle-y_{i})^{2},\]

and we have \(\mathbb{E}_{\mathcal{D}_{\star}}[\widehat{L}_{\text{val}}(\mathbf{w})]=L_{ \text{val},\mathbf{w}_{\star}}(\mathbf{w})\). For each \(i\in\mathcal{I}_{\text{val}}\),

\[y_{i}-\left\langle\mathbf{x}_{i},\widehat{\mathbf{w}}_{l}\right\rangle= \varepsilon_{i}-\left\langle\mathbf{x}_{i},\mathbf{w}_{\star}-\widehat{ \mathbf{w}}_{l}\right\rangle\sim\mathrm{SG}(\sigma_{k}^{2}+\|\mathbf{w}_{\star }-\widehat{\mathbf{w}}_{l}\|^{2}).\]

Note that under \(\mathcal{E}_{\text{train}}\), we have \(\widehat{\mathbf{w}}_{l}\in\mathcal{B}_{2}(B_{w})\) for all \(l\in[K]\), and hence \(\sigma_{k}^{2}+\|\mathbf{w}_{\star}-\widehat{\mathbf{w}}_{l}\|^{2}\leq 5B_{w}^{2}\). We then have \((y_{i}-\left\langle\mathbf{x}_{i},\widehat{\mathbf{w}}_{l}\right\rangle)^{2}\)'s are (conditional) i.i.d random variables in \(\mathrm{SE}(CB_{w}^{u})\). Then, by Bernstein's inequality, we have

\[\mathbb{P}_{\mathcal{D}_{\text{val}}}\Big{(}\Big{|}\widehat{L}_{\text{val}}( \widehat{\mathbf{w}}_{l})-L_{\text{val},\mathbf{w}_{\star}}(\widehat{\mathbf{ w}}_{l})\Big{|}\geq t\Big{)}\leq 2\exp\bigg{(}-cN_{\text{val}}\min\bigg{\{} \frac{t^{2}}{B_{w}^{2}},\frac{t}{B_{w}}\bigg{\}}\bigg{)},\]

where \(c\) is a universal constant. Applying the union bound, we obtain

\[\mathbb{P}_{\mathcal{D}_{\text{val}}}\bigg{(}\!\max_{l\in[K]}\Big{|}\widehat{ L}_{\text{val}}(\widehat{\mathbf{w}}_{l})-L_{\text{val},\mathbf{w}_{\star}}( \widehat{\mathbf{w}}_{l})\Big{|}\geq t\bigg{)}\leq K\exp\bigg{(}-cN_{\text{val }}\min\bigg{\{}\frac{t^{2}}{B_{w}^{2}},\frac{t}{B_{w}}\bigg{\}}\bigg{)}.\]

Taking integration completes the proof. 

### Generalized linear models with adaptive link function selection

Suppose that \((g_{k}:\mathbb{R}\rightarrow\mathbb{R})_{k\in[K]}\) is a set of link functions such that \(g_{k}\) is non-decreasing and \(C^{2}\)-smooth for each \(k\in[K]\). We consider the input format we introduce in Section 4.1 with \(\left|\mathcal{D}_{\text{train}}\right|=\left\lceil N/2\right\rceil,\left| \mathcal{D}_{\text{val}}\right|=\left\lfloor N/2\right\rfloor\).

**Theorem J.2** (GLMs with adaptive link function selection).: _For any fixed set of parameters defined in Assumption B, as long as \(N\geq\mathcal{O}\left(d\right)\), there exists a transformer \(\boldsymbol{\theta}\) with \(L\leq\mathcal{O}\left(\log(N)\right)\) layers, input dimension \(D=\Theta\left(dK\right)\) and \(\max_{\ell\in[L]}M^{\left(\ell\right)}\leq\widetilde{\mathcal{O}}\left(d^{3}N\right)\), such that the following holds._

_For any \(k^{\star}\in[K]\) and any distribution \(\mathsf{P}\) that is a generalized linear model of the link function \(g_{k^{\star}}\) and some parameter \(\boldsymbol{\beta}\), if Assumption B holds for each pair \((\mathsf{P},g_{k})\), then_

\[\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})\sim\mathsf{P}}\big{[}( \widehat{y}_{N+1}-y_{N+1})^{2}\big{]}\leq\mathbb{E}_{(\mathbf{x},y)\sim\mathsf{ P}}\big{[}(g_{k^{\star}}(\left\langle\mathbf{x},\boldsymbol{\beta}\right\rangle)-y)^{2} \big{]}+\mathcal{O}\left(\frac{d}{N}+\sqrt{\frac{\log(K)}{N}}\right),\]

_or equivalently, \(\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1})\sim\mathsf{P}}[(\widehat{y}_{N+1}- \mathbb{E}[y_{N+1}|\mathbf{x}_{N+1}])^{2}]\leq\mathcal{O}\left(d/N+\sqrt{\log (K)/N}\right)\)._

Proof.: For each \(k\in[K]\), we consider optimizing the following training loss:

\[\mathbf{w}_{\mathrm{GLM}}^{(k)}:=\operatorname*{arg\,min}_{\mathbf{w}}\widehat {L}_{\text{train}}^{(k)}(\mathbf{w}):=\frac{1}{N_{\text{train}}}\sum_{(\mathbf{ x}_{i},y_{i})\in\mathcal{D}_{\text{train}}}\ell_{k}(\left\langle\mathbf{x}_{i}, \mathbf{w}\right\rangle,y_{i}),\]

where \(\ell_{k}(t,y):=-yt+\int_{0}^{t}g_{k}(s)ds\) is the convex (integral) loss associated with \(g_{k}\) (as in Section 3.1). Also, for each predictor \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\), we consider the squared validation loss \(\widehat{L}_{\text{val}}\):

\[\widehat{L}_{\text{val}}(f):=\frac{1}{2N_{\text{val}}}\sum_{(\mathbf{x}_{i},y_{ i})\in\mathcal{D}_{\text{val}}}(f(\mathbf{x}_{i})-y_{i})^{2}.\]

Fix a large universal constant \(C_{0}\). Let us set

\[\alpha=\mu_{g}\mu_{x}/8,\qquad\beta=8L_{g}K_{x},\]

\[B_{x}=C_{0}K_{x}\sqrt{d\log(N)},\qquad B_{y}=C_{0}K_{y}\sqrt{\log(N)},\]Then, we define good events similarly to the proof of Corollary 6 (Appendix F.4):

\[\mathcal{E}_{w} =\Big{\{}\forall k\in[K],\ \forall\mathbf{w}\in\mathsf{B}_{2}(B_{w}), \ \alpha\leq\lambda_{\min}(\nabla^{2}\widehat{L}_{\mathsf{train}}^{(k)}(\mathbf{w}) )\leq\lambda_{\max}(\nabla^{2}\widehat{L}_{\mathsf{train}}^{(k)}(\mathbf{w}) )\leq\beta,\Big{\}},\] \[\mathcal{E}_{r} =\Big{\{}\forall k\in[K],\big{\|}\mathbf{w}_{\mathrm{GLM}}^{(k)} \big{\|}_{2}\leq B_{w}/2\Big{\}},\] \[\mathcal{E}_{b,\mathrm{train}} =\{\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{\mathsf{train}}, \big{\|}\mathbf{x}_{i}\big{\|}_{2}\leq B_{x},|y_{i}|\leq B_{y}\},\] \[\mathcal{E}_{b,\mathrm{val}} =\{\forall(\mathbf{x}_{i},y_{i})\in\mathcal{D}_{\mathsf{val}}, \big{\|}\mathbf{x}_{i}\big{\|}_{2}\leq B_{x},|y_{i}|\leq B_{y}\},\] \[\mathcal{E}_{b,N+1} =\{\|\mathbf{x}_{N+1}\|_{2}\leq B_{x},|y_{N+1}|\leq B_{y}\}.\]

Similar to the proof of Theorem G.2 (Appendix G.2), we know the good event \(\mathcal{E}:=\mathcal{E}_{w}\cap\mathcal{E}_{r}\cap\mathcal{E}_{b,\mathsf{ train}}\cap\mathcal{E}_{b,\mathsf{test}}\cap\mathcal{E}_{b,N+1}\) holds with high probability: \(\mathsf{P}(\mathcal{E}^{c})\leq\mathcal{O}\left(N^{-10}\right)\).

Similar to the proof of Theorem I.2, we can show that there exists a transformer \(\bm{\theta}\) with prediction \(\widehat{y}_{N+1}=\widetilde{\mathsf{read}}_{y}(\mathrm{TF}_{\bm{\theta}}( \mathbf{H}))\) (clipped by \(B_{y}\)), such that (for any \(\mathsf{P}\)) the following holds under \(\mathcal{E}\):

1. For each \(k\in[K]\), \(f_{k}=\mathcal{A}_{k}(\mathcal{D}_{\mathsf{train}})\) is a predictor such that \(\Big{|}f_{k}(\mathbf{x}_{i})-g_{k}(\langle\mathbf{x}_{i},\mathbf{w}_{\mathrm{ GLM}}^{(k)}\rangle)\Big{|}\leq\varepsilon\) for all \(i\in[N+1]\) (where \(\varepsilon\) is chosen as in Appendix G.2).
2. \(\widehat{y}_{N+1}=\mathsf{clip}_{B_{y}}(\widehat{f}(\mathbf{x}_{N+1}))\), where \(\widehat{f}=\mathcal{A}_{\mathsf{TF}}(\mathcal{D})\) is an aggregated predictor given by \(\widehat{f}=\sum_{k}\lambda_{k}f_{k}\), such that \((\lambda_{k})\) is a distribution supported on \(k\in[K]\) such that \(\widehat{L}_{\mathsf{val}}(f_{k})\leq\min_{k^{\prime}\in[K]}\widehat{L}_{ \mathsf{val}}(f_{k^{\prime}})+\gamma\).

Similar to the proof of Theorem G.2, for \(\mathcal{E}_{0}:=\mathcal{E}_{w}\cap\mathcal{E}_{r}\cap\mathcal{E}_{b,\mathsf{ train}}\cap\mathcal{E}_{b,\mathsf{test}}\), we have

\[\mathbb{E}_{(\mathcal{D},\mathbf{x}_{N+1},y_{N+1})\sim\mathsf{P}}(\widehat{y }_{N+1}-y_{N+1})^{2}\leq\mathbb{E}_{\mathcal{D}\sim\mathsf{P}}\Big{[}1\{ \mathcal{E}_{0}\}L_{\mathsf{val}}(\widehat{f})\Big{]}+\mathcal{O}\left(\frac{ B_{y}^{2}}{N^{5}}\right),\]

where we denote \(L_{\mathsf{val}}(f):=\mathbb{E}_{(\mathbf{x},y)\sim\mathsf{P}}\Big{[}1\{ \|\mathbf{x}\|_{2}\leq B_{x}\}(f(\mathbf{x})-y)^{2}\Big{]}\) for each predictor \(f\). By the definition of \(\widehat{f}\), we then have (under \(\mathcal{E}_{0}\))

\[L_{\mathsf{val}}(\widehat{f})\leq L_{\mathsf{val}}(f_{k^{*}})+\max_{l}\Big{|} \widehat{L}_{\mathsf{val}}(f_{l})-L_{\mathsf{val}}(f_{l})\Big{|}+\gamma.\]

For the first term, repeating the argument in the proof of Theorem G.2 directly yields that for \(\mathcal{E}_{\mathsf{train}}:=\mathcal{E}_{w}\cap\mathcal{E}_{r}\cap\mathcal{ E}_{b,\mathsf{train}}\),

\[\mathbb{E}_{D_{\mathsf{train}}\sim\mathsf{P}}[1\{\mathcal{E}_{\mathsf{train}} \}L_{\mathsf{val}}(f_{k^{*}})]\leq\mathbb{E}_{(\mathbf{x},y)\sim\mathsf{P}}(g _{k^{*}}(\langle\mathbf{x},\bm{\beta}\rangle)-y)^{2}+\mathcal{O}\left(d/N_{ \mathsf{train}}\right).\]

For the second term, similar to Lemma J.2, we can show that conditional on \(\mathcal{D}_{\mathsf{train}}\) such that \(\mathcal{E}_{\mathsf{train}}\) holds, it holds

\[\mathbb{E}_{\mathcal{D}_{\mathsf{val}}\sim\mathsf{P}|\mathcal{D}_{\mathsf{ train}}}\bigg{[}1\{\mathcal{E}_{0}\}\max_{l}\Big{|}\widehat{L}_{\mathsf{val}}(f_{l})-L_{ \mathsf{val}}(f_{l})\Big{|}\bigg{]}\leq\mathcal{O}\left(K_{y}^{2}\right)\cdot \Bigg{(}\sqrt{\frac{\log K}{N_{\mathsf{val}}}}+\frac{\log K}{N_{\mathsf{val}}} \Bigg{)}.\]

Combining these inequalities and suitably choosing \(\gamma\) complete the proof. 

## Appendix K Analysis of pretraining

Thus far, we have established the existence of transformers for performing various ICL tasks with good in-context statistical performance. We now analyze the sample complexity of pretraining these transformers from a finite number of training ICL instances.

### Generalization guarantee for pretraining

SetupAt pretraining time, each training ICL instance has form \(\mathbf{Z}:=(\mathbf{H},y_{N+1})\), where \(\mathbf{H}:=\mathbf{H}(\mathcal{D},\mathbf{x}_{N+1})\in\mathbb{R}^{D\times(N+1)}\) denote the input sequence formatted as in (3). We consider the square loss between the in-context prediction and the ground truth label:

\[\ell_{\mathsf{icl}}(\bm{\theta};\mathbf{Z}):=\frac{1}{2}\bigg{(}y_{N+1}- \underbrace{\mathsf{clip}_{B_{y}}(\mathsf{read}_{y}(\mathrm{TF}_{\bm{\theta}}^{R} (\mathbf{H})))}_{\mathsf{read}_{y}}\bigg{)}^{2}.\]

[MISSING_PAGE_EMPTY:78]

\(\widehat{\bm{\theta}}\) of (TF-ERM) with \(L=\mathcal{O}(\kappa\log(\kappa N/\sigma))\) layers, \(M=3\) heads, \(D^{\prime}=0\) (attention-only), and \(B=\mathcal{O}(\sqrt{\kappa d})\) achieves small excess ICL risk over \(\mathbf{w}_{\mathsf{P}}^{\star}\):_

\[L_{\mathsf{iel}}(\widehat{\bm{\theta}})-\mathbb{E}_{\mathsf{P}\sim\pi} \mathbb{E}_{(\mathbf{x},y)\sim\mathsf{P}}\bigg{[}\frac{1}{2}(y-\langle\mathbf{ w}_{\mathsf{P}}^{\star},\mathbf{x}\rangle)^{2}\bigg{]}\leq\widetilde{\mathcal{O}} \Bigg{(}\sqrt{\frac{\kappa^{2}d^{2}+\log(1/\xi)}{n}}+\frac{d\sigma^{2}}{N} \Bigg{)},\]

_where \(\widetilde{\mathcal{O}}(\cdot)\) only hides polylogarithmic factors in \(\kappa,N,1/\sigma\)._

To our best knowledge, Theorem K.2 offers the first end-to-end result for pretraining a transformer to perform in-context linear regression with explicit excess loss bounds. The \(\widetilde{\mathcal{O}}(\sqrt{\kappa^{2}d^{2}/n})\) term originates from the generalization of pretraining (Theorem K.1), where as the \(\widetilde{\mathcal{O}}(d\sigma^{2}/N)\) term agrees with the standard fast rate for the excess loss of linear regression [38]. Further, as long as \(n\geq\widetilde{\mathcal{O}}(\kappa^{2}N/\sigma^{2})\), the excess risk achieves the optimal rate \(\widetilde{\mathcal{O}}(d\sigma^{2}/N)\) (up to log factors).

Additional examplesBy similar arguments as in the proof of Theorem K.2, we can directly turn most of our other expressivity results into results on the pretrained transformers. Here we present three such additional examples (proofs in Appendix L.4-L.6). The first example is for the sparse linear regression problem considered in Theorem 8.

**Theorem K.3** (Pretraining transformers for in-context sparse linear regression).: _Suppose each \(\mathsf{P}\sim\pi\) is almost surely an instance of the sparse linear model specified in Theorem 8 with parameters \(B_{w}^{\star}\) and \(\sigma\). Suppose \(N\geq\widetilde{\mathcal{O}}(s\log((d\lor N)/\sigma))\) and let \(\kappa:=B_{w}^{\star}/\sigma\)._

_Then with probability at least \(1-\xi\) (over the training instances \(\mathbf{Z}^{(1:n)}\)), the solution \(\widehat{\bm{\theta}}\) of (TF-ERM) with \(L=\widetilde{\mathcal{O}}(\kappa^{2}(1+d/N))\) layers, \(M=2\) heads, \(D^{\prime}=2d\), and \(B=\widetilde{\mathcal{O}}(\operatorname{poly}(d,B_{w}^{\star},\sigma))\) achieves small excess ICL risk:_

\[L_{\mathsf{fel}}(\widehat{\bm{\theta}})-\sigma^{2}\leq\widetilde{\mathcal{O}} \Bigg{(}\sqrt{\frac{\kappa^{4}d^{2}(1+d/N)^{2}+\log(1/\xi)}{n}}+\sigma^{2} \frac{s\log d}{N}\Bigg{)},\]

_where \(\widetilde{\mathcal{O}}(\cdot)\) only hides polylogarithmic factors in \(d,N,1/\sigma\)._

Our next example is for the problem of noisy linear regression with mixed noise levels considered in Theorem 12 and Theorem J.1. There, the constructed transformer uses the post-ICL validation mechanism to perform ridge regression with an adaptive regulariation strength depending on the particular input sequence.

**Theorem K.4** (Pretraining transformers for in-context noisy linear regression with algorithm selection).: _Suppose \(\pi\) is the data generating model (noisy linear model with mixed noise levels) considered in Theorem J.1, with \(\sigma_{\max}\leq\mathcal{O}(1)\). Let \(N\geq d/10\)._

_Then, with probability at least \(1-\xi\) (over the training instances \(\mathbf{Z}^{(1:n)}\)), the solution \(\widehat{\bm{\theta}}\) of (TF-ERM) with input dimension \(D=\Theta(dK)\), \(L=\mathcal{O}(\sigma_{\min}^{-2}\log(N/\sigma_{\min}))\) layers, \(M=\mathcal{O}(K)\) heads, \(D^{\prime}=\mathcal{O}(K^{2})\), and \(B=\mathcal{O}(\operatorname{poly}(K,\sigma_{\min}^{-1},d,N))\) achieves small excess ICL risk:_

\[L_{\mathsf{fel}}(\widehat{\bm{\theta}})-\mathsf{BayesRisk}_{\pi}\leq \widetilde{\mathcal{O}}\Bigg{(}\sqrt{\frac{\sigma_{\min}^{-4}K^{3}d^{2}+\log( 1/\xi)}{n}}+\frac{\sigma_{\max}^{2}}{\sigma_{\min}^{2/3}}\Big{(}\frac{\log K}{ N}\Big{)}^{1/3}\Bigg{)},\]

_where \(\widetilde{\mathcal{O}}(\cdot)\) only hides polylogarithmic factors in \(d,N,K,1/\sigma_{\min}\)._

Our final example is for in-context logistic regression. For simplicity we consider the realizable case.

**Theorem K.5** (Pretraining transformers for in-context logistic regression; square loss guarantee).: _Suppose for \(\mathsf{P}\sim\pi\), \(\mathsf{P}\) is almost surely a realizable logistic model (i.e. \(\mathsf{P}=\mathsf{P}_{\bm{\beta}}^{\mathsf{log}}\) with \(\left\lVert\bm{\beta}\right\rVert_{2}\leq B_{w}^{\star}\) as in Corollary G.1). Suppose that \(B_{w}^{\star}=\mathcal{O}\left(1\right)\) and \(N\geq\mathcal{O}\left(d\right)\)._

_Then, with probability at least \(1-\xi\) (over the training instances \(\mathbf{Z}^{(1:n)}\)), the solution \(\widehat{\bm{\theta}}\) of (TF-ERM) with \(L=\mathcal{O}(\log(N))\) layers, \(M=\widetilde{\mathcal{O}}\left(d^{3}N\right)\) heads, \(D^{\prime}=0\), and \(B=\mathcal{O}(\operatorname{poly}(d,N))\) achieves small excess ICL risk:_

\[L_{\mathsf{iel}}(\widehat{\bm{\theta}})-\mathbb{E}_{\mathsf{P}_{\bm{\beta}}^{ \mathsf{log}}\sim\pi}\mathbb{E}_{(\mathbf{x},y)\sim\mathsf{P}_{\bm{\beta}}^{ \mathsf{log}}}\bigg{[}\frac{1}{2}(y-\sigma_{\log}(\langle\bm{\beta},\mathbf{x }\rangle))^{2}\bigg{]}\leq\widetilde{\mathcal{O}}\Bigg{(}\sqrt{\frac{d^{5}N+ \log(1/\xi)}{n}}+\frac{d}{N}\Bigg{)},\]

_where \(\widetilde{\mathcal{O}}(\cdot)\) only hides polylogarithmic factors in \(d,N\)._Remark on generality of transformerAll results above are established by the expressivity results in Section 3 & 4 for transformers to implement various ICL procedures (such as least squares, Lasso, GLM, and ridge regression with in-context algorithm selection), combined with the generalization bound (Theorem K.1). However, the transformer itself was not specified to encode any actual structure about the problem at hand in any result above, other than having sufficiently large number of layers, number of heads, and weight norms, which illustrates the flexibility of the transformer architecture.

## Appendix L Proofs for Section K

### Lipschitzness of transformers

For any \(p\in[1,\infty]\), let \(\left\lVert\mathbf{H}\right\rVert_{2,p}:=(\sum_{i=1}^{N}\left\lVert\mathbf{h }_{i}\right\rVert_{2}^{p})^{1/p}\) denote the column-wise \((2,p)\)-norm of \(\mathbf{H}\). For any radius \(\mathsf{R}>0\), we denote \(\mathcal{H}_{\mathsf{R}}:=\{\mathbf{H}:\left\lVert\mathbf{H}\right\rVert_{2, \infty}\leq\mathsf{R}\}\) be the ball of radius \(\mathsf{R}\) under norm \(\left\lVert\cdot\right\rVert_{2,\infty}\).

**Lemma L.1**.: _For a single MLP layer \(\bm{\theta}_{\mathsf{nlp}}=(\mathbf{W}_{1},\mathbf{W}_{2})\), we introduce its norm (as in (2))_

\[\left\lVert\!\left\lVert\bm{\theta}_{\mathsf{nlp}}\right\rVert\!\right\rVert =\left\lVert\mathbf{W}_{1}\right\rVert_{\mathrm{op}}+\left\lVert\mathbf{W}_{2 }\right\rVert_{\mathrm{op}}.\]

_For any fixed hidden dimension \(D^{\prime}\), we consider_

\[\Theta_{\mathsf{nlp},B}:=\big{\{}\bm{\theta}_{\mathsf{nlp}}:\left\lVert\bm{ \theta}_{\mathsf{nlp}}\right\rVert\leq B\big{\}}.\]

_Then for \(\mathbf{H}\in\mathcal{H}_{\mathsf{R}}\), \(\bm{\theta}_{\mathsf{nlp}}\in\Theta_{\mathsf{nlp},B}\), the function \((\bm{\theta}_{\mathsf{nlp}},\mathbf{H})\mapsto\mathrm{MLP}_{\bm{\theta}_{ \mathsf{nlp}}}(\mathbf{H})\) is \((B\mathsf{R})\)-Lipschitz w.r.t. \(\bm{\theta}_{\mathsf{nlp}}\) and \((1+B^{2})\)-Lipschitz w.r.t. \(\mathbf{H}\)._

Proof.: Recall that by our definition, for the parameter \(\bm{\theta}_{\mathsf{nlp}}=(\mathbf{W}_{1},\mathbf{W}_{2})\in\Theta_{\mathsf{ nlp},B}\) and the input \(\mathbf{H}=[\mathbf{h}_{i}]\in\mathbb{R}^{D\times N}\), the output \(\mathrm{MLP}_{\bm{\theta}_{\mathsf{nlp}}}(\mathbf{H})=\mathbf{H}+\mathbf{W}_ {2}\sigma(\mathbf{W}_{1}\mathbf{H})=[\mathbf{h}_{i}+\mathbf{W}_{2}\sigma( \mathbf{W}_{1}\mathbf{h}_{i})]_{i}\). Therefore, for \(\theta^{\prime}_{\mathsf{nlp}}=(\mathbf{W}^{\prime}_{1},\mathbf{W}^{\prime} _{2})\in\Theta_{\mathsf{nlp},B}\), we have

\[\left\lVert\mathrm{MLP}_{\bm{\theta}_{\mathsf{nlp}}}(\mathbf{H} )-\mathrm{MLP}_{\theta^{\prime}_{\mathsf{nlp}}}(\mathbf{H})\right\rVert_{2,\infty}\] \[= \max_{i}\left\lVert\mathbf{W}_{2}\sigma(\mathbf{W}_{1}\mathbf{h }_{i})-\mathbf{W}^{\prime}_{2}\sigma(\mathbf{W}^{\prime}_{1}\mathbf{h}_{i}) \right\rVert_{2}\] \[= \max_{i}\left\lVert(\mathbf{W}_{2}-\mathbf{W}^{\prime}_{2}) \sigma(\mathbf{W}_{1}\mathbf{h}_{i})+\mathbf{W}^{\prime}_{2}(\sigma(\mathbf{W} _{1}\mathbf{h}_{i})-\sigma(\mathbf{W}^{\prime}_{1}\mathbf{h}_{i}))\right\rVert _{2}\] \[\leq \max_{i}\left\lVert\mathbf{W}_{2}-\mathbf{W}^{\prime}_{2}\right\rVert _{\mathrm{op}}\left\lVert\sigma(\mathbf{W}_{1}\mathbf{h}_{i})\right\rVert_{2}+ \left\lVert\mathbf{W}^{\prime}_{2}\right\rVert_{\mathrm{op}}\left\lVert \sigma(\mathbf{W}_{1}\mathbf{h}_{i})-\sigma(\mathbf{W}^{\prime}_{1}\mathbf{h}_{ i})\right\rVert_{2}\] \[\leq \max_{i}\left\lVert\mathbf{W}_{2}-\mathbf{W}^{\prime}_{2}\right\rVert _{\mathrm{op}}\left\lVert\mathbf{W}_{1}\mathbf{h}_{i}\right\rVert_{2}+\left\lVert \mathbf{W}^{\prime}_{2}\right\rVert_{\mathrm{op}}\left\lVert\mathbf{W}_{1} \mathbf{h}_{i}-\mathbf{W}^{\prime}_{1}\mathbf{h}_{i}\right\rVert_{2}\] \[\leq B\mathsf{R}\left\lVert\mathbf{W}_{2}-\mathbf{W}^{\prime}_{2} \right\rVert_{\mathrm{op}}+B\mathsf{R}\left\lVert\mathbf{W}_{1}-\mathbf{W}^{ \prime}_{1}\right\rVert_{\mathrm{op}},\]

where the second inequality follows from the 1-Lipschitzness of \(\sigma=[\cdot]_{+}\). Similarly, for \(\mathbf{H}^{\prime}=[\mathbf{h}^{\prime}_{i}]\in\mathbb{R}^{D\times N}\),

\[\left\lVert\mathrm{MLP}_{\bm{\theta}_{\mathsf{nlp}}}(\mathbf{H} )-\mathrm{MLP}_{\bm{\theta}_{\mathsf{nlp}}}(\mathbf{H}^{\prime})\right\rVert _{2,\infty} = \max_{i}\left\lVert\mathbf{h}_{i}+\mathbf{W}_{1}\sigma(\mathbf{W}_{ 2}\mathbf{h}_{i})-\mathbf{h}^{\prime}_{i}-\mathbf{W}_{1}\sigma(\mathbf{W}_{2} \mathbf{h}^{\prime}_{i})\right\rVert_{2}\] \[\leq \left\lVert\mathbf{H}-\mathbf{H}^{\prime}\right\rVert_{2,\infty} +\max_{i}\left\lVert\mathbf{W}_{1}(\sigma(\mathbf{W}_{2}\mathbf{h}_{i})- \sigma(\mathbf{W}_{2}\mathbf{h}^{\prime}_{i}))\right\rVert_{2}\] \[\leq \left\lVert\mathbf{H}-\mathbf{H}^{\prime}\right\rVert_{2,\infty} +\max_{i}B\left\lVert\sigma(\mathbf{W}_{2}\mathbf{h}_{i})-\sigma(\mathbf{W}_{ 2}\mathbf{h}^{\prime}_{i})\right\rVert_{2}\] \[\leq \left\lVert\mathbf{H}-\mathbf{H}^{\prime}\right\rVert_{2,\infty} +B^{2}\left\lVert\mathbf{H}-\mathbf{H}^{\prime}\right\rVert_{2,\infty}.\]

**Lemma L.2**.: _For a single attention layer \(\bm{\theta}_{\mathsf{attn}}=\left\{(\mathbf{V}_{m},\mathbf{Q}_{m},\mathbf{K}_{m })\right\}_{m\in[M]}\subset\mathbb{R}^{D\times D}\), we introduce its norm (as in (2))_

\[\left\lVert\!\left\lVert\bm{\theta}_{\mathsf{attn}}\right\rVert\!\right\rVert :=\max_{m\in[M]}\left\{\left\lVert\mathbf{Q}_{m}\right\rVert_{ \mathrm{op}},\left\lVert\mathbf{K}_{m}\right\rVert_{\mathrm{op}}\right\}+\sum_{m =1}^{M}\left\lVert\mathbf{V}_{m}\right\rVert_{\mathrm{op}}.\]

_For any fixed dimension \(D\), we consider_

\[\Theta_{\mathsf{attn},B}:=\{\bm{\theta}_{\mathsf{attn}}:\left\lVert\bm{\theta}_{ \mathsf{attn}}\right\rVert\!\right\rVert\leq B\}.\]

_Then for \(\mathbf{H}\in\mathcal{H}_{\mathsf{R}}\), \(\bm{\theta}_{\mathsf{attn}}\in\Theta_{\mathsf{attn},B}\), the function \((\bm{\theta}_{\mathsf{attn}},\mathbf{H})\mapsto\mathrm{Attn}_{\bm{\theta}_{ \mathsf{attn}}}(\mathbf{H})\) is \((B^{2}\mathsf{R}^{3})\)-Lipschitz w.r.t. \(\bm{\theta}_{\mathsf{attn}}\) and \((1+B^{3}\mathsf{R}^{2})\)-Lipschitz w.r.t. \(\mathbf{H}\)._Proof.: Recall that by our definition, for the parameter \(\bm{\theta}_{\ttttn}=\{(\mathbf{V}_{m},\mathbf{Q}_{m},\mathbf{K}_{m})\}_{m\in[M]} \in\Theta_{\ttttn,B}\) and the input \(\mathbf{H}=[\mathbf{h}_{i}]\in\mathbb{R}^{D\times N}\), the output \(\mathrm{Attn}_{\bm{\theta}_{\ttttn}}(\mathbf{H})=[\widetilde{\mathbf{h}}_{i}]\) is given by

\[\widetilde{\mathbf{h}}_{i}=\mathbf{h}_{i}+\sum_{m=1}^{M}\frac{1}{N}\sum_{j=1}^ {N}\sigma(\langle\mathbf{Q}_{m}\mathbf{h}_{i},\mathbf{K}_{m}\mathbf{h}_{j} \rangle)\cdot\mathbf{V}_{m}\mathbf{h}_{j}.\]

Now, for \(\theta^{\prime}_{\ttttn}=\{(\mathbf{V}_{m}^{\prime},\mathbf{Q}_{m}^{\prime}, \mathbf{K}_{m}^{\prime})\}_{m\in[M]}\), we consider

\[\widetilde{\mathbf{h}}_{i}^{\prime}=\big{[}\mathrm{Attn}_{\theta^{\prime}_{ \ttttn}}(\mathbf{H})\big{]}_{i}=\mathbf{h}_{i}+\sum_{m=1}^{M}\frac{1}{N}\sum_{ j=1}^{N}\sigma(\langle\mathbf{Q}_{m}^{\prime}\mathbf{h}_{i},\mathbf{K}_{m}^{ \prime}\mathbf{h}_{j}\rangle)\cdot\mathbf{V}_{m}^{\prime}\mathbf{h}_{j},\qquad \forall i\in[N].\]

Clearly \(\big{\|}\mathrm{Attn}_{\bm{\theta}_{\ttttn}}(\mathbf{H})-\mathrm{Attn}_{\theta ^{\prime}_{\ttttn}}(\mathbf{H})\big{\|}_{2,\infty}=\max_{i}\left\|\widetilde{ \mathbf{h}}_{i}-\widetilde{\mathbf{h}}_{i}^{\prime}\right\|_{2}\). For any \(i\in[N]\), we have

\[\left\|\widetilde{\mathbf{h}}_{i}-\widetilde{\mathbf{h}}_{i}^{ \prime}\right\|_{2} =\left\|\sum_{m=1}^{M}\frac{1}{N}\sum_{j=1}^{N}\left[\sigma( \langle\mathbf{Q}_{m}\mathbf{h}_{i},\mathbf{K}_{m}\mathbf{h}_{j}\rangle) \mathbf{V}_{m}\mathbf{h}_{j}-\sigma(\langle\mathbf{Q}_{m}^{\prime}\mathbf{h}_{ i},\mathbf{K}_{m}^{\prime}\mathbf{h}_{j}\rangle)\mathbf{V}_{m}^{\prime} \mathbf{h}_{j}\right]\right\|_{2}\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\] \[\leq\]

\[\leq\]

where the second inequality uses the definition of operator norm, the third inequality follows from the triangle inequality, the forth inequality is because \(\left\|\mathbf{Q}_{m}\mathbf{h}_{i}\right\|_{2}\leq B\mathbf{R},\left\|\mathbf{ K}_{m}\mathbf{h}_{j}\right\|_{2}\leq B\mathbf{R}\), and \(\sigma\) is 1-Lipschitz. This completes the proof the Lipschitzness w.r.t. \(\bm{\theta}_{\ttttn}\).

Similarly, we consider \(\mathbf{H}^{\prime}=[\mathbf{h}_{i}^{\prime}]\), and

\[\widetilde{\mathbf{h}}_{i}^{\prime}=\big{[}\mathrm{Attn}_{\theta^{\prime}_{\ttttn }}(\mathbf{H})\big{]}_{i}=\mathbf{h}_{i}^{\prime}+\sum_{m=1}^{M}\frac{1}{N} \sum_{j=1}^{N}\sigma\big{(}\langle\mathbf{Q}_{m}\mathbf{h}_{i}^{\prime}, \mathbf{K}_{m}\mathbf{h}_{j}^{\prime}\rangle\big{)}\cdot\mathbf{V}_{m} \mathbf{h}_{j}^{\prime},\qquad\forall i\in[N].\]

By definition, we can similarly bound

\[\left\|\left(\widetilde{\mathbf{h}}_{i}^{\prime}-\mathbf{h}_{i}^{ \prime}\right)-\left(\widetilde{\mathbf{h}}_{i}-\mathbf{h}_{i}\right)\right\|_ {2}\] \[=\] \[\leq\]\[\leq\sum_{m=1}^{M}\frac{1}{N}\sum_{j=1}^{N}\left\|\mathbf{V}_{m} \right\|_{\mathrm{op}}\Big{\{}\big{|}\sigma(\langle\mathbf{Q}_{m}\mathbf{h}_{i},\mathbf{K}_{m}\mathbf{h}_{j}\rangle)\big{|}\cdot\left\|\mathbf{h}_{j}-\mathbf{ h}_{j}^{\prime}\right\|_{2}\] \[+\big{|}\sigma(\langle\mathbf{Q}_{m}\mathbf{h}_{i},\mathbf{K}_{m }\mathbf{h}_{j}\rangle)-\sigma(\langle\mathbf{Q}_{m}\mathbf{h}_{i}^{\prime}, \mathbf{K}_{m}\mathbf{h}_{j}\rangle)\big{|}\cdot\left\|\mathbf{h}_{j}^{\prime} \right\|_{2}\] \[\leq\sum_{m=1}^{M}\frac{1}{N}\sum_{j=1}^{N}\left\|\mathbf{V}_{m} \right\|_{\mathrm{op}}\cdot 3\left\|\mathbf{Q}_{m}\right\|_{\mathrm{op}} \left\|\mathbf{K}_{m}\right\|_{\mathrm{op}}\mathbb{R}^{2}\left\|\mathbf{h}_{j} -\mathbf{h}_{j}^{\prime}\right\|_{2}\] \[\leq B^{3}\mathbb{R}^{2}\left\|\mathbf{H}-\mathbf{H}^{\prime} \right\|_{2,\infty},\]

where the last inequality uses \(\left\|\boldsymbol{\theta}_{\mathtt{attn}}\right\|\leq B\) and the AM-GM inequality. This completes the proof the Lipschitzness w.r.t. \(\mathbf{H}\). 

**Corollary L.1**.: _For a fixed number of heads \(M\) and hidden dimension \(D^{\prime}\), we consider_

\[\Theta_{\mathrm{TF},1,B}=\big{\{}\boldsymbol{\theta}=(\boldsymbol{\theta}_{ \mathtt{attn}},\boldsymbol{\theta}_{\mathtt{mlp}}):M\text{ heads, hidden dimension }D^{\prime},\left\|\boldsymbol{\theta}\right\|\leq B\big{\}}.\]

_Then for the function \(\mathrm{TF}^{\mathsf{R}}\) given by_

\[\mathrm{TF}^{\mathsf{R}}:(\boldsymbol{\theta},\mathbf{H})\mapsto\mathsf{clip} _{\mathsf{R}}\big{(}\mathrm{MLP}_{\boldsymbol{\theta}_{\mathtt{alg}}}( \mathrm{Attn}_{\boldsymbol{\theta}_{\mathtt{attn}}}(\mathbf{H}))\big{)},\qquad \boldsymbol{\theta}\in\Theta_{\mathrm{TF},1,B},\mathbf{H}\in\mathcal{H}_{ \mathsf{R}}\]

\(\mathrm{TF}^{\mathsf{R}}\) _is \(B_{\Theta}\)-Lipschitz w.r.t \(\boldsymbol{\theta}\) and \(L_{H}\)-Lipschitz w.r.t. \(\mathbf{H}\), where \(B_{\Theta}:=B\mathsf{R}(1+B\mathbb{R}^{2}+B^{3}\mathbb{R}^{2})\) and \(B_{H}:=(1+B^{2})(1+B^{2}\mathbb{R}^{3})\)._

Proof.: For any \(\boldsymbol{\theta}=(\boldsymbol{\theta}_{\mathtt{attn}},\boldsymbol{\theta}_{ \mathtt{mlp}})\), \(\mathbf{H}\in\mathcal{H}_{\mathsf{R}}\), and \(\theta^{\prime}=(\theta^{\prime}_{\mathtt{attn}},\theta^{\prime}_{\mathtt{ mlp}})\), we have

\[\left\|\mathrm{TF}_{\boldsymbol{\theta}}(\mathbf{H})-\mathrm{TF }_{\theta^{\prime}}(\mathbf{H})\right\|_{2,\infty} \leq\left\|\mathrm{MLP}_{\boldsymbol{\theta}_{\mathtt{alg}}}( \mathrm{Attn}_{\boldsymbol{\theta}_{\mathtt{attn}}}(\mathbf{H}))-\mathrm{MLP}_{ \boldsymbol{\theta}_{\mathtt{alg}}^{\prime}}(\mathrm{Attn}_{\theta^{\prime}_ {\mathtt{attn}}}(\mathbf{H}))\right\|_{2,\infty}\] \[\leq(1+B^{2})B^{2}\mathbb{R}^{3}\left\|\boldsymbol{\theta}_{ \mathtt{attn}}-\boldsymbol{\theta}^{\prime}_{\mathtt{attn}}\right\|+B\overline {\mathbb{R}}\left\|\boldsymbol{\theta}_{\mathtt{nlp}}-\theta^{\prime}_{ \mathtt{mlp}}\right\|\] \[\leq B_{\Theta}\|\boldsymbol{\theta}-\theta^{\prime}\|,\]

where the second inequality follows from Lemma L.2 and Lemma L.1 and the fact that \(\left\|\mathrm{Attn}_{\boldsymbol{\theta}_{\mathtt{attn}}}(\mathbf{H})\right\|_ {2,\infty}\leq\overline{\mathbb{R}}:=\mathbb{R}+B^{3}\mathbb{R}^{3}\) for all \(\mathbf{H}\in\mathcal{H}_{\mathsf{R}}\).

Furthermore, for \(\mathbf{H}^{\prime}\in\mathcal{H}_{\mathsf{R}}\), we have

\[\left\|\mathrm{TF}_{\boldsymbol{\theta}}(\mathbf{H})-\mathrm{TF }_{\boldsymbol{\theta}}(\mathbf{H}^{\prime})\right\|_{2,\infty} \leq(1+B^{2})\left\|\mathrm{Attn}_{\boldsymbol{\theta}_{\mathtt{ attn}}}(\mathbf{H})-\mathrm{Attn}_{\boldsymbol{\theta}_{\mathtt{attn}}}( \mathbf{H}^{\prime})\right\|_{2,\infty}\] \[\leq(1+B^{2})(1+B^{3}\mathbb{R}^{2})\left\|\mathbf{H}-\mathbf{H}^ {\prime}\right\|_{2,\infty},\]

which also follows from Lemma L.2 and Lemma L.1. 

**Proposition L.1** (Lipschitzness of transformers).: _For a fixed number of heads \(M\) and hidden dimension \(D^{\prime}\), we consider_

\[\Theta_{\mathrm{TF},L,B}=\Big{\{}\boldsymbol{\theta}=(\boldsymbol{\theta}^{(1:L )}_{\mathtt{attn}},\boldsymbol{\theta}^{(1:L)}_{\mathtt{nlp}}):M^{(\ell)}=M,D^{( \ell)}=D^{\prime},\left\|\boldsymbol{\theta}\right\|\leq B\Big{\}}.\]

_Then the function \(\mathrm{TF}^{\mathsf{R}}\) is \((LB_{H}^{L-1}B_{\Theta})\)-Lipschitz w.r.t \(\boldsymbol{\theta}\in\Theta_{\mathrm{TF},L,B}\) for any fixed \(\mathbf{H}\)._

Proof.: For \(\boldsymbol{\theta}=\boldsymbol{\theta}^{(1:L)}\in\Theta_{\mathrm{TF},L,B}, \widetilde{\boldsymbol{\theta}}=\widetilde{\boldsymbol{\theta}}^{(1:L)}\in \Theta_{\mathrm{TF},L,B}\), we have

\[\left\|\mathrm{TF}^{\mathsf{R}}_{\boldsymbol{\theta}}(\mathbf{H})-\mathrm{TF}^ {\mathsf{R}}_{\widetilde{\boldsymbol{\theta}}}(\mathbf{H})\right\|_{2,\infty}\]\[\leq \sum_{\ell=1}^{L}B_{\Theta}^{L-\ell}\left\|\mathrm{TF}^{\mathsf{R}}_{ \boldsymbol{\theta}^{(\ell)}}\Big{(}\mathrm{TF}^{\mathsf{R}}_{\boldsymbol{ \theta}^{(1:\ell-1)}}(\mathbf{H})\Big{)}-\mathrm{TF}^{\mathsf{R}}_{\boldsymbol{ \theta}^{(\ell+1:L)}}\Big{(}\mathrm{TF}^{\mathsf{R}}_{\boldsymbol{\tilde{ \theta}}^{(1:\ell-1)}}(\mathbf{H})\Big{)}\right\|_{2,\infty}\] \[\leq \sum_{\ell=1}^{L}B_{\Theta}^{L-\ell}\left\|\mathrm{TF}^{\mathsf{R} }_{\boldsymbol{\theta}^{(\ell)}}\Big{(}\mathrm{TF}^{\mathsf{R}}_{\boldsymbol{ \tilde{\theta}}^{(1:\ell-1)}}(\mathbf{H})\Big{)}-\mathrm{TF}^{\mathsf{R}}_{ \boldsymbol{\tilde{\theta}}^{(\ell)}}\Big{(}\mathrm{TF}^{\mathsf{R}}_{ \boldsymbol{\tilde{\theta}}^{(1:\ell-1)}}(\mathbf{H})\Big{)}\right\|_{2,\infty}\] \[\leq \sum_{\ell=1}^{L}B_{H}^{L-\ell}B_{\Theta}\cdot\left\| \boldsymbol{\theta}^{(\ell)}-\boldsymbol{\tilde{\theta}}^{(\ell)}\right\|\leq LB _{H}^{L-1}B_{\Theta}\cdot\left\|\boldsymbol{\theta}-\boldsymbol{\tilde{ \theta}}\right\|,\]

where the second inequality follows from Corollary L.1, and the last inequality is because \(B_{H}\geq 1\). 

### Proof of Theorem k.1

In this section, we prove a slightly more general result by considering the general ICL loss

\[\ell_{\text{icl}}(\boldsymbol{\theta};\mathbf{Z}):=\ell(\widetilde{\mathsf{ read}}_{\boldsymbol{\theta}}(\mathrm{TF}^{\mathsf{R}}_{\boldsymbol{\theta}}( \mathcal{H})),y_{N+1}).\]

We assume that the loss function \(\ell\) satisfies \(\sup|\ell|\leq B_{\ell}^{0}\) and \(\sup|\partial_{1}\ell|\leq B_{\ell}^{1}\). For the special case \(\ell(s,t)=\frac{1}{2}(s-t)^{2}\), we can take \(B_{\ell}^{0}=4B_{y}^{2},B_{\ell}^{1}=2B_{y}\).

We then consider

\[X_{\boldsymbol{\theta}}:=\frac{1}{n}\sum_{j=1}^{n}\ell_{\text{icl}}( \boldsymbol{\theta};\mathbf{Z}^{j})-\mathbb{E}_{\mathbf{Z}}[\ell_{\text{icl} }(\boldsymbol{\theta};\mathbf{Z})],\]

where \(\mathbf{Z}^{(1:n)}\) are i.i.d copies of \(\mathbf{Z}\sim\mathsf{P},\mathsf{P}\sim\pi\). It remains to apply Proposition B.4 to the random process \(\{X_{\boldsymbol{\theta}}\}\). We verify the preconditions:

(a) By [87, Example 5.8], it holds that \(\log N(\delta;\mathsf{B}_{\|\cdot\|}(r),\left\|\cdot\right\|)\leq L(3MD^{2}+2DD ^{\prime})\log(1+2r/\delta)\), where \(\mathsf{B}_{\|\cdot\|}(r)\) is any ball of radius \(r\) under norm \(\left\|\cdot\right\|\).

(b) \(\left|\ell_{\text{icl}}(\boldsymbol{\theta};\mathbf{Z})\right|\leq B_{\ell}^ {0}\) and hence \(B_{\ell}^{0}\)-sub-Gaussian.

(c) \(\left|\ell_{\text{icl}}(\boldsymbol{\theta};\mathbf{Z})-\ell_{\text{icl}}( \boldsymbol{\tilde{\theta}};\mathbf{Z})\right|\leq B_{\ell}^{1}\cdot(LB_{H}^ {L-1}B_{\Theta})\cdot\left\|\boldsymbol{\theta}-\boldsymbol{\tilde{\theta}} \right\|\), by Proposition L.1.

Therefore, we can apply the uniform concentration result in Proposition B.4 to obtain that, with probability at least \(1-\xi\),

\[\sup_{\boldsymbol{\theta}}|X_{\boldsymbol{\theta}}|\leq CB_{\ell}^{0}\sqrt{ \frac{L(MD^{2}+DD^{\prime})\iota+\log(1/\xi)}{n}},\]

where \(\iota=\log(2+B\cdot LB_{H}^{L-1}B_{\Theta}B_{\ell}^{1}/B_{\ell}^{0})\leq 20L \log(2+\max\{B,\mathsf{R},B_{\ell}^{1}/B_{\ell}^{0}\})\). Recalling that

\[L_{\text{icl}}(\boldsymbol{\tilde{\theta}})\leq\inf_{\boldsymbol{\theta}}L_{ \text{icl}}(\boldsymbol{\theta})+2\sup_{\boldsymbol{\theta}}|X_{\boldsymbol{ \theta}}|\]

completes the proof. 

### Proof of Theorem k.2

By Corollary 5, there exists a transformer \(\mathrm{TF}_{\boldsymbol{\theta}}\) such that for every \(\mathsf{P}\) satisfying Assumption A with canonical parameters (and thus in expectation over \(\mathsf{P}\sim\pi\)) and every \(N\geq\widetilde{\mathcal{O}}(d)\), it outputs prediction \(\widetilde{y}_{N+1}=\widetilde{\mathsf{read}}_{\mathsf{J}}(\mathrm{TF}_{ \boldsymbol{\theta}}(\mathbf{H}))\) such that

\[L_{\text{icl}}(\boldsymbol{\theta})=\mathbb{E}_{\mathsf{P}\sim\pi,(\mathcal{D},\mathbf{x}_{N+1},y_{N+1}\sim\mathsf{P})}\bigg{[}\frac{1}{2}(\widetilde{y}_{N +1}-y_{N+1})^{2}\bigg{]}\leq\mathbb{E}_{\mathsf{P}\sim\pi}[L_{\mathsf{P}}( \mathbf{w}_{\mathsf{P}}^{\star})]+\mathcal{O}\bigg{(}\frac{d\sigma^{2}}{N} \bigg{)},\]

where we recall that \(L_{\mathsf{P}}(\mathbf{w}_{\mathsf{P}}^{\star}):=\frac{1}{2}\mathbb{E}_{( \mathbf{x},\mathbf{y})\sim\mathsf{P}}\big{[}(y-\langle\mathbf{w}_{\mathsf{P}}^{ \star},\mathbf{x}\rangle)^{2}\big{]}\). By inspecting the proof, the same result holds if we change \(\mathrm{TF}_{\boldsymbol{\theta}}\) to the clipped version \(\mathrm{TF}_{\boldsymbol{\theta}}^{\mathsf{F}}\) if we choose \(\mathsf{R}^{2}=\mathcal{O}(B_{x}^{2}+B_{y}^{2}+B_{w}^{2}+1)=\mathcal{O}(d+\kappa)\), so that on the good event \(E_{\mathrm{cov}}\cap E_{w}\) considered therein, all intermediate outputs within \(\mathrm{TF}_{\bm{\theta}}\) has \(\left\lVert\cdot\right\rVert_{2,\infty}\leq\mathrm{R}\) and thus the clipping does not modify the transformer output on \(E_{\mathrm{cov}}\cap E_{w}\). Further, recall by (33) that \(\bm{\theta}\) has size bounds

\[L\leq\mathcal{O}\bigg{(}\kappa\log\frac{N\kappa}{\sigma}\bigg{)},\quad\max_{ \ell\in[L]}M^{(\ell)}\leq 3,\quad\left\lVert\bm{\theta}\right\rVert\leq \mathcal{O}(\sqrt{\kappa d}).\]

We can thus apply Theorem K.1 to obtain that the solution \(\widehat{\bm{\theta}}\) to (TF-ERM) with the above choice of \((L,M,B)\) and \(D^{\prime}=0\) (attention-only) satisfies the following with probability at least \(1-\xi\):

\[L_{\text{icl}}(\widehat{\bm{\theta}})\leq\inf_{\bm{\theta}^{ \prime}\in\Theta_{L,M,D^{\prime},B}}L_{\text{icl}}(\bm{\theta}^{\prime})+ \mathcal{O}\Bigg{(}\sqrt{\frac{L^{2}MD^{2}\iota+\log(1/\xi)}{n}}\Bigg{)}\] \[\leq L_{\text{icl}}(\bm{\theta})+\widetilde{\mathcal{O}}\Bigg{(} \sqrt{\frac{L^{2}MD^{2}+\log(1/\xi)}{n}}\Bigg{)}\leq\widetilde{\mathcal{O}} \Bigg{(}\sqrt{\frac{\kappa^{2}d^{2}+\log(1/\xi)}{n}}+\frac{d\sigma^{2}}{N} \Bigg{)}.\]

Above, \(\iota=\mathcal{O}(\log(1+\max\left\{B_{y},\mathsf{R},B\right\}))=\widetilde{ \mathcal{O}}(1)\). This finishes the proof. 

### Proof of Theorem k.3

We invoke Theorem 8 (using the construction in Theorem 7 with a different choice of \(L\)) with the following parameters:

\[L=\widetilde{\mathcal{O}}\big{(}(B_{w}^{\star})^{2}/\sigma^{2}\times(1+d/N) \big{)}=\widetilde{\mathcal{O}}\big{(}\kappa^{2}(1+d/N)\big{)},\quad M=\Theta( 1),\quad D^{\prime}=2d,\]

\[B_{x}=\widetilde{\mathcal{O}}(\sqrt{d}),\quad B_{y}=\widetilde{\mathcal{O}}(B _{w}^{\star}+\sigma),\quad\delta=\bigg{(}\sigma^{2}\frac{1}{B_{y}^{2}N}\bigg{)} ^{2},\]

\[\left\lVert\bm{\theta}\right\rVert\leq B=\mathcal{O}\big{(}R+(1+\lambda_{N}) \beta^{-1}\big{)}\leq\mathcal{O}\Big{(}R+\sigma\sqrt{\log d}\Big{)}\leq \widetilde{\mathcal{O}}(\mathrm{poly}(d,B_{w}^{\star},\sigma)),\]

where \(\widetilde{\mathcal{O}}(\cdot)\) hides polylogarithmic factors in \(d,N,B_{w}^{\star},\kappa\).

Then, Theorem 8 shows that there exists a transformer \(\bm{\theta}\) with \(L\) layers, \(\max_{\ell\in[L]}M^{(\ell)}\leq M\) heads, \(D^{\prime}\) hidden dimension for the MLP layers, and \(\left\lVert\bm{\theta}\right\rVert\leq B\) such that, on almost surely every \(\mathsf{P}\sim\pi\), it returns a prediction \(\widehat{y}_{N+1}\) such that, on the good event \(\mathcal{E}_{0}\) considered therein (over \(\mathcal{D}\sim\mathsf{P}\)) which satisfies \(\mathsf{P}(\mathcal{E}_{0})\geq 1-\delta\),

\[\mathbb{E}_{(\mathbf{x}_{N+1},y_{N+1})\sim\mathsf{P}}\Big{[}(\widehat{y}_{N+1 }-y_{N+1})^{2}\Big{]}\leq\sigma^{2}[1+\mathcal{O}(s\log(d/\delta)/N)].\]

By inspecting the proof, the same result holds if we change \(\mathrm{TF}_{\bm{\theta}}\) to the clipped version \(\mathrm{TF}_{\bm{\theta}}^{\mathrm{R}}\) if we choose \(\mathsf{R}^{2}=\mathcal{O}(B_{x}^{2}+B_{y}^{2}+(B_{w}^{\star})^{2}+1)=\mathcal{ O}(d+(B_{w}^{\star})^{2}+\sigma^{2})\), so that on the good event \(\mathcal{E}_{0}\) considered therein, all intermediate outputs within \(\mathrm{TF}_{\bm{\theta}}\) has \(\left\lVert\cdot\right\rVert_{2,\infty}\leq\mathrm{R}\) and thus the clipping does not modify the transformer output on the good event. On the bad event \(\mathcal{E}_{0}^{c}\), using the same argument as in the proof of Theorem 8, we have

\[\mathbb{E}_{\mathcal{D},(\mathbf{x}_{N+1},y_{N+1})\sim\mathsf{P}}\big{[}1\{ \mathcal{E}_{0}^{c}\}(\widehat{y}_{N+1}-y_{N+1})^{2}\big{]}\leq\sqrt{\mathsf{ P}_{\mathcal{D}}(\mathcal{E}_{0}^{c})}\cdot\big{(}8\mathbb{E}_{y_{N+1}\sim \mathsf{P}}\big{[}B_{y}^{4}+y_{N+1}^{4}\big{]}\big{)}^{1/2}\leq\widetilde{ \mathcal{O}}\bigg{(}\frac{\sigma^{2}}{N}\bigg{)}.\]

Combining the above two bounds and further taking expectation over \(\mathsf{P}\sim\pi\) gives

\[L_{\text{icl}}(\bm{\theta})=\mathbb{E}_{\mathsf{P}\sim\pi,(\mathcal{D},\mathbf{ x}_{N+1},y_{N+1})\sim\mathsf{P}}\Bigg{[}\frac{1}{2}(\widehat{y}_{N+1}-y_{N+1})^{2} \Bigg{]}\leq\sigma^{2}+\widetilde{\mathcal{O}}\big{(}\sigma^{2}s\log d/N\big{)}.\]

We can thus apply Theorem K.1 to obtain that the solution \(\widehat{\bm{\theta}}\) to (TF-ERM) with the above choice of \((L,M,B,D^{\prime})\) satisfies the following with probability at least \(1-\xi\):

\[L_{\text{icl}}(\widehat{\bm{\theta}})\leq\inf_{\widehat{\bm{\theta} }^{\prime}\in\Theta_{L,M,D^{\prime},B}}L_{\text{icl}}(\bm{\theta}^{\prime})+ \mathcal{O}\Bigg{(}\sqrt{\frac{L^{2}(MD^{2}+DD^{\prime})\iota+\log(1/\xi)}{n}} \Bigg{)}\] \[\leq L_{\text{icl}}(\bm{\theta})+\widetilde{\mathcal{O}}\Bigg{(} \sqrt{\frac{L^{2}(MD^{2}+DD^{\prime})+\log(1/\xi)}{n}}\Bigg{)}\] \[\leq\sigma^{2}+\widetilde{\mathcal{O}}\Bigg{(}\sqrt{\frac{\kappa^{4 }d^{2}(1+d/N)^{2}+\log(1/\xi)}{n}}+\sigma^{2}\frac{s\log d}{N}\Bigg{)}.\]

Above, \(\iota=\mathcal{O}(\log(1+\max\left\{B_{y},\mathsf{R},B\right\}))=\widetilde{ \mathcal{O}}(1)\). This finishes the proof. 

### Proof of Theorem K.4

We invoke Theorem 12 and Theorem J.1, which shows that (recalling the input dimension \(D=\Theta(Kd)\)) there exists a transformer \(\bm{\theta}\) with the following size bounds:

\[L\leq\mathcal{O}\left(\sigma_{\min}^{-2}\log(N/\sigma_{\min}) \right),\qquad\max_{\ell\in[L]}M^{(\ell)}\leq M=\mathcal{O}\left(K\right), \qquad\max_{\ell\in[L]}D^{(\ell)}\leq D^{\prime}=\mathcal{O}(K^{2}),\] \[\left\|\bm{\theta}\right\|\leq\mathcal{O}\left(\sigma_{\max}Kd \log(N)\right),\]

such that it outputs \(\widehat{y}_{N+1}\) that satisfies

\[\mathbb{E}_{\pi}\!\left[\frac{1}{2}(y_{N+1}-\widehat{y}_{N+1})^{2} \right]\leq\mathsf{BayesRisk}_{\pi}+\widetilde{\mathcal{O}}\left(\frac{\sigma _{\max}^{2}}{\sigma_{\min}^{2/3}}\Big{(}\frac{\log K}{N}\Big{)}^{1/3}\right).\]

By inspecting the proof, the same result holds if we change \(\mathrm{TF}_{\bm{\theta}}\) to the clipped version \(\mathrm{TF}_{\bm{\theta}}^{\mathsf{R}}\) if we choose \(\mathsf{R}^{2}=\mathcal{O}(B_{x}^{2}+B_{y}^{2}+(B_{w}^{*})^{2}+1)=\mathcal{O }(d+\sigma_{\max}^{2})\), so that on the good event considered therein, all intermediate outputs within \(\mathrm{TF}_{\bm{\theta}}\) has \(\left\|\cdot\right\|_{2,\infty}\leq\mathsf{R}\) and thus the clipping does not modify the transformer output on the good event. Using this clipping radius, we obtain

\[L_{\text{id}}(\bm{\theta})=\mathbb{E}_{\mathsf{P}\sim\pi,\left( \mathcal{D},\mathbf{x}_{N+1},y_{N+1}\right)\sim\mathsf{P}}\!\left[\frac{1}{2} \big{(}\widehat{y}_{N+1}-y_{N+1}\big{)}^{2}\right]\leq\mathsf{BayesRisk}_{\pi }+\widetilde{\mathcal{O}}\left(\frac{\sigma_{\max}^{2}}{\sigma_{\min}^{2/3}} \Big{(}\frac{\log K}{N}\Big{)}^{1/3}\right).\]

We can thus apply Theorem K.1 to obtain that the solution \(\widehat{\bm{\theta}}\) to (TF-ERM) with the above choice of \((L,M,B,D^{\prime})\) satisfies the following with probability at least \(1-\xi\):

\[L_{\text{id}}(\widehat{\bm{\theta}})\leq\inf_{\bm{\theta}^{ \prime}\in\Theta_{L,M,D^{\prime},B}}L_{\text{id}}(\bm{\theta}^{\prime})+ \mathcal{O}\!\left(\sqrt{\frac{L^{2}(MD^{2}+DD^{\prime})\iota+\log(1/\xi)}{n}}\right)\] \[\leq L_{\text{id}}(\bm{\theta})+\widetilde{\mathcal{O}}\!\left( \sqrt{\frac{L^{2}(MD^{2}+DD^{\prime})+\log(1/\xi)}{n}}\right)\] \[\leq\mathsf{BayesRisk}_{\pi}+\widetilde{\mathcal{O}}\!\left( \sqrt{\frac{\sigma_{\min}^{-4}K^{3}d^{2}+\log(1/\xi)}{n}}+\frac{\sigma_{\max} ^{2}}{\sigma_{\min}^{2/3}}\Big{(}\frac{\log K}{N}\Big{)}^{1/3}\right)\!.\]

Above, \(\iota=\mathcal{O}(\log(1+\max\left\{B_{y},\mathsf{R},B\right\}))=\widetilde{ \mathcal{O}}(1)\). This finishes the proof. 

### Proof of Theorem K.5

The proof follows from similar arguments as of Theorem K.3 and Theorem K.4, where we plug in the size bounds (number of layers, heads, and weight norms) from Theorem G.2 and Corollary G.1. 

## Appendix M Experimental details and additional studies

### Additional details for Section 6

Architecture and optimizationWe train a 12-layer encoder-only transformer, where each layer consists of an attention layer as in Definition 1 with \(M=8\) heads, hidden dimension \(D=64\), and ReLU activation (normalized by the sequence length), as well as an MLP layer as in Definition 2 hidden dimension \(D^{\prime}=64\). We add Layer Normalization [3] after each attention and MLP layer to help optimization, as in standard implementations [84]. We append linear read-in layer and linear read-out layer before and after the transformer respectively, both applying a same affine transform to all tokens in the sequence and are trainable. The read-in layer maps any input vector to a \(D\)-dimensional hidden state, and the read-out layer maps a \(D\)-dimensional hidden state to a 1-dimensional scalar.

Each training sequence corresponds to a single ICL instance with \(N\) in-context training examples \(\left\{(\mathbf{x}_{i},y_{i})\right\}_{i=1}^{N}\subset\mathbb{R}^{d}\times \mathbb{R}\) and test input \(\mathbf{x}_{N+1}\in\mathbb{R}^{d}\). The input to the transformer is formatted asin (3) where each token has dimension \(d+1\) (no zero-paddings). The transformer is trained by minimizing the following loss with fresh mini-batches:

\[L(\bm{\theta})=\mathbb{E}_{\mathsf{P}_{\mathsf{P}\sim\pi,(\mathbf{H},y_{N+1}) \sim\mathsf{P}}}[\ell_{\mathsf{P}}(\mathsf{read}_{\mathsf{y}}(\mathrm{TF}_{\bm {\theta}}(\mathbf{H})),y_{N+1})],\] (47)

where the loss function \(\ell_{\mathsf{P}}:\mathbb{R}^{2}\rightarrow\mathbb{R}\) may depend on the training data distribution \(\mathsf{P}\) in general; we use the square loss when \(\mathsf{P}\) is regression data, and the logistic loss when \(\mathsf{P}\) is classification data. We use the Adam optimizer with a fixed learning rate \(10^{-4}\), which we find works well for all our experiments. Throughout all our experiments except for the sparse linear regression experiment in Figure 2(a), we train the model for 300K steps, where each step consists of a (fresh) minibatch with batch size \(64\) in the base mode, and \(K\) minibatches each with batch size \(64\) in the mixture mode.

For the sparse linear regression experiment, we find that minimizing the training objective (47) alone was not enough, e.g. for the learned transformer to achieve better loss than the least squares algorithm (which achieves much higher test loss than the Lasso; cf. Figure 2(a)). To help optimization, we augment (47) with another loss that encourages the second-to-last hidden states to recover the true (sparse) coefficient \(\mathbf{w}_{\star}\):

\[L_{\text{fit-}\mathbf{w}}(\bm{\theta})=\frac{1}{N_{0}}\sum_{j=1}^{N_{0}} \mathbb{E}_{\mathsf{P}=\mathsf{P}_{\mathbf{w}^{\star}}\sim\pi,(\mathbf{H},y_{ N+1})\sim\mathsf{P}}\Bigg{[}\bigg{\|}\Big{[}\mathrm{TF}_{\bm{\theta}}^{(1:L-1)}( \mathbf{H})\Big{]}_{j,(D-d+1):D}-\mathbf{w}^{\star}\Big{\|}_{2}^{2}\Bigg{]}.\] (48)

Specifically, the above loss encourages the first \(N_{0}\leq N\) tokens within the second-to-last layer to be close to \(\mathbf{w}^{\star}\). We choose \(N_{0}=5\) (recall that the total number of tokens is \(N=10\) and sequence length is \(N+1=11\) for this experiment). We minimize the loss \(L(\bm{\theta})+\lambda L_{\text{fit-}\mathbf{w}}(\bm{\theta})\) with \(\lambda=0.1\) for 2M steps for this task.

EvaluationAll evaluations are done on the trained transformer with 6400 test instances. We use the square loss for regression tasks, and the classification error (\(1-\)accuracy) between the true label \(y_{N+1}\in\{0,1\}\) and the predicted label \(1\{\widehat{y}_{N+1}\geq 1/2\}\). We report the means in all experiments, as well as their standard deviations (using one-std error bars) in Figure 1(a), 1(b), 1(a), 1(b). In Figure 1(c), 2(b), 2(c), all standard deviations are sufficiently small (not significantly exceeding the width of the markers), thus we did not show error bars in those plots.

Baseline algorithmsWe implement various baseline machine learning algorithms to compare with the learned transformers. A superset of the algorithms is shown in Figure 2(a):

* Least squares, Logistic regression: Standard algorithms for linear regression and linear classification, respectively. Note that least squares is also a valid algorithm for classification.
* Averaging: The simple algorithm which computes the linear predictor \(\widehat{\mathbf{w}}=\frac{1}{N}\sum_{i=1}^{N}y_{i}\mathbf{x}_{i}\) and predicts \(\widehat{y}_{N+1}=\langle\widehat{\mathbf{w}},\mathbf{x}_{N+1}\rangle\);
* 3-NN: 3-Nearest Neighbors.
* Ridge: Standard ridge regression as in (ICRidge). We specifically consider two \(\lambda\)'s (denoted as lam_1 and lam_2): \(\lambda_{1},\lambda_{2}=(0.005,0.125)\). These are the Bayes-optimal regularization strengths for the noise levels \((\sigma_{1},\sigma_{2})=(0.1,0.5)\) respectively under the noisy linear model (cf. Corollary 6), using the formula \(\lambda^{\star}=d\sigma^{2}/N\), with \((d,N)=(20,40)\).
* Lasso: Standard Lasso as in (ICLasso) with \(\lambda\in\{1,0.1,0.01,0.001\}\).

In Figure 1(c), the ridge_analytical curve plots the expected risk of ridge regression under the noisy linear model over 20 geometrically spaced values of \(\lambda\)'s in between \((\lambda_{1},\lambda_{2})\), using analytical formulae (with Monte Carlo simulations). The Bayes_err_{1,2} indicate the expected risks of \(\lambda_{1}\) on task 1 (with noise \(\sigma_{1}\)) and \(\lambda_{2}\) on task 2 (with noise \(\sigma_{2}\)), respectively.

### Decoder-based architecture

ICL capabilities have also been demonstrated in the literature for decoder-based architectures [31, 2, 47]. There, the transformer can do in-context predictions at every token \(\mathbf{x}_{i}\) using past tokens \(\{(\mathbf{x}_{j},\mathbf{y}_{j})\}_{j\leq i-1}\) as training examples. Here we show that such architectures is also able to perform in-context algorithm selection _at every token_; For results for this architecture on "base" ICL tasks (such as those considered in Figure 2(a)), we refer the readers to Garg et al. [31].

SetupOur setup is the same as the two "mixture" modes (linear model + linear classification model, and noisy linear models with two different noise levels) as in Section 6, except that the architecture is GPT-2 following Garg et al. [31], and the input format is changed to (11) (so that the input sequence has \(2N+1\) tokens) without positional encodings. For every \(i\in[N+1]\), we extract the prediction \(\widehat{y}_{i}\) using a linear read-out function applied on output token \(2i-1\), and the (learnable) linear read-out function is the same across all tokens, similar as in Appendix M.1. The rest of the setup (optimization, training, and evaluation) is the same as in Section 6 & M.1. Note that we also train on the objective (47) for all tokens averaged, instead of for the last test token as in Section 6.

ResultFigure 2 shows the results for noisy linear models with two different noise levels, and Figure 5 shows the results for linear model + linear classification model. We observe that at every token, In both cases, TF_alg_select nearly matches the strongest baseline for both tasks simultaneously, whereas transformers trained on a single task perform suboptimally on the other task. Further, this phenomenon consistently shows up at every token. For example, in Figure 1(a) & 1(b), TF_alg_select matches ridge regression with the optimal \(\lambda\) on all tokens \(i\in\{1,\dots,N\}\) (\(N=40\)). In Figure 1(a) & 1(b), TF_alg_select matches least squares on the regression task and logistic regression on the classification task on all tokens \(i\in[N]\). This demonstrates the in-context algorithm selection capabilities of standard decoder-based transformer architectures.

### Computational resource

All our experiments are performed on 8 Nvidia Tesla A100 GPUs (40GB memory). The total GPU time is approximately 5 days (on 8 GPUs), with the largest individual training run taking about a single day on a single GPU.

Figure 5: In-context algorithm selection abilities of transformers between linear regression and linear classification. _(a,b)_ On these two tasks, a **single transformer** TF_alg_select **simultaneously approaches the performance of the strongest baseline algorithm** Least Squares on linear regression and Logistic Regression on linear classification. _(c)_ At token 40 (using example \(\{0,\dots,39\}\) for training), TF_alg_select matches the performance of the best baseline algorithm for both tasks. _(a,b,c)_ Note that transformers pretrained on a single task (TF_reg, TF_cls) perform near-optimally on their pretraining task but suboptimally on the other task.