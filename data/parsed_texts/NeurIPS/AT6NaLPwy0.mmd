# A Reduction-based Framework for Sequential Decision Making with Delayed Feedback

Yunchang Yang\({}^{1}\) Han Zhong\({}^{1}\) Tianhao Wu\({}^{2}\) Bin Liu\({}^{3}\) Liwei Wang\({}^{1,4}\) Simon S. Du\({}^{5}\)

\({}^{1}\)Center for Data Science, Peking University

\({}^{2}\)University of California, Berkeley \({}^{3}\)Zhejiang Lab

\({}^{4}\)National Key Laboratory of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{5}\)University of Washington

Equal contribution. Correspondence to Yunchang Yang \(\langle\)yangyc@pku.edu.cn\(\rangle\) and Han Zhong \(\langle\)hanzhong@stu.pku.edu.cn\(\rangle\).

###### Abstract

We study stochastic delayed feedback in general sequential decision-making problems, which include bandits, single-agent Markov decision processes (MDPs), and Markov games (MGs). We propose a novel reduction-based framework, which turns any multi-batched algorithm for sequential decision making with instantaneous feedback into a sample-efficient algorithm that can handle stochastic delays in sequential decision-making problems. By plugging different multi-batched algorithms into our framework, we provide several examples demonstrating that our framework not only matches or improves existing results for bandits, tabular MDPs, and tabular MGs, but also provides the first line of studies on delays in sequential decision making with function approximation. In summary, we provide a complete set of sharp results for single-agent and multi-agent sequential decision-making problems with delayed feedback.

## 1 Introduction

Delayed feedback is a common and fundamental problem in sequential decision making (Sutton and Barto, 2018; Lattimore and Szepesvari, 2020; Zhang et al., 2021). Taking the recommendation system as an example, delayed feedback is an inherent part of this problem. Specifically, the learner adjusts her recommendation after receiving users' feedback, which may take a random delay after the recommendation is issued. More examples include but are not limited to robotics (Mahmood et al., 2018) and video steaming (Changuel et al., 2012). Furthermore, the delayed feedback issue is more serious in multi-agent systems (Chen et al., 2020) since observing the actions of other agents' may happen with a varying delay.

Although handling delayed feedback is a prominent challenge in practice, the theoretical understanding of delays in sequential decision making is limited. Even assuming the delays are stochastic (delays are sampled from a fixed distribution), the nearly minimax optimal result is only obtained by works on multi-armed bandits and contextual bandits (Agarwal and Duchi, 2011; Dudik et al., 2011; Joulani et al., 2013; Vernade et al., 2020, 2017; Gael et al., 2020). For more complex problems such as linear bandits (Lancewicki et al., 2021) and tabular reinforcement learning (RL) (Howson et al., 2022), the results are suboptimal. Meanwhile, to the best of our knowledge, there are no theoretical guarantees in RL with function approximation and multi-agent RL settings. Hence, we focus on stochastic delays and aim to answer the following two questions in this work:

1. Can we provide sharper regret bounds for basic models such as linear bandits and tabular RL?2. Can we handle delays in (multi-agent) sequential decision making in the context of function approximation?

We answer these two questions affirmatively by proposing a reduction-based framework for stochastic delays in both single-agent and multi-agent sequential decision making. Our contributions are summarized below.

Our Contributions.Our main contribution is proposing a new reduction-based framework for both single-agent and multi-agent sequential decision making with stochastic delayed feedback. The proposed framework can convert any multi-batched algorithm (cf. Section 3) for sequential decision making with instantaneous feedback to a provably efficient algorithm that is capable of handling stochastic delayed feedback in sequential decision making. Unlike previous works which require case by case algorithm design and analysis, we provide a unified framework to solve a wide range of problems all together.

Based on this framework, we obtain a complete set of results for single-agent and multi-agent sequential decision making with delayed feedback. In specific, our contributions can be summarized as follows:

* **New framework:** We propose a generic algorithm that can be integrated with any multi-batched algorithm in a black-box fashion. Meanwhile, we provide a unified theoretical analysis for the proposed generic algorithm.
* **Improved results and new results:** By applying our framework to different settings, we obtain state-of-the-art regret bounds for multi-armed bandits, and derive sharper results for linear bandits and tabular RL, which significantly improve existing results (Lancewicki et al., 2021; Howson et al., 2022). We also provide the first result for single-agent RL with delayed feedback in the context of linear or even general function approximation;
* **New algorithms:** We show that delayed feedback in Markov games (MGs) can also be handled by our framework. By plugging several new proposed multi-batched algorithms for MGs into our framework, we not only improve recent results on tabular MGs (Zhang et al., 2022), but

\begin{table}
\begin{tabular}{|c|c|c|} \hline Setting & Previous Result & This Work \\ \hline \hline \multirow{2}{*}{\begin{tabular}{c} Multi-armed \\ bandit \\ \end{tabular} } & \(O(\frac{A\log K}{q\min_{i}x_{i}}+d_{r}(q)\log K)\) & \(O\left(\frac{A\log K}{q\min_{i}\Delta_{i}}+\frac{\log^{2}K}{q}+d_{r}(q)\log K\right)\) \\  & (Lancewicki et al., 2021) & \(O\left(\frac{A\log K}{q\min_{i}\Delta_{i}}+\mathbb{E}[\tau]\log K\right)\) \\ \hline \multirow{2}{*}{\begin{tabular}{c} Linear \\ bandit \\ \end{tabular} } & \(O(d\sqrt{K}+d^{3/2}\mathbb{E}[\tau])\) & \(\tilde{O}\left(\frac{1}{4}d\sqrt{K}+d_{r}(q)\right)\) \\  & \(\tilde{O}(d\sqrt{K}+\mathbb{E}[\tau])\) & \(\tilde{O}(d\sqrt{K}+\mathbb{E}[\tau])\) \\  & (Vakil et al., 2023), concurrent work & \(\tilde{O}(d\sqrt{K}+\mathbb{E}[\tau])\) \\ \hline \multirow{2}{*}{\begin{tabular}{c} Tabular \\ MDP \\ \end{tabular} } & \(\tilde{O}(\sqrt{H^{3}SAK}+H^{2}SA\mathbb{E}[\tau]\log K)\) & \(\tilde{O}\left(\frac{1}{q}\sqrt{SAH^{3}K}+H(H+\log\log K)d_{r}(q)\right)\) \\  & (Hewison et al., 2022) & \(\tilde{O}\left(\sqrt{SAH^{3}K}+H(H+\log\log K)\mathbb{E}[\tau]\right)\) \\ \hline \multirow{2}{*}{\begin{tabular}{c} Linear \\ MDP \\ \end{tabular} } & \(--\) & \(\tilde{O}(\frac{1}{4}\sqrt{d^{3}H^{4}K}+dH^{2}d_{r}(q)\log K)\) \\ \cline{2-2}  & & \(\tilde{O}(\sqrt{d^{3}H^{4}K}+dH^{2}\mathbb{E}[\tau])\) \\ \hline \multirow{2}{*}{\begin{tabular}{c} RL with general \\ function approximation \\ \end{tabular} } & \(--\) & \(\tilde{O}\left(\frac{1}{4}\sqrt{\dim_{\mathbb{E}}^{2}(\tau,1/K)H^{4}K}+H^{2} \dim_{\mathbb{E}}(\mathcal{F},1/K)d_{r}(q)\right)\) \\  & & \(\tilde{O}\left(\sqrt{\dim_{\mathbb{E}}^{2}(\mathcal{F},1/K)H^{4}K}+H^{2} \dim_{\mathbb{E}}(\mathcal{F},1/K)\mathbb{E}[\tau]\right)\) \\ \hline \multirow{2}{*}{\begin{tabular}{c} Tabular two-player \\ zero-sum MG \\ \end{tabular} } & \(--\) & \(\tilde{O}(\frac{1}{4}\left(\sqrt{H^{3}SABK}+H^{3}S^{2}AB\right)+H^{2}SABd_{r}(q) \log K)\) \\ \cline{2-2}  & & \(\tilde{O}(\sqrt{H^{3}SABK}+H^{3}S^{2}AB+H^{2}SAB\mathbb{E}[\tau]\log K)\). \\ \hline \multirow{2}{*}{\begin{tabular}{c} Linear two-player \\ zero-sum MG \\ \end{tabular} } & \(--\) & \(\tilde{O}\left(\frac{1}{4}\sqrt{d^{3}H^{4}K}+dH^{2}d_{r}(q)\log K\right)\) \\ \cline{2-2}  & & \(\tilde{O}\left(\sqrt{d^{3}H^{4}K}+dH^{2}\mathbb{E}[\tau]\log K\right)\) \\ \hline \multirow{2}{*}{
\begin{tabular}{c} Tabular multi-player \\ general-sum MG \\ \end{tabular} } & \(\tilde{O}\left(H^{3}\sqrt{SA_{\max}}+H^{3}\sqrt{\mathbb{E}[\tau]K}\right)\) & \(\tilde{O}\left(\frac{1}{4}\sqrt{H^{5}SA_{\max}K}+H^{2}nSd_{r}(q)\log K\right)\) \\ \cline{2-2}  & (Zhang et al., 2022) & \(\tilde{O}\left(\sqrt{H^{5}SA_{\max}K}+H^{2}nS\mathbb{E}[\tau]\log K\right)\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of bounds for MSDM with delayed feedback. We denote by \(K\) the number of rounds the agent plays (episodes in RL), \(A\) and \(B\) the number of actions (or arms), \(A_{\max}=\max_{i}A_{i}\), \(S\) the number of states, \(H\) the length of an episode, \(d\) the linear feature dimension, and \(\dim_{E}(\mathcal{F},1/K)\) the eluder dimension of general function class \(\mathcal{F}\), \(d_{\tau}(q)\) is the quantile function of the random variable \(\tau\) and \(n\) is the number of players.

also present a new result for linear MGs. As a byproduct, we provide the first line of study on multi-batched algorithms for MGs, which might be of independent interest.

Our results and comparisons with existing works are summarized in Table 1.

### Related Works

Bandit/MDP with delayed feedback.Stochastically delayed feedback has been studied a lot in the multi-armed bandit and contextual bandit problems (Agarwal and Duchi, 2011; Dudik et al., 2011; Joulani et al., 2013; Vernade et al., 2020, 2017; Gael et al., 2020; Huang et al., 2023). A recent work (Lancewicki et al., 2021) shows that under stochastic delay, elimination algorithm performs better than UCB algorithm, and gives tight upper and lower bounds for multi-armed bandit setting. Howson et al. (2021) solves the linear bandit setting by adapting LinUCB algorithm. The concurrent work of Vakili et al. (2023) provides a bound for the kernel bandit setting with delayed feedback. Our work does not consider the kernel setting, but we conjecture that our framework can handle this problem by designing a multi-batched algorithm for kernel bandits.

In RL, despite practical importance, there is limited literature on stochastical delay in RL. Lancewicki et al. (2022) and Jin et al. (2022) considered adversarial delays. Their regret depends on the sum of the delays, the number of states and the number of steps per episode. However, when reducing their results to stochastic delay, the bounds will be too loose. Howson et al. (2022) considered stochastic delay in tabular MDP, but their bound is not tight. As far as we know, there is no work considering linear MDP and general function approximation with delayed feedback.

Another line of work considers adversarially delayed feedback (Lancewicki et al., 2022; Jin et al., 2022; Quanrud and Khashabi, 2015; Thune et al., 2019; Bistritz et al., 2019; Zimmert and Seldin, 2020; Ito et al., 2020; Gyorgy and Joulani, 2021; Van Der Hoeven and Cesa-Bianchi, 2022). In the adversarial setting the state-of-the-art result is of form \(\sqrt{K+D}\), where \(K\) is the number of the episodes and \(D\) is the total delay. Note that when adapting their result to the stochastic setting, the bound becomes \(\sqrt{K+K\tau}\) where \(\tau\) is the expectation of delay, while in stochastic delay setting the upper bound can be \(\sqrt{K}+\tau\). Therefore, a direct reduction from adversarial delay setting to stochastic delay setting will result in a suboptimal regret bound.

Low switching cost algorithm.Bandit problem with low switching cost has been widely studied in past decades. Cesa-Bianchi et al. (2013) showed an \(\tilde{\Theta}(K^{\frac{3}{3}})\) regret bound under adaptive adversaries and bounded memories. Perchet et al. (2016) proved a regret bound of \(\tilde{\Theta}(K^{\frac{1}{1-2^{1-M}}})\) for the two-armed bandit problem within \(M\) batches, and later Gao et al. (2019) extended their result to the general \(A\)-armed case. In RL, by doubling updates, the global switching cost is \(O(SAH\log_{2}(K))\) while keeping the regret \(\tilde{O}(\sqrt{SAKH^{3}})\)(Azar et al., 2017). Recently Zhang et al. (2022) proposed a policy elimination algorithm that achieves \(\tilde{O}(\sqrt{SAKH^{3}})\) regret and \(O(H+\log_{2}\log_{2}(K))\) switching cost. Besides, Gao et al. (2021) generalized the problem to linear MDPs, and established a regret bound of \(\tilde{O}(\sqrt{d^{3}H^{4}K})\) with \(O(dH\log(K))\) global switching cost. Recent work Qiao et al. (2022) achieved \(O(HSA\log_{2}\log_{2}(K))\) switching cost and \(\tilde{O}(\mathrm{poly}(S,A,H)\sqrt{K})\) regret with a computational inefficient algorithm. And Kong et al. (2021) consider MDP with general function approximation settings.

Markov Games.A standard framework to capture multi-agent RL is Markov games (also known as stochastic games) (Shapley, 1953). For two-player zero-sum Markov games, the goal is to solve for the Nash equilibrium (NE) (Nash, 1951). Recently, many works study this problem under the generative model (Zhang et al., 2020; Li et al., 2022), offline setting (Cui and Du, 2022, 2022; Xiong et al., 2022; Xiong et al., 2022; Yan et al., 2022), and online setting (Bai and Jin, 2020; Bai et al., 2020; Liu et al., 2021; Tian et al., 2021; Xie et al., 2020; Chen et al., 2021; Jin et al., 2022; Xiong et al., 2022; Huang et al., 2021). Our work is mostly related to Liu et al. (2021) and Xie et al. (2020). Specifically, Liu et al. (2021) studies online tabular two-player zero-sum MGs and proposes an algorithm with \(O(\sqrt{SABH^{3}K})\) regret. Xie et al. (2020) focused on online linear MGs provide an algorithm that enjoys \(O(\sqrt{d^{3}H^{4}K})\) regret. We also remark that Jin et al. (2022); Xiong et al. (2022); Huang et al. (2021) establish sharper regret bound for linear MGs but the algorithms therein are computationally inefficient. There is also a line of works (Liu et al., 2021; Jin et al., 2021;Song et al., 2021; Zhong et al., 2021; Mao and Basar, 2022; Cui and Du, 2022; Jin et al., 2022c; Daskalakis et al., 2022) studying multi-player general-sum MGs. Among these works, Jin et al. (2021); Song et al. (2021); Mao and Basar (2022) use V-learning algorithms to solve coarse correlated equilibrium (CCE). But all these algorithms can not handle delayed feedback nor are multi-batched algorithms that can be plugged into our framework. To this end, we present the first line of studies on multi-batched algorithms for MGs and provide a complete set of results for MGs with delayed feedback.

A very recent but independent work (Zhang et al., 2022) also studies multi-agent reinforcement learning with delays. They assume that the reward is delayed, and aim to solve for coarse correlated equilibrium (CCE) in general-sum Markov games. Our work has several differences from Zhang et al. (2022). First, they study the setting where the total delay is upper bounded by a certain threshold, while we focus on the setup with stochastic delay. Besides, their method seems only applicable for reward delay, while we allow the whole trajectory feedback to be delayed. Second, in the stochastic delay setting the delay-dependent term in their bound is worse than ours. Specifically, their bound scales as \(O(H^{3}\sqrt{S\mathbb{E}[\tau]K})\), while ours is \(O(H^{2}nS\mathbb{E}[\tau])\) where \(n\) is the number of players. Finally, it seems that their results cannot be extended to the multi-agent RL with function approximation.

## 2 Preliminary

The objective of this section is to provide a unified view of the settings considered in this paper, i.e., bandits, Markov Decision Processes (MDPs) and multi-agent Markov Games. We consider a general framework for interactive decision making, which we refer to as Multi-agent Sequential Decision Making (MSDM).

NotationsWe use \(\Delta(\cdot)\) to represent the set of all probability distributions on a set. For \(n\in\mathbb{N}_{+}\), we denote \([n]=\{1,2,\ldots,n\}\). We use \(O(\cdot),\Theta(\cdot),\Omega(\cdot)\) to denote the big-O, big-Theta, big-Omega notations. We use \(\widetilde{O}(\cdot)\) to hide logarithmic factors.

Multi-agent Sequential Decision MakingA Multi-agent Sequential Decision Making problem can be represented as a tuple \(M=(n,\mathcal{S},\{\mathcal{A}_{i}\}_{i=1}^{n},H,\{p_{h}\}_{h=1}^{H},s_{1},\{ r_{h}\}_{h=1}^{H})\), where \(n\) is the number of agents, \(\mathcal{S}\) is the state space, \(\mathcal{A}_{i}\) is the action space of player \(i\), \(H\) is the length of each episode and \(s_{1}\) is the initial state. We define \(\mathcal{A}=\bigotimes_{i}\mathcal{A}_{i}\) as the whole action space.

At each stage \(h\), for every player \(i\), every state-action pair \((s,a_{1},...,a_{n})\) is characterized by a reward distribution with mean \(r_{i,h}(s,a_{1},...,a_{n})\) and support in \([0,1]\), and a transition distribution \(p_{h}(\cdot|s,a)\) over next states. We denote by \(S=|\mathcal{S}|\) and \(A_{i}=|\mathcal{A}_{i}|\).

The protocol proceeds in several episodes. At the beginning of each episode, the environment starts with initial state \(s_{1}\). At each step \(h\in[1,H]\), the agents observe the current state \(s_{h}\), and take their actions \((a_{1},...,a_{n})\) respectively. Then the environment returns a reward signal, and transits to next state \(s_{h+1}\).

A (random) policy \(\pi_{i}\) of the \(i^{\text{th}}\) player is a set of \(H\) maps \(\pi_{i}:=\{\pi_{i,h}:\Omega\times\mathcal{S}\rightarrow\mathcal{A}_{i}\}_{h \in[H]}\), where each \(\pi_{i,h}\) maps a random sample \(\omega\) from a probability space \(\Omega\) and state \(s\) to an action in \(\mathcal{A}_{i}\). A joint (potentially correlated) policy is a set of policies \(\{\pi_{i}\}_{i=1}^{m}\), where the same random sample \(\omega\) is shared among all agents, which we denote as \(\pi=\pi_{1}\odot\pi_{2}\odot\ldots\odot\pi_{m}\). We also denote \(\pi_{-i}\) as the joint policy excluding the \(i^{\text{th}}\) player. For each stage \(h\in[H]\) and any state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\), the value function and Q-function of a policy \(\pi\) are defined as:

\[Q_{i,h}^{\pi}(s,a)=\mathbb{E}\bigg{[}\sum_{h^{\prime}=h}^{H}r_{i,h^{\prime}} \,\Big{|}\,s_{h}=s,a_{h}=a,\pi\bigg{]},\quad V_{i,h}^{\pi}(s)=\mathbb{E}\bigg{[} \sum_{h^{\prime}=h}^{H}r_{i,h^{\prime}}\,\Big{|}\,s_{h}=s,\pi\bigg{]}.\]

For each policy \(\pi\), we define \(V_{H+1}^{\pi}(s)=0\) and \(Q_{H+1}^{\pi}(s,a)=0\) for all \(s\in\mathcal{S},a\in\mathcal{A}\). Sometimes we omit the index \(i\) when it is clear from the context.

For the single agent case (\(n=1\)), the goal of the learner is to maximize the total reward it receives. There exists an optimal policy \(\pi^{\star}\) such that \(Q_{h}^{\star}(s,a)=Q_{h}^{\pi^{\star}}(s,a)=\max_{\pi}Q_{h}^{\pi}(s,a)\) satisfies the optimal Bellman equations

\[Q_{h}^{\star}(s,a)=r_{h}(s,a)+\mathbb{E}_{s^{\prime}\sim p_{h}(s,a)}[V_{h+1}^ {\star}(s^{\prime})],\quad V_{h}^{\star}(s)=\max_{a\in\mathcal{A}}\{Q_{h}^{ \star}(s,a)\},\quad\forall(s,a)\in\mathcal{S}\times\mathcal{A}.\]Then the optimal policy is the greedy policy \(\pi_{h}^{\star}(s)=\arg\max_{a\in\mathcal{A}}\{Q_{h}^{\star}(s,a)\}\). And we evaluate the performance of the learner through the notion of single-player regret, which is defined as

\[\mathrm{Regret}(K)=\sum_{k=1}^{K}\left(V_{1}^{\star}-V_{1}^{\pi_{k}}\right)(s_{ 1}).\] (1)

In the case of multi-player general-sum MGs, for any policy \(\pi_{-i}\), the best response of the \(i^{\text{th}}\) player is defined as a policy of the \(i^{\text{th}}\) player which is independent of the randomness in \(\pi_{-i}\), and achieves the highest value for herself conditioned on all other players deploying \(\pi_{-i}\). In symbol, the best response is the maximizer of \(\max_{\pi_{i}^{\prime}}V_{i,1}^{\pi_{i}^{\prime}\times\pi_{-i}}(s_{1})\) whose value we also denote as \(V_{i,1}^{\dagger,\pi_{-i}}(s_{1})\) for simplicity.

For general-sum MGs, we aim to learn the Coarse Correlated Equilibrium (CCE), which is defined as a joint (potentially correlated) policy where no player can increase her value by playing a different independent strategy. In symbol,

**Definition 1** (Coarse Correlated Equilibrium).: _A joint policy \(\pi\) is a CCE if \(\max_{i\in[m]}(V_{i,1}^{\dagger,\pi_{-i}}-V_{i,1}^{\pi})\left(s_{1}\right)=\) 0. A joint policy \(\pi\) is a \(\epsilon\)-approximate CCE if \(\max_{i\in[m]}(V_{i,1}^{\dagger,\pi_{-i}}-V_{i,1}^{\pi})(s_{1})\leq\epsilon\)._

For any algorithm that outputs policy \(\hat{\pi}^{k}\) at episode \(k\), we measure the performance of the algorithm using the following notion of regret:

\[\mathrm{Regret}(K)=\sum_{k=1}^{K}\max_{j}\left(V_{j,h}^{\dagger,\hat{\pi}^{k} _{-j,h}}-V_{j,h}^{\hat{\pi}^{k}_{h}}\right)(s_{1}).\] (2)

One special case of multi-player general-sum MGs is two-player zero-sum MGs, in which there are only two players and the reward satisfies \(r_{2,h}=-r_{1,h}\). In zero-sum MGs, there exist policies that are the best responses to each other. We call these optimal strategies the Nash equilibrium of the Markov game, which satisfies the following minimax equation:

\[\sup_{\mu}\inf_{\nu}V_{h}^{\mu,\nu}(s)=V_{h}^{\mu^{\star},\nu^{\star}}(s)=\inf _{\nu}\sup_{\mu}V_{h}^{\mu,\nu}(s).\]

We measure the suboptimality of any pair of general policies \((\hat{\mu},\hat{\nu})\) using the gap between their performance and the performance of the optimal strategy (i.e., Nash equilibrium) when playing against the best responses respectively:

\[V_{1}^{\dagger,\hat{\nu}}\left(s_{1}\right)-V_{1}^{\hat{\mu},\dagger}\left(s_{ 1}\right)=\left[V_{1}^{\dagger,\hat{\nu}}\left(s_{1}\right)-V_{1}^{\star}\left( s_{1}\right)\right]+\left[V_{1}^{\star}\left(s_{1}\right)-V_{1}^{\hat{\mu}, \dagger}\left(s_{1}\right)\right].\]

Let \((\mu^{k},\nu^{k})\) denote the policies deployed by the algorithm in the \(k^{\text{th}}\) episode. The learning goal is to minimize the two-player regret of \(K\) episodes, defined as

\[\mathrm{Regret}(K)=\sum_{k=1}^{K}\left(V_{1}^{\dagger,\nu^{k}}-V_{1}^{\mu^{k},\dagger}\right)\left(s_{1}\right).\] (3)

One can see that (3) is a natural reduction of (2) from general-sum games to zero-sum games.

MSDM with Delayed FeedbackWe focus on the case where there is delay between playing an episode and observing the sequence of states, actions and rewards sampled by the agent; we refer to this sequence as feedback throughout this paper.

We assume that the delays are stochastic, i.e. we introduce a random variable for the delay between playing the \(k\)-th episode and observing the feedback of the \(k\)-th episode, which we denote by \(\tau_{k}\). We assume the delays are i.i.d:

**Assumption 1**.: _The delays \(\{\tau_{k}\}_{k=1}^{k}\) are positive, independent and identically distributed random variables: \(\tau_{k}\overset{i.i.d.}{\sim}f_{\tau}(\cdot)\). We denote the expectation of \(\tau\) as \(\mathbb{E}[\tau]\) (which may be infinite), and denote the quantile function of \(\tau\) as \(d_{\tau}(q)\), i.e._

\[d_{\tau}(q)=\min\left\{\gamma\in\mathbb{N}\mid\Pr\left[\tau\leq\gamma\right] \geq q\right\}.\] (4)When delay exists, the feedback associated with an episode does not return immediately. Instead, the feedback of the \(k\)-th episode cannot be observed until the \(k+\tau_{k}\)-th episode ends.

Sometimes in order to control the scale and distribution of the delay, we make the assumption that the delays are \((v,b)\) subexponential random variables. This is a standard assumption used in previous works Howson et al. (2021), though it is not always necessary in our paper.

**Assumption 2**.: _The delays are non-negative, independent, and identically distributed \((v,b)\)- subexponential random variables. That is, their moment generating function satisfies the following inequality_

\[\mathbb{E}\left[\exp\left(\gamma\left(\tau_{t}-\mathbb{E}\left[\tau_{t}\right] \right)\right)\right]\leq\exp\left(\frac{1}{2}v^{2}\gamma^{2}\right)\] (5)

_for some \(v\) and \(b\), and all \(|\gamma|\leq 1/b\)._

## 3 Multi-batched Algorithm for MSDMs

In this section we mainly discuss multi-batched algorithms for MSDMs. This section aims to provide a formal definition of multi-batched algorithm, and give some examples of multi-batched algorithm for the ease of understanding.

Formally, a multi-batched algorithm consists of several batches. At the beginning of the \(m^{\text{th}}\) batch, the algorithm outputs a policy sequence \(\pi^{m}=(\pi_{1}^{m},\pi_{2}^{m},...,\pi_{l^{m}}^{m})\) for this batch using data collected in previous batches, along with a stopping criteria \(SC\), which is a mapping from a dataset to a boolean value. Then the agent executes the policy sequence \(\pi^{m}\) for several episodes and collects feedback to the current dataset \(\mathcal{D}\), until the stopping criteria are satisfied (i.e. \(SC(\mathcal{D})=1\)). If the length of the \(m^{\text{th}}\) batch exceeds \(l_{m}\), then we simply run the policy sequence cyclically, which means in the \(k^{\text{th}}\) episode we run policy \(\pi_{k\text{ mod }l^{m}}^{m}\). Algorithm 1 describes a protocol of multi-batched algorithm.

```
1:Initialize dataset \(\mathcal{D}^{0}=\emptyset\)
2:for batch \(m=1,...,M\)do
3:\(\mathcal{D}=\emptyset\)
4:\(k=0\)
5: Calculate a policy sequence \(\pi^{m}=(\pi_{1}^{m},\pi_{2}^{m},...,\pi_{l^{m}}^{m})\) using previous data \(\mathcal{D}^{m-1}\), and a stopping criteria \(SC\)
6:while\(SC(\mathcal{D})=0\)do
7:\(k\gets k+1\)
8: In episode \(k\), execute \(\pi_{k\text{ mod }l^{m}}^{m}\) and collect trajectory feedback \(o^{k}\)
9:\(\mathcal{D}=\mathcal{D}\cup\{o^{k}\}\)
10:endwhile
11: Update dataset \(\mathcal{D}^{m}=\mathcal{D}^{m-1}\cup\mathcal{D}\)
12:endfor
13:Output: A policy sequence \(\{\pi^{m}\}\) ```

**Algorithm 1** Protocol of Multi-batched Algorithm

Note that the major requirement is that the policy to be executed during the batch should be calculated at the beginning of the batch. Therefore, algorithms that calculate the policy at the beginning of each episode do not satisfy this requirement. For example, UCB algorithms in bandits (Lattimore and Szepesvari, 2020) and UCBVI (Azar et al., 2017) are not multi-batched algorithms.

Relation with low-switching cost algorithmOur definition of multi-batched algorithm is similar to that of low-switching cost algorithm, but is more general.

For any algorithm, the switching cost is the number of policy changes in the running of the algorithm in \(K\) episodes, namely:

\[N_{\text{switch}}\,\triangleq\,\sum_{k=1}^{K-1}\mathbb{I}\left\{\pi_{k}\neq \pi_{k+1}\right\}.\]

Most algorithms with \(N_{\text{switch}}\,\) low-switching cost are multi-batched algorithms with \(N_{\text{switch}}\,\) batches. For example, the policy elimination algorithm proposed in Zhang et al. (2022) is a multi-batchedalgorithm with \(2H+\log\log K\) batches. And the algorithm for linear MDP with low global switching cost in Gao et al. (2021) is a multi-batched algorithm with \(O(dH\log K)\) batches. On the other hand, the difference between batches and switching cost is that we do not require the policy to be the same during a batch. Therefore, a multi-batched algorithm with \(N\) batches may not be an algorithm with \(N\) switching cost. For example, the Phase Elimination Algorithm in linear bandit with finite arms setting (Lattimore and Szepesvari, 2020) is a multi-batched algorithm with \(O(\log K)\) batches, but the switching cost is \(O(A\log K)\) which is much larger.

## 4 A Framework for Sequential Decision Making with Delayed Feedback

In this section, we introduce our framework for sequential decision making problems with stochastic delayed feedback. We first describe our framework, which converts any multi-batched algorithm to an algorithm for MSDM with delayed feedback. Then we provide theoretical guarantees for the performance of such algorithms.

Before formally introducing our framework, we first give an intuitive explanation of why multi-batched algorithms work for delayed settings. Recall that Lancewicki et al. (2021) gives a lower bound showing that UCB-type algorithms are suboptimal in multi-armed bandits with stochastic delay (see Theorem 1 in Lancewicki et al. (2021)). The main idea of the lower bound is that UCB algorithm will keep pulling a suboptimal arm until the delayed feedback arrives and the policy is updated. Therefore, if the policy updates too frequently, then the algorithm has to spend more time waiting for the update, and the effect of delay will be greater. This observation motivates us to consider multi-batched algorithms, which have a low frequency of policy updating.

For any sequential decision making problem, assume that we have a multi-batched algorithm \(\mathcal{ALG}\) for the problem without delay. Then in the existence of delay, we can still run \(\mathcal{ALG}\). The difference is, in each batch of the algorithm, we run some extra steps until we observe enough feedback so that the stopping criteria are satisfied. For example, if the delay \(\tau\) is a constant and the stopping criteria is that the policy \(\pi_{k}\) is executed for a pre-defined number of times \(t_{k}\), then we only need to run \(\pi_{k}\) for \(t_{k}+\tau\) times. After the extra steps are finished, we can collect \(t_{k}\) delayed observations, which meet the original stopping criteria. Then we can continue to the next batch. Algorithm 2 gives a formal description.

```
1:Require: A multi-batched algorithm for the undelayed environment \(\mathcal{ALG}\)
2:Initialize dataset \(\mathcal{D}^{0}=\emptyset\)
3:for batch \(m=1,...,M\)do
4:\(\mathcal{ALG}\) calculates a policy \(\pi^{m}\) (or a policy sequence) and a stopping criteria \(SC\)
5:\(\mathcal{D}=\emptyset\)
6:while\(SC(\mathcal{D})=0\)do
7:\(k\gets k+1\)
8: In episode \(k\), execute \(\pi^{m}\)
9: Collect trajectory feedback in this batch that is observed by the end of the episode: \(\{o^{t}:t+\tau_{t}=k\}\)
10:\(\mathcal{D}=\mathcal{D}\cup\{o^{t}:t+\tau_{t}=k\}\)
11:endwhile
12: Update dataset \(\mathcal{D}^{m}=\mathcal{D}^{m-1}\cup\mathcal{D}\)
13:endfor
14:Output: A policy sequence \(\{\pi^{m}\}\) ```

**Algorithm 2** Multi-batched Algorithm With Delayed Feedback

For the performance of Algorithm 2, we have the following theorem, which relates the regret of delayed MSDM problems with the regret of multi-batched algorithms in the undelayed environment.

**Theorem 1**.: _Assume that in the undelayed environment, we have a multi-batched algorithm with \(N_{b}\) batches, and the regret of the algorithm in \(K\) episodes can be upper bounded by \(\widetilde{\mathrm{Regret}}(K)\) with probability at least \(1-\delta\). Then in the delayed feedback case, with probability at least \(1-\delta\), the regret can be upper bounded by_

\[\mathrm{Regret}(K)\leq\frac{1}{q}\widetilde{\mathrm{Regret}}(K)+\frac{2HN_{b} \log(K/\delta)}{q}+HN_{b}d_{\tau}(q)\] (6)_for any \(q\in(0,1)\). In addition, if the delays satisfy Assumption 2, then with probability at least \(1-\delta\) the regret can be upper bounded by_

\[\mathrm{Regret}(K)\leq\widetilde{\mathrm{Regret}}(K)+HN_{b}\left(\mathbb{E}[ \tau]+C_{\tau}\right),\] (7)

_where \(C_{\tau}=\min\{\sqrt{2v^{2}\log(3KH/(2\delta))},2b\log(3KH/(2\delta))\}\) is a problem-independent constant._

The proof is in Appendix A. The main idea is simple: in each batch, we can control the number of extra steps when waiting for the original algorithm to collect enough delayed feedback. And the extra regret can be bounded by the product of the number of batches and the number of extra steps per batch. Note that in Eq. (6) we do not need Assumption 2. This makes (6) more general.

Applying Theorem 1 to different settings, we will get various types of results. For example, in multi-armed bandit setting, using the Batched Successive Elimination (BaSE) algorithm in Gao et al. (2019), we can get a \(O(\frac{1}{q}A\log(K)+d_{\tau}(q)\log(K)\) upper bound from (6), which recovers the SOTA result in Lancewicki et al. (2021) up to a \(\log K\) factor in the delay-dependent term. We leave the details to Appendix C, and list the main results in Table 1.

## 5 Results for Markov Games

As an application, we study Markov games with delayed feedback using our framework. We first develop new multi-batched algorithms for Markov games, then combine them with our framework to obtain regret bounds for the delayed setting.

### Tabular Zero-Sum Markov Game

First we study the tabular zero-sum Markov game setting. The algorithm is formally described in Algorithm 4 in Appendix D.

Our algorithm can be decomposed into two parts: an estimation step and a policy update step. The estimation step largely follows the method in Liu et al. (2021). For each \((h,s,a,b)\), we maintain an optimistic estimation \(\bar{Q}_{h}^{k}(s,a,b)\) and a pessimistic estimation \(\underline{Q}_{h}^{k}(s,a,b)\) of the underlying \(Q_{h}^{\star}(s,a,b)\), by value iteration with bonus using the empirical estimate of the transition \(\hat{\mathbb{P}}\). And we compute the Coarse Correlated Equilibrium (CCE) policy \(\pi\) of the estimated value functions Xie et al. (2020). A CCE always exists, and can be computed efficiently.

In the policy update step, we choose the CCE policy w.r.t. the current estimated value function. Note that we use the doubling trick to control the number of updates: we only update the policy when there exists \((h,s,a,b)\) such that the visiting count \(N_{h}(s,a,b)\) is doubled. This update scheme ensures that the batch number is small.

For the regret bound of Algorithm 4, we have the following theorem.

**Theorem 2**.: _For any \(p\in(0,1]\), letting \(\iota=\log(SABT/p)\), then with probability at least \(1-p\), the regret of Algorithm 4 satisfies_

\[\mathrm{Regret}(K)\leq O\left(\sqrt{H^{3}SABK\iota}+H^{3}S^{2}AB\iota^{2} \right).\]

_and the batch number is at most \(O(HSAB\log K)\)._

The proof of Theorem 4 is similar to Theorem 4 in Liu et al. (2021), but the main difference is we have to deal with the sum of the Bernstein-type bonus under the doubling framework. To deal with this we borrow ideas from Zhang et al. (2021). The whole proof is in Appendix D.

Adapting the algorithm to the delayed feedback setting using our framework, we have the following corollary.

**Corollary 1**.: _For tabular Markov game with delayed feedback, running Algorithm 2 with \(\mathcal{ALG}\) being Algorithm 4, we can upper bound the regret by_

\[\mathrm{Regret}(K)\leq O\left(\frac{1}{q}\left(\sqrt{H^{3}SABK\iota}+H^{3}S^{2 }AB\iota^{2}\right)+H^{2}SABd_{\tau}(q)\log K\right)\]

_for any \(q\in(0,1)\). In addition, if the delays satisfy Assumption 2, the regret can be upper bounded by_

\[\mathrm{Regret}(K)\leq O\big{(}\sqrt{H^{3}SABK\iota}+H^{3}S^{2}AB\iota^{2}+H^ {2}SAB\mathbb{E}[\tau]\log K\big{)}.\]

### Linear Zero-Sum Markov Game

We consider the linear zero-sum Markov game setting, where the reward and transition have a linear structure.

**Assumption 3**.: _For each \((x,a,b)\in\mathcal{S}\times\mathcal{A}\times\mathcal{B}\) and \(h\in[H]\), we have_

\[r_{h}(x,a,b)=\phi(x,a,b)^{\top}\theta_{h},\quad\mathbb{P}_{h}(\cdot\mid x,a,b)= \phi(x,a,b)^{\top}\mu_{h}(\cdot),\]

_where \(\phi:\mathcal{S}\times\mathcal{A}\times\mathcal{A}\to\mathbb{R}^{d}\) is a known feature map, \(\theta_{h}\in\mathbb{R}^{d}\) is an unknown vector and \(\mu_{h}=(\mu_{h}^{(i)})_{i\in[d]}\) is a vector of d unknown (signed) measures on \(\mathcal{S}\). We assume that \(\|\phi(\cdot,\cdot,\cdot)\|\leq 1,\|\theta_{h}\|\leq\sqrt{d}\) and \(\|\mu_{h}(\mathcal{S})\|\leq\sqrt{d}\) for all \(h\in[H]\), where \(\|\cdot\|\) is the vector \(\ell_{2}\) norm._

The formal algorithm is in Algorithm 5 in Appendix D.2. It is also divided into two parts. The estimation step is similar to that in Xie et al. (2020), where we calculate an optimistic estimation \(\bar{Q}_{h}^{k}(s,a,b)\) and a pessimistic estimation \(\underline{Q}_{h}^{k}(s,a,b)\) of the underlying \(Q_{h}^{\star}(s,a,b)\) by least square value iteration. And we compute the CCE policy \(\pi\) of the estimated value functions.

In the policy update step, we update the policy to the CCE policy w.r.t. the current estimated value function. Note that instead of updating the policy in every episode, we only update it when there exists \(h\in[H]\) such that \(\det(\Lambda_{h}^{k})>\eta\cdot\det(\Lambda_{h})\). This condition implies that we have collected enough information at one direction. This technique is similar to Gao et al. (2021); Wang et al. (2021).

For the regret bound of Algorithm 5, we have the following theorem.

**Theorem 3**.: _In Algorithm 5, let \(\eta=2\). Then we have_

\[\mathrm{Regret}(K)\leq\tilde{O}(\sqrt{d^{3}H^{4}K}).\]

_And the batch number of Algorithm 5 is \(\frac{dH}{\log 2}\log\left(1+\frac{K}{\lambda}\right)\)._

Proof.: See Appendix D.2 for a detailed proof. 

Adapting the algorithm to the delayed feedback setting using our framework, we have the following corollary.

**Corollary 2**.: _For linear Markov game with delayed feedback, running Algorithm 2 with \(\mathcal{ALG}\) being Algorithm 5, we can upper bound the regret by_

\[\mathrm{Regret}(K)\leq\tilde{O}\left(\frac{1}{q}\sqrt{d^{3}H^{4}K}+dH^{2}d_{ \tau}(q)\log K\right)\]

_for any \(q\in(0,1)\). In addition, if the delays satisfy Assumption 2, the regret can be upper bounded by_

\[\mathrm{Regret}(K)\leq\tilde{O}\left(\sqrt{d^{3}H^{4}K}+dH^{2}\mathbb{E}[\tau ]\log K\right).\]

### Tabular General-Sum Markov Game

In this section, we introduce the multi-batched version of V-learning (Jin et al., 2021; Mao and Basar, 2022; Song et al., 2021) for general-sum Markov games. Our goal is to minimize the regret (2). The algorithm is formally described in Algorithm 6 in Appendix D.

The multi-batched V-learning algorithm maintains a value estimator \(V_{h}(s)\), a counter \(N_{h}(s)\), and a policy \(\pi_{h}(\cdot\mid s)\) for each \(s\) and \(h\). We also maintain \(S\times H\) different adversarial bandit algorithms. At step \(h\) in episode \(k\), the algorithm is divided into three steps: policy execution, \(V\)-value update, and policy update. In policy execution step, the algorithm takes action \(a_{h}\) according to \(\pi_{h}^{\bar{k}}\), and observes reward \(r_{h}\) and the next state \(s_{h+1}\), and updates the counter \(N_{h}(s_{h})\). Note that \(\pi_{h}^{\bar{k}}\) is updated only when the visiting count of some state \(N_{h}(s)\) doubles. Therefore it ensures a low batch number.

In the \(V\)-value update step, we update the estimated value function by

\[\tilde{V}_{h}\left(s_{h}\right)=\left(1-\alpha_{t}\right)\tilde{V}_{h}\left(s _{h}\right)+\alpha_{t}\left(r_{h}+V_{h+1}\left(s_{h+1}\right)+\beta(t)\right),\]where the learning rate is defined as

\[\alpha_{t}=\frac{H+1}{H+t},\quad\alpha_{t}^{0}=\prod_{j=1}^{t}\left(1-\alpha_{j} \right),\quad\alpha_{t}^{i}=\alpha_{i}\prod_{j=i+1}^{t}\left(1-\alpha_{j}\right).\]

and \(\beta(t)\) is the bonus to promote optimism.

In the policy update step, the algorithm feeds the action \(a_{h}\) and its "loss" \(\frac{H-r_{h}+V_{h+1}(s_{h+1})}{H}\) to the \((s_{h},h)^{\text{th}}\) adversarial bandit algorithm. Then it receives the updated policy \(\pi_{h}(\cdot\mid s_{h})\).

For two-player general sum Markov games, we let the two players run Algorithm 6 independently. Each player uses her own set of bonus that depends on the number of her actions. If we choose the adversarial bandit algorithm as Follow The Regularized Leader (FTRL) algorithm (Lattimore and Szepesvari, 2020), we have the following theorem for the regret of multi-batched V-learning.

**Theorem 4**.: _Suppose we choose the adversarial bandit algorithm as FTRL. For any \(\delta\in(0,1)\) and \(K\in\mathbb{N}\), let \(\iota=\log(HSAK/\delta)\). Choose learning rate \(\alpha_{t}\) and bonus \(\{\beta(t)\}_{t=1}^{K}\) as \(\beta(t)=c\cdot\sqrt{H^{3}A\iota/t}\) so that \(\sum_{i=1}^{t}\alpha_{t}^{i}\beta(i)=\Theta(\sqrt{H^{3}A\iota/t})\) for any \(t\in[K]\), where \(A=\max_{i}A_{i}\) Then, with probability at least \(1-\delta\), after running Algorithm 6 for \(K\) episodes, we have_

\[\operatorname{Regret}(K)\leq O\left(\sqrt{H^{5}SA\iota}\right).\]

_And the batch number is \(O(nHS\log K)\), where \(n\) is the number of players._

Proof.: See Appendix D for details. 

Finally, adapting the algorithm to the delayed feedback setting using our framework, we have the following corollary.

**Corollary 3**.: _For tabular general-sum Markov game with delayed feedback, running Algorithm 2 with \(\mathcal{ALG}\) being Algorithm 6, we can upper bound the regret by_

\[\operatorname{Regret}(K)\leq O\left(\frac{1}{q}\sqrt{H^{5}SA\iota}+H^{2}nSd_{ \tau}(q)\log K\right)\]

_for any \(q\in(0,1)\). In addition, if the delays satisfy Assumption 2, the regret can be upper bounded by_

\[\operatorname{Regret}(K)\leq O\left(\sqrt{H^{5}SA\iota}+H^{2}nS\mathbb{E}[ \tau]\log K\right).\]

Proof.: This corollary is directly implied by Theorems 1 and 4. 

## 6 Conclusion and Future Work

In this paper we study delayed feedback in general multi-agent sequential decision making. We propose a novel reduction-based framework, which turns any multi-batched algorithm for sequential decision making with instantaneous feedback into a sample-efficient algorithm that can handle stochastic delays in sequential decision making. Based on this framework, we obtain a complete set of results for (multi-agent) sequential decision making with delayed feedback.

Our work has also shed light on future works. First, it remains unclear whether the result is tight for MDP and Markov game with delayed feedback. Second, it is possible that we can derive a multi-batched algorithm for tabular Markov game with a smaller batch number, since in tabular MDP, Zhang et al. (2022b) gives an algorithm with \(O(H+\log_{2}\log_{2}K)\) batch number which is independent of \(S\). It is not clear whether we can achieve similar results in MDP for Markov games. We believe a tighter result is possible in this setting with a carefully designed multi-batched algorithm.

## Acknowledgements

Liwei Wang is supported in part by NSF IIS 2110170, NSF DMS 2134106, NSF CCF 2212261, NSF IIS 2143493, NSF CCF 2019844, NSF IIS 2229881.

## References

* Abbasi-Yadkori et al. (2011) Abbasi-Yadkori, Y., Pal, D., and Szepesvari, C. (2011). Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24.
* Agarwal and Duchi (2011) Agarwal, A. and Duchi, J. C. (2011). Distributed delayed stochastic optimization. _Advances in neural information processing systems_, 24.
* Azar et al. (2017) Azar, M. G., Osband, I., and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272. PMLR.
* Bai and Jin (2020) Bai, Y. and Jin, C. (2020). Provable self-play algorithms for competitive reinforcement learning. In _International conference on machine learning_, pages 551-560. PMLR.
* Bai et al. (2020) Bai, Y., Jin, C., and Yu, T. (2020). Near-optimal reinforcement learning with self-play. _Advances in neural information processing systems_, 33:2159-2170.
* Bistritz et al. (2019) Bistritz, I., Zhou, Z., Chen, X., Bambos, N., and Blanchet, J. (2019). Online exp3 learning in adversarial bandits with delayed feedback. _Advances in neural information processing systems_, 32.
* Cesa-Bianchi et al. (2013) Cesa-Bianchi, N., Dekel, O., and Shamir, O. (2013). Online learning with switching costs and other adaptive adversaries. _Advances in Neural Information Processing Systems_, 26.
* Changuel et al. (2012) Changuel, N., Sayadi, B., and Kieffer, M. (2012). Online learning for qoe-based video streaming to mobile receivers. In _2012 IEEE Globecom Workshops_, pages 1319-1324. IEEE.
* Chen et al. (2020) Chen, B., Xu, M., Liu, Z., Li, L., and Zhao, D. (2020). Delay-aware multi-agent reinforcement learning for cooperative and competitive environments. _arXiv preprint arXiv:2005.05441_.
* Chen et al. (2021) Chen, Z., Zhou, D., and Gu, Q. (2021). Almost optimal algorithms for two-player markov games with linear function approximation. _arXiv preprint arXiv:2102.07404_.
* Cui and Du (2022a) Cui, Q. and Du, S. S. (2022a). Provably efficient offline multi-agent reinforcement learning via strategy-wise bonus. _arXiv preprint arXiv:2206.00159_.
* Cui and Du (2022b) Cui, Q. and Du, S. S. (2022b). When is offline two-player zero-sum markov game solvable? _arXiv preprint arXiv:2201.03522_.
* Daskalakis et al. (2022) Daskalakis, C., Golowich, N., and Zhang, K. (2022). The complexity of markov equilibrium in stochastic games. _arXiv preprint arXiv:2204.03991_.
* Dudik et al. (2011) Dudik, M., Hsu, D., Kale, S., Karampatziakis, N., Langford, J., Reyzin, L., and Zhang, T. (2011). Efficient optimal learning for contextual bandits. _arXiv preprint arXiv:1106.2369_.
* Gael et al. (2020) Gael, M. A., Vernade, C., Carpentier, A., and Valko, M. (2020). Stochastic bandits with arm-dependent delays. In _International Conference on Machine Learning_, pages 3348-3356. PMLR.
* Gao et al. (2021) Gao, M., Xie, T., Du, S. S., and Yang, L. F. (2021). A provably efficient algorithm for linear markov decision process with low switching cost. _arXiv preprint arXiv:2101.00494_.
* Gao et al. (2019) Gao, Z., Han, Y., Ren, Z., and Zhou, Z. (2019). Batched multi-armed bandits problem. _Advances in Neural Information Processing Systems_, 32.
* Gyorgy and Joulani (2021) Gyorgy, A. and Joulani, P. (2021). Adapting to delays and data in adversarial multi-armed bandits. In _International Conference on Machine Learning_, pages 3988-3997. PMLR.
* Howson et al. (2021) Howson, B., Pike-Burke, C., and Filippi, S. (2021). Delayed feedback in episodic reinforcement learning. _arXiv preprint arXiv:2111.07615_.
* Howson et al. (2022) Howson, B., Pike-Burke, C., and Filippi, S. (2022). Delayed feedback in generalised linear bandits revisited. _arXiv preprint arXiv:2207.10786_.
* Huang et al. (2021) Huang, B., Lee, J. D., Wang, Z., and Yang, Z. (2021). Towards general function approximation in zero-sum markov games. _arXiv preprint arXiv:2107.14702_.
* Huang et al. (2021)Huang, J., Dai, Y., and Huang, L. (2023). Banker online mirror descent: A universal approach for delayed online bandit learning. _arXiv preprint arXiv:2301.10500_.
* Ito et al. (2020) Ito, S., Hatano, D., Sumita, H., Takemura, K., Fukunaga, T., Kakimura, N., and Kawarabayashi, K.-I. (2020). Delay and cooperation in nonstochastic linear bandits. _Advances in Neural Information Processing Systems_, 33:4872-4883.
* Jin et al. (2018) Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is q-learning provably efficient? _Advances in neural information processing systems_, 31.
* Jin et al. (2021) Jin, C., Liu, Q., Wang, Y., and Yu, T. (2021). V-learning-a simple, efficient, decentralized algorithm for multiagent rl. _arXiv preprint arXiv:2110.14555_.
* Jin et al. (2022a) Jin, C., Liu, Q., and Yu, T. (2022a). The power of exploiter: Provable multi-agent rl in large state spaces. In _International Conference on Machine Learning_, pages 10251-10279. PMLR.
* Jin et al. (2022b) Jin, T., Lancewicki, T., Luo, H., Mansour, Y., and Rosenberg, A. (2022b). Near-optimal regret for adversarial mdp with delayed bandit feedback. _arXiv preprint arXiv:2201.13172_.
* Jin et al. (2022c) Jin, Y., Muthukumar, V., and Sidford, A. (2022c). The complexity of infinite-horizon general-sum stochastic games. _arXiv preprint arXiv:2204.04186_.
* Joulani et al. (2013) Joulani, P., Gyorgy, A., and Szepesvari, C. (2013). Online learning under delayed feedback. In _International Conference on Machine Learning_, pages 1453-1461. PMLR.
* Kong et al. (2021) Kong, D., Salakhutdinov, R., Wang, R., and Yang, L. F. (2021). Online sub-sampling for reinforcement learning with general function approximation. _arXiv preprint arXiv:2106.07203_.
* Lancewicki et al. (2022) Lancewicki, T., Rosenberg, A., and Mansour, Y. (2022). Learning adversarial markov decision processes with delayed feedback. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7281-7289.
* Lancewicki et al. (2021) Lancewicki, T., Segal, S., Koren, T., and Mansour, Y. (2021). Stochastic multi-armed bandits with unrestricted delay distributions. In _International Conference on Machine Learning_, pages 5969-5978. PMLR.
* Lattimore and Szepesvari (2020) Lattimore, T. and Szepesvari, C. (2020). _Bandit algorithms_. Cambridge University Press.
* Li et al. (2022) Li, G., Chi, Y., Wei, Y., and Chen, Y. (2022). Minimax-optimal multi-agent rl in markov games with a generative model. In _Advances in Neural Information Processing Systems_.
* Liu et al. (2021) Liu, Q., Yu, T., Bai, Y., and Jin, C. (2021). A sharp analysis of model-based reinforcement learning with self-play. In _International Conference on Machine Learning_, pages 7001-7010. PMLR.
* Mahmood et al. (2018) Mahmood, A. R., Korenkevych, D., Komer, B. J., and Bergstra, J. (2018). Setting up a reinforcement learning task with a real-world robot. In _2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 4635-4640. IEEE.
* Mao and Basar (2022) Mao, W. and Basar, T. (2022). Provably efficient reinforcement learning in decentralized general-sum markov games. _Dynamic Games and Applications_, pages 1-22.
* Nash (1951) Nash, J. (1951). Non-cooperative games. _The Annals of Mathematics_, 54(2):286-295.
* Perchet et al. (2016) Perchet, V., Rigollet, P., Chassang, S., and Snowberg, E. (2016). Batched bandit problems. _The Annals of Statistics_, pages 660-681.
* Qiao et al. (2022) Qiao, D., Yin, M., Min, M., and Wang, Y.-X. (2022). Sample-efficient reinforcement learning with loglog (t) switching cost. _arXiv preprint arXiv:2202.06385_.
* Quanrud and Khashabi (2015) Quanrud, K. and Khashabi, D. (2015). Online learning with adversarial delays. _Advances in neural information processing systems_, 28.
* Shapley (1953) Shapley, L. S. (1953). Stochastic games. _Proceedings of the national academy of sciences_, 39(10):1095-1100.

[MISSING_PAGE_POST]

Song, Z., Mei, S., and Bai, Y. (2021). When can we learn general-sum markov games with a large number of players sample-efficiently? _arXiv preprint arXiv:2110.04184_.
* Sutton and Barto (2018) Sutton, R. S. and Barto, A. G. (2018). _Reinforcement learning: An introduction_. MIT press.
* Thune et al. (2019) Thune, T. S., Cesa-Bianchi, N., and Seldin, Y. (2019). Nonstochastic multiarmed bandits with unrestricted delays. _Advances in Neural Information Processing Systems_, 32.
* Tian et al. (2021) Tian, Y., Wang, Y., Yu, T., and Sra, S. (2021). Online learning in unknown markov games. In _International conference on machine learning_, pages 10279-10288. PMLR.
* Vakili et al. (2023) Vakili, S., Ahmed, D., Bernacchia, A., and Pike-Burke, C. (2023). Delayed feedback in kernel bandits. _arXiv preprint arXiv:2302.00392_.
* Van Der Hoeven and Cesa-Bianchi (2022) Van Der Hoeven, D. and Cesa-Bianchi, N. (2022). Nonstochastic bandits and experts with arm-dependent delays. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Vernade et al. (2017) Vernade, C., Cappe, O., and Perchet, V. (2017). Stochastic bandit models for delayed conversions. _arXiv preprint arXiv:1706.09186_.
* Vernade et al. (2020) Vernade, C., Carpentier, A., Lattimore, T., Zappella, G., Ermis, B., and Brueckner, M. (2020). Linear bandits with stochastic delayed feedback. In _International Conference on Machine Learning_, pages 9712-9721. PMLR.
* Wainwright (2019) Wainwright, M. J. (2019). _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge University Press.
* Wang et al. (2021) Wang, T., Zhou, D., and Gu, Q. (2021). Provably efficient reinforcement learning with linear function approximation under adaptivity constraints. _Advances in Neural Information Processing Systems_, 34:13524-13536.
* Xie et al. (2020) Xie, Q., Chen, Y., Wang, Z., and Yang, Z. (2020). Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium. In _Conference on learning theory_, pages 3674-3682. PMLR.
* Xiong et al. (2022a) Xiong, W., Zhong, H., Shi, C., Shen, C., Wang, L., and Zhang, T. (2022a). Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent mdp and markov game. _arXiv preprint arXiv:2205.15512_.
* Xiong et al. (2022b) Xiong, W., Zhong, H., Shi, C., Shen, C., and Zhang, T. (2022b). A self-play posterior sampling algorithm for zero-sum Markov games. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 24496-24523. PMLR.
* Yan et al. (2022) Yan, Y., Li, G., Chen, Y., and Fan, J. (2022). Model-based reinforcement learning is minimax-optimal for offline zero-sum markov games. _arXiv preprint arXiv:2206.04044_.
* Zhang et al. (2020) Zhang, K., Kakade, S., Basar, T., and Yang, L. (2020). Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity. _Advances in Neural Information Processing Systems_, 33:1166-1178.
* Zhang et al. (2021a) Zhang, K., Yang, Z., and Basar, T. (2021a). Multi-agent reinforcement learning: A selective overview of theories and algorithms. _Handbook of Reinforcement Learning and Control_, pages 321-384.
* Zhang et al. (2022a) Zhang, Y., Zhang, R., Li, G., Gu, Y., and Li, N. (2022a). Multi-agent reinforcement learning with reward delays. _arXiv preprint arXiv:2212.01441_.
* Zhang et al. (2021b) Zhang, Z., Ji, X., and Du, S. (2021b). Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon. In _Conference on Learning Theory_, pages 4528-4531. PMLR.
* Zhang et al. (2022b) Zhang, Z., Jiang, Y., Zhou, Y., and Ji, X. (2022b). Near-optimal regret bounds for multi-batch reinforcement learning. _arXiv preprint arXiv:2210.08238_.
* Zhang et al. (2022c)Zhong, H., Xiong, W., Tan, J., Wang, L., Zhang, T., Wang, Z., and Yang, Z. (2022). Pessimistic minimax value iteration: Provably efficient equilibrium learning from offline datasets. _arXiv preprint arXiv:2202.07511_.
* Zhong et al. (2021) Zhong, H., Yang, Z., Wang, Z., and Jordan, M. I. (2021). Can reinforcement learning find stackelberg-nash equilibria in general-sum markov games with myopic followers? _arXiv preprint arXiv:2112.13521_.
* Zimmerl and Seldin (2020) Zimmerl, J. and Seldin, Y. (2020). An optimal algorithm for adversarial bandits with arbitrary delays. In _International Conference on Artificial Intelligence and Statistics_, pages 3285-3294. PMLR.

Proof of Theorem 1

We first prove (7). First, since the delays are i.i.d. subexponential random variables, by standard concentration inequality and uniform bound Wainwright (2019), we have the following lemma:

**Lemma 5**.: _Let \(\left\{\tau_{t}\right\}_{t=1}^{\infty}\) be i.i.d \((v,b)\) subexponential random variables with expectation \(\mathbb{E}[\tau]\). Then we have_

\[\mathbb{P}\left(\exists t\geq 1:\tau_{t}\geq\mathbb{E}[\tau]+C_{\tau}\right) \leq 1-\delta,\] (8)

_where \(C_{\tau}=\min\left\{\sqrt{2v^{2}\log\left(\frac{3t}{2\delta}\right)},2b\log \left(\frac{3t}{2\delta}\right)\right\}\)_

Define the good event \(G=\left\{\exists t\geq 1:\tau_{t}\geq\mathbb{E}[\tau]+C_{\tau}\right\}\). By Lemma 5, the good event happens with probability at least \(1-\delta\). Therefore, conditioned on the good event happens, in each batch of the algorithm, we only need to run \(\mathbb{E}[\tau]+C_{\tau}\) episodes and the algorithm can observe enough feedback to finish this batch.

The regret in a batch can be divided into two parts: the first part is the regret caused by running the algorithm in the undelayed environment; the second part is caused by running extra episodes to wait for delayed feedback. The first part can be simply bounded by \(\widetilde{\mathrm{Regret}}(K)\). For the second part, since each episode contributes at most \(H\) regret, the total regret can be bounded by

\[\mathrm{Regret}(K)\leq\widetilde{\mathrm{Regret}}(K)+HN_{b}\left(\mathbb{E} [\tau]+C_{\tau}\right).\] (9)

For the second inequality, we have the following lemma which reveals the relation between the number of observed feedback and the actual number of episodes using quantile function.

**Lemma 6**.: _Denote the number of observed feedback at time \(t\) as \(n_{t}\). For any quantile \(q\) and any \(t\), we have_

\[\Pr\left[n_{t+d_{\tau}(q)}<\frac{q}{2}t\right]\leq\exp\left(-\frac{q}{8}t \right).\] (10)

Proof.: Define \(\mathbb{I}\left\{\tau_{s}\leq d_{\tau}(q)\right\}\) to be an indicator that on time \(s\) that the delay is smaller than \(d_{\tau}(q)\). Note that \(\mathbb{E}\left[\mathbb{I}\left\{\tau_{s}\leq d_{\tau}(q)\right\}\right]\geq q\). Thus,

\[\Pr\left[n_{t+d_{\tau}(q)}<\frac{q}{2}t\right] \leq\Pr\left[\sum_{s=1}^{t}\mathbb{I}\left\{\tau_{s}\leq d_{\tau }(q)\right\}<\frac{q}{2}t\right]\] (11) \[\leq\exp\left(-\frac{1}{8}\sum_{s=1}^{t}\mathbb{E}\left[\mathbb{I }\left\{\tau_{s}\leq d_{\tau}(q)\right\}\right]\right)\leq\exp\left(-\frac{q} {8}t\right),\]

where the third inequality follows from the relative Chernoff bound, and the last inequality is since \(\sum_{s=1}^{t}\mathbb{E}\left[\mathbb{I}\left\{\tau_{s}\leq d_{\tau}(q) \right\}\right]\geq q\cdot t\). 

Lemma 6 means, if the length of the batch in the undelayed environment \(m>\frac{\log(K/\delta)}{q}\), then with probability at least \(1-\frac{\delta}{K}\), if we run the batch for \(\frac{m}{q}+d_{\tau}(q)\) episodes, we will observe at least \(m\) feedbacks, which is enough for the algorithm to finish this batch. On the other hand, if \(m\leq\frac{\log(K/\delta)}{q}\), then the regret of this batch can be trivially bounded by \(\frac{H\log(K/\delta)}{q}\). Hence the total regret can be bounded by

\[\mathrm{Regret}(K)\leq\mathrm{Regret}\left(\frac{K}{q}+d_{\tau}(q)N_{b} \right)\leq\frac{1}{q}\widetilde{\mathrm{Regret}}(K)+\frac{2HN_{b}\log(K/ \delta)}{q}+HN_{b}d_{\tau}(q),\] (12)

which finishes the proof.

Details of Algorithms for Linear Bandits

The phase elimination algorithm for linear bandits in Lattimore and Szepesvari (2020) is as follows. This algorithm is naturally a multi-batched algorithm, with each phase as one batch. Since the length of phase \(\ell\) is \(O(2^{\ell})\), the number of phases is \(O(\log K)\). Note that in each phase the policy is pre-calculated at the beginning of the phase, and does not rely on the data of current phase.

```
1:Input:\(\mathcal{A}\subset\mathbb{R}^{d}\), \(\beta>1\), \(\delta\)
2:Set \(\ell=1\) and let \(\mathcal{A}_{1}=\mathcal{A}\)
3:Let \(t_{\ell}=t\) be the current timestep and find \(G\)-optimal design \(\pi_{\ell}\in\mathcal{P}\left(\mathcal{A}_{\ell}\right)\) with \(\operatorname{Supp}\left(\pi_{\ell}\right)\leq d(d+1)/2\) that maximises \[\log\det V\left(\pi_{\ell}\right)\text{ subject to }\sum_{a\in\mathcal{A}_{\ell}}\pi_{\ell}(a)=1\]
4:Let \(\varepsilon_{\ell}=2^{-\ell}\) and \[T_{\ell}(a)=\left\lceil\frac{2d\pi_{\ell}(a)}{\varepsilon_{\ell}^{2}}\log \left(\frac{k\ell(\ell+1)}{\delta}\right)\right\rceil\text{ and }T_{\ell}=\sum_{a\in\mathcal{A}_{\ell}}T_{\ell}(a)\]
5:Choose each action \(a\in\mathcal{A}_{\ell}\) exactly \(T_{\ell}(a)\) times.
6:Calculate the empirical estimate: \[\hat{\theta}_{\ell}=V_{\ell}^{-1}\sum_{t=t_{\ell}}^{t_{\ell}+T_{\ell}}A_{t}X_ {t}\quad\text{ with }\quad V_{\ell}=\sum_{a\in\mathcal{A}_{\ell}}T_{\ell}(a)aa^{\top}\] using observable data in this phase.
7:Eliminating low reward arms: \[\mathcal{A}_{\ell+1}=\left\{a\in\mathcal{A}_{\ell}:\max_{b\in\mathcal{A}_{ \ell}}\left\langle\hat{\theta}_{\ell},b-a\right\rangle\leq 2\varepsilon_{\ell} \right\}.\]
8:\(\ell\leftarrow\ell+1\) and go to Line 2. ```

**Algorithm 3** Phase Elimination

**Theorem 7** (Theorem 22.1 in Lattimore and Szepesvari (2020)).: _With probability at least \(1-\delta\), the regret of Algorithm 5 satisfies_

\[\operatorname{Regret}(K)\leq C\sqrt{Kd\log\left(\frac{A\log(K)}{\delta}\right)},\] (13)

_where \(C>0\) is a universal constant. If \(\delta=O(1/K)\), then \(\mathbb{E}\left[\operatorname{Regret}(K)\right]\leq C\sqrt{Kd\log(AK)}\) for an appropriately chosen universal constant \(C>0\)._

For infinite action set \(\mathcal{A}=[-1,1]^{d}\), we use a covering technique: we find a \(\epsilon\)-covering of the action space, and run Phase Elimination on the covering set. The covering number of the set \([-1,1]^{d}\) is \(O(d\log(1/\epsilon))\). And the error caused by using \(\epsilon\)-covering will lead to a \(O(\epsilon K)\) increase in regret. Therefore, by setting \(\epsilon=\frac{1}{K}\), the regret of the whole algorithm is \(O(d\sqrt{K}\log K)\).

## Appendix C Results for Bandits and MDPs

We now apply Theorem 1 to different bandit and MDP problems.

In multi-armed bandit settings, using the BaSE algorithm Gao et al. (2019), we can get the following bound.

**Corollary 4**.: _For multi-armed bandit with delayed feedback, running Algorithm 2 with \(\mathcal{ALG}\) being the BaSE algorithm in Gao et al. (2019), we can upper bound the regret by_

\[\mathrm{Regret}(K)\leq O\left(\frac{A\log K}{q\min_{i\neq\star} \Delta_{i}}+\frac{\log^{2}K}{q}+d_{\tau}(q)\log K\right)\]

_for any \(q\in(0,1)\), where \(\Delta_{i}\) is the optimal gap of the \(i^{\text{th}}\) arm. In addition, if the delays satisfy Assumption 2, the regret can be upper bounded by_

\[\mathrm{Regret}(K)\leq O\left(\frac{A\log K}{\min_{i\neq\star} \Delta_{i}}+\mathbb{E}[\tau]\log K\right).\]

This result recovers the SOTA result in Lancewicki et al. (2021) up to a \(\log K\) factor in the delay-dependent term.

In linear bandit settings, using the Phase Elimination algorithm (Lattimore and Szepesvari, 2020), we can get the following bound.

**Corollary 5**.: _For linear bandit with delayed feedback, let \(d\) be the dimension of the feature space. When the arm set \(\mathcal{A}\) is finite, running Algorithm 2 with \(\mathcal{ALG}\) being the Phase Elimination algorithm in Lattimore and Szepesvari (2020), we can upper bound the regret by_

\[\mathrm{Regret}(K)\leq O\left(\frac{1}{q}\sqrt{dK\log A}+\frac{ \log^{2}K}{q}+d_{\tau}(q)\log K\right)\]

_for any \(q\in(0,1)\). In addition, if the delays satisfy Assumption 2, the regret can be upper bounded by_

\[\mathrm{Regret}(K)\leq O\left(\sqrt{dK\log A}+\mathbb{E}[\tau] \log K\right).\]

When the action set is infinitely large (for example, \(\mathcal{A}=[-1,1]^{d}\)), we can use techniques such as covering to obtain regret bounds whose main term scales as \(\tilde{O}(d\sqrt{K})\). Compared with the result in Howson et al. (2021) which scales as \(\tilde{O}(d\sqrt{K}+d^{3/2}\mathbb{E}[\tau])\), our bound is tighter in the delay-dependent term as we remove the \(d^{3/2}\) factor.

In tabular MDP settings, using the policy elimination algorithm Zhang et al. (2022b), we can get the following bound.

**Corollary 6**.: _For tabular MDP with delayed feedback, running Algorithm 2 with \(\mathcal{ALG}\) being the policy elimination algorithm in Zhang et al. (2022b), we can upper bound the regret by_

\[\mathrm{Regret}(K)\leq O\Big{(}\frac{1}{q}\sqrt{SAH^{3}K\iota}+ \frac{(H+\log\log K)\log K}{q}+(H+\log\log K)d_{\tau}(q)\Big{)}\]

_for any \(q\in(0,1)\), where \(\iota\) is some log factor. In addition, if the delays satisfy Assumption 2, the regret can be upper bounded by_

\[\mathrm{Regret}(K)\leq O\left(\sqrt{SAH^{3}K\iota}+H(H+\log\log K )\mathbb{E}[\tau]\right).\]

Compared with the previous result \(\sqrt{H^{3}SAK\iota}+H^{2}SA\mathbb{E}[\tau]\log K\) in Howson et al. (2022), our bound removes the \(SA\) factor in the delay-dependent term, therefore is tighter.

We can also apply our framework to linear MDP and MDP with general function approximation settings with delay. These settings have not been studied before, therefore our result is novel. In linear MDP settings, using the algorithm in Gao et al. (2021) we can get the following bound.

**Corollary 7**.: _For linear MDP with delayed feedback, let \(d\) be the dimension of the feature space. Running Algorithm 2 with \(\mathcal{ALG}\) being the algorithm in Gao et al. (2021), we can upper bound the regret by_

\[\mathrm{Regret}(K)\leq \tilde{O}\big{(}\frac{1}{q}\sqrt{d^{3}H^{4}K}+dH^{2}d_{\tau}(q) \log K\big{)}\]

_for any \(q\in(0,1)\). In addition, if the delays satisfy Assumption 2, the regret can be upper bounded by_

\[\mathrm{Regret}(K)\leq\tilde{O}\left(\sqrt{d^{3}H^{4}K}+dH^{2} \mathbb{E}[\tau]\log K\right).\]And in MDP with general function approximation, using the algorithm in Kong et al. (2021) we get the following bound.

**Corollary 8**.: _For MDP with general function approximation and delayed feedback, let \(\dim_{E}(\mathcal{F},\epsilon)\) be the \(\epsilon\)-eluder dimension of the function class \(\mathcal{F}\). Running Algorithm 2 with \(\mathcal{ALG}\) being the algorithm in Kong et al. (2021), we can upper bound the regret by_

\[\mathrm{Regret}(K)\leq\tilde{O}\left(\frac{1}{q}\sqrt{\dim_{E}^{2}(\mathcal{F },1/K)H^{4}K}+H^{2}\dim_{E}(\mathcal{F},1/K)d_{\tau}(q)\right)\]

_for any \(q\in(0,1)\). In addition, if the delays satisfy Assumption 2, the regret can be upper bounded by_

\[\mathrm{Regret}(K)\leq\tilde{O}\left(\sqrt{\dim_{E}^{2}(\mathcal{F},1/K)H^{4} K}+H^{2}\dim_{E}(\mathcal{F},1/K)\mathbb{E}[\tau]\right).\]

## Appendix D Proofs for Tabular Markov Games

### Multi-batched Optimistic Nash-VI

```
1:Initialize: for any \((s,a,b,h),\bar{Q}_{h}(s,a,b)\gets H\), \(\underline{Q}_{h}(s,a,b)\gets 0,\Delta\gets H,N_{h}(s,a,b)\gets 0\), trigger set \(\mathcal{L}\leftarrow\left\{2^{i-1}\mid 2^{i}\leq KH,i=1,2,\dots\right\}\)
2:for episode \(k=1,2,\cdots,K\)do
3:if there exists \((h,s,a,b)\) such that \(N_{h}^{k}(s,a,b)\in\mathcal{L}\)then
4:for step \(h=H,H-1,...,1\)do
5:for\((s,a,b)\in\mathcal{S}\times\mathcal{A}\times\mathcal{B}\)do
6:\(t\gets N_{h}^{k}(s,a,b)\)
7:if\(t>0\)then
8:\(\beta_{h}^{k}\leftarrow\mathrm{BONUS}\left(t,\widehat{\mathcal{V}}_{h}\left[ \left(\bar{V}_{h+1}+\underline{V}_{h+1}\right)/2\right](s,a,b)\right)\)
9:\(\gamma_{h}^{k}\leftarrow(c/H)\widehat{\mathbb{P}}_{h}\left(\bar{V}_{h+1}- \underline{V}_{h+1}\right)(s,a,b)\)
10:\(\bar{Q}_{h}^{k}(s,a,b)\leftarrow\min\left\{\left(r_{h}+\widehat{\mathbb{P}}_{h }\bar{V}_{h+1}\right)(s,a,b)+\gamma_{h}^{k}+\beta_{h}^{k},H\right\}\)
11:\(\underline{Q}_{h}^{k}(s,a,b)\leftarrow\max\left\{\left(r_{h}+\widehat{\mathbb{P }}_{h}\underline{V}_{h+1}\right)(s,a,b)-\gamma_{h}^{k}-\beta_{h}^{k},0\right\}\)
12:endif
13:endfor
14:for\(s\in\mathcal{S}\)do
15:\(\pi_{h}^{k}(\cdot,\cdot\mid s)\leftarrow\mathrm{CCE}\left(\bar{Q}_{h}(s, \cdot,\cdot),\underline{Q}_{h}(s,\cdot,\cdot)\right)\)
16:\(\bar{V}_{h}(s)\leftarrow\left(\mathbb{E}_{\pi_{h}}\bar{Q}_{h}\right)(s)\); \(\underline{V}_{h}(s)\leftarrow\left(\mathbb{E}_{\pi_{h}}\underline{Q}_{h} \right)(s)\)
17:endfor
18:endfor
19:endif
20:for step \(h=1,...,H\)do
21: take action \((a_{h},b_{h})\sim\pi_{h}^{k}\left(\cdot,\cdot\mid s_{h}\right)\), observe reward \(r_{h}\) and next state \(s_{h+1}\).
22: add 1 to \(N_{h}^{k}\left(s_{h},a_{h},b_{h}\right)\) and \(N_{h}^{k}\left(s_{h},a_{h},b_{h},s_{h+1}\right)\).
23:\(\widehat{\mathbb{P}}_{h}\left(\cdot\mid s_{h},a_{h},b_{h}\right)\gets N_{h} ^{k}\left(s_{h},a_{h},b_{h},\cdot\right)/N_{h}^{k}\left(s_{h},a_{h},b_{h}\right)\)
24:endfor
25:endfor ```

**Algorithm 4** Multi-batched Algorithm for Tabular Markov Game

In this part, we list some of the important lemmas for the tabular Markov game algorithm, most of which are proven in the previous literature Liu et al. (2021). These lemmas are very useful for proving the regret bound in our main theorem.

We denote \(V^{k},Q^{k},\pi^{k},\mu^{k}\) and \(\nu^{k}\) for values and policies at the beginning of the \(k\)-th episode. In particular, \(N_{h}^{k}(s,a,b)\) is the number we have visited the state-action tuple \((s,a,b)\) at the \(h\)-th step before the \(k\)-th episode. \(N_{h}^{k}\left(s,a,b,s^{\prime}\right)\) is defined by the same token. Using this notation, we can further define the empirical transition by \(\widehat{\mathbb{P}}_{h}^{k}\left(s^{\prime}\mid s,a,b\right):=N_{h}^{k}\left(s,a,b,s^{\prime}\right)/N_{h}^{k}(s,a,b)\). If \(N_{h}^{k}(s,a,b)=0\), we set \(\widehat{\mathbb{P}}_{h}^{k}\left(s^{\prime}\mid s,a,b\right)=1/S\).

We define the empirical variance operator

\[\widehat{\mathbb{V}}_{h}^{k}V(s,a,b):=\mathrm{Var}_{s^{\prime}\sim\mathbb{P}_{ h}^{k}\left(\cdot\mid s,a,b\right)}\,V\left(s^{\prime}\right)\] (14)

and true variance operator

\[\mathbb{V}_{h}^{k}V(s,a,b):=\mathrm{Var}_{s^{\prime}\sim\mathbb{P}_{h}^{k} \left(\cdot\mid s,a,b\right)}\,V\left(s^{\prime}\right).\] (15)

The bonus terms can be written as

\[b_{h}^{k}(s,a,b)=\beta_{h}^{k}(s,a,b)+\gamma_{h}^{k}(s,a,b)\] (16)

where

\[\beta_{h}^{k}(s,a,b):=C\left(\sqrt{\frac{\widehat{\mathbb{V}}_{h}^{k}\left[ \left(\bar{\mathbb{V}}_{h+1}^{k}+\underline{V}_{h+1}^{k}\right)/2\right](s,a, b)}{\max\left\{N_{h}^{k}(s,a,b),1\right\}}}+\frac{H^{2}S_{l}}{\max\left\{N_{h}^{k} (s,a,b),1\right\}}\right)\] (17)

and the lower-order term

\[\gamma_{h}^{k}(s,a,b):=\frac{C}{H}\widehat{\mathbb{P}}_{h}\left(\bar{V}_{h+1} ^{k}-\underline{V}_{h+1}^{k}\right)(s,a,b)\] (18)

for some absolute constant \(C>0\).

The following lemma is the standard concentration result.

**Lemma 8** (Lemma 21, Liu et al. (2021)).: _Let \(c_{1}\) be some large absolute constant. Define event \(E_{1}\) to be: for all \(h,s,a,b,s^{\prime}\) and \(k\in[K]\),_

\[\left\{\begin{array}{l}\left|\left[\left(\widehat{\mathbb{P}}_{h}^{k}- \mathbb{P}_{h}\right)V_{h+1}^{\star}\right](s,a,b)\right|\leq c_{1}\left(\sqrt {\frac{\bar{\mathbb{V}}_{h}^{k}V_{h+1}^{\star}(s,a,b)_{t}}{\max\left\{N_{h}^{k} (s,a,b),1\right\}}}+\frac{H_{t}}{\max\left\{N_{h}^{k}(s,a,b),1\right\}}\right) \\ \left|\left(\widehat{\mathbb{P}}_{h}^{k}-\mathbb{P}_{h}\right)(s^{\prime}\mid s,a,b)\right|\leq c_{1}\left(\sqrt{\frac{\min\left\{P_{h}\left(s^{\prime}|s,a,b \right),\widehat{\mathbb{P}}_{h}^{k}\left(s^{\prime}|s,a,b\right)\right\}_{t} }{\max\left\{N_{h}^{k}(s,a,b),1\right\}}}+\frac{t}{\max\left\{N_{h}^{k}(s,a,b ),1\right\}}\right)\\ \left\|\left(\widehat{\mathbb{P}}_{h}^{k}-\mathbb{P}_{h}\right)(\cdot\mid s,a,b)\right\|_{1}\leq c_{1}\sqrt{\frac{S_{l}}{\max\left\{N_{h}^{k}(s,a,b),1 \right\}}}.\end{array}\right.\] (19)

_We have \(\mathbb{P}\left(E_{1}\right)\geq 1-p\)._

The following lemma shows the upper and lower bounds are indeed the upper and lower bounds of the best responses.

**Lemma 9** (Lemma 22, Liu et al. (2021)).: _Suppose event \(E_{1}\) holds. Then for all \(h,s,a,b\) and \(k\in[K]\), we have_

\[\left\{\begin{array}{l}\bar{Q}_{h}^{k}(s,a,b)\geq Q_{h}^{\dagger,\nu^{k}}(s, a,b)\geq Q_{h}^{\mu^{k},\dagger}(s,a,b)\geq\underline{Q}_{h}^{k}(s,a,b),\\ \bar{V}_{h}^{k}(s)\geq V_{h}^{\dagger,\nu^{k}}(s)\geq V_{h}^{\mu^{k},\dagger}(s )\geq V_{h}^{k}(s).\end{array}\right.\] (20)

**Lemma 10** (Lemma 23, Liu et al. (2021)).: _Suppose event \(E_{1}\) holds. Then for all \(h,s,a,b\) and \(k\in[K]\), we have_

\[\left|\widehat{\mathbb{V}}_{h}^{k}\left[\left(\bar{V}_{h+1}^{k}+ \underline{V}_{h+1}^{k}\right)/2\right]-\mathbb{V}_{h}V_{h+1}^{\pi^{k}}\right| (s,a,b)\] (21) \[\leq 4H\mathbb{P}_{h}\left(\bar{V}_{h+1}^{k}-\underline{V}_{h+1}^{k} \right)(s,a,b)+O\left(1+\frac{H^{4}S_{l}}{\max\left\{N_{h}^{k}(s,a,b),1 \right\}}\right).\]Proof of Theorem 4.: Suppose event \(E_{1}\) happens. We define

\[\left\{\begin{array}{l}\Delta_{h}^{k}:=\left(\bar{V}_{h}^{k}-\underline{V}_{h}^{ k}\right)\left(s_{h}^{k}\right)\\ \zeta_{h}^{k}:=\Delta_{h}^{k}-\left(\bar{Q}_{h}^{k}-\underline{Q}_{h}^{k} \right)\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right)\\ \xi_{h}^{k}:=\mathbb{P}_{h}\left(\bar{V}_{h+1}^{k}-\underline{V}_{h+1}^{k} \right)\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right)-\Delta_{h+1}^{k}.\end{array}\right.\] (22)

Using standard decomposition techniques we have

\[\begin{split}\Delta_{h}^{k}&=\zeta_{h}^{k}+\left(\bar{Q}_{h }^{k}-\underline{Q}_{h}^{k}\right)\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right) \\ &\leq\zeta_{h}^{k}+2\beta_{h}^{k}+2\gamma_{h}^{k}+\widehat{ \mathbb{P}}_{h}^{k}\left(\bar{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\right) \left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right)\\ \leq&\zeta_{h}^{k}+2\beta_{h}^{k}+2\gamma_{h}^{k}+ \mathbb{P}_{h}\left(\bar{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\right)\left(s_{ h}^{k},a_{h}^{k},b_{h}^{k}\right)+c_{2}\left(\frac{\mathbb{P}_{h}\left(\bar{V}_{h+1}^ {k}-\underline{V}_{h+1}^{k}\right)\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right)} {H}+\frac{H^{2}S_{t}}{\max\left\{N_{h}^{k},1\right\}}\right)\\ \leq&\zeta_{h}^{k}+2\beta_{h}^{k}+\mathbb{P}_{h}\left(\bar{V}_{h +1}^{k}-\underline{V}_{h+1}^{k}\right)\left(s_{h}^{k},a_{h}^{k},b_{h}^{k} \right)+2c_{2}C\left(\frac{\mathbb{P}_{h}\left(\bar{V}_{h+1}^{k}-\underline{V }_{h+1}^{k}\right)\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right)}{H}+\frac{H^{2}S_ {t}}{\max\left\{N_{h}^{k},1\right\}}\right)\\ &\leq\zeta_{h}^{k}+\left(1+\frac{c_{3}}{H}\right)\mathbb{P}_{h} \left(\bar{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\right)\left(s_{h}^{k},a_{h}^{ k},b_{h}^{k}\right)\\ &\quad+4c_{2}C\left(\sqrt{\frac{\iota\widehat{V}_{h}^{k}\left[\left(\bar{V} _{h+1}^{k}+\underline{V}_{h+1}^{k}\right)/2\right]\left(s_{h}^{k},a_{h}^{k},b_ {h}^{k}\right)}{\max\left\{N_{h}^{k}\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right),1\right\}}}+\frac{H^{2}S_{t}}{\max\left\{N_{h}^{k}\left(s_{h}^{k},a_{h}^{k}, b_{h}^{k}\right),1\right\}}\right).\end{split}\] (23)

By Lemma 10,

\[\begin{split}&\sqrt{\frac{\iota\widehat{\mathcal{V}}_{h}^{k}\left[ \left(\bar{V}_{h+1}^{k}+\underline{V}_{h+1}^{k}\right)/2\right]\left(s,a,b \right)}{\max\left\{N_{h}^{k}(s,a,b),1\right\}}}\\ \leq&\left(\sqrt{\frac{\iota\overline{V}_{h}V_{h+1}^ {\pi^{k}}(s,a,b)+\iota}{\max\left\{N_{h}^{k}(s,a,b),1\right\}}}+\sqrt{\frac{H \iota\mathbb{P}_{h}\left(\bar{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\right) \left(s,a,b\right)}{\max\left\{N_{h}^{k}(s,a,b),1\right\}}}+\frac{H^{2}\sqrt {S}_{t}}{\max\left\{N_{h}^{k}(s,a,b),1\right\}}\right)\\ \leq& c_{4}\left(\sqrt{\frac{\iota\overline{V}_{h}V_{ h+1}^{\pi^{k}}(s,a,b)+\iota}{\max\left\{N_{h}^{k}(s,a,b),1\right\}}}+\frac{ \mathbb{P}_{h}\left(\bar{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\right)\left(s,a,b \right)}{H}+\frac{H^{2}\sqrt{S}_{t}}{\max\left\{N_{h}^{k}(s,a,b),1\right\}} \right),\end{split}\] (24)

where \(c_{4}\) is some absolute constant. Define \(c_{5}:=4c_{2}c_{4}C+c_{3}\) and \(\kappa:=1+c_{5}/H\). Now we have

\[\Delta_{h}^{k}\leq\kappa\Delta_{h+1}^{k}+\kappa\xi_{h}^{k}+\zeta_{h}^{k}+O \left(\sqrt{\frac{\iota\overline{V}_{h}V_{h+1}^{\pi^{k}}\left(s_{h}^{k},a_{h}^ {k},b_{h}^{k}\right)}{N_{h}^{k}\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right)}}+ \sqrt{\frac{\iota}{N_{h}^{k}\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right)}}+ \frac{H^{2}S_{t}}{N_{h}^{k}\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right)}\right) \right\}.\] (25)

Recursing this argument for \(h\in[H]\) and summing over \(k\),

\[\sum_{k=1}^{K}\Delta_{1}^{k}\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\left[\kappa^{h-1} \zeta_{h}^{k}+\kappa^{h}\xi_{h}^{k}+O\left(\sqrt{\frac{\iota\overline{V}_{h}V_{h+1} ^{\pi^{k}}\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right)}{\max\left\{N_{h}^{k},1 \right\}}}+\sqrt{\frac{\iota}{\max\left\{N_{h}^{k},1\right\}}}+\frac{H^{2}S_{t} }{\max\left\{N_{h}^{k},1\right\}}\right)\right].\] (26)It is easy to check that \(\{\zeta_{h}^{k}\}\) and \(\{\xi_{h}^{k}\}\) are martingales. By Azuma-Hoeffding inequality, with probability at least \(1-p\),

\[\left\{\begin{array}{l}\sum_{k=1}^{K}\sum_{h=1}^{H}\kappa^{h-1}\zeta_{h}^{k} \leq O(H\sqrt{HK\iota})\\ \sum_{k=1}^{K}\sum_{h=1}^{H}\kappa^{h}\xi_{h}^{k}\leq O(H\sqrt{HK\iota}).\end{array}\right.\] (27)

Now we deal with the rest terms. Note that under the doubling epoch update framework, despite those episodes in which an update is triggered, the number of visits of \((s,a)\) between the \(i\)-th update of \(\hat{P}_{s,a}\) and the \(i+1\)-th update of \(\hat{P}_{s,a}\) do not exceed \(2^{i-1}\).

We define \(i_{\max}=\max\left\{i\mid 2^{i-1}\leq KH\right\}=\left|\log_{2}(KH)\right|+1\). To facilitate the analysis, we first derive a general deterministic result. Let \(\mathcal{K}\) be the set of indexes of episodes in which no update is triggered. By the update rule we have \(\left|\mathcal{K}^{C}\right|\leq SA\left(\log_{2}(KH)+1\right)\). Let \(h_{0}(k)\) be the first time an update is triggered in the \(k\)-th episode if there is an update in this episode and otherwise \(H+1\). Define \(\mathcal{X}=\left\{(k,h)\mid k\in\mathcal{K}^{C},h_{0}(k)+1\leq h\leq H\right\}\).

Let \(w=\left\{w_{h}^{k}\geq 0\mid 1\leq h\leq H,1\leq k\leq K\right\}\) be a group of non-negative weights such that \(w_{h}^{k}\leq 1\) for any \((k,h)\in[H]\times[K]\) and \(w_{h}^{k}=0\) for any \((k,h)\in\mathcal{X}\). As in Zhang et al. (2021b), We can calculate

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\sqrt{\frac{w_{h}^{k}}{N_{h}^{k}\left( s_{h}^{k},a_{h}^{k}\right)}}\] (28) \[\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\sum_{s,a}\sum_{i=3}^{i_{\max}} \mathbb{I}\left[\left(s_{h}^{k},a_{h}^{k}\right)=(s,a),N_{h}^{k}(s,a)=2^{i-1} \right]\sqrt{\frac{w_{h}^{k}}{2^{i-1}}}+8SA\left(\log_{2}(KH)+4\right)\] \[=\sum_{s,a}\sum_{i=3}^{i_{\max}}\frac{1}{\sqrt{2^{i-1}}}\sum_{k= 1}^{K}\sum_{h=1}^{H}\mathbb{I}\left[\left(s_{h}^{k},a_{h}^{k}\right)=(s,a),N_{ h}^{k}(s,a)=2^{i-1}\right]\sqrt{w_{h}^{k}}+8SA\left(\log_{2}(KH)+4\right)\] \[\leq\sum_{s,a}\sum_{i=3}^{i_{\max}}\sqrt{\frac{\sum_{k=1}^{K} \sum_{h=1}^{H}\mathbb{I}\left[\left(s_{h}^{k},a_{h}^{k}\right)=(s,a),N_{h}^{k} (s,a)=2^{i-1}\right]}{2^{i-1}}}.\] \[\sqrt{\left(\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{I}\left[\left(s_ {h}^{k},a_{h}^{k}\right)=(s,a),N_{h}^{k}(s,a)=2^{i-1}\right]w_{h}^{k}\right)}+8 SA\left(\log_{2}(KH)+4\right)\] \[\leq\sqrt{SAi_{\max}\sum_{k=1}^{K}\sum_{h=1}^{H}w_{h}^{k}}+8SA \left(\log_{2}(KH)+4\right).\]

By plugging \(w_{h}^{k}=\mathbb{V}_{h}V_{h+1}^{\tau^{k}}\left(s_{h}^{k},a_{h}^{k},b_{h}^{k} \right)\mathbb{I}[(k,h)\notin\mathcal{X}]\) and \(w_{h}^{k}=\mathbb{I}[(k,h)\notin\mathcal{X}]\) we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\left[O\left(\sqrt{\frac{\iota^{ \forall}V_{h}V_{h+1}^{\tau^{k}}\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right)}{ \max\left\{N_{h}^{k},1\right\}}}+\sqrt{\frac{\iota}{\max\left\{N_{h}^{k},1 \right\}}}+\frac{H^{2}S_{\iota}}{\max\left\{N_{h}^{k},1\right\}}\right)\right]\] (29) \[\leq O\left(\sqrt{SAi_{\max}\iota\sum_{k=1}^{K}\sum_{h=1}^{H} \mathbb{V}_{h}V_{h+1}^{\tau^{k}}\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right) \mathbb{I}[(k,h)\notin\mathcal{X}]}+\sqrt{SAi_{\max}\sum_{k=1}^{K}\sum_{h=1}^{H }\mathbb{I}[(k,h)\notin\mathcal{X}]\iota}+S^{2}A_{\iota}\log_{2}(KH)\right)+ \left|\mathcal{H}\right|\] \[\leq O\left(\sqrt{SAi_{\max}\iota\sum_{k=1}^{K}\sum_{h=1}^{H} \mathbb{V}_{h}V_{h+1}^{\tau^{k}}\left(s_{h}^{k},a_{h}^{k},b_{h}^{k}\right) \mathbb{I}[(k,h)\notin\mathcal{X}]}+\sqrt{SAi_{\max}K\iota}+S^{2}A_{\iota} \log_{2}(KH)\right)+\left|\mathcal{K}^{C}\right|\]By the Law of total variation and standard martingale concentration (see Lemma C.5 in Jin et al. (2018) for a formal proof), with probability at least \(1-p\), we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{V}_{h}V_{h+1}^{\pi^{k}}\left(s_{h}^{k},a_{h}^ {k},b_{h}^{k}\right)\leq O\left(H^{2}K+H^{3}\iota\right).\] (30)

By Pigeon-hole argument,

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{\sqrt{\max\left\{N_{h}^{k},1 \right\}}}\leq\sum_{s,a,b,h:N_{h}^{K}(s,a,b)>0}\sum_{n=1}^{N_{h}^{K}(s,a,b)} \frac{1}{\sqrt{n}}+HSAB\leq O(\sqrt{H^{2}SABK}+HSAB)\] (31) \[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{\max\left\{N_{h}^{k},1\right\} }\leq\sum_{s,a,b,h:N_{h}^{K}(s,a,b)>0}\sum_{n=1}^{N_{h}^{K}(s,a,b)}\frac{1}{n} +HSAB\leq O(HSAB\iota).\] (32)

Putting all relations together, we obtain that

\[\mathrm{Regret}(K)=\sum_{k=1}^{K}\left(V_{1}^{\dagger},\nu^{k}-V_{1}^{\mu^{k},\dagger}\right)(s_{1})\leq O\left(\sqrt{H^{3}SABK\iota}+H^{3}S^{2}AB\iota^{2}\right)\] (33)

which completes the proof. 

### Multi-batched Algorithms for Linear Markov Games

```
1:Require: regularization parameter \(\lambda\)
2:Initialize:\(\Lambda_{h}=\Lambda_{h}^{0}=\lambda I_{d}\)
3:for episode \(k=1,2,\cdots,K\)do
4:\(\Lambda_{h}^{k}=\sum_{\tau=1}^{k-1}\phi(s_{h}^{\tau},a_{h}^{\tau},b_{h}^{\tau })\phi(s_{h}^{\tau},a_{h}^{\tau},b_{h}^{\tau})^{\top}+\lambda I_{d}\)
5:endfor
6:if\(\exists h\in[H],\det(\Lambda_{h}^{k})>\eta\cdot\det(\Lambda_{h})\)then
7:\(\bar{V}_{H+1}^{k}\gets 0\), \(V_{H+1}^{k}\gets 0\)
8:for step \(h=H,H-1,\cdots,1\)do
9:\(\bar{\Lambda}_{h}\leftarrow\Lambda_{h}^{k}\)
10:\(\bar{w}_{h}^{k}\leftarrow(\Lambda_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\phi(s_{h} ^{\tau},a_{h}^{\tau},b_{h}^{\tau})\cdot[r_{h}(s_{h}^{\tau},a_{h}^{\tau},b_{h}^ {\tau})+\bar{V}_{h+1}^{k}(s_{h+1}^{\tau})]\)
11:\(\underline{w}_{h}^{k}\leftarrow(\Lambda_{h}^{k})^{-1}\sum_{\tau=1}^{k-1}\phi(s_ {h}^{\tau},a_{h}^{\tau},b_{h}^{\tau})\cdot[r_{h}(s_{h}^{\tau},a_{h}^{\tau},b_{ h}^{\tau})+\underline{V}_{h+1}^{k}(s_{h+1}^{\tau})]\)
12:\(\Gamma_{h}^{k}(\cdot,\cdot,\cdot)\leftarrow\beta\sqrt{\phi(\cdot,\cdot,\cdot)^ {\top}(\Lambda_{h}^{k})^{-1}\phi(\cdot,\cdot,\cdot)}\)
13:\(\bar{Q}_{h}^{k}(\cdot,\cdot,\cdot)\leftarrow\Pi_{H}\{(\underline{w}_{h}^{k})^ {\top}\phi(\cdot,\cdot,\cdot)+\Gamma_{h}^{k}(\cdot,\cdot,\cdot)\}\)
14:\(\underline{Q}_{h}^{k}(\cdot,\cdot,\cdot)\leftarrow\Pi_{H}\{(\underline{w}_{h}^ {k})^{\top}\phi(\cdot,\cdot,\cdot)-\Gamma_{h}^{k}(\cdot,\cdot,\cdot)\}\)
15: For each \(s\), let \(\pi_{h}^{k}(x)\leftarrow\mathrm{FIND\_CCE}(\bar{Q}_{h}^{k},\underline{Q}_{h}^{k},s)\) (cf. Algorithm 2 in Xie et al. (2020))
16:\(\bar{V}_{h}^{k}(s)\leftarrow\mathbb{E}_{(a,b)\sim\pi_{h}^{k}(s)}\bar{Q}_{h}^{k }(s,a,b)\) for each \(s\)
17:\(\underline{V}_{h}^{k}(s)\leftarrow\mathbb{E}_{(a,b)\sim\pi_{h}^{k}(s)}\underline {Q}_{h}^{k}(s,a,b)\) for each \(s\)
18:endfor
19:else
20:\(\bar{Q}_{h}^{k}\leftarrow\bar{Q}_{h}^{k-1}\), \(\underline{Q}_{h}^{k}\leftarrow\underline{Q}_{h}^{k-1}\), \(\pi_{h}^{k}\leftarrow\pi_{h}^{k-1}\), \(\forall h\in[H]\)
21:endif ```

**Algorithm 5** Multi-batched Algorithm for Linear Markov Games

Proof of Theorem 3.: Let \(\{k_{1},\cdots,k_{N_{\mathrm{batch}}}\}\) be the episodes where the learner updates the policies. Moreover, we define \(b_{k}=\max\{k_{i}:i\in[N_{\mathrm{batch}}],k_{i}\leq k\}\). Moreover, we denote the marginals of \(\pi_{h}^{k}\) as \(\mu_{h}^{k}\) and \(\nu_{h}^{k}\).

Part 1: Upper bound of the batch number.By the updating rule, we know there exists one \(h\in[H]\) such that \(\det(\Lambda_{h}^{k_{i}})>\eta\cdot\det(\Lambda_{h}^{k_{i-1}})\), which further implies that

\[\prod_{h=1}^{H}\det(\Lambda_{h}^{k_{i}})>\eta\cdot\prod_{h=1}^{H}\det(\Lambda_{ h}^{k_{i-1}}).\]

This yields that

\[\prod_{h=1}^{H}\det(\Lambda_{h}^{k_{N_{\mathrm{batch}}}})>\eta^{N_{\mathrm{ batch}}}\cdot\prod_{h=1}^{H}\det(\Lambda_{h}^{0})=\eta^{N_{\mathrm{batch}}} \cdot\lambda^{dH}.\] (34)

On the other hand, for any \(h\in[H]\), we have

\[\prod_{h=1}^{H}\det(\Lambda_{h}^{k_{N_{\mathrm{batch}}}})\leq\prod_{h=1}^{H} \det(\Lambda_{h}^{K+1}).\] (35)

Furthermore, by the fact that \(\|\phi(\cdot,\cdot)\|_{2}\leq 1\), we have

\[\Lambda_{h}^{K+1}=\sum_{k=1}^{K}\phi(s_{h}^{k},a_{h}^{k},b_{h}^{k})\phi(s_{h}^ {k},a_{h}^{k},b_{h}^{k})^{\top}+\lambda I_{d}\preceq(K+\lambda)\cdot I_{d},\]

which implies that

\[\prod_{h=1}^{H}\det(\Lambda_{h}^{K+1})\leq(K+\lambda)^{dH}.\] (36)

Combining (34), (35), and (36), we obtain that

\[N_{\mathrm{batch}}\leq\frac{dH}{\log\eta}\log\Big{(}1+\frac{K}{\lambda}\Big{)},\]

which concludes the first part of the proof.

Part2: Regret.We need the following two lemmas to bound the regret.

**Lemma 11**.: _For any \((s,a,b,k,h)\), it holds that_

\[\underline{Q}_{h}^{k}(s,a,b)-2(H-h+1)\epsilon\leq Q_{h}^{\mu^{k},*}(s,a,b)\leq Q _{h}^{*,\nu^{k}}(s,a,b)\leq\bar{Q}_{h}(s,a,b)+2(H-h+1)\epsilon,\]

_and_

\[V_{h}^{k}(s)-2(H-h+2)\epsilon\leq V_{h}^{\mu^{k},*}(s)\leq V_{h}^{*,\nu^{k}}( s)\leq\bar{V}_{h}^{k}(s)+2(H-h+2)\epsilon.\]

Proof.: See Lemma 5 of Xie et al. (2020) for a detailed proof. 

**Lemma 12**.: _For all \((s,a,b,h,k)\) and any fixed policy pair \((\mu,\nu)\), it holds with probability at least \(1-\delta\) that_

\[\left|\left\langle\phi(s,a,b),\bar{w}_{h}^{b_{k}}\right\rangle-Q_ {h}^{\mu,\nu}(s,a,b)-\mathbb{P}_{h}\left(\bar{V}_{h+1}^{b_{k}}-V_{h+1}^{\mu, \nu}\right)(s,a,b)\right|\leq\Gamma_{h}^{b_{k}}(s,a,b),\] \[\left|\left\langle\phi(s,a,b),\underline{w}_{h}^{b_{k}}\right\rangle -Q_{h}^{\mu,\nu}(s,a,b)-\mathbb{P}_{h}\left(\underline{V}_{h+1}^{b_{k}}-V_{h+1 }^{\mu,\nu}\right)(s,a,b)\right|\leq\Gamma_{h}^{b_{k}}(s,a,b).\]

Proof.: See Lemma 3 of Xie et al. (2020) for a detailed proof. 

We define

\[\delta_{h}^{k} :=\bar{V}_{h}^{b_{k}}(s_{h}^{k})-\underline{V}_{h}^{b_{k}}(s_{h}^ {k}),\] \[\zeta_{h}^{k} :=\mathbb{E}[\delta_{h+1}^{k}\mid s_{h}^{k},a_{h}^{k},b_{h}^{k}]- \delta_{h+1}^{k},\] \[\bar{\chi}_{h}^{k} :=\mathbb{E}_{(a,b)\sim\pi_{h}^{k}(s_{h}^{k})}[\bar{Q}_{h}^{b_{k} }(s_{h}^{k},a,b)]-\bar{Q}_{h}^{b_{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k}),\] \[\underline{\gamma}_{h}^{k} :=\mathbb{E}_{(a,b)\sim\pi_{h}^{k}(s_{h}^{k})}[\bar{Q}_{h}^{b_{k} }(s_{h}^{k},a,b)]-\bar{Q}_{h}^{b_{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k}).\]By Lemma 11, we have

\[V_{1}^{*,\nu^{k}}(s_{1})-V_{1}^{\pi^{k},*} \leq\bar{V}_{1}^{k}(s_{1})-V_{1}^{k}(s_{1})+8K\epsilon\] \[\leq\bar{V}_{1}^{k}(s_{1})-\underline{V}_{1}^{k}(s_{1})+\frac{8}{K}\] \[=\bar{V}_{1}^{b_{k}}(s_{1})-\underline{V}_{1}^{b_{k}}(s_{1})+\frac {8}{K}\] \[=\delta_{1}^{k}+\frac{8}{K},\] (37)

where the second inequality uses that \(\epsilon=1/(KH)\). By definition, we have

\[\delta_{h}^{k} =\bar{V}_{h}^{b_{k}}(s_{h}^{k})-\overline{V}_{h}^{b_{k}}(s_{h}^{k})\] \[=\mathbb{E}_{(a,b)\sim\pi_{h}^{k}(s_{h}^{k})}[\bar{Q}_{h}^{k}(s_{h }^{k},a,b)]-\mathbb{E}_{(a,b)\sim\pi_{h}^{k}(s_{h}^{k})}[\underline{Q}_{h}^{b_ {k}}(s_{h}^{k},a,b)]\] \[=\bar{Q}_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h}^{k})-\underline{Q}_{h }^{b_{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k})+\bar{\gamma}_{h}^{k}-\underline{ \gamma}_{h}^{k}.\] (38)

Meanwhile, for any \((s,a,b,k,h)\), we have

\[\bar{Q}_{h}^{b_{k}}(s,a,b)-\underline{Q}_{h}^{b_{k}}(s,a,b) =[(\bar{w}_{h}^{b_{k}})^{\top}\phi(s,a,b)+\Gamma_{h}^{b_{k}}(s,a, b)]-[(\underline{w}_{h}^{b_{k}})^{\top}\phi(s,a,b)-\Gamma_{h}^{b_{k}}(s,a,b)]\] \[=(\bar{w}_{h}^{b_{k}}-\underline{w}_{h}^{b_{k}})^{\top}\phi(s,a,b )+2\Gamma_{h}^{b_{k}}(s,a,b).\]

By Lemma 12, we further have

\[\bar{Q}_{h}^{b_{k}}(s,a,b)-\underline{Q}_{h}^{b_{k}}(s,a,b)\leq \mathbb{P}_{h}(\bar{V}_{h+1}^{b_{k}}-\underline{V}_{h+1}^{b_{k}})(s,a,b)+4 \Gamma_{h}^{b_{k}}(s,a,b).\] (39)

Plugging (39) into (38) gives that

\[\delta_{h}^{k} \leq\mathbb{P}_{h}(\bar{V}_{h+1}^{b_{k}}-\underline{V}_{h+1}^{b_ {k}})(s,a,b)+4\Gamma_{h}^{b_{k}}(s,a,b)+\bar{\gamma}_{h}^{k}-\underline{\gamma }_{h}^{k}\] \[=\delta_{h+1}^{k}+\zeta_{h}^{k}+\bar{\gamma}_{h}^{k}-\underline{ \gamma}_{h}^{k}+4\Gamma_{h}^{b_{k}}(s,a,b).\]

Hence, we have

\[\sum_{k=1}^{K}\delta_{1}^{k} \leq\sum_{k=1}^{K}\sum_{h=1}^{H}(\zeta_{h}^{k}+\bar{\gamma}_{h}^ {k}-\underline{\gamma}_{h}^{k})+\sum_{k=1}^{K}\sum_{h=1}^{H}\Gamma_{h}^{b_{k}} (s_{h}^{k},a_{h}^{k},b_{h}^{k})\] \[\leq\tilde{O}(H\cdot\sqrt{HK})+\sum_{k=1}^{K}\sum_{h=1}^{H}\Gamma _{h}^{b_{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k}),\] (40)

where the last inequality uses the Azuma-Hoeffding inequality. Note that

\[\frac{\Gamma_{h}^{b_{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k})}{\Gamma_{h}^{k}(s_{h}^ {k},a_{h}^{k},b_{h}^{k})}\leq\sqrt{\frac{\det(\Lambda_{h}^{k})}{\det(\Lambda_{ h}^{b_{k}})}}\leq\sqrt{\eta},\]

which implies that

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\Gamma_{h}^{b_{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k}) \leq\sqrt{\eta}\sum_{k=1}^{K}\sum_{h=1}^{H}\Gamma_{h}^{k}(s_{h}^{k},a_{h}^{k},b _{h}^{k})\leq\tilde{O}(\sqrt{\eta d^{3}H^{4}K}),\] (41)

where the last inequality uses the elliptical potential lemma (Abbasi-Yadkori et al., 2011). Combining (37), (40), and (41), together with the fact that \(\eta=2\), we obtain

\[\mathrm{Regret}(K)\leq\tilde{O}(\sqrt{d^{3}H^{4}K}),\]

which concludes the proof.

### Multi-batched V-learning

In this section, we introduce the multi-batched version of V-learning (Jin et al., 2021) for general-sum Markov games. Our goal is to minimize the following notion of regret. Let \(V_{i,1}^{\pi}\left(s_{1}\right)\) be the expected cumulative reward that the \(i^{\text{th}}\) player will receive if the game starts at initial state \(s_{1}\) at the \(1^{\text{st}}\) step and all players follow joint policy \(\pi\). Let \(\hat{\pi}^{k}\) be the policy executed at the \(k^{\text{th}}\) episode. The regret is defined as

\[\mathrm{Regret}(K)=\sum_{k=1}^{K}\max_{j}\left(V_{j,h}^{\dagger,\hat{\pi}^{k} _{-j,h}}-V_{j,h}^{\hat{\pi}^{k}_{h}}\right)(s_{1}),\] (42)

where \(V_{j,h}^{\dagger,\hat{\pi}^{k}_{-j,h}}\) is the best response of the \(j^{\text{th}}\) player when other players follow \(\hat{\pi}\).

```
1:Initialize:\(V_{h}(s)\gets H+1-h,N_{h}(s)\gets 0,\pi_{h}(a\mid s)\gets 1/A\), trigger set \(\mathcal{L}\leftarrow\left\{2^{i-1}\mid 2^{i}\leq KH,i=1,2,\dots\right\}\)
2:for episode \(k=1,...,K\)do
3: Receive \(s_{1}\).
4:for\(h=1,...,H\)do
5: take action \(a_{h}\sim\pi_{h}^{\tilde{k}}\left(\cdot\mid s_{h}\right)\), observe \(r_{h}\) and \(s_{h+1}\).
6:\(t=N_{h}\left(s_{h}\right)\gets N_{h}\left(s_{h}\right)+1\).
7:\(\tilde{V}_{h}\left(s_{h}\right)\leftarrow\left(1-\alpha_{t}\right)\tilde{V}_{h} \left(s_{h}\right)+\alpha_{t}\left(r_{h}+V_{h+1}\left(s_{h+1}\right)+\beta(t)\right)\)
8:\(V_{h}\left(s_{h}\right)\leftarrow\min\left\{H+1-h,\tilde{V}_{h}\left(s_{h} \right)\right\}\)
9:\(\pi_{h}^{k}\left(\cdot\mid s_{h}\right)\leftarrow\text{ADV\_BANDIT\_UPDATE}\left(a_{h},\frac{H-r_{h}-V_{h+1} \left(s_{h+1}\right)}{H}\right)\) on \(\left(s_{h},h\right)^{\text{th}}\) adversarial bandit.
10:endfor
11:if\(\exists s,N_{h}(s)\in\mathcal{L}\)then
12:\(\tilde{k}\gets k\)
13:endif
14:endfor ```

**Algorithm 6** Multi-batched V-learning

The multi-batched V-learning algorithm maintains a value estimator \(V_{h}(s)\), a counter \(N_{h}(s)\), and a policy \(\pi_{h}(\cdot\mid s)\) for each \(s\) and \(h\). We also maintain \(S\times H\) different adversarial bandit algorithms. At step \(h\) in episode \(k\), the algorithm is divided into three steps: policy execution, \(V\)-value update, and policy update. In policy execution step, the algorithm takes action \(a_{h}\) according to \(\pi_{h}^{\tilde{k}}\), and observes reward \(r_{h}\) and the next state \(s_{h+1}\), and updates the counter \(N_{h}\left(s_{h}\right)\). Note that \(\pi_{h}^{k}\) is updated only when the visiting count of some state \(N_{h}(s)\) doubles. Therefore it ensures a low batch number.

In the \(V\)-value update step, we update the estimated value function by

\[\tilde{V}_{h}\left(s_{h}\right)=\left(1-\alpha_{t}\right)\tilde{V}_{h}\left(s _{h}\right)+\alpha_{t}\left(r_{h}+V_{h+1}\left(s_{h+1}\right)+\beta(t)\right),\]

where the learning rate is defined as

\[\alpha_{t}=\frac{H+1}{H+t},\quad\alpha_{t}^{0}=\prod_{j=1}^{t}\left(1-\alpha_{ j}\right),\quad\alpha_{t}^{i}=\alpha_{i}\prod_{j=i+1}^{t}\left(1-\alpha_{j} \right).\]

and \(\beta(t)\) is the bonus to promote optimism.

In the policy update step, the algorithm feeds the action \(a_{h}\) and its "loss" \(\frac{H-r_{h}+V_{h+1}\left(s_{h+1}\right)}{H}\) to the \(\left(s_{h},h\right)^{\text{th}}\) adversarial bandit algorithm. Then it receives the updated policy \(\pi_{h}\left(\cdot\mid s_{h}\right)\).

The ADV_BANDIT_UPDATE subroutineConsider a multi-armed bandit problem with adversarial loss, where we denote the action set by \(\mathcal{B}\) with \(|\mathcal{B}|=B\). At round \(t\), the learner determines a policy \(\theta_{t}\in\Delta_{\mathcal{B}}\), and the adversary chooses a loss vector \(\ell_{t}\in[0,1]^{B}\). Then the learner takesan action \(b_{t}\) according to policy \(\theta_{t}\), and receives a noisy bandit feedback \(\tilde{\ell}_{t}\left(b_{t}\right)\in[0,1]\), where \(\mathbb{E}\left[\tilde{\ell}_{t}\left(b_{t}\right)\mid\ell_{t},b_{t}\right]= \ell_{t}\left(b_{t}\right)\). Then, the adversarial bandit algorithm performs updates based on \(b_{t}\) and \(\tilde{\ell}_{t}\left(b_{t}\right)\), and outputs the policy for the next round \(\theta_{t+1}\).

We first state our requirement for the adversarial bandit algorithm used in V-learning, which is to have a high probability weighted external regret guarantee as follows.

**Assumption 4**.: _For any \(t\in\mathbb{N}\) and any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have_

\[\max_{\theta\in\Delta_{\mathcal{B}}}\sum_{i=1}^{t}\alpha_{t}^{i} \left[\left\langle\theta_{i},\ell_{i}\right\rangle-\left\langle\theta,\ell_{i }\right\rangle\right]\leq\xi(B,t,\log(1/\delta)).\] (43)

_We further assume the existence of an upper bound \(\Xi(B,t,\log(1/\delta))\geq\sum_{t^{\prime}=1}^{t}\xi\left(B,t^{\prime},\log( 1/\delta)\right)\) where (i) \(\xi(B,t,\log(1/\delta))\) is non-decreasing in \(B\) for any \(t,\delta\); (ii) \(\Xi(B,t,\log(1/\delta))\) is concave in \(t\) for any \(B,\delta\)._

As is shown in Jin et al. (2021), the Follow-the-Regularized-Leader (FTRL) algorithm satisfies the assumption with bounds \(\xi(B,t,\log(1/\delta))\leq O(\sqrt{HB\log(B/\delta)/t})\) and \(\Xi(B,t,\log(1/\delta))\leq O(\sqrt{HBt\log(B/\delta)})\). And we will use FTRL as the subroutine in our algorithm.

For two-player general sum Markov games, we let the two players run Algorithm 6 independently. Each player will use her own set of bonus that depends on the number of her actions. We have the following theorem for the regret of multi-batched V-learning.

**Theorem 13**.: _Suppose we choose subroutine ADV_BANDIT_UPDATE as FTRL. For any \(\delta\in(0,1)\) and \(K\in\mathbb{N}\), let \(\iota=\log(HSAK/\delta)\). Choose learning rate \(\alpha_{t}\) and bonus \(\left\{\beta(t)\right\}_{t=1}^{K}\) as \(\beta(t)=c\cdot\sqrt{H^{3}A\iota/t}\) so that \(\sum_{i=1}^{t}\alpha_{t}^{i}\beta(i)=\Theta\left(\sqrt{H^{3}A\iota/t}\right)\) for any \(t\in[K]\), where \(A=\max_{i}A_{i}\) Then, with probability at least \(1-\delta\), after running Algorithm for \(K\) episodes, we have_

\[\mathrm{Regret}(K)\leq O\left(\sqrt{H^{5}SA\iota}\right).\] (44)

_And the batch number is \(O(HS\log K)\)._

Proof.: The proof mainly follows Jin et al. (2021). Below we list some useful lemmas, which have already been proved in Jin et al. (2021).

**Lemma 14** (Lemma 10, Jin et al. (2021)).: _The following properties hold for \(\alpha_{t}^{i}\) :_

1. \(\frac{1}{\sqrt{t}}\leq\sum_{i=1}^{t}\frac{\alpha_{t}^{i}}{\sqrt{i}}\leq\frac{2 }{\sqrt{t}}\) _and_ \(\frac{1}{t}\leq\sum_{i=1}^{t}\frac{\alpha_{t}^{i}}{i}\leq\frac{2}{t}\) _for every_ \(t\geq 1\)_._
2. \(\max_{i\in[t]}\alpha_{t}^{i}\leq\frac{2H}{t}\) _and_ \(\sum_{i=1}^{t}\left(\alpha_{t}^{i}\right)^{2}\leq\frac{2H}{t}\) _for every_ \(t\geq 1\)_._
3. \(\sum_{t=1}^{\infty}\alpha_{t}^{i}=1+\frac{1}{H}\) _for every_ \(i\geq 1\)_._

The next lemma shows that \(V_{h}^{k}\) is an optimistic estimation of \(V_{j,h}^{\dagger,\hat{\pi}_{-j,h}^{k}}\)

**Lemma 15** (Lemma 13, Jin et al. (2021)).: _For any \(\delta\in(0,1]\), with probability at least \(1-\delta\), for any \((s,h,k,j)\in\mathcal{S}\times[H]\times[K]\times[m]\), \(V_{j,h}^{k}(s)\geq V_{h}^{\dagger,v^{k}}(s)\)_

And we define the pessimistic pessimistic V-estimations \(\underline{V}\) that are defined similarly as \(\tilde{V}\) and \(V\). Formally, let \(t=N_{h}^{k}(s)\), and suppose \(s\) was previously visited at episodes \(k^{1},\ldots,k^{t}<k\) at the \(h\)-th step. Then

\[\underline{V}_{\!\!-\!j,h}^{k}(s)=\max\left\{0,\sum_{i=1}^{t}\alpha_{t}^{i} \left[r_{j,h}\left(s,\boldsymbol{a}_{h}^{k^{i}}\right)+\underline{V}_{\!\!-\!j,h+1}^{k^{i}}\left(s_{h+1}^{k^{i}}\right)-\beta(i)\right]\right\}.\] (45)

The next lemma shows \(\underline{V}_{\!\!-\!j,h}^{k}(s)\) is a lower bound of \(V_{j,h}^{\hat{\pi}_{-j,h}^{k}}(s)\).

**Lemma 16** (Lemma 14, Jin et al. (2021)).: _For any \(\delta\in(0,1]\), with probability at least \(1-\delta\), the following holds for any \((s,h,k,j)\in\mathcal{S}\times[H]\times[K]\times[m]\) and any player \(j,\underline{V}_{j,h}^{k}(s)\leq V_{j,h}^{\hat{\pi}_{h}^{k}}(s)\)._

It remains to bound the gap \(\sum_{k=1}^{K}\max_{j}\left(V_{1,j}^{k}-\underline{V}_{1,j}^{k}\right)(s_{1})\). For each player \(j\) we define \(\delta_{j,h}^{k}:=V_{j,h}^{k}\left(s_{h}^{k}\right)-\underline{V}_{j,h}^{k} \left(s_{h}^{k}\right)\geq 0\). The non-negativity here is a simple consequence of the update rule and induction. We need to bound \(\delta_{h}^{k}:=\max_{j}\delta_{j,h}^{k}\). Let \(n_{h}^{k}=N_{h}^{k}\left(s_{h}^{k}\right)\) and suppose \(s_{h}^{k}\) was previously visited at episodes \(k^{1},\ldots,k^{n_{h}^{k}}<k\) at the \(h\)-th step. Now by the update rule,

\[\begin{split}\delta_{j,h}^{k}&=V_{j,h}^{k}\left(s_ {h}^{k}\right)-\underline{V}_{j,h}^{k}\left(s_{h}^{k}\right)\\ &\leq\alpha_{n_{h}^{k}}^{0}H+\sum_{i=1}^{n_{h}^{k}}\alpha_{n_{h} ^{k}}^{i}\left[\left(V_{j,h+1}^{k^{i}}-\underline{V}_{j,h+1}^{k^{i}}\right) \left(s_{h+1}^{k^{i}}\right)+2\beta_{j}^{k}(i)\right]\\ &\leq\alpha_{n_{h}^{k}}^{0}H+\sum_{i=1}^{n_{h}^{k}}\alpha_{n_{h} ^{k}}^{i}\left[\left(V_{j,h+1}^{k^{i}}-\underline{V}_{j,h+1}^{k^{i}}\right) \left(s_{h+1}^{k^{i}}\right)+4\beta_{j}^{k}(i)\right]\\ &=\alpha_{n_{h}^{k}}^{0}H+\sum_{i=1}^{n_{h}^{k}}\alpha_{n_{h}^{k }}^{i}\delta_{j,h+1}^{k^{i}}+O\left(H\xi\left(A_{j},n_{h}^{k},\iota\right)+ \sqrt{H^{3_{\iota}}/n_{h}^{k}}\right)\end{split}\] (46)

The third line holds because we use FTRL as the subroutine, \(\beta_{j}(t)=c\cdot\sqrt{H^{3}A_{j}\iota/t}\), and due to our doubling scheme we have \(\beta(n_{h}^{\underline{k}})\leq 2\beta(n_{h}^{k})\). And in the last step we have used \(\sum_{i=1}^{t}\alpha_{i}^{i}\beta_{j,i}=\Theta\left(H\xi\left(A_{j},t,\iota \right)+\sqrt{H^{3}A\iota/t}\right)\).

The remaining step is the same as Jin et al. (2021). Summing the first two terms w.r.t. \(k\),

\[\sum_{k=1}^{K}\alpha_{n_{h}^{k}}^{0}H=\sum_{k=1}^{K}H\mathbb{I}\left\{n_{h}^{ k}=0\right\}\leq SH,\] (47)

\[\sum_{k=1}^{K}\sum_{i=1}^{n_{h}^{k}}\alpha_{n_{h}^{k}}^{i}\delta_{h+1}^{k^{i} }\leq\sum_{k^{\prime}=1}^{K}\delta_{h+1}^{k^{\prime}}\sum_{i=n_{h}^{\nu}+1}^{ \infty}\alpha_{i}^{n_{h}^{k^{\prime}}}\leq\left(1+\frac{1}{H}\right)\sum_{k=1 }^{K}\delta_{h+1}^{k}.\] (48)

Putting them together,

\[\begin{split}\sum_{k=1}^{K}\delta_{h}^{k}&=\sum_{k=1 }^{K}\alpha_{n_{h}^{k}}^{0}H+\sum_{k=1}^{K}\sum_{i=1}^{n_{h}^{k}}\alpha_{n_{h} ^{k}}^{i}\delta_{h+1}^{k^{i}}+\sum_{k=1}^{K}O\left(H\xi\left(A,n_{h}^{k},\iota \right)+\sqrt{\frac{H^{3_{\iota}}}{n_{h}^{k}}}\right)\\ &\leq HS+\left(1+\frac{1}{H}\right)\sum_{k=1}^{K}\delta_{h+1}^{k} +\sum_{k=1}^{K}O\left(H\xi\left(A,n_{h}^{k},\iota\right)+\sqrt{\frac{H^{3_{ \iota}}}{n_{h}^{k}}}\right).\end{split}\] (49)

Recursing this argument for \(h\in[H]\) gives

\[\sum_{k=1}^{K}\delta_{1}^{k}\leq eSH^{2}+e\sum_{h=1}^{H}\sum_{k=1}^{K}O\left(H \xi\left(A,n_{h}^{k},\iota\right)+\sqrt{\frac{H^{3_{\iota}}}{n_{h}^{k}}}\right).\] (50)

By pigeonhole argument,

\[\begin{split}\sum_{k=1}^{K}\left(H\xi\left(A,n_{h}^{k},\iota \right)+\sqrt{H^{3_{\iota}}/n_{h}^{k}}\right)&=O(1)\sum_{s}\sum_{ n=1}^{N_{h}^{K}(s)}\left(H\xi(A,n,\iota)+\sqrt{\frac{H^{3_{\iota}}}{n}}\right)\\ &\leq O(1)\sum_{s}\left(H\Xi\left(A,N_{h}^{K}(s),\iota\right)+ \sqrt{H^{3}N_{h}^{K}(s)_{\iota}}\right)\\ &\leq O\left(HS\Xi(A,K/S,\iota)+\sqrt{H^{3}SK\iota}\right),\end{split}\] (51)where in the last step we have used concavity. Finally take the sum w.r.t \(h\in[H]\), and plugging \(\Xi(B,t,\log(1/\delta))\leq O(\sqrt{HBt\log(B/\delta)})\) we have

\[\mathrm{Regret}(K)\leq O\left(\sqrt{H^{5}SA\iota}\right).\] (52)