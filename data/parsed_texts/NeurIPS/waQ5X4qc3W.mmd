# Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective

 Yongxin Zhu\({}^{1,3}\), Bocheng Li\({}^{2,3}\), Hang Zhang\({}^{4}\), Xin Li, Linli Xu\({}^{*}\)\({}^{2,3}\), Lidong Bing

\({}^{1}\)School of Data Science, University of Science and Technology of China

\({}^{2}\)School of Computer Science and Technology, University of Science and Technology of China

\({}^{3}\)State Key Laboratory of Cognitive Intelligence, \({}^{4}\)Zhejiang University

zyx2016@mail.ustc.edu.cn,bcli@mail.ustc.edu.cn,hangzhang_scu@foxmail.com

lixin4ever@gmail.com,linlixu@ustc.edu.cn,binglidong@gmail.com

Corresponding author.

###### Abstract

Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into a more compact latent space and learn the data distribution in the latent space instead of directly from pixels. However, this practice raises a pertinent question: Is it truly the optimal choice? In response, we begin with an intriguing observation: despite sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in image generation. This finding contrasts sharply with the field of NLP, where the autoregressive model GPT has established a commanding presence. To address this discrepancy, we introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Furthermore, we propose a simple but effective discrete image tokenizer to stabilize the latent space for image generative modeling by applying K-Means on the latent features of self-supervised learning models. Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models. Remarkably, for the first time, a GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size. Our findings underscore the potential of an optimized latent space and the integration of discrete tokenization in advancing the capabilities of image generative models. The code is available at https://github.com/DAMO-NLP-SG/DiGIT.

## 1 Introduction

In recent years, remarkable advancements have been achieved in the field of image generation, principally propelled by the development of latent-based generative models, such as Latent Diffusion Models (LDMs) [34, 30] and Mask Image Models (MIMs) [7, 26]. By employing reconstructive autoencoders such as VQGAN [15] or VAE [23] to compress images into a manageable low dimensional latent space, these models can generate highly realistic and imaginative samples. Concurrently, in light of the transformative impact of autoregressive (AR) generative models, such as Large Language Models [31, 32, 5, 27] in NLP, it becomes compelling to investigate the feasibility of similar paradigms to images. Despite the advances in image autoregressive pre-training, exemplified bymodels such as iGPT [8], AIM [14] and GIVT [36] in the pixel space, VIM [41] and LVM [1] in the latent space, their performance are still inferior to the leading models [34; 12; 7] in image generation, or self-supervised learning models [18; 10; 6; 28] in image understanding tasks.

An intriguing observation emerges regarding the presumed optimality of current practices in latent space: as illustrated in Figure 1(b), though sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in the image generation task. This discrepancy prompts reevaluating our understanding of latent spaces and their interaction with generative models, suggesting unexplored avenues worth investigating. A central premise in learning theory [20] is that the latent distribution should retain as much critical information of the data distribution as possible, akin to a compression goal. This leads to a common **misconception** that _an optimal latent space for reconstruction equates to optimal generative performance._ Nevertheless, in the investigation regarding the reconstruction and generation ability with the popular VQGAN model [42], it is observed that the generation FID will deteriorate when the reconstruction FID becomes lower (where a lower value indicates better performance), challenging the above assumption.

To address these intriguing discrepancies, we introduce a unified perspective on the relationship between latent spaces and generative models to analyze what constitutes an optimal latent space for generative models. Our findings reveal that, beyond the compression techniques employed by prevalent latent generative models, an optimal latent space should also aim to minimize the distance between distributions under the condition of incorporating a generative model, which is an aspect often overlooked. We critically assess prevalent methodologies and reveal that the stability of latent space is important for generative models. We argue that the reason why autoregressive models underperform iterative models such as LDMs and MIMs is that iterative models can correct errors brought by the instability of latent space.

Drawing from this insight, we propose a straightforward method to stabilize the existing latent space methods for image autoregressive generative models. Unlike conventional autoencoder-style approaches, our approach disentangles the concurrent training of encoders and decoders and commences with encoder-only training through a discriminative self-supervised model [28]. This phase does not necessitate a decoder for pixel reconstruction, enabling the encoder to discern the intrinsic and distinguishable features present within the data. Subsequently, a separate decoder of the autoencoder [15] is trained and tasked solely with the pixel reconstruction process, conditioned on the features identified by the encoder. By focusing initially on the encoder's capability to extract meaningful data features independently of pixel reconstruction, we lay a foundation for a more stable and feature-rich latent

Figure 1: **(a)**: Linear probe and class-unconditional generation performance of different methods trained and evaluated on ImageNet-1K. **(b)**: Class-conditional generation performance of different methods on ImageNet-1k. The size of the bubbles indicates the number of parameters in the models. DiGIT achieves SOTA performance in linear probing and establishes a new SOTA in image generation within a single model.

space. The subsequent independent training phase of the decoder ensures that these captured features can be accurately translated back to pixels.

In support of the autoregressive generative model, which requires discrete tokens for next token prediction, we employ a strategy inspired by VQGAN [15] to discretize the encoder's latent feature space with the K-Means clustering algorithm. With this novel image tokenizer induced from the stabilized latent space, the performance of autoregressive generative models in images is enhanced significantly in both image understanding and image generation tasks. We refer to this approach as call **D**iscriminative **G**enerative **I**mage **T**ransformer (DiGIT). Notably, when scaling up the model size, substantial improvements can be achieved. To the best of our knowledge, this is the first evidence that image autoregressive generative models behave analogously to GPT. In essence, this work endeavors to redefine the boundaries of what is possible in image autoregressive modeling through a unified perspective of latent space.

In summary, our contributions to the field of image generative models include:

* We introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling.
* We propose a novel method to stabilize latent space by disentangling the encoder and decoder training processes. Furthermore, a simple yet effective discrete image tokenizer is proposed to improve the image autoregressive generative model's performance under the philosophy of next token prediction.
* The experimental results show that the image autoregressive modeling with our tokenizer leads to SOTA performance in image understanding and generation, with further improvements witnessed when scaling up the model size.

## 2 Problem Formulation

In section 2.1, we formalize the latent space requirements for generative models and categorize current latent-based generative models. Furthermore, in section 2.2, we analyze the stability of different induced latent spaces and propose to stabilize the latent space for autoregressive generative models instead of stabilizing the generation process with an iterative decoding strategy like LDMs.

### Latent Space for Generative Models

Drawing inspiration from the complexity perspective of latent space induced from autoencoders in Hu et al. [21], we delve into the latent space for generative models. Generative models aim at learning a distribution to approximate the data distribution \(P_{X}\). Formerly, given a tractable prior distribution \(P_{Z}\) and a distance metric \(D(\cdot,\cdot)\) between distributions, the purpose of a generative model \(g\in\mathcal{G}\) is to minimize the distance between data distribution \(P_{X}\) and the distribution generated by \(g(Z)\):

\[\min_{g}D(P_{g(Z)},P_{X}).\] (1)

For example, GANs [16] employ the Gaussian distribution as their prior and utilize a discriminator network as the distance metric. However, the optimal strategy for data representation in generative models is still under-explored. Recent studies on latent diffusion models [34] have identified that direct learning in the pixel space of images is suboptimal. They propose to learn in a latent space induced by a constrained autoencoder model such as VAE [23] or VQGAN [15], which has been demonstrated to improve the perceptual quality.

A simple method to construct the latent space is using an encoder \(f\in\mathcal{F}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d_{z}}\) to map raw data samples \(x\in\mathbb{R}^{d}\) into a latent space \(f(X)\) of dimension \(d_{z}\). Consequently, the goal of latent-based generative models is to learn the distribution as per the following formula:

\[\min_{g}D(P_{g(Z)},P_{f(X)}),\] (2)

where \(P_{f(X)}\) denotes the data distribution in the latent space induced by the encoder \(f\). Despite these advances, determining the optimal latent space configuration for generative models remains unresolved.

Distance between distributions in different spaces.Given the ultimate goal of the generative model is to produce image pixels, a decoder model \(h\in\mathcal{H}:\mathbb{R}^{d_{z}}\rightarrow\mathbb{R}^{d}\), paired with the encoder model \(f\), is necessary to convert latent representations back into pixels. We define a generalized distance between different spaces associated with a decoder \(h\in\mathcal{H}\) as:

\[D^{\mathcal{H}}(P_{f(X)},P_{X}):=\inf_{h\in\mathcal{H}}D(P_{h(f(X))},P_{X}).\] (3)

By employing \(D^{\mathcal{H}}\) to compare distributions across different spaces, we can define the ideal latent space \(f(X)\) as the one that minimizes \(D^{\mathcal{H}}(P_{f(X)},P_{X})\). This implies selecting a latent representation that minimizes the empirical objective \(\mathcal{L}(h|P_{f(X)})\) with the same family \(\mathcal{H}\) of decoder models. Such a latent configuration depends on both the data and the decoder training methodology. We can formalize it with an autoencoder framework by looking at the encoder and decoder together, and the primary goal becomes the reconstruction of the input sample \(x\), formulated as \(\min_{f,h}\mathcal{L}(h(f(x)),x)\). Once a generative model \(g\) successfully approximates the latent distribution \(P_{f(X)}\), the generated sample can be efficiently transformed back into pixels using the decoder \(h\).

Similarly, we can define the distance between distributions in the latent space \(f(X)\in\mathbb{R}^{d_{z}}\) and data space \(X\in\mathbb{R}^{d}\) conditioned on the latent-based generative model \(g\in\mathcal{G}\),

\[D^{\mathcal{G}}(P_{f(X)},P_{X}):=\inf_{g\in\mathcal{G}}D(P_{g(Z)},P_{f(X)}),\] (4)

which aims to minimize the empirical objective with the generative model family \(\mathcal{G}\).

Optimal Latent Space for Generative Models.Now that we have characterized the ideal latent distribution given the family of generative models and the data, the next step is to determine how to find the optimal latent space. At the population level, the objective for the latent-based generative models with a decoder is:

\[\min_{h\in\mathcal{H},\mathcal{g}\in\mathcal{G}}D(P_{h(g(Z))},P_{X})=\min_{h \in\mathcal{H},g\in\mathcal{G}}D(P_{h(f(X))},P_{X})+D(P_{g(Z)},P_{f(X)}),\] (5)

where the first term focuses on optimizing the decoder to enhance the reconstruction quality and the second one is directed towards optimizing the generative model to more accurately approximate the latent space distribution. Inspired by this observation, we can characterize the optimal latent distribution \(P_{f(X)}^{*}\) for a given \(P_{X}\) from the perspective of minimizing the distance between distributions in different spaces by defining \(f^{*}\) as

\[\operatorname*{argmin}_{f\in\mathcal{F}}D^{\text{latent}}(P_{f(X)},P_{X}):=f^ {*}(X)=\operatorname*{argmin}_{f\in\mathcal{F}}D^{\mathcal{G}}(P_{f(X)},P_{X} )+D^{\mathcal{H}}(P_{f(X)},P_{X})\] (6)

Notice that \(P_{f^{*}(X)}\) depends on multiple factors, including \(P_{X}\), the distance metric \(D\), and the constructed model families \(\mathcal{G}\) and \(\mathcal{H}\). By integrating \(D^{\mathcal{G}}\) and \(D^{\mathcal{H}}\), we arrive at a comprehensive measure of the distance between the distribution in the latent space and the original data distribution as \(D^{\text{latent}}(P_{f(X)},P_{X})\). The second term, \(D^{\mathcal{H}}(P_{f(X)},P_{X})\), is exactly the objective of reconstructive autoencoders. Ultimately, from examining the learning objective pertinent to identifying the optimal latent space for generative models, it becomes evident that:

_A reconstructive autoencoder does not necessarily establish an advantageous latent space for generative models._

Two Pathways of the Latent Space Construction.Although we theoretically analyze the optimization of the optimal latent space for generative models, it is challenging to implement in practice because optimizing \((f,g,h)\) simultaneously is computationally complex. A practical solution is to optimize \(D^{\mathcal{G}}\) and \(D^{\mathcal{H}}\) separately, allowing for tractable training.

* When \(D^{\mathcal{H}}(P_{f(X)},P_{X})\) is not a target for optimization, it implies that the optimization of the decoder within the generative model framework is bypassed. The encoder _independently_ forms a latent space, aligning with self-supervised learning (SSL) strategies aimed at uncovering lower-dimensional features from unlabeled data. However, learning the generative models in the latent space induced by SSL models remains relatively unexplored.
* On the other hand, when \(D^{\mathcal{G}}(P_{f(X)},P_{X})\) remains fixed, the primary objective becomes optimizing the encoder and decoder to effectively learn and represent the latent space,where \(D^{\text{latent}}(P_{f(X)},P_{X})\) degrades into an autoencoder learning objective. This approach is evident in recent latent-space-oriented generative models, such as LDM [34; 30], VQGAN (AR)[15], and MaskGIT (MIM) [7], all of which concentrate on learning \(g\) in the latent space with the encoder and decoder frozen. 2

Footnote 2: \(D^{\mathcal{G}}\) and \(D^{\mathcal{H}}\) can achieve zero simultaneously. For example, in the well-known _posterior collapse_ phenomenon in the VAE literature, the latent space \(f(X)\) is a tractable Gaussian distribution and \(D^{\mathcal{G}}(P_{f(X)},P_{X})\) can be zero by simply setting the generative models as a Gaussian sampler. If the decoder is strong enough and directly generates samples without conditioning on the encoder output, \(D^{\mathcal{H}}(P_{f(X)},P_{X})\) can be zero as well.

While latent generative models such as LDM [34], MaskGIT [7], and VQGAN [15] share the same latent space induced by a reconstructive autoencoder [15] to minimize \(D^{\mathcal{H}}(P_{f(X)},P_{X})\), their image generation performances differ significantly. In the next section, we analyze the reason behind it from the perspective of the latent space.

### Stability of the Latent space

We first describe the decoding mechanism of various latent generative models. Both LDM [34] and MaskGIT [7] can be depicted as an iterative sampling procedure given by:

\[p(x^{T})=\prod_{i=1}^{T}p(x^{i}|x^{i-1}).\] (7)

The intermediate states \(x^{i}\) in LDMs represent images infused with Gaussian noise of varying variance, whereas for MaskGIT, they denote discretely tokenized images augmented with masks. In contrast, the autoregressive framework of VQGAN (AR) is described as:

\[p(x)=p(x_{1},\dots,x_{N})=\prod_{j=1}^{N}p(x_{j}|x_{<j}),\] (8)

where \(x_{i}\) represents the \(i\)-th patch in the image sequence. Notice that \(x^{i}\) denotes the _entire_ image while \(x_{j}\) means the _local_ patch tokens. In the autoregressive decoding process, if the previously sampled tokens are incorrect, the accuracy of subsequent tokens would be affected due to error aggregation. In contrast, the iterative decoding approach allows for the revision of earlier misjudged tokens. When the latent space is unstable that small perturbation in pixels can change the latent distribution significantly, the iterative decoding mechanism employed by LDM and MaskGIT can alleviate the error aggregation problem by allowing for revision of earlier misjudged tokens while autoregressive models cannot. Consequently, a stable latent space is required to reduce errors introduced in the generation process of autoregressive models.

This principle forms the foundation of our methodology for developing a metric to evaluate latent spaces with an emphasis on the stability of the latent representations. We examine two primary types of latent spaces: (1) autoencoder induced by minimizing \(D^{\mathcal{H}}(P_{f(X)},P_{X})\) and (2) self-supervised learning (SSL) model induced by minimizing \(D^{\mathcal{G}}(P_{f(X)},P_{X})\). By analyzing network parameters of these models in a linear regime, we derive the following propositions.

**Proposition 2.1**.: _The latent space spanned by a linear autoencoder is congruent with that spanned by the principal component loading vectors derived in Principal Component Analysis (PCA). Furthermore, the principal component loading vectors can be elucidated from the autoencoder's weights._

**Proposition 2.2**.: _The discriminative self-supervised model learns to separate data distributions in the latent space as Linear Discriminant Analysis in principle._

Motivated by these theoretical insights, we introduce a metric to assess the stability of the latent space induced by different encoder models. To exemplify these concepts, we refer to an example consisting of two Gaussian distributions in a two-dimensional space, as depicted in Figure 6(a). The results attained from applying the PCA and LDA algorithms are visually depicted in Figure 6(b) and (c) respectively. The distribution embedded by the LDA model exhibits greater separability than that by the PCA model. To quantitatively evaluate stability, we add Gaussian noise of different variances to the original 2D data and subsequently train a linear classifier on the latent space. As Figure 6(d) illustrates, the accuracy of the LDA model consistently surpasses that of PCA.

To evaluate the stability of latent space induced from autoencoders and SSL models, we add Gaussian noise to image pixels and then feed the noisy images to a VQGAN encoder and an SSL encoder DINOv2 [28]. This experiment aims to examine the resilience of the latent spaces induced by these encoders to such disturbances. We measure the rate of change in discrete tokens, specifically VQ tokens for the latent space induced from the VQGAN encoder and discriminative tokens for the latent space induced from the SSL model, and the cosine similarity in conjunction with the strength of the noise introduced. The experimental results in Table 1 demonstrate that the latent space induced from the SSL model exhibits heightened stability compared to that derived from the VQGAN autoencoder. Therefore, we propose to replace the unstable latent space induced by the reconstructive model with a stable latent space induced by the discriminative self-supervised model for autoregressive models.

## 3 Stabilize the Latent Space with Self-supervised Learning Model

In this section, we present a simple but effective image tokenizer that discretizes the feature representations of discriminative SSL models to form discrete tokens for autoregressive models. The architecture of our model is illustrated in Figure 2.

Discrete Image Discriminative TokenizerDrawing inspiration from the VQGAN tokenizer [15], which employs an implicit K-Means clustering algorithm within the latent space to generate discrete tokens for autoregressive modeling, we propose a straightforward approach to perform K-Means clustering on the feature space of discriminative SSL models to obtain discrete tokens. To process a given dataset, our initial step involves gathering the features of image patches, akin to the hidden states produced by SSL models. Then we employ a clustering algorithm to group these patches, resulting in a collection of \(K\) clustering centers. These centers constitute the codebook for the discrete tokenizer. To determine the discrete tokens for an image patch at inference, we identify its nearest neighbor in the codebook, which is then designated as the discrete token for the respective patch.

Image Autoregressive ModelingAfter converting images into discrete tokens with the discriminative tokenizer, we treat each image as a sequence by flattening the discrete tokens from images into a 1D sequence in raster order. We train a causal Transformer [38] model with the next token prediction objective, which is the same as the standard approach for language models.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline SNR & **30** & **25** & **20** & **15** & **10** & **5** & **1** & **0.01** \\ \hline
**VQ Token change \(\downarrow\)** & 0.187 & 0.317 & 0.487 & 0.663 & 0.805 & 0.901 & 0.948 & 0.956 \\
**Disc Token change \(\downarrow\)** & 0.114 & 0.178 & 0.260 & 0.355 & 0.457 & 0.570 & 0.687 & 0.721 \\
**VQ Token cos-sim \(\uparrow\)** & 0.972 & 0.949 & 0.910 & 0.853 & 0.777 & 0.682 & 0.594 & 0.571 \\
**Disc Token cos-sim \(\uparrow\)** & 0.975 & 0.960 & 0.940 & 0.916 & 0.888 & 0.855 & 0.816 & 0.803 \\ \hline \hline \end{tabular}
\end{table}
Table 1: The stability of latent spaces induced from VQ Token and Discriminative Token (introduced in Section 3), assessed across different Signal-to-Noise Ratio (SNR) levels to evaluate performance under varying signal and noise conditions.

Figure 2: The architecture of DiGIT.

## 4 Experiments

### Implementation Details

We take the discriminative SSL model DINOv2 [28] as the encoder for all experiments. The K-Means model is trained on the randomly selected 10% subset of the ImageNet [11] training set. We use the autoregressive model with the same architecture as GPT-2 [32]. We train the DiGIT models with the base and large sizes. The vocabulary size of the tokenizer for the base is \(8192\) and \(16000\) for the large size. More implementation details and hyper-parameters are provided in Appendix A.3.

### Image Understanding

The GPT model is famous for learning semantic features by a generative training objective of next token prediction. We compare the image understanding ability of different image autoregressive models with linear-probe as described in iGPT [8]. We train a linear classifier on top of the frozen features average from each layer on the ImageNet training set. We report the Top-1 accuracy compared with other image autoregressive models in Table 2. Remarkably, with only 219M parameters, DiGIT achieves a Top-1 accuracy of 71.7%, surpassing both iGPT and VIM-Base, which have a greater number of parameters and operate at more visual tokens. Despite representing images with a smaller token grid size (\(16\times 16\) as opposed to \(32\times 32\)), DiGIT still delivers superior top-1 accuracy, demonstrating the effectiveness of our tokenizer. Moreover, when we scale DiGIT's parameters from 219M to 732M, the Top-1 accuracy shows an additional increase of 8.6% and reaches 80% for the first time. The improvement indicates that DiGIT with the proposed discriminative tokenizer has the potential for the development of large vision models.

### Image Generation

Since the SSL models do not have a paired decoder to recover pixels from latent space, the generative models trained with our discriminative tokenizer require an auxiliary image decoder to render pixels. The discriminative tokenizer can be seamlessly integrated with any existing image generative models trained with a tokenizer induced from a reconstructive autoencoder. In our experiment, we train an autoregressive model VQGAN, and an MIM model MaskGIT as the pixel decoder respectively. The results are presented in Table 3 and Table 4. The autoregressive model equipped with our discriminative tokenizer achieves the SOTA performance with FID reaching \(3\) for the first time. Furthermore, the performance significantly improves as the model size increases, demonstrating the potential of a large vision model with next token prediction. Interestingly, when utilizing the DiGIT as the conditioning factor, the performance of both the autoregressive and MaskGIT decoders becomes close (4.62 and 4.79). This observation suggests that stabilizing the latent space produces effects analogous to the iterative stabilization decoding mechanism.

### Ablation Study

We conduct the ablation study to present a comprehensive analysis of the proposed discriminative tokenizer in image generation and understanding. The results are illustrated in Table 3(a) and

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Methods & \# Tokens & Features & \# Params & Top-1 Acc.\(\uparrow\) \\ \hline iGPTL [8] & \(32\times 32\) & 1536 & 1362M & 60.3 \\ iGPT-XL [8] & \(64\times 64\) & 3072 & 6801M & 68.7 \\ VIM+VQGAN [41] & \(32\times 32\) & 1024 & 650M & 61.8 \\ VIM+dVAE [41] & \(32\times 32\) & 1024 & 650M & 63.8 \\ GIVT [36] & \(16\times 16\) & 1024 & 304M & 65.1 \\ VIM+ViT-VQGAN [41] & \(32\times 32\) & 1024 & 650M & 65.1 \\ VIM+ViT-VQGAN [41] & \(32\times 32\) & 2048 & 1697M & 73.2 \\ AIM [14] & \(16\times 16\) & 1536 & 0.6B & 70.5 \\
**DiGIT (Ours)** & \(16\times 16\) & 1024 & 219M & 71.7 \\
**DiGIT (Ours)** & \(16\times 16\) & 1536 & 732M & **80.3** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Linear-probe accuracy of image autoregressive generative models on ImageNet [11].

Figure 3(b). For image generation tasks, we take the autoregressive model trained with the VQGAN tokenizer as the baseline. Introducing discriminative tokens leads to a significant improvement, reducing FID to \(9.66\) and increasing IS to \(69.15\), underscoring the effectiveness of stabilizing latent space for autoregressive models. Further extending the training duration to \(400\) epochs yielded additional improvements of \(0.53\). A substantial advancement is observed when scaling up the model size to 732M, resulting in FID dropping dramatically to \(4.59\) and IS more than doubling to \(141.29\). This indicates that increasing the model's capacity significantly enhances its ability to model complex relationships within the data, which is a similar phenomenon in GPT models. Overall, the study highlights the latent space stabilization and the potential of large-scale training of autoregressive modeling in images with our discriminative tokenizer.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline Type & Methods & \#Param & \#Epoch & FID\(\downarrow\) & IS\(\uparrow\) \\ \hline GAN & BigGAN [4] & 70M & - & 38.6 & 24.70 \\ Diff. & LDM [34] & 395M & - & 39.1 & 22.83 \\ Diff. & ADM [12] & 554M & - & 26.2 & 39.70 \\ MIM & MAGE [26] & 200M & 1600 & 11.1 & 81.17 \\ MIM & MAGE [26] & 463M & 1600 & 9.10 & 105.1 \\ \hline MIM & MaskGIT [7] & 227M & 300 & 20.7 & 42.08 \\ MIM & **DiGIT** (+MaskGIT) & 219M & 200 & **9.04** & **75.04** \\ \hline AR & VQGAN [15] & 214M & 200 & 24.38 & 30.93 \\ AR & GIVT [36] & 304M & 500 & 17.70 & - \\ AR & GIVT [36] & 1.67B & 500 & 11.02 & - \\ AR & **DiGIT** (+VQGAN) & 219M & 400 & **9.13** & **73.85** \\ AR & **DiGIT** (+VQGAN) & 732M & 200 & **4.59** & **141.29** \\ validation data & DiGIT + VQ & - & - & 1.92 & 184.40 \\ validation data & VQ only & - & - & 1.67 & 175.56 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Class-unconditional image generation on ImageNet with resolution \(256\times 256\). DiGIT + VQ indicates that we utilize golden discriminative tokens alongside VQ generated by autoregressive models.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline Type & Methods & \#Param & \#Epoch & FID\(\downarrow\) & IS\(\uparrow\) \\ \hline GAN & BigGAN [4] & 70M & - & 38.6 & 24.70 \\ Diff. & LDM [34] & 395M & - & 39.1 & 22.83 \\ Diff. & ADM [12] & 554M & - & 26.2 & 39.70 \\ MIM & MAGE [26] & 200M & 1600 & 11.1 & 81.17 \\ MIM & MAGE [26] & 463M & 1600 & 9.10 & 105.1 \\ \hline MIM & MaskGIT [7] & 227M & 300 & 20.7 & 42.08 \\ MIM & **DiGIT** (+MaskGIT) & 219M & 200 & **9.04** & **75.04** \\ \hline AR & VQGAN [15] & 214M & 200 & 24.38 & 30.93 \\ AR & GIVT [36] & 304M & 500 & 17.70 & - \\ AR & GIVT [36] & 1.67B & 500 & 11.02 & - \\ AR & **DiGIT** (+VQGAN) & 219M & 400 & **9.13** & **73.85** \\ AR & **DiGIT** (+VQGAN) & 732M & 200 & **4.59** & **141.29** \\ validation data & DiGIT + VQ & - & - & 1.92 & 184.40 \\ validation data & VQ only & - & - & 1.67 & 175.56 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Class-conditional image generation on ImageNet with resolution \(256\times 256\). \(\dagger\) denotes the model is trained with classifier-free guidance while all the other models are not.

For the image understanding task, we investigate the effect of K-Means clusters and features learned in the different layers of DiGIT. We can see that increasing the cluster number can further improve the accuracy of the linear probe, which means the image autoregressive model can benefit from a larger vocabulary. The linear probe accuracy increases quickly from the first transformer block, reaches its peak at the middle layers, and finally decreases a little bit for the last few blocks. This observation connects the image autoregressive model to the text language model where the semantic information is learned in the middle layers of the transformer.

### Comparison of Discrete Tokenizers

We conduct an experiment to investigate the effect of different SSL models on latent space. We generally categorize the SSL models into two types according to their pre-training objectives: (1) Global level (MoCo) and patch level (MAE,iBOT), (2) reconstructive (MAE) and discriminative (MoCo, iBOT). At the global level, the loss function is computed using an aggregate output such as [**CLS**] token or mean pooling. In contrast, patch-level models involve patches directly in loss computation. Reconstructive models, such as MAE, aim to recover image pixels in a manner akin to autoencoders, while discriminative models are optimized to learn the distinguishable features. As demonstrated in Table 4(a), the discriminative objective plays a pivotal role in image generation in that it can stabilize the latent space. Furthermore, because generative models need to predict patches, the inclusion of a patch-level loss function can enhance performance.

To assess the stability of latent space induced by our discriminative tokenizer and reconstructive tokenizer. We pre-train two auto-regressive generative models on the ImageNet dataset [11], employing the proposed discriminative tokenizer and VQGAN tokenizer respectively. We provide each model with the upper half of the target image as a conditional prompt for generation, challenging them to complete the lower half of the image. A stable latent space should be able to help the autoregressive model generate the lower half more robustly, maintaining thematic and aesthetic coherence. As shown in Figure 4(b), the FID decreases for both models when given a longer prefix context. However, when the prefix length is reduced from 75% to only 12.5% of the image, the model trained with the VQGAN tokenizer encounters difficulties in producing images that adhere to the specified prompt. In contrast, the model utilizing the discriminative tokenizer effectively continues to produce congruent visual tokens, maintaining low FID scores even with a significantly truncated prefix.

## 5 Related Work

Image TokenizerThe image tokenizer [37; 33; 15] is essential in converting pixels into discrete tokens for autoregressive generative modeling. VQVAE [37] first proposes to assign latent features learned by an encoder to the nearest entry in the learnable codebook embeddings, followed by a decoder to reconstruct the original image pixels. VQGAN [15] further incorporates adversarial loss and perceptual loss to improve the image synthesis quality. RQ-Transformer [24] extends the single-layer quantizer to the multi-layer residual quantizer to augment the visual tokenizer's ability to capture fine-grained details. ViT-VQGAN [41] incorporates the modern Vision-Transformer [13] into VQGAN to enhance the reconstruction quality. MAGVIT-v2 [42] substitute the online update codebook in VQGAN with a lookup-free quantizer to enable a larger vocabulary for generative language models.

Figure 3: Ablation study of DiGIT. (a) The comparison of tokenizer, training steps, and model size in the image generation task. (b) Linear-probe accuracy from different layers in the pre-trained DiGIT-base with different number of K-Means clusters.

Image Autoregressive ModelingInspired by the success of autoregressive Transformer [38] in text generation tasks, there have been several efforts to replicate it in image generation tasks. One of the pioneering works is iGPT [8], which pre-trains an autoregressive model on pixels with the same architecture as GPT2 [32], achieving promising results in unsupervised visual representation learning. LVM [1] proposes a large-scale vision dataset composed of images and videos, based on which a large vision model is trained. Empirical observations indicate that the model scales effectively across various tasks with in-context learning [40]. Similarly, AIM [14] follows ViT [13] and represents images in the patch. It is observed that the performance of image recognition continues to increase as the model size scales up. VAR [35] proposes a next-scale prediction to generate images from coarse to fine in a hybrid of autoregressive and nonautoregressive manner.

Self-supervised Learning ModelsSelf-supervised learning (SSL) [9; 17] plays an important role in learning fundamental visual representations for downstream tasks. Among them, SimCLR [9], MoCo [9; 18; 10] compute losses at the image level through \([\mathbf{CLS}]\) token aggregation or pooling operations with contrastive learning. iBOT-style models [43; 6; 28] extend the loss to the patch level, achieving improved performance in dense prediction tasks. BEiT [3] uses VQGAN tokenized sequences as the training target. MAE [19] randomly masks some patches of images and reconstructs the pixels with unmasked patches as the condition.

## 6 Conclusion

In this paper, we make an exploration in the latent space for generative modeling. We introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Subsequently, we propose a simple but effective discriminative image tokenizer, followed by an image autoregressive generative model DiGIT. Empirical results indicate that our tokenizer achieves superior performance across both image understanding and image generation tasks. Notably, when DiGIT is scaled up in model size, it exhibits even greater enhancements, indicating the potential for the development of large vision models. Our findings challenge the conventional wisdom that proficiency in reconstruction equates to an effective latent space for auto-regressive generation. Through this work, we aim to rekindle interest in the generative pre-training of image auto-regressive models and encourage a reevaluation of the fundamental components that define latent space for generative models.

## Acknowledgments and Disclosure of Funding

This research was supported by the National Key Research and Development Program of China (Grant No. 2022YFB3103100), the National Natural Science Foundation of China (Grant No. 62276245).

## References

* [1] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan L. Yuille, Trevor Darrell, Jitendra Malik, and Alexei A. Efros. Sequential modeling enables scalable learning for large

Figure 4: **(a)**: The comparison of tokenizers induced from different SSL models. Acc@LP is obtained by linear probing on the autoregressive model (model size of 39M for 100 epochs) trained with tokenizers. Acc@OL is the linear probe score of the SSL model. “P”: patch, “D”: discriminative, “R”: reconstructive. **(b)**: Generation quality curves in FID on ImageNet \(256\times 256\) valid set when scaling the prefix length with discriminative tokenizer and reconstructive VQGAN tokenizer. Both are autoregressive models with 219M parameters.

vision models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 22861-22872, June 2024.
* Baldi and Hornik [1989] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. _Neural Networks_, 2(1):53-58, 1989. ISSN 0893-6080. doi: https://doi.org/10.1016/0893-6080(89)90014-2. URL https://www.sciencedirect.com/science/article/pii/0893608089900142.
* Bao et al. [2022] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image transformers. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=p-BhZSz59o4.
* Brock et al. [2019] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=B1xsqj09Fm.
* Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS '20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.
* Caron et al. [2021] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9650-9660, October 2021.
* Chang et al. [2022] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11315-11325, June 2022.
* Chen et al. [2020] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 1691-1703. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/chen20s.html.
* Chen et al. [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 1597-1607. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/chen20j.html.
* Chen et al. [2020] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. _arXiv preprint arXiv:2003.04297_, 2020.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pages 248-255, 2009. doi: 10.1109/CVPR.2009.5206848.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 8780-8794. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf.
* Dosovitskiy et al. [2021] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=YicbFdNTTY.

* [14] Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Vaishaal Shankar, Alexander T Toshev, Joshua M. Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image models. In _Forty-first International Conference on Machine Learning_, 2024. URL https://openreview.net/forum?id=c92KDfEZTlg.
* [15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12873-12883, June 2021.
* [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8706494c97b1afcccf3-Paper.pdf.
* a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 21271-21284. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf.
* [18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2020.
* [19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 16000-16009, June 2022.
* [20] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. _Science_, 313(5786):504-507, 2006. doi: 10.1126/science.1127647. URL https://www.science.org/doi/abs/10.1126/science.1127647.
* [21] Tianyang Hu, Fei Chen, Haonan Wang, Jiawei Li, Wenjia Wang, Jiacheng Sun, and Zhenguo Li. Complexity matters: Rethinking the latent space for generative modeling. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 29558-29579. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/5e8023f07625374c6ff3aa08bb38e0e-Paper-Conference.pdf.
* [22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [23] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [24] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11523-11532, June 2022.
* [25] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and WOOK SHIN HAN. Draft-and-revise: Effective image generation with contextual rq-transformer. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 30127-30138. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/c276c3030c723c83a43b95a44a1cfbf-Paper-Conference.pdf.
* [26] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2142-2152, June 2023.

* [27] OpenAI. Gpt-4 technical report, 2023.
* [28] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. _Transactions on Machine Learning Research_, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=a68SUt6zFt.
* [29] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In _Proceedings of NAACL-HLT 2019: Demonstrations_, 2019.
* [30] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 4195-4205, October 2023.
* [31] Alec Radford. Improving language understanding by generative pre-training. 2018.
* [32] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [33] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf.
* [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, June 2022.
* [35] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. 2024.
* ECCV 2024_, pages 292-309, Cham, 2025. Springer Nature Switzerland. ISBN 978-3-031-72998-0.
* [37] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf.
* [38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fb0d053c1c4a845aa-Paper.pdf.
* [39] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 9929-9939. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/wang20k.html.

* Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 24824-24837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5509613524ecdf4f15af0f7b3labca4-Paper-Conference.pdf.
* Yu et al. [2022] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=pfNyExj7z2.
* tokenizer is key to visual generation. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=gzqrANCF4g.
* Zhou et al. [2022] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image BERT pre-training with online tokenizer. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=ydopy-e6Dg.

Appendix

### Limitation and Broader Impact

Our proposed discriminative tokenizer exhibits remarkable capabilities in both the image understanding and the image generation tasks, which is challenging for a single model. However, the discriminative tokenizer induced by the SSL model cannot directly render pixels. Consequently, we need to train another decoder model to convert tokens from discriminative tokenizers to VQ-GAN tokens. The potential for a direct generation of RGB pixels from the tokens produced by the discriminative tokenizer remains an uncharted avenue, which we leave for future work.

Image generative models possess a dichotomous nature, particularly within the realm of visual media. On the positive side, they foster a myriad of creative endeavors, and methodologies aim to minimize the costs of training and inference, holding the potential to broaden access and democratize the use of this technology. On the negative side, the simplicity with which manipulated content can be crafted and propagated raises serious concerns. This includes the proliferation of misinformation and spam. Furthermore, generative models may inadvertently unveil aspects of their training data, a particularly troubling issue when that data includes sensitive or personal information collected without explicit consent.

### Proof of claims in Section 2

**Proposition A.1**.: _Consider the optimization problem for \(X\in\mathbb{R}^{n}\):_

\[\min_{W_{1}\in\mathbb{R}^{m\times n},W_{2}\in\mathbb{R}^{n\times m}}\|X-W_{2} W_{1}X\|_{F}^{2},\] (9)

_which is a linear autoencoder. \(W_{2}\) is a minimizer of the problem if and only if its column space is spanned by the first \(m\) loading vectors of \(X\)._

Proof.: First, we derive the condition for the optimal \(W_{1}\) in the context of this optimization problem. Setting the gradient of the objective function with respect to \(W_{1}\) to zero leads to \(W_{1}\) being the left Moore-Penrose pseudoinverse of \(W_{2}\)[2]. Similarly, setting the gradient with respect to \(W_{2}\) to zero would identify \(W_{2}\) as the right pseudoinverse of \(W_{1}\):

\[W_{1}=W_{2}^{\dagger}=(W_{2}^{T}W_{2})^{-1}W_{2}^{T}.\] (10)

This finding indicates that the optimization can be simplified to focus on a single matrix (either \(W_{1}\) or \(W_{2}\)), hence removing the redundancy in the parameters:

\[\min_{W_{2}\in\mathbb{R}^{n\times m}}\|X-W_{2}W_{2}^{\dagger}X\|_{F}^{2}.\] (11)

The term \(W_{2}W_{2}^{\dagger}=W_{2}(W_{2}^{T}W_{2})^{-1}W_{2}^{T}\) is recognized as the matrix form of the orthogonal projection operator onto the column space of \(W_{2}\). This property holds true even when the column vectors of \(W_{2}\) are not orthonormal.

By performing QR decomposition on \(W_{2}\), \(W_{2}=QR\) where \(Q\) is an orthogonal matrix (\(Q^{T}Q=I\)) and \(R\) is an upper triangular matrix, we effectively transform the problem into optimizing over orthogonal matrices. The objective can thus be restated as:

\[\min_{W\in\mathbb{R}^{m\times n}}\|X-W^{T}WX\|_{F}^{2},\quad\text{subject to}\quad WW^{T}=I_{n\times n}.\] (12)

This revelation explicitly demonstrates that minimizing the reconstruction error in the space \(\mathbb{R}^{m\times n}\) demands that \(W\) (equivalent to \(W_{2}\) in our context) projects \(X\) onto a space spanned by its most significant structural components (in terms of variance), which are precisely the first \(m\) loading vectors, or principal components, of \(X\). 

**Proposition A.2**.: _The discriminative self-supervised model learns to separate data distributions in latent space as LDA in principle._

Proof.: We first consider the objective of Fisher LDA for two-class classification. LDA seeks to find a linear projection that maximizes the separation between the two classes:\[J(w)=\frac{w^{T}S_{B}w}{w^{T}S_{W}w},\] (13)

where \(S_{B}\) is the between-class scatter matrix, \(S_{W}\) is the within-class scatter matrix, and \(w\) is the projection vector.

We take contrastive learning with InfoNCE for analysis because it is the most popular learning objective in discriminative SSL models. For contrastive learning using the asymptotic form of the InfoNCE objective [39], we have two components:

\[\mathcal{L}=-\frac{1}{\tau}\mathbb{E}_{(x,x^{+})\sim p_{pos}}\left[f(x)^{\top }f(x^{+})\right]+\mathbb{E}_{x\sim p_{data}}\left[\log\mathbb{E}_{x^{-}\sim p_ {data}}\left[e^{f(x)^{\top}f(x^{-})/\tau}\right]\right],\] (14)

where the first term encourages similarity between positive pairs. In LDA, this is analogous to minimizing \(S_{W}\) because we want points from the same class to be close to each other when projected onto \(w\). The second term, when expanded using Jensen's inequality, represents an upper bound on the regularized sum of all pairwise inner products between different embeddings, effectively encouraging dissimilarity between all samples:

\[\mathbb{E}_{x\sim p_{data}}\left[\log\mathbb{E}_{x^{-}\sim p_{data}}\left[e^{ f(x)^{\top}f(x^{-})/\tau}\right]\right]=\frac{1}{m}\sum_{i=1}^{m}\log\left( \frac{1}{m}\sum_{j=1}^{m}e^{\mathbf{h}_{i}^{\top}\mathbf{h}_{j}/\tau}\right) \geq\frac{1}{\tau m^{2}}\sum_{i=1}^{m}\sum_{j=1}^{m}h_{i}^{\top}h_{j}.\] (15)

When \(\mathbf{h}_{i}=f(x_{i})\) are normalized, optimizing this term aims to decrease \(\mathrm{Sum}(\mathbf{W}\mathbf{W}^{\top})=\sum_{i=1}^{m}\sum_{j=1}^{m} \mathbf{h}_{i}^{\top}\mathbf{h}_{j}\). This term, being an upper bound for the largest eigenvalue of \(\mathbf{W}\mathbf{W}^{\top}\), when minimized, encourages a "flatter" singular value distribution of the embedding space. This flattening makes the embedding space more isotropic, which in turn increases the between-class scatter \(S_{B}\).

To make the connection explicit, consider that in LDA, we want to maximize \(S_{B}\), the between-class scatter, which is typically represented as:

\[S_{B}=(\mu_{1}-\mu_{2})(\mu_{1}-\mu_{2})^{T},\] (16)

where \(\mu_{1}\) and \(\mu_{2}\) are the means of the two classes.

Analogously, in the contrastive learning framework, minimizing the second term scatters the embeddings more uniformly in the high-dimensional space, akin to maximizing between-class scatter. The uniform distribution of the embeddings across the space increases the separation between classes or clusters of points, akin to the effect of maximizing \(S_{B}\).

### Implementation Details

We extract the hidden states from the third-to-last layer of DINOv2 for discriminative tokenizer training, as it suggests more generalized representations in the intermediate layers compared to the last layer. All DiGIT models are trained from scratch with a batch size of \(2048\) for the base model and \(1024\) for the large model, over a duration of \(200\) epochs. For the image generation task, the decoder model for pixel rendering is trained with a batch size of \(2048\) for \(400\) epochs, while the large model uses a batch size of \(1024\) for \(200\) epochs. The base model consists of \(16\) layers with a dimension of \(1024\) and a hidden dimension of \(4096\). The large model consists of \(24\) layers with a dimension of \(1536\) and a hidden dimension of \(6144\). Both configurations feature 16 attention heads. For all models, we utilize the Adam [22] optimizer with \(\beta=(0.9,0.98)\). We employ an inverse square root decay schedule for the learning rate with a peak value of \(3e-4\) for \(10\) epochs warm-up. We use the same VQGAN model as MAGE Li et al. [26]. During training, we apply data augmentations including random crops resized to the short edge and horizontal flips. For the decoding strategy, we apply top-k sampling with \(K=400\) for the large model and \(K=200\) for the base model. In the autoregressive decoder for pixel rendering, we use a probability of \(0.8\) with a temperature of \(1.0\) for top-p sampling. All experiments are conducted using the Fairseq[29] library.

Figure 5: FID and Inception Score as a function of top-k, top-p sampling on the image generation task with DiGIT-base. The decoding temperature is fixed to 1.0. The “stage2” denotes the autoregressive model for pixel rendering.

Figure 6: Toy example of PCA and LDA.

### Qualitative Cases

Figure 7: Class-unconditional image generation results on ImageNet 256\(\times\)256 by DiGIT.

Figure 8: Class-conditional image generation results on ImageNet 256x256 by DiGIT, where the images in the same row share the same class label.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Please refer to Section 6. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to Section A.1. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Please refer to Section 2 and Appendix A.2  Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Please refer to Appendix A.3 Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: A primary reason for not providing open access to the code is at the request of our collaborators. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Section 4 and Appendix A.3. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Error bars are not reported because computing them would be too computationally expensive, given the large-scale nature of our experiments, which involved multiple large models and extensive datasets. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please refer to Appendix A.3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research aligns with the NeurIPS Code of Ethics by ensuring ethical treatment of participants, maintaining data privacy, avoiding harmful applications, considering environmental impacts, providing transparency for reproducibility, and complying with legal and ethical standards. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please refer to Section A.1. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: Implementing effective safeguards is challenging now due to limited computational and human resources. We plan to develop and integrate appropriate measures prior to release to ensure responsible usage. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Please refer to Section 4.5. we have appropriately cited the ImageNet dataset and adhered strictly to its terms of access, as outlined at https://image-net.org/download.php. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with Human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.