# Fine-Grained Dynamic Framework for Bias-Variance Joint Optimization on Data Missing Not at Random

 Mingming Ha

MYbank, Ant Group

Beijing, China

hamingmingming.hmm@mybank.cn

&Xuewen Tao

MYbank, Ant Group

Shanghai, China

xuewen.txw@mybank.cn

&Wenfang Lin

MYbank, Ant Group

Hangzhou, China

moxi.lwf@mybank.cn

&Qiongxu Ma

MYbank, Ant Group

Shanghai, China

qiongxu.mqx@mybank.cn

&Wujiang Xu

MYbank, Ant Group

Shanghai, China

xuwujiang.xwj@mybank.cn

&Lixun Chen

MYbank, Ant Group

Beijing, China

linxun.clx@mybank.cn

###### Abstract

In most practical applications such as recommendation systems, display advertising, and so forth, the collected data often contains missing values and those missing values are generally missing-not-at-random, which deteriorates the prediction performance of models. Some existing estimators and regularizers attempt to achieve unbiased estimation to improve the predictive performance. However, variances and generalization bound of these methods are generally unbounded when the propensity scores tend to zero, compromising their stability and robustness. In this paper, we first theoretically reveal that limitations of regularization techniques. Besides, we further illustrate that, for more general estimators, unbiasedness will inevitably lead to unbounded variance. These general laws inspire us that the estimator designs is not merely about eliminating bias, reducing variance, or simply achieve a bias-variance trade-off. Instead, it involves a quantitative joint optimization of bias and variance. Then, we develop a systematic fine-grained dynamic learning framework to jointly optimize bias and variance, which adaptively selects an appropriate estimator for each user-item pair according to the predefined objective function. With this operation, the generalization bounds and variances of models are reduced and bounded with theoretical guarantees. Extensive experiments are conducted to verify the theoretical results and the effectiveness of the proposed dynamic learning framework.

## 1 Introduction

In virtually all real-world applications, the pieces of data we collected are partially missing with certain probabilities. A special case with the identical missing probability is known as missing at random (MAR) [1]. However, in online recommendation, search, and display advertising, there are lots of missing-not-at-random (MNAR) click, conversion, and rating records [2; 3; 4], which are missing with different probabilities, i.e., propensities. For example, in recommendation systems, a user usually clicks the items that she/he is likely to purchase and ignores other items with a low willingness to buy. Therefore, the observed click and conversion data is MNAR, which are not representative samples of all the events [5]. When the MNAR data is used to train a model, the prediction performance of this model on the MAR data is generally unacceptable. This is because MNAR data introduces sample selection bias [3; 6] into the prediction model. To eliminate sample selection bias, lots of debiasing estimators [3; 6; 7; 8; 9] have been developed, e.g., Error-ImputationBased (EIB) approach [10], Inverse Propensity-Scoring (IPS) technique [6], Doubly Robust (DR) method [11], and so forth.

However, in almost all debiased methods, the existence of propensities results in the high variance and generalization bound. [11; 12]. Therefore, various methods [5; 8; 12] have been developed to reduce estimation variances and improve the model stability. Even so, they still suffer from unbounded variances and generalization bounds when the propensity tends to zero. For the high variance and generalization bound caused by small propensities, some approaches compromise to self-normalized technique [12; 13] at the expense of unbiasedness. In addition, the overwhelming majority of previous works focus on the specific designs of the estimators or regularizers to reduce variance or eliminate bias while neglecting both the bias-variance relationship of estimators and the essence of the estimator designs.

In this paper, we reveal limitations of general regularization techniques. We find that it is impossible to reduce variance without sacrificing unbiasedness by introducing regularizers, and that regularization cannot guarantee estimators to have bounded variance and generalization bound. Besides, for general estimators, unbiasedness will inevitably result in unbounded variance and generalization bound. To some extent, the generalization bound can reflect the predictive performance of an estimator. Therefore, reducing and bounding the generalization bound can assist in improving the predictive performance of models. Since the generalization bounds of estimators contain the bias and variance terms, the essence of estimator design is not merely about eliminating bias, reducing variance, or simply achieving a bias-variance trade-off but about the quantitative joint optimization of bias and variance. Then, we develop a systematic dynamic learning framework to achieve this objective. To the best of our knowledge, this is the first work to systematically reveal limitations of general regularizers and the design perspective of the quantitative bias-variance joint optimization. Our main contributions can be summarized as follows:

1. We theoretically elaborate limitations of regularization techniques, and the relationship of unbiasedness, variance and generalization bound of general estimators.
2. Based on the general laws, we elaborate a novel design perspective for the estimator, namely the quantitative bias-variance joint optimization;
3. We develop a comprehensive dynamic learning framework with the bounded variances and generalization error to optimize a weighted objective with respect to bias and variance for each user-item pair \((u,i)\), which dynamically selects different estimators for different user-item pair from a family of estimators according to the given objective function;
4. We conduct extensive experiments to verify the theoretical results and the performance of the dynamic regularizer and estimators.

## 2 Preliminaries

Data missing not at random.Denote the sets of users and items as \(\mathcal{U}=\{u_{1},u_{2},\ldots,u_{M}\}\) and \(\mathcal{I}=\{i_{1},i_{2},\ldots,i_{N}\}\), respectively. The set of all user-item pairs is denoted as \(\mathcal{D}=\mathcal{U}\times\mathcal{I}\). Define the true and prediction matrices as \(Y\in\mathbb{R}^{M\times N}\) and \(\hat{Y}\in\mathbb{R}^{M\times N}\), where prediction tasks include rating, CTR and CVR predictions, and so forth. Each element \(y_{u,i}\) in \(Y\) and each entry \(\hat{y}_{u,i}\) in \(\hat{Y}\) are the true label and predicted output of a user \(u\) to an item \(i\). In general, it is impossible to observe all entries in the matrix \(Y\). The indicator entry of revealed elements is defined as \(o_{u,i}\in\{0,1\}\). If the true label \(y_{u,i}\) is revealed, the indicator entry of \((u,i)\) satisfies \(o_{u,i}=1\). If an entry in \(Y\) is missing, then \(o_{u,i}=0\). The corresponding indicator set is denoted as \(\mathcal{O}=\{o_{u,i}=1\}\). Considering the case that no entries are missing, the prediction inaccuracy [11] of \(\hat{Y}\) is defined as

\[L_{\text{real}}(\hat{Y},Y)=\frac{1}{MN}\sum_{u=1}^{M}\sum_{i=1}^{N}e_{u,i}= \frac{1}{|\mathcal{D}|}\sum_{(u,i)\in\mathcal{D}}e_{u,i},\]

where \(e_{u,i}\) is the prediction error. \(e_{u,i}\) can be selected as mean absolute error (MAE), mean square error (MSE) or other measures. The objective of prediction problems is to minimize the prediction inaccuracy \(L_{\text{real}}(\hat{Y},Y)\)[5; 7; 8; 11; 12; 14]. Actually, only the observed label set \(Y^{o}\) can be used to establish the prediction model. The naive approach uses \(Y^{o}\) to minimize the following prediction inaccuracy:

\[L_{\text{naive}}(\hat{Y},Y^{o})=\frac{1}{|\mathcal{O}|}\sum_{(u,i)\in \mathcal{O}}e_{u,i}=\frac{1}{|\mathcal{O}|}\sum_{(u,i)\in\mathcal{D}}o_{u,i}e_ {u,i}.\]As mentioned in [11], if the probability of every entry \(y_{u,i}\) in \(Y\) being missing is identical, then the naive estimator is unbiased, that is \(\mathbb{E}_{O}[L_{\text{naive}}]=L_{\text{real}}\), where \(O\) is taken to represent the random variable of observation. The unbiased estimation property of the naive approach is no longer valid when the data is MNAR, which even results in a large difference between \(L_{\text{real}}\) and \(\mathbb{E}_{O}[L_{\text{naive}}]\).

Quantitative Bias-Variance Joint Optimization.Considering the large difference between \(L_{\text{real}}\) and \(\mathbb{E}_{O}[L_{\text{naive}}]\), various unbiased estimation methods have been developed to overcome this problem, such as EIB [10], IPS estimator [6], DR method [11], and various variations of them [5; 7; 8; 12; 13; 15]. The corresponding estimators are given as follows:

\[L_{\text{EIB}}(\hat{Y},Y^{o})= \frac{1}{|\mathcal{D}|}\sum_{(u,i)\in\mathcal{D}}[o_{u,i}e_{u,i}+ (1-o_{u,i})\hat{e}_{u,i}],\] \[L_{\text{IPS}}(\hat{Y},Y^{o})= \frac{1}{|\mathcal{D}|}\sum_{(u,i)\in\mathcal{D}}\frac{o_{u,i}}{ \hat{p}_{u,i}}e_{u,i},\] \[L_{\text{DR}}(\hat{Y},Y^{o})= \frac{1}{|\mathcal{D}|}\sum_{(u,i)\in\mathcal{D}}\Big{[}\hat{e}_{ u,i}+\frac{o_{u,i}}{\hat{p}_{u,i}}(e_{u,i}-\hat{e}_{u,i})\Big{]},\]

where \(\hat{e}_{u,i}=w|\hat{y}_{u,i}-\gamma|\) for MAE or \(\hat{e}_{u,i}=w(\hat{y}_{u,i}-\gamma)^{2}\) for MSE of missing entries \(y_{u,i}\) is the imputed errors, and \(\hat{p}_{u,i}\in(0,1)\) is the estimation of the observation propensity, i.e., \(p_{u,i}=\text{Pr}(o_{u,i}=1)\in(0,1)\). Note that \(w\) and \(\gamma\) are hyper-parameters [10]. For the naive, EIB, IPS, and DR estimators, their biases, variances and generalization bounds are summarized in Table 4 (see Appendix A for more details), where \(\Delta_{u,i}=1-\frac{p_{u,i}}{\hat{p}_{u,i}}\) and \(\delta_{u,i}=e_{u,i}-\hat{e}_{u,i}\). In general, the learning of the imputation model also involves the MNAR problem. Some joint learning algorithms [11; 12] employ the propensity model to overcome this problem. Therefore, propensity estimation has a crucial role in unbiasedness and robustness. Besides, it is difficult to accurately estimate imputed errors for all user-item pair \((u,i)\) in the sense that it is difficult to achieve the unbiasedness of the EIB estimator. If the propensity estimation \(\hat{p}_{u,i}\) is accurate, that is \(\hat{p}_{u,i}=p_{u,i}\), then IPS and DR estimators are unbiased. For a new dataset, we cannot know in advance the range of the propensities in this dataset. Therefore, a new dataset may introduce extremely small propensities to lead to unbounded variances of IPS and DR, which will disrupt the stability of estimators, especially for larger datasets. It is unacceptable for real industrial scenarios. Specifically, the smaller the propensity, the larger the variance. When the propensity tends to zero, the variance tends to infinity (see Appendix B for more details). Similarly, variances of other IPS-based and DR-based unbiased estimation methods [15] are also unbounded. On the other hand, although the variances of naive and EIB methods are bounded when the prediction error \(e_{u,i}\) is bounded, it is difficult and even impossible to achieve an unbiased estimation. Other variance reduction estimation methods [5; 7; 8; 12] are generally biased. According to the expressions of estimators and Table 4, the bias and variance of an estimation are determined by the random variable \(O\). We found that slightly relaxing the requirements for unbiasedness will lead to a bounded variance for all propensities. Therefore, the core problem of estimation on MNAR data is the bias-variance joint optimization.

## 3 Fine-Grained Dynamic Framework for Quantitative Bias-Variance Joint Optimization

In this section, we first discuss limitations of regularization techniques and the relationship between unbiasedness of the generalized estimator and its generalization bound, which illustrate the core of the fine-grained estimator design. Then, the dynamic estimation framework for quantitative bias-variance optimization is present. Its generalization bounds and variances are reduced and bounded with theoretical guarantees.

### Limitations of Regularization Techniques

Define the general form of the estimator with regularization as

\[L_{\text{EIR+Reg}}=\underbrace{\frac{1}{|\mathcal{D}|}\sum_{(u,i)\in\mathcal{D }}\Big{[}f(o_{u,i},\hat{p}_{u,i})e_{u,i}+g(o_{u,i},\hat{p}_{u,i})\hat{e}_{u,i} \Big{]}}_{L_{\text{EIR}}}+\lambda\underbrace{\frac{1}{|\mathcal{D}|}\sum_{(u,i)\in\mathcal{D}}h(o_{u,i},\hat{p}_{u,i})}_{L_{\text{Reg}}},\] (1)where \(f(\cdot,\cdot)\neq 0\) with \(f(0,\hat{p}_{u,i})=0\), \(g(\cdot,\cdot)\), and \(h(\cdot,\cdot)\) are functions with respect to \(o\) and \(\hat{p}\). \(L_{\text{Est}}\) and \(L_{\text{Reg}}\) are prediction inaccuracies of the estimator and regularizer, respectively. For all \((u,i)\) pairs, they satisfy \(f(o_{u,i},\hat{p}_{u,i})e_{u,i}+g(o_{u,i},\hat{p}_{u,i})\hat{e}_{u,i}\geq 0\) and \(h(o_{u,i},\hat{p}_{u,i})\geq 0\). \(\lambda>0\) is a scalar weight. The generalized estimator form \(L_{\text{Est}}\) given in Eq. (1) covers the vast majority of existing estimators involving EIB [10], IPS [6], DR [11], More Robust DR (MRDR) [5], Targeted DR (TDR) [15], MIS [16], IPS/DR-SV [16], and other IPS-based and DR-based methods. On the other hand, almost all existing regularization designs, including the Sample Variance (SV) [16], mean inverse square (MIS) [16], Balancing-Mean-Square Error (BMSE) [8], and so forth, can be transformed into the form \(L_{\text{Reg}}\) given in (1). In previous works, the regularization technique plays a critical role in variance reduction of estimators and improvement of the generalization performance to a certain extent. However, it still have some inevitable limitations described in the following box.

_Core Results_

1. _For the general estimator with regularization_ \(L_{\text{Est+Reg}}\)_, it is impossible to reduce variance without sacrificing unbiasedness._
2. _Regularization_ \(L_{\text{Reg}}\) _cannot guarantee a bounded variance and generalization bound._

In what follows, we provide a detailed theoretical analysis to reveal the aforementioned limitations of the regularization technique. Considering the variance of \(L_{\text{Est+Reg}}\), we have

\[\mathbb{V}_{O}[L_{\text{Est+Reg}}]=\mathbb{V}_{O}[L_{\text{Est}}]+2\lambda \text{Cov}(L_{\text{Est}},L_{\text{Reg}})+\lambda^{2}\mathbb{V}_{O}[L_{\text{ Reg}}].\]

As mentioned in [8], when the parameter \(\lambda\) is set as the optimal parameter \(\lambda_{\text{opt}}=-\frac{\text{Cov}(L_{\text{Est}},L_{\text{Reg}})}{ \mathbb{V}_{O}[L_{\text{Est}}]}\), the variance \(\mathbb{V}_{O}[L_{\text{Est+Reg}}]\) achieves its minimum and satisfies \(\mathbb{V}_{O}[L_{\text{Est+Reg}}]\leq\mathbb{V}_{O}[L_{\text{Est}}]\) in the sense that the regularization term \(\lambda L_{\text{Reg}}\) enables the estimator \(L_{\text{Est+Reg}}\) to reduce its variance. However, the covariance \(\text{Cov}(L_{\text{Est}},L_{\text{Reg}})\) needs to fulfill \(\text{Cov}(L_{\text{Est}},L_{\text{Reg}})<0\) as \(\lambda>0\). Otherwise, an inappropriate parameter will result in an increased variance. The formal theoretical results are provided by Theorems 3.1 and Corollary 3.2, which reveal the limitation 1) (see Appendix D for proofs). Corollary 3.2 is the contrapositive of Theorems 3.1.

**Theorem 3.1**.: _Let \(L_{\text{Est+Reg}}\) be defined in (1) and the estimator \(L_{\text{Est}}\) be unbiased. If \(L_{\text{Est+Reg}}\) is unbiased, then the variance of \(L_{\text{Est+Reg}}\) is greater than the variance of the original estimator \(L_{\text{Est}}\)._

**Corollary 3.2**.: _If the variance of \(L_{\text{Est+Reg}}\) is less than the variance of the original estimator \(L_{\text{Est}}\), then \(L_{\text{Est+Reg}}\) is not unbiased._

We further find that, if the variance of the original estimator is unbounded, the variances of estimators cannot be bounded by introducing a regularizer even if \(\hat{p}_{u,i}=p_{u,i}\). The theoretical results are shown in Theorem 3.3(see Appendix D for proofs).

**Theorem 3.3**.: _Let the bias of \(L_{\text{Est+Reg}}\) be bounded and the variance of \(L_{\text{Est}}\) satisfy \(\lim_{p_{u,i}\to 0}\mathbb{V}_{O}[L_{\text{Est}}]\hat{p}_{u,i}=p_{u,i}]=\infty\). Then, there exists no regularizer \(L_{\text{Reg}}\) that enables the variance and generalization bound of the estimator bounded even the learned imputed errors or propensities are accurate._

According to the previous works and the present Theorem 3.3, regularizers enable variance reduction to a certain extent while cannot enable estimators to possess bounded variances and generalization bounds. In other words, regularization techniques have limited impact on improving the predictive performance of the model. In the next subsection, a novel perspective of dynamic estimator designs is proposed, which not only achieves quantitative bias-variance joint optimization but also guarantees bounded variances and generalization bounds.

### Dynamic Estimator Designs With Quantitative Optimization

Most of the existing estimators are based on IPS and DR methods, which are elaborately designed to reduce bias or variance. However, all these estimators are static estimators in the sense that they cannot achieve bias-variance joint optimization for each user-item pair \((u,i)\). Even though some methods [5; 8] can effectively reduce the variance of estimators, the estimators are biased and the corresponding variances are unbounded. In this subsection, the core results are provided in the following box. Also, based on theses results, we develop a fine-grained dynamic framework with quantitative optimization to guarantee the reduction and boundedness of variances and generalization bounds

_Core Results_

* For the generalized estimator \(L_{\text{Est}}\), unbiasedness of the estimator will inevitably lead to the unbounded variance and generalization bound.
* The core of the estimator design involves not merely a simple bias-variance trade-off, but rather a quantitative joint optimization of both bias and variance.

We find that the unbiased estimators with general form generally possess unbounded variances, which is formally derived in Theorem 3.4. Its proofs are provided in Appendix D.

**Theorem 3.4** (Limitation of Static Estimator).: _Given prediction errors \(e_{u,i}\), imputed errors \(\hat{e}_{u,i}\), and learned propensities \(\hat{p}_{u,i}\) for all user-item pairs \((u,i)\), if for any \(e_{u,i}-g(0,\hat{p}_{u,i})\hat{e}_{u,i}\neq 0\), \(L_{\text{Est}}\) given in (1) is unbiased, then the corresponding variance and generalization bound are unbounded._

According to Theorem 3.4, the core objective of estimators is not merely about eliminating bias, reducing variance, or simply achieving a bias-variance trade-off but about a quantitative joint optimization between bias and variance. Therefore, as mentioned in _Core Results_, it is necessary to develop a dynamic estimation framework to achieve the quantitative joint optimization.

Design Principle of Dynamic Estimators.The IPS-based and DR-based dynamic learning frameworks are designed as

\[L_{\text{D-IPS}}=\frac{1}{|\mathcal{D}|}\sum_{(u,i)\in\mathcal{D}}\frac{o_{u, i}}{f^{\alpha_{u,i}}(\hat{p}_{u,i})}e_{u,i},\ L_{\text{D-DR}}=\frac{1}{|\mathcal{D}|}\sum_{(u,i)\in \mathcal{D}}\bigg{(}\hat{e}_{u,i}+\frac{o_{u,i}}{f^{\alpha_{u,i}}(\hat{p}_{u,i })}\delta_{u,i}\bigg{)},\] (2)

where \(f(\cdot)\) is a designed function and \(\alpha_{u,i}\in[0,1]\) is optimizable parameters. When \(f(\hat{p}_{u,i})=\hat{p}_{u,i}\) and \(\alpha_{u,i}=1\), D-IPS and D-DR are equivalent to the original IPS and DR estimators, respectively, which possess unbiasedness. When \(f(\hat{p}_{u,i})=\hat{p}_{u,i}\) and \(\forall\alpha_{u,i}=0\), D-IPS and D-DR are equivalent to \(\frac{|\mathcal{O}|}{|\mathcal{D}|}L_{\text{naive}}\) and EIB estimators, which have bounded variances and generalization bounds. The function \(f(\hat{p}_{u,i})\) in (2) is actually a mapping, which balances the bias and variance of estimators. The design principles of \(f(\hat{p}_{u,i})\) are provided as follows:

* **(Isotonic Propensity)**\(f(\hat{p}_{u,i})\) with \(f(0)=0\), \(f(1)=1\), and \(f(\hat{p}_{u,i})>\hat{p}_{u,i}\) is a monotonically increasing function.
* **(Same Order)**\(\lim_{\hat{p}_{u,i}\to 0}\frac{\hat{p}_{u,i}}{f(\hat{p}_{u,i})}=C\), where \(C\) is a positive constant.

Some specific expressions of \(f(\hat{p}_{u,i})\) fulfilling the above design principles are summarized in Table 1. The corresponding biases, variances and tail bounds of D-IPS and D-DR estimators are formally formulated in Lemmas D.1-D.4 given in Appendix D. From the biases and variances of the D-IPS and D-DR methods given as

\[\text{Bias}(L_{\text{D-IPS}})=\frac{1}{|\mathcal{D}|}\bigg{|} \sum_{(u,i)\in\mathcal{D}}h_{V}^{\text{Est}}(\hat{p}_{u,i},p_{u,i},\alpha_{u,i} )e_{u,i}\bigg{|},\ \text{Bias}(L_{\text{D-DR}})=\frac{1}{|\mathcal{D}|}\bigg{|}\sum_{(u,i)\in \mathcal{D}}h_{B}^{\text{Est}}(\hat{p}_{u,i},p_{u,i},\alpha_{u,i})\delta_{u,i} \bigg{|},\] \[\mathbb{V}_{O}[L_{\text{D-IPS}}]=\frac{1}{|\mathcal{D}|^{2}}\sum_ {(u,i)\in\mathcal{D}}h_{V}^{\text{Est}}(\hat{p}_{u,i},p_{u,i},\alpha_{u,i})e_ {u,i}^{2},\ \mathbb{V}_{O}[L_{\text{D-DR}}]=\frac{1}{|\mathcal{D}|^{2}}\sum_ {(u,i)\in\mathcal{D}}h_{V}^{\text{Est}}(\hat{p}_{u,i},p_{u,i},\alpha_{u,i}) \delta_{u,i}^{2},\]

Figure 1: The surfaces of determining factors and the objective function of dynamic estimators, and the optimal objective values: (a) \(h_{B}^{\text{Est}}\); (b) \(h_{V}^{\text{Est}}\); (c) \(w_{1}(h_{B}^{\text{Est}})^{2}+w_{2}h_{V}^{\text{Est}}\); (d) Objective\({}_{\text{opt}}\).

where \(h_{B}^{\text{Est}}(\hat{p}_{u,i},p_{u,i},\alpha_{u,i})=1-\frac{p_{u,i}}{f^{w_{u,i} }(\hat{p}_{u,i})}\) and \(h_{V}^{\text{Est}}(\hat{p}_{u,i},p_{u,i},\alpha_{u,i})=\frac{p_{u,i}(1-p_{u,i})} {f^{2w_{u,i}}(\hat{p}_{u,i})}\), functions \(h_{B}^{\text{Est}}\) and \(h_{V}^{\text{Est}}\) determine the biases and variance, respectively. \(h_{B}^{\text{Est}}\) and \(h_{V}^{\text{Est}}\) corresponding the specific expressions of \(f(\hat{p}_{u,i})\) are given in Table 1. The monotonicity of bias and variance are provided in Appendix D Proposition D.3. The surfaces of \(h_{B}^{\text{Est}}\) and \(h_{V}^{\text{Est}}\) are plotted in Figs. 1(a) and (b). It is observed that \(h_{B}^{\text{Est}}\) is monotonically decreasing and \(h_{V}^{\text{Est}}\) is monotonically increasing as the number of \(\alpha_{u,i}\) increases.

Bias-Variance Quantitative Joint Optimization.According to Proposition D.3 given in Appendix D, the bias-variance trade-off problem can be quantitatively formalized as the following joint optimization problem:

\[\text{Objective}=\min_{\alpha_{u,i}}\Big{\{}w_{1}\text{Bias}(L( \alpha_{u,i}))+w_{2}\mathbb{V}_{O}[L(\alpha_{u,i})]\Big{\}},\text{ s.t. }0\leq\alpha_{u,i}\leq 1,\] (3)

where \(w_{1}\) and \(w_{2}\) are weights of the bias and variance.

According to determine factors \(h_{B}^{\text{Est}}\) and \(h_{V}^{\text{Est}}\) of bias and variance, respectively, the bias-variance joint optimization problem can be defined as

\[\text{Objective}^{\text{opt}}=\min_{\alpha_{u,i}}\Big{\{}w_{1}E_{ B}(h_{B}^{\text{Est}}(\alpha_{u,i}))+w_{2}E_{V}(h_{V}^{\text{Est}}(\alpha_{u,i})) \Big{\}},\text{ s.t. }0\leq\alpha_{u,i}\leq 1.\] (4)

For each user-item pair \((u,i)\), minimizing \(E_{B}(h_{B}^{\text{Est}})\) and \(E_{V}(h_{V}^{\text{Est}})\) given accurate propensity estimations \(\hat{p}_{u,i}\) leads to the bias and variance reduction, respectively. Therefore, the optimal parameter \(\alpha_{u,i}\in[0,1]\) for each user-item pair \((u,i)\) can achieve fine-grained bias-variance joint optimization. The function \(h_{B}^{\text{Est}}\) under \(\alpha_{u,i}\in[0,1]\), \(f(\hat{p}_{u,i})\geq\hat{p}_{u,i}\) and \(\hat{p}_{u,i}=p_{u,i}\) satisfies \(h_{B}^{\text{Est}}(\hat{p}_{u,i},p_{u,i},\alpha_{u,i})\in[0,1)\). On the other hand, \(h_{V}^{\text{Est}}\) under \(\alpha_{u,i}\in[0,1]\) and \(\hat{p}_{u,i}=p_{u,i}\) satisfies \(h_{V}^{\text{Est}}(\hat{p}_{u,i},p_{u,i},\alpha_{u,i})\in[0,\infty)\). Therefore, the objective function in (4) can be simplified as \(w_{1}h_{B}^{\text{Est}}(\alpha_{u,i})+w_{2}h_{V}^{\text{Est}}(\alpha_{u,i})\). The curves of objective functions under different designed functions \(f(\cdot)\) are given in Fig. 1(c). It can be observed that for a fixed propensity, there exists an \(\alpha\) such that the objective function attains the minimum value. Besides, different measure metrics are also applicable for dynamic estimators, such as \(E(h^{\text{Est}})=(h^{\text{Est}}(\alpha_{u,i}))^{2}\), \(E(h^{\text{Est}}(\alpha_{u,i}))=\ln(\cosh(h^{\text{Est}}(\alpha_{u,i})))\), and so on. In what follows, under the objective function \(w_{1}h_{B}^{\text{Est}}+w_{2}h_{V}^{\text{Est}}\), the analytical solution of the optimal parameter \(\alpha_{u,i}^{\text{opt}}\) is given in Theorem 3.5 (see Appendix D for proofs).

**Theorem 3.5** (The optimal parameter \(\alpha_{u,i}^{\text{opt}}\)).: _Let the learned propensities be accurate, i.e., \(\hat{p}_{u,i}=p_{u,i}\). For weights \(w_{1}\) and \(w_{2}\), the objective function \(w_{1}h_{B}^{\text{Est}}+w_{2}h_{V}^{\text{Est}}\) under \(\alpha_{u,i}\in[0,1]\) achieves its minimum at_

\[\alpha_{u,i}^{\text{opt}}=\min\Big{\{}\max\Big{\{}\frac{\ln\Big{(} \frac{2w_{2}}{w_{1}}(1-p_{u,i})\Big{)}}{\ln(f(p_{u,i}))},0\Big{\}},1\Big{\}}.\] (5)

From the expression of the optimal parameter (5), the optimal solution of (4) under different weights depends on the weight ratio \(w_{2}/w_{1}\). Under different designed function \(f(\cdot)\), the schematic diagram of optimal objective values corresponding to the optimal parameter \(\alpha_{u,i}^{\text{opt}}\) is shown in Fig. 1(d). Next, the generalization bounds of the developed dynamic estimator framework are further discussed. The formalized results are derived in Theorem 3.6 (see Appendix D for more details).

**Theorem 3.6** (Generalization Bounds of D-IPS and D-DR).: _For any finite hypothesis space \(\mathcal{H}\) of \(\hat{Y}\) and the optimal prediction matrix \(\hat{Y}^{-}\), given \(\hat{e}_{u,i}\) and \(\hat{p}_{u,i}\) for all \((u,i)\in\mathcal{D}\), with probability \(1-\rho\), the prediction inaccuracies \(L_{\text{D-IPS}}(\hat{Y}^{-},Y)\) and \(L_{\text{D-DR}}(\hat{Y}^{-},Y)\) under D-IPS and D-DR have the following upper bounds_

\[L_{\text{D-D}\text{PS}}(\hat{Y}^{-},Y^{O})+\underbrace{\sum_{(u,i) \in\mathcal{D}}\frac{|h_{B}^{Eq}(\hat{p}_{u,i},p_{u,i},\alpha_{u,i})e_{u,i}^{-} |}{|\mathcal{D}|}}_{\text{Bias~{}Term}}+\underbrace{h_{G}^{Est}(e_{u,i}^{+})}_{ \text{Variance~{}Term}}\] \[L_{\text{D-D}\text{R}}(\hat{Y}^{-},Y^{O})+\underbrace{\sum_{(u,i )\in\mathcal{D}}\frac{|h_{B}^{Eq}(\hat{p}_{u,i},p_{u,i},\alpha_{u,i})\delta_{u,i}^{-}|}{|\mathcal{D}|}}_{\text{Bias~{}Term}}+\underbrace{h_{G}^{Est}(\delta_{ u,i}^{+})}_{\text{Variance~{}Term}}\]

_where \(e_{u,i}^{+}\) and \(\delta_{u,i}^{+}\) are the error and error deviation corresponding to \(\hat{Y}^{+}=\arg\max_{\hat{Y}\in\mathcal{H}}\Big{\{}\sum_{(u,i)\in\mathcal{D }}\left(\frac{e_{u,i}}{f^{\alpha_{u,i}}(\hat{p}_{u,i})}\right)^{2}\Big{\}}\) and \(\hat{Y}^{+}=\arg\max_{\hat{Y}\in\mathcal{H}}\Big{\{}\sum_{(u,i)\in\mathcal{D }}\left(\frac{\delta_{u,i}}{f^{\alpha_{u,i}}(\hat{p}_{u,i})}\right)^{2}\Big{\}}\), respectively, and the function \(h_{G}^{Est}\) is formulated as \(h_{G}^{Est}(z_{u,i}^{+})=\sqrt{\frac{\log(\frac{2|\mathcal{H}|}{\rho})}{2| \mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D}}\left(\frac{z_{u,i}^{+}}{f^{\alpha _{u,i}}(\hat{p}_{u,i})}\right)^{2}}\) From Theorem 3.6, the bias-variance joint optimization is actually to minimize generalization bounds, which include both the bias term and the variance term. Besides, the dynamic estimators with the optimal parameter \(\alpha_{u,i}^{\text{opt}}\) make variances and generalization bounds bounded. The formal result is given in Theorem 3.7 (The corresponding proofs and bounds of variances are given in Appendix D).

**Theorem 3.7** (Boundedness of Variance and Generalization Bound).: _Let \(\alpha_{u,i}^{opt}\in[0,1]\) be the optimal parameter of (4). If the dynamic estimators adopt \(\alpha_{u,i}^{opt}\) as the parameter, then the corresponding variance and generalization bounds are bounded._

## 4 Experiments

In this section, we conduct extensive experiments to compare the performance of the present dynamic learning framework with existing SOTA approaches and to answer the following questions: **Q1**: Does the developed dynamic learning framework improve the prediction performance compared with the SOTA approaches? **Q2**: Do the present dynamic estimator designs reduce the variance and make performance more stable compared with the SOTA approaches? **Q3**: How do the performance and variance of the proposed method change under different optimization weights and estimator functions?

### Experimental Setup

Dataset and Preprocessing.Three real-world datasets with MNAR and MAR samples are used to conduct the experiments, namely Coat with 4,640 MAR and 6,960 MNAR ratings of 290 users to 300 coats, Yahoo! R3 with 54,000 MAR and 311,704 MNAR ratings of 15,400 users to 1,000 songs, and KualRec with 4,676,570 video watching ratio records of 1,411 users to 3,327 video. Similar to literature [8; 7; 5], the rating scores in Coat and Yahoo! R3 are binarized as 1 when it is greater than three, otherwise as 0. For the KualRec dataset, the video watching ratios are binarized as 0 when it is less than two, otherwise as 1.

Baselines and Experimental Details.To avoid the uncertainty caused by the prediction and observe the performance of the prediction model, we take the matrix factorization (MF) [17] as the base model and compare the present dynamic learning framework with the following representative IPS-based and DR-based approaches: naive **MF**[17], **IPS**[6], **SNIPS**[13], **IPS-AT**[18], **CVIB**[19], **IPS-V2**[8], **DR**[11], **DR-JL**[11], **MRDR-JL**[5], **Stable DR**[12], **Stable MRDR**[12], **TDR-CL**[15], **TMRDR-CL**[15], **TMRDR-CL**[15], **DR-V2**[8]. Here, we adopt the two common metrics used in recommender system, i.e., area under the ROC curve (AUC), and normalized discounted cumulative gain (NDCG), to evaluate the performance of prediction models. To guarantee the fair comparison, we set the same parameters for all approaches. The learning rates are tuned in \(\{0.001,0.005,0.01,0.05\}\) and weight decay is tuned in \(\{1,1e-1,1e-2,1e-3,1e-4,1e-5,1e-6\}\). Note that, for _XX_ and _D-XX_ approaches, their model structures and parameters are identical. Every approach is preformed 10 times to record its mean and standard deviation.

[MISSING_PAGE_FAIL:8]

bounds and then further improve the generalization performance. Meanwhile, we find that the variances of dynamic estimators is not decreasing when the weight ratio increases. This because, for different ratios, the global minimum of the objective function (4) cannot be reached within the interval \(\alpha\in[0,1]\). For SNIPS, the property of variance reduction might lead to the non-obvious performance and variance trends.

Under the identical weight ratio \(\frac{w_{2}}{w_{1}}=0.1\), we further discuss the effects of different functions \(f^{\alpha}(\hat{p}_{u,i})\) on the prediction performance and variance. The experimental results are shown in Fig. 3. Nearly all dynamic estimators with different function expressions outperform the corresponding debiased approaches given in Table 3. It further demonstrates that the proposed dynamic learning mechanism can greatly improve the performance of the original estimator. Besides, the prediction performance of the dynamic estimator with \(f^{\alpha}(\hat{p}_{u,i})=\left(\frac{\log(\hat{p}_{u,i}+1)}{\log(2)}\right)^{\alpha}\) outperforms other dynamic estimators.

## 5 Related Work

Aiming at the prediction model bias caused by the MNAR data, EIB [10] and IPS [6] approaches are two classical unbiased estimators. To leverage the advantages of EIB and IPS, the DR method [11] was designed to make the unbiasedness of estimator doubly robust. Focusing on the unbiasedness of estimators, various estimation methods have been proposed to overcome mixed or even unknown biases in the data [9], to solve the sample selection bias problem in the multi-task learning [20; 21; 22; 23], to improve the performance of the propensity model by different approaches [18; 14], and so forth. A multiple robust estimator is developed in [24] by taking the advantage of multiple candidate imputation and propensity models, which is unbiased when any of the imputation or propensity models, or a linear combination of these models is accurate. From a novel function balancing perspective, Li et al. propose to approximate the balancing functions in reproducing kernel Hilbert space [25]. Moreover, aimed at limitations of miscalibrated imputation and propensity models, Kweon and Yu [26] propose a doubly calibrated estimator and a tri-level joint learning framework to simultaneously optimize calibration experts alongside prediction and imputation models. For the variance of estimators, an increasing body of works have emerged to reduce the variance. The most common estimator reducing variance is Self-Normalized IPS (SNIPS) [13]. Based on DR, literature [5] designed a MRDR estimator to reduce the variance of the DR estimator by the present variance expression of DR. In [15], TDR estimator is elaborated to reduce the bias and variance of DR simultaneously by the present semi-parametric collaborative learning. Moreover, stable DR estimator [12] achieves the bounded bias, variance, and generalization error bound simultaneously for arbitrarily small propensities by combining SNIPS and DR methods. Various regularization designs, such as SV [16], MIS [16], BMSE [8], and so forth, are also introduced into the estimator to achieve variance reduction.

\begin{table}
\begin{tabular}{l|c c c|c c c} \hline \hline \(f^{\alpha}(\hat{p}_{u,i})\) & \multicolumn{4}{c|}{\(\hat{p}_{u,i}^{\alpha}\)} & \multicolumn{4}{c}{\(\frac{\left(\sin(\hat{p}_{u,i})\right)^{\alpha}}{\sin(1)}\)} \\ \hline Methods & AUC & Gain\({}_{\text{AUC}}\) & NDCG@5 & Gain & AUC & NDCG@5 & Gain \\ \hline D-IPS & 0.7702\(\pm\)**0.0011** & 2.16\% & 0.6362\(\pm\)**0.0043** & -2.06\% & 0.7753\(\pm\)0.0017 & 2.84\% & 0.6475\(\pm\)0.0043 & -0.32\% \\ D-SNIPS & 0.7413\(\pm\)0.0045 & -0.13\% & **0.6146\(\pm\)**0.0079 & 0.59\% & 0.7392\(\pm\)0.0038 & -0.42\% & 0.6109\(\pm\)0.0089 & -0.02\% \\ D-IPS-AT & 0.7711\(\pm\)0.0016 & 0.25\% & 0.6360\(\pm\)0.0051 & 11.17\% & 0.7710\(\pm\)0.0022 & 0.23\% & 0.6346\(\pm\)0.0036 & 0.89\% \\ D-DR & 0.7710\(\pm\)**0.0014** & 2.28\% & 0.6384\(\pm\)**0.0047** & -0.6476\(\pm\)0.0021 & 2.98\% & 0.6516\(\pm\)0.0052 & 1.42\% \\ D-DR-JL & 0.7695\(\pm\)0.0013 & 1.60\% & 0.8346\(\pm\)0.0058 & -2.31\% & 0.7748\(\pm\)0.0012 & 2.30\% & 0.6444\(\pm\)0.0053 & -0.80\% \\ D-MRDR-JL & 0.7711\(\pm\)0.0016 & 1.59\% & 0.6365\(\pm\)0.0040 & -2.11\% & 0.7751\(\pm\)**0.0012** & 2.12\% & 0.6470\(\pm\)**0.038** & -0.49\% \\ \hline \hline \(f^{\alpha}(\hat{p}_{u,i})\) & \multicolumn{4}{c|}{\(\frac{\left(\log(\hat{p}_{u,i})\right)^{\alpha}}{\sin(2)}\)} \\ \hline Methods & AUC & Gain\({}_{\text{AUC}}\) & NDCG@5 & Gain & AUC & Gain\({}_{\text{AUC}}\) & NDCG@5 & Gain\({}_{\text{AUC}}\) \\ \hline D-IPS & **0.7777\(\pm\)**0.0015 & 3.16\% & **0.6584\(\pm\)**0.0049 & 1.35\% & 0.7771\(\pm\)0.0016 & 3.08\% & 0.6578\(\pm\)0.0048 & 1.26\% \\ D-SNIPS & **0.7429\(\pm\)**0.0036** & 0.08\% & 0.6096\(\pm\)**0.0062** & **-0.23\%** & 0.7418\(\pm\)0.0070 & -0.07\% & 0.6115\(\pm\)0.0082 & 0.08\% \\ D-IPS-AT & 0.7705\(\pm\)0.0012 & 0.17\% & **0.6367\(\pm\)**0.0052 & 1.22\% & **0.7718\(\pm\)0.0011** & 0.34\% & 0.6357\(\pm\)**0.0029** & 1.07\% \\ D-DR & **0.7804\(\pm\)**0.0023 & 3.53\% & **0.6671\(\pm\)**0.0051 & 3.83\% & 0.7792\(\pm\)0.0019 & 3.37\% & 0.6680\(\pm\)0.0053 & 2.85\% \\ D-DR-JL & 0.7757\(\pm\)0.0016 & 2.65\% & **0.6577\(\pm\)**0.0036** & 1.25\% & **0.7782\(\pm\)**0.0011** & 2.75\% & 0.6537\(\pm\)0.0039 & 0.63\% \\ D-MRDR-JL & **0.7786\(\pm\)**0.0025 & 2.58\% & **0.6616\(\pm\)**0.00044 & 1.75\% & 0.7779\(\pm\)0.0017 & 2.49\% & 0.6576\(\pm\)0.0071 & 1.14\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: Effects of different functions on performances of dynamic estimators.

Conclusions

To the best of our knowledge, this is the first work to reveal that the essence of estimator designs is not merely to eliminate bias, to reduce variance, or to achieve a simple bias-variance trade-off but to quantitatively and simultaneously optimize bias and variance. Besides, the limitations of general regularization techniques and general static estimators are presented. Based on the general laws with respect to the relationship between bias and variance, we propose a systematic dynamic learning framework, which guarantees the bounded variances and generalization bounds by the present fine-grained bias-variance joint optimization scheme. Extensive experiment results have verified the theoretical results and the performance of the present dynamic estimators. The search for optimal weights in the objective function and the functions in the dynamic estimation framework remains an open question.

## References

* [1] Bianca Zadrozny. Learning and evaluating classifiers under sample selection bias. In _Proceedings of the twenty-first international conference on Machine learning_, page 114, 2004.
* [2] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 1930-1939, 2018.
* [3] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. Entire space multi-task model: An effective approach for estimating post-click conversion rate. In _The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval_, pages 1137-1140, 2018.
* [4] Dongbo Xi, Zhen Chen, Peng Yan, Yinger Zhang, Yongchun Zhu, Fuzhen Zhuang, and Yu Chen. Modeling the sequential dependence among audience multi-step conversions with multi-task learning in targeted display advertising. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 3745-3755, 2021.
* [5] Siyuan Guo, Lixin Zou, Yiding Liu, Wenwen Ye, Suqi Cheng, Shuaiqiang Wang, Hechang Chen, Dawei Yin, and Yi Chang. Enhanced doubly robust learning for debiasing post-click conversion rate estimation. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 275-284, 2021.
* [6] Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims. Recommendations as treatments: Debiasing learning and evaluation. In _international conference on machine learning_, pages 1670-1679. PMLR, 2016.
* [7] Quanyu Dai, Haoxuan Li, Peng Wu, Zhenhua Dong, Xiao-Hua Zhou, Rui Zhang, Rui Zhang, and Jie Sun. A generalized doubly robust learning framework for debiasing post-click conversion rate prediction. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 252-262, 2022.
* [8] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu, and Peng Cui. Propensity matters: Measuring and enhancing balancing for recommendation. In _International Conference on Machine Learning_, pages 20182-20194. PMLR, 2023.
* [9] Jiawei Chen, Hande Dong, Yang Qiu, Xiangnan He, Xin Xin, Liang Chen, Guli Lin, and Keping Yang. Autodebias: Learning to debias for recommendation. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 21-30, 2021.
* [10] Harald Steck. Training and testing of recommender systems on data missing not at random. In _Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 713-722, 2010.
* [11] Xiaojie Wang, Rui Zhang, Yu Sun, and Jianzhong Qi. Doubly robust joint learning for recommendation on data missing not at random. In _International Conference on Machine Learning_, pages 6638-6647. PMLR, 2019.

* [12] Haoxuan Li, Chunyuan Zheng, Xiao-Hua Zhou, and Peng Wu. Stabilized doubly robust learning for recommendation on data missing not at random. In _Proceedings of the 11th International Conference on Learning Representations_, 2023.
* [13] Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual learning. _advances in neural information processing systems_, 28, 2015.
* [14] Wei Ma and George H Chen. Missing not at random in matrix completion: The effectiveness of estimating missingness probabilities under a low nuclear norm assumption. _Advances in neural information processing systems_, 32, 2019.
* [15] Haoxuan Li, Yan Lyu, Chunyuan Zheng, and Peng Wu. Tdr-cl: Targeted doubly robust collaborative learning for debiased recommendations. In _Proceedings of the 11th International Conference on Learning Representations_, 2023.
* [16] Xiaojie Wang, Rui Zhang, Yu Sun, and Jianzhong Qi. Combating selection biases in recommender systems with a few unbiased ratings. In _Proceedings of the 14th ACM International Conference on Web Search and Data Mining_, pages 427-435, 2021.
* [17] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. _Computer_, 42(8):30-37, 2009.
* [18] Yuta Saito. Asymmetric tri-training for debiasing missing-not-at-random explicit feedback. In _Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 309-318, 2020.
* [19] Zifeng Wang, Xi Chen, Rui Wen, Shao-Lun Huang, Ercan Kuruoglu, and Yefeng Zheng. Information theoretic counterfactual learning from missing-not-at-random feedback. _Advances in Neural Information Processing Systems_, 33:1854-1864, 2020.
* [20] Wenhao Zhang, Wentian Bao, Xiao-Yang Liu, Keping Yang, Quan Lin, Hong Wen, and Ramin Ramezani. Large-scale causal approaches to debiasing post-click conversion rate estimation with multi-task learning. In _Proceedings of The Web Conference 2020_, pages 2775-2781, 2020.
* [21] Wujiang Xu, Qitian Wu, Runzhong Wang, Mingming Ha, Qiongxu Ma, Linxun Chen, Bing Han, and Junchi Yan. Rethinking cross-domain sequential recommendation under open-world assumptions. _arXiv preprint arXiv:2311.04590_, 2023.
* [22] Wujiang Xu, Shaoshuai Li, Mingming Ha, Xiaobo Guo, Qiongxu Ma, Xiaolei Liu, Linxun Chen, and Zhenfeng Zhu. Neural node matching for multi-target cross domain recommendation. In _2023 IEEE 39th International Conference on Data Engineering (ICDE)_, pages 2154-2166. IEEE, 2023.
* [23] Wujiang Xu, Xuying Ning, Wenfang Lin, Mingming Ha, Qiongxu Ma, Linxun Chen, Bing Han, and Minnan Luo. Towards open-world cross-domain sequential recommendation: A model-agnostic contrastive denoising approach. _arXiv preprint arXiv:2311.04760_, 2023.
* [24] Haoxuan Li, Quanyu Dai, Yuru Li, Yan Lyu, Zhenhua Dong, Xiao-Hua Zhou, and Peng Wu. Multiple robust learning for recommendation. In _In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence_, 2023.
* [25] Haoxuan Li, Chunyuan Zheng, Yanghao Xiao, Peng Wu, Zhi Geng, Xu Chen, and Peng Cui. Debiased collaborative filtering with kernel-based causal balancing. In _Proceedings of the 12th International Conference on Learning Representations_, 2024.
* [26] Wonbin Kweon and Hwanjo Yu. Doubly calibrated estimator for recommendation on data missing not at random. In _In Proceedings of the ACM Web Conference 2024 (WWW '24)_, page 3810-3820, 2024.

Derivation of bias and variance for naive, EIB, IPS, and DR estimators.

As mentioned in literature [11], the bias of an estimator is defined as

\[\text{Bias}(L)=|L_{\text{real}}-\mathbb{E}_{O}[L]|\] (6)

According to the prediction inaccuracy expressions of \(L_{\text{naive}}\) and the definition of bias (6), the bias of naive estimator satisfies

\[\text{Bias}(L_{\text{naive}})=\frac{1}{|\mathcal{D}|}\bigg{|}\sum_{(u,i)\in \mathcal{D}}e_{u,i}-|\mathcal{D}|\mathbb{E}_{O}[L_{\text{naive}}]\bigg{|}= \frac{1}{|\mathcal{D}|}\bigg{|}\sum_{(u,i)\in\mathcal{D}}\bigg{(}1-\frac{| \mathcal{D}|}{|\mathcal{O}|}p_{u,i}\bigg{)}e_{u,i}\bigg{|}.\] (7)

According to the definition of variance for an estimation given in [5], the variance of the naive approach can be formulated as

\[\mathbb{V}_{O}[L_{\text{naive}}]= \mathbb{E}_{O}[L_{\text{naive}}^{2}]-\mathbb{E}_{O}^{2}[L_{\text {naive}}]\] \[= \frac{1}{|\mathcal{O}|^{2}}\mathbb{E}_{O}\bigg{[}\bigg{(}\sum_{( u,i)\in\mathcal{D}}o_{u,i}e_{u,i}\bigg{)}^{2}\bigg{]}-\frac{1}{|\mathcal{O}|^{2}} \Big{(}\sum_{(u,i)\in\mathcal{D}}p_{u,i}e_{u,i}\bigg{)}^{2}\] \[= \frac{1}{|\mathcal{O}|^{2}}\sum_{(u,i)\in\mathcal{D}}p_{u,i}(1-p_ {u,i})e_{u,i}^{2}\]

The biases of EIB, IPS and DR methods have been given in Lemma 3.1 of [11]. The variance formulations of IPS and DR estimators have been provided in [5]. For the variance of EIB, we obtain

\[\mathbb{V}_{O}[L_{\text{EB}}]= \mathbb{E}_{O}[L_{\text{EB}}^{2}]-\mathbb{E}_{O}^{2}[L_{\text{EB}}]\] \[= \frac{1}{|\mathcal{D}|^{2}}\mathbb{E}_{O}\bigg{[}\bigg{(}\sum_{(u, i)\in\mathcal{D}}[o_{u,i}e_{u,i}+(1-o_{u,i})\hat{e}_{u,i}]\bigg{)}^{2}\bigg{]}-\frac{1}{| \mathcal{D}|^{2}}\Big{(}\sum_{(u,i)\in\mathcal{D}}[p_{u,i}e_{u,i}+(1-p_{u,i}) \hat{e}_{u,i}]\bigg{)}^{2}\] \[= \frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D}}p_{u,i}(1-p_ {u,i})(e_{u,i}-\hat{e}_{u,i})^{2}.\]

Therefore, the bias and variance of naive, EIB, IPS, and DR estimators shown in Table 4 can be obtained.

## Appendix B Unbounded Variance

According to the definitions of variances of IPS and SR methods, if \(\hat{p}_{u,i}=p_{u,i}\), when the propensity tends to zero, the variance of IPS and DR satisfy

\[\lim_{p_{u,i}\to 0}\mathbb{V}_{O}[L_{\text{IPS}}|\hat{p}_{u,i}=p_{u,i}]= \lim_{p_{u,i}\to 0}\frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D}}\frac{1-p_ {u,i}}{p_{u,i}}e_{u,i}^{2}=\infty,\] \[\lim_{p_{u,i}\to 0}\mathbb{V}_{O}[L_{\text{DR}}|\hat{p}_{u,i}=p_{u,i}]= \lim_{p_{u,i}\to 0}\frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D}} \frac{1-p_{u,i}}{p_{u,i}}\delta_{u,i}^{2}=\infty.\]

This demonstrates that the variances of IPS and DR are unbounded even if the propensities are accurate. If there exists one propensity going to zero then the variance tends to infinity

## Appendix C Limitations of Regularization Techniques and Static Estimators

**Theorem 3.1**.: Let \(L_{\text{Est+Reg}}\) be defined in (1) and the estimator \(L_{\text{Est}}\) be unbiased. If \(L_{\text{Est+Reg}}\) is unbiased, then the variance of \(L_{\text{Est+Reg}}\) is greater than the one of the original estimator \(L_{\text{Est}}\).

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Model & naive & EIB & IPS & DR \\ \hline Bias & \(\frac{1}{|\mathcal{D}|}\sum\limits_{(u,i)\in\mathcal{D}}\bigg{(}1-p_{u,i} \bigg{)}e_{u,i}\bigg{|}\) & \(\frac{1}{|\mathcal{D}|}\sum\limits_{(u,i)\in\mathcal{D}}(1-p_{u,i})\delta_{u,i} \bigg{|}\) & \(\frac{1}{|\mathcal{D}|}\sum\limits_{(u,i)\in\mathcal{D}}\Delta_{u,i}e_{u,i} \bigg{|}\) & \(\frac{1}{|\mathcal{D}|}\sum\limits_{(u,i)\in\mathcal{D}}\Delta_{u,i}\delta_{u,i} \bigg{|}\) \\ Variance & \(\frac{1}{|\mathcal{D}|^{2}}\sum\limits_{(u,i)\in\mathcal{D}}p_{u,i}(1-p_{u,i} )e_{u,i}^{2}\) & \(\frac{1}{|\mathcal{D}|^{2}}\sum\limits_{(u,i)\in\mathcal{D}}p_{u,i}(1-p_{u,i} )\delta_{u,i}^{2}\) & \(\frac{1}{|\mathcal{D}|^{2}}\sum\limits_{(u,i)\in\mathcal{D}}\frac{p_{u,i}(1-p_ {u,i})\delta_{u,i}^{2}}{p_{u,i}^{2}}\) & \(\frac{1}{|\mathcal{D}|^{2}}\sum\limits_{(u,i)\in\mathcal{D}}\frac{p_{u,i}(1-p_ {u,i})\delta_{u,i}^{2}}{p_{u,i}^{2}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Bias and variance of naive, EIB, IPS, and DR estimators.

Proof.: The variance of \(L_{\text{Est+Reg}}\) satisfies

\[\mathbb{V}_{O}[L_{\text{Est+Reg}}]=\mathbb{V}_{O}[L_{\text{Est}}]+2\lambda\text{Cov }(L_{\text{Est}},L_{\text{Reg}})+\lambda^{2}\mathbb{V}_{O}[L_{\text{Reg}}].\]

To reduce the variance of \(L_{\text{Est}}\), \(\mathbb{V}_{O}[L_{\text{Est+Reg}}]\) satisfies \(\mathbb{V}_{O}[L_{\text{Est+Reg}}]\leq\mathbb{V}_{O}[L_{\text{Est}}]\), which implies that

\[2\lambda\text{Cov}(L_{\text{Est}},L_{\text{Reg}})+\lambda^{2}\mathbb{V}_{O}[L _{\text{Reg}}]\leq 0.\] (8)

Therefore, the parameter \(\lambda\) satisfies \(0\leq\lambda\leq-\frac{2\text{Cov}(L_{\text{Est}},L_{\text{Reg}})}{\mathbb{V}_ {O}[L_{\text{Reg}}]}\) and the optimal parameter is \(\lambda_{\text{opt}}=-\frac{\text{Cov}(L_{\text{Est}},L_{\text{Reg}})}{ \mathbb{V}_{O}[L_{\text{Reg}}]}\). On the other hand, since \(L_{\text{Est}}\) and \(L_{\text{Est+Reg}}\) are unbiased, we obtain \(\mathbb{E}_{O}[L_{\text{Reg}}]=0\). Then \(\text{Cov}(L_{\text{Est}},L_{\text{Reg}})\) satisfies

\[\text{Cov}(L_{\text{Est}},L_{\text{Reg}})= \mathbb{E}_{O}(L_{\text{Est}}L_{\text{Reg}})-\mathbb{E}_{O}(L_{ \text{Est}})\mathbb{E}_{O}(L_{\text{Reg}})\] \[= \mathbb{E}_{O}(L_{\text{Est}}L_{\text{Reg}})\] \[= \frac{1}{|\mathcal{D}|^{2}}\mathbb{E}_{O}\Bigg{(}\bigg{[}\sum_{(u,i)\in\mathcal{D}}\Big{(}f(o_{u,i},\hat{p}_{u,i})e_{u,i}+g(o_{u,i},\hat{p}_{u,i} )\hat{e}_{u,i}\Big{)}\bigg{]}\bigg{[}\sum_{(u,i)\in\mathcal{D}}h(o_{u,i},\hat{p }_{u,i})\Big{]}\Bigg{)}\] (9)

To facilitate representation, \(f(o_{u,i},\hat{p}_{u,i})e_{u,i}+g(o_{u,i},\hat{p}_{u,i})\hat{e}_{u,i}\) is denoted as \(r(o_{u,i},\hat{p}_{u,i},e_{u,i},\hat{e}_{u,i})\), which satisfies \(r(o_{u,i},\hat{p}_{u,i},e_{u,i},\hat{e}_{u,i})\geq 0\). Then the equation (9) can be rewritten as

\[\text{Cov}(L_{\text{Est}},L_{\text{Reg}})= \frac{1}{|\mathcal{D}|^{2}}\mathbb{E}_{O}\Bigg{(}\bigg{[}\sum_{(u,i)\in\mathcal{D}}r(o_{u,i},\hat{p}_{u,i},e_{u,i},\hat{e}_{u,i})\bigg{]}\bigg{[} \sum_{(u,i)\in\mathcal{D}}h(o_{u,i},\hat{p}_{u,i})\bigg{]}\Bigg{)}\] \[= \frac{1}{|\mathcal{D}|^{2}}\mathbb{E}_{O}\Bigg{(}\sum_{(u,i)\in \mathcal{D}}\bigg{[}h(o_{u,i},\hat{p}_{u,i})\sum_{(u,i)\in\mathcal{D}}r(o_{u,i },\hat{p}_{u,i},e_{u,i},\hat{e}_{u,i})\bigg{]}\Bigg{)}\] \[= \frac{1}{|\mathcal{D}|^{2}}\mathbb{E}_{O}\Bigg{(}\sum_{j=1}^{|D|} \sum_{k=1}^{|D|}h(o_{j},\hat{p}_{j})r(o_{k},\hat{p}_{k},e_{k},\hat{e}_{k}) \Bigg{)}\] \[= \frac{1}{|\mathcal{D}|^{2}}\sum_{j=1}^{|D|}\sum_{k=1}^{|D|}\mathbb{ E}_{O}[h(o_{j},\hat{p}_{j})r(o_{k},\hat{p}_{k},e_{k},\hat{e}_{k})].\]

Let us consider the term \(\mathbb{E}_{O}[h(o_{j},\hat{p}_{j})r(o_{k},\hat{p}_{k},e_{k},\hat{e}_{k})]\) in (10), which fulfills \(\mathbb{E}_{O}[h(o_{j},\hat{p}_{j})r(o_{k},\hat{p}_{k},e_{k},\hat{e}_{k})]\geq 0\). Therefore, we obtain \(\text{Cov}(L_{Est},L_{Reg})=\mathbb{E}_{O}(L_{\text{Est}}L_{\text{Reg}})\geq 0\). 

**Corollary 3.2**.: If the variance of \(L_{\text{Est+Reg}}\) is less than the variance of the original estimator \(L_{\text{Est}}\), then \(L_{\text{Est+Reg}}\) is not unbiased.

Proof.: We use the method of proof by contradiction. Assume that when \(\mathbb{V}_{O}[L_{\text{Est+Reg}}]\leq\mathbb{V}_{O}[L_{\text{Est}}]\), \(L_{\text{Est+Reg}}\) is unbiased. According to the definition of \(\mathbb{V}_{O}[L_{\text{Est+Reg}}]\) and \(\mathbb{V}_{O}[L_{\text{Est+Reg}}]\leq\mathbb{V}_{O}[L_{\text{Est}}]\), we have

\[\mathbb{V}_{O}[L_{\text{Est+Reg}}]= \mathbb{V}_{O}[L_{\text{Est}}]+2\lambda\text{Cov}(L_{\text{Est}},L _{\text{Reg}})+\lambda^{2}\mathbb{V}_{O}[L_{\text{Reg}}]\] \[\leq \mathbb{V}_{O}[L_{\text{Est}}],\] (10)

which implies that \(2\lambda\text{Cov}(L_{\text{Est}},L_{\text{Reg}})+\lambda^{2}\mathbb{V}_{O}[L_{ \text{Reg}}]\leq 0\). Therefore, the parameter \(\lambda\) needs to satisfy \(0\leq\lambda\leq-\frac{2\text{Cov}(L_{\text{Est}},L_{\text{Reg}})}{\mathbb{V}_ {O}[L_{\text{Reg}}]}\). Since \(\mathbb{V}_{O}[L_{\text{Reg}}]\) and \(\lambda\geq 0\), \(\text{Cov}(L_{\text{Est}},L_{\text{Reg}})\leq 0\) needs to be satisfied. On the other hand, as shown in A1, when \(L_{\text{Est+Reg}}\) is unbiased, we have \(\text{Cov}(L_{Est},L_{Reg})=\mathbb{E}_{O}(L_{\text{Est}}L_{\text{Reg}})\geq 0\), which contradicts the condition \(\text{Cov}(L_{\text{Est}},L_{\text{Reg}})\leq 0\). Therefore, Corollary 3.2 holds. 

**Theorem 3.3**.: Let the bias of \(L_{\text{Est+Reg}}\) be bounded and the variance of \(L_{\text{Est}}\) satisfy \(\lim_{p_{u,i}\to 0}\mathbb{V}_{O}[L_{\text{Est}}]\hat{p}_{u,i}=p_{u,i}]=\infty\). Then, there exists no regularizer \(L_{\text{Reg}}\) that enables the variance and generalization bound of the estimator bounded even the learned imputed errors or propensities are accurate.

[MISSING_PAGE_FAIL:14]

According to (15) and (16), the variance of the estimator \(L\) satisfies

\[\begin{split}\mathbb{V}_{O}[L]\geq&\frac{1}{|\mathcal{D} |^{2}}\sum_{(u,i)\in\mathcal{D}}\left[\mathbb{E}_{O}\Big{[}\Big{(}f(o_{u,i}, \hat{p}_{u,i})e_{u,i}+g(o_{u,i},\hat{p}_{u,i})\hat{e}_{u,i}\Big{)}^{2}\Big{]} \right.\\ &-\Big{[}f(1,\hat{p}_{u,i})p_{u,i}e_{u,i}+\big{[}g(1,\hat{p}_{u,i} )p_{u,i}+g(0,\hat{p}_{u,i})(1-p_{u,i})\big{]}\hat{e}_{u,i}\Big{]}^{2}\Big{]} \\ =&\frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D} }\left[\mathbb{E}_{O}\Big{[}\Big{(}f(o_{u,i},\hat{p}_{u,i})e_{u,i}+g(o_{u,i}, \hat{p}_{u,i})\hat{e}_{u,i}\Big{)}^{2}\Big{]}-e_{u,i}^{2}\right]\\ =&\frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D }}\left[\Big{[}f(1,\hat{p}_{u,i})e_{u,i}+g(1,\hat{p}_{u,i})\hat{e}_{u,i}\Big{]} ^{2}p_{u,i}+g^{2}(0,\hat{p}_{u,i})\hat{e}_{u,i}^{2}(1-p_{u,i})-e_{u,i}^{2} \right]\\ =&\frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D }}\frac{1-p_{u,i}}{p_{u,i}}\big{[}e_{u,i}-g(0,\hat{p}_{u,i})\hat{e}_{u,i}\big{]} ^{2}.\end{split}\] (17)

Therefore, when the propensity tends to zero, for any \(e_{u,i}-g(0,\hat{p}_{u,i})\hat{e}_{u,i}\neq 0\), the limit of the variance satisfies \(\lim_{p_{u,i}\to 0}\mathbb{V}_{O}[L]=\infty\). Since the generalization bound contains the bias and variance terms, the generalization bound is also unbounded when the propensity tends to zero. The proof is completed. 

## Appendix D Proofs of Properties for Fine-Grained Dynamic Estimators

**Lemma D.1** (Bias of D-IPS and D-DR).: _Given prediction errors \(e_{u,i}\), imputed errors \(\hat{e}_{u,i}\), and learned propensities \(\hat{p}_{u,i}\) for all user-item pairs \((u,i)\), the biases of the D-IPS and D-DR methods are given as_

\[\text{Bias}(L_{\text{D-IPS}})=\frac{1}{|\mathcal{D}|}\bigg{|}\sum_{(u,i)\in \mathcal{D}}h_{B}^{\text{Ent}}(\hat{p}_{u,i},p_{u,i},\alpha)e_{u,i}\bigg{|}, \text{ Bias}(L_{\text{D-DR}})=\frac{1}{|\mathcal{D}|}\bigg{|}\sum_{(u,i)\in \mathcal{D}}h_{B}^{\text{Ent}}(\hat{p}_{u,i},p_{u,i},\alpha)\delta_{u,i}\bigg{|},\] (18)

_where the function \(h_{B}\) satisfies \(h_{B}^{\text{Ent}}(\hat{p}_{u,i},p_{u,i},\alpha)=1-\frac{p_{u,i}}{f^{\alpha}( \hat{p}_{u,i})}\)._

Proof.: According to the definition of bias (6), the biases of D-IPS and D-DR are formulated as

\[\begin{split}\text{Bias}(L_{\text{D-IPS}})=&\frac{ 1}{|\mathcal{D}|}\bigg{|}\sum_{(u,i)\in\mathcal{D}}e_{u,i}-\mathbb{E}_{O}[L_{ \text{D-IPS}}]\bigg{|}=\frac{1}{|\mathcal{D}|}\bigg{|}\sum_{(u,i)\in\mathcal{D }}\bigg{(}1-\frac{p_{u,i}}{f^{\alpha}(\hat{p}_{u,i})}\bigg{)}e_{u,i}\bigg{|},\\ \text{Bias}(L_{\text{D-DR}})=&\frac{1}{|\mathcal{D}|} \bigg{|}\sum_{(u,i)\in\mathcal{D}}e_{u,i}-\mathbb{E}_{O}[L_{\text{D-DR}}] \bigg{|}=\frac{1}{|\mathcal{D}|}\bigg{|}\sum_{(u,i)\in\mathcal{D}}\bigg{(}1- \frac{p_{u,i}}{f^{\alpha}(\hat{p}_{u,i})}\bigg{)}\delta_{u,i}\bigg{|}.\end{split}\] (19)

**Lemma D.2** (Variance of D-IPS and D-DR).: _Given \(e_{u,i}\), \(\hat{e}_{u,i}\), and \(\hat{p}_{u,i}\) for all \((u,i)\in\mathcal{D}\), the variances of the D-IPS and D-DR methods are given as_

\[\mathbb{V}_{O}[L_{\text{D-IPS}}]=\frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in \mathcal{D}}h_{V}^{\text{Ent}}(\hat{p}_{u,i},p_{u,i},\alpha)e_{u,i}^{2},\ \forall_{O}[L_{\text{D-DR}}]=\frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in \mathcal{D}}h_{V}^{\text{Ent}}(\hat{p}_{u,i},p_{u,i},\alpha)\delta_{u,i}^{2},\]

_where the function \(h_{V}\) satisfies \(h_{V}^{\text{Ent}}(\hat{p}_{u,i},p_{u,i},\alpha)=\frac{p_{u,i}(1-p_{u,i})}{f^{ \alpha}(\hat{p}_{u,i})}\)._

Proof.: Considering the definition of the variance, we obtain the variances of D-IPS and D-DR as

\[\mathbb{V}_{O}[L_{\text{D-IPS}}]= \mathbb{E}_{O}[L_{\text{D-IPS}}^{2}]-\mathbb{E}_{O}^{2}[L_{\text{D- IPS}}]\] \[= \frac{1}{|\mathcal{D}|^{2}}\mathbb{E}_{O}\bigg{[}\bigg{(}\sum_{(u,i )\in\mathcal{D}}\frac{o_{u,i}}{f^{\alpha}(\hat{p}_{u,i})}e_{u,i}\bigg{)}^{2} \bigg{]}-\frac{1}{|\mathcal{D}|^{2}}\Big{(}\sum_{(u,i)\in\mathcal{D}}\frac{p_{u, i}}{f^{\alpha}(\hat{p}_{u,i})}e_{u,i}\bigg{)}^{2}\] \[= \frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D}}\frac{p_{u,i}( 1-p_{u,i})}{f^{\alpha}(\hat{p}_{u,i})}e_{u,i}^{2}\]and

\[\mathbb{V}_{O}[L_{\text{D-DR}}]= \mathbb{E}_{O}[L_{\text{D-DR}}^{2}]-\mathbb{E}_{O}^{2}[L_{\text{EB}}]\] \[= \frac{1}{|\mathcal{D}|^{2}}\mathbb{E}_{O}\bigg{[}\bigg{(}\sum_{(u,i )\in\mathcal{D}}\frac{a_{u,i}}{f^{\alpha}(\hat{p}_{u,i})}\delta_{u,i}\bigg{)}^{ 2}\bigg{]}-\frac{1}{|\mathcal{D}|^{2}}\Big{(}\sum_{(u,i)\in\mathcal{D}}\frac{p_ {u,i}}{f^{\alpha}(\hat{p}_{u,i})}\delta_{u,i}\bigg{)}^{2}\] \[= \frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D}}\frac{p_{u,i} (1-p_{u,i})}{f^{\alpha}(\hat{p}_{u,i})}\delta_{u,i}^{2}.\]

**Proposition D.3** (Monotonicity of Bias and Variance).: _For IPS-based and DR-based dynamic learning frameworks, and given \(e_{u,i}\), \(\hat{e}_{u,i}\)\(\hat{p}_{u,i}\) for all \((u,i)\in\mathcal{D}\), if \(f(\hat{p}_{u,i})\geq\hat{p}_{u,i}\) and the parameter \(\alpha\in[0,1]\) is increasing, then biases of D-IPS and D-DR are monotonically decreasing and their variances are monotonically increasing when learned propensities are accurate._

Proof.: According to the determining functions \(h_{B}(\hat{p}_{u,i},p_{u,i},\alpha)\) and \(h_{V}(\hat{p}_{u,i},p_{u,i},\alpha)\) in the bias and variance, the first derivative of \(h_{B}\) and \(h_{V}\) versus \(\alpha\) are derived as

\[\frac{\partial h_{B}(\hat{p}_{u,i},p_{u,i},\alpha)}{\partial\alpha}=\frac{p_ {u,i}\ln(f(\hat{p}_{u,i}))}{f^{\alpha}(\hat{p}_{u,i})},\,\frac{\partial h_{V}( \hat{p}_{u,i},p_{u,i},\alpha)}{\partial\alpha}=-\frac{2p_{u,i}(1-p_{u,i})\ln( f(\hat{p}_{u,i}))}{f^{2\alpha}(\hat{p}_{u,i})}.\] (20)

For all \(\hat{p}_{u,i}\in(0,1)\), we have \(\ln(f(\hat{p}_{u,i}))<0\). Therefore, \(\frac{\partial h_{B}(\hat{p}_{u,i},p_{u,i},\alpha)}{\partial\alpha}<0\) and \(\frac{\partial h_{V}(\hat{p}_{u,i},p_{u,i},\alpha)}{\partial\alpha}>0\) result in the monotonically decreasing function \(h_{B}\) and the monotonically increasing \(h_{V}\) for all user-item pairs \((u,i)\) as the number of \(\alpha\in[0,1]\) increases. Note the function \(f(\hat{p}_{u,i})\) satisfies \(f(\hat{p}_{u,i})\geq\hat{p}_{u,i}\), which implies that \(h_{B}\geq 0\) and \(h_{V}>0\) when learned propensities are accurate. Therefore, biases of D-IPS and D-DR are monotonically decreasing and their variances are monotonically increasing when learned propensities are accurate. 

**Lemma D.4** (Tail Bounds of D-IPS and D-DR).: _Given \(\hat{e}_{u,i}\), and \(\hat{p}_{u,i}\) for all \((u,i)\in\mathcal{D}\), for any prediction results, with probability \(1-\rho\), the deviation of D-IPS and D-DR estimators from their expectations satisfy_

\[\begin{split}\Big{|}L_{\text{D-IPS}}-\mathbb{E}_{O}[L_{\text{D- IPS}}]\Big{|}\leq&\sqrt{\frac{\log(\frac{2}{\rho})}{2|\mathcal{D}|^{2}} \sum_{(u,i)\in\mathcal{D}}\bigg{(}\frac{e_{u,i}}{f^{\alpha}(\hat{p}_{u,i})} \bigg{)}^{2}},\\ \Big{|}L_{\text{D-DR}}-\mathbb{E}_{O}[L_{\text{D-DR}}]\Big{|} \leq&\sqrt{\frac{\log(\frac{2}{\rho})}{2|\mathcal{D}| }\sum_{(u,i)\in\mathcal{D}}\bigg{(}\frac{\delta_{u,i}}{f^{\alpha}(\hat{p}_{u,i })}\bigg{)}^{2}}.\end{split}\] (21)

Proof.: Let \(X_{u,i}^{\text{D-IPS}}\) and \(X_{u,i}^{\text{D-DR}}\) be new random variables, which are defined as \(X_{u,i}^{\text{D-IPS}}=\frac{o_{u,i}}{f^{\alpha}(\hat{p}_{u,i})}e_{u,i}\) and \(X_{u,i}^{\text{D-DR}}=\hat{e}_{u,i}+\frac{o_{u,i}}{f^{\alpha}(\hat{p}_{u,i})} \delta_{u,i}\), respectively. Considering the independent observation indicators \(\{o_{u,i}|(u,i)\in\mathcal{D}\}\), random variables \(\{X_{u,i}^{\text{D-IPS}}|(u,i)\in\mathcal{D}\}\) and \(\{X_{u,i}^{\text{D-DR}}|(u,i)\in\mathcal{D}\}\) are independent of each other. Then, the probability distributions of \(X_{u,i}^{\text{D-IPS}}\) and \(X_{u,i}^{\text{D-DR}}\) can be obtained as follows:

\[\text{Pr}\bigg{(}X_{u,i}^{\text{D-IPS}}=\frac{e_{u,i}}{f^{\alpha}(\hat{p}_{u,i} )}\bigg{)} =p_{u,i},\text{Pr}(X_{u,i}^{\text{D-IPS}}=0)=1-p_{u,i},\] \[\text{Pr}\bigg{(}X_{u,i}^{\text{D-DR}}=\hat{e}_{u,i}+\frac{\delta _{u,i}}{f^{\alpha}(\hat{p}_{u,i})}\bigg{)} =p_{u,i},\text{Pr}(X_{u,i}^{\text{D-DR}}=\hat{e}_{u,i})=1-p_{u,i}\]

According to the Hoeffding's inequality, for any \(\varepsilon>0\), we have the following inequality

\[\text{Pr}\bigg{(}\Big{|}\sum_{(u,i)\in\mathcal{D}}X_{u,i}-\mathbb{E}_{O}\bigg{[} \sum_{(u,i)\in\mathcal{D}}X_{u,i}\bigg{]}\bigg{|}\geq\varepsilon\bigg{)}\leq 2 \exp\bigg{(}\frac{-2\varepsilon^{2}}{\sum\limits_{(u,i)\in\mathcal{D}}g^{2}(\hat{p} _{u,i},z_{u,i})}\bigg{)},\] (22)

where \(g(\hat{p}_{u,i},e_{u,i})=\frac{e_{u,i}}{f^{\alpha}(\hat{p}_{u,i})}\) for D-IPS and \(g(\hat{p}_{u,i},\delta_{u,i})=\frac{\delta_{u,i}}{f^{\alpha}(\hat{p}_{u,i})}\) for D-DR. Let \(\gamma=\frac{\varepsilon}{|\mathcal{D}|}\). Therefore, (22) can be rewritten as

\[\text{Pr}\Big{(}\Big{|}L_{\text{D-IPS}}-\mathbb{E}_{O}[L_{\text{D-IPS}}]\Big{|} \geq\gamma\Big{)}\leq 2\exp\bigg{(}\frac{-2(\gamma|\mathcal{D}|)^{2}}{\sum\limits_{(u,i)\in \mathcal{D}}g^{2}(\hat{p}_{u,i},z_{u,i})}\bigg{)}.\] (23)

Let \(\text{Pr}\Big{(}\Big{|}L_{\text{D-IPS}}-\mathbb{E}_{O}[L_{\text{D-IPS}}]\Big{|} \geq\gamma\Big{)}=\rho\). According to the inequality (23), the errors \(\gamma\) for D-IPS and D-DR can be solved as in (21).

**Theorem 3.5**.: (The optimal parameter \(\alpha_{u,i}^{\text{opt}}\)). Let the learned propensities be accurate, i.e., \(\hat{p}_{u,i}=p_{u,i}\). For weights \(w_{1}\) and \(w_{2}\), the objective function \(w_{1}h_{B}^{\text{Est}}+w_{2}h_{V}^{\text{Est}}\) under \(\alpha\in[0,1]\) achieves its minimum at

\[\alpha_{\text{opt}}=\min\bigg{\{}\max\bigg{\{}\frac{\ln\left(\frac{2w_{2}}{w_{ 1}}(1-p_{u,i})\right)}{\ln(f(p_{u,i}))},0\bigg{\}},1\bigg{\}}.\] (24)

Proof.: The first derivative of the objective function \(w_{1}h_{B}^{\text{Est}}(\alpha_{\text{opt}})+w_{2}h_{V}^{\text{Est}}(\alpha_{ \text{opt}})\) versus \(\alpha\) is derived as

\[\frac{\partial\text{Objective}(\hat{p}_{u,i},p_{u,i},\alpha)}{ \partial\alpha} =w_{1}\frac{\partial h_{B}^{\text{Est}}(\hat{p}_{u,i},p_{u,i}, \alpha)}{\partial\alpha}+w_{2}\frac{\partial h_{B}^{\text{Est}}(\hat{p}_{u,i},p_{u,i},\alpha)}{\partial\alpha}\] \[=w_{1}\frac{p_{u,i}\ln(f(\hat{p}_{u,i}))}{f^{\alpha}(\hat{p}_{u,i })}-w_{2}\frac{2p_{u,i}(1-p_{u,i})\ln(f(\hat{p}_{u,i}))}{f^{2\alpha}(\hat{p}_{ u,i})}.\] (25)

Let \(\frac{\partial\text{Objective}(\alpha|\hat{p}_{u,i}=p_{u,i})}{\partial\alpha}\) be zero. Then the optimal \(\alpha\) satisfies

\[\alpha_{\text{opt}}=\frac{\ln\left(\frac{2w_{2}}{w_{1}}(1-p_{u,i})\right)}{\ln (f(p_{u,i}))}.\] (26)

Note that \(\alpha\) needs to fulfill \(0\leq\alpha\leq 1\). Therefore, the solution of the optimization problem with the constraint can be obtained. 

**Theorem 3.6**.: (Generalization Bounds of D-IPS and D-DR). For any finite hypothesis space \(\mathcal{H}\) of \(\hat{Y}\) and the optimal prediction matrix \(\hat{Y}^{-}\), given \(\hat{e}_{u,i}\) and \(\hat{p}_{u,i}\) for all \((u,i)\in\mathcal{D}\), with probability \(1-\rho\), the prediction inaccuracies \(L_{\text{D-IPS}}(\hat{Y}^{-},Y)\) and \(L_{\text{D-DR}}(\hat{Y}^{-},Y)\) under D-IPS and D-DR have the following upper bounds

\[L_{\text{D-IPS}}(\hat{Y}^{-},Y^{O})+\sum_{(u,i)\in\mathcal{D}} \frac{|h_{B}^{\text{Est}}(\hat{p}_{u,i},p_{u,i},\alpha)e_{u,i}^{-}|}{|\mathcal{ D}|}+h_{G}^{\text{Est}}(e_{u,i}^{+}),\] \[L_{\text{D-DR}}(\hat{Y}^{-},Y^{O})+\sum_{(u,i)\in\mathcal{D}} \frac{|h_{B}^{\text{Est}}(\hat{p}_{u,i},p_{u,i},\alpha)\delta_{u,i}^{-}|}{| \mathcal{D}|}+h_{G}^{\text{Est}}(\delta_{u,i}^{+}),\]

where \(e_{u,i}^{+}\) and \(\delta_{u,i}^{+}\) are the error and error deviation corresponding to \(\hat{Y}^{+}=\arg\max_{\hat{Y}\in\mathcal{H}}\Big{\{}\sum_{(u,i)\in\mathcal{D}} \left(\frac{e_{u,i}}{f^{\alpha}(\hat{p}_{u,i})}\right)^{2}\Big{\}}\) and \(\hat{Y}^{+}=\arg\max_{\hat{Y}\in\mathcal{H}}\Big{\{}\sum_{(u,i)\in\mathcal{D}} \left(\frac{\delta_{u,i}}{f^{\alpha}(\hat{p}_{u,i})}\right)^{2}\Big{\}}\), respectively, and the function \(h_{G}^{\text{Est}}\) is formulated as

\[h_{G}^{\text{Est}}(z_{u,i}^{+})=\sqrt{\frac{\log(\frac{2|\mathcal{H}|}{\rho}) }{2|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D}}\left(\frac{z_{u,i}^{+}}{f^{ \alpha}(\hat{p}_{u,i})}\right)^{2}}\] (27)

Proof.: According to the definition of bias, the differences between \(L_{\text{real}}(\hat{Y},Y)\) and expectations of \(L_{\text{D-IPS}}(\hat{Y}^{-},Y^{O})\) and \(L_{\text{D-DR}}(\hat{Y}^{-},Y^{O})\) satisfy

\[\begin{split} L_{\text{real}}(\hat{Y}^{-},Y)-L_{\text{D-IPS}}( \hat{Y}^{-})=& L_{\text{real}}(\hat{Y}^{-},Y)-\mathbb{E}_{O}[L_{ \text{D-IPS}}(\hat{Y}^{-})]+\mathbb{E}_{O}[L_{\text{D-IPS}}(\hat{Y}^{-})]-L_{ \text{D-IPS}}(\hat{Y}^{-})\\ \leq&\text{Bias}(L_{\text{D-IPS}}(\hat{Y}^{-}))+ \mathbb{E}_{O}[L_{\text{D-IPS}}(\hat{Y}^{-})]-L_{\text{D-IPS}}(\hat{Y}^{-}) \end{split}\] (28)

and

\[\begin{split} L_{\text{real}}(\hat{Y}^{-},Y)-L_{\text{D-DR}}(\hat{Y}^ {-})=& L_{\text{real}}(\hat{Y}^{-},Y)-\mathbb{E}_{O}[L_{\text{D-DR}}( \hat{Y}^{-})]+\mathbb{E}_{O}[L_{\text{D-DR}}(\hat{Y}^{-})]-L_{\text{D-DR}}( \hat{Y}^{-})\\ \leq&\text{Bias}(L_{\text{D-DR}}(\hat{Y}^{-}))+ \mathbb{E}_{O}[L_{\text{D-DR}}(\hat{Y}^{-})]-L_{\text{D-DR}}(\hat{Y}^{-}), \end{split}\] (29)

respectively. From the inequalities (28) and (29), the expressions of \(\text{Bias}(L_{\text{D-IPS}}(\hat{Y}^{-}))\) and \(\text{Bias}(L_{\text{D-DR}}(\hat{Y}^{-}))\) have been given in Lemma 3.6 and, in what follows, terms \(\mathbb{E}_{O}[L_{\text{D-IPS}}(\hat{Y}^{-})]-L_{\text{D-IPS}}(\hat{Y}^{-})\) in (28) and \(\mathbb{E}_{O}[L_{\text{D-IPS}}(\hat{Y}^{-})]-L_{\text{D-IPS}}(\hat{Y}^{-})\) in (29) are discussed. Considering the finitehypothesis space \(\mathcal{H}=\{\hat{Y}^{1},\hat{Y}^{2},\ldots,\hat{Y}^{|\mathcal{H}|}\}\) and the Hoeffding's inequality, for any \(\varepsilon>0\), the following inequalities can be obtained

\[\Pr\Bigl{(}\Bigl{|}L_{\text{D-IPS}}(\hat{Y}^{-})-\mathbb{E}_{O}[L_ {\text{D-IPS}}(\hat{Y}^{-})]\Bigr{|}\leq\gamma\Bigr{)}= 1-\Pr\Bigl{(}\Bigl{|}L_{\text{D-IPS}}(\hat{Y}^{-})-\mathbb{E}_{O}[ L_{\text{D-IPS}}(\hat{Y}^{-})]\Bigr{|}\geq\gamma\Bigr{)}\] \[\geq 1-\sum_{\ell=1}^{\mathcal{H}}\Pr\Bigl{(}\Bigl{|}L_{\text{D-IPS}}( \hat{Y}^{\ell})-\mathbb{E}_{O}[L_{\text{D-IPS}}(\hat{Y}^{\ell})]\Bigr{|}\geq \gamma\Bigr{)}\] \[= 1-\sum_{\ell=1}^{\mathcal{H}}2\exp\left(\frac{-2(\gamma|\mathcal{ D}|)^{2}}{\sum\limits_{(u,i)\in\mathcal{D}}g^{2}(\hat{p}_{u,i},e_{u,i}^{ \varepsilon})}\right)\] (30) \[\geq 1-2|\mathcal{H}|\exp\left(\frac{-2(\gamma|\mathcal{D}|)^{2}}{ \sum\limits_{(u,i)\in\mathcal{D}}g^{2}(\hat{p}_{u,i},e_{u,i}^{ \varepsilon})}\right)\] \[\Pr\Bigl{(}\Bigl{|}L_{\text{D-DR}}(\hat{Y}^{-})-\mathbb{E}_{O}[ L_{\text{D-DR}}(\hat{Y}^{-})]\Bigr{|}\leq\gamma\Bigr{)}\geq 1-2|\mathcal{H}|\exp\left(\frac{-2(\gamma|\mathcal{D}|)^{2}}{ \sum\limits_{(u,i)\in\mathcal{D}}g^{2}(\hat{p}_{u,i},\delta_{u,i}^{+})}\right).\]

Let \(2|\mathcal{H}|\exp\left(\frac{-2(\gamma|\mathcal{D}|)^{2}}{\sum\limits_{(u,i) \in\mathcal{D}}g^{2}(\hat{p}_{u,i},Z_{u,i}^{+})}\right)\) be \(\rho\). The errors \(\gamma_{\text{D-IPS}}\) and \(\gamma_{\text{D-DR}}\) for D-IPS and D-DR can be solved as

\[\gamma_{\text{D-IPS}}=\sqrt{\frac{\log(\frac{2|\mathcal{H}|}{\rho})}{2| \mathcal{D}|^{2}}\sum\limits_{(u,i)\in\mathcal{D}}\left(\frac{e_{u,i}^{+}}{f^ {\alpha}(\hat{p}_{u,i})}\right)^{2}},\ \gamma_{\text{D-DR}}=\sqrt{\frac{\log(\frac{2| \mathcal{H}|}{\rho})}{2|\mathcal{D}|^{2}}\sum\limits_{(u,i)\in\mathcal{D}}\left( \frac{\delta_{u,i}^{+}}{f^{\alpha}(\hat{p}_{u,i})}\right)^{2}}.\] (31)

Therefore, \(\mathbb{E}_{O}[L_{\text{D-IPS}}(\hat{Y}^{-})]-L_{\text{D-IPS}}(\hat{Y}^{-})\) in (28) and \(\mathbb{E}_{O}[L_{\text{D-IPS}}(\hat{Y}^{-})]-L_{\text{D-IPS}}(\hat{Y}^{-})\) in (29) fulfill

\[\mathbb{E}_{O}[L_{\text{D-IPS}}(\hat{Y}^{-})]-L_{\text{D-IPS}}( \hat{Y}^{-}) \leq\sqrt{\frac{\log(\frac{2|\mathcal{H}|}{\rho})}{2|\mathcal{D}| ^{2}}\sum\limits_{(u,i)\in\mathcal{D}}\left(\frac{e_{u,i}^{+}}{f^{\alpha}(\hat {p}_{u,i})}\right)^{2}},\] (32) \[\mathbb{E}_{O}[L_{\text{D-DR}}(\hat{Y}^{-})]-L_{\text{D-DR}}(\hat {Y}^{-}) \leq\sqrt{\frac{\log(\frac{2|\mathcal{H}|}{\rho})}{2|\mathcal{D}| ^{2}}\sum\limits_{(u,i)\in\mathcal{D}}\left(\frac{\delta_{u,i}^{+}}{f^{\alpha}( \hat{p}_{u,i})}\right)^{2}}.\]

Combining (28), (29) and (32), we can obtain the generalization bounds of D-IPS and D-DR given in Lemma 3.11. 

**Theorem 3.7**.: (Boundedness of Variance and Generalization Bounds). Let \(\alpha_{u,i}^{\text{opt}}\in[0,1]\) be the optimal parameter of (4). If the dynamic estimators adopt \(\alpha_{u,i}^{\text{opt}}\) as the parameter, then the corresponding variance and generalization bounds are bounded.

Proof.: Considering the optimal parameter \(\alpha_{u,i}^{\text{opt}}\) for each user-item pair \((u,i)\) and the optimization problem (4), we can obtain the corresponding optimal objective function

\[\text{Objective}^{\text{opt}}=w_{1}E_{B}(h_{B}^{\text{Ent}}(\alpha_{u,i}^{\text{ opt}}))+w_{2}E_{V}(h_{V}^{\text{Ent}}(\alpha_{u,i}^{\text{opt}}))\leq w_{1}E_{B}(h_{B}^{ \text{Ent}}(0))+w_{2}E_{V}(h_{V}^{\text{Ent}}(0)).\] (33)

Since \(h_{B}^{\text{Ent}}(\hat{p}_{u,i},p_{u,i},\alpha)>0\) and \(h_{V}^{\text{Ent}}(\hat{p}_{u,i},p_{u,i},\alpha)>0\), considering (33), we have

\[w_{2}E_{V}(h_{V}^{\text{Ent}}(\alpha_{u,i}^{\text{opt}})) \leq w_{1}E_{B}(h_{B}^{\text{Ent}}(0))+w_{2}E_{V}(h_{V}^{\text{ Ent}}(0))\] (34) \[=w_{1}E_{B}(1-p_{u,i})+w_{2}E_{V}(p_{u,i}(1-p_{u,i})),\]

which implies that

\[h_{V}^{\text{Ent}}(\hat{p}_{u,i},p_{u,i},\alpha_{u,i}^{\text{opt}}) =\frac{p_{u,i}(1-p_{u,i})}{f^{2\alpha_{u,i}^{\text{opt}}}(\hat{p} _{u,i})}\] (35) \[\leq E_{V}^{-1}\Bigl{(}\frac{w_{1}E_{B}(1-p_{u,i})}{w_{2}}+E_{V}(p _{u,i}(1-p_{u,i}))\Bigr{)}\] \[=E_{V}^{-1}\Bigl{(}\frac{w_{1}E_{B}(1)}{w_{2}}+E_{V}(0.25)\Bigr{)}.\]Therefore, the variance of dynamic estimators are bounded by

\[\mathbb{V}_{O}[L_{\text{D-IPS}}|\alpha=\alpha^{\text{opt}}_{u,i}]= \frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D}}h^{\text{Est}} _{V}(\hat{p}_{u,i},p_{u,i},\alpha^{\text{opt}}_{u,i})e^{2}_{u,i}\] (36) \[\leq \frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D}}E_{V}^{-1} \Big{(}\frac{w_{1}E_{B}(1)}{w_{2}}+E_{V}(0.25)\Big{)}e^{2}_{u,i},\] \[\mathbb{V}_{O}[L_{\text{D-DR}}|\alpha=\alpha^{\text{opt}}_{u,i}]= \frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D}}h^{\text{Est }}_{V}(\hat{p}_{u,i},p_{u,i},\alpha^{\text{opt}}_{u,i})\delta^{2}_{u,i}\] \[\leq \frac{1}{|\mathcal{D}|^{2}}\sum_{(u,i)\in\mathcal{D}}E_{V}^{-1} \Big{(}\frac{w_{1}E_{B}(1)}{w_{2}}+E_{V}(0.25)\Big{)}\delta^{2}_{u,i}.\]

Considering the expression of \(h^{\text{Est}}_{G}(z^{+}_{u,i})\) in generalization bounds and the boundedness of \(h^{\text{Est}}_{V}(\hat{p}_{u,i},p_{u,i},\alpha^{\text{opt}}_{u,i})\), it is easy to obtain that under \(\rho\neq 0\) the generalization bounds of \(L_{\text{D-IPS}}\) and \(L_{\text{D-DR}}\) are bounded.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: see Introduction Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: see section 5 Conclusions Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: See Appendix

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: see section 4 Experiments Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: see section 4 Experiments Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: see subsection 4.1 Experimental Setup Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: see subsection 4.3 Ablation Studies Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: see section 4 Experiments Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Every approach in Experiments is preformed261 10 times to record its mean and standard deviation. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: see Introduction Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: Three real-world public datasets are used in Experiments. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: see Section 4 Experiments Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This problem is inapplicable to this paper. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This problem is inapplicable to this paper. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [No] Justification: The datasets used in this paper are public. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.