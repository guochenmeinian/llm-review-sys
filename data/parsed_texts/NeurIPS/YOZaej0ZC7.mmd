# On Measuring Fairness in Generative Models

Christopher T. H. Teo

christopher_teo@mymmail.sutd.edu.sg

&Milad Abdollahzadeh

milad_abdollahzadeh@sutd.sg

Ngai-Man Cheung

ngaiman_cheung@sutd.edu.sg

Corresponding Author

Singapore University of Technology and Design (SUTD)

###### Abstract

Recently, there has been increased interest in fair generative models. In this work, we conduct, for the first time, an in-depth study on **fairness measurement**, a critical component in gauging progress on fair generative models. We make three contributions. First, we conduct a study that reveals that the existing fairness measurement framework has considerable measurement errors, even when highly accurate sensitive attribute (SA) classifiers are used. These findings cast doubts on previously reported fairness improvements. Second, to address this issue, we propose CLassifier Error-Aware Measurement (CLEAM), a new framework which uses a statistical model to account for inaccuracies in SA classifiers. Our proposed CLEAM reduces measurement errors significantly, e.g., **4.98%\(\rightarrow\)0.62%** for StyleGAN2 _w.r.t._ Gender. Additionally, CLEAM achieves this with minimal additional overhead. Third, we utilize CLEAM to measure fairness in important text-to-image generator and GANs, revealing considerable biases in these models that raise concerns about their applications. **Code and more resources:**https://sutd-visual-computing-group.github.io/CLEAM/.

## 1 Introduction

Fair generative models have been attracting significant attention recently [1; 2; 7; 8; 9; 10; 11; 12; 13]. In generative models [14; 15; 16; 17; 18], fairness is commonly defined as equal generative quality [11] or equal representation [1; 2; 7; 9; 12; 19; 20]_w.r.t._ some _Sensitive Attributes_ (SA). In this work, we focus on the more widely utilized definition - _equal representation_. In this definition, as an example, a generative model is regarded as fair _w.r.t._ Gender, if it generates Male and Female samples with equal probability. This is an important research topic as such biases in generative models could impact their application efficacy, e.g., by introducing racial bias in face generation of suspects [21] or reducing accuracy when supplementing data for disease diagnosis [22].

**Fairness measurement for generative models.** Recognizing the importance of fair generative models, several methods have been proposed to mitigate biases in generative models [1; 2; 7; 9; 12]. However, _in our work, we focus mainly on the accurate fairness measurement of deep generative models i.e. assessing and quantifying the bias of generative models_. This is a critical topic, as accurate measurements are essential to reliably gauge the progress of bias mitigation techniques. The general fairness measurement framework is shown in Fig. 1 (See Sec. 2 for details). This framework is utilized in existing works to assess their proposed fair generators. Central to the fairness measurement framework is a _SA classifier_, which classifies the generated samples _w.r.t._ a SA, in order to estimate the bias of the generator. For example, if eight out of ten generated face images are classified as Maleby the SA classifier, then the generator is deemed biased at \(0.8\) towards Male (further discussion in Sec. 2). We follow previous works [1, 2, 12] and focus on binary SA due to dataset limitations.

**Research gap.** In this paper, we study a critical research gap in fairness measurement. Existing works assume that when SA classifiers are highly accurate, measurement errors should be insignificant. As a result, the effect of errors in SA classifiers has not been studied. However, our study reveals that _even with highly accurate SA classifiers, considerable fairness measurement errors could still occur_. This finding raises concerns about potential errors in previous works' results, which are measured using existing framework. Note that the SA classifier is _indispensable_ in fairness measurement as it enables automated measurement of generated samples.

**Our contributions.** We make three contributions to fairness measurement for generative models. _As our first contribution_, we analyze the accuracy of fairness measurement on generated samples, which previous works [1, 2, 7, 9, 12] have been unable to carry out due to the unavailability of proper datasets. We overcome this challenge by proposing new datasets of _generated samples_ with manual labeling _w.r.t._ various SAs. The datasets include generated samples from Stable Diffusion Model (SDM) [5] -- a popular text-to-image generator-- as well as two State-of-The-Art (SOTA) GANs (StyleGAN2 [3] and StyleSwin [4]) _w.r.t._ different SAs. Our new datasets are then utilized in our work to evaluate the accuracy of the existing fairness measurement framework. Our results reveal that the accuracy of the existing fairness measurement framework is not adequate, due to the lack of consideration for the SA classifier inaccuracies. More importantly, we found that _even in setups where the accuracy of the SA classifier is high, the error in fairness measurement could still be significant_. Our finding raises concerns about the accuracy of previous works' results [1, 2, 12], especially since some of their reported improvements are smaller than the margin of measurement errors that we observe in our study when evaluated under the same setup; further discussion in Sec. 3.

To address this issue, _as our second (major) contribution_, we propose CLassifier Error-Aware Measurement (CLEAM), a new more accurate fairness measurement framework based on our developed statistical model for SA classification (further details on the statistical model in Sec. 4.1).

Figure 1: **General framework for measuring fairness in generative models.** Generated samples with unknown ground-truth (GT) probability \(\bm{p^{*}}\)_w.r.t._ sensitive attribute (SA) are fed into a SA classifier to obtain \(\hat{\bm{p}}\). Existing framework (Baseline) uses the classifier output \(\hat{\bm{p}}\) as estimation of \(\bm{p^{*}}\). In contrast, our proposed CLEAM includes an improved estimation that accounts for inaccuracies in the SA classifier (see Alg. 1). **Our statistical model for fairness measurement.** This model accounts for inaccuracies in the SA classifier and is the base of our proposed CLEAM (see Sec. 4.1). **Â© Improvements with CLEAM.** CLEAM improves upon Baseline [1, 2] by reducing the relative error in estimating the GT \(p_{0}^{*}\) for SOTA GANs: StyleGAN2 [3] and StyleSwin [4], and Stable Diffusion Model [5]. First two displays the Baseline and CLEAM estimates for each GAN, using ResNet-18 as the SA classifier for Gender and BlackHair. The Baseline incurs significant fairness measurement errors (_e.g._ 4.98%), even when utilizing a highly accurate ResNet-18 (\(\approx\)97% accuracy). Meanwhile, CLEAM reduces the error significantly in all setups, _e.g._ in the first panel, the error is reduced: 4.98% \(\rightarrow\) 0.62%. Similarly, in the second row, CLEAM reduces measurement error significantly in the Stable Diffusion Model [5], using CLIP [6] as the SA classifier for Gender, _e.g._ first panel: 9.14% \(\rightarrow\) 0.05% (Detailed evaluation in Tab. 1 and Tab. 2). **Best viewed in color.**

Specifically, CLEAM utilizes this statistical model to account for the classifier's inaccuracies during SA classification and outputs a more accurate fairness measurement. We then evaluate the accuracy of CLEAM and validate its improvement over existing fairness measurement framework. We further conduct a series of different ablation studies to validate performance of CLEAM. We remark that CLEAM is not a new fairness metric, but an improved fairness measurement framework that could achieve better accuracy in bias estimation when used with various fairness metrics for generative models.

_As our third contribution_, we apply CLEAM as an accurate framework to reliably measure biases in popular generative models. Our study reveals that SOTA GANs have considerable biases _w.r.t._ several SA. Furthermore, we observe an intriguing property in Stable Diffusion Model: slight differences in semantically similar prompts could result in markedly different biases for SDM. These results prompt careful consideration on the implication of biases in generative models. **Our contributions are:**

* We conduct a study to reveal that even highly-accurate SA classifiers could still incur significant fairness measurement errors when using existing framework.
* To enable evaluation of fairness measurement frameworks, we propose new datasets based on generated samples from StyleGAN, StyleSwin and SDM, with manual labeling _w.r.t._ SA.
* We propose a statistically driven fairness measurement framework, CLEAM, which accounts for the SA classifier inaccuracies to output a more accurate bias estimate.
* Using CLEAM, we reveal considerable biases in several important generative models, prompting careful consideration when applying them to different applications.

## 2 Fairness Measurement Framework

Fig.1(a) illustrates the fairness measurement framework for generative models as in [1; 2; 7; 9; 12]. Assume that with some input _e.g._ noise vector for a GAN or text prompt for SDM, a generative model synthesizes a sample \(\mathbf{x}\). Generally, as the generator does not label synthesized samples, the ground truth (GT) class probability of these samples _w.r.t._ a SA (denoted by \(\bm{p}^{\star}\)) is unknown. Thus, an SA classifier \(C_{\mathbf{u}}\) is utilized to estimate \(\bm{p}^{\star}\). Specifically, for each sample \(\mathbf{x}\in\{\mathbf{x}\}\), \(C_{\mathbf{u}}(\mathbf{x})\) is the argmax classification for the respective SA. In existing works, the expected value of the SA classifier output over a batch of samples, \(\hat{\bm{p}}=\mathbb{E}_{\mathbf{x}}[C_{\mathbf{u}}(\mathbf{x})]\) (or the average of \(\hat{\bm{p}}\) over multiple batches of samples), is used as an estimation of \(\bm{p}^{\star}\). This estimate may then be used in some fairness metric \(f\) to report the fairness value for the generator, _e.g._ fairness discrepancy metric between \(\hat{\bm{p}}\) and a uniform distribution \(\bar{\bm{p}}\)[1; 20](see Supp A.3 for details on how to calculate \(f\)). Note that _the general assumption behind the existing framework is that with a reasonably accurate SA classifier, \(\hat{\bm{p}}\) could be an accurate estimation of \(\bm{p}^{\star}\)[1; 9]._ In the next section, we will present a deeper analysis on the effects of an inaccurate SA classifier on fairness measurement. Our findings suggest that there could be a large discrepancy between \(\hat{\bm{p}}\) and \(\bm{p}^{\star}\), even for highly accurate SA classifiers, indicative of significant fairness measurement errors in the current measurement framework.

One may argue that conditional GANs (cGANs) [23; 24] may be used to generate samples conditioned on the SA, thereby eliminating the need for an SA classifier. However, cGANs are not considered in previous works due to several limitations. These include the limited availability of large _labeled_ training datasets, the unreliability of sample quality and labels [25], and the exponentially increasing conditional terms, per SA. Similarly, for SDM, Bianchi _et al._[26] found that utilizing well-crafted prompts to mitigate biases is ineffective due to the presence of existing biases in its training dataset. Furthermore in Sec. 6, utilizing CLEAM, we will discuss that even subtle prompt changes (while maintaining the semantics) result in drastically different SA biases. See Supp G for further comparison between [26] and our findings.

## 3 A Closer Look at Fairness Measurement

In this section, we take a closer look at the existing fairness measurement framework. In particular, we examine its performance in estimating \(\bm{p}^{\star}\) of the samples generated by SOTA GANs and SDM, a task previously unstudied due to the lack of a labeled generated dataset. We do so by designing an experiment to demonstrate these errors while evaluating biases in popular image generators. Following previous works, our main focus is on binary SA which takes values in \(\{0,1\}\). Note that,we assume that the accuracy of the SA classifier \(C_{\mathbf{u}}\) is known and is characterized by \(\bm{\alpha}=\{\alpha_{0},\alpha_{1}\}\), where \(\alpha_{i}\) is the probability of correctly classifying label \(i\). For example, for Gender attribute, \(\alpha_{0}\) and \(\alpha_{1}\) are the probability of correctly classifying Female, and Male classes, respectively. In practice, \(C_{\mathbf{u}}\) is trained on standard training procedures (more details in the Supp F) and \(\bm{\alpha}\) can be measured during the validation stage of \(C_{\mathbf{u}}\) and be considered a constant when the validation dataset is large enough. Additionally, \(\bm{p}^{\star}\) can be assumed to be a constant vector, given that the samples generated can be considered to come from an infinite population, as theoretically there is no limit to the number of samples from a generative model like GAN or SDM.

**New dataset by labeling generators output.** The major limitation of evaluating the existing fairness measurement framework is the unavailability of \(\bm{p}^{\star}\). _To pave the way for an accurate evaluation, we create a new dataset by manually labeling the samples generated by GANs and SDM_. More specifically, we utilize the official publicly released pre-trained StyleGAN2 [3] and StyleSwin [4] on CelebA-HQ [27] for sample generation. Then, we randomly sample from these GANs and utilize Amazon Mechanical Turks to hand-label the samples _w.r.t._ Gender and BlackHair, resulting in \(\approx\)9K samples for each GAN; see Supp H for more details and examples. Next, we follow a similar labeling process _w.r.t._ Gender, but with a SDM [5] pre-trained on LAION-5B[28]. Here, we input prompts using best practices [26, 29, 30, 31], beginning with a scene description ("A photo with the face of"), followed by four indefinite (gender-neutral) pronouns or nouns [32, 33] - ("an individual", "a human being", "one person", "a person") to collect \(\approx\)2k high-quality samples. We refer to this new dataset as Generated Dataset (**GenData**), which includes generated images from three models with corresponding SA labels: GenData-StyleGAN2, GenData-StyleSwin, GenData-SDM. We remark that these labeled datasets only provide a strong approximation of \(\bm{p}^{\star}\) for each generator, however as the datasets are reasonably large, we find this approximation sufficient and simply refer to it as the GT \(\bm{p}^{\star}\). Then utilizing this GT \(\bm{p}^{\star}\), we compare it against the estimated baseline (\(\hat{\bm{p}}\)). One interesting observation revealed by GenData is that all three generators exhibit a considerable amount of bias (see Tab.1 and 2); more detail in Sec. 6. Note that for a fair generator we have \(p_{0}^{*}=p_{1}^{*}=0.5\), and measuring the \(p_{0}^{*}\) and \(p_{1}^{*}\) is a good proxy for measuring fairness.

**Experimental setup.** Here, we follow Choi _et al._[1] as the _Baseline_ for measuring fairness. In particular, to calculate each \(\hat{\bm{p}}\) value for a generator, a corresponding batch of \(n=400\) samples is randomly drawn from GenData and passed into \(C_{\mathbf{u}}\) for SA classification. We repeat this for \(s=30\) batches and report the mean results denoted by \(\mu_{\texttt{Base}}\) and the 95% confidence interval denoted by \(\rho_{\texttt{Base}}\). For a comprehensive analysis of the GANs, we repeat the experiment using four different SA classifiers: Resnet-18, ResNet-34 [34], MobileNet2 [35], and VGG-16 [36]. Then, to evaluate the SDM, we utilize CLIP [6] to explore the utilization of pre-trained models for zero-shot SA classification; more details on the CLIP SA classifier in Supp. E. As CLIP does not have a validation dataset, to measure \(\bm{\alpha}\) for CLIP, we utilize CelebA-HQ, a dataset with a similar domain to our application. We found this to be a very accurate approximation; see Supp D.7 for validation results. Note that for SDM, a separate \(\hat{\bm{p}}\) is measured for each text prompt as SDM's output images are conditioned on the input text prompt. As seen in Tab. 1 and 2, all classifiers demonstrate reasonably high average accuracy \(\in[84\%,98.7\%]\). Note that as we focus on binary SA (_e.g._ Gender:{Male, Female}), both \(\bm{p}^{\star}\) and \(\hat{\bm{p}}\) have two components _i.e._\(\bm{p}^{\star}=\{p_{0}^{*},p_{1}^{*}\}\), and \(\hat{\bm{p}}=\{\hat{p}_{0},\hat{p}_{1}\}\). After computing the \(\mu_{\texttt{Base}}\) and \(\rho_{\texttt{Base}}\), we calculate _normalized \(L_{1}\) point error \(e_{\mu}\)_, and _interval max error_\(e_{\rho}\)_w.r.t._ the \(p_{0}^{*}\) (GT) to evaluate the measurement accuracy of the baseline method:

\[e_{\mu_{\texttt{Base}}}=\tfrac{1}{p_{0}^{*}}|p_{0}^{*}-\mu_{\texttt{Base}}| \quad;\quad e_{\rho_{\texttt{Base}}}=\tfrac{1}{p_{0}^{*}}\max\{|\min(\rho_{ \texttt{Base}})-p_{0}^{*}|,|\max(\rho_{\texttt{Base}})-p_{0}^{*}|\}\] (1)

**Based on our results in Tab. 1,** for GANs, we observe that despite the use of reasonably accurate SA classifiers, there are significant estimation errors in the existing fairness measurement framework, _i.e._\(e_{\mu_{\texttt{Base}}}\)\(\in[4.98\%,17.13\%]\). In particular, looking at the SA classifier with the highest average accuracy of \(\approx 97\%\) (ResNet-18 on Gender), we observe significant discrepancies between GT \(p_{0}^{*}\) and \(\mu_{\texttt{Base}}\), with \(e_{\mu_{\texttt{Base}}}=4.98\%\). These errors generally worsen as accuracy marginally degrades, _e.g._ MobileNetv2 with accuracy \(\approx 96\%\) results in \(e_{\mu_{\texttt{Base}}}=5.45\%\). These considerably large errors contradict prior assumptions - that for a reasonably accurate SA classifier, we can assume \(e_{\mu_{\texttt{Base}}}\) to be fairly negligible. Similarly, our results in Tab. 2 for the SDM, show large \(e_{\mu_{\texttt{Base}}}\)\(\in[1.49\%,9.14\%]\), even though the classifier is very accurate. We discuss the reason for this in more detail in Sec. 5.1.

_Overall, these results are concerning as they cast doubt on the accuracy of prior reported results._ For example, imp-weighting [1] which uses the same ResNet-18 source code as our experiment, reports a 2.35% relative improvement in fairness against its baseline _w.r.t._ Gender, which falls within the range of our experiments smallest relative error, \(e_{\mu_{\text{data}}}\)=4.98%. Similarly, Teo _et al._[2] and Um _et al._[12] report a relative improvement in fairness of 0.32% and 0.75%, compared to imp-weighting [1]. These findings suggest that some prior results may be affected due to oversight of SA classifier's inaccuracies; see Supp. A.4 for more details on how to calculate these measurements.

**Remark:** In this section, we provide the keystone for the evaluation of measurement accuracy in the current framework by introducing a labeled dataset based on generated samples. These evaluation results raise concerns about the accuracy of existing framework as considerable error rates were observed even when using accurate SA classifiers, an issue previously seen to be negligible.

## 4 Mitigating Error in Fairness Measurements

The previous section exposes the inaccuracies in the existing fairness measurement framework. Following that, in this section, we first develop a statistical model for the erroneous output of the SA classifier, \(\hat{\bm{p}}\), to help draw a more systematic relationship between the inaccuracy of the SA classifier and error in fairness estimation. Then, with this statistical model, we propose CLEAM - a new measurement framework that reduces error in the measured \(\hat{\bm{p}}\) by accounting for the SA classifier inaccuracies to output a more accurate statistical approximation of \(\bm{p}^{*}\).

### Proposed Statistical Model for Fairness Measurements

As shown in Fig.1(a), to measure the fairness of the generator, we feed \(n\) generated samples to the SA classifier \(C_{\mathbf{u}}\). The output of the SA classifier (\(\hat{\bm{p}}\)) is in fact a random variable that aims to approximate the \(\bm{p}^{*}\). Here, we propose a statistical model to derive the distribution of \(\hat{\bm{p}}\).

As Fig.1(b) demonstrates in our running example of a binary SA, each generated sample is from _class 0_ with probability \(p_{0}^{*}\), or from _class 1_ with probability \(p_{1}^{*}\). Then, generated sample from _class \(i\)_ where \(i\in\{0,1\}\), will be classified correctly with the probability of \(\alpha_{i}\), and wrongly with the probability of \(\alpha_{i}^{\prime}=1-\alpha_{i}\). Thus, for each sample, there are four mutually exclusive possible events denoted by \(\mathbf{c}\), with the corresponding probability vector \(\mathbf{p}\):

\[\mathbf{c}^{T}=\begin{bmatrix}c_{0|0}&c_{1|0}&c_{1|1}&c_{0|1}\end{bmatrix}\quad,\quad\mathbf{p}^{T}=\begin{bmatrix}p_{0}^{*}\alpha_{0}&p_{0}^{*}\alpha_{0}^{ \prime}&p_{1}^{*}\alpha_{1}&p_{1}^{*}\alpha_{1}^{\prime}\end{bmatrix}\] (2)

where \(c_{i|j}\) denotes the event of assigning label \(i\) to a sample with GT label \(j\). Given that this process is performed independently for each of the \(n\) generated images, the probability of the counts for each output \(\mathbf{c}^{T}\) in Eqn. 2 (denoted by \(\mathbf{N_{c}}\)) can be modeled by a multinomial distribution, _i.e._\(\mathbf{N_{c}}\sim Multi(n,\mathbf{p})\)[37; 38; 39]. Note that \(\mathbf{N_{c}}\) models the _joint probability distribution_ of these outputs, _i.e._\(\mathbf{N_{c}}\sim\mathbb{P}(N_{c_{0|0}},N_{c_{1|0}},N_{c_{1|1}},N_{c_{0|1}})\) where, \(N_{c_{i|j}}\) is the random variable of the count for event \(c_{i|j}\) after classifying \(n\) generated images. Since \(\mathbf{p}\) is not near the boundary of the parameter space, and as we utilize a large \(n\), based on the central limit theorem, \(Multi(n,\mathbf{p})\) can be approximated by a multivariate Gaussian distribution, \(\mathbf{N_{c}}\sim\bm{\mathcal{N}}(\bm{\mu},\bm{\Sigma})\), with \(\bm{\mu}=n\mathbf{p}\) and \(\bm{\Sigma}=n\mathbf{M}\)[40; 39], where \(\mathbf{M}\) is defined as:

\[\mathbf{M}=diag(\mathbf{p})-\mathbf{p}\mathbf{p}^{T}\] (3)

\(diag(\mathbf{p})\) denotes a square diagonal matrix corresponding to vector \(\mathbf{p}\) (see Supp A.1 for expanded form). The _marginal distribution_ of this multivariate Gaussian distribution gives us a univariate (one-dimensional) Gaussian distribution for the count of each output \(\mathbf{c}^{T}\) in Eqn. 2. For example, the distribution of the count for event \(c_{0|0}\), denoted by \(N_{c_{0|0}}\), can be modeled as \(N_{c_{0|0}}\sim\mathcal{N}(\bm{\mu}_{1},\bm{\Sigma}_{11})\).

Lastly, we find the total percentage of data points labeled as class \(i\) when labeling \(n\) generated images using the normalized sum of the related random variables, _i.e._\(\hat{p}_{i}=\frac{1}{n}\sum_{j}N_{c_{i|j}}\). For our binary example, \(\hat{p}_{i}\) can be calculated by summing random variables with Gaussian distribution, which results in another Gaussian distribution [41], _i.e._, \(\hat{p}_{0}\sim\mathcal{N}(\tilde{\mu}_{\hat{p}_{0}},\tilde{\sigma}_{\hat{p}_ {0}}^{2})\), where:

\[\tilde{\mu}_{\hat{p}_{0}}= \frac{1}{n}(\bm{\mu}_{1}+\bm{\mu}_{4})=p_{0}^{*}\alpha_{0}+p_{1} ^{*}\alpha_{1}^{\prime}\] (4) \[\tilde{\sigma}_{\hat{p}_{0}}^{2}= \frac{1}{n^{2}}(\bm{\Sigma}_{11}+\bm{\Sigma}_{44}+2\bm{\Sigma}_{14 })=\frac{1}{n}[(p_{0}^{*}\alpha_{0}-(p_{0}^{*}\alpha_{0})^{2})+(p_{1}^{*} \alpha_{1}^{\prime}-(p_{1}^{*}\alpha_{1}^{\prime})^{2})]+\frac{2}{n}p_{0}^{*} p_{1}^{*}\alpha_{0}\alpha_{1}^{\prime}\] (5)

Similarly \(\hat{p}_{1}\sim\mathcal{N}(\tilde{\mu}_{\hat{p}_{1}},\tilde{\sigma}_{\hat{p}_ {1}}^{2})\) with \(\tilde{\mu}_{\hat{p}_{1}}=(\bm{\mu}_{2}+\bm{\mu}_{3})/n\), and \(\tilde{\sigma}_{\hat{p}_{1}}^{2}=(\bm{\Sigma}_{22}+\bm{\Sigma}_{33}+2\bm{ \Sigma}_{23})/n^{2}\) which is aligned with the fact that \(\hat{p}_{1}=1-\hat{p}_{0}\).

**Remark:** In this section, considering the probability tree diagram in Fig.1(b), we propose a joint distribution for the possible events of classification (\(N_{c_{|ij}}\)), and use it to compute the marginal distribution of each event, and finally the distribution of the SA classifier outputs (\(\hat{p}_{0}\), and \(\hat{p}_{1}\)). Note that considering Eqn. 4, 5, only with a perfect classifier (\(\alpha_{i}=1\), _i.e._ acc\(=100\%\)) the \(\tilde{\mu}_{\hat{p}_{0}}\) converges to \(p_{0}^{*}\). However, training a perfect SA classifier is not practical _e.g._ due to the lack of an appropriate dataset and task hardness [42; 43]. As a result, in the following, we will propose CLEAM which instead utilizes this statistical model to mitigate the error of the SA classifier.

### CLEAM for Accurate Fairness Measurement

In this section, we propose a new estimation method in fairness measurement that considers the inaccuracy of the SA classifier. For this, we use the statistical model, introduced in Sec 4.1, to compute a more accurate estimation of \(\bm{p}^{*}\). Specifically, we first propose a Point Estimate (PE) by approximating the _maximum likelihood value_ of \(\bm{p}^{*}\). Then, we use the _confidence interval_ for the observed data (\(\bm{\hat{p}}\)) to propose an Interval Estimate (IE) for \(\bm{p}^{*}\).

**Point Estimate (PE) for \(\bm{p}^{*}\)**. Suppose that we have access to \(s\) samples of \(\bm{\hat{p}}\) denoted by \(\{\bm{\hat{p}}^{1},\ldots,\bm{\hat{p}}^{s}\}\), _i.e._ SA classification results on \(s\) batches of generated data. We can then use the proposed statistical model to approximate the \(\bm{p}^{*}\). In the previous section, we demonstrate that we can model \(\hat{p}_{j}^{*}\) using a Gaussian distribution. Considering this, first, we use the available samples to calculate sample-based statistics including the mean and variance of the \(\hat{p}_{j}\) samples:

\[\tilde{\mu}_{\hat{p}_{j}} =\tfrac{1}{s}\sum_{i=1}^{s}\hat{p}_{j}^{i}\] (6) \[\tilde{\sigma}_{\hat{p}_{j}}^{2} =\tfrac{1}{s-1}\sum_{i=1}^{s}(\hat{p}_{j}^{i}-\tilde{\mu}_{\hat{p }_{j}})^{2}\] (7)

For a Gaussian distribution, the Maximum Likelihood Estimate (MLE) of the population mean is its sample mean \(\tilde{\mu}_{\hat{p}_{j}}\)[44]. Given that \(s\) is large enough (_e.g._\(s>30\)), we can assume that \(\tilde{\mu}_{\hat{p}_{j}}\) is a good approximation of the population mean [45], and equate it to the statistical population mean \(\tilde{\mu}_{\hat{p}_{j}}\) in Eqn. 4 (see Supp A.2 for derivation). With that, we get the _maximum likelihood approximation of \(\bm{p}^{*}\)_, _which we call the CLEAM's point estimate, \(\mu_{\texttt{CLEAM}}\)_:

\[\mu_{\texttt{CLEAM}}(p_{0}^{*})=(\tilde{\mu}_{\hat{p}_{0}}-\alpha_{1}^{ \prime})/(\alpha_{0}-\alpha_{1}^{\prime})\quad,\quad\mu_{\texttt{CLEAM}}(p_{1 }^{*})=1-\mu_{\texttt{CLEAM}}(p_{0}^{*})\] (8)

Notice that \(\mu_{\texttt{CLEAM}}\) accounts for the inaccuracy of the SA classifier.

**Interval Estimate (IE) for \(\bm{p}^{*}\)**. In the previous part, we propose a PE for \(\bm{p}^{*}\) using the statistical model, and sample-based mean \(\tilde{\mu}_{\hat{p}_{0}}\). However, as we use only \(s\) samples of \(\bm{\hat{p}}\), \(\tilde{\mu}_{\hat{p}_{0}}\) may not capture the exact value of the population mean. This adds some degree of inaccuracy into \(\mu_{\texttt{CLEAM}}\). In fact, in our framework, \(\tilde{\mu}_{\hat{p}_{0}}\) equals \(\tilde{\mu}_{\hat{p}_{0}}\) when \(s\to\infty\). However, increasing each unit of \(s\) significantly increases the computational complexity, as each \(\bm{\hat{p}}\) requires \(n\) generated samples. To address this, we recall that \(\hat{p}_{0}\) follows a Gaussian distribution and instead utilize frequentist statistics [41] to propose a 95% confidence interval (CI) for \(\bm{p}^{*}\). To do this, first we derive the CI for \(\tilde{\mu}_{\hat{p}_{0}}\):

\[\tilde{\mu}_{\hat{p}_{0}}-1.96\tfrac{\hat{\sigma}_{\hat{p}_{0}}}{\sqrt{s}}\leq \tilde{\mu}_{\hat{p}_{0}}\leq\tilde{\mu}_{\hat{p}_{0}}+1.96\tfrac{\hat{\sigma} _{\hat{p}_{0}}}{\sqrt{s}}\] (9)

Then, applying Eqn.4 to Eqn.9 gives the lower and upper bounds of the approximated 95% CI for \(p_{0}^{*}\):

\[\mathcal{L}(p_{0}^{*}),\mathcal{U}(p_{0}^{*})=(\tilde{\mu}_{\hat{p}_{0}}\mp 1.96(\tilde{\sigma}_{\hat{p}_{0}}/\sqrt{s})-\alpha_{1}^{\prime})/(\alpha_{0}- \alpha_{1}^{\prime})\] (10)

This gives us the interval estimate of CLEAM, \(\rho_{\texttt{CLEAM}}=[\mathcal{L}(p_{0}^{*}),\mathcal{U}(p_{0}^{*})]\), a range of values that we can be approximately 95% confident to contain \(p_{0}^{*}\). The range of possible values for \(p_{1}^{*}\) can be simply derived considering \(p_{1}^{*}=1-p_{0}^{*}\). The overall procedure of CLEAM is summarized in Alg. 1. Now, with the IE, we can provide statistical significance to the reported fairness improvements.

[MISSING_PAGE_EMPTY:7]

Experiments

In this section, we first evaluate fairness measurement accuracy of CLEAM on both GANs and SDM (Sec.5.1) with our proposed GenData dataset. Then we evaluate CLEAM's robustness through some ablation studies (Sec. 5.2). To the best of our knowledge, there is no similar literature for improving fairness measurements in generative models. Therefore, we compare **CLEAM** with the two most related works: a) the **Baseline** used in previous works [1; 2; 7; 9; 12] b) **Diversity**[46] which computes disparity within a dataset via an intra-dataset pairwise similarity algorithm. We remark that, as discussed by Keswani _et al._[46] Diversity is model-specific using VGG-16 [36]; see Supp. D.2 for more details. Finally, unless specified, we repeat the experiments with \(s=30\) batches of images from the generators with batch size \(n=400\). For a fair comparison, all three algorithms use the exact same inputs. However, while Baseline and Diversity ignore the SA classifier inaccuracies, CLEAM makes good use of it to rectify the measurement error. As mentioned in Sec. 4.2, for CLEAM, we utilize \(\bm{\alpha}\) measured on real samples, which we found to be a good approximation of the \(\bm{\alpha}\) measured on generated samples (see Supp. D.7 for results). We repeat each experiment 5 times and report the mean value for each test point for both PE and IE. See Supp D.1 for the standard deviation.

### Evaluating CLEAM's Performance

**CLEAM for fairness measurement of SOTA GANs - StyleGAN2 and StyleSwin.** For a fair comparison, we first compute \(s\) samples of \(\hat{\bm{p}}\), one for each batch of \(n\) images. For Baseline, we use the mean \(\hat{\bm{p}}\) value as the PE (denoted by \(\mu_{\texttt{Base}}\)), and the \(95\%\) confidence interval as IE (\(\rho_{\texttt{Base}}\)). With the same \(s\) samples of \(\hat{\bm{p}}\), we apply Alg. 1 to obtain \(\mu_{\texttt{CLEAM}}\) and \(\rho_{\texttt{CLEAM}}\). For Diversity, following the original source code [46], a controlled dataset with fair representation is randomly selected from a held-out dataset of CelebA-HQ [27]. Then, we use a VGG-16 [36] feature extractor and compute Diversity, \(\delta\). With \(\delta\) we find \(\hat{p}_{0}=(\delta+1)/2\) and subsequently \(\mu_{\texttt{Div}}\) and \(\rho_{\texttt{Div}}\) from the mean and \(95\%\) CI (see Supp D.2 for more details on diversity). We then compute \(e_{\mu_{\texttt{CLEAM}}}\), \(e_{\mu_{\texttt{Div}}}\), \(e_{\rho_{\texttt{CLEAM}}}\) and \(e_{\rho_{\texttt{Div}}}\) with Eqn 1, by replacing the Baseline estimates with CLEAM and Diversity.

As discussed, our results in Tab.1 show that the baseline experiences significantly large errors of \(4.98\%\leq e_{\mu_{\texttt{Base}}}\leq 17.13\%\), due to a lack of consideration for the inaccuracies of the SA classifier. We note that this problem is prevalent throughout the different SA classifier architectures, even with higher capacity classifiers _e.g._ ResNet-34. Diversity, a method similarly unaware of the inaccuracies of the SA classifier, presents a similar issue with \(8.98\%\leq e_{\mu_{\texttt{Div}}}\leq 14.33\%\) In contrast, CLEAM dramatically reduces the error for all classifier architectures. Specifically, CLEAM reduces the average point estimate error from \(e_{\mu_{\texttt{Base}}}\geq 8.23\%\) to \(e_{\mu_{\texttt{CLEAM}}}\leq 1.24\%\), in both StyleGAN2 and StyleSwin. The IE presents similar results, where in most cases \(\rho_{\texttt{CLEAM}}\) bounds the GT value of \(\bm{p}^{\star}\).

**CLEAM for fairness measurement of SDM.** We evaluate CLEAM in estimating the bias of the SDM _w.r.t._**Gender**, based on the synonymous (gender-neutral) prompts introduced in Sec. 3. Recall that here we utilize CLIP as the zero-shot SA classifier. Our results in Tab 2, as discussed, show that utilizing the baseline results in considerable error (\(1.49\%\leq e_{\mu_{\texttt{Base}}}\leq 9.14\%\)) for all prompts, even though the SA classifier's average accuracy was high, \(\approx 98.7\%\) (visual results in Fig.2). A closer look at the theoretical model's Eqn. 4 reveals that this is due to the larger inaccuracies observed in the biased class (\(\alpha_{1}^{\prime}\)) coupled with the large bias seen in \(p_{1}^{\star}\), which results in \(\mu_{\texttt{Base}}\) deviating from \(p_{0}^{\star}\). In contrast, CLEAM accounts for these inaccuracies and significantly minimizes the error to \(e_{\mu_{\texttt{CLEAM}}}\leq 1.77\%\). Moreover, CLEAM's IE is able to consistently bound the GT value of \(p_{0}^{\star}\).

### Ablation Studies and Analysis

Here, we perform the ablation studies and compare CLEAM with classifier correction methods. _We remark that detailed results of these experiments are provided in the Supp due to space limitations._

**CLEAM for measuring varying degrees of bias.** As we cannot control the bias in trained generative models, to simulate different degrees of bias, we evaluate CLEAM with a _pseudo-generator_. Our results show that CLEAM is effective at different biases (\(p_{0}^{\star}\in\) [0.5,0.9]) reducing the average error from \(2.80\%\leq e_{\mu_{\texttt{Base}}}\leq 6.93\%\) to \(e_{\mu_{\texttt{CLEAM}}}\leq 0.75\%\) on CelebA [47]_w.r.t._ {Gender,BlackHair}, and AFHQ [48]_w.r.t._Cat/Dog. See Supp D.3 and D.4 for full experimental results.

**CLEAM vs Classifier Correction Methods [49]**. CLEAM generally accounts for the classifier's inaccuracies, without targeting any particular cause of inaccuracies, for the purpose of rectifying the fairness measurements. This objective is unlike traditional classifier correction methods as it does not aim to improve the actual classifier's accuracy. However, considering that classifier correction methods may improve the fairness measurements by directly rectifying the classifier inaccuracies, we compare its performance against CLEAM. As an example, we utilize the Black Box Shift Estimation / Correction (BBSE / BBSC) [49] which considers the label shift problem and aims to correct the classifier output by detecting the distribution shift. Our results, based on Sec. 5.1 setup, show that while BBSE does improve on the fairness measurements of the baseline _i.e._ 4.20% \(\leq e_{\mu_{\text{BBSE}}}\leq\) 3.38%, these results are far inferior to CLEAM's results seen in Tab. 1. In contrast, BBSC demonstrates no improvements in fairness measurements. See Supp D.8 for full experimental results. We postulate that this is likely due to the strong assumption of label shift made by both methods.

**Effect of batch-size.** Utilizing experimental setup in Sec. 5.1 for batch size \(n\in\)[100,600], our results in Fig. 3 show that \(n\)=400 is an ideal batch size, balancing computational cost and measurement accuracy. See Supp F for full experimental details and results.

## 6 Applying CLEAM: Bias in Current SOTA Generative Models

In this section, we leverage the improved reliability of CLEAM to study biases in the popular generative models. Firstly, with the rise in popularity of text-to-image generators [50; 51; 52; 5], we revisit our results when passing different prompts, with synonymous neutral meanings to an SDM, and take a closer look at how subtle prompt changes can impact bias _w.r.t._\(\mathsf{Gender}\). Furthermore, we further investigate if similar results would occur in other SA, Smiling. Secondly, with the shift in popularity from convolution to transformer-based architectures [53; 54; 55], due to its better sample quality, we determine whether the learned bias would also change. For this, we compare StylesSwin (transformer) and StyleGAN2 (convolution), which are both based on the same architecture backbone.

Our results, on SDM, demonstrate that the use of different synonymous neutral prompts [32; 33] results in different degrees of bias _w.r.t._ both \(\mathsf{Gender}\) and \(\mathsf{Smiling}\) attributes. For example in Fig. 2, a semantically insignificant prompt change from "one person" to "a person" results in a significant shift in \(\mathsf{Gender}\) bias. Then, in Fig. 4a, we observe that while the SDM _w.r.t._ our prompts appear to be heavily biased to not-\(\mathsf{Smiling}\), having "person" in the prompt appears to significantly reduce this bias. This suggests that for SDM, even semantically similar neutral prompts [32; 33] could result in different degrees of bias, thereby demonstrating certain instability in SDM. Next, our results in Fig. 4b compare the bias in StyleGAN2, StylesSwin, and the training CelebA-HQ dataset over an extended number of SAs. Overall, we found that while StyleSwin produces better quality samples [4], the same biases still remain statistically unchanged between the two architectures _i.e._ their IE overlap. Interestingly, our results also found that both the GANs were less biased than the training dataset itself.

## 7 Discussion

**Conclusion.** In this work, we address the limitations of the existing fairness measurement framework. Since generated samples are typically unlabeled, we first introduce a new labeled dataset based on three state-of-the-art generative models for our studies. Our findings suggest that the existing framework, which ignores classification inaccuracies, suffers from significant measurement errors, even when the SA classifier is very accurate. To rectify this, we propose CLEAM, which considers these inaccuracies in its statistical model and outputs a more accurate fairness measurement. Overall, CLEAM demonstrates improved accuracy over extensive experimentation, including both real

Figure 3: Comparing the point error \(e_{\mu}\) for Baseline and CLEAM when evaluating the bias of GenData-CelebA with ResNet-18, while varying sample size, \(n\).

generators and controlled setups. Moreover, by applying CLEAM to popular generative models, we uncover significant biases that raise efficacy concerns about these models' real-world application.

**Broader Impact.** Given that generative models are becoming more widely integrated into our everyday society text-to-image generation, it is important that we have reliable means to measure fairness in generative models, thereby allowing us to prevent these biases from proliferating into new technologies. CLEAM provides a step in this direction by allowing for more accurate evaluation. We remark that our work _does not introduce any social harm_ but instead improves on the already existing measurement framework.

**Limitations.** Despite the effectiveness of the proposed method along various generative models, our work addresses only one facet of the problems in the existing fairness measurement and there is still room for further improvement. For instance, it may be beneficial to consider SA to be non-binary when hair color is not necessary fully black (grey). Additionally, existing datasets used to train classifiers are commonly human-annotated, which may itself contain certain notions of bias. See Supp. I for further discussion.

## Acknowledgements

This research is supported by the National Research Foundation, Singapore under its AI Singapore Programmes (AISG Award No.: AISG2-TC-2022-007) and SUTD project PIE-SGP-AI-2018-01. This research work is also supported by the Agency for Science, Technology and Research (A*STAR) under its MTC Programmatic Funds (Grant No. M23L7b0021). This material is based on the research/work support in part by the Changi General Hospital and Singapore University of Technology and Design, under the HealthTech Innovation Fund (HTIF Award No. CGH-SUTD-2021-004). We thank anonymous reviewers for their insightful feedback and discussion.

Figure 4: Applying CLEAM to further assess the bias in popular generative models.

## References

* [1] Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, and Stefano Ermon. Fair Generative Modeling via Weak Supervision. In _Proceedings of the 37th International Conference on Machine Learning_, pages 1887-1898. PMLR, November 2020.
* [2] Christopher TH Teo, Milad Abdollahzadeh, and Ngai-Man Cheung. Fair generative models via transfer learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 2429-2437, 2023.
* [3] Tero Karras, Samuli Laine, and Timo Aila. A Style-Based Generator Architecture for Generative Adversarial Networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4401-4410, June 2019.
* [4] Bowen Zhang, Shuyang Gu, Bo Zhang, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang, and Baining Guo. Styleswin: Transformer-based gan for high-resolution image generation, 2021.
* [5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.
* [6] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision, February 2021.
* [7] Eric Frankel and Edward Vendrow. Fair Generation Through Prior Modification. _32nd Conference on Neural Information Processing Systems (NeurIPS 2018)_, June 2020.
* [8] Ahmed Imtiaz Humayun, Randall Balestriero, and Richard Baraniuk. MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining. In _International Conference on Learning Representations_, January 2022.
* [9] Shuhan Tan, Yujun Shen, and Bolei Zhou. Improving the Fairness of Deep Generative Models without Retraining. _arXiv:2012.04842 [cs]_, December 2020.
* [10] Ning Yu, Ke Li, Peng Zhou, Jitendra Malik, Larry Davis, and Mario Fritz. Inclusive GAN: Improving Data and Minority Coverage in Generative Models, August 2020.
* [11] Vongani H. Maluleke, Neerja Thakkar, Tim Brooks, Ethan Weber, Trevor Darrell, Alexei A. Efros, Angjoo Kanazawa, and Devin Guillory. Studying Bias in GANs through the Lens of Race, September 2022.
* [12] Soobin Um and Changho Suh. A Fair Generative Model Using Total Variation Distance. _openreview_, September 2021.
* [13] Milad Abdollahzadeh, Touba Malekzadeh, Christopher TH Teo, Keshigeyan Chandrasegaran, Guimeng Liu, and Ngai-Man Cheung. A survey on generative modeling with limited data, few shots, and zero shot. _arXiv preprint arXiv:2307.14397_, 2023.
* [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.
* [15] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. _Advances in neural information processing systems_, 33:12104-12114, 2020.
* [16] Yunqing Zhao, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and Ngai-Man Man Cheung. Few-shot image generation via adaptation-aware kernel modulation. _Advances in Neural Information Processing Systems_, 35:19427-19440, 2022.
* [17] Keshigeyan Chandrasegaran, Ngoc-Trung Tran, Alexander Binder, and Ngai-Man Cheung. Discovering transferable forensic features for cnn-generated images detection. In _European Conference on Computer Vision_, pages 671-689. Springer, 2022.
* [18] Yunqing Zhao, Chao Du, Milad Abdollahzadeh, Tianyu Pang, Min Lin, Shuicheng Yan, and Ngai-Man Cheung. Exploring incompatible knowledge transfer in few-shot image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7380-7391, 2023.

* Hutchinson and Mitchell [2019] Ben Hutchinson and Margaret Mitchell. 50 Years of Test (Un)fairness: Lessons for Machine Learning. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_, FAT* '19, pages 49-58, New York, NY, USA, January 2019. Association for Computing Machinery. ISBN 978-1-4503-6125-5. doi: 10.1145/3287560.3287600.
* Teo and Cheung [2021] Christopher T. H. Teo and Ngai-Man Cheung. Measuring Fairness in Generative Models. _38th International Conference on Machine Learning (ICML) Workshop_, July 2021.
* Jalan et al. [2020] Harsh Jaykumar Jalan, Gautam Maurya, Canute Corda, Sunny Dsouza, and Dakshata Panchal. Suspect Face Generation. In _2020 3rd International Conference on Communication System, Computing and IT Applications (CSCITA)_, pages 73-78, April 2020. doi: 10.1109/CSCITA47329.2020.9137812.
* Frid-Adar et al. [2018] Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit Greenspan. Gan-based synthetic medical image augmentation for increased cnn performance in liver lesion classification. _Neurocomputing_, 321:321-331, 2018.
* Mirza and Osindero [2014] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. _arXiv preprint arXiv:1411.1784_, 2014.
* Odena et al. [2017] Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxiliary classifier gans. In _ICML_, 2017.
* Thekumparampil et al. [2018] Kiran K Thekumparampil, Ashish Khetan, Zian Lin, and Sewoong Oh. Robustness of conditional GANs to noisy labels. In _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* Bianchi et al. [2022] Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale, November 2022.
* Lee et al. [2020] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.
* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.
* Liu and Chilton [2022] Vivian Liu and Lydia B Chilton. Design guidelines for prompt engineering text-to-image generative models. In _Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems_, pages 1-23, 2022.
* Diffusion [2022] Stable Diffusion: Prompt Guide and Examples. https://strikingloo.github.io/stable-diffusion-vs-dalle-2, August 2022.
* Zhou et al. [2022] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to Prompt for Vision-Language Models. _International Journal of Computer Vision_, 130(9):2337-2348, September 2022. ISSN 0920-5691, 1573-1405. doi: 10.1007/s11263-022-01653-1.
* Haspelmath [1997] Martin Haspelmath. _Indefinite Pronouns_. Oxford University Press, 1997. ISBN 978-0-19-829963-9 978-0-19-823560-6. doi: 10.1093/oso/9780198235606.001.0001.
* Saguy and Williams [2022] Abigail C Saguy and Juliet A Williams. A little word that means a lot: A reassessment of singular they in a new era of gender politics. _Gender & Society_, 36(1):5-31, 2022.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 770-778, June 2016.
* Sandler et al. [2018] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4510-4520, Salt Lake City, UT, June 2018. IEEE. ISBN 978-1-5386-6420-9. doi: 10.1109/CVPR.2018.00474.
* Simonyan and Zisserman [2014] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. _ICLR2015_, September 2014.
* Rao [1957] C Radhakrishna Rao. Maximum likelihood estimation for the multinomial distribution. _Sankhya: The Indian Journal of Statistics (1933-1960)_, 18(1/2):139-148, 1957.

* [38] Harry Kesten and Norman Morse. A property of the multinomial distribution. _The Annals of Mathematical Statistics_, 30(1):120-127, 1959.
* [39] Athanasios Papoulis. _Probability, Random Variables and Stochastic Processes with Errata Sheet_. McGraw-Hill Europe, Boston, Mass., 4th edition edition, January 2002. ISBN 978-0-07-122661-5.
* [40] Charles J Geyer. Stat 5101 Notes: Brand Name Distributions. _University of Minnesota_, Stat 5101:25, January 2010.
* [41] Nathaniel R Goodman. Statistical analysis based on a certain multivariate complex gaussian distribution (an introduction). _The Annals of mathematical statistics_, 34(1):152-177, 1963.
* [42] Milad Abdollahzadeh, Touba Malekzadeh, and Ngai-Man Man Cheung. Revisit multimodal meta-learning through the lens of multi-task learning. _Advances in Neural Information Processing Systems_, 35:14632-14644, 2021.
* [43] Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6430-6439, 2019.
* [44] Theodore Wilbur Anderson and Ingram Olkin. Maximum-likelihood estimation of the parameters of a multivariate normal distribution. _Linear algebra and its applications_, 70:147-171, 1985.
* [45] Renato A Krohling and Leandro dos Santos Coelho. Coevolutionary particle swarm optimization using gaussian distribution for solving constrained optimization problems. _IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)_, 36(6):1407-1416, 2006.
* [46] Vijay Keswani and L. Elisa Celis. Auditing for Diversity Using Representative Examples. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 860-870, Virtual Event Singapore, August 2021. ACM. ISBN 978-1-4503-8332-5. doi: 10.1145/3447548.3467433.
* [47] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. _Proceedings of International Conference on Computer Vision (ICCV)_, September 2015.
* [48] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. StarGAN v2: Diverse Image Synthesis for Multiple Domains, April 2020.
* [49] Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box predictors. In _Proceedings of the 35th International Conference on Machine Learning_, pages 3122-3130. PMLR, 10-15 Jul 2018.
* [50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-Image Generation, February 2021.
* [51] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators, December 2021.
* [52] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery, March 2021.
* [53] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do Vision Transformers See Like Convolutional Neural Networks?, March 2022.
* [54] Drew A. Hudson and Larry Zitnick. Generative Adversarial Transformers. In _Proceedings of the 38th International Conference on Machine Learning_, pages 4487-4499. PMLR, July 2021.
* ECCV 2022_, Lecture Notes in Computer Science, pages 709-727, Cham, 2022. Springer Nature Switzerland. ISBN 978-3-031-19827-4. doi: 10.1007/978-3-031-19827-4_41.