[MISSING_PAGE_FAIL:1]

meshed grids. This is often intractable as the domain of interest is, more often than not, irregular. An often taken trick for applying FNO to irregular domains is to embed the original domain into a larger rectangular domain and zero-pad or extrapolate on the redundant space (Lu et al., 2022). This poses two potential problems, one being the possible numerical errors and even instabilities due to the discontinuity at the original domain boundary (e.g., the Gibbs phenomenon (Gottlieb and Shu, 1997)) and the other, perhaps more importantly, being the fact that the padding/extrapolating techniques cannot handle domains with shallow gaps, as is the case in object contact and crack propagation problems. Meanwhile, another line of work emphasizes the learning of a diffeomorphic mapping between the original domain and a latent domain with uniform grids on which FNO can be applied (Li et al., 2022). However, in this approach the changes on the boundary and the domain topology can only be informed via the learned diffeomorphism, which results in approximation errors when tested on a new domain geometry and possible failure when a change in topology is involved on the domain geometry.

In this work, we aim to design FNO architectures that explicitly embed the boundary information of irregular domains, which we coin Domain Agnostic Fourier Neural Operator (DAFNO). This is inspired by the recent work in convolution-based peridynamics (Jafarzadeh et al., 2022) in which bounded domains of arbitrary shapes are explicitly encoded in the nonlocal integral formulation. We argue that, by explicitly embedding the domain boundary information into the model architecture, DAFNO is able to learn the underlying physics more accurately, and the learnt model is generalizable to changes on the domain geometry and topology. Concretely, we construct two practical DAFNO variants, namely, eDAFNO that inherits the explicit FNO architecture (Li et al., 2022) and iDAFNO that is built upon the implicit FNO (IFNO) architecture characterizing layer-independent kernels (You et al., 2022). Moreover, a boundary smoothening technique is also proposed to resolve the Gibbs phenomenon and retain the fidelity of the domain boundary. In summary, the primary contributions of the current work are as follows:

* We propose DAFNO, a novel Fourier neural operator architecture that explicitly encodes the boundary information of irregular domains into the model architecture, so that the learned operator is aware of the domain boundary, and generalizable to different domains of complicated geometries and topologies.
* By incorporating a (smoothened) domain characteristic function into the integral layer, our formulation resembles a nonlocal model, such that the layer update acts as collecting interactions between material points inside the domain and cuts the non-physical influence outside the domain. As such, the model preserves the fidelity of the domain boundary as well as the convolution form of the kernel that retains the computational efficiency of FFT.
* We demonstrate the expressivity and generalizability of DAFNO across a wide range of scientific problems including constitutive modeling of hyperelastic materials, airfoil design, and crack propagation in brittle fracture, and show that our learned operator can handle not only irregular domains but also topology changes over the evolution of the solution.

## 2 Background and related work

The goal of this work is to construct a neural network architecture to learn common physical models on various domains. Formally, given \(\mathcal{D}:=\{(\bm{g}_{i}|_{\Omega_{i}},\bm{u}_{i}|_{\Omega_{i}})\}_{i=1}^{N}\), a labelled set of function pair observations both defined on the domain \(\bm{x}\in\Omega_{i}\subset\mathbb{R}^{s}\). We assume that the input \(\{\bm{g}_{i}(\bm{x})\}\) is a set of independent and identically distributed (i.i.d.) random fields from a known probability distribution \(\mu\) on \(\mathcal{A}(\mathbb{R}^{d_{g}})\), a Banach space of functions taking values in \(\mathbb{R}^{d_{g}}\). \(\bm{u}_{i}(\bm{x})\in\mathcal{U}(\mathbb{R}^{d_{u}})\), possibly noisy, is the observed corresponding response taking values in \(\mathbb{R}^{d_{u}}\). Taking mechanical response modeling problem for example, \(\Omega_{i}\) is the shape of the object of interest, \(\bm{g}_{i}(\bm{x})\) may represent the boundary, initial, or loading conditions, and \(\bm{u}_{i}(\bm{x})\) can be the resulting velocity, pressure, or displacement field of the object. We assume that all observations can be modeled by a common and possibly unknown governing law, e.g., balance laws, and our goal is to construct a surrogate operator mapping, \(\tilde{\mathcal{G}}\), from \(\mathcal{A}\) to \(\mathcal{U}\) such that

\[\tilde{\mathcal{G}}[\bm{g}_{i};\theta](\bm{x})\approx\bm{u}_{i}(\bm{x}),\ \forall\bm{x}\in\Omega_{i}.\] (1)

Here, \(\theta\) represents the (trainable) network parameter set.

In real-world applications, the domain \(\Omega_{i}\) can possess different topologies, e.g., in contact problems (Benson and Okazawa, 2004; Simo and Laursen, 1992) and material fragmentation problems (De Luycker et al., 2011; Agwai et al., 2011; Silling, 2003), and/or evolve with time, as is the case in large-deformation problems (Shadden et al., 2010) and fluid-structure interaction applications (Kuhl et al., 2003; Kamensky et al., 2017). Hence, it is desired to develop an architecture with generalizability across various domains of complex shapes, so that the knowledge obtained from one geometry can be transferable to other geometries.

### Learning solution operators of hidden physics

In real-world physical problems, predicting and monitoring complex system responses are ubiquitous in many applications. For these purposes, physics-based PDEs and tranditional numerical methods have been commonly employed. However, traditional numerical methods are solved for specific boundary, initial, and loading conditions \(\bm{g}\) on a specific domain \(\Omega\). Hence, the solutions are not generalizable to other domains and operating conditions.

To provide a more efficient and flexible surrogate model for physical response prediction, there has been significant progress in the development of deep neural networks (NNs) and scientific machine learning models (Ghaboussi et al., 1998, 1991; Carleo et al., 2019; Karniadakis et al., 2021; Zhang et al., 2018; Cai et al., 2022; Pfau et al., 2020; He et al., 2021; Besnard et al., 2006). Among others, neural operators (Li et al., 2020, 2020, 2021; Liu et al., 2019, 2021; Goswami et al., 2022; Tripura and Chakraborty, 2022) show particular promise in learning physics of complex systems: compared with classical NNs, neural operators are resolution independent and generalizable to different input instances. Therefore, once the neural operator is trained, solving for a new instance of the boundary/initial/loading condition with a different discretization only requires a forward pass. These advantages make neural operators a useful tool to many physics modeling problems (Yin et al., 2022; Goswami et al., 2022; Yin et al., 2022; Li et al., 2020, 2020, 2021; Lu et al., 2022, 2021).

### Neural operator learning

Here, we first introduce the basic architecture of the general integral neural operators (Li et al., 2020, 2020, 2021, 2022), which are comprised of three building blocks. First, the input function, \(\bm{g}(\bm{x})\in\mathcal{A}\), is lifted to a higher-dimensional representation via \(\bm{h}^{0}(\bm{x})=\mathcal{P}[\bm{g}](\bm{x}):=P[\bm{x},\bm{g}(\bm{x})]^{T}+ \bm{p}\), where \(P\in\mathbb{R}^{(s+d_{g})\times d_{h}}\) and \(\bm{p}\in\mathbb{R}^{d_{h}}\) define an affine pointwise mapping. Then, the feature vector function \(\bm{h}^{0}(\bm{x})\) goes through an iterative layer block, where the layer update is defined via the sum of a local linear operator, a nonlocal integral kernel operator, and a bias function: \(\bm{h}^{l+1}(\bm{x})=\mathcal{J}^{l+1}[\bm{h}^{l}](\bm{x})\). Here, \(\bm{h}^{l}(\bm{x})\in\mathbb{R}^{d_{h}}\), \(l=0,\cdots,L\), is a sequence of functions representing the values of the network at each hidden layer. \(\mathcal{J}^{1},\cdots,\mathcal{J}^{L}\) are the nonlinear operator layers defined by the particular choice of networks. Finally, the output \(\bm{u}(\bm{x})\in\mathcal{U}\) is obtained via a projection layer by mapping the last hidden layer representation \(\bm{h}^{L}(\bm{x})\) onto \(\mathcal{U}\) as: \(\bm{u}(\bm{x})=\mathcal{Q}[\bm{h}^{L}](\bm{x}):=Q_{2}\sigma(Q_{1}\bm{h}^{L}( \bm{x})+\mathbf{q}_{1})+\mathbf{q}_{2}\). \(Q_{1}\), \(Q_{2}\), \(\mathbf{q}_{1}\) and \(\mathbf{q}_{2}\) are the appropriately sized matrices and vectors that are part of the learnable parameter set, and \(\sigma\) is an activation function (e.g., ReLU (He et al., 2018) or GeLU).

Then, the system response can be learnt by constructing a surrogate operator of (1): \(\tilde{\mathcal{G}}[\bm{g};\theta](\bm{x}):=\mathcal{Q}\circ\mathcal{J}^{1} \circ\cdots\circ\mathcal{J}^{L}\circ\mathcal{P}[\bm{g}](\bm{x})\approx\bm{u}( \bm{x})\), by solving the network parameter set \(\theta\) via an optimization problem:

\[\min_{\theta\in\Theta}\mathcal{L}_{\mathcal{D}}(\theta):=\min_{\theta\in\Theta }\sum_{i=1}^{N}[C(\tilde{\mathcal{G}}[\bm{g}_{i};\theta],\bm{u}_{i})]\.\] (2)

Here, \(C\) denotes a properly defined cost functional (e.g., the relative mean square error) on \(\Omega_{i}\).

### Fourier neural operators

The Fourier neural operator (FNO) is first proposed in Li et al. (2020) with its iterative layer architecture given by a convolution operator:

\[\mathcal{J}^{l}[\bm{h}](\bm{x}):= \sigma\bigg{(}W^{l}\bm{h}(\bm{x})+\bm{c}^{l}+\int_{\Omega}\kappa( \bm{x}-\bm{y};\bm{v}^{l})\bm{h}(\bm{y})d\bm{y}\bigg{)}\,\] (3)

where \(W^{l}\in\mathbb{R}^{d_{h}\times d_{h}}\) and \(\bm{c}^{l}\in\mathbb{R}^{d_{h}}\) are learnable tensors at the \(l\)-th layer, and \(\kappa\in\mathbb{R}^{d_{h}\times d_{h}}\) is a tensor kernel function with parameters \(\bm{v}^{l}\). When a rectangular domain \(\Omega\) with uniform meshes is considered, the above convolution operation can be converted to a multiplication operation through discrete Fourier transform:

\[\mathcal{J}^{l}[\bm{h}](\bm{x})= \sigma\left(W^{l}\bm{h}(\bm{x})+\bm{c}^{l}+\mathcal{F}^{-1}[ \mathcal{F}[\kappa(\cdot;\bm{v}^{l})]\cdot\mathcal{F}[\bm{h}(\cdot)]](\bm{x}) \right)\,\]here \(\mathcal{F}\) and \(\mathcal{F}^{-1}\) denote the Fourier transform and its inverse, respectively, which are computed using the FFT algorithm to each component of \(\bm{h}\) separately. The FFT calculations greatly improve the computational efficiency due to their quasilinear time complexity, but they also restrict the vanilla FNO architecture to rectangular domains \(\Omega\)(Lu et al., 2022).

To enhance the flexibility of FNO in modeling complex geometries, in Lu et al. (2022) the authors proposed to pad and/or extrapolate the input and output functions into a larger rectangular domain. However, such padding/extrapolating techniques are prone to numerical instabilities (Gottlieb and Shu, 1997), especially when the domain is concave and/or with complicated boundaries. As shown in Lu et al. (2022), the performance of dgFNO+ substantially deteriorates when handling complicated domains with notches and gaps. In Geo-FNO (Li et al., 2022), an additional neural network is employed and trained from data, to continuously map irregular domains onto a latent space of rectangular domains. As a result, the vanilla FNO can be employed on the rectangular latent domain. However, this strategy relies on the continuous mapping from the physical domain to a rectangular domain, hence it is restricted to relatively simple geometries with no topology change.

## 3 Domain Agnostic Fourier Neural Operators

In this section, we introduce Domain Agnostic Fourier Neural Operator (DAFNO), which features the generalizability to new and unseen domains of arbitrary shapes and different topologies. The key idea is to explicitly encode the domain information in the design while retaining the convolutional architecture in the iterative layer of FNOs. In what follows, we present the eDAFNO architecture based on the standard/explicit FNO model (Li et al., 2020), while the iDAFNO architecture based on the implicit FNO model (You et al., 2022) is provided in Appendix A.

Concretely, we enclose the physical domain of interest, \(\Omega\), by a (slightly larger) periodic box \(\mathbb{T}\), as shown in Figure 1. Next, we define the following domain characteristic function:

\[\chi(\bm{x})=\begin{cases}1&\bm{x}\in\Omega\\ 0&\bm{x}\in\mathbb{T}\setminus\Omega\end{cases},\] (4)

which encodes the domain information of different geometries. Inspired by Jafarzadeh et al. (2022), we incorporate the above-encoded domain information into the FNO architecture of (3), by multiplying the integrand in its convolution integral with \(\chi(\bm{x})\chi(\bm{y})\):

\[\mathcal{J}^{l}[\bm{h}]= \sigma\left(\int_{\mathbb{T}}\chi(\bm{x})\chi(\bm{y})\kappa(\bm {x}-\bm{y};\bm{v}^{l})(\bm{h}(\bm{y})-\bm{h}(\bm{x}))d\bm{y}+W^{l}\bm{h}(\bm{ x})+\bm{c}^{l}\right)\.\] (5)

Herein, we have followed the practice in You et al. (2022) and reformulated (3) to a nonlocal Laplacian operator, which is found to improve training efficacy. By introducing the term \(\chi(\bm{x})\chi(\bm{y})\), the integrand vanishes when either point \(\bm{x}\) or \(\bm{y}\) is positioned inside \(\Omega\) and the other is positioned outside. This modification eliminates any undesired interaction between the regions inside and outside of \(\Omega\). As a result, it tailors the integral operator to act on \(\Omega\) independently and is able to handle different domains and topologies. With this modification, the FFT remains applicable, since the convolutional structure of the integral is preserved and the domain of operation yet spans to the whole rectangular box \(\mathbb{T}\). In this context, (5) can be re-organized as:

\[\mathcal{J}^{l}[\bm{h}]= \sigma\left(\chi(\bm{x})(\int_{\mathbb{T}}\kappa(\bm{x}-\bm{y}; \bm{v}^{l})\chi(\bm{y})\bm{h}(\bm{y})d\bm{y}-\bm{h}(\bm{x})\int_{\mathbb{T}} \kappa(\bm{x}-\bm{y};\bm{v}^{l})\chi(\bm{y})d\bm{y}+W^{l}\bm{h}(\bm{x})+\bm{c} ^{l}\right)\right).\]

Note that multiplying \(W^{l}\bm{h}(\bm{x})+\bm{c}^{l}\) with \(\chi(\bm{x})\) does not alter the computational domain. Now that the integration region is a rectangular box \(\mathbb{T}\), the FFT algorithm and its inverse can be readily applied,

Figure 1: A schematic of extending an arbitrarily shaped domain \(\Omega\) to a periodic box \(\mathbb{T}\) and its discretized form in 2D.

and hence we can further express the eDAFNO architecture as:

\[\begin{split}\mathcal{J}^{l}[\bm{h}]:=& \sigma\bigg{(}\chi(\bm{x})\big{(}\mathcal{I}(\chi(\cdot)\bm{h}( \cdot);\bm{v}^{l})-\bm{h}(\bm{x})\mathcal{I}(\chi(\cdot);\bm{v}^{l})+W^{l}\bm{h }(\bm{x})+\bm{c}^{l}\big{)}\bigg{)}\,\\ \text{where}&\mathcal{I}(\circ;\bm{v}^{l}):= \mathcal{F}^{-1}\big{[}\mathcal{F}[\kappa(\cdot;\bm{v}^{l})]\cdot\mathcal{F}[ \circ]\big{]}\.\end{split}\] (6)

An illustration of the DAFNO architecture is provided in Figure 2. Note that this architectural modification is performed at the continuum level and therefore is independent of the discretization. Then, the box \(\mathbb{T}\) can be discretized with structured grids (cf. Figure 1), as is the case in standard FNO.

Although the proposed DAFNO architecture in (6) can handle complex generalization in domains, it has a potential pitfall: since the characteristic function is not continuous on the domain boundary, its Fourier series cannot converge uniformly and the FFT result would present fictitious wiggling near the discontinuities (i.e., the Gibbs phenomenon (Day et al., 1965)). As a consequence, the introduction of \(\chi\) can potentially jeopardize the computational accuracy. To improve the efficacy of DAFNO, we propose to replace the sharp characteristic function, \(\chi\), with a smoothed formulation:

\[\tilde{\chi}(\bm{x}):=\text{tanh}(\beta\text{dist}(\bm{x},\partial\Omega))( \chi(\bm{x})-0.5)+0.5\.\] (7)

Here, the hyperbolic tangent function \(\tanh(z):=\frac{\exp(z)-\exp(-z)}{\exp(z)+\exp(-z)}\), \(\text{dist}(\bm{x},\partial\Omega)\) denotes the (approximated) distance between \(\bm{x}\) and the boundary of domain \(\Omega\), and \(\beta\) controls the level of smoothness, which is treated as a tunable hyperparameter. An illustration of the effect of the smoothed \(\tilde{\chi}\) is displayed in Figure 3, with additional plots and prediction results with respect to different levels of smoothness, \(\beta\), provided in Appendix B.1. In what follows, the \(\cdot\) sign is neglected for brevity.

**Remark:** We point out that the proposed smoothed geometry encoding technique, although simple, is substantially different from existing function padding/extrapolation techniques proposed in Lu et al. (2022) who cannot handle singular boundaries (as in the airfoil tail of our example 2) nor notches/shallow gaps in the domain (as in the crack propagation of our example 3). Our proposed architecture in (5) is also more sophiscated than a trivial zero padding at each layer in that the characteristic function \(\chi(\bm{x})\) is multiplied with the integrand, whereas the latter breaks the convolutional structure and hinders the application of FFTs.

## 4 Numerical examples

In this section, we demonstrate the accuracy and expressivity of DAFNO across a wide variety of scientific problems. We compare the performance of DAFNO against other relevant scientific machine learning models, including FNO (Li et al., 2020), Geo-FNO (Li et al., 2022), IFNO (You et al., 2022), F-FNO (Tran et al., 2022), GNO (Li et al., 2020), DeepONet (Lu et al., 2019), and UNet (Ronneberger et al., 2015). In particular, we carry out three experiments on irregular domains, namely, constitutive modeling of hyperelasticity in material science, airfoil design in fluid mechanics, and crack propagation with topology change in fracture mechanics. For fair comparison, the hyperparameters of each model are tuned to minimize the error on validation datasets, including initial learning rate, decay rate for every 100 epochs, smoothing parameter, and regularization parameter, while the total number of epochs is restricted to 500 for computational efficiency. The

Figure 2: An illustration of the proposed DAFNO architecture. We start from the input function \(\bm{g}(x)\). After lifting, the iterative Fourier layers are built that explicitly embed the encoded domain information, \(\chi\). Lastly, we project the last hidden layer representation to the target function space.

[MISSING_PAGE_FAIL:6]

characteristic function as input (denoted as FNO/IFNO w/ smooth \(\chi\)). In this study, scenarios (b) and (c) aim to investigate the effects of our proposed boundary smoothing technique, and by comparing scenarios (a) and (c) we verify the effectiveness of encoding the boundary information in eDAFNO architecture. In addition, both FNO- and IFNO-based models are tested, with the purpose to evaluate the model expressivity when using layer-independent parameters in iterative layers. Three training dataset sizes (i.e., 10, 100, and 1000) are employed to explore the effect of the proposed algorithms on small, medium, and large datasets, respectively. The number of trainable parameters are reported in Table 1. Following the common practice as in Li et al. (2022), the hyperparameter choice of each model is selected by tuning the number of layers and the width (channel dimension) keeping the total number of parameters of the same magnitude.

The results of the ablation study are listed in Table 2. Firstly, by directly comparing the results of FNO (and IFNO) with mask and with smooth boundary encoding, one can tell that the boundary smoothing technique helps to reduce the error. This is supported by the observation that FNO and IFNO with smooth \(\chi\) consistently outperform their counterparts with mask in all data regimes, especially when a sufficient amount of data becomes available where a huge boost in accuracy can be achieved (by over 300%). On the other hand, by encoding the geometry information into the iterative layer, the prediction accuracy is further improved, where eDAFNO and iDAFNO outperform FNO and IFNO with smoothed \(\chi\) by 22.7% and 20.0% in large-data regime, respectively. Another interesting finding is that eDAFNO is 9.4% more accurate compared to iDAFNO in the large-data regime, although only a quarter of the total number of parameters is needed in iDAFNO due to its layer-independent parameter setting. This effect is less pronounced as we reduce the amount of available data for training, where the performance of iDAFNO is similar to that of eDAFNO in the small- and medium-data regimes. This is because iDAFNO has a much smaller number of trainable parameters and therefore is less likely to overfit with small datasets. Given that the performance of eDAFNO and iDAFNO is comparable, it is our opinion that both architectures are useful in different applications. In Figure 3, an example of the computational domain, the smoothened boundary encoding, the ground truth solution, and the eDAFNO prediction are demonstrated. To demonstrate the capability of prediction across resolutions, we train eDAFNO using data with 41\(\times\)41 grids then apply the model to provide prediction on 161\(\times\)161 grids-one can see that eDAFNO can generalize across different resolutions.

**Comparison against additional baselines** We further compare the performance of DAFNO against additional relevant baselines, including GNO, Geo-FNO, F-FNO, DeepONet, and UNet. Note that the results of GNO, DeepONet, and UNet are obtained using the same settings as in Li et al. (2022). Overall, the two DAFNO variants are significantly superior to other baselines in accuracy, with eDAFNO outperforming GNO, DeepONet, UNet, Geo-FNO, and F-FNO by 1088.9%, 975.0%, 399.3%, 111.7%, and 191.9%, respectively, in large-data regime. DAFNO is also more memory efficient compared to Geo-FNO (the most accurate baseline), as it foregoes the need for additional coordinate deformation network. As shown in Table 1, when the layer-independent parameter setting in iDAFNO is taken into account, DAFNO surpasses Geo-FNO by 407.4% in memory saving.

### Airfoil design

In this example, we investigate DAFNO's performance in learning transonic flow around various airfoil designs. Neglecting the effect of viscosity, the underlying physics can be described by the Euler equation:

\[\frac{\partial\rho}{\partial t}+\nabla\cdot(\rho\bm{v})=0\;,\;\;\;\frac{ \partial\rho\bm{v}}{\partial t}+\nabla\cdot(\rho\bm{v}\otimes\bm{v}+p\mathbb{ I})=0\;,\;\;\;\frac{\partial E}{\partial t}+\nabla\cdot((E+p)\,\bm{v})=0\;,\] (9)

Figure 4: An illustration on a test sample from the airfoil design problem. From left to right: an illustration of the discretized computational domain, the smoothed boundary encoding (i.e., smoothed \(\chi\)), the ground truth, and the eDAFNO prediction.

with \(\rho\), \(p\) and \(E\) being the fluid density, pressure and the total energy, respectively, and \(\bm{v}\) denoting the corresponding velocity field. The applied boundary conditions are: \(\rho_{\infty}=1\), Mach number \(M_{\infty}=0.8\), and \(p_{\infty}=1\) on the far field, with no penetration enforced on the airfoil. The dataset used for training is directly taken from Li et al. (2022), which consists of variations of the NACA-0012 airfoil and is divided into 1000, 100, 100 samples for training, validation and testing, respectively. For this problem, we aim to learn the resulting Mach number field based on a given mesh as input. An example of the computational domain, the smoothened boundary encoding, the ground truth, and the eDAFNO prediction is illustrated in Figure 4.

We report in Table 3 our experimental observations using eDAFNO and iDAFNO, along with the comparison against FNO, Geo-FNO, F-FNO and UNet, whose models are directly attained from Li et al. (2022); Tran et al. (2022). We can see that the proposed eDAFNO achieves the lowest error on the test dataset, beating the best result of non-DAFNO baselines by 24.9% and Geo-FNO by 63.9%, respectively. Additionally, iDAFNO reaches a similar level of accuracy with only a quarter of the total number of parameters employed, which is consistent with the findings in the previous example. With the properly trained DAFNO models, efficient optimization and inverse design are made possible.

Aside from the enhanced predictability, DAFNO can also be easily combined with the grid mapping technique in Geo-FNO to handle non-uniform grids. In particular, no modification on the model architecture is required, where one just needs to include an analytical or trainable mapping from

Figure 5: eDAFNO applied to the airfoil dataset on irregular grids. (a): the highly irregular and adaptive grids in the physical space is firstly deformed (via either an analytical mapping \(f\) or a trainable neural network for grid deformation) to uniform grids in the computational space, on which eDAFNO is applied to learn the underlying physics. The learned prediction is then converted back to the physical space via the inverse mapping \(f^{-1}\) or with another trainable neural network. (b): an illustration of the absolute error distribution of predictions in Geo-FNO, eDAFNO trained on uniform grids, and eDAFNO trained on irregular grids.

\begin{table}
\begin{tabular}{l l|c c} \hline \hline  & Model & Train error & Test error \\ \hline \multirow{2}{*}{Proposed model} & eDAFNO & 0.329\(\%\pm\)0.020\% & **0.596\(\%\pm\)0.005\%** \\  & iDAFNO & 0.448\(\%\pm\)0.012\% & 0.642\(\%\pm\)0.020\% \\  & eDAFNO on irregular grids & 0.331\(\%\pm\)0.003\% & 0.659\(\%\pm\)0.007\% \\ \hline \multirow{3}{*}{Baseline model} & Geo-FNO & 1.565\(\%\pm\)0.180\% & 1.650\(\%\pm\)0.175\% \\  & F-FNO & 0.566\(\%\pm\)0.066\% & 0.794\(\%\pm\)0.025\% \\ \cline{1-1}  & FNO w/ mask & 2.676\(\%\pm\)0.054\% & 3.725\(\%\pm\)0.108\% \\ \cline{1-1}  & UNet w/ mask & 2.781\(\%\pm\)1.084\% & 4.957\(\%\pm\)0.059\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results for the airfoil design problem, where bold numbers highlight the best method.

non-uniform/irregular mesh grids to uniform mesh grids. As a demonstration, we consider irregular grids of the airfoil dataset and use a pre-computed function to map irregular grids to regular grids. In the irregular grid set, we place more grid points near the airfoil to provide a better resolution near the important parts. The test error is provided in Table 3, where the test loss using the eDAFNO learned model is \(0.659\%\pm 0.007\%\), which is similar to the DAFNO results on uniform grids. In Figure 5, we demonstrate the irregular mesh and the absolute error comparison across Geo-FNO, DAFNO on regular grids, and DAFNO on irregular grids. One can see that, while both DAFNOs substantially outperform Geo-FNO, the error contours from eDAFNO with an irregular mesh show a smaller miss-match region near the airfoil, illustrating the flexibility of DAFNO in meshing and its capability in resolving fine-grained features.

### Crack propagation with topology change in domain

In this example, we aim to showcase DAFNO's capability in handling evolving domains by modeling crack propagation in brittle fracture. We emphasize that DAFNO represents the first neural operator that allows for learning with topology change. In the field of brittle fracture, a growing crack can be viewed as a change in topology, which corresponds to an evolving \(\chi(t)\) in the DAFNO architecture. In particular, we define the following time-dependent characteristic function:

\[\chi(\bm{x},t)=\begin{cases}1&\bm{x}\in\Omega(t)\\ 0&\bm{x}\in\mathbb{T}\setminus\Omega(t)\end{cases},\] (10)

where \(\Omega(t)\) denotes the time-dependent domain/evolving topology. Employing time-dependent \(\chi\) in (6) keeps the neural operator informed about the evolving topology. In general, the topology evolution rule that determines \(\Omega(t)\) can be obtained from a separate neural network or from physics as is the case in the current example. The high-fidelity synthetic data in this example is generated using 2D PeriFast software (Jarfarzadeh et al., 2022), which employs a peridynamics (PD) theory for modeling fracture (Bobaru et al., 2016). A demonstration of the evolving topology, as well as further details regarding the governing equations and data generation strategies, is provided in Appendix B.3.

In this context, we select eDAFNO as the surrogate model to learn the the internal force density operator. Specifically, let \(u_{1}(\bm{x},t)\), \(u_{2}(\bm{x},t)\) denote the two components of the displacement field \(\bm{u}\) at time \(t\), and \(L_{1}\), \(L_{2}\) be the two components of the internal force density \(\mathcal{L}[\bm{u}]\). Given \(u_{1}\), \(u_{2}\) and \(\chi\) as input data, we train two separate eDAFNO models to predict \(L_{1}\) and \(L_{2}\), respectively. Then, we substitute the trained surrogate models in the dynamic governing equation and adopt Velocity-Verlet time integration to update \(\bm{u}\) for the next time step, whereafter \(\Omega\) and \(\chi\) are updated accordingly.

The problem of interest is defined on a \(40\) mm \(\times\)\(40\) mm thin plate with a pre-crack of length 10 mm at one edge, which is subjected to sudden, uniform, and constant tractions on the top and bottom edges. Depending on the traction magnitude, crack grows at different speeds, may or may not bifurcate, and the final crack patterns can be of various shapes. Our training data is comprised of two parts, one consisting of 450 snapshots from a crack propagation simulation with a fixed traction magnitude of \(\sigma=4\) MPa, and the other consisting of randomized sinusoidal displacement fields and the corresponding \(L_{1}\), \(L_{2}\) fields computed by the PD operator. The sinusoidal subset contains 4,096 instances without fracture and is used to diversify the training data and mitigate overfitting on the crack data. For testing, we use the trained models in two scenarios with traction magnitudes different from what is used in training, which allows us to evaluate the generalizability of eDAFNO. Note that these tests are highly challenging for the trained models, as the predictions of the previous time step are used as the models' input in the current step, which leads to error accumulation as the simulation marches forward.

Figure 6 displays the test results on the crack patterns under different traction magnitudes, where the low traction magnitude (left column) shows a slow straight crack, the mid-level traction results in a crack with a moderate speed and a bifurcation event (middle column), and the highest traction leads to a rapid crack growth and initiates a second crack from the other side of the plate (right column). The trained eDAFNO model is able to accurately predict the left and right examples while only seeing the middle one, indicating that it has correctly learned the true constitutive behavior of the material and generalized well to previously unseen loading scenarios and the correspondingly changed domain topology. To further verify eDAFNO's generalizability to unseen geometries, in Figure 7 we compare eDAFNO with the baseline FNO w/ smoothed mask model, and plot their relative errors in \(\chi\) and \(\bm{u}\). As observed, the FNO predictions become unstable at a fairly early time in the two test scenarios,

since it creates a mask which is out of training distribution, while DAFNO can handle different crack patterns by hard coding it in the architecture, so the results remain stable for a much longer time.

## 5 Conclusion

We introduce two DAFNO variants to enable FNOs on irregular domains for PDE solution operator learning. By incorporating the geometric information from a smoothed characteristic function in the iterative Fourier layer while retaining the convolutional form, DAFNO possesses the computational efficiency from FFT together with the flexibility to operate on different computational domains. As a result, DAFNO is not only highly efficient and flexible in solving problems involving arbitrary shapes, but it also manifests its generalizability on evolving domains with topology change. In two benchmark datasets and a real-world crack propagation dataset, we demonstrate the state-of-the-art performance of DAFNO. We find both architectures helpful in practice: eDAFNO is slightly more accurate while iDAFNO is more computationally efficient and less overfitting with limited data.

**Limitation:** Due to the requirement of the standard FFT package, in the current DAFNO we focus on changing domains with uniformly meshed grids. However, we point out that this limitation can be lifted by using nonuniform FFT (Greengard & Lee, 2004) or an additional mapping for grid deformation, as shown in the airfoil experiment. Additionally, in our applications, we have focused on applying the same types of boundary conditions to the changing domains (e.g., all Dirichlet or all Neumann). In this context, another limitation and possible future direction would be on the transferability to PDEs with different types of boundary conditions.

Figure 6: Comparison of the fracture patterns with different loading scenarios between the high-fidelity solution and eDAFNO prediction.

Figure 7: Comparison of relative errors in the characteristic function \(\chi\) (left) and displacement fields (right) of the evolving topology predicted using the trained eDAFNO and FNO with mask at each time step for the training (loading = 4 MPa) and testing (loading = 2 MPa and 6 MPa) scenarios.

## Acknowledgments and Disclosure of Funding

S. Jafarzadeh would like to acknowledge support by the AFOSR grant FA9550-22-1-0197, and Y. Yu would like to acknowledge support by the National Science Foundation under award DMS-1753031 and the AFOSR grant FA9550-22-1-0197. Portions of this research were conducted on Lehigh University's Research Computing infrastructure partially supported by NSF Award 2019035.

## References

* Agwai et al. (2011) Agwai, A., Guven, I., and Madenci, E. Predicting crack propagation with peridynamics: a comparative study. _International journal of fracture_, 171(1):65, 2011.
* Benson & Okazawa (2004) Benson, D. J. and Okazawa, S. Contact in a multi-material Eulerian finite element formulation. _Computer Methods in Applied Mechanics and Engineering_, 193(39):4277-4298, 2004. ISSN 0045-7825.
* Besnard et al. (2006) Besnard, G., Hild, F., and Roux, S. "finite-element" displacement fields analysis from digital images: application to portevin-le chatelier bands. _Experimental mechanics_, 46(6):789-803, 2006.
* Bobaru et al. (2016) Bobaru, F., Foster, J. T., Geubelle, P. H., and Silling, S. A. _Handbook of peridynamic modeling_. CRC press, 2016.
* Brigham (1988) Brigham, E. O. _The fast Fourier transform and its applications_. Prentice-Hall, Inc., 1988.
* Cai et al. (2022) Cai, S., Mao, Z., Wang, Z., Yin, M., and Karniadakis, G. E. Physics-informed neural networks (PINNs) for fluid mechanics: A review. _Acta Mechanica Sinica_, pp. 1-12, 2022.
* Cao (2021) Cao, S. Choose a transformer: Fourier or galerkin. _Advances in neural information processing systems_, 34:24924-24940, 2021.
* Carleo et al. (2019) Carleo, G., Cirac, I., Cranmer, K., Daudet, L., Schuld, M., Tishby, N., Vogt-Maranto, L., and Zdeborova, L. Machine learning and the physical sciences. _Reviews of Modern Physics_, 91(4):045002, 2019.
* Day et al. (1965) Day, S. J., Mullineux, N., and Reed, J. Developments in obtaining transient response using fourier transforms: Part i: Gibbs phenomena and fourier integrals. _International Journal of Electrical Engineering Education_, 3(4):501-506, 1965.
* De Luycker et al. (2011) De Luycker, E., Benson, D. J., Belytschko, T., Bazilevs, Y., and Hsu, M.-C. X-FEM in isogeometric analysis for linear fracture mechanics. _International Journal for Numerical Methods in Engineering_, 87:541-565, 2011.
* Ghaboussi et al. (1991) Ghaboussi, J., Garrett Jr, J., and Wu, X. Knowledge-based modeling of material behavior with neural networks. _Journal of engineering mechanics_, 117(1):132-153, 1991.
* Ghaboussi et al. (1998) Ghaboussi, J., Pecknold, D. A., Zhang, M., and Haj-Ali, R. M. Autoprogressive training of neural network constitutive models. _International Journal for Numerical Methods in Engineering_, 42(1):105-126, 1998.
* Goswami et al. (2022) Goswami, S., Bora, A., Yu, Y., and Karniadakis, G. E. Physics-informed neural operators. _2022 arXiv preprint arXiv:2207.05748_, 2022.
* Gottlieb & Shu (1997) Gottlieb, D. and Shu, C.-W. On the gibbs phenomenon and its resolution. _SIAM review_, 39(4):644-668, 1997.
* Greengard & Lee (2004) Greengard, L. and Lee, J.-Y. Accelerating the nonuniform fast fourier transform. _SIAM review_, 46(3):443-454, 2004.
* Gupta et al. (2021) Gupta, G., Xiao, X., and Bogdan, P. Multiwavelet-based operator learning for differential equations. _Advances in Neural Information Processing Systems_, 34:24048-24062, 2021.
* Ha & Bobaru (2010) Ha, Y. D. and Bobaru, F. Studies of dynamic crack propagation and crack branching with peridynamics. _International Journal of Fracture_, 162(1):229-244, 2010.
* Huang & Li (2010)Hao, Z., Wang, Z., Su, H., Ying, C., Dong, Y., Liu, S., Cheng, Z., Song, J., and Zhu, J. Gnot: A general neural operator transformer for operator learning. In _International Conference on Machine Learning_, pp. 12556-12569. PMLR, 2023.
* He et al. (2018) He, J., Li, L., Xu, J., and Zheng, C. ReLu deep neural networks and linear finite elements. arXiv:1807.03973, 2018.
* He et al. (2021) He, Q., Laurence, D. W., Lee, C.-H., and Chen, J.-S. Manifold learning based data-driven modeling for soft biological tissues. _Journal of Biomechanics_, 117:110124, 2021.
* Jafarzadeh et al. (2022a) Jafarzadeh, S., Mousavi, F., and Bobaru, F. Perifast/dynamics: a matlab code for explicit fast convolution-based peridynamic analysis of deformation and fracture. 2022a.
* Jafarzadeh et al. (2022b) Jafarzadeh, S., Mousavi, F., Larios, A., and Bobaru, F. A general and fast convolution-based method for peridynamics: Applications to elasticity and brittle fracture. _Computer Methods in Applied Mechanics and Engineering_, 392:114666, 2022b.
* Kamensky et al. (2017) Kamensky, D., Hsu, M.-C., Yu, Y., Evans, J. A., Sacks, M. S., and Hughes, T. J. Immersogeometric cardiovascular fluid-structure interaction analysis with divergence-conforming b-splines. _Computer methods in applied mechanics and engineering_, 314:408-472, 2017.
* Karniadakis et al. (2021) Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., and Yang, L. Physics-informed machine learning. _Nature Reviews Physics_, 3(6):422-440, 2021.
* Kuhl et al. (2003) Kuhl, E., Hulshoff, S., and de Borst, R. An arbitrary Lagrangian Eulerian finite element approach for fluid-structure interaction phenomena. _International Journal for Numerical Methods in Engineering_, 57:117-142, 2003.
* Li et al. (2020a) Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. Neural operator: Graph kernel network for partial differential equations. _arXiv preprint arXiv:2003.03485_, 2020a.
* Li et al. (2020b) Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Stuart, A., Bhattacharya, K., and Anandkumar, A. Multipole graph neural operator for parametric partial differential equations. _Advances in Neural Information Processing Systems_, 33:NeurIPS 2020, 2020b.
* Li et al. (2020c) Li, Z., Kovachki, N. B., Azizzadenesheli, K., Bhattacharya, K., Stuart, A., and Anandkumar, A. Fourier Neural Operator for Parametric Partial Differential Equations. In _International Conference on Learning Representations_, 2020c.
* Li et al. (2022a) Li, Z., Huang, D. Z., Liu, B., and Anandkumar, A. Fourier neural operator with learned deformations for PDEs on general geometries. _arXiv preprint arXiv:2207.05209_, 2022a.
* Li et al. (2022b) Li, Z., Meidani, K., and Farimani, A. B. Transformer for partial differential equations' operator learning. _arXiv preprint arXiv:2205.13671_, 2022b.
* Liu et al. (2022) Liu, N., Yu, Y., You, H., and Tatikola, N. INO: Invariant neural operators for learning complex physical systems with momentum conservation. _arXiv preprint arXiv:2212.14365_, 2022.
* Lu et al. (2019) Lu, L., Jin, P., and Karniadakis, G. E. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. _arXiv preprint arXiv:1910.03193_, 2019.
* Lu et al. (2021a) Lu, L., He, H., Kasimbeg, P., Ranade, R., and Pathak, J. One-shot learning for solution operators of partial differential equations. _arXiv preprint arXiv:2104.05512_, 2021a.
* Lu et al. (2021b) Lu, L., Jin, P., Pang, G., Zhang, Z., and Karniadakis, G. E. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. _Nature Machine Intelligence_, 3(3):218-229, 2021b.
* Lu et al. (2022) Lu, L., Meng, X., Cai, S., Mao, Z., Goswami, S., Zhang, Z., and Karniadakis, G. E. A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair data. _Computer Methods in Applied Mechanics and Engineering_, 393:114778, 2022.

Ong, Y. Z., Shen, Z., and Yang, H. IAE-NET: Integral autoencoders for discretization-invariant learning. 03 2022. doi: 10.13140/RG.2.2.25120.87047/2.
* Pfau et al. (2020) Pfau, D., Spencer, J. S., Matthews, A. G., and Foulkes, W. M. C. Ab initio solution of the many-electron schrodinger equation with deep neural networks. _Physical Review Research_, 2(3):033429, 2020.
* Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In _International Conference on Medical image computing and computer-assisted intervention_, pp. 234-241. Springer, 2015.
* Shadden et al. (2010) Shadden, S. C., Astorino, M., and Gerbeau, J.-F. Computational analysis of an aortic valve jet with Lagrangian coherent structures. _Chaos_, 20:017512, 2010.
* Silling (2003) Silling, S. A. Dynamic fracture modeling with a meshfree peridynamic code. _Computational Fluid and Solid Mechanics_, 2003:641-644, 2003.
* Simo & Laursen (1992) Simo, J. C. and Laursen, T. A. An augmented Lagrangian treatment of contact problems involving friction. _Computers & Structures_, 42(1):97-116, 1992. ISSN 0045-7949.
* Tran et al. (2022) Tran, A., Mathews, A., Xie, L., and Ong, C. S. Factorized fourier neural operators. In _The Eleventh International Conference on Learning Representations_, 2022.
* Tripura & Chakraborty (2022) Tripura, T. and Chakraborty, S. Wavelet neural operator: a neural operator for parametric partial differential equations. _arXiv preprint arXiv:2205.02191_, 2022.
* Yin et al. (2022a) Yin, M., Ban, E., Rego, B. V., Zhang, E., Cavinato, C., Humphrey, J. D., and Em Karniadakis, G. Simulating progressive intramural damage leading to aortic dissection using DeepONet: an operator-regression neural network. _Journal of the Royal Society Interface_, 19(187):20210670, 2022a.
* Yin et al. (2022b) Yin, M., Zhang, E., Yu, Y., and Karniadakis, G. E. Interfacing finite elements with deep neural operators for fast multiscale modeling of mechanics problems. _Computer Methods in Applied Mechanics and Engineering, in press_, pp. 115027, 2022b.
* Yin et al. (2022c) Yin, Y., Kirchmeyer, M., Franceschi, J.-Y., Rakotomamonjy, A., et al. Continuous pde dynamics forecasting with implicit neural representations. In _The Eleventh International Conference on Learning Representations_, 2022c.
* You et al. (2022a) You, H., Yu, Y., D'Elia, M., Gao, T., and Silling, S. Nonlocal kernel network (NKN): A stable and resolution-independent deep neural network. _Journal of Computational Physics_, pp. arXiv preprint arXiv:2201.02217, 2022a.
* You et al. (2022b) You, H., Zhang, Q., Ross, C. J., Lee, C.-H., Hsu, M.-C., and Yu, Y. A physics-guided neural operator learning approach to model biological tissues from digital image correlation measurements. _Journal of Biomechanical Engineering, accepted for publication_, pp. arXiv preprint arXiv:2204.00205, 2022b.
* You et al. (2022c) You, H., Zhang, Q., Ross, C. J., Lee, C.-H., and Yu, Y. Learning deep implicit fourier neural operators (IFNOs) with applications to heterogeneous material modeling. _Computer Methods in Applied Mechanics and Engineering_, 398:115296, 2022c. doi: https://doi.org/10.1016/j.cma.2022.115296.
* Zhang et al. (2018) Zhang, L., Han, J., Wang, H., Car, R., and Weinan, E. Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics. _Physical Review Letters_, 120(14):143001, 2018.