# Soft ascent-descent as a stable and flexible alternative to flooding

Matthew J. Holland

Osaka University

&Kosuke Nakatani

Osaka University

Corresponding author.

###### Abstract

As a heuristic for improving test accuracy in classification, the "flooding" method proposed by Ishida et al. (2020) sets a threshold for the average surrogate loss at training time; above the threshold, gradient descent is run as usual, but below the threshold, a switch to gradient _ascent_ is made. While setting the threshold is non-trivial and is usually done with validation data, this simple technique has proved remarkably effective in terms of accuracy. On the other hand, what if we are also interested in other metrics such as model complexity or average surrogate loss at test time? As an attempt to achieve better overall performance with less fine-tuning, we propose a softened, pointwise mechanism called SoftAD (soft ascent-descent) that downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect of flooding, with no additional computational overhead. We contrast formal stationarity guarantees with those for flooding, and empirically demonstrate how SoftAD can realize classification accuracy competitive with flooding (and the more expensive alternative SAM) while enjoying a much smaller loss generalization gap and model norm.

## 1 Introduction

Modern machine learning makes use of sophisticated models that are trained through optimization of non-convex objective functions, which typically admit numerous local minima that make for natural candidates when taken at face value. While many such candidates are indeed essentially "optimal" from the viewpoint of classification error rates or other average losses incurred at training time, these often turn out to be highly sub-optimal in terms of _performance at test time_. It goes without saying that understanding and closing this gap is the problem of "generalization" that underlies most machine learning research (Jiang et al., 2020; Dziugaite et al., 2020; Johnson and Zhang, 2023).

When we are faced with multiple candidates which are essentially optimal and thus indistinguishable in terms of some "base" objective function (e.g., the average loss) at training time, one of best-known heuristics for identifying good candidates is that of the "landscape" or "geometry" of the base objective in a neighborhood around each candidate. Roughly speaking, one expects that candidates in regions which are in some sense "flat" (often said to be less "sharp") tend to perform better at test time. Strictly speaking, flatness is not necessary for generalization (Dinh et al., 2017), but our intuition can often be empirically verified to be correct, as good generalization is regularly observed in flat regions where the eigenvalues of the Hessian are mostly concentrated near zero (Chaudhari et al., 2017). The spectral density of the Hessian can in principle be used to evaluate sharpness, and has well-known links to norms that can be used for explicit regularization (Karakida et al., 2019), but for large-scale neural network training in practice, first-order approximations have shown the greatest utility. In particular, the sharpness-aware minimization (SAM) algorithm of Foret et al. (2021), extended for scale invariance by Kwon et al. (2021) and later captured as a special case of the gradient norm penalization (GNP) scheme of Zhao et al. (2022), has shown state-of-the-artperformance on a variety of deep learning tasks. All of these first-order procedures can be cast as (forward) finite-difference approximations of the curvature (Karakida et al., 2023), requiring at least double the computational cost of vanilla gradient descent (GD) at each iteration.

As an alternative approach, the "flooding" technique of Ishida et al. (2020) is worthy of attention for surprising improvements in test accuracy despite its apparent simplicity. Flooding is done as follows: fix a threshold \(\theta\) before training and run vanilla GD until the average loss goes below \(\theta\), and while below this threshold, run gradient _ascent_ rather than descent (see SS2.1 for details). Flooding appeared before SAM in the literature, but near the threshold, flooding can iterate between optimizing the empirical risk and the squared gradient norm (penalizing sharpness), establishing the former as an inexpensive alternative to the latter. On the other hand, it is not at all obvious how the flooding threshold \(\theta\) should be set given a particular data distribution and model class, and at present there is no methodology for settings which are "optimal" or at least "sufficient" from the viewpoint of test accuracy. More importantly, what if we are interested in performance criteria going beyond that of classification accuracy? Flooding just says "make the average loss as close to \(\theta\) as possible," and we hypothesize that this requirement is too weak to encourage low model complexity and/or good generalization in terms of losses, while also keeping test accuracy high.

In this work, we investigate the validity of this hypothesis, and consider the impact of making a stronger requirement, namely to ask the algorithm to "make sure the loss distribution is _well-concentrated_ near \(\theta\)." We show in SS3 that this can be implemented by introducing a smooth wrapper, applicable to any loss, which penalizes both over-performing and under-performing examples in a per-point fashion, instead of applying a hard threshold to the whole (mini-)batch as in flooding. We call this proposed procedure "soft ascent-decent" (SoftAD), and provide a detailed comparison with the flooding technique, highlighting the smoothness of SoftAD with implications in terms of formal stationarity guarantees, and emphasize how our mechanism leads to update directions that are qualitatively distinct from those used in flooding. Through rigorous empirical tests using both simulated and real-world benchmark classification datasets, featuring neural networks both large and small, we discover that compared with ERM, SAM, and flooding, the proposed SoftAD achieves far and away the smallest generalization error in terms of the base loss, while maintaining competitive accuracy and small model norms, without any explicit regularization.

Before diving into the main results just described, we introduce notation and background concepts in SS2. SoftAD is introduced in SS3, where we make basic empirical comparisons to flooding in SS3.1, and contrast formal guarantees of stationarity for these two methods in SS3.2. Our main empirical test results are given in SS4, with discussion and concluding remarks wrapping up the paper in SS5. All detailed proofs and supplementary empirical results are relegated to the appendix.

## 2 Background

To begin, we formulate performance metrics characterizing the problem of interest. Let \(\mathcal{W}\subseteq\mathbb{R}^{d}\) parameterize our hypothesis class, let \(\mathcal{Z}\) denote the set to which individual data points \(z\) belong. Let \(\mathsf{Z}\) represent a random (test) data point with distribution \(\mu\) over \(\mathcal{Z}\). For classification, where our data takes the form \(\mathsf{Z}=(\mathsf{X},\mathsf{Y})\) and for each \(w\in\mathcal{W}\) we let \(\hat{\mathsf{Y}}(w)\) denote the predicted label given \(\mathsf{X}\), the traditional performance metric of interest is the error probability at test time, denoted by

\[\mathcal{E}(w)\coloneqq\mathbf{P}\left\{\hat{\mathsf{Y}}(w)\neq\mathsf{Y} \right\}.\] (1)

When we refer to test _accuracy_, we mean the probability of correct prediction, namely \(1-\mathcal{E}(w)\). Even when high accuracy is desired, it is standard to make use of computationally congenial surrogate loss functions for training. Let \(\ell:\mathbb{R}^{d}\times\mathcal{Z}\to\mathbb{R}\) denote a generic loss function to be used for training. For example, if \(z=(x,y)\) represents (input, label) pairs, then a typical choice of \(\ell\) would be the cross-entropy loss. While the model is left abstract in our notation, note that for non-linear models such as neural networks, the mapping \(w\mapsto\ell(w;z)\) may be non-convex and non-smooth over \(\mathcal{W}\). As with the error probability (1), the expected loss (or "risk")

\[\mathrm{R}_{\mu}(w)\coloneqq\mathbf{E}_{\mu}\,\ell(w;\mathsf{Z})\] (2)

is also an important indicator of classifier performance. Both (1) and (2) are ideal quantities since \(\mu\) is unknown and \(\mathsf{Z}\) is not observed at training time. We assume the learning algorithm has access to an independent sample of \(n\) observations from \(\mu\), denoted by \(\mathbf{Z}_{n}\coloneqq(\mathsf{Z}_{1},\ldots,\mathsf{Z}_{n})\) for convenience. Traditional machine learning algorithms are driven by the empirical risk, denoted here by \(\mathsf{R}_{n}(w)\coloneqq(1/n)\sum_{i=1}^{n}\ell(w;\mathsf{Z}_{i})\), in that they seek out (local) minima of \(\mathsf{R}_{n}\). In this paper, we use the term _empirical risk minimization_ (_ERM_) to refer to algorithms that directly apply an optimizer to \(\mathsf{R}_{n}\). Under sophisticated models, the usual ERM objective \(\mathsf{R}_{n}(\cdot)\) tends to admit a complex landscape. As discussed in SS1, numerous alternatives to ERM have been proposed, with the aim of minimizing \(\mathcal{E}(\cdot)\) more reliably; next we take a closer look at one such technique, called "flooding."

### Flooding

The basic intuition underlying the proposal of Ishida et al. (2020) is that while minimizing \(\mathsf{R}_{n}(\cdot)\) may be sufficient for maximizing the training accuracy, it need not be _necessary_, and from the perspective of optimizing \(\mathcal{E}(\cdot)\), it may be worth it to sacrifice \(\mathsf{R}_{n}(\cdot)\) and even \(\mathrm{R}_{n}(\cdot)\). Fixing a threshold \(\theta\in\mathbb{R}\), the "flooded" objective is

\[\mathsf{F}_{n}(w;\theta)\coloneqq\theta+|\mathsf{R}_{n}(w)-\theta|.\] (3)

This objective can be implemented as a simple wrapper around typical loss functions, to which off-the-shelf gradient-based optimizers can be applied; running vanilla sub-gradient descent yields a sequence \((w_{1},w_{2},\ldots)\) generated using the update

\[w_{t+1}=w_{t}-\alpha\operatorname{sign}\left(\mathsf{R}_{n}(w_{t})-\theta \right)\nabla\mathsf{R}_{n}(w_{t})\] (4)

for all \(t\geq 1\), where \(\operatorname{sign}(x)\coloneqq x/|x|\) for all \(x\neq 0\) and \(\operatorname{sign}(0)\coloneqq 0\), and \(\alpha>0\) is a fixed step size. The update (4) characterizes what we call the _Flooding_ algorithm. From the above definitions, \(\mathsf{F}_{n}(w;\theta)\) is minimal if and only if \(\mathsf{R}_{n}(w)=\theta\). That is, Flooding seeks out \(w\) such that the training loss distribution induced by \((\ell(w;\mathsf{Z}_{1}),\ldots,\ell(w;\mathsf{Z}_{n}))\) has a mean which is close to \(\theta\); nothing more, nothing less.

### Links to sharpness

Assuming for now that the loss is differentiable, it has been appreciated for some time that the _distribution_ of the loss gradients \(\nabla\ell(w;\mathsf{Z})\) can convey important information about generalization (Zhang et al., 2020), and in particular the role of gradient regularization, both implicit and explicit, is receiving significant attention (Barrett and Dherin, 2021; Smith et al., 2021).2 As a concrete example of explicit regularization, consider modifying the ERM objective using the squared Euclidean norm as

Footnote 2: Throughout this paper, all gradients are taken with respect to \(w\mapsto\ell(w;z)\), assumed to exist on an open set containing \(\mathcal{W}\) for all \(z\in\mathcal{Z}\). It should however be noted that gradients taken with respect to parts of the data \(z\) have been used in objective function design for years; see Drucker and Le Cun (1992).

\[\widetilde{\mathsf{R}}_{n}(w;\lambda)\coloneqq\mathsf{R}_{n}(w)+\frac{\lambda }{2}\|\nabla\mathsf{R}_{n}(w)\|^{2}\] (5)

where \(\lambda\geq 0\) controls the degree of penalization. If one is to minimize this objective in \(w\) directly using gradient descent, this involves computing

\[\nabla\widetilde{\mathsf{R}}_{n}(w;\lambda)=\nabla\mathsf{R}_{n}(w)+\lambda \nabla^{2}\mathsf{R}_{n}(w)\left(\nabla\mathsf{R}_{n}(w)\right)\]

and thus doing matrix multiplication using a \(d\times d\) Hessian \(\nabla^{2}\mathsf{R}_{n}(w)\), an unattractive proposition when \(d\) is large. A linear approximation to the expensive term can be obtained via

\[\frac{\nabla\mathsf{R}_{n}(w+au)-\nabla\mathsf{R}_{n}(w)}{a}\approx\nabla^{2 }\mathsf{R}_{n}(w)\left(u\right)\]

where \(u\in\mathbb{R}^{d}\) is arbitrary and \(|a|\) is small; see Zhao et al. (2022, SS3.3) for example. Applying this to approximate \(\nabla\widetilde{\mathsf{R}}_{n}(w;\lambda)\), we have

\[\nabla\mathsf{R}_{n}(w)+\frac{\lambda}{a}\left(\nabla\mathsf{R}_{n}(w+a\nabla \mathsf{R}_{n}(w))-\nabla\mathsf{R}_{n}(w)\right)\approx\nabla\widetilde{ \mathsf{R}}_{n}(w;\lambda).\] (6)

The iterative update directions used by the _SAM_ algorithm of Foret et al. (2021) are captured by setting \(a=\lambda\), offering a nice link between loss-based sharpness control and gradient regularization. The extension of SAM in GNP (Zhao et al., 2022) can be expressed by an analogous derivation, replacingthe squared norm \(\|\cdot\|^{2}\) in (5) with \(\|\cdot\|\). Using updates of the form given in (6) with \(a>0\) is called a "forward" finite-difference (FD) approach to explicit gradient regularization (GR) (henceforth, \(\mathit{FD}\)-\(\mathit{GR}\)), and clearly requires two gradient calls per update.3 Better precision is available using "centered" FD, at the cost of additional gradient calls (Karakida et al., 2023). How does this all relate to Flooding? In repeating the update (4), once the empirical risk \(\mathsf{R}_{n}\) goes below \(\theta\) and the algorithm switches from ascent to descent, it is straightforward to show conditions where this leads to iteration between minimizing \(\mathsf{R}_{n}\) and \(\|\nabla\mathsf{R}_{n}(w)\|^{2}\). We give more details in SSB.1.

Footnote 3: In the current example, one call at \(w\), and another call at \(w+a\nabla\mathsf{R}_{n}(w)\).

## 3 Soft ascent-descent

With the context of SS2 in place, we consider making two qualitative changes to the Flooding update (4), described as follows.

1. **Pointwise thresholds:** Invert the order of applying \(\mathsf{R}_{n}(\cdot)\) and \(\mathrm{sign}(\cdot)\), i.e., do summation over data points _after_ per-loss truncation.
2. **Soft truncation:** Replace the hard threshold \(\mathrm{sign}(\cdot)\) with a continuous, bounded, monotonic (increasing) function \(\phi(\cdot)\) satisfying \(\phi(0)=0\).

The reason for making the thresholds pointwise is to allow the algorithm to view "ascent" and "descent" from the perspective of individual losses (rather than bundled up in \(\mathsf{R}_{n}\)), making it possible to utilize a sum of both ascent and descent update directions.4 To make this sum a _weighted_ sum, the soft truncation using \(\phi\) plays a key role. Keeping \(\phi\) bounded limits the impact of errant loss values, while the other assumptions allow for both ascent and descent, with "borderline" points near the threshold given _less_ weight. Written explicitly as an empirical objective function, we use

Footnote 4: This cannot be achieved by taking a mini-batch of size 1 when using Flooding, since the number of gradients summed over always equals the number of losses averaged when checking the ascent/descent condition.

\[\mathsf{S}_{n}(w;\theta)\coloneqq\theta+\frac{1}{n}\sum_{i=1}^{n}\rho(\ell(w; \mathsf{Z}_{i})-\theta)\] (7)

where once again \(\theta\in\mathbb{R}\) is a fixed threshold, and we set \(\rho(x)\coloneqq\sqrt{x^{2}+1}-1\). Running vanilla GD on \(\mathsf{S}_{n}(\cdot;\theta)\) in (7) yields the update

\[w_{t+1}=w_{t}-\frac{\alpha}{n}\sum_{i=1}^{n}\phi(\ell(w_{t};\mathsf{Z}_{i})- \theta)\nabla\ell(w_{t};\mathsf{Z}_{i}),\] (8)

with \(\phi(x)\coloneqq\rho^{\prime}(x)=x/\sqrt{x^{2}+1}\), and \(\alpha>0\) is once again a fixed step size. For convenience, we use _soft ascent-descent_ (or _SoftAD_ for short) to refer to the algorithm implied by the iterative update (8). Note that there is nothing particularly special about our choice of \(\rho\) here; it is just a simple algebraic function whose derivative also takes a simple form; note that the function \(\phi\) resulting from our choice of \(\rho\) satisfies the desired properties of continuity, boundedness, monotonicity, and \(\phi(0)=\rho^{\prime}(0)=0\).5 Note that for each point being summed over, \(\ell(w_{t};\mathsf{Z}_{i})>\theta\) implies descent while \(\ell(w_{t};\mathsf{Z}_{i})<\theta\) implies ascent, and borderline points with \(\ell(w_{t};\mathsf{Z}_{i})\approx\theta\) have a smaller relative impact. In contrast with Flooding, SoftAD requires that the training loss distribution induced by \((\ell(w;\mathsf{Z}_{1}),\ldots,\ell(w;\mathsf{Z}_{n}))\) be well-concentrated around \(\theta\), where the degree of dispersion is measured in a symmetric fashion using \(\rho\).

Footnote 5: There are many other possible candidates, but this is typical “smooth Huber function,” also known in the literature as pseudo-Huber, Charbonnier, and L1-L2 (Barron, 2019).

_Remark 1_ (Comparison with other variants of Flooding).: During the review phase for this work, we were made aware of another recent related work by Xie et al. (2022). Their proposed method is known as _iFlood_, and it is essentially a middle ground between our proposal above and the original Flooding procedure. They use pointwise thresholds as we do in SoftAD, but retain the hard ascent-descent switch as in Flooding. More concisely, replacing our soft truncated \(\phi(x)\) in (8) with the absolute value \(|x|\) yields the iFlood update. We have added empirical test results for iFlood to complement our original experiments at the end of SSC.2. Another very recent variant is _\(AdaFlood\)_(Bae et al., 2023), which sets the \(\theta\) threshold level individually for each point based on "difficulty" as evaluated using an auxiliary model.

_Remark 2_ (Difference from OCE-like criteria).: At first glance, our objective \(\mathsf{S}_{n}(w;\theta)\) in (7) may appear similar to the criteria used in OCE risk minimization (Lee et al., 2020; Li et al., 2021) and some varieties of DRO (Duchi and Namkoong, 2021). Aside from the obvious difference that \(\theta\) is fixed, rather than optimized alongside \(w\), the critical difference here is that our \(\rho(\cdot)\) is _not_ monotonic. Losses which are too large and too small are _both_ penalized. It is precisely this bi-directional property that allows for switching between ascent and descent; this is impossible to achieve with monotonic OCE and DRO risks (Holland, 2023; Holland and Tanabe, 2023; Hu et al., 2023; Royset, 2024). This bi-directional criterion can also be used as a straightforward method to provably avoid unintentional "collapse" into ERM solutions (Holland, 2024).

### Initial comparison with Flooding

To develop some intuition for our SoftAD (8) and the Flooding update (4), we carry out a few illustrative numerical experiments. To start, let us consider a simple, non-stochastic example in one dimension. Letting \(f:\mathbb{R}\rightarrow\mathbb{R}\) be some differentiable function, we consider how the transformed gradient \(\phi(f(x)-\theta)f^{\prime}(x)\) behaves under \(\phi=\mathrm{sign}\) and \(\phi=\rho^{\prime}\). In Figure 1, we give a numerical example using a quadratic function. The softened nature of the transformed gradients used in SoftAD is clear when compared with the hard switching mechanism underlying the Flooding update. In Figure 2, we continue the quadratic function example, looking at sequences \((x_{1},x_{2},\ldots)\) generated based on the Flooding and SoftAD procedures. That is, instead of \(n\) points from which to compute losses, we have just one "loss," namely \(f(x)=x^{2}/2\). Both procedures realize an effective "flood level" of sorts (i.e., a buffer around the loss minimizer), but as expected, the Flooding procedure tends to be far more "jagged" in its trajectory.

Figure 1: The left-most figure simply plots the graph of \(f(x)=x^{2}/2\) over \(x\in[-2,2]\). The two remaining figures show plots of the graphs of \(f^{\prime}(x)=x\) (dashed black line) and \(\phi((f(x)-\theta)/\sigma)f^{\prime}(x)\) for the same range of \(x\) values, with colors corresponding to modified values of \(\sigma\) (middle plot; \(\theta=0.5\) fixed) and \(\theta\) (right-most plot; \(\sigma=1.0\) fixed) respectively. Thick dotted lines are \(\phi=\mathrm{sign}\), thin solid lines are \(\phi=\rho^{\prime}\).

Figure 2: Gradient descent on the quadratic example from Figure 1. The horizontal axis denotes iteration number, and we plot sequences of iterates \((x_{t})\) and function values \((f(x_{t}))\) for each method. Here “GD” denotes vanilla gradient descent, with “Flood” and “SoftAD” corresponding to (4) and (8) respectively. Step size is \(\alpha=0.1\).

Finally, a simple example to illustrate how the per-point soft thresholding of SoftAD leads to distinct gradient-based update directions when compared to the Flooding strategy. Here we consider a dataset of \(n\) data points \(z_{1},\ldots,z_{n}\in\mathbb{R}^{2}\), and use the squared Euclidean norm as a loss, i.e., \(\ell(w;z)=\|w-z\|^{2}\). This is a natural extension of the quadratic example in the previous paragraph, to multiple points and two dimensions. In Figure 3, we look at two candidate points, and compute the Flooding and SoftAD update directions that arise at each candidate under a randomly generated dataset. We can clearly see how the Flooding update plunges directly towards the minimizer of \(\mathsf{R}_{n}\), unless it is too close (given threshold \(\theta\)), in which case it goes in the opposite direction. In contrast, the SoftAD update is composed of per-point update directions, some which attract toward the minimum, and some which repel from the minimum, with borderline points down-weighted (shorter update arrows). Since the final update averages over these, movement both toward and away from the minimum is clearly "softened" when compared with the Flooding updates.

### Comparison of convergence properties

With the Flooding and SoftAD methods in mind, next we consider concrete conditions under which stochastic gradient-based learning algorithms can be given guarantees of (approximate) stationarity. Throughout this section, we assume that the loss \(w\mapsto\ell(w;z)\) is locally Lipschitz on \(\mathcal{W}\) for each \(z\in\mathcal{Z}\), but convexity will not be used. Let us assume for simplicity that \((\mathsf{Z}_{1},\mathsf{Z}_{2},\ldots)\) is a sequence of independent random variables with distribution \(\mu\), the same as our test data point \(\mathsf{Z}\sim\mu\).

Starting with SoftAD, we are interested in procedures fuelled by stochastic gradients of the form

\[\mathsf{G}_{t}(w)\coloneqq\phi(\ell(w;\mathsf{Z}_{t})-\theta)\nabla\ell(w; \mathsf{Z}_{t})\] (9)

for all integers \(t\geq 1\) and \(w\in\mathcal{W}\), with threshold \(\theta\in\mathbb{R}\) fixed in advance, and \(\phi=\rho^{\prime}\) as before. Recalling the empirical SoftAD objective \(\mathsf{S}_{n}\) in (7), the underlying population objective is

\[\mathsf{S}_{\mu}(w)\coloneqq\theta+\mathbf{E}_{\mu}\,\rho(\ell(w;\mathsf{Z})- \theta).\] (10)

By design, we do not expect SoftAD to approach a stationary point of the original \(\mathrm{R}_{\mu}\). Note that under mild assumptions on the data distribution, we have an unbiased estimator with \(\mathbf{E}_{\mu}\,\mathsf{G}_{t}(w)=\nabla\mathsf{S}_{\mu}(w)\), suggesting stationarity in terms of \(\mathsf{S}_{\mu}(\cdot)\) as a natural goal. Assuming the losses are \(L_{\ell}\)-smooth in expectation, one can readily confirm that the objective (10) is \(L_{\text{AD}}\)-smooth, with

\[L_{\text{AD}}\coloneqq\mathbf{E}_{\mu}\left[\sup_{w\in\mathcal{W}}\|\nabla \ell(w;\mathsf{Z})\|^{2}\right]+L_{\ell}.\] (11)

A more detailed derivation is given in SSB.5. Assuming that second-order moments are finite in a uniform sense over \(\mathcal{W}\) ensures that \(L_{\text{AD}}<\infty\), and naturally implies pointwise variance bounds, allowing us to seek out stationarity guarantees using a combination of gradient norm control and momentum in the fashion of Cutkosky and Mehta (2021).

Figure 3: _Left:_ We randomly sample \(n=8\) points (black dots) from the 2D Gaussian distribution, zero mean, zero correlations, with standard deviation \(2\sqrt{2}\) in each coordinate. The two candidates are denoted by square-shaped points (red and green), and the minimizer of \(\mathsf{R}_{n}\) is given by a gold star. _Center:_ The Flooding updates (colored arrows) via (4) for each candidate. _Right:_ Analogous SoftAD update vectors via (8), with per-point transformed gradients (semi-transparent arrows) for reference. Throughout, we have fixed \(\theta=1.5\times\min_{w}\mathsf{R}_{n}(w)\) and \(\alpha=0.75\).

**Proposition 3** (Stationarity for SoftAD, smooth case).: _Starting with an arbitrary \(w_{1}\in\mathcal{W}\), update using \(w_{t+1}=w_{t}-\alpha\mathsf{M}_{t}/\|\mathsf{M}_{t}\|\), where \(\mathsf{M}_{t}:=b\mathsf{M}_{t-1}+(1-b)\overline{\mathsf{G}}_{t}(w_{t})\) for \(t\geq 1\), with \(\mathsf{M}_{0}:=0\) and \(\overline{\mathsf{G}}_{t}(\cdot)\coloneqq\mathsf{G}_{t}(\cdot)\min\{1,\gamma/ \|\mathsf{G}_{t}(\cdot)\|\}\), taking each gradient \(\mathsf{G}_{t}(\cdot)\) based on (9). Assuming we make \(T-1\) updates, set the momentum parameter to \(b=1-1/\sqrt{T}\), the norm threshold to \(\gamma=\sqrt{(L_{\text{AD}}-L_{\ell})/(1-b)}\), and the step size to \(\alpha=1/T^{3/4}\). The stationarity of this sequence \((w_{1},w_{2},\ldots)\), assumed to be in \(\mathcal{W}\), in terms of the modified objective (10) can be bounded by_

\[\frac{1}{T}\sum_{t=1}^{T}\lVert\nabla\mathrm{S}_{\mu}(w_{t})\rVert\leq\frac{1 }{T^{1/4}}\left(\mathrm{S}_{\mu}(w_{1})-\mathrm{S}_{\mu}(w_{T+1})+\frac{3L_{ \text{AD}}}{2}+2\sqrt{L_{\text{AD}}-L_{\ell}}\left(1+C_{\delta}\right)\right)\]

_using confidence-dependent factor \(C_{\delta}:=10\log(3T/\delta)+4\sqrt{\log(3T/\delta)}+1\), with probability no less than \(1-\delta\) over the draw of \((\mathsf{Z}_{1},\mathsf{Z}_{2},\ldots)\)._

For comparison, we next consider stationarity of the Flooding algorithm in the same context of smooth losses. The argument used in Proposition 3 critically depends on the smoothness of the underlying objective (10). Unfortunately, this smoothness cannot be leveraged when we consider the population objective underlying the Flooding procedure, namely the function

\[w\mapsto\theta+|\mathrm{R}_{\mu}(w)-\theta|.\] (12)

Further complicating things is the fact that stochastic gradients of the form (9) with \(\phi(\cdot)\) replaced by \(\mathrm{sign}(\cdot)\) do not yield unbiased sub-gradient estimates for (12), but rather for an upper bound \(\theta+\mathbf{E}_{\mu}|\ell(w;\mathsf{Z})-\theta|\) that follows via Jensen's inequality. A lack of smoothness means we cannot establish stationarity in terms of (12) nor the upper bound just given, but it is possible using a smoothed approximation (the Moreau envelope) of this bound:

\[\overline{\mathrm{F}}_{\mu}(w)\coloneqq\inf_{v\in\mathcal{W}}\left[\theta+ \mathbf{E}_{\mu}|\ell(v;\mathsf{Z})-\theta|+\frac{1}{2\beta}\lVert v-w\rVert^ {2}\right].\] (13)

The parameter \(\beta>0\) controls the degree of smoothness. The objective in (13) can be linked to "gradients" in (9) with \(\phi=\mathrm{sign}\), and leveraging the Lipschitz continuity of \(|\cdot|\) along with a sufficiently smooth loss, it is possible to show that this non-smooth objective satisfies a weak form of convexity, and using the techniques of Davis and Drusvyatskiy (2019) it is possible to show that stochastic gradient algorithms enjoy stationarity guarantees, albeit not in terms of the objective (12), but rather the smoothed upper bound (13).

**Proposition 4** (Stationarity for Flooding, smooth case).: _Letting \(\mathcal{W}\) be closed and convex, take an initial point \(w_{1}\in\mathcal{W}\), and make \(T-1\) updates using \(w_{t+1}=\Pi_{\mathcal{W}}[w_{t}-\alpha\mathsf{G}_{t}(w_{t})]\), where \(\Pi_{\mathcal{W}}[\cdot]\) denotes projection to \(\mathcal{W}\), and each \(\mathsf{G}_{t}(\cdot)\) is computed using (9) with \(\phi=\mathrm{sign}\). Assuming the loss \(w\mapsto\ell(w;z)\) is \(L_{\ell}^{*}\)-smooth on \(\mathcal{W}\) for all \(z\in\mathcal{Z}\), and taking a step size of_

\[\alpha^{2}=\frac{\Delta}{TL_{\ell}^{*}(L_{\text{AD}}-L_{\ell})},\text{ using }\Delta\text{ such that }\Delta\geq\overline{\mathrm{F}}_{\mu}(w_{1})-\inf_{w\in\mathcal{W}}\overline{ \mathrm{F}}_{\mu}(w),\]

_with \(L_{\text{AD}}\) and \(L_{\ell}\) as in (11), the expected squared stationarity in terms of the smoothed upper bound (13) at smoothness level \(\beta=1/(2L_{\ell}^{*})\) can be controlled as_

\[\frac{1}{T}\sum_{t=1}^{T}\mathbf{E}\lVert\nabla\overline{\mathrm{F}}_{\mu}(w_{ t})\rVert^{2}\leq\sqrt{\frac{2L_{\ell}^{*}(L_{\text{AD}}-L_{\ell})\Delta}{T}}\]

_with expectation taken over the draw of \((\mathsf{Z}_{1},\mathsf{Z}_{2},\ldots)\)._

_Remark 5_ (Comparing rates and assumptions).: Considering the preceding Propositions 3 and 4, one common point is that learning algorithms based on both the SoftAD and Flooding gradients (of mini-batch size \(1\)) can be shown to be approximately stationary in terms of functions of a similar form (i.e., (10) and (12)), differing only in how they measure deviations from the threshold \(\theta\). The rates of decrease (as a function of \(T\)) are essentially the same, noting that the bounds in Proposition 4 are in terms of _squared_ norms. That said, a lack of smoothness means the Flooding guarantees only hold for a smoothed variant, plus they require a stronger form of smoothness in the loss (over all \(z\) vs. in expectation). In addition, the SoftAD guarantees hold with high probability over the data sample, and can be readily strengthened to hold for an individual iterate (instead of summing over \(T\) iterates), using for example the technique of Cutkosky and Mehta (2021, Thm. 3).

## 4 Empirical study

In this section, we apply the proposed SoftAD procedure to a variety of classification tasks using neural network models, leading to losses that are non-convex and non-smooth. Our goal here is to compare and contrast the behavior and performance (accuracy, average loss, model norm) of SoftAD with three natural alternatives: ERM, Flooding, and SAM.6

Footnote 6: To re-create all of the numerical test results and figures from this paper, source code and Jupyter notebooks are available at a public GitHub repository: https://github.com/feedbackward/bdd-flood.

### Overview of experiments

Our core experiments are centered around re-creating the tests done by Ishida et al. (2020, SS4.1, SS4.2) and Foret et al. (2021, SS3.1) to include all four methods of interest. There are two main parts: simulation-based tests and real benchmark-based tests. We briefly describe the setup of each below.

Non-linear binary classification on the planeWe use three synthetic data-generators ("two Gaussians," "sinusoid," and "spiral," see Figure 7) to create a dataset on the 2D plane that is not linearly separable, but separable using relatively simple non-linear models. We treat the underlying model as unknown, and approximate it using a shallow feedforward neural network. All four methods of interest (ERM, Flooding, SAM, and SoftAD) are driven by the Adam optimizer, with the cross-entropy loss used as the base loss function. Complete experimental details are provided in SSC.1.

Image classification from scratchOur second set of experiments utilizes four well-known benchmark datasets for multi-class image classification. Compared to the synthetic experiments, the classification task is more difficult (much larger inputs, variation within classes, more classes), and so we utilize more sophisticated neural network models to tackle the classification task. That said, as the sub-section title indicates, this training is done "from scratch," i.e., no pre-trained models are used. The datasets we use are all standard benchmarks in the machine learning community: CIFAR-10, CIFAR-100, FashionMNIST, and SVHN. Model choice essentially mirrors that of Ishida et al. (2020, SS4.2). For FashionMNIST, we flatten each image into a vector, and use a simple feed-forward neural network with one hidden layer. For SVHN, we use ResNet-18 as implemented in torchvision.models, without any pre-trained weights. Finally, for both CIFAR-10 and CIFAR-100, we use ResNet-34 (again in torchvision.models) without pre-training. For the optimizer, we use vanilla SGD with a fixed step size. Full details are given in SSC.2 in the appendix.

### Main findings

Uniformly small loss generalization gapOne of the most lucid results we obtained is that SoftAD shows the smallest _loss_ generalization gap of all the methods studied, across all models and datasets used. In Table 1, we show the gaps incurred under each dataset. More precisely, for each trial and each epoch, we compute the average cross-entropy loss on test and training (less validation) datasets, and then respective average both of these over all trials. The difference of these two values (i.e., trial-averaged test loss minus trial-averaged training loss) after the final epoch of training is the value shown in each cell of the table.

Balance of accuracy and loss on real dataIn the previous paragraph we noted that SoftAD has superior loss gaps, but this is not much to celebrate if performance in terms of the key metrics of interest (i.e., test loss and test accuracy) is poor. In Figures 4-5, we show the trajectory of loss and accuracy (for both training and test data) over epochs run (averaged over trials). All four methods

\begin{table}
\begin{tabular}{|c||c|c|c|c|c|c|c|} \hline  & Gaussian & Sinusoid & Spiral & CIFAR-10 & CIFAR-100 & Fashion & SVHN \\ \hline \hline ERM & 0.080 & 0.150 & 0.297 & 3.265 & 7.603 & 0.801 & 0.762 \\ \hline Flood & 0.011 & 0.058 & 0.119 & 1.239 & 3.114 & 0.436 & 0.436 \\ \hline SAM & 0.024 & 0.096 & 0.154 & 1.512 & 3.672 & 0.639 & 0.493 \\ \hline SoftAD & **0.004** & **0.016** & **0.087** & **1.168** & **2.701** & **0.422** & **0.362** \\ \hline \end{tabular}
\end{table}
Table 1: Generalization gap (test - training) for trial-averaged cross entropy loss after final epoch.

are comparable in terms of accuracy, with SAM (at double the gradient cost) coming out slightly ahead. On the other hand, there is significant divergence between the different methods in terms of test loss. For each dataset, SoftAD achieves a superior test loss, often converging faster than any of the other methods; this may be a natural by-product of the fact that SoftAD is designed to ensure losses are _well-concentrated_ around threshold \(\theta\), instead of just asking that their mean get close to \(\theta\) (as in Flooding). While there is clearly a major difference between ERM and the other three methods, the stable nature of the "double descent" in SoftAD is quite stark compared with Flooding and SAM.

Uniformly smaller model normsWe do not do any explicit model regularization (e.g., L2 norm penalization) in our experiments here, and we only use fixed step-size parameters for Adam and SGD, so as we run for many iterations, the norm of the model weight parameters tends to grow. While this property holds across all methods tested here, we find that under all datasets and models tested, SoftAD uniformly results in the smallest model norm; see Figure 6 for trajectories over epochs for each benchmark dataset. The "Model norm" values plotted here are the L2 norm of all the model parameters (neural network weights) concatenated into a single vector, and these norm values are averaged over trials. Completely analogous trends hold for the simulated datasets as well.

Trends in hyperparameter selectionAside from ERM, the three key methods of interest (Flooding, SAM, SoftAD) each have one hyperparameter. Flooding and SoftAD have the threshold \(\theta\) as described

Figure 4: Trajectories over epochs for average test loss (top row) and test accuracy (bottom row). Horizontal axis is epoch number. Columns are associated with the CIFAR-10 and CIFAR-100 datasets (left to right).

Figure 5: Analogous to Figure 4, but with FashionMNIST and SVHN datasets.

in SS2-SS3, and SAM has the radius parameter (denoted by "\(\rho\)" in the original paper). In all tests, we select a representative candidate for each method with hyperparameters by using validation data held out from the training data, and in Table 2 we show the average and standard deviation of the validation-based hyperparameters over randomized trials (see SSC for exact hyperparameter grid values). One clear take-away from this table is that the "best" value of \(\theta\) (in terms of accuracy) for SoftAD tends to be _larger_ than that for Flooding, and this trend is uniform across all datasets, both simulated and real. In particular for the real benchmark datasets, it is interesting to note that while a larger threshold \(\theta\) (applied to _loss_ distribution) is selected for SoftAD, the resulting test loss value achieved is actually smaller/better than that achieved by Flooding (top row of Figures 4-5).

## 5 Limitations and concluding remarks

While previous work had already shown that it is possible to sacrifice performance in terms of losses to improve accuracy, the nature of that tradeoff was left totally unexplored, and in SS1 we put forward the hypothesis that simply asking the empirical loss mean to get close to a non-zero threshold \(\theta\), as in Flooding, would not be enough to realize a competitive tradeoff over varied learning tasks (datasets, models). Our main take-away is that we have empirical evidence that the slightly stronger requirement of "_losses well-concentrated around \(\theta\)_" (implemented as SoftAD) can result in an appealing balance of average test loss and accuracy, with the added benefit of a strong (implicit) regularization effect, likely due to the soft dampening effect on borderline points. A more formal theoretical understanding of this regularization effect is of interest, as are empirical studies going far beyond the limited choice of loss functions used here. Our biggest limitation is that the question of "how to set the threshold \(\theta\)?" still remains without an answer. Any meaningful answer will likely require some user judgement regarding tradeoffs between performance metrics. One potential first approach would be to leverage recent techniques for estimating the Bayes error (Ishida et al., 2023), combined with existing surrogate theory (Bartlett et al., 2006) to reverse-engineer a loss threshold given a user-specified "tolerable" drop in accuracy, for example.

\begin{table}
\begin{tabular}{|c||c|c|c|c|c|c|c|} \hline  & Gaussian & Sinusoid & Spiral & CIFAR-10 & CIFAR-100 & Fashion & SVHN \\ \hline \hline Flood & 0.05 (0.06) & 0.05 (0.06) & 0.05 (0.06) & 0.05 (0.06) & 0.01 (0) & 0.01 (0) & 0.03 (0.05) \\ \hline SAM & 0.36 (0.19) & 0.05 (0.06) & 0.05 (0.06) & 0.36 (0.19) & 0.5 (0) & 0.32 (0.16) & 0.28 (0.20) \\ \hline SoftAD & 0.08 (0.08) & 0.05 (0.06) & 0.05 (0.06) & 0.08 (0.08) & 0.22 (0.14) & 0.03 (0.04) & 0.13 (0.10) \\ \hline \end{tabular}
\end{table}
Table 2: Hyperparameters selected by validation for each method (averaged over trials). Flooding and SoftAD have threshold \(\theta\); SAM has radius parameter. Standard deviation (over trials) is given in small-text parentheses.

Figure 6: Model norm trajectories over epochs for each dataset in Figures 4–5.

## Acknowledgments and Disclosure of Funding

This work was supported by JST PRESTO (grant number JPMJPR21C6) and a grant from the SECOM Science and Technology Foundation.

## References

* Arora et al. (2018) Arora, S., Ge, R., Neyshabur, B., and Zhang, Y. (2018). Stronger generalization bounds for deep nets via a compression approach. In _Proceedings of the 35th International Conference on Machine Learning (ICML)_, volume 80 of _Proceedings of Machine Learning Research_, pages 254-263.
* Bae et al. (2023) Bae, W., Ren, Y., Ahmed, M. O., Tung, F., Sutherland, D. J., and Oliveira, G. L. (2023). AdaFlood: Adaptive flood regularization. _arXiv preprint arXiv:2311.02891_.
* Baker (2022) Baker, A. (2022). Simplicity. In Zalta, E. N., editor, _The Stanford Encyclopedia of Philosophy_. Metaphysics Research Lab, Stanford University, Summer 2022 edition.
* Barrett and Dherin (2021) Barrett, D. G. T. and Dherin, B. (2021). Implicit gradient regularization. In _The 9th International Conference on Learning Representations (ICLR)_.
* Barron (2019) Barron, J. T. (2019). A general and adaptive robust loss function. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4331-4339.
* Bartlett (1998) Bartlett, P. L. (1998). The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. _IEEE Transactions on Information Theory_, 44(2):525-536.
* Bartlett et al. (2006) Bartlett, P. L., Jordan, M. I., and McAuliffe, J. D. (2006). Convexity, classification, and risk bounds. _Journal of the American Statistical Association_, 101(473):138-156.
* Chaudhari et al. (2017) Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C., Chayes, J., Sagun, L., and Zecchina, R. (2017). Entropy-SGD: Biasing gradient descent into wide valleys. In _International Conference on Learning Representations_.
* Claeskens and Hjort (2008) Claeskens, G. and Hjort, N. L. (2008). _Model Selection and Model Averaging_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.
* Cutkosky and Mehta (2021) Cutkosky, A. and Mehta, H. (2021). High-probability bounds for non-convex stochastic optimization with heavy tails. In _Advances in Neural Information Processing Systems 34 (NeurIPS 2021)_, pages 4883-4895.
* Cutkosky et al. (2023) Cutkosky, A., Mehta, H., and Orabona, F. (2023). Optimal stochastic non-smooth non-convex optimization through online-to-non-convex conversion. _arXiv preprint arXiv:2302.03775v2_.
* Davis and Drusvyatskiy (2019) Davis, D. and Drusvyatskiy, D. (2019). Stochastic model-based minimization of weakly convex functions. _SIAM Journal on Optimization_, 29(1):207-239.
* Denker and LeCun (1990) Denker, J. and LeCun, Y. (1990). Transforming neural-net output levels to probability distributions. In _Advances in Neural Information Processing Systems 3 (NIPS 1990)_, volume 3.
* Devroye et al. (1996) Devroye, L., Gyorfi, L., and Lugosi, G. (1996). _A Probabilistic Theory of Pattern Recognition_, volume 31 of _Stochastic Modelling and Applied Probability_. Springer.
* Dinh et al. (2017) Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. (2017). Sharp minima can generalize for deep nets. In _Proceedings of the 34th International Conference on Machine Learning (ICML)_, volume 70 of _Proceedings of Machine Learning Research_, pages 1019-1028.
* Drucker and Le Cun (1992) Drucker, H. and Le Cun, Y. (1992). Improving generalization performance using double backpropagation. _IEEE Transactions on Neural Networks_, 3(6):991-997.
* Drusvyatskiy and Paquette (2019) Drusvyatskiy, D. and Paquette, C. (2019). Efficiency of minimizing compositions of convex functions and smooth maps. _Mathematical Programming_, 178:503-558.
* Drusvyatskiy et al. (2019)Duchi, J. C. and Namkoong, H. (2021). Learning models with uniform performance via distributionally robust optimization. _The Annals of Statistics_, 49(3):1378-1406.
* Dziugaite et al. (2020) Dziugaite, G. K., Drouin, A., Neal, B., Rajkumar, N., Caballero, E., Wang, L., Mitliagkas, I., and Roy, D. M. (2020). In search of robust measures of generalization. In _Advances in Neural Information Processing Systems 33 (NeurIPS 2020)_, pages 11723-11733.
* Efron and Hinkley (1978) Efron, B. and Hinkley, D. V. (1978). Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information. _Biometrika_, 65(3):457-483.
* Feldman (2016) Feldman, V. (2016). Generalization of ERM in stochastic convex optimization: The dimension strikes back. In _Advances in Neural Information Processing Systems 29 (NIPS 2016)_, pages 3576-3584.
* Flaxman et al. (2004) Flaxman, A. D., Kalai, A. T., and McMahan, H. B. (2004). Online convex optimization in the bandit setting: gradient descent without a gradient. _arXiv preprint arXiv:cs/0408007v1_.
* Foret et al. (2021) Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B. (2021). Sharpness-aware minimization for efficiently improving generalization. In _International Conference on Learning Representations_.
* Goodfellow et al. (2014) Goodfellow, I. J., Vinyals, O., and Saxe, A. M. (2014). Qualitatively characterizing neural network optimization problems. _arXiv preprint arXiv:1412.6544_.
* Grunwald (2007) Grunwald, P. D. (2007). _The Minimum Description Length Principle_. MIT Press.
* Hinton and van Camp (1993) Hinton, G. E. and van Camp, D. (1993). Keeping the neural networks simple by minimizing the description length of the weights. In _Proceedings of the 6th Annual Conference on Computational Learning Theory_, pages 5-13.
* Hochreiter and Schmidhuber (1994) Hochreiter, S. and Schmidhuber, J. (1994). Simplifying neural nets by discovering flat minima. In _Advances in Neural Information Processing Systems 7 (NIPS 1994)_.
* Hochreiter and Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. (1997). Flat minima. _Neural Computation_, 9(1):1-42.
* Holland (2022) Holland, M. J. (2022). Learning with risks based on M-location. _Machine Learning_, 111:4679-4718.
* Holland (2023) Holland, M. J. (2023). Flexible risk design using bi-directional dispersion. In _Proceedings of the 26th International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 206 of _Proceedings of Machine Learning Research_, pages 1586-1623.
* Holland (2024) Holland, M. J. (2024). Criterion collapse and loss distribution control. In _Proceedings of the 41st International Conference on Machine Learning (ICML)_, volume 235 of _Proceedings of Machine Learning Research_, pages 18547-18567.
* Holland and Tanabe (2023) Holland, M. J. and Tanabe, K. (2023). A survey of learning criteria going beyond the usual risk. _Journal of Artificial Intelligence Research_, 73:781-821.
* Hu et al. (2023) Hu, S., Wang, X., and Lyu, S. (2023). Rank-based decomposable losses in machine learning: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45.
* Ishida et al. (2023) Ishida, T., Yamane, I., Charoenphakdee, N., Niu, G., and Sugiyama, M. (2023). Is the performance of my deep network too good to be true? A direct approach to estimating the Bayes error in binary classification. In _The 11th International Conference on Learning Representations (ICLR)_.
* Ishida et al. (2020) Ishida, T., Yamane, I., Sakai, T., Niu, G., and Sugiyama, M. (2020). Do we need zero training loss after achieving zero training error? In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, volume 119 of _Proceedings of Machine Learning Research_, pages 4604-4614.
* Jia and Su (2020) Jia, Z. and Su, H. (2020). Information-theoretic local minima characterization and regularization. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, volume 119 of _Proceedings of Machine Learning Research_, pages 4773-4783.
* Jiang et al. (2020) Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S. (2020). Fantastic generalization measures and where to find them. In _International Conference on Learning Representations_.
* Jiang et al. (2021)Johnson, R. and Zhang, T. (2023). Inconsistency, instability, and generalization gap of deep neural network training. In _Advances in Neural Information Processing Systems 36 (NeurIPS 2023)_, pages 9479-9505.
* Karakida et al. (2019) Karakida, R., Akaho, S., and Amari, S.-i. (2019). Universal statistics of Fisher information in deep neural networks: Mean field approach. In _22nd International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 89 of _Proceedings of Machine Learning Research_, pages 1032-1041.
* Karakida et al. (2023) Karakida, R., Takase, T., Hayase, T., and Osawa, K. (2023). Understanding gradient regularization in deep learning: Efficient finite-difference computation and implicit bias. In _Proceedings of the 40th International Conference on Machine Learning (ICML)_, volume 202 of _Proceedings of Machine Learning Research_, pages 15809-15827.
* Keskar et al. (2017) Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. (2017). On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_.
* Kullback (1968) Kullback, S. (1968). _Information Theory and Statistics_. Dover.
* Kwon et al. (2021) Kwon, J., Kim, J., Park, H., and Choi, I. K. (2021). ASAM: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In _Proceedings of the 38th International Conference on Machine Learning (ICML)_, volume 139 of _Proceedings of Machine Learning Research_, pages 5905-5914.
* Lee et al. (2020) Lee, J., Park, S., and Shin, J. (2020). Learning bounds for risk-sensitive learning. In _Advances in Neural Information Processing Systems 33 (NeurIPS 2020)_, pages 13867-13879.
* Li et al. (2018) Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. (2018). Visualizing the loss landscape of neural nets. In _Advances in Neural Information Processing Systems 31 (NIPS 2018)_, pages 3576-3584.
* Li et al. (2021) Li, T., Beirami, A., Sanjabi, M., and Smith, V. (2021). Tilted empirical risk minimization. In _The 9th International Conference on Learning Representations (ICLR)_.
* MacKay (1992) MacKay, D. J. C. (1992). A practical Bayesian framework for backpropagation networks. _Neural Computation_, 4(3):448-472.
* Montavon et al. (2012) Montavon, G., Orr, G. B., and Muller, K.-R., editors (2012). _Neural Networks: Tricks of the Trade_, volume 7700 of _Lecture Notes in Computer Science_. Springer, 2nd edition.
* Nesterov and Spokoiny (2017) Nesterov, Y. and Spokoiny, V. (2017). Random gradient-free minimization of convex functions. _Foundations of Computational Mathematics_, 17(2):527-566.
* Neyshabur et al. (2017) Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. (2017). Exploring generalization in deep learning. In _Advances in Neural Information Processing Systems 30 (NIPS 2017)_.
* Royset (2024) Royset, J. O. (2024). Risk-adaptive approaches to stochastic optimization: A survey. _arXiv preprint arXiv:2212.00856v3_.
* Schmidt et al. (2021) Schmidt, R. M., Schneider, F., and Hennig, P. (2021). Descending through a crowded valley -- benchmarking deep learning optimizers. In _Proceedings of the 38th International Conference on Machine Learning (ICML)_, volume 139 of _Proceedings of Machine Learning Research_, pages 9367-9376.
* Sejnowski (2020) Sejnowski, T. J. (2020). The unreasonable effectiveness of deep learning in artificial intelligence. _Proceedings of the National Academy of Sciences_, 117(48):30033-30038.
* Shalev-Shwartz et al. (2010) Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K. (2010). Learnability, stability and uniform convergence. _Journal of Machine Learning Research_, 11:2635-2670.
* Smith et al. (2021) Smith, S. L., Dherin, B., Barrett, D., and De, S. (2021). On the origin of implicit regularization in stochastic gradient descent. In _International Conference on Learning Representations_.
* Smith et al. (2020)Sun, K. and Nielsen, F. (2021). A geometric modeling of Occam's razor in deep learning. _arXiv preprint arXiv:1905.11027v4_.
* Wu et al. (2017) Wu, L., Zhu, Z., and E, W. (2017). Towards understanding generalization of deep learning: Perspective of loss landscapes. _arXiv preprint arXiv:1706.10239_.
* Xie et al. (2022) Xie, Y., Wang, Z., Li, Y., Zhang, C., Zhou, J., and Ding, B. (2022). iFlood: A stable and effective regularizer. In _International Conference on Learning Representations_.
* Xie et al. (2020) Xie, Z., Sato, I., and Sugiyama, M. (2020). A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima. _arXiv preprint arXiv:2002.03495_.
* Zhang et al. (2017) Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deep learning requires rethinking generalization. In _International Conference on Learning Representations_.
* Zhang et al. (2020) Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. (2020). Why are adaptive methods good for attention models? In _Advances in Neural Information Processing Systems 33 (NeurIPS 2020)_, pages 15383-15393.
* Zhao et al. (2022) Zhao, Y., Zhang, H., and Hu, X. (2022). Penalizing gradient norm for efficiently improving generalization in deep learning. In _Proceedings of the 39th International Conference on Machine Learning (ICML)_, volume 162 of _Proceedings of Machine Learning Research_, pages 26982-26992.

Bibliographic notes

In this section, we provide additional references intended to complement those in the main body of the paper.

### Broad overview

The following questions succinctly summarize key aspects of the problem of generalization.7

Footnote 7: These questions are inspired by the concise and lucid problem setting of Jia and Su (2020).

* What properties at training time are reliable indicators of performance at test time?
* How can we efficiently find candidates with such desirable properties?

The easy answer to these questions is, of course, "it depends." There is no fixed procedure that can guarantee arbitrarily good performance on all statistical learning problems, even if restricted to binary classification tasks.8 A more subtle answer involves characterizing the problems on which abstract learning algorithms such as empirical risk minimization (ERM) yield tight bounds on tractable criteria of interest (e.g., the expected loss).9 Even more difficult is refining our understanding of the learning problems on which concrete algorithms used in practice can be reliably expected to perform well.10

Footnote 8: A lucid explanation is given by Devroye et al. (1996, Ch. 1).

Footnote 9: Even this refined problem is far from trivial; see Shalev-Shwartz et al. (2010) and Feldman (2016).

Footnote 10: Even the critical question of which core optimizer to use does not have a clear-cut answer on standard benchmark datasets Schmidt et al. (2021). With additional options such dropout, batch normalization, and all manners of data augmentation, it is not surprising that the practitioner often takes a trial-and-error approach.

For conceptual grounding, we make use of the two questions, QP (the "property" question) and QA (the "algorithm" question), particularly within the context of non-linear models such as neural networks. Broadly speaking, in the machine learning literature over the past three decades, most answers to the property question QP come in the form of quantifying some notion of "simplicity," a property of candidate \(w\). It goes without saying that the underlying heuristic is that all else equal (at training time), a "complex" candidate seems intuitively less likely to perform well at test time.11 As for the algorithm question, there are numerous "workhorse" procedures of machine learning that are computationally convenient, have high-quality software available, and tend to generalize very well in practice, providing a partial answer to QA. That said, the design principles underlying these procedures are often only very loosely related to the properties that satisfy QP.12 With this in mind, a large body of research can be understood as trying to develop new connections between answers to QP and QA, either through _post hoc_ analysis using existing concepts, or by introducing new properties and deriving algorithms in a more unified fashion.

Footnote 11: In a sense this is just human nature, not specific to machine learning Baker (2022).

Footnote 12: In the context of training machine learning models both big and small, “Goodhart’s law” suggests that this gap is to some extent probably a good thing (https://openai.com/research/measuring-goodharts-law).

### Notions of model complexity

Model complexity is a concept that has a long history in the context of statistical model selection Claeskens and Hjort (2008), with well-established ties to information theory Kullback (1968). Assuming the quality of "fit" is measured using negative log-likelihood, the second derivative of this objective function (Hessian matrix in multi-dimensional case) is known as the Fisher information (matrix).13 In the context of neural networks, it is common to use their outputs to model probabilities Denker and LeCun (1990), and thus at least conceptually, much of the existing statistical methodology can be inherited. For early work in the context of backprop-driven neural networks, MacKay (1992) looks at designing objective criteria for comparing and choosing between models (including norm regularization parameters). MacKay introduces a form of Bayesian "evidence" for candidate models using a Gaussian approximation that requires evaluating the (inverse) Hessian of the base objective function. More generally, the Hessian describes the curvature of the objective function, and is closely related to geometric notions of "flat regions" on the surface induced by the objective function Goodfellow et al. (2014); Li et al. (2018).

Footnote 13: Since the data is random, so is the Fisher information; some authors call this the _observed_ Fisher information, in contrast with the _expected_ Fisher information Efron and Hinkley (1978).

The notion of model complexity has also played an central role in statistical learning theory. It has long been known that even when the number of parameters far outnumber the number of training samples, a small weight norm can be used to guarantee off-sample generalization for empirical risk minimizers (Bartlett, 1998). Of course, due to the high expressive power of neural network models, even with strong weight regularization it is possible to perfectly fit random labels (Zhang et al., 2017), leading to a gap between the test error (chance level) and training error (zero) that is methodologically unsatisfactory. This has motivated a variety of new approaches to measure off-sample generalization (Jiang et al., 2020), as well as to quantify model complexity, such as the degree to which a model can be meaningfully compressed (Arora et al., 2018).

The notion of "flat minima" is seen in the early work of Hochreiter and Schmidhuber (1994, 1997), which considers both how to measure sharpness, and heuristics for actually finding candidates in flat regions. The basic underlying notion is that of measuring "volume," namely the idea that a "flat" point is one from which we need to go far in most (if not all) directions for the objective function to increase a certain fixed amount. See more recent work by Wu et al. (2017) for related notions of volume in this context. These notions of sharpness are intimately related to properties of the Hessian matrix of the underlying objective function, even when the loss is not based on negative log-likelihood, and an active line of research is centered around the eigenvalue distribution of this Hessian. See for example Chaudhari et al. (2017) and Karakida et al. (2019) for representative work. For sufficiently "regular" models, the determinant of the Fisher information matrix plays a central role in the complexity term used to implement the minimum description length (MDL) principle (Grinwald, 2007); see also early work from Hinton and van Camp (1993) and more recent work by Jia and Su (2020) in the context of neural networks. More generally, however, many neural networks do not satisfy these regularity conditions, and new technical innovations based on the Fisher information have been explored to bridge this gap in recent years (Sun and Nielsen, 2021).

### Algorithms that generalize well

The empirical effectiveness of deep learning goes well beyond what we would expect based purely on learning theoretical insights (Sejnowski, 2020). This success is driven by a handful of workhorse stochastic gradient-based solvers (Schmidt et al., 2021), often coupled with explicit norm-based regularization and a number of techniques used to stabilize learning and effectively constrain the model candidate which is selected by the learning algorithm.14 A rich line of research has developed over the past decade looking at why a certain algorithmic "recipe" tends to generalize well. The tendency for stochastic gradient descent to "escape" from regions near undesirable critical points is one key theme; see Xie et al. (2020) for example. For influential work on relating sharpness, mini-batch size, and (weight) norms to off-sample generalization, see Keskar et al. (2017) and Neyshabur et al. (2017). In both papers, the notion of measuring sharpness by a worst-case perturbation appears, and this is pursued even further by Foret et al. (2021) in the well-known sharpness-aware minimization (SAM) algorithm, and extensions due to Kwon et al. (2021) and Zhao et al. (2022). These algorithms, as well as the Fisher information-based procedure of Jia and Su (2020), all involve a forward-difference implementation of explicit gradient regularization (using squared Euclidean norm), and recent work from Karakida et al. (2023) compares this approach with that of direct back-propagation approach. Barrett and Dherin (2021) look at both _implicit_ and _explicit_ gradient regularization. The implicit side contrasts the path of "continuous" gradient-based updates with the "discrete" updates made in practice (continuous/discrete with respect to _time_), saying that the discrete updates, even when computed based on an unregularized objective function, tend to move closer to the (continuous) path of a regularized objective, where regularization is in terms of the (squared) gradient norms. Inspired by this finding, they also consider explicit GR in the same way; see also Smith et al. (2021).

Footnote 14: These include early stopping, modifying mini-batch size, dropout, batch normalization, stochastic depth, data augmentation, and “mixup” (mixed sample augmentations) among others. See also Montavon et al. (2012) for techniques that were established in the decades before the current wave of deep learning.

Technical appendix

### More details on Flooding and sharpness

When the empirical risk goes below the threshold \(\theta\), the Flooding update (4) attempts to push it back up above \(\theta\). Consider the case in which this occurs in a single step, i.e., the situation in which at some step \(t\), the pair of sequential iterates \((w_{t},w_{t+1})\) satisfy the following:

\[\mathsf{R}_{n}(w_{t})<\theta\text{ and }\mathsf{R}_{n}(w_{t+1})>\theta.\] (14)

When condition (14) holds, some basic algebra immediately shows us that running two iterations of the Flooding update (4) yields the equality

\[w_{t+2}=w_{t}-\alpha^{2}\left(\frac{\nabla\mathsf{R}_{n}(w_{t}+\alpha\nabla \mathsf{R}_{n}(w_{t}))-\nabla\mathsf{R}_{n}(w_{t})}{\alpha}\right),\] (15)

telling us that the result is equivalent to running one iteration of FD descent with step size \(\alpha^{2}\) on the GR penalty \(\|\nabla\mathsf{R}_{n}(\cdot)\|^{2}\) at \(w_{t}\), using the forward FD approximation described earlier in SS2.2. To the best of our knowledge, this link was first highlighted by Karakida et al. (2023, SS5.1). In a sense, this is a natural complement to the strategy employed in (6); instead of tackling the GR objective \(\widetilde{\mathsf{R}}_{n}(w;\lambda)\) in (5) directly, the Flooding algorithm can iterate back and forth between optimizing the empirical risk and the squared gradient norm. The GR effect is thus constrained to regions in \(\mathcal{W}\) with \(\theta\)-small empirical risk, but all updates outside this region enjoy the same per-step computational complexity as vanilla GD.

### Non-smooth loss setting

All of the analysis in SS3.2 relies heavily on smoothness of the underlying loss function. Here we consider the case in which the loss itself may not even be differentiable. All we ask is that the losses be \(L\)-Lipschitz on \(\mathcal{W}\) in expectation, i.e., \(\mathbf{E}_{\mu}|\ell(w_{1};\mathsf{Z})-\ell(w_{2};\mathsf{Z})|\leq L\|w_{1}- w_{2}\|\) for all \(w_{1},w_{2}\in\mathcal{W}\). As an explicit objective function, we start with \(\mathrm{S}_{\mu}\) as given in (10), but with the understanding that \(\rho\) can actually be any \(1\)-Lipschitz function, capturing the two special cases of interest, namely \(\rho(x)=\sqrt{x^{2}+1}-1\) for SoftAD and \(\rho(x)=|x|\) for Flooding. Shifting our focus to function values (rather than gradients), we will also need to assume a second moment bound

\[\mathbf{E}_{\mu}\left(\theta+\rho(\ell(w;\mathsf{Z})-\theta)\right)^{2}\leq V<\infty\] (16)

that holds over \(w\in\mathcal{W}\). With these basic assumptions in place, stationarity guarantees are available via a smooth approximation of \(\mathrm{S}_{\mu}\).

**Proposition 6** (Stationarity, non-smooth case).: _Choosing an initial value \(w_{1}\in\mathcal{W}\), run the algorithm described in Proposition 3, but re-defining the core gradients used for updating as_

\[\mathsf{G}_{t}(w):=\frac{d}{r}\left(\theta+\rho(\ell(w+r\mathsf{U}_{t}; \mathsf{Z}_{t})-\theta)\right)\mathsf{U}_{t}\]

_where \((\mathsf{U}_{1},\mathsf{U}_{2},\ldots)\) is a sequence of independent vectors sampled uniformly at random from the unit sphere, and \(r>0\) sets the smoothing radius. In addition, the norm threshold is set as \(\gamma=\sqrt{dL/((1-b)r)}\) (with \(b\) unchanged). Stationarity of the resulting sequence \((w_{1},w_{2},\ldots)\), assumed to be in \(\mathcal{W}\), can be controlled with probability \(1-\delta\) as_

\[\frac{1}{T}\sum_{t=1}^{T}\lVert\nabla\mathrm{S}_{\mu}(w_{t};r)\rVert\leq\frac {1}{T^{1/4}}\left(\overline{\mathrm{R}}_{\mu}(w_{1})-\overline{\mathrm{R}}_{ \mu}(w_{T+1})+\frac{3dL}{2r}+\frac{2d}{r}\sqrt{V}\left(1+C_{\delta}\right)\right)\]

_where \(\mathrm{\widetilde{S}}_{\mu}(w;r):=\mathbf{E}[\mathrm{S}_{\mu}(w+r\mathsf{V})]\) is the \(r\)-smoothed approximation of the objective \(\mathrm{S}_{\mu}\), with \(\mathsf{V}\) distributed uniformly over the unit ball. Probability is taken over the random draw of \((\mathsf{Z}_{1},\mathsf{Z}_{2},\ldots)\) and \((\mathsf{U}_{1},\mathsf{U}_{2},\ldots)\), and the confidence factor \(C_{\delta}\) matches that given in Proposition 3._

_Remark 7_ (Stationarity in the original objective).: When the original objective \(\mathrm{S}_{\mu}(\cdot)\) is sufficiently well-behaved, e.g., differentiable almost everywhere and Lipschitz, then stationarity guarantees in terms of the \(r\)-smoothed objective \(\mathrm{\widetilde{S}}_{\mu}(\cdot;r)\) can be easily translated into analogous guarantees for \(\widetilde{\mathrm{R}}_{\mu}(\cdot)\). In addition, recent work by Cutkosky et al. (2023) shows how a modified algorithmic approach can be used to achieve faster rates under such congenial (but still non-convex and non-smooth) conditions.

### Additional proofs

Proof of Proposition 3.: The machinery of Cutkosky and Mehta (2021, Thm. 2) gives us the ability to control the stationarity of sequences generated using the described procedure (norm-clipping, momentum, normalization), just assuming the "raw" stochastic gradients (here, \(\mathsf{G}_{t}\)) are unbiased estimators of a smooth function. As such, we just need to ensure the assumptions underlying their Theorem 2 (henceforth, _CHT_2) are met; the key points have already been described in the main text, so we just fill in the details here. The "unbiased estimator" property we refer to means that we want

\[\mathbf{E}_{\mu}\,\mathsf{G}_{t}(w)=\nabla\mathrm{S}_{\mu}(w)\] (17)

to hold for all \(w\in\mathcal{W}\). Fortunately, this holds under very weak assumptions; the running assumption that \(L_{\text{AD}}<\infty\) is more than sufficient.15 In addition, finite \(L_{\text{AD}}\) also implies that the objective (10) is smooth in the sense that

Footnote 15: For a more general result, see Holland (2023, Lem. 2) for example.

\[\|\nabla\mathrm{S}_{\mu}(w_{1})-\nabla\mathrm{S}_{\mu}(w_{2})\|\leq L_{\text{ AD}}\|w_{1}-w_{2}\|\] (18)

for any \(w_{1},w_{2}\in\mathcal{W}\); this is proved in SSB.5. In addition, uniform second moment bounds naturally imply pointwise bounds, so we have

\[\mathbf{E}_{\mu}\|\nabla\ell(w;\mathsf{Z})\|^{2}\leq\mathbf{E}_{\mu}\left[ \sup_{w\in\mathcal{W}}\|\nabla\ell(w;\mathsf{Z})\|^{2}\right]\leq L_{\text{AD} }-L_{\ell}\] (19)

for each \(w\in\mathcal{W}\). Taken with the construction of sequence \((w_{1},w_{2},\ldots)\) in the hypothesis, the properties (17)-(19) ensure all the basic requirements of CHT2 are met (with their "p" at \(2\)). Noting that we assume \(\mathcal{W}\subseteq\mathbb{R}^{d}\) using the standard norm and inner product on Euclidean space, the Banach space generality in CHT2 is not needed (their "\(C\)" and "\(p\)" can be fixed to \(1\) and \(2\) respectively). For reference, the complete upper bound implied by CHT2 is

\[\frac{\mathrm{S}_{\mu}(w_{1})-\mathrm{S}_{\mu}(w_{T+1})}{T\alpha}+\frac{ \alpha L_{\text{AD}}}{2}+\frac{2b\sqrt{L_{\text{AD}}-L_{\ell}}}{(1-b)T}+\frac {2b\alpha L_{\text{AD}}}{(1-b)}+2\sqrt{(1-b)(L_{\text{AD}}-L_{\ell})}C_{\delta}\] (20)

where for readability the coefficient in the right-most summand is defined by

\[C_{\delta}\coloneqq 10\log(3T/\delta)+4\sqrt{\log(3T/\delta)}+1.\]

We have simplified all the terms in CHT2 involving \(\max\{1,\log(3T/\delta)\}\), since as long as \(T>0\) and \(0<\delta<1\), we trivially have \(3T/\mathrm{e}\geq 1>\delta\) and thus \(\log(3T/\delta)\geq 1\). Furthermore, their free parameters "\(b\)" (different from our \(b\)) and "\(s\)" are both set to \(1\), without loss of generality. Plugging in our settings of \(\alpha\) and \(b\) to the bound in (20) and bounding \((1-1/\sqrt{T})\leq 1\) for readability yields the desired upper bound. 

Proof of Proposition 4.: Here we leverage the projected sub-gradient analysis done by Davis and Drusvyatskiy (2019), in particular their Theorem 3 (henceforth, _DDT3_). The core of their argument relies upon a weak convexity property held by a rather large class of composite functions, namely compositions of the form \(f=h\circ g\), where \(g\) is smooth and \(h\) is both convex and Lipschitz. Considering the non-smooth objective function

\[w\mapsto\theta+\mathbf{E}_{\mu}|\ell(w;\mathsf{Z})-\theta|,\] (21)

it can be taken as a compound function by writing \(\mathbf{E}_{\mu}\,f(w;\mathsf{Z})\) with \(f(w;z)\coloneqq h(g(w;z))\), where \(g(w;z)\coloneqq\ell(w;z)\) and \(h(x)\coloneqq\theta+|x-\theta|\). Fixing \(z\in\mathcal{Z}\) for now, clearly \(h\) is \(1\)-Lipschitz and convex. By assumption, we have that \(g(\cdot;z)\) is \(L_{\ell}^{*}\)-smooth and locally Lipschitz. Then, using standard arguments, it is straightforward to show that \(f(\cdot;z)\) is \(L_{\ell}^{*}\)-weakly convex.16 Since the weak convexity parameter \(L_{\ell}^{*}\) does not depend on the arbitrary choice of \(z\), it follows that the function (21) is \(L_{\ell}^{*}\)-weakly convex. In the setting of DDT3, their "\(f(\cdot)\)" is \(\theta+\mathbf{E}_{\mu}|\ell(w;\mathsf{Z})-\theta|\), and their "\(\mathcal{X}\)" is \(\mathcal{W}\) here. The bound on the expected squared stochastic gradient norms (their "\(L^{2}\)") is our \(L_{\text{AD}}-L_{\ell}\) just as in the proof of Proposition 3. Finally, the key "unbiased estimator" property in this case deals with sub-differentials, namely we require that

Footnote 16: See for example Drusvyatskiy and Paquette (2019, Lem. 4.2) and Holland (2022, Prop. 8).

\[\mathbf{E}_{\mu}\,\mathsf{G}_{t}(w)\in\partial\,\mathbf{E}_{\mu}|\ell(w; \mathsf{Z})-\theta|\] (22)for all \(w\in\mathcal{W}\). Fortunately this basic property holds under very weak assumptions that are trivially satisfied when \(L_{\mathrm{AD}}\) is finite.17 With these facts in place, we simple apply DDT3, in particular their inequality (3.5), with their "\(\rho\)" corresponding to our \(L_{t}^{\star}\) here, and their "\(\varphi_{\lambda}\)" corresponding to our (13), with "\(\lambda\)" as our \(\beta\). The desired bound follows by applying their result to the specified procedure over \(T-1\) updates (instead of their \(T\) updates). 

Footnote 17: See for example Holland (2022, Prop. 14).

Proof of Proposition 6.: To begin, under the assumptions given, the objective \(\mathrm{S}_{\mu}\) clearly inherits the Lipschitz property that the losses have in expectation; since \(\rho\) is \(1\)-Lipschitz, we have

\[\left|\mathrm{S}_{\mu}(w_{1})-\mathrm{S}_{\mu}(w_{2})\right|\leq\mathbf{E}_{ \mu}|\ell(w_{1};\mathsf{Z})-\ell(w_{2};\mathsf{Z})|\leq L\|w_{1}-w_{2}\|.\] (23)

We proceed by using a standard technique for function smoothing.18 If we let \(\mathsf{V}\) be a random vector distributed over the unit ball \(\{x\in\mathbb{R}^{d}:\|x\|\leq 1\}\), then regardless of whether \(\mathrm{S}_{\mu}(\cdot)\) is differentiable or not, one can obtain a smooth approximation by averaging over random \(r\)-length perturbations, namely

Footnote 18: See for example Flaxman et al. (2004); Nesterov and Spokoiny (2017).

\[\bar{\mathrm{S}}_{\mu}(w;r)\coloneqq\mathbf{E}\left[\mathrm{S}_{\mu}(w+r \mathsf{V})\right].\] (24)

A critical property of the function given in (24) is that it is differentiable and its gradient can be represented explicitly in terms of the function it is trying to smooth, namely we have

\[\nabla\bar{\mathrm{S}}_{\mu}(w;r)=\frac{d}{r}\,\mathbf{E}\left[\mathrm{S}_{\mu }(w+r\mathsf{U})\mathsf{U}\right]=\frac{d}{r}\,\mathbf{E}\left[(\theta+ \mathbf{E}_{\mu}\,\rho(\ell(w+r\mathsf{U})-\theta))\,\mathsf{U}\right]\] (25)

for any choice of \(r>0\) and \(w\in\mathcal{W}\), where \(\mathsf{U}\) is uniformly distributed on the unit sphere \(\{x\in\mathbb{R}^{d}:\|x\|=1\}\)(Flaxman et al., 2004, Lem. 1).19 This means that Lipschitz properties on the original function translate to smoothness properties for the new function. Making this more explicit, using the equality (25), note that for any choice of \(w_{1},w_{2}\in\mathcal{W}\), we have

Footnote 19: Not to be confused with \(\mathsf{V}\) in (24), which is uniform on the unit _ball_.

\[\nabla\bar{\mathrm{S}}_{\mu}(w_{1};r)-\nabla\bar{\mathrm{S}}_{\mu}(w_{2};r)= \frac{d}{r}\,\mathbf{E}\left[(\mathrm{S}_{\mu}(w_{1}+r\mathsf{U})-\mathrm{S}_ {\mu}(w_{2}+r\mathsf{U}))\,\mathsf{U}\right].\]

Taking norms and using the Lipschitz property (23) of \(\mathrm{S}_{\mu}\), we observe that

\[\left|\nabla\bar{\mathrm{S}}_{\mu}(w_{1};r)-\nabla\bar{\mathrm{S}} _{\mu}(w_{2};r)\right| \leq\frac{d}{r}\,\mathbf{E}\|U\|\|\mathrm{S}_{\mu}(w_{1}+r\mathsf{ U})-\mathrm{S}_{\mu}(w_{2}+r\mathsf{U})|\] \[\leq\frac{dL}{r}\|w_{1}-w_{2}\|\]

and thus have that the smoothed function \(\bar{\mathrm{S}}_{\mu}(\cdot;r)\) is \((dL/r)\)-smooth over \(\mathcal{W}\). This means the function is analogous to the objective function (10) used in Proposition 3, except with unbiased stochastic gradients taking the form

\[\mathsf{G}_{t}(w)\coloneqq\frac{d}{r}\left(\theta+\rho(\ell(w+r\mathsf{U}_{t };\mathsf{Z}_{t})-\theta)\right)\mathsf{U}_{t}\] (26)

for \(t\geq 1\), where each \(\mathsf{U}_{t}\) is an independent copy of \(\mathsf{U}\) from (25). From this point, the remainder of the proof is basically identical to that of Proposition 3; the only remaining changes are the smoothness factor and the second moment bound. For the former, we use \(dL/r\) in place of \(L_{\mathrm{AD}}\), which also impacts the norm clipping radius \(\gamma\). For the latter, since we are assuming \(w_{t}+r\mathsf{U}_{t}\in\mathcal{W}\) for each \(t\), and using the bound (16), we have

\[\mathbf{E}\|\mathsf{G}_{t}(w)\|^{2}\leq\sup_{w\in\mathcal{W}}\left(\frac{d}{r} \right)^{2}\mathbf{E}\|\mathsf{U}_{t}\|^{2}\left(\theta+\rho(\ell(w;\mathsf{Z} _{t})-\theta)\right)^{2}\leq\left(\frac{d}{r}\right)^{2}V.\]

Plugging in these two remaining modified factors to the bounds obtained in Proposition 3 yields the desired result.

### Gradient of GR objective

With \(w=(w_{1},\ldots,w_{d})\in\mathbb{R}^{d}\), we will frequently use \(\partial_{j}\) to denote partial derivatives taken with respect to \(w_{j}\), i.e., for a differentiable function \(f:\mathbb{R}^{d}\to\mathbb{R}\), we write

\[\partial_{j}f(w)\coloneqq\lim_{|a|\to 0}\frac{f(w_{1},\ldots,w_{j}+a,\ldots,w_{d} )-f(w)}{a}\] (27)

with analogous definitions for all \(j=1,\ldots,d\). With the above notation in place, note that basic calculus gives us

\[\partial_{j}\|\nabla\mathsf{R}_{n}(w)\|^{2}=\sum_{k=1}^{d}\partial_{j}\left( \partial_{k}\mathsf{R}_{n}(w)\right)^{2}=2\sum_{k=1}^{d}\left(\partial_{k} \mathsf{R}_{n}(w)\right)\left(\partial_{j}\partial_{k}\mathsf{R}_{n}(w)\right).\]

As such, the gradient takes the form

\[\nabla\|\nabla\mathsf{R}_{n}(w)\|^{2}=2\nabla^{2}\mathsf{R}_{n}(w)\left(\nabla \mathsf{R}_{n}(w)\right)\]

where \(\nabla^{2}\mathsf{R}_{n}\) denotes the \(d\times d\) Hessian matrix of \(\mathsf{R}_{n}\), and \(\nabla\mathsf{R}_{n}(w)\) is taken as a column vector (a \(d\times 1\) matrix) for the purpose of this multiplication.

### Smoothness check

Let us consider a simple, non-stochastic example in one dimension. Letting \(f:\mathbb{R}\to\mathbb{R}\) be some differentiable function, we consider how the transformed gradient \(\phi(f(x)-\theta)f^{\prime}(x)\) behaves under \(\phi=\mathrm{sign}\) and \(\phi=\rho^{\prime}(x)=x/\sqrt{x^{2}+1}\). As we have seen visually in Figure 1, the soft threshold of SoftAD makes it possible to have Lipschitz gradients, which impacts iterative optimization procedures. For arbitrary values \(x_{1}\) and \(x_{2}\), taking the difference of transformed gradients using arbitrary \(\phi\), we can write

\[\phi(f(x_{1})-\theta)f^{\prime}(x_{1})-\phi(f(x_{2})-\theta)f^{ \prime}(x_{2})\] \[\quad=\left(\phi(f(x_{1})-\theta)-\phi(f(x_{2})-\theta)\right)f^ {\prime}(x_{1})+\phi(f(x_{2})-\theta)\left(f^{\prime}(x_{1})-f^{\prime}(x_{2} )\right).\] (28)

Note that even if \(|x_{1}-x_{2}|<\varepsilon\) for some arbitrarily small \(\varepsilon>0\), if for example the threshold is such that \(f(x_{1})<\theta<f(x_{2})\), then under \(\phi=\mathrm{sign}\), the difference multiplying \(f^{\prime}(x_{1})\) cannot be arbitrarily small, even if \(f\) is Lipschitz. On the other hand, such a property follows easily when \(\phi=\rho^{\prime}\), since \(\rho^{\prime}\) itself is \(1\)-Lipschitz.

Returning to our more general learning setup, let us denote the modified gradients concisely as \(g(w;z)\coloneqq\phi(\ell(w;z)-\theta)\nabla\ell(w;z)\). With random variable \(\mathsf{Z}\sim\mu\), taking any two points \(w_{1},w_{2}\in\mathcal{W}\), based on the equality (28), the normed difference of the gradient expectations can be bounded as

\[\|\mathbf{E}_{\mu}\,g(w_{1};\mathsf{Z})-\mathbf{E}_{\mu}\,g(w_{2};\mathsf{Z}) \|\leq B_{1}+B_{2}\]

with \(B_{1}\) and \(B_{2}\) defined as

\[B_{1} \coloneqq\mathbf{E}_{\mu}\|\nabla\ell(w_{1};\mathsf{Z})\|\phi( \ell(w_{1};\mathsf{Z})-\theta)-\phi(\ell(w_{2};\mathsf{Z})-\theta)|\] \[B_{2} \coloneqq\mathbf{E}_{\mu}|\phi(\ell(w_{2};\mathsf{Z})-\theta)\| \nabla\ell(w_{1};\mathsf{Z})-\nabla\ell(w_{2};\mathsf{Z})\|.\]

Bounding each of these terms is trivial when the functions \(\ell\) and \(\phi\) are smooth enough. First, note that if \(\phi\) is \(L_{\phi}\)-Lipschitz and \(\mathcal{W}\) is a convex subset of \(\mathbb{R}^{d}\), we have

\[B_{1} \leq L_{\phi}\,\mathbf{E}_{\mu}\|\nabla\ell(w_{1};\mathsf{Z})\| \ell(w_{1};\mathsf{Z})-\ell(w_{2};\mathsf{Z})\|\] \[\leq L_{\phi}\|w_{1}-w_{2}\|\,\mathbf{E}_{\mu}\|\nabla\ell(w_{1}; \mathsf{Z})\|\sup_{0<a<1}\|\nabla\ell(aw_{1}+(1-a)w_{2};\mathsf{Z})\|\] \[\leq L_{\phi}\|w_{1}-w_{2}\|\,\mathbf{E}_{\mu}\left[\sup_{w\in \mathcal{W}}\|\nabla\ell(w;\mathsf{Z})\|^{2}\right],\]

noting that the second inequality uses the mean value theorem on differentiable \(\ell(\cdot;z)\), applied pointwise in \(z\in\mathcal{Z}\), and the last inequality uses convexity of \(\mathcal{W}\). This bounds \(B_{1}\). Moving on to \(B_{2}\), note that if \(|\phi(x)|\) is bounded by \(B_{\phi}\) and the losses are \(L_{\ell}\)-smooth in expectation, we have

\[B_{2} \leq B_{\phi}\,\mathbf{E}_{\mu}\|\nabla\ell(w_{1};\mathsf{Z})- \nabla\ell(w_{2};\mathsf{Z})\|\] \[\leq B_{\phi}L_{\ell}\|w_{1}-w_{2}\|.\]

Taking these new bounds together, we have

\[\|\mathbf{E}_{\mu}\,g(w_{1};\mathsf{Z})-\mathbf{E}_{\mu}\,g(w_{2};\mathsf{Z})\| \leq\left(L_{\phi}\,\mathbf{E}_{\mu}\left[\sup_{w\in\mathcal{W}}\|\nabla\ell(w; \mathsf{Z})\|^{2}\right]+B_{\phi}L_{\ell}\right)\|w_{1}-w_{2}\|,\]

namely a Lipschitz property in expectation for the modified gradients.

## Appendix C Empirical appendix

Here we provide additional details and results related to the empirical tests described in SS4.

Software and hardwareAll of the experiments done in this section have been implemented using PyTorch 2, utilizing three machines each using a single-GPU implementation, i.e., there is no parallelization across multiple machines or GPUs. Two units are equipped with an NVIDIA A100 (80GB), and the remaining machine uses an NVIDIA RTX 6000 Ada. We use the MLflow library for storing and retrieving metrics and experiment details. Our coding of SAM follows that of David Samuel (https://github.com/davda54/sam), which is the PyTorch implementation acknowledged in the original SAM paper of Foret et al. (2021).

### Non-linear binary classification on the plane

DataThe three types of synthetic data that we generate here differ chiefly in the degree of non-linearity; see Figure 7 for an example. The "two Gaussians" dataset is almost linearly separable, save for some overlap of the two distributions. The "sinusoid" data is separated by a simple curve, easily approximated by a low-order polynomial, but the curves in the "spiral" data are a bit more complicated. Exact implementation details, plus historical references, are given by Ishida et al. (2020, SS4.1). For each trial, we generate training and validation data of size 100, and test data of size 2000. All methods see the same data in each trial.

ModelFor each dataset, we use the same model, namely a simple feedforward neural network with four hidden layers, 500 units per layer, using batch normalization and ReLU activations at each layer.20

Footnote 20: Ishida et al. (2020) say they use a “five-hidden-layer feedforward neural network,” but looking at their public code, the number of hidden layers (i.e., number of linear transformations excluding that of the output layer) is actually four.

AlgorithmsIn line with the experiments we are trying to replicate, all methods (ERM, Flooding, SAM, and SoftAD) are driven by the Adam optimizer, using a fixed learning rate of 0.001, with no momentum or weight decay. All methods use the multi-class logistic loss as their base loss (i.e., nn.CrossEntropyLoss in PyTorch), and are run for 500 epochs. We use mini-batch size of 50 here, but key trends remain the same for full-batch (of size 100) runs.

Hyperparameter selectionERM has no hyperparameters, but all the other methods have one each. Flooding and SoftAD both have the threshold parameter \(\theta\) seen in SS2-SS3, and SAM has a radius parameter (denoted "\(\rho\)" in the original paper). For each of these methods, in each trial, we select from a grid of 40 points spaced linearly between \(0.01\) and \(2.0\). Selection is based on classification accuracy on validation data.

### Image classification from scratch

Our second set of experiments utilizes four well-known benchmark datasets for multi-class image classification. Compared to the synthetic experiments done in SSC.1, the classification task is more difficult (much larger inputs, variation within classes, more classes), and so we utilize more sophisticated neural network models to tackle the classification task. That said, as the sub-section title indicates, this training is done "from scratch," i.e., no pre-trained models are used.

Figure 7: Synthetic dataset examples. From left to right: “two Gaussians,” “sinusoid,” and “spiral.”DataThe datasets we use are all standard benchmarks in the machine learning community: CIFAR-10, CIFAR-100, FashionMNIST, and SVHN. All of these datasets are collected using classes defined in the torchvision.datasets module, with raw training/test splits left as-is with default settings. As such, across all trials the test set is constant, but in each trial we randomly select 80% of the raw training data to be used for actual training, with the remaining 20% used for validation. We normalize all pixel values in the image data to the unit interval \([0,1]\); this is done separately for training, validation, and testing data.

ModelsUnlike the previous sub-section, here we use different models for different data sets. Model choice essentially mirrors that of Ishida et al. (2020, SS4.2). For FashionMNIST, we flatten each image into a vector, and use a simple feedforward neural network composed of a single hidden layer with 1000 units, batch normalization, and ReLU activation before the output transformation. For SVHN, we use ResNet-18 as implemented in torchvision.models, without any pre-trained weights. Finally, for both CIFAR-10 and CIFAR-100, we use ResNet-34 (again in torchvision.models) without pre-training. Both of the ResNet models used do not flatten the images, but rather take each RGB image as-is.

AlgorithmsJust as in SSC.1, we are testing ERM, Flooding, SAM, and SoftAD. Again we use the cross entropy loss, and run for 500 epochs. However, instead of Adam as the base optimizer, here we use vanilla SGD with a fixed step size of \(0.1\), and momentum parameter of \(0.9\). For all datasets, we use a mini-batch size of 200. All these settings match the experimental setup of Ishida et al. (2020, SS4.2).21

Footnote 21: Batch size is not given in the original Flooding paper, but the size of 200 was confirmed by means of a private communication with the authors.

Hyperparameter selectionOnce again we select hyperparameters for Flooding, SoftAD, and SAM from a grid of candidate values, such that the classification accuracy on validation data is maximized. Unlike SSC.1 however, here we use different grids for each method. For Flooding, we follow the setup of the original paper, choosing from ten values: \(\{0.01,0.02,\dots,0.1\}\). For SAM, once again we follow the original paper (their SS3.1), which for analogous tests utilized the set \(\{0.01,0.02,0.05,0.1,0.2,0.5\}\). Finally, for SoftAD we match the set size used by Flooding (i.e., ten) by taking the union of \(\{0.15,0.25,0.35,0.75\}\) and the set used by SAM.

[MISSING_PAGE_EMPTY:23]

## 6 Conclusion

\begin{table}
\begin{tabular}{|c||c|c|c|c|} \hline  & CIFAR-10 & CIFAR-100 & Fashion & SVHN \\ \hline \hline iFlood & 0.04 (0.05) & 0.06 (0.04) & 0.15 (0.10) & 0.10 (0.05) \\ \hline SoftAD & 0.12 (0.06) & 0.34 (0.28) & 0.01 (0) & 0.12 (0.12) \\ \hline \end{tabular}
\end{table}
Table 4: Hyperparameters selected by validation for each method (averaged over trials). Flooding and SoftAD have threshold \(\theta\); SAM has radius parameter. Standard deviation (over trials) is given in small-text parentheses.

Figure 10: Model norm trajectories over epochs for each dataset in Figures 8–9.

### Linear binary classification

In Figure 11, we compare ERM with Flood and SoftAD run with a _common_ threshold level of \(\theta=0.25\), using the "two Gaussians" and "sinusoid" data described in SSC.1, and a simple linear model, i.e., a feed-forward neural network with no hidden layers. Training and test sizes match those described in SSC.1. Even with a very simple linear model, it is clear that SoftAD can be used to achieve competitive accuracy at much larger loss levels. Note that in the case of "sinusoid," the average loss does not reach the threshold \(\theta\), and thus Flooding is identical to ERM. These basic trends hold over a range of thresholds \(\theta\) and re-scaling parameters \(\sigma\) (i.e., using \(\phi((x-\theta)/\sigma)\) with \(\sigma\neq 1\)). These trends are captured by the heatmaps given in Figure 12, where for each setting of \(\theta\) (for SoftAD and Flooding) and \(\sigma\) (for SoftAD only), we generate a fresh dataset. Clearly taking the threshold level far too high leads to arbitrarily bad performance, but below a certain level, similar performance is observed over a wide range of values. It is interesting to note how while test loss changes in a rather predictable continuous fashion as a function of \(\theta\), the test _accuracy_ drops in a much sharper manner when \(\theta\) is set too high in the case of SoftAD, whereas this drop is smoother in the case of Flooding. That said, these trends are only within the confines of this very simple linear model example using full batch, and tend to change (even with the same model) as we modify the mini-batch size.

Figure 11: Average cross entropy loss and accuracy over epochs (full batch) for each method.

Figure 12: Test loss and accuracy heatmaps for Flooding and SoftAD, depending on threshold level (denoted “theta”) and scaling parameter (denoted “sigma”).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We are very clear that if our interest is purely in improving the test accuracy, the existing Flooding algorithm tends to work quite well. Our contributions are set in the context of learning tasks where accuracy is important, but average loss and/or model norms are also of importance; our new method SoftAD is shown to achieve an appealing balance between these metrics compared with Flooding (and ERM/SAM), as we claim. Furthermore, we make no claims that our method can circumvent the issue of how to set the threshold level \(\theta\); as we discuss in SS5, this is a broad question that underlies both Flooding and SoftAD, and the pursuit of an answer is stated as future work. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: As discussed above under "Claims," we emphasize the strong points of our method (smoothness, stability, good balance across loss/accuracy/norms), but are completely open about the fact that our method, just as with Flooding, still has a free parameter \(\theta\) whose setting is by no means trivial (though empirically, simple validation is shown to work well). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.

* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All assumptions are stated clearly, either in the main text or in the body of propositions/theorems, and complete proofs are given in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All experimental results can be recovered using code and Jupyter notebooks that we have already prepared in a GitHub repository. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: See the above answer; code is provided on GitHub (after the review phase). As for data, all datasets are public, clearly described, and accessible via the outlets we have mentioned. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In the main body of the paper we give a broad summary, but complete details are given in SSC, a fact that we clearly mention in the main body. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [Yes] Justification: For results with relatively large discrepancy between methods (namely hyperparameter selection results in Table 2), we give standard deviation (over randomized trials) in addition to averages. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: At the start of $C, we describe the hardware used in our experimental setup. Actual computation time is not a key factor in our results. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Datasets and software are all credited, both within the paper itself and within our code made available on GitHub.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

Answer: [Yes]

Justification: The only relevant new "assets" are code for re-creating the experiments, and this is all well-documented on our GitHub repository.

Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?

Answer: [NA]

Justification: [NA]

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: [NA]

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.