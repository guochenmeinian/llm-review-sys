# Capturing the Denoising Effect of PCA via Compression Ratio

Chandra Sekhar Mukherjee

chandrasekhar.mukherjee@usc.edu

&Nikhil Deorkar

deorkar@usc.edu

Thomas Lord Department of Computer Science, University of Southern California. Research supported by NSF CAREER award 2141536.

Jiapeng Zhang

jiapengz@usc.edu

###### Abstract

Principal component analysis (PCA) is one of the most fundamental tools in machine learning with broad use as a dimensionality reduction and denoising tool. In the later setting, while PCA is known to be effective at subspace recovery and is proven to aid clustering algorithms in some specific settings, its improvement of noisy data is still not well quantified in general.

In this paper, we propose a novel metric called _compression ratio_ to capture the effect of PCA on high-dimensional noisy data. We show that, for data with _underlying community structure_, PCA significantly reduces the distance of data points belonging to the same community while reducing inter-community distance relatively mildly. We explain this phenomenon through both theoretical proofs and experiments on real-world data.

Building on this new metric, we design a straightforward algorithm that could be used to detect outliers. Roughly speaking, we argue that points that have a _lower variance of compression ratio_ do not share a _common signal_ with others (hence could be considered outliers).

We provide theoretical justification for this simple outlier detection algorithm and use simulations to demonstrate that our method is competitive with popular outlier detection tools. Finally, we run experiments on real-world high-dimension noisy data (single-cell RNA-seq) to show that removing points from these datasets via our outlier detection method improves the accuracy of clustering algorithms. Our method is very competitive with popular outlier detection tools in this task.

## 1 Introduction

Principal component analysis, commonly known as PCA, is one of the most fundamental tools in machine learning. PCA is primarily used as a dimensionality reduction tool that transforms high-dimensional data to lower dimensions for better visualization as well as a heuristic that reduces the complexity of the algorithms that are to be run on the data. On the other hand, it is also known to have certain denoising effects on high-dimensional data. This denoising phenomenon has been observed in different domains, including biological data [1, 20], speech data [16], signal measurement data [1, 1], image data [2] among others. The denoising effect of PCA has been extensively studied over the last decades [1, 2, 1, 1, 2, 3, 4].

One of the most fundamental problems in unsupervised learning is the analysis of data in the presence of community structures [1]. This includes clustering of such data [13], visualization [14], outlier detection [1], and others. The primary progress in understanding the denoising effect of PCA has been solely in clustering, particularly in connection to the K-Means algorithm [15, 16, 1], where PCA in combination with a K-Means based iterative algorithm is shown to provide a good clustering of that dataset with mild assumptions with the underlying community structure.

However, PCA seems to have a more "general" denoising effect in data, as it improves the performance of various downstream algorithms, including clustering [17] as well community structure preserving graph embedding [13] and this denoising effect is evident in many real-world datasets.

### Contributions

To this end, we propose a metric, called _compression ratio_, that quantifies PCA's improvement of high dimensional noisy data with underlying community structure in a geometric, and thus algorithm-independent manner 2.

Footnote 2: From hereon, we use the word community to refer to the underlying structure of the data, whereas clustering of data refers to the outcome of a particular clustering algorithm on the dataset.

Compression ratio.Let \(\mathbf{u}\) and \(\mathbf{v}\) be two data points from a dataset and let \(\Pi_{t}\) be the \(t\)-dimensional PCA projection operator. Then the compression ratio between the two points is defined as the ratio between their pre-PCA and post-PCA distance, which is

\[\frac{\|\mathbf{u}-\mathbf{v}\|}{\|\Pi_{t}(\mathbf{u})-\Pi_{t}(\mathbf{v})\|}.\]

In a dataset with a community structure, we show the compression ratio for intra-community pairs is higher than that of inter-community pairs even in settings where the pre-PCA inter-community and intra-community distances are very similar. We demonstrate (through a _random vector mixture model_) that this ratio gap reflects the denoising effect of PCA. As a consequence, PCA brings points from the same community much closer, improving the performance of downstream algorithms such as K-Means.

As a motivating byproduct, we show that this metric can be used to design an outlier detection method that can detect points deviating from a community structure. Furthermore, we show that this method can improve the accuracy of clustering algorithms in real-world high-dimensional datasets.

Outlier detection method.Our outlier detection is a simple process inspired by compression ratio. Intuitively, any data point that belongs to an underlying community should have large compression ratios with many points from the same community, whereas it will have a lower compression ratio w.r.t inter-community points. On the other hand, outliers will have more similar compression ratios with all the other points. This difference can be captured by the variance of the list of compression ratios between one point and all of the other points, with outliers having a lower variance of compression. Thus our algorithm simply removes points with low variance of compression.

We analyze this simple algorithm in an extension of the standard random vector mixture model. We also compare our algorithm with popular algorithms such as the Local Outlier Factor (LOF) method [1] and KNN-dist [15] as well as more recent methods such as Isolation forest [12] and ECOD [12] through both simulations and experiments on real-world data. We show that this simple algorithm is very competitive with those popular outlier detection tools.

Overall, we believe the effect of PCA on denoising becomes more significant if for each datapoint, there are many data points with large compression ratio variance.

Real world experimentsFinally, we test the relevance of compression ratio as a metric and the outlier detection method in real-world data. We focus on single-cell data, as it is both high dimensional (\(20,000-40,000\) dimension) and noisy [1], using datasets from a popular benchmark database [15] with ground truth community labels. We first show that the average intra-community compression ratio is higher than the average inter-community compression ratio in all of the datasets. We then show that removing outliers in these datasets via our variance of compression technique improves the performance of clustering algorithms, such as PCA+K-Means, where we again outperform standard outlier detection methods.

Organization of the paperIn Section 2, we discuss our theoretical analysis. Concretely, we define the random vector mixture model and provide bounds on the intra-community and inter-community compression ratios. Next, we define our outlier detection metric and justify it in an extension of our generative model. Section 3 contains the simulation results validating the compression ratio metric and we also compare the performance of our outlier detection method with other methods. Finally in Section 4 we demonstrate that PCA exhibits an average version of compression ratio in real-world biological datasets [1] and then test our outlier detection-based clustering accuracy improvement idea discussed above.

### Related Works

PCA and its effect on noisy data has been subject to a lot of investigation in the last 50 years. Before 2008, most of the work focused on the asymptotic setting, where the number of points (\(n\)) and/or the dimension (\(d\)) are infinite (see [1, 2, 3] and the references therein). In the last two decades, several works have also considered the finite sample setting [13, 14]. These works have primarily focused on the denoising aspect of PCA in different variants of Gaussian noise. In a recent line of work [20] has studied the subspace recovery problem in the presence of bounded and (nice) sub-Gaussian noise. However, there seems to be no direct way to convert these results into a clustering setting. In comparison, we study PCA's denoising effect on data in random vector mixture model via the compression ratio metric, where the noise can be heavy sub-Gaussian.

PCA in clustering tasksWith regards to PCA's impact on data with community structure, the primary work has been in connection to K-Means. Here, one of the first works [1] showed that the outcome of PCA can be viewed as an approximation result to the K-Means outcome in clustering data. In this direction, a lot of progress has been made in the last two decades.

A beautiful recent work [15] has shown that PCA followed by several iterations of K-Means along with modifications can cluster data with reasonable parameters in the random vector mixture model that we discuss here, which was then improved in [1]. Both of the works focused on the setting of \(n\gg d\) (for example, [15] worked with \(n\geq d^{8}\)). More recently, tighter results have been obtained in the context of the Gaussian-mixture model in [11] (still on the setting of \(n\gg d^{2}\)).

In comparison, we study PCA's relative compression in an algorithm-independent fashion, focusing on its effect on the geometry of the data in the high-dimensional setting of \(n=\Omega(d)\) with sub-Gaussian noise. We are motivated to analyze this setting as single-cell datasets often have \(n<d\).

## 2 Random vector model and relative compression of PCA

To theoretically study the relative compression of PCA, we use a high-dimensional mixture model, similar to one in [15, 1]. We call this a random vector mixture model. This can also be interpreted as a signal-plus-noise model where the signal imposes a community structure on the data. The dataset of interest is a set of \(n\) many \(d\) dimensional real vectors \(\mathbf{x}_{\mathbf{i}}\in\mathbb{R}^{d},1\leq i\leq n\), which is together represented as the dataset \(X\). We express \(X\) as a \(d\times n\) matrix, with each column representing a data point. The data points have an underlying hidden community structure that is expressed as a partition of \([n]:=\{1,\ldots,n\}\) into \(k\) many sets \(V_{1},\ldots,V_{k}\) such that each \(i\in[n]\) lies in any one \(V_{j}\). We then have the following problem structure.

1. Each cluster \(V_{j},1\leq j\leq k\) is associated with a ground truth center \(\mathbf{c}_{\mathbf{j}}\in\mathbb{R}^{d}\).
2. Additionally, each cluster \(V_{j}\) is associated with a distribution \(\mathcal{D}^{(j)}\) such that \(\mathcal{D}^{(j)}\) is a _coordinate wise independent zero mean_ distribution. For ease of exposition, we define the support of \(\mathcal{D}^{(j)}\) to be \([-\alpha,\alpha]^{d}\) for some \(\alpha\) (which can also depend on \(n,d\)), but our methods also directly apply to sub-Gaussian distributions where each coordinate has a constant sub-Gaussian norm (resulting in \(\mathcal{O}(\sqrt{d})\) norm of any column vector). Then \(\alpha\) would be replaced with \(C^{\prime}\log n\) for some constant \(C^{\prime}\) in our bounds.

Then, the dataset \(X\) is set up as follows.

**Definition 2.1** (Random vector mixture model).: For each \(i\in[n]\), if \(i\in V_{j}\), then \(\mathbf{x_{i}}=\mathbf{c_{j}}+\mathbf{e_{i}}\) where \(\mathbf{e_{i}}\sim\mathcal{D}^{(j)}\), i.e. \(\mathbf{e_{i}}\) is independently sampled from \(\mathcal{D}^{(j)}\). Here we abuse notation and denote both \(i\in V_{j}\) as well as \(\mathbf{x_{i}}\in V_{j}\).

With this setup, now we define the PCA projection operator and the compression ratio metric formally.

**Definition 2.2** (The PCA operator \(\Pi_{X}^{k^{\prime}}\)).: Let \(X\) be a \(d\times n\) matrix. Then the \(k^{\prime}\) dimensional PCA projection operator is simply the projection operator onto the first \(k^{\prime}\) principal components of \(X\).

Next we formally define the compression ratio metric.

**Definition 2.3**.: For any pair \((i,i^{\prime})\) we define the \(k^{\prime}\)-PC compression of the pair of vectors in \(X\) as

\[\Delta_{X,k^{\prime}}(i,i^{\prime})=\frac{\|\mathbf{x_{i}}-\mathbf{x_{i^{\prime}}}\|} {\|\Pi_{X}^{k^{\prime}}(\mathbf{x_{i}})-\Pi_{X}^{k^{\prime}}(\mathbf{x_{i^{\prime}}})\|}\]

Before describing our results, we define certain parameters of the model.

1. The maximum variance of the entries, \(\sigma\) is defined as \(\sigma^{2}=\max_{1\leq j\leq j,1\leq\ell\leq d}\mathrm{Var}[\mathcal{D}_{\ell} ^{(j)}]\)
2. The average variance of a column in a distribution \(\mathcal{D}^{(j)}\), noted as \(\sigma_{j}\) is defined as \(\sigma_{j}^{2}=\frac{1}{d}\sum_{\ell}Var([\mathcal{D}_{\ell}^{(j)}])\). Here, \(\sigma_{j}\sqrt{d}\) is the perturbation on the data points of \(V_{j}\) due to the noise.

In this direction, we first lower, and upper bound the \((k-1)\)-PC intra-community and inter-community compression ratios respectively, as a function of the maximum variance, average variances, spectral structure of the noise and signal, and distance between the centers of the model, which can be found in Theorem B.1.

Although our result applies to any set of centers, the spectral properties of the resultant matrix, and their interactions make the result hard to interpret. To give more insight into our bounds, we instead define a restricted (still natural) structure on the centers, which allows us to give a more interpretable result in this case. For simplicity, we also work in the setting where \(d\geq 10\alpha\sqrt{n}\log n\).

**Definition 2.4** (Spatially unique centers).: We say a set of vectors \(\mathbf{C}=\{\mathbf{c_{1}},\ldots,\mathbf{c_{k}}\}\) are \(\gamma\)-spatially unique, if we have that

\[\min_{1\leq j\leq k}\ \min_{\mathbf{v}\in\mathsf{Span}(\mathbf{C})\mathbf{c_{j }})}\|\mathbf{c_{j}}-\mathbf{v}\|\geq\gamma\]

This implies that each center has a unique pattern that cannot be approximated by a combination of the other centers. Here note that \(\gamma\geq\min_{j\neq j^{\prime}}\|\mathbf{c_{j}}-\mathbf{c_{j^{\prime}}}\|\). For example, such a property is expected if the centers are mutually orthogonal. One can also think of them as vertices in a high-dimensional regular polygon. Then, we give some sufficient conditions for the separation of intra-community and inter-community compression ratios of PCA.

**Theorem 2.5** (Separation of compression ratio).: _Let \(\gamma\geq C\max\{\sigma\sqrt{k}d^{1/4},\sigma\sqrt{k}+\alpha\log n\}\) for some constant \(C\). Furthermore, let \(i\sim i^{\prime}\) denote that \(\mathbf{y_{i}}\) and \(\mathbf{y_{i^{\prime}}}\) belong to the same underlying community. Then, the following holds._

1. _The perturbation of the points due to noise can be much larger than the distance between the community centers, i.e., the noise dominates the distance between the centers._
2. _With probability_ \(1-\mathcal{O}(1/n)\)_,_ \(\min_{(i,i^{\prime}):i\sim i^{\prime}}\Delta_{X,k-1}(i,i^{\prime})\geq 4\cdot \max_{(i,i^{\prime}):i\nsim i^{\prime}}\Delta_{X,k-1}(i,i^{\prime})\)__

This shows that the compression ratio of PCA provides a separation between intra-community and inter-community pairs even in a setting where the noise highly dominates the distance between the centers. One can find a more general theorem w.r.t spatially unique centers in Theorem C.4.

A natural question is whether post-PCA distance is a good metric for denoising due to PCA. In this regard, we point out that the compression ratio has an added normalization property. For example, consider the case where all pair-wise center distances are the same. In such a case, the post-PCA distances are dependent on \(\sigma_{j}\), so communities with larger variances have larger intra-community distances. However, this gets normalized in the compression factor as per Equation (9) of Theorem C.4, as the numerator also has a dependency on \(\sigma_{j}\).

### Outlier detection with compression ratio

Now, we discuss the usefulness of compression ratio on outlier detection. We first describe the notion of variance of compression ratio.

**Definition 2.6** (Variance of compression ratio).: Given a dataset \(X\) and a PCA dimension \(k^{\prime}\), variance of compression ratio of a point \(\mathbf{u}\in\mathds{X}\) is defined as

\[\mathsf{VAR}\Delta_{X,k^{\prime}}(\mathbf{x_{i}})=\mathrm{Var}(\{\Delta_{X,k^{ \prime}}(i,i^{\prime})\}_{i^{\prime}\neq i}) \tag{1}\]

where \(\Delta_{X,k^{\prime}}(i,i^{\prime})=\frac{\|\mathbf{x_{i}}-\mathbf{x_{i^{\prime}}}\|}{ \|\Pi_{X}^{k}(\mathbf{x_{i}})-\Pi_{X}^{k^{\prime}}(\mathbf{x_{i^{\prime}}})\|}\) is the compression ratio between points \(i\) and \(i^{\prime}\).

That is, it is simply the variance of the list of compression ratios of \(\mathbf{x_{i}}\) with all the other points \(\mathbf{x_{i^{\prime}}}\).

Then, our intuition is that if data consists of many points from the high dimensional mixture model, as well as several outlier points that don't share a common signal (center), they have a lower variance of compression ratio. We concretize this notion with the following simple detection algorithm 1.

```
Input: data \(X\), PCA dimension \(k^{\prime}\), number of outliers \(o\). for\(i=1\)to\(n\)do \(SC[i]\leftarrow\mathsf{VAR}\Delta_{X,k^{\prime}}(\mathbf{x_{i}})\) (\(\mathsf{VAR}\Delta_{X,k^{\prime}}\) defined in Eq. 1) endfor return\(o\) many indexes with lowest \(SC\) values.
```

**Algorithm 1** Outlier detection via variance of compression ratio

Mixture-model with outliersNow let us consider an extension of the mixture model in Definition 2.1 to incorporate outliers.

**Definition 2.7** (Mixture model with outliers).: Let \(X\) be a \(d\times n\) dataset with the partition \(V_{1},\ldots,V_{k},\hat{V}\), a set of \(k\) centers \(\{\mathbf{c_{j}}\}_{j=1}^{k}\) and distributions \(\{\mathcal{D}^{(j)}\}_{j=1}^{k}+1\) with the following generation method.

1. _clean points:_ If \(i\in V_{j},1\leq j\leq k\), \(\mathbf{x_{i}}=\mathbf{c_{j}}+\mathbf{e_{i}}\) where \(\mathbf{e_{i}}\) is sampled from \(\mathcal{D}^{(j)}\).
2. _outliers:_ If \(i\in\hat{V}\), then we sample \(p_{i,1},\ldots p_{i,k}\in[0.5,1]\). Then \(\mathbf{u_{i}}=\sum_{j}\alpha_{i,j}\mathbf{c_{j}}+\mathbf{e_{i}}\) where \(\alpha_{i,j}=\frac{p_{i,j}}{\sum_{j}p_{i,j}}\) and \(\mathbf{e_{i}}\) is sampled from \(\mathcal{D}^{(k+1)}\).

Let \(|\hat{V}|=n_{o}\) and \(n=n_{o}+n_{c}\). To keep the results simple, we make the average variance of each distribution \(\mathcal{D}^{(j)}\)_Then, for any \(n_{0}\leq n/2\), the first \(n_{0}\) points ranked by Algorithm 1 all belong to the outlier group (\(\hat{V}\)) with probability \(1-o(1)\)._

We discuss the connection between our results and the role of spatially unique centers in Appendix C. The proof of Theorems 2.5 and 2.8 can be found in Appendix C and C.2 respectively.

This gives us initial theoretical evidence that in the random-mixture model with outliers, our simple outlier detection method can detect outliers when a non-negligible fraction of the points are outliers. Next, we use simulations of our model to test the efficacy of our outlier detection method and its impact on the community structure of data and compare them with some popular outlier detection methods.

## 3 Simulations for outlier detection

In this section, we first describe different instantiations of the random-vector mixture model, observe the intra-community and inter-community compression ratios in them, and then run simulations in the outlier mode. All simulations and experiments were run on a 2020 M1 MacBook Pro with 16 GB of memory within 1.5 hours of total running time.

Simulation setupFor this setup, we set \(n=3000,d=1000\), and \(k=3\), with each community having the same number of points. For simplicity, we choose \(3\) equidistant centers, with \(\|\mathbf{c_{i}}-\mathbf{c_{j}}\|=\mathbf{c}\). We set the noise distributions to be Bernoulli distributions with variance \(\sigma_{1},\sigma_{2},\sigma_{3}\) respectively. We work in two primary settings, of equal and unequal noise.

i) _Equal noise_. Here we have \(\sigma_{j}=\sigma\) for all \(i\). ii)_Unequal noise_. Here one of the communities has variance \(2\sigma\), whereas all the other communities have variance \(\sigma\).

Then, we test the algorithms for three values of \(\sigma\) in the following manner.

* _Low noise:_ We choose \(\sigma:\|\mathbf{c_{j}}-\mathbf{c_{j^{\prime}}}\|\approx 3\sigma\sqrt{d}\). This implies distance between the centers dominates the perturbation due to noise.
* _Significant noise:_ Here \(\sigma:\|\mathbf{c_{j}}-\mathbf{c_{j^{\prime}}}\|\approx\sigma\sqrt{d}\). Here the noise norm and distance between centers are of the same order.
* _High noise:_ We have \(\sigma:\|\mathbf{c_{j}}-\mathbf{c_{j^{\prime}}}\|\approx 0.3\sigma\sqrt{d}\). Here the noise heavily dominates the center distances.

Let us look at the equal noise setting, i.e. the case where the variance of noise distributions for all communities are the same. We observe that in the low-noise setting, all intra-community compression ratios are higher than all inter-community compression ratios. As the noise increases, the gap between them decreases, so that in the high-noise setting, there is now an overlap between intra-community and inter-community compression ratios. We demonstrate this in Figure 0(a). This further indicates that compression ratio is indeed a useful metric even when the noise has a strong perturbation effect on the data (even though there will be no clean separation between intra-community and inter-community compression ratios once the noise is very high).

Figure 1: Simulation results for compression ratio and outlier detection

### Outlier detection

Now, we discuss the outlier detection, starting with the simulation setup in this case. We follow the random-mixture-outlier model and add outlier points along with the clean points as follows.

We add \(n_{o}=n_{c}/10\) outliers following definition 2.7. That is, we randomly choose \(\alpha_{1},\ldots,\alpha_{3}\) and then a random mixture-center is chosen as \(\sum_{j}\alpha_{j}\mathbf{c_{j}}\), and then we add a random noise vector from the Bernoulli distribution of variance \(\sigma_{4}\).

Outlier detection algorithms for comparisonsOutlier detection has been an active area of study in unsupervised learning, providing several influential algorithms. In a recent, comprehensive benchmarking of outlier detection algorithms, [1] compared the performance of several unsupervised learning algorithms on different datasets. They found that for unsupervised outlier detection methods, success was related to whether the underlying model of the data assumed by the outlier detection method followed the dataset at hand. They found that for local outliers, the popular Local Outlier Factor (LOF) method [1] performed the best statistically, whereas for global outliers, KNN-dist (where the outlier score is simply the distance to the \(K\)-th nearest neighbors) [16] performed the best. Owing to their generally impressive performance, we use them for comparison with our variance of compression method. Furthermore, we select a popular method called Isolation forest [11] and also a very recent and popular outlier detection method ECOD [11]. We also use PCA+method for each of the methods as benchmarks, as both outliers and clean points are perturbed by zero-mean noise, and we now understand PCA can help mitigate the effect of said noise, as discussed in Section 2.

Outlier detection resultsWe compare the AUROC values of the \(5\) outlier methods of interest in these settings. We record the results in Figure 1(a) and 1(b) for the equal and unequal noise settings respectively.

As we can see, our method results in the highest AUROC value, followed by PCA+KNN-dist. we make two observations.

i) The performance gap between variance-of-compression and the next best method is higher in the unequal noise setting.

ii)As the noise level increases, the gap between our method and PCA+K-NN dist increases.

These two points further highlight the compression ratio's normalizing effect as well as effectiveness in high noise settings.

Here we remark that in real-world data, while some points may indeed behave like outliers, they need not all be the same kind of outlier. Thus, we would like to verify our method's performance in the presence of a different kind of outlier, which we concretize below.

Higher variance-based outliersWe consider the case that some points may have significantly higher noise perturbations than others. In this setting, we randomly select some points, and we generate some points with \(c\cdot\sigma\) coordinate-wise variance, where \(c=8\) for our experiments (recall that the noise in the other points has a coordinate-wise \(\sigma\) variance). It is well known that if noise is low-dimensional, then such outliers are well captured by LOF. We observe that while in the low noise setting our performance is worse than the other methods, as the overall noise increases, the performance of our method is more comparable to the other methods. We record the results in Figure 0(b).

Figure 2: Comparison of outlier removal in different noise settingsThis shows that our outlier detection method is adept at detecting different kinds of outliers, outperforming popular outlier detection tools in some settings, and being competitive to them in others. We also observe that as the overall noise in the dataset increases, the performance of our method compared to the other outlier detection tools improves. This further highlights the power of compression ratio when especially dealing with noisy data. Having demonstrated the validity of our outlier detection method in two different settings, across different noise levels, we now focus on real-world datasets.

## 4 Real world experiments

### Datasets of interest

In this section, we provide experimental results to exhibit the validity of compression ratio as a metric and the usefulness of our outlier detection method in improving the community structure of datasets. We focus on single-cell RNA sequencing datasets. The dataset consists of \(n\) many data points, each corresponding to a cell. The features are gene expressions, and for the cell, the expression of some \(d\geq 10,000\) genes are recorded. A fundamental problem here is to identify sub-populations of interest. However, the problem is challenging as the biological process of recording gene expressions is error-prone [19], and gene expressions within the same population may also vary due to internal randomness. Furthermore, experiments can cause cells to get merged during gene-expression recording [20]. This makes single-cell RNA sequencing data a good testing ground for high dimensional noisy data with outliers and underlying community structure.

In this direction, we consider the single-cell RNA sequencing datasets from a benchmark paper [2]. These datasets also have moderate to highly reliable ground truth labels, that help us understand the usefulness of our metrics and our algorithm. These datasets vary in the number of cells, genes, clusters, cells per cluster, and the "difficulty" of clustering. A summary of the datasets is provided in Table 4 in Appendix E.1.

### Average compression in the datasets

As discussed in Section 2 and described in Theorem 2.5, our primary result showed that the intra-community compression ratios are higher than inter-community compression ratios in a large range of parameters. Here we look at average statistics of compression ratio to provide a first layer of evidence supporting this phenomenon in real-world data. We define the following metric. For any community \(V_{j}\), we define the average intra-community compression ratio as \(\operatorname{\mathsf{intra}}_{X,k^{\prime}}(V_{j})=\mathop{\mathbb{E}}_{i,i^ {\prime}\in V_{j}}\left[\frac{\|\mathbf{x}_{i}-\mathbf{x}_{i^{\prime}}\|}{\|\Pi_{X}^{ \alpha}(\mathbf{x}_{i}-\mathbf{x}_{i^{\prime}})\|}\right]\) Similarly, the average inter-community compression ratio is defined as \(\operatorname{\mathsf{inter}}_{X,k^{\prime}}(V_{j})=\mathop{\mathbb{E}}_{i\in V _{j},i^{\prime}\in[n]_{V_{j}}}\left[\frac{\|\mathbf{x}_{i}-\mathbf{x}_{i^{\prime}}\|} {\|\Pi_{X}^{\alpha}(\mathbf{x}_{i}-\mathbf{x}_{i^{\prime}})\|}\right]\). This gives an average measurement of the compression ratio in the dataset. In this regard, we find that for each of the 9 datasets and each of the communities in the dataset, the intra-community compression ratio is higher than the inter-community compression ratio. We provide the results in the Appendix E.2. Here, for brevity we present the average of \(\operatorname{\mathsf{intra}}_{X,k-1}(V_{j})\) and \(\operatorname{\mathsf{inter}}_{X,k-1}(V_{j})\) for each dataset in Table 1.

[MISSING_PAGE_FAIL:9]

## References

* [AM13] Mohiuddin Ahmed and Abdun Naser Mahmood. A novel approach for outlier detection and clustering improvement. In _2013 IEEE 8th Conference on Industrial Electronics and Applications (iciea)_, pages 577-582. IEEE, 2013.
* [And58] TW Anderson. An introduction to multivariate statistical analysis. _Wiley google schola_, 2:289-300, 1958.
* [ARS\({}^{+}\)04] P. Antonelli, H. E. Revercomb, L. A. Sromovsky, W. L. Smith, R. O. Knuteson, D. C. Tobin, R. K. Garcia, H. B. Howell, H.-L. Huang, and F. A. Best. A principal component noise filter for high spectral resolution infrared measurements. _Journal of Geophysical Research: Atmospheres_, 109(D23), 2004.
* [AS12] Pranjal Awasthi and Or Sheffet. Improved spectral-norm bounds for clustering. In _International Workshop on Approximation Algorithms for Combinatorial Optimization_, pages 37-49. Springer, 2012.
* [BKNS00] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jorg Sander. Lof: identifying density-based local outliers. In _Proceedings of the 2000 ACM SIGMOD international conference on Management of data_, pages 93-104, 2000.
* [CA16] M Emre Celebi and Kemal Aydin. _Unsupervised learning algorithms_, volume 9. Springer, 2016.
* [CTP19] Joshua Cape, Minh Tang, and Carey E. Priebe. The two-to-infinity norm and singular subspace geometry with applications to high-dimensional statistics. _The Annals of Statistics_, 2019.
* [DH04] Chris Ding and Xiaofeng He. K-means clustering via principal component analysis. In _Proceedings of the Twenty-First International Conference on Machine Learning_, ICML '04, page 29, New York, NY, USA, 2004. Association for Computing Machinery.
* [DK70] Chandler Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. iii. _SIAM Journal on Numerical Analysis_, 7(1):1-46, 1970.
* [DRS20] Angelo Duo, Mark Robinson, and Charlotte Soneson. A systematic performance evaluation of clustering methods for single-cell ma-seq data. _F1000Research_, 7:1141, 11 2020.
* [Han14] Paul Hanoine. An eigenanalysis of data centering in machine learning. _Preprint_, 2014.
* [HHAN\({}^{+}\)21] Yuhan Hao, Stephanie Hao, Erica Andersen-Nissen, William M. Mauck III, Shiwei Zheng, Andrew Butler, Maddie J. Lee, Aaron J. Wilk, Charlotte Darby, Michael Zagar, Paul Hoffman, Marlon Stoeckius, Efthymia Papalexi, Eleni P. Mimitou, Jaison Jain, Avi Srivastava, Tim Stuart, Lamar B. Fleming, Bertrand Yeung, Angela J. Rogers, Juliana M. McElrath, Catherine A. Blish, Raphael Gottardo, Peter Smibert, and Rahul Satija. Integrated analysis of multimodal single-cell data. _Cell_, 2021.
* [HHHH\({}^{+}\)22] Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. Adbench: Anomaly detection benchmark. _Advances in Neural Information Processing Systems_, 35:32142-32159, 2022.
* [HR03] DC Hoyle and M Rattray. Pca learning for sparse high-dimensional data. _Europhysics Letters_, 62(1):117, 2003.
* [Jac05] J Edward Jackson. _A user's guide to principal components_. John Wiley & Sons, 2005.
* [KAH19] Vladimir Yu Kiselev, Tallulah S Andrews, and Martin Hemberg. Challenges in unsupervised clustering of single-cell rna-seq data. _Nature Reviews Genetics_, 20(5):273-282, 2019.
* [KHK19] Yasunari Kusaka, Takeshi Hasegawa, and Hironori Kaji. Noise reduction in solid-state nmr spectra using principal component analysis. _The Journal of Physical Chemistry A_, 123(47):10333-10338, 2019. PMID: 31682439.

* [KK10] Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means algorithm. In _2010 IEEE 51st Annual Symposium on Foundations of Computer Science_, pages 299-308. IEEE, 2010.
* [Li18] Bingbing Li. A principal component analysis approach to noise removal for speech denoising. In _2018 International Conference on Virtual Reality and Intelligent Systems (ICVRIS)_, pages 429-432, 2018.
* [LTZ08] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In _2008 eighth ieee international conference on data mining_, pages 413-422. IEEE, 2008.
* [LZH\({}^{+}\)22] Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and George H Chen. Ecod: Unsupervised outlier detection using empirical cumulative distribution functions. _IEEE Transactions on Knowledge and Data Engineering_, 35(12):12181-12193, 2022.
* [LZZ21] Matthias Loffler, Anderson Y Zhang, and Harrison H Zhou. Optimality of spectral clustering in the gaussian mixture model. _The Annals of Statistics_, 49(5):2506-2530, 2021.
* [MBSP12] Y Murali, Murali Babu, Dr Subramanyam, and Dr Prasad. Pca based image denoising. _Signal & Image Processing_, 3, 04 2012.
* [MZ23] Xinyu Mao and Jiapeng Zhang. On the power of svd in the stochastic block model. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [MZ24] Chandra Sekhar Mukherjee and Jiapeng Zhang. Detecting hidden communities by power iterations with connections to vanilla spectral algorithms. In _Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 846-879. SIAM, 2024.
* 2817, 2008.
* [Nad14] Raj Rao Nadakuditi. Optshrink: An algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage. _IEEE Transactions on Information Theory_, 60(5):3002-3018, 2014.
* [RRS00] Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In _Proceedings of the 2000 ACM SIGMOD international conference on Management of data_, pages 427-438, 2000.
* [RV08] Mark Rudelson and Roman Vershynin. The littlewood-offord problem and invertibility of random matrices. _Advances in Mathematics_, 218(2):600-633, 2008.
* [RVdBB96] Peter Reimann, Chris Van den Broeck, and Geert J Bex. A gaussian scenario for unsupervised learning. _Journal of Physics A: Mathematical and General_, 29(13):3521, 1996.
* [THL\({}^{+}\)19] Xiaoning Tang, Yongmei Huang, Jinli Lei, Hui Luo, and Xiao Zhu. The single-cell sequencing: new developments and medical applications. _Cell & Bioscience_, 9(1):53, 2019.
* [TWT21] Francesco Trozzi, Xinlei Wang, and Peng Tao. Umap as a dimensionality reduction tool for molecular dynamics simulations of biomacromolecules: a comparison study. _The Journal of Physical Chemistry B_, 125(19):5022-5034, 2021.
* [VKS17] K Kirschner V Kiselev and M Schaub. SC3: consensus clustering of single-cell RNA-seq data. _Nature Methods_, 14:483-486, 2017.
* [VN17] Namrata Vaswani and Praneeth Narayanamurthy. Finite sample guarantees for pca in non-isotropic and data-dependent noise. In _2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 783-789. IEEE, 2017.
* [Vu18] Van Vu. A simple svd algorithm for finding hidden partitions. _Combinatorics, Probability and Computing_, 27(1):124-140, 2018.

* [VW15] Van Vu and Ke Wang. Random weighted projections, random quadratic forms and random eigenvectors. _Random Structures & Algorithms_, 47(4):792-821, 2015.
* [XL21] Nan Miles Xi and Jingyi Jessica Li. Benchmarking computational doublet-detection methods for single-cell rna sequencing data. _Cell systems_, 12(2):176-194, 2021.
* [XT15] Dongkuan Xu and Yingjie Tian. A comprehensive survey of clustering algorithms. _Annals of Data Science_, 2:165-193, 2015.

Organization of the appendix

In Section B we obtain our first, generic proofs for compression ratio. Next, in Section C we interpret our results through the lens of spatially unique centers, and also prove our variance of compression result on outlier detection in this setting. Next in Section D we extend the results of Section B when number of components is more than \(k+1\).

Section E contains continuation of experimental results from the main paper. We conclude with some future directions in Section F.

## Appendix B Primary theorem and proof

In this section, we describe our primary compression ratio related result in the random vector mixture model. We first describe our result when the projection dimension is \(k-1\). We first define some notations and useful results that we will use.

### Preliminaries

We first define the SVD projection operator for a matrix \(X\). Let the \(k^{\prime}\)-dimensional SVD projection operator for a matrix \(X\) be \(P_{X}^{k^{\prime}}\).

Next, for the dataset matrix \(X\), we denote by \(Y\) its centered version. Then we have \(\Pi_{X}^{k^{\prime}}=P_{Y}^{k^{\prime}}\).

Then the compression ratio of the data pair \((i,i^{\prime})\), defined as \(\frac{\|\mathbf{x}_{i}-\mathbf{x}_{i^{\prime}}\|}{\|\Pi^{k^{\prime}}(\mathbf{x}_{i}-\mathbf{x} _{i^{\prime}})\|}\) is in fact \(\frac{\|\mathbf{y}_{i}-\mathbf{y}_{i^{\prime}}\|}{\|P_{Y}^{k^{\prime}}(\mathbf{y}_{i}- \mathbf{y}_{i^{\prime}})\|}\). Then we have the following bound on the compression ratios in the random vector mixture model.

**Theorem B.1** (Main result).: _Let \(X\) be a \(d\times n\) dataset setup in the random vector mixture model with \(k\) underlying communities, so that all centers \(\mathbf{c_{j}}\) and all column vectors \(\mathbf{x}_{i}\in X\) are in \([-\alpha,\alpha]^{d}\). Let \(Y\) be the corresponding centered dataset. Considering the following notations,_

1. \(\delta_{k^{\prime}}(M):=s_{k^{\prime}}(M)-s_{k^{\prime}+1}(M)\) _for any_ \(M\)_,_
2. \(\sigma^{2}\) _be the maximum variance of the random variables,_
3. \(\mathcal{N}:=C_{0}\sigma\sqrt{d+n}\) _for some constant_ \(C_{0}\)__

_If \(\sigma^{2}\geq C_{1}\frac{\log n}{n}\) for some constant \(C_{1}\) then with probability \(1-\mathcal{O}(1/n)\) we have that the \((k-1)\)-PC compression ratio, \(\Delta_{X,k-1}(i,i^{\prime})\) of all intra-cluster pairs \((i,i^{\prime})\) is lower bounded as_

\[\begin{split}&\Delta_{X,k-1}(i,i^{\prime})\geq\\ &\frac{\sqrt{2d\sigma_{j}^{2}-12\alpha\sqrt{d}\log n}}{2\sqrt{2} \left(\sigma\sqrt{k}+C_{1}\cdot\alpha\cdot\sqrt{\log n}+\frac{2\mathcal{N} \cdot\sqrt{\sigma_{j}^{2}d+12\sqrt{d}\log n}}{\delta_{k-1}(Y)}\right)}\end{split} \tag{2}\]

_Similarly, the compression ratio of all inter-cluster pairs (i,i') is upper bounded by_

\[\begin{split}&\Delta_{X,k-1}(i,i^{\prime})\leq\\ &\frac{\sqrt{d(\sigma_{j}^{2}+\sigma_{j^{\prime}}^{2})+\|c_{j}-c_{ j^{\prime}}\|^{2}+12\alpha\sqrt{d}\log n}}{\sqrt{2}\left(\|c_{j}-c_{j^{\prime}}\|-2 \left(\sigma\sqrt{k}+C_{1}\cdot\alpha\cdot\sqrt{\log n}+\frac{2\mathcal{N} \cdot\sqrt{\|c_{j}-c_{j^{\prime}}\|^{2}+2d\sigma^{2}+12\sqrt{d}\log n}}{ \delta_{k-1}(Y)}\right)\right)}\end{split} \tag{3}\]

_with probability \(1-\mathcal{O}(1/n)\)._

Here we make the following remark about the range of the datapoints.

### Definitions and notations

We start with the definition of the norm operator \(\|\cdot\|\), which we use in the following two contexts.

1. If \(\mathbf{u}\) is a \(d\) dimensional vector, then \(\|\mathbf{u}\|\) denotes the \(\ell_{2}\) norm of \(\mathbf{u}\), which is \(\sqrt{\sum_{i=1}^{d}(\mathbf{u}_{i})^{2}}\). Then \(\|\mathbf{u}-\mathbf{v}\|\) is the \(\ell_{2}\) distance between the two vectors.
2. If \(M\) is a \(d\times n\) matrix \(M\), \(\|M\|\) denotes the spectral norm of the matrix. That is, \[\|M\|=\max_{\mathbf{u},\|\mathbf{u}\|\leq 1}\{\|M\mathbf{u}\|\}\]

We follow this by defining some more structures related to random matrices.

1. For any matrix \(M\), we denote \(\bar{M}:=\mathbb{E}[M]\). Then by definition, \(\bar{X}\) is a \(d\times n\) matrix such that if \(i\in V_{j}\), the \(i\)-th column of \(\bar{X}\) is \(c_{j}\) (as \(\mathcal{D}^{(j)}\) is a coordinate wise zero mean distribution), the ground truth center of \(V_{j}\). Thus, we denote by \(\bar{X}\) as the ground-truth or expectation matrix of \(X\). Similarly \(\bar{Y}\) is the center matrix of \(Y\) (recall that \(Y\) is the column centered matrix of \(X\)). Furthermore let \(\bar{M}_{i}\) be the \(i\)-th column of \(\bar{M}\). Then \(\|\bar{y}_{i}-\bar{y}_{i^{\prime}}\|=\|\bar{x}_{i}-\bar{x}_{i^{\prime}}\|\) for any \((i,i^{\prime})\) pair. Thus we can call \(\bar{Y}\) as the ground truth matrix of \(Y\).
2. Corresponding to any matrix \(M\), we denote \(E_{M}:=M-\mathbb{E}[M]\).
3. **Choice of subscripts:** From hereon we use the subscript \(i\) to denote the columns of \(X\) and \(Y\). We use the subscript \(j\) for cluster identities and \(\ell\) for rows of the matrices or the column vectors.

With this a background, we give a short sketch of the proof.

Looking at the numerator and denominator separately:Proving the relative compressibility result requires the following results in turn. Recall that the compression ratio is the ratio between pre PCA and post PCA distances between pair of datapoints and we want to lower bound "intram-community" compression ratio and upper bound "inter-community" compression ratio. This means we need the following bounds to prove Theorem B.1.

1. For the intra-community pairs of vectors, prove a lower bound on the pre PCA distance and upper bound on the post PCA distances.
2. For the inter-community pairs of vectors, prove an upper bound on the pre PCA distance and a lower bound on the post PCA distance.

We first obtain the pre PCA distance bounds, which are straightforward to obtain using the fact that the randomness in the vectors of \(X\) are coordinate wise independent, and that \(\|\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}}\|=\|\mathbf{x_{i}}-\mathbf{x_{i^{\prime}}}\|\) for any \((i,i^{\prime})\) pair.

### Pre PCA distances

**Lemma B.2**.: _Let \(\mathbf{y_{i}}\) and \(\mathbf{y_{i^{\prime}}}\) be two vectors (datapoints) of \(Y\) with ground truth communities \(V_{j}\) and \(V_{j^{\prime}}\) respectively. If \(j=j^{\prime}\) then we have \(\|\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}}\|\geq\sqrt{2d\sigma_{j}^{2}-12\alpha\sqrt{d}\log n}\) with probability \(1-\mathcal{O}(n^{-3})\), otherwise if \(j\neq j^{\prime}\) then we have \(\|\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}}\|\leq\sqrt{d(\sigma_{j}^{2}+\sigma_{j^{ \prime}}^{2})+\|c_{j}-c_{j^{\prime}}\|^{2}+12\alpha\sqrt{d}\log n}\) with probability \(1-\mathcal{O}(n^{-3})\)._

Proof.: We know that for any \((i,i^{\prime})\) pair \(\|\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}}\|=\|\mathbf{x_{i}}-\mathbf{x_{i^{\prime}}}\|\). Using this fact we prove the bounds on the datapoints of \(X\).

First we consider the case when \(\mathbf{x_{i}}\) and \(\mathbf{x_{i^{\prime}}}\) belong to the same community. Then \(\|\mathbf{x_{i}}-\mathbf{x_{i^{\prime}}}\|^{2}=\sum_{t=1}^{d}((\mathbf{x_{i}})_{\ell}-( \mathbf{x_{i^{\prime}}})_{\ell})^{2}\). Here for each \(\ell\) we define \(\mathbf{w}_{\ell}=(\mathbf{x_{i}})_{\ell}-(\mathbf{x_{i^{\prime}}})_{\ell}=(c_{j})_{\ell}+ (\mathbf{e_{i}})_{\ell}-(c_{j})_{\ell}-(\mathbf{e_{i^{\prime}}})_{\ell}=(\mathbf{e_{i}})_{ \ell}-(\mathbf{e_{i^{\prime}}})_{\ell}\). Then \(\operatorname{E}\left[\mathbf{w}_{\ell}^{2}\right]=\operatorname{E}[((\mathbf{e_{i}}) _{\ell})^{2}]+\operatorname{E}[((\mathbf{e_{i^{\prime}}})_{\ell})^{2}]=Var((\mathbf{e_{ i}})_{\ell})+Var((\mathbf{e_{i^{\prime}}})_{\ell})\).

We define \(\sigma_{i,i}^{2}=Var\left((\mathbf{e_{i}})_{\ell}\right)\) (to use the more familiar row major representation). Recall that both \(\mathbf{e_{i}}\) and \(\mathbf{e_{i^{\prime}}}\) are sampled from \(\mathcal{D}^{(j)}\) and \(\sigma_{j}^{2}\) is the average of variance of the coordinates of the distribution \(\mathcal{D}^{(j)}\). i.e., \(\operatorname{E}\left[\sum_{\ell}\mathbf{w}_{\ell}^{2}\right]=\sum_{\ell=1}^{d} \sigma_{l,i}^{2}+\sigma_{l,i^{\prime}}^{2}=2d\sigma_{j}^{2}\). Now recall that the random variable \(\mathbf{w}_{\ell}\) is in the range \([-2,2]\) for any \(\ell\). Then applying Hoeffding bound on this setup we get

\[\Pr\left[\sum_{\ell=1}^{d}\mathbf{w}_{\ell}^{2}\leq 2d\sigma_{j}^{2}-12\alpha \sqrt{d}\log n\right]\leq n^{-3}.\]Thus, if \(\mathbf{x_{i}}\) and \(\mathbf{x_{i^{\prime}}}\) belong to the same community then with probability \(1-\mathcal{O}(n^{-3})\) we have \(\|\mathbf{x_{i}}-\mathbf{x_{i^{\prime}}}\|\geq\sqrt{2d\sigma_{j}^{2}-12\alpha^{2}\sqrt{ d}\log n}\).

Similarly, if \(\mathbf{x_{i}}\) and \(\mathbf{x_{i^{\prime}}}\) belong to different communities \(V_{j}\) and \(V_{j^{\prime}}\), we have the random variable \(\mathbf{w_{\ell}}=\left(\mathbf{x_{i}}\right)_{\ell}-\left(\mathbf{x_{i^{\prime}}}\right)_ {\ell}\) with mean \(\left(c_{j}\right)_{\ell}-\left(c_{j^{\prime}}\right)_{\ell}\) (due to the difference in the centers ) and variance \(\sigma_{l,i}^{2}+\sigma_{l,j}^{2}\), where we define \(c_{\ell,j}=\left(c_{j}\right)_{\ell}\). Then \(\mathrm{E}\left[\mathbf{w}_{\ell}^{2}\right]=\sigma_{l,i}^{2}+\sigma_{l,j}^{2}+ \left(c_{\ell,j}-c_{\ell,j^{\prime}}\right)^{2}\) and

\[\mathrm{E}\left[\sum_{\ell=1}^{d}\mathbf{w}_{\ell}^{2}\right]=\sum_{\ell=1}^{d} \sigma_{\ell,i}^{2}+\sigma_{\ell,j}^{2}+\left(c_{\ell,j}-c_{\ell,j^{\prime}} \right)^{2}=d(\sigma_{j}^{2}+\sigma_{j^{\prime}}^{2})+\|\mathbf{c_{j}}- \mathbf{c_{j^{\prime}}}\|^{2}\]

Applying Hoeffding bound we get

\[\Pr\left[\sum_{\ell=1}^{d}\mathbf{w}_{\ell}^{2}\geq d(\sigma_{j}^{2}+\sigma_{j^{ \prime}}^{2})+\|c_{j}-c_{j^{\prime}}\|^{2}+12\alpha\sqrt{d}\log n\right]\leq n ^{-3}.\]

Thus if \(\mathbf{x_{i}}\) and \(\mathbf{x_{i^{\prime}}}\) belong to different communities then with probability \(1-n^{-3}\) we have \(\|\mathbf{x_{i}}-\mathbf{x_{i^{\prime}}}\|\leq\sqrt{d(\sigma_{j}^{2}+\sigma_{j^{\prime }}^{2})+\|c_{j}-c_{j^{\prime}}\|^{2}+12\alpha\sqrt{d}\log n}\). 

Now, we move into the analysis of post-PCA distances, which is the more technical part of the proof.

### Post PCA distance

High-level idea.The idea behind the proof is simple.

In our setup, \(\bar{X}=\mathbb{E}[X]\) is the ground truth matrix, such that if the \(i\)-th column of \(X\) belongs to \(V_{j}\), then the \(i\)-th column of \(\bar{X}\) is \(\mathbf{c_{j}}\). Thus, \(\bar{X}\) is rank \(k\) and thus \(\|P_{\bar{X}}^{k}\left(\mathbf{c_{j}}-\mathbf{c_{j^{\prime}}}\right)\|=\| \mathbf{c_{j}}-\mathbf{c_{j^{\prime}}}\|\). This implies \(\bar{Y}\) has rank \(k-1\) and \(\|P_{\bar{Y}}^{k-1}(\mathbf{c_{j}}-\mathbf{c_{j^{\prime}}})\|=\|\mathbf{c_{j}} -\mathbf{c_{j^{\prime}}}\|\). The crux of the proof is to show that \(P_{\bar{Y}}^{k-1}\) can be well approximated with \(P_{Y}^{k-1}\), even when \(Y\) and \(\bar{Y}\) differ significantly (due to the noise).

To achieve this result we use tools from spectral analysis of random matrices, i.e. tools that study the behavior of eigenvalue and eigenvectors of random matrices. Here we face two hurdles.

1. First we note that the matrix \(Y\) is rectangular and unsymmetric. The vast majority of tools in spectral analysis of random matrix theory are focused on symmetric matrices. To use this to our advantage we focus on a closely related symmetric matrix through the following symmetrization trick, which is essential to the proof. We define the matrix \(Z\) as \(Z:=\begin{bmatrix}0&Y\\ Y^{T}&0\end{bmatrix}\). This is a \(d+n\times d+n\) symmetric matrix. We show that \(P_{Y}^{k-1}\) can be analyzed through \(P_{Z}^{k-1}\) and then approximate the second projection operator using \(P_{\mathbb{E}[Z]}^{k-1}\), borrowing tools from classical random matrix theory. This gives us preliminary post PCA distance bounds expressed using \(\|Y-\bar{Y}\|\).
2. Then obtaining the exact bounds of Theorem B.1 require bounds on the spectral norm of \(Y-\bar{Y}\). There exists a rich literature on spectral norm of random symmetric matrices with independent entries, but \(Y-\bar{Y}\) does not satisfy this either. This is because, since \(Y\) is obtained by subtracting the column mean from each vector of \(X\), the entries of \(Y\), and thus \(Y-\bar{Y}\) are not independent either. To this end, we first obtain the said properties for \(X-\mathbb{E}[X]\) borrowing tools from [21] on our symmetrization trick, and then accommodate for the effect of centering using results from [16] to complete our proof.

We now describe the symmetrization trick and its implications in detail.

#### b.4.1 A comparable symmetric case

We start by recalling the symmetric matrix corresponding to \(Y\), \(Z=\begin{bmatrix}0&Y\\ Y^{T}&0\end{bmatrix}\). As per our notations we denote \(\bar{Z}=\mathbb{E}[Z]\) and then we have \(\bar{Z}=\begin{bmatrix}0&\bar{Y}\\ \bar{Y}^{T}&0\end{bmatrix}\). Furthermore we have \(E_{Z}=Z-\bar{Z}\).

Then the eigenvectors of \(Z\) and singular vectors of \(Y\) (and similarly \(\bar{Z}\) and \(\bar{Y}\)) are related as follows.

_Fact_ B.3. Let the left and right singular vectors of \(Y\) be \(\hat{\mathbf{l}}_{t},1\leq t\leq d\) and \(\hat{\mathbf{r}}_{t},1\leq t\leq n\) respectively. Then the eigenvectors of \(Z\) are \(\frac{1}{\sqrt{2}}\begin{bmatrix}\hat{\mathbf{l}}_{t}\\ \hat{\mathbf{r}}_{t}\end{bmatrix}\) with eigenvalue \(\hat{\lambda}_{t}=s_{t}\) and \(\frac{1}{\sqrt{2}}\begin{bmatrix}\hat{\mathbf{l}}_{t}\\ -\hat{\mathbf{r}}_{t}\end{bmatrix}\) with eigenvalue \(\hat{\lambda}_{t}=-s_{t}\) where \(1\leq t\leq\min(d,n)\), The same follows for \(\bar{Y}\) and \(\bar{Z}\).

Here we also formally define \(P^{k}_{M}\) for symmetric matrices \(M\) as in this case we work with eigenvectors corresponding to top eigenvalues, instead of top singular values (as in case of \(Y\)), for clarity.

_Remark_ B.4. For any matrix \(M\), we have defined \(P^{k^{\prime}}_{X}\) as the matrix whose rows are the top \(k^{\prime}\) singular vectors of \(M\).

However, when we discuss a symmetric matrix \(M^{\prime}\), \(P^{k^{\prime}}_{M^{\prime}}\) is a matrix whose rows are the eigenvectors corresponding to the top \(k^{\prime}\)_eigenvalues_ of \(M^{\prime}\).

This in turn gives us the following results connecting \(P^{k^{\prime}}_{Y}\) and \(P^{k^{\prime}}_{Z}\).

_Fact_ B.5. Let \(0^{n}\) be the \(n\) dimensional zero vector. Furthermore let \(\mathbf{v}|0^{n}:=\begin{bmatrix}\mathbf{v}\\ 0^{n}\end{bmatrix}\) for any vector \(\mathbf{v}\). Then for any \(d\)-dimensional vector \(\mathbf{v}\) we have \(\|P^{k^{\prime}}_{Y}\mathbf{v}\|=\sqrt{2}\left\|P^{k}_{Z}(\mathbf{v}|0^{n})\right\|\)

This result allows us to work with the symmetric matrices \(Z\) and \(\bar{Z}\) instead of \(Y\). Now we obtain the results needed to approximate \(P^{k^{\prime}}_{Z}\) with \(P^{k^{\prime}}_{Z}\).

Difference in spectral projections of \(\bar{Z}\) and \(Z\):Here we use the Davis-Kahan Theorem [1] along with a result by Cape et. al. [2] to obtain an upper bound between the norm of difference of the leading eigenspaces of \(Z\) and \(\bar{Z}\) under some appropriate orthonormal rotation that we shall use to obtain our results. The main reason behind using these tools is that the SVD projection matrix due to \(\bar{Z}\) is well behaved.

**Theorem B.6** (Davis-Kahan Theorem: [1]).: _Let \(D\) and \(\hat{D}\) be \(p\times p\) symmetric matrices, with eigenvalues \(\lambda_{1},\ldots,\lambda_{p}\) and \(\hat{\lambda}_{1},\ldots,\hat{\lambda}_{p}\) respectively. Define \(E_{D}=\hat{D}-D\) and \(\delta_{k^{\prime}}=\lambda_{k^{\prime}}-\lambda_{k^{\prime}+1},1\leq k<p\). Let \(U=[\mathbf{u_{1}}\ldots,\mathbf{u_{k^{\prime}}}]\) and \(\hat{U}=[\mathbf{\hat{u}_{1}}\ldots,\mathbf{\hat{u}_{k^{\prime}}}]\) are matrices in \(\mathbb{R}^{p\times k^{\prime}}\) where \(\mathbf{u_{i}}\) and \(\mathbf{\hat{u_{i}}}\) are eigenvectors of \(D\) and \(\hat{D}\) w.r.t to the \(i\)-th top eigenvalue. Then_

\[\left\|\sin\Theta\left(U,\hat{U}\right)\right\|\leq\frac{2\|E_{D}\|}{\delta_{ k^{\prime}}} \tag{4}\]

**Theorem B.7** (Perturbation under Procrustes Transformation: [2]).: _Let \(U\) and \(\hat{U}\) be two \(p\times k^{\prime}\) matrices such that the columns of \(U\) (and similarly \(\hat{U}\)) comprise of \(k^{\prime}\) many unit vectors that are mutually orthogonal._

_Then there exists a \(k^{\prime}\times k^{\prime}\) orthonormal matrix \(W_{U}\) such that_

\[\left\|\sin\Theta\left(U,\hat{U}\right)\right\|\leq\|U-\hat{U}W_{U}\|\leq\sqrt {2}\left\|\sin\Theta\left(U,\hat{U}\right)\right\|\]

Combining them we get the following result.

**Theorem B.8**.: _Given the matrices \(Y\) and \(\bar{Y}\) and \(Z\) and \(\bar{Z}\) defined as described above, there exists an orthonormal matrix \(W_{Z}\) such that_

\[\left\|(P^{k^{\prime}}_{Z})^{T}-(P^{k^{\prime}}_{Z})^{T}(W_{Z})^{T}\right\|\leq \frac{2\|E_{Z}\|}{\delta_{k^{\prime}}(Y)}\]

_This in turn implies_

\[\left\|P^{k^{\prime}}_{Z}-W_{Z}P^{k^{\prime}}_{Z}\right\|\leq\frac{2\|E_{Z}\| }{\delta_{k^{\prime}}(Y)}\]

Next, we obtain a result on the projection of a random vector on a \(k^{\prime}\) dimensional subspace.

[MISSING_PAGE_FAIL:17]

From Theorem B.8 we have that for a choice of \(W\left\|P_{Z}^{k-1}-WP_{Z}^{k-1}\right\|\leq\frac{2\|Z-\bar{Z}\|}{\delta_{k-1}(Z)}= \frac{2\|Z-\bar{Z}\|}{\delta_{k-1}(Y)}\).

Next, we can analyze \(\|WP_{Z}^{k-1}\mathbf{e_{i}}|0^{n}\|+\|WP_{Z}^{k-1}\mathbf{e_{i^{\prime}}}|0^{n}\|\) as \(\bar{Z}\) and the vectors are independent of each other. Then applying Corollary B.10 with probability \(1-\mathcal{O}(n^{-3})\) we have \(\|WP_{Z}^{k-1}\mathbf{e_{i}}|0^{n}\|+\|WP_{Z}^{k-1}\mathbf{e_{i^{\prime}}}|0^{n}\|\leq 2 \sigma\sqrt{k-1}+2C_{1}\sqrt{\log n}\). This completes the proof.

Preliminary inter-community bounds.Now, we move to the inter-community results. In this part \(P_{Z}^{k-1}\) plays an important role. This is because as per our discussion \(\|P_{\bar{Y}}^{k-1}(\mathbf{c_{j}}-\mathbf{c_{j^{\prime}}})\|=\|\mathbf{c_{j}}-\mathbf{c_{j^{ \prime}}}\|\). This implies

\[\|P_{\bar{Z}}^{k-1}(\bar{Y}_{i}|0^{d}-\bar{Y}_{i^{\prime}}|0^{d})\|=\|\mathbf{c_{j }}-\mathbf{c_{j^{\prime}}}\| \tag{6}\]

Using this result we then prove the following inter-community post PCA bound.

**Lemma B.12**.: _Let \(\mathbf{y_{i}},\mathbf{y_{i^{\prime}}}\) be two columns of the data matrix \(Y\) so that \(i\in V_{j}\) and \(i^{\prime}\in V_{j^{\prime}}\), where \(j\neq j^{\prime}\). Then for the constant \(C_{1}\) we have_

\[\|P_{Y}^{k-1}(\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}})\|\geq\sqrt{2}\left(\|\mathbf{c_{j}}- \mathbf{c_{j^{\prime}}}\|-2\left(\sigma\sqrt{k}+C_{1}\cdot\alpha\cdot\sqrt{\log n} \right)-\frac{2\|Z-\bar{Z}\|\cdot\|\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}}\|}{\delta_{k- 1}(Y)}\right) \tag{7}\]

_with probability \(1-\mathcal{O}(n^{-3})\)._

Proof.: As before we have \(\|P_{Y}^{k-1}(\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}})\|=\sqrt{2}\|P_{Z}^{k-1}(\mathbf{y_{i }}|0^{n}-\mathbf{y_{i^{\prime}}}|0^{n})\|\). Then we proceed with a basic decomposition. We have for any \(k-1\) dimensional matrix \(W\),

\[\|P_{Z}^{k-1}(\mathbf{y_{i}}|0^{n}-\mathbf{y_{i^{\prime}}}|0^{n})\|\] \[\geq \|WP_{Z}^{k-1}(\mathbf{c_{j}}|0^{n}-\mathbf{c_{j^{\prime}}}|0^{n})\|-\|WP _{Z}^{k-1}(\mathbf{e_{i}}|0^{n}-\mathbf{e_{i^{\prime}}}|0^{n})\|-\|(P_{Z}^{k-1}-WP_{Z}^ {k-1})(\mathbf{y_{i}}|0^{n}-\mathbf{y_{i^{\prime}}}|0^{n})\|\]

Now, we have \(\|WP_{Z}^{k-1}(\mathbf{c_{j}}|0^{n}-\mathbf{c_{j^{\prime}}}|0^{n})\|=\|P_{\bar{Y}}^{k -1}(\mathbf{c_{j}}-\mathbf{c_{j^{\prime}}})\|=\|c_{j}-c_{j^{\prime}}\|\).

Next from Lemma B.11 we have \(\|WP_{Z}^{k-1}(\mathbf{e_{i}}|0^{n}-\mathbf{e_{i^{\prime}}}|0^{n})\|\leq 2\left(\sigma \sqrt{k}+C_{1}\sqrt{\log n}\right)\) with probability \(1-\mathcal{O}(n^{-3})\).

Finally from Lemma B.11 we know we can upper bound \(\|(P_{Z}^{k-1}-WP_{Z}^{k-1})(\mathbf{y_{i}}|0^{n}-\mathbf{y_{i^{\prime}}}|0^{n})\|\) with \(\frac{2\|Z-\bar{Z}\|}{\delta_{k-1}(Y)}\cdot\|\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}}\|\), which completes the proof.

At this point, we have obtained the pairwise post-PCA intra-community and inter-community distance bounds in terms of \(\|\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}}\|,\|Z-\bar{Z}\|,k,\sigma\) and \(\delta_{k-1}(Y)\). Here \(\delta_{k-1}(Y)\) is the spectral gap of \(Y\) and we already have bounds on \(\|\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}}\|\). Next, we obtain bounds on \(\|Z-\bar{Z}\|\) and then put together the results obtained so far to prove Theorem B.1.

#### b.4.4 Spectral norm of the square marrix

First, we note down a result by Vu [20] for upper bounds on the spectral norm of random matrices with independent entries.

**Theorem B.13** (Norm of random symmetric matrix [20]).: _Let \(E\) be a \(n\times n\) random symmetric matrix where each entry in the upper diagonal is an independent random variable with \(0\) mean and \(\sigma\) variance, then there is a constant \(C_{0}\) such that_

\[\Pr\left[\|E\|\geq C_{0}\sigma\sqrt{n}\right]\leq n^{-3}\]

_where \(\sigma^{2}\geq C_{1}\frac{\log n}{n}\)._

However, since the entries of \(Y\) are not independent, the same follows with \(E_{Z}\). To bypass this issue we define the matrix \(B:=\begin{bmatrix}0&X\\ X^{T}&0\end{bmatrix}\) and then \(\bar{B}:=\mathbb{E}[B]=\begin{bmatrix}0&\bar{X}\\ \bar{X}^{T}&0\end{bmatrix}\)

Furthermore recall that \(E_{M}=M-\mathbb{E}[M]\). Then we have the following results.

1. \(\|E_{Z}\|\) is the largest eigenvalue of \(E_{Z}\), which is same as the largest singular value of \(E_{Y}\), that we denote as \(s_{1}(E_{Y})\).
2. \(\|E_{B}\|\) is same as the largest singular value of \(E_{X}\), that we denote as \(s_{1}(E_{X})\).

Furthermore we have from Theorem B.13 that \(\|E_{B}\|\leq C_{0}\sigma\sqrt{n}\) with probability \(1-\mathcal{O}(n^{-3})\). Finally we connect \(s_{1}(E_{Y})\) with \(s_{1}(E_{X})\). To do so, note that \(E_{Y}\) is the centered matrix of \(E_{X}\). This follows from the fact that \(E_{Y}=Y-\bar{Y}\) and \(E_{X}=X-\bar{X}\). Then we use the following result by Hanoine [14].

**Theorem B.14** ([14]).: _Let \(M\) be a rank \(m\) matrix and \(\bar{M}\) be the matrix obtained upon centering, with singular values (in descending order) \(s_{1},\ldots,s_{m}\) and \(\bar{s}_{1},\ldots,\bar{s}_{m-1}^{\prime}\) respectively. Then for any \(1\leq i<m\) we have \(s_{i}\geq s_{i}^{\prime}\geq s_{i+1}\)._

Using this result we get

\[\|E_{Z}\|=s_{1}(E_{Y})\leq s_{1}(E_{X})\leq\|E_{B}\|\]

Now, we bound \(\|E_{B}\|\), i.e. \(\left\|\begin{bmatrix}0&E_{X}\\ (E_{X})^{T}&0\end{bmatrix}\right\|\). This is a \((d+n)\times(d+n)\) random symmetric matrix with zero mean and maximum variance \(\sigma^{2}\). Then applying Theorem B.13 we get the following bound.

**Lemma B.15**.: _Recall that we define \(\mathcal{N}=C_{0}\sigma\sqrt{d+n}\). Then in the setting of Lemma B.11 we have \(\|Z-\bar{Z}\|\leq\mathcal{N}\) with probability \(1-\mathcal{O}(n^{-3})\)_

Against this backdrop we summarize our bounds to prove Theorem B.1.

### Proof of Theorem b.1

From Lemma B.2 we have the lower bound on the intra-community distances and upper bound on the inter-community distances. Similarly, we can also use the results to obtain lower bound for the intra-community case. It is easy to see that if (i,i') belong to the same community \(V_{j}\) then with probability \(1-\mathcal{O}(n^{-3})\), \(\|\mathbf{y_{i}}-\mathbf{y_{i}}\|\leq\sqrt{2d\sigma_{j}^{2}+12\alpha\sqrt{d}\log n}\).

Substituting this and the bound on \(\|Z-\bar{Z}\|\) to Lemma B.11 we have with probability \(1-\mathcal{O}(n^{-3})\)

\[\|P_{Y}^{k-1}(\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}})\|\leq 2\sqrt{2}\left(\sigma\sqrt{ k}+C_{1}\cdot\alpha\cdot\sqrt{\log n}+\frac{\mathcal{N}\cdot\sqrt{2d\sigma_{j}^{2} +12\alpha\sqrt{d}\log n}}{\delta_{k-1}(Y)}\right)\]

Similarly for the inter-community with \(i\in V_{j},i^{\prime}\in V_{j^{\prime}}\) from Lemma B.12 we have

\[\|P_{Y}^{k-1}(\mathbf{y_{i}}-\mathbf{y_{i^{\prime}}})\|\geq\sqrt{2}\left(\|\mathbf{c_ {j}}-\mathbf{c_{j^{\prime}}}\|-2\left(\sigma\sqrt{k}+C_{1}\cdot\alpha\cdot \sqrt{\log n}\right)-\frac{\mathcal{N}\cdot\sqrt{\|\mathbf{c_{j}}-\mathbf{c_ {j^{\prime}}}\|^{2}+d(\sigma_{j}^{2}+\sigma_{j^{\prime}}^{2})+12\alpha\sqrt{d }\log n}}{\delta_{k-1}(Y)}\right)\]

Finally, using the bounds of Lemma B.2 and applying a union bound on the total \(n^{2}\) pairs of datapoints completes the proof of the theorem.

## Appendix C Spatially unique centers and proof for the outlier detection theorem

The primary quantity that is hard to interpret in a dataset with an underlying community structure is \(\delta_{k-1}(Y)\). Here we make some observations. First note that \(\delta_{k-1}(Y)=s_{k-1}(Y)-s_{k}(Y)\). Now, \(s_{k-1}(Y)\geq s_{k}(X)\) and \(s_{k}(Y)\leq C\sigma\sqrt{d+n}\) where the latter term comes from the fact that \(s_{k}(Y)\leq\|E_{Y}\|\leq\|E_{X}\|\leq C\sigma\sqrt{d+n}\). This follows from a simple application of Weyl's inequality and the effect of centering on eigenvalues. For simplicity, we consider the case when \(s_{k}(X)\geq 4C\sigma\sqrt{d+n}\). We will come back to this and show that this assumption does make sense. Then, we have \(\delta_{k-1}(Y)\geq 0.66s_{k}(X)\). Next note that \(s_{k}(X)\geq s_{k}(\mathbb{E}[X])-\|E\|\).

This then implies that given the aforementioned conditions, we have

\[\delta_{k-1}\geq 0.25s_{k}(\mathbb{E}[X]) \tag{8}\]

where \(\mathbb{E}[X]\) is the center matrix, where each column is the center of the community the corresponding point belongs to.

Bounds on the singular values of the center matrix for \(\gamma\)-spatially unique centersHere, we make a connection between \(s_{k}(\mathbb{E}[X])\) and the notion of spatially unique centers.

Given a \(n_{1}\times n_{2}\) matrix \(M\), we define the minimum hyperplane distance, \(\mathsf{dist_{M}}\) as

\[\mathsf{dist_{M}}=\min_{j}\min_{\mathbf{v}\in\mathsf{Span}(M_{-j})}\|M_{j}- \mathbf{v}\|\]

where \(M_{j}\) represents the \(j\)-th column of \(M\) and \(M_{-j}\) denotes the set of all columns of \(M\) except the \(j\)-th one.That is, it denotes the minimum distance between a data point and the span of the rest of the data points.

We have the following classic result of matrix theory.

**Lemma C.1** ([21]).: _For any \(n_{1}\times n_{2}\) matrix \(M\), the smallest singular value \(s_{\min}(M)\) is lower bounded by \(\frac{1}{\sqrt{n_{2}}}\cdot\mathsf{dist_{M}}\)._

Now, this result does not directly help us as \(\mathbb{E}[X]\) has multiple identical columns (it is after all a \(d\times n\) rank \(k\) matrix) and we only get a lower bound of \(0\). However, we can do a simple two-step analysis to get something nicer.

Consider the matrix \(\hat{C}\) which contains \(k\) columns that are each copy of one of the centers of \(X\). Then from the definition of \(\gamma\)-spatially unique centers, we immediately have the following.

_Fact C.2_.: If \(X\) comes from a setup with \(\gamma\)-spatially unique centers then \(s_{k}(\hat{C})=s_{\min}(\hat{C})\geq\frac{\gamma}{\sqrt{k}}\).

Next, let the size of the underlying communities in \(X\). Then we know that \(\mathbb{E}[C]\) has at least \(\min_{j}|V_{j}|\) many copies of \(\hat{C}\) in it (along with other columns corresponding to the larger communities). That means that the singular values in \(\mathbb{E}[X]\) is at least \(\sqrt{\min_{j}|V_{j}|}\) times the singular values in \(\hat{C}\). This gives us the following result.

**Lemma C.3**.: _Let \(X\) be generated from \(\gamma\)-spatially unique centers and let the minimum size of the underlying communities be \(\Omega(n/k)\). Furthermore, assume \(s_{k}(X)\gg\|E_{X}\|\). Then we get \(\delta_{k-1}(Y)\geq\frac{C\cdot\gamma\sqrt{n}}{k}\) for some constant \(C\)._

Proof.: This simply comes from putting the bounds on \(\hat{C}\) and multiplying them with \(\sqrt{\min_{j}|V_{j}|}\) and then connecting it with Equation 8. 

Now, to go back to the assumption of \(s_{k}(X)\geq 4C\sigma\sqrt{d+n}\), consider that \(n=\Omega(d)\) (this is where will work from hereon), then the assumption holds as long as \(\gamma\geq\sigma k\). Now, once we have this result, we can then obtain our main Theorem C.4 in the setting of Spatially unique centers.

**Theorem C.4** (Relative compression with spatially unique centers).: _Let \(X\) be a \(d\times n\) dataset \(k\) many \(\gamma\)-spatially unique centers where the size of the smallest community is \(\Omega(n/k)\). Then there is a constant \(C_{1}\) such that for all intra-community pairs in \(V_{j}\), the compression ratio is upper-bounded as_

\[\Delta_{X,k-1}(i,i^{\prime})\geq\frac{\sigma_{j}\sqrt{d}}{C_{1}\left(\sigma \sqrt{k}+\alpha\sqrt{\log n}+\frac{2\sigma\cdot\sigma_{j}\cdot k\sqrt{d}}{ \gamma}\right)} \tag{9}\]

_Similarly for any \(i\in V_{j}\) and \(i^{\prime}\in V_{j^{\prime}}\), the inter-community compression ratio is upper-bounded as_

\[\Delta_{X,k-1}(i,i^{\prime})\leq \tag{10}\] \[\frac{\sqrt{(\sigma_{j}^{2}+\sigma_{j^{\prime}}^{2})d^{2}+\| \mathbf{c_{j}}-\mathbf{c_{j^{\prime}}}\|^{2}}}{C_{1}\left(\|\mathbf{c_{j}}- \mathbf{c_{j^{\prime}}}\|-2\sigma\sqrt{k}-\alpha\sqrt{\log n}-\frac{\sigma \cdot\sqrt{\sigma_{j}^{2}+\sigma_{j^{\prime}}^{2}}\cdot k\cdot\sqrt{d}}{ \gamma}\right)}\]

_with probability \(1-\mathcal{O}(1/n)\)._Proof.: For simplicity of the statements we have made several assumptions, most of them to consider the harder setting (heavy noise). That is, \(\sigma_{j}^{2}d=\Omega(\max_{j^{\prime}}\|\mathbf{c_{j}}-\mathbf{c_{j^{\prime}}}\|)\). Furthermore, we assume \(\sigma\) is sufficiently large so that \(\sigma^{2}d\geq 100\alpha\sqrt{d}\log n\) (this happens as long as \(\alpha=o(d^{1/4})\)). This implies that the pre-PCA intra and inter-community distances are \(\Theta(2\sigma_{j}\sqrt{d})\) and \(\Theta\left(\sqrt{\sigma_{j}^{2}+\sigma_{j^{\prime}}^{2}}\sqrt{d}\right)\) respectively.

Next, in the intra-community compression ratio bound we have the term

\[\frac{2\mathcal{N}\cdot\sqrt{\sigma_{j}^{2}d+12\sqrt{d}\log n}}{\delta_{k-1}(Y) }=\frac{2\sigma\sqrt{d+n}\cdot\sqrt{\sigma_{j}^{2}d+12\alpha\sqrt{d}\log n}}{0.25\gamma\sqrt{n}/k}\]

Here recall that we assume \(n=\Omega(d)\) which implies \(2\sigma\sqrt{d+n}\leq C\sigma\sqrt{n}\) for large enough \(n\). Furthermore, \(12\alpha\sqrt{d}\log n\) is dominated by \(\sigma_{j}^{2}d\). Combining we get

\[\frac{2\sigma\sqrt{d+n}\cdot\sqrt{\sigma_{j}^{2}d+12\alpha\sqrt{d}\log n}}{0. 25\gamma\sqrt{n}/k}=\frac{C\sigma\sqrt{n}\sigma_{j}\sqrt{d}\cdot k}{0.25\gamma \sqrt{n}}=\frac{8C\sigma\sigma_{j}\cdot k\cdot\sqrt{d}}{\gamma}\]

Similarly the bound

\[\frac{2\sigma\sqrt{d+n}\cdot\sqrt{\|\mathbf{c_{j}}-\mathbf{c_{j^{\prime}}}\|^{ 2}+2(\sigma_{j}^{2}+\sigma_{j^{\prime}}^{2})d+12\alpha\sqrt{d}\log n}}{\delta_ {k-1}(Y)}\]

can be simplified to \(\frac{C\sqrt{\sigma_{j}^{2}+\sigma_{j^{\prime}}^{2}}k\sqrt{d}}{\gamma}\)

Combining these bounds directly gets us result. 

Then, Theorem 2.5 is immediately implied, as follows.

### Proof of Theorem 2.5

We know that \(\gamma\geq C\max\{\sigma\sqrt{k}d^{1/4},\sigma\sqrt{k}+\alpha\log n\}\). Furthermore, we assume \(\max_{i,j}\|\mathbf{c_{i}}-\mathbf{c_{j}}\|\ll\sigma\sqrt{\mathbf{d}}\), which is the heavy noise setting. The other case follows the same way. Let \(C>100C_{1}\).

Furthermore, note that \(\|\mathbf{c_{i}}-\mathbf{c_{j}}\|\geq\gamma\).

Then we have

\[\frac{2\sigma\cdot\sigma_{j}\cdot k\sqrt{d}}{\gamma}\leq 0.01\sigma_{j}\sqrt{k} d^{1/4}\]

Similarly, we have

\[\frac{\sqrt{\sigma_{j}^{2}+\sigma_{j^{\prime}}^{2}}\cdot k\cdot\sqrt{d}}{\gamma }\leq 0.01\sigma\sqrt{k}d^{1/4}\]

Then, the denominator of the lower bound on the intra-community compression ratio is upper bounded by \(0.02\sigma\sqrt{k}d^{1/4}\), and the denominator of the lower bound on the inter-community compression ratio is lower bounded by \(\|\mathbf{c_{i}}-\|\mathbf{c_{j}}\|-0.02\sigma\sqrt{k}d^{1/4}\geq 0.98\sigma \sqrt{k}d^{1/4}\).

Then, the intra-community compression ratio is lower bounded by \(10\sqrt{k}\sqrt{d^{1/4}}\) and the inter-community compression ratio is upper bounded by \(0.05\sqrt{k}\sqrt{d^{1/4}}\), obtaining the separation described in the Theorem 2.5.

### Proofs for variance of compression ratios

Having discussed the compression ratio bounds in the context of \(\gamma\)-spatially unique centers, we continue with the theoretical support for our outlier detection method in the random-mixture-outlier model. We recall the definition of this model for ease of exposition.

**Definition C.5** (Mixture model with outliers (revisited)).: Let \(X\) be a \(d\times n\) dataset with the partition \(V_{1},\ldots V_{k},\hat{V}\), a set of \(k\) centers \(\{\mathbf{c_{j}}\}_{j=1}^{k}\) and distributions \(\{\mathcal{D}^{(j)}\}_{j=1}^{k}+1\) with the following generation method.

1. _clean points:_ If \(i\in V_{j},1\leq j\leq k\), \(\mathbf{x_{i}}=\mathbf{c_{j}}+\mathbf{e_{i}}\) where \(\mathbf{e_{i}}\) is sampled from \(\mathcal{D}^{(j)}\).
2. _outliers:_ If \(i\in\hat{V}\), then we sample \(p_{i,1},\ldots p_{i,k}\in[0.5,1]\). Then \(\mathbf{u_{i}}=\sum_{j}\alpha_{i,j}\mathbf{c_{j}}+\mathbf{e_{i}}\) where \(\alpha_{i,j}=\frac{\mathbb{P}_{i,j}}{\sum_{j}p_{i,j}}\) and \(\mathbf{e_{i}}\) is sampled from \(\mathcal{D}^{(k+1)}\).

Let \(|\hat{V}|=n_{o}\) and \(n=n_{o}+n_{c}\). To keep the results simple, we make the average variance of each distribution \(\mathcal{D}^{(j)}\) same, which is \(\sigma^{\prime}\).

The concept of Algorithm 1 is simple. If each cluster has a large number of points, then even if there are a large number of outliers generated from the random-mixture-outlier model, the outliers will have a lower variance of compression than all the clean points.

First, let us obtain a lower bound on the variance of the compression ratio of clean points under the conditions of Theorem 2.8. We know that any clean point has high intra-community compression ratios. This implies that the expectation of the compression ratio for this point is high. On the other hand, the inter-compression ratio values are low. So just calculating the variance on the inter-community points yields a large value.

For the sake of simplicity, we will define \(\gamma\geq 2\beta\sqrt{\sigma}kd^{1/4}\). Then if we can show that under the other settings of Theorem 2.8, there is a separation in the variance of the compression ratios of the clean points and the outliers whenever \(\beta\geq C^{\prime}\frac{\sigma\sqrt{\log n}}{\sigma^{\prime}}\), we prove the theorem.

**Lemma C.6**.: _Let there be \(n_{c}\) clean points in the random-mixture-outlier setting where \(\min_{j}|V_{j}|=\Omega(n/k)\) and \(\gamma\geq 2\beta\sqrt{\sigma}kd^{1/4}\). Then the variance of all such points are lower bounded as \(C_{4}\cdot\frac{n_{c}-|V_{j}|}{n}\cdot\frac{d^{1/4}}{k}\cdot\left(\frac{\beta \sigma^{\prime}}{\sigma}-\frac{\sigma}{\beta\cdot\sigma^{\prime}}\right)\) with probability \(1-\mathcal{O}(1/n)\)._

Proof.: Consider any point \(\mathbf{x_{i}}\in V_{j}\). Then the inter-compression ratio of \(\mathbf{x_{i}}\) with any intra-community point is lower bounded by \(\frac{0.25\sigma^{\prime}\sqrt{d}}{\sigma\sqrt{k}+\alpha\sqrt{\log n}+2\sqrt{ \sigma}\sigma^{d^{1/4}}/(\beta)}\geq\frac{0.25\beta\sigma^{\prime}}{\sigma}d^ {1/4}\) with probability \(1-\mathcal{O}(1/n)\). Then the average of the compression ratios for \(\mathbf{x_{i}}\) is lower bounded as \(\frac{0.25\sigma^{\prime}/\sigma d^{1/4}|V_{j}|}{n}\geq\frac{C_{3}\beta\sigma^ {\prime}/\sigma d^{1/4}}{k}\) On the other hand, probability \(1-\mathcal{O}(1/n)\) we have that for any inter-community point, the compression ratio is upper-bounded with \(\frac{2\sigma\sqrt{d}}{(\gamma-(2\sigma\sqrt{k}-\alpha\sqrt{\log n}-C_{2} \sigma^{\prime}d^{1/4})}\leq\frac{2\sigma\sqrt{d}}{\beta k\sigma^{d^{1/4}}/C_ {2}}\leq\frac{2C_{2}\sigma/\sigma^{\prime}d^{1/4}}{\beta\cdot k}\).

Then, the variance of compression of \(\mathbf{x_{i}}\) is lower bounded by

\[C_{4}\cdot\frac{n-|V_{j}|}{n}\cdot\left(\frac{d^{1/4}}{k}\cdot\left(\frac{ \beta\sigma^{\prime}}{\sigma}-\frac{\sigma}{\beta\sigma^{\prime}}\right) \right)^{2}\]

Now, we aim to upper-bound the variance of compression for outliers. Here we want to show that since the underlying signal in any outlier is apart from the signal of any other point, they generally have a lower compression ratio with any other point, which then implies a lower variance of compression ratio.

First, we show that as long as there are not too many outliers, their underlying centers (which are random mixtures of the community centers) will not be too close (which implies they will not have a high compression ratio).

**Lemma C.7** (Distance between signals of the outliers).: _Let there be \(n_{o}\) many outliers in the dataset generated via the random-mixture model where \(k\geq\log n\). Let the set of outliers be \(\hat{V}\). Then, for with probability \(1-\mathcal{O}(n)\), \(\min_{\mathbf{u},\mathbf{v}\in\hat{V}}\|\mathbf{u}-\mathbf{v}\|\geq\frac{\gamma}{\log n}\)._Proof.: Let \(|\hat{V}|=n_{o}\). Then, for the underlying mixture-center any two points, denoted as \(\mathbf{u}=\sum_{j}\alpha_{1,j}\mathbf{c_{j}}\) and \(\sum_{j}\alpha_{2,j}\mathbf{c_{j}}\) we say they are \(\epsilon\)-far if \(\min_{j}|\alpha_{1,j}-\alpha_{2,j}|\geq\epsilon\).

Now, note that for any \(\epsilon\)-far mixture-centers, we have \(\|\mathbf{u}-\mathbf{v}\|\geq 0.5\epsilon\gamma\).

Now, it is easy to see that the probability that there is a pair of mixed centers that is not \(\epsilon\)-far is \(n_{0}^{2}\cdot(\epsilon)^{k}\). Then, setting \(\epsilon=1/\log n\) and applying \(k\geq\log n\) gives that even for \(n_{0}=n/2\), there all pairs of mixture centers are \(1/\log n\)-far with probability \(1-\mathcal{O}(1/n)\). 

Then, we show that in such a case, the variance of compression for any outlier point is quite low even when measured crudely.

**Lemma C.8** (Variance of compression of outliers).: _Let there be a set of \(n_{o}\) many outliers so that the underlying mixture-centers are pairwise \(1/\log n\)-far Then under the condition of Lemma C.6, we have that the variance of compression for any outlier is upper bounded by \(\left(\frac{4C_{2}\log n(\sigma/\sigma^{\prime})d^{1/4}}{\beta\cdot k}\right)^ {2}\) with probability \(1-\mathcal{O}(1/n)\)._

Proof.: Consider any outlier \(\mathbf{u_{i}}\in V_{0}\). First, consider the compression ratio between \(\mathbf{u_{i}}\) and any \(\mathbf{v}\) that is clean. Where \(\mathbf{u_{i}}=\sum_{j}\alpha_{j}\mathbf{c_{j}}+\mathbf{e_{i^{\prime}}}\) and \(\mathbf{v}=\mathbf{c_{j^{\prime}}}+\mathbf{e_{i}}\).

Next, remember that as every \(\alpha_{j}\geq 1/2k\), we have \(\max_{j}\alpha_{j}\leq 0.5\).

Then, from the definition of \(\gamma\)-spatially unique centers we have

\[\|\sum_{j}\alpha_{j}\mathbf{c_{j}}-\mathbf{c_{j^{\prime}}}\|\geq\|\sum_{j\neq j ^{\prime}}\alpha_{j}\mathbf{c_{j}}-(1-\alpha_{j^{\prime}})\mathbf{c_{j^{\prime }}}\|\geq 0.5\|\sum_{j\neq j^{\prime}}\alpha_{j}\mathbf{c_{j}}-\mathbf{c_{j^{ \prime}}}\|\geq 0.5\gamma\]

Then, following the analysis of Lemma C.6, we can show that in all such cases, the compression ratio is upper bounded by \(\frac{4C_{2}\sigma/\sigma^{\prime}d^{1/4}}{\beta\cdot k}\).

On the other hand, consider any two outliers. Then their compression ratios are upper bounded by \(\frac{4C_{2}\log n\sigma/\sigma^{\prime}d^{1/4}}{\beta\cdot k}\) (essentially replacing \(\gamma\) by \(\gamma/\log n\) in the center-distance calculation).

Then, we can upper bound the variance of compression for an outlier as

\[\frac{1}{n}\left(|\hat{V}|(\frac{4C_{2}\log n\sigma/\sigma^{\prime}d^{1/4}}{ \beta\cdot k})^{2}+(n-|\hat{V}|)(\frac{4C_{2}\sigma/\sigma^{\prime}d^{1/4}}{ \beta\cdot k})^{2}\right)\leq\left(\frac{4C_{2}\log n(\sigma/\sigma^{\prime} )d^{1/4}}{\beta\cdot k}\right)^{2}\text{[applying $k\geq\log n$ ]}\]

Proof of Theorem 2.8.: Lemma C.6 shows that in the setting of Theorem 2.8, the variance of compression ratios for a clean point is lower bounded by \(C_{4}\cdot\frac{n-|V_{j}|}{n}\cdot\left(\frac{d^{1/4}}{k}\cdot\left(\frac{ \beta\sigma^{\prime}}{\sigma}-\frac{\sigma}{\beta\sigma^{\prime}}\right) \right)^{2}\).

Next, Lemma C.8 shows that the variance of compression ratios for an outlier is upper-bounded as \(\left(\frac{4C_{2}\log n(\sigma/\sigma^{\prime})d^{1/4}}{\beta\cdot k}\right) ^{2}\) Both the aforementioned happen for all outlier and clean points with probability \(1-\mathcal{O}(1/n)\).

Then, to show that with high probability, the variance of compression ratios of any clean point is higher than the variance of compression ratios of any outlier is

\[C_{4}\cdot\frac{n-|V_{j}|}{n}\cdot\left(\frac{d^{1/4}}{k}\cdot \left(\frac{\beta\sigma^{\prime}}{\sigma}-\frac{\sigma}{\beta\sigma^{\prime}} \right)\right)^{2}>\left(\frac{4C_{2}\log n(\sigma/\sigma^{\prime})d^{1/4}}{ \beta\cdot k}\right)^{2}\] \[\implies \frac{\sqrt{d}}{k^{2}}\cdot\left(\frac{n-|\hat{V}|}{n}\cdot\frac {\beta\sigma^{\prime}}{\sigma}-\frac{C_{5}\log n\sigma}{\sigma^{\prime}\beta} \right)>0 \text{[For some constant $C_{5}$]}\] \[\implies \frac{\sqrt{d}}{k^{2}}\cdot\left(\frac{0.5\beta\sigma^{\prime}}{ \sigma}-\frac{C_{5}\log n\sigma}{\sigma^{\prime}\beta}\right)>0 \text{[As $n-|\hat{V}|\geq 0.5n$]}\]

Then, as long as \(\beta\geq 10C_{5}\sigma/\sigma^{\prime}\sqrt{\log n}\), this equation is satisfied.

Projection with more principal components

Here we show some results in the case of \(k^{\prime}=k-1+c\). The main challenge in theoretically proving our bounds for \(k^{\prime}\neq k-1\) comes from Theorem B.8. A key ingredient towards proving Theorem B.1 is the following spectral gap.

\[\left\|(P_{Z}^{k^{\prime}})^{T}-(P_{Z}^{k^{\prime}})^{T}W\right\|\leq\frac{2 \|E_{Z}\|}{\delta_{k^{\prime}}(Y)}\]

In general we work with the natural assumption \(\delta_{k-1}(Y)>>\|E_{Z}\|\). However, in our model we have \(s_{k}(Y)=\mathcal{O}(\|E_{Z}\|)\). This follows from Weyl's inequality, which states that if \(Z=\bar{Z}+E_{Z}\) and (\(k^{\prime}\))-th singular value of \(B\) is \(0\), then \((k-1+c)\)-th singular value of \(Z\) is upper bounded by \(\mathcal{O}(\|E_{Z}\|)\) for any \(c>0\).

Thus \(\delta_{k^{\prime}}(Y)=\mathcal{O}(\|E_{Z}\|)\) for any \(k^{\prime}\geq k\), and our previous results alone cannot prove relative compressibility.

Here we bypass this issue to a loose but non-trivial extent. First we note that the inter-community compression can only decrease if the the projection dimension increases. Thus we have that for any \(k^{\prime}\geq k\), \(\Delta_{X,k^{\prime}}(i,i^{\prime})\leq\Delta_{X,k-1}(i,i^{\prime})\).

**Theorem D.1**.: _Let us consider the random vector model as in Theorem B.1. Let Let \(k^{\prime}=k-1+c\) and any \(0<f<1\). Then we have that with probability \(1-\mathcal{O}(1/n)\),_

1. _If_ \((i,i^{\prime})\) _is an inter-community pair, then_ \(\Delta_{k^{\prime},X}(i,i^{\prime})\leq\Delta_{k-1,X}(i,i^{\prime})\)__
2. _If_ \((i,i^{\prime})\) _is an intra-community pair, then_ \[\Delta_{k-1,Y}(i,i^{\prime})\geq\frac{\sqrt{\|\mathbf{c_{j}}-\mathbf{c_{j^{ \prime}}}\|^{2}+d(\sigma_{j}^{2}+\sigma_{j^{\prime}}^{2})+12\sqrt{d}\log n}}{ \sqrt{\left\|P_{Y}^{k-1}(\boldsymbol{y_{i}}-\boldsymbol{y_{i^{\prime}}}) \right\|^{2}+4C_{0}^{2}\sigma^{2}(d+n)c^{2}f^{2}}}\] _for all but_ \(c^{2}/f^{4}\) _pairs of points with probability_ \(1-\mathcal{O}(1/n)\)_._

Proof.: The inter-community bound follows from definition and the numerator of the intra-community bound follows from Lemma B.2. We now prove the denominator (post PCA distance bounds) for the intra-community case.

Let us denote with \(P_{Y}^{k_{1},k_{2}}\) the projection operator due to the \(k_{1}\)-th to \(k_{2}\)-th top singular vectors of \(Y\). Then for any vector \(\boldsymbol{u}\) we have \(\|\Pi_{X}^{k^{\prime}}(\boldsymbol{u})\|=\sqrt{\|\Pi_{X}^{k-1}(\boldsymbol{u} )\|^{2}+\|P_{Y}^{k,k^{\prime}}(\boldsymbol{u})\|^{2}}\).

Then we are left with bounding \(\|P_{Y}^{k,k^{\prime}}(\boldsymbol{u})\|^{2}\) where \(\boldsymbol{u}=\boldsymbol{y_{i}}-\boldsymbol{y_{i^{\prime}}}\) so that \(i\in V_{j},i^{\prime}\in V_{j^{\prime}}\). We aim to show that if \(k^{\prime}-k\) is small then this value is small as well.

We first represent \(Y\) with its SVD decomposition. \(\boldsymbol{l_{\ell}}\) and \(\boldsymbol{r_{\ell}}\) represent the \(\ell\)-th left singular vector and right singular vector of \(Y\) respectively. Then we have \(Y=\sum_{\ell=1}^{t}s_{i}(Y)\boldsymbol{l_{\ell}}(\boldsymbol{r_{\ell}})^{T}\) where \(t=rank(Y)\). Then the projection of \(\boldsymbol{y_{i}}\) due to the \(\ell\)-th principal component of \(X\) is \(\langle\boldsymbol{l_{\ell}},\boldsymbol{y_{i}}\rangle=s_{\ell}(Y)r_{\ell},i\) where \(r_{\ell,i}\) is the \(i\)-th entry of the \(\ell\)-th right singular vector. Then we have

\[\leq s_{k}(Y)\sqrt{\sum_{\ell=k}^{k^{\prime}}(r_{\ell,i})^{2}}\]

Here recall that each \(\boldsymbol{r_{\ell}}\) is a \(n\)-dimensional vector with unit norm, i.e. \(\|\boldsymbol{r_{\ell}}\|=1\). Then for any \(f<1\), the number of coordinates of \(\boldsymbol{r_{\ell}}\) that are larger than \(f\) is less than \(1/f^{2}\). Thus considering all the \(k\leq\ell\leq k-1+c\), the total number of entries that are larger than \(f\) is less than \(c/f^{2}\). Then, for all but \(c/f^{2}\) many points \(\boldsymbol{y_{i}}\) we have \(\|P_{Y}^{k,k-1}(\boldsymbol{y_{i}})\|\leq s_{k}(Y)\cdot f\cdot c\). Here we substitute \(s_{k}(Y)\leq\|E_{Y}\|\leq C_{0}\sigma\sqrt{d+n}\) with probability \(1-\mathcal{O}(n^{-3})\).

This implies that with probability \(1-\mathcal{O}(1/n)\)\(\|P_{Y}^{k,k-1}(\boldsymbol{y_{i}}-\boldsymbol{y_{i^{\prime}}})\|\leq 2C_{0}\sigma\sqrt{d+n}cf\) for all but \(c^{2}/f^{4}\) pairs of points. This concludes our proof.

[MISSING_PAGE_FAIL:25]

Figure 4: NMI improvement via removing \(10\%\) points

Figure 5: Purity score improvement via \(5\%\) outlier removal

Figure 3: NMI improvement via removing \(5\%\) points

Figure 6: Purity score improvement via \(10\%\) outlier removal

### Different PCA dimension choice

Finally, we show that our experiments on real-world data, both for average compression as well as clustering accuracy improvement through outlier detection, are fairly stable to a change in the PCA dimension. The average compression ratios can be found in Table 7. The NMI and purity index baselines can be found in Tables 8 and 9 respectively.

\begin{table}
\begin{tabular}{|c|c|} \hline Dataset & Purity of PCA + k-means \\ \hline Koh & 0.895 \\ \hline Kumar & 0.983 \\ \hline Simkumar4easy & 0.918 \\ \hline Simkumar4hard & 0.563 \\ \hline Simkumar8hard & 0.667 \\ \hline Trapnell & 0.604 \\ \hline Zheng4eq & 0.715 \\ \hline Zheng4uneq & 0.873 \\ \hline ZhengSeq & 0.568 \\ \hline \end{tabular}
\end{table}
Table 6: Purity index before data removal (PCA dim = \(k-1\))

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline \multicolumn{10}{|c|}{Community wise average compression ratio} \\ \hline Koh (Inter) & 2.417 & 2.530 & 2.714 & 2.523 & 2.649 & 2.948 & 2.352 & 2.696 & 2.018 \\ \hline Koh (Intra) & 7.678 & 9.829 & 6.966 & 6.041 & 6.757 & 8.424 & 6.686 & 7.382 & 7.463 \\ \hline Kumar (Inter) & 2.107 & 2.105 & 1.696 & - & - & - & - & - & - \\ \hline Kumar (Intra) & 15.969 & 13.577 & 14.889 & - & - & - & - & - & - \\ \hline Simkumar4easy (Inter) & 4.534 & 3.724 & 3.200 & 2.850 & - & - & - & - & - \\ \hline Simkumar4easy (Intra) & 15.673 & 17.083 & 15.554 & 14.924 & - & - & - & - & - \\ \hline Simkumar4hard (Inter) & 5.984 & 5.653 & 4.960 & 4.472 & - & - & - & - & - \\ \hline Simkumar4hard (Intra) & 15.173 & 16.722 & 14.500 & 13.807 & - & - & - & - & - \\ \hline Simkumar8hard (Inter) & 4.425 & 4.668 & 4.397 & 5.233 & 4.390 & 4.004 & 3.998 & 3.681 & - \\ \hline Simkumar8hard (Intra) & 9.177 & 10.571 & 8.785 & 8.639 & 9.390 & 9.526 & 8.699 & 10.172 & - \\ \hline Trapnell (Inter) & 4.491 & 7.401 & 7.228 & - & - & - & - & - & - \\ \hline Trapnell (Intra) & 9.202 & 10.248 & 10.122 & - & - & - & - & - & - \\ \hline Zheng4eq (Inter) & 2.117 & 1.762 & 2.828 & 2.889 & - & - & - & - & - \\ \hline Zheng4eq (Intra) & 6.135 & 6.250 & 7.947 & 6.223 & - & - & - & - & - \\ \hline Zheng4uneq (Inter) & 2.059 & 1.753 & 2.870 & 2.176 & - & - & - & - & - \\ \hline Zheng4uneq (Intra) & 5.839 & 6.351 & 7.335 & 5.514 & - & - & - & - & - \\ \hline Zheng8eq (Inter) & 1.981 & 2.922 & 1.655 & 1.936 & 2.567 & 2.594 & 2.802 & 2.726 & - \\ \hline Zheng8eq (Intra) & 4.306 & 4.533 & 4.540 & 4.997 & 4.254 & 5.598 & 5.244 & 4.300 & - \\ \hline \end{tabular}
\end{table}
Table 5: Community-wise Inter and Intra-Community Compression RatiosFor brevity, we show the improvement in NMI and purity index for \(10\%\) point removal in Figures 7 and 8 respectively. As one can observe, our method continues to be the most consistent, being the best method in most datasets. Indeed, in this case our performance is even comparatively better than in the case of PCA-dimension=\(k-1\).

## Appendix F Future directions

In this paper, we have quantified PCA's denoising effect in high dimensional noisy data with underlying community structure via the metric of compression ratio. As an application, we have designed an outlier detection method that improves the community structure of datasets. We note two interesting theoretical and algorithmic questions.

i) Providing a more tight bound on the compression ratio seems an exciting and hard direction.

Figure 7: NMI improvement via removing \(10\%\) points when PCA dimension is \(2k\)

ii) Using compression ratio as a metric for clustering algorithms also seems an interesting direction, especially for single-cell-RNA-seq datasets.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We provide a novel quantification of PCA's denoising effect in high dimensional data with heavy noise. Then, we use this quantification to develop an outlier detection method in this setting. We provide comprehensive theoretical, simulation, and real-world experiment results in the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in the last paragraph of the main paper. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: We provide the proof of our theorems in the Appendix B.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe our experiments clearly in Section 4 and provide the full source code used to generate the results in the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The data is publicly available and we include its source. The supplementary material includes our simulation code, algorithms, and experiments. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all experimental details within Section 4 of the paper and additional results within Appendix E for different experimental settings. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide error bars for all the applicable experiments, mainly in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We describe the computational environment and running time used to generate the results in the first paragraph of Section 3. * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have read and understood the guidelines Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer:[NA] Justification: Our work focuses on understanding structures of graphs that appear in real-world data, and our application is focused on clustering of single-cell RNA sequencing datasets. As such, we do not see any immediate negative societal impact of our work. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not see any immediate risk of misuse of our work. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use publicly available datasets and cite them. * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. ** For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide our codes in the supplementary material. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.