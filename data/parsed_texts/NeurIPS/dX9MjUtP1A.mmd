# Self-Correcting Bayesian Optimization through Bayesian Active Learning

 Carl Hvarfner

carl.hvarfner@cs.lth.se

Lund University

&Erik Orm Hellsten

erik.hellsten@cs.lth.se

Lund University

&Frank Hutter

fh@cs.uni-freiburg.de

University of Freiburg&Luigi Nardi

luigi.nardi@cs.lth.se

Lund University

Stanford University

DBtune

###### Abstract

Gaussian processes are the model of choice in Bayesian optimization and active learning. Yet, they are highly dependent on cleverly chosen hyperparameters to reach their full potential, and little effort is devoted to finding good hyperparameters in the literature. We demonstrate the impact of selecting good hyperparameters for GPs and present two acquisition functions that explicitly prioritize hyperparameter learning. Statistical distance-based Active Learning (SAL) considers the average disagreement between samples from the posterior, as measured by a statistical distance. SAL outperforms the state-of-the-art in Bayesian active learning on several test functions. We then introduce Self-Correcting Bayesian Optimization (SCoreBO), which extends SAL to perform Bayesian optimization and active learning simultaneously. SCoreBO learns the model hyperparameters at improved rates compared to vanilla BO, while outperforming the latest Bayesian optimization methods on traditional benchmarks. Moreover, we demonstrate the importance of self-correction on atypical Bayesian optimization tasks.

## 1 Introduction

Bayesian Optimization (BO) is a powerful paradigm for black-box optimization problems, i.e., problems that can only be accessed by pointwise queries. Such problems arise in many applications, ranging from including drug discovery [21] to configuration of combinatorial problem solvers [27; 28], hardware design [14; 43], hyperparameter tuning [11; 30; 33; 52], and robotics [4; 9; 40; 41].

Gaussian processes (GPs) are a popular choice as surrogate models in BO applications. Given the data, the model hyperparameters are typically estimated using either Maximum Likelihood or Maximum a Posteriori estimation (MAP) [49]. Alternatively, a fully Bayesian treatment of the hyperparameters [46; 55] removes the need to choose any single set through Monte Carlo integration. This procedure effectively considers all possible hyperparameter values under the current posterior, thereby accounting for hyperparameter uncertainty. However, the relationship between accurate GP hyperparameter estimation and BO performance has received little attention [3; 7; 58; 69; 71], and active reduction of hyperparameter uncertainty is not an integral part of any prevalent BO acquisition function. In contrast, the field of Bayesian Active Learning (BAL) contains multiple acquisition functions based solely on reducing hyperparameter-induced measures of uncertainty [26; 34; 50], and the broader field of Bayesian Experimental Design [1; 10; 48] revolves around acquisition of data to best learns the model parameters.

The importance of the GP hyperparameters in BO is illustrated in Fig. 1, which shows average simple regret over 20 optimization runs of 8-dimensional functions drawn from a Gaussian process prior. The curves correspond to the performance of Expected Improvement with noisy experiments (NEI) [36] acquisition function under a fully Bayesian hyperparameter treatment using NUTS [25]. Two prevalent hyperparameter priors, described in detail in App. B.1, as well as the true model hyperparameters, are used. Clearly, good model hyperparameters have substantial impact on BO performance, and BO methods could greatly benefit from estimating the model hyperparameters as accurately as possible. Furthermore, the hyperparameter estimation task can become daunting under complex problem setups, such as non-stationary objectives (spatially varying lengthscales, heteroskedasticity) [6, 13, 16, 56, 64], high-dimensional search spaces [15, 47], and additively decomposable objectives [19, 32]. The complexity of such problems warrants the use of more complex, task-specific surrogate models. In such settings, the success of the optimization may increasingly hinge on the presumed accuracy of the task-specific surrogate.

We proceed in two steps. We first introduce _Statistical distance-based Active Learning_ (SAL), which improves Bayesian active learning by generalizing previous work [26, 50] and introduces a holistic measure of disagreement between the marginal posterior predictive distribution and each conditional posterior predictive. We consider the hyperparameter-induced disagreement between models in the acquisition function, thereby accelerating the learning of model hyperparameters. We then propose _Self-Correcting Bayesian Optimization_ (SCoreBO), which builds upon SAL by explicitly learning the location of the optimizer in conjunction with model hyperparameters. This achieves accelerated hyperparameter learning and yields improved optimization performance on both conventional and exotic BO tasks. Formally, we make the following contributions:

1. We introduce SAL, a novel and efficient acquisition function for hyperparameter-oriented Bayesian active learning based on statistical distances (Sec. 3.1),
2. We introduce SCoreBO, the first acquisition function for joint BO and hyperparameter learning (Sec. 3.2),
3. We display highly competitive performance on an array of conventional AL (Sec. 4.1) and BO tasks (Sec. 4.2), and demonstrate SCoreBOs, ability to enhance atypical models such as SAASBO [15] and HEBO [13], and identify decompositions in AddGPs [32](Sec. 4.3).

## 2 Background

### Gaussian processes

Gaussian processes (GPs) have become the model class of choice in most BO and active learning applications. They provide a distribution over functions \(f\sim\mathcal{GP}(m(\cdot),k(\cdot,\cdot))\) fully defined by the mean function \(m(\cdot)\) and the covariance function \(k(\cdot,\cdot)\). Under this distribution, the value of the function \(f(\bm{x})\), at a given point \(\bm{x}\), is normally distributed with a closed-form solution for the mean and variance. We assume that observations are perturbed by Gaussian noise, such that \(y_{\bm{x}}=f(\bm{x})+\varepsilon,\ \varepsilon\sim N(0,\sigma_{\varepsilon}^{2})\). We also assume the mean function to be constant, such that the dynamics are fully determined by the covariance function \(k(\cdot,\cdot)\).

To account for differences in variable importance, each dimension is individually scaled using length-scale hyperparameters \(\ell_{i}\). For \(D\)-dimensional inputs \(\bm{x}\) and \(\bm{x}^{\prime}\), the distance \(r(\bm{x},\bm{x}^{\prime})\) is subsequently computed as \(r^{2}=\sum_{i=1}^{D}(x_{i}-x_{i}^{\prime})^{2}/\ell_{i}^{2}\). Along with the outputscale \(\sigma_{f}\), the set \(\bm{\theta}=\{\bm{\ell},\sigma_{\varepsilon},\sigma_{f}\}\) comprises the set of hyperparameters that are conventionally learned. The likelihood surface for the GP hyperparameters is typically highly multi-modal [49, 70], where different modes represent different bias-variance trade-offs [49, 50]. To avoid having to choose a single mode, one can define a prior \(p(\bm{\theta})\) and marginalize with respect to the hyperparameters when performing predictions [35]. We outline fully Bayesian hyperparameter treatment in GPs App. G.1.

Figure 1: Simple regret of using true hyperparameters, BoTorch (v.0.8.4 default) and lognormal hyperparameter priors with fully Bayesian hyperparameter treatment. The prior substantially impacts final performance, and correct hyperparameters yield vastly better results.

### Bayesian Optimization

Bayesian Optimization (BO) seeks to maximize to a black-box function \(f\) over a compact domain \(\mathcal{X}\),

\[\bm{x}^{*}\in\operatorname*{arg\,max}_{\bm{x}\in\mathcal{X}}f(\bm{x}),\] (1)

such that \(f\) can only be sampled point-wise through expensive, noisy evaluations \(y_{\bm{x}}=f(\bm{x})+\varepsilon\), where \(\varepsilon\sim\mathcal{N}(0,\sigma_{\varepsilon}^{2})\). New configurations are chosen by optimizing an _acquisition function_, which uses the surrogate model to quantify the utility of evaluating new points in the search space. Examples of such heuristics are Expected Improvement (NEI) [8; 31] and Upper Confidence Bound (UCB) [3; 57; 60]. More sophisticated look-ahead approaches include Knowledge Gradient (KG) [17; 68] as well as a class of particular importance for our approach - the information-theoretic acquisition function class. These acquisition functions consider a mutual information objective to select the next query,

\[\alpha_{\textsc{ML}}(\bm{x})=I(y_{\bm{x}};\Join\mathcal{D}_{n}),\] (2)

where \(\Join\) can entail either the optimum \(\bm{x}^{*}\) as in (Predictive) Entropy Search (ES/PES) [23; 24], the optimal value \(f^{*}\) as in Max-value Entropy Search (MES) [42; 59; 65] or the tuple \((\bm{x}^{*},f^{*})\), used in Joint Entropy Search (JES) [29; 61]. FITBO [51] shares similarities with our work, in that the optimal value is governed by a hyperparameter, in their case of a transformed GP.

Within BO, the fully Bayesian hyperparameter treatment is conventionally extended from the predictive posterior to the acquisition function such that for \(M\) models with hyperparameters \(\bm{\theta}_{m},m\in\{1,\dots,M\}\) sampled from the posterior over hyperparameters \(p(\bm{\theta}|\mathcal{D})\), the acquisition function \(\alpha\) is computed as an expectation over the hyperparameters [46; 55]

\[\alpha(\bm{x}|\mathcal{D})=\mathbb{E}_{\bm{\theta}}[\alpha(\bm{x}|\bm{\theta},\mathcal{D})]\approx\frac{1}{M}\sum_{m=1}^{M}\alpha(\bm{x}|\bm{\theta}_{m}, \mathcal{D})\ \ \bm{\theta}_{m}\sim p(\bm{\theta}|\mathcal{D}).\] (3)

This is also the definition of fully Bayesian treatment considered in this work.

### Bayesian Active Learning

In contrast to BO, which aims to find a maximizer to an unknown function, Active Learning (AL) [54] seeks to accurately learn the black-box function globally. Thus, the objective is to minimize the expected prediction loss. AL acquisition functions are classified as either _decision-theoretic_, which minimize the prediction loss over a validation set, or _information-theoretic_, which minimize the space of plausible models given the observed data [26; 37].

In the information-theoretic category, _Active Learning McKay_ (ALM) [37] selects the point with the highest Shannon Entropy, which for GPs amounts to selecting the point with the highest variance. Under fully Bayesian hyperparameter treatment, it is referred to as Bayesian ALM (BALM). _Bayesian Active Learning by Disagreement_ (BALD) [26] was among the first Bayesian active learning approaches to explicitly focus on learning the model hyperparameters. It approximates the reduction in entropy over the GP hyperparameters from observing a new data point

\[\alpha_{\textsc{BALD}}(\bm{x})=I(y_{\bm{x}};\bm{\theta}|\mathcal{D})=\text{H} (p(y_{\bm{x}}|\mathcal{D}))-\mathbb{E}_{\bm{\theta}}[\text{H}(p(y_{\bm{x}}| \bm{\theta},\mathcal{D}))]\] (4)

and was later extended to deep Bayesian active learning [34] and active model (kernel) selection [18]. Lastly, Riis et al. [50] propose a _Bayesian Query-by-Committee_ (BQBC) strategy. BQBC queries where the variance \(V\) of the GP mean is the largest, with respect to changing model hyperparameters:

\[\alpha_{BQBC}(\bm{x})=V_{\bm{\theta}}[\mu_{\bm{\theta}}(\bm{x}|\mathcal{D})]= \mathbb{E}_{\bm{\theta}}[(\mu_{\bm{\theta}}(\bm{x}|\mathcal{D})-\mu(\bm{x}| \mathcal{D}))^{2}],\] (5)

where \(\mu(\bm{x})\) is the marginal posterior mean at \(\bm{x}\), and \(\mu_{\bm{\theta}}(\bm{x})\) is the posterior mean conditioned on \(\bm{\theta}\). As such, BQBC queries the location which maximizes the average distance between the marginal posterior and the conditionals according to some distance metric (here, the posterior mean), henceforth referred to as hyperparameter-induced _posterior disagreement_. However, disagreement in mean alone does not fully capture hyperparameter-induced disagreement. Thus, [50] also presents _Query-by-Mixture of Gaussian Processes_ (QBMGP), that adds the BALM criterion to the BQBC acquisition function.

### Statistical Distances

A statistical distance quantifies the distance between two statistical objects. We focus on three (semi-)metrics, which have closed forms for Gaussian random variables. The closed forms expressions, as well as additional intuition on their interaction with Gaussian random variables, can be found in App. G.2.

The Hellinger distanceis a dissimilarity measure between two probability distributions which has previously been employed in the context of BO-driven automated model selection by Malkomes et al. [39]. For two probability distributions \(p\) and \(q\), it is defined as

\[H^{2}(p,q)=\frac{1}{2}\int_{\mathcal{X}}\left(\sqrt{p(x)}-\sqrt{q(x)}\right)^{ 2}\lambda dx,\] (6)

for some auxiliary measure \(\lambda\) under which both \(p\) and \(q\) are absolutely continuous.

The Wasserstein distanceis dissimilarity metric between two distributions describing the average distance one distribution has to be moved to morph into another. The Wasserstein-\(k\) distance is defined as

\[W_{k}(p,q)=\left(\int_{0}^{1}|F_{q}(x)-F_{p}(x)|^{k}dx\right)^{1/k}\] (7)

where, in this work, we focus on the case where \(k=2\).

The KL divergenceThe KL divergence is a standard asymmetrical measure for dissimilarity between probability distributions. For two probability distributions \(P\) and \(Q\), it is given by \(\mathcal{D}_{KL}(P\mid\mid Q)=\int_{\mathcal{X}}P(x)\text{log}(P(x)/Q(x))dx\). The distances in Eq. (6), Eq. (16) and the KL divergence are used for the acquisition functions presented in Sec. 3.

## 3 Methodology

In Sec. 3.1, we introduce SAL, a novel family of metrics for BAL. In Sec. 3.2, we extend this to SCoreBO, the first acquisition function for joint BO and hyperparameter-oriented active learning, inspired by information-theoretic BO acquisition functions. In Sec. 3.3, we demonstrate how to efficiently approximate different types of statistical distances within the SAL context.

### Statistical distance-based Active Learning

In active learning for GPs, it is important to efficiently learn the correct model hyperparameters. By measuring where the posterior hyperparameter uncertainty causes high disagreement in model output, the search can be focused on where this uncertainty has a high impact. However, considering only the posterior disagreement in mean, as in BQBC, is overly restrictive as it does not fully utilize the available distributions for the hyperparameters. For example, it ignores uncertainty in the outputscale hyperparameter of the Gaussian process, which disincentives exploration. As such, we propose to generalize the acquisition function in Eq. (5) to instead consider the posterior disagreement as measured by any statistical distance. Locations where the posterior distribution changes significantly as a result of model uncertainty are good points to query, in order to quickly learn the model hyperparameters. When an observation at such a location is obtained, hyperparameters which predicted that observation poorly will have a substantially smaller likelihood, which in turn aids hyperparameter convergence. The resulting SAL acquisition function is as follows:

\[\alpha_{SAL}(\bm{x})=\mathbb{E}_{\bm{\theta}}[d(p(y_{\bm{x}}|\bm{\theta}, \mathcal{D}),p(y_{\bm{x}}|\mathcal{D}))]\approx\frac{1}{M}\sum_{m=1}^{M}d(p(y _{\bm{x}}|\bm{\theta}_{m},\mathcal{D}),p(y_{\bm{x}}|\mathcal{D})),\] (8)

where \(M\) is the number of hyperparameter samples drawn from its associated posterior, \(\bm{\theta}_{m}\sim p(\bm{\theta}|\mathcal{D})\), \(\bm{\theta}=\{\bm{\ell},\sigma_{f},\sigma_{\varepsilon}\}\), and \(d\) is a statistical distance. Notably, SAL generalizes both BQBC and BALD, which are exactly recovered by choosing the semimetric to the difference in mean or the forward KL divergence, with a short proof for the latter in App. F:

**Proposition 1**.: _SAL equipped with the KL-divergence is equivalent to BALD._Fig. 2 visualizes the SAL acquisition function. The marginal posterior (left) is made up of three vastly different conditional posteriors with hyperparameters sampled from \(p(\bm{\theta}|\mathcal{D})\) - one with high outputscale (blue), one with very high noise (orange), and one with short lengthscale (green). For each of the blue, orange and green conditionals, the distance to the marginal posterior is computed. Intuitively, disagreement in noise level \(\sigma_{\varepsilon}\) can cause large posterior disagreement at already queried locations. Similarly, uncertainty in outputscale \(\sigma_{f}\) between posteriors will yield disagreement in large-variance regions, which will result in global variance reduction. Compared to other active learning acquisition functions, SAL carries distinct advantages: it has incentive to query the same location multiple times to estimate noise levels, and accomplishes the typical active learning objectives of predictive accuracy and global exploration by alleviating uncertainty over the lengthscales and outputscale of the GP. As we show in our experiments (Sec. 4.1, App. D), SAL yields superior predictions and reduces hyperparameter uncertainty at drastically improved rates.

### Self-Correcting Bayesian Optimization

Equipped with the SAL objective from Eq. (8), we have an intuitive measure for the hyperparameter-induced posterior disagreement, which incentivizes hyperparameter learning by querying locations where disagreement is the largest. However, it does not inherently carry an incentive to _optimize_ the function. To inject an optimization objective into Eq. (8), we draw inspiration from information-theoretic BO and further condition on samples of the optimum. Conditioning on potential optima yields an additional source of disagreement reserved for promising regions of the search space.

We consider \((\bm{x}^{*},f^{*})\), representing the global optimum and optimal value considered in JES [29; 61], as hyperparameters. When conditioning on \((\bm{x}^{*},f^{*})\), we condition on an additional observation, which displaces the mean and reduces the variance at \(\bm{x}^{*}\). Moreover, the posterior over \(f\) becomes an upper truncated Gaussian, reducing the variance and pushing the mean marginally downwards in uncertain regions far away from the optimum as visualized in Fig. 3. Consequently, sampling and conditioning on \((\bm{x}^{*},f^{*})\) introduces an additional source of disagreement between the marginal posterior and the conditionals _globally_. The optimizer \((\bm{x}^{*},f^{*})\) is obtained through posterior sampling [67]. For brevity, we hereafter denote \((\bm{x}^{*},f^{*})\) by \(\bm{\ast}\). The resulting SCoreBO acquisition function is

\[\alpha_{SC}(\bm{x})=\mathbb{E}_{\bm{\theta},\bm{\ast}}[d(p(y_{\bm{x}}|\mathcal{ D}),p(y_{\bm{x}}|\bm{\theta},\bm{\ast},\mathcal{D}))].\] (9)

The joint posterior \(p(\bm{\theta},\bm{\ast}|\mathcal{D})=p(\bm{\ast}|\bm{\theta},\mathcal{D})p( \bm{\theta}|\mathcal{D})\) used for the expectation in Eq. (9) can be approximated by hierarchical sampling. We first draw \(M\) hyperparameters \(\bm{\theta}\) and thereafter \(N\) optimizers \(\bm{\ast}|\bm{\theta}\). As such, the expression for the SCoreBO acquisition function is:

\[\alpha(\bm{x})\approx\frac{1}{NM}\sum_{m=1}^{M}\sum_{n=1}^{N}d\left(p(y_{\bm{ x}}|\mathcal{D}),p(y_{\bm{x}}|\bm{\theta}_{m},\bm{\ast}_{\bm{\theta}_{m,n}}, \mathcal{D})\right),\] (10)

where \(N\) is the number of optimizers sampled per hyperparameter set. Notably, while the acquisition function in (9) considers the optimizer \((\bm{x}^{*},f^{*})\), SCoreBO is not restricted to employing that quantity alone. Drawing parallels to PES and MES, we can also choose to condition on either \(\bm{x}^{*}\) or \(f^{*}\) alone in place of \((\bm{x}^{*},f^{*})\). Doing so introduces a smaller disagreement in the posterior at the conditioned

Figure 2: Marginal posterior (top left, grey in other plots in top row), \(\alpha_{SAL}\) using the Hellinger distance (bottom left, black), and the three conditional GPs (blue, orange, green) and their marginal contribution to the total acquisition function (bottom row). The large disagreement in noise level and lengthscale, primarily caused by the orange GP (large noise, long lengthscale), makes \(\alpha_{SAL}\) query the lowest-valued point for a second time (selected location as vertical dashed line in the leftmost plot) to determine the mean and variance at that location.

location \(\bm{x}^{*}\), thus decreasing the acquisition value there. This will in turn decrease the emphasis that SCoreBO puts on optimization, relative to hyperparameter learning. In Fig. 3, the SCoreBO acquisition function is displayed for the same scenario as in Fig. 2. By conditioning on \(N=2\) optimizers per GP, we obtain \(N\times M\) posteriors (displaying the posterior for one out of two optimizers, i.e. the left star in (blue), in Fig. 3). The mean is pushed upwards around the extra observation and the posterior predictive distribution over \(f\) is truncated as it is now upper bounded by \(f^{*}\). While the preferred location under SAL is still attractive, the best location to query is now one that is more likely to be optimal, but still good under SAL.

Algorithm 1 displays how the involved densities are formed for one iteration of SCoreBO. For each hyperparameter set, a number of optima are sampled and individually conditioned on (CondGP) given the current data and hyperparameter set. After this procedure is completed for all hyperparameter sets, the statistical distance between each conditional posterior and the marginal is computed. The conditioning on the fantasized data point involves a rank-1 update of \(\mathcal{O}(n^{2})\) of the GP for each draw. As such, the complexity of constructing the acquisition functions is \(\mathcal{O}(MNn^{2})\) for \(M\) models, \(N\) optima per model and \(n\) data points. We utilize NUTS [25] for the MCMC involved with the fully Bayesian treatment, at a cost of \(\mathcal{O}(Dn^{3})\) per sample.

```
1:Input: Number of hyperparameter sets \(M\), number of sampled optima \(N\), current data \(\mathcal{D}\)
2:Output: Next query location \(\bm{x}^{\prime}\).
3:for\(m\in\{1,\dots,M\}\)do
4:\(\bm{\theta}_{m}\sim p(\bm{\theta}|\mathcal{D})\)
5:for\(n\in\{1,\dots,N\}\)do
6:\(\Re_{\bm{\theta}_{m},n}\leftarrow\max\limits_{f\bm{\theta}_{m},n},\text{where }f_{\bm{ \theta}_{m},n}\sim p(f|\bm{\theta}_{m},\mathcal{D})\)\(\{\)Draw \(n\) optima for each \(\bm{\theta}_{m}\)\(\}\)
7:\(p(y_{\bm{\theta}_{m}},\Re_{\bm{\theta}_{m},n},\mathcal{D})\leftarrow\texttt{ CondGP}(\Re_{\bm{\theta}_{m},n},\bm{\theta}_{m},\mathcal{D})\)\(\{\)Condition GPs on each optimum\(\}\)
8:endfor
9:endfor
10:\(\bm{x}^{\prime}=\arg\max\alpha(\bm{x})\)\(\{\)Defined in Eq. (10)\(\}\) ```

**Algorithm 1**SCoreBO iteration

### Approximation of Statistical Distances

We consider two proper statistical distances, Wasserstein distance and Hellinger distance. In contrast to BQBC, the statistical distance between the normally distributed conditionals and the marginal posterior predictive distribution (which is a Gaussian mixture), is not available in closed-form. We propose two approaches: estimating the distances using MC, which we outline for both distances in App E.1, and estimation using moment matching (MM), which we outline below.

Figure 3: Approximate marginal posterior after having conditioned on \((\bm{x}^{*},f^{*})\) (top left), \(\alpha_{SC}\) using the Hellinger distance (bottom left), the three conditional truncated posteriors and their marginal contribution to the total acquisition function for the same iteration as Fig. 2. Conditioning on \((\bm{x}^{*},f^{*})\) (marked as \(\star\), drawn from function samples in dashed) introduces additional disagreement between the marginal posterior and the sampled GPs in promising regions as a result of conditioning. In the figure, we marginalize over \(M=3\) sets of hyperparameters and \(N=2\) optimizers per GP, where each optimizerâ€™s contribution to the acquisition function is visible under its corresponding GP. Note that, since function draws are _noiseless_, the conditioned optimum does not need to surpass the best _noisy_ observation in value. This phenomenon is most notable in (orange).

Approximation through Moment MatchingWe propose to fully utilize the closed-form expressions of the involved distances for Gaussians, and approximate the full posterior mixture \(p(y_{\bm{x}}|\mathcal{D})\) with a Gaussian distribution using moment matching (MM) for the first and second moment. While a Gaussian mixture is not generally well approximated by a Normal distribution, we show empirically in App. E that the distance between the conditionals and the approximate posterior is small. In the moment matching approach, the conditional posterior \(p(y_{\bm{x}}|\bm{\theta},\bm{\ast},\mathcal{D})\) utilizes a lower bound on the change in the posterior induced by conditioning on \(\bm{\ast}\), as derived in GIBBON [42], which conveniently involves a second moment matching step of the extended skew Gaussian [45]\(p(y_{\bm{x}}|\bm{\theta},\bm{\ast},\mathcal{D})\). This naive approach circumvents a quadratic cost \(\mathcal{O}(N^{2}M^{2})\) in the number of samples of each pass through the acquisition function, and yields comparable performance to the MC estimation procedures proposed in App. E.1. In App. E, we qualitatively assess the accuracy of the MM approach for both distances, and display its ability to preserve the shape of the acquisition function.

## 4 Experiments

In this section we showcase the performance of the SAL and SCoreBO acquisition functions on a variety of tasks. For active learning, SAL shows state-of-the-art performance on a majority of benchmarks, and is more robust than the baselines. For the optimization tasks, SCoreBO more efficiently learns the model hyperparameters, and outperforms prominent Bayesian optimization acquisition functions on a variety of tasks. All experiments are implemented in BoTorch [2]1. We use the same \(\mathcal{LN}(0,3)^{2}\) hyperparameter priors as Riis et al. [50] unless specified otherwise. SCoreBO _and all baselines_ utilize fully Bayesian treatment of the hyperparameters. The complete experimental setup is presented in detail in Appendix B, and our code is publicly available at https://github.com/hvarner/scorebo.git. We utilize the moment matching approximation of the statistical distance. Experiments for the MC variant of SCoreBO are found in App. E.2.

Footnote 1: https://botorch.org/ (v0.8.4)

### Active Learning Tasks

To evaluate the performance of SAL, we compare it with BALD, BQBC and QBMGP on the same six functions used by Riis et al. [50]: Gramacy (1D) has a periodicity that is hard to distinguish from noise, Higdon and Gramacy (2D) varies in characteristics in different regions, whereas Branin, Hartmann-6 and Ishigami have a generally nonlinear structure. We display both the Wasserstein and Hellinger distance versions of SAL, denoted as SAL-WS and SAL-HR, respectively. We evaluate each method on their predictive power, measured by the negative Marginal Log Likelihood (MLL) of the model predictions over a large set of validation points. MLL emphasizes calibration (accurate uncertainty estimates) in prediction over an accurate predictive mean. In Fig. 11, we show how the average validation set MLL changes with increasing training data. SAL-HR is the top-performing acquisition function on three out of six tasks, and rivals BALD for stability in predictive performance. This is particularly evident on the Ishigami function, where most methods fluctuate in the quality of their predictions. This can be attributed to emphasis on rapid hyperparameter learning, which is visualized in detail in App. D, Fig. 15. In the rightmost plot, the real-time average per-seed ranking of acquisition function performance is displayed as a function of the fraction of budget expended. SAL-HR performs best, followed by BQBC andBALD. SAL-WS, however, does not display similarly consistent predictive quality as SAL-HR. The ability of SAL-HR to correctly estimate hyperparameters ensures calibrated uncertainty estimates, which makes it the better candidate for BO. In App. C.1, Fig. 11, we show the evolution of the average Root Mean Squared Error (RMSE) of the same tasks, where SAL-WS performs best and SAL-HR lags behind, which demonstrates the viability of various distance metrics on different tasks.

### Bayesian Optimization Tasks

For the BO tasks, we use the Hellinger distance for its proficiency in prediction calibration and hyperparameter learning. We compare against several state-of-the-art baselines from the BO literature: NEI for noisy experiments [36], as well as JES[29], the MES approach GIBBON [42] and PES [24]. As an additional reference, we include NEI for noisy experiments [36] using MAP estimation.

Efficiently learning the hyperparametersTo showcase SCoreBO's ability to find the correct model hyperparameters, we run all relevant acquisition functions on samples from the 8-dimensional GP in Fig. 1. We exploit that for GP samples, the objectively true hyperparameters are known (in contrast to typical synthetic test functions). We utilize the same priors as in Fig. 1 on all the hyperparameters and compare SCoreBO to NEI to assess the ability of each acquisition function to work independently of the choice of prior. In Fig. 5, for each acquisition function, we plot the average log regret over 20 different 8-dimensional instances of this task. The tasks at hand have lengthscales that vary substantially between dimensions, as detailed in App B. The explanation for the good performance of SCoreBO can be see in Fig. 17 in App. D, where SCoreBO converges substantially faster towards the correct hyperparameter values than NEI for both types of priors.

Synthetic test functionsWe run SCoreBO on a number of commonly used synthetic test functions for \(25|\bm{\theta}|\) iterations, and present how the log inference regret evolves over the iterations in Fig. 6. All benchmarks are perturbed by Gaussian noise. We evaluate inference regret, i.e., the current best guess of the optimal location \(\operatorname*{arg\,max}_{\bm{x}}\mu(\bm{x})\), which is conventional for non-myopic acquisition functions [22, 24, 29]. SCoreBO yields the the best final regret on four of the six tasks. In the relative rankings (rightmost plot), SCoreBO ranks poorly initially, but once hyperparameters are learned approximately halfway through the run, it substantially outperforms the competition. On Rosenbrock (4D), the relatively poor performance can explained by the apparent non-stationarity of the task, detailed in Fig. D.3, which makes hyperparameters diverge over time. This exposes a weakness of SCoreBO: When the modeling assumptions (such as stationarity) do not align with the task, optimization performance may suffer due to perpetual disagreement in the posterior. In App. C.2, we display the performance of SCoreBO-KL and SCoreBO-WS on the same set of benchmarks, where both display highly competitive performance.

Figure 4: Negative Marginal Log Likelihood (MLL) on six active learning functions and the (smoothed) relative rankings throughout each run for QBMGP, BQBC, BALD and SAL using Wasserstein and Hellinger distance. We plot mean and one standard error for 25 repetitions.. SAL-HR is the top performing method, placing first in relative rankings. On Ishigami, only SAL-HR and BALD produces stable results.

Figure 5: Regret for NEI and SCoreBO on the 8-dimensional GP sample for two different types of hyperparameter priors. Mean and standard deviation are plotted for all hyperparameter samples across 20 repetitions.

Figure 6: Average log inference regret and (smoothed) relative ranking across 50 repetitions between the acquisition functions for SCoreBO, JES, MES and NEI on six synthetic test functions. SCoreBO produces the best final regret on 4 out of 6 tasks, and has a substantially lower average ranking by the end of each run.

### A Practical Need for Self-correction

Lastly, we evaluate the performance of SCoreBO on three atypical tasks with increased emphasis on the surrogate model: (1) high-dimensional BO through sparse adaptive axis-aligned priors (SAASBO) [15], (2) BO with additively decomposable structure (AddGPs) [19; 32] and (3) non-stationary, heteroskedastic modelling with HEBO [13]. Eriksson & Jankowiak [15] consider their proposed method for noiseless tasks, where active variables easily distinguish from their non-active counterparts. However, SAASBO is not restricted to noiseless tasks. For AddGPs, data cross-covariance, and lack thereof, is similarly difficult to infer in the presence of noise.

In Fig. 7, we visualize the performance of SCoreBO and competing acquisition functions _with SAASBO priors_ on two noisy benchmarks, Ackley-4 and Hartmann-6, with dummy dimensions added, as well as two real-world benchmarks: fitting a weighted Lasso model in 180 dimensions [53], and the tuning of all 385 lengthscales and three regularization parameters of an SVM [12], a task also considered by Eriksson & Jankowiak [15]. On these benchmarks, where finding the correct hyperparameters is crucial for performance, SCoreBO clearly outperforms traditional methods. To further exemplify how SCoreBO identifies the relevant dimensions, in Fig. 8, we show how the hyperparameters evolve on the 25D-embedded Ackley (4D) task. SCoreBO quickly finds the correct lengthscales and outputscale with high certainty, whereas NEI remains uncertain of which dimensions are active throughout the optimization procedure. Impressively, SCoreBO finds accurate hyperparameters even faster than BALD, despite the latter being a pure active learning approach.

Secondly, we demonstrate the ability of SCoreBO to self-correct on _uncertainty in kernel design_, by considering AddGP tasks. We utilize the approach of Gardner et al. [19], where additive decompositions are marginalized over. Ideally, a sufficiently accurate decomposition is found quickly, which rapidly speeds up optimization through accurate cross-correlation of data. Fig. 9 demonstrates SCoreBO's performance on two GP sample tasks and a real-world task estimating cosmological constants (leftmost 3 plots) and its ability to find the correct additive decompositions (right). We observe that SCoreBO identifies correct decompositions substantially better than NEI. Final performance, however, is only marginally better, as substantial resources are expended finding the right decompositions. Notably, the Cosmological Constants task does not display additive decomposability. As such, SCoreBO unsuccessfully expends resources attempting to reduce disagreement over additive structures, which hampers performance. This demonstrates that while SCoreBO learns the problem structure at increased rates, improved BO performance does not automatically follow.

Lastly, we apply SCoreBO to the HEBO [13] GP model, the winner of the NeurIPS 2020 Black-box optimization challenge [62]. The model employs input [56] and output warpings, the former of

Figure 8: Hyperparameter convergence on the \(25D\)-embedded \(4D\) Ackley function with a SAASBO HP prior for SCoreBO, NEI and BALD. Log HP mean and 1 standard deviation is plotted per iteration. SCoreBO identifies \(\ell_{1},\dots,\ell_{4}\) as important (short lengthscales, \(\ell_{i}\approx 10^{-1}\)) with low uncertainty and \(\ell_{5},\dots,\ell_{25}\) as dummy dimensions (\(\ell_{i}\gg 10^{1}\)). NEI fails to identify any important lengthscales, whereas SCoreBO correctly identifies active dimensions with high certainty.. Notably, SCoreBO finds accurate hyperparameters even faster than BALD, a pure active learning approach. Reference HP values (where available) are marked with a dashed line.

Figure 7: Final loss using SAASBO priors on the noisy embedded Ackley-4, embedded Hartmann-6, the DNA classification and the SVM HPO task, mean and one standard error. SCoreBO identifies the important dimensions rapidly, and successfully optimizes the tasks. The optimal value is marked with a dashed line.

which are learnable to account for the heteroskedasticity that is prevalent in real-world optimization, and particularly HPO [13; 56], tasks. The complex model provides additional degrees of freedom in learning the objective. We evaluate SCoreBO and all baselines on three 4D deep learning HPO tasks: two involving large language models, and one from computer vision, from the PD1 [66] benchmarking suite. Fig. 10 displays that SCoreBO obtains the best final accuracy on 2 out of 3 tasks, suggesting that self-correction is warranted for optimization of deep learning pipelines.

## 5 Conclusion and Future Work

The hyperparameters of Gaussian processes play an integral role in the efficiency of both Bayesian optimization and active learning applications. In this paper, we propose Statistical distance-based Active Learning (SAL) and Self-Correcting Bayesian Optimization (SCoreBO), two acquisition functions that explicitly consider hyperparameter-induced disagreement in the posterior distribution when selecting which points to query. We achieve high-end performance on both active learning and Bayesian optimization tasks, and successfully learn hyperparameters and kernel designs at improved rates compared to conventional methods. SCoreBO breaks ground for new methods in the space of joint active learning and optimization of black-box functions, which allows it to excel in high-dimensional BO, where learning important dimensions are vital. Moreover, the potential downside of self-correction is displayed when the model structure does not support the task at hand, or when self-correction is not required to solve the task. For future work, we will explore additional domains in which SAL and SCoreBO can allow for increased model complexity in BO applications.

## 6 Limitations

SCoreBO displays the ability to increase optimization efficiency on complex tasks that necessitate accurate modeling. However, SCoreBO's efficiency is ultimately contingent on the intrinsic ability of the GP to model the task at hand. Appendix 19 demonstrates this issue for the Rosenbrock (4D) function, where SCoreBO performs worse relative to other acquisition functions. There, the hyperparameter values increase over time instead of converge, which suggests that the objective is not part of the class of functions defined by the kernel. Thus, the self-correction effort is less helpful towards optimization. Moreover, increasing the model capacity, such as in Sec. 4.3, comes with increasing resources allocated towards self-correction. In highly constrained-budget applications, such resource allocation may not yield the best result, especially if increased model complexity is unwarranted. This is evident from the synthetic AddGP tasks, where despite accurately identifying the additive components, SCoreBO does not provide substantial performance gains over NEI. Lastly, SCoreBO's reliance on fully Bayesian hyperparameter treatment makes it more computationally demanding than MAP-based alternatives, limiting its use in high-throughput applications.

Figure 10: Performance on the PD1 deep learning tasks over 20 repetitions using the warpings from HEBO [13]. SCoreBO obtains the best final accuracy on 2 out of 3 tasks, placing second on the third.

Figure 9: Final value of using AddGPs on 6D and 10D GP sample functions, fully decomposable in groups of two, and the Cosmological Constants tasks. SCoreBO achieves better final performance (left, middle) with low uncertainty, and successfully finds the additive components of the 6D task (right).

## Acknowledgements

We thank the anonymous reviewers for their valuable contributions related SAL-KL and and its relationship with BALD, as well as their general feedback on the clarity of the paper and how our method was conveyed. We also thank Eytan Bakshy for their helpful feedback on earlier versions of this paper. Luigi Nardi was supported in part by affiliate members and other supporters of the Stanford DAWN project -- Ant Financial, Facebook, Google, Intel, Microsoft, NEC, SAP, Teradata, and VMware. Carl Hyrafner, Erik Hellsten and Luigi Nardi were partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. Luigi Nardi was partially supported by the Wallenberg Launch Pad (WALP) grant Dnr 2021.0348. Frank Hutter acknowledges support through TAILOR, a project funded by the EU Horizon 2020 research and innovation programme under GA No 952215, by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828, by the state of Baden-Wurttemberg through bwHPC and the German Research Foundation (DFG) through grant no INST 39/963-1 FUGG, and by the European Research Council (ERC) Consolidator Grant "Deep Learning 2.0" (grant no. 101045765). The computations were also enabled by resources provided by the Swedish National Infrastructure for Computing (SNIC) at LUNARC partially funded by the Swedish Research Council through grant agreement no. 2018-05973. Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the ERC. Neither the European Union nor the ERC can be held responsible for them.

## References

* Atkinson and Donev [1992] Atkinson, A. and Donev, A. _Optimum Experimental Designs_. Oxford science publications. Clarendon Press, 1992. ISBN 9780198522546. URL https://books.google.se/books?id=cmmOA_-M7S0C.
* Balandat et al. [2020] Balandat, M., Karrer, B., Jiang, D. R., Daulton, S., Letham, B., Wilson, A. G., and Bakshy, E. Botorch: A framework for efficient monte-carlo bayesian optimization. In _Advances in Neural Information Processing Systems_, 2020. URL http://arxiv.org/abs/1910.06403.
* Berkenkamp et al. [2019] Berkenkamp, F., Schoellig, A. P., and Krause, A. No-regret bayesian optimization with unknown hyperparameters. _Journal of Machine Learning Research_, 20(50):1-24, 2019. URL http://jmlr.org/papers/v20/18-213.html.
* Berkenkamp et al. [2021] Berkenkamp, F., Krause, A., and Schoellig, A. Bayesian optimization with safety constraints: Safe and automatic parameter tuning in robotics. _Machine Learning_, 06 2021. doi: 10.1007/s10994-021-06019-1.
* Bingham et al. [2018] Bingham, E., Chen, J. P., Jankowiak, M., Obermeyer, F., Pradhan, N., Karaletsos, T., Singh, R., Szerlip, P., Horsfall, P., and Goodman, N. D. Pyro: Deep Universal Probabilistic Programming. _Journal of Machine Learning Research_, 2018.
* Bodin et al. [2020] Bodin, E., Kaiser, M., Kazlauskaite, I., Dai, Z., Campbell, N., and Ek, C. H. Modulating surrogates for Bayesian optimization. In III, H. D. and Singh, A. (eds.), _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pp. 970-979. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/bodin20a.html.
* Bogunovic and Krause [2021] Bogunovic, I. and Krause, A. Misspecified gaussian process bandit optimization. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 3004-3015. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/177db6acfe388526a4c7bff88e1feb15-Paper.pdf.
* Bull [2011] Bull, A. D. Convergence rates of efficient global optimization algorithms. 12:2879-2904, 2011.
* Calandra et al. [2014] Calandra, R., Gopalan, N., Seyfarth, A., Peters, J., and Deisenroth, M. Bayesian gait optimization for bipedal locomotion. In Pardalos, P. and Resende, M. (eds.), _Proceedings of the Eighth International Conference on Learning and Intelligent Optimization (LION'14)_, 2014.
* 304, 1995. doi: 10.1214/ss/1177009939. URL https://doi.org/10.1214/ss/1177009939.
* Chen et al. [2018] Chen, Y., Huang, A., Wang, Z., Antonoglou, I., Schrittwieser, J., Silver, D., and de Freitas, N. Bayesian optimization in alphago. _CoRR_, abs/1812.06855, 2018. URL http://arxiv.org/abs/1812.06855.
* Cortes and Vapnik [1995] Cortes, C. and Vapnik, V. Support vector networks. _Machine Learning_, 20:273-297, 1995.
* Cowen-Rivers et al. [2020] Cowen-Rivers, A. I., Lyu, W., Wang, Z., Tutunov, R., Hao, J., Wang, J., and Bou-Ammar, H. HEBO: heteroscedastic evolutionary bayesian optimisation. _CoRR_, abs/2012.03826, 2020. URL https://arxiv.org/abs/2012.03826.
* Eijeh et al. [2022] Eijeh, A., Medvinsky, L., Councilman, A., Nehra, H., Sharma, S., Adve, V., Nardi, L., Nurvitadhi, E., and Rutenbar, R. A. Hpvm2fpga: Enabling true hardware-agnostic fpga programming. In _Proceedings of the 33rd IEEE International Conference on Application-specific Systems, Architectures, and Processors_, 2022.
* Eriksson and Jankowiak [2021] Eriksson, D. and Jankowiak, M. High-dimensional Bayesian optimization with sparse axis-aligned subspaces. In de Campos, C. and Maathuis, M. H. (eds.), _Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence_, volume 161 of _Proceedings of Machine Learning Research_, pp. 493-503. PMLR, 27-30 Jul 2021. URL https://proceedings.mlr.press/v161/eriksson21a.html.

* Eriksson et al. [2019] Eriksson, D., Pearce, M., Gardner, J., Turner, R. D., and Poloczek, M. Scalable global optimization via local bayesian optimization. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf.
* Frazier [2018] Frazier, P. I. A tutorial on bayesian optimization. _arXiv preprint arXiv:1807.02811_, 2018.
* Gardner et al. [2015] Gardner, J., Malkomes, G., Garnett, R., Weinberger, K. Q., Barbour, D., and Cunningham, J. P. Bayesian active model selection with an application to automated audiometry. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf.
* Gardner et al. [2017] Gardner, J., Guo, C., Weinberger, K., Garnett, R., and Grosse, R. Discovering and Exploiting Additive Structure for Bayesian Optimization. In Singh, A. and Zhu, J. (eds.), _Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 54, pp. 1311-1319. Proceedings of Machine Learning Research, 2017.
* Gardner et al. [2018] Gardner, J. R., Pleiss, G., Bindel, D., Weinberger, K. Q., and Wilson, A. G. Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In _Advances in Neural Information Processing Systems_, 2018.
* Griffiths and Hernandez-Lobato [2017] Griffiths, R.-R. and Hernandez-Lobato, J. M. Constrained bayesian optimization for automatic chemical design. _arXiv: Machine Learning_, 2017.
* Hennig and Schuler [2012] Hennig, P. and Schuler, C. Entropy search for information-efficient global optimization. 98888 (1):1809-1837, 2012.
* Hennig and Schuler [2012] Hennig, P. and Schuler, C. J. Entropy search for information-efficient global optimization. _Journal of Machine Learning Research_, 13(1):1809-1837, June 2012. ISSN 1532-4435.
* Hernandez-Lobato et al. [2014] Hernandez-Lobato, J. M., Hoffman, M. W., and Ghahramani, Z. Predictive entropy search for efficient global optimization of black-box functions. In _Advances in Neural Information Processing Systems_, 2014. URL https://proceedings.neurips.cc/paper/2014/file/069d3bb002acd8d7dd095917f9efe4cb-Paper.pdf.
* Hoffman and Gelman [2014] Hoffman, M. D. and Gelman, A. The no-u-turn sampler: Adaptively setting path lengths in hamiltonian monte carlo. _Journal of Machine Learning Research_, 15(47):1593-1623, 2014. URL http://jmlr.org/papers/v15/hoffman14a.html.
* Houlsby et al. [2011] Houlsby, N., Huszar, F., Ghahramani, Z., and Lengyel, M. Bayesian active learning for classification and preference learning. _arXiv preprint arXiv:1112.5745_, 2011.
* Hutter et al. [2011] Hutter, F., Hoos, H., and Leyton-Brown, K. Sequential model-based optimization for general algorithm configuration. In Coello, C. (ed.), _Proceedings of the Fifth International Conference on Learning and Intelligent Optimization (LION'11)_, volume 6683, pp. 507-523, 2011.
* Hutter et al. [2017] Hutter, F., Lindauer, M., Balint, A., Bayless, S., Hoos, H., and Leyton-Brown, K. The configurable SAT solver challenge (CSSC). 243:1-25, 2017.
* Hvarfner et al. [2022] Hvarfner, C., Hutter, F., and Nardi, L. Joint entropy search for maximally-informed bayesian optimization. In _Proceedings of the 36th International Conference on Neural Information Processing Systems_, 2022.
* Hvarfner et al. [2022] Hvarfner, C., Stoll, D., Souza, A., Lindauer, M., Hutter, F., and Nardi, L. PiBO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization. In _International Conference on Learning Representations_, 2022.
* Jones et al. [1998] Jones, D., Schonlau, M., and Welch, W. Efficient global optimization of expensive black box functions. 13:455-492, 1998.

* Kandasamy et al. [2015] Kandasamy, K., Schneider, J., and Poczos, B. High Dimensional Bayesian Optimisation and Bandits via Additive Models. In Bach, F. and Blei, D. (eds.), _Proceedings of the 32nd International Conference on Machine Learning (ICML'15)_, volume 37, pp. 295-304. Omnipress, 2015.
* Kandasamy et al. [2018] Kandasamy, K., Neiswanger, W., Schneider, J., Poczos, B., and Xing, E. P. Neural architecture search with bayesian optimisation and optimal transport. _Advances in neural information processing systems_, 31, 2018.
* Kirsch et al. [2019] Kirsch, A., Van Amersfoort, J., and Gal, Y. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. _Advances in neural information processing systems_, 32, 2019.
* Lalchand & Rasmussen [2020] Lalchand, V. and Rasmussen, C. E. Approximate inference for fully bayesian gaussian process regression. In _Symposium on Advances in Approximate Bayesian Inference_, pp. 1-12. PMLR, 2020.
* Letham et al. [2018] Letham, B., Brian, K., Ottoni, G., and Bakshy, E. Constrained Bayesian optimization with noisy experiments. _Bayesian Analysis_, 2018.
* MacKay [1992] MacKay, D. J. Information-based objective functions for active data selection. _Neural computation_, 4(4):590-604, 1992.
* Malkomes & Garnett [2018] Malkomes, G. and Garnett, R. Automating bayesian optimization with bayesian optimization. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/2b64c2f19d868305aa8bbc2d72902cc5-Paper.pdf.
* Malkomes et al. [2016] Malkomes, G., Schaff, C., and Garnett, R. Bayesian optimization for automated model selection. In Hutter, F., Kotthoff, L., and Vanschoren, J. (eds.), _Proceedings of the Workshop on Automatic Machine Learning_, volume 64 of _Proceedings of Machine Learning Research_, pp. 41-47, New York, New York, USA, 24 Jun 2016. PMLR. URL https://proceedings.mlr.press/v64/malkomes_bayesian_2016.html.
* Mayr et al. [2022] Mayr, M., Ahmad, F., Chatzilygeroudis, K. I., Nardi, L., and Kruger, V. Skill-based Multi-objective Reinforcement Learning of Industrial Robot Tasks with Planning and Knowledge Integration. _CoRR_, abs/2203.10033, 2022. URL https://doi.org/10.48550/arXiv.2203.10033.
* Mayr et al. [2022] Mayr, M., Hvarfner, C., Chatzilygeroudis, K., Nardi, L., and Krueger, V. Learning skill-based industrial robot tasks with user priors. _IEEE 18th International Conference on Automation Science and Engineering_, 2022. URL https://arxiv.org/abs/2208.01605.
* Moss et al. [2021] Moss, H. B., Leslie, D. S., Gonzalez, J., and Rayson, P. Gibbon: General-purpose information-based bayesian optimisation. _Journal of Machine Learning Research_, 22(235):1-49, 2021. URL http://jmlr.org/papers/v22/21-0120.html.
* Nardi et al. [2019] Nardi, L., Koeplinger, D., and Olukotun, K. Practical design space exploration. In _2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)_, pp. 347-358. IEEE, 2019.
* Nguyen et al. [2022] Nguyen, Q., Wu, K., Gardner, J., and Garnett, R. Local bayesian optimization via maximizing probability of descent. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), _Advances in Neural Information Processing Systems_, volume 35, pp. 13190-13202. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/555479a201da27c97aaeed842d16ca49-Paper-Conference.pdf.
* Nguyen et al. [2022] Nguyen, Q. P., Low, B. K. H., and Jaillet, P. Rectified max-value entropy search for bayesian optimization, 2022. URL https://arxiv.org/abs/2202.13597.
* Osborne [2010] Osborne, M. A. _Bayesian Gaussian processes for sequential prediction, optimisation and quadrature_. PhD thesis, Oxford University, UK, 2010.

* Papenmeier et al. [2022] Papenmeier, L., Nardi, L., and Poloczek, M. Increasing the scope as you learn: Adaptive bayesian optimization in nested subspaces. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=e4WF6112DI.
* Rainforth et al. [2023] Rainforth, T., Foster, A., Ivanova, D. R., and Smith, F. B. Modern bayesian experimental design, 2023.
* Rasmussen and Williams [2006] Rasmussen, C. and Williams, C. _Gaussian Processes for Machine Learning_. The MIT Press, 2006.
* Riis et al. [2022] Riis, C., Antunes, F. N., Huttel, F. B., Azevedo, C. L., and Pereira, F. C. Bayesian active learning with fully bayesian gaussian processes. _arXiv preprint arXiv:2205.10186_, 2022.
* Ru et al. [2018] Ru, B., Osborne, M. A., Mcleod, M., and Granziol, D. Fast information-theoretic Bayesian optimisation. In Dy, J. and Krause, A. (eds.), _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pp. 4384-4392. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/ru18a.html.
* Ru et al. [2020] Ru, B., Wan, X., Dong, X., and Osborne, M. Interpretable neural architecture search via bayesian optimisation with weisfeiler-lehman kernels. _arXiv preprint arXiv:2006.07556_, 2020.
* Sehic et al. [2021] Sehic, K., Gramfort, A., Salmon, J., and Nardi, L. LassoBench: A High-Dimensional Hyperparameter Optimization Benchmark Suite for Lasso. _arXiv preprint arXiv:2111.02790_, 2021.
* Settles [2009] Settles, B. Active learning literature survey. 2009.
* Snoek et al. [2012] Snoek, J., Larochelle, H., and Adams, R. Practical Bayesian optimization of machine learning algorithms. In Bartlett, P., Pereira, F., Burges, C., Bottou, L., and Weinberger, K. (eds.), _Proceedings of the 26th International Conference on Advances in Neural Information Processing Systems (NeurIPS'12)_, pp. 2960-2968, 2012.
* Snoek et al. [2014] Snoek, J., Swersky, K., Zemel, R., and Adams, R. Input warping for Bayesian optimization of non-stationary functions. In Xing, E. and Jebara, T. (eds.), _Proceedings of the 31th International Conference on Machine Learning, (ICML'14)_, pp. 1674-1682. Omnipress, 2014.
* Srinivas et al. [2012] Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M. W. Information-theoretic regret bounds for gaussian process optimization in the bandit setting. _IEEE Transactions on Information Theory_, 58(5):3250-3265, May 2012. ISSN 1557-9654. doi: 10.1109/tit.2011.2182033. URL http://dx.doi.org/10.1109/TIT.2011.2182033.
* Stanton et al. [2023] Stanton, S., Maddox, W., and Wilson, A. G. Bayesian optimization with conformal prediction sets. In Ruiz, F., Dy, J., and van de Meent, J.-W. (eds.), _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pp. 959-986. PMLR, 25-27 Apr 2023. URL https://proceedings.mlr.press/v206/stanton23a.html.
* Takeno et al. [2020] Takeno, S., Fukuoka, H., Tsukada, Y., Koyama, T., Shiga, M., Takeuchi, I., and Karasuyama, M. Multi-fidelity Bayesian optimization with max-value entropy search and its parallelization. In III, H. D. and Singh, A. (eds.), _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pp. 9334-9345. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/takeno20a.html.
* Takeno et al. [2023] Takeno, S., Inatsu, Y., and Karasuyama, M. Randomized Gaussian process upper confidence bound with tighter Bayesian regret bounds. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 33490-33515. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/takeno23a.html.
* Tu et al. [2022] Tu, B., Gandy, A., Kantas, N., and Shafei, B. Joint entropy search for multi-objective bayesian optimization. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=ZChgD80oOds.

* Turner et al. [2020] Turner, R., Eriksson, D., McCourt, M., Kili, J., Laaksonen, E., Xu, Z., and Guyon, I. Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020. In Escalante, H. J. and Hofmann, K. (eds.), _Proceedings of the NeurIPS 2020 Competition and Demonstration Track_, volume 133 of _Proceedings of Machine Learning Research_, pp. 3-26. PMLR, 06-12 Dec 2021. URL https://proceedings.mlr.press/v133/turner21a.html.
* Wan et al. [2021] Wan, X., Nguyen, V., Ha, H., Ru, B., Lu, C., and Osborne, M. A. Think global and act local: Bayesian optimisation over high-dimensional categorical and mixed search spaces. In Meila, M. and Zhang, T. (eds.), _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pp. 10663-10674. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/wan21b.html.
* Wan et al. [2021] Wan, X., Nguyen, V., Ha, H., Ru, B., Lu, C., and Osborne, M. A. Think global and act local: Bayesian optimisation over high-dimensional categorical and mixed search spaces. _International Conference on Machine Learning (ICML) 38_, 2021.
* Wang and Jegelka [2017] Wang, Z. and Jegelka, S. Max-value entropy search for efficient bayesian optimization. In _International Conference on Machine Learning (ICML)_, 2017.
* Wang et al. [2023] Wang, Z., Dahl, G. E., Swersky, K., Lee, C., Nado, Z., Gilmer, J., Snoek, J., and Ghahramani, Z. Pre-trained Gaussian processes for Bayesian optimization. _arXiv preprint arXiv:2109.08215_, 2023.
* Wilson et al. [2020] Wilson, J. T., Borovitskiy, V., Terenin, A., Mostowsky, P., and Deisenroth, M. P. Efficiently sampling functions from gaussian process posteriors. In _International Conference on Machine Learning_, 2020. URL https://arxiv.org/abs/2002.09309.
* Wu et al. [2017] Wu, J., Poloczek, M., Wilson, A. G., and Frazier, P. Bayesian optimization with gradients. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/64a08e5fie6c39faeb90108c430eb120-Paper.pdf.
* Wynne et al. [2021] Wynne, G., Briol, F.-X., and Girolami, M. Convergence guarantees for gaussian process means with misspecified likelihoods and smoothness. _J. Mach. Learn. Res._, 22(1), jan 2021. ISSN 1532-4435.
* Yao et al. [2020] Yao, Y., Vehtari, A., and Gelman, A. Stacking for non-mixing bayesian computations: The curse and blessing of multimodal posteriors, 2020.
* Zhang et al. [2019] Zhang, B., Cole, D. A., and Gramacy, R. B. Distance-distributed design for gaussian process surrogates, 2019.

Additional Related Work

We review approaches that address model accuracy in Bayesian optimization.

Expanded Space of Models[38, 39] expands model uncertainty to kernel design in order to find the most accurate model possible within an expanded class of functions, whereas [56] expands model uncertainty to involve warpings of the input space. Since the expanded class of functions offers additional modelling flexibility, BO can ideally be conducted on a more accurate model than under a vanilla setting. As demonstrated with AddGPs in Sec. 4.3, \(\mathtt{SCoreBD}\) can work in conjunction with kernel search to find such models at an accelerated rate.

Simplified ModellingA contrasting line of work involve methods which restrict modeling by reducing the level of detail [6] or scope of the optimization [16, 44, 63]. This line of work acknowledges that modeling may not, or should not, be globally accurate or granular in order to conduct optimization efficiently within the allocated budget. As such, these lines of work address the issue of model accuracy by diametrically opposed philosophies. \(\mathtt{SCoreBD}\) offers an orthogonal approach to model accuracy by _accelerating_ the convergence of the model at hand, regardless of its level of complexity.

## Appendix B Supplementary Material on Experiments

### Experimental Setup

All the relevant methods are implemented as acquisition functions in BoTorch. For both the active learning and BO experiments, we run NUTS [25] in Pyro [5] to draw samples from the GP posterior over hyperparameters. Tab. 3 displays the parameters of the MCMC in detail, as well as other relevant parameters of various MC estimations throughout the article. For the active learning experiments, we mimic the experimental setup used in Riis et al. [50], and put a log-normal distribution \(\mathcal{LN}(0,3)\) on the lengthscales, outputscale variance and noise variance. Furthermore, we consider the mean constant \(c\) as a learnable parameter in the BO experiments, with a conventional \(\mathcal{N}(0,1)\) prior on the standardized inputs. When referring to the _BoTorch priors_, the priors are \(\Gamma(3,6)\), \(\Gamma(2,0.15)\), and \(\Gamma(1.1,0.05)\) for the lengthscales, outputscale, and noise variance, respectively, with the same prior on the learnable constant mean \(c\).

8D Gaussian Process sample taskFor the \(8D\) GP sample task, we utilize the lognormal prior from Sec. B on the hyperparameters. The hyperparameters of the _sampled objective functions_ are outlined in Tab. 1. As such, Tab. 1 displays the _true_ hyperparameters of the task we are trying to optimize. These hyperparameters are referenced in Fig. 17 and Fig. 18

SAASBO experimentsFor the SAASBO experiments, we utilize Ax 3, which runs the BoTorch 4 implementation of SAASBO with the deafuit prior on the hyperparameters as per BoTorch version 0.8.4, which differs slightly from the paper by Eriksson & Jankowiak [15]. The lengthscale parameters are given a hierarchical prior as \(\tau^{2}\sim\mathcal{H}\mathcal{C}(\alpha)\), \(\kappa_{i}^{2}\sim\mathcal{H}\mathcal{C}(1)\) and \(\ell_{i}=\frac{1}{\kappa_{i}\tau}\). We retain the default value of \(\alpha=0.1\). The noise variance, outputscale and mean constant are given the priors \(\sigma_{e}^{2}\sim\Gamma(0.9,10)\), \(\sigma_{f}^{2}\sim\Gamma(2,0.15)\) and \(c\sim\mathcal{N}(0,1)\).

Footnote 3: https://github.com/facebook/Ax

Footnote 4: https://github.com/pytorch/botorch

Additive Gaussian Process experimentsThe Additive GP setup closely resembles that of [19]. An additive partitioning is sampled, and the marginal likelihood of the model is maximized with regard to \(\boldsymbol{\theta}=\{\boldsymbol{\ell},\sigma_{f},\sigma_{\varepsilon}\}\). We utilize a slightly adapted proposal distribution, and fix a maximal number of additive partitions \(g_{max}\). Moreover, each dimension \(d\) belongs to one distinct additive decomposition

\begin{table}
\begin{tabular}{c|c|c|c} \hline Task & \(\sigma^{2}\) & \(\sigma_{\varepsilon}^{2}\) & \(\boldsymbol{\ell}\) & Kernel \\ \hline GP sample - \(8D\) & \(1\) & \(0.1\) & \(\exp 10\{[-1,-0.5,-0.5,0,0,0,1.5,1.5,1.5]\}\) & MatÃ©rn \\ \hline \end{tabular}
\end{table}
Table 1: Hyperparameters of the \(8D\) GP sample task.

[MISSING_PAGE_FAIL:18]

## Appendix C Additional Experiments

We display the RMSE performance of each of the SAL variants, a comparison of the MC and MM variants of SAL and SCoreBO.

### AL RMSE Performance

Fig. 11 displays the performance of both SAL variants and benchmark AL acquisition functions for the same set of tasks as in Sec. 4.1. We observe that SAL-WS consistently displays top performance, whereas SAL-HR lags behind substantially. This showcases SAL-HR's emphasis on hyperparameter learning as opposed to global exploration. By accurately assessing hyperparameters while sacrificing global exploration, SAL-HR ensures predictions with _calibrated uncertainty_, while sacrificing the _accuracy in predictive mean_ that follows from exploring the search space. SAL-WS offers a compromise which performs well under both metrics, sacrificing hyperparameter learning and calibration for accuracy in predictive mean.

### SCoreBO Distance Measure Ablation Analysis

In Fig. 13, We compare the Hellinger and Wasserstein variants of SCoreBO, both utilizing the MC approximation of the statistical distance. Since the MC approximation is asymptotically exact, we can better assess the performance of each distance metric, without having to consider the confounding factor that the MM approximation introduces. We note that SCoreBO-WS outperforms SCoreBO-HR

\begin{table}
\begin{tabular}{c|c|c|c} \hline Task & Dimensionality & \(\sigma_{\epsilon}\) & Search space \\ \hline Gramacy & \(1\) & \(0.1\) & \([0.5,2.5]\) \\ Higdon & \(1\) & \(0.1\) & \([0,20]\) \\ Gramacy & \(2\) & \(0.05\) & \([-2,6]^{D}\) \\ Branin & \(2\) & \(11.32\) & \([-5,10]\times[0,15]\) \\ Ishigami & \(3\) & \(0.187\) & \([-\pi,\pi]^{D}\) \\ Hartmann-6 & \(6\) & \(0.0192\) & \([0,1]^{D}\) \\ \hline \end{tabular}
\end{table}
Table 4: Benchmarks used for the active learning experiments.

Figure 11: Root Mean Squared Error (RMSE) on six active learning functions and the (smoothed) relative rankings throughout each run for QBMGP, BQBC, BALD and SAL using Wasserstein and Hellinger distance. We plot mean and one standard error for 25 repetitions.. SAL-WS is the top performing method, placing first in relative rankings. On Ishigami, only SAL-HR and BALD produces stable results.

\begin{table}
\begin{tabular}{c|c|c|c} \hline Task & Dimensionality & \(\sigma_{\epsilon}\) & Search space \\ \hline Branin & \(2\) & \(0.5\) & \([-5,10]\times[0,15]\) \\ Rosenbrock-2 & \(2\) & \(2.5\) & \([-1.5,1.5]^{D}\) \\ Hartmann-3 & \(6\) & \(0.5\) & \([0,1]^{D}\) \\ Rosenbrock-4 & \(4\) & \(2.5\) & \([-1.5,1.5]^{D}\) \\ Hartmann-4 & \(4\) & \(0.5\) & \([0,1]^{D}\) \\ Hartmann-6 & \(6\) & \(0.5\) & \([0,1]^{D}\) \\ \hline \end{tabular}
\end{table}
Table 5: Benchmarks used for the Bayesian optimization experiments.

on two tasks, but SCoreBO-HR is the overall more consistent approach. We hypothesize that the relative failure of SCoreBO-WS on Rosenbrock (4D) is caused by the objective's non-stationarity, which likely causes exceedingly large exploration of the hyperparameter space. This is supported by Fig. D.3, where the hyperparameters diverge over time on the Rosenbrock function.

Furthermore, in Fig. 12, we compare the KL and Hellinger variants of SCoreBO, both utilizing the moment matching estimation of the posterior. Notably, SCoreBO-KL is the natural extension of BALD to the self-correcting framework, in line with Prop. 1. SCoreBO-HR performs marginally better than SCoreBO-KL, winning 4 out of 6 tasks. However, the two variants are relatively close.

### SAASBO Noise Ablations

In Fig. 13, We measure the performance of SCoreBO and NEI on the embedded 25-dimensional Ackley (4D) and Hartmann (6D) tasks with varying noise levels using the SAAS [15] prior. We run three noise levels for each task: Noiseless (solid line), low (dashed line), where the noise standard deviation corresponds to 3% of total output range for Hartmann (6D), and 1.3% for Ackley, and high (dotted line, 13.3% / 4%). With increasing levels of noise, the difficulty of inferring active dimensions is expected to increase substantially, which should in turn hamper BO performance. In Fig. 14, we see that NEI and SCoreBO perform comparably on noiseless tasks (solid line) finding close-to-optimal solutions within 100 iterations. Moreover, for small levels of noise), the performance is still comparable. However, we observe a drastic fall-off in performance for NEI at the highest noise level, whereas SCoreBO's degrades gracefully. Notably, SCoreBO almost retains the performance of the noiseless at the highest noise level for Ackley.

## Appendix D Hyperparameter convergence

In Figures 15, 16, 17, 18, and 19 We demonstrate examples of hyperparameter convergence in AL and BO, as well as an example of hyperparameter divergence on the Rosenbrock \((4D)\) function, where SCoreBO performs marginally worse.

Figure 12: Log Regret of the Hellinger and Wasserstein MC-variants of SCoreBO. Both variants are competitive on all benchmarks, except for Wasserstein on Rosenbrock (4D) which lags behind slightly. Overall, Hellinger is more consistent, and wins 4 out of 6 benchmarks.

Figure 13: Log Regret of the Hellinger and KL moment matching variants of SCoreBO. Both variants are competitive on all benchmarks. Overall, Hellinger is marginally better, and wins 3 out of 6 benchmarks with an approximate tie on Branin.

### Active Learning Tasks

We display the hyperparameter convergence of SAL-WS, SAL-HR and the baseline active learning acquisition functions in Fig. 15. Both variants display accelerated hyperparameter learning compared to BQBC. SAL-HR in particular achieves low-variance hyperparameter uncertainty on Ishigami and the higher-dimensional Hartmann-6, where other methods struggle. We obtain approximately correct hyperparameters for these tasks by randomly sampling 300 points on the noiseless benchmark, thereafter performing MCMC and averaging the sampled hyperparameter estimates in logspace. The noise level is known a priori. and estimates the other hyperparameters with substantially greater certainty than other methods. We note that there is a drift in the hyperparameters as the number of observations increase, where output- and lengthscales trade off to reduce model complexity. As such, we provide an approximately stationary alternative in Fig. 16, where the outputscale is removed to avoid drift. In both cases, SAL-HR displays superior hyperparameter convergence, obtaining accurate hyperparameters in far fewer iterations, and with substantially less uncertainty than the alternatives.

### GP sample tasks

We display the convergence of SCoreBO and NEI with a wide lognormal and BoTorch prior on the GP sample task. We observe that the lognormal prior is well-aligned for most hyperparameters, whereas BoTorch prior is misaligned. This is evidenced by the unimportant dimensions \(\ell_{6},\ell_{7}\), and \(\ell_{8}\), which have suggested lengthscales that are incorrect by more than an order of magnitude. Nevertheless, SCoreBO suggests lengthscales that are approximately twice as long (\(10^{0.25}\approx 1.8\)) as NEI\((10^{-0.05}\approx 1.8\)).

Figure 16: Hyperparameter convergence on the Ishigami test function without outputscale. SAL-HR and BALD display stable hyperparameter convergence, and are the only acquisition function to accurately estimate all parameters.

Figure 14: Best observed value for varying levels of observation noise for NEI and SCoreBO using SAAS priors on the 25D-embedded lower-dimensional test functions. For Low levels of noise, the performance of NEI and SCoreBO are comparable, but SCoreBO retains performance substantially better for higher levels for noise.

Figure 15: Hyperparameter convergence on the Ishigami test function with outputscale. While no acquisition converges, SAL-HR and BALD display substantially more stable hyperparameters than other approaches.

\(0.9\)), and thus avoids unnecessary exploration along these dimensions. Moreover, SCoreBD correctly identifies the most important dimensions \(\ell_{1},\ell_{2}\), and \(\ell_{3}\) with good accuracy quickly, whereas NEI struggles to identify \(\ell_{1}\). SCoreBO slightly overestimates the importance of dimensions 2 and 3, likely to compensate for the inability to accurately estimate the importance of other hyperparameters.

### Hyperparameter Divergence on Synthetic BO tasks

We highlight additional examples on synthetic BO test functions where hyperparameters diverge. Due to the non-stationary structure of Rosenbrock in particular (and to a lesser extent, Branin), hyperparameters values diverge as the number of observations increase. In particular, the extreme steepness along the edges suggests an exceedingly large outputscale. With increasing observations, a lengthscale-outputscale trade-off occurs, where both hyperparameters grow seemingly indefinitely. Notably, this behavior is consistent regardless of the acquisition function (BO, AL, SOBOL). Due to the restricted hyperparameter set employed in the AL tasks, this problem is distinct to the BO tasks.

Figure 19: Hyperparameter divergence for SCoreBO and NEI on Rosenbrock (4D). The outputscale grows larger with increasing iterations, and the lengthscales grow similarly large as a countermeasure.

Figure 17: Hyperparameter convergence on the 8-dimensional GP sample for the broad log-normal prior. The black dashed line indicates true hyperparameter values. Mean and standard deviation are plotted across 20 repetitions, and a 3 iteration moving average of the plotted moments is applied to increase readability. Lengthscales \(\ell_{d}\) ordered smallest (most important) to largest (least important). SCoreBD finds accurate hyperparameters faster, has the most accurate values for all hyperparameters, and has substantially lower variance for all important (i.e. not \(\ell_{6},\ell_{7}\), and \(\ell_{8}\)) hyperparameters except for the noise variance.

Figure 18: Hyperparameter convergence on the 8-dimensional GP sample for the BoTorch priors. The black dashed line indicates true hyperparameter values. Mean and standard deviation are plotted across 20 repetitions, and a 3 iteration moving average of the plotted moments is applied to increase readability. Lengthscales \(\ell_{d}\) ordered smallest (most important) to largest (least important). SCoreBO finds accurate hyperparameters faster, has the most accurate values for all hyperparameters, and has substantially lower variance for all important (i.e. not \(\ell_{6},\ell_{7}\), and \(\ell_{8}\)) hyperparameters except for the noise variance.

Approximation Strategies

We display the quality of the moment matching approximation for both the Hellinger and Wasserstein distance. Moreover, we compare the performances of the MM and MC approaches.

### MC approximation of SAL and SCoreBO

Using Monte Carlo, different distances are most efficiently estimated in different manners. To approximate the Wasserstein distance, we utilize quasi-Monte Carlo. From the definition of the distance in one dimension, we obtain

\[W^{2}(p,q)=\int_{0}^{1}|Q(u)-P(u)|^{2}du\approx\sum_{\ell=1}^{L}|Q(u_{\ell})-P(u _{\ell})|^{2},\] (12)

where \(u_{\ell}\sim\mathcal{U}(0,1)\), and \(P(x)\) and \(Q(x)\) are the respective cumulative distributions for \(p(x)\) and \(q(x)\). To approximate the Hellinger distance, we obtain

\[H^{2}(p,q)=1-\int_{\mathcal{X}}\sqrt{\frac{q(x)}{p(x)}}p(x)dx\approx 1-\sum_{ \ell=1}^{L}\sqrt{\frac{q(x_{\ell})}{p(x_{\ell})}},\] (13)

where \(x_{\ell}\sim p(x)\) is sampled using MC. In SCoreBO, \(p(x)\) is the marginal \(p(y_{\bm{x}}|\mathcal{D})\), and \(q(x)\) each of the various conditionals \(p(y_{\bm{x}}|\divide,\bm{\theta},\mathcal{D})\).

### Performance of Monte Carlo

We display the performance of the MC variants of SAL-WS and SCoreBO-HR compared to their MM counterparts. Overall, performances are comparable, as each variant slightly exceeds the other on a couple of benchmarks. On the most complex benchmarks (Ishigami, Hartmann-4, Hartmann-6), the MC variant outperforms MM slightly, which suggests that MC is increasingly justified as disagreement in the posterior gets larger.

Figure 21: Log regret of SCoreBO-MM and SCoreBO-MC on the synthetic BO benchmarks. Overall performance is comparable, with MM outperforming marginally on 4 out of 6 tasks. MC notably outperforms slightly on the difficult Ishigami test function.

Figure 20: Negative marginal log likelihood (MLL) of the SAL MC (blue) and MM (red) variants on the active learning benchmarks. Overall performance is comparable, with three effectively tied benchmarks. MC outperforms slightly Hartmann-4 and Hartmann-6, and MM on Hartmann-3.

[MISSING_PAGE_FAIL:24]

## Appendix F Equivalence to BALD

We show that SAL, using the KL divergence as the metric \(d\), is equivalent to BALD.

\[BALD =I(y;\bm{\theta})=H(p(y))-E_{\bm{\theta}}[H(p(y|\bm{\theta}))]=\] \[=-\int_{-\infty}^{\infty}\int_{\bm{\theta}}p(\bm{\theta})p(y|\bm{ \theta})\ log[p(y)]d\bm{\theta}dy+\int_{\bm{\theta}}p(\bm{\theta})\int_{- \infty}^{\infty}p(y|\bm{\theta})log[p(y|\bm{\theta})]dyd\bm{\theta}=\] \[=\int_{\bm{\theta}}p(\bm{\theta})\int_{-\infty}^{\infty}p(y|\bm{ \theta})log\left[\frac{p(y|\bm{\theta})}{p(y)}\right]dyd\bm{\theta}=\int_{\bm {\theta}}p(\bm{\theta})KL(p(y|\bm{\theta})||p(y))d\bm{\theta}=\] \[=E_{\bm{\theta}}[KL(p(y|\bm{\theta})||p(y))]=SAL.KL\]

## Appendix G Additional Background

We provide additional background on fully Bayesian hyperparameter treatment in Gaussian processes and details regarding statistical distance metrics.

### Fully Bayesian Treatment

The posterior probability of observing a value \(y_{\bm{x}}\) for a point \(\bm{x}\) is given as:

Figure 24: Example of the per-sample Wasserstein distance computation using moment matching (solid lines) and large-scale, asymptotically exact quasi-MC with 2048 samples. The moment matching approximation mostly retains the shape of the asymptotically exact variant. The shape of the acquisition function is generally well captured, but high-variance regions have their distance underestimated by the moment matching approach, and low-variance regions have their distance over-estimated, leading to a biased approximation. The acquisition function y-axis is scaled individually per model to better highlight the difference in acquisition function value.

Figure 25: Example of the per-sample Wasserstein distance computation using moment matching (solid lines) and large-scale, asymptotically exact quasi-MC with 2048 samples. The moment matching approximation captures the shape of the asymptotically exact variant well, and only marginally over- and underestimates the distance. The acquisition function y-axis is scaled individually per model to better highlight the difference in acquisition function value.

\[p(y_{\bm{x}}|\mathcal{D}) =\int_{\bm{\theta}}\int_{f}p(y_{\bm{x}}|f,\bm{\theta})p(f|\bm{\theta},\mathcal{D})p(\bm{\theta}|\mathcal{D})df\,d\bm{\theta}\] \[=\int_{\bm{\theta}}\int_{f}p(y_{\bm{x}}|f,\bm{\theta})p(f|\bm{ \theta},\bm{x},\mathcal{D})p(\bm{\theta}|\mathcal{D})df)d\bm{\theta},\]

where \(f\) are the noiseless, latent function values as \(\bm{x}\) and \(\mathcal{D}\) is the observed data. The inner integral is equal to the GP predictive posterior,

\[\int_{f}p(y_{\bm{x}}|f,\bm{\theta})p(f|\bm{\theta},\mathcal{D})df=p(y_{\bm{x}} |\mathcal{D},\bm{\theta}).\]

However, the outer integral is intractable and is estimated using Markov Chain Monte Carlo (MCMC) methods. The resulting posterior prediction

\[p(y_{\bm{x}}|\mathcal{D})=\int_{\bm{\theta}}p(y_{\bm{x}}|\mathcal{D},\bm{ \theta})p(\bm{\theta}|\mathcal{D})d\bm{\theta}\approx\frac{1}{M}\sum_{j=1}^{ M}p(y_{\bm{x}}|\mathcal{D},\bm{\theta}_{j}),\;\;\bm{\theta}_{j}\sim p(\bm{ \theta}|\mathcal{D}),\]

is a Gaussian Mixture Model (GMM).

Within BAL and BO, the fully Bayesian treatment is often extended to involve the acquisition function, such that the acquisition function \(\alpha\) is computed as an expectation over the hyperparameters [46; 55]

\[\alpha(\bm{x}|\mathcal{D})=\mathbb{E}_{\bm{\theta}}\big{[}\alpha(\bm{x}|\bm{ \theta},\mathcal{D})\big{]}\approx\frac{1}{M}\sum_{j=1}^{M}\alpha(\bm{x}|\bm{ \theta}_{j},\mathcal{D})\;\;\bm{\theta}_{j}\sim p(\bm{\theta}|\mathcal{D}).\]

This is also the definition of fully Bayesian treatment considered in this work.

### Statistical Distance Details

The Hellinger distanceis a similarity measure between two probability distributions which has previously been employed in the context of BO-driven automated model selection by Malkomes et al. [39]. For two probability distributions \(p\) and \(q\), it is defined as

\[H^{2}(p,q)=\frac{1}{2}\int_{\mathcal{X}}\left(\sqrt{p(x)}-\sqrt{q(x)}\right)^ {2}\lambda dx,\] (14)

with some auxiliary measure \(\lambda\) with which both \(p\) and \(q\) are absolutely continuous. Specifically, for two normally distributed variables \(z_{1}\sim\mathcal{N}(\mu_{1},\sigma_{1}^{2}),\;z_{2}\sim\mathcal{N}(\mu_{2}, \sigma_{2}^{2})\),

\[H^{2}(z_{1},z_{2})=1-\sqrt{\frac{2\sigma_{1}\sigma_{2}}{\sigma_{1}^{2}+\sigma _{2}^{2}}}\exp\left[-\frac{1}{4}\frac{(\mu_{1}-\mu_{2})^{2}}{\sigma_{1}^{2}+ \sigma_{2}^{2}}\right].\] (15)

The Hellinger distance seeks to minimize the ratio between difference in mean and the sum of variances, which punishes outlier predictive distributions of high confidence. Similar to KL, initial queries have a tendency to be axis-aligned to attain selective length scale information.

The Wasserstein distanceis the average distance needed to move the probability mass of one distribution to morph into the other. The Wasserstein-\(k\) distance is defined as

\[W_{k}(p,q)=\left(\int_{0}^{1}|F_{q}(x)-F_{p}(x)|^{k}dx\right)^{1/k}\] (16)

For the normal distributions \(z_{1}\) and \(z_{2}\), the Wasserstein-2 distance is defined as

\[W_{2}(z_{1},z_{2})=\sqrt{(\mu_{1}-\mu_{2})^{2}+(\sigma_{1}-\sigma_{2})^{2}}.\] (17)

In practice, \(W_{2}\) places a premium on matching large-variance regions, leading to higher global exploration which can be detrimental for global optimization.

The KL divergenceThe KL divergence is a standard asymmetrical measure for dissimilarity between probability distributions. For two probability distributions \(P\) and \(Q\), it is given by \(\mathcal{D}_{KL}(P\mid\mid Q)=\int_{X}P(x)\text{log}(P(x)/Q(x))dx\). For Gaussian variables, it is computed as

\[KL(z_{1}||z_{2})=\text{log}\frac{\sigma_{1}}{\sigma_{1}}+\frac{\sigma_{1}^{2}+ (\mu_{1}-\mu_{1})^{2}}{\sigma_{1}^{2}}-\frac{1}{2}\] (18)

The KL divergence mainly prioritizes same order-of-magnitude variances, and will initially query the same location multiple times to assess noise levels. Thereafter, it tends to query in an axis-aligned fashion, close to previous queries, to attain information regarding the length scales, but places a low priority on global exploration.