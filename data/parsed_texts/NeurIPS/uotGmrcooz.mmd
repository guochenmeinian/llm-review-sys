# Optimal approximation

using complex-valued neural networks

 Paul Geuchen

MIDS,

KU Eichstatt-Ingolstadt,

Auf der Schanz 49,

85049 Ingolstadt, Germany

paul.geuchen@ku.de &Felix Voigtlaender

MIDS,

KU Eichstatt-Ingolstadt,

Auf der Schanz 49,

85049 Ingolstadt, Germany

felix.voigtlaender@ku.de

###### Abstract

Complex-valued neural networks (CVNNs) have recently shown promising empirical success, for instance for increasing the stability of recurrent neural networks and for improving the performance in tasks with complex-valued inputs, such as in MRI fingerprinting. While the overwhelming success of Deep Learning in the real-valued case is supported by a growing mathematical foundation, such a foundation is still largely lacking in the complex-valued case. We thus analyze the expressivity of CVNNs by studying their approximation properties. Our results yield the first quantitative approximation bounds for CVNNs that apply to a wide class of activation functions including the popular modReLU and complex cardioid activation functions. Precisely, our results apply to any activation function that is smooth but not polyharmonic on some non-empty open set; this is the natural generalization of the class of smooth and non-polynomial activation functions to the complex setting. Our main result shows that the error for the approximation of \(C^{k}\)-functions scales as \(m^{-k/(2n)}\) for \(m\to\infty\) where \(m\) is the number of neurons, \(k\) the smoothness of the target function and \(n\) is the (complex) input dimension. Under a natural continuity assumption, we show that this rate is optimal; we further discuss the optimality when dropping this assumption. Moreover, we prove that the problem of approximating \(C^{k}\)-functions using continuous approximation methods unavoidably suffers from the curse of dimensionality.

## 1 Introduction

Deep Learning currently predominantly relies on real-valued neural networks, which have led to breakthroughs in fields like image classification or speech recognition [23, 27, 43]. However, recent work has uncovered several application areas in which the use of complex-valued neural networks (CVNNs) leads to better results than the use of their real-valued counterparts. These application areas mainly include tasks where complex numbers inherently occur as inputs of a machine learning model such as Magnetic Resonance Imaging (MRI) [16, 38, 44] and Polarimetric Synthetic Aperture Radar (PolSAR) Imaging [8, 9, 48]. Moreover, CVNNs have been used to improve the stability of recurrent neural networks [5] and have been successfully applied in various other fields [32, 37]. The mathematical theory of these complex-valued neural networks, however, is still in its infancy. There is therefore a great interest in studying CVNNs and in particular in uncovering the differences and commonalities between CVNNs and their real-valued counterparts. A prominent example highlighting the unexpected differences between both network classes is the _universal approximation theorem_ for neural networks, whose most general real-valued version was proven in 1993 [28] (with a more restricted version appearing earlier [17]) and which was recently generalized to the case of CVNNs [45]. The two theorems describe necessary and sufficient conditions onan activation function which guarantee that arbitrarily wide neural networks of a fixed depth can approximate any continuous function on any compact set arbitrarily well (with respect to the uniform norm). Already here it was shown that complex-valued networks behave significantly different from real-valued networks: While real-valued networks are universal if and only if the activation function is non-polynomial, complex-valued networks with a single hidden layer are universal if and only if the activation function is non-polyharmonic (see below for a definition). Furthermore, there exist continuous activation functions for which deep CVNNs are universal but shallow CVNNs are not, whereas the same cannot happen for real-valued neural networks. This example shows that it is highly relevant to study the properties of CVNNs and to examine which of the fundamental properties of real-valued networks extend to complex-valued networks.

Essentially the only existing _quantitative_ result regarding the approximation-theoretical properties of CVNNs is [14], which provides results for approximating \(C^{k}\)-functions by _deep_ CVNNs using the modReLU activation function. However, for real-valued NNs it is known that already _shallow_ NNs can approximate \(C^{k}\)-functions at the optimal rate. Precisely, Mhaskar showed in [33] that one can approximate \(C^{k}\)-functions on \([-1,1]^{n}\) with an error of order \(m^{-k/n}\) as \(m\to\infty\), where \(m\) is the number of neurons in the hidden layer. Here he assumed that the activation function is smooth on an open interval and that at some point of this interval no derivative vanishes. This is equivalent to the activation function being smooth and non-polynomial on that interval, cf. [20, p. 53].

The present paper shows that a comparable result holds in the setting of complex-valued networks, by proving that one can approximate every function in \(C^{k}\) (\(\Omega_{n};\mathbb{C}\)) (where differentiability is understood in the sense of real variables) with an error of the order \(m^{-k/(2n)}\) (as \(m\to\infty\)) using shallow complex-valued neural networks with \(m\) neurons in the hidden layer. Here we define the cube \(\Omega_{n}:=[-1,1]^{n}+i[-1,1]^{n}\). The result holds whenever the activation function \(\phi:\mathbb{C}\to\mathbb{C}\) is smooth and non-polyharmonic on some non-empty open set. This is a very natural condition, since for polyharmonic activation functions there exist \(C^{k}\)-functions that cannot be approximated at all below some error threshold using shallow neural networks with this activation function [45].

Furthermore, the present paper studies in how far the approximation order of \(m^{-k/(2n)}\) is _optimal_, meaning that an order of \(m^{-(k/2n)-\alpha}\) (where \(\alpha>0\)) cannot be achieved. Here it turns out that the derived order of approximation is indeed optimal (even in the class of CVNNs with possibly more than one hidden layer) in the setting that the weight selection is _continuous_, meaning that the map that assigns to a function \(f\in C^{k}\left(\Omega_{n};\mathbb{C}\right)\) the weights of the approximating network is continuous with respect to some norm on \(C^{k}(\Omega_{n};\mathbb{C})\). This continuity assumption is natural since typical learning algorithms such as (stochastic) gradient descent use samples \(f(x_{j})\) of the target function and then apply continuous operations to them to update the network weights.

We investigate this optimality result further by dropping the continuity assumption and constructing two special smooth and non-polyharmonic activation functions with the first one having the property that the order of approximation can indeed be strictly improved via a _discontinuous_ selection of

\begin{table}
\begin{tabular}{c|c|c|c}  & Condition on & Continuity of & Approximation \\  & activation function & weight selection & Error \\ \hline Theorem 3.2 & smooth \& & \\  & non-polyharmonic & possible & \(\mathcal{O}(m^{-k/(2n)})\) \\ \hline \begin{tabular}{c} Consequence of \\ Theorem 4.1 \\ \end{tabular} & continuous & assumed & \(\Omega(m^{-k/(2n)})\) \\ \hline Theorem 4.2 & 
\begin{tabular}{c} very special \\ activation function \\ \end{tabular} & not assumed & \(\mathcal{O}(m^{-k/(2n-1)})\) \\ \hline Theorem 4.3 & \(\dfrac{1}{1+e^{-\operatorname{Re}(z)}}\) & not assumed & \(\widetilde{\Omega}(m^{-k/(2n)})\) \\ \end{tabular}
\end{table}
Table 1: Overview of the proven approximation bounds. \(k\) is the regularity of the approximated functions (which are assumed to be \(C^{k}\)), \(n\) the (complex) input dimension and \(m\) the number of neurons in the hidden layer of the network. The notation \(\widetilde{\Omega}\) is similar to \(\Omega\), but ignoring log factors.

the related weights. For the second activation function we show that the order of \(m^{-k/(2n)}\)_cannot_ be improved, even if one allows a discontinuous weight selection. This in particular shows that in the given generality of arbitrary smooth, non-polyharmonic activation functions, the upper bound \(\mathcal{O}\left(m^{-k/(2n)}\right)\) cannot be improved, even for a possibly discontinuous choice of the weights. An overview of the approximation bounds proven in this paper can be found in Table 1.

Moreover, we analyze the _tractability_ (in terms of the input dimension \(n\)) of the considered problem of approximating \(C^{k}\)-functions using neural networks. Theorem 5.1 shows that one necessarily needs a number of parameters _exponential_ in \(n\) to obtain a non-trivial approximation error. To the best of our knowledge, Theorem 5.1 is the first result showing that the problem of approximating \(C^{k}\)-functions using continuous approximation methods is intractable (in terms of the input dimension \(n\)).

### Related Work

**Real-valued neural networks.** By now, the approximation properties of real-valued neural networks are quite well-studied (cf. [10; 28; 33; 40; 41; 46; 47] and the references therein). We here only discuss a few papers that are most closely related to the present work.

In [33], Mhaskar analyzes the rate of approximation of shallow real-valued neural networks for target functions of regularity \(C^{k}\). Our results can be seen as the generalization of [33] to the complex setting. Our proofs rely on several techniques from [33]; however, significant modifications are required to make the proofs work for general smooth non-polyharmonic functions.

One of the first papers to observe that neural networks with general (smooth) activation function can be surprisingly expressive is [31] where it was shown that a neural network of _constant size_ can be universal. One of the activation functions in Section 4 is based on a similar idea.

The importance of distinguishing between continuous and discontinuous weight selection (which in our setting is discussed in Section 4) was observed for ReLU-networks in [47].

The paper [25] shows that neural network approximation is _not_ continuous in the following sense: The best approximating neural network \(\Phi(f)\) of a given size does not depend continuously on \(f\in C^{k}\). This result, however, is not in conflict with our results: We want to assign to any \(\widetilde{C}^{k}\)-function \(f\) a network \(\widetilde{\Phi}(f)\) that approximates \(f\) below the error threshold \(m^{-k/(2n)}\). The network \(\widetilde{\Phi}(f)\), however, does _not_ have to coincide with the best approximating network \(\Phi(f)\).

**Complex-valued neural networks.** When it comes to general literature about mathematical properties of complex-valued neural networks, surprisingly little work can be found. The _Universal Approximation Theorem for Complex-Valued Neural Networks_[45] has already been mentioned above. In particular, it has been shown that shallow CVNNs are universal if and only if the activation function \(\phi\) is not polyharmonic. Thus, the condition assumed in the present paper (that \(\phi\) should be smooth, but not polyharmonic) is quite natural.

Regarding _quantitative_ approximation results for CVNNs, the only existing work of which we are aware is [14], analyzing the approximation capabilities of _deep_ CVNNs where the modReLU is used as activation function. Since the modReLU satisfies our condition regarding the activation function, the present work can be seen as an improvement to [14]. Precisely, (i) we consider general activation functions, including but not limited to the modReLU, (ii) we improve the order of approximation by a log factor, and (iii) we show that this order of approximation can be achieved using shallow networks instead of the deep networks used in [14]. We stress that our proof techniques differ significantly from the ones applied in [14]: The arguments in [14] take their main ideas from [46] making heavy use of the specific definition of the modReLU. In contrast, since we consider quite general activation functions, we necessarily follow a much more general approach following the ideas from [33].

## 2 Preliminaries

**Shallow complex-valued neural networks.** In this paper we mainly consider so-called shallow complex-valued neural networks, meaning complex-valued neural networks with a single hidden layer. Precisely, we consider functions of the form

\[\mathbb{C}^{n}\ni z\mapsto\sum_{j=1}^{m}\sigma_{j}\phi\left(\rho_{j}^{T}\cdot z +\eta_{j}\right)\in\mathbb{C},\]with \(\rho_{1},...,\rho_{m}\in\mathbb{C}^{n}\), \(\sigma_{1},...,\sigma_{m},\eta_{1},...,\eta_{m}\in\mathbb{C}\) and an _activation function_\(\phi:\mathbb{C}\rightarrow\mathbb{C}\). Here, \(m\in\mathbb{N}\) denotes the number of neurons of the network and we write \(v^{T}\) for the transpose of a vector \(v\).

To simplify the formulation of the results, we introduce the following notation: We write \(\mathcal{F}^{\phi}_{n,m}\) for the set of _first layers_ of shallow complex-valued neural networks with activation function \(\phi\), with \(n\) input neurons and \(m\) hidden neurons, meaning

\[\mathcal{F}^{\phi}_{n,m}:=\left\{z\mapsto\left(\phi\left(\rho_{j}^{T}\cdot z+ \eta_{j}\right)\right)_{j=1}^{m}:\;\rho_{j}\in\mathbb{C}^{n},\;\eta_{j}\in \mathbb{C}\right\}\subseteq\left\{F:\mathbb{C}^{n}\rightarrow\mathbb{C}^{m} \right\}.\]

Hence, each shallow CVNN can be written as \(\sigma^{T}\Phi\) with \(\sigma\in\mathbb{C}^{m}\) and \(\Phi\in\mathcal{F}^{\phi}_{n,m}\); see Figure 1 for a graphical representation of a shallow CVNN.

**Approximation.** The paper aims to analyze the approximation of \(C^{k}\)-functions on the complex cube

\[\Omega_{n}:=[-1,1]^{n}+i[-1,1]^{n}\]

using shallow CVNNs. Here, we say that a function \(f:\Omega_{n}\rightarrow\mathbb{C}\) is in \(C^{k}(\Omega_{n};\mathbb{C})\) if and only if \(f\) is \(k\) times continuously differentiable on \(\Omega_{n}\), where the derivative is to be understood in the sense of real variables, i.e., in the sense of interpreting \(f\) as a function \([-1,1]^{2n}\rightarrow\mathbb{R}^{2}\) and taking usual real derivatives. We further define the \(C^{k}\)_-norm_ of a function \(f\in C^{k}(\Omega_{n};\mathbb{C})\) as

\[\|f\|_{C^{k}(\Omega_{n};\mathbb{C})}:=\sup_{\begin{subarray}{c} \mathbf{k}\in\mathbb{N}_{0}^{2}\\ |\mathbf{k}|\leq k\end{subarray}}\|\partial^{\mathbf{k}}f\|_{L^{\infty}( \Omega_{n};\mathbb{C})},\qquad\text{where}\qquad\|g\|_{L^{\infty}(\Omega_{n}; \mathbb{C})}:=\sup_{z\in\Omega_{n}}|g(z)|\] (2.1)

for any function \(g:\Omega_{n}\rightarrow\mathbb{C}\). Note that we write \(\mathbb{N}=\{1,2,3,...\}\) and \(\mathbb{N}_{0}=\{0\}\cup\mathbb{N}\). Using the previously introduced notation, we thus seek to bound the worst-case approximation error, i.e.,

\[\sup_{\begin{subarray}{c}f\in C^{k}(\Omega_{n};\mathbb{C})\\ \|f\|_{C^{k}(\Omega_{n};\mathbb{C})}\leq 1\end{subarray}}\inf_{\begin{subarray}{c} \Phi\in\mathcal{F}^{\phi}_{n,m}\\ \sigma\in\mathbb{C}^{m}\end{subarray}}\|f-\sigma^{T}\Phi\|_{L^{\infty}( \Omega_{n};\mathbb{C})}.\]

**Wirtinger calculus and polyharmonic functions.** For a function \(f:\mathbb{C}\rightarrow\mathbb{C}\) which is differentiable in the sense of real variables at a point \(z_{0}\in\mathbb{C}\) we define its _Wirtinger derivatives_ at \(z_{0}\) as

\[\partial_{\mathrm{wirt}}f(z_{0}):=\frac{1}{2}\left(\frac{\partial f}{\partial x }(z_{0})-i\cdot\frac{\partial f}{\partial y}(z_{0})\right)\quad\text{and} \quad\overline{\partial}_{\mathrm{wirt}}f(z_{0}):=\frac{1}{2}\left(\frac{ \partial f}{\partial x}(z_{0})+i\cdot\frac{\partial f}{\partial y}(z_{0}) \right).\]

Here, \(\frac{\partial}{\partial x}\) and \(\frac{\partial}{\partial y}\) denote the usual partial derivatives in the sense of real variables. We extend this definition to multivariate functions defined on open subsets of \(\mathbb{C}^{n}\) by considering _coordinatewise_ Wirtinger derivatives.

A function \(f:U\rightarrow\mathbb{C}\), where \(U\subseteq\mathbb{C}\) is an open set, is called _smooth_ if it is differentiable arbitrarily many times (in the sense of real variables). We write \(f\in C^{\infty}(U;\mathbb{C})\) in that case. Moreover, \(f\) is called _polyharmonic_ (on \(U\)) if it is smooth and if there exists \(m\in\mathbb{N}_{0}\) satisfying

\[\Delta^{m}f\equiv 0\quad\text{on }U.\]

Figure 1: Graphical representation of a shallow neural network. Input and output neurons are depicted as dots, hidden neurons are depicted as circles. The term _first layer_ describes the transformation from the input to the hidden neurons, including the application of the activation function.

Here, \(\Delta:=\frac{\partial^{2}}{\partial x^{2}}+\frac{\partial^{2}}{\partial y^{2}}=4 \partial_{\mathrm{wirt}}\overline{\partial}_{\mathrm{wirt}}\) denotes the usual Laplace-Operator.

The following Proposition 2.1 describes a property of non-polyharmonic functions which is crucial for proving the approximation results of this paper.

**Proposition 2.1**.: _Let \(\varnothing\neq U\subseteq\mathbb{C}\) be an open set and let \(\phi\in C^{\infty}(U;\mathbb{C})\) be non-polyharmonic. Then for every \(M\in\mathbb{N}_{0}\) there exists a point \(z_{M}\in U\) satisfying_

\[\partial_{\mathrm{wirt}}^{m}\overline{\partial}_{\mathrm{wirt}}^{\ell}\phi(z_ {M})\neq 0\quad\text{for all }m,\ell\in\mathbb{N}_{0}\text{ with }m,\ell\leq M.\]

The proof of Proposition 2.1 is an application of the Baire category theorem; see Appendix B.2.

**Important complex activation functions.** We briefly discuss in how far two commonly used complex activation functions satisfy our assumptions: The _modReLU_ proposed in [5] and the _complex cardioid_ used in [44] for MRI fingerprinting where the performance could be significantly improved using complex-valued neural networks. The modReLU is defined as

\[\sigma_{\mathrm{modReLU},b}:\quad\mathbb{C}\to\mathbb{C},\quad\sigma_{ \mathrm{modReLU},\beta}(z):=\begin{cases}(|z|+b)\frac{z}{|z|},&\text{if }|z|+b\geq 0,\\ 0,&\text{otherwise,}\end{cases}\]

where \(b<0\) is a fixed parameter. The complex cardioid is defined as

\[\mathrm{card}:\quad\mathbb{C}\to\mathbb{C},\quad\mathrm{card}(z):=\frac{1}{2}( 1+\cos(\spherical z))z.\]

Here, \(\spherical z=\theta\in\mathbb{R}\) denotes the polar angle of a complex number \(z=re^{i\theta}\), where we define \(\spherical 0:=0\); see Figure 2 for plots of the absolute value of the two functions.

Both functions are smooth and non-polyharmonic on a non-empty open subset of \(\mathbb{C}\), which is proven in Appendix A.2. Furthermore, they are both continuous on \(\mathbb{C}\). Therefore, our approximation bounds established in Theorems 3.1 and 3.2 in particular apply to those two functions.

## 3 Main results

In this section we state the main results of this paper and provide proof sketches for them. Detailed proofs of the two statements can be found in Appendices B.3 and C.3.

First we show in Theorem 3.1 that it is possible to approximate any complex polynomial in \(z\) and \(\overline{z}\) arbitrarily well using shallow CVNNs with the size of the networks only depending on the degree of the polynomial (not on the desired approximation accuracy). Using this result we can prove the main approximation bound, Theorem 3.2, by first approximating a given \(C^{k}\)-function using a polynomial in \(z\) and \(\overline{z}\) and then approximating this polynomial using Theorem 3.1.

For \(m,n\in\mathbb{N}\) let

\[\mathcal{P}_{m}^{n}:=\left\{\mathbb{C}^{n}\to\mathbb{C},\;z\mapsto\sum_{ \mathbf{m}\leq m}\;\sum_{\boldsymbol{\ell}\leq m}a_{\mathbf{m},\boldsymbol{ \ell}}z^{\mathbf{m}}\overline{z}^{\boldsymbol{\ell}}:\;a_{\mathbf{m}, \boldsymbol{\ell}}\in\mathbb{C}\right\}\]

Figure 2: Absolute value of the activation functions \(\sigma_{\mathrm{modReLU},-1}\) (left) and \(\mathrm{card}\) (right).

denote the space of all complex polynomials on \(\mathbb{C}^{n}\) in \(z\) and \(\overline{z}\) of componentwise degree at most \(m\). Here, we are summing over all multi-indices \(\mathbf{m},\boldsymbol{\ell}\in\mathbb{N}_{0}^{n}\) with \(\mathbf{m}_{j},\boldsymbol{\ell}_{j}\leq m\) for every \(j\in\{1,...,n\}\) and use the notation

\[z^{\mathbf{m}}:=\prod_{j=1}^{n}z_{j}^{\mathbf{m}_{j}}\quad\text{and}\quad \overline{z}^{\boldsymbol{\ell}}:=\prod_{j=1}^{n}\overline{z_{j}}^{ \boldsymbol{\ell}_{j}}.\]

The space \(\mathcal{P}_{m}^{n}\) is finite-dimensional; hence, it makes sense to talk about bounded subsets of \(\mathcal{P}_{m}^{n}\) without specifying a norm.

**Theorem 3.1**.: _Let \(m,n\in\mathbb{N}\), \(\varepsilon>0\) and \(\phi:\mathbb{C}\to\mathbb{C}\) be smooth and non-polyharmonic on an open set \(\varnothing\neq U\subseteq\mathbb{C}\). Let \(\mathcal{P}^{\prime}\subseteq\mathcal{P}_{m}^{n}\) be bounded and set \(N:=(4m+1)^{2n}\). Then there exists a first layer \(\Phi\in\mathcal{F}_{n,N}^{\phi}\) with the following property: For each polynomial \(p\in\mathcal{P}^{\prime}\) there exists \(\sigma\in\mathbb{C}^{N}\), such that_

\[\left\|p-\sigma^{T}\Phi\right\|_{L^{\infty}(\Omega_{n};\mathbb{C})}\leq\varepsilon.\]

Sketch of proof.: For any multi-indices \(\mathbf{m},\boldsymbol{\ell}\in\mathbb{N}_{0}^{n}\) an inductive argument shows for every fixed \(z\in\Omega_{n}\) and \(b\in\mathbb{C}\) that

\[\partial_{\mathrm{wirt}}^{\boldsymbol{\mathsf{m}}}\overline{\partial}_{ \mathrm{wirt}}^{\boldsymbol{\mathsf{\ell}}}\left[w\mapsto\phi(w^{T}z+b)\right] =z^{\mathbf{m}}\overline{z}^{\boldsymbol{\ell}}\cdot\left(\partial_{ \mathrm{wirt}}^{|\mathbf{m}|}\overline{\partial}_{\mathrm{wirt}}^{| \boldsymbol{\mathsf{\ell}}|}\phi\right)(w^{T}z+b).\]

Here, \(\partial_{\mathrm{wirt}}^{\mathbf{m}}\) and \(\overline{\partial}_{\mathrm{wirt}}^{\boldsymbol{\mathsf{\ell}}}\) denote the multivariate Wirtinger derivatives with respect to \(w\) according to the multi-indices \(\mathbf{m}\) and \(\boldsymbol{\ell}\), respectively. Evaluating this at \(w=0\) and taking \(b\in\mathbb{C}\) such that none of the mixed Wirtinger derivatives of \(\phi\) at \(b\) up to a sufficiently high order vanish (where such a \(b\) exists by Proposition 2.1) shows that we can rewrite

\[z^{\mathbf{m}}\overline{z}^{\boldsymbol{\ell}}=\left(\partial_{\mathrm{wirt}} ^{\boldsymbol{\mathsf{m}}}\overline{\partial}_{\mathrm{wirt}}^{\boldsymbol{ \mathsf{\ell}}}\left[w\mapsto\phi(w^{T}z+b)\right]\right)\Big{|}_{w=0}\cdot \left(\left(\partial_{\mathrm{wirt}}^{|\mathbf{m}|}\overline{\partial}_{ \mathrm{wirt}}^{|\boldsymbol{\mathsf{\ell}}|}\phi\right)(b)\right)^{-1}.\] (3.1)

The mixed Wirtinger derivatives can by definition be expressed as linear combinations of usual partial derivatives. Those partial derivatives can be approximated using a generalized version of _divided differences_: If \(g\in C^{k}((-r,r)^{s};\mathbb{R})\) and \(\mathbf{p}\in\mathbb{N}_{0}^{s}\) with \(|\mathbf{p}|\leq k\) we have

\[\partial^{\mathbf{p}}g(0)\approx(2h)^{-|\mathbf{p}|}\sum_{0\leq r\leq \mathbf{p}}(-1)^{|\mathbf{p}|-|\mathbf{r}|}\binom{\mathbf{p}}{\mathbf{r}}g \left(h(2\mathbf{r}-\mathbf{p})\right)\quad\text{for $h\searrow 0$}.\] (3.2)

See Appendix B.1 for a proof of this approximation. Note that when one takes \(g(w)=\phi(w^{T}z+b)\), the right-hand side of (3.2) is a shallow neural network, as a function of \(z\).

Combining (3.1) and (3.2) yields the desired result; see Appendix B.3 for the details. 

It is crucial that the size of the networks considered in Theorem 3.1 is independent of the approximation accuracy \(\varepsilon\). Moreover, the first layer \(\Phi\) can be chosen independently of the particular polynomial \(p\). Only the weights \(\sigma\) connecting hidden layer and output neuron have to be adapted to \(p\).

The final approximation result is as follows. Its full proof can be found in Appendix C.3.

**Theorem 3.2**.: _Let \(n,k\in\mathbb{N}\). Then there exists a constant \(c=c(n,k)>0\) with the following property: For any activation function \(\phi:\mathbb{C}\to\mathbb{C}\) that is smooth and non-polyharmonic on an open set \(\varnothing\neq U\subseteq\mathbb{C}\) and for any \(m\in\mathbb{N}\) there exists a first layer \(\Phi\in\mathcal{F}_{n,m}^{\phi}\) with the following property: For any \(f\in C^{k}\left(\Omega_{n};\mathbb{C}\right)\) there exist coefficients \(\sigma=\sigma(f)\in\mathbb{C}^{m}\), such that_

\[\left\|f-\sigma^{T}\Phi\right\|_{L^{\infty}(\Omega_{n};\mathbb{C})}\leq c\cdot m ^{-k/(2n)}\cdot\left\|f\right\|_{C^{k}(\Omega_{n};\mathbb{C})}.\]

_Furthermore, the map \(f\mapsto\sigma(f)\) is a continuous linear operator with respect to the \(L^{\infty}\)-norm._

Sketch of proof.: By splitting \(f\) into real and imaginary part we may assume that \(f\) is real-valued. Fourier-analytical results (recalled in Appendix C.1) imply that each \(C^{k}\)-function \(f\) can be well approximated by a linear combination of (multivariate) _Chebyshev polynomials_. Precisely, we have

\[\left\|f-P\right\|_{L^{\infty}(\Omega_{n};\mathbb{R})}\leq\frac{c}{m^{k}}\cdot \left\|f\right\|_{C^{k}(\Omega_{n};\mathbb{R})},\]where \(P\) is given via the formula

\[P(z)=\sum_{0\leq\mathbf{k}\leq 2m-1}\mathcal{V}_{\mathbf{k}}^{m}(f)\cdot T_{ \mathbf{k}}(z),\quad z\in\Omega_{n}.\]

Here, the functions \(T_{\mathbf{k}}\) are multivariate versions of Chebyshev polynomials and \(\mathcal{V}_{\mathbf{k}}^{m}(f)\) are continuous linear functionals in \(f\). The constant \(c>0\) only depends on \(n\) and \(k\). See Appendix C.1 for a rigorous proof of this approximation property. Approximating the polynomials \(T_{\mathbf{k}}\) by neural networks according to Theorem 3.1 yields the desired claim; see Appendix C.3 for the details. 

_Remark 3.3_.: Theorem 3.1 can not only be used to derive approximation rates for the approximation of \(C^{k}\)-functions but can be applied to any function class that is well approximable by algebraic polynomials. For example, it can be used to prove an order of approximation of \(\nu^{-m^{1/(2n)}/17}\) for the approximation of functions \(f:\Omega_{n}\to\mathbb{C}\) that can be _holomorphically extended_ onto some polyellipse in \(\mathbb{C}^{2n}\). The parameter \(\nu>1\) specifies the size of this polyellipse. We refer the interested reader to Appendix D for detailed definitions, statements and proofs for this fact.

## 4 Optimality of the derived approximation rate

In this section we discuss the optimality of the approximation rate proven in Theorem 3.2. We first deduce from general results by DeVore et al. [19] that the rate is optimal in the setting that the map which assigns to a function \(f\in C^{k}\left(\Omega_{n};\mathbb{C}\right)\) the weights of the approximating network is continuous, as is the case in Theorem 3.2. However, it might be possible to achieve a better degree of approximation if this map is _not_ required to be continuous, which is discussed in the second part of this section. Proofs for all the statements in this section are given in Appendices E.1, F.2 and F.3.

**Continuous weight selection.** We begin by considering the case of continuous weight selection. As mentioned in the introduction, this is a natural assumption, since in classical training algorithms such as (S)GD, continuous operations based on samples \(f(x_{j})\) are used to adjust the weights.

In [19, Theorem 4.2] a lower bound of \(m^{-k/s}\) is established for the rate of approximating functions \(f\) of Sobolev regularity \(W^{k,\infty}\) in the following very general setting: The set of functions that is used for approximation can be parametrized using \(m\) (real) parameters and the map that assigns to \(f\) the parameters of the approximating function is continuous with respect to _some_ norm on \(W^{k,\infty}([-1,1]^{s};\mathbb{R})\). A detailed version of the proof of that statement (for \(C^{k}\) instead of \(W^{k,\infty}\)) is contained in Appendix E.1. A careful transfer of this result to the complex-valued setting yields the following theorem (see also Appendix E.1).

**Theorem 4.1**.: _Let \(n,k\in\mathbb{N}\). Then there exists a constant \(c=c(n,k)>0\) with the following property: For any \(m\in\mathbb{N}\), any map \(\overline{a}:C^{k}(\Omega_{n};\mathbb{C})\to\mathbb{C}^{m}\) that is continuous with respect to some norm on \(C^{k}(\Omega_{n};\mathbb{C})\) and any map \(M:\mathbb{C}^{m}\to C(\Omega_{n};\mathbb{C})\) we have_

\[\sup_{f\in C^{k}(\Omega_{n};\mathbb{C}),\|f\|_{C^{k}(\Omega_{n};\mathbb{C})} \leq 1}\ \|f-M(\overline{a}(f))\|_{L^{\infty}(\Omega_{n};\mathbb{C})}\geq c\cdot m^{-k /(2n)}.\]

The interpretation of Theorem 4.1 is as follows: If one approximates \(C^{k}\)-functions on \(\Omega_{n}\) using a set of functions that can be parametrized using \(m\) (complex) parameters and one assumes that the weight assignment is continuous, one cannot achieve a better rate of approximation than \(m^{-k/(2n)}\). As a special case it can be deduced that the approximation rate is at most \(m^{-k/(2n)}\) when approximating \(C^{k}\)-functions using shallow CVNNs with \(m\) parameters under continuous weight assignment (see Corollary E.3). One can show that this even holds for _deep_ CVNNs. Hence, for continuous weight selection the rate proven in Theorem 3.2 is optimal, even in the class of (possibly) deep networks.

**Discontinuous weight selection.** When we drop the continuity assumption on the selection of the weights, the behavior of the optimal approximation rate is more subtle. Precisely, we show that there are activation functions for which the rate of approximation can be improved to \(m^{-k/(2n-1)}\). On the other hand, we also show that there are activation functions for which an improvement of the approximation rate (up to logarithmic factors) is not possible.

**Theorem 4.2**.: _There exists a function \(\phi\in C^{\infty}(\mathbb{C};\mathbb{C})\) which is non-polyharmonic with the following additional property: For every \(k\in\mathbb{N}\) and \(n\in\mathbb{N}\) there exists a constant \(c=c(n,k)>0\) such that _for any \(m\in\mathbb{N}\) and \(f\in C^{k}\left(\Omega_{n};\mathbb{C}\right)\) there is a first layer \(\Phi\in\mathcal{F}_{n,m}^{\phi}\) and a vector \(\sigma\in\mathbb{C}^{m}\) such that_

\[\left\|f-\sigma^{T}\Phi\right\|_{L^{\infty}\left(\Omega_{n};\mathbb{C}\right)} \leq c\cdot m^{-k/(2n-1)}\cdot\left\|f\right\|_{C^{k}\left(\Omega_{n}; \mathbb{C}\right)}.\]

_Sketch of proof._ The function \(\phi\) is constructed in the following way: Take a countable dense subset \(\left\{u_{\ell}\right\}_{\ell\in\mathbb{N}}\) of \(C(\Omega_{1};\mathbb{C})\), for instance the set of all polynomials in \(z\) and \(\overline{z}\) with coefficients in \(\mathbb{Q}+i\mathbb{Q}\). Define \(\phi\) in a way such that

\[\phi(z+3\ell)=u_{\ell}(z)\]

for every \(z\in\Omega_{1}\) and \(\ell\in\mathbb{N}\). Furthermore, to ensure that \(\phi\) is non-polyharmonic, let \(\phi(z)=e^{\mathrm{Re}(z)}\) for every \(z\in\Omega_{1}\). The smoothness of \(\phi\) can be accomplished by multiplying with a smooth _bump function_; see Lemma F.4 for details. The construction of \(\phi\) is illustrated in Figure 3.

Let then \(f\in C^{k}(\Omega_{n};\mathbb{C})\) be arbitrary. General results from the theory of _ridge functions_[22] show that there exist \(b_{1},...,b_{m}\in\mathbb{C}^{n}\) and \(g_{1},...,g_{m}\in C(\Omega_{1};\mathbb{C})\) such that

\[\left\|f(z)-\sum_{j=1}^{m}g_{j}\left(b_{j}^{T}\cdot z\right)\right\|_{L^{ \infty}\left(\Omega_{n};\mathbb{C}\right)}\leq c\cdot m^{-k/(2n-1)}\cdot\left\| f\right\|_{C^{k}\left(\Omega_{n};\mathbb{C}\right)};\]

see Proposition F.3 and Appendix F.1 for a detailed proof of this fact. Approximating the functions \(g_{j}\) by suitable functions \(u_{\ell_{j}}\) and expressing those functions via \(\phi(\bullet+3\ell_{j})\) yields the claim. 

The preceding theorem showed that there exists an activation function for which the rate in Theorem 3.2 can be strictly improved, if one allows a discontinuous weight selection. In contrast, the following theorem shows for a certain (quite natural) activation function that the rate \(m^{-k/(2n)}\)_cannot_ be improved (up to logarithmic factors), even if one allows a discontinuous weight assignment.

**Theorem 4.3**.: _Let \(n,k\in\mathbb{N}\) and_

\[\phi:\quad\mathbb{C}\rightarrow\mathbb{C},\quad\phi(z):=\frac{1}{1+e^{- \mathrm{Re}(z)}}.\]

_Then \(\phi\) is smooth but non-polyharmonic. Furthermore, there exists a constant \(c=c(n,k)>0\) with the following property: For any \(m\in\mathbb{N}_{\geq 2}\) there exists a function \(f\in C^{k}\left(\Omega_{n};\mathbb{C}\right)\) with \(\left\|f\right\|_{C^{k}\left(\Omega_{n};\mathbb{C}\right)}\leq 1\), such that for every \(\Phi\in\mathcal{F}_{n,m}^{\phi}\) and \(\sigma\in\mathbb{C}^{m}\) we have_

\[\left\|f-\sigma^{T}\Phi\right\|_{L^{\infty}\left(\Omega_{n};\mathbb{C}\right) }\geq c\cdot\left(m\cdot\ln(m)\right)^{-k/(2n)}.\]

Sketch of proof.: The idea of the proof is based on that of [46, Theorem 4] but instead of the bound for the VC dimension of ReLU networks used in [46], we will employ a bound for the VC dimension stated in [4, Theorem 8.11] using the real sigmoid function. For a detailed introduction to the concept of the VC dimension and related topics, see for instance [39, Chapter 6].

A technical reduction from the complex to the real case (see Appendix F.3) shows that it suffices to show the following: If \(\varepsilon\in(0,\frac{1}{2})\) and \(m\in\mathbb{N}\) are such that for every \(f\in C^{k}\left([-1,1]^{n};\mathbb{R}\right)\) with \(\left\|f\right\|_{C^{k}\left([-1,1]^{n};\mathbb{R}\right)}\leq 1\) there exists a real-valued shallow network \(\mathcal{N}\) with \(\gamma(x)=\frac{1}{1+e^{-\varepsilon}}\) as activation function satisfying \(\left\|f-\mathcal{N}\right\|_{L^{\infty}\left([-1,1]^{n};\mathbb{R}\right)}\leq\varepsilon\), then necessarily

\[m\geq c\cdot\frac{\varepsilon^{-n/k}}{\ln\left(1/\varepsilon\right)},\]

Figure 3: Illustration of the construction of the activation function in Theorem 4.2.

where the constant \(c\) only depends on \(n\) and \(k\).

To show that the latter claim holds, we assume that \(\varepsilon\) and \(m\) have the property stated above. Take \(N\in\mathbb{N}\) such that \(N^{-k}\asymp\varepsilon\) and consider the grid

\[\mathcal{G}:=\frac{1}{N}\{-N,...,N\}^{n}\subseteq[-1,1]^{n}.\]

For every \(\alpha\in\mathcal{G}\) we pick a number \(z_{\alpha}\in\{0,1\}\) arbitrarily and construct a map \(f\in C^{\infty}([-1,1]^{n};\mathbb{R})\) satisfying \(f(\alpha)=z_{\alpha}\) for very \(\alpha\in\mathcal{G}\). Scaling of \(f\) to \(\tilde{f}\) ensures \(\|\tilde{f}\|_{C^{k}([-1,1]^{n};\mathbb{R})}\leq 1\), but then \(\tilde{f}(\alpha)=c_{0}\cdot z_{\alpha}\cdot N^{-k}\) where \(c_{0}=c_{0}(n,k)>0\). By assumption, we can infer the existence of a shallow real-valued neural network \(\mathcal{N}\) with \(\gamma\) as activation function and \(m\) hidden neurons satisfying \(\|\tilde{f}-\mathcal{N}\|_{L^{\infty}([-1,1]^{n};\mathbb{R})}\leq\varepsilon\). But this shows

\[\mathcal{N}(\alpha)\begin{cases}>\tilde{c}N^{-k},&\text{if }z_{\alpha}=1,\\ <\tilde{c}N^{-k},&\text{if }z_{\alpha}=0\end{cases}\quad\text{for all }\alpha\in \mathcal{G}\]

with a constant \(\tilde{c}=\tilde{c}(n,k)>0\). Since the \(z_{\alpha}\) are arbitrary, it follows that the set

\[H:=\Big{\{}\mathbbm{1}(\mathcal{N}>\tilde{c}N^{-k})\big{|}_{\mathcal{G}}:\; \mathcal{N}\text{ shallow NN with activation }\gamma\text{ and }m\text{ hidden neurons}\Big{\}}\]

_shatters_ the whole grid \(\mathcal{G}\). This yields \(\operatorname{VC}(H)\geq|\mathcal{G}|=(2N+1)^{n}\). On the other hand, the bound provided by [4, Theorem 8.11] for linear threshold networks yields \(\operatorname{VC}(H)\lesssim m\cdot\ln(N)\). Combining the two bounds and using \(N^{-k}\asymp\varepsilon\) yields the claim. 

## 5 Tractability of the considered problem in terms of the input dimension

In this section we discuss the _tractability_ (in terms of the input dimension \(n\)) of the considered problem, i.e., the dependence of the approximation error on \(n\). We show a novel result stating that, assuming a continuous weight selection, the problem of approximating \(C^{k}\)-functions is _intractable_, i.e., that the number of neurons that is required to achieve a non-trivial approximation error is necessarily exponential in \(n\). In the literature this is referred to as the _curse of dimensionality_. The proof of the theorem combines ideas from [19] and [35] and is contained in Appendix E.2.

**Theorem 5.1**.: _Let \(s\in\mathbb{N}\). With \(\|\cdot\|_{C^{k}([-1,1]^{s};\mathbb{R})}\) defined similarly to (2.1), we write_

\[\|f\|_{C^{\infty}([-1,1]^{s};\mathbb{R})}:=\sup_{k\in\mathbb{N}}\|f\|_{C^{k}([ -1,1]^{s};\mathbb{R})}\in[0,\infty]\]

_for any function \(f\in C^{\infty}([-1,1]^{s};\mathbb{R})\) and denote by \(C^{\infty,*,s}\) the set of all \(f\in C^{\infty}([-1,1]^{s};\mathbb{R})\) for which this expression is finite. Let \(\overline{a}:C^{\infty,*,s}\to\mathbb{R}^{2^{2}-1}\) be continuous with respect to some norm on \(C^{\infty,*,s}\) and moreover, let \(M:\mathbb{R}^{2^{2}-1}\to C([-1,1]^{s};\mathbb{R})\) be an arbitrary map. Then it holds_

\[\sup_{\begin{subarray}{c}f\in C^{\infty},*,*\\ \|f\|_{C^{\infty}([-1,1]^{s};\mathbb{R})}\leq 1\end{subarray}}\|f-M( \overline{a}(f))\|_{L^{\infty}([-1,1]^{s};\mathbb{R})}\geq 1.\]

Note that Theorem 5.1 is formulated for real-valued functions but can be transferred to the complex-valued setting (see Corollary E.6). We decided to include the real-valued statement because it is expected to be of greater interest in the community than the complex-valued analog. Moreover, we stress that Theorem 5.1 is not limited to the class of shallow neural networks but refers to any function class that is parametrizable using finitely many parameters (in particular, e.g., the class of neural networks with possibly more than one hidden layer).

We now examine in what way the constant \(c\) appearing in Theorem 3.2 suffers from the curse of dimensionality. To this end, it is convenient to rewrite the result from Theorem 3.2 as

\[\sup_{\|f\|_{C^{k}(n_{i};\mathbb{C})}\leq 1}\inf_{\Phi\in\mathcal{F}^{0}_{n,m},\sigma\in\mathbb{C}^{m}}\|f-\sigma^{T}\Phi\|_{L^{\infty}(\Omega_{n};\mathbb{ C})}\leq\left(\tilde{c}\cdot m\right)^{-k/(2n)}\]

where the constant \(\tilde{c}=\tilde{c}(n,k)>0\) is independent of \(m\). Writing the result in that way, one sees immediately that, if one seeks to have a worst-case approximation error of less than \(\varepsilon>0\), it is sufficient to take \(m=\left\lceil\frac{1}{\tilde{c}}\cdot\varepsilon^{-(2n)/k}\right\rceil\) neurons in the hidden layer of the network. Corollary E.7 shows that it _necessarily_ holds \(\tilde{c}\leq 16\cdot 2^{-n}\) and therefore, the constant \(\tilde{c}\) unavoidably suffers from the curse of dimensionality. An analysis of the constant (where we refer to Appendices C.1 to C.3 for the details) shows that in our case we can establish the bound \(\tilde{c}(n,k)\geq\exp(-C\cdot n^{2})\cdot k^{-4n}\) with an _absolute_ constant \(C>0\). We remark that, since the constant suffers from the curse of dimensionality in any case, we have not put much effort into optimizing the constant; there is therefore probably ample room for improvement.

## 6 Limitations

To conduct a comprehensive evaluation of machine learning algorithms, one must analyze the questions of approximation, generalization, and optimization through training algorithms. The present paper, however, only focuses on the aspect of approximation. Analyzing if the proven approximation rate can be attained with learning algorithms such as (stochastic) gradient descent falls outside the scope of this paper. Furthermore, the examination of approximation rates under possibly discontinuous weight assignment is not yet fully resolved by our results. It is an open question which rate is optimally achievable in that case, depending on the choice of the activation function, and specifically in distinguishing between shallow and deep networks. We want to mention the two following points which indicate that this is a quite subtle question:

1. For deep NNs (with more than two hidden layers) with general smooth activation function, it is _not possible_ to derive any non-trivial lower bounds in the setting of unrestricted weight assignment, since there exists an activation function with the property that NNs of _constant size_ using this activation function can approximate any continuous function to arbitrary precision (see [31, Theorem 4]). Note that [31] considers real-valued NNs, but the results can be transferred to CVNNs with a suitable choice of the activation function.
2. In the real-valued case, fully general lower bounds for the approximation capabilities of shallow NNs have been derived by using results from [22] regarding the approximation properties of so-called ridge functions, i.e., functions of the form \(\sum_{j=1}^{m}\phi_{j}\langle\langle a_{j},x\rangle\rangle\) with \(a_{j}\in\mathbb{R}^{d}\) and each \(\phi_{j}:\mathbb{R}\to\mathbb{R}\). It is an interesting problem to generalize these results to higher-dimensional ridge functions of the form \(\sum_{j=1}^{m}\phi_{j}(A_{j}x)\), where each \(\phi_{j}:\mathbb{R}^{s}\to\mathbb{R}\) and \(A_{j}\in\mathbb{R}^{s\times d}\). This would imply lower bounds for shallow CVNNs. However, such a generalization seems to be highly non-trivial and is outside the scope of the present paper.

## 7 Conclusion

This paper analyzes error bounds for the approximation of complex-valued \(C^{k}\)-functions by means of complex-valued neural networks with smooth and non-polyharmonic activation functions. It is demonstrated that complex-valued neural networks with these activation functions achieve the _identical_ approximation rate as real-valued networks that employ smooth and non-polynomial activation functions. This is an important theoretical finding, since CVNNs are on the one hand more restrictive than real-valued neural networks (since the mappings between layers should be \(\mathbb{C}\)-linear and not just \(\mathbb{R}\)-linear), but on the other hand more versatile, since the activation function is a mapping from \(\mathbb{C}\) to \(\mathbb{C}\) (i.e., from \(\mathbb{R}^{2}\to\mathbb{R}^{2}\)) rather than from \(\mathbb{R}\) to \(\mathbb{R}\) as in the real case. Additionally, it is established that the proven approximation rate is optimal if one assumes a continuous weight selection. In summary, if one focuses on the approximation rate for \(C^{k}\)-functions, CVNNs have the same excellent approximation properties as real-valued networks.

The behavior of the approximation rate for unrestricted weight selection is more subtle. It is shown that a rate of \(m^{-k/(2n-1)}\) can be achieved for certain activation functions (Theorem 4.2) but in general, one cannot improve on the rate that is attainable for continuous weight selection (Theorem 4.3).

While the proven approximation _rate_ is optimal under the assumption of continuous weight selection, the involved constants suffer from the curse of dimensionality. Section 5, however, shows that this is inevitable in the given setting.

Such theoretical approximation results contribute to the mathematical understanding of Deep Learning. The remarkable approximation-theoretical properties of neural networks can be seen as one reason why neural networks provide outstanding results in many applications.

**Acknowledgments.** PG and FV acknowledge support by the German Science Foundation (DFG) in the context of the Emmy Noether junior research group VO 2594/1-1. FV acknowledges support by the Hightech Agenda Bavaria.

## References

* [1] M. Abramowitz and I. A. Stegun, eds. _Handbook of mathematical functions: with formulas, graphs, and mathematical tables_. 9. Dover print. Dover books on mathematics. New York, NY: Dover Publ, 2013. isbn: 978-0-486-61272-0.
* [2] B. Adcock, S. Brugiapaglia, and C. G. Webster. _Sparse polynomial approximation of high-dimensional functions_. Computational science and engineering 25. Philadelphia: Society for Industrial and Applied Mathematics, 2022. 292 pp. isbn: 978-1-61197-687-8.
* [3] C. D. Aliprantis and K. C. Border. _Infinite dimensional analysis: a hitchhiker's guide_. 3rd ed. Berlin ; New York: Springer, 2006. isbn: 978-3-540-29586-0.
* [4] M. Anthony and P. L. Bartlett. _Neural network learning: theoretical foundations_. Cambridge ; New York, NY: Cambridge University Press, 1999. isbn: 978-0-521-57353-5.
* [5] M. Arjovsky, A. Shah, and Y. Bengio. "Unitary evolution recurrent neural networks". In: _International conference on machine learning_. PMLR. 2016, pp. 1120-1128. url: http://proceedings.mlr.press/v48/arjovsky16.pdf.
* [6] K. E. Atkinson. _An introduction to numerical analysis_. 2nd ed. New York: Wiley, 1989. isbn: 978-0-471-62489-9.
* [7] M. B. Balk. _Polyanalytic functions_. Mathematical research 63. Berlin: Akademie Verlag, 1991. isbn: 978-3-05-501292-1.
* [8] J. A. Barrachina, C. Ren, C. Morisseau, G. Vieillard, and J.-P. Ovarlez. "Complex-valued vs. real-valued neural networks for classification perspectives: An example on non-circular data". In: _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE. 2021, pp. 2990-2994.
* Application on polarimetric SAR image segmentation". In: _Journal of Signal Processing Systems_ 95.1 (2023), pp. 57-66.
* [10] A. R. Barron. "Universal approximation bounds for superpositions of a sigmoidal function". In: _IEEE Transactions on Information theory_ 39.3 (1993), pp. 930-945. doi: 10.1109/18.256500.
* [11] H. Bauer. _Measure and integration theory_. De Gruyter studies in mathematics 26. Berlin ; New York: W. de Gruyter, 2001. 230 pp. isbn: 978-3-11-016719-1.
* [12] A. Benjamin and J. J. Quinn. _Proofs that really count: the art of combinatorial proof_. Dolciani mathematical expositions. Washington, DC: Mathematical Association of America, 2003. isbn: 978-0-88385-333-7.
* [13] D. Berend and T. Tassa. "Improved bounds on Bell numbers and on moments of sums of random variables". In: _Probability and Mathematical Statistics_ 30.2 (2010), pp. 185-205.
* [14] A. Caragea, D. G. Lee, J. Maly, G. Pfander, and F. Voigtlaender. "Quantitative Approximation Results for Complex-Valued Neural Networks". In: _SIAM Journal on Mathematics of Data Science_ 4.2 (2022), pp. 553-580. doi: 10.1137/21M1429540.
* [15] H. H. Cevik, Y. E. Acar, and M. Cunkas. "Day Ahead Wind Power Forecasting Using Complex Valued Neural Network". In: _2018 International Conference on Smart Energy Systems and Technologies (SEST)_. 2018, pp. 1-6. doi: 10.1109/SEST.2018.8495637.
* [16] E. Cole, J. Cheng, J. Pauly, and S. Vasanawala. "Analysis of deep complex-valued convolutional neural networks for MRI reconstruction and phase-focused applications". In: _Magnetic resonance in medicine_ 86.2 (2021), pp. 1093-1109.
* [17] G. Cybenko. "Approximation by superpositions of a sigmoidal function". In: _Mathematics of Control, Signals, and Systems_ 2.4 (Dec. 1989), pp. 303-314. doi: 10.1007/BF02551274.
* [18] K. Deimling. _Nonlinear Functional Analysis_. Dover books on mathematics. Dover Publications, 2013. isbn: 978-3-662-00549-1.
* [19] R. A. DeVore, R. Howard, and C. Micchelli. "Optimal nonlinear approximation". In: _Manuscripta Mathematica_ 63.4 (Dec. 1989), pp. 469-478. doi: 10.1007/BF01171759.
* [20] W. F. Donoghue. _Distributions and Fourier transforms_. OCLC: 45151. New York: Academic Press, 1969. isbn: 978-0-12-220650-4.
* [21] G. B. Folland. _Real analysis: modern techniques and their applications_. 2nd ed. Pure and applied mathematics. New York: Wiley, 1999. isbn: 978-0-471-31716-6.
* [22] Y. Gordon, V. Maiorov, M. Meyer, and S. Reisner. "On the Best Approximation by Ridge Functions in the Uniform Norm". In: _Constructive Approximation_ 18.1 (Jan. 1, 2001), pp. 61-85. doi: 10.1007/s00365-001-0009-5.

* [23] A. Graves, A.-R. Mohamed, and G. Hinton. "Speech recognition with deep recurrent neural networks". In: _2013 IEEE International Conference on Acoustics, Speech and Signal Processing_. 2013, pp. 6645-6649. doi: 10.1109/ICASSP.2013.6638947.
* [24] W. P. Johnson. "The Curious History of Faa di Bruno's Formula". In: _The American Mathematical Monthly_ 109.3 (Mar. 2002), p. 217. doi: 10.2307/2695352.
* [25] P. C. Kainen, V. Korkova, and A. Vogt. "Approximation by neural networks is not continuous". In: _Neurocomputing_ 29.1 (1999), pp. 47-56. doi: https://doi.org/10.1016/S0925-2312(99)00111-3.
* [26] L. Kaup and B. Kaup. _Holomorphic Functions of Several Variables: An Introduction to the Fundamental Theory_. De Gruyter, Dec. 1983. isbn: 978-3-11-004150-7. doi: 10.1515/9783110838350.
* [27] A. Krizhevsky, I. Sutskever, and G. E. Hinton. "ImageNet classification with deep convolutional neural networks". In: _Communications of the ACM_ 60.6 (May 2017), pp. 84-90. doi: 10.1145/3065386.
* [28] M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken. "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function". In: _Neural Networks_ 6.6 (Jan. 1993), pp. 861-867. doi: 10.1016/S0893-6080(05)80131-5.
* [29] G. Lorentz. _Approximation of Functions_. Approximation of Functions. Holt, Rinehart and Winston, 1966.
* [30] V. Maiorov. "On Best Approximation by Ridge Functions". In: _Journal of Approximation Theory_ 99.1 (July 1999), pp. 68-94. doi: 10.1006/jath.1998.3304.
* [31] V. Maiorov and A. Pinkus. "Lower bounds for approximation by MLP neural networks". In: _Neurocomputing_ 25.1 (1999), pp. 81-91. doi: https://doi.org/10.1016/S0925-2312(98)00111-8.
* [32] A. Marseet and F. Sahin. "Application of complex-valued convolutional neural network for next generation wireless networks". In: _2017 IEEE Western New York Image and Signal Processing Workshop (WNYISPW)_. 2017, pp. 1-5. doi: 10.1109/WNYIPW.2017.8356260.
* [33] H. N. Mhaskar. "Neural Networks for Optimal Approximation of Smooth and Analytic Functions". In: _Neural Computation_ 8.1 (Jan. 1996), pp. 164-177. doi: 10.1162/neco.1996.8.1.164.
* [34] C. Muscalu and W. Schlag. _Classical and multilinear harmonic analysis. Volume 1_. OCLC: 887742576. Cambridge, UK: Cambridge University Press, 2013. isbn: 978-0-521-88245-3.
* [35] E. Novak and H. Wozniakowski. "Approximation of infinitely differentiable multivariate functions is intractable". In: _Journal of Complexity_ 25.4 (2009), pp. 398-404. doi: https://doi.org/10.1016/j.jco.2008.11.002.
* [36] A. Pinkus. _Ridge functions_. Cambridge: Cambridge University Press, 2016. isbn: 978-1-107-12439-4.
* [37] Y.-D. Qu, R.-Q. Zhang, S.-Q. Shen, J. Yu, and M. Li. "Entanglement Detection with Complex-Valued Neural Networks". In: _International Journal of Theoretical Physics_ 62.9 (2023), p. 206.
* [38] H. El-Rewaidy, U. Neisius, J. Mancio, S. Kucukesymen, J. Rodriguez, A. Paskavitz, B. Menze, and R. Nezafat. "Deep complex convolutional network for fast reconstruction of 3D late gadolinium enhancement cardiac MRI". In: _NMR in Biomedicine_ 33.7 (2020), e4312.
* [39] S. Shalev-Shwartz and S. Ben-David. _Understanding machine learning: from theory to algorithms_. New York, NY, USA: Cambridge University Press, 2014. 397 pp. ISBN: 978-1-107-05713-5.
* [40] Z. Shen, H. Yang, and S. Zhang. "Deep Network Approximation Characterized by Number of Neurons". In: _Communications in Computational Physics_ 28.5 (2020), pp. 1768-1811. doi: https://doi.org/10.4208/cicp.0A-2020-0149.
* [41] J. W. Siegel and J. Xu. "Approximation rates for neural networks with general activation functions". In: _Neural Networks_ 128 (2020), pp. 313-321. doi: https://doi.org/10.1016/j.neunet.2020.05.019.
* [42] P. Stein. "A Note on the Volume of a Simplex". In: _The American Mathematical Monthly_ 73.3 (Mar. 1966), p. 299. doi: 10.2307/2315353.
* [43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. "Attention is All you Need". In: _Advances in Neural Information Processing Systems_. Ed. by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc., 2017. URL: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
* [44] P. Virtue, S. X. Yu, and M. Lustig. "Better than real: Complex-valued neural nets for MRI fingerprinting". In: _2017 IEEE International Conference on Image Processing (ICIP)_. Beijing: IEEE, Sept. 2017, pp. 3953-3957. isbn: 978-1-5090-2175-8. doi: 10.1109/ICIP.2017.8297024.
* [45] F. Voigtlaender. "The universal approximation theorem for complex-valued neural networks". In: _Applied and Computational Harmonic Analysis_ (Dec. 2022), S1063520322001014. doi: 10.1016/j.acha.2022.12.002.
* [46] D. Yarotsky. "Error bounds for approximations with deep ReLU networks". In: _Neural Networks_ 94 (Oct. 2017), pp. 103-114. doi: 10.1016/j.neunet.2017.07.002.

* [47] D. Yarotsky and A. Zhevnerchuk. "The phase diagram of approximation rates for deep neural networks". In: _Advances in neural information processing systems_ 33 (2020), pp. 13005-13015. url: https://proceedings.neurips.cc/paper/2020/file/979a3f14bae523dc5101c52120c53ce9-Paper.pdf.
* [48] Z. Zhang, H. Wang, F. Xu, and Y.-Q. Jin. "Complex-Valued Convolutional Neural Network and Its Application in Polarimetric SAR Image Classification". In: _IEEE Transactions on Geoscience and Remote Sensing_ 55.12 (2017), pp. 7177-7188. doi: 10.1109/TGRS.2017.2743222.

Notation, Wirtinger derivatives and special activation functions

### Notation and Wirtinger derivatives

Throughout the paper, multi-indices (i.e., elements of \(\mathbb{N}_{0}^{n}\)) are denoted using boldface. For \(\textbf{m}\in\mathbb{N}_{0}^{n}\) we have the usual notation \(|\textbf{m}|=\sum_{j=1}^{n}\textbf{m}_{j}\). For a number \(m\in\mathbb{N}_{0}\) and another multi-index \(\textbf{p}\in\mathbb{N}_{0}^{n}\) we write \(\textbf{m}\leq m\) iff \(\textbf{m}_{j}\leq m\) for every \(j\in\{1,...,n\}\) and \(\textbf{m}\leq\textbf{p}\) iff \(\textbf{m}_{j}\leq\textbf{p}_{j}\) for every \(j\in\{1,...,n\}\). Furthermore we write

\[\begin{pmatrix}\textbf{p}\\ \textbf{r}\end{pmatrix}:=\prod_{j=1}^{n}\begin{pmatrix}\textbf{p}_{j}\\ \textbf{r}_{j}\end{pmatrix}\]

for two multi-indices \(\textbf{p},\textbf{r}\in\mathbb{N}_{0}^{n}\) with \(\textbf{r}\leq\textbf{p}\). For a complex vector \(z\in\mathbb{C}^{n}\) we write

\[z^{\textbf{m}}:=\prod_{j=1}^{n}z_{j}^{\textbf{m}_{j}}\quad\text{and}\quad \bar{z}^{\textbf{m}}:=\prod_{j=1}^{n}\overline{z_{j}}^{\textbf{m}_{j}}.\]

For a point \(x\in\mathbb{R}^{n}\) and \(r>0\) we define

\[B_{r}(x):=\{y\in\mathbb{R}^{n}:\;\|x-y\|<r\}\]

with \(\|\cdot\|\) denoting the usual Euclidean distance. This definition is analogously transferred to \(\mathbb{C}^{n}\).

\(\mathbb{C}^{n}\) is canonically isomorphic to \(\mathbb{R}^{2n}\) by virtue of the isomorphism

\[\varphi_{n}:\quad\mathbb{R}^{2n}\to\mathbb{C}^{n},\quad(x_{1},...,x_{n},y_{1},...,y_{n})\mapsto(x_{1}+iy_{1},...,x_{n}+iy_{n})\,.\] (A.1)

The Wirtinger derivatives defined in Section 2 have the following properties that we are going to use, which can be found for example in [26, E.1a]. Here, we assume that \(U\subseteq\mathbb{C}\) is open and \(f\in C^{1}(U;\mathbb{C})\).

1. \(\partial_{\text{wirt}}\) and \(\overline{\partial}_{\text{wirt}}\) are both \(\mathbb{C}\)-linear operators on the set \(C^{1}(U;\mathbb{C})\).
2. \(f\) is complex-differentiable in \(z\in U\) iff \(\overline{\partial}_{\text{wirt}}f(z)=0\) and in this case the equality \[\partial_{\text{wirt}}f(z)=f^{\prime}(z)\] holds true, with \(f^{\prime}(z)\) denoting the complex derivative of \(f\) at \(z\).
3. We have the conjugation rules \[\overline{\partial_{\text{wirt}}f}=\overline{\partial}_{\text{wirt}}\overline{ f}\quad\text{and}\quad\overline{\overline{\partial}_{\text{wirt}}f}=\partial_{ \text{wirt}}\overline{f}.\]
4. If \(g\in C^{1}(U;\mathbb{C})\), the following product rules for Wirtinger derivatives hold for every \(z\in U\): \[\partial_{\text{wirt}}(f\cdot g)(z) =\partial_{\text{wirt}}f(z)\cdot g(z)+f(z)\cdot\partial_{\text{ wirt}}g(z),\] \[\overline{\partial}_{\text{wirt}}(f\cdot g)(z) =\overline{\partial}_{\text{wirt}}f(z)\cdot g(z)+f(z)\cdot \overline{\partial}_{\text{wirt}}g(z).\] This product rule is not explicitly stated in [26] but follows easily from the product rule for \(\frac{\partial}{\partial x}\) and \(\frac{\partial}{\partial y}\).
5. If \(V\subseteq\mathbb{C}\) is an open set and \(g\in C^{1}(V;\mathbb{C})\) with \(g(V)\subseteq U\), then the following chain rules for Wirtinger derivatives hold true: \[\partial_{\text{wirt}}(f\circ g) =[(\partial_{\text{wirt}}f)\circ g]\cdot\partial_{\text{wirt}}g+ \big{[}(\overline{\partial}_{\text{wirt}}f)\circ g\big{]}\cdot\partial_{ \text{wirt}}\overline{g},\] \[\overline{\partial}_{\text{wirt}}(f\circ g) =[(\partial_{\text{wirt}}f)\circ g]\cdot\overline{\partial}_{ \text{wirt}}g+\big{[}(\overline{\partial}_{\text{wirt}}f)\circ g\big{]}\cdot \overline{\partial}_{\text{wirt}}\overline{g}.\]
6. If \(f\in C^{2}(U;\mathbb{C})\) then we have \[\Delta f(z)=4\left(\partial_{\text{wirt}}\overline{\partial}_{\text{wirt}}f \right)(z)\] for every \(z\in U\), with \(\Delta\) denoting the usual Laplace-Operator \(\Delta=\partial^{(2,0)}+\partial^{(0,2)}\) (cf. [7, equation (1.7)]).

For an open set \(U\subseteq\mathbb{C}^{n}\), a function \(f\in C^{k}(U;\mathbb{C})\) and a multi-index \(\boldsymbol{\ell}\in\mathbb{N}_{0}^{n}\) with \(|\boldsymbol{\ell}|\leq k\) we write \(\partial_{\text{wirt}}^{\boldsymbol{\ell}}f\) and \(\overline{\partial}_{\text{wirt}}^{\boldsymbol{\ell}}f\) for the iterated Wirtinger derivatives according to the multi-index \(\boldsymbol{\ell}\).

**Proposition A.1**.: _Let \(U\subseteq\mathbb{C}^{n}\) be an open set and \(f\in C^{k}(U;\mathbb{C})\). Then, identifying \(f\) with the function \(f\circ\varphi_{n}\) with \(\varphi_{n}\) as in (A.1), for any multi-indices \(\bm{m},\bm{\ell}\in\mathbb{N}_{0}^{n}\) with \(|\bm{m}+\bm{\ell}|\leq k\) we have the representation_

\[\partial_{\mathrm{wirt}}^{\bm{m}}\overline{\partial}_{\mathrm{wirt}}^{\bm{ \ell}}\,f(a)=\sum_{\begin{subarray}{c}\bm{p}=(\bm{p}^{\prime},\bm{p}^{\prime \prime})\in\mathbb{N}_{0}^{2n}\\ \bm{p}^{\prime}+\bm{p}^{\prime\prime}=\bm{m}+\bm{\ell}\end{subarray}}b_{\bm{p} }\left(\partial^{\bm{p}}f\right)(a)\quad\forall a\in U\]

_with complex numbers \(b_{\bm{p}}\in\mathbb{C}\) only depending on \(\bm{p},\bm{m}\) and \(\bm{\ell}\) and writing \(\bm{p}=(\bm{p}^{\prime},\bm{p}^{\prime\prime})\) for the concatenation of the multi-indices \(\bm{p}^{\prime}\) and \(\bm{p}^{\prime\prime}\in\mathbb{N}_{0}^{n}\). In particular, the coefficients do not depend on \(f\)._

Proof.: The proof is by induction over \(\bm{\mathrm{m}}\) and \(\bm{\ell}\). We first assume \(\bm{\mathrm{m}}=0\) and show the claim for all \(\bm{\ell}\in\mathbb{N}_{0}^{n}\) with \(|\bm{\ell}|\leq k\). In the case \(\bm{\ell}=0\) there is nothing to show, so we assume the claim to be true for fixed \(\bm{\ell}\in\mathbb{N}_{0}^{n}\) with \(|\bm{\ell}|<k\) and take \(j\in\{1,...,n\}\) arbitrarily. Then we get

\[\overline{\partial}_{\mathrm{wirt}}^{\bm{\ell}+e_{j}}f(a) =\overline{\partial}_{\mathrm{wirt}}^{e_{j}}\overline{\partial}_ {\mathrm{wirt}}^{\bm{\ell}}f(a)\overset{\mathrm{IH}}{=}\sum_{\begin{subarray} {c}\bm{p}=(\bm{p}^{\prime},\bm{p}^{\prime\prime})\in\mathbb{N}_{0}^{2n}\\ \bm{p}^{\prime}+\bm{p}^{\prime\prime}=\bm{\ell}\end{subarray}}b_{\bm{p}} \overline{\partial}_{\mathrm{wirt}}^{e_{j}}\left(\partial^{\bm{p}}f\right)(a)\] \[=\sum_{\begin{subarray}{c}\bm{p}=(\bm{p}^{\prime},\bm{p}^{\prime \prime})\in\mathbb{N}_{0}^{2n}\\ \bm{p}^{\prime}+\bm{p}^{\prime\prime}=\bm{\ell}\end{subarray}}\frac{b_{\bm{p} }}{2}\left(\partial^{(\bm{p}^{\prime}+e_{j},\bm{p}^{\prime\prime})}f\right)(a)+ \frac{ib_{\bm{p}}}{2}\left(\partial^{(\bm{p}^{\prime},\bm{p}^{\prime\prime}+ e_{j})}f\right)(a)\] \[=:\sum_{\begin{subarray}{c}\bm{p}=(\bm{p}^{\prime},\bm{p}^{\prime \prime})\in\mathbb{N}_{0}^{2n}\\ \bm{p}^{\prime}+\bm{p}^{\prime\prime}=\bm{\ell}+e_{j}\end{subarray}}b_{\bm{p} }\left(\partial^{\bm{p}}f\right)(a),\]

as was to be shown.

Since we have shown the case \(\bm{\mathrm{m}}=0\) we may assume the claim to be true for fixed \(\bm{\mathrm{m}},\bm{\ell}\in\mathbb{N}_{0}^{n}\) with \(|\bm{\mathrm{m}}+\bm{\ell}|<k\). Then we deduce

\[\partial_{\mathrm{wirt}}^{\bm{m}+e_{j}}\overline{\partial}_{ \mathrm{wirt}}^{\bm{\ell}}f(a) =\partial_{\mathrm{wirt}}^{e_{j}}\partial_{\mathrm{wirt}}^{\bm{ \mathrm{m}}}\overline{\partial}_{\mathrm{wirt}}^{\bm{\ell}}f(a)\overset{ \mathrm{IH}}{=}\sum_{\begin{subarray}{c}\bm{p}=(\bm{p}^{\prime},\bm{p}^{\prime \prime})\in\mathbb{N}_{0}^{2n}\\ \bm{p}^{\prime}+\bm{p}^{\prime\prime}=\bm{m}+\bm{\ell}\\ \end{subarray}}b_{\bm{p}}\partial_{\mathrm{wirt}}^{e_{j}}\left(\partial^{(\bm{p }^{\prime}+e_{j},\bm{p}^{\prime\prime})}f\right)(a)\] \[=:\sum_{\begin{subarray}{c}\bm{p}=(\bm{p}^{\prime},\bm{p}^{ \prime\prime})\in\mathbb{N}_{0}^{2n}\\ \bm{p}^{\prime}+\bm{p}^{\prime\prime}=\bm{m}+\bm{\ell}+e_{j}\end{subarray}}b_{ \bm{p}}\left(\partial^{\bm{p}}f\right)(a).\]

Hence the claim follows using the principle of mathematical induction. 

Proposition A.1 is technical but crucial: In the course of the paper we will need to approximate Wirtinger derivatives of certain functions. In fact, however, we will approximate real derivatives and take advantage of the fact that Wirtinger derivatives are linear combinations of these.

### Concrete examples of activation functions

In this section we analyze concrete activation functions that are commonly used in practical applications of complex-valued neural networks. We are going to show that those activation functions are smooth and non-polyharmonic on some non-empty open subset of \(\mathbb{C}\). Our first result analyzes a family of "real activation functions interpreted as complex activation functions".

**Proposition A.2**.: _Let \(\rho\in C^{\infty}(\mathbb{R};\mathbb{R})\) be non-polynomial and let \(\psi:\mathbb{C}\to\mathbb{C}\) be defined as_

\[\psi(z):=\rho(\mathrm{Re}(z))\quad\text{resp.}\quad\psi(z):=\rho(\mathrm{Im}(z)).\]

_Then \(\psi\) is smooth and non-polyharmonic._Proof.: Since \(\psi\) depends only on the real-, resp. imaginary part of the input, we see directly from the definition of the Wirtinger derivatives that

\[\partial_{\mathrm{wirt}}\psi(z)=\overline{\partial}_{\mathrm{wirt}}\psi(z)= \frac{1}{2}\rho^{\prime}(\mathrm{Re}(z))\quad\text{resp.}\quad\partial_{ \mathrm{wirt}}\psi(z)=-\overline{\partial}_{\mathrm{wirt}}\psi(z)=-\frac{i}{2} \rho^{\prime}(\mathrm{Im}(z)).\]

Hence we see for arbitrary \(m,\ell\in\mathbb{N}_{0}\) that

\[\left|\partial_{\mathrm{wirt}}^{m}\overline{\partial}_{\mathrm{wirt}}^{\ell} \psi(z)\right|=\frac{1}{2^{m+\ell}}\left|\rho^{(m+\ell)}(\mathrm{Re}(z))\right| \quad\text{resp.}\quad\left|\partial_{\mathrm{wirt}}^{m}\overline{\partial}_{ \mathrm{wirt}}^{\ell}\psi(z)\right|=\frac{1}{2^{m+\ell}}\left|\rho^{(m+\ell)} (\mathrm{Im}(z))\right|.\]

Since \(\rho\) is non-polynomial we can choose a real number \(x\), such that \(\rho^{(n)}(x)\neq 0\) for all \(n\in\mathbb{N}_{0}\) (cf. for instance [20, p. 53]). Thus, \(\partial_{\mathrm{wirt}}^{m}\overline{\partial}_{\mathrm{wirt}}^{\ell}\psi(z)\neq 0\) for all \(m,\ell\in\mathbb{N}_{0}\), whenever \(z\in\mathbb{C}\) with \(\mathrm{Re}(z)=x\), or \(\mathrm{Im}(z)=y\), respectively. 

Next, we consider the modReLU which was defined in Section 2.

**Theorem A.3**.: _Let \(b\in(-\infty,0)\). Writing \(\sigma=\sigma_{\mathrm{modReLU},b}\), one has for every \(z\in\mathbb{C}\) with \(|z|+b>0\) the identity_

\[\left(\partial_{\mathrm{wirt}}^{m}\overline{\partial}_{\mathrm{wirt}}^{\ell} \sigma\right)(z)=\begin{cases}z+b\frac{z}{|z|},&m=\ell=0,\\ 1+\frac{b}{2}\cdot\frac{1}{|z|},&m=1,\ell=0,\\ b\cdot q_{m,\ell}\cdot\frac{z^{\ell-m+1}}{|z|^{2m+1}},&m\leq\ell+1,\ell\geq 1, \\ b\cdot q_{m,\ell}\cdot\frac{z^{m-\ell-1}}{|z|^{2m-1}},&m\geq\ell+1,m\geq 2 \end{cases}\]

_for every \(m\in\mathbb{N}_{0}\) and \(\ell\in\mathbb{N}_{0}\). Here, the numbers \(q_{m,\ell}\) are non-zero and rational. Furthermore, note that all cases for choices of \(m\) and \(\ell\) are covered, by observing that we can either have the case \(m\geq\ell+1\) (where either \(m\geq 2\) or \(m=1,\ell=0\)) or \(m\leq\ell+1\), where the latter case is again split into \(\ell=0\) and \(\ell\geq 1\)._

Proof.: We first calculate certain Wirtinger derivatives for \(z\neq 0\). First note

\[\overline{\partial}_{\mathrm{wirt}}\left(\frac{1}{|z|^{m}}\right) =\frac{1}{2}\left(\partial^{(1,0)}\left(\frac{1}{|z|^{m}}\right) +i\cdot\partial^{(0,1)}\left(\frac{1}{|z|^{m}}\right)\right)\] \[=\frac{1}{2}\left(\left(-\frac{m}{2}\right)\frac{2\,\mathrm{Re}(z )+i\cdot 2\,\mathrm{Im}(z)}{|z|^{m+2}}\right)\] \[=-\frac{m}{2}\cdot\frac{z}{|z|^{m+2}}\]

and similarly

\[\partial_{\mathrm{wirt}}\left(\frac{1}{|z|^{m}}\right)=-\frac{m}{2}\cdot\frac {\overline{z}}{|z|^{m+2}}\]

for any \(m\in\mathbb{N}\). Using the product rule for Wirtinger derivatives (see Appendix A.1), we see

\[\overline{\partial}_{\mathrm{wirt}}\left(\frac{z^{\ell}}{|z|^{m}}\right)= \underbrace{\overline{\partial}_{\mathrm{wirt}}\left(z^{\ell}\right)}_{=0} \cdot\frac{1}{|z|^{m}}+z^{\ell}\cdot\overline{\partial}_{\mathrm{wirt}}\left( \frac{1}{|z|^{m}}\right)=-\frac{m}{2}\cdot\frac{z^{\ell+1}}{|z|^{m+2}}\] (A.2)

for any \(m\in\mathbb{N}\) and \(\ell\in\mathbb{N}_{0}\) and furthermore

\[\partial_{\mathrm{wirt}}\left(\frac{z^{\ell}}{|z|^{m}}\right) =\partial_{\mathrm{wirt}}\left(z^{\ell}\right)\cdot\frac{1}{|z|^{ m}}+z^{\ell}\cdot\partial_{\mathrm{wirt}}\left(\frac{1}{|z|^{m}}\right)\] \[=\ell\cdot z^{\ell-1}\cdot\frac{1}{|z|^{m}}-z^{\ell}\cdot\frac{m} {2}\cdot\frac{\overline{z}}{|z|^{m+2}}\] \[=\left(\ell-\frac{m}{2}\right)\cdot\frac{z^{\ell-1}}{|z|^{m}}\] (A.3)

for \(m,\ell\in\mathbb{N}\), and finally

\[\partial_{\mathrm{wirt}}\left(\frac{\overline{z}^{\ell}}{|z|^{m}}\right)= \underbrace{\partial_{\mathrm{wirt}}\left(\overline{z}^{\ell}\right)}_{=0} \cdot\frac{1}{|z|^{m}}+\overline{z}^{\ell}\cdot\partial_{\mathrm{wirt}}\left( \frac{1}{|z|^{m}}\right)=-\frac{m}{2}\cdot\frac{\overline{z}^{\ell+1}}{|z|^{m+2}}\] (A.4)

[MISSING_PAGE_FAIL:17]

Next we analyze the complex cardioid, which was defined in Section 2.

**Theorem A.5**.: _For any \(z\in\mathbb{C}\) with \(z\neq 0\) and any \(m,\ell\in\mathbb{N}_{0}\) we have_

\[\partial_{\mathrm{wirt}}^{m}\overline{\partial}_{\mathrm{wirt}}^{\ell}\, \mathrm{card}(z)=\begin{cases}\frac{1}{2}z+\frac{1}{4}\frac{z^{2}}{|z|}+\frac{ |z|}{4},&m=\ell=0,\\ a_{m,\ell}\frac{z^{\ell-m}}{|z|^{2\ell-1}}+b_{m,\ell}\frac{z^{\ell+2-m}}{|z|^{2 \ell+1}},&m\leq\ell\neq 0,\\ \frac{1}{2}+\frac{1}{8}\frac{\overline{z}}{|z|}+\frac{3}{8}\cdot\frac{\overline {z}}{|z|},&m=\ell+1=1,\\ a_{m,\ell}\frac{z^{\ell+1}}{|z|^{2\ell+1}}+b_{m,\ell}\frac{z}{|z|^{2\ell+1}},& m=\ell+1>1,\\ a_{m,\ell}\frac{z^{\ell-m}}{|z|^{2m-1}}+b_{m,\ell}\frac{z^{\ell-m-\ell-2}}{|z|^{2 m-3}},&m\geq\ell+2.\end{cases}\]

_Here, the numbers \(a_{m,\ell}\) and \(b_{m,\ell}\) are non-zero and rational. Furthermore, note that all cases for possible choices of \(m\) and \(\ell\) are covered: The case \(m\leq\ell\) is split into \(\ell=0\) and \(\ell\neq 0\). The case \(m=\ell+1\) is split into \(m=1\) and \(m>1\). Then, the case \(m\geq\ell+2\) remains._

Proof.: For the following we always assume \(z\in\mathbb{C}\) with \(z\neq 0\). Then we can apply the identity \(\cos(\spherical z)=\frac{\operatorname{Re}(z)}{|z|}\), so we can rewrite

\[\mathrm{card}(z)=\frac{1}{2}\left(1+\frac{\operatorname{Re}(z)}{|z|}\right)z= \frac{1}{2}z+\frac{1}{4}\frac{(z+\overline{z})\,z}{|z|}=\frac{1}{2}z+\frac{1} {4}\frac{z^{2}}{|z|}+\frac{|z|}{4}.\] (A.5)

This establishes the case \(m=\ell=0\). Next, we compute

\[\overline{\partial}_{\mathrm{wirt}}\left(|z|\right)=\frac{1}{2}\left(\frac{1} {2}\frac{2\operatorname{Re}(z)}{|z|}+\frac{i}{2}\frac{2\operatorname{Im}(z)}{ |z|}\right)=\frac{1}{2}\frac{z}{|z|}\] (A.6)

and similarly

\[\partial_{\mathrm{wirt}}\left(|z|\right)=\frac{1}{2}\frac{\overline{z}}{|z|}.\] (A.7)

We deduce

\[\overline{\partial}_{\mathrm{wirt}}\,\mathrm{card}(z)\stackrel{{ \eqref{eq:wirt},\eqref{eq:wirt}}}{{=}}\underbrace{\frac{1}{4}\cdot \left(-\frac{1}{2}\right)}_{=:b_{0,1}}\cdot\frac{z^{3}}{|z|^{3}}+\underbrace{ \frac{1}{8}}_{=:a_{0,1}}\cdot\frac{z}{|z|}.\]

Using induction, we derive

\[\overline{\partial}_{\mathrm{wirt}}^{\ell+1}\,\mathrm{card}(z) =a_{0,\ell}\overline{\partial}_{\mathrm{wirt}}\left(\frac{z^{\ell }}{|z|^{2\ell-1}}\right)+b_{0,\ell}\overline{\partial}_{\mathrm{wirt}}\left( \frac{z^{\ell+2}}{|z|^{2\ell+1}}\right)\] \[\stackrel{{\eqref{eq:wirt},\eqref{eq:wirt}}}{{=}}:a_ {0,\ell+1}\cdot\frac{z^{\ell+1}}{|z|^{2\ell+1}}+\underbrace{b_{0,\ell}\cdot \left(-\frac{2\ell+1}{2}\right)}_{=:b_{0,\ell+1}}\cdot\frac{z^{\ell+3}}{|z|^{2 \ell+3}},\]

so the claim has been shown if \(m=0\). If we now fix any \(\ell\in\mathbb{N}\) and assume that the claim holds true for some \(m\in\mathbb{N}_{0}\) with \(m<\ell\), we get

\[\partial_{\mathrm{wirt}}^{m+1}\overline{\partial}_{\mathrm{wirt}}^ {\ell}\,\mathrm{card}(z) =a_{m,\ell}\partial_{\mathrm{wirt}}\left(\frac{z^{\ell-m}}{|z|^{2 \ell-1}}\right)+b_{m,\ell}\partial_{\mathrm{wirt}}\left(\frac{z^{\ell+2-m}}{| z|^{2\ell+1}}\right)\] \[\stackrel{{\eqref{eq:wirt},\eqref{eq:wirt}}}{{=}} \underbrace{a_{m,\ell}\cdot\left(\frac{1}{2}-m\right)}_{=:a_{m+1,\ell}} \cdot\frac{z^{\ell-m-1}}{|z|^{2\ell-1}}+\underbrace{b_{m,\ell}\cdot\left(\frac {3}{2}-m\right)}_{=:b_{m+1,\ell}}\cdot\frac{z^{\ell+2-m-1}}{|z|^{2\ell+1}},\]

so the claim holds true if \(m\leq\ell\).

The case \(m=\ell+1\) is split into the case \(m=1\) and \(m>1\). If \(m=1\), then \(\ell=0\) and we compute

\[\partial_{\mathrm{wirt}}\,\mathrm{card}(z)\stackrel{{\eqref{eq: wirt},\eqref{eq:wirt}}}{{=}}\frac{1}{2}+\frac{1}{4}\left(2-\frac{1}{2}\right)\cdot \frac{z}{|z|}+\frac{1}{8}\cdot\frac{\overline{z}}{|z|}.\]If \(m>1\), we get

\[\partial_{\mathrm{wirt}}^{\ell+1}\overline{\partial}_{\mathrm{wirt} }^{\ell}\operatorname{card}(z) =a_{\ell,\ell}\partial_{\mathrm{wirt}}\left(\frac{1}{|z|^{2\ell-1} }\right)+b_{\ell,\ell}\cdot\partial_{\mathrm{wirt}}\left(\frac{z^{2}}{|z|^{2 \ell+1}}\right)\] \[\stackrel{{\text{\eqref{eq:wirr},eqref{eq:wirr}}}}{{= :a_{\ell+1,\ell}}}\underbrace{a_{\ell+1,\ell}\cdot\left(-\frac{2\ell+1}{2} \right)}_{=:a_{\ell+2,\ell}}\cdot\frac{\overline{z}}{|z|^{2\ell+3}}+ \underbrace{b_{\ell+1,\ell}\cdot\left(1-\frac{2\ell+1}{2}\right)}_{=:b_{\ell+ 2,\ell}}\cdot\frac{z}{|z|^{2\ell+1}}.\]

If we assume the claim to be true for a fixed \(m\geq\ell+2\), we get

\[\partial_{\mathrm{wirt}}^{m+1}\overline{\partial}_{\mathrm{wirt} }^{\ell}\operatorname{card}(z) =a_{m,\ell}\cdot\partial_{\mathrm{wirt}}\left(\frac{\overline{z} ^{m-\ell}}{|z|^{2m-1}}\right)+b_{m,\ell}\cdot\partial_{\mathrm{wirt}}\left( \frac{\overline{z}^{m-\ell-2}}{|z|^{2m-3}}\right)\] \[\stackrel{{\text{\eqref{eq:wirr},eqref{eq:wirr}}}}{{= :a_{m+1,\ell}}}\cdot\frac{\overline{z}^{m+1-\ell}}{|z|^{2m+1}}+\underbrace{b_{m,\ell}\cdot\left(-\frac{2m-3}{2}\right)}_{=:b_{m+1,\ell}}\cdot\frac{\overline{ z}^{m-\ell-1}}{|z|^{2m-1}}.\]

Hence, using induction, we have proven the claimed identity. 

The statement regarding the non-polyharmonicity of the complex cardioid is formulated in the following corollary.

**Corollary A.6**.: _For every \(z\in\mathbb{C}\) with \(z\notin\mathbb{R}\cup i\mathbb{R}\) and every \(m,\ell\in\mathbb{N}_{0}\) we have_

\[\partial_{\mathrm{wirt}}^{m}\overline{\partial}_{\mathrm{wirt}}^{\ell} \operatorname{card}(z)\neq 0.\]

_In particular, \(\operatorname{card}\) is smooth and non-polyharmonic on \(\mathbb{C}\setminus(\mathbb{R}\cup i\mathbb{R})\). Futhermore, \(\operatorname{card}\) is continuous on the entire complex plane._

Proof.: We start with the first part of the corollary. For the following, let \(z\in\mathbb{C}\) with \(z\notin\mathbb{R}\cup i\mathbb{R}\), i.e., \(\operatorname{Re}(z)\neq 0\) and \(\operatorname{Im}(z)\neq 0\). Using the definition of \(\operatorname{card}\), we see

\[\operatorname{card}(z)=0\quad\Longleftrightarrow\quad z=0\quad\text{or} \quad\cos(\spherical z)=-1\quad\Longleftrightarrow\quad z\in\mathbb{R}_{\leq 0},\]

and thus, \(\operatorname{card}(z)\neq 0\) since \(\operatorname{Im}(z)\neq 0\). In the case \(m\leq\ell\neq 0\) we use Theorem A.5 to get the following chain of implications:

\[\partial_{\mathrm{wirt}}^{m}\overline{\partial}_{\mathrm{wirt}}^{\ell} \operatorname{card}(z)=0\Leftrightarrow a_{m,\ell}+b_{m,\ell}\frac{z^{2}}{|z|^{2 }}=0\Leftrightarrow z^{2}=-|z|^{2}\cdot\frac{a_{m,\ell}}{b_{m,\ell}} \Rightarrow z^{2}\in\mathbb{R}\Leftrightarrow z\in\mathbb{R}\cup i\mathbb{R}\]

holds, and thus \(\partial_{\mathrm{wirt}}^{m}\overline{\partial}_{\mathrm{wirt}}^{\ell} \operatorname{card}(z)\neq 0\) for \(z\notin\mathbb{R}\cup i\mathbb{R}\).

For the case \(m=\ell+1=1\), note

\[\partial_{\mathrm{wirt}}\operatorname{card}(z)=0\Leftrightarrow\frac{1}{8} \cdot\overline{z}+\frac{3}{8}\cdot z=-\frac{|z|}{2}\Rightarrow\frac{3}{8} \operatorname{Im}(z)-\frac{1}{8}\operatorname{Im}(z)=0\Leftrightarrow \operatorname{Im}(z)=0,\]

and thus \(\partial_{\mathrm{wirt}}\operatorname{card}(z)\neq 0\) since \(z\notin\mathbb{R}\).

For \(m=\ell+1>1\), we see by considering the real- and imaginary parts that

\[\partial_{\mathrm{wirt}}^{\ell+1}\overline{\partial}_{\mathrm{ wirt}}^{\ell}\operatorname{card}(z) =0 \Leftrightarrow a_{m,\ell}\overline{z}+b_{m,\ell}z=0\] \[\stackrel{{\operatorname{Re}(z),\operatorname{Im}(z) \neq 0}}{{\Leftrightarrow}}a_{m,\ell}+b_{m,\ell}=0\text{ and }-a_{m,\ell}+b_{m,\ell}=0\] \[\Leftrightarrow a_{m,\ell}=b_{m,\ell}=0,\]and hence \(\partial_{\text{wirt}}^{\ell+1}\overline{\partial}_{\text{wirt}}^{\ell}\,\mathrm{ card}(z)\neq 0\), since \(a_{m,\ell}\neq 0\neq b_{m,\ell}\) by Theorem A.5.

Therefore, it remains to consider the case \(m\geq\ell+2\). But here we easily see

\[\partial_{\text{wirt}}^{m}\overline{\partial}_{\text{wirt}}^{\ell}\,\mathrm{ card}(z)=0\quad\Leftrightarrow\quad a_{m,\ell}\frac{\overline{z}^{2}}{|z|^{2}}+b_{m, \ell}=0\quad\Rightarrow\quad\overline{z}^{2}\in\mathbb{R}\quad\Leftrightarrow \quad z\in\mathbb{R}\cup i\mathbb{R},\]

and hence \(\partial_{\text{wirt}}^{m}\overline{\partial}_{\text{wirt}}^{\ell}\,\mathrm{ card}(z)\neq 0\). Since all cases have been considered, the claim follows.

Regarding the second part of the corollary, first note that \(\mathrm{card}\) is trivially continuous on \(\mathbb{C}\setminus\{0\}\). Further note that

\[|\,\mathrm{card}(z)|=\left|\frac{1}{2}\left(1+\frac{\mathrm{Re}(z)}{|z|} \right)z\right|\leq|z|\to 0\]

as \(z\to 0\), showing the continuity of \(\mathrm{card}\) on the entire complex plane \(\mathbb{C}\). 

## Appendix B Postponed proofs concerning the approximation of polynomials

### Divided Differences

Divided differences are well-known in numerical mathematics as they are for example used to calculate the coefficients of an interpolation polynomial in its Newton representation. In our case, we are interested in divided differences since they can be used to obtain a generalization of the classical mean-value theorem for differentiable functions: Given an interval \(I\subseteq\mathbb{R}\) and \(n+1\) pairwise distinct data points \(x_{0},...,x_{n}\in I\) as well as an \(n\)-times differentiable real-valued function \(f:I\to\mathbb{R}\), there exists \(\xi\in\left(\text{min}\left\{x_{0},...,x_{n}\right\},\text{max}\left\{x_{0},...,x_{n}\right\}\right)\) such that

\[f\left[x_{0},...,x_{n}\right]=\frac{f^{(n)}(\xi)}{n!},\]

where the left-hand side is a divided difference of \(f\) (defined below). The classical mean-value theorem is obtained in the case \(n=1\). Our goal in this section is to generalize this result to a multivariate setting by considering divided differences in multiple variables. Such a generalization is probably well-known, but since we could not locate a convenient reference and to make the paper more self-contained, we provide a proof.

Let us first define divided differences formally. Given \(n+1\) data points \(\left(x_{0},y_{0}\right),...,\left(x_{n},y_{n}\right)\in\mathbb{R}\times \mathbb{R}\) with pairwise distinct \(x_{k}\), we define the associated divided differences recursively via

\[\left[y_{k}\right] :=y_{k},\ k\in\{0,...,n\},\] \[\left[y_{k},...,y_{k+j}\right] :=\frac{\left[y_{k+1},...,y_{k+j}\right]-\left[y_{k},...,y_{k+j-1 }\right]}{x_{k+j}-x_{k}},\ j\in\{1,...,n\},\ k\in\{0,...,n-j\}.\]

If the data points are defined by a function \(f\) (i.e. \(y_{k}=f\left(x_{k}\right)\) for all \(k\in\{0,...,n\}\)), we write

\[\left[x_{k},...,x_{k+j}\right]f:=\left[y_{k},...,y_{k+j}\right].\]

We first consider an alternative representation of divided differences, the so-called _Hermite-Genocchi-Formula_. To state it, we introduce the notation \(\Sigma^{k}\) to denote a certain \(k\)-dimensional simplex.

**Definition B.1**.: Let \(s\in\mathbb{N}\). Then we define

\[\Sigma^{s}:=\left\{x\in\mathbb{R}^{s}:\ x_{\ell}\geq 0\ \mathrm{for\ all}\ \ell\ \mathrm{and}\sum_{\ell=1}^{s}x_{\ell}\leq 1\right\}.\]

We further set \(\Sigma^{0}:=\{0\}\subseteq\mathbb{R}\). Denoting by \(\lambda^{s}\) the \(s\)-dimensional Lebesgue-measure (and by \(\lambda^{0}\) the counting measure), the identity \(\lambda^{s}\left(\Sigma^{s}\right)=\frac{1}{s!}\) holds true; see for instance [42] and note that the case \(s=0\) can be seen directly.

In the following, we consider integrals over the sets \(\Sigma^{s}\). These integrals are always formed with respect to the measure \(\lambda^{s}\). However, we only write, e.g., \(\int_{\Sigma^{s}}f(x)dx\), with the implicit understanding that the integral is formed with respect to the measure \(\lambda^{s}\). Using this convention, we can now consider the alternative representation of divided differences.

**Lemma B.2** (Hermite-Genocchi-Formula).: _Let \(k\in\mathbb{N}_{0}\). For real numbers \(a,b\in\mathbb{R}\) with \(a<b\), a function \(f\in C^{k}([a,b];\mathbb{R})\) (where \(C^{0}([a,b];\mathbb{R})\) denotes the set of continuous functions) and pairwise distinct \(x_{0},...,x_{k}\in[a,b]\), the divided difference of \(f\) at the data points \(x_{0},...,x_{k}\) satisfies the identity_

\[\left[x_{0},...,x_{k}\right]f=\int_{\Sigma^{k}}f^{(k)}\left(x_{0}+\sum_{\ell=1 }^{k}s_{\ell}\left(x_{\ell}-x_{0}\right)\right)ds.\] (B.1)

Proof.: The case \(k=0\) follows directly from \([x_{0}]f=f(x_{0})\). For the case \(k>0\), see [6, Theorem 3.3]. 

We will make use of the above formula by combining it with the following generalization of the mean-value theorem for integrals.

**Lemma B.3**.: _Let \(D\) be a connected and compact topological space. Let \(\mathcal{A}\) be some \(\sigma\)-algebra over \(D\) and let \(\mu:\mathcal{A}\rightarrow[0,\infty)\) be a finite measure on \(D\) with \(\mu(D)>0\). Let \(f:D\rightarrow\mathbb{R}\) be a continuous function with respect to the standard topology on \(\mathbb{R}\) and the topology on \(D\). Moreover, let \(f\) be measurable with respect to \(\mathcal{A}\) and the Borel \(\sigma\)-algebra on \(\mathbb{R}\). Then there exists \(\xi\in D\) such that_

\[f(\xi)=\frac{1}{\mu(D)}\cdot\int_{D}f(x)d\mu(x).\]

Proof.: Since \(D\) is compact and connected and \(f\) continuous, it follows that \(f(D)\subset\mathbb{R}\) is compact and connected, and hence \(f(D)\) is a compact interval in \(\mathbb{R}\). Therefore, there exist \(x_{\text{min}}\in D\) and \(x_{\text{max}}\in D\) satisfying

\[f\left(x_{\text{min}}\right)\leq f(x)\leq f\left(x_{\text{max}}\right)\]

for all \(x\in D\). Thus, one gets

\[f\left(x_{\text{min}}\right) =\frac{1}{\mu(D)}\int_{D}f(x_{\text{min}})d\mu(x)\leq\frac{1}{\mu (D)}\int_{D}f(x)d\mu(x)\] \[\leq\frac{1}{\mu(D)}\int_{D}f(x_{\text{max}})d\mu(x)=f\left(x_{ \text{max}}\right).\]

The claim now follows from the fact that \(f(D)\) is an interval. 

We also get a convenient representation of divided differences for the case of equidistant data points.

**Lemma B.4**.: _Let \(f:\mathbb{R}\rightarrow\mathbb{R}\), \(x_{0}\in\mathbb{R}\), \(k\in\mathbb{N}_{0}\) and \(h>0\). We consider the case of equidistant data points, meaning \(x_{j}:=x_{0}+jh\) for all \(j=1,...,k\). In this case, we have the formula_

\[\left[x_{0},...,x_{k}\right]f=\frac{1}{k!h^{k}}\cdot\sum_{r=0}^{k}(-1)^{k-r} \binom{k}{r}f\left(x_{r}\right).\] (B.2)

Proof.: We prove the result via induction over the number \(j\) of considered data points, meaning the following: For all \(j\in\{0,...,k\}\) we have

\[\left[x_{\ell},...,x_{\ell+j}\right]f=\frac{1}{j!h^{j}}\cdot\sum_{r=0}^{j}(-1)^ {j-r}\binom{j}{r}f\left(x_{\ell+r}\right)\]

for all \(\ell\in\{0,...,k\}\) satisfying \(\ell+j\leq k\). The case \(j=0\) is trivial. Therefore, we assume the claim to be true for a fixed \(j\in\{0,...,k-1\}\), and let \(\ell\in\{0,...,k\}\) be arbitrary with \(\ell+j+1\leq k\). We then get

\[\left[x_{\ell},...,x_{\ell+j+1}\right]f =\frac{\left[x_{\ell+1},...,x_{\ell+j+1}\right]f-\left[x_{\ell},...,x_{\ell+j}\right]f}{x_{\ell+j+1}-x_{\ell}}\] \[\overset{\text{IH.}}{=}\frac{1}{j!h^{j}}\cdot\frac{\sum_{r=0}^{j -}(-1)^{j-r}\binom{j}{r}\left(f\left(x_{\ell+r+1}\right)-f\left(x_{\ell+r} \right)\right)}{(j+1)h}\] \[=\frac{1}{(j+1)!h^{j+1}}\sum_{r=0}^{j}(-1)^{j-r}\binom{j}{r}\left( f\left(x_{\ell+r+1}\right)-f\left(x_{\ell+r}\right)\right).\]Using an index shift, we deduce

\[\sum_{r=0}^{j}(-1)^{j-r}\binom{j}{r}f\left(x_{\ell+r+1}\right)-\sum_{r =0}^{j}(-1)^{j-r}\binom{j}{r}f\left(x_{\ell+r}\right)\] \[=\sum_{r=1}^{j+1}(-1)^{j+1-r}\binom{j}{r-1}f\left(x_{\ell+r}\right) +\sum_{r=0}^{j}(-1)^{j+1-r}\binom{j}{r}f\left(x_{\ell+r}\right)\] \[=(-1)^{j+1}f\left(x_{\ell}\right)+\sum_{r=1}^{j}\left((-1)^{j+1-r }f\left(x_{\ell+r}\right)\left[\binom{j}{r-1}+\binom{j}{r}\right]\right)+f\left( x_{\ell+j+1}\right)\] \[=\sum_{r=0}^{j+1}(-1)^{j+1-r}\binom{j+1}{r}f\left(x_{\ell+r} \right),\]

which yields the claim. 

The final result for divided differences, which is the result that is actually used in the proof of Theorem 3.1 in Appendix B.3, reads as follows:

**Theorem B.5**.: _Let \(f:\mathbb{R}^{s}\rightarrow\mathbb{R}\) and \(k\in\mathbb{N}_{0},r>0\), such that \(f|_{(-r,r)^{s}}\in C^{k}\left((-r,r)^{s};\mathbb{R}\right)\). For \(\boldsymbol{p}\in\mathbb{N}_{0}^{s}\) with \(|\boldsymbol{p}|\leq k\) and \(h>0\) let_

\[f_{\boldsymbol{p},h}:=(2h)^{-|\boldsymbol{p}|}\sum_{0\leq\boldsymbol{r}\leq \boldsymbol{p}}(-1)^{|\boldsymbol{p}|-|\boldsymbol{r}|}\binom{\boldsymbol{p}}{ \boldsymbol{r}}f\left(h(2\boldsymbol{r}-\boldsymbol{p})\right).\]

_Let \(m:=\max\limits_{j}\boldsymbol{p}_{j}\). Then, for \(0<h<\frac{r}{\max\{1,m\}}\) there exists \(\xi\in h[-m,m]^{s}\) satisfying_

\[f_{\boldsymbol{p},h}=\partial^{\boldsymbol{p}}f(\xi).\]

Proof.: We may assume \(m>0\), since \(m=0\) implies \(\boldsymbol{p}=0\) and thus \(f_{\boldsymbol{p},h}=f(0)\), so that the claim holds for \(\xi=0\).

We prove via induction over \(s\in\mathbb{N}\) that the formula

\[f_{\boldsymbol{p},h}=\boldsymbol{p}!\int_{\Sigma^{\boldsymbol{p}_{s}}}\int_{ \Sigma^{\boldsymbol{p}_{s-1}}}...\int_{\Sigma^{\boldsymbol{p}_{1}}}\partial^ {\boldsymbol{p}}f\left(-h\boldsymbol{p}_{1}+2h\sum_{\ell=1}^{\boldsymbol{p}_ {1}}\ell\sigma_{\ell}^{(1)},...,-h\boldsymbol{p}_{s}+2h\sum_{\ell=1}^{ \boldsymbol{p}_{s}}\ell\sigma_{\ell}^{(s)}\right)d\sigma^{(1)}...d\sigma^{(s)}\] (B.3)

holds for all \(\boldsymbol{p}\in\mathbb{N}_{0}^{s}\) with \(1\leq|\boldsymbol{p}|\leq k\) and all \(0<h<\frac{r}{m}\). The case \(s=1\) is exactly the Hermite-Genocchi-Formula (B.1), combined with (B.2) applied to the data points

\[-hp,-hp+2h,...,hp-2h,hp.\]

By induction, assume that the claim holds for some \(s\in\mathbb{N}\). For a fixed point \(y\in(-r,r)\), let

\[f_{y}:\quad(-r,r)^{s}\rightarrow\mathbb{R},\quad x\mapsto f(x,y).\]

For \(\boldsymbol{p}\in\mathbb{N}_{0}^{s+1}\) with \(|\boldsymbol{p}|\leq k\) and \(\boldsymbol{p}^{\prime}:=(p_{1},...,p_{s})\), we define

\[\Gamma:\quad(-r,r)\rightarrow\mathbb{R},\quad y\mapsto(f_{y})_{\boldsymbol{p} ^{\prime},h}=(2h)^{-|\boldsymbol{p}^{\prime}|}\sum_{0\leq\boldsymbol{r}^{ \prime}\leq\boldsymbol{p}^{\prime}}(-1)^{|\boldsymbol{p}^{\prime}|-| \boldsymbol{r}^{\prime}|}\binom{\boldsymbol{p}^{\prime}}{\boldsymbol{r}^{ \prime}}f\left(h(2\boldsymbol{r}^{\prime}-\boldsymbol{p}^{\prime}),y\right).\]

Using the induction hypothesis, we get

\[\Gamma(y)\] \[= \boldsymbol{p}^{\prime}!\int\limits_{\Sigma^{\boldsymbol{p}_{s}}} \int\limits_{\Sigma^{\boldsymbol{p}_{s-1}}}\cdots\int\limits_{\Sigma^{ \boldsymbol{p}_{1}}}\partial^{(\boldsymbol{p}^{\prime},0)}f\left(-h \boldsymbol{p}_{1}+2h\sum_{\ell=1}^{\boldsymbol{p}_{1}}\ell\sigma_{\ell}^{(1)},...,-h\boldsymbol{p}_{s}+2h\sum_{\ell=1}^{\boldsymbol{p}_{s}}\ell\sigma_{ \ell}^{(s)},y\right)d\sigma^{(1)}\cdots d\sigma^{(s)}\]for all \(y\in(-r,r)\). Furthermore, we calculate

\[\textbf{p}_{s+1}!\cdot[-h\cdot\textbf{p}_{s+1},-h\cdot\textbf{p}_{s+1 }+2h,...,h\cdot\textbf{p}_{s+1}]\Gamma\] \[\overset{\eqref{eq:p_s+1}}{=}(2h)^{-\textbf{p}_{s+1}}\sum_{r^{ \prime}=0}^{\textbf{p}_{s+1}}(-1)^{\textbf{p}_{s+1}-r^{\prime}}\binom{\textbf{ p}_{s+1}}{r^{\prime}}\Gamma\left(h\left(2r^{\prime}-\textbf{p}_{s+1}\right)\right)\] \[=(2h)^{-\textbf{p}_{s+1}}\left[\sum_{r^{\prime}=0}^{\textbf{p}_{ s+1}}(-1)^{\textbf{p}_{s+1}-r^{\prime}}\binom{\textbf{p}_{s+1}}{r^{\prime}}(2h)^{-| \textbf{p}^{\prime}|}\right.\] \[\qquad\qquad\qquad\times\sum_{0\leq r^{\prime}\leq\textbf{p}^{ \prime}}(-1)^{|\textbf{p}^{\prime}|-|\textbf{r}^{\prime}|}\binom{\textbf{p}^ {\prime}}{\textbf{r}^{\prime}}f\left(h(2\textbf{r}^{\prime}-\textbf{p}^{ \prime}),h(2r^{\prime}-\textbf{p}_{s+1})\right)\Bigg{]}\] \[=(2h)^{-|\textbf{p}|}\sum_{0\leq\textbf{r}\leq\textbf{p}}(-1)^{| \textbf{p}|-|\textbf{r}|}\binom{\textbf{p}}{\textbf{r}}f\left(h(2\textbf{r}- \textbf{p})\right)\] \[=f_{\textbf{p},h}.\]

On the other hand, we get

\[[-h\cdot\textbf{p}_{s+1},-h\cdot\textbf{p}_{s+1}+2h,...,h\cdot \textbf{p}_{s+1}]\Gamma\] \[\overset{\eqref{eq:p_s+1}}{=}\int_{\Sigma\textbf{p}^{s+1}}\Gamma ^{(\textbf{p}_{s+1})}\left(-h\textbf{p}_{s+1}+2h\sum_{\ell=1}^{\textbf{p}_{s+ 1}}\ell\sigma_{\ell}^{(s+1)}\right)d\sigma^{(s+1)}\] \[=\textbf{p}^{\prime}!\int\limits_{\Sigma\textbf{p}^{s+1}}\cdots \int\limits_{\Sigma\textbf{p}^{1}}\partial^{\textbf{p}}f\left(-h\textbf{p}_{ 1}+2h\sum_{\ell=1}^{\textbf{p}_{1}}\ell\sigma_{\ell}^{(1)},...,-h\textbf{p}_{s +1}+2h\sum_{\ell=1}^{\textbf{p}_{s+1}}\ell\sigma_{\ell}^{(s+1)}\right)d\sigma^ {(1)}\cdots d\sigma^{(s+1)}.\]

Interchanging the order of integration and derivative is possible, since we integrate on compact sets and only consider continuously differentiable functions (see, e.g., [11, Lemma 16.2]).

We have thus proven (B.3) using the principle of induction. The claim of the theorem then follows directly using the mean-value theorem for integrals (Lemma B.3) applied to the topological space

\[D:=\Sigma^{\textbf{p}_{1}}\times\cdots\times\Sigma^{\textbf{p}_{s+1}}\]

equipped with the product topology where each factor is endowed with the standard topology on \(\Sigma^{\textbf{p}_{\ell}}\) (where the standard topology on \(\Sigma^{0}\) is the discrete topology), the measure

\[\mu:=\lambda^{\textbf{p}_{1}}\otimes\cdots\otimes\lambda^{\textbf{p}_{s+1}}\]

defined on the product of the Borel \(\sigma\)-algebras on \(\Sigma^{\textbf{p}_{\ell}}\) (and the \(\sigma\)-algebra of \(\{0\}\) is its power set) and to the function

\[(\sigma^{(1)},...,\sigma^{(s+1)})\mapsto\partial^{\textbf{p}}f\left(-h\textbf{ p}_{1}+2h\sum_{\ell=1}^{\textbf{p}_{1}}\ell\sigma_{\ell}^{(1)},...,-h\textbf{p}_{s+1 }+2h\sum_{\ell=1}^{\textbf{p}_{s+1}}\ell\sigma_{\ell}^{(s+1)}\right).\]

Moreover, note that all the simplices \(\Sigma^{\textbf{p}_{\ell}}\) are compact and connected (in fact convex) with

\[\lambda^{\textbf{p}_{1}}(\Sigma^{\textbf{p}_{1}})\cdots\lambda^{\textbf{p}_{s+ 1}}(\Sigma^{\textbf{p}_{s+1}})=\frac{1}{\textbf{p}!},\]

see Definition B.1. Therefore, Lemma B.3 yields the existence of a certain \((\xi^{(1)},...,\xi^{(s+1)})\in D\) with

\[f_{\textbf{p},h}=\partial^{\textbf{p}}f\left(-h\textbf{p}_{1}+2h\sum_{\ell=1} ^{\textbf{p}_{1}}\ell\xi_{\ell}^{(1)},...,-h\textbf{p}_{s+1}+2h\sum_{\ell=1}^ {\textbf{p}_{s+1}}\ell\xi_{\ell}^{(s+1)}\right).\]

Hence, the claim follows letting

\[\xi:=\left(-h\textbf{p}_{1}+2h\sum_{\ell=1}^{\textbf{p}_{1}}\ell\xi_{\ell}^{( 1)},...,-h\textbf{p}_{s+1}+2h\sum_{\ell=1}^{\textbf{p}_{s+1}}\ell\xi_{\ell}^{(s +1)}\right)\in h[-m,m]^{s}.\qed\]

### Proof of Proposition 2.1

Proof of Proposition 2.1.: Let \(M\in\mathbb{N}_{0}\). Since \(\phi\) is not polyharmonic we can pick \(z\in U\) with \(\Delta^{M}\phi(z)\neq 0\). By continuity we can choose \(\delta>0\) with \(B_{\delta}(z)\subseteq U\) and \(\Delta^{M}\phi(w)\neq 0\) for all \(w\in B_{\delta}(z)\). For all \(m,\ell\in\mathbb{N}_{0}\) let

\[A_{m,\ell}:=\left\{w\in B_{\delta}(z):\;\partial_{\text{wirt}}^{m}\overline{ \partial}_{\text{wirt}}^{\ell}\phi(w)=0\right\}\]

and assume towards a contradiction that

\[\bigcup_{m,\ell\leq M}A_{m,\ell}=B_{\delta}(z).\]

By [3, Corollary 3.35], \(B_{\delta}(z)\) with its usual topology is completely metrizable. By continuity of \(\partial_{\text{wirt}}^{m}\overline{\partial}_{\text{wirt}}^{\ell}\phi\) the sets \(A_{m,\ell}\) are closed in \(B_{\delta}(z)\). Hence, using the Baire category theorem [3, Theorems 3.46 and 3.47], there are \(m,\ell\in\mathbb{N}_{0}\) with \(m,\ell\leq M\), \(z^{\prime}\in A_{m,\ell}\) and \(\varepsilon>0\) such that

\[B_{\varepsilon}\left(z^{\prime}\right)\subseteq A_{m,\ell}\subseteq B_{\delta }(z).\]

But thanks to \(\Delta^{M}\phi=4^{M}\partial_{\text{wirt}}^{M}\overline{\partial}_{\text{wirt }}^{M}\phi=4^{M}\partial_{\text{wirt}}^{M-\ell}\overline{\partial}_{\text{wirt }}^{M-m}\partial_{\text{wirt}}^{\ell}\overline{\partial}_{\text{wirt}}^{m}\phi\) (see property 6 in Appendix A.1), this directly implies \(\Delta^{M}\phi\equiv 0\) on \(B_{\varepsilon}\left(z^{\prime}\right)\) in contradiction to the choice of \(B_{\delta}(z)\). 

### Proof of Theorem 3.1

The following section is dedicated to proving Theorem 3.1. We are going to show that it is possible to approximate complex polynomials in \(z\) and \(\overline{z}\) arbitrarily well on \(\Omega_{n}\) using shallow complex-valued neural networks. To do so, we follow the proof sketch given after the statement of Theorem 3.1, starting with the following lemma.

**Lemma B.6**.: _Let \(\phi:\mathbb{C}\to\mathbb{C}\) and \(\delta>0,\;b\in\mathbb{C},\;k\in\mathbb{N}_{0}\), such that \(\phi\big{|}_{B_{\delta}(b)}\in C^{k}\left(B_{\delta}(b);\mathbb{C}\right)\). For fixed \(z\in\Omega_{n}\), where we recall that \(\Omega_{n}=[-1,1]^{n}+i[-1,1]^{n}\), we consider the map_

\[\phi_{z}:\quad B_{\frac{\delta}{\sqrt{2n}}}(0)\to\mathbb{C},\quad w\mapsto \phi\left(w^{T}z+b\right),\]

_which is in \(C^{k}\) since for \(w\in B_{\frac{\delta}{\sqrt{2n}}}(0)\subseteq\mathbb{C}^{n}\) we have_

\[\left|w^{T}z\right|\leq\|w\|_{2}\cdot\|z\|_{2}<\frac{\delta}{\sqrt{2n}}\cdot \sqrt{2n}=\delta.\]

_For all multi-indices \(\boldsymbol{m},\boldsymbol{\ell}\in\mathbb{N}_{0}^{n}\) with \(|\boldsymbol{m}+\boldsymbol{\ell}|\leq k\) we have_

\[\partial_{\text{wirt}}^{\boldsymbol{m}}\overline{\partial}_{\text{wirt}}^{ \boldsymbol{\ell}}\phi_{z}(w)=z^{\boldsymbol{m}}\overline{z}^{\boldsymbol{ \ell}}\cdot\left(\partial_{\text{wirt}}^{|\boldsymbol{m}|}\overline{\partial} _{\text{wirt}}^{|\boldsymbol{\ell}|}\phi\right)\left(w^{T}z+b\right)\]

_for all \(w\in B_{\frac{\delta}{\sqrt{2n}}}(0)\)._

Proof.: First we prove the statement

\[\overline{\partial}_{\text{wirt}}^{\boldsymbol{\ell}}\phi_{z}(w)=\overline{z} ^{\boldsymbol{\ell}}\cdot(\overline{\partial}_{\text{wirt}}^{|\boldsymbol{ \ell}|}\phi)\left(w^{T}z+b\right)\quad\text{for all }w\in B_{\frac{\delta}{\sqrt{2n}}}(0)\] (B.4)

by induction over \(0\leq|\boldsymbol{\ell}|\leq k\). The case \(\boldsymbol{\ell}=0\) is trivial. Assuming that (B.4) holds for fixed \(\boldsymbol{\ell}\in\mathbb{N}_{0}^{n}\) with \(|\boldsymbol{\ell}|<k\), we want to show

\[\overline{\partial}_{\text{wirt}}^{\boldsymbol{\ell}+e_{j}}\phi_{z}(w)= \overline{z}^{\boldsymbol{\ell}+e_{j}}\cdot\left(\overline{\partial}_{\text{ wirt}}^{|\boldsymbol{\ell}|+1}\phi\right)\left(w^{T}z+b\right)\] (B.5)

for all \(w\in B_{\frac{\delta}{\sqrt{2n}}}(0)\), where \(j\in\{1,...,n\}\) is chosen arbitrarily. To this end, first note

\[\overline{\partial}_{\text{wirt}}^{\boldsymbol{\ell}+e_{j}}\phi_ {z}(w) =\overline{\partial}_{\text{wirt}}^{\boldsymbol{\ell}}\overline{ \partial}_{\text{wirt}}^{\boldsymbol{\ell}}\phi_{z}(w)\overset{\text{induction}}{=}\overline{\partial}_{\text{wirt}}^{e_{j}}\left[w \mapsto\overline{z}^{\boldsymbol{\ell}}\cdot\left(\overline{\partial}_{\text{ wirt}}^{|\boldsymbol{\ell}|}\phi\right)\left(w^{T}z+b\right)\right]\] \[=\overline{z}^{\boldsymbol{\ell}}\overline{\partial}_{\text{ wirt}}^{e_{j}}\left[w\mapsto\left(\overline{\partial}_{\text{wirt}}^{| \boldsymbol{\ell}|}\phi\right)\left(w^{T}z+b\right)\right].\]Applying the chain rule for Wirtinger derivatives and using that

\[\overline{\partial}_{\mathrm{wirt}}^{e_{j}}\left[w\mapsto w^{T}z+b\right]=0\]

since \(w\mapsto w^{T}z+b\) is holomorphic in every variable, we see

\[\overline{\partial}_{\mathrm{wirt}}^{e_{j}}\left[w\mapsto\left( \overline{\partial}_{\mathrm{wirt}}^{|\boldsymbol{\ell}|}\phi\right)\left(w^{T}z +b\right)\right] =\left(\partial_{\mathrm{wirt}}\overline{\partial}_{\mathrm{wirt }}^{|\boldsymbol{\ell}|}\phi\right)\left(w^{T}z+b\right)\cdot\overline{ \partial}_{\mathrm{wirt}}^{e_{j}}\left[w\mapsto w^{T}z+b\right]\] \[\quad+\left(\overline{\partial}_{\mathrm{wirt}}^{|\boldsymbol{ \ell}|+1}\phi\right)\left(w^{T}z+b\right)\cdot\overline{\partial}_{\mathrm{ wirt}}^{e_{j}}\left[w\mapsto\overline{w^{T}z+b}\right]\] \[=\left(\overline{\partial}_{\mathrm{wirt}}^{|\boldsymbol{\ell}|+1 }\phi\right)\left(w^{T}z+b\right)\cdot\overline{\partial}_{\mathrm{wirt}}^{e _{j}}\left[w\mapsto w^{T}z+b\right]\] \[=\overline{z}^{e_{j}}\cdot\left(\overline{\partial}_{\mathrm{ wirt}}^{|\boldsymbol{\ell}|+1}\phi\right)\left(w^{T}z+b\right),\]

using the fact that \(w_{j}\mapsto w^{T}z+b\) is holomorphic and hence

\[\overline{\partial}_{\mathrm{wirt}}^{e_{j}}\left[w\mapsto w^{T}z+b\right]=0 \quad\text{and}\quad\partial_{\mathrm{wirt}}^{e_{j}}\left[w\mapsto w^{T}z+b \right]=z_{j}.\]

Thus, we have proven (B.5) and induction yields (B.4).

It remains to show the full claim. We use induction over \(|\boldsymbol{\mathbf{m}}|\) and note that the case \(\boldsymbol{\mathbf{m}}=0\) has just been shown. We assume that the claim holds true for fixed \(\boldsymbol{\mathbf{m}}\in\mathbb{N}_{0}^{n}\) with \(|\boldsymbol{\mathbf{m}}+\boldsymbol{\ell}|<k\) and choose \(j\in\{1,...,n\}\). Thus, we get

\[\partial_{\mathrm{wirt}}^{\boldsymbol{\mathbf{m}}+e_{j}}\overline{ \partial}_{\mathrm{wirt}}^{\boldsymbol{\mathbf{\ell}}}\phi_{z}(w) =\partial_{\mathrm{wirt}}^{e_{j}}\partial_{\mathrm{wirt}}^{ \boldsymbol{\mathbf{m}}}\overline{\partial}_{\mathrm{wirt}}^{\boldsymbol{ \mathbf{\ell}}}\phi_{z}(w)\overset{\mathrm{IH}}{=}\partial_{\mathrm{wirt}}^{e_{ j}}\left(w\mapsto z^{\boldsymbol{\mathbf{m}}}\overline{z}^{\boldsymbol{\ell}} \cdot\left(\partial_{\mathrm{wirt}}^{|\boldsymbol{\mathbf{m}}|}\overline{ \partial}_{\mathrm{wirt}}^{|\boldsymbol{\ell}|}\phi\right)\left(w^{T}z+b\right)\right)\] \[=z^{\boldsymbol{\mathbf{m}}}\overline{z}^{\boldsymbol{\ell}} \cdot\partial_{\mathrm{wirt}}^{e_{j}}\left[w\mapsto\left(\partial_{\mathrm{wirt }}^{|\boldsymbol{\mathbf{m}}|}\overline{\partial}_{\mathrm{wirt}}^{|\boldsymbol{ \mathbf{\ell}}|}\phi\right)\left(w^{T}z+b\right)\right].\]

Using the chain rule again, we calculate

\[\partial_{\mathrm{wirt}}^{e_{j}}\left[w\mapsto\left(\partial_{ \mathrm{wirt}}^{|\boldsymbol{\mathbf{m}}|}\overline{\partial}_{\mathrm{wirt}}^{| \boldsymbol{\mathbf{\ell}}|}\phi\right)\left(w^{T}z+b\right)\right]\] \[=\left(\partial_{\mathrm{wirt}}^{|\boldsymbol{\mathbf{m}}|+1} \overline{\partial}_{\mathrm{wirt}}^{|\boldsymbol{\mathbf{\ell}}|}\phi\right) \left(w^{T}z+b\right)\cdot\partial_{\mathrm{wirt}}^{e_{j}}\left[w\mapsto w^{T }z+b\right]\] \[=z^{e_{j}}\cdot\left(\partial_{\mathrm{wirt}}^{|\boldsymbol{ \mathbf{m}}|+1}\overline{\partial}_{\mathrm{wirt}}^{|\boldsymbol{\mathbf{\ell}}|} \phi\right)\left(w^{T}z+b\right)+\left(\partial_{\mathrm{wirt}}^{|\boldsymbol{ \mathbf{m}}|}\overline{\partial}_{\mathrm{wirt}}^{|\boldsymbol{\mathbf{\ell}}|+1 }\phi\right)\left(w^{T}z+b\right)\cdot\overline{\overline{\partial}_{ \mathrm{wirt}}^{e_{j}}\left[w\mapsto w^{T}z+b\right]}\] \[=z^{e_{j}}\cdot\left(\partial_{\mathrm{wirt}}^{|\boldsymbol{ \mathbf{m}}|+1}\overline{\partial}_{\mathrm{wirt}}^{|\boldsymbol{\mathbf{\ell}}|} \phi\right)\left(w^{T}z+b\right).\]

By induction, this proves the claim. 

As the last preparation for the proof of Theorem 3.1, we need the following lemma.

**Lemma B.7**.: _Let \(\phi:\mathbb{C}\to\mathbb{C}\) and \(\delta>0,\;b\in\mathbb{C},\;k\in\mathbb{N}_{0}\), such that \(\phi\big{|}_{B_{\delta}(b)}\in C^{k}\left(B_{\delta}(b);\mathbb{C}\right)\). Let \(m,n\in\mathbb{N}\) and \(\varepsilon>0\). For \(\boldsymbol{p}\in\mathbb{N}_{0}^{2n},h>0\) and \(z\in\Omega_{n}\) we write_

\[\Phi_{\boldsymbol{p},h}(z) :=(2h)^{-|\boldsymbol{p}|}\sum_{0\leq r\leq\boldsymbol{p}}(-1)^{ |\boldsymbol{p}|-|\boldsymbol{r}|}\begin{pmatrix}\boldsymbol{p}\\ \boldsymbol{r}\end{pmatrix}\left(\phi_{z}\circ\varphi_{n}\right)\left(h(2 \boldsymbol{r}-\boldsymbol{p})\right)\] \[=(2h)^{-|\boldsymbol{p}|}\sum_{0\leq r\leq\boldsymbol{p}}(-1)^{ |\boldsymbol{p}|-|\boldsymbol{r}|}\begin{pmatrix}\boldsymbol{p}\\ \boldsymbol{r}\end{pmatrix}\phi\left(\left[\varphi_{n}\left(h(2\boldsymbol{r}- \boldsymbol{p})\right)\right]^{T}\cdot z+b\right),\]

_where \(\phi_{z}\) is as introduced in Lemma B.6 and \(\varphi_{n}\) is as in (A.1). Furthermore, let_

\[\phi_{\boldsymbol{p}}:\quad\Omega_{n}\times B_{\frac{\delta}{\sqrt{2n}}}(0) \to\mathbb{C},\quad(z,w)\mapsto\partial^{\boldsymbol{p}}\phi_{z}(w).\]

_Then there exists \(h^{*}>0\) such that_

\[\|\Phi_{\boldsymbol{p},h}-\phi_{\boldsymbol{p}}(\cdot,0)\|_{L^{\infty}(\Omega_{n };\mathbb{C})}\leq\varepsilon\]

_for all \(\boldsymbol{p}\in\mathbb{N}_{0}^{2n}\) with \(|\boldsymbol{p}|\leq k\) and \(\boldsymbol{p}\leq m\) and \(h\in(0,h^{*})\)._

[MISSING_PAGE_FAIL:26]

with \(|a_{\bm{m},\bm{\ell}}|\leq c\) for some constant \(c=c(\mathcal{P}^{\prime})>0\). In combination with (B.6), this easily implies that we can rewrite \(p\) as

\[p(z)=\sum_{\begin{subarray}{c}\mathbf{p}\in\mathbb{N}_{0}^{2n}\\ \mathbf{p}\leq 2m\end{subarray}}c_{\mathbf{p}}(p)\partial^{\mathbf{p}}\phi_{z}(0)\] (B.7)

with coefficients \(c_{\mathbf{p}}(p)\in\mathbb{C}\) satisfying \(|c_{\mathbf{p}}(p)|\leq c^{\prime}\) for some constant \(c^{\prime}=c^{\prime}(\phi,b,\mathcal{P}^{\prime},m,n)\). By Lemma B.7, we choose \(h^{*}>0\), such that

\[|\Phi_{\mathbf{p},h^{*}}(z)-\partial^{\mathbf{p}}\phi_{z}(0)|\leq\frac{ \varepsilon}{\sum_{\mathbf{q}\in\mathbb{N}_{0}^{2n},\mathbf{q}\leq 2m}c^{ \prime}}\]

for all \(z\in\Omega_{n}\) and \(\mathbf{p}\in\mathbb{N}_{0}^{2n}\) with \(\mathbf{p}\leq 2m\). Furthermore, we can rewrite each function \(\Phi_{\mathbf{p},h^{*}}\) as

\[\Phi_{\mathbf{p},h^{*}}(z)=\sum_{\begin{subarray}{c}\bm{\alpha}\in\mathbb{Z} ^{2n}\\ |\alpha_{j}|\leq 2m\ \forall j\end{subarray}}\lambda_{\bm{\alpha},\mathbf{p}} \phi(\left[\varphi_{n}\left(h^{*}\bm{\alpha}\right)\right]^{T}\cdot z+b)\]

with suitable coefficients \(\lambda_{\bm{\alpha},\mathbf{p}}\in\mathbb{C}\). Since the cardinality of the set

\[\left\{\bm{\alpha}\in\mathbb{Z}^{2n}:\ |\bm{\alpha}_{j}|\leq 2m\ \forall j\right\}\]

is \((4m+1)^{2n}\), this can be converted to

\[\Phi_{\mathbf{p},h^{*}}(z)=\sum_{j=1}^{N}\lambda_{j,\mathbf{p}}\phi\left( \rho_{j}^{T}\cdot z+b\right).\]

For \(p\) as in (B.7), we then define

\[\theta(z):=\sum_{\begin{subarray}{c}\mathbf{p}\in\mathbb{N}_{0}^{2n}\\ \mathbf{p}\leq 2m\end{subarray}}c_{\mathbf{p}}(p)\cdot\Phi_{\mathbf{p},h^{*}}(z )=\sum_{j=1}^{N}\left[\left(\sum_{\begin{subarray}{c}\mathbf{p}\in\mathbb{N} _{0}^{2n}\\ \mathbf{p}\leq 2m\end{subarray}}c_{\mathbf{p}}(p)\lambda_{j,\mathbf{p}} \right)\phi(\rho_{j}^{T}\cdot z+b)\right]\]

and note

\[|\theta(z)-p(z)|\leq\sum_{\mathbf{p}\leq 2m}|c_{\mathbf{p}}(p)|\cdot|\Phi_{ \mathbf{p},h^{*}}(z)-\partial^{\mathbf{p}}\phi_{z}(0)|\leq\varepsilon.\]

Since the coefficients \(\rho_{j}\) have been chosen independently of the polynomial \(p\), we can rewrite \(\theta\) in the desired form. 

## Appendix C Postponed proofs for the approximation of \(C^{k}\)-functions

### Prerequisites from Fourier Analysis

This section is dedicated to reviewing some notations and results from Fourier Analysis. In the end, a quantitative result for the approximation of \(C^{k}\left([-1,1]^{s};\mathbb{R}\right)\)-functions using linear combinations of multivariate Chebyshev polynomials is derived; see Theorem C.15.

We start by recalling several notations and concepts from Fourier Analysis.

**Definition C.1**.: Let \(s\in\mathbb{N}\) and \(k\in\mathbb{N}_{0}\). We define

\[C^{k}_{2\pi}\left(\mathbb{R}^{s};\mathbb{C}\right):=\left\{f\in C^{k}\left( \mathbb{R}^{s};\mathbb{C}\right):\ \forall\mathbf{p}\in\mathbb{Z}^{s}\ \forall x\in\mathbb{R}^{s}:\ f(x+2\pi \mathbf{p})=f(x)\right\}.\]

and \(C_{2\pi}\left(\mathbb{R}^{s};\mathbb{C}\right):=C^{0}_{2\pi}\left(\mathbb{R}^{ s};\mathbb{C}\right)\). For a function \(f\in C^{k}_{2\pi}\left(\mathbb{R}^{s};\mathbb{C}\right)\) we write

\[\|f\|_{C^{k}\left([-\pi,\pi]^{s};\mathbb{C}\right)}:=\max_{ \begin{subarray}{c}\mathbf{k}\in\mathbb{N}_{0}^{2n}\\ |\mathbf{k}|\leq k\end{subarray}}\|\partial^{\mathbf{k}}f\|_{L^{\infty}\left([- \pi,\pi]^{s};\mathbb{C}\right)}\ \text{and}\] \[\|f\|_{L^{p}\left([-\pi,\pi]^{s};\mathbb{C}\right)}:=\left(\frac{1} {(2\pi)^{s}}\cdot\int_{[-\pi,\pi]^{s}}|f(x)|^{p}\,dx\right)^{1/p}\ \text{for}\ p\in[1,\infty).\]

Moreover, we set \(\|f\|_{L^{\infty}\left([-\pi,\pi]^{s};\mathbb{R}\right)}:=\|f\|_{C^{0}\left([- \pi,\pi]^{s};\mathbb{C}\right)}\).

**Definition C.2**.: For any \(s\in\mathbb{N}\) and \(\textbf{k}\in\mathbb{Z}^{s}\), we write

\[e_{\textbf{k}}:\quad\mathbb{R}^{s}\rightarrow\mathbb{C},\quad e_{\textbf{k}}(x) =e^{i\left\langle\textbf{k},x\right\rangle}\]

where \(\left\langle\cdot,\cdot\right\rangle\) denotes the usual inner product of two vectors. For any \(f\in C_{2\pi}\left(\mathbb{R}^{s};\mathbb{C}\right)\) we define the \(\textbf{k}\)-th Fourier coefficient of \(f\) to be

\[\hat{f}(\textbf{k}):=\frac{1}{(2\pi)^{s}}\int_{[-\pi,\pi]^{s}}f(x)\overline{e_ {\textbf{k}}(x)}dx.\]

**Definition C.3**.: For two functions \(f,g\in C_{2\pi}\left(\mathbb{R}^{s};\mathbb{C}\right)\), we define their convolution as

\[f*g:\quad\mathbb{R}^{s}\rightarrow\mathbb{C},\quad(f*g)(x):=\frac{1}{(2\pi)^{ s}}\int_{[-\pi,\pi]^{s}}f(t)g(x-t)dt.\]

In the following we define several so-called kernels.

**Definition C.4**.: Let \(m\in\mathbb{N}_{0}\) be arbitrary.

1. The \(m\)-th one-dimensional _Dirichlet-kernel_ is defined as \[D_{m}:=\sum_{h=-m}^{m}e_{h}.\]
2. The \(m\)-th one-dimensional _Fejer-kernel_ is defined as \[F_{m}:=\frac{1}{m}\sum_{h=0}^{m-1}D_{h}.\]
3. The \(m\)-th one-dimensional _de-la-Vallee-Poussin-kernel_ is defined as \[V_{m}:=(1+e_{m}+e_{-m})\cdot F_{m}.\]
4. Let \(s\in\mathbb{N}\). We extend the above definitions to dimension \(s\) by letting \[D_{m}^{s}\left(x_{1},...,x_{s}\right):=\prod_{p=1}^{s}D_{m}\left(x_{p}\right),\] \[F_{m}^{s}\left(x_{1},...,x_{s}\right):=\prod_{p=1}^{s}F_{m}\left(x_{p} \right),\] \[V_{m}^{s}\left(x_{1},...,x_{s}\right):=\prod_{p=1}^{s}V_{m}\left(x_{p} \right).\]

We need the following property of the multivariate extension of the de-la-Vallee-Poussin-kernel.

**Proposition C.5**.: _Let \(m,s\in\mathbb{N}\). Then one has \(\|V_{m}^{s}\|_{L^{1}\left([-\pi,\pi]^{s};\mathbb{C}\right)}\leq 3^{s}\)._

Proof.: From [34, Exercise 1.3 and Lemma 1.4] it follows \(\|F_{m}\|_{L^{1}\left([-\pi,\pi];\mathbb{C}\right)}=1\) and hence using the triangle inequality \(\|V_{m}\|_{L^{1}\left([-\pi,\pi];\mathbb{C}\right)}\leq 3\). The claim then follows using Tonelli's theorem. 

The following definition introduces the term of trigonometric polynomial.

**Definition C.6**.: For any \(s\in\mathbb{N}\) and \(m\in\mathbb{N}_{0}\) we call a function of the form

\[\mathbb{R}^{s}\rightarrow\mathbb{C},\quad x\mapsto\sum_{\begin{subarray}{c} \textbf{k}\in\mathbb{Z}_{0}^{s}\\ -m\leq\textbf{k}\leq m\end{subarray}}a_{\textbf{k}}e^{i\left\langle\textbf{k},x\right\rangle}\]

with coefficients \(a_{\textbf{k}}\in\mathbb{C}\) a _trigonometric polynomial of coordinatewise degree at most \(m\)_ and denote the space of all those functions with \(\mathbb{H}_{m}^{s}\). Here, we consider the sum over all \(\textbf{k}\in\mathbb{Z}^{s}\) with \(-m\leq\textbf{k}_{j}\leq m\) for all \(j\in\{1,...,s\}\). We then write

\[E_{m}^{s}(f):=\min_{T\in\mathbb{H}_{m}^{s}}\|f-T\|_{L^{\infty}\left(\mathbb{R} ^{s};\mathbb{C}\right)}\] (C.1)

for any function \(f\in C_{2\pi}\left(\mathbb{R}^{s};\mathbb{C}\right)\).

The following proposition shows that convolving with the Fejer kernel produces a trigonometric polynomial of degree at most \(2m-1\), while reproducing trigonometric polynomials of degree \(m\). Furthermore, the norm of the convolution operator is bounded uniformly in \(m\). These properties will be useful for our proof of Theorem C.15.

**Proposition C.7**.: _Let \(s,m\in\mathbb{N}\) and \(k\in\mathbb{N}_{0}\). The map_

\[v_{m}:\quad C_{2\pi}\left(\mathbb{R}^{s};\mathbb{C}\right)\to \mathbb{H}_{2m-1}^{s},\quad f\mapsto f*V_{m}^{s}\]

_is well-defined and satisfies_

\[v_{m}(T)=T\quad\text{for all}\quad T\in\mathbb{H}_{m}^{s}.\] (C.2)

_Furthermore, there exists a constant \(c=c(s)>0\) (independent of \(m\)), such that_

\[\|v_{m}(f)\|_{C^{k}([-\pi,\pi]^{s};\mathbb{C})}\leq c\cdot\|f\|_{ C^{k}([-\pi,\pi]^{s};\mathbb{C})}\ \forall f\in C_{2\pi}^{k}\left(\mathbb{R}^{s};\mathbb{C}\right),\] \[\|v_{m}(f)\|_{L^{\infty}([\pi,\pi]^{s};\mathbb{C})}\leq c\cdot\|f \|_{L^{\infty}([-\pi,\pi]^{s};\mathbb{C})}\ \forall f\in C_{2\pi}\left(\mathbb{R}^{s};\mathbb{C}\right).\] (C.3)

_In fact, it holds \(c(s)\leq\exp(C\cdot s)\) with an absolute constant \(C>0\)._

Proof.: A direct computation shows that \(f*e_{\mathbf{k}}=\hat{f}(\mathbf{k})\cdot e_{\mathbf{k}}\). This implies that \(v_{m}\) is well-defined since \(V_{m}^{s}\) is a trigonometric polynomial of coordinatewise degree at most \(2m-1\).

The operator is bounded on \(C_{2\pi}^{k}(\mathbb{R}^{s};\mathbb{C})\) and \(C_{2\pi}(\mathbb{R}^{s};\mathbb{C})\) with norm at most \(c=3^{s}\), as follows from Young's inequality [34, Lemma 1.1 (ii)], Proposition C.5, and the fact that one has for all \(\mathbf{k}\in\mathbb{N}_{0}^{s}\) with \(|\mathbf{k}|\leq k\) the identity

\[\partial^{\mathbf{k}}\left(f*V_{m}^{s}\right)=\left(\partial^{ \mathbf{k}}f\right)*V_{m}^{s}\quad\text{for }f\in C_{2\pi}^{k}(\mathbb{R}^{s};\mathbb{C}).\]

It remains to show that \(v_{m}\) is the identity on \(\mathbb{H}_{m}^{s}\). We first prove that

\[e_{k}*V_{m}=e_{k}\] (C.4)

holds for all \(k\in\mathbb{Z}\) with \(|k|\leq m\). First note that

\[e_{k}*V_{m}=e_{k}*F_{m}+e_{k}*\left(e_{m}\cdot F_{m}\right)+e_{k}*\left(e_{-m} \cdot F_{m}\right).\]

We then compute

\[e_{k}*F_{m}=\frac{1}{m}\sum_{\ell=0}^{m-1}D_{\ell}*e_{k}=\frac{1}{m}\sum_{ \ell=0}^{m-1}\sum_{h=-\ell}^{\ell}\underbrace{e_{h}*e_{k}}_{=\delta_{k,h},e_{ k}}=\frac{1}{m}\sum_{\ell=|k|}^{m-1}e_{k}=\frac{m-|k|}{m}\cdot e_{k}.\]

Similarly, we get

\[e_{k}*\left(e_{m}\cdot F_{m}\right) =\frac{1}{m}\sum_{\ell=0}^{m-1}\left(e_{m}D_{\ell}\right)*e_{k}= \frac{1}{m}\sum_{\ell=0}^{m-1}\sum_{h=-\ell}^{\ell}\underbrace{e_{h+m}*e_{k}} _{=\delta_{k,h+m}\cdot e_{k}}\] \[=\frac{1}{m}\sum_{\begin{subarray}{c}0\leq\ell\leq m-1\\ \ell\geq m-k\end{subarray}}e_{k}=\delta_{k\geq 1}\cdot\frac{k}{m}\cdot e_{k}\]

and

\[e_{k}*\left(e_{-m}\cdot F_{m}\right) =\frac{1}{m}\sum_{\ell=0}^{m-1}\left(e_{-m}D_{\ell}\right)*e_{k}= \frac{1}{m}\sum_{\ell=0}^{m-1}\sum_{h=-\ell}^{\ell}\underbrace{e_{h-m}*e_{k}} _{=\delta_{k,h-m}\cdot e_{k}}\] \[=\frac{1}{m}\sum_{\begin{subarray}{c}0\leq\ell\leq m-1\\ \ell\geq k+m\end{subarray}}e_{k}=\delta_{k\leq-1}\cdot\frac{-k}{m}\cdot e_{k}.\]

Adding up those three identities yields (C.4).

To finally prove (C.2), it clearly suffices to show that

\[e_{\mathbf{k}}*V_{m}^{s}=e_{\mathbf{k}}\]for all \(\mathbf{k}\in\mathbb{Z}^{s}\) with \(-m\leq\mathbf{k}\leq m\). But for such \(\mathbf{k}\), using \(e_{\mathbf{k}}(x)=\prod_{j=1}^{s}e_{\mathbf{k}_{j}}\left(x_{j}\right)\), one obtains

\[\left(e_{\mathbf{k}}*V_{m}^{s}\right)\left(x\right) =\frac{1}{(2\pi)^{s}}\int_{[-\pi,\pi]^{s}}\prod_{j=1}^{s}e_{ \mathbf{k}_{j}}\left(t_{j}\right)\cdot V_{m}\left(x_{j}-t_{j}\right)dt\] \[\stackrel{{\text{Fubini}}}{{=}}\prod_{j=1}^{s}\left( e_{\mathbf{k}_{j}}*V_{m}\right)\left(x_{j}\right)\stackrel{{\text{(C.4)}}}{{=}}\prod_{j=1}^{s}e_{\mathbf{k}_{j}}\left(x_{j}\right)=e_{\mathbf{k}}(x)\]

for any \(x\in\mathbb{R}^{s}\), as was to be shown. 

The following result follows from a theorem in [29].

**Proposition C.8**.: _Let \(s,k\in\mathbb{N}\). Then there exists a constant \(c=c(s,k)>0\), such that, for \(E_{m}^{s}\) as defined in (C.1),_

\[E_{m}^{s}(f)\leq\frac{c}{m^{k}}\cdot\left\|f\right\|_{C^{k}([-\pi,\pi]^{s}; \mathbb{R})}\]

_for all \(m\in\mathbb{N}\) and \(f\in C_{2\pi}^{k}\left(\mathbb{R}^{s};\mathbb{R}\right)\). In fact, it holds \(c(s,k)\leq\exp(C\cdot ks)\cdot k^{k}\) with an absolute constant \(C>0\)._

Proof.: We apply [29, Theorem 6.6] with \(n_{i}=m\) and \(p_{i}=k\), which yields the existence of a constant \(c_{1}=c_{1}(s,k)>0\), such that

\[E_{m}^{s}(f)\leq c_{1}\cdot\sum_{\ell=1}^{s}\frac{1}{m^{k}}\cdot\omega_{\ell} \left(f,\frac{1}{m}\right)\]

for all \(m\in\mathbb{N}\) and \(f\in C_{2\pi}^{k}(\mathbb{R}^{s};\mathbb{R})\), where \(\omega_{\ell}(f,\bullet)\) denotes the modulus of continuity of \(\frac{\partial^{k}f}{\partial x_{\ell}^{k}}\) with respect to \(x_{\ell}\), where we have the trivial bound

\[\omega_{\ell}\left(f,\frac{1}{m}\right)\leq 2\cdot\left\|f\right\|_{C^{k}([-\pi, \pi]^{s};\mathbb{R})}.\]

Hence, we get

\[E_{m}^{s}(f)\leq c_{1}\cdot s\cdot 2\cdot\left\|f\right\|_{C^{k}([-\pi,\pi]^{s}; \mathbb{R})}\frac{1}{m^{k}},\]

so the claim follows by choosing \(c:=2s\cdot c_{1}\).

We refer to Appendix C.2 (see Theorem C.18) for a proof of the claimed bound on the constant \(c(s,k)\). 

The above proposition bounds the best possible error of approximating \(f\) by trigonometric polynomials of coordinatewise degree at most \(m\), but this is in general non-constructive. Our next result shows that a similar bound holds for approximating \(f\) by \(v_{m}(f)\).

**Theorem C.9**.: _Let \(s\in\mathbb{N}\). Then there exists a constant \(c=c(s)>0\), such that the operator \(v_{m}\) from Proposition C.7 satisfies_

\[\left\|f-v_{m}(f)\right\|_{L^{\infty}(\mathbb{R}^{s})}\leq c\cdot E_{m}^{s}(f)\]

_for any \(m\in\mathbb{N}\) and \(f\in C_{2\pi}\left(\mathbb{R}^{s};\mathbb{C}\right)\). In fact, it holds \(c(s)\leq\exp(C\cdot s)\) with an absolute constant \(C>0\)._

Proof.: For any \(T\in\mathbb{H}_{m}^{s}\) one has

\[\left\|f-v_{m}(f)\right\|_{L^{\infty}(\mathbb{R}^{s})}\stackrel{{ \text{(C.2)}}}{{\leq}}\left\|f-T\right\|_{L^{\infty}(\mathbb{R}^{s})}+ \left\|v_{m}(T)-v_{m}(f)\right\|_{L^{\infty}(\mathbb{R}^{s})}\stackrel{{ \text{(C.3)}}}{{\leq}}\left(c+1\right)\left\|f-T\right\|_{L^{\infty}( \mathbb{R}^{s})}.\]

Taking the infimum over all \(T\in\mathbb{H}_{m}^{s}\) yields the claim. 

By combining Proposition C.8 and Theorem C.9, we immediately get the following bound.

**Corollary C.10**.: _Let \(s,k\in\mathbb{N}_{0}\). Then there exists a constant \(c=c(s,k)>0\), such that_

\[\left\|f-v_{m}(f)\right\|_{L^{\infty}(\mathbb{R}^{s})}\leq\frac{c}{m^{k}}\cdot \left\|f\right\|_{C^{k}([-\pi,\pi]^{s};\mathbb{R})}\]

_for every \(m\in\mathbb{N}\) and \(f\in C_{2\pi}^{k}\left(\mathbb{R}^{s};\mathbb{R}\right)\). In fact, we have \(c(s,k)\leq\exp(C\cdot ks)\cdot k^{k}\) with an absolute constant \(C>0\)._

Up to now, we have studied the approximation of periodic functions by trigonometric polynomials, but our actual goal is to approximate non-periodic functions by algebraic polynomials. The next lemma establishes a connection between the two settings.

**Lemma C.11**.: _Let \(k\in\mathbb{N}_{0}\) and \(s\in\mathbb{N}\). For any function \(f\in C^{k}\left([-1,1]^{s};\mathbb{C}\right)\), we define the corresponding periodic function via_

\[f^{*}:\quad\mathbb{R}^{s}\to\mathbb{C},\quad f^{*}\left(x_{1},...,x_{s}\right) =f(\cos\left(x_{1}\right),...,\cos\left(x_{s}\right))\]

_and note \(f^{*}\in C_{2\pi}^{k}\left(\mathbb{R}^{s};\mathbb{C}\right)\). The map_

\[C^{k}\left([-1,1]^{s};\mathbb{C}\right)\to C_{2\pi}^{k}\left(\mathbb{R}^{s}; \mathbb{C}\right),\quad f\mapsto f^{*}\]

_is a continuous linear operator with respect to the \(C^{k}\)-norms on \(C^{k}\left([-1,1]^{s};\mathbb{C}\right)\) and \(C_{2\pi}^{k}\left(\mathbb{R}^{s};\mathbb{C}\right)\). The operator norm can be bounded from above by \(k^{k}\)._

Proof.: The map is well-defined since \(\cos\) is a smooth function and \(2\pi\)-periodic. The linearity of the operator is obvious, so it remains to show its continuity.

The goal is to apply the closed graph theorem [21, Theorem 5.12]. By definition of \(f^{*}\), and since \(\cos:[-\pi,\pi]\to[-1,1]\) is surjective, we have the equality \(\left\|f\right\|_{L^{\infty}([-1,1]^{s};\mathbb{C})}=\left\|f^{*}\right\|_{L^{ \infty}([-\pi,\pi]^{s};\mathbb{C})}\). Let then \(\left(f_{n}\right)_{n\in\mathbb{N}}\) be a sequence of functions \(f_{n}\in C^{k}\left([-1,1]^{s};\mathbb{C}\right)\) and \(g^{*}\in C_{2\pi}^{k}\left(\mathbb{R}^{s};\mathbb{C}\right)\) such that \(f_{n}\to f\) in \(C^{k}\left([-1,1]^{s};\mathbb{C}\right)\) and \(f_{n}^{*}\to g^{*}\) in \(C_{2\pi}^{k}\left(\mathbb{R}^{s};\mathbb{C}\right)\). We then have

\[\left\|f^{*}-g^{*}\right\|_{L^{\infty}([-\pi,\pi]^{s})} \leq\left\|f^{*}-f_{n}^{*}\right\|_{L^{\infty}([-\pi,\pi]^{s})}+ \left\|f_{n}^{*}-g^{*}\right\|_{L^{\infty}([-\pi,\pi]^{s})}\] \[=\left\|f-f_{n}\right\|_{L^{\infty}([-1,1]^{s};\mathbb{C})}+ \left\|f_{n}^{*}-g^{*}\right\|_{L^{\infty}([-\pi,\pi]^{s})}\] \[\leq\left\|f-f_{n}\right\|_{C^{k}([-1,1]^{s};\mathbb{C})}+\left\|f _{n}^{*}-g^{*}\right\|_{C^{k}([-\pi,\pi]^{s};\mathbb{C})}\to 0\,\,(n\to\infty).\]

It follows \(f^{*}=g^{*}\) and the closed graph theorem yields the desired continuity.

We refer to Appendix C.2 (see Theorem C.19 and Remark C.20) for a proof of the claimed bound on the operator norm. 

For a function \(f\in C^{k}([-1,1]^{s};\mathbb{C})\) we want to express \(v_{m}(f^{*})\) in a convenient way, involving a product of cosines. To this end, we make use of the following identity, which is a generalization of the well-known product-to-sum formula for \(\cos\).

**Lemma C.12**.: _Let \(s\in\mathbb{N}\). Then it holds for any \(x\in\mathbb{R}^{s}\) that_

\[\prod_{j=1}^{s}\cos(x_{j})=\frac{1}{2^{s}}\sum_{\sigma\in\{-1,1\}^{s}}\cos( \langle\sigma,x\rangle).\]

Proof.: This is an inductive generalization of the product-to-sum formula

\[2\cos(x)\cos(y)=\cos(x-y)+\cos(x+y)\] (C.5)

for \(x,y\in\mathbb{R}\), which can be found for instance in [1, Eq. 4.3.32]. The case \(s=1\) holds since \(\cos\) is an even function. Assume that the claim holds for a fixed \(s\in\mathbb{N}\) and take \(x\in\mathbb{R}^{s+1}\). Writing\(x^{\prime}=\left(x_{1},...,x_{s}\right)\), we derive

\[\prod_{j=1}^{s+1}\cos(x_{j}) =\left(\frac{1}{2^{s}}\sum_{\sigma\in\left\{-1,1\right\}^{s}}\cos( \langle\sigma,x^{\prime}\rangle)\right)\cdot\cos(x_{s+1})\] \[=\frac{1}{2^{s}}\sum_{\sigma\in\left\{-1,1\right\}^{s}}\cos( \langle\sigma,x^{\prime}\rangle)\cos(x_{s+1})\] \[\stackrel{{\eqref{eq:c_1}}}{{=}}\frac{1}{2^{s+1}} \sum_{\sigma\in\left\{-1,1\right\}^{s}}\left[\cos(\langle\sigma,x^{\prime} \rangle+x_{s+1})+\cos(\langle\sigma,x^{\prime}\rangle-x_{s+1})\right]\] \[=\frac{1}{2^{s+1}}\sum_{\sigma\in\left\{-1,1\right\}^{s+1}}\cos( \langle\sigma,x\rangle),\]

as was to be shown. 

The following proposition states that \(v_{m}(f^{*})\) can be expressed as a linear combination of products of cosines. This representation is useful since these cosines can be interpolated by Chebyshev polynomials which in the end leads to the desired approximation result.

**Proposition C.13**.: _Let \(s\in\mathbb{N}\) and \(k\in\mathbb{N}_{0}\). For any \(f\in C^{k}\left([-1,1]^{s};\mathbb{C}\right)\) and \(m\in\mathbb{N}\) the de-la-Vallee-Poussin operator given as \(f\mapsto v_{m}\left(f^{*}\right)\) with \(v_{m}\) as in Proposition C.7 and \(f\mapsto f^{*}\) as in Lemma C.11 has a representation_

\[v_{m}\left(f^{*}\right)\left(x_{1},...,x_{s}\right)=\sum_{ \begin{subarray}{c}\bm{k}\in\mathbb{N}_{0}^{s}\\ \bm{k}\leq 2m-1\end{subarray}}\mathcal{V}_{\bm{k}}^{m}(f)\prod_{j=1}^{s}\cos \left(\bm{k}_{j}x_{j}\right)\]

_for continuous linear functionals_

\[\mathcal{V}_{\bm{k}}^{m}:\ C^{k}\left([-1,1]^{s};\mathbb{C}\right)\to \mathbb{C},\quad f\mapsto 2^{\left\|\bm{k}\right\|_{0}}\cdot a_{\bm{k}}^{m} \cdot\widehat{f^{*}}(\bm{k}),\]

_where \(\left\|\bm{k}\right\|_{0}=\#\{j\in\{1,...,s\}:\bm{k}_{j}\neq 0\}\) and \(a_{\bm{k}}^{m}=\widehat{V_{m}^{s}}(\bm{k})\). Furthermore, if \(f\in C^{k}([-1,1]^{s};\mathbb{R})\), then \(\mathcal{V}_{\bm{k}}^{m}(f)\in\mathbb{R}\) for every \(\bm{k}\in\mathbb{N}_{0}^{s}\) with \(\bm{k}\leq 2m-1\)._

Proof.: First of all, it is easy to see that \(v_{m}\left(f^{*}\right)\) is even in each variable, which follows directly from the fact that \(f^{*}\) and \(V_{m}^{s}\) are both even in each variable. Furthermore, if we write

\[V_{m}^{s}=\sum_{\begin{subarray}{c}\bm{k}\in\mathbb{Z}^{s}\\ -\left(2m-1\right)\leq\bm{k}\leq 2m-1\end{subarray}}a_{\bm{k}}^{m}e_{\bm{k}}\]

with appropriately chosen coefficients \(a_{\bm{k}}^{m}\in\mathbb{R}\), we easily see

\[v_{m}\left(f^{*}\right)=\sum_{\begin{subarray}{c}\bm{k}\in\mathbb{Z}^{s}\\ -\left(2m-1\right)\leq\bm{k}\leq 2m-1\end{subarray}}a_{\bm{k}}^{m}\widehat{f^{*}}( \bm{k})e_{\bm{k}}.\]

Using Euler's identity and the fact that \(v_{m}\left(f^{*}\right)\) is an even function, we get the representation

\[v_{m}\left(f^{*}\right)\left(x\right)=\sum_{\begin{subarray}{c}\bm{k}\in \mathbb{Z}^{s}\\ -\left(2m-1\right)\leq\bm{k}\leq 2m-1\end{subarray}}a_{\bm{k}}^{m}\widehat{f^{*}}( \bm{k})\text{cos}(\langle\bm{k},x\rangle)\]

for all \(x\in\mathbb{R}^{s}\). Using \(\odot\) to denote the componentwise product of two vectors of the same size, i.e., \(x\odot y=(x_{i}\cdot y_{i})_{i}\), and using the identity \(\langle\bm{k},\sigma\odot x\rangle=\langle\sigma,\bm{k}\odot x\rangle\), we see since \(v_{m}\left(f^{*}\right)\) is even in 

[MISSING_PAGE_EMPTY:33]

for any number \(k\in\mathbb{N}_{0}\) shows

\[\widehat{g_{\mathbf{k}}}(\boldsymbol{\ell})=\begin{cases}\frac{1}{2^{|\mathbf{k} |_{0}}},&\mathbf{k}=\boldsymbol{\ell},\\ 0,&\text{otherwise}\end{cases}\quad\text{ for }\mathbf{k},\boldsymbol{\ell}\in \mathbb{N}_{0}^{s}.\]

Therefore, we have the bound \(|\mathcal{V}_{\boldsymbol{\ell}}^{m}(f)|\leq 2^{s}\cdot\left|\widehat{v_{m}\left(f^{*} \right)}(\boldsymbol{\ell})\right|\) for \(\boldsymbol{\ell}\in\mathbb{N}_{0}^{s}\) with \(|\boldsymbol{\ell}|\leq 2m-1\). Using the Cauchy-Schwarz and the Parseval inequality, we therefore see

\[\sum_{\begin{subarray}{c}\mathbf{k}\in\mathbb{N}_{0}^{s}\\ \mathbf{k}\leq 2m-1\end{subarray}}|\mathcal{V}_{\mathbf{k}}^{m}(f)| \leq 2^{s}\cdot\sum_{\begin{subarray}{c}\mathbf{k}\in\mathbb{N}_{0}^{s }\\ \mathbf{k}\leq 2m-1\end{subarray}}\left|\widehat{v_{m}\left(f^{*}\right)}( \mathbf{k})\right|\leq\frac{\text{CS}}{2}^{s}\cdot(2m)^{s/2}\cdot\left(\sum_ {\begin{subarray}{c}\mathbf{k}\in\mathbb{N}_{0}^{s}\\ \mathbf{k}\leq 2m-1\end{subarray}}\left|\widehat{v_{m}\left(f^{*}\right)}( \mathbf{k})\right|^{2}\right)^{1/2}\] \[\overset{\text{ Parseval}}{\leq 2^{s}\cdot 2^{s/2}\cdot m^{s/2}\cdot\|v_{m} \left(f^{*}\right)\|_{L^{2}([-\pi,\pi]^{s};\mathbb{C})}}\] \[\leq\underbrace{2^{s}\cdot 2^{s/2}\cdot m^{s/2}\cdot\|v_{m} \left(f^{*}\right)\|_{L^{\infty}([-\pi,\pi]^{s};\mathbb{C})}}_{=:c_{1}(s)}.\]

Using Proposition C.7, we get a constant \(c_{2}(s)\leq\exp(C_{0}\cdot s)\) such that

\[\sum_{\begin{subarray}{c}\mathbf{k}\in\mathbb{N}_{0}^{s}\\ \mathbf{k}\leq 2m-1\end{subarray}}|\mathcal{V}_{\mathbf{k}}^{m}(f)|\leq c_{1}(s) \cdot c_{2}(s)\cdot m^{s/2}\cdot\|f^{*}\|_{L^{\infty}([-\pi,\pi]^{s};\mathbb{ C})}=c(s)\cdot m^{s/2}\cdot\|f\|_{L^{\infty}([-1,1]^{s};\mathbb{C})},\]

as claimed. 

For any natural number \(\ell\in\mathbb{N}_{0}\), we denote by \(T_{\ell}\) the \(\ell\)-th _Chebyshev polynomial_, satisfying

\[T_{\ell}\left(\cos(x)\right)=\cos(\ell x),\quad x\in\mathbb{R}.\]

One can show that \(T_{\ell}\) is in fact a polynomial of degree \(\ell\). For a multi-index \(\mathbf{k}\in\mathbb{N}_{0}^{s}\), we define

\[T_{\mathbf{k}}(x):=\prod_{j=1}^{s}T_{\mathbf{k}_{j}}\left(x_{j}\right),\quad x \in\mathbb{R}^{s}.\]

We then get the following approximation result about approximating (non-periodic) \(C^{k}\)-functions by linear combinations of Chebyshev polynomials.

**Theorem C.15**.: _Let \(k,s,m\in\mathbb{N}\). Then there exists a constant \(c=c(s,k)>0\) with the following property: For any \(f\in C^{k}\left([-1,1]^{s};\mathbb{R}\right)\) the polynomial \(P\) defined as_

\[P(x):=\sum_{\begin{subarray}{c}\mathbf{k}\in\mathbb{N}_{0}^{s}\\ \mathbf{k}\leq 2m-1\end{subarray}}\mathcal{V}_{\mathbf{k}}^{m}(f)\cdot T_{ \mathbf{k}}(x),\]

_with \(\mathcal{V}_{\mathbf{k}}^{m}\) as in Proposition C.13, satisfies_

\[\|f-P\|_{L^{\infty}([-1,1]^{s};\mathbb{R})}\leq\frac{c}{m^{k}}\cdot\|f\|_{C^{k }([-1,1]^{s};\mathbb{R})}.\]

_Here, the maps_

\[C\left([-1,1]^{s};\mathbb{R}\right)\to\mathbb{R},\quad f\mapsto\mathcal{V}_{ \mathbf{k}}^{m}(f)\]

_are continuous and linear functionals with respect to the \(L^{\infty}\)-norm. Furthermore, there exists a constant \(\tilde{c}=\tilde{c}(s)>0\), such that the inequality_

\[\sum_{\begin{subarray}{c}\mathbf{k}\in\mathbb{N}_{0}^{s}\\ \mathbf{k}\leq 2m-1\end{subarray}}|\mathcal{V}_{\mathbf{k}}^{m}(f)|\leq\tilde{c} \cdot m^{s/2}\cdot\|f\|_{L^{\infty}([-1,1]^{s};\mathbb{R})}\]

_holds for all \(f\in C\left([-1,1]^{s};\mathbb{R}\right)\)._

_Moreover, we have \(c(s,k)\leq\exp(C\cdot ks)\cdot k^{2k}\) and \(\tilde{c}(s)\leq\exp(C\cdot s)\) with an absolute constant \(C>0\)._Proof.: We choose the constant \(c_{0}=c_{0}(s,k)\) according to Corollary C.10. Let \(f\in C^{k}\left([-1,1]^{*};\mathbb{R}\right)\) be arbitrary. Then we define the corresponding function \(f^{*}\in C^{k}_{2\pi}\left(\mathbb{R}^{*};\mathbb{R}\right)\) as above. Let \(P\) be defined as in the statement of the theorem. Then it follows from the definition of the Chebyshev polynomials \(T_{\mathbf{k}}\), the definition of \(P\), and the formula for \(v_{m}(f^{*})\) from Proposition C.13 that

\[P^{*}(x)=v_{m}\left(f^{*}\right)(x)\]

is satisfied, where \(P^{*}\) is the corresponding function to \(P\) defined similarly to \(f^{*}\). Overall, we get the bound

\[\left\|f-P\right\|_{L^{\infty}\left([-1,1]^{*};\mathbb{R}\right)}=\left\|f^{*} -P^{*}\right\|_{L^{\infty}\left([-\pi,\pi]^{*};\mathbb{R}\right)}\stackrel{{ \text{Cor. C.10}}}{{\leq}}\frac{c_{0}}{m^{k}}\cdot\left\|f^{*} \right\|_{C^{k}\left([-\pi,\pi]^{*};\mathbb{R}\right)}.\]

The first claim then follows using the continuity of the map \(f\mapsto f^{*}\) as proven in Lemma C.11. The second part of the theorem has already been proven in Lemma C.14. 

### Details on bounding the constant \(c\) in Theorem 3.2

In this appendix we provide details on the bound of the constant \(c\) that appears in the formulation of Theorem 3.2. Specifically, we perform a careful investigation of several results from [29] to get an upper bound for the constant appearing in Proposition C.8. Moreover, we analyze the operator norm of the operator

\[C^{k}([-1,1]^{*};\mathbb{C})\to C^{k}_{2\pi}(\mathbb{R}^{*};\mathbb{C}),\quad f \mapsto f^{*}\quad\text{with}\quad f^{*}(x):=f(\cos(x_{1}),...,\cos(x_{s}))\]

appearing in Lemma C.11 and show that it is bounded from above by \(k^{k}\).

We start with the analysis of some bounds in [29, Chapter 4.3]. Here, a generalization of _Jackson's kernel_ is defined for any \(m,r\in\mathbb{N}\) as

\[L_{m,r}(t):=\lambda_{m,r}^{-1}\cdot\left(\frac{\sin(mt/2)}{\sin(t/2)}\right)^{ 2r},\quad t\in\mathbb{R},\]

where \(\lambda_{m,r}\) is chosen such that

\[\int_{[-\pi,\pi]}L_{m,r}(t)\;dt=1.\]

The first two important bounds are provided in the following proposition.

**Proposition C.16**.: _Let \(m,r\in\mathbb{N}\). Then it holds_

\[\lambda_{m,r}^{-1}\leq\exp(C\cdot r)\cdot m^{1-2r}\quad\text{and}\quad\int_{ [0,\pi]}t^{k}L_{m,r}(t)\;dt\leq\exp(C\cdot r)\cdot m^{-k}\]

_for any \(k\leq 2r-2\), with an absolute constant \(C>0\)._

Proof.: Since \(L_{m,r}\geq 0\) and since \(\sin(t/2)\leq t/2\) for \(t\in[0,\pi]\), we get

\[\lambda_{m,r} \geq\int_{[0,\pi]}\left(\frac{\sin(mt/2)}{t/2}\right)^{2r}\;dt=2 ^{2r}\cdot\int_{[0,\pi]}\left(\frac{\sin(mt/2)}{t}\right)^{2r}\;dt\] \[=2^{2r}\cdot\int_{[0,\pi m/2]}\left(\frac{\sin(u)}{(2u)/m}\right) ^{2r}\;du\cdot\frac{2}{m}\geq m^{2r-1}\cdot\int_{[0,\pi m/2]}\left(\frac{\sin (u)}{u}\right)^{2r}\;du\] \[\geq m^{2r-1}\cdot\int_{[0,\pi/2]}\left(\frac{\sin(u)}{u}\right) ^{2r}\;du\geq m^{2r-1}\cdot\int_{[0,\pi/2]}\left(\frac{2u}{\pi\cdot u}\right) ^{2r}\;du\geq\left(\frac{2}{\pi}\right)^{2r}\cdot m^{2r-1}.\]

Here, we employed the inequality \(\sin(u)\geq\frac{2}{\pi}u\) for \(u\in[0,\pi/2]\) in the penultimate step.1 This shows the first part of the claim.

For the second part, we again use the estimate \(\sin(u)\geq\frac{2}{\pi}u\) for \(u\in[0,\pi/2]\) to compute

\[\int_{[0,\pi]}t^{k}L_{m,r}(t)\;dt =\lambda_{m,r}^{-1}\cdot\int_{[0,\pi]}t^{k}\left(\frac{\sin(mt/2)} {\sin(t/2)}\right)^{2r}\;dt\leq\lambda_{m,r}^{-1}\cdot\int_{[0,\pi]}t^{k} \left(\frac{\sin(mt/2)}{t/\pi}\right)^{2r}\;dt\] \[=\lambda_{m,r}^{-1}\cdot\pi^{2r}\cdot\int_{[0,\pi]}t^{k-2r}\sin( mt/2)^{2r}\;dt\] \[=\lambda_{m,r}^{-1}\cdot\pi^{2r}\cdot\int_{[0,\pi m/2]}\left( \frac{2u}{m}\right)^{k-2r}\sin(u)^{2r}\;du\cdot\frac{2}{m}\] \[\leq\lambda_{m,r}^{-1}\cdot\pi^{2r}\cdot m^{2r-1-k}\int_{[0,\pi m /2]}u^{k-2r}\sin(u)^{2r}\;du\] \[\leq\exp(C_{1}\cdot r)\cdot\int_{[0,\infty)}u^{k-2r}\cdot\sin(u )^{2r}\;du\cdot m^{-k}\]

with an absolute constant \(C_{1}>0\). Here, we employed the first part of this proposition. It remains to bound the integral. This is done via

\[\int_{[0,\infty)}u^{k-2r}\cdot\sin(u)^{2r}\;du =\int_{[0,1]}u^{k-2r}\cdot\sin(u)^{2r}\;du+\int_{[1,\infty)}u^{k-2 r}\cdot\sin(u)^{2r}\;du\] \[\leq\int_{[0,1]}\underbrace{u^{k}\cdot\left(\frac{\sin(u)}{u} \right)^{2r}}_{\leq 1}\;du+\int_{[1,\infty)}u^{-2}\;du\leq C_{2}\]

with an absolute constant \(C_{2}>0\). This proves the claim. 

The proof in [29] proceeds by defining

\[K_{m,r}(t):=L_{m^{\prime},r}(t),\quad m^{\prime}=\Big{\lfloor}\frac{m}{r} \Big{\rfloor}+1.\]

Proposition C.16 shows for \(k\leq 2r-2\) that

\[\int_{[0,\pi]}t^{k}K_{m,r}(t)\;dt\leq\exp(C\cdot r)\cdot(m^{\prime})^{-k}.\]

Since \(m^{\prime}\geq\frac{m}{r}\) we infer

\[\int_{[0,\pi]}t^{k}K_{m,r}(t)\;dt\leq\exp(C\cdot r)\cdot\left(\frac{r}{m} \right)^{k}\leq\exp(C\cdot r)\cdot r^{k}\cdot m^{-k}\] (C.6)

with an absolute constant \(C>0\).

We can now quantify the constant appearing in [29, Theorem 4.3].

**Theorem C.17** (cf. [29, Theorem 4.3]).: _Let \(k,m\in\mathbb{N}\) and \(f\in C_{2\pi}^{k}(\mathbb{R};\mathbb{R})\). Let_

\[\omega(f^{(k)},1/m):=\max_{x\in\mathbb{R},|t|\leq 1/m}|f^{(k)}(x+t)-f^{(k)}(x)|.\]

_Then it holds_

\[E^{1}_{m}(f)\leq(\exp(C\cdot k)\cdot k^{k})\cdot m^{-k}\cdot\omega(f^{(k)},1/ m).\]

_Here, we recall that \(E^{1}_{m}(f)\) denotes the best possible approximation error when approximating \(f\) using trigonometric polynomials of degree \(m\); see Equation (C.1)._

Proof.: We follow the proof of [29, Theorem 4.3]. Take \(r=k+1\) and define

\[I_{m}(x):=-\int_{[-\pi,\pi]}K_{m,r}(t)\sum_{\ell=1}^{k+1}(-1)^{\ell}\binom{k+1 }{\ell}f(x+\ell t)\;dt.\]

Then it is shown in the proof of [29, Theorem 4.3] that \(I_{m}\) is a trigonometric polynomial of degree at most \(m\) and that

\[|f(x)-I_{m}(x)|\leq 2\cdot\omega_{k+1}(f,1/m)\cdot\int_{[0,\pi]}(mt+1)^{k+1}K_{ m,r}(t)\;dt.\]Here, \(\omega_{k+1}(f,1/m)\) denotes the _modulus of smoothness_ of \(f\) as defined on [29, p. 47]. The integral can be bounded via

\[\int_{[0,\pi]}(mt+1)^{k+1}K_{m,r}(t)\;dt\] \[=\int_{[0,1/m]}(\underbrace{mt+1}_{\leq 2})^{k+1}K_{m,r}(t)\;dt+ \int_{[1/m,\pi]}(\underbrace{mt+1}_{\leq 2mt})^{k+1}K_{m,r}(t)\;dt\] \[\leq 2^{k+1}\cdot\underbrace{\int_{[-\pi,\pi]}K_{m,r}(t)\;dt}_{ \preceq 1}+2^{k+1}m^{k+1}\cdot\int_{[0,\pi]}\;t^{k+1}K_{m,r}(t)\;dt\] \[\stackrel{{\text{(C.6)}}}{{\leq}}2^{k+1}+2^{k+1}m^{ k+1}\exp(C_{1}\cdot r)\cdot(k+1)^{k+1}\cdot m^{-(k+1)}\stackrel{{ r\leq 2k}}{{\leq}}\exp(C\cdot k)\cdot k^{k}\]

with absolute constants \(C,C_{1}>0\). Since \(\omega_{k+1}(f,1/m)\leq m^{-k}\cdot\omega(f^{(k)},1/m)\) follows from [29, Equation 3.6(5)], the claim is shown. 

Therefore, we can bound the constant appearing in [29, Theorem 4.3] by \(\exp(C\cdot k)\cdot k^{k}\). It remains to deal with the approximation of _multivariate_ periodic functions by _multivariate_ trigonometric polynomials which is contained in [29, Theorem 6.6].

**Theorem C.18** (cf. [29, Theorem 6.6]).: _Let \(s,k\in\mathbb{N}\) and \(f\in C_{2\pi}^{k}(\mathbb{R}^{s};\mathbb{R})\). Let \(\omega_{j}\) denote the modulus of continuity of \(\frac{\partial^{k}f}{\partial x_{j}^{k}}\) for \(j=1,...,s\). Then, with \(E_{m}^{s}\) as introduced in Equation (C.1), it holds_

\[E_{m}^{s}(f)\leq\exp(C\cdot ks)\cdot k^{k}\cdot m^{-k}\sum_{j=1}^{s}\omega_{j }(1/m),\]

_with an absolute constant \(C>0\)._

Proof.: We follow the proof of [29, Theorem 6.6] with \(p_{j}=k\) and \(n_{j}=m\) for every index \(j=1,...,s\). For \(j=1,...,s+1\) define the set \(\mathcal{T}_{j}\) consisting of all functions \(g\in C_{2\pi}(\mathbb{R}^{s};\mathbb{R})\) that are a trigonometric polynomial in \(x_{\ell}\) of degree at most \(m\) for \(\ell<j\); in \(x_{\ell}\) for \(\ell\geq j\) they should have continuous partial derivatives \(\frac{\partial^{p}g}{\partial x_{\ell}^{k}}\) for \(0\leq p\leq k\); the modulus of continuity of \(\frac{\partial^{k}g}{\partial x_{\ell}^{k}}\) should not exceed \(2^{K_{j}}\omega_{j}\), where

\[K_{j}=(j-1)(k+1)\leq 2ks\quad\text{for $j>1$ and $K_{1}=1\leq 2ks$}.\]

Then it is shown that if \(j\in\{1,...,s\}\) and \(f_{j}\in\mathcal{T}_{j}\) there exists a function \(f_{j+1}\in\mathcal{T}_{j+1}\) for which

\[\|f_{j}-f_{j+1}\|_{L^{\infty}(\mathbb{R}^{s};\mathbb{R})}\leq\exp(C_{1}\cdot k )\cdot k^{k}\cdot m^{-k}\cdot 2^{2ks}\cdot\omega_{j}(1/m)\leq\exp(C_{2}\cdot ks )\cdot k^{k}\cdot m^{-k}\cdot\omega_{j}(1/m)\]

for absolute constants \(C_{1},C_{2}>0\). This is an application of Theorem C.17. Hence, defining \(f_{1}:=f\), we see

\[\|f-f_{s+1}\|_{L^{\infty}(\mathbb{R}^{s};\mathbb{R})}\leq\sum_{j=1}^{s}\|f_{j }-f_{j+1}\|_{L^{\infty}(\mathbb{R}^{s};\mathbb{R})}\leq\exp(C_{2}\cdot ks) \cdot k^{k}\cdot m^{-k}\cdot\sum_{j=1}^{s}\omega_{j}(1/m).\qed\]

Therefore, we have shown that the constant appearing in [29, Theorem 6.6] can be bounded from above by \(\exp(C\cdot ks)\cdot k^{k}\).

In the rest of this section, we discuss the operator norm of the operator defined in Lemma C.11. In Lemma C.11 the closed graph theorem is used to show that the operator is bounded. However, the closed graph theorem does not provide any bound on the norm of the operator. Therefore, in order to quantify the operator norm, we need to apply a different technique, which is _Faa di Bruno's formula_. This formula is a generalization of the chain rule to higher order derivatives.

**Theorem C.19**.: _Let \(s,k\in\mathbb{N}\). We define the operator_

\[T:\quad C^{k}([-1,1]^{s};\mathbb{C})\to C_{2\pi}^{k}([-\pi,\pi]^{s};\mathbb{C }),\quad(Tf)(x_{1},...,x_{s}):=f(\cos(x_{1}),...,\cos(x_{s})).\]

_Let \(\boldsymbol{\alpha}\in\mathbb{N}_{0}^{s}\) with \(|\boldsymbol{\alpha}|\leq k\). Then, for any \(f\in C^{k}([-1,1]^{s};\mathbb{C})\), we have_

\[\|\partial^{\boldsymbol{\alpha}}(Tf)\|_{L^{\infty}([\pi,\pi]^{s};\mathbb{C})} \leq\prod_{j=1}^{s}\boldsymbol{\alpha}_{j}^{\boldsymbol{\alpha}_{j}}\cdot\|f \|_{C^{|\boldsymbol{\alpha}|}([-1,1]^{s})}.\]Proof.: The proof is by induction over \(s\). The case \(s=1\) is an application of Faa di Bruno's formula: We can write \(Tf=f\circ g\) with \(g(x)=\cos(x)\). We then take \(\ell\in\mathbb{N}_{0}\) with \(\ell\leq k\) and some \(x\in[-\pi,\pi]\). The set partition version of Faa di Bruno's formula (see for instance [24, p. 219]) then yields

\[\left|(f\circ g)^{(\ell)}(x)\right|\leq\sum_{\pi\in\Pi_{\ell}}\left(\left|f^{( |\pi|)}(g(x))\right|\cdot\prod_{B\in\pi}\left|g^{(|B|)}(x)\right|\right).\]

Here, \(\Pi_{\ell}\) denotes the set of all partitions of the set \(\{1,...,\ell\}\). Since all derivatives of \(g\) are bounded by \(1\) in absolute value and \(|\pi|\leq\ell\) for every partition \(\pi\in\Pi_{\ell}\) we get

\[\|(f\circ g)^{(\ell)}\|_{L^{\infty}([-\pi,\pi];\mathbb{C})}\leq\|\Pi_{\ell}| \cdot\|f\|_{C^{\ell}([-1,1];\mathbb{C})}.\]

The number \(|\Pi_{\ell}|\) is the number of possible partitions of the set \(\{1,...,\ell\}\) and is the so-called \(\ell\)-th _Bell number_. It can be bounded from above by \(\ell^{\ell}\) (see [13, Theorem 2.1]). This proves the case \(s=1\).

We now assume that the claim holds for an arbitrary but fixed \(s\in\mathbb{N}\). Take \(\boldsymbol{\alpha}\in\mathbb{N}_{0}^{s+1}\) with \(|\boldsymbol{\alpha}|\leq k\). We decompose \(\boldsymbol{\alpha}=(\boldsymbol{\alpha}^{\prime},\boldsymbol{\alpha}_{s+1})\) with \(\boldsymbol{\alpha}^{\prime}\in\mathbb{N}_{0}^{s}\). For a fixed variable \(y_{s+1}\in[-1,1]\), we define

\[f_{y_{s+1}}(y_{1},...,y_{s}):=f(y_{1},...,y_{s},y_{s+1})\quad\text{for}\quad( y_{1},...,y_{s})\in[-1,1]^{s}.\]

We denote \(g(x_{1},...,x_{s+1}):=(\cos(x_{1}),...,\cos(x_{s+1}))\), \(g_{s}(x_{1},...,x_{s}):=(\cos(x_{1}),...,\cos(x_{s}))\) and \(\theta(x_{s+1}):=\cos(x_{s+1})\). For every \((x_{1},...,x_{s+1})\in[-\pi,\pi]^{s+1}\) it then holds

\[(f\circ g)(x_{1},...,x_{s+1})=\left(f_{\theta(x_{s+1})}\circ g_{s}\right)(x_{ 1},...,x_{s}).\]

We now differentiate \(f\circ g\) with respect to the multiindex \(\boldsymbol{\alpha}\) and get

\[\left[\partial^{\boldsymbol{\alpha}}(f\circ g)\right](x_{1},...,x _{s+1}) =\frac{\partial^{\boldsymbol{\alpha}_{s+1}}}{\partial x_{s+1}^{ \boldsymbol{\alpha}_{s+1}}}\left[\partial^{\boldsymbol{\alpha}^{\prime}} \left(f_{\theta(x_{s+1})}\circ g_{s}\right)(x_{1},...,x_{s})\right]\] \[=(h_{x_{1},...,x_{s}}\circ\theta)^{(\boldsymbol{\alpha}_{s+1})}( x_{s+1})\]

where we define

\[h_{x_{1},...,x_{s}}(y_{s+1}):=\partial^{\boldsymbol{\alpha}^{\prime}}\left(f_{y_{s+1}}\circ g_{s}\right)(x_{1},...,x_{s})\quad \text{for}\quad(x_{1},...,x_{s})\in[-\pi,\pi]^{s}\text{ and }y_{s+1}\in[-1,1].\]

Using the case \(s=1\), we get

\[\left|\left[\partial^{\boldsymbol{\alpha}}(f\circ g)\right](x_{1},...,x_{s+1} )\right|=\left|(h_{x_{1},...,x_{s}}\circ\theta)^{(\boldsymbol{\alpha}_{s+1})}( x_{s+1})\right|\leq\boldsymbol{\alpha}_{s+1}^{\boldsymbol{\alpha}_{s+1}}\cdot\|h_{x_{1},...,x_{s}}\|_{C^{\boldsymbol{\alpha}_{s+1}}([-1,1];\mathbb{C})}\]

for any fixed \((x_{1},...,x_{s})\in[-\pi,\pi]^{s}\).

It remains to bound \(\|h_{x_{1},...,x_{s}}\|_{C^{\boldsymbol{\alpha}_{s+1}}([-1,1];\mathbb{C})}\). To this end, we fix \(\ell\in\mathbb{N}_{0}\) with \(\ell\leq\boldsymbol{\alpha}_{s+1}\). We further denote

\[F_{y_{1},...,y_{s}}(y_{s+1}):=f(y_{1},...,y_{s},y_{s+1})\quad\text{for}\quad( y_{1},...,y_{s+1})\in[-1,1]^{s+1}.\]

For arbitrary \((x_{1},...,x_{s})\in[-\pi,\pi]^{s}\) and \(y_{s+1}\in[-1,1]\) we then see

\[h_{x_{1},...,x_{s}}^{(\ell)}(y_{s+1})=\partial^{\boldsymbol{\alpha}^{\prime}} \left[(x_{1},...,x_{s})\mapsto F_{g_{s}(x_{1},...,x_{s})}^{(\ell)}(y_{s+1}) \right]=\partial^{\boldsymbol{\alpha}^{\prime}}\left[H_{y_{s+1}}\circ g_{s} \right](x_{1},...,x_{s})\]

where

\[H_{y_{s+1}}(y_{1},...,y_{s}):=F_{y_{1},...,y_{s}}^{(\ell)}(y_{s+1})\quad\text{ for}\quad(y_{1},...,y_{s})\in[-1,1]^{s}.\]

Hence, we see by induction that

\[\left|h_{x_{1},...,x_{s}}^{(\ell)}(y_{s+1})\right| =\left|\partial^{\boldsymbol{\alpha}^{\prime}}\left[H_{y_{s+1}} \circ g_{s}\right](x_{1},...,x_{s})\right|\leq\prod_{j=1}^{\text{H}} \boldsymbol{\alpha}_{j}^{\boldsymbol{\alpha}_{j}}\cdot\left\|H_{y_{s+1}}\right\|_ {C^{|\boldsymbol{\alpha}^{\prime}|}([-1,1]^{s};\mathbb{C})}\] \[\leq\prod_{j=1}^{s}\boldsymbol{\alpha}_{j}^{\boldsymbol{\alpha} _{j}}\cdot\|f\|_{C^{|\boldsymbol{\alpha}|}([-1,1]^{s+1};\mathbb{C})}\]

as was to be shown. 

_Remark C.20_.: For a multiindex \(\boldsymbol{\alpha}\in\mathbb{N}_{0}^{s}\) with \(|\boldsymbol{\alpha}|\leq k\) we see

\[\prod_{j=1}^{s}\boldsymbol{\alpha}_{j}^{\boldsymbol{\alpha}_{j}}\leq k^{ \sum_{j=1}^{s}\boldsymbol{\alpha}_{j}}\leq k^{k}.\]

Hence, the norm of the operator introduced in Lemma C.11 can be bounded from above by \(k^{k}\).

### Proof of Theorem 3.2

For any natural number \(\ell\in\mathbb{N}_{0}\), we denote by \(T_{\ell}\) the \(\ell\)-th Chebyshev polynomial, satisfying

\[T_{\ell}\left(\cos(x)\right)=\cos(\ell x),\quad x\in\mathbb{R}.\]

For a multi-index \(\mathbf{k}\in\mathbb{N}_{0}^{s}\) we define

\[T_{\mathbf{k}}(x):=\prod_{j=1}^{s}T_{\mathbf{k}_{j}}\left(x_{j}\right),\quad x \in[-1,1]^{s}.\]

The proof of Theorem 3.2 relies on the fact that \(C^{k}\)-functions can by approximated at a certain rate using linear combinations of the \(T_{\mathbf{k}}\) (see Theorem C.15). We also refer to Figure 4 for an illustration of the overall proof strategy of Theorem 3.2.

Proof of Theorem 3.2.: Choose \(M\in\mathbb{N}\) as the largest integer for which \((16M-7)^{2n}\leq m\), where we assume without loss of generality that \(9^{2n}\leq m\), which can be done by choosing \(\sigma_{j}=0\) for all \(j\in\{1,...,m\}\) for \(m<9^{2n}\), at the cost of possibly enlarging \(c\). First we note that by the choice of \(M\) the inequality

\[m\leq(16M+9)^{2n}\]

holds true. Since \(16M+9\leq 25M\), we get \(m\leq 25^{2n}\cdot M^{2n}\) or equivalently

\[\frac{m^{1/2n}}{25}\leq M.\] (C.7)

According to Theorem C.15 we choose a constant \(c_{1}=c_{1}(n,k)\) with the property that for any function \(f\in C^{k}\left([-1,1]^{2n};\mathbb{R}\right)\) there exists a polynomial

\[P=\sum_{0\leq\mathbf{k}\leq 2M-1}\mathcal{V}_{\mathbf{k}}^{M}(f)\cdot T_{ \mathbf{k}}\]

of coordinatewise degree at most \(2M-1\) satisfying

\[\left\|f-P\right\|_{L^{\infty}([-1,1]^{2n};\mathbb{R})}\leq\frac{c_{1}}{M^{k} }\cdot\left\|f\right\|_{C^{k}\left([-1,1]^{2n};\mathbb{R}\right)}.\]

Furthermore, according to Theorem C.15, we choose a constant \(c_{2}=c_{2}(n)\), such that the inequality

\[\sum_{0\leq\mathbf{k}\leq 2M-1}\left|\mathcal{V}_{\mathbf{k}}^{M}(f)\right|\leq c _{2}\cdot M^{n}\cdot\left\|f\right\|_{L^{\infty}([-1,1]^{2n};\mathbb{R})}\leq c _{2}\cdot M^{n}\cdot\left\|f\right\|_{C^{k}([-1,1]^{2n};\mathbb{R})}\]

holds for all \(f\in C^{k}\left([-1,1]^{2n};\mathbb{R}\right)\). The final constant is then defined to be

\[c=c(n,k):=\sqrt{2}\cdot 25^{k}\cdot\left(c_{1}+c_{2}\right).\]

Fix \(\mathbf{k}\leq 2M-1\). Since \(T_{\mathbf{k}}\) is a polynomial of componentwise degree less or equal to \(2M-1\) with \(\varphi_{n}\) as in (A.1), we have a representation

\[\left(T_{\mathbf{k}}\circ\varphi_{n}^{-1}\right)(z)=\sum_{\begin{subarray}{c }\boldsymbol{\ell}^{1},\boldsymbol{\ell}^{2}\in\mathbb{N}_{0}^{n}\\ \boldsymbol{\ell}^{1},\boldsymbol{\ell}^{2}\leq 2M-1\end{subarray}}a_{ \boldsymbol{\ell}^{1},\boldsymbol{\ell}^{2}}^{\mathbf{k}}\prod_{t=1}^{n} \operatorname{Re}\left(z_{t}\right)^{\boldsymbol{\ell}^{1}_{t}}\operatorname{ Im}\left(z_{t}\right)^{\boldsymbol{\ell}^{2}_{t}}\]

with suitably chosen coefficients \(a_{\boldsymbol{\ell}^{1},\boldsymbol{\ell}^{2}}^{\mathbf{k}}\in\mathbb{C}\). By using the identities \(\operatorname{Re}\left(z_{t}\right)=\frac{1}{2}\left(z_{t}+\overline{z_{t}}\right)\) and also \(\operatorname{Im}\left(z_{t}\right)=\frac{1}{2i}\left(z_{t}-\overline{z_{t}}\right)\) we can rewrite \(T_{\mathbf{k}}\circ\varphi_{n}^{-1}\) into a complex polynomial in \(z\) and \(\overline{z}\), i.e.,

\[\left(T_{\mathbf{k}}\circ\varphi_{n}^{-1}\right)(z)=\sum_{\begin{subarray}{c }\boldsymbol{\ell}^{1},\boldsymbol{\ell}^{2}\in\mathbb{N}_{0}^{n}\\ \boldsymbol{\ell}^{1},\boldsymbol{\ell}^{2}\leq 4M-2\end{subarray}}b_{ \boldsymbol{\ell}^{1},\boldsymbol{\ell}^{2}}^{\mathbf{k}}z^{\boldsymbol{\ell}^ {1}}\overline{z}^{\boldsymbol{\ell}^{2}}\]

with complex coefficients \(b_{\boldsymbol{\ell}^{1},\boldsymbol{\ell}^{2}}^{\mathbf{k}}\in\mathbb{C}\). Using Theorem 3.1, we choose \(\rho_{1},...,\rho_{m}\in\mathbb{C}^{n}\) and \(b\in\mathbb{C}\), such that for any polynomial \(P\in\left\{T_{\mathbf{k}}\circ\varphi_{n}^{-1}:\,\mathbf{k}\leq 2M-1\right\} \subseteq\mathcal{P}_{4M-2}^{n}\) there are coefficients \(\sigma_{1}(P),...,\sigma_{m}(P)\in\mathbb{C}\), such that

\[\left\|g_{P}-P\right\|_{L^{\infty}(\Omega_{n};\mathbb{C})}\leq M^{-k-n},\] (C.8)where

\[g_{P}:=\sum_{t=1}^{m}\sigma_{t}(P)\phi\left(\rho_{t}^{T}z+b\right).\]

Note that here we implicitly use the bound \((4\cdot(4M-2)+1)^{2n}\leq m\). We are now going to show that the chosen constant and the chosen vectors \(\rho_{t}\) have the desired property.

Let \(f\in C^{k}\left(\Omega_{n};\mathbb{C}\right)\). By splitting \(f\) into real and imaginary part, we write \(f=f_{1}+i\cdot f_{2}\) with \(f_{1},f_{2}\in C^{k}\left(\Omega_{n};\mathbb{R}\right)\). For the following, fix \(j\in\{1,2\}\) and note that \(f_{j}\circ\varphi_{n}\in C^{k}\left([-1,1]^{2n};\mathbb{R}\right)\). By choice of \(c_{1}\), there exists a polynomial \(P\) with the property

\[\left\|f_{j}\circ\varphi_{n}-P\right\|_{L^{\infty}([-1,1]^{2n};\mathbb{R})} \leq\frac{c_{1}}{M^{k}}\cdot\left\|f_{j}\circ\varphi_{n}\right\|_{C^{k}([-1,1 ]^{2n};\mathbb{R})}\]

or equivalently

\[\left\|f_{j}-P\circ\varphi_{n}^{-1}\right\|_{L^{\infty}(\Omega_{n};\mathbb{R}) }\leq\frac{c_{1}}{M^{k}}\cdot\left\|f_{j}\right\|_{C^{k}(\Omega_{n};\mathbb{R })},\] (C.9)

where \(P\circ\varphi_{n}^{-1}\) can be written in the form

\[\left(P\circ\varphi_{n}^{-1}\right)(z)=\sum_{0\leq\mathbf{k}\leq 2M-1} \mathcal{V}_{\mathbf{k}}^{M}\left(f_{j}\circ\varphi_{n}\right)\cdot\left(T_{ \mathbf{k}}\circ\varphi_{n}^{-1}\right)(z).\]

We choose the function \(g_{T_{\mathbf{k}}\circ\varphi_{n}^{-1}}\) according to (C.8). Thus, writing

\[g_{j}:=\sum_{0\leq\mathbf{k}\leq 2M-1}\mathcal{V}_{\mathbf{k}}^{M}\left(f_{j} \circ\varphi_{n}\right)\cdot g_{T_{\mathbf{k}}\circ\varphi_{n}^{-1}},\]

we obtain

\[\left\|P\circ\varphi_{n}^{-1}-g_{j}\right\|_{L^{\infty}(\Omega_{n}; \mathbb{R})} \leq\sum_{0\leq\mathbf{k}\leq 2M-1}\left|\mathcal{V}_{\mathbf{k}}^{M} \left(f_{j}\circ\varphi_{n}\right)\right|\cdot\underbrace{\left\|T_{\mathbf{k}} \circ\varphi_{n}^{-1}-g_{T_{\mathbf{k}}\circ\varphi_{n}^{-1}}\right\|_{L^{ \infty}(\Omega_{n};\mathbb{R})}}_{\leq M^{-k-n}}\] \[\leq M^{-k-n}\cdot\sum_{0\leq\mathbf{k}\leq 2M-1}\left| \mathcal{V}_{\mathbf{k}}^{M}\left(f_{j}\circ\varphi_{n}\right)\right|\] \[\leq\frac{c_{2}}{M^{k}}\left\|f_{j}\circ\varphi_{n}\right\|_{C^{ k}([-1,1]^{2n};\mathbb{R})}=\frac{c_{2}}{M^{k}}\left\|f_{j}\right\|_{C^{k}(\Omega_{n}; \mathbb{R})}.\] (C.10)

Combining (C.9) and (C.10), we see

\[\left\|f_{j}-g_{j}\right\|_{L^{\infty}(\Omega_{n};\mathbb{R})}\leq\frac{c_{1} +c_{2}}{M^{k}}\cdot\left\|f_{j}\right\|_{C^{k}(\Omega_{n};\mathbb{R})}\leq \frac{c_{1}+c_{2}}{M^{k}}\cdot\left\|f\right\|_{C^{k}(\Omega_{n};\mathbb{C})}.\]

In the end, define

\[g:=g_{1}+i\cdot g_{2}.\]

Since the vectors \(\rho_{t}\) have been chosen fixed, it is clear that, after rearranging, \(g\) has the desired form, i.e., \(g=\sigma^{T}\Phi\) where \(\Phi(z)=\left(\phi(\rho_{t}z+b)\right)_{t=1}^{m}\). Furthermore, one obtains the bound

\[\left\|f-g\right\|_{L^{\infty}(\Omega_{n};\mathbb{C})} \leq\sqrt{\left\|f_{1}-g_{1}\right\|_{L^{\infty}(\Omega_{n}; \mathbb{R})}^{2}+\left\|f_{2}-g_{2}\right\|_{L^{\infty}(\Omega_{n};\mathbb{R}) }^{2}}\] \[\leq\frac{c_{1}+c_{2}}{M^{k}}\cdot\sqrt{\left\|f\right\|_{C^{k}( \Omega_{n};\mathbb{C})}^{2}+\left\|f\right\|_{C^{k}(\Omega_{n};\mathbb{C})}^{2}}\] \[\leq\frac{\sqrt{2}\cdot(c_{1}+c_{2})}{M^{k}}\cdot\left\|f\right\| _{C^{k}(\Omega_{n};\mathbb{C})}.\]

Using (C.7), we see

\[\left\|f-g\right\|_{L^{\infty}(\Omega_{n};\mathbb{C})}\leq\frac{\sqrt{2}\cdot 2 5^{k}\cdot(c_{1}+c_{2})}{m^{k/2n}}\cdot\left\|f\right\|_{C^{k}(\Omega_{n}; \mathbb{C})},\]

as desired.

The linearity and continuity of the maps \(f\mapsto\sigma_{j}(f)\) (with respect to the \(\|\cdot\|_{L^{\infty}}\)-norm) follow easily from the fact that the map \(f\mapsto\mathcal{V}_{\mathbf{k}}^{M}(f)\) is a continuous linear functional for every multiindex \(0\leq\mathbf{k}\leq 2M-1\)

## Appendix D Approximation of holomorphically extendable functions

In this appendix we provide the proofs for the statements contained in Remark 3.3. The proofs mainly rely on results about sparse polynomial approximation [2].

**Definition D.1** (cf. [2, Assumption 2.3]).: Let \(s\in\mathbb{N}\) and \(\boldsymbol{\nu}\in(1,\infty)^{s}\). For every \(j\in\{1,...,s\}\) let

\[\mathcal{E}_{\boldsymbol{\nu}_{j}}:=\left\{\frac{z+z^{-1}}{2}:\;z\in\mathbb{C},1\leq|z|\leq\boldsymbol{\nu}_{j}\right\}.\]

We then define the _(filled-in) Bernstein polyellipse_ of parameter \(\boldsymbol{\nu}\) as

\[\mathcal{E}_{\boldsymbol{\nu}}:=\mathcal{E}_{\boldsymbol{\nu}_{1}}\times \cdots\times\mathcal{E}_{\boldsymbol{\nu}_{s}}\subseteq\mathbb{C}^{s}\]

and observe that \([-1,1]^{s}\subseteq\mathcal{E}_{\boldsymbol{\nu}}\) when we interpret \([-1,1]^{s}\) as a subset of \(\mathbb{C}^{s}\). We further define

\[\mathcal{V}_{s}(\boldsymbol{\nu}):=\left\{f:[-1,1]^{s}\to\mathbb{C}:\;\exists \text{ open }U\supseteq\mathcal{E}_{\boldsymbol{\nu}}\text{ and }\tilde{f}:U\to\mathbb{C}\text{ holomorphic with }\tilde{f}\Big{|}_{[-1,1]^{s}}=f\right\}.\]

Here, \([-1,1]^{s}\) is again interpreted as a subset of \(\mathbb{C}^{s}\). Moreover, note that such an extension \(\tilde{f}\) is, if existent, unique, as follows from the identity theorem for holomorphic functions. Hence, the expression

\[\|f\|_{\mathcal{V}_{s}(\boldsymbol{\nu})}:=\|\tilde{f}\|_{L^{\infty}(\mathcal{ E}_{\boldsymbol{\nu}};\mathbb{C})}\]

is well-defined.

**Definition D.2** (cf. [2, pp. 25, 28 ff.]).: Let \(s\in\mathbb{N}\). We define a probability measure on \([-1,1]^{s}\) via

\[d\mu_{s}:=\prod_{j=1}^{s}\frac{1}{\pi\sqrt{1-x_{j}^{2}}}\;dx.\]

We define the _normalized_ Chebyshev polynomials for \(\textbf{k}\in\mathbb{N}_{0}^{s}\) as

\[\widetilde{T_{\textbf{k}}}(x):=2^{\|\textbf{k}\|_{0}/2}\prod_{j=1}^{s}\; \cos(\textbf{k}_{j}\arccos(x_{j})),\]

where \(\|\textbf{k}\|_{0}:=\#\{1\leq j\leq s:\;\textbf{k}_{j}\neq 0\}\). Note that this definition differs slightly from the notion used in Appendices C.1 and C.3.

The following lemma is crucial for deriving the approximation rate in Theorem D.5. The proofs can be found in [2].

**Lemma D.3** (cf. [2, Remark 2.15 and Theorem 3.2]).: _Let \(s\in\mathbb{N}\), \(\boldsymbol{k}\in\mathbb{N}_{0}^{s}\), \(\boldsymbol{\nu}\in(1,\infty)^{s}\) and \(f\in\mathcal{V}_{s}(\boldsymbol{\nu})\). Then it holds_

1. \(\|\widetilde{T_{\textbf{k}}}\|_{L^{\infty}([-1,1]^{s};\mathbb{R})}=2^{\| \textbf{k}\|_{0}/2}\)_, where_ \(\|\textbf{k}\|_{0}=\#\{1\leq j\leq s:\textbf{k}_{j}\neq 0\}\)_;_
2. \(|\langle\widetilde{T_{\textbf{k}}},f\rangle_{\mu_{s}}|\leq\boldsymbol{\nu}^{ -\textbf{k}}\cdot 2^{\|\textbf{k}\|_{0}/2}\cdot\|f\|_{\mathcal{V}_{s}( \boldsymbol{\nu})}\)_._

It is a well-known fact that the Chebyshev polynomials \(\widetilde{T_{\textbf{k}}}\) form an _orthonormal basis_ of \(L^{2}_{\mu_{s}}([-1,1]^{s};\mathbb{C})\). The following proposition states that functions from \(\mathcal{V}_{s}(\boldsymbol{\nu})\) can even be approximated uniformly by linear combinations of the \(\widetilde{T_{\textbf{k}}}\) at a certain rate. The proof follows essentially by applying Lemma D.3.

Figure 4: Schematic for the proof of the main result (Theorem 3.2). The first row shows the different steps of the proof and the second row indicates the main tools used.

**Proposition D.4**.: _Let \(s,m\in\mathbb{N}\) and \(\boldsymbol{\nu}\in(1,\infty)^{s}\). Let \(\nu:=\min\limits_{j=1,\ldots,s}\boldsymbol{\nu}_{j}\). Then there exists a constant \(c=c(s,\boldsymbol{\nu})>0\) with the following property: For every \(f\in\mathcal{V}_{s}(\boldsymbol{\nu})\), defining_

\[P_{m}:=\sum_{\begin{subarray}{c}\boldsymbol{k}\in\mathbb{N}_{0}^{s}\\ \boldsymbol{k}\leq m\end{subarray}}\langle f,\widetilde{\boldsymbol{T}}_{ \boldsymbol{k}}\rangle_{\mu_{s}}\cdot\widetilde{\boldsymbol{T}}_{\boldsymbol {k}},\]

_it holds_

\[\|f-P_{m}\|_{L^{\infty}([-1,1]^{s};\mathbb{C})}\leq c\cdot\nu^{-m}\cdot\|f\|_{ \mathcal{V}_{s}(\boldsymbol{\nu})}.\]

Proof.: Let \(f\in\mathcal{V}_{s}(\boldsymbol{\nu})\). Since the \(\widetilde{\boldsymbol{T}}_{\boldsymbol{k}}\) form an orthonormal basis of \(L^{2}_{\mu_{s}}([-1,1]^{s};\mathbb{C})\) and since \(\mu_{s}\) is a probability measure, so that \(f\in C([-1,1]^{s};\mathbb{C})\subseteq L^{2}_{\mu_{2}}([-1,1]^{s};\mathbb{C})\), it follows that

\[f=\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{s}}\langle f,\widetilde{\boldsymbol{ T}}_{\boldsymbol{k}}\rangle_{\mu_{s}}\cdot\widetilde{\boldsymbol{T}}_{ \boldsymbol{k}}\]

with unconditional convergence in \(L^{2}_{\mu_{s}}\). For \(x\in[-1,1]^{s}\), note that

\[\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{s}}|\langle f,\widetilde{\boldsymbol{ T}}_{\boldsymbol{k}}\rangle_{\mu_{s}}|\cdot|\widetilde{\boldsymbol{T}}_{ \boldsymbol{k}}(x)|\leq\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{s}}|\langle f, \widetilde{\boldsymbol{T}}_{\boldsymbol{k}}\rangle_{\mu_{s}}|\cdot\| \widetilde{\boldsymbol{T}}_{\boldsymbol{k}}\|_{L^{\infty}([-1,1]^{s}; \mathbb{C})}\leq 2^{s}\|f\|_{\mathcal{V}_{s}(\boldsymbol{\nu})}\cdot\sum_{ \boldsymbol{k}\in\mathbb{N}_{0}^{s}}\boldsymbol{\nu}^{-\boldsymbol{k}}.\]

Here, we employed Lemma D.3 at the last inequality. From \(\boldsymbol{\nu}>1\) it follows that

\[\sum_{\boldsymbol{k}\in\mathbb{N}_{0}^{s}}\langle f,\widetilde{\boldsymbol{ T}}_{\boldsymbol{k}}\rangle_{\mu_{s}}\cdot\widetilde{\boldsymbol{T}}_{ \boldsymbol{k}}\]

converges _pointwise_ (even uniformly) and, since all the involved functions are continuous, this pointwise limit then has to coincide with the \(L^{2}_{\mu_{s}}\)-limit \(f\). Hence, it holds

\[\|f-P_{m}\|_{L^{\infty}([-1,1]^{s};\mathbb{C})} \leq\sum_{\begin{subarray}{c}\boldsymbol{k}\in\mathbb{N}_{0}^{s} \\ \boldsymbol{k}\leq m\end{subarray}}|\langle f,\widetilde{\boldsymbol{T}}_{ \boldsymbol{k}}\rangle_{\mu_{s}}|\cdot\|\widetilde{\boldsymbol{T}}_{ \boldsymbol{k}}\|_{L^{\infty}([-1,1]^{s};\mathbb{C})}\] \[\leq 2^{s}\|f\|_{\mathcal{V}_{s}(\boldsymbol{\nu})}\cdot\sum_{ \begin{subarray}{c}\boldsymbol{k}\in\mathbb{N}_{0}^{s}\\ \boldsymbol{k}\leq m\end{subarray}}\boldsymbol{\nu}^{-\boldsymbol{k}},\]

where we again used Lemma D.3. To complete the proof we compute

\[\sum_{\begin{subarray}{c}\boldsymbol{k}\in\mathbb{N}_{0}^{s}\\ \boldsymbol{k}\leq m\end{subarray}}\boldsymbol{\nu}^{-\boldsymbol{k}} \leq\sum_{j=1}^{s}\sum_{\begin{subarray}{c}\boldsymbol{k}\in \mathbb{N}_{0}^{s}\\ \boldsymbol{k}_{j}>m\end{subarray}}\boldsymbol{\nu}^{-\boldsymbol{k}}=\sum_{j=1} ^{s}\left(\prod_{\begin{subarray}{c}1\leq\ell\leq s\\ l\neq j\end{subarray}}\left(\sum_{k=0}^{\infty}\boldsymbol{\nu}_{\ell}^{-k} \right)\cdot\sum_{k=m+1}^{\infty}\boldsymbol{\nu}_{j}^{-k}\right)\] \[\leq\sum_{j=1}^{s}\left(\boldsymbol{\nu}_{j}^{-(m+1)}\prod_{1\leq \ell\leq s}\left(\sum_{k=0}^{\infty}\boldsymbol{\nu}_{\ell}^{-k}\right)\right)\] \[=\prod_{1\leq\ell\leq s}\left(\sum_{k=0}^{\infty}\boldsymbol{\nu} _{\ell}^{-k}\right)\cdot\sum_{j=1}^{s}\boldsymbol{\nu}_{j}^{-(m+1)}\] \[=\prod_{1\leq\ell\leq s}\left(\frac{\boldsymbol{\nu}_{\ell}}{ \boldsymbol{\nu}_{\ell}-1}\right)\cdot s\cdot\nu^{-(m+1)}\]

and define \(c(s,\boldsymbol{\nu}):=2^{s}\cdot s\cdot\prod_{\ell=1}^{s}\frac{\boldsymbol{ \nu}_{j}}{\boldsymbol{\nu}_{j}-1}\cdot\nu^{-1}\). 

To formulate the result for the approximation of holomorphically extendable functions using CVNNs we need to transfer the definition of \(\mathcal{V}_{s}(\boldsymbol{\nu})\) to the complex setting. For \(\boldsymbol{\nu}\in(1,\infty)^{2n}\) and with \(\varphi_{n}\) as in Equation (A.1), we hence write

\[\mathcal{W}_{n}(\boldsymbol{\nu}):=\Big{\{}f:\Omega_{n}\to\mathbb{C}:\ f\circ \varphi_{n}\big{|}_{[-1,1]^{2n}}\in\mathcal{V}_{2n}(\boldsymbol{\nu})\Big{\}}\]For \(f\in\mathcal{W}_{n}(\bm{\nu})\) we define

\[\|f\|_{\mathcal{W}_{n}(\bm{\nu})}:=\|f\circ\varphi_{n}\|_{\mathcal{V}_{2n}(\bm{ \nu})}.\]

Thus, \(\mathcal{W}_{n}(\bm{\nu})\) consists of all complex-valued functions defined on \(\Omega_{n}\) that can be holomorphically extended onto some polyellipse in \(\mathbb{C}^{2n}\), where \(\Omega_{n}\subseteq\mathbb{C}^{n}\) is interpreted as a subset of \(\mathbb{R}^{2n}\) and then as a subset of \(\mathbb{C}^{2n}\). The final approximation result then reads as follows.

**Theorem D.5**.: _Let \(n\in\mathbb{N}\) and \(\bm{\nu}\in(1,\infty)^{2n}\). Set_

\[\nu:=\min_{1\leq j\leq 2n}\bm{\nu}_{j}.\]

_Then there exists a constant \(c=c(n,\bm{\nu})>0\) with the following property: For every function \(\phi:\mathbb{C}\to\mathbb{C}\) that is smooth and non-polyharmonic on some open set \(\varnothing\neq U\subset\mathbb{C}\) and for every \(m\in\mathbb{N}\) there exists a first layer \(\Phi\in\mathcal{F}_{n,m}^{\phi}\) with the property that for every \(f\in\mathcal{W}_{n}(\bm{\nu})\) there exist coefficients \(\sigma=\sigma(f)\in\mathbb{C}^{m}\) such that_

\[\|f-\sigma^{T}\Phi\|_{L^{\infty}(\Omega_{n};\mathbb{C})}\leq c\cdot\nu^{-m^{1 /(2n)}/17}\cdot\|f\|_{\mathcal{W}_{n}(\bm{\nu})}.\]

_Moreover, the map \(f\mapsto\sigma(f)\) is a continuous linear functional with respect to the \(L^{\infty}\)-norm._

Proof.: Choose \(M\in\mathbb{N}\) as the largest integer satisfying \((8M+1)^{2n}\leq m\), where we assume without loss of generality that \(9^{2n}\leq m\). This can be done by choosing \(\sigma=0\) for \(m<9^{2n}\) at the cost of possibly enlarging \(c\). Note that the maximality of \(M\) implies \((8M+9)^{2n}>m\). By using \(8M+9\leq 17M\), this gives us

\[M>\frac{1}{17}\cdot m^{1/(2n)}.\] (D.1)

Choose the constant \(c_{1}=c_{1}(2n,\bm{\nu})\) according to Proposition D.4.

Fix \(\mathbf{k}\in\mathbb{N}_{0}^{2n}\) with \(\mathbf{k}\leq M\). Since \(\widetilde{T_{\mathbf{k}}}\) is a polynomial of componentwise degree at most \(M\), we have a representation

\[\left(\widetilde{T_{\mathbf{k}}}\circ\varphi_{n}^{-1}\right)(z)=\sum_{ \begin{subarray}{c}\bm{\ell}^{1},\bm{\ell}^{2}\in\mathbb{N}_{0}^{c}\\ \bm{\ell}^{1},\bm{\ell}^{2}\leq M\end{subarray}}a^{\mathbf{k}}_{\bm{\ell}^{1 },\bm{\ell}^{2}}\prod_{t=1}^{n}\operatorname{Re}\left(z_{t}\right)^{\bm{\ell}^ {1}_{t}}\operatorname{Im}\left(z_{t}\right)^{\bm{\ell}^{2}_{t}}\]

with suitably chosen coefficients \(a^{\mathbf{k}}_{\bm{\ell}^{1},\bm{\ell}^{2}}\in\mathbb{C}\). By using the identities \(\operatorname{Re}\left(z_{t}\right)=\frac{1}{2}\left(z_{t}+\overline{z_{t}}\right)\) and also \(\operatorname{Im}\left(z_{t}\right)=\frac{1}{2i}\left(z_{t}-\overline{z_{t}}\right)\), we can rewrite \(\widetilde{T_{\mathbf{k}}}\circ\varphi_{n}^{-1}\) into a complex polynomial in \(z\) and \(\overline{z}\), i.e.,

\[\left(\widetilde{T_{\mathbf{k}}}\circ\varphi_{n}^{-1}\right)(z)=\sum_{ \begin{subarray}{c}\bm{\ell}^{1},\bm{\ell}^{2}\in\mathbb{N}_{0}^{c}\\ \bm{\ell}^{1},\bm{\ell}^{2}\leq 2M\end{subarray}}b^{\mathbf{k}}_{\bm{\ell}^{1},\bm{ \ell}^{2}}z^{\bm{\ell}^{1}}\overline{z}^{\bm{\ell}^{2}}\]

with complex coefficients \(b^{\mathbf{k}}_{\bm{\ell}^{1},\bm{\ell}^{2}}\in\mathbb{C}\). Using Theorem 3.1, we choose \(\rho_{1},...,\rho_{m}\in\mathbb{C}^{n}\) and \(b\in\mathbb{C}\), such that for any polynomial \(P\in\left\{\widetilde{T_{\mathbf{k}}}\circ\varphi_{n}^{-1}:\,\mathbf{k}\leq M \right\}\subseteq\mathcal{P}_{2M}^{n}\) there exist coefficients \(\sigma_{1}(P),...,\sigma_{m}(P)\in\mathbb{C}\), such that

\[\|g_{P}-P\|_{L^{\infty}(\Omega_{n};\mathbb{C})}\leq\left(\sum_{\mathbf{k}\in \mathbb{N}_{0}^{2n}}\bm{\nu}^{-\mathbf{k}}\right)^{-1}\cdot\nu^{-(M+1)},\] (D.2)

where

\[g_{P}:=\sum_{t=1}^{m}\sigma_{t}(P)\phi\left(\rho_{t}^{T}z+b\right).\]

Note that here we implicitly use the bound \((4\cdot(2M)+1)^{2n}\leq m\). We are now going to show that the first layer \(\Phi\in\mathcal{F}_{n,m}^{\phi}\) defined using the \(\rho_{t}\) and \(b\) (i.e., \(\Phi(z)=(\phi(\rho_{t}^{T}z+b))_{t=1}^{m}\)) has the desired property.

To this end, take an arbitrary function \(f\in\mathcal{W}_{n}(\bm{\nu})\). Proposition D.4 tells us that

\[\|f-P_{M}\circ\varphi_{n}^{-1}\|_{L^{\infty}(\Omega_{n};\mathbb{C})}=\|f\circ \varphi_{n}-P_{M}\|_{L^{\infty}([-1,1]^{2n};\mathbb{C})}\leq c_{1}\cdot\nu^{-(M +1)}\cdot\|f\circ\varphi_{n}\|_{\mathcal{V}_{2n}(\bm{\nu})},\] (D.3)

[MISSING_PAGE_EMPTY:44]

restriction of \(\overline{a}\) to \(\partial U_{\varrho}(X_{m+1})\) is a continuous mapping to \(\mathbb{R}^{m}\) with respect to \(\|\cdot\|_{V}\). Since all norms are equivalent on the finite-dimensional space \(X_{m+1}\), the Borsuk-Ulam-Theorem [18, Corollary 4.2] yields the existence of a point \(x_{0}\in\partial U_{\varrho}(X_{m+1})\) with \(\overline{a}(x_{0})=\overline{a}(-x_{0})\). We then see

\[2_{\varrho} =2\|x_{0}\|_{X}\leq\|x_{0}-M(\overline{a}(x_{0}))\|_{X}+\|x_{0}+ M(\overline{a}(-x_{0}))\|_{X}\] \[=\|x_{0}-M(\overline{a}(x_{0}))\|_{X}+\|-x_{0}-M(\overline{a}(-x_ {0}))\|_{X},\]

and hence at least one of the two summands on the right has to be larger than or equal to \(\varrho\). 

### Proof of Theorem 4.1

Using Proposition E.1, we can deduce our lower bound in the context of \(C^{k}\)-spaces. The proof is in fact almost identical to what is done in [19, Theorem 4.2]. However, we decided to include a detailed proof in this paper, since [19] considers Sobolev functions and not \(C^{k}\)-functions.

**Theorem E.2**.: _Let \(s,k\in\mathbb{N}\). Then there exists a constant \(c=c(s,k)>0\) with the following property: For any \(m\in\mathbb{N}\) and any map \(\overline{a}:C^{k}([-1,1]^{s};\mathbb{R})\to\mathbb{R}^{m}\) that is continuous with respect to some norm on \(C^{k}([-1,1]^{s};\mathbb{R})\) and any (possibly discontinuous) map \(M:\mathbb{R}^{m}\to C([-1,1]^{s};\mathbb{R})\), we have_

\[\sup_{\begin{subarray}{c}f\in C^{k}([-1,1]^{s};\mathbb{R})\\ \|f\|_{C^{k}([-1,1]^{s};\mathbb{R})}\leq 1\end{subarray}}\|f-M(\overline{a}(f)) \|_{L^{\infty}([-1,1]^{s};\mathbb{R})}\geq c\cdot m^{-k/s}.\]

Proof.: The idea is to apply Proposition E.1 to \(X:=C([-1,1]^{s};\mathbb{R})\), \(V:=C^{k}([-1,1]^{s};\mathbb{R})\) and the set \(K:=\{f\in C^{k}([-1,1]^{s};\mathbb{R}):\ \|f\|_{C^{k}([-1,1]^{s};\mathbb{R})}\leq 1\}\).

Assume in the beginning that \(m=n^{s}\) with an integer \(n>1\). Pick \(\phi\in C^{\infty}(\mathbb{R}^{s})\) with \(\phi\equiv 1\) on \([-3/4,3/4]^{s}\) and \(\phi\equiv 0\) outside of \([-1,1]^{s}\). Fix \(c_{0}=c_{0}(s,k)>0\) with

\[1\leq\|\phi\|_{C^{k}([-1,1]^{s};\mathbb{R})}\leq c_{0}.\]

Let \(Q_{1},...,Q_{m}\) be the partition (disjoint up to null-sets) of \([-1,1]^{s}\) into closed cubes of sidelength \(2/n\). For every \(j\in\{1,...,m\}\) we write \(Q_{j}=\bigtimes_{\ell=1}^{s}[a_{\ell}^{(j)}-1/n,a_{\ell}^{(j)}+1/n]\) with an appropriately chosen vector \(a=(a_{1}^{(j)},...,a_{s}^{(j)})\in[-1,1]^{s}\) and let

\[\phi_{j}(x):=\phi(n(x-a^{(j)}))\text{ for }x\in\mathbb{R}^{s}.\]

By choice of \(\phi\), the maps \(\phi_{j}\) are supported on a proper subset of \(Q_{j}\) for every \(j\in\{1,...,m\}\) and an inductive argument shows

\[\partial^{\textbf{k}}\phi_{j}(x)=n^{|\textbf{k}|}\cdot(\partial^{\textbf{k}} \phi)(n(x-a^{(j)}))\quad\text{for every }\textbf{k}\in\mathbb{N}_{0}^{s}\text{ and }x\in\mathbb{R}^{s}\]

and hence in particular

\[\|\phi_{j}\|_{C^{k}([-1,1]^{s};\mathbb{R})}\leq n^{|\textbf{k}|}\cdot c_{0}.\] (E.2)

Let \(X_{m}:=\operatorname{span}\{\phi_{1},...,\phi_{m}\}\) and \(S\in U_{1}(X_{m})=\{f\in X_{m}:\ \|f\|_{L^{\infty}([-1,1]^{s};\mathbb{R})}\leq 1\}\). Then we can write \(S\) in the form \(S=\sum_{j=1}^{m}c_{j}\phi_{j}\) with real numbers \(c_{1},...,c_{m}\in\mathbb{R}\). Suppose there exists \(j^{*}\in\{1,...,m\}\) with \(|c_{j^{*}}|>1\). Then we have

\[\|S\|_{L^{\infty}([-1,1]^{s};\mathbb{R})}\geq|S(a^{(j^{*})})|\geq|c_{j^{*}}|>1,\]

since the functions \(\phi_{j}\) have disjoint support and \(\phi_{j}(a^{(j)})=1\). This is a contradiction to \(S\in U_{1}(X_{m})\) and we can thus infer that \(\max_{j}|c_{j}|\leq 1\). Furthermore, we see again because the functions \(\phi_{j}\) have disjoint support that

\[\|\partial^{\textbf{k}}S\|_{L^{\infty}([-1,1]^{s};\mathbb{R})}\leq\max_{j}|c_{ j}|\cdot\|\partial^{\textbf{k}}\phi_{j}\|_{L^{\infty}([-1,1]^{s};\mathbb{R})} \overset{\eqref{eq:C_1}}{\leq}n^{|\textbf{k}|}\cdot c_{0}\leq c_{0}\cdot n^{k }=c_{0}\cdot m^{k/s}\]

for every \(\textbf{k}\in\mathbb{N}_{0}^{s}\) with \(|\textbf{k}|\leq k\) and hence

\[\|S\|_{C^{k}([-1,1]^{s};\mathbb{R})}\leq c_{0}\cdot m^{k/s}.\]Thus, letting \(\varrho:=c_{0}^{-1}\cdot m^{-k/s}\) yields \(U_{\varrho}(X_{m})\subseteq K\), so we see by Proposition E.1 that

\[\sup_{f\in K}\lVert f-M_{m-1}(\overline{a}(f))\rVert_{L^{\infty}([-1,1]^{s}; \mathbb{R})}\geq\varrho=c_{1}\cdot m^{-k/s}\]

with \(c_{1}=c_{0}^{-1}\) for every map \(\overline{a}:X\to\mathbb{R}^{m-1}\) which is continuous with respect to some norm on \(V\) and any map \(M_{m-1}:\mathbb{R}^{m-1}\to X\). Using the inequality \(m\leq 2(m-1)\) (note \(m>1\)) we get

\[\sup_{f\in K}\lVert f-M_{m-1}(\overline{a}(f))\rVert_{L^{\infty}([-1,1]^{s}; \mathbb{R})}\geq c_{1}\cdot m^{-k/s}\geq c_{1}\cdot(2(m-1))^{-k/s}\geq c_{2} \cdot(m-1)^{-k/s}\]

with \(c_{2}=c_{1}\cdot 2^{-k/s}\). Hence, the claim has been shown for all numbers \(m\) of the form \(n^{s}-1\) with an integer \(n>1\).

In the end, let \(m\in\mathbb{N}\) be arbitrary and pick \(n\in\mathbb{N}\) with \(n^{s}\leq m<(n+1)^{s}\). For given maps \(\overline{a}:V\to\mathbb{R}^{m}\) and \(M:\mathbb{R}^{m}\to X\) with \(\overline{a}\) continuous with respect to some norm on \(V\), let

\[\begin{array}{ll}\tilde{a}:&V\to\mathbb{R}^{(n+1)^{s}-1},\quad f\mapsto( \overline{a}(f),0)\quad\text{and}\\ M_{(n+1)^{s}-1}:&\mathbb{R}^{(n+1)^{s}-1}\to X,\quad(x,y)\mapsto M(x),\end{array}\]

where \(x\in\mathbb{R}^{m},\ y\in\mathbb{R}^{(n+1)^{s}-1-m}\). Then we get

\[\begin{array}{ll}\sup_{f\in K}\lVert f-M(\overline{a}(f))\rVert_{L^{\infty} ([-1,1]^{s};\mathbb{R})}&=\sup_{f\in K}\lVert f-M_{(n+1)^{s}-1}(\tilde{a}(f) )\rVert_{L^{\infty}([-1,1]^{s};\mathbb{R})}\\ &\geq c_{2}\cdot((n+1)^{s}-1)^{-k/s}\geq c_{2}\cdot(2^{s}n^{s})^{-k/s}\geq c_{ 3}\cdot m^{-k/s}\end{array}\]

with \(c_{3}=c_{2}\cdot 2^{-k}\). Here we used the bound \((n+1)^{s}-1\leq(2n)^{s}\). This proves the full claim. 

Using this theorem, we can now prove Theorem 4.1.

Proof of Theorem 4.1.: Let \(\overline{a}:C^{k}(\Omega_{n};\mathbb{C})\to\mathbb{C}^{m}\) be any map that is continuous with respect to some norm \(\lVert\cdot\rVert_{V}\) on \(C^{k}(\Omega_{n};\mathbb{C})\), and let \(M:\mathbb{C}^{m}\to C(\Omega_{n};\mathbb{C})\) be arbitrary. With \(\varphi_{n},\varphi_{m}\) defined as in Equation (A.1), let

\[\tilde{a}:\quad C^{k}([-1,1]^{2n};\mathbb{R})\to\mathbb{R}^{2m},\quad\tilde{f} \mapsto\varphi_{m}^{-1}\left(\overline{a}\left(\tilde{f}\circ\varphi_{n}^{-1} \big{|}_{\Omega_{n}}\right)\right).\]

Clearly, \(\tilde{a}\) is continuous on \(C^{k}([-1,1]^{2n};\mathbb{R})\) with respect to the norm \(\lVert\cdot\rVert_{\tilde{V}}\) on \(C^{k}([-1,1]^{2n};\mathbb{R})\) defined as

\[\lVert\tilde{f}\rVert_{\tilde{V}}:=\left\lVert\tilde{f}\circ\varphi_{n}^{-1} \big{|}_{\Omega_{n}}\right\rVert_{V}\quad\text{for }\tilde{f}\in C^{k}([-1,1]^{2n};\mathbb{R}).\]

Let

\[\widetilde{M}:\quad\mathbb{R}^{2m}\to C([-1,1]^{2n};\mathbb{R}),\quad\widetilde {M}(x):=\operatorname{Re}(M(\varphi_{m}(x)))\circ\varphi_{n}\big{|}_{[-1,1]^{2 n}}.\]

Then it holds

\[\begin{array}{ll}&\sup_{f\in C^{k}(\Omega_{n};\mathbb{C})}\lVert f-M( \overline{a}(f))\rVert_{L^{\infty}(\Omega_{n};\mathbb{C})}\\ &\lVert f\rVert_{C^{k}(\Omega_{n};c)\leq 1}\\ \geq&\sup_{f\in C^{k}(\Omega_{n};\mathbb{R})}\lVert f-\operatorname{Re}(M( \overline{a}(f)))\rVert_{L^{\infty}(\Omega_{n};\mathbb{R})}\\ &\lVert f\rVert_{C^{k}(\Omega_{n};n)\leq 1}\\ =&\sup_{\tilde{f}\in C^{k}([-1,1]^{2n};\mathbb{R})}\left\lVert\tilde{f}\circ \varphi_{n}^{-1}-\operatorname{Re}\left(M\left(\overline{a}\left(\tilde{f} \circ\varphi_{n}^{-1}\big{|}_{\Omega_{n}}\right)\right)\right)\right\rVert_{L ^{\infty}(\Omega_{n};\mathbb{R})}\\ &\lVert\tilde{f}\rVert_{C^{k}([-1,1]^{2n};\mathbb{R})}\leq 1\\ =&\sup_{\tilde{f}\in C^{k}([-1,1]^{2n};\mathbb{R})}\left\lVert\tilde{f}- \operatorname{Re}\left(M\left(\varphi_{m}\left(\varphi_{m}^{-1}\left(\overline{a} \left(\tilde{f}\circ\varphi_{n}^{-1}\big{|}_{\Omega_{n}}\right)\right)\right) \right)\right)\right)\circ\varphi_{n}\Big{\rVert}_{L^{\infty}([-1,1]^{2n}; \mathbb{R})}\\ &\lVert\tilde{f}\rVert_{C^{k}([-1,1]^{2n};\mathbb{R})}\leq 1\\ \end{array}\]

with a constant \(\tilde{c}=\tilde{c}(n,k)\) provided by Theorem E.2. Hence, the claim follows by choosing \(c=c(n,k):=2^{-k/(2n)}\cdot\tilde{c}\).

As a corollary, we formulate a special case of Theorem 4.1 for the case of shallow complex-valued neural networks.

**Corollary E.3**.: _Let \(n,k\in\mathbb{N}\). Then there exists a constant \(c=c(n,k)>0\) with the following property: For any \(m\in\mathbb{N}\), \(\phi\in C(\mathbb{C};\mathbb{C})\) and any map_

\[\eta:\quad C^{k}\left(\Omega_{n};\mathbb{C}\right)\rightarrow\left(\mathbb{C}^ {n}\right)^{m}\times\mathbb{C}^{m}\times\mathbb{C}^{m},\quad g\mapsto\left( \eta_{1}(g),\eta_{2}(g),\eta_{3}(g)\right)\]

_which is continuous with respect to some norm on \(C^{k}(\Omega_{n};\mathbb{C})\), there exists \(f\in C^{k}\left(\Omega_{n};\mathbb{C}\right)\) satisfying \(\|f\|_{C^{k}(\Omega_{n};\mathbb{C})}\leq 1\) and_

\[\|f-\Psi(f)\|_{L^{\infty}\left(\Omega_{n};\mathbb{C}\right)}\geq c\cdot m^{-k /(2n)},\]

_where \(\Psi(f)\in C(\Omega_{n};\mathbb{C})\) is given by_

\[\Psi(f)(z):=\sum_{j=1}^{m}\left(\eta_{3}(f)\right)_{j}\phi\left(\left[\eta_{1 }(f)\right]_{j}^{T}z+\left(\eta_{2}(f)\right)_{j}\right).\]

Proof.: Using Theorem 4.1, we deduce that there exists \(f\in C^{k}(\Omega_{n};\mathbb{C})\) satisfying \(\|f\|_{C^{k}(\Omega_{n};\mathbb{C})}\leq 1\) and

\[\|f-\Psi(f)\|_{L^{\infty}\left(\Omega_{n};\mathbb{C}\right)}\geq c^{\prime} \cdot(m(n+2))^{-k/(2n)}\]

for a constant \(c^{\prime}=c^{\prime}(n,k)>0\). Hence, the claim follows by letting \(c:=c^{\prime}\cdot(n+2)^{-k/(2n)}\). 

### Proof of Theorem 5.1

We can use Proposition E.1 not only to show that the rate of convergence established in this paper is optimal (which is done in Appendix E.1) but also to show that the problem of approximating \(C^{k}\)-functions using a set of functions that can be parametrized with finitely many parameters is intractable in the sense that it suffers from the curse of dimensionality, provided that the map which assigns to each \(C^{k}\)-function the parameters of the approximating function is continuous. This is the subject of this section.

In [35] a certain space of polynomials was used to show the intractability in the case of _linear_ approximation methods. We are also going to use this class of polynomials, but combine it with Proposition E.1 to infer intractability in the case of _continuous_ approximation methods. We start with a lemma discussing an important property of this space of polynomials. This property is stated as part of a proof in [35], but no complete proof is provided.

**Lemma E.4**.: _Let \(s\in\mathbb{N}\) and consider a function \(f\in C^{\infty}([-1,1]^{s};\mathbb{R})\) which is given via_

\[f(x)=\sum_{\bm{k}\in\{0,1\}^{s}}a_{\bm{k}}x^{\bm{k}}\] (E.3)

_with coefficients \(a_{\bm{k}}\in\mathbb{R}\) for every \(\bm{k}\in\{0,1\}^{s}\). Then it holds_

\[\|f\|_{C^{k}([-1,1]^{s};\mathbb{R})}=\|f\|_{L^{\infty}([-1,1]^{s};\mathbb{R})}\]

_for every \(k\in\mathbb{N}\)._

Proof.: The proof is by induction over \(s\). We start with the case \(s=1\) and note that we can write \(f(x)=ax+b\) with \(a,b\in\mathbb{R}\) in that case. Switching to \(-f\) if necessary, we can assume \(a\geq 0\). Clearly, \(\|f\|_{L^{\infty}([-1,1];\mathbb{R})}\leq|a|+|b|\). Conversely, if \(b\geq 0\) then \(|f(1)|=|a+b|=|a|+|b|\). If otherwise \(b<0\) then \(|f(-1)|=|b-a|=|a-b|=|a|+|b|\). Thus, \(\|f\|_{L^{\infty}([-1,1];\mathbb{R})}=|a|+|b|\). For the derivatives, we have \(\|f^{\prime}\|_{L^{\infty}([-1,1];\mathbb{R})}=|a|\) and \(\|f^{(k)}\|_{L^{\infty}([-1,1];\mathbb{R})}=0\) for \(k\geq 2\). This proves the claim in the case \(s=1\).

We now assume that the claim holds for some arbitrary but fixed \(s\in\mathbb{N}\). We further let \(\alpha\in\mathbb{N}_{0}^{s+1}\) and _fix_ a point \((x_{1},...,x_{s+1})\in[-1,1]^{s+1}\). We decompose \(\alpha=(\alpha^{\prime},\alpha_{s+1})\) with \(\alpha^{\prime}\in\mathbb{N}_{0}^{s}\). Let

\[\widetilde{f}:\quad[-1,1]\rightarrow\mathbb{R},\quad y_{s+1}\mapsto\partial^{ (\alpha^{\prime},0)}f(x_{1},...,x_{s},y_{s+1})\]

and note

\[\partial^{\alpha}f(x_{1},...,x_{s+1})=\widetilde{f}^{(\alpha_{s+1})}(x_{s+1}).\]Note that \(f\) is affine-linear with respect to each variable (with all other variables hold fixed). Hence, \(\widehat{f}\) is an affine function and we can thus apply the case \(s=1\) to \(\widehat{f}\) and get

\[\|\widehat{f}^{(\alpha_{s+1})}\|_{L^{\infty}([-1,1];\mathbb{R})}\leq\|\widehat{ f}\|_{L^{\infty}([-1,1];\mathbb{R})}.\]

Putting this together, we infer

\[|\partial^{\alpha}f(x_{1},...,x_{s+1})|\leq\|\widetilde{f}\|_{L^{\infty}([-1,1 ];\mathbb{R})}=\sup_{y_{s+1}\in[-1,1]}|\partial^{(\alpha^{\prime},0)}f(x_{1},...,x_{s},y_{s+1})|.\] (E.4)

We now _fix_ an arbitrary point \(y_{s+1}\in[-1,1]\) and consider

\[\widehat{f}:\quad[-1,1]^{s}\to\mathbb{R},\quad(y_{1},...,y_{s})\mapsto f(y_{1},...,y_{s},y_{s+1}).\]

Then it holds

\[\partial^{(\alpha^{\prime},0)}f(x_{1},...,x_{s},y_{s+1})=\partial^{\alpha^{ \prime}}\widehat{f}(x_{1},...,x_{s}).\]

Applying the induction hypothesis to \(\widehat{f}\) (which is easily seen to be of the form (E.3)) we get

\[|\partial^{\alpha^{\prime}}\widehat{f}(x_{1},...,x_{s})|\leq\|\widehat{f}\|_ {L^{\infty}([-1,1]^{s};\mathbb{R})}\leq\|f\|_{L^{\infty}([-1,1]^{s+1};\mathbb{ R})}.\] (E.5)

Combining (E.4) and (E.5) yields

\[|\partial^{\alpha}f(x_{1},...,x_{s+1})|\leq\|f\|_{L^{\infty}([-1,1]^{s+1}; \mathbb{R})}.\]

Since \(\alpha\in\mathbb{N}_{0}^{s+1}\) was arbitrary, we get the claim by noting that

\[\|f\|_{C^{k}([-1,1]^{s};\mathbb{R})}\geq\|f\|_{L^{\infty}([-1,1]^{s};\mathbb{ R})}\]

holds trivially for every \(k\in\mathbb{N}\). 

Using the above lemma, we can now deduce that the approximation of smooth functions using continuous approximation methods is intractable in terms of the input dimension.

Proof of Theorem 5.1.: We apply Proposition E.1 to \(X:=C([-1,1]^{s};\mathbb{R})\), \(V:=C^{\infty,*,s}\) and to the set \(\tilde{K}:=\{f\in C^{\infty,*,s}:\;\|f\|_{C^{\infty}([-1,1]^{s};\mathbb{R})} \leq 1\}\) and \(m:=2^{s}-1\). The space

\[X_{m+1}:=\left\{[-1,1]^{s}\ni x\mapsto\sum_{\mathbf{k}\in\{0,1\}^{s}}a_{ \mathbf{k}}x^{\mathbf{k}}:\;a_{\mathbf{k}}\in\mathbb{R}\right\}\]

consisting of all functions considered in the previous Lemma E.4 is an \((m+1)\)-dimensional subspace of \(C([-1,1]^{s};\mathbb{R})\). For every \(f\in X_{m+1}\) with \(\|f\|_{L^{\infty}([-1,1]^{s};\mathbb{R})}\leq 1\), Lemma E.4 tells us \(\|f\|_{C^{\infty}([-1,1]^{s};\mathbb{R})}\leq 1\). Hence, \(U_{1}(X_{m+1})\subseteq K\) and Proposition E.1 then yields the claim. 

_Remark E.5_.: The statement of Theorem 5.1 also holds if the functions satisfy \(\overline{a}:C^{\infty,*,s}\to\mathbb{R}^{m}\) and \(M:\mathbb{R}^{m}\to C([-1,1]^{s};\mathbb{R})\) with \(m\leq 2^{s}-1\). This can be seen by defining

\[\tilde{a}:\quad C^{\infty,*,s}\to\mathbb{R}^{2^{s}-1},\quad f\mapsto( \overline{a}(f),0,...,0)\]

and

\[\widetilde{M}:\quad\mathbb{R}^{2^{s}-1}\to C([-1,1]^{s};\mathbb{R}),\quad(a, b)\mapsto M(a)\]

with \(a\in\mathbb{R}^{m}\) and \(b\in\mathbb{R}^{2^{s}-1-m}\).

The following Corollary E.6 transfers Theorem 5.1 to the complex-valued setting.

**Corollary E.6**.: _Let \(n\in\mathbb{N}\). For any function \(f\in C^{\infty}(\Omega_{n};\mathbb{C})\) we write_

\[\|f\|_{C^{\infty}(\Omega_{n};\mathbb{C})}:=\sup_{k\in\mathbb{N}}\|f\|_{C^{k}( \Omega_{n};\mathbb{C})}\]

_and let \(C^{\infty,*,n}_{\mathbb{C}}\) denote the space consisting of all functions for which this expression is finite. Let \(\overline{a}:C^{\infty,*,n}_{\mathbb{C}}\to\mathbb{C}^{2^{2n-1}-1}\) be continuous with respect to some norm on \(C^{\infty,*,n}_{\mathbb{C}}\) and moreover, let \(M:\mathbb{C}^{2^{2n-1}-1}\to C(\Omega_{n};\mathbb{C})\) be an arbitrary map. Then it holds_

\[\sup_{\begin{subarray}{c}f\in C^{\infty,*,n}_{\mathbb{C}}\\ \|f\|_{C^{\infty}(\Omega_{n};\mathbb{C})}\leq 1\end{subarray}}\|f-M(\overline{a}(f))\|_{L^{ \infty}(\Omega_{n};\mathbb{C})}\geq 1.\]Proof.: The transfer to the complex-valued setting works in the same manner as the proof of Theorem 4.1 (see Appendix E.1). We write \(m:=2^{2n-1}-1\) and note \(2m=2^{2n}-2\leq 2^{2n}-1\). We define \(\tilde{a}:C^{\infty,*,2n}\to\mathbb{R}^{2m}\) and \(\widetilde{M}:\mathbb{R}^{2m}\to C([-1,1]^{2n};\mathbb{R})\) in the same way as in the proof of Theorem 4.1. Using again the same technique as in the proof of Theorem 4.1, we get

\[\sup_{\begin{subarray}{c}f\in C_{\mathbb{C}}^{\infty,*,n}\\ \|f\|_{C^{\infty}(\Omega_{n};\mathbb{C})}\leq 1\end{subarray}}\|f-M( \overline{a}(f))\|_{L^{\infty}(\Omega_{n};\mathbb{C})}\geq\sup_{ \begin{subarray}{c}\tilde{f}\in C^{\infty,*,2n}\\ \|\tilde{f}\|_{C^{\infty}([-1,1]^{2n};\mathbb{R})}\leq 1\end{subarray}}\left\| \tilde{f}-\widetilde{M}(\tilde{a}(\tilde{f}))\right\|_{L^{\infty}([-1,1]^{2n} ;\mathbb{R})}\geq 1,\]

applying Theorem 5.1 in the last inequality, using \(2m\leq 2^{2n}-1\). 

We conclude this appendix by adding a note on the constant appearing in our main approximation bound.

**Corollary E.7**.: _Let \(n\in\mathbb{N}\) with \(n\geq 2\) and \(\alpha>0\) and let \(\phi\in C(\mathbb{C};\mathbb{C})\). Let \(\tilde{c}=\tilde{c}(n,\alpha)>0\) be such that for every \(m\in\mathbb{N}\) there exists a mapping_

\[\eta:\quad C_{\mathbb{C}}^{\infty,*,n}\to\left(\mathbb{C}^{n}\right)^{m}\times \mathbb{C}^{m}\times\mathbb{C}^{m},\quad g\mapsto\left(\eta_{1}(g),\eta_{2}(g),\eta_{3}(g)\right)\]

_that is continuous with respect to any norm on \(C_{\mathbb{C}}^{\infty,*,n}\) and such that_

\[\|f-\Psi(f)\|_{L^{\infty}(\Omega_{n};\mathbb{C})}\leq\left(\tilde{c}\cdot m \right)^{-\alpha}\cdot\|f\|_{C^{\infty}(\Omega_{n};\mathbb{C})},\]

_for every \(f\in C_{\mathbb{C}}^{\infty,*,n}\). Here, \(\Psi(f)\in C(\Omega_{n};\mathbb{C})\) is given by_

\[\Psi(f)(z):=\sum_{j=1}^{m}\left(\eta_{3}(f)\right)_{j}\phi\left(\left[\eta_{1} (f)\right]_{j}^{T}z+\left(\eta_{2}(f)\right)_{j}\right).\]

_Then it necessarily holds \(\tilde{c}\leq 16\cdot 2^{-n}\)._

Proof.: We first assume \(n\geq 4\). We take \(m=\left\lfloor\frac{2^{2n-1}-1}{n+2}\right\rfloor\) and note that then \(m(n+2)\leq 2^{2n-1}-1\). Therefore, Corollary E.6 applies and we infer that for each \(\varepsilon\in(0,1)\), there exists \(f=f_{\varepsilon}\in C_{\mathbb{C}}^{\infty,*,n}\) with \(\|f\|_{C^{\infty}(\Omega_{n};\mathbb{C})}\leq 1\) and such that

\[1-\varepsilon\leq\|f-\Psi(f)\|_{L^{\infty}(\Omega_{n};\mathbb{C})}\leq\left( \tilde{c}\cdot m\right)^{-\alpha}\cdot\|f\|_{C^{\infty}(\Omega_{n};\mathbb{C} )}\leq\left(\tilde{c}\cdot m\right)^{-\alpha}.\]

This then necessarily implies \(\tilde{c}\cdot m\leq 1\) or equivalently \(\tilde{c}\leq 1/m\). It therefore suffices to derive a lower bound for \(m\). Firstly, we note

\[2^{2n-1}=2^{n-3}\cdot 2^{n+2}=2^{n-3}\cdot(1+1)^{n+2}\geq 2^{n-3}(n+3),\]

where we applied Bernoulli's inequality. Because of \(n\geq 4\geq 3\), this yields

\[2^{2n-1}-1\geq 2^{n-3}(n+3)-2^{n-3}=2^{n-3}(n+2).\]

Hence, we get

\[m\geq\frac{2^{2n-1}-1}{n+2}-1\geq 2^{n-3}-1=2^{n-3}(1-2^{3-n})\geq 2^{n-4}=\frac{ 2^{n}}{16}.\]

Here, we used \(n\geq 4\) in the last inequality. An explicit computation shows that the same bounds also holds in the cases \(n=2\) and \(n=3\). This proves the claim. 

## Appendix F Postponed proofs for the optimality results in the case of unrestricted weight selection

### Approximation using Ridge Functions

In this section we prove for \(s\in\mathbb{N}_{\geq 2}\) that every function in \(C^{k}([-1,1]^{s};\mathbb{R})\) can be uniformly approximated with an error of the order \(m^{-k/(s-1)}\) using a linear combination of \(m\) so-called _ridge functions_. In fact, we only consider ridge _polynomials_, meaning functions of the form

\[\mathbb{R}^{s}\to\mathbb{R},\quad x\mapsto p(a^{T}x)\]

[MISSING_PAGE_EMPTY:50]

Proof.: We first pick the constant \(c_{1}=c_{1}(s)\) according to Lemma F.1. Then we define the constant \(c_{2}=c_{2}(s):=(2s)^{s-1}\cdot c_{1}(s)\) and let \(M\in\mathbb{N}\) be the largest integer satisfying

\[c_{2}\cdot M^{s-1}\leq m.\]

Here, we assume without loss of generality that \(m\geq c_{2}\), which can be justified by choosing \(p_{j}=0\) for every \(j\in\{1,...,m\}\) if \(m<c_{2}\), at the cost of possibly enlarging \(c\). Note that the choice of \(M\) implies \(c_{2}\cdot(2M)^{s-1}\geq c_{2}\cdot(M+1)^{s-1}>m\), and thus

\[M\geq\frac{1}{2}\cdot c_{2}^{-1/(s-1)}\cdot m^{1/(s-1)}=c_{3}\cdot m^{1/(s-1)}\] (F.1)

with \(c_{3}=c_{3}(s):=1/2\cdot c_{2}^{-1/(s-1)}\).

Using [36, Proposition 5.9] and Lemma F.1 we can pick \(a_{1},...,a_{m}\in\mathbb{R}^{s}\setminus\{0\}\) satisfying

\[H^{s}_{s(2M-1)}=\operatorname{span}\left\{x\mapsto(a_{j}^{T}x)^{s(2M-1)}:\;j \in\{1,...,m\}\right\},\] (F.2)

where we used that

\[c_{1}\cdot(s(2M-1))^{s-1}\leq c_{1}\cdot(2s)^{s-1}\cdot M^{s-1}=c_{2}\cdot M ^{s-1}\leq m.\]

Here we can assume \(\|a_{j}\|_{2}=r\) for every \(j\in\{1,...,m\}\) since multiplying each \(a_{j}\) with a positive constant does not change the span in (F.2). From [36, Corollary 5.12] we infer that

\[P^{s}_{s(2M-1)}=\operatorname{span}\left\{x\mapsto(a_{j}^{T}x)^{r}:\;j\in\{1,...,m\},\;0\leq r\leq s(2M-1)\right\}.\] (F.3)

Let \(f\in C^{k}([-1,1]^{s};\mathbb{R})\). Then, according to Theorem C.15, there exists a polynomial \(P:\mathbb{R}^{s}\to\mathbb{R}\) of _coordinatewise_ degree at most \(2M-1\) satisfying

\[\|f-P\|_{L^{\infty}([-1,1]^{s};\mathbb{R})}\leq c_{4}\cdot M^{-k}\cdot\|f\|_{ C^{k}([-1,1]^{s};\mathbb{R})},\]

where \(c_{4}=c_{4}(s,k)>0\). Note that by construction it holds \(P\in P^{s}_{s(2M-1)}\). Using (F.3) we deduce the existence of polynomials \(p_{1},...,p_{m}:\mathbb{R}\to\mathbb{R}\) such that

\[P(x)=\sum_{j=1}^{m}p_{j}(a_{j}^{T}x)\quad\text{for all $x\in\mathbb{R}^{s}$}.\]

Combining the previously shown bounds, we get

\[\left\|f(x)-\sum_{j=1}^{m}p_{j}(a_{j}^{T}x)\right\|_{L^{\infty}([- 1,1]^{s};\mathbb{R})} =\|f(x)-P(x)\|_{L^{\infty}([-1,1]^{s};\mathbb{R})}\leq c_{4}\cdot M ^{-k}\cdot\|f\|_{C^{k}([-1,1]^{s};\mathbb{R})}\] \[\stackrel{{\eqref{eq:P_1}}}{{\leq}}c\cdot m^{-k/(s- 1)}\cdot\|f\|_{C^{k}([-1,1]^{s};\mathbb{R})},\]

as desired. Here, we defined \(c=c(s,k):=c_{4}\cdot c_{3}^{-k}\). 

### Proof of Theorem 4.2

Using Theorem F.2, we can prove the following statement for complex-valued \(C^{k}\)-functions, which will play an important role in the proof of Theorem 4.2.

**Proposition F.3**.: _Let \(n,k\in\mathbb{N}\). Then there exists a constant \(c=c(n,k)>0\) with the following property: For any \(m\in\mathbb{N}\) there exist complex vectors \(b_{1},...,b_{m}\in\mathbb{C}^{n}\) with \(\left\|b_{j}\right\|_{2}=1/\sqrt{2n}\) for \(j=1,...,m\) and with the property that for any function \(f\in C^{k}\left(\Omega_{n};\mathbb{C}\right)\) there exist functions \(g_{1},...,g_{m}\in C(\Omega_{1};\mathbb{C})\) such that_

\[\left\|f(z)-\sum_{j=1}^{m}g_{j}\left(b_{j}^{T}\cdot z\right)\right\|_{L^{ \infty}(\Omega_{n};\mathbb{C})}\leq c\cdot m^{-k/(2n-1)}\cdot\|f\|_{C^{k}( \Omega_{n};\mathbb{C})}\,.\]

_Note that the vectors \(b_{1},...b_{m}\) can be chosen independently from the considered function \(f\), whereas \(g_{1},...,g_{m}\) do depend on \(f\)._

[MISSING_PAGE_FAIL:52]

The special activation function that yields the improved approximation rate of \(m^{-k/(2n-1)}\) (see Theorem 4.2) is constructed in the following lemma.

**Lemma F.4**.: _Let \(\{u_{\ell}\}_{\ell=1}^{\infty}\) be an enumeration of the set of complex polynomials in \(z\) and \(\overline{z}\) with coefficients in \(\mathbb{Q}+i\mathbb{Q}\). Then there exists a function \(\phi\in C^{\infty}\left(\mathbb{C};\mathbb{C}\right)\) with the following properties:_

1. _For every_ \(\ell\in\mathbb{N}\) _and_ \(z\in\Omega_{1}\) _one has_ \[\phi(z+3\ell)=u_{\ell}(z).\]
2. \(\phi\) _is non-polyharmonic._

Proof.: Let \(\psi\in C^{\infty}\left(\mathbb{C};\mathbb{R}\right)\) with \(0\leq\psi\leq 1\) and

\[\psi\big{|}_{\Omega_{1}}\equiv 1,\qquad\mathrm{supp}(\psi)\subseteq\widetilde{ \Omega},\]

where \(\widetilde{\Omega}:=\left\{z\in\mathbb{C}:\left|\mathrm{Re}\left(z\right) \right|,\left|\mathrm{Im}\left(z\right)\right|<\frac{3}{2}\right\}\). We then define

\[\phi:=f\cdot\psi+\sum_{\ell=1}^{\infty}u_{\ell}(\bullet-3\ell)\cdot\psi( \bullet-3\ell),\]

where \(f(z)=e^{\mathrm{Re}(z)}\). Note that \(\phi\) is smooth since it is a locally finite sum of smooth functions. Furthermore, \(\phi\) is non-polyharmonic on the interior of \(\Omega_{1}\), since the calculation in the proof of Proposition A.2 shows for \(z\) in the interior of \(\Omega_{1}\) and \(\rho:\mathbb{R}\to\mathbb{R},\;t\mapsto e^{t}\) that

\[\left|\partial_{\mathrm{wirt}}^{m}\overline{\partial}_{\mathrm{wirt}}^{\ell} \phi(z)\right|=\left|\partial_{\mathrm{wirt}}^{m}\overline{\partial}_{ \mathrm{wirt}}^{\ell}f(z)\right|=\frac{1}{2^{m+\ell}}\left|\rho^{(m+\ell)}( \mathrm{Re}(z))\right|>0\]

for arbitrary \(m,\ell\in\mathbb{N}_{0}\). Finally, property (1) follows directly by construction of \(\phi\) because

\[(\widetilde{\Omega}+3\ell)\cap(\widetilde{\Omega}+3\ell^{\prime})=\varnothing\]

for \(\ell\neq\ell^{\prime}\). 

Using the properties of the special activation function constructed in Lemma F.4 and applying the approximation result from Proposition F.3 we can now prove Theorem 4.2.

Proof of Theorem 4.2.: Let \(\phi\) be the activation function constructed in Lemma F.4. We choose the constant \(c\) according to Proposition F.3. Let \(m\in\mathbb{N}\) and \(f\in C^{k}\left(\Omega_{n};\mathbb{C}\right)\). We can without loss of generality assume that \(f\not\equiv 0\). Again, according to Proposition F.3, we can choose \(\rho_{1},...,\rho_{m}\in\mathbb{C}^{n}\) with \(\|_{2}\rho_{j}\|=1/\sqrt{2n}\) and \(g_{1},...,g_{m}\in C\left(\Omega;\mathbb{C}\right)\) with the property

\[\left\|f(z)-\sum_{j=1}^{m}g_{j}\left(\rho_{j}^{T}z\right)\right\|_{L^{\infty} \left(\Omega_{n}\right)}\leq c\cdot m^{-k/(2n-1)}\cdot\left\|f\right\|_{C^{k} \left(\Omega_{n};\mathbb{C}\right)}.\]

Recall from Lemma F.4 that \(\{u_{\ell}\}_{\ell=1}^{\infty}\) is an enumeration of the set of complex polynomials in \(z\) and \(\overline{z}\). Hence, using the complex version of the Stone-Weierstrass-Theorem (see for instance [21, Theorem 4.51]), we can pick \(\ell_{1},...,\ell_{m}\in\mathbb{N}\) such that

\[\left\|g_{j}-u_{\ell_{j}}\right\|_{L^{\infty}\left(\Omega_{1};\mathbb{C} \right)}\leq m^{-1-k/(2n-1)}\cdot\left\|f\right\|_{C^{k}\left(\Omega_{n}; \mathbb{C}\right)}\] (F.5)for every \(j\in\{1,...,m\}\). Since \(\phi\left(\bullet+3\ell\right)=u_{\ell}\) on \(\Omega_{1}\) for each \(\ell\in\mathbb{N}\), and since \(\rho_{j}^{T}z\in\Omega_{1}\) for \(j\in\{1,...,m\}\) and \(z\in\Omega_{n}\), we estimate

\[\left\|f(z)-\sum_{j=1}^{m}\phi\left(\rho_{j}^{T}z+3\ell_{j}\right) \right\|_{L^{\infty}\left(\Omega_{n};\mathbb{C}\right)}\] \[\leq\left\|f(z)-\sum_{j=1}^{m}g_{j}\left(\rho_{j}^{T}z\right) \right\|_{L^{\infty}\left(\Omega_{n};\mathbb{C}\right)}+\sum_{j=1}^{m}\left\|g _{j}\left(\rho_{j}^{T}z\right)-\phi\left(\rho_{j}^{T}\cdot z+3\ell_{j}\right) \right\|_{L^{\infty}\left(\Omega_{n};\mathbb{C}\right)}\] \[\leq c\cdot m^{-k/(2n-1)}\cdot\left\|f\right\|_{C^{k}\left( \Omega_{n};\mathbb{C}\right)}+\sum_{j=1}^{m}\left\|g_{j}\left(z\right)-u_{ \ell_{j}}\left(z\right)\right\|_{L^{\infty}\left(\Omega_{1};\mathbb{C}\right)}\] \[\stackrel{{\text{(\ref{eq:1})}}}{{\leq}}c\cdot m^{-k /(2n-1)}\cdot\left\|f\right\|_{C^{k}\left(\Omega_{n};\mathbb{C}\right)}+m^{-k/ (2n-1)}\cdot\left\|f\right\|_{C^{k}\left(\Omega_{n};\mathbb{C}\right)}\] \[=(c+1)\cdot m^{-k/(2n-1)}\cdot\left\|f\right\|_{C^{k}\left(\Omega _{n};\mathbb{C}\right)}.\qed\]

### Proof of Theorem 4.3

As a preparation for the proof of Theorem 4.3, we first prove a similar result in the real-valued setting. We remark that the proof idea is inspired by the proof of [46, Theorem 4].

**Theorem F.5**.: _Let \(n,k\in\mathbb{N}\) and_

\[\phi:\quad\mathbb{R}\to\mathbb{R},\quad\phi(x):=\frac{1}{1+e^{-x}}\]

_be the sigmoid function. Then there exists a constant \(c=c(n,k)>0\) with the following property: If the numbers \(\varepsilon\in(0,\frac{1}{2})\) and \(m\in\mathbb{N}\) are such that for every function \(f\in C^{k}\left([-1,1]^{n};\mathbb{R}\right)\) with \(\left\|f\right\|_{C^{k}\left([-1,1]^{n};\mathbb{R}\right)}\leq 1\) there exist coefficients \(\rho_{1},...,\rho_{m}\in\mathbb{R}^{n}\), \(\eta_{1},...,\eta_{m}\in\mathbb{R}\) and \(\sigma_{1},...,\sigma_{m}\in\mathbb{R}\) satisfying_

\[\left\|f(x)-\sum_{j=1}^{m}\sigma_{j}\cdot\phi\left(\rho_{j}^{T}x+\eta_{j} \right)\right\|_{L^{\infty}\left([-1,1]^{n};\mathbb{R}\right)}\leq\varepsilon,\]

_then necessarily_

\[m\geq c\cdot\frac{\varepsilon^{-n/k}}{\ln\left(1/\varepsilon\right)}.\]

Proof.: We first pick a function \(\psi\in C^{\infty}\left(\mathbb{R}^{n};\mathbb{R}\right)\) with the property that \(\psi(0)=1\) and \(\psi(x)=0\) for every \(x\in\mathbb{R}^{n}\) with \(\left\|x\right\|_{2}>\frac{1}{4}\). We then choose

\[c_{1}=c_{1}(n,k):=\left(\left\|\psi\right\|_{C^{k}\left([-1,1]^{n};\mathbb{R} \right)}\right)^{-1}.\]

Now, let \(\varepsilon\in(0,\frac{1}{2})\) and \(m\in\mathbb{N}\) be arbitrary with the property stated in the formulation of the theorem. If \(\varepsilon>\frac{c_{1}}{2}\cdot\frac{1}{6^{k}}\), then \(m\geq c\cdot\frac{\varepsilon^{-n/k}}{\ln(1/\varepsilon)}\) trivially holds (as long as \(c=c(n,k)>0\) is sufficiently small). Hence, we can assume that \(\varepsilon\leq\frac{c_{1}}{2}\cdot\frac{1}{6^{k}}\). Now, let \(N\) be the smallest integer with \(N\geq 2\), for which

\[\frac{c_{1}}{2^{k+1}}\cdot N^{-k}\leq\varepsilon.\]

Note that this implies

\[N^{k}\geq\frac{c_{1}}{\varepsilon}\cdot\frac{1}{2^{k+1}}\geq\frac{c_{1}}{2^{k+ 1}}\cdot\frac{2}{c_{1}}\cdot 6^{k}=3^{k}\]

and hence \(N\geq 3\), whence \(N-1\geq 2\). Therefore, by minimality of \(N\), and since \(\frac{N}{2}\leq N-1\) because of \(N\geq 2\), it follows that

\[\varepsilon<\frac{c_{1}}{2^{k+1}}\cdot(N-1)^{-k}\leq\frac{c_{1}}{2^{k+1}}2^{k} \cdot N^{-k}=\frac{c_{1}}{2}\cdot N^{-k}.\] (F.6)Now, for every \(\alpha\in\{-N,...,N\}^{n}\) pick \(z_{\alpha}\in\{0,1\}\) arbitrary and let \(y_{\alpha}:=z_{\alpha}c_{1}N^{-k}\). Define the function

\[f(x):=\sum_{\alpha\in\{-N,...,N\}^{n}}y_{\alpha}\cdot\psi\left(Nx-\alpha\right),\quad x\in\mathbb{R}^{n}.\]

Clearly, \(f\in C^{\infty}(\mathbb{R}^{n};\mathbb{R})\). Furthermore, since the supports of the functions \(\psi(\bullet-\alpha),\;\alpha\in\mathbb{Z}^{n}\) are pairwise disjoint, we see for any multi-index \(\mathbf{k}\in\mathbb{N}_{0}^{n}\) with \(|\mathbf{k}|\leq k\) that

\[\left\|\partial^{\mathbf{k}}f\right\|_{L^{\infty}([-1,1]^{n}; \mathbb{R})} \leq N^{|\mathbf{k}|}\cdot\max_{\alpha}|y_{\alpha}|\cdot\left\| \partial^{\mathbf{k}}\psi\right\|_{L^{\infty}([-1,1]^{n};\mathbb{R})}\] \[\leq N^{k}\cdot\max_{\alpha}|y_{\alpha}|\cdot\left\|\psi\right\| _{C^{k}([-1,1]^{n};\mathbb{R})}\leq 1,\]

so we conclude that \(\left\|f\right\|_{C^{k}([-1,1]^{n};\mathbb{R})}\leq 1\). Additionally, for any fixed \(\beta\in\{-N,...,N\}^{n}\) we see

\[f\left(\frac{\beta}{N}\right)=y_{\beta},\]

by choice of \(\psi\). See also Figure 5 for an illustration of the function \(f\).

By assumption, we can choose suitable coefficients \(\rho_{1},...,\rho_{m}\in\mathbb{R}^{n}\), \(\eta_{1},...,\eta_{m}\in\mathbb{R}\) and furthermore \(\sigma_{1},...,\sigma_{m}\in\mathbb{R}\) such that

\[\left\|f-g\right\|_{L^{\infty}([-1,1]^{n};\mathbb{R})}\leq\varepsilon\]

for

\[g:=\sum_{j=1}^{m}\sigma_{j}\cdot\phi\left(\rho_{j}^{T}\cdot\bullet+\eta_{j} \right).\]

Letting

\[\tilde{g}:=g(\bullet/N)=\sum_{j=1}^{m}\sigma_{j}\cdot\phi\left(\frac{\rho_{j }^{T}}{N}\cdot\bullet+\eta_{j}\right),\] (F.7)

we see for every \(\alpha\in\{-N,...,N\}^{n}\) that

\[\tilde{g}(\alpha)=g\left(\frac{\alpha}{N}\right)\begin{cases}\geq y_{\alpha}- \varepsilon=c_{1}N^{-k}-\varepsilon\overset{\eqref{eq:f}}{>}(c_{1}/2)\,N^{-k},&\text{if }z_{\alpha}=1,\\ \leq y_{\alpha}+\varepsilon\overset{\eqref{eq:f}}{<}(c_{1}/2)\,N^{-k},&\text{ if }z_{\alpha}=0.\end{cases}\]

Therefore, we get \(\mathbbm{1}\left(\tilde{g}>(c_{1}/2)N^{-k}\right)(\alpha)=z_{\alpha}\) for any \(\alpha\in\{-N,...,N\}^{n}\). Since the choice of \(z_{\alpha}\) has been arbitrary, it follows that the set

\[H:=\left\{\mathbbm{1}\left(\tilde{g}>(c_{1}/2)N^{-k}\right)\big{|}_{[-N,...,N \}^{n}:\;\tilde{g}\text{ of form \eqref{eq:f}}\right\}\]

shatters the whole set \(\{-N,...,N\}^{n}\). Therefore, we conclude that

\[\text{VC}(H)\geq(2N+1)^{n}\geq N^{n}.\]

On the other hand, [4, Theorem 8.11] shows that

\[\text{VC}(H)\leq 2m(n+2)\log_{2}(60n\cdot N)\leq c_{3}\cdot m\cdot\ln(N)\]

with a suitably chosen constant \(c_{3}=c_{3}(n)\). Here we used that \(N\geq 3\) so that \(\ln(N)\geq\ln(3)>0\). Combining those two inequalities yields

\[m\geq\frac{N^{n}}{c_{3}\cdot\ln(N)}.\]

Using that \(N\geq c_{4}\cdot\varepsilon^{-1/k}\) with \(c_{4}:=c_{4}(n,k)=\left(\frac{c_{1}}{2^{k+1}}\right)^{1/k}\) and \(N\leq c_{5}\cdot\varepsilon^{-1/k}\) with the definition \(c_{5}:=c_{5}(n,k)=\left(\frac{c_{1}}{2}\right)^{1/k}\), we see that

\[m\geq\frac{c_{4}^{n}\cdot\varepsilon^{-n/k}}{c_{3}\cdot\ln\left(c_{5}\cdot \varepsilon^{-1/k}\right)}\geq c_{6}\cdot\frac{\varepsilon^{-n/k}}{\ln\left(1/ \varepsilon\right)}\]

with \(c_{6}=c_{6}(n,k)>0\) chosen appropriately.

As a corollary, we get a similar result for complex-valued neural networks.

**Corollary F.6**.: _Let \(n,k\in\mathbb{N}\) and_

\[\phi:\quad\mathbb{C}\rightarrow\mathbb{C},\quad\phi(z):=\frac{1}{1+e^{-\operatorname {Re}(z)}}.\]

_Then there exists a constant \(c=c(n,k)>0\) with the following property: If \(\varepsilon\in(0,\frac{1}{2})\) and \(m\in\mathbb{N}\) are such that for every function \(f\in C^{k}\left(\Omega_{n};\mathbb{C}\right)\) with \(\left\|f\right\|_{C^{k}\left(\Omega_{n};\mathbb{C}\right)}\leq 1\) there exist coefficients \(\rho_{1},...,\rho_{m}\in\mathbb{C}^{n}\), \(\eta_{1},...,\eta_{m}\in\mathbb{C}\) and \(\sigma_{1},...,\sigma_{m}\in\mathbb{C}\) satisfying_

\[\left\|f(z)-\sum_{j=1}^{m}\sigma_{j}\cdot\phi\left(\rho_{j}^{T}z+\eta_{j} \right)\right\|_{L^{\infty}\left(\Omega_{n};\mathbb{C}\right)}\leq\varepsilon,\]

_then necessarily_

\[m\geq c\cdot\frac{\varepsilon^{-2n/k}}{\ln\left(1/\varepsilon\right)}.\]

Proof.: We choose the constant \(c=c(2n,k)\) according to the previous Theorem F.5 and let \(\varphi_{n}\) as in (A.1). Then, let \(\varepsilon\in(0,\frac{1}{2})\) and \(m\in\mathbb{N}\) with the properties assumed in the statement of the corollary. If we then take an arbitrary function \(f\in C^{k}\left([-1,1]^{2n};\mathbb{R}\right)\) with \(\left\|f\right\|_{C^{k}\left([-1,1]^{2n};\mathbb{R}\right)}\leq 1\), we deduce the existence of \(\rho_{1},...,\rho_{m}\in\mathbb{C}^{n}\), \(\eta_{1},...,\eta_{m}\in\mathbb{C}\) and \(\sigma_{1},...,\sigma_{m}\in\mathbb{C}\), such that

\[\left\|f(x)-\operatorname{Re}\left(\sum_{j=1}^{m}\sigma_{j}\cdot \phi\left(\rho_{j}^{T}\cdot\varphi_{n}(x)+\eta_{j}\right)\right)\right\|_{L^{ \infty}\left([-1,1]^{2n};\mathbb{R}\right)}\] \[\leq \left\|(f\circ\varphi_{n}^{-1})(z)-\sum_{j=1}^{m}\sigma_{j}\cdot \phi\left(\rho_{j}^{T}z+\eta_{j}\right)\right\|_{L^{\infty}\left(\Omega_{n}; \mathbb{C}\right)}\leq\varepsilon.\]

In the next step, we show that

\[\mathbb{R}^{2n}\ni x\mapsto\operatorname{Re}\left(\sum_{j=1}^{m}\sigma_{j} \cdot\phi\left(\rho_{j}^{T}\cdot\varphi_{n}(x)+\eta_{j}\right)\right)\]

is a real-valued shallow neural network with \(m\) neurons in the hidden layer and the real sigmoid function as activation function. Then the claim follows using Theorem F.5.

For every \(j\in\{1,...,m\}\) we pick a matrix \(\tilde{\rho_{j}}\in\mathbb{R}^{2n\times 2}\) with the property that one has

\[\tilde{\rho_{j}}^{T}\cdot\varphi_{n}^{-1}(z)=\varphi_{1}^{-1}\left(\rho_{j}^{ T}\cdot z\right)\]

for every \(z\in\mathbb{C}^{n}\). This is possible, since this is equivalent to

\[\tilde{\rho_{j}}^{T}v=\varphi_{1}^{-1}(\rho_{j}^{T}\varphi_{n}(v))\]

Figure 5: Illustration of the function \(f\) considered in the proof of Theorem F.5.

[MISSING_PAGE_FAIL:57]