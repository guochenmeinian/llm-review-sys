# Amortizing intractable inference in diffusion models

for vision, language, and control

 Siddarth Venkatraman*

Mila, Universite de Montreal

Moksh Jain*

Mila, Universite de Montreal

Luca Scimeca*

Mila, Universite de Montreal

Mila, Universite de Montreal

Minsu Kim*

Mila, Universite de Montreal

KAIST

&Marcin Sendera*

Mila, Universite de Montreal

Jagiellonian University

&Luca Scimeca*

Mila, Universite de Montreal

Luke Rowe

Mila, Universite de Montreal

&Sarthak Mittal

Mila, Universite de Montreal

Pablo Lemos

Mila, Universite de Montreal

Ciela Institute

Dreamfold

Emmanuel Bengio

Recursion

&Alexandre Adam

Mila, Universite de Montreal

Ciela Institute

&Jarrid Rector-Brooks

Mila, Universite de Montreal

Dreamfold

Yoshua Bengio

Mila, Universite de Montreal

CIFAR

&Glen Berseth

Mila, Universite de Montreal

CIFAR

&Nikolay Malkin

Mila, Universite de Montreal

University of Edinburgh

&mila.quebec

###### Abstract

Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem. This paper studies _amortized_ sampling of the posterior over data, \(\mathbf{x}\sim p^{\text{post}}(\mathbf{x})\propto p(\mathbf{x})r(\mathbf{x})\), in a model that consists of a diffusion generative model prior \(p(\mathbf{x})\) and a black-box constraint or likelihood function \(r(\mathbf{x})\). We state and prove the asymptotic correctness of a data-free learning objective, _relative trajectory balance_, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases. Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage. We illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors across a collection of experiments: in vision (classifier guidance), language (infilling under a discrete diffusion LLM), and multimodal data (text-to-image generation). Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning. Code is available at this link.

## 1 Introduction

Diffusion models [68; 27; 72] are a powerful class of hierarchical generative models, used to model complex distributions over images [51; 12; 63], text [4; 13; 40; 24; 23; 43], and actions in reinforcement learning [30; 83; 32] to name a few. In each of these domains, downstream problems require sampling product distributions, where a pretrained diffusion model serves as a prior \(p(\mathbf{x})\) that is multiplied by an auxiliary constraint \(r(\mathbf{x})\). For example, if \(p(\mathbf{x})\) is a prior over images defined by a diffusion model, and \(r(\mathbf{x})=p(c\mid\mathbf{x})\) is the likelihood that an image \(\mathbf{x}\) belongs to class \(c\), then class-conditional image generation requires sampling from the Bayesian posterior \(p(\mathbf{x}\mid c)\propto p(\mathbf{x})p(c\mid\mathbf{x})\). In offline reinforcement learning, if \(\mu(a\mid s)\) is a conditional diffusion model over actions serving as a behavior policy, KL-constrained policy improvement [55; 44] requires sampling from the normalized product of \(\mu(a\mid s)\) with a Boltzmann distribution defined by a \(Q\)-function, \(\pi^{*}(a\mid s)\propto\mu(a\mid s)\exp(\beta Q(s,a))\). In language modeling, various conditional generation problems [43; 23; 29] amount to posterior sampling under a discrete diffusion model prior. Table 1 summarizes four such problems that the proposed method improves upon prior work.

The hierarchical nature of the generative process in diffusion models, which generate samples from \(p(\mathbf{x})\) by a deep chain of stochastic transformations, makes exact sampling from posteriors \(p(\mathbf{x})r(\mathbf{x})\) under a black-box function \(r(\mathbf{x})\) intractable. Common solutions to this problem involve inference techniques based on linear approximations [73; 33; 31; 11] or stochastic optimization [22; 48]. Others estimate the 'guidance' term - the difference in drift functions between the diffusion models sampling the prior and posterior - by training a classifier on noised data [12], but when such data is not available, one must resort to approximations or Monte Carlo estimates [70; 14; 10], which are challenging to scale to high-dimensional problems. Reinforcement learning methods that have recently been proposed for this problem [8; 16] are biased and prone to mode collapse (Fig. 1).

Contributions.Inspired by recent techniques in training diffusion models to sample distributions defined by unnormalized densities [89; 62; 78; 65], we propose an asymptotically unbiased training objective, called relative trajectory balance (RTB), for training diffusion models that sample from posterior distributions under a diffusion model prior (SS2.2). RTB is derived from the perspective of diffusion models as continuous generative flow networks [38]. This perspective also allows us to freely leverage off-policy training, when data with high density under the posterior is available (SS2.3). RTB can be applied to iterative generative processes beyond standard diffusion models: our methods generalize to discrete diffusion models and extend existing methods for autoregressive language models (SS2.4).

Our experiments demonstrate the versatility of our approach in a variety of domains:

* In **vision**, we show that RTB achieves competitive classifier-guided image generation for unconditional diffusion vision priors (SS3.1) and can be used to improve caption-conditioned generation under text-to-image foundation model priors (SS3.2).
* In **language modeling**, we report strong results for infilling tasks with discrete diffusion language models (SS3.3).
* Finally, we show that RTB achieves state-of-the-art results on **continuous control** benchmarks that leverage score-based behavior priors (SS3.4).

## 2 Learning posterior samplers with diffusion priors

We consider the problem of posterior inference under a prior given by a hierarchical generative model. In this section, we present the mathematical setting (SS2.1), our proposed RTB objective (SS2.2), and training methods for RTB (SS2.3). We will first discuss the case of a diffusion prior over \(\mathbb{R}^{d}\), and later discuss how the methods generalize to arbitrary hierarchical priors (SS2.4).

\begin{table}
\begin{tabular}{l l l l} \hline Domain & Prior \(p(\mathbf{x})\) & Constraint \(r(\mathbf{x})\) & Posterior \\ \hline Conditional image generation (ยง3.1) & Image diffusion model \(p(\mathbf{x})\) & Classifier likelihood \(p(c\mid\mathbf{x})\) & Class-conditional distribution \(p(\mathbf{x}\mid c)\) \\ Text-to-image generation (ยง3.2) & Text-to-image foundation model & RLBF reward model & Aligned text-to-image model \\ Language infilling (ยง3.3) & Discrete diffusion model & Autoregressive completion likelihood & Infilling distribution \\ Offline RL policy extraction (ยง3.4) & Diffusion model as behavior policy & Boltzmann data of \(Q\)-function & Optimal KL-constrained policy \\ \hline \end{tabular}
\end{table}
Table 1: Sources of diffusion priors and constraints.

### Background and setting: Diffusion models as hierarchical generative models

A denoising diffusion model generates data \(\mathbf{x}_{1}\) by a Markovian generative process:

\[\text{(noise)}\quad\mathbf{x}_{0}\to\mathbf{x}_{\Delta t}\to\mathbf{x}_{2 \Delta t}\to\ldots\to\mathbf{x}_{1}=\mathbf{x}\quad\text{(data)},\] (1)

where \(\Delta t=\frac{1}{T}\) and \(T\) is the number of discretization steps.1 The initial distribution \(p(\mathbf{x}_{0})\) is fixed (typically to \(\mathcal{N}(\mathbf{0},\bm{I})\)) and the transition from \(\mathbf{x}_{t-1}\) to \(\mathbf{x}_{t}\) is modeled as a Gaussian perturbation with time-dependent variance:

Footnote 1: The time indexing suggestive of an SDE discretization is used for consistency with the diffusion samplers literature [89; 65]. The indexing \(\mathbf{x}_{T}\to\mathbf{x}_{T-1}\to\cdots\to\mathbf{x}_{0}\) is often used for diffusion models trained from data.

\[p(\mathbf{x}_{t+\Delta t}\mid\mathbf{x}_{t})=\mathcal{N}(\mathbf{x}_{t+ \Delta t}\mid\mathbf{x}_{t}+u_{t}(\mathbf{x}_{t})\Delta t,\sigma_{t}^{2} \Delta t\bm{I}).\] (2)

The scaling of the mean and variance by \(\Delta t\) is insubstantial for fixed \(T\), but ensures that the diffusion process is well-defined in the limit \(T\to\infty\) assuming regularity conditions on \(u_{t}\)[64; 53]. The process given by (1, 2) is then identical to Euler-Maruyama integration of the stochastic differential equation (SDE) \(d\mathbf{x}_{t}=u_{t}(\mathbf{x}_{t})\ dt+\sigma_{t}\ d\mathbf{w}_{t}\).

The likelihood of a denoising trajectory \(\mathbf{x}_{0}\to\mathbf{x}_{\Delta t}\to\cdots\to\mathbf{x}_{1}\) factors as

\[p(\mathbf{x}_{0},\mathbf{x}_{\Delta t},\ldots,\mathbf{x}_{1})=p(\mathbf{x}_{0 })\prod_{i=1}^{T}p\left(\mathbf{x}_{i\Delta t}\mid\mathbf{x}_{(i-1)\Delta t}\right)\] (3)

and defines a marginal density over the data space:

\[p(\mathbf{x}_{1})=\int\,p(\mathbf{x}_{0},\mathbf{x}_{\Delta t},\ldots,\mathbf{ x}_{1})\ d\mathbf{x}_{0}\ d\mathbf{x}_{\Delta t}\ldots d\mathbf{x}_{1-\Delta t}.\] (4)

A reverse-time process, \(\mathbf{x}_{1}\to\mathbf{x}_{1-\Delta t}\to\cdots\to\mathbf{x}_{0}\), with densities \(q\), can be defined analogously, and similarly defines a conditional density over trajectories:

\[q(\mathbf{x}_{0},\mathbf{x}_{\Delta t},\ldots,\mathbf{x}_{1-\Delta t}\mid \mathbf{x}_{1})=\prod_{i=1}^{T}q(\mathbf{x}_{(i-1)\Delta t}\mid\mathbf{x}_{i \Delta t}).\] (5)

In the training of diffusion models, as discussed below, the process \(q\) is typically fixed to a simple distribution (usually a discretized Ornstein-Uhlenbeck process), and the result of training is that \(p\) and \(q\) are close as distributions over trajectories.

Diffusion model training as divergence minimization.Diffusion models parametrize the drift \(u_{t}(\mathbf{x}_{t})\) in (Equation 2) as a neural network \(u(\mathbf{x}_{t},t;\theta)\) with parameters \(\theta\) and taking \(\mathbf{x}_{t}\) and \(t\) as input. We denote the distributions over trajectories induced by (Equation 3, Equation 4) by \(p_{\theta}\) to show their dependence on the parameter.

In the most common setting, diffusion models are trained to maximize the likelihood of a dataset. In the notation above, this corresponds to assuming \(q(\mathbf{x}_{1})\) is fixed to an empirical measure (with the

Figure 1: Sampling densities learned by various posterior inference methods. The prior is a diffusion model sampling a mixture of 25 Gaussians (a) and the posterior is the product of the prior with a constraint that masks all but 9 of the modes (b). Our method (RTB) samples close to the true posterior (c). RL methods with tuned KL regularization yield inaccurate inference (d), while without KL regularization, they mode-collapse (e). A classifier guidance (CG) approximation (f) results in biased outcomes. For details, see ยงC.

points of a training dataset \(\mathcal{D}\) assumed to be i.i.d. samples from \(q(\mathbf{x}_{1})\)). Training minimizes with respect to \(\theta\) the divergence between the processes \(q\) and \(p_{\,\theta}\):

\[D_{\text{KL}}(q(\mathbf{x}_{0},\mathbf{x}_{M},\ldots,\mathbf{x}_{ 1})\parallel p_{\,\theta}(\mathbf{x}_{0},\mathbf{x}_{M},\ldots,\mathbf{x}_{1}))\] (6) \[=D_{\text{KL}}(q(\mathbf{x}_{1})\parallel p_{\,\theta}(\mathbf{x }_{1}))+\mathbb{E}_{\mathbf{x}_{1}-q(\mathbf{x}_{1})}D_{\text{KL}}(q(\mathbf{ x}_{0},\mathbf{x}_{M},\ldots,\mathbf{x}_{1-\Delta t}\mid\mathbf{x}_{1})\parallel p_{\, \theta}(\mathbf{x}_{0},\mathbf{x}_{M},\ldots,\mathbf{x}_{1-\Delta t}\mid \mathbf{x}_{1}))\] \[\geq D_{\text{KL}}(q(\mathbf{x}_{1})\parallel p_{\,\theta}( \mathbf{x}_{1}))=\mathbb{E}_{\mathbf{x}_{1}-q(\mathbf{x}_{1})}\left[-\log p_{ \,\theta}(\mathbf{x}_{1})\right]+\text{const}.\]

where the inequality - an instance of the data processing inequality for the KL divergence - shows that minimizing the divergence between distributions over trajectories is equivalent to maximizing a lower bound on the data log-likelihood under the model \(p_{\,\theta}\).

As shown in [71], minimization of the KL in (Equation 6) is essentially equivalent to the traditional approach to training diffusion models via denoising score matching [80; 68; 27]. Such training exploits that for typical choices of the noising process \(q\), the optimal \(u_{t}(\mathbf{x}_{t})\) can be expressed in terms of the Stein score of \(q(\mathbf{x}_{1})\) convolved with a Gaussian, allowing an efficient stochastic regression objective for \(u_{t}\). For full generality of our exposition for arbitrary iterative generative processes, we prefer to think of (Equation 6) as the primal objective and denoising score matching as an efficient means of minimizing it.

Trajectory balance and distribution-matching training.From (Equation 6) we also see that the bound is tight if the conditionals of \(p_{\,\theta}\) and \(q\) on \(\mathbf{x}_{1}\) coincide, _i.e._, \(q\) is equal to the posterior distribution of \(p_{\,\theta}\) conditioned on \(\mathbf{x}_{1}\). Indeed, the model \(p_{\,\theta}\) minimizes (Equation 6) for a distribution with continuous density \(q(\mathbf{x}_{1})\) if and only if, for all denoising trajectories,

\[p_{\,\theta}(\mathbf{x}_{0},\mathbf{x}_{\Delta t},\ldots,\mathbf{x}_{1})=q( \mathbf{x}_{1})q(\mathbf{x}_{0},\mathbf{x}_{M},\ldots,\mathbf{x}_{1-\Delta t} \mid\mathbf{x}_{1}).\] (7)

This was named the _trajectory balance (TB) constraint_ by [38] - by analogy with a constraint for discrete-space iterative sampling [46] - and is a time-discretized version of a constraint used for enforcing equality of continuous-_time_ path space measures in [52] (see [7] for asymptotic analysis).

In [61; 38], the constraint (7) was used for the training of diffusion models in a _data-free_ setting, where instead of i.i.d. samples from \(q(\mathbf{x}_{1})\) one has access to a (possibly unnormalized) density \(q(\mathbf{x}_{1})=e^{-\mathcal{E}(\mathbf{x}_{1})}/Z\) from which one wishes to sample. These objectives minimize the squared log-ratio between the two sides of (7), which allows the trajectories \(\mathbf{x}_{0}\rightarrow\mathbf{x}_{\Delta t}\rightarrow\cdots\rightarrow \mathbf{x}_{1}\) used for training to be sampled from any training distribution, such as 'exploratory' modifications of \(p_{\,\theta}\) or trajectories found by local search (MCMC) in the target space. The flexibility of off-policy exploration that this allows was studied by [65]. Such objectives contrast with on-policy, simulation-based approaches that require differentiating through the sampling process [_e.g._, 89; 78; 6; 79].

### Intractable inference under diffusion priors

Consider a diffusion model \(p_{\,\theta}\), defining a marginal density \(p_{\,\theta}(\mathbf{x}_{1})\), and a positive constraint function \(r:\mathbb{R}^{d}\rightarrow\mathbb{R}_{>0}\). We are interested in training a diffusion model \(p_{\,\phi}^{\text{post}}\), with drift function \(u_{\,\phi}^{\text{post}}\), that would sample the product distribution \(p^{\text{post}}(\mathbf{x}_{1})\propto p_{\,\theta}(\mathbf{x}_{1})r(\mathbf{ x}_{1})\). If \(r(\mathbf{x}_{1})=p(\,\mathbf{y}\mid\mathbf{x}_{1})\) is a conditional distribution over another variable \(\mathbf{y}\), then \(p^{\text{post}}\) is the Bayesian posterior \(p_{\,\theta}(\mathbf{x}_{1}\mid\mathbf{y})\).

Because samples from \(p^{\text{post}}(\mathbf{x}_{1})\) are not assumed to be available, one cannot directly train \(p\) using the objective (6). Nor can one directly apply objectives for distribution-matching training, such as those that enforce (7), since the marginal \(p_{\,\theta}(\mathbf{x}_{1})\) is not available. However, we make the following observation (proof in SSA).

**Proposition 1** (Relative TB constraint).: _If \(p_{\,\theta}\), \(p_{\,\phi}^{\text{post}}\), and the scalar \(Z_{\,\phi}\) jointly satisfy the relative trajectory balance (RTB) constraint_

\[Z_{\,\phi}\cdot p_{\,\phi}^{\text{post}}(\mathbf{x}_{0},\mathbf{x}_{\Delta t}, \ldots,\mathbf{x}_{1})=r(\mathbf{x}_{1})p_{\,\theta}(\mathbf{x}_{0},\mathbf{x}_ {\Delta t},\ldots,\mathbf{x}_{1})\] (8)

_for every denoising trajectory \(\mathbf{x}_{0}\rightarrow\mathbf{x}_{\Delta t}\rightarrow\cdots\rightarrow \mathbf{x}_{1}\), then \(p_{\,\phi}^{\text{post}}(\mathbf{x}_{1})\propto p_{\,\theta}(\mathbf{x}_{1})r( \mathbf{x}_{1})\), i.e., the diffusion model \(p_{\,\phi}^{\text{post}}\) samples the posterior distribution. Furthermore, if \(p_{\,\theta}\) also satisfies the TB constraint (7) with respect to the noising process \(q\) and some target density \(q(\mathbf{x}_{1})\), then \(p_{\,\phi}^{\text{post}}\) satisfies the TB constraint with respect to the target density \(q^{\text{post}}(\mathbf{x}_{1})\propto q(\mathbf{x}_{1})r(\mathbf{x}_{1})\), and \(Z=\int q(\mathbf{x}_{1})r(\mathbf{x}_{1})\,d\mathbf{x}_{1}\)._

Note that the two joints appearing in (8) are defined as products over transitions, via (3).

Relative trajectory balance as a loss.Analogously to the conversion of the TB constraint (7) into a trajectory-dependent training objective in [46; 38], we define the _relative trajectory balance loss_ as the discrepancy between the two sides of (8), seen as a function of the vector \(\phi\) that parametrizes the posterior diffusion model and the scalar \(Z_{\phi}\) (parametrized via \(\log Z_{\phi}\) for numerical stability):

\[\mathcal{L}_{\text{RTB}}(\mathbf{x}_{0}\to\mathbf{x}_{\Delta t}\to\dots\to \mathbf{x}_{1};\phi):=\left(\log\frac{Z_{\phi}\cdot p_{\phi}^{\text{post}}( \mathbf{x}_{0},\mathbf{x}_{\Delta t},\dots,\mathbf{x}_{1})}{r(\mathbf{x}_{1}) p_{\theta}(\mathbf{x}_{0},\mathbf{x}_{\Delta t},\dots,\mathbf{x}_{1})}\right)^{2}.\] (9)

Optimizing this objective to \(0\) for all trajectories ensures that (8) is satisfied. While the RTB constraint (8) has a similar form to TB (7), RTB involves the ratio of two denoising processes, while TB involves the ratio of a forward and a backward process. However, the name'relative TB' is justified by interpreting the densities in a TB constraint relative to a measure defined by the prior model; see SS2.4.

If we assume \(p_{\theta}(\mathbf{x}_{0})=p_{\phi}^{\text{post}}(\mathbf{x}_{0})\) are fixed (_e.g._, to a standard normal), then (9) reduces to

\[\left(\log\frac{Z_{\phi}}{r(\mathbf{x}_{1})}+\sum_{i=1}^{T}\log\frac{p_{\phi}^ {\text{post}}(\mathbf{x}_{i\Delta t}\mid\mathbf{x}_{(i-1)\Delta t})}{p_{\theta }(\mathbf{x}_{i\Delta t}\mid\mathbf{x}_{(i-1)\Delta t})}\right)^{2}.\] (10)

Notably, the gradient of this objective with respect to \(\phi\) does not require differentiation (backpropagation) into the sampling process that produced a trajectory \(\mathbf{x}_{0}\to\dots\to\mathbf{x}_{1}\). This offers two advantages over on-policy simulation-based methods: (1) the ability to optimize \(\mathcal{L}_{\text{RTB}}\) as an off-policy objective, _i.e._, sampling trajectories for training from a distribution different from \(p_{\phi}^{\text{post}}\) itself, as discussed further in SS2.3; (2) backpropagating only to a subset of the summands in (10), when computing and storing gradients for all steps in the trajectory is prohibitive for large diffusion models (see SSH.1).

Comparison with classifier guidance.It is interesting to contrast the RTB training objective with the technique of _classifier guidance_[12] used for some problems of the same form. If \(r(\mathbf{x}_{1})=p(\mathbf{y}\mid\mathbf{x}_{1})\) is a conditional likelihood, classifier guidance relies upon writing \(u_{t}(\mathbf{x}_{t})-u_{t}^{\text{post}}(\mathbf{x}_{t})\) explicitly in terms of \(\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}\mid\mathbf{x}_{t})\), by combining the expression of the optimal drift \(u_{t}\) in terms of the score of the target distribution convolved with a Gaussian (cf. SS2.1), with the 'Bayes' rule' for the Stein score: \(\nabla_{\mathbf{x}_{t}}\log p(\mathbf{x}_{t}\mid\mathbf{y})=\nabla_{\mathbf{x }_{t}}\log p(\mathbf{x}_{t})+\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}\mid \mathbf{x}_{t})\).

Classifier guidance gives the _exact_ solution for the posterior drift when a differentiable classifier on noisy data, \(p(\mathbf{y}\mid\mathbf{x}_{t})=\int p(\mathbf{y}\mid\mathbf{x}_{1})p( \mathbf{x}_{1}\mid\mathbf{x}_{t})\,d\mathbf{x}_{1}\), is available. Unfortunately, such a classifier is not, in general, tractable to derive from the classifier on noiseless data, \(p(\mathbf{y}\mid\mathbf{x}_{1})\), and cannot be learned without access to unbiased data samples. RTB is an asymptotically unbiased objective that recovers the difference in drifts (and thus the gradient of the log-convolved likelihood) in a data-free manner.

### Training, parametrization, and conditioning

Training and exploration.The choice of which trajectories we use to take gradient steps with the RTB loss can have a large impact on sample efficiency. In _on-policy_ training, we use the current policy \(p_{\phi}^{\text{post}}\) to generate trajectories \(\tau=(\mathbf{x}_{0}\to\dots\to\mathbf{x}_{1})\), evaluate the reward \(\log r(\mathbf{x}_{1})\) and the likelihood of \(\tau\) under \(p_{\theta}\), and a gradient updates on \(\phi\) to minimize \(\mathcal{L}_{\text{RTB}}(\tau;\phi)\).

However, on-policy training may be insufficient to discover the modes of the posterior distribution. In this case, we can perform _off-policy_ exploration to ensure mode coverage. For instance, given samples \(\mathbf{x}_{1}\) that have high density under the target distribution, we can sample _noising_ trajectories \(\mathbf{x}_{1}\leftarrow\mathbf{x}_{1-\Delta t}\leftarrow\dots\leftarrow \mathbf{x}_{0}\) starting from these samples and use such trajectories for training. Another effective off-policy training technique uses replay buffers. We expect the flexibility of mixing on-policy training with off-policy exploration to be a strength of RTB over on-policy RL methods, as was shown for distribution-matching training of diffusion models in [65].

Conditional constraints and amortization.Above we derived and proved the correctness of the RTB objective for an arbitrary positive constraint \(r(\mathbf{x}_{1})\). If the constraints depend on other variables \(\mathbf{y}\) - for example, \(r(\mathbf{x}_{1};\mathbf{y})=p(\mathbf{y}\mid\mathbf{x}_{1})\) - then the posterior drift \(u_{\phi}^{\text{post}}\) can be conditioned on \(\mathbf{y}\) and the learned scalar \(\log Z_{\phi}\) replaced by a model taking \(\mathbf{y}\) as input. Such conditioning achieves amortized inference and allows generalization to new \(\mathbf{y}\) not seen in training. Similarly, all of the preceding discussion easily generalizes to _priors_ that are conditioned on some context variable.

Efficient parametrization and Langevin inductive bias.Because the deep features learned by the prior model \(u_{\theta}\) are expected to be useful in expressing the posterior drift \(u_{\phi}^{\text{post}}\), we can choose to initialize \(u_{\phi}^{\text{post}}\) as a copy of \(u_{\theta}\) and to fine-tune it, possibly in a parameter-efficient way (as described in each section of SS3). This choice is inspired by the method of amortizing inference in large language models by fine-tuning a prior model to sample an intractable posterior [29].

Furthermore, if the constraint \(r(\mathbf{x}_{1})\) is differentiable, we can impose an inductive bias on the posterior drift similar to the one introduced for diffusion samplers of unnormalized target densities in [89] and shown to be useful for off-policy methods in [65]. namely, we write

\[u_{\phi}^{\text{post}}(\mathbf{x}_{t},t)=\text{NN}_{1}(\mathbf{x}_{t},t;\phi) +\text{NN}_{2}(\mathbf{x}_{t},t,\phi)\nabla_{\mathbf{x}_{t}}\log r(\mathbf{x}_ {t}),\] (11)

where \(\text{NN}_{1}\) and \(\text{NN}_{2}\) are neural networks outputting a vector and a scalar, respectively. This parametrization allows the constraint to provide a signal to guide the sampler at intermediate steps.

Stabilizing the loss.We propose two simple design choices for stabilizing RTB training. First, the loss in (9) can be replaced by the empirical _variance_ over a minibatch of the quantity inside the square, which removes dependence on \(\log Z_{\phi}\) and is especially useful in conditional settings, consistent with the findings of [65]. This amounts to a relative variant of the VarGrad objective [61] (see (23) in SSG). Second, we employ loss clipping: to reduce sensitivity to an imperfectly fit prior model, we do not perform updates on trajectories where the loss is close to 0 (see SSE,SSF).

### Generative flow networks and extension to other hierarchical processes

RTB as TB under the prior measure.The theoretical foundations for continuous generative flow networks [38] establish the correctness of enforcing constraints such as trajectory balance (7) for training sequential samplers, such as diffusion models, to match unnormalized target densities. While we have considered Gaussian transitions and identified transition kernels with their densities with respect to the Lebesgue measure over \(\mathbb{R}^{d}\), these foundations generalize to more general _reference measures_. In SSB, we show how the RTB constraint can be recovered as a special case of the TB constraint for a certain choice of reference measure derived from the prior.

Extension to arbitrary sequential generation.While our discussion was focused on diffusion models for continuous spaces, the RTB objective can be applied to any Markovian sequential generative process, in particular, one that can be formulated as a generative flow network in the sense of [5; 38]. This includes, in particular, generative models that generate objects by a sequence of discrete steps, including autoregressive models and discrete diffusion models. In the case of discrete diffusion, where the intermediate latent variables \(\mathbf{x}_{t}\) lie not in \(\mathbb{R}^{d}\) but in the space of sequences, one simply replaces the Gaussian transition densities by transition probability _masses_ in the RTB constraint (8) and objective (9). In the case of autoregressive models, where only one sequence of steps can generate any given object, the backward process \(q\) becomes trivial, and the RTB constraint for a model \(p_{\phi}^{\text{post}}\) to sample a sequence \(\mathbf{x}\) from a distribution with density \(r(\mathbf{x})p_{\theta}(\mathbf{x})\) is simply \(Z_{\phi}p_{\phi}^{\text{post}}(\mathbf{x})=r(\mathbf{x})p_{\theta}(\mathbf{x})\) for all sequences \(\mathbf{x}\). We note that a sub-trajectory generalization of this objective was used in [29] to amortize intractable inference in autoregressive language models.

## 3 Experiments

In this section, we present empirical results to validate the efficacy of relative trajectory balance. Our experiments are designed to demonstrate the wide applicability of RTB to sample from posteriors for diffusion priors with arbitrary rewards on vision, language, and continuous control tasks.

### Class-conditional posterior sampling from unconditional diffusion priors

We evaluate RTB in a classifier-guided visual task where we wish to learn a diffusion posterior \(p_{\phi}^{\text{post}}(\mathbf{x}\mid c)\propto p_{\theta}(\mathbf{x})p(c\mid \mathbf{x})\) given a pretrained diffusion prior \(p_{\theta}(\mathbf{x})\) and a classifier \(r(\mathbf{x})=p(c\mid\mathbf{x})\).

Setup.We consider two 10-class image datasets, MNIST and CIFAR-10, using off-the-shelf unconditional diffusion priors from [27] and standard classifiers \(p(c\mid\mathbf{x})\) for both datasets. We perform parameter-efficient fine-tuning of \(p_{\phi}^{\text{post}}\), initialized as a copy of the prior \(p_{\theta}\), using the RTB objective (see SSE.1 for details). The RTB objective is optimized on trajectories sampled

[MISSING_PAGE_FAIL:7]

images. Following DPOK [16], we use ImageReward [87], which has been trained to match human preferences as well as prompt accuracy to attributes such as the number of objects, color, and compositionality, as the reward \(\log r(\mathbf{x}_{1},\mathbf{z})\). As reference, we present comparisons against DPOK with the default KL regularization \(\gamma=0.01\) and DPOK with \(\gamma=0.0\), which is equivalent to DDPO [8]. We measure the final average reward and the diversity of the generated image, as measured by the average pairwise cosine distance between CLIP embeddings [58] of a batch of generated images. Further details about the experimental setup and ablations are discussed in SSH.

Results.Fig. 3 plots the diversity versus log reward on a set of prompts from [16; 87]. In terms of average \(\log r(\mathbf{x}_{1},\mathbf{z})\), RTB either matches or outperforms DPOK, while generally achieving lower reward than DDPO. The CLIP diversity score for RTB and DPOK are on average higher than DDPO, which is expected since it does not use KL regularization. For qualitative image assessments, refer to Fig. 4 and SSH.2. Through this experiment, we show that RTB scales well to high dimensional, multimodal data, matching state-of-the-art methods for fine-tuning text-to-image diffusion models.

### Text infilling with discrete diffusion language models

To evaluate our approach on discrete diffusion models, we consider the problem of text infilling [91], which involves filling in missing tokens given some context tokens. While discrete diffusion models - unlike their continuous counterparts - can be challenging to train [49; 4; 9; 74], _score entropy discrete diffusion_[SEDD; 43] matches the language modeling performance of autoregressive language models of similar scale. Non-autoregressive generation in diffusion language models can provide useful inductive biases for infilling, such as the ability to attend to context on both sides of a target token.

Setup.We use the ROCStories corpus [50], a dataset of short stories containing 5 sentences each. We adopt the task setup from [29], where the first 3 sentences of a story \(\mathbf{x}\) and the last sentence \(\mathbf{y}\) are given, and the goal is to generate the fourth sentence \(\mathbf{z}\) such that the overall story is coherent and consistent. The fourth sentence can involve a turning point in the story and is thus challenging to fill in. We aim to model the posterior \(p^{\text{post}}(\mathbf{z}\mid\mathbf{x},\mathbf{y})\propto p(\mathbf{z}\mid \mathbf{x})p_{\text{reward}}(\mathbf{y}\mid\mathbf{x},\mathbf{z})\) where \(p\) is a SEDD language model prior (a conditional model over \(\mathbf{z}\) given \(\mathbf{x}\)) and \(p_{\text{reward}}\) is an autoregressive language model fine-tuned with a maximum likelihood objective on a held-out subset of the dataset. As baselines, we consider simply prompting the diffusion language model with \(\mathbf{x}\) (Prompt \((\mathbf{x})\)) and \(\mathbf{x},\mathbf{y}\) (Prompt \((\mathbf{x},\mathbf{y})\)). Additionally, to contextualize the performance, we also consider autoregressive language model baselines from [29], which studied this problem under an autoregressive prior \(p(\mathbf{z}\mid\mathbf{x})\). SFT is trained on \(50,000\) examples compared to \(1000\) for RTB, and serves as an upper bound on the performance in this task. See SSF for further details about the experimental setup.

Figure 4: Images generated from prior (top row), DPOK (middle row) and RTB (bottom row) for 4 different prompts. Images in the same column share the random DDIM seed. More images in ยงH.2.

Figure 3: Fine-tuning Stable Diffusion with ImageReward. We report mean \(\log r(\mathbf{x}_{1},\mathbf{z})\) and diversity, measured as the mean cosine distance between CLIP embeddings for a batch of 100 generated images.2

Results.Following [29], we use three standard metrics to measure the similarity of the generated infills with the reference infills from the dataset: BERTScore [90] (with DEBRTa [26]), BLEU-4 [54], and GLEU-4 [86]. Table 3 summarizes the results. We observe that the diffusion language model performs significantly better than the autoregressive language model without any fine-tuning. RTB further improves the performance over prompting, and even outperforms the strongest autoregressive baseline of GFlowNet fine-tuning. We provide some examples of generated text in SSF.

### KL-constrained policy search in offline reinforcement learning

The goal of RL algorithms is to learn a policy \(\pi(a\mid s)\), _i.e._, a mapping from states \(s\) to actions \(a\) in an environment, that maximizes the expected cumulative discounted reward [75]. In the offline RL setting [39], the agent has access to a dataset \(\mathcal{D}=\{(s_{t}^{i},a_{t}^{i},s_{t+1}^{i},r_{t}^{i})\}_{i=1}^{N}\) of transitions (where each sample \((s_{t},a_{t},s_{t+1},r_{t})\) indicates that an agent taking action \(a_{t}\) at state \(s_{t}\) transitioned to the next state \(s_{t+1}\) and received reward \(r_{t}\)). This dataset is assumed to be generated by a _behavior policy_\(\mu(a\mid s)\), which may be a diffusion model trained on \(\mathcal{D}\). Offline RL algorithms must learn a new policy \(\pi\) which achieves high return using only this dataset without interacting with the environment.

An important problem in offline RL is policy extraction from trained \(Q\)-functions [55; 25; 44]. For reliable extrapolation, one wants the policy to predict actions that have high \(Q\)-values, but also have high density under the behavior policy \(\mu\), as naive maximization can result in choosing actions with low probability under \(\mu\) and thus unreliable predictions from the \(Q\)-function. This is formulated as a KL-constrained policy search problem:

\[\operatorname*{argmax}_{\pi}\mathbb{E}_{s-d_{\mu},a\cdot\pi(a\mid s)}\left[Q( s,a)\right],\quad\mathbb{E}_{s-d_{\mu}}\left[D_{\text{KL}}(\pi(a\mid s)\parallel \mu(a\mid s))\right]\leq\epsilon,\] (13)

where \(d_{\mu}\) is the distribution over states induced by following the policy \(\mu\). The optimal policy \(\pi\) in (13) is the product distribution \(\pi^{*}(a\mid s)\propto\mu(a\mid s)\exp(\beta Q(s,a))\) for some inverse temperature \(\beta\). If \(\mu(a\mid s)\) is a conditional diffusion model over continuous actions \(a\) conditioned on state \(s\), we use RTB to fine-tune a diffusion behavior policy to sample from \(\pi^{*}\), using \(\mu\) as the prior and \(\exp(\beta Q(s,a))\) as the target constraint. We use a \(Q\)-function trained using IQL [36].

Setup.We test on continuous control tasks in the D4RL suite [18], which consists of offline datasets collected using a mixture of SAC policies of varying performance. We evaluate on the halfcheetah, hopper and walker2d MuJoCo [76] locomotion tasks, each of which contains three datasets of transitions: "medium" (collected from an early-stopped policy), "medium-expert" (collected from both an expert and an early-stopped policy) and "medium-replay" (transitions stored in the replay buffer prior to early stopping). We compare against standard offline RL baselines (Behavior Cloning (BC), CQL [37], and IQL [36]) and diffusion-based offline RL methods which are currently state-of-the-art: Diffuser [D; 30], Decision Diffuser [DD; 2], D-QL [83], IDQL [25], and QGPO [44]. For algorithm implementation details, hyperparameters, and a report of baselines, see SSG.

Results.Table 4 shows that RTB matches state-of-the-art results across the D4RL tasks. In particular, RTB performs strongly in the medium-replay tasks, which contain the most suboptimal data and consequently the poorest behavior prior. We highlight that our performance is similar to QGPO [44], which learns intermediate energy densities for diffusion posterior sampling.

## 4 Other related work

Composing iterative generative processes.Beyond the approximate posterior sampling algorithms and application-specific techniques discussed in SS1 and SS3, several recent works have explored the use of hierarchical models, such as diffusion models, as modular components in generative processes. Diffusion models can be used to sample product distributions to induce compositional structure in images [41; 15]. Amortized Bayesian inference [35; 60; 59; 20] is another domain of sampling from product distributions where diffusion models are now being used [21]. Beyond product models, [19]

\begin{table}
\begin{tabular}{c l c c c} \hline \hline Model & Algorithm 1 Metric \(\rightarrow\) & BLEU-4 & BLEU-4 & BLEU-4 & BERTScore \\ \hline \multirow{4}{*}{Autores} & Proempting & 0.010+0.06 & 0.022+0.001 & 0.005+0.001 \\  & Supervised fine-tuning & 0.012+0.02 & 0.023+0.001 & 0.013+0.001 \\  & GSI fine-tuning [29] & 0.019+0.001 & 0.034+0.002 & 0.022+0.001 \\ \hline \multirow{4}{*}{Discrete diffusion} & Proempt (\(\mathbf{x}\)) & 0.011+0.002 & 0.023+0.002 & 0.014+0.003 \\  & Promote (\(\mathbf{x}\),\(\mathbf{y}\)) & 0.014+0.00 & 0.027+0.003 & 0.002+0.004 \\ \cline{1-1}  & **RTB (ours)** & 0.025+0.002 & 0.045+0.002 & 0.156+0.003 \\ \cline{1-1}  & SFT (upper bound) & 0.031\(\pm\)0.002 & 0.057\(\pm\)0.004 & 0.182+0.005 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on the story infilling task with autoregressive and discrete diffusion language models. Metrics are computed with respect to reference infills from the dataset. All metrics are mean\({}_{\text{sstd}}\) over 5 samples for each of the 100 test examples. RTB with discrete diffusion prior performs better than best baseline with autoregressive prior.

studies ways to amortize other kinds of compositions of hierarchical processes, including diffusion models, while [67] proposes methods to sample the product of many iterative processes in application to federated learning. Finally, models without hierarchical structure, such as normalizing flows, have been used to amortize intractable inference in pretrained diffusion models [_e.g._, 17]. In contrast, our method performs posterior inference by _fine-tuning_ a prior model, developing a direction on flexible extraction of information from large pretrained models [29].

Diffusion samplers.Several prior works seek to amortize MCMC sampling from unnormalized densities by training diffusion models for efficient mode-mixing [6, 89, 78, 62, 79, 3]. Our work is most closely related to continuous GFlowNets [38], which offer an alternative perspective on training diffusion samplers using off-policy flow consistency objectives [38, 88, 65]. Recently, Berner et al. [7] have shown connections among existing families of diffusion sampling algorithms and analyzed their continuous-time limits.

## 5 Conclusions and future work

Relative trajectory balance provides a new approach to training diffusion models to generate unbiased posterior samples given a diffusion prior and an arbitrary reward function. Through experiments on a variety of domains - vision, language, continuous control - we demonstrated the flexibility and general applicability of RTB. RTB can be optimized with off-policy trajectories, and future work can explore ways to leverage off-policy training, using techniques such as local search [34, 65] to improve sample efficiency and mode coverage. Simulation-based objectives in the style of [89] are also applicable to the amortized sampling problems we consider and should be explored, as should simulation-free extensions, _e.g._, through objectives that are local in time [45]. The ability to handle arbitrary black-box likelihoods also makes RTB a useful candidate for inverse problems in domains such as 3D object synthesis with likelihood computed via a renderer [_e.g._, 56, 82], imaging problems in astronomy [_e.g._, 1], medical imaging [_e.g._, 73], and molecular structure prediction [_e.g._, 84].

Moreover, RTB could facilitate a breakthrough in modeling molecular dynamics--a notoriously challenging task due to the need to sample rare-event trajectories in chemical simulations--by converting these problems into posterior inference over amplified distributions of rare-event samples. Notably, Seong et al. [66] have already explored a preliminary version of this concept by employing TB with a reward multiplied by the prior likelihood, which is effectively equivalent to RTB.

Limitations.RTB learns the posterior through simulation-based training, which can be slow and memory-intensive. Additionally, the RTB objective is computed on complete trajectories without any local credit-assignment signal, which can result in high variance in the gradients. Guarantees on the error incurred by imperfect fit of the prior model, amortization, and time discretization (analogous to [7]'s analysis for diffusion samplers) have not been obtained and should be considered in future work.

Broader impact.While our contributions focus on an algorithmic approach for learning posterior samplers with diffusion priors, we acknowledge that like other advances in generative modelling, our approach can potentially be used by nefarious actors to train generative models to produce harmful content and misinformation. At the same time, our approach can be also be used to mitigate biases captured in pretrained models and applied to various scientific problems.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline Task \(\downarrow\) Algorithm \(\rightarrow\) & BC & CQL & IQL & D & DD & D-QL & IDQL & QGPO & **RTB (ours)** \\ \hline halfcheetah-medium-expert & 55.2 & 91.6 & 86.7 & 79.8 & 90.6\({}_{1.3}\) & 96.1\({}_{0.3}\) & 95.9 & 93.5\({}_{0.3}\) & 74.93\({}_{1.72}\) \\ hopper-medium-expert & 52.5 & 105.4 & 91.5 & 107.2 & 111.8\({}_{1.8}\) & 110.7\({}_{1.3}\) & 108.6 & 108.0\({}_{2.5}\) & 96.11\({}_{1.53}\) \\ walker2d-medium-expert & 107.5 & 108.8 & 109.6 & 108.4 & 108.8\({}_{1.7}\) & 109.7\({}_{0.3}\) & 112.7 & 110.7\({}_{0.6}\) & 109.52\({}_{0.11}\) \\ \hline halfcheetah-medium & 42.6 & 44.0 & 47.4 & 44.2 & 49.1\({}_{1.1}\) & 50.6\({}_{0.5}\) & 51.0 & 54.1\({}_{1.0}\) & 53.70\({}_{0.38}\) \\ hopper-medium & 52.9 & 58.5 & 66.3 & 58.5 & 79.3\({}_{3.6}\) & 82.4\({}_{4.6}\) & 65.4 & 98.0\({}_{0.2}\) & 82.76\({}_{3.7}\) \\ walker2d-medium & 75.3 & 72.5 & 78.3 & 79.7 & 82.5\({}_{1.4}\) & 85.1\({}_{0.9}\) & 82.5 & 86.0\({}_{0.7}\) & 87.29\({}_{3.15}\) \\ \hline halfcheetah-medium-replay & 36.6 & 45.5 & 44.2 & 42.2 & 39.3\({}_{4.1}\) & 47.5\({}_{0.3}\) & 45.8 & 47.6\({}_{1.4}\) & 48.11\({}_{0.56}\) \\ hopper-medium-replay & 18.1 & 95.0 & 94.7 & 96.8 & 100.0\({}_{0.7}\) & 100.7\({}_{0.6}\) & 92.1 & 96.92\({}_{0.2}\) & 100.40\({}_{0.21}\) \\ walker2d-medium-replay & 26.0 & 77.2 & 73.9 & 61.2 & 75.0\({}_{4.3}\) & 39.4\({}_{3.15}\) & 85.1 & 84.4\({}_{4.1}\) & 93.57\({}_{2.63}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Average rewards of trained policies on D4RL locomotion tasks (mean\({}_{\text{std}}\) over 5 random seeds). Following past work, numbers within 5% of maximum in every row are highlighted.

## Acknowledgments and Disclosure of Funding

The authors thank Adam Coogan, Yashar Hezaveh, Guillaume Lajoie, and Laurence Perreault Levasseur for helpful suggestions in the course of this project and Mandana Samiei for comments on a draft of the paper.

The authors acknowledge funding from CIFAR, NSERC, IVADO, UNIQUE, FACS Acuite, NRC AI4Discovery, Samsung, and Recursion.

The research was enabled in part by computational resources provided by the Digital Research Alliance of Canada (https://allianecan.ca), Mila (https://mila.quebec), and NVIDIA.

## References

* [1] Alexandre Adam, Adam Coogan, Nikolay Malkin, Ronan Legin, Laurence Perreault-Levasseur, Yashar Hezaveh, and Yoshua Bengio. Posterior samples of source galaxies in strong gravitational lenses with score-based priors. _arXiv preprint arXiv:2211.03812_, 2022.
* [2] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S. Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? _International Conference on Learning Representations (ICLR)_, 2023.
* [3] Tara Akhound-Sadegh, Jarrid Rector-Brooks, Avishek Joey Bose, Sarthak Mittal, Pablo Lemos, Cheng-Hao Liu, Marcin Sendera, Siamak Ravanbakhsh, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, and Alexander Tong. Iterated denoising energy matching for sampling from Boltzmann densities. _International Conference on Machine Learning (ICML)_, 2024.
* [4] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. _Neural Information Processing Systems (NeurIPS)_, 2021.
* [5] Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. GFlowNet foundations. _Journal of Machine Learning Research_, 24(210):1-55, 2023.
* [6] Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusion-based generative modeling. _Transactions on Machine Learning Research (TMLR)_, 2024.
* [7] Julius Berner, Lorenz Richter, Marcin Sendera, Jarrid Rector-Brooks, and Nikolay Malkin. From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training. _arXiv preprint arXiv:2501.06148_, 2025.
* [8] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. _International Conference on Learning Representations (ICLR)_, 2024.
* [9] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. _Neural Information Processing Systems (NeurIPS)_, 2022.
* [10] Gabriel Cardoso, Yazid Janati el idrissi, Sylvain Le Corff, and Eric Moulines. Monte carlo guided denoising diffusion models for bayesian linear inverse problems. _International Conference on Learning Representations (ICLR)_, 2024.
* [11] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. _International Conference on Learning Representations (ICLR)_, 2023.
* [12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. _Neural Information Processing Systems (NeurIPS)_, 2021.
* [13] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. _arXiv preprint arXiv:2211.15089_, 2022.

* [14] Zehao Dou and Yang Song. Diffusion posterior sampling for linear inverse problem solving: A filtering perspective. _International Conference on Learning Representations (ICLR)_, 2024.
* [15] Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and MCMC. _International Conference on Machine Learning (ICML)_, 2023.
* [16] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. _Neural Information Processing Systems (NeurIPS)_, 2023.
* [17] Berthy T. Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L. Bouman, and William T. Freeman. Score-based diffusion models as principled priors for inverse imaging. _International Conference on Computer Vision (ICCV)_, 2023.
* [18] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* [19] Timur Garipov, Sebastiaan De Peuter, Ge Yang, Vikas Garg, Samuel Kaski, and Tommi Jaakkola. Compositional sculpting of iterative generative processes. _Neural Information Processing Systems (NeurIPS)_, 2023.
* [20] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and Yee Whye Teh. Neural processes. _arXiv preprint arXiv:1807.01622_, 2018.
* [21] Tomas Geffner, George Papamakarios, and Andriy Mnih. Compositional score modeling for simulation-based inference. _International Conference on Machine Learning (ICML)_, 2023.
* [22] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors. _Neural Information Processing Systems (NeurIPS)_, 2022.
* [23] Ishaan Gulrajani and Tatsunori B Hashimoto. Likelihood-based diffusion language models. _Neural Information Processing Systems (NeurIPS)_, 2023.
* [24] Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. SSD-LM: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. _Association for Computational Linguistics (ACL)_, 2023.
* [25] Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. IDQL: Implicit Q-learning as an actor-critic method with diffusion policies. _arXiv preprint arXiv:2304.10573_, 2023.
* [26] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhancedbert with disentangled attention. _International Conference on Learning Representations (ICLR)_, 2021.
* [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Neural Information Processing Systems (NeurIPS)_, 2020.
* [28] Edward J. Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. _International Conference on Learning Representations (ICLR)_, 2022.
* [29] Edward J. Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. _International Conference on Learning Representations (ICLR)_, 2024.
* [30] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. _International Conference on Machine Learning (ICML)_, 2022.
* [31] Zahra Kadkhodaie and Eero P. Simoncelli. Solving linear inverse problems using the prior implicit in a denoiser. _Neural Information Processing Systems (NeurIPS)_, 2021.

* [32] Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan. Efficient diffusion policies for offline reinforcement learning. _Neural Information Processing Systems (NeurIPS)_, 2024.
* [33] Bahjat Kawar, Gregory Vaksman, and Michael Elad. SNIPS: Solving noisy inverse problems stochastically. _Neural Information Processing Systems (NeurIPS)_, 2021.
* [34] Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search GFlowNets. _International Conference on Learning Representations (ICLR)_, 2024.
* [35] Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. _International Conference on Learning Representations (ICLR)_, 2014.
* [36] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit Q-learning. _International Conference on Learning Representations (ICLR)_, 2022.
* [37] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline reinforcement learning. _Neural Information Processing Systems (NeurIPS)_, 2020.
* [38] Salem Lahlou, Tristan Deleu, Pablo Lemos, Dinghuai Zhang, Alexandra Volokhova, Alex Hernandez-Garcia, Lena Nehale Ezzine, Yoshua Bengio, and Nikolay Malkin. A theory of continuous generative flow networks. _International Conference on Machine Learning (ICML)_, 2023.
* [39] Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* [40] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-LM improves controllable text generation. _Neural Information Processing Systems (NeurIPS)_, 2022.
* [41] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. _European Conference on Computer Vision (ECCV)_, 2022.
* [42] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-Eval: NLG evaluation using GPT-4 with better human alignment. _arXiv preprint arXiv:2303.16634_, 2023.
* [43] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. _arXiv preprint arXiv:2310.16834_, 2023.
* [44] Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, and Jun Zhu. Contrastive energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning. _International Conference on Machine Learning (ICML)_, 2023.
* [45] Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning GFlowNets from partial episodes for improved convergence and stability. _International Conference on Machine Learning (ICML)_, 2022.
* [46] Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in GFlowNets. _Neural Information Processing Systems (NeurIPS)_, 2022.
* [47] Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. _International Conference on Learning Representations (ICLR)_, 2023.
* [48] Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A variational perspective on solving inverse problems with diffusion models. _International Conference on Learning Representations (ICLR)_, 2024.

* [49] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. _Neural Information Processing Systems (NeurIPS)_, 2022.
* [50] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. _North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)_, 2016.
* [51] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabili1stic models. _International Conference on Machine Learning (ICML)_, 2021.
* [52] Nikolas Nusken and Lorenz Richter. Solving high-dimensional Hamilton-Jacobi-Bellman PDEs using neural networks: perspectives from the theory of controlled diffusions and measures on path space. _Partial Differential Equations and Applications_, 2(4):48, 2021.
* [53] Bernt Oksendal. _Stochastic Differential Equations: An Introduction with Applications_. Springer, 2003.
* [54] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. _Association for Computational Linguistics (ACL)_, 2002.
* [55] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_, 2019.
* [56] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. _International Conference on Learning Representations (ICLR)_, 2023.
* [57] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. 2021.
* [59] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. _International Conference on Machine Learning (ICML)_, 2015.
* [60] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. _International Conference on Machine Learning (ICML)_, 2014.
* [61] Lorenz Richter, Ayman Boustati, Nikolas Nusken, Francisco J. R. Ruiz, and Omer Deniz Akyildiz. VarGrad: A low-variance gradient estimator for variational inference. _Neural Information Processing Systems (NeurIPS)_, 2020.
* [62] Lorenz Richter, Julius Berner, and Guan-Horng Liu. Improved sampling via learned diffusions. _International Conference on Learning Representations (ICLR)_, 2023.
* [63] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. _Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [64] Simo Sarkka and Arno Solin. _Applied stochastic differential equations_. Cambridge University Press, 2019.
* [65] Marcin Sendera, Minsu Kim, Sarthak Mittal, Pablo Lemos, Luca Scimeca, Jarrid Rector-Brooks, Alexandre Adam, Yoshua Bengio, and Nikolay Malkin. On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling. _arXiv preprint arXiv:2402.05098_, 2024.
* [66] Kiyoung Seong, Seonghyun Park, Seonghwan Kim, Woo Youn Kim, and Sungsoo Ahn. Collective variable free transition path sampling with generative flow network. _arXiv preprint arXiv:2405.19961_, 2024.

* [67] Tiago Silva, Amauri H Souza, Luiz Max Carvalho, Samuel Kaski, and Diego Mesquita. Federated contrastive GFlowNets, 2024. URL https://openreview.net/forum?id=VJDFhkwQg6.
* [68] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. _International Conference on Machine Learning (ICML)_, 2015.
* [69] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _International Conference on Learning Representations (ICLR)_, 2021.
* [70] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. _International Conference on Machine Learning (ICML)_, 2023.
* [71] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. _Neural Information Processing Systems (NeurIPS)_, 2021.
* [72] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _International Conference on Learning Representations (ICLR)_, 2021.
* [73] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. _International Conference on Learning Representations (ICLR)_, 2022.
* [74] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. _International Conference on Learning Representations (ICLR)_, 2023.
* [75] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT Press, 2018.
* [76] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 5026-5033, 2012. doi: 10.1109/IROS.2012.6386109.
* [77] Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Tommaso Biancalani, and Sergey Levine. Fine-tuning of continuous-time diffusion models as entropy-regularized control. _arXiv preprint arXiv:2402.15194_, 2024.
* [78] Francisco Vargas, Will Grathwohl, and Arnaud Doucet. Denoising diffusion samplers. _International Conference on Learning Representations (ICLR)_, 2023.
* [79] Francisco Vargas, Shreyas Padhy, Denis Blessing, and Nikolas Nusken. Transport meets variational inference: Controlled Monte Carlo diffusions. _International Conference on Learning Representations (ICLR)_, 2024.
* [80] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* [81] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020.
* [82] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score Jacobian chaining: Lifting pretrained 2D diffusion models for 3D generation. _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.
* [83] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. _International Conference on Learning Representations (ICLR)_, 2023.

* [84] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. _Nature_, 620(7976):1089-1100, 2023.
* [85] Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho. Consistency of a recurrent language model with respect to incomplete decoding. _Empirical Methods in Natural Language Processing (EMNLP)_, 2020.
* [86] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. _arXiv preprint arXiv:1609.08144_, 2016.
* [87] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagreeward: Learning and evaluating human preferences for text-to-image generation. _Neural Information Processing Systems (NeurIPS)_, 2023.
* [88] Dinghuai Zhang, Ricky Tian Qi Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization. _International Conference on Learning Representations (ICLR)_, 2024.
* [89] Qinsheng Zhang and Yongxin Chen. Path integral sampler: a stochastic control approach for sampling. _International Conference on Learning Representations (ICLR)_, 2022.
* [90] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. BERTScore: Evaluating text generation with bert. _International Conference on Learning Representations_, 2020.
* [91] Wanrong Zhu, Zhiting Hu, and Eric Xing. Text infilling. _arXiv preprint arXiv:1901.00158_, 2019.

## Appendix A Proofs

**Proposition 1** (Relative TB constraint).: _If \(p_{\theta}\), \(p_{\phi}^{\mathrm{post}}\), and the scalar \(Z_{\phi}\) jointly satisfy the relative trajectory balance (RTB) constraint_

\[Z_{\phi}\cdot p_{\phi}^{\mathrm{post}}(\mathbf{x}_{0},\mathbf{x}_{ M},\ldots,\mathbf{x}_{1})=r(\mathbf{x}_{1})p_{\theta}(\mathbf{x}_{0},\mathbf{x}_{ \Delta t},\ldots,\mathbf{x}_{1})\] (8)

_for every denoising trajectory \(\mathbf{x}_{0}\rightarrow\mathbf{x}_{\Delta t}\rightarrow\cdots\rightarrow \mathbf{x}_{1}\), then \(p_{\phi}^{\mathrm{post}}(\mathbf{x}_{1})\propto p_{\theta}(\mathbf{x}_{1})r( \mathbf{x}_{1})\), i.e., the diffusion model \(p_{\phi}^{\mathrm{post}}\) samples the posterior distribution. Furthermore, if \(p_{\theta}\) also satisfies the TB constraint (7) with respect to the noising process \(q\) and some target density \(q(\mathbf{x}_{1})\), then \(p_{\phi}^{\mathrm{post}}\) satisfies the TB constraint with respect to the target density \(q^{\mathrm{post}}(\mathbf{x}_{1})\propto q(\mathbf{x}_{1})r(\mathbf{x}_{1})\), and \(Z=\int q(\mathbf{x}_{1})r(\mathbf{x}_{1})\,d\mathbf{x}_{1}\)._

Proof of Prop. 1.: Suppose that \(p_{\theta}\), \(p_{\phi}^{\mathrm{post}}\), and \(Z\) jointly satisfy (8). Then necessarily \(Z\neq 0\), since the quantities on the right side are positive. We then have, using (4),

\[p_{\phi}^{\mathrm{post}}(\mathbf{x}_{1}) =\int p_{\phi}^{\mathrm{post}}(\mathbf{x}_{0},\mathbf{x}_{\Delta t },\ldots,\mathbf{x}_{1})\,d\mathbf{x}_{0}\,d\mathbf{x}_{\Delta t}\ldots d \mathbf{x}_{1-\Delta t}\] \[=\frac{1}{Z}r(\mathbf{x}_{1})\int p_{\theta}(\mathbf{x}_{0}, \mathbf{x}_{\Delta t},\ldots,\mathbf{x}_{1})\,d\mathbf{x}_{0}\,d\mathbf{x}_{ \Delta t}\ldots d\mathbf{x}_{1-\Delta t}\] \[=\frac{1}{Z}r(\mathbf{x}_{1})p_{\theta}(\mathbf{x}_{1})\qquad \qquad\qquad\qquad\qquad\qquad\qquad\propto p_{\theta}(\mathbf{x}_{1})r( \mathbf{x}_{1}),\]

as desired.

Now suppose that \(p_{\theta}\) also satisfies the TB constraint (7) with respect to \(q(\mathbf{x}_{1})\). Then, for any denoising trajectory,

\[q(\mathbf{x}_{0},\mathbf{x}_{\Delta t},\ldots,\mathbf{x}_{1- \Delta t}\mid\mathbf{x}_{1})=\frac{p_{\theta}(\mathbf{x}_{0},\mathbf{x}_{ \Delta t},\ldots,\mathbf{x}_{1})}{q(\mathbf{x}_{1})}=\frac{p_{\phi}^{\mathrm{ post}}(\mathbf{x}_{0},\mathbf{x}_{\Delta t},\ldots,\mathbf{x}_{1})}{q( \mathbf{x}_{1})r(\mathbf{x}_{1})/Z}.\] (14)

showing that \(p_{\phi}^{\mathrm{post}}\) satisfies the TB constraint with respect to the noising process \(q\) and the (not yet shown to be normalized) density \(\frac{1}{Z}q(\mathbf{x}_{1})r(\mathbf{x}_{1})\). We integrate out the variables \(\mathbf{x}_{0},\mathbf{x}_{\Delta t},\ldots,\mathbf{x}_{1-\Delta t}\) in (14), giving

\[1 =\frac{p_{\phi}^{\mathrm{post}}(\mathbf{x}_{1})}{q(\mathbf{x}_{1}) r(\mathbf{x}_{1})/Z}\] \[q(\mathbf{x}_{1})r(\mathbf{x}_{1}) =Zp_{\phi}^{\mathrm{post}}(\mathbf{x}_{1}).\]

Integrating over \(\mathbf{x}_{1}\) shows \(\int q(\mathbf{x}_{1})r(\mathbf{x}_{1})\,d\mathbf{x}_{1}=Z\). 

## Appendix B Relative TB as TB under the prior measure

The theoretical foundations for continuous generative flow networks [38] establish the correctness of enforcing constraints such as trajectory balance (7) for training sequential samplers, such as diffusion models, to match unnormalized target densities. While we have considered Gaussian transitions and identified transition kernels with their densities with respect to the Lebesgue measure over \(\mathbb{R}^{d}\), these foundations generalize to more general _reference measures_. In application to diffusion samplers, suppose that \(\pi_{\mathrm{ref}}(\mathbf{x}_{t})\) is a collection of Lebesgue-absolutely continuous densities over \(\mathbb{R}^{d}\) for \(t=0,\Delta t,\ldots,1\) and that \(\overrightarrow{\pi}_{\mathrm{ref}}(\mathbf{x}_{t}\mid\mathbf{x}_{\mathrm{t}- \Delta t}),\overrightarrow{\pi}_{\mathrm{ref}}(\mathbf{x}_{t-\Delta t}\mid \mathbf{x}_{t})\) are collections of Lebesgue-absolutely continuous transition kernels. If these densities jointly satisfy the detailed balance condition \(\pi_{\mathrm{ref}}(\mathbf{x}_{t})\overleftarrow{\pi}_{\mathrm{ref}}(\mathbf{x} _{t-\Delta t}\mid\mathbf{x}_{t})=\pi_{\mathrm{ref}}(\mathbf{x}_{t-\Delta t}) \overrightarrow{\pi}_{\mathrm{ref}}(\mathbf{x}_{t}\mid\mathbf{x}_{t-\Delta t})\), then they satisfy the conditions to be reference measures. A main result of [38] is that if a pair of forward and backward processes satisfies the trajectory balance constraint (7) jointly with a reward density \(r\), then the forward process \(p\) samples from the distribution with density \(r\), with all densities interpreted as _relative to the reference measures_\(\pi_{\mathrm{ref}}\), \(\overrightarrow{\pi}_{\mathrm{ref}},\overrightarrow{\pi}_{\mathrm{ref}}\).3If \(p\,_{\theta}\) is a diffusion model that satisfies the TB constraint jointly with some reverse process \(q\) and target density \(q(\mathbf{x}_{1})\), then one can take the reference transition kernels \(\overrightarrow{\pi}_{\text{ref}},\overrightarrow{\pi}_{\text{ref}}\) to be \(p\) and \(q\), respectively. In this case, the TB constraint for a target density \(\frac{1}{Z}r(\mathbf{x}_{1})\) and forward transition \(p_{\phi}^{\text{post}}\) is

\[\frac{p_{\phi}^{\text{post}}(\mathbf{x}_{0},\mathbf{x}_{M}, \ldots,\mathbf{x}_{1})}{\overrightarrow{\pi}_{\text{ref}}(\mathbf{x}_{0}, \mathbf{x}_{M},\ldots,\mathbf{x}_{1})}=\underbrace{\frac{1}{Z}r(\mathbf{x}_{ 1})q(\mathbf{x}_{0},\mathbf{x}_{M},\ldots,\mathbf{x}_{1}-\Delta t\mid\mathbf{ x}_{1})}_{\overleftarrow{\pi}_{\text{ref}}(\mathbf{x}_{0},\mathbf{x}_{M}, \ldots,\mathbf{x}_{1}-\Delta t\mid\mathbf{x}_{1})},\] (15)

which is identical to the RTB constraint (8). If (15) holds, then \(p_{\phi}^{\text{post}}\) samples from the distribution with density \(\frac{1}{Z}r(\mathbf{x}_{1})\) relative to \(\pi_{\text{ref}}(\mathbf{x}_{1})\), which is exactly \(\frac{1}{Z}p\,_{\theta}(\mathbf{x}_{1})r(\mathbf{x}_{1})\). We have thus recovered RTB as a case of TB for non-Lebesgue reference measures.

Posterior inference on two-dimensional Gaussian mixture model

SetupWe conduct toy experiments in low-dimensional spaces using samples from a Gaussian mixture model with multiple modes to visually demonstrate its validity. The prior distribution \(p(\mathbf{x}_{1})\) is trained on a Gaussian mixture model with 25 evenly weighted modes, while the target posterior \(p^{\text{post}}(\mathbf{x}_{1})=r(\mathbf{x}_{1})p(\mathbf{x}_{1})\) uses a reward \(r(\mathbf{x}_{1})\) to select and re-weight 9 modes from \(p(\mathbf{x}_{1})\). More specifically, the resulting posterior is:

\[p^{\text{post}}(\mathbf{x}_{1}) =\frac{1}{\sum_{j}\tilde{\pi}_{j}}\sum_{i}\tilde{\pi}_{i}\bm{N}( \mathbf{x}_{1}\mid\mu_{i},\mathbf{I})\] (16) \[\{\mu_{i}\} =\{(-10,-5),(-5,-10),(-5,0),(10,-5),(0,0),(0,5),(5,-5),(5,0),(5,10)\}\] (17) \[\{\tilde{\pi}_{i}\} =\{4,10,4,5,10,5,4,15,4\}\] (18)

Our objective is to sample from the posterior \(p^{\text{post}}(\mathbf{x}_{1})\). We compare our method with several baselines, including policy gradient reinforcement learning (RL) with KL constraint and classifier-guided diffusion models. For RL, we implemented the REINFORCE method with a mean baseline and a KL constraint, following recent work training diffusion models to optimize a reward function [8]. Sampling according to the RL policy leads to a distribution \(q_{\theta}(\mathbf{x}_{1})\), which is trained with the objective:

\[J(\theta)=\mathbb{E}_{q_{\theta}(\mathbf{x}_{1})}\left[r(\mathbf{x}_{1}) \right]+\alpha D_{\text{KL}}\left(q_{\theta}(\mathbf{x}_{1})\|p(\mathbf{x}_{1 })\right)\] (19)

While the exact computation of \(KL\left(q_{\theta}(\mathbf{x}_{1})\|p(\mathbf{x}_{1})\right)\) is intractable, we follow the approximation method introduced by Fan et al. [16], which sums the divergence at every diffusion step. This approximation optimizes an upper bound of the marginal KL.

The other baseline is classifier (energy) guidance, which given a diffusion prior, samples using a posterior score function estimate:

\[\nabla_{\mathbf{x}_{t}}\log p^{\text{post}}(\mathbf{x}_{t})\approx\nabla_{ \mathbf{x}_{t}}\log p(\mathbf{x}_{t})+\nabla_{\mathbf{x}_{t}}\log r(\mathbf{x }_{t})\] (20)

Note that this is a biased approximation of the true intractable score:

\[\nabla_{\mathbf{x}_{t}}\log p^{\text{post}}(\mathbf{x}_{t})=\nabla_{\mathbf{x }_{t}}\log p(\mathbf{x}_{t})+\nabla_{\mathbf{x}_{t}}\log\mathbb{E}_{p(\mathbf{ x}_{1}|\mathbf{x}_{t})}\left[r(\mathbf{x}_{1})\right]\] (21)

For our experiments, we follow the source code4 provided in recent diffusion sampler benchmarks [65]. We utilize a batch size of 500, with finetuning at 5,000 training iterations, a learning rate of 0.0001, a diffusion time scale of 5.0, 100 steps, and a log variance range of 4.0. The neural architecture employed is identical to that used in [65]. For pretraining the prior model, we use the same hyperparameters as above, but with 10,000 training iterations using maximum likelihood estimation with true samples.

Footnote 4: https://github.com/GFNOrg/gfn-diffusion

Results.As we reported in the main text, in Fig. 1, we present illustrative results. The classifier-guided diffusion model shows biased posterior sampling (Fig. 1f), failing to provide accurate inference. RL with a per step KL constraint cannot exactly optimize for the posterior distribution, making the tuning of the KL weight \(\alpha\) crucial to achieving desirable output Fig. 1. RTB asymptotically achieves the true posterior without introducing a balancing hyperparameter \(\alpha\). Another advantage of our approach is off-policy exploration for efficient mode coverage. RL methods for fine-tuning diffusion models (e.g., DPOK [16], DDPO [8]) typically use policy gradient style methods that are on-policy. By using a simple off-policy trick introduced by [47, 38] and demonstrated by Sendera et al. [65], we can introduce randomness into the exploration process in diffusion by adding \(\frac{\epsilon^{2}}{T}\), where \(\epsilon\) is a noise hyperparameter and \(T\) is the diffusion timestep, into the variances and annealing it to zero over training iterations. We set \(\epsilon=0.5\) for off-policy exploration. As shown in Fig. 2, RTB with off-policy exploration gives very close posterior inferences, whereas off-policy exploration in RL with \(\alpha=0.5\) (which is a carefully selected hyperparameter) does not improve performance due to its on-policy nature.

## Appendix D Code

Code for all experiments is available at https://github.com/GFNOrg/diffusion-finetuning and will continue to be maintained and extended.

## Appendix E On classifier guidance and RTB posterior sampling

### Experimental Details

In our experiments, we fine-tune pretrained unconditional diffusion models with our RTB objective, to sample from a posterior distribution in the form \(p^{\text{post}}(x\mid y)=p(y\mid x)p(x)\). In this section, we detail the experimental settings for RTB as well as the compared baselines.

Experiments setting.For MNIST, we pretrain a noise-predicting diffusion model on \(28\times 28\) (upscaled to \(32\times 32\)) single channel images of digits from the MNIST datasets. We discretize the forward and backward processes into 200 steps and train our model until convergence. For CIFAR-10, we use a pretrained model from [27], trained to generate \(32\times 32\) 3-channel images from the CIFAR-10 dataset, while discretizing the noising/denoising processes into 1000 steps. For fine-tuning the prior, we parametrize the posterior with LoRA weights [28], with the number of parameters equal to about 3% of the prior model's parameter count. We train our models on a single NVIDIA V100 GPU.

We compute FID as a similarity score estimate of the _true_ posterior distribution from the data. As such, the computation is limited to the total number of per-class-samples present in the data, (between 5k and 6k for CIFAR-10 and MNIST digits, and 30k for the even/odd task).

Figure C.1: Tuning the KL weight \(\alpha\) in reinforcement learning: influences the balance between sticking to the prior distribution and moving towards the modes of the reward density. A higher \(\alpha\) value maintains closer adherence to the prior, while a lower \(\alpha\) allows a gradual shift towards high values of \(r(\mathbf{x})\). Setting \(\alpha\) below 0.3 tends to cause mode collapse, moving too far from the prior and focusing on maximizing rewards for single modes. \(\alpha=0.5\) gives us samples that closest resembles the posterior.

Figure C.2: Off-policy exploration benefits for RTB training. RTB, with simple off-policy exploration techniques that increase randomness in the diffusion process, significantly improves mode coverage. On the other hand, policy gradient RL methods which are typically used to finetune diffusion models are on-policy, and hence prone to mode collapse.

RtB.For RTB fine-tuning, we finetune a diffusion model following the objective in Equation 9. We impose the objective while sampling denoising paths following a DDPM sampling scheme, with only 20% to 50% of the original trained steps. We employ loss clipping at 0.1, to account for imperfect constraints in the pretrained prior, and train each of our models for 1500 training iterations, well into convergence trends.

Rl[16].We implement two RL-based fine-tuning techniques derived from DPOK [16] and DDPO [8], respectively with and without KL regularization. These implementations use a reinforcement learning baseline similar to the one in our experiments described in SS3.2. By following the same sampling scheme as in our RTB experiments, we enable a direct comparison with RTB. To fine-tune the KL weight, we perform a search over \(\alpha\in\{0.01,0.1,1.0\}\).

Dp[11].We implement and adapt the Gaussian version of the posterior sampling scheme in [11], originally devised for noisy inverse problems. This method relaxes some of our experimental constraints, as it requires a differentiable reward \(r(\mathbf{x})\). We perform a sweep over ten values of the suggest parameter range for the step size \(\zeta\in[.1,1.]\) on MNIST single-digit sampling, and choose \(\zeta=0.1\) for our experiments.

Lgd-Mc[70].We adapt the implementation of the algorithm in [70] to sample from the classifier-based posteriors in CIFAR-10 and MNIST. Similarly to the DP baseline, we use our pretrained classifier to perform measurements at each sampling step, and use a Monte Carlo estimate of the gradient correction to guide the denoising process. We choose \(\zeta=0.1\) following the DP experiments and default the number of particles to 10 as per the authors' guidelines.

### Additional findings.

Classifier-guidance baselines.We find that the DP and LGD-MC classifier-guidance based baselines struggle to sample from the true posterior distribution in our experimental settings. The baselines achieve the lowers classifier average rewards in all tested settings. Despite choosing \(\zeta=0.1\) as the validate best performing hyperparameter, we also also observe the posterior samples from DP and LGD-MC to be close to the prior. As such, DP and LGD-MC score high in diversity, and low in FID for the Even/Odd experimental scenario, as expected from prior sampling benchmarks, but failing to appropriately model the posterior distribution.

Rl and mode collapse.In the pure Reinforcement Learning objective imposed for the experiments in SS3.1 (no KL), we observe a significantly higher reward than other baseline methods, while showcasing increased FID and lower diversity. In Fig. E.1 we show a random set of 16 samples for posterior models trained on 4 different classes of the CIFAR-10 datasets, as well as the _Even_ objective from the MNIST dataset, after 500 training iterations. In the figure, we observe early mode collapse and reward exploitation, visually evident from the little to no variation amongst samples for each class class, and single-digit collapse in the multi-modal _even_ digits objective (see samples in Fig. 2 for comparison with our RTB-finetuned models).

Conditional architectures.We repeat the even/odd posterior sampling experiment of SS3.1 in a conditional setting, where the condition is an input to the posterior model. For the posterior architecture, we use a naive modification of the prior with an extra input channel, which is populated a full mask of 0 or 1 for conditioning on the even and odd classes, respectively. The results are shown in Table E.1. We look forward to future work which develops more specialized architectures for handling conditional constraints.

## Appendix F Infilling with discrete diffusion

Additional details.We illustrate some examples from the ROC Stories dataset used for training in Table F.1. For the prior we use the sedd-small5 model, which uses an absorbing noising process [4] with a log linear noise schedule, as the diffusion prior \(p(\mathbf{z}\mid\mathbf{x})\). The posterior model is

\begin{table}
\begin{tabular}{l l l} \hline \hline Dataset \(\rightarrow\) & \multicolumn{2}{c}{MNIST even/odd} \\ Algorithm \(\downarrow\) Metric \(\rightarrow\) & \(\mathbb{E}[\log r(\mathbf{x})]\) (\(\uparrow\)) & FID (\(\downarrow\)) \\ \hline DPS & \(-1.2270_{0.022}\) & \(1.1498_{0.0182}\) \\ LGDโMC & \(-1.1720_{0.199}\) & \(1.1445_{0.184}\) \\ DDPO & \(\mathbf{-8.6}_{0.12}\times 10^{-11}\) & \(1.8024_{0.023}\) \\ DPOK & \(-0.0783_{0.082}\) & \(1.2536_{0.026}\) \\
**RTB (unconditional)** & \(-0.1816_{0.175}\) & \(1.1794_{0.171}\) \\ \hline \hline
**RTB (conditional)** & \multicolumn{2}{c}{-0.1236} & \multicolumn{1}{c}{0.9112} \\ \hline \hline \end{tabular}
\end{table}
Table E.1: Conditional experiment for MNIST even/odd posterior. Note that the posterior model in the conditional experiment is different from that in the baselines because it uses a different architecture that includes an additional input channel.

parameterized as a copy of the prior. To condition the diffusion model on the beginning \(\mathbf{x}\) we set the tokens at the appropriate location in the state in the initial time step, \(\mathit{i.e.t}=0\). Our implementation is based on the original SEDD codebase 6. Training this model is computationally expensive (in terms of memory and speed) so we utilize the stochastic TB trick, only propagating the gradients through a subset of the steps of the trajectory. We also use the loss clipping trick as discussed in SS2.3. Specifically, we clip the loss below a certain threshold to 0, resulting in updates only when the loss is larger. This threshold - referred to as the loss clipping coefficient - is a hyperparameter. As this is a conditional problem we also use the relative VarGrad objective. We also use some tempering on the reward likelihood which helps in learning (\(\mathit{i.e.}\), \(\mathit{p_{\text{reward}}}(\mathbf{y}\mid\mathbf{x},\mathbf{z})^{\beta}\)) where \(\beta\) is the inverse temperature parameter. We perform all experiments on an NVIDIA A100-Large GPU. Note that we also tried a baseline of simply fine-tuning the diffusion model on the data but encountered some training instabilities that we could not fix. The hyperparameters used for training RTB in our experiments are detailed in Table F.2.

Footnote 6: https://github.com/louaaron/Score-Entropy-Discrete-Diffusion

Reward.For training \(\mathit{p_{\text{reward}}}\) we follow the training procedure and implementation from [29]7. Specifically, we fine-tune a GPT-2 Large model [57] on the stories dataset with full parameter fine-tuning using the trl library [81]. We trained for 20 epochs with a batch size of 64 and 32 gradient accumulation steps and a learning rate of 0.0005.

Footnote 7: https://github.com/GFNOrg/gfn-lm-tuning

Baselines.For the baselines, we adopt the implementations from [29]. A critical difference in our experiments compared to [29] is that the posterior model is not initialized with a base model that is fine-tuned on the stories dataset. To condition the model on \(X\) and \(Y\), as well as for the prompting baseline, we use the following prompt:

"Beginning: {X}\n End: {Y}\n Middle: "

During training for the autoregressive GFlowNet fine-tuning, a \((\mathbf{x},\mathbf{y})\) pair is sampled from the dataset and then sample (batch size) \(\mathbf{x}\)s for every \((X,Y)\), and \(\mathit{p_{\text{reward}}}(XZY)\) is used as the reward. Both the GFlowNet fine-tuning and supervised fine-tuning baseline use LoRA fine-tuning. We use the default hyperparameters from [29]. At test time, we sample 100 infills for each example in the test set from all the models at temperature 0.9, and average over 5 such draws.

Additional results.Table F.3, Table F.4 and Table F.5 illustrates some examples of the infills generated by the diffusion models. We note that the general quality of the samples is poor, due to a relatively weak prior. At the same time we can observe that the prompting baselines often generate infills that are unrelated to the current story. We also note that the RTB fine-tuned model can sometimes generate retitions as the reward model tends to assign high likelihood to repititions [85]. We also attempted a LLMEval [42] for evaluating the coherence of the stories but did not obtain statistically significant results.

\begin{table}
\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline
**Beginning (\(\mathbf{x}\))** & **Middle (\(\mathbf{z}\))** & **End (\(\mathbf{y}\))** \\ \hline I was going to a Hallowen party. I looked through my clothes but could not find a costume. I cut up my old clothes and constructed a costume. & I put my costume on and went to the party. & My friends loved my costume. \\ \hline Allen thought he was a very talented poet. He attended college to study creative writing. In college, he met a boy named Carl. & Carl told him that he wasnโt very good. & Because of this, Allen wore off poetry forever. \\ \hline \hline \end{tabular}
\end{table}
Table F.1: Examples of training samples for the language infilling task.

## Appendix G Offline RL

### Training details

Our method requires first training a diffusion-based behavior policy \(\pi_{\theta}\) and a Q-function \(Q_{\psi}\). Once \(\pi_{\theta}\) and \(Q_{\psi}\) are trained, The posterior policy \(\pi_{\gamma}\) is trained using RTB, with its weights initialized to the trained behavior policy weights \(\theta\).

The behavior policy \(\pi_{\theta}\) is parametrized as a state-conditioned noise-predicting denoising diffusion probabilistic model (DDPM) [27] with a linear schedule, and 75 denoising steps. The diffusion model takes as input a state \(s\), a noiseed action \(a_{t}\) and a noise level \(t\) and predicts the source noise \(\epsilon\). The state \(s\) and noised action \(a_{t}\) are concatenated with Fourier features computed on the noise level \(t\), which are then fed through a 3-layer MLP of hidden dimensionality 256, with layer normalization and a GeLU activation after each hidden layer. The behavior policy is trained using the Adam optimizer with batch size 512 and learning rate 5e-4 for 10000 epochs. The Q-function \(Q_{\psi}\) is trained using IQL. We use the same IQL experimental configurations and training hyperparameters as in [36]. That is, we set \(\tau=0.7\). The architecture for \(Q_{\psi}\) is a 3-layer MLP with hidden dimensionality 256 and ReLU activations, which is trained using the Adam optimizer with a learning rate 3e-4 and batch size 256 for 750000 gradient steps. The task rewards are normalized as in [36] and the target network is updated

\begin{table}
\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline
**Beginning (x)** & **Middle (z)** & **End (y)** \\ \hline David noticed he had put on a lot of weight recently. He examined his habits to try and figure out the reason. He realized heโd been eating too much fast food lately. & **He stopped going to burger places and started a vegetarian diet.** He reviewed his habits to try to figure out how to change He asked he thought try to cut down on the amount amount. He examined his habits to try and figure out the reason. He realized he had been eating too much fast food recently. & **After a few weeks, he started to feel much better.** \\ \hline Robbie was competing in a cross country meet. He was halfway through when his leg cramped up. Robbie wasnโt sure he could go on. & **He stopped for a minute and stretched his bad leg.** Robbie began to run again and finished the race in second place. \\ \hline Robbie was competing in a cross country meet. He was halfway through when his leg cramped up. Robbie wasnโt sure he could go on. & **He stopped for a minute and stretched his bad leg.** Robbie was sure he could go on. Robbie was sure he could go on. & Robbie began to run again and finished the race in second place. \\ \hline \hline \end{tabular}
\end{table}
Table F.2: Hyperparameters for the story infilling task.

\begin{table}
\begin{tabular}{p{113.8pt} p{113.8pt}} \hline \hline Batch size & 16 \\ Gradient accumulation steps & 8 \\ Learning rate & 1e-5 \\ Warmup Step & 20 \\ Optimizer & AdamW \\ Reward temperature start & 1.2 \\ Reward inverse temperature end & 0.9 \\ Reward inverse temperature horizon & 5000 \\ Number of training steps & 1500 \\ Loss clipping coefficient & 0.1 \\ Discretization steps \(T\) & 15 \\ \hline \hline \end{tabular}
\end{table}
Table F.3: Examples of infills generated by the posterior trained with RTB along with **reference infills** for the stories infilling task.

\begin{table}
\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline
**Beginning (x)** & **Middle (z)** & **End (y)** \\ \hline David noticed he had put on a lot of weight recently. He examined his habits to try and figure out the reason. He realized heโd been eating too much fast food lately. & **He stopped going to burger places and started a vegetarian diet.** & After a few weeks, he started to feel much better. \\ \hline Robbie was competing in a cross country meet. He was halfway through when his leg cramped up. Robbie wasnโt sure he could go on. & **He stopped going to burger places and started a vegetarian diet.** & **End (y)** \\ \hline Robbie was treated heโd been eating too much fast food lately. & **He stopped going to burger places and started a vegetarian diet.** & **After a few weeks, he started to feel much better.** \\ \hline \end{tabular}
\end{table}
Table 4: Examples of infills generated by Prompt \((\textbf{x},\textbf{y})\) along with **reference infills** for the stories infilling task.

\begin{table}
\begin{tabular}{p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline
**Beginning (x)** & **Middle (z)** & **End (y)** \\ \hline David noticed he had put on a lot of weight recently. He examined his habits to try and figure out the reason. He realized heโd been eating too much fast food lately. & **He stopped going to burger places and started a vegetarian diet.** & **After a few weeks, he started to feel much better.** \\ \hline \end{tabular}
\end{table}
Table 5: Examples of infills generated by Prompt \((\textbf{x},\textbf{y})\) along with **reference infills** for the stories infilling task.

with soft updates of \(m=0.005\). The posterior policy \(\pi_{\gamma}\) is trained using the relative trajectory balance objective. \(\pi_{\gamma}\) is also parametrized as a state-conditioned noise-predicting DDPM, initialized as a copy of the prior. We additionally use the Langevin dynamics inductive bias (11), and learn an additional MLP for the energy scaling network. The posterior noise prediction network also outputs an additive correction to the output of the prior noise prediction network. That is, the predicted noise of the posterior diffusion model is defined as \(\epsilon(s,a_{t},t):=\epsilon(s,a_{t},t;\theta)+\epsilon(s,a_{t},t;\gamma)\), where \(\epsilon(\cdot;\theta)\) is the output of the prior noise prediction network and \(\epsilon(\cdot;\gamma)\) is the output of the posterior noise prediction network. We train all models on a single NVIDIA A100-Large GPU. The only hyperparameter tuned per task is the temperature \(\alpha\) which we show in Table G.2.

Note that in these experiments both the prior and constraint are conditioned on the state \(\mathbf{s}\). To prevent having to learn a neural network for \(\log Z_{\phi}(\mathbf{s})\), we employ a variant of VarGrad objective [61]. For each state \(\mathbf{s}\) sampled in the minibatch, we further generate \(k=64\) on-policy trajectories \(\tau^{(i)}_{i=1}{}^{k}\) with \(\pi_{\gamma}\). Each of these trajectories can be used to implicitly estimate \(\log Z(\mathbf{s})\):

\[\log\hat{Z}(\mathbf{s})^{(i)}=\log\pi_{\theta}(\tau^{(i)}\mid\mathbf{s})+Q_{ \psi}(\mathbf{s},\mathbf{a}_{1}^{(i)})-\log\pi_{\gamma}(\tau^{(i)}\mid\mathbf{ s})\] (22)

We then minimize the sample variance across the batch:

\[\mathcal{L}_{\text{RTB}}^{\text{VarGrad}}(\gamma)=\frac{1}{k}\sum_{i=1}^{k} \left(\log\hat{Z}(\mathbf{s})^{(i)}-\frac{1}{k}\sum_{j=1}^{k}\log\hat{Z}( \mathbf{s})^{(j)}\right)^{2}\] (23)

RTB allows off-policy training so we are not restricted to train with samples generated on-policy. We thus also leverage the offline dataset, which are samples from the prior and noise them with the DDPM noising process to generate off-policy trajectories with high density under the prior. Since there are actions in the replay buffer from high reward episodes in the tasks, this can help training efficiency compared to purely online training. We ran 5 seeds of training each with mixed training (off-policy and on-policy) and pure on-policy training on the medium-replay tasks, with results shown in Table G.1, where mixed training outperforms pure online training on two of the three tasks.

### Baseline details

As is standard in offline RL, we use the reported performance numbers from the previous papers. CQL, IQL are reported from the IQL paper. Diffuser (D), DD, D-QL and QGPO are reported from the QGPO paper. Their implementation improved the performance of D and D-QL compared to their original papers. IDQL results are reported from the IDQL paper. We follow the evaluation protocol of previous work, and report the mean performance over 10 episodes, averaged across 5 random seeds at the end of training (150k training steps).

\begin{table}
\begin{tabular}{l c} \hline \hline Task & RTB (Online) & RTB (Mixed) \\ \hline halfcheetah-medium-replay & \(46.88\pm 0.51\) & \(48.11\pm 0.56\) \\ hopper-medium-replay & \(99.23\pm 3.22\) & \(100.40\pm 0.21\) \\ walker2d-medium-replay & \(94.01\pm 0.28\) & \(93.57\pm 2.63\) \\ \hline \hline \end{tabular}
\end{table}
Table G.1: Mixed vs. online training on DR4L Tasks. We report mean\({}_{\pm 84}\) over 5 random seeds.

Fine-tuning text-to-image diffusion models

We build off the DPOK implementation8, which fine-tunes stable-diffusion-v1-5 with ImageReward function. The posterior model to be fine-tuned is initialized as a copy of the prior model. We use LoRA [28] since it is significantly more efficient than fine-tuning the entire model. Sampling of images is done with 50 steps of DDIM [69]. Even with LoRA, it is still difficult to fit gradients of all steps in the diffusion trajectory in memory. To help with this, we use a "stochastic subsampling" trick (SSH.1).

Footnote 8: https://github.com/google-research/google-research/tree/master/dpok

We train all models on a single NVIDIA A100-Large GPU. For the main experiments, we use the default parameters for DPOK of reward weight \(\beta=10\) and KL weight = 0.01. For RTB we fix \(\beta=1.0\) for all prompts. We next perform an ablation over different values of \(\beta\).

We plot in TableH.1 the final average reward and diversity score for models trained with different values of reward weight \(\beta\) for the prompt "A green colored rabbit.". As expected, we find that increasing \(\beta\) increases reward at the cost of diversity for RTB and DPOK. The exception is \(\beta=10\) for RTB which has slightly lower final reward than \(\beta=1\), which we could attribute to more difficult optimization due to the peaky distribution associated with higher reward weight.

### Memory-efficient learning

We propose two methods to reduce the memory requirement of RTB fine-tuning.

Stochastic subsampling.The expected gradient of the RTB objective (9) is unaffected by propagating gradient to a randomly sampled subset of the timesteps in a trajectory and rescaling by the inverse proportion of timesteps sampled. Stochastically subsampling timesteps for gradient propagation in this way can significantly decrease memory consumption because computation graphs for the remaining timesteps do not need to be maintained; however, such subsampling increases gradient variance, so it is preferable to keep gradients for as many timesteps as possible to fit in memory. For our text-to-image experiments, we found sampling 8 timesteps out of 50 to keep gradients was sufficient.

Batched gradient computation.An important property of the RTB objective is that computing its gradient does not require storing the computation graph of all timesteps. The gradient of the RTB objective for a single trajectory is just the sum of per-step log-likelihood gradients scaled by the RTB residual:

\[\nabla_{\phi}\mathcal{L}_{\text{RTB}}(\tau;\phi)=2\left(\log\frac{Z_{\phi}}{r( \mathbf{x}_{1})}+\sum_{i=1}^{T}\log\frac{p_{\phi}^{post}(\mathbf{x}_{i\Delta t }\mid\mathbf{x}_{(i-1)\Delta t})}{p_{\theta}(\mathbf{x}_{i\Delta t}\mid \mathbf{x}_{(i-1)\Delta t})}\right)\cdot\nabla_{\phi}\sum_{i=1}^{T}\log p_{ \phi}^{\text{post}}(\mathbf{x}_{i\Delta t}\mid\mathbf{x}_{(i-1)\Delta t}).\]

Because the likelihood gradients can be accumulated during the forward pass, this allows for a batched gradient accumulation version of the update. For trajectory length (number of diffusion steps) \(T\) and accumulation batch size (number of time steps receiving a gradient signal in each backward pass) \(B\), the number of batched forward passes required scales as \(\frac{T}{B}\).

\begin{table}
\begin{tabular}{l l l l} \hline \hline Model \(\downarrow\) & \(\beta\downarrow\) Metric \(\rightarrow\) & Reward (\(\uparrow\)) & diversity (\(\uparrow\)) \\ \hline Prior & - & -0.113 & 0.597 \\ \hline \multirow{4}{*}{DPOK (KL weight=0.01)} & \(\beta=0.01\) & -0.27 & 0.1488 \\  & \(\beta=0.1\) & -0.06 & 0.1486 \\  & \(\beta=1.0\) & 0.638 & 0.1362 \\  & \(\beta=10.0\) & 1.492 & 0.076 \\ \hline DOPO (KL weight=0.0) & \(\beta=10.0\) & 1.795 & 0.0493 \\ \hline \multirow{4}{*}{RTB} & \(\beta=0.01\) & 0.485 & 0.1431 \\  & \(\beta=0.1\) & 1.525 & 0.0721 \\ \cline{1-1}  & \(\beta=1.0\) & 1.756 & 0.0436 \\ \cline{1-1}  & \(\beta=10.0\) & 1.568 & 0.0689 \\ \hline \hline \end{tabular}
\end{table}
TableH.1: Ablation of reward weights \(\beta\) for โA green colored rabbit.โ.

Only the accumulation batch size \(B\), not the trajectory length \(T\), is constrained by the memory budget. This means we can easily scale training with large number of diffusion steps without increasing the variance of the gradient through stochastic subsampling, with training time growing linearly with number of time steps under a fixed memory budget. Although this method is not used in the main experiments presented here, preliminary experiments confirm these observations.

We highlight that both methods are not applicable to diffusion samplers based on differentiable simulation (_e.g._, PIS and DDS), which need to store the entire computation graph of SDE integration. For these methods, the memory requirement scales linearly with the trajectory length.

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_EMPTY:29]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We discuss our theoretical claims about the RTB ojective in SS2, and report experimental results in SS3. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our proposed method in SS5. Guidelines: 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: See SSA. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided code to reproduce our experiments in SSD, and described training details in SSC, SSE, SSF, SSG and SSH. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.

* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code is provided in SSD and can be used to reproduce our main results. We will be releasing our code publicly.. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details**Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Training details are outlined in \(\lx@sectionsign\)C, \(\lx@sectionsign\)E, \(\lx@sectionsign\)F, and \(\lx@sectionsign\)G. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All experimental results have error bars, except the Stable Diffusion finetuning experiment \(\lx@sectionsign\)3.2 which was only trained on one seed per prompt due to compute constraints. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Compute resources used in experiments is outlined in \(\lx@sectionsign\)E, \(\lx@sectionsign\)F, \(\lx@sectionsign\)G, \(\lx@sectionsign\)H and summarised in \(\lx@sectionsign\)I. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.

* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper conforms to the stated ethics guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss broader impact of the work in SS5. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release models that have risk of misuse. Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Creaters of code we use are properly credited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not have any crowdsourcing experiments or research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not have any experiments with human subjects Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.