# On Socially Fair Low-Rank Approximation and Column Subset Selection

 Zhao Song

The Simons Institute for the Theory of Computing

UC Berkeley

magic.linuxkde@gmail.com &Ali Vakilian

Toyota Technological Institute at Chicago

vakilian@ttic.edu &David P. Woodruff

Department of Computer Science

Carnegie Mellon University

dwoodruf@andrew.cmu.edu &Samson Zhou

Department of Computer Science

Texas A&M University

samsonzhou@gmail.com

###### Abstract

Low-rank approximation and column subset selection are two fundamental and related problems that are applied across a wealth of machine learning applications. In this paper, we study the question of socially fair low-rank approximation and socially fair column subset selection, where the goal is to minimize the loss over all sub-populations of the data. We show that surprisingly, even constant-factor approximation to fair low-rank approximation requires exponential time under certain standard complexity hypotheses. On the positive side, we give an algorithm for fair low-rank approximation that, for a constant number of groups and constant-factor accuracy, runs in \(2^{\text{poly}(k)}\) time rather than the naive \(n^{\text{poly}(k)}\), which is a substantial improvement when the dataset has a large number \(n\) of observations. We then show that there exist bicriteria approximation algorithms for fair low-rank approximation and fair column subset selection that run in polynomial time.

## 1 Introduction

Machine learning algorithms are increasingly used in technologies and decision-making processes that affect our daily lives, from high volume interactions such as online advertising, e-mail filtering, smart devices, or large language models, to more critical processes such as autonomous vehicles, healthcare diagnostics, credit scoring, and sentencing recommendations in courts of law [17, 18, 21]. Machine learning algorithms frequently require statistical analysis, utilizing fundamental problems from numerical linear algebra, especially low-rank approximation and column subset selection.

In the classical low-rank approximation problem, the input is a data matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and an integral rank parameter \(k>0\), and the goal is to find the best rank \(k\) approximation to \(\mathbf{A}\), i.e., to find a set of \(k\) vectors in \(\mathbb{R}^{d}\) that span a matrix \(\mathbf{B}\), which minimizes \(\mathcal{L}(\mathbf{A}-\mathbf{B})\) across all rank-\(k\) matrices \(\mathbf{B}\), for some loss function \(\mathcal{L}\). The rank parameter \(k\) should be chosen to accurately represent the complexity of the underlying model chosen to fit the data, and thus the low-rank approximation problem is often used for mathematical modeling and data compression.

Similarly, in the classic column subset selection problem, the goal is to choose \(k\) columns \(\mathbf{U}\) of \(\mathbf{A}\) so as to minimize \(\mathcal{L}(\mathbf{A}-\mathbf{U}\mathbf{V})\) across all choices of \(\mathbf{V}\in\mathbb{R}^{k\times d}\). Although low-rank approximation can reveal important latent structure among the dataset, the resulting linear combinations may not be as interpretable as simply selecting \(k\) features. The column subset selection problem is thereforea version of low-rank approximation with the restriction that the left factor must be \(k\) columns of the data matrix. Column subset selection also tends to result in sparse models. For example, if the columns of \(\mathbf{A}\) are sparse, then the columns in the left factor \(\mathbf{U}\) are also sparse. Thus in some cases, column subset selection, often also called feature selection, can be more useful than general low-rank approximation.

**Algorithmic fairness.** Unfortunately, real-world machine learning algorithms across a wide variety of domains have recently produced a number of undesirable outcomes from the lens of generalization. For example, [1] noted that decision-making processes using data collected from smartphone devices reporting poor road quality could potentially underserve poorer communities with less smartphone ownership. [13] observed that search queries for CEOs overwhelmingly returned images of white men, while [1] observed that facial recognition software exhibited different accuracy rates for white men compared with dark-skinned women.

Initial attempts to explain these issues can largely be categorized into either "biased data" or "biased algorithms", where the former might include training data that is significantly misrepresenting the true statistics of some sub-population, while the latter might sacrifice accuracy on a specific sub-population in order to achieve better global accuracy. As a result, an increasingly relevant line of active work has focused on designing _fair_ algorithms. An immediate challenge is to formally define the desiderata demanded from fair algorithmic design and indeed multiple natural quantitative measures of fairness have been proposed [14, 15, 16, 17]. However, [17, 18] showed that many of these conditions for fairness cannot be simultaneously achieved.

In this paper, we focus on _socially fair_ algorithms, which seek to optimize the performance of the algorithm across all sub-populations. That is, for the purposes of low-rank approximation and column subset selection, the goal is to minimize the maximum cost across all sub-populations. For socially fair low-rank approximation, the input is a set of data matrices \(\mathbf{A}^{(1)}\in\mathbb{R}^{n_{1}\times d},\ldots,\mathbf{A}^{(\ell)}\in \mathbb{R}^{n_{\ell}\times d}\) corresponding to \(\ell\) groups and a rank parameter \(k\), and the goal is to determine a set of \(k\) factors \(\mathbf{U}_{1},\ldots,\mathbf{U}_{k}\in\mathbb{R}^{d}\) that span matrices \(\mathbf{B}^{(1)},\ldots,\mathbf{B}^{(\ell)}\), which minimize \(\max_{i\in[\ell]}\|\mathbf{A}^{(i)}-\mathbf{B}^{(i)}\|_{F}\). Due to the Eckart-Young-Mirsky theorem stating that the Frobenius loss is minimized when each \(\mathbf{A}^{(i)}\) is projected onto the span of \(\mathbf{U}=\mathbf{U}_{1}\circ\ldots\circ\mathbf{U}_{k}\), the problem is equivalent to \(\min_{\mathbf{U}\in\mathbb{R}^{k\times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i) }\mathbf{U}^{\dagger}\mathbf{U}-\mathbf{A}^{(i)}\|_{F}\), where \(\dagger\) denotes the Moore-Penrose pseudoinverse. We remark that as we can scale the columns in each group (even each individual column), our formulation captures min-max normalized cost relative to the total Frobenius norm of each group and the optimal rank-\(k\) reconstruction loss for each group.

### Our Contributions and Technical Overview

In this paper, we study socially fair low-rank approximation and socially fair column subset selection.

**Fair low-rank approximation.** We first describe our results for socially fair low-rank approximation. We first show that under the assumption that \(\mathrm{P}\neq\mathrm{NP}\), fair low-rank approximation cannot be approximated within any constant factor in polynomial time.

**Theorem 1.1**.: _Fair low-rank approximation is NP-hard to approximate within any constant factor._

We show Theorem1.1 by reducing to the problem of minimizing the distance of a set of \(n\) points in \(d\)-dimensional Euclidean space to all sets of \(k\) dimensional linear subspaces, which was shown by [1], DTV11] to be NP-hard to approximate within any constant factor. In fact, [1, 1] showed that a constant-factor approximation to this problem requires runtime exponential in \(k\) under a stronger assumption, the exponential time hypothesis [12]. We show similar results for the fair low-rank approximation problem.

**Theorem 1.2**.: _Under the exponential time hypothesis, the fair low-rank approximation requires \(2^{k^{\Omega(1)}}\) time to approximate within any constant factor._

Together, Theorem1.1 and Theorem1.2 show that under standard complexity assumptions, we cannot achieve a constant-factor approximation to fair low-rank approximation using time polynomial in \(n\) and exponential in \(k\). We thus consider additional relaxations, such as bicriteria approximation (Theorem1.4) or \(2^{\mathrm{poly}(k)}\) runtime (Theorem1.3). On the positive side, we first show that for a constant number of groups and constant-factor accuracy, it suffices to use runtime \(2^{\mathrm{poly}(k)}\) rather than the naive \(n^{\mathrm{poly}(k)}\), which is a substantial improvement when the dataset has a large number of observations, i.e., \(n\) is large.

**Theorem 1.3**.: _Given an accuracy parameter \(\varepsilon\in(0,1)\), there exists an algorithm which outputs \(\widetilde{\mathbf{V}}\in\mathbb{R}^{k\times d}\) such that with probability at least \(\frac{2}{3}\), \(\max_{i\in[\ell]}\|\mathbf{A}^{(i)}(\widetilde{\mathbf{V}})^{\dagger} \widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F}\leq(1+\varepsilon)\cdot\min_{ \mathbf{V}\in\mathbb{R}^{k\times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i)} \mathbf{V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}\). The algorithm uses runtime \(\frac{1}{\varepsilon}\operatorname{poly}(n)\cdot(2\ell)^{\mathcal{O}(N)}\), for \(n=\sum_{i=1}^{\ell}n_{i}\) and \(N=\operatorname{poly}\big{(}\ell,k,\frac{1}{\varepsilon}\big{)}\)._

Next, we show that there exists a bicriteria approximation algorithm for fair low-rank approximation that uses polynomial runtime.

**Theorem 1.4**.: _Given a trade-off parameter \(c\in(0,1)\), there exists an algorithm that outputs \(\widetilde{\mathbf{V}}\in\mathbb{R}^{t\times d}\) for \(t=\mathcal{O}\left(k(\log\log k)(\log^{2}d)\right)\) such that with probability at least \(\frac{2}{3}\),_

\[\max_{i\in[\ell]}\|\mathbf{A}^{(i)}(\widetilde{\mathbf{V}})^{\dagger} \widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F}\leq\ell^{c}\cdot 2^{1/c}\cdot \mathcal{O}\left(k(\log\log k)(\log d)\right)\cdot\min_{\mathbf{V}\in\mathbb{ R}^{k\times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{V}^{\dagger} \mathbf{V}-\mathbf{A}^{(i)}\|_{F}.\]

_The algorithm uses runtime polynomial in \(n\) and \(d\)._

The algorithm for Theorem1.4 substantially differs from that of Theorem1.3. For one, we can no longer use a polynomial system solver, because it would be infeasible to achieve polynomial runtime. Instead, we observe that for sufficiently large \(p\), we have \(\max\|\mathbf{x}\|_{\infty}=(1\pm\varepsilon)\|\mathbf{x}\|_{p}\) and thus focus on optimizing \(\min_{\mathbf{V}\in\mathbb{R}^{k\times d}}\left(\sum_{i\in[\ell]}\|\mathbf{A} ^{(i)}\mathbf{V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}^{p}\right)^{1/p}\). However, the terms \(\|\mathbf{A}^{(i)}\mathbf{V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}^{p}\) are difficult to handle, so we apply Dvoretzky's Theorem, i.e., Theorem3.2, to generate matrices \(\mathbf{G}\) and \(\mathbf{H}\) so that \((1-\varepsilon)\|\mathbf{G}\mathbf{M}\mathbf{H}\|_{p}\leq\|\mathbf{M}\|_{F} \leq(1+\varepsilon)\|\mathbf{G}\mathbf{M}\mathbf{H}\|_{p}\), for all matrices \(\mathbf{M}\in\mathbb{R}^{n\times d}\), so that it suffices to approximately solve \(\min_{\mathbf{X}\in\mathbb{R}^{k\times d}}\|\mathbf{G}\mathbf{A}\mathbf{H} \mathbf{S}\mathbf{X}-\mathbf{G}\mathbf{A}\mathbf{H}\|_{p}\), for \(\mathbf{A}=\mathbf{A}^{(1)}\circ\ldots\circ\mathbf{A}^{(\ell)}\).

Although low-rank approximation with \(L_{p}\) loss cannot be well-approximated in polynomial time, we recall that there exists a matrix \(\mathbf{S}\) that samples a "small" number of columns of \(\mathbf{A}\) to provide a coarse bicriteria approximation to \(L_{p}\) low-rank approximation [23]. However, we require a solution with dimension \(d\) and thus we seek to solve the regression problem \(\min_{\mathbf{X}}\|\mathbf{G}\mathbf{A}\mathbf{H}\mathbf{S}\mathbf{X}-\mathbf{ G}\mathbf{A}\mathbf{H}\|_{p}\). Thus, we consider a Lewis weight sampling matrix \(\mathbf{T}\) such that

\[\frac{1}{2}\|\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\mathbf{S}\mathbf{X}- \mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\|_{p}\leq\|\mathbf{G}\mathbf{A} \mathbf{H}\mathbf{S}\mathbf{X}-\mathbf{G}\mathbf{A}\mathbf{H}\|_{p}\leq 2\| \mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\mathbf{S}\mathbf{X}-\mathbf{T} \mathbf{G}\mathbf{A}\mathbf{H}\|_{p}.\]

and again note that \((\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\mathbf{S})^{\dagger}\mathbf{T} \mathbf{G}\mathbf{A}\mathbf{H}\) is the closed-form solution to the minimization problem \(\min_{\mathbf{X}}\|\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\mathbf{S}\mathbf{ X}-\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\|_{F}\), which only provides a small distortion to the \(L_{p}\) regression problem, since \(\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\) has a small number of rows due to the dimensionality reduction. We then observe that by Dvoretzky's Theorem, \((\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\mathbf{S})^{\dagger}\mathbf{T} \mathbf{G}\mathbf{A}\) is a "good" approximate solution to the original fair low-rank approximation problem. Given \(\delta\in(0,1)\), the success probabilities for both Theorem1.3 and Theorem1.4 can be boosted to arbitrary \(1-\delta\) by taking the minimum of \(\mathcal{O}\left(\log\frac{1}{\delta}\right)\) independent instances of the algorithm, at the cost of increasing the runtime by the same factor.

**Fair column subset selection.** We next describe our results for fair column subset selection. We give a bicriteria approximation algorithm for fair column subset selection that uses polynomial runtime.

**Theorem 1.5**.: _Given input matrices \(\mathbf{A}^{(i)}\in\mathbb{R}^{n_{i}\times d}\) with \(n=\sum n_{i}\), there exists an algorithm that selects a set \(S\) of \(k^{\prime}=\mathcal{O}\left(k\log k\right)\) columns such that with probability at least \(\frac{2}{3}\), \(S\) is a \(\mathcal{O}\left(k(\log\log k)(\log d)\right)\)-approximation to the fair column subset selection problem. The algorithm uses runtime polynomial in \(n\) and \(d\)._

The immediate challenge in adapting the previous approach for fair low-rank approximation to fair column subset selection is that we required the Gaussian matrices \(\mathbf{G},\mathbf{H}\) to embed the awkward maximum of Frobenius losses \(\min\max_{i\in[\ell]}\|\cdot\|_{F}\) into the more manageable \(L_{p}\) loss \(\min\|\cdot\|_{p}\) through \(\mathbf{G}\mathbf{A}\mathbf{H}\). However, selecting columns of \(\mathbf{G}\mathbf{A}\mathbf{H}\) does not correspond to selecting columns of the input matrices \(\mathbf{A}^{(1)},\ldots,\mathbf{A}^{(\ell)}\).

Instead, we view the bicriteria solution \(\widetilde{\mathbf{V}}\) from fair low-rank approximation as a good starting point for the right factor for fair low-rank approximation. Thus we consider the multi-response regression problem \(\max_{i\in[\ell]}\min_{\mathbf{B}^{(i)}}\|\mathbf{B}^{(i)}\widetilde{\mathbf{V}} -\mathbf{A}^{(i)}\|_{F}\). We then argue through Dvoretzky's theorem that a leverage score sampling matrix \(\mathbf{S}\) that samples \(\mathcal{O}\left(k\log k\right)\) columns of \(\widetilde{\mathbf{V}}\) will provide a good approximation to the column subset selection problem. We defer the formal exposition to Section4.

**Empirical evaluations.** Finally, in Section5, we perform a number of experimental results on socially fair low-rank approximation, comparing the performance of the socially fair low-rank objective values associated with the outputs of the bicriteria fair low-rank approximation algorithm and the standard (non-fair) low-rank approximation that outputs the top \(k\) right singular vectors of the singular value decomposition.

Our experiments are on the Default of Credit Card Clients dataset [10], which is a common human-centric data used for benchmarks on fairness, e.g., see [11]. We perform empirical evaluations comparing the objective value and the runtime of our bicriteria algorithm with the aforementioned baseline, across various subsample sizes and rank parameters. Our results demonstrate that our bicriteria algorithm can perform better than the standard low-rank approximation algorithm across various parameter settings, even when the bicriteria algorithm is not allowed a larger rank than the baseline. Moreover, we show that our algorithm is quite efficient and in fact, the final step of extracting the low-rank factors is faster than the singular value decomposition baseline due to a smaller input matrix. All in all, our empirical evaluations indicate that our bicriteria algorithm can perform well in practice, thereby reinforcing the theoretical guarantees of the algorithm. Finally, we give a number of additional experiments on synthetic datasets, in AppendixD.

**Paper organization.** We first described a number of related works in Section2. We detail our socially fair low-rank approximation algorithms in Section3 and our socially fair column subset selection algorithms in Section4. We defer all proofs to the supplementary material. We present our experiments in Section5 and AppendixD. The reader may also find it helpful to consult AppendixA for standard notation used in our paper and additional preliminaries.

## 2 Related Work

Initial insight into _socially fair data summarization_ methods were presented by [11], where the concept of _fair PCA_ was explored. This study introduced the fairness metric of average reconstruction loss, expressed by the loss function \(\mathrm{loss}(\mathbf{A},\mathbf{B}):=\|\mathbf{A}-\mathbf{B}\|_{F}^{2}-\| \mathbf{A}-\mathbf{A}_{k}\|_{F}^{2}\), aiming to identify a \(k\)-dimensional subspace that minimizes the loss across the groups, with \(\mathbf{A}_{k}\) representing the best rank-\(k\) approximation of \(\mathbf{A}\). Their proposed approach, in a two-group scenario, identifies a fair PCA of up to \(k+1\) dimensions that is not worse than the optimal fair PCA with \(k\) dimensions. When extended to \(\ell\) groups, this method requires an additional \(k+\ell-1\) dimensions. Subsequently, [11] explored fair PCA from a distinct objective perspective, seeking a projection matrix \(\mathbf{P}\) optimizing \(\min_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{P}\|_{F}^{2}\). A pivotal difference between these works and ours is our focus on the reconstruction error objective, a widely accepted objective for regression and low-rank approximation tasks. Alternatively, [1, 10, 11] explored a different formulation of fair PCA. The main objective is to ensure that data representations are not influenced by demographic attributes. In particular, when a classifier is exposed only to the projection of points onto the \(k\)-dimensional subspace, it should be unable to predict the demographic attributes.

Recently, [12] studied a fair column subset selection objective similar to ours, focusing on the setting with two groups (i.e., \(\ell=2\)). They established the problem's NP-hardness and introduced a polynomial-time solution that offers relative-error guarantees while selecting a column subset of size \(\mathcal{O}\left(k\right)\).

For fair regression, initial research focused on designing models that offer similar treatment to instances with comparable observed results by incorporating _fairness regularizers_[1]. However, in [1], the authors studied a fairness notion closer to our optimization problem, termed as "bounded group loss". In their work, the aim is to cap each group's loss within a specific limit while also optimizing the cumulative loss. Notably, their approach diverged from ours, with a focus on the sample complexity and the problem's generalization error bounds.

[1] studied a similar socially fair regression problem under the name min-max regression. In their setting, the goal is to minimize the maximum loss over a mixture distribution, given samples from the mixture; our fair regression setting can be reduced to theirs. [1] observed that a maximum of norms is a convex function and can therefore be solved using projected stochastic gradient descent.

The term "socially fair" was first introduced in the context of clustering, aiming to optimize clustering costs across predefined group sets [11, 1]. In subsequent studies, tight approximation algorithms [12, 13], FPT approaches [10], and bicriteria approximation algorithms [11] for socially fair clustering have been presented.

## 3 Socially Fair Low-Rank Approximation

In this section, we consider algorithms and hardness for socially fair low-rank approximation. Let \(n_{1},\ldots,n_{\ell}\) be positive integers and for each \(i\in[\ell]\), let \(\mathbf{A}^{(i)}\in\mathbb{R}^{n_{i}\times d}\). Then for a norm \(\|\cdot\|\), we define the fair low-rank approximation problem to be \(\min_{\mathbf{V}\in\mathbb{R}^{k\times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i)} \mathbf{V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|\).

### \((1+\varepsilon)\)-Approximation Algorithm for Fair Low-Rank Approximation

We first give a \((1+\varepsilon)\)-approximation algorithm for fair low-rank approximation that uses runtime \(\frac{1}{\varepsilon}\operatorname{poly}(n)\cdot(2\ell)^{\mathcal{O}(N)}\), for \(n=\sum_{i=1}^{\ell}n_{i}\) and \(N=\operatorname{poly}\big{(}\ell,k,\frac{1}{\varepsilon}\big{)}\).

The algorithm first finds a value \(\alpha\) that is an \(\ell\)-approximation to the optimal solution, i.e., \(\min_{\mathbf{V}\in\mathbb{R}^{k\times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i) }\mathbf{V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}\) is at most \(\alpha\leq\ell\cdot\min_{\mathbf{V}\in\mathbb{R}^{k\times d}}\max_{i\in[\ell] }\|\mathbf{A}^{(i)}\mathbf{V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}\). We then repeatedly decrease \(\alpha\) by \((1+\varepsilon)\) while checking if the resulting quantity is still achievable. To efficiently check if \(\alpha\) is achievable, we first apply dimensionality reduction to each of the matrices by right-multiplying by an affine embedding matrix \(\mathbf{S}\), so that

\[(1-\varepsilon)\|\mathbf{A}^{(i)}\mathbf{V}^{\dagger}\mathbf{V}- \mathbf{A}^{(i)}\|_{F}^{2}\leq \|\mathbf{A}^{(i)}\mathbf{V}^{\dagger}\mathbf{V}\mathbf{S}-\mathbf{ A}^{(i)}\mathbf{S}\|_{F}^{2}\leq(1+\varepsilon)\|\mathbf{A}^{(i)}\mathbf{V}^{ \dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}^{2},\]

for all rank \(k\) matrices \(\mathbf{V}\) and all \(i\in[\ell]\).

Now if we knew \(\mathbf{V}\), then for each \(i\in[\ell]\), we can find \(\mathbf{X}^{(i)}\) minimizing \(\|\mathbf{X}^{(i)}\mathbf{V}\mathbf{S}-\mathbf{A}^{(i)}\mathbf{S}\|_{F}^{2}\) and the resulting quantity will approximate \(\|\mathbf{A}^{(i)}\mathbf{V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}^{2}\). In fact, we know that the minimizer is \((\mathbf{A}^{(i)}\mathbf{S})(\mathbf{V}\mathbf{S})^{\dagger}\) through the closed form solution to the regression problem. Let \(\mathbf{R}^{(i)}\) be defined so that \((\mathbf{A}^{(i)}\mathbf{S})(\mathbf{V}\mathbf{S})^{\dagger}\mathbf{R}^{(i)}\) has orthonormal columns, so that

\[\|(\mathbf{A}^{(i)}\mathbf{S})(\mathbf{V}\mathbf{S})^{\dagger}\mathbf{R}^{(i) })((\mathbf{A}^{(i)}\mathbf{S})(\mathbf{V}\mathbf{S})^{\dagger}\mathbf{R}^{(i )})^{\dagger}\mathbf{A}^{(i)}\mathbf{S}-\mathbf{A}^{(i)}\mathbf{S}\|_{F}^{2}= \min_{\mathbf{X}^{(i)}}\|\mathbf{X}^{(i)}\mathbf{V}\mathbf{S}-\mathbf{A}^{(i)} \mathbf{S}\|_{F}^{2},\]

and so we require that if \(\alpha\) is feasible, then \(\alpha\geq\|(\mathbf{A}^{(i)}\mathbf{S})(\mathbf{V}\mathbf{S})^{\dagger} \mathbf{R}^{(i)})((\mathbf{A}^{(i)}\mathbf{S})(\mathbf{V}\mathbf{S})^{\dagger }\mathbf{R}^{(i)})^{\dagger}\mathbf{A}^{(i)}\mathbf{S}-\mathbf{A}^{(i)} \mathbf{S}\|_{F}^{2}\). Unfortunately, we do not know \(\mathbf{V}\), so instead we use a polynomial solver to check whether there exists such a \(\mathbf{V}\). We remark that similar guessing strategies were employed by [14, 15, 16, 17] and in particular, [14] also uses a polynomial system in conjunction with the guessing strategy. Thus we write \(\mathbf{Y}=\mathbf{V}\mathbf{S}\) and its pseudoinverse \(\mathbf{W}=(\mathbf{V}\mathbf{S})^{\dagger}\) and check whether there exists a satisfying assignment to the above inequality, given the constraints (1) \(\mathbf{Y}\mathbf{W}\mathbf{Y}=\mathbf{Y}\), (2) \(\mathbf{W}\mathbf{Y}\mathbf{W}=\mathbf{W}\), and (3) \(\mathbf{A}^{(i)}\mathbf{S}\mathbf{W}\mathbf{R}^{(i)}\) has orthonormal columns. Note that since \(\mathbf{V}\in\mathbb{R}^{k\times d}\), then implementing the polynomial solver naively could require \(kd\) variables and thus use \(2^{\Omega(dk)}\) runtime. Instead, we note that we only work with \(\mathbf{V}\mathbf{S}\), which has dimension \(k\times m\) for \(m=\mathcal{O}\left(\frac{k^{2}}{\varepsilon^{2}}\log\ell\right)\), so that the polynomial solver only uses \(2^{\operatorname{poly}(mk)}\) time.

We now show a crucial structural property that allows us to distinguish between the case where a guess \(\alpha\) for the optimal value \(\mathsf{OPT}\) exceeds \((1+\varepsilon)\mathsf{OPT}\) or is smaller than \((1-\varepsilon)\mathsf{OPT}\) by simply looking at a polynomial system solver on an affine embedding.

**Lemma 3.1**.: _Let \(\mathbf{V}\in\mathbb{R}^{k\times d}\) be the optimal solution to the fair low-rank approximation problem for inputs \(\mathbf{A}^{(1)},\ldots,\mathbf{A}^{(\ell)}\), where \(\mathbf{A}^{(i)}\in\mathbb{R}^{n_{i}\times d}\), and suppose \(\mathsf{OPT}=\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{V}^{\dagger}\mathbf{V}- \mathbf{A}^{(i)}\|_{F}^{2}\). Let \(\mathbf{S}\) be an affine embedding for \(\mathbf{V}\) and let \(\mathbf{W}=(\mathbf{V}\mathbf{S})^{\dagger}\in\mathbb{R}^{k\times m}\). For \(i\in[\ell]\), let \(\mathbf{Z}^{(i)}=\mathbf{A}^{(i)}\mathbf{S}\mathbf{W}\in\mathbb{R}^{n_{i} \times k}\) and \(\mathbf{R}^{(i)}\in\mathbb{R}^{k\times k}\) be defined so that \(\mathbf{A}^{(i)}\mathbf{S}\mathbf{W}\mathbf{R}^{(i)}\) has orthonormal columns. If \(\alpha\geq(1+\varepsilon)\cdot\mathsf{OPT}\), then for each \(i\in[\ell]\), \(\alpha\geq\|(\mathbf{A}^{(i)}\mathbf{S}\mathbf{W}\mathbf{R}^{(i)})(\mathbf{A}^{ (i)}\mathbf{S}\mathbf{W}\mathbf{R}^{(i)})^{\dagger}\mathbf{A}^{(i)}-\mathbf{A} ^{(i)}\|_{F}^{2}\). If \(\alpha<(1-\varepsilon)\cdot\mathsf{OPT}\), then there exists \(i\in[\ell]\), such that \(\alpha<\|(\mathbf{A}^{(i)}\mathbf{S}\mathbf{W}\mathbf{R}^{(i)})(\mathbf{A}^{ (i)}\mathbf{S}\mathbf{W}\mathbf{R}^{(i)})^{\dagger}\mathbf{A}^{(i)}-\mathbf{A} ^{(i)}\|_{F}^{2}\)._```
0:\(\mathbf{A}^{(i)}\in\mathbb{R}^{n_{i}\times d}\) for all \(i\in[\ell]\), rank parameter \(k>0\), accuracy parameter \(\varepsilon\in(0,1)\)
1: Generate Gaussian matrices \(\mathbf{G}\in\mathbb{R}^{n^{\prime}\times n},\mathbf{H}\in\mathbb{R}^{d\times d ^{\prime}}\) through Theorem 3.2
2: Let \(\mathbf{S}\in\mathbb{R}^{n^{\prime}\times t},\mathbf{Z}\in\mathbb{R}^{t\times d ^{\prime}}\) be the output of Theorem 3.3 on input \(\mathbf{GAH}\)
3: Let \(\mathbf{T}\in\mathbb{R}^{s\times n^{\prime}}\) be a Lewis weight sampling matrix for \(\mathbf{GAH}\mathbf{S}\mathbf{X}-\mathbf{GAH}\)
4: Let \(\widetilde{\mathbf{V}}\leftarrow(\mathbf{TGAHS})^{\dagger}(\mathbf{TGA})\)
5: Return \(\widetilde{\mathbf{V}}\) ```

**Algorithm 2**\((1+\varepsilon)\)-approximation for fair low-rank approximation

### Bicriteria Algorithm

To achieve polynomial time for our bicriteria algorithm, we can no longer use a polynomial system solver. Instead, we observe that for sufficiently large \(p\), we have \(\max\|\mathbf{x}\|_{\infty}=(1\pm\varepsilon)\|\mathbf{x}\|_{p}\). Thus, in place of optimizing \(\min_{\mathbf{V}\in\mathbb{R}^{k\times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i)} \mathbf{V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}\), we instead optimize \(\min_{\mathbf{V}\in\mathbb{R}^{k\times d}}\left(\sum_{i\in[\ell]}\|\mathbf{A} ^{(i)}\mathbf{V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}^{p}\right)^{1/p}\). However, the terms \(\|\mathbf{A}^{(i)}\mathbf{V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}^{p}\) are unwieldy to work with. Thus we instead use Dvoretzky's Theorem, i.e., Theorem 3.2, to embed \(L_{2}\) into \(L_{p}\), by generating matrices \(\mathbf{G}\) and \(\mathbf{H}\) so that \((1-\varepsilon)\|\mathbf{G}\mathbf{H}\mathbf{H}\|_{p}\leq\|\mathbf{M}\|_{F} \leq(1+\varepsilon)\|\mathbf{G}\mathbf{M}\mathbf{H}\|_{p}\), for all matrices \(\mathbf{M}\in\mathbb{R}^{n\times d}\).

Now, writing \(\mathbf{A}=\mathbf{A}^{(1)}\circ\ldots\circ\mathbf{A}^{(\ell)}\), it suffices to approximately solve \(\min_{\mathbf{X}\in\mathbb{R}^{k\times d}}\|\mathbf{GAH}\mathbf{S}\mathbf{X}- \mathbf{GAH}\|_{p}\). Unfortunately, low-rank approximation with \(L_{p}\) loss still cannot be approximated to \((1+\varepsilon)\)-factor in polynomial time, and in fact \(\mathbf{GAH}\) has dimension \(n^{\prime}\times d^{\prime}\) with \(n^{\prime}\geq n\) and \(d^{\prime}\geq d\). Hence, we first apply dimensionality reduction by appealing to a result of [13] showing that there exists a matrix \(\mathbf{S}\) that samples a "small" number of columns of \(\mathbf{A}\) to provide a coarse bicriteria approximation to \(L_{p}\) low-rank approximation. Now to lift the solution back to dimension \(d\), we would like to solve regression problem \(\min_{\mathbf{X}}\|\mathbf{GAH}\mathbf{S}\mathbf{X}-\mathbf{GAH}\|_{p}\). To that end, we consider a Lewis weight sampling matrix \(\mathbf{T}\) such that

\[\frac{1}{2}\|\mathbf{TGAHSX}-\mathbf{TGAH}\|_{p}\leq\|\mathbf{GAH}\mathbf{S} \mathbf{X}-\mathbf{GAH}\|_{p}\leq 2\|\mathbf{TGAHSX}-\mathbf{TGAH}\|_{p}.\]

We then note that \((\mathbf{TGAHS})^{\dagger}\mathbf{TGAH}\) is the minimizer of the problem \(\min_{\mathbf{x}}\|\mathbf{TGAHSX}-\mathbf{TGAH}\|_{F}\), which only provides a small distortion to the \(L_{p}\) regression problem, since \(\mathbf{TGAH}\) has a small number of rows due to the dimensionality reduction. By Dvoretzky's Theorem, we have that \((\mathbf{TGAHS})^{\dagger}\mathbf{TGA}\) is a "good" approximate solution to the original fair low-rank approximation problem. The algorithm appears in full in Algorithm 3.

```
0:\(\mathbf{A}^{(i)}\in\mathbb{R}^{n_{i}\times d}\) for all \(i\in[\ell]\), rank parameter \(k>0\), accuracy parameter \(\varepsilon\in(0,1)\)
0:\((1+\varepsilon)\)-approximation for fair LRA
1: Let \(\alpha\) be an \(\ell\)-approximation for the fair LRA problem
2: Let \(\mathbf{S}\) be generated from a random affine embedding distribution
3:while Algorithm 1 on input \(\mathbf{A}^{(1)},\ldots,\mathbf{A}^{(\ell)}\), \(\mathbf{S}\), and \(\alpha\) does not return \(\bot\)do
4: Let \(\mathbf{V}\) be the output of Algorithm 1 on input \(\mathbf{A}^{(1)},\ldots,\mathbf{A}^{(\ell)}\), \(\mathbf{S}\), and \(\alpha\)
5:\(\alpha\leftarrow\frac{\alpha}{1+\varepsilon}\)
6:endwhile
7: Return \(\mathbf{V}\) ```

**Algorithm 3** Bicriteria approximation for fair low-rank approximation

```
0:\(\mathbf{A}^{(i)}\in\mathbb{R}^{n_{i}\times d}\) for all \(i\in[\ell]\), rank parameter \(k>0\), trace-off parameter \(c\in(0,1)\)
0: Bicriteria approximation for fair LRA
1: Generate Gaussian matrices \(\mathbf{G}\in\mathbb{R}^{n^{\prime}\times n},\mathbf{H}\in\mathbb{R}^{d\times d ^{\prime}}\) through Theorem 3.2
2: Let \(\mathbf{S}\in\mathbb{R}^{n^{\prime}\times t},\mathbf{Z}\in\mathbb{R}^{t\times d ^{\prime}}\) be the output of Theorem 3.3 on input \(\mathbf{GAH}\)
3: Let \(\mathbf{T}\in\mathbb{R}^{s\times n^{\prime}}\) be a Lewis weight sampling matrix for \(\mathbf{GAH}\mathbf{S}\mathbf{X}-\mathbf{GAH}\)
4: Let \(\widetilde{\mathbf{V}}\leftarrow(\mathbf{TGAHS})^{\dagger}(\mathbf{TGA})\)
5: Return \(\widetilde{\mathbf{V}}\) ```

**Algorithm 4** Bicriteria approximation for fair LRA

```
0:\(\mathbf{A}^{(i)}\in\mathbb{R}^{n_{i}\times d}\) for all \(i\in[\ell]\), rank parameter \(k>0\), trace-off parameter \(c\in(0,1)\)
0: Bicriteria approximation for fair LRA
1: Generate Gaussian matrices \(\mathbf{G}\in\mathbb{R}^{n^{\prime}\times n},\mathbf{H}\in\mathbb{R}^{d\times d ^{\prime}}\) through Theorem 3.2
2: Let \(\mathbf{S}\in\mathbb{R}^{n^{\prime}\times t},\mathbf{Z}\in\mathbb{R}^{t\times d ^{\prime}}\) be the output of Theorem 3.

We use the following notion of Dvoretzky's theorem to embed the problem into entrywise \(L_{p}\) loss.

**Theorem 3.2** (Dvoretzky's Theorem, e.g., Theorem 1.2 in [11], Fact 15 in [12]).: _Let \(p\geq 1\) be a parameter and let_

\[m\gtrsim m(n,p,\varepsilon)=\begin{cases}\frac{p^{p}n}{\varepsilon^{2}},& \varepsilon\leq(Cp)^{\frac{p}{2}}n^{-\frac{p-2}{2(p-1)}}\\ \frac{(np)^{p/2}}{\varepsilon},&\varepsilon\in\left((Cp)^{\frac{p}{2}}n^{- \frac{p-2}{2(p-1)}},\frac{1}{p}\right]\\ \frac{n^{p/2}}{p^{p/2}\varepsilon^{p/2}}\log^{p/2}\frac{1}{\varepsilon},&\frac{ 1}{p}<\varepsilon<1.\end{cases}\]

_Then there exists a family \(\mathcal{G}\) of random scaled Gaussian matrices with dimension \(\mathbb{R}^{m\times n}\) such that for \(G\sim\mathcal{G}\), with probability at least \(1-\delta\), simultaneously for all \(\mathbf{y}\in\mathbb{R}^{n}\), \((1-\varepsilon)\|\mathbf{y}\|_{2}\leq\|\mathbf{G}\mathbf{y}\|_{p}\leq(1+ \varepsilon)\|\mathbf{y}\|_{2}\)._

We use the following algorithm from [11] to perform dimensionality reduction so that switching between \(L_{2}\) and \(L_{p}\) loss will incur smaller error. See also [10].

**Theorem 3.3** (Theorem 1.5 in [11]).: _Let \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and let \(k\geq 1\). Let \(s=\mathcal{O}\left(k\log\log k\right)\). Then there exists a polynomial-time algorithm that outputs a matrix \(\mathbf{S}\in\mathbb{R}^{d\times t}\) that samples \(t=\mathcal{O}\left(k(\log\log k)(\log^{2}d)\right)\) columns of \(\mathbf{A}\) and a matrix \(\mathbf{Z}\in\mathbb{R}^{t\times d}\) such that \(\|\mathbf{A}-\mathbf{A}\mathbf{S}\mathbf{Z}\|_{p}\leq 2^{p}\cdot\mathcal{O} \left(\sqrt{s}\right)\cdot\min_{\mathbf{U}\in\mathbb{R}^{n\times k},\mathbf{V }\in\mathbb{R}^{k\times d}}\|\mathbf{A}-\mathbf{U}\mathbf{V}\|_{p}\)._

We recall the following construction to use Lewis weights to achieve an \(L_{p}\) subspace embedding.

**Theorem 3.4** ([1]).: _Let \(\varepsilon\in(0,1)\) and \(p\geq 2\). Let \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and \(s=\mathcal{O}\left(d^{p/2}\log d\right)\). Then there exists a polynomial-time algorithm that outputs a matrix \(\mathbf{S}\in\mathbb{R}^{s\times n}\) that samples and reweights \(s\) rows of \(\mathbf{A}\), such that with probability at least \(0.99\), simultaneously for all \(\mathbf{x}\in\mathbb{R}^{d}\), \((1-\varepsilon)\|\mathbf{A}\mathbf{x}\|_{p}^{p}\leq\|\mathbf{S}\mathbf{A} \mathbf{x}\|_{p}^{p}\leq(1+\varepsilon)\|\mathbf{A}\mathbf{x}\|_{p}^{p}\)._

We then show that Algorithm 3 provides a bicriteria approximation.

**Lemma 3.5**.: _Let \(\widetilde{\mathbf{V}}\) be the output of Algorithm 3. Then with probability at least \(\frac{9}{10}\), \(\max_{i\in[\ell]}\|\mathbf{A}^{(i)}(\widetilde{\mathbf{V}})^{\dagger} \widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F}\) is at most \(\ell^{c}\cdot 2^{1/c}\cdot\mathcal{O}\left(k(\log\log k)(\log d)\right)\max_{i\in[ \ell]}\|\mathbf{A}^{(i)}(\widetilde{\mathbf{V}})^{\dagger}\widetilde{\mathbf{V }}-\mathbf{A}^{(i)}\|_{F}\), where \(c\) is the trade-off parameter input._

Since the generation of Gaussian matrices and the Lewis weight sampling matrix both only require polynomial time, it follows that our algorithm uses polynomial time overall. Hence, we have Theorem 1.4.

## 4 Socially Fair Column Subset Selection

In this section, we consider socially fair column subset selection, where the goal is to identify a matrix \(\mathbf{C}\in\mathbb{R}^{d\times k}\) that selects \(k\) columns to minimize \(\min_{\mathbf{C}\in\mathbb{R}^{d\times k},\|\mathbf{C}\|_{0}\leq k,\mathbf{B} ^{(i)}}\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{C}\mathbf{B}^{(i)}-\mathbf{ A}^{(i)}\|_{F}\).

```
0:\(\mathbf{A}^{(i)}\in\mathbb{R}^{n_{i}\times d}\) for all \(i\in[\ell]\), rank parameter \(k>0\), trade-off parameter \(c\in(0,1)\)
0: Bicriteria approximation for fair column subset selection
1:Acquire \(\widetilde{\mathbf{V}}\) from Algorithm 3
2:Generate Gaussian \(\mathbf{G}\in\mathbb{R}^{n^{\prime}\times n}\) through Theorem 3.2
3:Let \(\mathbf{S}\in\mathbb{R}^{d\times k^{\prime}}\) be a leverage score sampling matrix that samples \(k^{\prime}=\mathcal{O}\left(k\log k\right)\) columns of \(\widetilde{\mathbf{V}}\)
4:\(\mathbf{M}^{(i)}=\mathbf{S}^{\dagger}(\widetilde{\mathbf{V}})^{\dagger} \widetilde{\mathbf{V}}\) for all \(i\in[\ell]\)
5:Return \(\mathbf{A}^{(i)}\mathbf{S}\), \(\{\mathbf{M}^{(i)}\}\) ```

**Algorithm 4** Bicriteria approximation for fair column subset selection

We first provide preliminaries on leverage score sampling.

**Definition 4.1**.: _Given a matrix \(\mathbf{M}\in\mathbb{R}^{n\times d}\), we define the leverage score \(\sigma_{i}\) of each row \(\mathbf{m}_{i}\) with \(i\in[n]\) by \(\mathbf{m}_{i}(\mathbf{M}^{\dagger}\mathbf{M})^{-1}\mathbf{m}_{i}^{\top}\). Equivalently, for the singular value decomposition \(\mathbf{M}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}\), the leverage score of row \(\mathbf{m}_{i}\) is also the squared row norm of \(\mathbf{u}_{i}\)._We recall in Appendix C that it can be shown the sum of the leverage scores for an input matrix \(\mathbf{M}\in\mathbb{R}^{n\times d}\) is upper bounded by \(d\) and moreover, given the leverage scores of \(\mathbf{M}\), it suffices to sample only \(\mathcal{O}\left(d\log n\right)\) rows of \(\mathbf{M}\) to achieve a constant factor subspace embedding of \(\mathbf{M}\). Because the leverage scores of \(\mathbf{M}\) can be computed directly from the singular value decomposition of \(\mathbf{M}\), which can be computed in \(\mathcal{O}\left(nd^{\omega}+dn^{\omega}\right)\) time where \(\omega\) is the exponent of matrix multiplication, then the leverage scores of \(\mathbf{M}\) can be computed in polynomial time.

Finally, we recall that to provide a constant factor approximation to \(L_{p}\) regression, it suffices to compute a constant factor subspace embedding, e.g., through leverage score sampling. The proof is through the triangle inequality and is well-known among the active sampling literature [1, 1, 13, 14, 15, 16], e.g., a generalization of Lemma 2.1 in [13]. For completeness, we provide the proof in Appendix C.

**Lemma 4.2**.: _Given a matrix \(\mathbf{M}\in\mathbb{R}^{n\times d}\), let \(\mathbf{S}\) be a matrix such that for all \(\mathbf{x}\in\mathbb{R}^{d}\) and \(\mathbf{v}\in\mathbb{R}^{n}\),_

\[\frac{11}{12}\|\mathbf{M}\mathbf{x}\|_{2}\leq\|\mathbf{S}\mathbf{M}\mathbf{x }\|_{2}\leq\frac{13}{12}\|\mathbf{M}\mathbf{x}\|_{2},\ \mathbb{E}\left[\|\mathbf{S}\mathbf{v}\|_{2}^{2}\right]=\| \mathbf{v}\|_{2}^{2}.\]

_For a fixed \(\mathbf{B}\in\mathbb{R}^{n\times m}\) where \(\mathbf{B}=\mathbf{b}_{1}\circ\ldots\circ\mathbf{b}_{m}\) with \(\mathbf{b}_{i}\in\mathbb{R}^{n}\) for \(i\in[m]\), let \(\widetilde{\mathbf{x}_{i}}=(\mathbf{S}\mathbf{M})^{\dagger}(\mathbf{S}\mathbf{ b}_{i})\). Let \(\widetilde{\mathbf{X}}=\widetilde{\mathbf{x}_{1}}\circ\ldots\circ\widetilde{ \mathbf{x}_{m}}\). Then with probability at least \(0.97\), \(\|\mathbf{M}\mathbf{X}-\mathbf{B}\|_{2}\leq 99\min_{\mathbf{X}}\|\mathbf{M} \mathbf{X}-\mathbf{B}\|_{2}\)._

We now give the correctness guarantees of Algorithm 4.

**Lemma 4.3**.: _Let \(\mathbf{S},\mathbf{M}^{(1)},\ldots,\mathbf{M}^{(t)}\) be the output of Algorithm 4. Then with probability at least \(0.8\), \(\max_{i\in[t]}\|\mathbf{A}^{(t)}\mathbf{S}\mathbf{M}^{(i)}-\mathbf{A}^{(i)}\| _{F}\) is at most \(\ell^{\mathrm{c}}\cdot 2^{1/c}\cdot\mathcal{O}\left(k(\log\log k)(\log d) \right)\min_{\mathbf{V}\in\mathbb{R}^{k\times d}}\max_{i\in[t]}\|\mathbf{A}^{( i)}\mathbf{V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}\)._

We also have the following runtime guarantees on Algorithm 4.

**Lemma 4.4**.: _The runtime of Algorithm 4 is \(\mathrm{poly}(n,d)\)._

By combining Lemma 4.3 and Lemma 4.4, we have Theorem 1.5.

## 5 Empirical Evaluations

In this section, we describe our empirical evaluations for socially fair low-rank approximation on real-world datasets.

**Credit card dataset.** We used the Default of Credit Card Clients dataset [14], which has 30,000 observations across 23 features, including 17 numeric features. The dataset is a common human-centric data for experiments on fairness and was previously used as a benchmark by [14] for studies on fair PCA. The study collected information from various customers including multiple previous payment statements, previous payment delays, and upcoming bill statements, as well as if the customer was able to pay the upcoming bill statement or not, i.e., defaulted on the bill statement. The dataset was accessed through the UCI repository [14].

**Experimental setup.** For the purposes of reproducibility, our empirical evaluations were conducted using Python 3.10 using a 64-bit operating system on an AMD Ryzen 7 5700U CPU, with 8GB RAM and 8 cores with base clock 1.80 GHz. We compare our bicriteria algorithm from Algorithm 3 against the standard non-fair low-rank approximation algorithm that outputs the top \(k\) right singular vectors from the singular value decomposition. Gender was used as the sensitive attribute, so that all observations with one gender formed the matrix \(\mathbf{A}^{(1)}\) and the other observations formed the matrix \(\mathbf{A}^{(2)}\). As in Algorithm 3, we generate normalized Gaussian matrices \(\mathbf{G}\) and \(\mathbf{H}\) and then use \(L_{p}\) Lewis weight sampling to generate a matrix \(\mathbf{T}\). We generate matrices \(\mathbf{T}\), \(\mathbf{G}\), and \(\mathbf{H}\) with a small number of dimensions and thus do not compute the sampling matrix \(\mathbf{S}\) but instead use the full matrix. We first sampled a small number \(s\) of rows from \(\mathbf{A}^{(1)}\) and \(\mathbf{A}^{(2)}\) and compared our bicriteria algorithm to the standard non-fair low-rank approximation algorithm baseline. We plot the minimum ratio for \(s\in\{2,3,\ldots,21\}\), \(k=1\), and \(p=1\) over \(10,000\) iterations for each setup in Figure 0(a). Similarly, we plot both the minimum and average ratios for \(s=1000\), \(k\in\{1,2,\ldots,7,8\}\), and \(p=1\) over \(200\) iterations in Figure 0(b), where we permit the bicriteria solution to have rank \(2k\). Finally, we compare the runtimes of the algorithms in Figure 0(c), separating runtimes for our bicriteria into the total runtime bicrit1 that includes the process of generating the Gaussian matrices and performing the Lewis weight sampling, as well as the runtime bicrit2 for the step for extracting the factors

[MISSING_PAGE_FAIL:9]

## Acknowledgements

David P. Woodruff was supported in part by a Simons Investigator Award and NSF CCF-2335412. Samson Zhou is supported in part by NSF CCF-2335411. The work was conducted in part while Ali Vakilian, David P. Woodruff and Samson Zhou were visiting the Simons Institute for the Theory of Computing as part of the Sublinear Algorithms program.

## References

* [AAK\({}^{+}\)22] Jacob D. Abernethy, Pranjal Awasthi, Matthaus Kleindessner, Jamie Morgenstern, Chris Russell, and Jie Zhang. Active sampling for min-max fairness. In _International Conference on Machine Learning, ICML_, pages 53-65, 2022.
* [ABV21] Mohsen Abbasi, Aditya Bhaskara, and Suresh Venkatasubramanian. Fair clustering via equitable group representations. In _Proceedings of the 2021 ACM conference on fairness, accountability, and transparency_, pages 504-514, 2021.
* [ADW19] Alekh Agarwal, Miroslav Dudik, and Zhiwei Steven Wu. Fair regression: Quantitative definitions and reduction-based algorithms. In _International Conference on Machine Learning_, pages 120-129. PMLR, 2019.
* [AEK\({}^{+}\)20] Sara Ahmadian, Alessandro Epasto, Marina Knittel, Ravi Kumar, Mohammad Mahdian, Benjamin Moseley, Philip Pham, Sergei Vassilvitskii, and Yuyan Wang. Fair hierarchical clustering. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS_, 2020.
* [AKSZ22] Haris Angelidakis, Adam Kurpisz, Leon Sering, and Rico Zenklusen. Fair and fast k-center clustering for data summarization. In _International Conference on Machine Learning, ICML_, pages 669-702, 2022.
* [BCFN19] Suman Bera, Deeparnab Chakrabarty, Nicolas Flores, and Maryam Negahbani. Fair algorithms for clustering. _Advances in Neural Information Processing Systems_, 32, 2019.
* [BDM\({}^{+}\)20] Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco, Jalaj Upadhyay, David P. Woodruff, and Samson Zhou. Near optimal linear algebra in the online and sliding window models. In _61st IEEE Annual Symposium on Foundations of Computer Science, FOCS_, pages 517-528, 2020.
* [BG18] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In _Conference on Fairness, Accountability and Transparency, FAT_, volume 81 of _Proceedings of Machine Learning Research_, pages 77-91, 2018.
* [BGK00] Andreas Brieden, Peter Gritzmann, and Victor Klee. Inapproximability of some geometric and quadratic optimization problems. _Approximation and Complexity in Numerical Optimization: Continuous and Discrete Problems_, pages 96-115, 2000.
* [BGK\({}^{+}\)18] Ioana O Bercea, Martin Gross, Samir Khuller, Aounon Kumar, Clemens Rosner, Daniel R Schmidt, and Melanie Schmidt. On the cost of essentially fair clusterings. _arXiv preprint arXiv:1811.10319_, 2018.
* [BHJ\({}^{+}\)17] Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. A convex framework for fair regression. _arXiv preprint arXiv:1706.02409_, 2017.
* [BHJ\({}^{+}\)21] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in criminal justice risk assessments: The state of the art. _Sociological Methods & Research_, 50(1):3-44, 2021.
* [BIO\({}^{+}\)19] Arturs Backurs, Piotr Indyk, Krzysztof Onak, Baruch Schieber, Ali Vakilian, and Tal Wagner. Scalable fair clustering. In _International Conference on Machine Learning_, pages 405-413. PMLR, 2019.

* [BPR96] Saugata Basu, Richard Pollack, and Marie-Francoise Roy. On the combinatorial and algebraic complexity of quantifier elimination. _J. ACM_, 43(6):1002-1045, 1996.
* [BS16] Solon Barocas and Andrew D Selbst. Big data's disparate impact. _California law review_, pages 671-732, 2016.
* [BWZ19] Frank Ban, David P. Woodruff, and Qiuyi (Richard) Zhang. Regularized weighted low rank approximation. In _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, NeurIPS_, pages 4061-4071, 2019.
* [CDW18] Graham Cormode, Charlie Dickens, and David P. Woodruff. Leveraging well-conditioned bases: Streaming and distributed summaries in minkowski p-norms. In _Proceedings of the 35th International Conference on Machine Learning, ICML_, Proceedings of Machine Learning Research, pages 1048-1056, 2018.
* [CEM\({}^{+}\)15] Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimensionality reduction for \(k\)-means clustering and low rank approximation. In _Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC_, pages 163-172, 2015.
* [CGK\({}^{+}\)17] Flavio Chierichetti, Sreenivas Gollapudi, Ravi Kumar, Silvio Lattanzi, Rina Panigrahy, and David P. Woodruff. Algorithms for \(l_{p}\) low-rank approximation. In _Proceedings of the 34th International Conference on Machine Learning, ICML_, pages 806-814, 2017.
* [Cho17] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. _Big data_, 5(2):153-163, 2017.
* [CKLV17] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through fairlets. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems_, pages 5029-5037, 2017.
* [CMM17] Michael B. Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank approximation via ridge leverage score sampling. In _Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA_, pages 1758-1777, 2017.
* [CMV22] Eden Chlamtac, Yury Makarychev, and Ali Vakilian. Approximating fair clustering with cascaded norm objectives. In _Proceedings of the 2022 annual ACM-SIAM symposium on discrete algorithms (SODA)_, pages 2664-2683. SIAM, 2022.
* [CP15] Michael B. Cohen and Richard Peng. \(L_{p}\) row sampling by lewis weights. In _Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC_, pages 183-192, 2015.
* [CP19] Xue Chen and Eric Price. Active regression via linear-sample sparsification. In _Conference on Learning Theory, COLT_, pages 663-695, 2019.
* [CPF\({}^{+}\)17] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic decision making and the cost of fairness. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 797-806, 2017.
* [CXZC24] Wenjing Chen, Shuo Xing, Samson Zhou, and Victoria G. Crawford. Fair submodular cover. _CoRR_, abs/2407.04804, 2024.
* [DMM06a] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Subspace sampling and relative-error matrix approximation: Column-based methods. In _Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 9th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems, APPROX and 10th International Workshop on Randomization and Computation, RANDOM, Proceedings_, pages 316-326, 2006.

- ESA 2006, 14th Annual European Symposium, Proceedings_, pages 304-314, 2006.
* [DMV22] Zhen Dai, Yury Makarychev, and Ali Vakilian. Fair representation clustering with several protected classes. In _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 814-823, 2022.
* [DTV11] Amit Deshpande, Madhur Tulsiani, and Nisheeth K. Vishnoi. Algorithms and hardness for subspace approximation. In _Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA_, pages 482-496, 2011.
* [EBTD20] Seyed A. Esmaeili, Brian Brubach, Leonidas Tsepenekas, and John Dickerson. Probabilistic fair clustering. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS_, 2020.
* [EY36] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. _Psychometrika_, 1(3):211-218, 1936.
* [Fos53] Frederic G Foster. On the stochastic matrices associated with certain queuing processes. _The Annals of Mathematical Statistics_, 24(3):355-360, 1953.
* [GJ23] Dishant Goyal and Ragesh Jaiswal. Tight FPT approximation for socially fair clustering. _Information Processing Letters_, 182:106383, 2023.
* [GSV21] Mehrdad Ghadiri, Samira Samadi, and Santosh Vempala. Socially fair \(k\)-means clustering. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, pages 438-448, 2021.
* [GSV22] Mehrdad Ghadiri, Mohit Singh, and Santosh S Vempala. Constant-factor approximation algorithms for socially fair \(k\)-clustering. _arXiv preprint arXiv:2206.11210_, 2022.
* [HMV23] Sedjro Salomon Hotegni, Sepideh Mahabadi, and Ali Vakilian. Approximation algorithms for fair range clustering. In _International Conference on Machine Learning_, pages 13270-13284. PMLR, 2023.
* [HPS16] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In _Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems_, pages 3315-3323, 2016.
* [IP01] Russell Impagliazzo and Ramamohan Paturi. On the complexity of \(k\)-SAT. _J. Comput. Syst. Sci._, 62(2):367-375, 2001.
* [JKL20] Christopher Jung, Sampath Kannan, and Neil Lutz. Service in your neighborhood: Fairness in center location. In Aaron Roth, editor, _1st Symposium on Foundations of Responsible Computing, FORC 2020_, pages 5:1-5:15, 2020.
* [JLL\({}^{+}\)21] Shuli Jiang, Dennis Li, Irene Mengze Li, Arvind V. Mahankali, and David P. Woodruff. Streaming and distributed algorithms for robust column subset selection. In _Proceedings of the 38th International Conference on Machine Learning, ICML_, volume 139 of _Proceedings of Machine Learning Research_, pages 4971-4981, 2021.
* [JNN20] Matthew Jones, Huy Nguyen, and Thy Nguyen. Fair k-centers via maximum matching. In _International conference on machine learning_, pages 4940-4949. PMLR, 2020.
* [JPT13] Gabriela Jeronimo, Daniel Perrucci, and Elias P. Tsigaridas. On the minimum of a polynomial function on a basic closed semialgebraic set and applications. _SIAM J. Optim._, 23(1):241-255, 2013.
* [KAM19] Matthaus Kleindessner, Pranjal Awasthi, and Jamie Morgenstern. Fair k-center clustering for data summarization. In _Proceedings of the 36th International Conference on Machine Learning, ICML_, pages 3448-3457, 2019.

* [KDRZ23] Matthaus Kleindessner, Michele Donini, Chris Russell, and Muhammad Bilal Zafar. Efficient fair pca for fair representation learning. In _International Conference on Artificial Intelligence and Statistics_, pages 5250-5270. PMLR, 2023.
* [KLL\({}^{+}\)18] Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. Human decisions and machine predictions. _The quarterly journal of economics_, 133(1):237-293, 2018.
* [KMM15] Matthew Kay, Cynthia Matuszek, and Sean A. Munson. Unequal representation and gender stereotypes in image search results for occupations. In _Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI_, pages 3819-3828, 2015.
* [KMR17] Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determination of risk scores. In _8th Innovations in Theoretical Computer Science Conference, ITCS_, pages 43:1-43:23, 2017.
* [KPRW19] Ravi Kumar, Rina Panigrahy, Ali Rahimi, and David P. Woodruff. Faster algorithms for binary matrix factorization. In _Proceedings of the 36th International Conference on Machine Learning, ICML_, pages 3551-3559, 2019.
* [LKO\({}^{+}\)22] Junghyun Lee, Gwangsu Kim, Mahbod Olfat, Mark Hasegawa-Johnson, and Chang D Yoo. Fast and efficient mmd-based fair pca via optimization over stiefel manifold. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7363-7371, 2022.
* [Mag10] Malik Magdon-Ismail. Row sampling for matrix algorithms via a non-commutative bernstein bound. _CoRR_, abs/1008.0587, 2010.
* [MMM\({}^{+}\)22] Raphael A. Meyer, Cameron Musco, Christopher Musco, David P. Woodruff, and Samson Zhou. Fast regression for structured inputs. In _The Tenth International Conference on Learning Representations, ICLR_, 2022.
* [MMM\({}^{+}\)23] Raphael A. Meyer, Cameron Musco, Christopher Musco, David P. Woodruff, and Samson Zhou. Near-linear sample complexity for \(L_{p}\) polynomial regression. In _Proceedings of the 2023 ACM-SIAM Symposium on Discrete Algorithms, SODA_, pages 3959-4025, 2023.
* [MMWY22] Cameron Musco, Christopher Musco, David P. Woodruff, and Taisuke Yasuda. Active linear regression for \(L_{p}\) norms and beyond. In _63rd IEEE Annual Symposium on Foundations of Computer Science, FOCS_, pages 744-753, 2022.
* [MOT24] Antonis Matakos, Bruno Ordozgoiti, and Suhas Thejaswi. Fair column subset selection. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 2189-2199, 2024.
* [MV20] Sepideh Mahabadi and Ali Vakilian. Individual fairness for k-clustering. In _Proceedings of the 37th International Conference on Machine Learning, ICML_, pages 6586-6596, 2020.
* [MV21] Yury Makarychev and Ali Vakilian. Approximation algorithms for socially fair clustering. In _Conference on Learning Theory_, pages 3246-3264. PMLR, 2021.
* [NC21] Maryam Negahbani and Deeparnab Chakrabarty. Better algorithms for individually fair \(k\)-clustering. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS_, pages 13340-13351, 2021.
* [NNJ22] Huy Le Nguyen, Thy Nguyen, and Matthew Jones. Fair range k-center. _arXiv preprint arXiv:2207.11337_, 2022.
* [OA19] Matt Olfat and Anil Aswani. Convex formulations for fair principal component analysis. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 663-670, 2019.

* [PPP21] Aditya Parulekar, Advait Parulekar, and Eric Price. \(L_{1}\) regression with lewis weights subsampling. In _Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM_, pages 49:1-49:21, 2021.
* [PVZ17] Grigoris Paouris, Petros Valettas, and Joel Zinn. Random version of dvoretzky's theorem in \(\ell_{p}^{n}\). _Stochastic Processes and their Applications_, 127(10):3187-3227, 2017.
* [Ren92a] James Renegar. On the computational complexity and geometry of the first-order theory of the reals. part i: Introduction. preliminaries. the geometry of semi-algebraic sets. the decision problem for the existential theory of the reals. _Journal of symbolic computation_, 13(3):255-299, 1992.
* [Ren92b] James Renegar. On the computational complexity and geometry of the first-order theory of the reals. part ii: The general decision problem. preliminaries for quantifier elimination. _Journal of Symbolic Computation_, 13(3):301-327, 1992.
* [RS18] Clemens Rosner and Melanie Schmidt. Privacy preserving clustering with constraints. In _45th International Colloquium on Automata, Languages, and Programming, ICALP_, pages 96:1-96:14, 2018.
* [RSW16] Ilya P. Razenshteyn, Zhao Song, and David P. Woodruff. Weighted low rank approximations with provable guarantees. In _Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing, STOC_, pages 250-263, 2016.
* [STM\({}^{+}\)18] Samira Samadi, Uthaipon Tantipongpipat, Jamie H Morgenstern, Mohit Singh, and Santosh Vempala. The price of fair pca: One extra dimension. _Advances in neural information processing systems_, 31, 2018.
* [SW18] Christian Sohler and David P. Woodruff. Strong coresets for k-median and subspace approximation: Goodbye dimension. In _59th IEEE Annual Symposium on Foundations of Computer Science, FOCS_, pages 802-813, 2018.
* [TGOO22] Suhas Thejaswi, Ameet Gadekar, Bruno Ordozgoiti, and Michal Osadnik. Clustering with fair-center representation: Parameterized approximation algorithms and heuristics. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1749-1759, 2022.
* [TSS\({}^{+}\)19] Uthaipon Tantipongpipat, Samira Samadi, Mohit Singh, Jamie H Morgenstern, and Santosh Vempala. Multi-criteria dimensionality reduction with applications to fairness. _Advances in neural information processing systems_, 32, 2019.
* [VVWZ23] Ameya Velingker, Maximilian Votsch, David P. Woodruff, and Samson Zhou. Fast \((1+\epsilon)\)-approximation algorithms for binary matrix factorization. In _International Conference on Machine Learning, ICML_, pages 34952-34977, 2023.
* [VY22] Ali Vakilian and Mustafa Yalciner. Improved approximation algorithms for individually fair clustering. In _International conference on artificial intelligence and statistics_, pages 8758-8779. PMLR, 2022.
* [Woo14] David P. Woodruff. Sketching as a tool for numerical linear algebra. _Found. Trends Theor. Comput. Sci._, 10(1-2):1-157, 2014.
* [WY23] David P. Woodruff and Taisuke Yasuda. New subset selection algorithms for low rank approximation: Offline and online. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC_, pages 1802-1813, 2023.
* [Yeh16] I-Cheng Yeh. Default of Credit Card Clients. UCI Machine Learning Repository, 2016. DOI: https://doi.org/10.24432/C55S3H.
* [YL09] I-Cheng Yeh and Che-hui Lien. The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. _Expert systems with applications_, 36(2):2473-2480, 2009.

Preliminaries

We use the notation \([n]\) to represent the set \(\{1,\ldots,n\}\) for an integer \(n\geq 1\). We use the notation \(\operatorname{poly}(n)\) to represent a fixed polynomial in \(n\) and we use the notation \(\operatorname{polylog}(n)\) to represent \(\operatorname{poly}(\log n)\). We use \(\operatorname{poly}(n)\) to denote a fixed polynomial in \(n\) and \(\operatorname{polylog}(n)\) to denote \(\operatorname{poly}(\log n)\). We say an event holds with high probability if it holds with probability \(1-\frac{1}{\operatorname{poly}(n)}\).

We generally use bold-font variables to represent vectors and matrices, whereas we use default-font variables to represent scalars. For a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), we use \(\mathbf{A}_{i}\) to represent the \(i\)-th row of \(\mathbf{A}\) and \(\mathbf{A}^{(j)}\) to represent the \(j\)-th column of \(\mathbf{A}\). We use \(A_{i,j}\) to represent the entry in the \(i\)-th row and \(j\)-th column of \(\mathbf{A}\). For \(p\geq 1\), we use

\[\|\mathbf{A}\|_{p}=\left(\sum_{i\in[n]}\sum_{j\in[d]}A_{i,j}^{p}\right)^{1/p}\]

to represent the entrywise \(L_{p}\) norm of \(\mathbf{A}\) and we use

\[\|\mathbf{A}\|_{F}=\left(\sum_{i\in[n]}\sum_{j\in[d]}A_{i,j}^{2}\right)^{1/2}\]

to represent the Frobenius norm of \(\mathbf{A}\), which is simply the entrywise \(L_{2}\) norm of \(\mathbf{A}\). We use define the \(L_{p,q}\) of \(\mathbf{A}\) as the \(L_{p}\) norm of the vector consisting of the \(L_{q}\) norms of each row of \(\mathbf{A}\), so that

\[\|\mathbf{A}\|_{p,q}=\left(\sum_{i\in[n]}\left(\sum_{j\in[d]}(A_{i,j})^{q} \right)^{p/q}\right)^{1/p}.\]

Similarly, we use \(\|\mathbf{A}\|_{(p,q)}\) to denote the \(L_{p}\) norm of the vector consisting of the \(L_{q}\) norms of each column of \(\mathbf{A}\). Equivalently, we have \(\|\mathbf{A}\|_{(p,q)}=\|\mathbf{A}^{\top}\|_{p,q}\), so that

\[\|\mathbf{A}\|_{(p,q)}=\left(\sum_{j\in[d]}\left(\sum_{i\in[n]}(A_{i,j})^{q} \right)^{p/q}\right)^{1/p}.\]

We use \(\circ\) to represent vertical stacking of matrices, so that

\[\mathbf{A}^{(1)}\circ\ldots\circ\mathbf{A}^{(m)}=\begin{bmatrix}\mathbf{A}^{( 1)}\\ \vdots\\ \mathbf{A}^{(m)}\end{bmatrix}.\]

For a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) of rank \(k\), its singular decomposition is \(\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}\), where \(\mathbf{U}\in\mathbb{R}^{n\times k}\), \(\mathbf{\Sigma}\in\mathbb{R}^{k\times k}\), \(\mathbf{V}\in\mathbb{R}^{k\times d}\), so that the columns of \(\mathbf{U}\) are orthonormal and the rows of \(\mathbf{V}\) are orthonormal. The columns of \(\mathbf{U}\) are called the left singular vectors of \(\mathbf{A}\) and the rows of \(\mathbf{V}\) are called the right singular vectors of \(\mathbf{A}\).

### Regression and Low-Rank Approximation

In this section, we briefly describe some common techniques used to handle both regression and low-rank approximation, thus presenting multiple unified approaches for both problems. Thus in light of the abundance of techniques that can be used to handle both problems, it is somewhat surprising that socially fair regression and socially fair low-rank approximation exhibit vastly different complexities.

Closed form solutions.Given the regression problem \(\min_{\mathbf{x}\in\mathbb{R}^{d}}\|\mathbf{A}\mathbf{x}-\mathbf{b}\|_{2}\) for an input matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) and a label vector \(\mathbf{b}\in\mathbb{R}^{n}\), the closed form solution for the minimizer is \(\mathbf{A}^{\dagger}\mathbf{b}=\operatorname{argmin}_{\mathbf{x}\in\mathbb{R} ^{d}}\|\mathbf{A}\mathbf{x}-\mathbf{b}\|_{2}\), where \(\mathbf{A}^{\dagger}\) is the Moore-Penrose pseudoinverse of \(\mathbf{A}\). Specifically, for a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\) written in its singular value decomposition \(\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{v}\), we have \(\mathbf{V}^{\top}\mathbf{\Sigma}^{-1}\mathbf{U}^{\top}\). In particular, for if \(\mathbf{A}\) has linearly independent rows, then \(\mathbf{A}^{\dagger}=\mathbf{A}^{\top}(\mathbf{A}\mathbf{A}^{\top})^{-1}\). On the other hand, if \(\mathbf{A}\) has linearly independent columns, then \(\mathbf{A}^{\dagger}=(\mathbf{A}^{\top}\mathbf{A})^{-1}\mathbf{A}^{\top}\).

Similarly, given an input matrix \(\mathbf{A}\) and a rank parameter \(k>0\), there exists a closed form solution for the minimizer \(\operatorname*{argmin}_{\mathbf{V}\in\mathbb{R}^{k\times d}}\|\mathbf{A}- \mathbf{A}\mathbf{V}^{\dagger}\mathbf{V}\|_{F}^{2}\). Specifically, by the Eckart-Young-Mirsky theorem [1], the minimizer is the top \(k\) right singular vectors of \(\mathbf{A}\).

Dimensionality reduction.We next recall a unified set of dimensionality reduction techniques for both linear regression and low-rank approximation. We consider the "sketch-and-solve" paradigm, so that for both problems, we first acquire a low-dimension representation of the problem, and find the optimal solution in the low dimension using the above closed-form solutions. For "good" designs of the low-dimension representations, the low-dimension solution will also be near-optimal for the original problem.

We first observe that oblivious linear sketches serve as a common dimensionality reduction for both linear regression and low-rank approximation. For example, it is known [14] that there exists a family of Gaussian random matrices \(\mathcal{G}_{1}\) from which \(\mathbf{S}\sim\mathcal{G}_{1}\) satisfies with high probability,

\[(1-\varepsilon)\|\mathbf{S}\mathbf{A}\mathbf{x}-\mathbf{S}\mathbf{b}\|_{2} \leq\|\mathbf{A}\mathbf{x}-\mathbf{b}\|_{2}\leq(1+\varepsilon)\|\mathbf{S} \mathbf{A}\mathbf{x}-\mathbf{S}\mathbf{b}\|_{2},\]

simultaneously for all \(\mathbf{x}\in\mathbb{R}^{d}\). Similarly, there exists [14] a family of Gaussian random matrices \(\mathcal{G}_{2}\) from which \(\mathbf{S}\sim\mathcal{G}_{1}\) satisfies with high probability, that the row space of \(\mathbf{S}\mathbf{A}\) contains a \((1+\varepsilon)\)-approximation of the optimal low-rank approximation to \(\mathbf{A}\).

Alternatively, we can achieve dimensionality reduction for both linear regression and low-rank approximation by sampling a small subset of the input in related ways for both problems. For linear regression, we can generate a random matrix \(\mathbf{S}\) by sampling rows of \([\mathbf{A}\quad\mathbf{b}]\) by their leverage scores [13, 14, 15, 16]. In this manner, we again achieve a matrix \(\mathbf{S}\) such that with high probability,

\[(1-\varepsilon)\|\mathbf{S}\mathbf{A}\mathbf{x}-\mathbf{S}\mathbf{b}\|_{2} \leq\|\mathbf{A}\mathbf{x}-\mathbf{b}\|_{2}\leq(1+\varepsilon)\|\mathbf{S} \mathbf{A}\mathbf{x}-\mathbf{S}\mathbf{b}\|_{2},\]

simultaneously for all \(\mathbf{x}\in\mathbb{R}^{d}\). For low-rank approximation, we can generate a random matrix \(\mathbf{S}\) by sampling rows of \(\mathbf{A}\) with the related ridge-leverage scores [13]. Then with high probability, we have for all \(\mathbf{V}\in\mathbb{R}^{k\times d}\),

\[(1-\varepsilon)\|\mathbf{S}\mathbf{A}-\mathbf{S}\mathbf{A}\mathbf{V}^{\dagger }\mathbf{V}\|_{F}^{2}\leq\|\mathbf{A}-\mathbf{A}\mathbf{V}^{\dagger}\mathbf{V} \|_{F}^{2}\leq(1+\varepsilon)\|\mathbf{S}\mathbf{A}-\mathbf{S}\mathbf{A} \mathbf{V}^{\dagger}\mathbf{V}\|_{F}^{2}.\]

## Appendix B Missing Proofs from Section 3

### Lower Bound

We first show in Section B.1 that it is NP-hard to approximate fair low-rank approximation within any constant factor in polynomial time and moreover, under the exponential time hypothesis, it requires exponential time to achieve a constant factor approximation. We then give missing details from Section 3.1 and in Section 3.2.

Given points \(\mathbf{v}^{(1)},\dots,\mathbf{v}^{(n)}\in\mathbb{R}^{d}\), their outer \((d-k)\)-radius is defined as the minimum, over all \(k\)-dimensional linear subspaces, of the maximum Euclidean distance of these points to the subspace. We define this problem as \(\mathsf{Subspace}(k,\infty)\). It is known that it is NP-hard to approximate the \(\mathsf{Subspace}(n-1,\infty)\) problem within any constant factor:

**Theorem B.1** ([11, 12]).: _The \(\mathsf{Subspace}(n-1,\infty)\) problem is NP-hard to approximate within any constant factor._

Utilizing the NP-hardness of approximation of the \(\mathsf{Subspace}(n-1,\infty)\) problem, we show the NP-hardness of approximation of fair low-rank approximation.

**Theorem 1.1**.: _Fair low-rank approximation is NP-hard to approximate within any constant factor._

Proof.: Given an instance \(\mathbf{v}^{(1)},\dots,\mathbf{v}^{(n)}\in\mathbb{R}^{d}\) of \(\mathsf{Subspace}(n-1,\infty)\) with \(n<d\), we set \(\ell=k=n-1\) and \(\mathbf{A}^{(i)}=\mathbf{v}^{(i)}\) for all \(i\in[n]\). Then for a \(k\)-dimensional linear subspace \(\mathbf{V}\in\mathbb{R}^{k\times d}\), we have that \(\|\mathbf{A}^{(i)}\mathbf{V}^{\top}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}^{2}\) is the distance from \(\mathbf{v}^{(i)}\) to the subspace. Hence,\(\mathbf{A}^{(i)}\|_{F}^{2}\) is the maximum Euclidean distance of these points to the subspace and so the fair low-rank approximation problem is exactly \(\mathsf{Subspace}(n-1,\infty)\). By Theorem B.1, the \(\mathsf{Subspace}(n-1,\infty)\) problem is NP-hard to approximate within any constant factor. Thus, fair low-rank approximation is NP-hard to approximate within any constant factor. 

We next introduce a standard complexity assumption beyond NP-hardness. Recall that in the 3-SAT problem, the input is a Boolean satisfiability problem written in conjunctive normal form, consisting of \(n\) clauses, each with \(3\) literals, either a variable or the negation of a variable. The goal is to determine whether there exists a Boolean assignment to the variables to satisfy the formula.

**Hypothesis B.2** (Exponential time hypothesis [15]).: _The 3-SAT problem requires \(2^{\Omega(n)}\) runtime._

Observe that while NP-hardness simply conjectures that the 3-SAT problem cannot be solved in polynomial time, the exponential time hypothesis conjectures that the 3-SAT problem requires _exponential_ time.

We remark that in the context of Theorem B.1, [1] showed the hardness of approximation of \(\mathsf{Subspace}(n-1,\infty)\) through a reduction from the Max-Not-All-Equal-3-SAT problem, whose NP-hardness itself is shown through a reduction from 3-SAT. Thus under the exponential time hypothesis, Max-Not-All-Equal-3-SAT problem requires \(2^{\Omega(n)}\) to solve. Then it follows that:

**Theorem B.3** ([1, 1]).: _Assuming the exponential time hypothesis, then the \(\mathsf{Subspace}(n-1,\infty)\) problem requires \(2^{n^{\Omega(1)}}\) time to approximate within any constant factor._

It follows that under the exponential time hypothesis, any constant-factor approximation to socially fair low-rank approximation requires exponential time.

**Theorem 1.2**.: _Under the exponential time hypothesis, the fair low-rank approximation requires \(2^{k^{\Omega(1)}}\) time to approximate within any constant factor._

Proof.: Given an instance \(\mathbf{v}^{(1)},\ldots,\mathbf{v}^{(k)}\in\mathbb{R}^{d}\) of \(\mathsf{Subspace}(k-1,\infty)\) with \(k<d\), we set \(\ell=k-1\) and \(\mathbf{A}^{(i)}=\mathbf{v}^{(i)}\) for all \(i\in[k]\). Then for a \((k-1)\)-dimensional linear subspace \(\mathbf{V}\in\mathbb{R}^{(k-1)\times d}\), we have that \(\|\mathbf{A}^{(i)}\mathbf{V}^{\top}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}^{2}\) is the distance from \(\mathbf{v}^{(i)}\) to the subspace. Hence, \(\max_{i\in\ell}\|\mathbf{A}^{(i)}\mathbf{V}^{\top}\mathbf{V}-\mathbf{A}^{(i) }\|_{F}^{2}\) is the maximum Euclidean distance of these points to the subspace and so the fair low-rank approximation problem is exactly \(\mathsf{Subspace}(k-1,\infty)\). By Theorem B.3, the \(\mathsf{Subspace}(k-1,\infty)\) problem requires \(2^{k^{\Omega(1)}}\) time to approximate within any constant factor. Thus, fair low-rank approximation requires \(2^{k^{\Omega(1)}}\) time to approximate within any constant factor. 

### Missing Proofs from Section 3.1

We first recall the following result for polynomial system satisfiability solvers.

**Theorem B.4** ([14, 15, 16]).: _Given a polynomial system \(P(x_{1},\ldots,x_{n})\) over real numbers and \(m\) polynomial constraints \(f_{i}(x_{1},\ldots,x_{n})\otimes_{i}0\), where \(\otimes\in\{>,\geq,=,\neq,\leq,<\}\) for all \(i\in[m]\), let \(d\) denote the maximum degree of all the polynomial constraints and let \(B\) denote the maximum size of the bit representation of the coefficients of all the polynomial constraints. Then there exists an algorithm that determines whether there exists a solution to the polynomial system \(P\) in time \((md)^{\mathcal{O}(n)}\cdot\mathrm{poly}(B)\)._

To apply Theorem B.4, we utilize the following statement upper bounding the sizes of the bit representation of the coefficients of the polynomial constraints in our system.

**Theorem B.5** ([15]).: _Let \(\mathcal{T}=\{x\in\mathbb{R}^{n}\mid f_{1}(x)\geq 0,\ldots,f_{m}(x)\geq 0\}\) be defined by \(m\) polynomials \(f_{i}(x_{1},\ldots,x_{n})\) for \(i\in[m]\) with degrees bounded by an even integer \(d\) and coefficients of magnitude at most \(M\). Let \(\mathcal{C}\) be a compact connected component of \(\mathcal{T}\). Let \(g(x_{1},\ldots,x_{n})\) be a polynomial of degree at most \(d\) with integer coefficients of magnitude at most \(M\). Then the minimum nonzero magnitude that \(g\) takes over \(\mathcal{C}\) is at least \((2^{4-n/2}\widetilde{M}d^{n})^{-n2^{n}d^{n}}\), where \(\widetilde{M}=\max(M,2n+2m)\)._

To perform dimensionality reduction, recall the following definition of affine embedding.

[MISSING_PAGE_FAIL:18]

Since \(\mathbf{S}\) is an affine embedding for \(\mathbf{V}\), we have that for all \(\mathbf{X}^{(i)}\in\mathbb{R}^{n_{i}\times k}\),

\[(1-\varepsilon)\|\mathbf{X}^{(i)}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}^{2}\leq\| \mathbf{X}^{(i)}\mathbf{V}\mathbf{S}-\mathbf{A}^{(i)}\mathbf{S}\|_{F}^{2},\]

Therefore,

\[(1-\varepsilon)\min_{\mathbf{X}^{(i)}}\|\mathbf{X}^{(i)}\mathbf{V}-\mathbf{A}^ {(i)}\|_{F}^{2}\leq\min_{\mathbf{X}^{(i)}}\|\mathbf{X}^{(i)}\mathbf{V}\mathbf{S }-\mathbf{A}^{(i)}\mathbf{S}\|_{F}^{2}\]

From the above, we have

\[\|(\mathbf{Z}^{(i)}\mathbf{R}^{(i)})(\mathbf{Z}^{(i)}\mathbf{R}^{(i)})^{ \dagger}\mathbf{A}^{(i)}\mathbf{S}-\mathbf{A}^{(i)}\mathbf{S}\|_{F}^{2}=\min_{ \mathbf{X}^{(i)}}\|\mathbf{X}^{(i)}\mathbf{V}\mathbf{S}-\mathbf{A}^{(i)} \mathbf{S}\|_{F}^{2}.\]

Hence, putting these relations together,

\[\alpha <(1-\varepsilon)\mathsf{OPT}=(1-\varepsilon)\min_{\mathbf{X}^{( i)}}\|\mathbf{X}^{(i)}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}^{2}\] \[\leq\min_{\mathbf{X}^{(i)}}\|\mathbf{X}^{(i)}\mathbf{V}\mathbf{S }-\mathbf{A}^{(i)}\mathbf{S}\|_{F}^{2}\] \[=\|(\mathbf{Z}^{(i)}\mathbf{R}^{(i)})(\mathbf{Z}^{(i)}\mathbf{R} ^{(i)})^{\dagger}\mathbf{A}^{(i)}\mathbf{S}-\mathbf{A}^{(i)}\mathbf{S}\|_{F}^ {2},\]

as desired. 

We can thus utilize the structural property of Lemma3.1 by using the polynomial system solver in Algorithm1 on an affine embedding.

**Corollary B.8**.: _If \(\alpha\geq(1+\varepsilon)\cdot\mathsf{OPT}\), then Algorithm1 outputs a vector \(\mathbf{U}\in\mathbb{R}^{n\times k}\) such that_

\[\alpha\geq\|\mathbf{U}\mathbf{U}^{\dagger}\mathbf{A}^{(i)}-\mathbf{A}^{(i)}\| _{F}^{2}.\]

_If \(\alpha<(1-\varepsilon)\cdot\mathsf{OPT}\), then Algorithm1 outputs \(\bot\)_

Correctness of Algorithm2 then follows from CorollaryB.8 and binary search on \(\alpha\).

We now analyze the runtime of Algorithm2.

**Lemma B.9**.: _The runtime of Algorithm2 is at most \(\frac{1}{\varepsilon}\operatorname{poly}(n)\cdot(2\ell)^{\mathcal{O}(N)}\), for \(n=\sum_{i=1}^{\ell}n_{i}\) and \(N=\operatorname{poly}\left(\ell,k,\frac{1}{\varepsilon}\right)\)._

Proof.: Suppose the coefficients of \(\mathbf{A}^{(i)}\) are bounded in magnitude by \(2^{\operatorname{poly}(n)}\), where \(n=\sum_{i=1}^{\ell}n_{i}\). The number of variables in the polynomial system is at most

\[N:=2mk+\ell k^{2}=\operatorname{poly}\left(\ell,k,\frac{1}{\varepsilon}\right).\]

Each of the \(\mathcal{O}\left(\ell\right)\) polynomial constraints has degree at most \(20\). Thus by TheoremB.5, the minimum nonzero magnitude that any polynomial constraint takes over \(\mathcal{C}\) is at least \((2^{4-N/2}2^{\operatorname{poly}(n)}20^{N})^{-N2^{N}20^{N}}\). Hence, the maximum bit representation required is \(B=\operatorname{poly}(n)\cdot 2^{\mathcal{O}(N)}\). By TheoremB.4, the runtime of the polynomial system solver is at most \((\mathcal{O}\left(\ell\right)\cdot 20)^{\mathcal{O}(N)}\). \(\operatorname{poly}(B)=\operatorname{poly}(n)\cdot(2\ell)^{\mathcal{O}(N)}\). We require at most \(\mathcal{O}\left(\frac{1}{\varepsilon}\log\ell\right)\) iterations of the polynomial system solver. Thus, the total runtime is at most \(\frac{1}{\varepsilon}\operatorname{poly}(n)\cdot(2\ell)^{\mathcal{O}(N)}\). 

Putting things together, we have:

**Theorem 1.3**.: _Given an accuracy parameter \(\varepsilon\in(0,1)\), there exists an algorithm which outputs \(\widehat{\mathbf{V}}\in\mathbb{R}^{k\times d}\) such that with probability at least \(\frac{2}{3}\), \(\max_{i\in[\ell]}\|\mathbf{A}^{(i)}(\widetilde{\mathbf{V}})^{\dagger}\widehat{ \mathbf{V}}-\mathbf{A}^{(i)}\|_{F}\leq(1+\varepsilon)\cdot\min_{\mathbf{V}\in \mathbb{R}^{k\times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{V}^{\dagger} \mathbf{V}-\mathbf{A}^{(i)}\|_{F}\). The algorithm uses runtime \(\frac{1}{\varepsilon}\operatorname{poly}(n)\cdot(2\ell)^{\mathcal{O}(N)}\), for \(n=\sum_{i=1}^{\ell}n_{i}\) and \(N=\operatorname{poly}\left(\ell,k,\frac{1}{\varepsilon}\right)\)._

### Missing Proofs from Section3.2

**Lemma B.10**.: _Let \(\varepsilon\in(0,1)\) and \(\mathbf{x}\in\mathbb{R}^{\ell}\) and let \(p=\mathcal{O}\left(\frac{1}{\varepsilon}\log\ell\right)\). Then \(\|\mathbf{x}\|_{\infty}\leq\|\mathbf{x}\|_{p}\leq(1+\varepsilon)\|\mathbf{x}\| _{\infty}\)._

Proof.: Since it is clear that \(\|\mathbf{x}\|_{\infty}\leq\|\mathbf{x}\|_{p}\), then it remains to prove \(\|\mathbf{x}\|_{p}\leq(1+\varepsilon)\|\mathbf{x}\|_{\infty}\). Note that we have \(\|\mathbf{x}\|_{p}^{p}\leq\|\mathbf{x}\|_{\infty}^{p}\cdot\ell\). To achieve \(\ell^{1/p}\leq(1+\varepsilon)\), it suffices to have \(\frac{1}{p}\log\ell\leq\log(1+\varepsilon)\). Since \(\log(1+\varepsilon)=\mathcal{O}\left(\varepsilon\right)\) for \(\varepsilon\in(0,1)\), then for \(p=\mathcal{O}\left(\frac{1}{\varepsilon}\log\ell\right)\), we have \(\ell^{1/p}\leq(1+\varepsilon)\), and the desired claim follows. 

**Lemma 3.5**.: _Let \(\widetilde{\mathbf{V}}\) be the output of Algorithm3. Then with probability at least \(\frac{9}{10}\cdot\max_{i\in[\ell]}\|\mathbf{A}^{(i)}(\widetilde{\mathbf{V}}) ^{\dagger}\widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F}\) is at most \(\ell^{c}\cdot 2^{1/c}\cdot\mathcal{O}\left(k(\log\log k)(\log d)\right)\max_{i\in[ \ell]}\|\mathbf{A}^{(i)}(\widetilde{\mathbf{V}})^{\dagger}\widetilde{\mathbf{ V}}-\mathbf{A}^{(i)}\|_{F}\), where \(c\) is the trade-off parameter input._

Proof.: We have

\[\max_{i\in[\ell]}\|\mathbf{A}^{(i)}(\widetilde{\mathbf{V}})^{\dagger} \widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F}\leq\left(\sum_{i\in[\ell]}\| \mathbf{A}^{(i)}(\widetilde{\mathbf{V}})^{\dagger}\widetilde{\mathbf{V}}- \mathbf{A}^{(i)}\|_{F}^{p}\right)^{1/p}.\]

Let \(\mathbf{A}=\mathbf{A}^{(1)}\circ\ldots\circ\mathbf{A}^{(\ell)}\) and let

\[\widetilde{\mathbf{U}}:=\mathbf{A}\mathbf{H}\mathbf{S}.\]

For \(i\in[\ell]\), let \(\widetilde{\mathbf{U}^{(i)}}\) be the matrix of \(\widetilde{\mathbf{U}}\) whose rows correspond with the rows of \(\mathbf{A}^{(i)}\) in \(\mathbf{A}\), i.e., let \(\widetilde{\mathbf{U}^{(i)}}\) be the \(i\)-th block of rows of \(\widetilde{\mathbf{U}}\).

By the optimality of \(\mathbf{A}^{(i)}(\widetilde{\mathbf{V}})^{\dagger}\) with respect to the Frobenius norm, we have

\[\left(\sum_{i\in[\ell]}\|\mathbf{A}^{(i)}(\widetilde{\mathbf{V}})^{\dagger} \widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F}^{p}\right)^{1/p}\leq\left(\sum_ {i\in[\ell]}\|\widetilde{\mathbf{U}^{(i)}}\widetilde{\mathbf{V}}-\mathbf{A}^ {(i)}\|_{F}^{p}\right)^{1/p}\]

By Dvoretzky's Theorem, Theorem3.2, with distortion \(\varepsilon=\Theta(1)\), we have that with probability at least \(0.99\),

\[\left(\sum_{i\in[\ell]}\|\widetilde{\mathbf{U}^{(i)}}\widetilde{\mathbf{V}}- \mathbf{A}^{(i)}\|_{F}^{p}\right)^{1/p}\leq 2\left(\sum_{i\in[\ell]}\| \mathbf{G}\widetilde{\mathbf{U}^{(i)}}\widetilde{\mathbf{V}}\mathbf{H}- \mathbf{G}\mathbf{A}^{(i)}\mathbf{H}\|_{p}^{p}\right)^{1/p},\]

where we use \(\|\cdot\|_{p}\) to denote the entry-wise \(p\)-norm. Writing \(\widetilde{\mathbf{U}}=\widetilde{\mathbf{U}^{(1)}}\circ\ldots\circ\widetilde{ \mathbf{U}^{(\ell)}}\), then we have

\[\left(\sum_{i\in[\ell]}\|\widetilde{\mathbf{G}}\widetilde{\mathbf{U}^{(i)}} \widetilde{\mathbf{V}}\mathbf{H}-\mathbf{G}\mathbf{A}^{(i)}\mathbf{H}\|_{p}^{ p}\right)^{1/p}=\|\mathbf{G}\widetilde{\mathbf{U}}\widetilde{\mathbf{V}} \mathbf{H}-\mathbf{G}\mathbf{A}\mathbf{H}\|_{p}.\]

By the choice of the Lewis weight sampling matrix \(\mathbf{T}\), we have that with probability \(0.99\),

\[\|\mathbf{G}\widetilde{\mathbf{U}}\widetilde{\mathbf{V}}\mathbf{H} -\mathbf{G}\mathbf{A}\mathbf{H}\|_{p} \leq 2\|\mathbf{T}\mathbf{G}\widetilde{\mathbf{U}}\widetilde{ \mathbf{V}}\mathbf{H}-\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\|_{p}\] \[\leq 2\|\mathbf{T}\mathbf{G}\widetilde{\mathbf{U}}\widetilde{ \mathbf{V}}\mathbf{H}-\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\|_{(p,2)}\] \[=2\|\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\mathbf{S}(\mathbf{ T}\mathbf{G}\mathbf{A}\mathbf{H})^{\dagger}(\mathbf{T}\mathbf{G}\mathbf{A}) \mathbf{H}-\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\|_{(p,2)},\]

where \(\|\mathbf{M}\|_{(p,2)}\) denotes the \(L_{p}\) norm of the vector consisting of the \(L_{2}\) norms of the columns of \(\mathbf{M}\), and the last equality follows due to our setting of \(\widetilde{\mathbf{U}}=\mathbf{A}\mathbf{H}\mathbf{S}\) and \(\widetilde{\mathbf{V}}\leftarrow(\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H} \mathbf{S})^{\dagger}(\mathbf{T}\mathbf{G}\mathbf{A})\), the latter in Algorithm3. By optimality of \((\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H}\mathbf{S})^{\dagger}(\mathbf{T} \mathbf{G}\mathbf{A})\mathbf{H}\) for the choice of \(\mathbf{X}\) in the minimization problem

\[\min_{\mathbf{X}\in\mathbb{R}^{t\times d^{\prime}}}\|\mathbf{T}\mathbf{G} \mathbf{A}\mathbf{H}\mathbf{S}\mathbf{X}-\mathbf{T}\mathbf{G}\mathbf{A}\mathbf{H} \|_{(p,2)},\]we have

\[\|\mathbf{TGAHS}(\mathbf{TGAHS})^{\dagger}(\mathbf{TGA})\mathbf{H}-\mathbf{TGA }\mathbf{H}\|_{(p,2)}=\min_{\mathbf{X}\in\mathbb{R}^{t\times d^{\prime}}}\| \mathbf{TGAHSX}-\mathbf{TGA}\|_{(p,2)}.\]

Since \(\mathbf{S}\in\mathbb{R}^{n^{\prime}\times t}\) and \(\mathbf{T}\) is a Lewis weight sampling matrix for \(\mathbf{GAHSX}-\mathbf{GAH}\), then \(\mathbf{T}\) has \(t\) rows, where \(t=\mathcal{O}\left(k(\log\log k)(\log^{2}d)\right)\) by Theorem3.3. Thus, each column of \(\mathbf{TGAH}\) has \(t\) entries, so that

\[\min_{\mathbf{X}\in\mathbb{R}^{t\times d^{\prime}}}\|\mathbf{TGAHSX}-\mathbf{TGA}\mathbf{H}\|_{(p,2)}\leq\sqrt{t}\min_{\mathbf{X}\in\mathbb{R}^{t \times d^{\prime}}}\|\mathbf{TGAHSX}-\mathbf{TGA}\mathbf{H}\|_{p}.\]

By the choice of the Lewis weight sampling matrix \(\mathbf{T}\), we have that with probability \(0.99\),

\[\min_{\mathbf{X}\in\mathbb{R}^{t\times d^{\prime}}}\|\mathbf{TGAHSX}-\mathbf{TGA}\mathbf{H}\|_{p}\leq 2\min_{\mathbf{X}\in \mathbb{R}^{t\times d^{\prime}}}\|\mathbf{GAHSX}-\mathbf{GAH}\|_{p}.\]

By Theorem3.3, we have that with probability \(0.99\),

\[\min_{\mathbf{X}\in\mathbb{R}^{t\times d^{\prime}}}\|\mathbf{GAHSX}-\mathbf{ GAH}\|_{p}\leq 2^{p}\cdot\mathcal{O}\left(\sqrt{s}\right)\cdot\min_{\mathbf{U}\in \mathbb{R}^{n^{\prime}\times t},\mathbf{V}\in\mathbb{R}^{t\times d^{\prime}}} \|\mathbf{UV}-\mathbf{GAH}\|_{p},\]

for \(s=\mathcal{O}\left(k\log\log k\right)\). Let \(\mathbf{V}^{*}=\operatorname*{argmin}_{\mathbf{V}\in\mathbb{R}^{k\times d}} \max_{i\in[\ell]}\|\mathbf{A}^{(i)}-\mathbf{A}^{(i)}\mathbf{V}^{\dagger} \mathbf{V}\|_{p}\). Then since \(\mathbf{UV}\) has rank \(t\) with \(t\geq k\), we have

\[\min_{\mathbf{U}\in\mathbb{R}^{n^{\prime}\times t},\mathbf{V}\in \mathbb{R}^{t\times d^{\prime}}}\|\mathbf{UV}-\mathbf{GAH}\|_{p} \leq\|\mathbf{GA}(\mathbf{V}^{*})^{\dagger}\mathbf{V}^{*} \mathbf{H}-\mathbf{GAH}\|_{p}\] \[=\left(\sum_{i\in[\ell]}\|\mathbf{GA}^{(i)}(\mathbf{V}^{*})^{ \dagger}\mathbf{V}^{*}\mathbf{H}-\mathbf{GA}^{(i)}\mathbf{H}\|_{p}^{p}\right)^ {1/p}.\]

By Dvoretzky's Theorem, Theorem3.2, with distortion \(\varepsilon=\Theta(1)\), we have that with probability at least \(0.99\),

\[\left(\sum_{i\in[\ell]}\|\mathbf{GA}^{(i)}(\mathbf{V}^{*})^{\dagger}\mathbf{V }^{*}\mathbf{H}-\mathbf{GA}^{(i)}\mathbf{H}\|_{p}^{p}\right)^{1/p}\leq 2\left( \sum_{i\in[\ell]}\|\mathbf{A}^{(i)}(\mathbf{V}^{*})^{\dagger}\mathbf{V}^{*}- \mathbf{A}^{(i)}\|_{F}^{p}\right)^{1/p}.\]

For \(p=c\log\ell\) with \(c<1\), we have

\[\left(\sum_{i\in[\ell]}\|\mathbf{A}^{(i)}(\mathbf{V}^{*})^{\dagger}\mathbf{V} ^{*}-\mathbf{A}^{(i)}\|_{F}^{p}\right)^{1/p}\leq 2^{1/c}\max_{i\in[\ell]}\| \mathbf{A}^{(i)}(\mathbf{\tilde{V}})^{\dagger}\mathbf{\tilde{V}}-\mathbf{A}^{( i)}\|_{F}.\]

Putting together these inequalities successively, we ultimately have

\[\max_{i\in[\ell]}\|\mathbf{A}^{(i)}(\mathbf{\tilde{V}})^{\dagger}\mathbf{ \tilde{V}}-\mathbf{A}^{(i)}\|_{F}\leq 2^{p}\cdot 2^{1/c}\cdot\mathcal{O}\left( \sqrt{st}\right)\max_{i\in[\ell]}\|\mathbf{A}^{(i)}(\mathbf{\tilde{V}})^{ \dagger}\mathbf{\tilde{V}}-\mathbf{A}^{(i)}\|_{F},\]

for \(p=c\log\ell\), \(s=\mathcal{O}\left(k\log\log k\right)\), and \(t=\mathcal{O}\left(k(\log\log k)(\log^{2}d)\right)\). Therefore, we have

\[\max_{i\in[\ell]}\|\mathbf{A}^{(i)}(\mathbf{\tilde{V}})^{\dagger}\mathbf{ \tilde{V}}-\mathbf{A}^{(i)}\|_{F}\leq\ell^{c}\cdot 2^{1/c}\cdot\mathcal{O}\left(k(\log \log k)(\log d)\right)\max_{i\in[\ell]}\|\mathbf{A}^{(i)}(\mathbf{\tilde{V}})^{ \dagger}\mathbf{\tilde{V}}-\mathbf{A}^{(i)}\|_{F}.\]

## Appendix C Missing Proofs from Section4

It is known that the sum of the leverage scores of the rows of a matrix can be bounded by the rank of the matrix.

**Theorem C.1** (Generalization of Foster's Theorem, [11]).: _Given a matrix \(\mathbf{M}\in\mathbb{R}^{n\times d}\), the sum of its leverage scores is \(\operatorname{rank}(\mathbf{M})\)._

By sampling rows proportional to their leverage scores, we can obtain a subspace embedding as follows:

**Theorem C.2** (Leverage score sampling).: _[_DMM06a, DMM06b, Mag10, Woo14_]_ _Given a matrix \(\mathbf{M}\in\mathbb{R}^{n\times d}\), let \(\sigma_{i}\) be the leverage score of the \(i\)-th row of \(\mathbf{M}\). Suppose \(p_{i}=\min\left(1,\sigma_{i}\log n\right)\) for each \(i\in[n]\) and let \(\mathbf{S}\) be a random diagonal matrix so that the \(i\)-th diagonal entry of \(\mathbf{S}\) is \(\frac{1}{\sqrt{p_{i}}}\) with probability \(p_{i}\) and \(0\) with probability \(1-p_{i}\). Then for all vectors \(\mathbf{v}\in\mathbb{R}^{n}\),_

\[\mathbb{E}\left[\|\mathbf{S}\mathbf{v}\|_{2}^{2}\right]=\|\mathbf{v}\|_{2}^{2}\]

_and with probability at least \(0.99\), for all vectors \(\mathbf{x}\in\mathbb{R}^{d}\)_

\[\frac{99}{100}\|\mathbf{M}\mathbf{x}\|_{2}\leq\|\mathbf{S}\mathbf{M}\mathbf{x }\|_{2}\leq\frac{101}{100}\|\mathbf{M}\mathbf{x}\|_{2}.\]

_Moreover, \(\mathbf{S}\) has at most \(\mathcal{O}\left(d\log n\right)\) nonzero entries with high probability._

**Lemma 4.2**.: _Given a matrix \(\mathbf{M}\in\mathbb{R}^{n\times d}\), let \(\mathbf{S}\) be a matrix such that for all \(\mathbf{x}\in\mathbb{R}^{d}\) and \(\mathbf{v}\in\mathbb{R}^{n}\),_

\[\frac{11}{12}\|\mathbf{M}\mathbf{x}\|_{2}\leq\|\mathbf{S}\mathbf{M}\mathbf{x }\|_{2}\leq\frac{13}{12}\|\mathbf{M}\mathbf{x}\|_{2},\ \mathbb{E}\left[\|\mathbf{S}\mathbf{v}\|_{2}^{2}\right]=\|\mathbf{v}\|_{2}^{2}.\]

_For a fixed \(\mathbf{B}\in\mathbb{R}^{n\times m}\) where \(\mathbf{B}=\mathbf{b}_{1}\circ\ldots\circ\mathbf{b}_{m}\) with \(\mathbf{b}_{i}\in\mathbb{R}^{n}\) for \(i\in[m]\), let \(\widetilde{\mathbf{x}}_{i}=(\mathbf{S}\mathbf{M})^{\dagger}(\mathbf{S}\mathbf{ b}_{i})\). Let \(\widetilde{\mathbf{X}}=\widetilde{\mathbf{x}}_{1}\circ\ldots\circ\widetilde{ \mathbf{x}}_{m}\). Then with probability at least \(0.97\), \(\|\mathbf{M}\widetilde{\mathbf{X}}-\mathbf{B}\|_{2}\leq 99\min_{\mathbf{X}}\| \mathbf{M}\mathbf{X}-\mathbf{B}\|_{2}\)._

Proof.: Let \(\mathbf{X}^{*}=\operatorname*{argmin}_{\mathbf{X}}\|\mathbf{M}\mathbf{X}- \mathbf{B}\|_{2}\) and \(\mathsf{OPT}=\|\mathbf{M}\mathbf{X}^{*}-\mathbf{B}\|_{2}\). By triangle inequality,

\[\|\mathbf{S}\mathbf{M}\mathbf{X}-\mathbf{S}\mathbf{B}\|_{2}\geq\|\mathbf{S} \mathbf{M}(\mathbf{X}-\mathbf{X}^{*})\|_{2}-\|\mathbf{S}\mathbf{M}\mathbf{X}^ {*}-\mathbf{S}\mathbf{B}\|_{2}.\]

We have

\[\frac{99}{100}\|\mathbf{M}\mathbf{X}\|_{2}\leq\|\mathbf{S}\mathbf{M}\mathbf{X }\|_{2}\leq\frac{101}{100}\|\mathbf{M}\mathbf{X}\|_{2}\]

for all \(\mathbf{X}\in\mathbb{R}^{n\times m}\). Thus,

\[\|\mathbf{S}\mathbf{M}\mathbf{X}-\mathbf{S}\mathbf{B}\|_{2}\geq\frac{99}{100} \|\mathbf{M}(\mathbf{X}-\mathbf{X}^{*})\|_{2}-\|\mathbf{S}\mathbf{M}\mathbf{X }^{*}-\mathbf{S}\mathbf{B}\|_{2}.\]

By triangle inequality,

\[\|\mathbf{S}\mathbf{M}\mathbf{X}-\mathbf{S}\mathbf{B}\|_{2}\geq\frac{99}{100} \left(\|\mathbf{M}\mathbf{X}-\mathbf{B}\|_{2}-\|\mathbf{M}\mathbf{X}^{*}- \mathbf{B}\|_{2}\right)-\|\mathbf{S}\mathbf{M}\mathbf{X}^{*}-\mathbf{S}\mathbf{ B}\|_{2}.\]

Since \(\mathbb{E}\left[\|\mathbf{S}\mathbf{v}\|_{2}^{2}\right]=\|\mathbf{v}\|_{2}^{2}\) for all \(\mathbf{x}\in\mathbb{R}^{d}\), then by concavity and Markov's inequality, we have that

\[\mathbf{Pr}\left[\|\mathbf{S}\mathbf{M}\mathbf{X}^{*}-\mathbf{S}\mathbf{B}\|_ {2}\geq 49\|\mathbf{M}\mathbf{X}^{*}-\mathbf{B}\|_{2}\right]\leq\frac{1}{49}.\]

Thus with probability at least \(0.97\),

\[\|\mathbf{S}\mathbf{M}\mathbf{X}-\mathbf{S}\mathbf{B}\|_{2}\geq\frac{99}{100} \left(\|\mathbf{M}\mathbf{X}-\mathbf{B}\|_{2}-\|\mathbf{M}\mathbf{X}^{*}- \mathbf{B}\|_{2}\right)-49\|\mathbf{M}\mathbf{X}^{*}-\mathbf{B}\|_{2}.\]

Now since we have \(\|\mathbf{S}\mathbf{M}\mathbf{X}^{*}-\mathbf{S}\mathbf{B}\|_{2}\leq 49\|\mathbf{M} \mathbf{X}^{*}-\mathbf{B}\|_{2}\) and \(\|\mathbf{S}\mathbf{M}\widetilde{\mathbf{X}}-\mathbf{S}\mathbf{B}\|_{2}\leq\| \mathbf{S}\mathbf{M}\mathbf{X}^{*}-\mathbf{S}\mathbf{B}\|_{2}\), then

\[49\|\mathbf{M}\mathbf{X}^{*}-\mathbf{B}\|_{2}\geq\frac{99}{100}\left(\| \mathbf{M}\widetilde{\mathbf{X}}-\mathbf{B}\|_{2}-\|\mathbf{M}\mathbf{X}^{*}- \mathbf{B}\|_{2}\right)-49\|\mathbf{M}\mathbf{X}^{*}-\mathbf{B}\|_{2},\]

so that

\[\|\mathbf{M}\widetilde{\mathbf{X}}-\mathbf{B}\|_{2}\leq 99\|\mathbf{M} \mathbf{X}^{*}-\mathbf{B}\|_{2},\]

as desired. 

**Lemma 4.3**.: _Let \(\mathbf{S},\mathbf{M}^{(1)},\ldots,\mathbf{M}^{(\ell)}\) be the output of Algorithm 4. Then with probability at least \(0.8\), \(\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{S}\mathbf{M}^{(i)}-\mathbf{A}^{(i)}\| _{F}\) is at most \(\ell^{c}\cdot 2^{1/c}\cdot\mathcal{O}\left(k(\log\log k)(\log d)\right)\min_{ \mathbf{V}\in\mathbb{R}^{k\times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{ V}^{\dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}\)._Proof.: Let \(\widetilde{\mathbf{V}}\in\mathbb{R}^{t\times d}\) be the output of Algorithm 3, where \(t=\mathcal{O}\left(k(\log\log k)(\log^{2}d)\right)\). Then with probability at least \(\frac{2}{3}\), we have

\[\max_{i\in[\ell]}\|\mathbf{A}^{(i)}(\widetilde{\mathbf{V}})^{ \dagger}\widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F}\leq\ell^{c}\cdot 2^{1/c} \cdot\mathcal{O}\left(k(\log\log k)(\log d)\right)\min_{\mathbf{V}\in\mathbb{R }^{k\times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{V}^{\dagger}\mathbf{V }-\mathbf{A}^{(i)}\|_{F}.\]

Therefore, we have

\[\max_{i\in[\ell]}\min_{\mathbf{B}^{(i)}}\|\mathbf{B}^{(i)} \widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F}\leq\ell^{c}\cdot 2^{1/c} \cdot\mathcal{O}\left(k(\log\log k)(\log d)\right)\min_{\mathbf{V}\in\mathbb{R }^{k\times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{V}^{\dagger}\mathbf{V }-\mathbf{A}^{(i)}\|_{F}.\]

Let \(p\) be a sufficiently large parameter to be fixed. By Dvoretzky's theorem, i.e., Theorem 3.2, with \(\varepsilon=\mathcal{O}\left(1\right)\), we have

\[\max_{i\in[\ell]}\min_{\mathbf{B}^{(i)}}\|\mathbf{G}^{(i)}\mathbf{B}^{(i)} \widetilde{\mathbf{V}}-\mathbf{G}^{(i)}\mathbf{A}^{(i)}\|_{p,2}\leq\mathcal{O }\left(1\right)\cdot\min_{i\in[\ell]}\min_{\mathbf{B}^{(i)}}\|\mathbf{B}^{(i) }\widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F}.\]

For sufficiently large \(p=\mathcal{O}\left(\log\ell\right)\), we have by Lemma B.10,

\[\max_{i\in[\ell]}\min_{\mathbf{B}^{(i)}}\|\mathbf{B}^{(i)} \widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F}\cdot\leq\mathcal{O}\left(1 \right)\cdot\min_{\mathbf{B}^{(i)}}\|\mathbf{G}^{(i)}\mathbf{B}^{(i)} \widetilde{\mathbf{V}}-\mathbf{G}^{(i)}\mathbf{A}^{(i)}\|_{p,2}.\]

By a change of variables, we have

\[\min_{\mathbf{X}}\|\mathbf{X}\widetilde{\mathbf{V}}-\mathbf{G} \mathbf{A}\|_{p,2}\leq\min_{\mathbf{B}}\|\mathbf{G}\mathbf{B}\widetilde{ \mathbf{V}}-\mathbf{G}\mathbf{A}\|_{p,2}.\]

Note that minimizing \(\|\mathbf{X}\widetilde{\mathbf{V}}-\mathbf{G}\mathbf{A}\|_{p,2}\) over all \(\mathbf{X}\) corresponds to minimizing \(\|\mathbf{X}_{i}\widetilde{\mathbf{V}}-\mathbf{G}_{i}\mathbf{A}\|_{2}\) over all \(i\in[n^{\prime}]\). However, an arbitrary choice of \(\mathbf{X}_{i}\) may not correspond to selecting columns of \(\mathbf{A}\). Thus we apply a leverage score sampling matrix \(\mathbf{S}\) to sample columns of \(\widetilde{\mathbf{V}}\) which will correspondingly sample columns of \(\mathbf{G}\mathbf{A}\). Then by Lemma 4.2, we have

\[\min_{\mathbf{X}}\|\mathbf{X}\widetilde{\mathbf{V}}\mathbf{S}- \mathbf{G}\mathbf{A}\mathbf{S}\|_{p,2}\leq\mathcal{O}\left(1\right)\cdot\min_{ \mathbf{X}}\|\mathbf{X}\widetilde{\mathbf{V}}-\mathbf{G}\mathbf{A}\|_{p,2}.\]

Putting these together, we have

\[\min_{\mathbf{X}}\|\mathbf{X}\widetilde{\mathbf{V}}\mathbf{S}- \mathbf{G}\mathbf{A}\mathbf{S}\|_{p,2}\leq\ell^{c}\cdot 2^{1/c}\cdot \mathcal{O}\left(k(\log\log k)(\log d)\right)\min_{\mathbf{V}\in\mathbb{R}^{k \times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{V}^{\dagger}\mathbf{V}- \mathbf{A}^{(i)}\|_{F}.\] (1)

Let \(S\) be the selected columns from Algorithm 4 by \(\mathbf{S}\) and note that \(\mathbf{A}^{(i)}\mathbf{S}\mathbf{S}^{\dagger}(\widetilde{\mathbf{V}})^{ \dagger}\widetilde{\mathbf{V}}\) is in the column span of \(\mathbf{A}^{(i)}\mathbf{S}\) for each \(i\in[\ell]\). By Dvoretzky's theorem, i.e., Theorem 3.2, with \(\varepsilon=\mathcal{O}\left(1\right)\) and a fixed parameter \(p\), we have

\[\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{S}\mathbf{S}^{\dagger} (\widetilde{\mathbf{V}})^{\dagger}\widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_ {F}\leq\mathcal{O}\left(1\right)\cdot\max_{i\in[\ell]}\|\mathbf{G}^{(i)} \mathbf{A}^{(i)}\mathbf{S}\mathbf{S}^{\dagger}(\widetilde{\mathbf{V}})^{ \dagger}\widetilde{\mathbf{V}}-\mathbf{G}^{(i)}\mathbf{A}^{(i)}\|_{p,2}.\]

For sufficiently large \(p\), we have

\[\max_{i\in[\ell]}\|\mathbf{G}^{(i)}\mathbf{A}^{(i)}\mathbf{S} \mathbf{S}^{\dagger}(\widetilde{\mathbf{V}})^{\dagger}\widetilde{\mathbf{V}}- \mathbf{G}^{(i)}\mathbf{A}^{(i)}\|_{p,2}\leq\mathcal{O}\left(1\right)\cdot\| \mathbf{G}\mathbf{A}\mathbf{S}\mathbf{S}^{\dagger}(\widetilde{\mathbf{V}})^{ \dagger}\widetilde{\mathbf{V}}-\mathbf{G}\mathbf{A}\|_{p,2}.\]

By the correctness of the leverage score sampling matrix, we have

\[\|\mathbf{G}\mathbf{A}\mathbf{S}^{\dagger}(\widetilde{\mathbf{V}})^{ \dagger}\widetilde{\mathbf{V}}-\mathbf{G}\mathbf{A}\|_{p,2}\leq\mathcal{O} \left(1\right)\cdot\|\mathbf{G}\mathbf{A}\mathbf{S}\mathbf{S}^{\dagger}( \widetilde{\mathbf{V}})^{\dagger}\widetilde{\mathbf{V}}\mathbf{S}-\mathbf{G} \mathbf{A}\mathbf{S}\|_{p,2}.\]

Observe that minimizing \(\|\mathbf{X}\widetilde{\mathbf{V}}\mathbf{S}-\mathbf{G}\mathbf{A}\mathbf{S}\|_{p,2}\) over all \(\mathbf{X}\) corresponds to minimizing \(\|\mathbf{X}_{i}\widetilde{\mathbf{V}}\mathbf{S}-\mathbf{G}_{i}\mathbf{A}\mathbf{S} \|_{2}\) over all \(i\in[n^{\prime}]\). Moreover, the closed-form solution of the \(L_{2}\) minization problem is

\[\mathbf{G}_{i}\mathbf{A}\mathbf{S}\mathbf{S}^{\dagger}(\widetilde{\mathbf{V}})^{ \dagger}=\operatorname*{argmin}_{\mathbf{X}_{i}}\|\mathbf{X}_{i}\widetilde{ \mathbf{V}}\mathbf{S}-\mathbf{G}_{i}\mathbf{A}\mathbf{S}\|_{2}.\]

Therefore, we have

\[\|\mathbf{G}\mathbf{A}\mathbf{S}\mathbf{S}^{\dagger}(\widetilde{\mathbf{V}})^{ \dagger}\widetilde{\mathbf{V}}\mathbf{S}-\mathbf{G}\mathbf{A}\mathbf{S}\|_{p,2} \leq\min_{\mathbf{X}}\|\mathbf{X}\widetilde{\mathbf{V}}\mathbf{S}-\mathbf{G} \mathbf{A}\mathbf{S}\|_{p,2}.\]

Putting things together, we have

\[\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{S}\mathbf{S}^{\dagger} (\widetilde{\mathbf{V}})^{\dagger}\widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F} \leq\mathcal{O}\left(1\right)\cdot\min_{\mathbf{X}}\|\mathbf{X}\widetilde{ \mathbf{V}}\mathbf{S}-\mathbf{G}\mathbf{A}\mathbf{S}\|_{p,2}.\] (2)

By Equation 1 and Equation 2 and a rescaling of the constant hidden inside the big Oh notation, we have

\[\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{S}\mathbf{S}^{\dagger}( \widetilde{\mathbf{V}})^{\dagger}\widetilde{\mathbf{V}}-\mathbf{A}^{(i)}\|_{F} \leq\ell^{c}\cdot 2^{1/c}\cdot\mathcal{O}\left(k(\log\log k)(\log d)\right)\min_{\mathbf{ V}\in\mathbb{R}^{k\times d}}\max_{i\in[\ell]}\|\mathbf{A}^{(i)}\mathbf{V}^{ \dagger}\mathbf{V}-\mathbf{A}^{(i)}\|_{F}.\]

The desired claim then follows from the setting of \(\mathbf{M}^{(i)}=\mathbf{S}^{\dagger}(\widetilde{\mathbf{V}})^{\dagger} \widetilde{\mathbf{V}}\) for \(i\in[\ell]\) by Algorithm 4.

Additional Empirical Evaluations

In this section, we present a number of additional results from our empirical evaluations on socially fair low-rank approximation.

We first remark on a number of additional empirical evaluations for socially fair low-rank approximation. We first compare our bicriteria algorithm to the standard low-rank approximation baseline over a simple synthetic example. We then describe a simple fixed input matrix for socially fair low-rank approximation that serves as a proof-of-concept similarly demonstrating the improvement of fair low-rank approximation optimal solutions over the optimal solutions for standard low-rank approximation.

Synthetic dataset.Next, we show that for a simple dataset with two groups, each with two observations across four features, the performance of the fair low-rank approximation algorithm can be much better than standard low-rank approximation algorithm on the socially fair low-rank objective, even without allowing for bicriteria rank. For our synthetic dataset, we generate simple \(\mathbf{A}^{(1)}=\begin{bmatrix}2&0&0&0\\ 0&2&0&0\end{bmatrix}\) and \(\mathbf{A}^{(2)}=\begin{bmatrix}0&0&1.99&0\\ 0&0&0&1.99\end{bmatrix}\). An optimal fair low-rank solution for \(k=2\) is \(\begin{bmatrix}1&0&0&0\\ 0&0&1&0\end{bmatrix}\), which gives cost \(4\). However, due to the weight of the items in \(\mathbf{A}^{(1)}\), the standard low-rank algorithm will output low-rank factors with cost \(2\cdot 1.99^{2}\approx 7.92\), such as the factors \(\begin{bmatrix}1&0&0&0\\ 0&1&0&0\end{bmatrix}\). Thus the ratio between the objectives is roughly \(\frac{1}{2}\), i.e., the improvement is roughly a factor of two. We show in Figure 2 that our bicriteria algorithm achieves similar improvement.

Experimental setup.Again, we compare our bicriteria algorithm from Algorithm 3 against the standard non-fair low-rank approximation algorithm that outputs the top \(k\) right singular vectors from the singular value decomposition. Per Dvoretzky's Theorem, c.f., Theorem 3.2, we generate normalized Gaussian matrices \(\mathbf{G}\) and \(\mathbf{H}\) of varying size and then use \(L_{p}\) Lewis weight sampling to generate a matrix \(\mathbf{T}\) with varying values of \(p\). We generate matrices \(\mathbf{T}\), \(\mathbf{G}\), and \(\mathbf{H}\) with a small number of dimensions and thus do not compute the sampling matrix \(\mathbf{S}\) but instead use the full matrix. We first fix \(p=1\) and iterate over the number of rows/columns in the Gaussian sketch in the range \(\{1,2,\dots,19,20\}\) in Figure 1(a). We then fix the Gaussian sketch to have three rows/columns and iterate over \(p\in\{1,2,\dots,9,10\}\) in Figure 1(b). For each of the variables, we compare the outputs of the two algorithms across \(100\) iterations and plot the ratio of their fair costs. In particular, we set the same rank parameter of \(k=2\) for both algorithms; we remark that the theoretical guarantees for our bicriteria algorithm are even stronger when we permit the solution to have rank \(k^{\prime}\) for \(k^{\prime}>k\).

Figure 2: Ratio of the cost of our bicriteria algorithm to the cost of the standard low-rank approximation solution for \(k=2\), across 100 iterations.

Results and discussion.Our empirical evaluations in Figure 2 show that our algorithms can perform significantly better for socially fair low-rank approximation. We note that in both across all values of the number of rows/columns in the Gaussian sketch in Figure 1(a) and across all values of \(p\) in the Lewis weight sampling parameter in Figure 1(b), the average ratio between our bicriteria solution and the standard low-rank approximation is less than \(0.9\). We remark that any ratio less than \(1\) demonstrates the superiority of our algorithm, with smaller ratios indicating better performance. In fact, the minimum ratio is as low as \(0.76\).

Proof-of-concept.Finally, we give a toy example using a synthetic dataset showing the importance of considering fairness in low-rank approximation. Namely, we show that even for a simple dataset with four groups, each with a single observation across two features, the performance of the fair low-rank approximation algorithm can be much better than standard low-rank approximation algorithm on the socially fair low-rank objective. In this setup, we repeatedly generate matrices \(\mathbf{A}^{(1)},\mathbf{A}^{(2)},\mathbf{A}^{(3)},\mathbf{A}^{(4)}\in\{0,1\}^ {2}\), with \(\mathbf{A}^{(1)}=(1,0)\) and \(\mathbf{A}^{(2)}=\mathbf{A}^{(3)}=\mathbf{A}^{(4)}=(0,1)\). The optimal fair low-rank solution is \(\left(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2}\right)\) but due to the extra instances of \((0,1)\), the standard low-rank algorithm will output the factor \((0,1)\). Thus the optimal fair solution achieves value \(\frac{1}{4}\) on the socially fair low-rank approximation objective while the standard low-rank approximation solution achieves value \(\frac{1}{2}\), so that the ratio is a \(50\%\) improvement.

## Potential Broader Impact

This paper presents research aimed at reducing biases in key data summarization tasks. Our work is driven by the societal implications of machine learning techniques, representing a series of results towards developing more equitable methods in this field.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Indeed, our abstract and introduction clearly state the claims made, including the contributions made in the paper, as well as important assumptions and limitations. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, our theorem statements formally describe the limitations of our theoretical results. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Yes, the paper provides the complete proofs in the full version. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes, the paper fully discloses the information needed to reproduce the main experimental results of the paper, including the accessed datasets, as well as the code in the full version. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Yes, open access to the data and code are provided in the full version. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, all the testing parameters are described in the paper, and also provided in the code in the full version. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Yes, the paper provides the central statistics of our experiments when relevant. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, the computing resources are described in the paper. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, the research conducted in the paper conforms in every respect with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Yes, we address the potential broader impacts in the appendix. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our dataset was acquired from a publicly available repository and thus we do not introduce new data in this paper. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licenses for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All original assets used in this paper have been referenced appropriately. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Yes, the provided is well-documented. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.