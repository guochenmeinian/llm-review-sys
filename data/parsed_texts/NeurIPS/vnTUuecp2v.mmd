# Higher-Order Uncoupled Dynamics Do Not Lead to Nash Equilibrium -- Except When They Do

 Sarah A. Toonsi

Industrial and Enterprise Systems Engineering

University of Illinois Urbana-Champaign

Urbana, Illinois, USA

stoonsi2@illinois.edu

&Jeff S. Shamma

Industrial and Enterprise Systems Engineering

University of Illinois Urbana-Champaign

Urbana, Illinois, USA

jshamma@illinois.edu

###### Abstract

The framework of multi-agent learning explores the dynamics of how an agent's strategies evolve in response to the evolving strategies of other agents. Of particular interest is whether or not agent strategies converge to well-known solution concepts such as Nash Equilibrium (NE). In "higher-order" learning, agent dynamics include auxiliary states that can capture phenomena such as path dependencies. We introduce higher-order gradient play dynamics that resemble projected gradient ascent with auxiliary states. The dynamics are "payoff-based" and "uncoupled" in that each agent's dynamics depend on its own evolving payoff and has no explicit dependence on the utilities of other agents. We first show that for any polymatrix game with an isolated completely mixed-strategy NE, there exist higher-order gradient play dynamics that lead (locally) to that NE, both for the specific game and nearby games with perturbed utility functions. Conversely, we show that for any higher-order gradient play dynamics, there exists a game with a unique isolated completely mixed-strategy NE for which the dynamics do not lead to NE. Finally, we show that convergence to the mixed-strategy equilibrium in coordination games can come at the expense of the dynamics being inherently internally unstable.

## 1 Introduction

The field of learning in games explores how game-theoretic solution concepts emerge as the outcome of dynamic processes where agents adapt their strategies in response to the evolving strategies of other agents (1; 2; 3; 4). There is a multitude of specific cases of learning dynamics/game combinations that result in a range of outcomes, including convergence, limit cycles, chaotic behavior, and stochastic stability (5; 6; 7; 8; 9; 10; 11; 12; 13). The emphasis in the literature is on simple adaptive procedures, called "natural" dynamics in (14), that can result in various solution concepts (e.g., Nash equilibrium, correlated equilibrium, and coarse correlated equilibrium). (A separate concern is the complexity associated with such computations (e.g. (15; 16)).)

One "natural" restriction for learning is that the dynamics of one agent should not depend explicitly on the utility functions of other agents. This restriction was referred to as "uncoupled" dynamics in (17), where the authors constructed a specific anti-coordination matrix game for which no uncoupled learning dynamics could converge to the unique mixed-strategy NE. The dynamics considered in that setting were of fixed order, i.e., the order of the learning dynamics was restricted to match the dimension of the strategy space. More recent work showed that specific instances of fixed order uncoupled learning dynamics can never lead to mixed-strategy NE (18). Furthermore, there exist games for which any fixed order learning dynamics are bound to have an initial condition starting from which the dynamics do not converge to NE (19).

The restriction on the order of the learning dynamics in learning mixed-strategy NE turns out to be essential. In particular, by introducing additional auxiliary states, (20) showed that higher-order learning could overcome the obstacle of convergence to NE in the same anti-coordination game considered in (17) while remaining uncoupled.

Higher-order learning in games can be seen as a parallel to higher-order optimization algorithms, such as momentum-based or optimistic gradient algorithms (e.g., (21; 22)). Such algorithms utilize history to update the underlying search parameter. In this way, there is a path dependency on the trajectory of information. An early utilization of higher-order learning is in (23), in which a player's strategy update uses two stages of history of an opponent's strategies in a zero-sum setting to eliminate oscillations. Similar ideas were used in (24). Reference (25) modified gradient-based algorithms through the introduction of a cumulative (integral) term. In (20), higher-order dynamics were used to create a myopic forecast of the action of other agents. Authors in (26) introduce a version of higher-order replicator dynamics and show that, unlike fixed order replicator dynamics, weakly dominated strategies become extinct. Reference (27) utilizes the system theoretic notion of passivity to analyze a family of higher-order dynamics.

In this paper, we further explore the implications of learning dynamics that are uncoupled. We address "payoff based" dynamics, in which the learning dynamics depend on the evolution of a payoff vector that is viewed as an externality. When players are engaged in a game, then the payoff stream of one agent depends on the actions of other agents. However, the learning dynamics themselves do not change based on the source of the payoff streams.

First, we show that for any polymatrix game with a mixed-strategy NE, there exist payoff-based dynamics that converge locally to that NE. This result is established by making a connection between convergence to NE and the existence of decentralized stabilizing control (28; 29). A consequence of the payoff-based structure is that the dynamics also converge to the NE of nearby perturbations of the original game. The form of higher-order learning used for this stability result is higher-order gradient play, which generalizes gradient ascent. Next, we show that for any such dynamics, there exists a game with a unique mixed-strategy NE that is unstable under given dynamics. The tool utilized is a classical analysis method in feedback control systems known as root-locus (e.g., (30), (31)), which characterizes the locations of the eigenvalues of a matrix as a function of a scalar parameter. A combination of the above results suggests the lack of universality on the side of both learning dynamics and games. While any mixed-strategy NE can be stabilized by suitable higher-order gradient dynamics, any such dynamics can be destabilized by a suitable anti-coordination game. Finally, we examine the implications of higher-order dynamics being able to converge to the mixed-strategy NE of a \(2\times 2\) coordination game, which has two pure NE and one mixed-strategy NE. We show that such higher-order gradient play dynamics must have an inherent internal instability, which makes them unsuitable, if not irrational, as a model of learning.

## 2 Payoff-based learning dynamics

### Finite polymatrix games

We consider finite (normal form) games over mixed-strategies. There are \(n\) players. The strategy space of player \(i\in\{1,2,...,n\}\) is the probability simplex, \(\Delta(k_{i})\), where \(k_{i}\) is a positive integer and \(\Delta(\cdot)\) is defined as \(\Delta(\kappa)=\left\{s\in\mathbb{R}^{\kappa}\ \Big{|}\ s_{j}\geq 0,j=1,..., \kappa,\ \&\ \sum_{j=1}^{\kappa}s_{j}=1\right\}.\) The utility function of player \(i\) is a function \(u_{i}:\Delta(k_{1})\times...\times\Delta(k_{n})\rightarrow\mathbb{R}\). We sometimes will write \(u_{i}(x_{1},...,x_{n})=u_{i}(x_{i},x_{-i})\), for \(x_{i}\in\Delta(k_{i})\) and \(x_{-i}\in\mathcal{X}_{-i}\), where \(\mathcal{X}_{-i}=\Delta(k_{1})\times...\times\Delta(k_{i-1})\times\Delta(k_{ i+1})\times...\times\Delta(k_{n})\).

For convenience, we will restrict our discussion to pairwise interactions, also called polymatrix games. That is, the utility function of player \(i\) is defined as

\[u_{i}(x_{1},...,x_{n})=x_{i}^{T}\sum_{j=1\atop j\neq i}^{n}M_{ij}x_{j},\] (1)

for matrices, \(M_{ij}\), \(j=1,\ldots,n\), \(j\neq i\). We can write the utility function of player \(i\) as the inner product \(u_{i}(x_{1},...,x_{n})=x_{i}^{T}P_{i}(x_{-i})\) where \(P_{i}(x_{-i})=\sum_{j=1\atop j\neq i}^{n}M_{ij}x_{j}\in\mathbb{R}^{k_{i}}.\) Accordingly, each element of \(P_{i}(x_{-i})\) can be viewed as a payoff that is associated with a component of player \(i\)'s strategy vector, \(x_{i}\).

A Nash equilibrium (NE) is a tuple \((x_{1}^{*},...,x_{n}^{*})\in\mathcal{X}\) such that for all \(i=1,...,n\),

\[u_{i}(x_{i}^{*},x_{-i}^{*})\geq u_{i}(x_{i},x_{-i}^{*}),\quad\forall x_{i}\in \Delta(k_{i}).\]

A completely mixed-strategy NE is such that each \(x_{i}^{*}\) is in the interior of the simplex.

### Fixed order learning

Our model of learning is a dynamical system that relates trajectories of a payoff vector, \(p_{i}(t)\), to trajectories of the strategy, \(x_{i}(t)\). In particular, learning dynamics for player \(i\) are specified by a function \(f_{i}:\Delta(k_{i})\times\mathbb{R}^{k_{i}}\rightarrow\mathbb{R}^{k_{i}}\) according to

\[\dot{x_{i}}(t)=f_{i}(x_{i}(t),p_{i}(t)),\]

where \(x_{i}(t)\in\Delta(k_{i})\) and \(p_{i}:\mathbb{R}_{+}\rightarrow\mathbb{R}^{k_{i}}\). We assume implicitly that \(f_{i}\) and \(p_{i}\) are such that there exists a unique solution whenever \(x_{i}(0)\in\Delta(k_{i})\). We further assume that the dynamics satisfy the invariance property that

\[x_{i}(0)\in\Delta(k_{i})\Rightarrow x_{i}(t)\in\Delta(k_{i}),\quad\forall t \geq 0.\] (2)

Note that we define learning dynamics without specifying the source of the payoff vector, \(p_{i}(t)\), hence the terminology "payoff-based". Only once a player is coupled with other players in a game through their own (possibly heterogeneous) learning dynamics is when we make the connection \(p_{i}(t)=P_{i}(x_{-i}(t))\).

This formulation is illustrated in Figure 1, where the LD\({}_{i}\) denote payoff-based learning dynamics that are interconnected through the game matrices, \(M_{ij}\). Note that such learning dynamics are uncoupled by construction since each player can only access its own payoff vector. There is no dependence on the payoff stream of other players. Indeed, there is no dependence on the parameters of one's own utility function.

### Higher-order learning

The learning dynamics described in the previous section have a fixed order associated with the dimension of the strategy space. Higher-order learning dynamics allow for the introduction of auxiliary states as follows. For any fixed order learning dynamics, \(f_{i}\), we can define a higher-order version as

\[\dot{x}_{i}(t) =f_{i}(x_{i}(t),p_{i}(t)+\phi_{i}(p_{i}(t),z_{i}(t)))\] (3a) \[\dot{z}_{i}(t) =g_{i}(p_{i}(t),z_{i}(t)).\] (3b)

As before, \(x_{i}(t)\in\Delta(k_{i})\) and \(p_{i}:\mathbb{R}_{+}\rightarrow\mathbb{R}^{k_{i}}\). The new variable \(z_{i}\in\mathbb{R}^{\ell_{i}}\) represents \(\ell_{i}\) dimensional auxiliary states that evolve according to the \(p_{i}\)-dependent dynamics, \(g_{i}\). These enter into the original fixed order dynamics through \(\phi_{i}\). Accordingly, we can view \(p_{i}(t)+\phi_{i}(p_{i}(t),z_{i}(t))\) as a modified payoff stream that captures path dependencies in \(p_{i}\), and the original learning dynamics react to this modified payoff stream.

As with the fixed order counterparts, there is no specification of game parameters in higher-order learning dynamics. In order to enforce that the auxiliary states have no effect on the equilibria of games, we make the following assumption.

Figure 1: Payoff-based learning dynamics (LD\({}_{i}\)) in feedback with game matrices (\(M_{ij}\)).

**Assumption 2.1**.: _If \(p_{i}^{*}\) and \(z_{i}^{*}\) are an equilibrium of the higher-order dynamics, i.e.,_

\[0=g_{i}(p_{i}^{*},z_{i}^{*})\]

_then_

\[\phi_{i}(p_{i}^{*},z_{i}^{*})=0.\]

This assumption assures that the auxiliary states represent purely transient phenomena that disappear at equilibrium.

**Example 1** (**Anticipatory higher-order dynamics)**.: _A special case of higher-order dynamics is_

\[\dot{z}_{i} =\lambda(p_{i}-z_{i})\] \[\phi_{i} =\gamma\lambda(p_{i}-z_{i}),\]

_where the the higher-order modification is the linear system (see Appendix A.1)_

\[\left(\frac{-\lambda I}{-\gamma\lambda I}\left|\frac{\lambda I}{\gamma \lambda I}\right.\right),\]

_and \(\gamma,\lambda\in\mathbb{R}_{+}\). Inuition behind the connection to anticipation can be seen by viewing \(\lambda(p_{i}-z_{i})\) as an approximation of \(\dot{p}_{i}\), and so \(p_{i}(t)+\gamma\lambda(p_{i}(t)-z_{i}(t))\approx p_{i}(t+\gamma)\). Similar higher-order dynamics were used in reference (20) for smooth fictitious play to overcome the lack of convergence of uncoupled dynamics to NE in the anti-coordination game analyzed by (17). See also reference (32) for an analysis with replicator dynamics._

_Anticipatory higher-order dynamics can also be linked to optimistic optimization algorithms (e.g., (22)). An Euler discretization of step size, \(h\), results in_

\[z_{i}^{+}=z_{i}+h\lambda(p_{i}-z_{i}),\]

_with a modified payoff stream of_

\[p_{i}+\phi_{i}=p_{i}+\gamma\lambda(p_{i}-z_{i}).\]

_Setting \(h=1/\lambda\) and \(\gamma=1/\lambda\) results in_

\[p_{i}+\phi_{i} =p_{i}+(p_{i}-z_{i})\] \[=p_{i}+(p_{i}-p_{i}^{-}),\]

_where the superscripts '\(+\)' and '\(-\)' indicate the next and previous discrete time steps, respectively. There are also optimistic variants of discrete-time no-regret learning algorithms [33; 34; 35] that guarantee faster convergence rates to coarse correlated equilibria compared to the standard versions._

## 3 Gradient play

The main results of the paper will examine the behavior of gradient play and higher-order gradient play, which are the focus of this section.

### Fixed order gradient play

In gradient play dynamics, a player adjusts its strategy in the direction of the payoff stream, i.e.,

\[\dot{x}_{i}=\Pi_{\Delta}[x_{i}+p_{i}]-x_{i},\] (4)

where \(\Pi_{\Delta}[x]:\mathbb{R}^{n}\rightarrow\Delta\left(n\right)\) is the projection of \(x\) into the simplex, i.e., \(\Pi_{\Delta}(x)=\arg\min_{s\in\Delta\left(n\right)}\left\|x-s\right\|.\)

The terminology "gradient play" stems from the gradient of an agent's utility function in (1) with respect to its own strategy, \(x_{i}\), namely \(\nabla_{x_{i}}u_{i}(x_{i},x_{-i})=P_{i}(x_{-i})=\sum_{j\neq i}^{n}M_{ij}x_{j}.\) As was done in the description of payoff-based learning, we replace \(P_{i}(x_{-i})\) with the payoff stream \(p_{i}\) without regard to the game matrices \(M_{ij}\).

Our primary concern will be studying these dynamics near a completely mixed-strategy NE. To this end, let \(x^{*}=(x_{1}^{*},\ldots,x_{n}^{*})\) be an isolated completely mixed-strategy NE. The strategy vectorevolves on the simplex, which is a subset of dimension \(k_{i}-1\). Hence, the local behavior of the dynamics around \(x_{i}^{*}\) is characterized by evolution on a lower-dimensional subset. Thus, we can write

\[x_{i}=x_{i}^{*}+N_{i}w_{i}\] (5)

where

\[\mathbf{1}^{\mathrm{T}}N_{i}=0\ \ \&\ N_{i}^{\mathrm{T}}N_{i}=I.\] (6)

Therefore, \(w_{i}\in\mathbb{R}^{(k_{i}-1)}\) represents deviations from \(x_{i}^{*}\) and satisfies \(w_{i}(t)=N_{i}^{\mathrm{T}}(x_{i}(t)-x_{i}^{*}).\) When all players utilize fixed order gradient play, the collective dynamics near a completely mixed-strategy NE take the form

\[\dot{w}=\mathcal{M}w,\] (7)

where

\[\mathcal{M}=\mathcal{N}^{\mathrm{T}}\begin{pmatrix}0&M_{12}&M_{13}&\ldots&M_{ 1n}\\ M_{21}&0&M_{23}&\ldots&M_{2n}\\ M_{31}&M_{32}&0&\ldots&M_{3n}\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ M_{n1}&M_{n2}&M_{n3}&\ldots&0\end{pmatrix}\mathcal{N},\quad\mathcal{N}= \begin{pmatrix}N_{1}&&\\ &\ddots&\\ &&N_{n}\end{pmatrix}.\] (8)

Given the zero trace of \(\mathcal{M}\), standard gradient play is always unstable at a completely mixed-strategy NE (see Appendix A.3 for stability conditions). Also, for this equilibrium to be isolated, \(\mathcal{M}\) must be non-singular. Otherwise, \(\mathcal{M}\) has a non-trivial null space leading to an equilibrium subspace.

### Higher-order gradient play

We will be interested in a specific form of higher-order gradient play that uses the following linear structure of higher-order dynamics:

\[\dot{x}_{i} =-x_{i}+\Pi_{\Delta}\left[x_{i}+p_{i}+N_{i}(G_{i}\xi_{i}+H_{i}(N_{ i}^{\mathrm{T}}p_{i}-v_{i}))\right]\] \[\dot{\xi}_{i} =E_{i}\xi_{i}+F_{i}(N_{i}^{\mathrm{T}}p_{i}-v_{i})\] \[\dot{v}_{i} =N_{i}^{\mathrm{T}}p_{i}-v_{i},\]

for some matrices \(E_{i}\), \(F_{i}\), \(G_{i}\) and \(H_{i}\). Here, the auxiliary states are \(z_{i}=(v_{i},\xi_{i})\), which enter into the dynamics through \(\phi_{i}(p_{i},\xi_{i},v_{i})=N_{i}(G_{i}\xi_{i}+H_{i}(N_{i}^{\mathrm{T}}p_{i}- v_{i})),\) where \(N_{i}\) is defined as in (6).

The motivation behind this structure, illustrated in Figure 2, assures the enforcement of Assumption 2.1. The payoff stream is first preprocessed by a specific linear system to produce \(y_{i}\) and then by a general linear system \(K_{i}\) to produce \(u_{i}\). The matrices \((E_{i},F_{i},G_{i},H_{i})\) create the dynamical system

\[K_{i}\sim\left(\begin{array}{c|c}E_{i}&F_{i}\\ \hline G_{i}&H_{i}\end{array}\right)\] (9)

that maps \(y_{i}=N_{i}^{\mathrm{T}}p_{i}-v_{i}\) to \(u_{i}=G_{i}\xi_{i}+H_{i}y_{i}\) via \(\dot{\xi}_{i}=E_{i}\xi_{i}+F_{i}y_{i}\). The preprocessing system has the property (see "washout filters" in Appendix A.2) that if \(p_{i}(t)\) converges to a constant, then \(y_{i}(t)\) converges to zero. Accordingly, when \(E_{i}\) is non-singular, the preprocessing system guarantees Nash stationarity, i.e., if

\[\lim_{t\to\infty}(x_{i}(t),\xi_{i}(t),v_{i}(t))=(x_{i}^{*},\xi_{i}^{*},v_{i}^ {*})\quad\forall i\]

then \(x^{*}=(x_{1}^{*},\ldots,x_{n}^{*})\) is a NE. To conclude, the linear system \(K_{i}\) is the central entity that performs the payoff modification, and the preprocessing system ensures compliance with Assumption 2.1.

Figure 2: Cascade representation of linear higher-order dynamics for gradient play.

### Local stability analysis

As before, we can analyze the behavior near a completely mixed-strategy NE \(x^{*}\) through the variable \(w_{i}\) defined as in (5). Using the fact that \(N_{i}^{\mathrm{T}}\sum_{j\neq i}M_{ij}x_{j}^{*}=0\), we can write the local dynamics of a player as

\[\dot{w}_{i} =N_{i}^{\mathrm{T}}\sum_{j\neq i}M_{ij}N_{j}w_{j}+G_{i}\xi_{i}+H_{ i}\Big{(}N_{i}^{\mathrm{T}}\sum_{j\neq i}M_{ij}N_{j}w_{j}-v_{i}\Big{)}\] \[\dot{\xi}_{i} =E_{i}\xi_{i}+F_{i}\Big{(}N_{i}^{\mathrm{T}}\sum_{j\neq i}M_{ij}N _{j}w_{j}-v_{i}\Big{)}\] \[\dot{v}_{i} =N_{i}^{\mathrm{T}}\sum_{j\neq i}M_{ij}N_{j}w_{j}-v_{i}.\]

The collective dynamics near a mixed-strategy NE can be written as

\[\begin{pmatrix}\dot{w}\\ \dot{\xi}\\ \dot{v}\end{pmatrix}=\begin{pmatrix}(I+H)\mathcal{M}&G&-H\\ F\mathcal{M}&E&-F\\ \mathcal{M}&0&-I\end{pmatrix}\begin{pmatrix}w\\ \xi\\ v\end{pmatrix},\] (10)

where \(E\),\(F\),\(G\), and \(H\) are block diagonal matrcies with appropriate dimensions and \(\mathcal{M}\) is defined in (8). Local stability of a completely mixed NE is determined by whether the above collective dynamics are stable, i.e., the dynamics matrix in (10) is a stability matrix.

## 4 Uncoupled dynamics that lead to mixed-strategy NE

### Decentralized control formulation

The stability of a mixed-strategy equilibrium is tied to the existence of \(K_{1}\), \(K_{2}\),..., \(K_{n}\) so that the linear system in (10) is stable. When the \(K_{i}\) have yet to be determined, we can rewrite (10) as

\[\begin{pmatrix}\dot{w}\\ \dot{v}\end{pmatrix} =\begin{pmatrix}\mathcal{M}&0\\ \mathcal{M}&-I\end{pmatrix}\begin{pmatrix}w\\ v\end{pmatrix}+\begin{pmatrix}I\\ 0\end{pmatrix}u,\] (11a) \[y =(\mathcal{M}&-I)\begin{pmatrix}w\\ v\end{pmatrix},\] (11b)

where \(u=\begin{pmatrix}u_{1}\\ \vdots\\ u_{n}\end{pmatrix},y=\begin{pmatrix}y_{1}\\ \vdots\\ y_{n}\end{pmatrix},\) and the \(y_{i}\) and \(u_{i}\) are to be related through \(K_{i}\).

### Decentralized stabilization

Let

\[\mathcal{P}\sim\left(\begin{array}{c|c}\mathcal{A}&\mathcal{B}\\ \hline\mathcal{C}&0\end{array}\right)\]

with

\[\mathcal{A}=\begin{pmatrix}\mathcal{M}&0\\ \mathcal{M}&-I\end{pmatrix},\quad\mathcal{B}=\begin{pmatrix}I\\ 0\end{pmatrix},\quad\mathcal{C}=\left(\mathcal{M}&-I\right).\] (12)

We first establish that \(\mathcal{P}\) can be stabilized by verifying the conditions for stabilizability and detectability (see Appendix A.3). The assumption that \(\mathcal{M}\) is non-singular stems from our interest in isolated NE.

**Proposition 4.1**.: _For \(\mathcal{M}\) non-singular, the pair \((\mathcal{A},\mathcal{B})\) is stabilizable, and the pair \((\mathcal{A},\mathcal{C})\) is detectable._

While Proposition 4.1 establishes that \(\mathcal{P}\) can be stabilized, that property alone is inadequate for our purposes. In particular, for the learning dynamics to be uncoupled, we seek to establish decentralized stabilization (see Appendix A.4) according to the partition

\[\begin{pmatrix}\dot{w}\\ \dot{v}\end{pmatrix}=\mathcal{A}\begin{pmatrix}w\\ v\end{pmatrix}+\sum_{i=1}^{n}\mathcal{B}_{i}u_{i},\quad y_{i}=\mathcal{C}_{i} \begin{pmatrix}w\\ v\end{pmatrix},\] (13)where

\[\mathcal{A}=\begin{pmatrix}\mathcal{M}&0\\ \mathcal{M}&-I\end{pmatrix},\quad\mathcal{B}_{i}=\begin{pmatrix}\mathcal{E}_{i} \\ 0\end{pmatrix},\quad\mathcal{C}_{i}=\begin{pmatrix}\mathcal{M}_{i\bullet}&- \mathcal{E}_{i}^{\mathrm{T}}\end{pmatrix}.\] (14)

Here, \(\mathcal{M}_{i\bullet}\) denotes the \(i^{\text{th}}\) block row of \(\mathcal{M}\), i.e.,

\[\mathcal{M}_{i\bullet}=\begin{pmatrix}N_{i}^{\mathrm{T}}M_{i1}N_{1}&...&N_{i} ^{\mathrm{T}}M_{i(i-1)}N_{i-1}&0&N_{i}^{\mathrm{T}}M_{i(i+1)}N_{i+1}&...&N_{i} ^{\mathrm{T}}M_{in}N_{n}\end{pmatrix}\]

and

\[\mathcal{E}_{i}^{\mathrm{T}}=\begin{pmatrix}0&...&0&\underbrace{I}_{i^{\text{ th position}}}&0&...&0\end{pmatrix},\]

where \(I\) has a dimension (suppressed in the notation) of \(k_{i}-1\).

**Theorem 4.1**.: _For any isolated (i.e., \(\mathcal{M}\) is non-singular) completely mixed-strategy NE, there exist uncoupled higher-order gradient play dynamics such that (10) is stable._

The proof of Theorem 4.1 relies on the conditions of Theorem A.1 and is presented in Appendix B.1.

Theorem 4.1 should be viewed as a statement regarding whether uncoupled learning in itself is a barrier to learning dynamics leading to NE. The theorem makes no claim that the higher-order learning dynamics are interpretable (e.g., as in anticipatory learning). Nor does the theorem offer guidance on how agents may construct the matrices of higher-order learning that lead to convergence. In the next section, we will see that, while the structure is universal, any specific set of parameters is not universal in that one can construct a game for which they do not lead to NE. Despite the lack of universality, there is an inherent robustness that is a consequence of stability. The following follows from standard arguments on linear systems.

**Proposition 4.2**.: _Let the \(K_{i}\) and \(M_{ij}\), \(i=1,...,n\) and \(j=1,...,n\), be such that (10) is stable. Then there exists a \(\delta>0\) such that (10) is stable with the \(M_{ij}\) replaced by any \(\tilde{M}_{ij}\) as long as \(\left\|\tilde{M}_{ij}-M_{ij}\right\|<\delta\) for all \(i=1,...,n\) and \(j=1,...,n\)._

In words, this proposition guarantees that learning dynamics that lead to NE for a specific game continue to do so for nearby games.

### Stabilization through a single higher-order player

The previous section's analysis allowed all players to utilize higher-order learning. In some cases, it may not be necessary for all players to utilize higher-order learning. In this section, we present sufficient conditions under which a single player using higher-order gradient play with the remainder utilizing fixed order gradient play can still lead to NE.

**Assumption 4.1**.:
1. _Let_ \((w,\lambda)\) _be a left eigenvalue pair of_ \(\mathcal{M}\)_, i.e.,_ \[w^{\mathrm{T}}\mathcal{M}=\lambda w^{\mathrm{T}},\] _with_ \(\mathbf{Re}[\lambda]\geq 0\) _and_ \(w^{\mathrm{T}}=\begin{pmatrix}w_{1}^{\mathrm{T}}&...&w_{n}^{\mathrm{T}}\end{pmatrix}\) _partitioned consistently with (_8_). Then_ \(w_{i}\neq 0\) _for all_ \(i\)_._
2. _Let_ \((v,\lambda)\) _be a right eigenvalue pair of_ \(\mathcal{M}\)_, i.e.,_ \[\mathcal{M}v=\lambda v,\] _with_ \(\mathbf{Re}[\lambda]\geq 0\) _and_ \(v=\begin{pmatrix}v_{1}^{\mathrm{T}}&...&v_{n}^{\mathrm{T}}\end{pmatrix}^{ \mathrm{T}}\) _partitioned consistently with (_8_). Then_ \(v_{i}\neq 0\) _for all_ \(i\)_._

Recall the definitions of \(\mathcal{A}\), \(\mathcal{B}_{i}\), and \(\mathcal{C}_{i}\) from (14).

**Proposition 4.3**.: _Let \(\mathcal{M}\) be non-singular and satisfy Assumption 4.1. Then for any \(i\), the pair \((\mathcal{A},\mathcal{B}_{i})\) is stabilizable and the pair \((\mathcal{A},\mathcal{C}_{i})\) is detectable._

As a consequence of Proposition 4.3, it is possible for a completely mixed-strategy NE to be stabilized where a single player utilizes higher-order gradient play with the remaining players utilizing fixed order gradient play.

Non-convergence to NE in higher-order gradient play

We now show that linear higher-order gradient play dynamics need not lead to NE. Given any such dynamics, we construct a game with a unique NE that is unstable under given dynamics.

### The Jordan anti-coordination game

The Jordan anti-coordination game, introduced in (36), was used in (17) to prove that fixed order uncoupled learning dynamics do not lead to NE. The game consists of three players with

\[u_{1}(x_{1},x_{2})=x_{1}^{\mathrm{T}}\begin{pmatrix}0&1\\ 1&0\end{pmatrix}x_{2},\quad u_{2}(x_{2},x_{3})=x_{2}^{\mathrm{T}}\begin{pmatrix} 0&1\\ 1&0\end{pmatrix}x_{3},\quad u_{3}(x_{3},x_{1})=x_{3}^{\mathrm{T}}\begin{pmatrix} 0&1\\ 1&0\end{pmatrix}x_{1},\]

and a unique mixed-strategy NE at \(x_{1}^{*}=x_{2}^{*}=x_{3}^{*}=\begin{pmatrix}1&1\\ 1&0\end{pmatrix}^{\mathrm{T}}.\) We will let \(\Gamma(\mu)\) denote the Jordan anti-coordination game but with the utility function of player 1 modified to \(u_{1}(x_{1},x_{2})=x_{1}^{\mathrm{T}}(\mu M_{12})x_{2},\) where \(\mu\in\mathbb{R}_{+}.\) Since scaling payoffs does not change the nature of the game, \(\Gamma(\mu)\) has the same unique NE as \(\Gamma(1).\)

### Destabilization using rescaled anti-coordination

Suppose all three players use variants of linear higher-order gradient play in \(\Gamma(\mu)\). As before, we denote the higher-order dynamics of player \(i\) as the linear dynamical system (9). To study the local behavior of the dynamics around the unique mixed-strategy NE of \(\Gamma(\mu)\), we define \(w_{1}(t),\)\(w_{2}(t),\) and \(w_{3}(t)\) as in (5). Then, we analyze the local stability of the mixed-strategy NE through (10).

The following lemma will be essential in proving our main result.

**Lemma 5.1**.: _Let \(A\in\mathbb{R}^{n\times n}\), \(B\in\mathbb{R}^{n\times 1}\) and \(C\in\mathbb{R}^{1\times n}\). If \(CB=CAB=0\), and \(CA^{m}B\neq 0\) for some \(m\geq 2\). Then for sufficiently large \(\mu>0\), \(A-\mu BC\) is not a stability matrix._

The proof of Lemma 5.1 is presented in Appendix B.2 and it uses root-locus arguments (see references (31; 30)).

**Proposition 5.1**.: _If linear higher-order gradient play dynamics are locally exponentially stable at the unique NE of \(\Gamma(1)\), then there exists \(\mu>0\) such that the unique NE of \(\Gamma(\mu)\) is unstable under such dynamics._

The proof of Proposition 5.1 is presented in Appendix B.3. The main idea of the proof is to write the local dynamics matrix in the form \(A-\mu BC,\) and then use Lemma 5.1. We also provide a similar proof for sufficiently small \(\mu\) in Appendix C.

The results might be puzzling because, for all \(\mu>0\), all games \(\Gamma(\mu)\) are strategically equivalent. Convergence guarantees for learning dynamics are usually established amongst classes of games. Thus, it is generally expected that dynamics will behave similarly for all games in a particular class. In this case, we design linear learning dynamics that are affected by simple rescaling of the payoff matrices.

## 6 Strong stabilization of mixed-strategy NE

Results from Section 4 imply that the mixed-strategy NE in a two-player \(2\times 2\) (identical-interest) coordination game can be stabilized. Here, we argue why dynamics that stabilize this mixed-strategy equilibrium are not reasonable. Specifically, we show that such dynamics _must be_ inherently unstable as an open system, i.e., as dynamics that respond to an exogenous payoff stream, and this instability is problematic with respect to such payoffs.

First, we inspect which type of mixed-strategy NE requires unstable learning dynamics for stabilization. For this purpose, consider the system in (12) for \(n=k_{1}=k_{2}=2\):

\[\mathcal{A}=\begin{pmatrix}0&m_{12}&0&0\\ m_{21}&0&0&0\\ 0&m_{12}&-1&0\\ m_{21}&0&0&-1\end{pmatrix}\quad\mathcal{B}=\begin{pmatrix}1&0\\ 0&1\\ 0&0\\ 0&0\end{pmatrix}\quad\mathcal{C}=\begin{pmatrix}0&m_{12}&-1&0\\ m_{21}&0&0&-1\end{pmatrix}.\] (15)t must be that \(m_{12}\neq 0\) and \(m_{21}\neq 0\). The ability to stabilize a system via another stable system is referred to as _strong stabilization_ (see Appendix A.3). The next proposition gives a sufficient condition under which an isolated mixed-strategy NE is not strongly stabilizable.

**Proposition 6.1**.: _If \(m_{12}m_{21}>0\), then (15) is not strongly stabilizable._

The proof of Proposition 6.1 uses the "parity interlacing principle" (see reference (37)) and is presented in Appendix B.4.

The nature of the game can be inferred from the scalars \(m_{12}\) and \(m_{21}\). For example in zero-sum games we have \(M_{12}=-M_{21}^{\mathrm{T}}\), which gives

\[m_{12}=N^{\mathrm{T}}M_{12}N=N^{\mathrm{T}}M_{12}^{\mathrm{T}}N=-N^{\mathrm{T} }M_{21}N=-m_{21}.\]

In coordination games, we have \(M_{12}=M_{21}\), which gives

\[m_{12}=N^{\mathrm{T}}M_{12}N=N^{\mathrm{T}}M_{21}N=m_{21}.\]

Therefore, the mixed-strategy NE in a coordination game is not strongly stabilizable.

Now let us examine the implications of inherently unstable learning dynamics. A reasonable expectation of learning dynamics is that in the case of a constant payoff vector, i.e., \(p_{i}(t)\equiv p^{*}\), we expect

\[\lim_{t\to\infty}x_{i}(t)=\beta(p^{*}),\]

where \(\beta(p^{*})\) is a best response to \(p^{*}\), i.e.,

\[\beta(p^{*})=\operatorname*{arg\,max}_{x_{i}\in\Delta(k_{i})}x_{i}^{\mathrm{T }}p^{*}.\]

For any higher-order gradient play dynamics, if \(E_{i}\) is a stability matrix, then whenever \(p_{i}(t)\equiv p^{*}\) for some constant vector \(p^{*}\), one can show \(\xi_{i}(t)\to 0\), which implies that \(x_{i}(t)\) is generated by standard gradient play dynamics in the limit. However, if \(p_{i}(t)\equiv p^{*}\) and the dynamics are inherently unstable, the term \(N_{i}G_{i}\xi_{i}(t)\) need not vanish. Indeed, one can construct \(p^{*}\) such that \(x_{i}(t)\) does not converge to the best response of \(p^{*}\) (see the example in Section 7.2). The inability of learning dynamics to converge to the best response of a constant payoff vector does not reflect "natural" behavior.

## 7 Numerical experiments

### Jordan anti-coordination game: Stabilization through a single player

The payoff matrices of the Jordan anti-coordination game, introduced in Section 5.1, satisfy Assumption 4.1. Therefore, we can stabilize its mixed-strategy NE, allowing only one player to use higher-order learning while others continue to use standard gradient play. For this purpose, let \(\xi_{1}\in\mathbb{R}\) and choose \(H_{1}=\gamma\lambda\), \(G_{1}=-\gamma\lambda\), \(F_{1}=\lambda\) and \(E_{1}=-\lambda\), where \(\lambda=50\) and \(\gamma=5\). Such dynamics resemble anticipatory gradient play but on the filtered low-dimensional payoff. Figure 3 illustrates convergence of the players' strategies to NE.

Figure 3: Single-player stabilization of the Jordan game.

### Stabilization of mixed-strategy NE in coordination games

Consider the (identical interest) coordination game:

\[u_{1}(x_{1},x_{2})=x_{1}^{\mathrm{T}}\begin{pmatrix}1&0\\ 0&1\end{pmatrix}x_{2},\quad u_{2}(x_{2},x_{1})=x_{2}^{\mathrm{T}}\begin{pmatrix} 1&0\\ 0&1\end{pmatrix}x_{1}.\]

The game has two pure strategy NE and one completely mixed-strategy NE at \(x^{*}=\begin{pmatrix}1&1\\ 0&1\end{pmatrix}x_{1}\).

Consider the following set of parameters for higher-order gradient play: \(E_{1}=\lambda\), \(F_{1}=-2\lambda\), \(G_{1}=\gamma\lambda\), \(H_{1}=-\gamma\lambda\), \(E_{2}=-\lambda_{2}\), \(F_{2}=\lambda_{2}\), \(G_{2}=-\gamma_{2}\lambda_{2}\), and \(H_{2}=\gamma_{2}\lambda_{2}\). The numerical values are \(\lambda=0.5\), \(\gamma=20\), \(\lambda_{2}=50\), and \(\gamma_{2}=1\). Figure 3(a) illustrates convergence to the mixed-strategy NE of this coordination game. Suppose we break the feedback loop and use \(p^{*}=\begin{pmatrix}0&1\end{pmatrix}^{\mathrm{T}}\) as the input to the first player dynamics. The response to such input is illustrated in Figure 3(b). We see that \(E_{1}>0\) is a scalar, and so \(\xi_{1}\) grows without bound. The strategy \(x_{1}\), which is projected to the simplex, converges to \(\begin{pmatrix}1&0\end{pmatrix}^{\mathrm{T}}\), which is not a best response to the input \(p^{*}\).

## 8 Concluding remarks

To recap, we studied the role of higher-order gradient play with linear higher-order dynamics. We showed that for any game with an isolated completely mixed-strategy NE, there exist higher-order gradient play dynamics that lead to that NE, both for the original game and for nearby games. On the other hand, we showed that for any higher-order gradient play dynamics, the dynamics do not lead to NE for a suitably rescaled anti-coordination game. We also provided an argument against dynamics that lead to the mixed-strategy NE in a coordination game, showing they are not reasonable.

Regarding the higher-order gradient play dynamics that lead to NE, the interpretation of the results herein should not be that these dynamics are either a descriptive model of learning or a prescriptive recommendation for computation. Rather, the results are a contribution towards delineating what is possible or impossible in multi-agent learning. In that sense, they may be seen as a complement to the contributions in (17). Namely, dynamics being uncoupled is not a barrier to converging to mixed-strategy NE when allowing higher-order learning.

More generally, the present results open new questions related to the discussion in (14) on what constitutes "natural" learning dynamics. In the case of anticipatory higher-order learning, there is a clear interpretation of the effect of higher-order terms. However, it is unclear how to interpret general higher-order dynamics. Furthermore, the results herein regarding inherent instability of higher-order dynamics that converge to the mixed-strategy NE of a coordination game suggests that higher-order learning can be "unnatural". Possible restrictions on dynamics, in addition to being uncoupled, could include having no asymptotic regret; maintaining qualitative behavior in the face of strategically equivalent games (cf., Section 5.2); or having an interpretable relationship between payoff streams and strategic evolutions such as "passivity", which generalizes and extends the notion of contractive games to contractive learning dynamics (e.g., [38; 39; 40]).

In terms of limitations, the current paper only addresses the payoff vector setup and does not address the setup where players only have access to instantaneous scalar payoffs. Furthermore, results in Sections 5 and 6 are limited to certain setups and require further generalization.

Figure 4: Stabilizing the mixed-strategy equilibrium of a coordination game and its consequences.

## References

* [1] D. Fudenberg and D. K. Levine, _The Theory of Learning in Games_. Economic Learning and Social Evolution Series, MIT Press, 1998.
* [2] D. Fudenberg and D. K. Levine, "Learning and equilibrium," _Annual Review of Economics_, vol. 1, no. 1, pp. 385-420, 2009.
* [3] S. Hart, "Adaptive heuristics," _Econometrica_, vol. 73, no. 5, pp. 1401-1430, 2005.
* [4] H. Young, _Strategic Learning and its Limits_. Arne Ryde memorial lectures, Oxford University Press, 2004.
* [5] S. Hart and A. Mas-Colell, "A simple adaptive procedure leading to correlated equilibrium," _Econometrica_, vol. 68, no. 5, pp. 1127-1150, 2000.
* [6] U. Berger, "Fictitious play in 2\(\times\)n games," _Journal of Economic Theory_, vol. 120, no. 2, pp. 139-154, 2005.
* [7] D. Monderer and L. S. Shapley, "Fictitious play property for games with identical interests," _Journal of Economic Theory_, vol. 68, no. 1, pp. 258-265, 1996.
* [8] J. S. Shamma and G. Arslan, "Unified convergence proofs of continuous-time fictitious play," _IEEE Transactions on Automatic Control_, vol. 49, no. 7, pp. 1137-1141, 2004.
* [9] L. S. Shapley, "Some topics in two-person games," in _Advances in Game Theory_ (L. Shapley, M. Dresher, and A. Tucker, eds.), pp. 1-29, Princeton, NJ: Princeton University Press, 1964.
* [10] D. P. Foster and H. Young, "On the nonconvergence of fictitious play in coordination games," _Games and Economic Behavior_, vol. 25, no. 1, pp. 79-96, 1998.
* [11] D. P. Foster and R. V. Vohra, "Calibrated learning and correlated equilibrium," _Games and Economic Behavior_, vol. 21, no. 1, pp. 40-55, 1997.
* [12] G. Piliouras and J. S. Shamma, "Optimization despite chaos: Convex relaxations to complex limit sets via Poincare recurrence," in _Proceedings of the 2014 Annual ACM-SIAM Symposium on Discrete Algorithms_, pp. 861-873, 2014.
* [13] H. P. Young, _Individual Strategy and Social Structure: An Evolutionary Theory of Institutions_. Princeton University Press, 1998.
* [14] S. Hart and A. Mas-Colell, _Simple Adaptive Strategies_. WORLD SCIENTIFIC, 2013.
* [15] C. Daskalakis, P. W. Goldberg, and C. H. Papadimitriou, "The complexity of computing a Nash equilibrium," _SIAM Journal on Computing_, vol. 39, no. 1, pp. 195-259, 2009.
* [16] Y. Babichenko and A. Rubinstein, "Communication complexity of approximate Nash equilibria," _Games and Economic Behavior_, vol. 134, pp. 376-398, 2022.
* [17] S. Hart and A. Mas-Colell, "Uncoupled dynamics do not lead to Nash equilibrium," _American Economic Review_, vol. 93, pp. 1830-1836, December 2003.
* [18] E.-V. Vlatakis-Gkaragkounis, L. Flokas, T. Laineas, P. Mertikopoulos, and G. Piliouras, "No-regret learning and mixed Nash equilibria: They do not mix," in _Advances in Neural Information Processing Systems_ (H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, eds.), vol. 33, pp. 1380-1391, Curran Associates, Inc., 2020.
* [19] J. Milionis, C. Papadimitriou, G. Piliouras, and K. Spendlove, "An impossibility theorem in game dynamics," _Proceedings of the National Academy of Sciences_, vol. 120, no. 41, 2023.
* [20] J. S. Shamma and G. Arslan, "Dynamic fictitious play, dynamic gradient play, and distributed convergence to Nash equilibria," _IEEE Transactions on Automatic Control_, vol. 50, pp. 312-327, March 2005.
* [21] M. Muehlebach and M. I. Jordan, "Optimization with momentum: Dynamical, control-theoretic, and symplectic perspectives," _J. Mach. Learn. Res._, vol. 22, no. 1, 2021.

* [22] C. Daskalakis and I. Panageas, "The limit points of (optimistic) gradient descent in min-max optimization," in _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* [23] T. Basar, "Relaxation techniques and asynchronous algorithms for on-line computation of non-cooperative equilibria," _Journal of Economic Dynamics and Control_, vol. 11, no. 4, pp. 531-549, 1987.
* [24] J. Conlisk, "Adaptation in games: Two solutions to the Crawford puzzle," _Journal of Economic Behavior & Organization_, vol. 22, no. 1, pp. 25-50, 1993.
* [25] S. Flam and J. Morgan, "Newtonian mechanics and Nash play," _International Game Theory Review_, vol. 06, 07 2003.
* [26] R. Laraki and P. Mertikopoulos, "Higher order game dynamics," _Journal of Economic Theory_, vol. 148, pp. 2666-2695, 06 2013.
* [27] B. Gao and L. Pavel, "On passivity, reinforcement learning and higher order learning in multiagent finite games," _IEEE Transactions on Automatic Control_, vol. 66, no. 1, pp. 121-136, 2021.
* [28] S.-H. Wang and E. Davison, "On the stabilization of decentralized control systems," _IEEE Transactions on Automatic Control_, vol. 18, no. 5, pp. 473-478, 1973.
* [29] E. Davison and T. Chang, "Decentralized stabilization and pole assignment for general proper systems," _IEEE Transactions on Automatic Control_, vol. 35, no. 6, pp. 652-664, 1990.
* [30] A. M. Krall, "An extension and proof of the root-locus method," _Journal of the Society for Industrial and Applied Mathematics_, vol. 9, no. 4, pp. 644-653, 1961.
* [31] A. M. Krall, "The root locus method: A survey," _SIAM Review_, vol. 12, no. 1, pp. 64-72, 1970.
* [32] G. Arslan and J. S. Shamma, "Anticipatory learning in general evolutionary games," in _Proceedings of the 45th IEEE Conference on Decision and Control_, pp. 6289-6294, 2006.
* [33] V. Syrgkanis, A. Agarwal, H. Luo, and R. E. Schapire, "Fast convergence of regularized learning in games," in _Advances in Neural Information Processing Systems_ (C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, eds.), vol. 28, Curran Associates, Inc., 2015.
* [34] X. Chen and B. Peng, "Hedging in games: Faster convergence of external and swap regrets," in _Advances in Neural Information Processing Systems_ (H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, eds.), vol. 33, pp. 18990-18999, Curran Associates, Inc., 2020.
* [35] C. Daskalakis, M. Fishelson, and N. Golowich, "Near-optimal no-regret learning in general games," in _Advances in Neural Information Processing Systems_ (M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), vol. 34, pp. 27604-27616, Curran Associates, Inc., 2021.
* [36] J. Jordan, "Three problems in learning mixed-strategy Nash equilibria," _Games and Economic Behavior_, vol. 5, no. 3, pp. 368-386, 1993.
* [37] D. Youla, J. Bongiorno, and C. Lu, "Single-loop feedback-stabilization of linear multivariable dynamical plants," _Automatica_, vol. 10, no. 2, pp. 159-173, 1974.
* [38] M. J. Fox and J. S. Shamma, "Population games, stable games, and passivity," _Games_, vol. 4, no. 4, pp. 561-583, 2013.
* [39] M. Arcak and N. C. Martins, "Dissipativity tools for convergence to Nash equilibria in population games," _IEEE Transactions on Control of Network Systems_, vol. 8, no. 1, pp. 39-50, 2021.
* [40] L. Pavel, "Dissipativity theory in game theory: On the role of dissipativity and passivity in Nash equilibrium seeking," _IEEE Control Systems Magazine_, vol. 42, no. 3, pp. 150-164, 2022.
* [41] J. P. Hespanha, _Linear Systems Theory: Second Edition_. Princeton University Press, 2018.
* [42] W. J. Rugh, _Linear System Theory_. Prentice Hall, 1996.

* [43] M. Hassouneh, H.-C. Lee, and E. Abed, "Washout filters in feedback control: benefits, limitations and extensions," in _Proceedings of the 2004 American Control Conference_, pp. 3950-3955, 2004.

## Appendix A Background on linear systems

Here, we review some standard background material for linear dynamics systems. There are several references (e.g., [41; 42]) with more detailed exposition.

### Notation

The partitioned matrix

\[\left(\begin{array}{c|c}A&B\\ \hline C&D\end{array}\right)\]

represents a general linear dynamical system

\[\dot{x}(t) =Ax(t)+Bu(t),\quad x(0)=x_{o}\] (16a) \[y(t) =Cx(t)+Du(t)\] (16b)

whose solution is

\[y(t)=Ce^{At}x_{o}+\int_{0}^{t}Ce^{A(t-\tau)}Bu(\tau)\,d\tau+Du(t).\]

The variables \(x\in\mathbb{R}^{n}\), \(u\in\mathbb{R}^{m}\), and \(y\in\mathbb{R}^{p}\) here are used temporarily as generic placeholders (and will play a different role in the ensuing discussion). The notation assigns the label \(P\) to the dynamics (16).

### Washout filters

In the special case of

\[P\sim\left(\begin{array}{c|c}-I&I\\ \hline-I&I\end{array}\right)\]

where

\[\begin{array}{l}\dot{x}(t)=-x(t)+u(t),\quad x(0)=x_{o}\\ y(t)=-x(t)+u(t)\end{array}\]

and \(x,u,y\in\mathbb{R}^{n}\), it is straightforward to show that if

\[\lim_{t\to\infty}u(t)=u^{*},\]

then

\[\lim_{t\to\infty}y(t)=0.\]

Linear systems with this property are known as "washout filters". For an extended discussion, see reference [43].

### Stability and stabilization

A matrix, \(M\), is a _stability matrix_ if, for every eigenvalue, \(\lambda\), of \(M\), \(\mathbf{Re}[\lambda]<0\).

The linear system, \(P\sim\left(\begin{array}{c|c}A&B\\ \hline C&D\end{array}\right)\) is _stable_ if \(A\) is a stability matrix.

The linear system \(K\sim\left(\begin{array}{c|c}E&F\\ \hline G&H\end{array}\right)\)_stabilizes_\(P\) if the combined linear dynamics

\[\dot{x} =Ax+Bu\] \[\dot{\xi} =E\xi+Fy\] \[u =G\xi+Hy\] \[y =Cx\]

or

\[\left(\begin{array}{c}\dot{x}\\ \dot{\xi}\end{array}\right)=\underbrace{\left(\begin{array}{cc}A+BHC&BG\\ FC&E\end{array}\right)}_{M}\left(\begin{array}{c}x\\ \xi\end{array}\right)\]

are stable, i.e., \(M\) is a stability matrix.

The following conditions are necessary and sufficient for the existence of such a \(K\). For all complex \(\lambda\) with \(\mathbf{Re}[\lambda]\geq 0\):

* The pair \((A,B)\) is _stabilizable_: \((A-\lambda I\quad B)=n\) has full row rank.
* The pair \((A,C)\) is _detectable_: \(\left(\begin{array}{c}C\\ A-\lambda I\end{array}\right)=n\) has full column rank.

The linear system \(K\)_strongly stabilizes_\(P\) if (i) \(K\) stabilizes \(P\) and (ii) \(E\) is a stability matrix. Necessary and sufficient conditions for the existence of a strongly stabilizing \(K\) are presented in reference (37).

### Decentralized stabilization

Let \(P\sim\left(\begin{array}{c|c}A&B\\ \hline C&0\end{array}\right)\), with \(A\) having dimensions \(n\times n\), have the structure

\[B=(B_{1}\quad...\quad B_{k})\ \ \&\ C=\left(\begin{array}{c}C_{1}\\ \vdots\\ C_{k}\end{array}\right)\]

for some integer \(k\). Suppose there exist linear systems

\[K_{1}\sim\left(\begin{array}{c|c}E_{1}&F_{1}\\ \hline G_{1}&H_{1}\end{array}\right),...,K_{k}\sim\left(\begin{array}{c|c}E_{ k}&F_{k}\\ \hline G_{k}&H_{k}\end{array}\right)\]

such that the combined linear dynamics

\[\dot{x} =Ax+(B_{1}\quad...\quad B_{k})\left(\begin{array}{c}u_{1}\\ \vdots\\ u_{k}\end{array}\right)\] \[\dot{\xi}_{1} =E_{1}\xi_{1}+F_{1}y_{1}\] \[\vdots\] \[\dot{\xi}_{k} =E_{k}\xi_{k}+F_{k}y_{k}\] \[u_{1} =G_{1}\xi_{1}+H_{1}y_{1}\] \[\vdots\] \[u_{k} =G_{k}\xi_{k}+H_{k}y_{k}\] \[y_{1} =C_{1}x\] \[\vdots\] \[y_{k} =C_{k}x\]are stable. Then the \((K_{1},...,K_{k})\) achieve _decentralized stabilization_ of \(P\). The previous conditions of \((A,B)\) stabilizable and \((A,C)\) detectable are necessary, but not sufficient, conditions for decentralized stabilization.

The following theorem from reference (29) provides necessary and sufficient conditions for decentralized stabilization. First, for any partition \(Q\cup R=\{1,2,...,k\}\) define \(B|^{Q}\) as the matrix formed by extracting the block columns of \(B=(B_{1}\quad...\quad B_{k})\) with indices in \(Q\), i.e.,

\[B|^{Q}=\begin{pmatrix}B_{q_{1}}&...&B_{q_{|Q|}}\end{pmatrix}\]

with \(\begin{Bmatrix}q_{1},...,q_{|Q|}\end{Bmatrix}=Q\). Likewise, define \(C|_{R}\) as the matrix formed from the block rows of \(C=\begin{pmatrix}C_{1}\\ \vdots\\ C_{k}\end{pmatrix}\), i.e.,

\[C|_{R}=\begin{pmatrix}C_{r_{1}}\\ \vdots\\ C_{r_{|R|}}\end{pmatrix}\]

with \(\begin{Bmatrix}r_{1},...,r_{|R|}\end{Bmatrix}=R\).

**Theorem A.1** ((29), Theorem 3).: _There exist \((K_{1},...,K_{k})\) that achieve decentralized stabilization of \(P\) if and only if_

\[\mathbf{rank}\begin{pmatrix}A-\lambda I&B|^{Q}\\ C|_{R}&0\end{pmatrix}=n\]

_for all complex \(\lambda\) with \(\mathbf{Re}[\lambda]\geq 0\) and all partitions, \(Q\cup R=\{1,2,...,k\}\)._

The above rank condition must hold for _all_ partitions \(Q\cup R=\{1,...,k\}\). If \(R=\emptyset\), one recovers the rank condition for (centralized) stabilizability. Likewise, \(Q=\emptyset\) results in the rank condition for detectability.

## Appendix B Proofs

### Proof of Theorem 4.1

We will examine the conditions of Theorem A.1 on the system (13)-(14). We need to inspect the rank of

\[\begin{pmatrix}\mathcal{A}-\lambda I&\mathcal{B}|^{Q}\\ \mathcal{C}|_{R}&0\end{pmatrix}\]

for all partitions \(Q\cup R=\{1,2,...,n\}\). Note that the partitions of either \(Q=\emptyset\) or \(R=\emptyset\) are already covered by Proposition 4.1.

First, note that

\[\begin{pmatrix}\mathcal{A}-\lambda I&\mathcal{B}|^{Q}\\ \mathcal{C}|_{R}&0\end{pmatrix}=\begin{pmatrix}\mathcal{M}-\lambda I&0& \begin{pmatrix}\mathcal{E}_{q_{1}}&...&\mathcal{E}_{q_{|Q|}}\\ \mathcal{M}&-(\lambda+1)I&0\\ \mathcal{M}|_{R}&-I|_{R}&0\end{pmatrix}.\]

Let \(\mathcal{M}\) be an \(\ell\times\ell\) matrix. Then

\[\ell=\sum_{i=1}^{n}(k_{i}-1).\]

We need the rank of the above matrix to be \(2\ell\) for all \(\lambda\) with \(\mathbf{Re}[\lambda]\geq 0\). Because of the presence of \(\mathcal{A}-\lambda I\), loss of rank below \(2\ell\) is only possible at eigenvalues of \(\mathcal{A}\). Since we are only concerned with \(\mathbf{Re}[\lambda]\geq 0\), we focus on eigenvalues of \(\mathcal{M}\) (which excludes \(\lambda=0\) by hypothesis, since \(\mathcal{M}\) is non-singular).

Without affecting the rank, we can multiply the bottom block row by \(-(\lambda+1)\) and add the middle block rows corresponding \(R\) to the rescaled bottom block row to get

\[\mathbf{rank}\begin{pmatrix}\mathcal{A}-\lambda I&\mathcal{B}|^{Q}\\ \mathcal{C}|_{R}&0\end{pmatrix}=\mathbf{rank}\begin{pmatrix}\mathcal{M}- \lambda I&0&\begin{pmatrix}\mathcal{E}_{q_{1}}&...&\mathcal{E}_{q_{|Q|}}\\ \mathcal{M}&-(\lambda+1)I&0\\ -\lambda\mathcal{M}|_{R}&0&0\end{pmatrix}.\]Switching the top and bottom block rows results in

\[\begin{pmatrix}-\lambda\mathcal{M}|_{R}&0&0\\ \mathcal{M}&-(\lambda+1)I&0\\ \mathcal{M}-\lambda I&0&\left(\mathcal{E}_{q_{1}}&...&\mathcal{E}_{q_{|Q|}} \right)\end{pmatrix}.\]

We can now exploit the block triangular structure. The bottom block row provides a row rank of

\[\sum_{q\in Q}(k_{q}-1),\]

The middle block row provides a row rank of \(\ell\). Finally, the top block row provides a row rank of

\[\sum_{r\in R}(k_{r}-1).\]

The last assertion is because \(\mathcal{M}\) is non-singular, by hypothesis, and therefore it has linearly independent rows. Since \(Q\cup R=\{1,...n\}\), we have the desired row rank of \(2\ell\).

### Proof of Lemma 5.1

Define

\[H(s)=C(sI-A)^{-1}B.\]

Since \(H(s)\) is a rational function, we can write it as

\[H(s)=\frac{p(s)}{q(s)},\]

for polynomials \(p\) and \(q\) that have no common roots. The assumption that \(CA^{m}B\neq 0\) for some \(m\) assures that \(H(s)\) is not identically equal to zero.

Suppose that for some \(\mu\) and \(s^{\prime}\) that is not an eigenvalue of \(A\),

\[q(s^{\prime})+\mu p(s^{\prime})=0.\]

Then \(s^{\prime}\) is an eigenvalue of \(A-\mu BC\), since

\[\begin{split}\mathbf{det}\left[s^{\prime}I-(A-\mu BC)\right]& =\mathbf{det}\left[s^{\prime}I-A\right]\mathbf{det}\left[I+\mu(s^{ \prime}I-A)^{-1}BC\right]\\ &=\mathbf{det}\left[s^{\prime}I-A\right](1+\mu C(s^{\prime}I-A)^ {-1}B)\\ &=\mathbf{det}\left[s^{\prime}I-A\right](q(s^{\prime})+\mu p(s^{ \prime}))\frac{1}{q(s^{\prime})}.\end{split}\]

Note that the roots of \(q(s)\) are a subset of the roots of \(\mathbf{det}\left[sI-A\right]\). For sufficiently large \(|s|\), we can rewrite \(H(s)\) as

\[H(s) =\frac{1}{s}C(I-\frac{1}{s}A)^{-1}B\] \[=\frac{1}{s}C\Big{(}\sum_{k=0}^{\infty}\frac{1}{s^{k}}A^{k}\Big{)}B.\]

By assumption, \(CB=0\) and \(CAB=0\), which implies that the first two terms of the series equal zero. Accordingly,

\[\lim\sup_{|s|\to\infty}|s|^{3}|H(s)|<\infty.\]

The main implication here is that the degree of \(q(s)\) is at least 3 more than the degree of \(p(s)\). Root-locus arguments in references (30) and (31) (asymptote rule) imply that

\[q(s)+\mu p(s)\]

has roots with positive real parts for large \(\mu\).

### Proof of Proposition 5.1

The local dynamics matrix in the game \(\Gamma(\mu)\) can be written in the form \(A-\mu BC\) with

\[A=\begin{pmatrix}0&0&0&G_{1}&-H_{1}&0&0&0&0\\ -(1+H_{3})&0&0&0&0&G_{3}&-H_{3}&0&0\\ 0&-(1+H_{2})&0&0&0&0&G_{2}&-H_{2}\\ 0&0&0&E_{1}&-F_{1}&0&0&0&0\\ 0&0&0&0&-1&0&0&0&0\\ -F_{3}&0&0&0&0&E_{3}&-F_{3}&0&0\\ -1&0&0&0&0&0&-1&0&0\\ 0&-F_{2}&0&0&0&0&0&E_{2}&-F_{2}\\ 0&-1&0&0&0&0&0&0&-1\end{pmatrix}\quad B=\begin{pmatrix}H_{1}+1\\ 0\\ 0\\ F_{1}\\ 0\\ 0\\ 0\\ 0\end{pmatrix}\] (17a) \[C=(0\quad 0\quad 1\quad 0\quad 0\quad 0\quad 0\quad 0\quad 0)\,,\] (17b)

where we reordered the variables according to \((w_{1},w_{3},w_{2},\xi_{1},v_{1},\xi_{3},v_{3},\xi_{2},v_{2})\) for convenience. Following the same arguments in proving Lemma 5.1, if \(CA^{m}B=0\) for all \(m\), then eigenvalues of \(A-BC\) are the same as the eigenvalues of \(A\). Writing \(A\) in block matrix form yields

\[A=\begin{pmatrix}A_{11}&A_{12}\\ A_{21}&A_{22}\end{pmatrix},\]

where \(A_{11}\) is \(3\times 3\). Notice that \(A_{11}\) is strictly lower triangular, and \(A_{21}\) is strictly block lower triangular. Now examine

\[\operatorname{\mathbf{det}}\left[sI-A\right]=\operatorname{\mathbf{det}} \left[sI-A_{11}\right]\operatorname{\mathbf{det}}\left[(sI-A_{22})-A_{21}(sI- A_{11})^{-1}A_{12}\right].\]

One can show that \(A_{21}(sI-A_{11})^{-1}A_{12}\) is strictly block lower triangular. Therefore, we have \(\operatorname{\mathbf{det}}\left[sI-A\right]=\operatorname{\mathbf{det}} \left[sI-A_{11}\right]\operatorname{\mathbf{det}}\left[sI-A_{22}\right].\) Thus, \(A\) has eigenvalues at 0 with multiplicity 3 or more because of \(A_{11}\). By exponential stability, there exists \(m\geq 2\) such that \(CA^{m}B\neq 0\). Since \(CB=0\), and \(CAB=0\), we can now apply Lemma 5.1.

### Proof of Proposition 6.1

Reference (37) presents a necessary and sufficient condition for strong stabilizability. First, we compute

\[T(s)=\mathcal{C}(sI-\mathcal{A})^{-1}\mathcal{B}=\frac{s}{s+1}\mathcal{M}(sI- \mathcal{M})^{-1}.\]

There is blocking zero (i.e., \(T(s)=0\)) at \(s=0\) and \(|s|\to\infty\). According to reference (37), a necessary and sufficient condition for strong stabilizability is that there should be an _even_ number of eigenvalues in between such pairs of real zeros. This property is known as the "parity interlacing principle". The eigenvalues of \(\mathcal{A}\) are \(-1,-1,\pm\sqrt{m_{12}m_{21}},\) and so there is a single (and hence, an odd number) of real eigenvalue in between two real zeros of \(T(s)\).

## Appendix C Destabilization with a sufficiently small \(\mu\)

The proof in Section 5 can be modified to consider non-convergence over bounded games, e.g.,

\[\|M_{ij}\|_{\alpha}<1\quad\forall i,j.\]

Consider first the following lemma.

**Lemma C.1**.: _Let \(A\in\mathbb{R}^{n\times n}\), \(B\in\mathbb{R}^{n\times 1}\) and \(C\in\mathbb{R}^{1\times n}\). Assume that \(A\) has eigenvalues at \(0\) with multiplicity 3 or more. Then for sufficiently small \(\mu>0\), \(A-\mu BC\) is not a stability matrix._

Proof.: As in the proof of Lemma 5.1, we have

\[\operatorname{\mathbf{det}}\left[sI-(A-\mu BC)\right]=\operatorname{ \mathbf{det}}\left[sI-A\right](q(s)+\mu p(s))\frac{1}{q(s)}.\]

Recall that the roots of \(q(s)\) are a subset of the roots of \(\operatorname{\mathbf{det}}\left[sI-A\right]\). If \(q(s)\) does not have at least 3 roots at zero, then \(A-\mu BC\) is not a stability matrix. Otherwise, root-locus arguments in references (30), and (31) (angle of departure rule) imply there exist roots of \(q(s)+\mu p(s)\) with positive real parts for small \(\mu\)Using the structure of the local dynamics and the fact that \(A\) in (17a) has eigenvalues at 0 with multiplicity 3 or more, one can directly use Lemma C.1 to show the existence of a sufficiently small offending \(\mu\).