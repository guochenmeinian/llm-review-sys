# Eagle : Efficient Adaptive Geometry-based Learning in Cross-view Understanding

 Thanh-Dat Truong\({}^{1}\), Utsav Prabhu\({}^{2}\), Dongyi Wang\({}^{3}\)

**Bhiksha Raj\({}^{4,5}\), Susan Gauch\({}^{6}\), Jeyamkondan Subbiah\({}^{7}\), Khoa Luu\({}^{1}\)**

\({}^{1}\)CVIU Lab, University of Arkansas, USA \({}^{2}\)Google DeepMind, USA

\({}^{3}\)Dep. of BAEG, University of Arkansas, USA \({}^{4}\)Carnegie Mellon University, USA

\({}^{5}\)Mohammed bin Zayed University of AI, UAE

\({}^{6}\)Dep. of EECS, University of Arkansas, USA \({}^{7}\)Dep. of FDSC, University of Arkansas, USA

{tt032, dongyiw, ggauch, jsubbiah, khoaluu}@uark.edu

bhiksha@cs.cmu.edu, utsavprabhu@google.com

https://uark-cviu.github.io/projects/EAGLE

###### Abstract

Unsupervised Domain Adaptation has been an efficient approach to transferring the semantic segmentation model across data distributions. Meanwhile, the recent Open-vocabulary Semantic Scene understanding based on large-scale vision language models is effective in open-set settings because it can learn diverse concepts and categories. However, these prior methods fail to generalize across different camera views due to the lack of cross-view geometric modeling. At present, there are limited studies analyzing cross-view learning. To address this problem, we introduce a novel Unsupervised Cross-view Adaptation Learning approach to modeling the geometric structural change across views in Semantic Scene Understanding. First, we introduce a novel Cross-view Geometric Constraint on Unpaired Data to model structural changes in images and segmentation masks across cameras. Second, we present a new Geodesic Flow-based Correlation Metric to efficiently measure the geometric structural changes across camera views. Third, we introduce a novel view-condition prompting mechanism to enhance the view-information modeling of the open-vocabulary segmentation network in cross-view adaptation learning. The experiments on different cross-view adaptation benchmarks have shown the effectiveness of our approach in cross-view modeling, demonstrating that we achieve State-of-the-Art (SOTA) performance compared to prior unsupervised domain adaptation and open-vocabulary semantic segmentation methods.

## 1 Introduction

Modern segmentation models [3; 4; 63] have achieved remarkable results on the close-set training with a set of pre-defined categories and concepts. To work towards human-level perception where the scenes are interpreted with diverse categories and concepts, the open-vocabulary (open-vocab) perception model [38; 40] based on the power of large vision-language models [30; 39] has been introduced to address the limitations of close-set training. By using the power of language as supervision, the large-scale vision language model is able to learn the more powerful representations where languages offer better reasoning mechanisms and open-word concept representations compared to traditional close-set training methods [3; 63; 9].

Figure 1: _Our Proposed Cross-view Adaptation Learning Approach._ Prior models, e.g., FreeSeg [38], DenseCLIP [40], trained on the car view do not perform well on the drone-view images. Meanwhile, our cross-view adaptation approach is able to generalize well from the car to drone view.

Recent work is inspired by the success of large vision-language models [39; 27] that are able to learn informative feature representations of both visual and textual inputs from large-scale image-text pairs. These have been adopted to further develop open-vocab semantic segmentation models [38; 40; 31; 29] that can work well in open-world environments. However, the open-vocab perception models remain unable to generalize across camera viewpoints. As shown in Fig. 1, the open-vocab model trained on car views is not able to perform well on the images captured from unmanned aerial vehicles (UAVs) or drones. While this issue can be improved by training the segmentation model on drone-view data, the annotation process of high-resolution UAV data is costly and time-consuming. At present, there exist many large-scale datasets with dense labels captured from camera views on the ground, e.g., car views (SYNTHIA [44], GTA [43], Cityscapes [11], BDD100K [68]). They have been widely adopted to develop robust perception models. Since these car view and drone view datasets have many common objects of interest, incorporating knowledge from car views with drone views benefits the learning process by reusing large-scale annotations and saving efforts of manually labeling UAV images. Unsupervised domain adaptation (UDA) [58; 23; 1; 51; 53] is one of the potential approaches to transfer the knowledge from the car view (i.e., source domain) to the drone view (i.e., target domain). While UDA approaches have shown their effectiveness in transferring knowledge across domains, e.g., environment changes or geographical domain shifts, these methods remain limited in the cases of changing camera viewpoints. Indeed, the changes in camera positions, e.g., from the ground of cars to the high positions of drones, bring a significant difference in structures and topological layouts of scenes and objects (Fig. 2). Therefore, UDA is not a complete solution to this problem due to its lack of cross-view structural modeling. Additionally, although the open-vocab segmentation models have introduced several prompting mechanisms, e.g., context-aware prompting [40] or adaptive prompting [38] to improve context learning across various open-world concepts, they are unable to model the cross-view structure due to the lack of view-condition information in prompts and geometric modeling. To the best of our knowledge, there are limited studies that have exploited this cross-view learning. These limitations motivate us to develop a new adaptation learning paradigm, i.e., _Unsupervised Cross-view Adaptation_, that addresses prior methods to improve the performance of semantic segmentation models across views.

**Contributions:** This work introduces a novel _Efficient Adaptive Geometry-based Learning (EAGLE)_ to _Unsupervised Cross-view Adaptation_ that can adaptively learn and improve the performance of semantic segmentation models across camera viewpoints. First, by analyzing the geometric correlations across views, we introduce a novel _cross-view geometric constraint_ on _unpaired data_ of structural changes in images and segmentation masks. Second, to efficiently model _cross-view geometric structural changes_, we introduce a new _Geodesic Flow-based Metric_ to measure the structural changes across views via their manifold structures. In addition, to further improve the prompting mechanism of the open-vocab segmentation network in cross-view adaptation learning, we introduce a new _view-condition prompting_. Then, our _cross-view geometric constraint_ is also imposed on its feature representations of view-condition prompts to leverage its geometric knowledge embedded in our prompting mechanism. Our proposed method holds a promise to be an effective approach to addressing the problem of cross-view learning and contributes to improving UDA and open-vocab segmentation in cross-view learning. Thus, it increases the generalizability of the segmentation models across camera views. Finally, our experiments on three presented cross-view adaptation benchmarks, i.e., SYNTHIA \(\rightarrow\) UAVID, GTA \(\rightarrow\) UAVID, BDD \(\rightarrow\) UAVID, illustrate the effectiveness of our approach in cross-view modeling and our State-of-the-Art (SOTA) performance.

## 2 Related Work

**Unsupervised Domain Adaptation** Adversarial learning [6; 58; 59] and self-supervised learning [1; 69; 23; 14] are common approaches to UDA in semantic segmentation. The adversarial learning approaches are typically simultaneously trained on source and target data [58; 7; 6]. Chen et al. [7] first introduced an adversarial framework to domain adaptation. Later, several approaches improved

Figure 2: An Example of Illustration of Cross-View Adaptation From Car View to Drone View.

adversarial learning by utilizing generative models [74; 34; 21], using additional labels [28; 59], incorporating with entropy minimization [58; 65; 51; 52], or adopting the curriculum training [37]. Recently, the self-supervised approaches [1; 69; 23; 14] have achieved outstanding performance. Araslanov et al. [1] first proposed a self-supervised augmentation consistency framework for UDA. Hoyer et al. [23] utilized Transformers to improve the UDA performance. Later, this approach was further improved by utilizing multi-resolution cropped images [24] and masked image consistency strategy [25] to enhance contextual learning. Recent studies improved the self-supervised approach by aligning both output and attention levels via the cross-domain prediction consistency framework [60], using a prototypical representation [69], learning the cross-model consistency via depths [67], improving the class-relevant fairness [53; 55; 56], or exploring the relations of pseudo-labels [71]. Fashes et al. [15] introduced a prompt-based feature augmentation method to zero-shot UDA. Gong et al. [18] introduced a geodesic flow kernel to model the manifold structure between domains. Later, Simon et al. [47] designed distillation loss by the geodesic flow path.

**Vision-Language and Open-Vocab Segmentation** By pre-training on a large-scale vision-language dataset [39; 27], the vision-language models can learn various visual concepts and can further be transferred to other vision problems through "_prompting_" [17; 38; 31; 35], e.g., open-vocab segmentation [64; 13; 38]. Li et al. [29] first introduced the language-driving approach to semantic segmentation. Rao et al. [40] represented a context-aware prompting mechanism for dense prediction tasks. Ghiasi et al. [17] proposed an OpenSeg framework that learns the visual-semantic alignments. Qin et al. [38] presented a unified, universal, and open-vocab segmentation network based on Mask2Former [8] with an adaptive prompting mechanism. Xu et al. [64] proposed a two-stage open-vocab segmentation framework using the mask proposal generator and the pre-trained CLIP model. Ding et al. [13] decoupled the zero-shot semantic segmentation to class-agnostic segmentation and segment-level zero-shot classification. Liang et al. [31] improved the two-stage open-vocab segmentation model by further fine-tuning CLIP on masked image regions and corresponding descriptions.

**Cross-view Learning** The early studies exploited cross-view learning in geo-localization by using a polar transform across views [46; 45] or generative networks to cross-view images [41; 49]. Meanwhile, Zhu et al. [75] exploited the correlation between street- and aerial-view data via self-attention. In semantic segmentation, Coors et al. [10] first introduced a cross-view adaptation approach utilizing the depth labels and the cross-view transformation between car and truck views. However, this change of views in [10] is not as big a hurdle as the change of views in our problem, i.e., car view to drone view. Ren et al. [42] presented an adaptation approach across viewpoints using the 3D models of scenes to create pairs of cross-view images. Vidit et al. [57] modeled the geometric shift in cross FoV setting for object detection by learning position-invariant homography transform. Di Mauro et al. [12] introduced an adversarial method trained on a multi-view synthetic dataset where images are captured from different pitch and yaw angles at the same altitudes of the camera positions. Meanwhile, in our problem, the camera views could be placed at different altitudes (e.g., the car and the drone), which reveals large structural differences between the images. Truong et al. [50; 54] first introduced a simple approach to model the relation across views. CROVIA [50] measures the cross-view structural changes by measuring the distribution shift and only focuses on the cross-view adaptation setting in semantic segmentation. However, these methods [50; 54] lack a theory and a mechanism for cross-view geometric structural change modeling. To the best of our knowledge, there are limited studies exploiting cross-view adaptation in semantic segmentation. Therefore, our work presents a new approach to model the geometric correlation across views.

## 3 The Proposed EAGLE Approach

In this paper, we consider cross-view adaptation learning as UDA where the images of the source and target domains are captured from different camera positions (Fig. 2). Formally, let \(\mathbf{x}_{s},\mathbf{x}_{t}\) be the input images in the source and target domains, \(\mathbf{p}_{s},\mathbf{p}_{t}\) be the the corresponding prompts, and \(\mathbf{y}_{s},\mathbf{y}_{t}\) be the segmentation masks of \(\mathbf{x}_{s},\mathbf{x}_{t}\). Then, the open-vocab segmentation model \(F\) maps the input \(\mathbf{x}\) and the prompt \(\mathbf{p}\) to the corresponding output \(\mathbf{y}=F(\mathbf{x},\mathbf{p})\). It should be noted that in the case of traditional semantic segmentation, the prompt \(\mathbf{p}\) will be ignored, i.e., \(\mathbf{y}=F(\mathbf{x})\) The cross-view adaptation learning can be formulated as Eqn. (1).

\[\arg\min_{\theta}\left[\mathbb{E}_{\mathbf{x}_{s},\mathbf{p}_{s},\hat{ \mathbf{y}}_{s}}\mathcal{L}_{Mask}(\mathbf{y}_{s},\hat{\mathbf{y}}_{s})+ \mathbb{E}_{\mathbf{x}_{t},\mathbf{p}_{t}}\mathcal{L}_{Adapt}(\mathbf{y}_{t})\right]\] (1)where \(\theta\) is the parameters of \(F\), \(\mathbf{\hat{y}}_{s}\) is the ground truth, \(\mathcal{L}_{Mask}\) is the supervised (open-vocab) segmentation loss with ground truths, and \(\mathcal{L}_{Adapt}\) is unsupervised adaptation loss from the source to the target domain. In the open-vocab setting, we adopt the design of Open-Vocab Mask2Former [8; 38] to our network \(F\). Prior UDA methods defined the adaptation loss \(\mathcal{L}_{Adapt}\) via the adversarial loss [28; 5], entropy loss [51; 58], or self-supervised loss [23; 25]. Although these prior results have illustrated their effectiveness in UDA, these losses remain limited in cross-view adaptation setup. Indeed, the adaptation setting in prior studies [58; 1; 23; 15] is typically deployed in the context of environmental changes (e.g., simulation to real [58; 59; 15], day to night [25; 15], etc) where the camera positions between domains remain similar. Meanwhile, in cross-view adaptation, the camera position of the source and target domain remains largely different (as shown in Fig. 2). This change in camera positions leads to significant differences in the geometric layout and topological structures between the source and target domains. As a result, direct adoption of prior UDA approaches to cross-view adaptation would be ineffective due to the lack of cross-view geometric correlation modeling. To effectively address cross-view adaptation, the adaptation loss \(\mathcal{L}_{Adapt}\) should be able to model (1) _the geometric correlation between two views of source and target domains_ and (2) _the structural changes across domains_.

### Cross-View Geometric Modeling

To efficiently address the cross-view adaptation learning task, it is essential to explicitly model cross-view geometric correlations by analyzing the relation between two camera views. Therefore, we first re-reconsider the cross-view geometric correlation. In particular, let \(\mathbf{\bar{x}}_{t}\) be the corresponding image of \(\mathbf{x}_{s}\) captured from the target view, \(\mathbf{y}_{s}\) and \(\mathbf{\bar{y}}_{t}\) be the semantic segmentation outputs of source image \(\mathbf{x}_{s}\) and target image \(\mathbf{\bar{x}}_{t}\), \(\mathbf{\bar{p}}_{t}\) be the corresponding prompt of \(\mathbf{p}_{s}\) in target view, respectively. Formally, the images captured from the source and the target views can be modeled as Eqn. (2).

\[\mathbf{x}_{s}=\mathcal{R}(\mathbf{K}_{s},[\mathbf{R}_{s},\mathbf{t}_{s}], \Theta),\quad\mathbf{\bar{x}}_{t}=\mathcal{R}(\mathbf{K}_{t},[\mathbf{R}_{t},\mathbf{t}_{t}],\Theta)\] (2)

where \(\mathcal{R}\) is the rendering function, \(\mathbf{K}_{s}\) and \(\mathbf{K}_{t}\) are the intrinsic matrices, \([\mathbf{R}_{s},\mathbf{t}_{s}]\) and \([\mathbf{R}_{t},\mathbf{t}_{t}]\) are the extrinsic matrices, and \(\Theta\) represents the capturing scene. In addition, as the camera parameters of both source and target views are represented by matrices, there should exist linear transformations of camera parameters between two views as follow,

\[\mathbf{K}_{t}=\mathbf{T}_{\mathbf{K}}\times\mathbf{K}_{s},\quad[\mathbf{R}_{t },\mathbf{t}_{t}]=\mathbf{T}_{\mathbf{R}\mathbf{t}}\times[\mathbf{R}_{s}, \mathbf{t}_{s}]\] (3)

where \(\mathbf{T}_{\mathbf{K}}\) and \(\mathbf{T}_{\mathbf{R}\mathbf{t}}\) are the transformation matrices.

_Remark 1: The Geometric Transformation Between Camera Views._ From Eqn. (2) and Eqn. (3), we argue that there should exist a geometric transformation \(\mathcal{T}\) of images between two camera views as: \(\mathbf{\bar{x}}_{t}=\mathcal{T}(\mathbf{x}_{s};\mathbf{T}_{\mathbf{K}}, \mathbf{T}_{\mathbf{R}\mathbf{t}})\).

_Remark 2: The Equivalent Transformation Between Image and Segmentation Output._ As RGB images and segmentation maps are pixel-wised corresponding, the same geometric transformation \(\mathcal{T}\) in the image space can be adopted for segmentation space as: \(\mathbf{\bar{y}}_{t}=\mathcal{T}(\mathbf{y}_{s};\mathbf{T}_{\mathbf{K}}, \mathbf{T}_{\mathbf{R}\mathbf{t}})\)

Remarks 1-2 have depicted that the geometric transformation of both image and segmentation from the source to the target view can be represented by the shared transformation \(\mathcal{T}\) with the camera transformation matrices \(\mathbf{T}_{\mathbf{K}},\mathbf{T}_{\mathbf{R}\mathbf{t}}\). Let \(\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{\bar{x}}_{t})\) and \(\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{\bar{y}}_{t})\)_be the metrics the measure the cross-view structures changes_ of images and segmentation maps from the source to target domains.

We argue that the cross-view geometric correlation in the image space, i.e., \(\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{\bar{x}}_{t})\), is theoretically proportional to the one in the segmentation space, i.e., \(\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{\bar{y}}_{t})\). Since the camera transformations between the two views are linear (Eqn. (3)) and the images \(\mathbf{x}\) and outputs \(\mathbf{y}\) are pixel-wised corresponding, we hypothesize that the cross-view geometric correlation in the image space \(\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{\bar{x}}_{t})\) and the segmentation space \(\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{\bar{y}}_{t})\) can be modeled by a linear relation with linear scale \(\alpha\) as follows:

\[\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{\bar{x}}_{t})\propto\mathcal{D}_{y}( \mathbf{y}_{s},\mathbf{\bar{y}}_{t})\Leftrightarrow\mathcal{D}_{x}(\mathbf{x} _{s},\mathbf{\bar{x}}_{t})=\alpha\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{\bar{y }}_{t})\] (4)

### Cross-view Geometric Learning on Unpaired Data

Eqn. (4) defines a necessary condition to explicitly model the cross-view geometric correlation. Therefore, cross-view adaptation learning in Eqn. (1) can be re-formed as follows:

\[\theta^{*}=\arg\min_{\theta}\Big{[}\mathbb{E}_{\mathbf{x}_{s},\mathbf{p}_{s}, \mathbf{\hat{y}}_{s}}\mathcal{L}_{Mask}(\mathbf{y}_{s},\mathbf{p}_{s},\mathbf{ \hat{y}}_{s})+\mathbb{E}_{\mathbf{x}_{s},\mathbf{p}_{s},\mathbf{\bar{x}}_{t}, \mathbf{\bar{p}}_{t}}||\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{\bar{x}}_{t})- \alpha\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{\bar{y}}_{t})||\Big{]}\] (5)where, \(\mathcal{L}_{Adapt}(\mathbf{y}_{s},\mathbf{\bar{y}}_{t})=||\mathcal{D}_{x}( \mathbf{x}_{s},\mathbf{\bar{x}}_{t})-\alpha\mathcal{D}_{y}(\mathbf{y}_{s}, \mathbf{\bar{y}}_{t})||\) is the cross-view geocentric adaptation loss, \(||\cdot||\) is the mean squared error loss. However, in practice, the pair data between source and target views are inaccessible as data from these two views are often collected independently. Thus, optimizing Eqn. (5) without cross-view pairs of data remains an ill-posed problem. To address this limitation, instead of learning Eqn. (5) on paired data, we proposed to model this correlation on unpaired data. Instead of solving the cross-view geometric constraint of Eqn. (5) on pair data, let us consider all cross-view unpaired samples \((\mathbf{x}_{s},\mathbf{x}_{t})\). Formally, learning the _Cross-view Geometric Constraint_ between unpaired samples can be formulated as in Eqn. (6).

\[\theta^{*}=\arg\min_{\theta}\left[\mathbb{E}_{\mathbf{x}_{s},\mathbf{\hat{y}} _{s}}\mathcal{L}_{Mask}(\mathbf{y}_{s},\mathbf{p}_{s},\mathbf{\hat{y}}_{s})+ \mathbb{E}_{\mathbf{x}_{s},\mathbf{p}_{s},\mathbf{x}_{t},\mathbf{p}_{t}}|| \mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{x}_{t})-\alpha\mathcal{D}_{y}(\mathbf{ y}_{s},\mathbf{y}_{t})||\right]\] (6)

where \(\mathbf{x}_{s}\) and \(\mathbf{x}_{t}\) are unpaired data, and \(\mathcal{L}_{Adapt}(\mathbf{y}_{s},\mathbf{y}_{t})=||\mathcal{D}_{x}( \mathbf{x}_{s},\mathbf{x}_{t})-\alpha\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{y} _{t})||\) is the _Cross-view Geometric Adaptation_ loss on unpaired data. Intuitively, although the cross-view pair samples are not available, the cross-view geometric constraints on paired samples between two views can be indirectly imposed by modeling the cross-view geometric structural constraint among unpaired samples. Then, by modeling the cross-view structural changes in the image and segmentation spaces, the structural change on images of unpaired data could be considered as the reference for the cross-view structural change in the segmentation space during the optimization process. This action promotes the structures of segmentation that can be effectively adapted from the source view to the target view. Importantly, the cross-view geometric constraint imposed on unpaired data can be mathematically proved as an upper bound of the cross-view constraint on paired data as follows:

\[||\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{\bar{x}}_{t})-\alpha\mathcal{D}_{y}( \mathbf{y}_{s},\mathbf{\bar{y}}_{t})||=\mathcal{O}\left(\mathcal{D}_{x}(|| \mathbf{x}_{s},\mathbf{x}_{t})-\alpha\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{y} _{t})||\right)\] (7)

where \(\mathcal{O}\) is the Big O notation. The upper bound in Eqn. (19) can be proved by using the properties of triangle inequality and our correlation metrics \(\mathcal{D}_{x}\) and \(\mathcal{D}_{y}\) (Sec. 3.3). The detailed proof is provided in the appendix. Eqn. (19) has illustrated that by minimizing the cross-view geometric constraint on unpaired samples in Eqn. (6), the cross-view constraint on paired samples in Eqn. (5) is also maintained due to the upper bound. Therefore, our proposed Cross-view Geometric Constraint loss _does NOT require the pair data between source and target views_ during training. Fig. 3 illustrates our cross-view adaptation learning framework.

### Cross-view Structural Change Modeling via Geodesic Flow Path

Modeling the correlation metrics \(\mathcal{D}_{x}\) and \(\mathcal{D}_{y}\) is an important task in our approach. Indeed, the metrics should be able to model the structure changes from the source to the target view. Intuitively, the changes from the source to the target view are essentially the geodesic flow between two subspaces on the Grassmann manifold. Then, the images (or segmentation) of two views can be projected along the geodesic flow path to capture the cross-view structural changes. Therefore, to model \(\mathcal{D}_{x}\) and \(\mathcal{D}_{y}\), we adopt the _Geodesic Flow_ path to measure the cross-view structural changes by modeling the geometry in the latent space.

**Remark 3: Grassmann Manifold** is the set of \(N\)-dimensional linear subspaces of \(\mathbb{R}^{D}(0<N<D)\), i.e, \(\mathcal{G}(N,D)\). A matrix with orthonormal columns \(\mathbf{P}\in\mathbb{R}^{D\times N}\) define a subspace of \(\mathcal{G}(N,D)\), i.e., \(\mathbf{P}\in\mathcal{G}(N,D)\Rightarrow\mathbf{P}^{\top}\mathbf{P}=\mathbf{I }_{N}\) where \(\mathbf{I}_{N}\) is the \(N\times N\) identity matrix.

For simplicity, we present our approach to model the cross-view structural change \(\mathcal{D}_{x}\) in the image space. Formally, let \(\mathbf{P}_{s}\) and \(\mathbf{P}_{t}\) be the basis of the source and target domains. These bases can be obtained by the PCA algorithm. The geodesic flow between \(\mathbf{P}_{s}\) and \(\mathbf{P}_{t}\) in the manifold can be defined via the function \(\boldsymbol{\Pi}:\nu\in[0..1]\rightarrow\boldsymbol{\Pi}(\nu)\), where \(\boldsymbol{\Pi}(\nu)\in\mathcal{G}(N,D)\) is the subspace lying on

Figure 3: **Our Cross-View Learning Framework.**

the geodesic flow path from the source to the target view:

\[\bm{\Pi}(\nu)=[\mathbf{P}_{s}\ \ \ \mathbf{R}][\mathbf{U}_{1}\bm{\Gamma}(\nu)\ \ \ -\ \mathbf{U}_{2}\bm{\Sigma}(\nu)]^{\top}\] (8)

where \(\mathbf{R}\in\mathbb{R}^{D\times(D-N)}\) is the orthogonal complement of \(\mathbf{P}_{s}\), i.e., \(\mathbf{R}^{\top}\mathbf{P}_{s}=\mathbf{0}\). \(\bm{\Gamma}(\nu)\) and \(\bm{\Sigma}(\nu)\) are the diagonal matrices whose diagonal element at row \(i\) can be defined as \(\gamma_{i}=\cos(\nu\omega_{i})\) and \(\sigma_{i}=\sin(\nu\omega_{i})\). The list of \(\omega_{i}\) is the principal angles between source and target subspaces, i.e., \(0\leq\omega_{1}\leq...\leq\omega_{N}\leq\frac{\pi}{2}\). \(\mathbf{U}_{1}\) and \(\mathbf{U}_{2}\) are the orthonormal matrices obtained by the following pair of SVDs:

\[\mathbf{P}_{s}^{\top}\mathbf{P}_{T}=\mathbf{U}_{1}\bm{\Gamma}(1)\mathbf{V}^{ \top}\ \ \ \ \ \ \mathbf{R}^{\top}\mathbf{P}_{T}=-\mathbf{U}_{2}\bm{\Sigma}(1)\mathbf{V}^{\top}\] (9)

Since \(\mathbf{P}_{s}^{\top}\mathbf{P}_{t}\) and \(\mathbf{R}^{\top}\mathbf{P}_{t}\) share the same singular vectors \(\mathbf{V}\), we adopt the generalized Singular Value Decomposition (SVD) [18; 47] to decompose the matrices. In our approach, we model the cross-view structural changes \(\mathcal{D}_{x}\) by modeling the cosine similarity between projections along the geodesic flow \(\bm{\Pi}(\nu)\). In particular, given a subspace \(\bm{\Pi}(\nu)\) on the geodesic flow path from the source to the target view, the cross-view geometric correlation of images between the source and target views can formulated by the inner product \(g_{\bm{\Pi}(\nu)}(\mathbf{x}_{s},\mathbf{x}_{t})\) along the geodesic flow \(\bm{\Pi}(\nu)\) as follows:

\[g(\mathbf{x}_{s},\mathbf{x}_{t})=\int_{0}^{1}g_{\bm{\Pi}(\nu)}( \mathbf{x}_{s},\mathbf{x}_{t})d\nu=\int_{0}^{1}\mathbf{x}_{s}^{\top}\bm{\Pi}( \nu)\bm{\Pi}(\nu)^{\top}\mathbf{x}_{t}d\nu=\mathbf{x}_{s}^{\top}\left(\int_{0 }^{1}\bm{\Pi}(\nu)\bm{\Pi}(\nu)^{\top}d\nu\right)\mathbf{x}_{t}=\mathbf{x}_{s }^{\top}\mathbf{Q}\mathbf{x}_{t}\] (10)

where \(\mathbf{Q}=\int_{0}^{1}\bm{\Pi}(\nu)\bm{\Pi}(\nu)^{\top}d\nu\). Intuitively, the matrix \(\mathbf{Q}\) represents the manifold structure between the source to the target view. Then, Eqn. (10) measures the cross-view structural changes between the source and the target domain based on their manifold structures. The matrix \(\mathbf{Q}\) can be obtained in a closed form [18; 47] as follows:

\[\mathbf{Q}=[\mathbf{P}_{s}\mathbf{U}_{1}\ \ \ \ \mathbf{RU}_{2}]\begin{bmatrix} \mathbf{\Lambda}_{1}&\mathbf{\Lambda}_{2}\\ \mathbf{\Lambda}_{2}&\mathbf{\Lambda}_{3}\end{bmatrix}\begin{bmatrix} \mathbf{U}_{1}^{\top}\mathbf{P}_{s}^{\top}\\ \mathbf{U}_{2}^{\top}\mathbf{R}^{\top}\end{bmatrix}\] (11)

where \(\bm{\Lambda}_{1}\), \(\bm{\Lambda}_{2}\), and \(\bm{\Lambda}_{3}\) are the diagonal matrices, whose diagonal elements at row \(i\) can be defined as:

\[\lambda_{1,i}=1+\frac{\sin(2\omega_{i})}{2\omega_{i}},\ \lambda_{2,i}=\frac{ \cos(2\omega_{i})-1}{2\omega_{i}},\ \lambda_{3,i}=1-\frac{\sin(2\omega_{i})}{2\omega_{i}}\] (12)

In practice, we model the cross-view structural changes \(\mathcal{D}_{x}\) via the cosine similarity along the geodesic flows. Finally, the cross-view structural changes \(\mathcal{D}_{x}\) can be formulate as:

\[\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{x}_{t})=1-\frac{\mathbf{x}_{s}^{\top} \mathbf{Q}\mathbf{x}_{t}}{||\mathbf{Q}^{1/2}\mathbf{x}_{s}||||\mathbf{Q}^{1/ 2}\mathbf{x}_{t}||}\] (13)

Similarly, we can model the cross-view geometric correlation of segmentation \(\mathcal{D}_{y}\) via Geodesic Flow.

### View-Condition Prompting to Cross-View Learning

**View-Condition Prompting** Previous efforts [40; 38; 16; 73] in open-vocab segmentation have shown that a better prompting mechanism can provide more meaningful textual and visual knowledge. Prior work in open-vocab segmentation designed the prompt via the class names [64; 13; 38], e.g., "\(\text{class}_{1}\), \(\text{class}_{1}\),..., \(\text{class}_{K}\)". Meanwhile, other methods improve the prompting mechanism by introducing the learnable variables into the prompt [40] or adding the task information [38]. This action helps to improve the context learning of the vision-language model. In our approach, we also exploit the effectiveness of designing prompting to cross-view learning. In particular, describing the view information can further improve the visual context learning, e.g., "\(\text{class}_{1}\), \(\text{class}_{1}\),..., \(\text{class}_{K}\) captured from the [domain] view", where [domain] could be car (source domain) or drone (target domain). Therefore, we introduce a view-condition prompting mechanism by introducing the view information, i.e., captured from the [domain] view", into the prompt. Our view-condition prompt offers the context specific to visual learning, thus providing better transferability in cross-view segmentation.

**Cross-view Correlation of View-Condition Prompts** We hypothesize that the correlation of the input prompts across domains also provides the cross-view geometric correlation in their deep representations. In particular, let \(\mathbf{f}_{s}^{p}\) and \(\mathbf{f}_{t}^{p}\) be the deep textual embeddings of view-condition prompts \(\mathbf{p}_{s}\) and \(\mathbf{p}_{t}\), and \(\mathcal{D}_{p}\) be metric measuring the correlation between \(\mathbf{f}_{s}^{p}\) and \(\mathbf{f}_{t}^{p}\). In addition, since the textual encoder has been pre-trained on large-scale vision-language data [39; 27], the visual and the textual representations have been well aligned. Then, we argue that the correlation of textual feature representations across views, i.e., \(\mathcal{D}_{p}(\mathbf{f}_{s}^{p},\mathbf{f}_{t}^{p})\), also provides the cross-view geometric correlation due to the embedded view information in the deep representation of prompts aligned with visual representations. Therefore, similar to Eqn. (4), we hypothesize the cross-view correlation of segmentation masks and textual features can be modeled as a linear relation with a scale factor \(\gamma\) as:

\[\mathcal{D}_{p}(\mathbf{f}_{s}^{p},\mathbf{f}_{t}^{p})\propto\mathcal{D}_{y}( \mathbf{y}_{s},\mathbf{y}_{t})\Leftrightarrow\mathcal{D}_{p}(\mathbf{f}_{s}^{ p},\mathbf{f}_{t}^{p})=\gamma\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{y}_{t})\] (14)

Then, learning the cross-view adaptation with view-condition prompts can be formulated as follows:

\[\small\theta^{*}=\arg\min_{\theta}\left[\mathbb{z}_{\mathbf{x}_{s},\mathbf{p }_{s},\mathbf{y}_{s}}\mathcal{L}_{Mask}(\mathbf{y}_{s},\mathbf{y}_{s})+ \mathbb{z}_{\mathbf{x}_{s},\mathbf{x}_{s},\mathbf{x}_{t},\mathbf{p}_{t}}( \lambda_{I}||\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{x}_{t})-\alpha\mathcal{D} _{y}(\mathbf{y}_{s},\mathbf{y}_{t})+\lambda_{P}||\mathcal{D}_{p}(\mathbf{f}_{ s}^{p},\mathbf{f}_{t}^{p})-\gamma\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{y}_{t})|)\right]\] (15)

where \(\lambda_{I}\) and \(\lambda_{P}\) are the balanced-weight of losses. Similar to metrics \(\mathcal{D}_{x}\) and \(\mathcal{D}_{y}\), we also adopt the geodesic flow path to model the cross-view correlation metric \(\mathcal{D}_{p}\).

## 4 Experiments

### Datasets, Benchmarks, and Implementation

To efficiently evaluate cross-view adaptation, the cross-view benchmarks are set up from the car to the drone view. Following common practices in UDA [23; 58], we choose SYNTHIA [44], GTA [43], and BDD100K [68] as the source domains while UAVID [33] is chosen as the target domain. We chose to adopt these datasets because they share a class of interests and are commonly used in UDA and segmentation benchmarks [23; 61].

**SYNTHIA \(\rightarrow\) UAVID Benchmark** SYNTHIA and UAVID share five classes of interest, i.e., Road, Building, Car, Tree, and Person. Since the UAVID dataset annotated cars, trucks, and buses as a class of Car, we collapse these classes in SYNTHIA into a single class of Car.

**GTA \(\rightarrow\) UAVID Benchmark** consists of five classes in the SYNTHIA \(\rightarrow\) UAVID benchmark and includes one more class of Terrain. Therefore, the GTA \(\rightarrow\) UAVID benchmark has six classes of interest, i.e., Road, Building, Car, Tree, Terrain, and Person.

**BDD \(\rightarrow\) UAVID Benchmark** is a real-to-real cross-view adaptation setting. Similar to GTA \(\rightarrow\) UAVID benchmark, there are six classes of interest between BDD100K and UAVID. In our experiments, we adopt the mean Intersection over Union (mIoU) metric to measure the performance.

**Implementation** We adopt Mask2Former [8] (ResNet 101) with Semantic Context Interaction of FreeSeg [38] and pre-trained text encoder of CLIP [39] for our open-vocab segmentation networks. Our balanced weights of losses are set to \(\lambda_{I}=1.0\) and \(\lambda_{P}=0.5\). Further details of our networks and hyper-parameters are provided in the appendix.

### Ablation Study

**Effectiveness of Cross-view Adaptation and Prompting Mechanisms** Table 1 analyzes the effectiveness of prompting mechanisms, i.e., i.e., with and without _Prompting_, with and without _Cross-view Adaptation_ (in Eqn. (6)), with and without _View-Condition Prompting_ (in Eqn. (15)). For supervised results, we train two different models on UAVID with and without the Terrain class on two benchmarks. As in Table 1, the cross-view adaptation loss in Eqn. (6) significantly improve the performance of segmentation models. With prompting and cross-view adaptation,

\begin{table}
\begin{tabular}{c c c c|c c c c|c c c c c c} \hline \multicolumn{1}{c}{With} & \multicolumn{3}{c|}{SYNTHIA \(\rightarrow\) UAVID} & \multicolumn{3}{c}{GTA \(\rightarrow\) UAVID} \\ \hline Network & Metric & Road & Building & Car & Tree & Terrain & Person & mIoU \\ \hline \(X\) & \(X\) & \(X\) & 8.1 & 19.1 & 7.4 & 30.3 & 1.3 & 13.2 & 7.5 & 13.0 & 27.7 & 26.8 & 26.6 & 1.0 & 12.9 \\ \(X\) & \(Y\) & \(X\) & **31.4** & **75.1** & **57.5** & **59.2** & **19.5** & **48.6** & **22.9** & **64.6** & **37.8** & **52.8** & **48.5** & **13.8** & **40.1** \\ \hline \hline Supervised & & 75.8 & 91.6 & 79.1 & 77.4 & 24.1 & 73.2 & 76.8 & 91.8 & 81.1 & 17.6 & 62.8 & 43.4 & 73.0 \\ \hline ✓ & ✗ & \(X\) & 15.7 & 27.8 & 15.7 & 34.1 & 7.7 & 20.2 & 16.6 & 26.8 & 7.2 & 30.0 & 21.7 & 6.0 & 18.1 \\ ✓ & ✓ & ✗ & 36.8 & 75.5 & 61.3 & 60.8 & 21.2 & 51.1 & 27.3 & 66.8 & 42.3 & 55.5 & 47.1 & 25.1 & 44.0 \\ ✓ & ✓ & ✓ & \(X\) & **38.4** & **76.1** & **62.8** & **62.1** & **21.8** & **52.2** & **29.2** & **67.1** & **45.2** & **56.6** & **48.5** & **27.9** & **45.7** \\ \hline \multicolumn{1}{c}{} & Supervised & 79.8 & 92.6 & 82.9 & 79.1 & 48.0 & 76.5 & 80.5 & 93.3 & 82.7 & 79.2 & 71.3 & 49.9 & 76.1 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Effectiveness of Our Cross-view Adaptation Losses and Prompting Mechanism.

\begin{table}
\begin{tabular}{c|c|c|c c c c c} \hline \multicolumn{1}{c}{} & \multicolumn{3}{c}{SYNTHIA \(\rightarrow\) UAVID} & \multicolumn{3}{c}{GTA \(\rightarrow\) UAVID} \\ \hline Network & Metric & Road & Building & Car & Tree & Terrain & Person & mIoU \\ \hline \multirow{2}{*}{ResNet} & Euclidean & 23.7 & 31.2 & 33.2 & 36.7 & - & 11.5 & 27.2 \\  & Geodesic & **38.4** & **76.1** & **62.8** & **62.1** & **-** & **21.8** & **52.2** \\ \hline \multirow{2}{*}{Swin} & Euclidean & 24.7 & 31.9 & 41.2 & 39.7 & - & 14.1 & 30.3 \\  & Geodesic & **40.8** & **76.4** & **65.8** & **62.7** & **-** & **27.9** & **54.7** \\ \hline \multirow{2}{*}{ResNet} & Euclidean & 21.7 & 30.0 & 26.2 & 39.7 & 31.7 & 9.5 & 26.5 \\  & Geodesic & **29.2** & **67.1** & **45.2** & **56.6** & **48.5** & **27.9** & **45.7** \\ \hline \multirow{2}{*}{Swin} & Euclidean & 24.3 & 33.7 & 28.5 & 40.1 & 32.8 & 9.7 & 28.2 \\  & Geodesic & **31.0** & **67.1** & **46.8** & **56.9** & **48.7** & **31.9** & **47.1** \\ \hline \end{tabular}
\end{table}
Table 2: Effectiveness of Backbones and Cross-view Metrics.

[MISSING_PAGE_FAIL:8]

without adaptation, with cross-view adaptation, and with view-condition prompting. As shown in the results, our cross-view adaptation can efficiently model the segmentation of the view. By using the view-condition prompting, our model can further improve the segmentation of persons and vehicles.

### Comparisons with Prior UDA Methods

**SYNTHIA \(\rightarrow\) UAVID** As shown in Table 4, our EAGLE has achieved SOTA results and outperforms prior view transformation (i.e., Polar Transform [45]) UDA methods by a large margin. For fair comparisons, we adopt the DeepLab [3] and DAFormer [23] for the segmentation network. In particular, our mIoU results using DeepLab and DAFormer are \(45.2\%\) and \(50.8\%\). In the DAFormer backbone, the mIoU results of our approach are higher than CROVIA [50] and MIC [25] by \(+4.8\%\) and \(+9.0\%\). The IoU result of each class also consistently outperformed the prior methods. Highlighted that although our approach does NOT use depth labels, our results still outperform the one using depths, i.e., DADA [59]. It has emphasized that our approach is able to better capture the cross-view structural changes compared to prior methods. Figure 4 illustrates our qualitative results compared to ProDA [69] and CROVIA [50].

**GTA \(\rightarrow\) UAVID** As shown in Table 4, our effectiveness outperforms prior polar view transformation [45] and domain adaptation approaches when measured by both mIoU performance and the IoU accuracy of each class. In particular, our mIoU performance using DeepLab and DAFormer network achieves \(36.7\%\) and \(40.7\%\), respectively. Our results have substantially closed the performance gap with the supervised results. By using the better segmentation-based network, i.e., Mask2Former with ResNet, the performance of our approach is further improved to \(40.1\%\) compared to DeepLab.

### Comparisons with Open-vocab Segmentation

We compare EAGLE with the prior open-vocab segmentation methods, i.e., DenseCLIP [40] and an adaptive prompting FreeSeg [38] with four settings, i.e., Source Only, with AdvEnt [58], and with SAC [1], and our Cross-View Adaptation in Eqn. (6) (without view-condition).

**Open-vocab Semantic Segmentation** As in Table 5, the mIoU performance of our proposed approach with cross-view adaptation outperforms prior DenseCLIP by a large margin on SYNTHIA \(\rightarrow\) UAVID. By using our cross-view geometric adaptation loss, the performance of DenseCLIP and FreeSeg is further enhanced, i.e., higher than DenseCLIP and FreeSeg with SAC by +3.7% and +5.0%. While FreeSeg [38] with our cross-view adaptation slightly outperforms EAGLE due to its adaptive prompting, our EAGLE approach with the better view-condition prompting achieves higher mIoU

\begin{table}
\begin{tabular}{l|c|c c c c c c c c c c} \hline \multirow{2}{*}{Network} & \multirow{2}{*}{Method} & \multicolumn{6}{c}{SYNTHIA \(\rightarrow\) UAVID} & \multicolumn{6}{c}{GTA \(\rightarrow\) UAVID} \\ \cline{3-11}  & & Road & Building & \(\Delta\)T-Free & Person & Road & Building & \(\Delta\)T-Free & Person & mIoU \\ \hline \multirow{5}{*}{DepthLab} & AdvEnt [58] & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free \\  & DAFormer [23] & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free \\  & DAFormer [23] & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free \\  & DAFormer [23] & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \(\Delta\)T-Free & \

[MISSING_PAGE_FAIL:10]

## References

* [1] N. Araslanov,, and S. Roth. Self-supervised augmentation consistency for adapting semantic segmentation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.
* [2] M. Armando, S. Galaaoui, F. Baradel, T. Lucas, V. Leroy, R. Bregier, P. Weinzaepfel, and G. Rogez. Cross-view and cross-pose completion for 3d human understanding. _arXiv preprint arXiv:2311.09104_, 2023.
* [3] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. _TPAMI_, 2018.
* [4] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethinking atrous convolution for semantic image segmentation. _arXiv preprint arXiv:1706.05587_, 2017.
* [5] Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool. Domain adaptive faster r-cnn for object detection in the wild. In _CVPR_, 2018.
* [6] Y. Chen, W. Li, and L. Van Gool. Road: Reality oriented adaptation for semantic segmentation of urban scenes. In _CVPR_, 2018.
* [7] Y.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. F. Wang, and M. Sun. No more discrimination: Cross city adaptation of road scene segmenters. In _ICCV_, 2017.
* [8] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar. Masked-attention mask transformer for universal image segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1290-1299, 2022.
* [9] B. Cheng, A. G. Schwing, and A. Kirillov. Per-pixel classification is not all you need for semantic segmentation. In _NeurIPS_, 2021.
* [10] B. Coors, A. P. Condurache, and A. Geiger. Nova: Learning to see in novel viewpoints and domains. In _2019 International Conference on 3D Vision (3DV)_, pages 116-125, 2019.
* [11] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The Cityscapes dataset for semantic urban scene understanding. In _CVPR_, 2016.
* [12] D. Di Mauro, A. Furnari, G. Patane, S. Battiato, and G. M. Farinella. Sceneadapt: Scene-based domain adaptation for semantic segmentation using adversarial learning. _Pattern Recognition Letters_, 136:175-182, 2020.
* [13] J. Ding, N. Xue, G.-S. Xia, and D. Dai. Decoupling zero-shot semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11583-11592, 2022.
* [14] S. Ettedgui, S. Abu-Hussein, and R. Giryes. Procst: Boosting semantic segmentation using progressive cyclic style-transfer, 2022.
* [15] M. Fahes, T.-H. Vu, A. Bursuc, P. Perez, and R. de Charette. Poda: Prompt-driven zero-shot domain adaptation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 18623-18633, 2023.
* [16] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao. Clip-adapter: Better vision-language models with feature adapters. _International Journal of Computer Vision_, pages 1-15, 2023.
* [17] G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin. Scaling open-vocabulary image segmentation with image-level labels. In _European Conference on Computer Vision_, pages 540-557. Springer, 2022.
* [18] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In _2012 IEEE Conference on Computer Vision and Pattern Recognition_, pages 2066-2073, 2012.

* [19] G. Goyal, N. Noceti, and F. Odone. Cross-view action recognition with small-scale datasets. _Image and Vision Computing_, 120:104403, 2022.
* [20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _CVPR_, 2016.
* [21] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros, and T. Darrell. CyCADA: Cycle-consistent adversarial domain adaptation. In _ICML_, 2018.
* [22] C.-A. Hou, Y.-R. Yeh, and Y.-C. F. Wang. An unsupervised domain adaptation approach for cross-domain visual classification. In _2015 12th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)_, pages 1-6. IEEE, 2015.
* [23] L. Hoyer, D. Dai, and L. Van Gool. DAFormer: Improving network architectures and training strategies for domain-adaptive semantic segmentation. In _CVPR_, 2022.
* [24] L. Hoyer, D. Dai, and L. Van Gool. HRDA: Context-aware high-resolution domain-adaptive semantic segmentation. In _Proceedings of the European Conference on Computer Vision (ECCV)_, 2022.
* [25] L. Hoyer, D. Dai, H. Wang, and L. Van Gool. Mic: Masked image consistency for context-enhanced domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11721-11732, 2023.
* [26] J. Huang, D. Guan, A. Xiao, and S. Lu. Cross-view regularization for domain adaptive panoptic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10133-10144, 2021.
* [27] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _International conference on machine learning_, pages 4904-4916. PMLR, 2021.
* [28] K.-H. Lee, G. Ros, J. Li, and A. Gaidon. SPIGAN: Privileged adversarial learning from simulation. In _ICLR_, 2019.
* [29] B. Li, K. Q. Weinberger, S. Belongie, V. Koltun, and R. Ranftl. Language-driven semantic segmentation. In _International Conference on Learning Representations_, 2022.
* [30] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang, K.-W. Chang, and J. Gao. Grounded language-image pre-training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10965-10975, 2022.
* [31] F. Liang, B. Wu, X. Dai, K. Li, Y. Zhao, H. Zhang, P. Zhang, P. Vajda, and D. Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7061-7070, 2023.
* [32] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* 119, 2020.
* [34] Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and K. Kim. Image to image translation for domain adaptation. In _CVPR_, 2018.
* [35] H.-Q. Nguyen, T.-D. Truong, X. B. Nguyen, A. Dowling, X. Li, and K. Luu. Insect-foundation: A foundation model and large-scale 1m dataset for visual insect understanding. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 21945-21955, 2024.
* [36] B. Pan, J. Sun, H. Y. T. Leung, A. Andonian, and B. Zhou. Cross-view semantic segmentation for sensing surroundings. _IEEE Robotics and Automation Letters_, 5(3):4867-4873, 2020.

* [37] F. Pan, I. Shin, F. Rameau, S. Lee, and I. S. Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision. In _CVPR_, 2020.
* [38] J. Qin, J. Wu, P. Yan, M. Li, R. Yuxi, X. Xiao, Y. Wang, R. Wang, S. Wen, X. Pan, et al. Freeseg: Unified, universal and open-vocabulary image segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19446-19455, 2023.
* [39] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [40] Y. Rao, W. Zhao, G. Chen, Y. Tang, Z. Zhu, G. Huang, J. Zhou, and J. Lu. Denseclip: Language-guided dense prediction with context-aware prompting. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [41] K. Regmi and M. Shah. Bridging the domain gap for ground-to-aerial image matching. In _2019 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 470-479, 2019.
* [42] H. Ren, Y. Yang, H. Wang, B. Shen, Q. Fan, Y. Zheng, C. K. Liu, and L. Guibas. Adela: Automatic dense labeling with attention for viewpoint shift in semantic segmentation. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8069-8079, 2022.
* [43] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In _ECCV_, 2016.
* [44] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez. The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In _CVPR_, 2016.
* [45] Y. Shi, L. Liu, X. Yu, and H. Li. Spatial-aware feature aggregation for image based cross-view geo-localization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch'e-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32_, pages 10090-10100. Curran Associates, Inc., 2019.
* [46] Y. Shi, X. Yu, D. Campbell, and H. Li. Where am i looking at? joint location and orientation estimation by cross-view matching. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2020.
* [47] C. Simon, P. Koniusz, and M. Harandi. On learning the geodesic path for incremental learning. In _Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition_, pages 1591-1600, 2021.
* [48] N. Sun, Q. Lu, W. Zheng, J. Liu, and G. Han. Unsupervised cross-view facial expression image generation and recognition. _IEEE Transactions on Affective Computing_, 2020.
* [49] A. Toker, Q. Zhou, M. Maximov, and L. Leal-Taixe. Coming down to earth: Satellite-to-street view synthesis for geo-localization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6488-6497, June 2021.
* [50] T.-D. Truong, C. N. Duong, A. Dowling, S. L. Phung, J. Cothren, and K. Luu. Croatia: Seeing drone scenes from car perspective via cross-view adaptation, 2023.
* [51] T.-D. Truong, C. N. Duong, N. Le, S. L. Phung, C. Rainwater, and K. Luu. Bimal: Bijective maximum likelihood approach to domain adaptation in semantic scene segmentation. In _IEEE International Conference on Computer Vision (ICCV)_, pages 8548-8557, 2021.
* [52] T.-D. Truong, P. Helton, A. Moustafa, J. D. Cothren, and K. Luu. Conda: Continual unsupervised domain adaptation learning in visual perception for self-driving cars. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5642-5650, 2024.
* [53] T.-D. Truong, N. Le, B. Raj, J. Cothren, and K. Luu. Fredom: Fairness domain adaptation approach to semantic scene understanding. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.

* [54] T.-D. Truong and K. Luu. Cross-view action recognition understanding from exocentric to egocentric perspective. _arXiv preprint arXiv:2305.15699_, 2023.
* [55] T.-D. Truong, H.-Q. Nguyen, B. Raj, and K. Luu. Fairness continual learning approach to semantic scene understanding in open-world environments. In _Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)_, 2023.
* [56] T.-D. Truong, U. Prabhu, B. Raj, J. Cothren, and K. Luu. Falcon: Fairness learning via contrastive attention approach to continual semantic scene understanding in open world. _arXiv preprint arXiv:2311.15965_, 2023.
* [57] V. Vidit, M. Engilberge, and M. Salzmann. Learning transformations to reduce the geometric shift in object detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17441-17450, 2023.
* [58] T.-H. Vu, H. Jain, M. Bucher, M. Cord, and P. Perez. Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In _CVPR_, 2019.
* [59] T.-H. Vu, H. Jain, M. Bucher, M. Cord, and P. Perez. Dada: Depth-aware domain adaptation in semantic segmentation. In _ICCV_, 2019.
* [60] K. Wang, D. Kim, R. Feris, and M. Betke. Cdac: Cross-domain attention consistency in transformer for domain adaptive semantic segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11519-11529, 2023.
* [61] L. Wang, R. Li, C. Zhang, S. Fang, C. Duan, X. Meng, and P. M. Atkinson. Unetformer: A unet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery. _ISPRS Journal of Photogrammetry and Remote Sensing_, 190:196-214, 2022.
* [62] Q. Wang, W. Min, Q. Han, Z. Yang, X. Xiong, M. Zhu, and H. Zhao. Viewpoint adaptation learning with cross-view distance metric for robust vehicle re-identification. _Information Sciences_, 564:71-84, 2021.
* [63] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In _NeurIPS_, 2021.
* [64] M. Xu, Z. Zhang, F. Wei, Y. Lin, Y. Cao, H. Hu, and X. Bai. A simple baseline for open-vocabulary semantic segmentation with pre-trained vision-language model. In _European Conference on Computer Vision_, pages 736-753. Springer, 2022.
* [65] Z. Yan, X. Yu, Y. Qin, Y. Wu, X. Han, and S. Cui. Pixel-level intra-domain adaptation for semantic segmentation. In _Proceedings of the 29th ACM International Conference on Multimedia_, pages 404-413, 2021.
* [66] Y. Yao and H. S. Park. Multiview cross-supervision for semantic segmentation. _arXiv preprint arXiv:1812.01738_, 2018.
* [67] Y. Yin, W. Hu, Z. Liu, G. Wang, S. Xiang, and R. Zimmermann. Crossmatch: Source-free domain adaptive semantic segmentation via cross-modal consistency training. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 21786-21796, 2023.
* [68] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2636-2645, 2020.
* [69] P. Zhang, B. Zhang, T. Zhang, D. Chen, Y. Wang, and F. Wen. Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation. _arXiv preprint arXiv:2101.10979_, 2021.
* [70] Q. Zhang, W. Lin, and A. B. Chan. Cross-view cross-scene multi-view crowd counting. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 557-567, 2021.

* [71] D. Zhao, S. Wang, Q. Zang, D. Quan, X. Ye, R. Yang, and L. Jiao. Learning pseudo-relations for cross-domain semantic segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 19191-19203, 2023.
* [72] B. Zhou and P. Krahenbuhl. Cross-view transformers for real-time map-view semantic segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 13760-13769, 2022.
* [73] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. _International Journal of Computer Vision_, 130(9):2337-2348, 2022.
* [74] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _ICCV_, 2017.
* [75] S. Zhu, M. Shah, and C. Chen. Transgeo: Transformer is all you need for cross-view image geo-localization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1162-1171, 2022.

**Appendix**

## 1 Proof of Eqn. (9)

As shown in our Eqn. (16), our Geodesic Flow-based metrics have the upper bound as follows:

\[\begin{split}\forall\mathbf{x}_{s},\mathbf{x}_{t}:& \quad\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{x}_{t})=1-\frac{\mathbf{x}_{s}^{ \top}\mathbf{Q}\mathbf{x}_{t}}{||\mathbf{Q}^{1/2}\mathbf{x}_{s}||||\mathbf{Q}^ {1/2}\mathbf{x}_{t}||}\leq 2\\ \forall\mathbf{y}_{s},\mathbf{y}_{t}:&\quad \mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{y}_{t})=1-\frac{\mathbf{y}_{s}^{\top} \mathbf{Q}\mathbf{y}_{t}}{||\mathbf{Q}^{1/2}\mathbf{y}_{s}||||\mathbf{Q}^{1/ 2}\mathbf{y}_{t}||}\leq 2\end{split}\] (16)

In addition, as \(\mathcal{D}_{x}\) is the distance metric, this metric should satisfy the following triangular inequality as follows:

\[\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{\bar{x}}_{t})\leq\mathcal{D}_{x}( \mathbf{x}_{s},\mathbf{x}_{t})+\mathcal{D}_{x}(\mathbf{x}_{t},\mathbf{\bar{x} }_{t})\] (17)

Similarly, \(\mathcal{D}_{y}\) should satisfy the following triangular inequality as follows:

\[\begin{split}&\quad\mathcal{D}_{y}(\mathbf{y}_{t},\mathbf{\bar{y} }_{t})+\mathcal{D}_{y}(\mathbf{\bar{y}}_{t},\mathbf{y}_{s})\geq\mathcal{D}_{y} (\mathbf{y}_{s},\mathbf{y}_{t})\\ \Leftrightarrow&\quad\mathcal{D}_{y}(\mathbf{y}_{t}, \mathbf{\bar{y}}_{t})\geq\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{y}_{t})- \mathcal{D}_{y}(\mathbf{\bar{y}}_{t},\mathbf{y}_{s})\\ \Leftrightarrow&\quad-\alpha\mathcal{D}_{y}(\mathbf{y} _{t},\mathbf{\bar{y}}_{t})\leq-\alpha\left(\mathcal{D}_{y}(\mathbf{y}_{s}, \mathbf{y}_{t})-\mathcal{D}_{y}(\mathbf{\bar{y}}_{t},\mathbf{y}_{s})\right) \end{split}\] (18)

Then, from Eqn. (16) and Eqn. (17) above, we can further derive as follows:

\[\begin{split}&\quad\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{\bar{x} }_{t})-\alpha\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{\bar{y}}_{t})\\ &\leq\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{x}_{t})+\mathcal{D}_{ x}(\mathbf{x}_{t},\mathbf{\bar{x}}_{t})-\alpha\left(\mathcal{D}_{y}(\mathbf{y}_{s}, \mathbf{y}_{t})-\mathcal{D}_{y}(\mathbf{\bar{y}}_{t},\mathbf{y}_{s})\right)\\ &\leq\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{x}_{t})-\alpha \mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{y}_{t})+\mathcal{D}_{x}(\mathbf{x}_{t},\mathbf{\bar{x}}_{t})+\alpha\mathcal{D}_{y}(\mathbf{\bar{y}}_{t},\mathbf{y}_{ s})\\ &\leq\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{x}_{t})-\alpha \mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{y}_{t})+\underbrace{2(1+\alpha)}_{Constant} \end{split}\] (19)

Since \(\alpha\) is the constant linear scale value, therefore, we can further derive as follows:

\[\begin{split}\Rightarrow||\mathcal{D}_{x}(\mathbf{x}_{s}, \mathbf{\bar{x}}_{t})-\alpha\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{\bar{y}}_ {t})||\\ =\mathcal{O}(||\mathcal{D}_{x}(\mathbf{x}_{s},\mathbf{x}_{t})- \alpha\mathcal{D}_{y}(\mathbf{y}_{s},\mathbf{y}_{t})||)\end{split}\] (20)

## 2 Implementation

We follow the implementation of Mask2Former [8] and FreeSeg [38] with ResNet [20] and Swin backbones [32] for our segmentation network. In particular, we adopt Mask2Former with Semantic Context Interaction of FreeSeg [38] for our open-vocab segmentation network. We use the pre-trained text encoder of CLIP [39]. The textual features \(\mathbf{f}_{s}^{p}\) and \(\mathbf{f}_{t}^{p}\) are obtained by the CLIP textual encoder. Following common practices [38; 31], we adopt the open-vocab segmentation loss of FreeSeg [38] to our supervised loss \(\mathcal{L}_{Mask}\). For experiments without prompting, we use the Mask2Former network. Following the UAV protocol of [61], the image size is set to \(512\times 512\). The linear scale factors \(\alpha\) and \(\gamma\) are set to \(\alpha=1.5\) and \(\gamma=1.0\), respectively. For the Geodesic Flow modeling, we adopt the implementation of generalized SVD decomposition [18; 47] in the framework. The subspace dimension in our geodesic flow-based metrics is set to \(D=256\). The batch size and the base learning rate in our experiments are set to \(16\) and \(2.5\times 10^{-4}\). The balanced weights of losses in our experiments are set to \(\lambda_{I}=1.0\) and \(\lambda_{P}=0.5\). During training, the classes in the prompts are generated similarly for both view images.

In our Geodesic Flow-based metrics, the subspaces of images and ground-truth segmentation of the source domain are pre-computed on the entire data. For the language space, we compute the subspaces of each view based on the textual feature representations of all possible prompts in each domain. Meanwhile, the subspaces of the segmentation on the target domain are computed based on the current batch of training. For the implementation of DenseCLIP [40] and FreeSeg [38] with AdvEnt [58], we perform the adaptation process on the mask predictions. Meanwhile, we adopt the pseudo labels and the self-supervised framework of SAC[1] for the implementation of DenseCLIP [40] and FreeSeg [38] with SAC [1].

## 3 Ablation Study

**Effectiveness of Batch Size** Table 8 illustrates the impact of the batch size on the performance of cross-view domain adaptation. By increasing the batch size, the mIoU performance is also increased accordingly on both benchmarks. This result has illustrated that the small batch size could not have enough samples to approximate the subspace that represents geometric structures. Meanwhile, the subspace created from the large batch size will be ale to capture the geometric structure of drone-view scenes. However, due to the limitation of GPU resources, we could not evaluate the cross-view adaptation model with larger batch size.

**Subspace Representation of Geodesic Flow-based Metrics** To illustrate the ability of structural learning of our geodesic flow-based metrics, we use a subset of images of the car-view and the drone-view dataset to visualize the base structure of subspaces obtained from the PCA algorithm. Fig. 7 visualizes the mean structures of car-view and drone-view images. As shown in Fig. 7, The subspaces of car-view images represent the geometric structures of car-view data, i.e., the road in the middle, buildings, trees on two sides, etc. Meanwhile, the geometric structures of the drone view have also been illustrated in the figure with structures and topological distributions of objects (e.g., the road in the middle and trees and buildings on the sides) on the scenes. The results have illustrated the base geometric structures of the car-view and the drone-view data. Then, by modeling the geodesic flow path across two subspaces, our metric is able to measure the cross-view geometric structural changes (i.e., the change of structures and topological layouts of the scene) from the car view to the drone view. Our experimental results in other ablation studies have further confirmed our effectiveness in geometric structural modeling across views. Figure 8 illustrates the feature distributions with and without our proposed approach. As shown in Figure 8, our approach can help to improve the feature representations of classes, and the cluster of each class is more compact, especially in classes of car, tree, and person.

## 4 Discussion of Limitations and Broader Impact

**Limitations.** In our paper, we have specified a set of hyper-parameters and network designs to support our hypothesis and theoretical analysis. However, our proposed approach could potentially consist of several limitations. First, our work focuses on studying the impact of cross-view geometric adaptation loss and view-condition prompting mechanisms on the segmentation models across views. The balanced weights among weights, i.e., \(\lambda_{I}\) and \(\lambda_{P}\), have not been fully exploited. We leave this investigation as our future experiments. Second, although the datasets and benchmarks used in our

\begin{table}
\begin{tabular}{c|c c c c c c|c c c c c c} \hline Batch & \multicolumn{6}{c|}{SYNTHIA \(\rightarrow\) UAVID} & \multicolumn{6}{c}{GTA \(\rightarrow\) UAVID} \\ \cline{2-13} Size & Road & Building & Car & Tree & Person & mIoU & Road & Building & Car & Tree & Terrain & Person & mIoU \\ \hline \(4\) & 25.5 & 58.9 & 46.5 & 29.2 & 15.9 & 35.2 & 17.6 & 50.6 & 29.7 & 26.0 & 41.1 & 22.4 & 31.2 \\ \(8\) & 35.3 & 68.9 & 57.6 & 54.4 & 20.9 & 47.4 & 25.1 & 57.6 & 39.3 & 45.5 & 44.9 & 26.2 & 39.8 \\ \(16\) & **38.4** & **76.1** & **62.8** & **62.1** & **21.8** & **52.2** & **29.2** & **67.1** & **45.2** & **56.6** & **48.5** & **27.9** & **45.7** \\ \hline \end{tabular}
\end{table}
Table 8: Effectiveness of Batch Size.

Figure 7: The Structures of Subspaces of Car-View and Drone-View Dataset Learned From a Subset of Images.

experiments have sufficiently illustrated the effectiveness of our proposed cross-view adaptation learning approach, the lack of diverse classes and categories in datasets is also a potential limitation. Third, the hypothesis of the linear relations across views of images and segmentation mask, i.e., \(\alpha\), and textual representations and segmentation masks, i.e., \(\gamma\), could limit the performance of the relation. The non-trivial relations across views should be deeply exploited in future research. Also, while the implementation of Mask2Former and FreeSeg is adopted to develop our approach, the experiments with other open-vocab segmentation networks should be considered in subsequent research studies. These aforementioned limitations will motivate new studies to further improve the methodology, datasets, and benchmarks of the cross-view adaptation learning paradigm.

**Broader Impact.** Our paper could bring significant potential for various applications that require learning across camera viewpoints. Our approach enables generalizability across camera views, thus enhancing the robustness of the segmentation model across views. In addition, our approach helps to reuse off-the-shelf large-scale data while reducing the effort of manually labeling data of new camera views.

## 5 Other Related Work

While the important and closely related work to our approach has been presented in our main paper, we also would like to review some other research studies that are related to our method as follows. In particular, Brady et al. [72] presented a cross-view transformer that learns the camera-aware positional embeddings. Although the views are captured from left and right angles, the camera positions in the approach remain at the same altitude. Similarly, Pan et al. [36] present a View Parsing Network to accumulate features across first-view observations with multiple angles. Yao et al. [66] proposed a semi-supervised learning approach to learn the segmentation model from multiple views of an image. Huang et al. [26] a cross-style regularization for domain adaptation in panoptic segmentation by imposing the consistency of the segmentation between the target images and stylized target images. Wang et al. [62] proposed a viewpoint adaptation framework for the person re-identification problem by using the generative model to generate training data across various viewpoints. Hou et al. [22] presented a matching cross-domain data approach to domain adaptation in visual classification. Sun et al. [48] proposed a cross-view facial expression adaptation framework to parallel synthesize and recognize cross-view facial expressions. Goyal et al. [19] introduced a cross-view action recognition approach to transferring the feature representations to different views. Zhang et al. [70] proposed a multi-view crowd counting approach that adaptively chooses and aggregates multi-cameras and a noise view regularization. Armando et al. [2] proposed a self-supervised pre-training approach to human understanding learned on pairs of images captured from different viewpoints. Then, the pre-trained models are later used for various downstream human-centric tasks. In summary, these prior cross-view methods could require either a pair of cross-view images [2] or images captured at the same altitude with different angles [26; 72; 22]. In addition, the cross-view geometric correlation modeling has not been exploited in these prior studies [26; 72; 22; 2]

Figure 8: The Feature Distribution of Classes in SYNTHIA \(\rightarrow\) UAVID Experiments.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims declared in the abstract match with the contributions, experimental results, and scope of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: The limitations of the paper are discussed in the last section of the paper. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The proof of formula is provided in the supplementary. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The details of datasets and implementations are presented in the experimental sections. Guidelines:* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The code will be published may the paper be accepted. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ** Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The details of training and testing are presented in the experimental section. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Following the standard evaluation of semantic segmentation, we evaluate our model by the standard mIoU metrics instead of the statistical tests. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The computational resources used in our experiments are presented in the experimental section. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The content of the paper and datasets strictly follows the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper does not have a negative societal impact. The broader impact is discussed in the appendix. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: The paper does not have a risk. The released models will be available may the paper be accepted. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper provides all the references to code, data, and models used in the paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not introduce the new dataset. The code of the paper will be published may the paper be accepted. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The research in this paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The research in this paper does not involve crowdsourcing nor research with human subjects. Thus, there is no requirement for IRB. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.