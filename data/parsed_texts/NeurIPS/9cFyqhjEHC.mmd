# A Flexible, Equivariant Framework for Subgraph

GNNs via Graph Products and Graph Coarsening

Guy Bar-Shalom

Computer Science

Technion - Israel Institute of Technology

guy.b@campus.technion.ac.il

&Yam Eitan

Electrical & Computer Engineering

Technion - Israel Institute of Technology

yameitan1997@gmail.com

&Fabrizio Frasca

Electrical & Computer Engineering

Technion - Israel Institute of Technology

fabrizio.frasca.effe@gmail.com

Equal contribution.

Electrical & Computer Engineering

Technion - Israel Institute of Technology

NVIDIA Research

haggaimaron@gmail.com

###### Abstract

Subgraph GNNs enhance message-passing GNNs expressivity by representing graphs as sets of subgraphs, demonstrating impressive performance across various tasks. However, their scalability is hindered by the need to process large numbers of subgraphs. While previous approaches attempted to generate smaller subsets of subgraphs through random or learnable sampling, these methods often yielded suboptimal selections or were limited to small subset sizes, ultimately compromising their effectiveness. This paper introduces a new Subgraph GNN framework to address these issues. Our approach diverges from most previous methods by associating subgraphs with node clusters rather than with individual nodes. We show that the resulting collection of subgraphs can be viewed as the product of coarsened and original graphs, unveiling a new connectivity structure on which we perform generalized message passing.

Crucially, controlling the coarsening function enables meaningful selection of any number of subgraphs. In addition, we reveal novel permutation symmetries in the resulting node feature tensor, characterize associated linear equivariant layers, and integrate them into our Subgraph GNN. We also introduce novel node marking strategies and provide a theoretical analysis of their expressive power and other key aspects of our approach. Extensive experiments on multiple graph learning benchmarks demonstrate that our method is significantly more flexible than previous approaches, as it can seamlessly handle any number of subgraphs, while consistently outperforming baseline approaches. Our code is available at https://github.com/BarSGuy/Efficient-Subgraph-GNNs.

## 1 Introduction

Subgraph GNNs [4, 12, 39, 8, 27, 29, 38, 3] have recently emerged as a promising direction in graph neural network research, addressing the expressiveness limitations of Message Passing Neural Networks (MPNNs) [24, 35, 25]. In essence, a Subgraph GNN operates on a graph by transforming it into a collection of subgraphs, generated based on a specific selection policy. Examples of such policies include removing a single node from the original graph or simply marking a node without changing the graph's original connectivity [26]. The model then processes these subgraphs using an equivariant architecture, aggregates the derived representations, and makes graph- or node-level predictions. The growing popularity of Subgraph GNNs stems not only from their enhancedexpressive capabilities over MPNNs but also from their impressive empirical results, as notably demonstrated on well-known molecular benchmarks [38, 12, 3].

Unfortunately, Subgraph GNNs are hindered by substantial computational costs as they necessitate message-passing operations across all subgraphs within the bag. Typically, the number of subgraphs is the number of nodes in the graph, \(n\)-- for bounded degree graphs, this results in a time complexity scaling quadratically (\(\mathcal{O}(n^{2})\)), in contrast to the linear complexity of a standard MPNN. This significant computational burden makes Subgraph GNNs impractical for large graphs, hindering their applicability to important tasks and widely used datasets. To overcome this challenge, various studies have explored methodologies that process only a subset of subgraphs from the bag. These methods range from simple random sampling techniques [8, 4, 40, 3] to more advanced strategies that learn to select the most relevant subset of the bag to process [5, 20, 29]. However, while random sampling of subgraphs yields subpar performance, more sophisticated learnable selection strategies also have significant limitations. Primarily, they rely on _training-time_ discrete sampling which complicates the optimization process, as evidenced by the high number of epochs required to train them [20, 5, 29]. As a result, these methods often allow only a very small bag size, yielding only modest performance improvements compared to random sampling and standard MPNNs.

Our approach.The goal of this paper is to devise a Subgraph GNN architecture that can flexibly generate and process variable-sized bags, and deliver strong experimental results while sidestepping intricate and lengthy training protocols. Specifically, our approach aims to overcome the common limitation of restricting usage to a very small set of subgraphs.

Our proposed method builds upon and extends an observation made by Bar-Shalom et al. [3], who draw an analogy between using Subgraph GNNs and performing message-passing operations over a larger "product graph". Specifically, it was shown that when considering the maximally expressive (node-based) Subgraph GNN suggested by [38]2, the bag of subgraphs and its update rules can be obtained by transforming a graph through the _graph cartesian product_ of the original graph with itself, i.e., \(G\Box G\), and then processing the resulting graph using a standard MPNN. In our approach, we propose to modify the first term of the product and replace it with a _coarsened_ version of the original graph, denoted \(\mathcal{T}(G)\), obtained by mapping nodes to _super-nodes_ (e.g., by applying graph clustering, see Figure 1(left)), making the resulting product graph \(\mathcal{T}(G)\Box G\) significantly smaller. This construction is illustrated in Figure 1(right). This process effectively associates each subgraph - a row in Figure 1(right) - with a set of nodes produced by the coarsening function \(\mathcal{T}\). Different choices of \(\mathcal{T}\) allow for both flexible bag sizes and a simple, meaningful selection of the subgraphs.

Footnote 2: The architecture suggested in [38] was shown to be at least as expressive as all previously studied node-based Subgraph GNNs

While performing message passing on \(\mathcal{T}(G)\Box G\) serves as the core update rule in our architecture, we augment our message passing operations with another set of operations derived from the symmetry structure of the resulting node feature tensor, which we call _symmetry-based updates_. Specifically, our node feature tensor is indexed by pairs \((S,v)\) where \(S\) is a super-node and \(v\) is an original node. Accordingly, \(\mathcal{X}\) is a \(T\times n\times d\) tensor, where \(d\) is the feature dimension, and \(T\) is the number of super-nodes (a constant hyper-parameter). As super-nodes are sets of nodes, \(\mathcal{X}\) can also be viewed as a (very) sparse \(2^{n}\times n\times d\) tensor where \(2^{n}\) is the number of all subsets of the vertex set. Since the symmetric group \(S_{n}\) acts naturally on this representation, we use it to develop symmetry based updates.

Interestingly, we find that this node feature tensor, \(\mathcal{X}\), adheres to a specific set of symmetries, which, to the best of our knowledge, is yet unstudied in the context of machine learning: applying

Figure 1: Product graph construction. **Left:** Transforming of the graph into a coarse graph; **Right:** Cartesian product of the coarsened graph with the original graph. The vertical axis corresponds to the subgraph dimension (super-nodes), while the horizontal axis corresponds to the node dimension (nodes).

a permutation \(\sigma\in S_{n}\) to the nodes in \(S\) and to \(v\) results in an equivalent representation of our node feature tensor. We formally define the symmetries of this object and characterize all the affine equivariant operations in this space. We incorporate these operations into our message-passing by encoding the parameter-sharing schemes [30] as additional edge features. These additional update rules significantly improve experimental results. We note that our symmetry analysis may be useful for processing bags derived from other high-order generation policies [29; 20] by treating tuples of nodes as sets.

Inspired by these symmetries and traditional binary-based [4] and shortest path-based [38]_node-marking_ strategies, we propose four natural marking strategies for our framework. Interestingly, unlike the full-bag scenario, they vary in expressiveness, with the shortest path-based technique being the most expressive.

The flexibility and effectiveness of our full framework are illustrated in Figure 2, depicting detailed experimental results on the popular Zinc-12k dataset [31]. Our method demonstrates a significant performance boost over baseline models in the _small bag_ setting (for which they are designed), while achieving results that compare favourably to state-of-the-art Subgraph GNNs in the _full bag_ setting. Additionally, we can obtain results in-between these two regimes.

Contributions.The main contributions of this paper are: (1) the development of a novel, flexible Subgraph GNN framework that enables meaningful construction and processing of bags of subgraphs of any size; (2) a characterization of all affine invariant/equivariant layers defined on our node feature tensors; (3) a theoretical analysis of our framework, including the expressivity benefits of our node-marking strategy; and (4) a comprehensive experimental evaluation demonstrating the advantages of the new approach across both small and large bag sizes, achieving state-of-the-art results, often by a significant margin.

## 2 Related work

**Subgraph GNNs.** Subgraph GNNs [39; 8; 27; 4; 40; 26; 12; 29; 17; 38; 3] represent a graph as a collection of subgraphs, obtained by a predefined generation policy. For example, each subgraph can be generated by marking exactly one node in the original graph (see inset 3) - an approach commonly referred to as _node marking_[26]; this marked node is considered the root node in its subgraph. Several recent papers focused on scaling these methods to larger graphs, starting with basic random selection of subgraphs from the bag, and extending beyond with more sophisticated techniques that aim to learn how to select subgraphs. To elaborate, [5] introduced _Policy-Learn_ (PL), an approach based on two models, where the first model predicts a distribution over the nodes of the original graph, and the second model processes bags of subgraphs sampled from this distribution. _MAG-GNN_[20] employs a similar approach utilizing Reinforcement Learning.

Footnote 3: The Figure was taken with permission from [3]

Similarly to our approach, this method permits high-order policies by associating subgraphs with tuples rather than individual nodes, allowing for the marking of several nodes within a subgraph.

Figure 2: The performance landscape of Subgraph GNNs with varying number of subgraphs: Our method leads in the lower bag-size set, outperforming other approaches in nearly all cases. Additionally, our method matches the performance of state-of-the-art Subgraph GNNs in the full-bag setting. The full mean absolute error (MAE) scores along with standard deviations are available in Table 9 in the appendix.

However, as mentioned before, these approaches involve discrete sampling while training, making them very hard to train (1000-4000 epochs vs. \(\sim\)400 epochs of state-of-the-art methods [3; 38] on the Zinc-12k dataset), and limiting their usage to very small bags. Finally, we mention another high-order method, _OSAN_, introduced by [29], which learns a distribution over tuples that represent subgraphs with multiple node markings. In contrast to these previous approaches, we suggest a simpler and more effective way to select subgraphs and also show how to leverage the resulting symmetry structure to augment our message-passing operations.

**Symmetries in graph learning.** Many previous works have analyzed and utilized the symmetry structure that arises from graph learning setups [22; 23; 18; 2]. Specifically relevant to our paper is the work of [22] that characterized basic equivariant linear layers for graphs, the work of [1] that characterizes equivariant maps for many other types of incidence tensors that arise in graph learning, and the works [4; 12] that leveraged group symmetries for designing Subgraph GNNs in a principled way.

## 3 Preliminaries

**Notation.** Let \(\mathcal{G}\) be a family of undirected graphs, and consider a graph \(G=(V,E)\) within this family. The adjacency matrix \(A\in\mathbb{R}^{n\times n}\) defines the connectivity of the graph4, while the feature matrix \(X\in\mathbb{R}^{n\times d}\) represents the node features. Here, \(V\) and \(E\) represent the sets of nodes and edges, respectively, with \(|V|=n\) indicating the number of nodes. We use the notation \(v_{1}\sim_{A}v_{2}\) to denote that \(v_{1}\) and \(v_{2}\) are neighboring nodes according to the adjacency \(A\). Additionally, we define \([n]\coloneqq\{1,2,\ldots n\}\), and \(\mathcal{P}([n])\) as the power set of \([n]\).

Footnote 4: Edge features are also allowed but are omitted here for simplicity

**Subgraph GNNs as graph products.** In a recent work, [3] demonstrated that various types of update rules used by current Subgraph GNNs can be simulated by employing the _Cartesian graph product_ between the original graph and another graph, and running standard message passing over that newly constructed product graph. Formally, the cartesian product of two graphs \(G_{1}\) (\(n_{1}\) nodes) and \(G_{2}\) (\(n_{2}\) nodes), denoted by \(G_{1}\square G_{2}\), forms a graph with vertex set \(V(G_{1})\times V(G_{2})\). Two vertices \((u_{1},u_{2})\) and \((v_{1},v_{2})\) are adjacent if either \(u_{1}=v_{1}\) and \(u_{2}\) is adjacent to \(v_{2}\) in \(G_{2}\), or \(u_{2}=v_{2}\) and \(u_{1}\) is adjacent to \(v_{1}\) in \(G_{1}\). We denote by \(\mathcal{A}\in\mathbb{R}^{n_{1}\cdot n_{2}\times n_{1}\cdot n_{2}}\) and \(\mathcal{X}\in\mathbb{R}^{n_{1}\cdot n_{2}\times d}\) the adjacency and node feature matrices of the product graph; in general, we use calligraphic letters to denote the adjacency and feature matrices of product graphs, while capital English letters are used for those of the original graphs. In particular, for the graph cartesian product, \(G_{1}\square G_{2}\), the following holds:

\[\mathcal{A}_{G_{1}\square G_{2}}=A_{1}\otimes I+I\otimes A_{2}.\] (1)

For a detailed definition of the cartesian product of graphs, please refer to Definition A.1. As a concrete example for the analogy between Subgraph GNNs and the Cartesian product of graphs, we refer to a result by [3], which states that the maximally expressive node-based Subgraph GNN architecture GNN-SSWL\(+\)[38], can be simulated by an MPNN on the Cartesian product of the original graph with itself, denoted as \(G\square G\). As we shall see, our framework utilizes a cartesian product of the original graph and a coarsened version of it, as illustrated in Figure 1 (right).

**Equivariance.** A function \(L:U\to W\) is called equivariant if it commutes with the group action. More formally, given a group element, \(g\in\mathbb{G}\), the function \(L\) should satisfy \(L(g\cdot v)=g\cdot L(v)\) for all \(v\in U\) and \(g\in\mathbb{G}\). \(L\) is said to be invariant if \(L(g\cdot v)=L(v)\).

## 4 Coarsening-based Subgraph GNN

**Overview.** This section introduces the _Coarsening-based Subgraph GNN_ (CS-GNN) framework. The main idea is to select and process subgraphs in a principled and flexible manner through the following approach: (1) coarsen the original graph via a coarsening function, \(\mathcal{T}\) - see Figure 1(left); (2) Obtain the product graph - Figure 1(right) defined by the combination of two adjacencies, \(\mathcal{A}_{\mathcal{T}(G)}\) (red edges), \(\mathcal{A}_{G}\) (grey edges), which arise from the graph Cartesian product operation (details follow); (3) leveraging the symmetry of this product graph to develop _symmetry-based_ updates, described by \(\mathcal{A}_{\text{Equiv}}\) (this part is not visualized in Figure 1). The general update of our suggested layer takes the following form 5,\[\mathcal{X}^{t+1}(S,v)=f^{t}\Big{(}\mathcal{X}(S,v)^{t},\] (2) \[\underbrace{\{\mathcal{X}(S^{\prime},v^{\prime})^{t}\}_{\text{Original connectivity (horizontal)}}}_{\text{Original connectivity (horizontal)}},\underbrace{\{\mathcal{X}(S^{\prime},v^{\prime})^{t}\}_{\text{ $\mathcal{S}(S^{\prime},v^{\prime})\sim\mathcal{A}_{\mathcal{T}(G)}(S,v)$}} }_{\text{Induced connectivity (vertical)}},\underbrace{\{\mathcal{X}(S^{\prime},v^{\prime})^{t}\}_{\text{$ \mathcal{S}(S^{\prime},v^{\prime})\sim\mathcal{A}_{\text{Equity}}(S,v)$}}}_{ \text{Symmetry-based updates}}\Big{)},\]

where the superscript \({}^{t}\) indicates the layer index. In what follows, we further elaborate on these three steps (in Sections 4.1 to 4.2).

We note that each connectivity in Equation (2) is processed using a distinct MPNN, and after stacking of those layers, we apply a pooling layer5 to obtain a graph representation; that is, \(\rho(\mathcal{X}^{\text{T}})=\text{MLP}^{\text{T}}\Big{(}\sum_{S}\Big{(}\sum_{ v=1}^{n}\mathcal{X}^{\text{T}}(S,v)\Big{)}\Big{)}\); \(\text{T}\) denotes the final layer.

Footnote 5: For some of the theoretical analysis, this pooling operation is expressed as: \(\rho(\mathcal{X}^{\text{T}})=\text{MLP}^{\text{T}}\Big{(}\sum_{S}\big{(} \text{MLP}^{\text{T}}\big{(}\sum_{v=1}^{n}\mathcal{X}^{\text{T}}(S,v)\big{)} \big{)}\Big{)}\)

For more specific implementation details, we refer to Appendix F.

### Construction of the coarse product graph

As mentioned before, a maximally expressive node-based Subgraph GNN can be realized via the Cartesian product of the original graph with itself \(G\Box G\). In this work, we extend this concept by allowing the left operand in the product to be the coarsened version of \(G\), denoted as \(\mathcal{T}(G)\), as defined next. This idea is illustrated in Figure 1.

**Graph coarsening.** Consider a graph \(G=(V,E)\) with \(n\) nodes and an adjacency matrix \(A\). Graph coarsening is defined by the function \(\mathcal{T}:\mathcal{G}\rightarrow\mathcal{G}\), which maps \(G\) to a new graph \(\mathcal{T}(G)=(V^{\mathcal{T}},E^{\mathcal{T}})\) with an adjacency matrix \(A^{\mathcal{T}}\in\mathbb{R}^{2^{n}\times 2^{n}}\) and a feature matrix \(X^{\mathcal{T}}\in\mathbb{R}^{2^{n}\times d}\). Here, \(V^{\mathcal{T}}\), the vertex set of the new graph represents super-nodes - defined as subsets of \([n]\). Additionally, we require that nodes in \(V^{\mathcal{T}}\) induce a partition over the nodes of the original graph6. The connectivity \(E^{\mathcal{T}}\) is extremely sparse and induced from the original graph's connectivity via the following rule:

Footnote 6: Our method also supports the case of which it is not a partition.

\[A^{\mathcal{T}}(S_{1},S_{2})=\begin{cases}1&\text{if }\exists v\in S_{1}, \exists v\in S_{2}\text{ s.t. }A(v,u)=1,\\ 0&\text{otherwise},\end{cases}\] (3)

To clarify, in our running example (Figure 1), it holds that \(A^{\mathcal{T}}(\{a,b,c,d\},\{e\})=1\), while \(A^{\mathcal{T}}(\{e\},\{f\})=0\). For a more formal definition, refer to Definition A.3.

More specifically, our implementation of the graph coarsening function \(\mathcal{T}\) employs spectral clustering7[33] to partition the graph into \(T\) clusters, which in our framework controls the size of the bag. This results in a coarsened graph with fewer nodes and edges than \(G\). We highlight and stress that the space complexity of this sparse graph, \(\mathcal{T}(G)\), is upper bounded by that of the original graph \(G\) (we do not store \(2^{n}\) nodes).

Footnote 7: Other graph coarsening or clustering algorithms can be readily used as well.

Defining the (coarse) product graph \(\mathcal{T}(G)\Box G\).We define the connectivity of the product graph, see Figure 1(right), by applying the cartesian product between the coarsened graph, \(\mathcal{T}(G)\), and the original graph, \(G\). The product graph is denoted by \(\mathcal{T}(G)\Box G\), and is represented by the matrices \(\mathcal{A}_{\mathcal{T}(G)\Box G}\in\mathbb{R}^{(2^{n}\times n)\times(2^{n} \times n)}\) and \(\mathcal{X}\in\mathbb{R}^{2^{n}\times n\times d8}\), where by recalling Equation (1), we obtain,

\[\mathcal{A}_{\mathcal{T}(G)\Box G}=\overbrace{A^{\mathcal{T}(G)}\otimes I}^{ \triangleq\mathcal{A}_{\mathcal{T}(G)}}+\overbrace{I\otimes A}^{\triangleq \mathcal{A}_{G}}.\] (4)

The connectivity in this product graph induces the horizontal (\(\mathcal{A}_{G}\)) and vertical updates (\(\mathcal{A}_{\mathcal{T}(G)}\)) in Equation (2), visualized in Figure 1(right) via grey and red edges, respectively.

### Symmetry-based updates

In the previous subsection, we used a combination of a coarsening function and the graph Cartesian product to derive the two induced connectivities \(\mathcal{A}_{G},\mathcal{A}_{\mathcal{T}(G)}\) of our product graph. We use these connectivities to to perform message-passing on our product graph (see Equation (2)).

Inspired by recent literature on Subgraph GNNs [12; 3; 38], which incorporates and analyzes additional non-local updates arising from various symmetries (e.g., updating a node's representation via all nodes in its subgraphs), this section aims to identify potential new updates that can be utilized over our product graph. To that end, we study the symmetry structure of the node feature tensor in our product graph, \(\mathcal{X}(S,v)\).The new updates described below will result in the third term in Equation (2), dubbed _Symmetry-based updates_ (\(\mathcal{A}_{\text{Equiv}}\)). For better clarity in this derivation, we change the notation from nodes (\(v\)) to indices (\(i\)).

#### 4.2.1 Symmetries of our product graph

Since the order of nodes in the original graph \(G\) is arbitrary, each layer in our architecture must exhibit equivariance to any induced changes in the product graph. This requires maintaining equivariance to permutations of nodes in both the original graph and its transformation \(\mathcal{T}(G)\). As a result, recalling that \(\mathcal{A}\in\mathbb{R}^{(2^{n}\times n)\times(2^{n}\times n)}\) and \(\mathcal{X}\in\mathbb{R}^{2^{n}\times n\times d}\) represent the adjacency and feature matrices of the product graph, the symmetries of the product graph are defined by an action of the symmetric group \(S_{n}\). Formally, a permutation \(\sigma\in S_{n}\) acts on the adjacency and feature matrices by:

\[(\sigma\cdot\mathcal{A})\big{(}S_{1},i_{1},S_{2},i_{2}\big{)} =\mathcal{A}\big{(}\sigma^{-1}(S_{1}),\sigma^{-1}(i_{1}),\sigma^{ -1}(S_{2}),\sigma^{-1}(i_{2})\big{)},\] (5) \[(\sigma\cdot\mathcal{X})(S,i) =\mathcal{X}\big{(}\sigma^{-1}(S),\sigma^{-1}(i)\big{)},\] (6)

where we define the action of \(\sigma\in S_{n}\) on a set \(S=\{i_{1},i_{2},\ldots,i_{k}\}\) of size \(k\) as: \(\sigma\cdot S:=\{\sigma^{-1}(i_{1}),\sigma^{-1}(i_{2}),\ldots,\sigma^{-1}(i_{ k})\}\coloneqq\sigma^{-1}(S)\).

#### 4.2.2 Derivation of linear equivariant layers for the node feature tensor

We now characterize the linear equivariant layers with respect to the symmetry defined above, focusing on Equation (6). We adopt a similar notation to [22], and assume for simplicity that the number of feature channels is \(d=1\) (extension to multiple features is straightforward [22]). In addition, our analysis considers the case where \(V^{\mathcal{T}}\) encompasses all potential super-nodes formed by subsets of \([n]\) (i.e we use the sparse coarsened adjacency9).

Footnote 9: This is because the action of \(S_{n}\) is well defined over the index set \(P(V[[n]])\times[n]\) but not over \(V\times V^{\mathcal{T}}\)

Our main tool is the characterization of linear equivariant layers for permutation symmetries as parameter-sharing schemes [34; 30; 22]. In a nutshell, this characterization states that the parameter vectors of the biases, invariant layers, and equivariant layers can be expressed as a learned weighted sum of basis tensors, where the basis tensors are indicators of the orbits induced by the group action on the respective index spaces. We focus here on presenting the final results and summarize them in Proposition 4.1 at the end of this subsection. Detailed discussion and derivations are available in Appendix E.

**Equivariant bias and invariant layers.** The bias vectors of the linear layers in our space are in \(\mathbb{R}^{2^{n}\times n}\). As shown in Figure 3(right), the set of orbits induced by the action of \(S_{n}\) satisfies:

\[(\mathcal{P}([n])\times[n])/S_{n}\coloneqq\{\gamma^{k^{*}}:k=1,\ldots,n;*\in\{ +,-\}\}.\] (7)

Here, \(\gamma^{k^{+}}\) corresponds to all pairs \((S,i)\in\mathcal{P}([n])\times[n]\) with \(|S|=k\) and \(i\notin S\), and \(\gamma^{k^{-}}\) to all pairs with \(|S|=k\) and \(i\in S\).

As stated in [34; 30; 22], the tensor set \(\{\mathbf{B}^{\gamma}_{S,i}\}_{\gamma\in(\mathcal{P}([n])\times[n])/S_{n}}\) where:

\[\mathbf{B}^{\gamma}_{S,i}=\begin{cases}1,&\text{if }(S,i)\in\gamma;\\ 0,&\text{otherwise}.\end{cases}\] (8)

are a basis of the space of bias vectors of the invariant linear layers induced by the action of \(S_{n}\).

**Weight matrices.** Following similar reasoning, consider elements \((S_{1},i_{1},S_{2},i_{2})\in(\mathcal{P}([n])\times[n]\times\mathcal{P}([n])\times [n])\). In Appendix E we characterize the orbits of \(S_{n}\) in this space as a partition in which each partition set is defined according to six conditions. Some of these conditions include the sizes of \(S_{1}\), \(S_{2}\) and \(S_{1}\cap S_{2}\), which remain invariant under permutations. Given an orbit, \(\Gamma\in(\mathcal{P}([n])\times[n]\times\mathcal{P}([n])\times[n])/S_{n}\), we define a basis tensor, \(\mathbf{B}^{\Gamma}\in\mathbb{R}^{2^{n}\times n\times 2^{n}\times n}\) by setting:

\[\mathbf{B}^{\Gamma}_{S_{1},i_{1};S_{2},i_{2}}=\begin{cases}1,&\text{if }(S_{1},i_{1},S_{2},i_{2})\in\Gamma;\\ 0,&\text{otherwise.}\end{cases}\] (9)

A visualization of the two basis vectors in Equations (8) and (9), is available in Figure 3. The following (informal) proposition summarizes the results in this section (the proof is given in Appendix G),

**Proposition 4.1** (Basis of Invariant (Equivariant) Layers).: _The tensors \(\mathbf{B}^{\gamma}\) (\(\mathbf{B}^{\Gamma}\)) in Equation (8) (Equation (9)) form an orthogonal basis (in the standard inner product) of the invariant layers and biases (Equivariant layers - weight matrix)._

#### 4.2.3 Incorporating symmetry-based updates in our framework

In the previous subsection, we derived all possible linear invariant and equivariant operations that respect the symmetries of our product graph. We now use this derivation to define the symmetry-based updates in Equation (2), which correspond to the construction of \(\mathcal{A}_{\text{Equiv}}\) and the application of an MPNN.

To begin, we note that any linear equivariant layer can be realized through an MPNN [13] applied to a fully connected graph with appropriate edge features. This is formally stated in Lemma F.1, the main idea is to encode the parameters on the edges of this graph (see visualization inset). Thus, the natural construction of \(\mathcal{A}_{\text{Equiv}}\) corresponds to a fully connected graph, with appropriate edge features derived from the parameter-sharing scheme we have developed.

However, one of our main goals and guidelines in developing our flexible framework is to maintain efficiency, and to align with the (node-based) maximally expressive GNN, namely GNN-SSWL\(+\)[38, 3], for the case of a trivial coarsening function, \(\mathcal{T}(G)=G\) (which correspond to the full-bag setting). To achieve this, we opt for a sparser choice by using only a subset of the basis vectors (defined in Equation (9)) to construct \(\mathcal{A}_{\text{Equiv}}\). Specifically, the matrix \(\mathcal{A}_{\text{Equiv}}\) corresponding to the chosen subset of basis vectors is visualized inset - the parameter-sharing scheme is represented by edges with matching

Figure 3: Visualization via heatmaps (different colors correspond to different parameters) of the parameter-sharing scheme determined by symmetries for a graph with \(n=6\) nodes, zooming-in on the block which corresponds to sets of size two. **Left:** Visualization of the weight matrix for the equivariant basis \(\mathbf{B}^{\Gamma}_{S_{1},i_{1};S_{2},i_{2}}\) (a total of 35 parameters in the block). **Right:** Visualization of the bias vector for the invariant basis \(\mathbf{B}^{\gamma}_{S,i}\) (a total of 2 parameters in the block). Symmetry-based updates reduce parameters more effectively than previously proposed linear equivariant layers by treating indices as unordered tuples (see Appendix E.3 for a discussion).

colors. To clarify, the nodes \((S,v)\) that satisfy \(v\in S\) "send messages" (i.e., broadcast their representation) to all the nodes \((S^{\prime},v^{\prime})\) such that \(v=v^{\prime}\). A more formal discussion regarding our implementation of those symmetry based updates is given in Appendix F.4.

**Maintaining sparsity.** While the updates above are defined over the sparse representation of the coarse product graph, in practice we use its dense representation, treating it as a graph over the set of nodes \(V\times V^{\mathcal{T}}\), which requires space complexity \(\mathcal{O}(T\cdot|V|)\). The update rules above are adapted to this representation simply by masking all nodes \((S,v)\) in the sparse representation such that \(S\notin V^{\mathcal{T}}\). We note the models using the resulting update rule remain invariant to the action of \(S_{n}\). See discussion in [1].

### Marking Strategies and Theoretical Analysis

One of the key components of subgraph architectures is their marking strategy. Two widely used approaches in node-based subgraph architectures are binary-based node marking [4] and distance-based marking [38], which were proven to be equally expressive in the full-bag setup [38]. Empirically, distance-based marking has been demonstrated to outperform other strategies across several standard benchmarks. In this section, our aim is to develop and theoretically justify an appropriate marking strategy, specifically tailored to the structure of our product graph. We present and discuss here our main results, and refer to Appendix C for a more formal discussion.

Building on existing marking strategies and considering the unique structure of our product graph, we propose two natural extensions to both the binary node marking [4] and distance-based marking strategies [38]. Extending binary node marking, we first suggest _Simple Marking_ (\(\pi_{S}\)), where an element \((S,v)\) is assigned a binary feature that indicates whether node \(v\) belongs to subgraph \(S\) (\(v\in S\)). The second extension, _Node + Size Marking_ (\(\pi_{SS}\)), builds on the _simple marking_ by assigning an additional feature that encodes the size of the super-node \(S\).

For distance-based strategies, we propose _Minimum Distance_ (\(\pi_{MD}\)), where each element \((S,v)\) is assigned the smallest (minimal) shortest path distance (SPD) from node \(v\) to any node \(u\in S\). Finally, _Learned Distance Function_ (\(\pi_{LD}\)) extends this further by assigning to each element \((S,v)\) the output of a permutation-invariant learned function, which takes the set of SPDs between node \(v\) and the nodes in \(S\) as input.

Surprisingly, unlike the node-based full-bag case, we find that these marking strategies are not all equally expressive. We conveniently gather the first three strategies as \(\Pi=\{\pi_{S},\pi_{SS},\pi_{MD}\}\) and summarize the relation between all variants as follows:

**Proposition 4.2** (Informal - Expressivity of marking strategies.).: _(i) Strategies in \(\Pi\) are all equally expressive, independently of the transformation function \(\mathcal{T}\). (ii) The strategy \(\pi_{LD}\) is at least as expressive as strategies in \(\Pi\). Additionally, there exists transformation functions s.t. it is strictly more expressive than all of them._

The above is formally stated in Propositions C.1 and C.2, and more thoroughly discussed in Appendix C. In light of the above proposition, we instantiate the learned distance function \(\pi_{LD}\) strategy when implementing our model, as follows,

\[\mathcal{X}_{S,v}\leftarrow\sum_{u\in S}z_{d_{G}(v,u)}\] (10)

where \(d_{G}(v,u)\) denotes the _shortest path distance_ between nodes \(v\) and \(u\) in the original \(G\)10.

Footnote 10: To facilitate this, we maintain a lookup table where each index corresponds to a shortest path distance, assigning a learnable embedding, \(z_{d_{G}(v,u)}\in\mathbb{R}^{d}\), to each node \((S,v)\).

**Coarsening Function and Expressivity.** We investigate whether our CS-GNN framework offers more expressiveness compared to directly integrating information between the coarsened graph and the original graph.

The two propositions below illustrate that a simple, straight forward integration of the coarsen graph with the original graph (this integration is referred to as the _sum graph_ - formally defined in Definition D.2), and further processing it via standard message-passing, results in a less expressive architecture. Furthermore, when certain coarsening functions are employed within the CS-GNN framework, our resulting architecture becomes strictly more expressive than conventional node-basedsubgraph GNNs. These results suggest that the interplay between the coarsening function and the subgraph layers we have developed enhances the model's overall performance. We summarize this informally below and provide a more formal discussion in Appendix D.

**Proposition 4.3** (Informal - CS-GNN goes beyond coarsening).: _For any transformation function \(\mathcal{T}\), CS-GNN can implement message-passing on the sum graph, hence being at least as expressive. Also, there exist transformations \(\mathcal{T}\)'s s.t. CS-GNN is strictly more expressive than that._

**Proposition 4.4** (Informal - CS-GNN vs node based subgraphs).: _There exist transformations \(\mathcal{T}\)'s s.t. our CS-GNN model using \(\mathcal{T}\) as its coarsening function is strictly more expressive than GNN-SSWL\(+\)._

## 5 Experiments

We experimented extensively over seven different datasets to answer the following questions: _(Q1) Can CS-GNN outperform efficient Subgraph GNNs operating on small bags? (Q2) Does the additional symmetry-based updates boost performance? (Q3) Does CS-GNN offer a good solution in settings where full-bag Subgraph GNNs cannot be applied? (Q4) Does CS-GNN in the full-bag setting validate its theory and match state-of-the-art full-bag Subgraph GNNs?_

In the following sections, we present our main results and refer to Appendix F for additional experiments and details.

**Baselines.** For each task, we include several baselines. The Random baseline corresponds to random subgraph selection. We report the best performing random baseline from all prior work [5; 20; 29; 3]. The other two (non-random) baselines are: (1) Learned[5; 20; 29], which represents methods that learn the specific subgraphs to be used; and (2) Full[38; 3], which corresponds to full-bag Subgraph GNNs.

**ZINC.** We experimented with both the ZINC-12k and ZINC-Full datasets [31; 14; 10], adhering to a \(500k\) parameter budget as prescribed. As shown in Table 1, CS-GNN outperforms all efficient baselines by a significant margin, with at least a \(+0.008\) MAE improvement for bag sizes \(T\in\{3,4,5\}\). Additionally, in the full-bag setting, our method recovers state-of-the-art results. The results for ZINC-Full are available in Table 8 in the Appendix.

**OGB.** We tested our framework on several datasets from the OGB benchmark collection [16]. Table 4 shows the performance of our method compared to both efficient and full-bag Subgraph GNNs. Our CS-GNN outperforms all baselines across all datasets for bag sizes \(T\in\{2,5\}\), except for the moldiv dataset with \(T=2\), where PL achieves the best results and our method ranks second. In the full-bag setting, CS-GNN is slightly outperformed by the top-performing Subgraph GNNs but still offers comparable results.

**Peptides.** We experimented on the Peptides-func and Peptides-struct datasets [9] - which full-bag Subgraph GNNs already struggle to process - evaluating CS-GNN's ability to scale to larger graphs. The results are summarized in Table 2. CS-GNN outperforms all MPNN variants, even when incorporating structural encodings such as GATEDGCN+RWSE. Additionally, our method surpasses the random11 baseline on both datasets.

\begin{table}
\begin{tabular}{l c|c} \hline \hline \multirow{2}{*}{Model \(\downarrow\)/ Dataset \(\rightarrow\)} & Peptides-func & Peptides-struct \\  & (AP \(\uparrow\)) & (MAE \(\downarrow\)) \\ \hline GCN [19] & \(0.5930\pm 0.0023\) & \(0.3496\pm 0.0013\) \\ GIN [35] & \(0.5498\pm 0.0079\) & \(0.3547\pm 0.0045\) \\ GatedGCN [7] & \(0.5864\pm 0.0077\) & \(0.3420\pm 0.0013\) \\ GatedGCN+RWSE [9] & \(0.6069\pm 0.0035\) & \(0.3357\pm 0.0006\) \\ \hline Random [3] & \(0.5924\pm 0.005\) & \(0.2594\pm 0.0021\) \\ Ours & \(\mathbf{0.6156}\pm 0.0080\) & \(\mathbf{0.2539}\pm 0.0015\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on Peptides dataset.

\begin{table}
\begin{tabular}{l c|c} \hline \hline
**Method** & Bag size & ZINC (MAE \(\downarrow\)) \\ \hline GNN [35] & \(T=1\) & \(0.163\pm 0.004\) \\ \hline OGAN [29] & \(T=2\) & \(0.177\pm 0.016\) \\ Random [20] & \(T=2\) & \(0.131\pm 0.005\) \\ PL [5] & \(T=2\) & \(0.120\pm 0.003\) \\ Mag-GNN [20] & \(T=2\) & \(\mathbf{0.106}\pm 0.014\) \\ Ours & \(T=2\) & \(\mathbf{0.109}\pm 0.005\) \\ \hline Random [20] & \(T=3\) & \(0.124\pm N/A\) \\ Mag-GNN [20] & \(T=3\) & \(\mathbf{0.104}\pm N/A\) \\ Ours & \(T=3\) & \(\mathbf{0.096}\pm 0.005\) \\ \hline Random [20] & \(T=4\) & \(0.125\pm N/A\) \\ Map-GNN [20] & \(T=4\) & \(\mathbf{0.101}\pm N/A\) \\ Ours & \(T=4\) & \(\mathbf{0.090}\pm 0.003\) \\ \hline Random [5] & \(T=5\) & \(0.113\pm 0.006\) \\ PL [5] & \(T=5\) & \(\mathbf{0.109}\pm 0.005\) \\ Ours & \(T=5\) & \(\mathbf{0.095}\pm 0.003\) \\ \hline GNN-SSWL\(+\)[38] & Full & \(0.070\pm 0.005\) \\ Subgraphformer [3] & Full & \(0.067\pm 0.007\) \\ Subgraphformer+PE [3] & Full & \(\mathbf{0.063}\pm 0.001\) \\ Ours & Full & \(\mathbf{0.062}\pm 0.0007\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on Zinc-12k dataset. Top two results are reported as First and Second.

[MISSING_PAGE_EMPTY:10]

## References

* Albooyeh et al. [2019] Marjan Albooyeh, Daniele Bertolini, and Siamak Ravanbakhsh. Incidence networks for geometric deep learning. _arXiv preprint arXiv:1905.11460_, 2019.
* Azizian and Lelarge [2020] Waiss Azizian and Marc Lelarge. Expressive power of invariant and equivariant graph neural networks. _arXiv preprint arXiv:2006.15646_, 2020.
* Bar-Shalom et al. [2024] Guy Bar-Shalom, Beatrice Bevilacqua, and Haggai Maron. Subgraphormer: Unifying subgraph GNNs and graph transformers via graph products. In _Forty-first International Conference on Machine Learning_, 2024. URL https://openreview.net/forum?id=6djDWVTUEq.
* Bevilacqua et al. [2022] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. _International Conference on Learning Representations_, 2022.
* Bevilacqua et al. [2024] Beatrice Bevilacqua, Moshe Eliasof, Eli Meirom, Bruno Ribeiro, and Haggai Maron. Efficient subgraph gnns by learning effective selection policies. _International Conference on Learning Representations_, 2024.
* Biewald [2020] Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/. Software available from wandb.com.
* Bresson and Laurent [2017] Xavier Bresson and Thomas Laurent. Residual gated graph convnets. _arXiv preprint arXiv:1711.07553_, 2017.
* Cotta et al. [2021] Leonardo Cotta, Christopher Morris, and Bruno Ribeiro. Reconstruction for powerful graph representations. In _Advances in Neural Information Processing Systems_, volume 34, 2021.
* Dwivedi et al. [2022] Vijay Prakash Dwivedi, Ladislav Rampasek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. _Advances in Neural Information Processing Systems_, 35:22326-22340, 2022.
* Dwivedi et al. [2023] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. _Journal of Machine Learning Research_, 24(43):1-48, 2023.
* Fey and Lenssen [2019] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* Frasca et al. [2022] Fabrizio Frasca, Beatrice Bevilacqua, Michael Bronstein, and Haggai Maron. Understanding and extending subgraph gnns by rethinking their symmetries. _Advances in Neural Information Processing Systems_, 35:31376-31390, 2022.
* Gilmer et al. [2017] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.
* Gomez-Bombarelli et al. [2018] Rafael Gomez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose Miguel Hernandez-Lobato, Benjamin Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alan Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. _ACS central science_, 4(2):268-276, 2018.
* Hu et al. [2019] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. _arXiv preprint arXiv:1905.12265_, 2019.
* Hu et al. [2020] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_, 33:22118-22133, 2020.
* Huang et al. [2022] Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang. Boosting the cycle counting power of graph neural networks with i\({}^{2}\)-gnns. In _The Eleventh International Conference on Learning Representations_, 2022.

* [18] Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* [19] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _International Conference on Learning Representations_, 2016.
* [20] Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, and Muhan Zhang. Maggnn: Reinforcement learning boosted graph neural network. _Advances in Neural Information Processing Systems_, 36, 2024.
* [21] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In _International Conference on Machine Learning_, 2021.
* [22] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. _arXiv preprint arXiv:1812.09902_, 2018.
* [23] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. _Advances in neural information processing systems_, 32, 2019.
* [24] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 4602-4609, 2019.
* [25] Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M Krieg, Martin Grohe, Matthias Fey, and Karsten Borgwardt. Weisfeiler and leman go machine learning: The story so far. _arXiv preprint arXiv:2112.09992_, 2021.
* [26] Pal Andras Papp and Roger Wattenhofer. A theoretical comparison of graph neural network extensions. In _International Conference on Machine Learning_, pages 17323-17345. PMLR, 2022.
* [27] Pal Andras Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. Dropgnn: Random dropouts increase the expressiveness of graph neural networks. _Advances in Neural Information Processing Systems_, 34:21997-22009, 2021.
* [28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [29] Chendi Qian, Gaurav Rattan, Floris Geerts, Mathias Niepert, and Christopher Morris. Ordered subgraph aggregation networks. _Advances in Neural Information Processing Systems_, 35:21030-21045, 2022.
* [30] Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parameter-sharing. In _International conference on machine learning_, pages 2892-2901. PMLR, 2017.
* [31] Teague Sterling and John J Irwin. Zinc 15-ligand discovery for everyone. _Journal of chemical information and modeling_, 55(11):2324-2337, 2015.
* [32] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. _International Conference on Learning Representations_, 2017.
* [33] Ulrike Von Luxburg. A tutorial on spectral clustering. _Statistics and computing_, 17:395-416, 2007.
* [34] Jeffrey Wood and John Shawe-Taylor. Representation theory and invariant neural networks. _Discrete applied mathematics_, 69(1-2):33-60, 1996.
* [35] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _International Conference on Learning Representations_, 2018.

* [36] Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small relu networks are powerful memorizers: a tight analysis of memorization capacity. _Advances in Neural Information Processing Systems_, 32, 2019.
* [37] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. _Advances in neural information processing systems_, 30, 2017.
* [38] Bohang Zhang, Guhao Feng, Yiheng Du, Di He, and Liwei Wang. A complete expressiveness hierarchy for subgraph gnns via subgraph weisfeiler-lehman tests. _International Conference on Machine Learning_, 2023.
* [39] Muhan Zhang and Pan Li. Nested graph neural networks. In _Advances in Neural Information Processing Systems_, volume 34, 2021.
* [40] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any GNN with local structure awareness. In _International Conference on Learning Representations_, 2022.

## Appendix

The appendix is organized as follows:

* In Appendix A, we provide some basic definitions that will be used in later sections of the paper.
* In Appendix B we discuss some theoretical aspects of our model implementation, and its relation to Equation (2).
* In Appendix C we define four natural general node marking policies and analyze their theoretical effects on our model, as well as their relation to some node-based node marking policies. Finally, we provide a principled derivation of one of these policies using the natural symmetry of our base object.
* In Appendix D.1 we compare our model to node-based subgraph GNNs, which are the most widely used variant of subgraph GNNs. Additionally, we demonstrate that different choices of coarsening functions can recover various existing subgraph GNN designs.
* In Appendix D.2 we demonstrate how our model can leverage the information provided by the coarsening function in an effective way, comparing its expressivity to a natural baseline which also leverages the coarsening function. We show that for all coarsening functions, we are at least as expressive as the baseline and that for some coarsening functions, our model is strictly more expressive.
* In Appendix E we delve deeper into the characterization of all linear maps \(L:\mathbb{R}^{\mathcal{P}([n])\times[n]}\rightarrow\mathbb{R}^{\mathcal{P}([n ])\times[n]}\) that are equivariant to the action of the symmetric group.
* In Appendix F we provide experimental details to reproduce the results in Section 5, as well as a comprehensive set of ablation studies.
* In Appendix G we provide detailed proofs to all propositions in this paper.

## Appendix A Basic Definitions

We devote this section to formally defining the key concepts of this paper, as well as introducing new useful notation. We start by defining the two principle components of our pipeline, the cartesian product graph and the coarsening function:

**Definition A.1** (Cartesian Product Graph).: _Given two graphs \(G_{1}\) and \(G_{2}\), their Cartesian product \(G_{1}\Box G_{2}\) is defined as:_

* _The vertex set_ \(V(G_{1}\Box G_{2})=V(G_{1})\times V(G_{2})\)_._
* _Vertices_ \((u_{1},u_{2})\) _and_ \((v_{1},v_{2})\) _in_ \(G_{1}\Box G_{2}\) _are adjacent if:_
* \(u_{1}=v_{1}\) _and_ \(u_{2}\) _is adjacent to_ \(v_{2}\) _in_ \(G_{2}\)_, or_
* \(u_{2}=v_{2}\) _and_ \(u_{1}\) _is adjacent to_ \(v_{1}\) _in_ \(G_{1}\)_._

**Definition A.2** (Coarsening Function).: _A Coarsening function \(\mathcal{T}(\cdot)\) is defined as a function that, given a graph \(G=(V,E)\) with vertex set \(V=[n]\) and adjacency matrix \(A\in\mathbb{R}^{n\times n}\), takes \(A\) as input and returns a set of "super-nodes" \(\mathcal{T}(A)\subseteq\mathcal{P}([n])\). The function \(\mathcal{T}(\cdot)\) is considered equivariant if, for any permutation \(\sigma\in S_{n}\), the following condition holds:_

\[\mathcal{T}(\sigma\cdot A)=\sigma\cdot\mathcal{T}(A).\] (11)

_Here, \(\sigma\cdot A\), and \(\sigma\cdot\mathcal{T}(A)\) represent the group action of the symmetric group \(S_{n}\) on \(\mathbb{R}^{n\times n}\), and \(\mathcal{P}([n])\) respectively._

A coarsening function allows us to naturally define a graph structure on the "super-nodes" obtained from a given graph in the following way:

**Definition A.3** (Coarsened Graph).: _Given a coarsening function \(\mathcal{T}(\cdot)\) and a graph \(G=(V,E)\) with vertex set \(V=[n]\), adjacency matrix \(A\in\mathbb{R}^{n\times n}\), we abuse notation and define the coarsened graph \(\mathcal{T}(G)=(V^{\mathcal{T}},E^{\mathcal{T}})\) as follows:_

* \(V^{\mathcal{T}}=\mathcal{T}(A)\)* \(E^{\mathcal{T}}=\{\{S,S^{\prime}\}\mid S,S^{\prime}\in\mathcal{T}(A),\;\exists i\in S,i^{\prime}\in S^{\prime}\text{ s.t. }A_{i,i^{\prime}}=1\}\)_._

_The adjacency matrix of the coarsened graph can be expressed in two ways. The dense representation \(A^{\mathcal{T}}_{\text{dense}}\in\mathbb{R}^{|V^{\mathcal{T}}|\times|V^{ \mathcal{T}}|}\) is defined by:_

\[A^{\mathcal{T}}_{\text{dense}}(S,S^{\prime})=\begin{cases}1&\{S,S^{\prime}\} \in E^{\mathcal{T}}\\ 0&\text{otherwise}.\end{cases}\] (12)

_The sparse representation \(A^{\mathcal{T}}_{\text{sparse}}\in\mathbb{R}^{\mathcal{P}([n])\times\mathcal{ P}([n])}\) is defined by:_

\[A^{\mathcal{T}}_{\text{sparse}}(S,S^{\prime})=\begin{cases}1&S,S^{\prime}\in V ^{\mathcal{T}},\{S,S^{\prime}\}\in E^{\mathcal{T}}\\ 0&\text{otherwise}.\end{cases}\] (13)

We note that if the coarsened graph \(\mathcal{T}(G)\) has a corresponding node feature map \(\mathcal{X}:V^{\mathcal{T}}\rightarrow\mathbb{R}^{d}\), it also has sparse and dense vector representations defined similarly. Though the dense representation seems more natural, the sparse representation is also useful, as the symmetric group \(S_{n}\) acts on it by:

\[\sigma\cdot A^{\mathcal{T}}_{\text{sparse}}(S,S^{\prime})=A^{\mathcal{T}}_{ \text{sparse}}(\sigma^{-1}(S),\sigma^{-1}(S^{\prime})).\] (14)

When the type of representation is clear from context, we abuse notation and write \(A^{\mathcal{T}}\). Note also that in the above discussion, we have used the term "node feature map". Throughout this paper, in order to denote the node features of a graph \(G=(V,E)\) with \(|V|=n\), we use both the vector representation \(X\in\mathbb{R}^{n\times d}\) and the map representation \(\mathcal{X}:V\rightarrow\mathbb{R}^{d}\) interchangeably. Now, recalling that our pipeline is defined to create and update a node feature map \(\mathcal{X}(S,v)\) supported on the nodes of the product graph \(G\square\mathcal{T}(G)\), we define a general node marking policy, the following way:

**Definition A.4** (General Node Marking Policy).: _A general node marking policy \(\pi(\cdot,\cdot)\), is a function which takes as input a graph \(G=(V,E)\), and a coarsening function \(\mathcal{T}(\cdot)\), and returns a node feature map \(\mathcal{X}:V^{\mathcal{T}}\times V\rightarrow\mathcal{R}^{d}\)._

In Appendix C We provide four different node marking policies, and analyze the effect on our pipeline. We now move on to define the general way in which we update a given node feature map on the product graph.

**Definition A.5** (General CS-GNNLayer Update).: _Given a graph \(G=(V,E)\) and a coarsening function \(\mathcal{T}(\cdot)\), let \(\mathcal{X}^{t}(S,v):V\times V^{\mathcal{T}}\rightarrow\mathcal{R}^{d}\) denote the node feature map at layer \(t\). The general CS-GNNlayer update is defined by:_

\[\begin{split}\mathcal{X}^{t+1}(S,v)&=f^{t}\bigg{(} \mathcal{X}^{t}(S,v),\\ &\text{agg}_{1}^{t}\{(\mathcal{X}^{t}(S,v^{\prime}),e_{v,v^{ \prime}})\mid v^{\prime}\sim_{G}v\},\\ &\text{agg}_{2}^{t}\{(\mathcal{X}^{t}(S^{\prime},v),\tilde{e}_{S,S^{\prime}})\mid S^{\prime}\sim_{G^{\mathcal{T}}}S\},\\ &\text{agg}_{3}^{t}\{(\mathcal{X}^{t}(S^{\prime},v),z(S,v,S^{ \prime},v))\mid S^{\prime}\in V^{\mathcal{T}}\text{s.t. }v\in S^{\prime}\},\\ &\text{agg}_{4}^{t}\{(\mathcal{X}^{t}(S,v^{\prime}),z(S,v,S,v^{ \prime}))\mid v^{\prime}\in Vs.t.\ v^{\prime}\in S\}\bigg{)}.\end{split}\] (15)

_Here, \(f^{t}\) is an arbitrary (parameterized) continuous function, \(\text{agg}_{i}^{t},\;i=1,\ldots 4\) are learnable permutation invariant aggregation functions, \(e_{v,v^{\prime}},\tilde{e}_{S,S^{\prime}}\) are the (optional) edge features of \(G\) and \(\mathcal{T}(G)\) respectively and the function \(z:\mathcal{P}([n])\times[n]\times\mathcal{P}([n])\times[n]\rightarrow\mathbb{R} ^{d}\) maps each tuple of indices \(\mathbf{v}=(S,v,S^{\prime},v^{\prime})\) to a vector uniquely encoding the orbit of \(\mathbf{v}\) under the action of \(S_{n}\) as described in 73._

We note that for brevity, the notation used in the main body of the paper omits the aggregation functions \(\text{agg}_{1}^{t},\ldots,\text{agg}_{4}^{t}\) and the edge features from the formulation of some of the layer updates. However, we explicitly state each component of the update, as we heavily utilize them in later proofs. We also note that this update is different than the general layer update presented in Equation (2), as it doesn't use all global updates characterized in 9. The reason for this is that some of the global updates have an asymptotic runtime of \(\tilde{\mathcal{O}}(n^{2})\)where \(n\) is the number of nodes in the input graph. As our goal was to create models that improve on the scalability of standard subgraph GNNs which have an asymptotic runtime of \(\tilde{\mathcal{O}}(n^{2})\), We decided to discard some of the general global updates and keep only the ones that are induced by the last two entries in equation 15 which all have a linear runtime. After a stacking of the layers in Equation (15), we employ the following pooling procedure on the final node feature map \(\mathcal{X}^{T}\):

\[\rho(\mathcal{X}^{T})=\texttt{MLP}_{2}\left(\sum_{S\in V^{T}}\left(\texttt{ MLP}_{1}\big{(}\sum_{v\in V}\mathcal{X}^{T}(S,v)\big{)}\right)\right).\] (16)

Finally, we define the set of all functions that can be expressed by our model:

**Definition A.6** (Expressivity of Family of Graph Functions).: _Let \(\mathcal{F}\) be a family of graph functions, we say that \(\mathcal{F}\) can express a graph function \(g(\cdot)\) if for every finite family of graphs \(\mathcal{G}\) there exists a function \(f\in\mathcal{F}\) such that:_

\[f(G)=g(G)\quad\forall G\in\mathcal{G}.\] (17)

_Here, \(\mathcal{G}\) is a finite family of graphs if all possible values of node/edge features of the graphs in \(\mathcal{G}\) form a finite set, and the maximal size of the graphs within \(\mathcal{G}\) is bounded._

**Definition A.7** (Family of Functions Expressed By CS-GNN).: _Let \(\pi\) be a general node marking policy and \(\mathcal{T}\) be a coarsening function. Define \(\mathcal{S}(\mathcal{T},\pi)\) to be the family of graph functions, which when given input graph \(G=(V,E)\), first compute \(\mathcal{X}^{0}(S,v)\) using \(\pi(G,\mathcal{T})\), then update this node feature map by stacking \(T\) layers of the form 15, and finally pooling \(\mathcal{X}^{0}(S,v)\) using equation 16. We define CS-GNN\((\mathcal{T},\pi)\) to be the set of all functions that can be expressed by \(\mathcal{S}(\mathcal{T},\pi)\)._

## Appendix B Theoretical Validation of Implementation Details

In this section, we provide implementation details of our model and prove that they enable us to recover the conceptual framework of the model discussed thus far. First, we note that in Section 4.2, we characterized all equivariant linear maps \(L:\mathbb{R}^{\mathcal{P}([n])\times[n]}\rightarrow\mathbb{R}^{\mathcal{P}([n] )\times[n]}\) in order to incorporate them into our layer update. Given the high dimensionality of the space of all such linear maps, and in order to save parameters, we demonstrate that it is possible to integrate these layers into our layer update by adding edge features to a standard MPNN model. This is formalized in the following proposition:

**Lemma B.1** (Parameter Sharing as MPNN).: _Let \(B_{1},\ldots B_{k}:\mathbb{R}^{n\times n}\) be orthogonal matrices with entries restricted to 0 or 1, and let \(W_{1},\ldots W_{k}\in\mathbb{R}^{d\times d^{l}}\) denote a sequence of weight matrices. Define \(B_{+}=\sum_{i=1}^{k}B_{i}\) and choose \(z_{1},\ldots z_{k}\in\mathbb{R}^{d^{+}}\) to be a set of unique vectors representing an encoding of the index set. The function that represents an update via parameter sharing:_

\[f(X)=\sum_{i=1}^{k}B_{i}XW_{i},\] (18)

_can be implemented on any finite family of graphs \(\mathcal{G}\), by a stack of MPNN layers of the following form [13],_

\[m_{v}^{l}=\sum_{u\in N_{B_{+}}(v)}M^{l}(X_{u}^{l},e_{u,v}),\] (19)

\[X_{v}^{l+1}=U^{l}(X_{v}^{l},m_{v}^{l}),\] (20)

_where \(U^{l},M^{l}\) are multilayer perceptrons (MLPs). The inputs to this MPNN are the adjacency matrix \(B_{+}\), node feature vector \(X\), and edge features - the feature of edge \((u,v)\) is given by:_

\[e_{u,v}=\sum_{i=1}^{k}z_{i}\cdot B_{i}(u,v).\] (21)

_Here, \(B_{i}(u,v)\) denotes the \((u,v)\) entry to matrix \(B_{i}\)._

The proof is given in Appendix G. The analysis in Section 4.2 demonstrates that the basis of the space of all equivariant linear maps \(L:\mathbb{R}^{\mathcal{P}([n])\times[n]}\rightarrow\mathbb{R}^{\mathcal{P}([n ])\times[n]}\) satisfies the conditions of Lemma F.1. Additionally, we notice that some of the equivariant linear functions have an asymptotic runtime of \(\tilde{\mathcal{O}}(n^{2})\) where \(n\) is the number of nodes in the input graph. As our main goal is to construct a more scalable alternative to node-based subgraph GNNs, which also have a runtime of \(\tilde{\mathcal{O}}(n^{2})\), we limit ourselves to a subset of the basis for which all maps run in linear time. This is implemented by adding edge features to the adjacency matrices \(A_{P_{1}}\) and \(A_{P_{2}}\), defined later in this section.

We now move on to discussing our specific implementation of the general layer update from Definition A.5.

Given a graph \(G=(V,E)\) and a coarsening function \(\mathcal{T}\), we aim to implement this general layer update by combining several standard message passing updates on the product graph \(G\Box\mathcal{T}(G)\). In the next two definitions, we define the adjacency matrices supported on the node set \(V\times V^{\mathcal{T}}\), which serve as the foundation for these message passing procedures, and formalize the procedures themselves.

**Definition B.1** (Adjacency Matrices on Product Graph).: _Let \(G=(V,E)\) be a graph with adjacency matrix \(A\) and node feature vector \(X\), and let \(\mathcal{T}(\cdot)\) be a coarsening function. We define the following four adjacency matrices on the vertex set \(V^{\mathcal{T}}\times V\):_

\[A_{G}(S,v,S^{\prime},v^{\prime}) =\begin{cases}1&v\sim_{G}v^{\prime},\;S=S^{\prime}\\ 0&\text{otherwise}.\end{cases}\] (22) \[A_{\mathcal{T}(G)}(S,v,S^{\prime},v^{\prime}) =\begin{cases}1&S\sim_{\mathcal{T}(G)}S^{\prime},\;v=v^{\prime} \\ 0&\text{otherwise}.\end{cases}\] (23) \[A_{P_{1}}(S,v,S^{\prime},v^{\prime}) =\begin{cases}1&v\in S^{\prime},\;v=v^{\prime}\\ 0&\text{otherwise}.\end{cases}\] (24) \[A_{P_{2}}(S,v,S^{\prime},v^{\prime}) =\begin{cases}1&v^{\prime}\in S,\;S^{\prime}=S\\ 0&\text{otherwise}.\end{cases}\] (25)

_Given edge features \(\{e_{v,v^{\prime}}\mid v\sim_{G}v^{\prime}\}\) and \(\{\tilde{e}_{S,S^{\prime}}\mid s\sim_{\mathcal{T}(G)}s^{\prime}\}\) corresponding to the graphs \(G\) and \(\mathcal{T}(G)\), respectively, we can trivially define the edge features corresponding to \(A_{G}\) and \(A_{G^{\mathcal{T}}}\) as follows:_

\[e_{G}(S,v,S^{\prime},v^{\prime}) =e_{v,v^{\prime}},\] (26) \[e_{\mathcal{T}(G)}(S,v,S^{\prime},v^{\prime}) =\tilde{e}_{S,S^{\prime}}.\] (27)

_In addition, for \(i=1,2\), we define the edge features corresponding to adjacency matrices \(A_{P_{i}}\) as follows:_

\[e_{P_{i}}(S,v,S^{\prime},v^{\prime})=z(S,v,S^{\prime},v^{\prime}).\] (28)

_Here, the function \(z:\mathcal{P}([n])\times[n]\times\mathcal{P}([n])\times[n]\rightarrow\mathbb{ R}^{d}\) maps each tuple \(\mathbf{v}=(S,v,S^{\prime},v^{\prime})\) to a vector uniquely encoding the orbit of \(\mathbf{v}\) under the action of \(S_{n}\) as described in Equation 73._

**Definition B.2** (CS-GNN Update Implementation).: _Given a graph \(G=(V,E)\), and a coarsening function \(\mathcal{T}(\cdot)\), let \(A_{1}\dots A_{4}\) enumerate the set of adjacency matrices \(\{A_{G},A_{\mathcal{T}(G)},A_{P_{1}},A_{P_{2}}\}\). We define a CS-GNN layer update in the following way:_

\[\mathcal{X}_{i}^{t}(S,v)=U_{i}^{t}\left((1+\epsilon_{i}^{t})\cdot\mathcal{X} ^{t}(S,v)+\sum_{(S^{\prime},v^{\prime})\sim_{A_{i}}(S,v)}M^{t}(\mathcal{X}^{ t}(S^{\prime},v^{\prime})+e_{i}(S,v,S^{\prime},v^{\prime}))\right).\] (29)

\[\mathcal{X}^{t+1}(S,v)=U_{\text{fin}}^{t}\left(\sum_{i=1}^{4}\mathcal{X}_{i}^{ t}(S,v)\right).\] (30)

_Here \(\mathcal{X}^{t}(S,v)\) and \(\mathcal{X}^{t+1}(S,v)\) denote the node feature maps of the product graph at layers \(t\) and \(t+1\), respectively. \(e^{1}(S,v,S^{\prime},v^{\prime}),\dots,e^{4}(S,v,S^{\prime},v^{\prime})\) denote the edge features associated with adjacency matrices \(A_{1},\dots,A_{4}\). \(\epsilon_{1}^{t},\dots,\epsilon_{4}^{t}\) represent learnable parameters in \(\mathbb{R}\), and \(U_{1}^{t},\dots,U_{4}^{t}\), \(U_{\text{fin}}^{t}\), \(M^{t}\) all refer to multilayer perceptrons._The next proposition states that using the layer update defined in equations 29 and 30 is enough to efficiently recover the general layer update defined in equation 15.

**Proposition B.1** (Equivalence of General Layer and Implemented Layer).: _Let \(\mathcal{T}(\cdot)\) be a coarsening function, \(\pi\) be a generalized node marking policy, and \(\mathcal{G}\) be a finite family of graphs. Applying a stack of \(t\) general layer updates as defined in Equation 15 to the node feature map \(\mathcal{X}(S,v)\) induced by \(\pi(G,\mathcal{T})\), can be effectively implemented by applying a stack of \(t\) layer updates specified in Equations 29 and 30 to \(\mathcal{X}(S,v)\). Additionally, the depths of all MLPs that appear in 29 and 30 can be bounded by 4._

## Appendix C Node Marking Policies - Theoretical Analysis

In this section, we define and analyze various general node marking policies, starting with four natural choices.

**Definition C.1** (Four General Node Marking policies).: _Let \(G=(V,E)\) be a graph with adjacency matrix \(A\in\mathbb{R}^{n\times n}\) and node feature vector \(X\in\mathbb{R}^{n\times d}\), and let \(\mathcal{T}(\cdot)\) be a coarsening function. All of the following node marking policies take the form:_

\[\pi(G,\mathcal{T})=\mathcal{X}(S,v)=[X_{u},b_{\pi}(S,v)],\] (31)

_where \([\cdot,\cdot]\) denotes the concatenation operator. We focus on four choices for \(b_{\pi}(S,v)\):_

1. _Simple Node Marking:_ \[b_{\pi}(S,v)=\begin{cases}1&\text{if }v\in S,\\ 0&\text{if }v\notin S.\end{cases}\] (32) _We denote this node marking policy by_ \(\pi_{S}\)_._
2. _Node + Size Marking:_ \[b_{\pi}(S,v)=\begin{cases}(1,|S|)&\text{if }v\in S,\\ (0,|S|)&\text{if }v\notin S.\end{cases}\] (33) _We denote this node marking policy by_ \(\pi_{\text{SS}}\)_._
3. _Minimum Distance:_ \[b_{\pi}(S,v)=\min_{v^{\prime}\in S}d_{G}(v,v^{\prime})\] (34) _where_ \(d_{G}(v,v^{\prime})\) _is the shortest path distance between nodes_ \(v\) _and_ \(v^{\prime}\) _in the original graph. We denote this node marking policy by_ \(\pi_{\text{MD}}\)_._
4. _Learned Distance Function:_ \[b_{\pi}(S,v)=\phi(\{d_{G}(v,v^{\prime})\mid v^{\prime}\in S\})\] (35) _where_ \(\phi(\cdot)\) _is a learned permutation-invariant function. We denote this node marking policy by_ \(\pi_{\text{LD}}\)_._

We note that when using the identity coarsening function \(\mathcal{T}(G)=G\), our general node marking policies output node feature maps supported on the product \(V\times V\). Thus, they can be compared to node marking policies used in node-based subgraph GNNs. In fact, in this case, both \(\pi_{\text{S}}\) and \(\pi_{\text{SS}}\) reduce to classical node-based node marking, while \(\pi_{\text{MD}}\) and \(\pi_{\text{LD}}\) reduce to distance encoding. The definitions of these can be found in [38]. Interestingly, even though in the case of node-based subgraph GNNSs, both distance encoding and node marking were proven to be maximally expressive [38], in our case for some choices of \(\mathcal{T}\), \(\pi_{\text{LD}}\) is strictly more expressive than the other three choices. The exact effect of each generalized node marking policy on the expressivity of our model is explored in the following two propositions.

**Proposition C.1** (Equal Expressivity of Node Marking Policies).: _For any coarsening function \(\mathcal{T}(\cdot)\) the following holds:_

\[\text{CS-GNN}(\mathcal{T},\pi_{S})=\text{CS-GNN}(\mathcal{T},\pi_{\text{SS}} )=\text{CS-GNN}(\mathcal{T},\pi_{\text{MD}}).\] (36)

**Proposition C.2** (Expressivity of Learned Distance Policy).: _For any coarsening function \(\mathcal{T}(\cdot)\) the following holds:_

\[\text{CS-GNN}(\mathcal{T},\pi_{\text{S}})\subseteq\text{CS-GNN}(\mathcal{T}, \pi_{\text{LD}}).\] (37)

_In addition, for some choices of \(\mathcal{T}(\cdot)\) the containment is strict._

The proofs of both propositions can be found in Appendix G. Finally, we provide a principled approach to deriving a generalized node marking policy based on symmetry invariance, and prove its equivalence to \(\pi_{\text{SS}}\). Given a graph \(G=(V,E)\) with \(V=[n]\), adjacency matrix \(A\), and node feature vector \(X\in\mathbb{R}^{n\times d}\), along with a coarsening function \(\mathcal{T}(\cdot)\), We define an action of the symmetric group \(S_{n}\) on the space \(\mathbb{R}^{\mathcal{P}([n])\times[n]}\) as follows:

\[\sigma\cdot\mathcal{X}(S,v)=\mathcal{X}(\sigma^{-1}(S),\sigma^{-1}(v))\quad \text{for }\sigma\in S_{n},\mathcal{X}\in\mathbb{R}^{\mathcal{P}([n])\times[n]}.\] (38)

Now, for each orbit \(\gamma\in(\mathcal{P}([n])\times[n])/S_{n}\), we define \(\mathbf{1}_{\gamma}\in\mathbb{R}^{\mathcal{P}([n])\times[n]}\) as follows:

\[\mathbf{1}_{\gamma}(S,v)=\begin{cases}1&(S,v)\in\gamma,\\ 0&\text{otherwise}.\end{cases}\] (39)

Choosing some enumeration of the orbit set \((\mathcal{P}([n])\times[n])/S_{n}=\{\gamma_{1},\ldots,\gamma_{k}\}\), We now define the invariant generalized node marking policy \(\pi_{\text{inv}}\) by first setting:

\[b_{\pi_{\text{inv}}}^{\text{sparse}}(S,v):\mathcal{P}([n])\times[n]\to\mathbb{ R}^{k}\]

and

\[b_{\pi_{\text{inv}}}:V^{\mathcal{T}}\times V\to\mathbb{R}^{k}\]

as follows:

\[b_{\pi_{\text{inv}}}^{\text{sparse}}(S,v) =[\mathbf{1}_{\gamma_{1}}(S,v),\ldots,\mathbf{1}_{\gamma_{k}}(S, v)] S S\in\mathcal{P}(V),\;v\in V,\] (40) \[b_{\pi_{\text{inv}}}(S,v) =b_{\pi_{\text{inv}}}^{\text{sparse}}(S,v) S\in V^{\mathcal{T}},\;v\in V.\] (41)

Then, we define the node feature map induced by \(\pi_{\text{inv}}\) as:

\[\mathcal{X}^{\pi_{\text{inv}}}(S,v)=[X_{v},b_{\pi_{\text{inv}}}(S,v)].\] (42)

Interestingly, \(\pi_{\text{inv}}\), derived solely from the group action of \(S_{n}\) on \(\mathcal{P}([n])\times[n]\), is equivalent to the generalized node marking policy \(\pi_{\text{SS}}\). This is stated more rigorously in the following proposition:

**Proposition C.3** (Node + Size Marking as Invariant Marking).: _Given a graph \(G=(V,E)\) with node feature vector \(X\in\mathbb{R}^{n\times d}\), and a coarsening function \(\mathcal{T}(\cdot)\), let \(\mathcal{X}^{\pi_{\text{SS}}},\mathcal{X}^{\pi_{\text{inv}}}\) be the node feature maps induced by \(\pi_{\text{SS}}\) and \(\pi_{\text{inv}}\) respectively. Recall that:_

\[\mathcal{X}^{\pi_{\text{SS}}}(S,v) =[X_{v},b_{\pi_{\text{SS}}}(S,v)],\] (43) \[\mathcal{X}^{\pi_{\text{inv}}}(S,v) =[X_{v},b_{\pi_{\text{inv}}}(S,v)].\] (44)

_The following now holds:_

\[b_{\pi_{\text{inv}}}(S,v)=\text{OHE}(b_{\pi_{\text{SS}}}(S,v))\quad\forall S \in V^{\mathcal{T}},\;\forall v\in V.\] (45)

_Here, OHE denotes a one-hot encoder, independent of the choice of both \(G\) and \(\mathcal{T}\)._

The proof of proposition C.3 can be found in Appendix G.

## Appendix D Expressive Power of CS-GNN

### Recovering Subgraph GNNs

In this section, we demonstrate that by choosing suitable coarsening functions, our architecture can replicate various previous subgraph GNN designs. We begin by focusing on node-based models, which are the most widely used type. We define a variant of these models which was proven in [38] to be maximally expressive, and show that our approach can recover it.

**Definition D.1** (Maximally Expressive Subgraph GNN).: _We define MSGNN\((\pi_{\text{NM}})\) as the set of all functions expressible by the following procedure:_1. _Node Marking:_ _The representation of tuple_ \((u,v)\in V\times V\) _is initially given by:_ \[\mathcal{X}^{0}(u,v)=\begin{cases}1&\text{if }u=v,\\ 0&\text{if }u\neq v.\end{cases}\] (46)
2. _Update:_ _The representation of tuple_ \((u,v)\) _is updated according to:_ \[\begin{split}\mathcal{X}^{t+1}(u,v)&=f^{t}\bigg{(} \mathcal{X}^{t}(u,v),\mathcal{X}^{t}(u,u),\mathcal{X}^{t}(v,v),\\ &\text{agg}_{1}^{t}\llbracket(\mathcal{X}^{t}(u,v^{\prime}),e_{v,v^{ \prime}})\mid v^{\prime}\sim v\rrbracket,\\ &\text{agg}_{2}^{t}\llbracket(\mathcal{X}^{t}(v,u^{\prime}),e_{u,u^{ \prime}})\mid u^{\prime}\sim u\rrbracket\bigg{)}.\end{split}\] (47)
3. _Pooling:_ _The final node feature vector_ \(\mathcal{X}^{T}(u,v)\) _is pooled according to:_ \[\text{MLP}_{2}\left(\sum_{u\in V}\text{MLP}_{1}\left(\sum_{v\in V}\mathcal{X} ^{T}(u,v)\right)\right).\] (48)

_Here, for any \(t\in[T],\ f^{t}\) is any continuous (parameterized) functions, \(\text{agg}_{1}^{t},,\text{agg}_{2}^{t}\) are any continuous (parameterized) permutation-invariant functions and \(\text{MLP}_{1},\text{MLP}_{2}\) are multilayer preceptrons._

**Proposition D.1** (CS-GNN Can Implement MSGNN).: _Let \(\mathcal{T}(\cdot)\) be the identity coarsening function defined by:_

\[\mathcal{T}(G)=\{\{v\}\mid v\in V\}\quad\forall G=(V,E).\] (49)

_The following holds:_

\[\text{CS-GNN}(\mathcal{T},\pi_{S})=\text{MSGNN}(\pi_{\text{NM}}).\] (50)

The proof of proposition D.1 can be found in Appendix G. We observe that, similarly, by selecting the coarsening function:

\[\mathcal{T}(G)=E\quad\forall G=(V,E),\] (51)

one can recover edge-based subgraph GNNs. An example of such a model is presented in [4] (DS-GNN), where it was proven capable of distinguishing between two 3-WL indistinguishable graphs, despite having an asymptotic runtime of \(\tilde{\mathcal{O}}(m^{2})\), where \(m\) is the number of edges in the input graph. This demonstrates our model's ability to achieve expressivity improvements while maintaining a (relatively) low asymptotic runtime by exploiting the graph's sparsity through the coarsening function. Finally, we note that by selecting the coarsening function:

\[\mathcal{T}(G)=\{S\in\mathcal{P}(V)\mid|S|=k\}\quad G=(V,E),\] (52)

We can recover an unordered variant of the \(k\)-OSAN model presented in [29].

### Comparison to Natural Baselines

In this section, we demonstrate how our model can leverage the information provided by the coarsening function \(\mathcal{T}(\cdot)\) in an effective way. First, we define a baseline model that incorporates \(\mathcal{T}\) in a straightforward manner. We then prove that, for any \(\mathcal{T}(\cdot)\), our model is at least as expressive as this baseline. Additionally, we show that for certain choices of \(\mathcal{T}(\cdot)\), our model exhibits strictly greater expressivity. To construct the baseline model, we first provide the following definition:

**Definition D.2** (Coarsened Sum Graph).: _Given a graph \(G=(V,E)\) and a coarsening function \(\mathcal{T}(\cdot)\), we define the coarsened sum graph \(G_{+}^{\mathcal{T}}=(V_{+}^{\mathcal{T}},E_{+}^{\mathcal{T}})\) by:_

* \(V_{+}^{\mathcal{T}}=V\cup V^{\mathcal{T}}\)_._
* \(E_{+}^{\mathcal{T}}=E\cup E^{\mathcal{T}}\cup\{\{S,v\}\mid S\in V^{\mathcal{ T}},\ v\in V\ v\in S\}\)_._

_If graph \(G\) had a node feature vector \(X\in\mathbb{R}^{n\times d}\), we define the node feature vector of \(G_{+}^{T}\) as:_

\[X_{v}=\begin{cases}[X_{v},1]&v\in V\\ 0_{d+1}&v\in V^{\mathcal{T}}\end{cases}.\] (53)

_Here we concatenated a \(1\) to the end of node features of \(V\) to distinguish them from the nodes of \(V^{\mathcal{T}}\)._

[MISSING_PAGE_FAIL:21]

The proofs to the last two propositions can be found in Appendix G. Proposition D.3 demonstrates that CS-GNNis strictly more expressive than MPNN\({}_{+}\) when using the identity coarsening function. However, this result extends to more complex coarsening functions as well. We briefly discuss one such example. Let \(\mathcal{T}(\cdot)\) be the coarsening function defined by:

\[\mathcal{T}_{\triangle}(G)=\{v_{1},v_{2},v_{3}\mid G[v_{1},v_{2},v_{3}]\cong \triangle\},\] (62)

i.e. for an input graph \(G\), the set of super-nodes is composed of all triplets of nodes whose induced subgraph is isomorphic to a triangle. To see that CS-GNN is strictly more expressive then MPNN\({}_{+}\) when using \(\mathcal{T}_{\triangle}(\cdot)\), we look at the two graphs \(G\) and \(H\) depicted in Figure 4. In the figure, we see the two original graphs, \(G\) and \(H\), their corresponding sum graphs \(G_{+}^{\mathcal{T}_{\triangle}}\) and \(H_{+}^{\mathcal{T}_{\triangle}}\), and a subgraph of their corresponging product graphs \(G\square\mathcal{T}_{\triangle}(G)\) and \(H\square\mathcal{T}_{\triangle}(H)\) induced by the sets \(\{(S_{0},v)\mid v\in V_{G}\}\) and \(\{(S_{0},v)\mid v\in V_{H}\}\) respectively (this can be thought of as looking at a single subgraph from the bag of subgraphs induced by CS-GNN). One can clearly see that both the original graphs and their respective sum graphs are 1-WL indistinguishable. On the other hand, the subgraphs induced by our method are 1-WL distinguishable. Since for both \(G\) and \(H\) the "bag of subgraphs" induced by CS-GNN is composed of \(6\) isomorphic copies of the same graph, this would imply that our method can distinguish between \(G\) and \(H\), making it strictly mor expressive then MPNN\({}_{+}\).

Figure 4: Rows 1 and 3 depict two 1-WL indistinguishable graphs\(>\) Rows 2 and 4 depict the sum graph of each of these graphs, as well as one subgraph of their product graphs induced by all node, super-node tuples whose super-node is fixed.

We conclude this section with the following proposition, showing there exists coarsening functions which, when combined with CS-GNN, results in an architecture that is strictly more expressive then node-based subgraph GNNs.

**Proposition D.4** (CS-GNN can be strictly more expressive then node-based subgraph GNNs).: _Let \(\mathcal{T}\) be the coarsening function defined by:_

\[\mathcal{T}(G)=\{\{v\}\mid v\in V\}\cup E\quad G=(V,E).\] (63)

_The following holds:_

1. _Let_ \(G_{1},G_{2}\) _be a pair of graphs such that there exists a node-based subgraph GNN model M where_ \(M(G_{1})\neq M(G_{2})\)_. There exists a CS-GNNmodel_ \(M^{\prime}\) _which uses_ \(\mathcal{T}\) _such that_ \(M^{\prime}(G_{1})\neq M^{\prime}(G_{2})\)_._
2. _There exists a pair of graphs_ \(G_{1},G_{2}\) _such that for any subgraph GNN model_ \(M\) _it holds that_ \(M(G_{1})=M(G_{2})\)_, but there exists a CS-GNNmodel_ \(M^{\prime}\) _which uses_ \(\mathcal{T}\) _such that_ \(M^{\prime}(G_{1})\neq M^{\prime}(G_{2})\)_._

This proposition is proved in Appendix G.

## Appendix E Linear Invariant (Equivariant) Layer - Extended Section

We introduce some key notation. In the matrix \(\mathcal{X}\), the \(i\)-th row corresponds to the \(i\)-th subset \(S\) arranged in the lexicographic order of all subsets of \([n]\), namely, \(\{\{0\},\{0,1\},\{0,2\},\ldots,\{0,1,2,\ldots,n\}\}\). Each \(i\)-th position in this sequence aligns with the \(i\)-th row index in \(\mathcal{X}\). It follows, that the standard basis for such matrices in \(\mathbb{R}^{2^{n}\times n}\) is expressed as \(\mathbf{e}^{(S)}\cdot\mathbf{e}^{(i)^{T}}\), where \(\mathbf{e}^{(S)}\) is a 1-hot vector, with the value 1 positioned according to \(S\) in the lexicographic order. For a matrix \(X\in\mathbb{R}^{a\times b}\), the operation of vectorization, denoted by \(\operatorname{vec}(X)\), transforms \(X\) into a single column vector in \(\mathbb{R}^{ab\times 1}\) by sequentially stacking its columns; in the context of \(\mathcal{X}\), the basis vectors of those vectors are \(\mathbf{e}^{(i)}\otimes\mathbf{e}^{(S)}\). The inverse process, reshaping a vectorized matrix back to its original format, is denoted as \([\operatorname{vec}(X)]=X\). We also denote an arbitrary permutation by \(\sigma\in S_{n}\). The actions of permutations on vectors, whether indexed by sets or individual indices, are represented by \(\mathbf{P}_{\mathcal{S}}\in\text{GL}(2^{n})\) and \(\mathbf{P}_{\mathcal{I}}\in\text{GL}(n)\), respectively. This framework acknowledges \(S_{n}\) as a subgroup of the larger permutation group \(S_{2^{n}}\), which permutes all \(2^{n}\) positions in a given vector \(\mathbf{v}_{S}\in\mathbb{R}^{2^{n}}\).

Let \(\mathbf{L}\in\mathbb{R}^{1\times 2^{n}\cdot n}\) be the matrix representation of a general linear operator \(\mathcal{L}:\mathbb{R}^{2^{n}\times n}\to\mathbb{R}\) in the standard basis. The operator \(\mathcal{L}\) is order-invariant iff

\[\mathbf{L}\operatorname{vec}(\mathbf{P}_{\mathcal{S}}^{T}\mathcal{X}\mathbf{P} _{\mathcal{I}})=\mathbf{L}\operatorname{vec}(\mathcal{X}).\] (64)

Similarly, let \(\mathbf{L}\in\mathbb{R}^{2^{n}\cdot n\times 2^{n}\cdot n}\) denote the matrix for \(\mathcal{L}:\mathbb{R}^{2^{n}\times n}\to\mathbb{R}^{2^{n}\times n}\). The operator \(\mathcal{L}\) is order-equivariant if and only if

\[[\mathbf{L}\operatorname{vec}(\mathbf{P}_{\mathcal{S}}^{T}\mathcal{X}\mathbf{ P}_{\mathcal{I}})]=\mathbf{P}_{\mathcal{S}}^{T}[\mathbf{L}\operatorname{ vec}(\mathcal{X})]\mathbf{P}_{\mathcal{I}}.\] (65)

Using properties of the Kronecker product (see Appendices E.1 and E.2 for details), we derive the following conditions for invariant and equivariant linear layers:

\[\text{Invariant}\ \mathbf{L}:\quad\mathbf{P}_{\mathcal{I}}\otimes \mathbf{P}_{\mathcal{S}}\operatorname{vec}(\mathbf{L})=\operatorname{vec}( \mathbf{L}),\] (66) \[\text{Equivariant}\ \mathbf{L}:\quad\mathbf{P}_{\mathcal{I}}\otimes \mathbf{P}_{\mathcal{S}}\otimes\mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_{ \mathcal{S}}\operatorname{vec}(\mathbf{L})=\operatorname{vec}(\mathbf{L}).\] (67)

**Solving Equations (66) and (67)**. Let \(\sigma\in S_{n}\) denote a permutation corresponding to the permutation matrix \(\mathbf{P}\). Let \(\mathbf{P}\star\mathbf{L}\) denote the tensor that results from expressing \(\mathbf{L}\) after renumbering the nodes in \(V^{\mathcal{T}},V\) according to the permutation \(\sigma\). Explicitly, for \(\mathbf{L}\in\mathbb{R}^{2^{n}\times n}\), the \((\sigma(S),\sigma(i))\)-entry of \(\mathbf{P}\star\mathbf{L}\) equals to the \((S,i)\)-entry of \(\mathbf{L}\). The matrix that corresponds to the operator \(\mathbf{P}\star\mathbf{\mu}\) in the standard basis, \(\mathbf{e}^{(i)}\otimes\mathbf{e}^{(S)}\) is the kronecker product \(\mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}}\). Since \(\operatorname{vec}(\mathbf{L})\) is exactly the coordinate vector of the tensor \(\mathbf{L}\) in the standard basis we have,

\[\operatorname{vec}(\mathbf{P}\star\mathbf{L})=\mathbf{P}_{\mathcal{I}} \otimes\mathbf{P}_{\mathcal{S}}\operatorname{vec}(\mathbf{L}),\] (68)

following the same logic, the following holds for the equivariant case, where \(\mathbf{L}\in\mathbb{R}^{2^{n}\cdot n\times 2^{n}\cdot n}\),

\[\operatorname{vec}(\mathbf{P}\star\mathbf{L})=\mathbf{P}_{\mathcal{I}} \otimes\mathbf{P}_{\mathcal{S}}\otimes\mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_ {\mathcal{S}}\operatorname{vec}(\mathbf{L}).\] (69)Given Equations (66) and (68) and Equations (67) and (69), it holds that we should focus on solving,

\[\mathbf{P}\star\mathbf{L}=\mathbf{L},\quad\forall\mathbf{P}\text{ permutation matrices},\] (70)

for both cases where \(\mathbf{L}\in\mathbb{R}^{2^{n}\times n}\) and \(\mathbf{L}\in\mathbb{R}^{2^{n}\times n\times 2^{n}\times n}\), corresponding to the bias term, and linear term.

**Bias.** To this end, let us define an equivalence relation in the index space of a tensor in \(\mathbb{R}^{2^{n}\times n}\). Given a pair \((S,i)\in\mathcal{P}([n])\times[n]\), we define \(\gamma^{k^{+}}\) to correspond to all pairs \((S,i)\) such that \(|S|=k\) and \(i\notin S\). Similarly, \(\gamma^{k^{-}}\) corresponds to all pairs \((S,i)\) such that \(|S|=k\) and \(i\in S\). We denote this equivalence relation as follows:

\[(\mathcal{P}([n])\times[n])/_{\sim}\triangleq\{\gamma^{k^{*}}:k=1,\ldots,n;* \in\{+,-\}\}.\] (71)

For each set-equivalence class \(\gamma\in(\mathcal{P}([n])\times[n])_{\sim}\), we define a basis tensor, \(\mathbf{B}^{\gamma}\in\mathbb{R}^{2^{n}\times n}\) by setting:

\[\mathbf{B}^{\gamma}_{S,i}=\begin{cases}1,&\text{if }(S,i)\in\gamma;\\ 0,&\text{otherwise}.\end{cases}\] (72)

Following similar reasoning, consider elements \((S_{1},i_{1},S_{2},i_{2})\in(\mathcal{P}([n])\times[n]\times\mathcal{P}([n]) \times[n])\). We define a partition according to six conditions: the relationship between \(i_{1}\) and \(i_{2}\), denoted as \(i_{1}\leftrightarrow i_{2}\), which is determines by the condition: \(i_{1}=i_{2}\) or \(i_{1}\neq i_{2}\); the cardinalities of \(S_{1}\) and \(S_{2}\), denoted as \(k_{1}\) and \(k_{2}\), respectively; the size of the intersection \(S_{1}\cap S_{2}\), denoted as \(k^{\sim}\); the membership of \(i_{l}\) in \(S_{l}\) for \(l\in\{1,2\}\), denoted as \(\delta_{\text{same}}\in\{1,2,3,4\}\); and the membership of \(i_{l_{1}}\) in \(S_{l_{2}}\) for distinct \(l_{1},l_{2}\in\{1,2\}\), denoted as \(\delta_{\text{diff}}\in\{1,2,3,4\}\). The equivalence relation thus defined can be represented as:

\[(\mathcal{P}([n])\times[n]\times\mathcal{P}([n])\times[n])/_{\sim}\triangleq\{ \Gamma^{\leftrightarrow;k_{1};k_{2};k^{\cap};\delta_{\text{same}};\delta_{ \text{after}}}\}.\] (73)

For each set-equivalence class \(\Gamma\in(\mathcal{P}([n])\times[n]\times\mathcal{P}([n])\times[n])/_{\sim}\), we define a basis tensor, \(\mathbf{B}^{\Gamma}\in\mathbb{R}^{2^{n}\times n\times 2^{n}\times n}\) by setting:

\[\mathbf{B}^{\Gamma}_{S_{1},i_{1};S_{2},i_{2}}=\begin{cases}1,&\text{if }(S_{1},i_{1},S_{2},i_{2})\in\Gamma;\\ 0,&\text{otherwise}.\end{cases}\] (74)

The following two proposition summarizes the results in this section,

**Lemma E.1** (\(\gamma\) (\(\Gamma\)) are orbits).: _The sets \(\{\gamma^{k^{*}}:k=1,\ldots,n;*\in\{+,-\}\}\) and \(\{\Gamma^{\leftrightarrow;k_{1};k_{2};k^{\cap};\delta_{\text{same}};\delta_{ \text{after}}}\}\) are the orbits of \(S_{n}\) on the index space \((\mathcal{P}([n])\times[n])\) and \((\mathcal{P}([n])\times[n]\times(\mathcal{P}([n])\times[n])\), respectively._

**Proposition E.1** (Basis of Invariant (Equivariant) Layer).: _The tensors \(\mathbf{B}^{\gamma}\) (\(\mathbf{B}^{\Gamma}\)) in Equation (72) (Equation (74)) form an orthogonal basis (in the standard inner product) to the solution of Equation (66) (Equation (67))._

The proofs are given in Appendix G.

### Full Derivation of Equation (66).

Our goal is to transition from the equation,

\[\mathbf{L}\operatorname{vec}(\mathbf{P}^{T}_{\mathcal{S}}\mathcal{X}\mathbf{P }_{\mathcal{I}})=\mathbf{L}\operatorname{vec}(\mathcal{X})\] (64)

to the form,

\[\mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}}\operatorname{vec}( \mathbf{L})=\operatorname{vec}(\mathbf{L})\] (66)

We introduce the following property of the Kronecker product,

\[\operatorname{vec}(\mathbf{A}\mathbf{B}\mathbf{C})=(\mathbf{C}^{T}\otimes \mathbf{A})\text{vec}(\mathbf{B}).\] (75)

Using Equation (75) on the left side of Equation (64), we obtain

\[\mathbf{L}\mathbf{P}^{T}_{\mathcal{I}}\otimes\mathbf{P}^{T}_{\mathcal{S}} \operatorname{vec}(\mathcal{X})=\mathbf{L}\operatorname{vec}(\mathcal{X}),\] (76)

since this should be true for any \(\mathcal{X}\in\mathbb{R}^{2^{n}\times n}\), we derive

\[\mathbf{L}\mathbf{P}^{T}_{\mathcal{I}}\otimes\mathbf{P}^{T}_{\mathcal{S}}= \mathbf{L}.\] (77)

Applying the transpose operation on both sides, and noting that \((\mathbf{P}^{T}_{\mathcal{I}}\otimes\mathbf{P}^{T}_{\mathcal{S}})^{T}=\mathbf{ P}_{\mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}}\), we obtain

\[\mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}}\mathbf{L}^{T}= \mathbf{L}^{T}.\] (78)

Recalling that \(\mathbf{L}\in\mathbb{R}^{1\times 2^{n}\cdot n}\), and thus \(\mathbf{L}^{T}\in\mathbb{R}^{2^{n}\cdot n\times 1}\), we find that \(\mathbf{L}^{T}=\operatorname{vec}(\mathbf{L})\). Substituting this back into the previous equation we achieve Equation (66).

### Full Derivation of Equation (67).

Our goal is to transition from the equation,

\[[\mathbf{L}\operatorname{vec}(\mathbf{P}_{\mathcal{S}}^{T}\mathcal{X}\mathbf{P}_{ \mathcal{I}})]=\mathbf{P}_{\mathcal{S}}^{T}[\mathbf{L}\operatorname{vec}( \mathcal{X})]\mathbf{P}_{\mathcal{I}}\] (65)

to the form,

\[\mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}}\otimes\mathbf{P}_{ \mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}}\operatorname{vec}(\mathbf{L})= \operatorname{vec}(\mathbf{L}).\] (67)

Applying the property in Equation (75), after the reverse operation of the vectorization, namely,

\[[\operatorname{vec}(\mathbf{A}\mathbf{B}\mathbf{C})]=[(\mathbf{C}^{T}\otimes \mathbf{A})\operatorname{vec}(\mathbf{B})]\] (79)

on the right hand side of Equation (65), for

\[\mathbf{A} \triangleq\mathbf{P}_{\mathcal{S}}^{T};\] (80) \[\mathbf{B} \triangleq[\mathbf{L}\operatorname{vec}(\mathcal{X})];\] (81) \[\mathbf{C} \triangleq\mathbf{P}_{\mathcal{I}},\] (82)

we obtain,

\[[\mathbf{L}\operatorname{vec}(\mathbf{P}_{\mathcal{S}}^{T}\mathcal{X}\mathbf{ P}_{\mathcal{I}})]=[\mathbf{P}_{\mathcal{I}}^{T}\otimes\mathbf{P}_{\mathcal{S}}^{T }\mathbf{L}\operatorname{vec}(\mathcal{X})].\] (83)

Thus, by omitting the revere-vectorization operation,

\[\mathbf{L}\operatorname{vec}(\mathbf{P}_{\mathcal{S}}^{T}\mathcal{X}\mathbf{ P}_{\mathcal{I}})=\mathbf{P}_{\mathcal{I}}^{T}\otimes\mathbf{P}_{\mathcal{S}}^{T }\mathbf{L}\operatorname{vec}(\mathcal{X}).\] (84)

Noting that \((\mathbf{P}_{\mathcal{I}}^{T}\otimes\mathbf{P}_{\mathcal{S}}^{T})^{-1}= \mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}}\), and multiplying by this inverse both sides (from the left), we obtain,

\[\mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}}\mathbf{L} \operatorname{vec}(\mathbf{P}_{\mathcal{S}}^{T}\mathcal{X}\mathbf{P}_{ \mathcal{I}})=\mathbf{L}\operatorname{vec}(\mathcal{X}).\] (85)

Applying, again, the property in Equation (75), we obtain,

\[\mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}}\mathbf{L}\mathbf{P}_ {\mathcal{I}}^{T}\otimes\mathbf{P}_{\mathcal{S}}^{T}\operatorname{vec}( \mathcal{X})=\mathbf{L}\operatorname{vec}(\mathcal{X}).\] (86)

Since this should be true for any \(\mathcal{X}\in\mathbb{R}^{2^{n}\times n}\), we derive,

\[\mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}}\mathbf{L}\mathbf{P}_ {\mathcal{I}}^{T}\otimes\mathbf{P}_{\mathcal{S}}^{T}=\operatorname{vec}( \mathbf{L}).\] (87)

Again, applying Equation (75) on the left side, where,

\[\mathbf{A} \triangleq\mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}};\] (88) \[\mathbf{B} \triangleq\mathbf{L};\] (89) \[\mathbf{C} \triangleq\mathbf{P}_{\mathcal{I}}^{T}\otimes\mathbf{P}_{\mathcal{ S}}^{T},\] (90)

we get the following equality,

\[\mathbf{P}_{\mathcal{I}}\otimes\mathbf{P}_{\mathcal{S}}\mathbf{L}\mathbf{P}_{ \mathcal{I}}^{T}\otimes\mathbf{P}_{\mathcal{S}}^{T}=\mathbf{P}_{\mathcal{I}} \otimes\mathbf{P}_{\mathcal{S}}\otimes\mathbf{P}_{\mathcal{I}}\otimes \mathbf{P}_{\mathcal{S}}\operatorname{vec}(\mathbf{L}).\] (91)

By substituting this to the left side of Equation (87) we obtain Equation (67).

### Comparative Parameter Reduction in Linear Equivariant Layers

To demonstrate the effectiveness of our parameter-sharing scheme, which results from considering unordered tuples rather than ordered tuples, we present the following comparison. 3-IGNs [22] are structurally similar to our approach, with the main difference being that they consider indices as ordered tuples, while we consider them as sets. Both approaches use a total of six indices, as shown in the visualized block in Figure 3, making 3-IGNs a natural comparator. By leveraging our scheme, we reduce the number of parameters from 203 (the number of parameters in 3-IGNs) to just 35!

## Appendix F Extended Experimental Section

### Dataset Description

In this section we overview the eight different datasets considered; this is summarized in Table 5.

**ZINC-12k and ZINC-Full Datasets [31; 14; 10].** The ZINC-12k dataset includes 12,000 molecular graphs sourced from the ZINC database, a compilation of commercially available chemical compounds. These molecular graphs vary in size, ranging from 9 to 37 nodes, where each node represents a heavy atom, covering 28 different atom types. Edges represent chemical bonds and there are three types of bonds. The main goal when using this dataset is to perform regression analysis on the constrained solubility (logP) of the molecules. The dataset is divided into training, validation, and test sets with 10,000, 1,000, and 1,000 molecular graphs respectively. The full version, ZINC-Full, comprises approximately 250,000 molecular graphs, ranging from 9 to 37 nodes and 16 to 84 edges per graph. These graphs also represent heavy atoms, with 28 distinct atom types, and the edges indicate bonds between these atoms, with four types of bonds present.

**ogbg-molibiv, ogbg-molibc, ogbg-molibc Datasets [16].** These datasets are used for molecular property prediction and have been adopted by the Open Graph Benchmark (OGB, MIT License) from MoleculeNet. They use a standardized featurization for nodes (atoms) and edges (bonds), capturing various chemophysical properties.

**Peptides-func and Peptides-struct Datasets [9].** The Peptides-func and Peptides-struct datasets consist of atomic graphs representing peptides released with the Long Range Graph Benchmark (LRGB, MIT License). In Peptides-func, the task is to perform multi-label graph classification into ten non-exclusive peptide functional classes. Conversely, Peptides-struct is focused on graph regression to predict eleven three-dimensional structural properties of the peptides.

We note that for all datasets, we used the random splits provided by the public benchmarks.

### Experimental Details

**Implementation Details.** Our implementation of Equation (2) is given by:

\[\mathcal{X}^{(l+1)}=\text{MLP}\left(\sum_{i=1}^{3}\texttt{MPNN}^{(l+1,i)} \left(\mathcal{X},\mathcal{A}_{i}\right)\right),\] (92)

where \(\mathcal{A}_{1}=\mathcal{A}_{G}\), \(\mathcal{A}_{2}=\mathcal{A}_{\mathcal{T}(G)}\), and \(\mathcal{A}_{3}=\mathcal{A}_{\text{Equiv}}\).

For all considered datasets, namely, ZINC-12k, ZINC-Full, ogbg-molibiv, ogbg-molibc, and ogbg-molibc, except for the Peptides-func and Peptides-struc datasets, we use a GINE [15] base encoder. Given an adjacency matrix \(\mathcal{A}\), and defining \(e_{(S^{\prime},v^{\prime}),(S,v)}\) to denote the edge features from node \((S^{\prime},v^{\prime})\) to node \((S,v)\), it takes the following form:

\[\mathcal{X}(S,v)=\texttt{MLP}\bigg{(}(1+\epsilon)\cdot\mathcal{X}(S,v)+\sum_ {(S^{\prime},v^{\prime})\sim_{\mathcal{A}}(S,v)}\texttt{ReLU}\big{(}\mathcal{ X}(S^{\prime},v^{\prime})+e_{(S^{\prime},v^{\prime}),(S,v)}\big{)}\bigg{)}.\] (93)

We note that for the symmetry-based updates, we switch the ReLU to an MLP12 to align with the theoretical analyses13 (Appendix B), stating that we can implement the equivariant update developed

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \hline
**Dataset** & **\# Graphs** & **Avg. \# nodes** & **Avg. \# edges** & **Directed** & **Prediction task** & **Metric** \\ \hline ZINC-12k [31] & 12,000 & 23.2 & 24.9 & No & Regression & Mean Abs. Error \\ ZINC-Full [31] & 249,456 & 23.2 & 49.8 & No & Regression & Mean Abs. Error \\ ogbg-molibiv [16] & 41,127 & 25.5 & 27.5 & No & Binary Classification & AUROC \\ ogbg-molibc [16] & 1513 & 34.1 & 36.9 & No & Binary Classification & AUROC \\ ogbg-molibc [16] & 1,128 & 13.3 & 13.7 & No & Regression & Root Mean Squ. Error \\ \hline Peptides-func [9] & 15,535 & 150.9 & 307.3 & No & 10-task Classification & Avg. Precision \\ Peptides-struct [9] & 15,535 & 150.9 & 307.3 & No & 11-task Regression & Mean Abs. Error \\ \hline \hline \end{tabular}
\end{table}
Table 5: Overview of the graph learning datasets.

[MISSING_PAGE_FAIL:27]

**Optimizers and Schedulers.** For the ZINC-12k and Zinc-Full datasets, we employ the Adam optimizer paired with a ReduceLROnPlateau scheduler,factor set to 0.5, patience at 4014, and a minimum learning rate of 0. For the ogbg-molhiv dataset, we utilized the ASAM optimizer [21] without a scheduler. For both ogbg-molsel and ogbg-molbace, we employed a constant learning rate without any scheduler. Lastly, for the Peptides-func and Peptides-struct datasets, the AdamW optimizer was chosen in conjunction with a cosine annealing scheduler, incorporating 10 warmup epochs.

Footnote 14: For Zinc-12k, \(T\in\{2\), “full”\(\}\), we used a patience of 50.

### Implementation of Linear Equivariant and Invariant layers - Extended Section

In this section, in a more formal discussion, we specify how to integrate those invariant and equivariant layers to our proposed architecture. We start by drawing an analogy between parameter sharing in linear layers and the operation of an MPNN on a fully connected graph with edge features in the following lemma,

**Lemma F.1** (Parameter Sharing as MPNN).: _Let \(B_{1},\ldots B_{k}:\mathbb{R}^{n\times n}\) be orthogonal matrices with entries restricted to 0 or 1, and let \(W_{1},\ldots W_{k}\in\mathbb{R}^{d\times d^{l}}\) denote a sequence of weight matrices. Define \(B_{+}=\sum_{i=1}^{k}B_{i}\) and choose \(z_{1},\ldots z_{k}\in\mathbb{R}^{d^{*}}\) to be a set of unique vectors representing an encoding of the index set. The function, which represents an update via parameter sharing:_

\[f(X)=\sum_{i=1}^{k}B_{i}XW_{i},\] (95)

_can be implemented by a stack of MPNN layers of the following form [13],_

\[m_{u}^{l}=\sum_{v\in N_{B_{+}}(u)}M^{l}(X_{v}^{l},e_{u,v}),,\] (96)

\[X_{u}^{l+1}=U^{l}(X_{v}^{l},m_{v}^{l}),\] (97)

_where \(U^{l},M^{l}\) are multilayer perceptrons (MLPs). The inputs to this MPNN are the adjacency matrix \(B_{+}\), node feature vector \(X\), and edge features - the feature of edge \((u,v)\) is given by:_

\[e_{u,v}=\sum_{i=1}^{k}z_{i}\cdot B_{i}(u,v).\] (98)

_Here, \(B_{i}(u,v)\) denotes the \((u,v)\) entry to matrix \(B_{i}\)._

The proof is given in Appendix G.

\begin{table}
\begin{tabular}{l c|c|c|c|c|c|c|c|c} \hline Dataset & Bag size & Num. layers & Learning rate & Embedding size & Epochs & Batch size & Dropout & Laplacian dimension & SPD dimension \\ \hline Zinc-12k & \(T-2\) & 6 & 0.0055 & 96 & 400 & 128 & 0 & 1 & 10 \\ Zinc-12k & \(T-3\) & 6 & 0.0007 & 96 & 400 & 128 & 0 & 2 & 10 \\ Zinc-12k & \(T-4\) & 6 & 0.0007 & 96 & 400 & 128 & 0 & 1 & 10 \\ Zinc-12k & \(T-5\) & 6 & 0.0007 & 96 & 400 & 128 & 0 & 1 & 10 \\ Zinc-12k & \(T-8\) & 6 & 0.0007 & 96 & 400 & 128 & 0 & 1 & 10 \\ Zinc-12k & \(T-\text{18}\) & 6 & 0.0007 & 96 & 400 & 128 & 0 & 1 & 10 \\ Zinc-12k & \(T-\text{``full"}\) & 6 & 0.0007 & 96 & 500 & 128 & 0 & N/A & 10 \\ \hline Zinc-Full & \(T-\text{4}\) & 6 & 0.0007 & 96 & 400 & 128 & 0 & 1 & 10 \\ Zinc-Full & \(T-\text{``full"}\) & 6 & 0.0005 & 96 & 500 & 128 & 0 & N/A & N/A \\ \hline ogbg-molhiv & \(T-2\) & 2 & 0.01 & 60 & 100 & 32 & 0.5 & 1 & 1 \\ ogbg-molhiv & \(T-\text{5}\) & 2 & 0.01 & 60 & 100 & 32 & 0.5 & 1 & 1 \\ ogbg-molhiv & \(T-\text{``full"}\) & 2 & 0.01 & 60 & 100 & 32 & 0.5 & N/A & N/A \\ \hline ogbg-molsel & \(T-2\) & 3 & 0.001 & 60 & 100 & 32 & 0.3 & 1 & 2 \\ ogbg-molsel & \(T-\text{``full"}\) & 3 & 0.001 & 60 & 100 & 32 & 0.3 & 1 & 2 \\ ogbg-molsel & \(T-\text{``full"}\) & 3 & 0.001 & 60 & 100 & 32 & 0.3 & N/A & N/A \\ \hline ogbg-molhav & \(T-2\) & 3 & 0.01 & 60 & 100 & 32 & 0.3 & 1 & 1 \\ ogbg-molhav & \(T-\text{``5"}\) & 3 & 0.01 & 60 & 100 & 32 & 0.3 & 1 & 2 \\ ogbg-molhav & \(T-\text{``full"}\) & 3 & 0.01 & 60 & 100 & 32 & 0.3 & N/A & N/A \\ \hline Peptides-Func & \(T-\text{``full"}\) & 5 & 0.005 & 96 & 200 & 128 & 0 & 1 & 1 \\ Peptides-struc & \(T-\text{``full"}\) & 4 & 0.01 & 96 & 200 & 128 & 0 & 1 & 1 \\ \hline \end{tabular}
\end{table}
Table 7: Chosen Hyperparameters for CS-GNN.

Thus, our implementation for the global update is as follows,

\[\mathcal{X}(S,i)=\texttt{MLP}\left((1+\epsilon)\cdot\mathcal{X}(S,i)+\sum_{(S^{ \prime},i^{\prime})\sim\mathcal{A}_{Equ}(S,i)}\texttt{MLP}\bigg{(}\mathcal{X}(S^{ \prime},i^{\prime})+e_{(S^{\prime},i^{\prime}),(S,i)}\bigg{)}\right),\] (99)

where \(e_{(S^{\prime},i^{\prime}),(S,i)}=\sum_{\Gamma}z_{\Gamma}\cdot\mathbf{B}_{S,i ;S^{\prime},i^{\prime}}^{\Gamma}\) and \(z_{\Gamma}\) are orthogonal 1-hot vectors for different \(\Gamma\)'s. The connectivity \(\mathcal{A}_{Equiv}\) is such that \(\mathcal{A}_{Equiv}(S,v,S^{\prime},v^{\prime})\) contains the value one iff \(v\in S,v=v^{\prime}\). This corresponds to choosing only several \(\Gamma\)'s in the partition, and since each \(\Gamma\) is invariant to the permutation, this choice still maintains equivariance.

### Additional Results

**Zinc-full.** Below, we present our results on the Zinc-full dataset for a bag size of \(T=4\) and the full-bag. For the bag size \(T=4\), we benchmark against MAG-GNN [20], which in their experiments used the best out of the bag sizes \(T\in\{2,3,4\}\); however, they did not specify which one performed the best. The results are summarized in Table 8.

**Zinc-12k - additional results.** We present all the results from Figure 2, along with some additional ones, in Table 9.

**Runtime comparison.** We compare the training time and prediction performance on the Zinc-12k dataset. For all methods, we report the training and inference times on the entire training and test sets, respectively, using a batch size of 128. Our experiments were conducted using an NVIDIA L40 GPU, while for the baselines, we used the timing reported in [5], which utilized an RTX A6000 GPU. The runtime comparison is presented in Table 10.

### Zinc12k Product Graph Visualization

In this subsection, we visualize the product graph derived from the first graph in the Zinc12k dataset. Specifically, we present the right part of Figure 1, for the case of the real-world graphs in the Zinc12k dataset. We perform this visualization for different cluster sizes, \(T\in\{2,3,4,5,8,12\}\), which also define the bag size, hence the notation \(T\). The nodes in the product graph, \(\mathcal{T}(G)\Box G\), are \((S,v)\), where \(S\) is the coarsened graph node (again a tuple), and \(v\) is the node index (of a node from the original graph). For better clarity, we color the nodes \((S,v)\) with \(v\in S\) using different colors, while reserving the gray color exclusively for nodes \((S,v)\) where \(v\notin S\). The product graphs are visualized in Figures 5 to 10 below.

\begin{table}
\begin{tabular}{l|c} \hline \hline Model \(\downarrow\) / Dataset \(\rightarrow\) & 
\begin{tabular}{c} Zinc-Full \\ (MAE \(\downarrow\)) \\ \end{tabular} \\ \hline MAG-GNN [20] (\(T=4\)) & **0.030\(\pm\)**0.002 \\ Ours (\(T=4\)) & **0.027\(\pm\)**0.002 \\ \hline GNN-SSWL [38] (\(T=\) “full”) & 0.026\(\pm\)0.001 \\ GNN-SSWL+ [38] (\(T=\) “full”) & 0.022\(\pm\)0.001 \\ Subgraphormer [3](\(T=\) “full”) & **0.020\(\pm\)**0.002 \\ Subgraphormet + PE [3] (\(T=\) “full”) & 0.023\(\pm\)0.001 \\ Ours (\(T=\) “full”) & **0.021\(\pm\)**0.001 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison over the Zinc-Full molecular dataset under \(500k\) parameter budget. The best performing method is highlighted in **blue**, while the second best is highlighted in **red**.

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline Method & Train time (for a single epoch; ms) & Test time (ms) & MAE \(\downarrow\) \\ \hline GIN [35] & \(1370.10\pm 10.97\) & \(84.81\pm 0.26\) & \(0.163\pm 0.004\) \\ \hline OSAAN [29] (\(T=2\)) & \(2964.46\pm 30.36\) & \(227.93\pm 0.21\) & \(0.177\pm 0.016\) \\ PL [5] (\(T=2\)) & \(2489.25\pm 9.42\) & \(150.38\pm 0.33\) & \(0.120\pm 0.003\) \\ Ours (\(T=2\)) & \(2764.60\pm 234\) & \(383.14\pm 15.74\) & \(0.109\pm 0.005\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Run time comparison over the Zinc-12k dataset. Time taken at train for one epoch and at inference on the test set. All values are in milliseconds.

\begin{table}
\begin{tabular}{l c|c} \hline \hline
**Method** & Bag size & ZINC (MAE \(\downarrow\)) \\ \hline GCN [19] & \(T=1\) & \(0.321\pm 0.009\) \\ GIN [35] & \(T=1\) & \(0.163\pm 0.004\) \\ \hline OSAAN [29] & \(T=2\) & \(0.177\pm 0.016\) \\ Random [20] & \(T=2\) & \(0.131\pm 0.005\) \\ Pt. [5] & \(T=2\) & \(0.120\pm 0.003\) \\ Mag-GNN [20] & \(T=2\) & \(\mathbf{0.106\pm 0.014}\) \\ Ours & \(T=2\) & \(\mathbf{0.109\pm 0.005}\) \\ \hline Random [20] & \(T=3\) & \(0.124\pm\)N\(\lambda\) \\ Mag-GNN [20] & \(T=3\) & \(\mathbf{0.104\pm\)N\(\lambda\) \\ Ours & \(T=3\) & \(\mathbf{0.096\pm 0.005}\) \\ \hline Random [20] & \(T=4\) & \(0.125\pm\)N\(\lambda\) \\ Mag-GNN [20] & \(T=4\) & \(\mathbf{0.101\pm\)N\(\lambda\) \\ Ours & \(T=4\) & \(\mathbf{0.090\pm 0.003}\) \\ \hline Random [5] & \(T=5\) & \(0.113\pm 0.006\) \\ PL [5] & \(T=5\) & \(\mathbf{0.109\pm 0.005}\) \\ Ours & \(T=5\) & \(\mathbf{0.095\pm 0.003}\) \\ \hline Random [5] & \(T=8\) & \(0.102\pm 0.003\) \\ Pt. [5] & \(T=8\) & \(\mathbf{0.097\pm 0.005}\) \\ Ours & \(T=8\) & \(\mathbf{0.094\pm 0.006}\) \\ \hline Ours & \(T=18\) & \(\mathbf{0.082\pm 0.003}\) \\ \hline NGNN [39] & Full & \(0.111\pm 0.003\) \\ DS-GNN [4] & Full & \(0.116\pm 0.009\) \\ DSS-GNN [4] & Full & \(0.102\pm 0.003\) \\ GNN-Ax [40] & Full & \(0.105\pm 0.010\) \\ GNN-Ax* [40] & Full & \(0.091\pm 0.002\) \\ SUN [12] & Full & \(0.083\pm 0.003\) \\ OSAAN [29] & Full & \(0.154\pm 0.008\) \\ GNN-SSWL+ [38] & Full & \(0.070\pm 0.005\) \\ Subgraphmer [3] & Full & \(0.067\pm 0.007\) \\ Subgraphmer+PE [3] & Full & \(\mathbf{0.063\pm 0.001}\) \\ Ours & Full & \(\mathbf{0.062\pm 0.0007}\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Test results on the Zinc-12k molecular dataset under \(500k\) parameter budget. The top two results are reported as **First** and **Second**.

Figure 5: \(T=2\).

Figure 6: \(T=3\).

Figure 7: \(T=4\).

Figure 8: \(T=5\).

Figure 10: \(T=12\).

Figure 9: \(T=8\).

Proofs

### Proofs of Appendix B

We first state the memorization theorem, proven in [36], which will be heavily used in a lot of the proofs in this section.

**Theorem G.1** (Memorization Theorem).: _Consider a dataset \(\{x_{j},y_{j}\}_{j=1}^{N}\in\mathbb{R}^{d}\times\mathbb{R}^{d_{y}}\), with each \(x_{j}\) being distinct and every \(y_{j}\in\{0,1\}^{d_{y}}\). There exists a 4-layer fully connected ReLU neural network \(f_{\theta}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d_{y}}\) that perfectly maps each \(x_{j}\) to its corresponding \(y_{j}\), i.e., \(f_{\theta}(x_{j})=y_{j}\) for all \(j\)._

We now restate and prove the propositions and lemmas of Appendix B.

**Lemma B.1** (Parameter Sharing as MPNN).: _Let \(B_{1},\ldots B_{k}:\mathbb{R}^{n\times n}\) be orthogonal matrices with entries restricted to 0 or 1, and let \(W_{1},\ldots W_{k}\in\mathbb{R}^{d\times d^{l}}\) denote a sequence of weight matrices. Define \(B_{+}=\sum_{i=1}^{k}B_{i}\) and choose \(z_{1},\ldots z_{k}\in\mathbb{R}^{d^{k}}\) to be a set of unique vectors representing an encoding of the index set. The function that represents an update via parameter sharing:_

\[f(X)=\sum_{i=1}^{k}B_{i}XW_{i},\] (18)

_can be implemented on any finite family of graphs \(\mathcal{G}\), by a stack of MPNN layers of the following form [13],_

\[m_{v}^{l}=\sum_{u\in N_{B_{+}}(v)}M^{l}(X_{u}^{l},e_{u,v}),\] (19) \[X_{v}^{l+1}=U^{l}(X_{v}^{l},m_{v}^{l}),\] (20)

_where \(U^{l},M^{l}\) are multilayer perceptrons (MLPs). The inputs to this MPNN are the adjacency matrix \(B_{+}\), node feature vector \(X\), and edge features - the feature of edge \((u,v)\) is given by:_

\[e_{u,v}=\sum_{i=1}^{k}z_{i}\cdot B_{i}(u,v).\] (21)

_Here, \(B_{i}(u,v)\) denotes the \((u,v)\) entry to matrix \(B_{i}\)._

Proof.: Since we are concerned only with input graphs \(G\) from a finite family of graphs (where "finite" means that the maximal graph size is bounded and all possible node and edge feature values come from a finite set), we assume that for any \(v\in[n]\), \(i\in[k]\), both the input feature vectors \(X_{v}\in\mathbb{R}^{d}\) and the encoding vectors \(z_{i}\in\mathbb{R}^{d^{*}}\) are one-hot encoded. We aim to show that under these assumptions, any function \(f(\cdot)\) of the form 95 can be realized through a single-layer update detailed in Equations 97, 96, where \(M\) is a 4 layer MLP, and \(U\) is a single linear layer. The proof involves the following steps:

1. Compute \([B_{1}X,\ldots,B_{k}X]\) using the message function \(M\).
2. Compute \(f(X)\) using the update function \(U\).

**Step 1:** We notice that for every \(i\in[k]\), \(v\in[n]\) we have:

\[(B_{i}X)_{v}=\sum_{B_{i}(v,u)=1}X_{u}=\sum_{u\in N_{B_{+}}(v)}X_{u}\cdot \mathbf{1}_{z_{i}}(e_{u,v}).\] (100)

Here \(\mathbf{1}_{z_{i}}\) is the indicator function of the set \(\{z_{i}\}\). We notice that since \(X_{u}\) and \(z_{i}\) are one-hot encoded, there is a finite set of possible values for the pair \((X_{u},e_{u,v})\). In addition, the function:

\[enc(X_{u},e_{u,v})=[X_{u}\cdot\mathbf{1}_{z_{1}}(e_{u,v}),\ldots,X_{u}\cdot \mathbf{1}_{z_{k}}(e_{u,v})]\] (101)

outputs vectors in the set \(\{0,1\}^{d\times k}\). Thus, employing the memorization theorem G.1, we define a dataset \(\{x_{j},y_{j}\}_{j=1}^{N}\) by taking the \(x_{j}\)s to be all possible (distinct) values of \((X_{u},e_{u,v})\) with each corresponding \(y_{i}\) being the output \(\text{enc}(x_{i})\). We note that there are finitely many such values as both \(X_{u}\) and \(e_{u,v}\) are one-hot encoded. The theorem now tells us that there exists a a 4-layer fully connected ReLU neural network \(M\) such that:

\[M(X_{u},e_{u,v})=enc(X_{u},e_{u,v}).\] (102)

and so, equation 100 implies:

\[m_{v}=\sum_{u\in N_{B_{+}}(v)}M(X_{u},e_{u,v})=[(B_{1}X)_{v},\ldots,(B_{k}X)_{v }].\] (103)

**Step 2:** Define \(P_{i}:\mathbb{R}^{k\times d}\rightarrow\mathbb{R}^{d}\) as the projection operator, extracting coordinates \(d\cdot i+1\) through \(d\cdot(i+1)\) from its input vector:

\[P_{i}(V)=V|_{d\cdot i+1:d\cdot(i+1)}.\] (104)

We define the update function to be the following linear map:

\[U(X_{v},m_{v})=\sum_{i=1}^{k}P_{i}(m_{v})W_{i}.\] (105)

Combining equations 103 and 105 we get:

\[\tilde{X}_{v}=U(X_{v},m_{v})=\sum_{i=1}^{k}(B_{i}X)_{v}\cdot W_{i}=f(X)_{v}.\] (106)

**Proposition B.1** (Equivalence of General Layer and Implemented Layer).: _Let \(\mathcal{T}(\cdot)\) be a coarsening function, \(\pi\) be a generalized node marking policy, and \(\mathcal{G}\) be a finite family of graphs. Applying a stack of \(t\) general layer updates as defined in Equation 15 to the node feature map \(\mathcal{X}(S,v)\) induced by \(\pi(G,\mathcal{T})\), can be effectively implemented by applying a stack of \(t\) layer updates specified in Equations 29 and 30 to \(\mathcal{X}(S,v)\). Additionally, the depths of all MLPs that appear in 29 and 30 can be bounded by 4._

Proof.: For convenience, let us first restate the general layer update:

\[\begin{split}\mathcal{X}^{t+1}(S,v)=& f^{t}\bigg{(} \mathcal{X}^{t}(S,v),\\ &\text{agg}_{i}^{t}\,\{(\mathcal{X}^{t}(S,v^{\prime}),e_{v,v^{ \prime}})\mid v^{\prime}\sim_{G}v\}\hskip-1.422638pt\big{\}},\\ &\text{agg}_{2}^{t}\,\{(\mathcal{X}^{t}(S^{\prime},v),\tilde{e} _{S,S^{\prime}})\mid S^{\prime}\sim_{G^{\mathcal{T}}}S\hskip-1.422638pt\big{\}},\\ &\text{agg}_{3}^{t}\,\{(\mathcal{X}^{t}(S^{\prime},v),z(S,v,S^{ \prime},v))\mid S^{\prime}\in V^{\mathcal{T}}\text{s.t.}\hskip 2.845276ptv\in S^{ \prime}\}\hskip-1.422638pt\big{\}},\\ &\text{agg}_{4}^{t}\,\{(\mathcal{X}^{t}(S,v^{\prime}),z(S,v,S,v^{ \prime}))\mid v^{\prime}\in V\text{s.t.}\hskip 2.845276ptv^{\prime}\in S \hskip-1.422638pt\big{\}}\bigg{)},\end{split}\] (15)

as well as the two step implemented layer update:

\[\mathcal{X}^{t}_{i}(S,v)=U^{t}_{i}\left((1+\epsilon^{t}_{i})\cdot\mathcal{X}^ {t}(S,v)+\sum_{(S^{\prime},v^{\prime})\sim_{A_{i}}(S,v)}M^{t}(\mathcal{X}^{t} (S^{\prime},v^{\prime})+e_{i}(S,v,S^{\prime},v^{\prime}))\right).\] (29)

\[\mathcal{X}^{t+1}(S,v)=\mathbf{U}^{t}_{\text{fin}}\left(\sum_{i=1}^{4} \mathcal{X}^{t}_{i}(S,v)\right).\] (30)

We aim to demonstrate that any general layer, which updates the node feature map \(\mathcal{X}^{t}(S,v)\) at layer \(t\) to node feature map \(\mathcal{X}^{t+1}(S,v)\) at layer \(t+1\) as described in equation 15, can be effectively implemented using the layer update processes outlined in equations 29 and 30.

[MISSING_PAGE_FAIL:35]

\[\begin{split}&\{(\mathcal{X}^{t}(S,v),z(S,v,S^{\prime},v^{\prime})) \mid v^{\prime}\in S\}\\ &=\{(\mathcal{X}^{t}(S^{\prime},v^{\prime}),e_{P_{2}}(S,v,S^{ \prime},v^{\prime}))\mid(S,v)\sim_{A_{P_{2}}}(S^{\prime},v^{\prime})\}\end{split}\] (116)

Now, since \(m_{i}^{t}\) and \(\mathcal{X}^{t}(S,v)\) are supported on orthogonal sub-spaces of \(\mathbb{R}^{k}\), the sum \(\mathcal{X}^{t}(S,v)+m_{i}^{t}\) uniquely encodes the value of:

\[\begin{split}&\left(\mathcal{X}^{t}(S,v),\{\mathbb{\{}}(\mathcal{ X}^{t}(s,v),e_{i}(S,v,S^{\prime},v^{\prime}))\mid(S,v)\sim_{A_{i}}(S^{\prime},v^{ \prime})\}\right).\end{split}\] (117)

Thus, we choose \(\epsilon_{1}^{t},\ldots,epsilon_{4}\) to be all zeroes. To compute the aggregation functions \(\text{agg}_{1}^{t},\ldots,\text{agg}_{4}^{t}\) using these unique encodings, and to avoid repetition of the value \(\mathcal{X}^{t}(S,v)\), we define auxiliary functions \(\tilde{\text{agg}}_{i}^{t}:\mathbb{R}^{k}\rightarrow\mathbb{R}^{k_{i}}\) for \(i=1,\ldots,4\) as follows:

\[\tilde{\text{agg}}_{1}^{t}(\mathcal{X}^{t}(S,v)+m_{1}^{t})=\left(\mathcal{X}^ {t}(S,v),\text{agg}_{1}^{t}\{\mathbb{\{}}(\mathcal{X}^{t}(S,v^{\prime}),e_{1 }(S,v,S^{\prime},v^{\prime}))\mid(S,v)\sim_{A_{1}}(S^{\prime},v^{\prime})\}\right)\] (118)

and for \(i>1\):

\[\tilde{\text{agg}}_{i}^{t}(\mathcal{X}^{t}(S,v)+m_{i}^{t})=\text{agg}_{i}^{t }\{\mathbb{\{}}(\mathcal{X}^{t}(s,v),e_{i}(S,v,S^{\prime},v^{\prime}))\mid(S, v)\sim_{A_{i}}(S^{\prime},v^{\prime})\}.\] (119)

Here, since we avoided repeating the value of \(\mathcal{X}^{t}(S,v)\) by only adding it to the output of \(\tilde{\text{agg}}_{1}^{t}(\cdot)\), the expression:

\[\left(\tilde{\text{agg}}_{1}^{t}(\mathcal{X}^{t}(S,v)+m_{1}^{t}),\ldots, \tilde{\text{agg}}_{4}^{t}(\mathcal{X}^{t}(S,v)+m_{4}^{t})\right)\] (120)

is exactly equal to the input of \(f^{t}\). In addition, since the function \(\text{agg}_{i}^{t}\) outputs one-hot encoded vectors, and the vector \(\mathcal{X}^{t}(S,v)\) is one-hot encoded, the output of \(\tilde{\text{agg}}_{i}^{t}\) is always within the set \(\{0,1\}^{k_{i}}\). Now for any input vector \(X\in\mathbb{R}^{k}\) define:

\[V_{1}^{t}(X)=(\tilde{\text{agg}}_{1}^{t}(X),\ 0_{k_{2}},\ 0_{k_{3}},\ 0_{k_{4}}).\] (121)

\[V_{2}^{t}(X)=(0_{k_{1}},\ \tilde{\text{agg}}_{2}^{t}(X),\ 0_{k_{3}},\ 0_{k_{4}}).\] (122)

\[V_{3}^{t}(X)=(0_{k_{1}},\ 0_{k_{2}},\ \tilde{\text{agg}}_{3}^{t}(X),\ 0_{k_{4}}).\] (123)

\[V_{4}^{t}(X)=(0_{k_{1}},\ 0_{k_{2}},\ 0_{k_{3}},\tilde{\text{agg}}_{4}^{t}(X)).\] (124)

We note that since the output of \(\text{agg}_{i}^{t}\) is always within the set \(\{0,1\}^{k_{i}}\), the outputs of \(V_{i}^{t}\) is always within \(\{0,1\}^{k_{1}+\cdots+k_{4}}\). Now for \(i=1,\ldots 4\), employing theorem G.1 we define a dataset \(\{x_{j},y_{j}\}_{j=1}^{N}\) by taking the \(x_{j}\)s as all possible (distinct) values of \(\mathcal{X}^{t}(S,v)+m_{i}^{t}\), with each corresponding \(y_{j}\) being the output \(V_{i}^{t}(x_{j})\). We note that there are finitely many such values as both \(\mathcal{X}^{t}(S,v)\) and \(m_{i}^{t}\) are one-hot encoded vectors. The theorem now tells us that there exists a a 4-layer fully connected ReLU neural network capable of implementing the function \(V_{i}^{t}(\cdot)\). We choose \(U_{i}^{t}\) to be this network. Equations 121 - 124 now give us:

\[\sum_{i=1}^{4}\mathcal{X}_{i}^{t}(S,v)=\left(\tilde{\text{agg}}_{1}^{t}( \mathcal{X}^{t}(S,v)+m_{1}^{t}),\ldots,\tilde{\text{agg}}_{4}^{t}(\mathcal{X} ^{t}(S,v)+m_{4}^{t})\right).\] (125)

which as stated before, is exactly the input to \(f^{t}\). This proves step 2.

**Step 3:** We employ theorem G.1 for one final time, defining a dataset \(\{x_{j},y_{j}\}_{j=1}^{N}\) by taking the \(x_{j}\)s as all possible(distinct) values of:

\[\sum_{i=1}^{4}\mathcal{X}_{i}^{t}(S,v)\]

(which we showed is a unique encoding to the input of \(f^{t}(\cdot)\)), with each corresponding \(y_{j}\) being the output \(f^{t}(x_{j})\). We note that Given the finite nature of our graph set, there are finitely many such values. Recalling that \(f^{t}(\cdot)\) outputs one-hot encoded vectors, The theorem now tells us that there exists a a 4-layer fully connected ReLU neural network capable of implementing the function \(f^{t}(\cdot)\). We choose \(U_{\text{fin}}^{t}\) to be this network. This completes the proof.

### Proofs of Appendix C

**Proposition C.1** (Equal Expressivity of Node Marking Policies).: _For any coarsening function \(\mathcal{T}(\cdot)\) the following holds:_

\[\text{CS-GNN}(\mathcal{T},\pi_{S})=\text{CS-GNN}(\mathcal{T},\pi_{\text{SS}})= \text{CS-GNN}(\mathcal{T},\pi_{\text{MD}}).\] (36)

Proof.: Let \(\Pi=\{\pi_{\text{S}},\pi_{\text{SS}},\pi_{\text{MD}}\}\) be the set of all relevant node initialization policies, and assume for simplicity that our input graphs have no node features (the proof can be easily adjusted to account for the general case). For each \(\pi\in\Pi\), let \(\mathcal{X}^{\pi}(S,v)\) denote the node feature map induced by general node marking policy \(\pi\), as per Definition C.1. We notice it is enough to prove for each \(\pi_{1},\pi_{2}\in\Pi\) that \(\mathcal{X}^{\pi_{1}}(S,v)\) can be implemented by updating \(\mathcal{X}^{\pi_{2}}(S,v)\) using a stack of \(T\) layers of type 54. Thus, we prove the following four cases:

* Node + Size Marking \(\Rightarrow\) Simple Node Marking.
* Minimum Distance \(\Rightarrow\) Simple Node Marking.
* Simple Node Marking \(\Rightarrow\) Node + Size Marking.
* Simple Node Marking \(\Rightarrow\) Minimum Distance.

**Node + Size Marking \(\Rightarrow\) Simple Node Marking**:

In this case, we aim to update the node feature map:

\[\mathcal{X}^{0}(S,v)=\mathcal{X}^{\pi_{SS}}(S,v)=\begin{cases}(1,|S|)&v\in S \\ (0,|S|)&v\notin S.\end{cases}\] (126)

We notice that:

\[\mathcal{X}^{0}(S,v)=\langle(1,0),\;\mathcal{X}^{\pi_{S}}(S,v)\rangle,\] (127)

where \(\langle\cdot,\cdot\rangle\) denotes the standard inner product in \(\mathbb{R}^{2}\). Using a CS-GNN update as per equation 15, with the update function:

\[f^{1}(\mathcal{X}^{0}(S,v),\cdot,\cdot,\cdot,\cdot)=\langle(1,0),\mathcal{X}^ {0}(S,v)\rangle,\] (128)

where \(f(a,\cdot,\cdot,\cdot,\cdot)\) indicates that the function \(f\) depends solely on the parameter \(a\), we obtain:

\[\mathcal{X}^{1}(S,v)=f^{1}(\mathcal{X}^{0}(S,v),\cdot,\cdot,\cdot,\cdot)= \mathcal{X}^{\pi_{S}}(S,v).\] (129)

This implies that for any coarsening function \(\mathcal{T}(\cdot)\), the following holds:

\[\text{CS-GNN}(\mathcal{T},\pi_{\text{S}})\subseteq\text{CS-GNN}(\mathcal{T}, \pi_{\text{SS}}).\] (130)

**Minimum Distance \(\Rightarrow\) Simple Node Marking**:

In this case, we aim to update the node feature map:

\[\mathcal{X}^{0}(S,v)=\mathcal{X}^{\pi_{\text{MD}}}(S,v)=\min_{v\in s}d_{G}(u,v)\] (131)

We notice that:

\[\mathcal{X}^{\text{S}}(S,v)=g(\mathcal{X}^{0}(S,v))\] (132)

where \(g:\mathbb{R}\rightarrow\mathbb{R}\) is any continuous function such that:

1. \(g(x)=1\;\forall x>\frac{1}{2}\),
2. \(g(x)=0\;\forall x<\frac{1}{4}\).

Using a CS-GNN update as per equation 15, with the update function:

\[f^{1}(\mathcal{X}^{0}(S,v),\cdot,\cdot,\cdot,\cdot)=g(\mathcal{X}^{0}(S,v)),\] (133)

we obtain:

\[\mathcal{X}^{1}(S,v)=f^{1}(\mathcal{X}^{0}(S,v),\cdot,\cdot,\cdot,\cdot)= \mathcal{X}^{\pi_{S}}(S,v).\] (134)This implies that for any coarsening function \(\mathcal{T}(\cdot)\) the following holds:

\[\text{CS-GNN}(\mathcal{T},\pi_{\text{S}})\subseteq\text{CS-GNN}(\mathcal{T},\pi_ {\text{MD}}).\] (135)

**Simple Node Marking \(\Rightarrow\) Node + Size Marking**: In this case, we aim to update the node feature map:

\[\mathcal{X}^{0}(S,v)=\mathcal{X}^{\pi_{S}}(S,v)=\begin{cases}1&v\in S\\ 0&v\notin S.\end{cases}\] (136)

We notice that:

\[\sum_{v^{\prime}\in S}\mathcal{X}^{0}(S,v^{\prime})=|S|.\] (137)

Using a CS-GNN update as per Equation (15), with aggregation function:

\[\text{agg}^{l}_{4}\{\hskip-1.422638pt\|(\mathcal{X}^{0}(S,v^{\prime}),z(S,v, S,v^{\prime}))\mid v^{\prime}\in S\hskip-1.422638pt\}=\sum_{v^{\prime}\in S} \mathcal{X}^{0}(S,v^{\prime}),\] (138)

and update function:

\[f^{1}\left(\mathcal{X}^{0}(S,v),\cdot,\cdot,\cdot,\sum_{v^{\prime}\in S} \mathcal{X}^{0}(S,v^{\prime})\right)=\left(\mathcal{X}^{0}(S,v),\sum_{v^{ \prime}\in S}\mathcal{X}^{0}(S,v^{\prime})\right),\] (139)

we obtain:

\[\mathcal{X}^{1}(S,v)=f^{1}\left(\mathcal{X}^{0}(S,v),\cdot,\cdot,\cdot,\sum_{v ^{\prime}\in S}\mathcal{X}^{0}(S,v^{\prime})\right)=\mathcal{X}^{\pi_{SS}}(S,v).\] (140)

This implies that for any coarsening function \(\mathcal{T}(\cdot)\) the following holds:

\[\text{CS-GNN}(\mathcal{T},\pi_{\text{SS}})\subseteq\text{CS-GNN}(\mathcal{T}, \pi_{\text{S}}).\] (141)

**Simple Node Marking \(\Rightarrow\) Minimum Distance**:

In this case, we aim to update the node feature map:

\[\mathcal{X}^{0}(S,v)=\mathcal{X}^{\pi_{S}}(S,v)=\begin{cases}1&v\in S\\ 0&v\notin S.\end{cases}\] (142)

We shall prove that \(\mathcal{X}^{\pi_{\text{MD}}}\) can be expressed by updating \(\mathcal{X}^{0}(S,v)\) with a stack of CS-GNN layers. We do this by inductively showing that this procedure can express the following auxiliary node feature maps:

\[\mathcal{X}^{t}_{*}(S,v)=\begin{cases}\min_{v^{\prime}\in S}d_{G}(v,v^{\prime })+1&\min_{v^{\prime}\in S}d_{G}(v,v^{\prime})\leq t\\ 0&\text{otherwise}.\end{cases}\] (143)

We notice first that:

\[\mathcal{X}^{0}(S,v)=\mathcal{X}^{0}_{*}(S,v).\] (144)

Now for the induction step, assume that there exists a stack of \(t\) CS-GNN layers such that:

\[\mathcal{X}^{t}(S,v)=\mathcal{X}^{t}_{*}(S,v).\] (145)

We observe that equation:

\[\min_{v^{\prime}\in S}d_{G}(v,v^{\prime})=t+1\] (146)

holds if and only if the following two conditions are met:

\[\min_{v^{\prime}\in S}d_{G}(v,v^{\prime})>t\] (147)

\[\exists u\in N_{G}(v)\text{ s.t. }\min_{u^{\prime}\in S}d_{G}(u,u^{\prime})=t.\] (148)

Equations 143 imply:

\[\min_{v^{\prime}\in S}d_{G}(v,v^{\prime})>t\Leftrightarrow\mathcal{X}^{t}(S,v )=0.\] (149)In addition, since the node feature map \(\mathcal{X}^{t}=\mathcal{X}^{t}_{*}\) is bounded by \(t+1\), Equation (143) implies:

\[\exists u\in N_{G}(v)\text{ s.t. }\min_{u^{\prime}\in S}d_{G}(u,u^{\prime})=t \Leftrightarrow\max\{\mathcal{X}^{t}(s,u)\mid v\sim_{G}u\}=t+1.\] (150)

Now, let \(g_{t}:\mathbb{R}^{2}\rightarrow\mathbb{R}\) be any continuous function such that for every pair of natural numbers \(a,b\in\mathbb{N}\):

1. \(g_{t}(a,b)=t+2\quad\text{if }a=0,b=t+1\),
2. \(g_{t}(a,b)=a\quad\text{otherwise}\).

Equations 146 - 150 imply:

\[\mathcal{X}^{t+1}_{*}(S,v)=g_{t}(\mathcal{X}^{t}(S,v),\max\{\mathcal{X}^{t}(s, u)\mid v\sim_{G}u\}).\] (151)

Using a CS-GNN update as per Equation (15), with aggregation function:

\[\text{agg}^{t}_{1}\{\hskip-1.422638pt\{(\mathcal{X}^{t}(S,v^{\prime}),e_{v,v ^{\prime}})\mid v^{\prime}\sim_{G}v\}\hskip-1.422638pt\}=\max_{v^{\prime}\sim_ {G}v}\mathcal{X}^{t}(S,v^{\prime}).\] (152)

and update function:

\[f^{t}(\mathcal{X}^{t}(S,v),\max_{v^{\prime}\sim_{G}v}\mathcal{X}^{t}(S,v^{ \prime}),\cdot,\cdot,\cdot)=g_{t}(\mathcal{X}^{t}(S,v),\max_{v^{\prime}\sim_{G} v}\mathcal{X}^{t}(S,v^{\prime}))\] (153)

we obtain:

\[\mathcal{X}^{t+1}(S,v)=f^{t}(\mathcal{X}^{t}(S,v),\max_{v^{\prime}\sim_{G}v} \mathcal{X}^{t}(S,v^{\prime}),\cdot,\cdot,\cdot)=\mathcal{X}^{t+1}_{*}(S,v).\] (154)

This completes the induction step. Now, let \(\mathcal{G}\) be a finite family of graphs, whose maximal vertex size is \(n\). We notice that:

\[\mathcal{X}^{\tau_{\text{MD}}}(S,v)=\mathcal{X}^{n}_{*}(S,v)-1,\] (155)

Which implies that there exists a stack of \(n\) CS-GNN layers such that:

\[\mathcal{X}^{0}(S,v)=\mathcal{X}^{\tau_{\text{S}}}(S,v)\quad\text{and}\quad \mathcal{X}^{n}(S,v)=\mathcal{X}^{\tau_{\text{MD}}}(S,v).\] (156)

This implies:

\[\text{CS-GNN}(\mathcal{T},\pi_{\text{MD}})\subseteq\text{CS-GNN}(\mathcal{T },\pi_{\text{S}}).\] (157)

This concludes the proof. 

## Appendix D

Figure 11: Graphs G and H defined in the proof of Proposition C.2. In each graph, the circle marks the single super-node induced by \(\mathcal{T}\), while the number next to each node \(u\) is the maximal SPD between \(u\) and the nodes that compose the super-node.

**Proposition C.2** (Expressivity of Learned Distance Policy).: _For any coarsening function \(\mathcal{T}(\cdot)\) the following holds:_

\[\text{CS-GNN}(\mathcal{T},\pi_{\text{S}})\subseteq\text{CS-GNN}(\mathcal{T},\pi _{\text{LD}}).\] (37)

_In addition, for some choices of \(\mathcal{T}(\cdot)\) the containment is strict._

Proof.: First, since we are concerned with input graphs belonging to a finite graph family \(\mathcal{G}\), the learned function \(\phi(\cdot)\) implemented by an MLP can express any continuous function on \(\mathcal{G}\). This follows from Theorem G.1 (see the proof of Proposition B.1 for details). By choosing \(\phi=\min(\cdot)\) in equation 35, it is clear that for any coarsening function \(\mathcal{T}(\cdot)\) we have:

\[\text{CS-GNN}(\mathcal{T},\pi_{\text{S}})=\text{CS-GNN}(\mathcal{T},\pi_{ \text{MD}})\subseteq\text{CS-GNN}(\mathcal{T},\pi_{\text{LD}}).\] (158)

We now construct a coarsening function \(\mathcal{T}(\cdot)\) along with two graphs, \(G\) and \(H\), and demonstrate that there exists a function in \(\text{CS-GNN}(\mathcal{T},\pi_{\text{LD}})\) that can separate \(G\) and \(H\). However, every function in \(\text{CS-GNN}(\mathcal{T},\pi_{\text{S}})\) cannot separate the two.

For an input graph \(G=(V,E)\) define:

\[\mathcal{T}(G)=\{\{u\in V\mid\text{deg}_{G}(u)=3\}\}.\] (159)

i.e., \(\mathcal{T}(\cdot)\) returns a single super-node composed of all nodes with degree 3. Now, define \(G=(V_{G},E_{G})\) as the graph obtained by connecting two cycles of size four by adding an edge between a single node from each cycle. Additionally, define \(H=(V_{H},E_{H})\) as the graph formed by joining two cycles of size five along one of their edges. See Figure 11 for an illustration of the two graphs. By choosing \(\phi=\max(\cdot)\) in equation 35 a quick calculation shows that:

\[\sum_{S\in V_{\mathcal{T}(G)}}\sum_{v\in V_{G}}\mathcal{X}^{\pi_{\text{LD}}}(S,v)=16,\] (160)

while:

\[\sum_{S\in V_{\mathcal{T}(H)}}\sum_{v\in V_{H}}\mathcal{X}^{\pi_{\text{LD}}}(S,v)=14.\] (161)

Refer to Figure 11 for more details. Observe that:

\[f(G)=\sum_{s\in V_{\mathcal{T}(H)}}\sum_{u\in V_{H}}\mathcal{X}^{\pi_{\text{LD }}}(S,v)\in\text{CS-GNN}(\mathcal{T},\pi_{\text{LD}})\] (162)

Thus it is enough to show that:

\[f(G)=f(H),\quad\forall f\in\text{CS-GNN}(\mathcal{T},\pi_{\text{S}}).\] (163)

To achieve this, we use the layer update as per Definition B.2, which was demonstrated in Proposition B.1 to be equivalent to the general equivariant message passing update in Definition A.5. First, we observe that the graphs \(G\) and \(H\) are WL-indistinguishable. We then observe that since \(|V^{\mathcal{T}}|=1\), the graphs induced by the adjacency matrices \(A_{G}\) and \(A_{H}\) in Definition B.1 are isomorphic to the original graphs \(G\) and \(H\), respectively, and therefore they are also WL-indistinguishable. Additionally, we notice that the graphs induced by the adjacency matrices \(A_{\mathcal{T}(G)}\) and \(A_{\mathcal{T}(H)}\) in Definition B.1 are both isomorphic to the fully disconnected graph with 8 nodes, making them WL-indistinguishable as well. We also observe that there exists a bijection \(\sigma:V_{G}\to V_{H}\) that maps all nodes of degree 3 in \(G\) to all nodes of degree 3 in \(H\). The definition of \(\mathcal{T}(\cdot)\) implies that \(\sigma\) is an isomorphism between the adjacency matrices \(A_{P_{i}}\) corresponding to \(G\) and \(H\), where \(i=1,2\). Finally, we notice that for both \(G\), and \(H\), the node feature map induced by \(\pi_{\text{S}}\) satisfies:

\[\mathcal{X}^{\pi_{\text{S}}}(S,v)=\text{deg}(v)-2.\] (164)

This node feature map can be easily implemented by the layer update in definition B.2 and so it can be ignored. Since all four graphs corresponding to \(G\) that are induced by the adjacency matrices in Definition B.1, are WL-indistinguishable from their counterpart corresponding to \(H\), and equation 29 in definition B.2 is an MPNN update, which is incapable of distinguishing graphs that are WL-indistinguishable, we see that equation 163 holds, concluding the proof.

**Proposition C.3** (Node + Size Marking as Invariant Marking).: _Given a graph \(G=(V,E)\) with node feature vector \(X\in\mathbb{R}^{n\times d}\), and a coarsening function \(\mathcal{T}(\cdot)\), let \(\mathcal{X}^{\pi_{\text{SS}}},\mathcal{X}^{\pi_{\text{m}}}\) be the node feature maps induced by \(\pi_{\text{SS}}\) and \(\pi_{\text{inv}}\) respectively. Recall that:_

\[\mathcal{X}^{\pi_{\text{SS}}}(S,v) =[X_{v},b_{\pi_{\text{SS}}}(S,v)],\] (43) \[\mathcal{X}^{\pi_{\text{inv}}}(S,v) =[X_{v},b_{\pi_{\text{inv}}}(S,v)].\] (44)

_The following now holds:_

\[b_{\pi_{\text{inv}}}(S,v)=\text{OHE}(b_{\pi_{\text{SS}}}(S,v))\quad\forall S \in V^{\mathcal{T}},\;\forall v\in V.\] (45)

_Here, OHE denotes a one-hot encoder, independent of the choice of both \(G\) and \(\mathcal{T}\)._

Proof.: Let \(G=(V,E)\) be a graph with \(V=[n]\), and let \(\mathcal{T}(\cdot)\) be a coarsening function. Recall that the maps \(b_{\pi_{\text{SS}}}(\cdot,\cdot)\) and \(b_{\pi_{\text{inv}}}(\cdot,\cdot)\) are both independent of the connectivity of \(G\) and are defined as follows:

\[b_{\pi_{\text{NS}}}(S,v)=\begin{cases}(1,|S|)&v\in S,\\ (0,|S|)&v\notin S.\end{cases}\] (165)

\[b_{\pi_{\text{inv}}}(S,v)=[\mathbf{1}_{\gamma_{1}}(S,v),\dots,\mathbf{1}_{\gamma _{k}}(S,v)].\] (166)

Here, \(v\in[n]\), \(S\in\mathcal{T}([n])\subseteq\mathcal{P}([n])\), \(\gamma_{1},\dots,\gamma_{k}\) is any enumeration of the set of all orbits \((\mathcal{P}([n])\times[n])/S_{n}\), and \(\mathbf{1}_{\gamma_{i}}\) denotes the indicator function of orbit \(\gamma_{i}\). Since any tuple \((S,v)\in\mathcal{P}([n])\times[n]\) belongs to exactly one orbit \(\gamma_{i}\), we note that the right hand side of Equation (166) is a one-hot encoded vector. Thus, it suffices to show that for every \(v,v^{\prime}\in[n]\) and \(S,S^{\prime}\in\mathcal{P}([n])\), we have:

\[b_{\pi_{\text{SS}}}(S,v)=b_{\pi_{\text{NS}}}(S^{\prime},v^{\prime})\Leftrightarrow b _{\pi_{\text{inv}}}(S,v)=b_{\pi_{\text{inv}}}(S^{\prime},v^{\prime}).\] (167)

This is equivalent to:

\[(\mathcal{P}([n])\times[n])/S_{n}=\left\{(S,v)\;|\;|S|=i,\mathbf{1}_{S}(v)=j \right\}\;|\;i\in[n],j\in\left\{0,1\right\}.\] (168)

Essentially, this means that each orbit corresponds to a choice of the size of \(s\) and whether \(v\in S\) or not. To conclude the proof, it remains to show that for any two pairs \((S,v),(S^{\prime},v^{\prime})\in\mathcal{P}([n])\times[n]\), there exists a permutation \(\sigma\in S_{n}\) such that:

\[\sigma\cdot(S,v)=(S^{\prime},v^{\prime})\] (169)

if and only if

\[|S|=|S^{\prime}|\text{ and }\mathbf{1}_{S}(v)=\mathbf{1}_{S^{\prime}}(v^{\prime}).\] (170)

Assume first that \(\sigma\cdot(S,v)=(S^{\prime},v^{\prime})\), then \(\sigma^{-1}(S)=S^{\prime}\) and since \(\sigma\) is a bijection, \(|S|=|S^{\prime}|\). In addition \(\sigma^{-1}(v)=v^{\prime}\) thus:

\[v\in S\Leftrightarrow v^{\prime}=\sigma^{-1}(v)\in\sigma^{-1}(S)=S^{\prime}.\] (171)

Assume now that:

\[|S| =|S^{\prime}|\] (172) \[\mathbf{1}_{S}(v) =\mathbf{1}_{S^{\prime}}(v^{\prime})\] (173)

It follows that for some \(r,m\in[n]\):

\[|S\setminus\{v\}|=|S^{\prime}\setminus\{v^{\prime}\}|=r\quad\text{and}\quad| [n]\setminus(S\cup\{v\})|=|[n]\setminus(S^{\prime}\cup\{v^{\prime}\})|=m\] (174)

Write:

\[S\setminus\{v\}=\{i_{1},\dots,i_{r}\},\quad S^{\prime}\setminus\{v^{\prime}\} =\{i^{\prime}_{1},\dots,i^{\prime}_{r}\},\]

\[[n]\setminus(S\cup\{v\})=\{j_{1},\dots j_{m}\},\quad[n]\setminus(S^{\prime} \cup\{v^{\prime}\})=\{j^{\prime}_{1},\dots j^{\prime}_{m}\}\]

and define:

\[\sigma(x)=\begin{cases}v^{\prime}&x=v\\ i^{\prime}_{l}&x=i_{l},l\in[r]\\ j^{\prime}_{l}&x=j_{l},l\in[m]\end{cases}\] (175)

We now have:

\[\sigma\cdot(S,v)=(S^{\prime},v^{\prime}).\] (176)

This concludes the proof.

### Proofs of Appendix D.1

**Proposition D.1** (CS-GNN Can Implement MSGNN).: _Let \(\mathcal{T}(\cdot)\) be the identity coarsening function defined by:_

\[\mathcal{T}(G)=\{\{v\}\mid v\in V\}\quad\forall G=(V,E).\] (49)

_The following holds:_

\[\text{CS-GNN}(\mathcal{T},\pi_{\text{S}})=\text{MSGNN}(\pi_{\text{NM}}).\] (50)

Proof.: Abusing notation, for a given graph \(G=(V,E)\) we write \(\mathcal{T}(G)=G\), \(V^{\mathcal{T}}=V\). First, we observe that:

\[v\in\{u\}\Leftrightarrow u=v,\] (177)

This implies that the initial node feature map \(\mathcal{X}^{0}(u,v)\) induced by \(\pi_{\text{S}}\) is equivalent to the standard node marking described in equation 46. Additionally, we note that the pooling procedures for both models, as described in equations 16 and 55, are identical. Therefore, it is sufficient to show that the CS-GNN and MSGNN layer updates described in equations 15 and 47 respectively are also identical. For this purpose, let \(\mathcal{X}^{t}(v,u)\) be a node feature map supported on the set \(V\times V\). The inputs to the MSGNN layer are the following:

1. \(\mathcal{X}^{t}(u,v)\).
2. \(\mathcal{X}^{t}(u,u)\).
3. \(\mathcal{X}^{t}(v,v)\).
4. \(\text{agg}^{t}_{1}\{\!\!\{(\mathcal{X}^{t}(u,v^{\prime}),e_{v,v^{\prime}})\mid v ^{\prime}\sim v\}\!\!\}\).
5. \(\text{agg}^{t}_{2}\{\!\!\{(\mathcal{X}^{t}(u^{\prime},v),e_{u,u^{\prime}})\mid u ^{\prime}\sim u\}\!\!\}\).

The inputs to the CS-GNN layer are the following:

1. \(\mathcal{X}^{t}(S,v)\Rightarrow\mathcal{X}^{t}(u,v)\).
2. \(\text{agg}^{t}_{1}\{\!\!\{(\mathcal{X}^{t}(S,v^{\prime}),e_{v,v^{\prime}})\mid v ^{\prime}\sim_{G}v\}\!\!\}\Rightarrow\text{agg}^{t}_{1}\{\!\!\{(\mathcal{X}^{ t}(u,v^{\prime}),e_{v,v^{\prime}})\mid v^{\prime}\sim v\}\!\!\}\).
3. \(\text{agg}^{t}_{2}\{\!\!\{(\mathcal{X}^{t}(S^{\prime},v),\tilde{e}_{S,S^{ \prime}})\mid S^{\prime}\sim_{G^{\mathcal{T}}}S\}\!\!\}\Rightarrow\text{agg}^{ t}_{2}\{\!\!\{(\mathcal{X}^{t}(u,u^{\prime}),e_{u,v^{\prime}})\mid v^{\prime}\sim v\}\!\!\}\).
4. \(\text{agg}^{t}_{3}\{\!\!\{(\mathcal{X}^{t}(S^{\prime},v),z(S,v,S^{\prime},v ))\mid\forall s^{\prime}\in V^{\mathcal{T}}\text{s.t.}\ v\in S^{\prime}\}\!\!\} \Rightarrow\{\!\!\{(X^{t}(v,v),z(u,v,v,v))\}\!\!\}\).
5. \(\text{agg}^{t}_{4}\{\!\!\{(\mathcal{X}^{t}(S,v^{\prime}),z(S,v,S,v^{\prime} ))\mid\forall u^{\prime}\in V\text{s.t.}\ v^{\prime}\in S\}\!\!\}\Rightarrow \{\!\!\{(X^{t}(u,u),z(u,v,u,u))\}\!\!\}\).

The terms \(z(u,v,v,v)\) and \(z(u,v,u,u)\) appearing in the last two input terms of the CS-GNN layer uniquely encode the orbit tuples \((u,v,v)\) and \((u,v,u,u)\) belong to respectively. Since these orbits depend solely on whether \(u=v\), these values are equivalent to the node marking feature map \(\mathcal{X}^{0}(u,v)\). Therefore, these terms can be ignored. Observing the two lists above, we see that the inputs to both update layers are identical (ignoring the \(z(\cdot)\) terms), Thus, as both updates act on these inputs in the same way, the updates themselves are identical. and so

\[\text{MSGNN}(\pi_{\text{NM}})=\text{CS-GNN}(\mathcal{T},\pi_{\text{S}}).\] (178)

### Proofs of Appendix D.2

**Proposition D.2** (CS-GNN Is at Least as Expressive as Coarse MPNN ).: _For any coarsening function \(\mathcal{T}(\cdot)\) the following holds:_

\[\text{MPNN}\subseteq\text{MPNN}_{+}(\mathcal{T})\subseteq\text{CS-GNN}( \mathcal{T},\pi_{S})\] (58)Proof.: For convenience, let us first restate the CS-GNN layer update:

\[\begin{split}\mathcal{X}^{t+1}(S,v)&=f^{t}\bigg{(} \mathcal{X}^{t}(S,v),\\ &\mathsf{agg}_{1}^{t}\{\!\!\{(\mathcal{X}^{t}(S,v^{\prime}),e_{v,v ^{\prime}})\mid v^{\prime}\sim_{G}v\}\!\!\},\\ &\mathsf{agg}_{2}^{t}\{\!\!\{(\mathcal{X}^{t}(S^{\prime},v), \tilde{e}_{S,S^{\prime}})\mid S^{\prime}\sim_{G^{\mathcal{T}}}S\}\!\!\},\\ &\mathsf{agg}_{3}^{t}\{\!\!\{\mathcal{X}^{t}(S^{\prime},v),z(S, v,S^{\prime},v))\mid s^{\prime}\in V^{\mathcal{T}}\text{s.t.}\ v\in S^{\prime}\}\!\!\},\\ &\mathsf{agg}_{4}^{t}\{\!\!\{(\mathcal{X}^{t}(S,v^{\prime}),z(S, v,S,v^{\prime}))\mid u^{\prime}\in V\text{s.t.}\ v^{\prime}\in S\}\!\!\},\end{split}\] (15)

as well as the MPNN\({}_{+}\) layer update:

\[\begin{split}\text{For }v\in V:&\quad\mathcal{X}^{t+1}(v) =f^{t}_{V}\left(\mathcal{X}^{t}(v),\mathsf{agg}_{1}^{t}\{\!\!\{(\mathcal{X}^ {t}(v^{\prime}),e_{v,v^{\prime}})\mid v\sim_{G}v^{\prime}\}\!\!\},\right.\\ &\qquad\qquad\qquad\qquad\qquad\left.\mathsf{agg}_{2}^{t}\{\!\!\! \{\mathcal{X}^{t}(S)\mid S\in V^{T},v\in S\}\!\!\}\right),\\ \text{For }S\in V^{\mathcal{T}}:&\quad\mathcal{X}^{t+1}(S )=f^{t}_{V^{\mathcal{T}}}\left(\mathcal{X}^{t}(S),\mathsf{agg}_{1}^{t}\{\!\!\{ (\mathcal{X}^{t}(S^{\prime}),e_{S,S^{\prime}})\mid S\sim_{G^{\mathcal{T}}}S^{ \prime}\}\!\!\},\right.\\ &\qquad\qquad\qquad\qquad\left.\mathsf{agg}_{2}^{t}\{\!\!\!\{ \mathcal{X}^{t}(v)\mid v\in V,v\in S\}\!\!\}\right).\end{split}\] (54)

We note that by setting \(f^{t}_{V^{\mathcal{T}}}\) to be a constant zero and choosing \(f^{t}_{V}\) to be any continuous function that depends only on its first two arguments, the update in equation 54 becomes a standard MPNN layer. This proves:

\[\text{MPNN}\subseteq\text{MPNN}_{+}(T).\] (179)

Next, we prove the following 2 Lemmas:

**Lemma G.1**.: _Given a graph \(G=(V,E)\) such that \(V=[n]\) with node feature vector \(X\in\mathbb{R}^{n\times d}\), and a coarsening function \(\mathcal{T}(\cdot)\), there exists a CS-GNN\((\mathcal{T},\pi_{S})\) layer such that:_

\[\mathcal{X}^{1}(S,v)=[0_{d+1},X_{v},1]=[\tilde{\mathcal{X}}^{0}(S),\tilde{ \mathcal{X}}^{0}(v)].\] (180)

_Here \([\cdot,\cdot]\) denotes concatenation and \(\tilde{\mathcal{X}}^{0}(\cdot)\) denotes the initial node feature map of the coarsened sum graph \(G^{\mathcal{T}}_{+}\)._

**Lemma G.2**.: _Let \(\tilde{\mathcal{X}}^{t}(\cdot)\) denote the node feature maps of \(G^{T}_{+}\) at layers \(t\) of a stack of MPNN\({}_{+}(\mathcal{T})\) layers. There exists a stack of \(t+1\) CS-GNN\((\mathcal{T},\pi_{S})\) layers such that:_

\[\mathcal{X}^{t+1}(S,v)=[\tilde{\mathcal{X}}^{t}(S),\tilde{\mathcal{X}}^{t}(v)].\] (181)

Proof of Lemma G.1.: Recall that the initial node feature map of CS-GNN\((\mathcal{T},\pi_{S})\) is given by:

\[\mathcal{X}^{0}(S,v)=\begin{cases}[X_{v},1]&v\in S\\ [X_{v},0]&v\notin S.\end{cases}\] (182)

In addition, the initial node feature map of MPNN\({}_{+}(\mathcal{T})\) is given by:

\[\tilde{X}^{0}(v)=\begin{cases}[X_{v},1]&v\in V\\ 0_{d+1}&v\in V^{\mathcal{T}}.\end{cases}\] (183)

Thus, we choose a layer update as described in equation 15 with:

\[\mathcal{X}^{1}(S,v)=f^{0}(\mathcal{X}^{0}(S,v),\cdot,\cdot,\cdot,\cdot)=[0_{d+ 1},\mathcal{X}^{0}(S,v)_{1:d},1]\] (184)

Here, \(f(a,\cdot,\cdot,\cdot)\) denotes that the function depends only on the parameter \(a\), and \(X_{a:b}\) indicates that only the coordinates \(a\) through \(b\) of the vector \(X\) are taken. This gives us:

\[\mathcal{X}^{1}(S,v)=[\tilde{\mathcal{X}}^{0}(S),\tilde{\mathcal{X}}^{0}(v)].\] (185)

[MISSING_PAGE_FAIL:44]

We notice that:

\[\begin{split}\sum_{s\in V^{\mathcal{T}}}\mathcal{X}^{T+2}(S,v)& =\left[\sum_{S\in V^{\mathcal{T}}}\tilde{X}^{T}(S),\;\sum_{S\in V^{ \mathcal{T}}}\tilde{X}^{T}(v),\;\sum_{S\in V^{\mathcal{T}}}1\right]\\ &=\left[\sum_{S\in V^{\mathcal{T}}}\tilde{X}^{T}(S),\;|V^{ \mathcal{T}}|\cdot\tilde{X}^{T}(v),\;|V^{\mathcal{T}}|\right].\end{split}\] (196)

Thus, in order to get rid of the \(|V^{\mathcal{T}}|\) term, We define:

\[\text{MLP}_{1}(a,b,c)=[a,\frac{1}{c}\cdot b,1],\quad a,b\in\mathbb{R}^{d_{L}}, c>0.\] (197)

We note that since we are restricted to a finite family of input graphs, the use of an MLP in equation 200 can be justified using Theorem G.1 (see the proof of Proposition B.1 for a detailed explanation).

Equations 196 and 200 imply:

\[\text{MLP}_{1}\left(\sum_{s\in V^{\mathcal{T}}}\mathcal{X}^{T+2}(S,v)\right)= \left[\sum_{S\in V^{\mathcal{T}}}\tilde{X}^{T}(S),\;\tilde{X}^{T}(v),\;1\right]\] (198)

Thus, similarly to equation 196:

\[\sum_{v\in V}\text{MLP}_{1}\left(\sum_{S\in V^{\mathcal{T}}}\mathcal{X}^{T+2 }(S,v)\right)=\left[|V|\cdot\sum_{S\in V^{\mathcal{T}}}\tilde{X}^{T}(S),\;\sum _{v\in V}\tilde{X}^{T}(v),\;|V|\right]\] (199)

And so, in order to get rid of the \(|V|\) term, We define:

\[\text{MLP}_{2}(a,b,c)=U(a\cdot\frac{1}{c}+b,1),\quad a,b\in\mathbb{R}^{d_{T}},c>0.\] (200)

Thus for all \(G\in\mathcal{G}\):

\[\begin{split}&\text{MLP}_{2}\left(\sum_{v\in V}\text{MLP}_{1}\left(\sum_{S\in V^{ \mathcal{T}}}\mathcal{X}^{T+2}(S,v)\right)\right)\\ &=\text{MLP}_{2}\left(\left[|V|\cdot\sum_{S\in V^{\mathcal{T}}} \tilde{X}^{T}(S),\;\sum_{v\in V}\tilde{X}^{T}(v),\;|V|\right]\right)\\ &=U\left(\sum_{v\in V^{\mathcal{T}}_{+}}\tilde{X}^{T}(v)\right) \\ &=f(G).\end{split}\] (201)

and so \(f\in\text{CS-GNN}(\mathcal{T},\pi_{\text{S}})\). This proves:

\[\text{MPNN}_{+}(T)\subseteq\text{CS-GNN}(\mathcal{T},\pi_{\text{S}}).\] (202)

**Proposition D.3** (CS-GNN Can Be More Expressive Than MPNN\(+\)).: _Let \(\mathcal{T}(\cdot)\) be the identity coarsening function defined by:_

\[\mathcal{T}(G)=\{\{v\}\;|\;v\in V\}\quad G=(V,E).\] (59)

_The following holds:_

\[\text{MPNN}=\text{MPNN}_{+}(\mathcal{T}).\] (60)

_Thus:_

\[\text{MPNN}_{+}(\mathcal{T})\subset\text{CS-GNN}(\mathcal{T},\pi_{\text{S}}),\] (61)

_where this containment is strict._Proof.: First, using the notation \(\tilde{v}\) to mark the single element set \(\{v\}\in V^{\mathcal{T}}\), We notice that the MPNN\({}_{+}(\mathcal{T})\) layer update described in equation 54, becomes:

\[\begin{split}\text{For }v\in V:&\quad\mathcal{X}^{t+1}(v) =f^{t}_{V}\bigg{(}\mathcal{X}^{t}(v),\mathcal{X}^{t}(\tilde{v}),\text{agg}^{t} \{\hskip-1.0pt\{(\mathcal{X}^{t}(v^{\prime}),e_{v,v^{\prime}})\mid v^{\prime} \sim_{G}v\}\hskip-1.0pt\},\bigg{)},\\ \text{For }\tilde{v}\in V^{\mathcal{T}}:&\quad\mathcal{X} ^{t+1}(\tilde{v})=f^{t}_{V^{\mathcal{T}}}\bigg{(}\mathcal{X}^{t}(\tilde{v}), \mathcal{X}^{t}(v),\text{agg}^{t}\{\hskip-1.0pt\{(\mathcal{X}^{t}(\tilde{v^{ \prime}}),e_{\tilde{v},\tilde{v^{\prime}}})\mid v\sim_{G}v^{\prime}\}\hskip-1.0pt \}\bigg{)}.\end{split}\] (203)

Now, for a given finite family of graphs \(\mathcal{G}\) and a function \(f\in\text{MPNN}_{+}(\mathcal{T})\), there exists a stack of \(T\) MPNN\({}_{+}(\mathcal{T})\) layers such that:

\[f(G)=U\left(\sum_{v\in V^{\mathcal{T}}_{+}}\mathcal{X}^{T}(v)\right)\quad \forall G\in\mathcal{G}.\] (204)

Here, \(\mathcal{X}^{T}:V^{\mathcal{T}}_{+}\rightarrow\mathbb{R}^{d}\) denotes the final node feature map, and \(U\) is an MPL. We now prove by induction on \(t\) that there exists a stack of \(t\) standard MPNN layers, with corresponding node feature map \(X^{t}:V\rightarrow\mathbb{R}^{2d_{t}}\) such that :

\[X^{t}(v)=[\mathcal{X}^{t}(v),\mathcal{X}^{t}(\tilde{v})].\] (205)

Here, \([\cdot,\cdot]\) stands for concatenation. We assume for simplicity that the input graph \(G\) does not have node features, though the proof can be easily adapted for the more general case. We notice that for the base case \(t=0\), equation 53 in definition D.2 implies:

\[\mathcal{X}^{0}(v)=\begin{cases}1&v\in V,\\ 0&v\in V^{\mathcal{T}}.\end{cases}\] (206)

Thus, we define:

\[X^{0}(v)=(1,0).\] (207)

This satisfies Equation (205), establishing the base case of the induction. Assume now that Equation (205) holds for some \(t\in[T]\). Let \(\text{agg}^{t},f^{t}_{V},f^{t}_{V\mathcal{T}}\) be the components of layer \(t\), as in equation 203. We define:

\[\text{a}\tilde{\text{g}}\text{g}^{t}=[\text{agg}^{t}|_{1:d_{t}},\text{agg}^{t }|_{d_{t}+1:2d_{t}}].\] (208)

Here the operation \(\text{agg}_{i:b}\) initially projects all vectors in the input multi-set onto coordinates \(a\) through \(b\), and subsequently passes them to the function \(\text{agg}\).

Additionally, let \(d^{*}\) denote the dimension of the output of the function \(\text{agg}^{t}\). We define:

\[\tilde{f}^{t}(a,b)=\left[f^{t}_{V}\left(a|_{1:d_{t}},a|_{d_{t}+1:2d_{t}},b|_{1: d^{*}}\right),f^{t}_{V}\left(a|_{d_{t}+1:2d_{t}},a|_{1:d_{t}},b|_{d^{*}+1:2d ^{*}}\right)\right].\] (209)

Finally, we update our node feature map \(X^{t}\) using a standard MPNN update according to:

\[X^{t+1}(v)=\tilde{f}^{l}\left(X^{t}(v),\hskip-1.0pt\{(X^{t}(v^{\prime}),e_{v, v^{\prime}})\mid v^{\prime}\sim_{G}v\}\hskip-1.0pt\}\right).\] (210)

equations 203, 205 and 210 now guarantee that:

\[X^{t+1}(v)=[\mathcal{X}^{t}(v),\mathcal{X}^{t+1}(\tilde{v})].\] (211)

This concludes the inductive proof. We now define:

\[\text{MLP}(x)=U(x|_{1:d_{T}})+U(x|_{d_{T}+1:2d_{T}}).\] (212)

This gives us:

\[U\bigg{(}\sum_{v\in V^{\mathcal{T}}_{+}}\mathcal{X}^{T}(v)\bigg{)}=\text{MLP} \bigg{(}\sum_{v\in V}X^{T}(v)\bigg{)}=f(G).\] (213)

We have thus proven that \(f\in\text{MPNN}\) and so:

\[\text{MPNN}_{+}(\mathcal{N})\subseteq\text{MPNN}.\] (214)

Combining this result with Proposition D.2, we obtain:

\[\text{MPNN}=\text{MPNN}_{+}(\mathcal{T}).\] (215)Finally, since Proposition D.1 tells us that CS-GNN\((\mathcal{T},\pi_{\mathrm{S}})\) has the same implementation power as the maximally expressive node policy subgraph architecture MSGNN, which is proven to be strictly more expressive than the standard MPNN, we have:

\[\text{MPNN}_{+}(\mathcal{T})\subset\text{CS-GNN}(\mathcal{T},\pi_{\mathrm{S}}).\] (216)

**Proposition D.4** (CS-GNN can be strictly more expressive then node-based subgraph GNNs).: _Let \(\mathcal{T}\) be the coarsening function defined by:_

\[\mathcal{T}(G)=\{\{v\}\mid v\in V\}\cup E\quad G=(V,E).\] (63)

_The following holds:_

1. _Let_ \(G_{1},G_{2}\) _be a pair of graphs such that there exists a node-based subgraph GNN model M where_ \(M(G_{1})\neq M(G_{2})\)_. There exists a CS-GNNmodel_ \(M^{\prime}\) _which uses_ \(\mathcal{T}\) _such that_ \(M^{\prime}(G_{1})\neq M^{\prime}(G_{2})\)_._
2. _There exists a pair of graphs_ \(G_{1},G_{2}\) _such that for any subgraph GNN model_ \(M\) _it holds that_ \(M(G_{1})=M(G_{2})\)_, but there exists a CS-GNNmodel_ \(M^{\prime}\) _which uses_ \(\mathcal{T}\) _such that_ \(M^{\prime}(G_{1})\neq M^{\prime}(G_{2})\)_._

Proof.: First, notice that the super-nodes produced by \(\mathcal{T}\) are either of size 1, in which case they correspond to nodes, or they are of size two, in which case they correspond to edges. Since an CS-GNNmodel processes feature maps \(\mathcal{X}^{t}(S,v)\) where in the initial layer the set size of \(S\) is encoded in \(\mathcal{X}^{t}(S,v)\), we can easily use the CS-GNNupdate in Definition A.5 to ignore all values of \(\mathcal{X}^{t}(S,v)\) were \(|S|=2\) (This can be done by using \(f^{t},\text{agg}_{1}^{t},\dots\text{agg}_{1}^{t}\) in Definition A.5 to zero out these values at each update). This means CS-GNNusing \(\mathcal{T}\) is able to simulate an CS-GNNupdate with the identity coarsening function, which was shown in Proposition D.1 to be as expressive as GNN-SSWL+ (Definition D.1) which is a maximally expressive node-based subgraph GNN, thus proving part (1) of the proposition. To prove part (2), notice that using the same reasoning as before, an CS-GNNmodel using \(\mathcal{T}\) as a coarsening function cal implement an CS-GNNmodel using the edge coarsening function:

\[\mathcal{T}^{\prime}(G)=E\quad G=(V,E).\] (217)

An CS-GNNmodel with the identity coarsening function can be interpreted as a GNN-SSWL+ model. Similarly, an CS-GNNmodel using the edge coarsening function \(\mathcal{T}^{\prime}\) generalizes the GNN-SSWL+ framework by extending it from node-based subgraph GNNs to edge-based subgraph GNNs. In fact, the same proof in [38, 12] showing that GNN-SSWL+ is at least as expressive as a DSS subgraph GNN using the node deletion policy (see [4] for a definition of the DSS subgraph GNN), can be used to show that CS-GNNusing the edge coarsening function \(\mathcal{T}^{\prime}\) is at least as expressive as a DSS subgraph GNN with an edge deletion policy. The latter model was shown in [4] to be able to separate a pair of 3-WL indistinguishable graphs. In contrast, node-based subgraph GNNs were shown in [12] to not be able to separate any pair of 3-WL indistinguishable graphs. Thus, there exists a pair of graphs which CS-GNNusing \(\mathcal{T}\) can separate while node-based subgraph GNNs cant, proving part (2) of the proposition.

### Proofs of Appendix E

**Lemma E.1** (\(\gamma\) (\(\Gamma\)) are orbits).: _The sets \(\{\gamma^{k^{*}}:k=1,\dots,n;*\in\{+,-\}\}\) and \(\{\Gamma^{\leftrightarrow;k_{1};k_{2};k^{\Gamma};\delta_{\text{ave}};\delta_{ \text{after}}}\}\) are the orbits of \(S_{n}\) on the index space \((\mathcal{P}([n])\times[n])\) and \((\mathcal{P}([n])\times[n]\times(\mathcal{P}([n])\times[n])\), respectively._

Proof.: We will prove this lemma for \(\gamma\). The proof for \(\Gamma\) follows similar reasoning; we also refer the reader to [22] for a general proof.

We will prove this lemma through the following three steps.

**(1).** Given indices \((S,i)\in\mathcal{P}([n])\times[n]\), there exists \(\gamma\in(\mathcal{P}([n])\times[n])_{\sim}\) such that \((S,i)\in\gamma\).

**(2).** Given indices \((S,i)\in\gamma\), for any \(\sigma\in S_{n}\), it holds that \((\sigma^{-1}(S),\sigma^{-1}(i))\in\gamma\).

**(3).** Given \((S,i)\in\gamma\) and \((S^{\prime},i^{\prime})\in\gamma\) (the same \(\gamma\)), it holds that there exists a \(\sigma\in S_{n}\) such that \(\sigma\cdot(S,i)=(S^{\prime},i^{\prime})\).

We prove in what follows.

**(1).** Given indices \((S,i)\in\mathcal{P}([n])\times[n]\), w.l.o.g. we assume that \(|S|=k\), thus if \(i\in S\) (\(i\notin S\)) it holds that \((S,i)\in\gamma^{k^{{}^{\prime}}}\ \big{(}(S,i)\in\gamma^{k^{{}^{\prime}}}\big{)}\), recall Equation (71).

**(2).** Given indices \((S,i)\in\gamma\), note that any permutation \(\sigma\in S_{n}\) does not change the cardinality of \(S\) nor the inclusion (or exclusion) of \(i\) in \(S\). Recalling Equation (71), we complete this step.

**(3).** Given that \((S,i)\in\gamma\) and \((S^{\prime},i^{\prime})\in\gamma\), and recalling Equation (71), we note that \(|S|=|S^{\prime}|\) and that either both \(i\in S\) and \(i^{\prime}\in S^{\prime}\), or both \(i\notin S\) and \(i^{\prime}\notin S^{\prime}\).

**(3.1).** In **(3.1)** we focus on the case where \(i\notin S\) and \(i^{\prime}\notin S^{\prime}\). Let \(S=\{i_{1},\ldots,i_{k}\}\) and \(S^{\prime}=\{i^{\prime}_{1},\ldots,i^{\prime}_{k}\}\). Then, we have \((\{i_{1},\ldots,i_{k}\},j)\) and \((\{i^{\prime}_{1},\ldots,i^{\prime}_{k}\},j^{\prime})\). Define \(\sigma\in S_{n}\) such that \(\sigma(i_{i})=i^{\prime}_{l}\) for \(l\in[k]\), and \(\sigma(j)=j^{\prime}\). Since \((\{i_{1},\ldots,i_{k}\},j)\) consists of \(k+1\) distinct indices and \((\{i^{\prime}_{1},\ldots,i^{\prime}_{k}\},j^{\prime})\) also consists of \(k+1\) distinct indices, this is a valid \(\sigma\in S_{n}\).

**(3.2).** Here, we focus on the case where \(i\in S\) and \(i^{\prime}\in S^{\prime}\). This proof is similar to **(3.1)**, but without considering the indices \(j\) and \(j^{\prime}\), as they are included in \(S\) and \(S^{\prime}\), respectively.

**Proposition E.1** (Basis of Invariant (Equivariant) Layer).: _The tensors \(\mathbf{B}^{\gamma}\) (\(\mathbf{B}^{\Gamma}\)) in Equation (72) (Equation (74)) form an orthogonal basis (in the standard inner product) to the solution of Equation (66) (Equation (67))._

Proof.: We prove this proposition for the invariant case. The equivariant case is proved similarly - we also refer the reader for [22] for a general proof. We will prove this in three steps,

**(1).** For any \(\gamma\in(\mathcal{P}([n])\times[n])_{\sim}\) it holds that \(\mathbf{B}^{\gamma}_{S,i}\) solves Equation (66).

**(2).** Given a solution \(\mathbf{L}\) to Equation (66), it is a linear combination of the basis elements.

**(3).** We show that the basis vectors are orthogonal and thus linearly independent.

We prove in what follows.

**(1).** Given \(\gamma\in(\mathcal{P}([n])\times[n])_{\sim}\), we need to show that \(\mathbf{B}^{\gamma}_{S,i}=\mathbf{B}^{\gamma}_{\sigma^{-1}(S),\sigma^{-1}(i)}\). Since any \(\gamma\in(\mathcal{P}([n])\times[n])_{\sim}\) is an orbit in the index space (recall Lemma E.1), and \(\mathbf{B}^{\gamma}_{S,i}\) are indicator vectors of the orbits this always holds.

**(2).** Given a solution \(\mathbf{L}\) to Equation (66), it must hold that \(\mathbf{L}_{S,i}=\mathbf{L}_{\sigma^{-1}(S),\sigma^{-1}(i)}\). Since the set \(\{\gamma^{k^{{}^{\prime}}}:k=1,\ldots,n;*\in\{+,-\}\}\) corresponds to the orbits in the index space with respect to \(S_{n}\), \(\mathbf{L}\) should have the same values over the index space of these orbits. Let's define these values as \(\alpha^{\gamma}\) for each \(\gamma\in\{\gamma^{k^{{}^{\prime}}}:k=1,\ldots,n;*\in\{+,-\}\}\). Thus, we obtain that \(\mathbf{L}^{\prime}=\sum_{\gamma\in(\mathcal{P}([n])\times[n])_{\sim}}\alpha^{ \gamma}\cdot\mathbf{B}^{\gamma}\), since \(\mathbf{B}^{\gamma}\) are simply indicator vectors of the orbits. This completes this step.

**(3).** Once again, since the basis elements are indicator vectors of disjoint orbits we obtain their orthogonality, and thus linearly independent.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract spells out all the main contributions in the present paper, both theoretical and empirical ones. These are extensively discussed and recapitulated in the Introduction Section 1 (see paragraphs "Our approach" and "Contributions"). The scope of the paper is well defined in the first periods of the abstract and comprehensively articulated in the first two paragraphs of the Introduction Section 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations**Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to paragraph "Limitations" in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Please refer to Appendices B to E and F.4, which include precise and contextualized statements of all theoretical results and derivations, and to Appendix G for proofs thereof. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]Justification: Please refer to Appendix F.1 for a description of the employed datasets and splitting procedure, Appendices F.2 and F.3 for a list of experimental details and hyperparameter settings, and Appendix F.5 for a series of complementary results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example * If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. * If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. * If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). * We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code to reproduce our results can be found in the following GitHub repository: https://github.com/BarSGuy/Efficient-Subgraph-GNNs. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Appendix F.1 for a description of the employed datasets and splitting procedure, Appendices F.2 and F.3 for a list of experimental details and hyperparameter settings, including the utilized training procedures. The results for baselines approaches are reported according to what stated in Section 5 and Appendix F.2. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our results in Section 5 are reported in terms of mean and standard deviation calculated over different model initializations (i.e., by setting different random seeds prior to code execution). Table 9 reports error bars for the results illustrated in Figure 2. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The hardware employed to obtain all experimental results, as well as a runtime comparison, are described in Appendix F.2 (see "Implementation Details"). Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have made sure to comply to the Code of Ethics and to preserve our anonymity. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The models we developed are not generative, hence not posing risks of malicious use as for what concerns fabricating misleading or otherwise fake information, online profiles and media. Additionally, although our approach improves the efficiency of certain Graph Neural Networks, the models we developed are not scalable enough to apply and impact (online) social networks: represented as graphs, their scale is way beyond that considered in our experiments. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our model(s) do not have a high risk of misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: As explicitly mentioned in the main corpus of the paper, the inset figure in Section 2 is taken with permission by the original authors. Creators of datasets employed in this study, as well as the benchmark frameworks used are properly referenced and cited. For these last we report license information in Appendix F.1, also reported for code assets Appendix F.2. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets**Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.