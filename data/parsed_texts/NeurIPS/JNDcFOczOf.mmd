# RA-PbRL: Provably Efficient Risk-Aware Preference-Based Reinforcement Learning

 Yujie Zhao\({}^{1}\), Jose Efraim Aguilar Escamil\({}^{2}\), Weyl Lu\({}^{3}\), Huazheng Wang\({}^{2}\)

\({}^{1}\) University of California, San Diego, \({}^{2}\) Oregon State University, \({}^{3}\) University of California, Davis

yuz285@ucsd.edu, agujose@oregonstate.edu,

adslu@ucdavis.edu, huazheng.wang@oregonstate.edu

###### Abstract

Reinforcement Learning from Human Feedback (RLHF) has recently surged in popularity, particularly for aligning large language models and other AI systems with human intentions. At its core, RLHF can be viewed as a specialized instance of Preference-based Reinforcement Learning (PbRL), where the preferences specifically originate from human judgments rather than arbitrary evaluators. Despite this connection, most existing approaches in both RLHF and PbRL primarily focus on optimizing a mean reward objective, neglecting scenarios that necessitate risk-awareness, such as AI safety, healthcare, and autonomous driving. These scenarios often operate under a one-episode-reward setting, which makes conventional risk-sensitive objectives inapplicable. To address this, we explore and prove the applicability of two risk-aware objectives to PbRL: nested and static quantile risk objectives. We also introduce Risk-Aware-PbRL (RA-PbRL), an algorithm designed to optimize both nested and static objectives. Additionally, we provide a theoretical analysis of the regret upper bounds, demonstrating that they are sublinear with respect to the number of episodes, and present empirical results to support our findings. Our code is available in https://github.com/aguilarjose11/PbRLNeurips.

## 1 Introduction

Reinforcement Learning (RL) (Russell and Norvig, 2010) is a fundamental framework for sequential decision-making, enabling intelligent agents to interact with and learn from unknown environments. This framework utilizes a reward signal to guide the selection of policies, where optimal policies maximize this signal. RL has demonstrated state-of-the-art performance in various domains, including clinical trials (Coronato et al., 2020), gaming (Silver et al., 2017), and autonomous driving (Basu et al., 2017).

Despite its performance, a significant limitation of the standard RL paradigm is the selection of a state-action reward function. In many real-world scenarios, constructing an explicit reward function is often a complex or unfeasible task. As a compelling alternative, Preference-based Reinforcement Learning (PbRL) (Busa-Fekete et al., 2014; Wirth et al., 2016) addresses this challenge by deviating from traditional quantifiable rewards for each step. Instead, PbRL employs binary preference feedback on trajectory pairs generated by two policies, which can be provided directly by human subjects. This method is increasingly recognized as a more intuitive and direct approach in fields involving human interaction and assessment, such as autonomous driving (Basu et al., 2017), healthcare (Coronato et al., 2020), and language models (Bai et al., 2022).A specialized and increasingly popular variant of PbRL is Reinforcement Learning from Human Feedback (RLHF)Bai et al. (2022), which has garnered considerable attention for aligning AI systems with human intentions--particularly in the domain of large-scale language models.

Previous approaches to PbRL (Xu et al., 2020; Coronato et al., 2020; Xu et al., 2020; Chen et al., 2022; Zhan et al., 2023) mainly aim to maximize the mean reward or utility, which is risk-neutral. However, there is a growing need for risk-aware strategies in various fields where PbRL has shown empirical success. For example, in autonomous driving, PbRL reduces the computational burden by skipping the need to calculate reward signals for every state-action pair (Chen et al., 2022). Despite this improvement, the nature of the problem makes dangerous actions costly. Thus, such risk-sensitive problem settings require risk awareness to ensure safety.

Risk-Aware PbRL also has implications for fields like generative AI (OpenAI, 2023; Chen et al., 2023), where harmful content generation remains a challenge for fine-tuning. In this scenario, a large language model (LLM) is often fine-tuned with user feedback by generating two prompt responses, A and B. Many approaches using RLHF (Ouyang et al., 2022) consider human feedback to penalize harmful content generation. Unfortunately, current approaches only minimize the average harmfulness of a response. This can be a challenge when responses are only harmful to a minority of users. Risk-aware PbRL tackles this challenge by directly aiming to decrease the harmfulness directly, rather than indirectly as with human feedback fine-tuning.

Despite substantial evidence highlighting the importance of risk awareness in PbRL, a significant gap persists in the theoretical analysis and formal substantiation of risk-aware PbRL approaches. This deficiency has spurred us to develop risk-aware measures and their theoretical analysis within PbRL. In standard RL, a variety of risk-aware measures have been explored, including a general family of risk-aware utility functions (Fei et al., 2020), iterated (nested) Conditional Value at Risk (CVaR) (Du et al., 2022), and risk-sensitive with quantile function form (Bastani et al., 2022). In general, these measures can be categorized into two types: nested or static. Nested measures (Fei et al., 2020; Du et al., 2022) utilize MDPs to ensure risk sensitivity of the value iteration at each step under the current state, resulting in a more conservative approach. In contrast, static risk-aware measures Bastani et al., 2022 analyze the risk sensitivity of the whole trajectory's reward distribution. In developing and introducing risk-aware objectives in PbRL, we have encountered the following technical challenges in algorithm design and theoretical analysis:

Rewards are defined over trajectories preferenceIn PbRL, the reward function depends on the preference between two trajectories generated by the agent. We refer to this difference in how the reward function is computed as the one-episode-feedback characteristic. Consequently, the risk-aware objectives of standard RL like Du et al. (2022) and Fei et al. (2020) become unmeasurable since they depend on the state-action reward.

Trajectory embedding reward assumptionWhen computing the trajectory reward, it is assumed that an embedding mapping exists. By using the trajectory embedding along with some other vector embedding pointing towards high-rewarding trajectories, the reward is computed with a dot product. Unfortunately, the embedding mapping may not be linear. This means that the embedded trajectory vectors may not follow the Markovian assumption, making the embeddings history-dependent.

Loss of linearity of Bellman functionWhen using a quantile function to transform a risk-neutral PbRL algorithm into a risk-aware algorithm, the Bellman equation used to solve the problem becomes non-linear. This change to the bellman equation disrupts calculations on regret, making risk-neutral PbRL inapplicable. This is primarily due to the additional parameter \(\alpha\), which modifies the underlying distribution.

In this paper, we address these challenges by studying the feasibility of risk-aware objectives in PbRL. We propose a provably efficient algorithm, Risk-Aware-PbRL(RA-PbRL), with theoretical and empirical results on its performance and risk-awareness. Our summary of contributions is as follows:

1. We analyze the feasibility of several risk-aware measures in PbRL settings and prove that in the one-episode-reward setting, nested and static quantile risk-aware objectives are applicable since they can be solved and computed uniquely in a given PbRL MDP.
2. We expand the state space in our formulation of a PbRL MDP and modify value iteration to address its history-dependent characteristics from the one-episode setting. These modifications enable us to use techniques like DPP to search for the optimal policy.
3. We develop a provably efficient (both computationally and statistically) algorithm, RA-PbRL, for nested and static quantile risk-aware objectives. To the best of our knowledge, we are the first to formulate and analyze the finite time regret guarantee for a risk-aware algorithm with non-Markovian reward models for both nested and static risk-aware objectives. Moreover, we construct a hard-to-learn instance for RA-PbRL to establish a regret lower bound.

## 2 Related Work

### Preference-based Feedback Reinforcement Learning

The incorporation of human preferences in RL, such as Jain et al. (2013), has been a subject of study for over a decade. This approach has proved to be successful and has been widely used in various applications, including language model training (Ouyang et al., 2022), clinical trials (Coronato et al., 2020), gaming (Silver et al., 2017), and autonomous driving (Basu et al., 2017). PbRL can be categorized into three distinct types Wirth et al. (2017): action preference, policy preference, and trajectory preference. Among these, trajectory preference is identified as the most general and widely studied form of preference-based feedback, as evidenced by the rich literature on the topic Chen et al. (2022); Xu et al. (2020); Wu and Sun (2023). As noted in our introduction, previous theoretical explorations on PbRL have predominantly aimed at achieving higher average rewards, which encompasses risk-neutral PbRL. We distinguish our work by taking the novel approach of formalizing the risk-aware PbRL problem.

### Risk-aware Reinforcement Learning

In recent years, research on risk-aware RL has proposed various risk measures. Works such as Fei et al. (2020); Shen et al. (2014); Eriksson and Dimitrakakis (2019) integrate RL with a general family of risk-aware utility functions or the exponential utility criterion. Accordingly, studies like Bastani et al. (2022); Wu and Xu (2023) delve into the CVaR measure for the whole trajectory's reward distribution in standard RL. Further, Du et al. (2022) propose ICVaR-RL, a nested risk-aware RL formulation that addresses both regret minimization and best policy identification. Additionally, the work of Chen et al. (2023) presents an advancement in the form of a nested CVaR measure within the framework of RLHF. The limitation of this work lies in the selection of a random reference trajectory for comparison, causing an unavoidable linear strong nested CVaR regret. Consequently, we are left with only a preference equation from which we are unable to compute the state-action reward function for each step.

Practical and relevant trajectory or state-action embeddings are described in works such as (Pacchiano et al., 2021). Therefore, the one-episode-reward might not even be sum-decomposable (the trajectory embedding details can be seen in sec.3.1 ). Compared to previous work, we use non-Markovian reward models that do not require estimating the reward at each step and explore both nested and static risk-aware objectives, aiming to provide a more general method.

## 3 Problem Set-up and Preliminary Analysis

### PbRL MDP

We first define a modification of the classical Markov Decision Process (MDP) to account for risk: Risk Aware Preference-Based MDP (RA-PB-MDP). The standard MDP is described as a tuple, \(\mathcal{M}(\mathcal{S},\mathcal{A},r_{\xi}^{\star},\mathbf{P}^{\star},H)\), where \(\mathcal{S}\) and \(\mathcal{A}\) represent finite state and action spaces, and \(H\) denotes the length of episodes. Additionally, let \(S:=|\mathcal{S}|\) and \(A:=|\mathcal{A}|\) denote the cardinalities of the state and action spaces, respectively. \(\mathbf{P}^{\star}:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{P}( \mathcal{S})\) is the transition kernel, where \(\mathcal{P}(\mathcal{X})\) denotes the space of probability measures on space \(\mathcal{X}\). A _trajectory_ is a sequence

\[\xi_{h}\in\mathcal{Z}_{h},\quad\mathcal{Z}=\bigcup_{h=1}^{H}\mathcal{Z}_{h} \qquad\text{where}\qquad\mathcal{Z}_{h}=(\mathcal{S}\times\mathcal{A})^{h-1} \times\mathcal{S}.\]

Intuitively, a trajectory encapsulates the interactions between an agent and the environment \(\mathcal{M}\) up to step \(h\). In contrast to the standard RL setting, where the reward function \(r_{h}^{\star}(s_{h},a_{h})\) specifies the reward at each step \(h\), a significant distinction in the PbRL MDP framework is that the reward function \(r^{\star}\) is defined as \(r_{\xi}^{\star}(\xi_{H}):(\mathcal{S}\times\mathcal{A})^{H-1}\times\mathcal{S }\rightarrow[0,1]\), denoting the reward of the entire trajectory.

Reward model for the entire trajectory.For any trajectory \(\xi_{H}\), we assume the existence of a trajectory embedding mapping \(\phi:\mathcal{Z}_{H}\to\mathbb{R}^{dim\tau}\), and the reward of the entire trajectory is defined as the function: \(r_{\xi}^{*}(\xi_{H}):=\langle\phi(\xi_{H}),\mathbf{w}_{\star}^{*}\rangle\). Here, \(dim_{\mathbb{T}}\) denotes the trajectory embedding dimension. Finally, we denote \(\phi(\xi_{H})=(\phi_{1}(\xi_{H}),\ldots,\phi_{dim_{\mathbb{T}}}(\xi_{H}))\).

Assumption 3.1.: We assume that for all trajectories \(\xi_{H}\) and for all \(d\in\{1,\ldots,dim_{\mathbb{T}}\}\), \(\|\phi_{d}(\xi_{H})\|\in\{0\}\cup[b,B]\) where \(b,B>0\) are known. \(\|\mathbf{w}_{\star}^{*}\|\leq\rho_{w}\) and \(\rho_{w}\) is known as well.

_Remark 3.2_.: We assume the map \(\phi\) is known to the learner. The sum of the state-action reward used in Chen et al. (2023) is one case of such map, where \(\phi(\xi_{H})=\sum_{h=1}^{H}\mathbb{I}\left(s_{h},a_{h}\right)\) and \(dim_{\mathbb{T}}=S\times A\). Therefore, for all \(d\in\{1,\ldots,dim_{\mathbb{T}}\}\), \(\|\phi_{d}(\xi_{H})\|\in\{0\}\cup\{1,\ldots,H\}\)

_Remark 3.3_.: Assumption 3.1 implies that there is a gap between zero and some positive number \(b\) in the absolute values of components of trajectory embeddings. This is evident for finite-step discrete action spaces, where we can enumerate all trajectory embeddings to find the smallest non-zero component, satisfying most application scenarios.

At each iteration \(k\in[K]\), the agent selects two policies under a deterministic policy framework, \(\pi_{1,k}\) and \(\pi_{2,k}\), which generate two (randomized) trajectories \(\xi_{H}^{1,k}\) and \(\xi_{H}^{2,k}\). In PbRL, unlike standard RL where the agent receives rewards every step, the agent can only obtain the preference \(o_{k}\) between two trajectories \(\left(\xi_{H}^{1,k},\xi_{H}^{2,k}\right)\). By making a query, we obtain a preference feedback \(o_{k}\in\{0,1\}\) that is sampled from a Bernoulli distribution:

\[o_{k}\sim Ber\left(\sigma\left(r_{\xi}^{*}\left(\xi_{H}^{1,k}\right)-r_{\xi}^{ *}\left(\xi_{H}^{2,k}\right)\right)\right)\] (1)

where \(\sigma:\mathbb{R}\to[0,1]\) is a monotonically increasing link function. We assume \(\sigma\) is known like the popular Bradley-Terry model (Hunter, 2004), wherein \(\sigma\) is represented by the logistic function. It is known that we can not estimate the trajectory reward without a known \(\sigma\). Also, we assume \(\sigma\) is Lipschitz continuous and \(\kappa\) is its Lipschitz coefficient.

History dependent policy.Since the algorithm can only observe the reward for an entire episode until the end, it cannot make decisions based solely on the current state. The agent cannot observe the individual reward \(r_{h}(s,a)\) and thus cannot compute the target value at each step. To circumvent this challenge, the algorithm should take action according to a history-dependent policy. A history-dependent policy \(\Pi=\left\{\pi_{h}\right\}_{h\in[H]}\) is defined as a sequence of mappings \(\psi_{h}:\mathcal{Z}_{h}\to\mathcal{A}\), providing the agent with guidance to select an action, given a trajectory \(\xi_{h}\in\mathcal{Z}_{h}\) at time step \(h\). For notation convenience, let \(\Pi\) denote the set of all history-dependent deterministic policies.

### Risk Measure

Because in PbRL we can only estimate the reward for the entire trajectory, the risk measure selected for PbRL must rely solely on the reward of the entire trajectory. That is, two trajectories with the same trajectory reward should contribute equally to the risk measure, even if their potential rewards at each step are different. Unlike Chen et al. (2023) that decomposes the reward at each step (where the solution is likely not unique) and then calculates the risk measure, this requirement ensures that the risk measure consistently and holistically reflects the underlying preference. We refer to risk measures that suitable for PbRL problems as _PbRL-risk-measures_. Here, we introduce two different risk-measures: nested and static quantile risk-aware measures, which are appropriate for PbRL-MDPs.

We first introduce the definition of quantile function and risk-aware objective. The quantile function of a random variable \(X\) is \(F_{X}^{\dagger}(\tau)=\inf\left\{x\in\mathbb{R}\mid F_{X}(x)\geq\tau\right\}\). We assume \(F_{X}\) is strictly monotone, so it is invertible and we have \(F_{X}^{\dagger}(\tau)=F_{X}^{-1}(\tau)\). The risk-aware objective is given by the Riemann-Stieljes integral:

\[\Phi(X)=\int_{0}^{1}F_{X}^{\dagger}(\tau)\mathrm{d}G(\tau)\] (2)

where \(X\) is the random variable encoding the value of MDP, and \(G\) is a weighting function over the quantiles. This class captures a broad range of useful objectives, including the popular CVaR objective (Bastani et al., 2022).

_Remark 3.4_.: (\(\alpha\)-CVaR objective) Specifically, in \(\alpha\)-CVaR,

\[G(\tau)=\begin{cases}\frac{1}{\alpha}\tau&\text{if }\tau<\alpha,\\ 1&\text{if }\tau\geq\alpha.\end{cases}\]

and \(\Phi(X)\) becomes

\[\Phi(X)=\frac{1}{\alpha}\int_{0}^{\alpha}F_{X}^{-1}(\tau)\mathrm{d}\tau.\]

**Assumption 3.5**.: \(G\) is \(L_{G}\)-Lipschitz continuous for some \(L_{G}\in\mathbb{R}_{>0}\), and \(G(0)=0,G(1)=1\).

For example, for the \(\alpha\)-CVaR objective, we have \(L_{G}=1/\alpha\).

There are two prevalent approaches to Risk-aware-MDPs: _nested (or iterated)_ (such as Iterated CVaR (ICVAR) [Du et al., 2022] and Risk-Sensitive Value Iteration (RSVI) [Fei et al., 2020]), and _static_ (referenced in [Bastani et al., 2022, Wu and Xu, 2023]). MDPs characterized by an iterated risk-aware objective facilitate a value function and uphold a Bellman-type recursion.

**Nested PbRL-risk-measures.** For standard RL's MDP, the nested quantile risk-aware measure can be elucidated in Bellman equation type as follows:

\[\begin{cases}Q_{h}^{\pi}(s,a)&=r_{h}^{\star}(s,a)+\Phi\left(V_{h+1}^{\pi}\left( s^{\prime}\right),s^{\prime}\sim\mathbf{P}^{\star}(s,a)\right)\\ V_{h}^{\pi}(s)&=Q_{h}^{\pi}\left(s,\pi_{h}(s)\right)\\ V_{\vec{H}+1}^{\pi}(s)&=0,\quad\forall s\in\mathcal{S}\end{cases}\] (3)

Here \(r_{h}^{\star}(s,a)\) denotes the decomposed state-action reward in step \(h\).

For the PbRL-MDP setting \(\mathcal{M}(\mathcal{S},\mathcal{A},r_{\xi}^{\star},\mathbf{P}^{\star},H)\), the state-action's reward might not be calculated or the reward of the entire trajectory might not be decomposed. Therefore, the policy should be history-dependent. We rewrite the nested quantile objective's Bellman equation with the embedded trajectory reward as follows:

\[\begin{cases}\tilde{Q}_{h}^{\pi}(\xi_{h},a)&=\Phi\left(\tilde{V}_{h+1}^{\pi} \left(s^{\prime}\circ(\xi_{h},a)\right),s^{\prime}\sim\mathbf{P}^{\star}(s,a) \right),\\ \tilde{V}_{h}^{\pi}(\xi_{h})&=\tilde{Q}_{h}^{\pi}\left(\xi_{h},\pi_{h}(\xi_{h })\right),\\ V_{\vec{H}}^{\pi}(\xi_{H})&=r^{\star}(\xi_{H}),\end{cases}\] (4)

For any PbRL MDP \(\mathcal{M}(\mathcal{S},\mathcal{A},r_{\xi},\mathbf{P},H)\), we use \(\tilde{V}_{h}^{\pi,r_{\xi},\mathbf{P}}(\xi_{h})\) to denote the value iteration under the policy \(\pi\), where \(\pi\) is a history dependent policy.

**Lemma 3.6**.: _For a given tabular MDP, the reward on the entire trajectory can be decomposed as \(r_{\xi}^{\star}\left(\xi_{H}\right)=\sum_{h=1}^{H}r_{h}^{\star}\left(s_{h},a_{ h}\right)\), \(V_{1}^{\pi}\) in Eq. 3 and \(\tilde{V}_{1}^{\pi}\) in Eq. 4 are equivalent._

The proof is detailed in Appendix B.1 due to space limitations.

Static PbRL-risk-measures.Standard MDPs with a static risk aware objective [Bellemare et al., 2017, Dabney et al., 2018] can be written in the distributional Bellman equation as follows:

\[\begin{split}& Z_{h}^{(\pi)}\left(\xi_{h}\right)=\sum_{h^{\prime }=h}^{H}r_{h^{\prime}}^{\star}(s_{h^{\prime}},a_{h^{\prime}}),\quad\xi_{H}\sim \mathbb{P}\left(\cdot\mid\Xi_{h}(\xi_{H})=\xi_{h}\right)\\ & F_{Z_{h}^{(\pi)}(\xi)}(x)=\sum_{s^{\prime}\in\mathcal{S}}P(s^{ \prime}\mid S(\xi),\pi_{h}(\xi))F_{Z_{h+1}^{(\pi)}(\xi\circ(s^{\prime},\pi_{ h}(\xi)))}(x-r_{h}^{\star}(s_{h},a_{h}))\\ & V_{1}^{\pi}(s)=\int_{0}^{1}F_{Z_{1}}^{\dagger}(\pi)(\tau)\cdot dG (\tau)\end{split}\] (5)

Where \(S(\xi)=s\) for \(\xi=(\ldots,s)\) is the current state in trajectory \(\xi\), \(\Xi_{h}(\xi_{H})=(s_{1},a_{1},s_{2},a_{2},\ldots,s_{h-1},a_{h-1},s_{h})\) denotes the first h steps' trajectory. \(Z_{h}^{(\pi)}\left(\xi_{h}\right)\) denoted the reward from step \(t\) given the current history. The _static reward_ of \(\pi\) is \(Z_{1}^{(\pi)}(\xi)\), where \(\xi=(s)\in\mathcal{Z}_{1}\) for \(s\sim D\) is the initial history.

Also, we modify the distributional Bellman equation for PbRL MDP \(\mathcal{M}(\mathcal{S},\mathcal{A},r_{\xi}^{\star},\mathbf{P}^{\star},H)\) settings as follows:

\[\begin{split}& Z_{h}^{(\pi)}\left(\xi_{h}\right)=r_{\xi}^{\star}( \xi_{H}),\quad\xi_{H}\sim\mathbb{P}\left(\cdot\mid\Xi_{h}(\xi_{H})=\xi_{h} \right)\\ & F_{Z_{h}^{(\pi)}(\xi_{h})}(x)=\sum_{s^{\prime}\in\mathcal{S}}P(s ^{\prime}\mid S(\xi),\pi_{h}(\xi))F_{Z_{h+1}^{(\pi)}(\xi\circ(s^{\prime},\pi_{ h}(\xi)))}(x)\\ &\tilde{V}_{1}^{\pi}(s)=\int_{0}^{1}F_{Z_{1}}^{\dagger}(\pi)(\tau )\cdot dG(\tau)\end{split}\] (6)

**Lemma 3.7**.: _For a tabular MDP and a reward of the entire trajectory can be decomposed as \(r_{\xi}^{\star}\left(\xi_{H}\right)=\sum_{h=1}^{H}r_{h}^{\star}\left(s_{h},a_{ h}\right)\), \(V_{1}^{\pi}\) in Eq. 5 and \(\tilde{V}_{1}^{\pi}\) in Eq. 6 are equivalent._

The proof is detailed in Appendix B.2 due to space limitation.

Each of these risk measures possesses distinct advantages and limitations. Nested risk measures, which incorporate a Bellman-type recursion, can directly employ techniques such as the Dynamic Programming Principle (DPP) for computation. However, they are challenging to interpret and are not law-invariant (Hau et al., 2023). On the other hand, static risk measures are straightforward to interpret, but the resulting optimal policy may not remain Markovian and becomes history-dependent. Consequently, techniques such as the DPP and the Bellman equation become inapplicable.

### Objective

We define an optimal policy as:

\[\pi^{\star}\in\operatorname*{argmax}_{\pi\in\Pi}\tilde{V}_{1}^{\pi}(s_{1})\] (7)

i.e., it maximizes the given objective for \(\mathcal{M}\). \(\tilde{V}_{1}^{\pi}(s_{1})\) will be decided by the selected risk measure, where value iteration calculated using Eq. 4 and static calculated using Eq. 6.

**Assumption 3.8**.: Regardless of nested or static CVaR objectives, we are given an algorithm for computing \(\pi_{\mathcal{M}}^{\star}\) for a known \(\mathrm{PbRL}\)-\(\mathrm{MDP}\)\(\mathcal{M}\).

A formal proof of Assumption 3.8 is given in Appendix F. When unambiguous, we drop \(\mathcal{M}\) and simply write \(\pi^{\star}\).

At the beginning of each episode \(k\in[K]\), our algorithm \(\mathfrak{A}\) chooses two policies \(\left(\pi_{1,k},\pi_{2,k}\right)=\mathfrak{A}\left(H_{k}\right)\), where \(H_{k}=\left\{\xi_{1,k^{\prime},H},\xi_{2,k^{\prime},H},o_{k}\right\}_{k^{ \prime}=0}^{k}\) is the random set of episodes observed so far. Then, our goal is to design an algorithm \(\mathfrak{A}\) that minimizes regret, which is naturally defined as:

\[\mathrm{Regret}(K):=\sum_{k=0}^{K}\left(2\tilde{V}_{1}^{\pi^{\star}}\left(s_{1 }\right)-\tilde{V}_{1}^{\pi_{1,k}}\left(s_{1}\right)-\tilde{V}_{1}^{\pi_{2,k}} \left(s_{1}\right)\right)\] (8)

## 4 Risk Aware Preference based RL Algorithm

In this section, we introduce and analyze an algorithm called RA-PbRL for solving the PbRL problem with both nested and static risk aware objectives. Also, we establish a regret bound for it.

### Algorithm

RA-PbRL is formally described in Algorithm 1. The development of RA-PbRL is primarily inspired by the PbOP algorithm, as delineated in Chen et al. (2022), which was originally proposed for risk-neutral PbRL environments. Building upon this foundation, one significant difference is how to choose a risk aware policy in estimated PbRL MDP, where the value iteration is different. We also use novel techniques to estimate the confidence set and explore for a policy, instead of using a bonus (Chen et al., 2022) (which is difficult to calculate in risk-aware problems) as in standard RL.

```
0: episode \(K\), step \(H\), initial state space \(\mathcal{P}\), initial reward space \(\mathcal{R}\), risk level \(\alpha\), confidence parameter \(\delta\)
1: Set \(\mathcal{B}_{0}^{\text{P}}=\mathcal{P},\mathcal{B}_{0}^{r}=\mathcal{R}\), Execute two arbitrary policies \(\pi_{1,0}\) and \(\pi_{2,0}\) for one episode, respectively, and then observe the trajectory \(\tau_{1,0}\) and \(\tau_{2,0}\) and the preference \(o_{0}\).
2:for\(k=1\cdots K\)do
3: Calculate the probability estimation \(\hat{\mathbf{P}}_{k}\): \(\hat{\mathbf{P}}_{k}=\arg\min_{\mathcal{P}\in\mathcal{P}}\sum_{i=1}^{2}\sum_{ k^{\prime}=0}^{k-1}\sum_{h=1}^{H}|\langle\mathbf{P}(s_{i,k^{\prime},h},a_{i,k^{ \prime},h}),\mathbb{I}(s_{i,k^{\prime},h+1})\rangle|^{2}\).
4: Update transition confidence set : \[\mathcal{B}_{k}^{\text{P}}=\left\{\mathbf{P}^{\prime}\mid\sum_{s^{\prime}\in \mathcal{S}}\left|\hat{\mathbf{P}}^{\text{P}}\left(s^{\prime}\mid s,a\right)- \mathbf{P}^{\prime}\left(s^{\prime}\mid s,a\right)\right|\leq\sqrt{\frac{2S \log\left(\frac{2KKHSA}{\delta}\right)}{n_{k}(s,a)}}\right\}\bigcap\mathcal{B}_ {k-1}^{\text{P}}\]
5: Calculate the reward estimation: \(\hat{r}_{k}(\cdot)=\arg\min_{r\in\mathcal{R}}\sum_{k=0}^{k-1}\left(\sigma(r( \tau_{1,k^{\prime}})-r(\tau_{2,k^{\prime}}))-o_{k^{\prime}}\right)^{2}\)
6: Update the confidence set: \(\mathcal{B}_{k}^{r}=\left\{r^{\prime}(\cdot)\left|\sum_{k^{\prime}=0}^{k-1} \left[\sigma(\hat{r}_{k}(\tau_{1,k^{\prime}})-\hat{r}_{k}(\tau_{2,k^{\prime}}) )-\sigma(r^{\prime}(\tau_{1,k^{\prime}})-r^{\prime}(\tau_{2,k^{\prime}})) \right]^{2}\leq\beta_{r,k}(\delta)\right\}\bigcap\mathcal{B}_{k-1}^{r}\)
7: Update policy confidence set: \(\Pi_{k}=\left\{\pi\mid\max_{\tau_{\ell}\in\mathcal{B}_{k}^{\text{P}},\mathbf{ P}\in\mathcal{B}_{k}^{\text{P}}}\left(\tilde{V}_{1,\tau_{\ell},\mathbf{P}}^{ \pi}(s_{1})-\tilde{V}_{1,\tau_{\ell},\mathbf{P}}^{\pi^{\prime}}(s_{1})\right) \geq 0,\forall\pi^{\prime}\right\}\bigcap\Pi_{k-1}\)
8: Compute \((\pi_{1,k},\pi_{2,k})\): \((\pi_{1,k},\pi_{2,k})=\operatorname*{arg\,max}_{\pi_{1},\pi_{2}\in\Pi_{k}} \max_{\tau_{\ell}\in\mathcal{B}_{k}^{\text{P}},\mathbf{P}\in\mathcal{B}_{k}^{ \text{P}}}\left(\tilde{V}_{1,\tau_{\ell},\mathbf{P}}^{\pi_{1}}(s_{1})-\tilde{ V}_{1,\tau_{\ell},\mathbf{P}}^{\pi_{2}}(s_{1})\right)\)
9: Observe the trajectory \(\xi_{1,k,H},\xi_{2,k,H}\), and the preference \(o_{k}\)
10: Calculate the state-action visiting time before episode \(k\): \(n_{k}(s,a)\)
11:endfor ```

**Algorithm 1** RA-PbRL

The overview of the algorithm.Now we introduce the main part of our algorithm. In line 1, we initialize the transition kernel function and reward function confidence set, and execute two arbitrary policies at first. For every episode, we observe history samples and accordingly estimate the transition kernel function (line 3) and update its confidence set (line 4) as well as the reward function (line 5) and its confidence set (line 6). Both estimation and calculation used the standard least-squares regression. Based on the confidence sets, we maintain a policy set in which all policies are near-optimal with minor sub-optimality gap with high probability in line 7. In line 8, we execute the most exploratory policy pair in the policy set and observe the preference between the trajectories sampled using these two policies.

The key difference between nested and static objective.The estimation of the transition kernel (line 4 in Algorithm 1) and the construction of confidence set (line 6 in Algorithm 1) are similar for both nested and static objectives. The difference lies in the value iteration, which is defined in Eq. 4 for nested objective and Eq. 6 for static objective. The bounds for regrets are different since the estimation error's impact is different as we are going to show below.

### Analysis

**Theorem 4.1** ( Nested object regret upper bound).: _With at least probability \(1-\delta\), the nested quantile risk aware object regret of RA-PBRL is bounded by:_

\[\operatorname{Reg}_{\text{nested}}\left(K\right)\] \[\leq \mathcal{O}\left(L_{G}H^{\frac{3}{2}}\sqrt{K}SA\log\left(\frac{KHSA }{\delta}\right)\cdot\frac{1}{\sqrt{\min_{\pi,h,s:\omega_{\pi,h}(s)>0}\omega_{ \pi,h}(s)}}\right)\] \[+ \mathcal{O}\left(\frac{B}{\kappa b}dim_{\mathbb{T}}\sqrt{\log\left( \frac{Kdim_{\mathbb{T}}}{\delta}\right)\log\left(\frac{K\left(1+2B\rho_{w} \right)}{\delta}\right)}\frac{1}{\min_{\pi,d}\omega_{dim,\pi}(d)}\right)\] (9)_Where \(w_{\pi,h}(s)\) denotes the probability of visiting state-action pair at \(h\) th step with policy \(\pi\) and \(\min_{\pi,d}\omega_{dim,\pi}(d)\) denotes probability of trajectory \(\xi_{H}\)'s d th feature \(\Phi_{d}(\xi_{H})\neq 0\) with the policy \(\pi\)._

The proof of this theorem is provided in Appendix D.20. The first term of the regret arises from the estimation error of the transition kernel, primarily dominated by \(\min_{\pi,h,s:w_{\pi,h}(s)>0}w_{\pi,h}(s)\). The second term is due to the estimation error of the trajectory reward weights, significantly impacted by \(min_{\pi,d}\omega_{\pi}(d)\). In fact, these factors are unavoidable in the lower bound in certain challenging cases. Thus, they characterize the inherent problem difficulty, i.e., in achieving the nested risk-aware objective, the agent will be highly sensitive to some state-actions or features that are difficult to observe and require substantial effort to explore. This may result in inefficiency in many scenarios.

**Theorem 4.2** (**Static object regret upper bound)**.: _The static quantile risk aware object regret of RA-PBRL is bounded by:_

\[\begin{split}&\mathrm{Reg}_{static}(K)\\ \leq&\mathcal{O}\left(L_{G}S^{2}AH^{\frac{3}{2}} \sqrt{K}log\left(K/\delta\right)\right)+\mathcal{O}\left(L_{G}dim_{\mathbb{T}} \sqrt{K\log(KB\rho_{w})\log\left(\frac{K\left(1+2B\rho_{w}\right)}{\delta} \right)}\right)\end{split}\] (10)

The proof of this theorem is provided in Appendix D.21. Notice that the regret for both the nested risk objective and the static risk objective of Algorithm 1 are sublinear with respect to \(K\), making RA-PBRL the first provably efficient algorithm with one-episode-reward for these two objectives. Additionally, compared to Chen et al. [2023], we achieve the goal of having both policies gradually approach optimality. Moreover, in comparison to the nested risk-aware objective, the static objective focuses on the risk measure of the entire distribution, primarily influenced by the Lipschitz coefficient \(L_{G}\) of the quantile function and is less constrained by certain specific cases.

**Theorem 4.3** (**Nested object regret lower bound)**.: _The nested quantile risk aware object regret of RA-PBRL is bounded by:_

\[\mathrm{Regret}(K)\geq\mathcal{O}\left(\min\left\{B\rho_{w}\sqrt{\frac{AK}{ \min_{\pi,h,s:p_{\pi,h}(s)>0}w_{\pi,h}(s,a)}},B\sqrt{\frac{AK}{\min_{\pi,d} \omega_{dim,\pi}(d)}}\right\}\right)\] (11)

We provide our proof in E.1 by two hard-to-learn constructions. By the two instances, we show that the two factors, \(\min_{\pi,h,s:p_{\pi,h}(s)>0}w_{\pi,h}(s)\), \(\min_{\pi,d}\omega_{dim,\pi}(d)\), are unavoidable in some cases.

**Theorem 4.4** (**Static object regret lower bound)**.: _The static quantile risk aware object regret of RA-PBRL is bounded by:_

\[\mathrm{Regret}(K)\geq\mathcal{O}(S^{2}A+dim_{\mathbb{T}})\sqrt{K}\]

The proof of this theorem is similar to Theorem 4.5 in Chen et al. [2022].

## 5 Experiment Results

In this section, we assess the empirical performance of RA-PbRL (Algorithm 1). For a comparative analysis, we select two baseline algorithms: PbOP, as described in Chen et al. [2022], which is a PbRL algorithm utilizing general function approximation, and ICVaR-RLHF, detailed in Chen et al. [2023], which is a risk-sensitive Human Feedback RL algorithm. These baselines represent the most closely aligned algorithms with RA-PbRL, especially in terms of employing general function approximation. The evaluation of empirical performance is conducted through the lens of static regret, as defined in Eq. 8.

### Experiment settings: MDP

In our experimental framework, we configure a straightforward tabular MDP characterized by finite steps \(H=6\), finite actions \(A=3\), state space \(S=4\), and risk levels \(\alpha\in\{0.05,0.10,0.20,0.40\}\). For each configuration and algorithms, we perform 50 independent trials and report the mean regret across these trials, along with 95% confidence intervals. The outcomes are depicted in Figures 1 and 3, where the solid lines represent the empirical means obtained from the experiments, and the width of the shaded regions indicates the standard deviation of the experiments.

[MISSING_PAGE_EMPTY:9]

risk aversion (\(\alpha\to 0\)) introduces greater uncertainty, as evidenced by the larger variance regions observed in the experiments. Notably, the regret of the bad-scenarios associated with RA-PbRL is significantly lower compared to those of other algorithms. It can additionally be observed that as \(\alpha\) is increased, the regret improves, which is expected as riskier behavior can also improve the odds of finding useful behaviors.

## 6 Conclusion

In this paper, we investigate a novel PbRL algorithm for solving problems requiring risk awareness. We explore static and nested measures to introduce risk awareness to PbRL settings. To the best of our knowledge, our proposed RA-PbRL algorithm is the first provably efficient Preference-based Reinforcement Learning (PbRL) algorithm that incorporates both nested and static risk objectives in one algorithm. Our algorithm is built on innovative techniques for the efficient approximation of regret. A core finding in our investigation is the strong influence of the state and trajectory dimensions with respect to the nested risk objective regret. On the other hand, the static risk objective regret is mainly determined by the quantile function.

We have also identified the following four limitations to our work. (1) Our comparison feedback is limited to two trajectories. An interesting, more general approach could consider n-wise comparisons. (2) The reward functions are assumed to be linear in this work for the sake of simplicity. (3) Although this work has considered more general risk measures, we have still made certain assumptions that limit the generality of our results. (4) There is still room for future improvements to further close the gap between upper and lower bounds. We believe this work opens several avenues for future research, including establishing the concrete lower bounds of risk-aware PbRL, improving the computational complexity of the algorithm, and conducting experiments in more diverse and interesting environments/simulations.

## References

* Ayoub et al. (2018) Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_, pages

Figure 3: Cumulative regret for static CVaR in the MuJoCo setting over different \(\alpha\).

463-474. PMLR, 2020.
* Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* Bastani et al. (2022) Osbert Bastani, Jason Yecheng Ma, Estelle Shen, and Wanqiao Xu. Regret bounds for risk-sensitive reinforcement learning. _Advances in Neural Information Processing Systems_, 35:36259-36269, 2022.
* Basu et al. (2017) Chandrayee Basu, Qian Yang, David Hungerman, Mukesh Singhal, and Anca D Dragan. Do you want your autonomous car to drive like you? In _Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction_, pages 417-425, 2017.
* Bauerle and Ott (2011) Nicole Bauerle and Jonathan Ott. Markov decision processes with average-value-at-risk criteria. _Mathematical Methods of Operations Research_, 74:361-379, 2011.
* Bellemare et al. (2017) Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement learning. In _International conference on machine learning_, pages 449-458. PMLR, 2017.
* Busa-Fekete et al. (2014) Robert Busa-Fekete, Balazs Szorenyi, Paul Weng, Weiwei Cheng, and Eyke Hullermeier. Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm. _Machine learning_, 97:327-351, 2014.
* Chen et al. (2022) Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In _International Conference on Machine Learning_, pages 3773-3793. PMLR, 2022.
* Chen et al. (2023) Yu Chen, Yihan Du, Pibe Hu, Siwei Wang, Desheng Wu, and Longbo Huang. Provably efficient iterated cvar reinforcement learning with function approximation. _arXiv preprint arXiv:2307.02842_, 2023.
* Coronato et al. (2020) Antonio Coronato, Muddasar Naeem, Giuseppe De Pietro, and Giovanni Paragliola. Reinforcement learning for intelligent healthcare applications: A survey. _Artificial Intelligence in Medicine_, 109:101964, 2020.
* Dabney et al. (2018) Will Dabney, Mark Rowland, Marc Bellemare, and Remi Munos. Distributional reinforcement learning with quantile regression. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* Dann et al. (2017) Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning. _Advances in Neural Information Processing Systems_, 30, 2017.
* Du et al. (2022) Yihan Du, Siwei Wang, and Longbo Huang. Risk-sensitive reinforcement learning: Iterated cvar and the worst path. _arXiv preprint arXiv:2206.02678_, 2022.
* Eriksson and Dimitrakakis (2019) Hannes Eriksson and Christos Dimitrakakis. Epistemic risk-sensitive reinforcement learning. _arXiv preprint arXiv:1906.06273_, 2019.
* Fei et al. (2020) Yingjie Fei, Zhuoran Yang, Yudong Chen, Zhaoran Wang, and Qiaomin Xie. Risk-sensitive reinforcement learning: Near-optimal risk-sample tradeoff in regret. _Advances in Neural Information Processing Systems_, 33:22384-22395, 2020.
* Givan et al. (2003) Robert Givan, Thomas Dean, and Matthew Greig. Equivalence notions and model minimization in markov decision processes. _Artificial Intelligence_, 147(1-2):163-223, 2003.
* Hau et al. (2023) Jia Lin Hau, Marek Petrik, and Mohammad Ghavamzadeh. Entropic risk optimization in discounted mdps. In _International Conference on Artificial Intelligence and Statistics_, pages 47-76. PMLR, 2023.
* Hoeffding (1994) Wassily Hoeffding. Probability inequalities for sums of bounded random variables. _The collected works of Wassily Hoeffding_, pages 409-426, 1994.
* Hau et al. (2018)David R Hunter. Mm algorithms for generalized bradley-terry models. _The annals of statistics_, 32(1):384-406, 2004.
* Jain et al. [2013] Ashesh Jain, Brian Wojcik, Thorsten Joachims, and Ashutosh Saxena. Learning trajectory preferences for manipulators via iterative improvement. _Advances in neural information processing systems_, 26, 2013.
* Kim and Oh [2022] Dohyeong Kim and Songhwai Oh. Trc: Trust region conditional value at risk for safe reinforcement learning. _IEEE Robotics and Automation Letters_, 7(2):2621-2628, 2022.
* Lowd and Davis [2010] Daniel Lowd and Jesse Davis. Learning markov network structure with decision trees. In _2010 IEEE International Conference on Data Mining_, pages 334-343. IEEE, 2010.
* OpenAI [2023] R OpenAI. Gpt-4 technical report. arxiv 2303.08774. _View in Article_, 2:13, 2023.
* Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.
* Pacchiano et al. [2021] Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee. Dueling rl: reinforcement learning with trajectory preferences. _arXiv preprint arXiv:2111.04850_, 2021.
* Russell and Norvig [2010] Stuart J Russell and Peter Norvig. _Artificial intelligence a modern approach_. London, 2010.
* Russo and Van Roy [2013] Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration. _Advances in Neural Information Processing Systems_, 26, 2013.
* Shen et al. [2014] Yun Shen, Michael J Tobia, Tobias Sommer, and Klaus Obermayer. Risk-sensitive reinforcement learning. _Neural computation_, 26(7):1298-1328, 2014.
* Silver et al. [2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. _nature_, 550(7676):354-359, 2017.
* Wirth et al. [2016] Christian Wirth, Johannes Furnkranz, and Gerhard Neumann. Model-free preference-based reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 30, 2016.
* Wirth et al. [2017] Christian Wirth, Riad Akrour, Gerhard Neumann, Johannes Furnkranz, et al. A survey of preference-based reinforcement learning methods. _Journal of Machine Learning Research_, 18(136):1-46, 2017.
* Wu and Sun [2023] Runzhe Wu and Wen Sun. Making rl with preference-based feedback efficient via randomization. _arXiv preprint arXiv:2310.14554_, 2023.
* Wu and Xu [2023] Zhengqi Wu and Renyuan Xu. Risk-sensitive markov decision process and learning under general utility functions. _arXiv preprint arXiv:2311.13589_, 2023.
* Xu et al. [2020a] Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski. Preference-based reinforcement learning with finite-time guarantees. _Advances in Neural Information Processing Systems_, 33:18784-18794, 2020a.
* Xu et al. [2020b] Yichong Xu, Ruosong Wang, Lin F. Yang, Aarti Singh, and Artur Dubrawski. Preference-based reinforcement learning with finite-time guarantees. _arXiv preprint arXiv:2006.08910_, 2020b.
* Zanette and Brunskill [2019] Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In _International Conference on Machine Learning_, pages 7304-7312. PMLR, 2019.
* Zhan et al. [2023] Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D. Lee, and Wen Sun. Provable offline preference-based reinforcement learning. _arXiv preprint arXiv:2305.14816_, 2023.

## Appendix A Notation

We clarify the notations that appear uniquely in this paper to avoid confusion.

\begin{tabular}{|p{113.8pt}|p{113.8pt}|} \hline variable with \({}^{\star}\) & Ground truth of the variable. \\ \hline \(\mathcal{S},\ \mathcal{A}\) & Finite state space, action space. \\ \hline \(s,\ a\) & State, action. \\ \hline \(S,\ A\) & Dimension of state space, action space. \\ \hline \(\mathbf{P}\) & Probability transition kernel. \\ \hline \(H\) & Length of episode. \\ \hline \(\mathcal{Z}_{h}\) & Set of all h-step trajectories. \\ \hline \(\xi_{h}\) & A trajectory contains \(h\) steps. \\ \hline \(S_{h}(\xi)\) & State of \(h\)-th step of a trajectory. \\ \hline \(\Xi_{h}(\xi_{h^{\prime}})\) & \(1\sim h\) steps trajectory of a \(h^{\prime}\)-step trajectory. \\ \hline \(r_{h}(s_{h},a_{h})\) & Reward function of a single step \(h\). \\ \hline \(r_{\xi}(\xi_{H})\) & Reward function of a whole trajectory. \\ \hline \(r_{1:h}\) & Reward of the \(1\sim h\) steps of a trajectory. \\ \hline \(V_{h}^{\pi}(s)\) & Value function of state \(s\) at step \(h\) under policy \(\pi\) in normal MDP. \\ \hline \(V_{h}^{\pi}(s)\) & Value function of state \(s\) at step \(h\) under policy \(\pi\) in PbRL-MDP. \\ \hline \(\Phi(X)\) & Riemann-Stieljes integral of over a random variable \(X\), i.e. the risk-aware objective. \\ \hline \(N\left(\mathcal{F}_{\mathcal{R}},\frac{1}{K},\|\cdot\|_{\infty}\right)\) & Covering number of function class \(\mathcal{F}_{\mathcal{R}}\) at scale \(1/K\) under \(\|\cdot\|_{\infty}\) norm. \\ \hline \end{tabular}

## Appendix B Risk Aware Object Computability

Unlike standard RL where each step's reward can be observed, PbRL represents a type of RL characterized by once-per-episode feedback. As a result, our observable and estimable parameters are confined to the trajectory reward \(r_{\xi}^{\star}\) and transition probability functions \(\mathbf{P}^{\star}\). Consequently, the traditional risk-aware objective might be unsuitable, as the reduction in available information prevents the computation of the original risk-aware measure. The risk measure selected for PbRL must satisfy the following condition: it should remain unique across MDPs where policies, trajectory rewards and transition probability functions even when each trajectory is fixed, but different step rewards, \(r^{\star}(s,a)\), vary. This requirement ensures that the risk measure consistently reflects the underlying preferences regardless of variations in specific step rewards.

### Nested Object

**Theorem B.1**.: _For the tabular MDP and the reward of the entire trajectory can be decomposed as \(r_{\xi}^{\star}\left(\xi_{H}\right)=\sum_{h=1}^{H}r_{h}^{\star}\left(s_{h},a_{h}\right)\), \(V_{1}^{\pi}\) in Eq. 5 and \(\tilde{V}_{1}^{\pi}\) in Eq.6 are equivalent._

Proof.: Firstly, according to Givan et al. (2003), Lowd and Davis (2010), any tabular MDP can be reformulated as a decision tree-like MDP. Thus, considering a tree-like structure for an MDP implies the following characteristics:

1. The state transition graph of the MDP is connected and acyclic. 2. Each state in the MDP corresponds to a unique node in the tree. 3. There is a single root node from which every other node is reachable via a unique path. 4. The transition probabilities between states follow the Markov property, i.e., the probability of transitioning to any future state depends only on the current state and not on the sequence of events that preceded it.

Formally, let \(S\) be the set of states and \(p_{ij}\) the transition probabilities between states \(s_{i}\) and \(s_{j}\). The transition matrix \(P\) for an MDP with a tree-like structure is defined such that:

\[p_{ij}>0\text{ if there is an edge between }s_{i}\text{ and }s_{j}\text{ in the tree, and }p_{ij}=0\text{ otherwise.}\]

Moreover, for each non-root node \(s_{j}\), there exists exactly one \(s_{i}\) such that \(p_{ij}>0\), and \(s_{i}\) is the unique parent of \(s_{j}\) in the tree structure.

To classify the two value iteration in Eq. 3 and Eq. 4, we denote the value given by Eq. 4 as \(\tilde{V}_{h}^{\pi}\) and the value given by Eq. 3 as \(V_{h}^{\pi}\), thus, in tabular tree-like MDP with the reward of the entire trajectorywhich can be decomposed as \(r_{\xi}^{\star}\left(\xi_{H}\right)=\sum_{h=1}^{H}r_{h}^{\star}\left(s_{h},a_{h}\right)\), we have the following relationship:

\[\tilde{V}_{h}^{\pi}=V_{h}^{\pi}+r_{1:h-1}^{\star}\]

where \(r_{1:h}\) denotes Reward of the \(1\sim h\) steps of a trajectory. We prove this relationship by mathematical induction.

**Initial case.** Using the tree-like PbRL-MDP algorithm and the initial conditions of the Bellman equation, at the final step \(h=H\), we have

\[\tilde{V}_{H}^{\pi} =r_{H}^{\star}\left(s_{H}^{\prime},\pi(\xi_{H-1})\right)+r_{1:H-1}^ {\star}\] (12) \[=V_{H}^{\pi}+r_{1:H-1}^{\star}\] (13)

**Induction step.** We now proved that if

\[\tilde{V}_{h+1}^{\pi}=V_{h+1}^{\pi}+r_{1:h}^{\star}\]

holds, then

\[\tilde{V}_{h}^{\pi}=V_{h}^{\pi}+r_{1:h-1}^{\star}\]

also holds.

Since this tree-like MDP's policy \(\pi\) is fixed, it has only one path to arrive h th state (\(s_{h}\)), denoted as:

\[\Xi_{h}(\xi_{H,1})=\Xi_{h}(\xi_{H,2})\quad\forall\xi_{H,1},\xi_{H,2}\in\left\{ \xi_{H}\mid S_{h}(\xi_{H})=s_{h}\right\}\] (14)

Therefore, \(r_{1:h-1}^{\star}\) is unique.

\[\tilde{V}_{h}^{\pi} =\Phi\left(V_{h+1}^{\pi}(s_{h+1}^{\prime})+r_{1:h}^{\star} \right),\quad s_{h+1}^{\prime}\sim\mathbf{P}^{\star}(s,a)\] (15) \[=\Phi\left(V_{h+1}^{\pi}(s_{h+1}^{\prime})+r_{h}^{\star}(s_{h}, \pi_{h}(\xi_{h}))+r_{1:h-1}^{\star}\right),\quad s_{h+1}^{\prime}\sim\mathbf{ P}^{\star}(s,a)\] (16) \[=\Phi\left(V_{h+1}^{\pi}(s_{h+1}^{\prime})+r_{h}^{\star}(s_{h}, \pi_{h}(\xi_{h}))\right)+r_{1:h-1}^{\star},\quad s_{h+1}^{\prime}\sim\mathbf{ P}^{\star}(s,a)\] (17) \[=V_{h}^{\pi}+r_{1:h-1}^{\star}\] (18)

By applying conclusion, we observe that when \(h=1\)

\[\tilde{V}_{1}^{\pi}=V_{1}^{\pi}.\]

Thus, we have proven that the for the tabular MDP and the reward of the entire trajectory can be decomposed as \(r_{\xi}^{\star}\left(\xi_{H}\right)=\sum_{h=1}^{H}r_{h}^{\star}\left(s_{h},a_{ h}\right)\), \(V_{1}^{\pi}\) in Eq. 3 and Eq. 4 are equivalent.

### Static Object

**Lemma B.2**.: _For the tabular MDP and the reward of the entire trajectory can be decomposed as \(r_{\xi}^{\star}\left(\xi_{H}\right)=\sum_{h=1}^{H}r_{h}^{\star}\left(s_{h},a_{ h}\right)\), \(V_{1}^{\pi}\) in Eq. 5 and \(\tilde{V}_{1}^{\pi}\) in Eq.6 are equivalent._

Proof.: To classify the two value iteration in Eq. 5 and Eq. 6, we denote the value given by Eq. 5 as \(\tilde{V}_{h}^{\pi}\) and the value given by Eq. 6 as \(V_{h}^{\pi}\), thus, in tabular tree-like MDP with the reward of the entire trajectory which can be decomposed as \(r_{\xi}^{\star}\left(\xi_{H}\right)=\sum_{h=1}^{H}r_{h}^{\star}\left(s_{h},a_{ h}\right)\), we have the following relationship:

\[\tilde{V}_{h}^{\pi}=V_{h}^{\pi}+r_{1:h-1}^{\star}\]

where \(r_{1:h}\) denotes Reward of the \(1\sim h\) steps of a trajectory.

Now, We prove this relationship.

Since this tree-like MDP's policy \(\pi\) is fixed, it has only one path to arrive h th state (\(s_{h}\)), denoted as:

\[\Xi_{h}(\xi_{H,1})=\Xi_{h}(\xi_{H,2})\quad\forall\xi_{H,1},\xi_{H,2}\in\left\{ \xi_{H}\mid S_{h}(\xi_{H})=s_{h}\right\}\] (19)Therefore, \(r_{1:h-1}^{\star}\) is unique. By definition,

\[\tilde{V}_{h}^{\pi} =r_{\xi}^{\star}(\xi_{H})\quad\xi_{H}\sim\mathbb{P}\left(\cdot\mid \Xi_{h}(\xi_{H})=\xi_{h}\right)\] (20) \[=r_{\xi}^{\star}(\Xi_{h}(\xi_{H}))+r_{1:h-1}^{\star},\quad\xi_{H} \sim\mathbb{P}\left(\cdot\mid\Xi_{h}(\xi_{H})=\xi_{h}\right)\] (21) \[=V_{h}^{\pi}+r_{1:h-1}^{\star}\] (22)

By applying conclusion, we observe that when \(h=1\)

\[\tilde{V}_{1}^{\pi}=V_{1}^{\pi}.\]

Thus, we have proven that the for the tabular MDP and the reward of the entire trajectory can be decomposed as \(r_{\xi}^{\star}\left(\xi_{H}\right)=\sum_{h=1}^{H}r_{h}^{\star}\left(s_{h},a_{ h}\right)\), \(V_{1}^{\pi}\) in Eq. 5 and \(\tilde{V}_{1}^{\pi}\) in Eq.6 are equivalent.

## Appendix C Difference between nested and static risk measure

To explain the difference between nested and static risk measure, we present a simple example that demonstrates their characters.

First, we construct a MDP instance in fig. 4 where two policies exhibit identical reward distributions and consequently demonstrate equivalent preference will be observed. However, within this instance, these policies yield different outcomes under the nested and static CVaR metrics.

The state space is \(\mathcal{S}=\{S_{1},S_{2,1},S_{2,2},S_{3,1},S_{3,2},S_{3,3},S_{3,4}\}\), where \(S_{1}\) is the initial state.

The policy space is \(\Pi=\{\pi_{A},\pi_{B}\}\). Both policy have have the same action in the first step \((a_{1})\) and second step \((a_{2})\), but have the different action in the third step \((a_{3},a_{3}^{\prime})\).

The reward functions are as follows. \(r(S_{1},a_{1})=0\), \(r(S_{2,1},a_{2})=0.1\), \(r(S_{2,2},a_{2})=0.2\), \(r(S_{3,1},a_{3})=0.3\), \(r(S_{3,2},a_{3})=0.2\), \(r(S_{3,3},a_{3})=0.4\), \(r(S_{3,2},a_{3})=0.5\), \(r(S_{3,1},a_{3}^{\prime})=0.6\), \(r(S_{3,2},a_{3}^{\prime})=0.2\), \(r(S_{3,3},a_{3}^{\prime})=0.4\), \(r(S_{3,2},a_{3}^{\prime})=0.2\).

The transition distributions are as follows. \(P\left(S_{2,1}\mid S_{1},a_{1}\right)=0.5\), \(P\left(S_{2,2}\mid S_{1},a_{1}\right)=0.5\), \(P\left(S_{3,1}\mid S_{2,1},a_{2}\right)=0.1\), \(P\left(S_{3,2}\mid S_{2,1},a_{2}\right)=0.9\), \(P\left(S_{3,3}\mid S_{2,2},a_{2}\right)=0.1\), \(P\left(S_{3,4}\mid S_{2,2},a_{2}\right)=0.9\).

As depicted on the right side of the figure, the distribution of rewards is consistent. Consequently, the human feedback preferences for the two policies are identical. We list the differing risk measures for these two policies in Table C.

## Appendix D Regret Upper Bound for Algorithm 1

In this section, we present the full proof of Assumption 1's regret upper bound. The proof consists of parts.

Figure 4: Cumulative regret for the different \(\alpha\)

### Reward estimation error

**Lemma D.1**.: _Reward confidence set construction. Fix \(\delta\in(0,1)\), with probability at least \(1-\delta\), for all \(k\in[K]\),_

\[\sum_{k^{\prime}=1}^{k-1}[\sigma(\widehat{r}_{k}(\xi_{1})-\widehat{r}_{k}(\xi_{ 2}))-\sigma(r^{*}(\xi_{1})-r^{*}(\xi_{2}))]^{2}\leq\beta_{r,k}\] (23)

_where_

\[\beta_{r,k}\left(\delta,\frac{1}{K}\right)\leq\mathcal{O}\left(dim_{\mathbb{ T}}log\left(K\left(1+2B\rho_{w}\right)\right)+log(\frac{1}{\delta})\right)\] (24)

Proof.: This lemma can be proved by the direct application of Lemma G.5. Let \(X_{t}=(\xi_{t,1},\xi_{t,2})\) and \(Y_{t}=o_{t}\), and \(\mathcal{F}_{r,t}=\{f_{r}|f_{r}(\cdot,\cdot)=\sigma(r(\cdot)-r(\cdot))\}\). Then, we have that \(X_{t}\) is \(\mathcal{F}_{t-1}\) measurable and \(Y_{t}\) is \(\mathcal{F}_{t}\) measurable. According to Hoeffding's inequality (Theorem G.8), \(\{Y_{t}-f_{r}\left(X_{t}\right)\}\) is \(\frac{1}{2}\)-sub-gaussian conditioning, and \(\mathbb{E}\left[Y_{t}-f_{r}\left(X_{t}\right)\mid\mathcal{F}_{t-1}\right]=0\). By Lemma G.5 and G.4, since the linear trajectory embedding function \(\phi:\mathcal{Z}_{H}\rightarrow\mathbb{R}^{dim_{\mathbb{T}}}\), with probability at least \(1-\delta\), we have

\[\beta_{r,k}\left(\delta,\frac{1}{K}\right)\leq\mathcal{O}\left(dim_{\mathbb{ T}}log\left(K\left(1+2B\rho_{w}\right)\right)+log(\frac{1}{\delta})\right)\] (25)

**Lemma D.2**.: _Reward estimation error of trajectory embedding. For any \(k\in[0,\ldots,K]\), reward confidence set \(\mathcal{B}_{k}^{r}\), where the reward function embedding weight can be noted as \(\mathbf{w}_{r}=(w_{1},\ldots,w_{dim_{\mathbb{T}}})\), any fixed trajectory \(\xi_{H}\in\mathcal{Z}_{H}\), trajectory embedding \(\phi(\xi_{H})=(\phi_{1},\ldots,\phi_{dim_{\mathbb{T}}})\), with probability at least \(1-\delta\), it holds that,_

\[max_{r_{1},r_{2}\in\mathcal{B}_{k}^{r}}\left|(w_{r_{1},d}-w_{r_{2},d})\right| \leq\mathcal{O}\left(\frac{1}{\kappa b}\sqrt{\frac{dim_{\mathbb{T}}log\left( K\left(1+2B\rho_{w}\right)\right)+log(\frac{1}{\delta})}{n_{dim,K}(d)}}\right)\] (26)

_where \(n_{\xi,k}\) denotes the number of times \(\xi_{H}\) was visited up to episode k._

Proof.: According to Lemma D.1 and the assumption of link function, for fixed \(k\in[0,\ldots,K]\), and \(d\in[0,\ldots,dim_{\mathbb{T}}]\), we have,

\[max_{r_{1},r_{2}\in\mathcal{B}_{K}^{r}}\sum_{k^{\prime}=1}^{k} \left|(w_{r_{1},d}-w_{r_{2},d})\right|^{2}b^{2}\mathbb{I}(B\neq 0)\] (27) \[\leq \sum_{k=1}^{K}\left|((w_{r_{1},d}-w_{r^{*},d})\cdot B)\right|^{2}\] (28) \[\leq \frac{\beta_{r,K}}{\kappa^{2}}\] (29)

Where \(n_{dim,k}(d)\) denotes the number of \(\phi_{d}(\xi_{H})\neq 0\) among \(1\sim k\) episode's trajectory.

Using Cauchy-Schwarz inequality and Lemma G.6, we have,

\[\max_{r_{1},r_{2}\in\mathcal{B}_{k}^{r}}\left|(w_{r_{1},d}-w_{r_{2},d})\right| \leq\mathcal{O}\left(\frac{1}{\kappa b}\sqrt{\frac{dim_{\mathbb{T}}log\left( K\left(1+2B\rho_{w}\right)\right)+log(\frac{1}{\delta})}{n_{dim,K}(d)}}\right)\] (30)

**Lemma D.3**.: _Reward estimation error of the whole distribution. For \(b_{k}^{r}(\xi_{1},\xi_{2})=\max_{r_{1},r_{2}\in\mathcal{B}_{k}^{r}}[(r_{1}(\xi_ {1})-r_{1}(\xi_{2}))-(r_{2}(\xi_{1})-r_{2}(\xi_{2}))]\),_

\[\sum_{k=1}^{K}b_{k}^{r}(\xi_{H,1,k},\xi_{H,2,k})\leq\mathcal{O}(dim_{\mathbb{T}} \sqrt{K\log(KB\rho_{w})\log\left(\frac{K\left(1+2B\rho_{w}\right)}{\delta} \right)})\] (31)Proof.: Let \(\mathcal{F}_{k}=\{f_{r}|f_{r}(x,y)=\sigma(r(x)-r(y)),r\in\mathcal{B}_{k}^{r}\}\), then we define \(diam(\mathcal{F}_{\mathcal{B}_{k}^{r}})=b_{k}^{\sigma}(x,y)=\max_{r_{1},r_{2} \in\mathcal{B}_{k}^{r}}\sigma(r_{1}(x)-r_{1}(y))-\sigma(r_{2}(x)-r_{2}(y))\). According to Lemma. D.1, \(\delta_{k}=\max_{1\leq k\leq K}\operatorname{diam}\big{(}\left.\mathcal{F}_{k }\right|_{x_{1:k}}\big{)}\). Let \(\alpha=1/K\), \(T=k\), and \(d=\dim_{\mathcal{E}}(\mathcal{F}_{\mathcal{R}},1/K)\) According to lemma G.6,

\[\sum_{k=1}^{K}b_{k}^{r}(\xi_{H,1,k},\xi_{H,2,k})\leq\mathcal{O}(dim_{\mathbb{ T}}\sqrt{K\log(KB\rho_{w})\log\left(\frac{K\left(1+2B\rho_{w}\right)}{\delta} \right)})\] (32)

**Lemma D.4**.: _Transition estimation error of fixed state action pair.For any fixed \(k\), with probability at least \(1-2\delta\), for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), executed policy \(\pi_{1},\pi_{2}\in\Pi_{k}\)and any transition possibility kernel \(\mathbf{P}_{1},\mathbf{P}_{2}\in\mathcal{B}_{k}^{r}\)_

\[\sum_{i=1}^{2}\sum_{k=1}^{K}\max_{\mathbf{P}_{1},\mathbf{P}_{2} \in\mathcal{B}_{k}^{r}}E_{\xi_{i}\sim\pi_{i}}(\sum_{h=1}^{H}|\mathbf{P}_{1} \left(s^{\prime}\mid s_{i,k,h},a_{i,k,h}\right)-\mathbf{P}_{2}\left(s^{\prime }\mid s_{i,k,h},a_{i,k,h}\right)|)\] (33) \[\leq \mathcal{O}\left(S^{2}AH^{3/2}\sqrt{K\log(K)log\left(K/\delta \right)}\right)\] (34)

Proof.: The proof is same as Lemma. A.5 in Chen et al. [2022]. 

**Lemma D.6**.: _For any iteration value \(V:S\to R\), any two transition possibility kernel \(\mathbf{P},\hat{\mathbf{P}}:S\times S\times A\rightarrow\), and the risk aware object form:_

\[\Phi(V(s^{\prime}))=\int_{0}^{1}F_{V(s^{\prime})}^{\dagger}(\xi)\cdot dG(\xi) \quad s^{\prime}\sim\mathbf{P}(s,a)\] (35)

\[\left|\Phi_{s^{\prime}\sim\hat{\mathbf{P}}(\cdot\mid s,a)}\left(V \left(s^{\prime}\right)\right)-\Phi_{s^{\prime}\sim\mathbf{P}(\cdot\mid s,a)} \left(V\left(s^{\prime}\right)\right)\right|\] \[\leq L_{G}H\sum_{s^{\prime}\in\mathcal{S}}\left|\hat{\mathbf{P}} \left(s^{\prime}\mid s,a\right)-\mathbf{P}\left(s^{\prime}\mid s,a\right)\right|\] (36)

Proof.: We firstly sort all successor states \(s^{\prime}\in\mathcal{S}\) by \(V\left(s^{\prime}\right)\) in ascending order (from the left to the right) as \(s_{1}^{\prime},s_{2}^{\prime}\ldots s_{S}^{\prime}\). And we assume that \(V(s_{S+1}^{\prime}=1)\). Thus, according to the quantile function's definition,

\[\left|\Phi_{s^{\prime}\sim\hat{\mathbf{P}}(\cdot\mid s,a)}\left(V \left(s^{\prime}\right)\right)-\Phi_{s^{\prime}\sim\mathbf{P}(\cdot\mid s,a)} \left(V\left(s^{\prime}\right)\right)\right|\] (37) \[=|\int_{0}^{1}F_{V^{\prime}\mathbf{P}(s^{\prime})}^{\dagger}(\xi) \cdot dG(\xi)-\int_{0}^{1}F_{V^{\prime}\mathbf{P}(s^{\prime})}^{\dagger}(\xi) \cdot dG(\xi)|\] (38) \[=|\int_{0}^{1}G(F_{V^{\prime}\mathbf{P}(s^{\prime})}(\xi))\cdot d \xi-\int_{0}^{1}G(F_{V^{\prime}\mathbf{P}(s^{\prime})}(\xi))\cdot d\xi|\] (39) \[=\sum_{i=1}^{S}|V(s_{i+1}^{\prime})-V(s_{i}^{\prime})|\cdot|G(\sum _{j=1}^{i}\mathbf{P}(s_{j}^{\prime}|(s,a)))-G(\sum_{j=1}^{i}\hat{\mathbf{P}}(s _{j}^{\prime}|(s,a)))|\] (40) \[\leq\sum_{i=1}^{S}(V(s_{i+1}^{\prime})-V(s_{i}^{\prime}))\cdot L_{ G}\sum_{j=1}^{i}|\mathbf{P}(s_{j}^{\prime}|(s,a))-\hat{\mathbf{P}}(s_{j}^{\prime}|(s,a ))|\] (41)\[\leq \sum_{i=1}^{S}\left|\mathbf{P}(s^{\prime}_{j}|(s,a))-\hat{\mathbf{P} }(s^{\prime}_{j}|(s,a))\right|\] (42) \[\leq L_{G}\sum_{j=1}^{S}|\mathbf{P}(s^{\prime}_{j}|(s,a))-\hat{ \mathbf{P}}(s^{\prime}_{j}|(s,a))|\sum_{i=1}^{S}(V(s^{\prime}_{i+1})-V(s^{ \prime}_{i}))\] (43) \[\leq L_{G}\cdot H\sum_{j=1}^{S}|\mathbf{P}(s^{\prime}_{j}|(s,a))- \hat{\mathbf{P}}(s^{\prime}_{j}|(s,a))|\] (44)

The second to fourth lines follow the discussion on the equivalent expression of the discontinuous distribution \(\Phi\) in Lemma 5.1 of Bastani et al. (2022). The fifth line is derived from the properties of Lipschitz functions and Assumption 3.5.

Since we use the emperical estimation \(\hat{\mathbf{P}}_{k}\) of the transaction kernel \(\mathbf{P}^{\star}\),

\[\hat{\mathbf{P}}_{k}=argmin_{\mathbf{P}\in\mathcal{P}}\sum_{i=1}^{2}\sum_{k^{ \prime}=1}^{k-1}\sum_{h=1}^{H}|\langle\mathbf{P}(s_{i,k^{\prime},h},a_{i,k^{ \prime},h}),\mathbb{I}(s_{i,k^{\prime},h+1})\rangle|^{2}.\] (45)

**Lemma D.7**.: _Concentration for V. With probability at least \(1-2\delta\), it holds that, for any \(k\in[K]\), \((s,a)\in\mathcal{S}\times\mathcal{A}\), any transition possibility kernel \(\mathbf{P}_{1}\in\mathcal{B}_{k}^{\mathbf{P}}\) and function \(V:\mathcal{S}\mapsto[0,H]\),_

\[\left|\Phi_{s^{\prime}\sim\mathbf{P}_{1}(\cdot|s,a)}\left(V\left(s^{\prime} \right)\right)-\Phi_{s^{\prime}\sim\mathbf{P}^{\star}(\cdot|s,a)}\left(V \left(s^{\prime}\right)\right)\right|\leq 2L_{G}\cdot H\sqrt{\frac{2S\log\left( \frac{2KHSA}{\delta}\right)}{n_{k}(s,a)}}\] (46)

_Here \(n_{k}(s,a)\) is the number of times \((s,a)\) was visited up to episode \(k\)._

Proof.: According to Lemma D.4 and recall the definition of the transition possibility confidence set, with probability at least \(1-2\delta\),

\[\sum_{s^{\prime}\in\mathcal{S}}|\mathbf{P}^{\star}\left(s^{\prime }\mid s,a\right)-\mathbf{P}_{1}\left(s^{\prime}\mid s,a\right)|\] (47) \[\leq \sum_{s^{\prime}\in\mathcal{S}}\left|\mathbf{P}^{\star}\left(s^{ \prime}\mid s,a\right)-\hat{\mathbf{P}}_{k}\left(s^{\prime}\mid s,a\right)+ \hat{\mathbf{P}}_{k}\left(s^{\prime}\mid s,a\right)-\mathbf{P}_{1}\left(s^{ \prime}\mid s,a\right)\right|\] (48) \[\leq \sum_{s^{\prime}\in\mathcal{S}}\left|\hat{\mathbf{P}}_{k}\left(s^ {\prime}\mid s,a\right)-\mathbf{P}^{\star}\left(s^{\prime}\mid s,a\right) \right|+\sum_{s^{\prime}\in\mathcal{S}}\left|\hat{\mathbf{P}}_{k}\left(s^{ \prime}\mid s,a\right)-\mathbf{P}_{1}\left(s^{\prime}\mid s,a\right)\right|\] (50) \[\leq 2\sqrt{\frac{2S\log\left(\frac{2KHSA}{\delta}\right)}{n_{k}(s,a)}}\] (51)

Plugging Lemma D.6, we obtain that with probability at least \(1-2\delta\), for any \(k\in[K]\), \((s,a)\in\mathcal{S}\times\mathcal{A}\) and function \(V:\mathcal{S}\mapsto[0,H]\),

\[\left|\Phi_{s^{\prime}\sim\mathbf{P}^{k}(\cdot|s,a)}\left(V\left(s^{\prime} \right)\right)-\Phi_{s^{\prime}\sim\mathbf{P}^{\star}(\cdot|s,a)}\left(V\left(s ^{\prime}\right)\right)\right|\leq 2L_{G}\cdot H\sqrt{\frac{2S\log\left(\frac{2KHSA}{ \delta}\right)}{n_{k}(s,a)}}\] (52)Recall the definition of quantile function \(\Phi\), \(G\) is the quantile CDF weight. Given any target value \(V:S\to R\), we use \(\beta_{\mathbf{P}^{*}}^{G,V}\left(s^{\prime}\mid s,a\right)\) denotes the conditional probability of transitioning to \(s^{\prime}\) from \((s,a)\), conditioning on transitioning to the quantile distribution, and it holds that

\[\beta_{\mathbf{P}^{*}}^{G,V}\left(s^{\prime}_{i}\mid s,a\right)=G(\sum_{j=1}^{ i+1}\mathbf{P}^{*}(s^{\prime}_{j}|(s,a)))-G(\sum_{j=1}^{i}\mathbf{P}^{*}(s^{ \prime}_{j}|(s,a)))\] (53)

**Lemma D.8**.: _Quantile Reward Gap due to Value Function Shift. For any \((s,a)\in\mathcal{S}\times\mathcal{A}\) distribution \(\mathbf{P}\), and functions \(V,V^{\prime}:\mathcal{S}\mapsto[0,H]\), for any \(s^{\prime}\in\mathcal{S}\),_

\[\Phi_{s^{\prime}\sim\mathbf{P}(\cdot\mid s,a)}\left(V^{\prime}\left(s^{\prime }\right)\right)-\Phi_{s^{\prime}\sim\mathbf{P}(\cdot\mid s,a)}\left(V\left(s^ {\prime}\right)\right)\leq\beta_{\mathbf{P}}^{G,V}(\cdot\mid s,a)^{\top}\left| V^{\prime}-V\right|\] (54)

This Lemma's proof is similar to Lemma 11 in Du et al. (2022).

**Lemma D.9**.: _For any \((s,a)\in\mathcal{S}\times\mathcal{A}\), distribution \(\mathbf{P}\), and functions \(V,V^{\prime}:\mathcal{S}\mapsto[0,H]\), for any \(s^{\prime}\in\mathcal{S}\),_

\[\Phi_{s^{\prime}\sim\mathbf{P}(\cdot\mid s,a)}\left(V^{\prime}\left(s^{\prime }\right)\right)-\Phi_{s^{\prime}\sim\mathbf{P}(\cdot\mid s,a)}\left(V\left(s^ {\prime}\right)\right)\leq L_{G}\mathbf{P}(\cdot\mid s,a)^{\top}\left|V^{\prime }-V\right|\] (55)

Proof.: This Lemma comes from Lemma.D.8 and

\[\beta^{G,V}\left(s^{\prime}_{i}\mid s,a\right) =G(\sum_{j=1}^{i+1}\mathbf{P}^{*}(s^{\prime}_{j}|(s,a)))-G(\sum_{ j=1}^{i}\mathbf{P}^{*}(s^{\prime}_{j}|(s,a)))\] (56) \[\leq L_{G}\mathbf{P}(\cdot\mid s,a)\] (57)

For any \(k>0,h\in[H]\) and \((s,a)\in\mathcal{S}\times\mathcal{A}\), let \(p_{kh}(s,a)\) denote the probability of visiting \((s,a)\) at step \(h\) of episode \(k\). Then, it holds that for any \(k>0,h\in[H]\) and \((s,a)\in\mathcal{S}\times\mathcal{A},p_{kh}(s,a)\in[0,1]\) and \(\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}p_{kh}(s,a)=1\)

**Lemma D.10**.: _(Concentration of state-action visitation). It holds that_

\[\Pr\left[n_{k}(s,a)\geq\frac{1}{2}\sum_{k^{\prime}=1}^{k-1}\sum_{h=1}^{H}p_{k^ {\prime}h}(s,a)-H\log\left(\frac{HSA}{\delta}\right),\forall k>0,\forall(s,a) \in\mathcal{S}\times\mathcal{A}\right]\geq 1-\delta\]

This Lemma is a direct application of Lemma G.9 and same as Du et al. (2022).

**Lemma D.11**.: _(Concentration of trajectory visitation). It holds that_

\[\Pr\left[n_{dim,k}(d)\geq\frac{1}{2}\sum_{k^{\prime}=1}^{k-1}p_{dim,k^{\prime} }(\xi_{H})-\log\left(\frac{dim_{\mathbf{T}}}{\delta}\right),\forall d\in[0, \ldots,dim_{\mathbb{T}}]\right]\geq 1-\delta\]

Proof.: This Lemma a direct application of G.9. For any dimension \(d\in[0,\ldots,dim_{\mathbb{T}}]\), it holds that

\[\Pr\left[n_{dim,k}(d)\geq\frac{1}{2}\sum_{k^{\prime}=1}^{k-1}p_{dim,k^{\prime} }(\xi_{H})-\log\left(\frac{1}{\delta}\right)\right]\geq 1-\delta\]

Since \(d\in[0,\ldots,dim_{\mathbb{T}}]\), Therefore,

\[\Pr\left[n_{dim,k}(d)\geq\frac{1}{2}\sum_{k^{\prime}=1}^{k-1}p_{dim,k^{ \prime}}(\xi_{H})-\log\left(\frac{dim_{\mathbf{T}}}{\delta}\right),d\in[0, \ldots,dim_{\mathbb{T}}]\right]\geq 1-\delta\]

**Definition of sufficient state-action visitations.**Following Zanette and Brunskill (2019), for any episode \(k>0\), we define the set of state-action pairs which have sufficient visitations in expectation as follows.

\[\mathcal{L}_{k}:=\left\{(s,a)\in\mathcal{S}\times\mathcal{A}:\frac{1}{4}\sum_{ k^{\prime}=1}^{k-1}\sum_{h=1}^{H}p_{k^{\prime}h}(s,a)\geq H\log\left(\frac{HSA}{\delta}\right)+H\right\}\]

**Definition of sufficient trajectory visitations**. Following Zanette and Brunskill (2019), for any episode \(k>0\), we define the set of trajrctory dimension which have sufficient visitations in expectation as follows.

\[\mathcal{L}_{dim,k}:=\left\{d\in[0,\ldots,dim_{\mathbb{T}}]:\frac{1}{4}\sum_{ k^{\prime}=1}^{k-1}p_{dim,k^{\prime}}(d)\geq\log\left(\frac{dim_{\mathbb{T}}}{ \delta}\right)+1\right\}\]

We use \(n_{dim,k}(d)\) to denote the number of \(\phi_{d}(\xi_{H})\neq 0\) among \(1\sim k\) episode's trajectory.

**Lemma D.12**.: _(**Standard state action visitation ratio).** For any \(K>0\), we have_

\[\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{L}_{k}}\frac{p_{kh}( s,a)}{n_{k}(s,a)}}\leq 2\sqrt{SA\log\left(\frac{KHSA}{\delta}\right)}.\]

This proof is the same as that of Lemma 13 in Zanette and Brunskill (2019).

**Lemma D.13**.: _(**Standard trajectory visitation ratio).** For any \(K>0\), we have_

\[\sqrt{\sum_{k=1}^{K}\sum_{d=1}^{dim_{\mathbb{T}}}\frac{p_{dim,k}(d)}{n_{dim,k} (d)}}\leq 2\sqrt{dim_{\mathbb{T}}\log\left(\frac{Kdim_{\mathbb{T}}}{ \delta}\right)}.\]

This proof is the same as that of Lemma 13 in Zanette and Brunskill (2019).

**Lemma D.14**.: _(**Standard Inviliation Ratio).** For any \(K>0\), we have_

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\sum_{(s,a)\notin\mathcal{L}_{k}}p_{kh}(s,a)<\frac {1}{\min_{\pi,h,s;p_{\pi,h}(s)>0}p_{\pi,h}(s)}\cdot\left(4H\log\left(\frac{ HSA}{\delta}\right)+5H\right)\] (58)

This proof is the same as that of Lemma 10 in Du et al. (2022).

**Lemma D.15**.: _(**Standard trajectory Invisitation Ratio).** For any \(K>0\), we have_

\[\sum_{k=1}^{K}\sum_{d\notin\mathcal{L}_{d,k}}p_{dim,k}(d)<\min_{\pi,d;p_{dim, \pi}(d)>0}p_{dim,\pi}(d)\cdot\left(4\log\left(\frac{dim_{\mathbb{T}}}{ \delta}\right)+5\right)\]

This proof is the same as that of Lemma 10 in Du et al. (2022).

**Lemma D.16**.: _For any functions \(V:\mathcal{S}\mapsto\mathbb{R},k>0,h\in[H]\) and \((s,a)\in\mathcal{S}\times\mathcal{A}\) such that \(p_{kh}(s,a)>0\),_

\[\frac{p_{kh}^{G,V}(s,a)}{p_{kh}(s,a)}\leq\frac{1}{\min_{\pi,h,(s,a):w_{\pi,h}( s,a)>0}w_{\pi,h}(s,a)},\]

_where \(p_{kh}^{G,V}(s,a)\) denotes the conditional probability of visiting \((s,a)\) at step \(h\) of episode \(k\), conditioning on transitioning distortion by the quantion function \(G\) which works on at each step \(h^{\prime}=1,\ldots,h-1\)._

Proof.: Since \(p_{kh}^{G,V}(s,a)\) is the conditional probability of visiting \((s,a)\), we have \(p_{kh}^{G_{a},V}(s,a)\in[0,1]\). Since \(p_{kh}(s,a)\) is the probability of visiting \((s,a)\) at step \(h\) under policy \(\pi^{k}\) and \(\min_{\pi,h,(s,a):w_{\pi,h}(s,a)>0}w_{\pi,h}(s,a)\) is the minimum probability of visiting any reachable \((s,a)\) at any step \(h\) over all policies \(\pi\), we have

\[p_{kh}(s,a)\geq\min_{\pi,h,(s,a):w_{\pi,h}(s,a)>0}w_{\pi,h}(s,a).\]Hence, we have

\[\frac{p_{kh}^{G,\alpha,V}(s,a)}{p_{kh}(s,a)}\leq\frac{1}{\min_{\pi,h,(s,a):w_{s,h} (s,a)>0}w_{\pi,h}(s,a)}.\]

**Lemma D.17**.: _For any functions \(V:\mathcal{S}\mapsto\mathbb{R},k>0,h\in[H]\),_

\[\frac{p_{dim,k}^{G,V}(d)}{p_{dim,k}(d)}\leq\frac{1}{\min p_{\pi}(d)},\]

_where \(p_{dim,k}^{G,V}(d)\) denotes the conditional probability of \(\phi_{d}(\xi_{H})\neq 0\) of episode \(k\), conditioning on transitioning distortion by the quantion function \(G\) which always works on at each step \(h=1,\ldots,H\)._

The proof is similar to Lemma.D.16.

### Proof of Algorithm 1's regret upper bound

**Lemma D.18**.: _For a probability of \(1-2\delta\), it holds that \(\pi^{\star}\in\Pi_{k}\), which is calculated as follows:_

\[\Pi_{k}=\{\pi\mid\max_{r_{\xi}\in\mathcal{B}_{k}^{r},\mathbf{P}\in\mathcal{B} _{k}^{\mathbf{P}}}(\tilde{V}_{1,r_{\xi},\mathbf{P}}^{\pi}(\xi_{h})-\tilde{V}_{ 1,r_{\xi},\mathbf{P}}^{\pi_{0}}(\xi_{h}))\geq 0,\forall\pi_{0}\}\] (59)

Proof.: It equals to for any policy \(\pi_{0}\in\Pi\),

\[\max_{r_{\xi}\in\mathcal{B}_{k}^{r},\mathbf{P}\in\mathcal{B}_{k}^{\mathbf{P}} }(\tilde{V}_{1,r_{\xi},\mathbf{P}}^{\pi^{\star}}(\xi_{h})-\tilde{V}_{1,r_{\xi },\mathbf{P}}^{\pi_{0}}(\xi_{h}))\geq 0\] (60)

According to Lemma D.1 and Lemma D.4, with at least possibility \(1-2\delta\), it holds that:

\[r_{\xi}^{\star}\in\mathcal{B}_{k}^{r},\mathbf{P}^{\star}\in\mathcal{B}_{k}^{ \mathbf{P}}\] (61)

Thus,

\[\max_{r_{\xi}\in\mathcal{B}_{k}^{r},\mathbf{P}\in\mathcal{B}_{k}^ {\mathbf{P}}}(\tilde{V}_{1,r_{\xi},\mathbf{P}}^{\pi^{\star}}(\xi_{h})-\tilde{ V}_{1,r_{\xi},\mathbf{P}}^{\pi_{0}}(\xi_{h}))\] (62) \[\geq \tilde{V}_{1,r_{\xi},\mathbf{P}}^{\pi^{\star}}(\xi_{h})-\tilde{ V}_{1,r_{\xi},\mathbf{P}}^{\pi_{0}}(\xi_{h})\geq 0\] (63)

Where the last equation comes from the definition of optimal policy \(\pi^{\star}\) (Eq. 7). 

**Lemma D.19**.: _Given a positive constant \(\delta\in(0,1]\), with probability at least \(1-4K\delta\), we have the following inequality holds:_

\[\mathrm{Reg}_{nested}(K)\] (64) \[\leqslant \max_{r_{1}\in\mathcal{B}_{k}^{r},\mathbf{P}_{1}\in\mathcal{B}_{ k}^{\mathbf{P}}}\{\tilde{V}_{1,r^{\star},\mathbf{P}^{\star}}^{\pi^{\star}}, \mathbf{P}^{\star},\,(s_{k,1})-\tilde{V}_{1,r^{\star},\mathbf{P}^{\star}}^{ \pi_{1}},\,(s_{k,1})-(\tilde{V}_{1,r_{1},\mathbf{P}_{1}}^{\pi^{\star}}\,(s_{k, 1})-\tilde{V}_{1,r_{1},\mathbf{P}_{1}}^{\pi_{1}}\,(s_{k,1}))\}\] (65) \[+ \max_{r_{2}\in\mathcal{B}_{k}^{r},\mathbf{P}_{2}\in\mathcal{B}_{ k}^{\mathbf{P}}}\{\tilde{V}_{1,r^{\star},\mathbf{P}^{\star}}^{\pi^{\star}}, \,(s_{k,1})-\tilde{V}_{1,r^{\star},\mathbf{P}^{\star}}^{\pi_{2}},\,(s_{k,1})- (\tilde{V}_{1,r_{2},\mathbf{P}_{2}}^{\pi^{\star}}\,(s_{k,1})-\tilde{V}_{1,r_{2},\mathbf{P}_{2}}^{\pi_{2}}\,(s_{k,1}))\}\] (66)

Proof.: \[\mathrm{Reg}(K)= \sum_{k=1}^{K}\Big{(}\tilde{V}_{1,r^{\star},\mathbf{P}^{\star}}^ {\pi^{\star}},\,(s_{k,1})-\tilde{V}_{1,r^{\star},\mathbf{P}^{\star}}^{\pi_{1}},\,(s_{k,1})+\tilde{V}_{1,r^{\star},\mathbf{P}^{\star}}^{\pi^{\star}},\,(s_{k,1 })-\tilde{V}_{1,r^{\star},\mathbf{P}^{\star}}^{\pi_{2}},\,(s_{k,1})\Big{)}\] (67) \[= \sum_{k=1}^{K}\max_{r_{1}\in\mathcal{B}_{k}^{r},\mathbf{P}_{1} \in\mathcal{B}_{k}^{\mathbf{P}}}(\tilde{V}_{1,r_{1},\mathbf{P}_{1}}^{\pi^{ \star}},\,(s_{k,1})-\tilde{V}_{1,r_{1},\mathbf{P}_{1}}^{\pi_{1}}\,(s_{k,1}))\] (68) \[+ \tilde{V}_{1,r^{\star},\mathbf{P}^{\star}}^{\pi^{\star}},\,(s_{k,1 })-\tilde{V}_{1,r^{\star},\mathbf{P}^{\star}}^{\pi_{1}},\,(s_{k,1})-\max_{r_{1} \in\mathcal{B}_{k}^{r},\mathbf{P}_{1}\in\mathcal{B}_{k}^{\mathbf{P}}}(\tilde{V} _{1,r_{1},\mathbf{P}_{1}}^{\pi^{\star}}\,(s_{k,1})-\tilde{V}_{1,r_{1},\mathbf{ P}_{1}}^{\pi_{1}}\,(s_{k,1}))\] (69)\[+\sum_{k=1}^{K}\max_{r_{1}\in\mathcal{B}^{*}_{k},\mathbf{P}_{2}\in \mathcal{B}^{\mathbf{P}}_{k}}(\tilde{V}^{\pi^{*}}_{1,r_{2},\mathbf{P}_{2}}\left(s _{k,1}\right)-\tilde{V}^{\pi_{2}}_{1,r_{2},\mathbf{P}_{2}}\left(s_{k,1}\right))\] (70) \[+\tilde{V}^{\pi^{*}}_{1,r^{*},\mathbf{P}^{*}}\left(s_{k,1}\right) -\tilde{V}^{\pi_{2}}_{1,r^{*},\mathbf{P}^{*}}\left(s_{k,1}\right)-\max_{r_{2} \in\mathcal{B}^{\mathbf{P}}_{k},\mathbf{P}_{2}\in\mathcal{B}^{\mathbf{P}}_{k}} (\tilde{V}^{\pi^{*}}_{1,r_{2},\mathbf{P}_{2}}\left(s_{k,1}\right)-\tilde{V}^{ \pi_{2}}_{1,r_{2},\mathbf{P}_{2}}\left(s_{k,1}\right))\] (71) \[\overset{a}{\leq}\tilde{V}^{\pi^{*}}_{1,r^{*},\mathbf{P}^{*}} \left(s_{k,1}\right)-\tilde{V}^{\pi_{1}}_{1,r^{*},\mathbf{P}^{*}}\left(s_{k,1} \right)-\max_{r_{1}\in\mathcal{B}^{\mathbf{P}}_{k},\mathbf{P}_{1}\in\mathcal{ B}^{\mathbf{P}}_{k}}(\tilde{V}^{\pi^{*}}_{1,r_{1},\mathbf{P}_{1}}\left(s_{k,1} \right)-\tilde{V}^{\pi_{1}}_{1,r_{1},\mathbf{P}_{1}}\left(s_{k,1}\right))\] (72) \[+\tilde{V}^{\pi^{*}}_{1,r^{*},\mathbf{P}^{*}}\left(s_{k,1}\right) -\tilde{V}^{\pi_{2}}_{1,r^{*},\mathbf{P}^{*}}\left(s_{k,1}\right)-\max_{r_{2} \in\mathcal{B}^{\mathbf{P}}_{k},\mathbf{P}_{2}\in\mathcal{B}^{\mathbf{P}}_{k}} (\tilde{V}^{\pi^{*}}_{1,r_{2},\mathbf{P}_{2}}\left(s_{k,1}\right)-\tilde{V}^{ \pi_{2}}_{1,r_{2},\mathbf{P}_{2}}\left(s_{k,1}\right))\] (73) \[\overset{b}{\leq}\max_{r_{1}\in\mathcal{B}^{\mathbf{P}}_{k}, \mathbf{P}_{1}\in\mathcal{B}^{\mathbf{P}}_{k}}\{\tilde{V}^{\pi^{*}}_{1,r^{*}, \mathbf{P}^{*}}\left(s_{k,1}\right)-\tilde{V}^{\pi_{1}}_{1,r^{*},\mathbf{P}^{ *}}\left(s_{k,1}\right)-(\tilde{V}^{\pi^{*}}_{1,r_{1},\mathbf{P}_{1}}\left(s_{ k,1}\right)-\tilde{V}^{\pi_{1}}_{1,r_{1},\mathbf{P}_{1}}\left(s_{k,1}\right))\}\] (74) \[+\max_{r_{2}\in\mathcal{B}^{\mathbf{P}}_{k},\mathbf{P}_{2}\in \mathcal{B}^{\mathbf{P}}_{k}}\{\tilde{V}^{\pi^{*}}_{1,r^{*},\mathbf{P}^{*}} \left(s_{k,1}\right)-\tilde{V}^{\pi_{2}}_{1,r^{*},\mathbf{P}^{*}}\left(s_{k,1} \right)-(\tilde{V}^{\pi^{*}}_{1,r_{2},\mathbf{P}_{2}}\left(s_{k,1}\right)- \tilde{V}^{\pi_{2}}_{1,r_{2},\mathbf{P}_{2}}\left(s_{k,1}\right))\}\] (75)

Where (a) comes from the definition of the optimal policy confidence set (Lemma D.18) when \(\pi^{\star}\in\Pi_{k}\) at each episode, (b) derives from the characters of max value. 

### Nested Regret

**Lemma D.20**.: _Nested regret upper bound. Given a positive constant \(\delta\in(0,1]\), with probability at least \(1-\delta\), we have the following inequality holds for every \(k\in[K]\)._

\[\operatorname{Reg}_{\text{nested}}\left(K\right)\] \[\leq \mathcal{O}\left(L_{G}H^{\frac{3}{2}}\sqrt{K}SA\log\left(\frac{KHSA }{\delta}\right)\cdot\frac{1}{\sqrt{\min_{\pi,h,\left(s,a\right):p_{\pi,h, \left(s,a\right)>0}}w_{\pi,h}\left(s,a\right)}}\right)\] \[+ \mathcal{O}\left(\frac{B}{\kappa b}dim_{\mathbb{T}}\sqrt{\log\left( \frac{Kdim_{\mathbb{T}}}{\delta}\right)\log\left(\frac{K\left(1+2B\rho_{w} \right)}{\delta}\right)}\frac{1}{min_{\omega_{\pi}}\left(d\right)}\right)\] (76)

Proof.: We use \(V^{\pi,h}_{1,r^{*},p}\left(s_{k,1}\right)\) to denote that the first \(h\) steps in the trajectory \(\xi\) is sampled using policy \(\pi\) from the MDP with transition \(\widehat{\mathbf{P}}\), and the state-action pairs from step \(h+1\) up until the last step is sampled using policy \(\pi\) from the MDP with the true transition kernal \(\mathbf{P}^{\star}\). Therefore,

Here (a) is due to Lemma D.7 and D.8. (b) comes from that \(p^{CVaR,\alpha,{V^{\pi^{k}}}}_{kh}(s,a)\) is defined as the probability of visiting \((s,a)\) at step \(h\) of episode \(k\) under the conditional transition probability \(\beta^{\alpha,V^{\pi^{k}}_{h^{\prime}+1}}(\cdot\mid\cdot,\cdot)\) for each step \(h^{\prime}=1,\ldots,h-1\).

Firstly, we analyze the term \(I_{1}\) and \(I_{5}\). Recall that for any policy \(\pi,h\in[H]\) and \((s,a)\in\mathcal{S}\times\mathcal{A}\), \(w_{\pi,h}(s,a)\) and \(w_{\pi,h}(s)\) denote th probabilities of visiting \((s,a)\) and \(s\) at step \(h\) under policy \(\pi\), respectively. Thus, we have:

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{L}_{k}}p^{G, \alpha,{V^{\pi^{k}}}}_{kh}(s,a)L_{G}\cdot H\sqrt{\frac{2SA\log(\frac{2KHSA}{ \delta})}{n_{k}\left(s,a\right)}}\] (77) \[\overset{\mathrm{(a)}}{\leq}L_{G}H\sqrt{2SA\log(\frac{2KHSA}{ \delta})}\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{L}_{k}} \frac{p^{G,\alpha,{V^{\pi^{k}}}}_{kh}(s,a)}{n_{k}(s,a)}}\cdot\sqrt{\sum_{k=1}^ {K}\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{L}_{k}}p^{G,\alpha,{V^{\pi^{k}}}}_{kh}(s,a)}\] (78)

[MISSING_PAGE_FAIL:23]

\[\leq \sum_{k=1}^{K}E_{\xi_{1}\sim\pi^{*},\xi_{2}\sim\pi_{1}}(\max_{\mathbf{ P}_{1},\mathbf{P}_{2}\in\mathcal{B}_{k}^{\mathbf{r}}}(\sum_{i=1}^{2}\sum_{h=1}^{H} \left|\mathbf{P}_{1}\left(s^{\prime}\mid s_{i,k,h},a_{i,k,h}\right)-\mathbf{P}_{2 }\left(s^{\prime}\mid s_{i,k,h},a_{i,k,h}\right)\right|))\] (100) \[+ \sum_{k=1}^{K}E_{\xi_{1}\sim\pi^{*},\xi_{2}\sim\pi_{1}}(\max_{r_{1},r_{2}\in\mathcal{B}_{k}^{\mathbf{r}}}(\sum_{i=1}^{2}\sum_{h=1}^{H}\left\langle \mathbf{P}^{\star}\left(\cdot\mid s_{i,k,h},a_{i,k,h}\right),\tilde{V}^{r_{1}} \left(\cdot\mid s_{i,k,h},a_{i,k,h}\right)-\tilde{V}^{r_{2}}\left(\cdot\mid s_ {i,k,h},a_{i,k,h}\right)\right\rangle))\] (101) \[\leq \mathcal{O}\left(L_{G}S^{2}AH^{\frac{3}{2}}\sqrt{K}log\left(K/ \delta\right)\right)+\mathcal{O}\left(L_{G}dim_{\mathbb{T}}\sqrt{K\log(KB\rho _{w})\log\left(\frac{K\left(1+2B\rho_{w}\right)}{\delta}\right)}\right)\] (102)

## Appendix E Lower bound of the regret.

### Regret Lower Bound of Nested Reward

**Lemma E.1**.: _Nested Regret Lower Bound. There exists an instance of Nested CVaR RL-RM, where the regret of any algorithm is at least:_

\[\mathrm{Regret}(K)\geq\mathcal{O}\left(\min\left\{B\rho_{w}\sqrt{\frac{AK}{\min_ {\pi,h,s:p_{\pi,h}(s)>0}w_{\pi,h}(s,a)}},B\sqrt{\frac{AK}{\min_{\pi,d}\omega_{ dim,\pi}(d)}}\right\}\right)\] (103)

Proof.: **Hard to learn case 1.** Consider the instance shown in Figure 5.The state space is defined as \(\mathcal{S}=\{s_{1},s_{2},\ldots,s_{n},x_{1},x_{2},x_{3}\}\), where \(s_{1}\) is the initial state, and \(n=S-3<S\). We define the trajectory reward functions as follows: For any \(\xi_{H}\in\mathcal{Z}_{H}\), \(\phi(\xi_{H})=(\mathbb{I}(S_{H}(\xi_{H})=x_{1})B,\mathbb{I}(S_{H}(\xi_{H})=x_{ 2})B,\mathbb{I}(S_{H}(\xi_{H})=x_{3})B)\), \(w_{r}=(1\rho,0.8\rho,0.2\rho)\)

The transition distributions are as follows. Let \(\mu\) be a parameter which satisfies that \(0<\alpha<\mu<\frac{1}{3}\). For any \(a\in\mathcal{A},p\left(s_{2}\mid s_{1},a\right)=\mu,p\left(x_{1}\mid s_{1},a \right)=1-3\mu,p\left(x_{2}\mid s_{1},a\right)=\mu\) and \(p\left(x_{3}\mid s_{1},a\right)=\mu\). For any \(i\in\{2,\ldots,n-1\}\) and \(a\in\mathcal{A},p\left(s_{i+1}\mid s_{i},a\right)=\mu\) and \(p\left(x_{1}\mid s_{i},a\right)=1-\mu.x_{1},x_{2}\) and \(x_{3}\) are absorbing states, i.e., for any \(a\in\mathcal{A},p\left(x_{1}\mid x_{1},a\right)=1,p\left(x_{2}\mid x_{2},a \right)=1\) and \(p\left(x_{3}\mid x_{3},a\right)=1\). \(p\left(x_{2}\mid s_{n},a_{J}\right)=1-\alpha+\eta\) and \(p\left(x_{3}\mid s_{n},a_{J}\right)=\alpha-\eta\), where \(\eta\) is a parameter which satisfies \(0<\eta<\alpha\) and will be chosen later. For any suboptimal action \(a\in\mathcal{A}\backslash\left\{a_{J}\right\},p\left(x_{2}\mid s_{n},a\right) =1-\alpha\) and \(p\left(x_{3}\mid s_{n},a\right)=\alpha\) For any \(a_{j}\in\mathcal{A}\), let \(\mathbb{E}_{j}[\cdot]\) and \(\Pr_{j}[\cdot]\) denote the expectation and probability operators under the instance with \(a_{J}=a_{J}\). Let \(\mathbb{E}_{\text{unif}}\left[\cdot\right]\) and \(\Pr_{\text{unif}}\left[\cdot\right]\) denote the expectation and probability operators under the uniform instance.

Fix an algorithm \(\mathcal{A}\). Let \(\pi^{k}\) denote the policy taken by algorithm \(\mathcal{A}\) in episode \(k\). Let \(N_{s_{n},a_{j}}=\sum_{k=1}^{K}\mathbb{k}\left\{\pi^{k}\left(s_{n}\right)=a_{ j}\right\}\) denote the number of episodes that the policy chooses \(a_{j}\) in state \(s_{n}\). Let \(V_{s_{n},a_{j}}\) denote the number of episodes that the algorithm \(\mathcal{A}\) visits \(\left(s_{n},a_{j}\right)\). Let \(w\left(s_{n}\right)\) denote the probability of visiting \(s_{n}\) in an episode (the probability of visiting \(s_{n}\) is the same for all policies). Then, it holds that \(\mathbb{E}\left[V_{s_{n}},a_{j}\right]=w\left(s_{n}\right)\cdot\mathbb{E} \left[N_{s_{n},a_{j}}\right]\).

According to the definition of nested-CVaR-PbRL risk objective in Eq. 4, we have:

\[V_{1}^{*}\left(s_{1}\right)=\frac{\left(\alpha-\eta\right)\cdot 0.2+\eta \cdot 0.8}{\alpha}B\rho,\] (104)

and summing over all episodes \(k\in[K]\), we have

\[\mathbb{E}_{j}[\mathcal{R}(K)] =\sum_{k=1}^{K}\left(2V_{1}^{*}\left(s_{1}\right)-V_{1}^{\pi_{1, k}}\left(s_{1}\right)-V_{1}^{\pi_{2,k}}\left(s_{1}\right)\right)\] (105) \[=\frac{1}{A}\sum_{j=1}^{A}\frac{\eta B\rho}{\alpha}\cdot 0.6\left(2K- \mathbb{E}_{j}\left[N_{s_{n},a_{j}}\right]\right)\] (106) \[=0.6\cdot\frac{\eta B\rho}{\alpha}\cdot\left(K-\frac{1}{A}\sum_{ j=1}^{A}\mathbb{E}_{j}\left[N_{s_{n},a_{j}}\right]\right)\] (107)

Figure 5: Hard to learn case 1Therefore, we have

\[\mathbb{E}[\mathcal{R}(K)] =\frac{1}{A}\sum_{j=1}^{A}\sum_{k=1}^{K}\left(V_{1}^{*}\left(s_{1} \right)-V_{1}^{\pi^{b}}\left(s_{1}\right)\right)\] (108) \[=\frac{1}{A}\sum_{j=1}^{A}\frac{\eta}{\alpha}\cdot 0.6B\rho\left(K- \mathbb{E}_{j}\left[N_{s_{n},a_{j}}\right]\right)\] (109) \[=0.6B\rho\cdot\frac{\eta}{\alpha}\cdot\left(K-\frac{1}{A}\sum_{j =1}^{A}\mathbb{E}_{j}\left[N_{s_{n},a_{j}}\right]\right)\] (110)

For any \(j\in\{A,B\}\), using Pinsker's inequality and \(0<\alpha<\frac{1}{3}\), we have that \(\mathrm{KL}\left(p_{\text{unif }}(s_{n},\pi_{j}(s_{n}))\left\|p_{j}\left(s_{n},\pi_{j}(s_{n}) \right)\right)=\mathrm{KL}(\mathrm{Ber}(\alpha)\|\mathrm{Ber}(\alpha-\eta)) \leq\frac{\eta^{2}}{(\alpha-\eta)(1-\alpha+\eta)}\leq\frac{c_{1}\eta^{2}}{\alpha}\) for some constant \(c_{1}\) and small enough \(\eta\). Then, using Lemma A. 1 in Du et al. [2022], we have,

\[\mathbb{E}_{j}\left[N_{\pi_{j}}\right] \leq\mathbb{E}_{\text{unif }}\left[N_{\pi_{j}}\right]+\frac{K}{2}\sqrt{ \mathbb{E}_{\text{unif }}\left[V_{\pi_{j}}\right]\cdot\mathrm{KL}\left(\pi_{j}(s_{n}) \right)\|p_{j}\left(s_{n},\pi_{j}(s_{n})\right)}\] (111) \[\leq\mathbb{E}_{\text{unif }}\left[N_{\pi_{j}}\right]+\frac{K}{2} \sqrt{w\left(s_{n}\right)\cdot\mathbb{E}_{\text{unif }}\left[N_{\pi_{j}}\right]\cdot\frac{c_{1}\eta^{2}}{\alpha}}\] (112)

Then, Using \(\sum_{j=1}^{A}\mathbb{E}_{unif}\left[N_{s_{n},a_{j}}\right]=2K\) and Cauchy-Schwarz inequality, we have:

\[\frac{1}{A}\sum_{j=1}^{A}\mathbb{E}_{j}\left[N_{s_{n},a_{j}}\right] \leq\frac{1}{A}\sum_{j=1}^{A}\mathbb{E}_{unif}\left[N_{s_{n},a_{ j}}\right]+\frac{K\eta}{2A}\sum_{j=1}^{A}\sqrt{\frac{c_{1}}{\alpha}\cdot w \left(s_{n}\right)\cdot\mathbb{E}_{unif}\left[N_{s_{n},a_{j}}\right]}\] (113) \[\leq\frac{1}{A}\sum_{j=1}^{A}\mathbb{E}_{unif}\left[N_{s_{n},a_{j} }\right]+\frac{K\eta}{2A}\sqrt{A\sum_{j=1}^{A}\frac{c_{1}}{\alpha}\cdot w \left(s_{n}\right)\cdot\mathbb{E}_{unif}\left[N_{s_{n},a_{j}}\right]}\] (114) \[\leq\frac{K}{A}+\frac{K\eta}{2}\sqrt{\frac{c_{1}\cdot w\left(s_{ n}\right)K}{\alpha A}}\] (115)

Thus, we have :

\[\mathbb{E}[\mathcal{R}(K)]\geq 0.6B\rho\cdot\frac{\eta}{\alpha}\cdot\left(K- \frac{K}{A}-\frac{K\eta}{2}\sqrt{\frac{c_{1}\cdot w\left(s_{n}\right)K}{ \alpha A}}\right).\] (116)

Let \(\eta=c_{2}\sqrt{\frac{\alpha A}{w\left(s_{n}\right)K}}\) for a small enough constant \(c_{2}\). We have

\[\mathbb{E}[\mathcal{R}(K)] =\Omega\left(B\rho\sqrt{\frac{A}{\alpha\cdot w\left(s_{n}\right)K }}\cdot K\right)\] \[=\Omega\left(B\rho\sqrt{\frac{AK}{\alpha\cdot w\left(s_{n}\right) }}\right)\]

#### Hard to learn case 2.

The trajectory reward functions are as follows. For any \(\xi_{H}\in\mathcal{Z}_{H}\), \(\phi(\xi_{H})=(\mathbb{I}(S_{H}(\xi_{H})=x_{1})B,\mathbb{I}(S_{H}(\xi_{H})=x_{2 })B,\mathbb{I}(S_{H}(\xi_{H})=x_{3})B,\mathbb{I}(S_{H}(\xi_{H})=x_{4})B)\), \(w_{r}=(1\rho,0.8\rho,0.2\rho,(0.2-\eta)\rho)\)

The transition distributions are as follows. Let \(\mu\) be a parameter which satisfies that \(0<\alpha<\mu<\frac{1}{3}\). For any \(a\in\mathcal{A},p\left(s_{2}\mid s_{1},a\right)=\mu,p\left(x_{1}\mid s_{1},a \right)=1-3\mu,p\left(x_{2}\mid s_{1},a\right)=\mu\) and \(p\left(x_{3}\mid s_{1},a\right)=\mu\). For any \(i\in\{2,\ldots,n-1\}\) and \(a\in\mathcal{A},p\left(s_{i+1}\mid s_{i},a\right)=\mu\) and \(p\left(x_{1}\mid s_{i},a\right)=1-\mu.x_{1},x_{2}\) and \(x_{3}\) are absorbing states, i.e., for any \(a\in\mathcal{A},p\left(x_{1}\mid x_{1},a\right)=1,p\left(x_{2}\mid x_{2},a\right) =1\) and \(p\left(x_{3}\mid x_{3},a\right)=1\).

\(p\left(x_{2}\mid s_{n},a_{J}\right)=1-\alpha\) and \(p\left(x_{3}\mid s_{n},a_{J}\right)=\alpha\), where \(\eta\) is a parameter which satisfies \(0<\eta<\alpha\) and will be chosen later. For any suboptimal action \(a\in\mathcal{A}\backslash\left\{a_{J}\right\},p\left(x_{2}\mid s_{n},a\right)=1-\alpha\) and \(p\left(x_{4}\mid s_{n},a\right)=\alpha\) For any \(a_{j}\in\mathcal{A}\), let \(\mathbb{E}_{j}[\cdot]\) and \(\Pr_{j}[\cdot]\) denote the expectation and probability operators under the instance with \(a_{J}=a_{j}\). Let \(\mathbb{E}_{\text{unif}}\left[\cdot\right]\) and \(\Pr_{\text{unif}}\left[\cdot\right]\) denote the expectation and probability operators under the uniform instance.

According to the definition of nested-CVaR-PbRL risk objective in 4, we have:

\[V_{1}^{*}\left(s_{1}\right)=0.2\eta B\rho,\] (117)

Thus,

\[\mathbb{E}_{j}[\mathcal{R}(K)] =\sum_{k=1}^{K}\left(2V_{1}^{*}\left(s_{1}\right)-V_{1}^{\pi_{1, k}}\left(s_{1}\right)-V_{1}^{\pi_{2,k}}\left(s_{1}\right)\right)\] (118) \[=\frac{1}{A}\sum_{j=1}^{A}0.2\eta B\rho\left(2K-\mathbb{E}_{j} \left[N_{s_{n},a_{j}}\right]\right)\] (119) \[=0.2\eta B\rho\cdot\left(K-\frac{1}{A}\sum_{j=1}^{A}\mathbb{E}_ {j}\left[N_{s_{n},a_{j}}\right]\right)\] (120)

Therefore, we have

\[\mathbb{E}[\mathcal{R}(K)] =\frac{1}{A}\sum_{j=1}^{A}\sum_{k=1}^{K}\left(V_{1}^{*}\left(s_{ 1}\right)-V_{1}^{\pi^{k}}\left(s_{1}\right)\right)\] (121) \[=\frac{1}{A}\sum_{j=1}^{A}0.2\eta B\rho\rho\left(K-\mathbb{E}_{j} \left[N_{s_{n},a_{j}}\right]\right)\] (122) \[=0.2\eta B\rho\cdot\frac{\eta}{\alpha}\cdot\left(K-\frac{1}{A} \sum_{j=1}^{A}\mathbb{E}_{j}\left[N_{s_{n},a_{j}}\right]\right)\] (123)

For any \(j\in\left\{A,B\right\}\), since the preference based on Bernoulli distribution, using Pinsker's inequality and \(0<\alpha<\frac{1}{3}\), we have that \(\operatorname{KL}\left(p_{\text{unif}}\left(s_{n},\pi_{j}(s_{n})\right) \left\|p_{j}\left(s_{n},\pi_{j}(s_{n})\right)\right)=\operatorname{KL}( \operatorname{Ber}(\alpha)\|\operatorname{Ber}(\alpha-\eta))\leq\frac{\eta^{2} }{(0.2-\eta)(0.8+\eta)}\leq\frac{c_{1}\eta^{2}}{0.16}\) for some constant \(c_{1}\) and small enough \(\eta\). Then, we have,

\[\mathbb{E}_{j}\left[N_{\pi_{j}}\right] \leq\mathbb{E}_{\text{unif}}\,\left[N_{\pi_{j}}\right]+\frac{K}{ 2}\sqrt{\mathbb{E}_{\text{unif}}\,\left[V_{\pi_{j}}\right]\cdot\operatorname{ KL}\left(\pi_{j}(s_{n})\right)\left\|p_{j}\left(s_{n},\pi_{j}(s_{n})\right)\right.\] (124) \[\leq\mathbb{E}_{\text{unif}}\,\left[N_{\pi_{j}}\right]+\frac{K}{ 2}\sqrt{w\left(s_{n}\right)\cdot\mathbb{E}_{\text{unif}}\,\left[N_{\pi_{j}} \right]\cdot\frac{c_{1}\eta^{2}}{0.16}}\] (125)

Then, Using \(\sum_{j=1}^{A}\mathbb{E}_{unif}\left[N_{s_{n},a_{j}}\right]=2K\) and Cauchy-Schwarz inequality, we have:

Figure 6: Hard to learn case 2

\[\frac{1}{A}\sum_{j=1}^{A}\mathbb{E}_{j}\left[N_{s_{n},a_{j}}\right] \leq\frac{1}{A}\sum_{j=1}^{A}\mathbb{E}_{unif}\left[N_{s_{n},a_{j} }\right]+\frac{K\eta}{2A}\sum_{j=1}^{A}\sqrt{\frac{c_{1}}{0.16}\cdot w\left(d \right)\cdot\mathbb{E}_{unif}\left[N_{s_{n},a_{j}}\right]}\] (126) \[\leq\frac{1}{A}\sum_{j=1}^{A}\mathbb{E}_{unif}\left[N_{s_{n},a_{j }}\right]+\frac{K\eta}{2A}\sqrt{A\sum_{j=1}^{A}\frac{c_{1}}{0.16}\cdot w\left( d\right)\cdot\mathbb{E}_{unif}\left[N_{s_{n},a_{j}}\right]}\] (127) \[\leq\frac{K}{A}+\frac{K\eta}{2}\sqrt{\frac{c_{1}\cdot w\left(d \right)K}{0.16A}}\] (128)

Thus, we have :

\[\mathbb{E}[\mathcal{R}(K)]\geq 0.2B\rho\cdot\eta\cdot\left(K-\frac{K}{A}- \frac{K\eta}{2}\sqrt{\frac{c_{1}\cdot w\left(d\right)K}{0.16A}}\right).\] (129)

Let \(\eta=c_{2}\sqrt{\frac{0.16A}{w\left(d\right)K}}\) for a small enough constant \(c_{2}\). We have

\[\mathbb{E}[\mathcal{R}(K)] =\Omega\left(B\rho\sqrt{\frac{A}{w\left(d\right)K}}\cdot K\right)\] \[=\Omega\left(B\rho\sqrt{\frac{AK}{w\left(d\right)}}\right)\]

## Appendix F The optimal policy calculation for known PbRL MDP

In this section, we describe how to compute the optimal policy for the CVaR objective when the PbRL-MDP is known; this approach is described in detail in Bauerle and Ott (2011), Bastani et al. (2022). Following this work, we consider the setting where we are trying to minimize cost rather than maximize reward. In particular, consider an know PbRL MDP \(\mathcal{M}(\mathcal{S},\mathcal{A},\tau_{\xi}^{\star},\mathbf{P}^{\star},H)\), where \(\mathcal{S}\) and \(\mathcal{A}\), and our goal is to compute a policy \(\pi\) that maximizes its CVaR objective.

**Lemma F.1**.: _CVaR definition. For any random variable \(Z\), we have_

\[\mathrm{CVaR}_{\alpha}(Z)=\sup_{\rho\in\mathbb{R}}\left\{\rho-\frac{1}{ \alpha}\cdot\mathbb{E}_{Z}\left[\left(\rho-Z\right)^{+}\right]\right\},\]

_where the minimum is achieved by \(\rho^{\star}=\mathrm{VaR}_{\alpha}(Z)\)._

### Static CVaR object

Since the optimal policy for static CVaR object satisfies:

\[\pi^{\star}\in\mathrm{argmax}_{\pi\in\Pi}\,\mathrm{CVaR}(Z(\pi))\] (130)

As a consequence of Lemma F.1, we have

\[\mathrm{CVaR}\left(Z^{\left(\pi^{\star}\right)}\right) =\max_{\pi\in\Pi}\mathrm{CVaR}\left(Z^{\left(\pi\right)}\right)\] (131) \[=\max_{\pi\in\Pi}\sup_{\rho\in\mathbb{R}}\left\{\rho-\frac{1}{ \alpha}\cdot\mathbb{E}_{Z}(\pi)\left[\left(\rho-Z^{\left(\pi\right)}\right)^{ +}\right]\right\}\] (132) \[=\sup_{\rho\in\mathbb{R}}\left\{\rho-\frac{1}{\alpha}\cdot\max_{ \pi\in\Pi}\mathbb{E}_{Z^{\left(\pi\right)}}\left[\left(\rho-Z^{\left(\pi \right)}\right)^{+}\right]\right\}.\] (133)Thus, the optimal policy is:

\[\pi^{\star}=\underset{\tau\in\Pi}{\arg\max}\mathbb{E}_{Z^{(\pi)}}\left[\left(\rho^ {\star}-Z^{(\pi)}\right)^{+}\right]\] (134)

where

\[\rho^{\ast}=\underset{\rho\in\mathbb{R}}{\arg\sup}J(\rho)\quad\text{ where }\quad J(\rho)=\rho-\frac{1}{\alpha}\cdot\max_{\pi\in\Pi}\mathbb{E}_{Z}(\pi) \left[\left(\rho-Z^{(\pi)}\right)^{+}\right].\] (135)

**Value iteration.** we reconstruct the MDP as enlarge the state space \(\tilde{s}_{h}=(\xi_{h},\rho)\), where \(\rho\) will work as a quantile value. Letting \(S_{1}\) be the initial state of the original PbRL MDP \(\mathcal{M}\).

We iterate the value and calculate the policy as follows:

\[\widetilde{V}_{H}((\xi_{H},\rho)) =\max\{\rho-r_{\xi}^{\star}(\xi_{H}),0\}\] (136) \[\widetilde{V}_{h}((\xi_{h},\rho)) =\max_{a\in A}\int\widetilde{V}_{h+1}\left((s^{\prime}\circ(\xi_ {h},a),\rho)\right)\cdot\mathbf{P}^{\star}\left(s^{\prime}\mid(S_{h}(\xi_{h}), a)\right)\] (137) \[\pi(\xi_{h}) =argmax_{a\in A}\int\widetilde{V}_{h+1}\left((s^{\prime}\circ( \xi_{h},a),\rho)\right)\cdot\mathbf{P}^{\star}\left(s^{\prime}\mid(S_{h}(\xi_{ h}),a)\right)\] (138)

Then, given an initial state \(s_{1}\), we construct state \(\Im_{1}=(s_{1},-\rho^{\ast})\), where

\[\rho^{\ast}=\underset{\rho\in\mathbb{R}}{\argsup}\left\{\rho-\frac{1}{\alpha }\cdot\widetilde{V}_{1}^{(\pi)}((s_{1},-\rho))\right\},\]

and then acting optimally in \(\mathcal{M}\).

### Nested CVaR object

According to Eq. 4, nested CVaR object could directly use the Bellman equation to iterate the value.

**Value iteration.** we reconstruct the MDP as enlarge the state space \(\tilde{s}_{h}=(\xi_{h},\rho)\), where \(\rho\) will work as a quantile value. Letting \(S_{1}\) be the initial state of the original PbRL MDP \(\mathcal{M}\).

We iterate the value and calculate the policy as follows:

\[\widetilde{V}_{H}((\xi_{H},\rho)) =r_{\xi}^{\star}(\xi_{H})\] (139) \[\widetilde{V}_{h}((\xi_{h},\rho)) =\max_{\pi\in\Pi}\sup_{\rho\in\mathbb{R}}\left\{\rho-\frac{1}{ \alpha}\cdot\mathbb{E}_{\widetilde{V}_{h+1}((s^{\prime}\circ(\xi_{h},a),\rho) )}\left[\left(\rho-\widetilde{V}_{h+1}\left((s^{\prime}\circ(\xi_{h},a),\rho) \right)\right)^{+}\right]\right\}\] (140) \[\pi(\xi_{h}) =\arg\max_{\pi\in\Pi}\sup_{\rho\in\mathbb{R}}\left\{\rho-\frac{1} {\alpha}\cdot\mathbb{E}_{\widetilde{V}_{h+1}((s^{\prime}\circ(\xi_{h},a),\rho ))}\left[\left(\rho-\widetilde{V}_{h+1}\left((s^{\prime}\circ(\xi_{h},a),\rho )\right)\right)^{+}\right]\right\}\] (141)

then acting optimally in \(\mathcal{M}\).

## Appendix G Auxiliary Lemmas

**Definition G.1**.: \(\alpha\)**-dependence in Russo and Van Roy [2013]**. For \(\alpha>0\) and function class \(\mathcal{Z}\) whose elements are with domain \(\mathcal{X}\), an element \(x\in\mathcal{X}\) is \(\alpha\)-dependent on the set \(\mathcal{X}_{n}:=\left\{x_{1},x_{2},\cdots,x_{n}\right\}\subset\mathcal{X}\) with respect to \(\mathcal{Z}\), if any pair of functions \(z,z^{\prime}\in\mathcal{Z}\) with \(\sqrt{\sum_{i=1}^{n}\left(z\left(x_{i}\right)-z^{\prime}\left(x_{i}\right) \right)^{2}}\leqslant\alpha\) satisfies \(z(x)-z^{\prime}(x)\leqslant\alpha\). Otherwise, \(x\) is \(\alpha\)-independent on \(\mathcal{X}_{n}\) if it does not satisfy the condition.

**Definition G.2**.: **Eluder dimension in Russo and Van Roy [2013]**. For \(\alpha>0\) and function class \(\mathcal{Z}\) whose elements are with domain \(\mathcal{X}\), the Eluder dimension \(\dim_{E}(\mathcal{Z},\alpha)\), is defined as the length of the longest possible sequence of elements in \(\mathcal{X}\) such that for some \(\alpha^{\prime}\geqslant\alpha\), every element is \(\alpha^{\prime}\) independent of its predecessors.

**Definition G.3**.: **Covering number** Given two functions \(l\) and \(u\), the bracket \([l,u]\) is the set of all functions \(f\) satisfying \(l\leq f\leq u\). An \(\alpha\)-bracket is a bracket \([l,u]\) with \(\left\|u-l\right\|<\alpha\). The covering number \(N_{[\cdot]}(\mathcal{F},\alpha,\left\|\cdot\right\|)\) is the minimum number of \(\alpha\)-brackets needed to cover \(\mathcal{F}\).

**Lemma G.4**.: _(**Linear Preference Models Eluder dimension and Covering number). For the case of \(d\)-dimensional generalized trajectory linear feature models \(r_{\xi}\left(\xi_{H}\right):=\left\langle\phi\left(\xi_{H}\right),\mathbf{w}_{ r}\right\rangle\), where \(\phi:\) Traj \(\rightarrow\)\(\mathbb{R}^{dim_{\mathbb{T}}}\) is a known \(dim_{\mathbb{T}}\) dimension feature map satisfying \(\left\|\psi\left(\xi_{H}\right)\right\|_{2}\leq B\) and \(\theta\in\mathbb{R}^{d}\) is an unknown parameter with \(\left\|\mathbf{w}_{r}\right\|_{2}\leq\rho_{w}\). Then the \(\alpha\)-Eluder dimension of \(r_{\xi}(\xi_{H})\) is at most \(\mathcal{O}(dim_{\mathbb{T}}\log(B\rho_{w}/\alpha))\). The \(\alpha\) - covering number is upper bounded by \(\left(\frac{1+2B\rho_{w}}{\alpha}\right)^{dim_{\mathbb{T}}}\)._

Let \(\left(X_{p},Y_{p}\right)_{p=1,2,\ldots}\) be a sequence of random elements, \(X_{p}\in X\) for some measurable set \(X\) and \(Y_{p}\in\mathbb{R}\). Let \(\mathcal{F}\) be a subset of the set of real-valued measurable functions with domain \(X\). Let \(\mathbb{F}=\left(\mathbb{F}_{p}\right)_{p=0,1,\ldots}\) be a filtration such that for all \(p\geq 1,\left(X_{1},Y_{1},\cdots,X_{p-1},Y_{p-1},X_{p}\right)\) is \(\mathbb{F}_{p-1}\) measurable and such that there exists some function \(f_{\star}\in\mathcal{F}\) such that \(\mathbb{E}\left[Y_{p}\mid\mathbb{F}_{p-1}\right]=f_{\star}\left(X_{p}\right)\) holds for all \(p\geq 1\). The (nonlinear) least square predictor given \(\left(X_{1},Y_{1},\cdots,X_{t},Y_{t}\right)\) is \(\hat{f}_{t}=\operatorname*{argmin}_{f\in\mathcal{F}}\sum_{p=1}^{t}\left(f \left(X_{p}\right)-Y_{p}\right)^{2}\). We say that \(Z\) is conditionally \(\rho\)-subgaussian given the \(\sigma\)-algebra \(\mathbb{F}\) is for all \(\lambda\in\mathbb{R},\log\mathbb{E}[\exp(\lambda Z)\mid\mathbb{F}]\leq\frac{1 }{2}\lambda^{2}\rho^{2}\). For \(\alpha>0\), let \(N_{\alpha}\) be the \(\|\cdot\|_{\infty}\)-covering number of \(\mathcal{F}\) at scale \(\alpha\). For \(\beta>0\), define

\[\mathcal{F}_{t}(\beta)=\left\{f\in\mathcal{F}:\sum_{p=1}^{t}\left(f\left(X_{p }\right)-\hat{f}_{t}\left(X_{p}\right)\right)^{2}\leq\beta\right\}.\] (142)

**Lemma G.5**.: _(Theorem 5 of Ayoub et al. (2020)). Let \(\mathbb{F}\) be the filtration defined above and assume that the functions in \(\mathcal{F}\) are bounded by the positive constant \(C>0\). Assume that for each \(s\geq 1,\left(Y_{p}-f_{\star}\left(X_{p}\right)\right)\) is conditionally \(\sigma\)-subgaussian given \(\mathbb{F}_{p-1}\). Then, for any \(\alpha>0\), with probability \(1-\delta\), for all \(t\geq 1,f_{\star}\in\mathcal{F}_{t}\left(\beta_{t}(\delta,\alpha)\right)\), where_

\[\beta_{t}(\delta,\alpha)=8\sigma^{2}\log\left(2N_{\alpha}/\delta\right)+4t \alpha\left(C+\sqrt{\sigma^{2}\log(4t(t+1)/\delta)}\right).\]

**Lemma G.6**.: _(Lemma 5 of Russo and Van Roy (2013)). Let \(\mathcal{F}\in B_{\infty}(X,C)\) be a set of functions bounded by \(C>0\), \(\left(\mathcal{F}_{t}\right)_{t\geq 1}\) and \(\left(x_{t}\right)_{t\geq 1}\) be sequences such that \(\mathcal{F}_{t}\subset\mathcal{F}\) and \(x_{t}\in\mathcal{X}\) hold for \(t\geq 1\). Let \(\mathcal{F}|_{x_{1:t}}=\left\{\left(\tilde{f}\left(x_{1}\right),\ldots,\tilde{f }\left(x_{t}\right)\right):f\in\mathcal{F}\right\}\left(\subset\mathbb{R}^{t}\right)\) and for \(S\subset\mathbb{R}^{t}\), let \(\operatorname*{diam}(S)=\sup_{u,v\in S}\left\|u-v\right\|_{2}\) be the diameter of \(S\). Then, for any \(T\geq 1\) and \(\alpha>0\) it, holds that_

\[\sum_{t=1}^{T}\operatorname*{diam}\left(\left.\mathcal{F}_{t}\right|_{x_{t}} \right)\leq\alpha+C(d\wedge T)+2\delta_{T}\sqrt{dT},\]

_where \(\delta_{T}=\max_{1\leq t\leq T}\operatorname*{diam}\left(\left.\mathcal{F}_{t }\right|_{x_{1:t}}\right)\) and \(d=\dim_{\mathcal{E}}(\mathcal{F},\alpha)\)._

**Lemma G.7**.: _If \(\left(\beta_{t}\geq 0\mid t\in\mathbb{N}\right)\) is a nondecreasing sequence and \(\mathcal{F}_{t}:=\left\{f\in\mathcal{F}:\left\|f-\hat{f}_{t}^{LS}\right\|_{2,E _{t}}\leq\sqrt{\beta_{t}}\right\}\), where \(\hat{f}_{t}^{LS}\in\arg\min_{f\in\mathcal{F}}L_{2,t}(f)\) and \(L_{2,t}(f)=\sum_{1}^{t-1}\left(f\left(A_{t}\right)-R_{t}\right)^{2}\), then for all \(T\in\mathbb{N}\) and \(\epsilon>0\),_

\[\sum_{t=1}^{T}\mathbf{1}\left(w_{\mathcal{F}_{t}}\left(A_{t}\right)>\epsilon \right)\leq\left(\frac{4\beta_{T}}{\epsilon^{2}}+1\right)\dim_{E}(\mathcal{F},\epsilon)\]

_where \(w_{\mathcal{F}}(a):=\sup_{f\in\mathcal{F}}f(a)-\inf_{f\in\mathcal{F}}f(a)\) denotes confidence interval widths._

**Theorem G.8**.: _Hoeffding's inequality[Hoeffding, 1994]. Let \(X_{1},X_{2},\ldots,X_{n}\) be independent random variables that are sub-Gaussian with parameter \(\sigma\). Define \(S_{n}=\sum_{i=1}^{n}X_{i}\). Then, for any \(t>0\), Hoeffding's inequality provides an upper bound on the tail probabilities of \(S_{n}\), which is given by:_

\[\Pr\left(|S_{n}-\mathbb{E}[S_{n}]|\geq t\right)\leq 2\exp\left(-\frac{t^{2}}{2n \sigma^{2}}\right).\]

_This result emphasizes the robustness of the sum \(S_{n}\) against deviations from its expected value, particularly useful in applications requiring high confidence in estimations from independent sub-Gaussian observations._

**Lemma G.9**.: _(Lemma F.4. in Dann et al. [2017]) Let \(\mathcal{F}_{i}\) for \(i=1\ldots\) be a filtration and \(X_{1},\ldots X_{n}\) be a sequence of Bernoulli random variables with \(\mathbb{P}\left(X_{i}=1\mid\mathcal{F}_{i-1}\right)=P_{i}\) with \(P_{i}\) being \(\mathcal{F}_{i-1}\)-measurable and \(X_{i}\) being \(\mathcal{F}_{i}\) measurable. It holds that_

\[\mathbb{P}\left(\exists n:\sum_{t=1}^{n}X_{t}<\sum_{t=1}^{n}P_{t}/2-W\right) \leq e^{-W}\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contribution and scope: RA-PbRL algorithm. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discussed the limitations of the work in section 3.2, and section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The paper provided the full set of assumptions and a complete proof in Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The experimental results presented in the paper are reproducible. We have provided detailed pseudocode and full information for implementing the code, allowing any researcher to easily implement our algorithm and benchmark it against other algorithms. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide a link to our code repository. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experimental details are provided in section 5.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: the results are accompanied standard deviation. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The computer resources needed to implement experiments are provided in the section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: the paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subject. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subject. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.