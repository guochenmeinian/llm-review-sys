# Plug-and-Play Bayesian Online Change-Point Detection in Gaussian Processes

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study the problem of detecting non-stationarity in online time series, when the underlying distribution is assumed to be a piecewise 1d-Gaussian process. Drawing inspiration from Bayesian online change-point detection methods such as that of Adams and MacKay (2007), we construct a restarted variant of that to specifically deal with arbitrary changes both in mean and variance of 1d-Gaussian processes. We evaluate our algorithm on both synthetic datasets of varying task difficulty and on prevalent real-world data across a variety of fields. Our results compare favorably with state-of-the-art, as measured by the detections' F1-Score. Code will be provided to ensure easy reproducibility.

## 1 Introduction

While most statistical models assume that the input data is generated according to some underlying distribution, it is often assumed that the latter does not actively change throughout instances of training or inference. This presents a major setback when tackling real-world problems where the data collection process is often done in a sequential manner and where the parameters of the generating distribution, if assumed parametric, are changing continuously. This applies in various ways across a variety of fields in machine learning and statistics: distribution shifts in deep learning (DL), environment non-stationarity in reinforcement learning (RL), to name a few.

More precisely, we consider the general setting of a sequential decision-making process, during which a series of abrupt changes, commonly referred to as _change-points_ (CP), take place. Change-points are sudden shifts in the underlying parameters of the data generating distribution of a given sequence. Detecting these change-points in real-time is of vital importance for the analysis and forecasting of time series, especially in high volatility scenarios, in consumer decision modeling (Xu and Yun (2020)), service provider adaptation to customers, and pricing (Taylor (2018); Kanoria and Qian (2019); Bimpikis et al. (2019); Gurvich et al. (2019)), wireless communication networks (Zhou and Bambos (2015); Zhou et al. (2016)), epidemic networks and control (Nowzari et al. (2016); Kiss et al. (2017)), inventory management (Agrawal and Jia (2019); Huh and Rusmevichientong (2009)), non-stationary multi armed bandits (Alami et al. (2017); Alami and Azizi (2020); Alami (2023, 2018); Garivier and Moulines (2011), safety-aware bandits: Alami et al. (2023a)), RL (Alami et al., 2023b), and notions of automated quality control (El Mekkaoui et al. (2024)), to name a few.

**Key contributions.** We outline our contributions as follows

* We propose a novel variant of the Restarted Bayesian Online Change-Point Detection algorithm (R-bocpd), that generalizes the modeling scope to the setting where online observation stream is generated from an underlying piecewise stationary 1d-Gaussian process (GP). Our model incorporates changes in either means or variances (or both) of the underlying GP.
* We demonstrate our results experimentally across a wide range of tasks of varying difficulty, on both synthetic and real-world datasets. Our algorithm compares favorably tostate-of-the-art in both online and offline settings, as measured by false-alarm and misdetection rates, detection delay, and finite-time runtime.
* We present concrete promising directions for future work relevant both to theoreticians and practitioners working both on probabilistic modeling under uncertainty and general modeling of time-series data in a variety of fields.

## 2 Main Results

Given the prevalent use of 1d-Gaussian distributions as statistical priors for a wide range of problems across a variety of domains, we choose to model a piecewise stationary GP, where a set of unknown abrupt change-points \(\{c_{\ell}\}_{\ell=1}^{L}\) take place such that

\[x_{t}\sim\mathcal{N}(\mu_{\ell},\sigma_{\ell})\quad\forall t\in[c_{\ell},c_{ \ell+1}],\forall\ell\in[1,L]\] (1)

where \(L\) is the unknown total number of change-points throughout \([1,t]\). We provide Figure 1 for visualization.

**Challenge.** We aim to detect the change-points online and sequentially from data in real-time, without _a-priori_ knowledge on their number, their location, or how often they occur. The latter, indeed, can be used as a plug-and-play prior, as showcased later on in Algorithm 1. In particular, we are interested in designing an algorithm that detects change-points

* _reliably_, with as few misdetections and false-alarms as possible.
* _in real-time_, with as low of a detection delay as possible.
* _incorporating uncertainty_, yielding statistically optimal or near-optimal probabilistic guarantees, allowing for flexibility and control to the decision make.

### Change-point Detection as _Runlength Inference_

We first introduce the notion of _runlength_\(r_{t}\), which is defined as the number of time steps since the last change-point to the process, given an observed data sequence \(\mathbf{x}_{1:t}\) (up to current time step \(t\)). Adams and MacKay (2007) introduce an efficient Bayesian approach for handling piecewise stationary processes via computing the posterior distribution over the current runlength \(r_{t}\). The exact inference on the runlength distribution is done recursively via message-passing as follows

\[p\left(r_{t}|\mathbf{x}_{1:t}\right)\propto\sum_{r_{t-1}}\underbrace{p\left(r _{t}|r_{t-1}\right)}_{\text{hazard function}}\underbrace{p\left(x_{t}|r_{t-1}, \mathbf{x}_{1:t-1}\right)}_{\text{UPM}}p\left(r_{t-1}|\mathbf{x}_{1:t-1}\right)\]

where the _hazard function_ is defined as

\[p\left(r_{t}|r_{t-1}\right)=\begin{cases}H\left(r_{t-1}\right)&\text{if }r_{t}=0\\ 1-H\left(r_{t-1}\right)&\text{if }r_{t}=r_{t-1}+1\\ 0&\text{otherwise}\end{cases}\] (2)

where \(H\) is defined as \(H(s)=\frac{\mathbb{P}_{\text{change}}(s+1)}{\sum\limits_{t=s+1}^{\infty}\mathbb{ P}_{\text{change}}(t)}\), \(\mathbb{P}_{\text{change}}\) denotes the probability distribution over the interval between changepoints, and the _underlying probability model_ (UPM) depends on the probability distribution of \(x_{t}\). We showcase an illustrating example for our reasoning and algorithmic construction in Appendix A.

Figure 1: Piecewise Stationary Gaussian Process. Starting from change-point \(c_{\ell}\), the model is assumed to be Gaussian of parameters \((\mu_{\ell},\sigma_{\ell})\).

### Algorithmic Construction of R-bocpd

For an algorithmic construction that adheres to our previously outlined design objectives, we draw inspiration from Alami et al. (2020)'s extension to the seminal _Bayesian Online Change-point Detection_ (Bocpd) work of (Fearnhead and Liu, 2007). Indeed, Alami et al. (2020) introduce a pruned variant of the latter, which they refer to as the _Restarted Bayesian Online Change-point Detection_, particularly designed to model online changes in the means of univariate Bernoulli-distributed data samples. We propose to extend the R-bocpd construction to change-points in an online piecewise stationary GP. In particular, we start by introducing a few definitions in the following

**Definition 2.1** (Predictor).: Given a sequence of observations \(\mathbf{x}_{s:t}=(x_{s},...,x_{t})\), we define an instance of a predictor as follows

\[\textsc{Pred}\left(x_{t+1}|\mathbf{x}_{s:t}\right)=\frac{\Gamma\left(\frac{2 \alpha_{s:t}+1}{2}\right)}{\Gamma\left(\alpha_{s:t}\right)\sqrt{2\alpha_{s:t} \pi}}\left(1+\frac{1}{2}\times\frac{\left(x_{t+1}-\mu_{s:t}\right)^{2}}{\frac{ \beta_{s:t}\times\left(\mathbf{x}_{s:t}+1\right)}{n_{s:t}}}\right)^{-\frac{2 \alpha_{s:t}+1}{2}}\] (4)

where \(\textsc{Pred}\left(x|\emptyset\right)=\frac{\Gamma\left(\frac{2\alpha_{0}+1}{2 }\right)}{\Gamma\left(\alpha_{0}\right)\sqrt{2\alpha_{0}\pi}}\) for some chosen \(\alpha_{0}>0\) at initialization and incrementing procedure

\[\alpha_{s:t+1}=\alpha_{s:t}+\frac{1}{2}\quad n_{s:t+1}=n_{s:t}+1\quad\beta_{s: t+1}=\beta_{s:t}+\frac{n_{s:t}\times\left(x_{t+1}-\mu_{s:t}\right)^{2}}{2 \times\left(n_{s:t}+1\right)}\]

Akin to the our considered construction, instead of dealing with run-length, we introduce the notion of forecaster loss as loss incurred by predictors across time

**Definition 2.2** (Forecaster Loss).: Using the predictor, the instantaneous loss of the forecaster \(s\) at time \(t\) is given by:

\[\ell_{s:t}:=-\log\textsc{Pred}\left(x_{t}|\mathbf{x}_{s:t-1}\right).\]

Then, let \(\widehat{L}_{s:t}:=\sum\limits_{s^{\prime}=s}^{t}\ell_{s^{\prime}:t}\) denotes the cumulative loss incurred by the forecaster \(s\) from time \(s\) until time \(t\) which takes the following form

\[\widehat{L}_{s:t}:=\sum\limits_{s^{\prime}=s}^{t}-\log\textsc{Pred}\left(x_{t }|\mathbf{x}_{s^{\prime}:t-1}\right)\] (5)

Thus, the forecaster weights update will remain the same (for some parameter \(h\in(0,1)\)).

\[\nu_{r,s:t}=\begin{cases}\left(1-h\right)\exp\left(-\ell_{s,t}\right)\nu_{r, s:t-1}&\forall s<t,\\ h\times V_{r:t-1}&s=t\end{cases}\text{ with }\quad V_{r:t-1}:=\sum\limits_{i=r}^{t-1} \exp\left(-\ell_{i:t}\right)\nu_{r,i:t-1}\] (6)

Finally, we keep the same restart procedure as in Alami et al. (2020), namely

\[\textsc{Restart}(x_{r},...,x_{t})=\mathbb{I}\left[\exists\;s\in(r,t]:\nu_{r, s:t}>\nu_{r,r:t}\right]\] (7)

where \(\mathbb{I}(.)\) is the indicator function. Finally, we describe our algorithm in full pseudo-code in Algorithm 1.

### Empirical Results

**Measuring Performance.** We design a diverse synthetic task suite and also evaluate on prevalent open-source real-world time-series data (https://github.com/alan-turing-institute/TCPD). We evaluate our algorithm on two key metrics: _F1-Score_ and _detection delay_. We normalize the latter by the length of its corresponding stationary period, i.e for change-point \(c_{\ell}\), delay \(d_{\ell}\) is normalized by \(c_{\ell+1}-c_{\ell}\).

**Runtime.** Given the online nature of our proposed algorithm, we analyze to what extent our algorithm (and others) allow for smooth inference in real-time. We see that our runtime compares favorably with state-of-the-art, which makes it especially useful for modeling long-context sequences.

**Discussion & Future Work.** Our work sheds light on online change-point detection when the underlying distribution of the streaming data is a 1d-Gaussian. Being both quite relevant in practice and theoretically interpretable, this lays the ground for a variety of possible extensions and future work. Among these we list- establishing potential theoretical optimality guarantees, scaling to mixtures of detectors and building hybrid algorithms. We defer a detailed discussion of the latter to Appendix B.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{2}{*}{Algorithm} & \multicolumn{6}{c}{Dataset} \\ \cline{2-9}  & \multicolumn{2}{c}{Very Easy} & \multicolumn{2}{c}{Easy} & \multicolumn{2}{c}{Medium} & \multicolumn{2}{c}{Hard} \\ \cline{2-9}  & F1-Score \(\uparrow\) & Delay \(\downarrow\) & F1-Score \(\uparrow\) & Delay\(\downarrow\) & F1-Score \(\uparrow\) & Delay\(\downarrow\) & F1-Score \(\uparrow\) & Delay\(\downarrow\) \\ \hline \hline Bishop [56] & 0.87 & — & **1.0** & — & 0.8 & — & 0.6 & — \\ Pelt [10] & **1.0** & — & **1.0** & — & 0.909 & — & 0.67 & — \\ \hline \hline CSUM[20] & 0.632 & 0.444 & — & — & — & — & — & — \\ GIRT[20] & **1.0** & 0.009 & **1.0** & 0.14 & 0.889 & 0.339 & 0.174 & 0.879 \\ \hline R-bocpd (Ours) & **1.0** & 0.01 & **1.0** & **0.016** & **1.0** & **0.038** & **0.9** & **0.498** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Benchmark results for various algorithms across different synthetic datasets of varying difficulty. We report the F1-Score and Delay for each method. Delay is normalized by the size of the stationary periods for each detected change-point for appropriate unified performance assessment across sequences of different lengths. We highlight here that the first family of methods is offline, hence naturally would get much smaller delays. The second group of methods is online, akin to our proposed method. For the interest of comparison within our considered setting, we only measure detection delays for online methods.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Algorithm & \multicolumn{2}{c}{Dataset} \\ \cline{2-5}  & Jefk Passengers & Co\({}_{2}\) Canada & Businv & Bitcoin \\ \hline \hline Binese[56] & 1.0 & 0.67 & 0.24 & 0.43 \\ Pelt[10] & 0.5 & 0.67 & 0.20 & 0.43 \\ \hline \hline GIRT[20] & **1.0** & 0.8 & 0.57 & 0.58 \\ R-bocpd[20] & — & — & 0.27 & — \\ Gpts-cp[20] & — & — & 0.62 & — \\ Adaga[11] & — & — & 0.77 & — \\ \hline \hline R-bocpd (Ours) & **1.0** & **1.0** & **0.8** & **0.8** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Benchmark results for various algorithms across different real-world datasets belonging to a variety of domains. We mainly compare in terms of F1-Score. We were unable to reproduce R-bocpdms, Gpts-cp and Adaga at the time of submission. Instead, we reproduce their exact setting for the Businv dataset.

\begin{table}
\begin{tabular}{l c} \hline \hline Algorithm & Runtime (ms) \(\downarrow\) \\ \hline Binese[56] & 2.82666 \\ Pelt[10] & 15.3721 \\ \hline GIRT[20] & 3.0112 \\ \hline R-bocpd(Ours) & **0.7097** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Runtime (in milliseconds) per iteration. The first family of methods is offline, while the second one is online, similarly to R-Bocpd.

## References

* Adams and MacKay [2007] R. P. Adams and D. J. MacKay. Bayesian online changepoint detection. _arXiv preprint arXiv:0710.3742_, 2007.
* Agrawal and Jia [2019] S. Agrawal and R. Jia. Learning in structured mdps with convex cost functions: Improved regret bounds for inventory management. In _Proceedings of the 2019 ACM Conference on Economics and Computation_, pages 743-744, 2019.
* Alami [2018] R. Alami. Thompson Sampling for the non-Stationary Corrupt Multi-Armed Bandit. In _The 14th European Workshop on Reinforcement Learning_, volume 14, Lille, France, Oct. 2018. URL https://hal.science/hal-01963539.
* Alami [2019] R. Alami. Bayesian change-point detection for bandit feedback in non-stationary environments. In E. Khan and M. Gonen, editors, _Proceedings of The 14th Asian Conference on Machine Learning_, volume 189 of _Proceedings of Machine Learning Research_, pages 17-31. PMLR, 12-14 Dec 2023. URL https://proceedings.mlr.press/v189/alami23a.html.
* Alami and Azizi [2020] R. Alami and O. Azizi. TS-GLR: an Adaptive Thompson Sampling for the Switching Multi-Armed Bandit Problem. In _NeurIPS 2020 challenges of real world reinforcement learning workshop_, Virtual, Canada, Dec. 2020. URL https://hal.science/hal-03628791.
* 31st Conference on Neural Information Processing Systems_, Long Beach, United States, Dec. 2017. URL https://hal.science/hal-01811697.
* Alami et al. [2020a] R. Alami, O. Maillard, and R. Feraud. Restarted bayesian online change-point detector achieves optimal detection delay. In _International conference on machine learning_, pages 211-221. PMLR, 2020a.
* Alami et al. [2020b] R. Alami, O. Maillard, and R. Feraud. Restarted Bayesian online change-point detector achieves optimal detection delay. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 211-221. PMLR, 13-18 Jul 2020b. URL https://proceedings.mlr.press/v119/alami20a.html.
* Alami et al. [2023a] R. Alami, M. Mahfoud, and M. Achab. A risk-averse framework for non-stationary stochastic multi-armed bandits, 2023a. URL https://arxiv.org/abs/2310.19821.
* Alami et al. [2023b] R. Alami, M. Mahfoud, and E. Moulines. Restarted bayesian online change-point detection for non-stationary markov decision processes, 2023b. URL https://arxiv.org/abs/2304.00232.
* Basseville and Nikiforov [1993] M. Basseville and I. V. Nikiforov. _Detection of abrupt changes: theory and application_. Prentice-Hall, 1993.
* Bimpikis et al. [2019] K. Bimpikis, O. Candogan, and D. Saban. Spatial pricing in ride-sharing networks. _Operations Research_, 67(3):744-769, 2019.
* Caldarelli et al. [2022] E. Caldarelli, P. Wenk, S. Bauer, and A. Krause. Adaptive Gaussian process change point detection. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 2542-2571. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/caldarelli22a.html.
* Mekkaoui et al. [2024] S. El Mekkaoui, G. Boukachab, L. Benabbou, and A. Berrado. Deep learning based vessel arrivals monitoring via autoregressive statistical control charts. _WMU Journal of Maritime Affairs_, pages 1-18, 2024.
* Fearnhead and Liu [2007] P. Fearnhead and Z. Liu. On-line inference for multiple changepoint problems. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 69(4):589-605, 2007.
* Garivier and Moulines [2011] A. Garivier and E. Moulines. On upper-confidence bound policies for switching bandit problems. In _Algorithmic Learning Theory: 22nd International Conference, ALT 2011, Espoo, Finland, October 5-7, 2011. Proceedings 22_, pages 174-188. Springer, 2011.

I. Gurvich, M. Larivere, and A. Moreno. Operations in the on-demand economy: Staffing services with self-scheduling capacity. _Sharing Economy: Making Supply Meet Demand_, pages 249-278, 2019.
* Huh and Rusmevichientong [2009] W. T. Huh and P. Rusmevichientong. A nonparametric asymptotic analysis of inventory planning with censored demand. _Mathematics of Operations Research_, 34(1):103-123, 2009.
* Kanoria and Qian [2019] Y. Kanoria and P. Qian. Blind dynamic resource allocation in closed networks via mirror backpressure. _arXiv preprint arXiv:1903.02764_, 2019.
* Keshavarz et al. [2018] H. Keshavarz, C. Scott, and X. Nguyen. Optimal change point detection in gaussian processes. _Journal of Statistical Planning and Inference_, 193:151-178, 2018.
* Killick et al. [2012] R. Killick, P. Fearnhead, and I. A. Eckley. Optimal detection of changepoints with a linear computational cost. _Journal of the American Statistical Association_, 107(500):1590-1598, 2012.
* Kiss et al. [2017] I. Z. Kiss, J. C. Miller, P. L. Simon, et al. Mathematics of epidemics on networks. _Cham: Springer_, 598:31, 2017.
* Knoblauch et al. [2018] J. Knoblauch, J. E. Jewson, and T. Damoulas. Doubly robust bayesian inference for non-stationary streaming data with beta -divergences. _Advances in Neural Information Processing Systems_, 31, 2018.
* Nowzari et al. [2016] C. Nowzari, V. M. Preciado, and G. J. Pappas. Analysis and control of epidemics: A survey of spreading processes on complex networks. _IEEE Control Systems Magazine_, 36(1):26-46, 2016.
* Saatci et al. [2010] Y. Saatci, R. D. Turner, and C. E. Rasmussen. Gaussian process change point models. In _Proceedings of the 27th International Conference on Machine Learning (ICML-10)_, pages 927-934, 2010.
* Scott and Knott [1974] A. J. Scott and M. Knott. A cluster analysis method for grouping means in the analysis of variance. _Biometrics_, 30(3):507-512, 1974.
* Taylor [2018] T. A. Taylor. On-demand service platforms. _Manufacturing & Service Operations Management_, 20(4):704-720, 2018.
* van den Burg and Williams [2022] G. J. J. van den Burg and C. K. I. Williams. An evaluation of change point detection algorithms, 2022. URL https://arxiv.org/abs/2003.06222.
* Xu and Yun [2020] K. Xu and S.-Y. Yun. Reinforcement with fading memories. _Mathematics of Operations Research_, 45(4):1258-1288, 2020.
* Zhou and Bambos [2015] Z. Zhou and N. Bambos. Wireless communications games in fixed and random environments. In _2015 54th IEEE Conference on Decision and Control (CDC)_, pages 1637-1642. IEEE, 2015.
* Zhou et al. [2016] Z. Zhou, P. Glynn, and N. Bambos. Repeated games for power control in wireless communications: Equilibrium and regret. In _2016 IEEE 55th Conference on Decision and Control (CDC)_, pages 3603-3610. IEEE, 2016.

**Supplementary Material**Learning through "Runlength" Inference

A simple example of the aforementioned approach would be to use a constant hazard function \(h\) taking some value in \((0,1)\). Given that, In the sense that \(p(r_{t}=0|r_{t-1})\) is independent of \(r_{t-1}\) and is constant, giving rise, a priori, to geometric inter-arrival times for change points (\(\mathbb{P}_{\text{change}}(s+1)=h\left(1-h\right)^{s}\)). Thus, the recursive runlength distribution computation becomes:

\[p(r_{t}\neq 0|\mathbf{x}_{1:t}) \propto(1-h)\,p(x_{t}|r_{t-1},\mathbf{x}_{1:t-1})p(r_{t-1}| \mathbf{x}_{1:t-1})\] \[p(r_{t}=0|\mathbf{x}_{1:t}) \propto h\sum_{r_{t-1}}p(x_{t}|r_{t-1},\mathbf{x}_{1:t-1})p(r_{t-1 }|\mathbf{x}_{1:t-1})\]

## Appendix B Future Work

We list a few directions for future work, as outlined in the discussion.

* Establishing potential theoretical optimality guarantees (for instance in terms of detection delay and false-alarm rate, akin to that in Alami et al. (2023) in the case where the underlying distribution is piecewise multinomial), given the particular algorithm construction we adopt.
* Scaling to online probabilistic (posterior-weighted) mixtures of forecasters, which would reduce the sensitivity to the prior choice of \(h\) in Algorithm 1 and would potentially allow to systematically increase the confidence predictions given an increase in resources.
* Given the increasing availability of ground-truth annotators such as that of van den Burg and Williams (2022), this would allow to potentially build hybrid algorithms on top where parts of the algorithm can be learned/tuned from past data.