# Policy Space Diversity for Non-Transitive Games

 Jian Yao\({}^{1}\), Weiming Liu \({}^{1}\), Haobo Fu\({}^{1}\), Yaodong Yang\({}^{2}\),

**Stephen McAleer\({}^{3}\)**, **Qiang Fu\({}^{1}\)**, **Wei Yang\({}^{1}\)**

\({}^{1}\)Tencent AI Lab, Shenzhen, China

\({}^{2}\)Peking University, Beijing, China

\({}^{3}\)Carnegie Mellon University

Equal contribution. Correspondence to: Haobo Fu (haobofu@tencent.com).

###### Abstract

Policy-Space Response Oracles (PSRO) is an influential algorithm framework for approximating a Nash Equilibrium (NE) in multi-agent non-transitive games. Many previous studies have been trying to promote policy diversity in PSRO. A major weakness in existing diversity metrics is that a more diverse (according to their diversity metrics) population does not necessarily mean (as we proved in the paper) a better approximation to a NE. To alleviate this problem, we propose a new diversity metric, the improvement of which guarantees a better approximation to a NE. Meanwhile, we develop a practical and well-justified method to optimize our diversity metric using only state-action samples. By incorporating our diversity regularization into the best response solving in PSRO, we obtain a new PSRO variant, _Policy Space Diversity_ PSRO (PSD-PSRO). We present the convergence property of PSD-PSRO. Empirically, extensive experiments on various games demonstrate that PSD-PSRO is more effective in producing significantly less exploitable policies than state-of-the-art PSRO variants. The experiment code is available at https://github.com/nigelyaoj/policy-space-diversity-psro.

## 1 Introduction

Most real-world games demonstrate strong non-transitivity [9], where the winning rule follows a cyclic pattern (e.g., the strategy cycle in Rock-Paper-Scissors) [6; 3]. A common objective in solving non-transitive games is to find a Nash Equilibrium (NE), which has the best worst-case performance in the whole policy space. Traditional algorithms, like simple self-play, fail to converge to a NE in games with strong non-transitivity [26]. Recently, many game-theoretic methods have been proposed to approximate a NE in such games. For example, Counterfactual Regret Minimization (CFR) [54] minimizes the so-called counterfactual regret. Neural fictitious self play [18; 19] extends the classical game-theoretic approach, Fictitious Play (FP) [4], to larger games using Reinforcement Learning (RL) to approximate a Best Response (BR). Another well-known algorithm is Policy-Space Response Oracles (PSRO) [26], which generalizes the double oracle approach [38] by adopting a RL subroutine to approximate a BR.

Improving the performance of PSRO on approximating a NE is an active research topic, and many PSRO variants have been proposed so far, which generally fall into three categories. The first category [36; 45; 12; 29] aims to improve the training efficiency at each iteration. For instance, pipeline-PSRO [36] trains multiple BRs in parallel at each iteration. Neural population learning [29] enables fast transfer learning across policies via representing a population of policies within a single conditional model. The second category [37; 53] incorporates no-regret learning into PSRO, which solves an unrestricted-restricted game with a no-regret learning method to guarantee the decrease of _exploitability_ of the meta-strategy across each iteration. The third category [2; 41; 32; 33]promotes policy diversity in the population, which is usually implemented by incorporating a diversity regularization term into the BR solving in the original PSRO.

Despite achieving promising improvements over the original PSRO, the theoretical reason why the diversity metrics in existing diversity-enhancing PSRO variants [2; 41; 32; 33] help PSRO in terms of approximating a NE is unclear. More specifically, those diversity metrics are 'justified' in the sense that adding the corresponding diversity-regularized BR strictly enlarges the _gamescape_. However, as we prove and demonstrate later in the paper, a population with a larger _gamescape_ is neither a sufficient nor a necessary condition for a better approximation (we will define the precise meaning later) to a NE. One fundamental reason is that _gamescape_ is a concept that varies significantly according to the choice of opponent policies. In contrast, _exploitability_ (the distance to a NE) measures the worst-case performance that is invariant to the choice of opponent policies.

In this paper, we seek for a new and better justified diversity metric that improves the approximation of a NE in PSRO. To achieve this, we introduce a new concept, named _Population Exploitability_ (PE), to quantify the strength of a population. The PE of a population is the optimal _exploitability_ that can be achieved by selecting a policy from its _Policy Hull_ (PH), which is simply a complete set of polices that are convex combinations of individual polices in the population. In addition, we show that a larger PH means a lower PE. Based on these insights, we make the following contributions:

* We point out a major and common weakness of existing diversity-enhancing PSRO variants: their goal of enlarging the _gamescape_ of the population in PSRO is somewhat deceptive to the extent that it can lead to a weaker population in terms of PE. In other words, a more diverse (according to their diversity metrics) population \(\Rightarrow\) a larger _gamescape_\(\Rightarrow\) closer to a full game NE.
* We develop a new diversity metric that encourages the enlargement of a population's PH. In addition, we develop a practical and well-justified method to optimize our diversity metric using only state-action samples. We then incorporate our diversity metric (as a regularization term) into the BR solving in the original PSRO and obtain a new algorithm: Policy Space Diversity PSRO (PSD-PSRO). Our method PSD-PSRO establishes the causality: a more diverse (according to our diversity metric) population \(\Rightarrow\) a larger PH \(\Rightarrow\) a lower PE \(\Rightarrow\) closer to a full game NE.
* We prove that a full game NE is guaranteed once PSD-PSRO is converged. In contrast, it is not clear, in other state-of-the-art diversity-enhancing PSRO variants [2; 41; 32; 33], whether a full game NE is found once they are converged in terms of their optimization objectives. Notably, PSRO\({}_{rN}\)[2] is not guaranteed to find a NE once converged [36].

## 2 Notations and Preliminary

### Extensive-form Games, NE, and Exploitability

Extensive-form games are used to model sequential interaction involving multiple agents, which can be defined by a tuple \(\langle\mathcal{N},\mathcal{S},P,\mathcal{A},u\rangle\). \(\mathcal{N}=\{1,2\}\) denotes the set of players (we focus on the two-player zero-sum games). \(\mathcal{S}\) is a set of information states for decision-making. Each information state node \(s\in\mathcal{S}\) includes a set of actions \(\mathcal{A}(s)\) that lead to subsequent information states. The player function \(P:\mathcal{S}\rightarrow\mathcal{N}\cup\{c\}\), with \(c\) denoting chance, determines which player takes action in \(s\). We use \(s_{i}\), \(\mathcal{S}_{i}=\{s\in\mathcal{S}|P(s)=i\}\), and \(\mathcal{A}_{i}=\cup_{s\in\mathcal{S}}\mathcal{A}(s)\) to denote player \(i\)'s state, set of states, and set of actions respectively. We consider games with _perfect recall_, where each player remembers the sequence of states to the current state.

A player's _behavioral strategy_ is denoted by \(\pi_{i}(s)\in\Delta(\mathcal{A}(s)),\forall s\in\mathcal{S}_{i}\), and \(\pi_{i}(a|s)\) is the probability of player \(i\) taking action \(a\) in \(s\). A _strategy profile_\(\pi=(\pi_{1},\pi_{2})\) is a pair of strategies for each player, and we use \(\pi_{-i}\) to refer to the strategy in \(\pi\) except \(\pi_{i}\). \(u_{i}(\pi)=u_{i}(\pi_{i},\pi_{-i})\) denotes the payoff for player \(i\) when both players follow \(\pi\). The BR of player \(i\) to the opponent's strategy \(\pi_{-i}\) is denoted by \(\mathcal{BR}(\pi_{-i})=\arg\max_{\pi_{i}^{\prime}}u_{i}(\pi_{i}^{\prime},\pi_{ -i})\). The _exploitability_ of strategy profile \(\pi\) is defined as:

\[\mathcal{E}(\pi)=\frac{1}{2}\sum_{i\in\mathcal{N}}[\max_{\pi_{i}^{\prime}}u_{i }(\pi_{i}^{\prime},\pi_{-i})-u_{i}(\pi_{i},\pi_{-i})].\] (1)

When \(\mathcal{E}(\pi)=0\), \(\pi\) is a NE of the game.

### Meta-Games, PH, and PSRO

Meta-games are introduced to represent games at a higher level. Denoting a population of mixed strategies for player \(i\) by \(\Pi_{i}:=\{\pi_{i}^{1},\pi_{i}^{2},...\}\), the payoff matrix on the joint population \(\Pi=\Pi_{i}\times\Pi_{-i}\) is denoted by \(\mathbf{M}_{\Pi_{i},\Pi_{-i}}\), where \(\mathbf{M}_{\Pi_{i},\Pi_{-i}}[j,k]:=u_{i}(\pi_{i}^{j},\pi_{-i}^{k})\). The meta-game on \(\Pi\) and \(\mathbf{M}_{\Pi_{i},\Pi_{-i}}\) is simply a normal-form game where selecting an action means choosing which \(\pi_{i}\) to play for player \(i\). Accordingly, we use \(\sigma_{i}\) (\(\sigma_{i}\) is called a meta-strategy and could be, e.g., playing \([\pi_{i}^{1},\pi_{i}^{2}]\) with probability \([0.5,0.5]\)) to denote a mixed strategy over \(\Pi_{i}\), i.e., \(\sigma_{i}\in\Delta_{\Pi_{i}}\). A meta-policy \(\sigma_{i}\) over \(\Pi_{i}\) can be viewed as a convex combination of polices in \(\Pi_{i}\), and we define the PH of a population \(\mathcal{H}(\Pi_{i})\) as the set of all convex combinations of the policies in \(\Pi_{i}\). Meta-games are often open-ended in the sense that there exist an infinite number of mixed strategies and that new policies will be successively added to \(\Pi_{i}\) and \(\Pi_{-i}\) respectively. We give a summary of notations in Appendix A.

PSRO operates on meta-games and consists of two components: an oracle and a meta-policy solver. At each iteration \(t\), PSRO maintains a population of policies, denoted by \(\Pi_{i}^{t}\), for each player \(i\). The joint meta-policy solver first computes a NE meta-policy \(\sigma^{t}\) on the restricted meta-game represented by \(\mathbf{M}_{\Pi_{i}^{t},\Pi_{-i}^{t}}\). Afterwards, for each player \(i\), the oracle computes an approximate BR (i.e., \(\pi_{i}^{t+1}\)) against the meta-policy \(\sigma_{-i}^{t}\): \(\pi_{i}^{t+1}\in\mathcal{BR}(\sigma_{-i}^{t})\). The new policy \(\pi_{i}^{t+1}\) is then added to its population (\(\Pi_{i}^{t+1}=\Pi_{i}^{t}\cup\{\pi_{i}^{t+1}\}\)), and the next iteration starts. In the end, PSRO outputs a meta-policy NE on the final joint population as an approximation to a full game NE.

### Previous Diversity Metrics for PSRO

**Effective diversity**[2] measures the variety of effective strategies (strategies with support under a meta-policy NE) and uses a rectifier to focus on how these effective strategies beat each other. Let \((\sigma_{i}^{*},\sigma_{-i}^{*})\) denote a meta-policy NE on \(\mathbf{M}_{\Pi_{i},\Pi_{-i}}\). The _effective diversity_ of \(\Pi_{i}\) is:

\[\mathrm{Div}(\Pi_{i})={\sigma_{i}^{*}}^{T}[\mathbf{M}_{\Pi_{i},\Pi_{-i}}]_{+} \sigma_{-i}^{*},\] (2)

where \(\lfloor x\rfloor_{+}:=x\) if \(\mathrm{x}\geq 0\;\mathrm{else}\;0\).

**Expected Cardinality**[41], inspired by the determinantal point processes [35], measures the diversity of a population \(\Pi_{i}\) as the expected cardinality of the random set \(\mathbf{Y}\) sampled according to \(det(\mathcal{L}_{\Pi})\):

\[\mathrm{Div}(\Pi_{i})=\mathbb{E}_{\mathbf{Y}\sim\mathbb{P}_{\mathcal{L}_{\Pi}} }[|\mathbf{Y}|]=\mathrm{Tr}(\mathbf{I}-(\mathcal{L}_{\Pi}+\mathbf{I})^{-1}),\] (3)

where \(|\mathbf{Y}|\) is the cardinality of \(\mathbf{Y}\), and \(\mathcal{L}_{\Pi}=\mathbf{M}_{\Pi_{i},\Pi_{-i}}\mathbf{M}^{T}_{\Pi_{i},\Pi_{-i}}\).

**Convex Hull Enlargement**[32] builds on the idea of enlarging the convex hull of all row vectors in the payoff matrix:

\[\mathrm{Div}(\Pi_{i}\cup\{\pi_{i}^{\prime}\})=\min_{\mathbf{1}^{T}\beta=1, \beta\geq 0}||\mathbf{M}_{\Pi_{i},\Pi_{-i}}^{T}\beta-\mathbf{m}||,\] (4)

where \(\pi_{i}^{\prime}\) is the new strategy to add, and \(\mathbf{m}\) is the payoff vector of policy \(\pi_{i}^{\prime}\) against each opponent policy in \(\Pi_{-i}\): \(\mathbf{m}[j]=u_{i}(\pi_{i}^{\prime},\pi_{-i}^{j})\).

**Occupancy Measure Mismatching**[32] considers the state-action distribution \(\rho_{\pi}(s,a)\) induced by a joint policy \(\pi\). When considering adding a new policy \(\pi_{i}^{\prime}\), the corresponding diversity metric is:

\[\mathrm{Div}(\Pi_{i}\cup\{\pi_{i}^{\prime}\})=D_{f}(\rho_{(\pi_{i}^{\prime}, \sigma_{-i}^{*})}||\rho_{(\sigma_{i}^{*},\sigma_{-i}^{*})}),\] (5)

where \(\pi_{i}^{\prime}\) is the new policy to add; \((\sigma_{i}^{*},\sigma_{-i}^{*})\) is a meta-policy NE on \(\mathbf{M}_{\Pi_{i},\Pi_{-i}}\), and \(D_{f}\) is a general \(f\)-divergence between two distributions. It is worth noting that Equation 5 only considers the difference between two policies (\(\pi_{i}^{\prime}\) and \(\sigma_{i}^{*}\)), instead of \(\pi_{i}^{\prime}\) and \(\Pi_{i}\). In practice, this diversity metric is used together with the **convex hull enlargement** in [32].

**Unified Diversity Measure**[33] offers a unified view on existing diversity metrics and is defined as:

\[\mathrm{Div}(\Pi_{i})=\sum_{m=1}^{|\Pi_{i}|}f(\lambda_{m}),\] (6)

where \(f\) takes different forms for different existing diversity metrics; \(\lambda_{m}\) is the eigenvalues of \([K(\phi_{m},\phi_{n})]_{|\Pi_{i}|\times|\Pi_{i}|};K(\cdot,\cdot)\) is a predefined kernel function; and \(\phi_{m}\) is the strategy feature for the \(m\)-th policy in \(\Pi_{i}\). It is worth mentioning that only payoff vectors in \(\mathbf{M}_{\Pi_{i},\Pi_{-i}}\) were investigated for the strategy feature of the new diversity metric proposed in [33].

A Common Weakness of Existing Diversity-Enhancing PSRO Variants

As shown in last section, all previous diversity-enhancing PSRO variants [2; 41; 32; 33] try to enlarge the _gamescape_ of \(\Pi_{i}\), which is the convex hull of the rows in the empirical payoff matrix:

\[\mathcal{GS}(\Pi_{i}|\Pi_{-i}):=\{\sum_{j}\alpha_{j}\mathbf{m}_{j}:\boldsymbol {\alpha}\geq 0,\boldsymbol{\alpha}^{T}\mathbf{1}=1\},\]

where \(\mathbf{m}_{j}\) is the \(j\)-th row vector in \(\mathbf{M}_{\Pi_{i},\Pi_{-i}}\). However, the _gamescape_ of a population depends on the choice of opponent policies, and two policies with the same payoff vector are not necessarily the same. Moreover, enlarging the _gamescape_ without careful tuning would encourage the current player to deliberately lose to the opponent to get 'diverse' payoffs. We suspect this might be the reason why the optimization of the _gamescape_ is activated later in the training procedure in [32]. More importantly, it is not theoretically clear from previous diversity-enhancing PSRO variants why enlarging the _gamescape_ would help in approximating a full game NE in PSRO.

To rigorously answer the question whether a diversity metric is helpful in approximating a NE in PSRO, we need a performance measure to monitor the progress of PSRO across iterations in terms of finding a full game NE. In other words, we need to quantify the strength of a population of policies. Previously, the _exploitability_ of a meta NE of the joint population is usually employed to monitor the progress of PSRO. Yet, as demonstrated in [37], this _exploitability_ may increase after an iteration. Intuitively, a better alternative is the _exploitability_ of the least exploitable mixed strategy supported by a population. We define this _exploitability_ as the _population exploitability_:

**Definition 3.1**.: For a joint population \(\Pi=\Pi_{i}\times\Pi_{-i}\), let \((\sigma_{i}^{*},\sigma_{-i}^{*})\) be a meta NE on \(\mathbf{M}_{\Pi_{i},\Pi_{-i}}\). The _relative population performance_ of \(\Pi_{i}\) against \(\Pi_{-i}\)[2] is:

\[\mathcal{P}_{i}(\Pi_{i},\Pi_{-i})={\sigma_{i}^{*}}^{T}\mathbf{M}_{\Pi_{i},\Pi _{-i}}\sigma_{-i}^{*}.\] (7)

The _population exploitability_ of the joint population \(\Pi\) is defined as:

\[\mathcal{PE}(\Pi)=\frac{1}{2}\sum_{i=1,2}\max_{\Pi_{i}^{\prime}\subseteq\Omega_ {i}}\mathcal{P}_{i}(\Pi_{i}^{\prime},\Pi_{-i}),\] (8)

where \(\Omega_{i}\) is the full set of all possible mixed strategies of player \(i\).

We notice that PE is equal to the sum of negative _population effectivity_ defined in [32]. Yet, we prefer PE as it is more of a natural extension to _exploitability_ in Equation 1. Some properties of PE and its relation to PH are presented in the following.

**Proposition 3.2**.: _Considering a joint population \(\Pi=\Pi_{i}\times\Pi_{-i}\), we have:_

1. \(\mathcal{PE}(\Pi)\geq 0\)_,_ \(\forall\,\Pi\)_._
2. _For another joint population_ \(\Pi^{\prime}\)_, if_ \(\mathcal{H}(\Pi_{i})\times\mathcal{H}(\Pi_{2})\subseteq\mathcal{H}(\Pi_{i}^{ \prime})\times\mathcal{H}(\Pi_{-i}^{\prime})\)_, then_ \(\mathcal{PE}(\Pi)\geq\mathcal{PE}(\Pi^{\prime})\)_._
3. _If_ \(\Pi_{i}=\{\pi_{i}\}\) _and_ \(\Pi_{-i}=\{\pi_{-i}\}\)_, then_ \(\mathcal{PE}(\Pi)=\mathcal{E}(\pi)\)_, where_ \(\pi=(\pi_{i},\pi_{-i})\)_._
4. \(\exists\pi=(\pi_{i},\pi_{-i})\in\mathcal{H}(\Pi_{i})\times\mathcal{H}(\Pi_{-i})\) _s.t._ \(\mathcal{E}(\pi)=\mathcal{PE}(\Pi)=\min_{\pi^{\prime}\in\mathcal{H}(\Pi_{i}) \times\mathcal{H}(\Pi_{-i})}\mathcal{E}(\pi^{\prime})\)_._
5. _Let_ \((\sigma_{i}^{*},\sigma_{-i}^{*})\) _denote an arbitrary NE of the full game._ \(\mathcal{PE}(\Pi)=0\) _if and only if_ \((\sigma_{i}^{*},\sigma_{-i}^{*})\in\mathcal{H}(\Pi_{i})\times\mathcal{H}(\Pi_{ -i})\)_._

The proof is in Appendix B.2. Once we use PE to monitor the progress of PSRO, we have:

**Proposition 3.3**.: _The PE of the joint population \(\Pi^{t}\) at each iteration \(t\) in PSRO is monotonically decreasing and will converge to \(0\) in finite iterations for finite games. Once \(\mathcal{PE}(\Pi^{T})=0\), a meta NE on \(\Pi^{T}\) is a full game NE._

The proof is in Appendix B.3. From Proposition 3.2 and 3.3, we are convinced that PE is indeed an appropriate performance measure for populations of polices. Using PE, we can now formally present why enlarging the _gamescape_ of the population in PSRO is somewhat deceptive:

**Theorem 3.4**.: _The enlargement of the gamescape is neither sufficient nor necessary for the decrease of PE. Considering two populations (\(\Pi_{i}^{1}\) and \(\Pi_{i}^{2}\)) for player \(i\) and one population \(\Pi_{-i}\) for player \(-i\), and denoting \(\Pi^{j}=\Pi_{i}^{j}\times\Pi_{-i}\), \(j=1,2\) we have_

\[\mathcal{GS}(\Pi_{i}^{1}|\Pi_{-i})\varsubsetneq\mathcal{GS}(\Pi_{i}^{2}|\Pi_{ -i})\nRightarrow\mathcal{PE}(\Pi^{1})\geq\mathcal{PE}(\Pi^{2})\] \[\mathcal{PE}(\Pi^{1})\geq\mathcal{PE}(\Pi^{2})\nRightarrow \mathcal{GS}(\Pi_{i}^{1}|\Pi_{-i})\varsubsetneq\mathcal{GS}(\Pi_{i}^{2}|\Pi_{ -i})\] (9)The proof of Theorem 3.4 is in Appendix B.5, where we provide concrete examples.

In other words, enlarging the gamescape in either short term or long term does not necessarily lead to a better approximation to a full game NE.

## 4 Policy Space Diversity PSRO

In this section, we develop a new diversity-enhancing PSRO variant, i.e., PSD-PSRO. In contrast to methods that enlarge the _gamescape_, PSD-PSRO encourages the enlargement of PH of a population, which helps reduce a population's PE (according to Proposition 3.2). In addition, we develop a well-justified state-action sampling method to optimize our diversity metric in practice. Finally, we present the convergence property of PSD-PSRO and discuss its relation to the original PSRO.

### A New Diversity Regularization Term for PSRO

Our purpose of promoting diversity in PSRO is to facilitate the convergence to a full game NE. We follow the conventional scheme in previous diversity-enhancing PSRO variants [41; 32; 33], which introduces a diversity regularization term to the BR solving in PSRO. Nonetheless, our diversity regularization encourages the enlargement of PH of the current population, which is in contrast to the enlargement of _gamescape_ in previous methods [2; 41; 32; 33]. We thus name our diversity metric _policy space diversity_. Intuitively, the larger the PH of a population is, the more likely it will include a full game NE. More formally, a larger PH means a lower PE (Proposition 3.2), which means our diversity metric avoids the common weakness (Section 3) of existing ones.

Recall that the PH of a population is simply the complete set of polices that are convex combinations of individual polices in the population. To quantify the contribution of a new policy to the enlargement of the PH of the current population, a straightforward idea is to maximize a distance between the new policy and the PH. Such a distance should be \(0\) for any policy that belongs to the PH and greater than \(0\) otherwise. Without loss of generality, the distance between a policy and a PH could be defined as the minimal distance between the policy and any policy in the PH. We can now write down the diversity regularized BR solving objective in PSD-PSRO, where at each iteration \(t\) for player \(i\) we add a new policy \(\pi_{i}^{t+1}\) by solving:

\[\pi_{i}^{t+1}=\arg\max_{\pi_{i}}\left\{u(\pi_{i},\sigma_{-i}^{t})+\lambda\min _{\pi_{i}^{k}\in\mathcal{H}(\Pi_{i}^{t})}\mathrm{dist}(\pi_{i},\pi_{i}^{k}) \right\},\] (10)

where \(\sigma_{-i}^{t}\) is the opponent's meta NE policy at the \(t\)-th iteration, \(\lambda\) is a Lagrange multiplier, and \(\mathrm{dist}(\cdot,\cdot)\) is a distance function (will be specified in the next subsection) between two polices.

### Diversity Optimization in Practice

To be able to optimize our diversity metric (the right part in Equation 10) in practice, we need to encode a policy into some representation space and specify a distance function there. Such a representation should be a one-to-one mapping between a policy and its representation. Also, to ensure that enlarging the convex hull in the representation space results in the enlargement of the PH, we require the representation to satisfy the linearity property. Formally, we have the following definition:

**Definition 4.1**.: A _fine policy representation_ for our purpose is a function \(\rho:\Pi_{i}\to R^{N_{i}}\), which satisfies the following two properties:

* (bijection) For any representation \(\rho(\pi_{i})\), there exists a unique behavior policy \(\pi_{i}\) whose representation is \(\rho(\pi_{i})\), and vice-versa.
* (linearity) For any two policies (\(\pi_{i}^{j}\) and \(\pi_{i}^{k}\)) and \(\alpha\) (\(0\leq\alpha\leq 1\)), the following holds: \[\rho(\alpha\pi_{i}^{j}+(1-\alpha)\pi_{i}^{k})=\alpha\rho(\pi_{i}^{j})+(1- \alpha)\rho(\pi_{i}^{k}),\] where \(\alpha\pi_{i}^{j}+(1-\alpha)\pi_{i}^{k}\) means playing \(\pi_{i}^{j}\) with probability \(\alpha\) and \(\pi_{i}^{k}\) with probability \((1-\alpha)\).

Existing diversity metrics explicitly or implicitly define a policy representation [33]. For instance, the _gamescape_-based methods [2, 41, 32, 33] represent a policy using its payoff vector against the opponent's population. Yet, this representation is not a _fine policy representation_ as it is not a bijection (different policies can have the same payoff vector). The (joint) occupancy measure, which is a _fine policy representation_, is usually used to encode a policy in the RL community [48, 20, 32].The \(f\)-divergence is then employed to measure the distance between two policies [32, 24, 16]. However, computing the \(f\)-divergence based on the occupancy measure is usually intractable and often in practice roughly approximated using the prediction of neural networks [32, 24, 16].

Instead, we use another _fine policy representation_, i.e., the sequence-form representation [21, 11, 31, 27, 1], which was originally developed for representing a policy in multi-agent games. We then define the distance between two policies using the Bregman divergence, which can be further simplified to a tractable form and optimized using only state-action samples in practice.

The sequence-form representation of a policy remembers the realization probability of reaching a state-action pair. We follow the definition in [11, 31], where the sequence form representation \(\bm{x}_{i}\in\mathcal{X}_{i}\subseteq[0,1]^{|\mathcal{S}_{i}\times\mathcal{A }_{i}|}\) of \(\pi_{i}\) is a vector:

\[\bm{x}_{i}(s,a)=\prod_{\widetilde{s}_{i},\widetilde{a}\in\tau(s,a)}\pi_{i}( \widetilde{a}|\widetilde{s}_{i}),\] (11)

where \(\tau(s,a)\) is a trajectory from the beginning to \((s,a)\). By the perfect-recall assumption, there is a unique \(\tau\) that leads to \((s,a)\). The policy \(\pi_{i}\) can be written as \(\pi_{i}(a|s)=\bm{x}_{i}(s,a)/\|\bm{x}_{i}(s)\|_{1}\), where \(\bm{x}_{i}(s)\) is \((\bm{x}_{i}(s,a_{1}),\dots,\bm{x}_{i}(s,a_{n}))\) with \(a_{1},\dots,a_{n}\in\mathcal{A}(s)\). Unlike the payoff vector representation or the occupancy measure representation, \(\bm{x}_{i}\) is independent of the opponent's policy as well as the environmental dynamics. Therefore, it should be more appropriate in representing a policy for the diversity optimization. Without loss of generality and following [21, 11, 31], we define the distance \(\operatorname{dist}(\pi_{i},\pi_{i}^{\prime})\) between two policies as the Bregman divergence on the sequence form representation \(\mathcal{B}_{d}(\bm{x}_{i}\|\bm{x}_{i}^{\prime})\), which can be further written in terms of state-action pairs (the derivation is presented in Appendix B.1) in the following:

\[\operatorname{dist}(\pi_{i},\pi_{i}^{\prime}):=\mathcal{B}_{d}(\bm{x}_{i}\| \bm{x}_{i}^{\prime})=\sum_{s,a\in\mathcal{S}_{i}\times\mathcal{A}_{i}}\left( \prod_{\widetilde{s}_{i},\widetilde{a}\in\tau(s,a)}\pi_{i}(\widetilde{a}| \widetilde{s}_{i})\right)\beta_{s}\mathcal{B}_{d_{s}}(\pi_{i}(s)\|\pi_{i}^{ \prime}(s)),\] (12)

where \(\mathcal{B}_{d_{s}}(\pi_{i}(s)\|\pi_{i}^{\prime}(s))\) is the Bregman divergence between \(\pi_{i}(s)\) and \(\pi_{i}^{\prime}(s)\). In our experiment, we let \(\mathcal{B}_{d_{s}}(\pi_{i}(s)\|\pi_{i}^{\prime}(s))=\sum_{a}\pi_{i}(a|s)\log \pi_{i}(a|s)/\pi_{i}^{\prime}(a|s)=\operatorname{KL}(\pi_{i}(s)\|\pi_{i}^{ \prime}(s))\), i.e., the KL divergence. In previous work [21, 11, 31], the coefficient \(\beta_{s}\) usually declines monotonically as the length of the sequence increases. In our case, we make \(\beta_{s}\) depend on an opponent's policy \(b_{-i}\): \(\beta_{s}=\prod_{\widetilde{s}_{-,i},\widetilde{a}\in\tau(s,a)}b_{-i}( \widetilde{a}|\widetilde{s}_{-i})\)2. This weighting method allows us to estimate the distance using the sampled average KL divergence and avoids importance sampling:

Footnote 2: Assume \(\beta_{s}>0\), i.e., \(b_{-i}(a|s_{-i})>0,\forall(s_{-i},a)\in\mathcal{S}_{-i}\times\mathcal{A}_{-i}\).

\[\operatorname{dist}(\pi_{i},\pi_{i}^{\prime})= \sum_{s,a\in\mathcal{S}_{i}\times\mathcal{A}_{i}}\left(\prod_{ \widetilde{s}_{i},\widetilde{a}\in\tau(s,a)}\pi_{i}(\widetilde{a}|\widetilde{ s}_{i})\right)\left(\prod_{\widetilde{s}_{-,i},\widetilde{a}\in\tau(s,a)}b_{-i}( \widetilde{a}|\widetilde{s}_{-i})\right)\mathcal{B}_{d_{s}}(\pi_{i}(s)\|\pi_{i} ^{\prime}(s))\] \[= \mathbb{E}_{s_{i}\sim\pi_{i},b_{-i}}[\operatorname{KL}(\pi_{i}(s _{i})\|\pi_{i}^{\prime}(s_{i}))],\] (13)

where \(s_{i}\sim\pi_{i},b_{-i}\) means sampling player \(i\)'s information states from the trajectories that are collected by playing \(\pi_{i}\) against \(b_{-i}\). As a result, we can rewrite the diversity regularized BR solving objective in PSD-PSRO as follows:

\[\pi_{i}^{t+1}=\arg\max_{\pi_{i}}\left\{u(\pi_{i},\sigma_{-i}^{t})+\lambda\min_{ \pi_{i}^{t}\in\mathcal{H}(\Pi_{i}^{t})}\mathbb{E}_{s_{i}\sim\pi_{i},b_{-i}}[ \operatorname{KL}(\pi_{i}(s_{i})\|\pi_{i}^{k}(s_{i}))]\right\}.\] (14)

Now we provide a practical way to optimize Equation 14. By regarding the opponent and chance as the environment, we can use the policy gradient method [47, 43, 44] in RL to train \(\pi_{i}^{t+1}\). Denote the probability of generating \(\tau\) by \(\pi_{i}(\tau)\), and the payoff of player \(i\) for the trajectory \(\tau\) by \(R(\tau)\). Let \(\pi_{i}^{min}=\arg\min_{\pi_{i}^{k}\in\mathcal{H}(\Pi_{i}^{t})}\mathbb{E}_{s_{i }\sim\pi_{i},b_{-i}}[\operatorname{KL}(\pi_{i}(s_{i})\|\pi_{i}^{k}(s_{i}))]\) be the policy in \(\mathcal{H}(\Pi_{i}^{t})\) that minimizes the distance and let \(R^{KL}(\tau)=\mathbb{E}_{s_{i}\sim\tau}[\operatorname{KL}(\pi_{i}(s_{i})\|\pi_{i}^ {min}(s_{i}))]\).

The gradient of the first term \(u(\pi_{i},\sigma^{t}_{-i})\) in Equation 14 with respect to \(\pi_{i}\) can be written as:

\[\nabla u(\pi_{i},\sigma^{t}_{-i})= \nabla\int\pi_{i}(\tau)R(\tau)\mathrm{d}\tau=\int\nabla\pi_{i}( \tau)\mathrm{R}(\tau)\mathrm{d}\tau\] \[= \int\pi_{i}(\tau)\nabla\log\pi_{i}(\tau)R(\tau)\mathrm{d}\tau\] \[= \mathbb{E}_{\tau\sim\pi_{i},\sigma^{t}_{-i}}[\nabla\log\pi_{i}( \tau)R(\tau)].\] (15)

The gradient of the diversity term in Equation 14 with respect to \(\pi_{i}\) can be written as:

\[\nabla_{\pi_{i}}\min_{\pi_{i}^{k}\in\mathcal{H}(\Pi^{t}_{i})} \mathbb{E}_{s_{i}\in\tau,\tau\sim\pi_{i},b_{-i}}[\mathrm{KL}(\pi_{i}(s_{i}) \|\pi_{i}^{k}(s_{i}))]\] \[= \nabla\mathbb{E}_{s_{i}\in\tau,\tau\sim\pi_{i},b_{-i}}[\mathrm{ KL}(\pi_{i}(s_{i})\|\pi_{i}^{min}(s_{i}))]\] \[= \nabla\int\pi_{i}(\tau)R^{KL}(\tau)\mathrm{d}\tau\] \[= \int\nabla\pi_{i}(\tau)R^{KL}(\tau)\mathrm{d}\tau+\int\pi_{i}( \tau)\mathbb{E}_{s_{i}\sim\tau}[\nabla\,\mathrm{KL}(\pi_{i}(s_{i})\|\pi_{i}^{ min}(s_{i}))]\mathrm{d}\tau\] \[= \mathbb{E}_{\tau\sim\pi_{i},b_{-i}}[\nabla\log\pi_{i}(\tau)R^{KL} (\tau)]+\mathbb{E}_{s_{i}\sim\pi_{i},b_{-i}}[\nabla\,\mathrm{KL}(\pi_{i}(s_{i} )\|\pi_{i}^{min}(s_{i}))],\] (16)

where we use the property that \(\nabla\mathbb{E}[KL(\pi_{i}(s_{i})|\pi_{i}^{min}(s_{i})]\in\partial\min_{\pi_{ i}^{k}}\mathbb{E}[KL(\pi_{i}(s_{i})|\pi_{i}^{k}(s_{i})]\), whose correctness can be shown from the following proposition:

**Proposition 4.2**.: _For any local Lipschitz continuous function \(f(x,y)\), assume \(\forall x,\min_{y}f(x,y)\) exists, then \(\partial_{x}f(x,y)|_{y\in\arg\min f(x,y)}\in\partial_{x}\min_{y}f(x,y)\), where \(\partial f\) is the generalized gradient [7]._

Proof.: According to Theorem 2.1 (property (4) in [7], the result is immediate, as \(\partial_{x}\min_{y}f(x,y)\) is the convex hull of \(\{\partial_{x}f(x,y)|y\in\arg\max g(x,y)\}\). 

Combining the above two equations, we have,

\[\nabla_{\pi_{i}}\left(u(\pi_{i},\sigma^{t}_{-i})+\lambda\min_{ \pi_{i}^{k}\in\mathcal{H}(\Pi^{t}_{i})}\mathbb{E}_{s_{i}\sim\pi_{i},b_{-i}}[ \mathrm{KL}(\pi_{i}(s_{i})\|\pi_{i}^{k}(s_{i}))]\right)\] (17) \[= \mathbb{E}_{\tau\sim\pi_{i},\sigma^{t}_{-i}}[\nabla\log\pi_{i}( \tau)R(\tau)]+\lambda\mathbb{E}_{\tau\sim\pi_{i},b_{-i}}[\nabla\log\pi_{i}( \tau)R^{KL}(\tau)]\] \[+\lambda\mathbb{E}_{s_{i}\sim\pi_{i},b_{-i}}[\nabla\,\mathrm{KL} (\pi_{i}(s_{i})\|\pi_{i}^{min}(s_{i}))].\]

According to Equation 17, we can see that optimizing \(\pi_{i}^{t+1}\) requires maximizing two types of rewards: \(R(\tau)\) and \(R^{KL}(\tau)\). This can be done by averaging the gradients using samples that are generated by playing \(\pi_{i}\) against \(\sigma^{t}_{-i}\) and \(b_{-i}\) separately. The last term in Equation 17 is easily estimated by sampling states in the sampled trajectories via playing \(\pi_{i}\) against \(b_{-i}\). For training efficiency, we simply set \(b_{-i}=\sigma^{t}_{-i}\) in our experiments, although other settings of \(b_{-i}\) are possible, e.g., the uniform random policy. We also want to emphasize that we optimize \(\pi_{i}^{t+1}\) for each iteration \(t\), and \(\sigma^{t}_{-i}\) is fixed during an iteration and thus can be viewed as a part of the environment. Finally, to estimate the distance between \(\pi_{i}\) and \(\pi_{i}^{min}\), we should ideally compute \(\pi_{i}^{min}\) exactly first by solving a convex optimization problem. In practice, we find sampling the policies in \(\mathcal{H}(\Pi^{t}_{i})\) and using the minimal sampled distance already gives us satisfactory performance. The pseudo-code of PSD-PSRO is provided in Appendix C.

### The Convergence Property of PSD-PSRO

We first show how the PH evolves in the original PSRO:

**Proposition 4.3**.: _Before the PE of the joint population reaches \(0\), adding a BR to the meta NE policy from last iteration in PSRO will strictly enlarge the PH of the current population._

The proof is in Appendix B.3. From Proposition 4.3, we can see that adding a BR serves one way (an implicit way) of enlarging the PH and hence reducing the PE. In contrast, the optimization of our diversity term in Equation 14 aims to explicitly enlarge the PH. In other words, adding a diversity regularized BR in PSD-PSRO serves as a mixed way of enlarging the PH. More formally, we have the following theorem:

**Theorem 4.4**.: _(1) In PSD-PSRO, before the PE of the joint population reaches \(0\), adding an optimal solution in Equation 14 will strictly enlarge the PH and hence reduce the PE. (2) Once the PH can not be enlarged (i.e., PSD-PSRO converges) by adding an optimal solution in Equation 14, the PE reaches 0, and PSD-PSRO finds a full game NE._

The proof is in Appendix B.6. In terms of convergence property, one significant benefit of PSD-PSRO over other state-of-the-art diversity-enhancing PSRO variants [2; 41; 32; 33] is the convergence of PH guarantees a full game NE in PSD-PSRO. Yet, it is not clear in those papers whether a full game NE is found once the PH of their populations converge. Notably, \(\text{PSRO}_{rN}\)[2] is not guaranteed to find a NE once converged [36]. In practice, we expect a significant performance improvement of PSD-PSRO over PSRO in approximating a NE. As for different games, there might exist different optimal trade-offs between 'exploitation' (adding a BR) and 'exploration' (optimizing our diversity metric) in enlarging the PH and reducing the PE. In other words, PSD-PSRO generalizes PSRO (a PSD-PSRO instance when \(\lambda\) = 0) in ways of enlarging the PH and reducing the PE.

## 5 Related Work

Diversity has been widely studied in evolutionary computation [13], with a central focus that mimics the natural evolution process. One of the ideas is novelty search [28], which searches for policies that lead to novel outcomes. By hybridizing novelty search with a fitness objective, quality-diversity [42] aims for diverse behaviors of good qualities. Despite these methods achieving good empirical results [8; 23], the diversity metric is often hand-crafted for different tasks.

Promoting diversity is also intensively studied in RL. By adding a distance regularization between the current policy and a previous policy, a diversity-driven approach has been proposed for good exploration [22]. Unsupervised learning of diverse policies [10] has been studied to serve as an effective pretraining mechanism for downstream RL tasks. A diversity metric based on DPP [39] has been proposed to improve exploration in population-based training. Diverse behaviors were learned in order to improve generalization ability for test environments that are different from training [25]. A diversity-regularized collaborative exploration strategy has been proposed in [40]. Reward randomization [49] has been employed to discover diverse strategies in multi-agent games. Trajectory diversity has been studied for better zero-shot coordination in a multi-agent environment [34]. Quality-similar diversity has been investigated in [51].

Diversity also plays a role in game-theoretic methods. Smooth FP [17] adds a policy entropy term when finding a BR. \(\text{PSRO}_{rN}\)[2] encourages effective diversity, which considers amplifying the strength over the weakness in a policy. DPP-PSRO [41] introduces a diversity metric based on DPP and provides a geometric interpretation of behavioral diversity. BD&RD-PSRO [32] combines the occupancy measure mismatch and the diversity on payoff vectors as a unified diversity metric. UDM [33] summarizes existing diversity metrics, by providing a unified diversity framework. In both opponent modeling [15] and opponent-limited subgame solving [30], diversity has been shown to have a large impact on the performance.

Figure 1: (a): _Exploitability_ of the meta NE. (b): PE of the joint population.

## 6 Experiments

The main purpose of the experiments is to compare PSD-PSRO with existing state-of-the-art PSRO variants in terms of approximating a full game NE. The baseline methods include PSRO [26], Pipeline-PSRO (P-PSRO) [36], PSRO\({}_{rN}\)[2], DPP-PSRO [41], and BD&RD-PSRO [32]. The benchmarks consist of single-state games (AlphaStar888 and non-transitive mixture game) and complex extensive games (Leduc poker and Goofspiel). For AlphaStar888 and Leduc poker, we report the _exploitability_ of the meta NE and the PE of the joint population through the training process. For the non-transitive mixture game, we illustrate the 'diversity' of the population and report the final _exploitability_. For Goofspiel where the exact _exploitability_ is intractable, we report the win rate between the final agents. In addition, we illustrate how the PH evolves for each method using the Disc game [2] in Appendix D.3, where PSD-PSRO is more effective at enlarging the PH and approximating a NE. An ablation study on \(\lambda\) of PSD-PSRO in Appendix D.2 reveals that, for different benchmarks, an optimal trade-off between 'exploitation' and 'exploration' in enlarging the PH to approximate a NE usually happens when \(\lambda\) is greater than zero. Error bars or stds in the results are obtained via \(5\) independent runs. In Appendix D.3, we also investigate the time cost of calculating our policy space diversity. More details for the environments and hyper-parameters are given in Appendix E.

**AlphaStar888** is an empirical game generated from the process of solving Starcraft II [50], which contains a payoff table for \(888\) RL policies. be viewed as a zero-sum symmetric two-player game where there is only one state \(s_{0}\). In \(s_{0}\), there are 888 legal actions. Any mixed strategy is a discrete probability distribution over the 888 actions. Hence, the distance function in Equation 14 for AlphaStar888 reduces to the KL divergence between two 888-dim discrete probability distributions.

In Figure 1, we can see that PSD-PSRO is more effective at reducing both the _exploitability_ and PE than other methods.

**Non-Transitive Mixture Game** consists of \(7\) equally-distanced Gaussian humps on the \(2\)D plane. Each strategy can be represented by a point on the \(2\)D plane, which is equivalent to the weights (the likelihood of that point in each Gaussian distribution) that each player puts on the humps. The optimal strategy is to stay close to the center of the Gaussian humps and explore all the distributions. In Figure 2, we show the exploration trajectories for different methods during training, where PSRO and PSRO\({}_{rN}\) get trapped and fail in this non-transitive game. In contrast, PSD-PSRO tends to find diverse strategies and explore all Gaussians. Also, the _exploitability_ of the meta NE of the final population is significantly lower in PSD-PSRO than others.

Figure 3: (a): _Exploitability_ of the meta NE. (b): PE of the joint population.

Figure 2: Non-Transitive Mixture Game. Exploration trajectories during training. For each method, the final _exploitability_\(\times 100\) (Exp) is reported at the bottom.

**Leduc Poker** is a simplified poker [46], where the deck consists of two suits with three cards in each suit. Each player bets one chip as an ante, and a single private card is dealt to each player. Since DPP-PSRO cannot scale to the RL setting [32] and the code of BD&RD-PSRO for complex games is not available, we compare PSD-PSRO only to P-PSRO, PSRO\({}_{FN}\) and PSRO. As demonstrated in Figure 3, PSD-PSRO is more effective at reducing both the _exploitability_ and PE.

**Goofspiel** is commonly used as a large-scale multi-stage simultaneous move game. Goofspiel features strong non-transitivity, as every pure strategy can be exploited by a simple counter-strategy. We compare PSD-PSRO with PSRO, P-PSRO, and PSRO\({}_{FN}\) on Goofspiel with 5 point cards and 8 point cards settings. In the game with 5 point cards setting, due to the relatively small game size, we can calculate the exact _exploitability_. We report the results in Appendix D.1, in which we see that PSD-PSRO reduces the _exploitability_ more effectively than other methods. In the game with 8 point cards setting, the game size is too large to show exact _exploitability_ for each iteration. In this setting, we provide a comparison among final solutions produced by different methods. We report the win rate between each two methods in Table 1, where we can see that PSD-PSRO consistently beats existing methods with a \(62\%\) win rate on average.

## 7 Conclusions and Limitations

In this paper, we point out a major and common weakness of existing diversity metrics in previous diversity-enhancing PSRO variants, which is their goal of enlarging the _gamescape_ does not necessarily result in a better approximation to a full game NE. Based on the insight that a larger PH means a lower PE (a better approximation to a NE), we develop a new diversity metric (_policy space diversity_) that explicitly encourages the enlargement of a population's PH. We then develop a practical method to optimize our diversity metric using only state-action samples, which is derived based on the Bregman divergence on the sequence form of policies. We incorporate our diversity metric into the BR solving in PSRO to obtain PSD-PSRO. We present the convergence property of PSD-PSRO, and extensive experiments demonstrate that PSD-PSRO is significantly more effective in approximating a NE than state-of-the-art PSRO variants.

The diversity regularization term \(\lambda\) in PSD-PSRO plays an important role in balancing the 'exploitation' and 'exploration' in terms of enlarging the PH to approximate a NE. In this paper, other than showing that different problems have different optimal settings of \(\lambda\), we have not discussed about the guidance of choosing \(\lambda\) optimally. Although there has been related work on how to adapt a diversity regularization term using online bandits [39], future work is still needed on how our _policy space diversity_ could benefit the approximation of a full game NE most. Another interesting direction of future work is to extend PSD-PSRO to larger scale games, such as poker [5], Mahjong [14], and dark chess [52].

## References

* [1] Yu Bai, Chi Jin, Song Mei, and Tiancheng Yu. Near-optimal learning of extensive-form games with imperfect information. In _International Conference on Machine Learning_, pages 1337-1382. PMLR, 2022.
* [2] David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech Czarnecki, Julien Perolat, Max Jaderberg, and Thore Graepel. Open-ended learning in symmetric zero-sum games. In _International Conference on Machine Learning_, pages 434-443. PMLR, 2019.
* [3] David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel. The mechanics of n-player differentiable games. In _International Conference on Machine Learning_, pages 354-363. PMLR, 2018.
* [4] George W Brown. Iterative solution of games by fictitious play. _Act. Anal. Prod Allocation_, 13(1):374, 1951.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline  & PSRO & PSRO\({}_{FN}\) & P-PSRO & PSD-PSRO(Ours) \\ \hline PSRO & - & 0.613\(\pm\)0.019 & 0.469\(\pm\)0.034 & 0.422\(\pm\)0.025 \\ \hline PSRO\({}_{FN}\) & 0.387\(\pm\)0.019 & - & 0.412\(\pm\)0.030 & 0.358\(\pm\)0.019 \\ \hline P-PSRO & 0.531\(\pm\)0.034 & 0.588\(\pm\)0.030 & - & 0.370\(\pm\)0.031 \\ \hline PSD-PSRO(Ours) & **0.578\(\pm\)0.025** & **0.642\(\pm\)0.019** & **0.630\(\pm\)0.031** & - \\ \hline \end{tabular}
\end{table}
Table 1: The win rate of the row agents against the column agents on Goofspiel.

* [5] Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. _Science_, 359(6374):418-424, 2018.
* [6] Ozan Candogan, Ishai Menache, Asuman Ozdaglar, and Pablo A Parrilo. Flows and decompositions of games: Harmonic and potential games. _Mathematics of Operations Research_, 36(3):474-503, 2011.
* [7] Frank H Clarke. Generalized gradients and applications. _Transactions of the American Mathematical Society_, 205:247-262, 1975.
* [8] Antoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. Robots that can adapt like animals. _Nature_, 521(7553):503-507, 2015.
* [9] Wojciech M Czarnecki, Gauthier Gidel, Brendan Tracey, Karl Tuyls, Shayegan Omidshafiei, David Balduzzi, and Max Jaderberg. Real world games look like spinning tops. In _Advances in Neural Information Processing Systems_, volume 33, pages 17443-17454, 2020.
* [10] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In _International Conference on Learning Representations_, 2018.
* [11] Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Optimistic regret minimization for extensive-form games via dilated distance-generating functions. _Advances in neural information processing systems_, 32, 2019.
* [12] Xidong Feng, Oliver Slumbers, Ziyu Wan, Bo Liu, Stephen McAleer, Ying Wen, Jun Wang, and Yaodong Yang. Neural auto-curricula in two-player zero-sum games. In _Advances in Neural Information Processing Systems_, volume 34, pages 3504-3517, 2021.
* [13] David B Fogel. _Evolutionary computation: toward a new philosophy of machine intelligence_. John Wiley & Sons, 2006.
* [14] Haobo Fu, Weiming Liu, Shuang Wu, Yijia Wang, Tao Yang, Kai Li, Junliang Xing, Bin Li, Bo Ma, Qiang Fu, et al. Actor-critic policy optimization in a large-scale imperfect-information game. In _International Conference on Learning Representations_, 2021.
* [15] Haobo Fu, Ye Tian, Hongxiang Yu, Weiming Liu, Shuang Wu, Jiechao Xiong, Ying Wen, Kai Li, Junliang Xing, Qiang Fu, et al. Greedy when sure and conservative when uncertain about the opponents. In _International Conference on Machine Learning_, pages 6829-6848. PMLR, 2022.
* [16] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. _arXiv preprint arXiv:1710.11248_, 2017.
* [17] Drew Fudenberg and David K Levine. Consistency and cautious fictitious play. _Journal of Economic Dynamics and Control_, 19(5-7):1065-1089, 1995.
* [18] Johannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games. In _International conference on machine learning_, pages 805-813. PMLR, 2015.
* [19] Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-information games. _arXiv preprint arXiv:1603.01121_, 2016.
* [20] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. _Advances in neural information processing systems_, 29, 2016.
* [21] Samid Hoda, Andrew Gilpin, Javier Pena, and Tuomas Sandholm. Smoothing techniques for computing nash equilibria of sequential games. _Mathematics of Operations Research_, 35(2):494-512, 2010.
* [22] Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Tsu-Jui Fu, and Chun-Yi Lee. Diversity-driven exploration strategy for deep reinforcement learning. In _Advances in neural information processing systems_, volume 31, 2018.
* [23] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Human-level performance in 3d multiplayer games with population-based reinforcement learning. _Science_, 364(6443):859-865, 2019.
* [24] Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization perspective on imitation learning methods. _arXiv e-prints_, pages arXiv-1911, 2019.
* [25] Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn. One solution is not all you need: Few-shot extrapolation via structured maxent rl. In _Advances in Neural Information Processing Systems_, volume 33, pages 8198-8210, 2020.

* [26] Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. In _Advances in neural information processing systems_, volume 30, 2017.
* [27] Chung-Wei Lee, Christian Kroer, and Haipeng Luo. Last-iterate convergence in extensive-form games. _Advances in Neural Information Processing Systems_, 34:14293-14305, 2021.
* [28] Joel Lehman and Kenneth O Stanley. Abandoning objectives: Evolution through the search for novelty alone. _Evolutionary computation_, 19(2):189-223, 2011.
* [29] Siqi Liu, Luke Marris, Daniel Hennes, Josh Merel, Nicolas Heess, and Thore Graepel. Neupl: Neural population learning. _arXiv preprint arXiv:2202.07415_, 2022.
* [30] Weiming Liu, Haobo Fu, Qiang Fu, and Yang Wei. Opponent-limited online search for imperfect information games. In _International Conference on Machine Learning_. PMLR, 2023.
* [31] Weiming Liu, Huacong Jiang, Bin Li, and Houqiang Li. Equivalence analysis between counterfactual regret minimization and online mirror descent. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162, pages 13717-13745, 2022.
* [32] Xiangyu Liu, Hangtian Jia, Ying Wen, Yujing Hu, Yingfeng Chen, Changjie Fan, Zhipeng Hu, and Yaodong Yang. Towards unifying behavioral and response diversity for open-ended learning in zero-sum games. In _Advances in Neural Information Processing Systems_, volume 34, pages 941-952, 2021.
* [33] Zongkai Liu, Chao Yu, Yaodong Yang, Zifan Wu, Yuan Li, et al. A unified diversity measure for multiagent reinforcement learning. In _Advances in Neural Information Processing Systems_, 2022.
* [34] Andrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. Trajectory diversity for zero-shot coordination. In _International Conference on Machine Learning_, pages 7204-7213. PMLR, 2021.
* [35] Odile Macchi. The fermion process--a model of stochastic point process with repulsive points. In _Transactions of the Seventh Prague Conference on Information Theory, Statistical Decision Functions, Random Processes and of the 1974 European Meeting of Statisticians_, pages 391-398. Springer, 1977.
* [36] Stephen McAleer, John B Lanier, Roy Fox, and Pierre Baldi. Pipeline psro: A scalable approach for finding approximate nash equilibria in large games. In _Advances in neural information processing systems_, volume 33, pages 20238-20248, 2020.
* [37] Stephen McAleer, Kevin Wang, Marc Lanctot, John Lanier, Pierre Baldi, and Roy Fox. Anytime optimal psro for two-player zero-sum games. _arXiv preprint arXiv:2201.07700_, 2022.
* [38] H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. Planning in the presence of cost functions controlled by an adversary. In _Proceedings of the 20th International Conference on Machine Learning (ICML-03)_, pages 536-543, 2003.
* [39] Jack Parker-Holder, Aldo Pacchiano, Krzysztof M Choromanski, and Stephen J Roberts. Effective diversity in population based reinforcement learning. In _Advances in Neural Information Processing Systems_, volume 33, pages 18050-18062, 2020.
* [40] Zhenghao Peng, Hao Sun, and Bolei Zhou. Non-local policy optimization via diversity-regularized collaborative exploration. _arXiv preprint arXiv:2006.07781_, 2020.
* [41] Nicolas Perez-Nieves, Yaodong Yang, Oliver Slumbers, David H Mguni, Ying Wen, and Jun Wang. Modelling behavioural diversity for learning in open-ended games. In _International Conference on Machine Learning_, pages 8514-8524. PMLR, 2021.
* [42] Justin K Pugh, Lisa B Soros, and Kenneth O Stanley. Quality diversity: A new frontier for evolutionary computation. _Frontiers in Robotics and AI_, page 40, 2016.
* [43] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* [44] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [45] Max Smith, Thomas Anthony, and Michael Wellman. Iterative empirical game solving via single policy best response. In _International Conference on Learning Representations_, 2020.
* [46] Finnegan Southey, Michael P Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings, and Chris Rayner. Bayes' bluff: Opponent modelling in poker. _arXiv preprint arXiv:1207.1411_, 2012.

* [47] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. _Advances in neural information processing systems_, 12, 1999.
* [48] Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming. In _Proceedings of the 25th international conference on Machine learning_, pages 1032-1039, 2008.
* [49] Zhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon Shaolei Du, Yu Wang, and Yi Wu. Discovering diverse multi-agent strategic behavior via reward randomization. In _International Conference on Learning Representations_, 2020.
* [50] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019.
* [51] Shuang Wu, Jian Yao, Haobo Fu, Ye Tian, Chao Qian, Yaodong Yang, Qiang Fu, and Yang Wei. Quality-similar diversity via population based reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2022.
* [52] Brian Zhang and Tuomas Sandholm. Subgame solving without common knowledge. _Advances in Neural Information Processing Systems_, 34:23993-24004, 2021.
* [53] Ming Zhou, Jingxiao Chen, Ying Wen, Weinan Zhang, Yaodong Yang, Yong Yu, and Jun Wang. Efficient policy space response oracles. _arXiv preprint arXiv:2202.00633_, 2022.
* [54] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. In _Advances in neural information processing systems_, volume 20, 2007.

## Appendix A Notation

We provide the notation in Table 2.

## Appendix B Theoretical Analysis

### Derivation of Bregman Divergence

We define the Bregman divergence on the sequence-form representation of the policies. Given a strictly convex function \(d\) defined on \(\mathbb{R}^{|\mathcal{S}_{i}\times\mathcal{A}_{i}|}\), a Bregman divergence is defined as

\begin{table}
\begin{tabular}{r l} \hline \hline Notation & Meaning \\ \hline  & Extensive-Form Games \\ \hline \(\mathcal{N}\) & \(\mathcal{N}=\{1,2\}\); set of players \\ \(\mathcal{S}\) & set of information states \\ \(s\) & \(s\in\mathcal{S}\); state node \\ \(\mathcal{A}(s)\) & set of actions allowed to be performed at state \(s\) \\ \(P\) & player function \\ \(i\) / \(-i\) & \(i\in\mathcal{N}\); player \(i\) / players except \(i\) \\ \(s_{i}\) & player \(i\)’s information state \\ \(\mathcal{S}_{i}\) & \(\mathcal{S}_{i}=\{s\in\mathcal{S}|P(s)=i\}\); set of player’s information state \\ \(\mathcal{A}_{i}\) & \(\mathcal{A}_{i}=\cup_{s\in\mathcal{S}_{i}}\mathcal{A}(s)\); set of player ‘\(i\)’s actions \\ \(\pi_{i}\) & player \(i\)’s behavioral strategy \\ \(\pi\) & \(\pi=(\pi_{i},\pi_{-i})\) strategy profile \\ \(u_{i}(\pi)\) & \(u_{i}(\pi)=u_{i}(\pi_{i},\pi_{-i})\); the payoff for player \(i\) \\ \(\mathcal{BR}(\pi_{-i})\) & \(\mathcal{BR}(\pi_{-i}):=\arg\max_{\pi^{\prime}_{i}\in\Delta_{i}}u(\pi^{\prime} _{i},\pi_{-i})\); best response for player \(i\) against players \(-i\) \\ \(\mathcal{E}(\pi)\) & \(\mathcal{E}(\pi)=\frac{1}{N}\sum_{i\in\mathcal{N}}[\max_{\pi^{\prime}_{i}}u_{ i}(\pi^{\prime}_{i},\pi_{-i})-u_{i}(\pi_{i},\pi_{-i})]\); exploitability for policy \(\pi\) \\ \hline  & Meta Games \\ \hline \(i\) / \(-i\) & \(i\in\{1,2\}\); player \(i\) / players except \(i\) \\ \(\pi^{j}_{i}\) / \(\pi_{i}\) & \(j\)-th policy / any policy for player \(i\) \\ \(\pi\) & \(\pi=(\pi_{1},\pi_{2})\); joint policy \\ \(\Pi_{i}\) & \(\Pi_{i}:=\{\pi^{i}_{1},\pi^{2}_{1},...,\}\); policy set for player \(i\) \\ \(\Pi\) & \(\Pi=\Pi_{1}\times\Pi_{2}\); joint policy set \\ \(\sigma_{i}\) & \(\sigma_{i}\subseteq\Pi_{i}\); meta-policy over \(\Pi_{i}\) \\ \(\sigma=(\sigma_{i},\sigma_{2})\); joint meta-policy \\ \(\mathbf{M}_{\Pi_{i},\Pi_{-i}}\) & \((\mathbf{M}_{\Pi_{i},\Pi_{-i})_{jk}}:=u_{i}(\pi^{\prime}_{i},\pi^{\prime}_{-i})\); player \(i\)’s payoff table in the meta-games \((\Pi_{i},\Pi_{-i})\) \\ \(\Omega_{i}\) & the full set of all possible mixed strategies of player \(i\) \\ \(\mathcal{H}(\Pi_{i})\) & \(\mathcal{H}(\Pi_{i})=\{\sum_{j}\beta_{j}\pi^{j}_{i}|\beta_{j}\geq 0,\sum_{j=1}^{ \prime}\beta_{j}=1\}\); Policy Hull of \(\Pi_{i}=\{\pi^{1}_{i},\pi^{2}_{i},...,\pi^{i}_{i}\}\) \\ \(\mathcal{P}_{i}(\cdot,\cdot)\) & \(\mathcal{P}_{i}(\Pi_{i},\Pi_{-i})=\pi^{*T}_{i}\mathbf{M}_{\Pi_{i},\Pi_{-i}} \pi^{-}_{-i}\); Relative Population Performance for player \(i\) \\ \(\mathcal{PE}(\cdot)\) & \(\mathcal{PE}(\Pi)=\frac{1}{2}\sum_{i=1,2}\max_{\Pi^{\prime}_{i}\subseteq\Omega _{i}}\mathcal{P}_{i}(\Pi^{\prime}_{i},\Pi_{-i})\); Population Exploitability \\ \(\mathcal{GS}(\Pi_{i}|\Pi_{-i})\) & \(\mathcal{GS}(\Pi_{i}|\Pi_{-i})=\{\sum_{j}\alpha_{j}\mathbf{m}_{j}:\mathbf{ \alpha}\geq 0,\mathbf{\alpha}^{T}\mathbf{1}=1,\mathbf{m}_{j}=\mathbf{M}_{\Pi_{i}, \Pi_{-i}\{j,\}}\}\); semescape of \(\Pi_{i}\) \\ \hline  & PSRO / PSD-PSRO Training \\ \hline \(\Pi^{t}_{i}\) & \(\Pi^{t}_{i}=\{\pi^{t}_{i},...,\pi^{t}_{i}\}\); the policy set of player \(i\) at the \(t\)-th iteration in PSRO \\ \(\sigma^{t}_{i}\) & \(\sigma^{t}_{i}\in\Delta_{\Pi^{t}_{i}}\); the meta-policy of player \(i\) at \(t\)-th iteration in PSRO \\ \(\rho(\cdot)\) & policy representation \\ \(\pi_{i}\) & \(\mathbf{x}_{i}\in\mathcal{X}_{i}\subseteq[0,1]^{|\mathcal{S}_{i}\times\mathcal{ A}_{i}|}\) sequence form representation for a policy \\ \(\tau(s,a)\) & trajectory from the initial state to \((s,a)\) \\ \(\mathcal{B}_{d_{s}}(\pi_{i}(s)\|\pi^{t}_{i}(s))\) & Bregman divergence between \(\pi_{i}(s)\) and \(\pi^{\prime}_{i}(s)\) \\ Div(\(\cdot\)) & diversity on the policy set specified in different methods \\ \(\mathrm{dist}(\cdot,\cdot)\) & distance function \\ \(\lambda\) & diversity weight \\  & KL & Kullback-Leibler divergence \\ \hline \hline \end{tabular}
\end{table}
Table 2: Notations\[\mathcal{B}_{d}(\bm{x}_{i}\|\bm{x}_{i}^{\prime})=d(\bm{x}_{i})-d(\bm{x}_{i}^{ \prime})-\langle\nabla d(\bm{x}_{i}^{\prime}),\bm{x}_{i}-\bm{x}_{i}^{\prime} \rangle\,,\] (18)

for any \(\bm{x}_{i},\bm{x}_{i}^{\prime}\in\mathcal{X}_{i}\). Intuitively, Bregman divergence is the gap between \(d(\bm{x}_{i})\) and its first-order approximation at \(\bm{x}_{i}^{\prime}\). It is non-negative (zero is achieved if and only if \(\bm{x}_{i}=\bm{x}_{i}^{\prime}\)) and strictly convex in its first argument. So, it can be used to measure the difference between \(\bm{x}_{i}\) and \(\bm{x}_{i}^{\prime}\). Examples of Bregman divergence include the Squared Euclidean distance, which is generated by the \(l_{2}\) function, and the Kullback-Leibler (KL) divergence, which is generated by the negative entropy function. For sequence-form policies, we use the dilated Distance-Generating Function (dilated DGF) [21, 11, 31]:

\[d(\bm{x}_{i})=\sum_{s,a\in\mathcal{S}_{i}\times\mathcal{A}_{i}}\bm{x}_{i}(s,a )\beta_{s}d_{s}\left(\frac{\bm{x}_{i}(s)}{\|\bm{x}_{i}(s)\|_{1}}\right),\] (19)

where \(\beta_{s}>0\) is a hyper-parameter and \(d_{s}\) is a strictly convex function defined on \(\mathbb{R}^{|\mathcal{A}(s)|}\), for example, the negative entropy function. It is proven that for the dilated DGF (Lemma D.2 in [31] ), we have the Bregman divergence

\[\mathcal{B}_{d}(\bm{x}_{i}\|\bm{x}_{i}^{\prime})=\sum_{s,a\in\mathcal{S}_{i} \times\mathcal{A}_{i}}\bm{x}_{i}(s,a)\beta_{s}\mathcal{B}_{d_{s}}\left(\frac{ \bm{x}_{i}(s)}{\|\bm{x}_{i}(s)\|_{1}}\Bigg{\|}\frac{\bm{x}_{i}^{\prime}(s)}{\| \bm{x}_{i}^{\prime}(s)\|_{1}}\right).\] (20)

The result allows us to compute the Bergman divergence using the behavior policy instead of explicitly using the sequence-form representation, which is prohibited in large-scale games. Formally, we define the distance of two policies \(\pi_{i},\pi_{i}^{\prime}\) as

\[\mathrm{dist}(\pi_{i},\pi_{i}^{\prime}):=\mathcal{B}_{d}(\bm{x}_{i}\|\bm{x}_{ i}^{\prime})=\sum_{s,a\in\mathcal{S}_{i}\times\mathcal{A}_{i}}\left(\prod_{ \widetilde{s}_{i},\widetilde{a}\in\tau(s,a)}\pi_{i}(\widetilde{a}|\widetilde{ s}_{i})\right)\beta_{s}\mathcal{B}_{d_{s}}(\pi_{i}(s)\|\pi_{i}^{\prime}(s)).\] (21)

### Proof of Proposition 3.2

Proof.: Recall that the PE of joint policy set \(\Pi\) in two-player zero-sum games can be written into

\[\mathcal{PE}(\Pi) =\frac{1}{2}\sum_{i=1,2}\max_{\Pi_{i}^{\prime}\subseteq\Omega_{i} }\mathcal{P}_{i}(\Pi_{i}^{\prime},\Pi_{-i})\] \[=\frac{1}{2}(\max_{\Pi_{i}^{\prime}\subseteq\Omega_{i}}\mathcal{ P}_{i}(\Pi_{i}^{\prime},\Pi_{2})-\min_{\Pi_{-i}^{\prime}\subseteq\Omega_{2}} \mathcal{P}_{i}(\Pi_{i},\Pi_{2}^{\prime})),\]

since \(\mathcal{P}_{-i}(\Pi_{-i}^{\prime},\Pi_{1})=-\mathcal{P}_{i}(\Pi_{i},\Pi_{2}^ {\prime})\).

1. We can check this property directly, since \[\mathcal{PE}(\Pi) =\frac{1}{2}(\max_{\Pi_{i}^{\prime}\subseteq\Omega_{i}}\mathcal{ P}_{i}(\Pi_{i}^{\prime},\Pi_{2})-\min_{\Pi_{2}^{\prime}\subseteq\Omega_{2}} \mathcal{P}_{i}(\Pi_{i},\Pi_{2}^{\prime}))\] \[\geq\frac{1}{2}(\mathcal{P}_{i}(\Pi_{i},\Pi_{2})-\mathcal{P}_{i}( \Pi_{i},\Pi_{2}))=0\]
2. We first note that \(\mathcal{H}(\Pi_{1})\times\mathcal{H}(\Pi_{2})\subseteq\mathcal{H}(\Pi_{i}^{ \prime})\times\mathcal{H}(\Pi_{-i}^{\prime})\) means \(\mathcal{H}(\Pi_{1})\subseteq\mathcal{H}(\Pi_{i}^{\prime})\) and \(\mathcal{H}(\Pi_{2})\subseteq\mathcal{H}(\Pi_{-i}^{\prime})\). \(\mathcal{H}(\Pi_{2})\subseteq\mathcal{H}(\Pi_{2}^{\prime})\) suggests that \(\forall\)\(\Phi_{i}\in\Omega_{i}\), we have \(\mathcal{P}_{i}(\Phi_{i},\Pi_{2})\geq\mathcal{P}_{i}(\Phi_{i},\Pi_{2}^{\prime})\), which is a property of relative population performance [2]. Take the maximum for both sides, we have, \[\max_{\Phi_{i}\subseteq\Omega_{i}}\mathcal{P}_{i}(\Phi_{i},\Pi_{-i})\geq\max_ {\Phi_{i}\subseteq\Omega_{i}}\mathcal{P}_{i}(\Phi_{i},\Pi_{-i}^{\prime})\] (22) Similarly, using the condition \(\mathcal{H}(\Pi_{1})\subseteq\mathcal{H}(\Pi_{i}^{\prime})\), we have, \[\min_{\Phi_{-i}\subseteq\Omega_{-i}}\mathcal{P}_{i}(\Pi_{i},\Phi_{-i})\leq\min _{\Phi_{-i}\subseteq\Omega_{-i}}\mathcal{P}_{i}(\Pi_{i}^{\prime},\Phi_{-i})\] (23) Combining Equation 22 and 23, we obtain \(\mathcal{PE}(\Pi)\geq\mathcal{PE}(\Pi^{\prime})\)3. By the monotonicity of PE, we have \[\mathcal{PE}(\Pi)=\frac{1}{2}(\mathcal{P}_{i}(\Omega_{i},\Pi_{-i})-\mathcal{P}_{i }(\Pi_{i},\Omega_{-i}))\] (24) Since \(\Pi_{-i}=\{\pi_{-i}\}\) contains only one policy, by the definition of relative population performance, we have, \[\mathcal{P}_{i}(\Omega_{i},\{\pi_{-i}\})=u(BR(\pi_{-i}),\pi_{-i});\mathcal{P}_ {i}(\{\pi_{i}\},\Omega_{-i}))=u(\pi_{i},BR(\pi_{i}))\] (25) Substituting Equation 25 into Equation 24, we obtain \[\mathcal{PE}(\Pi)=\mathcal{E}(\pi)\] (26)
4. By Equation 24 and the definition of the relative population performance, there exists NE \((\sigma_{i},\pi_{-i})\) in game \(\mathbf{M}_{\Omega_{i},\Pi_{-i}}\) and NE \((\pi_{i},\sigma_{-i})\) in game \(\mathbf{M}_{\Pi_{i},\Omega_{-i}}\) s.t. \[\mathcal{PE}(\Pi)=\frac{1}{2}(u(\sigma_{i},\pi_{-i})-u(\pi_{i},\sigma_{-i}))\] By the definition of NE, we have, \[\mathcal{PE}(\Pi) =\frac{1}{2}(\max_{\pi^{\prime}_{i}\in\Omega_{i}}u(\pi^{\prime}_{ i},\pi_{-i})-\min_{\pi^{\prime}_{-i}\in\Omega_{-i}}u(\pi_{i},\pi^{\prime}_{-i}))\] \[=\mathcal{E}(\pi)\] (27) We now prove \((\pi_{i},\pi_{-i})\) is also the least exploitable policy in the joint policy set. \(\forall\ \pi^{\prime}=(\pi^{\prime}_{i},\pi^{\prime}_{-i})\in\Pi_{i}\times\Pi_{-i}\), we have, \[\mathcal{E}(\pi)=\mathcal{PE}(\Pi)\leq\mathcal{PE}(\{\pi^{\prime}_{i}\}\times \{\pi^{\prime}_{-i}\})=\mathcal{E}(\pi^{\prime})\] (28) where the inequality is from the monotonicity of PE. Hence, \((\pi_{i},\pi_{-i})\) is the least exploitable policy in the Policy Hull of the population.
5. **Necessity** If \((\sigma^{*}_{i},\sigma^{*}_{-i})\) is the NE of the game, we can regard the \(\{\sigma^{*}_{i}\}\) and \(\{\sigma^{*}_{-i}\}\) as the populations which contain a single policy. Then by property \(2\), we have, \[\mathcal{PE}(\Pi) \leq\mathcal{PE}(\{\sigma^{*}_{i}\}\times\{\sigma^{*}_{-i}\})\] \[=\frac{1}{2}(\mathcal{P}_{i}(\Omega_{i},\{\sigma^{*}_{-i}\})- \mathcal{P}_{i}(\{\sigma^{*}_{i}\},\Omega_{-i}))\] \[=\frac{1}{2}(u(\sigma^{*}_{i},\sigma^{*}_{-i})-u(\sigma^{*}_{i}, \sigma^{*}_{-i}))=0\] Note that by property \(1\), we have \(\mathcal{PE}(\Pi)\geq 0\), so we have \(\mathcal{PE}(\Pi)=0\). **Sufficiency** Using the conclusion from 3, It is easy to check the sufficiency. If \(\mathcal{PE}(\Pi)=0\), there exists \(\sigma=(\sigma_{i},\sigma_{-i})\) s.t. \(\mathcal{E}(\sigma)=0\), which means \(\sigma\) is the NE of the game.

### Proof of Proposition 3.3

Proof.: Recall that at each iteration \(t\), the PSRO calculates the NE \((\sigma^{t}_{i},\sigma^{t}_{-i})\) of the meta-game \(\mathbf{M}_{\Pi^{t}_{i},\Pi^{t}_{-i}}\), then adds the new policy \(\pi^{t}=(BR(\sigma^{t}_{-i}),BR(\sigma^{t}_{i}))\) to the population.

If \(PE(\Pi^{t})>0\), then by the second property in Proposition 3.2, we know the population does not contain the NE of the game. Therefore, since \((\sigma^{t}_{i},\sigma^{t}_{-i})\in\Pi^{t}\), it is not the NE of the full game, which means at least one of the conditions holds: (1) \(BR(\sigma^{t}_{-i})\notin\mathcal{H}(\Pi^{t}_{i})\); (2) \(BR(\sigma^{t}_{i})\notin\mathcal{H}(\Pi^{t}_{-i})\), which suggests the enlargement of the Policy Hull.

In finite games, since the PSRO adds pure BR at each iteration, this Policy Hull extension stops at some iteration, saying \(T\), in which case we have \(BR(\sigma^{T}_{-i})\in\mathcal{H}(\Pi^{T}_{i})\) and \(BR(\sigma^{T}_{i})\in\mathcal{H}(\Pi^{T}_{-i})\) (by above analysis). Let \((\pi^{T+1}_{i},\pi^{T+1}_{-i}):=(BR(\sigma^{T}_{-i}),BR(\sigma^{T}_{i}))\), we claim that \((\pi^{T+1}_{i},\pi^{T+1}_{-i})\) is the NE of the whole game. To see this, note that \((\sigma_{i}^{T},\sigma_{-i}^{T})\) is the NE of the game \(\text{M}_{\Pi_{i}^{T},\Pi_{-i}^{T}}\) and \(\pi_{i}^{T+1}\in\mathcal{H}(\Pi_{i}^{T})\), hence we have,

\[u(\sigma_{i}^{T},\sigma_{-i}^{T})\geq u(\pi_{i}^{T+1},\sigma_{-i}^{T}).\] (29)

On the other hand, since \(\pi_{i}^{T+1}\) is the BR to \(\sigma_{-i}^{T}\), we have,

\[u(\pi_{i}^{T+1},\sigma_{-i}^{T})\geq u(\sigma_{i}^{T},\sigma_{-i}^{T}).\] (30)

Combining Equation 29 and 30, we obtain \(u(\pi_{i}^{T+1},\sigma_{-i}^{T})=u(\sigma_{i}^{T},\sigma_{-i}^{T})\), which suggests \(\sigma_{i}^{T}\) is also the BR of \(\sigma_{-i}^{T}\). Similarly, we can prove that \(\sigma_{-i}^{T}\) is the BR of \(\sigma_{i}^{T}\), and hence \((\sigma_{i}^{T},\sigma_{-i}^{T})\) is the NE of the game.

Finally, by the fourth property in Proposition 3.2, we have \(\mathcal{PE}(\Pi^{T})=0\). 

### Proof of Proposition 4.3

This is an intermediate result in the proof of 3.3 (Appendix B.3).

### Proof of Theorem 3.4

Proof.: We prove this theorem by providing the counterexamples. Let's first focus on the following two-player zero-sum game:

\[\begin{array}{c}R\quad\quad P\quad\quad S\\ R\quad P\quad S\\ S\quad R^{\prime}\quad P^{\prime}\\ S^{\prime}\quad\end{array}\left(\begin{array}{rrr}0\quad-1\quad 1\\ 1\quad 0\quad-1\\ -1\quad 1\quad 0\\ 1\quad-1\quad 1\\ 1\quad 1\quad-1\\ -1\quad 1\quad 1\end{array}\right)\] (31)

The game is inspired by Rock-Paper-Scissors. We add high-level strategies \(\{R^{\prime},P^{\prime},S^{\prime}\}\) to player \(i\) in the original R-P-S game. The high-level strategy can beat the low-level strategy with the same type.

Let \(\Pi_{i}=\{S,R^{\prime},P^{\prime}\}\) and \(\Pi_{-i}=\{R,P,\frac{1}{2}R+\frac{1}{2}P\}\), where \(\frac{1}{2}R+\frac{1}{2}P\) means playing \([R,P]\) with probability \([\frac{1}{2},\frac{1}{2}]\). The payoff matrix \(\text{M}_{\Pi_{i},\Pi_{-i}}\)is

\[\begin{array}{c}R\quad\quad P\quad\frac{1}{2}R+\frac{1}{2}P\\ S\quad\begin{array}{rrr}-1\quad 1\quad 0\\ 1\quad-1\quad 0\\ 1\quad 1\quad\quad\end{array}\\ P^{\prime}\quad\end{array}\] (32)

It is in player \(i\)'turn to choose his new strategy. Considering two candidates \(\pi_{i}^{1}=S^{\prime}\) and \(\pi_{-i}^{2}=\frac{1}{2}R+\frac{1}{2}R^{\prime}\) (There are many choices for \(\pi_{i}^{2}\) to construct the counterexample, we just take one for example).

The payoff of \(\pi_{i}^{1}\) against \(\Pi_{-i}\) is \((-1,1,0)\), the same as the first row in \(\text{M}_{\Pi_{i},\Pi_{-i}}\). The payoff of \(\pi_{i}^{2}\) is \((\frac{1}{2},-1,-\frac{1}{4})\), which is out of the gamescape of \(\Pi_{i}\).

Denote \(\Pi_{i}^{1}=\Pi_{i}\cup\{\pi_{i}^{1}\}\), \(\Pi_{i}^{2}=\Pi_{i}\cup\{\pi_{i}^{2}\}\). By the analysis above, we have

\[\mathcal{GS}(\Pi_{i}^{1}|\Pi_{-i})\subsetneqq\mathcal{GS}(\Pi_{i}^{2}|\Pi_{-i})\] (33)

Hence, to promote the diversity defined on the payoff matrix and aim to enlarge the gamescape [2, 41], we should choose \(\pi_{i}^{2}\) to add. However, in this case, we show adding \(\pi_{i}^{1}\) is more helpful to decrease PE.

Denote \(\Pi^{1}=\Pi_{i}^{1}\times\Pi_{-i}\) and \(\Pi^{2}=\Pi_{i}^{2}\times\Pi_{-i}\), by Equation 24 and a few calculation, we have,

\[\mathcal{PE}(\Pi^{1})-\mathcal{PE}(\Pi^{2})=-\frac{1}{2}\mathcal{P}(\Pi_{i}^{1},\Omega_{-i})+\frac{1}{2}\mathcal{P}(\Pi_{i}^{2},\Omega_{-i})\approx-0.17+0.1 0<0,\] (34)i.e., \(\mathcal{PE}(\Pi^{1})<\mathcal{PE}(\Pi^{2})\). We can see that adding a new policy following the guidance of enlarging gamescape may not be the best choice to decrease PE.

Back to the Proposition 3.4, since we have found an example that,

\[\mathcal{GS}(\Pi^{1}_{i}|\Pi_{-i})\subsetneqq\mathcal{GS}(\Pi^{2}_{i}|\Pi_{-i}) \quad\mathrm{but}\quad\mathcal{PE}(\Pi^{1})<\mathcal{PE}(\Pi^{2})\] (35)

hence,

\[\mathcal{GS}(\Pi^{1}_{i}|\Pi_{-i})\subsetneqq\mathcal{GS}(\Pi^{2}_{i}|\Pi_{-i })\nRightarrow\mathcal{PE}(\Pi^{1})\geq\mathcal{PE}(\Pi^{2})\] (36)

Rewrite the example in Equation 35, we have,

\[\mathcal{PE}(\Pi^{2})\geq\mathcal{PE}(\Pi^{1})\quad\mathrm{but}\quad\mathcal{ GS}(\Pi^{2}_{i}|\Pi_{-i})\supsetneqq\mathcal{GS}(\Pi^{1}_{i}|\Pi_{-i})\] (37)

Rename the superscripts (exchange the identity of \(1,2\) in the superscript), we obtain,

\[\mathcal{PE}(\Pi^{1})\geq\mathcal{PE}(\Pi^{2})\nRightarrow\mathcal{GS}(\Pi^ {1}_{i}|\Pi_{-i})\subsetneqq\mathcal{GS}(\Pi^{2}_{i}|\Pi_{-i})\] (38)

Combine Equation 36 and 38, we complete the proof. 

**Remark** We offer another example in a symmetric game, proving that this proposition is also valid in this type of game.

\[\begin{array}{c}A\quad\quad B\quad\quad C\quad D\quad\quad E\\ A\quad\quad B\quad\quad\left(\begin{array}{ccccc}0&-1&-0.5&-1&-4\\ 1&0&0.5&-1&-4\\ 0.5&-0.5&0&0&4\\ 1&1&0&0&-4\\ E&4&4&-4&4&0\end{array}\right)\end{array}\] (39)

Current policy set \(\Pi_{i}=\Pi_{-i}=\{A,B\}\), the payoff is

\[\begin{array}{c}A\quad\quad B\\ A\quad\quad B\quad\left(\begin{array}{cc}0&-1\\ 1&0\end{array}\right)\end{array}\] (40)

Considering \(\pi^{1}_{i}=C,\pi^{2}_{i}=D\), the payoff of \(\pi^{1}_{i}=C\) against \(\Pi_{-i}\) is \((0.5,-0.5)\), contained in \(\mathcal{GS}(\Pi_{i}|\Pi_{-i})\), while the payoff of \(\pi^{2}_{i}=D\) against \(\Pi_{-i}\) is \((1,1)\), which is out of \(\mathcal{GS}(\Pi_{i}|\Pi_{-i})\).

Denote \(\Pi^{1}_{i}=\Pi_{i}\cup\{\pi^{1}_{i}\}\), \(\Pi^{2}_{i}=\Pi_{i}\cup\{\pi^{2}_{i}\}\), \(\Pi^{1}=\Pi^{1}_{i}\times\Pi_{-i}\) and \(\Pi^{2}=\Pi^{2}_{i}\times\Pi_{-i}\), we have \(\mathcal{GS}(\Pi^{1}_{i}|\Pi_{-i})\subsetneqq\mathcal{GS}(\Pi^{2}_{i}|\Pi_{-i})\). However,

\[\mathcal{PE}(\Pi^{1})-\mathcal{PE}(\Pi^{2})=-\frac{1}{2}\mathcal{P}_{i}(\Pi^{1 }_{i},\Omega_{-i})+\frac{1}{2}\mathcal{P}_{i}(\Pi^{2}_{i},\Omega_{-i})\approx 0.1 7-2<0\] (41)

Hence, similar to the above analysis, we can prove the Proposition 3.4 in symmetric games.

### Proof of Theorem 4.4

Proof.: Similar to Appendix B.3, we denote the meta-strategy at iteration \(t\) as \((\sigma^{t}_{i},\sigma^{t}_{-i})\). At iteration \(t\) of PSRO, if \(\mathcal{PE}(\Pi^{t})>0\), then from Appendix B.3, we know that at least one of the conditions holds: (1) \(BR(\sigma^{t}_{-i})\notin\mathcal{H}(\Pi^{t}_{i})\); (2) \(BR(\sigma^{t}_{i})\notin\mathcal{H}(\Pi^{t}_{-i})\). Without loss of generality, we assume

\[BR(\sigma^{t}_{-i})\notin\mathcal{H}(\Pi^{t}_{i}).\] (42)

Then \(\forall\ \hat{\sigma}_{i}\in\mathcal{H}(\Pi^{t}_{i})\) and \(\lambda>0\),

\[u(\hat{\sigma}_{i},\sigma^{t}_{-i})+\lambda\min_{\pi^{t}_{i}\in \mathcal{H}(\Pi^{t}_{i})}\mathbb{E}_{s_{i}\sim\hat{\sigma}_{i},b_{2}}[\mathrm{ KL}(\hat{\sigma}_{i}(s_{i})\|\pi^{k}_{i}(s_{i}))]\] \[= u(\hat{\sigma}_{i},\sigma^{t}_{-i})+0\] \[< u(BR(\sigma^{t}_{-i}),\sigma^{t}_{-i})\] \[< u(BR(\sigma^{t}_{-i}),\sigma^{t}_{-i})+\lambda\min_{\pi^{t}_{i} \in\mathcal{H}(\Pi^{t}_{i})}\mathbb{E}_{s_{i}\sim\hat{\sigma}_{i},b_{2}}[ \mathrm{KL}(\hat{\sigma}_{i}(s_{i})\|\pi^{k}_{i}(s_{i}))]\] (43)

where the first equation is because \(\hat{\sigma}_{i}\in\mathcal{H}(\Pi^{t}_{i})\) and the last two inequalities is due to the definition of BR and Equation 42 respectively.

From Equation 43, we know that any policy in \(\mathcal{H}(\Pi_{i}^{t})\) will not be considered as the optimal solution in Equation 14, since they are dominated by at least one of the policy that out of \(\mathcal{H}(\Pi_{i}^{t})\), i.e., the BR. Hence, before PE reaches 0, solving Equation 14 will find a policy not in \(\mathcal{H}(\Pi_{i}^{t})\), which guarantees the enlargement of the Policy Hull. Once we can not enlarge the Policy Hull by adding the optimal solution in Equation 14, the PE of the Policy Hull must be \(0\), otherwise the optimal solution will be outside of the Policy Hull. 

## Appendix C Algorithm for PSD-PSRO

We provide the algorithm for PSD-PSRO in Algorithm 1, which generates the algorithm of PSRO to incorporate the estimation of the diversity in Equation 14.

``` Input: Initial policy sets \(\Pi_{i}^{1},\Pi_{-i}^{1}\)  Compute payoff matrix \(\mathbf{M}_{\Pi_{i}^{1},\Pi_{-i}^{1}}\)  Initialize meta policies \(\sigma_{i}^{1}\sim\mathrm{UNIFORM}(\Pi_{i}^{1})\) for\(t\) in \(\{1,2,...\}\)do forplayer\(i\in\{1,2\}\)do  Initialize \(\pi_{i}=\pi_{i}^{t}\) and sample \(K\) policies \(\{\pi_{i}^{k}\}_{k=1}^{K}\) from Policy Hull \(\Pi_{i}^{t}\) formany episodesdo  Sample \(\pi_{-i}\sim\sigma_{-i}^{t}\) and collect the trajectory \(\tau\) by playing \(\pi_{i}\) against \(\pi_{-i}\)  Discount the terminal reward \(u(\pi_{i},\pi_{-i})\) to each state as the extrinsic reward \(r_{1}\)  Discount \(R^{KL}(\tau)\) to each state as the intrinsic reward \(r_{2}\)  Store \((s,a,s^{\prime},r)\) to the buffer, where \(s^{\prime}\) is the next state and \(r=r_{1}+r_{2}\)  Estimate the gradient in Equation 17 with the samples in the buffer and update \(\pi_{i}\)  endfor \(\pi_{i}^{t+1}=\pi_{i}\) and \(\Pi_{i}^{t+1}=\Pi_{i}^{t}\cup\{\pi_{i}^{t+1}\}\) endfor  Compute missing entries in \(\mathbf{M}_{\Pi_{i}^{t},\Pi_{-i}^{t}}\)  Compute meta-strategies \((\sigma_{i}^{t+1},\sigma_{-i}^{t+1})\) from \(\mathbf{M}_{\Pi_{i}^{t},\Pi_{-i}^{t}}\) endfor Output: current meta-strategy for each player. ```

**Algorithm 1** PSD-PSRO

## Appendix D Additional Study

### PE on Leduc

In Figure 4, we report the _exploitability_ of the meta NE on Goofspiel with 5 point cards setting.

### Ablation Study on Diversity Weight

We conduct the ablation study on the sensitivity of the diversity weight \(\lambda\) on the Non-Transitive Mixture Game and Leduc Poker. In Table 3 and Table 4, we show the _exploitability_ of the final population against different diversity weights in Non-Transitive Mixture Game and Leduc Poker, respectively. We can see that the \(\lambda\) can significantly affect the training convergence and the final _exploitability_. A large or small \(\lambda\) can lead to high _exploitability_. This suggests that properly combing the BR (implicitly enlarges the Policy Hull) and PSD (explicitly enlarges the Policy Hull) is critical to improving the efficiency at reducing the _exploitability_.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \(\lambda\) & 0 & 1 & 2 & 3 & 5 & 10 & 50 \\ \hline Exp & 40.88 \(\pm\) 2.18 & 2.81 \(\pm\) 0.66 & **1.72 \(\pm\) 0.54** & 2.87 \(\pm\) 0.70 & 3.73 \(\pm\) 0.77 & 4.31 \(\pm\) 0.67 & 8.22 \(\pm\) 1.73 \\ \hline \hline \end{tabular}
\end{table}
Table 3: The _exploitability_\(\times 100\) for populations generated by PSD with different diversity weights in Non-transitive Mixture Games. The standard error is calculated by running \(5\) experiments.

### The Visualization of PH

we demonstrate the expansion of PH on the Disc game [2]. In Disc game, each pure strategy is represented by a point in the circle. Due to the linearity in the payoff function, a mixed strategy is equivalent to a pure strategy and thus can be equivalently visualized. Figure 5 shows the expansion of PH on the Disc game [2]. It demonstrates that PSD-PSRO is more effective than other PSRO variants at enlarging the PH.

### Time Consumption in PSD-PSRO

We run an experiment on Leduc poker to estimate the time portion of different parts in our algorithm PSD-PSRO. We can see from Table 5 that the majority of time in PSD-PSRO is in BR solving and computing the payoff matrix, which is shared by PSRO and all its variants. The time spent on calculating our diversity metric is not significant, which accounts for \(7.9\%\) computational time.

## Appendix E Benchmark and Implementation Details

### AlphaStar888

AlphaStar888 is an empirical game generated from the process of solving Starcraft II [50], which contains a payoff table for \(888\) RL policies. The occupancy measure is reduced to the action distribution as the game can be regarded as a single-state Markov Game. We choose diversity weight \(\lambda\) to be \(0.85\) in this game.

### Non-Transitive Mixture Game

This game consists 7 equally-distanced Gaussian humps on the 2D plane. We represent each strategy by a point on the 2D plane, which is equivalent to the weights (the likelihood of that point in each Gaussian distribution) that each player puts on the humps. The payoff of the game which contains

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \(\lambda\) & 0 & 0.1 & 0.5 & 1 & 2 & 5 \\ \hline Exp & \(0.590\pm 0.017\) & \(0.403\pm 0.027\) & \(\mathbf{0.356\pm 0.012}\) & \(0.398\pm 0.030\) & \(0.412\pm 0.042\) & \(0.664\pm 0.062\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: The _exploitability_ for populations generated by PSD with different diversity weights in Leduc Poker. The standard error is calculated by running \(3\) experiments.

Figure 4: Exploitability on Goofspiel (5 point cards).

both non-transitive and transitive components is \(\pi_{i}^{T}\mathcal{S}\pi_{-i}+\frac{1}{2}\sum_{k=1}^{7}(\pi_{i}^{k}-\pi_{-i}^{k})\), where

\[\mathcal{S}=\begin{bmatrix}0&1&1&1&-1&-1&-1\\ -1&0&1&1&1&-1&-1\\ -1&-1&0&1&1&1&-1\\ -1&-1&-1&0&1&1&1\\ 1&-1&-1&-1&0&1&1\\ 1&1&-1&-1&-1&0&1\\ 1&1&1&-1&-1&-1&0\end{bmatrix}.\]

We choose diversity weight \(\lambda\) to be \(1.9\) in this game.

Figure 5: Equivalent visualization of PH of different methods across iterations in the Disc game

### Leduc Poker

Since the DPP-PSRO needs evolutionary updates and cannot scale to the RL setting, and the code for BD&RD-PSRO on complex games, where f-divergence on occupancy measure is approximated using prediction errors of neural networks, is not available, we compare PSD-PSRO with P-PSRO, PSRO\({}_{FN}\) and PSRO in this benchmark. We implement the PSRO framework with Nash solver, using PPO as the oracle agent. Hyper-parameters are shown in Table 6.

### Goofspiel

In game theory and artificial intelligence, Goofspiel is commonly used as an example of a multi-stage simultaneous move game. In two-player Goofspiel, each player has one suit of cards, which is ranked \(A\) (low), \(2\),..., \(10\), \(J\), \(Q\), \(K\) (high). Another suit of cards serves as the prize (competition cards). Play proceeds in a series of rounds. The players make sealed bids for the top (face up) prize by selecting a card from their hand (keeping their choice secret from their opponent). Once these cards are selected, they are simultaneously revealed, and the player making the highest bid takes the competition card. If tied, the competition card is discarded. The cards used for bidding are discarded, and play continues

\begin{table}
\begin{tabular}{l l} \hline \hline Hyperparameters & Value \\ \hline _Oracle_ & \\ Oracle agent & PPO \\ Replay buffer size & \(10^{4}\) \\ Mini-batch size & 512 \\ Optimizer & Adam \\ Learning rate & \(3\times 10^{-4}\) \\ Discount factor (\(\gamma\)) & 0.99 \\ Clip & 0.2 \\ Max Gradient Norm & 0.05 \\ Policy network & MLP (state\_dim-256-256-action\_dim) \\ Activation function in MLP & ReLu \\ \hline _PSRO_ & \\ Episodes for each BR training & \(2\times 10^{4}\) \\ meta-policy solver & Nash \\ \hline _PSRO\({}_{rN}\)_ & \\ Episodes for each BR training & \(2\times 10^{4}\) \\ meta-policy solver & rectified Nash \\ \hline _P-PSRO_ & \\ Episodes for each BR training & \(2\times 10^{4}\) \\ meta-policy solver & Nash \\ number of threads & 3 \\ \hline _PSD-PSRO_ & \\ Episodes for each BR training & \(2\times 10^{4}\) \\ meta-policy solver & Nash \\ diversity weight \(\lambda\) & 0.1 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Hyperparameters for Leduc poker

\begin{table}
\begin{tabular}{|c|c|} \hline \hline Main Part of PSD-PSRO & Percentage \\ \hline compute NE of the meta-game & 0.9\% \\ \hline compute the approximate BR & 59.9\% \\ \hline compute the extra diversity metric & 7.9\% \\ \hline compute the payoff matrix & 31.3\% \\ \hline \hline \end{tabular}
\end{table}
Table 5: Time Consumption in PSD-PSRO on Leduc pokerwith a new upturned prize card. After 13 rounds, there are no remaining cards and the game ends. Typically, players earn points equal to the sum of the ranks of cards won (i.e. \(A\) is worth one point, 2 is worth two points, etc., \(J\) 11, \(Q\) 12, and \(K\) 13 points). Goofspiel demonstrates high non-transitivity. Any pure strategy in this game has a simple counter-strategy where the opponent bids one rank higher, or as low as possible against the King bid. As an example, consider the strategy of matching the upturned card value mentioned in the previous section. The final score will be 78 - 13 with the \(K\) being the only lost prize. we focus on the simple versions of this game: goofspiel with 5 point cards and 8 point cards settings.

Similar to the setting in Leduc poker, we compare PSD-PSRO with P-PSRO, PSRO\({}_{rN}\), and PSRO in this benchmark. We implement the PSRO framework with Nash solver, using DQN as the Oracle agent. We use Linear Programming to solve NE from the payoff table. As the game size of Goofspiel is larger than Leduc poker, we improve the capacity of the model by setting the hidden dimension in MLP to be 512 and the episodes for each BR training to be \(3\times 10^{4}\). Other settings are similar to Leduc poker in Table 6.