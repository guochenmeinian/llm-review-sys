# SEEDS: Exponential SDE Solvers for Fast

High-Quality Sampling from Diffusion Models

 Martin Gonzalez

IRT SystemX

Nelson Fernandez

Air Liquide

Thuy Tran

IRT SystemX

Elies Gherbi

IRT SystemX

Hatem Hajri

IRT SystemX

& Safran

& Nader Masmoudi

New York University

Corresponding author: martin.gonzalez@irt-systemx.fr

###### Abstract

A potent class of generative models known as Diffusion Probabilistic Models (DPMs) has become prominent. A forward diffusion process adds gradually noise to data, while a model learns to gradually denoise. Sampling from pre-trained DPMs is obtained by solving differential equations (DE) defined by the learnt model, a process which has shown to be prohibitively slow. Numerous efforts on speeding-up this process have consisted on crafting powerful ODE solvers. Despite being quick, such solvers do not usually reach the optimal quality achieved by available slow SDE solvers. Our goal is to propose SDE solvers that reach optimal quality without requiring several hundreds or thousands of NFEs to achieve that goal. We propose Stochastic Explicit Exponential Derivative-free Solvers (SEEDS), improving and generalizing Exponential Integrator approaches to the stochastic case on several frameworks. After carefully analyzing the formulation of exact solutions of diffusion SDEs, we craft SEEDS to analytically compute the linear part of such solutions. Inspired by the Exponential Time-Differencing method, SEEDS use a novel treatment of the stochastic components of solutions, enabling the analytical computation of their variance, and contains high-order terms allowing to reach optimal quality sampling \(\sim 3\)-\(5\times\) faster than previous SDE methods. We validate our approach on several image generation benchmarks, showing that SEEDS outperform or are competitive with previous SDE solvers. Contrary to the latter, SEEDS are derivative and training free, and we fully prove strong convergence guarantees for them. Our code is publicly available in this link.

## 1 Introduction

Diffusion Probabilistic Models (DPMs) [32; 12] have emerged as a powerful category of generative models and have proven to quickly become SOTA for generative tasks such as image, video, audio generation [8; 24; 13; 3; 4], and more [29; 36]. These models employ a forward diffusion process where noise is gradually added to the data, and the model learns to remove the noise progressively. However, sampling from most pre-trained DPMs is done by simulating the trajectories of associated differential equations (DE) and has been found to be prohibitively slow [34]. Previous attempts to accelerate this process have mainly focused on developing efficient ODE solvers. On one hand, training-based methods speed-up sampling by using auxiliary training such as Progressive Distillation [31] and Fourier Neural Operators [42], learning the noise schedule, scaling, variance, or trajectories. On the other hand, training-free methods [16; 22; 15; 40; 23] are slower but are more versatile forbeing employed on different models and achieve higher quality results than current training-based methods. Although these solvers are fast, they often fall short of achieving the optimal quality attained by slower SDE solvers [16]. The latter usually do not present theoretical convergence guarantees and, while being training-free, they often still require costly parameter optimization to achieve optimal results which might be difficult to estimate for large datasets.

Our objective is to introduce SDE solvers that can achieve optimal quality without requiring an excessively large number of function evaluations (NFEs). To accomplish this, we propose Stochastic Explicit Exponential Derivative-free Solvers (SEEDS). These are _off-the-shelf_ SDE samplers: they offer promising high-quality sampling without further training or parameter optimization. SEEDS enhance and generalize existing Exponential Integrator [22; 41; 23] approaches to the stochastic case on several practical frameworks and is based on the following 4 building blocks: (1) the exponential representation of semi-linear SDE exact solutions which isolate linear terms to be computed analytically; (2) a general change-of-variables recipe to simplify the integrals involved in the solutions in order to better approximate the deterministic one; (3) a method to analytically compute the variance of the stochastic one; (4) a method to decompose the obtained stochastic components in such a way that the resulting sequences of higher-stage numerical approximations yield Markov chains.

Overall, we make the following contributions: (a) our change-of-variables method allow us to re-frame the gDDIM solver [41] as a special case of SEEDS and to craft bespoke solvers for the EDM-preconditioned DPMs in [16] which attain equivalent sampling quality twice faster than the previous SOTA sampling method [16]; (b) based on the Stochastic Exponential Time-Differencing method, we analytically compute of our solver's stochastic components in terms of the so-called \(\varphi\)-functions, allowing for efficient implementation; (c) our noise decomposition method (4), which is the key of success of SEEDS, is both theoretically grounded, experimentally shown to be optimal, and has no deterministic equivalent; (d) we provide full proofs of strong/weak convergence guarantees for our SDE solvers which, to our knowledge, has no precedent in the DPM literature. In particular, although the formula used for the truncated Ito-Taylor expansion might seem similar to that of [22], our convergence theorems leverage the full Ito-Taylor expansion of solutions, making our proofs not incremental and different from [22]; (e) we conduct extensive experiments demonstrating that SEEDS establishes SOTA results among available solvers on several image generation benchmarks, or is competitive with existing SDE solvers while being 2-5 times faster than the latter.

Although our solvers theoretically apply to certain non-isotropic DPMs such as Critically-damped Langevin Dynamics (CLD) [10] (see Prop. 4.5 and Rem. 4.6), we will restrict our presentation to the isotropic case for which many notations become simpler.

## 2 Background on Diffusion Probabilistic Models

General Isotropic DE Formulation.The evolution of a data sample \(\mathbf{x}_{0}\in\mathbb{R}^{d}\) taken from an unknown data distribution \(p_{\mathrm{data}}\) into standard Gaussian noise can be defined as a forward diffusion process \(\{\mathbf{x}_{t}\}_{t\in[0,T]}\), with \(T>0\), which is a solution to a linear SDE:

\[\mathrm{d}\mathbf{x}_{t}=f(t)\mathbf{x}_{t}\mathrm{d}t+g(t)\mathrm{d} \boldsymbol{\omega}_{t},\qquad f(t):=\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{ d}t},\quad g(t)=\alpha_{t}\sqrt{\frac{\mathrm{d}[\sigma_{t}^{2}]}{\mathrm{d}t}},\] (1)

where \(f(t),g(t)\in\mathbb{R}^{d\times d}\) are called the drift and diffusion coefficients respectively and \(\boldsymbol{\omega}\) is a \(d\)-dimensional standard Wiener process, and \(\alpha_{t},\sigma_{t}\in\mathbb{R}^{>0}\) are differentiable functions with bounded derivatives. In practice, when specifying the SDE (1), \(\sigma_{t}\) acts as a schedule controlling the noise levels of an input at time \(t\), and \(\alpha_{t}\) as a time-dependent signal scaling controlling its dynamic range.

By denoting \(p_{t}(\mathbf{x}_{t})\) the marginal distribution of \(\mathbf{x}_{t}\) at time \(t\), functions \(\alpha_{t}\) and \(\sigma_{t}\) are designed so that the end-time distribution of the process process is \(p_{T}(\mathbf{x}_{T})\approx\mathcal{N}(\mathbf{x}_{T}|\boldsymbol{0}, \tilde{\sigma}^{2}\mathbf{I}_{d})\) for some \(\tilde{\sigma}>0\). As (1) is linear, the transition probability \(p_{0t}(\mathbf{x}_{t}|\mathbf{x}_{0})\) from \(\mathbf{x}_{0}\) to \(\mathbf{x}_{t}\) is Gaussian whose mean and variance can be expressed in terms of \(\alpha_{t}\) and \(\sigma_{t}\). For simplicity, we will denote it

\[p_{0t}(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathcal{N}(\mathbf{x}_{t};\mu_{t} \mathbf{x}_{0},\Sigma_{t}),\qquad\mu_{t},\Sigma_{t}\in\mathbb{R}^{d\times d}.\]

The evolution of the reverse time process of \(\{\mathbf{x}_{t}\}_{t\in[0,T]}\) (which we will still denote \(\{\mathbf{x}_{t}\}_{t\in[0,T]}\) for simplicity) is then driven by a backward differential equation

\[\mathrm{d}\mathbf{x}_{t}=[f(t)\mathbf{x}_{t}-\frac{1+\ell^{2}}{2}g^{2}(t) \nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t})]\mathrm{d}t+\ell g(t) \mathrm{d}\tilde{\omega}_{t},\] (2)where \(\mathrm{d}t\) are negative infinitesimal time-steps and \(\bm{\omega}_{t}\) is now a Wiener process with variance \(-\mathrm{d}t\). In this article, we will concentrate in the cases \(\ell=0,1\), known in the literature as the Probability Flow ODE (PPO) and diffusion reverse SDE (RSDE), respectively.

Training.Denoising score-matching is a technique to train a time-dependent model \(D_{\theta}(\mathbf{x}_{t},t)\) to approach the score function \(\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t})\) at each time \(t\). Intuitively, as \(D_{\theta}\) approaches the score, it produces a sample which maximizes the log-likelihood. As such, this model is coined as a _data prediction_ model. However, in practice DPDs can be more efficiently trained by reparameterizing \(D_{\theta}\) into a different model \(F_{\theta}(\mathbf{x}_{t},t)\) whose objective is to predict the noise to be removed from a sample at time \(t\). This _noise prediction_ model is trained by means of the loss

\[\mathbb{E}_{t\sim\mathcal{U}[0,T],\mathbf{x}_{0}\sim p_{\mathrm{data}}, \epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{\delta})}[\|\epsilon-F_{\theta }(\mu_{t}\mathbf{x}_{0}+\bm{K}_{t}\epsilon,t)\|^{2}_{\bm{K}_{t}^{-1}\gamma_{t }\bm{K}_{t}^{-\top}}],\]

where \(\gamma_{t}\) is a time dependent weighting parameter and \(\bm{K}_{t}\bm{K}_{t}^{\top}=\Sigma_{t}\).

## 3 Accelerating Optimal Quality Solvers for Diffusion SDEs

Once \(F_{\theta}\) or \(D_{\theta}\) have been trained, one can effectively solve (2) after replacing the score function by its corresponding expression involving either one of these models. For instance, taking the noise prediction model and \(\ell=1\), sampling is conducted by simulating trajectories of a SDE of the form

\[\mathrm{d}\mathbf{x}_{t}=[A(t)\mathbf{x}_{t}+b(t)F_{\theta}(\mathbf{x}_{t},t) ]\mathrm{d}t+g(t)\mathrm{d}\bm{\omega}_{t},\] (3)

for some functions \(A(t),b(t)\) which are usually not equal to \(f(t),g^{2}(t)\). In what follows, we consider a time discretization \(\{t_{i}\}_{i=0}^{M}\) going backwards in time starting from \(t_{0}=T\) to \(t_{M}=0\) and to ease the notation we will always denote \(t<s\) for two consecutive time-steps \(t_{i+1}<t_{i}\).

The usual representation of the analytic solution \(\mathbf{x}_{t}\) at time \(t\) of (3) w.r.t. an initial condition \(\mathbf{x}_{s}\) is:

\[\mathbf{x}_{t}=\mathbf{x}_{s}+\int_{s}^{t}[A(\tau)\mathbf{x}_{\tau}+b(\tau)F_ {\theta}(\mathbf{x}_{\tau},\tau)]\mathrm{d}\tau+\int_{s}^{t}g(\tau)\mathrm{d} \bm{\omega}_{\tau}.\] (4)

The numerical schemes we propose for approaching the trajectories of (3) based on representation (4) are grounded in the 4 following principles:

1. The variation-of-parameters formula: representing analytic solutions with linear term extracted from the integrand;
2. Exponentially weighted integrals: extracting the time-varying linear coefficient attached to the network from the integrand by means of a specific choice of change of variables which allows analytic computation of the leading coefficients in the truncated Ito-Taylor expansion associated to \(F_{\theta}(\mathbf{x}_{\tau},\tau)\) up to any arbitrary order;
3. Modified Gaussian increments: after replicating such change of variables onto the stochastic integral, analytically computing its variance.
4. Markov-preserving noise decomposition: stochastic integrals need to be dependent on overlapping time intervals and independent on non-overlapping ones.

Exponential representation of exact solutions of diffusion SDEs.The first key insight of this work is that, using the _variation-of-parameters_ formula, we can represent the analytic solution \(\mathbf{x}_{t}\) at time \(t\) of (3) with respect to an initial condition \(\mathbf{x}_{s}\) as follows:

\[\mathbf{x}_{t}=\Phi_{A}(t,s)\mathbf{x}_{s}+\int_{s}^{t}\Phi_{A}(t,\tau)b(\tau) F_{\theta}(\mathbf{x}_{\tau},\tau)\mathrm{d}\tau+\int_{s}^{t}\Phi_{A}(t,\tau)g( \tau)\mathrm{d}\bm{\omega}_{\tau},\] (5)

where \(\Phi_{A}(t,s)=\exp\left(\int_{s}^{t}A(\tau)\mathrm{d}\tau\right)\) is called the transition matrix associated with \(A(t)\). The separation of the linear and nonlinear components is achieved by this formulation and also appears in [22; 23; 41]. It differs from black-box SDE solvers as it enables the exact calculation of the linear portion, thereby removing any approximation errors associated with it. However, the integration of the nonlinear portion remains complex due to the interaction of the new coefficient \(\Phi_{A}(t,\tau)b(\tau)\) and the intricate neural network, making it challenging to approximate.

Exponentially weighted integrals.Due to the regularity conditions usually imposed on the drift and diffusion coefficients of (1), one can make several choices of change-of-variables on the integral components in (5) in order to simplify it. Our second key insight is that there is a specific choice of change of variables allowing the analytic computation of the Ito-Taylor coefficients of \(F_{\theta}(\mathbf{x}_{\tau},\tau)\) with respect to \(\tau\), and based at \(s\) that will be used for crafting SEEDS. More specifically, this expansion reads

\[F_{\theta}(\mathbf{x}_{\tau},\tau)=\sum_{k=0}^{n}\frac{(\tau-s)^{k}}{k!}F_{ \theta}^{(k)}(\mathbf{x}_{s},s)+\mathcal{R}_{n},\]

where the residual \(\mathcal{R}_{n}\) consists of deterministic iterated integrals of length greater than \(n+1\) and all iterated integrals with at least one stochastic component. As such, we obtain

\[\int_{s}^{t}\Phi_{A}(t,\tau)b(\tau)F_{\theta}(\mathbf{x}_{\tau},\tau)\mathrm{d }\tau=\sum_{k=0}^{n}F_{\theta}^{(k)}(\mathbf{x}_{s},s)\int_{s}^{t}\Phi_{A}(t, \tau)b(\tau)\frac{(\tau-s)^{k}}{k!}\mathrm{d}\tau+\tilde{\mathcal{R}}_{n},\] (6)

where \(\tilde{\mathcal{R}}_{n}\) is easily obtained from \(\mathcal{R}_{n}\) and \(\int_{s}^{t}\Phi_{A}(t,\tau)b(\tau)\mathrm{d}\tau\). The third key contribution of our work is to rewrite, for any \(k\geqslant 0\), the integral \(\int_{s}^{t}\Phi_{A}(t,\tau)b(\tau)\frac{(\tau-s)^{k}}{k!}\mathrm{d}\tau\) as an integral of the form \(\int_{\lambda_{s}}^{\lambda_{t}}e^{\lambda\,\frac{(\lambda-\lambda_{s})^{k}}{k!}}\mathrm{d}\lambda\) since the latter is recursively analytically computed in terms of the \(\varphi\)-functions

\[\varphi_{0}(t):=e^{t},\qquad\varphi_{k+1}(t):=\int_{0}^{1}e^{(1-\tau)t}\frac{ \tau^{k}}{k!}\mathrm{d}\tau=\frac{\varphi_{k}(t)-\varphi_{k}(0)}{t},\qquad k \geqslant 0.\]

Modified Gaussian increments.In order for making such change of variables to be consistent on the overall system, one needs to replicate it accordingly in the stochastic integral \(\int_{s}^{t}\Phi_{A}(t,\tau)g(\tau)\mathrm{d}\bar{\bm{\omega}}_{\tau}\). As such, our last key contribution is to transform it into an exponentially weighted stochastic integral with integration endpoints \(\lambda_{s},\lambda_{t}\) and apply the Stochastic Exponential Time Differencing (SETD) method [1] to compute its variance analytically, as illustrated in (14) below.

Let us test our methodology in two key examples. As we explained in Section 2, sampling from pre-trained DPMs amounts on choosing a schedule \(\sigma_{t}\), a scaling \(\alpha_{t}\), and a parameterized learnt approximation of the score function \(\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t})\). In what follows, we denote by \(t_{\lambda}\) the inverse of a chosen change of variables \(\lambda_{t}\) and we denote \(\widehat{\mathbf{x}}_{\lambda}:=\mathbf{x}(t_{\lambda}(\lambda)),\hat{F}_{ \theta}(\widehat{\mathbf{x}}_{\lambda},\lambda):=F_{\theta}(\mathbf{x}(t_{ \lambda}(\lambda)),t_{\lambda}(\lambda))\).

The VPSDE case.Let \(\tilde{\alpha}_{t}:=\frac{1}{2}\beta_{d}t^{2}+\beta_{m}t\), where \(\beta_{d},\beta_{m}>0\) and \(t\in[0,1]\). Then, by denoting

\[\sigma_{t}:=\sqrt{e^{\tilde{\alpha}_{t}}-1},\qquad\alpha_{t}:=e^{-\frac{1}{2} \tilde{\alpha}_{t}},\qquad\bar{\sigma}_{t}:=\alpha_{t}\sigma_{t},\qquad\nabla _{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t})\simeq\bar{\sigma}_{t}^{-1}F_{ \theta}(\mathbf{x}_{t},t),\] (7)

we obtain the VP SDE framework from [34] and the following result.

**Proposition 3.1**.: _Let \(t<s\). The analytic solution at time \(t\) of the RSDE (2) with coefficients (7) and initial value \(\mathbf{x}_{s}\) is_

\[\mathbf{x}_{t}=\frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-2\alpha_{t}\int_{ \lambda_{s}}^{\lambda_{t}}e^{-\lambda}\hat{F}_{\theta}(\widehat{\mathbf{x}}_{ \lambda},\lambda)\mathrm{d}\lambda-\sqrt{2}\alpha_{t}\int_{\lambda_{s}}^{ \lambda_{t}}e^{-\lambda}\mathrm{d}\bar{\bm{\omega}}_{\lambda},\qquad\lambda_{t}: =-\log(\sigma_{t}).\] (8)

The change of variables of (8) is interesting as it allows to compute analytically the Ito-Taylor coefficients in (6) by using, for \(h=\lambda_{t}-\lambda_{s}\), the following key result which will be used in Prop. 4.2:

\[\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\frac{(\lambda-\lambda_{s})^{k}}{k!}\mathrm{d}\lambda=\sigma_{t}h^{k+1}\varphi_{k+1}(h).\] (9)

For instance, in the case when \(k=0\), it is easy to see that \(\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\lambda=\sigma_{t}(e^{h}-1)\) and \(\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\bar{\bm{\omega}}_{\lambda}\) obeys a normal distribution with zero mean, and one can analytically compute its variance:

\[\int_{\lambda_{s}}^{\lambda_{t}}e^{-2\lambda}\mathrm{d}\lambda=\frac{\sigma_{t} ^{2}}{2}(e^{2h}-1).\] (10)The EDM case.Denote \(\sigma_{d}^{2}\) the variance of the considered initial dataset and set

\[\sigma_{t}:=t,\alpha_{t}:=1,\nabla_{\mathbf{x}_{t}}\log p_{t}(\mathbf{x}_{t}) \simeq\frac{1}{t^{2}}\left[\frac{\sigma_{d}^{2}\mathbf{x}_{t}}{t^{2}+\sigma_{d }^{2}}+\frac{t\sigma_{d}}{\sqrt{t^{2}+\sigma_{d}^{2}}}F_{\theta}\left(\frac{ \mathbf{x}_{t}}{\sqrt{t^{2}+\sigma_{d}^{2}}},\frac{\log(t)}{4}\right)\right].\] (11)

These parameters correspond to the preconditioned EDM framework introduced in [16, Sec. 5, App. B.6]. The following result is the basis for constructing our customized SEEDS in this case, and for which we report experimental results in Table 1. For simplicity, we will write \(F_{\theta}(\mathbf{x}_{t},t)\) for the preconditioned model in (11) and we refer to Appendix B for details.

**Proposition 3.2**.: _Let \(t<s\). The analytic solution at time \(t\) of (2) with coefficients (11) and initial value \(\mathbf{x}_{s}\) is, for \(\ell=1\),_

\[\mathbf{x}_{t}=\frac{t^{2}+\sigma_{d}^{2}}{s^{2}+\sigma_{d}^{2}}\mathbf{x}_{s }+2(t^{2}+\sigma_{d}^{2})\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\hat{F}_{ \theta}(\widehat{\mathbf{x}}_{\lambda},\lambda)\mathrm{d}\lambda-\sqrt{2}(t^ {2}+\sigma_{d}^{2})\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d} \overline{\omega}_{\lambda},\] (12)

_where \(\lambda_{t}:=-\log\left[\frac{t}{\sigma_{d}\sqrt{t^{2}+\sigma_{d}^{2}}}\right]\). In the case when \(\ell=0\), it is given by_

\[\mathbf{x}_{t}=\sqrt{\frac{t^{2}+\sigma_{d}^{2}}{s^{2}+\sigma_{d}^{2}}} \mathbf{x}_{s}+\sqrt{t^{2}+\sigma_{d}^{2}}\int_{\lambda_{s}}^{\lambda_{t}}e^{ -\lambda}\hat{F}_{\theta}(\widehat{\mathbf{x}}_{\lambda},\lambda)\mathrm{d} \lambda,\quad\lambda_{t}:=-\log\left[\arctan\left[\frac{t}{\sigma_{d}}\right] \right].\] (13)

_Remark 3.3_.: One can wonder about the generality of such change of variables. Our method is very general in that one can always make such change of variables with very mild regularity conditions: for \(c:[0,T]\longrightarrow\mathbb{R}^{>0}\) integrable, with primitive \(C(t)>0\), we have \(c(t)=e^{\log(c(t))}\). This means we can write \(c(t)=\dot{C}(t)=e^{\lambda_{t}}\dot{\lambda}_{t}\) with \(\lambda_{t}=\log(C(t))\). In other words, for such \(c\), we have

\[\int_{s}^{t}c(\tau)\mathrm{d}\tau=\int_{s}^{t}e^{\lambda_{\tau}}\dot{\lambda} _{\tau}\mathrm{d}\tau=\int_{\lambda_{s}}^{\lambda_{t}}e^{\lambda}\mathrm{d}\lambda.\]

## 4 Higher Stage SEEDS for DPMs

In this section we present our SEEDS algorithms by putting together all the ingredients presented in the previous section. Let \(t<s\). In all what follows, we consider the analytic solution at time \(t\) of the RSDE (2) with coefficients (7), \(h=\lambda_{t}-\lambda_{s}\) and initial value \(\mathbf{x}_{s}\). Plugging (9) with \(k=0\) and (10) into the exact solution (8) allow us to infer the first SEEDS scheme, given by iterations of the form

\[\tilde{\mathbf{x}}_{t}=\frac{\alpha_{t}}{\alpha_{s}}\tilde{\mathbf{x}}_{s}-2 \bar{\sigma}_{t}(e^{h}-1)\hat{F}_{\theta}(\widehat{\mathbf{x}}_{\lambda_{s}}, \lambda_{s})-\bar{\sigma}_{t}\sqrt{e^{2h}-1}\epsilon,\qquad\epsilon\sim\mathcal{ N}(\mathbf{0},\mathbf{I}_{d}).\] (14)

The following Theorem gives strong order convergence guarantees for this method, which we call SEEDS-1, under mild conditions which apply to all our experiments. We stress out that its proof (App. C) is a non-trivial result, it is fundamentally different in nature from [22] and involves mathematical tools which have no deterministic counterparts.

**Theorem 4.1**.: _Under Assumption C.1, the numerical solution \(\tilde{\mathbf{x}}_{t}\) produced by the SEEDS-1 method (14) converges to the exact solution \(\mathbf{x}_{t}\) of_

\[\mathrm{d}\mathbf{x}_{t}=[f(t)\mathbf{x}_{t}+g^{2}(t)\bar{\sigma}_{t}^{-1}F_{ \theta}(\mathbf{x}_{t},t)]\mathrm{d}t+g(t)\mathrm{d}\boldsymbol{\omega}_{t}, \qquad(\bar{\sigma}_{t}^{-1}:=1/\bar{\sigma}_{t})\] (15)

_with coefficients (7) in Mean-Square sense with strong order 1.0: there is a constant \(C>0\) such that_

\[\sqrt{\mathbb{E}\left[\sup_{0\leqslant t\leqslant 1}|\tilde{\mathbf{x}}_{t}- \mathbf{x}_{t}|^{2}\right]}\leqslant Ch,\qquad\text{as }h\longrightarrow 0.\]

Higher stage SEEDS.As announced, by fully exploiting the analytic computations enabled by the expansion (9) we now turn into crafting our multi-step SEEDS. Usually, SDE solvers are constructed by using the full Ito-Taylor expansion of the SDE solutions and usually need a big number of evaluations of the network \(\hat{F}_{\theta}\) to achieve higher order of convergence. As our main concern is to present stochastic solvers with a minimal amount of NFE, we choose to truncate such Ito-Taylor expansion so that the neural networks only appear in the deterministic contributions.

**Proposition 4.2**.: _Assume that \(\hat{F}_{\theta}\) is a \(\mathcal{C}^{2n+1}\)-function with respect to \(\lambda\). Then the truncated Ito-Taylor expansion of (8) reads, for \(\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\),_

\[\mathbf{x}_{t}=\frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-2\bar{\sigma}_{t} \sum_{k=0}^{n}h^{k+1}\varphi_{k+1}(h)\hat{F}_{\theta}^{(k)}(\widehat{\mathbf{ x}}_{\lambda_{s}},\lambda_{s})-\bar{\sigma}_{t}\sqrt{e^{2h}-1}\epsilon+\mathcal{R}_{n+1},\] (16)

_with \(\hat{F}_{\theta}^{(k)}(\widehat{\mathbf{x}}_{\lambda},\lambda)=L_{\lambda}^{k }\hat{F}_{\theta}(\widehat{\mathbf{x}}_{\lambda},\lambda)\), with \(L_{\lambda}\) is an infinitesimal operator defined in Appendix E.2.2 and \(\mathcal{R}_{n+1}\) consists on the usual deterministic residue and all iterated integrals of length at greater or equal to 2 in which there is at least one stochastic component among them._

Our approach for constructing derivative-free 2-stage and 3-stage SEEDS schemes consists on exploiting the analytic computation of the Ito-Taylor coefficients in Proposition 4.2 and replace the \(\hat{F}_{\theta}^{(k)}(\widehat{\mathbf{x}}_{\lambda},\lambda)\) terms by well-adapted correction terms which _do not need any derivative evaluation_ and dropping the \(\mathcal{R}_{n+1}\) contribution as in the Runge-Kutta approach.

Markov-preserving noise decomposition.We use collocation methods for constructing higher-stage derivative-free solvers. Although the chosen truncated Ito-Taylor expansion produces approximations for the deterministic integral similar to [22], adding the corresponding noise contribution found by the SETD method at each step does not yield Markov chains in general. The reason is that stochastic integrals on overlapping time intervals need to be dependent, a phenomenon that has no deterministic counterpart. As such, our last and key element to construct SEEDS consists on a novel decomposition of stochastic integrals which enforces the Markov property for multi-stage SEEDS.

Algorithms 1 to 4 prescribe all SEEDS schemes obtained by this procedure in the VP case. We now show (see App. C for the proofs) that all methods yield Markov chains and are weakly convergent.

**Proposition 4.3**.: _The sequences \(\{\tilde{\mathbf{x}}_{t}\}_{t}\) induced by the choice of stochastic noise contributions presented in Algorithms.3 and 4 satisfy the Markov property._

**Corollary 4.4**.: _Under Assumption C.2, the numerical solutions \(\tilde{\mathbf{x}}_{t}\) produced by the SEEDS methods (3) and (4) converge to the exact solution \(\mathbf{x}_{t}\) of (15) with coefficients (7) in weak sense with global order 1 in both cases: there is a constant \(C>0\) such that, for any continuous bounded function \(G\):_

\[|\mathbb{E}[G(\tilde{\mathbf{x}}_{t_{M}})]-\mathbb{E}[G(\mathbf{x}_{t_{M}})] |\leqslant Ch.\]Comparison with existing sampling methods.Let us now examine the connection between SEEDS and existing sampling techniques used for DPMs, emphasizing the contrasts between them.

The main distinctive feature of SEEDS is that they are _off-the-shelf_ solvers. This means that, not only they are _training-free_, contrary to [9], but they do not require any kind of optimization procedure to achieve their optimal results. This is in contrast to methods such as: gDDIM, which is training-free but not off-the-shelf as one needs to make preliminary optimization procedures such as simulating the transition matrix of their method in the CLD case; Heun-Like method from EDM (for all baseline models and the EDM-optimized models for ImageNet) since they need preliminary optimization procedures on 4 parameters which actually break the convergence criteria. Moreover, neither gDDIM, EDM nor the SSCS method in [10] present full proofs of convergence for their solvers. Also, both DEIS and gDDIM identify their methods with stochastic DDIM theoretically, but the poor results obtained by their stochastic solvers do not yield to further experimentation in their works. In a way, SEEDS can be thought as improved and generalized DPM-Solver to SDEs. Nevertheless, such generalization is not incremental as the tools for proving convergence in our methods involve concepts which are exclusive to SDEs. We now make rigorous statements of the above discussion.

**Proposition 4.5**.: _Consider the SEEDS approximation of (15) with coefficients (7). Then_

1. _If we set_ \(g=0\) _in (_15_), the resulting SEEDS do not yield DPM-Solver._
2. _If we parameterize (_15_) in terms of the data prediction model_ \(D_{\theta}\)_, the resulting SEEDS are not equivalent to their noise prediction counterparts defined in Alg._ 1 _to 4_._
3. _The gDDIM solver_ _[_41_]__, Th. 1]_ _equals to SEEDS-1 in the data prediction mode, for_ \(\ell=1\)_._

The first point makes it explicit that SEEDS are not incremental based on DPM-Solver. The second point in Prop. 4.5 is analog to the result in Appendix B of [23], where the authors compare DPM-Solver2 and DPM-Solver++(2S), that is the noise and data prediction approaches, and find that they do not equate. The last point exhibits gDDIM as a special case of SEEDS-1 for isotropic DPMs.

_Remark 4.6_.: Building solvers from the representation of the exact solution in (5) requires computing the transition matrix \(\Phi_{A}(t,s)\), which cannot be analytically computed for non-isotropic DPMs such as CLD [10]. Nevertheless, the SEEDS approach can be applied in this scenario in at least two different ways. On the one hand, the SSCS method from [10] resides splitting \(\Phi_{A}(t,s)\) into two separate terms. The first can be analytically computed. The second describes the evolution of a semi-linear differential equation [10, Eq. 92]. While [10] approximates the latter by the Euler method, crafting exponential integrators for approximating such DE may yield an acceleration of the SSCS method. On the other hand, gDDIM [41] proposes an extension of DEIS sampling [40] to CLD by setting a pre-sampling phase [41, App. C.4] in which they compute an approximation of \(\Phi_{A}(t,s)\) in order to apply their method, and the latter was shown in Prop. 4.5. to be a special case of our method. Unfortunately, the authors did not release pre-trained models in [41], and the latter are not the same as those in [10]. Sampling in this scenario may also benefit from our approach.

## 5 Experiments

We compare SEEDS with several previous methods on discretely and continuously pre-trained DPMs. We report results of many available sources, such as DDPM [12], Analytic DDPM [2], PNDM [20], GGF [15], DDIM [33], gDDIM [41], DEIS [40] and DPM-Solver [22]. Although we do not include training-based schemes here, we still included GENIE [9], which trains a small additional network but still solves the correct generative ODE at higher-order. For each experiment, we compute the FID score for 50K sampled images on multiple runs and report the minimum along different solvers. Details on model specifications and experiment illustrations are shown in Appendix F.

Practical considerations.For continuously trained models, SEEDS use the EDM discretization [16, Eq. 5] with default parameters and does _not_ use the _last-step iteration trick_, meaning that the last iteration of SEEDS is trivial. For discretely trained models, SEEDS use the linear step schedule in the interval \([\lambda_{t_{0}},\lambda_{t_{S}}]\) interval following [22, Sec. 3.3, 3.4]. All the reported SEEDS results were obtained using the noise prediction mode. We conducted comparative experiments on SEEDS for both the data and noise prediction modes and found better results with the latter (see Tab. 3 for details). EDM solvers [16, Alg. 2] depend on four parameters controlling the amount of noise to be injected in a specific sub-interval of the iterative procedure. We consider three scenarios: when stochasticity is injected along all the iterative procedure, we denote it stochastic EDM, when no stochasticity is injected we denote it EDM (\(S_{\rm churn}=0\)) and we denote EDM (Optimized) the case where such parameters were subject to an optimization procedure. To better evaluate sampling quality along the sample pre-trained DPM, we recalculate DPM-Solver for sampling from the _non-deep_ VP continuous model on the CIFAR-10 dataset. All implementation details can be found in Appendix D.

Comparison with previous works.In Table 1 we compare SEEDS with other sampling methods for pre-trained DPMs, and report the minimum FID obtained and their respective NFE. For each of the reported pre-trained models in CIFAR-10, CelebA-64 and ImageNet-64, SEEDS outperform all off-the-shelf methods in terms of quality with relatively low NFEs. For the discrete pre-trained DPM on CIFAR-10 (VP Uncond.) it is \(\sim 5\times\) faster than the second best performant solver. Additionally, SEEDS remain competitive with the optimized EDM sampler. For ImageNet-64, it is nearly as good

\begin{table}
\begin{tabular}{l c c} \hline \hline Sampling method & FID\(\downarrow\) & NFE \\ \hline CIFAR-10* vp-uncond. & & \\ \hline DDIM [33] & 3.95 & 1000 \\ analytic-DDPM [2] & 3.84 & 1000 \\ GENIE [9] & 3.64 & 25 \\ Analytic-DDIM [2] & 3.60 & 200 \\ F-PNDM (linear) [20] & 3.60 & 250 \\ DPM-Solver\({}^{\dagger}\)[22] & 3.48 & 44 \\ F-PNDM (cosine) [20] & 3.26 & 1000 \\ DDPM [12] & 3.16 & 1000 \\ SEEDS-3 (ours) & **3.08** & **201** \\ \hline CIFAR-10* vp-cond. & & \\ \hline DPM-solver\({}^{\dagger}\)[22] & 3.57 & 195 \\ EDM (\(S_{\rm churn}=0\)) [16] & 2.48 & 35 \\ SEEDS-3 (Ours) & **2.08** & **129** \\ \hline CIFAR-10* vp-uncond. & & \\ \hline DPM-Solver\({}^{\dagger}\)[22] & 2.59 & 51 \\ GGF [15] & 2.59 & 180 \\ GODIM [41] & 2.56 & 100 \\ DEIS \(\rho\)3Kutta [40] & 2.55 & 50 \\ Euler-Maruyama [34] & 2.54 & 1024 \\ Stochastic EDM [16] & 2.54 & 1534 \\ SEEDS-3 (Ours) & 2.39 & 165 \\ EDM (Optimized) [16] & **2.27** & **511** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Sample quality measured by FID\(\downarrow\) on pre-trained DPMs. We report the minimum FID obtained by each model and the NFE at which it was obtained. For CIFAR, CelebA and FFHQ, we use baseline pre-trained models [34, 16]. For ImageNet, we use the optimized pre-trained model from [16]. *discrete-time model, *continuous-time model, \({}^{\dagger}\)recomputed FID for the non-deep model.

Figure 1: (a-b) Comparison of sample quality measured by FID \(\downarrow\) of SEEDS, DPM-Solver and other methods for discretely trained DPMs on CIFAR-10 with varying number of function evaluations. (c) Effect of \(S_{\rm churn}\) on SEEDS-3 (at NFE = 270) and EDM method (at NFE = 511) on class-conditional ImageNet-64. \({}^{\dagger}\)baseline ADM model. *EDM preconditioned model.

as the optimized EDM sampler while being almost twice faster than the latter. Figure 1 (a) compares the FID score of SEEDS and DPM-Solver with varying NFEs. While DPM-Solver methods stabilize faster in a very low NFE regime, our methods eventually surpass them. Interestingly, after reaching their minimum, SEEDS methods tend to become worse at higher NFEs, a fact that is also visible in Figure 1 (b), where we notice that such phenomenon is also present on other SDE methods. We report in Appendix F, the results of our SEEDS methods in the low NFE regime and connect their behavior with their proven convergence rate.

Combining SEEDS with other methods.While being an off-the-shelf method, SEEDS can be combined with the Churn-like method used in EDM incurring into SDE solvers with an additional source of stochasticity. As done in [16], we evaluate the effect of this second kind of stochasticity, measured by a parameter denoted \(S_{\mathrm{churn}}\). Figure 1 (c) shows that SEEDS and EDM show similar behavior, although SEEDS-3 is twice faster, more sensitive to \(S_{\mathrm{churn}}\), and quickly achieves comparable performance to EDM. This indicates that SEEDS could possibly outperform EDM after a proper parameter optimization that will be left for future works. Nevertheless, we highlight the fact that obtaining such optimal parameters is costly and might scale poorly.

Stiffness reduction with SEEDS.In Fig. 2, we illustrate the impact of different choices of discretization steps, noise schedule and dynamic scaling on SEEDS and stochastic EDM. We see that choosing the EDM discretization over the linear one has the effect of flattening the pixel trajectories at latest stages of the simulation procedure. Also, choosing the parameters (11) over those in (7) has the effect of greatly changing the distribution variances as the trajectories evolve. Notice that all the SEEDS trajectories seem perceptually more stable than those from EDM. It would be interesting to relate this to the _stiffness_ of the semi-linear DE describing these trajectories, and to the magnitude of the parameters involved in the noise injection for EDM solver amplifying this phenomenon.

Ablation studies.As said earlier, our principled use of the Chasles rule to enforce independence only on non-overlapping paths for SEEDS-2/3 ensures that the set of resulting iterations of our solvers satisfy the Markov property, is new and is the central key of success of SEEDS. To highlight this,

Figure 2: Trajectories of 10 pixels (R channel) sampled from SEEDS (1st line) and Stochastic EDM (2nd line) on the optimized pre-trained model [16] on ImageNet64. Schedule=scaling=vp corresponds to the VP coefficients in (7) and schedule=linear, scaling=none to the EDM coefficients (11). We use the time discretizations disc=vp (linear) and disc=edm given in [16, Tab.1].

we conduct an ablation study on how 4 different combinations of the noise components \(\bm{A}\) and \(\bm{B}\) in Algorithms 3 and 4 have an impact on the sampling quality of SEEDS.

For simplicity, we explain this for SEEDS-2 (Alg. 3). Set \((z^{1},z^{2},z^{3})\) three independent standard Gaussian random variables. Denote \(\bm{A}=\bar{\sigma}_{s_{1}}\sqrt{e^{h}-1}z^{1}\) for the noise contribution in the \(\mathbf{u}\) term. We have the following choices for the noise contribution \(\bm{B}\) in \(\bar{\mathbf{x}}_{t}\):

* SEEDS-2-Correct: our noise combination \(\bm{B}=\bar{\sigma}_{t}\left(\sqrt{e^{2h}-e^{h}}z^{1}+\sqrt{e^{h}-1}z^{2}\right)\)
* SEEDS-2-Naive-1: one noise per stage \(\bm{B}=\bar{\sigma}_{t}\left(\sqrt{e^{2h}-e^{h}}+\sqrt{e^{h}-1}\right)z^{2}\)
* SEEDS-2-Naive-2: one noise per step \(\bm{B}=\bar{\sigma}_{t}\left(\sqrt{e^{2h}-e^{h}}+\sqrt{e^{h}-1}\right)z^{1}\)
* SEEDS-2-Naive-3: one noise per integral evaluation \(\bm{B}=\bar{\sigma}_{t}\left(\sqrt{e^{2h}-e^{h}}z^{3}+\sqrt{e^{h}-1}z^{2}\right)\)
* SEEDS-2-Naive-4: noises in inverse position \(\bm{B}=\bar{\sigma}_{t}\left(\sqrt{e^{2h}-e^{h}}z^{2}+\sqrt{e^{h}-1}z^{1}\right)\).

Figure 3 experimentally shows that all naive combinations of noises for SEEDS-2 and SEEDS-3 lead to FID/NFE curves with different behavior and a sharp drop of performance both in quality and speed in all of them, a phenomenon having no deterministic parallel.

## 6 Conclusions

Our focus is on addressing the challenge of training-free sampling from DPMs without compromising sampling quality. To achieve this, we introduce SEEDS, an off-the-shelf solution for solving diffusion SDEs. SEEDS capitalize on the semi-linearity of diffusion SDEs by approximating a simplified formulation of their exact solutions. Inspired by numerical methods for stochastic exponential integrators, we propose three SEEDS schemes with proven convergence order. They transform the integrals involved in the exact solution into exponentially weighted integrals, and estimate the deterministic one while analytically computing the variance of the stochastic integral. We extend our approach to handle other isotropic DPMs, and evaluate its performance on various benchmark tests. Our experiments demonstrate that SEEDS can generate images of optimal quality, outperforming existing SDE solvers while being \(3\sim 5\times\) faster. **Limitations and broader impact.** While SEEDS prioritize optimal quality sampling, they may require substantial computational resources and energy consumption, making them less suitable for scenarios where speed is the primary concern. In such cases, alternative ODE methods may be more appropriate. Additionally, as with other generative models, DPMs can be employed to create misleading or harmful content, and our proposed solver could inadvertently amplify the negative impact of generative AI for malicious purposes.

Figure 3: Quality (measured by FID at increasing NFEs) comparison of SEEDS-2/3 with the enumerated ablation versions of it on CIFAR-10 in the (baseline) VP conditional framework.

**Acknowledgments.** This work has been supported by the French government under the "France 2030" program, as part of the SystemX Technological Research Institute.

## References

* [1]Adamu, I. Numerical Approximation of SDEs and Stochastic Swift-Hohenberg Equation (PhD Thesis, Heriot-Watt University), 2011.
* [2]Bao, F., Li, C., Zhu, J., and Zhang, B. Analytic-DPM: An Analytic Estimate Of The Optimal Reverse Variance In Diffusion Probabilistic Models. _ICLR_ (2022).
* [3]Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. WaveGrad: Estimating gradients for waveform generation. _arXiv preprint arXiv:2009.00713_ (2020).
* [4]Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., Dehak, N., and Chan, W. WaveGrad 2: Iterative refinement for text-to-speech synthesis. _arXiv preprint arXiv:2106.09660_ (2021).
* [5]Chen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. Sampling is as Easy as Learning the Score: Theory for Diffusion Models with Minimal Data Assumptions. _arXiv:2209.11215_ (2022).
* [6]Clark, J. M. C., and Cameron, R. J. The Maximum Rate of Convergence of Discrete Approximations for Stochastic Differential Equations. In _Stochastic Differential Systems Filtering and Control_ (Berlin, Heidelberg, 1980), Springer Berlin Heidelberg, pp. 162-171.
* [7]Debrabant, K., Kverno, A., and Mattsson, N. C. Runge-Kutta Lawson schemes for Stochastic Differential Equations. _BIT Numerical Mathematics 61_ (2021), 381-409.
* [8]Dhariwal, P., and Nichol, A. Diffusion Models Beat GANs On Image Synthesis. _Advances in Neural Information Processing Systems 34_ (2021), 8780-8794.
* [9]Dockhorn, T., Vahdat, A., and Kreis, K. GENIE: Higher-order Denoising Diffusion Solvers. _arXiv preprint arXiv:2210.05475_ (2022).
* [10]Dockhorn, T., Vahdat, A., and Kreis, K. Score-Based Generative Modeling with Critically-Damped Langevin Diffusion. In _International Conference on Learning Representations (ICLR)_ (2022).
* [11]Gonzalez, M., Hajri, H., Cantat, L., and Petreczky, M. Noisy Learning of Neural ODEs Acts as a Robustness Locus Widening. In _International Conference on Machine Learning (PODS Workshop)_ (2022).
* [12]Ho, J., Jain, A., and Abbeel, P. Denoising Diffusion Probabilistic Models. _Advances in Neural Information Processing Systems 33_ (2020), 6840-6851.
* [13]Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video Diffusion Models. _arXiv:2204.03458_ (2022).
* [14]Hochbruck, M., and Ostermann, A. Exponential Integrators. _Acta Numerica 19_ (2010), 209-286.
* [15]Jolicoeur-Martineau, A., Li, K., Piche-Taillefer, R., Kachman, T., and Mitliagkas, I. Gotta Go Fast When Generating Data With Score-Based Models. _arXiv preprint arXiv:2105.14080_ (2021).
* [16]Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the Design Space of Diffusion-Based Generative Models. In _Proc. NeurIPS_ (2022).
* [17]Kloeden, P. E., and Platen, E. _Numerical Solution Of Stochastic Differential Equations. Stochastic Modelling And Applied Probability_. Applications of Mathematics. Springer, 1992.
* [18]Komori, Y. Exponential Runge-Kutta Methods for Stiff Stochastic Differential Equations. _RIMS_ (2016), 128-140.
* [19]Komori, Y., Cohen, D., and Burrage, K. Weak Second Order Explicit Exponential Runge-Kutta Methods For Stochastic Differential Equations. _SIAM Journal on Scientific Computing 39_, 6 (2017), A2857-A2878.
* [20]Liu, L., Ren, Y., Lin, Z., and Zhao, Z. Pseudo Numerical Methods for Diffusion Models on Manifolds. _arXiv preprint arXiv:2202.09778_ (2022).
* [21]Lu, C., Zheng, K., Bao, F., Chen, J., Li, C., and Zhu, J. Maximum Likelihood Training for Score-Based Diffusion ODEs by High Order Denoising Score Matching. In _International Conference on Machine Learning_ (2022), PMLR, pp. 14429-14460.
* [22]Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. _Advances in Neural Information Processing Systems_ (2022).
* [23]Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. DPM-Solver++: Fast Solver For Guided Sampling Of Diffusion Probabilistic Models. _arXiv preprint arXiv:2211.01095_ (2022).

* [24]Meng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. SDEdit: Image Synthesis And Editing With Stochastic Differential Equations. _arXiv preprint arXiv:2108.01073_ (2021).
* [25]Milstein, G., and Tretyakov, M. _Stochastic Numerics for Mathematical Physics_. Scientific Computation series, Springer, Berlin, 2021.
* [26]Nichol, A. Q., and Dhariwal, P. Improved Denoising Diffusion Probabilistic Models. In _International Conference on Machine Learning_ (2021), PMLR, pp. 8162-8171.
* [27]Nie, W., Guo, B., Huang, Y., Xiao, C., Vahdat, A., and Anandkumar, A. Diffusion Models for Adversarial Purification. In _International Conference on Machine Learning (ICML)_ (2022).
* [28]Peng, S., Xu, W., Cornelius, C., Hull, M., Li, K., Duggal, R., Phute, M., Martin, J., and Chau, D. H. Robust Principles: Architectural Design Principles for Adversarially Robust CNNs. _arXiv preprint arXiv:2308.16258_ (2023).
* [29]Popov, V., Vovk, I., Gogoryan, V., Sadekova, T., and Kudinov, M. Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech. In _Proceedings of the 38th International Conference on Machine Learning_ (2021), M. Meila and T. Zhang, Eds., vol. 139, PMLR.
* [30]Rossler, A. Runge-Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations. _SIAM Journal on Numerical Analysis 48_, 3 (2010), 922-952.
* [31]Salimans, T., and Ho, J. Progressive Distillation For Fast Sampling Of Diffusion Models. _arXiv preprint arXiv:2202.00512_ (2022).
* [32]Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep Unsupervised Learning Using Nonequilibrium Thermodynamics. In _International Conference on Machine Learning_ (2015), PMLR, pp. 2256-2265.
* [33]Song, J., Meng, C., and Ermon, S. Denoising Diffusion Implicit Models. _arXiv:2010.02502_ (2020).
* [34]Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-Based Generative Modeling through Stochastic Differential Equations. In _International Conference on Learning Representations_ (2021).
* [35]Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the Inception Architecture for Computer Vision. In _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2016), pp. 2818-2826.
* [36]Vahdat, A., Kreis, K., and Kautz, J. Score-based Generative Modeling in Latent Space. In _Neural Information Processing Systems (NeurIPS)_ (2021).
* [37]Wang, Z., Pang, T., Du, C., Lin, M., Liu, W., and Yan, S. Better Diffusion Models Further Improve Adversarial Training. In _International Conference on Machine Learning (ICML)_ (2023).
* [38]Xu, Y., Deng, M., Cheng, X., Tian, Y., Liu, Z., and Jaakkola, T. Restart Sampling for Improving Generative Processes. _arXiv:2306.14878_ (2023).
* [39]Yan, H., Du, J., Tan, V. Y. F., and Feng, J. On robustness of neural ordinary differential equations. In _International Conference on Learning Representations_ (2020).
* [40]Zhang, Q., and Chen, Y. Fast Sampling of Diffusion Models with Exponential Integrator. _arXiv preprint arXiv:2204.13902_ (2022).
* [41]Zhang, Q., Tao, M., and Chen, Y. gDDIM: Generalized Denoising Diffusion Implicit Models. _arXiv preprint arXiv:2206.05564_ (2022).
* [42]Zheng, H., Nie, W., Vahdat, A., Azizzadenesheli, K., and Anandkumar, A. Fast Sampling of Diffusion Models via Operator Learning. _arXiv preprint arXiv:2211.13449_ (2022).

###### Contents

* 1 Introduction
* 2 Background on Diffusion Probabilistic Models
* 3 Accelerating Optimal Quality Solvers for Diffusion SDEs
* 4 Higher Stage SEEDS for DPMs
* 5 Experiments
* 6 Conclusions
* A Discussion
* B Detailed Derivation of the SEEDS Design Space
* B.1 The Isotropic General SDE framework
* B.2 Re-framing and Generalizing Previous Exponential Solvers
* B.2.1 The VP case
* B.2.2 Proof of Proposition 3.1
* B.2.3 Proof of Proposition 4.2
* B.2.4 Generalization to the remaining data prediction and deterministic modes
* B.2.5 Proof of Proposition 4.5
* B.2.6 The VE, DDIM and iDDPM cases
* B.2.7 The EDM case
* B.2.8 Proof of Proposition 3.2
* B.2.9 The SEEDS algorithms in the EDM Noise Prediction case
* C Convergence Proofs
* C.1 Preliminaries
* C.2 Convergence of SEEDS-1
* C.2.1 Strong Ito-Taylor approximation
* C.2.2 Continuous approximation of SEEDS-1
* C.2.3 Proof of Theorem 4.1
* C.2.4 Discrete-time approximation
* C.2.5 Useful Lemmas
* C.3 Proof of Proposition 4.3
* C.4 Proof of Corollary 4.4
* C.4.1 Convergence of SEEDS-2
* C.4.2 Convergence of SEEDS-3
* D Implementation Details

* [21] S. A. K. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of a neural network using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-122.
* [22] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of a neural network using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [23] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [24] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [25] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [26] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [27] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [28] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [29] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [30] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [31] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [32] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [33] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [34] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [35] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [36] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [37] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [38] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [39] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning the shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [40] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [41] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [42] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [43] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [44] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [45] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [46] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [47] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [48] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [49] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [50] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [51] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [52] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [53] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [54] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [55] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [56] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [57] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [58] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [59] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [60] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [61] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [62] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [63] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-126.
* [64] S. A. Krizhevsky, A. Krizhevsky, and A. Krizhevsky, "Learning shape of neural networks using deep neural networks," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016, pp. 115-resolution images in the realm of unguided image generation (unconditional and conditional). For unconditional generation using Latent Diffusion Model, SEEDS are able to generate good quality images already at 100 NFEs. Now, for guided image generation, higher-stage SEEDS will, as expected, see their performance sharply drop as the guidance scale grows for the same reasons DPM-solver do (see [23] for details). Yet, SEEDS-1 still maintains high quality sampling in this scenario. In Appendix F.8, Fig. 15 exhibits a \(512^{2}\) image generated with SEEDS-1 at 90 NFEs on Stable Diffusion with default guidance scale.

Do SEEDS exhibit other distinguishing features beyond generation optimal quality?As SDE solvers, SEEDS have a distinctive (although indirect) capability in the realm of adversarial robustness compared to DPM-Solver:

1. For more than 2 years, the leader-board of RobustBench has been dominated by Diffusion-based data augmentation techniques on top of Adversarial Training. The current SOTA [28, 37] uses EDM-preconditioned DPMs and need to generate as many as 50M images to achieve SOTA robustness results. For ImageNet-64, [37] use the EDM pretrained model and optimization-based stochastic EDM sampler for data augmentation, leading to a 5% robust accuracy improvement compared to doing so for the baseline ADM pretrained model. Since SEEDS reach same FID quality as EDM, but twice faster, we believe it will have a positive impact in this domain, making diffusion- based data augmentation schemes more affordable with limited computational capacity.
2. The work [27] uses DPM-based adversarial purification as a test-time adversarial defense. The idea is to use off-the-shelf DPMs to annihilate the adversarial content in inputs in test-time before feeding it to a pretrained classifier. In [27, Table 6], one can see that SDE solvers show substantial robustness capabilities compared to ODE solvers.

We'd like to stress out that, although indirect, such capabilities are on the side of the sampling methods rather than on the side of intrinsic robustness properties of score-based learning method. More broadly, robustness properties of ML models determined as neural differential equations (DPMs being in this scope) has been studied in [39, 11].

How do SEEDS compare to Stochastic Runge-Kutta methods?Contrary to the ODE case, there are many stochastic Runge-Kutta approaches, usually tailored for SDEs of a specific form. Nonetheless, a common way of distinguishing solvers with same strong order is to assign them couples \((p_{d},p_{s})\), where \(p_{s}\) is the (stochastic) strong convergence order and \(p_{d}\) is the resulting (deterministic) order determined if setting \(g=0\) in the considered SDE i.e. when they are deterministic. For instance, [30, Tab. 6.2] determines solvers with orders (1,1.0) and (2,1.0) respectively and [30, Tab. 6.3] determines solvers SRA1 and SRA3 with orders (2,1.5) and (3,1.5) respectively. Many of these solvers' speed was already tested in [15, Table 3] on CIFAR-10 (VP). Experimentally, SEEDS-1 shows to be 6.83x faster than the baseline Euler-Maruyama scheme.

To our knowledge, the only available strong order SERKs method for SDEs with in-homogeneous diffusion coefficients are the exponential Euler-Maruyama (EEM) method [18] and the stochastic RK Lawson (SRKL) schemes [7]. In short, the SRKL schemes only compute analytically the linear coefficient and use the Integrating Factor (IF) method to approximate the integrals in the representation of the exact solution given by the variation-of-parameters formula. This way, by a special change of variables (see [7, Alg. 1]), one can create exponential integrator versions of many SDE methods. We implemented our own version of the SRKL schemes, which we denote SRKL-1/2/3, to take into account that \(\sigma,\alpha\) are not constant and used the Integrating Factor method to approximate the integrals in the representation of the exact solution given by the variation-of-parameters formula and under the VP \(\lambda\)-change-of-variables. Interestingly, the SRKL schemes stabilize at increasing NFEs but at much higher FID values than their SETD counterparts.

In Table 2 below, we draw a comparison of SEEDS with current Stochastic RKL methods (on the \(\lambda\) temporal parameter space) on CIFAR-10 for the discretely trained DPM in the VP unconditional regime.

Is the proven convergence order of each of the SEEDS methods optimal?The proposed convergence orders for each SEEDS-1/2/3 is optimal: this is a consequence of the general result from [6] about maximum convergence rates for SDE schemes with uncorrelated Gaussian increments.

The underlying idea, fully detailed in Appendix E, is that any solver with strong order \(\geq 1.5\) has to account for double stochastic integrals in the non-truncated Ito-Taylor expansion, ultimately forcing any SRK-like solver to use correlated random variables (see [30, Tab. 6.3] and [17] more generally). SEEDS avoid this additional complexity but an interesting future avenue would be to extend SEEDS to the higher strong order case (and not in the IF approach but the SETD approach) as long as it doesn't incur into an explosion of needed NFEs per step to craft such solvers. Another interesting path would be to craft weak second order SERK methods for DPMs (the work [19] addressed this only for homogeneous semi-linear SDEs with constant linear coefficients).

## Appendix B Detailed Derivation of the SEEDS Design Space

In Sections 2 and 3 we proposed a simplified presentation of the design space of diffusion models and of the ingredients that constitute our proposed SEEDS methodology. In this section, we further develop our presentation in a technical manner, making explicit the formalization of our design choices.

### The Isotropic General SDE framework

In Section 2, we presented a parametric family of differential equations (2) driving the generative process for DPMs, based on time-reversing the forward noising diffusion process (1). While doing so, we presented two parameters - the noise schedule \(\sigma_{t}\) and the scaling \(\alpha_{t}\) - for which the effects on DPMs have been widely studied in [16].

As the shape of the trajectories of (1) and (2) (for \(\ell=0,1\)) are defined by \(\alpha_{t}\) and \(\sigma_{t}\), we start by writing down, for the scaling \(\mathbf{x}_{t}=\alpha_{t}\widehat{\mathbf{x}}_{t}\), the scaled generalization of the proposed SDEs in [16, Eq. 103] which unifies in a single framework the forward and reverse trajectories:

\[\mathrm{d}\mathbf{x}_{t}^{\pm}=\left[\frac{\dot{\alpha}_{t}}{\alpha_{t}} \mathbf{x}_{t}^{\pm}-\alpha_{t}^{2}\dot{\sigma}_{t}\sigma_{t}\nabla_{\mathbf{ x}_{t}^{\pm}}\log p(\widehat{\mathbf{x}}_{t}^{\pm};\sigma_{t})\pm\alpha_{t}^{2} \dot{\sigma}_{t}\sigma_{t}\nabla_{\mathbf{x}_{t}^{\pm}}\log p(\widehat{ \mathbf{x}}_{t}^{\pm};\sigma_{t})\right]\mathrm{d}t+\alpha_{t}\sigma_{t}\sqrt {2\frac{\dot{\sigma}_{t}}{\sigma_{t}}}\mathrm{d}\boldsymbol{\omega}_{t}^{\pm}.\] (17)

Following [16], the previous VP, VE, iDDPM, DDIM and EDM frameworks all are unified as different choices of \(\alpha_{t}\), \(\sigma_{t}\), among other choices presented in [16, Tab. 1] and we will use this as a basis for all the proofs contained in this Appendix. In particular, forward time means taking \(\mathbf{x}_{t}^{+}\) for which the score vanishes in this context. Now set \(\mathbf{x}_{t}=\mathbf{x}_{t}^{-}\).

The formulation in (2) involves a family of backwards differential equations controlled by a parameter \(\ell\in[0,1]\) which all yield reverse-time processes for (1), a fact that can be obtained by studying the Fokker-Planck equation for marginals \(p(\widehat{\mathbf{x}}_{t}^{+};\sigma_{t})\) of (17).

When \(\ell=1\), (2) the obtained SDE is known as the reverse SDE (RSDE) and, when \(\ell=0\), we obtain an ODE that is known as the Probability Flow ODE (PFO):

\[\mathrm{d}\mathbf{x}_{t} = \left[\frac{\dot{\alpha}_{t}}{\alpha_{t}}\mathbf{x}_{t}-\alpha_{t }^{2}\dot{\sigma}_{t}\sigma_{t}\nabla_{\mathbf{x}_{t}}\log p(\widehat{ \mathbf{x}}_{t};\sigma_{t})\right]\mathrm{d}t.\] (18)

Now, finding a minimum for the loss function in [16, Eq. 51] is formulated as a convex optimization problem. As such, for the ideal model \(D(\mathbf{x}_{t};\sigma_{t})=\arg\min_{D}\mathcal{L}(D;\mathbf{x}_{t},\sigma _{t})\), the score function with

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline Method \(\backslash\) NFE & 10 & 20 & 50 & 100 & Best known \\ \hline SRKL-1(\(\lambda\)) & 332.52 & 282.96 & 33.42 & 8.62 & - \\ SEEDS-1 & 303.48 & 153.21 & 22.70 & 7.97 & (500 NFE) 3.13 \\ SRKL-2(\(\lambda\)) & 475.20 & 469.64 & 134.82 & 7.74 & - \\ SEEDS-2 & 476.90 & 226.70 & 7.17 & 3.23 & (90 NFE) 3.21 \\ SRKL-3(\(\lambda\)) & 462.24 & 376.15 & 8.36 & 7.46 & - \\ SEEDS-3 & 483.00 & 428.60 & 43.30 & 3.41 & (201 NFE) 3.08 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of SEEDS with adapted Stochastic RRL methods on CIFAR-10 in the VP unconditional discrete framework.

scaled input is expressed as

\[\nabla_{\mathbf{x}_{t}}\log p(\widehat{\mathbf{x}}_{t};\sigma_{t}) = \frac{D(\widehat{\mathbf{x}}_{t};\sigma_{t})-\widehat{\mathbf{x}}_ {t}}{\alpha_{t}\sigma_{t}^{2}}.\]

This ideal model is usually subtracted by a _raw_ network \(F\) in the form of a time-dependent preconditioning:

\[D(\mathbf{x}_{t};\sigma_{t}):=c_{1}(t)\mathbf{x}_{t}+c_{2}(t)F(c_{3}(t) \mathbf{x}_{t};c_{4}(t)),\qquad c_{i}(t)\in\mathbb{R}^{d},\quad i=1,\ldots,4.\]

As such, we can express the score function as two parameterizations involving \(D\) or \(F\) as follows:

\[\nabla_{\mathbf{x}_{t}}\log p(\widehat{\mathbf{x}}_{t};\sigma_{t}) = \frac{D(\widehat{\mathbf{x}}_{t};\sigma_{t})-\widehat{\mathbf{x}} _{t}}{\alpha_{t}\sigma_{t}^{2}}=\frac{(c_{1}(t)-1)\widehat{\mathbf{x}}_{t}+c_ {2}(t)F(c_{3}(t)\widehat{\mathbf{x}}_{t};c_{4}(t))}{\alpha_{t}\sigma_{t}^{2}}.\] (19)

Let us now denote \(D^{1}_{\theta,t}:=D_{\theta}(\widehat{\mathbf{x}}_{t};\sigma_{t})\) for a pre-trained network approximating the ideal denoiser and let \(D^{2}_{\theta,t}:=F_{\theta}(c_{3}(t)\widehat{\mathbf{x}}_{t};c_{4}(t))\) be the corresponding raw pre-trained network. Substituting the score function in the RSDE and the PFO with each of these models yields four _different_ differential equations with a neural network as one of their components. These are given, for \(i=1,2\), by

\[\mathrm{d}\mathbf{x}_{t} = [A^{i}(t)\mathbf{x}_{t}+B^{i}(t)D^{i}_{\theta,t}]\mathrm{d}t+g(t )\mathrm{d}\tilde{\omega}_{t},\] (20) \[\mathrm{d}\mathbf{x}_{t} = [A^{i+2}(t)\mathbf{x}_{t}+B^{i+2}(t)D^{i}_{\theta,t}]\mathrm{d}t.\] (21)

When \(D^{1}_{\theta,t}\) (resp. \(D^{2}_{\theta,t}\)) is employed to replace the score function in (17) using (19), the resulting SDE (20) will be called _data (resp. noise) prediction neural SDE_. Proceeding analogously for the PFO (18) yield two ODEs (21) which will be called _data (resp. noise) prediction neural PFO_. The general form of the \(A^{i}\) and \(B^{i}\) coefficients determining each of these DEs is as follows:

\[A^{1}(t) =\frac{\dot{\alpha}_{t}}{\alpha_{t}}+2\frac{\dot{\sigma}_{t}}{ \sigma_{t}} B^{1}(t) =-2\alpha_{t}\frac{\dot{\sigma}_{t}}{\sigma_{t}}\] (DP NRSDE) \[A^{2}(t) =\frac{\dot{\alpha}_{t}}{\alpha_{t}}+2\frac{\dot{\sigma}_{t}}{ \sigma_{t}}(1-c_{1}(t)) B^{2}(t) =-2\alpha_{t}\frac{\dot{\sigma}_{t}}{\sigma_{t}}c_{2}(t)\] (NP NRSDE) \[A^{3}(t) =\frac{\dot{\alpha}_{t}}{\alpha_{t}}+\frac{\dot{\sigma}_{t}}{ \sigma_{t}} B^{3}(t) =-\alpha_{t}\frac{\dot{\sigma}_{t}}{\sigma_{t}}\] (DP NPFO) \[A^{4}(t) =\frac{\dot{\alpha}_{t}}{\alpha_{t}}+\frac{\dot{\sigma}_{t}}{ \sigma_{t}}(1-c_{1}(t)) B^{4}(t) =-\alpha_{t}\frac{\dot{\sigma}_{t}}{\sigma_{t}}c_{2}(t)\] (NP NPFO)

_Remark B.1_.: At first glance, it would seem misleading to differentiate four DEs as these essentially correspond to different choices of \(\alpha_{t}\), \(\sigma_{t}\), \(c_{1}(t),\ldots,c_{4}(t)\). But the reason why we do so is that, after applying the _variation of constants_ formula, each of these DEs will yield a different representation of their exact solutions (see (22) and (23) below). As we will see below, constructing exponential integrators heavily depends on such representation and will show to lead to four different modes of SEEDS solvers, each one showing different behavior and performance for sampling from pre-trained DPMs. As such, we will articulate this difference already at the DE formulation.

For \(t<s\), the variation of constants formulae for NSDEs allows to represent the exact solutions of (20) as

\[\mathbf{x}_{t}=\Phi^{i}(t,s)\mathbf{x}_{s}+\int_{s}^{t}\Phi^{i}(t,\tau)B^{i}( \tau)D^{i}_{\theta,\tau}\mathrm{d}\tau+\int_{s}^{t}\Phi^{i}(t,\tau)g(\tau) \mathrm{d}\tilde{\omega}_{t},\qquad i=1,2\] (22)

and those for NPFOs (21) as

\[\mathbf{x}_{t}=\Phi^{i}(t,s)\mathbf{x}_{s}+\int_{s}^{t}\Phi^{i}(t,\tau)B^{i}( \tau)D^{i-2}_{\theta,\tau}\mathrm{d}\tau,\qquad i=3,4\] (23)

where

\[\Phi_{A^{i}}(t,s)=\exp\left(\int_{s}^{t}A^{i}(\tau)\mathrm{d}\tau\right)\] (24)

is called the transition matrix associated with \(A^{i}(t)\) and is defined as the solution to

\[\frac{\partial}{\partial t}\Phi_{A^{i}}(t,s)=A^{i}(t)\Phi_{A^{i}}(t,s),\qquad \Phi_{A^{i}}(s,s)=\bm{I}_{d}.\]When \(A^{i}(t)\) is constant and \(B^{i}(t)=1\), there is a well-established literature on exponential ODE and SDE solvers with explicit _stiff order conditions_ and prescribed by different forms of Butcher tableaux. When \(A^{i}(t)\) is not constant, for the expression in (24) to make sense in the usual sense (in terms of exponential series expansion) instead of having to make use of time-ordered exponentials/ Magnus expansions, the \(f(t):=A^{i}(t)\) coefficients must satisfy \([f^{(k)}(t),f^{(l)}(s)]=0\). This condition is trivially satisfied here as the \(A^{i}(t)\) considered here are \(d\)-dimensional diagonal matrices.

Notice that, if \(A^{i}\neq A^{j}\) for some \(i\neq j\), their associated transition matrices will not be equal. In particular, if \(A^{1}\neq A^{2}\), then the variances of the stochastic integrals in (22) are different for \(i=1\) and \(i=2\). This is the first step in explaining the statement in Rem. B.1, and we refer the reader to the proof of Proposition 4.5 where we put into evidence its validity.

### Re-framing and Generalizing Previous Exponential Solvers

#### b.2.1 The VP case

Let \(\tilde{\alpha}_{t}:=\int_{0}^{t}(\beta_{d}\tau-\beta_{m})\mathrm{d}\tau= \frac{1}{2}\beta_{d}t^{2}+\beta_{m}t\), where \(\beta_{d}>\beta_{m}>0\). Set

\[f(t):=\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{d}t},\quad g(t)=\alpha_{t}\sqrt {\frac{\mathrm{d}[\sigma_{t}^{2}]}{\mathrm{d}t}},\qquad\sigma_{t}=\sqrt{e^{ \tilde{\alpha}_{t}}-1},\qquad\alpha_{t}=e^{-\frac{1}{2}\tilde{\alpha}_{t}}= \frac{1}{\sqrt{\sigma_{t}^{2}+1}}.\]

Recall that in the VP case, and the _noise prediction mode_, [22] construct exponential solvers on the base of the following ODE

\[\mathrm{d}\mathbf{x}_{t} = \left[f(t)\mathbf{x}_{t}+\frac{g^{2}(t)}{2\bar{\sigma}_{t}}\epsilon _{\theta}(\mathbf{x}_{t};t)\right]\mathrm{d}t,\quad t\in[T,0],\] (25)

where \(\bar{\sigma}_{t}:=\alpha_{t}\sigma_{t}\). The ODE (25) identifies with that in [16] for the VP case for which the authors identify the preconditioning

\[c_{1}(t)=1,\quad c_{2}(t)=-\sigma_{t},\quad c_{3}(t)=\frac{1}{\sqrt{\sigma_{t }^{2}+1}},\quad c_{4}(t)=(M-1)\sigma^{-1}(\sigma_{t})=(M-1)t.\]

As such, we obtain the following coefficients for the NP NPFO:

\[A^{4}(t)=\frac{\dot{\alpha}_{t}}{\alpha_{t}},\quad B^{4}(t)=\alpha_{t}\dot{ \sigma}_{t},\quad\Phi^{4}(t,s)=\frac{\alpha_{t}}{\alpha_{s}}\]

and

\[\nabla_{\mathbf{x}_{t}}\log p(\widehat{\mathbf{x}}_{t};\sigma_{t} )=\frac{D_{\theta}(\widehat{\mathbf{x}}_{t};\sigma_{t})-\widehat{\mathbf{x}}_{ t}}{\alpha_{t}\sigma_{t}^{2}} = \frac{(c_{1}(t)-1)\widehat{\mathbf{x}}_{t}+c_{2}(t)F(c_{3}(t) \widehat{\mathbf{x}}_{t};c_{4}(t))}{\alpha_{t}\sigma_{t}^{2}}\] \[= \frac{-\sigma_{t}F_{\theta}(\mathbf{x}_{t},(M-1)t)}{\alpha_{t} \sigma_{t}^{2}}.\]

#### b.2.2 Proof of Proposition 3.1

First of all, denote \(F_{\theta}(\mathbf{x}_{t},(M-1)t)=\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{ t},t)\). We have

\[f(t)=\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{d}t},\quad g^{2}(t)=2\bar{\sigma}_ {t}^{2}\left(\frac{\mathrm{d}\log\bar{\sigma}_{t}}{\mathrm{d}t}-\frac{ \mathrm{d}\log\alpha_{t}}{\mathrm{d}t}\right)=-2\bar{\sigma}_{t}^{2}\frac{ \mathrm{d}\lambda_{t}}{\mathrm{d}t},\qquad\frac{\bar{\sigma}_{t}}{\alpha_{t}}=e ^{-\lambda_{t}}.\]

This way, one can directly relate \(\lambda_{t}\) with the _signal-to-noise ratio_\(\mathrm{SNR}(t)=\alpha_{t}^{2}/\bar{\sigma}_{t}^{2}\), also being used in [22]. As such, \(\mathrm{SNR}(t)\) is strictly monotonically decreasing in time. Thus, the analytic solution to (2) yields

\[\mathbf{x}_{t} = e^{\int_{s}^{t}f(\tau)\mathrm{d}\tau}\mathbf{x}_{s}+\int_{s}^{t} \left(e^{\int_{\tau}^{t}f(r)\mathrm{d}r}\frac{g^{2}(\tau)}{\bar{\sigma}_{\tau}} \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{\tau},\tau)\right)\mathrm{d}\tau+ \int_{s}^{t}\left(e^{\int_{\tau}^{t}f(r)\mathrm{d}r}g(\tau)\right)\mathrm{d} \bar{\boldsymbol{\omega}}(\tau)\] \[= \frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}+\alpha_{t}\int_{s}^{t }\frac{g^{2}(\tau)}{\alpha_{\tau}\bar{\sigma}_{\tau}}\boldsymbol{\epsilon}_{ \theta}(\mathbf{x}_{\tau},\tau)\mathrm{d}\tau+\alpha_{t}\int_{s}^{t}\frac{g( \tau)}{\alpha_{\tau}}\mathrm{d}\bar{\boldsymbol{\omega}}(\tau)\] \[= \frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-\alpha_{t}\int_{s}^{t }\frac{2\sigma_{\tau}^{2}}{\alpha_{\tau}\bar{\sigma}_{\tau}}\frac{\mathrm{d} \lambda_{\tau}}{\mathrm{d}\tau}\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{\tau },\tau)\mathrm{d}\tau+\alpha_{t}\int_{s}^{t}\frac{g(\tau)}{\alpha_{\tau}} \mathrm{d}\bar{\boldsymbol{\omega}}(\tau)\] \[= \frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-2\alpha_{t}\int_{s}^{ t}e^{-\lambda_{\tau}}\frac{\mathrm{d}\lambda_{\tau}}{\mathrm{d}\tau} \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{\tau},\tau)\mathrm{d}\tau-\sqrt{2} \alpha_{t}\int_{s}^{t}e^{-\lambda_{\tau}}\sqrt{\frac{\mathrm{d}\lambda_{\tau} }{\mathrm{d}\tau}}\mathrm{d}\bar{\boldsymbol{\omega}}(\tau).\]

By using the change of variables to \(\lambda(t)\), our equation now reads

\[\mathbf{x}_{t} = \frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-2\alpha_{t}\int_{ \lambda_{s}}^{\lambda_{t}}e^{-\lambda}\hat{\boldsymbol{\epsilon}}_{\theta}( \widehat{\mathbf{x}}_{\lambda},\lambda)\mathrm{d}\lambda-\sqrt{2}\alpha_{t} \int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\bar{\boldsymbol{ \omega}}(\lambda).\] (26)

Finally, notice that \(\alpha_{t}=\sqrt{\frac{1}{1+e^{-2\lambda_{t}}}}\) and \(\bar{\sigma}_{t}=\sqrt{\frac{1}{1+e^{2\lambda_{t}}}}\) so that (26) is equivalent to

\[\widehat{\mathbf{x}}_{\lambda_{t}} = \frac{\hat{\alpha}_{\lambda_{t}}}{\hat{\alpha}_{\lambda_{s}}} \widehat{\mathbf{x}}_{\lambda_{s}}-2\hat{\alpha}_{\lambda_{t}}\int_{\lambda_{ s}}^{\lambda_{t}}e^{-\lambda}\hat{\boldsymbol{\epsilon}}_{\theta}(\widehat{ \mathbf{x}}_{\lambda},\lambda)\mathrm{d}\lambda-\sqrt{2}\hat{\alpha}_{\lambda_{ t}}\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\bar{\boldsymbol{\omega}}( \lambda).\]

This finishes the proof.

#### b.2.3 Proof of Proposition 4.2

Recall that the functions \(\varphi_{k}\) are the integrals

\[\varphi_{k+1}(t)=\int_{0}^{1}e^{(1-\delta)t}\frac{\delta^{k}}{k!}\mathrm{d}\delta,\]

which satisfy \(\varphi_{k}(0)=\frac{1}{k!}\). The truncated Ito-Taylor expansion of \(\widehat{\boldsymbol{\epsilon}}_{\theta}\) with respect to \(\lambda\) reads

\[\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{\lambda}, \lambda)=\sum_{k=0}^{n}\frac{\left(\lambda-\lambda_{s}\right)^{k}}{k!} \widehat{\boldsymbol{\epsilon}}_{\theta}^{(k)}(\widehat{\mathbf{x}}_{\lambda_ {s}},\lambda_{s})+\mathcal{R}_{n+1},\]

where here \(\widehat{\boldsymbol{\epsilon}}_{\theta}^{(k)}\) denotes the \(L_{t}^{k}\) operators defined in (58) applied to \(\widehat{\boldsymbol{\epsilon}}_{\theta}\). On the one hand, since \(\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}d\lambda=\frac{\bar{\sigma}_{t}} {\alpha_{t}}(e^{h}-1)\), we obtain by iteratively integrating by parts

\[\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\widehat{\boldsymbol{ \epsilon}}_{\theta}(\widehat{\mathbf{x}}_{\lambda},\lambda)\mathrm{d}\lambda = \sum_{k=0}^{n}\widehat{\boldsymbol{\epsilon}}_{\theta}^{(k)}(\widehat {\mathbf{x}}_{\lambda_{s}},\lambda_{s})\int_{\lambda_{s}}^{\lambda_{t}}e^{- \lambda}\frac{(\lambda-\lambda_{s})^{k}}{k!}d\lambda+\mathcal{R}_{n+2}\] \[= \frac{\bar{\sigma}_{t}}{\alpha_{t}}\sum_{k=0}^{n}\widehat{ \boldsymbol{\epsilon}}_{\theta}^{(k)}(\widehat{\mathbf{x}}_{\lambda_{s}}, \lambda_{s})h^{k+1}\varphi_{k+1}(h)+\mathcal{R}_{n+2}.\]

On the other hand, we have \(s>t,h=\lambda_{t}-\lambda_{s}>0\). Note that since the stochastic integrals \(\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\bar{\boldsymbol{\omega}}(\lambda)\) are measurable with respect to \((\bar{\boldsymbol{\omega}}(\lambda)-\bar{\boldsymbol{\omega}}(\lambda_{s}),0 \leq\lambda\leq\lambda_{t}-\lambda_{s})\), they are independent on disjoint time intervals by the independence of increments property of Brownian motion. Thus the random variable \(\epsilon:=\epsilon_{s,t}\) in our algorithms are independent on disjoint time intervals. We then write

\[\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\bar{ \boldsymbol{\omega}}(\lambda) = \mathcal{N}\left(0,\int_{\lambda_{s}}^{\lambda_{t}}e^{-2\lambda} \mathrm{d}\lambda\right)\] \[= \frac{1}{\sqrt{2}}\sqrt{e^{-2\lambda_{s}}-e^{-2\lambda_{t}}} \epsilon,\quad\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\] \[= \frac{1}{\sqrt{2}}\sqrt{\left(\frac{\bar{\sigma}_{s}}{\alpha_{s}}- \frac{\bar{\sigma}_{t}}{\alpha_{t}}\right)\left(\frac{\bar{\sigma}_{s}}{\alpha_{s}} +\frac{\bar{\sigma}_{t}}{\alpha_{t}}\right)}\epsilon,\quad\epsilon\sim\mathcal{ N}(\mathbf{0},\mathbf{I}_{d})\] \[= \frac{1}{\sqrt{2}}\frac{\bar{\sigma}_{t}}{\alpha_{t}}\sqrt{e^{2h}-1} \epsilon,\quad\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d}).\]In conclusion, the truncated Ito-Taylor expansion of the analytic expression

\[\mathbf{x}_{t} = \frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-2\alpha_{t}\int_{\lambda _{s}}^{\lambda_{t}}e^{-\lambda}\widehat{\boldsymbol{\epsilon}}_{\theta}( \widehat{\mathbf{x}}_{\lambda},\lambda)\mathrm{d}\lambda-\sqrt{2}\alpha_{t} \int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\widehat{\boldsymbol{ \sigma}}(\lambda)\] (27)

simplifies to

\[\mathbf{x}_{t} = \frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-2\bar{\sigma}_{t} \sum_{k=0}^{n}h^{k+1}\varphi_{k+1}(h)\widehat{\boldsymbol{\epsilon}}_{\theta} ^{(k)}(\widehat{\mathbf{x}}_{\lambda_{s}},\lambda_{s})-\bar{\sigma}_{t}\sqrt{e ^{2h}-1}\epsilon+\mathcal{R}_{n+2},\quad\epsilon\sim\mathcal{N}(\mathbf{0}, \mathbf{I}_{d}).\]

This finishes the proof.

#### b.2.4 Generalization to the remaining data prediction and deterministic modes

Propositions 3.1 and 4.2 consist on the first steps for crafting SEEDS solvers in the VP case associated to the noise prediction neural RSDE (NP NRSDE). Generalizing the above procedure for crafting SEEDS for the 4 modes associated to (22) and (23) yields the following sets of coefficients

\[A^{1}(t) =\frac{\dot{\alpha}_{t}}{\alpha_{t}}+2\frac{\dot{\sigma}_{t}}{ \sigma_{t}} B^{1}(t) =-2\alpha_{t}\frac{\dot{\sigma}_{t}}{\sigma_{t}}\] (DP NRSDE) \[A^{2}(t) =\frac{\dot{\alpha}_{t}}{\alpha_{t}} B^{2}(t) =2\alpha_{t}\dot{\sigma}_{t}\] (NP NRSDE) \[A^{3}(t) =\frac{\dot{\alpha}_{t}}{\alpha_{t}}+\frac{\dot{\sigma}_{t}}{ \sigma_{t}} B^{3}(t) =-\alpha_{t}\frac{\dot{\sigma}_{t}}{\sigma_{t}}\] (DP NPFO) \[A^{4}(t) =\frac{\dot{\alpha}_{t}}{\alpha_{t}} B^{4}(t) =\alpha_{t}\dot{\sigma}_{t}\] (NP NPFO)

We readily obtain

\[\Phi^{2}(t,s)=\Phi^{4}(t,s)=\frac{\alpha_{t}}{\alpha_{s}},\qquad\Phi^{3}(t,s)= \frac{\bar{\sigma}_{t}}{\bar{\sigma}_{s}},\qquad\Phi^{1}(t,s)=\frac{\sigma_{t }^{2}\alpha_{t}}{\sigma_{s}^{2}\alpha_{s}}.\]

Then, by setting the simpler change of variables \(\lambda_{t}:=-\log(\sigma_{t})\), we obtain

\[\int_{s}^{t}\Phi^{4}(t,\tau)B^{4}(\tau)\mathrm{d}\tau=\alpha_{t}\int_{s}^{t} \frac{1}{\alpha_{\tau}}\alpha_{\tau}\dot{\sigma}_{\tau}\mathrm{d}\tau=\alpha_ {t}\int_{s}^{t}\dot{\sigma}_{\tau}\mathrm{d}\tau=-\alpha_{t}\int_{\lambda_{s}} ^{\lambda_{t}}e^{-\lambda}\mathrm{d}\lambda=-\bar{\sigma}_{t}(e^{h}-1).\]

By recursion we obtain

\[\int_{s}^{t}\Phi^{4}(t,\tau)B^{4}(\tau)F_{\theta,\tau}\mathrm{d}\tau=-\bar{ \sigma}_{t}\sum_{k=0}^{n-1}h^{k+1}\varphi_{k+1}(h)F_{\theta,s}^{(k)}+\mathcal{ O}(h^{n+1}).\]

In the same way, we obtain

\[\int_{s}^{t}\Phi^{3}(t,\tau)B^{3}(\tau)\mathrm{d}\tau=\sigma_{t} \alpha_{t}\int_{s}^{t}\frac{-1}{\alpha_{\tau}\sigma_{\tau}}\alpha_{\tau}\frac{ \dot{\sigma}_{\tau}}{\sigma_{\tau}}\mathrm{d}\tau = \sigma_{t}\alpha_{t}\int_{s}^{t}\frac{-\dot{\sigma}_{\tau}}{\sigma _{\tau}^{2}}\mathrm{d}\tau\] \[= \sigma_{t}\alpha_{t}\int_{\lambda_{s}}^{\lambda_{t}}e^{\lambda} \mathrm{d}\lambda=-\alpha_{t}(e^{-h}-1).\]

Next,

\[\int_{s}^{t}\Phi^{2}(t,\tau)B^{2}(\tau)\mathrm{d}\tau=\alpha_{t}\int_{s}^{t} \frac{2}{\alpha_{\tau}}\alpha_{\tau}\dot{\sigma}_{\tau}\mathrm{d}\tau=-2\bar{ \sigma}_{t}(e^{h}-1),\]

and finally, as already shown in Propositions 3.1 and 4.2:

\[\int_{s}^{t}\Phi^{1}(t,\tau)B^{1}(\tau)\mathrm{d}\tau=\sigma_{t}^ {2}\alpha_{t}\int_{s}^{t}\frac{-2\dot{\sigma}_{\tau}}{\sigma_{\tau}^{3}}\mathrm{ d}\tau = \sigma_{t}^{2}\alpha_{t}\int_{s}^{t}-2\frac{\mathrm{d}\lambda_{\tau }}{\mathrm{d}\tau}e^{2\lambda_{\tau}}\mathrm{d}\tau\] \[= \sigma_{t}^{2}\alpha_{t}\int_{\lambda_{s}}^{\lambda_{t}}e^{2 \lambda}\mathrm{d}\lambda=-\alpha_{t}(e^{-2h}-1).\]Now, since \(g^{2}(t)=2\alpha_{t}^{2}\partial_{t}\sigma_{t}\), the stochastic integrals \(\int_{s}^{t}\Phi^{i}(t,\tau)g(\tau)\mathrm{d}\widetilde{\bm{\omega}}_{\tau}\), for \(i=1,2\), have zero mean and variances:

\[\int_{s}^{t}(\Phi^{1}(t,\tau))^{2}g^{2}(\tau)\mathrm{d}\tau = \sigma_{t}^{4}\alpha_{t}^{2}\int_{s}^{t}\frac{1}{\sigma_{\tau}^{ 4}\alpha_{\tau}^{2}}g^{2}(\tau)\mathrm{d}\tau=\bar{\sigma}_{t}^{2}(1-e^{-2h})\] \[\int_{s}^{t}(\Phi^{2}(t,\tau))^{2}g^{2}(\tau)\mathrm{d}\tau = \alpha_{t}^{2}\int_{s}^{t}2\partial_{\tau}\sigma_{\tau}\mathrm{d} \tau=-\bar{\sigma}_{t}^{2}(e^{2h}-1).\]

We deduce from this the SEEDS-1 schemes in all four modes as given by iterates:

\[\widetilde{\mathbf{x}}_{t} = \frac{\sigma_{t}^{2}\alpha_{t}}{\sigma_{s}^{2}\alpha_{s}} \widetilde{\mathbf{x}}_{s}-\alpha_{t}(e^{-2h}-1)D_{\theta}(\widetilde{\mathbf{ x}}_{s},s)+\bar{\sigma}_{t}\sqrt{1-e^{-2h}}\epsilon\quad\epsilon\sim\mathcal{N}( \mathbf{0},\mathbf{I}_{d})\] (28) \[\widetilde{\mathbf{x}}_{t} = \frac{\alpha_{t}}{\alpha_{s}}\widetilde{\mathbf{x}}_{s}-2\bar{ \sigma}_{t}(e^{h}-1)\bm{\epsilon}_{\theta}(\widetilde{\mathbf{x}}_{s},s)- \bar{\sigma}_{t}\sqrt{e^{2h}-1}\epsilon\quad\epsilon\sim\mathcal{N}(\mathbf{ 0},\mathbf{I}_{d})\] (29) \[\widetilde{\mathbf{x}}_{t} = \frac{\bar{\sigma}_{t}}{\bar{\sigma}_{s}}\widetilde{\mathbf{x}}_{ s}-\alpha_{t}(e^{-h}-1)D_{\theta}(\widetilde{\mathbf{x}}_{s},s)\] (30) \[\widetilde{\mathbf{x}}_{t} = \frac{\alpha_{t}}{\alpha_{s}}\widetilde{\mathbf{x}}_{s}-\bar{ \sigma}_{t}(e^{h}-1)\bm{\epsilon}_{\theta}(\widetilde{\mathbf{x}}_{s},s).\] (31)

Notice that the iterates (30) and (31) are exactly the iterates of the first stage solvers in [22] and [23] with \(F_{\theta}(\mathbf{x}_{t};(M-1)t)=\bm{\epsilon}_{\theta}(\mathbf{x}_{t},t)\). The iterates (29) coincide with the SEEDS-1 method presented in (14) and (28) consist on our SEEDS-1 method in the _data prediction mode_, which we will use in the following section.

#### b.2.5 Proof of Proposition 4.5

We will write in **bold** the statement to be proven.

**If we set \(g=0\) in (15), the resulting SEEDS solvers do not yield DPM-Solver.** Indeed, if we set \(g=0\) in (15), then the method (29) does not contain a noise contribution and we readily see that it cannot be equal to (31). As the latter has been shown to be DPM-Solver-1, the conclusion follows.

**If we parameterize (15) in terms of the data prediction model \(D_{\theta}\), the resulting SEEDS solvers are not equivalent to their noise prediction counterparts defined in Alg. 1 to 4.**

One can check that the SEEDS solver are not the same between the noise and data prediction modes by simply noticing that the noise contributions in (28) and (29) do not equate.

**The gDDIM solver [41] Theorem 1, for \(\ell=1\), is equal to SEEDS-1 in the data prediction mode.**

As shown in (29), our proposed method SEEDS-1 in the data prediction mode for the VP case has iterates of the form

\[\widetilde{\mathbf{x}}_{t} = \frac{\sigma_{t}^{2}\alpha_{t}}{\sigma_{s}^{2}\alpha_{s}} \widetilde{\mathbf{x}}_{s}-\alpha_{t}(e^{-2h}-1)D_{\theta}(\widetilde{\mathbf{ x}}_{s},s)+\bar{\sigma}_{t}\sqrt{1-e^{-2h}}\epsilon,\] (32)

where \(\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\), \(\bar{\sigma}_{t}=\alpha_{t}\sigma_{t}\) and \(h=\log\frac{\sigma_{s}}{\sigma_{t}}\). As our notation and that of [41, Theorem 1] overlap, we will use blue color when referring to their notation.

On the one hand, gDDIM constructs iterates over a representation of the exact solution of the following family of neural differential equations:

\[\mathrm{d}\mathbf{u}_{t} = \left[f(t)\mathbf{u}_{t}+\frac{1+\lambda^{2}}{2}\frac{g^{2}(t)}{ \sqrt{1-\alpha_{t}}}\epsilon_{\theta}(\mathbf{u}_{t};t)\right]\mathrm{d}t+ \lambda g(t)\mathrm{d}\widetilde{\bm{\omega}}_{t},\] (33)

where \(\alpha_{t}\) decreases from \(\alpha_{0}=1\) to \(\alpha_{T}=0\), and with coefficients

\[f(t):=\frac{1}{2}\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{d}t},\quad g(t)= \sqrt{-\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{d}t}}.\]

In particular, they choose an approximation, for \(\tau\in[t-\Delta t,t]\), given by

\[\bm{s}_{\theta}(\mathbf{u},\tau)=\frac{\epsilon_{\theta}(\mathbf{u}_{\tau}, \tau)}{\sqrt{1-\alpha_{t}}}\approx\frac{1-\alpha_{t}}{1-\alpha_{\tau}}\sqrt{ \frac{\alpha_{\tau}}{\alpha_{t}}}\bm{s}_{\theta}(\mathbf{u}(t),t)-\frac{1}{1- \alpha_{\tau}}(\mathbf{u}-\sqrt{\frac{\alpha_{\tau}}{\alpha_{t}}}\mathbf{u}(t)).\]The gDDIM iterates, for \(\lambda=1=\ell\), are then written as follows:

\[\mathbf{u}(t-\Delta t)=\sqrt{\frac{\alpha_{t-\Delta t}}{\alpha_{t}}}\mathbf{u}(t) +\left[-\sqrt{\frac{\alpha_{t-\Delta t}}{\alpha_{t}}}\sqrt{1-\alpha_{t}}+\sqrt{ 1-\alpha_{t-\Delta t}-\sigma_{t}^{2}}\right]\epsilon_{\theta}(\mathbf{u}(t),t )+\sigma_{t}\epsilon,\] (34)

where \(\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\) and

\[\sigma_{t}^{2}=(1-\alpha_{t-\Delta t})\left[1-\left(\frac{1-\alpha_{t-\Delta t }}{1-\alpha_{t}}\right)\left(\frac{\alpha_{t}}{\alpha_{t-\Delta t}}\right)\right].\] (35)

Now set \((s,t)\leftarrow(t,t-\Delta t)\). Then

\[\sigma_{s}^{2} = (1-\alpha_{t})\left[1-\left(\frac{1-\alpha_{t}}{1-\alpha_{s}} \right)\left(\frac{\alpha_{s}}{\alpha_{t}}\right)\right]\!,\] \[\mathbf{u}(t) = \sqrt{\frac{\alpha_{t}}{\alpha_{s}}}\mathbf{u}(s)+\left[\sqrt{ 1-\alpha_{t}-\sigma_{s}^{2}}-\sqrt{\frac{\alpha_{t}}{\alpha_{s}}}\sqrt{1- \alpha_{s}}\right]\epsilon_{\theta}(\mathbf{u}(s),s)+\sigma_{s}\epsilon.\]

Next, we identify \(\alpha_{t}=\sqrt{\alpha_{t}}\), and \(\bar{\sigma}_{t}=\sqrt{1-\alpha_{t}}\). Then the variance of the noise in (35) is

\[\sigma_{s}^{2} = \bar{\sigma}_{t}^{2}\left[1-\left(\frac{\bar{\sigma}_{t}}{\bar{ \sigma}_{s}}\right)^{2}\left(\frac{\alpha_{s}}{\alpha_{t}}\right)^{2}\right]= \bar{\sigma}_{t}^{2}\left[1-\left(\frac{\alpha_{t}\sigma_{t}}{\alpha_{s} \sigma_{s}}\right)^{2}\left(\frac{\alpha_{s}}{\alpha_{t}}\right)^{2}\right]\] \[= \bar{\sigma}_{t}^{2}\left[1-\left(\frac{\sigma_{t}}{\sigma_{s}} \right)^{2}\right]\] \[= \bar{\sigma}_{t}^{2}(1-e^{-2h}).\]

Hence, by denoting \(\widetilde{\mathbf{x}}_{t}=\mathbf{u}(t)\), the gDDIM iterate (34) reads, for \(\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\):

\[\widetilde{\mathbf{x}}_{t} = \frac{\alpha_{t}}{\alpha_{s}}\widetilde{\mathbf{x}}_{s}+\left[ \sqrt{\bar{\sigma}_{t}^{2}-\bar{\sigma}_{t}^{2}(1-e^{-2h})}-\frac{\alpha_{t}}{ \alpha_{s}}\bar{\sigma}_{s}\right]\epsilon_{\theta}(\widetilde{\mathbf{x}}_{s},s)+\sqrt{\bar{\sigma}_{t}^{2}(1-e^{-2h})}\epsilon\] (36) \[= \frac{\alpha_{t}}{\alpha_{s}}\widetilde{\mathbf{x}}_{s}+\left[ \bar{\sigma}_{t}\sqrt{e^{-2h}}-\alpha_{t}\sigma_{s}\right]\epsilon_{\theta}( \widetilde{\mathbf{x}}_{s},s)+\bar{\sigma}_{t}\sqrt{1-e^{-2h}}\epsilon\] \[= \frac{\alpha_{t}}{\alpha_{s}}\widetilde{\mathbf{x}}_{s}+\bar{ \sigma}_{t}\left[\frac{\sigma_{t}}{\sigma_{s}}-\frac{\sigma_{s}}{\sigma_{t}} \right]\epsilon_{\theta}(\widetilde{\mathbf{x}}_{s},s)+\bar{\sigma}_{t}\sqrt{1 -e^{-2h}}\epsilon.\]

On the other hand, the data and noise prediction models in our case are related by the following equation:

\[D_{\theta}(\widetilde{\mathbf{x}}_{s},s)=c_{1}(s)\frac{\widetilde{\mathbf{x}}_ {s}}{\alpha_{s}}+c_{2}(s)\boldsymbol{\epsilon}_{\theta}(\widetilde{\mathbf{x}} _{s},s)=\frac{\widetilde{\mathbf{x}}_{s}}{\alpha_{s}}-\sigma_{s}\boldsymbol{ \epsilon}_{\theta}(\widetilde{\mathbf{x}}_{s},s).\]

As such, and as \(h=\lambda_{t}-\lambda_{s}=\log\frac{\sigma_{s}}{\sigma_{t}}\), one can rewrite (32) in terms of \(\boldsymbol{\epsilon}_{\theta}\) as follows:

\[\widetilde{\mathbf{x}}_{t} = \frac{\sigma_{t}^{2}\alpha_{t}}{\sigma_{s}^{2}\alpha_{s}} \widetilde{\mathbf{x}}_{s}-\alpha_{t}(e^{-2h}-1)D_{\theta}(\widetilde{\mathbf{x }}_{s},s)+\bar{\sigma}_{t}\sqrt{1-e^{-2h}}\epsilon\] \[= \frac{\sigma_{t}^{2}\alpha_{t}}{\sigma_{s}^{2}\alpha_{s}} \widetilde{\mathbf{x}}_{s}-\alpha_{t}\left(\frac{\sigma_{t}^{2}}{\sigma_{s}^{2} }-1\right)\left[\frac{\widetilde{\mathbf{x}}_{s}}{\alpha_{s}}-\sigma_{s} \boldsymbol{\epsilon}_{\theta}(\widetilde{\mathbf{x}}_{s},s)\right]+\bar{ \sigma}_{t}\sqrt{1-e^{-2h}}\epsilon\] \[= \left[\frac{\sigma_{t}^{2}\alpha_{t}}{\sigma_{s}^{2}\alpha_{s}} -\frac{\alpha_{t}}{\alpha_{s}}\left(\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}-1 \right)\right]\widetilde{\mathbf{x}}_{s}+\alpha_{t}\left(\frac{\sigma_{t}^{2}}{ \sigma_{s}^{2}}-1\right)\sigma_{s}\boldsymbol{\epsilon}_{\theta}(\widetilde{ \mathbf{x}}_{s},s)+\bar{\sigma}_{t}\sqrt{1-e^{-2h}}\epsilon\] \[= \frac{\alpha_{t}}{\alpha_{s}}\widetilde{\mathbf{x}}_{s}+\bar{ \sigma}_{t}\left(\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}-1\right)\frac{\sigma_{s}} {\sigma_{t}}\boldsymbol{\epsilon}_{\theta}(\widetilde{\mathbf{x}}_{s},s)+\bar{ \sigma}_{t}\sqrt{1-e^{-2h}}\epsilon\] \[= \frac{\alpha_{t}}{\alpha_{s}}\widetilde{\mathbf{x}}_{s}+\bar{ \sigma}_{t}\left(\frac{\sigma_{t}}{\sigma_{s}}-\frac{\sigma_{s}}{\sigma_{t}} \right)\boldsymbol{\epsilon}_{\theta}(\widetilde{\mathbf{x}}_{s},s)+\bar{\sigma} _{t}\sqrt{1-e^{-2h}}\epsilon,\]

which coincides with the gDDIM iterate in Equation (36).

#### b.2.6 The VE, DDIM and iBDPM cases

Following [16, Eq. 217], here \(\alpha_{t}=1\) and \(c_{1}(t)=1\) and so the only possibilities incurring into semi-linear differential equations are

\[A^{1}(t) =2\frac{\dot{\sigma}_{t}}{\sigma_{t}} B^{1}(t)=-2\frac{\dot{\sigma}_{t}}{\sigma_{t}}\] (DP SDE) \[A^{3}(t) =\frac{\dot{\sigma}_{t}}{\sigma_{t}} B^{3}(t)=-\frac{\dot{\sigma}_{t}}{\sigma_{t}}.\] (DP PFO)

We obtain, again with the choice \(\lambda_{t}=-\log(\sigma_{t})\) and setting \(h=\lambda_{t}-\lambda_{s}\), the following:

\[\Phi^{1}(t,s)=\frac{\sigma_{t}^{2}}{\sigma_{s}^{2}},\ \ \ \ \ \ \Phi^{3}(t,s)=\frac{\sigma_{t}}{\sigma_{s}},\]

\[\int_{s}^{t}\Phi^{1}(t,\tau)B^{1}(\tau)\mathrm{d}\tau = -\sigma_{t}^{2}\int_{\lambda_{s}}^{\lambda_{t}}e^{2\lambda} \mathrm{d}\lambda=\frac{1}{2}(e^{-2h}-1)\] \[\int_{s}^{t}\Phi^{3}(t,\tau)B^{3}(\tau)\mathrm{d}\tau = -\sigma_{t}\int_{\lambda_{s}}^{\lambda_{t}}e^{\lambda} \mathrm{d}\lambda=e^{-h}-1.\]

Next,

\[\int_{s}^{t}(\Phi^{1}(t,\tau))^{2}g^{2}(\tau)\mathrm{d}\tau = \sigma_{t}^{4}\int_{s}^{t}\frac{1}{\sigma_{\tau}^{4}}\sigma_{ \tau}^{2}2\frac{\dot{\sigma}_{\tau}}{\sigma_{\tau}}\mathrm{d}\tau=\sigma_{t}^ {4}\int_{s}^{t}\frac{\dot{2}\sigma_{\tau}}{\sigma_{\tau}^{3}}\mathrm{d}\tau= \sigma_{t}^{2}(e^{-2h}-1).\]

We readily see that these cases are identical to the VP case with \(\alpha_{t}=1\). In particular, the obtained SEEDS-1 iterates are

\[\widetilde{\mathbf{x}}_{t} = \frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}\widetilde{\mathbf{x}}_{s}- (e^{-2h}-1)D_{\theta}(\widetilde{\mathbf{x}}_{s};\sigma_{s})+\sigma_{t}\sqrt{ 1-e^{-2h}}\epsilon.\] \[\widetilde{\mathbf{x}}_{t} = \frac{\sigma_{t}}{\sigma_{s}}\widetilde{\mathbf{x}}_{s}-(e^{-h}-1 )D_{\theta}(\widetilde{\mathbf{x}}_{s};\sigma_{s}).\]

Now denote \(s_{1}=t_{\lambda}(\lambda_{s}+rh)\), for \(0<r\leqslant 1\), where \(t_{\lambda}=e^{-\lambda}\). There are two families of single-step one-parameter two-stage exponential ODE schemes:

\[\widetilde{\mathbf{x}}_{t} = \frac{\sigma_{t}}{\sigma_{s}}\widetilde{\mathbf{x}}_{s}-(e^{-h}-1 )\left[\left(1-\frac{1}{2r}\right)D_{\theta}(\widetilde{\mathbf{x}}_{s}; \sigma_{s})+\frac{1}{2r}D_{\theta}(\widetilde{\mathbf{x}}_{1};\sigma_{s_{1}})\right]\] \[\widetilde{\mathbf{x}}_{t} = \frac{\sigma_{t}}{\sigma_{s}}\widetilde{\mathbf{x}}_{s}-(e^{-h}-1 )D_{\theta}(\widetilde{\mathbf{x}}_{s};\sigma_{s})+\frac{1}{r}\left(\frac{e^{ -h}-1}{h}+1\right)[D_{\theta}(\widetilde{\mathbf{x}}_{1};\sigma_{s_{1}})-D_{ \theta}(\widetilde{\mathbf{x}}_{s};\sigma_{s})],\]

with same supporting value

\[\widetilde{\mathbf{x}}_{1} = \frac{\sigma_{s_{1}}}{\sigma_{s}}\widetilde{\mathbf{x}}_{s}-(e^{ -rh}-1)D_{\theta}(\widetilde{\mathbf{x}}_{s};\sigma_{s}).\]

In the same vein we define a single-step two stage exponential SDE scheme:

\[\widetilde{\mathbf{x}}_{1} = \frac{\sigma_{s_{1}}^{2}}{\sigma_{s}^{2}}\widetilde{\mathbf{x}}_{ s}-(e^{-2rh}-1)D_{\theta}(\widetilde{\mathbf{x}}_{s};\sigma_{s})+\sigma_{s_{1}} \sqrt{e^{-2rh}-1}\epsilon_{1}\] \[\widetilde{\mathbf{x}}_{t} = \frac{\sigma_{t}^{2}}{\sigma_{s}^{2}}\widetilde{\mathbf{x}}_{s}-(e ^{-2h}-1)\left[\left(1-\frac{1}{2r}\right)D_{\theta}(\widetilde{\mathbf{x}}_{s} ;\sigma_{s})+\frac{1}{2r}D_{\theta}(\widetilde{\mathbf{x}}_{1};\sigma_{s_{1}})\right]\] \[+\sigma_{t}\left[\sqrt{e^{-2h}-e^{-2rh}}\epsilon_{1}+\sqrt{e^{-2rh }-1}\epsilon_{2}\right].\]

#### b.2.7 The EDM case

In the EDM-preconditioned case [16, App. B.6], we set \(\sigma_{t}:=t\) and \(\alpha_{t}:=1\). We denote \(\sigma_{d}:=\sigma_{\mathrm{data}}\) the variance of the considered initial dataset and we set

\[c_{1}(t)=\frac{\sigma_{d}^{2}}{t^{2}+\sigma_{d}^{2}},\ \ \ c_{2}(t)=\frac{t\sigma_{d} }{\sqrt{t^{2}+\sigma_{d}^{2}}},\ \ \ c_{3}(t)=\frac{1}{\sqrt{t^{2}+\sigma_{d}^{2}}},\ \ \ c_{4}(t)=\frac{1}{4}\log(t),\]so we obtain the following coefficients:

\[A^{1}(t) =\frac{2}{t} B^{1}(t) =-\frac{2}{t}\] (DP NRSDE) \[A^{2}(t) =\frac{2}{t}\left(1-\frac{\sigma_{d}^{2}}{t^{2}+\sigma_{d}^{2}}\right) B^{2}(t) =\frac{-2\sigma_{d}}{\sqrt{t^{2}+\sigma_{d}^{2}}}\] (NP NRSDE) \[A^{3}(t) =\frac{1}{t} B^{3}(t) =-\frac{1}{t}\] (DP NPFO) \[A^{4}(t) =\frac{1}{t}\left(1-\frac{\sigma_{d}^{2}}{t^{2}+\sigma_{d}^{2}}\right) B^{4}(t) =\frac{-\sigma_{d}}{\sqrt{t^{2}+\sigma_{d}^{2}}}\] (NP NPFO)

In particular, the data prediction neural SDE/PFO are identical to those in the VE case with \(\sigma_{t}=t\). So let us concentrate on the noise prediction regime, leading us to prove Proposition 3.2.

#### b.2.8 Proof of Proposition 3.2

In the Noise Prediction case, we have

\[\Phi^{2}(t,s)=\frac{t^{2}+\sigma_{d}^{2}}{s^{2}+\sigma_{d}^{2}},\quad\Phi^{4} (t,s)=\sqrt{\frac{t^{2}+\sigma_{d}^{2}}{s^{2}+\sigma_{d}^{2}}},\]

so we readily compute:

\[\int_{s}^{t}\Phi^{2}(t,\tau)B^{2}(\tau)\mathrm{d}\tau = (t^{2}+\sigma_{d}^{2})\int_{s}^{t}\frac{1}{\tau^{2}+\sigma_{d}^{2 }}\cdot\frac{-2\sigma_{d}}{\sqrt{\tau^{2}+\sigma_{d}^{2}}}\mathrm{d}\tau,\] \[\int_{s}^{t}\Phi^{4}(t,\tau)B^{4}(\tau)\mathrm{d}\tau = \sqrt{t^{2}+\sigma_{d}^{2}}\int_{s}^{t}\frac{\sigma_{d}}{\sqrt{ \tau^{2}+\sigma_{d}^{2}}}\cdot\frac{-1}{\sqrt{\tau^{2}+\sigma_{d}^{2}}} \mathrm{d}\tau.\]

Let us consider two different changes of variables:

\[\lambda_{t}:=-\log\left(\arctan\left(\frac{t}{\sigma_{d}}\right)\right)\qquad \text{and}\qquad\lambda_{t}:=-\log\left(\frac{t}{\sigma_{d}\sqrt{t^{2}+\sigma _{d}^{2}}}\right),\] (37)

that will be used for the (NP NPFO) and (NP NRSDE), respectively. For the former case, we have

\[e^{-\lambda_{t}}\mathrm{d}\lambda_{t}=-\frac{\frac{1}{\sigma_{d}}\mathrm{d}t} {1+\frac{t^{2}}{\sigma_{d}^{2}}}=-\frac{\sigma_{d}\mathrm{d}t}{\sigma_{d}^{2} +t^{2}}.\]

Therefore, we can deduce that

\[\int_{s}^{t}\Phi^{4}(t;\tau)B^{4}(\tau)\mathrm{d}\tau = \int_{s}^{t}\sqrt{\frac{t^{2}+\sigma_{d}^{2}}{\tau^{2}+\sigma_{d }^{2}}}\cdot\frac{-\sigma_{d}}{\sqrt{\tau^{2}+\sigma_{d}^{2}}}d\tau\] \[= \sqrt{t^{2}+\sigma_{d}^{2}}\int_{\lambda_{s}}^{\lambda_{t}}\frac{ -\sigma_{d}}{\tau^{2}+\sigma_{d}^{2}}\mathrm{d}\tau\] \[= \sqrt{t^{2}+\sigma_{d}^{2}}\int_{\lambda_{s}}^{\lambda_{t}}e^{- \lambda}\mathrm{d}\lambda\] \[= \sqrt{t^{2}+\sigma_{d}^{2}}\arctan\left(\frac{t}{\sigma_{d}} \right)(e^{h}-1).\]

For the latter case, we have

\[e^{-\lambda_{t}}\mathrm{d}\lambda_{t} = -\frac{\sigma_{d}\sqrt{t^{2}+\sigma_{d}^{2}}-t\sigma_{d}\frac{t}{ \sqrt{t^{2}+\sigma_{d}^{2}}}}{\sigma_{d}^{2}(t^{2}+\sigma_{d}^{2})}\] \[= -\frac{\sigma_{d}^{2}}{\sigma_{d}(t^{2}+\sigma_{d}^{2})\sqrt{t^{2} +\sigma_{d}^{2}}}=-\frac{\sigma_{d}}{(t^{2}+\sigma_{d}^{2})\sqrt{t^{2}+\sigma _{d}^{2}}}.\]We then obtain

\[\int_{s}^{t}\Phi^{2}(t;\tau)B^{2}(\tau)\mathrm{d}\tau = \int_{s}^{t}\frac{t^{2}+\sigma_{d}^{2}}{\tau^{2}+\sigma_{d}^{2}} \cdot\frac{-2\sigma_{d}}{\sqrt{\tau^{2}+\sigma_{d}^{2}}}\mathrm{d}\tau\] \[= 2(t^{2}+\sigma_{d}^{2})\int_{s}^{t}e^{-\lambda}\mathrm{d}\lambda\] \[= \frac{2t\sqrt{t^{2}+\sigma_{d}^{2}}}{\sigma_{d}}(e^{h}-1).\]

The stochastic integral \(\int_{s}^{t}\Phi^{2}(t,\tau)g(\tau)\mathrm{d}\bm{\bar{\omega}}_{\tau}\) in noise prediction case is a Gaussian random variable with zero mean and whose variance can be computed by Ito isometry as

\[\int_{t}^{s}[\Phi^{2}(t;\tau)g^{2}(\tau)]^{2}\mathrm{d}\tau = (t^{2}+\sigma_{d}^{2})^{2}\int_{t}^{s}\frac{1}{[\tau^{2}+\sigma_{ d}^{2}]^{2}}2\tau\mathrm{d}\tau\] \[= (t^{2}+\sigma_{d}^{2})^{2}\int_{t}^{s}\frac{1}{[\tau^{2}+\sigma_{ d}^{2}]^{2}}\mathrm{d}\tau^{2}\] \[= (t^{2}+\sigma_{d}^{2})^{2}\left(-\frac{1}{s^{2}+\sigma_{d}^{2}}+ \frac{1}{t^{2}+\sigma_{d}^{2}}\right)\] \[= \frac{(t^{2}+\sigma_{d}^{2})(s^{2}-t^{2})}{(s^{2}+\sigma_{d}^{2} )}.\]

Putting everything together, we obtain the analytic solution at time \(t\) of (2) with coefficients (11) and initial value \(\mathbf{x}_{s}\) for the (NP NRSDE):

\[\mathbf{x}_{t}=\frac{t^{2}+\sigma_{d}^{2}}{s^{2}+\sigma_{d}^{2}}\mathbf{x}_{s} +2(t^{2}+\sigma_{d}^{2})\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\hat{F}_{ \theta}(\widehat{\mathbf{x}}_{\lambda},\lambda)\mathrm{d}\lambda-\sqrt{2}(t^ {2}+\sigma_{d}^{2})\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\bm{ \bar{\omega}}_{\lambda},\] (38)

where \(\lambda_{t}:=-\log\left[\frac{t}{\sigma_{d}\sqrt{t^{2}+\sigma_{d}^{2}}}\right]\). For the (NP NPFO), it is given by

\[\mathbf{x}_{t}=\sqrt{\frac{t^{2}+\sigma_{d}^{2}}{s^{2}+\sigma_{d}^{2}}} \mathbf{x}_{s}+\sqrt{t^{2}+\sigma_{d}^{2}}\int_{\lambda_{s}}^{\lambda_{t}}e^{ -\lambda}\hat{F}_{\theta}(\widehat{\mathbf{x}}_{\lambda},\lambda)\mathrm{d} \lambda,\quad\lambda_{t}:=-\log\left[\arctan\left[\frac{t}{\sigma_{d}}\right] \right].\] (39)

This finishes the proof.

_Remark B.2_.: From the above proof, we immediately deduce the SEEDS-1 iterates in the EDM-preconditioned noise prediction case. These are given, for the (NP NPFO) and (NP NRSDE) respectively, by

\[\widetilde{\mathbf{x}}_{t} = \sqrt{\frac{t^{2}+\sigma_{d}^{2}}{s^{2}+\sigma_{d}^{2}}}\widetilde {\mathbf{x}}_{s}+\sqrt{t^{2}+\sigma_{d}^{2}}\arctan\left(\frac{t}{\sigma_{d}} \right)(e^{h}-1)\epsilon_{\theta}(\widetilde{\mathbf{x}}_{s},s),\] (40) \[\widetilde{\mathbf{x}}_{t} = \frac{t^{2}+\sigma_{d}^{2}}{s^{2}+\sigma_{d}^{2}}\widetilde{ \mathbf{x}}_{s}+\frac{2t\sqrt{t^{2}+\sigma_{d}^{2}}}{\sigma_{d}}(e^{h}-1) \epsilon_{\theta}(\widetilde{\mathbf{x}}_{s},s)+\sqrt{\frac{(t^{2}+\sigma_{d}^ {2})(s^{2}-t^{2})}{(s^{2}+\sigma_{d}^{2})}}\epsilon,\] (41)

where \(\epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\).

#### b.2.9 The SEEDS algorithms in the EDM Noise Prediction case

In the same way we presented the SEEDS algorithms 3 and 4 for the VP case, the change-of-variables optimized for the EDM framework in Proposition 3.2 induces algorithms 5 for SEEDS-2 and 6 for SEEDS-3 respectively, under the Noise Prediction regime. This is the version of SEEDS that we use to achieve same performance as EDM solver but with twice less NFEs than the latter for ImageNet-64 (see Table 1 and Fig. 1 (c) in the main part of this article).

## Appendix C Convergence Proofs

In this Section, we give detailed proofs of Theorem 4.1, Proposition 4.3 and Corollary 4.4 stated in the main part of this paper. Let us start recalling its framework. We start by considering the NP NRSDE (15) with VP coefficients:

\[\mathrm{d}\mathbf{x}_{t} = \left[f(t)\mathbf{x}_{t}+\frac{g^{2}(t)}{\check{\sigma}_{t}} \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t},t)\right]\mathrm{d}t+g(t) \mathrm{d}\widetilde{\boldsymbol{\omega}}_{t}\] \[= \left[f(t)\mathbf{x}_{t}+\frac{2\alpha_{t}^{2}\check{\sigma}_{t }\sigma_{t}}{\check{\sigma}_{t}}\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t},t)\right]\mathrm{d}t+g(t)\mathrm{d}\widetilde{\boldsymbol{\omega}}_{t}\] \[= \left[\frac{\mathrm{d}\log\alpha_{t}}{\mathrm{d}t}\mathbf{x}_{t}+2 \alpha_{t}\check{\sigma}_{t}\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t},t) \right]\mathrm{d}t+\alpha_{t}\sqrt{\frac{\mathrm{d}[\sigma_{t}^{2}]}{\mathrm{d }t}}\mathrm{d}\widetilde{\boldsymbol{\omega}}_{t}.\]

Denote \(t_{\lambda}\) the inverse of \(\lambda_{t}:=-\log(\sigma_{t})\) (which is a strictly decreasing function of \(t\)) and denote \(\widetilde{\mathbf{x}}_{\lambda}:=\mathbf{x}(t_{\lambda}(\lambda)),\hat{ \boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{\lambda},\lambda):= \boldsymbol{\epsilon}_{\theta}(\mathbf{x}(t_{\lambda}(\lambda)),t_{\lambda}( \lambda))\). We consider a time discretization \(\{t_{i}\}_{i=0}^{M+1}\) going backwards in time starting from \(t_{0}=T\) to \(t_{M+1}=0\) and to ease the notation we will alwaysdenote \(t<s\) for two consecutive time-steps \(t_{i}<t_{i-1}\). The analytic solution at time \(t\) of the RSDE (2) with coefficients (7) and initial value \(\mathbf{x}_{s}\) reads

\[\mathbf{x}_{t} = \frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-2\alpha_{t}\int_{ \lambda_{s}}^{\lambda_{t}}e^{-\lambda}\hat{\boldsymbol{\varepsilon}}_{\theta}( \widehat{\mathbf{x}}_{\lambda},\lambda)\mathrm{d}\lambda-\sqrt{2}\alpha_{t} \int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\tilde{\boldsymbol{ \omega}}(\lambda).\] (43)

Given an initial condition \(\widetilde{\mathbf{x}}_{t_{0}}=\mathbf{x}_{T}\), the SEEDS-1 iterates read, for \(h_{i}=\lambda_{t_{i}}-\lambda_{t_{i-1}}=\lambda_{t}-\lambda_{s}\),

\[\widetilde{\mathbf{x}}_{t}=\frac{\alpha_{t}}{\alpha_{s}}\widetilde{\mathbf{x} }_{s}-2\bar{\sigma}_{t}(e^{h_{i}}-1)\boldsymbol{\epsilon}_{\theta}(\widetilde {\mathbf{x}}_{s},s)-\bar{\sigma}_{t}\sqrt{e^{2h_{i}}-1}\epsilon\quad\epsilon \sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d}).\]

**Assumption C.1**.:
1. The function \(\boldsymbol{\epsilon}_{\theta}(\mathbf{x},t)\) is continuous (and hence bounded) on \([0,T]\), Lipschitz with respect to \(\mathbf{x}\) and there is a constant \(C\) such that, for \(t,s\in[0,T]\) with \(t<s\), we have \[|\boldsymbol{\epsilon}_{\theta}(\mathbf{x},t)-\boldsymbol{ \epsilon}_{\theta}(\mathbf{y},t)|^{2} \leqslant L_{1}|\mathbf{x}-\mathbf{y}|^{2}\] (44) \[|\boldsymbol{\epsilon}_{\theta}(\mathbf{x},t)|^{2}\vee|g(t)|^{2} \leqslant L_{2}(1+|\mathbf{x}|^{2})\] (45) \[|\boldsymbol{\epsilon}_{\theta}(\mathbf{x},t)-\boldsymbol{ \epsilon}_{\theta}(\mathbf{x},s)|^{2} \leqslant L_{3}(1+|\mathbf{x}|^{2})|t-s|^{2}\] (46)
2. \(h=\max_{1\leq i\leq M}|h_{i}|\sim O(1/M)\), where \(h_{i}=\lambda_{t_{i}}-\lambda_{t_{i-1}}\).

Let \(\mathcal{C}_{P}^{l}(\mathbb{R}^{d},\mathbb{R})\) denote the family of \(l\) times continuously differentiable real-valued functions on \(\mathbb{R}^{d}\) whose partial derivatives of order \(\leqslant l\) have polynomial growth and let \(\mathcal{C}_{P}^{k,l}(I\times\mathbb{R}^{d},\mathbb{R})\) be the space of functions \(g(\cdot,\cdot)\) such that, for all \((t,x)\in I\times\mathbb{R}^{d}\), \(g(\cdot,x)\in\mathcal{C}^{k}(I,\mathbb{R})\) and \(g(t,\cdot)\in\mathcal{C}_{P}^{l}(\mathbb{R}^{d},\mathbb{R})\).

**Assumption C.2**.: In addition to Assumption C.1, assume that all the components of \(\boldsymbol{\epsilon}_{\theta}\) belong to \(\mathcal{C}_{P}^{4,2}(\mathbb{R}^{d}\times[0,1],\mathbb{R})\).

Before going into the proofs, we give some context that lead us to necessitate such assumptions.

### Preliminaries

For an interval \(I=[t_{0},T]\), let \(\mathbf{x}=(\mathbf{x}(t))_{I}\) the solution of the following SDE

\[\mathrm{d}\mathbf{x}(t)=f(\mathbf{x}(t),t)\mathrm{d}t+g(t)\mathrm{d} \boldsymbol{\omega}(t),\] (47)

where \(g(t)=\hat{g}(t)\cdot\mathrm{Id}_{d}\) is considered here as a diagonal matrix with identical diagonal entries \(\hat{g}(t)\). Suppose that \(f,g\) are continuous, and satisfy a linear growth and Lipschitz condition so that the conditions of the Existence and Uniqueness Theorem are fulfilled for the SDE (47).

Let \(I_{h}=\{t_{0},\ldots,t_{M}\}\) be a time discretization of \(I\) with step sizes \(h_{n}=t_{n+1}-t_{n}\) for \(n=0,\ldots,M-1\) and let \(h=\max_{0\leqslant n<M}h_{n}\). A time discrete approximation scheme \(\widehat{\mathbf{x}}=(\widehat{\mathbf{x}}_{n})_{I_{h}}\) will be defined as a sequence \(\widehat{\mathbf{x}}_{0}=\mathbf{x}(t_{0})\) and

\[\widehat{\mathbf{x}}_{n+1}=\Phi(\widehat{\mathbf{x}}_{n},h_{n},I_{n}),\qquad n =0,\ldots,M-1,\]

where \(I_{0}\) is independent of \(\widehat{\mathbf{x}}_{0}\), with \(I_{n}=\boldsymbol{\omega}(t_{n+1}-t_{n})\) Wiener increments drawn from the normal distributions with zero mean and variance \(h_{n}\) and which are independent of \(\widehat{\mathbf{x}}_{0},\ldots,\widehat{\mathbf{x}}_{n}\) and \(I_{0},\ldots,I_{n-1}\).

A scheme \(\widehat{\mathbf{x}}\) converges in the strong (resp. weak) sense, with global order \(p>0\), to the solution \(\mathbf{x}\) of the SDE (47) if there is a constant \(C>0\), independent of \(h\) and \(\delta>0\), such that, for each \(h\in]0,\delta]\), we have

\[(\mathbb{E}[|\mathbf{x}(T)-\widehat{\mathbf{x}}_{M}|^{2}])^{1/2}\leq Ch^{p}, \quad(\mathrm{resp}\,.|\mathbb{E}[G(\mathbf{x}(T))]-\mathbb{E}[G(\widehat{ \mathbf{x}}_{M})]|\leq Ch^{p},\forall G\in\mathcal{C}_{P}^{2(p+1)}(\mathbb{R }^{d},\mathbb{R})).\]

Notice that if \((\mathbb{E}[|\mathbf{x}(T)-\widehat{\mathbf{x}}_{M}|^{2}])^{1/2}=\mathcal{O}(h^ {p})\) then for every function \(f\) satisfying a Lipschitz condition, we have \(|\mathbb{E}[f(\mathbf{x}(T))]-\mathbb{E}[f(\widehat{\mathbf{x}}_{M})]|= \mathcal{O}(h^{p})\). Nevertheless, this is not enough to infer the optimal weak order of convergence of such method.

Strong convergence is concerned with the precision of the path, while the weak convergence is with the precision of the moments. As, for DPMs, the center of attention is the evolution of the probability densities rather than that of the noising process of single data samples, weak convergence is enough to guarantee the well-conditioning of our numerical schemes. Moreover, when the diffusion coefficient vanishes, then both strong and weak convergence (with the choice \(G=\mathrm{id}\)) reduce to the usual deterministic convergence criterion for ODEs.

Let us now state some useful results that will be used later on.

**Assumption C.3**.: All moments of the initial value \(\widehat{\mathbf{x}}_{0}\) exist, \(f\) is continuous, satisfy a linear growth and globally Lipschitz condition.

In particular, since \(I\) is a closed finite interval in \(\mathbb{R}\), \(f(\cdot,x)\) and \(g\) are bounded by some constant.

**Theorem C.4** ([25]).: _In addition to C.3, suppose that_

\[|\mathbb{E}[\mathbf{x}(t_{1})-\widehat{\mathbf{x}}_{1}]| \leqslant Ch^{p_{1}}\] \[(\mathbb{E}[|\mathbf{x}(t_{1})-\widehat{\mathbf{x}}_{1}|^{2}])^{1/2} \leqslant Ch^{p_{2}}\]

_with \(p_{2}\geqslant 1/2\) and \(p_{1}\geqslant p_{2}+1/2\). Then \(\widehat{\mathbf{x}}\) is of strong global order \(p=p_{2}-1/2\)._

**Assumption C.5**.: All moments of the initial value \(\widehat{\mathbf{x}}_{0}\) exist, \(f\) is continuous, satisfy a linear growth and Lipschitz condition with all their components belonging to \(\mathcal{C}_{P}^{p+1,2(p+1)}(I\times\mathbb{R}^{d},\mathbb{R})\) and \(g\in\mathcal{C}^{p+1}(I,\mathbb{R})\).

**Theorem C.6** ([25]).: _In addition to C.5, suppose that_

1. _for large enough_ \(r\in\mathbb{N}\)_, the moments_ \(\mathbb{E}[|\widehat{\mathbf{x}}_{n}|^{2r}]\) _exist and are unformingly bounded with respect to_ \(M\) _and_ \(n=0,\ldots,M\)__
2. _for all_ \(G\in\mathcal{C}_{P}^{2(p+1)}(\mathbb{R}^{d},\mathbb{R})\)_, if_ \(\widehat{\mathbf{x}}_{n}=\mathbf{x}(t_{n})\)_, then for some_ \(K\in\mathcal{C}_{P}^{0}(\mathbb{R}^{d},\mathbb{R})\)_, we have_ \[|\mathbb{E}[G(\mathbf{x}(t_{n+1}))]-\mathbb{E}[G(\widehat{\mathbf{x}}_{n+1}) ]|\leq K(\widehat{\mathbf{x}}_{n})h^{p+1}.\]

_Then \(\widehat{\mathbf{x}}\) is of weak global order \(p\)._

**Lemma C.7**.: _Suppose that \(\widehat{\mathbf{x}}_{0}\) has moments of all orders and that, for \(h<1\),_

\[|\mathbb{E}[\Phi(\widehat{\mathbf{x}}_{n},h_{n},I_{n})-\widehat{ \mathbf{x}}_{n}]| \leqslant K(1+|\widehat{\mathbf{x}}_{n}|)h\] \[|\Phi(\widehat{\mathbf{x}}_{n},h_{n},I_{n})-\widehat{\mathbf{x} }_{n}| \leqslant X_{n}(1+|\widehat{\mathbf{x}}_{n}|)h^{1/2}\]

_where \(X_{n}\) has moments of all orders. Then Condition 1 in Theorem C.6 is fulfilled._

### Convergence of SEEDS-1

In this section we will prove that that SEEDS-1 as described above is of global strong order 1.0.

#### c.2.1 Strong Ito-Taylor approximation

Then the truncated Ito-Taylor expansion of the analytic solution \(\mathbf{x}_{t}\) of the VP NP NRSDE starting from \(\mathbf{x}_{s}\) is given, for \(\epsilon\sim\mathcal{N}(0,1)\), by

\[\mathbf{x}_{t} = \frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-2\alpha_{t}\int_{ \lambda_{s}}^{\lambda_{t}}e^{-\lambda}\widehat{\bm{\epsilon}}_{\theta}( \widehat{\mathbf{x}}_{\lambda},\lambda)\mathrm{d}\lambda-\sqrt{2}\alpha_{t} \int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\bm{\omega}_{\lambda}\] \[= \frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-2\bar{\sigma}_{t}(e^ {h}-1)\bm{\epsilon}_{\theta}(\mathbf{x}_{s},s)-\bar{\sigma}_{t}\sqrt{e^{2h}-1} \epsilon+\mathcal{R}_{1}\] \[= \frac{\alpha_{t}}{\alpha_{s}}\mathbf{x}_{s}-2\bar{\sigma}_{t}(e^ {h}-1)\bm{\epsilon}_{\theta}(\mathbf{x}_{s},s)-\bar{\sigma}_{t}\sqrt{e^{2h}-1 }\epsilon+\mathcal{O}(h),\]where the symbol \(\mathcal{O}(h^{p})\) represent terms \(\mathbf{u}\) such that \(\|\mathbf{u}\|\leqslant|K(\mathbf{x}_{s})|h^{p}\), for \(K\in\mathcal{C}_{P}^{0}(\mathbb{R}^{d},\mathbb{R})\) and small \(h>0\). The SEEDS-1 scheme corresponds to such truncated Ito-Taylor expansion containing only the time and Wiener integrals of multiplicity one. As such, since \(G_{t}g(t)=0\) and assuming Lipschitz and linear growth conditions on \(\boldsymbol{\epsilon}_{\theta}\) as in Assumption C.1, \(\mathbf{x}_{t}\) can be interpreted as an order 1.0 strong Ito-Taylor approximation [17, Theorem 10.6.3] of the solution to (42).

#### c.2.2 Continuous approximation of SEEDS-1

Let \(\tilde{\alpha}_{t}:=\frac{1}{2}\beta_{d}t^{2}+\beta_{m}t\), where \(\beta_{d}=\beta_{\max}-\beta_{\min}=19.9\), \(\beta_{m}=\beta_{\min}=0.1\). We have

\[\sigma_{t}=\sqrt{e^{\tilde{\alpha}_{t}}-1},\qquad\alpha_{t}=e^{-\frac{1}{2} \tilde{\alpha}_{t}}=\frac{1}{\sqrt{\sigma_{t}^{2}+1}}\]

so, in particular, as \(T=1\), we have \(\tilde{\alpha}_{t_{0}}=\tilde{\alpha}_{1}=\frac{1}{2}(\beta_{\max}-\beta_{ \min})+\beta_{\min}=\frac{1}{2}(\beta_{\max}+\beta_{\min})\approx 10.05\) and \(\tilde{\alpha}_{t_{M+1}}=\tilde{\alpha}_{0}=0\). We deduce \(\alpha_{t_{0}}=\alpha_{1}\approx e^{-\frac{10.05}{2}}<1\), \(\alpha_{t_{M+1}}=\alpha_{0}=1\). Next, \(\sigma_{t_{0}}=\sigma_{1}\approx\sqrt{e^{10.05}-1}>1\) and \(\sigma_{t_{M+1}}=\sigma_{0}=0\). As such, \(\lambda_{t_{0}}=-\log(\sigma_{1}):=-L_{0}<0\) and \(\lambda_{t}\underset{t\to t_{M+1}}{\longrightarrow}+\infty\).

As such, we will set \(t_{M}=t_{M+1}+\varepsilon\) the end time so that \(\lambda_{t_{M}}=L_{0}\) is finite. This implies that, for \(\lambda\in[-L_{0},+\infty[\), \(0<e^{-L_{0}}\leqslant e^{\lambda}<1<e^{\lambda_{M}}\) and, for \(\hat{T}=\lambda_{t_{M}}-\lambda_{t_{0}}\), \(e^{h}=e^{\lambda_{t}-\lambda_{s}}\leqslant e^{\hat{T}}\). Now set, for \(\lambda\in[-L_{0},+\infty[\),

\[\hat{\alpha}_{\lambda}:=\sqrt{\frac{1}{1+e^{-2\lambda}}},\qquad\hat{\sigma}_{ \lambda}:=\sqrt{\frac{1}{1+e^{+2\lambda}}}.\]

Then, as \(\lambda\) increases, \(\hat{\alpha}_{\lambda}\) increases starting from \(0<\hat{\alpha}_{\lambda_{t_{0}}}<1\) and \(\hat{\alpha}_{\lambda}\underset{\lambda\to+\infty}{\longrightarrow}1\) while at the same time \(\hat{\sigma}_{\lambda}\) decreases starting from \(0<\hat{\sigma}_{\lambda_{t_{0}}}<1\) and \(\hat{\sigma}_{\lambda}\underset{\lambda\to+\infty}{\longrightarrow}0\). As such, we can rewrite the exact solution (43) as

\[\widehat{\mathbf{x}}_{\lambda_{t}}=\frac{\hat{\alpha}_{\lambda_{t}}}{\hat{ \alpha}_{\lambda_{s}}}\widehat{\mathbf{x}}_{\lambda_{s}}-2\hat{\alpha}_{ \lambda_{t}}\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\hat{\boldsymbol{ \epsilon}}_{\theta}(\widehat{\mathbf{x}}_{\lambda},\lambda)\mathrm{d}\lambda -\sqrt{2}\hat{\alpha}_{\lambda_{t}}\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda }\mathrm{d}\widetilde{\boldsymbol{\omega}}(\lambda).\] (48)

Notice that \(\frac{\hat{\alpha}_{\lambda_{t}}}{\hat{\alpha}_{\lambda_{s}}}\) is bounded for all \(t,s\) by

\[\frac{\hat{\alpha}_{\lambda_{t}}}{\hat{\alpha}_{\lambda_{s}}}\leqslant\frac{1 }{\hat{\alpha}_{\lambda_{t_{0}}}}\]

and \(0<\hat{\alpha}_{\lambda}<1\) for all \(\lambda\in[-L_{0},+\infty[\).

Recall that the SEEDS-1 is defined recursively as

\[\mathbf{y}_{t_{0}}\leftarrow\mathbf{x}_{T},\mathbf{y}_{t_{i}}\leftarrow\frac{ \alpha_{t_{i}}}{\alpha_{t_{i-1}}}\mathbf{y}_{t_{i-1}}-2\overline{\sigma}_{t_{i }}(e^{h_{i}}-1)\boldsymbol{\epsilon}_{\theta}(\mathbf{y}_{t_{i-1}},t_{i-1})- \bar{\sigma}_{t_{i}}\sqrt{e^{2h_{i}}-1}\epsilon_{i}\]

and, for simplicity, we will denote \(\mathbf{y}_{\lambda_{t_{i}}}\) for the iterates (48).

Define a continuous approximation of SEEDS-1 as follows. For \(\hat{h}=t-s\), we write \(\hat{s}=[s/\hat{h}]\hat{h}\) where \([x]\) denotes the largest integer lesser or equal to \(x\) and \(I_{[A]}\) is the indicator function associated to a set \(A\). We define the step function:

\[\widehat{\mathbf{y}}(\lambda):=\sum_{k\geqslant 0}I_{[\lambda_{t_{k}},\lambda_{t_{k +1}}]}\mathbf{y}_{\lambda_{t_{k}}}\]

and the continuous approximation

\[\mathbf{y}(t):=\frac{\alpha_{t}}{\alpha_{t_{0}}}\mathbf{y}(t_{0})-2\alpha_{t} \int_{\lambda_{t_{0}}}^{\lambda_{t}}e^{-\hat{\lambda}}\hat{\boldsymbol{ \epsilon}}_{\theta}(\widehat{\mathbf{y}}(\lambda),\hat{\lambda})\mathrm{d} \lambda-\sqrt{2}\alpha_{t}\int_{\lambda_{t_{0}}}^{\lambda_{t}}e^{-\lambda} \mathrm{d}\boldsymbol{\omega}_{\lambda}.\]

**Proposition C.8**.: _There are two constants \(C_{1},C_{2}\) independent of \(h\) such that, for all \(t\in[0,T]\), we have_

\[\mathbb{E}\left[\sup_{t_{0}\leqslant t\leqslant t_{M}}\left|\mathbf{ y}(t)\right|^{2}\right] \leqslant C_{1}\] \[\mathbb{E}\left[\left|\mathbf{y}(t)-\widehat{\mathbf{y}}(t) \right|^{2}\right] \leqslant C_{2}h^{2}.\]Proof.: Recall the standard inequality \((a+b+c)^{2}\leqslant 3(a^{2}+b^{2}+c^{2})\) for \(a,b,c\in\mathbb{R}\). Then

\[\left|\mathbf{y}(t)\right|^{2} \leqslant 3\left[\left|\frac{\alpha_{t}}{\alpha_{t_{0}}}\mathbf{y}_{t_{0}} \right|^{2}+4\alpha_{t}^{2}\left|\int_{\lambda_{t_{0}}}^{\lambda_{t}}e^{- \hat{\lambda}}\hat{\epsilon}_{\theta}(\widehat{\mathbf{y}}(\lambda),\hat{ \lambda})\mathrm{d}\lambda\right|^{2}+2\alpha_{t}^{2}\left|\int_{\lambda_{t_{0} }}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\boldsymbol{\omega}_{\lambda}\right|^{2} \right].\]

Using the fact that \(\alpha_{t}\leqslant 1\), we have, by writing \(\hat{T}=\lambda_{t_{M}}-\lambda_{t_{0}}\) and taking the expectation:

\[\mathbb{E}\left[\sup_{t_{0}\leqslant t\leqslant t_{M}}\left| \mathbf{y}(t)\right|^{2}\right]\] \[\leqslant 3\left[\left|\frac{\alpha_{t}}{\alpha_{t_{0}}}\right|^{2} \mathbb{E}\left[\left|\mathbf{y}_{t_{0}}\right|^{2}\right]+4\mathbb{E}\left[ \left|\int_{\lambda_{t_{0}}}^{\lambda_{t}}e^{-\hat{\lambda}}\hat{\epsilon}_{ \theta}(\widehat{\mathbf{y}}(\lambda),\hat{\lambda})\mathrm{d}\lambda\right|^ {2}\right]+2\alpha_{t}^{2}\mathbb{E}\left[\left|\int_{\lambda_{t_{0}}}^{ \lambda_{t}}e^{-\lambda}\mathrm{d}\boldsymbol{\omega}_{\lambda}\right|^{2} \right]\right]\] \[\leqslant 3\left[\left|\frac{\alpha_{t}}{\alpha_{t_{0}}}\right|^{2} \mathbb{E}\left[\left|\mathbf{y}_{t_{0}}\right|^{2}\right]+4\hat{T}\int_{ \lambda_{t_{0}}}^{\lambda_{t}}|e^{-\hat{\lambda}}|^{2}\mathbb{E}\left[\left| \hat{\epsilon}_{\theta}(\widehat{\mathbf{y}}(\lambda),\hat{\lambda})\right|^ {2}\right]\mathrm{d}\lambda+2\alpha_{t}^{2}\int_{\lambda_{t_{0}}}^{\lambda_{t} }e^{-2\lambda}\mathrm{d}\lambda\right]\] \[\leqslant 3K\left[\mathbb{E}\left[\left|\mathbf{y}_{t_{0}}\right|^{2} \right]+4\hat{T}\hat{\lambda}_{t_{0}}\mathbb{E}\left[\left|\hat{\epsilon}_{ \theta}(\widehat{\mathbf{y}}(\lambda),\hat{\lambda})\right|^{2}\right]\mathrm{ d}\lambda+\left(e^{2(\lambda_{t}-\lambda_{t_{0}})}-1\right)\right]\] \[\leqslant 3K\left[\mathbb{E}\left[\left|\mathbf{y}_{t_{0}}\right|^{2} \right]+4\hat{T}L_{2}\int_{\lambda_{t_{0}}}^{\lambda_{t}}\left(1+\mathbb{E} \left[\left|\widehat{\mathbf{y}}(\lambda)\right|^{2}\right]\right)\mathrm{d} \lambda+\left(e^{2\hat{T}}-1\right)\right]\] \[\leqslant 3K\left[\mathbb{E}\left[\left|\mathbf{y}_{t_{0}}\right|^{2} \right]+4\hat{T}L_{2}\hat{T}+e^{2\hat{T}}-1+4\hat{T}L_{2}\int_{\lambda_{t_{0}}} ^{\lambda_{t}}\mathbb{E}\left[\left|\widehat{\mathbf{y}}(\lambda)\right|^{2} \right]\mathrm{d}\lambda\right]\] \[\leqslant 3K\left(\mathbb{E}\left[\left|\mathbf{y}_{t_{0}}\right|^{2} \right]+4\hat{T}L_{2}\hat{T}+e^{2\hat{T}}-1\right)+3K4\hat{T}L_{2}\int_{ \lambda_{t_{0}}}^{\lambda_{t}}\mathbb{E}\left[\left|\widehat{\mathbf{y}}( \lambda)\right|^{2}\right]\mathrm{d}\lambda\] \[\leqslant 3K\left(\mathbb{E}\left[\left|\mathbf{x}(t_{0})\right|^{2} \right]+4\hat{T}^{2}L_{2}+e^{2\hat{T}}-1\right)+12K\hat{T}L_{2}\int_{\lambda_{ t_{0}}}^{\lambda_{t}}\mathbb{E}\left[\sup_{\lambda_{0}\leqslant r\leqslant \lambda}\left|\mathbf{y}(r)\right|^{2}\right]\mathrm{d}\lambda,\]

where we used the linearity of expectation, Holder's inequality, Doob's martingale inequality, Ito isometry, the linear growth condition of \(\hat{\epsilon}_{\theta}\), and we set \(K=\max\left(\left|\frac{\alpha_{t}}{\alpha_{t_{0}}}\right|^{2},|e^{L_{0}}|^{2},\bar{\sigma}_{t}^{2},1\right)\). As we know that \(\mathbb{E}\left[\left|\mathbf{x}(t_{0})\right|^{2}\right]<\infty\), we apply Gronwall's inequality in the last line to obtain

\[\mathbb{E}\left[\sup_{t_{0}\leqslant t\leqslant t_{M}}\left|\mathbf{y}(t) \right|^{2}\right]\leqslant C_{1},\qquad C_{1}:=3K\left[\mathbb{E}\left[\left| \mathbf{x}(t_{0})\right|^{2}\right]+4\hat{T}^{2}L_{2}+e^{2\hat{T}}-1\right]e^{1 2K\hat{T}L_{2}}.\]

Second, we have, for \(s=t_{i}\), \(u=t_{i+1}\) and \(t\in[t_{\lambda}(u),t_{\lambda}(s)]\),

\[\mathbf{y}(t)-\widehat{\mathbf{y}}(t)=\left(\frac{\alpha_{t}}{\alpha_{s}}-1 \right)\mathbf{y}_{s}-2\alpha_{t}\int_{\lambda_{s}}^{\lambda_{t}}e^{-\hat{ \lambda}}\hat{\epsilon}_{\theta}(\mathbf{y}_{\lambda_{s}},\lambda_{s})\mathrm{d}\lambda,\]

so that, using Holder's inequality, we get

\[\left|\mathbf{y}(t)-\widehat{\mathbf{y}}(t)\right|^{2} \leqslant 3\left[\left|\frac{\alpha_{t}}{\alpha_{s}}-1\right|^{2}\left| \mathbf{y}_{s}\right|^{2}+4h\int_{\lambda_{s}}^{\lambda_{t}}|e^{-\hat{\lambda}}|^ {2}\left|\hat{\epsilon}_{\theta}(\mathbf{y}_{\lambda_{s}},\lambda_{s})\right|^{2} \mathrm{d}\lambda\right].\]

Now using Ito isometry we obtain:

\[\mathbb{E}\left[\left|\mathbf{y}(t)-\widehat{\mathbf{y}}(t)\right| ^{2}\right] \leqslant 3\left[\left|\frac{\alpha_{t}}{\alpha_{s}}-1\right|^{2}\mathbb{E} \left[\left|\mathbf{y}_{s}\right|^{2}\right]+4h\mathbb{E}\left[\int_{\lambda_{s}}^{ \lambda_{t}}|e^{-\hat{\lambda}}|^{2}\left|\hat{\epsilon}_{\theta}(\mathbf{y}_{ \lambda_{s}},\lambda_{s})\right|^{2}\mathrm{d}\lambda\right]\right].\]Now, using the bound \(\mathbb{E}\left[\max\left|\mathbf{y}_{t}\right|^{2}\right]\leqslant C_{1}\), the fact that \(\epsilon_{\theta}\) is bounded and the same arguments as above, we get

\[\mathbb{E}\left[\left|\mathbf{y}(t)-\widehat{\mathbf{y}}(t)\right| ^{2}\right] \leqslant 3\left[\left|\frac{\alpha_{t}}{\alpha_{s}}-1\right|^{2}C_{1}+4Kh \mathbb{E}\left[\int_{\lambda_{s}}^{\lambda_{t}}\left|\hat{\epsilon}_{\theta}( \mathbf{y}_{\lambda_{s}},\lambda_{s})\right|^{2}\mathrm{d}\lambda\right]\right]\] \[\leqslant 3\left[\left|\frac{\alpha_{t}}{\alpha_{s}}-1\right|^{2}C_{1}+4KhL _{2}\int_{\lambda_{s}}^{\lambda_{t}}(1+C_{1})\mathrm{d}\lambda\right]\] \[\leqslant 3\left[\left|\frac{\alpha_{t}}{\alpha_{s}}-1\right|^{2}C_{1}+4Kh ^{2}L_{2}(1+C_{1})\right].\]

Finally, as we have

\[e^{h}=\frac{\sigma_{s}}{\sigma_{t}}=\frac{\alpha_{t}}{\alpha_{s}}\frac{\sqrt{ 1-\alpha_{s}^{2}}}{\sqrt{1-\alpha_{t}^{2}}},\]

and \(1<\frac{\sqrt{1-\alpha_{t}^{2}}}{\sqrt{1-\alpha_{t}^{2}}}\underset{M\to \infty}{\longrightarrow}1\), we obtain \(\left|\frac{\alpha_{t}}{\alpha_{s}}-1\right|^{2}\sim|e^{h}-1|^{2}\sim\mathcal{ O}(h^{2})\). Now, by denoting \(|e^{h}-1|^{2}\leqslant K_{2}h^{2}\), we conclude that

\[\mathbb{E}\left[\left|\mathbf{y}(t)-\widehat{\mathbf{y}}(t)\right|^{2}\right] \leqslant C_{2}h^{2},\]

with \(C_{2}:=3K_{2}C_{1}+4KL_{2}(1+C_{1})\). This finishes the proof. 

#### c.2.3 Proof of Theorem 4.1

Let's now take a look at the approximation given \(\mathbf{y}_{t_{0}}=\mathbf{x}_{t_{0}}\). We have

\[\mathbf{y}_{t}-\mathbf{x}_{t} = 2\alpha_{t}\int_{\lambda_{t_{0}}}^{\lambda_{t}}\left[e^{- \lambda}\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{ \lambda},\lambda)-e^{-\hat{\lambda}}\widehat{\boldsymbol{\epsilon}}_{\theta}( \widehat{\mathbf{y}}_{\lambda},\hat{\lambda})\right]\mathrm{d}\lambda.\]

Using the inequality \(\alpha_{t}\leq 1\), the Lipschitz property of \(\widehat{\boldsymbol{\epsilon}}_{\theta}\), Assumption (C.1), and Holder's inequality we deduce the bound:

\[\left|\mathbf{y}_{t}-\mathbf{x}_{t}\right|^{2} \leqslant 2\left[4\left|\int_{\lambda_{t_{0}}}^{\lambda_{t}}\left[e^{- \lambda}\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{ \lambda},\lambda)-e^{-\hat{\lambda}}\widehat{\boldsymbol{\epsilon}}_{\theta}( \widehat{\mathbf{y}}_{\lambda},\hat{\lambda})\right]\mathrm{d}\lambda\right|^{2}\right]\] \[\leqslant 8\hat{T}\int_{\lambda_{t_{0}}}^{\lambda_{t}}\left|e^{-\lambda} \widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{\lambda}, \lambda)-e^{-\hat{\lambda}}\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat {\mathbf{y}}_{\lambda},\hat{\lambda})\right|^{2}\mathrm{d}\lambda.\]

Taking the expectation yields

\[\mathbb{E}\left[\sup_{t_{0}\leqslant s\leqslant t}\left| \mathbf{y}_{s}-\mathbf{x}_{s}\right|^{2}\right]\] \[\leqslant 8\hat{T}\mathbb{E}\left[\int_{\lambda_{t_{0}}}^{\lambda_{t}} \left|e^{-\lambda}\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x} }_{\lambda},\lambda)-e^{-\hat{\lambda}}\widehat{\boldsymbol{\epsilon}}_{\theta }(\widehat{\mathbf{y}}_{\lambda},\hat{\lambda})\right|^{2}\mathrm{d}\lambda\right].\]

Now, for the first integral, by writing:

\[e^{-\lambda}\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{ \mathbf{x}}_{\lambda},\lambda)-e^{-\hat{\lambda}}\widehat{\boldsymbol{ \epsilon}}_{\theta}(\widehat{\mathbf{y}}_{\lambda},\hat{\lambda})\] \[= e^{-\lambda}\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{ \mathbf{x}}_{\lambda},\lambda)-e^{-\hat{\lambda}}\widehat{\boldsymbol{ \epsilon}}_{\theta}(\widehat{\mathbf{x}}_{\lambda},\lambda)+e^{-\hat{\lambda}} \widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{\lambda}, \lambda)-e^{-\hat{\lambda}}\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{ \mathbf{x}}_{\lambda},\hat{\lambda})\] \[+e^{-\hat{\lambda}}\widehat{\boldsymbol{\epsilon}}_{\theta}( \widehat{\mathbf{x}}_{\lambda},\hat{\lambda})-e^{-\hat{\lambda}}\widehat{ \boldsymbol{\epsilon}}_{\theta}(\mathbf{y}_{\lambda},\hat{\lambda})+e^{-\hat{ \lambda}}\widehat{\boldsymbol{\epsilon}}_{\theta}(\mathbf{y}_{\lambda},\hat{ \lambda})-e^{-\hat{\lambda}}\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{ \mathbf{y}}_{\lambda},\hat{\lambda})\] \[= (e^{-\lambda}-e^{-\hat{\lambda}})\widehat{\boldsymbol{\epsilon}}_{ \theta}(\widehat{\mathbf{x}}_{\lambda},\lambda)+e^{-\hat{\lambda}}\left( \widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{\lambda},\lambda)- \widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{\lambda},\hat{ \lambda})\right)\] \[+e^{-\hat{\lambda}}\left(\widehat{\boldsymbol{\epsilon}}_{\theta}( \widehat{\mathbf{x}}_{\lambda},\hat{\lambda})-\widehat{\boldsymbol{\epsilon}}_{ \theta}(\mathbf{y}_{\lambda},\hat{\lambda})+\widehat{\boldsymbol{\epsilon}}_{ \theta}(\mathbf{y}_{\lambda},\hat{\lambda})-\widehat{\boldsymbol{\epsilon}}_{ \theta}(\widehat{\mathbf{y}}_{\lambda},\hat{\lambda})\right),\]we can state the following inequalities:

\[\mathbb{E}\left[\int_{\lambda_{t_{0}}}^{\lambda_{t}}\left|e^{-\lambda }\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{\lambda}, \lambda)-e^{-\hat{\lambda}}\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{ \mathbf{y}}_{\lambda},\hat{\lambda})\right|^{2}\mathrm{d}\lambda\right]\] \[\leqslant 3\int_{\lambda_{t_{0}}}^{\lambda_{t}}|e^{-\lambda}-e^{-\hat{ \lambda}}|^{2}\mathbb{E}\left[|\widehat{\boldsymbol{\epsilon}}_{\theta}( \widehat{\mathbf{x}}_{\lambda},\lambda)|^{2}\right]\mathrm{d}\lambda+3\mathbb{ E}\left[\int_{\lambda_{t_{0}}}^{\lambda_{t}}|e^{-\hat{\lambda}}|^{2}\left| \widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{\lambda}, \lambda)-\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{ \lambda},\hat{\lambda})\right|^{2}\mathrm{d}\lambda\right]\] \[+3\mathbb{E}\left[\int_{\lambda_{t_{0}}}^{\lambda_{t}}|e^{-\hat{ \lambda}}|^{2}\left|\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{ x}}_{\lambda},\hat{\lambda})-\widehat{\boldsymbol{\epsilon}}_{\theta}( \mathbf{y}_{\lambda},\hat{\lambda})+\widehat{\boldsymbol{\epsilon}}_{\theta}( \mathbf{y}_{\lambda},\hat{\lambda})-\widehat{\boldsymbol{\epsilon}}_{\theta}( \widehat{\mathbf{y}}_{\lambda},\hat{\lambda})\right|^{2}\mathrm{d}\lambda\right]\] \[\leqslant 3\int_{\lambda_{t_{0}}}^{\lambda_{t}}|e^{-\lambda}-e^{-\hat{ \lambda}}|^{2}\mathbb{E}\left[|\widehat{\boldsymbol{\epsilon}}_{\theta}( \widehat{\mathbf{x}}_{\lambda},\lambda)|^{2}\right]\mathrm{d}\lambda+3KL_{3} \mathbb{E}\left[\int_{\lambda_{t_{0}}}^{\lambda_{t}}(1+|\widehat{\mathbf{x}}_{ \lambda}|^{2})|(\lambda-\hat{\lambda})|^{2}\mathrm{d}\lambda\right]\] \[+3\mathbb{E}\left[\int_{\lambda_{t_{0}}}^{\lambda_{t}}|e^{-\hat{ \lambda}}|^{2}\left|\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{ x}}_{\lambda},\hat{\lambda})-\widehat{\boldsymbol{\epsilon}}_{\theta}( \mathbf{y}_{\lambda},\hat{\lambda})+\widehat{\boldsymbol{\epsilon}}_{\theta}( \mathbf{y}_{\lambda},\hat{\lambda})-\widehat{\boldsymbol{\epsilon}}_{\theta}( \widehat{\mathbf{y}}_{\lambda},\hat{\lambda})\right|^{2}\mathrm{d}\lambda\right]\] \[\leqslant 3L_{2}\int_{\lambda_{t_{0}}}^{\lambda_{t}}|e^{-\hat{\lambda}}|^ {2}|e^{\lambda-\hat{\lambda}}-1|^{2}(1+\mathbb{E}[|\widehat{\mathbf{x}}_{ \lambda}|^{2}])\mathrm{d}\lambda+3KL_{3}h^{2}\mathbb{E}\int_{\lambda_{t_{0}}}^ {\lambda_{t}}(1+|\widehat{\mathbf{x}}_{\lambda}|^{2})\mathrm{d}\lambda\] \[+3KL_{1}\int_{\lambda_{t_{0}}}^{\lambda_{t}}\mathbb{E}\left[| \widehat{\mathbf{x}}_{\lambda}-\mathbf{y}_{\lambda}+\mathbf{y}_{\lambda}- \widehat{\mathbf{y}}_{\lambda}|^{2}\right]\mathrm{d}\lambda.\]

Thus, we obtain

\[\mathbb{E}\left[\int_{\lambda_{t_{0}}}^{\lambda_{t}}\left|e^{- \lambda}\widehat{\boldsymbol{\epsilon}}_{\theta}(\widehat{\mathbf{x}}_{ \lambda},\lambda)-e^{-\hat{\lambda}}\widehat{\boldsymbol{\epsilon}}_{\theta}( \widehat{\mathbf{y}}_{\lambda},\hat{\lambda})\right|^{2}\mathrm{d}\lambda\right]\] \[\leqslant 3L_{2}K\hat{T}|e^{h}-1|^{2}(1+C_{1})+3L_{3}h^{2}K(1+C_{1})\hat {T}\] \[+3KL_{1}\int_{\lambda_{t_{0}}}^{\lambda_{t}}\mathbb{E}\left[| \widehat{\mathbf{x}}_{\lambda}-\mathbf{y}_{\lambda}+\mathbf{y}_{\lambda}- \widehat{\mathbf{y}}_{\lambda}|^{2}\right]\mathrm{d}\lambda\] \[\leqslant 3L_{2}K\hat{T}|e^{h}-1|^{2}(1+C_{1})+3L_{3}h^{2}K(1+C_{1})\hat {T}\] \[+6KL_{1}\int_{\lambda_{t_{0}}}^{\lambda_{t}}\mathbb{E}\left[| \widehat{\mathbf{x}}_{\lambda}-\mathbf{y}_{\lambda}|^{2}+|\mathbf{y}_{\lambda}- \widehat{\mathbf{y}}_{\lambda}|^{2}\right]\mathrm{d}\lambda\] \[\leqslant 3L_{2}K\hat{T}|e^{h}-1|^{2}(1+C_{1})+3L_{3}h^{2}K(1+C_{1})\hat {T}\] \[+6KL_{1}\hat{T}C_{2}h^{2}+6KL_{1}\int_{\lambda_{t_{0}}}^{\lambda_{ t}}\mathbb{E}\left[\sup_{\lambda_{0}\leqslant\tau\leqslant\lambda}|\widehat{ \mathbf{x}}(r)-\mathbf{y}(r)|^{2}\right]\mathrm{d}\lambda\] \[\leqslant 3K\hat{T}(1+C_{1})[L_{2}|e^{h}-1|^{2}+L_{3}h^{2}]+6KL_{1}\hat{T}C _{2}h^{2}\] \[+6KL_{1}\int_{\lambda_{t_{0}}}^{\lambda_{t}}\mathbb{E}\left[\sup_{ \lambda_{0}\leqslant r\leqslant\lambda}|\widehat{\mathbf{x}}(r)-\mathbf{y}(r )|^{2}\right]\mathrm{d}\lambda.\]Putting everything together yields

\[\mathbb{E}\left[\sup_{t_{0}\leqslant s\leqslant t}|\mathbf{y}_{s}- \mathbf{x}_{s}|^{2}\right]\] \[\leqslant 8\hat{T}[3K\hat{T}(1+C_{1})[L_{2}|e^{h}-1|^{2}+L_{3}h]+6KL_{1} \hat{T}C_{2}h^{2}]\] \[+8\hat{T}6KL_{1}\int_{\lambda_{t_{0}}}^{\lambda_{t}}\mathbb{E} \left[\sup_{\lambda_{0}\leqslant r\leqslant s}|\widehat{\mathbf{x}}(r)- \mathbf{y}(r)|^{2}\right]\mathrm{d}\lambda\] \[\leqslant 24K\hat{T}^{2}(1+C_{1})[L_{2}|e^{h}-1|^{2}+L_{3}h^{2}]+48KL_{1} \hat{T}^{2}C_{2}h^{2}\] \[+48\hat{T}KL_{1}\int_{\lambda_{t_{0}}}^{\lambda_{t}}\mathbb{E} \left[\sup_{\lambda_{0}\leqslant r\leqslant s}|\widehat{\mathbf{x}}(r)- \mathbf{y}(r)|^{2}\right]\mathrm{d}\lambda.\]

Now, by denoting \(|e^{h}-1|^{2}\leqslant K_{2}h^{2}\), we apply the continuous version of the Gronwall Lemma to obtain:

\[\mathbb{E}\left[\sup_{t_{0}\leqslant s\leqslant t}|\mathbf{y}_{t}-\mathbf{x} _{t}|^{2}\right]\leqslant C_{0}h^{2},\]

where

\[C_{0}=[24K\hat{T}^{2}(1+C_{1})[L_{2}K_{2}+L_{3}]+48K\hat{T}^{2}L_{1}C_{2}]e^{ 48\hat{T}KL_{1}}.\]

Finally, using Lyapunov's inequality we obtain, for \(C=\sqrt{C_{0}}\)

\[\mathbb{E}[|\mathbf{y}_{T}-\mathbf{x}_{T}|]\leqslant\left(\mathbb{E}[| \mathbf{y}_{T}-\mathbf{x}_{T}|^{2}]\right)^{1/2}\leqslant Ch.\]

In other words, for \(C\) as stated above, we have the following inequality

\[\sqrt{\mathbb{E}\left[\sup_{t_{0}\leqslant t\leqslant t_{M}}|\widehat{\mathbf{ x}}_{t}-\mathbf{x}_{t}|^{2}\right]}\leqslant Ch,\qquad\text{as $h\longrightarrow 0$}.\]

_Remark C.9_.: From the above, it is easy to induce that the following order for the one step error

\[\mathbb{E}[|\mathbf{x}(t_{1})-\mathbf{y}_{1}|^{2}]=\mathcal{O}(h^{3}).\]

Now, since \(G_{t}g(t)=0\) in this case (additive noise), it is easy to see from the truncated Ito-Taylor expansion of \(\mathbf{x}\) that \(|\mathbb{E}[\mathbf{x}(t_{1})-\mathbf{y}_{1}]|=\mathcal{O}(h^{2})\). As such, we apply Theorem C.4 to conclude that SEEDS-1 has strong convergence of global order 1.0.

#### c.2.4 Discrete-time approximation

By Theorem 4.1, we know that SEEDS-1, being of strong order 1.0, it is immediately also of weak order 1. Nonetheless, let's give a discrete approach of this statement that we will use for the proofs of convergence for the remaining solvers as stated in Corollary 4.4. Define the discrete time process:

\[\mathbf{y}_{t_{0}}\leftarrow\mathbf{x}_{T},\mathbf{y}_{t_{i}}\leftarrow\frac {\alpha_{t_{i}}}{\alpha_{t_{i-1}}}\mathbf{y}_{t_{i-1}}-2\sigma_{t_{i}}(e^{h_{i} }-1)\boldsymbol{\epsilon}(\mathbf{y}_{t_{i-1}},t_{i-1})-\sqrt{2}\alpha_{t_{i}} \int_{\lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-s}\mathrm{d}\widehat{\omega}(s).\]

We will prove that \(\mathbb{E}[|\mathbf{y}_{t_{M}}-\mathbf{x}_{t_{M}}|]\) is of order \(h\) as \(h\longrightarrow 0\). Note that \((\mathbf{y}_{t_{i}})_{i}\) has the same distribution as \((\widehat{\mathbf{x}}_{t_{i}})_{i}\) described in Algorithm 2 since the stochastic integrals \(\left(\int_{\lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-s}\mathrm{d}\widehat{ \omega}(s)\right)_{i}\) are independent and each \(\int_{\lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-s}\mathrm{d}\widehat{\omega}(s)\) is distributed as \(\frac{1}{\sqrt{2}}\frac{\sigma_{t_{i}}}{\alpha_{t_{i}}}\sqrt{e^{2h_{i}}-1}\epsilon\) with \(h_{i}=\lambda_{t_{i}}-\lambda_{t_{i-1}},\quad\epsilon\sim\mathcal{N}( \mathbf{0},\mathbf{I}_{d})\). We have:

\[\mathbf{y}_{t_{i}}-\mathbf{x}_{t_{i}}=\frac{\alpha_{t_{i}}}{\alpha_{t_{i-1}}} (\mathbf{y}_{t_{i-1}}-\mathbf{x}_{t_{i-1}})+2\alpha_{t_{i}}\int_{\lambda_{t_{i- 1}}}^{\lambda_{t_{i}}}e^{-\lambda}(\widehat{\boldsymbol{\epsilon}}(\widehat{ \mathbf{x}}_{\lambda},\lambda)-\boldsymbol{\epsilon}(\mathbf{y}_{t_{i-1}},t_{i- 1}))\mathrm{d}\lambda.\]

For simplicity, in what follows \(C\) will denote a constant not dependent on the subdivision of \([0,T]\) that may change from one line to the next by systematically denoting the maximum value of the different constants appearing in the line before. Using the inequality \(\alpha_{t}\leq 1\), the Lipschitz property of \(\epsilon\), we deduce the bound:

\[|\mathbf{y}_{t_{i}}-\mathbf{x}_{t_{i}}|\leq\frac{\alpha_{t_{i}}}{\alpha_{t_{i-1} }}|\mathbf{y}_{t_{i-1}}-\mathbf{x}_{t_{i-1}}|+C\left[\int_{\lambda_{t_{i-1}}}^{ \lambda_{t_{i}}}e^{-\lambda}|\widehat{\mathbf{x}}_{\lambda}-\mathbf{y}_{t_{i-1} }|\mathrm{d}\lambda+\int_{\lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-\lambda}|t_{ \lambda}(\lambda)-t_{i-1}|\mathrm{d}\lambda\right].\]

Notice that

\[\int_{\lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-\lambda}|\widehat{ \mathbf{x}}_{\lambda}-\mathbf{y}_{t_{i-1}}|\mathrm{d}\lambda \leq \int_{\lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-\lambda}|\widehat{ \mathbf{x}}_{\lambda}-\mathbf{x}_{t_{i-1}}|\mathrm{d}\lambda+\int_{\lambda_{t _{i-1}}}^{\lambda_{t_{i}}}e^{-\lambda}|\mathbf{x}_{t_{i-1}}-\mathbf{y}_{t_{i- 1}}|\mathrm{d}\lambda\] \[\leq \int_{\lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-\lambda}|\widehat{ \mathbf{x}}_{\lambda}-\mathbf{x}_{t_{i-1}}|\mathrm{d}\lambda+Ch|\mathbf{x}_{t_ {i-1}}-\mathbf{y}_{t_{i-1}}|,\]

and recall that \(\widehat{\mathbf{x}}_{u}=\mathbf{x}_{t_{\lambda}(u)}\). Using the fact that \(t_{\lambda}\) is increasing and Lemma C.11, we have:

\[\mathbb{E}[\int_{\lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-\lambda}|\widehat{ \mathbf{x}}_{\lambda}-\mathbf{x}_{t_{i-1}}|\mathrm{d}\lambda]\leq C\int_{ \lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-u}\sqrt{|t_{\lambda}-t_{i-1}|}\mathrm{ d}\lambda\leq Ch^{3/2}.\]

Introduce \(U_{i}=\mathbb{E}[|\mathbf{y}_{t_{i}}-\mathbf{x}_{t_{i}}|]\). Since \(\int_{\lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-\lambda}|t_{\lambda}(\lambda)-t_ {i-1}|\mathrm{d}\lambda\leq Ch^{2}\):

\[U_{i}\leq\left(\frac{\alpha_{t_{i}}}{\alpha_{t_{i-1}}}+Ch\right)U_{i-1}+Ch^{2}.\]

Let \(a_{i}=\frac{\alpha_{t_{i}}}{\alpha_{t_{i-1}}}+Ch\) and \(b_{i}=Ch^{2}\). By applying Lemma C.12, we have: \(U_{M}\leq A_{M}U_{0}+\sum_{k=1}^{M}A_{k,n}b_{k}\) with \(A_{M}=\prod_{k=1}^{M}a_{k}\) and \(A_{k,M}=A_{M}/A_{k}=\prod_{i=k+1}^{M}a_{i}\). Note that \(U_{0}=0\) since \(\mathbf{y}_{t_{0}}=\mathbf{x}_{T}\) and so:

\[U_{M}\leq Ch^{2}\sum_{k=0}^{M-1}(\sup_{1\leq i\leq M}a_{i})^{k}.\]

Using our hypothesis, we can bound:

\[\sum_{k=0}^{M-1}(\sup_{1\leq i\leq M}a_{i})^{k}\leq\sum_{k=0}^{M-1}(\exp(Ch)+ Ch)^{k}=\frac{(\exp(Ch)+Ch)^{1/h}-1}{(\exp(Ch)+Ch)-1}.\]

The quantity on the right is of order \(C/h\). Indeed, as \(h\) goes to \(0\), \(\exp(Ch)+Ch-1\) is equivalent to \(2Ch\) and \((\exp(Ch)+Ch)^{1/h}\) converges to a constant. This gives the bound \(U_{M}\leq Ch\), when \(h\to 0\) and by using Proposition C.8 and Theorem C.6 we conclude that SEEDS-1 is convergent of weak order 1.

#### c.2.5 Useful Lemmas

**Lemma C.10**.: _(Continuous Gromwall Lemma) Let \(I=[a,b]\) denote a compact interval of the real line with \(a<b\). Let \(\alpha,\beta,u\) be continuous real-valued functions defined on \(I\). Assume \(\beta\) is non-negative, \(\alpha\) is non-decreasing and \(u\) satisfies the integral inequality_

\[u(t)\leq\alpha(t)+\int_{a}^{t}\beta(s)u(s)\,\mathrm{d}s,\qquad\forall t\in I.\]

_Then_

\[u(t)\leq\alpha(t)\exp\left(\int_{a}^{t}\beta(s)\,\mathrm{d}s\right),\qquad \forall t\in I.\]

**Lemma C.11**.: _Assume the following forward SDE is satisfied \(dX_{t}=F(t,X_{t})dt+G(t)dW_{t},t\in[0,T]\), where \(T>0\), \(F(t,x)\) is Lipschitz with respect to \((t,x)\), \(G\) is continuous and \(X_{0}\) is an integrable random variable. Then, there exists \(C>0\) such that for all \(s<t\in[0,T]\) with \(t-s\leq 1\),_

\[\mathbb{E}[|X_{t}-X_{s}|]\leq C\sqrt{t-s}.\]Proof.: We take \(s=0\) and apply the triangular inequality:

\[|X_{t}-X_{0}|\leq\int_{0}^{t}|F(u,X_{u})-F(0,X_{0}))|\mathrm{d}u+|\int_{0}^{t}G(u )dW_{u}|+t|F(0,X_{0})|.\]

Setting \(u(t)=\mathbb{E}[|X_{t}-X_{0}|]\) and taking the expectation, we deduce:

\[u(t)\leq K\frac{t^{2}}{2}+t\mathbb{E}[|F(0,X_{0})|]+\mathbb{E}[(\int_{0}^{t}G( u)dW_{u})^{2}]^{\frac{1}{2}}+K\int_{0}^{t}u(s)ds,\]

where \(K\) is a positive constant. Note that \(\mathbb{E}[(\int_{0}^{t}G(u)dW_{u})^{2}]^{\frac{1}{2}}=[\int_{0}^{t}G^{2}(s)ds] ^{\frac{1}{2}}\) by the Ito isometry property which is less than \(C\sqrt{t}\) since \(G\) is continuous. Thus, we have proved that:

\[u(t)\leq\alpha(t)+K\int_{0}^{t}u(s)ds,\]

where \(\alpha(t)=K\frac{t^{2}}{2}+t\mathbb{E}[|F(0,X_{0})|]+C\sqrt{t}\) is non-decreasing. By Lemma C.10: \(u(t)\leq\alpha(t)\exp(Kt)\leq\alpha(t)\exp(KT)\). Since \(\alpha(t)\leq C\sqrt{t}\) for \(t\in[0,1]\), the lemma holds. 

**Lemma C.12**.: _(Discrete Gronwall Lemma) Consider a real number sequence \((u_{n})_{n}\) such that_

\[u_{n+1}\leq a_{n+1}u_{n}+b_{n+1},n\geq 0\]

_where \((a_{n})\) and \((b_{n})_{n}\) are two given sequences such that \(b_{n}\) is positive. Then_

\[u_{n}\leq A_{n}u_{0}+\sum_{k=1}^{n}A_{k,n}b_{k},\]

_where \(A_{n}=\prod_{k=1}^{n}a_{k}\) and \(A_{k,n}=A_{n}/A_{k}=\prod_{i=k+1}^{n}a_{i}\)._

### Proof of Proposition 4.3

Let us prove this statement for SEEDS-2. On the one hand, we have

\[\mathbf{u}_{1}=\frac{\alpha_{s_{1}}}{\alpha_{s}}\tilde{\mathbf{x}}_{s}-2 \bar{\sigma}_{s_{1}}\left(e^{\frac{h}{2}}-1\right)F_{\theta}(\tilde{\mathbf{ x}}_{s},s)-B^{1},\qquad B^{1}:=\sqrt{2}\alpha_{s_{1}}\int_{\lambda_{s}}^{ \lambda_{s_{1}}}e^{-\lambda}\mathrm{d}\omega_{\lambda}.\]

Now \(B^{1}\) depends on the Brownian movement \(\left(\omega_{\lambda_{s_{1}}}-\omega_{\lambda_{s}}\right)_{\lambda_{s_{1}} \geqslant\lambda_{s}}\). By the Markov property, this is independent of \((\omega_{\lambda_{u}})_{\lambda_{u}\leqslant\lambda_{s}}\). Since \(\tilde{\mathbf{x}}_{s}\) is a function of \((\omega_{\lambda_{u}})_{\lambda_{u}\leqslant\lambda_{s}}\), we deduce that \(B^{1}\) has to be independent of all \((\tilde{\mathbf{x}}_{u})_{u\geqslant s}\). By Ito Isometry, we obtain

\[B^{1}=\bar{\sigma}_{s_{1}}\sqrt{e^{h}-1}z^{1},\quad z^{1}\sim\mathcal{N}( \mathbf{0},\mathbf{I}_{d}).\]

On the other hand, the update \(\tilde{\mathbf{x}}_{t}\) is

\[\tilde{\mathbf{x}}_{t}=\frac{\alpha_{t}}{\alpha_{s}}\tilde{\mathbf{x}}_{s}-2 \bar{\sigma}_{t}(e^{h}-1)F_{\theta}(\mathbf{u}_{1},s_{1})-B^{0},\qquad B^{0}= \sqrt{2}\alpha_{t}\int_{\lambda_{s}}^{\lambda_{t}}e^{-\lambda}\mathrm{d} \omega_{\lambda}.\]

Hence, the Wiener process \(W=\{\omega_{\lambda}:\lambda\in[\lambda_{s},\lambda_{t}]\}\) is predetermined on the interval \([\lambda_{s},\lambda_{s_{1}}]\). Then, by using independent increments property of Wiener process \(W\), we can deduce, for \(0\leq\lambda_{s}<\lambda_{s_{1}}<\lambda_{t}\), that

\[\omega_{\lambda_{s_{1}}}-\omega_{\lambda_{s}}\quad\text{and}\quad\omega_{ \lambda_{t}}-\omega_{\lambda_{s_{1}}}\text{ are independent}.\]

Then, by the above Brownian independence property, the random variables \(B^{0}\) and \(B^{1}\) are

* Independent on non-overlapping time intervals* Dependent on overlapping intervals.

By the Chasles rule, we then decompose

\[B^{0}=\sqrt{2}\alpha_{t}\int_{\lambda_{s}}^{\lambda_{s_{1}}}e^{-\lambda}\mathrm{d }\omega_{\lambda}+\sqrt{2}\alpha_{t}\int_{\lambda_{s_{1}}}^{\lambda_{t}}e^{- \lambda}\mathrm{d}\omega_{\lambda}\]

and, as

\[\int_{\lambda_{s}}^{\lambda_{s_{1}}}e^{-\lambda}\mathrm{d}\omega_{\lambda}= \frac{B^{1}}{\sqrt{2}\alpha_{s_{1}}},\]

we obtain

\[B^{0}=\sqrt{2}\alpha_{t}\left[\frac{B^{1}}{\sqrt{2}\alpha_{s_{1}}}+\int_{ \lambda_{s_{1}}}^{\lambda_{t}}e^{-\lambda}\mathrm{d}\omega_{\lambda}\right].\]

Then we have

\[B = \sqrt{2}\alpha_{t}\int_{\lambda_{s}}^{\lambda_{s_{1}}}e^{-\lambda} \mathrm{d}\omega_{\lambda}+\sqrt{2}\alpha_{t}\int_{\lambda_{s}}^{\lambda_{t}}e ^{-\lambda}\mathrm{d}\omega_{\lambda}\] \[= \sqrt{2}\alpha_{t}\frac{1}{\sqrt{2}}\sigma_{s_{1}}\sqrt{e^{2( \lambda_{s_{1}}-\lambda_{s})}-1}z^{1}+\sqrt{2}\alpha_{t}\frac{1}{\sqrt{2}} \sigma_{t}\sqrt{e^{2(\lambda_{t}-\lambda_{s})}-1}z^{2}\] \[= \alpha_{t}\sigma_{s_{1}}\sqrt{e^{h}-1}z^{1}+\alpha_{t}\sigma_{t} \sqrt{e^{2h}-1}z^{2}\] \[= \bar{\sigma}_{t}\left(\frac{\sigma_{s_{1}}}{\sigma_{t}}\sqrt{e^{ h}-1}z^{1}+\sqrt{e^{2h}-1}z^{2}\right)\] \[= \bar{\sigma}_{t}\left(e^{\lambda_{t}-\lambda_{s_{1}}}\sqrt{e^{h}- 1}z^{1}+\sqrt{e^{2h}-1}z^{2}\right)\] \[= \bar{\sigma}_{t}\left(e^{\frac{1}{2}}\sqrt{e^{h}-1}z^{1}+\sqrt{e ^{2h}-1}z^{2}\right)\] \[= \bar{\sigma}_{t}\left(\sqrt{e^{2h}-e^{h}}z^{1}+\sqrt{e^{2h}-1}z^{ 2}\right),\]

which completes the proof.

The proof for SEEDS-3 is straightforward from the proof for SEEDS-2.

### Proof of Corollary 4.4

#### c.4.1 Convergence of SEEDS-2:

Let \(\{\mathbf{y}_{t_{i}}\}_{i}\) be the discrete stochastic process defined as follows:

\[\mathbf{y}_{t_{0}}\leftarrow\mathbf{x}_{T},\mathbf{y}_{t_{i}}\leftarrow\frac{ \alpha_{t_{i}}}{\alpha_{t_{i-1}}}\mathbf{y}_{t_{i-1}}-2\alpha_{t_{i}}\int_{ \lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-u}\boldsymbol{\epsilon}(\boldsymbol{u }_{i},s_{i})\mathrm{d}u-\sqrt{2}\alpha_{t_{i}}\int_{\lambda_{t_{i-1}}}^{ \lambda_{t_{i}}}e^{-s}\mathrm{d}\bar{\boldsymbol{\omega}}(s),\]

with \(s_{i}\gets t_{\lambda}\left(\lambda_{t_{i-1}}+\frac{h_{i}}{2}\right)\) and

\[\boldsymbol{u}_{i}\leftarrow\frac{\alpha_{s_{i}}}{\alpha_{t_{i-1}}}\mathbf{y }_{t_{i-1}}-2\sigma_{s_{i}}\left(e^{\frac{h_{i}}{2}}-1\right)\boldsymbol{ \epsilon}(\mathbf{y}_{t_{i-1}},t_{i-1})-\sqrt{2}\alpha_{s_{i}}\int_{\lambda_{ t_{i-1}}}^{\lambda_{s_{i}}}e^{-s}\mathrm{d}\bar{\boldsymbol{\omega}}(s),\]

Then \(\{\mathbf{y}_{t_{i}}\}_{i}\) has the same distribution as \(\{\widetilde{\mathbf{x}}_{t_{i}}\}_{i}\) in Algorithm 3. We can compute the difference:

\[\mathbf{y}_{t_{i}}-\mathbf{x}_{t_{i}}=\frac{\alpha_{t_{i}}}{\alpha_{t_{i-1}}}( \mathbf{y}_{t_{i-1}}-\mathbf{x}_{t_{i-1}})+\Gamma;\Gamma=\Gamma_{1}+\Gamma_{2}\]

and:

\[\Gamma_{1} = -2\alpha_{t_{i}}\int_{\lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-u} \left[\boldsymbol{\epsilon}(\boldsymbol{u}_{i},s_{i})-\boldsymbol{\epsilon}(x _{s_{i}},s_{i})\right]\mathrm{d}u\] \[\Gamma_{2} = -2\alpha_{t_{i}}\int_{\lambda_{t_{i-1}}}^{\lambda_{t_{i}}}e^{-u} \left[\boldsymbol{\epsilon}(x_{s_{i}},s_{i})-\boldsymbol{\widehat{\epsilon}}( \widehat{\mathbf{x}}_{u},u)\right]\mathrm{d}u\]

[MISSING_PAGE_FAIL:37]

[MISSING_PAGE_FAIL:38]

for small values of \(h\). We have for values \(r_{1}=1/3\), \(r_{2}=2/3\), the following identity

\[\sqrt{e^{2h_{i}}-e^{h_{i}}z_{i}}+\sqrt{e^{h_{i}}-1}v_{i} = \sqrt{e^{h_{i}}-1}\left(\sqrt{e^{h_{i}}}z_{i}+v_{i}\right)\] \[= \sqrt{e^{h_{i}}-1}\left(e^{\frac{h_{i}}{2}}z_{i}+v_{i}\right)\] \[= \sqrt{\mathtt{expr1}(h_{i})}\left(\left(\mathtt{expr1}\left( \frac{h_{i}}{2}\right)+1\right)z_{i}+v_{i}\right)\] \[= \sqrt{\mathtt{expr1}(h_{i})}\left(\mathtt{expr1}\left(\frac{h_{ i}}{2}\right)+1\right)z_{i}+\sqrt{\mathtt{expr1}(h_{i})}v_{i}.\]

Now by using

\[e^{2(r_{2}-r_{1})h_{i}} = e^{2\left(\frac{2}{3}-\frac{1}{3}\right)h_{i}}=e^{2\frac{1}{3}h _{i}}=e^{r_{2}h_{i}},\]

we now compute

\[\sqrt{e^{2r_{2}h_{i}}-e^{r_{2}h_{i}}}z_{i}^{1}+\sqrt{e^{r_{2}h_{ i}}-1}z_{i}^{2} = \sqrt{e^{r_{2}h_{i}}-1}\left(\sqrt{e^{r_{2}h_{i}}}z_{i}^{1}+z_{i }^{2}\right)\] \[= \sqrt{e^{r_{2}h_{i}}-1}\left(e^{\frac{h_{i}}{2}}z_{i}^{1}+z_{i}^{ 2}\right)\] \[= \sqrt{\mathtt{expr1}(r_{2}h_{i})}((\mathtt{expr1}(r_{1}h_{i})+1) z_{i}^{1}+z_{i}^{2})\]

\[\sqrt{e^{2h_{i}}-e^{2r_{2}h_{i}}}z_{i}^{1} = \sqrt{e^{3r_{2}h_{i}}-e^{2r_{2}h_{i}}}z_{i}^{1}\] \[= e^{r_{2}h_{i}}\sqrt{e^{r_{2}h_{i}}-1}z_{i}^{1}\] \[= \sqrt{\mathtt{expr1}(r_{2}h_{i})}(\mathtt{expr1}(r_{2}h_{i})+1) z_{i}^{1}.\]

Finally, we obtain

\[\sqrt{e^{2h_{i}}-e^{2r_{2}h_{i}}}z_{i}^{1}+\sqrt{e^{2r_{2}h_{i}}- e^{r_{2}h_{i}}}z_{i}^{2}+\sqrt{e^{r_{2}h_{i}}-1}z_{i}^{3}\] \[= \sqrt{\mathtt{expr1}(r_{2}h_{i})}((\mathtt{expr1}(r_{2}h_{i})+1 )z_{i}^{1}+(\mathtt{expr1}(r_{1}h_{i})+1)z_{i}^{2}+z_{i}^{3}).\]

### Noise schedules parameterizations

The inverses of \(\lambda(t)\) are given in the (VP) linear and cosine schedules respectively by

\[t_{\lambda}(\lambda) = \frac{2\log(e^{-2\lambda}+1)}{\sqrt{\beta_{\min}^{2}+2(\beta_{ \max}-\beta_{\min})\log(e^{-2\lambda}+1)}+\beta_{\min}},\] \[t_{\lambda}(\lambda) = \frac{2(1+s)}{\pi}\arccos\left(\text{exp}\Big{(}-\frac{1}{2}\log( e^{-2\lambda}+1)+\log\cos\big{(}\frac{\pi s}{2(1+s)}\big{)}\Big{)}\right)-s.\]

In EDM Noise Prediction case, the inverses of \(\lambda(t)\), as given in (37), with respect to the NPFO and NRSDE are respectively given by

\[t_{\lambda}(\lambda)=\sigma_{d}\tan(e^{-\lambda})\quad\text{and}\quad t_{ \lambda}(\lambda)=\frac{\sigma_{d}}{\sqrt{\frac{1}{\sigma_{d}^{2}e^{-2\lambda }}-1}}.\]

### EDM discretization

We follow Karras et al. [16] to implement the EDM discretization timesteps \(\{t_{i}\}_{i=0}^{M}\) as \(t_{i}=\sigma^{-1}(\sigma_{i})\) such that, for \(\rho>0\),

\[\sigma_{i<M}=\left(\sigma_{\text{max}}^{\frac{1}{\rho}}+\frac{i}{N-1}\left( \sigma_{\text{min}}^{\frac{1}{\rho}}-\sigma_{\text{max}}^{\frac{1}{\rho}} \right)\right)^{\rho}\quad\text{and}\quad\sigma_{N}=0.\]From the definition, we note that \(\sigma_{0}=\sigma_{\text{max}}\) and \(\sigma_{M-1}=\sigma_{\text{min}}\), where \(\sigma_{\text{min}}\) and \(\sigma_{\text{max}}\) denote the minimum and maximum noise magnitude, respectively. We also keep default value \(\rho=7\) as in [16]. However, we figured out that when using EDM discretization with linear schedule, the noise schedule improvement in iDDPM pre-trained models [26] would result in two consecutive time steps of the same value, i.e. \(t_{j}=t_{j+1}\) for some index \(j=1,\ldots,M-1\) and large steps (\(M\geq 61\)). Thus, for SEEDS-3 and DPM-Solver-3 [22], the usage of the function \(\varphi_{2}(h)=\dfrac{e^{h}-h-1}{h^{2}}\) (see Appendix E for more details) will cause zero division error where \(h=\lambda_{t_{j+1}}-\lambda_{t_{j}}=0\). Therefore, in our implementation, we ignore the noise schedule improvement of iDDPM [26] models when using solvers of order three.

### Final sampling step

The sampling phase in DPMs using SEEDS follows the RSDE, which requires gradual computing through discretization time steps \(\{t_{i}\}_{i=0}^{M}\) and the latter goes from \(t_{0}=T\) to \(t_{M}=0\). In our implementation, to avoid the logarithm of zero error at the last step, i.e. \(\log\sigma_{t_{M}}=\log(0)\), we stop the sampling phase at step \((M-1)\). Hence, the NFE used in a run will be given as

\[\text{NFE}=k\times(M-1),\]

where \(k\) represents the order of the solver. We also do not use the "denoising" trick, i.e., ignoring the random noise at the last step and leave it to further research.

## Appendix E Reminders on Stochastic Exponential Integrators

Let us consider a SDE of the form

\[\mathrm{d}\mathbf{x}(t)=[a(t)\mathbf{x}(t)+c(t)f(\mathbf{x}(t),t)]\mathrm{d}t +g(t)\mathrm{d}\boldsymbol{\omega}(t),\] (50)

where \(a,c:[0,T]\to\mathbb{R}\) and \(g:[0,T]\to\mathbb{R}^{d\times d}\). In other words we concentrate to high-dimensional semi-linear non autonomous SDEs with additive noise. The objective of this section is to construct explicit stochastic exponential derivative-free methods for the above equation following the Runge-Kutta (RK) approach. These methods ideally should fulfill the following properties:

1. If \(f\equiv 0\), then (50) can be solved _exactly_;
2. If \(g\equiv 0\), then a SEEDS method for (50) identifies with an exponential RK (ERK) method;
3. If \(a\equiv 0\), then a SEEDS method for (50) identifies with a stochastic RK (SRK) method and if moreover \(g\equiv 0\) then it identifies with a classical RK ODE method.

Before tackling the aimed SEEDS problem let us rapidly recall elementary constructions of RK, ERK, weak and strong SRK methods. We will not deal with time-adaptive methods here.

### Derivative-free exponential ODE schemes

#### e.1.1 Runge-Kutta approach

Derivative-free schemes are obtained by comparing the Ito-Taylor expansion of the above paragraph with expressions of \(\mathbf{x}(t)\) in terms of its intermediate evaluations between \(s\) and \(t=s+h\) and the Taylor expansions of such evaluations. As a simple example, in the ODE regime

\[\mathrm{d}\mathbf{x}(t)=f(\mathbf{x}(t),t)\mathrm{d}t.\] (51)

As such, analytic solutions to the above equations are of the form

\[\mathbf{x}(t) = \mathbf{x}(s)+\int_{s}^{t}f(\mathbf{x}(\tau),\tau)\mathrm{d}\tau.\]

[MISSING_PAGE_FAIL:41]

which satisfies \(\Phi^{\prime}(t;s)=f(t)\Phi(t;s)\) and \(\Phi(s;s)=1\). Then the exact solution for (52) is given, via the variation of constants formula, by

\[{\bf x}(t)=\Phi(t;s)\left({\bf x}(s)+\int_{s}^{t}\Phi^{-1}(\tau;s)g({\bf x}(\tau ),\tau){\rm d}\tau\right)=\Phi(t;s){\bf x}(s)+\int_{s}^{t}\Phi(t;\tau)g({\bf x} (\tau),\tau){\rm d}\tau.\]

In light of this integral form, one can formalize a general class of

exponential \(n\)-stage RK methods

\[{\bf x}_{i} = \gamma_{i}(h,f_{s}){\bf x}(s)+\sum_{j=1}^{n}a_{ij}(h,f_{s})g({\bf x }_{j},s+c_{j}h)\] \[{\bf x}(t) = \gamma_{0}(h,f_{s}){\bf x}(s)+\sum_{i=1}^{n}b_{i}(h,f_{s})g({\bf x }_{i},s+c_{i}h),\]

where \(\gamma_{0},\gamma_{i},a_{ij},b_{j}\) are functions of the step-size \(h\), \(f\)and \(f_{s}(r):=\int_{0}^{\tau}f(s+\tau){\rm d}\tau\). There are two possible approaches to create exponential integrators, namely the exponential time-differencing (ETD) approach that uses the variation of constants formula and makes use of the \(\varphi\) functions, and the Lawson approach, also know as integrating factor (IF), which makes a change of variables on the above SDE thus avoiding the use of the \(\varphi\) functions but computing exponential factors step-wise.

**The Lawson and the ETD approaches for Exponential Euler schemes**

There are two choices one can make when computing first order approximations of

\[{\bf x}(t) = \Phi(t;s){\bf x}(s)+\int_{s}^{t}\Phi(t;\tau)g({\bf x}(\tau),\tau) {\rm d}\tau.\]

First, by interpolating \(g({\bf x}(\tau),\tau)\) as \(g({\bf x}(s),s)\) we obtain

\[{\bf x}(t) = \Phi(t;s){\bf x}(s)+g({\bf x}(s),s)\int_{s}^{t}\Phi(t;\tau){\rm d}\tau.\]

Now two choices remain. Either \(\int_{s}^{t}\Phi(t;\tau){\rm d}\tau\) is computed exactly or we again interpolate \(\Phi(t;\tau)\) as \(\Phi(t;s)\). Taking for simplicity \(f\equiv A\) to be constant and by denoting \(h=t-s\), the first case yields the ETD Euler method

\[\widehat{{\bf x}}(t) = \Phi(t;s)\widehat{{\bf x}}(s)+h\gamma_{1}(f,t,s)g(\widehat{{\bf x }}(s),s)\] \[= e^{Ah}\widehat{{\bf x}}(s)+h\varphi_{1}(Ah)g(\widehat{{\bf x}}( s),s)\] \[= \widehat{{\bf x}}(s)+h\varphi_{1}(Ah)[g(\widehat{{\bf x}}(s),s)- \widehat{{\bf x}}(s)],\]

and the second yields the IF Euler (also called Lawson-Euler) method

\[\widehat{{\bf x}}(t) = \Phi(t;s)\widehat{{\bf x}}(s)+h\Phi(t;s)g(\widehat{{\bf x}}(s),s)\] \[= e^{Ah}(\widehat{{\bf x}}(s)+hg(\widehat{{\bf x}}(s),s)).\]

Now, consider a solution

\[{\bf x}(t) = \Phi(t;s){\bf x}(s)+\int_{0}^{h}\Phi(h;\tau)g({\bf x}(s+\tau),s+ \tau){\rm d}\tau.\]

Exponential methods then aim to approximate the term \(g({\bf x}(s+\tau),s+\tau)\) by its interpolation polynomial in certain non-confluent quadrature nodes \(c_{1},\ldots,c_{n}\).

**The ETD approach**

In this case, the variation of constants formula yields

\[{\bf x}(s+ch) = e^{\int_{s}^{s+ch}a(\tau){\rm d}\tau}{\bf x}(s)+\int_{s}^{s+ch}e ^{\int_{\tau}^{s+ch}a(\tau){\rm d}\tau}f({\bf x}(\tau),\tau){\rm d}\tau.\] \[= e^{\int_{0}^{ch}a(s+\tau){\rm d}\tau}{\bf x}(t)+\int_{0}^{ch}e ^{\int_{\tau_{-s}}^{ch}a(s+\tau){\rm d}\tau}f({\bf x}(s+\tau),s+\tau){\rm d}\tau.\]Now, as before, the Taylor expansion of \(f\) yields

\[f(\mathbf{x}(s+\tau),s+\tau) = \sum_{j=1}^{q}\frac{\tau^{j-1}}{(j-1)!}f^{(j-1)}(\mathbf{x}(s),s)\] \[+\int_{0}^{\tau}\frac{(\tau-\tau_{1})^{q-1}}{(q-1)!}f^{(q)}( \mathbf{x}(s+\tau_{1}),s+\tau_{1})\mathrm{d}\tau_{1}.\]

Recall that the \(\varphi\) functions are given in an integral form as follows

\[\varphi_{k+1}(t)=\int_{0}^{1}e^{(1-\delta)t}\frac{\delta^{k}}{k!}\mathrm{d}\delta,\]

which satisfy \(\varphi_{k}(0)=\frac{1}{k!}\). Now denote

\[\varphi_{j}(t,a) := \frac{1}{t^{j}}\int_{0}^{t}e^{\int_{\tau}^{t}a(r)\mathrm{d}r}\frac {\tau^{j-1}}{(j-1)!}\mathrm{d}\tau,\qquad j\geqslant 1.\] \[\varphi_{1}(h,a) = \frac{1}{h}\int_{0}^{h}e^{\int_{\tau}^{h}a(r)\mathrm{d}r}\mathrm{ d}\tau=\int_{0}^{1}e^{h\int_{\sigma}^{1}a(r)\mathrm{d}r}\mathrm{d}\theta.\]

The exact solution at \(s+ch\) now reads

\[\mathbf{x}(s+ch) = e^{\int_{0}^{ch}a(s+\tau)\mathrm{d}\tau}\mathbf{x}(s)+\sum_{j=1} ^{q}(ch)^{j}\varphi_{j}(ch,a)f^{(j-1)}(\mathbf{x}(s),s)\] \[+\int_{0}^{ch}e^{\int_{\tau}^{ch}a(s+r)\mathrm{d}r}\left(\int_{0} ^{\tau}\frac{(\tau-\tau_{1})^{q-1}}{(q-1)!}f^{(q)}(\mathbf{x}(s+\tau_{1}),s+ \tau_{1})\mathrm{d}s\right)\mathrm{d}\tau_{1}.\]

Using the left endpoint rule yields

\[\mathbf{x}(t) = e^{\int_{0}^{h}a(s+\tau)\mathrm{d}\tau}\mathbf{x}(s)+f(\mathbf{ x}(s),s)\int_{0}^{h}e^{\int_{\tau}^{h}a(s+r)\mathrm{d}r}\mathrm{d}\tau+\mathcal{O}(h ^{2})\] \[= e^{\int_{0}^{h}a(s+\tau)\mathrm{d}\tau}\mathbf{x}(s)+h\varphi_{1 }(h,a)f(\mathbf{x}(s),s)+\mathcal{O}(h^{2}).\]

**Second order examples:**

The condition \(b_{2}(z)c_{2}=\varphi_{2}(z)\) implies \(b_{2}=\varphi_{2}(z)/c_{2}\) and we obtain

\[\widehat{\mathbf{x}} = e^{chA}\mathbf{x}(s)+ch\varphi_{1}(chA)f(\mathbf{x}(s),s)\] \[\mathbf{x}(t) = e^{hA}\mathbf{x}(s)+h\left(\varphi_{1}(hA)-\frac{1}{c}\varphi_{2 }(hA)\right)f(\mathbf{x}(s),s)+\frac{h}{c}\varphi_{2}(hA)f(\widehat{\mathbf{x} },s+ch).\] (53)

A second method is obtained by weakening the above condition to \(b_{2}(0)c_{2}=\varphi_{2}(0)=1/2\) giving

\[\widehat{\mathbf{x}} = e^{chA}\mathbf{x}(s)+ch\varphi_{1}(chA)f(\mathbf{x}(s),s)\] \[\mathbf{x}(t) = e^{hA}\mathbf{x}(s)+h\varphi_{1}(hA)\left(1-\frac{1}{2c}\right)f (\mathbf{x}(s),s)+\frac{h}{2c}\varphi_{1}(hA)f(\widehat{\mathbf{x}},s+ch).\] (54)

In some cases this method can suffer from order reduction and not reach order 2 of convergence. Moreover, setting \(c=1/2\) gives the exponential midpoint method:

\[\widehat{\mathbf{x}} = e^{chA}\mathbf{x}(s)+ch\varphi_{1}(chA)f(\mathbf{x}(s),s)\] \[\mathbf{x}(t) = e^{hA}\mathbf{x}(s)+h\varphi_{1}(hA)f(\widehat{\mathbf{x}},s+ ch).\] (55)

The above order 1 and 2 exponential methods are represented by the following _exponential_ Butcher tableaux

\begin{tabular}{c|c c|c c c|c c|c c} \(0\) & \(0\) & \(0\) & & & & & \(0\) & & & \\ \cline{2-11} \cline{3-11} \(0\) & \(\varphi_{1}\) & \(c_{2}\) & \(c_{2}\varphi_{1,2}\) & & & & \(c_{2}\varphi_{1,2}\) & & & \(c_{2}\) & \(c_{2}\varphi_{1,2}\) \\ \cline{2-11} \cline{3-11} \(0\) & \(\varphi_{1}\) & \(\varphi_{1}-\frac{1}{c_{2}}\varphi_{2}\) & \(\frac{1}{c_{2}}\varphi_{2}\) & & & & & \(0\) & & \(0\) & \\ \cline{2-11} \end{tabular}

We check that when formally setting \(A=0\) then the first two methods are identical and give the generic 2nd order RK method, the choice \(c=1\) gives the Heun method and the choice \(c=1/2\) gives the midpoint method.

#### Fourth order methods

It can be shown that ERK methods need at least 5 stages to achieve order 4. By setting formally \(A=0\) these methods do not have a non-exponential counterpart to our knowledge.

**5-stage sequential** We have a fourth-order ERK scheme given by

\[\begin{array}{c|ccc}0&&\\ \frac{1}{2}&\frac{1}{2}\varphi_{1,2}&\\ \frac{1}{2}&\frac{1}{2}\varphi_{1,3}-\varphi_{2,3}&\varphi_{2,3}&\\ 1&\varphi_{1,4}-2\varphi_{2,4}&\varphi_{2,4}&\\ \frac{1}{2}&\frac{1}{2}\varphi_{1,5}-2a_{5,2}-a_{5,4}&a_{5,2}&\frac{a_{5,2}}{ 4}\varphi_{2,5}-a_{5,2}&\\ \hline&\varphi_{1}-3\varphi_{2}+4\varphi_{3}&0&0&-\varphi_{2}+4\varphi_{3}&4 \varphi_{2}-8\varphi_{3}\end{array}\]

with

\[a_{5,2}=c_{5}\varphi_{2,5}-\varphi_{3,4}+c_{5}^{2}\varphi_{2,4}-c_{5}\varphi_{ 3,5}.\]

This can also be represented by Rosenbrock-like Butcher tableau

\[\begin{array}{c|ccc}0&&\\ \frac{1}{2}&\frac{1}{2}\varphi_{1,2}&\\ \frac{1}{2}&\frac{1}{2}\varphi_{1,3}&\varphi_{2,3}&\\ 1&\varphi_{1}&\varphi_{2}&\varphi_{2}&\\ \frac{1}{2}&\frac{1}{2}\varphi_{1,5}&a_{5,2}&a_{5,2}&\frac{1}{4}\varphi_{2,5}- a_{5,2}&\\ \hline&\varphi_{1}&0&0&4\varphi_{3}-\varphi_{2}&4\varphi_{2}-8\varphi_{3}\end{array}\]

with

\[a_{5,2}=c_{5}\varphi_{2,5}-c_{3}\varphi_{3,3}+c_{5}^{2}\varphi_{2}-\varphi_{3}.\]

This traduces into (recall that \(D_{i}=f(\mathbf{x}_{i},s+c_{i}h)-f(\mathbf{x}(s),s)\))

\[\mathbf{x}(t) = \mathbf{x}(s)+h(\varphi_{1}(hA))F(\mathbf{x}(s),s)+h(4\varphi_{3 }(hA)-\varphi_{2}(hA))D_{4}\] \[+h(4\varphi_{2}(hA)-8\varphi_{3}(hA))D_{5}\] \[\mathbf{x}_{2} = \mathbf{x}(s)+c_{2}h\varphi_{1}(c_{2}hA)F(\mathbf{x}(s),s)\] \[\mathbf{x}_{3} = \mathbf{x}(s)+hc_{3}\varphi_{1}(c_{3}hA)F(\mathbf{x}(s),s)+h \varphi_{2}(c_{3}hA)D_{2}\] \[\mathbf{x}_{4} = \mathbf{x}(s)+h\varphi_{1}(hA)F(\mathbf{x}(s),s)+h\varphi_{2}(hA )(D_{2}+D_{3})\] \[\mathbf{x}_{5} = \mathbf{x}(s)+h\varphi_{1}(hA)F(\mathbf{x}(s),s)+ha_{5,2}(hA)(D_ {2}+D_{3})+h(c_{5}^{2}\varphi_{2}(c_{5}hA)-a_{5,2}(hA))D_{4}\]

Inspired by this, we deduce the following fourth order Algorithm 7 specialized for DPMs in the VP noise prediction regime.

### Derivative-free exponential SDE schemes

Let \(\mathbf{x}_{t}\) be the path at the continuous limit \(h\to 0\), and \(\{\widehat{\mathbf{x}}_{t}\}_{t_{0}}^{t_{M}}\) be the discretized numerical path, computed by a numerical scheme \(\mathcal{S}\) with \(M=1/h\) steps of length \(h>0\). Then, \(\mathcal{S}\) has

1. _strong order of convergence_\(\gamma\) if there is \(K>0\) such that \[\mathbb{E}[|\mathbf{x}_{t_{M}}-\widehat{\mathbf{x}}_{t_{M}}|]\leq Kh^{\gamma},\] (56)
2. _weak order of convergence_\(\beta\) if there is \(K>0\) and a function class \(\mathcal{K}\) such that \[|\mathbb{E}[\phi(\mathbf{x}_{t_{M}})]-\mathbb{E}[\phi(\widehat{\mathbf{x}}_{t_ {M}})]|\leq Kh^{\beta},\qquad\forall\phi(\cdot)\in\mathcal{K}.\] (57)

Strong convergence is concerned with the precision of the path, while the weak convergence is with the precision of the moments. As, for diffusion models, the center of attention is the evolution of the probability densities rather than that of the noising process of single data samples, weak convergence is enough to guarantee the well-conditioning of our numerical schemes. Moreover, when the diffusion coefficient vanishes, then both strong and weak convergence (with the choice \(\phi=\mathrm{id}\)) reduce to the usual deterministic convergence criterion for ODEs.

#### e.2.1 Strong and Weak Stochastic Runge-Kutta approach

In all what follows \(\omega\) is considered a \(d\)-dimensional Wiener process (with identity diffusion matrix).

Consider the following SDE

\[\mathrm{d}\mathbf{x}(t)=f(\mathbf{x}(t),t)\mathrm{d}t+g(t)\mathrm{d}\omega(t),\]

where \(g(t)=\hat{g}(t)\cdot\mathrm{Id}_{d}\) is considered here as a diagonal matrix with identical diagonal entries \(\hat{g}(t)\). Given an initial value independent of \(\omega\), the integral form of \(\mathbf{x}(t)\) is given by

\[\mathbf{x}(t) = \mathbf{x}(s)+\int_{s}^{t}f(\mathbf{x}(\tau),\tau)d\tau+\int_{s}^ {t}g(\tau)\mathrm{d}\omega(\tau).\]

The idea underlying stochastic numerical schemes is very similar to the one used in the deterministic approach, that is to take expansions of the terms inside the integrals based at the integral's initial value and replace the obtained derivatives that appear by interpolated approximations. A key difference here is that as we have to consider Ito-Taylor expansions, the infinitesimal operators are different but most importantly most of the stochastic iterated integrals will need to be approximated in an appropriate sense, whenever that is possible. We will develop this expansion up to triple integrals.

#### e.2.2 Truncated Ito-Taylor expansions

Applying Ito formula to \(h=f\) or \(g\) yields

\[h(\mathbf{x}(t),t) = h(\mathbf{x}(s),s)+\int_{s}^{t}g(\tau)\cdot\partial_{\mathbf{x} }h(\mathbf{x}(\tau),\tau)\mathrm{d}\omega(\tau)\] \[+\int_{s}^{t}\left(\partial_{t}h(\mathbf{x}(\tau),\tau)+f( \mathbf{x}(\tau),\tau)\cdot\partial_{\mathbf{x}}h(\mathbf{x}(\tau),\tau)+ \frac{g^{2}(\tau)}{2}\partial_{\mathbf{x}^{2}}^{2}h(\mathbf{x}(\tau),\tau) \right)\mathrm{d}\tau.\]

This allows us to define two differential operators \(L,G\) as

\[L_{t} = \partial_{t}+f(\mathbf{x}(t),t)\cdot\partial_{\mathbf{x}}+\frac{ g^{2}(t)}{2}\cdot\partial_{\mathbf{x}}^{2}\] (58) \[G_{t} = g(t)\cdot\partial_{\mathbf{x}}.\] (59)In particular \(L_{t}g(t)=\partial_{t}g(t)\) and \(G_{t}g(t)=0\). With this notation, we have

\[h({\bf x}(t),t) = h({\bf x}(s),s)+\int_{s}^{t}L_{t}h({\bf x}(\tau),\tau){\rm d}\tau+ \int_{s}^{t}G_{t}h({\bf x}(\tau),\tau){\rm d}\omega(\tau),\]

so our solution reads

\[{\bf x}(t) = {\bf x}(s)+\int_{s}^{t}f({\bf x}(\tau_{1}),\tau_{1}){\rm d}\tau_{1 }+\int_{s}^{t}g(\tau_{1}){\rm d}\omega(\tau_{1})\] \[= {\bf x}(s)+\int_{s}^{t}\left(f({\bf x}(t),t)+\int_{s}^{\tau_{1}}L _{t}f({\bf x}(\tau_{2}),\tau_{2}){\rm d}\tau_{2}+\int_{s}^{\tau_{1}}G_{t}f({\bf x }(\tau_{2}),\tau_{2})d\omega(\tau_{2})\right){\rm d}\tau_{1}\] \[+\int_{s}^{t}\left(g(t)+\int_{s}^{\tau_{1}}L_{t}g(\tau_{2}){\rm d }\tau_{2}+\int_{s}^{\tau}G_{t}g(\tau_{2}){\rm d}\omega(\tau_{2})\right){\rm d }\omega(\tau_{1})\] \[= {\bf x}(s)+f({\bf x}(s),s)h+g(t)(\omega(t)-\omega(s))+R_{1}.\]

Now \(G_{t}g(\tau_{2})=0\) and

\[g(s)+\int_{s}^{\tau_{1}}L_{t}g(\tau_{2}){\rm d}\tau_{2}=g(s)+\int_{s}^{\tau_{1 }}\partial_{t}g(\tau_{2}){\rm d}\tau_{2}=g(s)+g(\tau_{1})-g(s)=g(\tau_{1}).\]

So we have

\[\int_{s}^{t}\left(g(s)+\int_{s}^{\tau_{1}}L_{t}g(\tau_{2}){\rm d} \tau_{2}+\int_{s}^{\tau}G_{t}g(\tau_{2}){\rm d}\omega(\tau_{2})\right){\rm d} \omega(\tau_{1})=g(s)(\omega(t)-\omega(s)),\]

and now our solution reads

\[{\bf x}(t)={\bf x}(s)+f({\bf x}(s),s)h+g(s)(\omega(t)-\omega(s))+R_{1},\]

where

\[R_{1} = \int_{s}^{t}\int_{s}^{\tau_{1}}L_{t}f({\bf x}(\tau_{2}),\tau_{2}) {\rm d}\tau_{2}{\rm d}\tau_{1}+\int_{s}^{t}\int_{s}^{\tau_{1}}G_{t}f({\bf x}( \tau_{2}),\tau_{2}){\rm d}\omega(\tau_{2}){\rm d}\tau_{1}.\]

Now we have

\[L_{t}f({\bf x}(t),t)\] \[= L_{t}f({\bf x}(s),s)+\int_{s}^{t}g({\bf x}(\tau),\tau)\cdot \partial_{\bf x}L_{t}f({\bf x}(\tau),\tau){\rm d}\omega(\tau)\] \[+\int_{s}^{t}\left(\partial_{t}L_{t}f({\bf x}(\tau),\tau)+f({\bf x }(\tau),\tau)\cdot\partial_{\bf x}L_{t}f({\bf x}(\tau),\tau)+\frac{g^{2}({\bf x }(\tau),\tau)}{2}\partial_{{\bf x}^{2}}^{2}L_{t}f({\bf x}(\tau),\tau)\right){ \rm d}\tau\] \[= L_{t}f({\bf x}(s),s)+\int_{s}^{t}L_{t}^{2}f({\bf x}(\tau),\tau){ \rm d}\tau+\int_{s}^{t}G_{t}L_{t}f({\bf x}(\tau),\tau){\rm d}\omega(\tau),\]

and

\[G_{t}f({\bf x}(t),t)\] \[= G_{t}f({\bf x}(s),s)+\int_{s}^{t}g({\bf x}(\tau),\tau)\cdot \partial_{\bf x}G_{t}f({\bf x}(\tau),\tau){\rm d}\omega(\tau)+\] \[\int_{s}^{t}\left(\partial_{t}G_{t}f({\bf x}(\tau),\tau)+f({\bf x }(\tau),\tau)\cdot\partial_{\bf x}G_{t}f({\bf x}(\tau),\tau)+\frac{g^{2}({\bf x }(\tau),\tau)}{2}\partial_{{\bf x}^{2}}^{2}G_{t}f({\bf x}(\tau),\tau)\right){ \rm d}\tau\] \[= G_{t}f({\bf x}(s),s)+\int_{s}^{t}L_{t}G_{t}f({\bf x}(\tau),\tau){ \rm d}\tau+\int_{s}^{t}G_{t}^{2}f({\bf x}(\tau),\tau){\rm d}\omega(\tau).\]

Now, if we denote \({\rm d}\omega^{0}(\tau)={\rm d}\tau\), \({\rm d}\omega^{1}(\tau)={\rm d}\omega(\tau)\) and

\[I_{(i)}=\int_{s}^{t}{\rm d}\omega^{i}(\tau_{1})\qquad I_{(i,j)}=\int_{s}^{t} \int_{s}^{\tau_{1}}{\rm d}\omega^{j}(\tau_{2}){\rm d}\omega^{i}(\tau_{1}) \qquad i,j\in\{0,1\},\]applying the same procedure to \(R_{1}\) leads to

\[R_{1}\] \[= \int_{s}^{t}\int_{s}^{\tau_{1}}L_{t}f(\mathbf{x}(\tau_{2}),\tau_{2} )\mathrm{d}\tau_{2}\mathrm{d}\tau_{1}+\int_{s}^{t}\int_{s}^{\tau_{1}}G_{t}f( \mathbf{x}(\tau_{2}),\tau_{2})\mathrm{d}\omega(\tau_{2})\mathrm{d}\tau_{1}\] \[= \int_{s}^{t}\int_{s}^{\tau_{1}}\left[L_{t}f(\mathbf{x}(t),t)+\int _{s}^{\tau_{2}}L_{t}^{2}f(\mathbf{x}(\tau_{3}),\tau_{3})\mathrm{d}\tau_{3}+\int _{s}^{\tau_{2}}G_{t}L_{t}f(\mathbf{x}(\tau_{3}),\tau_{3})\mathrm{d}\omega(\tau _{3})\right]\mathrm{d}\tau_{2}\tau_{1}\] \[+ \int_{s}^{t}\int_{s}^{\tau_{1}}\left[G_{t}f(\mathbf{x}(t),t)+\int _{s}^{\tau_{2}}L_{t}G_{t}f(\mathbf{x}(\tau_{3}),\tau_{3})\mathrm{d}\tau+\int _{s}^{\tau_{2}}G_{t}^{2}f(\mathbf{x}(\tau_{3}),\tau_{3})\mathrm{d}\omega(\tau _{3})\right]\mathrm{d}\omega(\tau_{2})\mathrm{d}\tau_{1}\] \[= L_{t}f(\mathbf{x}(t),t)\int_{s}^{t}\int_{s}^{\tau_{1}}\mathrm{d }\tau_{2}\mathrm{d}\tau_{1}+G_{t}f(\mathbf{x}(t),t)\int_{s}^{t}\int_{s}^{\tau _{1}}\mathrm{d}\omega(\tau_{2})\mathrm{d}\tau_{1}\] \[+ \int_{s}^{t}\int_{s}^{\tau_{1}}\int_{s}^{\tau_{2}}L_{t}^{2}f( \mathbf{x}(\tau_{3}),\tau_{3})\mathrm{d}\tau_{3}\mathrm{d}\tau_{2}\mathrm{d} \tau_{1}+\int_{s}^{t}\int_{s}^{\tau_{1}}\int_{s}^{\tau_{2}}G_{t}L_{t}f(\mathbf{ x}(\tau_{3}),\tau_{3})\mathrm{d}\omega(\tau_{3})\mathrm{d}\tau_{2}\mathrm{d}\tau_{1}\] \[+ \int_{s}^{t}\int_{s}^{\tau_{1}}\int_{s}^{\tau_{2}}L_{t}G_{t}f( \mathbf{x}(\tau_{3}),\tau_{3})\mathrm{d}\tau_{3}\mathrm{d}\omega(\tau_{2}) \mathrm{d}\tau_{1}\] \[+ \int_{s}^{t}\int_{s}^{\tau_{1}}\int_{s}^{\tau_{2}}G_{t}^{2}f( \mathbf{x}(\tau_{3}),\tau_{3})\mathrm{d}\omega(\tau_{3})\mathrm{d}\omega(\tau _{2})\mathrm{d}\tau_{1}\] \[= L_{t}f(\mathbf{x}(s),s)I_{(0,0)}+G_{t}f(\mathbf{x}(s),s)I_{(0,1) }+L_{t}^{2}f(\mathbf{x}(s),s)I_{(0,0,0)}+G_{t}L_{t}f(\mathbf{x}(s),s)I_{(0,0,1)}\] \[+L_{t}G_{t}f(\mathbf{x}(s),s)I_{(0,1,0)}+G_{t}^{2}f(\mathbf{x}(s),s)I_{(1,1,0)}+R_{2},\]

and with \(R_{2}\) consisting on quadruple integrals. As such, our solution now reads

\[\mathbf{x}(t) = \mathbf{x}(s)+f(\mathbf{x}(s),s)h+g(s)(\omega(t)-\omega(s))+L_{t }f(\mathbf{x}(s),s)I_{(0,0)}+G_{t}f(\mathbf{x}(s),s)I_{(0,1)}\] \[+L_{t}^{2}f(\mathbf{x}(s),s)I_{(0,0,0)}+G_{t}L_{t}f(\mathbf{x}(s ),s)I_{(0,0,1)}\] \[+L_{t}G_{t}f(\mathbf{x}(s),s)I_{(0,1,0)}+G_{t}^{2}f(\mathbf{x}( s),s)I_{(1,1,0)}+R_{2}.\]

Now, in the SDE regime, one cannot get rid of the iterated Ito integral and so stochastic RK methods cannot be derived as simple extensions of their deterministic counterparts. In order to continue we now take into account the fact that the diffusion SDE has additive and diagonal noise. In this case, both the Ito and the Stratonovich SDE coincide.

**Iterated integrals**

Now, for simplicity, set \(t=0\). We then have \(I_{(0)}=h\), \(I_{(0,0)}=\int_{0}^{h}\int_{0}^{\tau_{1}}\mathrm{d}\tau_{2}\mathrm{d}\tau_{1}= \frac{h^{2}}{2}\), \(I_{(0,0,0)}=\frac{h^{3}}{6}\). Now notice that

\[I_{(1)} := \hat{w}_{h}\sim\mathcal{N}(0,h)\] \[I_{(0,1)} := \hat{z}_{h}:=\int_{0}^{h}\int_{0}^{\tau_{1}}\mathrm{d}\omega(\tau _{2})\mathrm{d}\tau_{1}=\lim_{n\to\infty}\frac{h}{n}\sum_{i=0}^{n-1}\sum_{j=1}^{ i}\epsilon_{j},\qquad\epsilon_{j}\sim\mathcal{N}\left(0,\frac{h}{n}\right).\]

Additionally, \(\hat{w}_{h}\) and \(\hat{z}_{h}\) satisfy \(\mathbb{E}[\hat{w}_{h}^{2}]=h\),

\[\mathbb{E}[(\hat{w}_{h}h-\hat{z}_{h})^{2}] = \mathbb{E}\left[\left(\int_{0}^{h}\tau\mathrm{d}\omega(\tau) \right)^{2}\right]=\frac{1}{3}h^{3}\] \[\mathbb{E}[\hat{w}_{h}\hat{z}_{h}] = \mathbb{E}\left[\hat{w}_{h}\int_{0}^{h}\tau\mathrm{d}\omega(\tau) \right]=\mathbb{E}\left[\int_{0}^{h}\tau\mathrm{d}\tau\right]=\frac{1}{2}h^{2}\] \[\mathbb{E}[\hat{z}_{h}^{2}] = \mathbb{E}[(\hat{w}_{h}h-\hat{z}_{h})^{2}-h^{2}\hat{w}_{h}^{2}+2h \hat{w}_{h}\hat{z}_{h}]=\frac{1}{3}h^{3}.\]

#### e.2.3 Integral approximations

**Weak Approximations**When crafting weak stochastic approximations to SDEs one may replace multiple Ito integrals by other random variables satisfying the corresponding moment conditions. We will denote \(\hat{I}_{\alpha}\) the approximation of \(I_{\alpha}\) for \(\alpha\) a multi-index following [17, Corollary 5.12.1]. Of course, the deterministic integrals \(I_{(0,\ldots,0)}\) need not to be approximated.

**First order approximations**

The random variable \(\hat{I}_{(1)}\) must satisfy for some constant \(K\):

\[|\mathbb{E}[\hat{I}_{(1)}]|+|\mathbb{E}[(\hat{I}_{(1)})^{3}]|+|\mathbb{E}[( \hat{I}_{(1)})^{2}-h]|\leqslant Kh^{2}\]

Two possible choices for \(\hat{I}_{(1)}\) are either \(\hat{I}_{(1)}\sim\mathcal{N}(0,h)\) or \(\hat{I}_{(1)}\) is a two-pointed distributed discrete random variable with

\[\mathbb{P}\left[\hat{I}_{(1)}=\pm\sqrt{h}\right]=\frac{1}{2}.\]

**Second order approximations**

The random variable \(\hat{I}_{(1)}\) must satisfy for some constant \(K\):

\[|\mathbb{E}[\hat{I}_{(1)}]|+|\mathbb{E}[(\hat{I}_{(1)})^{3}]|+|\mathbb{E}[( \hat{I}_{(1)})^{5}]|+|\mathbb{E}[(\hat{I}_{(1)})^{2}-h]|+|\mathbb{E}[(\hat{I} _{(1)})^{4}-3h^{2}]|\leqslant Kh^{2}.\]

Two possible choices for \(\hat{I}_{(1)}\) are either \(\hat{I}_{(1)}\sim\mathcal{N}(0,h)\) or \(\hat{I}_{(1)}\) is a three-pointed distributed random variable with

\[\mathbb{P}\left[\hat{I}_{(1)}=\pm\sqrt{3h}\right]=\frac{1}{6},\qquad\mathbb{P }[\hat{I}_{(1)}=0]=\frac{2}{3}.\]

The rest follows from the above calculations:

\[\hat{I}_{(0,1)}=\frac{1}{2}h\hat{I}_{(1)},\qquad i=0,1.\]

**Third order approximations**

One can choose \(\hat{I}_{(1)}\sim\mathcal{N}(0,h)\), \(\hat{I}_{(0,1)}\sim\mathcal{N}\left(0,\frac{1}{3}h^{3}\right)\) satisfying \(\mathbb{E}[\hat{I}_{(1)}\hat{I}_{(0,1)}]=\frac{1}{2}h^{2}\).

Then, one can deduce the following:

\[\hat{I}_{(1,0,0)}=\hat{I}_{(0,1,0)}=\hat{I}_{(0,0,1)} = \frac{1}{6}h^{2}\hat{I}_{(1)}\] \[\hat{I}_{(0,1,1)}=\hat{I}_{(1,0,1)}=\hat{I}_{(1,1,0)} = \frac{1}{6}h(\hat{I}_{(1)}^{2}-h).\]

Thus, we can write the solution weak approximation as

\[\mathbf{x}(t) = \mathbf{x}(s)+f(\mathbf{x}(s),s)h+g(s)\hat{I}_{(1)}+L_{t}f( \mathbf{x}(s),s)\frac{h^{2}}{2}+R_{2}\] (60) \[+G_{t}f(\mathbf{x}(s),s)\hat{I}_{(0,1)}+L_{t}^{2}f(\mathbf{x}(s), s)\frac{h^{3}}{6}+G_{t}L_{t}f(\mathbf{x}(s),s)\frac{1}{6}h^{2}\hat{I}_{(1)}\] \[+L_{t}G_{t}f(\mathbf{x}(s),s)\frac{1}{6}h^{2}\hat{I}_{(1)}+G_{t}^ {2}f(\mathbf{x}(s),s)\frac{1}{6}h(\hat{I}_{(1)}^{2}-h).\]

An example of such a pair \((\hat{I}_{(1)},\hat{I}_{(0,1)})=(\hat{w}_{h},\hat{z}_{h})\) can be easily obtained as follows

\[\left[\begin{array}{c}\hat{w}_{h}\\ \hat{z}_{h}\end{array}\right] = \left[\begin{array}{cc}\sqrt{h}&0\\ \frac{h\sqrt{h}}{2}&\frac{h\sqrt{h}}{2\sqrt{3}}\end{array}\right]\left[ \begin{array}{c}u_{1}\\ u_{2}\end{array}\right],\qquad u_{1},u_{2}\stackrel{{\text{iid.}}}{{ \sim}}\mathcal{N}(0,1).\]

Indeed, for such a pair we have

\[\mathbb{E}\left[\begin{array}{c}\hat{w}_{h}\\ \hat{z}_{h}\end{array}\right]\left[\begin{array}{cc}\hat{w}_{h}&\hat{z}_{h} \end{array}\right]\right] = \left[\begin{array}{cc}h&\frac{h^{2}}{2}\\ \frac{h^{2}}{2}&\frac{h^{3}}{3}\end{array}\right].\]

In light of the above expression, the truncated Taylor expansions we refer in the main part of the paper consists on the consideration of only the coefficients in \(L_{t}^{k}\). The only noise noise contribution we will consider corresponds to \(g(s)\hat{I}_{(1)}\).

Experiment Details

We evaluate the Frechet inception distance (FID) after generating 50K samples with each solver, and compare with the statistics of real-data. In our experiments we make use of the code from [16] for continuously trained models as well as their reference FID stats2 and that of [22] for discretely trained models.

Footnote 2: https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/

All the experiments of SEEDS for continuous-time models are parameterized within the EDM framework with the discretization of type EDM, linear schedule, and scaling none as described on [16] in noise prediction mode unless explicitly stated. We use the SEEDS-3 method that has 3 NFEs per step and fixed step-size and report FID scores at NFEs divisible by 3.

We leverage the explicit Langevin-like "churn" trick using in [16] to add or remove noise in the sampling phase. Specifically, [16] uses 4 hyper-parameters \(S_{\text{churn}},S_{\text{min}},S_{\text{max}}\) and \(S_{\text{noise}}\) in which \(S_{\text{churn}}\) controls the overall amount of stochasticity added before giving the input to the SEEDS-3 method when the noise level (or time step in EDM configuration) \(t_{i}\) is contained in the noise interval \([S_{\text{min}},S_{\text{max}}]\). It means that the EDM proposed sampler is stochastic under some conditions of those hyper-parameters and deterministic otherwise, while our method is completely stochastic. In our experiments, we set \(S_{\text{churn}}=0\) except for ImageNet-64 EDM optimized model. We noticed that using the additional stochasticity indeed helps to improve the image quality as in Fig. 1 (c). Moreover, setting \(S_{\text{noise}}\) slightly above 1 might correct the errors in earlier steps more effectively as indicated in [16].

### Pre-trained model specifications

For producing the CIFAR-10 time-continuous results in Table 1, we use the VP DDPM++ continuous architecture. These models are publicly available in conditional3 and unconditional4 versions and were directly derived from [34] under the Apache 2.0 license. On the unconditional mode (Figure 1 (a-b)), we first generate the FID curves of 3 types of DPM-Solver (with orders 1, 2 and 3) using the updates from their official implementation5 in noise prediction mode. Taking profit of the tuning advancements proposed by [16], we used a linear noise schedule with \(\beta_{4}=19.1\) and \(\beta_{\min}=0.1\) that slightly differs from the original parameters from [34] but were proven beneficial. We set the end time of sampling \(\varepsilon\) to 1e-4 as recommended by [22, Appendix D.2]. The values of all benchmark models for Figure 1 (b) were taken directly from tables provided by [22].

Footnote 3: https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/baseline/baseline-cifar10-32x32-cond-vp.pkl

Footnote 4: https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/baseline/baseline-cifar10-32x32-uncond-vp.pkl

Footnote 5: https://github.com/LuchengTHU/dpm-solver

Footnote 6: https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/baseline/baseline-ffhq-64x64-uncond-vp.pkl

In our FFHQ-64 experiments, we employ the unconditional VP pretrained6 model provided by [16].

Footnote 7: https://drive.google.com/file/d/1R_H-fJYXSH79wfSKs9D-fuKQyan5L-GR/view?usp=sharing

For the CelebA-64 experiments, we use the pre-trained VP unconditional model whose checkpoint7 is provided by [33]. We use the Type-1 discretization proposed in [22] to ensure compatibility of our method with the prescribed trained steps of such model.

Footnote 7: https://drive.google.com/file/d/1R_H-fJYXSH79wfSKs9D-fuKQyan5L-GR/view?usp=sharing

For ImageNet-64, we both use the baseline and the optimized pre-trained models given in [16]. We note that the baseline is trained on the iDDPM class of model [26], which actually uses different preconditioning and thus the change of variables compared to the optimized model. The Figure 1 (c) was obtained using the EDM-preconditioned checkpoint8. The added noise settings of SEEDS-3 solver were not subject to a grid-search optimization procedure. The chosen hyper-parameters were \(S_{\text{churn}}=11\), \(S_{\text{noise}}=1.003\), \(S_{\min}=0.05\), and \(S_{\max}=15\) but we are confident that this configuration can be optimized to further improve our results.

Footnote 8: https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-imagenet-64x64-cond-adm.pkl

### Noise vs. Data Prediction approaches

In Appendix B of [23], the authors compare DPM-Solver2 and DPM-Solver++(2S), which amounts on comparing in our framework the difference between the obtained exponential integrators for the PFO in the noise and data prediction regimes to detect exactly a coefficient on the non-linear term that is absent in the noise prediction regime. The term they find corresponds exactly to the difference between applying the variation of constants formula before (instead of after) replacing the score function with the desired neural network. In Tab. 3 we report both data and noise prediction SEEDS-3. At low NFEs the DP approach gives better results but stabilizes in high NFEs at a FID score that is worse than the one the NP approach reaches.

### Low vs. High stage Solvers

Similar to DPM-Solver [22], the FID scores in Tab. 4 and 5 show that at low NFEs, higher stage methods performs more poorly while at higher NFEs, DPM-Solver-3 and SEEDS-3 are better than their counterparts with 1 and 2 stages.

\begin{table}
\begin{tabular}{l r r r r r r r r} \hline \hline Method \textbackslash{} NFE & 10 & 12 & 15 & 20 & 30 & 40 & 50 & 100 \\ \hline DPM-Solver-1 & 22.90 & 17.73 & 13.36 & 9.78 & 6.87 & 5.77 & 5.17 & 4.22 \\ DPM-Solver-2 & **12.22** & **6.52** & **\({}^{\star}\)4.55** & – & 3.75 & 3.68 & 3.64 & 3.60 \\ DPM-Solver-3 & \({}^{\dagger}\)66.92 & 9.72 & 5.32 & \({}^{\star}\)3.83 & **3.66** & – & \({}^{\star}\)**3.61** & \({}^{\dagger}\)3.58 \\ SEEDS-1 & 303.48 & 239.79 & 279.84 & 192.68 & 84.78 & 45.26 & 28.18 & 8.24 \\ SEEDS-2 & 481.09 & 473.48 & \({}^{\star}\)430.98 & 305.88 & 223.01 & 51.41 & 11.10 & **3.19** \\ SEEDS-3 & \({}^{\dagger}\)483.04 & 482.19 & 479.63 & \({}^{\star}\)462.61 & 280.48 & \({}^{\dagger}\)247.44 & \({}^{\star}\)62.62 & \({}^{\dagger}\)3.53 \\ \hline \hline \end{tabular}
\end{table}
Table 4: FID comparison between SEEDS (Ours) and DPM-Solver for low NFEs on CIFAR-10 VP uncond. discrete. We recomputed the DPM-Solver score using the “non-deep” model while [22] reports results for the “deep” architecture. The symbol \({}^{\star}\) is used when using 1 NFE more and \({}^{\dagger}\) when using 1 NFE less because the given NFE cannot be divided by 2 or 3. This corresponds to Figure 1 (a).

\begin{table}
\begin{tabular}{l r r r r r r r} \hline \hline Method \textbackslash{} NFE & 9 & 30 & 60 & 90 & 150 & 165 & 180 \\ \hline SEEDS-3 data prediction & 60.75 & 22.42 & 12.47 & 2.95 & 2.51 & 2.54 & 2.55 \\ SEEDS-3 noise prediction & 471.29 & 288.20 & 33.92 & 3.76 & 2.40 & **2.39** & 2.47 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison between noise prediction \(F_{\theta,t}\) and data prediction \(D_{\theta,t}\) modes of SEEDS-3 on CIFAR-10 (VP uncond. cont.).

\begin{table}
\begin{tabular}{l r r r r r r} \hline \hline Method \textbackslash{} NFE & 150 & 200 & 300 & 510 \\ \hline DPM-Solver-3 & 3.59 & \({}^{\star}\)3.58 & – & 3.58 \\ SEEDS-1 & – & 4.07 & 3.40 & – \\ SEEDS-3 & 3.12 & \({}^{\star}\)3.08 & 3.14 & 3.24 \\ \hline \hline \end{tabular}
\end{table}
Table 5: FID comparison between SEEDS (Ours) and DPM-Solver for high NFEs on CIFAR-10 VP uncond. discrete. We recomputed the DPM-Solver score using the “non-deep” model while [22] reports results for the “deep” architecture. The symbol \({}^{\star}\) is used when using 1 NFE more because the given NFE cannot be divided by 2 or 3. This corresponds to Figure 1 (a).

[MISSING_PAGE_FAIL:51]

### Hardware configuration

During the experiments, we used three Linux-based servers with 60GB memory each and 4 GPUs NVIDIA V100 32GB, 4 GPUs NVIDIA V100 16GB, and 2 GPUs NVIDIA V100 32GB, respectively. Table 12 shows the detail of the configuration utilized for each experiment. We noted that when using the 4 GPUs configuration, the FID results were slightly lower (around 2%), even after using a stacked fixed random seed. We run the experiments multiple times and reported the minimum FID value each time.

### Licences

Pre-trained models:

* CIFAR-10 models by [34]: Apache V2.0 license
* FFHQ-64 model by [16]: Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.
* CelebA-64 model by [33]: Apache V2.0 license
* ImageNet-64 model by [16]: Apache V2.0 license
* Inception-v3 model by [35]: Apache V2.0 license

### Supporting samples

In this subsection, we report the image grids supporting our claims in section 5.

\begin{table}
\begin{tabular}{l r r r r r r r r r} \hline \hline Method \textbackslash{} NFE & 9 & 12 & 15 & 21 & 51 & 60 & 90 & 102 & 200 \\ \hline E-M & \({}^{\star}\)310.22 & 227.16 & 207.97 & \({}^{\dagger}\)120.44 & \({}^{\dagger}\)29.25 & - & - & - & 3.90 \\ An.-DDPM & \({}^{\star}\)28.99 & 25.27 & 21.80 & \({}^{\dagger}\)18.14 & \({}^{\dagger}\)11.23 & - & - & - & 6.51 \\ An.-DDIM & \({}^{\star}\)15.62 & 13.90 & 12.29 & \({}^{\dagger}\)10.45 & \({}^{\dagger}\)6.13 & - & - & - & 3.46 \\ DDIM & \({}^{\star}\)10.85 & 9.99 & 7.78 & \({}^{\dagger}\)6.64 & \({}^{\dagger}\)5.23 & - & - & - & 4.78 \\ DPM-Solver & 6.92 & 4.20 & 3.05 & 2.82 & 2.72 & - & - & - & - \\ SEEDS & 460.87 & 374.48 & 301.66 & 261.87 & 3.84 & 6.58 & **1.88** & 1.97 & - \\ \hline \hline \end{tabular}
\end{table}
Table 8: FID comparison of different solvers on CelebA 64x64 discrete. The symbol \({}^{\star}\) is used when using 1 NFE more and \({}^{\dagger}\) when using 1 NFE less because the given NFE cannot be divided by 2 or 3.

\begin{table}
\begin{tabular}{l r r r r r r r r r} \hline \hline Method \textbackslash{} NFE & 15 & 21 & 30 & 60 & 90 & 120 & 129 & 150 & 165 & 180 \\ \hline SEEDS & 239.2 & 167.5 & 131.1 & 25.06 & 3.19 & 2.17 & **2.08** & 2.15 & 2.16 & 2.19 \\ \hline \hline \end{tabular}
\end{table}
Table 9: SEEDS-3 on CIFAR-10-cond-vp-continuous (using the baseline model in [16]).

[MISSING_PAGE_FAIL:53]

Figure 4: Samples on CIFAR-10 from low to high NFEs by SEEDS-3 in Noise Prediction (NP) and Data Prediction (DP) modes, using conditional VP continuous baseline model [16].

Figure 5: Samples on CIFAR-10 from low to high NFEs by SEEDS-3 in Noise Prediction (NP) and Data Prediction (DP) modes, using unconditional VP continuous baseline model [16].

Figure 6: Samples on CelebA-64 from low to high NFEs by SEEDS-3 and DPM-Solver-3, using pre-trained model from [33].

Figure 7: Samples on FFHQ-64 from low to high NFEs by SEEDS-3 in Noise Prediction (NP) and Data Prediction (DP) modes, using unconditional VP continuous baseline model [16].

Figure 8: Samples on ImageNet-64 from low to high NFEs in Noise Prediction (NP) and Data Prediction (DP) modes, using conditional EDM optimized model [16].

Figure 9: Samples on ImageNet-64 from low to high NFEs in Noise Prediction (NP) and Data Prediction (DP) modes, using conditional EDM baseline model [16].

Figure 11: Visual sample quality comparison between DPM-Solver-3 and SEEDS-3 using their optimal settings and unconditional CIFAR-10 discrete model [34].

Figure 10: Example of samples on CelebA-64 generated by SEEDS-3 in 90 NFEs, using pre-trained model from [33].

Figure 12: Visual sample quality comparison between DPM-Solver-3, EDM and SEEDS-3 using their optimal settings and unconditional VP FFHQ-64 continuous model [16].

Figure 13: Visual sample quality comparison between DPM-Solver-3, EDM and SEEDS-3 using their optimal settings and conditional VP CIFAR-10 baseline continuous model [16].

Figure 14: Example of samples on LSUN-Bedroom-256 by SEEDS-3 in 201 NFEs, using pre-trained model from [8].

Figure 15: StableDiffusion-\(512\times 512\) by SEEDS-1 with prompt “_High quality photo of an astronaut riding a horse in space_” and NFE \(=90\).