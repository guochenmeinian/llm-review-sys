Diffusion Model for Graph Inverse Problems: Towards Effective Source Localization on Complex Networks

Xin Yan\({}^{1,*}\) Hui Fang\({}^{2,*}\) Qiang He\({}^{1,\dagger}\)

\({}^{1}\)College of Medicine and Biological Information Engineering, Northeastern University, China

\({}^{2}\)RIIS & SIME, Shanghai University of Finance and Economics, China

yanx_in@outlook.com,fang.hui@mail.shufe.edu.cn,heqiangcai@gmail.com

Equal contribution. Corresponding author.

###### Abstract

Information diffusion problems, such as the spread of epidemics or rumors, are widespread in society. The inverse problems of graph diffusion, which involve locating the sources and identifying the paths of diffusion based on currently observed diffusion graphs, are crucial to controlling the spread of information. The problem of localizing the source of diffusion is highly ill-posed, presenting a major obstacle in accurately assessing the uncertainty involved. Besides, while comprehending how information diffuses through a graph is crucial, there is a scarcity of research on reconstructing the paths of information propagation. To tackle these challenges, we propose a probabilistic model called DDMSL (Discrete Diffusion Model for Source Localization). Our approach is based on the natural diffusion process of information propagation over complex networks, which can be formulated using a message-passing function. First, we model the forward diffusion of information using Markov chains. Then, we design a reversible residual network to construct a denoising-diffusion model in discrete space for both source localization and reconstruction of information diffusion paths. We provide rigorous theoretical guarantees for DDMSL and demonstrate its effectiveness through extensive experiments on five real-world datasets.

## 1 Introduction

Information diffusion is a pervasive phenomenon in our daily lives. Data scientists have conducted extensive research on information diffusion along the direction of entropy increase, such as maximizing the influence of nodes in social networks [15; 3], and developing control policies to curb the scale of epidemics in disease transmission networks [53]. However, merely comprehending the mechanism of forward diffusion is insufficient. When destructive information spreads across a network, it can cause huge damage on the entire system. For example, the rapid spread of rumors in social networks can cause harm to society [54], and the spread of computer viruses on the Internet can paralyze a system consisting of millions of users [1]. Additionally, pandemics such as SARS and COVID-19 in human interaction networks pose serious challenges to human health and social functioning [31]. Therefore, accurately identifying the sources of transmission and cutting off their possible transmission paths in time is crucial. This can help limit the spread of negative information and maintain the stability of the network.

Common information dissemination models like SIR (Susceptible-Infected-Recovered) and SI (Susceptible-Infected) [19] are subject to uncertainties during the process of information diffusion. As illustrated in Figure 1, the diffusion graph of information should follow a non-explicit distribution, implying that a single diffusion source may correspond to multiple different diffusion graphs, and a diffusion graph may have multiple different diffusion sources. Therefore, the inverse problem of information diffusion is underdetermined.

Previous source localization research has primarily relied on manually formulating rules to filter source nodes, such as using Jordan centrality [24, 26] to locate multiple source nodes in SI model, or unbiased intermediary centrality to identify source nodes in SIR model [49]. Besides, IVGD [43] exploited fixed point theorem for source localization of IC model [18]. However, these algorithms impose **relatively strict requirements on information diffusion patterns**, which could hardly be satisfied in real-world applications. To address this issue, some algorithms [38, 36] have been proposed using graph neural networks (GNN) [35] to learn source nodes under different diffusion models. Unfortunately, most deep learning-based algorithms ignore the underdeterminacy of information propagation and attempt to establish a direct mapping between observed data and source nodes. As a result, inference results are often deterministic and **fail to quantify the uncertainty of source localization**. In addition to source localization, **it is equally important to recover the possible propagation paths of information**. For example, when an infectious disease breaks out, it is necessary to promptly trace the virus transmission trajectory and corresponding close contacts. However, in reality, this can only be achieved by analyzing the genetic evolution tree of the virus strain [40], which is both time-consuming and labor-intensive. In summary, **there is a current dearth of methods that can simultaneously locate information sources and restore information propagation paths**.

Inspired by diffusion phenomena, the diffusion denoising model [16] has garnered significant achievements in the realm of image generation. It effectively restores the original image within a noise distribution through gradual refinement. This process bears resemblance to the challenges encountered in information reconstruction and diffusion, demanding our attention. Throughout the propagation process, the source information continually undergoes blurring, presenting an exceedingly arduous task of restoring the original source node. In this scenario, employing a diffusion model for the reverse recovery of the entire information dissemination process proves highly appropriate. However, using diffusion models to solve inverse problems in graphs presents several challenges: (1) Information diffusion occurs on non-Euclidean space graphs, making learning discrete non-Euclidean data difficult; (2) Diffusion over graphs is governed by information propagation rules that operate in non-Euclidean spaces, making it challenging to establish both the forward process and reverse inference process of information diffusion within the network; and (3) Most existing algorithms only work for specific propagation patterns, suffering from the problem of model generalization.

To address the aforementioned challenges, we propose a new framework, DDMSL2(Discrete Diffusion Model for Source Localization). DDMSL excels at source localization and reconstructing the evolution of diffusion, showing promising results across different propagation patterns. Our contributions can be summarized as follows:

Figure 1: The SIR diffusion process of information over complex networks. Distinct sources may produce identical infection graphs (\(x_{T}\)), and initiating forward diffusion from \(x_{T}\) may lead to different diffusion outcomes. The distribution of sources and forward diffusion can be represented by probability models \(P^{-1}\) and \(P\), respectively.

* We model the information diffusion process using Markov chains and demonstrate that at each moment, the state transition matrix converges, enabling us to reconstruct the information diffusion paths from any time point.
* We design a residual network block based on graph convolutional networks to approximate the information diffusion process and provide a theoretical proof for the model's reversibility.
* We propose an end-to-end framework being capable of both reconstructing the evolution of information diffusion and source localization. To the best of our knowledge, this is the first study to employ denoising diffusion models for solving graph inverse problems. Extensive experiments on five real-world datasets demonstrate its effectiveness.

## 2 Related work

**Diffusion models for inverse problems**. The diffusion denoising model [16; 29] is currently one of the best available deep generative models, which has been extensively applied in image-related inverse problems [33; 23; 34]. Previous studies have extended DP3Ms [2] to discrete spaces and used this work as a foundation for developing multiple generative diffusion models for graph data [14; 42]. Additionally, Stevens et al. [39] and Chung et al. [7] demonstrated that the diffusion model can solve nonlinear inverse problems. These works collectively demonstrate the effectiveness of diffusion models across different domains in addressing inverse problems.

**Localization of information sources on the graph.** To facilitate the localization of infection sources, various algorithms have been proposed [25; 52; 44; 9]. Early algorithms focused on screening source nodes by employing feature engineering. For instance, Luo et al. [25] identified the diffusion source by the centrality of the diffusion subgraphs, while Prakash et al. [30] and Zhu et al. [52] respectively developed NETSLEUTH and OJC algorithms based on the minimum description length of nodes and the shortest path. Wang et al. [44] designed a series of label propagation algorithms based on the aggregation process of information diffusion, on which Dong et al. [9] further improved using GCN [20]. Recently, the proposed reversible perception algorithm IVGD based on graph diffusion [43] has been applied to the IC model. However, **these algorithms generally have strict limitations on diffusion patterns or network structures.** DDMIX [8] was the first algorithm to use a generative model for reconstructing the dissemination paths of information. It leverages a VAE (variational autoencoder) to learn the state of nodes across all time steps; however, as the number of time steps increases, the solution space of the dissemination path becomes significantly more complex, leading to low accuracy in source localization. Ling et al. later introduced SLVAE [22], which is also based on VAE and can efficiently localize the source node but cannot directly reconstruct the propagation paths. Current generative model-based approaches for source localization **cannot simultaneously handle the problems of source localization and reconstructing diffusion paths**.

## 3 Methodology

### Problem definition

We consider an undirected graph \(\mathcal{G}=(\mathbf{V},\mathbf{E})\) where \(\mathbf{V}=\left\{x^{1},x^{2},\ldots,x^{N}\right\}\) is the node set and \(\mathbf{E}\) is the edge set. Let \(\mathbb{S}=\left\{x_{0}^{s_{1}},x_{0}^{s_{2}},\ldots x_{0}^{s_{m}}\right\}\) denote the set of initial diffusion source nodes at \(t=0\). The source nodes undergo diffusion on the graph \(\mathcal{G}\) for \(T\) time steps, governed by the diffusion pattern \(g(\cdot)\), and the set of node states at time step \(t\) is denoted by \(\mathbf{X_{t}}=\left\{x_{t}^{1},\ldots,x_{t}^{N}\right\}\) with \(x_{t}^{i}\in\left\{0,1\right\}^{M}\) (e.g., if \(g(\cdot)\) is the SIR model, then \(M\in\left\{S,I,R\right\}\), where \(S,I,R\) denote susceptible, infected and recovered state, respectively). If the node \(i\in\mathbb{S}\), then \(x_{0}^{i}=1\), otherwise \(x_{0}^{i}=0\). We define the research problem as finding the intermediate state \(\mathbf{X}=\left\{\mathbf{X_{0}},\mathbf{X_{1}},\ldots,\mathbf{X_{T-1}}\right\}\) that maximizes the likelihood function \(\mathbf{X}^{*}=argmax_{\mathbf{X}}^{P\left\{\mathbf{X_{T}}\left|\mathbf{X}, \mathcal{G}\right\}}\), given the observed \(\mathbf{X_{T}}\).

### Information diffusion in discrete spaces

Susceptible-Infected-Recovered (SIR) and Susceptible-Infected (SI) models [19] are commonly used to model diffusion phenomena in nature, such as the spread of epidemics [51] and rumors [50]. In this paper, we will demonstrate our approach using the SIR model. The SIR model categorizes node states into susceptible (S), infected (I), and recovered (R) states. The S state represents individuals who are susceptible to infection, while the I state represents those who have been infected, and the R state indicates recovery from the infected state. The transition between these three states is irreversible. We assume that all nodes on the graph are homogeneous and follow the same transition process for the different states of the SIR model3:

Footnote 3: \(A\) denotes the graphâ€™s adjacency matrix, and \(I_{j}(t)\) represents whether neighbor \(j\) is in an infected state at \(t\).

\[\left\{\begin{array}{l}P\left(x_{t+1}^{i}=I\mid x_{t}^{i}=S\right)=1-\prod_{ j}\left(1-\beta_{I}^{j}(t)A_{ij}I_{j}(t)\right)\\ P\left(x_{t+1}^{i}=R\mid x_{t}^{i}=I\right)=\gamma_{R}^{i}(t)\end{array}\right.\] (1)

And, the SIR diffusion process can be represented using a state transfer matrix:

\[Q_{t}^{i}=\begin{bmatrix}1-\beta_{I}^{j}(t)&\beta_{I}^{i}(t)&0\\ 0&1-\gamma_{R}^{i}(t)&\gamma_{R}^{i}(t)\\ 0&0&1\end{bmatrix}\] (2)

where \(Q_{t}^{i}\) denotes the state transfer matrix of node \(i\) at moment \(t\), and \(\left[Q_{t}^{i}\right]_{uv}\) denotes the probability of transferring from state \(u\) to state \(v\)4. \(\beta_{I}^{i}(t)\) and \(\gamma_{R}^{i}(t)\) denote the infection rate and recovery rate at moment \(t\), respectively. Let \([P_{S}^{i}(t),P_{I}^{i}(t),P_{R}^{i}(t)]\) to be the probabilities of node \(i\) being in three states \(S\), \(I\), and \(R\) at time \(t\), then \(\beta_{I}^{i}(t)\) and \(\gamma_{R}^{i}(t)\) can be calculated by the following equation:

Footnote 4: In the SIR model, \(u,v\in\{1,2,3\}\), representing the three states: S, I and R, respectively.

\[\left\{\begin{aligned} \beta_{I}^{i}(t)&= \frac{P_{I}^{i}(t+1)-P_{I}^{i}(t)(1-\gamma_{R}^{i}(t))}{P_{S}^{i}(t)}=1-\frac {P_{S}^{i}(t+1)}{P_{S}^{i}(t)}\\ \gamma_{R}^{i}(t)&=\frac{P_{R}^{i}(t+1)-P_{R}^{i}(t)}{P _{I}^{i}(t)}\end{aligned}\right.\] (3)

**Theorem 3.1**: _Given graph \(\mathcal{G}\) and the infected seed set \(\mathbb{S}\), it can be determined that the state transfer matrix \(Q_{t}^{i}\) for the \(SIR\) diffusion process on \(\mathcal{G}\) converges at all times._

**Sketch of Proof**. Given \(\mathcal{G}\) and \(\mathbb{S}\), the initial state of information diffusion can be determined. Based on the SIR propagation rule, an iterative equation can be constructed for the state distribution of the nodes at any given time. Finally, \(Q_{t}^{i}\) can be calculated according to Equations 2 and 3.

Referring to Equations 2 and 3, we can readily demonstrate the convergence of the state transition matrix \(Q_{t}^{i}\) at every moment, rendering the discrete diffusion model a practicable solution [2]. The proof is shown in Theorem3.1.

### Discrete diffusion model for source localization

#### 3.3.1 Forward process

We represent the state of node \(i\) by an one-hot vector \(\boldsymbol{x}_{t}^{i}\in\mathbb{R}^{1\times M}\), and the state distribution of node \(i\) at time \(t\) is written as:

\[q\left(\boldsymbol{x}_{t}^{i}\mid\boldsymbol{x}_{t-1}^{i}\right)=\boldsymbol {x}_{t-1}^{i}Q_{t}^{i}\boldsymbol{x_{t}^{i}}^{T}\sim\mathrm{Cat}\left( \boldsymbol{x}_{t}^{i};\boldsymbol{p}=\boldsymbol{x}_{t-1}^{i}Q_{t}^{i}\right)\] (4)

Information diffusion is a Markov process that allows for inference of the node state at any given moment based on the initial state:

\[q\left(\boldsymbol{x}_{t}^{i}\mid\boldsymbol{x}_{0}^{i}\right)=\sum_{ \boldsymbol{x}_{1:t-1}^{i}}\prod_{k=1}^{t}q\left(\boldsymbol{x}_{k}^{i}\mid \boldsymbol{x}_{k-1}^{i}\right)=\boldsymbol{x}_{0}^{i}\bar{Q}_{t}^{i} \boldsymbol{x_{t}^{i}}^{T}\sim\mathrm{Cat}\left(\boldsymbol{x}_{t}^{i}; \boldsymbol{p}=\boldsymbol{x}_{0}^{i}\bar{Q}_{t}^{i}\right)\] (5)

#### 3.3.2 Reverse process

Reconstructing the information diffusion process requires backward inference of the forward process, which can be achieved through Bayesian formulation5:

Footnote 4: In the SIR model, \(u,v\in\{1,2,3\}\), representing the three states: S, I and R, respectively.

Footnote 5: Please refer to Appendix B for the formula derivation of forward and backward processes.

\[q\left(\boldsymbol{x}_{t-1}^{i}\mid\boldsymbol{x}_{t}^{i},\boldsymbol{x}_{0}^ {i}\right)\sim\mathrm{Cat}\left(\boldsymbol{x}_{t-1}^{i};\boldsymbol{p}= \frac{\left(\boldsymbol{x}_{t}^{i}Q_{t}^{i^{T}}\right)\odot\left(\boldsymbol{x }_{0}^{i}\bar{Q}_{t-1}^{i}\right)}{\boldsymbol{x}_{0}^{i}\bar{Q}_{t}^{i} \boldsymbol{x_{t}^{i}}^{T}}\right)\] (6)In the absence of knowledge about \(x_{0}^{i}\), the posterior distribution \(q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i},\bm{x}_{0}^{i}\right)\) becomes intractable to compute directly. As a result, we must approximate this distribution using other methods. Continuing with Bayes' theorem, we can express the posterior distribution as follows:

\[q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i},\bm{x}_{0}^{i}\right)=q\left(\bm{x}_{ t-1}^{i}\mid\bm{x}_{t}^{i}\right)=\frac{\sum_{\bm{x}_{0}^{i}}q\left(\bm{x}_{t-1}^{i},\bm{x}_{t}^{i},\bm{x}_{0}^{i}\right)}{q\left(\bm{x}_{t}^{i}\right)}=\mathbb{E }_{q\left(\bm{x}_{0}^{i}\mid\bm{x}_{t}^{i}\right)}q\left(\bm{x}_{t-1}^{i}\mid \bm{x}_{t}^{i},\bm{x}_{0}^{i}\right)\] (7)

**We utilize a neural network model (\(nn_{\theta}\)) to learn about \(p_{\theta}\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i}\right)\) and estimate \(q\left(\bm{x}_{0}^{i}\mid\bm{x}_{t}^{i}\right)\). The detailed design of \(nn_{\theta}\) will be elaborated later in Section 3.4.**

\[\begin{split} q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i}\right) \approx p_{\theta}\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i}\right)& =\mathbb{E}_{\bm{x}_{0}^{i}\sim p_{\theta}\left(\bm{x}_{0}^{i} \mid\bm{x}_{t}^{i}\right)}q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i},\bm{x}_{0 }^{i}\right)\\ &=\frac{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{t-1}^{i}\right)\left[ \sum_{j}q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{0}^{i}\right)p_{\theta}\left(\bm{x }_{0}^{i}\mid\bm{x}_{t}^{i}\right)\right]}{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{0 }^{i}\right)}\end{split}\] (8)

Similar to [2], we predict \(x_{0}^{i}\) and then use this prediction to learn the distribution of \(q\left(\bm{x}_{0}^{i}\mid\bm{x}_{t}^{i}\right)\). This approach offers several benefits. First, it leverages prior knowledge by exploiting the fact that at \(t=0\) there are only two states: susceptible and infected. This makes \(nn_{\theta}\) easier to train. Second, locating \(x_{0}^{i}\) is a key target of our task. Finally, predicting \(x_{0}^{i}\) can help constrain errors in the information reconstruction process (Refer to Appendix C).

\[\begin{split} L_{\mathrm{vb}}^{i}=&\mathbb{E}_{q \left(\bm{x}_{0}^{i}\right)}\left[\underbrace{D_{\mathrm{KL}}\left[q\left(\bm{ x}_{T}^{i}\mid\bm{x}_{0}^{i}\right)\left\|p\left(\bm{x}_{T}^{i}\right)\right\| \right.}_{L_{T}}+\right.\\ &\left.\sum_{t=2}^{T}\underbrace{\mathbb{E}_{q\left(\bm{x}_{t}^{ i}\mid\bm{x}_{0}^{i}\right)}\left[D_{\mathrm{KL}}\left[q\left(\bm{x}_{t-1}^{i} \mid\bm{x}_{t}^{i},\bm{x}_{0}^{i}\right)\left\|p_{\theta}\left(\bm{x}_{t-1}^{i }\mid\bm{x}_{t}^{i}\right)\right]\right]}_{L_{t-1}}-\underbrace{\mathbb{E}_{q \left(\bm{x}_{1}^{i}\mid\bm{x}_{0}^{i}\right)}\left[\log p_{\theta}\left(\bm{ x}_{0}^{i}\mid\bm{x}_{1}^{i}\right)\right]}_{L_{0}}\right]\end{split}\] (9)

Equation 9 presents the variational lower bound loss function of the denoising diffusion model [16], where \(D_{KL}\) represents relative entropy. Furthermore, as highlighted in our earlier discussion, it is crucial for us to supervise \(x_{0}^{i}\) directly. As a result, we arrive at the simplified variational lower bound loss (\(L_{simple}\)), which is denoted as:

\[L_{simple}=\frac{1}{N}{\sum_{i=1}^{N}}\left(L_{vb}^{i}+\mathbb{E}_{q\left(\bm {x}_{0}^{i}\right)}\mathbb{E}_{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{0}^{i}\right)} \left[-\log\widetilde{p}_{\theta}\left(\bm{x}_{0}^{i}\mid\bm{x}_{t}^{i}\right) \right]\right)\] (10)

#### 3.3.3 Supervision of information propagation rules

Our reconstructed information diffusion must adhere to the propagation rules of model \(g(\cdot)\). During early training, the predictions of \(nn_{\theta}\) for \(x_{t-1}^{i}\) may not be accurate enough, which could lead to violations of the propagation rules. For example, using the SIR model, if the state of node \(i\) at time \(t\) is classified as \(I\), then the state of \(i\) at time \(t-1\) can only be either \(S\) or \(I\). However, the prediction result of \(nn_{\theta}\) may erroneously output state \(R\), resulting in the failure to calculate \(q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i},\bm{x}_{0}^{i}\right)\).

To prevent violations of the propagation rules, we can take two measures: (1) In cases where the inference result of \(nn_{\theta}\) violates the propagation rule, \(q(x_{t-1}^{i}|x_{t}^{i},x_{0}^{i})\) should be set to \([1,0,0]\). and (2) To supervise the training of \(nn_{\theta}\), the propagation rule loss function \(L_{constrain}=L_{constrain1}+L_{constrain2}\) is introduced. The detailed derivations of (1) and (2) are shown in Appendix C.

\[\left\{\begin{array}{l}L_{constrain1}=Relu\left(\mathbf{X_{t-1}}-\left( \mathbf{A+I}\right)\mathbf{X_{0}}\right)\\ L_{constrain2}=\left\|\max\left(\mathbf{0},\mathbf{X_{t-1}^{(j)}-X_{t-1}^{(i)} }\right)\right\|_{2}^{2},\forall\mathbf{X_{0}^{(i)}}\supseteq\mathbf{X_{0}^{(j)}} \end{array}\right.\] (11)

In order to evaluate Equation 11, we must first sample \(\mathbf{X_{t-1}}\) from the discrete probability distribution computed in Equation 8. To ensure gradient preservation during the sampling process, we employ the \(Gumbel\)-\(Softmax\) technique [17]: \(\mathbf{X_{t-1}}\sim Gumbel\)-\(Softmax(q(x_{t-1}^{i}|x_{t}^{i},x_{0}^{i}))\). Equation 11 demonstrates that each node's contribution to the information diffusion process is non-negative. In summary, the constrained loss function is \(L_{constrain=}=L_{constrain1}+L_{constrain2}\), and the loss function of DDMSL is:

\[Loss=L_{simple}+L_{constrain}\] (12)

### Design of the inference model

**In this section, we discuss the design of the \(nn_{\theta}\) in detail**. Let \(\mathbf{X_{0}}=\left\{x_{0}^{1},x_{0}^{2}\dots x_{0}^{N}\right\}\) and \(\mathbf{Y_{T}}=\left\{x_{T}^{1},x_{T}^{2}\dots,x_{T}^{N}\right\}\). The diffusion of information on complex networks can be modeled using the following processes [46; 43]:

\[\mathbf{X_{0}}\xrightarrow[]{f_{w}}\xi\xrightarrow[]{\mathbf{F}}\mathbf{Y_{T}}\] (13)

Within this framework, \(f_{w}\) and \(\mathbf{F}\) serve as the feature vector construction and feature propagation functions, respectively. The relationship between \(\mathbf{Y_{T}}\) and \(\mathbf{X_{0}}\) be expressed as:\(\mathbf{Y_{T}}=\mathbf{P}(\mathbf{X_{0}})=\mathbf{F}(f_{w}(\mathbf{X_{0}}))\). Our research objective focuses on the inverse problem of the SIR or SI diffusion model, which entails a large number of stochastic processes. Thus, when \(\mathbf{F}=\mathbf{RD}\), solving \(\mathbf{P}^{-1}\) can be extremely challenging. In the following subsections, we will explore how to construct suitable functions \(f_{w}\) and \(\mathbf{F}\) to facilitate the calculation of \(\mathbf{P^{-1}}\).

#### 3.4.1 Relationship between GCN and SIR diffusion models

According to [37], diffusion models such as SIR are analogous to message passing neural networks (MPNNs [11]), where each node's state is only updated based on the states of its neighboring nodes. However, the operational architecture must be designed to enable each node to aggregate both its own features and those of its neighbors, and update its state via nonlinear activation as shown in Equation 26. Therefore, we propose using MPNNs to remodel the SIR diffusion process. Denoting \(P_{\Omega}^{i}(t)\equiv P(x_{t}^{i}=\Omega)\) as the probability that node \(i\) is in state \(\Omega\in\{S,I,R\}\) at time \(t\) in the SIR diffusion process. It can be observed that this process is structurally equivalent to:

\[\left\{\begin{array}{lll}m_{\Omega}^{i}(t+1)&=&\sum_{j\in N(i)}M_{t}\left(h( P_{\Omega}^{i}(t)),h(P_{\Omega}^{j}(t)),e_{ij}\right)\\ P_{\Omega}^{i}(t+1)&=&U_{t}\left(h(P_{\Omega}^{i}(t)),m_{\Omega}^{i}(t+1) \right)\\ h(P_{\Omega}^{i}(t))&=&\sigma\left(\mathbf{W_{\Omega}}P_{\Omega}^{i}(t)+ \mathbf{b_{\Omega}}\right)\end{array}\right.\] (14)

where \(M_{t}\) is the aggregate function and \(U_{t}\) represents the node status update function. \(\sigma(\cdot)\) is a nonlinear activation function. \(e_{ij}\) represents the edge between nodes i and j. Additionally, since \(h(P_{\Omega}^{i}(t))\) already contains nonlinear transformations, more complex forms of transformation are not necessary and \(U_{t}\) can be defined as: \(U_{t}(a,b)=a+b\). To sum up, Equation 14 can be simplified as:

\[P_{\Omega}^{i}(t+1)=h(P_{\Omega}^{i}(t))+\sigma\left(\sum_{j\in N(i)}h(P_{ \Omega}^{j}(t))\right)\] (15)

We can achieve this using a residual block composed of graph convolutional networks, which allows us to easily fit the SIR model:

\[\left\{\begin{array}{l}\mathbf{h}_{i,t}^{(0)}=\mathbf{SN}(\mathbf{U}x_{i}^ {t})\text{ with }\mathbf{U}\in\mathbb{R}^{C\times M}\\ g(\mathbf{h})=\sigma_{g}(\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}\cdot \mathbf{h}\cdot\mathbf{w}+\mathbf{b})\\ \mathbf{h}_{\mathbf{i},\mathbf{t}}^{(\mathbf{l+1})}=\mathbf{h}_{\mathbf{i}, \mathbf{t}}^{(\mathbf{l})}+\sigma\left(\mathbf{SN}\left(\mathbf{BN}\left(g \left(\mathbf{h}_{\mathbf{i},\mathbf{t}}^{(\mathbf{l})}\right)\right)\right) \right)\end{array}\right.\] (16)

Among them, \(\mathbf{U}\) is a linear transformation, \(\mathbf{h}_{i,t}^{(l)}\) denotes the representation of the \(l\)-\(th\) layer of the network, \(\mathbf{D}\) is the degree matrix, \(\mathbf{SN}(\cdot)\) and \(\mathbf{BN}(\cdot)\) represent spectral normalization and batch normalization, respectively. \(\sigma(\cdot)\) and \(\sigma_{g}(\cdot)\) are nonlinear activation functions6. In fact, \(f_{w}\) and \(\mathbf{F}^{7}\) in Equation 13 are the \(\mathbf{SN}\left(\mathbf{U}(\cdot)\right)\) and residual blocks in Equation 16.

Footnote 6: We set \(\sigma(\cdot)\) and \(\sigma_{g}(\cdot)\) to \(Mish\) and \(LeakyReLu\) respectively.

Figure 2: Multi-layer reversible residual network.

#### 3.4.2 Reversibility of residual blocks

We now discuss the invertibility of Equation 13 as reconstructed from Equation 16. Equation 13 can be written as:

\[\left\{\begin{array}{l}\mathbf{SN}\left(\mathbf{U}(\mathbf{X_{0}})\right)=f_{w }(\mathbf{X_{0}})=\xi;\\ \xi+\sigma\left(\mathbf{SN}\left(\mathbf{BN}\left(g(\xi)\right)\right)\right)= \xi+f_{\theta}(\xi)=\mathbf{F}(\xi)=\mathbf{Y_{T}}.\end{array}\right.\] (17)

**Lemma 3.1**: _Denoting the Lipschitz constants of \(f_{w}\) and \(f_{\theta}\) to be \(L_{w}\) and \(L_{\theta}\), then \(L_{w}<1\) and \(L_{\theta}<1\)._

**Sketch of Proof**. Since \(f_{w}\) and \(\mathbf{BN}\left(g(\xi)\right)\) are spectrally normalized, their Lipschitz coefficients are less than 1 [28]. Additionally, since the \(\sigma(\cdot)\) is set to \(Mish(\cdot)\), it can be determined that the Lipschitz constant of their composite function (i.e., \(f_{\theta}\)) is less than 1.

**Theorem 3.2**: _If \(L_{w}<1\) and \(L_{\theta}<1\), then \(\mathbf{Y_{T}}=\mathbf{F}(f_{w}(\mathbf{X_{0}}))\) is reversible._

**Sketch of Proof**. \(\mathbf{Y_{T}}=\mathbf{F}(f_{w}(\mathbf{X_{0}}))\) can be written as \(\left\{\begin{array}{l}\mathbf{X_{0}}=\xi+\mathbf{X_{0}}-f_{w}(\mathbf{X_{0 }})\\ \xi=\mathbf{Y_{T}}-f_{\theta}(\xi)\end{array}\right..\) Since \(f_{w}<1\) and \(f_{\theta}<1\), constructing the iteration according to the Banach fixed point theorem [4], \(\mathbf{Y_{T}}=\mathbf{F}(f_{w}(\mathbf{X_{0}}))\) is invertible.

In practical applications, we typically use multiple residual blocks to form a residual network. This approach is employed to improve the receptive field of the GCN and alleviate the problem of node smoothing caused by multi-layer graph convolution. The multi-layer reversible residual network that we designed is depicted in Figure 2, where \(\mathbf{DP}\) represents dropout. If the dropout rate is \(r\) and the \(i\)-th layer residual block is \(\mathbf{F_{i}}\), then we have \(\mathbf{F_{i}}(\xi)=\mathbf{DP}(\xi+f_{w}(\xi))\).

**Theorem 3.3**: _If \(dropout\)-\(rate=0.5\), \(L_{w}<1\), and \(L_{\theta}<1\), the residual network \(\mathbf{Y_{T}}=(\mathbf{F_{1}}\circ\mathbf{F_{2}}\circ\ldots\circ\mathbf{F_{n} })(f_{w}(\mathbf{X_{0}}))\) is reversible._

**Sketch of Proof**. Denoting this multilayer residual network as \(F\), the upper bound of \(L_{F}\) is the product of the Lipschitz constants of each function [13]. Additionally, with a dropout rate of \(r\), the Lipschitz constants of the functions will be limited to \((1-r)\) times their original values. Using these two conclusions, we can calculate the upper bound of the Lipschitz constant of a multilayer reversible network with \(L_{F}\leq 1\) when \(r=0.5\).

## 4 Experiments

### Datasets and evaluation metrics

**Datasets**. The diffusion of information occurs in a broad range of network types, and DDMSL was evaluated on five distinct realistic datasets: **Karate**[48], **Jazz**[12], **Cora-ML**[27], **Power Grid**[45], and **PGP**[5]. The detailed parameters of these networks are provided in the Appendix F.2. Following previous research [43; 9; 22], we conducted SIR and SI diffusion simulations on each dataset by randomly selecting 10% of the nodes as source nodes at the initial moment, and stopping the simulation when approximately 50% of the nodes were infected8 (For PGP dataset, simulation stopped at 30% infection rate). We randomly divided each generated dataset into a training set, a validation set and a test set in the ratio of \(8:1:1\).

Footnote 8: While some previous studies [9; 44] commenced source localization when the infection rate reached 30%, we set it to 50% to consider more complex scenarios.

**Evaluation Metrics**. Our objective consists of two components: source localization and information diffusion paths recovery. The source localization task is a binary classification task, and thus evaluated using four metrics: Precision (**PR**), which denotes the proportion of nodes predicted as sources that are true sources; Recall (**RE**), which represents the proportion of actual source nodes that are correctly predicted; **F1** score, the harmonic mean of PR and RE; and ROC-AUC (**AUC**), quantifying the model's ability to classify accurately. To evaluate the performance of the recovering information diffusion paths, we adopted the Mean Squared Error (MSE) error in [8]: \(MSE=\frac{1}{NT}\sum_{t=0}^{T-1}\left\|\mathbf{X_{t}}-\mathbf{\hat{X_{t}}} \right\|^{2}\), where \(\hat{X_{t}}\) is the ground truth. This metric is solely computed for infected nodes, with nodes in the recovered and susceptible states being marked as 0.

[MISSING_PAGE_FAIL:8]

**Reconstructing information diffusion paths**. Both DDMSL and DDMIX can reconstruct the evolution of information diffusion, but DDMIX can only recover the states of susceptible nodes (\(S\)) and infected nodes (\(I\)), whereas DDMSL can reconstruct nodes of all states. To facilitate fair comparison, we calculate MSE loss for node reconstruction in states \(S\) and \(I\), as shown in Figure 3. DDMSL consistently outperforms DDMIX on all datasets, especially in SI diffusion mode where DDMSL's average reconstruction loss is 69% lower than DDMIX's. Unlike DDMIX, which recovers the entire graph state, DDMSL allows for fine-grained node-level reconstruction.

**Additional experiments**. DDMSL represents a source detection algorithm relying on diffusion models, necessitating an assessment of its aptitude for generalization. Concurrently, we assessed its computational complexity while also incorporating experiments utilizing two real diffusion datasets. We have documented the results of these experiments in Appendix G.

**Visualization**. To further showcase the efficacy of DDMSL in solving the inverse problem of information diffusion on complex networks, we designed a visualization that demonstrates source localization and the propagation evolution process. Due to space constraints, we have presented the visualization in Appendix H.

### Ablation study

To evaluate the effectiveness of each component, we conducted ablation experiments on DDMSL in SIR diffusion mode. One essential component of DDMSL is the reversible residual network. We removed this component and replaced each reversible residual block with a GCN network, denoted as DDMSL(a). Additionally, DDMSL is also supervised by \(L_{constarin}\) on the propagation rules. The model without the propagation rule supervision module is denoted as DDMSL(b). Finally, to assess the impact of GCN (Graph Convolutional Network) in reversible residual blocks, DDMSL(c) represents the model after removing the GCN modules from residual blocks. We ran the three variants on five datasets and compared the results,

Figure 4: MSE error in ablations.

Figure 3: Comparisons of DDMSL and DDMIX on reconstructed information diffusion processes.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline  & \multicolumn{3}{c|}{**Karate**} & \multicolumn{3}{c|}{**Jazz**} & \multicolumn{3}{c|}{**Cora MI**} & \multicolumn{3}{c|}{**Power Grid**} & \multicolumn{3}{c|}{**PGP**} \\ \hline
**Methods** & **PR** & **RE** & **FI** & **AUC** & **PR** & **RE** & **FI** & **AUC** & **PR** & **RE** & **FI** & **AUC** & **PR** & **RE** & **FI** & **AUC** & **PR** & **RE** & **FI** & **AUC** \\ \hline
**DDMSL** & 0.708 & 0.706 & 0.722 & 0.853 & 0.817 & 0.88 & 0.848 & 0.930 & 0.884 & 0.867 & 0.880 & 0.928 & 0.833 & 0.879 & 0.855 & 0.930 & 0.856 & 0.903 & 0.879 & 0.943 \\ DDMSL(a) & 0.264 & 0.910 & 0.379 & 0.827 & 0.254 & 0.683 & 0.367 & 0.741 & 0.277 & 0.626 & 0.384 & 0.722 & 0.459 & 0.605 & 0.522 & 0.706 & 0.366 & 0.857 & 0.513 & 0.846 \\ DDMSL(b) & 0.655 & 0.850 & 0.718 & 0.860 & 0.648 & 0.822 & 0.723 & 0.889 & 0.834 & 0.856 & 0.845 & 0.915 & 0.818 & 0.875 & 0.845 & 0.919 & 0.811 & 0.888 & 0.848 & 0.922 \\ DDMSL(c) & 0.339 & 0.185 & 0.236 & 0.592 & 0.931 & 0.111 & 0.1943 & 0.556 & 0.817 & 0.043 & 0.08 & 0.521 & 0.998 & 0.2756 & 0.432 & 0.618 & 0.997 & 0.656 & 0.792 & 0.823 \\ \hline \end{tabular}
\end{table}
Table 3: The results of the ablation study.

which are shown in Table 3 and Figure 4. As can be seen in table 3, we have these observations: (1) removing the residual module (DDMSL(a)) leads to a significant degradation in model performance, with the F1 score and AUC decreasing by 49% and 17%, respectively. Although the GCN network is also a form of MPNNs, we noticed that the output of the inference network composed of GCNs were very similar, indicating node feature oversmoothing, and further highlighting the effectiveness of the reversible residual network module; (2) \(L_{constraint}\) is also effective, which contributes to 5% and 2% performance improvement in terms of F1 score and AUC, respectively. Similar results can be obtained in Figure 4 regarding the task of reconstructing the information diffusion processes; and (3) upon the drop of the GCNs from the reversible residual network (DDMSL(c)), a noteworthy deterioration model performance was observed, manifesting as a substantial decrease in F1 scores and AUC by 59% and 32% respectively. This compellingly signifies the indispensable role of GCN in acquiring the diffusion mode of the SIR model through the process of learning. Similar results can be obtained in Figure 4 regarding the task of reconstructing the information diffusion processes.

## 5 Conclusions

In this paper, we introduced a reversible residual network block based on the relationship between diffusion phenomena and message passing neural networks, while ensuring the reversibility of the network by limiting its Lipshitz coefficient. Using this, we constructed a discrete denoising diffusion model (DDMSL) which can locate the source of graph diffusion and restore the diffusion paths. Extensive experiments on five real datasets have demonstrated the effectiveness of DDMSL and its constituent modules. Our work offers insights into how to calculate the distribution of solutions to graph diffusion inverse problems based on the information propagation laws on complex networks.

Solving the inverse problem of graph diffusion plays a crucial role in many social operations, including controlling the spread of infectious diseases, rumors, and computer viruses. It provides valuable insights on enhancing source detection performance and fills the gap in methods for recovering diffusion evolution. However, our work has some limitations. For instance, DDMSL requires the prior knowledge about propagation models, such as infection rate and recovery rate. Although we can infer these parameters from existing observation data [32; 21], it limits the application of DDMSL in situations with insufficient observations of propagation conditions. Our future research will focus on reducing the dependence of DDMSL on prior conditions.

## 6 Acknowledgement

This work was supported in part by the National Key R&D Program of China under Grant No. 2021YFC3300302, the National Natural Science Foundation of China (Grant No. 62202089, 72192832 and U22A2004), the General project of Liaoning Provincial Department of Education (Grant No. LJKZ0005), the Shanghai Rising-Star Program (Grant No. 23QA1403100), the Natural Science Foundation of Shanghai (Grant No. 21ZR1421900), and the Fundamental Research Funds for the Central Universities (Grant No. N2319004).

## References

* [1]O. Adeyinka (2008) Internet attack methods and internet security technology. In 2008 Second Asia International Conference on Modelling & Simulation (AMS), pp. 77-82. Cited by: SS1.
* [2]J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. van den Berg (2021) Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems34, pp. 17981-17993. Cited by: SS1.
* [3]S. Banerjee, M. Jenamani, and D. K. Pratihar (2020) A survey on influence maximization in a social network. Knowledge and Information Systems62, pp. 3417-3455. Cited by: SS1.
* [4]J. Behrmann, W. Grathwohl, R. Chen, D. Duvenaud, and J. H. Jacobsen (2019) Invertible residual networks. In International Conference on Machine Learning, Cited by: SS1.
* [5]M. Boguna, R. Pastor-Satorras, A. Diaz-Guilera, and A. Arenas (2004) Models of social networks based on social distance attachment. Physical review E70 (5), pp. 056122. Cited by: SS1.
* [6]Z. Cao, K. Han, and J. Zhu (2021) Information diffusion prediction via dynamic graph neural networks. In 2021 IEEE 24th international conference on computer supported cooperative work in design (CSCWD), pp. 1099-1104. Cited by: SS1.
* [7]H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye (2022) Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687. Cited by: SS1.
* [8]G. Cutura, B. Li, A. Swami, and S. Segarra (2021) Deep demixing: reconstructing the evolution of epidemics using graph neural networks. In 2021 29th European Signal Processing Conference (EUSIPCO), pp. 2204-2208. Cited by: SS1.
* [9]M. Dong, B. Zheng, N. Quoc Viet Hung, H. Su, and G. Li (2019) Multiple rumor source detection with graph convolutional networks. In Proceedings of the 28th ACM international conference on information and knowledge management, pp. 569-578. Cited by: SS1.
* [10]F. Gao, J. Zhang, and Y. Zhang (2022) Neural enhanced dynamic message passing. In International Conference on Artificial Intelligence and Statistics, pp. 10471-10482. Cited by: SS1.
* [11]J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl (2017) Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263-1272. Cited by: SS1.
* [12]P. M. Gleiser and L. Danon (2003) Community structure in jazz. Advances in complex systems6 (04), pp. 565-573. Cited by: SS1.
* [13]H. Gouk, E. Frank, B. Pfahringer, and M. J. Cree (2021) Regularisation of neural networks by enforcing lipschitz continuity. Machine Learning110 (2), pp. 393-416. Cited by: SS1.
* [14]K. Konstantin Haefeli, K. Martikus, N. Perraudin, and R. Wattenhofer (2022) Diffusion models for graphs benefit from discrete state spaces. arXiv preprint arXiv:2210.01549. Cited by: SS1.
* [15]Q. He, H. Fang, J. Zhang, and X. Wang (2021) Dynamic opinion maximization in social networks. IEEE Transactions on Knowledge and Data Engineering35 (1), pp. 350-361. Cited by: SS1.
* [16]J. Ho, A. Jain, and P. Abbeel (2020) Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems33, pp. 6840-6851. Cited by: SS1.
* [17]E. Jang, S. Gu, and B. Poole (2016) Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144. Cited by: SS1.
* [18]D. Kempe, J. Kleinberg, and E. Tardos (2003) Maximizing the spread of influence through a social network. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 137-146. Cited by: SS1.

* [19] William Ogilvy Kermack and Anderson G McKendrick. A contribution to the mathematical theory of epidemics. _Proceedings of the royal society of london. Series A, Containing papers of a mathematical and physical character_, 115(772):700-721, 1927.
* [20] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _ArXiv_, abs/1609.02907, 2017.
* [21] Daniel B Larremore, Bailey K Fosdick, Kate M Bubar, Sam Zhang, and Yonatan H Grad. Estimating sars-cov-2 seroprevalence and epidemiological parameters with uncertainty from serological surveys. _eLife Sciences_, 10, 2021.
* [22] Chen Ling, Junji Jiang, Junxiang Wang, and Zhao Liang. Source localization of graph diffusion via variational autoencoders for graph inverse problems. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1010-1020, 2022.
* [23] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11461-11471, 2022.
* [24] Wuqiong Luo and Wee Peng Tay. Estimating infection sources in a network with incomplete observations. In _2013 IEEE Global Conference on Signal and Information Processing_, pages 301-304. IEEE, 2013.
* [25] Wuqiong Luo and Wee Peng Tay. Estimating infection sources in a network with incomplete observations. In _2013 IEEE Global Conference on Signal and Information Processing_, pages 301-304. IEEE, 2013.
* [26] Wuqiong Luo, Wee Peng Tay, and Mei Leng. How to identify an infection source with limited observations. _IEEE Journal of Selected Topics in Signal Processing_, 8(4):586-597, 2014.
* [27] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the construction of internet portals with machine learning. _Information Retrieval_, 3:127-163, 2000.
* [28] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. _arXiv preprint arXiv:1802.05957_, 2018.
* [29] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* [30] B Aditya Prakash, Jilles Vreeken, and Christos Faloutsos. Spotting culprits in epidemics: How many and which ones? In _2012 IEEE 12th International Conference on Data Mining_, pages 11-20. IEEE, 2012.
* [31] Maciel M Queiroz, Dmitry Ivanov, Alexandre Dolgui, and Samuel Fosso Wamba. Impacts of epidemic outbreaks on supply chains: mapping a research agenda amid the covid-19 pandemic through a structured literature review. _Annals of operations research_, pages 1-38, 2020.
* [32] Jonathan M. Read, Jessica R. E. Bridgen, Derek A. T. Cummings, Antonia Ho, and Chris P. Jewell. Novel coronavirus 2019-ncov (covid-19): early estimation of epidemiological parameters and epidemic size estimates. _Philosophical Transactions of the Royal Society B: Biological Sciences_, 376(1829):20200265-, 2021.
* [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [34] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [35] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. _IEEE transactions on neural networks_, 20(1):61-80, 2008.

* [36] Hao Sha, Mohammad Al Hasan, and George Mohler. Source detection on networks using spatial temporal graph convolutional networks. In _2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA)_, pages 1-11. IEEE, 2021.
* [37] Chintan Shah, Nima Dehmamy, Nicola Perra, Matteo Chinazzi, Albert-Laszlo Barabasi, Alessandro Vespignani, and Rose Yu. Finding patient zero: Learning contagion source with graph neural networks. _CoRR_, abs/2006.11913, 2020.
* [38] Xincheng Shu, Bin Yu, Zhongyuan Ruan, Qingpeng Zhang, and Qi Xuan. Information source estimation with multi-channel graph neural network. In _Graph Data Mining_, pages 1-27. Springer, 2021.
* [39] Tristan SW Stevens, Jean-Luc Robert, Faik C Yu, Jun Seob Shin, and Ruud JG van Sloun. Removing structured noise with diffusion models. _arXiv preprint arXiv:2302.05290_, 2023.
* [40] Xiaolu Tang, Changcheng Wu, Xiang Li, Yuhe Song, Xinmin Yao, Xinkai Wu, Yuange Duan, Hong Zhang, Yirong Wang, Zhaohui Qian, et al. On the origin and continuing evolution of sars-cov-2. _National science review_, 7(6):1012-1023, 2020.
* [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [42] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. _arXiv preprint arXiv:2209.14734_, 2022.
* [43] Junxiang Wang, Junji Jiang, and Liang Zhao. An invertible graph diffusion neural network for source localization. In _Proceedings of the ACM Web Conference 2022_, pages 1058-1069, 2022.
* [44] Zheng Wang, Chaokun Wang, Jisheng Pei, and Xiaojun Ye. Multiple source detection without knowing the underlying propagation model. In _Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence_, AAAI'17, page 217-223. AAAI Press, 2017.
* [45] Duncan J Watts and Steven H Strogatz. Collective dynamics of'small-world'networks. _nature_, 393(6684):440-442, 1998.
* [46] Wenwen Xia, Yuchen Li, Jun Wu, and Shenghong Li. Deepis: Susceptibility estimation on social networks. In _Proceedings of the 14th ACM International Conference on Web Search and Data Mining_, pages 761-769, 2021.
* [47] KiJung Yoon, Renjie Liao, Yuwen Xiong, Lisa Zhang, Ethan Fetaya, Raquel Urtasun, Richard Zemel, and Xaq Pitkow. Inference in probabilistic graphical models by graph neural networks. In _2019 53rd Asilomar Conference on Signals, Systems, and Computers_, pages 868-875. IEEE, 2019.
* [48] Wayne W Zachary. An information flow model for conflict and fission in small groups. _Journal of anthropological research_, 33(4):452-473, 1977.
* [49] Wenyu Zang, Peng Zhang, Chuan Zhou, and Li Guo. Locating multiple sources in social networks under the sir model: A divide-and-conquer approach. _Journal of Computational Science_, 10:278-287, 2015.
* [50] Laijun Zhao, Hongxin Cui, Xiaoyan Qiu, Xiaoli Wang, and Jiajia Wang. Sir rumor spreading model in the new media age. _Physica A: Statistical Mechanics and its Applications_, 392(4):995-1003, 2013.
* [51] Linhua Zhou and Meng Fan. Dynamics of an sir epidemic model with limited medical resources revisited. _Nonlinear Analysis: Real World Applications_, 13(1):312-324, 2012.
* [52] Kai Zhu, Zhen Chen, and Lei Ying. Catch'em all: Locating multiple diffusion sources in networks with partial observations. In _Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence_, AAAI'17, page 1676-1682. AAAI Press, 2017.

* [53] Linhe Zhu, Gui Guan, and Yimin Li. Nonlinear dynamical analysis and control strategies of a network-based sis epidemic model with time delay. _Applied Mathematical Modelling_, 70:512-531, 2019.
* [54] Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria Liakata, and Rob Procter. Detection and resolution of rumours in social media: A survey. _ACM Computing Surveys (CSUR)_, 51(2):1-36, 2018.

Broader impacts

Overall, our work offers valuable insights into how to limit the spread of malicious information. For example, by tracing the spread of infectious diseases, we can identify potential contacts of infected individuals, effectively containing the outbreak. However, it is important to consider the potential negative implications of this approach for society, such as compromising the privacy of individuals living with infectious diseases like HIV. Addressing these concerns should be important.

## Appendix B Derivation of forward and backward processes

**Derivation of forward processes**. Extending Equation 4 by means of the Markov property:

\[\begin{split} q\left(\bm{x}_{t}^{i}\mid\bm{x}_{0}^{i}\right)& =\sum_{\bm{x}_{1:t-1}^{i}}\prod_{k=1}^{t}q\left(\bm{x}_{k}^{i} \mid\bm{x}_{k-1}^{i}\right)\\ &=\sum_{\bm{x}_{1:t-1}^{i}}\prod_{k=1}^{t}\bm{x}_{k-1}^{i}Q_{k}^{ i}\bm{x}_{\bm{k}}^{t\,T}\\ &=\sum_{\bm{x}_{1:t-1}^{i}}\bm{x}_{0}^{i}Q_{1}^{i}\bm{x}_{\bm{1} }^{i\,T}\cdots\bm{x}_{\bm{k-1}}^{i}Q_{k}^{i}\bm{x}_{\bm{k}}^{i\,T}\cdots\bm{x}_ {t-1}^{i}Q_{t}^{i}\bm{x}_{\bm{t}}^{i\,T}\\ &=\bm{x}_{0}^{i}Q_{1}^{i}\left(\sum_{\bm{x}_{1}^{i}}\bm{x}_{\bm{1 }}^{i\,T}\bm{x}_{1}^{i}\right)\cdots\left(\sum_{\bm{x}_{t-1}^{i}}\bm{x}_{\bm{t -1}}^{i\,T}\bm{x}_{t-1}^{i}\right)Q_{t}^{i}\bm{x}_{\bm{t}}^{i\,T}\\ &=\bm{x}_{0}^{i}Q_{1}^{i}IQ_{1}^{i}\cdots IQ_{k}^{i}I\cdots IQ_{ t}^{i}\bm{x}_{\bm{t}}^{i\,T}\\ &=\bm{x}_{0}^{i}\bar{Q}_{t}^{i}\bm{x}_{\bm{t}}^{i\,T}\sim\mathrm{Cat }\left(\bm{x}_{t}^{i};\bm{p}=\bm{x}_{0}^{i}\bar{Q}_{t}^{i}\right)\end{split}\] (18)

Where \(I\) is the identity matrix.

**Derivation of backward processes**. The detailed derivation of Equation 6 is as follows:

\[\begin{split} q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i},\bm{x}_ {0}^{i}\right)&=\frac{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{t-1}^{i}, \bm{x}_{0}^{i}\right)q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{0}^{i}\right)}{q\left( \bm{x}_{t}^{i}\mid\bm{x}_{0}^{i}\right)}\\ &=\frac{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{t-1}^{i}\right)q\left( \bm{x}_{t-1}^{i}\mid\bm{x}_{0}^{i}\right)}{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{0}^ {i}\right)}\\ &=\frac{\bm{x}_{t-1}^{i}Q_{t}^{i}\bm{x}_{\bm{t}}^{i\,T}\cdot\bm{x} _{0}^{i}\bar{Q}_{t-1}^{i}\bm{x}_{\bm{t-1}}^{i\,T}}{\bm{x}_{\bm{0}}^{i}\bar{Q} _{t}^{i}\bm{x}_{\bm{t}}^{i\,T}}\\ &=\frac{\left(\bm{x}_{t}^{i}Q_{t}^{i\,T}\bm{x}_{\bm{t-1}}^{i\,T} \right)\cdot\left(\bm{x}_{0}^{i}\bar{Q}_{t-1}^{i}\bm{x}_{\bm{t-1}}^{i\,T} \right)}{\bm{x}_{0}^{i}\bar{Q}_{t}^{i}\bm{x}_{\bm{t}}^{i\,T}}\\ &=\frac{\left(\bm{x}_{t}^{i}Q_{t}^{i\,T}\right)\odot\left(\bm{x}_ {0}^{i}\bar{Q}_{t-1}^{i}\right)\left(\bm{x}_{\bm{t-1}}^{i\,T}\right)}{\bm{x}_ {0}^{i}\bar{Q}_{t}^{i}\bm{x}_{\bm{t}}^{i\,T}}\\ &\sim\mathrm{Cat}\left(\bm{x}_{t-1}^{i};\bm{p}=\frac{\left(\bm{x} _{t}^{i}Q_{t}^{i\,T}\right)\odot\left(\bm{x}_{0}^{i}\bar{Q}_{t-1}^{i}\right)}{ \bm{x}_{0}^{i}\bar{Q}_{t}^{i}\bm{x}_{\bm{t}}^{i\,T}}\right)\end{split}\] (19)From the Bayesian formula, it follows that:

\[\begin{split} q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i},\bm{x}_{0}^{ i}\right)=q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i}\right)&=\frac{ \sum_{\bm{x}_{0}^{i}}q\left(\bm{x}_{t-1}^{i},\bm{x}_{t}^{i},\bm{x}_{0}^{i} \right)}{q\left(\bm{x}_{t}^{i}\right)}\\ &=\frac{\sum_{\bm{x}_{0}^{i}}q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t }^{i},\bm{x}_{0}^{i}\right)q\left(\bm{x}_{0}^{i}\mid\bm{x}_{t}^{i}\right)q \left(\bm{x}_{t}^{i}\right)}{q\left(\bm{x}_{t}^{i}\right)}\\ &=\sum_{\bm{x}_{0}^{i}}q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i}, \bm{x}_{0}^{i}\right)q\left(\bm{x}_{0}^{i}\mid\bm{x}_{t}^{i}\right)\\ &=\mathbb{E}_{q\left(\bm{x}_{0}^{i}\mid\bm{x}_{t}^{i}\right)}q \left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i},\bm{x}_{0}^{i}\right)\end{split}\] (20)

Fitting this distribution using a neural network:

\[\begin{split} q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i}\right) \approx p_{\theta}\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i}\right)& =\mathbb{E}_{\bm{x}_{0}^{i}\sim p_{\theta}\left(\bm{x}_{0}^{i} \mid\bm{x}_{t}^{i}\right)}q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{t}^{i},\bm{x}_{0 }^{i}\right)\\ &=\mathbb{E}_{p_{\theta}\left(\bm{x}_{0}^{i}\mid\bm{x}_{t}^{i} \right)}\frac{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{t-1}^{i},\bm{x}_{0}^{i}\right)q \left(\bm{x}_{t-1}^{i}\mid\bm{x}_{0}^{i}\right)}{q\left(\bm{x}_{t}^{i}\mid \bm{x}_{0}^{i}\right)}\\ &=\mathbb{E}_{p_{0}\left(\bm{x}_{0}^{i}\mid\bm{x}_{t}^{i}\right)} \frac{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{t-1}^{i}\right)q\left(\bm{x}_{t-1}^{i} \mid\bm{x}_{0}^{i}\right)}{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{0}^{i}\right)}\\ &=\frac{\sum_{j}\left[q\left(\bm{x}_{t}^{i}\mid\bm{x}_{t-1}^{i} \right)q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{0}^{i}\right)p_{\theta}\left(\bm{x }_{0}^{i}\mid\bm{x}_{t}^{i}\right)\right]}{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{0 }^{i}\right)}\\ &=\frac{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{t-1}^{i}\right)\left[ \sum_{j}q\left(\bm{x}_{t-1}^{i}\mid\bm{x}_{0}^{i}\right)p_{\theta}\left(\bm{x }_{0}^{i}\mid\bm{x}_{t}^{i}\right)\right]}{q\left(\bm{x}_{t}^{i}\mid\bm{x}_{0 }^{i}\right)}\\ \end{split}\] (21)

## Appendix C Propagation rule constraint of information diffusion reconstruction

To investigate the circumstances under which the propagation rules may be violated, let's revisit Equation 6. Note that the denominator serves as the normalization term, while the numerator is composed of two key terms - \(\left(\bm{x}_{t}^{i}{Q_{t}^{i}}^{T}\right)\) and \(\left(\bm{x}_{0}^{i}\bar{Q}_{t-1}^{i}\right)\) - that are crucial in preserving the propagation rule.

The three rows of \({Q_{t}^{i}}^{T}\) correspond to the distribution of \(q(x_{t}^{i}|x_{t-1}^{i},x_{0}^{i})\) when \(x_{t}^{i}\) is in one of three states: \(S\), \(I\), and \(R\). Similarly, the three rows of \(\bar{Q}_{t}^{i}\) represent the distribution of \(q(x_{t-1}^{i}|x_{0}^{i})\) when \(x_{0}^{i}\) is in one of three states: \(S\), \(I\), and \(R\). Let \(\left[\bar{Q}_{t}^{i}\right]_{12}\) and \(\left[\bar{Q}_{t}^{i}\right]_{13}\) be denoted by \(q_{t}^{a}\) and \(q_{t}^{b}\) respectively, then \(q_{1}^{a}=\gamma_{1}^{i},q_{1}^{b}=0\).

\[\bar{Q}_{t}^{i}=\begin{bmatrix}\prod_{k=1}^{t}(1-\beta_{k}^{i})&\prod_{k=1}^{ t-1}(1-\beta_{k}^{i})\beta_{t}^{i}+q_{t-1}^{a}(1-\gamma_{t}^{i})&q_{t-1}^{a}( \gamma_{t}^{i})+q_{t-1}^{b}\\ 0&\prod_{k=1}^{t}(1-\gamma_{k}^{i})&\sum_{j=1}^{t}\prod_{k=1}^{t-j}(1-\gamma_ {k}^{i})\gamma_{k+1}^{i}\\ 0&0&1\end{bmatrix}\] (22)

When \(x_{t}^{i}\) and \(x_{0}^{i}\) are in different states, the results of \(q(x_{t-1}^{i}|x_{t}^{i},x_{0}^{i})=\left(\bm{x}_{0}^{i}{Q_{t}^{i}}^{T}\right) \odot\left(\bm{x}_{0}^{i}\bar{Q}_{t-1}^{i}\right)\) are shown in Table 4. The table reveals that \(q(x_{t-1}^{i}|x_{t}^{i},x_{0}^{i})=\left[0,0,0\right]\) when the propagation rule is violated. Since we are predicting \(x_{0}^{i}\), there are only two possible states for \(x_{0}^{i}\): \(S\) (Susceptible) and \(I\) (Infected), with \(x_{0}^{i}=R\) (Recovered) being excluded. In such instances, \(q(x_{t-1}^{i}|x_{t}^{i},x_{0}^{i})\) can be set to \([1,0,0]\) to resolve the issue. Furthermore, if \(x_{t}^{i}=R\) and \(x_{0}^{i}=S\), then \(x_{t-1}^{i}=S\) would violate the propagation rules. However, as shown in the \(9th\) row of Table 4, the probability of \(x_{t-1}^{i}=S\) is much smaller than that of \(x_{t-1}^{i}=I\) and \(x_{t-1}^{i}=R\), and such situations will not lead to training failure. Hence, there is no need for any specific handling of this scenario.

To further minimize propagation rule violations during the training process, we incorporate supervision of the propagation rule. Specifically, when using this supervision function, nodes that have a state of \(R\) are set to \(I\) to enforce the propagation rule.

\[L_{constrain1}=Relu\left(\mathbf{X_{t-1}}-(\mathbf{A}+\mathbf{I})\mathbf{X_{0}}\right)\] (23)where \((\mathbf{A}+\mathbf{I})\mathbf{X_{0}}\) represents the total number of infected nodes in a given node's first-order neighborhood. Equation 23 penalizes \(\mathbf{X_{0}}\) if the node is infected and there are no other infected nodes within its first-order neighborhood. To maintain the stability of inferred \(\mathbf{X_{0}}\) generating \(\mathbf{X_{t-1}}\), we apply the same constraint to the process. Specifically, we utilize the monotonicity regularization of information diffusion from [22]. If the source set \(\mathbf{X_{0}^{(i)}}\) is a superset of \(\mathbf{X_{0}^{(j)}}\), then the generated \(\mathbf{X_{t-1}}\) resulting from their diffusion needs to satisfy the following equation.

\[L_{constrain2}=\left\|\max\left(\mathbf{0},\mathbf{X_{t-1}^{(j)}}-\mathbf{X_{ t-1}^{(i)}}\right)\right\|^{2},\forall\mathbf{X_{0}^{(i)}}\supseteq\mathbf{X_{0}^{(j)}}\] (24)

## Appendix D Proofs of lemmas and theorems

### The proof of theorem 3.1

**Proof** _Set the initial infection seed set as: \(\mathbb{S}=\{x_{0}^{s_{1}},x_{0}^{s_{2}},\ldots x_{0}^{s_{m}}\}\). At the initial moment, the infection status distribution of node \(i\) is:_

\[\left\{\begin{array}{l}P_{I}^{i=S_{m}}(0)=0,P_{I}^{i=S_{m}}(0)=1\\ P_{S}^{i\neq S_{m}}(0)=1,P_{I}^{i\neq S_{m}}(0)=0\\ P_{R}^{i}(0)=0\end{array}\right.\] (25)

_At time \(t\), the infection status distribution of node \(i\) is:_

\[\left\{\begin{array}{l}P_{I}^{i}(t)=P_{I}^{i}(t-1)(1-\gamma_{R}^{i}(t-1))+P_ {S}^{i}(t-1)\left[1-\prod_{j}(1-\beta_{I}^{j}(t))A_{ij}P_{I}^{j}(t-1)\right]\\ P_{S}^{i}(t)=P_{S}^{i}(t-1)\left[\prod_{j}(1-\beta_{I}^{j}(t))A_{ij}P_{I}^{j}(t- 1)\right]\\ P_{R}^{i}(t)=P_{I}^{i}(t-1)\gamma_{R}^{i}(t)\end{array}\right.\] (26)

_where \(A\) is the adjacency matrix and \(j\) is the neighbor of node \(i\). When dealing with a static graph \(\mathcal{G}\), \(A\) is fixed, allowing for determination of the state distribution at any time based on the initial node state. Specifically, if both graph \(G\) and the seed node set \(\mathbb{S}\) are known, it becomes possible to calculate the state distribution of each node at any given time using Equation 26. Utilizing Equations 2 and 3, \(Q_{t}^{i}\) can also be determined._

### Proof of lemma 3.1

**Proof** _The graph convolution layer with batch normalization \(\mathbf{BN}\left(g(\xi)\right)\) can be abbreviated as \(GConv\). In our approach, we apply spectral normalization to both the linear transformation \(\mathbf{U}\) and convolutional layers \(GConv\). As a result, the weight parameters of both the networks \(f_{w}\) and \(GConv\) possess 1-Lipschitz continuity after spectral normalization, as described in [28]._

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline \multirow{2}{*}{\(x_{t}^{i}\)} & \multirow{2}{*}{\(x_{0}^{i}\)} & \multicolumn{3}{c|}{\(q(x_{t-1}^{i}|x_{t}^{i},x_{0}^{i})\)} \\ \cline{2-4}  & & \multicolumn{1}{c|}{S} & \multicolumn{1}{c|}{I} & \multicolumn{1}{c|}{R} \\ \hline  & \(\mathbf{S}\) & \(\prod_{k=1}^{t}(1-\beta_{k}^{i})\prod_{k=1}^{t-1}(1-\beta_{k}^{i})\) & 0 & 0 \\ \cline{2-4} S & 1 & 0 & 0 & 0 \\ \cline{2-4}  & \(\mathbf{R}\) & 0 & 0 & 0 \\ \hline  & \(\mathbf{S}\) & \(q_{t}^{a}\prod_{k=1}^{t-1}(1-\beta_{k}^{i})\) & \(\prod_{k=1}^{t}(1-\gamma_{k}^{i})q_{t-1}^{a}\) & 0 \\ \cline{2-4} I & 1 & 0 & \(\prod_{k=1}^{t}(1-\gamma_{k}^{i})\prod_{k=1}^{t-1}(1-\gamma_{k}^{i})\) & 0 \\ \cline{2-4}  & \(\mathbf{R}\) & 0 & 0 & 1 \\ \hline  & \(\mathbf{S}\) & \(q_{t}^{a}\prod_{k=1}^{t-1}(1-\beta_{k}^{i})\) & \(q_{t-1}^{a}\sum_{j=1}^{t}\prod_{k=1}^{t-1}(1-\gamma_{k}^{i})\gamma_{k+1}^{i}\) & \(q_{t-1}^{b}\) \\ \cline{2-4} R & 1 & 0 & \(\sum_{j=1}^{t}\prod_{k=1}^{t-j}(1-\gamma_{k}^{i})\gamma_{k+1}^{i}\) & \(\sum_{j=1}^{t}\prod_{k=1}^{t-j}(1-\gamma_{k}^{i})\gamma_{k+1}^{i}\) \\ \cline{2-4}  & \(\mathbf{R}\) & 0 & 0 & 1 \\ \hline \end{tabular}
\end{table}
Table 4: The distribution of the unnormalized \(q(x_{t-1}^{i}|x_{t}^{i},x_{0}^{i})\) in different cases.

_Let \(GConv:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}\), \(x_{1},x_{2}\in\mathbb{R}^{n}\). Note that the nonlinear activation function \(\sigma(\cdot)\) is set to \(Mish(\cdot)\), which obviously possesses 1-Lipschitz continuity._

\[\begin{split}&\left\|f_{\theta}(x_{1})-f_{\theta}(x_{2})\right\|_ {p}=\left\|\sigma(GConv(x_{1}))-\sigma(GConv(x_{2}))\right\|_{p}\\ &\leq\left\|GConv(x_{1})-GConv(x_{2})\right\|_{p}\\ &<\left\|x_{1}-x_{2}\right\|_{p}\end{split}\] (27)

_where \(\left\|\cdot\right\|_{p}\) represents the p-norm (\(p=2\) or \(p=\infty\)). Therefore, the Lipschitz constant of \(f_{\theta}\) is less than 1._

### Proof of theorem 3.2

Proof.: _To prove that \(\mathbf{Y_{T}}=\mathbf{F}(f_{w}(\mathbf{X_{0}}))\) is reversible, it is necessary to ensure that \(F\) and \(f_{w}\) are reversible [43]._

\[\left\{\begin{array}{l}f_{w}(\mathbf{X_{0}})=\mathbf{X_{0}}+f_{w}(\mathbf{X _{0}})-\mathbf{X_{0}}=\xi\\ \xi+f_{\theta}(\xi)=\mathbf{Y_{T}}\end{array}\right.\Leftrightarrow\left\{ \begin{array}{l}\mathbf{X_{0}}=\xi+\mathbf{X_{0}}-f_{w}(\mathbf{X_{0}})\\ \xi=\mathbf{Y_{T}}-f_{\theta}(\xi)\end{array}\right.\] (28)

_We construct the following iterative formula:_

\[\left\{\begin{array}{l}\mathbf{X_{0}^{k+1}}=\xi+\mathbf{X_{0}^{k}}-f_{w}( \mathbf{X_{0}^{k}}),\mathbf{X_{0}^{0}}=\xi\\ \xi^{k+1}=\mathbf{Y_{T}}-f_{\theta}(\xi^{k}),\xi^{0}=\mathbf{Y_{T}}\end{array} \right.\Rightarrow\left\{\begin{array}{l}\lim_{k\rightarrow\infty}\mathbf{X _{0}^{k}}=\mathbf{X_{0}}\\ \lim_{k\rightarrow\infty}\xi^{k}=\xi\end{array}\right.\] (29)

_By Lemma 2, we can ensure that \(L_{w}<1\) and \(L_{\theta}<1\). Moreover, the Lipschitz constant of \((\mathbf{X_{0}^{k}}-f_{w}(\mathbf{X_{0}^{k}})\) is \(1-L_{w}\), which is less than 1. Thus, when the number of iterations \(k\) is sufficiently large, it follows that the transformation \(\mathbf{Y_{T}}=\mathbf{F}(f_{w}(\mathbf{X_{0}}))\) is reversible according to the Banach fixed point theorem [4]._

### Proof of theorem 3.3

Proof.: _Specifically, Theorem 3.2 proves that \((\mathbf{F_{1}}\circ\mathbf{F_{2}}\circ\ldots\circ\mathbf{F_{n}})(\xi)\) is invertible when \(n=1\). Therefore, we are currently examining whether \((\mathbf{F_{1}}\circ\mathbf{F_{2}}\circ\ldots\circ\mathbf{F_{n}})(\xi)\) retains its reversibility for \(n>1\). To make the notation simpler, we denote \((\mathbf{F_{1}}\circ\mathbf{F_{2}}\circ\ldots\circ\mathbf{F_{1}})(\xi)\) as \(\tilde{\mathbf{F_{i}}}\)._

\[\hat{\mathbf{F_{n}}}=(\mathbf{F_{1}}\circ\mathbf{F_{2}}\circ\ldots\circ \mathbf{F_{n}})(\xi)=\underbrace{\mathbf{D}\mathbf{P}\circ\ldots\circ\mathbf{ D}\mathbf{P}}_{n}\left[\xi+f_{w}(\xi)+\sum_{i=1}^{n-1}f_{w}\left(\hat{ \mathbf{F_{i}}}(\xi)\right)\right]\] (30)

_The application of the dropout function \(\mathbf{D}\mathbf{P}\) will limit the Lipschitz constant of any function \(f\) to \(1-r\) times its original value: \(L_{\mathbf{D}\mathbf{P}(f)}=(1-r)L_{f}\). Additionally, we have \(L_{\mathbf{F_{i}}}\leq\prod_{j=1}^{i}\left(L_{F_{j}}\right)\leq(1-r)^{i}(1+L _{f_{w}})^{i}\)[13]. Therefore, the Lipschitz constant of \(\tilde{\mathbf{F_{n}}}\) is expressed as:_

\[\begin{split}& L_{\tilde{\mathbf{F_{n}}}}\leq(1-r)^{n}\left[1+L_{f_{w} }+\sum_{i=1}^{n-1}L_{f_{w}}\cdot L_{\tilde{\mathbf{F_{i}}}}\right]\\ &\leq(1-r)^{n}\left[1+L_{f_{w}}\sum_{i=0}^{n-1}\left[(1-r)(1+L_{f _{w}})\right]^{i}\right]\\ &=(1-r)^{n}\left[1+L_{f_{w}}\cdot\frac{1-[(1-r)(1+L_{f_{w}})]^{n} }{1-(1-r)(1+L_{f_{w}})}\right]\end{split}\] (31)

_Note that when \(L_{f_{w}}<1\) and \(n>1\), we only need to set \(r=1/2\) to ensure that \(L_{\mathbf{F_{n}}}<1\), hereby guaranteeing that \(\tilde{\mathbf{F_{n}}}\) is reversible._

## Appendix E DDMSL implementation details

The DDMSL approach has been previously explained, and we will now provide further details on the implementation of DDMSL. The linear transform \(\mathbf{U}\) refers to a fully connected layer, and in Figure 2, the dense layer is composed of two fully connected layers that undergo spectral normalization. The final output of the \(nn_{\theta}\) is represented by an \(N\times 1\) matrix, indicating the probability that each node is in an infected state at \(t=0\). We designate nodes with an infection probability higher than a certain threshold as infected nodes.

Given a complete information diffusion instance \(\mathbf{X}=\{\mathbf{X_{0}},\ldots,\mathbf{X_{T}}\}\) where \(\mathbf{X_{t}}=\left\{x_{t}^{1},\ldots,x_{t}^{N}\right\}\), we sample \(t\in\{1,\ldots,T\}\) to be included in the neural network \(nn_{\theta}\) based on the probability distribution \(p(t)\sim\frac{1}{\sum_{t=1}^{T}t}\), and use the sine-cosine position encoding [41] to embed \(t\). The training and inference processes are shown in Algorithm 1 and Algorithm 2, respectively. Additionally, the variable \(T\) remains consistent with the information diffusion step size.

``` Input:\(\mathbf{X_{0}}\), \(\mathcal{G}\), \(\mathbf{X_{t}}\), threshold \(\alpha\)
1repeat
2\(\mathbf{Q_{t}}\leftarrow\) Equation \(2\) and Equation \(3\)
3\(q(x_{t-1}|x_{t})\leftarrow\) Calculate Equation \(6\) using \(\mathbf{X_{t}}\) and \(\mathbf{Q_{t}}\)
4\(t\sim\mathrm{P}(\{1,\ldots,T\}),P(t)=\frac{t}{\sum_{t=1}^{T}t}\)
5\(\mathbf{X_{0}^{t-1}}=nn_{\theta}(\mathcal{G},\mathbf{X_{t}},t)\)
6// Using data from \(t\), \(nn_{\theta}\) infers the source node \(\mathbf{X_{0}^{t-1}}\), which is then used to reconstruct the diffusion graph at \(t-1\).
7\(\mathbf{X_{0}^{t-1}}[\mathbf{X_{0}^{t-1}}>\alpha]=1\)
8\(\mathbf{X_{0}^{t-1}}[\mathbf{X_{0}^{t-1}}!=1]=0\)
9\(\mathbf{Q_{t}}\leftarrow\) Equation \(2\) and Equation \(3\) // \(\mathbf{Q_{t}^{t}}\) is generated by \(\mathbf{X_{0}^{t-1}}\).
10\(P_{\theta}(x_{t-1}|x_{t})\leftarrow\) Calculate Equation \(8\) using \(\mathbf{X_{t}}\), \(\mathbf{X_{0}^{t-1}}\) and \(\mathbf{Q_{t}^{{}^{\prime}}}\)
11\(\mathbf{X_{t-1}}\gets Gumbel-Softmax(P_{\theta}(x_{t-1}|x_{t}))\)
12Take gradient descent step on:
13\(\nabla_{\theta}(L_{simple}+L_{constrain})\) ```

**Algorithm 1**Training

``` Input:\(\mathbf{X_{t}}\), \(\mathcal{G}\), Empty set \(\mathbb{X}\), threshold \(\alpha\) Output:\(\mathbb{X}\)
1for\(t\) starts from \(T\) to \(1\)do
2\(\mathbf{X_{0}^{t-1}}=nn_{\theta}(\mathcal{G},\mathbf{X_{t}},t)\)
3\(\mathbf{X_{0}^{t-1}}[\mathbf{X_{0}^{t-1}}>\alpha]=1\)
4\(\mathbf{X_{0}^{t-1}}[\mathbf{X_{0}^{t-1}}!=1]=0\)
5\(\mathbf{Q_{t}}\leftarrow\) Equation \(2\) and Equation \(3\)
6\(P_{\theta}(x_{t-1}|x_{t})\leftarrow\) Calculate Equation \(8\) using \(\mathbf{X_{t}}\), \(\mathbf{X_{0}^{t-1}}\) and \(\mathbf{Q_{t}^{{}^{\prime}}}\)
7\(\mathbf{X_{t-1}}\gets Gumbel-Softmax(P_{\theta}(x_{t-1}|x_{t}))\)
8\(\mathbb{X}[t]\leftarrow\mathbf{X_{t-1}}\)
9 end for ```

**Algorithm 2**Infering

In Algorithm 1, we have \(\mathbf{Q_{t}}=\left\{Q_{t}^{1},\ldots,Q_{t}^{N}\right\}\), where \(Q_{t}^{i}\) can be calculated using Equations 2 and 3. Additionally, the \(P_{S}^{i}(t)\), \(P_{I}^{i}(t)\), and \(P_{R}^{i}(t)\) in Equation 3 can be computed using various methods. Monte Carlo simulations provide the most accurate results, but require at least \(10^{5}\) simulations to be sufficiently precise, leading to a high time complexity. An alternative approach is to utilize a neural network model [10, 47] to learn \(\left[P_{S}^{i}(t),P_{I}^{i}(t),P_{R}^{i}(t)\right]\), which significantly reduces the training time of the model. When applied to the SI model, DDMSL utilizes the state transfer matrix by:

\[Q_{t}^{i}=\begin{bmatrix}1-\beta_{t}^{i}(t)&\beta_{I}^{i}(t)\\ 0&1\end{bmatrix}\] (32)

where \(\mathbf{X_{0}^{t-1}}\) represents the predicted source node using \(\mathbf{X_{t}}\), while \(Q_{t}^{{}^{\prime}}\) is generated using the same method as above. Ultimately, \(P_{\theta}(x_{t-1}|x_{t})\) is calculated using \(\mathbf{X_{0}^{t-1}}\), \(\mathbf{X_{t}}\), and \(\mathbf{X_{0}^{t-1}}\). We obtain \(\mathbf{X_{t-1}}\) by sampling from \(P_{\theta}(x_{t-1}|x_{t})\), and label nodes as \(0(S)\), \(1(I)\), or \(2(R)\) based on their state. Algorithm 2 proceeds in a similar manner to the process described above.

Additional algorithms and dataset parameters

### Hyperparameters of the algorithms

The hyperparameters for each algorithm have been set according to the values shown in Table 5. Any parameter that is not stated as default is common to both SI and SIR models. In the updated version of SLVAE, the original three-layer MLP network encoder was replaced with a three-layer GCN network, resulting in improved performance. For hyperparameters and implementation details of other algorithms, please refer to the corresponding original papers. DDMSL and deep learning comparison algorithms are both running on Windows 10 systems and trained using a 4090 graphics card. The code for DDMSL is already open source, please refer to: https://github.com/Ashenone2/DDMSL.

### Additional details of the datasets

The description of the data sets used for the experiments and their statistics are shown as below:

* **Karate**[48]: It includes a network of interrelationships between 34 members of the Karate club, comprising 34 nodes and 78 edges. The Karate dataset is a real dataset, widely employed in complex network community discovery research.
* **Jazz**[12]: The Jazz dataset is a network dataset that captures the collaborative relationships between jazz musicians. It comprises 198 nodes and 2,742 edges, and has been extensively used in research on complex network community discovery, node importance metrics, and other related studies.
* **Cora-ML**[27]: Cora-ML is a citation network dataset containing papers from the field of machine learning. Nodes represent papers and edges represent citation relationships between papers.
* **Power Grid**[45]: The Power Grid dataset is a network dataset describing the topology of the Northeastern US power grid, containing 4,941 nodes and 6,594 edges.
* **PGP**[5]: It is a User network of the Pretty-Good-Privacy algorithm for secure information exchange, consisting of 10,680 nodes and 24,316 edges.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline Datasets & \#Nodes & \#Edges & \#Avg(degree) & \#Average clustering coefficient & \#Density & \#Diameter \\ \hline Karate & 34 & 78 & 2.29 & 0.57 & 0.14 & 5 \\ \hline Jazz & 198 & 2,742 & 27.70 & 0.62 & 0.14 & 6 \\ \hline Cora\_ml & 2,810 & 7,981 & 5.68 & 0.28 & 0.002 & 17 \\ \hline Power Grid & 4,941 & 6,594 & 1.33 & 0.08 & 0.005 & 46 \\ \hline PGP & 10,680 & 24,316 & 4.55 & 0.27 & 0.0004 & 24 \\ \hline \end{tabular}
\end{table}
Table 6: Statistics of the five datasets.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline
**Algorithms** & **Hyper-parameter** & **karate** & ** Jazz** & **cora\_ml** & **power grid** & **PGP** & **Search space** & **Description** \\ \hline \multirow{6}{*}{DDMSL} & Initial learning rate & \(2\times 10^{-3}\) & \(2\times 10^{-3}\) & \(2\times 10^{-3}\) & \(2\times 10^{-3}\) & \(3\times 10^{-3}\) & \(2\times 10^{-3}\),\(3.4\times 10^{-3}\),\(5\times 10^{-3}\) \\  & Learning Rate & \(\begin{bmatrix}1200,1500\\ \end{bmatrix}\) & \(\begin{bmatrix}200,1000\\ \end{bmatrix}\) & \(\begin{bmatrix}500,1200\\ \end{bmatrix}\) & \(\begin{bmatrix}500,1200\\ \end{bmatrix}\) & \(\begin{bmatrix}500,1200\\ \end{bmatrix}\) & \(\begin{bmatrix}200,5000,800,1200\\ \end{bmatrix}\) & \(\begin{bmatrix}200,5000,8000,1200\\ \end{bmatrix}\) \\  & In & 6 & 6 & 6 & 8 & \(\begin{bmatrix}min=3,max=9,step=1\\ \end{bmatrix}\) & Number of \\  & Dropout rate & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & Determinated by Thoosen (3,3) \\  & \(\alpha\) in SIR model & 0.4 & 0.6 & 0.4 & 0.4 & 0.45 & \(\begin{bmatrix}min=3,max=7,step=0.05\\ \end{bmatrix}\) & Threshold \\  & In & 0.45 & 0.4 & 0.4 & 0.45 & \(\begin{bmatrix}min=3,max=7,step=0.05\\ \end{bmatrix}\) & Threshold \\  & From & 2000 & 1600 & 1600 & 1600 & 1600 & 1600 & 1600 \\ \hline \multirow{6}{*}{SLAVE} & \(\alpha\) & 0.55 & 0.5 & 0.5 & 0.55 & 0.45 & \(\begin{bmatrix}min=3,max=9,step=11\\ \end{bmatrix}\) & Threshold \\  & Learning rate & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) & \(\begin{bmatrix}4.4\times 10^{-3}.5\times 10^{-3}\\ \end{bmatrix}\) \\  & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) & \(\begin{bmatrix}2\times 10^{-3}\\ \end{bmatrix}\) \\  & In & & & & & & \\  & In & & & &

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]

Figure 6: DDMSL reconstructs SIR diffusion on Jazz.

Figure 7: DDMSL reconstructs SIR diffusion on Coral ml.

Figure 8: DDMSL reconstructs SIR diffusion on Power grid.

Figure 9: DDMSL reconstructs SIR diffusion on PGP.

Figure 11: Visualization comparisons of source localization on Jazz.

Figure 10: Visualization comparisons of source localization on Karate.