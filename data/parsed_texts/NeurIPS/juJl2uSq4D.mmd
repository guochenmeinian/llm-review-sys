# RL in Latent MDPs is Tractable:

Online Guarantees via Off-Policy Evaluation

 Jeongyeol Kwon

University of Wisconsin-Madison

jeongyeol.kwon@wisc.edu

Shie Mannor

Technion / NVIDIA AI

shie@ee.technion.ac.il

Constantine Caramanis

University of Texas at Austin

constantine@utexas.edu

Yonathan Efroni

Meta AI

jonathan.efroni@gmail.com

###### Abstract

In many real-world decision problems there is partially observed, hidden or latent information that remains fixed throughout an interaction. Such decision problems can be modeled as Latent Markov Decision Processes (LMDPs), where a latent variable is selected at the beginning of an interaction and is not disclosed to the agent. In the last decade, there has been significant progress in solving LMDPs under different structural assumptions. However, for general LMDPs, even in the tabular case, no algorithm is known to provably match the existing lower bound [41]. We introduce the first sample-efficient algorithm for LMDPs without _any additional distributional assumptions_. Our result builds off a new perspective on the role of off-policy evaluation guarantees and coverage coefficients in LMDPs, a perspective, that has been overlooked in the context of exploration in partially observed environments. Specifically, we establish a novel off-policy evaluation lemma and introduce a new coverage coefficient for LMDPs. Then, we show how these can be used to derive near-optimal guarantees of an optimistic exploration algorithm. These results, we believe, can be valuable for a wide range of interactive learning problems beyond LMDPs, and especially, for partially observed environments.

## 1 Introduction

In Reinforcement Learning (RL) [54], an agent aims to maximize the long-term cumulative rewards through interactions within an _unknown_ environment. Markov Decision Processes (MDPs) are perhaps the most well-studied and popular framework for this goal. As the name suggests, MDPs heavily rely on the Markovian assumption that requires the state to be fully observable. However, many real-world decision problems involve critical partially observed or latent information, such as sensitive or unknown preference information of users in recommendation systems [28], undiagnosed illness in medical treatments [62, 53], and adaptation to uninformed tasks in robotics [67, 48]. Even when such latent factors remain fixed throughout a period of interactions the fundamental Markovian property of MDPs is no longer valid.

A line of work has proposed efficient RL algorithms in the presence of latent contexts [12, 24, 11, 21, 41, 37] within the framework that we here collectively refer to as Latent Markov Decision Processes (LMDP) following [41]. In LMDPs, nature selects an MDP from a finite set of \(M\) candidate MDP models at the beginning of a period of interactions (a.k.a. episode), and an agent interacts with the chosen MDP for \(H\) time steps of an episode (the horizon). However, the identity of the chosen MDP is not given to the agent. We call this unknown identity the _latent context_.

Most prior work on LMDPs has relied on strict separation assumptions (_e.g.,_[11, 24, 37]). The applicability of these approaches is limited to scenarios where the horizon is sufficiently large and identification of the latent model can be guaranteed, _i.e.,_\(H\gg\Omega(SA)\)[24], where \(S\) and \(A\) are the state and action spaces size. Without these explicit horizon requirements, as far we know, all existing algorithms suffer _the curse of horizon_, requiring sample complexity \(\Omega(A^{H})\) - which frequently arises in the more general framework of Partially Observed MDPs (POMDPs) [52, 39]. Without the ability to identify the underlying latent model, it remains unclear how to address the curse of horizon inherent in partially observed systems [39].

Recently, a series of works [40, 42, 43] proposed sample efficient algorithms without separation assumptions when \(M=O(1)\), assuming the transition dynamics of models with different latent context is similar. While this is still a substantial contribution, their results cannot be easily extended to the general LMDP setting with different transition dynamics (see Section 1.1). Consequently, to date, the following question has remained open:

_Can we break the curse of horizon in LMDPs if \(M=O(1)\) without any assumptions?_

In this work we provide the first sample-efficient exploration algorithm for LMDPs without any assumptions. Throughout the paper, we assume that \(H>2M\), and focus on whether we can improve the trivial upper bound that incurs complexity \(\Omega(A^{H})\). Since a \(\Omega(SA)^{M}\) lower bound for LMDPs has been established [41], our goal is to achieve an upper bound of \(\operatorname{poly}(S,A)^{M}\) without any assumptions, namely, to get a matching upper bound up to polynomial factors.

### Technical Challenges

Many online RL algorithms follow a similar pattern. They make use of a confidence set - a set of candidate models (hypothesis) that can explain the observed data with high probability - and execute a policy that will shrink the volume of the confidence set is produced and executed [6, 38, 27, 7, 33, 36]. The entirety of the statistical problem is to analyze the decaying rate of confidence sets under proper model class assumptions [49, 34, 19].

Challenge 1: Limitation of Existing POMDP Algorithms.Existing approaches for online exploration in partially observed systems largely fall into the category of Optimistic Maximum Likelihood Estimation (OmLE) [45, 46]. This class of algorithms often requires an assumption that allows the construction of shrinking confidence sets. These algorithms also assume access to a set of special policies - called _core-tests_ - to be executed to generate trajectories [5, 8, 16, 17, 45, 57, 13, 22, 46, 26]. Without specifying the proper core-tests, the volume of confidence sets may not decay in a desired rate, leading to the curse of horizon \(\Omega(A^{H})\)[39, 15]. Further, existing POMDP approaches require an ability to recover the belief of the underlying model from observations, _e.g.,_ by assuming the distribution of observations when executing the core-tests is invertible to the belief over hidden states. Consequently, existing literature on POMDPs has two limitations: (i) it requires to specify _a priori_ a set of core-tests policies, and (ii) it assumes the full-rankness of the state-observation emission matrix when the core-tests are being executed.

While LMDPs are a special class of POMDPs, neither the existence of a set of core-tests is known _a priori_, nor it is possible to recover the belief over latent contexts from distribution of trajectories (see Section B for details). This creates a fundamental challenge for existing approaches when applied to LMDPs. Further, little is understood on learning a near-optimal policy among "doubly-exponential" number of candidate history-dependent policies without either the visibility of contexts or core-tests. This calls for a new perspective on the question of efficient exploration in LMDPs.

Challenge 2: Limitation of Existing LMDP Algorithms.The work of [43] suggested an alternative strategy to learn a near-optimal policy in LMDPs: the moment-matching approach for exploration in LMDPs. When all contexts share the same state-transition dynamics, the notion of moments can be defined as the joint distribution of rewards under _a fixed prior_ at a tuple of at most \(d:=2M-1\) state-action pairs \(\boldsymbol{x}=\big{(}(s_{[1]},a_{[1]}),...,(s_{[d]},a_{[d]})\big{)}\). This in turn suggests that the exploration algorithm must learn how to visit these length-\(d\) state-action tuples simultaneously, _i.e.,_ find a policy that ensures that \(\boldsymbol{x}\) appears as a subsequence of the entire trajectory with high enough probability.

When the transition dynamics of different latent contexts is similar, reaching optimally to \(d\) state-action pairs is a minor challenge; _e.g.,_ we can first learn the shared transition kernel with any reward-freeexploration scheme for MDPs [33], and then execute the policy that maximizes the probability of reaching the \(d\) state-action pairs. However, for general LMDP, when the transition dynamics of different latent contexts may differ, this approach is no longer available since the latent transition dynamics may not be learnable in general. Furthermore, to follow the notion of moments suggested in [43], the data collected for estimating the correlation tensor must be collected under the same prior (belief) over all latent contexts. Unfortunately, ensuring this for general LMDPs, when the transition dynamics of different latent contexts are not equal, is impossible, since even if we obtain the samples of correlations, different policies may result in different and unknown priors over contexts. These challenges hint we need an alternative approach to solve general LMDPs, when the transition dynamics vary between latent contexts.

Challenge 3: Limitations of Existing Complexity Measures in RL.Numerous studies have examined complexity measures for RL with function approximation or in the rich-observation settings [30; 34; 19]. These studies are based on the Markovian assumption, which does not hold in the LMDP setting where the entire history may be needed to decode the latent state. When defining the effective state as the entire history at each time step, it is unclear how to analyze the complexity measures from these studies without resorting to exponential guarantees in the horizon.

### Overview of Our Contribution

Recent studies have found some fundamental connections between off-policy evaluation (OPE) and online exploration in RL [59; 2; 29; 9; 55; 3; 4]. In this work, we offer a fresh viewpoint, which deviates from existing works, on the connection between OPE and online exploration. This perspective, together with new analysis tools, allows us to provide a sample-efficient algorithm for the LMDP setting. This further showcases the usefulness of OPE for online exploration in POMDPs.

Arguably, the fundamental question in OPE is the following: how much does a behavioral policy \(\psi\) tell about a target policy \(\pi\)? The simplest form of the OPE guarantee in MDPs relies on the notion of _coverage coefficient_ given by:

\[C(\psi;\pi)=\max_{s,a,t}\frac{\mathds{P}^{\pi}(s_{t}=s,a_{t}=a)}{\mathds{P}^{ \psi}(s_{t}=s,a_{t}=a)}.\]

How would this quantity be related to online exploration? A key observation to start developing intuition is the following: an unbounded coverage coefficient, _i.e._, \(C(\psi;\pi)=\infty\) implies there exists a state-action pair, at some time-steps, that cannot be reached under \(\psi\), but can be reached with \(\pi\).

The algorithmic framework we develop in this work builds off OME[46]. In Section 3, we consider the MDP setting to provide intuition of our analysis. There, OME iteratively tests new policies on models from the confidence set which predict different outcomes, until the trajectory distribution of all policies is reliably estimated. Since the number of new state-action pairs is bounded for MDPs, the number of times the coverage coefficient can be large must be bounded during an interaction. We provide new analysis for the MDP setting based on OPE tools.

To apply this approach for LMDPs, we are required to develop a new notion of coverage coefficient and new OPE tools. We propose a coverage coefficient that can be informally described as follows:

\[C(\psi;\pi)=\max_{(\mathcal{E},\mathcal{I})}\max_{m}\frac{\mathds{P}^{\pi}( \mathcal{T}\in\mathcal{E}\mid m)}{\mathds{P}^{\psi}(\mathcal{T}\in\mathcal{E }\mid m,\ \textbf{do}\ \mathcal{I})}\]

Figure 1: Highlevel description of LMDP-OME. In the online phase, we find a new test policy under which models in the confidence set do not agree. Then the exploration policy is constructed with our new notion of _segmentation_ of policies within \(\Psi_{\text{test}}\) that are executed throughout. In the offline phase, we add the batched sample trajectories to dataset and update the confidence set of models.

where \(m\) is the unobserved latent context, \(\mathcal{T}=(s_{t},a_{t},r_{t})_{t\in[H]}\) is a sampled trajectory, \(\mathcal{E}\) is an event of interest, _e.g.,_ visiting length of at most \(d\) tuples of states and actions within an episode, and \(\mathcal{I}\) is an intervention of interest, _e.g.,_ force an action \(a\) at the \(t^{th}\) time step regardless of \(\psi\) (for the formal definition, see Definition 4.1). Note that the coverage coefficient cannot be measured explicitly, since \(m\) is a latent variable; nevertheless, this concept is central to our analysis and our ability to analyze the sample complexity of the proposed algorithm. Its usefulness lies in an OPE guarantee we develop (see Lemma 4.2):

\[\mathtt{TV}(\mathtt{P}_{\theta^{*}}^{\pi},\mathtt{P}_{\theta}^{\pi})(\mathcal{T })\lesssim C(\psi;\pi)\cdot\sum_{\mathcal{I}}\mathtt{TV}(\mathtt{P}_{\theta^{ *}}^{\psi},\mathtt{P}_{\theta}^{\psi})(\mathcal{T}\mid\textbf{do }\mathcal{I}),\]

where \(\mathtt{TV}(\mathtt{P}_{1},\mathtt{P}_{2})(\cdot)\) is the total-variation (TV) distance between two probability measures \(\mathtt{P}_{1},\mathtt{P}_{2}\).

With these tools at hand, we design an iterative online exploration algorithm for the LMDP setting, and prove its sample complexity matches the lower bound, up to polynomial factors. The algorithm, we refer as LMDP-OmLE (see Figure 1 for highlevel illustration), repeats the following: _(i)_ find a policy for which the trajectory distributions between models in the confidence set is large or terminate, or _(ii)_ collect new data with exploration policies constructed with a set of (obtained) test policies and interventions, an exploration strategy for LMDPs that we introduce.

## 2 Preliminaries

We consider an episodic RL with time-horizon \(H\) in LMDPs defined as follows:

**Definition 2.1** (Latent Markov Decision Process (LMDP)): _An LMDP \(\mathcal{M}\) consists of a tuple \((\mathcal{S},\mathcal{A},\mathcal{R},\theta,H)\) with a state space \(\mathcal{S}\); action space \(\mathcal{A}\); reward space \(\mathcal{R}\), and a finite-time horizon \(H\). \(\theta\) is a model parameter consisting of multiple MDPs in the model \(\theta:=\left(\{w_{m},T_{m},R_{m}\}\right)_{m=1}^{M}\). In each \(m^{th}\) MDP, \(T_{m}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]\) maps a state-action pair and a next state to a probability; \(R_{m}:\mathcal{S}\times\mathcal{A}\times\mathcal{R}\rightarrow[0,1]\) is a probability of rewards; \(\{w_{m}\}_{m=1}^{M}\) are the mixing weights such that at the beginning of every episode the \(m^{th}\) model is chosen with probability \(w_{m}\)._

Without loss of generality, we assume that there exists a null state that represents the starting and terminal state \(s_{0}=s_{H+1}=\emptyset\), and a null action at the beginning of an episode \(a_{0}=\emptyset\), even though actual policies do not take any action at the beginning. \(T_{m}(\cdot|s_{0},a_{0})\) is the initial state distribution of the \(m^{th}\) MDP. We assume that the number of latent contexts is constant \(M=O(1)\), and the time-horizon is larger than the number of contexts \(H>2M\). Further, we assume the reward values are finite and bounded:

**Assumption 2.2** (Finite and Bounded Reward): _The reward distribution has finite support with (arbitrarily large) cardinality, and each reward is bounded: \(|r|\leq 1\) for all \(r\in\mathcal{R}\)._

We also note that this concept can be easily generalized to instantaneous observations that include rewards, and thus, we do not lose much generality due to Assumption 2.2. We consider a policy class \(\Pi\) which contains all history-dependent policies \(\pi:\Xi\times(\mathcal{S},\mathcal{A},\mathcal{R})^{*}\times(\mathcal{S} \times[H])\rightarrow\Delta(\mathcal{A})\), where \(\Xi\) is the space of independent variables decided at the beginning of execution. As a special case, we consider the class of memoryless policies: \(\Pi_{\mathrm{mls}}:(\mathcal{S}\times[H])\rightarrow\Delta(\mathcal{A})\) We are interested in finding an optimal history dependent policy \(\pi\in\Pi\) that maximizes the expected reward: \(V_{\theta^{*}}^{*}:=\max_{\pi\in\Pi}\mathbb{E}_{\theta^{*}}^{\pi}\left[\sum_{ t=1}^{H}r_{t}\right],\) where \(\theta^{*}\in\Theta\) is the true model parameter and \(\mathbb{E}_{\theta^{*}}^{\pi}[\cdot]\) is expectation taken over the true LMDP model \(\mathcal{M}^{*}\) when policy \(\pi\) is executed.

NotationWe use \([n]:=\{1,\ldots,n\}\) and \([n]_{+}:=\{0\}\cup[n]\). We define \(d:=2M-1\) and assume \(H>2M\). Let \(\mathtt{SubSeq}(H,d)\) be the set of subsequences of \((1,2,...,H)\) with length less than or equal to \(d\), _i.e.,_\(\mathtt{SubSeq}(H,d):=\{(\tau_{1},\tau_{2},...,\tau_{q})|q\in[d],1\leq\tau_{1}<...<\tau_{q}\leq H\}\). We often denote a state-action pair \((s,a)\) as one symbol \(x=(s,a)\in\mathcal{X}=(\mathcal{S}\times\mathcal{A})\), and an reward-next state pair \((r,s^{\prime})\) as one symbol \(y=(r,s^{\prime})\in\mathcal{Y}=(\mathcal{R}\times\mathcal{S})\). We often express the next state at time step \(t\) as either \(s_{t+1}\) or \(s^{\prime}_{t}\), and the pair of instantaneous observation and next state as \(y_{t}=(r_{t},s_{t+1})=(r_{t},s^{\prime}_{t})\). For any segment of a sequence \((z_{1},z_{2},...,z_{H})\) from \(t_{1}\) to \(t_{2}\), we often simplify the notation as \(z_{t_{1}:t_{2}}\). We denote the entire trajectory as \(\mathcal{T}:=(s,a,r)_{1:H}\), and \(\mathcal{T}_{1:t}=((s,a,r)_{1:t-1},s_{t})\) for a history of length \(t\). For any set \(\mathcal{S}\), we define \(\mathcal{S}^{\bigotimes k}\) as a short-hand for the \(k\)-times Cartesian power of \(\mathcal{S}\)```
1:Input:\(n_{\texttt{cent}}\in\mathbb{N},\beta,\epsilon_{\texttt{TV}},\eta>0\), \(\mathcal{C}^{0}=\Theta\)
2:Initialize \(k=0\)
3:while there exists \(\pi^{k}\in\Pi_{\text{mls}}\), and \(\theta_{1},\theta_{2}\in\mathcal{C}^{k}\) such that \(\texttt{TV}\left(\mathds{P}_{\theta_{1}}^{\pi^{k}},\mathds{P}_{\theta_{2}}^{ \pi^{k}}\right)(\mathcal{T})>4\epsilon_{\texttt{TV}}\)do
4: Generate data \(\{\mathcal{T}^{k}_{j}\}_{j=1}^{n_{\texttt{cent}}}\) by executing \(\pi^{k}\), update \(\mathcal{D}^{k}\leftarrow\mathcal{D}^{k-1}\cup\{(\mathcal{T}^{k}_{j},\pi^{k}) \}_{j=1}^{n_{\texttt{cent}}}\)
5: Refine the confidence set with the dataset: \[\mathcal{C}^{k+1}=\Big{\{}\theta\in\Theta\Big{|}\sum_{(\mathcal{T},\pi)\in \mathcal{D}^{k}}\log\mathds{P}_{\theta}^{\pi}(\mathcal{T})\geq\arg\max_{ \theta\in\Theta}\sum_{(\mathcal{T},\pi)\in\mathcal{D}^{k}}\log\mathds{P}_{ \theta}^{\pi}(\mathcal{T})-\beta\Big{\}}\] (1) \(k\gets k+1\)
6:endwhile
7:Pick any \(\theta\in\mathcal{C}^{k}\) and return the optimal policy of \(\mathcal{M}:=(\mathcal{S},\mathcal{A},\mathcal{O},\theta)\).
```

**Algorithm 1**MDP-OmLE

We define \(\texttt{SubTraj}(\mathcal{T},\mathbf{\tau})\subseteq(\mathcal{X}\times\mathcal{Y}) ^{\bigotimes|\mathbf{\tau}|}\) as a valid subsequence of trajectories at time-steps \(\mathbf{\tau}\in\texttt{SubSeq}(H,d)\), _i.e._, if \((x_{\mathbf{\tau}},y_{\mathbf{\tau}})\in\texttt{SubTraj}(\mathcal{T},\mathbf{\tau})\), for any \(i\) such that \(\tau_{i}=\tau_{i+1}\), \(y_{\tau_{i}}=(r_{\tau_{i}},s^{\prime}_{\tau_{i}})\) and \(x_{\tau_{i+1}}=(s_{\tau_{i+1}},a_{\tau_{i+1}})\) must have \(s^{\prime}_{\tau_{i}}=s_{\tau_{i+1}}\).

For a tuple of state-action pairs (or states) of length \(q\), we denote \(\mathbf{x}=(x_{[1]},...,x_{[q]})\) (or \(\mathbf{s}=(s_{[1]},...,s_{[q]})\) with bracketed indices for each element to distinguish from time steps. We use \(|\mathbf{x}|\) for the length of sequence \(\mathbf{x}\). We denote the cardinality of the state and action space as \(S:=|\mathcal{S}|\) and \(A:=|\mathcal{A}|\). For any two models \(\theta_{1},\theta_{2}\), we often denote \(\mathds{P}_{1}(\cdot):=\mathds{P}_{\theta_{1}}(\cdot)\) and \(\mathds{P}_{2}(\cdot):=\mathds{P}_{\theta_{2}}(\cdot)\) whenever the context is clear. We denote \(P_{m}(\cdot)\) for a probability measured conditioned on the context \(m\in[M]\) over the ground-truth model (\(\theta_{1}\) when we compare \(\theta_{1}\) and \(\theta_{2}\)). We denote \(\texttt{Unif}(\mathcal{A})\) as the uniform distribution over a set \(\mathcal{A}\). Let \(\texttt{TV}(\mathds{P}_{1},\mathds{P}_{2})(X)\) be the total-variation distance between two probability measures \(\mathds{P}_{1}(\cdot),\mathds{P}_{2}(\cdot)\) over a random variable \(X\).

## 3 New Perspective on \(\texttt{Omle}\): Online Guarantees via Off-Policy Evaluation

In this section, we present our new approach for analyzing the \(\texttt{Omle}\) algorithm, and, for establishing intuition in the Markovian setting. Differently than prior analysis [45; 46] which is based on the generalized eluder-type condition assumption (see [46], Condition 3.2), we show that a certain type of an OPE guarantee can be used to study the performance of \(\texttt{Omle}\). This alternative perspective is instrumental in designing a sample-efficient algorithm for the LMDP class.

Consider \(\texttt{MDP-OmLE}\) depicted in Algorithm 1. \(\texttt{MDP-OmLE}\) is an adaptation of \(\texttt{Omle}\) for the MDP setting with the goal of learning a near-optimal policy. The algorithm iteratively refines the confidence set, i.e., the set of statistically valid models, until it terminates. Specifically, it iteratively repeats the two steps: _(i)_ find a policy for which the TV distance between trajectory distributions of models in the confidence set is sufficiently large, and _(ii)_ collect data with that policy, and use the data to refine the confidence set. To bound the sample complexity of the algorithm we attempt to upper bound the number of iterations, namely, to bound the number of times the TV distance between trajectory distributions can be sufficiently large.

The following OPE lemma is a tool that allows us to bound the number of iterations of \(\texttt{MDP-OmLE}\). Before discussing its application, we present the result.

**Lemma 3.1** (**Tv Bound via OPE for MDPs**): _For any behavioral and target policies \(\psi,\pi\in\Pi\), let the coverage coefficient be defined by:_

\[C(\psi;\pi)=\max_{t\in[H]}\max_{x\in\mathcal{X}}\frac{\mathds{P}_{\theta^{*}}^ {\pi}(x_{t}=x)}{\mathds{P}_{\theta^{*}}^{\psi}(x_{t}=x)}. \tag{2}\]

_For any two models \(\theta,\theta^{*}\in\Theta\), the TV distance between trajectory distributions following a target policy \(\pi\in\Pi\) is bounded as follows:_

\[\texttt{TV}(\mathds{P}_{\theta^{*}}^{\pi},\mathds{P}_{\theta}^{\pi})(\mathcal{ T})\leq 2C(\psi;\pi)\sum_{t\in[H]}\texttt{TV}(\mathds{P}_{\theta^{*}}^{ \psi},\mathds{P}_{\theta}^{\psi})(x_{t},y_{t}). \tag{3}\]

How can we use this result to bound the number of iterations of \(\texttt{MDP-OmLE}\)? Consider the infinite sample regime, when \(\texttt{MDP-OmLE}\) collects infinite data at each iteration by executing a policy \(\pi^{k}\) on the \(k\)th iteration, _i.e.,_\(n_{\mathtt{test}}=\infty\). Further, assume the algorithm is at the beginning of its \(k+1\) iteration. In the infinite sample regime all models in the confidence set must have matching event distribution relatively to the underlying model measured when policy \(\pi^{k}\) is tested. Specifically, for all \(\theta\in\mathcal{C}^{k}\) and \(t\in[H]\) it holds that \(\mathtt{TV}(\mathds{P}^{\pi^{k}}_{\theta^{*}},\mathds{P}^{\pi^{k}}_{\theta})=0\). Then Lemma 3.1 implies the following: for all policies \(\pi\) for which \(C(\pi^{k};\pi)<\infty\) it also holds that \(\mathtt{TV}(\mathds{P}^{\pi}_{\theta^{*}},\mathds{P}^{\pi}_{\theta})(\mathcal{ T})=0\). Conversely, assume the condition of the while loop at the beginning of the \(k+1\) iteration holds true, namely, there exists a policy \(\bar{\pi}\) for which \(\mathtt{TV}(\mathds{P}^{\bar{\pi}}_{\theta^{*}},\mathds{P}^{\bar{\pi}}_{\theta })(\mathcal{T})>0\). Then Lemma 3.1 also implies that \(C(\pi^{k};\bar{\pi})=\infty\), namely, there exists an \(x\in\mathcal{X}\) and \(t\in[H]\) such that \(\mathds{P}^{\bar{\pi}}_{\theta^{*}}(x_{t}=x)>0\) whereas \(\mathds{P}^{\pi^{k}}_{\theta^{*}}(x_{t}=x)=0\). Next, recall that \(\mathtt{MDP\text{-}O\!MLE}\) sets the data collection policy at the \(k+1\) iteration to be \(\pi^{k+1}=\bar{\pi}\). Hence, the data collection policy at the \(k+1\) iteration will visit some state-action pair at some time step \(\pi^{k}\) did not visit. Hence, in the infinite sample regime, \(\mathtt{MDP\text{-}O\!MLE}\) halts after at most \(HSA\) iterations, as there are at most \(HSA\) different state-action pairs in different time steps.

The intuition presented above is robust to sampling error, _i.e.,_ when \(n_{\mathtt{test}}<\infty\). To simplify the discussion, let us temporarily assume that \(\mathds{P}^{\bar{\pi}}_{\theta^{*}}(x_{t}=x)>\gamma\) for all \(\pi\in\Pi\) and \(x\in\mathcal{S}\times\mathcal{A}\) (we do not require this assumption in our final result by analyzing a perturbed MDP). The key intuition on which the finite sample analysis builds upon is formalized in the following lemma:

**Lemma 3.2** (**Coverage Multiplicative Increase**): _For all \(k>0\) in Algorithm 1, there exists at least one \(t\in[H]\) and \(x\in\mathcal{X}\) such that_

\[\mathds{P}^{\pi^{k}}_{\theta^{*}}(x_{t}=x)\geq c\cdot\frac{\epsilon_{\mathtt{ TV}}}{H}\sqrt{\frac{n_{\mathtt{test}}}{(HSA)\beta}}\cdot\max_{j<k} \mathds{P}^{\pi^{j}}_{\theta^{*}}(x_{t}=x).\]

_with some absolute constant \(c>0\)._

Therefore, by setting the number of samples to be \(n_{\mathtt{test}}\geq(4H^{2}SA\beta)/(c\epsilon_{\mathtt{TV}})^{2}\), we ensure that in every iteration \(\mathtt{MDP\text{-}O\!MLE}\) doubles the coverage of at least one state-action pair at a certain time step. Therefore, the algorithm terminates within at most \(K=O(HSA\cdot\log(1/\gamma))\) iterations with high probability. After termination, we are guaranteed that any two models in the confidence set are \(\epsilon_{\mathtt{TV}}\)-close in TV-distance for any policy, hence we can obtain \(\epsilon=(H\epsilon_{\mathtt{TV}})\)-optimal policy. To summarize, we state the following theorem:

**Theorem 3.3**: _Let \(K=O(HSA)\log(HSA/\epsilon)\) and \(\beta=\log(K|\Theta|/\eta)\). Then, with probability at least, \(1-\delta\), \(\mathtt{MDP\text{-}O\!MLE}\) terminates after \(K\) iterations with at most \(N\) episodes being generated, where_

\[N\geq O(H^{6}S^{2}A^{2})\cdot\log(HSA/\epsilon)\log(K|\Theta|/\eta)/\epsilon^{ 2},\]

_and outputs an \(\epsilon\)-optimal policy with probability at least \(1-\eta\)._

In a typical tabular MDP setting, we take \(O(\log|\Theta|)=\tilde{O}(SA)\), by discretizing the class of MDPs. Hence the sample complexity of \(\mathtt{MDP\text{-}O\!MLE}\) is \(N=\tilde{O}(H^{6}S^{3}A^{3}/\epsilon^{2})\). While this upper bound is suboptimal compared to the minimax rate [7], the appeal of this type of analysis is its ability to bypass the need for analyzing the decaying volume of the constructed confidence sets (Section 1.1, Challenge 1).

## 4 Efficient Exploration in LMDPs

In previous section we presented a new approach to analyze the \(\mathtt{O\!MLE}\) algorithm for MDPs. Next, we develop an analogous technique for the LMDP setting and design the \(\mathtt{LMDP\text{-}O\!MLE}\) algorithm. Central to its design and analysis is an OPE lemma and a new coverage coefficient which we now present.

Intuition from moment-exploration algorithm in [43].Before we dive into our key results, let us provide our intuition on how we construct the OPE lemma for LMDPs. Our construction is inspired by the moment-exploration algorithm proposed in [43]: when state-transition dynamics are identical across latent contexts, _i.e.,_\(T_{1}=T_{2}=...=T_{M}\), we can first learn the transition dynamics with any reward-free type exploration scheme for MDPs [33], and then set the exploration policy that sufficiently visits some tuples of state-actions \(\boldsymbol{x}\) of length at most \(d\). Specifically, they set a memorlyess exploration policy \(\psi\in\Pi_{\mathtt{als}}\) which sets \(\mathds{P}^{\psi}(x_{\boldsymbol{\tau}}=\boldsymbol{x})\) sufficiently large for some \(\boldsymbol{\tau}\in\mathtt{SubSeq}(H,d)\)and \(\mathbf{x}\in\mathcal{X}^{\bigotimes|\mathbf{\tau}|}\). We note that the same moment-exploration strategy cannot be applied to general LMDPs with different state-transition dynamics since learning the transition dynamics itself involves latent contexts. Nevertheless, the intuition from [43] suggests that our key statistics are this visitation probabilities to all tuples of state-actions within a trajectory.

### Off-Policy Evaluation in LMDPs

The OPE lemma we derive in this section makes use of a behavior policy of a special form which we refer to as a _segmented policy_, inspired by the notion of moment-exploration in [43]. Let us formally define the key quantities to establish our OPE lemma. A segmented policy, which we denote by \(\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{z})\), takes as an input a sequence of history-dependent policies, \(\mathbf{\psi}=(\psi_{0},...,\psi_{d})\), a sequence of time steps, we call checkpoints, \(\mathbf{\tau}=(\tau_{1},...,\tau_{|\mathbf{\tau}|})\in\mathtt{SubSeq}(H,d)\), and a sequence of binary numbers \(\mathbf{z}=(z_{1},...,z_{|\mathbf{\tau}|})\in\{0,1\}^{|\mathbf{\tau}|}\) where \(|\mathbf{\tau}|\leq d\), and returns a history-dependent policy.

The segmented policy \(\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{z})\) switches sequentially between different policies in \(\mathbf{\psi}\). The time steps in which the switch occurs are determined by \(\mathbf{\tau}\): starting from time step \(\tau_{i}+1\) policy \(\psi_{i}\) will be executed. Finally, the sequence \(\mathbf{z}\) determines whether an intervention with a random action will occur at the \(\tau_{i}\) time-step. If \(z_{i}=1\) the executed action at time step \(\tau_{i}\) is the uniform action, \(\mathtt{Unif}(\mathcal{A})\), and, otherwise, the policy \(\psi_{i-1}\) is executed. The segmented policy is also denoted by

\[\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{z}):=\psi_{0}\mathop{\circ}_{(\tau_{1},z_{1})}\psi _{1}\mathop{\circ}_{(\tau_{2},z_{2})}...\mathop{\circ}_{(\tau_{|\mathbf{\tau}|},z_ {|\mathbf{\tau}|})}\psi_{|\mathbf{\tau}|},\]

where "\(\pi_{a}\mathop{\circ}_{(t,z)}\pi_{b}\)" means switch to policy \(\pi_{b}\) at starting from time step \(t+1\), and at time step \(t\) take random action if \(z=1\) and otherwise execute \(\pi_{a}\).

We are now ready to define a coverage coefficient for the LMDP class of models. This new coverage coefficient is central to the analysis and design of LMDP-OmLE.

**Definition 4.1** (LMDP Coverage Coefficient): _The LMDP coverage coefficient of a sequence of policies \(\mathbf{\psi}\in\Pi^{\bigotimes(d+1)}\) with respect to a target policy \(\pi\in\Pi\) in is given by:_

\[C(\mathbf{\psi};\pi):=\max_{\mathbf{\tau}\in\mathtt{SubSeq}(H,d)}\max_{\mathbf{z}\in\{0,1 \}^{\otimes|\mathbf{\tau}|}}\max_{(\mathbf{x},\mathbf{y})\in\mathtt{SubSeq}\Pi\mathbf{\tau} \mathbf{\beta}(\mathbf{\tau},\mathbf{\tau})}\max_{m\in[M]}\frac{P_{m}^{\pi}(x_{\mathbf{\tau}} =\mathbf{x},y_{\mathbf{\tau}}=\mathbf{y})}{P_{m}^{\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{z})}(x_{\bm {\tau}}=\mathbf{x},y_{\mathbf{\tau}}=\mathbf{y})}. \tag{4}\]

The LMDP coverage coefficient \(C(\mathbf{\psi};\pi)\) between a sequence of policies, \(\mathbf{\psi}\), and a target, history-dependent, policy \(\pi\), depends on the worst-case way to generate a segmented policy, \(\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{z})\) from \(\mathbf{\psi}\). Further, it is a worst-case ratio of the probability of a sequence of observations within \(|\mathbf{\tau}|=d\) different time steps, namely, \(x_{\mathbf{\tau}},y_{\mathbf{\tau}}\). This is different than the standard coverage coefficient (see equation (2)), that depends on observation from a single time. Fortunately, \(C(\mathbf{\psi};\pi)\) requires only a partial set of observations, instead of using full trajectories. This is crucial towards developingsample complexity guarantees that are not exponential in \(H\). Lastly, observe that the LMDP coverage coefficient depends on the latent context \(m\), and thus, we cannot measure \(C(\mathbf{\psi};\pi)\) from samples.

We are now ready to provide the key OPE lemma, which makes use of the LMDP coverage coefficient.

**Lemma 4.2** (TV Bound via OPE for LMDPs): _Let \(d=2M-1\). For any two models \(\theta,\theta^{*}\in\Theta\), and for any \(\pi\in\Pi\) and \(\mathbf{\psi}\in\Pi^{\bigotimes(d+1)}\), let \(C(\mathbf{\psi};\pi)\) be defined as (4) over \(\theta^{*}\). Then the following holds:_

\[\mathcal{TV}(\mathds{P}^{\pi}_{\theta^{*}},\mathds{P}^{\pi}_{\theta})(\mathcal{ T})\leq M\cdot C(\mathbf{\psi};\pi)\sum_{\mathbf{\tau}\in\mathbf{subSeq}(H,d)}\sum_{\mathbf{z}\in \{0,1\}^{\mathbf{\beta}}\setminus\mathbf{\tau}}\mathcal{TV}\left(\mathds{P}^{\nu(\mathbf{ \psi};\mathbf{\tau},\mathbf{z})}_{\theta^{*}},\mathds{P}^{\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{ z})}_{\theta}\right)(x_{\mathbf{\tau}},y_{\mathbf{\tau}}).\]

This result is analogous to the OPE result for MDPs (see Lemma 3.1). It is a tool that allows us to bound the TV distance between trajectory distributions of a history-dependent policy \(\pi\) by a term that depends on a segmented policy \(\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{z})\) and an LMDP coverage coefficient. Importantly, the term on the RHS that depends on the segmented policy, \(\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{z})\), is a sum of distributions of partial trajectories of size \(|\mathbf{\tau}|\leq d\), which is independent of the horizon length, \(H\).

**Remark 4.3** (Why is single latent-state coverability coefficient not enough?): _One may wonder why it is not sufficient to consider a single latent-state coverability analogous to Lemma 3.1, namely an analogous to (2) defined as:_

\[\max_{t\in[H]}\max_{x\in\mathcal{X}}\max_{m\in[M]}\frac{P^{\pi}_{m}(x_{t}=x)}{ P^{\psi}_{m}(x_{t}=x)}.\]

_In Appendix D.5 we provide a counter-example where such single latent-state coverage coefficient is finite, and yet, off-policy evaluation guarantee cannot be established._

### Coverage Doubling via Sufficiency of Memoryless Polices

To convert the OPE guarantee to an online exploration algorithm, we aim to use the coverage-doubling argument. Ideally, we could apply the coverage-doubling argument with the general policy class similarly to the MDP case as presented in Section 3. However, in its current form, Lemma 4.2 requires the behavior policy to be a segmented policy, and is not valid for any general behavioral policy. Hence, it is not obvious on which probabilistic events we can apply the coverage doubling argument. We leave it as future work whether we can obtain an off-policy evaluation lemma with general history-dependent behavioral policies, and its clearer conversion to online guarantees.

In this work, we present an alternative plan to the above issue: we reduce the search space from history-dependent policies to memoryless policies. This allows us to track quantities on a _segmentwise_ coverage. Specifically, we first note that the LMDP coverage coefficient can be bounded (after maximizing over the sequence \(\mathbf{z}\)) by:

\[C(\mathbf{\psi};\pi)\leq\max_{\mathbf{\tau}\in\mathbf{subSeq}(H,d)}\max_{ \begin{subarray}{c}\mathbf{z}\in\mathcal{X}^{\bigotimes|\mathbf{\tau}|}\\ \mathbf{s}^{\prime}\in\mathcal{S}^{\bigotimes|\mathbf{\tau}|-1}\end{subarray}}\max_{m \in[M]}\prod_{i=0}^{d-1}\frac{\max_{\mathcal{T}_{1:\tau_{i}}}P^{\pi}_{m}(s_{ \tau_{i+1}}=s_{[i+1]}|s^{\prime}_{\tau_{i}}=s^{\prime}_{[i]},\mathcal{T}_{1: \tau_{i}})}{(1/A)\cdot P^{\nu(\psi_{i};\tau_{i})}_{m}(s_{\tau_{i+1}}=s_{[i+1]} |s^{\prime}_{\tau_{i}}=s^{\prime}_{[i]})}, \tag{5}\]

where \(\nu(\psi_{i};\tau_{i})\) denotes a segmented policy executing \(\psi_{i}\) after the \(\tau_{i}^{th}\) time-step with memory reset, hence ignoring the history up to \(\tau_{i}\) (the conditioning event \(s^{\prime}_{\tau_{0}}=s^{\prime}_{[0]}\) at \(i=0\) can be ignored). Inspired by the form in denominator, we aim to double the following probability defined over a _context-segment_ pair:

\[\max_{\psi\in\Psi_{\text{out}}}P^{\nu(\psi;\tau_{1})}_{m}(s_{t_{2}}=s|s^{\prime }_{t_{1}}=s^{\prime}), \tag{6}\]

for at least one \(m\in[M],s,s^{\prime}\in\mathcal{S}\) and \(t_{1}<t_{2}\). However, another challenge remains: the RHS in equation (5) consists of the maximum over all possible histories in the numerator, whereas in the denominator we force the data collection policy to reset the memory at checkpoints. We still have to side-step this discrepancy to apply the coverage doubling argument.

The restriction to the class of memoryless policies allows us to resolve these issues since

\[\max_{\mathcal{T}_{1:t_{1}}}P^{\pi}_{m}(s_{t_{2}}=s|s^{\prime}_{t_{1}}=s^{ \prime},\mathcal{T}_{1:t_{1}})=P^{\pi}_{m}(s_{t_{2}}=s|s^{\prime}_{t_{1}}=s^{ \prime}),\]\[\text{and}\ P_{m}^{\nu(\psi;t_{1})}(s_{t_{2}}=s|s^{\prime}_{t_{1}}=s^{\prime})=P_{m} ^{\psi}(s_{t_{2}}=s|s^{\prime}_{t_{1}}=s^{\prime}),\]

if \(\pi,\psi\in\Pi_{\mathrm{mls}}\) since \(P_{m}\) represents the latent Markovian transition dynamics. Thus, we can aim to double up the quantity in equation (6). To apply this argument, we establish our second key lemma, a crucial building block for the coverage doubling argument:

**Lemma 4.4** (Sufficiency of Memoryless Polices for LMDPs): _Suppose the following holds:_

\[\max_{\pi_{\mathrm{mls}}\in\Pi_{\mathrm{mls}}}\mathcal{I}\!\!V(\mathrm{P}_{ \theta^{*}}^{\pi_{\mathrm{mls}}},\mathrm{P}_{\theta}^{\pi_{\mathrm{mls}}})( \mathcal{T})\leq\epsilon_{\texttt{test}}. \tag{7}\]

_Then for any history-dependent policies \(\pi\in\Pi\), the following holds:_

\[\mathcal{I}\!\!V(\mathrm{P}_{1}^{\pi},\mathrm{P}_{2}^{\pi})(\mathcal{T})\leq M (2H^{2})^{d}\cdot(MSA)^{d}\cdot\epsilon_{\texttt{test}}.\]

Therefore, we reduced our goal of learning an optimal policy to finding a set of models which satisfies equation (7) with respect to all memoryless policies. Importantly, upon estimating the trajectory distribution up to accuracy \(\epsilon_{\texttt{test}}>0\) for memoryless policies, we have a bounded TV distance between the trajectory distribution of all history-dependent policies, includes the optimal policy.

### The Lmdp-omle Algorithm

Once the search space is reduced to memoryless policies, we aim to match trajectory distributions for all memoryless policies. At a high-level, LMDP-omle follows similar recipe to MDP-omle, and is summarized in Algorithm 2. It can be described as follows:

1. Find a memoryless policy \(\pi^{k}\in\Pi_{\mathrm{mls}}\) whose prediction on trajectory distributions does not match between two models in the confidence set \(\mathcal{C}^{k}\). Add \(\pi^{k}\) to the collection of test policies \(\Psi^{k}_{\texttt{test}}\), that forms each segment of (segmented) exploration policies.
2. Collect new sample trajectories following the new set of segmented policies for exploration, generated by different combinations of collected test policies and switching operations.
3. Update the confidence set \(\mathcal{C}^{k}\) with Maximum Likelihood Estimation (MLE) on the updated dataset \(\mathcal{D}^{k}\) by equation (1).

The data collection policy of LMDP-omle (second step above) is inspired and leverages Lemma 4.2 to give upper bounds on the TV distance of untested policies. Next, by Lemma 4.4, we know that when the while loop terminates, any optimal policy of a model contained in the confidence set is a near-optimal policy of the underlying LMDP. We conclude this section with our main theorem on the sample complexity of learning the optimal policy in Latent MDPs:

**Theorem 4.5** (Sample Complexity of Lmdp-omle): _Let \(d=2M-1\) and assume \(H>2M\). After at most \(K=O(MS^{2}H)\cdot\log(MSAH/\epsilon)\) iterations, LMDP-omle (Algorithm 2) terminates with at most \(N\) episodes being generated where_

\[N\gtrsim\left(M^{4}S^{6}A^{4}H^{7}\cdot\log(MSAH/\epsilon)\right)^{d}\cdot M^ {4}H^{2}\cdot\log(K|\Theta|/\eta)/\epsilon^{2},\]

_and outputs an \(\epsilon\)-optimal policy with probability at least \(1-\eta\)._

Note that in tabular LMDPs with finite support rewards, we have \(\log(|\Theta|)=O(S^{2}A|\mathcal{R}|\log(1/\epsilon))\). The appeal of \(\log(|\Theta|)\) dependence is an flexible extension of the same result to parameterized reward distributions. In Section D.1 and D.4 we provide a proof overview and a full proof of Theorem 4.5.

## 5 Conclusion and Future Work

In this work, we presented the first sample-efficient algorithm for LMDPs, resolving an open question of efficient exploration with latent contexts. While our result is specialized to LMDPs, we believe our new perspectives and techniques on deriving online guarantees through the lens of OPE can be useful for a broader range of interactive learning, and, especially, partially observed problems. While resolving the open problem, there are a few remaining questions for the LMDP setting.

Tightness of the Result.The upper bound in Theorem 4.5 scales with \(\tilde{O}\left(MSAH\cdot\log(1/\epsilon)\right)^{O(M)}\), while the existing lower bound is \(\Omega(SA)^{M}\). Closing this polynomial gap in the exponent, and having a matching upper and lower bounds can be valuable for a deeper understanding of LMDPs and possibly for POMDPs in general.

General OPE lemma and Regret Guarantees for LMDPs.The OPE lemma derived in this work (Lemma 4.2) assumes the behavior policy is a segmented policy with intervention at different checkpoints. While this result allows us to provide guarantees on LMDP-OmLE and prove it learns a near-optimal policy, this result is restrictive, in that it does not provide general guarantees for OPE nor makes it possible to derive regret guarantees. In particular, can we evaluate \(\pi\in\Pi\) without policy-switching or intervention when the behavioral policy is a generic history-dependent policy \(\psi\in\Pi\)? Further, is there an algorithm with provable \(\operatorname{poly}(S,A)^{M}\cdot\sqrt{T}\) regret for the general LMDP setting?

Towards Practical Settings.Our result gives a worst-case guarantee. Yet, practical instances may be much simpler under different set of assumptions _e.g.,_ with provided side-information [66, 44] or additional structural assumptions [41, 63, 14]. Deriving new conditions can be of great importance for real-world applications, _e.g.,_ there could be more practical notion of separation, or the set of instances that allows the notion of coverage-coefficient with a (significantly) shorter length \(d=o(M)\) of state-action tuples. Further, developing practical RL methodologies for the LMDP setting remains an unexplored challenge with significant importance for numerous applications. These are remained to be explored in future works.

## Acknowledgment

This research was partially funded by AFOSR/AFRL grant no. FA9550-18-1-0166, NSF Grants 2019844 and 2112471, and Israel Science Foundation Grant No. 2199/20.

## References

* [1] A. Agarwal, S. Kakade, A. Krishnamurthy, and W. Sun. FLAMBE: Structural complexity and representation learning of low rank MDPs. _Advances in neural information processing systems_, 33:20095-20107, 2020.
* [2] A. Al-Marjani, A. Tirinzoni, and E. Kaufmann. Active coverage for PAC reinforcement learning. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 5044-5109. PMLR, 2023.
* [3] P. Amortila, D. J. Foster, N. Jiang, A. Sekhari, and T. Xie. Harnessing density ratios for online reinforcement learning. In _The Twelfth International Conference on Learning Representations_, 2023.
* [4] P. Amortila, D. J. Foster, and A. Krishnamurthy. Scalable online exploration via coverability. _arXiv preprint arXiv:2403.06571_, 2024.
* [5] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning latent variable models. _The Journal of Machine Learning Research_, 15(1):2773-2832, 2014.
* [6] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. _SIAM journal on computing_, 32(1):48-77, 2002.
* [7] M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272. PMLR, 2017.
* [8] K. Azizzadenesheli, A. Lazaric, and A. Anandkumar. Reinforcement learning of POMDPs using spectral methods. In _Conference on Learning Theory_, pages 193-256, 2016.
* [9] P. J. Ball, L. Smith, I. Kostrikov, and S. Levine. Efficient online reinforcement learning with offline data. In _International Conference on Machine Learning_, pages 1577-1594. PMLR, 2023.
* [10] A. Bennett and N. Kallus. Proximal reinforcement learning: Efficient off-policy evaluation in partially observed markov decision processes. _Operations Research_, 2023.

* [11] E. Brunskill and L. Li. Sample complexity of multi-task reinforcement learning. In _Uncertainty in Artificial Intelligence_, page 122. Citeseer, 2013.
* [12] I. Chades, J. Carwardine, T. Martin, S. Nicol, R. Sabbadin, and O. Buffet. MOMDPs: a solution for modelling adaptive management problems. In _Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI-12)_, 2012.
* [13] F. Chen, Y. Bai, and S. Mei. Partially observable RL with B-stability: Unified structural condition and sharp sample-efficient algorithms. In _The Eleventh International Conference on Learning Representations_, 2022.
* [14] F. Chen, C. Daskalakis, N. Golowich, and A. Rakhlin. Near-optimal learning and planning in separated latent MDPs. _arXiv preprint arXiv:2406.07920_, 2024.
* [15] F. Chen, H. Wang, C. Xiong, S. Mei, and Y. Bai. Lower bounds for learning in revealing POMDPs. In _International Conference on Machine Learning_, pages 5104-5161. PMLR, 2023.
* [16] C. Dann, N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. On oracle-efficient PAC RL with rich observations. In _Advances in neural information processing systems_, pages 1422-1432, 2018.
* [17] Y. Efroni, C. Jin, A. Krishnamurthy, and S. Miryoosefi. Provable reinforcement learning with a short-term memory. In _International Conference on Machine Learning_, pages 5832-5850. PMLR, 2022.
* [18] Y. Efroni, D. Misra, A. Krishnamurthy, A. Agarwal, and J. Langford. Provably filtering exogenous distractors using multistep inverse dynamics. In _International Conference on Learning Representations_, 2021.
* [19] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* [20] S. A. Geer. _Empirical Processes in M-estimation_, volume 6. Cambridge university press, 2000.
* [21] C. Gentile, S. Li, P. Kar, A. Karatzoglou, G. Zappella, and E. Etrue. On context-dependent clustering of bandits. In _International Conference on Machine Learning_, pages 1253-1262. PMLR, 2017.
* [22] N. Golowich, A. Moitra, and D. Rohatgi. Learning in observable POMDPs, without computationally intractable oracles. _Advances in neural information processing systems_, 35:1458-1473, 2022.
* [23] Z. D. Guo, S. Doroudi, and E. Brunskill. A PAC RL algorithm for episodic POMDPs. In _Artificial Intelligence and Statistics_, pages 510-518, 2016.
* [24] A. Hallak, D. Di Castro, and S. Mannor. Contextual markov decision processes. _arXiv preprint arXiv:1502.02259_, 2015.
* [25] D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden markov models. _Journal of Computer and System Sciences_, 78(5):1460-1480, 2012.
* [26] R. Huang, Y. Liang, and J. Yang. Provably efficient UCB-type algorithms for learning predictive state representations. In _The Twelfth International Conference on Learning Representations_, 2023.
* [27] T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. _Journal of Machine Learning Research_, 11:1563-1600, 2010.
* [28] A. J. Jeckmans, M. Beye, Z. Erkin, P. Hartel, R. L. Lagendijk, and Q. Tang. Privacy in recommender systems. _Social media retrieval_, pages 263-281, 2013.
* [29] Z. Jia, G. Li, A. Rakhlin, A. Sekhari, and N. Srebro. When is agnostic reinforcement learning statistically tractable? _Advances in Neural Information Processing Systems_, 36, 2023.

* [30] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision processes with low bellman rank are PAC-learnable. In _International Conference on Machine Learning_, pages 1704-1713. PMLR, 2017.
* [31] N. Jiang and L. Li. Doubly robust off-policy value evaluation for reinforcement learning. In _International conference on machine learning_, pages 652-661. PMLR, 2016.
* [32] C. Jin, S. M. Kakade, A. Krishnamurthy, and Q. Liu. Sample-efficient reinforcement learning of undercomplete POMDPs. _arXiv preprint arXiv:2006.12484_, 2020.
* [33] C. Jin, A. Krishnamurthy, M. Simchowitz, and T. Yu. Reward-free exploration for reinforcement learning. In _International Conference on Machine Learning_, pages 4870-4879. PMLR, 2020.
* [34] C. Jin, Q. Liu, and S. Miryoosefi. Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. _Advances in neural information processing systems_, 34:13406-13418, 2021.
* [35] S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In _Proceedings of the Nineteenth International Conference on Machine Learning_, pages 267-274, 2002.
* [36] E. Kaufmann, P. Menard, O. D. Domingues, A. Jonsson, E. Leurent, and M. Valko. Adaptive reward-free exploration. In _Algorithmic Learning Theory_, pages 865-891. PMLR, 2021.
* [37] C. Kausik, K. Tan, and A. Tewari. Learning mixtures of markov chains and MDPs. In _International Conference on Machine Learning_, pages 15970-16017. PMLR, 2023.
* [38] M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. _Machine learning_, 49:209-232, 2002.
* [39] A. Krishnamurthy, A. Agarwal, and J. Langford. PAC reinforcement learning with rich observations. In _Advances in Neural Information Processing Systems_, pages 1840-1848, 2016.
* [40] J. Kwon, Y. Efroni, C. Caramanis, and S. Mannor. Reinforcement learning in reward-mixing MDPs. _Advances in Neural Information Processing Systems_, 34, 2021.
* [41] J. Kwon, Y. Efroni, C. Caramanis, and S. Mannor. RL for latent MDPs: Regret guarantees and a lower bound. _Advances in Neural Information Processing Systems_, 34, 2021.
* [42] J. Kwon, Y. Efroni, C. Caramanis, and S. Mannor. Tractable optimality in episodic latent MABs. _Advances in Neural Information Processing Systems_, 35:23634-23645, 2022.
* [43] J. Kwon, Y. Efroni, C. Caramanis, and S. Mannor. Reward-mixing MDPs with few latent contexts are learnable. In _International Conference on Machine Learning_, pages 18057-18082. PMLR, 2023.
* [44] J. Kwon, Y. Efroni, S. Mannor, and C. Caramanis. Prospective side information for latent MDPs. _arXiv preprint arXiv:2310.07596_, 2023.
* [45] Q. Liu, A. Chung, C. Szepesvari, and C. Jin. When is partially observable reinforcement learning not scary? In _Conference on Learning Theory_, pages 5175-5220. PMLR, 2022.
* [46] Q. Liu, P. Netrapalli, C. Szepesvari, and C. Jin. Optimistic MLE: A generic model-based algorithm for partially observable sequential decision making. In _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 363-376, 2023.
* [47] H. Namkoong, R. Keramati, S. Yadlowsky, and E. Brunskill. Off-policy policy evaluation for sequential decisions under unobserved confounding. _Advances in Neural Information Processing Systems_, 33:18819-18831, 2020.
* [48] Z. Rimon, A. Tamar, and G. Adler. Meta reinforcement learning with finite training tasks-a density estimation approach. _Advances in Neural Information Processing Systems_, 35:13640-13653, 2022.

* [49] D. Russo and B. Van Roy. Eluder dimension and the sample complexity of optimistic exploration. _Advances in Neural Information Processing Systems_, 26, 2013.
* [50] C. Shi, M. Uehara, J. Huang, and N. Jiang. A minimax learning approach to off-policy evaluation in confounded partially observable markov decision processes. In _International Conference on Machine Learning_, pages 20057-20094. PMLR, 2022.
* [51] S. Singh, M. R. James, and M. R. Rudary. Predictive state representations: a new theory for modeling dynamical systems. In _Proceedings of the 20th conference on Uncertainty in artificial intelligence_, pages 512-519, 2004.
* [52] R. D. Smallwood and E. J. Sondik. The optimal control of partially observable markov processes over a finite horizon. _Operations research_, 21(5):1071-1088, 1973.
* [53] L. N. Steimle, D. L. Kaufman, and B. T. Denton. Multi-model markov decision processes. _Optimization Online URL http://www. optimization-online. org/DB_FILE/2018/01/6434. pdf_, 2018.
* [54] R. S. Sutton and A. G. Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* [55] K. Tan and Z. Xu. A natural extension to online algorithms for hybrid RL with limited coverage. _arXiv preprint arXiv:2403.09701_, 2024.
* [56] G. Tennenholtz, U. Shalit, and S. Mannor. Off-policy evaluation in partially observable environments. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 10276-10283, 2020.
* [57] M. Uehara, A. Sekhari, J. D. Lee, N. Kallus, and W. Sun. Provably efficient reinforcement learning in partially observable dynamical systems. _Advances in Neural Information Processing Systems_, 35:578-592, 2022.
* [58] M. Uehara, C. Shi, and N. Kallus. A review of off-policy evaluation in reinforcement learning. _arXiv preprint arXiv:2212.06355_, 2022.
* [59] T. Xie, D. J. Foster, Y. Bai, N. Jiang, and S. M. Kakade. The role of coverage in online reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2022.
* [60] T. Xie, Y. Ma, and Y.-X. Wang. Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling. _Advances in neural information processing systems_, 32, 2019.
* [61] Y. Xu, J. Zhu, C. Shi, S. Luo, and R. Song. An instrumental variable approach to confounded off-policy evaluation. In _International Conference on Machine Learning_, pages 38848-38880. PMLR, 2023.
* [62] G. Yauney and P. Shah. Reinforcement learning with action-derived rewards for chemotherapy and clinical trial dosing regimen selection. In _Machine Learning for Healthcare Conference_, pages 161-226. PMLR, 2018.
* [63] W. Zhan, M. Uehara, W. Sun, and J. D. Lee. PAC reinforcement learning for predictive state representations. _arXiv preprint arXiv:2207.05738_, 2022.
* [64] T. Zhang. From \(\varepsilon\)-entropy to KL-entropy: Analysis of minimum information complexity density estimation. _Annals of Statistics_, 34(5):2180-2210, 2006.
* [65] Y. Zhang and N. Jiang. On the curses of future and history in future-dependent value functions for off-policy evaluation. _arXiv preprint arXiv:2402.14703_, 2024.
* [66] R. Zhou, R. Wang, and S. S. Du. Horizon-free and variance-dependent reinforcement learning for latent markov decision processes. In _International Conference on Machine Learning_, pages 42698-42723. PMLR, 2023.
* [67] L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson. VariBAD: A very good method for bayes-adaptive deep RL via meta-learning. In _International Conference on Learning Representations_, 2019.

## Appendix A Additional Preliminaries

The difference between the values of any policy \(\pi\in\Pi\) measured on two models \(\theta_{1},\theta_{2}\in\Theta\) are bounded by the total-variation (TV) distance between trajectory distributions, that is,

\[|V_{1}^{\pi}-V_{2}^{\pi}|\leq H\cdot\mathsf{TV}(\mathrm{P}_{1}^{\pi},\mathrm{P}_ {2}^{\pi})(\mathcal{T}),\]

since the maximum reward that can be obtained in an episode is bounded by \(H\) due to Assumption 2.2. Hence, if we can show that

\[\mathsf{TV}(\mathrm{P}_{\theta^{\pi}}^{\pi},\mathrm{P}_{\theta}^{\pi})( \mathcal{T})\leq\epsilon/H=:\epsilon_{\mathsf{TV}},\qquad\forall\pi\in\Pi, \tag{8}\]

then an optimal policy \(\hat{\pi}^{*}\) of the empirical model \(\hat{\theta}\) is guaranteed to be \(2\epsilon\)-optimal in the true model \(\theta^{*}\). Henceforth, we focus on finding an empirical model \(\hat{\theta}\) that satisfies (8).

To bound the TV-distance between trajectory distributions for all history-dependent policies \(\pi\in\Pi\) between any two LMDP models, we start by unfolding the expression of statistical distance

\[\mathsf{TV}(\mathrm{P}_{1}^{\pi},\mathrm{P}_{2}^{\pi})(\mathcal{ T})=\sum_{(x_{t},r_{t})_{t\in[H]}}|\mathrm{P}_{1}^{\pi}((x_{t},r_{t})_{t\in[H]} )-\mathrm{P}_{2}^{\pi}((x_{t},r_{t})_{t\in[H]})|\] \[=\sum_{\begin{subarray}{c}x_{1:H}\\ r_{1:H}\end{subarray}}\prod_{t=1}^{H}\pi(a_{t}|\mathcal{T}_{1:t})\times\left| \sum_{m=1}^{M}w_{m}^{1}\prod_{t=0}^{H}T_{m}^{1}(s_{t+1}|x_{t})R_{m}^{1}(r_{t}| x_{t})-\sum_{m=1}^{M}w_{m}^{2}\prod_{t=0}^{H}T_{m}^{2}(s_{t+1}|x_{t})R_{m}^{2}(r_{t}| x_{t})\right|.\]

When the context is clear, we compare trajectory distributions between any two given model parameters \(\theta_{1},\theta_{2}\in\Theta\), and denote the probability measure from each model following \(\pi\) as \(\mathrm{P}_{1}^{\pi}(\cdot)\), \(\mathrm{P}_{2}^{\pi}(\cdot)\).

### Additional Notation

To reduce the notation overload, we use \(P_{m}(y_{t}|x_{t})=T_{m}(s_{t+1}|x_{t})R_{m}(r_{t}|x_{t})\). When the context is clear, we often use a shorthand \(\pi_{t}=\pi(a_{t}|\mathcal{T}_{1:t})\), and denote \(\pi_{t_{1}\cdot t_{2}}\) as a shorthand of the product of a time-consecutive sequence from \(t_{1}\) to \(t_{2}\), _i.e.,_\(\pi_{t_{1}\cdot t_{2}}=\prod_{t=t_{1}}^{t_{2}}\pi_{t}\). When we sum over both \(x_{t}\) and \(y_{t}\), we implicitly mean that the \(s^{\prime}_{t}\) part of \(y_{t}\), which we denote as \(s^{\prime}(y_{t})\), must match to the \(s_{t+1}\) part of \(x_{t+1}\), which we denote as \(s(x_{t+1})\). Using the notation, we rewrite the unfolded TV-distance equation as the following:

\[\sum_{x_{1:H}}\sum_{y_{1:H}}\pi_{1:H}\left|\sum_{m=1}^{M}w_{m}^{1}\prod_{t=0}^ {H}P_{m}^{1}(y_{t}|x_{t})-\sum_{m=1}^{M}w_{m}^{2}\prod_{t=0}^{H}P_{m}^{2}(y_{ t}|x_{t})\right|.\]

We use a shorthand \(\delta_{\pi}(X)\) for \(|\mathrm{P}_{1}^{\pi}(X)-\mathrm{P}_{2}^{\pi}(X)|=|\sum_{m=1}^{M}w_{m}^{1}P_{ m}^{1,\pi}(X)-\sum_{m=1}^{M}w_{m}^{2}P_{m}^{2,\pi}(X)|\), and thus \(\sum_{X}\delta_{\pi}(X)=\mathsf{TV}(\mathrm{P}_{1}^{\pi},\mathrm{P}_{2}^{\pi} )(X)\) where the summation is over all possible realizations of a random variable \(X\). Finally, we denote \(d=2M-1\).

### Preliminaries for Lemma 4.2

Here we define a few more quantities that will be crucial for the proofs for Section 4. In bounding the total variation distance in terms of tested policies without exponential blow-up in \(H\), the key is to marginalize events across time steps. Let us first fix the _checkpoint_ time-steps \(\boldsymbol{\tau}=(\tau_{1},...,\tau_{q})\) for \(q\in[d]\), and a sequence of executable policies \(\boldsymbol{\psi}=(\psi_{0},\psi_{1},...,\psi_{d})\). Each \(i^{th}\) segment policy will be executed within time interval \((\tau_{i},\tau_{i+1}]\) for \(i\geq 0\).

To proceed, for the initial policy \(\psi_{0}\), let \(l^{0}\) be the smallest quantity, among the contexts, of the ratio between the state visitation probabilities in consecutive steps when \(\psi_{0}\) is executed (recall that we denote \(x_{t}=(s_{t},a_{t})\), \(y_{t}=(r_{t},s_{t+1})\)):

\[l^{0}(x_{t},r_{t};s_{t+1}):=\min_{n\in\{1,2\}}\left(\min_{m\in[M_{n}]}\frac{P_{ m}^{n,\psi_{0}}(s_{t})}{P_{m}^{n,\psi_{0}}(s_{t+1})}P_{m}^{n}(r_{t},s_{t+1}|x_{t}) \right). \tag{9}\]

This quantity can be understood as the minimum (over latent contexts) of the "pseudo-posterior" probabilities of 1-step event given the future state if \(\psi_{0}\) is memoryless, since

\[l^{0}(x_{t},r_{t};s_{t+1})\cdot\psi_{0}(a_{t}|s_{t})=\min_{n\in\{1,2\}}\min_{m \in[M]}P_{m}^{\psi_{0}}(x_{t},r_{t}|s_{t+1}),\text{ if }\psi_{0}\in\Pi_{\mathrm{mls}}.\]Henceforth we use a shorthand \(l^{0}_{t}:=l^{0}(x_{t},r_{t};s_{t+1})\). We recursively define a sequence of the above quantity.

Next, we fix the event \((x_{\mathbf{\tau}},y_{\mathbf{\tau}})\) at the event-log time-steps. For all \(i\geq 0\) and \(\tau_{i}<t\leq\tau_{i+1}\), we define \(l^{i}_{t}(x_{\tau_{i:i}},y_{\tau_{1:i}})\) and \(p^{n,i}_{m}(x_{\tau_{i:i}},y_{\tau_{1:i}})\) recursively as the following:

\[l^{i}_{t}(x_{\tau_{1:i}},y_{\tau_{1:i}}):=\min_{n\in\{1,2\}}\left(\min_{m\in[M] :p^{n,i}_{m}(x_{\tau_{1:i}},y_{\tau_{1:i}})>0}\frac{P^{n,\nu(\psi_{i};\tau_{i} )}_{m}(s_{t}|s_{\tau_{i}+1})P^{n}_{m}(r_{t},s_{t+1}|x_{t})}{P^{n,\nu(\psi_{i}; \tau_{i})}_{m}(s_{t+1}|s_{\tau_{i}+1})}\right), \tag{10}\]

and

\[p^{n,i+1}_{m}(x_{\tau_{1:i+1}},y_{\tau_{1:i+1}}) =p^{n,i}_{m}(x_{\tau_{1:i}},y_{\tau_{1:i}})\times\left(P^{n,\nu( \psi_{i};\tau_{i})}_{m}(s_{\tau_{i+1}}|s_{\tau_{i}+1})P^{n}_{m}(y_{\tau_{i+1}} |x_{\tau_{i+1}})\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad-P^{n,\nu(\psi_{i};\tau_{i})} _{m}\left(s^{\prime}(y_{\tau_{i+1}})|s_{\tau_{i+1}}\right)l^{i}_{\tau_{i+1}+1} (x_{\tau_{i:i}},y_{\tau_{1:i}})\Big{)}, \tag{11}\]

Here, we define \(t_{[0]}\equiv-1\), where we recall that \(\nu(\pi;t)\) means the memory reset of a policy \(\pi\) at time step \(t+1\). \(x_{\tau_{1:0}},y_{\tau_{1:0}}\equiv\phi\), and \(l^{0}_{0}\equiv 1\) and \(p^{n,0}_{m}\equiv w^{n}_{m}\) for \(n=1,2\). The key point here is that in this recursive construction, as \(i\) increase, we have at least one \(i\) such that either \(p^{1,i+1}_{m}=0\) or \(p^{2,i+1}_{m}=0\), _i.e.,_ at least one context is removed from consideration at each checkpoint.

In the subsequent steps in our proof, we often omit the dependence on \((x_{\tau_{1:i}},y_{\tau_{1:i}})\), as well as \(\pi_{\tau_{0:i}}\) in \(l^{i}_{t}\) and \(p^{n,i+1}_{m}\) when the context is clear. Finally, we define

\[\Delta(x_{\mathbf{\tau}},y_{\mathbf{\tau}}):=\left|\sum_{m=1}^{M}p^{1,|\mathbf{\tau}|}_{m} -\sum_{m=1}^{M}p^{2,|\mathbf{\tau}|}_{m}\right|,\]

Now we are ready to state our key intermediate lemma:

**Lemma A.1**: _For any target policy \(\pi\in\Pi\) and a sequence of segment policies \(\mathbf{\psi}=(\psi_{0},\psi_{1},...,\psi_{d})\), the following holds:_

\[\sum_{x_{1:H}}\sum_{y_{1:H}}\pi_{1:H}\left|\sum_{m=1}^{M}w^{1}_{m }\prod_{t=0}^{H}P^{1}_{m}(y_{t}|x_{t})-\sum_{m=1}^{M}w^{2}_{m}\prod_{t=0}^{H} P^{2}_{m}(y_{t}|x_{t})\right|\] \[\leq\sum_{\mathbf{\tau}\in\mathbf{SubSeq}}\sum_{x_{\mathbf{\tau}},y_{\mathbf{\tau }}}\Delta(x_{\mathbf{\tau}},y_{\mathbf{\tau}})\times\left(\frac{P^{\pi}_{m(x_{\mathbf{\tau }},y_{\mathbf{\tau}})}(x_{\mathbf{\tau}},y_{\mathbf{\tau}})}{\prod_{i=0}^{|\mathbf{\tau}|-1}P^{ \nu(\psi_{i};\tau_{i})}_{m(x_{\mathbf{\tau}},y_{\mathbf{\

### Auxiliary Concentration Lemmas

The following lemmas are the standard concentration of log-likelihood values of the models within the confidence set. The proofs are standard and can also be in _e.g.,_[1, 45, 44] and [20, 64]. We let \(\mathcal{D}^{k}\) be the dataset at the beginning of the \(k^{th}\) iteration in Algorithm 2. We denote \(\beta:=\log(K|\Theta|/\eta)\).

**Lemma A.4** (Uniform Bound on the Likelihood Ratios): _With probability \(1-\eta\) for any \(\eta>0\), for all \(k\in[K]\) and for any \(\theta\in\Theta\),_

\[\sum_{(\mathcal{T},\pi)\in\mathcal{D}^{k}}\log(\mathds{P}^{\pi}_{\theta}( \mathcal{T}))-\beta\leq\sum_{(\mathcal{T},\pi)\in\mathcal{D}^{k}}\log(\mathds{ P}^{\pi}_{\theta^{*}}(\mathcal{T})). \tag{15}\]

**Lemma A.5** (Concentration of Maximum Likelihood Estimators): _With probability \(1-\eta\), for all \(k\in[K]\), \(t\in[H]\) and \(\theta\in\Theta\), we have_

\[\sum_{(\mathcal{T},\pi)\in\mathcal{D}^{k}}\mathds{P}^{\pi}\left(\mathds{P}^{ \pi}_{\theta},\mathds{P}^{\pi}_{\theta^{*}}\right)(\mathcal{T})\leq\sum_{( \mathcal{T},\pi)\in\mathcal{D}^{k}}\log\left(\frac{\mathds{P}^{\pi}_{\theta^{* }}(\mathcal{T})}{\mathds{P}^{\pi}_{\theta}(\mathcal{T})}\right)+3\beta.\]

## Appendix B Additional Related Work

The literature on reinforcement learning theory is fast growing. While learning in fully observable systems has been extensively studied in the past decades, relatively little has been understood for online exploration in partially observable systems until recently. We review recent theoretical advances in reinforcement learning with partial observations that are closely related to us.

Exploration in POMDPsLearning a near-optimal policy in POMDPs is a notoriously hard problem [52] due to its full generality. In particular, the statistical complexity of online exploration in a general POMDP fundamentally suffers from the curse of horizon [39]. A earlier breakthrough involved introducing structural assumptions on system dynamics, which enable the recovery of underlying POMDPs model under the uniform ergodicity assumption [25, 5, 8, 23].

Recent theoretical breakthrough concerns the exploration problem in POMDPs without the ergodicity assumption [32, 45], initiating a remarkable progress in understanding the statistical complexity of reinforcement learning in POMDPs under proper structural assumptions. To this date, well-studied tractable POMDP classes (that overcome the curse of horizon) can be considered largely as a system with a "short-window" for efficient exploration [16, 1, 18, 17, 22, 45, 13]. The curse of the short-window assumption is the prior knowledge that the _a short consecutive_ execution of purely random actions is enough to obtain sufficient statistics of histories, _i.e.,_ the full-rankness of latent state-future observation emission matrices. A short sequence of uniformly random actions become the set of core tests of the system [51]. In such systems, learning the optimal policy only incurs \(\operatorname{poly}(S,A)\cdot A^{L}\) complexity [46] (with window-size \(L\)), breaking the curse of horizon.

Unfortunately, the same story does not apply in LMDPs, as there is no such "short-window" assumption that allows us to learn the sufficient statistics of histories. This calls for a set of new ideas and concepts, which could be of independent interest.

Off-Policy Evaluation in POMDPsAlong with the fast progress in online reinforcement learning with partial observations, there is a growing interest in off-policy evaluation with partial observations [47, 56, 61]. While the sample complexity of OPE has been extensively studied in MDPs under various model class assumptions [31, 60, 58], most existing OPE results in POMDPs are asymptotic in nature, and often suffers the curse of horizon due to their use of importance-weight sampling.

Recently, several recent works have proposed an alternative measure of coverage in the latent space, breaking the curse of horizon [50, 10, 65]. However, their results rely on the weakly-revealing assumptions that is often made in tractable POMDP classes [45], and can only evaluate within the class of memoryless policies. Our results are developed for the off-policy evaluation in LMDPs with several new concepts, which can also be of independent interest to off-policy evaluation problems in partially observed systems.

### Additional Details on the Full-Rankness Assumption

We give a more detailed explanation of the full-rankness assumption that has become popular in the POMDP literature [45]. As mentioned, the statistical sufficiency of core-tests, which is represented as the minimum singular value of the "latent state-future trajectory" emission matrix \(L\). Such an assumption has been exploited in the earlier work of LMDPs in [41], where the matrix \(L\) is defined in the following form:

\[L_{s}[m,(\psi_{\texttt{test}},\mathcal{T}_{t:H})]=P_{m}^{\nu(\psi_{\texttt{ test}};t)}(\mathcal{T}_{t:H}|s),\]

where \(\psi_{\texttt{test}}\in\Psi_{\texttt{test}}\) is a test policy, and \(\mathcal{T}_{t:H}\) is the future trajectory after time step \(t\). Therefore, direct application of POMDP approaches such as OMLE require the prior knowledge of \(\Psi_{\texttt{test}}\) and \(\sigma_{\min}(L_{s})>0\) for all \(s\in\mathcal{S}\). The rationale behind such assumptions is to ensure that a distribution of future trajectories can be converted to a belief of latent contexts, hence a distribution of future observations can serve as an alternative to a belief state. However, we are not given the set of core-tests \(\Psi_{\texttt{test}}\), or even the existence of \(\Psi_{\texttt{test}}\) that ensures \(\sigma_{\min}(L_{s})>0\) for general Latent MDPs.

With Separation.In a recent work by Chen et al. [14], a polynomial upper bound in \(M\) has been established under a notion of strong-separation between contexts with a sufficiently long time horizon \(H\). In essence, their assumptions guarantee that \(\sigma_{\min}(L_{s})>0\) holds for most of the states with _a priori_ given test policy, along with additional analysis for the tail of episodes. It is of great importance to identify such practical assumptions that lead to _fully_ polynomial upper bounds, especially for instances with some proper notion of separations even when no prior knowledge of test policies is provided.

## Appendix C Proofs for Section 3

### Proof of Lemma 3.1

This base case corresponds to Lemma 4.2 with \(M_{1}=M_{2}=1\). For convenience, we let \(\theta_{1}=\theta^{*}\) and \(\theta_{2}=\theta\), and thus, \(\mathds{P}_{1}=\mathds{P}_{\theta^{*}}\) and \(\mathds{P}_{2}=\mathds{P}_{\theta}\). We can show the inequality recursively:

\[\sum_{x_{1:H}}\sum_{y_{1:H}}\pi_{1:H}\left|\prod_{t=0}^{H}\mathds{ P}_{1}(y_{t}|x_{t})-\prod_{t=0}^{H}\mathds{P}_{2}(y_{t}|x_{t})\right|\] \[\leq\sum_{x_{1:H}}\sum_{y_{1:H-1}}\pi_{1:H}\left|\prod_{t=0}^{H-1 }\mathds{P}_{1}(y_{t}|x_{t})-\prod_{t=0}^{H-1}\mathds{P}_{2}(y_{t}|x_{t})\right| \sum_{y_{H}}\mathds{P}_{2}(y_{H}|x_{H})\] \[\quad+\sum_{x_{1:H}}\sum_{y_{1:H-1}}\pi_{1:H}\prod_{t=0}^{H-1} \mathds{P}_{1}(y_{t}|x_{t})\sum_{y_{H}}\left|\mathds{P}_{1}(y_{H}|x_{H})- \mathds{P}_{2}(y_{H}|x_{H})\right|.\]

Note that \(\sum_{y_{H}}\mathds{P}_{2}(y_{H}|x_{H})=1\) and

\[\sum_{x_{1:H-1}}\sum_{y_{1:H-1}}\pi_{1:H-1}\prod_{t=0}^{H-1}\mathds{P}_{1}(y_{ t}|x_{t})=\mathds{P}_{1}^{\pi}(s_{H}),\]

since we implicitly sum over \(s_{H}=s^{\prime}(y_{H-1})\) as we described in Appendix A.1. Thus,

\[\sum_{x_{1:H}}\sum_{y_{1:H}}\pi_{1:H}\left|\prod_{t=0}^{H}\mathds{ P}_{1}(y_{t}|x_{t})-\prod_{t=0}^{H}\mathds{P}_{2}(y_{t}|x_{t})\right| \leq\sum_{x_{1:H}}\sum_{y_{1:H-1}}\pi_{1:H}\left|\prod_{t=0}^{H-1 }\mathds{P}_{1}(y_{t}|x_{t})-\prod_{t=0}^{H-1}\mathds{P}_{2}(y_{t}|x_{t})\right|\] \[\quad+\sum_{x_{H},y_{H}}\mathds{P}_{1}^{\pi}(x_{H})\left|\mathds{ P}_{1}(y_{H}|x_{H})-\mathds{P}_{2}(y_{H}|x_{H})\right|.\]

Then we can show that

\[\sum_{x_{H},y_{H}}\mathds{P}_{1}^{\pi}(x_{H})\left|\mathds{P}_{1}(y _{H}|x_{H})-\mathds{P}_{2}(y_{H}|x_{H})\right|=\sum_{x_{H},y_{H}}\left(\frac{ \mathds{P}_{1}^{\pi}(x_{H})}{\mathds{P}_{1}^{\psi}(x_{H})}\right)\mathds{P}_{1 }^{\psi}(x_{H})\left|\mathds{P}_{1}(y_{H}|x_{H})-\mathds{P}_{2}(y_{H}|x_{H})\right|\] \[\leq C(\psi;\pi)\left(\sum_{x_{H},y_{H}}\left|\mathds{P}_{1}^{ \psi}(x_{H},y_{H})-\mathds{P}_{2}^{\pi}(x_{H},y_{H})\right|+\sum_{x_{H},y_{H}} \left|\mathds{P}_{1}^{\psi}(x_{H})-\mathds{P}_{2}^{\pi}(x_{H})\right|\mathds{P}_{ 2}(y_{H}|x_{H})\right)\]\[\leq 2C(\psi;\pi)\mathtt{TV}(\mathds{P}_{1}^{\psi},\mathds{P}_{2}^{\psi})( x_{H},y_{H}).\]

Applying the same step inductively from \(t=H\) to \(t=1\), we get the lemma.

### Proof of Lemma 3.2

Let \(\Psi_{\xi}:=\left\{\pi^{j^{*}(x,t)},\forall x,t\mid j^{*}(x,t):=\arg\max_{j\in 0,1,\ldots,k-1}\mathds{P}^{\pi^{j}}(x_{t}=x)\right\}\). Then, let \(\psi_{\xi}\in\Pi\) be a policy that can adapt to the predetermined checkpoint \(l\), such that \(\psi_{\xi}=\frac{1}{|\Psi_{\xi}|}\sum_{\psi\in\Psi_{\xi}}\psi\), _i.e.,_ a mixture of policies in \(\Psi_{\xi}\). Note that \(|\Psi_{\xi}|\leq HSA\). Lemma 3.1 tells us that

\[\mathtt{TV}(\mathds{P}_{1}^{\pi},\mathds{P}_{2}^{\pi})(\mathcal{T}) \leq 2\sum_{t\in[H]}C(\psi_{\xi};\pi)\cdot\mathtt{TV}(\mathds{P}_{ 1}^{\psi_{\xi}},\mathds{P}_{2}^{\psi_{\xi}})(\mathcal{T})\] \[\leq 2\sum_{t\in[H]}\sum_{\psi\in\Psi_{\xi}}\frac{C(\psi_{\xi}; \pi)}{|\Psi_{\xi}|}\cdot\mathtt{TV}(\mathds{P}_{1}^{\psi},\mathds{P}_{2}^{\psi })(\mathcal{T})\] \[\leq 2H\cdot\left(\frac{C(\psi_{\xi};\pi)}{\sqrt{|\Psi_{\xi}|}} \right)\sqrt{\sum_{j=0}^{k-1}\mathtt{TV}^{2}(\mathds{P}_{1}^{\pi^{j}},\mathds{P }_{2}^{\pi^{j}})(\mathcal{T})}. \tag{16}\]

Then we apply Lemma A.4 and Lemma A.5 to deduce that

\[\sum_{j=0}^{k-1}\mathtt{TV}^{2}(\mathds{P}_{1}^{\pi^{j}},\mathds{P}_{2}^{\pi^{ j}})(\mathcal{T})\leq\frac{16\beta}{n_{\mathtt{test}}}.\]

On the other hand, note that

\[C(\psi_{\xi};\pi^{k})=\max_{t\in[H]}\max_{x\in\mathcal{X}}\frac{\mathds{P}^{ \pi^{k}}(x_{t}=x)}{\mathds{P}^{\psi_{\xi}}(x_{t}=x)}\leq\max_{x\in\mathcal{S} \times\mathcal{A}}\frac{|\Psi_{\xi}|\cdot\mathds{P}^{\pi^{k}}(x_{t}=x)}{\max_ {j<k}\mathds{P}^{\pi^{j}}(x_{t}=x)}.\]

Now using the while loop condition, we have

\[\epsilon_{\mathtt{TV}} <\mathtt{TV}(\mathds{P}_{1}^{\pi^{k}},\mathds{P}_{2}^{\pi^{k}})( \mathcal{T})\leq 8H\cdot\max_{t\in[H]}\frac{C(\psi_{\xi};\pi^{k})}{\sqrt{|\Psi_{ \xi}|}}\sqrt{n_{\mathtt{test}}\beta}\] \[\leq 8H\cdot\sqrt{\frac{HSA\beta}{n_{\mathtt{test}}}}\max_{t\in[H] }\max_{x\in\mathcal{X}}\frac{\mathds{P}^{\pi^{k}}(x_{t}=x)}{\max_{j<k} \mathds{P}^{\pi^{j}}(x_{t}=x)}.\]

Rearranging the inequality, implies that there exists a \(t\in[H]\) and an \(x\in\mathcal{X}\) such that

\[\max_{j<k}\mathds{P}^{\pi^{j}}(x_{t}=x)\leq\frac{8H}{\epsilon_{\mathtt{TV}}} \sqrt{\frac{HSA\beta}{n_{\mathtt{test}}}}\cdot\mathds{P}^{\pi^{k}}(x_{t}=x).\]

### Proof of Theorem 3.3

We first show that Algorithm 1 terminates after \(K=HSA\log(1/\gamma)\) iterations where \(\gamma=\epsilon_{\mathtt{test}}^{2}/H^{2}\). We consider a perturbed model \(\hat{\theta}^{*}=(w,\hat{T},R)\) where

\[\hat{T}(\cdot|s,a)=(1-\gamma)T^{*}(\cdot|s,a)+\gamma\mathds{1},\]

By simulation lemma [38], for any \(\pi\in\Pi\), note that \(\mathtt{TV}(\mathds{P}_{1}^{\pi},\mathds{P}_{2}^{\pi})(y|x)\leq 2\gamma S\) for all \(x,y\), and thus

\[\mathtt{TV}(\mathds{P}_{\hat{\theta}^{*}}^{\pi},\mathds{P}_{\hat{ \theta}^{*}}^{\pi})(\mathcal{T}) =\sum_{x_{1:H}}\sum_{y_{1:H}}\pi_{1:H}\left|\prod_{t=0}^{H} \mathds{P}_{1}(y_{t}|x_{t})-\prod_{t=0}^{H}\mathds{P}_{2}(y_{t}|x_{t})\right|\] \[\leq\sum_{x_{1:H-1}}\sum_{y_{1:H-1}}\pi_{1:H-1}\left|\prod_{t=0}^ {H-1}\mathds{P}_{1}(y_{t}|x_{t})-\prod_{t=0}^{H-1}\mathds{P}_{2}(y_{t}|x_{t})\right|\] \[\quad+\sum_{x_{H}}\sum_{y_{H}}\mathds{P}_{1}^{\pi}(x_{H})\left| \mathds{P}_{1}(y_{H}|x_{H})-\mathds{P}_{2}(y_{H}|x_{H})\right|\]\[\leq\sum_{x_{1:H-1}}\sum_{y_{1:H-1}}\pi_{1:H-1}\left|\prod_{t=0}^{H-1} \mathds{P}_{1}(y_{t}|x_{t})-\prod_{t=0}^{H-1}\mathds{P}_{2}(y_{t}|x_{t})\right|+ 2\gamma S\] \[\leq...\leq 2S\gamma H.\]

For an arbitrary \(k\) iteration, we check whether the coverage doubling argument (Lemma D.1) still holds. To see this, first note that we can define \(\hat{\Psi}_{\xi}\), \(\hat{\psi}_{\xi}\) and \(\hat{C}(\hat{\psi}_{\xi};\pi)\) as in Lemma 3.2 in terms of \(\hat{\theta}^{*}\):

\[\hat{\Psi}_{\xi}:=\left\{\pi^{j^{*}(x,t)},\forall x,t\mid j^{*}(x,t):=\arg\max _{j\in 0,1,...,k-1}\mathds{P}_{\hat{\theta}^{*}}^{\gamma}(x_{t}=x),\right\},\]

and \(\hat{\psi}_{\xi}\) is a checkpoint-dependent policy where \(\hat{\psi}_{\xi}\) is a mixture of \(\hat{\Psi}_{\xi}\), and

\[\hat{C}(\psi;\pi):=\max_{t\in[H]}\max_{x\in\mathcal{X}}\frac{\mathds{P}_{\hat{ \theta}^{*}}^{\pi}(x_{t}=x)}{\mathds{P}_{\hat{\theta}^{*}}^{\psi}(x_{t}=x)}.\]

Now we invoke Lemma A.4 and Lemma A.5 to show that

\[\mathsf{TV}\left(\mathds{P}_{\hat{\theta}^{*}}^{\hat{\psi}_{\xi}},\mathds{P}_{\hat{\theta}}^{\hat{\psi}_{\xi}}\right)(\mathcal{T}) \leq\mathsf{TV}\left(\mathds{P}_{\hat{\theta}^{*}}^{\hat{\psi}_{ \xi}},\mathds{P}_{\hat{\theta}^{*}}^{\hat{\psi}_{\xi}}\right)(\mathcal{T})+ \mathsf{TV}\left(\mathds{P}_{\theta^{*}}^{\hat{\psi}_{\xi}},\mathds{P}_{\theta }^{\hat{\psi}_{\xi}}\right)(\mathcal{T})\] \[\leq 2SH\gamma+\frac{1}{|\hat{\Psi}_{\xi}|}\sum_{\psi\in\hat{\Psi }_{\xi}}\mathsf{TV}\left(\mathds{P}_{\theta^{*}}^{\psi},\mathds{P}_{\theta}^{ \psi}\right)(\mathcal{T}),\]

for all \(\theta\in\mathcal{C}^{k}\) using the triangle inequality for TV distance and \((a+b)^{2}\leq 2(a^{2}+b^{2})\). Thus, we can derive equation (16) in terms of \(\hat{\theta}^{*}\) as

\[\mathsf{TV}(\mathds{P}_{\hat{\theta}^{*}}^{\pi},\mathds{P}_{ \theta}^{\pi})(\mathcal{T}) \leq 2H\cdot\frac{\hat{C}(\hat{\psi}_{\xi};\pi)}{\sqrt{|\hat{\Psi }_{\xi}|}}\sqrt{\frac{16\beta}{n_{\mathsf{test}}}+(2SH)^{2}\gamma^{2}}\] \[\leq 2H\sqrt{HSA}\sqrt{\frac{16\beta}{n_{\mathsf{test}}}+(2SH)^{2 }\gamma^{2}}\cdot\max_{t\in[H]}\max_{x\in\mathcal{X}}\frac{\mathds{P}_{\hat{ \theta}^{*}}^{\pi^{k}}(x_{t}=x)}{\max_{j<k}\mathds{P}_{\hat{\theta}^{*}}^{\psi }(x_{t}=x)}\]

On the other hand,

\[\max\left(\mathsf{TV}(\mathds{P}_{\hat{\theta}^{*}}^{\pi_{k}},\mathds{P}_{ \theta_{1}}^{\pi_{k}})(\mathcal{T}),\mathsf{TV}(\mathds{P}_{\hat{\theta}^{*}} ^{\pi_{k}},\mathds{P}_{\theta_{2}}^{\pi_{k}})(\mathcal{T})\right)>2\epsilon_{ \mathsf{TV}}-2SH\gamma>1.5\epsilon_{\mathsf{TV}},\]

by setting \(\gamma=\epsilon_{\mathsf{test}}/(4SAH)^{4}\). Let \(n_{\mathsf{test}}=64\beta(H^{3}SA)/\epsilon_{\mathsf{test}}^{2}\), and we have

\[2\epsilon_{\mathsf{TV}}<\epsilon_{\mathsf{test}}\cdot\max_{t\in[H]}\max_{x \in\mathcal{X}}\frac{\mathds{P}_{\hat{\theta}^{*}}^{\pi}(x_{t}=x)}{\max_{j<k} \mathds{P}_{\hat{\theta}^{*}}^{\psi}(x_{t}=x)}.\]

Hence the same doubling argument holds, and MDP-OmLE (Algorithm 1) will terminate after at most

\[K=O(HSA\log(HSA/\epsilon_{\mathsf{test}}))\]

iterations. We note that all inequalities hold for all \(K\) iterations with probability at least \(1-\eta\). Finally, by setting \(\epsilon_{\mathsf{test}}=\epsilon_{\mathsf{TV}}\) and \(\epsilon_{\mathsf{TV}}=\epsilon/H\), we can conclude that the total number of trajectories that was generated by MDP-OmLE is bounded by

\[O(1)\cdot H^{6}S^{2}A^{2}\beta\log(HSA/\epsilon)/\epsilon^{2}.\]

## Appendix D Proofs for Section 4

### Proof Sketch

In this section, we provide an overview of the proof for Theorem 4.5. Compared to Algorithm 1, the main differences in LMDPs from the MDP cases are two-fold:

1. Our goal is to find the optimal _history-dependent_ (non-Markovian) policy.

2. We cannot observe any context-segment pair that previous behavioral policies could not cover.

For the first point, (a), we already have reduced the problem from matching all history-dependent policies to a set of behavioral policies generated by the concatenation of memoryless policies in Lemma 4.4. For the second point, (b), even though we cannot observe the latent context \(m\) under which each segment is covered, we can improve the coverage of each context-segment pair given the test set \(\Psi^{k}_{\texttt{test}}\) every iteration:

**Lemma D.1**: _Let \(n_{\texttt{test}}\geq 3\beta M^{2}(8H^{2})^{d}(MS^{2}A^{2})^{d}/\epsilon_{ \texttt{test}}^{2}\). Then with probability at least \(1-\eta\), at every \(k^{th}\) iteration in Algorithm 2, there must exist at least one \((m,t_{1},t_{2},s,s^{\prime})\), such that_

\[P^{\pi^{k}}_{m}(s_{t_{2}}=s|s^{\prime}_{t_{1}}=s^{\prime})>2\cdot\max_{\psi\in \Psi^{k-1}_{\texttt{test}}}P^{\psi}_{m}(s_{t_{2}}=s|s^{\prime}_{t_{1}}=s^{ \prime}).\]

That is, we can ensure that the coverage of at least one context-segment pair is being exponentially improved despite the unobservability of latent contexts \(m\).

For a moment, to simplify the discussion, we first assume that the uniformly random policy \(\texttt{Unif}(A)\) has a non-zero \(\gamma>0\) probability for covering all segments in all contexts:

\[P^{\texttt{Unif}(A)}_{m}(s_{t_{2}}=s_{2}|s_{t_{1}}=s_{1})\geq\gamma,\qquad \forall t_{1}<t_{2},m,s_{1},s_{2}. \tag{17}\]

We note that this assumption will be eventually removed in our final result. Thus, if we start from the initial coverage \(\gamma>0\) over all context-segment pairs, then since every probability is in the range of \([0,1]\), this doubling-up event can happen at most \(\log(1/\gamma)\) times for every context-segment pair. Therefore, Algorithm 2 must terminate after at most \(K=MS^{2}H\log(1/\gamma)\) iterations.

Separately from the coverage improvement in Lemma D.1, the standard concentration of the confidence set on the generated trajectory data is given by the maximum-likelihood estimation:

**Lemma D.2**: _With probability at least \(1-\eta\), for all \(k^{th}\) iterations in Algorithm 2, let \(\Psi_{\xi}=\{\psi_{1},...,\psi_{n}\}\subseteq\Psi^{k-1}_{\texttt{test}}\) be any subset of candidate test policies, and let \(\mathbf{\psi}_{\xi}=(\psi_{\xi},...,\psi_{\xi},\)\(\texttt{Unif}(\mathcal{A}))\) where \(\psi_{\xi}:=\frac{1}{n}\sum_{i\in[n]}\psi_{i}\) is a mixture of policies in \(\Psi_{\xi}\). Then for any model in the confidence set \(\theta\in\mathcal{C}\), the following holds:_

\[\sum_{\mathbf{\tau}\in\texttt{SubSeq}(H,d)}\sum_{\mathbf{z}\subseteq\{0,1\}^{\emptyset }\otimes|\tau|}\text{TV}^{2}\left(\mathds{P}^{\nu(\mathbf{\psi}_{\xi};\mathbf{\tau}, \mathbf{z})}_{\theta^{*}},\mathds{P}^{\nu(\mathbf{\psi}_{\xi};\mathbf{\tau},\mathbf{z})}_{ \theta}\right)(x_{\mathbf{\tau}},y_{\mathbf{\tau}})\leq\frac{4\beta}{n^{d}\cdot n_{ \texttt{test}}}.\]

Equipped with Lemma D.2 With probability at least \(1-\eta\), we terminate the while-loop after at most \(K=HS^{2}M\log(1/\gamma)\) iterations, and each while-loop generates \(K^{d-1}\cdot n_{\texttt{test}}\) new trajectories, leading to a total

\[K^{d}\cdot n_{\texttt{test}}=(8M^{2}S^{4}H^{3}A^{2}\log(1/\gamma))^{d}\cdot O( M^{2}\beta/\epsilon_{\texttt{test}}^{2})\]

sample complexity. The near-optimality guarantee for the final empirical model is given by Lemma 4.4 where we set \(\epsilon_{\texttt{test}}=M^{-1}(2H^{2}MSA)^{-d}\cdot\epsilon_{\texttt{TV}}\) to obtain an \((H\epsilon_{\texttt{TV}})\)-optimal policy.

Without Initial Coverage.Now we remove the initial \(\gamma>0\) coverage assumption (17). To do so, consider a virtual LMDP model \(\hat{\theta}=(\{w^{*}_{m},\hat{T}_{m},R^{*}_{m}\})^{M}_{m=1}\) with perturbed transition models, _i.e., \(\hat{T}_{m}(\cdot|s,a)=(1-\gamma)T^{*}_{m}(\cdot|s,a)+\gamma\mathds{1}\)_ for all \((s,a)\in\mathcal{S}\times\mathcal{A}\). For \(\hat{\theta}\), it is easy to see that for all \(\pi\in\Pi\), we have \(\text{TV}(\mathds{P}^{\pi}_{\hat{\theta}},\mathds{P}^{\pi_{*}}_{\hat{\theta}}) (\tau)\leq 2HS\gamma\). Thus, now we can shift our arguments to this perturbed model with sufficiently small \(\gamma\), and it is straightforward that the perturbed model has the \(\gamma>0\) segment coverage for any policy, which concludes Theorem 4.5.

### Proof of Lemma 4.2

We start from equation (12) in Lemma A.1. We refer the reader to Appendix A.2 for the used notation here, with additional notation we define here: for a subset of indices \(I\subseteq[|\mathbf{\tau}|]\), we write \(\mathbf{\tau}_{I}:=(\tau_{i})_{i\in I}\) to refer to a subsequence of \(\mathbf{\tau}\) at positions \(I\), and \(\mathbf{\tau}_{/I}\) a subsequence at positions outside of \(I\).

We can first bound \(\Delta(x_{\mathbf{\tau}},y_{\mathbf{\tau}})\) as the following:

\[\Delta(x_{\mathbf{\tau}},y_{\mathbf{\tau}})\leq\sum_{I\subseteq[|\mathbf{\tau}|]} \left(\prod_{i\in I}l_{\tau_{i}}^{i-1}(x_{\tau_{1:i-1}},y_{\tau_{1:i-1}})\right)\times\] \[\Bigg{|}\sum_{m}w_{m}^{1}\left(\prod_{i\in I}P_{m}^{1,\nu(\psi_{i- 1};\tau_{i-1})}(s_{\tau_{i}+1}|s_{\tau_{i-1}+1})\right)\left(\prod_{i\in[|\mathbf{ \tau}|]/I}P_{m}^{1,\nu(\psi_{i-1};\tau_{i-1})}(s_{\tau_{i}}|s_{\tau_{i-1}+1})P_ {m}^{1}(y_{\tau_{i}}|x_{\tau_{i}})\right)\] \[\quad-\sum_{m}w_{m}^{2}\left(\prod_{i\in I}P_{m}^{2,\nu(\psi_{i-1} ;\tau_{i-1})}(s_{\tau_{i}+1}|s_{\tau_{i-1}+1})\right)\left(\prod_{i\in[|\mathbf{ \tau}|]/I}P_{m}^{2,\nu(\psi_{i-1};\tau_{i-1})}(s_{\tau_{i}}|s_{\tau_{i-1}+1})P_ {m}^{2}(y_{\tau_{i}}|x_{\tau_{i}})\right)\Bigg{|}\] \[=\sum_{I\subseteq[|\mathbf{\tau}|]}\frac{\prod_{i\in I}l_{\tau_{i}}^{i -1}(x_{\tau_{1:i-1}},y_{\tau_{1:i-1}})}{\prod_{i\in[\mathbf{\tau}]/I}(1/A)}\delta_ {\nu(\mathbf{\psi};\mathbf{\tau},z(I;\mathbf{\tau}))}(s^{\prime}_{\mathbf{\tau}_{I}},x_{\mathbf{ \tau}_{I}},y_{\mathbf{\tau}_{I}}),\]

where \(z(I;\mathbf{\tau})\) satisfies \(z(I;\mathbf{\tau})_{j}=0\) if \(j\in I\) and 1 otherwise. Then we can observe that

\[\frac{\prod_{i\in I}l_{\tau_{i}}^{i-1}(x_{\tau_{1:i-1}},y_{\tau_{1 :i-1}})}{\prod_{i\in[q]/I}(1/A)}\cdot\frac{1}{\prod_{i=0}^{q-1}P_{m(x_{\mathbf{ \tau}},y_{\mathbf{\tau}})}^{\nu(\psi_{i};\tau_{i})}(s_{\tau_{i+1}}|s_{\tau_{i}+1})P _{m(x_{\mathbf{\tau}},y_{\mathbf{\tau}})}(y_{\tau_{i+1}}|x_{\tau_{i+1}})}\] \[\leq\frac{1}{\prod_{i\in I}P_{m(x_{\mathbf{\tau}},y_{\mathbf{\tau}})}^{ \nu(\psi_{i-1};\tau_{i-1})}(s_{\tau_{i}+1}|s_{\tau_{i-1}+1})\cdot\prod_{i\in[q ]/I}P_{m(x_{\mathbf{\tau}},y_{\mathbf{\tau}})}^{\nu(\psi_{i-1};\tau_{i-1})}(x_{\tau_{ i}}|s_{\tau_{i-1}+1})P_{m(x_{\mathbf{\tau}},y_{\mathbf{\tau}})}(y_{\tau_{i}}|x_{\tau_{i}})}\] \[=\frac{1}{P_{m(x_{\mathbf{\tau}},y_{\mathbf{\tau}})}^{\nu(\psi;\mathbf{\tau},z(I;\mathbf{\tau}))}(s^{\prime}_{\mathbf{\tau}_{I}},x_{\mathbf{\tau}_{I}},y_{\mathbf{\tau}_{ I}})}, \tag{18}\]

using inequality that can be derived by definition of \(l_{t}^{i}\):

\[l_{\tau_{i}}^{i-1}\leq\frac{P_{m(x_{\mathbf{\tau}},y_{\mathbf{\tau}})}^{ \nu(\psi_{i-1};\tau_{i-1})}(s_{\tau_{i}}|s_{\tau_{i-1}+1})}{P_{m(x_{\mathbf{\tau}}, y_{\mathbf{\tau}})}^{\nu(\psi_{i-1};\tau_{i-1})}(s_{\tau_{i}+1}|s_{\tau_{i-1}+1})}P_{m(x_{ \mathbf{\tau}},y_{\mathbf{\tau}})}(y_{\tau_{i}}|x_{\tau_{i}}).\]

We are now ready to use this inequality to bound the TV distance between trajectory distribution via equation (12). To do so, we rearrange the summation orders as follows:

\[\sum_{x_{1:H}}\sum_{y_{1:H}}\pi_{1:H}\left|\sum_{m=1}^{M_{1}}w_{m }^{1}\prod_{t=0}^{H}P_{m}^{1}(y_{t}|x_{t})-\sum_{m=1}^{M_{2}}w_{m}^{2}\prod_{t= 0}^{H}P_{m}^{2}(y_{t}|x_{t})\right|\] \[\leq\sum_{\mathbf{\tau}\subseteq[H]}\sum_{m\in[M_{1}]}\sum_{x_{\mathbf{ \tau}},y_{\mathbf{\tau}}:m(x_{\mathbf{\tau}},y_{\mathbf{\tau}})=m}\Delta(x_{\mathbf{\tau}},y_{ \mathbf{\tau}})\times\left(\frac{P_{m}^{\pi}(x_{\mathbf{\tau}},y_{\mathbf{\tau}})}{\prod_{i =0}^{q-1}P_{m}^{\nu(\psi_{i};\tau_{i})}(s_{\tau_{i+1}}|s_{\tau_{i}+1})P_{m}(y _{\tau_{i+1}}|x_{\tau_{i+1}})}\right)\] \[\leq\sum_{\mathbf{\tau}\subseteq[H]}\sum_{m\in[M_{1}]}\sum_{x_{\mathbf{ \tau}},y_{\mathbf{\tau}}:m(x_{\mathbf{\tau}},y_{\mathbf{\tau}})}\sum_{m}\sum_{I\subseteq[q ]}\left(\frac{P_{m}^{\pi}(x_{\mathbf{\tau}},y_{\mathbf{\tau}})\cdot\delta_{\nu(\mathbf{ \psi};\mathbf{\tau},z(I;\mathbf{\tau}))}(s^{\prime}_{\mathbf{\tau}_{I}},x_{\mathbf{\tau}_{I}},y _{\mathbf{\tau}_{I}})}{P_{m}^{\nu(\mathbf{\psi};\mathbf{\tau},z(I;\mathbf{\tau}))}(s^{\prime}_{ \mathbf{\tau}_{I}},x_{\mathbf{\tau}_{I}},y_{\mathbf{\tau}_{I}})}\right)\] \[\leq\sum_{\mathbf{\tau}\subseteq[H]}\sum_{m\in[M_{1}]}\sum_{I \subseteq[q]}\sum_{s^{\prime}_{\mathbf{\tau}_{I}},x_{\mathbf{\tau}_{I}},y_{\mathbf{\tau} _{I}}}\left(\frac{P_{m}^{\nu(\mathbf{\psi};\mathbf{\tau},z(I;\mathbf{\tau}))}}{P_{m}^{\nu (\mathbf{\psi};\mathbf{\tau},z(I;\mathbf{\tau}))}(s^{\prime}_{\mathbf{\tau}_{I}},x_{\mathbf{\tau}_{ I}},y_{\mathbf{\tau}_{I}})}{P_{m}^{\nu(\mathbf{\psi};\mathbf{\tau},z(I;\mathbf{\tau}))}(s^{\prime}_{\mathbf{ \tau}_{I}},x_{\mathbf{\tau}_{I}},y_{\mathbf{\tau}_{I}})}\right)\] \[\leq M\cdot C(\mathbf{\psi};\mathbf{\tau})\cdot\sum_{q\leq d}\sum_{\mathbf{ \tau}\subseteq[H],|\mathbf{\tau}|=q}\sum_{I\subseteq[q]}\text{TV}\left(\mathbb{P }_{1}^{\nu(\mathbf{\psi};\mathbf{\tau},z(I;\mathbf{\tau}))},\mathbb{P}_{2}^{\nu(\mathbf{\psi}; \mathbf{\tau},z(I;\mathbf{\tau}))}\right)(s^{\prime}_{\mathbf{\tau}_{I}},x_{\mathbf{\tau}_{I} },y_{\mathbf{\tau}_{I}})\] \[\leq M\cdot C(\mathbf{\psi};\mathbf{\tau})\cdot\sum_{q\leq d}\sum_{\mathbf{ \tau}\subseteq[H],|\mathbf{\tau}|=q}\sum_{I\subseteq[q]}\sum_{I\subseteq[q]}\text{TV} \left(\mathbb{P}_{1}^{\nu(\mathbf{\psi};\mathbf{\tau},z(I;\mathbf{\tau}))},\mathbb{P}_{2}^{ \nu(\mathbf{\psi};\mathbf{\tau},z(I;\mathbf{\tau}))}\right)(x_{\mathbf{\tau}},y_{\mathbf{\tau}}),\]

proving Lemma 4.2.

### Proof of Lemma 4.4

Before proceeding to the proof, we refer the reader to Appendix A.3 for additional preliminaries. Recall the definition of \(\Psi_{\xi}\) in Lemma A.3:

\[\Psi_{\xi}=\left\{\arg\max_{\psi\in\Psi_{\texttt{test}}}P_{m}^{ \nu(\psi;t_{0})}(s_{t_{0}+t}=s|s^{\prime}_{t_{0}}=s^{\prime}),\;\forall m,s, s^{\prime},t\right\}\subseteq\Psi_{\texttt{test}}.\]Note that the above definition is invariant to \(t_{0}\).

Now consider \(\Psi_{\texttt{test}}=\Pi_{\text{mls}}\). Then, \(\Psi_{\xi}\) is the set of policies that maximize the probability \(P^{\nu(\psi;t_{0})}_{m}(s_{t_{0}+t}=s|s^{\prime}_{t_{0}}=s^{\prime})\), _i.e.,_ a memoryless policy that aims to reach \(s\) after \(t\) time steps (this policy is invariant to the starting state \(s^{\prime}\)). Therefore, we can first induce that \(|\Psi_{\xi}|\leq MHS\). Then, the Markovian policy maximizing the probability to reach a certain state under a fixed context \(m\) is also best within the history-dependent policy class \(\Pi\), and therefore

\[\rho(\Pi_{\text{mls}};\pi):=\max_{t,t_{0}}\max_{m,s,s^{\prime}}\frac{\max_{ \Upsilon_{t:t_{0}}}P^{\pi}_{m}(s_{t+t_{0}}=s|s^{\prime}_{t_{0}}=s^{\prime}, \mathcal{T}_{1:t_{0}})}{\max_{\psi\in\Psi_{\xi}}P^{\nu(\psi;t_{0})}_{m}(s_{t+ t_{0}}=s|s^{\prime}_{t_{0}}=s^{\prime})}\leq 1.\]

We can now invoke Lemma 4.2 and Lemma A.3, and noting that

\[\texttt{TV}(\mathds{P}^{\pi}_{1},\mathds{P}^{\pi}_{2})(s^{\prime}_{\mathbf{\tau}_ {I}},x_{\mathbf{\tau}_{J/1}},y_{\mathbf{\tau}_{J/1}})\leq\texttt{TV}(\mathds{P}^{\pi}_ {1},\mathds{P}^{\pi}_{2})(\mathcal{T}).\]

Let \(\mathbf{\psi}_{\xi}\) be as defined in Lemma A.3, and for any \(\pi\in\Pi\), we can conclude that

\[\texttt{TV}(\mathds{P}^{\pi}_{1},\mathds{P}^{\pi}_{2})(\mathcal{T}) \leq M\cdot C(\mathbf{\psi}_{\xi};\pi)\sum_{\mathbf{\tau}\in\text{Subseq}(H,d)}\sum_{\mathbf{\ell}\in[|\mathbf{\tau}|]}\texttt{TV}\left(\mathds{P}^{\nu(\mathbf{\psi} _{\xi};\mathbf{\tau},z(I;\mathbf{\tau}))}_{1},\mathds{P}^{\nu(\mathbf{\psi}_{\xi};\mathbf{\tau },z(I;\mathbf{\tau}))}\right)(s^{\prime}_{\mathbf{\tau}_{I}},x_{\mathbf{\tau}_{J/1}},y_{ \mathbf{\tau}_{J/1}})\] \[\leq M\cdot(MHSA)^{d}\sum_{\mathbf{\tau}\in\text{Subseq}(H,d)}\sum_{ \mathbf{z}\in\{0,1\}^{|\mathbf{\tau}|}}\texttt{TV}\left(\mathds{P}^{\nu(\mathbf{\psi}_{ \xi};\mathbf{\tau},\mathbf{z})}_{1},\mathds{P}^{\nu(\mathbf{\psi}_{\xi};\mathbf{\tau},\mathbf{z})} _{2}\right)(\mathcal{T}).\]

Finally, it only remains to bound the total variation distance with \(\nu(\mathbf{\psi}_{\xi};\mathbf{\tau},\mathbf{z})\). To see this, for a given \(q\leq d\), \(\mathbf{\tau}\in\text{Subseq}(H,d)\), and \(\mathbf{z}\in\{0,1\}^{|\mathbf{\tau}|}\), it is easy to check that

\[\texttt{TV}\left(\mathds{P}^{\nu(\mathbf{\psi}_{\xi},\mathbf{\tau},\mathbf{z})}_{1}, \mathds{P}^{\nu(\mathbf{\psi}_{\xi};\mathbf{\tau},\mathbf{z})}_{2}\right)(\mathcal{T}) \leq\max_{\mathbf{\psi}\in\Pi^{\text{O}(q+1)}_{\text{mls}}}\texttt{TV}\left( \mathds{P}^{\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{z})}_{1},\mathds{P}^{\nu(\mathbf{\psi};\bm {\tau},\mathbf{z})}_{2}\right)(\mathcal{T})\leq\epsilon_{\texttt{test}}.\]

Noting that \(\sum_{q\leq d}\binom{H}{q}\leq\min(H^{d},2^{H})\), assuming we are in the regime \(H\gg d\), we conclude that

\[\texttt{TV}(\mathds{P}^{\pi}_{1},\mathds{P}^{\pi}_{2})(\mathcal{T})\leq M( MSA)^{d}\cdot(2H)^{d}\cdot\epsilon_{\texttt{test}},\]

concluding the proof.

### Proof of Theorem 4.5

We continue from the conclusion of Lemma D.1, and the remaining step is to ensure that \(\texttt{LMDP-OMLE}\) terminates after \(K=MS^{2}H\log(1/\gamma)\) iterations where \(\gamma=\epsilon_{\texttt{test}}^{2}/(H^{2d})\) without the initial coverage assumption (17). We consider a perturbed model \(\hat{\theta}^{*}=(\{w^{*}_{m},\hat{T}_{m},R^{*}_{m}\}_{m=1}^{M})\) where

\[\hat{T}_{m}(\cdot|s,a)=(1-\gamma)T^{*}_{m}(\cdot|s,a)+\gamma\mathds{1},\]

for all \(m\). By the performance difference lemma [35], for any \(\pi\in\Pi\),

\[\texttt{TV}(\mathds{P}^{\pi}_{\hat{\theta}^{*}},\mathds{P}^{\pi}_{\hat{\theta}^ {*}})(\mathcal{T})\leq\sum_{m}w^{*}_{m}\cdot\texttt{TV}(\mathds{P}^{\pi}_{ \hat{\theta}^{*}},\mathds{P}^{\pi}_{\hat{\theta}^{*}})(\mathcal{T}|m)\leq 2S \gamma H.\]

For every \(k^{th}\) iteration, we check whether the coverage doubling argument (Lemma D.1) still holds. To see this, first note that we can define \(\hat{\rho}(\Psi^{k}_{\texttt{test}};\pi)\) and \(\hat{\Psi}_{\xi}\), \(\hat{\psi}_{\xi}\) and \(\hat{C}(\mathbf{\psi}_{\xi};\pi)\) as in Lemma A.3 in terms of \(\hat{\theta}^{*}\). Then Lemma D.2 can be modified to guarantee that

\[\sum_{\mathbf{\tau}\in\text{Subseq}(H,d)}\sum_{\mathbf{z}\in\{0,1\}^{|\mathbf{\tau}|}} \texttt{TV}^{2}\left(\mathds{P}^{\nu(\hat{\mathbf{\psi}}_{\xi};\mathbf{\tau},\mathbf{z})}_{ \hat{\theta}^{*}},\mathds{P}^{\nu(\hat{\mathbf{\psi}}_{\xi};\mathbf{\tau},\mathbf{z})}_{ \hat{\theta}^{*}}\right)(\mathcal{T})\leq\frac{16\beta}{n^{d}\cdot n_{\texttt{ test}}}+2(2H)^{d}(2SH)^{2}\gamma^{2},\]

for all \(\theta\in\mathcal{C}^{k}\) using the triangle inequality for TV distance and \((a+b)^{2}\leq 2(a^{2}+b^{2})\). Thus, we can derive (19) in terms of \(\hat{\theta}^{*}\) as

\[\texttt{TV}(\mathds{P}^{\pi}_{\hat{\theta}^{*}},\mathds{P}^{\pi}_{\hat{\theta}}) (\mathcal{T})\leq 8M(nA\hat{\rho}(\hat{\Psi}_{\xi};\pi))^{d}\sqrt{\frac{(2H)^{d}\beta}{n^{d} \cdot n_{\texttt{test}}}+(2H)^{d}(4SH)^{2}\gamma^{2}}.\]On the other hand,

\[\max\left(\mathtt{TV}(\mathds{P}_{\hat{\theta}^{*}}^{\pi_{*}},\mathds{P}_{\theta_{1 }}^{\pi_{*}})(\mathcal{T}),\mathtt{TV}(\mathds{P}_{\hat{\theta}^{*}}^{\pi_{*}}, \mathds{P}_{\theta_{2}}^{\pi_{*}})(\mathcal{T})\right)>2\epsilon_{\mathtt{TV}}- 2SH\gamma>1.5\epsilon_{\mathtt{TV}},\]

Now we set \(\gamma=\epsilon_{\mathtt{test}}^{2}(8HnA)^{-2d}(4MSH)^{-2}\) with \(n_{\mathtt{test}}=64M^{2}\beta(HnA^{2})^{d}/\epsilon_{\mathtt{test}}^{2}\) and \(n=MHS^{2}\), and we have

\[2\epsilon_{\mathtt{TV}}^{2}<4^{-d}\cdot\hat{\rho}(\mathds{P}_{\mathtt{test}}^{ k-1},\pi^{k})^{d}\epsilon_{\mathtt{test}}^{2}.\]

Hence the same doubling argument holds, and Algorithm 2 will terminate after at most

\[K=O(MdS^{2}H\log(MHSA/\epsilon_{\mathtt{test}}))\]

iterations. We note that all inequalities hold for all \(K\) iterations with probability at least \(1-\eta\). Finally, we invoke Lemma 4.4 with \(\epsilon_{\mathtt{test}}=\epsilon_{\mathtt{TV}}\cdot\mathrm{poly}(H,M,S,A)^{-d}\), \(\epsilon_{\mathtt{TV}}=\epsilon/(4MSAH)^{d}\) and \(d=2M-1\), we can conclude that the total number of trajectories we generated is bounded by

\[\mathrm{poly}(M,H,S,A,\log(MHSA/\epsilon))^{d}/\epsilon^{2}.\]

### A Counter Example for Remark 4.3

One may wonder why it is not sufficient to consider a single latent-state coverability analogous to Lemma 3.1, analogous to equation (2), defined as the following:

\[\max_{t\in[H]}\max_{x\in\mathcal{X}}\max_{m\in[M]}\frac{P_{m}^{\pi}(x_{t}=x)}{ P_{m}^{\pi^{0}}(x_{t}=x)}.\]

Here we present a counter example where the multi-step events must be considered even in the latent state space: the LMDP consists of \(M=3\) MDPs with \(\mathcal{S}=\{1,2\},\mathcal{A}=\{1,2\}\), \(\mathcal{R}=\{-1,0,1\}\), and \(H=2\). All MDP starts from \(s_{1}=1\) and with a transition kernel \(T_{m}(s_{2}=2|s_{1},a)=1\) for all \(m\in[M]\) and \(a\in\mathcal{A}\). Reward models are given by \(R_{1}(r=-1|s=1,a=1)=1\), \(R_{2}(r=1|s=1,a=2)=1\), and \(R_{m}(r|s=1,a=1)=0.5\) for \(r\in\{0,1\}\), \(m\in\{2,3\}\), and \(R_{m}(r|s=1,a=2)=0.5\) for \(r\in\{-1,0\}\), \(m\in\{1,3\}\).

Now we target to measure the expected rewards of actions executed by a behavioral policy at \(s_{2}=2\) as in Table 1. In this example, all actions are covered under all contexts following the behavior policy, _i.e._, \(P_{m}(s_{2},a_{2})>0\). Yet, we cannot estimate the expected reward of action \(a_{2}=1\) under context \(m=1\). Therefore, the speculated single latent-state coverage coefficient is finite for this problem, however, off-policy evaluation guarantee cannot be established only with the single latent-state coverability.

## Appendix E Deferred Proofs

### Proof of Lemma d.1

For any fixed checkpoint \(t_{0}\), recall the definition \(\Psi_{\xi}\) as defined in equation (14):

\[\Psi_{\xi}=\left\{\arg\max_{\psi\in\Pi_{\mathtt{test}}^{k}}P_{m}^{\nu(\psi;t_ {0})}(s_{t+t_{0}}=s|s_{t_{0}}^{\prime}=s^{\prime}),\ \forall m,s,s^{\prime},t \right\}.\]

Note that \(|\Psi_{\xi}|\leq MS^{2}H\) and invariant to \(t_{0}\) since \(\Psi_{\mathtt{test}}^{k-1}\subset\Pi_{\mathrm{mls}}\). Now for any memoryless policy \(\pi\in\Pi_{\mathrm{mls}}\), we recall Lemma 4.2 with \(\psi_{\xi}=(\psi_{\xi},...,\psi_{\xi},\mathtt{Unif}(\mathcal{A}))\), where \(\psi_{\xi}\in\Pi\) is a mixture policy of \(\Psi_{\xi}\). We have

\[\mathtt{TV}(\mathds{P}_{\theta^{*}}^{\pi},\mathds{P}_{\theta}^{\pi})(\mathcal{ T})\leq MC(\mathbf{\psi}_{\xi};\pi)\sum_{\mathbf{\tau}\in\mathtt{Subseq}(H,d)}\sum_{\mathbf{z} \in\{0,1\}^{|\mathbf{\tau}|}}\mathtt{TV}\left(\mathds{P}_{\theta^{*}}^{\nu(\mathbf{ \psi};\mathbf{\tau},\mathbf{z})},\mathds{P}_{\theta}^{\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{z} )}\right)(\mathcal{T})\]

\begin{table}
\begin{tabular}{|c|c|c|} \hline History (with Possible Contexts) & \(a_{2}=1\) & \(a_{2}=2\) \\ \hline \(a_{1}=1,r_{1}=-1\) (\(m=1\)) &? & \(\mathbb{E}[r_{2}]=1\) \\ \hline \(a_{1}=2,r_{1}=1\) (\(m=2\)) & \(\mathbb{E}[r_{2}]=-1\) &? \\ \hline \(a_{1}=2,r_{1}\neq 1\) (\(m=1\) or \(3\)) & \(\mathbb{E}[r_{2}]=0\) &? \\ \hline \(a_{1}=1,r_{1}\neq-1\) (\(m=2\) or \(3\)) &? & \(\mathbb{E}[r_{2}]=0\) \\ \hline \end{tabular}
\end{table}
Table 1: The first step generates one of four possible beliefs of a history. Measured elements in the table denote the expected rewards of actions at \(t=2\) following a behavioral policy. We can see that for all \(m\in[M],a\in\mathcal{A}\) it holds that \(\mathds{P}_{m}(s_{2},a_{2})>0\) for all \(m\in[M],a_{2}\in\mathcal{A}\); yet, we cannot estimate \(\mathbb{E}[r_{2}]\) given some histories.

\[\leq MC(\mathbf{\psi}_{\xi};\pi)\sqrt{(2H)^{d}}\cdot\sqrt{\sum_{\mathbf{\tau} \in\mathtt{Subseq}(H,d)}\sum_{\mathbf{z}\in\{0,1\}^{|\mathbf{\tau}|}}\mathtt{TV}^{2} \left(\mathbb{E}_{\theta^{*}}^{\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{z})},\mathbb{P}_{ \theta}^{\nu(\mathbf{\psi};\mathbf{\tau},\mathbf{z})}\right)(\mathcal{T})\] \[\leq 4M(nA\rho(\Psi_{\mathtt{test}}^{k-1};\pi))^{d}\sqrt{\frac{(2H) ^{d}\beta}{n^{d}\cdot n_{\mathtt{test}}}}, \tag{19}\]

where the last inequality follows by Lemma D.2. By the while-loop condition, for \(\pi_{k}\in\Pi_{\mathrm{mls}}\), we have

\[\max\left(\mathtt{TV}(\mathbb{E}_{\theta^{*}}^{\pi_{k}},\mathbb{P}_{\theta_{1} }^{\pi_{k}})(\mathcal{T}),\mathtt{TV}(\mathbb{P}_{\theta^{*}}^{\pi_{k}}, \mathbb{P}_{\theta_{2}}^{\pi_{k}})(\mathcal{T})\right)>2\varepsilon_{\mathtt{ TV}},\]

by the triangle inequality for TV distance. Therefore, we can conclude that

\[4\varepsilon_{\mathtt{TV}}^{2}<M^{2}(A\rho(\Psi_{\mathtt{test}}^{k-1};\pi^{k} ))^{2d}\cdot\frac{16(2nH)^{d}\beta}{n_{\mathtt{test}}}.\]

Plugging \(n_{\mathtt{test}}=64M^{2}\beta(8HnA^{2})^{d}/\epsilon_{\mathtt{test}}^{2}\) with \(n=MHS^{2}\), we have

\[4^{d}<\rho(\Psi_{\mathtt{test}}^{k-1};\pi^{k})^{2d}=\rho(\Psi_{\mathtt{test}}^ {k-1};\pi^{k})^{2d}.\]

Thus, \(\rho(\Psi_{\mathtt{test}}^{k-1};\pi^{k})>2\), which in turn implies Lemma D.1.

### Proof of Lemma d.2

By the construction of confidence set in equation (1) and Lemma A.5, for all \(k\in[K]\) and \(\theta\in\mathcal{C}_{k}\), we have

\[\sum_{(\mathcal{T},\pi)\in\mathcal{D}_{k}}\mathtt{TV}^{2}(\mathbb{P}_{\theta^{ *}}^{\pi},\mathbb{P}_{\theta}^{\pi})(\mathcal{T})\leq 3\beta,\]

where \(\beta=\log(K|\Theta|/\eta)\). Let \(\psi_{\xi}\) be a mixture of a subset of candidate policies \(\Psi_{\xi}=\{\psi_{1},\psi_{2},...,\psi_{n}\}\subseteq\Psi_{\mathtt{test}}^{k -1}\). With \(\mathbf{\psi}_{\xi}=(\psi_{\xi},\psi_{\xi},...,\psi_{\xi},\mathtt{Unif}(\mathcal{A }))\) and for every \(\mathbf{\tau}\in\mathtt{Subseq}(H,d),\mathbf{z}\in\{0,1\}^{|\mathbf{\tau}|}\), we have

\[\mathtt{TV}\left(\mathbb{P}_{\theta^{*}}^{\nu(\mathbf{\psi}_{\xi}; \mathbf{\tau},\mathbf{z})},\mathbb{P}_{\theta}^{\nu(\mathbf{\psi}_{\xi};\mathbf{\tau},\mathbf{z})} \right)(\mathcal{T})\] \[\leq\sqrt{\frac{1}{n^{d}}}\sqrt{\sum_{i_{1},i_{2},...,i_{d}\in[n ]}\mathtt{TV}^{2}\left(\mathbb{P}_{\theta^{*}}^{\nu((\psi_{i_{1}},...,\psi_{ i_{d}},\mathtt{Unif}(\mathcal{A}));\mathbf{\tau},\mathbf{z})},\mathbb{P}_{\theta}^{\nu( (\psi_{i_{1}},...,\psi_{i_{d}},\mathtt{Unif}(\mathcal{A}));\mathbf{\tau},\mathbf{z}) }\right)(\mathcal{T})}.\]

Therefore,

\[\sum_{\mathbf{\tau}\in\mathtt{Subseq}(H,d)}\sum_{I\subseteq[|\mathbf{t}| ]}\mathtt{TV}^{2}\left(\mathbb{P}_{\theta^{*}}^{\nu(\mathbf{\psi}_{\xi};\mathbf{\tau},\mathbf{z}(I;\mathbf{\tau}))},\mathbb{P}_{\theta}^{\nu(\mathbf{\psi}_{\xi};\mathbf{\tau},\bm {z}(I;\mathbf{\tau}))}\right)(s_{\mathbf{\tau}_{I}}^{\prime},x_{\mathbf{\tau}_{I}},y_{ \mathbf{\tau}_{I}})\] \[\leq\frac{1}{n^{d}}\sum_{\mathbf{\tau}\in\mathtt{Subseq}(H,d)}\sum_{ \mathbf{z}\in\{0,1\}^{|\mathbf{\tau}|}}\sum_{i_{1},i_{2},...,i_{d}\in[n]}\mathtt{TV}^{2 }\left(\mathbb{P}_{\theta^{*}}^{\nu((\psi_{i_{1}},...,\psi_{i_{d}},\mathtt{ Unif}(\mathcal{A}));\mathbf{\tau},\mathbf{z})},\mathbb{P}_{\theta}^{\nu((\psi_{i_{1}},..., \psi_{i_{d}},\mathtt{Unif}(\mathcal{A}));\mathbf{\tau},\mathbf{z}))}\right)(\mathcal{T})\] \[\leq\frac{3\beta}{n^{d}\cdot n_{\mathtt{test}}}.\]

where the last inequality is due to the construction of our dataset \(\mathcal{D}^{k}\) and the concentration guarantee for the confidence set \(\mathcal{C}^{k}\).

### Proof of Lemma a.1

First note that we can rewrite an _atomic_ probability \(P_{m}^{n}(y_{t}|x_{t})=P_{m}^{n}(r_{t},s_{t+1}|x_{t})\) as

\[P_{m}^{n}(r_{t},s_{t+1}|x_{t})=\frac{P_{m}^{n,\psi_{0}}(s_{t+1})}{P_{m}^{n,\psi_ {0}}(s_{t})}\left(\frac{P_{m}^{n,\psi_{0}}(s_{t})}{P_{m}^{n,\psi_{0}}(s_{t+1})} P_{m}^{n}(r_{t},s_{t+1}|x_{t})-l^{0}(x_{t},r_{t};s_{t+1})+l^{0}(x_{t},r_{t};s_{t+1}) \right),\]

In turn, moving from conditioning on the event \((x_{t},y_{t})\), we view the LMDP model after induction as if there are at most \(M_{1}-1\) contexts in the first LMDP model (or \(M_{2}-1\) in the second LMDP model if \(l^{0}(x_{t},r_{t};s_{t})\) is attained with \(n=2\)). We often use a shorthand \(l^{0}(x_{t},y_{t})=l^{0}(x_{t},r_{t};s_{t+1})\) to reduce the burden on the notation.

Proof.: The basic strategy is to apply iteratively the triangle inequality. To illustrate, we first expand the equation at \(t=1\):

\[\sum_{x_{1:H}}\sum_{y_{1:H}}\pi_{1:H}\left|\sum_{m=1}^{M_{1}}w_{m}^{ 1}\prod_{t=0}^{H}P_{m}^{1}(y_{t}|x_{t})-\sum_{m=1}^{M_{2}}w_{m}^{2}\prod_{t=0}^{ H}P_{m}^{2}(y_{t}|x_{t})\right|\] \[=\sum_{x_{1:y_{1}}}\sum_{\begin{subarray}{c}x_{2:H}\\ y_{2:H}\end{subarray}}\pi_{1:H}\Bigg{|}\sum_{m=1}^{M_{1}}w_{m}^{1}P_{m}^{1,\psi _{0}}(s_{2})\left(\frac{P_{m}^{1,\psi_{0}}(s_{1})}{P_{m}^{1,\psi_{0}}(s_{2})}P_ {m}^{1}(y_{1}|x_{1})-l^{0}(x_{1},y_{1})+l^{0}(x_{1},y_{1})\right)\prod_{t=2}^{ H}P_{m}^{1}(y_{t}|x_{t})\] \[\qquad-\sum_{m=1}^{M_{2}}w_{m}^{2}P_{m}^{2,\psi_{0}}(s_{2})\left( \frac{P_{m}^{2,\psi_{0}}(s_{1})}{P_{m}^{2,\psi_{0}}(s_{2})}P_{m}^{2}(y_{1}|x_{1 })-l^{0}(x_{1},y_{1})+l^{0}(x_{1},y_{1})\right)\prod_{t=2}^{H}P_{m}^{2}(y_{t}| x_{t})\Bigg{|}\] \[\leq\sum_{x_{1:y_{1}}}\sum_{\begin{subarray}{c}x_{2:H}\\ y_{2:H}\end{subarray}}\pi_{1:H}l^{0}(x_{1},y_{1})\Bigg{|}\sum_{m=1}^{M_{1}}w_{m} ^{1}P_{m}^{1,\psi_{0}}(s_{2})\prod_{t=2}^{H}P_{m}^{1}(y_{t}|x_{t})-\sum_{m=1}^{ M_{2}}w_{m}^{2}P_{m}^{2,\psi_{0}}(s_{2})\prod_{t=2}^{H}P_{m}^{2}(y_{t}|x_{t})\Bigg{|}\] \[\qquad+\sum_{x_{1:y_{1}}}\sum_{\begin{subarray}{c}x_{2:H}\\ y_{2:H}\end{subarray}}\pi_{1:H}\Bigg{|}\sum_{m=1}^{M_{1}}w_{m}^{1}\left(P_{m}^{1,\psi_{0}}(s_{1})P_{m}^{1}(y_{1}|x_{1})-l^{0}(x_{1},y_{1})P_{m}^{1,\psi_{0}}(s_ {2})\right)\prod_{t=2}^{H}P_{m}^{1}(y_{t}|x_{t})\] \[\qquad\qquad-\sum_{m=1}^{M_{2}}w_{m}^{2}\left(P_{m}^{2,\psi_{0}}(s _{1})P_{m}^{2}(y_{1}|x_{1})-l^{0}(x_{1},y_{1})P_{m}^{2,\psi_{0}}(s_{2})\right) \prod_{t=2}^{H}P_{m}^{2}(y_{t}|x_{t})\Bigg{|}. \tag{20}\]

We can continue applying triangle inequalities to all possible first event-logging time step, and we can start with the following inequality:

\[\sum_{x_{1:H}}\sum_{y_{1:H}}\pi_{1:H}\left|\sum_{m=1}^{M}w_{m}^{1 }\prod_{t=0}^{H}P_{m}^{1}(y_{t}|x_{t})-\sum_{m=1}^{M}w_{m}^{2}\prod_{t=0}^{H}P_ {m}^{2}(y_{t}|x_{t})\right|\] \[\leq\sum_{\tau_{1}\in[H]}\sum_{x_{1:\tau_{1}},y_{1:\tau_{1}}}\pi_ {1:\tau_{1}}l_{1:\tau_{1}-1}^{0}\times\] \[\sum_{\begin{subarray}{c}x_{\tau_{1}+1:H}\\ y_{\tau_{1}+1:H}\end{subarray}}\pi_{\tau_{1}+1:H}\left|\sum_{m=1}^{M}p_{m}^{1,1 }(x_{\tau_{1}},y_{\tau_{1}})\prod_{t=\tau_{1}+1}^{H}P_{m}^{1}(y_{t}|x_{t})-\sum_ {m=1}^{M}p_{m}^{2,1}(x_{\tau_{1}},y_{\tau_{1}})\prod_{t=\tau_{1}+1}^{H}P_{m}^{2 }(y_{t}|x_{t})\right|.\]

Recall that at least one \(p_{m}^{1,1}\) or \(p_{m}^{2,1}\) is 0, that is, one of the contexts in either of the two systems is eliminated.

Now for \((i)\), we can pick the next event-logging time step \(\tau_{2}>\tau_{1}\), and apply the triangle inequality similarly, and repeat such event-logging until all contexts are exhausted. Since there are at most \(2M\) contexts, we cannot repeat this process more than \(d=2M-1\) times. Hence, we arrive to the following inequality:

\[\sum_{x_{1:H}}\sum_{y_{1:H}}\pi_{1:H}\left|\sum_{m=1}^{M}w_{m}^{1 }\prod_{t=0}^{H}P_{m}^{1}(y_{t}|x_{t})-\sum_{m=1}^{M}w_{m}^{2}\prod_{t=0}^{H}P _{m}^{2}(y_{t}|x_{t})\right|\] \[\leq\sum_{\tau\in\mathsf{Subseq}(H,d)}\sum_{x_{\tau},y_{\tau}} \Delta(x_{\tau},y_{\tau})\times\sum_{\begin{subarray}{c}x_{0:\tau_{1}-1}\\ y_{0:\tau_{1}-1}\end{subarray}}...\sum_{\begin{subarray}{c}x_{\tau_{1}+1:H}\\ y_{\tau_{1}+1:H}\end{subarray}}\prod_{i=0}^{|\tau|}\left(\pi_{\tau_{i}+1:\tau _{i+1}}\cdot l_{\tau_{i}+1:\tau_{i+1}-1}^{i}\right),\]

where we set \(\tau_{|\tau|+1}\equiv H+1\). To proceed from here, we first observe that for any \(m\in[M]\) and \(i\geq 0\) such that \(p_{m}^{1,i}>0\),

\[\pi_{\tau_{i}+1:\tau_{i+1}}\cdot l_{\tau_{i}+1:\tau_{i+1}-1}^{i} \leq\pi_{\tau_{i}+1:\tau_{i+1}}\frac{\prod_{t=\tau_{i}+1}^{\tau_{i +1}-1}P_{m}(y_{t}|x_{t})}{P_{m}^{\nu(\psi_{i};\tau_{i})}(s_{\tau_{i+1}}|s_{\tau_ {i}+1})}\] \[\leq\pi(a_{\tau_{i+1}}|h_{\tau_{i+1}})\cdot\frac{\prod_{t=\tau_{i}+ 1}^{\tau_{i}-1}\pi(a_{t}|h_{t})P_{m}(y_{t}|x_{t})}{P_{m}^{\mu(\psi_{i};\tau_{i}) }(s_{\tau_{i+1}}|s_{\tau_{i}+1})}\] \[=\frac{P_{m}^{\pi}(x_{\tau_{i}+1:\tau_{i+1}},y_{\tau_{i}+1:\tau_{i+ 1}-1}|h_{\tau_{i}+1})}{P_{m}^{\nu(\psi_{i};\tau_{i})}(s_{\tau_{i+1}}|s_{\tau_ {i}+1})}. \tag{21}\]Thus, we can summarize that

\[\sum_{x_{1:H}}\sum_{y_{1:H}}\pi_{1:H}\left|\sum_{m=1}^{M}w_{m}^{1}\prod _{t=0}^{H}P_{m}^{1}(y_{t}|x_{t})-\sum_{m=1}^{M_{2}}w_{m}^{2}\prod_{t=0}^{H}P_{m} ^{2}(y_{t}|x_{t})\right|\] \[\leq\sum_{\boldsymbol{\tau}\in\mathbf{Subseq}(H,d)}\sum_{x_{ \boldsymbol{\tau}},y_{\boldsymbol{\tau}}}\Delta(x_{\boldsymbol{\tau}},y_{ \boldsymbol{\tau}})\times\sum_{\begin{subarray}{c}x_{\boldsymbol{\tau}},x_{ \boldsymbol{\tau}_{1}-1}\\ y_{0:\tau_{1}-1}\end{subarray}}...\sum_{\begin{subarray}{c}x_{\boldsymbol{\tau} _{1}\boldsymbol{\tau}+1:H}\\ y_{1\boldsymbol{\tau}+1:H}\end{subarray}}\prod_{t=0}^{|\boldsymbol{\tau}|}\left( \frac{P_{m(x_{\boldsymbol{\tau}},y_{\boldsymbol{\tau}})}^{\boldsymbol{\tau}}(x_ {\boldsymbol{\tau}_{i}+1:\tau_{i+1}},y_{\boldsymbol{\tau}+1:\tau_{i+1}-1}|h_{ \boldsymbol{\tau}_{i}+1})}{P_{m(x_{\boldsymbol{\tau}},y_{\boldsymbol{\tau}})}^{ \boldsymbol{\tau}(\psi_{i};\tau_{i})}(s_{\tau_{i+1}}|s_{\tau_{i}+1},\phi)} \right).\]

We note that each term in the product sequence is equivalent to

\[\frac{P_{m(x_{\boldsymbol{\tau}},y_{\boldsymbol{\tau}})}^{\boldsymbol{\tau}}(h _{\boldsymbol{\tau}_{i+1}+1}|h_{\boldsymbol{\tau}_{i}+1})}{P_{m(x_{\boldsymbol {\tau}},y_{\boldsymbol{\tau}})}^{\boldsymbol{\nu}(\psi_{i};\tau_{i})}(s_{\tau_ {i+1}}|s_{\tau_{i}+1})P_{m(x_{\boldsymbol{\tau}},y_{\boldsymbol{\tau}})}(y_{ \boldsymbol{\tau}_{i+1}}|x_{\boldsymbol{\tau}_{i+1}})},\]

and thus

\[\prod_{i=0}^{|\boldsymbol{\tau}|}\left(\frac{P_{m(x_{\boldsymbol{ \tau}},y_{\boldsymbol{\tau}})}^{\boldsymbol{\tau}}(x_{\boldsymbol{\tau}_{i}+1: \tau_{i+1}},y_{\boldsymbol{\tau}_{i+1}:\tau_{i+1}-1}|h_{\boldsymbol{\tau}_{i}+ 1})}{P_{m(x_{\boldsymbol{\tau}},y_{\boldsymbol{\tau}})}^{\boldsymbol{\nu}(\psi _{i};\tau_{i})}(s_{\tau_{i+1}}|s_{\tau_{i}+1})}\right)\] \[=\left(\frac{P_{m(x_{\boldsymbol{\tau}},y_{\boldsymbol{\tau}})}^{ \boldsymbol{\tau}}(x_{1:H},y_{1:H})}{\prod_{i=0}^{q}P_{m(x_{\boldsymbol{\tau}},y_{\boldsymbol{\tau}})}^{\boldsymbol{\nu}(\psi_{i};\tau_{i})}(s_{\tau_{i+1}}|s _{\tau_{i}+1})P_{m(x_{\boldsymbol{\tau}},y_{\boldsymbol{\tau}})}(y_{\boldsymbol {\tau}_{i+1}}|x_{\boldsymbol{\tau}_{i+1}})}\right).\]

Here we set \(\tau_{q+1}=H+1\) and \(P_{m}^{\boldsymbol{\tau}}(s_{H+1}|\cdot)=1\) for any \(m\), \(\pi\) and conditional event. Since the denominator does not depend on events outside of event-logging time-steps \(\boldsymbol{\tau}\), we can marginalize the probabilities in the inner summation and conclude the lemma. 

### Proof of Lemma a.3

Let us slightly extend the lemma such that we consider different sets of behavioral policies for different checkpoint time-steps.

Proof.: Note that for all \(m,s^{\prime},s,t_{1},t_{2}\), by the construction of \(\psi_{\xi}\), it follows that

\[P_{m}^{\boldsymbol{\nu}(\psi_{\xi};\tau_{1})}(s_{t_{2}}=s|s^{\prime}_{t_{1}}=s^ {\prime})\geq\max_{\psi\in\Psi_{\xi}}\frac{P_{m}^{\boldsymbol{\nu}(\psi;t_{1}) }(s_{t_{2}}=s|s^{\prime}_{t_{1}}=s^{\prime})}{n}.\]

We can observe that for any \(m\),

\[P_{m}^{\boldsymbol{\nu}(\psi_{\xi};\boldsymbol{\tau},z(I;\boldsymbol{\tau}))}(s ^{\prime}_{\boldsymbol{\tau}_{I}},x_{\boldsymbol{\tau}_{I}},y_{\boldsymbol{ \tau}_{I}})=\prod_{i\in I}P_{m}^{\boldsymbol{\nu}(\psi_{\xi};\tau_{i-1})}(s_{ \tau_{i}+1}|s^{\prime}_{\tau_{i-1}})\cdot\prod_{i\in[q]/I}\frac{1}{A}\cdot P_{ m}^{\boldsymbol{\nu}(\psi_{\xi};\tau_{i-1})}(s_{\tau_{i}}|s^{\prime}_{\tau_{i-1}})P_{m}(y_{ \tau_{i}}|x_{\tau_{i}}).\]

On the other hand, for any \(\pi\in\Pi\), \(\boldsymbol{\tau}\subseteq[H]\), \(I\in[|\boldsymbol{\tau}|]\), \(x_{\boldsymbol{\tau}}\), \(y_{\boldsymbol{\tau}}\),

\[P_{m}^{\boldsymbol{\tau}}(s^{\prime}_{\boldsymbol{\tau}_{I}},x_{\boldsymbol{ \tau}_{I}/1},y_{\boldsymbol{\tau}_{I}})\leq\prod_{i\in I}\max_{\boldsymbol{ \tau}_{I-1}}P_{m}^{\boldsymbol{\tau}}(s_{\tau_{i}+1}|s^{\prime}_{\tau_{i-1}}, \mathcal{T}_{1:\tau_{i-1}})\cdot\prod_{i\in[q]/I}\max_{\tau_{i:\tau_{i-1}}}P_{ m}^{\boldsymbol{\tau}}(x_{\tau_{i}}|s^{\prime}_{\tau_{i-1}},\mathcal{T}_{1:\tau_{i-1}})P_{m}(y_{ \tau_{i}}|x_{\tau_{i}}).\]

Applying the inequality with the definition of \(\rho(\Psi_{\xi};\pi)\), we have

\[C(\psi_{\xi};\pi) =\max_{\boldsymbol{\tau}\subseteq[H],|\boldsymbol{\tau}|\leq d} \max_{I\subseteq[|\boldsymbol{\tau}|]}\max_{\begin{subarray}{c} \boldsymbol{\tau}^{\prime}\in\mathcal{S}^{\emptyset}\mid I\\ (\boldsymbol{x},\boldsymbol{y})\in(\mathcal{X},\mathcal{Y})\emptyset\mid \boldsymbol{\tau}\mid-|I\end{subarray}}\max_{m\in[M]}\frac{P_{m}^{\boldsymbol{ \tau}}(s^{\prime}_{\boldsymbol{\tau}_{I}}=\boldsymbol{s}^{\prime},x_{ \boldsymbol{\tau}_{I}/I}=\boldsymbol{x},y_{\boldsymbol{\tau}_{I}/I}=\boldsymbol {y})}{P_{m}^{\boldsymbol{\nu}(\psi_{\xi};\tau_{i}=I;\boldsymbol{\tau})}(s^{ \prime}_{\boldsymbol{\tau}_{I}}=\boldsymbol{s}^{\prime},x_{\boldsymbol{\tau}_{I} /I}=\boldsymbol{x},y_{\boldsymbol{\tau}_{I}/I}=\boldsymbol{y})}\] \[\leq\max_{\boldsymbol{\tau}\subseteq[H],|\boldsymbol{\tau}|\leq d }\max_{I\subseteq[|\boldsymbol{\tau}|]}\max_{\begin{subarray}{c} \boldsymbol{s}^{\prime}\in\mathcal{S}^{\emptyset}\mid I\\ (\boldsymbol{x},\boldsymbol{y})\in(\mathcal{X},\mathcal{Y})\emptyset\mid \boldsymbol{\tau}\mid-|I\end{subarray}}\max_{m\in[M]}\prod_{i\in I}\frac{\max_{ \boldsymbol{\tau}_{i:\tau_{i-1}}}P_{m}^{\boldsymbol{\tau}}(s_{\tau_{i}+1}|s^{ \prime}_{\tau_{i-1}},\mathcal{T}_{1:\tau_{i-1}})}{P_{m}^{\boldsymbol{\nu}( \psi_{\xi};\tau_{i-1})}(s_{\tau_{i}+1}|s^{\prime}_{\tau_{i-1}})}\] \[\qquad\times\prod_{i\in[q]/I}A\cdot\frac{\max_{\boldsymbol{\tau}_{ 1:\tau_{i-1}}}P_{m}^{\boldsymbol{\nu}}(s_{\tau_{i}}|s^{\prime}_{\tau_{i-1}}, \mathcal{T}_{1:\tau_{i-1}})}{P_{m}^{\boldsymbol{\nu}(\psi_{\xi};\tau_{i-1})} (s_{\tau_{i}}|s^{\prime}_{\tau_{i-1}})}\] \[\leq(nA\cdot\rho(\Psi_{\xi};\pi))^{d},\]

concluding Lemma a.3.

### Proof of Lemma a.4

This is by now a standard MLE technique for constructing confidence sets in RL [1].

_Proof._ The proof follows a Chernoff bound type of technique:

\[\mathds{P}_{\theta^{*}}\left(\sum_{(\tau,\pi)\in\mathcal{D}^{k}} \log\left(\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{\theta^{*}}^{\pi}( \tau)}\right)\geq\mathbb{E}_{\theta^{*}}\left[\sum_{(\tau,\pi)\in\mathcal{D}^{ k}}\log\left(\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{\theta^{*}}^{\pi}( \tau)}\right)\right]+\beta\right)\] \[\leq\mathds{P}_{\theta^{*}}\left(\exp\left(\sum_{(\tau,\pi)\in \mathcal{D}^{k}}\log\left(\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{ \theta^{*}}^{\pi}(\tau)}\right)\right)\geq\exp\left(\beta\right)\right)\] \[\leq\mathbb{E}_{\theta^{*}}\left[\exp\left(\sum_{(\tau,\pi)\in \mathcal{D}^{k}}\log\left(\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{ \theta^{*}}^{\pi}(\tau)}\right)\right)\right]\exp(-\beta).\]

The last inequality is by Markov's inequality. Note that random variables are \((\tau,\pi)\) in the trajectory dataset \(\mathcal{D}\), and

\[\mathbb{E}_{\theta^{*}}\left[\sum_{(\tau,\pi)\in\mathcal{D}^{k}} \log\left(\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{\theta^{*}}^{\pi}( \tau)}\right)\right]=-\text{KL}(\mathds{P}_{\theta^{*}}(\mathcal{D}^{k})|| \mathds{P}_{\theta}(\mathcal{D}^{k}))\leq 0.\]

Therefore,

\[\mathbb{E}_{\theta^{*}}\left[\exp\left(\sum_{(\tau,\pi)\in\mathcal{ D}^{k}}\log\left(\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{\theta^{*}}^{ \pi}(\tau)}\right)\right)\right]=\mathbb{E}_{\theta^{*}}\left[\Pi_{(\tau,\pi) \in\mathcal{D}^{k}}\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{\theta^ {*}}^{\pi}(\tau)}\right]=\sum_{\mathcal{D}^{k}}\mathds{P}_{\theta}(\mathcal{D} ^{k})=1.\]

Combining the above, taking a union bound over \(k\in[K]\) rounds and \(\theta\in\Theta\), letting \(\beta=\log(K|\Theta|/\eta)\), with probability \(1-\eta\), the inequality in Lemma A.4 holds. \(\square\)

### Proof of Lemma a.5

_Proof._ By the TV-distance and Hellinger distance relation, for any \(\iota,\tau\), \(\pi\) and \(t\in[H]\),

\[\mathds{T}\mathds{V}^{2}\left(\mathds{P}_{\theta}^{\pi}(\tau), \mathds{P}_{\theta^{*}}^{\pi}(\tau)\right) \leq 2\mathbb{H}^{2}\left(\mathds{P}_{\theta}^{\pi}(\tau), \mathds{P}_{\theta^{*}}^{\pi}(\tau)\right)\] \[= 2\left(1-\mathbb{E}_{\tau\sim\mathds{P}_{\theta^{*}}^{*}}\left[ \sqrt{\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{\theta^{*}}^{\pi}( \tau)}}\right]\right)\leq-2\log\left(\mathbb{E}_{\tau\sim\mathds{P}_{\theta^ {*}}^{*}}\left[\sqrt{\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{\theta ^{*}}^{\pi}(\tau)}}\right]\right).\]

To bound the summation over samples, we start from

\[\sum_{(\tau,\pi)\in\mathcal{D}^{k}}\mathbb{H}^{2}\left(\mathds{P }_{\theta}^{\pi}(\tau),\mathds{P}_{\theta^{*}}^{\pi}(\tau)\right)\leq-\sum_{( \tau,\pi)\in\mathcal{D}^{k}}\log\left(\mathbb{E}_{\tau\sim\mathds{P}_{\theta^ {*}}^{*}}\left[\sqrt{\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{\theta ^{*}}^{\pi}(\tau)}}\right]\right).\]

On the other hand, by the Chernoff bound,

\[\mathds{P}_{\theta^{*}}\left(\sum_{(\tau,\pi)\in\mathcal{D}^{k}} \log\left(\sqrt{\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{\theta^{*}} ^{\pi}(\tau)}}\right)\geq\sum_{(\tau,\pi)\in\mathcal{D}^{k}}\log\mathbb{E}_{ \tau\sim\mathds{P}_{\theta^{*}}^{\pi}}\left[\sqrt{\frac{\mathds{P}_{\theta}^{ \pi}(\tau)}{\mathds{P}_{\theta^{*}}^{\pi}(\tau)}}\right]+\beta\right)\] \[\leq\mathbb{E}_{\theta^{*}}\left[\frac{\exp\left(\sum_{(\tau,\pi )\in\mathcal{D}^{k}}\log\left(\sqrt{\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{ \mathds{P}_{\theta^{*}}^{\pi}(\tau)}}\right)\right)}{\exp\left(\sum_{(\tau, \pi)\in\mathcal{D}^{k}}\log\mathbb{E}_{\tau\sim\mathds{P}_{\theta^{*}}^{\pi} }\left[\sqrt{\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{\theta^{*}}^{ \pi}(\tau)}}\right]\right)}\right]\exp(-\beta)\] \[=\mathbb{E}_{\theta^{*}}\left[\frac{\Pi_{(\tau,\pi)\in\mathcal{D} ^{k}}\sqrt{\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{\mathds{P}_{\theta^{*}}^{ \pi}(\tau)}}}{\Pi_{(\tau,\pi)\in\mathcal{D}^{k}}\mathbb{E}_{\tau\sim\mathds{P} _{\theta^{*}}^{\pi}}\left[\sqrt{\frac{\mathds{P}_{\theta}^{\pi}(\tau)}{ \mathds{P}_{\theta^{*}}^{\pi}(\tau)}}\right]}\right]\exp(-\beta)\]\[=\mathbb{E}_{\theta^{*}}\left[\mathbb{E}_{\theta^{*}}\left[\frac{\Pi_{( \tau,\pi)\in\mathcal{D}^{k-1}}\sqrt{\frac{\mathbb{P}_{\theta^{*}}^{\pi}(\tau)}{ \mathbb{P}_{\theta^{*}}^{\pi}(\tau)}}}{\Pi_{(\tau,\pi)\in\mathcal{D}^{k-1}} \mathbb{E}_{\tau\sim\mathbb{P}_{\theta^{*}}^{\pi}}\left[\sqrt{\frac{\mathbb{P}_ {\theta^{*}}^{\pi}(\tau)}{\mathbb{P}_{\theta^{*}}^{\pi}(\tau)}}\right]}\left| \mathcal{D}^{k-1}\right|\right]\right]\exp(-\beta)\] \[=\mathbb{E}_{\theta^{*}}\left[\left.\frac{\Pi_{(\tau,\pi)\in \mathcal{D}^{k-1}}\sqrt{\frac{\mathbb{P}_{\theta^{*}}^{\pi}(\tau)}{\mathbb{P} _{\theta^{*}}^{\pi}(\tau)}}}{\Pi_{(\tau,\pi)\in\mathcal{D}^{k-1}}\mathbb{E}_{ \tau\sim\mathbb{P}_{\theta^{*}}^{\pi}}\left[\sqrt{\frac{\mathbb{P}_{\theta^{*}} ^{\pi}(\tau,\tau)}{\mathbb{P}_{\theta^{*}}^{\pi}(\tau,\tau)}}\right]}\right] \exp(-\beta)=...=\exp(-\beta),\]

where in the last line, we used the tower property of expectation. Thus, again by setting \(\beta=\log(K|\Theta|/\eta)\), with probability at least \(1-\eta\), we have

\[\sum_{(\tau,\pi)\in\mathcal{D}^{k}} \mathbb{H}^{2}(\mathbb{P}_{\theta}^{\pi}(\tau),\mathbb{P}_{\theta ^{*}}^{\pi}(\tau))\leq-\frac{1}{2}\sum_{(\tau,\pi)\in\mathcal{D}^{k}}\log \left(\frac{\mathbb{P}_{\theta}^{\pi}(\tau)}{\mathbb{P}_{\theta^{*}}^{\pi}( \tau)}\right)+\beta\] \[=-\frac{1}{2}\sum_{(\tau,\pi)\in\mathcal{D}^{k}}\log\left(\frac{ \mathbb{P}_{\theta}^{\pi}(\tau)}{\mathbb{P}_{\theta^{*}}^{\pi}(\tau)}\right)+ \frac{1}{2}\sum_{(\tau,\pi)\in\mathcal{D}^{k}}\log\left(\frac{\mathbb{P}_{ \theta}^{\pi}(\tau)}{\mathbb{P}_{\theta^{*}}^{\pi}(\tau)}\right)+\beta,\]

for all \(k\in[K]\) and \(\theta\in\Theta\). Now we can apply Lemma A.4, and finally have

\[\sum_{(\tau,\pi)\in\mathcal{D}^{k}}\mathbb{H}^{2}(\mathbb{P}_{\theta}^{\pi}( \tau),\mathbb{P}_{\theta^{*}}^{\pi}(\tau))\leq-\frac{1}{2}\sum_{(\tau,\pi)\in \mathcal{D}^{k}}\log\left(\frac{\mathbb{P}_{\theta}^{\pi}(\tau)}{\mathbb{P}_{ \theta^{*}}^{\pi}(\tau)}\right)+\frac{3}{2}\beta.\]

Since \(\mathbb{T}\mathbb{V}^{2}\leq 2\mathbb{H}^{2}\), we get the lemma.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract present the problem we consider in this work - designing an efficient learning algorithm for the RL setting - and elaborate, in a highlevel, on our new approach for solving this open problem.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In the conclusion part we highlight the limitations of our work the main ones are: (i) our algorithm is optimal only up to polynomial factors and closing this gap is left as future work, (ii) designing computationally and sample efficient algorithm, under some oracle assumptions, is an open problem which is left for future work.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The complete proof is given in the Appendix. Further, we made substantial effort to provide intuition for the proof in the main paper: by providing analysis for the simpler MDP problem (and complementary analysis in the Appendix), as well as by connecting the analysis of this simpler setting to the analysis of the LMDP setting.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: There are no potential harms caused by the research process or required mitigation measures that should have been taken for this work. Further, this work focuses on a mathematical framework with no immediate societal impact or potential harmful consequences, to our opinion.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work focuses on establishing the learnability of the LMDP setting, was not established by previous works. The LMDP setting has been investigated in the past, as we mentioned in the introduction section, by both empirical and theoretical communities. We do not see immediate societal impacts of our work between the new results we derived, and the promise for improving algorithms for the LMDP, and POMDP settings in future works.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer:[NA] Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: There are no experimental results that make use of data or models in this paper, but a resolution of an algorithmic open problem.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: There are no experimental results in this paper, but a resolution of an algorithmic open problem.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects.