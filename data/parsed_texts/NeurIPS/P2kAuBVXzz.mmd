# It's All Relative: Relative Uncertainty in Latent Spaces using Relative Representations

Fabian Martin Mager

Technical University of Denmark

fmager@dtu.dk

&Valentino Maiorca

Sapienza University of Rome, Italy

maiorca@di.uniroma1.it

&Lars Kai Hansen

Technical University of Denmark

lkai@dtu.dk

###### Abstract

Many explainable artificial intelligence (XAI) methods investigate the embedding space of a given neural network. Uncertainty quantification in these spaces can lead to a better understanding of the mechanisms learned by the given network. When concerned with the uncertainty of functions in latent spaces we can invoke ensembles of trained models. Such ensembles can be confounded by reparameterization, i.e., lack of identifiability. We consider two mechanisms for reducing reparametrization "noise", one based on relative representations and one based on interpolation in weight space. By sampling embedding spaces along a curve connecting two fully converged networks without an increase in loss, we show that the latent uncertainty becomes overestimated when comparing embedding spaces without considering the reparametrization issue. By changing the absolute embedding space to a space of relative proximity, we show that the spaces become aligned, and the measured uncertainty decreases. Using this method, we show that the most non-trivial changes to the latent space occur around the midpoint of the curve connecting the independently trained networks.

## 1 Introduction

A neural network (NN) trained to perform a certain downstream task, e.g., regression or classification, learns a mapping \(f:\mathbf{x}_{i}\rightarrow\mathbf{y}_{i}\) given a set of observations \(\mathcal{D}=\{(\mathbf{x}_{i},\mathbf{y}_{i})\}_{i}^{N}\), where \(\mathbf{x}_{i}=[x_{1},x_{2},\...,x_{D}]\) and \(N\) is the number of available observations and \(D\) is the latent space dimension. The learned function \(f\) is hierarchical, i.e. \(f(\mathbf{x})=h_{L}(h_{L-1}(...h_{1}(\mathbf{x})))\) where the intermediate representation of the observation \(\mathbf{x}_{i}\) after layer \(l\) is denoted as \(\mathbf{z}_{i}\). We note the mapping of the observation \(\mathbf{x}_{i}\) to its latent representation \(\mathbf{z}_{i}\) as \(h_{l}:\mathbf{x}_{i}\rightarrow\mathbf{z}_{i}\). The latent space is assumed to be semantically meaningful for relevant downstream tasks. Therefore, many XAI methods investigate the properties of the latent space of a NN, for example, its geometry [9] or the semantic structure [5]. XAI has many dimensions [11], one of which is referred to as _local_ vs. _global_. Local explainability concerns predictions of individual samples, whereas global explainability considers the network as a whole. Most local methods give attributions to the input features \([x_{1},x_{2},...,x_{D}]\), which are important to the output \(\hat{\mathbf{y}}\). Recently, Wickstrom et al. [10] proposed a method that maps attributions to input features based on the importance of its latent vector \(\mathbf{z}\). In contrast, global explainability methods suchas _Concept Activation Vectors_ (TCAV) [5] aim to explain the entire latent space based on semantics. One persistent challenge in assessing geometry and semantic structure of the latent space is the _reparametrization issue_, driven by the fact that there is no "set of optimal parameters and that we can always parametrize the manifold in a different, but equally good, way" ([2]). In other words, while the end-to-end function \(f\) remains the same, different parametrizations lead to different but equally performing embedding functions \(h\).

Moschella et al. [7] introduced Relative Representations, which define an alternative representation of the latent space. They empirically show that the latent spaces of two identical models trained on the same data but different initializations are identical up to angle-preserving transformations. Their proposed method uses a set of latent vectors \(\mathcal{A}=\{\mathbf{z}_{i}\}_{i}^{A}\), called _anchors_, and redefines the position of each latent vector to be _relative_ to the set of anchors, according to a similarity function \(sim:\mathbb{R}^{D}\times\mathbb{R}^{D}\rightarrow\mathbb{R}\). Depending on the choice of the similarity function, the latent space becomes invariant to certain transformations.

The reparametrization issue is also related to the loss landscape in NNs. Several works have exploited the reparametrization issue to efficiently train ensembles of models, which outperform their single-model counterparts [4]. Garipov et al. [1] investigated the geometrical properties of loss landscapes and showed that two identical NNs trained on different seeds can be connected by a simple curve in weight space, such that the loss under the curve is constant. This curve \(\phi\) parameterized by \(\theta\) connects two points \(\hat{w}_{1}\) and \(\hat{w_{2}}\) in parameter space and is found by minimizing the loss \(l(\theta)=\int_{0}^{1}\mathcal{L}(\phi(t)),dt=\mathbb{E}_{t\sim U(0,1)}[ \mathcal{L}(\phi_{\theta}(t))]]\), where \(\mathcal{L}\) is the loss function used to find \(\hat{w}_{1}\) and \(\hat{w_{2}}\). We refer to section B for details on the curve-finding procedure. Once \(\theta\) is fitted, one can sample from \(\phi_{\theta}(t))\) for \(0\leq t\leq 1\) and build an ensemble of models from living on the curve \(\phi_{\theta}(t)\).

Ensembling methods reduce both the bias and variance of \(\hat{\mathbf{y}}\). The uncertainty over \(K\) samples of the end-to-end function \(f\) in the prediction of \(\hat{y}_{i}\) is given by

\[\sigma^{2}(\hat{\mathbf{y}}_{i})=\text{trace}\left(\mathbb{E}_{k\in K}[\left( f_{k}\left(\mathbf{x}_{i}\right)-\bar{\mathbf{y}}_{i}\right)(f_{k}\left( \mathbf{x}_{i}\right)-\bar{\mathbf{y}}_{i})^{T}]\right)\] (1)

where \(\bar{\mathbf{y}}_{i}=\mathbb{E}_{k\in K}[f_{k}(\mathbf{x}_{i})]\). Following the above reasoning, the embedding function's uncertainty \(h\) could be obtained by replacing \(f_{k}\) with \(h_{k}\) and \(\mathbf{y}_{i}\) with \(\mathbf{z}_{i}\). Due to reparametrization however, the latent uncertainty \(\sigma^{2}(\mathbf{z}_{i})\) will be overestimated (see Appx. A). We argue that there are two factors contributing to the overall observed uncertainty \(\sigma^{2}_{\mathcal{O}}(\mathbf{z}_{i})\),

\[\sigma^{2}_{\mathcal{O}}(\mathbf{z}_{i})=\sigma^{2}_{\mathcal{R}}(\mathbf{z}_ {i})+\sigma^{2}_{\mathcal{M}}(\mathbf{z}_{i})\,\] (2)

where \(\sigma^{2}_{\mathcal{R}}\) refers to the uncertainty caused by reparametrization and \(\sigma^{2}_{\mathcal{M}}\) to the uncertainty of the model. In equation 2 we assume independence of the model and reparametrization uncertainty. Such independence could arise as a consequence of initialization being independent of the data. In order to isolate the model uncertainty \(\sigma^{2}_{\mathcal{M}}(\mathbf{z}_{i})\), the reparametrization uncertainty \(\sigma^{2}_{\mathcal{R}}(\mathbf{z}_{i})\to 0\). In Appx. A we show that, for rotation and scaling transformations, the Relative Representation framework, using a cosine similarity function, can eliminate \(\sigma^{2}_{\mathcal{R}}(\mathbf{z}_{i})\). In this work, we

* Define an alignment score \(\rho\), which estimates the signal-to-noise ratio for a given ensemble of latent observations. The alignment score \(\rho\) is a measure of separability between \(N\) latent points given \(K\) samples each, similar to Fisher's Linear Discriminant objective for \(N\) classes.
* Show empirically that when sampling models along the curve \(\phi\), transforming the embeddings into a space of relative proximity increases alignment between latent observations.
* Show that the latent observations for the curve \(\phi(t)\) have high alignment around the endpoints and little alignment at the bending point, suggesting limited information gain when sampling around the endpoints and high information gain when sampling across the midpoint.

## 2 Methods

The intuition behind our alignment metric is the following: Given \(K\) samples of a latent vector \(\mathbf{z}_{i}\), where each sample is an embedding of a given observation \(\mathbf{x}_{i}\) obtained from a unique function \(h_{k}\), the alignment score \(\rho_{i}\) should be high if all samples form a compact region in the latent space and are well separated from all other points (see Figure 1). The alignment score \(\rho\) of the entire embedding space can be estimated as the average of individual scores. This formulation is similar to Fisher's Linear Discriminant (FLD) objective, which measures the separability of two or more classes of objects. The FLD objective is given by the ratio of between-class vs. within-class covariance. Given \(K\) realizations of latent vector \(\mathbf{z}_{i}\), the objective can be applied on the scale of a single observation, i.e. the ratio of between-observation vs. within-observation covariance. Due to the high dimensionality of \(\mathbf{z}\) and the few number of available samples, it is not feasible to calculate the full covariance matrix. We therefore use the variance estimator and express the ratio as a signal-to-noise ratio, such that the alignment metric \(\rho\) is bounded \(0\leq\rho\leq 1\). We define the within-observation variance \(\sigma_{W}^{2}\) and between-observation variance \(\sigma_{B}^{2}\) of \(k\) realizations of the latent space and their alignment \(\rho\) as

\[\sigma_{W}^{2} =\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{K}(\mathbf{z}_{ik}-\bar{ \mathbf{z}}_{i})^{2}\] (3) \[\sigma_{B}^{2} =\frac{1}{N}\sum_{i=1}^{N}(\bar{\mathbf{z}}_{i}-\mathbb{E}(\bar{ \mathbf{z}}_{i}))^{2}\] (4) \[\rho =\frac{\sigma_{B}^{2}}{\sigma_{B}^{2}+\sigma_{W}^{2}}\] (5)

We conduct the following experiment independently using a VGG-16 [8] and a Preactivation-ResNet [3] on CIFAR-100 [6]. First, fully train on three different seeds until convergence, resulting in three unique models \(\hat{w}_{1}\), \(\hat{w}_{2}\), and \(\hat{w}_{3}\). For each pairwise combination of seeds, we fit a Bezier curve with fixed endpoints and a single bend following the procedure described in [1]. Once a curve \(\phi_{ij}(t)\) between two modes \(\hat{w}_{i}\), \(\hat{w}_{j}\) is found, we sample \(\hat{K}\) models at steps \(t=k*\Delta t\), where \(k\in[0,\...,\ K-1]\) and \(\Delta t=\frac{1}{K}\). For all experiments, we set \(K=21\). We proceed by evaluating the performance at each step \(t\), as well as the ensemble performance over all models up to step \(t\) using a hold-out test set \(\mathcal{D}_{test}=\left(x_{i},y_{i}\right)_{i}^{N}\). We investigate the alignment of the embedding spaces along the curve, i.e. the last layer before the classification layer. We define the absolute embedding space \(\mathcal{Z}_{k}\) as the ensemble of all latent vectors \(\{\mathbf{z}_{i}\}\) in \(\mathcal{D}_{test}\) and the space of relative proximity \(\mathcal{Q}_{k}\) as the ensemble of all transformed vectors \(\{\mathbf{q}_{i}\}\) given a set of anchors \(\mathcal{A}\) and a similarity function \(sim:\mathbb{R}^{D}\times\mathbb{R}^{D}\rightarrow\mathbb{R}\). The number of anchors is chosen to match the number of dimensions \(D\) of the absolute embeddings. For the VGG16, \(D=512\), and for the ResNet110, \(D=256\). Anchors are sampled randomly from \(\mathcal{D}_{test}\), following the procedure of [7]. Table 1 provides an overview of the proposed similarity functions. For the cosine and basis transformations, we center the embedding space \(\mathcal{Z}_{k}\) before calculating similarities using an estimate of the mean based on \(\mathcal{D}_{train}\). For the Euclidean transformation, the space is additionally scaled to unit variance. Finally, using the proposed metric, we measure the cumulative alignment for increasing \(k\) of both the absolute and relative

Figure 1: _Left_: Conceptual visualization of the alignment measure. Given the two observations \(\mathbf{x}_{i=1}\) and \(\mathbf{x}_{i=2}\) and \(K\) realizations of their embeddings \(\mathbf{z}_{i=1}\) and \(\mathbf{z}_{i=2}\) obtained from a unique embedding function \(h_{k}\), we seek small within-observation variance \(\mathbf{S}_{W}\) relative to the between-observation variance \(\mathbf{S}_{B}\). _Right_: Visualization of uncertainties in latent space using the ratio of within vs. between sample variance. The plots show two identical t-SNE projections of the VGG-16 model and 10 classes of CIFAR100. The dot size represents uncertainty measured in absolute space (left) and relative space (right) using a cosine similarity projection.

embedding spaces. We compare the observed alignment with the alignment of eleven independently trained networks. Code is available here1

Footnote 1: https://github.com/fmager/fit-s-all-relative

## 3 Results

In Figure 2 we show the individual and cumulative ensemble error rates for each model type and curve. For all models and curves, the test accuracy increases when approaching the midpoint. The cumulative ensemble error decreases with increasing ensemble size. Notably, there is little variation across the pairwise curves. The embeddings of relative representations show higher alignment than the embeddings of absolute representations. This shows that the embedding spaces are confounded by reparametrization. However, the results vary across projection methods. For the VGG, the cosine similarity performs best, whereas for the ResNet it is Euclidean distance. Alignment measured in absolute space decreases almost linearly. The alignment measured in relative space, however, seems to converge. Compared to the baseline experiment, where alignment is measured across eleven independently trained networks, the measured alignments in relative space are closer than those measured in absolute space. Furthermore, the figure shows that the highest negative slope occurs around \(t=0.5\), whereas the slope has limited variation around the endpoints. This finding suggests that ensembles used for uncertainty quantification should be samples far from the endpoints of a connecting curve.

Figure 2: _Top_: Curve fitting results (solid) for VGG (red) and PreResNet (blue), pairwise connecting three modes with individual (left) and cumulative ensemble error rates (right). The individual error rate is the classification error rate for CIFAR100 for a given set of parameters \(t\) on the curve. The cumulative error rate is determined by the arg max of the sum of probabilities across all sampled parameters on the curve in \([0,t]\). _Bottom_: Alignment of embedding spaces along the curve for the absolute (green) and relative (purple) space. Dashed lines represent reference values, given as the average error and alignment of eleven independently trained networks.

Discussion

In this work, we investigate the alignment of latent observations when sampling along a curve connecting two modes. Our results show that transforming the spaces into a space of relative proximity reduces uncertainty significantly. This suggests that the uncertainty of ensembles is indeed confounded by reparametrization. The measured uncertainty varies across different relative projections within each modality. This is a limitation, as no general recommendation for a projection method can be made. Each projection will make the latent space invariant to some transformation, and the optimal choice of projection is model-dependent. Certain projections might bias uncertainty estimates. For example, the unit-norm scaling of the cosine similarity measure will increase uncertainties close to the zero vector. In this work, we used a random set of anchors, which eases the workflow of sampling from the curve. However the choice of anchors, e.g. archetypes or centroids, will likely influence the projection quality. The influence of projection function and anchor choice should be further investigated.

## Acknowledgments and Disclosure of Funding

FMM is supported by the Danish Data Science Academy, which is funded by the Novo Nordisk Foundation (NNF21SA0069429) and VILLUM FONDEN (40516) and the Pioneer Centre for AI, DNRF grant number P1. VM is supported by the PNRR MUR project PE0000013-FAIR. LKH is supported by the Pioneer Centre for AI, DNRF grant number P1 and the Novo Nordisk Foundation grant NNF22OC0076907 "Cognitive spaces - Next generation explainability".

## References

* Garipov et al. [2016] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson. Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs. URL http://arxiv.org/abs/1802.10026.
* Hauberg [2016] Soren Hauberg. _Differential Geometry for Generative Modeling_.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2016. doi: 10.1109/CVPR.2016.90.
* Huang et al. [2017] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. Snapshot ensembles: Train 1, get m for free. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=BJYwwV91l.
* Kim et al. [2017] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory Sayres. Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV). In _Proceedings of the 35th International Conference on Machine Learning_, pages 2668-2677. PMLR. URL https://proceedings.mlr.press/v80/kim18d.html.
* Krizhevsky [2009] Alex Krizhevsky. Learning multiple layers of features from tiny images. pages 32-33, 2009. URL https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
* Moschella et al. [2023] Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodola. Relative representations enable zero-shot latent space communication. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=SrC-nwieGJ.
* Simonyan and Zisserman [2015] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition, 2015. URL https://arxiv.org/abs/1409.1556.
* Valeriani et al. [2023] Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and Alberto Cazzaniga. The geometry of hidden representations of large transformer models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 51234-51252. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/a0e66093d7168b40246af1cdc025daa-Paper-Conference.pdf.

* [10] Kristoffer Wickstrom, Daniel Johansen Trosten, Sigurd Eivindson Lokse, Ahcene Boubekki, Karl Oyvind Mikalsen, Michael Kampffmeyer, and Robert Jenssen. RELAX: Representation Learning Explainability. ISSN 0920-5691. doi: 10.1007/s11263-023-01773-2. URL https://munin.uit.no/handle/10037/30298.
* [11] Yu Zhang, Peter Tino, Ales Leonardis, and Ke Tang. A Survey on Neural Network Interpretability. 5(5):726-742. ISSN 2471-285X. doi: 10.1109/TETCI.2021.3100641. URL http://arxiv.org/abs/2012.14261.

## Appendix A Reparametrization issue and uncertainty in latent space

Consider the linear function \(f:\mathbf{x}\rightarrow\mathbf{y}\) defined as

\[\mathbf{y}=\mathbf{W}_{2}\mathbf{W}_{1}\mathbf{x},\] (6)

where \(\mathbf{W}_{1}\in\mathbb{R}\ ^{\times D}\) and \(\mathbf{W}_{2}\in\mathbb{R}^{D\times.}\) are the learned parameters \(\theta\) of the linear map using a loss function \(\mathcal{L}(\theta)\). The intermediate representation \(\mathbf{z}\) takes the form

\[\mathbf{z}=h(\mathbf{x})=W_{1}\mathbf{x}\.\]

It is evident that there exist infinitely many optimal solutions of \(\mathbf{W}_{1}\) and \(\mathbf{W}_{2}\) for the same function \(f\). While \(f\) remains unchanged, the reparametrization might have a influence on \(h\). Consider the following example, where we define \(\mathbf{W}_{1}^{\prime}=\mathbf{Q}\mathbf{W}_{1}\) and \(\mathbf{W}_{2}^{\prime}=\mathbf{Q}^{-1}\mathbf{W}_{2}\). From Eq. 6 it is easy to see that \(f\) does not change, however, the embedding function becomes \(h^{\prime}(\mathbf{x})=\mathbf{Q}\mathbf{W}_{1}\mathbf{x}=\mathbf{Q}\mathbf{z}\).

Given two samples of a latent vector \(\mathbf{z}_{n}\) as \(\mathbf{z}_{n}^{1}=h(\mathbf{x}_{n})\) and \(\mathbf{z}_{n}^{2}=h^{\prime}(\mathbf{x}_{n})\), the variance estimate according to Eq. 1 becomes

\[\sigma^{2}(\mathbf{z}_{n}) =\frac{1}{2}[(\mathbf{z}_{n}^{1}-\bar{\mathbf{z}}_{n})^{2}+( \mathbf{z}_{n}^{2}-\bar{\mathbf{z}}_{n})^{2}]\] (7) \[=\left(\frac{\mathbf{I}+\mathbf{Q}}{2}\ \mathbf{z}_{n}^{1}\right)^{2}\] (8)

where \((\mathbf{z}_{n}^{1}-\bar{\mathbf{z}}_{n})^{2}=(\mathbf{z}_{n}^{2}-\bar{ \mathbf{z}}_{n})^{2}\) and \(\bar{\mathbf{z}}_{n}=\frac{\mathbf{I}+\mathbf{Q}}{2}\mathbf{z}_{n}^{1}\). Consider now the transformation \(\mathcal{T}:\mathbf{z}\rightarrow\mathbf{q}\) proposed by [7]. The transformed latent vector \(\mathbf{q}\) is expressed relative to a set of anchors \(\mathcal{A}=\{\mathbf{z}_{j}\}_{j}^{A}\). Usually one chooses the number of anchors to match the dimensionality of \(\mathbf{z}\), in which case one can write \(\mathcal{A}\) as a square matrix \(\mathbf{A}\in\mathbb{R}^{D\times D}\), where the \(d\)'th row vector \(\mathbf{a}_{d}\) is the \(j\)'th latent vector \(\mathbf{z}_{j}\) in \(\mathcal{A}\). Using a similarity function \(sim:\mathbb{R}^{D}\times\mathbb{R}^{D}\rightarrow\mathbb{R}\), [7] defines the transformed latent vector \(q_{i}\) as

\[\mathbf{q}_{i}=\left[sim(\mathbf{z}_{i},\mathbf{a}_{1}),\ sim(\mathbf{z}_{i}, \mathbf{a}_{2}),\...,\ sim(\mathbf{z}_{i},\mathbf{a}_{D})\right].\] (9)

If we use the cosine similarity as a similarity function and pose the same reparametrization problem to the transformed latent vector \(q_{i}\), the \(d\)'th element of the transformed latent vector \(\mathbf{q}_{i}^{\prime}\in\mathbb{R}^{D}\) is calculated as

\[q_{id}^{\prime}=\frac{(\mathbf{Q}\mathbf{z}_{i})^{T}\mathbf{Q}\mathbf{a}_{d}}{ ||\mathbf{Q}\mathbf{z}_{i}||\ ||\mathbf{Q}\mathbf{a}_{d}||}=\frac{\mathbf{z}_{i}^{T}\mathbf{Q}^{T}\mathbf{Q} \mathbf{a}_{d}}{||\mathbf{Q}\mathbf{z}_{i}||\ ||\mathbf{Q}\mathbf{a}_{d}||}\] (10)

Equation 10 shows that Relative Representation with a cosine similarity makes the latent space invariant if \(q_{id}^{\prime}=q_{id}\). This holds true for rotation and scaling transformations, i.e \(\bar{\mathbf{Q}}=\alpha\mathbf{U}\), where \(\mathbf{U}^{T}\mathbf{U}=\mathbf{I}\).

## Appendix B Latent space sampling

The following is a reformulation of the curve finding experiments by [1]. Consider two fully converged models \(f_{1}\) and \(f_{2}\) with parameters \(\hat{w}_{1}\) and \(\hat{w}_{2}\). A Bezier curve \(\phi_{12}(t)\) with the endpoints fixed at \(\hat{w}_{1}\) and \(\hat{w}_{2}\) and a single bend is defined as

\[\phi_{12}(t)=(1-t)^{2}\hat{w}_{1}+2t(1-t)\theta+t^{2}\hat{w}_{2},\quad 0<t<1\.\] (11)

Here, \(\theta\) are the parameters of the curve and \(t\) is the interpolation variable along the curve, where \(\phi_{12}(t=0)=\hat{w}_{1}\) and \(\phi_{12}(t=0)=\hat{w}_{2}\).

Note that the number of parameters of \(\theta\) is equivalent to the number of parameters in \(\hat{w}_{1}\) or \(\hat{w}_{2}\) times the number of bends along the curve. The optimal curve is found by minimizing the loss \(\mathcal{L}\) below the entire curve, which is defined as

\[l(\theta)=\int_{0}^{1}\mathcal{L}(\phi(t)),dt=\mathbb{E}_{t\sim U(0,1)}[L(\phi_ {\theta}(t))]]\,\]

where \(\mathcal{L}\) is the loss function used to find \(\hat{w}_{1}\) and \(\hat{w}_{2}\).

## Appendix C Transformations

Table 1 shows transformations of a latent observation \(\mathbf{z}_{n}\) into a latent observation of relative proximity \(\mathbf{q}_{n}\) based on a set of anchors \(\mathcal{A}=\{\mathbf{z}_{i}\}_{i}^{A}\).

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Transformation** & **Description** & \\ \hline Rel. Cosine & Basis Transformation, such that the new basis is & \(\mathbf{q}_{n}=\frac{\mathbf{z}_{n}\mathbf{A}^{T}}{||\mathbf{z}_{n}||\,|| \mathbf{A}_{i}||}\) \\  & the cosine distance of each anchor to each point & \\ Rel. Euclidean & Basis transformation, such that the new basis is & \(\mathbf{q}_{n}=\left[q_{1},q_{2},...,q_{D}\right]\), \\  & the euclidean distance of each anchor to each point & where \(q_{i}=||\mathbf{z}_{n}-\mathbf{a}_{i}||_{2}\) \\  & & for \(i\in[1,...,D]\) \\ Rel. Basis & Change of basis based on anchor points & \(\mathbf{q}_{n}=\frac{\mathbf{z}_{n}\mathbf{A}^{T}}{||\mathbf{A}_{i}||^{2}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of transformations for a latent vector \(\mathbf{z}_{i}\in\mathbb{R}^{D}\) and a matrix of anchors \(\mathbf{A}\in\mathbb{R}^{D\times D}\), where each row vector \(\mathbf{a}_{i}\) in \(\mathbf{A}\) corresponds to an element in the set of anchors \(\mathcal{A}=\{\mathbf{z}_{i}\}_{i}^{A}\)