# MathWriting: A Dataset For Handwritten Mathematical Expression Recognition

 Philippe Gervais

pgervais@acm.org

&Asya Fadeeva

Google

fadeich@google.com

&Andrii Maksai

Google

amaksai@google.com

Work performed while employed at Google

###### Abstract

Recognition of handwritten mathematical expressions allows to transfer scientific notes into their digital form. It facilitates the sharing, searching, and preservation of scientific information. We introduce MathWriting, the largest online handwritten mathematical expression dataset to date. It consists of **230k human-written samples** and an additional **400k synthetic ones**. This dataset can also be used in its rendered form for offline HME recognition. One MathWriting sample consists of a formula written on a touch screen and a corresponding LaTeX expression. We also provide a normalized version of LaTeX expression to simplify the recognition task and enhance the result quality. We provide baseline performance of standard models like OCR and CTC Transformer as well as Vision-Language Models like PaLI on the dataset. The dataset together with an example colab is accessible on Github.

## 1 Introduction

Three examples of HME from MathWriting. More examples can be found in Appendix K. Each ink is accompanied by a unique identifier that matches a corresponding filename in the dataset.

**MathWriting dataset (2.9 GB):**

https://storage.googleapis.com/mathwriting_data/mathwriting-2024.tgz

**Associated code:**

https://github.com/google-research/google-research/tree/master/mathwriting

Online _text_ recognition models have improved a lot over the past years, because of improvements in model structure [1; 2; 3] and also because of an increase in the amount of training data [4; 5; 6]. Mathematical expression (**ME**) recognition is a challenging task that has received less attention than regular recognition of words and characters [7]. ME recognition is different from regular text recognition in a number of interesting ways which can prevent improvements from transferring from one to the other. Though MEs share with text most of their symbols, they follow a more rigid structure which is also two-dimensional, see Figure 1. Where text can be treated to some extent as a one-dimensional problem amenable to sequence modeling, MEs cannot because the relative positionof symbols in space is meaningful. It is also different from symbol segmentation or object detection because the output of a recognizer has to contain the relationship between symbols, serialized in some form (LIFEX, a graph, InkML, etc.). Similarly to the case of text, _handwritten_ MEs (**HME**) are more difficult to recognize than _printed_ ones as they are more ambiguous and less training data is available.

Handwritten data is costly to obtain as it must be written by hand, which is compounded in the case of online representation (**ink**) by the necessity to use dedicated hardware (touchscreen, digital pen, etc.). By publishing the MathWriting dataset, we hope to alleviate some of the needs for data for research purposes. Samples include a large number of human-written inks, as well as synthetic ones. MathWriting can readily be used with other online datasets like CROHME [8] or Detexify [9] - we publish the data in InkML format to facilitate this. It can also be used for offline ME recognition simply by rasterizing the inks, using code provided on the Github page2.

Footnote 2: https://github.com/google-research/google-research/tree/master/mathwriting

MathWriting is the largest set of online HME published so far - both human-written and synthetic. It significantly expands the set of symbols covered by CROHME [8], enabling more sophisticated recognition capabilities. Since inks can be rasterized, MathWriting can also been seen as larger than existing offline HME datasets [10; 11; 12]. For these reasons we introduce a new benchmark, applicable to both online and offline ME recognition.

This work's main contributions are:

* a large dataset of Handwritten Mathematical Expressions under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International3. Footnote 3: https://creativecommons.org/licenses/by-nc-sa/4.0/
* LIFEX ground truth expressions in normalized form to simplify training and to make evaluation more robust.
* Evaluation of different models like CTC Transformer and PaLI on the dataset to show what recognition quality could be achieved with the provided data.

The paper focuses on the high-level description of the dataset: creation process, postprocessing, train/test split, ground truth normalization, statistics, and a general discussion of the dataset content to help practitioners understand what can and cannot be achieved with it. All the low-level technical information like file formats can be found in the readme.md file present at the root of the dataset archive linked above. We also provide code examples on Github2, to show how to read the various files, process and rasterize the inks, and tokenize the LaTeX ground truth.

Footnote 3: https://creativecommons.org/licenses/by-nc-sa/4.0/

## 2 Dataset Creation

MathWriting dataset primarily consists of LaTeX expressions from Wikipedia, more details about the acquisition of expressions are provided in Appendix B. These expressions were used for both ink collection from human contributors Section 2.1 as well as synthetic data generation Section 2.2. We did a very limited filtering of very noisy human-written examples (described in Appendix C).

### Ink Collection

Inks were obtained from human contributors through an in-house Android app. Participants agreed to the standard Google terms of use and privacy policy. The task consisted in copying a rendered mathematical expression (prompt) shown on the device's screen using either a digital pen or a finger on a touch screen. Mathematical expressions used as prompt were first obtained in LaTeX format, then rendered into a bitmap through the LaTeX compiler (see Appendix A for the template used). 95% of MathWriting expressions were obtained from Wikipedia. The remaining ones were generated to cover underrepresented cases in Wikipedia, like isolated letters with nested sub/superscripts or complicated fractions (see Section B). Contributors were hired internally at Google. 6 collection campaigns were run between 2016 and 2019, each lasting between 2 to 3 weeks. Collected data contains only inks and labels, so no personally identifiable information is present in the dataset. Offensive content is highly unlikely because LaTeX expressions were taken from Wikipedia and we conducted a filtering of noisy data (described in Appendix C).

### Synthetic Samples and Isolated Symbols

We created synthetic samples in order to further increase the label diversity for training. This also enabled compensating for limitations of the human collection like the maximum length of the expressions, which were limited by the size of the screen they were written on. We used LaTeX expressions from Wikipedia that were not used in the data collection. The resulting synthetic data has a 90th percentile of expression length of 68 characters, compared to 51 in train. This is especially important as deep neural nets often fail to generalize to inputs longer than their training data [13; 14]. Using synthetic long inks together with the original human-written inks can help to eliminate that problem as shown in [15; 16]. The synthesis technique is as follows: starting from a raw LaTeX mathematical expression, we computed a DVI file using the LaTeX compiler, from which we extracted bounding boxes. We then used those bounding boxes to place handwritten individual symbols, resulting in a complete expression. See Figure 1 for an example of extracted bounding boxes and the resulting synthetic example.

Inks for individual symbols are all from the symbols split. They have been manually extracted from inks in train. For each symbol that we wanted to support, we manually selected strokes corresponding to it for 20-30 distinct occurrences in train, and used that information to generate a set of individual inks. Similar synthesis techniques have been used by [8] with inks, [10] and [12] with raster images.

A significant difference between synthetic and human-written inks is the stroke order. For synthetic inks, stroke order follows the order of the bounding boxes in the DVI file, which can be different from the usual order of writing for mathematical expressions. However, the writing order within a given symbol is consistent with human writing.

### Dataset Split

MathWriting is composed of five different sets of samples, which we call'splits': train, valid, test, symbols, and synthetic. The splits train, valid and test consist only of human-written examples. The split symbols is provided for synthetic data generation and is not used in training. The split of human-written samples between train, valid and test was partially done based on writers, partially based on labels. More details are provided in Appendix D. Experiments have shown that a more important factor than the handwriting style was whether the _label_ had already been seen during training. This fact is also supported by research in the area of compositional generalization [17]. In the published version, valid has a 55% (8.5k samples) intersection with train based on unique normalized labels, and test has an 8% intersection (647 samples). We chose to have a low intersection between train and test in order to correctly measure generalization of trained models to unseen labels.

### Label Normalization

All samples in the dataset come with two labels: the LaTeX expression that was used during the data collection (annotation label in the InkML files), and a normalized version of it meant for model training, which is free from a few sources of confusions for an ML model (annotation normalizedLabel). An example with original and normalized labels is provided in Figure 2. Label normalization covers three main categories (details are provided in Appendix E):

Figure 1: An example of a synthetic ink created from bounding boxes with label ((p+q)+(p-q))/2=q- e.g. bold, italic
- or that haven't been reproduced consistently by contributors.
* non-uniqueness of the LaTeX syntax. e.g. \frac{1}{2} and 1over 2 are equivalent.
* visual variations that can reproduced in handwriting but can't reliably be inferred by a model. This includes size modifiers like \left, \right.

We provide the raw labels to make it possible to experiment with alternative normalization schemes, which could lead to better outcomes for different applications.

#### 2.4.1 Limitations of normalization

The normalization process is purely syntactic, and can not cover cases where the meaning of the expression has to be taken into account. For example, a lot of expressions from Wikipedia use cos instead of \cos. It is often clear to a human reader whether the sequence of characters c,o,s represents the \cos command or simply three letters. However, this can not be reliably inferred by a syntactic parser, for example in tacos vs ta\cos. An alternative would be to update the raw labels, which we didn't do because we wanted to keep the information that was used during the collection as untouched as possible. Similarly, cases like 10^{-1} usually mean {10}^{-1}, though they render exactly the same. We made the choice to normalize to the former because it's the only option with a purely syntactic normalizer. It's also better than not removing these extra braces because it gives more consistent label structures, which simplifies the model training problem.

## 3 Dataset Statistics

In this section we describe the key characteristics of MathWriting and compare it to CROHME23 [8]. In Table 1 we provide the information about the volume of the dataset splits both in terms of examples (inks) and unique labels.

### Label Statistics

MathWriting contains 457k unique labels after normalization (see Section 2.4). From Table 1 we see that most unique expressions are covered by the synthetic portion of the dataset. However, the absolute number of unique expressions in human-written part is still high - 61k. This underlines the importance of synthetic data as it allows models to see a much bigger variety of expressions. It is important to note that the synthetic split has essentially no repeated expressions. On the other hand, in real data multiple different writings of the same expression are quite common (see Figure 11 in Appendix F). This fact allows us to separately evaluate model's quality on expressions that were

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & train & synthetic & valid & test \\ \hline \# distinct inks & 230k & 396k & 16k & 8k \\ \# distinct labels & 53k & 396k & 8k & 4k \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics on different subsets of MathWriting dataset.

observed during training and that those that hadn't. As seen in Table 2 the biggest intersection in expressions is between valid and train. The minimal overlap between test and train splits is beneficial for assessing a model's ability to generalize to expressions that were not seen in train.

The median length of expressions in characters is 26 which is comparable to one of the most popular English recognition datasets IAMonDB [18] which has median of 29 characters. However, it is important to note that LaTeX expressions have tokens that span multiple characters like \(\backslash\)frac. The median length of expressions in tokens (provided in Appendix J) is 17, thus making training a model on tokens rather then characters easier due to shorter target lengths [19; 20]. We want to emphasize that MathWriting can be used with a different tokenization scheme and token vocabulary from what we propose in Appendix J. In Figure 3 we show the number of occurrences for the most frequent tokens. Tokens \(\{\) and \(\}\) are by far the most frequent as they are integral to the LaTeX syntax.

### Ink Statistics

Each ink in MathWriting dataset is a sequence of strokes \(\text{I}=[s_{0},\dots,s_{n}]\), each stroke \(s_{i}\) consisting of points. A point is represented as a triplet \((x,y,t)\) where \(x\) and \(y\) are coordinates on the screen and \(t\) is a timestamp. In Table 3 we provide statistics on number of strokes, points, and duration of writing. It's important to note that as inks were collected on different devices, the absolute coordinate values can vary a lot. In human-written data the time information \(t\) always starts from 0 but it is not always the case in the synthetic split. Different samples often have different sampling rates (number of points written in one second) due to the use of different devices (see Figure 4). More details in Section 3.3. Consequently, the same ink written on two different devices can result in inks with a different number of points. For human-written inks, the sampling rate is consistent between strokes, but it is not the case for synthetic ones. In order to accommodate a model and make sequences shorter, inks can be resampled in time (see example in Figure 13, Appendix F).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & train & synthetic & valid & test \\ \hline train & - & 0 & 3.6k & 355 \\ synthetic & 0 & - & 0 & 0 \\ valid & 3.6k & 0 & - & 239 \\ test & 355 & 0 & 239 & - \\ \hline \hline \end{tabular}
\end{table}
Table 2: Counts of unique labels shared between MathWriting splits

Figure 3: Histogram of the top-100 most frequent tokens in MathWriting.

### Devices Used

Around 150 distinct device types have been used by contributors. In most cases inks were written on smartphones using a finger on a touchscreen. However, there are cases where tablets with styluses were used. The main device used in this case is Google Pixelbook, which accounted for 51k inks total (see Table 7, Appendix F). Out of all device types, 37 contributed more than 1000 inks. Note that writing on a touchscreen with a finger or a stylus results in different low-level artifacts. All devices were running the same Android application for ink collection, regardless of whether their operating system was Android or ChromeOS.

### Comparison With CROHME23

In this section we compare main dataset statistics of MathWriting and CROHME23 [8] as it is a popular publicly available dataset for HME recognition. In terms of overall size, MathWriting has nearly 3.9 times as many samples and 4.5 times as many distinct labels after normalization, see Table 4. A significant number of labels can be found in both datasets (47k), but the majority is dataset-specific. This suggests that combining both datasets during training could yield improved HME recognition quality. MathWriting has more human-written inks than CROHME23 as seen in Table 5, and contains a much larger variety of tokens. It has 254 distinct tokens including all Latin capital letters and almost the entire Greek alphabet. It also contains matrices, which are not included in CROHME23. Therefore, more scientific fields like quantum mechanics, differential calculus, and linear algebra can be represented using MathWriting.

## 4 Experiments

### Evaluation setup

We propose the following evaluation setup based on MathWriting for the quality of handwriting math expression recognition.

* **evaluation samples**: the test split of MathWriting.
* **metric**: character error rate (CER) [21], where a "character" is a LaTeX token as defined by the code in Appendix I.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & \multicolumn{2}{l}{MathWriting} & \multicolumn{2}{l}{CROHME23 Common} \\ \hline Inks & 650k & 164k & 0 \\ Labels & 457k & 102k & 47k \\ Vocab & 254 & 105 & 104 \\ \hline \hline \end{tabular} 
\begin{tabular}{l l l} \hline \hline  & \multicolumn{2}{l}{MathWriting} & \multicolumn{2}{l}{CROHME23 Common} \\ \hline human & 253k & 17k \\ synthetic & 396k & 147k \\ \hline \hline \end{tabular}
\end{table}
Table 4: Counts of inks, distinct labels and distinct tokens used in MathWriting and CROHME23. The single token present in CROHME23 but not in MathWriting is the literal dollar sign \(\backslash\$\).

Figure 4: Left: an ink with very low sampling rate (9.4 points per second) Right: an ink with very high sampling rate (260 points per second)We provide a reference implementation of the evaluation metric at the Github page 2. We propose the use of CER as a metric to make results comparable to other recognition tasks like text recognition [22; 23], and the use of LaTeX tokens instead of ASCII characters so that an error on a single non-latin letter (e.g. \alpha recognized as a) counts as one instead of many.

Footnote 2: https://github.com/en/gur/gur/gur/

### Baseline Recognition Models

In Table 6 we provide results for different models. All models are trained exclusively on the MathWriting dataset (train and synthetic), except for the OCR API that was trained on other datasets as well. The following models represent different approaches to handwriting recognition - offline [23], online [24] and mixed [25].

OcrThis is a publicly available Document AI OCR API [26], which processes bitmap images. It has been trained partly on samples from MathWriting. We sent inks rendered with black ink on a white background and searched for optimal image size and stroke width to get the best evaluation result from the model.

CTC TransformerThis model is a transformer base with a Connectionist Temporal Classification loss on top (**CTC**) [27]. It contains 11 transformer layers with an embedding size of 512. We used swish activation function and dropout of 0.15 as those parameters performed best on valid. We train with an Adam optimizer, learning rate of 1e-3, batch size 256 for 100k steps. One training run took 4 hours on 4 TPU v2. We trained from scratch and exclusively on MathWriting (train and synthetic). The model is similar to [24], replacing LSTM layers by Transformer layers and not using any external language model on top.

VlmWe fine-tuned a large Vision-Language Model PaLI [28] on MathWriting (train and synthetic). We used the representation proposed in [25] where an ink is represented as both a sequence of points (similar to CTC Transformer) and its rasterized version (similar to OCR). We train three models with different data shuffling for 200k steps with batch size 128, learning rate 0.3 and dropout 0.2. One training run took 14 hours on 16 TPU v5p. Models were finetuned exclusively on train and synthetic MathWriting data. Overall, it took 2 TPU v2 days and 28 TPU v5p days to run the experiments.

Table 6 shows the evaluation comparison between the three models. The OCR model has no information about the order of writing and speed (offline recognition), which explains its lower performance than methods that take time information into account (online recognition). The two other methods - PaLI and CTC Transformer perform significantly better than OCR. These results show that our dataset can be used to train classical recognition models like CTC transformer as well as more recent architectures like VLM.

Figure 5 shows examples of model mistakes. Two of the main causes of mistakes are confusing similar-looking characters like "z" and "2", and errors in the structural arrangement of the characters, for instance not placing a sub-expression in a subscript or superscript.

## 5 Discussion

### Differences in Writing Style

The number of contributors was large enough that a variety of writing styles are represented in the dataset. An example for different ways of writing letter 'r' can be seen in Figure 6. Additional

\begin{table}
\begin{tabular}{l l l c c} \hline \hline Model & Input & Parameters & CER on valid & CER on test \\ \hline OCR [26] & Image & - & 6.50 & 7.17 \\ CTC Transformer [25] & Ink & 35M & 4.52 (0.08) & 5.49 (0.05) \\ PaLI [25] & Image+Ink & 700M & 4.47 (0.08) & 5.95 (0.06) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Recognition results for different models. The evaluation metric is reported on both the valid and test splits.

examples are provided in Figure 7. Similar though less obvious differences exist for other letters. Style differences also show through writing order (example - Figure 14, Appendix G).

### Recognition Challenges

MathWriting presents some inherent recognition challenges, which are typical of handwritten representations. For example, it's not really possible to distinguish these pairs from the ink alone: \frac{\nder{a}\}\bs vs\frac{\nder{a}\}\bs, and \overline\omega\)\(\ve{\ve{\text{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\text{overline}}}}\)\(\ve{\text{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0} \pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\text{overline}}}\)\(\ve{\text{\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\text{overline}}}\)\(\ve{\text{\color[rgb]{0,0,0}\definecolor[named]{to improve this process by modifying the location, size or orientation of bounding boxes prior to generating the synthetic inks. This would soften LaTeX's rigid structure and make synthetic data closer to human handwriting. Another application of these bounding boxes would be to bootstrap a recognizer that would also return character segmentation information. This kind of output is critical for some UI features - for example, editing an handwritten expression.

MathWriting can also be improved by varying the label normalization. Changing it can have different benefits depending on the application, as mentioned above. We provide the source LaTeX string for that reason. Another possible improvement in recognition can come from additional contextual information, for instance the scientific field [29] that can be added post-hoc. Combining recognizers with a language model [24] trained on a large set of mathematical expressions would be a step in a similar direction.

## 6 Limitations

A single sample in MathWriting dataset has one handwritten LaTeX formula, see Figure 2. As a result, models that are trained on this dataset would probably perform poorly on complete handwritten documents, such as the IAMonDo dataset [30]. Also, as the dataset contains only LaTeX expressions, it is unlikely that models trained on it will accurately recognize handwritten text in English or other languages. As shown in Figure 3, some LaTeX tokens are way more frequent than others. Some infrequent tokens like \(\verb|ni|\) could be hard to recognise.

## 7 Conclusion

We introduced MathWriting, the largest dataset of online handwritten mathematical expressions to date, together with the experimental results of three different types of models. We hope this dataset will help advance research in both online and offline mathematical expression recognition. Additionally, we invite data practitioners to build on the dataset. We intentionally chose a file format for MathWriting close to the one used by CROHME to facilitate their combined use. We also provided original or intermediate representations (raw LaTeX strings, bounding boxes) to enable experimentation with the data itself, and suggested a few directions (Section 5.3).

Figure 8: Left: character ambiguity. Is it \(1\leq x_{n}<x_{n+1}\) or \(1\leq n_{\eta}<n_{\eta+1}\)? Right: what is the fraction nesting order?

## References

* [1] Alex Graves, Marcus Liwicki, Santiago Fernandez, Roman Bertolami, Horst Bunke, and Jurgen Schmidhuber. A novel connectionist system for unconstrained handwriting recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 31:855-868, 2009.
* [2] Johannes Michael, Roger Labahn, Tobias Gruning, and Jochen Zollner. Evaluating sequence-to-sequence models for handwritten text recognition. _2019 International Conference on Document Analysis and Recognition (ICDAR)_, pages 1286-1293, 2019.
* [3] Yutian Liu, Wenjun Ke, and Jianguo Wei. Attention guidance mechanism for handwritten mathematical expression recognition, 2024.
* [4] Emre Aksan, Fabrizio Pece, and Otmar Hilliges. Deepwriting: Making digital ink editable via deep generative modeling, 2018.
* [5] Harold Mouchere, Christian Viard-Gaudin, Richard Zanibbi, and Utpal Garain. Icfhr 2014 competition on recognition of on-line handwritten mathematical expressions (crohme 2014). volume 2014, 09 2014.
* [6] Harold Mouchere, Christian Viard-Gaudin, Richard Zanibbi, and U. Garain. Icfhr2016 crohme: Competition on recognition of online handwritten mathematical expressions. In _2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR)_, pages 607-612, 2016.
* [7] May Mowaffaq AL-Taee, Sonia Ben Hassen Neij, and Mondher Frikha. Handwritten recognition: A survey. _2020 IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS)_, pages 199-205, 2020.
* [8] Yejing Xie, Harold Mouchere, Foteini Liwicki, Sumit Rakesh, Rajkumar Saini, Masaki Nakagawa, Cuong Nguyen, and Thanh-Nghia Truong. _ICDAR 2023 CROHME: Competition on Recognition of Handwritten Mathematical Expressions_, pages 553-565. 08 2023.
* [9] Detexify data. https://github.com/kirel/detexify-data.
* [10] Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, and Alexander M. Rush. Image-to-markup generation with coarse-to-fine attention, 2017.
* [11] Ye Yuan, Xiao Liu, Wondimu Dikubab, Hui Liu, Zhilong Ji, Zhongqin Wu, and Xiang Bai. Syntax-aware network for handwritten mathematical expression recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4553-4562, 2022.
* [12] Aida calculus math handwriting recognition dataset. https://www.kaggle.com/datasets/aidapearson/ocr-data.
* [13] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models, 2022.
* [14] Dusan Varis and Ondrej Bojar. Sequence length is a domain: Length-based overfitting in transformer models. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, 2021.
* [15] Aleksandr Timofeev, Anastasiia Fadeeva, Andrei Afonin, Claudiu Musat, and Andrii Maksai. _DSS: Synthesizing Long Digital Ink Using Data Augmentation, Style Encoding and Split Generation_, page 217-235. Springer Nature Switzerland, 2023.
* [16] Arun Narayanan, Rohit Prabhavalkar, Chung-Cheng Chiu, David Rybach, Tara N. Sainath, and Trevor Strohman. Recognizing long-form speech using streaming end-to-end models, 2019.
* [17] Daniel Keysers, Nathanael Scharli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: A comprehensive method on realistic data, 2020.

* [18] M. Liwicki and H. Bunke. IAM-OnDB-an on-line english sentence database acquired from handwritten text on a whiteboard. In _ICDAR'05_. IEEE, 2005.
* [19] Dusan Varis and Ondrej Bojar. Sequence length is a domain: Length-based overfitting in transformer models. pages 8246-8257, 01 2021.
* [20] Masato Neishi and Naoki Yoshinaga. On the relation between position information and sentence length in neural machine translation. In _Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)_, pages 328-338, Hong Kong, China, November 2019. Association for Computational Linguistics.
* [21] Johannes Michael, Roger Labahn, Tobias Gruning, and Jochen Zollner. Evaluating sequence-to-sequence models for handwritten text recognition, 2019.
* [22] George Retsinas, Giorgos Sfikas, Basilis Gatos, and Christophoros Nikou. Best practices for a handwritten text recognition system, 2024.
* [23] Dmitrijs Kass and Ekta Vats. Attentionhtr: Handwritten text recognition based on attention encoder-decoder networks, 2022.
* [24] Victor Carbune, Pedro Gonnet, Thomas Deselaers, Henry A. Rowley, Alexander Daryin, Marcos Calvo, Li-Lun Wang, Daniel Keysers, Sandro Feuz, and Philippe Gervais. Fast multi-language lstm-based online handwriting recognition, 2020.
* [25] Anastasia Fadeeva, Philippe Schlattner, Andrii Maksai, Mark Collier, Efi Kokiopoulou, Jesse Berent, and Claudiu Musat. Representing online handwriting for recognition in large vision-language models, 2024.
* [26] Google Cloud. Detect handwriting in image, 2023.
* [27] Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In _Proceedings of the 23rd International Conference on Machine Learning_, ICML '06, page 369-376, New York, NY, USA, 2006. Association for Computing Machinery.
* [28] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model, 2023.
* [29] Mark Collier, Rodolphe Jenatton, Efi Kokiopoulou, and Jesse Berent. Transfer and marginalize: Explaining away label noise with privileged information. In _International Conference on Machine Learning_, 2022.
* [30] Emanuel Indermuhle, Marcus Liwicki, and Horst Bunke. Iamondo-database: An online handwritten document database with non-uniform contents. pages 97-104, 06 2010.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We claim to provide a large dataset of HME - see Section 1, together with normalized labels - the process is described in Section 2.4 and example is provide in Figure 2. Experimental results on this dataset are presented in Section 4.2.
2. Did you describe the limitations of your work? [Yes] We described general limitations of MathWriting dataset in Section 6, limitations of label normalization in Section 2.4.1, recognition challenges of mathematical expressions in Section 5.2 and sources of noise in the dataset in Section H.

3. Did you discuss any potential negative societal impacts of your work? [NA] The type of the dataset we are publishing is not new, there are similar datasets like CROHME23 [8]. Given the widespread use of handwriting recognition, we don't see any potential negative impacts of our work.
4. Have you read the ethics review guidelines and ensured that your paper conforms to them? All the participants were payed the minimum hourly rate as discussed in Section 2.1. The dataset doesn't include any personal information about contributors apart from their handwriting samples that they agreed to share.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [NA] Our paper doesn't include any theoretical results. 2. Did you include complete proofs of all theoretical results? Our paper doesn't include any theoretical results.
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? All experiments in this paper are conducted using publicly available datasets. We provide code in Github for ink rasterisation, CER computation and expression tokenization. The Visual-Language Model PaLI used in the experiments is non-open-sourced, so full results from Table 6 cannot be reproduced. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? We specify the training details like number of parameters, learning rate, dropout, training data, etc. for the models - CTC transformer and PaLI in Section 4.2. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? In Section 4.2 we report average and variance of three training runs with different shuffling of training data. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? In Section 4.2 we provide the total number of TPU days it took to run our experiments.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? We used pretrained PaLI model for finetuning and cited [28]. 2. Did you mention the license of the assets? As the PaLI model is non-open-sourced there is no license for it. 3. Did you include any new assets either in the supplemental material or as a URL? We discuss the dataset and code are provided in Section 1. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? We discussed the conditions of data collection in Section 2.1. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? We mention in Section 2.1 that there is no personally identifiable information present in the dataset and offensive content is highly unlikely given the nature of the dataset.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? We describe the instructions of the data campaigns in Section 2.1 as they are quite simple - to write a rendered expression provided on the screen. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? The paper does not involve research with human subjects. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? We write about the employment of participants in Section 2.1, we don't disclose the exact amount of compensation as it is confidential information.

## Appendix A LaTeX template for label rendering

All the packages and definitions that are required to compile all the normalized and raw labels:

``` \usepackage{amsmath} \usepackage{amsfonts} \usepackage{amssmbb} \usepcommand{R}{mathbbb{R}} \newcommand{C}{mathbbb{C}} \newcommand{Q}{Q}{mathbbb{Q}} \newcommand{Q}{Z}{mathbbb{Z}} \newcommand{N}{mathbb{N}} ```

## Appendix B Acquisition of LaTeX Expressions

The labels we publish mostly come from Wikipedia (95% of all samples have labels from Wikipedia). A small part were generated, to cover deeply nested fractions, number-heavy expressions, and isolated letters with nested superscripts and subscripts, which are rare in Wikipedia.

The extraction process from Wikipedia followed these steps:

* download an XML Wikipedia dump which provides Wikipedia's raw textual content. enwiki-20231101-pages-articles.xml was used for synthetic samples, older dumps for human-written ones
* extract all LaTeX expressions from that file. This gives the list of all mathematical expressions in LaTeX notation from Wikipedia
* keep those which could be compiled using the packages listed in Appendix A. Wikipedia contains a significant number of expressions that are not accepted by the LaTeX compiler, because of syntax errors or other reasons
* keep only those which can be processed by our normalizer which only supports a subset of all LaTeX commands and structures

For expressions used for synthesis, the following extra steps were performed:

* keep only the expressions whose normalized form contains more than a single LaTeX token. Example: \alpha is rejected but \alpha^{2} is kept. This step is useful to eliminate trivial expressions that wouldn't add any useful information
* de-duplicate expressions based on their normalized form. e.g. \frac{}{}{} frac12 and \frac{}{} frac{1}{2} normalize to the same thing, we kept only one of them in raw form
* restrict the list of expressions to the same set of tokens used in the train split: if the normalized form of an expression contained at least one token that was not also present somewhere in train, it was discarded.

## Appendix C Postprocessing of MathWriting dataset

We applied no postprocessing to the collected inks other than dropping entirely those that were completely unreadable or had stray marks. Inks are provided in their original form, as they were recorded with the collection app. What we _did not do_ was to discard samples that were very hard to read or ambiguous, because we believe this type of sample to be essential in training a high-quality model.

Some cleanup was performed on the labels (ground truths). The goal was to make the dataset better suited to training an ML model, and eliminate unavoidable issues that occurred during the collection. After training some initial models, we manually reviewed samples for which they performed poorly. This helped identify a lot of unusable inks (near-blank, lots of stray strokes, scribbles, etc.), anda lot of ink/label discrepancies. A fairly common occurrence was a contributor forgetting to copy part of the prompted expression. We adjusted the label to what was actually written unless the ink contained a partially-drawn symbol, in which case we discarded the sample entirely. In this process we eliminated or fixed around 20k samples.

The most important postprocessing step was to normalize the labels: there are many different ways to write a mathematical expression in LaTeX format that will render to images that are equivalent in handwritten form. We applied a series of transformations to eliminate as many variations as possible while retaining the same semantic. This greatly improved the performance of models and made their evaluation more precise. We publish both the normalized and raw (unnormalized) labels, to enable people to experiment with other normalization procedures.

This normalization is similar to what [10] did, but pushed further because of the specifics of handwritten MEs. See Section 2.4 for more detail.

## Appendix D Dataset split

The valid and test splits are the result of multiple operations performed between 2016 and 2019. The first split operation, performed on the data available in 2016, was based on the contributor id: any given contributor's samples would not appear in more than one split (either train, valid, test). This is common practice for handwriting recognition systems, to test how the recognizer performs on unseen handwriting styles.

Experiments then showed that a more important factor than the handwriting style was whether the _label_ had already been seen during training. Subsequent data collection campaigns focused on increasing label variety, and new samples were added to valid and test, this time split by label: a given normalized mathematical expression would not appear in more than one split.

## Appendix E Label Normalization

### Syntactic Variations

There are several ways to change a LaTeX string without changing the rendered output significantly. The normalization we implemented does the following:

* all unnecessary space is dropped
* all command arguments are consistently put in curly braces
* superscripts and subscripts are put in curly braces and their order is normalized. e.g. a^2_1 becomes a_{1}^{2}.
* redundant braces are dropped
* infix commands are replaced by their prefix versions. e.g. \over is replaced by \frac
* a lot of synonyms are collapsed. e.g. \le and \leq,\longrightarrow and \rightarrow, etc. Some of the synonyms are only synonyms in handwriting. For example \star (\star) and \(*\) are different in print (5-prong and 6-prong stars), but the difference was not expressed in handwriting by our contributors.
* functions commands like \sin are replaced by the sequence of letters of the function name (e.g. \sin is replaced by \sin). This reduces the output vocabulary, and eliminates a source of confusion because we found that LaTeX expressions from Wikipedia come with a mix of function commands and sequences of letters.
* expansion of abbreviations. e.g. \cdots, \ldots, etc. have been replaced by the corresponding sequence of characters.
* matrix environments are normalized to use only the'matrix' environment surrounded by the proper delimiters like brackets or parentheses.
* \binom is turned into a 2-element column matrix. Expressions from Wikipedia did not use those consistently, so we made the choice to normalize \binom away.

### Differences Between Print And Handwriting

The following characteristics can not be represented in handwriting and have been normalized away:

* color
* accurate spacing: e.g. ~, \quad.
* font style and size: e.g. \mathrm, \mathrm, \mathrm, \mathrmathbf, \scriptstyle.

There are others that can be represented in handwriting, but that are not consistent enough in MathWriting to be preserved:

* font families: Fraktur, Calligraphic. In practice, only Blackboard (\mathrmathbb) has been written consistently enough by contributors that we were able to keep it: \mathrmathcal and \mathrmathfrak are dropped.
* some variations like \rightarrow\(\rightarrow\) and \longrightarrow\(\rightarrow\).
* some character variations. e.g. \varrrho, \varepsilon
* size modifiers like \left, \right, \big. Similarly, variable-width diacritics like \widehat.

## Appendix F Additional dataset statistics

In this section we show additional graphs that illustrate dataset statistics that are described in Section 3. The frequencies of normalized LaTeX expressions are presented in Figure 11. Figure 12 illustrates the

Figure 10: Examples of expression normalization. See Section 2.4 for details.

distribution of sampling rates within human-written data. Results of resampling points in time are presented in Figure 13.

with time resampling are given on Figure 13.

## Appendix G Variety of Writing Styles

In this section we provide additional examples of differences in the writing order of fractions - Figure 14. These examples show that MathWriting dataset contains a variety of writing styles.

Figure 11: Counts of links corresponding to the same normalized expression, ordered by increasing count. Each position on the horizontal corresponds to a unique normalized expression. Almost 5k unique expressions have been written 10 times or more by contributors.

Figure 12: Histogram of sampling rates in human-written data of MathWriting dataset.

Figure 13: Examples of time resampling with different time periods. Larger periods result in shorter sequences of points.

## Appendix H Sources of Noise

The result of any task performed by humans will contain mistakes, and MathWriting is no exception. We've done our best to remove most of the mistakes, but we know that some remain.

Stray strokesThese do not carry any meaning and should be ignored by any recognizer. Since they also appear in real applications, there could be some benefit in having some in the dataset to teach the model about them. That said, it being usually easier to add noise rather than to remove it, we made the choice of discarding as many inks containing stray strokes as possible. Not all inks with stray strokes have been found and removed though (e.g. train/9e64be65cb874902.inkml that was discovered post-publication). The fraction of inks containing stray strokes is significantly lower than 1%, and should not be an issue for training a model.

Incorrect ground truthContributors did not always copy the prompt perfectly, leading to a variety of differences. In most of the cases we spotted, we were able to fix the label to match what had actually been written. A short manual review once the dataset was in its final state showed the rate of incorrect ground truth to be between 1% and 2%. Most of the mistakes are very minor, usually a single token added, missing or incorrect. Errors here also come from ambiguities or misuse of

\begin{table}
\begin{tabular}{l c} \hline \hline Device type & Ink \\ \hline Google PixelBook & 51k \\ Google Nexus 5X & 28k \\ Coolpad Mega 2.5D & 14k \\ OnePlus One & 13k \\ Google Nexus 5 & 11k \\ Google Nexus 6 & 11k \\ Coolpad Mega 3 & 8k \\ LG Optimus L9 & 8k \\ Galaxy Grand Duos & 7k \\ Google Pixel XL & 6k \\ Samsung Galaxy S7 & 5k \\ \hline \hline \end{tabular}
\end{table}
Table 7: Top-12 devices used, with the number of samples obtained from each device. The bias towards Google devices simply reflects the conditions in which inks were collected.

Figure 14: Examples of various writing orders found in the training set. Red arrows show the movement of the pen between strokes. Top left: most common writing order (top-down, fraction bar drawn left-to-right), top right: fraction bar written first, bottom left: fraction bar drawn right-to-left, bottom right: fraction written bottom-up.

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_EMPTY:19]

### Synthetic Samples: Expressions from Wikipedia

[FIGURE:S4.F1][ENDFIGU\[\begin{array}{c}\includegraphics[width=140.0pt]{28.eps}\end{array}\]

#### k.2.1. Synthetic Samples: Generated Fractions

[FIGURE:A1.F1][E