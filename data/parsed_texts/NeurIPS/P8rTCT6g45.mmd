# Memory-Efficient LLM Training with Online Subspace Descent

 Kaizhao Liang\({}^{\dagger}\), Bo Liu\({}^{\dagger}\), Lizhang Chen\({}^{\dagger}\), Qiang Liu\({}^{\dagger}\)

\(\dagger\)The University of Texas at Austin

{kaizhaol,bliu,lzchen,lqiang}@utexas.edu

###### Abstract

Recently, a wide range of memory-efficient LLM training algorithms have gained substantial popularity. These methods leverage the low-rank structure of gradients to project optimizer states into a subspace using projection matrix found by singular value decomposition (SVD). However, convergence of these algorithms is highly dependent on the update rules of their projection matrix. In this work, we provide the _first_ convergence guarantee for arbitrary update rules of projection matrix. This guarantee is generally applicable to optimizers that can be analyzed with Hamiltonian Descent, including most common ones, such as LION, Adam. Inspired by our theoretical understanding, we propose Online Subspace Descent, a new family of subspace descent optimizer without SVD. Instead of updating the projection matrix with eigenvectors, Online Subspace Descent updates the projection matrix with online PCA. Online Subspace Descent is flexible and introduces only minimum overhead to training. We show that for the task of pretraining LLaMA models ranging from 60M to 7B parameters on the C4 dataset, Online Subspace Descent achieves lower perplexity and better downstream tasks performance than state-of-the-art low-rank training methods across different settings and narrows the gap with full-rank baselines.1

Footnote 1: Code is available at https://github.com/kyleliang919/Online-Subspace-Descent.

## 1 Introduction

The continual advancement in training large language models (LLMs) presents a compelling challenge in balancing computational efficiency with model performance. As the scope and complexity of these models grow, so does the necessity for innovative strategies that optimize memory usage without compromising the learning capabilities of the model. Recent approaches in low-rank adaptation strategies, including Stochastic Subspace Descent [13], LoRA [11], ReLoRA [15], Gradient Low-Rank Projection (GaLore) [25] and Sketchy [9], have paved the way for memory-efficient training by utilizing a periodically updated low-rank projection matrix to manage parameter updates. In particular, GaLore and Sketchy both utilize expensive singular value decomposition to determine the projection matrix, whereas stochastic subspace descent suggests using random matrices as projection matrices and provides convergence analysis on convex functions and objectives. However, to the best of our knowledge, no one has offered any guarantee of convergence for this class of methods on non-convex functions and objectives.

In this work, we provide the first convergence guarantee for arbitrary update rules of the projection matrix. This guarantee is significant because it is broadly applicable to a wide range of optimizers that can be analyzed within the Hamiltonian descent framework [18]. By establishing this convergence guarantee, we demonstrate that our approach is not limited to specific or narrowly defined update rules, but can be extended to include many commonly used optimizers in the field. In particular, thisincludes popular algorithms such as LION [4] and Adam [12], which are widely used in various machine learning and optimization tasks. Our results therefore offer a robust theoretical foundation for understanding and analyzing the behavior of these optimizers, ensuring their effectiveness and reliability in diverse applications.

Inspired by our theoretical understanding, we introduce a novel family of memory-efficient optimizers named Online Subspace Descent, which incorporates a dynamically changing projection matrix, replacing the conventional periodic updating approach (SVD) with online PCA. By allowing the projection matrix to evolve in response to the changing gradient landscape, Online Subspace Descent enhances the model's ability to navigate the parameter space more effectively. This dynamic adaptation aligns more closely with the natural progression of learning in deep neural networks, which is characterized by changes in the importance of different characteristics and interactions as training progresses. Through extensive experiments and comparative analysis, we demonstrate that our approach presents lower perplexity in pretraining LLaMA models (ranging from 60M to 1B parameters) on the C4 dataset compared to state-of-the-art low-rank training methods, closing the perplexity gap with full-rank baselines on language model pretraining.

## 2 Optimization Background

The training of deep learning models reduces to an optimization problem

\[\min_{\bm{W}}L(\bm{W}),\]

where \(\bm{W}\) is the set of weight matrices of the model. For simplicity, we assume \(\bm{W}\in\mathbb{R}^{n\times m}\) is a single matrix of size \((n,m)\) without loss of generality. For notation, we write \(\langle\bm{A},\bm{B}\rangle=\operatorname{tr}(\bm{A}^{\top}\bm{B})\) for inner products of matrices, and \(\|A\|^{2}=\operatorname{tr}(\bm{A}^{\top}\bm{A})\) the Frobenius norm. We use \(A\odot B\) to denote the elementwise product, and \(A^{\odot 2}=A\odot A\).

**Example 2.1**.: _Update rules of common optimizers:_

\[\begin{array}{llll}\text{Gradient Descent}:&\bm{W}_{t+1}=\bm{W}_{t}- \epsilon_{t}\nabla L(\bm{W}_{t}),\\ \text{Momentum}:&\bm{W}_{t+1}=\bm{W}_{t}-\epsilon_{t}\bm{M}_{t},&\bm{M}_{t}= (1-\beta)\nabla L(\bm{W}_{t})+\beta\bm{M}_{t-1},\\ \text{Lion-$\mathcal{K}$ \@@cite[cite]{[\@@bibref{}{Lion-K}{}{}]}}:&\bm{W}_{t+1}=\bm{W}_{t}- \epsilon_{t}\nabla\mathcal{K}(\bm{N}_{t}),&\bm{N}_{t}=(1-\beta_{1})\nabla L( \bm{W}_{t})+\beta_{1}\bm{M}_{t}\\ \text{Adam \@@cite[cite]{[\@@bibref{}{Lion-K}{}{}]}}:&\bm{M}_{t}=(1-\beta_{2}) \nabla L(\bm{W}_{t})+\beta_{2}\bm{M}_{t-1},\\ \text{Adam \@@cite[cite]{[\@@bibref{}{Lion-K}{}{}]}}:&\bm{W}_{t+1}=\bm{W}_{t}- \epsilon_{t}\frac{\bm{M}_{t}}{\sqrt{\bm{V}_{t}+e}},&\bm{M}_{t}=(1-\beta_{1t}) \nabla L(\bm{W}_{t})+\beta_{1t}\bm{M}_{t-1},\\ &\bm{V}_{t}=(1-\beta_{2t})\nabla L(\bm{W}_{t})^{\odot 2}+\beta_{2t}\bm{V}_{t-1}, \end{array}\]

_where \(\epsilon_{t}\) are step sizes, and \(\bm{M}_{t},\bm{V}_{t}\) are the first and second order momentum, and \(\beta,\beta_{1},\beta_{2}\) are momentum coefficients in \((0,1)\), with \(\beta_{it}=\frac{\beta_{i}-\beta_{i}^{t+1}}{1-\beta_{i}^{t+1}}\), \(i=1,2\) for \(\beta_{1},\beta_{2}\in(0,1)\) in Adam, and \(\mathcal{K}\) is any convex function with \(\nabla K(\bm{0})=\bm{0}\) for Lion-\(\mathcal{K}\)[4], and Lion [5] uses \(\mathcal{K}(\bm{X})=\left\|\bm{X}\right\|_{1,1}\) and \(\nabla\mathcal{K}(\bm{X})=\operatorname{sign}(\bm{X})\)._These optimizers can be unifiedly viewed as updating \(\bm{W}_{t}\) together with an optimizer state \(\bm{S}_{t}\):

\[\bm{W}_{t+1}=\bm{W}_{t}+\phi_{t}(\bm{S}_{t}), \bm{S}_{t}=\psi_{t}(\bm{S}_{t-1},\nabla L(\bm{W}_{t})),\] (1)

with some mapping \(\phi_{t},\psi_{t}\). We have \(\bm{S}_{t}=\bm{M}_{t}\) for momentum and \(\bm{S}_{t}=\{\bm{M}_{t},\bm{V}_{t}\}\) for Adam. Note that both \(\bm{M}_{t},\bm{V}_{t}\) are of the same size as the model weights \(\bm{W}_{t}\), resulting in high memory consumption for large models. This issue is particularly pronounced for Adam, which typically yields the best performance for large language models (LLMs) but incurs the highest memory cost due to the need to maintain both \(\bm{M}_{t}\) and \(\bm{V}_{t}\). One key challenge is to retain the high performance of Adam while enhancing its memory efficiency.

Hamiltonian+DescentOne powerful approach to studying the dynamic properties of optimizers is to examine their continuous-time ODE forms in the limit of infinitesimal step size. The continuous-time forms provide clearer insights into the asymptotic convergence of the algorithm, abstracting away the choices of step size, discretization, and stochastic errors. The underlying logic is that a "sound" optimizer should be guaranteed to converge to local optima of the loss, at least when using sufficiently small step sizes.

Inspired by [4, 18], we observe that the continuous-time form of many common optimizers yields a _Hamiltonian+Descent_ structure,

\[\begin{split}\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=\partial_{ \bm{S}}H(\bm{W}_{t},\bm{S}_{t})-\Phi(\partial_{\bm{W}}H(\bm{W}_{t},\bm{S}_{t})) \\ \frac{\mathrm{d}}{\mathrm{d}t}\bm{S}_{t}=-\partial_{\bm{W}}H(\bm {W}_{t},\bm{S}_{t})-\Psi(\partial_{\bm{S}}H(\bm{W}_{t},\bm{S}_{t})),\end{split}\] (2)

where \(H(\bm{W},\bm{S})\) is a Hamiltonian (or Lyapunov) function that satisfies

\[\min_{\bm{S}}H(\bm{W},\bm{S})=L(\bm{W}),\quad\forall\bm{W},\]

so that minimizing \(L(\bm{W})\) reduces to minimizing \(H(\bm{W},\bm{S})\); and \(\Phi(\cdot),\Psi(\cdot)\) are two monotonic mappings satisfying

\[\|\bm{X}\|_{\Phi}^{2}\coloneqq\langle\bm{X},\Phi(\bm{X})\rangle\geq 0, \|\bm{X}\|_{\Psi}^{2}\coloneqq\langle\bm{X},\Psi(\bm{X})\rangle\geq 0, \forall\bm{X}.\]

With \(\Phi(\bm{X})=\Psi(\bm{X})=0\), the system in (2) reduces to the standard Hamiltonian system that keeps \(H(\bm{W}_{t},\bm{S}_{t})=const\) along the trajectory. When adding the descending components with \(\Phi\) and \(\Psi\), the system then keeps \(H(\bm{W},\bm{S})\) monotonically non-decreasing:

\[\begin{split}\frac{\mathrm{d}}{\mathrm{d}t}H(\bm{W}_{t},\bm{S}_{t })&=\left\langle\partial_{\bm{W}}H_{t},\frac{\mathrm{d}}{\mathrm{ d}t}\bm{W}_{t}\right\rangle+\left\langle\partial_{\bm{S}}H_{t},\frac{\mathrm{d}}{ \mathrm{d}t}\bm{S}_{t}\right\rangle\\ &=\left\langle\partial_{\bm{W}}H_{t},\partial_{\bm{S}}H_{t}-\Phi( \partial_{\bm{W}}H_{t})\right\rangle+\left\langle\partial_{\bm{S}}H_{t},- \partial_{\bm{W}}H_{t}-\Psi(\partial_{\bm{S}}H_{t})\right\rangle\\ &=-\left\|\partial_{\bm{W}}H_{t}\right\|_{\Psi}^{2}-\left\| \partial_{\bm{S}}H_{t}\right\|_{\Psi}^{2}\leq 0,\end{split}\] (3)

where we write \(\partial_{\bm{W}}H_{t}=\partial_{\bm{W}}H(\bm{W}_{t},\bm{S}_{t})\) and similarly for \(\partial_{\bm{S}}H_{t}\). The main idea is that the cross terms \(\left\langle\partial_{\bm{W}}H_{t},\partial_{\bm{S}}H_{t}\right\rangle\) are canceled, leaving only the negative terms.

**Example 2.2**.: _The momentum method yields following continuous-time form and Hamiltonian:_

\[\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=-\bm{M}_{t},\qquad\frac{\mathrm{d}}{ \mathrm{d}t}\bm{M}_{t}=a(\nabla L(\bm{W}_{t})-\bm{M}_{t}),\qquad\quad\text{with }\quad H(\bm{W},\bm{M})=L(\bm{W})+\frac{\|\bm{M}\|^{2}}{2a}.\]

**Example 2.3**.: _Adam [12] yields the following continuous-time form and Hamiltonian,_

\[\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=-\frac{\bm{M}_{t}}{\sqrt{\bm{V}_{t}+ e}},\quad\frac{\mathrm{d}}{\mathrm{d}t}\bm{M}_{t}=a(\nabla L(\bm{W}_{t})-\bm{M}_{t}), \quad\frac{\mathrm{d}}{\mathrm{d}t}\bm{V}_{t}=b(\nabla L(\bm{W}_{t})^{\odot 2}- \bm{V}_{t}),\]

\[\text{with }\quad H(\bm{W},\bm{M},\bm{V})=L(\bm{W})+\frac{1}{2a}\left\langle \frac{\bm{M}}{\sqrt{\bm{V}+e}},\ \bm{M}\right\rangle,\]

_for which we can show that \(\frac{\mathrm{d}}{\mathrm{d}t}H(\bm{W}_{t},\bm{M}_{t},\bm{V}_{t})\leq 0\) when \(a\geq b/4\)._

**Example 2.4**.: _The Lion-\(\mathcal{K}\) optimizer [5, 4] (without weight decay) can be written into_

\[\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=\nabla\mathcal{K}((1-b)\bm{M}_{t}-b \nabla L(\bm{W}_{t})),\qquad\quad\frac{\mathrm{d}}{\mathrm{d}t}\bm{M}_{t}=-a( \nabla L(\bm{W}_{t})+\bm{M}_{t}),\]

_where \(a\geq 0,\,b\in[0,1]\) and \(\mathcal{K}(\bm{X})\) is any convex function that attains the minimum at \(\bm{X}=0\). One of its Hamiltonians that yields the Hamiltonian+descent structure (Eq (13) in Chen et al. [4]) is_

\[H(\bm{W},\bm{M})=aL(\bm{W})+\frac{1}{1-b}\mathcal{K}((1-b)\bm{M}).\]Memory-Efficient Optimizers via Online Subspace Descent

We introduce the idea of constructing memory efficient optimzers by descending in the subspaces that dynamically changes across iterations as motivated by GaLore [25] and Sketchy [9]. We first derive _static_ subspace descent by restricting the whole optimization on a subspace (Section 3.1), and then propose to dynamically change the subspace across iterations as a heuristic to attain the optimization in the full space while only using subspace descent (Section 3.2). In particular, we propose to update the subspaces via continuous online PCA like updates to avoids the need of exact SVD like in GaLore and Sketchy (Section 3.2). Finally, we remark in Section 3.3 the heuristic nature of the derivation of the method and highlight the difficulty in theoretical understanding, which motivates our analysis based on Hamiltonian dynamics in Section 4.

### Static Subspace Descent

One popular approach to improving memory efficiency is to confine the optimization to a low-dimensional space. To do this, we impose a low rank structure of \(\bm{W}=\bm{P}\bm{\hat{W}}\), where \(\bm{P}\in\mathbb{R}^{n\times k}\) is a projection matrix to be determined later and \(\bm{\hat{W}}\in\mathbb{R}^{k\times m}\) is a dimension-reduced parameter. When \(k\ll n\), \(\bm{P}\) and \(\bm{\hat{W}}\) are much smaller in size compared to \(\bm{W}\). Now consider

\[\min_{\bm{\hat{W}}}L(\bm{P}\bm{\hat{W}}).\]

Applying the optimizer from (1) to update \(\bm{\hat{W}}\) along with an optimizer state \(\bm{\hat{S}}\), and mapping the update rule \(\bm{\hat{W}}_{t+1}=\bm{\hat{W}}_{t}+\phi_{t}(\bm{\hat{S}}_{t})\) to that of \(\bm{W}=\bm{P}\bm{\hat{W}}_{t}\), we get

\[\bm{W}_{t+1}=\bm{W}_{t}+\bm{P}\phi_{t}(\bm{\hat{S}}_{t}), \bm{\hat{S}}_{t}=\psi_{t}(\bm{\hat{S}}_{t-1},\bm{P}^{\top}\nabla L(\bm{W}_{t})),\] (4)

where we used the fact that \(\nabla_{\bm{W}}L(\bm{P}\bm{\hat{W}})=\bm{P}^{\top}\nabla_{\bm{W}}L(\bm{W})\). This yields a more memory-efficient optimizer, as the size of \(\bm{\hat{S}}_{t}\) is proportional to that of \(\hat{W}_{t}\), much smaller than \(\bm{S}_{t}\) in (1) when \(k\ll n\).

### Online Subspace Descent

With a static \(\bm{P}\), regardless of its values, the parameter \(\bm{W}\) is restricted to have a low rank structure. Although low rank assumption is proved to be useful for fine-tuning with LoRA-like methods [11], it is often too limited for pre-training or when the desirable model weights are not inherently low-rank.

To address this problem, Zhao et al. [25] suggested to keep the projected updated in (4), but use different \(\bm{P}\) across the iterations:

\[\bm{W}_{t+1}=\bm{W}_{t}+\bm{P}_{t}\phi_{t}(\bm{\hat{S}}_{t}), \bm{\hat{S}}_{t}=\psi_{t}(\bm{\hat{S}}_{t-1},\bm{P}_{t}^{\top}\nabla L(\bm{W}_ {t})), \bm{P}_{t+1}=\chi_{t}(\bm{P}_{t},\bm{W}_{t},\bm{\hat{S}}_{t}),\]

where \(\chi_{t}\) is a update rule of \(\bm{P}_{t}\) that will be determined in the sequel. The intuition is to open up different projection directions at different iterations, so that optimization can be conducted in different subspaces across different iterations. This is similar to the update of coordinate descent, except in a continuous fashion. Note that the update of \(\bm{P}_{t}\) can be done in parallel with that of \((\bm{W}_{t},\bm{\hat{S}}_{t})\), and incurs no slowdown once it is fast enough to not cause a speed bottleneck.

**Example 3.1**.: _Examples of common optimizers equipped with online subspace descent:_

\[\begin{array}{llll}\text{Gradient Descent}:&\bm{W}_{t+1}=\bm{W}_{t}- \epsilon_{t}\bm{P}_{t}\bm{P}_{t}^{\top}\bm{G}_{t},&\bm{G}_{t}=\nabla L(\bm{W} _{t}),\\ \text{Momentum}:&\bm{W}_{t+1}=\bm{W}_{t}-\epsilon_{t}\bm{P}_{t}\bm{\hat{M}}_{ t},&\bm{\hat{M}}_{t}=(1-\beta)\bm{P}_{t}^{\top}\bm{G}_{t}+\beta\bm{\hat{M}}_{t-1}, \\ \text{Lion-K}:&\bm{W}_{t+1}=\bm{W}_{t}-\epsilon_{t}\bm{P}_{t}\nabla\mathcal{K}( \bm{\hat{N}}_{t}),&\bm{\hat{G}}_{t}=\bm{P}_{t}^{\top}\bm{G}_{t}\\ &\bm{\hat{N}}_{t}=(1-\beta_{1})\bm{\hat{G}}_{t}+\beta_{1}\bm{\hat{M}}_{t},&\bm{ \hat{M}}_{t}=(1-\beta_{2})\bm{\hat{G}}_{t}+\beta_{2}\bm{\hat{M}}_{t-1},\\ \text{Adam}:&\bm{W}_{t+1}=\bm{W}_{t}-\epsilon_{t}\bm{P}_{t}\frac{\bm{\hat{M}}_{ t}}{\sqrt{\bm{V}_{t}+e}},&\bm{\hat{G}}_{t}=\bm{P}_{t}^{\top}\bm{G}_{t},\\ &\bm{\hat{M}}_{t}=(1-\beta_{1t})\bm{\hat{G}}_{t}+\beta_{1t}\bm{\hat{M}}_{t-1},& \bm{\hat{V}}_{t}=(1-\beta_{2t})\bm{\hat{G}}_{t}^{\odot 2}+\beta_{2t}\bm{\hat{V}}_{t-1}. \end{array}\]

How Should \(\bm{P}_{t}\) be Updated? It is useful to draw intuition from the projected gradient descent rule

\[\bm{W}_{t+1}=\bm{W}_{t}-\epsilon_{t}\bm{P}_{t}\bm{P}_{t}^{\top}\bm{G}_{t}, \bm{G}_{t}=\nabla L(\bm{W}_{t}),\] (5)in which \(\bm{P}_{t}\bm{P}_{t}^{\top}\) can be viewed as a low rank preconditioning of \(\bm{G}_{t}\). To make it follow the exact gradient descent, we hope to make \(\bm{P}_{t}\bm{P}_{t}^{\top}\bm{G}_{t}\) approximate \(\bm{G}_{t}\) as much as possible. In Galore, this is achieved by performing singular value decomposition (SVD) on \(\bm{G}_{t}\) periodically every \(T\) iterations:

\[\bm{P}_{t},\ \underline{\ },\ \underline{\ }=\texttt{torch.linalg.svd}(\bm{G}_{T\lfloor t/T \rfloor}),\]

where \(T\lfloor t/T\rfloor\) is the largest multiple of \(T\) less than or equal to \(t\). However, numerical SVD incurs a large computational cost for very large models. Also, since \(\bm{P}_{t}\) is fully determined by \(\bm{G}_{T\lfloor t/T\rfloor}\) calculated from a single mini-batch at the last periodic point, it does not incorporate the gradient information from all data in a timely fashion.

In this work, we propose to update \(\bm{P}_{t}\) in a continuous online fashion that incorporates the most recent gradient information in a timely fashion, without calling torch.linalg.decompositions routines. We view the update of \(\bm{P}_{t}\) as conducting an online principal component analysis (PCA) based on the streaming of \(\{\bm{G}_{t}\}\). In particular, we propose to update \(\bm{P}_{t}\) at time \(t\) by minimizing the following PCA objective:

\[L_{\bm{G}_{t}}(\bm{P})=\left\|\bm{P}\bm{P}^{\top}\bm{\tilde{G}}_{t}-\bm{ \tilde{G}}_{t}\right\|^{2}+\lambda\left\|\bm{P}^{\top}\bm{P}-\bm{I}_{k\times k }\right\|^{2},\quad\bm{\tilde{G}}_{t}=\frac{\bm{G}_{t}}{\left\|\bm{G}_{t} \right\|},\] (6)

where \(\left\|\bm{A}\right\|=\operatorname{tr}(\bm{A}^{\top}\bm{A})^{1/2}\) and \(\bm{I}_{k\times k}\) is the \(k\times k\) identity matrix; we introduced an auxiliary loss to encourage the columns of \(\bm{P}\) to be orthonormal and normalizes \(\bm{G}_{t}\) to increase stability.

The key property of \(L_{\bm{G}_{t}}(\bm{P})\) in (6) is that all its stable local minimum is a global minimum, and \(\bm{P}\) is a global minimum iff \(\bm{P}\bm{P}^{\top}\bm{\tilde{G}}_{t}\) forms the optimal rank-\(k\) approximation of \(\bm{\tilde{G}}_{t}\) [e.g., 3]; moreover, we have \(\bm{P}^{\top}\bm{P}=I_{k\times k}\) at optima when \(\lambda>0\).

Instead of minimizing \(L_{\bm{G}_{t}}(\bm{P})\) exactly, to retain computational efficiency, we propose to update \(\bm{P}_{t}\) by only performing one step of optimization on \(L_{\bm{G}_{t}}(\bm{P})\):

\[\bm{P}_{t+1}=\texttt{OptimizerP.step}(\bm{P}_{t},\ \ \nabla_{\bm{P}}L_{\bm{G}_{t}}( \bm{P}_{t})),\]

where OptimizerP.step can be a favorite optimizer, such as gradient descent or Adam. Note that when using Adam, we introduce a copy of optimizer state \(\bm{S}_{t}^{P}\) for \(\bm{P}_{t}\). See Algorithm 1. Compared to the exact SVD, each online update of \(\bm{P}_{t}\) here is fast and can be executed in parallel with the \((\bm{W}_{t},\bm{\hat{S}}_{t})\) updates to avoid slowdown.

### Difficulty in Theoretical Understanding

The idea above of projecting an arbitrary optimizer with a dynamically changing \(\bm{P}_{t}\) is heuristically motivated and lacks an immediate rigorous theoretical justification. The main challenge lies in the complex interaction between the update of \(\bm{U}_{t}\) and the optimization state \(\bm{S}_{t}\), which could potentially degrade the convergence and other theoretical properties of the original optimizer. A key question is whether we can develop a theoretical framework to understand how \(\bm{P}_{t}\) impacts the optimizer's convergence behavior and provide guidance for the design of the update rules of \(\bm{P}_{t}\).

To gain understanding, it is useful to first exam the simple case of projected gradient descent in (5) which does not have an optimizer state (\(\bm{S}_{t}=\emptyset\)). In this case, since \(\bm{P}_{t}\bm{P}_{t}^{\top}\) is positive semi-finite, the update \(\bm{P}_{t}\bm{P}_{t}^{\top}\bm{G}_{t}\) is always non-increasing direction of \(L(\bm{W})\) for any \(\bm{P}_{t}\). The algorithm is essentially a variant of coordinate or subspace descent, where \(\bm{P}_{t}\) defines the subspace on which one step of gradient descent is conduced at iteration \(t\). To ensure that (5) finds a local optimum, we mainly need to ensure that \(\bm{P}_{t}\bm{G}_{t}=0\) only if \(\bm{G}_{t}=0\) to prevent the optimizer from stopping prematurely; this is a mild condition that can be satisfied e.g. when \(\bm{P}_{t}\) is updated by (online) PCA on \(\bm{G}_{t}\).

Unfortunately, this coordinate-descent-like interpretation does not apply to more advanced optimizers that track a momentum state \(\bm{S}_{t}\). This is because \(\bm{S}_{t}\) accumulates the information from the projected gradient \(\bm{P}_{\tau}\bm{G}_{\tau}\) at all earlier iterations \(\tau\leq t\). As \(\bm{P}_{t}\) changes across time, it is unclear whether the gradient projected to different subspaces \(\bm{P}_{\tau}\) would be coherent with each other, and useful for future updates that are conducted in different subspaces \(\bm{P}_{t}\) for \(t>\tau\). The difficulty is the inertia effect of \(\bm{S}_{t}\) that entangles the different subspaces, making the dynamic behavior fundamentally more complicated than naive coordinate descent where the descent in different subspaces is uncoupled. This is what we address in Section 4 via the Hamiltonian descent framework.

Hamiltonian Descent Meets Subspace Descent: A Lyapunov Analysis

In this section, we show a surprising result that the complication outlined above in Section 3.3_is not_ a problem for optimizers that yields the Hamiltonian+descent structure in (2). Our result is two-fold:

\(\bullet\) Section 4.1: When applying Online Subspace Descent on systems in (2), the Hamiltonian+descent structure is preserved once the update rule of \(\bm{P}_{t}\) has a smooth continuous-time limit. Hence, under very mild conditions, Online Subspace Descent equipped with common optimizers like Adam and Lion automatically yield a Lyapunov function and hence benign continuous-time convergence. Moreover, \(\bm{P}_{t}\) can, in fact, be generalized to an arbitrary linear operator as shown in Section 4.3.

\(\bullet\) Section 4.2: For any smooth \(\bm{P}_{t}\) update rules that eliminates the degenerate case of \(\bm{P}_{t}^{\top}\bm{G}_{t}=0\) while \(\bm{G}_{t}=0\) at convergence, the online subspace optimizer guarantees to converge in continuous time to a stationary point of the loss \(L(\bm{W})\). This mild condition is satisfied, for example, when \(\bm{P}_{t}\) is updated by a typical optimizer on the online PCA objective \(L_{\bm{G}_{t}}(\bm{P})\).

### Online Subspace Descent Preserves the Hamiltonian+Descent Structure

Applying dynamic projection to Hamiltonian descent in (2), we obtain the following systems:

\[\begin{split}&\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=\bm{P}_{t} \partial_{\hat{\bm{S}}}H(\bm{W}_{t},\hat{\bm{S}}_{t})-\Phi(\partial_{\bm{W}}H (\bm{W}_{t},\hat{\bm{S}}_{t}))\\ &\frac{\mathrm{d}}{\mathrm{d}t}\hat{\bm{S}}_{t}=-\bm{P}_{t}^{ \top}\partial_{\bm{W}}H(\bm{W}_{t},\hat{\bm{S}}_{t})-\Psi(\partial_{\hat{\bm {S}}}H(\bm{W}_{t},\hat{\bm{S}}_{t}))\\ &\frac{\mathrm{d}}{\mathrm{d}t}\bm{P}_{t}=\Gamma(\bm{P}_{t}, \nabla L(\bm{W}_{t})),\end{split}\] (7)

where \(\Gamma\) specifies the update rule of \(\bm{P}_{t}\). Following essentially the same derivation as (3), one can show that \(H(\bm{W},\bm{S})\) remains a Lyapunov function of (7), regardless of the choice of \(\Gamma\):

\[\begin{split}\frac{\mathrm{d}}{\mathrm{d}t}H(\bm{W}_{t},\hat{ \bm{S}}_{t})&=-\left\|\partial_{\bm{W}}H_{t}\right\|_{\Phi}^{2} -\left\|\partial_{\bm{S}}H_{t}\right\|_{\Psi}^{2}+\left\langle\partial_{\bm{ W}}H_{t},\bm{P}_{t}\partial_{\hat{\bm{S}}}H_{t}\right\rangle-\left\langle \partial_{\hat{\bm{S}}}H_{t},\bm{P}_{t}^{\top}\partial_{\bm{W}}H_{t}\right\rangle \\ &=-\left\|\partial_{\bm{W}}H_{t}\right\|_{\Phi}^{2}-\left\| \partial_{\bm{S}}\bm{H}_{t}\right\|_{\Psi}^{2}\leq 0,\end{split}\] (8)

where the key is to use the _adjoint_ property of \(\bm{P}\) and \(\bm{P}^{\top}\) that \(\langle\bm{P}_{t}\bm{X},\bm{Y}\rangle=\langle\bm{X},\bm{P}_{t}^{\top}\bm{Y}\rangle\), which cancels the crossing terms, independent of the values of \(\bm{P}_{t}\). There is no requirement on \(\bm{\Gamma}\) here, besides that the derivative in (8) should exist. As shown in Section 4.3, we can generalize (8) by replacing \(\bm{P}_{t}\) and \(\bm{P}_{t}^{\top}\) with a general linear operator \(\mathcal{P}_{t}\) and its adjoint \(\mathcal{P}_{t}^{*}\).

Please refer to Appendix A for continuous-time Momentum, Lion-\(\mathcal{K}\) and Adam with subspace descent and their Hamiltonian functions.

### Convergence to Local Optima

In addition to the Lyapunov structure, we need an additional mild condition on the update rule of \(\bm{P}_{t}\) to ensure the system converges to the local optimum of the loss \(L(\bm{W})\). The main idea is to prevent the system from stopping prematurely before reaching zero gradient \(\bm{G}_{t}=0\) by excluding the degenerate case of \(\bm{P}_{t}\bm{G}_{t}=0\) while \(\bm{G}_{t}\neq 0\) in the invariant set of the system.

**Assumption 4.1**.: _Assume the functions in system (7) are continuously differentiable and_

_i) \(\frac{\mathrm{d}}{\mathrm{d}t}H(\bm{W}_{t},\hat{\bm{S}}_{t})=0\) implies \(\hat{\bm{G}}_{t}=\bm{P}_{t}^{\top}\nabla L(\bm{W}_{t})=0\) and \(\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=0\)._

_ii) When \(\bm{G}_{t}\equiv\bm{G}\neq 0\), the set \(\{\bm{P}\colon\bm{P}^{\top}\bm{G}=0\}\) is not a positive invariant set of \(\frac{\mathrm{d}}{\mathrm{d}t}\bm{P}_{t}=\Gamma(\bm{P}_{t},\bm{G}_{t})\)._

This is a mild condition. Assumption i) says that the optimizer should stop at a point with \(\hat{\bm{G}}_{t}=0\), which is easy to verify for the common optimizers like momentum, Adam, Lion-\(\mathcal{K}\). Assumption ii) ensures \(\hat{\bm{G}}_{t}=0\) would imply \(\bm{G}_{t}=0\) in invariance sets, which is satisfied when for example, \(\bm{P}_{t}\) is updated by a reasonable optimizer of the online PCA loss that converges to a stable local minimum.

**Theorem 4.2**.: _Assume Assumption 4.1 holds. Let \((\bm{W}_{t},\bm{S}_{t},\bm{P}_{t})_{t}\) be a bounded solution of (7), then all the accumulation points \(\{\bm{W}_{t}\}\) as \(t\to+\infty\) are stationary points of \(L(\bm{W})\)._

Proof.: By LaSalle's invariance principle, the positive limit set of \((\bm{W}_{t},\bm{S}_{t},\bm{P}_{t})_{t}\) must be contained in \(\mathcal{I}\), where \(\mathcal{I}=\{\text{the union of complete trajectories satisfying }\frac{\mathrm{d}}{\mathrm{d}t}H(\bm{W}_{t},\hat{\bm{S}}_{t})=0,\,\forall t\ \}\).

From the Assumption i), the trajectories contained in \(\mathcal{I}\) must satisfy \(\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=0\), which implies \(\frac{\mathrm{d}}{\mathrm{d}t}\bm{G}_{t}=\frac{\mathrm{d}}{\mathrm{d}t}\nabla L( \bm{W}_{t})=0\) and \(\bm{\hat{G}}_{t}=0\) and hence \(\bm{G}_{t}\equiv\bm{G}\) is a constant with \(\bm{P}_{t}^{\top}\bm{G}=0\). Moreover, from Assumption ii), we must have \(\nabla L(\bm{W}_{t})=\bm{G}_{t}\equiv 0\), since otherwise the trajectory is not invariant. As a result, all trajectories in the limit set \(\mathcal{I}\) must have \(\nabla L(\bm{W}_{t})=0\). Because \(\frac{\mathrm{d}}{\mathrm{d}t}W_{t}=0\), these trajectories are static points of \(\bm{W}_{t}\). 

### Online Subspace Descent with General Linear Projection Operators

We can generalize the online subspace descent with general linear operators:

\[\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=\mathcal{P}_{t}(\partial _{\bm{\hat{S}}}H(\bm{W}_{t},\bm{\hat{S}}_{t}))-\Phi(\partial_{\bm{W}}H(\bm{W}_ {t},\bm{\hat{S}}_{t}))\] \[\frac{\mathrm{d}}{\mathrm{d}t}\bm{\hat{S}}_{t}=-\mathcal{P}_{t}^ {*}(\partial_{\bm{W}}H(\bm{W}_{t},\bm{\hat{S}}_{t}))-\Psi(\partial_{\bm{\hat{S }}}H(\bm{W}_{t},\bm{\hat{S}}_{t}))\] \[\frac{\mathrm{d}}{\mathrm{d}t}\mathcal{P}_{t}=\Gamma(\mathcal{P}_ {t},\nabla L(\bm{W}_{t})),\]

where we generalize \(\bm{P}_{t}\) to be any linear operator \(\mathcal{P}_{t}\) with an adjoint operator \(\mathcal{P}_{t}^{*}\), satisfying

\[\langle\bm{X},\mathcal{P}_{t}(\bm{Y})\rangle=\langle\mathcal{P}_{t}^{*}(\bm{X }),\bm{Y}\rangle,\quad\forall\bm{X},\bm{Y}.\]

The derivation of Lyapunov follows a similar way:

\[\frac{\mathrm{d}}{\mathrm{d}t}H(\bm{W}_{t},\bm{\hat{S}}_{t}) =-\|\partial_{\bm{W}}H_{t}\|_{\Phi}^{2}-\|\partial_{\bm{S}}H_{t} \|_{\Psi}^{2}+\langle\partial_{\bm{W}}H_{t},\mathcal{P}_{t}(\partial_{\bm{ \hat{S}}}H_{t})\rangle-\langle\partial_{\bm{\hat{S}}}H_{t},\mathcal{P}_{t}^{*} (\partial_{\bm{W}}H_{t})\rangle\] \[=-\|\partial_{\bm{W}}H_{t}\|_{\Phi}^{2}-\|\partial_{\bm{S}}H_{t} \|_{\Psi}^{2}\leq 0,\]

where the crossing terms are again canceled due to the adjoint property.

As an example of the general framework, consider \(\mathcal{P}_{t}(\bm{X})=\bm{P}_{t}\bm{X}\bm{Q}_{t}\), where \(\bm{Q}_{t}\) is another projection matrix applied on the different dimension of \(\bm{X}\) (see also [25]). The adjoint operator of \(\mathcal{P}_{t}\) is \(\mathcal{P}_{t}^{*}(\bm{X})=\bm{P}_{t}^{\top}\bm{X}\bm{Q}_{t}^{\top}\). This can be verified by

\[\langle\bm{P}_{t}\bm{X}\bm{Q}_{t},\bm{Y}\rangle=\mathrm{tr}(\bm{P}_{t}\bm{X} \bm{Q}_{t}\bm{Y}^{\top})=\mathrm{tr}(\bm{X}\bm{Q}_{t}\bm{Y}^{\top}\bm{P}_{t})= \mathrm{tr}(\bm{X}(\bm{P}_{t}^{\top}\bm{Y}\bm{Q}_{t}^{\top})^{\top})=\langle \bm{X},\bm{P}_{t}^{\top}\bm{Y}\bm{Q}_{t}^{\top}\rangle.\]

The subspace descent system of this operator is

\[\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=\bm{P}_{t}\partial_{\bm{ \hat{S}}}H(\bm{W}_{t},\bm{\hat{S}}_{t})\bm{Q}_{t}-\Phi(\partial_{\bm{W}}H(\bm{ W}_{t},\bm{\hat{S}}_{t}))\] \[\frac{\mathrm{d}}{\mathrm{d}t}\bm{\hat{S}}_{t}=-\bm{P}_{t}^{\top} \partial_{\bm{W}}H(\bm{W}_{t},\bm{\hat{S}}_{t}))\bm{Q}_{t}^{\top}-\Psi( \partial_{\bm{\hat{S}}}H(\bm{W}_{t},\bm{\hat{S}}_{t}))\] \[\frac{\mathrm{d}}{\mathrm{d}t}\bm{P}_{t}=\Gamma_{P}(\bm{P}_{t}, \bm{Q}_{t},\nabla L(\bm{W}_{t}))\] \[\frac{\mathrm{d}}{\mathrm{d}t}\bm{Q}_{t}=\Gamma_{Q}(\bm{P}_{t}, \bm{Q}_{t},\nabla L(\bm{W}_{t})),\]

where \(\bm{P}_{t},\bm{Q}_{t}\) can be updated jointly via an online SVD on \(\bm{G}_{t}\).

Another linear operator that involves two matrices is \(\mathcal{P}_{t}(\bm{X})=\bm{P}_{t}\bm{X}+\bm{X}\bm{Q}_{t}\), which yields \(\mathcal{P}_{t}^{*}(\bm{X})=\bm{P}_{t}^{\top}\bm{X}+\bm{X}\bm{Q}_{t}^{\top}\).

## 5 Experiment

We answer a number of key questions with pretraining experiments of LLaMA [22] on the C4 dataset [20]. All experiments except for large 7B experiments are conducted on a _single_ NVIDIA A100 GPU.

### Why do we Need Online Subspace Descent?

Overall, Online Subspace Descent offers two major advantages over previous methods that rely on SVD, better convergence and lower overhead. In this section, we discuss both in detail.

First, Online Subspace Descent closes the gap between the state-of-the-art low-rank method and full rank baseline uniformly across different model sizes, as shown in figure 1. A highlight amongst these results is LLaMA 1B (SS 256). As shown in table 1, Online Subspace Descent attains significant improvement over GaLore in perplexity, while consuming a similar amount of GPU memory (8.64 GB v.s 9.01 GB). One additional observation in 1 shows as model size and sequence length grow, Online Subspace Descent becomes more effective. We hypothesize that this is due to the higher intrinsic rank of the underlying optimization problem in larger models. Hence, the positive impact on the convergence of the online update of \(\bm{P}_{t}\) becomes more obvious. See more details in Appendix B.

Another favorable characteristic of Online Subspace Descent is its minimum overhead. In figure 2, we measure and analyze the execution time of SVD and online PCA on a popular data center GPU (A100) and a consumer GPU (RTX 3090). The typical Pytorch implementation of SVD can be up to \(142\) times slower than running a single-step online PCA on representative weight tensors from LLaMA architectures. Online PCA is fast because it is implemented as a single optimization step with respect to a simple loss function. Hence, each step of online PCA can be cleverly scheduled and hidden in the weight optimization step when executed in parallel, whereas SVD is too expensive to be hidden.

### What Rank Should we Pick for _Online Subspace Descent_?

We conduct an ablation study on the rank of Online Subspace Descent. Figure 3 shows that the final perplexity is inversely correlated with rank: higher ranks result in lower convergent perplexity. However, the rate of reduction of perplexity decreases as the rank increases, eventually reaching a saturation point. We propose an intuitive explanation for this phenomenon. In language modeling, high-frequency tokens can be effectively learned with low-rank training. However, learning lower-frequency tokens requires higher ranks. Once these lower-frequency tokens are adequately learned, further increasing the rank does not significantly decrease perplexity. In conclusion, given sufficient time and resources, higher ranks yield better performance for Online Subspace Descent. It is recommended that the highest rank be selected until the perplexity reduction saturates.

### What are the Best Hyperparameters?

\(\alpha\)**and**\(\lambda\): The parameter \(\alpha\) controls the update speed of \(\bm{P}_{t}\), while \(\lambda\) determines the regularization strength on the optimization objective of \(\bm{P}_{t}\). Empirically, we find that the result is not sensitive to \(\lambda\)

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{**Perplexity(\(\downarrow\))**} \\ \cline{2-4}  & **60M** & **350M** & **1B** \\ \hline
8bit-AdamW (Full Rank) & 32.75 & 30.43 & 29.40 \\ \hline GaLore (Rank = 512) & 57.03 & 44.34 & 35.52 \\
**Ours** (Rank = 512) & **56.12** & **43.67** & **31.30** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Pretraining LLaMA 1B with a sequence length of 256 and for 10K steps, perplexity was reported as the training average of the last 10 steps. AdamW8bit serves as the base optimizer for both.

Figure 2: The execution time of torch.svd and that a single-step backward() call for online PCA in PyTorch, on matrices of typical shapes in linear layers in the LLaMA 60M to 7B. Thanks to the high speed of single-step online PCA, \(\bm{P}_{t}\) updates can be executed in parallel with weight updates, adding no overhead to the training process. In contrast, SVD incurs significant overhead as the model and weight tensor sizes increase.

for small models (60M). and set \(\lambda=0.1\) for all subsequent experiments. We find that \(\alpha\) must be kept small to avoid instability (Figure 3), and we set \(\alpha=5\) for all experiments.

**Learning rate**: For the small model (60M), learning rate choices are more flexible, producing similar results. However, for larger models (350M, 1B), we recommend using a learning rate that is 10 times smaller, specifically 0.001. Larger learning rates cause unrecoverable spikes and instability, a general characteristic observed across all methods. See additional hyperparameter choices in Appendix B.

### Can _Online Subspace Descent_ be Applied to Different Optimizers?

One straightforward extension of Online Subspace Descent is to apply it to other base optimizers beyond AdamW8bit. We conduct ablation studies on LION [6] and Adafactor [21], finding that Online Subspace Descent behaves similarly to how it does with AdamW8bit. Despite the initial observation that updating \(\bm{P}_{t}\) with AdamW8bit consistently yields better results, we discover that updating \(P_{t}\) with simple SGD can achieve similar performance.

### Can Online Subspace Descent Scale to Larger Model?

We pretrain from scratch a 7B LLaMA model on the C4 dataset for 10K steps, where the \(\bm{P}_{t}\) matrix is updated by SGD. The perplexity is the lower the better. The final perplexity and training wall-clock time are provided in Table 3. We further provide the downstream evaluation of the pretrained checkpoints using Galore and our method on the GLUE benchmark in Table 4. Our method consistently outperforms Galore when the model size scales up.

## 6 Related Works

We discuss related works on memory-efficient optimization and low-rank adaptation techniques.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Method** & \multicolumn{2}{c}{GaLore} & \multicolumn{4}{c}{Ours} \\ \cline{2-6}  & Lion & Adaf. & Lion+Lion & Adaf.+Adaf. & Lion+AdamW & Adaf.+AdamW \\ \hline
**Perplexity** & 46.90 & 34.32 & 57.97 & 47.61 & **44.76** & **34.15** \\ \hline \hline \end{tabular}
\end{table}
Table 2: LLaMA 60M on C4 with sequence length 1024, with optimizers on \(\bm{P}_{t}\) and \(\bm{W}_{t}\), denote as “Ours \(\{\bm{W}_{t}\) optimizer\(\}\) + \(\{\bm{P}_{t}\) optimizer\(\}\)”. Adaf., and Adam refer to Adafactor and 8bit-AdamW, respectively.

Figure 3: From left to right are loss curves of 10K steps on LLaMA 60M: leftmost is the sweep of rank, middle is the sweep of \(\alpha\) and rightmost is the sweep of \(\lambda\).

Low-Rank AdaptationLow-Rank Adaptation (LoRA) [11] adds a low-rank adaptor to spefic linear layers in a model, and finetune only the low-rank adaptor. As the adaptors are small, LoRA is widely applied for finetuning large models. Many variants have been proposed since LoRA, including support for multi-task learning Wang et al. [23] and further memory reductions Dettmers et al. [8]. Notably, Lialin et al. [15] proposed ReLoRA for pretraining, requiring a full-rank training warmup to match standard performance levels. It's important to note that LoRA is fundamentally distinct from subspace descent. While subspace descent optimizes within the original model parameter space, LoRA focuses its optimization efforts within the space of the adaptors.

Memory-Efficient OptimizationSeveral approaches aim to reduce memory costs associated with gradient statistics in adaptive optimization algorithms [21; 2; 7]. In particular, Adafactor [21] factorizes the second-order statistics by a row-column outer product and update the factorized bases on the fly, hence achieving a sub-linear memory cost. K-Fac [19] presents a factorized approximation of the Fisher information matrix which leads to a sublinear natural gradient method. More recently, Feinberg et al. [9] observes that the spectra of the Kronecker-factored gradient covariance matrix in deep learning (DL) training tasks are concentrated on a small leading eigenspace and propose to maintain a matrix preconditioner using the frequent directions sketch. However, their method requires conducting the eigendecomposition at every step, which can be costly for large models. Other than factorization methods, quantization techniques [7; 1; 24; 16] are also widely used, where the gradient (or the momentum and the preconditioner) are directly quantized to tradeoff performance for memory. Fused gradient computation method [17] have also been used to minimize memory costs during training. GaLore [25] is the most relevant work to ours. GaLore focuses on low-rank gradient structures, reducing memory costs for both first and second-order statistics. Our method can be viewed as a general extension to GaLore where we replace the infrequent SVD by a continuous subspace descent [14; 10]. As a result, our method not only provides a more general framework to study memory-efficient subspace descent, but is also more performant than GaLore in practice.

## 7 Conclusion

In conclusion, we provide the first convergence guarantee for arbitrary update rules of projection matrix, applicable to a range of optimizers that can be analyzed using Hamiltonian Descent, including common ones like LION, AdamW, and Adafactor. Inspired by this theoretical foundation, we introduce Dynamic Subspace Descent, a novel family of subspace descent optimizers that eschews SVD in favor of online PCA for updating projection matrix. Dynamic Subspace Descent is both flexible and minimally intrusive, and our experiments show that it achieves lower perplexity in pretraining LLaMA models (ranging from 60M to 1B parameters) on the C4 dataset compared to state-of-the-art low-rank training methods, while also closing the perplexity gap with full-rank baselines.

For future research, we propose several open and intriguing questions: (1) Are there alternative methods for updating projection matrix that could accelerate convergence? (2) What is the impact of weight decay on convergence in Dynamic Subspace Descent? (3) Can low-rank gradients and updates be combined with dynamic low-rank weights (e.g., Mixture of Experts) to further enhance training efficiency? (4) Can this method be applied to problems beyond language modeling? We hope that our work provides a strong foundation for exploring these questions.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Method** & **MRPC** & **RTE** & **SST2** & **MNLI** & **QNLI** & **QQP** & **AVG** \\ \hline Galore & 0.6838 & **0.5018** & 0.5183 & 0.3506 & 0.4946 & 0.3682 & 0.4862 \\ Ours & **0.6982** & 0.4901 & **0.5233** & **0.3654** & **0.5142** & **0.3795** & **0.4951** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Standardized GLUE evaluation for 7B model checkpoints using eval-harness. Results are reported for various downstream tasks.

Acknowledgment

The research is conducted in Statistics & AI group at UT Austin, which receives supports in part from NSF CAREER1846421, SenSE2037267, Office of Navy Research, and NSF AI Institute for Foundations of Machine Learning (IFML).

## References

* [1] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communication-efficient sgd via gradient quantization and encoding. _Advances in neural information processing systems_, 30, 2017.
* [2] Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive optimization. _Advances in Neural Information Processing Systems_, 32, 2019.
* [3] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. _Neural networks_, 2(1):53-58, 1989.
* [4] Lizhang Chen, Bo Liu, Kaizhao Liang, and Qiang Liu. Lion secretly solves constrained optimization: As lyapunov predicts. _arXiv preprint arXiv:2310.05898_, 2023.
* [5] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. _arXiv preprint arXiv:2302.06675_, 2023.
* [6] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. _Advances in Neural Information Processing Systems_, 36, 2024.
* [7] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. _arXiv preprint arXiv:2110.02861_, 2021.
* [8] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. _Advances in Neural Information Processing Systems_, 36, 2024.
* [9] Vladimir Feinberg, Xinyi Chen, Y Jennifer Sun, Rohan Anil, and Elad Hazan. Sketchy: Memory-efficient adaptive regularization with frequent directions. _Advances in Neural Information Processing Systems_, 36, 2024.
* [10] Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. _arXiv preprint arXiv:1812.04754_, 2018.
* [11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [12] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [13] David Kozak, Stephen Becker, Alireza Doostan, and Luis Tenorio. Stochastic subspace descent. _arXiv preprint arXiv:1904.01145_, 2019.
* [14] Brett W Larsen, Stanislav Fort, Nic Becker, and Surya Ganguli. How many degrees of freedom do we need to train deep networks: a loss landscape perspective. _arXiv preprint arXiv:2107.05802_, 2021.
* [15] Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky. Relora: High-rank training through low-rank updates. In _Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)_, 2023.
* [16] Bo Liu, Lemeng Wu, Lizhang Chen, Kaizhao Liang, Jiaxu Zhu, Chen Liang, Raghuraman Krishnamoorthi, and Qiang Liu. Communication efficient distributed training with distributed lion. _arXiv preprint arXiv:2404.00438_, 2024.
* [17] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. Full parameter fine-tuning for large language models with limited resources. _arXiv preprint arXiv:2306.09782_, 2023.
* [18] Chris J Maddison, Daniel Paulin, Yee Whye Teh, Brendan O'Donoghue, and Arnaud Doucet. Hamiltonian descent methods. _arXiv preprint arXiv:1809.05042_, 2018.

* [19] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In _International conference on machine learning_, pages 2408-2417. PMLR, 2015.
* [20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _arXiv e-prints_, 2019.
* [21] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In _International Conference on Machine Learning_, pages 4596-4604. PMLR, 2018.
* [22] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [23] Yiming Wang, Yu Lin, Xiaodong Zeng, and Guannan Zhang. Multilora: Democratizing lora for better multi-task learning. _arXiv preprint arXiv:2311.11501_, 2023.
* [24] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. _Advances in neural information processing systems_, 30, 2017.
* [25] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. _arXiv preprint arXiv:2403.03507_, 2024.

## Appendix A Hamiltonian Examples

**Example A.1**.: _Momentum + Online Subspace Descent is_

\[\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=-\bm{P}_{t}\bm{\hat{M}}_{t}, \bm{\hat{G}}_{t}=\bm{P}_{t}^{\top}\nabla L(\bm{W}_{t}), \frac{\mathrm{d}}{\mathrm{d}t}\bm{\hat{M}}_{t}=a(\bm{\hat{G}}_{t}-\bm{\hat{M}} _{t}),\]

_with Lyapunov function \(H(\bm{W},\bm{\hat{M}})=L(\bm{W})+\frac{\left\|\bm{\hat{M}}\right\|^{2}}{2a},\) for which_

\[\frac{\mathrm{d}}{\mathrm{d}t}H(\bm{W}_{t},\bm{\hat{M}}_{t})=- \nabla L(\bm{W}_{t})^{\top}\bm{P}_{t}\bm{\hat{M}}_{t}+\bm{\hat{M}}_{t}^{\top} (\bm{P}_{t}^{\top}\nabla L(\bm{W}_{t})-\bm{\hat{M}}_{t})=-\left\|\bm{\hat{M}}_ {t}\right\|_{2}^{2}\leq 0.\]

**Example A.2**.: _Adam + Online Subspace Descent is_

\[\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=\bm{P}_{t}\frac{\bm{\hat{M}}_{t}}{ \sqrt{\bm{\hat{V}}_{t}}+e}, \bm{\hat{G}}_{t}=\bm{P}_{t}^{\top}\nabla L(\bm{W}_{t}), \frac{\mathrm{d}}{\mathrm{d}t}\bm{\hat{M}}_{t}=a(\bm{\hat{G}}_{t}-\bm{\hat{M} }_{t}), \frac{\mathrm{d}}{\mathrm{d}t}\bm{\hat{V}}_{t}=b(\bm{\hat{G}}_{t}^{2}-\bm{ \hat{V}}_{t}).\]

_with Lyapunov function \(H(\bm{W},\bm{M},\bm{V})=L(\bm{W})+\frac{1}{2a}\left\langle\frac{\bm{\hat{M}}} {\sqrt{\hat{V}}+e},\ \bm{\hat{M}}\right\rangle,\) for which_

\[\frac{\mathrm{d}}{\mathrm{d}t}H(\bm{W}_{t},\bm{\hat{M}}_{t},\bm{ \hat{V}}_{t})\] \[=-\left\langle\bm{G}_{t},\bm{P}_{t}\frac{\bm{\hat{M}}_{t}}{\sqrt{ \bm{\hat{V}}_{t}}+e}\right\rangle+\frac{1}{a}\left\langle\frac{\bm{\hat{M}}_{t }}{\sqrt{\bm{\hat{V}}_{t}}+e},\ a(\bm{P}_{t}^{\top}\bm{G}_{t}-\bm{\hat{M}}_{t} )\right\rangle-\frac{b}{4a}\left\langle\frac{\bm{\hat{M}}_{t}^{\odot^{2}}}{ \sqrt{\bm{\hat{V}}_{t}}\odot(\sqrt{\bm{\hat{V}}_{t}}+e)^{\odot^{2}}},\ (\bm{\hat{G}}_{t}^{\odot 2 }-\bm{\hat{V}}_{t})\right\rangle\] \[=-\left\langle 1-\frac{b}{4a}\frac{\sqrt{\bm{\hat{V}}_{t}}}{\sqrt{ \bm{\hat{V}}_{t}}+e},\ \frac{\bm{\hat{M}}_{t}^{\odot^{2}}}{\sqrt{\bm{\hat{V}}_{t}}+e}\right\rangle- \frac{b}{4a}\left\langle\frac{\bm{\hat{M}}_{t}^{\odot^{2}}}{\sqrt{\bm{\hat{V}} _{t}}\odot(\sqrt{\bm{\hat{V}}_{t}}+e)^{\odot^{2}}},\ \bm{\hat{G}}_{t}^{\odot 2 }\right\rangle\] \[\leq-\left(1-\frac{b}{4a}\right)\left\|\frac{\bm{\hat{M}}_{t}}{ \sqrt{\sqrt{\hat{V}_{t}}+e}}\right\|^{2}-\frac{b}{4a}\left\|\frac{\bm{\hat{M}} _{t}\bm{\hat{G}}_{t}}{\sqrt[4]{\bm{\hat{V}}_{t}}(\sqrt{\bm{\hat{V}}_{t}}+e)} \right\|^{2}\leq 0,\]

_where we assume \(a\geq b/4\)._

**Example A.3**.: _The Lion-\(\mathcal{K}\) + Online Subspace Descent is_

\[\frac{\mathrm{d}}{\mathrm{d}t}\bm{W}_{t}=\bm{P}_{t}\nabla\mathcal{K}((1-b)\bm{ \hat{M}}_{t}-b\bm{\hat{G}}_{t}),\ \ \ \ \frac{\mathrm{d}}{\mathrm{d}t}\bm{M}_{t}=-a(\bm{\hat{G}}_{t}+\bm{\hat{M}}_{t}), \ \ \ \ \bm{\hat{G}}_{t}=\bm{P}_{t}^{\top}\nabla L(\bm{W}_{t})\]

_Consider the Hamiltonian function in Eq (13) of [4]:_

\[H(\bm{W},\bm{\hat{M}})=aL(\bm{W})+\frac{1}{1-b}\mathcal{K}((1-b)\bm{\hat{M}}).\]

\[\frac{\mathrm{d}}{\mathrm{d}t}H(\bm{W}_{t},\bm{M}_{t}) =a\langle\bm{G}_{t},\bm{P}_{t}\nabla\mathcal{K}((1-b)\bm{\hat{M}}_ {t}-b\bm{\hat{G}}_{t})\rangle-a\langle\nabla\mathcal{K}((1-b)\bm{\hat{M}}_{t} ),\bm{\hat{G}}_{t}+\bm{\hat{M}}_{t}\rangle\] \[=a\langle\bm{\hat{G}}_{t},\nabla\mathcal{K}((1-b)\bm{\hat{M}}_{t} -b\bm{\hat{G}}_{t})-\nabla\mathcal{K}((1-b)\bm{\hat{M}}_{t})\rangle-a\langle \nabla\mathcal{K}((1-b)\bm{\hat{M}}_{t}),\bm{\hat{M}}_{t}\rangle\] \[=-\frac{a}{b}[(1-b)\bm{\hat{M}}_{t};\ -b\bm{\hat{G}}_{t}]_{\nabla \mathcal{K}}-\frac{a}{(1-b)}[\bm{0};\ (1-b)\bm{\hat{M}}_{t}]_{\nabla \mathcal{K}}\leq 0Experiments

### Hyperparameters

We sweep learning rate from [0.01, 0.005, 0.001]. For GaLore as well as Adam8bit, we follow the recommended hyperparameters as much as possible. For instance, GaLore update gap is set to recommended default, \(200\). Warmup is set to \(10\%\) of the total training steps. Batch size is set to \(512\) and gradient clipping is set to \(1.0\).

### Rank Sweep

In the following table, is a sweep on different ranks and their final perplexity of LLaMA 60M (SS = 1024) on C4. All other hyperparameters are fixed and using recommended default. Notice that as the rank increases, both Dynamic Subspace Descent and GaLore improve.

### Optimizer Sweep

\begin{table}
\begin{tabular}{l l} \hline \hline
**Method** & **Perplexity** \\ \hline Lion & \(52.65\) \\ Adafactor & \(33.45\) \\ Adam8bit & \(29.77\) \\ SGD & \(3469.14\) \\ \hline Galore LION & \(46.90\) \\ Galore Adafactor & \(34.32\) \\ Galore AdamW8bit & \(48.05\) \\ Ours LION + LION & \(57.97\) \\ Ours Adafactor + Adafactor & \(47.61\) \\ Ours AdamW8bit + AdamW8bit & \(49.01\) \\ Ours LION + Adam8bit & \(44.76\) \\ Ours Adafactor + AdamW8bit & \(34.15\) \\ Ours AdamW8bit + SGD & \(53.53\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: In this experiment, we train LLaMA 60M on C4 with sequence length of 1024. We combine different base optimizers to update both \(P_{t}\) and \(W_{t}\), denote as ”Ours weight optimizer + \(P_{t}\) optimizer”.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Rank** & **Perplexity (Ours)** & **Perplexity (GaLore)** \\ \hline
32 & \(85.90\) & \(86.16\) \\
128 & \(49.01\) & \(48.05\) \\
512 & \(37.41\) & \(36.93\) \\ Full & \(37.18\) & \(36.51\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: On LLaMA 60M SS 1024, we sweep across different ranks, the trend is clear and intuitive that higher rank is preferred when it’s feasible.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our theoretical and empirical results align with our main claims. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed the limitation and difficulty in analyzing the class of method theoretically. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All assumptions are clearly stated in the body of the paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Hyperparameters choices are provided and later we will make code publicly available anonymously. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: All dataset and codebase used in the experiments are opensourced. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All hyperparameters, including learning rate, rank of projectors, sequence length are specified in each experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer:[No] Justification: Training LLM is expensive, we can afford running multiple runs on large models. However, previous empirical studies have shown that variation across seed is generally small in language model pretraining. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: GPU type used in experiment is specified. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We comply with the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our paper aims to promote deeper theoretical understanding of a class of method. The algorithm itself is not practical enough to produce powerful enough LLM that could potentially have any negative societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: same reasoning as above Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Everything is opensourced. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new asset has been introduced. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No human subject has been involved. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We did not perform any studies involving human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.