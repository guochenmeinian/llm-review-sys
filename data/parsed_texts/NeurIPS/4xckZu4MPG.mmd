# Attention as Implicit Structural Inference

 Ryan Singh

School of Engineering and Informatics,

University of Sussex.

rs773@sussex.ac.uk

&Christopher L. Buckley

School of Engineering and Informatics,

University of Sussex.

VERSES AI Research Lab,

Los Angeles, CA, USA.

###### Abstract

Attention mechanisms play a crucial role in cognitive systems by allowing them to flexibly allocate cognitive resources. Transformers, in particular, have become a dominant architecture in machine learning, with attention as their central innovation. However, the underlying intuition and formalism of attention in Transformers is based on ideas of keys and queries in database management systems. In this work, we pursue a structural inference perspective, building upon, and bringing together, previous theoretical descriptions of attention such as; Gaussian Mixture Models, alignment mechanisms and Hopfield Networks. Specifically, we demonstrate that attention can be viewed as inference over an implicitly defined set of possible adjacency structures in a graphical model, revealing the generality of such a mechanism. This perspective unifies different attentional architectures in machine learning and suggests potential modifications and generalizations of attention. Here we investigate two and demonstrate their behaviour on explanatory toy problems: (a) extending the value function to incorporate more nodes of a graphical model yielding a mechanism with a bias toward attending multiple tokens; (b) introducing a geometric prior (with conjugate hyper-prior) over the adjacency structures producing a mechanism which dynamically scales the context window depending on input. Moreover, by describing a link between structural inference and precision-regulation in Predictive Coding Networks, we discuss how this framework can bridge the gap between attentional mechanisms in machine learning and Bayesian conceptions of attention in Neuroscience. We hope by providing a new lens on attention architectures our work can guide the development of new and improved attentional mechanisms.

## 1 Introduction

Designing neural network architectures with favourable inductive biases lies behind many recent successes in Deep Learning. The Transformer, and in particular the attention mechanism has allowed language models to achieve human like generation abilities previously thought impossible [1, 2]. The success of the attention mechanism as a domain agnostic architecture has prompted adoption across a diverse range of tasks beyond language modelling, notably reaching state-of-the-art performance in visual reasoning and segmentation tasks [3, 4].

This depth and breadth of success indicates the attention mechanism expresses a useful computational primitive. Recent work has shown interesting theoretical links to kernel methods [5, 6, 7], Hopfield networks [8], and Gaussian mixture models [9, 10, 11, 12, 13], however a formal understanding that captures the generality of this computation remains outstanding. In this paper, we show the attention mechanism can naturally be described as inference on the structure of a graphical model, agreeing with observations that transformers are able to flexibly choose between models based on context [14, 15]. This Bayesian perspective complements previous theory [16, 8, 12], adding newmethods for reasoning about inductive biases and the functional role of attention variables. Further, understanding the core computation as inference permits a unified description of multiple attention mechanisms in the literature as well as narrowing the explanatory gap to ideas in neuroscience.

This paper proceeds in three parts: First in Sec.3, we show that'soft' attention mechanisms (e.g. self-attention, cross-attention, graph attention, which we call _transformer attention_ hereafter) can be understood as taking an expectation over possible connectivity structures, providing an interesting link between softmax-based attention and marginal likelihood. Second in Sec.4, we extend the inference over connectivity to a Bayesian setting which, in turn, provides a theoretical grounding for iterative attention mechanisms (slot-attention and block-slot attention) [17; 18; 19], Modern Continuous Hopfield Networks [8] and Predictive Coding Networks. Finally in Sec.5, we leverage the generality of this description in order to design new mechanisms with predictable properties.

Intuitively, the attention matrix can be seen as the posterior distribution over edges \(E\) in a graph, \(\mathcal{G}=(K\cup Q,E)\) consisting of a set of query and key nodes \(Q,K\) each of dimension \(d\). Where the full mechanism computes an expectation of a function defined on the graph \(V:\mathcal{G}\rightarrow\mathbb{R}^{d\times|\mathcal{G}|}\) with respect to this posterior.

\[Attention(Q,K,V) =\overbrace{softmax(\frac{QW_{Q}W_{K}^{T}K^{T}}{\sqrt{d}})}^{p (E\mid Q,K)}W_{V}K\] \[=\mathbb{E}_{p(E|Q,K)}[V]\]

Crucially, when \(\mathcal{G}\) is seen as a graphical model, the posterior over edges becomes an inference about dependency structure and the functional form becomes natural. This formalism provides an alternate Bayesian theoretical framing within which to understand attention models, shifting the explanation from one centred around retrieval to one that is fundamentally concerned with in-context inference of probabilistic relationships (including retrieval). Within this framework different attention architectures can be described by considering different implicit probabilistic models, by making these explicit we hope to support more effective analysis and the development of new architectures.

## 2 Related Work

A key benefit of the perspective outlined here is to tie together different approaches taken in the literature. Specifically, structural variables can be seen as the alignment variables discussed in previous Bayesian descriptions [16; 20; 21], on the other hand Gaussian Mixture Models (GMMs) can be seen as a specific instance of the framework developed here. This description maintains the explanatory power of GMMs by constraining the alignment variables to be the edges of an implicit graphical model, while offering the increased flexibility of alignment approaches to describe multiple forms of attention.

**Latent alignment and Bayesian Attention,** several attempts have been made to combine the benefits of soft (differentiability) and stochastic attention, often viewing attention as a probabilistic alignment problem. Most approaches proceed by sampling, e.g., using the REINFORCE estimator [20] or a \(topK\) approximation [22]. Two notable exceptions are [16] which embeds an inference algorithm within the forward pass of a neural network, and [21] which employs the re-parameterisation trick for the alignment variables. In this work, rather than treating attention weights as an independent learning problem, we aim to provide a parsimonious implicit model that would give rise to the attention weights. Additionally showing that'soft' attention weights arise naturally in variational inference from either collapsed variational inference or a mean-field approximation.

**Relationship to Gaussian mixture model,** previous works that have taken a probabilistic perspective on the attention mechanism note the connection to inference in a gaussian mixture model [11; 10; 12; 13]. Indeed [23] directly show the connection between the Hopfield energy and the variational free energy of a Gaussian mixture model. Although Gaussian mixture models, a special case of the framework we present here, are enough to explain cross attention they do not capture slot or self-attention, obscuring the generality underlying attention mechanisms. In contrast, the description presented here extends to structural inductive biases beyond what can be expressed in a Gaussian mixture model, additionally offering a route to describing the whole transformer block.

**Attention as bi-level optimisation,** mapping feed-forward architecture to a minimisation step on a related energy function has been called unfolded optimisation [24]. Taking this perspective can lead to insights about the inductive biases involved for each architecture. It has been shown that the cross-attention mechanism can be viewed as an optimisation step on the energy function of a form of Hopfield Network [8], providing a link between attention and associative memory. while [25] extend this view to account for self-attention. Our framework distinguishes Hopfield attention, which does not allow an arbitrary value matrix, from transformer attention. Although there remains a strong theoretical connection, we interpret the Hopfield Energy as an instance of variational free energy, aligning more closely with iterative attention mechanisms such as slot-attention.

## 3 Transformer Attention

### Attention as Expectation

We begin by demonstrating transformer attention can be seen as calculating an expectation over graph structures. Specifically, let \(x=(x_{1},..,x_{n})\) be observed input variables, \(\phi\) be some set of discrete latent variables representing edges in a graphical model of \(x\) given by \(p(x\mid\phi)\), and \(y\) a variable we need to predict. Our goal is to find \(\mathbb{E}_{y|x}[y]\), however the graph structure \(\phi\) is unobserved so we calculate the marginal likelihood.

\[\mathbb{E}_{y|x}[y]=\sum_{\phi}p(\phi\mid x)\mathbb{E}_{y|x,\phi}[y]\]

Importantly, the softmax function is a natural representation for the posterior,

\[p(\phi\mid x)=\frac{p(x,\phi)}{\sum_{\phi}p(x,\phi)}=softmax(\ln p(x,\phi))\]

in order to expose the link to transformer attention, let the model of \(y\) given the graph (\(x\), \(\phi\)) be parameterised by a function \(\mathbb{E}_{y|x,\phi}[y]=v(x,\phi)\).

\[\mathbb{E}_{y|x}[y]=\sum_{\phi}softmax(\ln p(x,\phi))v(x,\phi)=\mathbb{E}_{ \phi|x}[v(x,\phi)]\] (1)

In general, transformer attention can be seen as weighting \(v(x,\phi)\) by the posterior distribution \(p(\phi\mid x)\) over different graph structures. We show Eq.1 is exactly the equation underlying self and cross-attention by presenting the specific generative models corresponding to them. In this description the latent variables \(\phi\) are identified as edges between observed variables \(x\) (keys and queries) in a pairwise Markov Random Field, parameterised by matrices \(W_{K}\) and \(W_{Q}\), while the function \(v\) is parameterised by \(W_{V}\).

**Pairwise Markov Random Fields** are a natural tool for modelling the dependencies of random variables, with prominent examples including Ising models (Boltzmann Machine) and multivariate Gaussians. While typically defined given a known structure, the problem of inferring the latent graph is commonly called structural inference.

Formally, given a set of random variables \(X=(X_{v})_{v\in V}\) with probability distribution \([p]\) and a graph \(G=(V,E)\). The variables form a pairwise Markov Random Field (pMRF) [26] with respect to \(G\) if the joint density function \(P(X=x)=p(x)\) factorises as follows

\[p(x)=\frac{1}{Z}\exp\left(\sum_{v\in V}\psi_{v}+\sum_{e\in E}\psi_{e}\right)\]

where \(Z\) is the partition function \(\psi_{v}(x_{v})\) and \(\psi_{e}=\psi_{u,v}(x_{u},x_{v})\) are known as the node and edge potentials respectively. Bayesian structural inference also requires a structural prior \(p(\phi)\) over the space of possible adjacency structures, \(\phi\in\Phi\), of the underlying graph.

**Factorisation,** without constraints this space grows exponentially in the number of nodes (\(2^{|V|}\) possible graphs leading to intractable softmax calculations), all the models we explore here implicitly assume a factorised prior1. We briefly remark that Eq.1 respects factorisation of \([p]\) in the followingsense; if the distribution admits a factorisation (a partition of the space of graphs \(\Phi=\prod_{i}\Phi_{i}\)) with respect to the latent variables \(p(x,\phi)=\prod_{i}e^{f_{i}(x,\phi_{i})}\) where \(\phi_{i}\in\Phi_{i}\), and the value function distributes over the same partition of edges \(v(x,\phi)=\sum_{i}v_{i}(x,\phi_{i})\) then each of the factors can be marginalised independently:

\[\mathbb{E}_{\phi|x}[v(x,\phi)]=\sum_{i}\mathbb{E}_{\phi_{i}|x}[v_{i}]\] (2)

To recover cross-attention and self-attention we need to specify the structural prior, potential functions and a value function. (In order to ease notation, when \(\Phi_{i}\) is a set of edges involving a common node \(x_{i}\), such that \(\phi_{i}=(x_{i},x_{j})\) represents a single edge, we use the notation \(\phi_{i}=[j]\), suppressing the shared index.)

### Cross Attention and Self Attention

We first define the model that gives rise to cross-attention:

* Key nodes \(K=(x_{1},..,x_{n})\) and query nodes \(Q=(x^{\prime}_{1},...,x^{\prime}_{m})\)
* Structural prior \(p(\phi)=\prod_{i=1}^{m}p(\phi_{i})\), where \(\Phi_{i}=\{(x_{1},x^{\prime}_{i}),..,(x_{n},x^{\prime}_{i})\}\) is the set of edges involving \(x^{\prime}_{i}\) and \(\phi_{i}\sim Uniform(\Phi_{i})\) such that each query node is uniformly likely to connect to each key node.
* Edge potentials \(\psi(x_{j},x^{\prime}_{i})=x^{\prime T}_{i}W^{T}_{Q}W_{K}x_{j}\), in effect measuring the similarity of \(x_{j}\) and \(x^{\prime}_{i}\) in a projected space.
* Value functions \(v_{i}(x,\phi_{i}=[j])=W_{V}x_{j}\), a linear transformation applied to the node at the start of the edge \(\phi_{i}\).

Taking the expectation with respect to the posterior in each of the factors defined in Eq.2 gives the standard cross-attention mechanism,

\[\mathbb{E}_{p(\phi_{i}|Q,K)}[v_{i}]=\sum_{j}softmax_{j}(x^{\prime T}_{i}W^{T}_ {Q}W_{K}x_{j})W_{V}x_{j}\]

If the key nodes are in fact the same as the query nodes and the prior is instead over a directed graph we recover self-attention (A.8.1).

## 4 Iterative Attention

We continue by extending attention to a latent variable setting, where not all the nodes are observed. In essence applying the attention trick, i.e., a marginalisation of structural variables, to a variational free energy (Evidence Lower Bound). This allows us to recover models such as slot attention [17] and block-slot attention [18]. These mechanisms utilise an EM-like procedure using the current estimation of latent variables to infer the structure and then using the inferred structure to improve estimation of latent variables. Interestingly, Modern Continuous Hopfield Networks fit within this paradigm rather than the one discussed in Sec.3; collapsed variational inference produces an identical energy function to the one proposed by Ramsauer et al. [8].

### Collapsed Inference

We present a version of collapsed variational inference [27], where the collapsed variables \(\phi\) are again structural, showing how this results in a Bayesian attention mechanism. In contrast to the previous section, we have a set of (non-structural) latent variables \(z\). The goal is to infer \(z\) given the observed variables, \(x\), and a latent variable model \(p(x,z,\phi)\). Collapsed inference proceeds by marginalising out the extraneous latent variables \(\phi\)[27]:

\[p(x,z)=\sum_{\phi}p(x,z,\phi)\] (3)

We define a gaussian recognition density \(q(z)\sim N(z;\mu,\Sigma)\) and optimise the variational free energy \(\mathcal{F}(\lambda)=\mathbb{E}_{q}[\ln q_{\lambda}(z)-\ln p(x,z)]\) with respect to the parameters, \(\lambda=(\mu,\Sigma)\), of this distribution. Application of Laplace's method yields approximate derivatives of the variational free energy \(\nabla_{\mu}\mathcal{F}\approx-\nabla_{\mu}\ln p(x,\mu)\) and \(\nabla_{\Sigma}\mathcal{F}\approx-\nabla_{\mu}^{2}\ln p(x,\mu)\), here we focus on the first order terms 2. Substituting in Eq.3:

Footnote 2: As the first order terms are independent of the second order ones, see A.7.1 for details.

\[\nabla_{\mu}\mathcal{F} \approx-\nabla_{\mu}\ln\sum_{\phi}p(x,\mu,\phi)\] (4) \[=-\frac{1}{\sum_{\phi}p(x,\mu,\phi)}\sum_{\phi}\nabla_{\mu}p(x, \mu,\phi)\] (5)

In order to make the link to attention, we employ the log-derivative trick, substituting \(p(\cdot)=e^{\ln p(\cdot)}\) and re-express Eq.5 in two ways:

\[=-\sum_{\phi}softmax_{\phi}(\ln p(x,\mu,\phi))\nabla_{\mu}\ln p(x, \mu,\phi)\] (6) \[=\mathbb{E}_{\phi|x,\mu}[-\nabla_{\mu}\ln p(x,\mu,\phi)]\] (7)

The first form reveals the softmax which is ubiquitous in all attention models. The second, suggests the variational update should be evaluated as the expectation of the typical variational gradient (the term within the square brackets) with respect to the posterior over the parameters represented by the random variable \(\phi\). In other words, iterative attention is exactly transformer attention applied iteratively where the value function is the variational free energy gradient. We derive updates for a general pMRF before again recovering (iterative) attention models in the literature by specifying particular distributions.

**Free Energy of a marginalised pMRF,** recall the factorised pMRF, \(p(x,\phi)=\frac{1}{Z}\prod_{i}e^{f_{i}(x,\phi_{i})}\). Again, independence properties simplify the calculation, the marginalisation can be expressed as a product of local marginals, \(\sum_{\phi}p(x,\phi)=\frac{1}{Z}\prod_{i}\sum_{\phi_{i}}e^{f_{i}(x,\phi_{i})}\). Returning to the inference setting, the nodes are partitioned into observed nodes, \(x\), and variational parameters \(\mu\). Hence the (approximate) collapsed variational free energy Eq.5, can be expressed as, \(F(x,\mu)=-\sum_{i}\ln\sum_{\phi_{i}}e^{f_{i}(x,\mu,\phi_{i})}+C\) and it's derivative:

\[\frac{\partial F}{\partial\mu_{j}}=-\sum_{i}\sum_{\phi_{i}}softmax(f_{i}) \frac{\partial f_{i}}{\partial\mu_{j}}\]

Finally, we follow [8] in using the Convex-Concave Procedure (CCCP) to derive a simple fixed point equation which necessarily reduces the free energy.

**Quadratic Potentials and the Convex Concave Procedure,** assuming the node potentials are quadratic \(\psi(x_{i})=-\frac{1}{2}x_{i}^{2}\) and the edge potentials have the form \(\psi(x_{i},x_{j})=x_{i}Wx_{j}\), and define \(\hat{f}_{i}=\sum_{e\in\Phi_{i}}\psi_{e}\). Consider the following fixed point equation,

\[\mu_{j}^{*}=\sum_{i}\sum_{\phi_{i}}softmax(\tilde{f}_{i})\frac{\partial\tilde {f}_{i}}{\partial\mu_{j}}\] (8)

Figure 1: Comparison of models involved in different attention mechanisms. In each case, the highlighted edges indicate \(\Phi_{i}\) the support of the uniform prior over \(\phi_{i}\). Attention proceeds by calculating a posterior over these edges, given the current state of the nodes, before using this inference to calculate an expectation of the value function \(v\). For iterative attention mechanisms the value function can be identified as the gradient of a variational free energy, in contrast, transformer attention uses a learnable function.

since (under mild conditions) node potentials are convex and edge potentials are concave (A.7.1.1), we can invoke the CCCP [28] to show this fixed point equation descends on the energy \(F(x,\mu_{j}^{*})\leq F(x,\mu_{j})\) with equality if and only if \(\mu_{j}^{*}\) is a stationary point of \(F\). We follow Sec.3 in specifying specific structural priors and potential functions that recover different iterative attention mechanisms.

### Modern Continuous Hopfield Network

Let the observed, or memory, nodes \(x=(x_{1},..,x_{n})\) and latent nodes \(z=(z_{1},..,z_{m})\) have the following structural prior \(p(\phi)=\prod_{i=1}^{m}p(\phi_{i})\), where \(\phi_{i}\sim Uniform\{(x_{1},z_{i}),..,(x_{n},z_{i})\}\), meaning each latent node is uniformly likely to connect to a memory node. Define edge potentials \(\psi(x_{j},z_{i})=z_{i}^{T}x_{j}\). Application of Eq.8:

\[\mu_{i}^{*}=\sum_{j}softmax_{j}(\mu_{i}^{T}x_{j})x_{j}\]

When \(\mu_{i}\) is initialised to some query \(\xi\) the system the fixed point update is given by \(\mu_{i}^{*}(\xi)=\mathbb{E}_{\phi_{i}|x,\xi}[x_{[j]}]\). If the patterns \(x\) are well separated, \(\mu_{i}^{*}(\xi)\approx x_{j^{\prime}}\), where \(x_{j^{\prime}}\) is the closest vector and hence can be used as an associative memory.

### Slot Attention

Slot attention [17] is an object centric learning module centred around an iterative attention mechanism. Here we show this is a simple adjustment of the prior beliefs on our edge set. With edge potentials of the form \(\psi(x_{j},z_{i})=z_{i}^{T}W_{Q}^{T}W_{K}x_{j}\), replace the prior over edges with \(p(\phi)=\prod_{j=1}^{n}p(\phi_{j})\), \(\phi_{j}\sim Uniform\{(x_{j},z_{1}),..,(x_{j},z_{m})\}\). Notice, in comparison to MCHN, the prior over edges is swapped, each observed node is uniformly likely to connect to a latent node, in turn altering the index of the softmax.

\[\mu_{i}^{*}=\sum_{j}softmax_{i}(\mu_{i}^{T}W_{Q}^{T}W_{K}x_{j})W_{Q}^{T}W_{K}x _{j}\]

while the original slot attention employed an RNN to aid the basic update shown here, the important feature is that the softmax is taken over the'slots'. This forces competition between slots to account for the observed variables, creating object centric representations.

### Predictive Coding Networks

Predictive Coding Networks (PCN) have emerged as an influential theory in Computational Neuroscience [29; 30; 31]. Building on theories of perception as inference and the Bayesian brain, PCNs perform approximate Bayesian inference by minimising a variational free energy of a graphical model, where incoming sensory data are used as observations. Typical implementations use a hierarchical model with Gaussian conditionals, resulting in a local prediction error minimising scheme. The minimisation happens on two distinct time-scales, which can be seen as E-step and M-steps on the variational free energy: a (fast) inference phase encoded by neural activity corresponding to perception and a (slow) learning phase associated with synaptic plasticity. Gradient descent on the free energy gives the inference dynamics for a particular neuron \(\mu_{i}\), [32]

\[\frac{\partial\mathcal{F}}{\partial\mu_{i}}=-\sum_{\phi^{-}}k_{\phi}\epsilon_ {\phi}+\sum_{\phi^{+}}k_{\phi}\epsilon_{\phi}w_{\phi}\]

Where \(\epsilon\) are prediction errors, \(w\) represent synaptic strength, \(k\) are node specific precisions representing uncertainty in the generative model and \(\phi^{-},\phi^{+}\) represent pre-synaptic and post-synaptic terminals resectively. Applying a uniform prior over the incoming synapses results in a slightly modified dynamics,

\[\frac{\partial\mathcal{F}}{\partial\mu_{i}}=-\sum_{\phi^{-}}softmax(-{\epsilon _{\phi}}^{2})k_{\phi}\epsilon_{\phi}+\sum_{\phi^{+}}softmax(-{\epsilon_{\phi} }^{2})k_{\phi}\epsilon_{\phi}w_{\phi}\]

where the softmax function induces a normalisation across prediction errors received by a neuron. This dovetails with theories of attention as normalisation in Psychology and Neuroscience [33; 34; 35]. In contrast previous predictive coding based theories of attention have focused on the precision terms, \(k\), due to their ability to up and down regulate the impact of prediction errors [36; 37; 38]. Here we see the softmax terms play a functionally equivalent role to precision variables, inheriting their ability to account for bottom-up and top-down attention, while exhibiting the fast winner-takes-all dynamics that are associated with cognitive attention.

## 5 New Designs

By identifying the attention mechanism in terms of an implicit probabilistic model, we can review and modify the underlying modelling assumptions in a principled manner to design new attention mechanisms. Recall transformer attention can be written as the marginal probability \(p(y\mid x)=\sum_{\phi}p(\phi\mid x)\mathbb{E}_{y|x,\phi}[y]\), the specific mechanism is therefore informed by three pieces of data: (a) the value function \(p(y\mid x,\phi)\), (b) the likelihood \(p(x\mid\phi)\) and (c) the prior \(p(\phi)\). Here, we explore modifying (a) and (c) and show they can exhibit favourable biases on toy problems.

### Multi-hop Attention

Our description makes it clear that the value function employed by transformer attention can be extended to any function over the graph. For example, consider the calculation of \(\mathbb{E}_{y|x,\phi}[y_{i}]\) in transformer attention, a linear transformation is applied to the most likely neighbour, \(x_{j}\), of \(x_{i}\). A natural extension is to include a two-hop neighbourhood, additionally using the most likely neighbour \(x_{k}\) of \(x_{j}\). The attention mechanism then takes a different form \(\mathbb{E}_{p(\phi_{i}|\phi_{i})p(\phi_{i}|x)}[V(x_{\phi_{i}}+x_{\phi_{j}})]= (P_{\phi}+P_{\phi}^{2})VX\), where \(P_{\phi}\) is the typical attention matrix. While containing the same number of parameters as a single-layer of transformer attention, for some datasets two-hop attention should be able to approximate the behaviour of two-layers of transformer attention.

**Task Setup** We simulate a simple dataset that has this property using the following data generation process: Initialise a projection matrix \(W_{y}\in\mathbb{R}^{d\times 1}\) and a relationship matrix \(W_{r}\in\mathbb{R}^{d\times d}\). \(X\) is then generated causally, using the relationship \(x_{i+1}=W_{r}x_{i}+N(0,\sigma)\) to generate \(x_{0}\), \(x_{1}\) and \(x_{2}\), while the remaining nodes are sampled from the noise distribution \(N(0,\sigma)\). Finally, the target \(y\) is generated from the history of \(x_{2}\), \(y=W_{y}(x_{1}+x_{0})\) and the nodes of \(X\) are shuffled. Importantly \(W_{r}\) is designed to be low rank, such that performance on the task requires paying attention to both \(x_{1}\) and \(x_{0}\), Figure 2.

### Expanding Attention

One major limitation of transformer attention is the reliance on a fixed context window. From one direction, a small context window does not represent long range relationships, on the other hand a large window does an unnecessary amount of computation when modelling a short range relationship. By replacing the uniform prior with a geometric distribution \(p(\phi\mid q)\sim Geo(q)\)

Figure 2: Multihop Attention: (left) Graphical description of the toy problem, \(x_{2}\) is generated causally from \(x_{1}\) and \(x_{0}\), which are used to generate \(y\). (centre) Comparison of the attention employed by Multihop which takes two steps on the attention graph (top) contrasted with Self Attention (bottom). Multihop Attention has the correct bias to learn the task approaching the performance of two-layer Self Attention, while a single layer of Self Attention is unable (top right). Empirically examining the attention weights, Multihop Attention is able to balance attention across two positions, while self-attention favours a single position.

and a conjugate hyper-prior \(p(q)\sim Beta(\alpha,\beta)\) we derive a mechanism that dynamically scales depending on input. We use a (truncated) mean-field variational inference procedure [39] to iteratively approximate \(p(\phi\mid x)\) using the updates: 1. \(q_{t}\leftarrow\frac{\beta_{t}}{\alpha_{t}+\beta_{t}}\), 2. \(p_{t}=p(\phi\mid x,q_{t})\), 3. \(\alpha_{t+1}\leftarrow\alpha_{t}+1\), \(\beta_{t+1}\leftarrow\beta_{t}+\sum_{<H(q_{t})}i(p_{t})_{i}\). Where \(\alpha\) and \(\beta\) are hyperparameters determining the strength of the prior and \(H\) is the truncation horizon. Since attention dot products can be cached and reused for each calculation of step 2. the iterative procedure is computationally cheap.

The attention mechanism has asymptotic time complexity \(O(n^{2}d)\) where \(n\) is the size of the size of the context window and \(d\) is dimension over which the inner product is computed. In comparison, expanding attention \(O(n(md+k))\) where \(m\) is the size of the window at convergence, and \(k\) is the number of steps to converge. If, as is typical, \(d\) is large such that \(d>>k\) the time complexity of expanding attention should be favourable.

**Task Setup** Input and target sequence are generated similarly to above (without \(x_{0}\)). Here \(x_{1}\) is moved away from \(x_{2}\) according to a draw from a geometric distribution, Figure 3.

## 6 Discussion

### The Full Transformer Block

Transformer attention is typically combined with residual connections and a feedforward network, both of which have been shown important in preventing 'token collapse'. Here we briefly touch upon how these features might relate to the framework presented here.

**Feedforward layer,** it has previously been noticed the feedforward component can also be understood as a key-value memory where the memories are stored as persistent weights [40; 41]. This is due to the observation \(ff(x)=W_{2}\sigma(W_{1}x)\) is equivalent to attention when the non-linearity \(\sigma\) is a softmax, although a ReLU is typically used. We speculate the framework presented here could be extended explain this discrepancy, intuitively the ReLU relates to an edge prior that fully factorises into binary variables.

**Residual connections** have been shown to encourage iterative inference [42]. This raises the possibility transformer attention, rather than having an arbitrary transformation \(v\) as presented in Sec.3, is in fact approximately implementing the iterative inference of Sec.4 through a form of iterative amortised inference [43]. The view that the transformer is performing iterative refinement is additionally supported by empirical studies of early-decoding [44].

**Temperature and positional encodings,** both positional encodings and the temperature scaling can be seen as adjustments to the prior edge probability. In the case of relative positional encodings, by breaking the permutation invariance of the prior (A.8.2). While the temperature may be understood

Figure 3: Expanding Attention: (left) Graphical description of the toy problem, \(x_{2}\) and \(y\) are generated from \(x_{1}\) which is shuffled with a (exponentially decaying) recency bias. (centre) Comparison of the geometric prior, with different shades of red representing the iterative refinements during inference, used by Expanding and uniform prior used by Self Attention. (right) The relative number of operations used by Expanding Attention is beneficial when either the recency bias (\(1/p\)) or the number of feature dimensions (\(d\)) is large, training curves (overlaid) across each of these settings remained roughly equivalent.

in terms of tempered (or generalised) Bayesian inference [45], adjusting the strength of the prior relative to the likelihood.

### Limitations

The connection to structural inference presented here is limited to the attention computation of a single transformer head, an interesting future direction would be to investigate whether multiple layers and multiple heads typically used in a transformer can also be interpreted within this framework. Additionally, the extension to iterative inference employed a crude approximation to the variational free energy, arguably destroying the favourable properties of Bayesian methods. Suggesting the possibility of creating iterative attention mechanisms with alternative inference schemes, possibly producing more robust mechanisms.

### Conclusion

In this paper, we presented a probabilistic description of the attention mechanism, formulating attention as structural inference within a probabilistic model. This approach builds upon previous research that connects cross attention to inference in a Gaussian Mixture Model. By considering the discrete inference step in a Gaussian Mixture Model as inference on marginalised structural variables, we bridge the gap with alignment-focused descriptions. This framework naturally extends to self-attention, graph attention, and iterative mechanisms, such as Hopfield Networks. We hope this work will contribute to a more unified understanding of the functional advantages and disadvantages brought by Transformers.

Furthermore, we argue that viewing Transformers from a structural inference perspective provides different insights into their central mechanism. Typically, optimising structure is considered a learning problem, changing on a relatively slow timescale compared to inference. However, understanding Transformers as fast structural inference suggests that their remarkable success stems from their ability to change effective connectivity on the same timescale as inference. This general idea can potentially be applied to various architectures and systems. For instance, Transformers employ relatively simple switches in connectivity compared to the complex dynamics observed in the brain [46]. Exploring inference over more intricate structural distributions, such as connectivity motifs or modules in network architecture, could offer artificial systems even more flexible control of resources.

## Acknowledgements

This work was supported by The Leverhulme Trust through the be.AI Doctoral Scholarship Programme in biomimetic embodied AI. Additional thanks to Alec Tschantz, Tomasso Salvatori, Miguel Aguilera and Tomasz Korbak for their invaluable feedback and discussions.

## References

* [1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need, December 2017.
* [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc., 2020.
* [3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, June 2021.
* [4] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks, August 2022.

* [5] Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\({}^{\text{\text{\textregistered}}}\)om Method, October 2021.
* [6] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel, November 2019.
* [7] Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, and Nhat Ho. Robustify Transformers with Robust Kernel Density Estimation, October 2022.
* [8] Hubert Ramsauer, Bernhard Schaff, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Gunter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. Hopfield Networks is All You Need, April 2021.
* [9] Javier R. Movellan and Prasad Gabbur. Probabilistic Transformers, November 2020.
* [10] Prasad Gabbur, Manjot Bilkhu, and Javier Movellan. Probabilistic Attention for Interactive Segmentation, July 2021.
* [11] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, and Hong Liu. Expectation-Maximization Attention Networks for Semantic Segmentation. In _2019 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 9166-9175, Seoul, Korea (South), October 2019. IEEE. ISBN 978-1-72814-803-8. doi: 10.1109/ICCV.2019.00926.
* [12] Alexander Shim. A Probabilistic Interpretation of Transformers, April 2022.
* [13] Tam Minh Nguyen, Tan Minh Nguyen, Dung D. D. Le, Duy Khuong Nguyen, Viet-Anh Tran, Richard Baraniuk, Nhat Ho, and Stanley Osher. Improving Transformers with Probabilistic Attention Keys. In _Proceedings of the 39th International Conference on Machine Learning_, pages 16595-16621. PMLR, June 2022.
* [14] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent, December 2022.
* [15] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What Can Transformers Learn Context? A Case Study of Simple Function Classes.
* [16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured Attention Networks, February 2017.
* [17] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-Centric Learning with Slot Attention, October 2020.
* [18] Gautam Singh, Yeongbin Kim, and Sungjin Ahn. Neural Block-Slot Representations, November 2022.
* [19] Helen C. Barron, Ryszard Aussztauewicz, and Karl Friston. Prediction and memory: A predictive coding account. _Progress in Neurobiology_, 192:101821, September 2020. ISSN 03010082. doi: 10.1016/j.pneurobio.2020.101821.
* [20] Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander Rush. Latent Alignment and Variational Attention. In _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [21] Xinjie Fan, Shujian Zhang, Bo Chen, and Mingyuan Zhou. Bayesian Attention Modules. In _Advances in Neural Information Processing Systems_, volume 33, pages 16362-16376. Curran Associates, Inc., 2020.
* [22] Shiv Shankar, Siddhant Garg, and Sunita Sarawagi. Surprisingly Easy Hard-Attention for Sequence to Sequence Learning. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 640-645, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1065.
* [23] Louis Annabi, Alexandre Pitti, and Mathias Quoy. On the Relationship Between Variational Inference and Auto-Associative Memory, October 2022.
* [24] Jordan Frecon, Gilles Gasso, Massimiliano Pontil, and Saverio Salzo. Bregman Neural Networks. In _Proceedings of the 39th International Conference on Machine Learning_, pages 6779-6792. PMLR, June 2022.

* [25] Yongyi Yang, Zengfeng Huang, and David Wipf. Transformers from an Optimization Perspective, May 2022.
* [26] Martin J. Wainwright and Michael I. Jordan. Graphical Models, Exponential Families, and Variational Inference. _Foundations and Trends\(\otimes\) in Machine Learning_, 1(1-2):1-305, November 2008. ISSN 1935-8237, 1935-8245. doi: 10.1561/220000001.
* [27] Yee Teh, David Newman, and Max Welling. A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation. In _Advances in Neural Information Processing Systems_, volume 19. MIT Press, 2006.
* [28] Alan L Yuille and Anand Rangarajan. The Concave-Convex Procedure (CCCP). In _Advances in Neural Information Processing Systems_, volume 14. MIT Press, 2001.
* [29] Rajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: A functional interpretation of some extra-classical receptive-field effects. _Nature Neuroscience_, 2(1):79-87, January 1999. ISSN 1546-1726. doi: 10.1038/4580.
* [30] Karl Friston and Stefan Kiebel. Predictive coding under the free-energy principle. _Philosophical Transactions of the Royal Society B: Biological Sciences_, 364(1521):1211-1221, May 2009. ISSN 0962-8436. doi: 10.1098/rstb.2008.0300.
* [31] Christopher L. Buckley, Chang Sub Kim, Simon McGregor, and Anil K. Seth. The free energy principle for action and perception: A mathematical review. _Journal of Mathematical Psychology_, 81:55-79, December 2017. ISSN 0022-2496. doi: 10.1016/j.jmp.2017.09.004.
* [32] Beren Millidge, Yuhang Song, Tommaso Salvatori, Thomas Lukasiewicz, and Rafal Bogacz. A Theoretical Framework for Inference and Learning in Predictive Coding Networks, August 2022.
* [33] John H. Reynolds and David J. Heeger. The Normalization Model of Attention. _Neuron_, 61(2):168-185, January 2009. ISSN 08966273. doi: 10.1016/j.neuron.2009.01.002.
* [34] Matteo Carandini and David J. Heeger. Normalization as a canonical neural computation. _Nature Reviews Neuroscience_, 13(1):51-62, January 2012. ISSN 1471-0048. doi: 10.1038/nrn3136.
* [35] Grace W. Lindsay. Attention in Psychology, Neuroscience, and Machine Learning. _Frontiers in Computational Neuroscience_, 14, 2020. ISSN 1662-5188.
* [36] Andy Clark. The many faces of precision (Replies to commentaries on "Whatever next? Neural prediction, situated agents, and the future of cognitive science"). _Frontiers in Psychology_, 4, 2013. ISSN 1664-1078.
* [37] Harriet Feldman and Karl Friston. Attention, Uncertainty, and Free-Energy. _Frontiers in Human Neuroscience_, 4, 2010. ISSN 1662-5161.
* [38] M. Berk Mirza, Rick A. Adams, Karl Friston, and Thomas Parr. Introducing a Bayesian model of selective attention based on active inference. _Scientific Reports_, 9(1):13915, September 2019. ISSN 2045-2322. doi: 10.1038/s41598-019-50138-8.
* [39] O. Zobay. Mean field inference for the Dirichlet process mixture model. _Electronic Journal of Statistics_, 3 (none):507-545, January 2009. ISSN 1935-7524, 1935-7524. doi: 10.1214/08-EJS339.
* [40] Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. Augmenting Self-attention with Persistent Memory, July 2019.
* [41] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer Feed-Forward Layers Are Key-Value Memories, September 2021.
* [42] Stanislaw Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che, and Yoshua Bengio. Residual Connections Encourage Iterative Inference, March 2018.
* [43] Joe Marino, Yisong Yue, and Stephan Mandt. Iterative Amortized Inference. In _Proceedings of the 35th International Conference on Machine Learning_, pages 3403-3412. PMLR, July 2018.
* [44] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting Latent Predictions from Transformers with the Tuned Lens, March 2023.
* [45] Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. Generalized Variational Inference: Three arguments for deriving new Posteriors, December 2019.
* [46] Emmanuelle Tognoli and J. A. Scott Kelso. The Metastable Brain. _Neuron_, 81(1):35-48, January 2014. ISSN 0896-6273. doi: 10.1016/j.neuron.2013.12.022.

## 7 Appendix

Here we include some more detailed derivations of claims made in the paper, and list the hyperparameters used for the experiments.

### Iterative Attention

In this section we provide a more detailed treatment of the Laplace approximation, and provide proper justification for invoking the CCCP. For both, the following lemma is useful:

**Lemma 7.1**.: _The function \(\ln p(x)=\ln\sum_{\phi}p(x,\phi)=\ln\sum_{\phi}\exp E_{\phi}(x)\) has derivatives (i) \(\frac{\partial}{\partial x}\ln p(x)=\mathbb{E}_{\phi|x}[\frac{\partial}{ \partial x}E_{\phi}]\) and (ii) \(\frac{\partial^{2}}{\partial x^{2}}\ln p(x)=Var_{\phi|x}[\frac{\partial}{ \partial x}E_{\phi}]+\mathbb{E}_{\phi|x}[\frac{\partial^{2}}{\partial x^{2}}E_ {\phi}]\)_

Proof.: Let \(E=(E_{\phi})\) the vector of possible energies, and \(p=(p_{\phi})=(p(\phi\mid x))_{\phi}\) the vector of conditional probabilities. Consider \(\ln p(\phi\mid x)\) written in canonical form,

\[\ln p(\phi\mid x)=\langle E_{\phi}(x),\mathbbm{1}_{\phi}\rangle-A[E_{\phi}(x) ]+h(\phi)\]

Where \(A[E(x)]=\ln Z(E)\) is the cumulant generating function. By well known properties of the cumulant: \(\frac{\partial A}{\partial E_{i}}=p(\phi=i\mid x)=p_{i}\). Hence by the chain rule for partial derivatives, \(\frac{\partial A}{\partial x}=\sum_{\phi}p(\phi\mid x)\frac{\partial}{\partial x }E_{\phi}\), which is (i).

To find the second derivative we apply again the chain-rule \(\frac{d}{dt}f(g(t))=f^{\prime\prime}(g(t))g^{\prime}(t)^{2}+f^{\prime}(g(t))g^ {\prime\prime}(t)\). Again by properties of the cumulant \(\frac{\partial^{2}A}{\partial E_{i}\partial E_{j}}=Cov(\mathbbm{1}_{i}, \mathbbm{1}_{j})=[diag(p)-p^{T}p]_{i,j}=\)

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Name & Graph (\(G\)) & Prior (\(p(\phi)\)) & Potentials (\(\psi\)) & Value \(v(x,\phi)\) \\ \hline Cross Attention & Key nodes \(K\), query nodes \(Q\) & Uniform & \(x_{i}^{\prime T}W_{Q}^{T}W_{K}x_{j}\) & \(Vx_{j}\) \\ \hline Self Attention & K = Q, directed edges & Uniform & \(x_{i}^{T}W_{Q}^{T}W_{K}x_{j}\) & \(Vx_{j}\) \\ \hline Graph Attention, Sparse Attention & K = Q, directed edges & Uniform (restricted) & \(x_{i}^{T}W_{Q}^{T}W_{K}x_{j}\) & \(Vx_{j}\) \\ \hline Relative Positional Encodings & K = Q, directed edges & Categorical & \(x_{i}^{T}W_{Q}^{T}W_{K}x_{j}\) & \(Vx_{j}\) \\ \hline Absolute Positional Encodings & K = Q & Uniform & \(\tilde{x_{i}}^{T}W_{Q}^{T}W_{K}\tilde{x_{j}}\) & \(Vx_{j}\) \\ \hline Classification Layer & NN output \(f_{\theta}(X)\), classes \(y\) & Uniform & \(f_{\theta}(X)_{i}^{T}y_{j}\) & \(y_{j}\) \\ \hline MCHN & Observed nodes \(X\), latent nodes \(Z\) & Uniform (observed) & \(z_{i}^{T}W_{Q}^{T}W_{K}x_{j}\) & \(\frac{\partial F}{\partial z}\) \\ \hline Slot Attention & Observed nodes \(X\) latent nodes \(Z\) & Uniform (latent) & \(z_{i}^{T}W_{Q}^{T}W_{K}x_{j}\) & \(\frac{\partial F}{\partial z}\) \\ \hline Block-Slot Attention & Observed nodes \(X\), latent nodes \(Z\), memory nodes \(M\) & Uniform (latent) & \(z_{i}^{T}W_{Q}^{T}W_{K}x_{j}\), & \(\frac{\partial F}{\partial z}\) \\ \hline PCN & Observed nodes \(X\), multiple layers of latent nodes \(\{Z^{(l)}\}_{l\leq L}\) & Uniform (latent) & \(z_{i}^{T}W_{Q}^{T}W_{K}x_{j}\) & \(\frac{\partial F}{\partial z}\) \\ \hline Multihop Attention & K = Q, directed edges & Uniform & \(x_{i}^{T}W_{Q}^{T}W_{K}x_{j}\) & \(Vx_{j}+Vx_{k}\) \\ \hline Expanding Attention & K = Q, directed edges & Geometric x Beta & \(x_{i}^{T}W_{Q}^{T}W_{K}x_{j}\) & \(Vx_{j}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Different attention modules\(\mathbb{V}_{i,j}\). Hence the second derivative is

\[\frac{\partial^{2}A}{\partial^{2}x}=\frac{\partial E}{\partial x}^{T}\mathbb{V} \frac{\partial E}{\partial x}+\mathbb{E}[\frac{\partial^{2}E_{\phi}}{\partial x ^{2}}]\] (9)

**Second order Laplace Approximation** With these derivatives in hand we can calculate the second order laplace approximation of the free energy \(\mathcal{F}=\mathbb{E}_{q}[\ln q_{\lambda}(z)-\ln p(x,z)]\).

\[\mathcal{F} \approx\mathbb{E}_{q}[\ln p(\mu,x)+\frac{\partial}{\partial z} \ln p(\mu,x)^{T}(z-\mu)+(z-\mu)^{T}\frac{\partial^{2}}{\partial z^{2}}\ln p( \mu,x)(z-\mu)]+H[q]\] \[\approx\ln p(\mu,x)+tr(\Sigma_{q}^{-1}Var_{\phi|\mu,x}[\frac{ \partial}{\partial z}E_{\phi}])+tr(\Sigma_{q}^{-1}\mathbb{E}_{\phi|\mu,x}[ \frac{\partial^{2}}{\partial z^{2}}E_{\phi}])+\frac{1}{2}log\mid\Sigma_{q} \mid+C\]

We can see optimising the first order variational parameter in this approximation is independent of \(\Sigma_{q}\), hence we can first find \(\mu\) and the fill in our uncertainty \(\Sigma_{q}=\frac{\partial^{2}}{\partial z^{2}}\ln p(\mu^{*},x)=Var_{\phi|\mu,x }[\frac{\partial}{\partial z}E_{\phi}]+\mathbb{E}_{\phi|\mu,x}[\frac{\partial ^{2}}{\partial z^{2}}E_{\phi}]\). Finding this uncertainty can be costly in the general case where the hessian of \(E\) is not analytically available.

As alluded to in the paper, iterative attention mechanisms can also be viewed as an alternating maximisation procedure, which may provide a route to more general inference schemes:

**As Alternating Minimisation** Collapsed Inference can also be seen as co-ordinate wise variational inference [27]. Consider the family of distributions \(Q=\{q(z;\lambda)q(\phi\mid z)\}\), where \(q(z;\lambda)\) is parameterised, however \(q(\phi)\) is unconstrained.

\[\mathcal{F} =\min_{q\in Q}\mathbb{E}_{q}[\ln q(z,\phi)-\ln p(x,z,\phi)]\] \[=\min_{q\in Q}\mathbb{E}_{q(z)}[\mathbb{E}_{q(\phi)}[\ln q(\phi) -\ln p(x,\phi\mid z)]+\ln q(z)-\ln p(z)]\]

The inner expectation is maximised for \(q(\phi)=p(\phi\mid x,z)\) and the inner expectation evaluates to \(-\ln p(x\mid z)\) which recovers the marginalised objective

\[\min_{q\in Q}\mathbb{E}_{q(z)}[q(z)-\ln\sum_{\phi}p(x,z,\phi)]\]

This motivates an alternate derivation of iterative attention as structural inference which is less reliant on the Laplace approximation; Consider optimising over the variational family \(Q=\{q(z;\lambda)q(\phi)\}\) coordinate wise:

\[\ln q_{t+1}(\phi) =\mathbb{E}_{q_{t}(z;\lambda_{t})}[\ln p(\phi\mid x,z)]+C\] \[\lambda_{t+1} =arg\min_{\lambda}\mathbb{E}_{q_{t}(\phi)}[\mathbb{E}_{q(z; \lambda)}[\ln q(z)-\ln p(x,z\mid\phi)]\] \[=arg\min_{\lambda}\mathbb{E}_{q_{t}(\phi)}[\mathcal{F}_{\phi}]\]

In the case of quadratic potentials, \(q_{t+1}(\phi)=p(\phi\mid x,\lambda_{t})\), hence the combined update step can be written

\[arg\min_{\lambda}\mathbb{E}_{p(\phi|x,\lambda_{t})}[\mathcal{F}_{\phi}( \lambda)]\]

Each step necessarily reduces the free energy of the mean-field approximation, so this process converges. This derivation is independent of which approximation or estimation is used to minimise the typical variational free energy.

#### 7.1.1 Convexity details for the CCCP

Given a pairwise pMRF with quadratic potentials \(\psi(x_{i})=-\frac{1}{2}x_{i}^{2}\) and the edge potentials have the form \(\psi(x_{i},x_{j})=x_{i}Wx_{j}\) and W p.s.d., s.t. \(\ln p(x,\phi)=-\frac{1}{2}\sum_{v\in\mathcal{G}}x_{v}^{2}+\ln\sum_{\phi}\exp g _{\phi}(x)\), where \(g_{\phi}(x)=\sum_{e\in\phi}\psi_{e}\). We need the following lemma to apply the CCCP:

**Lemma 7.2**.: \(\ln\sum_{\phi}\exp g_{\phi}(x)\) _is convex in \(x\)._

Proof.: We reapply Lemma7.1, with \(E_{\phi}=g_{\phi}(x)\), hence \(\frac{\partial^{2}}{\partial x^{2}}\ln\sum_{\phi}\exp g_{\phi}(x)=Var_{\phi|x}[ \frac{\partial}{\partial x}g_{\phi}]+\mathbb{E}_{\phi|x}[\frac{\partial^{2}}{ \partial x^{2}}g_{\phi}]\). The first matrix is a variance, so p.s.d. The second term \(\mathbb{E}_{\phi|x}[\sum_{e\in\phi}\frac{\partial^{2}}{\partial x^{2}}\psi_{e}]\) is a convex sum of p.s.d matrices. Hence both terms are p.s.d, implying \(\ln\sum_{\phi}\exp g_{\phi}(x)\) is indeed convex. 

### PCN Detailed Derivation

Here we go through the derivations for the equations presented in section 4.4. PCNs typically assume a hierarchical model with gaussian residuals:

\[z_{0} \sim N(\dot{\mu_{0}},\Sigma_{0})\] \[z_{i+1} \mid z_{i} \sim N(f_{i}(z_{i};\theta_{i}),\Sigma_{i})\] \[y \mid z_{N} \sim N(f_{N}(z_{N};\theta_{N}),\Sigma_{N})\]

Under these conditions, a delta approximation of the variational free energy is given by:

\[\mathcal{F}[p,q] =\mathbb{E}_{q(z;\mu)}[-\ln p(y,z)]+H[q]\] \[\mathcal{F}(\mu,\theta) \approx\sum_{l=0}^{N}\Sigma_{l}^{-1}\epsilon_{l}^{2}\]

Where \(\epsilon_{l}=(\mu_{l+1}-f_{l}(\mu_{l};\theta_{l}))^{2}\). The inference phase involves adjusting the parameters, \(\mu\) in the direction of the gradient of \(\mathcal{F}\), which for a given layer is:

\[\frac{\partial\mathcal{F}}{\partial\mu_{l}}=\Sigma_{l-1}^{-1}\epsilon_{l-1}- \Sigma_{l}^{-1}\epsilon_{l}f^{\prime}(\mu_{l})\] (10)

Here, for ease of comparison, we consider the case where the link functions are linear, \(f_{i}(\cdot)=W_{i}(\cdot)\) and further the precision matrices are diagonal \(\Sigma_{i}^{-1}=diag(k_{i})\). Under these conditions we can write the derivative component-wise as sums of errors over incoming and outgoing edges :

\[(\frac{\partial\mathcal{F}}{\partial\mu_{l}})_{i}=-\sum_{\phi^{-}}k_{\phi} \epsilon_{\phi}+\sum_{\phi^{+}}k_{\phi}\epsilon_{\phi}w_{\phi}\]

Where \(\phi^{-},\phi^{+}\) represent the set of incoming and outgoing edges respectively, and we redefine \(\epsilon_{\phi}=(\mu_{i}-\mu_{j}w_{ij})\) for an edge \(\phi=(z_{i},z_{j})\) and \(k_{\phi}=K(z_{j})\) the precision associated with the node at the terminus of \(\phi\).

Now if we instead assume a uniform prior over incoming edges, or concretely;

\[z_{0} \sim N(\dot{\mu_{0}},\Sigma_{0})\] \[\phi_{l}^{i} \sim Uniform(\{(z_{l+1}^{i},z_{l}^{0}),(z_{l+1}^{i},z_{l}^{1}),...\}\] \[z_{l+1}^{i} \mid z_{l},\phi_{l}^{i} \sim N(w_{l}^{ij}z_{l}^{\phi_{l}^{i}},1/k_{l}^{i})\] \[y \mid z_{N} \sim N(f_{N}(z_{N};\theta_{N}),\Sigma_{N})\]

The system becomes a pMRF with edge potentials given by the prediction errors, recall applying Eq.4:

\[\frac{\partial F}{\partial\mu_{j}}=-\sum_{i}\sum_{\phi_{i}}softmax(f_{i}(x, \mu,\phi_{i}))\frac{\partial f_{i}}{\partial\mu_{j}}\]

Here for a node in a given layer, it participates in one \(\Phi_{l-1}^{j}\) and all the \(\Phi_{l+1}^{k}\) from the layer above, where every \(f_{i}(x,\mu,\phi_{i})\) here is a squared prediction error corresponding to the given edge\(k_{i}^{ij}(z_{l}^{i}-w_{l}^{ij}z_{l-1}^{j})^{2}\), hence:

\[\frac{\partial F}{\partial\mu_{j}}= -\sum_{i\in\Phi_{l-1}^{j}}softmax_{i}(-(\epsilon_{l-1}^{ij})^{2}) \epsilon_{l-1}^{ij}k_{j}\] \[+\sum_{k\in[l]}\sum_{i^{\prime}\in\Phi_{l}^{k}}softmax_{i^{\prime }}(-(\epsilon_{l}^{i^{\prime}k})^{2})\epsilon_{l}^{i^{\prime}k}w_{l}^{i^{ \prime}k}1(i^{\prime}=j)\] \[\frac{\partial F}{\partial\mu_{j}}= -\sum_{i\in\Phi_{l-1}^{j}}softmax_{i}(-(\epsilon_{l-1}^{ij})^{2}) \epsilon_{l-1}^{ij}\] \[+\sum_{k\in[l]}softmax_{i^{\prime}}(-(\epsilon_{l}^{i^{\prime}k}) ^{2})\epsilon_{l}^{i^{\prime}k}w_{l}^{i^{\prime}k}\]

Here incoming signals (nodes \(i\)) compete through the softmax, whilst the outgoing signal competes with other outgoing signals from nodes (nodes \(i^{\prime}\)) in the same layer for representation in the next layer (nodes \(k\)), see block-slot attention diagram for intuition. By abuse of notation (reindexing edges as \(\phi\))

\[\frac{\partial\mathcal{F}}{\partial\mu_{i}}=-\sum_{\phi^{-}}softmax(-{ \epsilon_{\phi}}^{2})k_{\phi}\epsilon_{\phi}+\sum_{\phi^{+}}softmax(-{ \epsilon_{\phi}}^{2})k_{\phi}\epsilon_{\phi}w_{\phi}\]

While we derived these equations for individual units to draw an easy comparison to standard Predictive Coding, we note it is likely more useful to consider blocks of units competing with each other for representation, similar to multidimensional token representations in typical attention mechanisms. We also briefly note here, the Hammersley-Clifford theorem indicates a deeper duality between attention as mediated by precision matrices and as structural inference.

### New Designs

**Multihop Derivation**\(\mathbb{E}_{y|x,\phi}[y_{i}]\) in transformer attention, a linear transformation is applied to the most likely neighbour, \(x_{j}\), of \(x_{i}\). A natural extension is to include a two-hop neighbourhood, additionally using the most likely neighbour \(x_{k}\) of \(x_{j}\). Formally, the value function \(v\) no longer neatly distributes over the partition \(\Phi_{i}\), however the attention mechanism then takes a different form: \(\mathbb{E}_{p(\phi_{j}|\phi_{i})p(\phi_{i}|x)}[V(x_{\phi_{i}}+x_{\phi_{j}})]=( P_{\phi}+P_{\phi}^{2})VX\). Where we use \(\phi_{j(i)}=\phi_{j}\) to denote the edge set of the node at the end of \(\phi_{i}\). To see this note:

\[\mathbb{E}_{p(\phi|x)}[V(x_{\phi_{i}}+x_{\phi_{j}})] =\sum_{\phi}\prod_{k}p(\phi_{k}\mid x)V(x_{\phi_{i}}+x_{\phi_{j}})\] \[=\sum_{\phi}\prod_{k}p(\phi_{k}\mid x)V(x_{\phi_{i}}+x_{\phi_{j}})\] \[=\sum_{\phi}\prod_{k}p(\phi_{k}\mid x)Vx_{\phi_{i}}+\sum_{\phi} \prod_{k}p(\phi_{k}\mid x)Vx_{\phi_{j}}\] by independence properties \[=\sum_{\phi_{i}}p(\phi_{i}\mid x)Vx_{\phi_{i}}+\sum_{\phi_{i}, \phi_{j}}p(\phi_{i}\mid x)p(\phi_{j}\mid x)Vx_{\phi_{j}}\]

Denoting the typical attention matrix, \(P\), where \(p_{ij}=p(\phi_{i}=[j]\mid x)\)

\[=\sum_{k}\sum_{j}p_{jk}p_{ij}Vx_{k}+\sum_{j}p_{ij}Vx_{j}\] \[=(P_{\phi}+P_{\phi}^{2})VX\]

**Expanding Derivation** As in the main text, let \(p(\phi\mid q)\sim Geo(q)\) and \(p(q)\sim Beta(\alpha,\beta)\), such that we have the full model \(p(x,\phi,q;\alpha,\beta)=p(x\mid\phi)p(\phi\mid q)p(q;\alpha,\beta)\). In order to find \(p(\phi\mid x)\) we employ a truncated Mean Field Variational Bayes [39], assuming a factorisation \(p_{t}(\phi,q)=p_{t}(\phi)p_{t}(q)\), and using the updates:

\[\ln p_{t+1}(\phi) =\mathbb{E}_{p_{t}(q)}[\ln p(x\mid\phi)+\ln p(\phi\mid q)]+C_{1}\] \[\ln p_{t+1}(q) =\mathbb{E}_{p_{t}(\phi)}[\ln p(\phi\mid q)+\ln p(q;\alpha,\beta)] +C_{2}\]By conjugacy the second equation simplifies to a simple update of the beta distribution

\[\implies p_{t+1}(q) =Beta(\alpha_{t+1},\beta_{t+1})\] \[\alpha_{t+1} =\alpha_{t}+1\] \[\beta_{t+1} =\beta_{t}+\mathbb{E}_{p_{t}(\phi)}[\phi]\]

While the second update can be seen as calculating the posterior given \(q_{t}=\mathbb{E}_{p_{t}(q)}[q]\),

\[\ln p_{t+1}(\phi) =\ln p(x\mid\phi)+\mathbb{E}_{p_{t}(q)}[\ln p(\phi\mid q)]+C_{2}\] \[=\ln p(x\mid\phi)+\phi\mathbb{E}_{p_{t}(q)}[\ln q]+C_{2}\] \[=\ln p(\phi\mid x,q_{t})\]

Finally, we use a truncation to approximate the infinite sum \(\mathbb{E}_{p_{t}(\phi)}[\phi]=\sum_{k}p_{t}(\phi=k)k\approx\sum_{<H}p_{t}( \phi=k)k\). Where we set the horizon according to the current distribution of \(q\). For example in our experiments we chose \(H(q_{t})=\ln 0.05/\ln(1-q_{t})\) the truncation that would capture 95% of the probability mass of the prior.

## 8 Attention Variants

Here we briefly discuss some variants of attention that there wasn't space for in the paper.

### Self Attention

* Nodes \(K=Q=(x_{1},..,x_{n})\)
* Structural prior, over a fully connected, directed graph \(p(\overrightarrow{\phi})=\prod_{i=1}^{n}p(\overrightarrow{\phi}_{i})\), where \(\overrightarrow{\Phi}_{i}=\{(x_{1},x_{i}),..,(x_{n},x_{i})\}\) is the set of edges involving \(x_{i}\) and \(\overrightarrow{\phi}_{i}\sim Uniform(\overrightarrow{\Phi}_{i})\), such that each node is uniformly likely to connect to every other node in a given direction.
* Edge potentials \(\psi(x_{j},x_{i})=x_{i}^{T}W_{Q}^{T}W_{K}x_{j}\), in effect measuring the similarity of \(x_{j}\) and \(x_{i}^{\prime}\) in a projected space.
* Value functions \(v_{i}(x,\phi_{i}=[j])=W_{V}x_{j}\), a linear transformation applied to the node at the start of the edge \(\phi_{i}\).

Again, taking the posterior expectation in each of the factors defined in two Eq.2 gives the standard self-attention mechanism

\[\mathbb{E}_{p(\phi_{i}|Q)}[v_{i}]=\sum_{j}softmax_{j}(x_{i}^{T}W_{Q}^{T}W_{K} x_{j})W_{V}x_{j}\]

### Positional Encodings and Graph Neural Networks

In Table.1 we show that positional encodings and graph attention are naturally incorporated in this framework. Absolute positional encoding as suggested by Vaswani et al. [1] can be seen as modifying the edge potentials with a vector that depends on position, while relative position encodings can be seen as a categorical prior, where the prior depends on the relative distance between nodes. Graph and Sparse attention operate similarly to graph attention, except the uniform prior is restricted to edges in the provided graph, or according to predefined sparsity pattern.

**Relative Position Encodings** If the prior over edges is categorical i.e. \(P(\phi_{i}=[j])=p_{i,j}\), it can be fully specified by the matrix \((P)_{i,j}=p_{i,j}\). This leads to the modified attention update

\[\sum_{j}softmax_{j}(x_{i}Q^{T}Kx_{j}+\ln p_{ij})x_{j}\]

However this requires local parameters for each node \(z_{i}\). A more natural prior assign a different probability to the relative distance of \(i\) from \(j\). This is achieved with \(P=circ(p_{1},p_{2},..,p_{n})\), where \(circ\) is the circulant matrix of \((p)_{i\leq n}\). Due to properties of circulant matrices \(\ln P_{ij}\) can be reparameterised with the hurley transform

\[\ln p_{i,j}=\sum_{k}\beta_{k}[cos(k\theta_{i,j})+sin(k\theta_{i,j})]=\beta \cdot b^{(i,j)}\]

Where \(b_{k}^{(i,j)}=cos(k\frac{i-j}{2\pi n})+sin(k\frac{i-j}{2\pi n})\) can be thought of as a relative position encoding, and \(\beta\) are parameters to be learnt.

#### 8.2.1 Block-Slot Attention

Singh et al. [18] suggest combining an associative memory ability with an object-centric slot-like ability and provide an iterative scheme for doing so, alternating between slot-attention and hopfield updates. Our framework permits us to flexibly combine different attention mechanisms through different latent graph structures, allowing us to derive a version of block-slot attention.

In this setting we have three sets of variables \(X\), the observations, \(Z\) the latent variables to be inferred and \(M\) which are parameters. Define the pairwise MRF \(X=\{x_{1},...,x_{n}\}\), \(Z=\{z_{1},...,z_{m}\}\) and \(M=\{m_{1},...,m_{l}\}\) with a prior over edges \(p(E)=\prod_{j=1}^{m}p(E_{j})\prod_{k=1}^{l}p(\tilde{E_{k}})\), \(E_{j}\sim Uniform\{(x_{j},z_{1}),..,(x_{j},z_{m})\}\), \(\tilde{E_{k}}\sim Uniform\{(z_{1},m_{k}),..,(z_{m},m_{k})\}\), with edge potentials between \(X\) and \(Z\) given by \(\psi(x_{j},z_{i})=z_{i}Q^{T}Kx_{j}\) and between \(Z\) and \(M\), \(\psi(z_{i},m_{k})=z_{i}\cdot m_{k}\) applying (8) gives

\[\mu_{i}^{*}= \sum_{j}softmax_{i}(\mu_{i}Q^{T}Kx_{j})Q^{T}Kx_{j}\] \[+\sum_{k}softmax_{k}(\mu_{i}\cdot m_{k})m_{k}\]

In the original block-slot attention each slot \(z_{i}\) is broken into blocks, where each block can access block-specific memories i.e. \(z_{i}^{(b)}\) can has possible connections to memory nodes \(\{m_{k}^{(b)}\}_{k\leq l}\). Allowing objects to be represented by slots which in turn disentangle features of each object in different blocks. We presented a single block version above, however it is easy to see that the update extends to the multiple block version applying (8) gives

\[\mu_{i}^{*}= \sum_{j}softmax_{i}(\mu_{i}Q^{T}Kx_{j})Q^{T}Kx_{j}\] \[+\sum_{k,b}softmax_{k}(\mu_{i}^{(b)}\cdot m_{k}^{(b)})m_{k}^{(b)}\]

## 9 Experimental Details

### Multihop

Following the notation in the text; data generation parameters:

* Total number of tokens: \(10\)
* Embedding dimension (dimension of each \(x\)):\(10\)
* Output dimension (dimension of \(y\)): \(1\)
* \(\sigma^{2}\) (autoregressive noise): \(1\)
* Random matrix initialisation was performed with torch.rand

Training parameters (across all models):

* batch size:200
* number of batches: 10
* optimiser: ADAM
* learning rate: \(1e-3\)
* Number of different random seeds: 10

Model: To make analysis easier, all models were prevented from self-attending to the final token.

### Expanding

Following the notation in the text; data generation parameters:

* Total number of tokens: \(50\)* Embedding dimension(s) (dimension of each \(x\)):[\(10\), \(50\)]
* \(p\) the parameter for generating a geometric shuffle:[\(0.5\), \(.2\), \(.1\), \(.04\)]
* Output dimension (dimension of \(y\)): \(1\)
* \(\sigma^{2}\) (autoregressive noise): \(.1\)
* Random matrix initialisation was performed with torch.rand

Training parameters (across all models):

* batch size:1*
* number of batches: \(10000\)
* optimiser: ADAM
* learning rate: \(5e-4\)

Model: To make analysis easier, all models were prevented from self-attending to the final token. For expanding attention the hyperparameters were set as \(\alpha=.1\), \(\beta=.9\) these were chosen to have a mean value at roughly a quarter of the (size 50) window.

*Training was performed with single samples, despite the iterative process being completely parallel (no shared state). Naive parallel implementation of expanding attention would encounter synchronisation locks, as the fastest samples wait for the longest ones to complete. In order to take full advantage of a dynamic window over a batch, intelligent asynchronous processing would be necessary.

```
0:\(X,W_{q},W_{k},W_{v}\) \(Q\gets W_{q}X\) \(K\gets W_{k}X\) \(V\gets W_{v}X\) \(A\gets softmax(K^{T}Q)\) \(P\leftarrow\sum_{k<N}A^{k}\) \(Y\gets PV\) ```

**Algorithm 1** Attention

```
0:\(X,x_{q},W_{q},W_{k},W_{v},N\) \(Q\gets W_{q}X\) \(K\gets W_{k}X\) \(V\gets W_{v}X\) \(A\gets softmax(K^{T}Q)\) \(P\leftarrow\sum_{k<N}A^{k}\) \(Y\gets PV\) ```

**Algorithm 2** Multihop

```
0:\(X,x_{q},W_{q},W_{k},W_{v},\alpha,\beta,k=0\) \(q\gets W_{q}x_{q}\) \(K\gets W_{k}X\) \(V\gets W_{v}X\) while\(k\) not converged do \(p=\alpha/(\alpha+\beta)\) \(k\leftarrow\ln(.05)/\ln(1-p)\) \(g_{p}[i]=-i/k\) \(A\gets softmax(q^{T}K[:k,:]+g_{p})\) \(\alpha\leftarrow\alpha+1\) \(\beta\leftarrow\beta+\sum_{i<k}A[i]i\) endwhile \(y\gets AV\) ```

**Algorithm 3** Expanding