# Mechanism Design for Collaborative Normal Mean Estimation

 Yiding Chen

UW-Madison

ychen695@wisc.edu &Xiaojin Zhu

UW-Madison

jerryzhu@cs.wisc.edu &Kirthevasan Kandasamy

UW-Madison

kandasamy@cs.wisc.edu

###### Abstract

We study collaborative normal mean estimation, where \(m\) strategic agents collect i.i.d samples from a normal distribution \(\mathcal{N}(\mu,\sigma^{2})\) at a cost. They all wish to estimate the mean \(\mu\). By sharing data with each other, agents can obtain better estimates while keeping the cost of data collection small. To facilitate this collaboration, we wish to design mechanisms that encourage agents to collect a sufficient amount of data and share it truthfully, so that they are all better off than working alone. In naive mechanisms, such as simply pooling and sharing all the data, an individual agent might find it beneficial to under-collect and/or fabricate data, which can lead to poor social outcomes. We design a novel mechanism that overcomes these challenges via two key techniques: first, when sharing the others' data with an agent, the mechanism corrupts this dataset proportional to how much the data reported by the agent differs from the others; second, we design minimax optimal estimators for the corrupted dataset. Our mechanism, which is Nash incentive compatible and individually rational, achieves a social penalty (sum of all agents' estimation errors and data collection costs) that is at most a factor 2 of the global minimum. When applied to high dimensional (non-Gaussian) distributions with bounded variance, this mechanism retains these three properties, but with slightly weaker results. Finally, in two special cases where we restrict the strategy space of the agents, we design mechanisms that essentially achieve the global minimum.

## 1 Introduction

With the rise in popularity of machine learning, data is becoming an increasingly valuable resource for businesses, scientific organizations, and government institutions. However, data collection is often costly. For instance, to collect data, businesses may need to carry out market research, scientists may need to conduct experiments, and government institutions may need to perform surveys on public services. However, once data has been generated, it can be freely replicated and used by many organizations [20]. Hence, instead of simply collecting and learning from their own data, by sharing data with each other, organizations can mutually reduce their own data collection costs and improve the utility they derive from data [21]. In fact, there are already several platforms to facilitate data sharing among businesses [1, 40], scientific organizations [2, 3], and public institutions [16, 34].

However, simply pooling everyone's data and sharing with each other can lead to free-riding [23, 35]. For instance, if an agent (e.g an organization) sees that other agents are already contributing a large amount of data, then, the cost she incurs to collect her own dataset may not offset the marginal improvement in _her own_ learned model due to diminishing returns of increasing dataset sizes (we describe this rigorously in SS2). Hence, while she benefits from others' data, she has no incentive to collect and contribute data to the pool. A seemingly simple fix to this free-riding problem is to only return the datasets of the others if an agent submits a large enough dataset herself. However, this can be easily manipulated by a strategic agent who submits a large fabricated (fake) dataset without incurring any cost, receives the others' data, and then discards her fabricated dataset when learning. While the agent has benefited by this bad behavior, other agents who may use this fabricated datasetare worse off. Moreover, a naive test by the mechanism to check if the agent has fabricated data can be sidestepped by agents who collect only a small dataset and fabricate a larger dataset using this small dataset (e.g by fitting a model to the small dataset and then sampling from this fitted model).

In this work, we study these challenges in data sharing in one of the most foundational statistical problems, normal mean estimation, where the goal is to estimate the mean \(\mu\) of a normal distribution \(\mathcal{N}(\mu,\sigma^{2})\) with known variance \(\sigma^{2}\). We wish to design _mechanisms_ for data sharing that satisfy the three fundamental desiderata of mechanism design; _Nash incentive compatibility (NIC):_ agents have incentive to collect a sufficiently large amount of data and share it truthfully provided that all other agents are doing so; _individual rationality (IR):_ agents are better off participating in the mechanism than working on their own; and _efficiency:_ the mechanism leads to outcomes with small estimation error and data collection costs for all agents.

**Contributions:**_(i)_ In SS2, we formalize collaborative normal mean estimation in the presence of strategic agents. _(ii)_ In SS3, we design an NIC and IR mechanism for this problem to prevent free-riding and data fabrication and show that its social penalty, i.e sum of all agents' estimation errors and data collection costs, is at most twice that of the global minimum. _(iii)_ In Appendix E, we study the same mechanism in high dimensional settings and relax the Gaussian assumption to distributions with bounded variance. We show that the mechanism retains its properties, with only a slight weakening of the NIC and efficiency guarantees. _(iv)_ In SS4, we consider two special cases where we impose natural restrictions on the agents' strategy space. We show that it is possible to design mechanisms which essentially achieve the global minimum social penalty in both settings. Next, we will summarize our primary mechanism and the associated theorem in SS3.

### Summary of main results

_Formalism:_ We assume that all agents have a fixed cost for collecting one sample, and define an agent's penalty (negative utility) as the sum of her estimation error and the cost she incurred to collect data. To make the problem well-defined, for the estimation error, we find it necessary to consider the _maximum risk_, i.e maximum expected error over all \(\mu\in\mathbb{R}\). A mechanism asks agents to collect data, and then shares the data among the agents in an appropriate manner to achieve the three desiderata. An agent's strategy space consists of three components: how much data she wishes to collect, what she chooses to submit after collecting the data, and how she estimates the mean \(\mu\) using the dataset she collected, the dataset she submitted, and the information she received from the mechanism.

_Mechanism and theoretical result:_ In our mechanism, which we call C3D (Cross-Check and Corrupt based on Difference), each agent \(i\) collects a dataset \(X_{i}\) and submits a possibly fabricated or altered version \(Y_{i}\) to the mechanism. The mechanism then determines agent \(i\)'s allocation in the following manner. It pools the data from the other agents and splits them into two subsets \(Z_{i},Z^{\prime}_{i}\). Then, \(Z_{i}\) is returned as is, while \(Z^{\prime}_{i}\) is corrupted by adding noise that is proportional to the difference between \(Y_{i}\) and \(Z_{i}\). If an agent collects less or fabricates, she risks looking different to the others, and will receive a dataset \(Z^{\prime}_{i}\) of poorer quality. We show that this mechanism has a Nash equilibrium where all agents collect a sufficiently large amount of data, submit it truthfully, and use a carefully weighted average of the three datasets \(X_{i},Z_{i},\) and \(Z^{\prime}_{i}\) as their estimate for \(\mu\). The weighting uses some additional side information that the mechanism provides to each agent. Below, we state an informal version of the main theoretical result of this paper, which summarizes the properties of our mechanism.

_Theorem 1 (informal):_ The above mechanism is Nash incentive compatible, individually rational, and achieves a social penalty that is at most twice the globally minimum social penalty._

Corruption is the first of two ingredients to achieving NIC. The second is the design of the weighted average estimator which is (minimax) optimal after corruption. To illustrate why this is important, say that the mechanism had assumed that the agents will use any other sub-optimal estimator (e.g a simple average). Then it will need to lower the amount of corruption to ensure IR and efficiency. However, a strategic agent will realize that she can achieve a lower maximum risk with a better estimator (instead of collecting more data herself and/or receiving less corrupt data from the mechanism). She can leverage this insight to collect less data and lower her overall penalty.

_Proof techniques:_ The most challenging part of our analysis is to show NIC, First, to show minimax optimality of our estimator, we construct a sequence of normal priors for \(\mu\) and show that the minimum Bayes' risk converges to the maximum risk of the weighted average estimator. However, when compared to typical minimax proofs, we face more significant challenges. The first of these is that the combined dataset \(X_{i}\cup Z_{i}\cup Z_{i}^{\prime}\) is neither independent nor identically distributed as the corruption is data-dependent. The second is that the agent's submission \(Y_{i}\) also determines the degree of corruption, so we cannot look at the estimator in isolation when computing the minimum Bayes' risk; we should also consider the space of functions an agent may use to determine \(Y_{i}\) from \(X_{i}\). The third is that the expressions for the minimum Bayes' risk do not have closed form solutions and require non-trivial algebraic manipulations. To complete the NIC proof, we show that due to the carefully chosen amount of corruption, the agent should collect a sufficient amount of data to avoid excessive corruption, but not too much so as to increase her data collection costs.

### Related Work

Mechanism design is one of the core areas of research in game theory [13; 18; 36]. Our work here is more related to mechanism design without payments, which has seen applications in fair division [31], matching markets [32], and kidney exchange [33] to name a few. There is a long history of work in the intersection of machine learning and mechanism design, although the overwhelming majority apply learning techniques when there is incomplete information about the mechanism or agent preferences, (e.g [6; 8; 22; 28; 30]). On the flip side, some work have designed data marketplaces, where customers may purchase data from contributors [4; 5; 19; 38]. These differ from our focus where we wish to incentivize agents to collaborate without payments.

Due to the popularity of shared data platforms [1; 2; 16; 34] and federated learning [21], there has been a recent interest in designing mechanisms for data sharing. Sim et al. [35] and Xu et al. [39] study fairness in collaborative data sharing, where the goal is to reward agents according to the amount of data they contribute. However, their mechanisms do not apply when strategic agents may try to manipulate a mechanism. Blum et al. [9] and Karimireddy et al. [23] study collaboration in federated learning. However, the strategy space of an agent is restricted to how much data they collect and their mechanism rewards each agent according to the quantity of the data she submitted. The above four works recognize that free-riding can be detrimental to data sharing, but assume that agents will not fabricate data. As discussed above, if this assumption is not true, agents can easily manipulate such mechanisms. Fraboni et al. [17] and Lin et al. [25] study federated learning settings where free-riders may send in fabricated gradients without incurring the computational cost of computing the gradients. However, their focus is on designing gradient descent algorithms that are robust to such attacks and not on incentivizing agents to perform the gradient computations. Some work have designed mechanisms for federated learning so as to elicit private information (such as data collection costs), but their focus is not on preventing free-riding or fabrication [15; 26]. Miller et al. [29] uses scoring systems to develop mechanisms that prevent signal fabrication. However, the agents in their settings can only choose to report either their true signal or something else but can not freely choose how much data to collect. Cai et al. [11] study mechanism design where a learner incentivizes agents to collect data via payments. Their mechanism, which also cross-checks the data submitted by the agents, has connections to our setting in SS4.2 where we consider a restricted strategy space for the agents.

Our approach of using corruption to engender good behaviour draws inspiration from the robust estimation literature, which design estimators that are robust to data from malicious agents [12; 14; 27]. However, to the best of our knowledge, the specific form of corruption and the subsequent design of the minimax optimal estimator are new in this work, and require novel analysis techniques.

## 2 Problem Setup

We will now formally define our problem. We have \(m\) agents, who are each able to collect i.i.d samples from a normal distribution \(\mathcal{N}(\mu,\sigma^{2})\), where \(\sigma^{2}\) is known. They wish to estimate the mean \(\mu\) of this distribution. To collect one sample, the agent has to incur a cost \(c\). We will assume that \(\sigma^{2}\), \(c\), and \(m\) are public information. However, \(\mu\in\mathbb{R}\) is unknown, and no agent has auxiliary information, such as a prior, about \(\mu\). An agent wishes to minimize her estimation error, while simultaneously keeping the cost of data collection low. While an agent may collect data on her own to manage this trade-off, by sharing data with other agents, she can reduce costs while simultaneously improving her estimate. We wish to design mechanisms to facilitate such sharing of data.

**Mechanism:** A mechanism receives a dataset from each agent, and in turn returns an _allocation_\(A_{i}\) to each agent. An agent will use her allocation to estimate \(\mu\). This allocation could be, for instance, a larger dataset obtained with other agents' datasets. The mechanism designer is free to choose a space of allocations \(\mathcal{A}\) to achieve the desired goals. Formally, we define a mechanism as a tuple \(M=(\mathcal{A},b)\) where \(\mathcal{A}\) denotes the space of allocations, and \(b\) is a procedure to map the datasets collected from the \(m\) agents to \(m\) allocations. Denoting the universal set by \(\mathcal{U}\), we write the space of mechanisms \(\mathcal{M}\) as

\[\mathcal{M}=\big{\{}M=(\mathcal{A},\,b):\ \ \mathcal{A}\subset\mathcal{U},\ \ b:(\bigcup_{n \geq 0}\mathbb{R}^{n})^{m}\rightarrow\mathcal{A}^{m}\big{\}}.\] (1)

As is customary, we will assume that the mechanism designer will publish the space of allocations \(\mathcal{A}\) and the mapping \(b\) (the procedure used to obtain the allocations) ahead of time, so that agents can determine their strategies. However, specific values computed/realized during the execution of the mechanism are not revealed, unless the mechanism chooses to do so via the allocation \(A_{i}\).

**Agents' strategy space:** Once the mechanism is published, the agent will choose a strategy. In our setting, this will be the tuple \((n_{i},f_{i},h_{i})\), which determines how much data she wishes to collect, what she chooses to submit, and how she wishes to estimate the mean \(\mu\). First, the agent samples \(n_{i}\) points to collect her initial dataset \(X_{i}=\{x_{i,j}\}_{j=1}^{n_{i}}\), where \(x_{i,j}\sim\mathcal{N}(\mu,\sigma^{2})\), incurring \(cn_{i}\) cost. She then submits \(Y_{i}=\{y_{i,j}\}_{j}=f_{i}(X_{i})\) to the mechanism. Here \(f_{i}\) is a function which maps the collected dataset to a possibly fabricated or falsified dataset of a potentially different size. In particular, this fabrication can depend on the data she has collected. For instance, the agent could collect only a small dataset, fit a Gaussian, and then sample from it.

Finally, the mechanism returns the agent's allocation \(A_{i}\), and the agent computes an estimate \(h_{i}(X_{i},Y_{i},A_{i})\) for \(\mu\) using her initial dataset \(X_{i}\), the dataset she submitted \(Y_{i}\), and the allocation she received \(A_{i}\). We include \(Y_{i}\) as part of the estimate since an agent's submission may affect the allocation she receives. Consequently, agents could try to elicit additional information about \(\mu\) via a carefully chosen \(Y_{i}\). We can write the strategy space of an agent as \(\mathcal{S}=\mathbb{N}\times\mathcal{F}\times\mathcal{H}\), where \(\mathcal{F}\) is the space of functions mapping the dataset collected to the dataset submitted, and \(\mathcal{H}\) is the space of all estimators using all the information she has. We have:

\[\mathcal{F}=\big{\{}f:\bigcup_{n\geq 0}\mathbb{R}^{n}\rightarrow\bigcup_{n \geq 0}\mathbb{R}^{n}\big{\}},\hskip 28.452756pt\mathcal{H}=\big{\{}h: \bigcup_{n\geq 0}\mathbb{R}^{n}\ \times\ \bigcup_{n\geq 0}\mathbb{R}^{n}\ \times\ \mathcal{A}\ \rightarrow\mathbb{R}\big{\}}.\] (2)

One element of interest in \(\mathcal{F}\) is the identity \(\mathbf{I}\) which maps a dataset to itself. A mechanism designer would like an agent to use \(f_{i}=\mathbf{I}\), i.e to submit the data that she collected as is, so that other agents can benefit from her data.

Going forward, when \(s=\{s_{i}\}_{i}\in\mathcal{S}^{m}\) denotes the strategies of all agents, we will use \(s_{-i}=\{s_{j}\}_{j\neq i}\) to denote the strategies of all agents except \(i\). Without loss of generality, we will assume that agent strategies are deterministic. If they are stochastic, our results will carry through for every realization of any external source of randomness that the agent uses to determine \((n_{i},f_{i},h_{i})\).

**Agent penalty:** The agent's _penalty_\(p_{i}\) (i.e negative utility) is the sum of her squared estimation error and the cost \(cn_{i}\) incurred to collect her dataset \(X_{i}\) of \(n_{i}\) points. The agent's penalty depends on the mechanism \(M\) and the strategies \(s=\{s_{j}\}_{j}\) of all the agents. Making this explicit, \(p_{i}\) is defined as:

\[p_{i}(M,s)=\sup_{\mu\in\mathbb{R}}\mathbb{E}\left[\left(h_{i}(X_{i},Y_{i},A_{i })-\mu\right)^{2}\Bigm{|}\mu\right]\ +\ cn_{i}\] (3)

The term inside the expectation is the squared difference between the agent's estimate and the true mean (conditioned on the true mean \(\mu\)). The expectation is with respect to the randomness of all agents' data and possibly any randomness in the mechanism. We consider the _maximum risk_, i.e supremum over \(\mu\in\mathbb{R}\), since the true mean \(\mu\) is unknown to the agent a priori, and their strategy should yield good estimates, and hence small penalty, over all possible values \(\mu\). To illustrate this further, note that when the value of true mean \(\mu\) is \(\mu^{\prime}\), the optimal strategy for an agent will always be to not collect any data and choose the estimator \(h_{i}(\cdot,\cdot,\cdot)=\mu^{\prime}\) leading to \(0\) penalty. However, this strategy can be meaningfully realized by an agent only if she knew that \(\mu=\mu^{\prime}\) a priori which renders the problem meaningless1. Considering the maximum risk accounts for the fact that \(\mu\) is unknown and makes the problem well-defined.

**Recommended strategies:** In addition to publishing the mechanism, the mechanism designer will recommend strategies \(s^{\star}=\{s^{\star}_{i}\}_{i}\in\mathcal{S}^{m}\) for the agents so as to incentivize collaboration and induce optimal social outcomes.

**Desiderata:** We can now define the three desiderata for a mechanism:

1. _Nash Incentive compatibility (NIC):_ A mechanism \(M=(\mathcal{A},b)\) is said to be NIC at the recommended strategy profile \(s^{\star}\) if, for each agent \(i\), and for every other alternative strategy \(s_{i}\in\mathcal{S}\) for that agent, we have \(p_{i}(M,s^{\star})\leq p_{i}(M,(s_{i},s^{\star}_{-i}))\). That is, \(s^{\star}\) is a Nash equilibrium so no agent has incentive to deviate if all other agents are following \(s^{\star}\).
2. _Individual rationality (IR):_ We say that a mechanism \(M\) is IR at \(s^{\star}\) if no agent suffers from a higher penalty by participating in the mechanism than the lowest possible penalty she could achieve on her own when all other agents are following \(s^{\star}\). If an agent does not participate, she does not submit nor receive any data from the mechanism; she will simply choose how much data to collect and design the best possible estimator. Formally, we say that a mechanism \(M\) is IR if the following is true for each agent \(i\): \[p_{i}(M,s^{\star})\leq\inf_{n_{i}\in\mathbb{N},\,h_{i}\in\mathcal{H}}\,\left\{ \sup_{\mu\in\mathbb{R}}\,\mathbb{E}\left[(h_{i}(X_{i},\varnothing,\varnothing )-\mu)^{2}\,|\,\mu\right]\ +\ cn_{i}\right\}.\] (4)
3. _Efficiency:_ The _social penalty_\(P(M,s)\) of a mechanism \(M\) when agents follow strategies \(s\), is the sum of agent penalties (defined below). We define \(\mathrm{PR}(M,s^{\star})\) to be the ratio between the social penalty of a mechanism at the recommended strategies \(s^{\star}\), and the lowest possible social penalty among all possible mechanisms and strategies (_without_ NIC or IR constraints). We have: \[P(M,s)=\sum_{i\in[m]}p_{i}(M,s),\qquad\quad\mathrm{PR}(M,s^{\star})=\frac{P(M,s^{\star})}{\inf_{M^{\prime}\in\mathcal{M},\,s\in\mathcal{S}^{m}}P(M^{\prime },s)}\] (5) Note that \(\mathrm{PR}\geq 1\). We say that a mechanism is efficient if \(\mathrm{PR}(M,s^{\star})=1\) and that it is approximately efficient if \(\mathrm{PR}(M,s^{\star})\) is bounded by some constant that does not depend on \(m\). If \(s^{\star}\) is a Nash equilibrium, then \(\mathrm{PR}(M,s^{\star})\) can be viewed as an upper bound on the price of stability [7].

For what follows, we will discuss optimal strategies for agents working on her own and present a simple mechanism which minimizes the social penalty, but has a poor Nash equilibrium.

**Optimal strategies for an agent working on her own:** Recall that, given \(n\) samples \(\{x_{i}\}_{i=1}^{n}\) from \(\mathcal{N}(\mu,\sigma^{2})\), the sample mean is a minimax optimal estimator [24]; i.e among all possible estimators \(h\), the sample mean minimizes the maximum risk \(\sup_{\mu\in\mathbb{R}}\mathbb{E}[(\mu-h(\{x_{i}\}_{i=1}^{n},\varnothing, \varnothing))^{2}\,|\,\mu]\) (note that the agent only has the dataset she collected). Moreover, its mean squared error is \(\sigma^{2}/n\) for all \(\mu\in\mathbb{R}\). Hence, an agent acting on her own will choose the sample mean and collect \(n_{i}=\sigma/\sqrt{c}\) samples so as to minimize their penalty; as long as the amount of data is less than \(\sigma/\sqrt{c}\), an agent has incentive to collect more data since the cost of collecting one more point is offset by the marginal decrease in estimation error. This can be seen via the following simple calculation:

\[\inf_{\begin{subarray}{c}n_{i}\in\mathbb{R}\\ h_{i}\in\mathcal{H}\end{subarray}}\Big{(}\sup_{\mu}\mathbb{E}\left[(h_{i}(X_{i },\varnothing,\varnothing)-\mu)^{2}\ \Big{|}\,\mu\right]\ +\ cn_{i}\Big{)}=\min_{n_{i}\in\mathbb{R}}\Big{(}\frac{\sigma^{2}}{n_{i}}+ cn_{i}\Big{)}=2\sigma\sqrt{c}\ \ \stackrel{{\Delta}}{{=}}\ p_{\min}^{\mathrm{IR}}\,.\] (6)

Let \(p_{\min}^{\mathrm{IR}}=2\sigma\sqrt{c}\) denote the lowest achievable penalty by an agent working on her own. If all \(m\) agents work independently, then the total social penalty is \(mp_{\min}^{\mathrm{IR}}=2\sigma m\sqrt{c}\). Next, we will look at a simple mechanism and an associated set of strategies which achieve the global minimum penalty. This will show that it is possible for all agents to achieve a significantly lower penalty via collaboration.

**A globally optimal mechanism _without_ strategic considerations:** The following simple mechanism \(M_{\texttt{pool}}\), pools all the data from the other agents and gives it back to an agent. Precisely, it chooses the space of allocation \(\mathcal{A}=\bigcup_{n\geq 0}\mathbb{R}^{n}\) to be datasets of arbitrary length, and sets agent \(i\)'s allocation to be \(A_{i}=\bigcup_{j\neq i}Y_{i}\). The recommended strategies \(s^{\texttt{pool}}=\{(n_{i}^{\texttt{pool}},f_{i}^{\texttt{pool}},h_{i}^{ \texttt{pool}})\}_{i}\) asks each agent to collect \(n_{i}^{\texttt{pool}}=\sigma/\sqrt{cm}\) points2, submit it as is \(f_{i}^{\texttt{pool}}=\mathbf{I}\), and use the sample mean of all pointsas her estimate \(h_{i}^{\text{\tiny{pool}}}(X_{i},X_{i},A_{i})=\frac{1}{|X_{i}\cup A_{i}|}\sum_{z\in X _{i}\cup A_{i}}z\). It is straightforward to show that this minimizes the social penalty if all agents follow \(s^{\text{\tiny{pool}}}\). After each agent has collected their datasets \(\{X_{i}\}_{i}\), the social penalty is minimized if all agents have access to each other's datasets and they all use a minimax optimal estimator: this justifies using \(M_{\text{\tiny{pool}}}\) with \(f_{i}^{\text{\tiny{pool}}}=\mathbf{I}\) and setting \(h_{i}^{\text{\tiny{pool}}}\) to be the sample mean. The following simple calculation justifies the choice of \(\sum_{i}n_{i}^{\text{\tiny{pool}}}\):

\[\inf_{s\in\mathcal{S}^{m}}\sum_{i=1}^{m}\left(\sup_{\mu}\mathbb{E}\left[(h_{i }(X_{i},f_{i},A_{i})-\mu)^{2}\ \Big{|}\,\mu\right]+cn_{i}\right)=\min_{\{n_{i}\}_{i}}\left(\frac{m\sigma^{2} }{\sum_{i}n_{i}}+c\sum_{i}n_{i}\right)=2\sigma\sqrt{mc}.\]

However, \(s^{\text{\tiny{pool}}}\) is not a Nash equilibrium of this mechanism, as an agent will find it beneficial to free-ride. If all other agents are submitting \(\sigma/\sqrt{cm}\) points, by collecting no points, an agent's penalty is \(\sigma\sqrt{mc}/(m-1)\), as she does not incur any data collection cost. This is strictly smaller than \(2\sigma\sqrt{c/m}\) when \(m\geq 3\). In fact, it is not hard to show that \(M_{\text{\tiny{pool}}}\) is at a Nash equilibrium only when the total amount of data is \(\sigma/\sqrt{c}\); for additional points, the marginal reduction in the estimation error for an individual agent does not offset her data collection costs. The social penalty at these equilibria is \(\sigma\sqrt{c}(m+1)\) which is significantly larger than the global minimum when there are many agents.

A seemingly simple way to fix this mechanism is to only return the datasets of the other agents if an agent submits at least \(\sigma/\sqrt{cm}\) points. However, as we will see in SS4.1, such a mechanism can also be manipulated by an agent who submits a fabricated dataset of \(\sigma/\sqrt{cm}\) points without actually collecting any data and incurring any cost and then discarding the fabricated dataset when estimating. Any naive test to check for the quality of the data can also be sidestepped by agents who sample only a few points, and use that to fabricate a larger dataset (e.g by sampling a large number of points from a Gaussian fitted to the small sample). Next, we will present our mechanism for this problem which satisfies all three desiderata.

## 3 Method and Results

We have outlined our mechanism \(M_{\text{\tiny{Cbn}}}\), and its interaction with the agents in Algorithm 1 in the natural order of events. We will first describe it procedurally, and then motivate our design choices. Our mechanism uses the following allocation space, \(\mathcal{A}=\bigcup_{n\geq 0}\mathbb{R}^{n}\times\bigcup_{n\geq 0}\mathbb{R}^{n} \times\mathbb{R}_{+}\). An allocation \(A_{i}=(Z_{i},Z_{i}^{\prime},\eta_{i}^{2})\in\mathcal{A}\) consists of an uncorrupted dataset \(Z_{i}\), a corrupted dataset \(Z_{i}^{\prime}\), and the variance \(\eta_{i}^{2}\) of the noise added to \(Z_{i}^{\prime}\) for corruption. Once the mechanism and the allocation space are published, agent \(i\) chooses her strategy \(s=(n_{i},f_{i},h_{i})\). She collects a dataset \(X_{i}=\{x_{i,j}\}_{j=1}^{n_{i}}\), where \(x_{i,j}\sim\mathcal{N}(\mu,\sigma^{2})\), and submits \(Y_{i}=f_{i}(X_{i})\) to the mechanism.

Our mechanism determines agent \(i\)'s allocation as follows. Let \(Y_{-i}\) be the union of all datasets submitted by the other agents. If there are at most four agents, we simply return all of the other agents' data without corruption by setting \(A_{i}\leftarrow(Y_{-i},\varnothing,0)\). If there are more agents, the mechanism first chooses a random subset of size \(\min\{|Y_{-i}|,\ \sigma/\sqrt{cm}\}\) from \(Y_{-i}\); denote this \(Z_{i}\). In line 13, the mechanism individually adds Gaussian noise to the remaining points \(Y_{-i}|Z_{i}\) to obtain \(Z_{i}^{\prime}\) (line 14). The variance \(\eta_{i}^{2}\) of the noise depends on the difference between the sample means of the subset \(Z_{i}\) and the agent's submission \(Y_{i}\). It is modulated by a value \(\alpha\), which is a function of \(c\), \(m\), and \(\sigma^{2}\). Precisely, \(\alpha\) is the smallest number larger than \(\sqrt{\sigma}(cm)^{-1/4}\) which satisfies \(G(\alpha)=0\), where:

\[G(\alpha):=\!\left(\!\frac{m-4}{m-2}\frac{4\alpha^{2}}{\sigma/\sqrt{cm}}-1\! \right)\!\frac{4\alpha}{\sqrt{\sigma}(m/c)^{1/4}}-\left(\!\left(4(m+1)\frac{ \alpha^{2}}{\sigma\sqrt{m/c}}-1\!\right)\!\sqrt{\frac{2}{2\pi}}\!\exp\!\left( \frac{\sigma\sqrt{m/c}}{8\alpha^{2}}\right)\!\operatorname{\textsc{Erfc}}\! \left(\!\frac{\sqrt{\sigma}(m/c)^{1/4}}{2\sqrt{2\alpha}}\right)\!\right)\] (7)

Finally, the mechanism returns the allocation \(A_{i}=(Z_{i},Z_{i}^{\prime},\eta_{i}^{2})\) to agent \(i\) and the agent estimates \(\mu\).

_Recommended strategies:_ The recommended strategy \(s_{i}^{\star}=(n_{i}^{\star},f_{i}^{\star},h_{i}^{\star})\) for agent \(i\) is given in (8). The agent should collect \(n_{i}^{\star}=\sigma/(m\sqrt{c})\) samples if there are at most four agents, and \(n_{i}^{\star}=\sigma/\sqrt{cm}\) samples otherwise. She should submit it without fabrication or alteration \(f_{i}=\mathbf{I}\), and then use a weighted average of the datasets \((X_{i},Z_{i},Z_{i}^{\prime})\) to estimate \(\mu\). The weighting is proportional to the inverse variance of the data. For \(X_{i}\) and \(Z_{i}\) this is simply \(\sigma^{2}\), but for \(Z_{i}^{\prime}\), the variance is \(\sigma^{2}+\eta_{i}^{2}\) since the mechanism adds Gaussian noise with variance \(\eta_{i}^{2}\). We have:

\[n_{i}^{\star}=\begin{cases}\frac{\sigma}{m\sqrt{c}}&\text{ if }m \leq 4\\ \frac{\sigma}{\sqrt{cm}}&\text{ if }m>4\end{cases},\hskip 56.905512ptf_{i}^{\star}= \mathbf{I},\] \[h_{i}^{\star}(X_{i},Y_{i},(Z_{i},Z_{i}^{\prime},\eta_{i}^{2}))= \frac{\frac{1}{\sigma^{2}}\sum_{u\in X_{i}\cup Z_{i}}u+\frac{1}{\sigma^{2}+ \eta_{i}^{2}}\sum_{u\in Z_{i}^{\prime}}u}{\frac{1}{\sigma^{2}}|X_{i}\cup Z_{i}^ {\prime}|+\frac{1}{\sigma^{2}+\eta_{i}^{2}}|Z_{i}^{\prime}|}\] (8)

_Design choices:_ Next, we will describe our design choices and highlight some key challenges. When \(m\leq 4\), it is straightforward to show that the mechanism satisfies all our desired properties (see beginning of SS3.1), so we will focus on the case \(m>4\). First, recall that the mechanism needs to incentivize agents to collect a sufficient amount of samples. However, simply counting the number of samples can be easily manipulated by an agent who simply submits a fabricated dataset of a large number of points. Instead, Algorithm 1 attempts to infer the quality of the data submitted by the agents using how well an agent's submission \(Y_{i}\) approximates \(\mu\). Ideally, we would set the variance \(\eta_{i}^{2}\) of this corruption to be proportional to the difference \((\frac{1}{|Y_{i}|}\sum_{y\in Y_{i}}y-\mu)^{2}\), so that the more data she submits, the less the variance of \(Z_{i}^{\prime}\), which in turn yields a more accurate estimate for \(\mu\). However, since \(\mu\) is unknown, we use a subset \(Z_{i}\) obtained from other agents' data as a proxy for \(\mu\), and set \(\eta_{i}^{2}\) proportional to \(\left(\frac{1}{|Y_{i}|}\sum_{y\in Y_{i}}y-\frac{1}{|Z_{i}|}\sum_{z\in Z_{i}}z \right)^{2}\). If all agents are following \(s^{\star}\), then \(|Y_{i}|=|Z_{i}|=\sigma/\sqrt{cm}=n_{i}^{\star}\); it is sufficient to use only \(n_{i}^{\star}\) points for validating \(Y_{i}\) since both \(\frac{1}{|Y_{i}|}\sum_{y\in Y_{i}}y\) and \(\frac{1}{|Z_{i}|}\sum_{z\in Z_{i}}z\) will have the same order of error in approximating \(\mu\).

The second main challenge is the design of the recommended estimator \(h_{i}^{\star}\). In SS3.1 we show how splitting \(Y_{-i}\) into a clean and corrupted parts \(Z_{i},Z_{i}^{\prime}\) allows us to design a minimax optimal estimator. A minimax optimal estimator is crucial to achieving NIC. To explain this, say that the mechanism assumes that agents will use a sub-optimal estimator, e.g sample mean of \(X_{i}\cup Z_{i}\cup Z_{i}^{\prime}\). Then, to account for the larger estimation error, it will need to choose a lower level of corruption \(\eta_{i}^{2}\) to minimize the social penalty. However, a smart agent will realize that she can achieve a lower maximum risk by using a better estimator, such as the weighted average, instead of collecting more data in order to reduce the amount of corruption used by the mechanism. She can leverage this insight to collect less data and reduce her overall penalty.

This concludes the description of our mechanism. The following theorem, which is the main theoretical result of this paper, states that \(M_{\text{c2b}}\) achieves the three desiderata outlined in SS2.

**Theorem 1**.: _Let \(m>1\), \(\alpha\) be as defined in (7), and \(s_{i}^{\star}\) be as defined in (8). Then, the following statements are true about the mechanism \(M_{\text{c2b}}\) in Algorithm 1. (i) The strategy profile \(s^{\star}\) is a Nash equilibrium. (ii) The mechanism is individually rational at \(s^{\star}\). (iii) The mechanism is approximately efficient, with \(\operatorname{\mathrm{PR}}(M_{\text{c2b}},s^{\star})\leq 2\)._

The mechanism is NIC as, provided that others are following \(s_{i}^{\star}\), there is no reason for any one agent to deviate. Moreover, we achieve low social penalty at \(s_{i}^{\star}\). Other than \(s^{\star}\), there is also a set of similar Nash equilibria with the same social penalty: the agents can each add a same constant to the data points they collect and subtract the same value from the final estimate. Before we proceed,the expression for \(\alpha\) in (7) warrants explanation. If we treat \(\alpha\) is a variable, we find that different choices of \(\alpha\) can lead to other Nash equilibria with corresponding bounds on PR. This specific choice of \(\alpha\) leads to a Nash equilibrium where agents collect \(\sigma/\sqrt{cm}\) points, and a small bound on \(\mathrm{PR}\). Throughout this manuscript, we will treat \(\alpha\) as the specific value obtained by solving (7), and _not_ as a variable.

High dimensional non-Gaussian distributions:In Appendix E, we study \(M_{\texttt{c20}}\) when applied to \(d\)-dimensional distributions. In Theorem 7, we show that under bounded variance assumptions, \(s^{\star}\) is an \(\varepsilon_{m}\)-approximate Nash equilibrium and that \(\mathrm{PR}(M_{\texttt{C30}},s^{\star})\leq 2+\varepsilon_{m}\) where \(\varepsilon_{m}\in\mathcal{O}(1/m)\).

### Proof sketch of Theorem 1

_When \(m\leq 4\):_ First, consider the (easy) case \(m\leq 4\). At \(s_{i}^{\star}\), the total amount of data collected is \(\sigma/\sqrt{c}\) (see \(n_{i}^{\star}\) in (8)), and as there is no corrupted dataset, \(h_{i}^{\star}\) simply reduces to the sample mean of \(X_{\mathsf{I}}\cup Y_{-i}\). The mechanism is IR since an agent's penalty will be \(\sigma\sqrt{c}(1+1/m)\) which is smaller than \(p_{\min}^{\mathrm{IR}}\) (6). It is approximately efficient since the social penalty is \(\sigma\sqrt{c}(m+1)\) which is at most twice the global minimum \(2\sigma\sqrt{mc}\) when \(m\leq 4\). Finally, NIC is guaranteed by the same argument used in (6); as long as the total amount of data is less than \(\sigma/\sqrt{c}\), the cost of collecting one more point is offset by the marginal decrease in the estimation error; hence, the agent is incentivized to collect more data. Moreover, as \(A_{i}\) does not depend on \(f_{i}\) under these conditions, there is no incentive to fabricate or falsify data.

_When \(m>4\):_ We will divide this proof into four parts. We first show that \(G(\alpha)=0\) in line (6) has a solution \(\alpha\) larger than \(\sqrt{n_{i}^{\star}}=\sqrt{\sigma}(cm)^{-1/4}\). This will also be useful when analyzing the efficiency.

**1. Equation** (7) **has a solution.** We derive an asymptotic expansion of \(\mathrm{Erfc}(\cdot)\) using integration by parts to analyze the solution to (7). When \(m\geq 5\), we show that \(G\big{(}\sqrt{n_{i}^{\star}}\big{)}\times G\big{(}\sqrt{n_{i}^{\star}}(1+8/ \sqrt{m})\big{)}<0\). By continuity of \(G\), there exists \(\alpha_{m}\in\big{(}\sqrt{n_{i}^{\star}},\sqrt{n_{i}^{\star}}(1+8/\sqrt{m}) \big{)}\) s.t. \(G(\alpha_{m})=0\). For \(m\) large enough such that the residual in the asymptotic expansion is negligible, we show \(\alpha_{m}\in\big{(}\sqrt{n_{i}^{\star}},\sqrt{n_{i}^{\star}}(1+\log m/m)\big{)}\) via an identical technique.

**2. The strategies \(s^{\star}\) in (8) is a Nash equilibrium:** We show this via the following two steps. First (**2.1**), We show that fixing any \(n_{i}\), the maximum risk and thus the penalty \(p_{i}\) is minimized when agent \(i\) submits the raw data and uses the weighted average as specified in (8), i.e for all \(n_{i}\),

\[p_{i}(M_{\texttt{C30}},((n_{i},f_{i}^{\star},h_{i}^{\star}),s_{-i}^{\star})) \leq p_{i}(M_{\texttt{C30}},((n_{i},f_{i},h_{i}),s_{-i}^{\star})),\ \ \forall(n_{i},f_{i},h_{i})\in\mathbb{N}\times\mathcal{F}\times\mathcal{H}.\] (9)

Second (**2.2**), we show that \(p_{i}\) is minimized when agent \(i\) collects \(n_{i}^{\star}\) samples under \((f_{i}^{\star},h_{i}^{\star})\), i.e.

\[p_{i}(M_{\texttt{C30}},((n_{i}^{\star},f_{i}^{\star},h_{i}^{\star}),s_{-i}^{ \star}))\leq p_{i}(M_{\texttt{C30}},((n_{i},f_{i}^{\star},h_{i}^{\star}),s_{-i} ^{\star})),\qquad\forall\,n_{i}\in\mathbb{N}.\] (10)

_2.1: Proof of_ (9). As the data collection cost does not change for fixed \(n_{i}\), it is sufficient to show that \((f_{i}^{\star},h_{i}^{\star})\) minimizes the maximum risk. Our proof is inspired by the following well-known recipe for proving minimax optimality of an estimator [24]: design a sequence of priors \(\{\Lambda_{\ell}\}_{\ell}\), compute the minimum Bayes' risk \(\{R_{\ell}\}_{\ell}\) for any estimator, and then show that \(R_{\ell}\) converges to the maximum risk of the proposed estimator as \(\ell\to\infty\).

To apply this recipe, we use a sequence of normal priors \(\Lambda_{\ell}=\mathcal{N}(0,\ell^{2})\) for \(\mu\). However, before we proceed, we need to handle two issues. The first of these concerns the posterior for \(\mu\) when conditioned on \((X_{i},Z_{i},Z_{i}^{\prime})\). Since the corruption terms \(\epsilon_{z,i}\) added to \(Z_{i}^{\prime}\) depend on \(X_{i}\) and \(Z_{i}\), this dataset is not independent. Moreover, as the variance \(\eta_{i}^{2}\) is the difference between two normal random variables, \(Z_{i}^{\prime}\) is not normal. Despite these, we are able to show that the posterior \(\mu|(X_{i},Z_{i},Z_{i}^{\prime})\) is normal. The second challenge is that the submission \(f_{i}\) also affects the estimation error as it determines the amount of noise \(\eta_{i}^{2}\). We handle this by viewing \(\mathcal{F}\times\mathcal{H}\) as a rich class of estimators and derive the optimal Bayes' estimator \((f_{i,\ell}^{\mathrm{B}},h_{i,\ell}^{\mathrm{B}})\in\mathcal{F}\times\mathcal{H}\) under the prior \(\Lambda_{\ell}\). We then show that the minimum Bayes' risk converges to the maximum risk when using \((f_{i}^{\star},h_{i}^{\star})\).

Next, under the prior \(\Lambda_{\ell}=\mathcal{N}(0,\ell^{2})\), we can minimize the Bayes' risk with respect to \(h_{i}\in\mathcal{H}\) by setting \(h_{i,\ell}^{\mathrm{B}}\) to be the posterior mean. Then, the minimum Bayes' risk \(R_{\ell}\) can be written as,

\[R_{\ell}=\inf_{f_{i}\in\mathcal{F}}\mathbb{E}\left[\left(|Z_{i}^{\prime}|\Big{(} \sigma^{2}+\alpha^{2}\Big{(}\frac{1}{|Y_{i}|}\sum_{y\in Y_{i}}y-\frac{1}{|Z_{i }|}\sum_{z\in Z_{i}}z\Big{)}^{2}\right)^{-1}+\frac{|X_{i}|+|Z_{i}|}{\sigma^{2}} +\frac{1}{\ell^{2}}\right)^{-1}\right]\]

Note that \(Y_{i}=f_{i}(X_{i})\) depends on \(f_{i}\). Via the Hardy-Littlewood inequality [10], we can show that the above quantity is minimized when \(f_{i,\ell}^{\mathrm{B}}\) is chosen to be a shrunk version of the agent's initial dataset \(X_{i}\), i.e \(f_{i,\ell}^{\mathrm{B}}(X_{i})=\left\{\left(1+\sigma^{2}/(|X|\ell^{2})\right) ^{-1}x,\ \forall\,x\in X_{i}\right\}\). This gives us an expression for the minimum Bayes' risk \(R_{\ell}\) under prior \(\Lambda_{\ell}\). To conclude the proof, we note that the minimum Bayes' risk under any prior is a lower bound on the maximum risk, and show that \(R_{\ell}\) approaches the maximum risk of \((f_{i}^{\star},h_{i}^{\star})\) from below. Hence, \((f_{i}^{\star},h_{i}^{\star})\) is minimax optimal for any \(n_{i}\). (Above, it is worth noting that \(f_{i,\ell}^{\mathrm{B}}\to f_{i}^{\star}=\mathbf{I}\) as \(\ell\to\infty\). In the Appendix, we also find that \(h_{i,\ell}^{\mathrm{B}}\to h_{i}^{\star}\). )

_2.2: Proof of (10)_. We can now write \(p_{i}(M_{\texttt{c2D}},((n_{i},f_{i}^{\star},h_{i}^{\star}),s_{-i}^{\star}))= R_{\infty}+cn_{i}\), where \(R_{\infty}\) is the maximum risk of \((f_{i}^{\star},h_{i}^{\star})\) (and equivalently, the limit of the minimum Bayes' risk):

\[R_{\infty}:=\mathbb{E}_{x\sim\mathcal{N}(0,1)}\bigg{[}\Big{(}(m-2)n_{i}^{ \star}\big{(}\sigma^{2}+\alpha^{2}\big{(}\sigma^{2}/n_{i}+\sigma^{2}/n_{i}^{ \star}\big{)}x^{2}\big{)}^{-1}+(n_{i}+n_{i}^{\star})\sigma^{-2}\Big{)}^{-1} \bigg{]}\]

The term inside the expectation is convex in \(n_{i}\) for each fixed \(x\). As expectation preserves convexity, we can conclude that \(p_{i}\) is a convex function of \(n_{i}\). The choice of \(\alpha\) in (7) ensures that the derivative is \(0\) at \(n^{\star}\) which implies that \(n^{\star}\) is a minimum of this function.

**3. \(M_{\texttt{c3D}}\) is individually rational at \(s^{\star}\):** This is a direct consequence of step 2 as we can show that an agent 'working on her own' is a valid strategy in \(M_{\texttt{c3D}}\).

**4. \(M_{\texttt{c3D}}\) is approximately efficient at \(s^{\star}\):** By observing that the global minimum penalty is \(2\sigma\sqrt{cm}\), we use a series of nontrivial algebraic manipulations to show \(\mathrm{PR}(M_{\texttt{c3D}},s^{\star})=\frac{1}{2}\Big{(}\frac{10\alpha^{2}/ n_{i}^{\star}-1}{4(m+1)\alpha^{2}/(mn_{i}^{\star})-1}+1\Big{)}\). As \(\alpha>\sqrt{n_{i}^{\star}}\), some simple algebra leads to \(\mathrm{PR}(M_{\texttt{c3D}},s^{\star})<2\).

## 4 Special Cases: Restricting the Agents' Strategy Space

In this section, we study two special cases motivated by some natural use cases, where we restrict the agents' strategy space. In addition to providing better guarantees on the efficiency, this will also help us better illustrate the challenges in our original setting.

### Agents cannot fabricate or falsify data

First, we study a setting where agents are not allowed to fabricate data or falsify data. Specifically, in (2), \(\mathcal{F}\) is restricted to functions which map a dataset to any subset. This is applicable when there are regulations preventing such behavior (e.g government institutions, hospitals)

_Mechanism:_ The discussion at the end of SS2 motivates the following modification to the pooling mechanism. We set the allocation space to be \(\mathcal{A}=\bigcup_{n\geq 0}\mathbb{R}^{n}\), i.e the space of all datasets. If an agent \(i\) submits at least \(\sigma/\sqrt{cm}\) points, then give her all the other agents' datasets, i.e \(A_{i}=\cup_{j\neq i}Y_{j}\); otherwise, set \(A_{i}=\varnothing\). The recommended strategy \(s_{i}^{\star}=(n_{i}^{\star},f_{i}^{\star},h_{i}^{\star})\) of each agent is to collect \(\sigma/\sqrt{cm}\) points, submit it as is \(f_{i}^{\star}=\mathbf{I}\), and then use the sample mean of \(Z_{i}\cup A_{i}\) to estimate \(\mu\). The theorem below, whose proof is straightforward, states the main properties of this mechanism.

**Theorem 2**.: _The following statements about the mechanism and strategy profile \(s^{\star}\) in the paragraph above are true when \(\mathcal{F}\) is restricted to functions which map a dataset to any subset: (i) \(s^{\star}\) is a Nash equilibrium. (ii) The mechanism is individually rational at \(s^{\star}\). (iii) At \(s^{\star}\), the mechanism is efficient._

It is not hard to see that this mechanism can be easily manipulated by the agent if there are no restrictions on \(\mathcal{F}\). As the mechanism only checks for the amount of data submitted, the agent can submit a fabricated dataset of \(\sigma/\sqrt{cm}\) points, and then discard this dataset when computing the estimate, which results in detrimental free-riding.

### Agents accept an estimated value from the mechanism

Our next setting is motivated by use cases where the mechanism may directly deploy the estimated value for \(\mu\) in some downstream application for the agent, i.e the agents are forced to use this value. This is motivated by federated learning, where agents collect and send data to a server (mechanism), which deploys a model (estimate) directly on the agent's device [9, 23]. This requires modifying the agent's strategy space to \(\mathcal{S}=\mathbb{N}\times\mathcal{F}\). Now, an agent can only choose \((n_{i},f_{i})\), how much data she wishes to collect, and how to fabricate or falsify the dataset. A mechanism is defined as a procedure \(b:\big{(}\bigcup_{n\geq 0}\mathbb{R}^{n}\big{)}^{m}\rightarrow\mathbb{R}^{m}\), which maps \(m\) datasets to \(m\) estimated mean values.

Algorithm 3 (see Appendix D) outlines a family of mechanisms parametrized by \(\epsilon>0\) for this setting. As we will see shortly, with parameter \(\epsilon\), the mechanism can achieve a \(\mathrm{PR}\) of \((1+\epsilon)\). This mechanism computes agent \(i\)'s estimate for \(\mu\) as follows. First, let \(Y_{-i}\) be the union of all datasets submitted by the other agents. Similar to Algorithm 1, the algorithm individually adds Gaussian to each \(Y_{-i}\) to obtain \(Z_{i}\) (line 10). Unlike before, this noise is added to the entire dataset and the variance \(\eta_{i}^{2}\) of this noise depends on the difference between the sample means of the agent's submission \(Y_{i}\) and all of the other agents' submissions \(Y_{-i}\). It also depends on two \(\epsilon\)-dependent parameters defined in line 6. Finally, the mechanism deploys the sample mean of \(Y_{i}\cup Z_{i}\) as the estimate for \(\mu\). The recommended strategies \(s^{*}_{i}=(n^{*}_{i},f^{*}_{i})\) for the agents is to simply collect \(n^{*}_{i}=\sigma/\sqrt{cm}\) points and submit it as is \(f^{*}_{i}=\mathbf{I}\). The following theorem states the main properties of the mechanism.

**Theorem 3**.: _Let \(\epsilon>0\). The following statements about Algorithm 3 and the strategy profile \(s^{*}\) given in the paragraph above are true: (i) \(s^{*}\) is a Nash equilibrium. (ii) The mechanism is individually rational at \(s^{*}\). (iii) At \(s^{*}\), the mechanism is approximately efficient with \(\mathrm{PR}(M,s^{*})\leq 1+\epsilon\)._

The above theorem states that it is possible to obtain a social penalty that is arbitrarily close to the global minimum under the given restriction of the strategy space. However, this mechanism is not NIC if agents are allowed to design their own estimator. For instance, if the mechanism returns \(A_{i}=Z_{i}\) (line 10), then using a weighted average of the data in \(X_{i}\) and \(Z_{i}\) yields a lower estimation error than simple average used by the mechanism (see Appendix D). An agent can leverage this insight to collect and submit less data and obtain a lower overall penalty at the expense of other agents. Cai et al. [11] study a setting where agents are incentivized to collect data and submit it truthfully via payments. Interestingly, their corruption method can be viewed as a special case of Algorithm 3 with \(k_{\epsilon}=1\) and only achieves a \(1.5\times\) factor of the global minimum social penalty. Moreover, when applied to the more general strategy space, it shares the same shortcomings as the mechanism in Theorem 3.

## 5 Conclusion

We studied collaborative normal mean estimation in the presence of strategic agents. Naive mechanisms which only look at the quantity of the dataset submitted, can be manipulated by agents who under-collect and/or fabricate data, leaving all agents worse off. To address this issue, when sharing the others' data with an agent, our mechanism \(M_{\text{c20}}\) corrupts this dataset proportional to how much the data reported by the agent differs from the other agents. We design minimax optimal estimators for this corrupted dataset to achieve a socially desirable Nash equilibrium.

**Future directions:** We believe that designing mechanisms for other collaborative learning settings may require relaxing the _exact_ NIC guarantees to make the analysis tractable. For many learning problems, it is difficult to design exactly optimal estimators, and it is common to settle for rate-optimal (i.e up to constants) estimators [24]. For instance, even simply relaxing to high dimensional distributions with bounded variance, \(M_{\text{c20}}\) can only provide an approximate NIC guarantee.

## Acknowledgment

This project is supported in part by NSF grants 1545481, 1704117, 1836978, 2023239, 2041428, 2202457, ARO MURI W911NF2110317, and AF CoE FA9550-18-1-0166.

## References

* [1] Ads Data Hub. https://developers.google.com/ads-data-hub/guides/intro. Accessed: 2022-05-10.

* [2] PubChem. https://pubchem.ncbi.nlm.nih.gov/. Accessed: 2022-05-10.
* [3] Data Coordinating Center Burton Robert 67 Jensen Mark A 53 Kahn Ari 53 Pihl Todd 53 Pot David 53 Wan Yunhu 53 and Tissue Source Site Levine Douglas A 68. The cancer genome atlas pan-cancer analysis project. _Nature genetics_, 45(10):1113-1120, 2013.
* [4] Anish Agarwal, Munther Dahleh, and Tuhin Sarkar. A marketplace for data: An algorithmic solution. In _Proceedings of the 2019 ACM Conference on Economics and Computation_, pages 701-726, 2019.
* [5] Anish Agarwal, Munther Dahleh, Thibaut Horel, and Maryann Rui. Towards data auctions with externalities. _arXiv preprint arXiv:2003.08345_, 2020.
* [6] Kareem Amin, Afshin Rostamizadeh, and Umar Syed. Learning Prices for Repeated Auctions with Strategic Buyers. In _Advances in Neural Information Processing Systems_, pages 1169-1177, 2013.
* [7] Elliot Anshelevich, Anirban Dasgupta, Jon Kleinberg, Eva Tardos, Tom Wexler, and Tim Roughgarden. The price of stability for network design with fair cost allocation. _SIAM Journal on Computing_, 38(4):1602-1623, 2008.
* [8] Susan Athey and Ilya Segal. An Efficient Dynamic Mechanism. _Econometrica_, 81(6):2463-2485, 2013.
* [9] Avrim Blum, Nika Haghtalab, Richard Lanas Phillips, and Han Shao. One for one, or all for all: Equilibria and optimality of collaboration in federated learning. In _International Conference on Machine Learning_, pages 1005-1014. PMLR, 2021.
* [10] Almut Burchard. A short course on rearrangement inequalities. _Lecture notes, IMDEA Winter School, Madrid_, 2009.
* [11] Yang Cai, Constantinos Daskalakis, and Christos Papadimitriou. Optimum statistical estimation with strategic data sources. In _Conference on Learning Theory_, pages 280-296. PMLR, 2015.
* [12] Yiding Chen, Xuezhou Zhang, Kaiqing Zhang, Mengdi Wang, and Xiaojin Zhu. Byzantine-robust online and offline distributed reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 3230-3269. PMLR, 2023.
* [13] Edward H Clarke. Multipart Pricing of Public Goods. _Public Choice_, 1971.
* [14] Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In _2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)_.
* [15] Ningning Ding, Zhixuan Fang, and Jianwei Huang. Incentive mechanism design for federated learning with multi-dimensional private information. In _2020 18th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOPT)_, pages 1-8. IEEE, 2020.
* [16] Mona Flores, Ittai Dayan, Holger Roth, Aoxiao Zhong, Ahmed Harouni, Amilcare Gentili, Anas Abidin, Andrew Liu, Anthony Costa, Bradford Wood, et al. Federated learning used for predicting outcomes in sars-cov-2 patients. _Research Square_, 2021.
* [17] Yann Fraboni, Richard Vidal, and Marco Lorenzi. Free-rider attacks on model aggregation in federated learning. In _International Conference on Artificial Intelligence and Statistics_, pages 1846-1854. PMLR, 2021.
* [18] Theodore Groves. Efficient Collective Choice when Compensation is Possible. _The Review of Economic Studies_, 1979.
* [19] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Gurel, Bo Li, Ce Zhang, Dawn Song, and Costas J Spanos. Towards efficient data valuation based on the shapley value. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1167-1176. PMLR, 2019.

* [20] Charles I Jones and Christopher Tonetti. Nonrivalry and the economics of data. _American Economic Review_, 110(9):2819-58, 2020.
* [21] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* [22] Sham M Kakade, Ilan Lobel, and Hamid Nazerzadeh. An Optimal Dynamic Mechanism for Multi-armed Bandit Processes. _arXiv preprint arXiv:1001.4598_, 2010.
* [23] Sai Praneeth Karimireddy, Wenshuo Guo, and Michael I Jordan. Mechanisms that incentivize data sharing in federated learning. _arXiv preprint arXiv:2207.04557_, 2022.
* [24] Erich L Lehmann and George Casella. _Theory of point estimation_. Springer Science & Business Media, 2006.
* [25] Jierui Lin, Min Du, and Jian Liu. Free-riders in federated learning: Attacks and defenses. _arXiv preprint arXiv:1911.12560_, 2019.
* [26] Yuan Liu, Mengmeng Tian, Yuxin Chen, Zehui Xiong, Cyril Leung, and Chunyan Miao. A contract theory based incentive mechanism for federated learning. In _Federated and Transfer Learning_, pages 117-137. Springer, 2022.
* [27] Gabor Lugosi and Shahar Mendelson. Robust multivariate mean estimation: the optimality of trimmed mean. 2021.
* [28] Yishay Mansour, Aleksandrs Slivkins, and Vasilis Syrgkanis. Bayesian Incentive-compatible Bandit Exploration. In _Proceedings of the Sixteenth ACM Conference on Economics and Computation_, pages 565-582, 2015.
* [29] Nolan Miller, Paul Resnick, and Richard Zeckhauser. Eliciting informative feedback: The peer-prediction method. _Management Science_, 51(9):1359-1373, 2005.
* [30] Hamid Nazerzadeh, Amin Saberi, and Rakesh Vohra. Dynamic Cost-per-action Mechanisms and Applications to Online Advertising. In _Proceedings of the 17th International Conference on World Wide Web_, pages 179-188, 2008.
* [31] Ariel D Procaccia. Cake Cutting: Not just Child's Play. _Communications of the ACM_, 56(7):78-87, 2013.
* [32] Alvin E Roth. On the Allocation of Residents to Rural Hospitals: A General Property of Two-sided Matching Markets. _Econometrica: Journal of the Econometric Society_, pages 425-427, 1986.
* [33] Alvin E Roth, Tayfun Sonmez, and M Utku Unver. Kidney Exchange. _The Quarterly journal of economics_, 119(2):457-488, 2004.
* [34] Micah J Sheller, G Anthony Reina, Brandon Edwards, Jason Martin, and Spyridon Bakas. Multi-institutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation. In _Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part I 4_, pages 92-104. Springer, 2019.
* [35] Rachael Hwee Ling Sim, Yehong Zhang, Mun Choon Chan, and Bryan Kian Hsiang Low. Collaborative machine learning with incentive-aware model rewards. In _International conference on machine learning_, pages 8927-8936. PMLR, 2020.
* [36] William Vickrey. Counterspeculation, Auctions, and Competitive Sealed Tenders. _The Journal of Finance_, 1961.
* [37] Abraham Wald. Contributions to the theory of statistical estimation and testing hypotheses. _The Annals of Mathematical Statistics_, 10(4):299-326, 1939.

* [38] Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song. A principled approach to data valuation for federated learning. _Federated Learning: Privacy and Incentive_, pages 153-167, 2020.
* [39] Xinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao, Chuan Sheng Foo, and Bryan Kian Hsiang Low. Gradient driven rewards to guarantee fairness in collaborative machine learning. _Advances in Neural Information Processing Systems_, 34:16104-16117, 2021.
* [40] Wenting Zheng, Raluca Ada Popa, Joseph E Gonzalez, and Ion Stoica. Helen: Maliciously secure coopetitive learning for linear models. In _2019 IEEE symposium on security and privacy (SP)_, pages 724-738. IEEE, 2019.

Proof of Theorem 1

In this section, we prove Theorem 1. This section is organized as follows. First, in SSA.1, we consider the case where \(m\leq 4\). In the remainder of this section, we will assume \(m\geq 5\). First, in SSA.2, we will show that (7) can be solved for \(\alpha\) and state some properties about the solution. Then, in SSA.3, we will prove the Nash incentive compatibility result, in SSA.4 we will prove individual rationality, and in SSA.5, we will prove the result on efficiency.

### When \(m\leq 4\)

First, consider the (easy) case \(m\leq 4\). At \(s_{i}^{\star}\), the total amount of data collected is \(\sigma/\sqrt{c}\) as each agent will be collecting \(n_{i}^{\star}=\frac{\sigma}{m\sqrt{c}}\) (see (8)). As there is no corrupted dataset, \(h_{i}^{\star}\) simply reduces to the sample mean of \(X_{i}\cup Y_{-i}\). The individual rationality property follows from the following simple calculation:

\[p_{i}(M_{\scalebox{0.5}{\text{\tiny{GB}}}},s^{\star})=\bigg{(}1+\frac{1}{m} \bigg{)}\sqrt{c}\sigma<2\sqrt{c}\sigma=p_{\min}^{\text{IR}}.\]

Similarly, the bound on the ratio between the penalties can also be obtained via the following calculation:

\[\text{PR}=\frac{m\big{(}1+\frac{1}{m}\big{)}\sqrt{c}\sigma}{2\sigma\sqrt{cm}}< \sqrt{m}\leq 2.\]

Finally, to show NIC, consider agent \(i\) and assume that all other agents have followed the recommended strategies, i.e collected \(\sigma/(m\sqrt{c})\). Then, the agent will have an _uncorrupted_ dataset \(Y_{-i}=\bigcup_{j\neq i}X_{j}\) of \(n_{-i}^{\star}=(m-1)\sigma/(m\sqrt{c})\) points with no corruption. Regardless of what she chooses to submit, the best estimator she could use with the union of this dataset \(Y_{-i}\) and the data she collects \(X_{i}\) and will be the sample mean as it is minimax optimal. The number of points that minimizes her penalty is,

\[\operatorname*{argmin}_{n_{i}}\Big{(}\sup_{\mu}\mathbb{E}\left[(h_{i}(X_{i},Y _{i},Y_{-i})-\mu)^{2}\ \Big{|}\,\mu\right]\ +\ cn_{i}\Big{)}=\operatorname*{argmin}_{n_{i}\in\mathbb{R}}\Big{(}\frac{ \sigma^{2}}{n_{i}+n_{-i}^{\star}}+cn_{i}\Big{)}=\frac{\sigma}{m\sqrt{c}}\]

Finally, as \(A_{i}\) does not depend on \(f_{i}\) under these conditions, there is no incentive to fabricate or falsify data, i.e choosing anything other than \(f^{\star}=\mathbf{I}\) does not lower her utility.

In the remainder of this section, will study the harder case, \(m\geq 4\).

### Existence of a solution to (7) and some of its properties

In this section, we show that \(G\Big{(}\frac{\sigma^{1/2}}{(cm)^{1/4}}\Big{)}<0\) and \(G\Big{(}\big{(}1+\frac{C_{m}}{m}\big{)}\frac{\sigma^{1/2}}{(cm)^{1/4}}\Big{)}>0\), where \(C_{m}=20\) when \(m\leq 20\) and \(C_{m}=5\) when \(m>20\). This means equation \(G(\alpha)=0\) has solution in \(\Big{(}\frac{\sigma^{1/2}}{(cm)^{1/4}},\ \big{(}1+\frac{C_{m}}{m}\big{)}\frac{ \sigma^{1/2}}{(cm)^{1/4}}\Big{)}\).

First, in Lemma 12, we derive an asymptotic expansion of the Gaussian complementary error function, and construct lower and upper bounds for \(G(\alpha)\) that are easier to work with. We have restated these lower (\(\operatorname{Erfc}_{\text{LB}}\)) and upper (\(\operatorname{Erfc}_{\text{UB}}\)) bounds below.

\[\operatorname{Erfc}_{\text{UB}}(x):= \frac{1}{\sqrt{\pi}}\bigg{(}\frac{\exp(-x^{2})}{x}-\frac{\exp(-x ^{2})}{2x^{3}}+\frac{3\exp(-x^{2})}{4x^{5}}\bigg{)}\] (11) \[\operatorname{Erfc}_{\text{LB}}(x):= \frac{1}{\sqrt{\pi}}\bigg{(}\frac{\exp(-x^{2})}{x}-\frac{\exp(-x ^{2})}{2x^{3}}\bigg{)}\] (12)

We can now use this to derive the following lower (\(G_{\text{LB}}\)) and upper (\(G_{\text{UB}}\)) bounds on \(G\). Here, we have used the fact that \(4(m+1)\frac{\alpha^{2}}{\sigma\sqrt{m/c}}-1>0\) when \(\alpha\geq(\sigma/\sqrt{cm})^{1/2}\). We have:

\[G_{\text{LB}}(\alpha):= \bigg{(}\frac{m-4}{m-2}\frac{4\alpha^{2}}{\sigma/\sqrt{cm}}-1 \bigg{)}\frac{4\alpha}{\sqrt{\sigma}(m/c)^{1/4}}\] \[-\bigg{(}4(m+1)\frac{\alpha^{2}}{\sigma\sqrt{m/c}}-1\bigg{)}\sqrt{ 2\pi}\exp\bigg{(}\frac{\sigma\sqrt{m/c}}{8\alpha^{2}}\bigg{)}\operatorname{ Erfc}_{\text{UB}}\bigg{(}\frac{\sqrt{\sigma}(m/c)^{1/4}}{2\sqrt{2}\alpha} \bigg{)},\]\[G_{\rm UB}(\alpha):= \bigg{(}\frac{m-4}{m-2}\frac{4\alpha^{2}}{\sigma/\sqrt{cm}}-1\bigg{)} \frac{4\alpha}{\sqrt{\sigma}(m/c)^{1/4}}\] \[-\bigg{(}4(m+1)\frac{\alpha^{2}}{\sigma\sqrt{m/c}}-1\bigg{)}\sqrt{ 2\pi}\exp\bigg{(}\frac{\sigma\sqrt{m/c}}{8\alpha^{2}}\bigg{)}\operatorname{ Erfc}_{\rm LB}\bigg{(}\frac{\sqrt{\sigma}(m/c)^{1/4}}{2\sqrt{2}\alpha}\bigg{)}.\]

By first, substituting \(\sigma/\sqrt{cm}\) for \(\alpha\) in the expressions for \(G_{\rm UB}\) and \(\operatorname{Erfc}_{\rm UB}\), and then via a sequence of algebraic manipulations, we can verify that

\[G\bigg{(}\frac{\sigma^{1/2}}{(cm)^{1/4}}\bigg{)}\leq G_{\rm UB} \bigg{(}\frac{\sigma^{1/2}}{(cm)^{1/4}}\bigg{)}\] \[= \frac{4\left(\frac{4(m-4)}{m-2}-1\right)\left(\frac{\sigma}{ \sqrt{cm}}\right)^{1/2}}{\sqrt{\sigma}\big{(}\frac{m}{c}\big{)}^{1/4}}-\sqrt {2}\left(\frac{4(m+1)}{\sqrt{\frac{m}{c}}\sqrt{cm}}-1\right)\left(\frac{2 \sqrt{2}\big{(}\frac{\sigma}{\sqrt{cm}}\big{)}^{1/2}}{\sqrt{\sigma}\big{(} \frac{m}{c}\big{)}^{1/4}}-\frac{8\sqrt{2}\left(\frac{\sigma}{\sqrt{cm}} \right)^{3/2}}{\sigma^{3/2}\left(\frac{m}{c}\right)^{3/4}}\right)\] \[= -\frac{128}{(m-2)m^{5/2}}<0.\]

Next, we will show that \(G\Big{(}\big{(}1+\frac{C_{m}}{m}\big{)}\frac{\sigma^{1/2}}{(cm)^{1/4}}\Big{)}>0\) by studying the lower bound \(G_{\rm LB}\). For \(m\in[5,500]\), we can verify individually that \(G\Big{(}\big{(}1+\frac{C_{m}}{m}\big{)}\frac{\sigma^{1/2}}{(cm)^{1/4}}\Big{)}>0\) (See Figure 1). For \(m>500\), we have:

\[G\bigg{(}\bigg{(}1+\frac{C_{m}}{m}\bigg{)}\frac{\sigma^{1/2}}{( cm)^{1/4}}\bigg{)}=G\bigg{(}\bigg{(}1+\frac{5}{m}\bigg{)}\frac{\sigma^{1/2}}{(cm)^{1/4}} \bigg{)}\geq G_{\rm LB}\bigg{(}\bigg{(}1+\frac{5}{m}\bigg{)}\frac{\sigma^{1/2} }{(cm)^{1/4}}\bigg{)}\] \[= \frac{4\left(\frac{4\left(\frac{5}{m}+1\right)^{2}(m-4)}{m-2}-1 \right)\left(\frac{5}{m}+1\right)\left(\frac{\sigma}{\sqrt{cm}}\right)^{1/2}} {\sqrt{\sigma}\big{(}\frac{m}{c}\big{)}^{1/4}}\] \[-\sqrt{2}\left(\frac{4\left(\frac{5}{m}+1\right)^{2}(m+1)}{\sqrt {\frac{m}{c}}\sqrt{cm}}-1\right)\left(\frac{2\sqrt{2}\left(\frac{5}{m}+1 \right)\left(\frac{\sigma}{\sqrt{cm}}\right)^{1/2}}{\sqrt{\sigma}\big{(} \frac{m}{c}\big{)}^{1/4}}-\frac{8\sqrt{2}\left(\frac{5}{m}+1\right)^{3}\left( \frac{\sigma}{\sqrt{cm}}\right)^{3/2}}{\sigma^{3/2}\left(\frac{m}{c}\right)^{ 3/4}}\right.\] \[+\frac{96\sqrt{2}\left(\frac{5}{m}+1\right)^{5}\left(\frac{ \sigma}{\sqrt{cm}}\right)^{5/2}}{\sigma^{5/2}\left(\frac{m}{c}\right)^{5/4}}\bigg{)}\] \[= \frac{64(m+5)^{3}\left(m^{6}-191m^{5}-1566m^{4}-3920m^{3}+2100m^{2 }+19500m+15000\right)}{(m-2)m^{21/2}}.\]

When \(m>500\),

\[m^{6}-191m^{5}-1566m^{4}-3920m^{3}=m^{3}(m^{3}-191m^{2}-1566m-3920)\]\[> m^{3}((200+200+100)m^{2}-191m^{2}-1566m-3920)\] \[> m^{3}(200m^{2}+10^{5}m+2.5\times 10^{7}-191m^{2}-1566m-3920)>0.\]

Combining the results from the two previous displays, we have, \(G\Big{(}\big{(}1+\frac{C_{m}}{m}\big{)}\frac{\sigma^{1/2}}{(cm)^{1/4}}\Big{)}>0\) which completes the proof for this section.

### Algorithm 1 is Nash incentive compatible

In this section, we will prove the following lemma which states that \(s_{i}^{\star}\), as defined in (8) is a Nash equilibrium in \(M_{\texttt{C30}}\).

**Lemma 4** (Nic).: _The recommended strategies \(s^{\star}=\{(n_{i}^{\star},f_{i}^{\star},h_{i}^{\star})\}_{i}\) as defined in (8) in mechanism \(M_{\texttt{C30}}\) (Algorithm 1) satisfies:_

\[p_{i}(M_{\texttt{C30}},s^{\star})\leq p_{i}(M_{\texttt{C30}},(s_{i},s_{-i}^{ \star}))\]

_for all \(i\in[m]\) and \(s_{i}\in\mathbb{N}\times\mathcal{F}\times\mathcal{H}\)._

The Proof of Lemma 4 relies on the following two lemmas:

**Lemma 5** (Optimal Estimation and Submission).: _For all \(i\in[m]\) and \((n_{i},f_{i},h_{i})\in\mathbb{N}\times\mathcal{F}\times\mathcal{H}\)._

\[p_{i}(M_{\texttt{C30}},((n_{i},f_{i}^{\star},h_{i}^{\star}),s_{-i}^{\star})) \leq p_{i}(M_{\texttt{C30}},((n_{i},f_{i},h_{i}),s_{-i}^{\star})).\]

See the Proof of Lemma 5 in SSA.3.1

**Lemma 6** (Optimal Sample Size).: _For all \(i\in[m]\) and \(n_{i}\in\mathbb{N}\)._

\[p_{i}(M_{\texttt{C30}},((n_{i}^{\star},f_{i}^{\star},h_{i}^{\star}),s_{-i}^{ \star}))\leq p_{i}(M_{\texttt{C30}},((n_{i},f_{i}^{\star},h_{i}^{\star}),s_{-i}^ {\star})).\]

See the Proof of Lemma 6 in SSA.3.2

Proof of Lemma 4.: By Lemma 5 and 6, we have, for all \(i\in[m]\) and \(s_{i}^{\prime}=(n_{i},f_{i},h_{i})\in\mathbb{N}\times\mathcal{F}\times \mathcal{H}\).

\[p_{i}(M_{\texttt{C20}},s^{\star})= p_{i}(M_{\texttt{C20}},((n_{i}^{\star},f_{i}^{\star},h_{i}^{ \star}),s_{-i}^{\star}))\leq p_{i}(M_{\texttt{C20}},((n_{i},f_{i}^{\star},h_{i}^ {\star}),s_{-i}^{\star}))\] \[\leq p_{i}(M_{\texttt{C20}},((n_{i},f_{i},h_{i}),s_{-i}^{\star}))=p_{i }(M_{\texttt{C20}},(s_{i}^{\prime},s_{-i}^{\star}))\]

#### a.3.1 Proof of Lemma 5

In this section, we will prove Lemma 5, which, intuitively states that, regardless of the amount of data collected, agent \(i\) should submit the data as is (\(f_{i}^{\star}=\mathbf{I}\)) and use the weighted average estimator in (8) to estimate \(\mu\). We will do so via the following three step procedure, inspired by well-known techniques for proving minimax optimality of estimators (e.g see Theorem 1.12, Chapter 5 of Lehmann and Casella [24]).

1. First, we construct a sequence of prior distributions \(\left\{\Lambda_{\ell}\right\}_{\ell\geq 1}\) for \(\mu\) and calculate the sequence of Bayesian risks under the prior distributions: \[R_{\ell}:=\inf_{f_{i}\in\mathcal{A},h_{i}\in\mathcal{H}}\mathbb{E}_{\mu\sim \Lambda_{\ell}}\Big{[}\mathbb{E}\Big{[}\big{(}h_{i}(X_{i},f_{i}(X_{i}),A_{i})- \mu\big{)}^{2}\big{|}\,\mu\Big{]}\Big{]},\quad\ell\geq 1.\]
2. Then, we will show that \(\lim_{\ell\to\infty}R_{\ell}=\sup_{\mu}\mathbb{E}\big{[}\big{(}h_{i}^{\star}(X _{i},f_{i}^{\star}(X_{i}),A_{i})-\mu\big{)}^{2}\,\big{|}\,\mu\big{]}\).
3. Finally, as the Bayesian risk is a lower bound on maximum risk, we will conclude that \((f_{i}^{\star},h_{i}^{\star})\) is minimax optimal.

Without loss of generality, we focus only on the deterministic \(f_{i}\) and \(h_{i}\). If either of them are stochastic, we can condition on the external source of randomness and treat them as deterministic functions. Our proof holds for any realization of this external source of randomness, and hence it will hold in expectation as well. Similarly, \(Z_{i}\) is randomly chosen in Algorithm 1. In the following, we condition on this randomness and the entire proof will carry through.

Note that \(Y_{i}=f_{i}(X_{i})\). We will use both of them interchangeably in the subsequent proof.

Step 1 (Bounding the Bayes' risk under the sequence of priors):We will use a sequence of normal priors \(\Lambda_{\ell}:=\mathcal{N}(0,\ell^{2})\) for all \(\ell\geq 1\). To bound the Bayes' risk under these priors, we will first note that for a fixed \(f_{i}\in\mathcal{F}\),

\[x|\mu\sim\mathcal{N}(\mu,\sigma^{2}) \forall x\in X_{i}\cup Z_{i};\] (13) \[x|\mu,\eta_{i}^{2}\sim\mathcal{N}(\mu,\sigma^{2}+\eta_{i}^{2}) \forall x\in Z_{i}^{\prime}.\] (14)

Here, recall that \(\eta_{i}^{2}\) is a function of \(Y_{i}\) and \(Z_{i}\). Because both \(Y_{i}=f_{i}(X_{i})\) and \(\eta_{i}^{2}\) are deterministic functions of \(X_{i},Z_{i}\) when \(f_{i}\) is fixed, the posterior distribution for \(\mu\) conditioned on \((X_{i},Y_{i},A_{i})\) can be calculated as follows:

\[p(\mu|X_{i},Y_{i},A_{i})=p\big{(}\mu|X_{i},Y_{i},Z_{i},Z_{i}^{ \prime},\eta_{i}^{2}\big{)}=p(\mu|X_{i},Z_{i},Z_{i}^{\prime})\] \[\propto p(\mu,X_{i},Z_{i},Z_{i}^{\prime})=p(Z_{i}^{\prime}|X_{i},Z _{i},\mu)p(X_{i},Z_{i}|\mu)p(\mu)=p(Z_{i}^{\prime}|X_{i},Z_{i},\mu)p(X_{i}|\mu )p(Z_{i}|\mu)p(\mu)\] \[\propto\exp\!\left(-\frac{1}{2(\sigma^{2}+\eta_{i}^{2})}\sum_{x \in Z_{i}^{\prime}}(x-\mu)^{2}\right)\exp\!\left(-\frac{1}{2\sigma^{2}}\sum_{ x\in X_{i}\cup Z_{i}}(x-\mu)^{2}\right)\exp\!\left(-\frac{\mu^{2}}{2\ell^{2}}\right)\] \[\propto\exp\!\left(-\frac{1}{2}\!\left(\frac{|Z_{i}^{\prime}|}{ \sigma^{2}+\eta_{i}^{2}}+\frac{|X_{i}|+|Z_{i}|}{\sigma^{2}}+\frac{1}{\ell^{2}} \right)\!\mu^{2}\right)\exp\!\left(\frac{1}{2}2\!\left(\frac{\sum_{x\in Z_{i} ^{\prime}}x}{\sigma^{2}+\eta_{i}^{2}}+\frac{\sum_{x\in X_{i}\cup Z_{i}}x}{ \sigma^{2}}\right)\!\mu\right)\] \[=\exp\!\left(-\frac{1}{2}\!\left(\frac{1}{\sigma_{\ell}^{2}}\mu^ {2}-2\frac{\mu_{\ell}}{\sigma_{\ell}^{2}}\mu\right)\right)\propto\exp\!\left(- \frac{1}{2\sigma_{\ell}^{2}}(\mu-\mu_{\ell})^{2}\right)\!,\]

where

\[\mu_{\ell}=\frac{\frac{\sum_{x\in Z_{i}^{\prime}}x}{\sigma^{2}+\eta_{i}^{2}}+ \frac{\sum_{x\in X_{i}\cup Z_{i}}x}{\sigma^{2}}}{\frac{|Z_{i}^{\prime}|}{\sigma ^{2}+\eta_{i}^{2}}+\frac{|X_{i}|+|Z_{i}|}{\frac{|X_{i}|+|Z_{i}|}{\sigma^{2}}+ \frac{1}{\ell^{2}}}},\ \ \text{and}\ \ \sigma_{\ell}^{2}=\frac{1}{\frac{|Z_{i}^{\prime}|}{\sigma^{2}+\eta_{i}^{2}}+ \frac{|X_{i}|+|Z_{i}|}{\sigma^{2}}+\frac{1}{\ell^{2}}}.\] (15)

We can therefore conclude that (despite the non i.i.d nature of the data), the posterior for \(\mu\) is Gaussian with mean and variance as shown above. We have:

\[\mu|X_{i},Y_{i},A_{i}\sim\mathcal{N}(\mu_{\ell},\sigma_{\ell}^{2}).\]

Next, following standard steps (See Corollary 1.2 in Chapter 4 of [24]), we know that \(\mathbb{E}_{\mu}\Big{[}(h_{i}(X_{i},Y_{i},A_{i})-\mu)^{2}|X_{i},Y_{i},A_{i} \Big{]}\) is minimized when \(h_{i}(X_{i},Y_{i},A_{i})=\mathbb{E}_{\mu}[\mu|X_{i},Y_{i},A_{i}]=\mu_{\ell}\). This shows that for any \(f_{i}\in h_{i}\), the optimal \(h_{i}\) is simply the posterior mean of \(\mu\) under the prior \(\Lambda_{\ell}\) conditioned on \((X_{i},f_{i}(X_{i}),A_{i})\). We can rewrite the minimum averaged risk over \(\mathcal{H}\) by switching the order of expectation:

\[\inf_{h_{i}\in\mathcal{H}}\mathbb{E}_{\mu\sim\Lambda_{\ell}}\Big{[} \mathbb{E}\Big{[}\big{(}h_{i}(X_{i},Y_{i},A_{i})-\mu\big{)}^{2}|\mu\big{]} \Big{]}\] \[=\mathbb{E}_{X_{i},Z_{i},Z_{i}^{\prime}}\Big{[}\mathbb{E}_{\mu} \Big{[}(\mu_{\ell}-\mu)^{2}|X_{i},Z_{i},Z_{i}^{\prime}\Big{]}\Big{]}=\mathbb{E} _{X_{i},Z_{i},Z_{i}^{\prime}}\Big{[}\sigma_{\ell}^{2}\Big{]}\] \[=\mathbb{E}_{X_{i},Z_{i}}\Bigg{[}\frac{1}{\frac{\big{|}Z_{i}^{ \prime}\big{|}}{\sigma^{2}+\eta_{i}^{2}}+\frac{|X_{i}|+|Z_{i}|}{\sigma^{2}}+ \frac{1}{\ell^{2}}}\Bigg{]},\] (16)

the expectation in the last step involves only \(X_{i},Z_{i}\) because \(\sigma_{\ell}^{2}\) depends only on \(X_{i},Z_{i}\) and \(|Z_{i}^{\prime}|\), but not the instantiation of \(Z_{i}^{\prime}\).

Next, we will show that (16) is minimized for the following choice of \(f_{i}\) which shrinks each points in \(X_{i}\) by an amount that depends on the prior \(\Lambda_{\ell}\)'s variance \(\ell^{2}\):

\[f_{i}(X_{i})=\left\{\frac{\left|X_{i}\right|/\sigma^{2}}{\left|X_{i}\right|/ \sigma^{2}+1/\ell^{2}}\ x\,\,\text{for each}\ x\in X_{i}\right\}.\] (17)

**Remark 1**.: _An interesting observation (albeit not critical to the proof) here is that \(f_{i}\) in (17) converges pointwise to \(f_{i}^{*}\), i.e._ **I**_, as \(\ell\to\infty\). This shows that the optimal submission function under the prior converges to \(f_{i}^{*}\). We can make a similar observation about the posterior mean in (15), where \(\mu_{\ell}\) converges to \(h_{i}^{*}\) as \(\ell\to\infty\)._To prove (17), we first define the following quantities.

\[\widehat{\mu}(X_{i}):=\frac{1}{\left|X_{i}\right|}\sum_{x\in X_{i}}x,\quad\widehat {\mu}(Y_{i}):=\frac{1}{\left|Y_{i}\right|}\sum_{x\in Y_{i}}x,\quad\widehat{\mu} (Z_{i}):=\frac{1}{\left|Z_{i}\right|}\sum_{s\in Z_{i}}x.\]

We will also find it useful to express \(\eta_{i}^{2}\) as follows. Here \(\alpha\) is as defined in (7). We have:

\[\eta_{i}^{2}=\alpha^{2}(\widehat{\mu}(Y_{i})-\widehat{\mu}(Z_{i}))^{2}\]

The following calculations show that, conditioned on \(X_{i}\), \(\widehat{\mu}(Z_{i})-\mu\) and \(\mu-\frac{\left|X_{i}\right|/\sigma^{2}}{\left|X_{i}\right|/\sigma^{2}+1/ \ell^{2}}\widehat{\mu}(X_{i})\) are independent Gaussian random variables3:

Footnote 3: This is akin to the observation that given \(u,v\sim\mathcal{N}(0,1)\), then \(u-v\) and \(u+v\) are independent.

\[p(\widehat{\mu}(Z_{i})-\mu,\mu|X_{i})\propto p(\widehat{\mu}(Z_{i})-\mu,\mu,X_{i})\] \[= p(\widehat{\mu}(Z_{i})-\mu,X_{i}|\mu)p(\mu)=p(\widehat{\mu}(Z_{i })-\mu|\mu)p(X_{i}|\mu)p(\mu)\] \[\propto \exp\!\left(-\frac{1}{2}\frac{\left|Z_{i}\right|}{\sigma^{2}}( \widehat{\mu}(Z_{i})-\mu)^{2}\right)\exp\!\left(-\frac{1}{2\sigma^{2}}\sum_{x \in X_{i}}(x-\mu)^{2}\right)\exp\!\left(-\frac{1}{2\ell^{2}}\mu^{2}\right)\] \[\propto \underbrace{\exp\!\left(-\frac{1}{2}\frac{\left|Z_{i}\right|}{ \sigma^{2}}(\widehat{\mu}(Z_{i})-\mu)^{2}\right)}_{\propto p(\widehat{\mu}(Z_ {i})-\mu|X_{i})}\underbrace{\exp\!\left(-\frac{1}{2}\!\left(\frac{\left|X_{i} \right|}{\sigma^{2}}+\frac{1}{\ell^{2}}\right)\!\left(\mu-\frac{\left|X_{i} \right|/\sigma^{2}}{\left|X_{i}\right|/\sigma^{2}+1/\ell^{2}}\widehat{\mu}(X_{ i})\right)^{2}\right)}_{\propto p\left(\mu-\frac{\left|X_{i}\right|/\sigma^{2}}{ \left|X_{i}\right|/\sigma^{2}+1/\ell^{2}}\widehat{\mu}(X_{i})|X_{i}\right)}\]

Thus conditioning on \(X_{i}\), we can write

\[\left(\mu-\frac{\left|X_{i}\right|/\sigma^{2}}{\left|X_{i}\right|/\sigma^{2}+1 /\ell^{2}}\widehat{\mu}(X_{i})\right)\sim\mathcal{N}\!\left(\begin{pmatrix}0 \\ 0\end{pmatrix},\left(\begin{matrix}\frac{\sigma^{2}}{\left|Z_{i}\right|}&0\\ 0&\frac{1}{\left|X_{i}\right|/\sigma^{2}+1/\ell^{2}}\end{matrix}\right)\right)\!.\]

which leads us to

\[\widehat{\mu}(Z_{i})-\frac{\left|X_{i}\right|/\sigma^{2}}{\left|X_{i}\right|/ \sigma^{2}+1/\ell^{2}}\widehat{\mu}(X_{i})\bigg{|}\,X_{i}\sim\mathcal{N}\! \left(\begin{matrix}0,&\frac{\sigma^{2}}{\left|Z_{i}\right|}+\frac{1}{\left|X_ {i}\right|/\sigma^{2}+1/\ell^{2}}\\ =:\widetilde{\sigma}_{\ell}^{2}\end{matrix}\right)\] (18)

Next, we will rewrite the squared difference in \(\eta_{i}^{2}\) as follows:

\[\frac{\eta_{i}^{2}}{\alpha^{2}}= (\widehat{\mu}(Y_{i})-\widehat{\mu}(Z_{i}))^{2}\] \[= \left(\underbrace{\widehat{\mu}(Z_{i})-\frac{\left|X_{i}\right|/ \sigma^{2}}{\left|X_{i}\right|/\sigma^{2}+1/\ell^{2}}\widehat{\mu}(X_{i})}_{= \widehat{\sigma}_{\ell}e}+\left(\underbrace{\frac{\left|X_{i}\right|/\sigma^{2} }{\left|X_{i}\right|/\sigma^{2}+1/\ell^{2}}\widehat{\mu}(X_{i})-\widehat{\mu}( Y_{i})}_{=:\phi(X_{i},f_{i})}\right)\right)^{2}.\]

Here, we observe that the first part of the RHS above is equal to \(\tilde{\sigma}_{\ell}\), where \(e\) is a normal noise \(e|X_{i}\sim\mathcal{N}(0,1)\) and \(\tilde{\sigma}_{\ell}\) is as defined in (18). For brevity, we will denote the second part of the RHS as \(\phi(X_{i},f_{i})\), which intuitively characterizes the difference between \(X_{i}\) and \(Y_{i}\). Importantly, \(\phi(X_{i},f_{i})=0\) when \(f_{i}\) is chosen to be (17).

Using \(e\) and \(\phi\), we can rewrite (16) using conditional expectation:

\[\mathbb{E}_{X_{i},Z_{i}}\left[\frac{1}{\frac{\left|Z_{i}^{\prime} \right|}{\sigma^{2}+\eta_{i}^{2}}+\frac{\left|X_{i}\right|+\left|Z_{i}\right|}{ \sigma^{2}}+\frac{1}{\ell^{2}}}\right]=\mathbb{E}_{X_{i}}\left[\mathbb{E}_{Z_ {i}\left|X_{i}\right.}\left[\frac{1}{\frac{\left|Z_{i}^{\prime}\right|}{\sigma^{2 }+\eta_{i}^{2}}+\frac{\left|X_{i}\right|+\left|Z_{i}\right|}{\sigma^{2}}+\frac{ 1}{\ell^{2}}}\right]\right]\] \[=\mathbb{E}_{X_{i}}\left[\mathbb{E}_{e\left|X_{i}\right.}\left[ \frac{1}{\frac{\left|Z_{i}^{\prime}\right|}{\sigma^{2}+\alpha^{2}(\tilde{\sigma}_ {\ell}e+\phi(X_{i},f_{i}))^{2}}+\frac{\left|X_{i}\right|+\left|Z_{i}\right|}{ \sigma^{2}}+\frac{1}{\ell^{2}}}\right]\right]\]\[=\mathbb{E}_{X_{i}}\left[\int_{-\infty}^{\infty}\underbrace{\frac{1}{ \frac{|Z_{i}^{\prime}|}{\sigma^{2}+\alpha^{2}\tilde{\sigma}_{\ell}^{2}(e+\phi(X_ {i},f_{i})/\tilde{\sigma}_{\ell})^{2}}+\frac{|X_{i}|+|Z_{i}|}{\sigma^{2}}+ \frac{1}{\ell^{2}}}}_{=:F_{2}(e)}\frac{1}{\sqrt{2\pi}}\exp\!\left(\!-\frac{e^ {2}}{2}\right)de\right],\] (19)

where we use the fact that \(e|X_{i}\sim\mathcal{N}(0,1)\) in the last step. To proceed, we will consider the inner expectation in the RHS above. For any fixed \(X_{i}\), \(F_{1}(\cdot)\) (as marked on the RHS) is an even function that monotonically increases on \([0,\infty)\) bounded by \(\frac{\sigma}{|X_{i}|+|Z_{i}|}\) and \(F_{2}(\cdot)\) (as marked on the RHS) is an even function that monotonically decreases on \([0,\infty)\). That means, for any \(a\in\mathbb{R}\),

\[\int_{-\infty}^{\infty}F_{1}(e-a)F_{2}(e)de\leq\int_{-\infty}^{ \infty}\frac{\sigma}{|X_{i}|+|Z_{i}|}F_{2}(e)de=\frac{\sigma}{|X_{i}|+|Z_{i}|}<\infty.\]

By a corollary of the Hardy-Littlewood inequality in Lemma 9, we have

\[\int_{-\infty}^{\infty}F_{1}(e+\phi(X_{i},f_{i})/\tilde{\sigma}_ {\ell})F_{2}(e)de\geq\int_{-\infty}^{\infty}F_{1}(e)F_{2}(e)de,\] (20)

the equality is achieved when \(\phi(X_{i},f_{i})/\tilde{\sigma}_{\ell}=0\). In particular, the equality holds when \(f_{i}\) is chosen as specified in (17).

Now, to complete Step 1, we combine (16), (19) and (20) to obtain

\[\inf_{h_{i}\in\mathcal{H}}\mathbb{E}_{\mu\sim\Lambda_{\ell}} \Big{[}\mathbb{E}\Big{[}\big{(}h_{i}(X_{i},Y_{i},A_{i})-\mu\big{)}^{2}|\mu \Big{]}\Big{]}=\mathbb{E}_{X_{i}}\bigg{[}\int_{-\infty}^{\infty}F_{1}(e+\phi(X _{i},f_{i})/\tilde{\sigma}_{\ell})F_{2}(e)de\bigg{]}\] \[\geq\mathbb{E}_{X_{i}}\bigg{[}\int_{-\infty}^{\infty}F_{1}(e)F_{ 2}(e)de\bigg{]}=\int_{-\infty}^{\infty}F_{1}(e)F_{2}(e)de,\] (21)

where the last step is because conditioning on each realization of \(X_{i}\), the term inside the expectation is a constant. Using (21), we can write the Bayes risk \(R_{\ell}\) under any prior \(\Lambda_{\ell}\) as:

\[R_{\ell}:= \inf_{f_{i}\in\mathcal{A},h_{i}\in\mathcal{H}}\mathbb{E}_{\mu\sim \Lambda_{\ell}}\Big{[}\mathbb{E}\Big{[}\big{(}h_{i}(X_{i},Y_{i},A_{i})-\mu \big{)}^{2}\big{|}\mu\Big{]}\Big{]}=\int_{-\infty}^{\infty}F_{1}(e)F_{2}(e)de\] \[= \mathbb{E}_{e\sim\mathcal{N}(0,1)}\left[\frac{1}{\frac{|Z_{i}^{ \prime}|}{\sigma^{2}+\alpha^{2}\tilde{\sigma}_{\ell}^{2}e^{2}}+\frac{|X_{i}|+ |Z_{i}|}{\sigma^{2}}+\frac{1}{\ell^{2}}}\right]\]

Because the term inside the expectation is bounded by \(\frac{\sigma^{2}}{|X_{i}|+|Z_{i}|}\) and \(\lim_{\ell\to\infty}\tilde{\sigma}_{\ell}^{2}=\frac{\sigma^{2}}{|Z_{i}|}+\frac {\sigma^{2}}{|X_{i}|}\), we can use dominated convergence theorem to show that:

\[R_{\infty}:=\lim_{\ell\to\infty}R_{\ell}=\mathbb{E}_{e\sim\mathcal{N}(0,1)} \left[\frac{1}{\frac{|Z_{i}^{\prime}|}{\sigma^{2}+\alpha^{2}\left(\frac{\sigma ^{2}}{|Z_{i}|}+\frac{\sigma^{2}}{|X_{i}|}\right)e^{2}}+\frac{|X_{i}|+|Z_{i}|}{ \sigma^{2}}}\right]\] (22)

Step 2: Maximum risk of \((f_{i}^{*},h_{i}^{*})\):Next, we will compute the maximum risk of the \((f_{i}^{*},h_{i}^{*})\) (see (8)) and show that it is equal to the RHS of (22). First note that we can write,

\[\begin{pmatrix}\widehat{\mu}(X_{i})-\mu\\ \widehat{\mu}(Z_{i})-\mu\end{pmatrix}\sim\mathcal{N}\Bigg{(}\begin{pmatrix}0 \\ 0\end{pmatrix},\begin{pmatrix}\frac{\sigma^{2}}{|X_{i}|}&0\\ 0&\frac{\sigma^{2}}{|Z_{i}|}\end{pmatrix}\Bigg{)}.\]

By a linear transformation of this Gaussian vector, we obtain

\[\begin{pmatrix}\frac{|X_{i}|}{\sigma^{2}}(\widehat{\mu}(X_{i})-\mu )+\frac{|Z_{i}|}{\sigma^{2}}(\widehat{\mu}(Z_{i})-\mu)\\ \widehat{\mu}(X_{i})-\widehat{\mu}(Z_{i})\end{pmatrix}=\begin{pmatrix}\frac{|X_{ i}|}{\sigma^{2}}&\frac{|Z_{i}|}{\sigma^{2}}\\ 1&-1\end{pmatrix}\begin{pmatrix}\widehat{\mu}(X_{i})-\mu\\ \widehat{\mu}(Z_{i})-\mu\end{pmatrix}\] \[\sim \mathcal{N}\Bigg{(}\begin{pmatrix}0\\ 0\end{pmatrix},\begin{pmatrix}\frac{|X_{i}|+|Z_{i}|}{\sigma^{2}}&0\\ 0&\frac{\sigma^{2}}{|X_{i}|}+\frac{\sigma^{2}}{|Z_{i}|}\end{pmatrix}\Bigg{)},\]which means \(\frac{|X_{i}|}{\sigma^{2}}(\widehat{\mu}(X_{i})-\mu)+\frac{|Z_{i}|}{\sigma^{2}}( \widehat{\mu}(Z_{i})-\mu)\) and \(\frac{\eta_{i}}{\sigma^{2}}=\widehat{\mu}(X_{i})-\widehat{\mu}(Z_{i})\) are independent Gaussian random variables. Therefore, the the maximum risk of \((f_{i}^{\star},h_{i}^{\star})\) is:

\[\sup_{\mu}\mathbb{E}\big{[}(h_{i}^{\star}(X_{i},Y_{i},A_{i})-\mu)^ {2}|\mu\big{]}=\sup_{\mu}\mathbb{E}_{\eta_{i}}\left[\left.\left(\frac{\frac{ \sum_{x\in\mathcal{E}_{i}^{\prime}}x}{\sigma^{2}+\eta_{i}^{2}}+\frac{|X_{i}|}{ \sigma^{2}}\widehat{\mu}(X_{i})+\frac{|Z_{i}|}{\sigma^{2}}\widehat{\mu}(Z_{i} )}{\frac{|Z_{i}^{\prime}|}{\sigma^{2}+\eta_{i}^{2}}+\frac{|X_{i}|+|Z_{i}|}{ \sigma^{2}}}-\mu\right)^{2}\right|\eta_{i}\right]\right]\] \[= \sup_{\mu}\mathbb{E}_{\eta_{i}}\left[\left.\left[\left.\left( \frac{\frac{\sum_{x\in\mathcal{E}_{i}^{\prime}}(x-\mu)}{\sigma^{2}+\eta_{i}^{ 2}}+\frac{|X_{i}|}{\sigma^{2}}(\widehat{\mu}(X_{i})-\mu)+\frac{|Z_{i}|}{\sigma ^{2}}(\widehat{\mu}(Z_{i})-\mu)}{\left(\frac{|Z_{i}^{\prime}|}{\sigma^{2}+\eta_ {i}^{2}}+\frac{|X_{i}|+|Z_{i}|}{\sigma^{2}}\right)^{2}}\right)^{2}\right|\eta_ {i}\right]\right]\right.\] \[= \sup_{\mu}\mathbb{E}_{\eta_{i}}\left[\frac{1}{\frac{\left(\frac{ |Z_{i}^{\prime}|}{\sigma^{2}+\eta_{i}^{2}}+\frac{|X_{i}|+|Z_{i}|}{\sigma^{2}} \right)^{2}}}\bigg{(}\frac{|Z_{i}^{\prime}|\left(\sigma^{2}+\eta_{i}^{2} \right)}{(\sigma^{2}+\eta_{i}^{2})^{2}}+\frac{|X_{i}|+|Z_{i}|}{\sigma^{2}} \bigg{)}\right]\] \[= \mathbb{E}_{\eta_{i}}\left[\frac{1}{\frac{\left|Z_{i}^{\prime} \right|}{\sigma^{2}+\eta_{i}^{2}}+\frac{|X_{i}|+|Z_{i}|}{\sigma^{2}}}\right]= \mathbb{E}\left[\frac{1}{\frac{|Z_{i}^{\prime}|}{\sigma^{2}+\alpha^{2}( \widehat{\mu}(Z_{i})-\widehat{\mu}(X_{i}))^{2}}+\frac{|X_{i}|+|Z_{i}|}{\sigma ^{2}}}\right]\]

Because \(\widehat{\mu}(Z_{i})-\widehat{\mu}(X_{i})\sim\mathcal{N}\Big{(}0,\frac{\sigma ^{2}}{|X_{i}|}+\frac{\sigma^{2}}{|Z_{i}|}\Big{)}\), we can further write the maximum risk as:

\[\sup_{\mu}\mathbb{E}\big{[}(h_{i}^{\star}(X_{i},Y_{i},A_{i})-\mu)^ {2}|\mu\big{]}=\mathbb{E}_{e\sim\mathcal{N}(0,1)}\left[\frac{\frac{|Z_{i}^{ \prime}|}{\sigma^{2}+\alpha^{2}\Big{(}\frac{\sigma^{2}}{|Z_{i}|}+\frac{\sigma^ {2}}{|X_{i}|}\Big{)}e^{2}}+\frac{|X_{i}|+|Z_{i}|}{\sigma^{2}}}{\frac{|X_{i}|+| Z_{i}|}{\sigma^{2}}}\right]=R_{\infty}\]

Here, we have observed that the final expression in the above equation is exactly the same as the Bayes' risk in the limit in (22) from Step 1.

Step 3: Minimax optimality of \((f_{i}^{\star},h_{i}^{\star})\):As the maximum is larger than the average, we can write, for any prior \(\Lambda_{\ell}\), and any \((f_{i},h_{i})\in\mathcal{F}\times\mathcal{H}\),

\[\sup_{\mu}\mathbb{E}\big{[}(h_{i}(X_{i},f_{i}(X_{i}),A_{i})-\mu)^ {2}|\mu\big{]}\geq\mathbb{E}_{\Lambda_{\ell}}\big{[}\mathbb{E}\big{[}(h_{i}(X _{i},f_{i}(X_{i}),A_{i})-\mu)^{2}|\mu\big{]}\big{]}\geq R_{\ell}.\]

As this is true for all \(\ell\), by taking the limit we have, for all \((f_{i},h_{i})\in\mathcal{F}\times\mathcal{H}\),

\[\sup_{\mu}\mathbb{E}\big{[}(h_{i}(X_{i},f_{i}(X_{i}),A_{i})-\mu)^ {2}|\mu\big{]}\geq R_{\infty}=\sup_{\mu}\mathbb{E}\big{[}(h_{i}^{\star}(X_{i},f_{i}^{\star}(X_{i}),A_{i})-\mu)^{2}|\mu\big{]}.\]

That is, the recommended \((f_{i}^{\star},h_{i}^{\star})\) has a smaller maximum risk than all other \((f_{i},h_{i})\in\mathcal{F}\times\mathcal{H}\). This establishes that for any \(n_{i}\),

\[p_{i}(M_{\texttt{Cab}},((n_{i},f_{i}^{\star},h_{i}^{\star}),s_{-i}^{\star}))= \inf_{f_{i}\in\mathcal{A}}\inf_{h_{i}\in\mathcal{H}}p_{i}(M_{\texttt{Cab}},((n_{i },f_{i},h_{i}),s_{-i}^{\star})).\]

#### a.3.2 Proof of Lemma 6

In the previous section, we showed that for any \(n_{i}\), the optimal \((f_{i},h_{i})\) were \((f_{i}^{\star},h_{i}^{\star})\) as given in (8). Now, we show that for the given \((f_{i}^{\star},h_{i}^{\star})\), the optimal number of samples is \(n_{i}^{\star}=\sigma/\sqrt{cm}\). For this, we will show that \(p_{i}\) is a convex function of \(n_{i}\) and then show that its gradient is \(0\) at \(n_{i}^{\star}\).

First, noting that

\[\widehat{\mu}(Z_{i})-\widehat{\mu}(X_{i})\sim\mathcal{N}\bigg{(}0,\frac{\sigma ^{2}}{|X_{i}|}+\frac{\sigma^{2}}{|Z_{i}|}\bigg{)},\]we can rewrite the penalty term as:

\[p(n_{i}):= p_{i}\big{(}M_{\text{c30}},((n_{i},f_{i}^{\star},h_{i}^{\star}),s_{-i} ^{\star})\big{)}=\mathbb{E}\left[\frac{1}{\frac{\big{|}Z_{i}^{\prime}\big{|}}{ \sigma^{2}+\alpha^{2}(\tilde{\mu}(Z_{i})-\tilde{\mu}(X_{i}))^{2}}+\frac{|X_{i }|+|Z_{i}|}{\sigma^{2}}}\right]+cn_{i}\] \[= \mathbb{E}_{x\sim\mathcal{N}(0,1)}\left[\frac{1}{\frac{\sigma^{2} +\alpha^{2}\left(\frac{\sigma^{2}}{n_{i}^{2}}+\frac{\sigma^{2}}{n_{i}^{2}} \right)x^{2}}+\frac{n_{i}+n_{i}^{\star}}{\sigma^{2}}}\right]+cn_{i}\] \[= \mathbb{E}_{x\sim\mathcal{N}(0,1)}\left[\frac{1}{\frac{\sigma^{2 }+\alpha^{2}\left(\frac{\sigma^{2}}{n_{i}^{2}}+\frac{\sigma^{2}}{n_{i}^{2}} \right)x^{2}}+\frac{n_{i}+n_{i}^{\star}}{\sigma^{2}}}\right]}+cn_{i}\] (23)

_Convexity of penalty function:_ To show that \(p(n_{i})\) is convex in \(n_{i}\), let us consider \(l(n_{i},x;\alpha)\). Fixing \(\alpha\) and \(x\), we have

\[\frac{\partial}{\partial n_{i}}l(n_{i},x;\alpha)=-\sigma^{2}\frac{1+\frac{(m-2 )n_{i}^{\star}}{\left(1+\alpha^{2}\left(\frac{1}{n_{i}}+\frac{1}{n_{i}^{\star} }\right)x^{2}\right)^{2}}\frac{\alpha^{2}x^{2}}{n_{i}^{2}}}{\left(\frac{(m-2) n_{i}^{\star}}{1+\alpha^{2}\left(\frac{1}{n_{i}}+\frac{1}{n_{i}^{\star}} \right)x^{2}}+n_{i}+n_{i}^{\star}\right)^{2}}=-\sigma^{2}\frac{1+\frac{(m-2)n _{i}^{\star}\alpha^{2}x^{2}}{\left(\frac{(m-2)n_{i}^{\star}}{1+\alpha^{2} \left(\frac{1}{n_{i}}+\frac{1}{n_{i}^{\star}}\right)x^{2}}\right)^{2}}}{\left( \frac{(m-2)n_{i}^{\star}}{1+\alpha^{2}\left(\frac{1}{n_{i}}+\frac{1}{n_{i}^{ \star}}\right)x^{2}}+n_{i}+n_{i}^{\star}\right)^{2}}\] (24)

As \(\frac{\partial}{\partial n_{i}}l(n_{i},x;\alpha)\) is an increasing function of \(n_{i}\), we have that \(l(n_{i},x;\alpha)\) is a convex function in \(n_{i}\). As expectation preserves convexity (see Lemma 10), \(p(n_{i})\) is a convex function.

_Penalty is minimized when \(n_{i}=n_{i}^{\star}\)._ Lemma 13 provides an expression for the derivative of \(p(n_{i})\) (obtained purely via algebraic manipulations). Using this, we have

\[p^{\prime}(n_{i}^{\star})= -\frac{\sigma^{2}}{64\frac{\alpha^{2}}{m-2}\frac{\alpha}{\sqrt{mn_ {i}^{\star}}}mn_{i}^{\star}}\Bigg{(}\frac{4\alpha}{\sqrt{mn_{i}^{\star}}} \Bigg{(}\frac{4\alpha^{2}m}{(m-2)n_{i}^{\star}}-1\Bigg{)}\] \[-\exp\!\left(\frac{mn_{i}^{\star}}{8\alpha^{2}}\right)\!\left( \frac{4\alpha^{2}}{mn_{i}^{\star}}(m+1)-1\right)\!\sqrt{2\pi}\operatorname{ Erfc}\!\left(\frac{1}{2\sqrt{2}\sqrt{\frac{\alpha^{2}}{mn_{i}^{\star}}}} \right)\Bigg{)}\] \[+c\] (By Lemma 13) \[= -\frac{\sigma^{2}}{64\frac{\alpha^{2}}{m-2}\frac{\alpha}{\sqrt{mn_ {i}^{\star}}}mn_{i}^{\star}}\Bigg{(}\frac{4\alpha}{\sqrt{mn_{i}^{\star}}} \Bigg{(}\frac{4\alpha^{2}(m-4)}{(m-2)n_{i}^{\star}}-1\Bigg{)}\] \[-\exp\!\left(\frac{mn_{i}^{\star}}{8\alpha^{2}}\right)\!\left( \frac{4\alpha^{2}}{mn_{i}^{\star}}(m+1)-1\right)\!\sqrt{2\pi}\operatorname{ Erfc}\!\left(\frac{1}{2\sqrt{2}\sqrt{\frac{\alpha^{2}}{mn_{i}^{\star}}}} \right)\Bigg{)}\] \[= G(\alpha)=0.\]

Here, the second step uses the fact that \(n_{i}^{\star}=\frac{\sigma}{\sqrt{cm}}\). Finally, we have observed that the expression is equal to \(G(\alpha)\) as defined in (7) which is \(0\) by our choice of \(\alpha\). Since \(p^{\prime}(n_{i}^{\star})=0\) and \(p(\cdot)\) is convex, we can conclude that \(p(n_{i})\) is minimized when \(n_{i}=n_{i}^{\star}\). Therefore,

\[p_{i}(M_{\text{c30}},((n_{i}^{\star},f_{i}^{\star},h_{i}^{\star}),s_{-i}^{\star}) )\leq p_{i}(M_{\text{c30}},((n_{i},f_{i}^{\star},h_{i}^{\star}),s_{-i}^{\star})).\]

### Algorithm 1 is individually rational

As outlined in the main text, the NIC property implies IR since 'working on her own' is a valid strategy in the mechanism. Precisely, if an agent collects any number of points \(n_{i}\), chooses not to submit anything \(f_{i}(\cdot)=\varnothing\), and then uses the sample average of the points she collected \(h_{i}(X_{i},\varnothing,A_{i})=|X_{i}|^{-1}\sum_{x\in X_{i}}x\), then \((n_{i},f_{i},h_{i})\in\mathcal{S}\).

Below, we will prove this more formally and also show that the agent's penalty is strictly smaller when participating. For any fixed \(n_{i}\), without participating in the mechanism, the smallest penalty the agent can achieve is by using empirical mean estimation and the penalty is:

\[\frac{\sigma^{2}}{n_{i}}+cn_{i}\]

When participating, the agent gets an additional \(n_{i}^{\star}\) number of clean data along with some noisy data, provided that all other agents are following \(s_{-i}^{\star}\). By using the empirical mean over the clean data, the penalty is:

\[\frac{\sigma^{2}}{n_{i}+n_{i}^{\star}}+cn_{i}<\frac{\sigma^{2}}{n_{i}}+cn_{i}\]

Now, since the weighted average estimator in \(s_{i}^{\star}\) is minimax optimal, the agent gets even smaller maximum risk and hence smaller penalty. In other words, for any \(n_{i}\),

\[p_{i}(M_{\texttt{GD}},s^{\star})\leq p_{i}(M_{\texttt{GD}},((n_{i},f_{i}^{ \star},h_{i}^{\star}),s_{-i}^{\star}))\leq\frac{\sigma^{2}}{n_{i}+n_{i}^{ \star}}+cn_{i}<\frac{\sigma^{2}}{n_{i}}+cn_{i}\]

By minimizing the RHS with respect to \(n_{i}\), we get \(p_{i}(M_{\texttt{GD}},s^{\star})<p_{\min}^{\mathrm{IR}}\). Thus Algorithm 1 is IR.

### Algorithm 1 is approximately efficient

In this section, we will bound the penalty ratio \(\mathrm{PR}\) for \(M_{\texttt{GD}}\) at the strategy profiles \(s_{i}^{\star}\).

First, noting that \(G(\alpha)=0\) (see (7)), we can rearrange the terms in the equation to obtain:

\[\exp\!\left(\frac{mn_{i}^{\star}}{8\alpha^{2}}\right)\mathrm{ Erfc}\!\left(\frac{1}{2\sqrt{2}\sqrt{\frac{\alpha^{2}}{mn_{i}^{\star}}}}\right)= \frac{1}{\sqrt{2\pi}}\frac{\frac{4\alpha}{\sqrt{mn_{i}^{\star}}}\left(\frac{4 \alpha^{2}(m-4)}{(m-2)n_{i}^{\star}}-1\right)}{\frac{4\alpha^{2}}{mn_{i}^{ \star}}(m+1)-1}\] (25)

Next, we will use the expression for \(p(n_{i})=p_{i}(M_{\texttt{GD}},(s_{-i}^{\star},(n_{i},f_{i}^{\star},h_{i}^{ \star})))\) in Lemma 13 and the equation in (25) to simplify \(p(n_{i}^{\star})\) as follows:

\[p(n_{i}^{\star})= \frac{\sqrt{\frac{\alpha^{2}}{mn_{i}^{\star}}}\sigma^{2}\!\left( 2m\sqrt{2\pi}\sqrt{\frac{\alpha^{2}}{mn_{i}^{\star}}}-(m-2)\pi\frac{1}{\sqrt{ 2\pi}}\frac{\frac{4\alpha}{(m-4)}{(m-2)n_{i}^{\star}}-1}{\frac{4\alpha^{2}}{ mn_{i}^{\star}}(m+1)-1}\right)}{4\sqrt{2\pi}\alpha^{2}}+cn_{i}^{\star}}\] (By Lemma 13) \[= \frac{\sqrt{\frac{\alpha^{2}}{mn_{i}^{\star}}}\sigma^{2}\!\left( 2m\sqrt{2\pi}\sqrt{\frac{\alpha^{2}}{mn_{i}^{\star}}}-(m-2)\pi\frac{1}{\sqrt{ 2\pi}}\frac{\frac{4\alpha^{2}}{m-4}\left(\frac{4\alpha^{2}(m-4)}{(m-2)n_{i}^{ \star}}-1\right)}{\frac{4\alpha^{2}}{mn_{i}^{\star}}(m+1)-1}\right)}{\frac{4 \alpha^{2}}{mn_{i}^{\star}}+cn_{i}^{\star}}\] (By (25)) \[= \frac{\sigma^{2}\!\left(m-(m-2)\frac{\frac{\frac{4\alpha^{2}(m-4 )}{(m-2)n_{i}^{\star}}-1}{\frac{4\alpha^{2}}{mn_{i}^{\star}}(m+1)-1}}{2mn_{i}^ {\star}}+cn_{i}^{\star}\right.}{2mn_{i}^{\star}}+cn_{i}^{\star}\] \[= \frac{\sigma^{2}}{2mn_{i}^{\star}}\frac{\frac{4\alpha^{2}}{n_{i}^ {\star}}(m+1)-m-\frac{4\alpha^{2}}{n_{i}^{\star}}(m-4)+(m-2)}{\frac{4\alpha^{2 }}{n_{i}^{\star}}\frac{m+1}{m}-1}+cn_{i}^{\star}\] \[= \frac{\sigma^{2}}{2mn_{i}^{\star}}\frac{\frac{20\alpha^{2}}{n_{i} ^{\star}}-2}{\frac{4\alpha^{2}}{n_{i}^{\star}}\frac{m+1}{m}-1}+cn_{i}^{\star} =\frac{\sigma^{2}}{mn_{i}^{\star}}\frac{\frac{10\alpha^{2}}{n_{i}^{\star}}-1}{ \frac{4\alpha^{2}}{n_{i}^{\star}}\frac{m+1}{m}-1}+cn_{i}^{\star}\]\[= \sigma\sqrt{\frac{c}{m}}\Bigg{(}\frac{\frac{10\alpha^{2}}{n_{i}^{*}}-1}{ \frac{4\alpha^{2}}{n_{i}^{*}}\frac{m+1}{m}-1}+1\Bigg{)}\]

From our conclusion in SSA.2, we have \(\alpha^{2}>\frac{\sigma}{\sqrt{cm}}=n_{i}^{*}\), i.e. \(\frac{\alpha^{2}}{n_{i}^{*}}>1\). Therefore, we have:

\[\mathrm{PR}(M_{\text{c30}},s^{*})= \frac{mp(n_{i}^{*})}{2\sigma\sqrt{cm}}=\frac{1}{2}\Bigg{(}\frac{ \frac{10\alpha^{2}}{n_{i}^{*}}-1}{\frac{4\alpha^{2}}{n_{i}^{*}}\frac{m+1}{m}- 1}+1\Bigg{)}\] \[< \frac{1}{2}\Bigg{(}\frac{\frac{10\alpha^{2}}{n_{i}^{*}}-1+\frac{1 0\alpha^{2}}{n_{i}^{*}}\frac{1}{m}+\Big{(}\frac{2\alpha^{2}}{n_{i}^{*}}\frac{ m+1}{m}-2\Big{)}}{\frac{4\alpha^{2}}{n_{i}^{*}}\frac{m+1}{m}-1}+1\Bigg{)}=2.\]

## Appendix B Proof of Theorem 2

We will use \(M_{\text{pcs}}\) to denote the mechanism in SS4.1, as it _pools_ the datsets, but _checks_ for the _size_ of the dataset submitted by each agent. For clarity, we have stated \(M_{\text{pcs}}\) algorithmically in Algorithm 2. We will also re-state the recommended strategies \(s_{i}^{*}=\{(n_{i}^{*},f_{i}^{*},h_{i}^{*})\}_{i}\) below:

\[n_{i}^{*}=\frac{\sigma}{\sqrt{cm}},\hskip 14.226378ptf_{i}^{*}=\mathbf{I}, \hskip 14.226378ptn_{i}^{*}(X_{i},Y_{i},A_{i})=\frac{1}{|X_{i}\cup A_{i}|}\sum_ {u\in X_{i}\cup A_{i}}u\] (26)

Throughout this section, \(s_{i}^{*}\) will refer to (26) (and not (8)).

We will first prove that \(s_{i}^{*}\) is a Nash equilibrium. Because the sample mean achieves minimax error for Normal mean estimation [24], we immediately have, for all \((n_{i},f_{i},h_{i})\in\mathcal{S}\).

\[p_{i}(M_{\text{pcs}},((n_{i},f_{i},h_{i}^{*}),s_{-i}^{*}))\leq p_{i}(M_{\text{ pcs}},((n_{i},f_{i},h_{i}),s_{-i}^{*})).\]

Because the agent can only submit the raw dataset or a subset, and the agent's allocation only depends on the size of the dataset, the size of the dataset she receives can always be maximized by submitting the whole data set she collects, i.e. chooses \(f_{i}=\mathbf{I}\). Therefore, we have for all \((n_{i},f_{i},h_{i})\in\mathcal{S}\),

\[p_{i}(M_{\text{pcs}},((n_{i},f_{i}^{*},h_{i}^{*}),s_{-i}^{*}))\leq p_{i}(M_{ \text{pcs}},((n_{i},f_{i},h_{i}^{*}),s_{-i}^{*}))\leq p_{i}(M_{\text{pcs}},((n_{ i},f_{i},h_{i}),s_{-i}^{*})).\]

Finally, we can use the fact that the maximum risk of the sample mean estimator using \(n\) points is \(\sigma^{2}/n\) to show that the penalty is minimized when \(n_{i}=n_{i}^{*}=\sigma/\sqrt{cm}\). In particular, we have that if \(n_{i}<\sigma/\sqrt{cm}\),

\[p_{i}(M_{\text{pcs}},((n_{i},f_{i}^{*},h_{i}^{*}),s_{-i}^{*}))=\frac{\sigma^{ 2}}{n_{i}}+cn_{i}>2\sigma\sqrt{c}.\]

And if \(n_{i}\geq\sigma/\sqrt{cm}\),

\[p_{i}(M_{\text{pcs}},((n_{i},f_{i}^{*},h_{i}^{*}),s_{-i}^{*}))= \frac{\sigma^{2}}{n_{i}+(m-1)\sigma/\sqrt{cm}}+cn_{i}\geq 2\sigma \sqrt{\frac{c}{m}}\]Because \(2\sigma\sqrt{c}\geq 2\sigma\sqrt{c/m}\), \(p_{i}(M_{\texttt{pcs}},((n_{i},f_{i}^{\star},h_{i}^{\star}),s_{-i}^{\star}))\) is minimized when \(n_{i}=\sigma/\sqrt{cm}\). We thus conclude that \(s^{\star}\) is a Nash equilibrium. That is, for all \((n_{i},f_{i},h_{i})\in\mathbb{N}\times\mathcal{F}\times\mathcal{H}\)

\[p_{i}(M_{\texttt{pcs}},s^{\star})\leq p_{i}(M_{\texttt{pcs}},((n_{i},f_{i},h_{ i}),s_{-i}^{\star})).\]

Next, the IR and efficiency properties follow trivially from the fact that \(p_{i}(M_{\texttt{pcs}},s^{\star})=2\sigma\sqrt{c/m}\) for each agent \(i\). In particular, \(p_{i}(M_{\texttt{pcs}},s^{\star})<p_{\min}^{\mathrm{IR}}\) and \(P(M_{\texttt{pcs}},s^{\star})=2\sigma\sqrt{cm}\).

## Appendix C Proof of Theorem 3

We will use \(M_{\texttt{cen}}\) to denote our mechanism in SS4.2, as it _corrupts_ the _deployed estimate_ based on the _difference_. We have stated this mechanism formally in Algorithm 3. We will also re-state the recommended strategies \(s_{i}^{\star}=\{(n_{i}^{\star},f_{i}^{\star})\}_{i}\) below:

\[n_{i}^{\star}=\frac{\sigma}{\sqrt{cm}},\qquad\quad f_{i}^{\star}=\mathbf{I}.\] (27)

Throughout this section, \(s_{i}^{\star}\) will refer to (27) (and not (8) or (26)).

We will now present the proof of Theorem 3. First, in SSC.1, we show that \(s^{\star}\) is a Nash equilibrium of \(M_{\texttt{cen}}\) as the Nash incentive compatibility result. Then, in SSC.2, we show individual rationality at \(s_{i}^{\star}\). In SSC.3, we conclude by showing that \(M_{\texttt{cen}}\) is approximately efficient by showing that its social penalty at most a \((1+\epsilon)\) factor of the global minimum.

### Algorithm 3 is Nash incentive compatible

**Step 1.** We will first show that fixing any \(n_{i}\), the best strategy is to submit the raw data, i.e. for all \((n_{i},f_{i})\in\mathbb{N}\times\mathcal{F}\).

\[p_{i}(M_{\texttt{cen}},((n_{i},f_{i}^{\star}),s_{-i}^{\star}))\leq p_{i}(M_{ \texttt{cen}},((n_{i},f_{i}),s_{-i}^{\star})).\] (28)

Let \(e_{z,i}=\epsilon_{z,i}/\eta_{i}\), where \(\eta_{i}\), and \(\epsilon_{z,i}\) are as given in lines 9 and 10 respectively. We have that \(e_{z,i}\)'s are i.i.d. standard Normal samples. Because the cost term \(cn_{i}\) is fixed when \(n_{i}\) is fixed, we only need to consider the risk term. We will first define,

\[\widehat{\mu}(X_{i}):=\frac{1}{|X_{i}|}\sum_{x\in X_{i}}x,\quad\widehat{\mu}( Y_{i}):=\frac{1}{|Y_{i}|}\sum_{x\in Y_{i}}x,\quad\widehat{\mu}(Y_{-i}):=\frac{1}{|Y_{-i }|}\sum_{x\in Y_{-i}}x.\] (29)

Via some algebraic manipulations, we can express the maximum risk as:

\[\sup_{\mu}\mathbb{E}\left[\left.\left(\frac{1}{|Y_{i}|+(m-1)n_{ i}^{\star}}\left(\sum_{y\in Y_{i}}(y-\mu)+\sum_{z\in Y_{-i}}(z+e_{z,i}\eta_{i}- \mu)\right)\right)^{2}\right|\mu\right]\] \[= \frac{1}{\left(|Y_{i}|+(m-1)n_{i}^{\star}\right)^{2}}\sup_{\mu} \mathbb{E}\left[\left.\left(\sum_{y\in Y_{i}}(y-\mu)\right)^{2}+\left(\sum_{z \in Y_{-i}}(z+e_{z,i}\eta_{i}-\mu)\right)^{2}\right|\mu\right]\] \[= \frac{1}{\left(|Y_{i}|+(m-1)n_{i}^{\star}\right)^{2}}\sup_{\mu} \mathbb{E}\left[\left.\left(|Y_{i}|\left(\widehat{\mu}(Y_{i})-\mu\right) \right)^{2}+\left(\sum_{z\in Y_{-i}}(z-\mu)\right)^{2}+\left(\sum_{z\in Y_{- i}}e_{z,i}\eta_{i}\right)^{2}\right|\mu\right]\] \[= \frac{1}{\left(|Y_{i}|+(m-1)n_{i}^{\star}\right)^{2}}\sup_{\mu} \mathbb{E}\left[\left.\left(|Y_{i}|\left(\widehat{\mu}(Y_{i})-\mu\right) \right)^{2}+(m-1)n_{i}^{\star}\beta_{\epsilon}^{2}(\widehat{\mu}(Y_{i})-\widehat {\mu}(Y_{-i}))^{2k_{\epsilon}}\right|\mu\right]\] \[+\frac{(m-1)n_{i}^{\star}\sigma^{2}}{\left(|Y_{i}|+(m-1)n_{i}^{ \star}\right)^{2}}\]

Recall that \(\beta_{\epsilon}\) also involves \(|Y_{i}|\). Note that as we have fixed \(n_{i}\) and \(s_{-i}=s_{-i}^{\star}\), the maximum risk depends only on \(|Y_{i}|\) and \(\widehat{\mu}(Y_{i})\), that is, the agent's maximum risk and hence penalty only depends on the number of points she submitted, and their average value. Hence, to find the optimal submission \(Y_{i}\), we will first fix the size of the agent's submission \(|Y_{i}|\) and optimize for the sample mean \(\widehat{\mu}(Y_{i})\) (step 1.1), and then we will optimize for \(|Y_{i}|\) (step 1.2).

**Step 1.1.** Since the other agents have each collected \(\sigma/\sqrt{cm}=n_{i}^{\star}\) points and submitted it truthfully, we have \(\widehat{\mu}(Y_{-i})\sim\mathcal{N}\Big{(}\mu_{i},\frac{\sigma^{2}}{(m-1)n_{ i}^{\star}}\Big{)}\). Via a binomial expansion, we can write,

\[\mathbb{E}\Big{[}(\widehat{\mu}(Y_{i})-\widehat{\mu}(Y_{-i}))^{2 k_{\epsilon}}\Big{]}= \mathbb{E}\Big{[}((\widehat{\mu}(Y_{i})-\mu)-(\widehat{\mu}(Y_{-i})- \mu))^{2k_{\epsilon}}\Big{]}\] \[= \sum_{j=0}^{2k_{\epsilon}}(-1)^{j}\binom{2k_{\epsilon}}{j} \mathbb{E}\Big{[}(\widehat{\mu}(Y_{i})-\mu)^{j}\Big{]}\mathbb{E}\Big{[}( \widehat{\mu}(Y_{-i})-\mu)^{2k_{\epsilon}-j}\Big{]}\] \[= \sum_{j=0}^{k_{\epsilon}}\binom{2k_{\epsilon}}{2j}\mathbb{E} \Big{[}(\widehat{\mu}(Y_{i})-\mu)^{2j}\Big{]}\mathbb{E}\Big{[}(\widehat{\mu} (Y_{-i})-\mu)^{2k_{\epsilon}-2j}\Big{]}\]

Thus the maximum risk can be written as:

\[\sup_{\mu}\mathbb{E}\Bigg{[}\sum_{j=0}^{k_{\epsilon}}A_{j}(\widehat{\mu}(Y_{i} )-\mu)^{2j}\Bigg{|}\mu\Bigg{]}\] (30)

where \(A_{0},\dots,A_{k_{\epsilon}}\) is a sequence of positive coefficients.

Similar to the proof of Theorem 1, we construct a lower bound on the maximum risk using a sequence of Bayesian risks. Let \(\Lambda_{\ell}:=\mathcal{N}(0,\ell^{2})\), \(\ell=1,2,\dots\) be a sequence of prior for \(\mu\). For fixed \(\ell\), the posterior distribution is:

\[p(\mu|X_{i})\propto p(X_{i}|\mu)p(\mu)\propto\exp\Biggl{(}-\frac{1}{2\sigma^{2}} \sum_{x\in X_{i}}(x-\mu)^{2}\Biggr{)}\exp\biggl{(}-\frac{1}{2\ell^{2}}\mu^{2} \Biggr{)}\] \[\propto \exp\biggl{(}-\frac{1}{2}\bigg{(}\frac{n_{i}}{\sigma^{2}}+\frac{ 1}{\ell^{2}}\bigg{)}\mu^{2}+\frac{1}{2}2\frac{\sum_{x\in X_{i}}x}{\sigma^{2}} \mu\bigg{)}.\]

This means the posterior of \(\mu\) given \(X_{i}\) is Gaussian with:

\[\mu|X_{i}\sim\mathcal{N}\bigg{(}\frac{n_{i}\widehat{\mu}(X_{i})/\sigma^{2}}{ n_{i}/\sigma^{2}+1/\ell^{2}},\frac{1}{n_{i}/\sigma^{2}+1/\ell^{2}}\bigg{)}=: \mathcal{N}\big{(}\mu_{\ell},\sigma_{\ell}^{2}\big{)}.\]

Therefore, the posterior risk is:

\[\mathbb{E}\Bigg{[}\sum_{j=0}^{k_{\epsilon}}A_{j}(\widehat{\mu}(Y_ {i})-\mu)^{2j}\Bigg{|}X_{i}\Bigg{]}= \mathbb{E}\Bigg{[}\sum_{j=0}^{k_{\epsilon}}A_{j}((\widehat{\mu}(Y _{i})-\mu_{\ell})-(\mu-\mu_{\ell}))^{2j}\Bigg{|}X_{i}\Bigg{]}\]\[= \int_{-\infty}^{\infty}\underbrace{\sum_{j=0}^{k_{\epsilon}}A_{j}(e-( \widehat{\mu}(Y_{i})-\mu_{\ell}))^{2j}}_{=:F_{1}(e-(\widehat{\mu}(Y_{i})-\mu_{ \ell}))}\frac{1}{\sigma_{\ell}\sqrt{2\pi}}\exp\!\left(-\frac{e^{2}}{2\sigma_{ \ell}^{2}}\right)de\]

Because:

* \(F_{1}(\cdot)\) is even function and increases on \([0,\infty)\);
* \(F_{2}(\cdot)\) is even function and decreases on \([0,\infty\), and \(\int_{\mathbb{R}}F_{2}(e)de<\infty\)
* For any \(a\in\mathbb{R}\), \(\int_{\mathbb{R}}F_{1}(e-a)F_{2}(e)de<\infty\),

By the corollary of Hardy-Littlewood inequality in Lemma 9,

\[\int_{\mathbb{R}}F_{1}(e-a)F_{2}(e)de\geq\int_{\mathbb{R}}F_{1}(e)F_{2}(e)de,\]

which means the posterior risk is minimized when \(\widehat{\mu}(Y_{i})=\mu_{\ell}\). In Lemma 11, we have stated expressions for the expected value of the power of a normal random variable. Using this, we can write the Bayes risk as:

\[R_{\ell}:=\mathbb{E}\!\left[\sum_{j=0}^{k_{\epsilon}}A_{j}\mathbb{E}\!\left[ \left.\left(\mu-\mu_{\ell}\right)^{2j}\right|X_{i}\right]\right]=\sum_{j=0}^{ k_{\epsilon}}A_{j}(2j-1)!!\sigma_{\ell}^{2j}\]

and the limit of Bayesian risk as \(\ell\to\infty\) is

\[R_{\infty}:=\lim_{\ell\to\infty}\sum_{j=0}^{k_{\epsilon}}A_{j}(2j-1)!!\frac{ \sigma^{2j}}{n_{i}^{j}}\]

When \(\widehat{\mu}(Y_{i})=\widehat{\mu}(X_{i})\), the maximum risk is:

\[\sup_{\mu}\mathbb{E}\left[\sum_{j=0}^{k_{\epsilon}}A_{j}(\widehat {\mu}(Y_{i})-\mu)^{2j}\right|\mu\right]=\sup_{\mu}\mathbb{E}\left[\sum_{j=0}^ {k_{\epsilon}}A_{j}(\widehat{\mu}(X_{i})-\mu)^{2j}\right|\mu\right]\] \[= \sum_{j=0}^{k_{\epsilon}}A_{j}(2j-1)!!\sigma^{2j}n_{i}^{-j}=R_{ \infty}.\]

This means, fixing \(n_{i}\) and \(|Y_{i}|\), agent \(i\) achieves minimax risk when choosing \(\widehat{\mu}(Y_{i})=\widehat{\mu}(X_{i})\); as the maximum is larger than the average, this follows using a similar argument to Step 3 in SSA.3.

**Step 1.2**.: Next, we will show that the best size of the submission is \(|Y_{i}|=|X_{i}|=n_{i}\), assuming \(\widetilde{\mu}(Y_{i})=\widetilde{\mu}(X_{i})\). For this, we will first use \(n_{i}^{*}\) to rewrite \(\beta_{\epsilon}^{2}\) as

\[\beta_{\epsilon}^{2}=\frac{n_{i}^{*k_{\epsilon}-2}(m-1)^{k_{\epsilon}-1}{(|Y_{ i}|+(m-1)n_{i}^{*})}^{2}}{k_{\epsilon}(2k_{\epsilon}-1)!!m^{k_{\epsilon}+1} \sigma^{2k_{\epsilon}-2}}.\]

Because

\[\widehat{\mu}(X_{i})-\widehat{\mu}(Y_{-i})\sim\mathcal{N}\!\left(0,\left( \frac{1}{n_{i}}+\frac{1}{(m-1)n_{i}^{*}}\right)\!\sigma^{2}\right)\!,\]

the risk term in the penalty can be rewritten and lower bounded as follows:

\[\frac{1}{{(|Y_{i}|+(m-1)n_{i}^{*})}^{2}}\!\left({|Y_{i}|}^{2} \,\sigma^{2}/n_{i}+(m-1)n_{i}^{*}\beta_{\epsilon}^{2}(2k_{\epsilon}-1)!!\left( \frac{1}{n_{i}}+\frac{1}{(m-1)n_{i}^{*}}\right)^{k_{\epsilon}}\sigma^{2k_{ \epsilon}}\right)\] \[+\frac{(m-1)n_{i}^{*}\sigma^{2}}{{(|Y_{i}|+(m-1)n_{i}^{*})}^{2}}\] \[= \frac{{|Y_{i}|}^{2}\,\frac{\sigma^{2}}{n_{i}}+(m-1)n_{i}^{*}\sigma ^{2}}{{(|Y_{i}|+(m-1)n_{i}^{*})}^{2}}+\frac{n_{i}^{*k_{\epsilon}-1}{(m-1)^{k_{ \epsilon}}}}{k_{\epsilon}m^{k_{\epsilon}+1}}\!\left(\frac{1}{n_{i}}+\frac{1}{( m-1)n_{i}^{*}}\right)^{k_{\epsilon}}\!\sigma^{2}\]\[\geq \frac{\sigma^{2}}{n_{i}+(m-1)n_{i}^{\star}}+\frac{{n_{i}^{\star}}^{k_ {\epsilon}-1}(m-1)^{k_{\epsilon}}}{k_{\epsilon}m^{k_{\epsilon}+1}}\bigg{(}\frac{ 1}{n_{i}}+\frac{1}{(m-1)n_{i}^{\star}}\bigg{)}^{k_{\epsilon}}\sigma^{2}.\]

Here, the last step follows from the fact that

\[\frac{\left|Y_{i}\right|^{2}\frac{\sigma^{2}}{n_{i}}+(m-1)n_{i}^ {\star}\sigma^{2}}{\left(\left|Y_{i}\right|+(m-1)n_{i}^{\star}\right)^{2}}= \frac{\left|Y_{i}\right|^{2}\frac{\sigma^{2}}{n_{i}}+(m-1)n_{i}^{\star}\sigma^ {2}}{n_{i}\frac{\left|Y_{i}\right|^{2}}{n_{i}}+2\left|Y_{i}\right|(m-1)n_{i}^{ \star}+(m-1)^{2}{n_{i}^{\star}}^{2}}\] \[\geq \frac{\left|Y_{i}\right|^{2}\frac{\sigma^{2}}{n_{i}}+(m-1)n_{i}^ {\star}\sigma^{2}}{n_{i}\frac{\left|Y_{i}\right|^{2}}{n_{i}}+\Big{(}n_{i}+ \frac{\left|Y_{i}\right|^{2}}{n_{i}}\Big{)}(m-1)n_{i}^{\star}+(m-1)^{2}{n_{i} ^{\star}}^{2}}=\frac{\left|Y_{i}\right|^{2}\frac{\sigma^{2}}{n_{i}}+(m-1)n_{i} ^{\star}\sigma^{2}}{(n_{i}+(m-1)n_{i}^{\star})\Big{(}\frac{\left|Y_{i}\right|^ {2}}{n_{i}}+(m-1)n_{i}^{\star}\Big{)}}\] \[= \frac{\sigma^{2}}{n_{i}+(m-1)n_{i}^{\star}}.\]

Equality holds in this inequality if and only if \(\left|Y_{i}\right|=n_{i}\).

In conclusion, fixing \(n_{i}\), the agent can minimize her penalty by submitting \(n_{i}\) points with the same sample mean as the dataset \(X_{i}\) she collected. One way to achieve this is set \(f_{i}=\mathbf{I}\). This completes the proof of (28).

**Step 2:** Our next step is to show that the agent's best strategy is to collect \(n_{i}^{\star}\) data points. That is, we will show for all \(n_{i}\in\mathbb{N}\).

\[p_{i}(M_{\texttt{GED}},((n_{i}^{\star},f_{i}^{\star}),s_{-i}^{ \star}))\leq p_{i}(M_{\texttt{GED}},((n_{i},f_{i}^{\star}),s_{-i}^{\star})).\] (31)

In the following, we will use \(p(n_{i})\) as a shorthand for \(p_{i}(M_{\texttt{GED}},((n_{i},f_{i}^{\star}),s_{-i}^{\star}))\). The penalty can be rewritten as:

\[p(n_{i})= \frac{\sigma^{2}}{n_{i}+(m-1)n_{i}^{\star}}+\frac{{n_{i}^{\star}} ^{k_{\epsilon}-1}(m-1)^{k_{\epsilon}}}{k_{\epsilon}m^{k_{\epsilon}+1}}\bigg{(} \frac{1}{n_{i}}+\frac{1}{(m-1)n_{i}^{\star}}\bigg{)}^{k_{\epsilon}}\sigma^{2} +cn_{i}\]

We need to show that \(p_{i}(n_{i})\) achieves minimum at \(n_{i}=n_{i}^{\star}\). The derivative of \(p_{i}(\cdot)\) is:

\[p^{\prime}(n_{i})= -\frac{\sigma^{2}}{(n_{i}+(m-1)n_{i}^{\star})^{2}}+\frac{{n_{i}^ {\star}}^{k_{\epsilon}-1}(m-1)^{k_{\epsilon}}}{m^{k_{\epsilon}+1}}\bigg{(} \frac{1}{n_{i}}+\frac{1}{(m-1)n_{i}^{\star}}\bigg{)}^{k_{\epsilon}-1}\sigma^{2 }\bigg{(}-\frac{1}{n_{i}^{2}}\bigg{)}+c\]

Because \(p^{\prime}(n_{i})\) increase in \(n_{i}\), \(p(n_{i})\) is convex. Moreover, because

\[p^{\prime}(n_{i}^{\star})= -\frac{\sigma^{2}}{{m^{2}{n_{i}^{\star}}^{2}}}+\frac{{n_{i}^{\star }}^{k_{\epsilon}-1}(m-1)^{k_{\epsilon}}}{m^{k_{\epsilon}+1}}\bigg{(}\frac{1}{n_ {i}^{\star}}+\frac{1}{(m-1)n_{i}^{\star}}\bigg{)}^{k_{\epsilon}-1}\sigma^{2} \bigg{(}-\frac{1}{{n_{i}^{\star}}^{2}}\bigg{)}+c\] \[= -\frac{\sigma^{2}}{{m^{2}{n_{i}^{\star}}^{2}}}-\frac{(m-1)\sigma^ {2}}{{m^{2}{n_{i}^{\star}}^{2}}}+c=-\frac{\sigma^{2}}{{m^{\star}}^{2}}+c=0,\]

we know \(p(n_{i})\) reaches minimum at \(n_{i}=n_{i}^{\star}\). This concludes the proof for (31).

### Algorithm 3 is individually rational

The penalty of an agent at the recommended strategies can be expressed as:

\[p_{i}(M_{\texttt{GED}},s_{i}^{\star})= p(n_{i}^{\star})=\frac{\sigma^{2}}{mn_{i}^{\star}}+\frac{{n_{i}^{ \star}}^{k_{\epsilon}-1}(m-1)^{k_{\epsilon}}}{k_{\epsilon}m^{k_{\epsilon}+1}} \bigg{(}\frac{1}{n_{i}^{\star}}+\frac{1}{(m-1)n_{i}^{\star}}\bigg{)}^{k_{ \epsilon}}\sigma^{2}+cn_{i}^{\star}\] \[= \frac{\sigma^{2}}{mn_{i}^{\star}}+\frac{n_{i}^{\star}}{k_{\epsilon }m^{k_{\epsilon}+1}}\frac{m^{k_{\epsilon}}}{n_{i}^{\star}}(m-1)^{k_{\epsilon}} \sigma^{2}+cn_{i}^{\star}\] \[= \frac{\sigma^{2}}{mn_{i}^{\star}}+\frac{1}{k_{\epsilon}}\frac{\sigma^ {2}}{mn_{i}^{\star}}+cn_{i}^{\star}=\bigg{(}2+\frac{1}{k_{\epsilon}}\bigg{)} \frac{\sigma\sqrt{c}}{\sqrt{m}}.\] (32)

We have that \(M_{\texttt{GED}}\) is IR when \(m\geq 2\), via the following simple calculation:

\[\bigg{(}2+\frac{1}{k_{\epsilon}}\bigg{)}\frac{\sigma\sqrt{c}}{\sqrt{m}}\leq \bigg{(}2+\frac{1}{2}\bigg{)}\frac{\sigma\sqrt{c}}{\sqrt{2}}<2\sigma\sqrt{c}=p_{ \min}^{\rm IR}\]

### Algorithm 3 is approximately efficient

Using the expression for \(p_{i}(M_{\texttt{CRED}},s_{i}^{*})\) in (32), the penalty ratio can be bounded by:

\[\mathrm{PR}(M_{\texttt{CRED}},s^{*})=\frac{\Big{(}2+\frac{1}{k_{*}} \Big{)}\sigma\sqrt{cm}}{2\sigma\sqrt{cm}}=1+\frac{1}{2k_{*}}\leq 1+\epsilon.\]

## Appendix D Additional Materials for Section 4.2

### Mechanism detail

See Algorithm 3.

### Using a weighted average under the original strategy space from SS2

In this section, we will consider a variation of \(M_{\texttt{CRED}}\) when applied to our original strategy space \(\mathbb{N}\times\mathcal{F}\times\mathcal{H}\). For this, we will assume that \(M_{\texttt{CRED}}\) will return \(A_{i}=Z_{i}\) as the agent's allocation, and then an agent can use \(X_{i},Y_{i},Z_{i}\) to estimate \(\mu\). In this situation, below we show that the agent can achieve a smaller penalty using a weighted average over \(X_{i}\cup Z_{i}\) instead of the sample mean used by the mechanism. Here, the weights are proportional to the inverse of the variance of each data point. (Our mechanism purposefully uses the sub-optimal sample mean in the restricted strategy space \(\mathbb{N}\times\mathcal{F}\) as a way to shape the agent's penalty and incentivize good behavior.)

This shows that \(M_{\texttt{CRED}}\) (with the above modification) is not NIC in this more general strategy space. The agent can obtain a lower penalty using a better estimator (such as the weighted average we show over here) and achieve a lower penalty. More importantly, as the agent knows that she can achieve a lower estimation error via a better estimator instead of more data, she can leverage this insight to collect less data and reduce her penalty even further.

We should emphasize that it is unclear if this weighted average is minimax optimal. It is also unclear if there exists a Nash equilibrium for \(M_{\texttt{CRED}}\) (or any straightforward modification of \(M_{\texttt{CRED}}\)) in the expanded strategy space.

The weighted average estimator:We will now present the weighted average estimator that achieves a lower maximum risk. To show this, first note that for all \(x\in X_{i}\), \(\mathbb{V}[x]=\sigma^{2}\); when \((n_{i},f_{i})=(n_{i}^{*},f_{i}^{*})\), for all \(x\in Z_{i}\),

\[\mathbb{V}[x]= \mathbb{E}\Big{[}\big{(}z+\epsilon_{z,i}-\mu\big{)}^{2}\Big{]}= \sigma^{2}+\beta_{e}^{2}\mathbb{E}\Big{[}(\widehat{\mu}(X_{i})-\widehat{\mu}( Y_{-i}))^{2k_{*}}\Big{]}\] \[= \sigma^{2}+\frac{n_{i}^{*k_{*}-2}(m-1)^{k_{*}-1}(mn_{i}^{*})^{2}} {k_{*}(2k_{*}-1)!!m^{k_{*}+1}\sigma^{2k_{*}-2}}(2k_{*}-1)!!\bigg{(}\frac{1}{n_ {i}^{*}}+\frac{1}{(m-1)n_{i}^{*}}\bigg{)}^{k_{*}}\sigma^{2k_{*}}\] \[= \sigma^{2}+\frac{n_{i}^{*k_{*}}(m-1)^{k_{*}-1}}{k_{*}m^{k_{*}-1}} \frac{m^{k_{*}}}{(m-1)^{k_{*}}n_{i}^{*k_{*}}}\sigma^{2}\] \[= \sigma^{2}+\frac{1}{k_{*}}\frac{m}{m-1}\sigma^{2}\]

Consider the following weighted-average estimator:

\[h_{i}(X_{i},Y_{i},(Z_{i},\eta_{i}^{2}))=\frac{\frac{1}{\sigma^{2}}\sum_{x\in X _{i}}x+\frac{1}{\sigma^{2}+\frac{1}{k_{*}}\frac{m}{m-1}\sigma^{2}}\sum_{x\in Z _{i}}x}{\frac{n_{i}^{*}}{\sigma^{2}}+\frac{(m-1)n_{i}^{*}}{\sigma^{2}+\frac{1} {k_{*}}\frac{m}{m-1}\sigma^{2}}}\]

The maximum risk of \(h_{i}\) is

\[\mathbb{E}\Big{[}\big{(}h_{i}(X_{i},Y_{i},(Z_{i},\eta_{i}^{2}))- \mu\big{)}^{2}\Big{]}= \frac{1}{\frac{n_{i}^{*}}{\sigma^{2}}+\frac{(m-1)n_{i}^{*}}{ \sigma^{2}+\frac{1}{k_{*}}\frac{m}{m-1}\sigma^{2}}}=\frac{1}{1+\frac{m-1}{1+ \frac{1}{k_{*}}\frac{m}{m-1}}}\frac{\sigma^{2}}{n_{i}^{*}}=\frac{1+\frac{1}{k_ {*}}\frac{m}{m-1}}{m+\frac{1}{k_{*}}\frac{m}{m-1}}\frac{\sigma^{2}}{n_{i}^{*}}\] \[< \frac{\Big{(}1+\frac{1}{k_{*}}\Big{)}\Big{(}1+\frac{1}{k_{*}} \frac{1}{m-1}\Big{)}}{m+\frac{1}{k_{*}}\frac{m}{m-1}}\frac{\sigma^{2}}{n_{i}^{ *}}=\bigg{(}1+\frac{1}{k_{*}}\bigg{)}\frac{\sigma^{2}}{mn_{i}^{*}}\] (33)Note that the RHS of (33) is the risk of the sample average deployed by \(M_{\texttt{CBB}}\). This means, suppose all other agents choose \(s^{\star}\), then agent \(i\) can choose a weighted average to reduce her penalty without collecting more data.

## Appendix E High dimensional mean estimation with bounded variance

In this section, we will study estimating a \(d\)-dimensional mean \(\mu(\theta)\in\mathbb{R}^{d}\) for distributions \(\theta\) with bounded variance. We will focus on our original setting in SS2, but will outline the modifications to the formalism to accommodate the generality. For \(x\in\mathbb{R}^{d}\), let \(x^{(i)}\) denote the \(i^{\text{n}}\) dimension.

**Modifications to the setting in SS2:** First, we should change the definitions of \(\mathcal{F},\mathcal{H}\) and \(\mathcal{M}\) in equations 1 and (2) to account for the fact that the data is \(d\) dimensional. For instance, the space of functions mapping the dataset collected to the dataset submitted should be defined as \(\mathcal{F}=\{f:\bigcup_{n\geq 0}\mathbb{R}^{d\times n}\rightarrow\bigcup_{n \geq 0}\mathbb{R}^{d\times n}\}\). Next, let \(\Theta=\{\theta;\ \mathbf{supp}\left(\theta\right)\subset\mathbb{R}^{d},\ \mathbb{E}_{x \sim\theta}\left[(x^{(i)}-\mu(\theta)^{(i)})^{2}\right]\leq\sigma^{2},\ \forall\ i\in[d]\}\) be the class of all \(d\)-dimensional distributions where the variance along each dimension is bounded by \(\sigma^{2}\). Here, the maximum variance \(\sigma^{2}\) is known and is public information. Note that we do not assume that the individual dimensions are independent. An agent's penalty \(p_{i}\) is defined similar to (3) but considers the maximum risk over \(\Theta\), i

\[p_{i}(M,s)=\sup_{\theta\in\Theta}\mathbb{E}\big{[}\|h_{i}(X_{i},Y_{i},A_{i})- \mu(\theta)\|_{2}^{2}\ \big{|}\,\theta\big{]}\ +\ cn_{i}.\] (34)

Finally, the social penalty and ratio \(\mathrm{PR}\) are as defined in (5), but with the above definition for \(p_{i}\).

**Mechanism:** Our mechanism for this problem is the same as the one outlined in Algorithm 1, with the following cosmetic modifications. First, the allocation space should now be \(\mathcal{A}=\bigcup_{n\geq 0}\mathbb{R}^{d\times n}\times\bigcup_{n\geq 0} \mathbb{R}^{d\times n}\times\mathbb{R}^{d}_{+}\). The noise modulating parameter \(\alpha\) is determined by a similar equation as in (7), but with \(c\) replaced with \(c/d\). In line 12 of Algorithm 1, we should set the size of the dataset \(Z_{i}\) to be \(\min\{|Y_{-i}|,\,\sigma\sqrt{d/(cm)}\}\). Finally, the operations in lines 13 and 14 should be interpreted as \(d\)-dimensional operations that are performed elementwise. The recommended strategy \(s_{i}^{\star}=(n_{i}^{\star},f_{i}^{\star},h_{i}^{\star})\) for agent \(i\) is as follows:

\[n_{i}^{\star}=\begin{cases}\frac{\sigma}{m}\sqrt{\frac{d}{c}}& \text{if }m\leq 4,\\ \sigma\sqrt{\frac{d}{cm}}&\text{if }m\geq 5\end{cases},\qquad\qquad\qquad\qquad f _{i}^{\star}=\mathbf{I},\] (35) \[h_{i}^{\star}(X_{i},Y_{i},(Z_{i},Z_{i}^{\prime},\eta_{i}^{2}))= \frac{\frac{1}{\sigma^{2}}\sum_{u\in X_{i}\cup Z_{i}}u+\frac{1}{\sigma^{2}+ \tau_{i}^{2}}\sum_{u\in Z_{i}^{\prime}}u}{\frac{1}{\sigma^{2}}|X_{i}\cup Z_{i }^{\prime}|+\frac{1}{\sigma^{2}+\tau_{i}^{2}}|Z_{i}^{\prime}|},\qquad\text{ where, }\tau_{i}^{2}=\frac{2\alpha^{2}\sigma^{2}}{n_{i}^{\star}}\in\mathbb{R}_{+}.\]

Above, one difference worth highlighting is the change in the recommended estimator \(h_{i}^{\star}\). Previously, the weighting used the \(\eta_{i}^{2}\) term returned by the mechanism, which is a function of \(Y_{i}\) and \(Z_{i}\). This data-dependent weighting was necessary to obtain an _exactly_ (i.e including constants) minimax optimal estimator for the corrupted dataset, which in turn was necessary to achieve an exact Nash equilibrium. However, bounding the risk when using a data-dependent weighting is challenging when the Gaussian assumption does not hold. Instead, here we use a deterministic weighting via the quantity \(\tau_{i}^{2}\). While this is not exactly minimax optimal, we can show that its maximum risk is very close to a lower bound, which helps us obtain an approximate Nash equilibrium. It is worth pointing out that designing exactly minimax optimal estimators, even under i.i.d assumptions, is challenging for general classes of distributions [24].

The following theorem states the main properties of this mechanism.

**Theorem 7**.: _The following statements are true about the mechanism \(M_{\texttt{CBD}}\) in Algorithm 1 with the above modifications. (i) The strategy profile \(s^{\star}\) as defined in (35) is an approximate Nash equilibrium, i.e if all agents except \(i\) are following \(s^{\star}\), then for any alternative strategy \(s_{i}\) for agent \(i\), we have \(p_{i}(M_{\texttt{CBD}},s^{\star})\leq p_{i}(M_{\texttt{CBD}},(s_{i}^{\star},s_ {i}))(1+5/m)\) (ii) The mechanism is individually rational at \(s^{\star}\). (iii) The mechanism is approximately efficient at \(s_{i}^{\star}\), with \(\mathrm{PR}(M_{\texttt{CBD}},s^{\star})<2+10/m\)._

We see that even under this more general setting, our mechanism retains its main properties with only a slight weakening of the results. We now have approximate, instead of exact, NIC, with the benefit of deviation diminishing as there are more agents. Similarly, the bound on the efficiency is only slightly weaker than the one in Theorem 1.

### Proof of Theorem 7

When \(m\leq 4\), the claims follow using the exact steps in SSA.1. Therefore, we focus on the case \(m\geq 5\). Moreover, some of the key steps of this proof follows along similar lines to Theorem 1, so we will provide an outline and focus on the differences.

**Approximate Nash incentive compatibility.** We will first prove the statement _(i)_ of Theorem 7, which states that \(s_{i}^{*}\), as defined in (35), is an approximate Nash equilibrium for \(M_{\text{c20}}\). That is, we will show that the maximum possible reduction in penalty for an agent \(i\) when deviating from \(s_{i}^{*}\) is small, provided that all other agents are following \(s_{-i}^{*}\).

For this, we will first lower bound the penalty \(p_{i}\) (34) using the family of independent Gaussian distributions. Let \(\Theta_{\mathcal{N}}=\left\{\mathcal{N}(\mu,\sigma^{2}I_{d}):\mu\in\mathbb{R}^ {d}\right\}\) denote the space of \(d\)-dimensional normal distributions with identity covariance matrix. For any mechanism \(M\) and strategy profile \(s\in\mathcal{S}^{m}\), we define the penalty of agent \(i\) restricted to \(\Theta_{\mathcal{N}}\) as:

\[p_{i}^{\mathcal{N}}(M,s)=\sup_{\theta\in\Theta_{\mathcal{N}}}\mathbb{E}\big{[} \big{\|}h_{i}(X_{i},Y_{i},A_{i})-\mu(\theta)\big{\|}_{2}^{2}\ \big{|}\,\theta\big{]}\ +\ cn_{i}.\]

Since \(\Theta_{\mathcal{N}}\subset\Theta\), it is straightforward to see that for all \(M\in\mathcal{M}\) and \(s\in\mathcal{S}^{m}\),

\[p_{i}^{\mathcal{N}}(M,s)\leq p_{i}(M,s).\] (36)

We will now use this result to lower bound the penalty of an agent for any other alternative strategy. First note that, by independence, the mean estimation problem on \(\Theta_{\mathcal{N}}\) can be viewed as \(d\) independent copies of the univariate normal mean estimation problem considered in Theorem 1 but with \(c\) replaced with \(c/d\). Let \(\tilde{h}_{i}^{*}\) be the weighted average that applies the estimator in (8) along each dimension. And let \(\tilde{s}_{i}^{*}=(n_{i}^{*},f_{i}^{*},\tilde{h}_{i}^{*})\). We can now lower bound the penalty of agent \(i\) when following any (alternative) strategy \(s_{i}\in\mathcal{S}\), provided that other agents are following \(s_{-i}^{*}\). We have:

\[p_{i}(M_{\text{c30}},(s_{i},s_{-i}^{*})) =p_{i}\big{(}M_{\text{c30}},\big{(}s_{i},\big{(}n_{-i}^{*},f_{-i}^ {*},h_{-i}^{*}\big{)}\big{)}\big{)}\] \[\geq p_{i}^{\mathcal{N}}\big{(}M_{\text{c30}},\big{(}s_{i},\big{(}n _{-i}^{*},f_{-i}^{*},h_{-i}^{*}\big{)}\big{)}\big{)}\] (By (36)) \[=p_{i}^{\mathcal{N}}\Big{(}M_{\text{c30}},\Big{(}s_{i},\Big{(}n_{ -i}^{*},f_{-i}^{*},\tilde{h}_{-i}^{*}\big{)}\Big{)}\Big{)}\] \[\qquad\qquad(\text{As agent $i$'s penalty will not be affected by other agents' estimators})\] \[\geq p_{i}^{\mathcal{N}}\Big{(}M_{\text{c30}},\Big{(}\Big{(}n_{i}^{*},f_{i}^ {*},\tilde{h}_{i}^{*}\Big{)},\Big{(}n_{-i}^{*},f_{-i}^{*},\tilde{h}_{-i}^{*} \Big{)}\Big{)}\Big{)}\] \[\qquad\qquad(\text{ By adapting the analysis in SSA.3}.\ )\] \[=p_{i}^{\mathcal{N}}(M_{\text{c30}},\tilde{s}^{*})\] (37)

Above, the second step uses (36) and the third step uses the fact that other agent's _estimator_ will not affect agent \(i\)'s penalty. The fourth step uses the fact that for estimation problems in \(\Theta_{\mathcal{N}}\), the strategy profile \(\tilde{s}^{*}=\{(n_{i}^{*},f_{i}^{*},\tilde{h}_{i}^{*})\}_{i}\) is a Nash equilibrium; in SSA.3, we showed this for the one dimensional case, but this proof can be easily adapted to \(d\) dimensions since we are assuming an identity covariance matrix in \(\Theta_{\mathcal{N}}\). Finally, by adapting the analysis in SSA.5, we can obtain the following expression for agent \(i\)'s penalty \(p_{i}^{\mathcal{N}}(M_{\text{c20}},\tilde{s}^{*})\) in \(\Theta_{\mathcal{N}}\):

\[p_{i}^{\mathcal{N}}(M_{\text{c20}},\tilde{s}^{*})=\ d\sigma\sqrt{\frac{c/d}{m}} \left(\frac{\frac{10\alpha^{2}}{n_{i}^{*}}-1}{\frac{4\alpha^{2}}{n_{i}^{*}} \frac{m+1}{m}-1}+1\right)\] (38)

To state the approximate NIC result, we will now upper bound the penalty of the agent when following \(s_{i}^{*}\). Using the bounded variance assumption, we have:

\[p_{i}(M,s^{*})=\sup_{\theta\in\Theta}\mathbb{E}\Bigg{[}\left|\frac {\frac{1}{\sigma^{2}}\sum_{u\in X_{i}\cup Z_{i}}u+\frac{1}{\sigma^{2}+\tau_{i} ^{*}}\sum_{u\in Z_{i}^{*}}u}{\frac{1}{\sigma^{2}}|X_{i}\cup Z_{i}^{\prime}|+ \frac{1}{\sigma^{2}+\tau_{i}^{*}}|Z_{i}^{\prime}|}-\mu(\theta)\bigg{|}^{2} \Bigg{|}\theta\right]+cn_{i}^{*}\] \[= \sup_{\theta\in\Theta}\sum_{k=1}^{d}\mathbb{E}\Bigg{[}\left( \frac{\frac{1}{\sigma^{2}}\sum_{u\in X_{i}\cup Z_{i}}\big{(}u^{(k)}-\mu( \theta)^{(k)}\big{)}+\frac{1}{\sigma^{2}+\tau_{i}^{*}}\sum_{u\in Z_{i}^{*}}(u ^{(k)}-\mu(\theta)^{(k)})}{\frac{1}{\sigma^{2}}|X_{i}\cup Z_{i}^{\prime}|+ \frac{1}{\sigma^{2}+\tau_{i}^{*}}|Z_{i}^{\prime}|}\right)^{2}\Bigg{|}\theta \Bigg{]}+cn_{i}^{*}\]\[= \sup_{\theta\in\Theta}\sum_{k=1}^{d}\frac{\frac{1}{\sigma^{7}}\sum_{u \in X_{i}\cup Z_{i}}\mathbb{E}\big{[}\big{(}u^{(k)}-\mu(\theta)^{(k)}\big{)} \big{]}+\frac{1}{\sigma^{2}+\tau_{i}^{2}}\sum_{u\in Z_{i}^{\prime}}\mathbb{E} \big{[}\big{(}u^{(k)}-\mu(\theta)^{(k)}\big{)}\big{]}}{\frac{1}{\sigma^{2}}|X_ {i}\cup Z_{i}^{\prime}|+\frac{1}{\sigma^{2}+\tau_{i}^{2}}|Z_{i}^{\prime}|}+cn_ {i}^{*}\] (39) \[\leq \frac{d}{\frac{2n_{i}^{*}}{\sigma^{2}}+\frac{(m-2)n_{i}^{*}}{ \sigma^{2}+\frac{2n_{i}^{*}}{n_{i}^{*}}}}+cn_{i}^{*}=\frac{\sigma^{2}}{n_{i}^ {*}}\frac{d}{2+\frac{m-2}{1+\frac{2\sigma^{2}}{n_{i}^{*}}}}+cn_{i}^{*}=\sigma \sqrt{\frac{cd}{m}}\Bigg{(}\frac{m}{2+\frac{m-2}{1+\frac{2\alpha^{2}}{n_{i}^{ *}}}}+1\Bigg{)},\] (40)

where (39) is because: for all \(k\in[d]\), \(\forall x_{1}^{(k)},x_{2}^{(k)}\in X_{i}\cup Z_{i}\), \(\forall z_{1}^{(k)},z_{2}^{(k)}\in Z_{i}^{\prime}\), \(x_{1}^{(k)}-\mu^{(k)},x_{2}^{(k)}-\mu^{(k)},z_{1}^{(k)}-\mu^{(k)},z_{2}^{(k) }-\mu^{(k)}\) are uncorrelated pairwise. The final inequality is due to the bounded variance assumption.

Next, for brevity, let us write \(A_{m}:=\frac{\alpha}{\sqrt{n_{i}^{*}}}\) where \(\alpha\) is as defined in (7). By adapting the analysis in SSA.2, we can show that

\[A_{m}:=\frac{\alpha}{\sqrt{n_{i}^{*}}}\in\bigg{(}1,1+\frac{C_{m}}{m}\bigg{)}, \qquad\text{ where},\quad C_{m}=\begin{cases}20,&\text{if }m\leq 20\\ 5,&\text{if }m>20\end{cases}.\] (41)

By combining the results in (37), (40), and (41), we obtain the following bound:

\[\frac{p_{i}(M_{\texttt{c30}},s^{\star})}{\inf_{s_{i}}p_{i}(M_{ \texttt{c30}},(s_{i},s_{-i}^{\star}))}-1\leq \frac{p_{i}(M_{\texttt{c30}},s^{\star})}{p_{i}^{\star}(M_{ \texttt{c30}},\tilde{s}^{\star})}-1\] \[\leq \frac{\sigma\sqrt{\frac{cd}{m}}\bigg{(}\frac{m}{2+\frac{m}{1+2 \alpha_{m}^{*}}}+1\bigg{)}}{d\sigma\sqrt{\frac{c/d}{m}}\bigg{(}\frac{10A_{m}^{2 }-1}{4A_{m}^{2}\frac{m-1}{m-1}}+1\bigg{)}}-1=\frac{\frac{m}{2+\frac{m-2}{1+ \frac{2\alpha_{m}^{*}}{1+2\alpha_{m}^{*}}}}+1}{\frac{\frac{10\alpha_{m}^{*}}{ 1+2\alpha_{m}^{*}}}-1}-1\] \[= \frac{4A_{m}^{2}\big{(}(A_{m}^{2}-1)m+1-4A_{m}^{2}\big{)}m}{(4A_{ m}^{2}+m)((7A_{m}^{2}-1)m+2A_{m}^{2}\big{)}}=:E(m).\] (42)

Let \(E(m)\) denote the final upper bound obtained above. Next, we will prove \(E(m)<5/m\). When \(m\in[5,500]\), this can be individually verified for each value of \(E(m)\) (see Figure 2). When \(m\geq 500\), we have \(A_{m}\leq 1.01\) (see (41)). From this we can conclude,

\[E(m)\leq \frac{4\times 1.01^{2}\times(2.01\times\frac{5}{m}m-3)m}{6m^{2}}<\frac{ 5}{m}.\] (43)

Combining the results in (42) and (43), we obtain the following approximate NIC result:

\[\forall\ i\in[m],\ s_{i}\in\mathcal{S},\ \ \ \ \ p_{i}(M_{\texttt{c30}},s^{\star})\leq p_{i}(M_{\texttt{c30}},(s_{i},s_{-i}^{\star}))\left(1+\frac{5}{m} \right).\]

**Individual rationality:** This proof is very similar to the proof in SSA.4. In particular, using calculations similar to (40), we can show that regardless of the choice of \(n_{i}\), the agent's penalty is strictly

Figure 2: \(E(m)\) plot. See G_em_plot.py.

[MISSING_PAGE_EMPTY:32]

Proof.: We will break this proof into two cases. The first is when \(\sup f<\infty\) and the second is when \(\sup f=\infty\). First consider the case \(\sup f<\infty\). Let

\[M:=\lim_{x\to\infty}f(x).\]

By using Lemma 8, \(\forall a\),

\[\int_{\mathbb{R}}(M-f(x))g(x)dx\geq\int_{\mathbb{R}}(M-f(x-a))g(x)dx.\]

The result follows after rearrangement.

If \(\sup f=\infty\), let \(f_{n}(x):=\min\{f(x),n\}\). For all \(n\) and \(a\), by Lemma 8,

\[\int_{\mathbb{R}}(n-f_{n}(x))g(x)dx\geq\int_{\mathbb{R}}(n-f_{n}(x-a))g(x)dx,\]

thus

\[\int_{\mathbb{R}}f_{n}(x)g(x)dx\leq\int_{\mathbb{R}}f_{n}(x-a)g(x)dx.\]

Note that \(|f_{n}(x)g(x)|\leq f(x)g(x)\), the result follows by letting \(n\to\infty\) on both sides and using dominated convergence theorem. 

Below, we provide a brief example on using Lemma 9 to calculate the Bayes risk in a normal mean estimation problem with i.i.d data. While it is not necessary to use Hardy-Littlewood for this problem, this example will illustrate how we have used it in our proofs.

**Example 1**.: _Consider the Normal mean estimation problem given samples \(X_{[n]}\sim\mathcal{N}(\mu,\sigma^{2})\), where \(\mu\) admits a prior distribution \(\mathcal{N}(0,\ell^{2})\). The goal is to minimize the average risk:_

\[\mathbb{E}_{\mu\sim\mathcal{N}(0,\ell^{2})}\Big{[}\mathbb{E}_{X_{[n]}\sim \mathcal{N}(\mu,\sigma^{2})}[L(\hat{\mu}-\mu)|\mu]\Big{]},\]

_where the loss function, \(L(\cdot)\), is an even function that increases on \([0,\infty)\). By a standard argument, one can show that the posterior distribution of \(\mu\) conditioned on \(X_{[n]}\) is Gaussian with data-dependent parameters \(\bar{\mu},\bar{\sigma}^{2}\):_

\[\mu|X_{[n]}\sim\mathcal{N}(\overline{\mu},\bar{\sigma}^{2}).\]

_The posterior risk is:_

\[\mathbb{E}_{\mu|X_{[n]}}[L(\hat{\mu}-\mu)]=\mathbb{E}_{\mu|X_{[n]}}[L((\mu- \overline{\mu})+(\overline{\mu}-\hat{\mu}))]=\int_{\mathbb{R}}\underbrace{L( x+(\overline{\mu}-\hat{\mu}))}_{=:f(x+(\overline{\mu}-\hat{\mu}))}\underbrace{ \frac{\exp\Bigl{(}-\frac{x^{2}}{2\bar{\sigma}^{2}}\Bigr{)}}{\overline{\sigma} \sqrt{2\pi}}}_{=:g(x)}dx\]

_By applying Lemma 9 with \(f\) and \(g\), the posterior risk above is minimized when \(\hat{\mu}=\overline{\mu}\). So is the average risk._

The next Lemma shows that convexity is preserved under expectation under certain conditions.

**Lemma 10**.: _Let \(y\) be a random variable and \(f(x,y)\) be a function s.t._

* \(f(x,y)\) _is convex in_ \(x\)_;_
* \(\mathbb{E}_{y}[|f(x,y)|]<\infty\) _for all_ \(x\)_._

_Then \(\mathbb{E}_{y}[f(x,y)]\) is also convex in \(x\)._

Proof.: For any \(x_{1},x_{2}\), we have

\[\frac{\mathbb{E}_{y}[f(x_{1},y)]+\mathbb{E}_{y}[f(x_{2},y)]}{2}= \mathbb{E}_{y}\bigg{[}\frac{f(x_{1},y)+f(x_{2},y)}{2}\bigg{]}\geq \mathbb{E}_{y}\bigg{[}f\bigg{(}\frac{x_{1}+x_{2}}{2},y\bigg{)}\bigg{]}\]

**Lemma 11** (Centered moments of normal random variable).: _Let \(X\sim\mathcal{N}(\mu,\sigma^{2})\) be a normal random variable and \(p\in\mathbb{Z}_{+}\), then_

\[\mathbb{E}[(X-\mu)^{p}]=\begin{cases}0&\text{if $p$ is odd}\\ \sigma^{p}(p-1)!!&\text{if $p$ is even}\end{cases}.\]

### Some technical results

Next, we will state some technical results that were obtained purely using algebraic manipulations and are not central to the main proof ideas. The first result states upper and lower bounds on the Gaussian complementary error function using an asymptotic expansion.

**Lemma 12** (\(\mathrm{Erfc}\) bound).: _For all \(x>0\),_

\[\mathrm{Erfc}(x)\leq \frac{1}{\sqrt{\pi}}\bigg{(}\frac{\exp(-x^{2})}{x}-\frac{\exp(-x^ {2})}{2x^{3}}+\frac{3\exp(-x^{2})}{4x^{5}}\bigg{)}\] (46) \[\mathrm{Erfc}(x)\geq \frac{1}{\sqrt{\pi}}\bigg{(}\frac{\exp(-x^{2})}{x}-\frac{\exp(-x ^{2})}{2x^{3}}\bigg{)}\] (47)

Proof.: By integration by parts:

\[\frac{\sqrt{\pi}}{2}\,\mathrm{Erfc}(x)=\int_{x}^{\infty}\exp(-t^ {2})dt=\left.\left(-\frac{\exp(-t^{2})}{2t}\right)\right|_{x}^{\infty}-\int_{ x}^{\infty}\frac{\exp(-t^{2})}{2t^{2}}dt\] \[= \frac{\exp(-x^{2})}{2x}-\bigg{(}\left.\left(-\frac{\exp(-t^{2})} {4t^{3}}\right)\right|_{x}^{\infty}-\int_{x}^{\infty}\frac{3\exp(-t^{2})}{4t ^{4}}dt\right)\] \[= \frac{\exp(-x^{2})}{2x}-\frac{\exp(-x^{2})}{4x^{3}}+\underbrace{ \int_{x}^{\infty}\frac{3\exp(-t^{2})}{4t^{4}}dt}_{\geq 0}\] (48) \[= \frac{\exp(-x^{2})}{2x}-\frac{\exp(-x^{2})}{4x^{3}}+\left.\left(- \frac{3\exp(-t^{2})}{8t^{5}}\right)\right|_{x}^{\infty}-\int_{x}^{\infty}\frac {15\exp(-t^{2})}{8t^{6}}dt\] \[= \frac{\exp(-x^{2})}{2x}-\frac{\exp(-x^{2})}{4x^{3}}+\frac{3\exp(- x^{2})}{8x^{5}}\underbrace{-\int_{x}^{\infty}\frac{15\exp(-t^{2})}{8t^{6}}dt}_{ \leq 0}\] (49)

The results follow by (48) and (49). 

Our next result, states an expression for the function \(p(n_{i})\) and its derivative as defined in (23).

**Lemma 13** (Value and derivative of penalty function at \(s^{*}\)).: _Let \(p(n_{i})=p_{i}(M_{\mathrm{50}},((n_{i},f_{i}^{*},h_{i}^{*}),s_{-i}^{*}))\) (see (23)) and \(s_{i}^{*},f_{i}^{*},h_{i}^{*}\) be as specified in (8). The penalty of agent \(i\) in Algorithm 1 satisfies:_

\[p(n_{i}^{*})= \frac{\sqrt{\frac{\alpha^{2}}{mn_{i}^{*}}}\sigma^{2}\Bigg{(}2m \sqrt{2\pi}\sqrt{\frac{\alpha^{2}}{mn_{i}^{*}}}-\exp\!\left(\frac{mn_{i}^{*}}{ 8\alpha^{2}}\right)(m-2)\pi\,\mathrm{Erfc}\!\left(\frac{1}{2\sqrt{2}\sqrt{ \frac{\alpha^{2}}{mn_{i}^{*}}}}\right)\Bigg{)}}{4\sqrt{2\pi}\alpha^{2}}+cn_{i}^ {*}\] (50) \[p^{\prime}(n_{i}^{*})= -\frac{\sigma^{2}}{64\frac{\alpha^{2}}{m-2}\frac{\alpha}{\sqrt{mn _{i}^{*}}}mn_{i}^{*}}\Bigg{(}\frac{4\alpha}{\sqrt{mn_{i}^{*}}}\bigg{(}\frac{4 \alpha^{2}m}{(m-2)n_{i}^{*}}-1\bigg{)}\] \[-\exp\!\left(\frac{mn_{i}^{*}}{8\alpha^{2}}\right)\!\left(\frac{4 \alpha^{2}}{mn_{i}^{*}}(m+1)-1\bigg{)}\sqrt{2\pi}\,\mathrm{Erfc}\!\left(\frac{ 1}{2\sqrt{2}\sqrt{\frac{\alpha^{2}}{mn_{i}^{*}}}}\right)\right)+c.\] (51)

This proof involves several algebraic manipulations, so we will provide an outline of our proof strategy. First, we will rearrange the denominator inside the expectation in (23), to write the LHS of (50) as \(J+K\mathbb{E}\!\left[\frac{1}{L+x^{2}}\right]\), and the LHS of (51) as \(J^{\prime}+K^{\prime}\mathbb{E}\!\left[\frac{1}{L+x^{2}}\right]+K^{\prime\prime }\mathbb{E}\!\left[\frac{1}{(L+x^{2})^{2}}\right]\), where the expectation is with respect to a standard normal \(\mathcal{N}(0,1)\) variable, \(J,K,K^{\prime},K^{\prime\prime},L\) are quantities that depend on \(n_{i},m,c,\sigma^{2},\alpha^{2}\), and importantly, \(L\) is strictly larger than \(0\). Using properties of the normal distribution, in Lemma 14, we prove the following result:

\[\mathbb{E}\left[\frac{1}{L+x^{2}}\right]=\sqrt{\frac{\pi}{2L}}\exp\!\left(\frac {L}{2}\right)\mathrm{Erfc}\left(\sqrt{\frac{L}{2}}\right)\] (52)\[\mathbb{E}\left[\frac{1}{(L+x^{2})^{2}}\right]=\frac{\sqrt{\pi}}{2\sqrt{2}L^{3/2}}(1 -L)\exp\!\left(\frac{L}{2}\right)\text{Erfc}\!\left(\sqrt{\frac{L}{2}}\right)+ \frac{1}{2L}\] (53)

By plugging in these expressions and then substituting \(n_{i}=n_{i}^{\star}\), we obtain (50) and (51).

Proof of Lemma 13.: We will rewrite \(p(n_{i}^{\star})\) and \(p^{\prime}(n_{i}^{\star})\) as the Gaussian integral of rational functions and use (52) to calculate their values. By (23),

\[p(n_{i}^{\star})= \mathbb{E}_{x\sim\mathcal{N}(0,1)}\left[\frac{1}{\frac{(m-2)n_{i} ^{\star}}{\sigma^{2}+\alpha^{2}\left(\frac{\sigma^{2}}{n_{i}^{\star}}+\frac{ \sigma^{2}}{n_{i}^{\star}}\right)x^{2}}+\frac{n_{i}^{\star}+n_{i}^{\star}}{ \sigma^{2}}}\right]+cn_{i}^{\star}\] \[= \mathbb{E}_{x\sim\mathcal{N}(0,1)}\left[\frac{1}{\frac{(m-2)n_{i }^{\star}}{\sigma^{2}+\alpha^{2}\frac{2\sigma^{2}}{n_{i}^{\star}}x^{2}}+\frac{ 2n_{i}^{\star}}{\sigma^{2}}}\right]+cn_{i}^{\star}=\frac{\sigma^{2}}{n_{i}^{ \star}}\mathbb{E}_{x\sim\mathcal{N}(0,1)}\left[\frac{1}{\frac{m-2}{1+\frac{ 2\alpha^{2}}{n_{i}^{\star}}x^{2}}+2}\right]+cn_{i}^{\star}\] \[= \frac{\sigma^{2}}{n_{i}^{\star}}\mathbb{E}_{x\sim\mathcal{N}(0,1 )}\left[\frac{1}{2}-\frac{m-2}{2}\frac{1}{\frac{4\alpha^{2}}{n_{i}^{\star}}x^ {2}+m}\right]+cn_{i}^{\star}\] \[= \frac{\sigma^{2}}{2n_{i}^{\star}}-\frac{\sigma^{2}}{n_{i}^{\star} }\frac{m-2}{2}\frac{n_{i}^{\star}}{4\alpha^{2}}\mathbb{E}_{x\sim\mathcal{N}(0,1)}\left[\frac{1}{x^{2}+\frac{mn_{i}^{\star}}{4\alpha^{2}}}\right]+cn_{i}^{ \star}\] \[= \frac{\sigma^{2}}{2n_{i}^{\star}}-\frac{\sigma^{2}}{4\alpha^{2}} \frac{m-2}{2}\exp\!\left(\frac{mn_{i}^{\star}}{8\alpha^{2}}\right)\text{Erfc} \!\left(\sqrt{\frac{mn_{i}^{\star}}{8\alpha^{2}}}\right)\!\sqrt{\frac{\pi}{ \frac{mn_{i}^{\star}}{2\alpha^{2}}}}+cn_{i}^{\star}\] \[\left(\text{In (\ref{eq:21}), let }L=\frac{mn_{i}^{\star}}{4\alpha^{2}}\right)\] \[= \text{RHS of (\ref{eq:22})}.\]

To prove the second statement of Lemma 13, by (24) and the dominated convergence theorem, we have:

\[p^{\prime}(n_{i}^{\star})=\mathbb{E}_{x\sim\mathcal{N}(0,1)}\! \left[-\sigma^{2}\frac{1+\frac{(m-2)n_{i}^{\star}}{\left(1+\alpha^{2}\left( \frac{1}{n_{i}^{\star}}+\frac{1}{n_{i}^{\star}}\right)x^{2}\right)^{2}}\frac{ \alpha^{2}x^{2}}{{n_{i}^{\star}}^{\star}}}{\left(\frac{(m-2)n_{i}^{\star}}{1+ \alpha^{2}\left(\frac{1}{n_{i}^{\star}}+\frac{1}{n_{i}^{\star}}\right)x^{2}}+ n_{i}^{\star}+n_{i}^{\star}\right)^{2}}\right]+c\] \[= -\frac{\sigma^{2}}{{n_{i}^{\star}}^{2}}\mathbb{E}_{x\sim\mathcal{ N}(0,1)}\left[\frac{1+\frac{(m-2)n_{i}^{\star}\alpha^{2}x^{2}}{\left(\frac{(m-2)n_{i}^{ \star}}{1+2\alpha^{2}x^{2}}+2\right)^{2}}}{\left(\frac{(m-2)n_{i}^{\star}}{1+2 \alpha^{2}x^{2}}+2\right)^{2}}\right]+c\] \[= -\frac{\sigma^{2}}{{4{n_{i}^{\star}}^{2}}}\mathbb{E}_{x\sim\mathcal{ N}(0,1)}\left[\frac{4\left(n_{i}^{\star}+2\alpha^{2}x^{2}\right)^{2}+4(m-2)n_{i}^{ \star}\alpha^{2}x^{2}}{\left((m-2)n_{i}^{\star}+2(n_{i}^{\star}+2\alpha^{2}x^{ 2})\right)^{2}}\right]+c\] \[= -\frac{\sigma^{2}}{{4{n_{i}^{\star}}^{2}}}\mathbb{E}_{x\sim \mathcal{N}(0,1)}\left[1+\frac{-(m-2)^{2}{n_{i}^{\star}}^{2}-4(m-2)n_{i}^{ \star}\left(n_{i}^{\star}+2\alpha^{2}x^{2}\right)+4(m-2)n_{i}^{\star}\alpha^{ 2}x^{2}}{\left((m-2)n_{i}^{\star}+2(n_{i}^{\star}+2\alpha^{2}x^{2})\right)^{2 }}\right]+c\]

[MISSING_PAGE_EMPTY:36]

This means \(I(t)\) satisfies the following ODE:

\[\begin{cases}-I^{\prime}(t)+LI(t)=\frac{1}{\sqrt{2t}}\\ I(0)=\sqrt{\frac{\pi}{2L}}\end{cases}.\] (54)

We solve (54) by multiplying integrating factor \(-\exp(-Lt)\):

\[\exp(-Lt)I^{\prime}(t)-L\exp(-Lt)I(t)=-\frac{1}{\sqrt{2t}}\exp(-Lt)\]

Note that the LHS is the derivative of \(\exp(-Lt)I(t)\), the ODE becomes:

\[\frac{d}{dt}(\exp(-Lt)I(t))=-\frac{1}{\sqrt{2t}}\exp(-Lt)\]

Integrating both sides over \(t\), we get:

\[\exp(-Lt)I(t)=-\int\frac{1}{\sqrt{2t}}\exp(-Lt)dt=-\int\frac{2}{\sqrt{2L}}\exp (-Lt)d\sqrt{Lt}=\operatorname{Erfc}(\sqrt{Lt})\sqrt{\frac{\pi}{2L}}+C,\]

where we use integration by substitution for the last two equalities and \(C\) is some constant that does not depend on \(t\). This means \(I(t)\) satisfies the following form:

\[I(t)=\exp(Lt)\bigg{(}\operatorname{Erfc}(\sqrt{Lt})\sqrt{\frac{\pi}{2L}}+C \bigg{)}\]

Using the initial condition \(I(0)=\sqrt{\frac{\pi}{2L}}\) and the fact that \(\operatorname{Erfc}(0)=0\), we conclude that \(C=0\). Thus

\[I(t)=\exp(Lt)\operatorname{Erfc}(\sqrt{Lt})\sqrt{\frac{\pi}{2L}}.\]

We can similarly derive an ODE for \(J(t)\). By calculation:

\[-J^{\prime}(t)+LJ(t)= \int_{-\infty}^{\infty}\frac{x^{2}+L}{(L+x^{2})^{2}}\frac{1}{ \sqrt{2\pi}}\exp\bigl{(}-tx^{2}\bigr{)}dx=I(t)\] \[J(0)= \int_{-\infty}^{\infty}\frac{1}{(L+x^{2})^{2}}\frac{1}{\sqrt{2 \pi}}dx=\frac{1}{2L^{3/2}}\sqrt{\frac{\pi}{2}}\]

Thus \(J(t)\) satisfies the following ODE:

\[\begin{cases}-J^{\prime}(t)+LJ(t)=I(t)\\ J(0)=\frac{1}{2L^{3/2}}\sqrt{\frac{\pi}{2}}\end{cases}.\] (55)

We similarly multiply integrating factor \(-\exp(-Lt)\) and integrate both sides:

\[\int_{0}^{t}d\exp(-Lx)J(x)=-\int_{0}^{t}I(x)\exp(-Lx)dx=-\int_{0} ^{t}\operatorname{Erfc}(\sqrt{Lx})\sqrt{\frac{\pi}{2L}}dx\] \[= -\left(\left.x\operatorname{Erfc}(\sqrt{Lx})\sqrt{\frac{\pi}{2L} }\right|_{0}^{t}+\int_{0}^{t}x\frac{\exp(-Lx)}{\sqrt{2x}}dx\right)\] \[(\text{Integration by parts})\] \[= -t\operatorname{Erfc}(\sqrt{Lt})\sqrt{\frac{\pi}{2L}}+\frac{ \sqrt{2}}{L^{3/2}}\frac{1}{2}\sqrt{Lt}\exp(-Lt)-\frac{\sqrt{2}}{L^{3/2}}\int_ {0}^{\sqrt{Lt}}\frac{1}{2}\exp(-y^{2})dy\]\[= -t\operatorname{Erfc}(\sqrt{Lt})\sqrt{\frac{\pi}{2L}}+\frac{\sqrt{t}}{ \sqrt{2}L}\exp(-Lt)-\frac{\sqrt{\pi}}{2\sqrt{2}L^{3/2}}\operatorname{Erf}\! \left(\sqrt{Lt}\right)\] \[(\text{By definition of Erf})\] \[= -t\operatorname{Erfc}(\sqrt{Lt})\sqrt{\frac{\pi}{2L}}+\frac{ \sqrt{t}}{\sqrt{2}L}\exp(-Lt)-\frac{\sqrt{\pi}}{2\sqrt{2}L^{3/2}}\!\left(1- \operatorname{Erfc}\!\left(\sqrt{Lt}\right)\right)\] \[(\text{By definition of Erfc})\] \[= \!\left(\frac{1}{2L}-t\right)\operatorname{Erfc}(\sqrt{Lt}) \sqrt{\frac{\pi}{2L}}+\frac{\sqrt{t}}{\sqrt{2}L}\exp(-Lt)-J(0)\] \[(\text{By (\ref{eq:22})})\]

This means:

\[J(t)= \exp(Lt)\!\left(\int_{0}^{t}d\exp(-Lx)J(x)+J(0)\right)\] \[= \exp(Lt)\!\left(\left(\frac{1}{2L}-t\right)\operatorname{Erfc}( \sqrt{Lt})\sqrt{\frac{\pi}{2L}}+\frac{\sqrt{t}}{\sqrt{2}L}\exp(-Lt)\right)\] \[= \sqrt{\frac{\pi}{2L}}\!\left(\frac{1}{2L}-t\right)\exp(Lt) \operatorname{Erfc}(\sqrt{Lt})+\frac{\sqrt{t}}{\sqrt{2}L}\]