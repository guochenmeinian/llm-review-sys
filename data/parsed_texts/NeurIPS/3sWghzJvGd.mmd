# Towards Unraveling and Improving Generalization

in World Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

World models have recently emerged as a promising approach for reinforcement learning (RL), as evidenced by its stimulating successes that world model based agents achieve state-of-the-art performance on a wide range of tasks in empirical studies. The primary goal of this study is to obtain a deep understanding of the mysterious generalization capability of world models, based on which we devise new methods to enhance it further. Thus motivated, we develop a stochastic differential equation formulation by treating the world model learning as a stochastic dynamic system in the latent state space, and characterize the impact of latent representation errors on generalization, for both cases with zero-drift representation errors and with non-zero-drift representation errors. Our somewhat surprising findings, based on both theoretic and experimental studies, reveal that for the case with zero drift, modest latent representation errors can in fact function as implicit regularization and hence result in generalization gain. We further propose a Jacobian regularization scheme to mitigate the compounding error propagation effects of non-zero drift, thereby enhancing training stability and generalization. Our experimental results corroborate that this regularization approach not only stabilizes training but also accelerates convergence and improves performance on predictive rollouts.

## 1 Introduction

Model-based reinforcement learning (RL) has emerged as a promising learning paradigm to improve sample efficiency by enabling agents to exploit a learned model for the physical environment. Notably, in recent works [14; 13; 15; 16; 21; 10; 32; 22] on world models, an RL agent learns the latent dynamics model of the environment, based on the observations and action signals, and then optimizes the policy over the learned dynamics model. Different from conventional approaches, world-model based RL takes an _end-to-end learning_ approach, where the building blocks (such as dynamics model, perception and action policy) are trained and optimized to achieve a single overarching goal, offering significant potential to improve generalization capability. For example, DreamerV2 and DreamerV3 achieve great progress in mastering diverse tasks involving continuous and discrete actions, image-based inputs, and both 2D and 3D environments, thereby facilitating robust learning across unseen task domains [14; 13; 15]. Recent empirical studies have also demonstrated the capacity of world models to generalize to unseen states in complex environments, such as autonomous driving [19]. Nevertheless, it remains not well understood when and how world models can generalize well in unseen environments.

In this work, we aim to first obtain a deep understanding of the _generalization_ capability of world models by examining the impact of _latent representation errors_, and then to devise new methods to enhance its generalization. While one may expect that optimizing a latent dynamics model (LDM) prior to training the task policy would minimize latent representation errors and hence can achieve better world model training, our somewhat surprising findings, based on both theoretical and empiricalstudies, reveal that modest latent representation errors in the training phase may in fact be beneficial. In particular, the alternating training strategy for world model learning, which simultaneously refines both the LDM and the action policy, could actually bring generalization gain, because the modest latent representation errors (and the corresponding induced gradient estimation errors) could enable the world model to visit unseen states and thus lead to improved generalization capacities. For instance, as shown in Table 1, our experimental results suggest that moderate batch sizes (e.g., 16 or 32) appear to position the induced errors within a regime conferring notable generalization benefits, leading to higher generalization improvement, when compared to the cases with very small (e.g., 8) or large (e.g., 64) batch sizes.

In a nutshell, _latent representation errors_ incurred by latent encoders, if designed properly, may actually facilitate world model training and enhance generalization. This insight aligns with recent advances in deep learning, where noise injection schemes have been studied as a form of implicit regularization to enhance models' robustness. For instance, recent study [2] analyzes the effects of introducing isotropic Gaussian noise at each layer of neural networks, identifying it as a form of implicit regularization. Another recent work [27] explores the addition of zero-drift Brownian motion to RNN architectures, demonstrating its regularizing effects in improving network's stability against noise perturbations.

We caution that _latent representation errors_ in world models differ from the above noise injection schemes ([27, 2]), in the following aspects: 1) Unlike the artificially injected noise only added in training, these errors are inherent in world models, leading to error propagation in the rollouts; 2) Unlike the controlled conditions of isotropic or zero-drift noise examined in prior studies, the errors in world models may not exhibit such well-behaved properties in the sense that the drift may be non-zero and hence biased; 3) additionally, in the iterative training of world models and agents, the error originating from the encoder affects the policy learning and agent exploration. In light of these observations, we develop a continuous-time stochastic differential equation (SDE) formulation by treating the world model learning as a stochastic dynamic system with stochastic latent states. This approach offers an insightful view on model errors as stochastic perturbation, enabling us to obtain an explicit characterization to quantify the impacts of the errors on world models' generalization capability. Our main contributions can be summarized as follows.

* _Latent representation errors as implicit regularization:_ Aiming to understand the generalization capability of world models and improve it further, we develop a continuous-time SDE formulation by treating the world model learning as a stochastic dynamic system in latent state space. Leveraging tools in stochastic calculus and differential geometry, we characterize the impact of latent representation errors on world models' generalization. Our findings reveal that under some technical conditions, modest latent representation errors can in fact function as implicit regularization and hence result in generalization gain.
* _Improving generalization in non-zero drift cases via Jacobian regularization:_ For the case where latent representation errors exhibit non-zero drifts, we show that the additional bias term would degrade the implicit regulation and hence may make the learning unstable. We propose to add Jacobian regularization to mitigate the effects of non-zero-drift errors in training. Experimental studies are carried out to evaluate the efficacy of Jacobian regularization.
* _Reducing error propagation in predictive rollouts:_ We explicitly characterize the effect of latent representation errors on predictive rollouts. Our experimental results corroborate that Jacobian regularization can reduce the impact of error propagation on rollouts, leading to enhanced prediction performance and accelerated convergence in tasks with longer time horizons.
* _Bounding Latent Representation Error:_ We establish a novel bound on the latent representation error within CNN encoder-decoder architectures. To our knowledge, this is the first quantifiable

\begin{table}
\begin{tabular}{c|c c c|c c c} \hline batch size & perturbation & \(\alpha=10\) & \(\alpha=20\) & \(\alpha=30\) & \(\beta=25\) & \(\beta=50\) & \(\beta=75\) \\ \hline
8 & 691.62 & 363.73 & 153.67 & 624.67 & 365.31 & 216.52 \\
16 & **830.39** & 429.62 & **213.78** & **842.26** & **569.42** & **375.61** \\
32 & **869.39** & **436.87** & **312.99** & **912.12** & **776.86** & **655.26** \\
64 & 754.47 & **440.44** & 80.24 & 590.41 & 255.2 & 119.62 \\ \hline \end{tabular}
\end{table}
Table 1: Reward values on unseen perturbed states by rotation (\(\alpha\)) or mask (\(\beta\%\)) with \(\mathcal{N}(0.15,0.5)\).

bound applied to a learned latent representation model, and the analysis carries over to other architectures (e.g., ReLU) along the same line.

**Notation.** We use Einstein summation convention for succinctness, where \(a_{i}b_{i}\) denotes \(\sum_{i}a_{i}b_{i}\). We denote functions in \(\mathcal{C}^{k,\alpha}\) as being \(k\)-times differentiable with \(\alpha\)-Holder continuity. The Euclidean norm of a vector is represented by \(\|\cdot\|\), and the Frobenius norm of a matrix by \(|\cdot|_{F}\); this notation may occasionally extend to tensors. The notation \(x^{i}\) indicates the \(i^{th}\) coordinate of the vector \(x\), and \(A^{ij}\) the \((i,j)\)-entry of the matrix \(A\). Function composition is denoted by \(f\circ g\), implying \(f(g)\). For a differentiable function \(f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}\), its Jacobian matrix is denoted by \(\frac{\partial f}{\partial x}\in\mathbb{R}^{m\times n}\). Its gradient, following conventional definitions, is denoted by \(\nabla f\). The constant \(C\) may represent different values in distinct contexts.

## 2 Related Work

**World model based RL.** World models have demonstrated remarkable efficacy in visual control tasks across various platforms, including Atari [1] and Minecraft [8], as detailed in the studies by Hafner et al. [14; 13; 15]. These models typically integrate encoders and memory-augmented neural networks, such as RNNs [33], to manage the latent dynamics. The use of variational autoencoders (VAE) [7; 23] to map sensory inputs to a compact latent space was pioneered by Ha et al. [12]. Furthermore, the Dreamer algorithm [13; 16] employs convolutional neural networks (CNNs) [24] to enhance the processing of both hidden states and image embeddings, yielding models with improved predictive capabilities in dynamic environments.

**Continuous-time RNNs.** The continuous-time assumption is standard for theoretical formulations of RNN models. Li et al. [26] study the optimization dynamics of linear RNNs on memory decay. Chang et al. [4] propose AntisymmetricRNN, which captures long-term dependencies through the control of eigenvalues in its underlying ODE. Chen et al. [5] propose the symplectic RNN to model Hamiltonians. As continuous-time formulations can be discretized with Euler methods [4; 5] (or with Euler-Maruyama methods if stochastic in [27]) and yield similar insights, this step is often eliminated for brevity.

**Implicit regularization by noise injection in RNN.** Studies on noise injection as a form of implicit regularization have gained traction, with Lim et al. [27] deriving an explicit regularizer under small noise conditions, demonstrating bias towards models with larger margins and more stable dynamics. Camuto et al. [2] examine Gaussian noise injections at each layer of neural networks. Similarly, Wei et al. [31] provide analytic insights into the dual effects of dropout techniques.

## 3 Demystifying World Model: A Stochastic Differential Equation Approach

As pointed out in [14; 13; 15; 16], critical to the effectiveness of the world model representation is the stochastic design of its latent dynamics model. The model can be outlined by the following key components: an encoder that compresses high dimensional observations \(s_{t}\) into a low-dimensional latent state \(z_{t}\) (Eq.1), a sequence model that captures temporal dependencies in the environment (Eq.2), a transition predictor that estimates the next latent state (Eq.3), and a latent decoder that reconstructs observed information from the posterior (Eq.4):

\[\text{Latent Encoder:}\ \ z_{t}\sim q_{\text{enc}}(z_{t}\,|\,h_{t},s_{t}),\] (1) \[\text{Sequence Model:}\ h_{t}=f(h_{t-1},z_{t-1},a_{t-1}),\] (2) \[\text{Transition Predictor:}\ \tilde{z}_{t}\sim p(\tilde{z}_{t}\,|\,h_{t}),\] (3) \[\text{Latent Decoder:}\ \tilde{s}_{t}\sim q_{\text{dec}}(\tilde{s}_{t}\,|\,h_{t}, \tilde{z}_{t})\] (4)

In this work, we consider a popular class of world models, including Dreamer and PlaNet, where \(\{z,\,\tilde{z},\,\tilde{s}\}\) have distributions parameterized by neural networks' outputs, and are Gaussian when the outputs are known. It is worth noting that \(\{z,\,\tilde{z},\,\tilde{s}\}\) may not be Gaussian and are non-Gaussian in general. This is because while \(z\) is conditional Gaussian, its mean and variance are random variables which are learned by the encoder with \(s\) and \(h\) being the inputs, rendering that \(z\) is non-Gaussian due to the mixture effect. For this setting, we have a continuous-time formulation where the latent dynamics model can be interpreted as stochastic differential equations (SDEs) with coefficient functions of known inputs. Due to space limitation, we refer to Proposition B.1 in the Appendix for a more detailed treatment.

Consider a complete, filtered probability space \((\Omega,\,\mathcal{F},\,\{\mathcal{F}_{t}\}_{t\in[0,T]},\,\mathbb{P}\,)\) where independent standard Brownian motions \(B_{t}^{\,\text{enc}},\,B_{t}^{\,\text{pred}},B_{t}^{\,\text{seq}},\,B_{t}^{\, \text{dec}}\) are defined such that \(\mathcal{F}_{t}\) is their augmented filtration, and \(T\in\mathbb{R}\) as the time length of the task environment. We interpret the stochastic dynamics of LDM with latent representation errors through coupled SDEs representing continuous-time analogs of the discrete components:

\[\text{Latent Encoder:}\,d\,z_{t} =(q_{\text{enc}}(h_{t},s_{t})+\varepsilon\,\sigma(h_{t},s_{t}))\, dt+(\bar{q}_{\text{enc}}(h_{t},s_{t})+\varepsilon\,\bar{\sigma}(h_{t},s_{t}))\, dB_{t}^{\,\text{enc}},\] (5) \[\text{Sequence Model:}\,d\,h_{t} =f(h_{t},z_{t},\pi(h_{t},z_{t}))\,dt+\bar{f}(h_{t},z_{t},\pi(h_{t},z_{t}))\,dB_{t}^{\,\text{seq}}\] (6) \[\text{Transition Predictor:}\,d\,\tilde{z}_{t} =p(h_{t})\,dt+\bar{p}(h_{t})\,dB_{t}^{\,\text{pred}},\] (7) \[\text{Latent Decoder:}\,d\,\tilde{s}_{t} =q_{\text{dec}}(h_{t},\tilde{z}_{t})\,dt+\bar{q}_{\text{dec}}(h_{t },\tilde{z}_{t})\,dB_{t}^{\,\text{dec}},\] (8)

where \(\pi(h,\tilde{z})\) is a policy function as a local maximizer of value function and the stochastic process \(s_{t}\) is \(\mathcal{F}_{t}\)-adapted. Notice that \(\bar{f}\) is often a zero function indicating that Equation (6) is an ODE, as the sequence model is generally designed as deterministic. Generally, the coefficient functions in \(dt\) and \(dB_{t}\) terms in SDEs are referred to as the _drift_ and _diffusion_ coefficients. Intuitively, the diffusion coefficients here represent the stochastic model components. In Equation (5), \(\sigma(\cdot,\cdot)\) and \(\bar{\sigma}(\cdot,\cdot)\) denotes the drift and diffusion coefficients of the _latent representation errors_, respectively. Both are assumed to be functions of hidden states \(h_{t}\) and task states \(s_{t}\). In addition, \(\varepsilon\) indicates the magnitude of the error.

Next, we impose standard assumptions on these SDEs (5) - (8) to guarantee the well-definedness of the solution to SDEs. For further technical details, we refer readers to fundamental works on SDEs in the literature (e.g.,[30; 17]).

**Assumption 3.1**.: The drift coefficient functions \(q_{\text{enc}},\,f,\,p\) and \(q_{\text{dec}}\) and the diffusion coefficient functions \(\bar{q}_{\text{enc}},\,\bar{p}\) and \(\bar{q}_{\text{dec}}\) are bounded and Borel-measurable over the interval \([0,T]\), and of class \(\mathcal{C}^{3}\) with bounded Lipschitz continuous partial derivatives. The initial values \(z_{0},h_{0},\tilde{z}_{0},\tilde{s}_{0}\) are square-integrable random variables.

**Assumption 3.2**.: \(\sigma\) and \(\bar{\sigma}\) are bounded and Borel-measurable and are of class \(\mathcal{C}^{3}\) with bounded Lipschitz continuous partial derivatives over the interval \([0,T]\).

### Latent Representation Errors in CNN Encoder-Decoder Networks

As shown in the empirical studies with different batch sizes (Table 1), the latent representation error would also enrich generalization when it is within a moderate regime. In this section, we show that the latent representation error, in the form of approximation error corresponding to widely used CNN encoder-decoder, could be made sufficiently small by finding appropriate CNN network configuration. In particular, this result provides theoretical justification to interpreting latent representation error as stochastic perturbation in the dynamical system defined in Equations (5 - 8), as the error magnitude \(\varepsilon\) can be made sufficiently small by CNN network configuration.

Consider the state space \(\mathcal{S}\subset\mathbb{R}^{d_{\mathcal{S}}}\) and the latent space \(\mathcal{Z}\). Consider a state probability measure \(Q\) on the state space \(\mathcal{S}\) and a probability measure \(P\) on the latent space \(\mathcal{Z}\). As high-dimensional state space in image-based tasks frequently exhibit _intrinsic lower-dimensional geometric structure_, we adopt the latent manifold assumption, formally stated as follows:

**Assumption 3.3**.: (Latent manifold assumption) For a positive integer \(k\), there exists a \(d_{\mathcal{M}}\)-dimensional \(\mathcal{C}^{k,\alpha}\) submanifold \(\mathcal{M}\) (with \(\mathcal{C}^{k+3,\alpha}\) boundary) with Riemannian metric \(g\) and has positive reach and also isometrically embedded in the state space \(\mathcal{S}\subset\mathbb{R}^{d_{\mathcal{S}}}\) and \(d_{\mathcal{M}}\ll\)\(d_{\mathcal{S}}\), where the state probability measure is supported on. In addition, \(\mathcal{M}\) is a compact, orientable, connected manifold.

**Assumption 3.4**.: (Smoothness of state probability measure) \(Q\) is a probability measure supported on \(\mathcal{M}\) with its Radon-Nikodym derivative \(q\in\mathcal{C}^{k,\alpha}(\mathcal{M},\mathbb{R})\) w.r.t \(\mu_{\mathcal{M}}\).

Let \(\mathcal{Z}\) be a closed ball in \(\mathbb{R}^{d_{\mathcal{M}}}\), that is \(\{x\in\mathbb{R}^{d_{\mathcal{M}}}\,:\,\|x\|\leq 1\,\}\). \(P\) is a probability measure supported on \(\mathcal{Z}\) with its Radon-Nikodym derivative \(p\in\mathcal{C}^{k,\alpha}(\mathcal{Z},\mathbb{R})\) w.r.t \(\mu_{\mathcal{Z}}\). In practice, it is usually an easy-to-sample distribution such as uniform distribution which is determined by a specific encoder-decoder architecture choice.

**Latent Representation Learning**.: We define the _latent representation learning_ as to find encoder \(g_{\text{enc}}:\mathcal{M}\rightarrow\mathcal{Z}\) and decoder \(g_{\text{dec}}:\mathcal{Z}\rightarrow\mathcal{M}\) as maps that optimize the following objectives:

\[\min_{g_{\text{enc}}\in\mathcal{G}}W_{1}\left(g_{\text{enc}_{\text{alg}}}\,Q,\,P\right);\quad\min_{g_{\text{dec}}\in\mathcal{G}}W_{1}\left(Q,\,g_{\text{ dec}_{\text{alg}}}\,P\right).\]Here, \(g_{\text{enc}_{\#}}\,Q\) and \(g_{\text{dec}_{\#}}\,P\) represent the pushforward measures of \(Q\) and \(P\) through the encoder map \(g_{\text{enc}}\) and decoder map \(g_{\text{dec}}\), respectively. The latent representation error is understood as the "difference" of pushforward measure by the encoder/decoder and target measure. Here, _to understand the "scale" of the error \(\varepsilon\) in Equation (5), we use \(W_{1}\) for the discrepancy between probability measures._ In particular, for Dreamer-type loss function that uses KL-divergence, we note that squared \(W_{1}\) distance between two probability measures can be upper bounded by their KL-divergence up to a constant [11], implying that one could reasonably expect the \(W_{1}\) distance to also decrease when KL-divergence is used in the model.

**CNN configuration.** As a popular choice choice in encoder-decoder architecture is CNN, we consider a general CNN function \(f_{\text{CNN}}:\mathcal{X}\rightarrow\mathbb{R}\). Let \(f_{\text{CNN}}\) have \(L\) hidden layers, represented as: for \(x\in\mathcal{X},f_{\text{CNN}}(x):=A_{L+1}\circ A_{L}\circ\cdots\circ A_{2} \circ A_{1}(x),\) where \(A_{i}\)'s are either convolutional or downsampling operators. For convolutional layers, \(A_{i}(x)=\sigma(W_{i}^{c}x+b_{i}^{c}),\) where \(W_{i}^{c}\in\mathbb{R}^{d_{i}\times d_{i-1}}\) is a structured sparse Toeplitz matrix from the convolutional filter \(\{w_{i}^{(i)}\}_{j=0}^{s(i)}\) with filter length \(s(i)\in\mathbb{N}_{+},\,b_{i}^{c}\in\mathbb{R}^{d_{i}}\) is a bias vector, and \(\sigma\) is the ReLU activation function. For downsampling layers, \(A_{i}(x)=D_{i}(x)=(x_{jm_{i}})_{j=1}^{\left\lfloor d_{i-1}/m_{i}\right\rfloor},\) where \(D_{i}:\mathbb{R}^{d_{i}\times d_{i-1}}\) is the downsampling operator with scaling parameter \(m_{i}\leq d_{i-1}\) in the \(i\)-th layer. We examine the class of functions represented by CNNs, denoted by \(\mathcal{F}_{\text{CNN}}\), defined as:

\[\mathcal{F}_{\text{CNN}}=\{f_{\text{CNN}}\text{ as in defined above with any choice of }A_{i},\,i=1,\ldots,L+1\}.\]

For the specific definition of \(\mathcal{F}_{\text{CNN}}\), we refer to [29]'s (4), (5) and (6).

**Assumption 3.5**.: Assume that \(\mathcal{M}\) and \(\mathcal{Z}\) are locally diffeomorphic, that is there exists a map \(F:\mathcal{M}\rightarrow\mathcal{Z}\) such that at every point \(x\) on \(\mathcal{M},\ \det(d\,F(x))\neq 0\).

**Theorem 3.6**.: _(Approximation Error of Latent Representation). Under Assumption 3.3, 3.4 and 3.5, for \(\theta\in(0,1),\) let \(d_{\theta}:=\mathcal{O}(d_{\mathcal{M}}\theta^{-2}\log\frac{d}{\theta})\). For positive integers \(M\) and \(N\), there exists an encoder \(g_{\text{enc}}\) and decoder \(g_{\text{dec}}\in\mathcal{F}_{\text{CNN}}(L,S,W)\) s.t._

\[W_{1}(g_{\text{enc}_{\#}}Q,P)\leq d_{\mathcal{M}}C(NM)^{-\frac{2(k+1)}{d_{ \theta}}},\quad W_{1}(g_{\text{dec}_{\#}}P,Q)\leq d_{\mathcal{M}}C(NM)^{-\frac {2(k+1)}{d_{\theta}}}.\]

Theorem 3.6 indicates that with an appropriate CNN configuration, the \(W_{1}\) approximation error can be made to reside in a small region, as the best candidate within the function class is indeed capable of approximating the oracle encoder/decoder. In particular, this result indicates that the error magnitude \(\varepsilon\) in SDE (5) can be assumed to be small. This allows us to apply the perturbation analysis of the dynamical system defined in Equations (5 - 8) in the following sections.

### Latent Representation Errors as Implicit Regularization towards Generalization

In this section, we investigate the impact of latent representation errors on generalization, for the two cases with _zero drift_ and _non-zero drift_, respectively. We show that under mild conditions, the _zero-drift_ errors can function as a natural form of _implicit regularization_, promoting wider landscapes for improved robustness. Nevertheless, we caution that when latent representation errors have non-zero drift, it could lead to poor regularization with _unstable bias_ and degrade world model's generalization, calling for explicit regularization.

To simplify the notation here, we consider the system equations, specifically Equations (5), (6) - (8), as one stochastic system. Let \(x_{t}=(z_{t},h_{t},\tilde{z}_{t},\tilde{s}_{t})\) and \(B_{t}=(B_{t}^{\text{enc}},B_{t}^{\text{seq}},B_{t}^{\text{pred}},B_{t}^{\text{ dec}})\):

\[d\,x_{t}=(g(x_{t},t)+\varepsilon\,\sigma(x_{t},t))\ dt+\sum_{i}\bar{g}_{i}(x_{t}, t)+\varepsilon\,\bar{\sigma}_{i}(x_{t},t)\,dB_{t}^{i},\] (9)

where \(g\), and \(\bar{g}_{i}\) are structured accordingly for the respective components, employing the Einstein summation convention for concise representation. For abuse of notation, \(\sigma=(\sigma,0,0,0),\bar{\sigma}=(\bar{\sigma},0,0,0)\). For a given error magnitude \(\varepsilon\), we denote the solution to SDE (9) as \(x_{t}^{\varepsilon}\). Intuitively, \(x_{t}^{\varepsilon}\) is the perturbed trajectory of the latent dynamics model. In particular, when \(\varepsilon=0\), indicating that the absence of latent representation error in the model, the solution is denoted as \(x_{t}^{0}\).

#### 3.2.1 The Case with Zero-drift Representation Errors

When the drift coefficient \(\sigma=0\), the latent representation errors correspond to a class of well-behaved stochastic processes. The following result translates the induced perturbation on the stochastic latent dynamics model's loss function \(\mathcal{L}\) to a form of explicit regularization. We assume that \(\mathcal{L}\in\mathcal{C}^{2}\) and depends on \(z_{t},h_{t},\tilde{z}_{t},\tilde{s}_{t}\). Loss functions used in practical implementation, e.g. in DreamerV3, reconstruction loss \(J_{O}\), reward loss \(J_{R}\), consistency loss \(J_{D}\), all satisfy this condition.

**Theorem 3.7**.: _(Explicit Effect Induced by Zero-Drift Representation Error) Under Assumptions 3.1 and 3.2 and considering a loss function \(\mathcal{L}\in\mathcal{C}^{2}\), the explicit effects of the zero-drift error can be marginalized out as follows: as \(\varepsilon\to 0\),_

\[\mathbb{E}\,\mathcal{L}\left(x_{t}^{\varepsilon}\right)=\mathbb{E}\, \mathcal{L}(x_{t}^{0})+\mathcal{R}+\mathcal{O}(\varepsilon^{3}),\] (10)

_where the regularization term \(\mathcal{R}\) is given by \(\mathcal{R}:=\varepsilon\,\mathcal{P}+\varepsilon^{2}\left(\mathcal{Q}+ \frac{1}{2}\,\mathcal{S}\right),\) with_

\[\mathcal{P}:= \,\mathbb{E}\,\nabla\mathcal{L}(x_{t}^{0})^{\top}\Phi_{t}\sum_{ k}\xi_{t}^{k},\] (11) \[\mathcal{S}:= \,\mathbb{E}\sum_{k_{1},k_{2}}(\Phi_{t}\xi_{t}^{k_{1}})^{\nabla 2}\mathcal{L}(x_{t}^{0},t)\,(\Phi_{t}\xi_{t}^{k_{2}})^{j},\] (12) \[\mathcal{Q}:= \,\mathbb{E}\,\nabla\mathcal{L}(x_{t}^{0})^{\top}\Phi_{t}\int_{0 }^{t}\Phi_{s}^{-1}\,\mathcal{H}^{k}(x_{s}^{0},s)dB_{t}^{k}.\] (13)

_Square matrix \(\Phi_{t}\) is the stochastic fundamental matrix of the corresponding homogeneous equation:_

\[d\Phi_{t}=\frac{\partial\bar{g}_{k}}{\partial x}(x_{t}^{0},t)\,\Phi_{t}\,dB_{ t}^{k},\quad\Phi(0)=I,\]

_and \(\xi_{t}^{k}\) is the shorthand for \(\int_{0}^{t}\Phi_{s}^{-1}\bar{\sigma}_{k}(x_{s}^{0},s)dB_{t}^{k}\). Additionally, \(\mathcal{H}^{k}(x_{s}^{0},s)\) is represented by for \(\sum_{k_{1},k_{2}}\frac{\partial^{2}\bar{g}_{k}}{\partial x^{i}\partial x_{s} }(x_{s}^{0},s)\left(\xi_{s}^{k_{1}}\right)^{i}\left(\xi_{s}^{k_{2}}\right)^{j}\)._

The proof is relegated to Appendix B in the Supplementary Materials.

When the loss \(\mathcal{L}\) is convex, then its Hessian, \(\nabla^{2}\mathcal{L}\), is positive semi-definite, which ensures that the term \(\mathcal{S}\) is non-negative. _The presence of this Hessian-dependent term \(\mathcal{S}\), under latent representation error, implies a tendency towards wider minima in the loss landscape._ Empirical results from [20] indicates that wider minima correlate with improved robustness of implicit regularization during training. This observation also aligns with the theoretical insights in [27] that the introduction of Brownian motion, which is indeed zero-drift by definition, in training RNN models promotes robustness. We note that in addition, when the error \(\bar{\sigma}_{t}(\cdot)\) is too small, the effect of term \(\mathcal{S}\) as implicit regularization would not be as significant as desired. Intuitively, this insight resonates with the empirical results in Table 1 that model's robustness gain is not significant when the error induced by small batch sizes is too small.

We remark that the exact loss form treated here is simplified compared to that in the practical implementation of world models, which frequently depends on the probability density functions (PDFs) of \(z_{t},h_{t},\tilde{z}_{t},\tilde{s}_{t}\). In principle, the PDE formulation corresponding to the PDFs of the perturbed \(x_{t}^{\varepsilon}\) can be derived from the Kolmogorov equation of the SDE (9), and the technicality is more involved but can offer more direct insight. We will study this in future work.

#### 3.2.2 The Case with Non-Zero-Drift Representation Errors

In practice, latent representation errors may not always exhibit _zero drift_ as in idealized noise-injection schemes for deep learning ([27], [2]). When the drift coefficient \(\sigma\) is non-zero or a function of input data \(h_{t}\) and \(s_{t}\) in general, the explicit regularization terms induced by the latent representation error may lead to unstable bias in addition to the regularization term \(\mathcal{R}\) in Theorem 3.7. With a slight abuse of notation, we denote \(\bar{g}_{0}\) as \(g\) from Equation (9) for convenience.

**Corollary 3.8**.: _(Additional Bias Induced by Non-Zero Drift Representation Error) Under Assumptions 3.1 and 3.2 and considering a loss function \(\mathcal{L}\in\mathcal{C}^{2}\), the explicit effects of the general form error can be marginalized out as follows as \(\varepsilon\to 0\):_

\[\mathbb{E}\,\mathcal{L}\left(x_{t}^{\varepsilon}\right)=\mathbb{E}\,\mathcal{L}( x_{t}^{0})+\mathcal{R}+\tilde{\mathcal{R}}+\mathcal{O}(\varepsilon^{3}),\] (14)

_where the additional bias term \(\tilde{\mathcal{R}}\) is given by \(\tilde{\mathcal{R}}:=\,\varepsilon\,\tilde{\mathcal{P}}+\varepsilon^{2}\left( \tilde{\mathcal{Q}}+\tilde{\mathcal{S}}\right)\), with_

\[\tilde{\mathcal{P}}:= \,\mathbb{E}\,\nabla\mathcal{L}(x_{t}^{0})^{\top}\Phi_{t}\, \tilde{\xi}_{t},\] (15) \[\tilde{\mathcal{Q}}:= \,\mathbb{E}\,\nabla\mathcal{L}(x_{t}^{0})^{\top}\Phi_{t}\int_{ 0}^{t}\Phi_{s}^{-1}\,\mathcal{H}^{0}(x_{s}^{0},s)\,dt,\] (16) \[\tilde{\mathcal{S}}:= \,\mathbb{E}\sum_{k}(\Phi_{t}\tilde{\xi}_{t})^{\top}\nabla^{2} \mathcal{L}(x_{t}^{0},t)\,(\Phi_{t}\xi_{t}^{k})^{j},\] (17)and \(\tilde{\xi}_{t}\) being the shorthand for \(\int_{0}^{t}\Phi_{s}^{-1}\sigma_{k}(x_{s}^{0},s)dt\)._

The presence of the new bias term \(\tilde{\mathcal{R}}\) implies that regularization effects of latent representation error could be unstable. The presence of \(\tilde{\xi}\) in \(\tilde{\mathcal{P}}\), \(\tilde{\mathcal{Q}}\) and \(\tilde{\mathcal{S}}\) induces a bias to the loss function with its magnitude dependent on the error level \(\varepsilon\), since \(\tilde{\xi}\) is a non-zero term influenced on the drift term \(\sigma\). This contrasts with the scenarios described in [27] and [2], where the noise injected for implicit regularization follows a zero-mean Gaussian distribution. To modulate the regularization and bias terms \(\mathcal{R}\) and \(\tilde{\mathcal{R}}\) respectively, we note that a common factor, the fundamental matrix \(\Phi\), can be bounded by

\[\mathbb{E}\sup_{\epsilon}\|\Phi_{t}\|_{F}^{2}\leq\sum_{k}C\exp \left(C\,\mathbb{E}\sup_{\epsilon}\left\|\frac{\partial g_{k}}{\partial x}(x_ {\epsilon}^{0},t)\right\|_{F}^{2}\right)\] (18)

which can be shown by using the Burkholder-Davis-Gundy Inequality and Gronwall's Lemma. Based on this observation, we next propose a regularizer on input-output Jacobian norm \(\|\frac{\partial g_{k}}{\partial x}\|_{F}\) that could modulate the new bias term \(\tilde{\mathcal{R}}\) for stabilized implicit regularization.

## 4 Enhancing Predictive Rollouts via Jacobian Regularization

In this section, we study the effects of latent representation errors on predictive rollouts using latent state transitions, which happen in the inference phase in world models. We then propose to use Jacobian regularization to enhance the quality of rollouts. In particular, we first obtain an upper bound of state trajectory divergence in the rollout due to the representation error. We show that the error effects on task policy's \(Q\) function can be controlled through model's input-output Jacobian norm.

In world model learning, the task policy is optimized over the rollouts of dynamics model with the initial latent state \(z_{0}\). Recall that latent representation error is introduced to \(z_{0}\) when latent encoder encodes the initial state \(s_{0}\) from task environment. Intuitively, the latent representation error would propagate under the sequence model and impact the policy learning, which would then affect the generalization capacity through increased exploration.

Recall that the sequence model and the transition predictor are given as follows:

\[d\,h_{t}=f(h_{t},\tilde{z}_{t},\pi(h_{t},\tilde{z}_{t}))\,dt, \quad d\,\tilde{z}_{t}=p(h_{t})dt+\bar{p}(h_{t})\,dB_{t},\] (19)

with random variables \(h_{0}\), \(\tilde{z}_{0}+\varepsilon\) as the initial values, respectively. In particular, \(\varepsilon\) is a random variable of proper dimension, representing the error from encoder introduced at the initial step. We impose the standard assumption on the error to ensure the well-definedness of the SDEs.

Under Assumption 3.1, there exists a unique solution to the SDEs (for Equations 19 with square-integrable \(\varepsilon\)), denoted as \((h_{t}^{\varepsilon},z_{t}^{\varepsilon})\). In the case of no error introduced, i.e., \(\varepsilon=0\), we denote the solution of the SDEs as \((h_{t}^{0},z_{t}^{0})\) understood as the rollout under the absence of latent representation error. To understand how to modulate impacts of the error in rollouts, our following result gives an upper bound on the expected divergence between the perturbed rollout trajectory \((h_{t}^{\varepsilon},z_{t}^{\varepsilon})\) and the original \((h_{t}^{0},z_{t}^{0})\) over the interval \([0,T]\).

**Theorem 4.1**.: _(Bounding trajectory divergence) For a square-integrable random variable \(\varepsilon\), let \(\delta:=\mathbb{E}\,\|\varepsilon\|\) and \(d_{\varepsilon}:=\mathbb{E}\sup_{t\in[0,T]}\left\|h_{t}^{\varepsilon}-h_{t}^{0 }\right\|^{2}+\left\|\tilde{z}_{t}^{\varepsilon}-\tilde{z}_{t}^{0}\right\|^{2}\). As \(\delta\to 0\),_

\[d_{\varepsilon}\,\leq\delta\,C\left(\mathcal{J}_{0}+\mathcal{J}_{1} \right)+\,\delta^{2}\,C\exp\left(\,\mathcal{H}_{0}\left(\mathcal{J}_{0}+ \mathcal{J}_{1}\right)\right)+\delta^{2}\,C\exp\left(\,\mathcal{H}_{1}\left( \mathcal{J}_{0}+\mathcal{J}_{1}\right)\right)+\mathcal{O}(\delta^{3}),\]

_where \(C\) is a constant dependent on T. \(\mathcal{J}_{1}\) and \(\mathcal{J}_{2}\) are Jacobian-related terms, and \(\mathcal{H}_{1}\) and \(\mathcal{H}_{2}\) are Hessian-related terms._

The Jacobian-related terms \(\mathcal{J}_{1}\) and \(\mathcal{J}_{2}\) are defined as \(\mathcal{J}_{0}:=\exp\left(\mathcal{F}_{h}+\mathcal{F}_{z}+\mathcal{P}_{h} \right),\,\mathcal{J}_{1}:=\exp\left(\tilde{\mathcal{P}}_{h}\right)\); the Hessian-related terms \(\mathcal{H}_{0}\) and \(\mathcal{H}_{1}\) are defined as \(\mathcal{H}_{0}:=\mathcal{F}_{hh}+\mathcal{F}_{hz}+\mathcal{F}_{zh}+\mathcal{F} _{zz}+\mathcal{P}_{hh},\mathcal{H}_{1}:=\tilde{\mathcal{P}}_{hh}\), where \(\mathcal{F}_{h}\), \(\mathcal{F}_{z}\) are the expected \(\sup\) Frobenius norm of Jacobians of \(f\) w.r.t \(h\), \(z\), respectively, and \(\mathcal{F}_{hh},\mathcal{F}_{hz},\mathcal{F}_{zh},\mathcal{F}_{zz}\) are the corresponding expected \(\sup\) Frobenius norm of second-order derivatives. Other terms are similarly defined. A detailed description of all terms, can be found in Appendix C.1.

Theorem 4.1 correlates with the empirical findings in [14] regarding the diminished predictive accuracy of latent states \(\tilde{z}_{t}\) over the extended horizons. In particular, Theorem 4.1 suggests that the expected divergence from error accumulation hinges on the expected error magnitude, the Jacobian norms within the latent dynamics model and the horizon length \(T\).

Our next result reveals how initial latent representation error influences the value function \(Q\) during the prediction rollouts, which again verifies that the perturbation is dependent on expected error magnitude, the model's Jacobian norms and the horizon length \(T\):

**Corollary 4.2**.: _For a square-integrable \(\varepsilon\), let \(x_{t}:=(h_{t},z_{t})\). Then, for any action \(a\in\mathcal{A}\), the following holds for value function \(Q\) almost surely:_

\[Q(x_{t}^{\varepsilon},a)= Q(x_{t}^{0},a)+\frac{\partial}{\partial x}Q(x_{t}^{0},a)\left( \varepsilon^{i}\partial_{i}\,x_{t}^{0}+\frac{1}{2}\varepsilon^{i}\,\varepsilon ^{j}\,\partial_{ij}^{2}\,x_{t}^{0}\right)\] \[+\frac{1}{2}(\varepsilon^{i}\,\partial_{i}\,x_{t}^{0})^{\top} \frac{\partial^{2}}{\partial x^{2}}Q(x_{t}^{0},a)\left(\varepsilon^{i}\, \partial_{i}\,x_{t}^{0}\right)+\mathcal{O}(\delta^{3}),\]

_as \(\delta\to 0\), where stochastic processes \(\partial_{i}\,x_{t}^{0}\), \(\partial_{ij}^{2}\,x_{t}^{0}\) are the first and second derivatives of \(x_{t}^{0}\) w.r.t \(\varepsilon\) and are bounded as follows:_

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{i}\,x_{t}^{0}\right\|\leq C\left( \mathcal{J}_{0}+\mathcal{J}_{1}\right),\,\mathbb{E}\sup_{t\in[0,T]}\left\| \partial_{ij}^{2}\,x_{t}^{0}\right\|\leq C\exp\left(\,\mathcal{H}_{0}\left( \mathcal{J}_{0}+\mathcal{J}_{1}\right)\right)+C\exp\left(\,\mathcal{H}_{1} \left(\mathcal{J}_{0}+\mathcal{J}_{1}\right)\right).\]

This corollary reveals that latent representation errors implicitly encourage exploration of unseen states by inducing a stochastic perturbation in the value function, which again can be regularized through a controlled Jacobian norm.

**Jacobian Regularization against Non-Zero Drift.** The above theoretical results have established a close connection of input-output Jacobian matrices with the stabilized generalization capacity of world models (shown in 18 under non-zero drift form), and perturbation magnitude in predictive rollouts (indicated in the presence of Jacobian terms in Theorem 4.1 and Corollary 4.2.) Based on this, we propose a regularizer on input-output Jacobian norm \(\|\,\frac{\partial x_{t}}{\partial x}\|_{F}\) that could modulate \(\tilde{\xi}\) ( and in addition \(\xi_{k}\)) for stabilized implicit regularization.

The regularized loss function for LDM is defined as follows:

\[\bar{\mathcal{L}}_{\text{dyn}}=\mathcal{L}_{\text{dyn}}+\lambda\,\|J_{\theta }\|_{F},\] (20)

where \(\mathcal{L}_{\text{dyn}}\) is the original loss function for dynamics model, \(J_{\theta}\) denotes the data-dependent Jacobian matrix associated with the \(\theta\)-parameterized dynamics model, and \(\lambda\) is the regularization weight. Our empirical results in 5 with an emphasis on sequential case align with the experimental findings from [18] that Jacobian regularization can enhance robustness against random and adversarial input perturbation in machine learning models.

## 5 Experimental Studies

In this section, experiments are carried out over a number of tasks in Mujoco environments. Due to space limitation, implementation details and additional results, including the standard deviation of the trials, are relegated to Section D in the Appendix.

**Enhanced generalization to unseen noisy states.** We investigated the effectiveness of Jacobian regularization in model trained against a vanilla model during the inference phase with perturbed state images. We consider three types of perturbations: (1) Gaussian noise across the full image, denoted as \(\mathcal{N}(\mu_{1},\sigma_{1}^{2})\) ; (2) rotation; and (3) noise applied to a percentage of the image, \(\mathcal{N}(\mu_{2},\sigma_{2}^{2})\). (In Walker task, \(\mu_{1}=\mu_{2}=0.5,\sigma_{2}^{2}=0.15\); in Quadruped task, \(\mu_{1}=0,\mu_{2}=0.05,\sigma_{2}^{2}=0.2\).) In each case of perturbations, we examine a collection of noise levels: (1) variance \(\sigma^{2}\) from \(0.05\) to \(0.55\); (2) rotation degree \(\alpha\)\(20\) and \(30\); and (3) masked image percentage \(\beta\%\) from \(25\) to \(75\).

Figure 1: Generalization against increasing degree of perturbation.

It can be seen from Table 3 and Figure 1 that thanks to the adoption of Jacobian regularization in training, the rewards (averaged over 5 trials) are higher compared to the baseline, indicating improved generalization to unseen image states in all cases. The experimental results corroborate the findings in Corollary 3.8 that the regularized Jacobian norm could stabilize the induced implicit regularization.

**Robustness against encoder errors.** Next, we focus on the effects of Jacobian regularization on controlling the error process to the latent states \(z\) during training. Since it is very challenging, if not impossible, to characterize the latent representation errors and hence the drift therein explicitly, we consider to evaluate the robustness against two exogenous error signals, namely (1) zero-drift error with \(\mu_{t}=0,\sigma_{t}^{2}\) (\(\sigma_{t}^{2}=5\) in Walker, \(\sigma_{t}^{2}=0.1\) in Quadruped), and (2) non-zero-drift error with \(\mu_{t}\sim[0,5],\sigma_{t}^{2}\sim[0,5]\) uniformly. Table 3 shows that the model with regularization can consistently learn policies with high returns and also converges faster, compared to the vanilla case. This corroborates our theoretical findings in Corollary 3.8 that the impacts of error to loss \(\mathcal{L}\) can be controlled through the model's Jacobian norm.

**Faster convergence on tasks with extended horizon.** We further evaluate the efficacy of Jacobian regularization in tasks with extended horizon, particularly by extending the horizon length in MuJoCo Walker from \(50\) to \(100\) steps. Table 4 shows that the model with regularization converges significantly faster (\(\sim\) 100K steps) than the case without Jacobian regularization in training. This corroborates results in Theorem 4.1 that regularizing the Jacobian norm can reduce error propagation.

## 6 Conclusion

In this study, we investigate the impacts of latent representation errors on the generalization capacity of world models. We utilize a stochastic differential equation formulation to characterize the effects of latent representation errors as implicit regularization, for both cases with zero-drift errors and with non-zero drift errors. We develop a Jacobian regularization scheme to address the compounding effects of non-zero drift, thereby enhancing training stability and generalization. Our empirical findings validate that Jacobian regularization improves the generalization performance, expanding the applicability of world models in complex, real-world scenarios. Future research is needed to investigate how stabilizing latent errors can enhance generalization across more sophisticated tasks for general non-zero drift cases.

The broader social impact of our work resides in its potential to enhance the robustness and reliability of RL agents deployed in real-world applications. By improving the generalization capacities of world models, our work could contribute to the development of RL agents that perform consistently across diverse and unseen environments. This is particularly relevant in safety-critical domains such as autonomous driving, where reliable agents can provide intelligent and trustworthy decision-making.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline  & Walker 100 len (increased from original 50 len) \\ \hline Num steps & 100k & 200k & 280k \\ \hline With Jacobian (\(\lambda=0.05\)) & **639.1** & **936.3** & 911.1 \\ With Jacobian (\(\lambda=0.1\)) & 537.5 & 762.6 & **927.7** \\ Baseline & 582.3 & 571.2 & 886.6 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Accumulated rewards of Walker with extended horizon.

\begin{table}
\begin{tabular}{c|c|c c|c c|c c} \hline \hline  & & \multicolumn{2}{c|}{full, \(\mathcal{N}(\mu_{1},\,\sigma_{1}^{2})\)} & \multicolumn{2}{c|}{rotation, \(+\alpha^{\circ}\)} & \multicolumn{2}{c}{mask \(\beta\%,\mathcal{N}(\mu_{2},\,\sigma_{2}^{2})\)} \\ \hline  & clean & \(\sigma_{t}^{2}=0.35\) & \(\sigma_{t}^{2}=0.5\) & \(\alpha=20\) & \(\alpha=30\) & \(\beta=50\) & \(\beta=75\) \\ \hline With Jacobian (Walker) & **967.12** & **742.32** & **618.98** & **423.81** & **226.04** & **725.81** & **685.49** \\ Baseline (Walker) & 966.53 & 615.79 & 333.47 & 391.65 & 197.53 & 583.41 & 446.74 \\ With Jacobian (Quad) & **971.98** & **269.78** & **242.15** & **787.63** & **610.53** & **321.55** & **304.92** \\ Baseline (Quad) & 967.91 & 207.33 & 194.08 & 681.03 & 389.41 & 222.22 & 169.58 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluation on unseen states by various perturbation (Clean means without perturbation). \(\lambda=0.01\).

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c} \hline \hline  & & \multicolumn{2}{c|}{Zero drift, Walker} & \multicolumn{2}{c|}{Non-zero drift, Walker} & \multicolumn{2}{c}{Zero drift, Quad} & \multicolumn{2}{c}{Non-zero drift, Quad} \\ \hline  & 300k & 600k & 300k & 600k & 600k & 1.2M & 1M & 2M \\ \hline With Jacobian & **666.2** & **966** & **905.7** & **912.4** & **439.8** & **889** & **348.3** & **958.7** \\ Baseline & 24.5 & 43.1 & 404.6 & 495 & 293.6 & 475.9 & 48.98 & 32.87 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Accumulated rewards under additional encoder errors. \(\lambda=0.01\).

## References

* [1] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. _Journal of Artificial Intelligence Research_, 47:253-279, 2013.
* [2] Alexander Camuto, Matthew Willetts, Umut Simsekli, Stephen Roberts, and Chris Holmes. Explicit regularisation in gaussian noise injections, 2021.
* [3] Henri Cartan. _Differential calculus on normed spaces_. Createspace Independent Publishing Platform, North Charleston, SC, August 2017.
* [4] Bo Chang, Minmin Chen, Eldad Haber, and Ed H. Chi. Antisymmetricrnn: A dynamical system view on recurrent neural networks, 2019.
* [5] Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and Leon Bottou. Symplectic recurrent neural networks, 2020.
* [6] Bernard Dacorogna and Jurgen Moser. On a partial differential equation involving the jacobian determinant. _Annales de l'I.H.P. Analyse non lineaire_, 7(1):1-26, 1990.
* [7] Carl Doersch. Tutorial on variational autoencoders. _arXiv preprint arXiv:1606.05908_, 2016.
* [8] Sean C Duncan. Minecraft, beyond construction and survival. 2011.
* [9] Lawrence Craig Evans and Ronald F Gariepy. _Measure theory and fine properties of functions, revised edition_. Textbooks in Mathematics. Apple Academic Press, Oakville, MO, April 2015.
* [10] C. Daniel Freeman, Luke Metz, and David Ha. Learning to predict without looking ahead: World models without forward prediction. _Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019)_, 2019.
* [11] Alison L. Gibbs and Francis Edward Su. On choosing and bounding probability metrics. _International Statistical Review / Revue Internationale de Statistique_, 70(3):419-435, 2002.
* [12] David Ha and Jurgen Schmidhuber. World models. _arXiv preprint arXiv:1803.10122_, 2018.
* [13] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination, 2020.
* [14] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In _International conference on machine learning_, pages 2555-2565. PMLR, 2019.
* [15] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models, 2022.
* [16] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.
* [17] Paul Louis Hennequin, R. M. Dudley, H. Kunita, and F. Ledrappier. _Ecole d'ete de Probabilites de Saint-Flour XII-1982_. Springer-Verlag, 1984.
* [18] Judy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust learning with jacobian regularization, 2019.
* [19] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. _arXiv preprint arXiv:submit/1234567_, Sep 2023. Submitted on 29 Sep 2023.
* [20] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima, 2017.

* [21] Samuel Kessler, Mateusz Ostaszewski, Michal Bortkiewicz, Mateusz Zarski, Maciej Wolczyk, Jack Parker-Holder, Stephen J. Roberts, and Piotr Milos. The effectiveness of world models for continual reinforcement learning. _CoLLAs 2023_, 2023.
* [22] Kuno Kim, Megumi Sano, Julian De Freitas, Nick Haber, and Daniel Yamins. Active world model learning with progress curiosity. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, 2020.
* [23] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [24] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. _Neural computation_, 1(4):541-551, 1989.
* [25] John M. Lee. _Introduction to Riemannian Manifolds_. Springer International Publishing, 2018.
* [26] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and optimization theory for linear continuous-time recurrent neural networks. _Journal of Machine Learning Research_, 23(42):1-85, 2022.
* [27] Soon Hoe Lim, N Benjamin Erichson, Liam Hodgkinson, and Michael W Mahoney. Noisy recurrent neural networks. _Advances in Neural Information Processing Systems_, 34:5124-5137, 2021.
* [28] Lynn Harold Loomis and Shlomo Sternberg. _Advanced calculus (revised edition)_. World Scientific Publishing, Singapore, Singapore, March 2014.
* [29] Guohao Shen, Yuling Jiao, Yuanyuan Lin, and Jian Huang. Approximation with cnns in sobolev space: with applications to classification. In _NeurIPS_, Oct 2022.
* [30] J. Michael Steele. _Stochastic calculus and Financial Applications_. Springer, 2001.
* [31] Colin Wei, Sham Kakade, and Tengyu Ma. The implicit and explicit regularization effects of dropout, 2020.
* [32] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Day-dreamer: World models for physical robot learning. In _Proceedings of The 6th Conference on Robot Learning_, volume 205 of _PMLR_, pages 2226-2240, 2023.
* [33] Yong Yu, Xiaosheng Si, Changhua Hu, and Jianxun Zhang. A review of recurrent neural networks: Lstm cells and network architectures. _Neural computation_, 31(7):1235-1270, 2019.

**Supplementary Materials**

In this appendix, we provide the supplementary materials supporting the findings of the main paper on the latent representation of latent representations in world models. The organization is as follows:

* In Section A, we provide proof on showing the approximation capacity of CNN encoder-decoder architecture in latent representation of world models.
* In Section B, we provide proof on implicit regularization of zero-drift errors and additional effects of non-zero-drift errors by showing a proposition on the general form.
* In Section C, we provide proof on showing the effects of non-zero-drift errors during predictive rollouts by again showing a result on the general form.
* In Section D, we provide additional results and implementation details on our empirical studies.

Approximation Power of Latent Representation with CNN Encoder and Decoder

To mathematically describe this _intrinsic lower-dimensional geometric structure_, for an integer \(k>0\) and \(\alpha\in(0,1]\), we consider the notion of smooth manifold (in the \(\mathcal{C}^{k,\alpha}\) sense), formally defined by

**Definition A.1** (\(\mathcal{C}^{k,\alpha}\) manifold).: A \(\mathcal{C}^{k,\alpha}\) manifold \(\mathcal{M}\) of dimension \(n\) is a topological manifold (i.e. a topological space that is locally Euclidean, with countable basis, and Hausdorff) that has a \(\mathcal{C}^{k,\alpha}\) structure \(\Xi\) that is a collection of coordinate charts \(\{U_{\alpha},\psi_{\alpha}\}_{\alpha\in A}\) where \(U_{\alpha}\) is an open subset of \(\mathcal{M}\), \(\psi_{\alpha}:U_{\alpha}\to V_{\alpha}\subseteq\mathbb{R}^{n}\) such that

* \(\bigcup_{\alpha\in A}U_{\alpha}\supseteq\mathcal{M}\), meaning that the the open subsets form an open cover,
* Each chart \(\psi_{\alpha}\) is a diffeomorphism that is a smooth map with smooth inverse (in the \(\mathcal{C}^{k,\alpha}\) sense),
* Any two charts are \(\mathcal{C}^{k,\alpha}\)-compatible with each other, that is for all \(\alpha_{1},\alpha_{2}\in A\), \(\psi_{\alpha_{1}}\circ\psi_{\alpha_{2}}^{-1}:\psi_{\alpha_{2}}(U_{\alpha_{1}} \cap U_{\alpha_{2}})\to\psi_{\alpha_{1}}(U_{\alpha_{1}}\cap U_{\alpha_{2}})\) is \(\mathcal{C}^{k,\alpha}\).

Intuitively, a \(\mathcal{C}^{k,\alpha}\) manifold is a generalization of Euclidean space by allowing additional spaces with nontrivial global structures through a collection of charts that are diffeomorphisms mapping open subsets from the manifold to open subsets of euclidean space. For technical utility, the defined charts allow to transfer most familiar real analysis tools to the manifold space. For more references, see [25].

**Definition A.2** (Riemannian volume form).: Let \(\mathcal{X}\) be a smooth, oriented \(d\)-dimensional manifold with Riemannian metric \(g\). A volume form \(d\text{vol}_{\mathcal{M}}\) is the canonical volume form on \(\mathcal{X}\) if for any point \(x\in\mathcal{X}\), for a chosen local coordinate chart \((x_{1},...,x_{d})\), \(d\text{vol}_{\mathcal{M}}=\sqrt{\det g_{ij}}\,dx_{1}\,\wedge\,...\,\wedge\,dx_ {d}\), where \(g_{ij}(x):=g\,(\frac{\partial}{\partial x_{i}},\frac{\partial}{\partial x_{j}} )(x)\).

Then the induced volume measure by the canonical volume form \(d\text{vol}_{\mathcal{X}}\) is denoted as \(\mu_{\mathcal{X}}\), defined by \(\mu_{\mathcal{X}}:\,A\mapsto\int_{A}d\text{vol}_{\mathcal{X}}\), for any Borel-measurable subset \(A\) on the space \(\mathcal{X}\). For more references, see [9].

We recall the latent representation problem defined in the main paper.

Consider the state space \(\mathcal{S}\subset\mathbb{R}^{ds}\) and the latent space \(\mathcal{Z}\). Consider a state probability measure \(Q\) on the state space \(\mathcal{S}\) and a probability measure \(P\) on the latent space \(\mathcal{Z}\).

**Assumption A.3**.: (Latent manifold assumption) For a positive integer \(k\), there exists a \(d_{\mathcal{M}}\)-dimensional \(\mathcal{C}^{k,\alpha}\) submanifold \(\mathcal{M}\) (with \(\mathcal{C}^{k+3,\alpha}\) boundary) with Riemannian metric \(g\) and has positive reach and also isometrically embedded in the state space \(\mathcal{S}\subset\mathbb{R}^{ds}\) and \(d_{\mathcal{M}}\ll d_{\mathcal{S}}\), where the state probability measure is supported on. In addition, \(\mathcal{M}\) is a compact, orientable, connected manifold.

**Assumption A.4**.: (Smoothness of state probability measure) \(Q\) is a probability measure supported on \(\mathcal{M}\) with its Radon-Nikodym derivative \(q\in\mathcal{C}^{k,\alpha}(\mathcal{M},\mathbb{R})\) w.r.t \(\mu_{\mathcal{M}}\).

Let \(\mathcal{Z}\) be a closed ball in \(\mathbb{R}^{d_{\mathcal{M}}}\), that is \(\{x\in\mathbb{R}^{d_{\mathcal{M}}}\,:\,\|x\|\leq 1\,\}\). \(P\) is a probability measure supported on \(\mathcal{Z}\) with its Radon-Nikodym derivative \(p\in\mathcal{C}^{k,\alpha}(\mathcal{Z},\mathbb{R})\) w.r.t \(\mu_{\mathcal{Z}}\).

We consider a general CNN function \(f_{\text{CNN}}:\mathcal{X}\to\mathbb{R}\). Let \(f_{\text{CNN}}\) have \(L\) hidden layers, represented as:

\[f_{\text{CNN}}(x)=A_{L+1}\circ A_{L}\circ\cdots\circ A_{2}\circ A_{1}(x),\quad x \in\mathcal{X},\]

where \(A_{i}\)'s are either convolutional or downsampling operators. For convolutional layers,

\[A_{i}(x)=\sigma(W_{i}^{c}x+b_{i}^{c}),\]

where \(W_{i}^{c}\in\mathbb{R}^{d_{i}\times d_{i-1}}\) is a structured sparse Toeplitz matrix from the convolutional filter \(\{w_{j}^{(i)}\}_{j=0}^{s(i)}\) with filter length \(s(i)\in\mathbb{N}_{+}\), \(b_{i}^{c}\in\mathbb{R}^{d_{i}}\) is a bias vector, and \(\sigma\) is the ReLU activation function.

For downsampling layers,

\[A_{i}(x)=D_{i}(x)=(x_{jm_{i}})_{j=1}^{\lfloor d_{i-1}/m_{i}\rfloor}\,,\]where \(D_{i}:\mathbb{R}^{d_{i}\times d_{i-1}}\) is the downsampling operator with scaling parameter \(m_{i}\leq d_{i-1}\) in the \(i\)-th layer. The convolutional and downsampling operations are elaborated in Appendix [63]. We examine the class of functions represented by CNNs, denoted by \(\mathcal{F}_{\text{CNN}}\), defined as:

\[\mathcal{F}_{\text{CNN}}=\{f_{\text{CNN}}\text{ as in defined above with any choice of }A_{i},\,i=1,\dots,L+1\}.\]

For more details in the definitions of CNN functions, we refer to [29].

**Assumption A.5**.: Assume that \(\mathcal{M}\) and \(\mathcal{Z}\) are locally diffeomorphic, that is there exists a map \(F:\mathcal{M}\to\mathcal{Z}\) such that at every point \(x\) on \(\mathcal{M},\ \det(d\,F(x))\neq 0\).

**Theorem A.6**.: _(Approximation Error of Latent Representation). Under Assumption A.3, A.4 and A.5, for \(\theta\in(0,1),\) let \(d_{\theta}=\mathcal{O}(d_{\mathcal{M}}\theta^{-2}\log\frac{d}{\theta})\). For positive integers \(M\) and \(N\), there exists an encoder \(g_{\text{enc}}\) and decoder \(g_{\text{dec}}\in\mathcal{F}_{\text{CNN}}(L,S,W)\) s.t._

\[W_{1}(g_{\text{enc}_{\#}}Q,P) \leq d_{\mathcal{M}}C(NM)^{-\frac{2(k+1)}{d_{\theta}}},\] \[W_{1}(g_{\text{dec}_{\#}}P,Q) \leq d_{\mathcal{M}}C(NM)^{-\frac{2(k+1)}{d_{\theta}}}.\]

The primary challenge to show Theorem A.6 is in demonstrating the existence of oracle encoder and decoder maps. These maps, denoted as \(g_{\text{enc}}^{*}:\mathcal{M}\to\mathcal{Z}\) and \(g_{\text{dec}}^{*}:\mathcal{Z}\to\mathcal{M}\) respectively, must satisfy

\[g_{\text{enc}\,\#}^{*}\,Q=P,\quad g_{\text{dec}\,\#}^{*}\,P=Q.\] (21)

and importantly they have the proper smoothness guarantee, namely \(g_{\text{enc}}^{*}\in\mathcal{C}^{k+1,\alpha}(\mathcal{M},\mathcal{Z})\) and \(g_{\text{dec}}^{*}\in\mathcal{C}^{k+1,\alpha}(\mathcal{Z},\mathcal{M})\). Proposition A.7 shows the existence of such oracle map(s).

**Proposition A.7** (\(\mathcal{C}^{k,\alpha}\), compact).: _Let \(\mathcal{M},\mathcal{N}\) be compact, oriented d-dimensional Riemannian manifolds with \(\mathcal{C}^{k+3,\alpha}\) boundary with the volume measure \(\mu_{\mathcal{M}}\) and \(\mu_{\mathcal{N}}\) respectively. Let \(Q,\,P\) be distributions supported on \(\mathcal{M},\,\mathcal{N}\) respectively with their \(\mathcal{C}^{k,\alpha}\) density functions \(q,\,p\), that is \(Q,\,P\) are probability measures supported on \(\mathcal{M},\,\mathcal{N}\) with their Radon-Nikodym derivatives \(q\in\mathcal{C}^{k,\alpha}(\mathcal{M},\mathbb{R})\) w.r.t \(\mu_{\mathcal{M}}\) and \(p\in\mathcal{C}^{k,\alpha}(\mathcal{N},\mathbb{R})\) w.r.t \(\mu_{\mathcal{N}}\). Then, there exists a \(\mathcal{C}^{k+1,\alpha}\) map \(g:\mathcal{N}\to\mathcal{M}\) such that the pushforward measure \(g_{\#}P=Q\), that is for any measurable subset \(A\in\mathcal{B}(\mathcal{M}),\ Q(A)=P(g^{-1}(A))\)._

Proof.: (_Proposition A.7_) Let \(\omega:=p\,d\text{vol}_{\mathcal{N}}\), then \(\omega\) is a \(\mathcal{C}^{k,\alpha}\) volume form on \(\mathcal{N}\), as \(p\in\mathcal{C}^{k,\alpha}\) and for any point \(x\in\mathcal{N}\), we have \(p(x)>0\). In addition, \(\int_{\mathcal{N}}\omega=\int_{\mathcal{N}}p\,d\text{vol}_{\mathcal{N}}=\int_{ \mathcal{N}}p\,d\mu_{\mathcal{N}}=P(\mathcal{N})=1\). Similarly, let \(\eta:=q\,d\text{vol}_{\mathcal{M}}\) a \(\mathcal{C}^{k,\alpha}\) volume form on \(\mathcal{M}\) and \(\int_{\mathcal{M}}\eta=1\).

Let \(F:\mathcal{N}\to\mathcal{M}\) be an orientation-preserving local diffeomorphism, we then have \(\det(dF)>0\) everywhere on \(\mathcal{N}\).

As \(\mathcal{N}\) is compact and \(\mathcal{M}\) is connected by assumption, \(F\) is a covering map, that is for every point \(x\in\mathcal{M}\), there exists an open neighborhood \(U_{x}\) of \(x\) and a discrete set \(D_{x}\) such that \(F^{-1}(U)=\sqcup_{\alpha\in D}V_{\alpha}\subset\mathcal{N}\) and \(F|_{V_{\alpha}}=V_{\alpha}\to U\) is a diffeomorphism. Furthermore, \(|D_{x}|=|D_{y}|\) for any points \(x,y\in\mathcal{M}\). In addition, \(|D_{x}|\) is finite from the compactness of \(\mathcal{N}\).

Figure 2: _Latent Representation Problem_: The left and right denote the manifold \(\mathcal{M}\) with lower dim \(d_{\mathcal{M}}\) embedded in a larger Euclidean space, with latent space \(Z\) a \(d_{\mathcal{M}}\)-dimensional ball in middle. Encoder and decoder as maps respectively pushing forward Q to P and P to Q.

Let \(\bar{\eta}\) be the pushforward of \(\omega\) via \(F\), defined by for any point \(x\in\mathcal{M}\) and a neighborhood \(U_{x}\),

\[\bar{\eta}(x):=\frac{1}{|D_{x}|}\sum_{\alpha\in D_{x}}\left(F\big{|}_{V_{\alpha }}^{-1}\right)^{*}\omega\big{|}_{V_{\alpha}}.\] (22)

\(\bar{\eta}\) is well-defined as it is not dependent on the choice of neighborhoods and the sum and \(\frac{1}{|D_{x}|}\) are always finite. Furthermore, \(\bar{\eta}\) is a \(\mathcal{C}^{k,\alpha}\) volume form on \(\mathcal{M}\), as \(p\circ\left(F\big{|}_{V_{\alpha}}^{-1}\right)\) is \(\mathcal{C}^{k,\alpha}\).

Notice that \(F\big{|}_{V_{\alpha}}^{-1}\) is orientation-preserving as \(\det d\,F\big{|}_{V_{\alpha}}^{-1}=\frac{1}{\det d\,F\big{|}_{V_{\alpha}}}>0\) everywhere on \(V_{\alpha}\). In addition, \(F\big{|}_{V_{\alpha}}^{-1}\) is proper: as for any compact subset \(K\) of \(\mathcal{N}\), \(K\) is closed; and as \(F\big{|}_{V_{\alpha}}^{-1}\) is continuous, the preimage of \(K\) via \(F\big{|}_{V_{\alpha}}^{-1}\) a closed subset of \(\mathcal{M}\) which is compact, then the preimage of \(K\) must also be compact. Hence, \(F\big{|}_{V_{\alpha}}^{-1}\) is proper. As every \(F\big{|}_{V_{\alpha}}^{-1}\) is proper, orientation-preserving and surjective, then \(c:=\deg(F\big{|}_{V_{\alpha}}^{-1})=1\).

Then, \(\int_{\mathcal{M}}\bar{\eta}=c\int_{\mathcal{N}}\omega=1\).

As we have shown that \(\eta\) and \(\bar{\eta}\in\mathcal{C}^{k,\alpha}\) and \(\int_{\mathcal{M}}\bar{\eta}=\int_{\mathcal{M}}\eta\), by [6], there exists a diffeomorphism \(\psi:\mathcal{M}\to\mathcal{M}\) fixing on the boundary such that \(\psi^{*}\eta=\bar{\eta}\), where \(\psi,\psi^{-1}\in\mathcal{C}^{k+1,\alpha}\).

Let \(g:=\psi\circ F\), then it holds that \(g^{*}\eta=(\psi\circ F)^{*}\eta=F^{*}\circ\psi^{*}\eta=F^{*}\bar{\eta}=\omega\).

Then, for any measurable subset \(A\) on the manifold \(\mathcal{M}\), we verify that \(Q(A)=\int_{A}\eta=\int_{g^{-1}(A)}g^{*}\eta=\int_{g^{-1}(A)}\omega=\int_{g^{-1 }(A)}p\,d\text{vol}_{\mathcal{N}}=\int_{g^{-1}(A)}p\,d\mu_{\mathcal{N}}=P(g^{- 1}(A))\).

Hence, we have shown the existence by an explicit construction. As \(\psi\in\mathcal{C}^{k+1,\alpha}\), and \(F\in\mathcal{C}^{\infty}\), then we have \(g\in\mathcal{C}^{k+1,\alpha}\). 

We are now ready to show Theorem A.6 with the existence of oracle map and the low-dimensional approximation results from [29].

Proof.: (_Theorem A.6_) For encoder, from Proposition A.7, there exists an \(\mathcal{C}^{k+1,\alpha}\) oracle map \(g:\mathcal{M}\to\mathcal{Z}\) such that the pushforward measure \(g_{\#}Q=P\). Then,

\[W_{1}((g_{\text{enc}})_{\#}Q\,,\,P)= W_{1}((g_{\text{enc}})_{\#}Q\,,\,g_{\#}Q)\] \[= \sup_{f\in\text{Lip}_{1}(\mathcal{Z})}\left|\int_{\mathcal{Z}}f(y )\,d((g_{\text{enc}})_{\#}Q)-\int_{\mathcal{Z}}f(y)\,d(g_{\#}Q)\right|\] \[\leq \sup_{f\in\text{Lip}_{1}(\mathcal{Z})}\int_{\mathcal{M}}|f\circ g _{\text{enc}}(x)-f\circ g(x)|\,\,dQ\] \[\leq \int_{\mathcal{M}}\|g_{\text{enc}}(x)-g(x)\|\,\,dQ\] \[\leq \,d_{\mathcal{M}}C(NM)^{-\frac{2(k+1)}{d_{\theta}}},\]

where the last inequality follows from the special case \(\rho=0\) of Theorem 2.4 in [29].

Similarly, for decoder, from Proposition A.7, there exists an \(\mathcal{C}^{k+1,\alpha}\) oracle map \(\bar{g}:\mathcal{Z}\to\mathcal{M}\) such that the pushforward measure \(\bar{g}_{\#}P=Q\).

\[W_{1}((g_{\text{dec}})_{\#}P\,,\,Q)= W_{1}((g_{\text{dec}})_{\#}P\,,\,\bar{g}_{\#}P)\] \[\leq \,\int_{\mathcal{Z}}\|g_{\text{dec}}(y)-\bar{g}(y)\|\,\,dP\] \[\leq \,d_{\mathcal{M}}C(NM)^{-\frac{2(k+1)}{d_{\theta}}}.\]Explicit Regularization of Latent Representation Error in World Model Learning

We recall the SDEs for latent dynamics model defined in the main paper. Consider a complete, filtered probability space \((\Omega,\,\mathcal{F},\,\{\mathcal{F}_{t}\}_{t\in[0,T]},\,\mathbb{P}\,)\) where independent standard Brownian motions \(B_{t}^{\text{enc}},\,B_{t}^{\text{pred}},B_{t}^{\text{seq}},\,B_{t}^{\text{dec}}\) are defined such that \(\mathcal{F}_{t}\) is their augmented filtration, and \(T\in\mathbb{R}\) as the time length of the task environment. We consider the stochastic dynamics of LDM through the following coupled SDEs after error perturbation:

\[d\,z_{t} =\left(q_{\text{enc}}(h_{t},s_{t})+\sigma(h_{t},s_{t})\right)\,dt +\left(\bar{q}_{\text{enc}}(h_{t},s_{t})+\bar{\sigma}(h_{t},s_{t})\right)dB_{t} ^{\text{enc}},\] (23) \[d\,h_{t} =f(h_{t},z_{t},\pi(h_{t},z_{t}))\,dt+\bar{f}(h_{t},z_{t},\pi(h_{t },z_{t}))\,dB_{t}^{\text{seq}}\] (24) \[d\,\tilde{z}_{t} =p(h_{t})\,dt+\bar{p}(h_{t})\,dB_{t}^{\text{pred}},\] (25) \[d\,\tilde{s}_{t} =q_{\text{dec}}(h_{t},\tilde{z}_{t})\,dt+\bar{q}_{\text{dec}}(h_{ t},\tilde{z}_{t})\,dB_{t}^{\text{dec}},\] (26)

where \(\pi(h,\tilde{z})\) is a policy function as a local maximizer of value function and the stochastic process \(s_{t}\) is \(\mathcal{F}_{t}\)-adapted.

As discussed in the main paper, our analysis applies to a common class of world models that uses Gaussian distributions parameterized by neural networks' outputs for \(z\), \(\tilde{z}\), \(\tilde{s}\). Their distributions are not non-Gaussian in general.

For example, as \(z\) is conditional Gaussian and its mean and variance are random variables which are learned by the encoder from r.v.s \(s\) and \(h\) as inputs, thus rendering \(z\) non-Gaussian. However, \(z\) is indeed Gaussian when the inputs are known. Under this conditional Gaussian class of world models, to see that the continuous formulation of latent dynamics model can be interrupted as SDEs, one notices that SDEs with coefficient functions of known inputs are indeed Gaussian, matching to this class of world models. Formally, in the context of \(z\) without latent representation error:

**Proposition B.1**.: _(Latent states SDE with known inputs is Gaussian) For the latent state process \(z_{t\in[0,T]}\) without error,_

\[d\,z_{t}=q_{\text{enc}}(h_{t},s_{t})\,dt+\bar{q}_{\text{enc}}(h_{t},s_{t}))dB _{t}^{\text{enc}},\] (27)

_with zero initial value. Given known \(h_{t\in[0,T]}\) and \(s_{t\in[0,T]}\), the process \(z_{t}\) is a Gaussian process. Furthermore, for any \(t\in[0,T]\), \(z_{t}\) follows a Gaussian distribution with mean \(\mu_{t}=\int_{0}^{t}q_{\text{enc}}(h_{s},s_{s})ds\) and variance \(\sigma_{t}^{2}=\int_{0}^{t}\bar{q}_{\text{enc}}(h_{s},s_{s})^{2}ds\)._

Proof.: Proof follows from Proposition 7.6 in [30]. 

Next, we recall our assumptions from the main text:

**Assumption B.2**.: The drift coefficient functions \(q_{\text{enc}},\,f,\,p\) and \(q_{\text{dec}}\) and the diffusion coefficient functions \(\bar{q}_{\text{enc}},\,\bar{p}\) and \(\bar{q}_{\text{dec}}\) are bounded and Borel-measurable over the interval \([0,T]\), and of class \(\mathcal{C}^{3}\) with bounded Lipschitz continuous partial derivatives. The initial values \(z_{0},h_{0},\tilde{z}_{0},\tilde{s}_{0}\) are square-integrable random variables.

**Assumption B.3**.: \(\sigma\) and \(\bar{\sigma}\) are bounded and Borel-measurable and are of class \(\mathcal{C}^{3}\) with bounded Lipschitz continuous partial derivatives over the interval \([0,T]\).

One of our main results is the following:

**Theorem B.4**.: _(Explicit Regularization Induced by Zero-Drift Representation Error) Under Assumption B.2 and B.3 and considering a loss function \(\mathcal{L}\in\mathcal{C}^{2}\), the explicit effects of the zero-drift error can be marginalized out as follows:_

\[\mathbb{E}\,\mathcal{L}\left(x_{t}^{\varepsilon}\right)=\mathbb{E}\,\mathcal{L} (x_{t}^{0})+\mathcal{R}+\mathcal{O}(\varepsilon^{3}),\] (28)_as \(\varepsilon\to 0\), where the regularization term \(\mathcal{R}\) is given by \(\mathcal{R}:=\,\varepsilon\,\mathcal{P}+\varepsilon^{2}\left(\mathcal{Q}+\frac{1 }{2}\,\mathcal{S}\right).\) Each term of \(\mathcal{R}\) is as follows:_

\[\mathcal{P}:= \,\mathbb{E}\,\nabla\mathcal{L}(x_{t}^{0})^{\top}\Phi_{t}\sum_{k} \xi_{t}^{k},\] (29) \[\mathcal{Q}:= \,\mathbb{E}\,\nabla\mathcal{L}(x_{t}^{0})^{\top}\Phi_{t}\int_{0 }^{t}\Phi_{s}^{-1}\,\mathcal{H}^{k}(x_{s}^{0},s)dB_{t}^{k},\] (30) \[\mathcal{S}:= \,\mathbb{E}\sum_{k_{1},k_{2}}(\Phi_{t}\xi_{t}^{k_{1}})^{i}\nabla ^{2}\mathcal{L}(x_{t}^{0},t)\,(\Phi_{t}\xi_{t}^{k_{2}})^{j},\] (31)

_where square matrix \(\Phi_{t}\) is the stochastic fundamental matrix of the corresponding homogeneous equation:_

\[d\Phi_{t}=\frac{\partial\bar{g}_{k}}{\partial x}(x_{t}^{0},t)\,\Phi_{t}\,dB_{ t}^{k},\quad\Phi(0)=I,\]

_and \(\xi_{t}^{k}\) is as the shorthand for \(\int_{t}^{t}\Phi_{s}^{-1}\bar{\sigma}_{k}(x_{s}^{0},s)dB_{t}^{k}\). Additionally, \(\mathcal{H}^{k}(x_{s}^{0},s)\) is represented by for \(\sum_{k_{1},k_{2}}\frac{\partial^{2}\bar{g}_{k}}{\partial x^{i}\partial x^{j} }(x_{s}^{0},s)\left(\xi_{s}^{k_{1}}\right)^{i}\left(\xi_{s}^{k_{2}}\right)^{j}\)._

Before proving Theorem B.4, we first show Proposition B.5 on the general case of perturbation to the stochastic system. Consider the following perturbed system given by

\[d\,x_{t}=\left(g_{0}\left(x_{t},t\right)+\varepsilon\,\eta_{0}\left(x_{t},t \right)\right)dt+\sum_{k=1}^{m}\left(g_{k}\left(x_{t},t\right)+\varepsilon\, \eta_{k}\left(x_{t},t\right)\right)dB_{t}^{k}\] (32)

with initial values \(x(0)=x_{0}\),

**Proposition B.5**.: _Suppose that \(f\) is a real-valued function that is \(\mathcal{C}^{2}\). Then it holds that, with probability 1, as \(\varepsilon\to 0\), for \(t\in[0,T]\),_

\[f\left(x_{t}^{\varepsilon}\right)=f\left(x_{t}^{0}\right)+\varepsilon\nabla f \left(x_{t}^{0}\right)^{\top}\partial_{\varepsilon}\,x_{t}^{0}+\varepsilon^ {2}\left(\nabla f\left(x_{t}^{0}\right)^{\top}\partial_{\varepsilon}^{2}x_{t }^{0}\right.\left.+\frac{1}{2}\partial_{\varepsilon}\,{x_{t}^{0}}^{\top} \nabla^{2}f\left(x_{t}^{0}\right)\partial_{\varepsilon}\,x_{t}^{0}\right)+ \mathcal{O}\left(\varepsilon^{3}\right),\] (33)

_where the stochastic process \(x_{t}^{0}\) is the solution to SDE 32 with \(\varepsilon=0\), with its first and second-order derivatives w.r.t \(\varepsilon\) denoted as \(\partial_{\varepsilon}\,x_{t}^{0},\partial_{\varepsilon}^{2}\,x_{t}^{0}\). Furthermore, it holds that \(\partial_{\varepsilon}\,x_{t}^{0},\partial_{\varepsilon}^{2}\,x_{t}^{0}\) satisfy the following SDEs with probability 1,_

\[d\,\partial_{\varepsilon}x_{t}^{0} =\left(\frac{\partial g_{k}}{\partial x}\left(x_{t}^{0},t\right) \partial_{\varepsilon}x_{t}^{0}+\eta_{k}\left(x_{t}^{0},t\right)\right)dB_{t} ^{k},\] (34) \[d\,\partial_{\varepsilon}^{2}x_{t} =\left(\Psi_{k}\left(\partial_{\varepsilon}x_{t}^{0},x_{t}^{0},t \right)+2\frac{\partial\eta_{k}}{\partial x}\left(x_{t}^{0},t\right)\partial_ {\varepsilon}x_{t}^{0}+\frac{\partial g_{k}}{\partial x}\left(x_{t}^{0},t \right)\partial_{\varepsilon}^{2}x_{t}^{0}\right)dB_{t}^{k},\]

_with initial values \(\partial_{\varepsilon}\,x(0)=0,\partial_{\varepsilon}^{2}\,x(0)=0\), where_

\[\Psi_{k}:(\partial_{\varepsilon}\,x,x,t)\mapsto\partial_{\varepsilon}\,x^{i} \frac{\partial g_{k}}{\partial x^{i}\partial x^{j}}(x,t)\partial_{\varepsilon} \,x^{j},\]

_for \(k=0,1,...,m\)._

Proof.: We first apply the stochastic version of perturbation theory to SDE 32. For brevity, we will write \(t\) as \(B_{t}^{0}\) and use Einstein summation convention. Hence, SDE 32 is rewritten as

\[dx_{t}=\gamma_{k}^{\varepsilon}\left(x_{t},t\right)dB_{t}^{k},\] (35)

with initial value \(x(0)=x_{0}\).

_Step 1_: We begin with the corresponding systems to derive the SDEs that characterize \(\partial_{\varepsilon}\,x_{t}^{\varepsilon}\) and \(\partial_{\varepsilon}^{2}\,x_{t}^{\varepsilon}\). Our main tool is an important result on smoothness of solutions w.r.t. initial data from Theorem 3.1 from Section 2 in [17].

For \(\partial_{\varepsilon}\,x\), consider the SDEs

\[d\,x_{t} =\gamma_{k}^{\varepsilon}\left(x_{t},t\right)dB_{t}^{k},\] (*) \[d\,\varepsilon_{t} =0,\]with initial values \(x_{(0)}=x_{0},\varepsilon(0)=\varepsilon.\) From an application of Theorem 3.1 from Section 2 in [17] on *, we have \(\partial_{\varepsilon}\,x\) that satisfies the following SDE with probability 1:

\[d\,\partial_{\varepsilon}x_{t}=\left(\alpha_{k}^{\varepsilon}\left(x_{t},t \right)\partial_{\varepsilon}x_{t}+\eta_{k}\left(x_{t},t\right)\right)dB_{t}^ {k},\] (36)

with initial value \(\partial_{\varepsilon}x_{0}=0\in\mathbb{R}^{n},\) with probability 1, where \(x_{t}\) is the solution to Equation (35) and the functions \(\alpha_{k}^{\varepsilon}\) are given by

\[\alpha_{k}^{\varepsilon}:\left(x,t\right)\mapsto\frac{\partial g_{k}}{ \partial x^{j}}\left(x,t\right)+\varepsilon\frac{\partial\eta_{k}}{\partial x ^{j}}\left(x,t\right),\]

where \(k=0,\,...,\,m.\)

To characterize \(\partial_{\varepsilon}^{2}\,x_{t},\) consider the following SDEs

\[d\,x_{t}=\gamma_{k}^{\varepsilon}\left(x_{t},t\right)dB_{t}^{k},\] (**) \[d\,\partial_{\varepsilon}\,x_{t}=\left(\alpha_{k}^{\varepsilon }\left(x_{t},t\right)\partial_{\varepsilon}\,x_{t}+\eta_{k}\left(x_{t},t \right)\right)dB_{t}^{k},\] \[d\,\varepsilon_{t}=0,\]

with initial value \(x(0)=x_{0},\,\partial_{\varepsilon}\,x(0)=0,\,\varepsilon(0)=\varepsilon.\)

From a similar application of Theorem 3.1 from Section 2 in [17], the second derivative \(\partial_{\varepsilon}^{2}\,x\) satisfies the following SDE with probability 1:

\[d\,\partial_{\varepsilon}^{2}\,x_{t}=\left(\beta_{k}^{\varepsilon}\left( \partial_{\varepsilon}x_{t},x_{t},t\right)+2\frac{\partial\,\eta_{k}}{\partial x }\left(x_{t},t\right)\partial_{\varepsilon}\,x_{t}+\alpha_{k}^{\varepsilon} \left(x_{t},t\right)\partial_{\varepsilon}^{2}x_{t}\right)dB_{t}^{k},\] (37)

with initial value \(\partial_{\varepsilon}^{2}\,x(0)=0\in\mathbb{R}^{n},\) where \(\partial_{\varepsilon}\,x_{t}\) is the solution to Equation(36), \(x(t)\) is the solution to Equation (35), and the functions

\[\beta_{k}^{\varepsilon}:\left(\partial_{\varepsilon}\,x,t\right)\mapsto \partial_{\varepsilon}\,x^{j}\left(\frac{\partial g_{k}^{i}}{\partial x^{j} \partial x^{j}}(x,t)+\varepsilon\frac{\partial\eta_{k}^{i}}{\partial x^{j} \partial x^{j}}(x,t)\right)\partial_{\varepsilon}\,x^{l},\text{where }k=0,\,...,\,m.\]

When \(\varepsilon=0\) in the obtained SDEs (35), (36) and (37), the corresponding solutions of which are \(x_{t}^{0},\partial_{\varepsilon}\,x_{t}^{0},\partial_{\varepsilon}^{2}\,x_{t}^ {0},\) we now have the following:

\[d\,x_{t}^{0}=g_{k}\left(x_{t}^{0},t\right)dB_{t}^{k},\] (38) \[d\,\partial_{\varepsilon}\,x_{t}^{0}=\left(\frac{\partial g_{k} }{\partial x}\left(x_{t}^{0},t\right)\partial_{\varepsilon}\,x^{0}+\eta_{k} \left(x_{t}^{0},t\right)\right)dB_{t}^{k},\] (39) \[d\,\partial_{\varepsilon}^{2}\,x_{t}^{0}=\left(\Psi_{k}\left( \partial_{\varepsilon}\,x_{t}^{0},x_{t}^{0},t\right)+2\frac{\partial\eta_{k}} {\partial x}\left(x_{t}^{0},t\right)\partial_{\varepsilon}\,x_{t}^{0}+\frac{ \partial g_{k}}{\partial x}\left(x_{t}^{0},t\right)\partial_{\varepsilon}^{2} \,x_{t}^{0}\right)dB_{t}^{k},\] (40)

with initial values \(x(0)=x_{0},\partial_{\varepsilon}\,x(0)=0,\partial_{\varepsilon}^{2}\,x(0)=0\). In particular, \(\Psi_{k}:=\beta_{k}^{0}\) is given by

\[\left(\partial_{\varepsilon}x,x,t\right)\mapsto\partial_{\varepsilon}x^{i} \frac{\partial g_{k}}{\partial x^{i}\partial x^{i}}(x,t)\partial_{\varepsilon}x ^{j}.\]

_Step 2:_ For the next step, we show that the solutions \(x_{t}^{0},\partial_{s}\,x_{t}^{0},\partial_{\varepsilon}^{2}\,x_{t}^{0}\) are indeed bounded by proving the following lemma B.6:

**Lemma B.6**.: \[\mathbb{E}\sup_{t\in[0,T]}\left\|x_{t}^{0}\right\|^{2},\,\mathbb{E}\sup_{t\in[ 0,T]}\left\|\partial_{\varepsilon}\,x_{t}^{0}\right\|^{2},\text{and}\,\, \mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{\varepsilon}^{2}\,x_{t}^{0}\right\| ^{2}\,\text{are bounded.}\]

Proof.: To simplify the notations, we take the liberty to write constants as \(C\) and notice that \(C\) is not necessarily identical in its each appearance.

(1) We first show that \(\mathbb{E}\sup_{t\in[0,T]}\left\|x_{t}^{0}\right\|^{2}\) is bounded.

From Equation (38), we have that

\[x_{t}^{0}=x_{0}+\int_{0}^{t}g_{k}\left(x_{\tau},\tau\right)dB_{\tau}^{k}.\]By Jensen's inequality. it holds that

\[\mathbb{E}\sup_{t\in[0,T]}\left\|x_{t}\right\|^{2}\leq C\,\mathbb{E}\left\|x_{0} \right\|^{2}+C\,\mathbb{E}\sup_{t\in[0,T]}\left\|\int_{0}^{t}g_{k}\left(x_{\tau }^{0},\tau\right)dB_{\tau}^{k}\right\|^{2}.\] (41)

For the second term on the right hand side, it is a sum over \(k\) from \(0\) to \(m\) by Einstein notation.

For \(k=0\), recall that we write \(t\) as \(B_{t}^{0}\) :

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\int_{0}^{t}g_{0}\left(x_{\tau}^{ 0},\tau\right)d\tau\right\|^{2} \leq C\,\mathbb{E}\sup_{t\in[0,T]}t\int_{0}^{t}\left\|g_{0}\left(x_{ \tau}^{0},\tau\right)\right\|^{2}d\tau,\] (i) \[\leq C\,\mathbb{E}\sup_{t\in[0,T]}\int_{0}^{t}C\left(1+\left\|x_{\tau }^{0}\right\|\right)^{2}d\tau,\] (ii) \[\leq C\,+C\int_{0}^{T}\mathbb{E}\sup_{s\in[0,\tau]}\left\|x_{s}^{0} \right\|^{2}d\tau,\] (iii)

where we used Jensen's inequality, the assumption on the linear growth, the inequality property of \(\sup\) and Fubini's theorem, respectively.

For \(k\) is equal to \(1,\ldots,m\),

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\int_{0}^{t}g_{1}\left(x_{\tau, \tau}^{0},\tau\right)dB_{\tau}\right\|^{2} \leq C\,\mathbb{E}\int_{0}^{T}\left\|g_{1}\left(x_{\tau}^{0},\tau \right)\right\|^{2}d\tau,\] (iv) \[\leq C\,+C\int_{0}^{T}\mathbb{E}\sup_{s\in[0,\tau]}\left\|x_{s}^{0} \right\|d\tau,\] (v)

where (iv) holds from the Burkholder-Davis-Gundy inequality as \(\int_{0}^{t}g_{k}\left(x_{\tau}^{0},\tau\right)dB_{\tau}\) is a continuous local martingale with respect to the filtration \(\mathcal{F}_{t}\); and then one can obtain (v) by following a similar reasoning of (ii) and (iii).

Hence, now from the previous inequality (41),

\[\mathbb{E}\sup_{t\in[0,T]}\left\|x_{t}^{0}\right\|^{2}\leq\,\mathbb{E}\left\| x_{0}\right\|^{2}+C+C\int_{0}^{T}\mathbb{E}\sup_{s\in[0,\tau]}\left\|x_{s}^{0} \right\|d\tau.\]

By the Gronwall's lemma, it holds true that

\[\mathbb{E}\sup_{t\in[0,T]}\left\|x_{t}^{0}\right\|^{2}\leq\left(C\,\mathbb{E }\left\|x_{0}\right\|^{2}+C\right)\exp(C).\]

As \(x_{0}\) is square-integrable by assumption, therefore we have shown that \(\mathbb{E}\sup_{t\in[0,T]}\left\|x_{t}^{0}\right\|^{2}\) is bounded.

(2) We then show that \(\mathbb{E}\sup_{t\in[0,T]}||\partial_{\varepsilon}\,x_{t}^{0}||^{2}\) is also bounded.

From the SDE (39), as we have derived that

\[\partial_{\varepsilon}\,x_{t}^{0}=\int_{0}^{t}\frac{\partial g_{k}}{\partial x }\left(x_{\tau}^{0},\tau\right)\partial_{\varepsilon}\,x_{\tau}^{0}+\eta_{k} \left(x_{\tau}^{0},\tau\right)dB_{\tau}^{k},\]

then we have

\[\mathbb{E}\sup_{t\in[0,\tau]}\left\|\partial_{\varepsilon}\,x_{t}^{0}\right\| ^{2}\leq\,C\,\mathbb{E}\sup_{t\in[0,\tau]}\left\|\int_{0}^{t}\frac{\partial g_ {k}}{\partial x}\left(x_{\tau}^{0},\tau\right)\partial_{\varepsilon}\,x_{\tau }^{0}\,dB_{\tau}^{k}\right\|^{2}+C\,\mathbb{E}\sup_{t\in[0,T]}\left\|\int_{0} ^{t}\eta_{k}\left(x_{\tau}^{0},\tau\right)dB_{\tau}^{k}\right\|^{2}.\]For \(k=0\), we have

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\int_{0}^{t}\frac{\partial g_{0}}{ \partial x}\left(x_{\tau}^{0},\tau\right)\partial_{\varepsilon}\,x_{\tau}^{0} dt\right\|^{2}+\mathbb{E}\sup_{t\in[0,T]}\left\|\int_{0}^{t}\eta_{0}\left(x_{ \tau}^{0},\tau\right)d\tau\right\|^{2},\] (vi) \[\leq C\,\mathbb{E}\sup_{t\in[0,T]}\int_{0}^{t}\left\|\frac{\partial g _{0}}{\partial x}\left(x_{\tau}^{0},t\right)\right\|^{2}\left\|\partial_{ \varepsilon}\,x_{\tau}^{0}\right\|^{2}d\tau+C\mathbb{E}\sup_{t\in[0,T]}\int_{0 }^{t}\left\|\eta_{0}\left(x_{\tau}^{0},\tau\right)\right\|^{2}d\tau,\] (vii) \[\leq C\,\mathbb{E}\sup_{s\in[0,T]}\left\|\frac{\partial g_{0}}{ \partial x}\left(x_{s}^{0},s\right)\right\|^{2}\sup_{t\in[0,T]}\int_{0}^{t} \left\|\partial_{\varepsilon}\,x_{\tau}^{0}\right\|^{2}d\tau+C\,\mathbb{E} \sup_{t\in[0,T]}\int_{0}^{t}C\left(1+\left\|x_{\tau}^{0}\right\|\right)^{2}d\tau,\] \[\leq C+C\,\mathbb{E}\sup_{t\in[0,T]}\int_{0}^{t}\left\|\partial_{ \varepsilon}\,x_{\tau}^{0}\right\|^{2}d\tau+C\,\mathbb{E}\sup_{t\in[0,T]}\int_ {0}^{t}\left\|x_{\tau}^{0}\right\|^{2}d\tau,\] (viii) \[\leq C+C\,\int_{0}^{T}\mathbb{E}\sup_{s\in[0,\tau]}\left\|\partial_{ \varepsilon}\,x_{s}^{0}\right\|^{2}d\tau+C\,\mathbb{E}\sup_{t\in[0,T]}\left\| x_{t}^{0}\right\|^{2},\]

where to get to (vi), we used Jensen's inequality; for (vii), we used the linear growth assumption an \(\eta_{0}\), then we obtain (viii) by as derivatives of function \(g_{0}\) are bounded by assumption.

Similarly, for \(k=1,\,...,\,m\),

\[C\,\mathbb{E}\sup_{t\in[0,T]}\left\|\int_{0}^{t}\frac{\partial g _{1}}{\partial x^{i}}\left(x_{\tau}^{0},\tau\right)\partial_{\varepsilon}\,x_ {\tau}^{0}dB_{\tau}\right\|^{2}+C\,\mathbb{E}\sup_{t\in[0,T]}\left\|\int_{0}^ {t}\eta_{1}\left(x_{\tau}^{0},\tau\right)dB_{\tau}\right\|^{2},\] \[\leq C\,\mathbb{E}\int_{0}^{T}\left\|\frac{\partial g_{1}}{ \partial x}\left(x_{\tau}^{0},\tau\right)\right\|^{2}\left\|\partial_{ \varepsilon}\,x_{\tau}^{0}\right\|^{2}d\tau+C\,\mathbb{E}\int_{0}^{T}\left\| \eta_{1}\left(x_{\tau}^{0},\tau\right)\right\|^{2}d\tau,\] (ix) \[\leq C+C\int_{0}^{T}\mathbb{E}\sup_{s\in[0,\tau]}||\partial_{ \varepsilon}\,x_{s}^{0}||^{2}d\tau+C\,\mathbb{E}\sup_{t\in[0,T]}||x_{t}^{0}|| ^{2},\] (x)

where we obtain (ix) by the Burkholder-Davis-Gundy inequality and (x) by following similar steps as have shown in (vii) and (viii).

We are now ready to sum up each term to acquire a new inequality:

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{\varepsilon}\,x_{t}^{0}\right\|^{2 }\leq C+C\,\mathbb{E}\sup_{t\in[0,T]}\left\|x_{t}^{0}\right\|^{2}+C\,\int_{0}^ {T}\mathbb{E}\sup_{s\in[0,\tau]}\left\|\partial_{\varepsilon}\,x_{s}^{0} \right\|^{2}d\tau.\]

By Gronwall's lemma, we have that

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{\varepsilon}\,x_{t}^{0}\right\|^{2 }\leq\left(C+C\,\mathbb{E}\sup_{t\in[0,T]}\left\|x_{t}^{0}\right\|^{2}\right) \exp(C).\]

As it is previously shown that \(\mathbb{E}\sup_{t\in[0,\tau]}\left\|x^{\circ}(t)\right\|^{2}\) is bounded, it is clear that \(\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{\varepsilon}\,x_{t}^{0}\right\|^{2}\) is bounded too.

(3) From similar steps, one can also show that \(\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{\varepsilon}^{2}\,x_{t}^{0}\right\| ^{2}\) is bounded. 

_Step 3_: Having shown that \(x_{t}^{0},\partial_{\varepsilon}\,x_{t}^{0},\partial_{\varepsilon}^{2}\,x_{t}^ {0}\) are bounded, we proceed to bound the remainder term by proving the following lemma.

**Lemma B.7**.: _For a given \(\varepsilon\in\mathbb{R}\), let_

\[\mathcal{R}^{\varepsilon}:=(t,\omega)\mapsto\frac{1}{\varepsilon^{3}}\left(x^{ \varepsilon}(t,\omega)-x^{0}(t,\omega)-\varepsilon\partial_{\varepsilon}x^{0} (t,\omega)-\varepsilon^{2}\partial_{\varepsilon}^{2}\,x^{0}(t,\omega)\right),\]

_where the stochastic process \(x_{t}^{\varepsilon}\) is the solution to Equation (32). Then it holds true that_

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\mathcal{R}^{\varepsilon}(t)\right\|^{2}\text{ is bounded.}\]Proof.: The main strategy of this proof is to first rewrite \(\varepsilon^{3}\mathcal{R}^{\varepsilon}\) as the sum of some simpler terms and then to bound each term. To simplify the notation, we denote \(\tilde{x}_{t}^{\varepsilon}\) as \(x_{t}^{0}+\varepsilon\partial_{\varepsilon}\,x_{t}^{0}+\varepsilon^{2}\, \partial_{\varepsilon}^{2}x_{t}^{0}\).

For \(k=0,..,n\), we define the following terms:

\[\theta_{k}(t) :=\int_{0}^{t}g_{k}\left(x_{\tau}^{\varepsilon},\tau\right)-g_{k} \left(\tilde{x}_{\tau}^{\varepsilon},\tau\right)dB_{\tau}^{k},\] \[\varphi_{k}(t) :=\int_{0}^{t}g_{k}\left(\tilde{x}_{\tau}^{\varepsilon},\tau \right)-g_{k}\left(x_{\tau}^{0},\tau\right)-\varepsilon\frac{\partial g_{k}}{ \partial x}\left(x_{\tau}^{0},\tau\right)\partial_{\varepsilon}\,x_{\tau}^{0 }-\varepsilon^{2}\Psi_{k}\left(\partial_{\varepsilon}\,x_{\tau}^{0},x_{\tau}^ {0},\tau\right)-\varepsilon^{2}\frac{\partial g_{k}}{\partial x^{i}}\left(x_ {\tau}^{0},\tau\right)\partial_{\varepsilon}^{2}\,x_{\tau}^{0}dB_{\tau}^{k},\] \[\sigma_{k}(t) :=-\varepsilon\int_{0}^{t}\eta_{k}\left(x_{\tau}^{0},\tau\right) +2\varepsilon\,\frac{\partial\eta}{\partial x}\left(x_{\tau}^{0},\tau\right) \partial_{\varepsilon}\,x_{\tau}^{0}dB_{\tau}^{k}.\]

Hence, we have \(\varepsilon^{3}\mathcal{R}^{\varepsilon}(t)=\sum_{k=0}^{1}\theta_{k}(t)+ \varphi_{k}(t)+\sigma_{k}(t)\).

For \(\theta_{k}(t)\), we have

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\theta_{k}(t)\right\|^{2} \leq C\,\mathbb{E}\sup_{t\in[0,T]}\int_{0}^{t}\left\|g_{k}\left(x_ {\varphi}^{\varepsilon},e\right)-g_{k}\left(\tilde{x}_{\varphi}^{\varepsilon },\tau\right)\right\|^{2}d\tau,\] (i) \[\leq C\,\int_{0}^{T}\mathbb{E}\sup_{t\in[0,tau]}\left\|x_{t}^{ \varepsilon}-\tilde{x}_{t}^{\varepsilon}\right\|^{2}d\tau,\] (ii) \[\leq C\,\int_{0}^{T}\mathbb{E}\sup_{t\in[0,\tau]}\left\| \mathcal{R}^{\varepsilon}(t)\right\|^{2}d\tau,,\] (iii)

where to obtain (i) we used Jensen's inequality when \(k=0\) and by the Burkholder-Davis-Gundy inequality when \(k=1\), used the Lipschitz condition of \(g_{k}\) to obtain (ii), and for (iii), it is because \(\varepsilon^{3}\mathcal{R}^{\varepsilon}(t)=\tilde{x}_{t}^{\varepsilon}-x_{t}^ {\varepsilon}\).

We note that from Taylor's theorem, for any \(s\in[0,t],\,k=0,1\), there exists some \(\varepsilon_{s}\in(0,\varepsilon)\) s.t.

\[g_{k}\left(\tilde{x}_{s}^{\varepsilon},s\right)-g_{k}\left(x_{s}^{0},s\right) -\varepsilon\frac{\partial g_{k}}{\partial x}\left(x_{s}^{0},s\right) \partial_{\varepsilon}x_{s}^{0}=\varepsilon^{2}\frac{\partial g_{k}}{ \partial x}\left(\tilde{x}_{s}^{\varepsilon_{s}}\right)\partial_{ \varepsilon}^{2}\,x_{s}^{0}+\varepsilon^{2}\Psi\left(\partial_{\varepsilon} \,x_{s}^{0},\tilde{x}_{s}^{\varepsilon_{s}},s\right).\] (42)

For \(\varphi_{k}(t)\), we have

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\varphi_{k}(t)\right\|^{2}\] \[\leq C\,\mathbb{E}\sup_{t\in[0,T]}\int_{0}^{t}\left\|\frac{\partial g _{k}}{\partial x}\left(\tilde{x}_{s}^{\varepsilon_{s}}\right)\partial_{ \varepsilon}^{2}\,x_{s}^{0}+\Psi_{k}\left(\partial_{\varepsilon}\,x_{s}^{0}, \tilde{x}_{s}^{\varepsilon_{s}},s\right)-\frac{\partial g_{k}}{\partial x}\left( x_{s}^{0}\right)\partial_{\varepsilon}^{2}\,x_{s}^{0}-\Psi_{k}\left(\partial_{ \varepsilon}\,x_{s}^{0},x_{s}^{0},s\right)\right\|^{2}ds,\] (iv) \[\leq C\,\mathbb{E}\sup_{t\in[0,T]}\int_{0}^{t}\left\|\frac{\partial g _{k}}{\partial x}\left(\tilde{x}_{s}^{\varepsilon_{s}}\right)-\frac{\partial g_{ k}}{\partial x}\left(x_{s}^{0}\right)\right\|^{2}\left\|\partial_{ \varepsilon}^{2}\,x_{s}^{0}\right\|^{2}+\left\|\Psi_{k}\left(\partial_{ \varepsilon}x_{s}^{0},\tilde{x}_{s},s\right)-\Psi_{k}\left(\partial_{ \varepsilon}x_{s}^{0},x_{s}^{0},s\right)\right\|^{2}ds,\] (v) \[\leq C\,\mathbb{E}\sup_{t\in[0,T]}\int_{0}^{t}\left\|\tilde{x}_{s}^{ \varepsilon_{s}}-x_{s}^{0}\right\|^{2}\left(C+\left\|\partial_{\varepsilon}^{2} \,x_{s}^{0}\right\|^{2}\right)ds,\] (vi) \[\leq C\,\mathbb{E}\sup_{t\in[0,T]}\int_{0}^{t}\left\|\varepsilon \partial_{\varepsilon}\,x_{s}^{0}+\varepsilon^{2}\partial_{\varepsilon}^{2}\,x_{s}^ {0}\right\|^{2}\left(C+\left\|\partial_{\varepsilon}^{2}\,x_{s}^{0}\right\|^{2 }\right)ds,\] \[\leq C\left(\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{\varepsilon} \,x_{s}^{0}\right\|^{2}\right)+\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{ \varepsilon}^{2}\,x_{s}^{0}\right\|^{2}\right)\left(C+\mathbb{E}\sup_{t\in[0,T]} \left\|\partial_{\varepsilon}^{2}\,x_{s}^{0}\right\|^{2}\right),\] (vii)

where for (iv), we used Equation (42) and Jensen's inequality for \(k=0\) and the Burkholder-Davis-Gundy inequality for \(k=1\); to obtain (v), we applied Jensen's equality; we then derived (vi) from the Lipschitz conditions of \(g_{k}\) and \(\Psi_{k}\); and finally another application of Jensen's inequality gives (vii) which is bounded as a result from the Lemma B.6.

For \(\sigma_{k}(t)\),

\[\sup_{t\in[0,T]}\left\|\sigma_{0}(t)\right\|^{2}\leq \,C\,\varepsilon\,\int_{0}^{T}\mathbb{E}\sup_{s\in[0,t]}\left\| \eta_{k}\left(x_{s}^{0},s\right)\right\|^{2}+C\mathbb{E}\sup_{t\in[0,T]}\left\| \frac{\partial\eta_{k}}{\partial x}\left(x_{s}^{0},s\right)\right\|^{2}\left\| \partial_{\varepsilon}\,x_{s}^{0}\right\|^{2}dt,\] (ix) \[\leq \,C\,\int_{0}^{T}C\left(1+\mathbb{E}\,\sup_{s\in[0,t]}\left\|x_{s} ^{0}\right\|^{2}\right)+C\mathbb{E}\sup_{t\in[0,T]}\left\|\frac{\partial\eta_{ k}}{\partial x}\left(x_{t}^{0},t\right)\right\|^{2}\,\int_{0}^{T}\mathbb{E}\sup_{s \in[0,t]}\left\|\partial_{\varepsilon}x_{s}^{0}\right\|^{2}dt,\] (xi) \[\leq \,c+C\,\mathbb{E}\sup_{t}\in[0,T]\left\|x_{s}^{0}\right\|^{2}+C \,\mathbb{E}\sup_{t\in[0,T]}\left\|\frac{\partial\eta}{\partial x}\left(x_{t} ^{0},t\right)\right\|^{2}\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{ \varepsilon}x_{t}^{0}\right\|^{2},\] (xi)

where we obtained (ix) by Jensen's inequality when \(k=0\) and by Burkholder-Davis-Gundy inequality when \(k=1\), and (x) by the linear growth assumption on \(\eta_{k}\); one can see that (xi) is bounded by recalling the Lemma B.6 and the assumption that \(\eta_{k}\) has bounded derivatives.

Hence, by Jensen's inequality and Gronwall's lemma, we have

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\mathcal{R}^{\varepsilon}(t) \right\|^{2}\leq \,C\,\sum_{k=0}^{K}\,\mathbb{E}\sup_{t\in[0,T]}\left\|\theta_{k}( t)\right\|^{2}+\mathbb{E}\sup_{t\in[0,T]}\left\|\varphi_{k}(t)\right\|^{2}+ \mathbb{E}\sup_{t\in[0,T]}\left\|\sigma_{k}(t)\right\|^{2},\] \[\leq \,C\,\sup\left\|\mathcal{R}^{\varepsilon}(t)\right\|^{2}\text{ is bounded.}\]

Finally, it is now straightforward to show Equation (33) by applying a second-order Taylor expansion on \(f\left(x_{t}^{0}+\varepsilon\partial_{\varepsilon}x_{t}^{0}+\varepsilon^{2} \partial_{\varepsilon}^{2}x_{t}^{0}\right.\left.+\varepsilon^{3}R^{ \varepsilon}(t)\right)\).

We are now ready to show Theorem 3.7. One notes that Corollary 3.8 directly follows from the result too.

Proof.: _(Theorem 3.7)_ From Proposition B.5, it is noteworthy to point out that the derived SDEs (34) for \(\partial_{\varepsilon}\,x_{t}^{0}\) and \(\partial_{\varepsilon}^{2}\,x_{t}^{0}\) are vector-valued general linear SDEs. With some steps of derivations, one can express the solutions as:

\[\partial_{\varepsilon}\,x_{t}^{0}= \,\Phi_{t}\int_{0}^{t}\Phi_{s}^{-1}\left(\eta_{0}(x_{s}^{0},s)- \sum_{k=1}^{m}\frac{\partial g_{k}}{\partial x}(x_{s}^{0},s)\eta_{k}(x_{s}^{0},s)\right)ds+\,\Phi_{t}\int_{0}^{t}\Phi_{s}^{-1}\eta_{k}(x_{s}^{0},s)dB_{s}^{k}\] (a) \[\partial_{\varepsilon}^{2}\,x_{t}^{0}= \,\Phi_{t}\int_{0}^{t}\Phi_{s}^{-1}\bigg{(}\Psi_{0}(x_{s}^{0}, \partial_{\varepsilon}\,x_{s}^{0},s)+2\,\frac{\partial\eta_{0}}{\partial x}(x_{ s}^{0},s)\partial_{\varepsilon}\,x_{s}^{0}\] \[\qquad-\sum_{k=1}^{m}\frac{\partial g_{k}}{\partial x}(x_{s}^{0},s )\Big{(}\Psi_{k}(x_{s}^{0},\partial_{\varepsilon}\,x_{s}^{0},s)+2\,\frac{ \partial\eta_{k}}{\partial x}(x_{s}^{0},s)\partial_{\varepsilon}\,x_{s}^{0} )\Big{)}\bigg{)}ds,\] \[+\Phi_{t}\int_{0}^{t}\Phi_{s}^{-1}\sum_{k=1}^{m}\left(\Psi_{k}(x_{ s}^{0},\partial_{\varepsilon}\,x_{s}^{0},s)+2\,\frac{\partial\eta_{k}}{\partial x}(x_{ s}^{0},s)\partial_{\varepsilon}\,x_{s}^{0}\right)dB_{s}^{k},\] (b)

where \(n\times n\) matrix \(\Phi_{t}\) is the fundamental matrix of the corresponding homogeneous equation:

\[d\Phi_{t}=\frac{\partial g_{k}}{\partial x}(x_{t}^{0},t)\,\Phi_{t}\,dB_{t}^{k},\] (43)

with initial value

\[\Phi(0)=I.\] (44)It is worthy to note that the fundamental matrix \(\Phi_{t}\) is non-deterministic and when \(\frac{\partial g_{i}}{\partial x}\) and \(\frac{\partial g_{j}}{\partial x}\) commutes, \(\Phi_{t}\) has explicit solution

\[\Phi_{t}=\exp\left(\int_{0}^{t}\frac{\partial g_{k}}{\partial x}(x_{s}^{0},s) dB_{s}^{k}-\frac{1}{2}\int_{0}^{t}\frac{\partial g_{k}}{\partial x}(x_{s}^{0},s) \frac{\partial g_{k}}{\partial x}(x_{s}^{0},s)^{\top}ds\right).\] (45)

Having obtained the explicit solutions, one can plug in corresponding terms and obtain the results of _Theorem 3.7_) after a Taylor expansion of the loss function \(\mathcal{L}\).

## Appendix C Error Accumulation During the Inference Phase and its Effects to Value Functions

**Theorem C.1**.: _(Error accumulation due to initial representation error ) Let \(\delta:=\mathbb{E}\left\|\varepsilon\right\|\) and \(d_{\varepsilon}:=\mathbb{E}\sup_{t\in[0,T]}\left\|h_{t}^{\varepsilon}-h_{t}^{0} \right\|^{2}+\left\|\tilde{z}_{t}^{\varepsilon}-\tilde{z}_{t}^{0}\right\|^{2}\). It holds that as \(\delta\to 0\),_

\[d_{\varepsilon}\leq\,\delta\,C\left(\mathcal{J}_{0}+\mathcal{J}_{1}\right)+ \,\delta^{2}\,C\left(\exp\left(\,\mathcal{H}_{0}\left(\mathcal{J}_{0}+ \mathcal{J}_{1}\right)\right)+\exp\left(\,\mathcal{H}_{1}\left(\mathcal{J}_{0}+ \mathcal{J}_{1}\right)\right)\right)+\mathcal{O}(\delta^{3}),\] (46)

_where_

\[\mathcal{J}_{0}= \exp\left(\mathcal{F}_{h}+\mathcal{F}_{z}+\mathcal{P}_{h}\right),\,\mathcal{J}_{1}=\exp\left(\bar{\mathcal{P}}_{h}\right),\] \[\mathcal{H}_{0}= \mathcal{F}_{hh}+\mathcal{F}_{hz}+\mathcal{F}_{zh}+\mathcal{F}_{ zz}+\mathcal{P}_{hh},\,\mathcal{H}_{1}=\bar{\mathcal{P}}_{hh}\]

\[\mathcal{F}_{h}= C\,\mathbb{E}\,\sup_{t\in[0,T]}\left\|\frac{\partial f}{ \partial h}+\frac{\partial f}{\partial a}\partial_{h}\rho\right\|_{F}^{2},\, \,\,\,\mathcal{F}_{z}=C\,\mathbb{E}\,\sup_{t\in[0,T]}\left\|\frac{\partial f}{ \partial z}+\frac{\partial f}{\partial a}\partial_{z}\rho\right\|_{F}^{2},\] \[\mathcal{P}_{h}= C\,\mathbb{E}\,\sup_{t\in[0,T]}\left\|\frac{\partial p}{ \partial h^{2}}+\frac{\partial^{2}f}{\partial h\partial a}\partial_{h}\rho+ \frac{\partial f}{\partial a}\partial_{hh}^{2}\rho\right\|_{F}^{2},\] \[\mathcal{F}_{hz}= C\,\mathbb{E}\,\sup_{t\in[0,T]}\left\|\frac{\partial^{2}f}{ \partial h\partial z}+\frac{\partial^{2}f}{\partial z\partial a}\partial_{h} \rho+\frac{\partial f}{\partial a}\partial_{zh}^{2}\rho\right\|_{F}^{2}\] \[\mathcal{F}_{zh}= C\,\mathbb{E}\,\sup_{t\in[0,T]}\left\|\frac{\partial^{2}f}{ \partial h\partial z}+\frac{\partial^{2}f}{\partial h\partial a}\partial_{z} \rho+\frac{\partial f}{\partial a}\partial_{iz}^{2}\rho\right\|_{F}^{2}\] \[\mathcal{F}_{zz}= C\,\mathbb{E}\,\sup_{t\in[0,T]}\left\|\frac{\partial^{2}f}{ \partial z^{2}}+\frac{\partial^{2}f}{\partial z\partial a}\partial_{z}\rho+ \frac{\partial f}{\partial a}\partial_{zz}^{2}\rho\right\|_{F}^{2},\] \[\mathcal{P}_{hh}= C\,\mathbb{E}\,\sup_{t\in[0,T]}\left\|\frac{\partial^{2}p}{ \partial h^{2}}\right\|_{F}^{2},\,\,\mathcal{P}_{hh}=C\,\mathbb{E}\,\sup_{t \in[0,T]}\left\|\frac{\partial^{2}\bar{p}}{\partial h^{2}}\right\|_{F}^{2},\] (48)

_where for brevity, when functions always have inputs \((\tilde{z}_{t}^{0},h_{t}^{0},t)\), we adopt the shorthand to write, for example, \(f(\tilde{z}_{t}^{0},h_{t}^{0},t)\) as \(f\)._

Before proving the main result C.1, we first show the general case of perturbation in initial values. Consider the following general system with noise at the initial value:

\[dx_{t}=g_{0}\left(x_{t},t\right)dt+g_{k}\left(x_{t},t\right)dB_{t}^{k},\] (47) \[x(0)=x_{0}+\varepsilon,\] (49)

where the initial perturbation \(\varepsilon\in\mathbb{R}^{n}\times\Omega\). As \(g_{k}\) are \(\mathcal{C}_{g}^{2,\alpha}\) functions, by the classical result on the existence and the uniqueness of solution to SDE, there exists a unique solution to Equation (47), denoted as \(x_{t}^{x}\) or \(x^{\varepsilon}(t)\).

To simplify the notation, we write \(\partial_{i}\,x_{t}^{x}:=\frac{\partial x^{\varepsilon}(t)}{\partial x^{ \varepsilon}},\partial_{ij}^{2}\,x_{t}^{x}=\frac{\partial^{2}x_{t}^{x}}{ \partial x^{\varepsilon}(t)x}\), for \(i,j=1,\,\ldots,n\) that are, respectively, the first and second-order derivatives of the solution \(x^{\varepsilon}(t)\) w.r.t. the changes in the corresponding coordinates of the initial value. When \(\varepsilon=0\in\mathbb{R}^{n}\), we denote the solutions to Equation (47) as \(x_{t}^{0}\) with its first and second derivatives \(\partial_{i}\,x_{t}^{0},\partial_{ij}^{2}\,x_{t}^{0}\), respectively.

**Proposition C.2**.: _Let \(\delta:=\mathbb{E}\left\|\varepsilon\right\|\), it holds that_

\[\mathbb{E}\sup_{t\in[0,T]}\left\|x_{t}^{\varepsilon}-x_{t}^{0}\right\|^{2} \leq\sum_{k=0,1}C\,\delta\left(C\,\mathbb{E}\sup_{t\in[0,T]} \left\|\frac{\partial g_{k}}{\partial x}(x_{t}^{0},t)\right\|_{F}^{2}\right)\] (50)

_as \(\delta\to 0\)._Proof.: Similar to the previous section, for notational convenience, we write \(t\) as \(B_{i}^{0}\) and employs Einstein summation notation. Hence, Equation (47) can be shorten as

\[dx_{t}=g_{k}\left(x_{t},t\right)dB_{t}^{k},\] (51)

with initial values \(x(0)=x_{0}+\varepsilon\).

To begin, we find the SDEs that characterize \(\partial_{i}\,x_{t}^{\varepsilon}\) and \(\partial_{ij}^{2}\,x_{t}^{\varepsilon}\), for \(i,\,j=1,\,...,\,n\).

For \(\partial_{i}\,x_{t}^{\varepsilon}\), we apply Theorem 3.1 from Section 2 in [17] on Equation (51) and \(\partial_{i}\,x_{t}^{\varepsilon}\) satisfy the following SDE with probability 1,

\[d\partial_{i}\,x_{t}^{\varepsilon}=\frac{\partial g_{k}}{\partial x}\left(x_{ t}^{\varepsilon},t\right)\partial_{i}\,x_{t}^{\varepsilon}dB_{t}^{k}\] (52)

with initial value \(\partial_{i}\,x_{0}^{\varepsilon}\) to be the unit vector \(e_{i}=\left(0,\,0,\,\dots,\,1,\,\dots,\,0\right)\) that is all zeros except one in the \(i^{\text{th}}\) coordinate.

For \(\partial_{ij}^{2}\,x_{t}^{\varepsilon}\), we again apply Theorem 3.1 from Section 2 in [17] on the SDE (52) and obtain that \(\partial_{ij}^{2}x_{b}^{\varepsilon}\) satisfy the following SDE with probability 1,

\[d\partial_{ij}^{2}\,x_{t}^{\varepsilon}=\Psi_{k}\left(x_{t}^{\varepsilon}, \partial_{i}\,x_{t}^{\varepsilon},t\right)\partial_{ij}^{2}\,x_{t}^{ \varepsilon}dB_{t}^{k},\] (53)

with the initial value \(\partial_{ij}\,x^{\varepsilon}(0)=e_{j}\), where

\[\Psi_{k}:\mathbb{R}^{d}\times\mathbb{R}^{d}\times[0,T]\to\mathbb{R}^{d\times d },\,(x,\partial_{i}\,x,t)\mapsto\left(\frac{\partial^{2}g_{k}^{l}}{\partial x ^{u}\partial x^{v}}\left(x_{t}^{\varepsilon},t\right)\right)_{l,u,v}\partial_ {i}\,x^{v}.\]

For the next step, we show that with probability 1, the following holds

\[x_{t}^{\varepsilon}=x_{t}^{0}+\varepsilon^{i}\,\partial_{i}\,x_{t}^{0}+\frac{ 1}{2}\,\varepsilon^{i}\varepsilon^{j}\,\partial_{ij}^{2}\,x_{t}^{0}+O\left( \varepsilon^{3}\right),\] (54)

as \(\left\|\varepsilon\right\|\to 0\).

One can follow the similar steps of proofs for Lemma (B.6) and (B.7) in the previous section to show that \(\mathbb{E}\sup_{t\in[0,T]}\left\|x_{t}^{0}\right\|^{2},\,\mathbb{E}\sup_{t\in[ 0,T]}\left\|\partial_{i}x_{t}^{0}\right\|^{2},\,\mathbb{E}\sup_{t\in[0,T]} \left\|\partial_{ij}^{2}x_{t}^{0}\right\|^{2}\) and the remainder term are bounded. Hence, Equation (54) holds with probability 1.

Indeed, for \(\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{i}\,x_{t}^{0}\right\|^{2}\), it holds that

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{i}\,x_{t}^{0}\right\|^ {2}\leq \,C\left\|e_{i}\right\|^{2}+\sum_{k=0,1}\mathbb{E}\sup_{t\in[0,T]}C \int_{0}^{t}\left\|\frac{\partial g_{k}}{\partial x}(x_{s}^{0},s)\right\|_{F} ^{2}\left\|\partial_{i}\,x_{s}\right\|^{2}ds\] (55) \[\leq \sum_{k=0,1}C\exp\left(C\,\mathbb{E}\sup_{t\in[0,T]}\left\|\frac{ \partial g_{k}}{\partial x}(x_{t}^{0},t)\right\|_{F}^{2}\right).\] (56)

Similarly, for \(\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{ij}^{2}\,x_{t}^{0}\right\|^{2}\), it holds that

\[\mathbb{E}\sup_{t\in[0,T]}\left\|\partial_{ij}^{2}\,x_{t}^{0}\right\|^{2}\leq \,C\left\|e_{i}\right\|^{2}+\sum_{k=0,1}\mathbb{E}\sup_{t\in[0,T]}C \int_{0}^{t}\left\|\frac{\partial^{2}g_{k}}{\partial x^{2}}(x_{s}^{0},s) \right\|_{F}^{2}\left\|\partial_{i}\,x_{s}^{0}\right\|^{2}\left\|\partial_{ij} ^{2}\,x_{s}^{0}\right\|^{2}ds\] (57) \[\leq \,C\sum_{k=0}^{1}\exp\left(C\,\mathbb{E}\sup_{t\in[0,T]}\left\| \frac{\partial^{2}g_{k}}{\partial x^{2}}(x_{t}^{0},t)\right\|_{F}^{2}\left\| \partial_{i}\,x_{t}^{0}\right\|^{2}\right)\] (58) \[\leq \,C\sum_{k=0,1}\exp\left(C\,\mathbb{E}\sup_{t\in[0,T]}\left\| \frac{\partial^{2}g_{k}}{\partial x^{2}}(x_{t}^{0},t)\right\|_{F}^{2}\exp\left(C \,\mathbb{E}\sup_{t\in[0,T]}\left\|\frac{\partial g_{k}}{\partial x}(x_{t}^{0},t)\right\|_{F}^{2}\right)\right).\] (59)

Therefore, we could obtain the proposition by applying Jensen's inequality to Equation (54) and plugging with 56 and 57.

Now we are ready to prove Theorem C.1. We note that one could then obtain Corollary 4.2 without much more effort by a standard application of Taylor's theorem.

Proof.: (Proof for Theorem C.1)

At \((h_{t},\tilde{z}_{t},\pi(h_{t},\tilde{z}_{t}))\), where the local optimal policy \(\pi(h_{t},\tilde{z}_{t})\), denoted as \(a_{t}^{*}\), there exists an open neighborhood \(V\subseteq\mathcal{A}\) of \(a_{t}^{*}\) such that \(a_{t}^{*}\) is the local maximizer for \(Q(h_{t},\tilde{z}_{t},\cdot)\) by definition. Then, \(\frac{\partial Q}{\partial a}(h_{t},\tilde{z}_{t},a_{t}^{*})=0\), and \(\frac{\partial^{2}Q}{\partial a^{2}}(h_{t},\tilde{z}_{t},a)\) is negative definite. As \(\frac{\partial^{2}Q}{\partial a^{2}}\) is non-degenerate in the neighborhood \(V\), by the implicit function theorem, there exists a neighborhood \(U\times V\) of \((h_{t},\tilde{z}_{t},a_{t}^{*})\) such that there exists a \(\mathcal{C}^{2}\) map \(\rho:U\to V\) such that \(\frac{\partial Q}{\partial a}(h,\tilde{z},\rho(h,\tilde{z}))=0\) and \(\rho(h,\tilde{z})\) is the local maximizer of \(Q(h,\tilde{z},\cdot)\) for any \(h,\tilde{z}\in U\). Furthermore, we have that \(\partial_{h}\ \rho=-\frac{\partial^{2}Q^{-1}}{\partial a^{2}}\frac{ \partial^{2}Q}{\partial a\partial h}\). Similarly, other first-terms and second-order terms \(\partial_{z}\rho\), \(\partial_{z,\rho}^{2},\partial_{zh}^{2}\rho,\partial_{hz}^{2}\rho,\partial_{ hh}^{2}\rho\) can be explicitly expressed without much additional effort (e.g., in [28], [3]).

The rest of the proof is easy to see after plugging in the corresponding terms from Proposition C.2.

Experimental Details

In this section, we provide additional details and results beyond thoese in the main paper.

### Model Implementation and Training

Our baseline is based on the DreamerV2 Tensorflow implementation. Our theoretical and empirical results should not matter on the choice of specific version; so we chose DreamerV2 as its codebase implementation is simpler than V3. We incorporated a computationally efficient approximation of the Jacobian norm for the sequence model, as detailed in [18], using a single projection. During our experiments, all models were trained using the default hyperparameters (see Table 5) for the MuJoCo tasks. The training was conducted on an NVIDIA A100 and a GTX 4090, with each session lasting less than 15 hours.

\begin{table}
\begin{tabular}{|l|l|} \hline
**Hyperparameter** & **Value** \\ \hline eval\_every & 1e4 \\ \hline prefill & 1000 \\ \hline train\_every & 5 \\ \hline rssm\_hidden & 200 \\ \hline rssm\_deter & 200 \\ \hline model\_opt.lr & 3e-4 \\ \hline actor\_opt.lr & 8e-5 \\ \hline replay\_capacity & 2e6 \\ \hline dataset\_batch & 16 \\ \hline precision & 16 \\ \hline clip\_rewards & tanh \\ \hline expl\_behavior & greedy \\ \hline encoder\_cnn\_depth & 48 \\ \hline decoder\_cnn\_depth & 48 \\ \hline loss\_scales\_kl & 1.0 \\ \hline discount & 0.99 \\ \hline jac\_lambda & 0.01 \\ \hline \end{tabular}
\end{table}
Table 5: Hyperparameters for DreamerV2 model.

[MISSING_PAGE_FAIL:28]

[MISSING_PAGE_EMPTY:29]

[MISSING_PAGE_FAIL:30]

[MISSING_PAGE_EMPTY:31]

[MISSING_PAGE_FAIL:32]

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and the contribution section 1 in introduction, we provide a clear list of statements outlining the paper's contributions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: For each of our theoretical results, we state the required assumptions and provide relevant discussions that compare our assumptions to the practical implementations which involves certain limitations for theoretical simplifications. For empirical results, we also state the experiment settings and the number of trials run. We also discuss the possible future research to extend our work in the conclusion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?

Answer: [Yes]

Justification: For each of our theoretical results, we state the assumptions required in both the main text and the provided appendix. We provide the full proofs of all of our theoretical results in Sections A, B and C in Appendix,

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The full source code required to reproduce the experimental results is included in the submission. Guidelines: * The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our task environments Walker and Quadruped are from open source package MuJoCo. Our baseline implementation is from open source codebase DreamerV2. Our implementation of Jacobian regularization has a full description in Section D.1. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We state the hyperparameters used in Table 5. The perturbations we considered is fully described in the experiment section form the main text. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: While our work is predominantly theoretical, we conducted 5 random trials for each perturbation degree and type. For additional results including standard deviation of trials, see Section \(D\). Guidelines: ** The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Relevant computing information is provided in Section D.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This work conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The work is of theoretical nature and has no societal impact of the work performed.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks as we do not have any released data or models. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cited the baseline implementation in Section D.1. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our work is mostly of theoretical nature and does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.