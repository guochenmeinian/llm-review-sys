# LightSpeed: Light and Fast

Neural Light Fields on Mobile Devices

 Aarush Gupta\({}^{1}\)  Junli Cao\({}^{2,}\)\({}^{,}\)\({}^{\dagger}\)  Chaoyang Wang\({}^{2,}\)\({}^{\dagger}\)  Ju Hu\({}^{2}\)  Sergey Tulyakov\({}^{2}\)

**Jian Ren\({}^{2}\)  Laszlo A Jeni\({}^{1}\)**

\({}^{1}\)Robotics Institute, Carnegie Mellon University \({}^{2}\)Snap Inc.

Project page: https://lightspeed-r21.github.io

###### Abstract

Real-time novel-view image synthesis on mobile devices is prohibitive due to the limited computational power and storage. Using volumetric rendering methods, such as NeRF and its derivatives, on mobile devices is not suitable due to the high computational cost of volumetric rendering. On the other hand, recent advances in neural light field representations have shown promising real-time view synthesis results on mobile devices. Neural light field methods learn a direct mapping from a ray representation to the pixel color. The current choice of ray representation is either stratified ray sampling or Plucker coordinates, overlooking the classic light slab (two-plane) representation, the preferred representation to interpolate between light field views. In this work, we find that using the light slab representation is an efficient representation for learning a neural light field. More importantly, it is a lower-dimensional ray representation enabling us to learn the 4D ray space using feature grids which are significantly faster to train and render. Although mostly designed for frontal views, we show that the light-slab representation can be further extended to non-frontal scenes using a divide-and-conquer strategy. Our method offers superior rendering quality compared to previous light field methods and achieves a significantly improved trade-off between rendering quality and speed.

## 1 Introduction

Real-time rendering of photo-realistic 3D content on mobile devices such as phones is crucial for mixed-reality applications. However, this presents a challenge due to the limited computational power and memory of mobile devices. The current graphics pipeline requires storing tens of thousands of meshes for complex scenes and performing ray tracing for realistic lighting effects, which demands powerful graphics processing power that is not feasible on current mobile devices. Recently, neural radiance field (NeRF [23]) has been the next popular choice for photo-realistic view synthesis, which offers a simplified rendering pipeline. However, the computational cost of integrating the radiance field remains a bottleneck for real-time implementation on mobile devices. There have been several attempts to reduce the computational cost of this integration step, such as using more efficient radiance representations [13, 40, 28, 17, 5, 10] or distilling meshes from radiance field [34, 6, 39, 35, 27, 29]. Among these approaches, only a handful of mesh-based methods [6, 29] have demonstrated real-time rendering capabilities on mobile phones, but with a significant sacrifice in rendering fidelity. Moreover, all aforementioned methods require significant storage space (over \(200\)MB), which is undesirable for mobile devices with limited onboard storage.

Alternatively, researchers have used 4D light field1 (or lumigraph) to represent radiance along rays in empty space [11, 24, 12, 19], rather than attempting to model the 5D plenoptic function as in NeRF-based approaches. Essentially, the light field provides a direct mapping from rays to pixel values since the radiance is constant along rays in empty space. This makes the light field suitable for view synthesis, as long as the cameras are placed outside the convex hull of the object of interest. Compared to integrating radiance fields, rendering with light fields is more computationally efficient. However, designing a representation of light field that compresses its storage while maintaining high view-interpolation fidelity remains challenging. Previous methods, such as image quilts [38] or multiplane images (MPI) [41, 16, 32, 9], suffer from poor trade-offs between fidelity and storage due to the high number of views or image planes required for reconstructing the complex light field signal. Recent works [36, 4, 2, 31] have proposed training neural networks to represent light fields, achieving realistic rendering with a relatively small memory footprint. Among those, MobileR2L [4] uses less than 10MB of storage per scene, and it is currently the only method that demonstrates real-time performance on mobile phones.

Footnote 1: For the rest of the paper, we will use the term ‘light field’ to refer to the 4D light field, without explicitly stating the dimensionality.

However, prior neural light field (NeLF) representations, including MobileR2L, suffer from inefficiencies in learning due to the high number of layers (over \(60\) layers), and consequently, a long training time is required to capture fine scene details. One promising strategy to address this issue is utilizing grid-based representations, which have proven to be effective in the context of training NeRFs [30, 25, 17, 10]. Nonetheless, incorporating such grid-based representation directly to prior NeLFs is problematic due to the chosen ray parameterization. R2L [36] and MobileR2L [4] parameterize light rays using a large number of stratified 3D points along the rays, which were initially motivated by the discrete formulation of integrating radiance. However, this motivation is unnecessary and undermines the simplicity of 4D light fields because stratified sampling is redundant for rays with constant radiance. This becomes problematic when attempting to incorporate grid-based representations for more efficient learning, as the high-dimensional stratified-point representation is not feasible for grid-based discretization. Similarly, the \(6\)-dimensional Plucker coordinate used by Sitzmann _et al_. [31] also presents issues for discretization due to the fact that Plucker coordinates exist in a projective \(5\)-space, rather than Euclidean space.

In this paper, we present _LightSpeed_, the first NeLF method designed for mobile devices that uses a grid-based representation. As shown in Fig. 1, our method achieves a significantly better trade-off between rendering quality and speed compared to prior NeLF methods, while also being faster to train. These advantages make it well-suited for real-time applications on mobile devices. To achieve these results, we propose the following design choices:

**First**, we revisit the classic 4D light-slab (or two-plane) representation [12, 19] that has been largely overlooked by previous NeLF methods. This lower-dimensional parameterization allows us to compactly represent the rays and efficiently represent the light field using grids. To our knowledge,

Figure 1: Our LightSpeed approach demonstrates a superior trade-off between on-device rendering quality and latency while maintaining a significantly reduced training time and boosted rendering quality. **(a)** rendering quality and latency on the \(400\times 400\) Lego scene [23] running on an iPhone 13. **(b)** training curves for the \(756\times 1008\) Fern scene [22].

Attal _et al_. [2] is the only other NeLF method that has experimented with the light-slab representation. However, they did not take advantage of the grid-based representation, and their method is not designed for real-time rendering. **Second**, to address the heavy storage consumption of 4D light field grids, we take inspiration from k-planes [10] and propose decomposing the 4D grids into six 2D feature grids. This ensures that our method remains competitive for storage consumption compared to prior NeLF methods. **Third**, we apply the super-resolution network proposed by MobileR2L [4], which significantly reduces the computational cost when rendering high-resolution images. **Finally**, the light-slab representation was originally designed for frontal-view scenes, but we demonstrate that it can be extended to represent non-frontal scenes using a divide-and-conquer strategy.

Our contributions pave the way for efficient and scalable light field representation and synthesis, making it feasible to generate high-quality images of real-world objects and scenes. Our method achieves the highest PSNR and among the highest frame rates (\(55\) FPS on iPhone 14) on LLFF (frontal-view), Blender (\(360^{\circ}\)), and unbounded \(360^{\circ}\) scenes, proving the effectiveness of our approach.

## 2 Related work

**Light Field.** Light field representations have been studied extensively in the computer graphics and computer vision communities [38]. Traditionally, light fields have been represented using the 4D light slab representation, which parameterizes the light field by two planes in 4D space [12; 19]. More recently, neural-based approaches have been developed to synthesize novel views from the light field, leading to new light field representations being proposed.

One popular representation is the multi-plane image (MPI) representation, which discretizes the light field into a set of 2D planes. The MPI representation has been used in several recent works, including [41; 16; 32; 9; 7]. However, the MPI representation can require a large amount of memory, especially for high-resolution light fields. Another recent approach that has gained substantial attention is NeRF [23] (Neural Radiance Fields), which can synthesize novel views with high accuracy, but is computationally expensive to render and train due to the need to integrate radiance along viewing rays. There has been a substantial amount of works [37; 26; 28; 21; 13; 40; 28; 17; 5; 10; 34; 6; 39; 35; 27; 29; 36; 4; 2; 31] studying how to accelerate training and rendering of NeRF, but in the following, we focus on recent methods that achieve real-time rendering with or without mobile devices.

**Grid Representation of Radiance Field.** The first group of methods trade speed with space, by precomputing and caching radiance values using grid or voxel-like data structures such as sparse voxels [30; 13], octrees [40], and hash tables [25]. Despite the efficient data structures, the memory consumption for these methods is still high, and several approaches have been proposed to address this issue. First, Chen _et al_. [5] and Fridovich-Keil _et al_. [10] decompose voxels into matrices that are cheaper to store. Takikawa _et al_. [33] performs quantization to compress feature grids. These approaches have enabled real-time applications on desktop or server-class GPUs, but they still require significant computational resources and are not suitable for resource-constrained devices such as mobile or edge devices.

**Baking High Resolution Mesh.** Another group of methods adopts the approach of extracting high-resolution meshes from the learned radiance field [6; 29; 35]. The texture of the mesh stores the plenoptic function to account for view-dependent rendering. While these approaches have been demonstrated to run in real-time on mobile devices, they sacrifice rendering quality, especially for semi-transparent objects, due to the mesh-based representation. Additionally, storing high-resolution meshes with features is memory-intensive, which limits the resolution and complexity of the mesh that can be used for rendering.

**Neural Light Fields.** Recent works such as R2L [36], LFNS [31] and NeuLF [20] have framed the view-synthesis problem as directly predicting pixel colors from camera rays, making these approaches fast at inference time without the need for multiple network passes to generate a pixel color. However, due to the complexity of the 4D light field signal, the light field network requires sufficient expressibility to be able to memorize the signal. As a result, Wang _et al_. [36] end up using as many as 88 network layers, which takes three seconds to render one 200 x 200 image on iPhone 13. In this regard, Cao _et al_. [4] introduce a novel network architecture that dramatically reduces R2L's computation through super-resolution. The deep networks are only evaluated on a low-resolution ray bundle and then upsampled to the full image resolution. This approach, termed MobileR2L, achieves real-time rendering on mobile phones. NeuLF [20] also proposes to directly regress pixel colors using a light slab ray representation but is unable to capture fine-level details due to lack of any sort of high-dimensional input encoding and is limited to frontal scenes. Another notable work, SIGNET [8], utilizes neural methods to compress a light field by using a ultra spherical input encoding to the light slab representation. However, SIGNET doesn't guarantee photorealistic reconstruction and hence deviates from task at hand. Throughout the paper, we will mainly compare our method to MobileR2L [4], which is currently the state-of-the-art method for real-time rendering on mobile devices and achieves the highest PSNR among existing methods.

It is important to note that training NeLF's requires densely sampled camera poses in the training images and may not generalize well if the training images are sparse, as NeLF's do not explicitly model geometry. While there have been works, such as those by Attal _et al_. [2], that propose a mixture of NeRF and local NeLF's, allowing learning from sparse inputs, we do not consider this to be a drawback since NeLF's focus on photo-realistic rendering rather than reconstructing the light field from sparse inputs, and they can leverage state-of-the-art reconstruction methods like NeRF to create dense training images. However, it is a drawback for prior NeLF's [36; 4] that they train extremely slowly, often taking more than two days to converge for a single scene. This is where our new method comes into play, as it offers improvements in terms of training efficiency and convergence speed.

## 3 Methodology

### Prerequisites

**4D Light Fields** or Lumigraphs are a representation of light fields that capture the radiance information along rays in empty space. They can be seen as a reduction of the higher-dimensional plenoptic functions. While plenoptic functions describe the amount of light (radiance) flowing in every direction through every point in space, which typically has five degrees of freedom, 4D light fields assume that the radiance is constant along the rays. Therefore, a 4D light field is a vector function that takes a ray as input (with four degrees of freedom) and outputs the corresponding radiance value. Specifically, assuming that the radiance \(\mathbf{c}\) is represented in the RGB space, a 4D light field is mathematical defined as a function, _i.e._:

\[\mathcal{F}:\mathbf{r}\in\mathbb{R}^{M}\mapsto\mathbf{c}\in\mathbb{R}^{3},\] (1)

where \(\mathbf{r}\) is \(M\)-dimensional coordinates of the ray depending how it is parameterized.

Generating images from the 4D light field is a straightforward process. For each pixel on the image plane, we calculate the corresponding viewing ray \(\mathbf{r}\) that passes through the pixel, and the pixel value is obtained by evaluating the light field function \(\mathcal{F}(\mathbf{r})\). In this paper, our goal is to identify a suitable representation for \(\mathcal{F}(\mathbf{r})\) that minimizes the number of parameters required for learning and facilitates faster evaluation and training.

**MobileR2L.** We adopt the problem setup introduced by MobileR2L [6] and its predecessor R2L [36], where the light field \(\mathcal{F}(\mathbf{r})\) is modeled using neural networks. The training of the light field network is framed as distillation, leveraging a large dataset that includes both real images and images generated by a pre-trained NeRF. Both R2L and MobileR2L represent \(\mathbf{r}\) using stratified points, which involves concatenating the 3D positions of points along the ray through stratified sampling. In addition, the 3D positions are encoded using sinusoidal positional encoding [23]. Due to the complexity of the light field, the network requires a high level of expressiveness to capture fine details in the target scene. This leads to the use of very deep networks, with over 88 layers in the case of R2L. While this allows for detailed rendering, it negatively impacts the rendering speed since the network needs to be evaluated for every pixel in the image.

To address this issue, MobileR2L proposes an alternative approach. Instead of directly using deep networks to generate high-resolution pixels, they employ deep networks to generate a low-resolution feature map, which is subsequently up-sampled to obtain high-resolution images using shallow super-resolution modules. This approach greatly reduces the computational requirements and enables real-time rendering on mobile devices. In our work, we adopt a similar architecture, with a specific focus on improving the efficiency of generating the low-resolution feature map.

### LightSpeed

We first describe the light-slab ray representation for both frontal and non-frontal scenes in Sec. 3.2.1. Next, we detail our grid representation for the light-slab in Sec. 3.2.2 and explain the procedure for synthesizing images from this grid representation in Sec. 3.3. Refer to Fig. 2 for a visual overview.

#### 3.2.1 Ray Parameterization

**Light Slab (two-plane representation).** Instead of utilizing stratified points or Plucker coordinates, we represent each directed light ray using the classic two-plane parameterization[19] as an ordered pair of intersection points with two fixed planes. Formally,

\[\textbf{r}=(x,y,u,v),\] (2)

where \((x,y)\in\mathbb{R}^{2}\) and \((u,v)\in\mathbb{R}^{2}\) are ray intersection points with fixed planes \(P_{1}\) and \(P_{2}\) in their respective coordinate systems. We refer to these four numbers as the ray coordinates in the 4D ray space. To accommodate unbounded scenes, we utilize normalized device coordinates (NDC) and select the planes \(P_{1}\) and \(P_{2}\) as the near and far planes (at infinity) defined in NDC.

Divided Light Slabs for Non-frontal Scenes.A single light slab is only suitable for modeling a frontal scene and cannot capture light rays that are parallel to the planes. To model non-frontal scenes, we employ a divide-and-conquer strategy by using a composition of multiple light slab representations to learn the full light field. We partition the light fields into subsets, and each subset is learned using a separate NeLF model. The partitions ensure sufficient overlap between sub-scenes, resulting in a continuous light field representation without additional losses while maintaining the frontal scene assumption. To perform view synthesis, we identify the scene subset of the viewing ray and query the corresponding NeLF to generate pixel values. Unlike Attal _et al_. [2], we do not perform alpha blending of multiple local light fields because our division is based on ray space rather than partitioning 3D space.

For _object-centric_\(360^{\circ}\) scenes, we propose to partition the scene into \(5\) parts using surfaces of a near-isometric trapezoidal prism and approximate each sub-scene as frontal (as illustrated in Fig. 3). For _unbounded_\(360^{\circ}\) scenes, we perform partitioning using k-means clustering based on camera orientation and position. We refer the reader to the supplementary material for more details on our choice of space partitioning.

#### 3.2.2 Feature Grids for Light Field Representation

Storing the 4D light-slab directly using a high-resolution grid is impractical in terms of storage and inefficient for learning due to the excessive number of parameters to optimize. The primary concern arises from the fact that the 4D grid size increases quartically with respect to resolutions. To address this, we suggest the following design choices to achieve a compact representation of the light-slab without exponentially increasing the parameter count.

Figure 2: **LightSpeed Model for Frontal Scenes. Taking a low-resolution ray bundle as input, our approach formulates rays in two-plane ray representation. This enables us to encode each ray using multi-scale feature grids, as shown. The encoded ray bundle is fed into a decoder network consisting of convolutions and super-resolution modules yielding the high-resolution image.**

**Lower Resolution Feature Grids.** Instead of storing grids at full resolution, we choose to utilize low-resolution feature grids to take advantage of the quartic reduction in storage achieved through resolution reduction. We anticipate that the decrease in resolution can be compensated by employing high-dimensional features. In our implementation, we have determined that feature grids of size \(128^{4}\) are suitable for synthesizing full HD images. Additionally, we adopt the approach from Instant-NGP [25] to incorporate multi-resolution grids, which enables an efficient representation of both global and local scene structures.

**Decompose 4D Grids into 2D Grids.** Taking inspiration from k-planes [10], we propose to decompose the 4D feature grid using \(\binom{4}{2}=6\) number of 2D grids, with each 2D grid representing a sub-space of the 4D ray space. This results in a storage complexity of \(\mathcal{O}(6N^{2})\), greatly reducing the storage required to deploy our grid-based approach to mobile devices.

### View Synthesis using Feature Grids

Similar to MobileR2L [4], LightSpeed takes two steps to render a high resolution image (see Fig. 2).

**Encoding Low-Resolution Ray Bundles.** The first step is to render a low-resolution (\(H_{L}\times W_{L}\)) feature map from the feature grids. This is accomplished by generating ray bundles at a reduced resolution, where each ray corresponds to a pixel in a downsampled image. We project each ray's 4D coordinates \(\mathbf{r}=(x,y,u,v)\) onto 6 2D feature grids \(\mathbf{G}_{xy},\mathbf{G}_{xu},\mathbf{G}_{xv},\mathbf{G}_{yu},\mathbf{G}_{ yv},\mathbf{G}_{uv}\) to obtain feature vectors from corresponding sub-spaces. The feature values undergo bilinear interpolation from the 2D grids, resulting in six interpolated \(F\)-dimensional features. These features are subsequently concatenated to form a \(6F\)-dimensional feature vector. As the feature grids are multi-resolutional with \(L\) levels, features \(g_{l}(\mathbf{r})\in\mathbb{R}^{6F}\) from different levels (indexed by \(l\)) are concatenated together to create a single feature \(g(\mathbf{r})\in\mathbb{R}^{6LF}\). Combining the features from all rays generates a low-resolution 2D feature map \(\tilde{\mathbf{G}}\in\mathbb{R}^{H_{L}\times W_{L}\times 6LF}\), which is then processed further in the subsequent step.

**Decoding High-Resolution Image.** To mitigate the approximation introduced by decomposing 4D grids into 2D grids, the features \(g(\mathbf{r})\) undergo additional processing through a MLP. This is implemented by applying a series of \(1\times 1\) convolutional layers to the low-resolution feature map. Subsequently, the processed feature map is passed through a sequence of upsampling layers (similar to MobileR2L [4]) to generate a high-resolution image.

## 4 Experiments

**Datasets.** We benchmark our approach on the real-world forward-facing [22][23], the realistic synthetic \(360^{\circ}\) datasets [23] and unbounded \(360^{\circ}\) scenes [3]. The forward-facing dataset consists of \(8\) real-world scenes captured using cellphones, with \(20\)-\(60\) images per scene and 1/8th of the images used for testing. The synthetic \(360^{\circ}\) dataset has \(8\) scenes, each having \(100\) training views and \(200\) testing views. The unbounded \(360^{\circ}\) dataset consists of \(5\) outdoor and \(4\) indoor scenes with a central object and a detailed background. Each scene has between \(100\) to \(300\) images, with \(1\) in \(8\) images used for testing. We use \(756\times 1008\) LLFF dataset images, \(800\times 800\) resolution for the \(360^{\circ}\) scenes, and 1/4th of the original resolution for the unbounded \(360^{\circ}\) scenes.

Figure 3: **Space Partitioning for Non-frontal scenes.** We partition _object-centric_\(360^{\circ}\) scenes into 5 parts as shown. Each colored face of the trapezoidal prism corresponds to a partitioning plane. Each scene subset is subsequently learned as a separate NeLF

**Training Details.** We follow a similar training scheme as MobileR2L: train the LightSpeed model using pseudo-data mined from a pre-trained NeRF teacher. We specifically train MipNeRF teachers to sample \(10\)k pseudo-data points for the LLFF dataset. For synthetic and unbounded \(360^{\circ}\) scenes, we mine \(30\)k samples per scene using Instant-NGP [25] teachers. Following this, we fine-tune the model on the original data. We optimize for the mean-squared error between generated and ground truth images. We refer the reader to the supplementary material for more training details.

We use \(63\times 84\) (\(12\times\) downsampled from the desired \(756\times 1008\) resolution) input ray bundles for the forward-facing scenes. For \(360^{\circ}\) scenes, we use \(100\times 100\) (\(8\times\) downsampled from the desired \(800\times 800\) image resolution) ray bundles. For unbounded scenes, we use ray bundles \(12\times\) downsampled from the image resolution we use. We train our frontal LightSpeed models as well as each sub-scene model in non-frontal scenes for \(200\)k iterations.

**Baselines and Metrics.** We compare our method's performance on bounded scenes with MobileR2L[6], MobileNeRF[6] and SNeRG[13]. We evaluate our method for rendering quality using three metrics: PSNR, LPIPS, and SSIM. For unbounded scenes, we report the PSNR metric on 6 scenes and compare it with MobileNeRF [6] and NeRFMeshing [27]. To further demonstrate the effectiveness of our approach, we compare our approach with others on two other criteria: (a) **On-device Rendering Speed**: We report and compare average inference times per rendered frame on various mobile chips, including Apple A15, Apple M1 Pro and Snapdragon SM8450 chips; and (b) **Efficient Training**: We compare the number of iterations LightSpeed and MobileR2L require to reach a target PSNR. We pick Lego scene from \(360^{\circ}\) scenes and Fern from forward-facing scenes as representative scenes to compare. We also report the storage requirements of our method per frontal scene and compare it with baselines.

### Results and Analysis

**Rendering Quality.** As in Tab. 1, we obtain better results on all rendering fidelity metrics on the two bounded datasets. We also outperform MobileNeRF and NeRFMeshing on 4 out of 6 unbounded \(360^{\circ}\) scenes. We refer the reader to Fig. 4 for a visual comparison of our approach with MobileR2L and NeRF. Our method has much better rendering quality, capturing fine-level details where MobileR2L, and in some cases, even the original NeRF model, fails. Note that we use Instant-NGP teachers for \(360^{\circ}\) scenes, which have slightly inferior performance to MipNeRF teachers used by MobileR2L. This further shows the robustness of our approach to inferior NeRF teachers.

**Storage Cost.** We report storage requirements in Tab. 1. Our approach has a competitive on-device storage to the MobileR2L model. Specifically, we require a total of \(16.3\) MB of storage per frontal scene. The increase in storage is expected since we're using grids to encode our light field. We also report storage values for lighter LightSpeed networks in the ablation study (see Tab. 5), all of which have similar or better rendering quality than the full-sized MobileR2L network.

**Training Speed.** We benchmark the training times and the number of iterations required for LightSpeed and MobileR2L in Tab. 2 with a target PSNR of \(24\) for Fern scene and \(32\) for the Lego scene. Our approach demonstrates a training speed-up of \(2.5\times\) on both scenes. Since we are modeling \(360^{\circ}\) scenes as a composition of \(5\) light fields, we can train them in parallel (which is not

Figure 4: **Qualitative Results on frontal and non-frontal scenes. Zoomed-in comparison between NeRF [23], MobileR2L [4] and our LightSpeed approach.**possible for MobileR2L), further trimming down the training time. Moreover, the training speedup reaches \(\sim 4\times\) when networks are trained beyond the mentioned target PSNR (see Fig. 1).

**Inference Speed.** Tab. 3 shows our method's inference time as compared to MobileR2L and MobileNeRF. We maintain a comparable runtime as MobileR2L while having better rendering fidelity. Since on-device inference is crucial to our problem setting, we also report rendering times of a smaller 30-layered decoder network that has similar rendering quality as the MobileR2L model (see Tab. 5).

### Ablations

**Data Requirements.** We use \(10\)k samples as used by MobileR2L to train LightField models for frontal scenes. However, for non-frontal scenes, we resort to using \(30\)k pseudo-data samples per

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c}{Synthetic \(360^{\circ}\)} & \multicolumn{3}{c}{Forward-Facing} \\ \cline{2-9}  & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & Storage \(\downarrow\) \\ \hline NeRF [23] & 31.01 & 0.947 & 0.081 & 26.50 & 0.811 & 0.250 & - \\ NeRF-PyTorch & 30.92 & 0.991 & 0.045 & 26.26 & 0.965 & 0.153 & - \\ \hline SNeRG [13] & 30.38 & 0.950 & 0.050 & 25.63 & 0.818 & 0.183 & 337.3 MB \\ MobileNeRF [6] & 30.90 & 0.947 & 0.062 & 25.91 & 0.825 & 0.183 & 201.5 MB \\ MobileR2L [4] & 31.34 & 0.993 & 0.051 & 26.15 & 0.966 & 0.187 & **8.2 MB** \\ \hline LightSpeed (Ours) & **32.23** & **0.994** & **0.038** & **26.50** & **0.968** & **0.173** & 16.3 MB \\ \hline Our Teacher & 32.96 & - & - & 26.85 & 0.827 & 0.226 & - \\ \hline \hline \multirow{4}{*}{Method} & \multicolumn{3}{c}{Unbounded \(360^{\circ}\)} \\ \cline{2-9}  & Bicycle & Garden & Stump & Bonsai & Counter & Kitchen \\ \hline MobileNeRF [6] & 21.70 & 23.54 & **23.95** & - & - & - \\ NeRFMeshing [27] & 21.15 & 22.91 & 22.66 & 25.58 & 20.00 & 23.59 \\ \hline LightSpeed (Ours) & **22.51** & **24.54** & 22.22 & **28.24** & 25.46 & **27.82** \\ \hline Instant-NGP (Our teacher) [25] & 21.70 & 23.40 & 23.20 & 27.4 & **25.80** & 27.50 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Quantitative Comparison** on Forward-facing, Synthetic \(360^{\circ}\) and Unbounded \(360^{\circ}\) Datasets. HighSpeed achieves the best rendering quality with competitive storage. We use an out-of-the-box Instant-NGP [25] implementation [1] (as teachers for \(360^{\circ}\) scenes) which dose not report SSIM and LPIPS values. We omit storage for NeRF-based methods since they are not comparable.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{2}{c}{Forward-Facing: Fern} & \multicolumn{2}{c}{Synthetic \(360^{\circ}\): Lego} \\ \cline{2-5} Method & Duration \(\downarrow\) & Iterations \(\downarrow\) & Duration \(\downarrow\) & Iterations \(\downarrow\) \\ \hline MobileR2L & 12.5 hours & 70k & 192 hours & 860k \\ LightSpeed & **4 hours** & **27k** & **75 hours** & **425k** \\ LightSpeed (Parallelized) & - & - & **15 hours** & **85k** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Training Time** for Lego and Fern scenes with 32 and 24 target PSNRs. LightSpeed trains significantly faster than MobileR2L. It achieves even greater speedup when trained in parallel for \(360^{\circ}\) scenes (parallel training is not applicable for frontal scenes).

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Forward-Facing} & \multicolumn{3}{c}{Synthetic \(360^{\circ}\)} \\ \cline{2-9}  & MobileNeRF & MobileR2L & Ours & Ours (30-L) & MobileNeRF & MobileR2L & Ours & Ours (30-L) \\ \hline Apple A13 (Low-end) & - & 40.23 & 41.06 & 32.29 & - & 65.54 & 66.10 & 53.89 \\ Apple A15(Low-end) & 27.15 \(\frac{2}{5}\) & 18.04 & 19.05 & 15.28 & 17.54 & 26.21 & 27.10 & 20.15 \\ Apple A15(High-end) & 20.98 \(\frac{8}{5}\) & 16.48 & 17.68 & 15.03 & 16.67 & 22.65 & 26.47 & 20.35 \\ Apple M1 Pro & - & 17.65 & 17.08 & 13.86 & - & 27.37 & 27.14 & 20.13 \\ Snapdragon SM8450 & - & 39.14 & 45.65 & 32.89 & - & 40.86 & 41.26 & 33.87 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Rendering Latency Analysis.** LightSpeed maintains a competitive rendering latency (ms) to prior works. MobileNeRF is not able to render \(2\) out of \(8\) real-world scenes (\(\frac{N}{M}\) in table) due to memory constraints, and no numbers are reported for A13, M1 Pro and Snapdragon chips.

scene. Dividing \(10\)k samples amongst \(5\) sub-scenes assigns too few samplers per sub-scene, which is detrimental to grid learning. We experimentally validate data requirements by comparing MobileR2L and LightSpeed trained for different amounts of pseudo-data. We train one \(400\times 400\) sub-scene from the Lego scene for 200k iterations with 1/5th of \(10\)k and \(30\)k samples, _i.e._, \(2\)k and \(6\)k samples. Tab. 4 exhibits significantly decreased rendering quality for the LightSpeed network as compared to MobileR2L when provided with less pseudo-data.

**Decoder Network Size.** We further analyze the trade-off between inference speed and rendering quality of our method and MobileR2L. To this end, we experiment with decoders of different depths and widths. Each network is trained for \(200\)k iterations and benchmarked on an iPhone 13. Tab. 5 shows that a \(30\)-layered LightSpeed model has a better inference speed and rendering quality as compared to the \(60\)-layered MobileR2L model. This \(30\)-layered variant further occupies less storage as compared to its full-sized counterpart. Furthermore, lighter LightSpeed networks obtain a comparable performance as the \(60\)-layered MobileR2L. Note that reducing the network capacity of MobileR2L results in significant drops in performance. This means that we can get the same rendering quality as MobileR2L with considerably reduced on-device resources, paving the way for a much better trade-off between rendering quality and on-device inference speed.

**Ray-Space Grid Encoding.** We provide an ablation in Tab. 6 below on how the proposed ray-space grid encoder helps as compared to just using the light-slab representation with a traditional frequency encoder. We compare different LightSpeed configurations with grid-encoder and frequency encoders. Networks are trained for 200k iterations on a full-resolution 800\(\times\)800 Lego sub-scene from Synthetic

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & \multicolumn{3}{c}{2k Samples} & \multicolumn{3}{c}{6k Samples} \\ \cline{2-6} Method & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline MobileR2L & 30.19 & 0.9894 & 0.0354 & 30.56 & 0.9898 & 0.0336 \\ LightSpeed (Ours) & 30.44 & 0.9899 & 0.0299 & **31.2** & **0.9906** & **0.0284** \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Pseudo-Data Requirement for Non-Frontal Scenes.** We analyze the importance of mining more pseudo-data for non-frontal scenes. Using 1/5th of \(10\)k and \(30\)k sampled pseudo-data points, we find more pseudo-data is crucial for the boosted performance of the LightSpeed model.

Figure 5: **Test PSNR v/s Training Iterations.** We compare test set PSNR obtained by LightSpeed (Grid)(ours), LightSpeed (frequency encoded), and Plücker-based neural light field as the training progresses for 3 different network configurations.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Method & PSNR \(\uparrow\) & Latency \(\downarrow\) & Storage \(\downarrow\) & FLOPs \(\downarrow\) \\ \hline
15-L W-256 MobileR2L & 27.69 & 14.54 ms & 2.4 MB & 12626M \\
30-L W-128 MobileR2L & 27.54 & 14.47 ms & 1.4 MB & 8950M \\
30-L W-256 MobileR2L & 29.21 & 18.59 ms & 4.5 MB & 23112M \\
60-L W-256 MobileR2L & 30.34 & 22.65 ms & 8.2 MB & 42772M \\ \hline
15-L W-256 LightSpeed & 30.37 & 14.94 ms & 10.5 MB & 12833M \\
30-L W-128 LightSpeed & 30.13 & 14.86 ms & 9.5 MB & 9065M \\
30-L W-256 LightSpeed & 31.70 & 20.35 ms & 12.6 MB & 23319M \\
60-L W-256 LightSpeed & 32.34 & 26.47 ms & 16.3 MB & 42980M \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Decoder Network Size.** Our approach maintains a much better tradeoff between inference speeds v/s rendering quality, with our smallest network achieving comparable quality to the MobileR2L. Benchmarking done on an iPhone 13. L is network depth, and W is network width.

\(360^{\circ}\) dataset. Further, we show the training dynamics of all the trained variants in Fig. 5 (red and green plots). As claimed, our approach offers better visual fidelity and training dynamics (iterations to reach a target PSNR) for both computationally cheaper small networks as well as full sized networks.

Comparison with Plucker Representation.Given the challenges of discretizing Plucker representation, we compare between using positionally encoded Plucker coordinates and our grid-based light-slab approach in Tab. 7 below for different network sizes to demonstrate the effectiveness of our approach. We train all models for 200k iterations on one 800\(\times\)800 Lego sub-scene. We also share training curves for the variants in question in Fig. 5 (red and blue curves). As claimed, our integrated approach performs better in terms of training time and test-time visual fidelity for large and small models (having less computational costs) alike whereas the Plucker-based network shows a sharp decline in visual fidelity and increased training times to reach a target test PSNR as network size is reduced.

## 5 Discussion and Conclusion

In this paper, we propose an efficient method, LightSpeed, to learn neural light fields using the classic two-plane ray representation. Our approach leverages grid-based light field representations to accelerate light field training and boost rendering quality. We demonstrate the advantages of our approach not only on frontal scenes but also on non-frontal scenes by following a divide-and-conquer strategy and modeling them as frontal sub-scenes. Our method achieves SOTA rendering quality amongst prior works at same time providing a significantly better trade-off between rendering fidelity and latency, paving the way for real-time view synthesis on resource-constrained mobile devices.

**Limitations.** While LightSpeed excels at efficiently modeling frontal and \(360^{\circ}\) light fields, it currently lacks the capability to handle free camera trajectories. The current implementation does not support refocusing, anti-aliasing, and is limited to static scenes without the ability to model deformable objects such as humans. We plan to explore these directions in future work.

**Broader Impact.** Focused on finding efficiencies in novel view synthesis, our study could significantly reduce costs, enabling wider access to this technology. However, potential misuse, like unsolicited impersonations, must be mitigated.

\begin{table}
\begin{tabular}{l l} \hline \hline Method & PSNR \(\uparrow\) \\ \hline
15-L W-256 LS (PE) & 28.84 \\
30-L W-256 LS (PE) & 30.63 \\
60-L W-256 LS (PE) & 32.16 \\ \hline
15-L W-256 LS (Grid) & 30.37 \\
30-L W-256 LS (Grid) & 31.70 \\
60-L W-256 LS (Grid) & 32.34 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Effect of using a Ray-Space Grid Encoder.** We demonstrate the effect of using a grid-based LightSpeed by comparing with a frequency encoded variant (no grid). L is network depth, and W is network width.

\begin{table}
\begin{tabular}{l l} \hline \hline Method & PSNR \(\uparrow\) \\ \hline
15-L W-256 Plücker & 28.65 \\
30-L W-256 Plücker & 30.84 \\
60-L W-256 Plücker & 32.14 \\ \hline
15-L W-256 LS & 30.37 \\
30-L W-256 LS & 31.70 \\
60-L W-256 LS & 32.34 \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Light-Slab Grid Representation vs. Plücker Coordinates.** We compare the light-slab based LightSpeed (LS) with a positionally encoded variant of the Plücker ray representation. L is network depth, and W is network width.

## References

* [1]N. K. Kalantari, T. Wang, and R. Ramamoorthi (2016) Learning-based view synthesis for light field cameras. ACM Transactions on Graphics (TOG)35 (6), pp. 1-10. Cited by: SS1.

[MISSING_PAGE_POST]

* [22] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. _ACM Transactions on Graphics (TOG)_, 2019.
* [23] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.
* [24] Parry Moon and Domina Eberle Spencer. Theory of the photic field. _Journal of the Franklin Institute_, 255(1):33-50, 1953.
* [25] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Trans. Graph._, 41(4):102:1-102:15, July 2022. doi: 10.1145/3528223.3530127. URL https://doi.org/10.1145/3528223.3530127.
* [26] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H Mueller, Chakravarty R Alla Chaitanya, Anton Kaplanyan, and Markus Steinberger. Donerf: Towards real-time rendering of compact neural radiance fields using depth oracle networks. In _Computer Graphics Forum_, volume 40, pages 45-59. Wiley Online Library, 2021.
* [27] Marie-Julie Rakotosaona, Fabian Manhardt, Diego Martin Arroyo, Michael Niemeyer, Abhijit Kundu, and Federico Tombari. Nerfmeshing: Distilling neural radiance fields into geometrically-accurate 3d meshes. _arXiv preprint arXiv:2303.09431_, 2023.
* [28] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 14335-14345, 2021.
* [29] Sara Rojas, Jesus Zarzar, Juan Camilo Perez, Artsiom Sanakoyeu, Ali Thabet, Albert Pumarola, and Bernard Ghanem. Re-rend: Real-time rendering of nerfs across devices. _arXiv preprint arXiv:2303.08717_, 2023.
* [30] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In _CVPR_, 2022.
* [31] Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. In _Proc. NeurIPS_, 2021.
* [32] Pratul P Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi Ramamoorthi, and Ren Ng. Learning to synthesize a 4d rgbd light field from a single image. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 2243-2251, 2017.
* [33] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas Muller, Morgan McGuire, Alec Jacobson, and Sanja Fidler. Variable bitrate neural fields. In _ACM SIGGRAPH 2022 Conference Proceedings_, pages 1-9, 2022.
* [34] Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Errui Ding, Jingdong Wang, and Gang Zeng. Delicate textured mesh recovery from nerf via adaptive surface refinement. _arXiv preprint arXiv:2303.02091_, 2022.
* [35] Ziyu Wan, Christian Richardt, Aljaz Bozic, Chao Li, Vijay Rengarajan, Seonghyeon Nam, Xiaoyu Xiang, Tuotuo Li, Bo Zhu, Rakesh Ranjan, et al. Learning neural duplex radiance fields for real-time view synthesis. _arXiv preprint arXiv:2304.10537_, 2023.
* [36] Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, and Sergey Tulyakov. R2l: Distilling neural radiance field to neural light field for efficient novel view synthesis. In _ECCV_, 2022.
* [37] Peng Wang, Yuan Liu, Guying Lin, Jiatao Gu, Lingjie Liu, Taku Komura, and Wenping Wang. Progressively-connected light field network for efficient view synthesis. _arXiv preprint arXiv:2207.04465_, 2022.
* [38] Gaochang Wu, Belen Masia, Adrian Jarabo, Yuchen Zhang, Liangyong Wang, Qionghai Dai, Tianyou Chai, and Yebin Liu. Light field image processing: An overview. _IEEE Journal of Selected Topics in Signal Processing_, 11(7):926-954, 2017.

* [39] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron, and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-time view synthesis. _arXiv preprint arXiv:2302.14859_, 2023.
* [40] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for real-time rendering of neural radiance fields. In _ICCV_, 2021.
* [41] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. In _SIGGRAPH_, 2018.

Training Details

Network architecture.Our multi-scale feature grids have \(16\) levels, with resolutions exponentially growing from \(16\) to \(256\), and 4-D features in every grid. Our LightSpeed network follows a similar architecture to MobileR2L: \(60\) point-wise residual convolutions with \(256\) channels and BatchNorm [15]and GeLU [14] activation interleaved. The convolutions are followed by \(3\) super-resolution modules to upsample the low-resolution input to the desired resolution. The first two super-resolution modules upsample the input by \(2\times\) and consist of transposed convolution layers with \(4\times 4\) kernel size followed by \(2\) residual convolution layers each. The third super-resolution module consists of transposed kernel size with \(4\times 4\) kernel size (upsample by \(2\times\)) for \(360^{\circ}\) scenes (both bounded and unbounded) and \(3\times 3\) kernel size (upsample by \(3\times\)) for forward-facing [22] scenes.

Training details.We use Adam [18] optimizer with a batch size of \(32\) to train the feature grids and decoder network. We use an initial learning rate of 1e-5 with \(100\) warmup steps taking the learning rate to 5e-4. Beyond that, the learning rate decays linearly until the training finishes. All our experiments are conducted on Nvidia V100s and A100s.

## Appendix B More Ablation Analysis

Choice of Splitting Planes.We discuss two aspects of dividing non-frontal scenes into separate light fields: the number of parts to divide the scene into and the placement of the splitting planes. We find the optimal number of splits for \(360^{\circ}\) scenes to be \(5\) since more number of splits would mean increased storage cost, which is detrimental to mobile deployment. We also want the scene splits to be collectively exhaustive (but not mutually exclusive to maintain continuity while switching from one light field to another) in the poses sampled around the object. Consequently, fewer planes would mean placing the splitting planes near the scene origin to cover the entire scene, which starts to violate the frontal assumption for each sub-scene.

Given poses distributed on the surface of a sphere with radius \(r\), we propose assigning each pose to (possibly multiple) sub-scenes based on the camera origin satisfying one or more of the \(5\) following criteria:

\[\begin{bmatrix}0&0&\sqrt{2}\\ \sqrt{2}&0&\sqrt{2}-1\\ -\sqrt{2}&0&\sqrt{2}-1\\ 0&\sqrt{2}&\sqrt{2}-1\\ 0&-\sqrt{2}&\sqrt{2}-1\end{bmatrix}\begin{bmatrix}x\\ y\\ z\end{bmatrix}\geq\begin{bmatrix}r\\ r\\ r\\ r\end{bmatrix}\] (3)

These five hyperplanes form the surface of a near-isometric trapezoidal prism, as shown in Fig. 3 (main paper). We experimentally show the effect of the choice of splitting plane by training LightSpeed models on a Lego sub-scene with different plane placements and compare with the corresponding MobileR2L models trained on the same data. Specifically, we choose two axis-aligned planes at a distance of \(\frac{radius}{\sqrt{2}}\) and \(\frac{radius}{\sqrt{3}}\) from the scene origin and train models with 6k pseudo data points sampled independently from the two resulting sub-scenes. As shown in Tab. 8, placing the splitting plane at a distance of \(\frac{radius}{\sqrt{3}}\) results in inferior performance as compared to placing the splitting plane at a distance of \(\frac{radius}{\sqrt{2}}\) from the origin. This suggests that frontal sub-scene approximation starts to break down as we move the splitting plane closer to the origin.

\begin{table}
\begin{tabular}{l l l l} \hline \hline LF Representation & PSNR \(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) \\ \hline radius /\(\sqrt{2}\) & **30.44** & **0.9903** & **0.028** \\ radius /\(\sqrt{3}\) & 30.23 & 0.9899 & 0.031 \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Choice of Splitting Planes. We experiment with two planes parallel to the x-y sub-space at different distances. Splitting planes further from the origin work better empirically maintaining the frontal sub-scene assumption.**

[MISSING_PAGE_FAIL:15]

## Appendix D Limitations

**Results on Unbounded Scenes.** The rendering fidelity of LightSpeed is closely tied to the performance of the corresponding NeRF teacher. LightSpeed uses Instant NGP [25] teachers for both bounded and unbounded scenes to maintain experimental consistency. We would like to highlight that Instant-NGP introduces the artifacts to unbounded scenes, which are carried forward to LightSpeed via the mined pseudo-data. We share some of the pseudo-data images from Instant-NGP in Fig. 6. MipNeRF360 [3] specifically uses space contraction techniques to model the unbounded nature of the scene and deal with blurriness in the renderings. It further introduces a distortion-based regularizer to remove floater artifacts and prevent background collapse. The techniques introduced by MipNeRF360 tackle the same type of artifacts pointed out in Fig. 6. Hence, using MipNeRF360 teachers will mitigate both these issues and could boost the visual fidelity on unbounded scenes for LightSpeed.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Method & Room & Fern & Leaves & Fortress & Orchids & Flower & T-Rex & Hons & Average \\ \hline NeRF[23] & 0.178 & 0.280 & 0.316 & 0.171 & 0.321 & 0.219 & 0.249 & 0.268 & 0.250 \\ MobileR2L [4] & 0.088 & 0.239 & 0.280 & 0.103 & 0.296 & 0.150 & 0.121 & 0.217 & 0.187 \\ LightSpeed (Ours) & 0.085 & 0.211 & 0.255 & 0.093 & 0.272 & 0.145 & 0.119 & 0.209 & 0.173 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Per-scene LPIPS \(\downarrow\) comparison on the forward-facing dataset between NeRF [23], MobileR2L [4], and our approach.

Figure 6: **Instant NGP Failure Cases for Unbounded Scenes.** Such artifacts carry over to LightSpeed, affecting its visual fidelity on unbounded scenes.

Figure 7: **Qualitative Results on frontal and non-frontal scenes. Zoomed-in comparison between MobileR2L and our LightSpeed approach.**