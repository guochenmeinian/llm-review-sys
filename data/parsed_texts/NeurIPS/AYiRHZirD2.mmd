# A Unified Solution for Privacy and Communication Efficiency in Vertical Federated Learning

Ganyu Wang

Western University

gwang382@uwo.ca

Bin Gu

Jilin University and MBZUAI

jsgubin@gmail.com

Qingsong Zhang

Xidian University

qszhang1995@gmail.com

Xiang Li

Western University

lixiang41@126.com

&Boyu Wang

Western University

bwang@csd.uwo.ca

Charles X. Ling

Western University

charles.ling@uwo.ca

Co-corresponding authors

###### Abstract

Vertical Federated Learning (VFL) is a collaborative machine learning paradigm that enables multiple participants to jointly train a model on their private data without sharing it. To make VFL practical, privacy security and communication efficiency should both be satisfied. Recent research has shown that Zero-Order Optimization (ZOO) in VFL can effectively conceal the internal information of the model without adding costly privacy protective add-ons, making it a promising approach for privacy and efficiency. However, there are still two key problems that have yet to be resolved. First, the convergence rate of ZOO-based VFL is significantly slower compared to gradient-based VFL, resulting in low efficiency in model training and more communication round, which hinders its application on large neural networks. Second, although ZOO-based VFL has demonstrated resistance to state-of-the-art (SOTA) attacks, its privacy guarantee lacks a theoretical explanation. To address these challenges, we propose a novel cascaded hybrid optimization approach that employs a zeroth-order (ZO) gradient on the most critical output layer of the clients, with other parts utilizing the first-order (FO) gradient. This approach preserves the privacy protection of ZOO while significantly enhancing convergence. Moreover, we theoretically prove that applying ZOO to the VFL is equivalent to adding Gaussian Mechanism to the gradient information, which offers an implicit differential privacy guarantee. Experimental results demonstrate that our proposed framework achieves similar utility as the Gaussian mechanism under the same privacy budget, while also having significantly lower communication costs compared with SOTA communication-efficient VFL frameworks.

## 1 Introduction

Federated Learning [30, 22, 23, 18, 31] is an emerging technique that has raised wide attention recently. It has become one of the most important distributed learning frameworks for enabling multiple data holders to train a model collaboratively without sharing their local privacy data explicitly.

Our research focuses on VFL, where each client possesses all the data points, but only a non-intersecting subset of the features (vertically distributed). In VFL, all participants collaborate to train a single global model. The client trains a feature extraction model that maps its local data sample to embeddings, and the server collects the embeddings from all clients to make a prediction [21; 34; 6; 39; 16; 37; 14; 42; 43; 13].

Ensuring both privacy and computation-communication efficiency is crucial for practical VFL implementations. One common approach to achieve this is starting with a basic VFL [5; 6; 28] and subsequently applying privacy protection techniques, such as Secure Multiparty Computation (SMC) [8], Differential Privacy (DP) [32; 36], and Homomorphic Encryption (HE) [36; 32] to protect privacy. However, the addition of these protection techniques often increases computation-communication costs. To avoid these costly protective add-ons, research attention has turned to Zero-Order Optimization (ZOO). Literature reports that ZOO-based VFL is able to conceal the internal information of the model (gradient/model parameter) making the SOTA privacy inference attack ineffective [41]. This finding highlights the potential of ZOO for efficient and secure VFL.

However, two critical problems remained. First, the convergence rate of ZOO-based VFL is significantly slower [41] compared to VFL frameworks optimized with gradient, resulting in a high number of communication rounds and increased communication costs. Furthermore, the convergence rate is negatively correlated to the size of the parameters optimized using ZOO [41; 27; 12], which hinders the application of ZOO-based VFL on large neural networks. Second, although ZO-based VFL has demonstrated resistance to SOTA attacks, its privacy guarantee has not been theoretically explained.

To solve the first problem, we reevaluate the advantages and disadvantages of applying first-order (FO) or zero-order (ZO) gradient methods for each component of the VFL, and find an optimal balance between efficiency and privacy protection. This analysis leads to a novel cascaded hybrid optimization framework that efficiently solves the convergence problems while fully preserving privacy protection. Specifically, our proposed framework (VFL-CZOFO) utilizes the ZO gradient for optimizing the most critical output layer of the clients, while other parts are optimized with the FO gradient2. To address the second problem, we theoretically prove that applying ZOO on VFL is equivalent to adding Gaussian Mechanism [7] to the gradient information and can provide a corresponding intrinsic (\(\epsilon,\delta\))-DP guarantee.

Footnote 2: Additionally, we apply compression to further reduce the communication cost of our framework.

For a clear illustration, a schematic diagram of our proposed framework is presented in Figure 1-(c). The advantage of our framework is two-fold. First, it greatly alleviates the slow convergence problem of ZOO-based VFL (Figure 1-b), where our framework's convergence is solely limited by the output size of the clients, rather than the parameter size of the entire model. Second, compared with the FOO-based VFL (Figure 1-a), the connection layer of our framework used ZOO, allowing the backward messages to be solely losses, thus inheriting the privacy protection of the ZOO-based VFL.

The primary innovations of our paper are as follows: 1) Our framework employs a novel cascaded hybrid optimization method, in which different optimization methods are applied to different layers of the global model in each iteration, which significantly improves the convergence rate of ZOO-based VFL while preserving privacy. To the best of our knowledge, no prior research in VFL has proposed a similar method that cascaded a hybrid optimization and exploits its advantages. 2) We provide

Figure 1: Comparing the Asyn-VFL Frameworks

a theoretical explanation of the intrinsic privacy guarantee of ZOO, based on \((\epsilon,\delta)\)-DP, which is fundamentally different from the common DP mechanism, where noise is added to items afterward.

In summary, the contributions of our paper are:

* We propose the cascade hybrid optimization method in VFL, where ZOO is applied to the most essential part of the VFL, which provides intrinsic privacy protection and significantly improves the convergence rate compared with the ZOO-based VFL.
* Theoretically analysis shows: 1) the intrinsic \((\epsilon,\delta)\)-differential privacy guarantee provided by ZOO within our framework, 2) the convergence of our framework and its superiority compared with the ZOO-based VFL, 3) the compatibility of communication compression for both forward and backward messages within our framework.
* Extensive experiments show: 1) with the same \((\epsilon,\delta)\)-differential privacy guarantee, our method can achieve a similar utility as the Gaussian Mechanism, 2) Our method significantly reduces the communication cost compared with the SOTA communication-efficient VFL.

## 2 A Detailed Comparison with SOTA VFL Frameworks

Table 1 presents a comparison of our VFL framework with other SOTA VFL frameworks, including "split-learning" [34], "compressed-VFL" [5], "Syn-ZOO-VFL"3, "VAFL" [6], "ZOO-VFL" [41]. In the table, Asyn/Syn are abbreviations for asynchronous and synchronous. The \(T\) is the iteration of the model. \(d\) and \(d_{h}\) represent the size of the entire global model's parameter and the size of the client's output embeddings, respectively. The "communication size per iteration" column summarizes the number of elements in the original messages before any post-processing, such as compression. \(B\) is the batch size. \(q\) is the sampling times for multiple point estimation of ZOO [27].

Footnote 3: This is the synchronous version of ZOO-VFL, the algorithm is in Appendix E.1.

Compared to the FOO-based VFL [34, 5, 6] our framework has two distinct advantages. First, our framework leverages the intrinsic privacy protection of ZOO, which is a result of its stochastic characteristics and the concealment of internal information. Second, we leverage the advantage of ZOO to reduce the communication cost from the server to the client where the backward message consists of merely \(q\) elements, which is typically significantly smaller than the FO-based VFL with \(d_{h}B\) elements. The key difference between our framework and the FOO-based framework is that our approach reduces communication costs while simultaneously ensuring privacy protection. In contrast, FOO-based VFL typically shares internal information (parameters/gradient) [6, 5] and then applies for extra privacy protection mechanism [36, 32].

Compared with the ZOO-based VFL [41], this framework and ours both leverage the privacy protection of ZOO in VFL. However, ZOO-based VFL suffers from a slow convergence rate of \(\mathcal{O}(d/\sqrt{T})\) which is constrained by the global model parameter size \(d\). This hinders its effectiveness when dealing with "larger" networks containing millions of parameters, causing higher communication costs due to increased communication rounds. To address this issue, we applied ZOO to the crucial connection layer between the server and the client. By doing so, we maintained the privacy protection offered by ZOO while significantly mitigating the slow convergence problem, reducing the constraint from the entire global model size \(d\) to merely the output size of the clients \(d_{h}\).

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Framework & Privacy & Asyn/Syn & Optimization & Convergence \(\mathcal{O}(\cdot)\) & Comm. Size per Iter. \\  & & & & & (Forward + Backward) \\ \hline Split learning [34] & ✗ & Syn & FO & \(1/\sqrt{T}\) & \(d_{h}B\) + \(d_{h}B\) \\ Compressed-VFL [5] & ✗ & Syn & FO & \(1/\sqrt{T}\) & \(d_{h}B\) + \(d_{h}B\) \\ VAFL [6] & ✗ & Asyn & FO & \(1/\sqrt{T}\) & \(d_{h}B\) + \(d_{h}B\) \\ Syn-ZOO-VFL & ✓ & Syn & ZO & \(d/\sqrt{T}\) & \(2d_{h}B\) + 1 \\ ZOO-VFL [41] & ✓ & Asyn & ZO & \(d/\sqrt{T}\) & \(2d_{h}B\) + 1 \\ VFL-CZOFO (Ours) & ✓ & Asyn & ZO\&FO \(d_{h}/\sqrt{T}\) & \(d_{h}B\) + \(q\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Compare with Different VFL Algorithm

## 3 Method

Problem DefinitionIn the VFL framework, there is one server and \(M\) clients4. The server holds the label \(y_{i}\) for each sample \(i\in[n]\), \(([n]\triangleq\{1,2\cdots n\})\), while each client holds a non-intersecting feature set for all the samples. Specifically, the features for sample \(i\) on client \(m\) are denoted as \(x_{m,i}\). Client \(m\)'s model, \(h_{m}(w_{m};x_{m,i})\), is parameterized by \(w_{m}\) and takes local feature \(x_{m,i}\) as input, outputting the embedding. The server inputs the embeddings from all clients into its own model, which is parameterized by \(w_{0}\), and then calculates the losses for updating the parameters. The VFL framework can be viewed as solving a finite-sum problem in composite form:

Footnote 4: Server and clients are the roles in the framework, in practice, one participant may take more than one role.

\[f(w_{0},\mathbf{w};X,Y)=\frac{1}{n}\sum_{i=1}^{n}\underbrace{[\mathcal{L}(w_{0 },h_{1,i},\ldots,h_{M,i};y_{i})]}_{f_{i}(w_{0},h_{1,i},\ldots,h_{M,i})}\ \ \text{with}\ \ h_{m,i}=h_{m}(w_{m};x_{m,i})\ \ \ \forall m\in[M]\] (1)

where \(\mathbf{w}=[w_{1},\cdots w_{M}]\) denotes the parameter for all clients, \(X\) denotes the entire feature set across all clients, and \(y\) denotes the labels for all samples. For notation brevity, we define the outputs from all clients as \(\Phi_{i}(\mathbf{w})=[h_{1}(w_{1};x_{1,i}),\cdots,h_{M}(w_{M};x_{M,i})]=[h_{1, i},\ldots,h_{M,i}]\). Therefore, the loss for the \(i\)-th sample5 can be represented as \(f_{i}(w_{0},\Phi)\). \(\mathcal{L}(\ \cdot\ ;y_{true})\) is the loss function.

Footnote 5: Although sample \(i\) was used throughout this paper, it is easy to generalize our approach to a mini-batch \(B\).

Cascaded and Minimal Application of ZOO on the Connection LayerApplying ZOO on VFL resolves the privacy leakage issue from the gradient [41], but simply applying ZOO to the entire VFL has a slow convergence problem, leading to a high number of communication rounds. Specifically, the major problem of ZOO is that with more dimensions on the gradient that need to be estimated, more variance will be introduced in the estimation of the gradient [27; 3]. This is especially problematic for machine learning scenarios where even relatively small models may have millions of parameters. To address the slow convergence problem of ZOO-based VFL, we precisely apply ZOO to the client's output layer, while the rest of the model utilizes the FO gradient. This approach significantly improves the convergence of the ZOO-based VFL, since only a small portion of the entire model uses ZOO.

ZOO [12] estimates the gradient of a function by sampling a random perturbation within the function's domain of definition and evaluating its shift on the value domain. To estimate the partial derivative with respect to the output layer of the client's model, we apply the average random gradient estimation (Avg-RandGradEst) [27; 25] which utilizes \(q\) i.i.d. samples \(\{u^{j}_{m,i}\}_{j=1}^{q}\) on the objective function:

\[\hat{\nabla}_{h_{m,i}}f_{i}\left(w_{0},h_{1,i},\cdots,h_{M,i}\right)=\frac{ \phi(d_{h_{m}})}{q\mu_{m}}\sum_{j=1}^{q}[\underbrace{f_{i}(h_{m,i}+\mu_{m}u^{j }_{m,i})-f_{i}\left(h_{m,i}\right)}_{\delta^{\prime}_{m,i}}]u^{j}_{m,i}\] (2)

where \(f_{i}\left(h_{m,i}+\mu_{m}u_{m,i}\right)\) is the abbreviation for \(f_{i}\left(w_{0},h_{1,i},\cdots h_{m,i}+\mu_{m}u_{m,i}\cdots,h_{M,i}\right)\), \(d_{h_{m}}\) is the size of the output layer of client \(m\). \(\phi(d_{h_{m}})\) is a dimension-dependent factor [26] related to the choice of the distribution \(p\) of the random vector \(u^{j}_{m,i}\). If \(p\) is the multivariate uniform distribution \(\mathcal{U}(\mathcal{S}(\mathbf{0},1))\) on a unit sphere centered at \(\mathbf{0}\) with radius \(1\), then \(\phi(d_{h_{m}})=d_{h_{m}}\). If \(p\) is the multivariate normal distribution \(\mathcal{N}(\mathbf{0},\mathbf{I})\), then \(\phi(d_{h_{m}})=1\). \(q\) is the sampling times. The clients cannot calculate \(\delta^{j}_{m,i}\) by themselves because the loss function \(f_{i}(\cdot)\) is held by the server, therefore the server has to send the \(\delta^{j}_{m,i}\) back to the clients6. To implement multiple-point estimation, the server must transmit multiple distinct values of \(\delta^{j}_{m,i}\) to the client, with the number of values sent equaling the number of sampling points \(q\) used in the Avg-RandGradEst.

Footnote 6: The server and the client also need to share an identical random sequence \(u_{m,i}\), but this can be achieved by sharing one random seed at the beginning of the training, or use the sample ID as the random seed.

The theoretical difficulty in analyzing our framework is that different optimization methods are cascaded within a single model. Unlike most VFL research [34; 5; 28; 6; 41] which updates the global model with a unified optimization method, e.g. \(\nabla f_{i}\left(w_{m}\right)\) (VAFL [6]) or the \(\hat{\nabla}f_{i}\left(w_{m}\right)\) (ZOO-VFL [41]), we apply a cascaded hybrid gradient via chain rule, i.e. \(\hat{\nabla}_{h_{m}}f_{i}\left(h_{m,i}\right)\cdot\nabla_{w_{m}}h_{m,i}\), where \(\hat{\nabla}_{h_{m}}f_{i}\left(h_{m,i}\right)\) is the stochastic gradient estimator of the partial derivative w.r.t. the output layer of the client, and \(\nabla_{w_{m}}h_{m}(w_{m})\) is the local gradient of the client.

Compress the CommunicationFurthermore, we demonstrate that compression is compatible with our approach, and can be applied to all communications in the framework, including the forward message [5] and the backward message [35]. We define the compressor of the client as \(\mathcal{C}_{m}(\cdot)\), and the compressor of the server as \(\mathcal{C}_{0}(\cdot)\). In each communication round, the client \(m\) sends compressed embeddings instead of the raw message to the server, and the server replies with the compressed messages. We apply the uniform scale compressor [2] in the experiment. However, other compression schemes such as Top-K [24], sign-SGD [4], and lattice quantization [40] can also be applied to the different parts of our framework to further reduce the message size.

The theoretical challenge in proving the convergence with compression lies in that two errors (forward and backward) are introduced in the analysis. The first error affects the embeddings sent from the client to the server, where the error influences the server's model input, creating uncertainty in the loss value and gradients for updating the parameter. We bound this error by imposing assumptions on the curvature of the loss function, i.e. an upper bound for the norm of the Hessian matrix. The second error affects the message sent from the server to the clients, which then updates using the hybrid gradient with compression error. As a result, there are three different sources of uncertainty in the clients' parameter update: the two mentioned above, as well as the error caused by ZOO.

AlgorithmIn this section, we propose our asynchronous VFL7 with ZOO on the connection layer and compression on the communication messages. Starting with the client, which randomly selects a sample \(i\) and sends its local model output to the server. Receiving the forward message from the client \(m\), the server calculates the \(\delta_{m,i}^{j}\) with Avg-RandGradEst (Eq. 2) and group \(\delta_{m,i}^{j}\) with \(j=1,2,\cdots,q\) into a vector \(\Delta_{m,i}\). The server compresses \(\Delta_{m,i}\) and sends it back to the client. Then the server and client update their local model with the "gradient".

Footnote 7: A schematic graph and illustration of asynchronous VFL is in the Appendix section D.1

```
0: Learning rate \(\eta_{m}\), smoothing parameter \(\mu_{m}\), compressor \(\mathcal{C}_{m}\) for \(m\in\{0,1,...M\}\).
0: Parameter \(w_{m}\) for \(m\in\{0,1,...M\}\).
0: Initialize model parameter \(w_{m}\) for all participants \(m\in\{0,1,...M\}\)
1:while not convergent do
2:when a client \(m\) is activated, do:
3: Randomly select a sample \(x_{m,i}\)
4: Compute and send \(\mathcal{C}_{m}(h_{m}(w_{m};x_{m,i}))\) to server
5: Receive \(\mathcal{C}_{0}(\Delta_{m,i})\) from the server (in a listen manner)
6: Compute \(\hat{\nabla}_{h_{m,i}}\hat{f}_{i}\left(w_{0},\Phi_{i}\right)\) via Avg-RandGradEst (Eq. 2)
7: Compute \(\nabla_{w_{m}}h_{m}(w_{m};x_{m,i})\) via backpropagation.
8: Compute \(v_{m}=\hat{\nabla}_{h_{m,i}}\hat{f}_{i}\left(w_{0},\Phi_{i}\right)\cdot\nabla _{w_{m}}h_{m,i}\)
9: Update \(w_{m}\gets w_{m}-\eta_{m}v_{m}\)
10:when server receives from client \(m\), do:
11: Compute \(\delta_{m,i}^{l}\) and group into \(\Delta_{m,i}\)
12: Send \(\mathcal{C}_{0}(\Delta_{m,i})\) to the client \(m\)
13: Compute \(v_{0}=\nabla_{w_{0}}f_{i}(w_{0},\Phi_{i})\), (FOO)
14: Update \(w_{0}\gets w_{0}-\eta_{0}v_{0}\)
15:endwhile ```

**Algorithm 1** VFL-CZOFO

## 4 Security Analysis

Threat ModelWe discuss privacy under two scenarios: "honest-but-curious" and "honest-but-colluded". In both scenarios, all participants are assumed to follow the protocol and perform their assigned tasks. Under the "honest-but-curious" scenario, a curious client attempts to obtain private information from other participants using the information they have received. In the "honest-but-colluded" scenario, some participants collude to obtain private information from other participants. The attacker in this scenario can access all information and messages from the colluding participants. In the "honest-but-colluded" scenario, the attacker's capability is either equal to or stronger than in the "honest-but-curious" scenario. This is because the attacker can acquire more information from other participants when colluding, therefore increasing their ability to infer sensitive data.

**Theorem 4.1**.: **Differential Privacy Guarantee of Sharing the ZO Gradient:** Under the "honest-but-colluded" threat model where the attacker can access all information of all clients through the entire training process, including the client's dataset, model, and the ZO gradient received from the server. Under algorithm 1, with \(q\) being sufficiently large, \(\zeta\) being the maximum \(l_{2}\)-norm of the partial gradient w.r.t. any client's output through the entire training. If the following condition holds:

\[\sigma_{m_{t},s}=\sqrt{\frac{1}{qd_{h}T}\sum_{t=0}^{T-1}\text{tr}(\Psi_{m_{t}} ^{t})}>\frac{2\zeta\sqrt{2\ln(1.25/\delta)}}{N\cdot\epsilon}\] (3)

we can derive that sharing the stochastic gradient estimation from the server ensures \((\bar{\epsilon},\bar{\delta})\)-DP for all \(\epsilon,\delta,\delta^{\prime}>0\), where \(\bar{\epsilon}=\sqrt{2T\ln(1/\delta^{\prime})\epsilon}+T\epsilon(e^{\epsilon} -1)\), \(\bar{\delta}=T\delta+\delta^{\prime}\).

_Proof sketch:_ The proof can be found in Appendix C.1. Firstly, we establish that each client's update can be modeled with an ideal sequence, where the unbiased parameter is updated with the gradient of the smoothed loss function \(f_{\mathbf{u},i}\left(w_{0}^{t},\Phi_{i}\right)=\mathbb{E}_{\mathbf{u}}[f_{i} (w_{0},\Phi(\mathbf{w}))+\mu\mathbf{u}]\). Since Avg-RandGradEst in Eq. 2 is an average of \(q\) independently and identically distributed samples from \(f_{i}(\cdot)\), applying the central limit theorem, the estimated gradient is normally distributed for sufficiently large \(q\). We can then apply the differential privacy guarantee for the Gaussian Mechanism to the estimated gradient. Finally, the accumulated privacy guarantee is derived from the advanced composition theorem.

_Remark 4.2_.: This theorem tells that under the worse case where a strong attacker has obtained all of the ZO information \(\Delta_{m,i}\) from all client \(m\in[M]\), of all sample \(i\in[N]\), during the entire training process \(t\in[T]\), the attacker cannot differ a single data point in the dataset, with \((\bar{\epsilon},\bar{\delta})\)-DP guarantee.

_Remark 4.3_.: Since the ZO gradient is shared from the server to clients, this theorem mainly provides a privacy guarantee for the label on the server.

_Remark 4.4_.: For a weaker attacker, where the attacker can only access the information from fewer clients ("honest-but-colluded"), or only one client ("honest-but-curious"), the privacy guarantee either remains the same or is strengthened.

Additionally, in Appendix C.2, we provided a detailed discussion of the privacy protection of our approach at the framework level, plus a discussion on the SOTA inference attacks under "honest-but-curious" [10, 33, 45, 45, 44, 17, 9] and "honest-but-colluded"[10, 45, 29, 38] threat model.

## 5 Convergence Analysis

**Assumption 5.1**.: The formal definition and detailed discussion of the assumptions are in Appendix B. We assume that there is a feasible optimal solution for \(f(\cdot)\), \(\nabla f_{i}\left(\cdot\right)\) is \(L\)-Lipschitz continuous, \(f_{i}\left(\cdot\right)\) has an unbiased gradient, bounded Hessian \(H_{m}\) and bounded block-coordinate gradient \(\mathbf{G}_{m}\). The activation of the clients is independent, and the client has a uniformly bounded delay \(\tau\).

**Theorem 5.2**.: _Under assumption 5.1, to solve the problem 1 with algorithm 1, the following inequality holds._

\[\frac{1}{T}\!\sum_{t=0}^{T-1}\!\mathbb{E}\left\|\nabla f\left(w_{0 }^{t},\mathbf{w}^{t}\right)\right\|^{2}\leq \frac{4p_{\star}\mathbb{E}\left(f^{0}-f^{*}\right)}{T\eta}+2\eta p _{\star}L_{*}\left(4d_{\star}\mathbf{G}_{*}^{4}+\mu_{*}^{2}L_{*}^{2}d_{*}^{2} \mathbf{G}_{*}^{2}+2\sigma_{0}^{2}\right)\] \[+7\eta^{2}p_{\star}\tau\mathbf{G}_{*}^{2}\left(4d_{\star} \mathbf{G}_{*}^{2}+\mu_{*}^{2}L_{*}^{4}d_{*}^{2}+2L_{*}^{2}\Gamma\right)\] \[+\mu_{*}^{2}p_{\star}L_{*}^{2}d_{*}^{2}\mathbf{G}_{*}^{2}+ \mathcal{E}p_{\star}H_{*}^{2}\left(6+16\mathbf{G}_{*}^{2}\right)+17\Gamma p_{ \star}\mathbf{G}_{*}^{2}\]

_where \(L_{*}=\max_{m}\left\{L,L_{0},L_{m}\right\}\), \(\eta_{0}=\eta_{m}=\eta\), \(\frac{1}{p_{\star}}=\min_{m}p_{m}\), \(\mu_{*}=\max_{m}\left\{\mu_{m}\right\}\), \(d_{*}=\max_{m}\left\{d_{h_{m}}\right\}\), \(\mathbf{G}_{*}=\max_{m}\left\{\mathbf{G}_{0},\mathbf{G}_{m}\right\}\), \(H_{*}=\max_{m}\left\{H_{0},H_{m}\right\}\), \(\mathcal{E}\) and \(\Gamma\) are the upper bound for the square of the norm of the forward and backward compression error respectively. \(T\) is the total number of iterations (communication rounds)._

_Remark 5.3_.: If we choose \(\eta=\frac{1}{\sqrt{T}}\) and \(\mu_{*}=\frac{1}{\sqrt{T}}\). Design the compression to make \(\mathcal{E}=\mathcal{O}\left(\frac{1}{\sqrt{T}}\right)\) and \(\Gamma=\mathcal{O}\left(\frac{1}{\sqrt{T}}\right)\) we can derive:

\[\frac{1}{T}\!\sum_{t=0}^{T-1}\!\mathbb{E}\left\|\nabla f\left(w_{0}^{t}, \mathbf{w}^{t}\right)\right\|^{2}=\mathcal{O}\left(\frac{d_{h}}{\sqrt{T}}\right)\]where \(d_{h}=\max\limits_{m}\left\{d_{h_{m}}\right\}\) is the maximum output layer size of the clients.

_Remark 5.4_.: This theorem proves that we significantly relieve the slow convergence problem of ZOO-based VFL [41] from \(\mathcal{O}(\frac{d}{\sqrt{T}})\) to \(\mathcal{O}(\frac{d_{h}}{\sqrt{T}})\).

## 6 Experiment

We conducted a systematic experiment on the SOTA and baseline VFL and our proposed framework. The primary objective of our study was to empirically validate the security measures of our approach and demonstrate its capability in reducing communication overhead. Furthermore, we conducted an ablation study to quantify the contributions of each component toward improving communication efficiency. Due to space limitations, extra experiment details, the CIFAR-10 experiments, extra experiments on less common datasets, and extra experiments on other aspects of our framework have been placed in appendix E. The source code for this project is available at the following URL: https://github.com/GanyuWang/VFL-CZOFO.

### Experiment Setups

DatasetsThe datasets were vertically partitioned among all participants in our experiments. Each client held a portion of the features of each sample, while the server held the corresponding labels. Both the server and the client kept the sample IDs during training. We utilized two datasets in our main experiments: MNIST [20] and CIFAR-10 [19]. For both datasets, we employed two clients in our experiments, with each client being responsible for half of the images8 in the respective dataset. Therefore, we denoted these datasets as dist-MNIST and dist-CIFAR-10.

Footnote 8: Details of the feature splitting can be found in Appendix E.1

ModelsIn our dist-MNIST experiment, we implemented a multilayer perception (MLP). Each client utilized a one-layer Fully Connected Layer (FCL), with an input size equal to the flattened local data input size, and an output size of 64. The server employed a two-layer FCL. The first layer concatenated all the outputs from all clients and produced embeddings with 128 neurons. The second layer outputs 10 neurons for prediction. ReLU [11] was used as the activation function for the server and clients. Practically, the server kept a table of the outputs from all clients for all of the samples, i.e. \(\tilde{\Phi}_{i}^{t}=[h_{1}(w_{1}^{t-\tau_{1,i}^{t}};x_{1,i}),\ldots,w_{M}^{t- \tau_{M,i}^{t}}]\) was maintained by the server. The server updated the corresponding value in the table when it received \(h_{m}(w_{m}^{t-\tau_{m,i}^{t}};x_{m,i})\) from client \(m\) during each communication round. The clients' outputs from the table were then used for prediction. Regarding the dist-CIFAR-10 dataset, each client padded its half-image to full size and trained a ResNet-18 [15]. The server then summed the outputs from all the clients.

Frameworks for ComparisonWe compare our framework with other SOTA VFL frameworks including "split-learning" [34], "compressed-VFL" [5], "Syn-VFL-ZO"9, "VAFL" [6], "ZOOVFL" [41]. All frameworks utilized identical base models and training procedures. While some frameworks also introduced alternative VFL settings, e.g. server and clients both processing the labels, we only focus on the VFL setting where the server stores the labels and multiple clients possess non-intersecting features.

Footnote 9: This is the synchronous version of ZOO-VFL, the algorithm is in Appendix E.1

Training ProceduresFor the experiment applying the split MLP model on dist-MNIST, a batch size of 64 was utilized, and the model was trained for 100 epochs. For the experiment applying the ResNet-18 on dist-CIFAR-10, a batch size of 128 was used, and the model was trained for 50 epochs. The learning rate \(\eta\) was chosen from \([0.1,0.01,0.001,\cdots]\), and \(\mu\) for ZOO was chosen from \([1,0.1,0.001,0.0001,\cdots]\). We use a uniform scale compressor [2] on the forward and backward messages with different compression bits \([8,4,2,1]\). The number of sampling \(q\) used for Avg-RandGradEst in the dist-MNIST experiment was set to \([10,100]\), while in the dist-CIFAR-10 experiment, it was set to either \([100,500,1000]\). The sampling distribution is \(\mathcal{U}(\mathcal{S}(\bm{0},1))\). We used a vanilla SGD with a fixed learning rate for all VFL frameworks to ensure a fair comparison. The reported test accuracy values in the tables have been obtained from five independent runs.

### Evaluation on the Privacy Protection

In this part, we evaluate the privacy protection of our method and Gaussian Mechanism. The major difference between the ZOO and Gaussian Mechanism in DP is that the variance of the stochastic gradient estimation (ZOO) is intrinsically controlled by the hyper-parameter of the ZOO and the characteristic of the objective function, while in the Gaussian Mechanism, the variance is a parameter that directly controls the magnitude of the noise added.

We obtain the DP guarantee w.r.t. the sampling times of ZOO in the dist-MNIST experiment and report the result in Table 2. We estimate \(\sigma^{2}\) by freezing the model and repeatedly applying Eq.2 \(100\) times, then calculating the variance of the estimated gradients. We obtain the maximum gradient norm \(\zeta\) by recording the norm throughout the entire training process10. We set \(\delta=10^{-6}\) and calculate \(\epsilon\) based on the given sampling times \(q\). The iteration \(T\) was set to the total iteration of 50 epochs, which is \(93800\).

Footnote 10: Alternatively, one can also apply gradient clipping[1] to get a better privacy guarantee by bounding \(\zeta\), however, we want to focus on the intrinsic privacy of our method, therefore only record the maximum norm.

In the second part, we compared the utility of our framework with the VAFL[6] using the Gaussian Mechanism and gradient clipping [1] on the partial derivative w.r.t. the client's output with the corresponding \((\epsilon,\delta)\)-DP. We set the gradient norm clipping bound to \(0.1\) And we plotted the training accuracy for each epoch in Figure 2. As illustrated in the figure, our scheme exhibits comparable utility to VAFL with the same privacy guarantee.

### Evaluation of Training Efficiency and Communication Cost

In this part, we compare our framework with SOTA VFL in the effectiveness of training and communication efficiency. We apply the best tuning for all other frameworks with respect to test accuracy and communication efficiency. (The experiment for dist-CIFAR-10 is in Appendix E.3)

Figure 3-(a) demonstrates that our VFL-CZOFO framework has the fastest convergence rate comparable to VAFL, demonstrating the superiority of our cascade hybrid optimization method. It is worth noting that the pure-ZOO-based VFL suffers from a slower convergence compared to other frameworks, even with an MLP model. Figure 3-(b) plots the total communication cost against the training accuracy, where the crosses indicate the point where the curve reaches 95% training accuracy for MNIST. It shows that our VFL-CZOFO framework achieves the same level of convergence with much lower communication costs.

Table 3 report the important statistic about the training. As shown in the table, our framework significantly outperforms other communication-efficient frameworks in terms of communication cost.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \(q\) & \(\sigma^{2}\) & \(\zeta\) & \(\delta\) & \(\epsilon\) & \(\delta^{\prime}\) & \(\bar{\epsilon}\) & \(\bar{\delta}\) \\ \hline
10 & 1.61 & 20.1 & \(10^{-6}\) & \(2.8\times 10^{-3}\) & \(10^{-6}\) & 85.9 & \(9.4\times 10^{-2}\) \\
100 & 0.15 & 7.3 & \(10^{-6}\) & \(3.3\times 10^{-3}\) & \(10^{-6}\) & 93.6 & \(9.4\times 10^{-2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Sampling Times of Avg-RandGradEst and the Corresponding Differential Privacy Guarantee

Figure 3: Compare with other VFL Frameworks

Figure 2: The Utility Comparing with the VAFL with Gaussian Mechanism

[MISSING_PAGE_EMPTY:9]

## 7 Limitation

While the utilization of ZOO improves communication efficiency and security, it comes at the expense of increased computational costs for the server. Specifically, the server needs to perform \(q\) extra forward propagations on its local model compared to other FOO-based VFL methods. A more comprehensive analysis and supplementary experiments can be found in Appendix E.2. However, it is important to note that the server typically has higher computational capabilities, which makes the associated costs manageable and acceptable.

## 8 Conclusion

We propose a solution in VFL that improves communication efficiency and privacy simultaneously. Our method has been theoretically proven to outperform ZOO-based VFL in terms of convergence, while also providing proof of the intrinsic differential privacy guarantees. Through our experiments, we demonstrate that our method substantially reduces the communication cost of VFL in comparison to the state-of-the-art communication-efficient VFL. Furthermore, our method achieves comparable utility to VFL models that offer the same level of privacy guarantee.

## Acknowledgments and Disclosure of Funding

This work has been supported by the Natural Sciences and Engineering Research Council of Canada (NSERC), Discovery Grants program. Bin Gu was partially supported by the National Natural Science Foundation of China under Grant 62076138.

## References

* Abadi et al. [2016] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _Proceedings of the 2016 ACM SIGSAC conference on computer and communications security_, pages 308-318, 2016.
* Bennett [1948] William Ralph Bennett. Spectra of quantized signals. _The Bell System Technical Journal_, 27(3):446-472, 1948.
* Berahas et al. [2022] Albert S Berahas, Liyuan Cao, Krzysztof Choromanski, and Katya Scheinberg. A theoretical and empirical comparison of gradient approximations in derivative-free optimization. _Foundations of Computational Mathematics_, 22(2):507-560, 2022.
* Bernstein et al. [2018] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd: Compressed optimisation for non-convex problems. In _International Conference on Machine Learning_, pages 560-569. PMLR, 2018.
* Castiglia et al. [2022] Timothy J Castiglia, Anirban Das, Shiqiang Wang, and Stacy Patterson. Compressed-vfl: Communication-efficient learning with vertically partitioned data. In _International Conference on Machine Learning_, pages 2738-2766. PMLR, 2022.
* Chen et al. [2020] Tianyi Chen, Xiao Jin, Yuejiao Sun, and Wotao Yin. Vafl: a method of vertical asynchronous federated learning. _arXiv preprint arXiv:2007.06081_, 2020.
* Dwork et al. [2014] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* Fang et al. [2021] Wenjing Fang, Derun Zhao, Jin Tan, Chaochao Chen, Chaofan Yu, Li Wang, Lei Wang, Jun Zhou, and Benyu Zhang. Large-scale secure xgb for vertical federated learning. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 443-452, 2021.
* Fredrikson et al. [2015] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In _Proceedings of the 22nd ACM SIGSAC conference on computer and communications security_, pages 1322-1333, 2015.
* Fu et al. [2022] Chong Fu, Xuhong Zhang, Shouling Ji, Jinyin Chen, Jingzheng Wu, Shanqing Guo, Jun Zhou, Alex X Liu, and Ting Wang. Label inference attacks against vertical federated learning. In _31st USENIX Security Symposium (USENIX Security 22), Boston, MA_, 2022.

* [11] Kunihiko Fukushima. Cognitron: A self-organizing multilayered neural network. _Biological cybernetics_, 20(3-4):121-136, 1975.
* [12] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368, 2013.
* [13] Bin Gu, Zhiyuan Dang, Xiang Li, and Heng Huang. Federated doubly stochastic kernel learning for vertically partitioned data. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 2483-2493, 2020.
* [14] Bin Gu, An Xu, Zhouyuan Huo, Cheng Deng, and Heng Huang. Privacy-preserving asynchronous vertical federated learning algorithms for multiparty collaborative learning. _IEEE transactions on neural networks and learning systems_, 2021.
* [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [16] Yaochen Hu, Di Niu, Jianming Yang, and Shengping Zhou. Fdml: A collaborative machine learning framework for distributed features. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 2232-2240, 2019.
* [17] Xiao Jin, Pin-Yu Chen, Chia-Yi Hsu, Chia-Mu Yu, and Tianyi Chen. Cafe: Catastrophic data leakage in vertical federated learning. _Advances in Neural Information Processing Systems_, 34:994-1006, 2021.
* [18] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pages 5132-5143. PMLR, 2020.
* [19] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
* [20] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2, 2010.
* [21] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future directions. _IEEE Signal Processing Magazine_, 37(3):50-60, 2020.
* [22] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. _Proceedings of Machine Learning and Systems_, 2:429-450, 2020.
* [23] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learning on non-IID features via local batch normalization. In _International Conference on Learning Representations_, 2021.
* [24] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. _arXiv preprint arXiv:1712.01887_, 2017.
* [25] Sijia Liu, Jie Chen, Pin-Yu Chen, and Alfred Hero. Zeroth-order online alternating direction method of multipliers: Convergence analysis and applications. In _International Conference on Artificial Intelligence and Statistics_, pages 288-297. PMLR, 2018.
* [26] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K Varshney. A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications. _IEEE Signal Processing Magazine_, 37(5):43-54, 2020.
* [27] Sijia Liu, Bhavya Kailkhura, Pin-Yu Chen, Paishun Ting, Shiyu Chang, and Lisa Amini. Zeroth-order stochastic variance reduction for nonconvex optimization. _Advances in Neural Information Processing Systems_, 31, 2018.
* [28] Yang Liu, Yan Kang, Liping Li, Xinwei Zhang, Yong Cheng, Tianjian Chen, Mingyi Hong, and Qiang Yang. A communication efficient vertical federated learning framework. _Scanning Electron Microsc Meet at_, 2019.
* [29] Xinjian Luo, Yuncheng Wu, Xiaokui Xiao, and Beng Chin Ooi. Feature inference attack on model predictions in vertical federated learning. In _2021 IEEE 37th International Conference on Data Engineering (ICDE)_, pages 181-192. IEEE, 2021.
* [30] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.

* [31] Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtarik. Proxskip: Yes! local gradient steps provably lead to communication acceleration! finally! In _International Conference on Machine Learning_, pages 15750-15769. PMLR, 2022.
* [32] Thilina Ranbaduge and Ming Ding. Differentially private vertical federated learning. _arXiv preprint arXiv:2211.06782_, 2022.
* [33] Jiankai Sun, Xin Yang, Yuanshu Yao, and Chong Wang. Label leakage and protection from forward embedding in vertical federated learning. _arXiv preprint arXiv:2203.01451_, 2022.
* [34] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning for health: Distributed deep learning without sharing raw patient data. _arXiv preprint arXiv:1812.00564_, 2018.
* [35] Yujia Wang, Lu Lin, and Jinghui Chen. Communication-efficient adaptive federated learning. _arXiv preprint arXiv:2205.02719_, 2022.
* [36] Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS Quek, and H Vincent Poor. Federated learning with differential privacy: Algorithms and performance analysis. _IEEE Transactions on Information Forensics and Security_, 15:3454-3469, 2020.
* [37] Kang Wei, Jun Li, Chuan Ma, Ming Ding, Sha Wei, Fan Wu, Guihai Chen, and Thilina Ranbaduge. Vertical federated learning: Challenges, methodologies and experiments. _arXiv preprint arXiv:2202.04309_, 2022.
* [38] Haiqin Weng, Juntao Zhang, Feng Xue, Tao Wei, Shouling Ji, and Zhiyuan Zong. Privacy leakage of real-world vertical federated learning. _arXiv preprint arXiv:2011.09290_, 2020.
* [39] Kai Yang, Tao Fan, Tianjian Chen, Yuanming Shi, and Qiang Yang. A quasi-newton method based vertical federated learning framework for logistic regression. _arXiv preprint arXiv:1912.00513_, 2019.
* [40] Ram Zamir and Meir Feder. On lattice quantization noise. _IEEE Transactions on Information Theory_, 42(4):1152-1159, 1996.
* [41] Qingsong Zhang, Bin Gu, Zhiyuan Dang, Cheng Deng, and Heng Huang. Desirable companion for vertical federated learning: New zeroth-order gradient based algorithm. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 2598-2607, 2021.
* [42] Qingsong Zhang, Bin Gu, Cheng Deng, Songxiang Gu, Liefeng Bo, Jian Pei, and Heng Huang. AsySQN: Faster vertical federated learning algorithms with better computation resource utilization. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 3917-3927, 2021.
* [43] Qingsong Zhang, Bin Gu, Cheng Deng, and Heng Huang. Secure bilevel asynchronous vertical federated learning with backward updating. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 10896-10904, 2021.
* [44] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. idlg: Improved deep leakage from gradients. _arXiv preprint arXiv:2001.02610_, 2020.
* [45] Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. _Advances in neural information processing systems_, 32, 2019.