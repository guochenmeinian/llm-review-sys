# Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis

 Hongru Yang

The University of Texas at Austin

& Princeton University

hy6385@utexas.edu

&Bhavya Kailkhura

Lawrence Livermore National Laboratory

kailkhura1@llnl.gov

&Zhangyang Wang

The University of Texas at Austin

atlaswang@utexas.edu

&Yingbin Liang

The Ohio State University

liang.889@osu.edu

Work done while doing a internship at Lawrence Livermore National Laboratory and visiting Princeton University.

###### Abstract

Understanding the training dynamics of transformers is important to explain the impressive capabilities behind large language models. In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system. We establish near minimum loss and characterize the attention model after training. We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed _automatic balancing of gradients_, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss. We also conduct experiments to verify our theoretical results.

## 1 Introduction

Ever since the invention of self-attention [20], transformers have become a dominating backbone architecture in many machine learning applications such as computer vision [14, 15] and natural language processing [13]. Nowadays, ChatGPT and GPT-4 [2] have demonstrated astonishing abilities in many areas such as language understanding, mathematics and coding, which have sparked artificial general intelligence [1]. In the meantime, there has been a burgeoning development of large language models (LLMs) [12, 13, 14] as well as multi-modal models [11].

Despite the huge empirical success, theoretical understanding of why a pre-trained language model can possess such impressive performance has been significantly lagging behind. Some previousefforts have been made in understanding the capacity and representational power of transformers [2, 1, 1, 2, 3, 4]. However, most results of this type of works are existential and rely on manual construction of the weights. It is unclear whether the constructed weights are the actual solutions after training transformers. In order to understand the mechanism behind those pre-trained language models, a line of studies have aimed to open the black box of optimization via studying training dynamics of transformers and explaining why transformers can be trained to perform well [1, 2, 3, 4, 5]. However, those previous works often relied on various simplifications in their analysis such as weight reparameterization, attention linearization, special initialization, lazy regime, etc. One goal of this paper is to take a further step to demystify the training dynamics of transformers and consider more practical training setup, thus better capturing the actual training process.

Our study of transformers' training dynamics will focus on a basic problem of recognizing co-occurrence of words under a binary classification setup, which is an important ability of LLMs to perform many tasks correctly in natural language processing (NLP). For example, the classical n-gram model [13, 2] predicts the next word based on co-occurrence of multiple words. Consider the following scenario: if the task for the language model is to read a paragraph describing a children and then answer some questions, say, "Is Bob eating a banana?". In order to answer the question correctly, the model must be able to detect the co-occurrence of the two words "Bob" and "banana" in the paragraph. Motivated by this, we study the problem of detecting co-occurrence of two target words via the model of a one-layer transformer with a self-attention module followed by a linear multi-layer perceptrons (MLP) layer. Our goal is to characterize the dynamics of the training process via the gradient flow analysis, thus providing a theory to explain how transformers can be trained to perform well.

Our contribution is summarized below:

* We study the gradient flow dynamics of detecting word co-occurrence. The training starts with random initialization and then simultaneously updates _four_ weight matrices (including key, query, and value matrices and a linear MLP) in the transformer architecture via gradient flow. We show that gradient flow can achieve small loss although the loss function is highly nonconvex. We further characterize the explicit form of attention matrices after training, which captures the strong positive correlation between the two target signals and strong negative correlation between one target signal and the common token, both leading to large classification margin.
* We characterize the training process into two phases. In Phase 1 (alignment of MLP for correct classification), we show that the linear MLP of the transformer quickly aligns with the two target word tokens whereas all other variables in the dynamical system stay almost unchanged from their initialization values. All training samples are correctly classified at the end of Phase 1, but the loss value is still large due to small classification margin. In Phase 2 (evolution of attention and MLP for large classification margin), along with the continual evolution of MLP, correct classification by MLP also encourages the gradients of attention matrices to learn. Specifically, the softmax probability increases if the key and query tokens correspond to the two target words, and the value transform of the two words becomes more positively correlated, both leading to enlarge the classification margin. Thus, the training and test loss values both are driven down to nearly zero.
* Technically, our proof techniques do not rely on several commonly used assumptions in the literature such as weight reparameterization, attention linearization, special initialization, lazy regime, etc. Our main idea is to treat the problem as a _coupled_ dynamical system with _six_ different types of dynamic variables, for which we provide an articulated analysis on the gradient flow dynamics. In particular, we prove a novel property of the gradient flow, termed _automatic balancing of gradients_, which shows that the ratio of several important gradients will evolve closely within the same range during training. This enables us to show that the losses of all training samples can decrease almost at the same rate, and is also a key component in proving the near minimum training loss as well as analyzing the changes of softmax.

### Related Work

**Transformer representational power.** Several previous works have studied the expressiveness of transformers. One line of work was from a universal approximation perspective and thus provided the existential results [1, 2, 3, 4, 5]. As a separate view, [1] showed that a single attention head can represent a sparse function over the input sequence with sample complexity much smaller than the context length. [22] studied the approximation and generalization performance of transformers in in-context learning. [15] proved that transformers can represent certain functions more efficiently than MLPs.

**Training transformers.** Various settings of training transformers have been studied recently. [23] studied the impact of head and prompt tuning of transformer on the downstream learning tasks. [16] proved that transformers can learn spatial structures. [15] studied how a shallow transformer learns a dataset with both label-relevant and label-irrelevant tokens. [23] studied a next-token prediction problem and showed that self-attention behaves like a discriminating scanning algorithm. [14] analyzed a layer-wise optimization scheme on how transformers learn topic structures. [13, 15] studied a setting where transformers can learn a SVM solution. [15] provided analysis of training graph transformers for node classification tasks. [16] studied the implicit bias in the next-token prediction problem. For in-context linear regression, [23] constructed transformer weights to solve this task and showed empirically that this is similar to what the transformer learned by gradient descent, [1] proved that the critical points of the training objective of linear transformers implement a pre-conditioned gradient descent, [13] provided the training dynamics of linear attention models, [14] characterized the training dynamics of softmax transformers, and [22] studied a multi-task linear regression problem with a multi-headed softmax transformer. Further, [15] focused on nonlinear self-attention and nonlinear MLP over classification tasks in in-context learning. [15] proved the convergence of transformers via neural tangent kernel. [16] showed that two-layer transformers can learn causal structure via gradient descent. [13] developed algorithms for provably learning a multi-head attention layer. [12] studied how transformers learn feature-position correlation.

This paper studies a different problem of detecting co-occurrence of words via transformers. Such a setting has not been considered in the literature. More importantly, the previous studies of training dynamics of transformers have adopted various assumptions/simplifications such as weight reparameterization, special initialization, attention linearization, lazy regime, etc. In contrast, our analysis here based on gradient flow does not rely on those simplifications, which can be of independent interest for studying transformers in other settings.

## 2 Problem Setting

**Notations.** For a vector \(v\in\mathbb{R}^{d}\), we use \(\text{diag}(v)\) to denote a diagonal matrix with \(v\) being the diagonal entries. When we subtract the vector \(v\) by a scalar \(a\), we subtract each entry of \(v\) by \(a\), i.e., \(v-a\in\mathbb{R}^{d}\) and \((v-a)_{i}=v_{i}-a\). We use \(\widetilde{\Omega},\widetilde{\Theta},\widetilde{O}\) to hide polylogarithmic factors.

### Data Model

**Definition 2.1** (Data distribution).: _Given a set of orthonormal vectors \(\{\mu_{i}\}_{i=1}^{d}\) as word embedding, let \(\mu_{1},\mu_{2}\in\mathbb{R}^{d}\) be two target signals whose co-occurrence needs to be detected by the model, and let \(\mu_{3}\in\mathbb{R}^{d}\) be a common token vector. A data entry \((X,y)\in\mathbb{R}^{d\times L}\times\{\pm 1\}\), where \(X=[x_{1},x_{2},\ldots,x_{L}]\) consists of \(L\) tokens, is generated by the distribution \(\mathcal{D}\) as follows:_

1. _Uniformly randomly select an index_ \(i_{3}\in[L]\) _and set_ \(x_{i_{3}}=\mu_{3}\)_._
2. _Then, one of the following cases occurs:_ * _With probability_ \(1/2\)_, set_ \(y=1\) _and uniformly randomly select two indices_ \(i_{1}\neq i_{2}\in[L]\setminus\{i_{3}\}\) _and set_ \(x_{i_{1}}=\mu_{1},\;x_{i_{2}}=\mu_{2}\)_. For_ \(i\in[L]\setminus\{i_{1},i_{2},i_{3}\}\)_, set_ \(x_{i}=\mathtt{Uniform}(\{\mu_{i}\}_{i=4}^{d})\)_._ * _With probability_ \(1/6\)_, set_ \(y=-1\) _and uniformly randomly select one index_ \(i_{1}\in[L]\setminus\{i_{3}\}\) _and set_ \(x_{i_{1}}=\mu_{1}\)_. For_ \(i\in[L]\setminus\{i_{3},i_{1}\}\)_, we set_ \(x_{i}=\mathtt{Uniform}(\{\mu_{i}\}_{i=4}^{d})\)_._ * _With probability_ \(1/6\)_, set_ \(y=-1\) _and uniformly randomly select one index_ \(i_{2}\in[L]\setminus\{i_{3}\}\) _and set_ \(x_{i_{2}}=\mu_{2}\)_. For_ \(i\in[L]\setminus\{i_{3},i_{2}\}\)_, we set_ \(x_{i}=\mathtt{Uniform}(\{\mu_{i}\}_{i=4}^{d})\)_._ * _With probability_ \(1/6\)_, set_ \(y=-1\)_. For all_ \(i\in[L]\setminus\{i_{3}\}\)_, we set_ \(x_{i}=\mathtt{Uniform}(\{\mu_{i}\}_{i=4}^{d})\)_._

In summary, there are 4 types of data: (1) both \(\mu_{1},\mu_{2}\) appear, (2) only \(\mu_{1}\) appears, (3) only \(\mu_{2}\) appears, and (4) neither \(\mu_{1}\) nor \(\mu_{2}\) appears. We denote the set of indices of the above 4 different types of data by \(I_{1},I_{2},I_{3},I_{4}\subseteq[n]\). We further define \(\mathcal{R}=\{\mu_{i}\}_{i=4}^{d}\). For simplicity, our data distribution assumes \(\mu_{1},\mu_{2},\mu_{3}\) appear only once in a data entry. The occurrence probability of each type of data is chosen in the above way to make the distribution label-balanced. We assume there is a fixed set of orthonormal vectors as word embedding, which is analogous to the one-hot embedding of a set of vocabularies. Furthermore, in our daily language, there are some words appearing in almost every sentence such as "a" and "the". Thus, to model those words, we include a common token in every data entry. Finally, notice that if we ignore the common token and random tokens, the data distribution simplifies to a logical AND problem.

**Remark 2.2**.: _Recognizing co-occurrence of words is an important ability for language models to perform many NLP tasks correctly. Consider the example of a language model first reading a paragraph describing a children and then answering the question "Is Bob eating a banana?" If the description is "Bob is watching a television while eating a banana", then the model should answer "Yes". If the description is "Bob is playing computer games", then the model should answer "No". Thus, the model needs to recognize the co-occurrence of "Bob" and "banana"._

For simplicity of our analysis, we make the following assumption on our training data set.

**Assumption 2.3**.: _The training set satisfies: (i) \(\frac{|I_{1}|}{n}=\frac{1}{2}\) and \(\frac{|I_{2}|}{n}=\frac{|I_{3}|}{n}=\frac{|I_{4}|}{n}=\frac{1}{6}\); and (ii) for all \(i_{1},i_{2}\in[n],\;l_{1},l_{2}\in[L]\), if \(X^{(i_{1})}_{l_{1}},X^{(i_{2})}_{l_{2}}\notin\{\mu_{1},\mu_{2},\mu_{3}\}\), then \(X^{(i_{1})}_{l_{1}}\neq X^{(i_{2})}_{l_{2}}\), i.e., all irrelevant words are different._

The first assumption can be approximately satisfied with high probability given the total number \(n\) of samples is large enough. Such an assumption can be removed by applying the standard concentration theorems. The second assumption implicitly assumes \(nL\leq d\). If the irrelevant words are uniformly sampled from a large entire vocabulary, then each irrelevant word appears only very few times in the training set. Thus, letting irrelevant words appear only once in the entire training set is a reasonable way to simplify our analysis.

### Transformer Architecture and Training

Consider a training set \(\{(X^{(i)},y_{i})\}_{i=1}^{n}\) with \(n\) training samples. Each data point \(X^{(i)}\in\mathbb{R}^{d\times L}\) contains \(L\) tokens, i.e., \(X^{(i)}=[x^{(i)}_{1},x^{(i)}_{2},\ldots,x^{(i)}_{L}]\). We consider the transformer model with a self-attention module followed by a linear MLP:

\[F(X;W,W_{V},W_{K},W_{Q})=\sum_{l=1}^{L}\sum_{j=1}^{m_{1}}a_{j}\left(w_{j}^{ \top}W_{V}X\cdot\text{Softmax}\Big{(}\frac{X^{\top}W_{K}^{\top}W_{Q}x_{l}}{ \sqrt{m}}\Big{)}\right)\] (1)

where the query matrix \(W_{Q}\in\mathbb{R}^{m\times d}\), the key matrix \(W_{K}\in\mathbb{R}^{m\times d}\), the value matrix \(W_{V}\in\mathbb{R}^{m\times d}\), the hidden-layer MLP weights \(W\in\mathbb{R}^{m_{1}\times m}\) (with \(w_{j}^{\top}\) being the \(j\)-th row of \(W\)), and the output-layer weights of the MLP \(a\in\mathbb{R}^{m_{1}}\). We define the linear MLP function of the transformer to be \(G(\mu)=\sum_{j=1}^{m_{1}}a_{j}w_{j}^{\top}W_{V}\mu\). We now introduce some shorthand notations \(K=W_{K}X,\;Q=W_{Q}X,\;V=W_{V}X\) and let \(k_{l}=W_{K}x_{l}\). Notice that \(K=[k_{1},k_{2},\ldots,k_{L}]\). We further extend this shorthand to \(q_{l}\) and \(\;v_{l}\). We also define functions \(k(\mu)=W_{K}\mu,\;q(\mu)=W_{Q}\mu,\;v(\mu)=W_{V}\mu\). We introduce the shorthand for the **score vector**\(s_{l}:=\frac{X^{\top}W_{K}^{\top}W_{Q}x_{l}}{\sqrt{m}}\) and the attention vector \(p_{l}:=\text{Softmax}(s_{l})\). For the attention vector, if \(\mu,\nu\in X^{(i)}\), let \(l(i,\mu),l(i,\nu)\) be the indices such that \(X^{(i)}_{l(i,\mu)}=\mu,\;X^{(i)}_{l(i,\nu)}=\nu\), and we define \(p^{(i)}_{q\leftarrow\mu,k\leftarrow\nu}:=p^{(i)}_{q\leftarrow l(i,\mu),k \leftarrow l(i,\nu)}:=\text{Softmax}\Big{(}\frac{X^{(i)^{\top}}W_{K}^{\top}W_{ Q}\mu}{\sqrt{m}}\Big{)}_{l(i,\nu)}\).

**Initialization.** We initialize \(a_{j}\overset{i.i.d.}{\sim}\texttt{Uniform}(\pm 1)\) and the value of \(a\) is fixed during training. The trainable parameters are \([W,W_{V},W_{K},W_{Q}]\). We initialize \([W,W_{V},W_{K},W_{Q}]\) by \(W_{i,j}\overset{i.i.d.}{\sim}\mathcal{N}(0,\sigma_{1}^{2})\) and \((W_{V})_{i,j},(W_{K})_{i,j},(W_{Q})_{i,j}\overset{i.i.d.}{\sim}\mathcal{N}(0, \sigma_{0}^{2})\).

**Training.** We adopt the **cross-entropy loss**\(l(x)=\log(1+\exp(-x))\). The gradient of the cross-entropy loss is given by \(l^{\prime}(x)=-\frac{1}{1+\exp(x)}\) and we define \(g(x)=\frac{1}{1+\exp(x)}\). The model is trained by gradient flow to minimize the following empirical loss:

\[\widehat{\mathcal{L}}(W,W_{V},W_{K},W_{Q})=\frac{1}{n}\sum_{i=1}^{n}l(y_{i}F(X^ {(i)};W,W_{V},W_{K},W_{Q})).\] (2)

[MISSING_PAGE_FAIL:5]

* \(\mu_{1}^{\top}{W_{V}^{(t)}}^{T}W_{V}^{(t)}\mu_{2}\) _increases, whereas_ \(\mu_{1}^{\top}{W_{V}^{(t)}}^{\top}W_{V}^{(t)}\mu_{3}\) _and_ \(\mu_{2}^{\top}{W_{V}^{(t)}}^{\top}W_{V}^{(t)}\mu_{3}\) _decrease._
* _Linear MLP functions satisfy:_ \(G^{(t)}(\mu_{1})\geq\Omega(1)\)_,_ \(G^{(t)}(\mu_{2})\geq\Omega(1)\)_,_ \(-G^{(t)}(\mu_{3})\leq\Omega(1)\)_._
* \(G^{(t)}(\mu_{1})+G^{(t)}(\mu_{2})+G^{(t)}(\mu_{3})\geq\Omega(1)\)_,_ \(G^{(t)}(\mu_{1})+G^{(t)}(\mu_{3})\leq-\Omega(1)\) _and_ \(G^{(t)}(\mu_{2})+G^{(t)}(\mu_{3})\leq-\Omega(1)\)_._

In Theorem 3.2, item 1 indicates that, during Phase 2, gradient flow drives the self-attention module to weigh more between the two target signals \(\mu_{1}\) and \(\mu_{2}\), and to weigh less between one of these signals and the common token \(\mu_{3}\). Item 2 indicates that gradient flow drives the value matrix \(W_{V}\) to positively align the two target signals \(\mu_{1}\) and \(\mu_{2}\), but negatively align one target signal (\(\mu_{1}\) or \(\mu_{2}\)) with the common token \(\mu_{3}\). Further, the last two items indicate that the MLP continue to classify correctly and further enlarge the classification margin. Hence, all items in Theorem 3.2 collectively indicate that attention and MLP evolve jointly to enlarge the classification margin and hence drive the loss value to decrease in Phase 2.

**Theorem 3.3** (Near Minimum Training Loss and Attention).: _With probability at least \(1-\delta\), there exists a time \(T^{\star}=\Theta(\text{poly}(m))\) such that_

* _The training and generalization losses satisfy_ \(\widehat{L}^{(T^{\star})}\leq 1/\text{poly}(m)\) _and_ \(L^{(T^{\star})}\leq 1/\text{poly}(m)\)_._
* _The attention matrices satisfies:_ \[{W_{K}^{(T^{\star})}}^{\top}{W_{Q}^{(T^{\star})}}={W_{K}^{(0)}}^{\top}{W_{Q}^{ (0)}}+\sum_{i_{1},i_{2}\in[d]}{C_{i_{1},i_{2}}^{(T^{\star})}}\mu_{i_{1}}\mu_{i _{2}}^{\top},\] (3) _where_ \(C_{1,2}^{(T^{\star})},C_{2,1}^{(T^{\star})},-C_{3,1}^{(T^{\star})},-C_{3,2}^ {(T^{\star})}=\Theta\left(\frac{\sigma_{0}^{2}m}{\sqrt{mL}\sigma_{1}^{\top}mm_ {1}}+\frac{\sigma_{0}^{2}\sqrt{m}}{\sqrt{m}\sigma_{1}^{\top}mm_{1}}\right)\) _and_ \(C_{i_{1},i_{2}}^{(T^{\star})}\leq\widetilde{O}\left(\frac{\sigma_{0}^{2}m}{n \sqrt{mL}\sigma_{1}^{\top}mm_{1}}+\frac{\sigma_{0}^{2}\sqrt{m}}{\sqrt{m}\sigma _{1}^{\top}mm_{1}}\right)\) _if one of_ \(i_{1},i_{2}\in[d]\setminus[3]\)_._

Theorem 3.3 indicates that both training and test losses converge nearly to zero as long as the embedding dimension \(m\) is sufficiently large, because both the attention and MLP matrices are trained towards enlarging the classification margin in Phase 2. Theorem 3.3 also provides the explicit form of the attention matrix in Equation (3), in which the second term captures the learned information of the self-attention module. It can be seen that the large coefficients \(C_{1,2}^{(T^{\star})}\) and \(C_{2,1}^{(T^{\star})}\) capture strong coupling of the two target signals \(\mu_{1}\) and \(\mu_{2}\), and the large negative coefficients \(C_{3,1}^{(T^{\star})}\) and \(C_{3,2}^{(T^{\star})}\) encourages strong negative coupling of one target signal \(\mu_{1}\) or \(\mu_{2}\) and the common token \(\mu_{3}\). All these attention terms contribute to enlarge correct classification margin. Further, the coefficients between all other random tokens are order-level smaller and hence do not corrupt the correct classification.

**Synthetic Experiment:** We next verify our theory and the two-phase characterization of the training process via synthetic experiments (see the experiment setup in Appendix A).

Figure 1 (a) shows how the attention score correlation \(\mu_{i}^{\top}{W_{K}^{(t)}}^{\top}{W_{Q}^{(t)}}\mu_{j}\) evolves during the training. It is clear that these scores do not change significantly in Phase 1, verifying Theorem 3.1. In Phase 2, the score correlation between two target signals \(\mu_{1}\) and \(\mu_{2}\) increases, and the score between one target signal and the common token decreases, verifying Theorem 3.2.

Figure 1: Synthetic experiments with illustration of two training phases. The detailed experiment setup can be found in Appendix A.

Figure 1 (b) plots how the training loss changes during the two phases of training. The blue curve (indexed by 'loss') represents the overall training loss of all samples. The other curves correspond to the training loss of four types of samples as indicated in the legend. In Phase 1, the training loss for samples with both target signals (i.e., orange curve) decreases because the linear MLP layer aligns with the target signals (verifying Lemma 4.1 in Section 4.1). The training loss for samples with one target signal and the common token (i.e., green or red curves) first increases because the linear MLP layer initially has not aligned negatively enough with the common token yet (as captured by Lemma 4.2 in Section 4.1), and then decreases in the later stage of Phase 1 when the MLP layer aligns negatively with the common token (as captured by Lemma 4.3 in Section 4.1). All loss functions decrease in Phase 2 because all attention matrices and linear MLP jointly enlarge the classification margin, verifying Theorem 3.2.

## 4 Proof Outline: Two-phase Gradient Flow Analysis

### Phase 1: Alignment of Linear MLP for Correct Classification

In Phase 1, the linear MLP quickly aligns with the two target word tokens while all attention matrices stay roughly unchanged from their initialization values. We show that the linear MLP functions \(|G^{(t)}(\mu_{1})|,|G^{(t)}(\mu_{2})|,|G^{(t)}(\mu_{3})|\) become sufficiently large (larger than some constant threshold) so that all training samples are correctly classified at the end of Phase 1.

We first analyze the dynamical system at the initialization. In particular, the following lemma shows that at the initialization, the linear MLP layer receives a sufficiently large gradient from the two target signals, and hence samples with the two target signals will be classified correctly as co-occurrence soon after the training starts.

**Lemma 4.1** (Same as Lemma E.4).: _With probability at least \(1-\delta\) over the weight initialization,_

\[\forall\mu\in\{\mu_{1},\mu_{2}\}:\quad\frac{\partial a_{j}w_{j}^{(0)}W_{V}^{( 0)}\mu}{\partial t}=\Theta((\sigma_{0}^{2}+\sigma_{1}^{2})m).\]

Further, by the definition of Phase 1 (see Definition E.2 for a formal definition), the gradients of the attention matrices in the dynamical system are much smaller than that of linear MLP given in Lemma 4.1. This implies that during Phase 1, mainly the linear MLP is performing learning, whereas all the attention matrices are changing slowly from their initialization. Based on this, we have \(\frac{\partial}{\partial t}G^{(t)}(\mu_{1})=\Theta((\sigma_{0}^{2}+\sigma_{1}^ {2})mm_{1})\) which implies that it takes only \(O(1/(\sigma_{0}^{2}+\sigma_{1}^{2})mm_{1})\) iterations for \(G^{(t)}(\mu_{1})\) to reach a certain constant magnitude.

Lemma 4.1 indicates that the samples with co-occurrence of the two target signals are classified correctly. The following lemma shows that the initial gradient from the common token, i.e., the gradient of \(G^{(t)}(\mu_{3})\), is much smaller than the gradient from the two target signals, which implies that the samples with only one target signal may be classified _incorrectly_ as co-occurrence (since the network in this case will output a positive value). This is verified empirically by our experiments in Figure 1 (b), where the loss function corresponding to only one target signal and the common token first increases in Phase 1.

**Lemma 4.2** (Same as Lemma E.16).: _Let \(F=\max_{i}|F_{i}^{(0)}|\). With probability at least \(1-\delta\) over the weight initialization,_

\[\left|\frac{\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\mu_{3}}{\partial t}\right|= \widetilde{O}\left(\sigma_{1}^{2}\sqrt{mm_{1}}+\sigma_{0}^{2}\sqrt{mL}+ \sigma_{1}^{2}mF\right).\]

Notice that the model output \(F\) depends on the weight initialization scale and can be made small.

We next show in the following lemma that the gradient \(\frac{\partial G^{(t)}(\mu_{3})}{\partial t}\) of the common token will quickly become negative soon after the training begins, which drives the transformer model to output a negative value when it sees those types of samples. This implies that negative samples (without co-occurrence of two target tokens) will be classified correctly towards the end of Phase 1. This is also verified empirically by our experiments in Figure 1 (b), where the loss function corresponding to only one target signal and the common token descreases towards the end of Phase 1.

**Lemma 4.3** (Abbreviated from Theorem E.19).: _There exists a time \(T_{0.5}\leq T_{1}\) and a constant \(C\) such that for all \(t\in[T_{0.5},T_{1}]\)_

\[(1+C)\max\left(\frac{\partial G^{(t)}(\mu_{1})}{\partial t},\frac{\partial G^ {(t)}(\mu_{2})}{\partial t}\right)\leq-\frac{\partial G^{(t)}(\mu_{3})}{ \partial t}\leq(1-C)\left(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}+\frac{ \partial G^{(t)}(\mu_{2})}{\partial t}\right).\]Proof Intuition of Lemma 4.3.: We first note that the term

\[\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\sum_{l_{2}=1}^{L}\sum_{j_{2}=1}^{m_{1}} \|w_{j_{2}}^{(0)}\|_{2}^{2}(X^{(i)}p_{l_{2}}^{(0,i)})^{\top}\mu\]

makes the major contribution to the gradient \(\frac{\partial G^{(i)}(\mu_{3})}{\partial t}\). Such a term is small at the initialization due to the cancellation effect from positive and negative \(y_{i}\)'s. However, since the linear MLP \(G\) will positively align the two target signals at the beginning, for the samples with positive labels, \(g_{i}^{(t)}\) will decrease, whereas for samples with only one target signal, \(g_{i}^{(t)}\) will increase. Hence, \(\frac{\partial a_{j}w_{j_{1}}^{(t)}W_{V}^{(t)}\mu_{3}}{\partial t}\) will become negative. This trend will continue until the gradient from \(\mu_{3}\) starts to match the gradients from \(\mu_{1},\mu_{2}\), which is what we establish in Lemma 4.3. 

Using Lemma 4.3, we can show that all training samples are correctly classified at end of Phase 1.

### Phase 2: Evolution of Attention and MLP for Large Classification Margin

In Phase 2, both attention and MLP matrices evolve towards enlarging the classification margin, thus driving the loss value small.

We now analyze what happens in Phase 2. Let \(T_{2}\) denote the end of Phase 2. Recall that at the end of Phase 1, we have \(G^{(t)}(\mu_{1}),G^{(t)}(\mu_{2}),-G^{(t)}(\mu_{3})\geq\Omega(1)\). We will mainly need to show that such a condition continues to hold in Phase 2, so that attention matrices will evolve with MLP to learn better classifiers. To this end, we exam the following gradient flow in the dynamical system:

\[\frac{\partial G^{(t)}(\mu)}{\partial t}= \frac{1}{n}\sum_{i_{2}:\ \mu\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}\sum_{l_{2}=1}^{L}p_{q \gets l_{2},k\leftarrow\mu}^{(t,i_{2})}\cdot\sum_{j_{1}=1}^{m_{1}}\sum_{j _{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle\] (4) \[+\frac{m_{1}}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\sum_{l_ {1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)}\mu.\]

It has been proved that at the end of Phase 1, for \(\mu\in\{\mu_{1},\mu_{2},\mu_{3}\}\), we have \(\left|\sum_{j_{1}}\frac{\partial w_{j_{1}}^{(t)\top}}{\partial t}W_{V}^{(t)} \mu\right|\ll\left|\sum_{j_{1}}w_{j_{1}}^{(t)\top}\frac{\partial W_{V}^{(t)}} {\partial t}\mu\right|\) since the magnitude of \(\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{( t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\) is large. Assume this can hold for long enough (which we can indeed prove later). Then, we only need to focus on the first term in the sum on the right-hand side in Equation (4). On the other hand, from the dynamical system, we can calculate

\[\frac{\partial}{\partial t}\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left \langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle=\frac{2 m_{1}}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}F_{i}^{(t)}.\] (5)

Thus, if \(y_{i}F_{i}^{(t)}>0\) for all \(i\in[n]\), then \(\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{( t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\) is always increasing. Thus, \(\frac{\partial G^{(t)}(\mu)}{\partial t}\) mainly depends on the behavior of \(\frac{1}{n}\sum_{i_{2}:\ \mu\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}\sum_{l_{2}=1}^{L}p_{q \gets l_{2},k\leftarrow\mu}^{(t,i_{2})}\). Further, this is also a key quantity we need to analyze \(\frac{\partial\nu^{\top}W_{V}^{(t)\top}W_{V}^{(t)}}{\partial t}\) and \(\frac{\partial\nu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}}{\partial t}\). Note that \(\frac{1}{n}\sum_{i_{2}:\ \mu\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}\sum_{l_{2}=1}^{L}p_{ q\gets l_{2},k\leftarrow\mu}^{(t,i_{2})}\approx\frac{1}{n}\sum_{i_{2}:\ \mu\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}\) if \(p_{q\gets l_{2},k\leftarrow\mu}^{(t,i_{2})}\approx 1/L\) which holds at the beginning of Phase 2. Later, we are going to prove convergence of the training loss via the following: (1) the training loss can decrease if the softmax probability is uniform; (2) even though the softmax probability will deviate from uniform distribution during training, we can bound such deviation and the loss value can still decrease.

**Automatic balancing of gradients.** As argued above, our main focus is on analyzing the behavior of \(\frac{1}{n}\sum_{i_{2}:\ \mu\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}\). This consists of two parts: (i) Lemma 4.4, which shows that the two groups of samples with only the presence of one target signal have gradients \(\sum_{i\in I_{2}}g_{i}^{(t)}\) and \(\sum_{i\in I_{3}}g_{i}^{(t)}\) close to each other during training; and (ii) Lemma 4.5, which shows that the gradient gaps \(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}\) and \(\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}-\sum_{i\in I_{1}}g_{i}^{(t)}\) are not too small compared with \(\sum_{i\in[n]}g_{i}^{(t)}\). BothLemmas 4.4 and 4.5 establish that the ratio of those important gradients are kept within certain ranges during training. We call such a key property as _automatic balancing of gradients_, which is further used for proving that the gradient flow can drive the training loss small.

**Lemma 4.4** (Same as Lemma F.5).: _For \(t\in[T_{1},T_{2}]\), there exists a small constant \(C\ll 1\) such that_

\[\frac{\left|\sum_{i\in I_{2}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}\right| }{\min(\sum_{i\in I_{2}}g_{i}^{(t)},\sum_{i\in I_{3}}g_{i}^{(t)})}\leq C.\]

Proof Intuition of Lemma 4.4.: The intuition behind the result is as follows. If \(\sum_{i\in I_{2}}g_{i}^{(t)}\) becomes much bigger than \(\sum_{i\in I_{3}}g_{i}^{(t)}\) during the training, then \(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}\) is much smaller than \(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}\) which makes \(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}<\frac{\partial G^{(t)}(\mu_{2})} {\partial t}\). It is not hard to show that random tokens make negligible contributions to the gradient. Thus, for \(i\in I_{2}\), we have \(\frac{\partial F_{i}^{(t)}}{\partial t}\approx\frac{\partial G^{(t)}(\mu_{1} )}{\partial t}+\frac{\partial G^{(t)}(\mu_{3})}{\partial t}\). By the chain rule, we have \(\frac{\partial g_{i}^{(t)}}{\partial t}=g^{\prime}(y_{i}F_{i}^{(t)})y_{i} \frac{\partial F_{i}^{(t)}}{\partial t}\). Since \(\frac{\partial G^{(t)}(\mu_{3})}{\partial t}<0\), if \(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}<\frac{\partial G^{(t)}(\mu_{2})} {\partial t}\), then \(\sum_{i\in I_{2}}g_{i}^{(t)}\) will drop faster than \(\sum_{i\in I_{3}}g_{i}^{(t)}\), i.e., \(-\frac{\partial}{\partial t}\sum_{i\in I_{2}}g_{i}^{(t)}>-\frac{\partial}{ \partial t}\sum_{i\in I_{3}}g_{i}^{(t)}\). In Appendix, we formally prove Lemma 4.4 by analyzing the ratio \(\sum_{i\in I_{2}}g_{i}^{(t)}/\sum_{i\in I_{3}}g_{i}^{(t)}\), and show that this ratio hangs over around \(1\) during training. 

**Lemma 4.5** (Abbreviated from Lemma F.6).: _For \(t\in[T_{1},T_{2}]\), the gradient satisfies that_

\[\frac{\sum_{i\in[n]}g_{i}^{(t)}}{\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{ (t)}-\sum_{i\in I_{1}}g_{i}^{(t)}}=O(1),\qquad\quad\frac{\sum_{i\in[n]}g_{i}^ {(t)}}{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}=O(1).\]

_Further, for some constant \(C\), we have_

\[(1+C)\max\left(\frac{\partial G^{(t)}(\mu_{1})}{\partial t},\frac{\partial G^ {(t)}(\mu_{2})}{\partial t}\right)\leq-\frac{\partial G^{(t)}(\mu_{3})}{ \partial t}\leq(1-C)\left(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}+\frac {\partial G^{(t)}(\mu_{2})}{\partial t}\right).\]

Proof Sketch of Lemma 4.5.: The proof of Lemma 4.5 relies on analyzing how the ratio between \(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}\) and \(-\frac{\partial G^{(t)}(\mu_{3})}{\partial t}\) changes. We show that this ratio will hang over around some range. Recall the relationship that \(\frac{\partial G^{(t)}(\mu)}{\partial t}\approx\frac{1}{n}\sum_{i_{2}:\ \mu\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}\cdot\sum_{j_{1}=1}^{m_{1}}\sum_{j_{ 2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle\). It is not hard to show that

\[\frac{-\frac{\partial G^{(t)}(\mu_{3})}{\partial t}}{\frac{\partial G^{(t)}( \mu_{1})}{\partial t}}\approx\frac{-\sum_{i\in I_{1}}g_{i}^{(t)}+\sum_{i\in I _{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}}{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{2}}g_{i}^{(t)}}.\]

Define \(R(t):=\frac{-\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}\cup I_{3}\cup I_{4 }}g_{i}^{(t)}}{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}\). Solving when \(\frac{\partial}{\partial t}R(t)\geq 0\) yields a quadratic inequality, and further analysis shows that the root is contractive and is within some specific range. 

Utilizing the _gradient automatic balancing_ properties, the following corollary characterizes how the attention matrices in the dynamical system change in Phase 2. In particular, we can show that after \(W_{V}\)-transform, \(\mu_{1}\) and \(\mu_{2}\) become more positively correlated whereas \(\mu_{1}\) and \(\mu_{3}\) (also \(\mu_{2}\) and \(\mu_{3}\)) become negatively correlated. This is a direct result following from updates of the dynamical system.

**Corollary 4.6** (Abbreviated from Corollary F.13).: _For \(t\in[T_{1},T_{2}]\),_

\[\frac{\partial}{\partial t}\mu_{2}^{\top}W_{V}^{(t)\top}W_{V}^{(t)}\mu_{1}>0, \qquad\qquad\qquad\frac{\partial}{\partial t}\mu_{1}^{\top}W_{V}^{(t)\top}W_{V}^ {(t)}\mu_{3}<0.\]

Since we have analyzed how \(G^{(t)}(\cdot)\) will change in stage 2, we can utilize this information to analyze the change of softmax attention via the following relationship: by Appendix C, we can derive

\[\frac{\partial\mu_{1}^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu_{2}}{\partial t}\]\[=\frac{1}{n\sqrt{m}}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}\mu_{1 }^{\top}{W_{K}^{(t)}}^{\top}K^{(t,i)}\cdot\text{diag}\left(G^{(t)}(X^{(i)})-(G^{(t )}(X^{(i)}))^{\top}p_{l}^{(t,i)}\right)p_{l}^{(t,i)}x_{l}^{(i)\top}\mu_{2}\] \[+\frac{1}{n\sqrt{m}}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \mu_{2}^{\top}{W_{Q}^{(t)}}^{\top}q_{l}^{(t,i)}p_{l}^{(t,i)}{}^{\top}\cdot \text{diag}\left(G^{(t)}(X^{(i)})-(G^{(t)}(X^{(i)}))^{\top}p_{l}^{(t,i)}\right) X^{(i)}{}^{\top}\mu_{1}.\]

The following lemma shows that the attention score between the two target signals \(\mu_{1}\) and \(\mu_{2}\) increases, whereas that between one target signal \(\mu_{1}\) or \(\mu_{2}\) and the common token \(\mu_{3}\) decreases.

**Lemma 4.7** (Abbreviated from Lemma F.16).: _For \(\mu,\nu\in\{\mu_{1},\mu_{2}\},\ \mu\neq\nu\), and for \(t\in[T_{1},T_{2}]\),_

\[\frac{\partial}{\partial t}\nu^{\top}{W_{K}^{(t)}}^{\top}{W_{Q}^{(t)}}\mu= \frac{1}{\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2}m)\frac{1 }{L},\qquad\frac{\partial}{\partial t}\mu_{3}^{\top}{W_{K}^{(t)}}^{\top}{W_{ Q}^{(t)}}\mu=-\frac{1}{\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2}m) \frac{1}{L}.\]

Lemma 4.7 is keeping track of the attention coefficients \(C_{i_{1},i_{2}}^{(t)}\) in Theorem 3.3 via gradient flow, which proves the second item of Theorem 3.3.

## 5 Discussion and Future Directions

In this work, we developed a novel gradient flow based framework for analyzing the training dynamics of a one-layer transformer to recognize co-occurring tokens. We provided a two-phase characterization of the training process. In Phase 1, the linear MLP layer is trained to classify samples correctly, with attention weights almost unchanged. In Phase 2, both attention matrices and the linear MLP jointly evolve to enlarge the classification margin, thus reducing the loss to near minimum.

As future work, it will be interesting to analyze more general transformer architectures such as multi-headed attention, multi-layer transformer, etc. Further, it is of interest to study the dynamics of more advanced gradient descent algorithms such as gradient descent with adaptive learning rate, with momentum, etc., and explore how the hyperparameters will affect the training dynamics. Another direction is to study more practical language sequences where tokens are generated in a correlated fashion. Then the next token prediction becomes an intriguing problem.

## Acknowledgement

H. Yang would like to thank Jason D. Lee and Yunwei Ren for insightful discussion and suggestions. This work was performed under the auspices of the U.S. Department of Energy by the Lawrence Livermore National Laboratory under Contract No. DE-AC52-07NA27344 and supported by the LLNL-LDRD Program under Project No. 22-SI-004 and 24-ERD-010. The work of Y. Liang was supported in part by the U.S. National Science Foundation under the grants ECCS-2113860 and DMS-2134145. The work of Z. Wang was in part supported by an NSF Scale-MoDL grant (award number: 2133861) and the CAREER Award (award number: 2145346).

## References

* [ACDS24] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [ADF\({}^{+}\)23] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.
* [BCE\({}^{+}\)23] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* [BCW\({}^{+}\)23] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. _arXiv preprint arXiv:2306.04637_, 2023.

* [CL24] Sitan Chen and Yuanzhi Li. Provably learning a multi-head attention layer. _arXiv preprint arXiv:2402.04084_, 2024.
* [CSWY24] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. _arXiv preprint arXiv:2402.19442_, 2024.
* [Dam18] Friederick J Damerau. _Markov models and linguistic theory_, volume 95. Walter de Gruyter GmbH & Co KG, 2018.
* [DBK\({}^{+}\)20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2020.
* [DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [EGKZ22] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_, pages 5793-5831. PMLR, 2022.
* [HCL23] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. _arXiv preprint arXiv:2310.05249_, 2023.
* [HWCL24] Yu Huang, Zixin Wen, Yuejie Chi, and Yingbin Liang. Transformers provably learn feature-position correlations in masked image modeling. _arXiv preprint arXiv:2403.02233_, 2024.
* [JSL22] Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. _Advances in Neural Information Processing Systems_, 35:37822-37836, 2022.
* [LAG\({}^{+}\)22] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. In _The Eleventh International Conference on Learning Representations_, 2022.
* [LLC\({}^{+}\)21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* [LLR23] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. _arXiv preprint arXiv:2303.04245_, 2023.
* [LWL\({}^{+}\)24] Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin-Yu Chen. Training nonlinear transformers for efficient in-context learning: A theoretical learning and generalization analysis. _arXiv preprint arXiv:2402.15607_, 2024.
* [LWLC22] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. In _The Eleventh International Conference on Learning Representations_, 2022.
* [LWM\({}^{+}\)23] Hongkang Li, Meng Wang, Tengfei Ma, Sijia Liu, Zaixi Zhang, and Pin-Yu Chen. What improves the generalization of graph transformer? a theoretical dive into self-attention and positional encoding. _NeurIPS 2023 Workshop: New Frontiers in Graph Learning, 2023_, 2023.
* [MH23] James Manyika and Sissie Hsiao. An overview of bard: an early experiment with generative ai. _AI. Google Static Documents_, 2, 2023.
* [MS99] Christopher Manning and Hinrich Schutze. _Foundations of statistical natural language processing_. MIT press, 1999.

* [NDL24] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. _arXiv preprint arXiv:2402.14735_, 2024.
* [Ope23] OpenAI. Gpt-4 technical report, 2023.
* [PBM21] Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing complete. _The Journal of Machine Learning Research_, 22(1):3463-3497, 2021.
* [SHT23] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers. _arXiv preprint arXiv:2306.02896_, 2023.
* [Tea23] Gemini Team. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [Thr24] Christos Thrampoulidis. Implicit bias of next-token prediction. _arXiv:2402.18551_, 2024.
* [TLI\({}^{+}\)23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [TLTO23] Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support vector machines. In _NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning_, 2023.
* [TLZO23] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [TWCD23] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. _arXiv preprint arXiv:2305.16380_, 2023.
* [TWZ\({}^{+}\)23] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Shaolei Du. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. In _The Twelfth International Conference on Learning Representations_, 2023.
* [VDT24] Bhavya Vasudeva, Puneesh Deora, and Christos Thrampoulidis. Implicit bias and fast convergence rates for self-attention. _arXiv preprint arXiv:2402.05738_, 2024.
* [VONR\({}^{+}\)23] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174. PMLR, 2023.
* [VSP\({}^{+}\)17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [WCM22] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. _Advances in Neural Information Processing Systems_, 35:12071-12083, 2022.
* [WLCC23] Yongtao Wu, Fanghui Liu, Grigorios Chrysos, and Volkan Cevher. On the convergence of encoder-only shallow transformers. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [WXM21] Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning. _Advances in Neural Information Processing Systems_, 34:16158-16170, 2021.
* [YBR\({}^{+}\)19] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In _International Conference on Learning Representations_, 2019.

* [ZFB24] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. _Journal of Machine Learning Research_, 25(49):1-55, 2024.
* [ZPGA23] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while predicting the masked word? _arXiv preprint arXiv:2303.08117_, 2023.
* [ZZYW23] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. _arXiv preprint arXiv:2305.19420_, 2023.

###### Contents

* 1 Introduction
	* 1.1 Related Work
* 2 Problem Setting
	* 2.1 Data Model
	* 2.2 Transformer Architecture and Training
* 3 Main Results
* 4 Proof Outline: Two-phase Gradient Flow Analysis
	* 4.1 Phase 1: Alignment of Linear MLP for Correct Classification
	* 4.2 Phase 2: Evolution of Attention and MLP for Large Classification Margin
* 5 Discussion and Future Directions
* A Setup of Synthetic Experiment
* B Gradient Flow Update for Weight Matrices
* C Gradient Flow Dynamical System
* C.1 Derivation of the Dynamical System
* D Initialization
* E Training Dynamics: Phase 1
* E.1 Initial Gradients
* E.2 Maximum Perturbation of Neuron Outputs
* E.3 Perturbation Term Involving Correlation of Value-transformed Data
* E.4 Perturbation Term Involving Correlation of Neurons
* E.5 Neuron Weights Align with Signal Value
* E.6 Alignment of Common Token
* E.7 Small Score Movement in Phase 1
* E.8 All Variables are within Range in Definition of Phase 1
* F Training Dynamics: Phase 2
* F.1 Automatic Balancing of Gradients
* F.2 How Fast the Loss Decreases
* F.3 Growth of Neuron Correlation
* F.4 Growth of Correlation of Value-Transformed Data
* F.5 Change of Random-Token Sub-Network
* F.6 Change of Score and Softmax Probability
* F.7 Change of Self-Correlation of Key/Query-Transformed data

F.8 Small Loss is Achieved F.9 Proof of Theorem 3.2 F.10 Proof of Theorem 3.3
* G Auxiliary Results
* H ProbabilitySetup of Synthetic Experiment

We conduct synthetic experiment to verify our theoretical results. We create a dataset following our data distribution in Definition 2.1 with 60 training samples: 30 samples have both \(\mu_{1}\) and \(\mu_{2}\) in it, 10 samples have only \(\mu_{1}\), 10 samples have only \(\mu_{2}\), and 10 samples have neither \(\mu_{1}\) nor \(\mu_{2}\). Each data consists of 5 patches and each patch has dimension 64. The embedding dimension \(m\) is set to be 128 and the number of neurons is set to be 256. We use Kaiming initialization to initialize the transformer weights. The transformer is trained by gradient descent with learning rate 0.01 for 30000 epochs.

## Appendix B Gradient Flow Update for Weight Matrices

We provide the gradient flow update for each weight matrix as follows.

\[\frac{\partial w_{j}^{(t)}}{\partial t} =\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}a_{j}V^{( t,i)}p_{l}^{(t,i)}=\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}a_{j} \sum_{h=1}^{L}v_{h}^{(t,i)}p_{l,h}^{(t,i)}\] \[\frac{\partial W_{V}^{(t)}}{\partial t} =\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}\sum_{j=1 }^{m_{1}}a_{j}w_{j}^{(t)}\left(X^{(i)}p_{l}^{(t,i)}\right)^{\top}\] \[\frac{\partial W_{K}^{(t)}}{\partial t} =\frac{1}{n\sqrt{m}}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}\sum_{h=1}^{L}w_{j}^{(t)\top}v_{h}^{(t,i)}\sum_{h^{ \prime}=1}^{L}\frac{\partial p_{l,h}^{(t,i)}}{\partial s_{l,h^{\prime}}}q_{l}^ {(t,i)}x_{h^{\prime}}^{(i)\top}\] \[=\frac{1}{n\sqrt{m}}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}\sum_{h=1}^{L}w_{j}^{(t)\top}v_{h}^{(t,i)}\sum_{h^{ \prime}=1}^{L}p_{l,h}^{(t,i)}(\mathbb{I}(h=h^{\prime})-p_{l,h^{\prime}}^{(t,i) })q_{l}^{(t,i)}x_{h^{\prime}}^{(i)\top}\] \[=\frac{1}{n\sqrt{m}}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}\left(-w_{j}^{(t)\top}V^{(t,i)}p_{l}^{(t,i)}q_{l}^{(t,i )}(X^{(i)}p_{l}^{(t,i)})^{\top}+q_{l}^{(t,i)}w_{j}^{(t)\top}V^{(t,i)}\text{ diag}(p_{l}^{(t,i)})X^{(i)\top}\right)\] \[=\frac{1}{n\sqrt{m}}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}q_{l}^{(t,i)}\left(-w_{j}^{(t)\top}V^{(t,i)}p_{l}^{(t,i )}p_{l}^{(t,i)\top}+w_{j}^{(t)\top}V^{(t,i)}\text{diag}(p_{l}^{(t,i)})\right) X^{(i)\top}\] \[=\frac{1}{n\sqrt{m}}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}q_{l}^{(t,i)}p_{l}^{(t,i)\top}\text{diag}\left(w_{j}^{( t)\top}V^{(t,i)}-w_{j}^{(t)\top}V^{(t,i)}p_{l}^{(t,i)}\right)X^{(i)\top}\] \[\frac{\partial W_{Q}^{(t)}}{\partial t} =\frac{1}{n\sqrt{m}}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}\sum_{h=1}^{L}w_{j}^{(t)\top}v_{h}^{(t,i)}\sum_{h^{ \prime}=1}^{L}p_{l,h}^{(t,i)}(\mathbb{I}(h=h^{\prime})-p_{l,h^{\prime}}^{(t,i)} k_{h^{\prime}}^{(t,i)}x_{l}^{(i)\top}\] \[=\frac{1}{n\sqrt{m}}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}\left(-w_{j}^{(t)\top}V^{(t,i)}p_{l}^{(t,i)}(K^{(t,i)}p_ {l}^{(t,i)})+K^{(t,i)}\text{diag}(p_{l}^{(t,i)})V^{(t,i)\top}w_{j}^{(t)}\right) x_{l}^{(i)\top}\] \[=\frac{1}{n\sqrt{m}}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}K^{(t,i)}\left(-w_{j}^{(t)\top}V^{(t,i)}p_{l}^{(t,i)}p_ {l}^{(t,i)}+\text{diag}(p_{l}^{(t,i)})V^{(t,i)\top}w_{j}^{(t)}\right)x_{l}^{(i) \top}\] \[=\frac{1}{n\sqrt{m}}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}K^{(t,i)}\text{diag}\left(V^{(t,i)\top}w_{j}^{(t)}-w_{j }^{(t)\top}V^{(t,i)}p_{l}^{(t,i)}\right)p_{l}^{(t,i)}x_{l}^{(i)\top}\]

## Appendix C Gradient Flow Dynamical System

We first provide our complete dynamical system. The derivation of each equation is provided in Appendix C.1.

\[\frac{\partial w_{j_{1}}^{(t)\top}W_{V}^{(t)}\mu}{\partial t}\]\[\frac{\partial w_{j_{1}}^{(t)\top}W_{V}^{(t)}\mu}{\partial t} =\frac{1}{n}\sum_{i_{2}=1}^{n}g_{i_{2}}^{(t)}y_{i_{2}}\sum_{l_{2}=1 }^{L}\sum_{j_{2}=1}^{m_{1}}a_{j_{2}}\Big{\langle}w_{j_{1}}^{(t)}w_{j_{2}}^{(t) \top}W_{V}^{(t,i)}p_{l}^{(t,i)}+\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_ {l=1}^{L}a_{j_{1}}w_{j_{2}}^{(t)\top}V^{(t,i)}p_{l}^{(t,i)}\] \[\frac{\partial\nu^{\top}W_{V}^{(t)\top}W_{V}^{(t)}\mu}{\partial t }\] \[=\frac{1}{n}\sum_{i;\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q\gets l,k \leftarrow\mu}+\frac{1}{n}\sum_{i;\nu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L }\sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q\gets l,k \leftarrow\nu}\] \[\frac{\partial\nu^{\top}W_{Q}^{(t)\top}W_{Q}^{(t)}\mu}{\partial t }\] \[=\frac{1}{n\sqrt{m}}\sum_{i;\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1 }^{m_{1}}a_{j}\nu^{\top}W_{Q}^{(t)\top}K^{(t,i)}\cdot\text{diag}\left(V^{(t,i) \top}w_{j}^{(t)}-w_{j}^{(t)\top}V^{(t,i)}p_{l(i,\mu)}^{(t,i)}\right)p_{l(i,\mu )}^{(t,i)}\] \[\quad+\frac{1}{n\sqrt{m}}\sum_{i;\nu\in X^{(i)}}g_{i}^{(t)}y_{i} \sum_{l=1}^{m_{1}}a_{j}\mu^{\top}W_{Q}^{(t)\top}K^{(t,i)}\cdot\text{diag}\left( V^{(t,i)\top}w_{j}^{(t)}-w_{j}^{(t)\top}V^{(t,i)}p_{l(i,\nu)}^{(t,i)}\right)p_{l(i, \nu)}^{(t,i)}\] \[\frac{\partial\nu^{\top}W_{K}^{(t)\top}W_{K}^{(t)}\mu}{\partial t }\] \[=\frac{1}{n\sqrt{m}}\sum_{i;\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1 }^{L}\sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{K}^{(t)\top}q_{l}^{(t,i)}p_{q\gets l,k\leftarrow\mu}^{(t,i)}\cdot\left(w_{j}^{(t)\top}v^{(t,i)}(\mu)-w_{j}^{(t) \top}V^{(t,i)}p_{l}^{(t,i)}\right)\] \[+\frac{1}{n\sqrt{m}}\sum_{i;\nu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l =1}^{L}\sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{K}^{(t)\top}q_{l}^{(t,i)}p_{q\gets l,k\leftarrow\nu}^{(t,i)}\cdot\left(w_{j}^{(t)\top}v^{(t,i)}(\nu)-w_{j}^{(t) \top}V^{(t,i)}p_{l}^{(t,i)}\right)\]

### Derivation of the Dynamical System

**Lemma C.1**.: _Let \(\mu\in\{\mu_{i}\}_{i=1}^{d}\). For all \(j\in[m]\), we have_

\[\frac{\partial w_{j_{1}}^{(t)\top}W_{V}^{(t)}\mu}{\partial t} =\frac{1}{n}\sum_{i_{2}=1}^{n}g_{i_{2}}^{(t)}y_{i_{2}}\sum_{l_{2} =1}^{L}\sum_{j_{2}=1}^{m_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{ (t)}\right\rangle\left(X^{(i_{2})}p_{l_{2}}^{(t,i_{2})}\right)^{\top}\mu\] \[\quad+\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\sum_{l _{1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)}\mu\] \[=\frac{1}{n}\sum_{i;\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} a_{j}\left(\|w_{j}^{(t)}\|_{2}^{2}+\|v^{(t)}(\mu)\|_{2}^{2}\right)p_{q\gets l,k\leftarrow\mu}^{(t,i)}+\varepsilon,\]_where_

\[\varepsilon =\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)} \right\rangle p_{q\gets l,k\leftarrow\mu}^{(t,i)}\] \[\quad+\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l_{1}=1}^{L} a_{j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)},v^{(t)} (\mu)\right\rangle\mathbb{I}(v_{l_{2}}^{(t,i)}\neq v^{(t)}(\mu)).\]

Proof.: Let \(l(i,\mu)\) denote the index such that \(X_{l(i,\mu)}^{(i)}=\mu\). By the gradient flow update, we have

\[\frac{\partial w_{j_{1}}^{(t)\top}W_{V}^{(t)}\mu}{\partial t} =\frac{1}{n}\sum_{i_{2}=1}^{n}g_{i_{2}}^{(t)}y_{i_{2}}\sum_{l_{2}= 1}^{L}\sum_{j_{2}=1}^{m_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{( t)}\right\rangle\left(X^{(i_{2})}p_{l_{2}}^{(t,i_{2})}\right)^{\top}\mu\] \[\quad+\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\sum_{ l_{1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)}\mu\] \[=\frac{1}{n}\sum_{i_{2}:\,\mu\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_ {2}}\sum_{l_{2}=1}^{L}\sum_{j_{2}=1}^{m_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{( t)},w_{j_{2}}^{(t)}\right\rangle p_{q\gets l_{2},k\leftarrow\mu}^{(t,i_{2})}\] \[\quad+\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\sum_{ l_{1}=1}^{L}a_{j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(t,i_{1})}v_{l _{2}}^{(t,i_{1})},v^{(t)}(\mu)\right\rangle\] \[=\frac{1}{n}\sum_{i_{2}:\,\mu\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_ {2}}\sum_{l_{2}=1}^{L}\left(a_{j_{1}}\|w_{j_{1}}^{(t)}\|_{2}^{2}+\sum_{j_{2} \neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle \right)p_{q\gets l_{2},k\leftarrow\mu}^{(t,i_{2})}\] \[\quad+\frac{1}{n}\sum_{i_{1}:\mu\notin X^{(i_{1})}}g_{i_{1}}^{(t)} y_{i_{1}}\sum_{l_{1}=1}^{L}a_{j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{ (t,i_{1})}v_{l_{2}}^{(t,i_{1})},v^{(t)}(\mu)\right\rangle\] \[\quad+\frac{1}{n}\sum_{i_{1}:\mu\in X^{(i_{1})}}g_{i_{1}}^{(t)}y_{ i_{1}}\sum_{l_{1}=1}^{L}a_{j_{1}}\left(\|v^{(t)}(\mu)\|_{2}^{2}p_{q\gets l _{1},k\leftarrow\mu}^{(t,i_{1})}+\sum_{l_{2}\neq l(i_{1},\mu)}\left\langle p_ {l_{1},l_{2}}^{(t,i_{1})}v_{l_{2}}^{(t,i_{1})},v^{(t)}(\mu)\right\rangle\right)\] \[=\frac{1}{n}\sum_{i:\mu\in X^{(i_{1})}}g_{i}^{(t)}y_{i}\sum_{l=1}^ {L}a_{j_{1}}\left(\|w_{j_{1}}^{(t)}\|_{2}^{2}+\|v^{(t)}(\mu)\|_{2}^{2}\right)p_ {q\gets l,k\leftarrow\mu}^{(t,i)}\] \[\quad+\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^ {L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)} \right\rangle p_{q\gets l,k\leftarrow\mu}^{(t,i)}\] \[\quad+\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l_{1}=1}^{L} a_{j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)},v^{(t)} (\mu)\right\rangle\mathbb{I}(v_{l_{2}}^{(t,i)}\neq v^{(t)}(\mu)).\]

**Lemma C.2**.: _The following equation on the gradient flow holds:_

\[\frac{\partial\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle}{ \partial t} =\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}\left(\sum_{l^{ \prime}:X_{l^{\prime}}^{(i)}\in\mathcal{U}}a_{j_{2}}w_{j_{1}}^{(t)\top}V_{l^{ \prime}}^{(t,i)}p_{l,l^{\prime}}^{(t,i)}+a_{j_{1}}w_{j_{2}}^{(t)\top}V_{l^{ \prime}}^{(t,i)}p_{l,l^{\prime}}^{(t,i)}\right)\] \[\quad+\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \left(\sum_{l^{\prime}:X_{l^{\prime}}^{(i)}\notin\mathcal{U}}a_{j_{2}}w_{j_{1}}^{( t)\top}V_{l^{\prime}}^{(t,i)}p_{l,l^{\prime}}^{(t,i)}+a_{j_{1}}w_{j_{2}}^{(t) \top}V_{l^{\prime}}^{(t,i)}p_{l,l^{\prime}}^{(t,i)}\right).\]Proof.: By gradient flow update, we have

\[\frac{\partial\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle }{\partial t} =\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}a_{j_{2}}w_ {j_{1}}^{(t)\top}V^{(t,i)}p_{l}^{(t,i)}+\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_ {i}\sum_{l=1}^{L}a_{j_{1}}w_{j_{2}}^{(t)\top}V^{(t,i)}p_{l}^{(t,i)}\] \[=\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}\left( \sum_{l^{\prime}:X_{l^{\prime}}^{(i)}\in\mathcal{U}}a_{j_{2}}w_{j_{1}}^{(t) \top}V_{l^{\prime}}^{(t,i)}p_{l,l^{\prime}}^{(t,i)}+a_{j_{1}}w_{j_{2}}^{(t) \top}V_{l^{\prime}}^{(t,i)}p_{l,l^{\prime}}^{(t,i)}\right)\] \[\quad+\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \left(\sum_{l^{\prime}:X_{l^{\prime}}^{(i)}\notin\mathcal{U}}a_{j_{2}}w_{j_{1} }^{(t)\top}V_{l^{\prime}}^{(t,i)}p_{l,l^{\prime}}^{(t,i)}+a_{j_{1}}w_{j_{2}}^{ (t)\top}V_{l^{\prime}}^{(t,i)}p_{l,l^{\prime}}^{(t,i)}\right).\]

**Lemma C.3**.: _Let \(\mu,\nu\in\{\mu_{i}\}_{i=1}^{d}\). Then the following equation on gradient flow holds:_

\[\frac{\partial\nu W_{V}^{(t)\top}W_{V}^{(t)}\mu}{\partial t} =\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q\gets l,k \leftarrow\mu}^{(t,i)}\] \[\quad+\frac{1}{n}\sum_{i:\nu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1 }^{L}\sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q\gets l,k\leftarrow\nu}^{(t,i)}.\]

Proof.: The gradient flow update can be derived as follows:

\[\frac{\partial\nu W_{V}^{(t)\top}W_{V}^{(t)}\mu}{\partial t} =\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}\sum_{j=1 }^{m_{1}}a_{j}\nu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}\left(X^{(i)}p_{l}^{(t,i)} \right)^{\top}\mu\] \[\quad+\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}\left(X^{(i)}p_{l} ^{(t,i)}\right)^{\top}\nu\] \[=\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q\gets l,k \leftarrow\mu}^{(t,i)}\] \[\quad+\frac{1}{n}\sum_{i:\nu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1 }^{L}\sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q\gets l,k\leftarrow\nu}^{(t,i)}.\]

**Lemma C.4**.: _Let \(\mu,\nu\in\{\mu_{i}\}_{i=1}^{d}\). Then the following equations on gradient flow hold._

\[\frac{\partial\nu^{\top}W_{Q}^{(t)\top}W_{Q}^{(t)}\mu}{\partial t}\] \[=\frac{1}{n\sqrt{m}}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{j =1}^{m_{1}}a_{j}\nu^{\top}W_{Q}^{(t)\top}K^{(t,i)}\text{diag}\left(V^{(t,i) \top}w_{j}^{(t)}-w_{j}^{(t)\top}V^{(t,i)}p_{l(i,\mu)}^{(t,i)}\right)p_{l(i,\mu) }^{(t,i)}\] \[\quad+\frac{1}{n\sqrt{m}}\sum_{i:\nu\in X^{(i)}}g_{i}^{(t)}y_{i} \sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{Q}^{(t)\top}K^{(t,i)}\text{diag}\left(V^{(t,i)\top}w_{j}^{(t)}-w_{j}^{(t)\top}V^{(t,i)}p_{l(i,\nu)}^{(t,i)}\right)p_{l(i, \nu)}^{(t,i)},\] \[\frac{\partial\nu^{\top}W_{K}^{(t)\top}W_{K}^{(t)}\mu}{\partial t }\] \[=\frac{1}{n\sqrt{m}}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1 }^{L}\sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{K}^{(t)\top}q_{l}^{(t,i)}p_{q\gets l,k\leftarrow\mu}^{(t,i)}\left(w_{j}^{(t)\top}v^{(t,i)}(\mu)-w_{j}^{(t)\top}V^{ (t,i)}p_{l}^{(t,i)}\right)\]\[\frac{\partial\nu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu}{\partial t}\]

[MISSING_PAGE_EMPTY:21]

Proof.: Following from Lemma G.2 and from the first-order Taylor approximation on the softmax function from \(0\), we have

\[p_{l,k}^{(0,i)}=\frac{1}{L}\pm O\left(\frac{\sigma_{0}^{2}m}{L\sqrt{m}}\left(\sqrt {\frac{4}{m}\log\frac{2d}{\delta}}+\frac{4}{m}\log\frac{2d}{\delta}\right) \right).\]

The corollary then follows from Condition 1. 

**Lemma D.3**.: _With probability at least \(1-\delta\) over the randomness of the initialization of \(W\) and \(W_{V}\), then for \(l_{1}\neq l_{2}\in[d]\), we have_

\[\left|\left\langle W_{V}^{(0)}\mu_{l_{1}},W_{V}^{(0)}\mu_{l_{2}}\right\rangle \right|\leq\sigma_{0}^{2}m\left(\sqrt{\frac{4}{m}\log\frac{2d}{\delta}}+\frac{ 4}{m}\log\frac{2d}{\delta}\right),\]

_for \(j_{1}\neq j_{2}\in[m_{1}]\), we have_

\[\left|\left\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\right\rangle \right|\leq\sigma_{1}^{2}m\left(\sqrt{\frac{4}{m}\log\frac{2m_{1}^{2}}{\delta }}+\frac{4}{m}\log\frac{2m_{1}^{2}}{\delta}\right),\]

_and for all \(j\in[m_{1}],\;l\in[d]\), we have_

\[\left|\left\langle w_{j}^{(0)},W_{V}^{(0)}\mu_{l}\right\rangle \right|\leq\sigma_{0}\sigma_{1}m\left(\sqrt{\frac{4}{m}\log\frac{2 m_{1}d}{\delta}}+\frac{4}{m}\log\frac{2m_{1}d}{\delta}\right)\] \[\|w_{j}\|_{2}^{2} =\sigma_{1}^{2}m\left(1\pm\left(\sqrt{\frac{4}{m}\log\frac{2m_{1 }}{\delta}}+\frac{4}{m}\log\frac{2m_{1}}{\delta}\right)\right)\] \[\|W_{V}^{(0)}\mu_{l}\|_{2}^{2} =\sigma_{0}^{2}m\left(1\pm\left(\sqrt{\frac{4}{m}\log\frac{2d}{ \delta}}+\frac{4}{m}\log\frac{2d}{\delta}\right)\right).\]

Proof.: The proof is similar to that for Lemma D.1 and is omitted. 

**Lemma D.4**.: _Conditioned on the success of the event in Corollary D.2, for all \(i\in[n],\;l\in[L]\), with probability at least \(1-\delta\) over the randomness in the initialization of \(W_{V}\),_

\[\|W_{V}^{(0)}X^{(i)}p_{l}^{(0,i)}\|_{2}^{2}=\frac{\sigma_{0}^{2}m}{L}\left(1\pm \sqrt{\frac{4}{m}\log\frac{2nL}{\delta}}\pm\frac{4}{m}\log\frac{2nL}{\delta}\right)\]

Proof.: First of all, by Corollary D.2 and Assumption 2.3,

\[\mathbb{E}\left[\|W_{V}^{(0)}X^{(i)}p_{l}^{(0,i)}\|_{2}^{2}\right] =\mathbb{E}\left[\sum_{j=1}^{m}\left(\left\langle\left(W_{V}^{(0)} \right)_{j},X^{(i)}p_{l}^{(0,i)}\right\rangle\right)^{2}\right]=\sigma_{0}^{2} m\|X^{(i)}p_{l}^{(0,i)}\|_{2}^{2}=\frac{\sigma_{0}^{2}m}{L}.\]

Finally, applying Bernstein's inequality and taking a union bound over \([n]\) and \([L]\) we finish the proof. 

**Corollary D.5**.: _Conditioned on the success of Lemma D.4, with probability at least \(1-\delta\) over the randomness of \(W\), for all \(j\in[m_{1}],\;i\in[n],\;l\in[L]\), we have_

\[\left|w_{j}^{(0)\top}V^{(0,i)}p_{l}^{(0,i)}\right|\leq\sigma_{1}\sigma_{0} \sqrt{\frac{m}{L}\log\frac{m_{1}nL}{\delta}}.\]

Proof.: Conditioned on \(V^{(0,i)}p_{l}^{(0,i)}\), we have \(w_{j}^{(0)\top}V^{(0,i)}p_{l}^{(0,i)}\sim\mathcal{N}(0,\sigma_{1}^{2}\|V^{(0,i )}p_{l}^{(0,i)}\|_{2}^{2})\). Thus, by Gaussian tail bound and a union bound over \(j\in[m_{1}],\;i\in[n],\;l\in[L]\), with probability at least \(1-\delta\), we have

\[\left|w_{j}^{(0)\top}V^{(0,i)}p_{l}^{(0,i)}\right|\leq\sigma_{1}\sigma_{0} \sqrt{\frac{m}{L}\log\frac{m_{1}nL}{\delta}}.\]

**Lemma D.6**.: _With probability at least \(1-\delta\) over the randomness in the initialization of \(a\), we have_

\[|\mathcal{S}_{+1}| =m_{1}\left(\frac{1}{2}\pm\sqrt{\frac{2\log(4/\delta)}{m_{1}}} \right),\] \[|\mathcal{S}_{-1}| =m_{1}\left(\frac{1}{2}\pm\sqrt{\frac{2\log(4/\delta)}{m_{1}}} \right).\]

Proof.: The proof follows by applying Hoeffding's inequality. 

**Lemma D.7** (Initial sub-network output).: _Assume that the success of the events in Lemma D.3 holds. For all \(i\in[d]\), with probability at least \(1-\delta\) over the randomness in the weight initialization, we have_

\[|G^{(0)}(\mu_{i})|\leq\widetilde{O}(\sigma_{1}\sigma_{0}\sqrt{mm_{1}}).\]

Proof.: Consider a fixed \(i\in[d]\). By Lemma D.3, we have \(\|W_{V}^{(0)}\mu_{i}\|_{2}^{2}=\Theta(\sigma_{0}^{2}m)\). Thus, conditioned on \(W_{V}^{(0)}\mu_{i}\), we have \(\sum_{j=1}^{m_{1}}w_{j}^{(0)}W_{V}^{(0)}\mu_{i}\sim\mathcal{N}(0,\sigma_{1}^ {2}\sigma_{0}^{2}mm_{1})\). Thus, by Gaussian concentration bound, we have \(|G^{(0)}(\mu_{i})|\leq\widetilde{O}(\sigma_{1}\sigma_{0}\sqrt{mm_{1}})\). 

**Lemma D.8** (Initial network output).: _Assume that the success of the events in Lemma D.3 and Lemma D.4 holds. For all \(i\in[n]\), with probability at least \(1-\delta\) over the randomness in the weight initialization, we have_

\[|F^{(0)}(X^{(i)})|\leq 2\sigma_{0}\sigma_{1}\sqrt{2Lm_{1}m\log(2Ln/\delta)} \leq 0.01.\]

Proof.: For fixed \(l\in L,\ j\in[m],\ i\in[n]\), by Corollary D.5, we have

\[\left|w_{j}^{(0)\top}V^{(0,i)}p_{l}^{(0,i)}\right|\leq\sigma_{1}\sigma_{0} \sqrt{\frac{m}{L}\log\frac{m_{1}nL}{\delta}}.\]

Thus, this implies that \(a_{j}w_{j}^{(0)\top}V^{(0,i)}p_{l}^{(0,i)}\) is a sub-Gaussian random variable with variance proxy \(\sigma_{0}^{2}\sigma_{1}^{2}\frac{m}{L}\log\frac{m_{1}nL}{\delta}\). Therefore, the following inequality holds.

\[\mathbb{P}\left[\left|\sum_{j=1}^{m_{1}}a_{j}w_{j}^{(0)\top}V^{(0,i)}p_{l}^{(0,i)}\right|\geq 2\sigma_{0}\sigma_{1}\sqrt{\frac{2m_{1}m(\log\frac{m_{1}nL}{ \delta})\log(2/\delta)}{L}}\right]\leq\delta.\]

Taking a union bound over \(i\in[n],\ l\in[L]\), with probability at least \(1-\delta\), for all \(i\in[n]\), we have

\[|F^{(0)}(X^{(i)})|\leq 2\sigma_{0}\sigma_{1}\sqrt{2Lm_{1}m(\log\frac{m_{1}nL}{ \delta})\log(2Ln/\delta)}.\]

Finally, by Condition 1, we can make \(|F^{(0)}(X^{(i)})|\leq 0.01\). 

## Appendix E Training Dynamics: Phase 1

During Phase 1 of training, the linear layer quickly aligns with the target signals and all the remaining quantities stay roughly the same. The analysis need to keep track of the evolution of the above quantities with respect to the two signals \(\mu_{1},\mu_{2}\), the common token \(\mu_{3}\) and the random tokens.

**Definition E.1** (Radius of keys and queries).: _Define the radius of keys and queries \(R_{K},R_{Q}\) respectively to be_

\[R_{K} :=\max_{i,j\in[d]}\left|\mu_{i}^{\top}W_{K}^{(t)\top}W_{K}^{(t)} \mu_{j}-\mu_{i}^{\top}W_{K}^{(0)\top}W_{K}^{(0)}\mu_{j}\right|,\] \[R_{Q} :=\max_{i,j\in[d]}\left|\mu_{i}^{\top}W_{Q}^{(t)\top}W_{Q}^{(t)} \mu_{j}-\mu_{i}^{\top}W_{Q}^{(0)\top}W_{Q}^{(0)}\mu_{j}\right|.\]

**Definition E.2** (Phase 1).: _Define the range of Phase 1 to be \([0,T_{1}]\), where \(T_{1}=\min\{t^{\prime},C_{T_{1}}/(\sigma_{1}^{2}mm_{1})\}\) for some sufficiently large constant \(C_{T_{1}}\) and \(t^{\prime}\) is defined to be the maximum time such that for all \(t\leq t^{\prime}\), all of the following hold:_

1. \(\max_{j\in[m],\mu\in\{\mu_{i}\}_{i=1}^{3}}\left|w_{j}^{(t)}W_{V}^{(t)}\mu-w_{j }^{(0)}W_{V}^{(0)}\mu\right|\leq R\) _where_ \(R<O(1/m_{1})\)_;_
2. \(\max_{j\in[m],\mu\notin\{\mu_{i}\}_{i=1}^{3}}\left|w_{j}^{(t)}W_{V}^{(t)}\mu-w_ {j}^{(0)}W_{V}^{(0)}\mu\right|\leq O(R/n+R/\sqrt{m})\)_;_
3. \(\max_{\mu,\nu\in\{\mu\}_{i=1}^{4}}\left|\mu^{\top}W_{Q}^{(t)\top}W_{K}^{(t)} \nu-\mu^{\top}W_{Q}^{(0)\top}W_{K}^{(0)}\nu\right|\leq R_{S}\) _where_ \(R_{S}\leq O(1/(m\sqrt{m}))\)_;_
4. \(R_{K},R_{Q}\leq\widetilde{O}(\sigma_{0}^{2}\sqrt{m})\)_._

Based on this definition, we can further obtain the maximum softmax probability change as follows.

**Proposition E.3**.: _Define_

\[R_{P}:=\max_{i\in[n],\ \mu,\nu\in X^{(i)}}\left|p_{q\gets\mu,k\gets\nu }^{(t,i)}-p_{q\gets\mu,k\gets\nu}^{(0,i)}\right|.\]

_Then_

\[R_{P}=O\left(\frac{1}{\sqrt{m}L}+\frac{L}{m}\right).\]

Proof.: By Lemma G.2, we have \(R_{P}\leq O(R_{S}/L+R_{S}^{2}L)=O\left(\frac{1}{\sqrt{m}L}+\frac{L}{m}\right)\). 

Initially, the loss for the samples with one signal will increase.

### Initial Gradients

**Lemma E.4** (Signal updates, same as Lemma 4.1).: _At \(t=0\), for \(\mu\in\{\mu_{1},\mu_{2}\}\), we have_

\[\frac{\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\mu}{\partial t}=\Theta((\sigma_{0} ^{2}+\sigma_{1}^{2})m).\]

Proof.: Take \(\mu=\mu_{1}\). First of all, by the gradient flow update in Lemma C.1, we have

\[\frac{\partial w_{j_{1}}^{(t)\top}W_{V}^{(t)}\mu}{\partial t} =\frac{1}{n}\sum_{i_{2}=1}^{n}g_{i_{2}}^{(t)}y_{i_{2}}\sum_{l_{2} =1}^{L}\sum_{j_{2}=1}^{m_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^ {(t)}\right\rangle\left(X^{(i_{2})}p_{l_{2}}^{(t,i_{2})}\right)^{\top}\mu\] \[\quad+\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\sum_ {l_{1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)}\mu\] \[=\frac{1}{n}\sum_{i_{2}:\ \mu\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}} \sum_{l_{2}=1}^{L}\sum_{j_{2}=1}^{m_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle p_{q\gets l_{2},k\gets\mu}^{(t,i_{2})}\] \[\quad+\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\sum_{l _{1}=1}^{L}a_{j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(t,i_{1}) }v_{l_{2}}^{(t,i_{1})},v^{(t)}(\mu)\right\rangle\] \[=\frac{1}{n}\sum_{i_{2}:\ \mu\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}} \sum_{l_{2}=1}^{L}\left(a_{j_{1}}\|w_{j_{1}}^{(t)}\|_{2}^{2}+\sum_{j_{2}\neq j _{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle\right) p_{q\gets l_{2},k\gets\mu}^{(t,i_{2})}\] \[\quad+\frac{1}{n}\sum_{i_{1}:\mu\notin X^{(i_{1})}}g_{i_{1}}^{(t)} y_{i_{1}}\sum_{l_{1}=1}^{L}a_{j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(t,i_{1}) }v_{l_{2}}^{(t,i_{1})},v^{(t)}(\mu)\right\rangle\]\[=\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}a_{j_ {1}}\left(\|w_{j_{1}}^{(t)}\|_{2}^{2}+\|v^{(t)}(\mu)\|_{2}^{2}\right)p_{q\gets l,k\leftarrow\mu_{1}}^{(t,i)}\] \[\quad+\underbrace{\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_ {i}\sum_{l=1}^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w _{j_{2}}^{(t)}\right\rangle p_{q\gets l,k\leftarrow\mu}^{(t,i)}}_{ \varepsilon_{1}}\] \[\quad+\underbrace{\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_ {l_{1}=1}^{L}a_{j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(t,i)}v _{l_{2}}^{(t,i)},v^{(t)}(\mu)\right\rangle\mathbb{I}(v_{l_{2}}^{(t,i)}\neq v^ {(t)}(\mu))}_{\varepsilon_{2}}.\]

Now, by Lemma D.3, Corollary D.2 and Lemma D.8, we have

\[a_{j_{1}}\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)}}g_{i}^{(0)}y_{i} \sum_{l=1}^{L}a_{j_{1}}\left(\|w_{j_{1}}^{(0)}\|_{2}^{2}+\|v^{(0)}(\mu)\|_{2}^ {2}\right)p_{q\gets l,k\leftarrow\mu_{1}}^{(0,i)}\] \[=\frac{1}{n}\sum_{i\in I_{1}}g_{i}^{(0)}\sum_{l=1}^{L}\left(\|w_ {j}^{(0)}\|_{2}^{2}+\|v^{(0)}(\mu_{1})\|_{2}^{2}\right)p_{q\gets l,k \leftarrow\mu_{1}}^{(0,i)}-\frac{1}{n}\sum_{i\in I_{2}}g_{i}^{(0)}\sum_{l=1}^{ L}\left(\|w_{j}^{(0)}\|_{2}^{2}+\|v^{(0)}(\mu_{1})\|_{2}^{2}\right)p_{q\gets l,k\leftarrow\mu_{i}}^{(0,i)}\] \[=\left(\frac{1}{3}\pm 0.01\right)L\cdot\left(\sigma_{1}^{2}m+ \sigma_{0}^{2}m\right)\left(1\pm\left(\sqrt{\frac{4}{m}\log\frac{2d}{\delta}}+ \frac{4}{m}\log\frac{2d}{\delta}\right)\right)\frac{1}{L}(1+o(1)).\]

On the other hand, by Proposition E.5, we have

\[|\varepsilon_{1}| =\left|\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)}}g_{i}^{(0)}y_{i} \sum_{l=1}^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(0)},w_{ j_{2}}^{(0)}\right\rangle p_{q\gets l,k\leftarrow\mu_{1}}^{(0,i)}\right|\] \[\leq\sigma_{1}^{2}\sqrt{m_{1}}m\left(\sqrt{\frac{4}{m}\log\frac{2 m_{1}^{2}}{\delta}}+\frac{4}{m}\log\frac{2m_{1}^{2}}{\delta}\right)\sqrt{\log \frac{m_{1}}{\delta}};\] \[|\varepsilon_{2}| =\left|\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\sum_{l_{1}=1}^{ L}a_{j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)},v^{ (0)}(\mu_{1})\right\rangle\mathbb{I}(v_{l_{2}}^{(0,i)}\neq v^{(0)}(\mu_{1}))\right|\] \[\leq\sigma_{0}^{2}m\sqrt{L}\left(\sqrt{\frac{4}{m}\log\frac{2nL}{ \delta}}+\frac{4}{m}\log\frac{2nL}{\delta}\right).\]

If \(m\geq Cm_{1}\log^{2}\frac{m_{1}}{\delta}\) in Condition 1 for some sufficiently large \(C\), then \(|\varepsilon_{1}|\leq 0.01\sigma_{1}^{2}m\); and if \(m\geq C^{\prime}L\log\frac{2nL}{\delta}\) for some sufficiently large \(C^{\prime}\), then \(|\varepsilon_{2}|\leq 0.01\sigma_{0}^{2}m\). 

**Proposition E.5**.: _Assume the events in Lemma D.3 and Corollary D.2 succeed. With probability at least \(1-\delta\) over the randomness in the weight initialization, for all \(j_{1}\in[m_{1}]\), we have_

\[\left|\sum_{j_{2}:j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(0)},w_{j_{2 }}^{(0)}\right\rangle\right|\leq\sigma_{1}^{2}\sqrt{m_{1}}m\left(\sqrt{\frac{ 4}{m}\log\frac{2m_{1}^{2}}{\delta}}+\frac{4}{m}\log\frac{2m_{1}^{2}}{\delta} \right)\sqrt{\log\frac{4m_{1}}{\delta}}.\]

_Further, for all \(\mu\in\{\mu_{i}\}_{i=1}^{d}\), we have_

\[\left|\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(0)}y_{i}\sum_{l=1 }^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)} \right\rangle p_{q\gets l,k\leftarrow\mu}^{(0,i)}\right|\] \[\leq\frac{|i:\mu\in X^{(i)}|}{n}\sigma_{1}^{2}\sqrt{m_{1}}m\left( \sqrt{\frac{4}{m}\log\frac{2m_{1}^{2}}{\delta}}+\frac{4}{m}\log\frac{2m_{1}^{2}}{ \delta}\right)\sqrt{\log\frac{m_{1}}{\delta}},\]

_and_

\[\left|\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\sum_{l_{1}=1}^{L}a_{j_ {1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)},v^{(0 )}(\mu)\right\rangle\mathbb{I}(v_{l_{2}}^{(0,i)}\neq v^{(0)}(\mu))\right|\]\[\leq\sigma_{0}^{2}m\sqrt{L}\left(\sqrt{\frac{4}{m}\log\frac{2nLd}{ \delta}}+\frac{4}{m}\log\frac{2nLd}{\delta}\right).\]

Proof.: First, fix \(i,l\), and consider the randomness of \(a\). By Lemma D.3, \(a_{j_{2}}\left\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\right\rangle\) is a sub-Gaussian random variable with variance proxy \(\sigma_{1}^{4}m^{2}\left(\sqrt{\frac{4}{m}\log\frac{2m_{1}^{2}}{\delta}}+\frac {4}{m}\log\frac{2m_{1}^{2}}{\delta}\right)^{2}.\) This implies that with probability at least \(1-\delta/2\), for all \(j_{1}\in[m_{1}]\), we have

\[\left|\sum_{j_{2}:j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(0)},w_{j_{ 2}}^{(0)}\right\rangle\right|\leq\sigma_{1}^{2}\sqrt{m_{1}}m\left(\sqrt{\frac {4}{m}\log\frac{2m_{1}^{2}}{\delta}}+\frac{4}{m}\log\frac{2m_{1}^{2}}{\delta }\right)\sqrt{\log\frac{4m_{1}}{\delta}}.\]

Thus, by Corollary D.2, for all \(\mu\in\{\mu_{i}\}_{i=1}^{d}\), we have

\[\left|\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(0)}y_{i}\sum_{l=1} ^{L}\sum_{j_{2}:j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(0)},w_{j_{2} }^{(0)}\right\rangle p_{q\gets l,k\leftarrow\mu}^{(0,i)}\right|\] \[\leq\frac{|i:\,\mu\in X^{(i)}|}{n}\sigma_{1}^{2}\sqrt{m_{1}}m \left(\sqrt{\frac{4}{m}\log\frac{2m_{1}^{2}}{\delta}}+\frac{4}{m}\log\frac{2m_ {1}^{2}}{\delta}\right)\sqrt{\log\frac{m_{1}}{\delta}}.\]

We next derive the second inequality. Consider the randomness in \(W_{V}^{(0)}\). Note that

\[\sum_{l_{2}=1}^{L}p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)}\mathbb{I}(v_{l_{2}} ^{(0,i)}\neq v^{(0)}(\mu))\sim\mathcal{N}\left(0,\sigma_{0}^{2}\sum_{l_{2}=1}^ {L}(p_{l_{1},l_{2}}^{(0,i)})^{2}\mathbb{I}(v_{l_{2}}^{(0,i)}\neq v^{(0)}(\mu) )I\right).\]

Thus, by Lemma H.2 and Corollary D.2 and taking a union bound over \(i\in[n],\,\,l_{1}\in[L],\,\,\mu\in\{\mu_{i}\}_{i=1}^{d}\), we have with probability at least \(1-\delta/2\),

\[\left|\left\langle\sum_{l_{2}=1}^{L}p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)} \mathbb{I}(v_{l_{2}}^{(0,i)}\neq v^{(0)}(\mu)),v^{(0)}(\mu)\right\rangle\right| \leq\sigma_{0}^{2}m\frac{1}{\sqrt{L}}\left(\sqrt{\frac{4}{m}\log\frac{2nLd}{ \delta}}+\frac{4}{m}\log\frac{2nLd}{\delta}\right).\]

Therefore,

\[\left|\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\sum_{l_{1}=1}^{L }a_{j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i)},v^{(0)}(\mu)\right\rangle\mathbb{I}(v_{l_{2}}^{(0,i)}\neq v^{(0)}(\mu))\right|\] \[\leq\sigma_{0}^{2}m\sqrt{L}\left(\sqrt{\frac{4}{m}\log\frac{2nLd} {\delta}}+\frac{4}{m}\log\frac{2nLd}{\delta}\right).\]

**Lemma E.6** (Random token updates).: _For \(\mu\in\{\mu_{i}\}_{i=d}^{d}\), we have_

\[\frac{\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\mu}{\partial t}=O\left(\frac{1}{n}( \sigma_{0}^{2}+\sigma_{1}^{2})m+\sigma_{0}^{2}\sqrt{mL}\right).\]

Proof.: Following the proof of Lemma E.4, we have

\[\frac{\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\mu}{\partial t} =\frac{1}{n}\sum_{i_{2}=1}^{n}g_{i_{2}}^{(0)}y_{i_{2}}\sum_{l_{2} =1}^{L}\sum_{j_{2}=1}^{m_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(0)},w_{j_{2}}^{( 0)}\right\rangle\left(X^{(i_{2})}p_{l_{2}}^{(0,i_{2})}\right)^{\top}\mu\] \[\quad+\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(0)}y_{i_{1}}\sum_{ l_{1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(0,i_{1})\top}V^{(0,i_{1})\top}W_{V}^{(0)}\mu\] \[=\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(0)}y_{i}\sum_{l=1}^{L }a_{j_{1}}\left(\|w_{j_{1}}^{(0)}\|_{2}^{2}+\|v^{(0)}(\mu)\|_{2}^{2}\right)p_{q \gets l,k\leftarrow\mu}^{(0,i)}\]\[+\underbrace{\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(0)}y_{i}\sum_{l=1 }^{L}\sum_{j_{2}:j_{2}\neq j_{1}}a_{j_{2}}\left<w_{j_{1}}^{(0)},w_{j_{2}}^{(0)} \right>p_{q\to l,k\leftarrow\mu}}_{\varepsilon_{1}}\] \[+\underbrace{\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(0)}y_{i}\sum_{l_{1} =1}^{L}a_{j_{1}}\sum_{l_{2}=1}^{L}\left<p_{l_{1},l_{2}}^{(0,i)}v_{l_{2}}^{(0,i )},v^{(0)}(\mu)\right>\mathbb{I}(v_{l_{2}}^{(0,i)}\neq v^{(0)}(\mu))}_{ \varepsilon_{2}}\]

By Proposition E.5 and the fact that only one \(X^{(i)}\) satisfies \(\mu\in X^{(i)}\), we have

\[\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(0)}y_{i}\sum_{l=1}^{L}a_{j_{1}} \left(\|w_{j_{1}}^{(0)}\|_{2}^{2}+\|v^{(0)}(\mu)\|_{2}^{2}\right)p_{q\to l,k\leftarrow\mu}^{(0,i)}=\Theta\left(\frac{1}{n}(\sigma_{0}^{2}+\sigma_{1}^{2 })m\right),\]

and

\[|\varepsilon_{1}| \leq\frac{1}{n}\sigma_{1}^{2}\sqrt{m_{1}}m\left(\sqrt{\frac{4}{m }\log\frac{2m_{1}^{2}}{\delta}}+\frac{4}{m}\log\frac{2m_{1}^{2}}{\delta} \right)\sqrt{\log\frac{4m_{1}}{\delta}},\] \[|\varepsilon_{2}| \leq\sigma_{0}^{2}m\sqrt{L}\left(\sqrt{\frac{4}{m}\log\frac{2nLd }{\delta}}+\frac{4}{m}\log\frac{2nLd}{\delta}\right).\]

### Maximum Perturbation of Neuron Outputs

**Lemma E.7**.: _For all \(t\leq T_{1}\), for all \(i\in[n],\;l\in[L]\), we have_

\[\left|w_{j}^{(t)}W_{V}^{(t)}X^{(i)}p_{l}^{(t,i)}-w_{j}^{(0)}W_{V}^{(0)}X^{(i)} p_{l}^{(0,i)}\right|\leq\widetilde{O}(LR_{P}\sigma_{0}\sigma_{1}\sqrt{m})+4R/L.\]

Proof.: By Definition E.2, we have

\[\left|w_{j}^{(t)}V^{(t,i)}p_{l}^{(t,i)}-w_{j}^{(0)}V^{(0,i)}p_{l}^ {(0,i)}\right|\] \[\leq\sum_{l^{\prime}:V_{l^{\prime}}\in\nu(\mu_{1},\mu_{2},\mu_{3 })}^{L}(R_{P}\widetilde{O}(\sigma_{0}\sigma_{1}\sqrt{m})+R/L)+\sum_{l^{\prime}: V_{l^{\prime}}\notin v(\mu_{1},\mu_{2},\mu_{3})}(R_{P}\widetilde{O}(\sigma_{0} \sigma_{1}\sqrt{m})+R/(nL))\] \[\leq\widetilde{O}(R_{P}\sigma_{0}\sigma_{1}\sqrt{m}+R/L)+ \widetilde{O}(LR_{P}\sigma_{0}\sigma_{1}\sqrt{m}+R/n)\] \[=\widetilde{O}(LR_{P}\sigma_{0}\sigma_{1}\sqrt{m})+4R/L.\]

By our choice of parameters in Condition 1 and Definition E.2, we have \(\widetilde{O}(LR_{P}\sigma_{0}\sigma_{1}\sqrt{m})+4R/L<\sigma_{0}\sigma_{1} \sqrt{m/L}\). 

### Perturbation Term Involving Correlation of Value-transformed Data

**Proposition E.8**.: _With probability at least \(1-\delta\), for all \(\mu\in\{\mu_{i}\}_{i=1}^{d}\), we have_

\[\left|\sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{V}^{(0)\top}w_{j}^{(0)}\right|\leq \widetilde{O}(\sigma_{0}\sigma_{1}\sqrt{mm_{1}}).\]

Proof.: The proof is similar to Proposition E.22, and is omitted.

**Lemma E.9** (Value correlation change).: _For all \(\mu,\nu\in\left\{\mu_{i}\right\}_{i=1}^{d}\), we have_

\[\left|\frac{\partial\left\langle v^{(t)}(\mu),v^{(t)}(\nu)\right\rangle}{ \partial t}\right|\leq\frac{\left|\left\{i:\mu\in X^{(i)}\right\}\right|+\left| \left\{i:\nu\in X^{(i)}\right\}\right|}{n}O(1),\]

_and thus,_

\[\left|\left\langle v^{(t)}(\mu),v^{(t)}(\nu)\right\rangle-\left\langle v^{(0) }(\mu),v^{(0)}(\nu)\right\rangle\right|\leq t\frac{\left|\left\{i:\mu\in X^{( i)}\right\}\right|+\left|\left\{i:\nu\in X^{(i)}\right\}\right|}{n}O(1)\]

_for all \(t\leq T_{1}\). Thus, for \(\mu\neq\nu\), we have \(\left|\left\langle v^{(t)}(\mu),v^{(t)}(\nu)\right\rangle\right|\leq\widetilde {O}(\sigma_{0}^{2}\sqrt{m})\) and \(\|v^{(t)}(\mu)\|_{2}^{2}=\Theta(\sigma_{0}^{2}m)\) for \(t\leq T_{1}\)._

Proof.: By Lemma C.3, we have

\[\frac{\partial\nu W_{V}^{(t)\top}W_{V}^{(t)}\mu}{\partial t} =\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L} \sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q\gets l,k \leftarrow\mu}^{(t,i)}\] \[\quad+\frac{1}{n}\sum_{i:\nu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l= 1}^{L}\sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q\gets l,k\leftarrow\nu}^{(t,i)}.\]

Further,

\[\left|\sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}-\sum_{j=1}^ {m_{1}}a_{j}\nu^{\top}W_{V}^{(0)\top}w_{j}^{(0)}\right|\leq m_{1}R.\]

Thus, by Proposition E.8, we have

\[\left|\sum_{j=1}^{m_{1}}a_{j}w_{j}^{(t)\top}W_{V}^{(t)}\nu\right| \leq\left|\sum_{j=1}^{m_{1}}a_{j}\hat{\sigma}_{i,l,j}^{(0)}w_{j}^ {(0)\top}W_{V}^{(0)}\nu\right|+\left|\sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{V}^{ (t)\top}w_{j}^{(t)}-\sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{V}^{(0)\top}w_{j}^{(0 )}\right|\] \[\leq\widetilde{O}(\sigma_{0}\sigma_{1}\sqrt{mm_{1}})+m_{1}R,\]

which implies

\[\left|\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1 }^{L}\sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q\gets l,k\leftarrow\mu}^{(t,i)}\right.\] \[\quad+\frac{1}{n}\sum_{i:\nu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l= 1}^{L}\sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q\gets l,k\leftarrow\nu}^{(t,i)}\Bigg{|}\] \[\leq\frac{1}{n}\left(\left|\left\{i:\mu\in X^{(i)}\right\}\right| +\left|\left\{i:\nu\in X^{(i)}\right\}\right|\right)\left(\widetilde{O}( \sigma_{0}\sigma_{1}\sqrt{mm_{1}})+m_{1}R\right)\] \[\leq\frac{\left|\left\{i:\mu\in X^{(i)}\right\}\right|+\left| \left\{i:\nu\in X^{(i)}\right\}\right|}{n}O(1)\]

where the last inequality applies Lemma E.7. 

**Corollary E.10**.: _For all \(t\leq T_{1}\), we have_

\[\left|\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l_{1}=1}^{L}a_{j_{1}} \sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)},v^{(t) }(\mu_{1})\right\rangle\mathbb{I}(v_{l_{2}}^{(t,i)}\neq v^{(t)}(\mu_{1})) \right|\leq L\cdot\widetilde{O}\left(\sigma_{0}^{2}\sqrt{m}\right).\]

Proof.: We derive the following bound:

\[\left|\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l_{1}=1}^{L}a_ {j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)}, v^{(t)}(\mu)\right\rangle\mathbb{I}(v_{l_{2}}^{(t,i)}\neq v^{(t)}(\mu))\right|\] \[\leq\sum_{l_{1}=1}^{L}\sum_{l_{2}=1}^{L}p_{l_{1},l_{2}}^{(t,i)} \left|\left\langle v_{l_{2}}^{(t,i)},v^{(t)}(\mu)\right\rangle\right| \mathbb{I}(v_{l_{2}}^{(t,i)}\neq v^{(t)}(\mu))\] \[\leq L\cdot\widetilde{O}\left(\sigma_{0}^{2}\sqrt{m}\right),\]

where the last inequality follows from Lemma E.9 and Lemma D.3.

### Perturbation Term Involving Correlation of Neurons

**Lemma E.11**.: _For all \(t\leq T_{1}\), we have_

\[\left|\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}\sum_{ j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle p _{q\gets l,k\leftarrow\mu_{1}}^{(t,i)}\right|\leq\frac{|\{i:\mu_{1}\in X ^{(i)}\}|}{n}\widetilde{O}(\sigma_{1}^{2}\sqrt{mm_{1}}).\]

Proof.: We first derive:

\[\left|\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_ {l=1}^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^ {(t)}\right\rangle p_{q\gets l,k\leftarrow\mu_{1}}^{(t,i)}\right|\] \[\leq\left|\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)}}g_{i}^{(t)}y_{i} \sum_{l=1}^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j _{2}}^{(t)}\right\rangle\right|\cdot\max_{i,l}p_{q\gets l,k\leftarrow\mu_ {1}}^{(t,i)}\] \[\leq\underbrace{\left|\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)}}g_{i} ^{(t)}y_{i}\sum_{l=1}^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}} ^{(0)},w_{j_{2}}^{(0)}\right\rangle\right|\cdot\max_{i,l}p_{q\gets l,k \leftarrow\mu_{1}}^{(t,i)}}_{(1)}\] \[\quad+\underbrace{\left|\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)}}g_{ i}^{(t)}y_{i}\sum_{l=1}^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left(\left\langle w_{j_{1}} ^{(t)},w_{j_{2}}^{(t)}\right\rangle-\left\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0 )}\right\rangle\right)\right|\cdot\max_{i,l}p_{q\gets l,k\leftarrow\mu_{1} }^{(t,i)}}_{(2)}.\]

For term \((1)\), by Proposition E.5, we have

\[\left|\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}\sum _{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\right\rangle \right|\leq\frac{|\{i:\;\mu_{1}\in X^{(i)}\}|}{n}\widetilde{O}\left(\sigma_{1} ^{2}\sqrt{mm_{1}}L\right).\]

For term \((2)\), note that

\[\left|\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_ {l=1}^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left(\left\langle w_{j_{1}}^{(t)},w_ {j_{2}}^{(t)}\right\rangle-\left\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\right\rangle\right)\right|\] \[\leq \frac{|\{i:\mu_{1}\in X^{(i)}\}|}{n}Lm_{1}\cdot O\left(\frac{1}{m }\right),\]

where the inequality is by Lemma D.3, Proposition E.12.

Finally, since \(p_{q\gets l,k\leftarrow\mu_{1}}^{(t,i)}\leq\frac{1}{L}+\frac{1}{L^{2}}+R_ {P}\), combining the upper bound for both terms \((1)\) and \((2)\), we have

\[\left|\frac{\alpha}{n}\sum_{i:\mu_{1}\in X^{(i)}}g_{i}^{(t)}y_{i} \sum_{l=1}^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\dot{\sigma}_{i,l,j_{2}}^{(t)} \left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle p_{q\gets l,k \leftarrow\mu_{1}}^{(t,i)}\right|\] \[\qquad\leq\alpha\frac{|\{i:\mu_{1}\in X^{(i)}\}|}{n}\cdot \widetilde{O}\left(\sigma_{1}^{2}\sqrt{mm_{1}}L\right).\]

**Proposition E.12** (Neuron correlation change, Phase 1).: _For \(t\leq T_{1}\), for all \(j_{1},j_{2}\in[m_{1}]\), we have_

\[\left|\frac{\partial\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle}{ \partial t}\right|\leq 2L\left(\sigma_{1}\sigma_{0}\sqrt{\frac{m}{L}\log \frac{m_{1}nL}{\delta}}+\widetilde{O}(LR_{P}\sigma_{0}\sigma_{1}\sqrt{m})+4R/L \right),\]

_and thus,_

\[\left|\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle-\left\langle w _{j_{1}}^{(0)},w_{j_{2}}^{(0)}\right\rangle\right|\leq t2L\left(\sigma_{1} \sigma_{0}\sqrt{\frac{m}{L}\log\frac{m_{1}nL}{\delta}}+\widetilde{O}(LR_{P} \sigma_{0}\sigma_{1}\sqrt{m})+4R/L\right).\]Proof.: By the gradient flow update in Lemma C.2, we have

\[\frac{\partial\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle }{\partial t} =\frac{1}{n}\sum_{i^{\prime}=1}^{n}g_{i^{\prime}}^{(t)}y_{i^{ \prime}}\sum_{l^{\prime}=1}^{L}a_{j_{2}}w_{j_{1}}^{(t)\top}V^{(t,i^{\prime})}p _{l^{\prime}}^{(t,i^{\prime})}+\frac{1}{n}\sum_{i^{\prime}=1}^{n}g_{i^{\prime }}^{(t)}y_{i^{\prime}}\sum_{l^{\prime}=1}^{L}a_{j_{1}}w_{j_{2}}^{(t)\top}V^{(t, i^{\prime})}p_{l^{\prime}}^{(t,i^{\prime})}\] \[=\frac{1}{n}\sum_{i^{\prime}=1}^{n}g_{i^{\prime}}^{(t)}y_{i^{ \prime}}\sum_{l^{\prime}=1}^{L}a_{j_{2}}\left(w_{j_{1}}^{(0)\top}V^{(0,i^{ \prime})}p_{l^{\prime}}^{(0,i^{\prime})}+(w_{j_{1}}^{(t)\top}V^{(t,i^{\prime})} p_{l^{\prime}}^{(t,i^{\prime})}-w_{j_{1}}^{(0)\top}V^{(0,i^{\prime})}p_{l^{ \prime}}^{(0,i^{\prime})})\right)\] \[\quad+\frac{1}{n}\sum_{i^{\prime}=1}^{n}g_{i^{\prime}}^{(t)}y_{i^ {\prime}}\sum_{l^{\prime}=1}^{L}a_{j_{1}}\left(w_{j_{2}}^{(0)\top}V^{(0,i^{ \prime})}p_{l^{\prime}}^{(0,i^{\prime})}+(w_{j_{2}}^{(t)\top}V^{(t,i^{\prime}) }p_{l^{\prime}}^{(t,i^{\prime})}-w_{j_{2}}^{(0)\top}V^{(0,i^{\prime})}p_{l^{ \prime}}^{(0,i^{\prime})})\right).\]

Now, since by Lemma E.7 we have

\[\left|w_{j}^{(t)}W_{V}^{(t)}X^{(i)}p_{l}^{(t,i)}-w_{j}^{(0)}W_{V}^{(0)}X^{(i) }p_{l}^{(0,i)}\right|\leq\widetilde{O}(LR_{P}\sigma_{0}\sigma_{1}\sqrt{m})+4R/L,\]

by Definition E.2 and Corollary D.5, we have

\[\left|\frac{1}{n}\sum_{i^{\prime}=1}^{n}g_{i^{\prime}}^{(t)}y_{i ^{\prime}}\sum_{l^{\prime}=1}^{L}a_{j_{2}}\left(w_{j_{1}}^{(0)\top}V^{(0,i^{ \prime})}p_{l^{\prime}}^{(0,i^{\prime})}+(w_{j_{1}}^{(t)\top}V^{(t,i^{\prime} )}p_{l^{\prime}}^{(t,i^{\prime})}-w_{j_{1}}^{(0)\top}V^{(0,i^{\prime})}p_{l^{ \prime}}^{(0,i^{\prime})})\right)\right|\] \[\leq L\left(\sigma_{1}\sigma_{0}\sqrt{\frac{m}{L}\log\frac{m_{1} nL}{\delta}}+\widetilde{O}(LR_{P}\sigma_{0}\sigma_{1}\sqrt{m})+4R/L\right)\] \[\left|\frac{1}{n}\sum_{i^{\prime}=1}^{n}g_{i^{\prime}}^{(t)}y_{i ^{\prime}}\sum_{l^{\prime}=1}^{L}a_{j_{1}}\left(w_{j_{2}}^{(0)\top}V^{(0,i^{ \prime})}p_{l^{\prime}}^{(0,i^{\prime})}+(w_{j_{2}}^{(t)\top}V^{(t,i^{\prime}) }p_{l^{\prime}}^{(t,i^{\prime})}-w_{j_{2}}^{(0)\top}V^{(0,i^{\prime})}p_{l^{ \prime}}^{(0,i^{\prime})})\right)\right|\] \[\leq L\left(\sigma_{1}\sigma_{0}\sqrt{\frac{m}{L}\log\frac{m_{1} nL}{\delta}}+\widetilde{O}(LR_{P}\sigma_{0}\sigma_{1}\sqrt{m})+4R/L\right).\]

Thus,

\[\left|\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle- \left\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\right\rangle\right|\] \[\leq\int_{\tau=0}^{t}\left|\frac{\partial\left\langle w_{j_{1}}^ {(\tau)},w_{j_{2}}^{(\tau)}\right\rangle}{\partial\tau}\right|\leq t2L\left( \sigma_{1}\sigma_{0}\sqrt{\frac{m}{L}\log\frac{m_{1}nL}{\delta}}+\widetilde{O} (LR_{P}\sigma_{0}\sigma_{1}\sqrt{m})+4R/L\right).\]

**Corollary E.13** (Neuron Norm Change, Phase 1).: _For all \(t\leq T_{1}\) and all \(j\in[m]\), we have_

\[\left|\|w_{j}^{(t)}\|_{2}^{2}-\|w_{j}^{(0)}\|_{2}^{2}\right|\leq t2L(\sigma_{1} \sigma_{0}\sqrt{\frac{m}{L}\log\frac{m_{1}nL}{\delta}}+\widetilde{O}(LR_{P} \sigma_{0}\sigma_{1}\sqrt{m})+4R/L).\]

### Neuron Weights Align with Signal Value

**Theorem E.14** (Signal correlation growth, phase 1).: _For \(t\leq T_{1}\), for \(\mu\in\{\mu_{1},\mu_{2}\}\),_

\[\frac{\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\mu}{\partial t}=\frac{1}{n}\Bigg{(} \sum_{\begin{subarray}{c}i:\mu\in X^{(i)},\\ y_{i}=1\end{subarray}}g_{i}^{(t)}\sum_{l=1}^{L}p_{q\gets l,k\leftarrow\mu}^{(t,i)}-\sum_{\begin{subarray}{c}i:\mu\in X^{(i)},\\ y_{i}=-1\end{subarray}}g_{i}^{(t)}\sum_{l=1}^{L}p_{q\gets l,k\leftarrow\mu}^{(t,i)}\Bigg{)}\Theta((\sigma_{0}^{2}+\sigma_{1}^{2})m)+\varepsilon\]

_where_

\[|\varepsilon|\leq\widetilde{O}(L\sigma_{0}^{2}\sqrt{m}+\sigma_{1}^{2}\sqrt{mm_{1}}).\]

Proof.: We take \(\mu=\mu_{1}\) and the proof is similar for \(\mu=\mu_{2}\). By Lemma C.1, we have

\[\frac{\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\mu}{\partial t}\]\[=\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)},y_{i}=1}g_{i}^{(t)}\sum_{l=1}^{L }\left(\|w_{j}^{(t)}\|_{2}^{2}+\|v^{(t)}(\mu_{1})\|_{2}^{2}\right)p_{q\gets l,k\leftarrow\mu_{1}}^{(t,i)}\] \[\quad-\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)},y_{i}=-1}g_{i}^{(t)} \sum_{l=1}^{L}\left(\|w_{j}^{(t)}\|_{2}^{2}+\|v^{(t)}(\mu_{1})\|_{2}^{2}\right) p_{q\gets l,k\leftarrow\mu_{1}}^{(t,i)}+\varepsilon\] \[=\left(\|w_{j}^{(t)}\|_{2}^{2}+\|v^{(t)}(\mu_{1})\|_{2}^{2}\right) \frac{1}{n}\Bigg{(}\sum_{\begin{subarray}{c}:\mu_{1}\in X^{(i)}\\ y_{i}=1\end{subarray},}g_{i}^{(t)}\sum_{l=1}^{L}p_{q\gets l,k\leftarrow\mu _{1}}^{(t,i)}-\sum_{\begin{subarray}{c}:\mu_{1}\in X^{(i)}\\ y_{i}=-1\end{subarray},}g_{i}^{(t)}\sum_{l=1}^{L}p_{q\gets l,k\leftarrow\mu _{1}}^{(t,i)}\Bigg{)}+\varepsilon\]

where

\[\varepsilon =\frac{1}{n}\sum_{i:\mu_{1}\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1} ^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t) }\right\rangle p_{q\gets l,k\leftarrow\mu_{1}}^{(t,i)}\] \[\quad+\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l_{1}=1}^{L} a_{j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)},v^{( t)}(\mu_{1})\right\rangle\mathbb{I}(v_{l_{2}}^{(t,i)}\neq v^{(t)}(\mu_{1})).\]

We can now bound the magnitude of \(\varepsilon\) by Corollary E.10, Lemma E.11 and Proposition E.5 and obtain:

\[|\varepsilon|\leq L\cdot\widetilde{O}\left(\sigma_{0}^{2}\sqrt{m}\right)+ \widetilde{O}(\sigma_{1}^{2}\sqrt{mm_{1}}).\]

Finally, by Lemma D.3 and Corollary E.13, we have \(\|w_{j}^{(t)}\|_{2}^{2}=\Theta(\sigma_{1}^{2}m)\) and by Lemma E.9, we have \(\|v^{(t)}(\mu_{1})\|_{2}^{2}=\Theta(\sigma_{0}^{2}m)\). The proof is completed. 

**Theorem E.15** (Random token growth, Phase 1).: _For \(t\leq T_{1}\), for \(\mu\in\mathcal{R}\), we have_

\[\frac{\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\mu}{\partial t}=O\left(\frac{1}{n}( \sigma_{0}^{2}+\sigma_{1}^{2})m\right)+\widetilde{O}(\sigma_{0}^{2}L\sqrt{m}).\]

Proof.: Fix a \(\mu\in\mathcal{R}\). By our Assumption 2.3, \(\mu\) appears at most once in the training data set. Now, assume \(\mu\) is in the training set and let \(i^{*}\) be the index of the sample containing \(\mu\). Applying Lemma C.1 on the random token \(\mu\), we have

\[\frac{\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\mu}{\partial t}=\frac{1}{n}g_{i^{* }}^{(t)}y_{i^{*}}\sum_{l=1}^{L}a_{j}\left(\|w_{j}^{(t)}\|_{2}^{2}+\|v^{(t)}( \mu)\|_{2}^{2}\right)p_{q\gets l,k\leftarrow\mu}^{(t,i^{*})}+\varepsilon,\]

where

\[\varepsilon =\frac{1}{n}g_{i^{*}}^{(t)}y_{i}\sum_{l=1}^{L}\sum_{j_{2}\neq j_{ 1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle p_{q \gets l,k\leftarrow\mu}^{(t,i^{*})}\] \[\quad+\frac{1}{n}\sum_{i=1}^{n}g^{(t)}y_{i}\sum_{l_{1}=1}^{L}a_{j _{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^{(t,i)}v_{l_{2}}^{(t,i)},v ^{(t)}(\mu)\right\rangle\mathbb{I}(v_{l_{2}}^{(t,i)}\neq v^{(t)}(\mu)).\]

We can now bound the magnitude of \(\varepsilon\) by Corollary E.10 and Lemma E.11 as follows:

\[|\varepsilon|\leq L\cdot\widetilde{O}\left(\sigma_{0}^{2}\sqrt{m}\right)+ \frac{1}{n}\widetilde{O}(\sigma_{1}^{2}\sqrt{mm_{1}}).\]

Finally, by Lemma D.3 and Corollary E.13, we have

\[\frac{\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\mu}{\partial t}=O\left(\frac{1}{n}( \sigma_{0}^{2}+\sigma_{1}^{2})m\right)+\widetilde{O}(\sigma_{0}^{2}L\sqrt{m}).\]

### Alignment of Common Token

**Lemma E.16** (Initial Per Neuron Gradient of Common Token, same as Lemma 4.2).: _Let \(F=\max_{i}F_{i}^{(0)}\). We have_

\[\left|\frac{\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\mu_{3}}{\partial t}\right|= \widetilde{O}\left(\sigma_{1}^{2}\sqrt{mm_{1}}+\sigma_{0}^{2}\sqrt{mL}+(\sigma _{0}^{2}+\sigma_{1}^{2})mF\right).\]

Proof.: Following from the proof of Lemma E.4, we have

\[\frac{\partial a_{j_{1}}w_{j_{1}}^{(0)}W_{V}^{(0)}\mu_{3}}{\partial t} =\frac{1}{n}\sum_{i:\mu_{3}\in X^{(i)}}g_{i}^{(0)}y_{i}\sum_{l=1}^ {L}\left(\|w_{j_{1}}^{(0)}\|_{2}^{2}+\|v^{(0)}(\mu_{1})\|_{2}^{2}\right)p_{q \gets l,k\leftarrow\mu_{3}}^{(0,i)}\] \[\quad+a_{j_{1}}\underbrace{\frac{1}{n}\sum_{i:\mu_{3}\in X^{(i) }}g_{i}^{(0)}y_{i}\sum_{l=1}^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w _{j_{1}}^{(0)},w_{j_{2}}^{(0)}\right\rangle p_{q\gets l,k\leftarrow\mu_{3 }}^{(0,i)}}_{\varepsilon_{1}}\] \[\quad+a_{j_{1}}\underbrace{\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(0)}y _{i}\sum_{l_{1}=1}^{L}a_{j_{1}}\sum_{l_{2}=1}^{L}\left\langle p_{l_{1},l_{2}}^ {(0,i)}v_{l_{2}}^{(0,i)},v^{(0)}(\mu_{3})\right\rangle\mathbb{I}(v_{l_{2}}^{(0,i)}\neq v^{(0)}(\mu_{3}))}_{\varepsilon_{2}}.\]

For the first term, we have

\[\left|\frac{1}{n}\sum_{i:\mu_{3}\in X^{(i)}}g_{i}^{(0)}y_{i}\sum_ {l=1}^{L}\left(\|w_{j_{1}}^{(0)}\|_{2}^{2}+\|v^{(0)}(\mu_{3})\|_{2}^{2}\right) p_{q\gets l,k\leftarrow\mu_{3}}^{(0,i)}\right|\] \[=\left|\frac{1}{n}\sum_{y_{i}=1}g_{i}^{(0)}\sum_{l=1}^{L}\left(\| w_{j_{1}}^{(0)}\|_{2}^{2}+\|v^{(0)}(\mu_{3})\|_{2}^{2}\right)p_{q\gets l,k\leftarrow\mu_{3}}^{(0,i)}\right.\] \[\quad-\frac{1}{n}\sum_{y_{i}=-1}g_{i}^{(0)}\sum_{l=1}^{L}\left(\| w_{j_{1}}^{(0)}\|_{2}^{2}+\|v^{(0)}(\mu_{3})\|_{2}^{2}\right)p_{q\gets l,k\leftarrow\mu_{3}}^{(0,i)}\Bigg{|}\] \[=\frac{1}{n}\left(\|w_{j_{1}}^{(0)}\|_{2}^{2}+\|v^{(0)}(\mu_{3})\| _{2}^{2}\right)\left|\sum_{y_{i}=1}g_{i}^{(0)}\sum_{l=1}^{L}p_{q\gets l,k \leftarrow\mu_{3}}^{(0,i)}-\sum_{y_{i}=-1}g_{i}^{(0)}\sum_{l=1}^{L}p_{q \gets l,k\leftarrow\mu_{3}}^{(0,i)}\right|.\]

By Lemma D.8, and Corollary D.2, we have

\[\frac{1}{n}\left|\sum_{y_{i}=1}g_{i}^{(0)}\sum_{l=1}^{L}p_{q \gets l,k\leftarrow\mu_{3}}^{(0,i)}-\sum_{y_{i}=-1}g_{i}^{(0)}\sum_{l=1}^ {L}p_{q\gets l,k\leftarrow\mu_{3}}^{(0,i)}\right|\] \[\leq\frac{1}{n}\left(\sum_{y_{i}=1}\left(\frac{1}{2}+F\right)\sum _{l=1}^{L}\left(\frac{1}{L}+\frac{1}{L^{2}}\right)-\sum_{y_{i}=-1}\left(\frac {1}{2}-F\right)\sum_{l=1}^{L}\left(\frac{1}{L}-\frac{1}{L^{2}}\right)\right)\] \[\leq\left(\frac{1}{2}+F\right)\left(\frac{1}{L}+\frac{1}{Lm} \right)\frac{L}{2}-\left(\frac{1}{2}-F\right)\left(\frac{1}{L}-\frac{1}{Lm} \right)\frac{L}{2}\] \[\leq 2F+O(1/m).\]

By Lemma D.3, we have \(\|w_{j_{1}}^{(0)}\|_{2}^{2}=\Theta(\sigma_{1}^{2}m)\) and \(\|v^{(0)}(\mu_{3})\|_{2}^{2}=\Theta(\sigma_{0}^{2}m)\), and thus,

\[\left|\frac{1}{n}\sum_{i:\mu_{3}\in X^{(i)}}g_{i}^{(0)}y_{i}\sum_{l=1}^{L}a_{j_ {1}}\left(\|w_{j_{1}}^{(0)}\|_{2}^{2}+\|v^{(0)}(\mu_{3})\|_{2}^{2}\right)p_{q \gets l,k\leftarrow\mu_{3}}^{(0,i)}\right|\leq\widetilde{O}(\alpha(\sigma_ {0}^{2}+\sigma_{1}^{2})mF).\]

Further, by Proposition E.5, we have

\[|\varepsilon_{1}|=\left|\frac{1}{n}\sum_{i:\mu_{3}\in X^{(i)}}g_{i}^{(0)}y_{i} \sum_{l=1}^{L}\sum_{j_{2}\neq j_{1}}a_{j_{2}}\left\langle w_{j_{1}}^{(0)},w_{j_ {2}}^{(0)}\right\rangle p_{q\gets l,k\leftarrow\mu_{3}}^{(0,i)}\right|\]\[\frac{\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\mu_{1}}{\partial t}-\frac{ \partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\mu_{3}}{\partial t}=\Theta((\sigma_{0}^{2}+ \sigma_{1}^{2})m),\]\[\frac{\partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\mu_{2}}{\partial t}-\frac{ \partial a_{j}w_{j}^{(0)}W_{V}^{(0)}\mu_{3}}{\partial t}=\Theta((\sigma_{0}^{2}+ \sigma_{1}^{2})m).\]

By Theorem E.15 and Corollary E.24, we have

\[\sum_{l_{1}=1}^{L}\sum_{j=1}^{m_{1}}\sum_{l_{2}=1}^{L}\left(\frac{ \partial a_{j}w_{j}^{(t)}W_{V}^{(t)}X_{l_{2}}}{\partial t}\mathbb{I}(X_{l_{2}} \notin\{\mu_{i}\}_{i=1}^{3})p_{l_{1},l_{2}}^{(t,i)}+a_{j}w_{j}^{(t)}W_{V}^{(t )}X_{l_{2}}\frac{\partial p_{l_{1},l_{2}}^{(t,i)}}{\partial t}\right)\] \[\qquad=O\left(\frac{L}{n}(\sigma_{0}^{2}+\sigma_{1}^{2})mm_{1} \right).\]

Thus, for all \(i\in I_{1}\cup I_{2}\cup I_{3}\), \(\frac{\partial F_{i}^{(0)}}{\partial t}>0\), which implies \(\frac{\partial g_{i}^{(0)}}{\partial t}<0\) for \(i\in I_{1}\) and \(\frac{\partial g_{i}^{(0)}}{\partial t}>0\) for \(i\in I_{2}\cup I_{3}\).

Next, we show that there must exist a time such that \(-\frac{\partial G^{(t)}(\mu_{3})}{\partial t}=\max(\frac{\partial G^{(t)}(\mu_ {1})}{\partial t},\frac{\partial G^{(t)}(\mu_{2})}{\partial t})\). Without loss of generality, assume \(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}>\frac{\partial G^{(t)}(\mu_{2}) }{\partial t}\). We now analyze the condition when the magnitude of the update of the common token is less than the magnitude of the update of the signals. By Lemma C.1, we have

\[\sum_{j=1}^{m_{1}}\frac{\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}\mu_ {1}}{\partial t}\geq-\sum_{j=1}^{m_{1}}\frac{\partial a_{j}w_{j}^{(t)}W_{V}^{ (t)}\mu_{3}}{\partial t}\] \[\Leftrightarrow\frac{1}{n}\left(\|W^{(t)}\|_{F}^{2}+m_{1}\|v^{(t )}(\mu_{1})\|_{2}^{2}\right)\left(\sum_{i\in I_{1}}g_{i}^{(t)}\sum_{l=1}^{L}p_ {q\gets l,k\leftarrow\mu_{1}}^{(t,i)}-\sum_{i\in I_{2}}g_{i}^{(t)}\sum_{ l=1}^{L}p_{q\gets l,k\leftarrow\mu_{1}}^{(t,i)}\right)+\sum_{j=1}^{m_{1}}a_{j} \varepsilon^{(t)}(\mu_{1})\] \[\quad\geq\frac{1}{n}\left(\|W^{(t)}\|_{F}^{2}+m_{1}\|v^{(t)}(\mu_ {3})\|_{2}^{2}\right)\left(\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)} \sum_{l=1}^{L}p_{q\gets l,k\leftarrow\mu_{3}}^{(t,i)}-\sum_{i\in I_{1}}g_{ i}^{(t)}\sum_{l=1}^{L}p_{q\gets l,k\leftarrow\mu_{3}}^{(t,i)}\right)+\sum_{j=1}^{m_{1}}a_{j} \varepsilon^{(t)}(\mu_{3})\] \[\Leftrightarrow\frac{1}{n}\sum_{i\in I_{1}}g_{i}^{(t)}\sum_{l=1}^ {L}p_{q\gets l,k\leftarrow\mu_{1}}^{(t,i)}+\frac{1}{n}\frac{\|W^{(t)}\|_{F }^{2}+m_{1}\|v^{(t)}(\mu_{3})\|_{2}^{2}}{\|W^{(t)}\|_{F}^{2}+m_{1}\|v^{(t)}( \mu_{1})\|_{2}^{2}}\sum_{i\in I_{1}}g_{i}^{(t)}\sum_{l=1}^{L}p_{q\gets l,k \leftarrow\mu_{3}}^{(t,i)}\] \[\quad\geq\frac{1}{n}\frac{\|W^{(t)}\|_{F}^{2}+m_{1}\|v^{(t)}(\mu_ {3})\|_{2}^{2}}{\|W^{(t)}\|_{F}^{2}+m_{1}\|v^{(t)}(\mu_{1})\|_{2}^{2}}\sum_{i \in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}\sum_{l=1}^{L}p_{q\gets l,k \leftarrow\mu_{3}}^{(t,i)}+\frac{1}{n}\sum_{i\in I_{2}}g_{i}^{(t)}\sum_{l=1}^{ L}p_{q\gets l,k\leftarrow\mu_{1}}^{(t,i)}\] \[\quad+\frac{\sum_{j=1}^{m_{1}}a_{j}\varepsilon^{(t)}(\mu_{3})}{\|W ^{(t)}\|_{F}^{2}+m_{1}\|v^{(t)}(\mu_{1})\|_{2}^{2}}-\frac{\sum_{j=1}^{m_{1}}a_{ j}\varepsilon^{(t)}(\mu_{1})}{\|W^{(t)}\|_{F}^{2}+m_{1}\|v^{(t)}(\mu_{1})\|_{2}^{2}}\] \[\Leftrightarrow\frac{1}{n}(1+o(1))\sum_{i\in I_{1}}g_{i}^{(t)}+ \frac{1}{n}(1+o(1))\sum_{i\in I_{1}}g_{i}^{(t)}\] \[\quad\geq\frac{1}{n}(1+o(1))\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g _{i}^{(t)}+\frac{1}{n}(1+o(1))\sum_{i\in I_{2}}g_{i}^{(t)}\pm O\left(\sqrt{ \frac{m_{1}}{m}}\right)\]

where the last equality applies Corollary E.24. If \(m\) is sufficiently larger than \(m_{1}\) (by some large constant factor \(C\)), then the last term is negligible. Note that the above implies that if \(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}\geq\frac{\partial G^{(t)}(\mu_{3})}{ \partial t}\) then \(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}=\Omega(1)\) since \(\sum_{i\in I_{3}\cup I_{4}}g_{i}^{(t)}=\Omega(1)\) for all \(t\leq T_{1}\). Now, by Proposition E.21, if the initialization level is small enough, then \(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}\geq\Omega(1)\). Thus, if \(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}\geq-\frac{\partial G^{(t)}(\mu_{3})}{ \partial t}\), then \(\frac{1}{n}\sum_{i\in I_{1}}\frac{\partial g_{i}^{(t)}}{\partial t}=-\Theta(( \sigma_{0}^{2}+\sigma_{1}^{2})mm_{1})\). Therefore, there must exist a time \(t^{\prime}=\Theta(1/m)\) such that \(\frac{\partial G^{(t^{\prime})}(\mu_{1})}{\partial t}=-\frac{\partial G^{(t^{ \prime})}(\mu_{3})}{\partial t}\). Further, for \(t\geq\Theta(F/m)\), we have \(y_{i}F_{i}^{(t)}>0\) for all \(i\in I_{4}\) and \(\sum_{i\in I_{4}}g_{i}^{(t^{\prime})}<\min(\sum_{i\in I_{2}}g_{i}^{(t)},\sum_{i \in I_{3}}g_{i}^{(t)})\), where \(F=\max_{i\in[n]}|F_{i}^{(0)}|\).

Now, consider a time point \(t^{\prime}\) when \(\frac{\partial G^{(t^{\prime})}(\mu_{1})}{\partial t}=-\frac{\partial G^{(t^{ \prime})}(\mu_{3})}{\partial t}\). At \(t^{\prime}\), we must have \(\min(\sum_{i\in I_{2}}g_{i}^{(t^{\prime})},\sum_{i\in I_{3}}g_{i}^{(t^{\prime })})-\sum_{i\in I_{4}}g_{i}^{(t^{\prime})}=\Omega(1)\). Thus,

\[2\sum_{i\in I_{1}}g_{i}^{(t^{\prime})}-2\sum_{i\in I_{4}}g_{i}^{(t^{\prime})}= \Omega(1),\]

which implies

\[\frac{\partial}{\partial F}2\sum_{i\in I_{1}}g_{i}^{(t^{\prime})}-\frac{ \partial}{\partial F}\sum_{i\in I_{4}}g_{i}^{(t^{\prime})}=\Omega(1).\]

For \(i\in I_{1}\), we have

\[\frac{\partial F_{i}^{(t^{\prime})}}{\partial t} =\frac{\partial G^{(t^{\prime})}(\mu_{1})}{\partial t}+\frac{ \partial G^{(t^{\prime})}(\mu_{2})}{\partial t}+\frac{\partial G^{(t^{\prime })}(\mu_{3})}{\partial t}+O\left(\frac{L}{n}\sigma_{1}^{2}mm_{1}\right)\] \[=\frac{\partial G^{(t^{\prime})}(\mu_{2})}{\partial t}+O\left( \frac{L}{n}\sigma_{1}^{2}mm_{1}\right).\]

For \(i\in I_{2}\),

\[\frac{\partial F_{i}^{(t)}}{\partial t} =\frac{\partial G^{(t^{\prime})}(\mu_{1})}{\partial t}+\frac{ \partial G^{(t^{\prime})}(\mu_{3})}{\partial t}+O\left(\frac{L}{n}\sigma_{1}^ {2}mm_{1}\right)=O\left(\frac{L}{n}\sigma_{1}^{2}mm_{1}\right).\]

For \(i\in I_{3}\), by Proposition E.21,

\[\frac{\partial F_{i}^{(t)}}{\partial t} =\frac{\partial G^{(t^{\prime})}(\mu_{2})}{\partial t}+\frac{ \partial G^{(t^{\prime})}(\mu_{3})}{\partial t}+O\left(\frac{L}{n}\sigma_{1}^ {2}mm_{1}\right)=O\left(F\sigma_{1}^{2}mm_{1}\right),\]

and for \(i\in I_{4}\),

\[\frac{\partial F_{i}^{(t)}}{\partial t} =\frac{\partial G^{(t^{\prime})}(\mu_{3})}{\partial t}+O\left( \frac{L}{n}\sigma_{1}^{2}mm_{1}\right).\]

Thus, by chain rule \(\frac{\partial g_{i}^{(t)}}{\partial t}=\frac{\partial g_{1}}{\partial F_{i}} \frac{\partial F_{i}^{(t)}}{\partial t}\), we have

\[\frac{\partial}{\partial t}\left(2\sum_{i\in I_{1}}g_{i}^{(t^{\prime})}-2\sum _{i\in I_{2}}g_{i}^{(t^{\prime})}-\sum_{i\in I_{3}\cup I_{4}}g_{i}^{(t^{ \prime})}\right)=-\Theta(\sigma_{1}^{2}mm_{1}).\] (8)

This implies that there exists a constant \(L\) such that

\[\frac{\partial}{\partial t}\left(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2 }}g_{i}^{(t)}+(1+L)\left(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}\cup I_ {3}\cup I_{4}}g_{i}^{(t)}\right)\right)=-\Theta(\sigma_{1}^{2}mm_{1}),\]

which implies that there must exist a time \(T_{0.5}\leq O(1/m)\) such that for all \(t\geq T_{0.5}\),

\[-\frac{\partial G^{(t)}(\mu_{3})}{\partial t}\geq(1+L)\frac{\partial G^{(t)}( \mu_{1})}{\partial t}.\]

Moreover, by Theorem E.26, if \(C_{T_{1}}\) is sufficiently large, we have \(T_{0.5}<T_{1}\).

Finally, consider the time when \(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}+\frac{\partial G^{(t)}(\mu_{2})}{ \partial t}>-\frac{\partial G^{(t)}(\mu_{3})}{\partial t}\). During this time, note that we have

\[\frac{\partial G^{(t)}(\mu_{1})}{\partial t},\frac{\partial G^{(t)}(\mu_{2})} {\partial t}\geq\Theta(\sigma_{1}^{2}mm_{1}).\]

Further, if \(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}+\frac{\partial G^{(t)}(\mu_{2})} {\partial t}=-\frac{\partial G^{(t)}(\mu_{3})}{\partial t}\), then \(\frac{\partial}{\partial t}g_{i}^{(t)}=-\Theta(\sigma_{1}^{2}mm_{1})\) for all \(i\in I_{2}\cup I_{3}\cup I_{4}\). Thus, there must exist a constant \(U\) such that

\[-\frac{\partial G^{(t)}(\mu_{3})}{\partial t}\leq(1-U)\left(\frac{\partial G^{( t)}(\mu_{1})}{\partial t}+\frac{\partial G^{(t)}(\mu_{2})}{\partial t}\right)\]

for all \(t\leq T_{1}\). \(\Box\)

**Corollary E.20**.: _There exists a small positive constant \(C\) such that if we define \(T^{\prime}:=\min\{t\,:\,\min_{i\in I_{2}\cup l_{3}}F_{i}^{(t)}\geq C\}\), then \(T^{\prime}\leq O(1/m)\)._

**Proposition E.21**.: _Let \(F=\max_{i\in[n]}|F_{i}^{(0)}|\). For \(t\leq T_{1}\), we have_

\[\left|\frac{1}{n}\sum_{i\in I_{2}}g_{i}^{(t)}-\frac{1}{n}\sum_{i\in I_{3}}g_{i }^{(t)}\right|\leq O\left(F\right).\]

Proof.: First of all, by Lipschitzness of \(g\), we have

\[\left|\frac{1}{n}\sum_{i\in I_{2}}g_{i}^{(t)}-\frac{1}{n}\sum_{i\in I_{3}}g_{i }^{(t)}\right|\leq|G^{(t)}(\mu_{1})-G^{(0)}(\mu_{1})-(G^{(t)}(\mu_{2})-G^{(0)} (\mu_{2}))|+2\max_{i\in[n]}\left|F_{i}^{(0)}\right|+o(1).\]

Without loss of generality, assume \(G^{(t)}(\mu_{1})>G^{(t)}(\mu_{2})\) for all \(t\leq T_{1}\); otherwise, we can break the interval \([0,T_{1}]\) into sub-intervals by the time points when \(G^{(t)}(\mu_{1})-G^{(t)}(\mu_{2})\) changes its sign and then apply the analysis below to each sub-interval. We first derive

\[\frac{\partial G^{(t)}(\mu_{1})}{\partial t}-\frac{\partial G^{(t )}(\mu_{2})}{\partial t}\] \[=\sum_{j_{1}=1}^{m_{1}}\left(\frac{1}{n}\sum_{i_{2}\in I_{1}U_{2 }}g_{i_{2}}^{(t)}y_{i_{2}}\sum_{l_{2}=1}^{L}\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_ {j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle p_{q\gets l _{2},k\leftarrow\mu_{1}}^{(t,i_{2})}+\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^ {(t)}y_{i_{1}}\sum_{l_{1}=1}^{L}p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{ V}^{(t)}\mu_{1}\right.\] \[\quad-\frac{1}{n}\sum_{i_{2}\in I_{1}\cup I_{3}}g_{i_{2}}^{(t)}y _{i_{2}}\sum_{l_{2}=1}^{L}\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}}\left\langle w _{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle p_{q\gets l_{2},k\gets\mu _{2}}^{(t,i_{2})}-\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\sum_{l _{1}=1}^{L}p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)}\mu_{2}\right)\] \[=\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}} \left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle\left(\frac{1}{n} \sum_{i\in I_{1}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}p_{q\gets l,k\gets\mu _{1}}^{(t,i)}-\frac{1}{n}\sum_{i\in I_{1}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}p_{q \gets l,k\gets\mu_{2}}^{(t,i)}\right)\] \[\quad+m_{1}\left(\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i _{1}}\sum_{l_{1}=1}^{L}p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)} \mu_{1}-\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\sum_{l_{1}=1}^{L }p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)}\mu_{2}\right)\] \[=\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}a_{j_{2}}\left\langle w _{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle\left(\frac{1}{n}\sum_{i\in I_{3}}g_ {i}^{(t)}-\frac{1}{n}\sum_{i\in I_{2}}g_{i}^{(t)}+o(1)\right)\] \[\quad+m_{1}\left(\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i _{1}}\sum_{l_{1}=1}^{L}p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)} \mu_{1}-\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\sum_{l_{1}=1}^{L }p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)}\mu_{2}\right)\] \[\leq\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}} \left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle\left(G^{(t)}(\mu_{1} )-G^{(0)}(\mu_{1})-(G^{(t)}(\mu_{2})-G^{(0)}(\mu_{2}))\right.\] \[\quad+G^{(0)}(\mu_{1})+G^{(0)}(\mu_{2})+O\left(\max_{i\in[n]}|F_ {i}^{(0)}|\right)+o(1)\Bigg{)}\] \[\quad+m_{1}\left(\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i _{1}}\sum_{l_{1}=1}^{L}p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)} \mu_{1}-\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\sum_{l_{1}=1}^{L }p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)}\mu_{2}\right).\]

By Theorem E.14, we have

\[m_{1}\int_{0}^{T}\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i _{1}}\sum_{l_{1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)} \mu_{1}-\frac{1}{n}\sum_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}\sum_{l_{1}=1}^{L }a_{j_{1}}p_{l_{1}}^{(t,i_{1})\top}V^{(t,i_{1})\top}W_{V}^{(t)}\mu_{2}\;dt\] \[\leq T\cdot\widetilde{O}(L\sigma_{0}^{2}\sqrt{m}m_{1}),\]\[\int_{0}^{T}\left(G^{(0)}(\mu_{1})+G^{(0)}(\mu_{2})+O\left(\max_{i\in[n]}|F _{i}^{(0)}|\right)+o(1)\right)\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}a_{j_ {1}}a_{j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle\,dt\] \[\leq O\left(\left(G^{(0)}(\mu_{1})+G^{(0)}(\mu_{2})+\max_{i\in[n] }|F_{i}^{(0)}|\right)T\sigma_{1}^{2}mm_{1}\right),\]

and

\[\exp\left(\int_{0}^{T}\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{ j_{2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle\,dt\right)=O(1).\]

By Gronwall's inequality, we have

\[G^{(T)}(\mu_{1})-G^{(0)}(\mu_{1})-(G^{(T)}(\mu_{2})-G^{(0)}(\mu_ {2}))\] \[\leq T\cdot\widetilde{O}(L\sigma_{0}^{2}\sqrt{m}m_{1})+O\left( \left(G^{(0)}(\mu_{1})+G^{(0)}(\mu_{2})+\max_{i\in[n]}|F_{i}^{(0)}|\right)T \sigma_{1}^{2}mm_{1}\right)\] \[\quad+\int_{0}^{T}\left(t\cdot\widetilde{O}(L\sigma_{0}^{2}\sqrt {m})+O\left(\left(G^{(0)}(\mu_{1})+G^{(0)}(\mu_{2})+\max_{i\in[n]}|F_{i}^{(0) }|\right)t\sigma_{1}^{2}m\right)\right)\cdot O(1)\,dt\] \[\leq O\left(G^{(0)}(\mu_{1})+G^{(0)}(\mu_{2})+\max_{i\in[n]}|F_{i }^{(0)}|\right).\]

This implies that

\[\left|\frac{1}{n}\sum_{i\in I_{2}}g_{i}^{(t)}-\frac{1}{n}\sum_{i\in I_{3}}g_{ i}^{(t)}\right|\leq O\left(G^{(0)}(\mu_{1})+G^{(0)}(\mu_{2})+\max_{i\in[n]}|F_{i }^{(0)}|\right).\]

Thus, if the initialization scale is sufficiently small, \(\frac{1}{n}\sum_{i\in I_{2}}g_{i}^{(t)}\) and \(\frac{1}{n}\sum_{i\in I_{3}}g_{i}^{(t)}\) are only differed by a small constant. 

### Small Score Movement in Phase 1

**Proposition E.22**.: _Conditioned on the success of Lemma D.1 and Lemma D.3, with probability at least \(1-\delta\) over the randomness of \(a\), for all \(i\in[n]\) and \(\mu,\nu\in X^{(i)}\), we have_

\[\left|\sum_{j=1}^{m_{1}}a_{j}\|k^{(0)}(\mu)\|_{2}^{2}w_{j}^{(0) \top}v^{(0)}(\nu)\right|\leq O\left(\sigma_{0}^{2}m\sigma_{1}\sigma_{0}\sqrt{ mm_{1}\log\frac{m_{1}d}{\delta}}\sqrt{\log\frac{nd}{\delta}}\right),\] \[\left|\sum_{j=1}^{m_{1}}a_{j}\|q^{(0)}(\mu)\|_{2}^{2}w_{j}^{(0) \top}v^{(0)}(\nu)\right|\leq O\left(\sigma_{0}^{2}m\sigma_{1}\sigma_{0}\sqrt{ mm_{1}\log\frac{m_{1}d}{\delta}}\sqrt{\log\frac{nd}{\delta}}\right)\]

_and for all \(l,l^{\prime}\in[L]\) with \(K_{l}^{(0,i)}\neq k^{(0)}(\nu)\) and \(q_{l}^{(0,i)}\neq Q^{(0)}(\nu)\), we have_

\[\left|\sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{K}^{(0)\top}K_{l}^{(0,i )}V_{l^{\prime}}^{(0,i)\top}w_{j}^{(0)}\right|\leq\widetilde{O}(\sigma_{0}^{2} \sqrt{m}\sigma_{0}\sigma_{1}\sqrt{mm_{1}}),\] \[\left|\sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{Q}^{(0)\top}q_{l}^{(0,i )}V_{l^{\prime}}^{(0,i)\top}w_{j}^{(0)}\right|\leq\widetilde{O}(\sigma_{0}^{2} \sqrt{m}\sigma_{0}\sigma_{1}\sqrt{mm_{1}}).\]

Proof.: Fix \(i\in[n]\) and \(\mu,\nu\in X^{(i)}\). Consider the randomness of \(a\). By Lemma D.1, Corollary D.2 and Lemma D.3, \(a_{j}\|k^{(0)}(\mu)\|_{2}^{2}w_{j}^{(0)\top}v^{(0)}(\nu)\) is a sub-Gaussian random variable with variance proxy \(O((\sigma_{0}^{2}m\sigma_{1}\sigma_{0}\sqrt{m}\sqrt{\log(dm_{1}/\delta)})^{2})\). Then the following inequality holds.

\[\left|\sum_{j=1}^{m_{1}}a_{j}\|k^{(0)}(\mu)\|_{2}^{2}w_{j}^{(0)\top}v^{(0)}( \nu)\right|\leq O\left(\sigma_{0}^{2}m\sigma_{1}\sigma_{0}\sqrt{mm_{1}\log\frac {m_{1}d}{\delta}}\sqrt{\log\frac{2}{\delta}}\right).\]Finally, take a union bound over \(i\in[n],\;\mu,\nu\in\{\mu_{i}\}_{i=1}^{d}\). The analysis for the second term is similar.

Next, note that \(a_{j}\nu^{\top}W_{K}^{(0)\top}K_{l}^{(0,i)}V_{l^{\prime}}^{(0,i)\top}w_{j}^{(0)}\) is a sub-Gaussian random variable with variance proxy \(\widetilde{O}((\sigma_{0}^{2}\sqrt{m}\sigma_{0}\sigma_{1}\sqrt{m})^{2})\). Thus,

\[\left|\sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{K}^{(0)\top}K_{l}^{(0,i)}V_{l^{\prime }}^{(0,i)\top}w_{j}^{(0)}\right|\leq\widetilde{O}(\sigma_{0}^{2}\sqrt{m}\sigma _{0}\sigma_{1}\sqrt{mm_{1}}).\]

**Lemma E.23** (Score change).: _For all \(t\leq T_{1}\), for all \(\nu,\mu\in\{\mu_{i}\}_{i=1}^{d}\), we have_

\[\left|\frac{\partial\nu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu}{\partial t} \right|\leq\frac{1}{\sqrt{m}}\frac{\left|\left\{i:\;\mu\in X^{(i)}\right\} \right|+\left|\left\{i:\;\nu\in X^{(i)}\right\}\right|}{n}\widetilde{O}\left( \frac{1}{L}+\frac{1}{\sqrt{m}}\right)\]

_and thus,_

\[\left|\nu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu-\nu^{\top}W_{K}^{(0)\top}W_{Q}^ {(0)}\mu\right|\leq t\frac{1}{\sqrt{m}}\frac{\left|\left\{i:\;\mu\in X^{(i)} \right\}\right|+\left|\left\{i:\;\nu\in X^{(i)}\right\}\right|}{n}\widetilde{O }\left(\frac{1}{L}+\frac{1}{\sqrt{m}}\right).\]

Proof.: First of all, by Lemma C.4, we expand the per step gradient descent update as follows:

\[\frac{\partial\nu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu}{\partial t }\] \[=\underbrace{\frac{1}{n\sqrt{m}}\sum_{i:\mu,\nu\in X^{(i)}}g_{i}^ {(t)}y_{i}\sum_{j=1}^{m_{1}}a_{j}\|k^{(t)}(\nu)\|_{2}^{2}\left(v^{(t)\top}(\nu )w_{j}^{(t)}-w_{j}^{(t)\top}V^{(t,i)}p_{l(i,\mu)}^{(t,i)}\right)p_{q\leftarrow \mu,k\leftarrow\nu}^{(t,i)}}_{(1)}\] \[\quad+\underbrace{\frac{1}{n\sqrt{m}}\sum_{i:\mu\in X^{(i)}}g_{i}^ {(t)}y_{i}\sum_{j=1}^{m_{1}}a_{j}\sum_{l=1}^{L}\nu^{\top}W_{K}^{(t)\top}K_{l}^ {(t,i)}\left(V_{l}^{(t,i)\top}w_{j}^{(t)}-w_{j}^{(t)\top}V^{(t,i)}p_{l(i,\mu) }^{(t,i)}\right)p_{q\leftarrow\mu,k\leftarrow l}^{(t)}\mathbb{I}(K_{l}^{(t,i) }\neq k^{(t)}(\nu))}_{(2)}\] \[\quad+\underbrace{\frac{1}{n\sqrt{m}}\sum_{i:\nu,\mu\in X^{(i)}}g _{i}^{(t)}y_{i}\sum_{j=1}^{m_{1}}a_{j}\|q^{(t)}(\mu)\|_{2}^{2}p_{q\leftarrow \mu,k\leftarrow\nu}^{(t,i)}\left(w_{j}^{(t)\top}v^{(t,i)}(\nu)-w_{j}^{(t)\top }V^{(t,i)}p_{l(i,\mu)}^{(t,i)}\right)}_{(3)}\] \[\quad+\underbrace{\frac{1}{n\sqrt{m}}\sum_{i:\nu\in X^{(i)}}g_{i} ^{(t)}y_{i}\sum_{l=1}^{L}\sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{Q}^{(t)\top}q_{l }^{(t,i)}p_{q\gets l,k\leftarrow\nu}^{(t,i)}\left(w_{j}^{(t)\top}v^{(t,i)} (\nu)-w_{j}^{(t)\top}V^{(t,i)}p_{l}^{(t,i)}\right)\mathbb{I}(q_{l}^{(t,i)}\neq q ^{(t)}(\mu))}_{(4)}\]

**Analysis of \((1)\)**: By triangle inequality, we have

\[|(1)| \leq\underbrace{\frac{1}{n\sqrt{m}}\left|\sum_{i:\mu,\nu\in X^{(i )}}g_{i}^{(t)}y_{i}\sum_{j=1}^{m_{1}}a_{j}\|k^{(t)}(\nu)\|_{2}^{2}v^{(t)\top} (\nu)w_{j}^{(t)}p_{q\leftarrow\mu,k\leftarrow\nu}^{(t,i)}\right|}_{(a)}\] \[\quad+\underbrace{\frac{1}{n\sqrt{m}}\left|\sum_{i:\mu,\nu\in X^{ (i)}}g_{i}^{(t)}y_{i}\sum_{j=1}^{m_{1}}a_{j}\|k^{(t)}(\nu)\|_{2}^{2}w_{j}^{(t) \top}V^{(t,i)}p_{l(i,\mu)}^{(t,i)}p_{q\leftarrow\mu,k\leftarrow\nu}^{(t,i)} \right|}_{(b)}.\]

To analyze \((a)\), since \((a+\varepsilon_{1})(b+\varepsilon_{2})=ab+a\varepsilon_{2}+b\varepsilon_{1}+ \varepsilon_{1}\varepsilon_{2}\), we have

\[\left|\sum_{j=1}^{m_{1}}a_{j}\|k^{(t)}(\nu)\|_{2}^{2}w_{j}^{(t)\top}v^{(t)}( \nu)p_{q\leftarrow\mu,k\leftarrow\nu}^{(t,i)}-\sum_{j=1}^{m_{1}}a_{j}\|k^{(0)} (\nu)\|_{2}^{2}w_{j}^{(0)\top}v^{(0)}(\nu)p_{q\leftarrow\mu,k\leftarrow\nu}^{(0,i)}\right|\]\[\leq m_{1}\left(R_{K}\widetilde{O}(\sigma_{0}\sigma_{1}\sqrt{m})+R \widetilde{O}(\sigma_{0}^{2}m)+R_{K}R\right)\left(\frac{2}{L}+R_{P}\right)\] \[\quad+\widetilde{O}\left(R_{P}\sigma_{0}^{2}m\sigma_{1}\sigma_{0} \sqrt{mm_{1}}\right)+\widetilde{O}\left(\sigma_{0}^{2}m\sigma_{1}\sigma_{0} \sqrt{mm_{1}}\frac{1}{L}\right).\]

On the other hand, to analyze \((b)\), we have

\[\left|\sum_{j=1}^{m_{1}}a_{j}\|k^{(t)}(\nu)\|_{2}^{2}w_{j}^{(t) \top}V^{(t,i)}p_{l(i,\mu)}^{(t,i)}p_{q\leftarrow\mu,k\leftarrow\nu}^{(t,i)}- \sum_{j=1}^{m_{1}}a_{j}\|k^{(0)}(\nu)\|_{2}^{2}w_{j}^{(0)\top}V^{(0,i)}p_{l(i, \mu)}^{(0,i)}p_{q\leftarrow\mu,k\leftarrow\nu}^{(0,i)}\right|\] \[\leq\left|\sum_{j=1}^{m_{1}}a_{j}\|k^{(t)}(\nu)\|_{2}^{2}w_{j}^{( t)\top}V^{(t,i)}p_{l(i,\mu)}^{(t,i)}-\sum_{j=1}^{m_{1}}a_{j}\|k^{(0)}(\nu)\|_{2}^{2} w_{j}^{(0)\top}V^{(0,i)}p_{l(i,\mu)}^{(0,i)}\right|\cdot p_{q\leftarrow\mu,k \leftarrow\nu}^{(t,i)}\] \[\quad+\left|\sum_{j=1}^{m_{1}}a_{j}\|k^{(0)}(\nu)\|_{2}^{2}w_{j}^ {(0)\top}V^{(0,i)}p_{l(i,\mu)}^{(0,i)}\right|\cdot\left|p_{q\leftarrow\mu,k \leftarrow\nu}^{(t,i)}-p_{q\leftarrow\mu,k\leftarrow\nu}^{(0,i)}\right|.\] (10)

[MISSING_PAGE_EMPTY:40]

\[\leq\frac{1}{\sqrt{m}}\frac{\big{|}\big{\{}i:\;\mu\in X^{(i)}\big{\}} \big{|}+\big{|}\big{\{}i:\;\nu\in X^{(i)}\big{\}}\big{|}}{n}\widetilde{O}(1/L+1/ \sqrt{m}).\]

**Corollary E.24** (Softmax change).: _For \(t\leq T_{1}\), we have the following:_* _if both_ \(X^{(i)}_{l_{1},l_{2}},X^{(i)}_{l_{2}}\in\mathcal{R}\)_, then_ \[\left|\frac{\partial p^{(t,i)}_{l_{1},l_{2}}}{\partial t}\right|\leq\widetilde{O} \left(\frac{1}{L(L^{2}+n)\sqrt{m}}\right),\]
* _otherwise,_ \[\left|\frac{\partial p^{(t,i)}_{l_{1},l_{2}}}{\partial t}\right|\leq\widetilde {O}\left(\frac{1}{L^{2}\sqrt{m}}\right).\]

_Thus,_

* _if both_ \(X^{(i)}_{l_{1}},X^{(i)}_{l_{2}}\in\mathcal{R}\)_, then_ \[\left|p^{(t,i)}_{l_{1},l_{2}}-p^{(0,i)}_{l_{1},l_{2}}\right|\leq\widetilde{O} \left(\left(\frac{1}{L}+\frac{1}{n}\right)\frac{1}{Lm^{2}}\left(\frac{1}{L}+ \frac{1}{\sqrt{m}}\right)\right),\]
* _otherwise,_ \[\left|p^{(t,i)}_{l_{1},l_{2}}-p^{(0,i)}_{l_{1},l_{2}}\right|\leq\widetilde{O }\left(\frac{1}{Lm^{2}}\left(\frac{1}{L}+\frac{1}{\sqrt{m}}\right)\right).\]

Proof.: Consider fixed \(i,l_{1},l_{2}\). By Lemma G.2, we have

\[\left|\frac{\partial p^{(t,i)}_{l_{1},l_{2}}}{\partial t}\right|\leq p^{(t,i) }_{l_{1},l_{2}}\left|\frac{\partial s^{(t,i)}_{l_{1},l_{2}}}{\partial t}\right| +p^{(t,i)}_{l_{1},l_{2}}\left|p^{(t,i)\top}_{l_{1}}\frac{\partial s^{(t,i)}_{ l_{1}}}{\partial t}\right|.\]

By Lemma E.23, we have the following cases:

* if both \(X^{(i)}_{l_{1}},X^{(i)}_{l_{2}}\in\mathcal{R}\), then \[\left|\frac{\partial s^{(t,i)}_{l_{1},l_{2}}}{\partial t}\right|\leq\widetilde {O}\left(\frac{1}{nm}\left(\frac{1}{L}+\frac{1}{\sqrt{m}}\right)\right);\]
* otherwise, \[\left|\frac{\partial s^{(t,i)}_{l_{1},l_{2}}}{\partial t}\right|\leq\widetilde {O}\left(\frac{1}{m}\left(\frac{1}{L}+\frac{1}{\sqrt{m}}\right)\right).\]

Next, during Phase 1, we have \(p^{(t,i)}_{l_{1},l_{2}}\leq\frac{1}{L}+\frac{1}{Lm}+R_{P}\). Therefore,

* if \(X^{(i)}_{l_{1}}\in\mathcal{R}\), then \[\left|p^{(t,i)\top}_{l_{1}}\frac{\partial s^{(t,i)}_{l_{1}}}{\partial t}\right| \leq\widetilde{O}\left(\left(\frac{1}{L}+\frac{1}{n}\right)\frac{1}{m}\left( \frac{1}{L}+\frac{1}{\sqrt{m}}\right)\right);\]
* otherwise, \[\left|p^{(t,i)\top}_{l_{1}}\frac{\partial s^{(t,i)}_{l_{1}}}{\partial t} \right|\leq\widetilde{O}\left(\frac{1}{m}\left(\frac{1}{L}+\frac{1}{\sqrt{m}} \right)\right).\]

Thus,

* if both \(X^{(i)}_{l_{1}},X^{(i)}_{l_{2}}\in\mathcal{R}\), then \[\left|\frac{\partial p^{(t,i)}_{l_{1},l_{2}}}{\partial t}\right|\leq\widetilde {O}\left(\left(\frac{1}{L}+\frac{1}{n}\right)\frac{1}{Lm}\left(\frac{1}{L}+ \frac{1}{\sqrt{m}}\right)\right);\]* otherwise, \[\left|\frac{\partial p_{l_{1},l_{2}}^{(t,i)}}{\partial t}\right|\leq\widetilde{O} \left(\frac{1}{Lm}\left(\frac{1}{L}+\frac{1}{\sqrt{m}}\right)\right).\]

This implies that

* if both \(X_{l_{1}}^{(i)},X_{l_{2}}^{(i)}\in\mathcal{R}\), then \[\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^{(0,i)}\right|\leq\widetilde{O} \left(\left(\frac{1}{L}+\frac{1}{n}\right)\frac{1}{Lm^{2}}\left(\frac{1}{L}+ \frac{1}{\sqrt{m}}\right)\right);\]
* otherwise, \[\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^{(0,i)}\right|\leq\widetilde{O} \left(\frac{1}{Lm^{2}}\left(\frac{1}{L}+\frac{1}{\sqrt{m}}\right)\right).\]

**Lemma E.25** (\(K,Q\) self-correlation change).: _For \(t\leq T_{1}\), for \(\mu,\nu\in\{\mu_{i}\}_{i=1}^{d}\), we have_

\[\left|\mu^{\top}W_{K}^{(t)\top}W_{K}^{(t)}\nu-\mu^{\top}W_{K}^{(0 )\top}W_{K}^{(0)}\nu\right|\leq t\frac{1}{\sqrt{m}}\frac{\left|\left\{i:\;\mu \in X^{(i)}\right\}\right|+\left|\left\{i:\;\nu\in X^{(i)}\right\}\right|}{n} \widetilde{O}\left(\frac{1}{\sqrt{m}}\right),\] \[\left|\mu^{\top}W_{Q}^{(t)\top}W_{Q}^{(t)}\nu-\mu^{\top}W_{Q}^{(0 )\top}W_{Q}^{(0)}\nu\right|\leq t\frac{1}{\sqrt{m}}\frac{\left|\left\{i:\;\mu \in X^{(i)}\right\}\right|+\left|\left\{i:\;\nu\in X^{(i)}\right\}\right|}{n} \widetilde{O}\left(\frac{1}{\sqrt{m}}\right).\]

Proof.: By Lemma C.4, we only need to replace \(\widetilde{O}(\sigma_{0}^{2}m)\) by \(\widetilde{O}(\sigma_{0}^{2}\sqrt{m})\) in the proof of Lemma E.23. Thus, we omit the proof here. 

### All Variables are within Range in Definition of Phase 1

Finally, we prove that at the end of Phase 1, all the variables in Definition E.2 stay in the range.

**Theorem E.26**.: _For \(t\leq C_{T_{1}}/(\sigma_{1}^{2}mm_{1})\) (where the constant \(C_{T_{1}}\) is from the definition of \(T_{1}\) in Definition E.2), all of the following hold:_

1. \(\max_{j\in[m],\mu\in\{\mu_{i}\}_{i=1}^{3}}\left|w_{j}^{(t)}W_{V}^{(t)}\mu-w_{ j}^{(0)}W_{V}^{(0)}\mu\right|\leq R\)_, where_ \(R=O(1/m_{1})\)_;_
2. \(\max_{j\in[m],\mu\notin\{\mu_{i}\}_{i=1}^{3}}\left|w_{j}^{(t)}W_{V}^{(t)}\mu-w _{j}^{(0)}W_{V}^{(0)}\mu\right|\leq O(R/n+R/\sqrt{m})\)_;_
3. \(\max_{\mu,\nu\in\{\mu_{i}\}_{i=1}^{d}}\left|\mu^{\top}{W_{Q}^{(t)\top}}^{W}_{ K}\nu-\mu^{\top}{W_{Q}^{(0)\top}}^{W}_{K}^{(0)}\nu\right|\leq R_{S}\)_, where_ \(R_{S}\leq O(1/m\sqrt{m})\)_;_
4. \(\max_{\mu,\nu\in\{\mu_{i}\}_{i=1}^{d}}\left|\mu^{\top}{W_{Q}^{(t)\top}}^{W}_{ Q}\nu-\mu^{\top}{W_{Q}^{(0)\top}}^{W}_{Q}\nu\right|\leq R_{Q}\)_;_
5. \(\max_{\mu,\nu\in\{\mu_{i}\}_{i=1}^{d}}\left|\mu^{\top}{W_{K}^{(t)\top}}^{W}_{ K}\nu-\mu^{\top}{W_{K}^{(0)\top}}^{W}_{K}^{(0)}\nu\right|\leq R_{K}\)_._

_Thus, \(T_{1}=C_{T_{1}}/(\sigma_{1}^{2}mm_{1})\)._

Proof.: The first two results are proved by Theorem E.14, Theorem E.15, Lemma E.17. The third result is proved by Lemma E.23, The fourth and fifth results are proved by Lemma E.25. 

**Theorem E.27** (End of Phase 1).: _At \(t=T_{1}\), we have \(y_{i}F_{i}^{(T_{1})}=\Theta(1)\) for all \(i\in[n]\), and_

\[G^{(T_{1})}(\mu_{1})=\Theta(1),\quad G^{(T_{1})}(\mu_{2})=\Theta(1),\quad-G^{(T_ {1})}(\mu_{3})=\Theta(1),\]

\[\forall\mu\in\mathcal{R}:\;G^{(T_{1})}(\mu)=\widetilde{O}(\sigma_{0}\sigma_{1} \sqrt{mm_{1}}).\]

_Further,_

\[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(T_ {1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\right\rangle=\Theta(\sigma_{1}^{2}mm_{1}).\]Proof.: By Theorem E.26, we have \(y_{i}F_{i}^{(T_{1})}=O(1)\) and

\[G^{(T_{1})}(\mu_{1})=O(1),\quad G^{(T_{1})}(\mu_{2})=O(1),\quad-G^{(T_{1})}(\mu_{ 3})=O(1).\]

Further, by Theorem E.14 and Theorem E.19, we have \(y_{i}F_{i}^{(T_{1})}\geq\Omega(1)\) for \(i\in I_{1}\) and

\[G^{(T_{1})}(\mu_{1})\geq\Omega(1),\quad G^{(T_{1})}(\mu_{2})\geq\Omega(1).\]

Finally, by Theorem E.26, we have that \(T_{1}=C_{T_{1}}/m\). And note that if the constant \(C_{T_{1}}\) in Definition E.2 is sufficiently large, then by Corollary E.20, we have \(T^{\prime}\leq T_{1}\). Thus, we have \(y_{i}F_{i}^{(T_{1})}\geq\Omega(1)\) for \(i\in I_{2}\cup I_{3}\cup I_{4}\) and \(-G^{(T_{1})}(\mu_{3})\geq\Omega(1)\).

By Lemma D.7 and Theorem E.15, we have

\[\forall\mu\in\mathcal{R}:\;G^{(T_{1})}(\mu)=\widetilde{O}(\sigma_{0}\sigma_{1 }\sqrt{mm_{1}}).\]

Finally, by Lemma D.3, Proposition E.5 and Proposition E.12, we have

\[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{ (T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\right\rangle=\Theta(\sigma_{1}^{2}mm_{1}).\]

**Theorem E.28** (Phase 1, formal restatement of Theorem 3.1).: _With probability at least \(1-\delta\) over the randomness of weight initialization, there exists a time \(T_{1}=\widetilde{O}(1/m)\) such that_

* \(G^{(T_{1})}(\mu_{1})\geq\Omega(1),\;G^{(T_{1})}(\mu_{2})\geq\Omega(1),\;G^{(T_ {1})}(\mu_{3})\leq-\Omega(1)\)_._
* _All the training samples are correctly classified:_ \(y_{i}F_{i}^{(T_{1})}=\Omega(1)\) _for all_ \(i\in[n]\)_._
* _For_ \(t\in[0,T_{1}]\)_,_ \[\left|\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle- \left\langle w_{j_{1}}^{(0)},w_{j_{2}}^{(0)}\right\rangle\right|\leq\widetilde {O}\left(\frac{1}{m}\right)\] \[\left|\left\langle v^{(t)}(\mu),v^{(t)}(\nu)\right\rangle-\left \langle v^{(0)}(\mu),v^{(0)}(\nu)\right\rangle\right|\leq\widetilde{O}\left( \frac{1}{m}\right)\] \[\left|\nu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu-\nu^{\top}W_{K}^{(0 )\top}W_{Q}^{(0)}\mu\right|\leq\widetilde{O}\left(\frac{1}{m^{3/2}}\left( \frac{1}{L}+\frac{1}{\sqrt{m}}\right)\right)\] \[\left|\mu^{\top}W_{K}^{(t)\top}W_{K}^{(t)}\nu-\mu^{\top}W_{K}^{(0 )\top}W_{K}^{(0)}\nu\right|\leq\widetilde{O}\left(\frac{1}{m^{2}}\right)\] \[\left|\mu^{\top}W_{Q}^{(t)\top}W_{Q}^{(t)}\nu-\mu^{\top}W_{Q}^{(0 )\top}W_{Q}^{(0)}\nu\right|\leq\widetilde{O}\left(\frac{1}{m^{2}}\right)\]
* _The training loss satisfies_ \(\widehat{L}^{(T_{1})}=\Theta(1)\)_._

Proof.: The first two results are proved in Theorem E.27. The third result is proved by Lemma E.9, Proposition E.12, Lemma E.23, and Lemma E.25. The result on the training loss is a direct consequence of Definition E.2. 

## Appendix F Training Dynamics: Phase 2

The idea of the proof is to first define conditions for Phase 2, which guarantees that the small training loss can be achieved. Then we will show that those conditions can be satisfied starting from the end of Phase 1 and up to at least \(\Omega(\text{poly}(m))\) time, which will serve as the end of Phase 2.

**Definition F.1**.: _We define Phase 2 of the training to be \(t\in[T_{1},T_{2}]\) such that_* _The change of_ \(K,Q\) _self-correlation is small:_ \[\max_{\mu,\nu}\left|\mu^{\top}{W_{K}^{(t)}}^{\top}{W_{K}^{(t)}} \nu-\mu^{\top}{W_{K}^{(0)}}^{\top}{W_{K}^{(0)}}\nu\right|=\widetilde{O}(\sigma_{ 0}^{2}\sqrt{m})\] \[\max_{\mu,\nu}\left|\mu^{\top}{W_{Q}^{(t)}}^{\top}{W_{Q}^{(t)}} \nu-\mu^{\top}{W_{Q}^{(0)}}^{\top}{W_{Q}^{(0)}}\nu\right|=\widetilde{O}(\sigma_ {0}^{2}\sqrt{m})\]
* _The change of softmax probability satisfies:_ \[\max_{l_{1},l_{2}\in[L],\;i\in[n]}\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2} }^{(0,i)}\right|<O(1/L^{2})\]
* _The sum of neuron correlation satisfies_ \[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{ (t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\leq O(\sigma_{1}^{2}mm_{1}).\]
* _The gradient of_ \(W,W_{V}\) _satisfies_ \[\frac{\max_{\mu\in\{\mu_{i}\}_{i=1}^{d}}\left|\sum_{j=1}^{m_{1}}a_ {j}\frac{\partial w_{j_{1}}^{(t)}}{\partial t}W_{V}^{(t)}\mu\right|}{\sum_{j_{ 1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_ {2}}w_{j_{2}}^{(t)}\right\rangle}\leq o(1/L)\frac{1}{n}\sum_{i\in[n]}g_{i}^{(t)}\] \[\max_{i\in[n]}\left|\sum_{l_{1}=1}^{L}\sum_{l_{2}=1}^{L}G^{(t)}(X_{l_{2}}^{ (i)})\frac{\partial p_{l_{1},l_{2}}^{(t,i)}}{\partial t}\right|\] \[\frac{1}{\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}} w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle}\leq o(1)\frac{1}{n}\sum_{i\in[n]}g_{i}^{(t)}\]
* _The gradients_ \(\sum_{i\in I}g_{i}^{(t)}\) _for_ \(I\in\{I_{1},I_{2},I_{3},I_{4}\}\) _satisfies_ \[\sum_{i\in I_{4}}g_{i}\leq\min\left(\sum_{i\in I_{2}}g_{i}^{(t)},\sum_{i\in I _{3}}g_{i}^{(t)}\right)\leq\max\left(\sum_{i\in I_{2}}g_{i}^{(t)},\sum_{i\in I _{3}}g_{i}^{(t)}\right)\leq\sum_{i\in I_{1}}g_{i}^{(t)}\leq\sum_{i\in I_{2} \cup I_{3}\cup I_{4}}g_{i}^{(t)}\] _and_ \[\frac{1}{2}\leq\frac{\sum_{i\in I_{2}}g_{i}^{(t)}}{\sum_{i\in I_{3}}g_{i}^{(t )}}\leq 2.\]
* \(y_{i}F_{i}^{(t)}>C\) _for all_ \(i\in[n]\) _for some fixed constant_ \(C\)_._
* \(|G^{(t)}(\mu)|\leq O(\log m)\) _for_ \(\mu\in\{\mu_{i}\}_{i=1}^{3}\)_._
* \(\sum_{l_{1}=1}^{L}\sum_{l_{2}:\;X_{l_{2}}^{(i)}\in\{\mu_{k}\}_{k=4}^{d}}G^{(t )}(X_{l_{2}}^{(i)})p_{q\gets l_{1},k\gets l_{2}}^{(t,i)}\leq O(1)\)_._
* \(T_{2}\leq O(\text{poly}(m))\)_._

**Corollary F.2**.: _For \(t\in[T_{1},T_{2}]\), if \(i,j\in I\) where \(I\in\{I_{1},I_{2},I_{3},I_{4}\}\), then_

\[\max_{i,j\in I}\frac{g_{i}^{(t)}}{g_{j}^{(t)}}\leq O(1).\]

Proof.: Take \(I=I_{1}\) and the proof is similar for the remaining cases. For fixed \(i,j\in I_{1}\), we have

\[\frac{g_{i}^{(t)}}{g_{j}^{(t)}}=\frac{1+\exp(y_{j}F_{j}^{(t)})}{1+\exp(y_{i}F_ {i}^{(t)})}\leq 2\frac{\exp(y_{j}F_{j}^{(t)})}{\exp(y_{i}F_{i}^{(t)})},\]

where the inequality is due to \(y_{i}F_{i}^{(t)}\geq C\) in Definition F.1. Now we consider

\[\frac{\exp(y_{j}F_{j}^{(t)})}{\exp(y_{i}F_{i}^{(t)})}\]\[\max_{\mu,\nu}\left|\mu^{\top}W_{K}^{(T_{1})\top}W_{K}^{(T_{1})}\nu- \mu^{\top}W_{K}^{(0)\top}W_{K}^{(0)}\nu\right|\right.=\widetilde{O}(\sigma_{0}^{2 }\sqrt{m}),\] \[\max_{\mu,\nu}\left|\mu^{\top}W_{Q}^{(T_{1})\top}W_{Q}^{(T_{1})} \nu-\mu^{\top}W_{Q}^{(0)\top}W_{Q}^{(0)}\nu\right|\right.=\widetilde{O}(\sigma_ {0}^{2}\sqrt{m}).\]

Next, by Proposition E.12, Proposition E.5 and Lemma D.3, we have

\[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}} w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\right\rangle=\Theta(\sigma_{1}^{2} mm_{1}).\]

Recall that \(\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(T_{1})}=\Theta(1)\). On the other hand, by Corollary E.10, Lemma D.3, we have

\[\max_{\mu\in\{\mu_{i}\}_{i=1}^{d}}\left|\sum_{j=1}^{m_{1}}a_{j} \frac{\partial w_{j}^{(T_{1})}}{\partial t}W_{V}^{(T_{1})}\mu\right|=O(\sigma_ {0}^{2}mm_{1})\] \[\implies \frac{\max_{\mu\in\{\mu_{i}\}_{i=1}^{d}}\left|\sum_{j=1}^{m_{1}}a_ {j}\frac{\partial w_{j}^{(T_{1})}}{\partial t}W_{V}^{(T_{1})}\mu\right|}{\sum _{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(T_{ 1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\right\rangle}\leq O\left(\frac{\sigma_{0}^{2 }}{\sigma_{1}^{2}}\right)=\widetilde{O}\left(\frac{m_{1}}{Lm}\right).\]Also, by Corollary E.24, we have

\[\max_{i\in[n]}\left|\sum_{l_{1}=1}^{L}\sum_{l_{2}=1}^{L}G^{(T_{1})}(X _{l_{2}}^{(i)})\frac{\partial p_{l_{1},l_{2}}^{(T_{1},i)}}{\partial t}\right|= \widetilde{O}\left(\frac{1}{\sqrt{m}}\right)\] \[\implies \frac{\max_{i\in[n]}\left|\sum_{l_{1}=1}^{L}\sum_{l_{2}=1}^{L}G^{ (T_{1})}(X_{l_{2}}^{(i)})\frac{\partial p_{l_{1},l_{2}}^{(T_{1},i)}}{\partial t }\right|}{\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w _{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\right\rangle}\leq\widetilde{O} \left(\frac{1}{m^{3/2}}\right)\leq o(1)\sum_{i\in[n]}g_{i}^{(T_{1})}.\]

Further, by Corollary E.24, we have

\[\max_{l_{1},l_{2}\in[L],\ i\in[n]}\left|p_{l_{1},l_{2}}^{(T_{1},i)}-p_{l_{1}, l_{2}}^{(0,i)}\right|<O(1/L^{2}).\]

Now, by Proposition E.21, if \(F\) is small enough (which can be achieved by making the initialization scale small enough), then

\[\frac{1}{2}<\frac{\sum_{i\in I_{2}}g_{i}^{(T_{1})}}{\sum_{i\in I_{3}}g_{i}^{(T _{1})}}<2.\]

Lastly, by Theorem E.27, we have \(y_{i}F_{i}^{(T_{1})}\geq\Omega(1)\). And it is straightforward to see that \(|G^{(T_{1})}(\mu)|\leq O(\log m)\) for \(\mu\in\{\mu_{i}\}_{i=1}^{3}\).

Finally, we prove \(\sum_{l_{1}=1}^{L}\sum_{l_{2}:\ X_{l_{2}}^{(i)}\in\{\mu_{k}\}_{k=4}^{d}}G^{(T _{1})}(X_{l_{2}}^{(i)})p_{q\gets l_{1},k\gets l_{2}}^{(T_{1},i)}\leq O (1)\). A simple corollary from Lemma D.7 and Lemma D.8 is that

\[\left|\sum_{l_{1}=1}^{L}\sum_{l_{2}:\ X_{l_{2}}^{(i)}\in\{\mu_{k}\}_{k=4}^{d}}G ^{(0)}(X_{l_{2}}^{(i)})p_{q\gets l_{1},k\gets l_{2}}^{(0,i)}\right| \leq O(1).\]

Next, we have

\[\left|\sum_{l_{1}=1}^{L}\sum_{l_{2}:\ X_{l_{2}}^{(i)}\in\{\mu_{k} \}_{k=4}^{d}}G^{(T_{1})}(X_{l_{2}}^{(i)})p_{q\gets l_{1},k\gets l_{2}} ^{(T_{1},i)}-\sum_{l_{1}=1}^{L}\sum_{l_{2}:\ X_{l_{2}}^{(i)}\in\{\mu_{k}\}_{k=4 }^{d}}G^{(0)}(X_{l_{2}}^{(i)})p_{q\gets l_{1},k\gets l_{2}}^{(0,i)}\right|\] \[\leq\sum_{l_{1}=1}^{L}\sum_{l_{2}:\ X_{l_{2}}^{(i)}\in\{\mu_{k}\}_{ k=4}^{d}}\left|(G^{(T_{1})}(X_{l_{2}}^{(i)})-G^{(0)}(X_{l_{2}}^{(i)})) \right|p_{q\gets l_{1},k\gets l_{2}}^{(0,i)}\] \[\quad+\sum_{l_{1}=1}^{L}\sum_{l_{2}:\ X_{l_{2}}^{(i)}\in\{\mu_{k} \}_{k=4}^{d}}G^{(0)}(X_{l_{2}}^{(i)})\left|p_{q\gets l_{1},k\gets l_{2}} ^{(T_{1},i)}-p_{q\gets l_{1},k\gets l_{2}}^{(0,i)}\right|\] \[\quad+\sum_{l_{1}=1}^{L}\sum_{l_{2}:\ X_{l_{2}}^{(i)}\in\{\mu_{k} \}_{k=4}^{d}}\left|G^{(T_{1})}(X_{l_{2}}^{(i)})-G^{(0)}(X_{l_{2}}^{(i)})\right| \left|p_{q\gets l_{1},k\gets l_{2}}^{(T_{1},i)}-p_{q\gets l_{1},k \gets l_{2}}^{(0,i)}\right|\] \[\leq O(1)\]

which proves that

\[\sum_{l_{1}=1}^{L}\sum_{l_{2}:\ X_{l_{2}}^{(i)}\in\{\mu_{k}\}_{k=4}^{d}}G^{(T_{ 1})}(X_{l_{2}}^{(i)})p_{q\gets l_{1},k\gets l_{2}}^{(T_{1},i)}\leq O(1).\]

In Phase 2, we will analyze the dynamical system in a different way since all the variables now might change dramatically from their values at initialization.

**Lemma F.4**.: _For \(t\in[T_{1},T_{2}]\), we have_

\[\frac{\partial}{\partial t}\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left<a_{j _{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right>=\frac{2m_{1}}{n}\sum_{i=1 }^{n}g_{i}^{(t)}y_{i}F_{i}^{(t)}>0\]

_and thus,_

\[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left<a_{j_{1}}w_{j_{1}}^{(t)},a_{j _{2}}w_{j_{2}}^{(t)}\right>=\Omega(\sigma_{1}^{2}mm_{1}).\]

Proof.: By Lemma C.2, we obtain

\[\frac{\partial}{\partial t}\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m _{1}}\left<a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right>\] \[=\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left(\frac{1}{n} \sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l_{1}=1}^{L}a_{j_{1}}w_{j_{1}}^{(t)\top}V ^{(t,i)}p_{l_{1}}^{(t,i)}+\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}\sum_{l_{2} =1}^{L}a_{j_{2}}w_{j_{2}}^{(t)\top}V^{(t,i)}p_{l_{2}}^{(t,i)}\right)\] \[=\frac{2m_{1}}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}F_{i}^{(t)}.\]

Then, note that the final term is positive since by Definition F.1, we have \(y_{i}F_{i}^{(t)}>0\) for all \(i\in[n]\). Finally, by Theorem E.27, we have for all \(t\in[T_{1},T_{2}]\),

\[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left<a_{j_{1}}w_{j_{1}}^{(t)},a_{ j_{2}}w_{j_{2}}^{(t)}\right>\geq\Omega(\sigma_{1}^{2}mm_{1}).\]

### Automatic Balancing of Gradients

**Lemma F.5** (Same as Lemma 4.4).: _For \(t\in[T_{1},T_{2}]\), there exists a small constant \(C\ll 1\) such that_

\[\frac{\left|\sum_{i\in I_{2}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}\right|}{ \min(\sum_{i\in I_{2}}g_{i}^{(t)},\sum_{i\in I_{3}}g_{i}^{(t)})}\leq C.\]

Proof.: Without loss of generality, assume \(\sum_{i\in I_{2}}g_{i}^{(t)}>\sum_{i\in I_{3}}g_{i}^{(t)}\). Then, we have

\[\frac{\left|\sum_{i\in I_{2}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}\right|}{ \min(\sum_{i\in I_{2}}g_{i}^{(t)},\sum_{i\in I_{3}}g_{i}^{(t)})}=\frac{\sum_{i \in I_{2}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}}{\sum_{i\in I_{3}}g_{i}^{(t )}}=\frac{\sum_{i\in I_{2}}g_{i}^{(t)}}{\sum_{i\in I_{3}}g_{i}^{(t)}}-1.\]

Now by the quotient rule, we have \(\frac{\partial}{\partial t}\left(\frac{\sum_{i\in I_{2}}g_{i}^{(t)}}{\sum_{i \in I_{3}}g_{i}^{(t)}}\right)\leq 0\) if and only if

\[\frac{\partial}{\partial t}\left(\sum_{i\in I_{2}}g_{i}^{(t)} \right)\left(\sum_{i\in I_{3}}g_{i}^{(t)}\right)-\left(\sum_{i\in I_{2}}g_{i}^ {(t)}\right)\frac{\partial}{\partial t}\left(\sum_{i\in I_{3}}g_{i}^{(t)} \right)\leq 0\] \[\Leftrightarrow\left(\sum_{i\in I_{2}}g_{i}^{(t)}\right)\left( \sum_{i\in I_{3}}g_{i}^{(t)}\right)\left(\frac{1}{\sum_{i\in I_{2}}g_{i}^{(t )}}\sum_{i\in I_{2}}g_{i}^{(t)}\right)\left(\frac{\partial y_{i}F_{i}^{(t)}}{ \partial t}-\frac{1}{\sum_{i\in I_{3}}g_{i}^{(t)}}\sum_{i\in I_{3}}g^{\prime} (y_{i}F_{i}^{(t)})\frac{\partial y_{i}F_{i}^{(t)}}{\partial t}\right)\leq 0\] \[\Leftrightarrow\left(\frac{1}{\sum_{i\in I_{2}}g_{i}^{(t)}}\sum_{i \in I_{2}}g_{i}^{(t)}\frac{\partial y_{i}F_{i}^{(t)}}{\partial t}-\frac{1}{ \sum_{i\in I_{3}}g_{i}^{(t)}}\sum_{i\in I_{3}}g_{i}^{\prime}(y_{i}F_{i}^{(t)}) \frac{\partial y_{i}F_{i}^{(t)}}{\partial t}\right)\leq 0\]\[\Leftrightarrow\left(\frac{1}{\sum_{i\in I_{2}}g_{i}^{(t)}}\sum_{i\in I_ {2}}\frac{-1}{(1+\exp(y_{i}F_{i}^{(t)}))(1+\exp(-y_{i}F_{i}^{(t)}))}\frac{ \partial y_{i}F_{i}^{(t)}}{\partial t}\right)\] \[\quad-\left(\frac{1}{\sum_{i\in I_{3}}g_{i}^{(t)}}\sum_{i\in I_{3} }\frac{-1}{(1+\exp(y_{i}F_{i}^{(t)}))(1+\exp(-y_{i}F_{i}^{(t)}))}\frac{ \partial y_{i}F_{i}^{(t)}}{\partial t}\right)\leq 0\] \[\Leftrightarrow\left(\frac{1}{\sum_{i\in I_{3}}g_{i}^{(t)}}\sum_{i \in I_{3}}\frac{1}{(1+\exp(y_{i}F_{i}^{(t)}))(1+\exp(-y_{i}F_{i}^{(t)}))}\frac{ \partial y_{i}F_{i}^{(t)}}{\partial t}\right)\] \[\leq\left(\frac{1}{\sum_{i\in I_{2}}g_{i}^{(t)}}\sum_{i\in I_{2} }\frac{1}{(1+\exp(y_{i}F_{i}^{(t)}))(1+\exp(-y_{i}F_{i}^{(t)}))}\frac{ \partial y_{i}F_{i}^{(t)}}{\partial t}\right).\] (12)

Note that

\[\frac{\sum_{i\in I}g_{i}^{(t)}}{1+\min_{i\in I}\exp(-y_{i}F_{i}^{(t)})}\geq \sum_{i\in I}\frac{1}{(1+\exp(y_{i}F_{i}^{(t)}))(1+\exp(-y_{i}F_{i}^{(t)}))} \geq\frac{\sum_{i\in I}g_{i}^{(t)}}{1+\max_{i\in I}\exp(-y_{i}F_{i}^{(t)})}.\]

By Definition F.1, we have that for \(i^{\prime}\in I_{2}\),

\[\frac{\partial y_{i^{\prime}}F_{i^{\prime}}^{(t)}}{\partial t}\] \[=y_{i^{\prime}}\sum_{l_{1}=1}^{L}\sum_{j=1}^{m_{1}}\sum_{l_{2}=1 }^{L}\left(\frac{\partial a_{j}w_{j}^{(t)}W_{V}^{(t)}X_{l_{2}}^{(i^{\prime})} }{\partial t}p_{l_{1},l_{2}}^{(t,i^{\prime})}+a_{j}w_{j}^{(t)}W_{V}^{(t)}X_{l _{2}}^{(i^{\prime})}\frac{\partial p_{l_{1},l_{2}}^{(t,i^{\prime})}}{\partial t }\right)\] \[=y_{i^{\prime}}\sum_{l_{2}=1}^{L}\frac{\partial G^{(t)}(X_{l_{2}} ^{(i^{\prime})})}{\partial t}(1+o(1))+\sum_{l_{1}=1}^{L}\sum_{l_{2}=1}^{L}y_{i ^{\prime}}G^{(t)}(X_{l_{2}}^{(i^{\prime})})\frac{\partial p_{l_{1},l_{2}}^{(t,i^{\prime})}}{\partial t}\] \[=-\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_ {1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\frac{1}{n}\left( \sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)} +\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}+o(1)\sum_{i\in[n]} g_{i}^{(t)}\right).\]

Similarly, for \(i^{\prime}\in I_{3}\), we have

\[\frac{\partial y_{i^{\prime}}F_{i^{\prime}}^{(t)}}{\partial t}\] \[=-\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_ {1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\frac{1}{n}\left( \sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)} +\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}+o(1)\sum_{i\in[n]} g_{i}^{(t)}\right).\]

Now, we analyze Equation (12). Note that for \(t\in[T_{1},T_{2}]\), if \(\sum_{i\in I_{2}}g_{i}^{(t)}\) is sufficiently larger than \(\sum_{i\in I_{3}}g_{i}^{(t)}\) (i.e., \(\sum_{i\in I_{2}}g_{i}^{(t)}/\sum_{i\in I_{3}}g_{i}^{(t)}\) is bigger than some threshold), then \(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}<\frac{\partial G^{(t)}(\mu_{2})}{ \partial t}\) and \(\min_{i\in I_{2}}\frac{\partial y_{i}F_{i}^{(t)}}{\partial t}>\max_{i\in I_{3} }\frac{\partial y_{i}F_{i}^{(t)}}{\partial t}\). Thus, the ratio will decrease. Similarly, if \(\sum_{i\in I_{2}}g_{i}^{(t)}/\sum_{i\in I_{3}}g_{i}^{(t)}\) is too small, the ratio will increase. Next, we compute a bound on \(\sum_{i\in I_{2}}g_{i}^{(t)}/\sum_{i\in I_{3}}g_{i}^{(t)}\) so that \(\frac{\partial}{\partial t}\left(\frac{\sum_{i\in I_{2}}g_{i}^{(t)}}{\sum_{i \in I_{3}}g_{i}^{(t)}}\right)=0\). Substituting \(\frac{\partial y_{i}F_{i}^{(t)}}{\partial t}\) in Equation (12), we have

\[\left(\frac{1}{\sum_{i\in I_{3}}g_{i}^{(t)}}\sum_{i\in I_{3}}\frac {1}{(1+\exp(y_{i}F_{i}^{(t)}))(1+\exp(-y_{i}F_{i}^{(t)}))}\right)\] \[\quad\cdot\left(-\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}} \left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle \left(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{(t )}+\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}\pm o(1)\sum_{i\in[n] }g_{i}^{(t)}\right)\right)\] \[=\left(\frac{1}{\sum_{i\in I_{2}}g_{i}^{(t)}}\sum_{i\in I_{2}} \frac{1}{(1+\exp(y_{i}F_{i}^{(t)}))(1+\exp(-y_{i}F_{i}^{(t)}))}\right)\]\[\Rightarrow\frac{1+\min_{i\in I_{2}}\exp(-y_{i}F_{i}^{(t)})}{1+\max_{i\in I _{3}}\exp(-y_{i}F_{i}^{(t)})}\left(-\sum_{i\in I_{1}}g_{i}^{(t)}+\sum_{i\in I_{2 }}g_{i}^{(t)}+\sum_{i\in I_{3}}g_{i}^{(t)}+\sum_{i\in I_{2}}g_{i}^{(t)}+o(1) \sum_{i\in[n]}g_{i}^{(t)}\right)\] \[\quad\cdot\left(-\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}} \left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle \left(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{ (t)}+\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}\pm o(1)\sum_{i \in[n]}g_{i}^{(t)}\right)\right)\] \[\Rightarrow\frac{1+\min_{i\in I_{2}}\exp(-y_{i}F_{i}^{(t)})}{1+ \max_{i\in I_{3}}\exp(-y_{i}F_{i}^{(t)})}\left(-\sum_{i\in I_{1}}g_{i}^{(t)} +\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}-\sum_{i\in I_{1}}g_{i}^{(t)} +\sum_{i\in I_{2}}g_{i}^{(t)}\pm o(1)\sum_{i\in[n]}g_{i}^{(t)}\right)\] \[\quad=\left(-\sum_{i\in I_{1}}g_{i}^{(t)}+\sum_{i\in I_{2}\cup I_ {3}\cup I_{4}}g_{i}^{(t)}-\sum_{i\in I_{1}}g_{i}^{(t)}+\sum_{i\in I_{3}}g_{i}^ {(t)}\pm o(1)\sum_{i\in[n]}g_{i}^{(t)}\right)\] \[\Rightarrow\frac{\sum_{i\in I_{2}}g_{i}^{(t)}}{\sum_{i\in I_{3}}g _{i}^{(t)}}=1+\left(\frac{\frac{1}{\sum_{i\in I_{3}}g_{i}^{(t)}}\sum_{i\in I _{3}}\frac{1}{(1+\exp(y_{i}F_{i}^{(t)}))(1+\exp(-y_{i}F_{i}^{(t)}))}}{\frac{1} {\sum_{i\in I_{2}}g_{i}^{(t)}}\sum_{i\in I_{2}}\frac{1}{(1+\exp(y_{i}F_{i}^{(t) }))(1+\exp(-y_{i}F_{i}^{(t)}))}}\right.\] \[\quad\cdot\frac{-\sum_{i\in I_{1}}g_{i}^{(t)}+\sum_{i\in I_{2} \cup I_{3}\cup I_{4}}g_{i}^{(t)}-\sum_{i\in I_{1}}g_{i}^{(t)}+\sum_{i\in I_{2 }}g_{i}^{(t)}\pm o(1)\sum_{i\in[n]}g_{i}^{(t)}}{\sum_{i\in I_{3}}g_{i}^{(t)}} \right).\]

Since

\[\frac{1}{\sum_{i\in I}g_{i}^{(t)}}\sum_{i\in I}\frac{1}{(1+\exp(y _{i}F_{i}^{(t)}))(1+\exp(-y_{i}F_{i}^{(t)}))}\] \[\quad\in\left[\frac{1}{1+\max_{i\in I}\exp(-y_{i}F_{i}^{(t)})}, \frac{1}{1+\min_{i\in I}\exp(-y_{i}F_{i}^{(t)})}\right],\]

we have

\[\frac{\frac{1}{\sum_{i\in I_{3}}g_{i}^{(t)}}\sum_{i\in I_{3}}\frac{ 1}{(1+\exp(y_{i}F_{i}^{(t)}))(1+\exp(-y_{i}F_{i}^{(t)}))}}{\sum_{i\in I_{2}}g_{ i}^{(t)}}\sum_{i\in I_{2}}\frac{1}{(1+\exp(y_{i}F_{i}^{(t)}))(1+\exp(-y_{i}F_{i}^{( t)}))}\] \[\quad\in\left[\frac{1+\min_{i\in I_{2}}\exp(-y_{i}F_{i}^{(t)})}{1 +\max_{i\in I_{3}}\exp(-y_{i}F_{i}^{(t)})},\frac{1+\max_{i\in I_{2}}\exp(-y_{i }F_{i}^{(t)})}{1+\min_{i\in I_{3}}\exp(-y_{i}F_{i}^{(t)})}\right].\]

By Definition F.1, we have

\[\left|\frac{-\sum_{i\in I_{1}}g_{i}^{(t)}+\sum_{i\in I_{2}\cup I_{3}\cup I_{4} }g_{i}^{(t)}-\sum_{i\in I_{1}}g_{i}^{(t)}+\sum_{i\in I_{2}}g_{i}^{(t)}\pm o(1) \sum_{i\in[n]}g_{i}^{(t)}}{\sum_{i\in I_{3}}g_{i}^{(t)}}\right|\leq 6.\]

Thus,

\[\frac{\sum_{i\in I_{2}}g_{i}^{(t)}}{\sum_{i\in I_{3}}g_{i}^{(t)}}\in 1\pm 6 \cdot\left(\left[\frac{1+\min_{i\in I_{2}}\exp(-y_{i}F_{i}^{(t)})}{1+ \max_{i\in I_{3}}\exp(-y_{i}F_{i}^{(t)})},\frac{1+\max_{i\in I_{2}}\exp(-y_{i }F_{i}^{(t)})}{1+\min_{i\in I_{3}}\exp(-y_{i}F_{i}^{(t)})}\right]-1\right).\]

**Lemma F.6** (Complete version of Lemma 4.5).: _For \(t\in[T_{1},T_{2}]\), we have_

\[\frac{\sum_{i\in[n]}g_{i}^{(t)}}{\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{(t) }-\sum_{i\in I_{1}}g_{i}^{(t)}}=O(1),\qquad\frac{\sum_{i\in[n]}g_{i}^{(t)}}{ \sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}=O(1)\] \[\qquad\frac{\sum_{i\in[n]}g_{i}^{(t)}}{\sum_{i\in I_{1}}g_{i}^{(t) }-\sum_{i\in I_{3}}g_{i}^{(t)}}=O(1),\qquad\frac{\frac{\partial G^{(t)}(n_{1})}{ \partial t}\frac{\partial G^{(t)}(\mu_{2})}{\partial t}}=\Theta(1).\]

_Further, there exists a constant \(C\) such that_

\[(1+C)\max\left(\frac{\partial G^{(t)}(\mu_{1})}{\partial t},\frac{\partial G^{(t)}( \mu_{2})}{\partial t}\right)\leq-\frac{\partial G^{(t)}(\mu_{3})}{\partial t }\leq(1-C)\left(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}+\frac{\partial G^{(t)}( \mu_{2})}{\partial t}\right).\]Proof.: Without loss of generality, assume \(\frac{\partial G^{(t)}(\mu_{1})}{\partial t}>\frac{\partial G^{(t)}(\mu_{2})}{ \partial t}\). First of all, by Definition F.1, we have

\[\frac{\partial G^{(t)}(\mu_{1})}{\partial t} =\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1 }}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\frac{1}{n}\left(\sum_{i \in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}\pm o(1)\sum_{i\in[n]}g_{i}^ {(t)}\right),\] \[\frac{\partial G^{(t)}(\mu_{2})}{\partial t} =\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1 }}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\frac{1}{n}\left(\sum_{ i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}\pm o(1)\sum_{i\in[n]}g_{i}^ {(t)}\right),\] \[\frac{\partial G^{(t)}(\mu_{3})}{\partial t} =\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1 }}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\frac{1}{n}\left(\sum_{ i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}\pm o(1) \sum_{i\in[n]}g_{i}^{(t)}\right).\]

This implies that

\[\frac{\frac{\partial G^{(t)}(\mu_{1})}{\partial t}}{\frac{\partial G ^{(t)}(\mu_{2})}{\partial t}} =\frac{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)} \pm o(1)\sum_{i\in[n]}g_{i}^{(t)}}{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{ 3}}g_{i}^{(t)}\pm o(1)\sum_{i\in[n]}g_{i}^{(t)}},\] \[\frac{-\frac{\partial G^{(t)}(\mu_{3})}{\partial t}}{\frac{\partial G ^{(t)}(\mu_{1})}{\partial t}} =\frac{-\sum_{i\in I_{1}}g_{i}^{(t)}+\sum_{i\in I_{2}\cup I_{3} \cup I_{4}}g_{i}^{(t)}\pm o(1)\sum_{i\in[n]}g_{i}^{(t)}}{\sum_{i\in I_{1}}g_{ i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}\pm o(1)\sum_{i\in[n]}g_{i}^{(t)}}.\]

We next analyze \(\frac{\partial}{\partial t}\frac{-\frac{\partial G^{(t)}(\mu_{3})}{\partial t }}{\frac{\partial G^{(t)}(\mu_{1})}{\partial t}}\). We first define the ratio \(R(t)=\frac{\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}-\sum_{i\in I_{1}}g_ {i}^{(t)}}{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}\). Since the dependence of \(R\) on \(t\) is clear and for the ease of notation, we omit this dependence below. Rearranging this definition, we obtain

\[(1+R)\left(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}\right)= \sum_{i\in I_{3}\cup I_{4}}g_{i}^{(t)}.\] (13)

We consider the range of \(R\in[1,3]\). By Definition F.1, we have

\[\frac{\sum_{i\in[n]}g_{i}^{(t)}}{\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{( t)}-\sum_{i\in I_{1}}g_{i}^{(t)}}=O(1),\qquad\frac{\sum_{i\in[n]}g_{i}^{(t)}}{ \sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}=O(1),\] (14)

where the second equation follows from Lemma F.5. This implies that

\[\frac{\frac{\partial G^{(t)}(\mu_{1})}{\partial t}}{\frac{\partial G^{(t)}(\mu_ {2})}{\partial t}}=\Theta(1),\qquad\frac{\sum_{i\in[n]}g_{i}^{(t)}}{\sum_{i\in I _{1}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}}=O(1),\]

which proves the first result and

\[\frac{-\frac{\partial G^{(t)}(\mu_{3})}{\partial t}}{\frac{\partial G^{(t)}( \mu_{1})}{\partial t}} =\frac{-\sum_{i\in I_{1}}g_{i}^{(t)}+\sum_{i\in I_{2}\cup I_{3} \cup I_{4}}g_{i}^{(t)}}{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{( t)}}+o(1).\]

Thus, to analyze \(\frac{\partial}{\partial t}\frac{-\frac{\partial G^{(t)}(\mu_{3})}{\partial t}}{ \frac{\partial G^{(t)}(\mu_{1})}{\partial t}}\), we can instead analyze

\[\frac{\partial}{\partial t}\frac{\sum_{i\in I_{2}\cup I_{3}\cup I _{4}}g_{i}^{(t)}-\sum_{i\in I_{1}}g_{i}^{(t)}}{\sum_{i\in I_{1}}g_{i}^{(t)}- \sum_{i\in I_{2}}g_{i}^{(t)}}\geq 0\] \[\Leftrightarrow\frac{\partial}{\partial t}\left(\sum_{i\in I_{2} \cup I_{3}\cup I_{4}}g_{i}^{(t)}-\sum_{i\in I_{1}}g_{i}^{(t)}\right)\left(\sum_{ i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}\right)\] \[\quad-\left(\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}-\sum_ {i\in I_{1}}g_{i}^{(t)}\right)\frac{\partial}{\partial t}\left(\sum_{i\in I_{1}}g_ {i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}\right)\geq 0\]\[\Leftrightarrow\sum_{i\in I_{2}\cup I_{3}\cup I_{4}}\frac{\partial g_{i}^{(t)}}{ \partial t}+R\sum_{i\in I_{2}}\frac{\partial g_{i}^{(t)}}{\partial t}\geq(1+R) \sum_{i\in I_{1}}\frac{\partial g_{i}^{(t)}}{\partial t}.\] (15)

Recall that by Definition F.1, we have for \(i_{1}\in I_{1}\),

\[\frac{\partial y_{i_{1}}F_{i_{1}}^{(t)}}{\partial t}=\sum_{j_{1}=1}^{m_{1}} \sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}} ^{(t)}\right\rangle\frac{1}{n}\left(3\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}-\sum_{i\in I_{2}\cup I_{3}}g_{i}^{(t)}+o(1 )\sum_{i\in[n]}g_{i}^{(t)}\right);\]

for \(i_{2}\in I_{2}\),

\[\frac{\partial y_{i_{2}}F_{i_{2}}^{(t)}}{\partial t}=-\sum_{j_{1}=1}^{m_{1}} \sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}} ^{(t)}\right\rangle\frac{1}{n}\left(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}+\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{2}}g_{i}^{(t)}+o(1)\sum_{i\in[n]}g_{i}^{(t)}\right);\]

for \(i_{3}\in I_{3}\),

\[\frac{\partial y_{i_{3}}F_{i_{3}}^{(t)}}{\partial t}=-\sum_{j_{1}=1}^{m_{1}} \sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}} ^{(t)}\right\rangle\frac{1}{n}\left(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}+\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{3}}g_{i}^{(t)}+o(1)\sum_{i\in[n]}g_{i}^{(t)}\right);\]

and for \(i_{4}\in I_{4}\),

\[\frac{\partial y_{i_{4}}F_{i_{4}}^{(t)}}{\partial t}=-\sum_{j_{1}=1}^{m_{1}} \sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}} ^{(t)}\right\rangle\frac{1}{n}\left(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}+o(1)\sum_{i\in[n]}g_{i}^{(t)}\right).\]

The above implies that for \(i_{1}\in I_{1}\),

\[\frac{\frac{\partial y_{i_{1}}F_{i_{1}}^{(t)}}{\partial t}}{\sum_{i\in I_{1}} g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}=\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}} \left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle \frac{1}{n}\left(1+\frac{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^ {(t)}}{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}-R+o(1)\right);\]

for \(i_{2}\in I_{2}\),

\[\frac{\frac{\partial y_{i_{2}}F_{i_{2}}^{(t)}}{\partial t}}{\sum_{i\in I_{1}} g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}=-\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}} \left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle \frac{1}{n}\left(1-R+o(1)\right);\]

for \(i_{3}\in I_{3}\),

\[\frac{\frac{\partial y_{i_{3}}F_{i_{3}}^{(t)}}{\partial t}}{\sum_{i\in I_{1}} g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}=-\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}} \left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle \frac{1}{n}\left(\frac{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t )}}{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}-R+o(1)\right);\]

and for \(i_{4}\in I_{4}\),

\[\frac{\frac{\partial y_{i_{1}}F_{i_{1}}^{(t)}}{\partial t}}{\sum_{i\in I_{1}}g_{ i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}=-\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}} \left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle \frac{1}{n}\left(-R+o(1)\right).\]

Substituting the above into Equation (15) and divide both sides by \(\frac{1}{n}\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j _{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\), we have

\[(1+R)\sum_{i_{2}\in I_{2}}g^{\prime}(F_{i_{2}}^{(t)})(R-1+o(1))+ \sum_{i_{3}\in I_{3}}g^{\prime}(F_{i_{3}}^{(t)})\left(R-\frac{\sum_{i\in I_{1}}g_{ i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}}{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{2}}g_{i}^ {(t)}}+o(1)\right)\] \[\quad+\sum_{i_{4}\in I_{4}}g^{\prime}(F_{i_{4}}^{(t)})(R+o(1))\] \[\geq(1+R)\sum_{i_{1}\in I_{1}}g^{\prime}(F_{i_{1}}^{(t)})\left(1+ \frac{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}}{\sum_{i\in I_{1}}g_{ i}^{(t)}-\sum_{i\in I_{2}}g_{i}^{(t)}}-R+o(1)\right).\] (16)By Lemma F.5, we obtain

\[\left|\frac{\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I_{3}}g_{i}^{(t)}}{\sum_{i\in I _{1}}g_{i}^{(t)}}-\sum_{i\in I_{2}}g_{i}^{(t)}-1\right|\leq\varepsilon,\]

where we use \(\varepsilon\) to denote the small deviation. Note that Equation (16) is a quadratic inequality in \(R\) and can be rearranged as \(aR^{2}+bR+c\geq 0\), where

\[a =\sum_{i_{1}\in I_{1}}g^{\prime}(F_{i_{1}}^{(t)})+\sum_{i_{2}\in I _{2}}g^{\prime}(F_{i_{2}}^{(t)}),\] \[b =\sum_{i_{3}\in I_{3}}g^{\prime}(F_{i_{3}}^{(t)})+\sum_{i_{4}\in I _{4}}g^{\prime}(F_{i_{4}}^{(t)})-\sum_{i_{1}\in I_{1}}g^{\prime}(F_{i_{1}}^{(t) }),\] \[c =(1+o(1))\left(-\sum_{i_{2}\in I_{2}}g^{\prime}(F_{i_{2}}^{(t)})- (1\pm\varepsilon)\sum_{i_{3}\in I_{3}}g^{\prime}(F_{i_{3}}^{(t)})-(2\pm \varepsilon)\sum_{i_{1}\in I_{1}}g^{\prime}(F_{i_{1}}^{(t)})\right).\]

Now we analyze the equality condition in Equation (16) where we calculate the root of the equation \(aR^{2}+bR+c=0\). We have

\[\frac{b}{a} =\frac{\sum_{i_{3}\in I_{3}}g^{\prime}(F_{i_{3}}^{(t)})+\sum_{i_{ 4}\in I_{4}}g^{\prime}(F_{i_{4}}^{(t)})-\sum_{i_{1}\in I_{1}}g^{\prime}(F_{i_{ 1}}^{(t)})}{\sum_{i_{1}\in I_{1}}g^{\prime}(F_{i_{1}}^{(t)})+\sum_{i_{2}\in I_ {2}}g^{\prime}(F_{i_{2}}^{(t)})},\] \[\frac{c}{a} =\frac{(1+o(1))\left(-\sum_{i_{2}\in I_{2}}g^{\prime}(F_{i_{2}}^{ (t)})-(1\pm\varepsilon)\sum_{i_{3}\in I_{3}}g^{\prime}(F_{i_{3}}^{(t)})-(2\pm \varepsilon)\sum_{i_{1}\in I_{1}}g^{\prime}(F_{i_{1}}^{(t)})\right)}{\sum_{i_{ 1}\in I_{1}}g^{\prime}(F_{i_{1}}^{(t)})+\sum_{i_{2}\in I_{2}}g^{\prime}(F_{i_{ 2}}^{(t)})}.\]

Note that

\[\sum_{i\in I}g_{i}^{(t)}\min_{i\in I}\frac{1}{1+\exp(y_{i}F_{i}^{(t)})}\leq- \sum_{i\in I}g^{\prime}(F_{i}^{(t)})\leq\sum_{i\in I}g_{i}^{(t)}\max_{i\in I} \frac{1}{1+\exp(y_{i}F_{i}^{(t)})}.\]

By Equation (14), we have \(-\frac{b}{2a}\geq-1/4\) and

\[\left|\frac{b}{a}\right| \leq(1+O(\max_{i\in[n]}\exp(-y_{i}F_{i}^{(t)})))\max\left(\left| \frac{\sum_{i_{1}\in I_{1}}g(F_{i_{1}}^{(t)})-\sum_{i_{3}\in I_{3}}g(F_{i_{3}}^ {(t)})}{\sum_{i_{1}\in I_{1}}g(F_{i_{1}}^{(t)})+\sum_{i_{2}\in I_{2}}g(F_{i_{2} }^{(t)})}\right|,\left|\frac{\sum_{i_{4}\in I_{4}}g(F_{i_{4}}^{(t)})}{\sum_{i_ {1}\in I_{1}}g(F_{i_{1}}^{(t)})+\sum_{i_{2}\in I_{2}}g(F_{i_{2}}^{(t)})} \right|\right)\] \[\leq C<1,\]

and

\[\left|\frac{c}{a}\right|=(1\pm O(\varepsilon)\pm O(\max_{i\in[n]}\exp(-y_{i}F_ {i}^{(t)})))\cdot 2.\]

Recall that we are considering the case of \(R\in[1,3]\). Thus, we only need to consider the root that is positive and we can calculate the root \(R^{\star}=-\frac{b}{2a}+\sqrt{\frac{b^{2}}{4a^{2}}-\frac{c}{a}}\in(1,2)\) if \(\varepsilon\) and \(\max_{i\in[n]}\exp(-y_{i}F_{i}^{(t)})\) are both suffiently small.

Next, since \(g^{\prime}(F_{i}^{(t)})<0\), the root \(R^{\star}\) is contractive (i.e., if \(R(t)>R^{\star}\) then \(R(t)\) is decreasing and if \(R(t)<R^{\star}\) then \(R(t)\) is increasing).

Finally, the result at the end of Phase 1 implies that \(R(T_{1})\in[1,3]\), which completes the proof. 

**Corollary F.7**.: _For \(t\in[T_{1},T_{2}]\), we have_

\[\frac{G^{(t)}(\mu_{1})}{G^{(t)}(\mu_{2})},\frac{G^{(t)}(\mu_{1})}{-G^{(t)}(\mu_ {3})},\frac{G^{(t)}(\mu_{2})}{-G^{(t)}(\mu_{3})}=\Theta(1).\]

Proof.: We first prove \(\frac{G^{(t)}(\mu_{1})}{G^{(t)}(\mu_{2})}=\Theta(1)\). Following from Lemma F.6, we have \(\frac{\frac{\partial G^{(t)}(\mu_{1})}{\partial G^{(t)}(\mu_{2})}}{\frac{ \partial G^{(t)}(\mu_{2})}{\partial t}}=\Theta(1)\). By Theorem E.27, we have \(\frac{G^{(T_{1})}(\mu_{1})}{G^{(T_{1})}(\mu_{2})}=\Theta(1)\). Thus, for \(t\in[T_{1},T_{2}]\), we obtain

\[\frac{G^{(t)}(\mu_{1})}{G^{(t)}(\mu_{2})}=\frac{G^{(T_{1})}(\mu_{1})+\int_{T_{1} }^{t}\frac{\partial G^{(t)}(\mu_{1})}{\partial\tau}\ d\tau}{G^{(T_{1})}(\mu_{2}) +\int_{T_{1}}^{t}\frac{\partial G^{(t)}(\mu_{2})}{\partial\tau}\ d\tau}=\Theta(1).\]Note that Lemma F.6 and Definition F.1 imply that \(\frac{\frac{\partial G^{(t)}(\mu_{1})}{\partial t}}{-\frac{\partial G^{(t)}(\mu_{ 1})}{\partial t}}=\Theta(1)\). Since \(\frac{G^{(T_{1})}(\mu_{1})}{-G^{(T_{1})}(\mu_{3})}=\Theta(1)\), similarly, we have \(\frac{G^{(t)}(\mu_{1})}{-G^{(t)}(\mu_{3})}=\Theta(1)\). Finally, \(\frac{G^{(t)}(\mu_{1})}{G^{(t)}(\mu_{2})}=\Theta(1)\) and \(\frac{G^{(t)}(\mu_{1})}{-G^{(t)}(\mu_{3})}=\Theta(1)\) imply that \(\frac{G^{(t)}(\mu_{2})}{-G^{(t)}(\mu_{3})}=\Theta(1)\). 

### How Fast the Loss Decreases

**Theorem F.8**.: _For \(t\in[T_{1},T_{2}]\), we have_

\[\widehat{L}(t)=\frac{1}{\Theta(\sigma_{1}^{2}mm_{1})(t-T_{1})+(1/\widehat{L} (T_{1}))}.\]

Proof.: First of all, the gradient flow update for the empirical loss is given by \(\frac{\partial\widehat{L}}{\partial t}=\sum_{i=1}^{n}\ell^{\prime}(y_{i}F_{i}^ {(t)})\frac{\partial y_{i}F_{i}^{(t)}}{\partial t}\). By Definition F.1, we have for \(i_{1}\in I_{1}\),

\[\frac{\partial y_{i_{1}}F_{i_{1}}^{(t)}}{\partial t}=\sum_{j_{1}=1}^{m_{1}} \sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}} ^{(t)}\right\rangle\frac{1}{n}\left(3\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}-\sum_{i\in I_{2}\cup I_{3}}g_{i}^{(t)}+o (1)\sum_{i\in[n]}g_{i}^{(t)}\right);\]

for \(i_{2}\in I_{2}\),

\[\frac{\partial y_{i_{2}}F_{i_{2}}^{(t)}}{\partial t}=-\sum_{j_{1}=1}^{m_{1}} \sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}} ^{(t)}\right\rangle\frac{1}{n}\left(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}+\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{2}}g_{i}^{(t)}+o(1)\sum_{i\in[n]}g_{i}^{(t)}\right);\]

for \(i_{3}\in I_{3}\),

\[\frac{\partial y_{i_{3}}F_{i_{3}}^{(t)}}{\partial t}=-\sum_{j_{1}=1}^{m_{1}} \sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}} ^{(t)}\right\rangle\frac{1}{n}\left(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}+\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{3}}g_{i}^{(t)}+o(1)\sum_{i\in[n]}g_{i}^{(t)}\right).\]

and for \(i_{4}\in I_{4}\),

\[\frac{\partial y_{i_{4}}F_{i_{4}}^{(t)}}{\partial t}=-\sum_{j_{1}=1}^{m_{1}} \sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}} ^{(t)}\right\rangle\frac{1}{n}\left(\sum_{i\in I_{1}}g_{i}^{(t)}-\sum_{i\in I _{2}\cup I_{3}\cup I_{4}}g_{i}^{(t)}+o(1)\sum_{i\in[n]}g_{i}^{(t)}\right).\]

By Lemma F.6, we have

\[\frac{\partial y_{i}F_{i}^{(t)}}{\partial t}=\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}= 1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)} \right\rangle\Theta\left(\frac{1}{n}\sum_{i\in[n]}g_{i}^{(t)}\right)\]

for all \(i\in[n]\). Therefore,

\[\frac{\partial\widehat{L}}{\partial t} =\frac{1}{n}\sum_{i=1}^{n}\ell^{\prime}(y_{i}F_{i}^{(t)})\frac{ \partial y_{i}F_{i}^{(t)}}{\partial t}\] \[=\frac{1}{n}\sum_{i=1}^{n}-g_{i}^{(t)}\sum_{j_{1}=1}^{m_{1}}\sum_{j _{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)} \right\rangle\Theta\left(\frac{1}{n}\sum_{i^{\prime}\in[n]}g_{i^{\prime}}^{(t)}\right)\] \[=-\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1 }}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\Theta\left(\left(\frac{ 1}{n}\sum_{i\in[n]}g_{i}^{(t)}\right)^{2}\right)\] \[=-\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{ 1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\Theta\left(\widehat{L} ^{2}\right),\]where the last equality follows from the property of binary cross-entropy loss that \(\ell(x)=\Theta(-\ell^{\prime}(x))\) for \(x>0\). By Definition F.1 and Lemma F.4, we have for all \(t\in[T_{1},T_{2}]\),

\[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w _{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle=\Theta(\sigma_{1}^{2}mm_{ 1}).\]

Thus, we have

\[\frac{\partial\widehat{L}}{\partial t}=-\Theta(\sigma_{1}^{2}mm_{ 1})\widehat{L}^{2}.\]

Now, consider the differential equation \(\frac{dL}{dt}=-C_{1}L^{2}\). Note that this is a separable differential equation in \(t\) and we can solve it by

\[\frac{1}{L^{2}}\frac{dL}{dt}+C_{1}=0\quad\Rightarrow\quad\frac{ d}{dt}\left(C_{1}t-L^{-1}+C_{2}\right)=0\quad\Rightarrow\quad L(t)=\frac{1}{C_{1}t+C_{2}}.\]

This implies for \(t\in[T_{1},T_{2}]\), we have

\[\widehat{L}(t)=\frac{1}{\Theta(\sigma_{1}^{2}mm_{1})(t-T_{1})+( 1/\widehat{L}(T_{1}))}\]

**Corollary F.9**.: _The following bound holds:_

\[\left|\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{ j_{1}}w_{j_{1}}^{(T_{2})},a_{j_{2}}w_{j_{2}}^{(T_{2})}\right\rangle-\sum_{j_{1}=1 }^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_ {2}}w_{j_{2}}^{(T_{1})}\right\rangle\right|\leq\widetilde{O}\left(\frac{m_{1} }{m}\right).\]

Proof.: This is a direct consequence of Lemma F.4 and Theorem F.8. 

### Growth of Neuron Correlation

**Lemma F.10**.: _For \(t\in[T_{1},T_{2}]\), we have_

\[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1} }w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle-\sum_{j_{1}=1}^{m_{1}} \sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_ {2}}^{(T_{1})}\right\rangle\] \[\qquad=O\left(\frac{m_{1}\log m\log t}{\sigma_{1}^{2}mm_{1}} \right)=O\left(\frac{m_{1}\log^{2}m}{\sigma_{1}^{2}mm_{1}}\right)\]

_and thus,_

\[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1} }w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle=\Theta(\sigma_{1}^{2} mm_{1}).\]

Proof.: By Lemma F.4, we have

\[\frac{\partial}{\partial t}\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1} ^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle =\frac{2m_{1}}{n}\sum_{i=1}^{n}g_{i}^{(t)}y_{i}F_{i}^{(t)}.\]

By Theorem F.8, we have \(\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(t)}=O(\widehat{L}^{(t)})=O\left(\frac{1}{( \sigma_{1}^{2}mm_{1})(t-T_{1})+1/\widehat{L}(T_{1})}\right)\). Further, by Definition F.1, we have \(|F_{i}^{(t)}|\leq O(\log m)\). Thus, for \(t\in[T_{1},T_{2}]\), we obtain

\[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1} }w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle-\sum_{j_{1}=1}^{m_{1}} \sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(T_{1})},a_{j_{2}}w_{j_ {2}}^{(T_{1})}\right\rangle\] \[=2m_{1}\int_{T_{1}}^{t}\frac{1}{n}\sum_{i=1}^{n}g_{i}^{(r)}y_{i} F_{i}^{(\tau)}\;d\tau\]\[\leq O(m_{1}\log m)\int_{T_{1}}^{t}O\left(\frac{1}{(\sigma_{1}^{2}mm_{1})( \tau-T_{1})+1/\widehat{L}(T_{1})}\right)\,d\tau\] \[=O\left(\frac{m_{1}\log m\log t}{\sigma_{1}^{2}mm_{1}}\right)=O \left(\frac{m_{1}\log^{2}m}{\sigma_{1}^{2}mm_{1}}\right)\]

where the last line follows because \(T_{2}\leq O(\text{poly}(m))\) in Definition F.1. Finally, by Theorem E.27 we have

\[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{( T_{1})},a_{j_{2}}w_{j_{2}}^{(T_{1})}\right\rangle=\Theta(\sigma_{1}^{2}mm_{1}).\]

### Growth of Correlation of Value-Transformed Data

We now analyze the correlation term with value-transformed data.

**Lemma F.11** (Growth of correlation of value-transformed data).: _For \(\mu,\nu\in\{\mu_{i}\}_{i=1}^{d}\), we have_

\[\frac{\partial}{\partial t}\mu^{\top}W_{V}^{(t)\top}W_{V}^{(t)}\nu=G^{(t)}( \nu)\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}p_{q-l,k \leftarrow\mu}^{(t,i)}+G^{(t)}(\mu)\frac{1}{n}\sum_{i:\nu\in X^{(i)}}g_{i}^{( t)}y_{i}\sum_{l=1}^{L}p_{q\leftarrow l,k\leftarrow\nu}^{(t,i)}.\]

_Thus, for \(t\in[T_{1},T_{2}]\), we have_

\[\max_{\mu,\nu}\left|\mu^{\top}W_{V}^{(t)\top}W_{V}^{(t)}\nu-\mu^{\top}W_{V}^{ (T_{1})\top}W_{V}^{(T_{1})}\nu\right|\leq O\left(\frac{\log m\log t}{\sigma_{1} ^{2}mm_{1}}\right).\]

Proof.: By the gradient flow update, we have

\[\frac{\partial\mu W_{V}^{(t)\top}W_{V}^{(t)}\nu}{\partial t}\] \[=\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L }\sum_{j=1}^{m_{1}}a_{j}\nu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q\gets l,k \leftarrow\mu}^{(t,i)}+\frac{1}{n}\sum_{i:\nu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_ {l=1}^{L}\sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{V}^{(t)\top}w_{j}^{(t)}p_{q \gets l,k\leftarrow\nu}^{(t,i)}\] \[=G^{(t)}(\nu)\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i} \sum_{l=1}^{L}p_{q\gets l,k\leftarrow\mu}^{(t,i)}+G^{(t)}(\mu)\frac{1}{n} \sum_{i:\nu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_{l=1}^{L}p_{q\gets l,k \leftarrow\nu}^{(t,i)}.\]

Thus, we obtain

\[\left|\mu W_{V}^{(\tau)\top}W_{V}^{(\tau)}\nu-\mu W_{V}^{(T_{1}) \top}W_{V}^{(T_{1})}\nu\right|\] \[\leq\int_{T_{1}}^{\tau}|G^{(t)}(\nu)|\left|\frac{1}{n}\sum_{i:\mu \in X^{(i)}}g_{i}^{(t)}y_{i}\right|\left|\sum_{l=1}^{L}p_{q\gets l,k \leftarrow\mu}^{(t,i)}\right|+|G^{(t)}(\mu)|\left|\frac{1}{n}\sum_{i:\nu\in X ^{(i)}}g_{i}^{(t)}y_{i}\right|\left|\sum_{l=1}^{L}p_{q\gets l,k \leftarrow\nu}^{(t,i)}\right|dt.\]

By Definition F.1, for \(t\in[T_{1},T_{2}]\), we have \(|G^{(t)}(\mu)|\leq O(\log m)\) and \(\sum_{l=1}^{L}p_{q\gets l,k\leftarrow\mu}^{(t,i)}\leq O(1)\). Further, by the property of the cross-entropy loss, we have

\[\frac{1}{n}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}=O(\widehat{L}^{(t)}).\]

Therefore, by Theorem F.8, we obtain

\[\left|\mu W_{V}^{(\tau)\top}W_{V}^{(\tau)}\nu-\mu W_{V}^{(T_{1})\top}W_{V}^{(T _{1})}\nu\right|\leq O(\log m)\int_{T_{1}}^{\tau}\widehat{L}^{(t)}dt\leq O(\log m )\cdot O\left(\frac{\log\tau}{\sigma_{1}^{2}mm_{1}}\right).\]

**Corollary F.12**.: _For \(t\in[T_{1},T_{2}]\), we have_

\[\max_{\mu\in\{\mu_{i}\}_{i=1}^{d}}\left|\sum_{j=1}^{m_{1}}a_{j} \frac{\partial w_{j}^{(t)}}{\partial t}W_{V}^{(t)}\mu\right|\] \[\frac{\partial}{\partial t}\mu_{1}^{\top}W_{V}^{(t)\top}W_{V}^{(t )}\mu_{3}<0,\qquad\frac{\partial}{\partial t}\mu_{2}^{\top}W_{V}^{(t)\top}W_{V }^{(t)}\mu_{3}<0.\]

Proof.: This is a direct consequence of Lemma F.11, Lemma F.6 and Definition F.1. 

### Change of Random-Token Sub-Network

**Lemma F.14**.: _For \(t\in[T_{1},T_{2}]\) and \(\mu\in\{\mu_{i}\}_{i=4}^{d}\), we have_

\[\left|\frac{\partial G^{(t)}(\mu)}{\partial t}\right|\leq O\left( \frac{1}{n}\widehat{L}^{(t)}\sigma_{1}^{2}mm_{1}\right)+\widetilde{O}( \widehat{L}^{(t)}m_{1}(L(\sigma_{0}^{2}\sqrt{m}+1/m)+\sigma_{0}^{2}m)).\]

_Thus,_

\[\left|G^{(t)}(\mu)-G^{(T_{1})}(\mu)\right|\leq\widetilde{O}\left( \frac{1}{n}+\frac{m_{1}(L(\sigma_{0}^{2}\sqrt{m}+1/m)+\sigma_{0}^{2}m)}{ \sigma_{1}^{2}mm_{1}}\right).\]

Proof.: By Lemma C.1 and Definition F.1, we have

\[\left|\frac{\partial G^{(t)}(\mu)}{\partial t}\right| =\left|\sum_{j=1}^{m_{1}}a_{j}w_{j}^{(t)}\frac{\partial W_{V}^{( t)}\mu}{\partial t}+a_{j}\frac{\partial w_{j}^{(t)}}{\partial t}W_{V}^{(t)}\mu\right|\] \[\leq\left|\frac{1}{n}\sum_{i_{2}:\ \mu\in X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}} \sum_{l_{2}=1}^{L}\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}a_{j_{1}}a_{j_{ 2}}\left\langle w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\right\rangle p_{q\gets l_{ 2},k\leftarrow\mu}^{(t,i_{2})}\right|\]\[\leq\sum_{l_{1}=1}^{L}\sum_{l_{2}:\ X_{l_{2}}^{(i)}\in\{\mu_{k}\}_{k=4}^{d}} \left|(G^{(T_{1})}(X_{l_{2}}^{(i)})-G^{(t)}(X_{l_{2}}^{(i)}))\right|p_{q\gets l _{1},k\gets l_{2}}^{(t,i)}\] \[\quad+\sum_{l_{1}=1}^{L}\sum_{l_{2}:\ X_{l_{2}}^{(i)}\in\{\mu_{k} \}_{k=4}^{d}}\left|(G^{(T_{1})}(X_{l_{2}}^{(i)})-G^{(t)}(X_{l_{2}}^{(i)})) \right|p_{q\gets l_{1},k\gets l_{2}}^{(T_{1},i)}-p_{q\gets l_{1},k \gets l_{2}}^{(t,i)}\Big{|}\] \[\leq O(1),\]where the last inequality applies Definition F.1. This implies that, for \(t\in[T_{1},T_{2}]\), by Lemma F.14, we have

\[\left|\sum_{l_{1}=1}^{L}\sum_{l_{2}:\ X_{l_{2}}^{(i)}\in\{\mu_{k}\}_{ k=4}^{d}}G^{(t)}(X_{l_{2}}^{(i)})p_{q\gets l_{1},k\gets l_{2}}^{(t,i)} \right|\leq O(1).\]

### Change of Score and Softmax Probability

**Lemma F.16** (Change of score, complete version of Lemma 4.7).: _For \(t\in[T_{1},T_{2}]\), the attention scores are changing in the following way:_

* _For_ \(\mu,\nu\in\{\mu_{1},\mu_{2}\},\ \mu\neq\nu\)_, the query-key-correlation score between the two target signals increases, while the query-key-correlation score between one target signal and the common token decreases, i.e.,_ \[\frac{\partial}{\partial t}\nu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu =\frac{1}{\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^ {2}m)\frac{1}{L},\] \[\frac{\partial}{\partial t}\mu_{3}^{\top}W_{K}^{(t)\top}W_{Q}^{( t)}\mu =-\frac{1}{\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2}m)\frac{1 }{L}.\]
* _The change of score satisfies:_ \[\max_{\mu,\nu\in\{\mu_{i}\}_{i=1}^{3}}\left|\nu^{\top}W_{K}^{(t)}W_ {Q}^{(t)}\mu-\nu^{\top}W_{K}^{(T_{1})}W_{Q}^{(T_{1})}\mu\right|=\Theta\left( \frac{\sigma_{0}^{2}m}{\sqrt{m}L\sigma_{1}^{2}mm_{1}}+\frac{\sigma_{0}^{2} \sqrt{m}}{\sqrt{m}\sigma_{1}^{2}mm_{1}}\right).\]
* _For all_ \(\mu\in\{\mu_{1},\mu_{2},\mu_{3}\},\ \gamma\in\{\mu_{i}\}_{i=4}^{d}\)_, the query-key-correlation score changes as follows:_ \[\left|\frac{\partial}{\partial t}\mu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\gamma \right|=\frac{1}{n\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2} m)\frac{1}{L}+\widetilde{O}\left(\frac{1}{\sqrt{m}}\widehat{L}^{(t)}\sigma_{0}^{2} \sqrt{m}\right),\] _and_ \[\left|\mu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\gamma-\mu^{\top}W_{K} ^{(T_{1})\top}W_{Q}^{(T_{1})}\gamma\right| \leq\widetilde{O}\left(\frac{\sigma_{0}^{2}m}{n\sqrt{m}L\sigma_{1 }^{2}mm_{1}}+\frac{\sigma_{0}^{2}\sqrt{m}}{\sqrt{m}\sigma_{1}^{2}mm_{1}} \right),\] \[\left|\gamma^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu-\gamma^{\top}W_ {K}^{(T_{1})\top}W_{Q}^{(T_{1})}\mu\right| \leq\widetilde{O}\left(\frac{\sigma_{0}^{2}m}{n\sqrt{m}L\sigma_{1 }^{2}mm_{1}}+\frac{\sigma_{0}^{2}\sqrt{m}}{\sqrt{m}\sigma_{1}^{2}mm_{1}} \right).\]

Proof.: By Lemma C.4, we have

\[\frac{\partial\nu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu}{\partial t}\] \[=\frac{1}{n\sqrt{m}}\sum_{i:\mu,\nu\in X^{(t)}}g_{i}^{(t)}y_{i} \sum_{j=1}^{m_{1}}a_{j}\|k^{(t)}(\nu)\|_{2}^{2}\left(v^{(t)\top}(\nu)w_{j}^{( t)}-w_{j}^{(t)\top}V^{(t,i)}p_{l(i,\mu)}^{(t,i)}\right)p_{q\gets\mu,k \gets\nu}^{(t,i)}\] \[\quad+\frac{1}{n\sqrt{m}}\sum_{i:\mu\in X^{(t)}}g_{i}^{(t)}y_{i} \sum_{j=1}^{m_{1}}a_{j}\sum_{l=1}^{L}\nu^{\top}W_{K}^{(t)\top}K_{l}^{(t,i)} \left(V_{l}^{(t,i)\top}w_{j}^{(t)}-w_{j}^{(t)\top}V^{(t,i)}p_{l(i,\mu)}^{(t,i) }\right)p_{q\gets\mu,k\gets l}^{(t,i)}\mathbb{I}(K_{l}^{(t,i)}\neq k^ {(t)}(\nu))\] \[\quad+\frac{1}{n\sqrt{m}}\sum_{i:\nu,\mu\in X^{(t)}}g_{i}^{(t)}y_{i }\sum_{j=1}^{m_{1}}a_{j}\|q^{(t)}(\mu)\|_{2}^{2}p_{q\gets\mu,k\gets\nu} ^{(t,i)\top}\left(w_{j}^{(t)\top}v^{(t,i)}(\nu)-w_{j}^{(t)\top}V^{(t,i)}p_{l( i,\mu)}^{(t,i)}\right)\] \[\quad+\frac{1}{n\sqrt{m}}\sum_{i:\nu\in X^{(t)}}g_{i}^{(t)}y_{i} \sum_{l=1}^{L}\sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{Q}^{(t)\top}q_{l}^{(t,i)}p_{q \gets l,k\gets\nu}^{(t,i)\top}\left(w_{j}^{(t)\top}v^{(t,i)}(\nu)-w_{ j}^{(t)\top}V^{(t,i)}p_{l}^{(t,i)}\right)\mathbb{I}(q_{l}^{(t,i)}\neq q^{(t)}(\mu)).\]

Now, we take \(\mu=\mu_{1},\ \nu=\mu_{2}\). By Theorem E.27, we have \(G^{(T_{1})}(\mu_{2})\geq\Omega(1)\). A consequence of Definition F.1 and Lemma F.6 is that \(\frac{\partial G^{(t)}(\mu_{2})}{\partial t}>0\) for \(t\in[T_{1},T_{2}]\). Thus, we have \(G^{(t)}(\mu_{2})\geq\Omega(1)\).

Further by Theorem E.27, we have \(G^{(T_{1})}(\mu)=\widetilde{O}(\sigma_{0}\sigma_{1}\sqrt{mm_{1}})\) for \(\mu\in\mathcal{R}\) and then by Lemma F.14, we have \(G^{(t)}(\mu)=\widetilde{O}(\sigma_{0}\sigma_{1}\sqrt{mm_{1}})+\widetilde{O} \left(\frac{1}{n}+\frac{m_{1}(L(\sigma_{0}^{2}\sqrt{m}+1/m)+\sigma_{0}^{2}m)}{ \sigma_{1}^{2}mm_{1}}\right)\) for \(t\in[T_{1},T_{2}]\). Also, Definition F.1 implies that \(G^{(t)}(\mu_{2})-\sum_{j=1}^{m_{1}}w_{j}^{(t)}V^{(t,i)}p_{I(i,\mu)}^{t,i}\geq \Omega(1)\). Now, this yields

\[\frac{\partial\mu_{2}^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu_{1}}{\partial t}= \frac{1}{\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2}m)\frac{1 }{L}+\widetilde{O}\left(\frac{1}{\sqrt{m}}\widehat{L}^{(t)}\sigma_{0}^{2} \sqrt{m}\right).\]

On the other hand, by the analysis similar to the above, we obtain

\[\frac{\partial\mu_{3}^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu_{1}}{\partial t}=- \frac{1}{\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2}m)\frac{1 }{L}+\widetilde{O}\left(\frac{1}{\sqrt{m}}\widehat{L}^{(t)}\sigma_{0}^{2}\sqrt {m}\right).\]

Next, to prove the maximum change of the score, we have

\[\max_{\mu,\nu}\left|\frac{\partial\nu^{\top}W_{K}^{(t)}W_{Q}^{(t)}\mu}{ \partial t}\right|\leq\frac{1}{\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)} \sigma_{0}^{2}m)\frac{1}{L}+\widetilde{O}\left(\frac{1}{\sqrt{m}}\widehat{L} ^{(t)}\sigma_{0}^{2}\sqrt{m}\right).\]

By Theorem F.8, we have

\[\left|\nu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu-\nu^{\top}W_{K}^{( T_{1})\top}W_{Q}^{(T_{1})}\mu\right| \leq\int_{T_{1}}^{t}\frac{1}{\sqrt{m}}\widetilde{\Theta}(\widehat{ L}^{(\tau)}\sigma_{0}^{2}m)\frac{1}{L}+\widetilde{O}\left(\frac{1}{\sqrt{m}} \widehat{L}^{(\tau)}\sigma_{0}^{2}\sqrt{m}\right)\,d\tau\] \[\leq\widetilde{O}\left(\frac{\sigma_{0}^{2}m}{\sqrt{m}L\sigma_{1 }^{2}mm_{1}}+\frac{\sigma_{0}^{2}\sqrt{m}}{\sqrt{m}\sigma_{1}^{2}mm_{1}}\right).\]

Finally, for \(\gamma\in\{\mu_{i}\}_{i=4}^{d},\;\mu\in\{\mu_{i}\}_{i=1}^{d}\), we have

\[\left|\frac{\partial}{\partial t}\mu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\gamma \right|=\frac{1}{n\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2} m)\frac{1}{L}+\widetilde{O}\left(\frac{1}{\sqrt{m}}\widehat{L}^{(t)}\sigma_{0}^{2} \sqrt{m}\right),\]

which implies that

\[\left|\mu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\gamma-\mu^{\top}W_{K}^{(T_{1}) \top}W_{Q}^{(T_{1})}\gamma\right|\leq\widetilde{O}\left(\frac{\sigma_{0}^{2}m }{n\sqrt{m}L\sigma_{1}^{2}mm_{1}}+\frac{\sigma_{0}^{2}\sqrt{m}}{\sqrt{m}\sigma _{1}^{2}mm_{1}}\right).\]

**Corollary F.17** (Change of softmax).: _For \(t\in[T_{1},T_{2}]\), the softmax probability is changing in the following way:_

* _For_ \(\mu,\nu\in\{\mu_{1},\mu_{2}\},\;\mu\neq\nu\)_, the softmax probability between the two target signals increases, whereas the softmax probability between one target signal and the common token decreases, i.e.,_ \[\frac{\partial}{\partial t}p_{q\leftarrow\mu,k\leftarrow\nu}^{(t,i)} =\frac{1}{\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2}m) \frac{1}{L^{2}},\] \[\frac{\partial}{\partial t}p_{q\leftarrow\mu,k\leftarrow\mu_{3}}^{(t,i)} =-\frac{1}{\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2}m) \frac{1}{L^{2}}.\]
* _For all_ \(\mu\in\{\mu_{1},\mu_{2}\},\;\gamma\in\{\mu_{i}\}_{i=4}^{d}\)_, the softmax probability between one target signal and a random token changes as follows:_ \[\left|\frac{\partial}{\partial t}p_{q\leftarrow\mu,k\leftarrow\gamma}^{(t,i)} \right|\leq\frac{1}{n\sqrt{m}}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2} m)\frac{1}{L^{2}}+\widetilde{O}\left(\frac{1}{L\sqrt{m}}\widehat{L}^{(t)} \sigma_{0}^{2}\sqrt{m}\right).\]

_Furthermore, we have_

\[\max_{i\in[n],l_{1},l_{2}\in[L]}\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^{ (0,i)}\right|\leq O(1/L^{2}).\]Proof.: Take \(\mu=\mu_{1},\;\nu=\mu_{2}\). First of all, by Definition F.1 and Lemma F.16, we have

\[\left|p_{q\leftarrow\mu_{1}}^{(t,i)\top}\frac{\partial X^{(i)\top}W_{K}^{(t) \top}W_{Q}^{(t)}\mu_{1}}{\partial t}\right|\leq\frac{1}{\sqrt{m}}O(\widehat{L} ^{(t)}\sigma_{0}^{2}m)\frac{1}{L^{2}}+\frac{1}{n\sqrt{m}}\widetilde{\Theta}( \widehat{L}^{(t)}\sigma_{0}^{2}m)\frac{1}{L}+\widetilde{O}\left(\frac{1}{ \sqrt{m}}\widehat{L}^{(t)}\sigma_{0}^{2}\sqrt{m}\right).\]

Thus, by Lemma G.1 and Lemma F.16, for \(i\in I_{1}\), we obtain

\[\frac{\partial}{\partial t}p_{q\leftarrow\mu_{1},k\leftarrow\mu_{2}}^{(t,i)} =\frac{1}{m}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2}m)\frac{1}{L^{ 2}},\]

\[\frac{\partial}{\partial t}p_{q\leftarrow\mu_{1},k\leftarrow\mu_{3}}^{(t,i)} =-\frac{1}{m}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2}m)\frac{1}{L^ {2}}.\]

On the other hand, for \(\gamma\in\{\mu_{i}\}_{i=4}^{d}\), we have

\[\left|\frac{\partial}{\partial t}p_{q\leftarrow\mu,k\leftarrow\gamma}^{(t,i) }\right|\leq\frac{1}{nm}\widetilde{\Theta}(\widehat{L}^{(t)}\sigma_{0}^{2}m) \frac{1}{L^{2}}+\widetilde{O}\left(\frac{1}{Lm}\widehat{L}^{(t)}\sigma_{0}^{2 }\sqrt{m}\right).\]

Finally, by Lemma F.16, we have

\[\max_{\mu,\nu}\left|\nu^{\top}W_{K}^{(t)}W_{Q}^{(t)}\mu-\nu^{\top}W_{K}^{(T_{ 1})}W_{Q}^{(T_{1})}\mu\right|\leq\widetilde{O}\left(\frac{\sigma_{0}^{2}m}{ \sqrt{m}L\sigma_{1}^{2}mm_{1}}+\frac{\sigma_{0}^{2}\sqrt{m}}{\sqrt{m}\sigma_{1 }^{2}mm_{1}}\right).\]

Thus, by Lemma G.2, we obtain

\[\max_{i,l_{1},l_{2}}\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}}^ {(T_{1},i)}\right|\] \[\qquad\leq\widetilde{O}\left(\frac{\sigma_{0}^{2}m}{mL^{2}\sigma_ {1}^{2}mm_{1}}+\frac{\sigma_{0}^{2}\sqrt{m}}{m\sigma_{1}^{2}mm_{1}L}+L\left( \frac{\sigma_{0}^{2}m}{mL\sigma_{1}^{2}mm_{1}}+\frac{\sigma_{0}^{2}\sqrt{m}}{ m\sigma_{1}^{2}mm_{1}}\right)^{2}\right)=O(1/L^{2}).\]

**Corollary F.18**.: _For \(t\in[T_{1},T_{2}]\), we have_

\[\frac{\max_{i\in[n]}\left|\sum_{l_{1}=1}^{L}\sum_{l_{2}=1}^{L}G^{(t)}(X_{l_{2} }^{(i)})\frac{\partial p_{l_{1},l_{2}}^{(t,i)}}{\partial t}\right|}{\sum_{j_{ 1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j _{2}}w_{j_{2}}^{(t)}\right\rangle}\leq o(1)\frac{1}{n}\sum_{i\in[n]}g_{i}^{(t )}.\]

Proof.: This is a direct consequence of Definition F.1, Lemma F.10 and Corollary F.17. 

### Change of Self-Correlation of Key/Query-Transformed data

**Lemma F.19** (Change of self-correlation).: _For \(\mu,\nu\in\{\mu_{i}\}_{i=1}^{d}\), we have_

\[\left|\nu^{\top}W_{Q}^{(t)\top}W_{Q}^{(t)}\mu-\nu^{\top}W_{Q}^{(T_{1})\top}W_ {Q}^{(T_{1})}\mu\right| \leq\widetilde{O}\left(\frac{\sigma_{0}^{2}\sqrt{m}}{\sqrt{m} \sigma_{1}^{2}mm_{1}}\right),\]

\[\left|\nu^{\top}W_{K}^{(t)\top}W_{K}^{(t)}\mu-\nu^{\top}W_{K}^{(T_{1})\top}W_ {K}^{(T_{1})}\mu\right| \leq\widetilde{O}\left(\frac{\sigma_{0}^{2}\sqrt{m}}{\sqrt{m} \sigma_{1}^{2}mm_{1}}\right).\]

Proof.: We prove the result for \(\nu^{\top}W_{Q}^{(t)\top}W_{Q}^{(t)}\mu\) and the proof for \(\nu^{\top}W_{K}^{(t)\top}W_{K}^{(t)}\mu\) is similar. By Lemma C.4, we have

\[\frac{\partial\nu^{\top}W_{Q}^{(t)\top}W_{Q}^{(t)}\mu}{\partial t}\] \[=\frac{1}{n\sqrt{m}}\sum_{i:\mu\in X^{(i)}}g_{i}^{(t)}y_{i}\sum_ {j=1}^{m_{1}}a_{j}\nu^{\top}W_{Q}^{(t)\top}K^{(t,i)}\text{diag}\left(V^{(t,i) \top}w_{j}^{(t)}-w_{j}^{(t)\top}V^{(t,i)}p_{l(i,\mu)}^{(t,i)}\right)p_{l(i,\mu) }^{(t,i)}\] \[\quad+\frac{1}{n\sqrt{m}}\sum_{i:\nu\in X^{(i)}}g_{i}^{(t)}y_{i} \sum_{j=1}^{m_{1}}a_{j}\mu^{\top}W_{Q}^{(t)\top}K^{(t,i)}\text{diag}\left(V^{(t,i)\top}w_{j}^{(t)}-w_{j}^{(t)\top}V^{(t,i)}p_{l(i,\nu)}^{(t,i)}\right)p_{l(i, \nu)}^{(t,i)}.\]A simple result from Lemma F.16 is that \(|\nu^{\top}W_{K}^{(t)\top}W_{Q}^{(t)}\mu|\leq\widetilde{O}(\sigma_{0}^{2}\sqrt{m})\) for \(t\in[T_{1},T_{2}]\) and \(\mu,\nu\in\{\mu_{i}\}_{i=1}^{d}\). Therefore, by Definition F.1,

\[\left|\frac{\partial\nu^{\top}W_{Q}^{(t)\top}W_{Q}^{(t)}\mu}{\partial t}\right| \leq\widetilde{O}\left(\frac{1}{\sqrt{m}}\widehat{L}^{(t)}\sigma_{0}^{2}\sqrt{ m}\log m\right),\]

which implies

\[\left|\nu^{\top}W_{Q}^{(t)\top}W_{Q}^{(t)}\mu-\nu^{\top}W_{Q}^{(T_{1})\top}W_{ Q}^{(T_{1})}\mu\right|\leq\int_{T_{1}}^{t}\widetilde{O}\left(\frac{1}{ \sqrt{m}}\widehat{L}^{(t)}\sigma_{0}^{2}\sqrt{m}\log m\right)\;d\tau\leq \widetilde{O}\left(\frac{\sigma_{0}^{2}\sqrt{m}}{\sqrt{m}\sigma_{1}^{2}mm_{1}} \right),\]

where the inequality follows from Theorem F.8 and Definition F.1. 

### Small Loss is Achieved

**Theorem F.20**.: _Define \(T^{\star}=\min_{t}\{t:\;\widehat{L}^{(t)}=\Theta(1/\text{poly}(m))\}\). Then, \(T^{\star}\in[T_{1},T_{2}]\)._

Proof.: The following results altogether show that Phase 2 can last for at least \(\Theta(\text{poly}(m))\) time:

* Lemma F.19 proves that the change of \(K,Q\) self-correlation as follows: \[\max_{\mu,\nu}\left|\mu^{\top}W_{K}^{(t)\top}W_{K}^{(t)}\nu-\mu^ {\top}W_{K}^{(0)\top}W_{K}^{(0)}\nu\right| =\widetilde{O}(\sigma_{0}^{2}\sqrt{m}),\] \[\max_{\mu,\nu}\left|\mu^{\top}W_{Q}^{(t)\top}W_{Q}^{(t)}\nu-\mu^ {\top}W_{Q}^{(0)\top}W_{Q}^{(0)}\nu\right| =\widetilde{O}(\sigma_{0}^{2}\sqrt{m}).\]
* Corollary F.9 proves that \[\sum_{j_{1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{ (t)},a_{j_{2}}w_{j_{2}}^{(t)}\right\rangle\leq O(\sigma_{1}^{2}mm_{1}).\]
* Corollary F.12 proves that \[\frac{\max_{\mu\in\{\mu_{i}\}_{i=1}^{d}}\left|\sum_{j=1}^{m_{1}}a_{j}\frac{ \partial w_{j}^{(t)}}{\partial t}W_{V}^{(t)}\mu\right|}{\sum_{j_{1}=1}^{m_{1} }\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2 }}^{(t)}\right\rangle}\leq o(1/L)\frac{1}{n}\sum_{i\in[n]}g_{i}^{(t)}.\]
* Corollary F.15 proves that \[\left|\sum_{l_{1}=1}^{L}\sum_{l_{2}:\;X_{l_{2}}^{(i)}\in\{\mu_{k}\}_{k=4}^{d} }G^{(t)}(X_{l_{2}}^{(i)})p_{q\gets l_{1},k\gets l_{2}}^{(t,i)}\right| \leq O(1).\]
* Corollary F.17 proves that \[\max_{i\in[n],l_{1},l_{2}\in[L]}\left|p_{l_{1},l_{2}}^{(t,i)}-p_{l_{1},l_{2}} ^{(0,i)}\right|\leq O(1/L^{2}).\]
* Corollary F.18 proves that \[\frac{\max_{i\in[n]}\left|\sum_{l_{1}=1}^{L}\sum_{l_{2}=1}^{L}G^{(t)}(X_{l_{2 }}^{(i)})\frac{\partial p_{l_{1},l_{2}}^{(t,i)}}{\partial t}\right|}{\sum_{j_ {1}=1}^{m_{1}}\sum_{j_{2}=1}^{m_{1}}\left\langle a_{j_{1}}w_{j_{1}}^{(t)},a_{j _{2}}w_{j_{2}}^{(t)}\right\rangle}\leq o(1)\frac{1}{n}\sum_{i\in[n]}g_{i}^{(t)}.\]
* Lemma F.5 implies that \[\frac{1}{2}\leq\frac{\sum_{i\in I_{2}}g_{i}^{(t)}}{\sum_{i\in I_{3}}g_{i}^{(t )}}\leq 2.\]* Lemma F.6 implies that the gradients \(\sum_{i\in I}g_{i}^{(t)}\) for \(I\in\{I_{1},I_{2},I_{3},I_{4}\}\) satisfies \[\sum_{i\in I_{4}}g_{i}\leq\min\left(\sum_{i\in I_{2}}g_{i}^{(t)},\sum_{i\in I_{ 3}}g_{i}^{(t)}\right)\leq\max\left(\sum_{i\in I_{2}}g_{i}^{(t)},\sum_{i\in I_{3 }}g_{i}^{(t)}\right)\leq\sum_{i\in I_{1}}g_{i}^{(t)}\leq\sum_{i\in I_{2}\cup I_ {3}\cup I_{4}}g_{i}^{(t)}.\]
* Lemma F.6 and Corollary F.17 proves that \(y_{i}F_{i}^{(t)}\geq y_{i}F_{i}^{(T_{1})}\geq C\).

The above shows that all the requirements needed to satisfy the definition of Phase 2 (Definition F.1) can hold for at least \(\Omega(\text{poly}(m))\) time. Thus, \(T_{2}-T_{1}\geq\tilde{\Omega}(\text{poly}(m))\). Further, by Theorem F.8, we have that \(\widehat{L}^{(t)}\leq O(1/\text{poly}(m))\) implies \(t=\Theta(\text{poly}(m))\). 

### Proof of Theorem 3.2

Proof.: This is proved in Corollary F.13 and Lemma F.16. 

### Proof of Theorem 3.3

Proof.: This is proved as a direct consequence of Lemma F.6 and Theorem F.20.

For the generalization loss, since the training loss satisfies \(\widehat{L}^{(T^{*})}\leq 1/\text{poly}(m)\), for each class \(I\in\{I_{1},I_{2},I_{3},I_{4}\}\) there exists a sample \(X_{t}^{*}\) such that \(\ell(X_{t}^{*})\leq 1/\text{poly}(m)\). Note that by Definition F.1 the random tokens only contributes to \(O(1)\) in \(F^{(T^{*})}(X)\). Thus, given a fixed new sample \(X\sim\mathcal{D}\), we have \(|F^{(T^{*})}(X)-F^{(T^{*})}(X_{t}^{*})|\leq O(1)\) which implies \(\ell(yF^{(T^{*})}(X))\leq 1/\text{poly}(m)\). Since this holds for all \(X\sim\mathcal{D}\), we have \(L^{(T^{*})}\leq 1/\text{poly}(m)\). 

## Appendix G Auxiliary Results

The gradient of \(p(x)_{i}=\text{Softmax}(x)_{i}\) for \(x\in\mathbb{R}^{n}\):

\[\frac{\partial p(x)_{i}}{\partial x_{j}} =\frac{\partial}{\partial x_{j}}\frac{\exp(x_{i})}{\sum_{k=1}^{n} \exp(x_{k})}=\begin{cases}-\frac{\exp(x_{i})}{\sum_{k}\exp(x_{k})}\frac{\exp( x_{j})}{\sum_{k}\exp(x_{k})}=-p(x)_{i}p(x)_{j}&i\neq j\\ \frac{\exp(x_{i})}{\sum_{k}\exp(x_{k})}-\left(\frac{\exp(x_{i})}{\sum_{k}\exp( x_{k})}\right)^{2}=p(x)_{i}(1-p(x)_{i})&i=j\end{cases}\] \[=p(x)_{i}(\mathbb{I}(i=j)-p(x)_{j})\] \[\Rightarrow J(p(x)) =\text{diag}(p(x))-p(x)p(x)^{\top}.\] (17)

**Lemma G.1** (Gradient of softmax).: _Let \(s(t)\in\mathbb{R}^{l}\) be differentiable in \(t\) and \(p(s)=\text{Softmax}(s)\). Denote \(p_{i}(s)=\text{Softmax}(s)_{i}\). Then_

\[\frac{\partial p(s(t))}{\partial t}=\frac{\partial p(s)}{\partial s}\frac{ \partial s(t)}{\partial t}=(\text{diag}(p(s))-p(s)p(s)^{\top})\frac{\partial s (t)}{\partial t}.\]

Proof.: By the chain rule and the gradient of softmax in Equation (17), we obtain

\[\frac{\partial p(s(t))}{\partial t}=\frac{\partial p(s)}{\partial s}\frac{ \partial s(t)}{\partial t}=J(p(s))\frac{\partial s(t)}{\partial t}=(\text{diag }(p(s))-p(s)p(s)^{\top})\frac{\partial s(t)}{\partial t}.\]

**Lemma G.2** (Perturbation of softmax).: _Let \(s\in\mathbb{R}^{l}\) and \(p(s)=\text{Softmax}(s)\). Denote \(p_{i}(s)=\text{Softmax}(s)_{i}\). Consider a small perturbation \(\varepsilon\in\mathbb{R}^{l}\) to \(s\). Then_

\[p(s+\varepsilon)-p(s)=(\text{diag}(p(s))-p(s)p(s)^{\top})\varepsilon+\xi,\]

_where \(\|\xi\|_{\infty}=O(\|\varepsilon\|_{2}^{2})\)._

Proof.: By Taylor's expansion theorem on softmax and the gradient of softmax in Equation (17), we have

\[p(s+\varepsilon)-p(s)=J(p(s))\varepsilon+\xi=(\text{diag}(p(s))-p(s)p(s)^{ \top})\varepsilon+\xi,\]

where \(\|\xi\|_{\infty}=O(\|\varepsilon\|_{2}^{2})\).

Probability

**Lemma H.1** (Bernstein's inequality for bounded random variables).: _Assume \(Z_{1},\ldots,Z_{n}\) are \(n\) i.i.d. random variables with \(\mathbb{E}[Z_{i}]=0\) and \(|Z_{i}|\leq M\) for all \(i\in[n]\) almost surely. Let \(Z=\sum_{i=1}^{n}Z_{i}\). Then, for all \(t>0\),_

\[\mathbb{P}[Z>t]\leq\exp\left(-\frac{t^{2}/2}{\sum_{j=1}^{n}\mathbb{E}[Z_{j}^{2 }]+Mt/3}\right)\leq\exp\left(-\min\left\{\frac{t^{2}}{2\sum_{j=1}^{n}\mathbb{ E}[Z_{j}^{2}]},\frac{t}{2M}\right\}\right),\]

_which implies with probability at least \(1-\delta\),_

\[Z\leq\sqrt{2\sum_{j=1}^{n}\mathbb{E}[Z_{j}^{2}]\log\frac{1}{\delta}}+2M\log \frac{1}{\delta}.\]

**Lemma H.2**.: _For \(w_{1},w_{2}\in\mathbb{R}^{m}\) with \(w_{1},w_{2}\stackrel{{ i.i.d.}}{{\sim}}\mathcal{N}(0,I_{m}/m)\), we have_

\[\mathbb{P}\left[|\|w_{1}\|_{2}^{2}-1\right|\geq\sqrt{\frac{4}{m} \log\frac{2}{\delta}}+\frac{4}{m}\log\frac{2}{\delta}\right]\leq\delta,\] \[\mathbb{P}\left[|\langle w_{1},w_{2}\rangle|\geq\sqrt{\frac{4}{m }\log\frac{2}{\delta}}+\frac{4}{m}\log\frac{2}{\delta}\right]\leq\delta.\]

Proof.: We first have

\[\mathbb{E}\left[\|w_{1}\|_{2}^{2}\right]=\mathbb{E}\left[\sum_{i=1}^{m}w_{1,i} ^{2}\right]=1.\]

Note that \(w_{1,i}^{2}\) is a sub-Gamma random variable with parameters \((\frac{4}{m^{2}},\frac{4}{m})\). Thus, by Bernstein's inequality,

\[\mathbb{P}\left[|\|w_{1}\|_{2}^{2}-\mathbb{E}\left[\|w_{1}\|_{2}^{2}\right]| \geq\sqrt{\frac{4}{m}\log\frac{2}{\delta}}+\frac{4}{m}\log\frac{2}{\delta} \right]\leq\delta.\]

Next,

\[\mathbb{E}[\langle w_{1},w_{2}\rangle]=\mathbb{E}\left[\sum_{i=1}^{m}w_{1,i}w_ {2,i}\right]=0\]

By Bernstein's inequality, we obtain

\[\mathbb{P}\left[|\langle w_{1},w_{2}\rangle|\geq\sqrt{\frac{4}{m}\log\frac{2}{ \delta}}+\frac{4}{m}\log\frac{2}{\delta}\right]\leq\delta.\]

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: [NA] Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: [NA]Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: [NA] Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: [NA] Guidelines:

* The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ** Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: [NA] Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The experiments are done with synthetic data of small scale and the behavior of the experiment results are pretty consistent. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: [NA] Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA]Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.