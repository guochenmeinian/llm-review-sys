# Are Spiking Neural Networks more expressive than Artificial Neural Networks?

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This article studies the expressive power of spiking neural networks with firing-time-based information encoding, highlighting their potential for future energy-efficient AI applications when deployed on neuromorphic hardware. The computational power of a network of spiking neurons has already been studied via their capability of approximating any continuous function. By using the Spike Response Model as a mathematical model of a spiking neuron and assuming a linear response function, we delve deeper into this analysis and prove that spiking neural networks generate continuous piecewise linear mappings. We also show that they can emulate any multi-layer (ReLU) neural network with similar complexity. Furthermore, we prove that the maximum number of linear regions generated by a spiking neuron scales exponentially with respect to the input dimension, a characteristic that distinguishes it significantly from an artificial (ReLU) neuron. Our results further extend the understanding of the approximation properties of spiking neural networks and open up new avenues where spiking neural networks can be deployed instead of artificial neural networks without any performance loss.

## 1 Introduction

Despite the remarkable success of deep neural networks (ANNs) [12], the downside of training and inferring on large deep neural networks implemented on classical digital hardware lies in their substantial time and energy consumption [23]. The rapid advancement in the field of neuromorphic computing allows for both analog and digital computation, energy-efficient computational operations, and faster inference ([21], [2]). In practice, a neuromorphic computer is typically programmed by deploying a network of spiking neurons (SNNs) [21], i.e., programs are defined by the structure and parameters of the neural network rather than explicit instructions.

SNNs are more biologically realistic as compared to ANNs, as they involve neurons transmitting information asynchronously through spikes to other neurons [9]. Different encoding schemes enable spiking neurons to represent analog-valued inputs, broadly categorized into rate coding (spike count) and temporal coding (spike time) ([8], [17]). In this work, we assume that information is encoded in the precise timing of a spike. The event-driven nature and the sparse information propagation through relatively few spikes enhance system efficiency by lowering computational demands and improving energy efficiency.

It is intuitively clear that the described differences in the processing of the information between ANNs and SNNs should also lead to differences in the computations performed by these models. Several groups have analyzed the expressive power of ANNs from the perspective of approximation theory ([24], [4], [11], [20]) and by quantifying the number of the linear regions ([10], [18]). At the same time, few attempts have been made that aim to understand the computational power of SNNs. ([13], [3]) showed that continuous functions can be approximated to arbitrary precision using

[MISSING_PAGE_FAIL:2]

### Computation in terms of firing time

Using (4) enables us to iteratively compute the firing time \(t_{v}\) of each neuron \(v\in V\setminus V_{\text{in}}\) if we know the firing time \(t_{u}\) of each neuron \(u\in V\) with \((u,v)\in E\) by solving for \(t\) in

\[\inf_{t\geq\min_{(u,v)\in E}\{t_{u}+d_{uv}\}}P_{v}(t)=\inf_{t\geq(u,v)\in E}\{t _{u}+d_{uv}\}\sum_{(u,v)\in E}\mathbf{1}_{\{0<t-t_{u}-d_{uv}\leq\delta\}}w_{uv} (t-t_{u}-d_{uv})=\theta_{v}.\] (5)

Set \(E(\mathbf{t}_{U}):=\{(u,v)\in E:d_{uv}+t_{u}<t_{v}\leq d_{uv}+t_{u}+\delta\}\), where \(\mathbf{t}_{U}:=(t_{u})_{(u,v)\in E}\) is a vector containing the given firing times of the presynaptic neurons. The firing time \(t_{v}\) satisfies

\[\theta_{v}=\sum_{(u,v)\in E}\mathbf{1}_{\{0<t-t_{u}-d_{uv}\leq\delta\}}w_{uv} (t_{v}-t_{u}-d_{uv})=\sum_{(u,v)\in E(\mathbf{t}_{U})}w_{uv}(t_{v}-t_{u}-d_{uv }),\] (6)

\[\text{i.e., }t_{v}=\frac{\theta_{v}}{\sum_{(u,v)\in E(\mathbf{t}_{U})}w_{uv}}+ \frac{\sum_{(u,v)\in E(\mathbf{t}_{U})}w_{uv}(t_{u}+d_{uv})}{\sum_{(u,v)\in E( \mathbf{t}_{U})}w_{uv}}.\] (7)

Here, \(E(\mathbf{t}_{U})\) identifies the presynaptic neurons that actually have an effect on \(t_{v}\) based on \(\mathbf{t}_{U}\). For instance, if \(t_{w}>t_{v}\) for some synapse \((w,v)\in E\), then \(w\) did not contribute to the firing of \(v\) since the spike from \(w\) arrived after \(v\) already fired so that \((w,v)\notin E(\mathbf{t}_{U})\). Equation (7) shows that \(t_{v}\) is a weighted sum (up to a positive constant) of the firing times of neurons \(u\) with \((u,v)\in E(\mathbf{t}_{U})\). Flexibility, i.e., non-linearity, in this model is provided through the variation of the set \(E(\mathbf{t}_{U})\). Depending on the firing time of the presynaptic neurons \(\mathbf{t}_{U}\) and the associated parameters (weights, delays, threshold), \(E(\mathbf{t}_{U})\) contains a set of different synapses so that \(t_{v}\) via (7) alters accordingly.

We formally define SNNs and ANNs by a sequence of their parameters and their corresponding realizations in Appendix A.1. To employ an SNN, the (typically analog) input information needs to be encoded in the firing times of the neurons in the input layer, and similarly, the firing times of the output neurons need to be translated back to an appropriate target domain. The encoding scheme in Definition 3 in Appendix A.1 translates analog information into firing times and vice versa in a continuous manner. Note that the following results are valid within the aforementioned setting.

## 3 Main results

A broad class of ANNs based on a wide range of activation functions such as ReLU generate Continuous Piecewise Linear (CPWL) mappings ([6], [5]). In other words, these ANNs partition the input domain into regions, the so-called linear regions, on which an affine function represents the ANN's realization. The result in Theorem 1 shows that SNNs also express CPWL mappings under very general conditions.

**Theorem 1**.: _Any SNN \(\Phi\) realizes a CPWL function provided that the sum of synaptic weights of each neuron is positive and the encoding scheme is a CPWL function._

Proof.: We show in the Appendix (see Theorem 5) that the firing time of a spiking neuron with arbitrarily many input neurons is a CPWL function with respect to the input under the assumption that the sum of its weight is positive. Since \(\Phi\) consists of spiking neurons arranged in layers it immediately follows that each layer realizes a CPWL mapping. Thus, as a composition of CPWL mappings, \(\Phi\) itself realizes a CPWL function provided that the input and output encoding are also CPWL functions. 

Next, we show that an SNN has the capacity to effectively reproduce the output of any (ReLU) ANN. In order to accurately realize the output of a ReLU network, the initial step involves realizing the ReLU activation function. Despite the fact that ReLU is a very basic CPWL function, we remark that it is not straightforward to realize ReLU via SNNs.

**Theorem 2**.: _Let \(a<0<b\). There does not exist a one-layer SNN that realizes \(\sigma(x)=\max(0,x)\) on \([a,b]\). However, \(\sigma\) can be realized by a two-layer SNN on \([a,b]\)._

The proof is constructive, and we refer to Appendix A.4 for a detailed proof. Next, we extend the realization of a ReLU neuron to the entire network. We only provide a short proof sketch; the details are deferred to the Appendix A.5.

**Theorem 3**.: _Let \(L,d\in\mathbb{N}\), \([a,b]^{d}\subset\mathbb{R}^{d}\) and let \(\Psi\) be an arbitrary ANN of depth \(L\) and fixed width \(d\) employing a ReLU non-linearity, and having a one-dimensional output. Then, there exists an SNN \(\Phi\) with \(N(\Phi)=N(\Psi)+L(2d+3)-(2d+2)\) and \(L(\Phi)=3L-2\) that realizes \(\mathcal{R}_{\Psi}\) on \([a,b]^{d}\)._

Sketch of proof.: Any multi-layer ANN with ReLU activation is simply an alternating composition of affine-linear functions and a non-linear function represented by ReLU. To realize the mapping generated by some arbitrary ANN, it suffices to realize the composition of affine-linear functions and the ReLU non-linearity and then extend the construction to the whole network using concatenation and parallelization operations defined in Appendix A.2. 

The aforementioned result can be generalized to ANNs with varying widths that employ any type of piecewise linear activation function. Our expressivity result in Theorem 3 implies that SNNs can essentially approximate any function with the same accuracy and (asymptotic) complexity bounds as (deep) ANNs employing a piecewise linear activation function, given the response function satisfies the introduced basic assumptions. The number of linear regions is another measure of expressivity that describes how well a neural network can fit a family of functions. The following result establishes the number of linear regions generated by a one-layer SNN.

**Theorem 4**.: _Let \(\Phi\) be a one-layer SNN with a single output neuron \(v\) and \(d\) input neurons \(u_{1},\ldots,u_{d}\) such that \(\sum_{i=1}^{d}w_{u_{i},v}>0\). Then \(\Phi\) partitions the input domain into at most \(2^{d}-1\) linear regions. In particular, for a sufficiently large input domain, the maximal number of linear regions is attained if and only if all synaptic weights are positive._

Proof.: The maximum number of regions directly corresponds to \(E(\mathbf{t}_{U})\) defined in (7). Recall that \(E(\mathbf{t}_{U})\) identifies the presynaptic neurons that based on their firing times \(\mathbf{t}_{U}=(t_{u_{i}})_{i=1}^{d}\) triggered the firing of \(v\) at time \(t_{v}\). Therefore, each region in the input domain is associated to a subset of input neurons that is responsible for the firing of \(v\) on this specific domain. Hence, the number of regions is bounded by the number of non-empty subsets of \(\{u_{1},\ldots,u_{d}\}\), i.e., \(2^{d}-1\). Now, observe that any subset of input neurons can cause a spike in \(v\) if and only if the sum of their weights is positive. Otherwise, the corresponding input region either does not exist or inputs from the corresponding region do not trigger a spike in \(v\) since they can not increase the potential \(P_{v}(t)\) as their net contribution is negative, i.e., the potential does not reach the threshold \(\theta_{v}\). Hence, the maximal number of regions is attained if and only if all weights are positive and thereby the sum of weights of any subset of input neurons is positive as well. 

One-layer ReLU-ANNs and one-layer SNNs with one output neuron both partition the input domain into linear regions. A one-layer ReLU-ANN will partition the input domain into at most two linear regions, independent of the dimension of the input. In contrast, for a one-layer SNN, the maximum number of linear regions scales exponentially in the input dimension. This distinct behaviour stems from the intrinsic non-linearity of SNNs, originating from the subset of neurons affecting the output neuron's firing time, while in ANNs a non-linear function is applied to the output of neurons. Our result in Theorem 4 suggests that a shallow SNN can be as expressive as a deep ReLU network in terms of the number of linear regions required to express certain types of CPWL functions.

## 4 Discussion

The central aim of this paper is to study and compare the expressive power of SNNs and ANNs employing any piecewise linear activation function. The imperative role of time in biological neural systems accounts for differences in computation between SNNs and ANNs. The key difference in the realization of arbitrary CPWL mappings is the necessary size and complexity of the respective ANN and SNN. Recall that realizing the ReLU activation via SNNs required more computational units than the corresponding ANN (see Theorem 2). Conversely, using SNNs (see Theorem 4), one can also realize certain CPWL functions with fewer number of computational units and layers compared to ReLU-based ANNs. While neither model is clearly beneficial in terms of network complexity to express all CPWL functions, each model has distinct advantages and disadvantages. The significance of our results lies in investigating theoretically the approximation and expressivity capabilities of SNNs, highlighting their potential as an alternative computational model for complex tasks. The insights obtained from this work can further aid in designing architectures that can be implemented on neuromorphic hardware for energy-efficient applications.

## References

* Berner et al. [2022] J. Berner, P. Grohs, G. Kutyniok, and P. Petersen. The modern mathematics of deep learning. In _Mathematical Aspects of Deep Learning_, pages 1-111. Cambridge University Press, dec 2022. doi: 10.1017/9781009025096.002.
* Christensen et al. [2021] D. V. Christensen, R. Dittmann, B. Linares-Barranco, A. Sebastian, M. Le Gallo, A. Redaelli, S. Slesazeck, T. Mikolajick, S. Spiga, S. Menzel, I. Valov, G. Milano, C. Ricciardi, S.-J. Liang, F. Miao, M. Lanza, T. J. Quill, S. T. Keene, A. Salleo, J. Grollier, D. Markovic, A. Mizrahi, P. Yao, J. J. Yang, G. Indiveri, J. P. Strachan, S. Datta, E. Vianello, A. Valentini, J. Feldmann, X. Li, W. H. Pernice, H. Bhaskaran, S. Furber, E. Neftci, F. Scherr, W. Maass, S. Ramaswamy, J. Tapson, P. Panda, Y. Kim, G. Tanaka, S. Thorpe, C. Bartolozzi, T. A. Cleland, C. Posch, S.-C. Liu, G. Panuccio, M. Mahmud, A. N. Mazumder, M. Hosseini, T. Mohsenin, E. Donati, S. Tolu, R. Galeazzi, M. E. Christensen, S. Holm, D. Ielmini, and N. Pryds. 2022 Roadmap on Neuromorphic Computing and Engineering. _Neuromorph. Comput. Eng._, 2(2), 2022.
* 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 8529-8533, 2020. doi: 10.1109/ICASSP40776.2020.9053856.
* Cybenko [1989] G. V. Cybenko. Approximation by superpositions of a sigmoidal function. _Mathematics of Control, Signals and Systems_, 2:303-314, 1989.
* DeVore et al. [2021] R. DeVore, B. Hanin, and G. Petrova. Neural network approximation. _Acta Numerica_, 30:327-444, 2021. doi: 10.1017/S0962492921000052.
* Dym et al. [2020] N. Dym, B. Sober, and I. Daubechies. Expression of fractals through neural network functions. _IEEE Journal on Selected Areas in Information Theory_, 1(1):57-66, 2020. doi: 10.1109/JSAIT.2020.2991422.
* Gerstner [1995] W. Gerstner. Time structure of the activity in neural network models. _Phys. Rev. E_, 51:738-758, 1995.
* Gerstner and van Hemmen [1993] W. Gerstner and J. van Hemmen. How to describe neuronal activity: Spikes, rates, or assemblies? In _Advances in Neural Information Processing Systems_, volume 6. Morgan-Kaufmann, 1993.
* Gerstner et al. [2014] W. Gerstner, W. M. Kistler, R. Naud, and L. Paninski. _Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition_. Cambridge University Press, 2014.
* Goujon et al. [2022] A. Goujon, A. Etemadi, and M. A. Unser. The role of depth, width, and activation complexity in the number of linear regions of neural networks. _ArXiv_, abs/2206.08615, 2022.
* Guhring et al. [2020] I. Guhring, M. Raslan, and G. Kutyniok. Expressivity of deep neural networks. _arXiv:2007.04759_, 2020.
* LeCun et al. [2015] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. _Nature_, 521(7553):436-444, 2015.
* Maass [1995] W. Maass. An efficient implementation of sigmoidal neural nets in temporal coding with noisy spiking neurons. Technical report, Technische Universitat Graz, 1995.
* Maass [1996] W. Maass. Networks of spiking neurons: The third generation of neural network models. _Electron. Colloquium Comput. Complex._, 3, 1996.
* Maass [1996] W. Maass. Noisy spiking neurons with temporal coding have more computational power than sigmoidal neurons. In _Advances in Neural Information Processing Systems_, volume 9. MIT Press, 1996.
* Maass [1996] W. Maass. Lower bounds for the computational power of networks of spiking neurons. _Neural Computation_, 8(1):1-40, 1996. doi: 10.1162/neco.1996.8.1.1.
* Maass [2001] W. Maass. On the relevance of time in neural computation and learning. _Theoretical Computer Science_, 261(1):157-178, 2001. ISSN 0304-3975.

- Volume 2_, NIPS'14, page 2924-2932, Cambridge, MA, USA, 2014. MIT Press.
* Mostafa et al. [2018] H. Mostafa, V. Ramesh, and G. Cauwenberghs. Deep supervised learning using local errors. _Frontiers in Neuroscience_, 12, 2018.
* Petersen and Voigtlaender [2018] P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep relu neural networks. _Neural Networks_, 108:296-330, 2018. ISSN 0893-6080.
* Schuman et al. [2022] C. D. Schuman, S. R. Kulkarni, M. Parsa, J. P. Mitchell, P. Date, and B. Kay. Opportunities for neuromorphic computing algorithms and applications. _Nature Computational Science_, 2(1):10-19, 2022.
* Stanojevic et al. [2022] A. Stanojevic, S. Wozniak, G. Bellec, G. Cherubini, A. Pantazi, and W. Gerstner. An exact mapping from ReLU networks to spiking neural networks. _arXiv:2212.12522_, 2022.
* Thompson et al. [2021] N. C. Thompson, K. Greenewald, K. Lee, and G. F. Manso. Deep learning's diminishing returns: The cost of improvement is becoming unsustainable. _IEEE Spectrum_, 58(10):50-55, 2021.
* Yarotsky [2017] D. Yarotsky. Error bounds for approximations with deep relu networks. _Neural Networks_, 94:103-114, 2017. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2017.07.002.

## Appendix A Appendix

OutlineWe start by defining spiking and artificial neural networks and encoding scheme used in Section A.1. Subsequently, we introduce the spiking network calculus in Section A.2 to compose and parallelize different networks. In Section A.3, we provide the proof of Theorem 5. The proof of Theorem 5 is given in Section A.4. Finally, in Section A.5, we prove that an SNN can realize the output of any ReLU network.

### Input and output encoding

By restricting our framework of SNNs to acyclic graphs, we can arrange the underlying graph in layers and equivalently represent SNNs by a sequence of their parameters. This is analogous to the common representation of feedforward ANNs via a sequence of matrix-vector tuples [1, 20].

**Definition 2**.: _Let \(L\in\mathbb{N}\). A spiking neural network \(\Phi\) associated to the acyclic graph \((V,E)\) is a sequence of matrix-matrix-vector tuples_

\[\Phi=((W^{1},D^{1},\Theta^{1}),(W^{2},D^{2},\Theta^{2}),\ldots,(W^{L},D^{L}, \Theta^{L}))\]

_where \(N_{0},\ldots,N_{L}\in\mathbb{N}\) and each \(W^{l}\in\mathbb{R}^{N_{l-1}\times N_{l}}\), \(D^{l}\in\mathbb{R}^{N_{l-1}\times N_{l}}_{+}\), and \(\Theta^{l}\in\mathbb{R}^{N_{l}}_{+}\). The matrix entries \(W^{l}_{uv}\) and \(D^{l}_{uv}\) represent the weight and delay value associated with the synapse \((u,v)\in E\), respectively, and the entry \(\Theta^{l}_{v}\) is the firing threshold associated with node \(v\in V\). \(N_{0}\) is the input dimension and \(N_{L}\) is the output dimension of \(\Phi\). We call \(N(\Phi):=\sum_{j=0}^{L}N_{j}\) the number of neurons and \(L(\Phi):=L\) denotes the number of layers of \(\Phi\)._

**Remark 1**.: _In an ANN, the input signal is propagated in a synchronized manner layer-wise through the network (see Definition 5). In contrast, in an SNN, information is transmitted via spikes, where spikes from layer \(l-1\) affect the membrane potential of layer \(l\) neurons, resulting in asynchronous propagation due to variable firing times among neurons._

To employ an SNN, the (typically analog) input information needs to be encoded in the firing times of the neurons in the input layer, and similarly, the firing times of the output neurons need to be translated back to an appropriate target domain. We will refer to this process as input encoding and output decoding. The applied encoding scheme certainly depends on the specific task at hand and the potential power and suitability of different encoding schemes is a topic that warrants separate investigation on its own. Our focus in this work lies on exploring the intrinsic capabilities of SNNs, rather than the specifics of the encoding scheme. Thus, we can formulate some guiding principlesfor establishing a reasonable encoding scheme. First, the firing times of input and output neurons should encode analog information in a consistent way so that different networks can be concatenated in a well-defined manner. This enables us to construct suitable subnetworks and combine them appropriately to solve more complex tasks. Second, in the extreme case, the encoding scheme might directly contain the solution to a problem, underscoring the need for a sufficiently simple and broadly applicable encoding scheme to avoid this.

**Definition 3**.: _Let \([a,b]^{d}\subset\mathbb{R}^{d}\) and \(\Phi\) be an SNN with input neurons \(u_{1},\ldots,u_{d}\) and output neurons \(v_{1},\ldots,v_{n}\). Fix reference times \(T_{\text{in}}\in\mathbb{R}^{d}\) and \(T_{\text{out}}\in\mathbb{R}^{n}\). For any \(x\in[a,b]^{d}\), we set the firing times of the input neurons to \((t_{u_{1}},\ldots,t_{u_{d}})^{T}=T_{\text{in}}+x\) and the corresponding firing times of the output neurons \((t_{v_{1}},\ldots,t_{v_{n}})^{T}=T_{\text{out}}+y\), determined via (7), encode the target \(y\in\mathbb{R}^{n}\)._

**Remark 2**.: _A bounded input range ensures that appropriate reference times can be fixed. Note that the introduced encoding scheme translates analog information into input firing times in a continuous manner. Occasionally, we will point out the effect of adjusting the scheme and we will sometimes with a slight abuse of notation refer to \(T_{\text{in}},T_{\text{out}}\) as one-dimensional objects, i.e., \(T_{\text{in}},T_{\text{out}}\in\mathbb{R}\) which is justified if the corresponding vectors contain the same element in each dimension._

Next, we distinguish between a network and the target function it realizes. A network is a structured set of weights, delays and thresholds as defined in Definition 2, and the target function it realizes is the result of the asynchronous propagation of spikes through the network.

**Definition 4**.: _On \([a,b]^{d}\subset\mathbb{R}^{d}\), the realization of an SNN \(\Phi\) with output neurons \(v_{1},\ldots,v_{n}\) and reference times \(T_{\text{in}}\in\mathbb{R}^{d}\) and \(T_{\text{out}}\in\mathbb{R}^{n}\), where \(T_{\text{out}}>T_{\text{in}}\), is defined as the map \(\mathcal{R}_{\Phi}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{n}\),_

\[\mathcal{R}_{\Phi}(x)=-T_{\text{out}}+(t_{v_{1}},\ldots,t_{v_{n}})^{T}.\]

Next, we give a corresponding definition of an ANN and its realization.

**Definition 5**.: _Let \(L\in\mathbb{N}\). An artificial neural network \(\Psi\) is a sequence of matrix-vector tuples_

\[\Psi=((W^{1},B^{1}),(W^{2},B^{2}),\ldots,(W^{L},B^{L})),\]

_where \(N_{0},\ldots,N_{L}\in\mathbb{N}\) and each \(W^{l}\in\mathbb{R}^{N_{l-1}\times N_{l}}\) and \(B^{l}\in\mathbb{R}^{N_{l}}\). \(N_{0}\) and \(N_{L}\) are the input and output dimension of \(\Psi\). We call \(N(\Psi):=\sum_{j=0}^{L}N_{j}\) the number of neurons of the network \(\Psi\), \(L(\Psi):=L\) the number of layers of \(\Psi\) and \(N_{l}\) the width of \(\Psi\) in layer \(l\). The realization of \(\Psi\) with component-wise activation function \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) is defined as the map \(\mathcal{R}_{\Psi}:\mathbb{R}^{N_{0}}\rightarrow\mathbb{R}^{N_{L}}\), \(\mathcal{R}_{\Psi}(x)=y_{L}\), where \(y_{L}\) results from_

\[y_{0}=x,\quad y_{l}=\sigma(W^{l}y_{l-1}+B^{l}),\text{ for }l=1,\ldots,L-1, \quad\text{and }\ y_{L}=W^{L}y_{L-1}+B^{L}.\] (8)

In the remainder, we always employ the ReLU activation function \(\sigma(x)=\max(0,x)\). One can perform basic actions on neural networks such as concatenation and parallelization to construct larger networks from existing ones. Adapting a general approach for ANNs as defined in [1, 20], we formally introduce the concatenation and parallelization of networks of spiking neurons in the next Section A.2.

### Spiking neural network calculus

It can be observed from Definition 3 that both inputs and outputs of SNNs are encoded in a unified format. This characteristic is crucial for concatenating/parallelizing two spiking network architectures that further enable us to attain compositions/parallelizations of network realizations.

We operate in the following setting: Let \(L_{1}\), \(L_{2},d_{1},d_{2},d_{1}^{\prime},d_{2}^{\prime}\in\mathbb{N}\). Consider two SNNs \(\Phi_{1}\), \(\Phi_{2}\) given by

\[\Phi_{i}=((W_{1}^{i},D_{1}^{i},\Theta_{1}^{i}),\ldots,(W_{L_{i}}^{i},D_{L_{i}}^ {i},\Theta_{L_{i}}^{i})),\quad i=1,2,\]

with input domains \([a_{1},b_{1}]^{d_{1}}\subset\mathbb{R}^{d_{1}}\), \([a_{2},b_{2}]^{d_{2}}\subset\mathbb{R}^{d_{2}}\) and output dimension \(d_{1}^{\prime},d_{2}^{\prime}\), respectively. Denote the input neurons by \(u_{1},\ldots,u_{d_{i}}\) with respective firing times \(t_{u_{j}}^{i}\) and the output neurons by \(v_{1},\ldots,v_{d_{i}^{\prime}}\) with respective firing times \(t_{v_{j}}^{i}\) for \(i=1,2\). By Definition 3, we can express the firing times of the input neurons as

\[t_{u}^{1}(x) :=(t_{u_{1}}^{1},\ldots,t_{u_{d_{1}}}^{1})^{T}=T_{\text{in}}^{1}+x \quad\text{ for }x\in[a_{1},b_{1}]^{d_{1}},\] \[t_{u}^{2}(x) :=(t_{u_{1}}^{2},\ldots,t_{u_{d_{2}}}^{2})^{T}=T_{\text{in}}^{2}+x \quad\text{ for }x\in[a_{2},b_{2}]^{d_{2}}\] (9)and, by Definition 4, the realization of the networks as

\[\mathcal{R}_{\Phi_{1}}(x)=-T^{1}_{\text{out}}+t^{1}_{v}(t^{1}_{u}(x)) :=-T^{1}_{\text{out}}+(t^{1}_{v_{1}},\dots,t^{1}_{v_{d_{1}^{\prime}}})^{T}\quad \text{ for }x\in[a_{1},b_{1}]^{d_{1}},\] \[\mathcal{R}_{\Phi_{2}}(x)=-T^{2}_{\text{out}}+t^{2}_{v}(t^{2}_{u} (x)) :=-T^{2}_{\text{out}}+(t^{2}_{v_{1}},\dots,t^{2}_{v_{d_{2}^{\prime}}})^{T}\quad \text{ for }x\in[a_{2},b_{2}]^{d_{2}}\] (10)

for some constants \(T^{1}_{\text{in}}\in\mathbb{R}^{d_{1}}\), \(T^{2}_{\text{in}}\in\mathbb{R}^{d_{2}}\), \(T^{1}_{\text{out}}\in\mathbb{R}^{d_{1}^{\prime}}\), \(T^{2}_{\text{out}}\in\mathbb{R}^{d_{2}^{\prime}}\).

We define the concatenation of the two networks in the following way.

**Definition 6**.: _(Concatenation) Let \(\Phi_{1}\) and \(\Phi_{2}\) be such that the input layer of \(\Phi_{1}\) has the same dimension as the output layer of \(\Phi_{2}\), i.e., \(d_{2}^{\prime}=d_{1}\). Then, the concatenation of \(\Phi_{1}\) and \(\Phi_{2}\), denoted as \(\Phi_{1}\bullet\Phi_{2}\), represents the \((L_{1}+L_{2})\)-layer network_

\[\Phi_{1}\bullet\Phi_{2}:=((W_{1}^{2},D_{1}^{2},\Theta_{1}^{2}),\dots,(W_{L_{2 }}^{2},D_{L_{2}}^{2},\Theta_{L_{2}}^{2}),(W_{1}^{1},D_{1}^{1},\Theta_{1}^{1}), \dots,(W_{L_{1}}^{1},D_{L_{1}}^{1},\Theta_{L_{1}}^{1})).\]

**Lemma 1**.: _Let \(d_{2}^{\prime}=d_{1}\) and fix \(T_{\text{in}}=T^{2}_{\text{in}}\) and \(T_{\text{out}}=T^{1}_{\text{out}}\). If \(T^{2}_{\text{out}}=T^{1}_{\text{in}}\) and \(\mathcal{R}_{\Phi_{2}}([a_{2},b_{2}]^{d_{2}})\subset[a_{1},b_{1}]^{d_{1}}\), then_

\[\mathcal{R}_{\Phi_{1}\bullet\Phi_{2}}(x)=\mathcal{R}_{\Phi_{1}}(\mathcal{R}_{ \Phi_{2}}(x))\quad\text{for all }\,x\in[a,b]^{d_{2}}\]

_with respect to the reference times \(T_{\text{in}},T_{\text{out}}\). Moreover, \(\Phi_{1}\bullet\Phi_{2}\) is composed of \(N(\Phi_{1})+N(\Phi_{2})-d_{1}\) computational units._

Proof.: It is straightforward to verify via the construction that the network \(\Phi_{1}\bullet\Phi_{2}\) is composed of \(N(\Phi_{1})+N(\Phi_{2})-d_{1}\) computational units. Moreover, under the given assumptions \(\mathcal{R}_{\Phi_{1}}\circ\mathcal{R}_{\Phi_{2}}\) is well-defined so that (9) and (10) imply

\[\mathcal{R}_{\Phi_{1}\bullet\Phi_{2}}(x) =-T_{\text{out}}+t^{1}_{v}(t^{2}_{v}(T_{\text{in}}+x))=-T^{1}_{ \text{out}}+t^{1}_{v}(t^{2}_{v}(T^{2}_{\text{in}}+x))=-T^{1}_{\text{out}}+t^{1} _{v}(t^{2}_{v}(t^{2}_{u}(x)))\] \[=-T^{1}_{\text{out}}+t^{1}_{v}(T^{2}_{\text{out}}+\mathcal{R}_{ \Phi_{2}}(x))=-T^{1}_{\text{out}}+t^{1}_{v}(T^{1}_{\text{in}}+\mathcal{R}_{ \Phi_{2}}(x))\] \[=-T^{1}_{\text{out}}+t^{1}_{v}(t^{1}_{u}(\mathcal{R}_{\Phi_{2}}(x )))=\mathcal{R}_{\Phi_{1}}(\mathcal{R}_{\Phi_{2}}(x))\quad\text{ for }x\in[a_{2},b_{2}]^{d_{2}}.\]

In addition to concatenating networks, we also perform parallelization operation on SNNs.

**Definition 7**.: _(Parallelization) Let \(\Phi_{1}\) and \(\Phi_{2}\) be such that they have the same depth and input dimension, i.e., \(L_{1}=L_{2}=:L\) and \(d_{1}=d_{2}=:d\). Then, the parallelization of \(\Phi_{1}\) and \(\Phi_{2}\), denoted as \(P(\Phi_{1},\Phi_{2})\), represents the \(L\)-layer network with \(d\)-dimensional input_

\[P(\Phi_{1},\Phi_{2}):=((\tilde{W}_{1},\tilde{D}_{1},\tilde{\Theta}_{1}),\dots,( \tilde{W}_{L},\tilde{D}_{L},\tilde{\Theta}_{L})),\]

_where_

\[\tilde{W}_{1}=\begin{pmatrix}W_{1}^{1}&W_{1}^{2}\end{pmatrix},\quad\tilde{D}_{1 }=\begin{pmatrix}D_{1}^{1}&D_{1}^{2}\end{pmatrix},\quad\tilde{\Theta}_{1}= \begin{pmatrix}\Theta_{1}^{1}\\ \Theta_{1}^{2}\end{pmatrix}\]

_and_

\[\tilde{W}_{l}=\begin{pmatrix}W_{l}^{1}&0\\ 0&W_{l}^{2}\end{pmatrix},\quad\tilde{D}_{l}=\begin{pmatrix}D_{l}^{1}&0\\ 0&D_{l}^{2}\end{pmatrix},\quad\tilde{\Theta}_{l}=\begin{pmatrix}\Theta_{l}^{1} \\ \Theta_{l}^{2}\end{pmatrix},\quad\text{for }\,1<l\leq L.\]

**Lemma 2**.: _Let \(d:=d_{2}=d_{1}\) and fix \(T_{\text{in}}:=T^{1}_{\text{in}}\). \(T_{\text{out}}:=(T^{1}_{\text{out}},T^{2}_{\text{out}})\), \(a:=a_{1}\) and \(b:=b_{1}\). If \(T^{2}_{\text{in}}=T^{1}_{\text{in}}\), \(T^{2}_{\text{out}}=T^{1}_{\text{out}}\) and \(a_{1}=a_{2}\), \(b_{1}=b_{2}\), then_

\[\mathcal{R}_{P(\Phi_{1},\Phi_{2})}(x)=(\mathcal{R}_{\Phi_{1}}(x),\mathcal{R}_{ \Phi_{2}}(x))\quad\text{ for }x\in[a,b]^{d}\]

_with respect to the reference times \(T_{\text{in}},T_{\text{out}}\). Moreover, \(P(\Phi_{1},\Phi_{2})\) is composed of \(N(\Phi_{1})+N(\Phi_{2})-d\) computational units._

Proof.: The number of computational units is an immediate consequence of the construction. Since the input domains of \(\Phi_{1}\) and \(\Phi_{2}\) agree, (9) and (10) show that

\[\mathcal{R}_{P(\Phi_{1},\Phi_{2})}(x) =-T_{\text{out}}+(t^{1}_{v}(T_{\text{in}}+x),t^{2}_{v}(T_{\text{ in}}+x))=(-T^{1}_{\text{out}}+t^{1}_{v}(T^{1}_{\text{in}}+x),-T^{2}_{\text{out}}+t^{2}_{v}(T^{2}_{ \text{in}}+x))\] \[=(-T^{1}_{\text{out}}+t^{1}_{v}(t^{1}_{u}(x)),-T^{2}_{\text{out}}+t ^{2}_{v}(t^{2}_{u}(x)))=(\mathcal{R}_{\Phi_{1}}(x),\mathcal{R}_{\Phi_{2}}(x)) \quad\text{ for }x\in[a,b]^{d}.\]

**Remark 3**.: _Note that parallelization and concatenation can be straightforwardly extended (recursively) to a finite number of networks. Additionally, more general forms of parallelization and concatenations of networks, e.g., parallelization of networks with different depths, can be established. However, for the constructions presented in this work, the introduced notions suffice._

### Realizations of spiking neural networks

In this section, we show that a spiking neuron generates a CPWL mapping.

**Theorem 5**.: _Let \(v\) be a spiking neuron with \(d\) input neurons \(u_{1},\ldots,u_{d}\). The firing time \(t_{v}(t_{u_{1}},\ldots,t_{u_{d}})\) as a function of the firing times \(t_{u_{1}},\ldots,t_{u_{d}}\) is a CPWL mapping provided that \(\sum_{i=1}^{d}w_{u_{i}v}>0\), where \(w_{u_{i}v}\in\mathbb{R}\) is the synaptic weight between \(u_{i}\) and \(v\)._

Proof.: The condition \(\sum_{i=1}w_{u_{i}v}>0\) simply ensures that the input domain is decomposed into regions associated with subsets of input neurons with positive net weight. If \(\sum_{i=1}w_{u_{i}v}<0\), then the corresponding input region either does not exist or inputs from the corresponding region do not trigger a spike in \(v\) since they can not increase the potential \(P_{v}(t)\) as their net contribution is negative, i.e., the potential does not reach the threshold \(\theta_{v}\). Hence, with \(\sum_{i=1}w_{u_{i}v}>0\), the situation described above can not arise and the notion of a CPWL mapping on \(\mathbb{R}^{d}\) is well-defined. Denote the associated delays by \(d_{u_{i}v}\geq 0\) and the threshold of \(v\) by \(\theta_{v}>0\). We distinguish between the \(2^{d}-1\) variants of input combinations that can cause a firing of \(v\). Let \(I\subset\{1,\ldots,d\}\) be a non-empty subset and \(I^{c}\) the complement of \(I\) in \(\{1,\ldots,d\}\), i.e., \(I^{c}=\{1,\ldots,d\}\setminus I\). Assume that all \(u_{i}\) with \(i\in I\) contribute to the firing of \(v\) whereas spikes from \(u_{i}\) with \(i\in I^{c}\) do not influence the firing of \(v\). Then \(\sum_{i\in I}w_{u_{i}v}\) is required to be positive, and by (6) and (7) the following holds:

\[t_{u_{k}}+d_{u_{k}v}\geq t_{v}=\frac{\theta_{v}}{\sum_{i\in I}w_{u_{i}v}}+\sum _{i\in I}\frac{w_{u_{i}v}}{\sum_{j\in I}w_{u_{j}v}}(t_{u_{i}}+d_{u_{i}v})\quad \text{ for all }k\in I^{c}\] (11)

and

\[t_{u_{k}}+d_{u_{k}v}<t_{v}=\frac{\theta_{v}}{\sum_{i\in I}w_{u_{i}v}}+\sum_{i \in I}\frac{w_{u_{i}v}}{\sum_{j\in I}w_{u_{j}v}}(t_{u_{i}}+d_{u_{i}v})\quad \text{ for all }k\in I.\] (12)

Rewriting yields

\[t_{u_{k}}\geq\frac{\theta_{v}}{\sum_{i\in I}w_{u_{i}v}}+\sum_{i\in I}\frac{w _{u_{i}v}}{\sum_{j\in I}w_{u_{j}v}}(t_{u_{i}}+d_{u_{i}v})-d_{u_{k}v}\quad\text { for all }k\in I^{c}\] (13)

and

\[t_{u_{k}}\begin{cases}<\frac{\theta_{v}}{\sum_{j\in I\setminus k}w_{u_{j}v}}+ \sum_{i\in I\setminus k}\frac{w_{u_{i}v}}{\sum_{j\in I\setminus k}w_{u_{j}v}} (t_{u_{i}}+d_{u_{i}v})-d_{u_{k}v},&\text{ if }\frac{\sum_{i\in I\setminus k}w_{u_{i}v}}{\sum_{i\in I}w_{u_{i}v}}>0\\ >\frac{\theta_{v}}{\sum_{j\in I\setminus k}w_{u_{j}v}}+\sum_{i\in I\setminus k }\frac{w_{u_{i}v}}{\sum_{j\in I\setminus k}w_{u_{j}v}}(t_{u_{i}}+d_{u_{i}v})-d _{u_{k}v},&\text{ if }\frac{\sum_{i\in I\setminus k}w_{u_{i}v}}{\sum_{i\in I}w_{u_{i}v}}<0 \end{cases}\forall k\in I.\]

It is now clear that the firing time \(t_{v}(t_{u_{1}},\ldots,t_{u_{d}})\) as a function of the input \(t_{u_{1}},\ldots,t_{u_{d}}\) is a piecewise linear mapping on polytopes decomposing \(\mathbb{R}^{d}\). To show that the mapping is additionally continuous, we need to assess \(t_{v}(t_{u_{1}},\ldots,t_{u_{d}})\) on the breakpoints. Let \(I,J\subset\{1,\ldots,d\}\) be index sets corresponding to input neurons \(\{u_{i}:i\in I\}\),\(\{u_{j}:j\in J\}\) that cause \(v\) to fire on the input region \(R^{I}\subset\mathbb{R}^{d}\), \(R^{J}\subset\mathbb{R}^{d}\) respectively. Assume that it is possible to transition from \(R^{I}\) to \(R^{J}\) through a breakpoint \(t^{I,J}=(t^{I,J}_{u_{1}},\ldots,t^{I,J}_{u_{d}})\in\mathbb{R}^{d}\) without leaving \(R^{I}\cup R^{J}\). Crossing the breakpoint is equivalent to the fact that the input neurons \(\{u_{i}:i\in I\setminus J\}\) do not contribute to the firing of \(v\) anymore and the input neurons \(\{u_{i}:i\in J\setminus I\}\) begin to contribute to the firing of \(v\).

Assume first that \(J\subset I\). Then, we observe that the breakpoint \(t^{I,J}\) is necessarily an element of the linear region corresponding to the index set with smaller cardinality, i.e., \(t^{I,J}\in R^{J}\). This is an immediate consequence of (12) and the fact that \(t^{I,J}\) is characterized by

\[t^{I,J}_{u_{k}}+d_{u_{k}v}=t_{v}(t^{I,J})\quad\text{ for all }k\in I\setminus J.\] (14)

Indeed, if \(t^{I,J}_{u_{k}}+d_{u_{k}v}>t_{v}(t^{I,J})\), then there exists \(\varepsilon_{k}>0\) such that (13) also holds for \(t^{I,J}_{u_{k}}\pm\varepsilon\), where \(0\leq\varepsilon<\varepsilon_{k}\), i.e., a small change in \(t^{I,J}_{u_{k}}\) is not sufficient to change the corresponding linear region, contradicting our assumption that \(t^{I,J}\) is a breakpoint.

The firing time \(t_{v}(t^{I,J})\) is explicitly given by

\[t_{v}(t^{I,J})=\frac{\theta_{v}}{\sum_{i\in J}w_{u_{i}v}}+\sum_{i\in J}\frac{w_{u _{i}v}}{\sum_{j\in J}w_{u_{j}v}}(t^{I,J}_{u_{i}}+d_{u_{i}v})\]Using (14), we obtain

\[0=-\frac{w_{u_{k}v}}{\sum_{j\in J}w_{u_{j}v}}(t_{v}(t^{I,J})-(t^{I,J}_{u_{k}}+d_{u _{k}v}))\quad\text{ for all }k\in I\setminus J\]

so that

\[t_{v}(t^{I,J})=\frac{\theta_{v}}{\sum_{i\in J}w_{u_{i}v}}+\sum_{i\in J}\frac{w_{ u_{i}v}}{\sum_{j\in J}w_{u_{j}v}}(t^{I,J}_{u_{i}}+d_{u_{i}v})-\sum_{i\in I \setminus J}\frac{w_{u_{i}v}}{\sum_{j\in J}w_{u_{j}v}}(t_{v}(t^{I,J})-(t^{I,J}_ {u_{i}}+d_{u_{i}v})).\]

Solving for \(t_{v}(t^{I,J})\) yields

\[t_{v}(t^{I,J}) =\Big{(}1+\sum_{i\in I\setminus J}\frac{w_{u_{i}v}}{\sum_{j\in J} w_{u_{j}v}}\Big{)}^{-1}\cdot\Big{(}\frac{\theta_{v}}{\sum_{i\in J}w_{u_{i}v}}+ \sum_{i\in I}\frac{w_{u_{i}v}}{\sum_{j\in J}w_{u_{j}v}}(t^{I,J}_{u_{i}}+d_{u_{ i}v})\Big{)}\] \[=\sum_{i\in J}\frac{w_{u_{i}v}}{\sum_{j\in I}w_{u_{j}v}}\cdot \Big{(}\frac{\theta_{v}}{\sum_{i\in J}w_{u_{i}v}}+\sum_{i\in I}\frac{w_{u_{i} v}}{\sum_{j\in J}w_{u_{j}v}}(t^{I,J}_{u_{i}}+d_{u_{i}v})\Big{)}\] \[=\frac{\theta_{v}}{\sum_{i\in I}w_{u_{i}v}}+\sum_{i\in I}\frac{w_ {u_{i}v}}{\sum_{j\in I}w_{u_{j}v}}(t^{I,J}_{u_{i}}+d_{u_{i}v}),\]

which is exactly the expression for the firing time on \(R^{I}\). This shows that \(t_{v}(t_{u_{1}},\ldots,t_{u_{d}})\) is continuous in \(t^{I,J}\). Since the breakpoint \(t^{I,J}\) was chosen arbitrarily, \(t_{v}(t_{u_{1}},\ldots,t_{u_{d}})\) is continuous at any breakpoint.

The case \(I\subset J\) follows analogously. It remains to check the case when neither \(I\subset J\) nor \(J\subset I\). To that end, let \(i^{*}\in I\setminus J\) and \(j^{*}\in J\setminus I\). Assume without loss of generality that \(t^{I,J}\in R^{I}\) so that (11) and (12) imply

\[t^{I,J}_{u_{i^{*}}}+d_{u_{i^{*}}v}<t_{v}(t^{I,J})\leq t^{I,J}_{u_{j^{*}}}+d_{u _{j^{*}}v}.\]

Hence, there exists \(\varepsilon>0\) such that

\[t^{I,J}_{u_{i^{*}}}+d_{u_{i^{*}}v}<t^{I,J}_{u_{j^{*}}}+d_{u_{j^{*}}v}-\varepsilon.\] (15)

Moreover, due to the fact that \(t^{I,J}\) is a breakpoint we can find \(t^{J}\in R^{J}\cap\mathcal{B}(t^{I,J};\frac{\varepsilon}{3})\), where \(\mathcal{B}(t^{I,J};\frac{\varepsilon}{3})\) denotes the open ball with radius \(\frac{\varepsilon}{3}\) centered at \(t^{I,J}\). In particular, this entails that

\[-\frac{\varepsilon}{3}<(t^{J}_{u_{i^{*}}}-t^{I,J}_{u_{i^{*}}}),(t^{I,J}_{u_{j^{* }}}-t^{J}_{u_{j^{*}}})<\frac{\varepsilon}{3},\]

and therefore together with (15)

\[t^{J}_{u_{i^{*}}}+d_{u_{i^{*}}v}-(t^{J}_{u_{j^{*}}}+d_{u_{j^{*}} v}) =(t^{J}_{u_{i^{*}}}-t^{I,J}_{u_{i^{*}}})+(t^{I,J}_{u_{i^{*}}}+d_{u _{i^{*}}v}-(t^{I,J}_{u_{j^{*}}}+d_{u_{j^{*}}v}))+(t^{I,J}_{u_{j^{*}}}-t^{J}_{ u_{j^{*}}})\] \[<0,\quad\text{i.e., }t^{J}_{u_{i^{*}}}+d_{u_{i^{*}}v}<t^{J}_{u_{j^{* }}}+d_{u_{j^{*}}v}.\]

However, (11) and (12) require that

\[t^{J}_{u_{j^{*}}}+d_{u_{j^{*}}v}<t_{v}(t^{J})\leq t^{J}_{u_{i^{*}}}+d_{u_{j^{* }}v}\]

since \(t^{J}\in R^{J}\). Thus, \(t^{I,J}\) can not exist and the case when neither \(I\subset J\) nor \(J\subset I\) can not arise. 

### Realizing ReLU with spiking neural networks

**Proposition 1**.: _Let \(c_{1}\in\mathbb{R}\), \(c_{2}\in(a,b)\subset\mathbb{R}\) and consider \(f_{1},f_{2}:[a,b]\to\mathbb{R}\) defined as_

\[f_{1}(x)=\begin{cases}x+c_{1}&\text{, if }x>c_{2}\\ c_{1}&\text{, if }x\leq c_{2}\end{cases}\quad\text{ or }\quad f_{2}(x)= \begin{cases}x+c_{1}&\text{, if }x<c_{2}\\ c_{1}&\text{, if }x\geq c_{2}\end{cases}.\]

_There does not exist a one-layer SNN with output neuron \(v\) and input neuron \(u_{1}\) such that \(t_{v}(x)=f_{i}(x)\), \(i=1,2\), on \([a,b]\), where \(t_{v}(x)\) denotes the firing time of \(v\) on input \(t_{u_{1}}=x\)._Proof.: First, note that a one-layer SNN realizes a CPWL function. For \(c_{2}\neq 0\), \(f_{i}\) is not continuous and therefore can not be emulated by the firing time of any one-layer SNN. Hence, it is left to consider the case \(c_{2}=0\). If \(u_{1}\) is the only input neuron, then \(v\) fires if and only if \(w_{u_{1}v}>0\) and by (7) the firing time is given by

\[t_{v}(x)=\frac{\theta}{w_{u_{1}v}}+x+d_{u_{1}v}\quad\text{ for all }x\in[a,b],\]

i.e., \(t_{v}\neq f_{i}\). Therefore, we introduce auxiliary input neurons \(u_{2},\ldots,u_{n}\) and assume without loss of generality that \(t_{u_{i}}+d_{u_{i}v}<t_{u_{j}}+d_{u_{j}v}\) for \(j>i\). Here, the firing times \(t_{u_{i}}\), \(i=2,\ldots,n\), are suitable constants. We will show that even in this extended setting \(t_{v}\neq f_{i}\) still holds and thereby also the claim.

For the sake of contradiction, assume that \(t_{v}(x)=f_{1}(x)\) for all \(x\in[a,b]\). This implies that there exists an index set \(J\subset\{1,\ldots,n\}\) with \(\sum_{j\in J}w_{u_{j}v}>0\) and a corresponding interval \((a_{1},0]\subset[a,b]\) such that

\[c_{1}=t_{v}(x)=\frac{1}{\sum_{i\in J}w_{u_{i}v}}\Big{(}\theta_{v}+\sum_{i\in J }w_{u_{i}v}(t_{u_{i}}+d_{u_{i}v})\Big{)}\quad\text{ for all }x\in(a_{1},0],\]

where we have applied (7). Moreover, \(J\) is of the form \(J=\{2,\ldots,\ell\}\) for some \(\ell\in\{1,\ldots,n\}\) because \((t_{u_{i}}+d_{u_{i}v})_{i=2}^{n}\) is in ascending order, i.e., if the spike from \(u_{\ell}\) has reached \(v\) before \(v\) fired, then so did the spikes from \(u_{i}\), \(2\leq i<\ell\). Additionally, we know that \(1\notin J\) since otherwise \(t_{v}\) is non-constant on \((a_{1},0]\) (due to the contribution from \(u_{1}\)), i.e., \(t_{v}\neq c_{1}\) on \((a_{1},0]\). In particular, the spike from \(u_{1}\) reaches \(v\) after the neurons \(u_{2},\ldots,u_{\ell}\) already caused \(v\) to fire, i.e., we have

\[x+d_{u_{1}v}\geq t_{v}(x)=c_{1}\quad\text{ for all }x\in(a_{1},0].\]

However, it immediately follows that

\[x+d_{u_{1}v}>d_{u_{1}v}\geq c_{1}\quad\text{ for all }x>0.\]

Thus, we obtain \(t_{v}(x)=c_{1}\) for \(x>0\) (since the spike from \(u_{1}\) still reaches \(v\) only after \(v\) emitted a spike), which contradicts \(t_{v}(x)=f_{1}(x)\) for all \(x\in[a,b]\).

We perform a similar analysis to show that \(f_{2}\) can not be emulated. For the sake of contradiction, assume that \(t_{v}(x)=f_{2}(x)\) for all \(x\in[a,b]\). This implies that there exists an index set \(I\subset\{1,\ldots,n\}\) with \(\sum_{i\in I}w_{u_{i}v}>0\) and a corresponding interval \((a_{2},0)\subset[a,b]\) such that

\[x+c_{1}=t_{v}(x)=\frac{1}{\sum_{i\in I}w_{u_{i}v}}\Big{(}\theta_{v}+w_{u_{1}v} (x+d_{u_{1}v})+\sum_{i\in I\setminus\{1\}}w_{u_{i}v}(t_{u_{i}}+d_{u_{i}v}) \Big{)}\quad\text{ for }x\in(a_{2},0),\] (16)

where we have applied (7). We immediately observe that \(1\in I\), since otherwise \(t_{v}\) is constant on \((a_{2},0)\). Moreover, by the same reasoning as before we can write \(I=\{1,\ldots,\ell\}\) for some \(\ell\in\{1,\ldots,n\}\). In order for \(t_{v}(x)=f_{2}(x)\) for all \(x\in[a,b]\) to hold, there needs to exist an index set \(J\subset\{1,\ldots,n\}\) with \(\sum_{j\in J}w_{u_{j}v}>0\) and a corresponding interval \([0,b_{2})\subset[a,b]\) such that \(t_{v}=c_{1}\) on \([0,b_{2})\). We conclude that \(J=\{1,\ldots,m\}\) or \(J=\{2,\ldots,m\}\) for some \(m\in\{1,\ldots,n\}\). In the former case, \(t_{v}\) is non-constant on \([0,b_{2})\) (due to the contribution from \(u_{1}\)), i.e., \(t_{v}\neq c_{1}\) on \([0,b_{2})\). Hence, it remains to consider the latter case. Note that \(m<\ell\) implies that \(b_{2}\leq a_{2}\) (as \(u_{2},\ldots,u_{m}\) already triggered a firing of \(v\) before the spike from \(u_{\ell}\) arrived) contradicting the construction \(a_{2}<0<b_{2}\). Similarly, \(m=\ell\), i.e., \(J=I\setminus\{1\}\) is not valid because (16) requires that

\[\frac{w_{u_{1}v}}{\sum_{i\in I}w_{u_{i}v}}=1\Leftrightarrow\sum_{i\in I \setminus\{1\}}w_{u_{i}v}=0\Leftrightarrow\sum_{j\in J}w_{u_{j}v}=0.\]

Finally, \(m>\ell\) also results in a contradiction since

\[0<\sum_{j\in J}w_{u_{j}v}=\sum_{i\in I\setminus\{1\}}w_{u_{i}v}+\sum_{j\in J \setminus I}w_{u_{j}v}=\sum_{j\in J\setminus I}w_{u_{j}v}\]

together with

\[0<\sum_{i\in I}w_{u_{i}v}=\sum_{i\in I\setminus\{1\}}w_{u_{i}v}+w_{u_{1}v}=w_{ u_{1}v}\]

imply that the neurons \(\{u_{j}:j\in\{1\}\cup J\}\) also trigger a spike in \(v\). However, the corresponding interval where the firing of \(v\) is caused by \(\{u_{j}:j\in\{1\}\cup J\}\) is necessarily located between \((a_{2},0)\) and \([0,b_{2})\), which is not possible.

**Remark 4**.: _The proof shows that \(-f_{1}\) also can not be emulated by a one-layer SNN. Moreover, by adjusting (16) we observe that a necessary condition for \(-f_{2}\) to be realized is that_

\[\frac{w_{u_{1}v}}{\sum_{i\in I}w_{u_{i}v}}=-1\Leftrightarrow-\sum_{i\in I \setminus\{1\}}w_{u_{i}v}=2w_{u_{1}v}\Leftrightarrow-\frac{1}{2}\sum_{i\in I \setminus\{1\}}w_{u_{i}v}=w_{u_{1}v}.\]

_Under this condition \(-f_{2}\) can indeed be realized by a one-layer SNN as the following statement confirms._

**Proposition 2**.: _Let \(a<0<b,c\) and consider \(f:[a,b]\to\mathbb{R}\) defined as_

\[f(x)=\begin{cases}-x+c&,\text{ if }x<0\\ c&,\text{ if }x\geq 0\end{cases}.\]

_There exists a one-layer SNN \(\Phi\) with output neuron \(v\) and input neuron \(u_{1}\) such that \(t_{v}(x)=f(x)\) on \([a,b]\), where \(t_{v}(x)\) denotes the firing time of \(v\) on input \(t_{u_{1}}=x\)._

Proof.: We introduce an auxiliary input neuron with constant firing time \(t_{u_{2}}\in\mathbb{R}\) and specify the parameter of \(\Phi=((W,D,\Theta))\) in the following manner (see Figure 0(a)):

\[W=\begin{pmatrix}-\frac{1}{2}\\ 1\end{pmatrix},D=\begin{pmatrix}d_{1}\\ d_{2}\end{pmatrix},\Theta=\theta,\]

where \(\theta,d_{1},d_{2}>0\) are to be specified. Note that either \(u_{2}\) or \(u_{1}\) together with \(u_{2}\) can trigger a spike in \(v\) since \(w_{u_{1}v}<0\). Therefore, applying (7) yields that \(u_{2}\) triggers a spike in \(v\) under the following circumstances:

\[t_{v}(x)=\theta+t_{u_{2}}+d_{2}\quad\text{ if }t_{v}(x)\leq t_{u_{1}}+d_{1}=x+d_{ 1}.\]

Hence, this case only arises when

\[\theta+t_{u_{2}}+d_{2}\leq x+d_{1}\Leftrightarrow\theta+t_{u_{2}}+d_{2}-d_{1} \leq x.\]

To emulate \(f\) the parameter needs to satisfy

\[\theta+t_{u_{2}}+d_{2}-d_{1}\leq x\text{ for all }x\in[0,b]\quad\text{ and } \quad\theta+t_{u_{2}}+d_{2}-d_{1}>x\text{ for all }x\in[a,0)\]

which simplifies to

\[\theta+t_{u_{2}}+d_{2}-d_{1}=0.\] (17)

If the additional condition

\[\theta+t_{u_{2}}+d_{2}=c\] (18)

is met, we can infer that

\[t_{v}(x)=\begin{cases}2(\theta+t_{u_{2}}+d_{2})-(x+d_{1})&,\text{ if }x<0\\ \theta+t_{u_{2}}+d_{2}&,\text{ if }x\geq 0\end{cases}=\begin{cases}-x+c&, \text{ if }x<0\\ c&,\text{ if }x\geq 0\end{cases}.\]

Finally, it is immediate to verify that the conditions (17) and (18) can be satisfied simultaneously due to the assumption that \(c>0\), e.g., choosing \(d_{1}=d_{2}=c\) and \(t_{u_{2}}=-\theta\) is sufficient. 

**Remark 5**.: _We wish to mention that we can not adapt the previous construction to emulate ReLU with a consistent encoding scheme, i.e., such that the input and output firing times encode analog values in the same format with respect to reference times \(T_{in},T_{out}\in\mathbb{R}\), \(T_{in}<T_{out}\). Indeed, it is obvious that using the input encoding \(T_{in}+x\) and output decoding \(-T_{out}+t_{v}\), does not realize ReLU. Similarly, one verifies that the input encoding \(T_{in}-x\) and output decoding \(T_{out}-t_{v}\) also does not yield the desired function. However, choosing the input encoding \(T_{in}-x\) and output decoding \(-T_{out}+t_{v}\) gives_

\[\mathcal{R}_{\Phi}(x)=\begin{cases}-T_{out}-T_{in}+c+x&,\text{ if }x>T_{in}\\ -T_{out}+c&,\text{ if }x\leq T_{in}\end{cases}.\]

_Setting \(T_{in}=0\) and \(T_{out}=c\) implies that \(\Phi\) realizes ReLU with inconsistent encoding \(T_{in}-x\) and \(T_{out}+\mathcal{R}_{\Phi}(x)\). Nevertheless, we want a consistent encoding scheme that allows us to compose ReLU (as typically is the case in ANNs) whereby an inconsistent scheme is disadvantageous._

Applying the previous construction and adding another layer is adequate to emulate \(f_{1}\) defined in Proposition 1 by a two-layer SNN.

**Proposition 3**.: _Let \(a<0<b<0.5\cdot c\) and consider \(f:[a,b]\rightarrow\mathbb{R}\) defined as_

\[f(x)=\begin{cases}x+c&,\,\text{if}\,\,x>0\\ c&,\,\text{if}\,\,x\leq 0\end{cases}\]

_There exists a 2-layer SNN \(\Phi\) with output neuron \(v\) and input neuron \(u_{1}\) such that \(t_{v}(x)=f(x)\) on \([a,b]\), where \(t_{v}(x)\) denotes the firing time of \(v\) on input \(t_{u_{1}}=x\)._

Proof.: We introduce an auxiliary input neuron \(u_{2}\) with constant firing time \(t_{u_{2}}\in\mathbb{R}\) and specify the parameter of \(\Phi=((W^{1},D^{1},\Theta^{1}),(W^{2},D^{2},\Theta^{2}))\) in the following manner:

\[W^{1}=\begin{pmatrix}-\frac{1}{2}&0\\ 1&2\end{pmatrix},D^{1}=\begin{pmatrix}d&0\\ d&\frac{d}{2}\end{pmatrix},\Theta^{1}=\begin{pmatrix}\theta\\ 2\theta\end{pmatrix},W^{2}=\begin{pmatrix}-\frac{1}{2}\\ 1\end{pmatrix},D^{2}=\begin{pmatrix}d\\ d\end{pmatrix},\Theta^{2}=\theta,\] (19)

where \(d\geq 0\) and \(\theta>0\) is chosen such that \(\theta+t_{u_{2}}>b\). We denote the input neurons by \(u_{1},u_{2}\), the neurons in the hidden layer by \(z_{1},z_{2}\) and the output neuron by \(v\). Note that the firing time of \(z_{1}\) depends on \(u_{1}\) and \(u_{2}\). In particular, either \(u_{2}\) or \(u_{1}\) together with \(u_{2}\) can trigger a spike in \(z_{1}\) since \(w_{u_{1}z_{1}}<0\). Therefore, applying (7) yields that \(u_{2}\) triggers a spike in \(z_{1}\) under the following circumstances:

\[t_{z_{1}}(x)=\theta+t_{u_{2}}+d\quad\text{ if }t_{z_{1}}(x)\leq t_{u_{1}}+d=x+d.\]

Hence, this case only arises when

\[\theta+t_{u_{2}}+d\leq x+d\Leftrightarrow\theta+t_{u_{2}}\leq x.\] (20)

However, by construction \(\theta+t_{u_{2}}>b\), so that (20) does not hold for any \(x\in[a,b]\). Thus, we conclude via (7) that

\[t_{z_{1}}(x)=2(\theta+t_{u_{2}}+d)-(x+d)=2(\theta+t_{u_{2}})+d-x.\]

By construction, the firing time \(t_{z_{2}}=\theta+2t_{u_{2}}+d\) of \(z_{2}\) is a constant which depends on the input only via \(u_{2}\). A similar analysis as in the first layer shows that

\[t_{v}(x)=\theta+t_{z_{2}}+d\quad\text{ if }t_{v}(x)\leq t_{z_{1}}+d=2(\theta+t_ {u_{2}})+d-x+d=2(\theta+t_{u_{2}}+d)-x.\]

Hence, \(z_{2}\) triggers a spike in \(v\) when

\[\theta+\theta+2t_{u_{2}}+d+d\leq 2(\theta+t_{u_{2}}+d)-x\quad\Leftrightarrow \quad x\leq 0.\]

If the additional condition

\[\theta+t_{z_{2}}+d=c\quad\Leftrightarrow\quad 2(\theta+d+t_{u_{2}})=c\] (21)

Figure 1: (a) Computation graph associated with a spiking network with two input neurons and one output neuron that realizes \(f\) as defined in Proposition 2. (b) Stacking the network in (a) twice results in a spiking network that realizes the ReLU activation function.

is met, we can infer that

\[t_{v}(x) =\begin{cases}2(\theta+t_{z_{2}}+d)-(t_{z_{1}}(x)+d)&,\text{ if }x>0\\ \theta+t_{z_{2}}+d&,\text{ if }x\leq 0\end{cases}\] \[=\begin{cases}2c-(2(\theta+t_{u_{2}})+d-x+d)&,\text{ if }x>0\\ c&,\text{ if }x\leq 0\end{cases}\] \[=\begin{cases}x+c&,\text{ if }x>0\\ c&,\text{ if }x\leq 0\end{cases}.\]

Choosing \(\theta\), \(t_{u_{2}}\) and \(d\) sufficiently small under the given constraints guarantees that (21) holds, i.e., \(\Phi\) emulates \(f\) as desired. 

**Remark 6**.: _It is again important to specify the encoding scheme via reference times \(T_{\text{in}},T_{\text{out}}\in\mathbb{R}\), \(T_{\text{in}}<T_{\text{out}}\) to ensure that \(\Phi\) realizes ReLU. The input encoding \(T_{\text{in}}-x\) and output decoding \(T_{\text{out}}-t_{v}\) does not yield the desired output since it results in a realization of the type \(-\text{ReLU}(-x)\). In contrast, the input encoding \(T_{\text{in}}+x\) and output decoding \(-T_{\text{out}}+t_{v}\) with \(T_{\text{in}}=0\) and \(T_{\text{out}}=c\) gives_

\[\mathcal{R}_{\Phi}(x)=-T_{\text{out}}+t_{v}(T_{\text{in}}+x)=-T_{\text{out}}+ f(T_{\text{in}}+x)=\begin{cases}x&,\text{ if }x>0\\ 0&,\text{ if }x\leq 0\end{cases}\ =\text{ReLU}(x).\]

_In this case, it is necessary to choose the reference time \(T_{\text{in}}=0\) to ensure that the breakpoint is also at zero. Next, we show that there is actually more freedom in choosing the reference time by analysing the construction in the proof more carefully._

**Proposition 4**.: _Let \(a<0<b\) and consider \(f:[a,b]\to\mathbb{R}\) defined as_

\[f(x)=\begin{cases}x&,\text{ if }x>0\\ 0&,\text{ if }x\leq 0\end{cases}\]

_There exists a 2-layer SNN \(\Phi\) with realization \(\mathcal{R}_{\Phi}=f\) on \([a,b]\) with encoding scheme \(T_{\text{in}}+x\) and decoding \(-T_{\text{out}}+t_{v}\), where \(v\) is the output neuron of \(\Phi\), \(T_{\text{in}}\in\mathbb{R}\) and \(T_{\text{out}}=T_{\text{in}}+c\) for some constant \(c>0\) depending on the parameters of \(\Phi\)._

Proof.: Performing a similar construction with the following changes and the same analysis as in the proof of Proposition 3 yields the claim. First, we slightly adjust \(\Phi=((W^{1},D^{1},\Theta^{1}),(W^{2},D^{2},\Theta^{2}))\) in comparison to (19) and consider the network

\[W^{1}=\begin{pmatrix}-\frac{1}{2}&0\\ 1&1\end{pmatrix},D^{1}=\begin{pmatrix}d&0\\ d&d\end{pmatrix},\Theta^{1}=\begin{pmatrix}\theta\\ \theta\end{pmatrix},W^{2}=\begin{pmatrix}-\frac{1}{2}\\ 1\end{pmatrix},D^{2}=\begin{pmatrix}d\\ d\end{pmatrix},\Theta^{2}=\theta,\]

where \(d\geq 0\) and \(\theta>b\) are fixed (see Figure 0(b)). Second, we choose the input reference time \(T_{\text{in}}\in\mathbb{R}\) and fix the input of the auxiliary input neuron \(u_{2}\) as \(t_{u_{2}}=T_{\text{in}}\in\mathbb{R}\). Finally, setting the output reference time \(T_{\text{out}}=2(\theta+d)+T_{\text{in}}\) is sufficient to guarantee that \(\Phi\) realizes \(f\) on \([a,b]\). 

### Realizing ReLU networks by spiking neural networks

In this section, we show that an SNN has the capability to reproduce the output of any ReLU network. Specifically, given access to the weights and biases of an ANN, we construct an SNN and set the parameter values based on the weights and biases of the given ANN. This leads us to the desired result. The essential part of our proof revolves around choosing the parameters of an SNN such that it effectively realizes the composition of an affine-linear map and the non-linearity represented by the ReLU activation. The realization of ReLU with SNNs is proved in the previous Section A.4. To realize an affine-linear function using a spiking neuron, it is necessary to ensure that the spikes from all the input neurons together result in the firing of an output neuron instead of any subset of the input neurons. We achieve that by appropriately adjusting the value of the threshold parameter. As a result, a spiking neuron, which implements an affine-linear map, avoids partitioning of the input space.

Setup for the proof of Theorem 3Let \(d,L\in\mathbb{N}\) be the width and the depth of an ANN \(\Psi\), respectively, i.e.,

\[\Psi=((A^{1},B^{1}),(A^{2},B^{2}),\ldots,(A^{L},B^{L})),\text{ where }(A^{\ell},B^{\ell})\in\mathbb{R}^{d\times d}\times\mathbb{R}^{d},1\leq\ell<L,\] \[(A^{L},B^{L})\in\mathbb{R}^{1\times d}\times\mathbb{R}.\]

For a given input domain \([a,b]^{d}\subset\mathbb{R}^{d}\), we denote by \(\Psi^{\ell}=((A^{\ell},B^{\ell}))\) the \(\ell\)-th layer, where \(y^{0}\in[a,b]^{d}\) and

\[y^{l}=\mathcal{R}_{\Psi^{l}}(y^{l-1})=\sigma(A^{l}y^{l-1}+B^{l} ),1\leq\ell<L,\] \[y^{L}=\mathcal{R}_{\Psi^{L}}(y^{L-1})=A^{L}y^{L-1}+B^{L}\] (22)

so that \(\mathcal{R}_{\Psi}=\mathcal{R}_{\Psi^{L}}\circ\cdots\circ\mathcal{R}_{\Psi^{1}}\).

For the construction of the corresponding SNN we refer to the associated weights and delays between two spiking neurons \(u\) and \(v\) by \(w_{uv}\) and \(d_{uv}\), respectively.

Proof of Theorem 3.: Any multi-layer ANN \(\Psi\) with ReLU activation is simply an alternating composition of affine-linear functions \(A^{l}y^{l-1}+B^{l}\) and a non-linear function represented by \(\sigma\). To generate the mapping realized by \(\Psi\), it suffices to realize the composition of affine-linear functions and the ReLU non-linearity and then extend the construction to the whole network using concatenation and parallelization operations. We prove the result via the following steps; see also Figure 2 for a depiction of the intermediate constructions.

**Step 1:** Realizing ReLU non-linearity.

Proposition 4 gives the desired result.

**Step 2:** Realizing affine-linear functions with one-dimensional range.

Let \(f:[a,b]^{d}\rightarrow\mathbb{R}\) be an affine-linear function

\[f(x)=C^{T}x+s,\quad C^{T}=(c_{1},\ldots,c_{d})\in\mathbb{R}^{d},s\in\mathbb{R}.\] (23)

Consider a one-layer SNN that consists of an output neuron \(v\) and d input units \(u_{1},\ldots,u_{d}\). Via (7) the firing time of \(v\) as a function of the input firing times on the linear region \(R^{I}\) corresponding to the

Figure 2: (a) Computation graph of an ANN with two input and one output unit realizing \(\sigma(f(x_{1},x_{2}))\), where \(\sigma\) is the ReLU activation function. (b) Computation graph associated with an SNN resulting from the concatenation of \(\Phi^{\sigma}\) and \(\Phi^{f}\) that realizes \(\sigma(f(x_{1},x_{2}))\). The auxiliary neurons are shown in red. (c) Same computation graph as in (b); when parallelizing two identical networks, the dotted auxiliary neurons can be removed and auxiliary neurons from (b) can be used for each network instead. (d) Computation graph associated with a spiking network as a result of the parallelization of two subnetworks \(\Phi^{\sigma\circ f_{1}}\) and \(\Phi^{\sigma\circ f_{2}}\). The auxiliary neuron in the output layer serves the same purpose as the auxiliary neuron in the input layer and is needed when concatenating two such subnetworks \(\Phi_{\sigma\circ f}\).

index set \(I=\{1,\ldots,d\}\) is given by

\[t_{v}(t_{u_{1}},\ldots,t_{u_{d}})=\frac{\theta_{v}}{\sum_{i\in I}w_{u_{i}v}}+\frac {\sum_{i\in I}w_{u_{i}v}(t_{u_{i}}+d_{u_{i}v})}{\sum_{i\in I}w_{u_{i}v}}\quad \text{provided that }\sum_{i\in I}w_{u_{i}v}>0.\]

Introducing an auxiliary input neuron \(u_{d+1}\) with weight \(w_{u_{d+1}v}=1-\sum_{i\in I}w_{u_{i}v}\) ensures that \(\sum_{i\in I\cup\{d+1\}}w_{u_{i}v}>0\) and leads to the firing time

\[t_{v}(t_{u_{1}},\ldots,t_{u_{d+1}})=\theta_{v}+\sum_{i\in I\cup\{d+1\}}w_{u_{i} v}(t_{u_{i}}+d_{u_{i}v})\quad\text{ on }R^{I\cup\{d+1\}}.\]

Setting \(w_{u_{i}v}=c_{i}\) for \(i\in I\) and \(d_{u_{j}v}=d^{\prime}\geq 0\) for \(j\in I\cup\{d+1\}\) yields

\[t_{v}(t_{u_{1}},\ldots,t_{u_{d+1}})=\theta_{v}+w_{u_{d+1}v}\cdot t_{u_{d+1}}+ d^{\prime}+\sum_{i\in I}c_{i}t_{u_{i}}\text{ on }R^{I\cup\{d+1\}}\cap[a,b]^{d}.\]

Therefore, an SNN \(\Phi^{f}=(W,D,\Theta)\) with parameters

\[W=\left(\begin{array}{c}c_{1}\\ \vdots\\ c_{d+1}\end{array}\right),D=\left(\begin{array}{c}d^{\prime}\\ \vdots\\ d^{\prime}\end{array}\right),\Theta=\theta>0,\quad\text{ where }c_{d+1}=1-\sum_{i\in I}c_{i},\]

and the usual encoding scheme \(T_{\text{in}}/T_{\text{out}}+\cdot\) and fixed firing time \(t_{u_{d+1}}=T_{\text{in}}\in\mathbb{R}\) realizes

\[\mathcal{R}_{\Phi^{f}}(x) =-T_{\text{out}}+t_{v}(T_{\text{in}}+x_{1},\ldots,T_{\text{in}}+ x_{d},T_{\text{in}})=-T_{\text{out}}+\theta+T_{\text{in}}+d^{\prime}+\sum_{i \in I}c_{i}x_{i}\] (24) \[=-T_{\text{out}}+\theta+T_{\text{in}}+d^{\prime}+f(x_{1},\ldots, x_{d})-s\quad\text{ on }R^{I\cup\{d+1\}}\cap[a,b]^{d}.\] (25)

Choosing a large enough threshold \(\theta\) ensures that a spike in \(v\) is necessarily triggered after all the spikes from \(u_{1},\ldots,u_{d+1}\) reached \(v\) so that \([a,b]^{d}\subset R^{I\cup\{d+1\}}\) holds. It suffices to set

\[\theta\geq\sup_{x\in[a,b]^{d}}\sup_{x_{\text{min}}\leq t-T_{\text{in}}-d^{ \prime}\leq x_{\text{max}}}P_{v}(t),\]

where \(x_{\text{min}}=\min\{x_{1},\ldots,x_{d},0\}\) and \(x_{\text{max}}=\max\{x_{1},\ldots,x_{d},0\}\), since this implies that the potential \(P_{v}(t)\) is smaller than the threshold to trigger a spike in \(v\) on the time interval associated to feasible input spikes, i.e., \(v\) emits a spike after the last spike from an input neuron arrived at \(v\). Applying (5) shows that for \(x\in[a,b]^{d}\) and \(t\in[x_{\text{min}}+T_{\text{in}}+d^{\prime},x_{\text{max}}+T_{\text{in}}+d^{ \prime}]\)

\[P_{v}(t) =\sum_{i\in I}w_{u_{i}v}(t-(T_{\text{in}}+x_{i})-d_{u_{i}v})+w_{u _{d+1}v}(t-T_{\text{in}}-d_{u_{d+1}v})=t-d^{\prime}-T_{\text{in}}+\sum_{i\in I} c_{i}x_{i}\] \[\leq x_{\text{max}}+d\left\|C\right\|_{\infty}\left\|x\right\|_{ \infty}\leq(1+d\left\|C\right\|_{\infty})\max\{|a|,|b|\}.\]

Hence, we set

\[\theta=(1+d\left\|C\right\|_{\infty})\max\{|a|,|b|\}+s+|s|\quad\text{ and }\quad T_{\text{out}}=\theta-s+T_{\text{in}}+d^{\prime}\]

to obtain via (24) that

\[\mathcal{R}_{\Phi^{f}}(x)=-T_{\text{out}}+t_{v}(T_{\text{in}}+x_{1},\ldots,T_{ \text{in}}+x_{d},T_{\text{in}})=f(x)\quad\text{ for }x\in[a,b]^{d}.\] (26)

Note that the reference time \(T_{\text{out}}=(1+d\left\|C\right\|_{\infty})\max\{|a|,|b|\}+|s|+T_{\text{in}}+d ^{\prime}\) is independent of the specific parameters of \(f\) in the sense that only upper bounds \(\left\|C\right\|_{\infty},|s|\) on the parameters are relevant. Therefore, \(T_{\text{out}}\) (with the associated choice of \(\theta\)) can be applied for different affine linear functions as long as the upper bounds remain valid. This is necessary for the composition and parallelization of subnetworks in the subsequent construction.

**Step 3:** Realizing compositions of affine-linear functions with one-dimensional range and \(\text{ReLU}\).

The next step is to realize the composition of ReLU \(\sigma\) with an affine linear mapping \(f\) defined in (23). To that end, we want to concatenate the networks \(\Phi^{\sigma}\) and \(\Phi^{f}\) constructed in Step 1 and Step 2, respectively, via Lemma 1. To employ the concatenation operation we need to perform the following steps:1. Find an appropriate input domain \([a^{\prime},b^{\prime}]\subset\mathbb{R}\), that contains the image \(f([a,b]^{d})\) so that parameters and reference times of \(\Phi^{\sigma}\) can be fixed appropriately (see Proposition 4 for the detailed conditions on how to choose the parameter).
2. Ensure that the output reference time \(T_{\text{out}}^{f}\) of \(\Phi^{f}\) equals the input reference time \(T_{\text{in}}^{\sigma}\) of \(\Phi^{\sigma}\).
3. Ensure that the number of neurons in the output layer of \(\Phi^{f}\) is the same as the number of input neurons in \(\Phi^{\sigma}\).

For the first point, note that

\[\left|f(x)\right|=\left|C^{T}x+s\right|\leq d\left\|C\right\|_{\infty}\cdot \left\|x\right\|_{\infty}+\left|s\right|\leq d\left\|C\right\|_{\infty}\cdot \max\{\left|a\right|,\left|b\right|\}+\left|s\right|\text{ for all }x\in[a,b]^{d}.\]

Hence, we can use the input domain

\[[a^{\prime},b^{\prime}]=\left[-d\left\|C\right\|_{\infty}\cdot\max\{\left|a \right|,\left|b\right|\}+\left|s\right|,d\left\|C\right\|_{\infty}\cdot\max\{ \left|a\right|,\left|b\right|\}+\left|s\right|\right]\]

and specify the parameters of \(\Phi^{\sigma}\) accordingly. Additionally, recall from Proposition 4 that \(T_{\text{in}}^{\sigma}\) can be chosen freely, so we may fix \(T_{\text{in}}^{\sigma}=T_{\text{out}}^{f}\), where \(T_{\text{out}}^{f}\) is established in Step 2. It remains to consider the third point. In order to realize ReLU an additional auxiliary neuron in the input layer of \(\Phi^{\sigma}\) with constant input \(T_{\text{in}}^{\sigma}\) was introduced. Hence, we also need to add an additional output neuron in \(\Phi^{f}\) with (constant) firing time \(T_{\text{out}}^{f}=T_{\text{in}}^{\sigma}\) so that the corresponding output and input dimension and their specification match. This is achieved by introducing a single synapse from the auxiliary neuron in the input layer of \(\Phi^{f}\) to the newly added output neuron and by specifying the parameters of the newly introduced synapse and neuron suitably. Formally, the adapted network \(\Phi^{f}=(W,D,\Theta)\) is given by

\[W=\begin{pmatrix}c_{1}&0\\ \vdots&\vdots\\ c_{d}&0\\ c_{d+1}&1\end{pmatrix},D=\begin{pmatrix}d^{\prime}&0\\ \vdots&\vdots\\ d^{\prime}&0\\ d^{\prime}&d^{\prime}\end{pmatrix},\Theta=\begin{pmatrix}\theta\\ T_{\text{out}}^{f}-T_{\text{in}}^{f}-d^{\prime}\end{pmatrix},\]

where the values of the parameters are specified in Step 2.

Then the realization of the concatenated network \(\Phi^{\sigma\circ f}\) is the composition of the individual realizations. This is exemplarily demonstrated in Figure 1(b) for the two-dimensional input case. By analyzing \(\Phi^{\sigma\circ f}\), we conclude that a three-layer SNN with

\[N(\Phi^{\sigma\circ f})=N(\Phi^{\sigma})-N_{0}(\Phi^{\sigma})+N(\Phi^{f})=5-2 +d+3=d+6\]

computational units can realize \(\sigma\circ f\) on \([a,b]^{d}\), where \(N_{0}(\Phi^{\sigma})\) denotes the number of neurons in the input layer of \(\Phi^{\sigma}\).

**Step 4:** Realizing layer-wise computation of \(\Psi\).

The computations performed in a layer \(\Psi^{\ell}\) of \(\Psi\) are described in (8). Hence, for \(1\leq\ell<L\) the computation can be expressed as

\[\mathcal{R}_{\Psi^{\ell}}(y^{l-1})=\sigma(A^{\ell}y^{l-1}+B^{l})=\begin{pmatrix} \sigma(\sum_{i=1}^{d}A_{1,i}^{l}y_{i}^{l-1}+B_{1}^{l})\\ \vdots\\ \sigma(\sum_{i=1}^{d}A_{d,i}^{l}y_{i}^{l-1}+B_{d}^{l})\end{pmatrix}=:\begin{pmatrix} \sigma(f_{1}(y^{l-1}))\\ \vdots\\ \sigma(f_{d}(y^{l-1}))\end{pmatrix},\]

where \(f_{1}^{\ell},\ldots,f_{d}^{\ell}\) are affine linear functions with one-dimensional range on the same input domain \([a^{\ell-1},b^{\ell-1}]\subset\mathbb{R}^{d}\), where \([a^{0},b^{0}]=[a,b]\) and \([a^{\ell},b^{\ell}]\) is the range of

\[(\sigma\circ f_{1}^{\ell-1},\ldots,\sigma\circ f_{d}^{\ell-1})([a^{\ell-1},b^{ \ell-1}]^{d}).\]

Thus, via Step 3, we construct SNNs \(\Phi_{1}^{\ell},\ldots,\Phi_{d}^{\ell}\) that realize \(\sigma\circ f_{1}^{\ell},\ldots,\sigma\circ f_{d}^{\ell}\) on \([a^{\ell-1},b^{\ell-1}]\). Note that by choosing appropriate parameters in the construction performed in Step 2 (as described below (26)), e.g., \(\left\|A^{\ell}\right\|_{\infty}\) and \(\left\|B^{\ell}\right\|_{\infty}\), we can employ the same input and output reference time for each \(\Phi_{1}^{\ell},\ldots,\Phi_{d}^{\ell}\). Consequently, we can parallelize \(\Phi_{1}^{\ell},\ldots,\Phi_{d}^{\ell}\) (see Lemma 2) and obtain networks \(\Phi^{\ell}=P(\Phi_{1}^{\ell},\ldots,\Phi_{d}^{\ell})\) realizing \(\mathcal{R}_{\Psi^{\ell}}\) on \([a^{\ell-1},b^{\ell-1}]\). Finally, \(\Psi^{L}\) can be directly realized via Step 2 by an SNN \(\Phi^{L}\) (as in the last layer no activation function is applied and the output is one-dimensional). Although \(\Phi^{\ell}\) already performs the desired task of realizing \(\mathcal{R}_{\Psi^{\ell}}\) we can slightly simplify the network.

By construction in Step 3, each \(\Phi^{\ell}_{i}\) contains two auxiliary neurons in the hidden layers. Since the input and output reference time is chosen consistently for \(\Phi^{\ell}_{1},\ldots,\Phi^{\ell}_{d}\), we observe that the auxiliary neurons in each \(\Phi^{\ell}_{i}\) perform the same operations and have the same firing times. Therefore, without changing the realization of \(\Phi^{\ell}\) we can remove the auxiliary neurons in \(\Phi^{\ell}_{2},\ldots,\Phi^{\ell}_{d}\) and introduce synapses from the auxiliary neurons in \(\Phi^{\ell}_{1}\) accordingly. This is exemplarily demonstrated in Figure 2c for the case \(d=2\). After this modification, we observe that \(L(\Phi^{\ell})=L(\Phi^{\ell}_{i})=3\) and

\[N(\Phi^{\ell}) =N(\Phi^{\ell}_{1})+\sum_{i=2}^{d}\left(N(\Phi^{\ell}_{i})-2-N_{0} (\Phi^{\ell}_{i})\right)=dN(\Phi^{\ell}_{1})-(d-1)(2+N_{0}(\Phi^{\ell}_{1}))\] \[=d(d+6)-2(d-1)-(d-1)(d+1)=4d+3\quad\text{ for }1\leq\ell<L,\]

whereas \(L(\Phi^{L})=1\) and \(N(\Phi^{L})=d+2\).

**Step 5:** Realizing compositions of layer-wise computations of \(\Psi\).

The last step is to compose the realizations \(\mathcal{R}_{\Phi^{1}},\ldots,\mathcal{R}_{\Phi^{L}}\) to obtain the realization

\[\mathcal{R}_{\Phi^{L}}\circ\cdots\circ\mathcal{R}_{\Phi^{1}}=\mathcal{R}_{ \Psi^{L}}\circ\cdots\circ\mathcal{R}_{\Psi^{1}}=\mathcal{R}_{\Psi}.\]

As in Step 3, it suffices again to verify that the concatenation of the networks \(\mathcal{R}_{\Phi^{1}},\ldots,\mathcal{R}_{\Phi^{L}}\) is feasible. First, note that for \(\ell=1,\ldots,L\) the input domain of \(\mathcal{R}_{\Phi^{\ell}}\) is given by \([a^{\ell-1},b^{\ell-1}]\) so that, we can fix the suitable output reference time \(T^{\Phi^{\ell}}_{\text{out}}\) based on the parameters of the network, the input domain \([a^{\ell-1},b^{\ell-1}]\), and some input reference time \(T^{\Phi^{\ell}}_{\text{in}}\in\mathbb{R}\). By construction in Steps 2 - 4 \(T^{\Phi^{\ell}}_{\text{in}}\) can be chosen freely. Hence setting \(T^{\Phi^{\ell+1}}_{\text{in}}=T^{\Phi^{\ell}}_{\text{out}}\) ensures that the reference times of the corresponding networks agree. It is left to align the input dimension of \(\Phi^{\ell+1}\) and the output dimension of \(\Phi^{\ell}\) for \(\ell=1,\ldots,L-1\). Due to the auxiliary neuron in the input layer of \(\Phi^{\ell+1}\), we also need to introduce an auxiliary neuron in the output layer of \(\Phi^{\ell}\) (see Figure 2d) with the required firing time \(T^{\Phi^{\ell+1}}_{\text{in}}=T^{\Phi^{\ell}}_{\text{out}}\). Similarly, as in Step 3, it suffices to add a single synapse from the auxiliary neuron in the previous layer to obtain the desired firing time.

Thus, we conclude that \(\Phi=\Phi^{L}\bullet\cdots\bullet\Phi^{1}\) realizes \(\mathcal{R}_{\Psi}\) on \([a,b]\), as desired. The complexity of \(\Phi\) in the number of layers and neurons is given by

\[L(\Phi)=\sum_{\ell=1}^{L}L(\Phi^{\ell})=3L-2=3L(\Psi)-2\]

and

\[N(\Phi) =N(\Phi^{1})+\sum_{\ell=2}^{L}\left(N(\Phi^{\ell})-N_{0}(\Phi^{ \ell})\right)+(L-1)\] \[=4d+3+(L-2)(4d+3-(d+1))+(d+2-(d+1))+(L-1)\] \[=3L(d+1)-(2d+1)\] \[=N(\Psi)+L(2d+3)-(2d+2)\]

**Remark 7**.: _Note that the delays play no significant role in the proof of the above theorem. Nevertheless, they can be employed to alter the timing of spikes, consequently impacting the firing time and the resulting output. However, the exact function of delays requires further investigation. The primary objective is to present a construction that proves the existence of a spiking network capable of accurately reproducing the output of any ReLU network._