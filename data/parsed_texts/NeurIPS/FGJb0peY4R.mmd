Unveil Benign Overfitting for Transformer in Vision: Training Dynamics, Convergence, and Generalization

 Jiarui Jiang\({}^{*}\)1, Wei Huang\({}^{*}\)2, Miao Zhang\({}^{\dagger}\)1, Taiji Suzuki\({}^{3}\)2, Liqiang Nie\({}^{1}\)

\({}^{1}\)Harbin Institute of Technology, Shenzhen

\({}^{2}\)RIKEN AIP

\({}^{3}\)University of Tokyo

jiaruij@outlook.com, wei.huang.vr@riken.jp, zhangmiao@hit.edu.cn

taiji@mist.i.u-tokyo.ac.jp, nieliqiang@hit.edu.cn

Equal contributionCorresponding author

###### Abstract

Transformers have demonstrated great power in the recent development of large foundational models. In particular, the Vision Transformer (ViT) has brought revolutionary changes to the field of vision, achieving significant accomplishments on the experimental side. However, their theoretical capabilities, particularly in terms of generalization when trained to overfit training data, are still not fully understood. To address this gap, this work delves deeply into the _benign overfitting_ perspective of transformers in vision. To this end, we study the optimization of a Transformer composed of a self-attention layer with softmax followed by a fully connected layer under gradient descent on a certain data distribution model. By developing techniques that address the challenges posed by softmax and the interdependent nature of multiple weights in transformer optimization, we successfully characterized the training dynamics and achieved generalization in post-training. Our results establish a sharp condition that can distinguish between the small test error phase and the large test error regime, based on the signal-to-noise ratio in the data model. The theoretical results are further verified by experimental simulation. To the best of our knowledge, this is the first work to characterize benign overfitting for Transformers.

## 1 Introduction

Transformers (Vaswani et al., 2017) have revolutionized numerous fields in artificial intelligence, ranging from natural language processing (Devlin et al., 2018; OpenAI, 2023), computer vision Dosovitskiy et al. (2020); He et al. (2022), graph Yun et al. (2019); Hu et al. (2020) to reinforcement learning Chen et al. (2021); Janner et al. (2021). In particular, Vision Transformer (ViT) Dosovitskiy et al. (2020) has been developed to advancing the computer vision tasks (Liu et al., 2021; Khan et al., 2022; Guo et al., 2022; Tolstikhin et al., 2021; Lin et al., 2014) compared to Convolutional neural networks. Since then, with the high performance, ViT emerged as a hot area of research and application. Numerous technologies and methods began to surface, including enhancing ViT's data and resource efficiency (Touvron et al., 2021; Hassani et al., 2021; Chen et al., 2021; Touvron et al., 2022), reducing heavy computation cost Chen et al. (2022); Zhu et al. (2024); Wang et al. (2023).

Recent success brought by ViT has been inspiring an increasing number of works to understand the ViT through empirical and theoretical studies. The empirical investigation works study the robustness (Bhojanapalli et al., 2021; Naseer et al., 2021; Paul and Chen, 2022; Bai et al., 2021); and the role in self-supervision (Caron et al., 2021; Chen et al., 2021; Oquab et al., 2023) and others. On the other hand side, the theoretical works have been conducted to understand the ViT from the perspective of expressivity (Vuckovic et al., 2020; Edelman et al., 2022), optimization (Zhang et al., 2020; Tian et al., 2023, 2023), generalization (Li et al., 2023), and in-context learning (Huang et al., 2023; Garg et al., 2022; Bai et al., 2024; Ahn et al., 2024).

Despite the insightful understanding provided by the above investigations, it remains unclear how ViTs generalize to unseen data when they are trained to overfit the training data. In particular, the conditions under which ViTs can exhibit _benign overfitting_(Barlett et al., 2020), where the test error remains small despite overfitting to the training data, are not well understood. Moreover, prior work on the optimization and generalization of ViTs often focuses on simplified settings, such as linear transformers or unrealistic loss functions, due to technical limitations. In this work, we aim to fill this gap through a feature learning theory (Allen-Zhu and Li, 2020; Cao et al., 2022). Based on a data generative model with \(M\) tokens composed of signal tokens and noise tokens and a two-layer ViT with softmax attention, we characterize the training dynamics from a random initialization and the generalization ability of the ViT after convergence. We establish a sharp separation in condition to distinct the benign overfitting and harmful overfitting regime for ViTs. Our contributions are summarized as follows:

* We successfully characterize the optimization of the ViT through three different phases in dynamics that exhibit rich and unique behaviors related to self-attention. Building on the convergence results, we further characterize the generalization bounds on unseen datasets.
* Technically, we develop novel methods to handle non-linear attention and the inter-dependent nature of multiple weights in transformer optimization when training from scratch.
* We establish a sharp separation condition in the signal-to-noise ratio in data to distinguish the benign overfitting and harmful overfitting regimes of ViTs. This separation is then verified by experimental simulation.

## 2 Related Work

Benign Overfitting in Deep LearningThe most prominent behavior of deep learning is its breaking of bias-variance trade-off in statistical learning theory, i.e., good generalization on unseen data even with overfitting on training data. This line of research starts from Bartlett et al. (2020) who studied benign overfitting in linear regression, learning data generated by a linear model with additive Gaussian noises. It is shown that at large input dimensions (which leads to over-parameterization), the excessive risk of the interpolation can be asymptotically optimal. Hastie et al. (2022); Wu and Xu (2020) studied linear regression when both the dimension and the number of samples scale together with a fixed ratio. Timor et al. (2023) proposed a unified data model for linear predictors and studied conditions under which benign overfitting occurs in different problems. Besides the studies on linear models (Wang et al., 2021; Zou et al., 2021; Zhou and Ge, 2023), several recent works tried to study benign overfitting in neural networks (Frei et al., 2022; Kornowski et al., 2024; Chatterji et al., 2022; Xu et al., 2023). In particular, Li et al. (2021) investigated the benign overfitting in two-layer neural networks with the first layer parameters fixed at random initialization. Furthermore, _benign overfitting_ was characterized in convolution neural networks (Cao et al., 2022; Kou et al., 2023), OOD (Chen et al., 2024), federated learning (Huang et al., 2023), graph neural network (Huang et al., 2023) by feature learning theory (Allen-Zhu and Li, 2020; Cao et al., 2022). Unlike existing research on benign overfitting, this work focuses on ViTs.

Optimization of TransformersTowards understanding the optimization of Transformers, Zhang et al. (2020) provided the analysis for adaptive gradient methods. Jelassi et al. (2022) proposed a spatially structured dataset and a simplified ViT model and showed that ViT implicitly learns the spatial structure of the dataset while generalizing. Besides, Li et al. (2023) provided fine-grained mechanistic understanding of how transformers learn "semantic structure" for BERT-like framework (Devlin et al., 2018). Furthermore, Tian et al. (2023, 2023) characterizes the SGD training dynamics of a 1-layer Transformer and multi-layer Transformer respectively. They focused on the unique phenomena and the role of attention in the training dynamics. Huang et al. (2023) studied the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. In addition, Li et al. (2024) provided a theoretical analysis of the training dynamics of Transformers with nonlinear self-attention within In-Context Learning. In particular, Li et al. (2023a) is most relevant to us, as they also study the training dynamics of ViTs with a similar data model. However, they considered hinge loss and a specific initialization to simplify analysis and did not characterize the harmful overfitting regime. In summary, while the above work studies the optimization dynamics of attention-based models, they do not characterize benign overfitting as we investigate.

## 3 Problem Setup

In this section, we outline the data generation model, the Vision Transformer model, and the gradient descent training algorithm.

**Notations.** For two sequences \(\{x_{n}\}\) and \(\{y_{n}\}\), we denote \(x_{n}=O(y_{n})\) if there exist some absolute constant \(C>0\) and \(N>0\) such that \(|x_{n}|\leq C|y_{n}|\) for all \(n\geq N\), denote \(x_{n}=\Omega(y_{n})\) if \(y_{n}=O(x_{n})\), and denote \(x_{n}=\Theta(y_{n})\) if \(x_{n}=O(y_{n})\) and \(x_{n}=\Omega(y_{n})\). We also denote \(x_{n}=o(y_{n})\) if \(\text{lim}\ |x_{n}/y_{n}|=0\). Finally, we use \(\widetilde{O}(\cdot)\), \(\widetilde{\Omega}(\cdot)\) and \(\widetilde{\Theta}(\cdot)\) to hide logarithmic factors in these notations respectively.

**Definition 3.1** (Data Generation Model).: _Let \(\bm{\mu}_{+},\bm{\mu}_{-}\in\mathbb{R}^{d}\) be fixed vectors representing the signals contained in data points, where \(\|\bm{\mu}_{+}\|_{2}=\|\bm{\mu}_{-}\|_{2}=\|\bm{\mu}\|_{2}\) and \(\langle\bm{\mu}_{+},\bm{\mu}_{-}\rangle=0\,\). Then each data point \((\bm{X},y)\) with \(\bm{X}=(\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{M})^{\top}\in\mathbb{R}^{M\times d}\) and \(y\in\{-1,1\}\) is generated from the following distribution \(D\):_

1. _The label_ \(y\) _is generated as a Rademacher random variable._
2. _If_ \(y=1\) _then_ \(\bm{x}_{1}\) _is given as_ \(\bm{\mu}_{+}\)_, if_ \(y=-1\) _then_ \(\bm{x}_{1}\) _is given as_ \(\bm{\mu}_{-}\)_, which represents signals._
3. _A noise vector_ \(\bm{\xi}_{2}\) _is generated from the Gaussian distribution_ \(\mathcal{N}(0,\tilde{\sigma}_{p}^{2}\cdot(\bm{I}-\bm{\mu}_{+}\bm{\mu}_{+}^{ \top}\cdot\|\bm{\mu}\|_{2}^{-2}-\bm{\mu}_{-}\bm{\mu}_{-}^{\top}\cdot\|\bm{\mu} \|_{2}^{-2}))\)_._
4. _Noise vectors_ \(\bm{\xi}_{3},\ldots,\bm{\xi}_{M}\) _is generated i.i.d from the Gaussian distribution_ \(\mathcal{N}(0,\sigma_{p}^{2}\cdot(\bm{I}-\bm{\mu}_{+}\bm{\mu}_{+}^{\top}\cdot\| \bm{\mu}\|_{2}^{-2}-\bm{\mu}_{-}\bm{\mu}_{-}^{\top}\cdot\|\bm{\mu}\|_{2}^{-2}))\)_._
5. \(\bm{x}_{2},\ldots,\bm{x}_{M}\) _are given by_ \(\bm{\xi}_{2},\ldots,\bm{\xi}_{M}\)_, which represent noises._

Our data model is envisioned by considering \(M\) tokens of the data points: \(\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{M}\), which can be seen as patches derived from image data. \(\bm{x}_{1}\) embodies the signal that is inherently linked to the data's class label, while \(\bm{x}_{2},\bm{x}_{3},\ldots,\bm{x}_{M}\) represents the noise, which is not associated with the label. For simplicity, we assume that the noise patches is independently drawn from Gaussian distribution \(\mathcal{N}(0,\tilde{\sigma}_{p}^{2}\cdot(\bm{I}-\bm{\mu}_{+}\bm{\mu}_{+}^{ \top}\cdot\|\bm{\mu}\|_{2}^{-2}-\bm{\mu}_{-}\bm{\mu}_{-}^{\top}\cdot\|\bm{\mu} \|_{2}^{-2}))\) and \(\mathcal{N}(0,\sigma_{p}^{2}\cdot(\bm{I}-\bm{\mu}_{+}\bm{\mu}_{+}^{\top}\cdot\| \bm{\mu}\|_{2}^{-2}-\bm{\mu}_{-}\bm{\mu}_{-}^{\top}\cdot\|\bm{\mu}\|_{2}^{-2}))\), ensuring the noise vectors remains orthogonal to the signal vectors \(\bm{\mu}_{+}\) and \(\bm{\mu}_{-}\). Also, if \(\tilde{\sigma}_{p}\) is sufficiently larger than \(\sigma_{p}\), the input tokens will become sparse, which is consistent with the empirical observation that the _attention_ and activation values in Transformer-based models are usually sparse (Child et al., 2019; Robinson et al., 2023). We denote \(\mathrm{SNR}=\|\bm{\mu}\|_{2}/(\sigma_{p}\sqrt{d})\) to represent the signal-to-noise ratio.

**Two-layer Transformer.** We consider a two layer Transformer network with a self-attention layer and a fixed linear layer, which is defined as:

\[f(\bm{X},\theta)=\frac{1}{M}\sum_{l=1}^{M}\bm{\varphi}(\bm{x}_{l}^{\top}\bm{W}_ {Q}\bm{W}_{K}^{\top}\bm{X}^{\top})\bm{X}\bm{W}_{V}\bm{w}_{O}.\] (1)

Here, \(\bm{\varphi}(\cdot):\mathbb{R}^{M}\rightarrow\mathbb{R}^{M}\) denote the softmax function, \(\bm{W}_{Q},\bm{W}_{K}\in\mathbb{R}^{d\times d_{h}},\bm{W}_{V}\in\mathbb{R}^{d \times d_{v}}\) denote the query matrix, key matrix, and value matrix, respectively, and \(\bm{w}_{O}\in\mathbb{R}^{d_{v}}\) denote the weight for the linear layer. We use \(\theta\) to denote the collection of all the model weights. This model is a simplified version of the ViT model from Dosovitskiy et al. (2020), making our analysis focus on the self-attention mechanism which is the most critical component of the ViTs.

Given a training data set \(S=\{(\bm{X}_{n},y_{n})\}_{n=1}^{N}\) generated from the distribution \(D\) defined in Definition 3.1, where the subscript \(n\) represents the \(n\)-th sample, we train the two-layer Transformer by minimizing the empirical cross-entropy loss function:

\[L_{S}(\theta)=\frac{1}{N}\sum_{n=1}^{N}\ell(y_{n}f(\bm{X}_{n},\theta)),\]

where \(\ell(z)=\log(1+\exp(-z))\) and \(f(\bm{X},\theta)\) is the two-layer Transformer. We further defined the population loss (test loss) \(L_{D}(\theta):=\mathbb{E}_{(\bm{X},y)\sim D}\ell(yf(\bm{X},\theta))\).

**Training algorithm.** We consider Gaussian initialization for the network weights \(\bm{W}_{Q},\bm{W}_{K}\) and \(\bm{W}_{V}\), where each entry of \(\bm{W}_{Q}\) and \(\bm{W}_{K}\) is sampled from a Gaussian distribution \(\mathcal{N}(0,\sigma_{h}^{2})\), and \(\bm{W}_{V}\) is sampled from \(\mathcal{N}(0,\sigma_{V}^{2})\) at initialization. We use gradient descent to optimize training loss \(L_{S}(\theta)\), and the update of \(\bm{W}_{Q},\bm{W}_{K}\) and \(\bm{W}_{V}\) can be written as follows:

\[\bm{W}^{(t+1)}=\bm{W}^{(t)}-\eta\cdot\nabla_{\bm{W}}L_{S}(\theta(t)),\]

where \(\bm{W}^{(t)}\) can be designated as \(\bm{W}_{Q},\bm{W}_{K}\) or \(\bm{W}_{V}\).

Recalling the ViT defined in Eq. (1), we can intuitively analyze its training dynamics and generalization: if more attention is paid to signals, i.e., \(\bm{x}_{l}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{\mu}_{\pm}\geq\bm{x}_{l}^{\top }\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{\xi}_{l}\) for \(i\in[M]/\{1\}\), then the vector \(\bm{\varphi}(\bm{x}_{l}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{X}^{\top})\bm{X}\) has a higher similarity with signals \(\bm{\mu}_{\pm}\) rather than with noises \(\bm{\xi}_{i}\). In turn, \(\bm{W}_{V}\) together with \(\bm{w}_{O}\) have more chance to learn signals \(\bm{\mu}_{\pm}\) and utilize them to make prediction. At the same time, if vector \(\bm{W}_{V}\bm{w}_{O}\) learns signals \(\bm{\mu}_{\pm}\) more than noises \(\bm{\xi}_{i}\), i.e., \(\bm{\mu}_{\pm}\bm{W}_{V}\bm{w}_{O}\geq\bm{\xi}_{i}\bm{W}_{V}\bm{w}_{O}\), then during the gradient descent training, the gradient send to \(\bm{x}_{l}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{\mu}_{\pm}\) will be larger than that send to \(\bm{x}_{l}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{\xi}_{i}\). As a result, more and more attention is paid on signals and the similarity between signals \(\bm{\mu}_{\pm}\) and \(\bm{W}_{V}\bm{w}_{O}\) becomes increasingly high. Therefore, the Transformer can perform well on new data points. On the contrary, if much attention is paid to some noises in the training dataset and the model utilizes them for making prediction and optimizing the training loss, then the model can fit the training dataset well but might not perform well on the new test data.

## 4 Main Results

This section presents our main theoretical results which characterize the convergence and generalization of the ViT model under different sample size \(N\) and signal-to-noise ratio \(\mathrm{SNR}=\|\bm{\mu}\|_{2}/(\sigma_{p}\sqrt{d})\). The results are based on the following conditions.

**Condition 4.1**.: _Given a sufficiently small failure probability \(\delta>0\) and a target training loss \(\epsilon>0\), suppose that:(1) Dimension \(d_{h}=\widetilde{\Omega}^{1}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N ^{2}\epsilon^{-2}\Big{)}\). (2) Dimension \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\). (3) Training sample size \(N=\Omega(\mathrm{polylog}(d))\).(4) The number of input tokens \(M=\Theta(1)\). (5) The \(\ell_{2}\)-norm of linear layer weights \(\|\bm{w}_{O}\|_{2}=\Theta(1)\). (6) The learning rate \(\eta\leq\widetilde{O}(\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\} \cdot d_{h}^{-\frac{1}{2}})\). (7) The standard deviation of Gaussian initialization \(\sigma_{V}\) satisfies: \(\sigma_{V}\leq\widetilde{O}\big{(}\|\bm{w}_{O}\|_{2}^{-1}\cdot\min\{\|\bm{\mu} \|_{2}^{-1},(\sigma_{p}\sqrt{d})^{-1}\}\cdot d_{h}^{-\frac{1}{4}}\big{)}\). (8) The variance of Gaussian initialization \(\sigma_{h}^{2}\) satisfies: \(\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d_{h}^{-\frac{1}{2} }\cdot\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{-2}\leq\sigma_{h}^{2}\leq\min\{\| \bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d_{h}^{-\frac{1}{2}}\cdot \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{-\frac{3}{2}}\). (9) Target training loss \(\epsilon\leq O(1/\mathrm{polylog}(d))\). (10) The relationship between \(\sigma_{p}\) and \(\tilde{\sigma}_{p}\) satisfies \(\tilde{\sigma}_{p}=C_{p}\sigma_{p}\) and \(C_{p}=5\sqrt{M}\)._

Conditions (1) and (2) ensure an over-parametrized learning setting, and similar conditions have been made in the theoretical analysis of CNN models (Cao et al., 2022; Kou et al., 2023). Condition (3) ensures that there are enough samples in each class with high probability. Conditions (4)-(5) are intended to simplify the calculation, and can be easily generalized to \(M=\Omega(1)\), \(\|\bm{w}_{O}\|_{2}=o(1)\) or \(\|\bm{w}_{O}\|_{2}=\omega(1)\) setting. Conditions (6)-(8) ensure that the Transformer can be effectively trained well. Condition (9) ensures that the Transformer is sufficiently overfitting the training data. Condition (10) ensures the sparsity of the input token features.

**Theorem 4.1** (**Benign Overfitting**).: _Under Condition 4.1, if \(N\cdot\mathrm{SNR}^{2}=\Omega(1)\), then with probability at least \(1-d^{-1}\), there exist \(T=\Theta(\eta^{-1}\epsilon^{-1}\|\bm{\mu}\|_{2}^{-2}\|\bm{w}_{O}\|_{2}^{-2})\) such that:_

1. _The training loss converges to_ \(\epsilon\)_:_ \(L_{S}(\theta(T))\leq\epsilon\)_._
2. _The test loss is nearly zero:_ \(L_{D}(\theta(T))\leq o(1)\)_._

Theorem 4.1 characterizes the case of _benign overfitting_. It shows that as long as \(N\cdot\mathrm{SNR}^{2}=\Omega(1)\), the Transformer can generalize well, even though it overfit the training data. To complement the above result and highlight the sharpness of this condition, we present the following theorem for the regime of _harmful overfitting_.

**Theorem 4.2** (**Harmful Overfitting**).: _Under Condition 4.1, if \(N^{-1}\cdot\mathrm{SNR}^{-2}=\Omega(1)\), then with probability at least \(1-d^{-1}\), there exist \(T=\Theta(N\eta^{-1}\epsilon^{-1}\sigma_{p}^{-2}d^{-1}\|\bm{w}_{O}\|_{2}^{-2})\) such that:_

1. _The training loss converges to_ \(\epsilon\)_:_ \(L_{S}(\theta(T))\leq\epsilon\)_._
2. _The test loss is high:_ \(L_{D}(\theta(T))=\Theta(1)\)_._

Theorem 4.2 shows that if \(N^{-1}\cdot\mathrm{SNR}^{-2}=\Omega(1)\), the trained Transformer has a high test loss as a result of overfitting the noises in the training data. Theorem 4.1 and Theorem 4.2 reveals a sharp phase transition between _benign overfitting_ and _harmful overfitting_.

Comparison with other results.Firstly, compared with CNNs with \(\mathrm{ReLU}^{q}\) network result (Cao et al., 2022), when signal-to-noise ratio is small (\(\mathrm{SNR}\leq 1\)), our results show that ViTs require less number of samples to generalize well, which reflects the advantage of Transformers. Secondly, previous theoretical benign overfitting analysis of CNNs with ReLU activations rely heavily on signal strength \(\|\bm{\mu}\|_{2}\) and signal-to-noise ratio \(\mathrm{SNR}\)(Kou et al., 2023; Meng et al., 2023), i.e., \(\mathrm{SNR}^{2}=\widetilde{O}(1/N)\) and require \(\|\bm{\mu}\|_{2}\) to be large enough, while our analysis does not need to impose restrictions on these conditions. Thirdly, previous works only show when Transformer can generalize well (Li et al., 2023; Deora et al., 2023), while we demonstrate harmful overfitting as a complementary, which reflect that our benign overfitting condition is tighter and more precise.

## 5 Proof Sketch

This section discusses our main challenges in studying ViT's training dynamic, and presents our technical solutions to overcoming them. The complete proofs are given in the appendix.

### Vectorized Q & K and scalarized V

Our first main challenge is to deal with the three matrices \(\bm{W}_{Q}\), \(\bm{W}_{K}\) and \(\bm{W}_{V}\). In contrast to CNN models, whose convolutional kernels can be treated as vectors and thus allow for a more straightforward analytical approach (Cao et al., 2022; Kou et al., 2023), the QKV matrices within the Transformer model are inherently complex to analyze. Moreover, their mutual interactions further complicate the analysis. To circumvent this complexity, Oymak et al. (2023) and Tian et al. (2023) merge the key-query weights(e.g. \(\bm{W}:=\bm{W}_{Q}\bm{W}_{K}^{\top}\)), Huang et al. (2023); Zhang et al. (2024); Jelassi et al. (2022) employ a specific initialization (e.g. \(\bm{W}_{Q}^{(0)}=\bm{W}_{K}^{(0)}=\bm{I}\)). Although their approach simplifies the analysis, it is not conducive to showing the dynamics of QKV interactions, i.e., how \(\bm{W}_{Q}\), \(\bm{W}_{K}\), and \(\bm{W}_{V}\) affect each other.

In order to simplify the analysis of QKV dynamics without losing rigor, we propose key techniques called _Vectorized Q & K_ and _scalarized V_. The basic idea comes from the property that the product of token feature vectors with the QKV matrices results in vectors, e.g., \(\bm{\mu}_{+}^{\top}\bm{W}_{Q},\bm{\xi}_{2}^{\top}\bm{W}_{Q}\), etc., which are more amenable to analysis. Further, each entry of matrix \(\bm{X}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{X}^{\top}\) can be regarded as the inner product of two vectors, e.g., \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{\mu}_{+}=\langle\bm{\mu}_{+} ^{\top}\bm{W}_{Q},\bm{\mu}_{+}^{\top}\bm{W}_{K}\rangle\). Therefore, the dynamics of _attention_ can be studied by analyzing the dynamics of the _vectorized Q & K_ defined as follows:

**Definition 5.1** (Vectorized Q & K).: _Let \(\bm{W}_{Q}^{(t)}\) and \(\bm{W}_{K}^{(t)}\) be the QK matrices of the ViT at the t-th iteration of gradient descent. Then we define the vectorized Q and vectorized K as follows_

\[\bm{q}_{+}^{(t)}=\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(t)}, \bm{q}_{-}^{(t)}=\bm{\mu}_{-}^{\top}\bm{W}_{Q}^{(t)}, \bm{q}_{n,i}^{(t)}=\bm{\xi}_{n,i}^{\top}\bm{W}_{Q}^{(t)},\] \[\bm{k}_{+}^{(t)}=\bm{\mu}_{+}^{\top}\bm{W}_{K}^{(t)}, \bm{k}_{-}^{(t)}=\bm{\mu}_{-}^{\top}\bm{W}_{K}^{(t)}, \bm{k}_{n,i}^{(t)}=\bm{\xi}_{n,i}^{\top}\bm{W}_{K}^{(t)},\]

_for \(i\in[M]\backslash\{1\},n\in[N]\)._

With Definition 5.1, denoting \(S_{+}:=\{n\in[N]:y_{n}=1\}\), \(S_{-}:=\{n\in[N]:y_{n}=-1\}\), we further analyze the dynamics of the _vectorized Q & K_. By carefully computing the dynamic of \(\bm{q}_{+}^{(t)}\), we have

\[\bm{q}_{+}^{(t+1)}-\bm{q}_{+}^{(t)}=\frac{\eta}{NM}\sum_{n\in S_{+}}-\ell_{n}^ {\prime(t)}\|\bm{\mu}\|_{2}^{2}\bm{w}_{O}^{\top}\bm{W}_{V}^{(t)\top}\bm{X}_{n }^{\top}(diag(\bm{\varphi}_{n,1}^{(t)})-\bm{\varphi}_{n,1}^{(t)\top}\bm{ \varphi}_{n,1}^{(t)})\bm{X}_{n}\bm{W}_{K}^{(t)},\] (2)

where \(\bm{\varphi}_{n,i}^{(t)}:=\bm{\varphi}(\bm{x}_{n,i}^{\top}\bm{W}_{Q}^{(t)} \bm{W}_{K}^{(t)\top}\bm{X}_{n}^{\top})\) is a shorthand notation, and \(\bm{w}_{O}^{\top}\bm{W}_{V}^{(t)\top}\bm{X}_{n}^{\top}\) and \(\bm{X}_{n}\bm{W}_{K}^{(t)}\) can be viewed in the following forms

\[\bm{w}_{O}^{\top}\bm{W}_{V}^{(t)\top}\bm{X}_{n}^{\top}=\big{(}\bm{\mu}_{+}^{ \top}\bm{W}_{V}^{(t)}\bm{w}_{O},\bm{\xi}_{n,2}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{ O},\ldots,\bm{\xi}_{n,M}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\big{)},\]

\[\bm{X}_{n}\bm{W}_{K}^{(t)}=\big{(}\bm{k}_{+}^{(t)\top},\bm{k}_{n,2}^{(t)\top}, \ldots,\bm{k}_{n,M}^{(t)\top}\big{)}^{\top}.\]

Thus \(\bm{q}_{+}^{(t+1)}-\bm{q}_{+}^{(t)}\) can be decomposed into a linear combination of \(\bm{k}_{+}^{(t)}\) and \(\bm{k}_{n,i}^{(t)}\). Therefore, Eq. (2) can be further expanded as \(\bm{q}_{+}^{(t+1)}-\bm{q}_{+}^{(t)}=\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n \in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\), where \((\alpha_{+,+}^{(t)},\alpha_{n,+,2}^{(t)},\ldots,\alpha_{n,+,M}^{(t)})=\frac{ \eta}{NM}\sum_{n\in S_{+}}-\ell_{n}^{\prime(t)}\|\bm{\mu}\|_{2}^{2}\bm{w}_{O}^{ \top}\bm{W}_{V}^{(t)\top}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,1}^{(t)})- \bm{\varphi}_{n,1}^{(t)\top}\bm{\varphi}_{n,1}^{(t)})\). Intuitively, if \(\alpha_{+,+}^{(t)}\) is larger enough than \(\alpha_{n,+,i}^{(t)}\), then \(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\) will grow faster than \(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\), which means token \(\bm{\mu}_{+}\) will pay more _attention_ to \(\bm{\mu}_{+}\) rather than \(\bm{\xi}_{n,i}\). The dynamics of \(\bm{q}_{-}^{(t)}\), \(\bm{q}_{n,i}^{(t)}\), \(\bm{k}_{+}^{(t)}\), \(\bm{k}_{-}^{(t)}\) and \(\bm{k}_{n,i}^{(t)}\) are similar to \(\bm{q}_{+}^{(t)}\), and we can study the dynamics of \(QK\) matries by analysing the linear combination coefficients such as \(\alpha_{+,+}^{(t)}\), \(\alpha_{n,+,i}^{(t)}\).

As \(\bm{x}_{n,i}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\) are scalars and their dynamics contain a factor \(\|\bm{w}_{O}\|_{2}^{2}\), we define the _scalarized V_ as follows.

**Definition 5.2** (Scalarized V).: _Let \(\bm{W}_{V}^{(t)}\) be the V matrix of the ViT at the t-th iteration of gradient descent. Then there exist coefficients \(\gamma_{V,+}^{(t)}\), \(\gamma_{V,-}^{(t)}\), \(\rho_{V,n,i}^{(t)}\) such that_

\[\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}=\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(0) }\bm{w}_{O}+\gamma_{V,+}^{(t)}\|\bm{w}_{O}\|_{2}^{2},\]

\[\bm{\mu}_{-}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}=\bm{\mu}_{-}^{\top}\bm{W}_{V}^{(0) }\bm{w}_{O}+\gamma_{V,-}^{(t)}\|\bm{w}_{O}\|_{2}^{2},\]

\[\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}=\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(0) }\bm{w}_{O}+\rho_{V,n,i}^{(t)}\|\bm{w}_{O}\|_{2}^{2}\]

_for \(i\in[M]\backslash\{1\},n\in[N]\)._

With the _Vectorized Q & K_ and _scalarized V_, one can simplify the study of the Transformer learning process to a meticulous calculation of the coefficients such as \(\alpha\), \(\gamma\), \(\rho\) throughout the training period. So how to calculate the dynamics of these coefficients is a key point in our analysis. In the next subsection we describe how to handle _softmax_ function and further give bounds for these coefficients.

### Dealing with the softmax function

Our second challenge is to deal with the _softmax_ function, which is the critical component and introduces non-linear transformation in our Transformer model.

As \(\bm{W}_{Q}\) and \(\bm{W}_{K}\) are within the _softmax_ function and \(\bm{W}_{V}\) is outside the _softmax_ function, we divide the dynamic of training process into two key aspects: (1) How \(\bm{W}_{Q}\) and \(\bm{W}_{K}\) affect \(\bm{W}_{V}\); (2) How \(\bm{W}_{V}\) affects \(\bm{W}_{Q}\) and \(\bm{W}_{K}\). Next, we present our approach to addressing these two critical issues.

1. How \(\bm{W}_{Q}\) and \(\bm{W}_{K}\) affect \(\bm{W}_{V}\):Without loss of generality, we take a data point \((\bm{X}_{n},y_{n})\) with \(y_{n}=1\) as an example and provide the dynamics for \(\gamma_{V,+}^{(t)}\) and \(\rho_{V,n,i}^{(t)}\) in \(\bm{W}_{V}\) as follows:

**Lemma 5.1** (Dynamics of \(\gamma\) and \(\rho\)).: \[\gamma_{V,+}^{(t+1)}-\gamma_{V,+}^{(t)}=\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM} \sum_{n\in S_{+},s\in[M]}\frac{-\ell_{n}^{\prime(t)}\exp(\bm{x}_{n,s}^{\top} \bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}\bm{\mu}_{+})}{\exp(\bm{x}_{n,s}^{\top}\bm{ W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}\bm{\mu}_{+})+\sum\limits_{k=2}^{M}\exp(\bm{x}_{n,s}^{ \top}\bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}\bm{x}_{n,k})}\]

\[|\rho_{V,n,i}^{(t+1)}-\rho_{V,n,i}^{(t)}|\leq\Big{|}\frac{2\eta C_{p}^{2}\sigma _{p}^{2}d}{NM}\sum_{s\in[M]}\frac{-\ell_{n}^{\prime(t)}\exp(\bm{x}_{n,s}^{\top} \bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}\bm{\xi}_{n,i})}{\exp(\bm{x}_{n,s}^{\top} \bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}\bm{\mu}_{+})+\sum\limits_{k=2}^{M}\exp( \bm{x}_{n,s}^{\top}\bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}\bm{\xi}_{n,k})}\Big{|}\]

_for \(i\in[M]\backslash\{1\},n\in S_{+}\), where \(\ell_{n}^{\prime(t)}:=\ell^{\prime}(y_{n}f(\bm{X}_{n},\theta(t)))\) is a shorthand notation._

In benign overfitting regime where \(N\cdot\mathrm{SNR}^{2}=\Omega(1)\), the factor \(\sum\limits_{n\in S_{+}}\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\) is larger than \(\frac{2\eta C_{p}^{2}\sigma_{p}^{2}d}{NM}\). Therefore, as long as \(\bm{x}_{n,s}^{\top}\bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}\bm{\mu}_{+}\) are not less than \(\bm{x}_{n,s}^{\top}\bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}\bm{\xi}_{n,i}\), \(\gamma_{V,+}^{(t)}\) will grow faster than \(\rho_{V,n,i}^{(t)}\). In other words, if \(\bm{q}_{\pm}^{(t)}\) and \(\bm{q}_{n,i}^{(t)}\) prefer to align with \(\bm{k}_{\pm}^{(t)}\) rather than \(\bm{k}_{n,i}^{(t)}\), then \(\bm{W}_{V}\) would prefer to learn signals rather than memorize noises.

2. How \(\bm{W}_{V}\) affects \(\bm{W}_{Q}\) and \(\bm{W}_{K}\):Recalling that \((\alpha_{+,+}^{(t)},\alpha_{n,+,2}^{(t)},\ldots,\alpha_{n,+,M}^{(t)})=\frac{ \eta}{NM}\sum\limits_{n\in S_{+}}-\ell_{n}^{\prime(t)}\|\bm{\mu}\|_{2}^{2}\bm{ w}_{O}^{\top}\bm{W}_{V}^{(t)\top}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,1}^{(t)})- \bm{\varphi}_{n,1}^{(t)\top}\bm{\varphi}_{n,1}^{(t)})\), where the most complicated part is the matrix \((diag(\bm{\varphi}_{n,1}^{(t)})-\bm{\varphi}_{n,1}^{(t)\top}\bm{\varphi}_{n,1 }^{(t)})\). We observe that the matrix \(diag(\bm{\varphi}_{n,1}^{(t)})-\bm{\varphi}_{n,1}^{(t)\top}\bm{\varphi}_{n,1 }^{(t)}\) has two important properties:

1. The diagonal elements are positive, while the elements on the off-diagonal are negative.
2. The sum of each row and column of this matrix is 0.

Based on properties 1 and 2, we can deduce the following conclusion: as long as \(\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\) is larger enough than \(\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\), we have \(\alpha_{+,+}^{(t)}\geq 0\) and \(\alpha_{n,+,i}^{(t)}\leq 0\), implying that \(\bm{q}_{+}^{(t)}\) prefers to align \(\bm{k}_{+}^{(t)}\) rather than \(\bm{k}_{n,i}^{(t)}\).

In summary, if more _attention_ is paid to signals, \(\bm{W}_{V}\) learns the signals faster than it memorizes the noises; conversely, if \(\bm{W}_{V}\) learns the signals significantly more than it memorizes the noises, more _attention_ will be paid to signals rather than noises. In other words, \(\bm{W}_{Q}\), \(\bm{W}_{K}\) and \(\bm{W}_{V}\) promote each other.

### Three-Stage Decoupling

Our third main challenge is to deal with the complicated relation among the coefficients, e.g., \(\gamma\), \(\rho\), \(\alpha\). Inspired by the two-stage analysis utilized by Cao et al. (2022) to decouple the coefficients in CNN models, we analyze the ViT training process in three stage. In the following, we explain the key steps for proving Theorem 4.1. The proof for Theorem 4.2 is similar and we detail it in the appendix.

Stage 1:The analysis in 5.2 shows that \(\bm{W}_{Q}\), \(\bm{W}_{K}\) and \(\bm{W}_{V}\) can promote each other. But this process of mutual reinforcement requires some conditions, e.g. \(\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\) is sufficiently larger than \(\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\) which does not necessarily hold for Gaussian initialization, complicating the proof. Stage 1 was introduced to solve this problem, and Lemma 5.2 formally describes this stage:

**Lemma 5.2** (V's Beginning of Learning Signals).: _Under the same conditions as Theorem 4.1, there exist \(T_{1}=\frac{10M(3M+1)N}{\eta d_{h}^{\frac{1}{4}}(N\|\bm{\mu}\|_{2}^{2}-60M^{2}C _{p}^{2}\sigma_{d}^{2}\|\bm{w}_{O}\|_{2}^{2}}\) such that the first element of vector \(\bm{X}_{n}\bm{W}_{V}^{(t)}\bm{w}_{O}\) dominate its other elements, that is, \(\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\geq 3M\cdot|\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(t)} \bm{w}_{O}|\) for all \(n\in S_{+}\), \(i\in[M]\backslash\{1\}\) and \(\bm{\mu}_{-}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\leq-3M\cdot|\bm{\xi}_{n,i}^{\top} \bm{W}_{V}^{(t)}\bm{w}_{O}|\) for all \(n\in S_{-}\), \(i\in[M]\backslash\{1\}\)._With Lemma 5.2, we can guarantee the monotonicity of _attention_ on signals for \(t\geq T_{1}\), which allows us to concentrate on analyzing the growth rate of _attention_ in the following stages.

Stage 2:Note that the output of _softmax_ function has an upper bound 1, thus the output of ViT can be bounded by \(\max\{|\bm{\mu}_{\pm}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}|,|\bm{\xi}_{n,i}^{\top} \bm{W}_{V}^{(t)}\bm{w}_{O}|\}\). By Lemma 5.1, it can be proved that there exists \(T_{2}=\Theta\Big{(}\eta^{-1}\|\bm{\mu}\|_{2}^{-2}\|\bm{w}_{O}\|_{2}^{-2}\log(6N ^{2}M^{2}/\delta)^{-1}\Big{)}\) such that: \(\max\{|\bm{\mu}_{\pm}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}|,|\bm{\xi}_{n,i}^{\top} \bm{W}_{V}^{(t)}\bm{w}_{O}|\}=o(1)\), and further get \(1/2-o(1)\leq-\ell_{n}^{\prime(t)}\leq 1/2+o(1)\). Therefore, one can simplify the dynamics of the coefficients (e.g., \(\gamma\), \(\rho\), \(\alpha\)) by plugging the tight bounds of \(-\ell_{n}^{\prime(t)}\). The following lemma provides some bounds for the dynamics of \(\bm{W}_{Q}\), \(\bm{W}_{K}\) and\(\bm{W}_{V}\) in Stage 2.

**Lemma 5.3** (Dynamics of QKV in Stage 2).: _Under Condition 4.1, if \(N\cdot\mathrm{SNR}^{2}=\Omega(1)\), then with probability at least \(1-d^{-1}\), there exist constant \(C\), \(T_{1}=\Theta\Big{(}\eta^{-1}d_{h}^{-1}\|\bm{\mu}\|_{2}^{-2}\|\bm{w}_{O}\|_{2}^ {-2}\Big{)}\) and \(T_{2}=\widetilde{O}\Big{(}\eta^{-1}\|\bm{\mu}\|_{2}^{-2}\|\bm{w}_{O}\|_{2}^{-2} \Big{)}\) such that:_

\[\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\geq\eta C\|\bm{\mu} \|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{1}), \quad\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\geq 3M\cdot|\bm{\xi}_{n,i}^{ \top}\bm{W}_{V}^{(T)}\bm{w}_{O}|,\] \[\bm{\mu}_{-}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\leq-\eta C\|\bm{\mu} \|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{1}),\quad\bm{\mu}_{-}^{\top}\bm{W}_{V}^{ (t)}\bm{w}_{O}\leq-3M\cdot|\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(T)}\bm{w}_{O}|,\]

\[\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle-\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\geq\log\Big{(}\exp\left(\langle\bm{q}_{\pm}^{(T_{1}) },\bm{k}_{\pm}^{(T_{1})}\rangle-\langle\bm{q}_{\pm}^{(T_{1})},\bm{k}_{n,j}^{( T_{1})}\rangle\right)+\frac{\eta^{2}C|\bm{\mu}_{\pm}^{\top}|\bm{w}_{O}|_{2}^{2} \delta_{\pm}^{2}}{N\left(\log(6N^{2}M^{2}/\delta)\right)^{2}}\cdot(t-T_{1})(t -T_{1}-1)\Big{)}\]

\[\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)}\rangle-\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\geq\log\Big{(}\exp\left(\langle\bm{q}_{n,i}^{(T_{1}) },\bm{k}_{\pm}^{(T_{1})}\rangle-\langle\bm{q}_{n,i}^{(T_{1})},\bm{k}_{n,j}^{( T_{1})}\rangle\right)+\frac{\eta^{2}C|\bm{\mu}_{\pm}^{\top}|\bm{w}_{O}|_{2}^{2} \delta_{\pm}^{2}}{N\left(\log(6N^{2}M^{2}/\delta)\right)^{2}}\cdot(t-T_{1})(t -T_{1}-1)\Big{)}\]

_for \(i,j\in[M]\backslash\{1\},n\in[N],t\in[T_{1},T_{2}]\)._

Lemma 5.3 presents the training dynamics of \(\bm{W}_{Q}\), \(\bm{W}_{K}\) and \(\bm{W}_{V}\) under benign overfitting regime in two aspects: _direction_ and _speed_, that is,

* **direction:**\(\bm{W}_{V}\bm{w}_{O}\) prefer to learn the signals rather than memorize the noises; more and more _attention_ is paid to signals, while the _attention_ on noises is decreasing.
* **speed:** The inner products of \(\bm{\mu}_{\pm}\) and \(\bm{W}_{V}\bm{w}_{O}\) grow at a linear rate; the inner products of label-related _vectorized Q & K_ grow at a logarithmic rate.

Stage 3:As the training process going on, the loss begins to converge, and the loss derivatives \(-\ell_{n}^{\prime(t)}\) no longer remain near \(1/2\). Therefore, the increasing rate of the inner products of _vectorize Q & K_ and \(\bm{\mu}_{\pm}^{\top}\bm{W}_{V}\bm{w}_{O}\) begins to diminish. Based on the analysis in Stage 2, the _attention_ is sufficiently sparse at the beginning of Stage 3. In other words, the _attention_ on the signals is nearly 1, while the _attention_ on the noises is nearly 0. According to this sparsity, \(\bm{\mu}_{\pm}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\) will grow much faster than \(\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\) by Lemma 5.1, and Lemma 5.4 provides the lower bound of \(\bm{\mu}_{\pm}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\).

**Lemma 5.4**.: _Under the same conditions as Theorem 4.1, there exists \(T_{3}=\Theta\Big{(}\eta^{-1}\epsilon^{-1}\|\bm{\mu}\|_{2}^{-2}\|\bm{w}_{O}\|_{2} ^{-2}\log(6N^{2}M^{2}/\delta)^{-1}\Big{)}\) such that:_

\[\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\geq\log\Big{(}\exp(\bm{\mu}_{+}^{ \top}\bm{W}_{V}^{(T_{2})}\bm{w}_{O})+\eta C\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2 }^{2}(t-T_{2})\Big{)},\]

\[\bm{\mu}_{-}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\leq-\log\Big{(}\exp(-\bm{\mu}_{-}^{ \top}\bm{W}_{V}^{(T_{2})}\bm{w}_{O})+\eta C\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2 }^{2}(t-T_{2})\Big{)}\]

_for \(t\in[T_{2},T_{3}]\), where \(C\) is a constant, \(T_{2}\) is the last iteration of stage 2._

Convergence:Lemma 5.4 provides logarithmic lower bounds for \(\bm{\mu}_{\pm}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\). Plugging \(t=T_{3}\) into these inequality, we have \(|\bm{\mu}_{\pm}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}|\geq\log\big{(}\Theta(1/ \epsilon)\big{)}\), then as long as \(|\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}|\) is sufficiently small, it can be proved that \(y_{n}f(\bm{X}_{n},\theta(t))\geq\log\big{(}1/\epsilon\big{)}\) for all \(n\in[N]\), thus \(\ell(y_{n}f(\bm{X}_{n},\theta(t)))=\log(1+\exp(-\log(1/\epsilon)))\leq\epsilon\) and \(L_{S}(\theta(t))\leq\epsilon\). The convergence of ViT is accordingly obtained.

Generalization:Consider a new data point \((\bm{X},y)\) generated from the distribution defined in Definition 3.1. Without loss of generality, we suppose that the signal token is \(\bm{\mu}_{+}\) and the label is 1, i.e. \(\bm{X}=(\bm{\mu}_{+},\bm{\xi}_{2},\dots,\bm{\xi}_{M})\), \(y=1\). It is clear that \(\bm{\xi}_{i}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{\mu}_{\pm}\) has a mean zero, which implies that the _attention_ on the signals in the test data may not necessarily be as high as that in the training data. However, the sparsity of _attention_ during training process creates conditions for \(\bm{W}_{V}\bm{w}_{O}\) to utilize the signals to make predictions, facilitating the generalization of the ViT. We provide the lower bound for the output of ViT on the unseen data with high probability as follows:

**Lemma 5.5**.: _Under the same conditions as Theorem 4.1, there exists \(T_{3}=\Theta\Big{(}\eta^{-1}\epsilon^{-1}\|\bm{\mu}\|_{2}^{-2}\|\bm{w}_{O}\|_{2 }^{-2}\log(6N^{2}M^{2}/\delta)^{-1}\Big{)}\), with probability at least \(1-\delta/N^{2}M\),_

\[yf(\bm{X},\theta(T_{3}))\geq\frac{\log(C/\epsilon)}{C^{\prime}}-1,\]

where \(C\) and \(C^{\prime}\) are constants. Lemma 5.5 shows that \(yf(\bm{X},\theta(T_{3}))\) is large with high probability, and through careful calculation, we can provide a small bound for test loss. More details are in Appendix D.4.

## 6 Experimental Verification

We present simulation results on synthetic data and MNIST dataset to verify our theoretical results.

**Synthetic data experiments setting:** We follow Definition 3.1 to generate the training set and test set. Specifically, we set token size \(M=16\) and feature dimension \(d=1024\). Without loss of generality, we set \(\bm{\mu}_{+}=\|\bm{\mu}\|_{2}\cdot[1,0,\dots,0]^{\top}\) and \(\bm{\mu}_{-}=\|\bm{\mu}\|_{2}\cdot[0,1,0,\dots,0]^{\top}\). We generate noise vector \(\bm{\xi}_{2}\) from the Gaussian distribution \(\mathcal{N}(0,\bar{\sigma}_{p}^{2}\bm{I})\), where \(\bar{\sigma}_{p}\) is fixed to 4. Similarly, we generate the other noise vectors \(\bm{\xi}_{i}\) for \(i\in[M]/\{1,2\}\) from the Gaussian distribution \(\mathcal{N}(0,\sigma_{p}^{2}\bm{I})\) where \(\sigma_{p}\) is fixed to 0.2.

We consider a two-layer Transformer defined in Section 3. The dimensions of matrix \(\bm{W}_{Q}\), \(\bm{W}_{K}\) and \(\bm{W}_{V}\) are set as \(d_{h}=d_{v}=512\). The ViT parameters are initialized using PyTorch's default initialization method, and then they are divided by 16 to ensure that the weights are initialized small enough. We train the ViT with full-batch gradient descent and learning rate \(\eta=0.1\), target training loss \(\epsilon=0.01\). We consider different traning sample size \(N\) ranging from 2 to 20, and different signal-to noise-ratio \(\mathrm{SNR}\) ranging from 0.16 to 15.6. We evaluate the test loss with 100 test data points after training loss converges to \(\epsilon\). All experiments are performed on an NVIDIA A100 GPU.

**Synthetic data experiments results:** Figure 0(a) shows that as \(N\) and \(\mathrm{SNR}\) increase, the test loss tends to decrease. As Figure 0(b) shows, the theoretically derived red curve (\(N\cdot\mathrm{SNR}^{2}=1000\)

Figure 1: (a) is a heatmap of test loss on synthetic data across various signal-to-noise ratios (\(\mathrm{SNR}\)) and sample sizes (N). High test losses are indicated with yellow, while low test losses are indicated with purple. (b) is a heatmap that applies a cutoff value 0.2. It categorizes values below 0.2 as 0 (purple), and above 0.2 as 1 (yellow). The expression for the red curves in (a) and (b) is \(N\cdot\mathrm{SNR}^{2}=1000\).

almost follows the experimental dividing line between the yellow area (test loss > 0.2) and the purple area (test loss < 0.2). These experimental results further validate our theoretical results, that is, the sharp condition separation between benign and harmful overfitting, where \(N\cdot\mathrm{SNR}^{2}=\Omega(1)\) is a precise condition for benign overfitting and \(N^{-1}\cdot\mathrm{SNR}^{-2}=\Omega(1)\) is a precise condition for harmful overfitting. More experimental results on the dynamics of QKV can be found in Appendix A.

**MNIST experiments setting:** We add Gaussian random noises to the outer regions of the images with a width of 4. The original images and the noises are multiplied by diferent factors to generate diferent dataset with specifc SNR. For example, in Figure 2 (a), we mutiply the original image by 1/6 and the noises by 5/6, thus SNR=0.2; in Figure 2 (b),we mutiply the original image by 2/3 and the noises by 1/3, thus SNR=2.0. We consider a ViT model that consists of two attention layers, each equipped with four self-attention heads, followed by a MLP with ReLU activation. The training sample size N ranges from 3000 to 8400 and the SNR ranges from 0.2 to 2.0.

**MNIST experiments results:** Figure 2 (c) shows the heatmap of test loss, which shows a transition between benign and harmful overfitting regimes. The larger the sample size N and signal-to-noise ratio SNR, the better the generalization performance.

## 7 Conclusion

This paper studies the training dynamics, convergence, and generalization for a two-layer Transformer in vision. By analyzing the _Vectorized Q & K_ and _scalarized V_ in three-stage decomposition and carefully handling the _softmax_ function, we give the precise increasing rate of the _attention_ and output of the Transformer in the training process, and further analyze its generalization performance. Our theoretical results reveal a sharp condition separation between benign and harmful overfitting. One limitation of our methodology is our proof technique relies heavily on the sparsity of feature strength, e.g., we assume the standard deviation \(\tilde{\sigma}_{p}\) is sufficiently larger than \(\sigma_{p}\), ensuring that more _attention_ is paid to \(\bm{\xi}_{2}\) rather than other tokens. One future direction is to generalize our analysis to study other abilities of Transformers, such as in-context learning, prompt-tuning and time series forecasting.

## Acknowledgement

We thank the reviewers for their constructive comments on both theoretical and experimental aspects. Miao Zhang was partially sponsored by the National Natural Science Foundation of China under Grant 62306084 and U23B2051, and Shenzhen College Stability Support Plan under Grant GXWD20231128102243003.

## References

* Ahn et al. (2024) Ahn, K., Cheng, X., Daneshmand, H., and Sra, S. (2024). Transformers learn to implement preconditioned gradient descent for in-context learning. _Advances in Neural Information Processing Systems_, 36.
* Allen-Zhu and Li (2020) Allen-Zhu, Z. and Li, Y. (2020). Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. _arXiv preprint arXiv:2012.09816_.
* Chen et al. (2017)

Figure 2: MNIST experiments

Bai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. (2024). Transformers as statisticians: Provable in-context learning with in-context algorithm selection. _Advances in neural information processing systems_, 36.
* Bai et al. (2021) Bai, Y., Mei, J., Yuille, A. L., and Xie, C. (2021). Are transformers more robust than cnns? _Advances in neural information processing systems_, 34:26831-26843.
* Bartlett et al. (2020) Bartlett, P. L., Long, P. M., Lugosi, G., and Tsigler, A. (2020). Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_, 117(48):30063-30070.
* Bhojanapalli et al. (2021) Bhojanapalli, S., Chakrabarti, A., Glasner, D., Li, D., Unterthiner, T., and Veit, A. (2021). Understanding robustness of transformers for image classification. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10231-10241.
* Cao et al. (2022) Cao, Y., Chen, Z., Belkin, M., and Gu, Q. (2022). Benign overfitting in two-layer convolutional neural networks. _Advances in neural information processing systems_, 35:25237-25250.
* Caron et al. (2021) Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660.
* Chatterji et al. (2022) Chatterji, N. S., Long, P. M., and Bartlett, P. L. (2022). The interplay between implicit bias and benign overfitting in two-layer linear networks. _Journal of machine learning research_, 23(263):1-48.
* Chen et al. (2021a) Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. (2021a). Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097.
* Chen et al. (2021b) Chen, T., Cheng, Y., Gan, Z., Yuan, L., Zhang, L., and Wang, Z. (2021b). Chasing sparsity in vision transformers: An end-to-end exploration. _Advances in Neural Information Processing Systems_, 34:19974-19988.
* Chen et al. (2022) Chen, W., Huang, W., Du, X., Song, X., Wang, Z., and Zhou, D. (2022). Auto-scaling vision transformers without training. _arXiv preprint arXiv:2202.11921_.
* Chen et al. (2021c) Chen, X., Xie, S., and He, K. (2021c). An empirical study of training self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9640-9649.
* Chen et al. (2024) Chen, Y., Huang, W., Zhou, K., Bian, Y., Han, B., and Cheng, J. (2024). Understanding and improving feature learning for out-of-distribution generalization. _Advances in Neural Information Processing Systems_, 36.
* Child et al. (2019) Child, R., Gray, S., Radford, A., and Sutskever, I. (2019). Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_.
* Deora et al. (2023) Deora, P., Ghaderi, R., Taheri, H., and Thrampoulidis, C. (2023). On the optimization and generalization of multi-head attention. _arXiv preprint arXiv:2310.12680_.
* Devlin et al. (2018) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_.
* Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_.
* Edelman et al. (2022) Edelman, B. L., Goel, S., Kakade, S., and Zhang, C. (2022). Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_, pages 5793-5831. PMLR.
* Frei et al. (2022) Frei, S., Chatterji, N. S., and Bartlett, P. (2022). Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In _Conference on Learning Theory_, pages 2668-2703. PMLR.
* Frei et al. (2021)Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. (2022). What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598.
* Guo et al. (2022) Guo, M.-H., Xu, T.-X., Liu, J.-J., Liu, Z.-N., Jiang, P.-T., Mu, T.-J., Zhang, S.-H., Martin, R. R., Cheng, M.-M., and Hu, S.-M. (2022). Attention mechanisms in computer vision: A survey. _Computational visual media_, 8(3):331-368.
* Hassani et al. (2021) Hassani, A., Walton, S., Shah, N., Abuduweili, A., Li, J., and Shi, H. (2021). Escaping the big data paradigm with compact transformers. _arXiv preprint arXiv:2104.05704_.
* Hastie et al. (2022) Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2022). Surprises in high-dimensional ridgeless least squares interpolation. _Annals of statistics_, 50(2):949.
* He et al. (2022) He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. (2022). Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16000-16009.
* Hu et al. (2020) Hu, Z., Dong, Y., Wang, K., and Sun, Y. (2020). Heterogeneous graph transformer. In _Proceedings of the web conference 2020_, pages 2704-2710.
* Huang et al. (2023a) Huang, W., Cao, Y., Wang, H., Cao, X., and Suzuki, T. (2023a). Graph neural networks provably benefit from structural information: A feature learning perspective. _arXiv preprint arXiv:2306.13926_.
* Huang et al. (2023b) Huang, W., Shi, Y., Cai, Z., and Suzuki, T. (2023b). Understanding convergence and generalization in federated learning through feature learning theory. In _The Twelfth International Conference on Learning Representations_.
* Huang et al. (2023c) Huang, Y., Cheng, Y., and Liang, Y. (2023c). In-context convergence of transformers. _arXiv preprint arXiv:2310.05249_.
* Janner et al. (2021) Janner, M., Li, Q., and Levine, S. (2021). Offline reinforcement learning as one big sequence modeling problem. _Advances in neural information processing systems_, 34:1273-1286.
* Jelassi et al. (2022) Jelassi, S., Sander, M., and Li, Y. (2022). Vision transformers provably learn spatial structure. _Advances in Neural Information Processing Systems_, 35:37822-37836.
* Khan et al. (2022) Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., and Shah, M. (2022). Transformers in vision: A survey. _ACM computing surveys (CSUR)_, 54(10s):1-41.
* Kornowski et al. (2024) Kornowski, G., Yehudai, G., and Shamir, O. (2024). From tempered to benign overfitting in relu neural networks. _Advances in Neural Information Processing Systems_, 36.
* Kou et al. (2023) Kou, Y., Chen, Z., Chen, Y., and Gu, Q. (2023). Benign overfitting in two-layer relu convolutional neural networks. In _International Conference on Machine Learning_, pages 17615-17659. PMLR.
* Li et al. (2023a) Li, H., Wang, M., Liu, S., and Chen, P.-Y. (2023a). A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. _arXiv preprint arXiv:2302.06015_.
* Li et al. (2024) Li, H., Wang, M., Lu, S., Cui, X., and Chen, P.-Y. (2024). Training nonlinear transformers for efficient in-context learning: A theoretical learning and generalization analysis. _arXiv preprint arXiv:2402.15607_.
* Li et al. (2023b) Li, Y., Li, Y., and Risteski, A. (2023b). How do transformers learn topic structure: Towards a mechanistic understanding. In _International Conference on Machine Learning_, pages 19689-19729. PMLR.
* Li et al. (2021) Li, Z., Zhou, Z.-H., and Gretton, A. (2021). Towards an understanding of benign overfitting in neural networks. _arXiv preprint arXiv:2106.03212_.
* Lin et al. (2014) Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. (2014). Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer.
* Li et al. (2020)Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022.
* Meng et al. (2023) Meng, X., Zou, D., and Cao, Y. (2023). Benign overfitting in two-layer relu convolutional neural networks for xor data. _arXiv preprint arXiv:2310.01975_.
* Naseer et al. (2021) Naseer, M. M., Ranasinghe, K., Khan, S. H., Hayat, M., Shahbaz Khan, F., and Yang, M.-H. (2021). Intriguing properties of vision transformers. _Advances in Neural Information Processing Systems_, 34:23296-23308.
* OpenAI (2023) OpenAI (2023). Gpt-4 technical report.
* Oquab et al. (2023) Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al. (2023). Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_.
* Oymak et al. (2023) Oymak, S., Rawat, A. S., Soltanolkotabi, M., and Thrampoulidis, C. (2023). On the role of attention in prompt-tuning. In _International Conference on Machine Learning_, pages 26724-26768. PMLR.
* Paul and Chen (2022) Paul, S. and Chen, P.-Y. (2022). Vision transformers are robust learners. In _Proceedings of the AAAI conference on Artificial Intelligence_, volume 36, pages 2071-2081.
* Robinson et al. (2023) Robinson, B. S., Drenkow, N., Conwell, C., and Bonner, M. (2023). A sparse null code emerges in deep neural networks. In _UniReps: the First Workshop on Unifying Representations in Neural Models_.
* Tian et al. (2023a) Tian, Y., Wang, Y., Chen, B., and Du, S. S. (2023a). Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. _Advances in Neural Information Processing Systems_, 36:71911-71947.
* Tian et al. (2023b) Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. (2023b). Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. _arXiv preprint arXiv:2310.00535_.
* Timor et al. (2023) Timor, N., Vardi, G., and Shamir, O. (2023). Implicit regularization towards rank minimization in relu networks. In _International Conference on Algorithmic Learning Theory_, pages 1429-1459. PMLR.
* Tolstikhin et al. (2021) Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., et al. (2021). Mlp-mixer: An all-mlp architecture for vision. _Advances in neural information processing systems_, 34:24261-24272.
* Touvron et al. (2022) Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-Nouby, A., Grave, E., Izacard, G., Joulin, A., Synnaeve, G., Verbeek, J., et al. (2022). Resmlp: Feedforward networks for image classification with data-efficient training. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(4):5314-5321.
* Touvron et al. (2021) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. (2021). Training data-efficient image transformers & distillation through attention. In _International conference on machine learning_, pages 10347-10357. PMLR.
* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. _Advances in neural information processing systems_, 30.
* Vuckovic et al. (2020) Vuckovic, J., Baratin, A., and Combes, R. T. d. (2020). A mathematical theory of attention. _arXiv preprint arXiv:2007.02876_.
* Wang et al. (2021) Wang, K., Muthukumar, V., and Thrampoulidis, C. (2021). Benign overfitting in multiclass classification: All roads lead to interpolation. _Advances in Neural Information Processing Systems_, 34:24164-24179.
* Wang et al. (2023) Wang, W., Ma, S., Xu, H., Usuyama, N., Ding, J., Poon, H., and Wei, F. (2023). When an image is worth 1,024 x 1,024 words: A case study in computational pathology. _arXiv preprint arXiv:2312.03558_.
* Wang et al. (2021)Wu, D. and Xu, J. (2020). On the optimal weighted \(\ell_{2}\) regularization in overparameterized linear regression. _Advances in Neural Information Processing Systems_, 33:10112-10123.
* Xu et al. (2023) Xu, Z., Wang, Y., Frei, S., Vardi, G., and Hu, W. (2023). Benign overfitting and grokking in relu networks for xor cluster data. _arXiv preprint arXiv:2310.02541_.
* Yun et al. (2019) Yun, S., Jeong, M., Kim, R., Kang, J., and Kim, H. J. (2019). Graph transformer networks. _Advances in neural information processing systems_, 32.
* Zhang et al. (2020) Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. (2020). Why are adaptive methods good for attention models? _Advances in Neural Information Processing Systems_, 33:15383-15393.
* Zhang et al. (2024) Zhang, R., Frei, S., and Bartlett, P. L. (2024). Trained transformers learn linear models in-context. _Journal of Machine Learning Research_, 25(49):1-55.
* Zhou and Ge (2023) Zhou, M. and Ge, R. (2023). Implicit regularization leads to benign overfitting for sparse linear regression. In _International Conference on Machine Learning_, pages 42543-42573. PMLR.
* Zhu et al. (2024) Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., and Wang, X. (2024). Vision mamba: Efficient visual representation learning with bidirectional state space model. _arXiv preprint arXiv:2401.09417_.
* Zou et al. (2021) Zou, D., Wu, J., Braverman, V., Gu, Q., and Kakade, S. (2021). Benign overfitting of constant-stepsize sgd for linear regression. In _Conference on Learning Theory_, pages 4633-4635. PMLR.

**Appendix**

###### Contents

* A More Experimental Results on Training Dynamics
* B Basic Calculation
* B.1 Notations
* B.2 Gradient Calculation
* B.3 Update Rules
* C Concentration Inequalities
* D Benign Overfitting
* D.1 Stage I
* D.2 Stage II
* D.3 Stage III
* D.4 Population Loss
* E Harmful Overfitting
* E.1 Stage I
* E.2 Stage II
* E.3 Stage III
* E.4 Population Loss
* F Complete Calculation Process For Benign Overfitting
* F.1 Calculations for \(\alpha\) and \(\beta\)
* F.2 Update Rules for Inner Products
* F.3 Proof of Lemma D.4
* F.4 Lower Bounds of \(\alpha\) and \(\beta\)
* F.5 Lower Bounds of \(\langle\) q, k \(\rangle\)
* F.6 Upper Bounds of \(\langle\) q, k \(\rangle\)
* F.7 Bounds for the Sum of \(\alpha\) and \(\beta\)
* F.8 Proof of Claim 3
* F.9 Upper Bounds of \(\langle\) q, k \(\rangle\)
* F.10 Bounds for the Sum of \(\alpha\) and \(\beta\)
* F.11 Proof of Claim 7
* F.12 Explanations of Lower Order Terms
* G Takeaways for Practitioners
* H Broader Impacts

[MISSING_PAGE_FAIL:16]

\begin{table}
\begin{tabular}{l l} \hline \hline
**Symbols** & **Definitions** \\ \hline \(\bm{x}_{n,i}\) & the i-th token in the n-th training sample \\  & if \(i\in[M]\backslash\{1\}\), \(\bm{x}_{n,i}=\bm{\xi}_{n,i}\). \\ \hline \(\bm{\varphi}_{n,i}^{(t)}\) & the i-th row of _attention_ for the n-th sample, i.e., \(\bm{\varphi}_{n,i}^{(t)}:=\bm{\varphi}(\bm{x}_{n,i}^{\top}\bm{W}_{Q}^{(t)}\bm {W}_{K}^{(t)\top}\bm{X}_{n}^{\top})\) \\ \hline \(S_{+},S_{-}\) & the training samples with +1 labels and -1 labels, \\  & i.e.,\(S_{+}:=\{n\in[N]:y_{n}=1\}\), \(S_{-}:=\{n\in[N]:y_{n}=-1\}\) \\ \hline \(\bm{q}_{+}^{(t)},\bm{q}_{-}^{(t)},\bm{q}_{n,i}^{(t)}\) & vectorized Q, defined as \(\bm{q}_{+}^{(t)}=\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(t)},\bm{q}_{-}^{(t)}=\bm{\mu }_{-}^{\top}\bm{W}_{Q}^{(t)},\bm{q}_{n,i}^{(t)}=\bm{\xi}_{n,i}^{\top}\bm{W}_{Q}^ {(t)}\) \\ \hline \(\bm{k}_{+}^{(t)},\bm{k}_{-}^{(t)},\bm{k}_{n,i}^{(t)}\) & vectorized K, defined as \(\bm{k}_{+}^{(t)}=\bm{\mu}_{+}^{\top}\bm{W}_{K}^{(t)},\bm{k}_{-}^{(t)}=\bm{\mu }_{-}^{\top}\bm{W}_{K}^{(t)},\bm{k}_{n,i}^{(t)}=\bm{\xi}_{n,i}^{\top}\bm{W}_{K} ^{(t)}\) \\ \hline \(V_{+}^{(t)},V_{-}^{(t)},V_{n,i}^{(t)}\) & scalarized V, defined as \(V_{+}^{(t)}:=\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\), \(V_{-}^{(t)}:=\bm{\mu}_{-}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\), \(V_{n,i}^{(t)}:=\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\) \\ \hline \(\alpha_{\pm,\pm}^{(t)},\alpha_{n,\pm,i}^{(t)}\) & linear combinations coefficients for the dynamics of \(\bm{q}_{+}^{(t)}\) and \(\bm{q}_{-}^{(t)}\), \\  & i.e., \(\bm{q}_{\pm}^{(t+1)}-\bm{q}_{\pm}^{(t)}=\alpha_{\pm,\pm}^{(t)}\bm{k}_{\pm}^{(t) }+\sum_{n\in S_{\pm}}\sum_{i=2}^{M}\alpha_{n,\pm,i}^{(t)}\bm{k}_{n,i}^{(t)}\) \\ \hline \(\alpha_{n,i,\pm}^{(t)},\alpha_{n,i,n^{\prime},i^{\prime}}^{(t)}\) & linear combinations coefficients for the dynamics of \(\bm{q}_{n,i}^{(t)}\), \\  & i.e., \(\bm{q}_{n,i}^{(t+1)}-\bm{q}_{n,i}^{(t)}=\alpha_{n,i,i}^{(t)}\bm{k}_{+}^{(t)}+ \alpha_{n,i,-}^{(t)}\bm{k}_{-}^{(t)}+\sum_{n^{\prime}=1}\sum_{i^{\prime}=2}^{M }\alpha_{n,i,n^{\prime},i^{\prime}}^{(t)}\bm{k}_{n^{\prime},i^{\prime}}^{(t)}\) \\ \hline \(\beta_{\pm,\pm}^{(t)},\beta_{n,\pm,i}^{(t)}\) & linear combinations coefficients for the dynamics of \(\bm{k}_{+}^{(t)}\) and \(\bm{k}_{-}^{(t)}\), \\  & i.e., \(\bm{k}_{\pm}^{(t+1)}-\bm{k}_{\pm}^{(t)}=\beta_{\pm,\pm}^{(t)}\bm{q}_{\pm}^{(t) }+\sum_{n\in S_{\pm}}\sum_{i=2}^{M}\beta_{n,\pm,i}^{(t)}\bm{q}_{n,i}^{(t)}\) \\ \hline \(\beta_{n,i,\pm}^{(t)},\beta_{n,i,n^{\prime},i^{\prime}}^{(t)}\) & linear combinations coefficients for the dynamics of \(\bm{q}_{n,i}^{(t)}\), \\  & i.e., \(\bm{k}_{n,i}^{(t+1)}-\bm{k}_{n,i}^{(t)}=\beta_{n,i,+}^{(t)}\bm{q}_{+}^{(t)}+ \beta_{n,i,-}^{(t)}\bm{q}_{-}^{(t)}+\sum_{n^{\prime}=1}\sum_{i^{\prime}=2}^{M }\beta_{n,i,n^{\prime},i^{\prime}}^{(t)}\bm{q}_{n^{\prime},i^{\prime}}^{(t)}\) \\ \hline \(softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle)\) & a general references to \(\frac{\exp((\langle\bm{q}_{\pm}^{(t)},\bm{k}_{+}^{(t)}\rangle))}{\exp(\langle \bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\frac{M}{k\!\!

### Gradient Calculation

**Gradient of Softmax**. Before computing the gradient of QKV, we solve for the gradient of Softmax function. Suppose there is an input vector \(\bm{a}=(a_{1},a_{2},\dots,a_{m})\), then the softmax function can be defined as follows:

\[softmax(a_{i})=\frac{\exp(a_{i})}{\sum\limits_{j=1}^{m}\exp(a_{j})}\]

Then we calculate the gradient of \(softmax(a_{i})\) with respect to \(a_{i}\) and \(a_{j}\) for \(i,j\in[m],i\neq j\).

\[\begin{split}\frac{\partial\ softmax(a_{i})}{\partial\ a_{i}}& =\frac{\exp(a_{i})\sum\limits_{j=1}^{m}\exp(a_{j})-\exp(a_{i}) \exp(a_{i})}{(\sum\limits_{j=1}^{m}\exp(a_{j}))^{2}}\\ &=\frac{\exp(a_{i})}{\sum\limits_{j=1}^{m}\exp(a_{j})}\Big{(}1- \frac{\exp(a_{i})}{\sum\limits_{j=1}^{m}\exp(a_{j})}\Big{)}\\ &=softmax(a_{i})\cdot\Big{(}1-softmax(a_{i})\Big{)}\end{split}\] (3)

\[\begin{split}\frac{\partial\ softmax(a_{i})}{\partial\ a_{j}}& =-\frac{\exp(a_{i})\exp(a_{j})}{(\sum\limits_{j=1}^{m}\exp(a_{j}) )^{2}}\\ &=softmax(a_{i})\cdot softmax(a_{j})\end{split}\] (4)

(3) and (4) show that the gradient of a Softmax function can be represented as a Jacobi matrix where the elements on the diagonal are \(softmax(a_{i})\cdot\Big{(}1-softmax(a_{i})\Big{)}\) and the elements on the off-diagonal are \(softmax(a_{i})\cdot softmax(a_{j})\). With these properties, the gradient of our attention vector \(\bm{\varphi}_{n,i}\) (\(i\in[M],n\in[N]\)) can be expressed as follows

\[\frac{\partial\bm{\varphi}_{n,i}}{\partial\big{(}\bm{x}_{n,i}^{\top}\bm{W}_{Q} \bm{W}_{K}^{\top}\bm{X}_{n}^{\top}\big{)}}=diag(\bm{\varphi}_{n,i})-\bm{ \varphi}_{n,i}^{\top}\bm{\varphi}_{n,i}\] (5)

**Lemma B.1**.: _the gradients of loss function \(L_{S}(\theta)\) with respect to \(\bm{W}_{Q},\bm{W}_{K}\) and \(\bm{W}_{V}\) are given by_

\[\begin{split}\nabla_{\bm{W}_{Q}}L_{S}(\theta)&= \frac{1}{NM}\sum\limits_{n\in S_{+}}\ell_{n}^{\prime}(\theta)(\bm{\mu}_{+}\bm{ w}_{O}^{\top}\bm{W}_{V}^{\top}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,1})-\bm{ \varphi}_{n,1}^{\top}\bm{\varphi}_{n,1})\\ &+\sum\limits_{i=2}^{M}\bm{\xi}_{n,i}\bm{w}_{O}^{\top}\bm{W}_{V}^{ \top}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{\varphi}_{n,i}^{\top}\bm{ \varphi}_{n,i}))\bm{X}_{n}\bm{W}_{K}\\ &-\frac{1}{NM}\sum\limits_{n\in S_{-}}\ell_{n}^{\prime}(\theta)( \bm{\mu}_{-}\bm{w}_{O}^{\top}\bm{W}_{V}^{\top}\bm{X}_{n}^{\top}(diag(\bm{ \varphi}_{n,1})-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1})\\ &+\sum\limits_{i=2}^{M}\bm{\xi}_{n,i}\bm{w}_{O}^{\top}\bm{W}_{V}^{ \top}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{\varphi}_{n,i}^{\top}\bm{ \varphi}_{n,i}))\bm{X}_{n}\bm{W}_{K},\end{split}\]\[\nabla_{\bm{W}_{K}}L_{S}(\theta) =\frac{1}{NM}\sum_{n\in S_{+}}\ell^{\prime}_{n}(\theta)(\bm{X}_{n}^{ \top}(diag(\bm{\varphi}_{n,1})-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1})\bm{X }_{n}\bm{W}_{V}\bm{w}_{O}\bm{\mu}_{+}^{\top}\] \[+\sum_{i=2}^{M}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{ \varphi}_{n,i}^{\top}\bm{\varphi}_{n,i})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\xi} _{n,i}^{\top})\bm{W}_{Q}\] \[-\frac{1}{NM}\sum_{n\in S_{-}}\ell^{\prime}_{n}(\theta)(\bm{X}_{n }^{\top}(diag(\bm{\varphi}_{n,1})-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1}) \bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\mu}_{-}^{\top}\] \[+\sum_{i=2}^{M}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{ \varphi}_{n,i}^{\top}\bm{\varphi}_{n,i})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\xi} _{n,i}^{\top})\bm{W}_{Q},\]

\[\nabla_{\bm{W}_{V}}L_{S}(\theta)=\frac{1}{NM}\sum_{n=1}^{N}y_{n}\ell^{\prime}_ {n}(\theta)[\bm{w}_{O}\sum_{l=1}^{M}\bm{\varphi}(\bm{x}_{n,l}\bm{W}_{Q}\bm{W}_ {K}^{\top}\bm{X}_{n}^{\top})\bm{X}_{n}]^{\top}\]

Proof of Lemma B.1.: Considering that in the vector \(\bm{x}_{n,i}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{X}_{n}^{\top}\), the \(j\)-th element of \(\bm{x}_{n,i}^{\top}\), i.e. \((\bm{x}_{n,i})_{[j]}\), is only used to multiply \(\bm{W}_{Q[j,:]}\), then we have

\[\frac{\partial(\bm{x}_{n,i}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{X}_{n}^{\top} )}{\partial\bm{W}_{Q[j,:]}}=(\bm{x}_{n,i})_{[j]}\cdot(\bm{X}_{n}\bm{W}_{K})\]

And then we get

\[\nabla_{\bm{W}_{Q[j,:]}}\big{(}\bm{\varphi}(\bm{x}_{n,i}^{\top} \bm{W}_{Q}\bm{W}_{K}^{\top}\bm{X}_{n}^{\top})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O} \big{)}\] \[=(\bm{x}_{n,i})_{[j]}\big{[}\frac{\partial(\bm{\varphi}_{n,i}\bm{ X}_{n}\bm{W}_{V}\bm{w}_{O})}{\partial(\bm{x}_{n,i}^{\top}\bm{W}_{Q}\bm{W}_{K}^{ \top}\bm{X}_{n}^{\top})}\big{]}(\bm{X}_{n}\bm{W}_{K})\]

Considering all \(j\in[d]\), we get

\[\nabla_{\bm{W}_{Q}}\big{(}\bm{\varphi}(\bm{x}_{n,i}^{\top}\bm{W}_ {Q}\bm{W}_{K}^{\top}\bm{X}_{n}^{\top})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\big{)}\] (6) \[=\bm{x}_{n,i}\big{[}\frac{\partial(\bm{\varphi}_{n,i}\bm{X}_{n}\bm {W}_{V}\bm{w}_{O})}{\partial(\bm{x}_{n,i}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{ X}_{n}^{\top})}\big{]}\bm{X}_{n}\bm{W}_{K}\]\[\nabla_{\bm{W}_{Q}}L_{S}(\theta) =\frac{1}{N}\sum_{n=1}^{N}y_{n}\ell^{\prime}(y_{n}f(\bm{X}_{n},\theta ))\nabla_{\bm{W}_{Q}}f(\bm{X}_{n},\theta)\] (7) \[=\frac{1}{NM}\sum_{n=1}^{N}y_{n}\ell^{\prime}_{n}(\theta)\sum_{i= 1}^{M}\nabla_{\bm{W}_{Q}}\big{(}\bm{\varphi}(\bm{x}_{n,i}^{\top}\bm{W}_{Q}\bm{W }_{K}^{\top}\bm{X}_{n}^{\top})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\big{)}\] \[=\frac{1}{NM}\sum_{n=1}^{N}y_{n}\ell^{\prime}_{n}(\theta)\sum_{i= 1}^{M}\bm{x}_{n,i}\big{[}\frac{\partial(\bm{\varphi}_{n,i}\bm{X}_{n}\bm{W}_{V} \bm{w}_{O})}{\partial(\bm{x}_{n,i}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{X}_{n }^{\top})}\big{]}\bm{X}_{n}\bm{W}_{K}\] \[=\frac{1}{NM}\sum_{n=1}^{N}y_{n}\ell^{\prime}_{n}(\theta)\sum_{i= 1}^{M}\bm{x}_{n,i}\big{[}(\bm{X}_{n}\bm{W}_{V}\bm{w}_{O})^{\top}\frac{\partial \bm{\varphi}_{n,i}}{\partial(\bm{x}_{n,i}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm {X}_{n}^{\top})}\big{]}\bm{X}_{n}\bm{W}_{K}\] \[=\frac{1}{NM}\sum_{n=1}^{N}y_{n}\ell^{\prime}_{n}(\theta)\sum_{i= 1}^{M}\bm{x}_{n,i}\bm{w}_{O}^{\top}\bm{W}_{V}^{\top}\bm{X}_{n}^{\top}(diag( \bm{\varphi}_{n,i})-\bm{\varphi}_{n,i}^{\top}\bm{\varphi}_{n,i})\bm{X}_{n}\bm{ W}_{K}\] \[=\frac{1}{NM}\sum_{n\in S_{+}}\ell^{\prime}_{n}(\theta)(\bm{\mu}_ {+}\bm{w}_{O}^{\top}\bm{W}_{V}^{\top}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,1} )-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1})\] \[+\sum_{i=2}^{M}\bm{\xi}_{n,i}\bm{w}_{O}^{\top}\bm{W}_{V}^{\top} \bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{\varphi}_{n,i}^{\top}\bm{ \varphi}_{n,i}))\bm{X}_{n}\bm{W}_{K}\] \[-\frac{1}{NM}\sum_{n\in S_{-}}\ell^{\prime}_{n}(\theta)(\bm{\mu}_ {-}\bm{w}_{O}^{\top}\bm{W}_{V}^{\top}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,1 })-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1})\] \[+\sum_{i=2}^{M}\bm{\xi}_{n,i}\bm{w}_{O}^{\top}\bm{W}_{V}^{\top} \bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{\varphi}_{n,i}^{\top}\bm{ \varphi}_{n,i}))\bm{X}_{n}\bm{W}_{K}\]

where the third equality is by (6), the fourth equality is by chain rule, the fifth equality is by (5), For the last equality, we expand the equality by materializing all the \(\bm{x}_{n,i}\) (e.g., \(\bm{x}_{n,1}=\bm{\mu}_{+}\ for\ n\in S_{+}\)). Using the similar method, we obtain the gradient of \(\bm{W}_{K}\).

\[\nabla_{\bm{W}_{K}}L_{S}(\theta) =\frac{1}{N}\sum_{n=1}^{N}y_{n}\ell^{\prime}(y_{n}f(\bm{X}_{n}, \theta))\nabla_{\bm{W}_{K}}f(\bm{X}_{n},\theta)\] (8) \[=\frac{1}{NM}\sum_{n=1}^{N}y_{n}\ell^{\prime}_{n}(\theta)\sum_{i= 1}^{M}\nabla_{\bm{W}_{K}}\big{(}\bm{\varphi}(\bm{x}_{n,i}^{\top}\bm{W}_{Q}\bm{ W}_{K}^{\top}\bm{X}_{n}^{\top})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\big{)}\] \[=\frac{1}{NM}\sum_{n=1}^{N}y_{n}\ell^{\prime}_{n}(\theta)\sum_{i= 1}^{M}\bm{X}_{n}^{\top}\big{[}\frac{\partial(\bm{\varphi}_{n,i}\bm{X}_{n}\bm{W }_{V}\bm{w}_{O})}{\partial(\bm{x}_{n,i}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{X} _{n}^{\top})}\big{(}\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\big{)}\big{]}\bm{x}_{n,i}^{ \top}\bm{W}_{Q}\] \[=\frac{1}{NM}\sum_{n\in S_{+}}\ell^{\prime}_{n}(\theta)(\bm{X}_{n }^{\top}(diag(\bm{\varphi}_{n,1})-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1}) \bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\mu}_{+}^{\top}\] \[+\sum_{i=2}^{M}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{ \varphi}_{n,i}^{\top}\bm{\varphi}_{n,i})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\xi} _{n,i}^{\top})\bm{W}_{Q}\] \[-\frac{1}{NM}\sum_{n\in S_{-}}\ell^{\prime}_{n}(\theta)(\bm{X}_{n }^{\top}(diag(\bm{\varphi}_{n,1})-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1}) \bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\mu}_{-}^{\top}\] \[+\sum_{i=2}^{M}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{ \varphi}_{n,i}^{\top}\bm{\varphi}_{n,i})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\xi} _{n,i}^{\top})\bm{W}_{Q}\]The gradient of \(\bm{W}_{V}\) can be obtained using the chain rule as follows

\[\begin{split}\nabla_{\bm{W}_{V}}L_{S}(\theta)&=\frac{1} {N}\sum_{n=1}^{N}y_{n}\ell^{\prime}(y_{n}f(\bm{X}_{n},\theta))\nabla_{\bm{W}_{V} }f(\bm{X}_{n},\theta)\\ &=\frac{1}{NM}\sum_{n=1}^{N}y_{n}\ell^{\prime}_{n}(\theta)[\bm{w} _{O}\sum_{l=1}^{M}\bm{\varphi}(\bm{x}_{n,l}\bm{W}_{Q}\bm{W}_{K}^{\top}(\bm{X}_ {n})^{\top})\bm{X}_{n}]^{\top}\end{split}\] (9)

### Update Rules

**Definition B.1** (Scalarized V).: _Let \(\bm{W}_{V}^{(t)}\) be the V matrix of the ViT at the t-th iteration of gradient descent. Then there exist coefficients \(\gamma_{V,+}^{(t)}\), \(\gamma_{V,-}^{(t)}\), \(\rho_{V,n,i}^{(t)}\) such that_

\[\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O} =\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(0)}\bm{w}_{O}+\gamma_{V,+}^{(t)} \|\bm{w}_{O}\|_{2}^{2},\] \[\bm{\mu}_{-}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O} =\bm{\mu}_{-}^{\top}\bm{W}_{V}^{(0)}\bm{w}_{O}+\gamma_{V,-}^{(t)} \|\bm{w}_{O}\|_{2}^{2},\] \[\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O} =\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(0)}\bm{w}_{O}+\rho_{V,n,i}^{(t)} \|\bm{w}_{O}\|_{2}^{2}\]

_for \(i\in[M]\backslash\{1\},n\in[N]\). We further denote \(V_{+}^{(t)}:=\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\), \(V_{-}^{(t)}:=\bm{\mu}_{-}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\) and \(V_{n,i}^{(t)}:=\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}\). We refer to it as scalarized V._

**Definition B.2** (Vectorized Q & K).: _Let \(\bm{W}_{Q}^{(t)}\) and \(\bm{W}_{K}^{(t)}\) be the QK matrices of the ViT at the t-th iteration of gradient descent. Then we define the vectorized Q and vectorized K as follows_

\[\bm{q}_{+}^{(t)}=\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(t)},\qquad\bm{q}_{-}^{(t)}= \bm{\mu}_{-}^{\top}\bm{W}_{Q}^{(t)},\qquad\bm{q}_{n,i}^{(t)}=\bm{\xi}_{n,i}^{ \top}\bm{W}_{Q}^{(t)},\]

\[\bm{k}_{+}^{(t)}=\bm{\mu}_{+}^{\top}\bm{W}_{K}^{(t)},\qquad\bm{k}_{-}^{(t)}= \bm{\mu}_{-}^{\top}\bm{W}_{K}^{(t)},\qquad\bm{k}_{n,i}^{(t)}=\bm{\xi}_{n,i}^{ \top}\bm{W}_{K}^{(t)}\]

_for \(i\in[M]\backslash\{1\},n\in[N]\)._

**Definition B.3** (Gradient Decomposition).: _There exist coefficients \(\alpha_{+,+}^{(t)}\), \(\alpha_{n,+,i^{\prime}}^{(t)}\), \(\alpha_{-,-}^{(t)}\), \(\alpha_{n,-,i^{\prime}}^{(t)}\), \(\alpha_{n,i,+}^{(t)}\), \(\alpha_{n,i,-}^{(t)}\), \(\alpha_{n,i,-}^{(t)}\), \(\alpha_{n,i,-}^{(t)}\), \(\alpha_{n,i,-}^{(t)}\), \(\beta_{n,i,-}^{(t)}\), \(\beta_{n,i,+}^{(t)}\), \(\beta_{n,i,-}^{(t)}\), \(\beta_{n,i,n^{\prime},i^{\prime}}^{(t)}\) such that_

\[\Delta\bm{q}_{+}^{(t)}:=\bm{q}_{+}^{(t+1)}-\bm{q}_{+}^{(t)}=\alpha_{+,+}^{(t)} \bm{k}_{+}^{(t)}+\sum_{n\in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n, i}^{(t)},\]

\[\Delta\bm{q}_{-}^{(t)}:=\bm{q}_{-}^{(t+1)}-\bm{q}_{-}^{(t)}=\alpha_{-,-}^{(t)} \bm{k}_{-}^{(t)}+\sum_{n\in S_{-}}\sum_{i=2}^{M}\alpha_{n,-,i}^{(t)}\bm{k}_{n, i}^{(t)},\]

\[\Delta\bm{q}_{n,i}^{(t)}:=\bm{q}_{n,i}^{(t+1)}-\bm{q}_{n,i}^{(t)}=\alpha_{n,i, +}^{(t)}\bm{k}_{+}^{(t)}+\alpha_{n,i,-}^{(t)}\bm{k}_{-}^{(t)}+\sum_{n^{\prime}= 1}^{N}\sum_{i^{\prime}=2}^{M}\alpha_{n,i,n^{\prime},i^{\prime}}^{(t)}\bm{k}_{n ^{\prime},i^{\prime}}^{(t)},\]

\[\Delta\bm{k}_{+}^{(t)}:=\bm{k}_{+}^{(t+1)}-\bm{k}_{+}^{(t)}=\beta_{+,+}^{(t)} \bm{q}_{+}^{(t)}+\sum_{n\in S_{+}}\sum_{i=2}^{M}\beta_{n,+,i}^{(t)}\bm{q}_{n, i}^{(t)},\]

\[\Delta\bm{k}_{-}^{(t)}:=\bm{k}_{-}^{(t+1)}-\bm{k}_{-}^{(t)}=\beta_{-,-}^{(t)} \bm{q}_{-}^{(t)}+\sum_{n\in S_{-}}\sum_{i=2}^{M}\beta_{n,-,i}^{(t)}\bm{q}_{n, i}^{(t)},\]

\[\Delta\bm{k}_{n,i}^{(t)}:=\bm{k}_{n,i}^{(t+1)}-\bm{k}_{n,i}^{(t)}=\beta_{n,i,+ }^{(t)}\bm{q}_{+}^{(t)}+\beta_{n,i,-}^{(t)}\bm{q}_{-}^{(t)}+\sum_{n^{\prime}= 1}^{N}\sum_{i^{\prime}=2}^{M}\beta_{n,i,n^{\prime},i^{\prime}}^{(t)}\bm{q}_{n^{ \prime},i^{\prime}}^{(t)}\]

_for \(i,i^{\prime}\in[M]\backslash\{1\}\) and \(n,n^{\prime}\in[N]\)._

**Remark.** With the scalarized V in B.1, we can portray the learning process of \(\bm{W}_{V}\) on signals and noises by analyzing the dynamics of \(\gamma^{(t)}_{V,+}\), \(\gamma^{(t)}_{V,-}\) and \(\rho^{(t)}_{V,n,i}\) (or \(V^{+}_{+}\), \(V^{(t)}_{-}\) and \(V^{(t)}_{n,i}\)). Also, as scalars, the effect of \(\bm{W}_{V}\) on the learning process of \(\bm{W}_{Q}\) and \(\bm{W}_{K}\) is more conveniently to describe. With the Vectorized Q & K in B.2, we can decompose \(\bm{x}^{\top}\bm{W}_{Q}\bm{W}_{K}\bm{X}^{\top}\) into the inner product of vectors \(\bm{q}\) (a generalized reference to \(\bm{q}_{+}\), \(\bm{q}_{-}\) and \(\bm{q}_{n,i}\)) and \(\bm{k}\) (a generalized reference to \(\bm{k}_{+}\), \(\bm{k}_{-}\) and \(\bm{k}_{n,i}\)), e.g., \(\langle\bm{q}_{+},\bm{k}_{+}\rangle=\bm{\mu}_{+}^{\top}\bm{W}_{Q}\bm{W}_{K}\bm{ \mu}_{+}\). Then the dynamics of \(\bm{x}^{\top}\bm{W}_{Q}\bm{W}_{K}\bm{X}^{\top}\) can be characterized by analyzing the dynamics of the inner products of \(\bm{q}\) and \(\bm{k}\), and then further get the dynamics of attentions. The gradient decomposition in B.3 comes from (7) and (8). Note that (7) ends in \(\bm{X}_{n}\bm{W}_{K}\) and (8) ends in \(\bm{x}_{n,i}^{\top}\bm{W}_{Q}\). This suggests that the dynamics of \(\bm{q}_{\pm}^{(t)}\) and \(\bm{q}_{n,i}^{(t)}\) can be decomposed into linear combinations of \(\bm{k}_{\pm}^{(t)}\) and \(\bm{k}_{n,i}^{(t)}\), and similarly, the gradient of \(\bm{k}_{\pm}^{(t)}\) and \(\bm{k}_{n,i}^{(t)}\) can be decomposed into linear combinations of \(\bm{q}_{\pm}^{(t)}\) and \(\bm{q}_{n,i}^{(t)}\). We calculate \(\alpha\) (a generalized reference to \(\alpha_{+,+}\), \(\alpha_{+}^{n,i}\), \(\alpha_{-,-}\), etc.) and \(\beta\) (a generalized reference to \(\beta_{+,+}\), \(\beta_{n,+,i}\), \(\beta_{-,-}\), etc.) in detail in Appendix F.

**Lemma B.2** (Update Rule for V).: _The coefficients \(\gamma^{(t)}_{V,+}\), \(\gamma^{(t)}_{V,-}\), \(\rho^{(t)}_{V,n,i}\) defined in Definition B.1 satisfy the following iterative equations:_

\[\gamma^{(t+1)}_{V,+}=\gamma^{(t)}_{V,+}\] \[-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_{n\in S_{+}} \ell^{\prime(t)}_{n}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{k= 2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)}\] \[+\sum\limits_{j=2}^{M}\frac{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k }_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{+}^{(t)}\rangle)+ \sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)}\] \[\gamma^{(t+1)}_{V,-}=\gamma^{(t)}_{V,-}\] \[+\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_{n\in S_{-}}\ell ^{\prime(t)}_{n}\big{(}\frac{\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{-}^{(t)} \rangle)}{\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{-}^{(t)}\rangle)+\sum\limits_{k =2}^{M}\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)}\] \[+\sum\limits_{j=2}^{M}\frac{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k }_{-}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{-}^{(t)}\rangle)+ \sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)} \big{)},\] \[\rho^{(t+1)}_{V,n,i}=\rho^{(t)}_{V,n,i}\] \[-\frac{\eta}{NM}\sum\limits_{n^{\prime}\in S_{+}}\ell^{\prime(t)} _{n^{\prime}}\big{(}\sum\limits_{i^{\prime}=2}^{M}(\langle\bm{\xi}_{n,i},\bm{ \xi}_{n^{\prime},i^{\prime}}\rangle\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{ n^{\prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n^{\prime},k }^{(t)}\rangle)}\] \[+\sum\limits_{j=2}^{M}\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime},i^{\prime}}\rangle\frac{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{n^{ \prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{ n^{\prime},i^{\prime}}\rangle)})\] \[+\frac{\eta}{NM}\sum\limits_{n^{\prime}\in S_{-}}\ell^{\prime(t)} _{n^{\prime}}\big{(}\sum\limits_{i^{\prime}=2}^{M}(\langle\bm{\xi}_{n,i},\bm{\xi} _{n^{\prime},i^{\prime}}\rangle\frac{\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{n^{ \prime},i^{\prime}}^{(t)}\rangle)}{\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{-}^{(t)} \rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{n^{\prime},k }^{(t)}\rangle)}\] \[+\sum\limits_{j=2}^{M}\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime},i^ {\prime}}\rangle\frac{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{n^{\prime},i ^{\prime}}\rangle)}{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{-}^{(t)} \rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{n^{ \prime},k}^{(t)}\rangle)})\big{)}\]

_for \(i\in[M]\backslash\{1\},n\in[N]\)._

Proof of Lemma B.2.: Base on (9), we have

[MISSING_PAGE_EMPTY:23]

where the second equality we expand \(\bm{X}_{n}\) into vectors and make inner products with \(x\), the third equality we materializing all the \(\bm{x}_{n,l}\) (e.g., \(\bm{x}_{n,1}=\bm{\mu}_{+}\) for \(n\in S_{+}\)). Note the orthogonality between \(\bm{\mu}\) and \(\bm{\xi}_{n,i}\), we can remove many of the terms in this equation. Take \(\bm{x}=\bm{\mu}_{+}\) as an example, we have

\[\begin{split}&\bm{\mu}_{+}^{\top}\nabla_{\bm{W}_{V}}L_{S}(\theta) \bm{w}_{O}\\ &=\frac{\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_{n\in S_{+}}\ell_{n} ^{\prime}(\theta)\big{(}\frac{\exp(\bm{\mu}_{+}^{\top}\bm{W}_{Q}\bm{W}_{K}^{ \top}\bm{\mu}_{+})}{\exp(\bm{\mu}_{+}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top}\bm{\mu} _{+})+\sum\limits_{k=2}^{M}\exp(\bm{\mu}_{+}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top} \bm{\xi}_{n,k})}\\ &+\sum\limits_{j=2}^{M}\frac{\exp(\bm{\xi}_{n,j}^{\top}\bm{W}_{Q }\bm{W}_{K}^{\top}\bm{\mu}_{+})}{\exp(\bm{\xi}_{n,j}^{\top}\bm{W}_{Q}\bm{W}_{K }^{\top}\bm{\mu}_{+})+\sum\limits_{k=2}^{M}\exp(\bm{\xi}_{n,j}^{\top}\bm{W}_{Q }\bm{W}_{K}^{\top}\bm{\xi}_{n,k})}\big{)}\|\bm{w}_{O}\|_{2}^{2}\end{split}\] (11)

Then we have

\[\begin{split}&\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t+1)}\bm{w}_{O}- \bm{\mu}_{+}^{\top}\bm{W}_{V}^{(t)}\bm{w}_{O}=\bm{\mu}_{+}^{\top}\big{(}-\eta \nabla_{\bm{W}_{V}}L_{S}(\theta(t))\big{)}\bm{w}_{O}\\ &-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_{n\in S_{+}}\ell _{n}^{\prime(t)}\big{(}\frac{\exp((\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}))}{\exp( (\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}))+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}_ {+}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)}\\ &+\sum\limits_{j=2}^{M}\frac{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{ k}_{+}^{(t)}))}{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum \limits_{k=2}^{M}\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)} \|\bm{w}_{O}\|_{2}^{2}\end{split}\] (12)

Dividing by \(\|\bm{w}_{O}\|_{2}^{2}\) we get

\[\begin{split}&\gamma_{V,+}^{(t+1)}=\gamma_{V,+}^{(t)}\\ &-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_{n\in S_{+}}\ell _{n}^{\prime(t)}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{k =2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)}\\ &+\sum\limits_{j=2}^{M}\frac{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{ k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{+}^{(t)}\rangle)+ \sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{n,k}^{(t)}\rangle) }\big{)}\end{split}\] (13)

This proves the update rule for \(\gamma_{V,+}^{(t)}\), The proof for \(\gamma_{V,+}^{(t)}\) and \(\rho_{V,n,i}^{(t)}\) is similar to it.

**Lemma B.3** (Update Rule for QK).: _The dynamics of \(\bm{x}^{\top}\bm{W}_{Q}\bm{W}_{K}\bm{x}\) can be characterized as follows_

\[\begin{split}&\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)} \rangle-\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\\ &=\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\sum\limits_{n\in S _{+}}\sum\limits_{i=2}^{M}\alpha_{n,+,i}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_ {n,i}^{(t)}\rangle\\ &+\beta_{+,+}^{(t)}\|\bm{q}_{+}^{(t)}\|_{2}^{2}+\sum\limits_{n\in S _{+}}\sum\limits_{i=2}^{M}\beta_{n,+,i}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)}\rangle\\ &+\Big{(}\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}+\sum\limits_{n\in S _{+}}\sum\limits_{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\\ &\cdot\Big{(}\beta_{+,+}^{(t)}\bm{q}_{+}^{(t)\top}+\sum\limits_{n \in S_{+}}\sum\limits_{i=2}^{M}\beta_{n,+,i}^{(t)}\bm{q}_{n,i}^{(t)\top}\Big{)},\end{split}\]\[\langle\bm{q}_{-}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle-\langle\bm{q}_{-}^ {(t)},\bm{k}_{-}^{(t)}\rangle\] \[=\alpha_{-,-}^{(t)}\|\bm{k}_{-}^{(t)}\|_{2}^{2}+\sum_{n\in S_{-}} \sum_{i=2}^{M}\alpha_{n,-,i}^{(t)}\langle\bm{k}_{-}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\] \[+\beta_{-,-}^{(t)}\|\bm{q}_{-}^{(t)}\|_{2}^{2}+\sum_{n\in S_{-}} \sum_{i=2}^{M}\beta_{n,-,i}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{q}_{n,i}^{(t)}\rangle\] \[+\Big{(}\alpha_{-,-}^{(t)},\bm{k}_{-}^{(t)}+\sum_{n\in S_{-}} \sum_{i=2}^{M}\alpha_{n,-,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{-,-}^{(t)}\bm{q}_{-}^{(t)\top}+\sum_{n\in S_{- }}\sum_{i=2}^{M}\beta_{n,-,i}^{(t)}\bm{q}_{n,i}^{(t)\top}\Big{)},\]

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle\bm {q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle\] \[=\alpha_{n,i,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\alpha_{n,i,-} ^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle+\sum_{n^{\prime}=1}^{N} \sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{ n^{\prime},l}^{(t)}\rangle\] \[+\beta_{++,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)} \rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(t)} \langle\bm{q}_{n,i}^{(t)},\bm{q}_{n^{\prime},l}^{(t)}\rangle\] \[+\Big{(}\alpha_{n,i,+}^{(t)}\bm{k}_{+}^{(t)}+\alpha_{n,i,-}^{(t) }\bm{k}_{-}^{(t)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{++,+}^{(t)}\bm{q}_{+}^{(t)\top}+\sum_{n^{ \prime}\in S_{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(t)\top}\bm{q}_{n^{ \prime},l}^{(t)\top}\Big{)},\]

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle-\langle\bm {q}_{n,i}^{(t)},\bm{k}_{-}^{(t)}\rangle\] \[=\alpha_{n,i,-}^{(t)}\|\bm{k}_{-}^{(t)}\|_{2}^{2}+\alpha_{n,i,+} ^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle+\sum_{n^{\prime}=1}^{N} \sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\langle\bm{k}_{-}^{(t)},\bm{k}_{ n^{\prime},l}^{(t)}\rangle\] \[+\beta_{-,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{q}_{n,i}^{(t)} \rangle+\sum_{n^{\prime}\in S_{-}}\sum_{l=2}^{M}\beta_{n^{\prime},-,l}^{(t)} \langle\bm{q}_{n,i}^{(t)},\bm{q}_{n^{\prime},l}^{(t)}\rangle\] \[+\Big{(}\alpha_{n,i,+}^{(t)}\bm{k}_{+}^{(t)}+\alpha_{n,i,-}^{(t) }\bm{k}_{-}^{(t)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{--,-}^{(t)}\bm{q}_{-}^{(t)\top}+\sum_{n^{ \prime}\in S_{-}}\sum_{l=2}^{M}\beta_{n^{\prime},-,l}^{(t)}\bm{q}_{n^{\prime},l}^{(t)\top}\Big{)},\]\[\langle\bm{q}_{+}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle-\langle\bm{q}_{+} ^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] \[=\alpha_{+,+}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{n,j}^{(t)} \rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\alpha_{n^{\prime},+,l}^{(t)} \langle\bm{k}_{n,j}^{(t)},\bm{k}_{n^{\prime},l}^{(t)}\rangle\] \[+\beta_{n,j,+}^{(t)}\|\bm{q}_{+}^{(t)}\|^{2}_{2}+\beta_{n,j,-}^{( t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{-}^{(t)}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^ {M}\beta_{n,j,n^{\prime},l}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n^{\prime},l} ^{(t)}\rangle\] \[+\Big{(}\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n^{\prime}\in S _{+}}\sum_{l=2}^{M}\alpha_{n^{\prime}+,l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)} \Big{)}\] \[\cdot\Big{(}\beta_{n,j,+}^{(t)}\bm{q}_{+}^{(t)\top}+\beta_{n,j,-}^ {(t)\top}\bm{q}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(t)\top}\bm{q}_{n^{\prime},l}^{(t)\top}\Big{)},\]

\[\langle\bm{q}_{-}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle-\langle\bm{ q}_{-}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] \[=\alpha_{-,-}^{(t)}\langle\bm{k}_{-}^{(t)},\bm{k}_{n,j}^{(t)} \rangle+\sum_{n^{\prime}\in S_{-}}\sum_{l=2}^{M}\alpha_{n^{\prime},-,l}^{(t)} \langle\bm{k}_{n,j}^{(t)},\bm{k}_{n^{\prime},l}^{(t)}\rangle\] \[+\beta_{n,j,-}^{(t)}\|\bm{q}_{-}^{(t)}\|^{2}_{2}+\beta_{n,j,+}^{( t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{-}^{(t)}\rangle+\sum_{n^{\prime}=1}^{N} \sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{q}_{n^ {\prime},l}^{(t)}\rangle\] \[+\Big{(}\alpha_{-,-}^{(t)}\bm{k}_{-}^{(t)}+\sum_{n^{\prime}\in S _{-}}\sum_{l=2}^{M}\alpha_{n^{\prime},-,l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)} \Big{)}\] \[\cdot\Big{(}\beta_{n,j,+}^{(t)}\bm{q}_{+}^{(t)\top}+\beta_{n,j,-}^ {(t)\top}\bm{q}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(t)}\bm{q}_{n^{\prime},l}^{(t)\top}\Big{)},\]

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle-\langle \bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] \[=\alpha_{n,i,+}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{n,j}^{(t)} \rangle+\alpha_{n,i,-}^{(t)}\langle\bm{k}_{-}^{(t)},\bm{k}_{n,j}^{(t)}+\sum_{n ^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\langle\bm{k}_{n ^{\prime},l}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] \[+\beta_{n,j,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)} \rangle+\beta_{n,j,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{q}_{n,i}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(t)}\langle\bm{ q}_{n^{\prime},l}^{(t)},\bm{q}_{n,i}^{(t)}\rangle\] \[+\Big{(}\alpha_{n,i,+}^{(t)}\bm{k}_{+}^{(t)}+\alpha_{n,i,-}^{(t)} \bm{k}_{-}^{(t)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^ {(t)}\bm{k}_{n^{\prime},l}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{n,j,+}^{(t)}\bm{q}_{+}^{(t)\top}+\beta_{n,j,- }^{(t)\top}\bm{q}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n, j,n^{\prime},l}^{(t)\top}\bm{q}_{n^{\prime},l}^{(t)\top}\Big{)},\]

_for \(i,j\in[M]\backslash\{1\},n\in[N]\)._

Proof of Lemma b.3.: Based on Definition B.3, we have

\[\Delta\bm{q}_{+}^{(t)}:=\bm{q}_{+}^{(t+1)}-\bm{q}_{+}^{(t)}=\alpha_{ +,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n\in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(t)} \bm{k}_{n,i}^{(t)}\] (14)

\[\Delta\bm{k}_{+}^{(t)}:=\bm{k}_{+}^{(t+1)}-\bm{k}_{+}^{(t)}=\beta_{ +,+}^{(t)}\bm{q}_{+}^{(t)}+\sum_{n\in S_{+}}\sum_{i=2}^{M}\beta_{n,+,i}^{(t)} \bm{q}_{n,i}^{(t)}\] (15)Then we get

\[\begin{split}&\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle- \langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\\ &=\bm{\mu}_{+}^{\top}(\bm{W}_{Q}^{(t)}+\Delta\bm{W}_{Q}^{(t)})( \bm{W}_{K}^{(t)\top}+\Delta\bm{W}_{K}^{(t)\top})\bm{\mu}_{+}-\langle\bm{q}_{+}^ {(t)},\bm{k}_{+}^{(t)}\rangle\\ &=\langle\Delta\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle+\langle \bm{q}_{+}^{(t)},\Delta\bm{k}_{+}^{(t)}\rangle+\langle\Delta\bm{q}_{+}^{(t)}, \Delta\bm{k}_{+}^{(t)}\rangle\\ &=\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\sum_{n\in S_{+} }\sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{n,i}^{(t) }\rangle\\ &+\beta_{+,+}^{(t)}\|\bm{q}_{+}^{(t)}\|_{2}^{2}+\sum_{n\in S_{+} }\sum_{i=2}^{M}\beta_{n,+,i}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)} \rangle\\ &+\left(\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n\in S_{+}}\sum _{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\right)\\ &\cdot\left(\beta_{+,+}^{(t)}\bm{q}_{+}^{(t)\top}+\sum_{n\in S_{+ }}\sum_{i=2}^{M}\beta_{n,+,i}^{(t)\top}\bm{q}_{n,i}^{(t)\top}\right)\end{split}\] (16)

where \(\Delta\bm{W}_{Q}^{(t)}:=\bm{W}_{Q}^{(t+1)}-\bm{W}_{Q}^{(t)}\), \(\Delta\bm{W}_{K}^{(t)}:=\bm{W}_{K}^{(t+1)}-\bm{W}_{K}^{(t)}\), the second equality is by \(\Delta\bm{q}_{+}^{(t)}=\bm{\mu}_{+}^{\top}\Delta\bm{W}_{Q}^{(t)}\) and \(\Delta\bm{k}_{+}^{(t)}=\bm{\mu}_{+}^{\top}\Delta\bm{W}_{K}^{(t)}\), the third equality is by plugging (14) and (15). This proves the update rule for \(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\), the proof for \(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle\), \(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\), \(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\) are exactly the same.

**Remark**. In (16), note that the magnitude of \(\langle\bm{k}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\), \(\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)}\rangle\) is much smaller than that of \(\|\bm{k}_{+}^{(t)}\|_{2}^{2}\), \(\|\bm{q}_{+}^{(t)}\|_{2}^{2}\), and that the magnitude of \(\alpha_{n,+,i}^{(t)},\beta_{n,i}^{(t)}\) will not be greater than that of \(\alpha_{+,+}^{(t)},\beta_{+,+}^{(t)}\) (we will prove it later). Also, since the learning rate \(\eta\) is sufficiently small, the last term of (16) will be very small. Then (16) can be expressed in the following form

\[\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle\bm{q }_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\] \[=\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\beta_{+,+}^{(t)} \|_{2}^{q}+\{lower\;order\;term\}\]

The dynamics of \(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle\), \(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\), \(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\) can be expressed in a similar way. Later we will rigorously explain the so-call {lower order term}.

## Appendix C Concentration Inequalities

In this section, we will give some concentration inequalities that show some important properties of the data and the ViT parameters at random initialization.

**Lemma C.1** (Lemma B.1 in Cao et al. (2022)).: _Suppose that \(\delta>0\) and \(n\geq 8\log(4/\delta)\). Then with probability at least \(1-\delta\),_

\[\frac{N}{4}\leq|\{n\in[N]:y_{n}=1\}|,|\{n\in[N]:y_{n}=-1\}|\leq\frac{3N}{4}.\]

**Lemma C.2** (Initialization of V).: _Suppose that \(\delta>0\). Then with probability at least \(1-\delta\),_

\[|V_{\pm}^{(0)}| \leq d_{h}^{-\frac{1}{4}},\] \[|V_{n,i}^{(0)}| \leq d_{h}^{-\frac{1}{4}}\]

_for \(i\in[M]\backslash\{1\},n\in[N]\)._

Proof of Lemma c.2.: It is clear that \(\bm{\mu}_{\pm}^{\top}\bm{W}_{V}^{(0)}\bm{w}_{O}\) is a random variable with mean zero and variance \(\sigma_{V}^{2}\|\bm{w}_{O}\|_{2}^{2}\|\bm{\mu}\|_{2}^{2}\). Therefore, by Gaussian tail bound and the condition that \(\sigma_{V}\leq\widetilde{O}\big{(}\|\bm{w}_{O}\|_{2}^{-1}\cdot\min\{\|\bm{\mu} \|_{2}^{-1},(\sigma_{p}\sqrt{d})^{-1}\}\cdot d_{h}^{-\frac{1}{4}}\big{)}\), with probability at least \(1-\delta/NM\),

\[|\bm{\mu}_{\pm}^{\top}\bm{W}_{V}^{(0)}\bm{w}_{O}|\leq\sigma_{V}\|\bm{w}_{O}\|_{2} \|\bm{\mu}\|_{2}\sqrt{\log(2NM/\delta)}\leq d_{h}^{-\frac{1}{4}}\]Moreover, each element of vector \(\bm{w}_{O}^{\top}\bm{W}_{V}^{(0)}\) is a random variable with mean zero and variance \(\sigma_{V}^{2}\|\bm{w}_{O}\|_{2}^{2}\). Therefore, by Bernstein's inequality and the condition of \(\sigma_{V}\), with probability at least \(1-\delta/NM\)

\[|\bm{\xi}_{n,i}^{\top}\bm{W}_{V}^{(0)}\bm{w}_{O}|\leq 2\sigma_{V}\sigma_{p}\|\bm{ w}_{O}\|_{2}\sqrt{d\log(2NM/\delta)}\leq d_{h}^{-\frac{1}{4}}\]

Applying a union bound completes the proof.

**Lemma C.3** (Initialization of QK).: _Suppose that \(\delta>0\). Then with probability at least \(1-\delta\),_

\[\frac{\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}}{2} \leq\|\bm{q}_{\pm}^{(0)}\|_{2}^{2}\leq\frac{3\|\bm{\mu}\|_{2}^{2 }\sigma_{h}^{2}d_{h}}{2},\] \[\frac{\tilde{\sigma}_{p}^{2}\sigma_{h}^{2}dd_{h}}{2} \leq\|\bm{q}_{n,2}^{(0)}\|_{2}^{2}\leq\frac{3\sigma_{p}^{2}\sigma _{h}^{2}dd_{h}}{2},\] \[\frac{\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}}{2} \leq\|\bm{q}_{n,i}^{(0)}\|_{2}^{2}\leq\frac{3\sigma_{p}^{2} \sigma_{h}^{2}dd_{h}}{2},\] \[\frac{\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}}{2} \leq\|\bm{k}_{\pm}^{(0)}\|_{2}^{2}\leq\frac{3\|\bm{\mu}\|_{2}^{2 }\sigma_{h}^{2}d_{h}}{2},\] \[\frac{\tilde{\sigma}_{p}^{2}\sigma_{h}^{2}dd_{h}}{2} \leq\|\bm{k}_{n,2}^{(0)}\|_{2}^{2}\leq\frac{3\sigma_{p}^{2}\sigma _{h}^{2}dd_{h}}{2},\] \[\frac{\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}}{2} \leq\|\bm{k}_{n,i}^{(0)}\|_{2}^{2}\leq\frac{3\sigma_{p}^{2}\sigma _{h}^{2}dd_{h}}{2},\] \[|\langle\bm{q}_{+}^{(0)},\bm{q}_{-}^{(0)}\rangle|\leq 2\|\bm{\mu}\|_{ 2}^{2}\sigma_{h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)},\] \[|\langle\bm{q}_{\pm}^{(0)},\bm{q}_{n,i}^{(0)}\rangle|\leq 2\|\bm{ \mu}\|_{2}\tilde{\sigma}_{p}\sigma_{h}^{2}d^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2 }/\delta)},\] \[|\langle\bm{k}_{+}^{(0)},\bm{k}_{-}^{(0)}\rangle|\leq 2\|\bm{\mu}\|_{ 2}^{2}\sigma_{h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)},\] \[|\langle\bm{q}_{+}^{(0)},\bm{k}_{+}^{(0)}\rangle|\leq 2\|\bm{ \mu}\|_{2}^{2}\sigma_{h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)},\] \[|\langle\bm{q}_{+}^{(0)},\bm{k}_{+}^{(0)}\rangle|\leq 2\|\bm{ \mu}\|_{2}^{2}\sigma_{h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)},\] \[|\langle\bm{q}_{+}^{(0)},\bm{k}_{+}^{(0)}\rangle|\leq 2\|\bm{ \mu}\|_{2}^{2}\sigma_{h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)},\] \[|\langle\bm{q}_{+}^{(0)},\bm{k}_{n,i}^{(0)}\rangle|\leq 2\tilde{ \sigma}_{p}\sigma_{h}^{2}d^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)},\] \[|\langle\bm{q}_{n,i}^{(0)},\bm{k}_{n^{\prime},j}^{(0)}\rangle|\leq 2 \tilde{\sigma}_{p}^{2}\sigma_{h}^{2}d\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)},\] \[|\langle\bm{k}_{n,i}^{(0)},\bm{k}_{n^{\prime},j}^{(0)}\rangle|\leq 2 \tilde{\sigma}_{p}^{2}\sigma_{h}^{2}d\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)},\] \[|\langle\bm{k}_{\pm}^{(0)},\bm{k}_{n,i}^{(0)}\rangle|\leq 2\|\bm{ \mu}\|_{2}\tilde{\sigma}_{p}\sigma_{h}^{2}d^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/ \delta)},\] \[|\langle\bm{q}_{\pm}^{(0)},\bm{k}_{n,i}^{(0)}\rangle|\leq 2\|\bm{ \mu}\|_{2}\tilde{\sigma}_{p}\sigma_{h}^{2}d^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/ \delta)},\] \[|\langle\bm{q}_{\pm}^{(0)},\bm{k}_{n,i}^{(0)}\rangle|\leq 2\tilde{ \sigma}_{p}^{2}\sigma_{h}^{2}d\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\] \[|\langle\bm{q}_{n,i}^{(0)},\bm{k}_{n^{\prime},j}^{(0)}\rangle| \leq 2\tilde{\sigma}_{p}^{2}\sigma_{h}^{2}d\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/ \delta)}\]

_for \(i,j\in[M]\backslash\{1\}\), \(i^{\prime}\in[M]\backslash\{1,2\}\) and \(n,n^{\prime}\in[N]\)._

Proof of Lemma c.3.: It is clear that each element of vector \(\bm{q}_{+}^{(0)}\) is a random variable with mean zero and variance \(\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}\). By Bernstein's inequality, with probability at least \(1-\delta/3N^{2}M^{2}\), we have

\[\|\bm{q}_{+}^{(0)}\|_{2}^{2}-\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}|=O(\|\bm{ \mu}\|_{2}^{2}\sigma_{h}^{2}\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}).\]

Therefore, as long as \(d_{h}=\Omega(\log(6N^{2}M^{2}/\delta))\), we have

\[\frac{\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}}{2} \leq\|\bm{q}_{+}^{(0)}\|_{2}^{2}\leq\frac{3\|\bm{\mu}\|_{2}^{2} \sigma_{h}^{2}d_{h}}{2},\]

Similarly, we have

\[\frac{\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}}{2} \leq\|\bm{q}_{-}^{(0)}\|_{2}^{2}\leq\frac{3\|\bm{\mu}\|_{2}^{2} \sigma_{h}^{2}d_{h}}{2},\] \[\frac{\tilde{\sigma}_{p}^{2}\sigma_{h}^{2}dd_{h}}{2} \leq\|\bm{q}_{n,2}^{(0)}\|_{2}^{2}\leq\frac{3\tilde{\sigma_{p}}^{2} \sigma_{h}^{2}dd_{h}}{2},\]\[\frac{\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}}{2}\leq\|\bm{q}_{n,i^{\prime}}^{(0)}\| _{2}^{2}\leq\frac{3\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}}{2},\]

\[\frac{\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}}{2}\leq\|\bm{k}_{\pm}^{(0)}\|_{2} ^{2}\leq\frac{3\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}}{2},\]

\[\frac{\tilde{\sigma}_{p}^{2}\sigma_{h}^{2}dd_{h}}{2}\leq\|\bm{k}_{n,2}^{(0)}\| _{2}^{2}\leq\frac{3\tilde{\sigma_{p}}^{2}\sigma_{h}^{2}dd_{h}}{2},\]

\[\frac{\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}}{2}\leq\|\bm{k}_{n,i^{\prime}}^{(0)} \|_{2}^{2}\leq\frac{3\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}}{2},\]

Moreover, \(\langle\bm{q}_{+}^{(0)},\bm{q}_{-}^{(0)}\rangle\) has mean zero. By Bernstein's inequality, with probability at least \(1-\delta/3N^{2}M^{2}\), we have

\[|\langle\bm{q}_{+}^{(0)},\bm{q}_{-}^{(0)}\rangle|\leq 2\|\bm{\mu}\|_{2}^{2} \sigma_{h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}.\]

We can prove the rest of this lemma in a similar way.

**Lemma C.4** (Lemma B.2 in Cao et al. (2022)).: _Suppose that \(\delta>0\) and d = \(\Omega(\log(4NM/\delta))\). Then with probability at least \(1-\delta\)_

\[\tilde{\sigma}_{p}^{2}d/2\leq\|\bm{\xi}_{n,2}\|_{2}^{2}\leq 3\tilde{ \sigma}_{p}^{2}d/2,\]

\[\sigma_{p}^{2}d/2\leq\|\bm{\xi}_{n,i}\|_{2}^{2}\leq 3\sigma_{p}^{2}d/2,\]

\[|\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime},i^{\prime}}\rangle|\leq 2\tilde{ \sigma}_{p}^{2}\cdot\sqrt{d\log(4N^{2}M^{2}/\delta)}\]

_for \(i,i^{\prime}\in[M]\backslash\{1\},n,n^{\prime}\in[N],i\neq i^{\prime}\ or\ n\neq n^{\prime}\)._

## Appendix D Benign Overfitting

In this section, we consider the benign overfitting regime under the condition that \(N\cdot\mathrm{SNR}^{2}=\Omega(1)\). We analyze the dynamics of \(V_{\pm}\), \(V_{n,i}\), the inner product of \(\bm{q}_{\pm}\), \(\bm{q}_{n,i}\) and \(\bm{k}_{\pm}\), \(\bm{k}_{n,i}\) during gradient descent training, and further give the upper bound for population loss. The proofs in this section are based on the results in Section C, which hold with high probability.

### Stage I

In Stage I, \(V_{\pm}^{(t)}\), \(V_{n,i}^{(t)}\) begin to pull apart until \(|V_{\pm}^{(t)}|\) is sufficiently larger than \(|V_{n,i}^{(t)}|\). At the same time, the inner products of \(\bm{q}\) and \(\bm{k}\) maintain their magnitude.

**Lemma D.1** (Gradient of Loss).: _As long as \(\max\{|V_{+}^{(t)}|,|V_{-}^{(t)}|,|V_{n,i}^{(t)}|\}=o(1)\), we have \(-\ell^{\prime}(y_{n}f(\bm{X}_{n},\theta(t)))\) remains \(1/2\pm o(1)\)._

Proof of Lemma d.1.: Note that \(\ell(z)=\log(1+\exp(-z))\) and \(-\ell^{\prime}=\exp(-z)/(1+\exp(-z))\), without loss of generality, we assume \(y_{n}=1\), we have

\[-\ell^{\prime}(f(\bm{X}_{n},\theta(t)))=\frac{1}{1+\exp(\frac{1}{M}\sum\limits _{l=1}^{M}\bm{\varphi}(\bm{x}_{n,l}^{\top}\bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top} \bm{X}_{n}^{\top})\bm{X}_{n}\bm{W}_{V}^{(t)}\bm{w}_{O})}.\]

Note that

\[-\max\{|V_{+}^{(t)}|,|V_{-}^{(t)}|,|V_{n,i}^{(t)}|\}\leq\frac{1}{M}\sum\limits _{l=1}^{M}\bm{\varphi}(\bm{x}_{n,l}^{\top}\bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top }\bm{X}_{n}^{\top})\bm{X}_{n}\bm{W}_{V}^{(t)}\bm{w}_{O}\leq\max\{|V_{+}^{(t)}|,|V_{-}^{(t)}|,|V_{n,i}^{(t)}|\}.\]

Then we have

\[-\ell^{\prime}(f(\bm{X}_{n},\theta(t)))\geq\frac{1}{1+\exp(0+o(1))}\geq\frac{ 1}{2+o(1)}\geq\frac{1}{2}-o(1),\]

\[-\ell^{\prime}(f(\bm{X}_{n},\theta(t)))\leq\frac{\exp(0+o(1))}{1+\exp(0+o(1))} \leq\frac{1+o(1)}{1+1+o(1)}\leq\frac{1}{2}+o(1).\]

**Lemma D.2** (Bound of Attention).: _As long as \(|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)}, \bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle|=o(1)\), we have_

\[\frac{1}{M}-o(1) \leq softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle) \leq\frac{1}{M}+o(1),\] \[\frac{1}{M}-o(1) \leq softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)}\rangle) \leq\frac{1}{M}+o(1),\] \[\frac{1}{M}-o(1) \leq softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{(t)}\rangle) \leq\frac{1}{M}+o(1),\] \[\frac{1}{M}-o(1) \leq softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle) \leq\frac{1}{M}+o(1).\]

Proof of Lemma D.2.: It is clear that \(\exp(o(1))=1+o(1)\). Therefore, as long as \(|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|=o(1)\), we have

\[\frac{1}{M}-o(1)=\frac{1}{1+(M-1)+(M-1)o(1)}=\frac{1}{1+(M-1)\exp (o(1))}=\] \[\frac{\exp(-o(1))}{\exp(-o(1))+(M-1)\exp(o(1))}\leq softmax( \langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle)\leq\frac{\exp(o(1))}{ \exp(o(1))+(M-1)\exp(-o(1))}\] \[=\frac{\exp(o(1))}{\exp(o(1))+(M-1)}=\frac{1+o(1)}{1+o(1)+(M-1)}= \frac{1}{M}+o(1)\]

Similarly, we have

\[\frac{1}{M}-o(1) \leq softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)}\rangle) \leq\frac{1}{M}+o(1),\] \[\frac{1}{M}-o(1) \leq softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{(t)}\rangle) \leq\frac{1}{M}+o(1),\] \[\frac{1}{M}-o(1) \leq softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle) \leq\frac{1}{M}+o(1).\]

**Lemma D.3** (Upper bound of V).: _Let \(T_{0}=O(\frac{1}{\eta d_{h}^{\frac{1}{4}}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{0}\|_{2} ^{2}})\). Then under the same conditions as Theorem 4.1, we have_

\[|V_{+}^{(t)}|,|V_{-}^{(t)}|,|V_{n,i}^{(t)}|=O(d_{h}^{-\frac{1}{4}})\]

_for \(t\in[0,T_{0}]\)._

Proof of Lemma D.3.: By Lemma B.2, we have

\[|\gamma_{V,+}^{(t+1)}-\gamma_{V,+}^{(t)}|\] \[\leq-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_{n\in S_{+}} \ell_{n}^{\prime(t)}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{k =2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)}\] \[+\sum\limits_{j=2}^{M}\frac{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{ k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{+}^{(t)}\rangle)+ \sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)}\] \[\leq\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\cdot\frac{3N}{4}\big{(}1+( M-1)\big{)}\] \[\leq\frac{3\eta\|\bm{\mu}\|_{2}^{2}}{4},\]

where the second inequality is by Lemma C.1 and \(-\ell_{n}^{\prime(t)}\leq 1\). Similarly, we have

\[|\gamma_{V,-}^{(t+1)}-\gamma_{V,-}^{(t)}|\leq\frac{3\eta\|\bm{\mu}\|_{2}^{2}}{ 4}.\]By Definition B.1, we have

\[\begin{split}|V_{+}^{(t)}|&=\big{|}V_{+}^{(0)}+\sum_{s= 0}^{t-1}(\gamma_{V,+}^{(s+1)}-\gamma_{V,+}^{(s)})\|\bm{w}_{O}\|_{2}^{2}\big{|}\\ &\leq|V_{+}^{(0)}|+\sum_{s=0}^{t-1}|\gamma_{V,+}^{(s+1)}-\gamma_{ V,+}^{(s)}|\cdot\|\bm{w}_{O}\|_{2}^{2}\\ &\leq d_{h}^{-\frac{1}{4}}+\frac{3\eta\|\bm{\mu}\|_{2}^{2}}{4} \cdot\|\bm{w}_{O}\|_{2}^{2}\cdot O(\frac{1}{\eta d_{h}^{\frac{1}{4}}\|\bm{\mu} \|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}})\\ &=O(d_{h}^{-\frac{1}{4}}),\end{split}\]

where the first inequality is by triangle inequality, the second inequality is by Lemma C.2. Similarly, we have \(|V_{-}^{(t)}|=O(d_{h}^{-\frac{1}{4}})\).

By Lemma B.2, we have

\[\begin{split}&|\rho_{V,n,i}^{(t+1)}-\rho_{V,n,i}^{(t)}|\\ &\leq\Big{|}-\frac{\eta}{NM}\sum_{n^{\prime}\in S_{+}}\ell_{n^{ \prime}}^{\prime(t)}\big{(}\sum_{i=2}^{M}(\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{ \prime},i^{\prime}}\rangle\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n^{\prime },i^{\prime}}^{(t)}\rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n^{\prime },k}^{(t)}\rangle)}\\ &+\sum_{j=2}^{M}\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime},i^{ \prime}}\rangle\frac{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{n^{\prime },i^{\prime}}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{+} ^{(t)}\rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm {k}_{n^{\prime},k}^{(t)}\rangle)})\big{)}\\ &+\frac{\eta}{NM}\sum_{n^{\prime}\in S_{-}}\ell_{n^{\prime}}^{ \prime(t)}\big{(}\sum_{i=2}^{M}(\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime},i^ {\prime}}\rangle\frac{\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{n^{\prime},i^{ \prime}}^{(t)}\rangle)}{\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{-}^{(t)}\rangle)+ \sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{n^{\prime},k}^{(t)} \rangle)}\\ &+\sum_{j=2}^{M}\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime},i^{ \prime}}\rangle\frac{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{n^{\prime },i^{\prime}}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{- }^{(t)}\rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n^{\prime},j}^{(t)}, \bm{k}_{n^{\prime},k}^{(t)}\rangle)})\Big{|}\\ &\leq\frac{3\eta\tilde{\sigma}_{p}^{2}d}{2NM}\cdot M+\frac{\eta}{ NM}\cdot MN\cdot 2\tilde{\sigma}_{p}^{2}\cdot\sqrt{d\log(4N^{2}M^{2}/\delta)}\\ &\leq\frac{2\eta\tilde{\sigma}_{p}^{2}d}{N}\\ &=O(\eta\|\bm{\mu}\|_{2}^{2})\end{split}\] (17)

where the second inequality is by Lemma C.4 and \(-\ell_{n}^{\prime(t)}\leq 1\), the third inequality is by \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\geq 4N\sqrt{\log(4N^{2}M^{2}/ \delta)}\), the last inequality is by \(N\cdot\mathrm{SNR}^{2}=\Omega(1)\). Then by Definition B.1, we have

\[\begin{split}|V_{n,i}^{(t)}|&=\big{|}V_{n,i}^{(0)}+ \sum_{s=0}^{t-1}(\rho_{V,n,i}^{(s+1)}-\rho_{V,n,i}^{(s)})\|\bm{w}_{O}\|_{2}^{2} \big{|}\\ &\leq|V_{n,i}^{(0)}|+\sum_{s=0}^{t-1}|\rho_{V,n,i}^{(s+1)}-\rho_{ V,n,i}^{(s)}|\cdot\|\bm{w}_{O}\|_{2}^{2}\\ &\leq d_{h}^{-\frac{1}{4}}+O(\eta\|\bm{\mu}\|_{2}^{2})\cdot\|\bm{ w}_{O}\|_{2}^{2}\cdot O(\frac{1}{\eta d_{h}^{\frac{1}{4}}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}}) \\ &=O(d^{-\frac{1}{4}}),\end{split}\]

where the first inequality is by triangle inequality, the second inequality is by Lemma C.2, which completes the proof.

**Lemma D.4** (Inner Products Hold Magnitude).: _Let \(T_{0}=O(\frac{1}{\eta d_{h}^{\frac{1}{4}}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{0}\|_{2}^{ 2}})\). Then under the same conditions as Theorem 4.1, we have_

\[|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{ q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{ (t)}|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_{ h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[|\langle\bm{q}_{\pm}^{(t)},\bm{q}_{\mp}^{(t)}\rangle|,|\langle\bm {q}_{n,i}^{(t)},\bm{q}_{\pm}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{q}_ {n,j}^{(t)}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma _{h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[|\langle\bm{k}_{\pm}^{(t)},\bm{k}_{\mp}^{(t)}\rangle|,|\langle\bm {k}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{k}_{n,i}^{(t)},\bm{k}_ {n,i^{\prime}}^{(t)}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma _{h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[\|\bm{q}_{\pm}^{(t)}\|_{2}^{2},\|\bm{k}_{\pm}^{(t)}\|_{2}^{2}=\Theta(\|\bm{ \mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}),\]

\[\|\bm{q}_{n,i}^{(t)}\|_{2}^{2},\|\bm{k}_{n,i}^{(t)}\|_{2}^{2}=\Theta(\sigma_{ p}^{2}\sigma_{h}^{2}dd_{h})\]

_for \(i,j\in[M]\backslash\{1\}\), \(n,n^{\prime}\in[N]\) and \(t\in[0,T_{0}]\)._

The proof for Lemma D.4 is in Section F.3. Note that \(\sigma_{h}^{2}\leq\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d _{h}^{-\frac{1}{2}}\cdot\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{-\frac{3}{2}}\), thus \(O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_{h}^{2}\cdot \sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)}=o(1)\).

**Lemma D.5** (V's Beginning of Learning Signals).: _Under the same conditions as Theorem 4.1, there exist \(T_{1}=\frac{10M(3M+1)N}{\eta d_{h}^{\frac{1}{4}}(N\|\bm{\mu}\|_{2}^{2}-60M^{2} C_{p}^{2}\sigma_{p}^{2}d)\|\bm{w}_{0}\|_{2}^{2}}\) such that the first element of vector \(\bm{X}_{n}\bm{W}_{V}^{(t)}\bm{w}_{O}\) dominate its other elements, that is, \(V_{+}^{(t)}\geq 3M\cdot|V_{n,i}^{(t)}|\) for all \(n\in S_{+}\), \(i\in[M]\backslash\{1\}\) and \(V_{-}^{(t)}\leq-3M\cdot|V_{n,i}^{(t)}|\) for all \(n\in S_{-}\), \(i\in[M]\backslash\{1\}\)._

Proof of Lemma D.5.: Let \(C\) be a constant larger than \(10M(3M+1)\), then as long as \(N\cdot\mathrm{SNR}^{2}=\frac{N\|\bm{\mu}\|_{2}^{2}}{\sigma_{p}^{2}d}\geq\frac{ 60CM^{2}C_{p}^{2}}{C-10M(3M+1)}\), we have \(N\|\bm{\mu}\|_{2}^{2}-60M^{2}C_{p}^{2}\sigma_{p}^{2}d\geq\frac{10M(3M+1)N\|\bm{ \mu}\|_{2}^{2}}{C}\), and further get

\[T_{1}=\frac{10M(3M+1)N}{\eta d_{h}^{\frac{1}{4}}(N\|\bm{\mu}\|_{2}^{2}-60M^{2} C_{p}^{2}\sigma_{p}^{2}d)\|\bm{w}_{O}\|_{2}^{2}}\leq\frac{C}{\eta d_{h}^{\frac{1}{4 }}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}}=O\Big{(}\frac{1}{\eta d_{h}^{ \frac{1}{4}}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}}\Big{)},\]

which satisfies the time condition in Lemma D.3 and Lemma D.4. Then by Lemma D.1 and Lemma D.2 we have

\[-\ell_{n}^{\prime(t)}=\frac{1}{2}\pm o(1),\]

\[\frac{1}{M}-o(1)\leq softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)} \rangle)\leq\frac{1}{M}+o(1),\]

\[\frac{1}{M}-o(1)\leq softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)} \rangle)\leq\frac{1}{M}+o(1),\]

\[\frac{1}{M}-o(1)\leq softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{(t)} \rangle)\leq\frac{1}{M}+o(1),\]

\[\frac{1}{M}-o(1)\leq softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)} \rangle)\leq\frac{1}{M}+o(1)\]for \(i,j\in[M]\backslash\{1\}\), \(n\in[N]\) and \(t\in[0,T_{1}]\). Plugging them in the update rule for \(\gamma_{V,+}^{(t)}\) showed in Lemma B.2 and we have

\[\gamma_{V,+}^{(t+1)}-\gamma_{V,+}^{(t)}\] (18) \[=-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum_{n\in S_{+}}\ell_{n}^{ \prime(t)}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)}{ \exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{k=2}^{M} \exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)}\] \[+\sum\limits_{j=2}^{M}\frac{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k} _{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{+}^{(t)}\rangle)+ \sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{n,k}^{(t)} \rangle)}\] \[\geq\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\cdot\frac{N}{4}\cdot( \frac{1}{2}\pm o(1))\cdot M(\frac{1}{M}\pm o(1))\] \[\geq\frac{\eta\|\bm{\mu}\|_{2}^{2}}{10M}.\]

Then by Definition B.1 and taking a summation, we have

\[V_{+}^{(T_{1})} \geq-|V_{+}^{(0)}|+T_{1}\frac{\eta\|\bm{\mu}\|_{2}^{2}}{10M}\| \bm{w}_{O}\|_{2}^{2}\] (19) \[=-d_{h}^{-\frac{1}{4}}+\frac{10M(3M+1)N}{\eta d_{h}^{\frac{1}{4} }(N\|\bm{\mu}\|_{2}^{2}-60M^{2}C_{p}^{2}\sigma_{p}^{2}d)\|\bm{w}_{O}\|_{2}^{2} }\frac{\eta\|\bm{\mu}\|_{2}^{2}}{10M}\|\bm{w}_{O}\|_{2}^{2}\] \[=-d_{h}^{-\frac{1}{4}}+\frac{(3M+1)N\|\bm{\mu}\|_{2}^{2}}{d_{h}^{ \frac{1}{4}}(N\|\bm{\mu}\|_{2}^{2}-60M^{2}C_{p}^{2}\sigma_{p}^{2}d)}.\]

Similarly, we have

\[V_{-}^{(T_{1})}\leq d_{h}^{-\frac{1}{4}}-\frac{(3M+1)N\|\bm{\mu}\|_{2}^{2}}{d _{h}^{\frac{1}{4}}(N\|\bm{\mu}\|_{2}^{2}-60M^{2}C_{p}^{2}\sigma_{p}^{2}d)}.\] (20)

Similarly, by \(|\rho_{V,n,i}^{(t+1)}-\rho_{V,n,i}^{(t)}|\leq\frac{2\eta\tilde{\sigma}_{p}^{2} d}{N}\) in (17), we have

\[|V_{n,i}^{(T_{1})}| \leq|V_{n,i}^{(0)}|+T_{1}\frac{2\eta\tilde{\sigma}_{p}^{2}d}{N}\| \bm{w}_{O}\|_{2}^{2}\] (21) \[=d_{h}^{-\frac{1}{4}}+\frac{10M(3M+1)N}{\eta d_{h}^{\frac{1}{4}}( N\|\bm{\mu}\|_{2}^{2}-60M^{2}C_{p}^{2}\sigma_{p}^{2}d)\|\bm{w}_{O}\|_{2}^{2}} \frac{2\eta\tilde{\sigma}_{p}^{2}d}{N}\|\bm{w}_{O}\|_{2}^{2}\] \[=d_{h}^{-\frac{1}{4}}+\frac{20MC_{p}^{2}(3M+1)\sigma_{p}^{2}d}{d _{h}^{\frac{1}{4}}(N\|\bm{\mu}\|_{2}^{2}-60M^{2}C_{p}^{2}\sigma_{p}^{2}d)}.\]

According to (19), (20) and (21), it is easy to verify that \(V_{+}^{(T_{1})}-3M\cdot|V_{n,i}^{(T_{1})}|\geq 0\) and \(V_{-}^{(T_{1})}+3M\cdot|V_{n,i}^{(T_{1})}|\leq 0\), which completes the proof.

### Stage II

In stage II, \(\langle\bm{q}_{+},\bm{k}_{+}\rangle\), \(\langle\bm{q}_{n,i},\bm{k}_{+}\rangle\) grows while \(\langle\bm{q}_{+},\bm{k}_{n,j}\rangle\), \(\langle\bm{q}_{n,i},\bm{k}_{n,j}\rangle\) decreases, resulting in attention focusing more and more on the signals and less on the noises. By the results of stage I, we have the following conditions at the beginning of stage II

\[V_{+}^{(T_{1})}\geq 3M\cdot|V_{n,i}^{(T_{1})}|,\] \[V_{-}^{(T_{1})}\leq-3M\cdot|V_{n,i}^{(T_{1})}|,\] \[|V_{+}^{(T_{1})}|,|V_{-}^{(T_{1})}|,|V_{n,i}^{(T_{1})}|=O(d_{h}^{- \frac{1}{4}}),\]\[|\langle\bm{q}_{\pm}^{(T_{1})},\bm{k}_{\pm}^{(T_{1})}\rangle|,| \langle\bm{q}_{n,i}^{(T_{1})},\bm{k}_{\pm}^{(T_{1})}\rangle|,|\langle\bm{q}_{ \pm}^{(T_{1})},\bm{k}_{n,j}^{(T_{1})}\rangle|,|\langle\bm{q}_{n,i}^{(T_{1})}, \bm{k}_{n^{\prime},j}^{(T_{1})}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_{ h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[|\langle\bm{q}_{\pm}^{(T_{1})},\bm{k}_{\mp}^{(T_{1})}\rangle|,| \langle\bm{q}_{n,i}^{(T_{1})},\bm{q}_{\pm}^{(T_{1})}\rangle|,|\langle\bm{q}_{ n,i}^{(T_{1})},\bm{q}_{n^{\prime},j}^{(T_{1})}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_ {h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[|\langle\bm{k}_{\pm}^{(T_{1})},\bm{k}_{\mp}^{(T_{1})}\rangle|,| \langle\bm{k}_{n,i}^{(T_{1})},\bm{k}_{\pm}^{(T_{1})}\rangle|,|\langle\bm{k}_{ n,i}^{(T_{1})},\bm{k}_{n^{\prime},j}^{(T_{1})}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_ {h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[\|\bm{q}_{\pm}^{(T_{1})}\|_{2}^{2},\|\bm{k}_{\pm}^{(T_{1})}\|_{2}^{2}=\Theta( \|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}),\]

\[\|\bm{q}_{n,i}^{(T_{1})}\|_{2}^{2},\|\bm{k}_{n,i}^{(T_{1})}\|_{2}^{2}=\Theta( \sigma_{p}^{2}\sigma_{h}^{2}dd_{h})\]

for \(i,j\in[M]\backslash\{1\}\), \(n,n^{\prime}\in[N]\).

Some of the proofs at this stage are based on the above conditions.

**Notations.** To better characterize the gap between different inner products, we define the following notations:

* denote \(\Lambda_{n,+,j}^{(t)}=\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle- \langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle,\quad n\in S_{+}\).
* denote \(\Lambda_{n,-,j}^{(t)}=\langle\bm{q}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle- \langle\bm{q}_{-}^{(t)},\bm{k}_{n,j}^{(t)}\rangle,\quad n\in S_{-}\).
* denote \(\Lambda_{n,i,+,j}^{(t)}=\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle- \langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle,\quad n\in S_{+}\).
* denote \(\Lambda_{n,i,-,j}^{(t)}=\langle\bm{q}_{n,i}^{(t)},\bm{k}_{-}^{(t)}\rangle- \langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle,\quad n\in S_{-}\).

**Lemma D.6** (Upper bound of V).: _Let \(T_{0}=O\Big{(}\frac{1}{\eta\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}\log(6N^{2 }M^{2}/\delta)}\Big{)}\). Then under the same conditions as Theorem 4.1, we have_

\[|V_{+}^{(t)}|,|V_{-}^{(t)}|,|V_{n,i}^{(t)}|=o(1)\]

_for \(t\in[0,T_{0}]\)._

The proof of Lemma D.6 is similar to that of Lemma D.3, except that the time \(T_{0}\) is changed.

Let \(T_{2}=\Theta\Big{(}\frac{1}{\eta\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}\log (6N^{2}M^{2}/\delta)}\Big{)}\), then by Lemma D.6 and Lemma D.1 we have \(\frac{1}{2}-o(1)\leq-\ell_{n}^{\prime(t)}\leq\frac{1}{2}+o(1)\) for \(n\in[N],t\in[T_{1},T_{2}]\), which can simplify the calculations of \(\alpha\) and \(\beta\) defined in Definition B.3 by replacing \(-\ell_{n}^{\prime(t)}\) by their bounds. Next we prove the following four propositions \(\mathcal{B}(t)\), \(\mathcal{C}(t)\), \(\mathcal{D}(t)\), \(\mathcal{E}(t)\) by induction on t for \(t\in[T_{1},T_{2}]\):

* \(\mathcal{B}(t):\) \[V_{+}^{(t)}\geq\eta C_{3}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{1})\] \[V_{-}^{(t)}\leq-\eta C_{3}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{1})\] \[V_{+}^{(t)}\geq 3M\cdot|V_{n,i}^{(t)}|,\] \[V_{-}^{(t)}\leq-3M\cdot|V_{n,i}^{(t)}|,\] \[|V_{\pm}^{(t)}|\leq O(d_{h}^{-\frac{1}{4}})+\eta C_{4}\|\bm{\mu}\|_{2}^{2}\| \bm{w}_{O}\|_{2}^{2}(t-T_{1})\] \[|V_{n,i}^{(t)}|\leq O(d_{h}^{-\frac{1}{4}})+\eta C_{4}\|\bm{\mu}\|_{2}^{2} \|\bm{w}_{O}\|_{2}^{2}(t-T_{1})\] for \(i\in[M]\backslash\{1\},n\in[N]\).

* \(\mathcal{C}(t):\) \[\|\bm{q}_{\pm}^{(t)}\|_{2}^{2},\|\bm{k}_{\pm}^{(t)}\|_{2}^{2}=\Theta\Big{(}\|\bm{ \mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}\Big{)},\] \[\|\bm{q}_{n,i}^{(t)}\|_{2}^{2},\|\bm{k}_{n,i}^{(t)}\|_{2}^{2}=\Theta\Big{(} \sigma_{p}^{2}\sigma_{h}^{2}dd_{h}\Big{)},\] \[|\langle\bm{q}_{+}^{(t)},\bm{q}_{-}^{(t)}\rangle|,|\langle\bm{q}_{\pm}^{(t)},\bm{q}_{n,i}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{q}_{n^{\prime},j}^{ (t)}\rangle|=o(1),\] \[|\langle\bm{k}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle|,|\langle\bm{k}_{\pm}^{(t)},\bm{k}_{n,i}^{(t)}\rangle|,|\langle\bm{k}_{n,i}^{(t)},\bm{k}_{n^{\prime},j}^{ (t)}\rangle|=o(1),\] for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],i\neq j\) or \(n\neq n^{\prime}\).
* \(\mathcal{D}(t):\) \[\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{\pm}^{(t+1)}\rangle\geq\langle\bm{q}_{\pm }^{(t)},\bm{k}_{\pm}^{(t)}\rangle\] \[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{\pm}^{(t+1)}\rangle\geq\langle\bm{q}_{n,i }^{(t)},\bm{k}_{\pm}^{(t)}\rangle\] \[\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle\leq\langle\bm{q}_{\pm }^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] \[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle\leq\langle\bm{q}_{n,i }^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] \[\Lambda_{n,\pm,j}^{(t+1)}\geq\log\Big{(}\exp(\Lambda_{n,\pm,j}^{(T_{1})})+ \frac{\eta^{2}C_{8}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{0}\|_{2}^{2}d_{h}^{\frac{1}{ 2}}}{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}+1) \Big{)}\] \[\Lambda_{n,i,\pm,j}^{(t+1)}\geq\log\Big{(}\exp(\Lambda_{n,i,\pm,j}^{(T_{1})}) +\frac{\eta^{2}C_{8}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{0}\|_{2}^{2}d _{h}^{\frac{1}{2}}}{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1}) (t-T_{1}+1)\Big{)}\] for \(i,j\in[M]\backslash\{1\},n,\overline{n}\in[N]\).
* \(\mathcal{E}(t):\) \[|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{q}_{\pm}^{( t)},\bm{k}_{n,j}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)} \rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle|\leq\log(d_{h}^ {\frac{1}{2}})\] \[|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\mp}^{(t)}\rangle|,|\langle\bm{q}_{n,i }^{(t)},\bm{k}_{\mp,j}^{(t)}\rangle|=o(1)\] for \(i,j\in[M]\backslash\{1\},n,\overline{n}\in[N],n\neq\overline{n}\).

By the results of Stage I, we know that \(\mathcal{B}(T_{1}),\mathcal{C}(T_{1}),\mathcal{E}(T_{1})\) are true. To prove that \(\mathcal{B}(t),\mathcal{C}(t),\mathcal{D}(t)\) and \(\mathcal{E}(t)\) are true in stage 2, we will prove the following claims holds for \(t\in[T_{1},T_{2}]\):

**Claim 1**.: \(\mathcal{D}(T_{1}),\ldots,\mathcal{D}(t-1)\Longrightarrow\mathcal{B}(t+1)\)__

**Claim 2**.: \(\mathcal{B}(T_{1}),\ldots,\mathcal{B}(t),\mathcal{C}(T_{1}),\ldots,\mathcal{C} (t),\mathcal{D}(T_{1}),\ldots,\mathcal{D}(t-1)\Longrightarrow\mathcal{D}(t)\)__

**Claim 3**.: \(\mathcal{B}(T_{1}),\ldots,\mathcal{B}(t),\mathcal{D}(T_{1}),\ldots,\mathcal{D} (t-1),\mathcal{E}(T_{1}),\ldots,\mathcal{E}(t)\Longrightarrow\mathcal{C}(t+1)\)__

**Claim 4**.: \(\mathcal{B}(T_{1}),\ldots,\mathcal{B}(t),\mathcal{C}(T_{1}),\ldots,\mathcal{C} (t),\mathcal{D}(T_{1}),\ldots,\mathcal{D}(t-1)\Longrightarrow\mathcal{E}(t+1)\)__

#### d.2.1 Proof of Claim 1

By the results of Stage I, we have

\[|\langle\bm{q}_{\pm}^{(T_{1})},\bm{k}_{\pm}^{(T_{1})}\rangle|,|\langle\bm{q}_{ \pm}^{(T_{1})},\bm{k}_{n,j}^{(T_{1})}\rangle|,|\langle\bm{q}_{n,i}^{(T_{1})}, \bm{k}_{\pm}^{(T_{1})}\rangle|,|\langle\bm{q}_{n,i}^{(T_{1})},\bm{k}_{n,j}^{(T_ {1})}\rangle|=o(1)\]

Assume that \(\mathcal{D}(T_{1}),\ldots,\mathcal{D}(t-1)\) (\(t\in[T_{1},T_{2}]\)) are true, then \(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle\), \(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{\pm}^{(s)}\rangle\) are monotonically non-decreasing and \(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\), \(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\) are monotonically non-increasing for \(s\in[T_{1},t-1]\), so we have

\[\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle,\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{\pm}^{(s)}\rangle\geq-o(1),\]

\[\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j}^{(s)}\rangle,\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{n,j}^{(s)}\rangle\leq o(1),\]for \(s\in[T_{1},t]\). Further we have the lower bounds for the attention on signal \(\bm{\mu}_{\pm}\) as follows for \(s\in[T_{1},t]\):

\[softmax(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle) \geq\frac{\exp(-o(1))}{\exp(-o(1))+(M-1)\exp(o(1))}\] (22) \[=\frac{1}{1+(M-1)\exp(o(1))}\] \[=\frac{1}{1+(M-1)+(M-1)o(1)}\] \[=\frac{1}{M}-o(1),\]

where the second equality is by \(\exp(o(1))=1+o(1)\). Similarly, we have \(softmax(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{\pm}^{(s)}\rangle)\geq\frac{1}{M}-o(1)\). Plugging them in the update rule for \(\gamma_{V,+}^{(s)}\) showed in Lemma B.2 and we have

\[\gamma_{V,+}^{(s+1)}-\gamma_{V,+}^{(s)}\] (23) \[=-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_{n\in S_{+}} \ell_{n}^{\prime(t)}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s) }\rangle)}{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{ k=2}^{M}\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,k}^{(s)}\rangle)}\] \[+\sum\limits_{j=2}^{M}\frac{\exp(\langle\bm{q}_{n,j}^{(s)},\bm{k} _{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,j}^{(s)},\bm{k}_{+}^{(s)}\rangle)+ \sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n,j}^{(s)},\bm{k}_{n,k}^{(s)}\rangle)}\] \[=\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\cdot\frac{N}{4}\cdot(\frac{ 1}{2}-o(1))\cdot M(\frac{1}{M}-o(1))\] \[\geq\frac{\eta\|\bm{\mu}\|_{2}^{2}}{10M},\]

for \(s\in[T_{1},t]\). Then by Definition B.1 and taking a summation, we have

\[V_{+}^{(t+1)} \geq V_{+}^{(T_{1})}+(t-T_{1}+1)\frac{\eta\|\bm{\mu}\|_{2}^{2}}{ 10M}\|\bm{w}_{O}\|_{2}^{2}\] (24) \[\geq\eta C_{3}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{1}+1),\]

where the last inequality is by \(V_{+}^{(T_{1})}\geq 0\) and \(M=\Theta(1)\). Similarly, we have

\[V_{-}^{(t+1)} \leq-\eta C_{3}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{1}+1).\]

By \(|\rho_{V,n,i}^{(t+1)}-\rho_{V,n,i}^{(t)}|\leq\frac{2\eta\tilde{\sigma}_{p}^{2} d}{N}\) in (17) and taking a summation, we have

\[|V_{n,i}^{(t+1)}|\leq|V_{n,i}^{(T_{1})}|+(t-T_{1}+1)\frac{2\eta\tilde{\sigma}_ {p}^{2}d}{N}\|\bm{w}_{O}\|_{2}^{2}\leq|V_{n,i}^{(T_{1})}|+(t-T_{1}+1)\frac{2 \eta C_{p}^{2}\sigma_{p}^{2}d}{N}\|\bm{w}_{O}\|_{2}^{2}.\] (25)

Combining (24) and (25) we have

\[V_{+}^{(t+1)}-3M\cdot|V_{n,i}^{(t+1)}|\] (26) \[\geq V_{+}^{(T_{1})}+(t-T_{1}+1)\frac{\eta\|\bm{\mu}\|_{2}^{2}}{1 0M}\|\bm{w}_{O}\|_{2}^{2}-3M\cdot\big{(}|V_{n,i}^{(T_{1})}|+(t-T_{1}+1)\frac{2 \eta C_{p}^{2}\sigma_{p}^{2}d}{N}\|\bm{w}_{O}\|_{2}^{2}\big{)}\] \[\geq V_{+}^{(T_{1})}-3M\cdot|V_{n,i}^{(T_{1})}|+(t-T_{1}+1)\frac{ \eta\big{(}N\|\bm{\mu}\|_{2}^{2}-60M^{2}C_{p}^{2}\sigma_{p}^{2}d\big{)}}{10NM}\] \[\geq 0,\]

where the last inequality is by \(V_{+}^{(T_{1})}\geq 3M\cdot|V_{n,i}^{(T_{1})}|\) and requires \(N\cdot\mathrm{SNR}^{2}\geq 60M^{2}C_{p}^{2}\). The proof for \(V_{-}^{(t+1)}\leq-3M\cdot|V_{n,i}^{(t+1)}|\) is the same.

Next, we prove the upper bound for \(V_{\pm}\) and \(V_{n,i}\). Based on the upper bound of attention(<1) and \(-\ell_{n}^{\prime}\leq 1\), we have

\[\gamma_{V,+}^{(s+1)} \leq\gamma_{V,+}^{(s)}-\frac{\eta}{NM}\sum_{n\in S_{+}}\ell_{n}^{ \prime}(\theta(s))(\|\bm{\mu}\|_{2}^{2}+\sum_{j=2}^{M}\|\bm{\mu}\|_{2}^{2})\] (27) \[\leq\gamma_{V,+}^{(s)}+\frac{3\eta\|\bm{\mu}\|_{2}^{2}}{4}\] \[\leq\gamma_{V,+}^{(s)}+\eta C_{4}\|\bm{\mu}\|_{2}^{2}\]

Then we can get that

\[|V_{+}^{(t+1)}| \leq V_{+}^{(T_{1})}+(\gamma_{V,+}^{(t+1)}-\gamma_{V,+}^{(T_{1})} )\|\bm{w}_{O}\|_{2}^{2}\] (28) \[\leq V_{+}^{(T_{1})}+\sum_{s=T_{1}}^{t}\eta C_{4}\|\bm{\mu}\|_{2} ^{2}\|\bm{w}_{O}\|_{2}^{2}\] \[\leq O(d_{h}^{-\frac{1}{4}})+\eta C_{4}\|\bm{\mu}\|_{2}^{2}\|\bm{ w}_{O}\|_{2}^{2}(t-T_{1}+1)\]

where the first inequality is by the monotonicity of \(\gamma_{V,+}\) and the definition of \(V_{+}\), the last inequality is by the result of stage 1 where \(V_{+}^{(T_{1})}=O(d^{-1})\). Similarly, we have

\[|V_{-}^{(t+1)}|\leq O(d_{h}^{-\frac{1}{4}})+\eta C_{4}\|\bm{\mu}\|_{2}^{2}\|\bm {w}_{O}\|_{2}^{2}(t-T_{1}+1)\] (29)

which completes the proof for the upper bound of \(V_{\pm}\).

Expanding (25) yields

\[|V_{n,i}^{(t+1)}| \leq|V_{n,i}^{(T_{1})}|+\frac{2\eta C_{p}^{2}\sigma_{p}^{2}d}{N} \cdot\|\bm{w}_{O}\|_{2}^{2}(t-T_{1}+1)\] (30) \[\leq O(d_{h}^{-\frac{1}{4}})+\eta C_{4}\|\bm{\mu}\|_{2}^{2}\|\bm{ w}_{O}\|_{2}^{2}(t-T_{1}+1)\]

where the last inequality is by the result of phase 1 where \(|V_{n,i}^{(T_{1})}|=O(d_{h}^{-\frac{1}{4}})\) and the condition that \(N\cdot\mathrm{SNR}^{2}>\Omega(1)\).

#### d.2.2 Proof of Claim 2

By the results of F.5, we have the dynamic of \(\langle\bm{q},\bm{k}\rangle\) as follows

\[\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle-\langle\bm{q} _{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\] (31) \[\geq\frac{\eta^{2}C_{6}\|\bm{\mu}\|_{2}^{6}\|\bm{w}_{O}\|_{2}^{2} \sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,+,j}^{(s)})},\]

\[\langle\bm{q}_{-}^{(s+1)},\bm{k}_{-}^{(s+1)}\rangle-\langle\bm{q} _{-}^{(s)},\bm{k}_{-}^{(s)}\rangle\] (32) \[\geq\frac{\eta^{2}C_{6}\|\bm{\mu}\|_{2}^{6}\|\bm{w}_{O}\|_{2}^{2} \sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,-,j}^{(s)})},\]

\[\langle\bm{q}_{+}^{(s+1)},\bm{k}_{n,j}^{(s+1)}\rangle-\langle\bm{q} _{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\] (33) \[\leq-\frac{\eta^{2}C_{6}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{4}\|\bm{ w}_{O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,+,j}^{(s)})},\]

\[\langle\bm{q}_{-}^{(s+1)},\bm{k}_{n,j}^{(s+1)}\rangle-\langle\bm{ q}_{-}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\] (34) \[\leq-\frac{\eta^{2}C_{6}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{4}\|\bm{ w}_{O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,-,j}^{(s)})},\]\[\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle-\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle\] (35) \[\geq\frac{\eta^{2}C_{6}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{4}\|\bm{w}_ {O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,i,+, j}^{(s)})},\] \[\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{-}^{(s+1)}\rangle-\langle\bm {q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle\] \[\geq\frac{\eta^{2}C_{6}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{4}\|\bm{w }_{O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,i,-,j}^{(s)})},\] \[\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{n,j}^{(s+1)}\rangle-\langle \bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\] \[\leq-\frac{\eta^{2}C_{6}\sigma_{p}^{4}d^{2}\|\bm{\mu}\|_{2}^{2}\| \bm{w}_{O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_ {n,i,\pm,j}^{(s)})}\] (37)

for \(s\in[T_{1},t]\). The seven equations above show that \(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle\), \(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{\pm}^{(s)}\rangle\) are monotonically increasing and \(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\), \(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\) are monotonically decreasing. Next, we provide the logarithmic increasing lower bounds of \(\Lambda_{n,\pm,j}^{(s+1)}\) and \(\Lambda_{n,i,\pm,j}^{(s+1)}\).

By (31) and (33), we have

\[\Lambda_{n,+,j}^{(s+1)}-\Lambda_{n,+,j}^{(s)} =(\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle-\langle \bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)-(\langle\bm{q}_{+}^{(s+1)},\bm{k}_{ n,j}^{(s+1)}\rangle-\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)\] (38) \[\geq\frac{\eta^{2}C_{6}\|\bm{\mu}\|_{2}^{6}\|\bm{w}_{O}\|_{2}^{2} \sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,+,j}^{(s)})}\] \[+\frac{\eta^{2}C_{6}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{ O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,+,j}^{(s)})}\] \[\geq\frac{\eta^{2}C_{7}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d \}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N} \cdot\frac{1}{\exp(\Lambda_{n,+,j}^{(s)})}\] \[\geq\frac{\eta^{2}C_{7}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2} d_{h}^{\frac{3}{2}}(s-T_{1})}{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot \frac{1}{\exp(\Lambda_{n,+,j}^{(s)})},\]

where the last inequality is by \(\sigma_{h}^{2}\geq\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}d_{h}^{- \frac{1}{2}}(\log(6N^{2}M^{2}/\delta))^{-2}\). Multiply both sides simultaneously by \(\exp(\Lambda_{n,+,j}^{(s)})\) and get

\[\exp(\Lambda_{n,+,j}^{(s)})\Big{(}\Lambda_{n,+,j}^{(s+1)}-\Lambda_{n,+,j}^{(s) }\Big{)}\geq\frac{\eta^{2}C_{7}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h}^ {\frac{1}{2}}(s-T_{1})}{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}.\] (39)

Taking a summation from \(T_{1}\) to \(t\) and get

\[\sum_{s=T_{1}}^{t}\exp(\Lambda_{n,+,j}^{(s)})\Big{(}\Lambda_{n,+,j} ^{(s+1)}-\Lambda_{n,+,j}^{(s)}\Big{)}\] (40) \[\geq\sum_{s=T_{1}}^{t}\frac{\eta^{2}C_{7}\|\bm{\mu}\|_{2}^{4}\| \bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}(s-T_{1})}{N\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{2}}\] \[\geq\frac{\eta^{2}C_{8}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2} d_{h}^{\frac{1}{2}}}{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1} +1).\]

By the property that \(\Lambda_{n,+,j}^{(t)}\) is monotonically increasing, we have

\[\int_{\Lambda_{n,+,j}^{(T_{1})}}^{\Lambda_{n,+,j}^{(t+1)}}\exp(x)dx \geq\sum_{s=T_{1}}^{t}\exp(\Lambda_{n,+,j}^{(s)})\Big{(}\Lambda_{n,+,j}^{(s+1)} -\Lambda_{n,+,j}^{(s)}\Big{)}\] (41) \[\geq\frac{\eta^{2}C_{8}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_ {h}^{\frac{1}{2}}}{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_ {1}+1).\]By \(\int_{\Lambda_{n,i,j}^{(T_{1})}}^{\Lambda_{n,i+1}^{(t+1)}}\exp(x)dx=\exp(\Lambda_{n,+,j}^{(t+1)})-\exp(\Lambda_{n,+,j}^{(T_{1})})\) we get

\[\Lambda_{n,+,j}^{(t+1)}\geq\log\Big{(}\exp(\Lambda_{n,+,j}^{(T_{1})})+\frac{ \eta^{2}C_{8}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}+1)\Big{)}.\] (42)

Similarly, we have

\[\Lambda_{n,-,j}^{(t+1)}\geq\log\Big{(}\exp(\Lambda_{n,-,j}^{(T_{1})})+\frac{ \eta^{2}C_{8}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}+1)\Big{)}.\] (43)

By (35) and (37), we have

\[\Lambda_{n,i,+,j}^{(s+1)}-\Lambda_{n,i,+,j}^{(s)} =(\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle-\langle \bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)-(\langle\bm{q}_{n,i}^{(s+1)},\bm{k }_{n,j}^{(s+1)}\rangle-\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)\] \[\geq\frac{\eta^{2}C_{6}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{4}\|\bm{w }_{O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,i, +,j}^{(s)})}\] \[+\frac{\eta^{2}C_{6}\sigma_{p}^{4}d^{2}\|\bm{\mu}\|_{2}^{2}\|\bm{ w}_{O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,i, +,j}^{(s)})}\] \[\geq\frac{\eta^{2}C_{7}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d \}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}\sigma_{h}^{2}d_{h}( s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,i,+,j}^{(s)})}\] \[\geq\frac{\eta^{2}C_{7}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2}\|\bm{ w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}(s-T_{1})}{N\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{2}}\cdot\frac{1}{\exp(\Lambda_{n,i,+,j}^{(s)})}.\] (44)

Then using the similar method as for \(\Lambda_{n,\pm,j}^{(t)}\), we get

\[\Lambda_{n,i,\pm,j}^{(t+1)}\geq\log\Big{(}\exp(\Lambda_{n,i,\pm,j}^{(T_{1})})+ \frac{\eta^{2}C_{8}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}d_{ h}^{\frac{1}{2}}}{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}+ 1)\Big{)},\] (45)

which complete the proof. The proof for Claim 3 is in Section F.8

#### d.2.3 Proof of Claim 4

By the results of F.6, we have

\[\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle-\langle\bm{q}_{+}^{(s)}, \bm{k}_{+}^{(s)}\rangle\leq\frac{\eta C_{10}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d _{h}}{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)}\] (46)

for \(s\in[T_{1},t]\). Further we have

\[\exp(\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle) \leq\exp\Big{(}\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle+ \frac{\eta C_{10}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_{h}}{\exp(\langle\bm{q}_{+ }^{(s)},\bm{k}_{+}^{(s)}\rangle)}\Big{)}\] \[=\exp\Big{(}\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\Big{)} \cdot\exp\Big{(}\frac{\eta C_{10}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_{h}}{\exp( \langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)}\Big{)}\] (47) \[\leq C_{11}\exp\Big{(}\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)} \rangle\Big{)}.\]

For the last inequality, by \(\eta\leq\widetilde{O}(\min\{\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d _{h}^{-\frac{1}{2}})\), \(\sigma_{h}^{2}\leq\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}d_{h}^{- \frac{1}{2}}(\log(6N^{2}M^{2}/\delta))^{-\frac{3}{2}}\), \(\langle\bm{q}_{+}^{(T_{1})},\bm{k}_{+}^{(T_{1})}\rangle=o(1)\) and the monotonicity of \(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\) for \(s\in[T_{1},t]\), we have \(\exp\Big{(}\frac{\eta C_{10}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_{h}}{\exp( \langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)}\Big{)}\leq\exp(o(1))\leq C_{ 11}\). Multiplying both sides by \(\Big{(}\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle-\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\Big{)}\) simultaneously gives

\[\begin{split}&\exp(\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)} \rangle)\Big{(}\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle-\langle\bm {q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\Big{)}\\ &\leq C_{11}\exp\Big{(}\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)} \rangle\Big{)}\cdot\Big{(}\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle- \langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\Big{)}\\ &\leq\eta C_{12}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_{h},\end{split}\] (48)

where the last inequality is by plugging (46). Taking a summation we have

\[\begin{split}&\int_{\langle\bm{q}_{+}^{(T_{1})},\bm{k}_{+}^{(t+ 1)}\rangle}^{\langle\bm{q}_{+}^{(T_{1})},\bm{k}_{+}^{(T_{1})}\rangle}\exp(x) dx\\ &\leq\sum_{s=T_{1}}^{t}\exp(\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+ }^{(s+1)}\rangle)\Big{(}\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle- \langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\Big{)}\\ &\leq\sum_{s=T_{1}}^{t}\eta C_{12}\|\bm{\mu}\|_{2}^{4}\sigma_{h} ^{2}d_{h}\\ &\leq T_{2}\cdot\eta C_{12}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_{h }\\ &\leq\frac{d_{h}^{3}}{\log(6N^{2}M^{2}/\delta)^{2}},\end{split}\] (49)

where the first inequality is due to \(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\) is monotone increasing, the last inequality is by \(T_{2}=\Theta(\eta^{-1}\|\bm{\mu}\|_{2}^{-2}\|\bm{w}_{O}\|_{2}^{-2}\log(6N^{2}M ^{2}/\delta)^{-1})\) and \(\sigma_{h}^{2}\leq\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}d_{h}^{- \frac{1}{2}}(\log(6N^{2}M^{2}/\delta))^{-\frac{3}{2}}\). By \(\int_{\langle\bm{q}_{+}^{(T_{1})},\bm{k}_{+}^{(T_{1})}\rangle}^{\langle\bm{q} _{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle}\exp(x)dx=\exp(\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle)-\exp(\langle\bm{q}_{+}^{(T_{1})},\bm{k}_{+}^{(T_{1 })}\rangle)\), we have

\[\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle\leq\log\Big{(}\langle\bm{ q}_{+}^{(T_{1})},\bm{k}_{+}^{(T_{1})}\rangle+\frac{d_{h}^{\frac{1}{2}}}{\log(6N^{2}M ^{2}/\delta)}\Big{)}\leq\log\Big{(}d_{h}^{\frac{1}{2}}\Big{)},\] (50)

By the results of F.6, we also have

\[\langle\bm{q}_{-}^{(s+1)},\bm{k}_{-}^{(s+1)}\rangle-\langle\bm{q}_{-}^{(s)}, \bm{k}_{-}^{(s)}\rangle\leq\frac{\eta C_{10}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2} d_{h}}{\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{-}^{(s)}\rangle)}.\] (51)

\[\langle\bm{q}_{\pm}^{(s+1)},\bm{k}_{n,j}^{(s+1)}\rangle-\langle\bm{q}_{\pm}^{( s)},\bm{k}_{n,j}^{(s)}\rangle\geq-\frac{\eta C_{10}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2} \sigma_{h}^{2}d_{h}}{N}\cdot\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j}^{(s) }\rangle).\] (52)

\[\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{\pm}^{(s+1)}\rangle-\langle\bm{q}_{n,i}^{( s)},\bm{k}_{\pm}^{(s)}\rangle\leq\frac{\eta C_{10}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2} \sigma_{h}^{2}d_{h}}{N\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{\pm}^{(s)}\rangle)}.\] (53)

\[\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{n,j}^{(s+1)}\rangle-\langle\bm{q}_{n,i}^{( s)},\bm{k}_{n,j}^{(s)}\rangle\geq-\frac{\eta C_{10}\sigma_{p}^{4}d^{2}\sigma_{h}^{2} d_{h}}{N}\cdot\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle).\] (54)

Then using the similar method as for \(\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle\), we get

\[\langle\bm{q}_{-}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle\leq\log\Big{(}d_{h}^{ \frac{1}{2}}\Big{)},\]

\[\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle\geq-\log\Big{(}d_{h}^{ \frac{1}{2}}\Big{)},\]\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{\pm}^{(t+1)}\rangle\leq\log\Big{(}d_{h}^{\frac{ 1}{2}}\Big{)},\]

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle\geq-\log\Big{(}d_{h}^{ \frac{1}{2}}\Big{)}.\]

Next we provide the upper bound for \(|\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{\mp}^{(t+1)}\rangle|,|\langle\bm{q}_{n,i }^{(t+1)},\bm{k}_{n^{\prime},j}^{(t+1)}\rangle|\). By the results of F.7, we have

\[\sum_{s=T_{1}}^{t}|\alpha_{+,+}^{(s)}|,\sum_{s=T_{1}}^{t}|\alpha_ {-,-}^{(s)}|,\sum_{s=T_{1}}^{t}|\beta_{+,+}^{(s)}|,\sum_{s=T_{1}}^{t}|\beta_{ -,-}^{(s)}|,\sum_{s=T_{1}}^{t}|\beta_{n,i,+}^{(s)}|,\sum_{s=T_{1}}^{t}|\beta_{ n,i,-}^{(s)}|=O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)},\] (55)

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\sum_{s=T_{1}}^{t}|\alpha_{n,+}^{(s)}|,\sum_{s=T_{1}}^{t}|\alpha_ {n,-,i}^{(s)}|=O\Big{(}N^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)},\] (56)

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\sum_{s=T_{1}}^{t}|\beta_{n,+,i}^{(s)}|,\sum_{s=T_{1}}^{t}|\beta_ {n,-,i}^{(s)}|=O\Big{(}\text{SNR}\cdot N^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}} \Big{)}\] (57)

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\sum_{s=T_{1}}^{t}|\alpha_{n,i,+}^{(s)}|,\sum_{s=T_{1}}^{t}|\alpha_ {n,i,-}^{(s)}|,\sum_{s=T_{1}}^{t}|\alpha_{n,i,n,j}^{(s)}|,\sum_{s=T_{1}}^{t}| \beta_{n,j,n,i}^{(s)}|=O\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\] (58)

for \(i,j\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\sum_{s=T_{1}}^{t}|\alpha_{n,i,n^{\prime},j}^{(s)}|,\sum_{s=T_{1}}^{t}|\beta_ {n,j,n^{\prime},i}^{(s)}|=O\Big{(}d^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^ {2}M^{2}/\delta)\Big{)}\] (59)for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],n\neq n^{\prime}\). Plugging these into the update rule of \(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\mp}^{(t)}\rangle,\langle\bm{q}_{n,i}^{(t)}, \bm{k}_{\bar{n},j}^{(t)}\rangle\) and assume that propositions \(\mathcal{C}(T_{1}),\ldots,\mathcal{C}(t)\) hold, we have

\[|\langle\bm{q}_{+}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle|\leq|\langle \bm{q}_{+}^{(T_{1})},\bm{k}_{-}^{(T_{1})}\rangle|+\sum_{s=T_{1}}^{t}|\langle \bm{q}_{+}^{(s+1)},\bm{k}_{-}^{(s+1)}\rangle-\langle\bm{q}_{+}^{(s)},\bm{k}_{- }^{(s)}\rangle|\] \[\leq|\langle\bm{q}_{+}^{(T_{1})},\bm{k}_{-}^{(T_{1})}\rangle|\] \[+\sum_{s=T_{1}}^{t}\Big{|}\alpha_{+,+}^{(s)}\langle\bm{k}_{+}^{( s)},\bm{k}_{-}^{(s)}\rangle+\sum_{n\in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(s)} \langle\bm{k}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle\] \[+\beta_{-,-}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{-}^{(s)} \rangle+\sum_{n\in S_{-}}\sum_{i=2}^{M}\beta_{n,-,i}^{(s)}\langle\bm{q}_{n,i} ^{(s)},\bm{q}_{+}^{(s)}\rangle\] \[+\Big{(}\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n\in S_{+}}\sum_ {i=2}^{M}\alpha_{n,+,i}^{(s)}\bm{k}_{n,i}^{(s)}\Big{)}\] \[\cdot\Big{(}\beta_{-,-}^{(s)}\bm{q}_{-}^{(s)\top}+\sum_{n\in S_{- }}\sum_{i=2}^{M}\beta_{n,-,i}^{(s)}\bm{q}_{n,i}^{(s)\top}\Big{)}\Big{|}\] \[\leq|\langle\bm{q}_{+}^{(T_{1})},\bm{k}_{-}^{(T_{1})}\rangle|\] (60) \[+\sum_{s=T_{1}}^{t}|\alpha_{+,+}^{(s)}||\langle\bm{k}_{+}^{(s)}, \bm{k}_{-}^{(s)}\rangle|+\sum_{n\in S_{+}}\sum_{i=2}^{M}\sum_{s=T_{1}}^{t}| \alpha_{n,+,i}^{(s)}||\langle\bm{k}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\beta_{-,-}^{(s)}||\langle\bm{q}_{+}^{(s)}, \bm{q}_{-}^{(s)}\rangle|+\sum_{n\in S_{-}}\sum_{i=2}^{M}\sum_{s=T_{1}}^{t}| \beta_{n,-,i}^{(s)}||\langle\bm{q}_{n,i}^{(s)},\bm{q}_{+}^{(s)}\rangle|\] \[+\{lower\;order\;term\}\] \[=|\langle\bm{q}_{+}^{(T_{1})},\bm{k}_{-}^{(T_{1})}\rangle|\] \[+O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}\cdot o(1)+N \cdot M\cdot O\Big{(}N^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}\cdot o(1)\] \[+O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}\cdot o(1)+N \cdot M\cdot O\Big{(}\text{SNR}\cdot N^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}} \Big{)}\cdot o(1)\] \[=|\langle\bm{q}_{+}^{(T_{1})},\bm{k}_{-}^{(T_{1})}\rangle|+o \Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}+o\Big{(}\text{SNR}\cdot N^{ \frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}\] \[=o(1),\]

where the first inequality is by triangle inequality, the second inequality is by (213), the last equality is by \(|\langle\bm{q}_{+}^{(T_{1})},\bm{k}_{-}^{(T_{1})}\rangle|=o(1)\) and \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\text{SNR}^{4},\text{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\). Similarly we have \(|\langle\bm{q}_{-}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle|=o(1)\).

\[\begin{split}&|\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{\overline{n},j}^{( t+1)}\rangle|\leq|\langle\bm{q}_{n,i}^{(T_{1})},\bm{k}_{\overline{n},j}^{(T_{1})} \rangle|+\sum_{s=T_{1}}^{t}|\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{\overline{n},j }^{(s+1)}\rangle-\langle\bm{q}_{n,i}^{(s)},\bm{k}_{\overline{n},j}^{(s)}\rangle |\\ &\leq|\langle\bm{q}_{n,i}^{(T_{1})},\bm{k}_{\overline{n},j}^{(T_{ 1})}\rangle|\\ &+\sum_{s=T_{1}}^{t}\Big{|}\alpha_{n,i,+}^{(s)}(\bm{k}_{+}^{(s)}, \bm{k}_{\overline{n},j}^{(s)}+\alpha_{n,i,-}^{(s)}(\bm{k}_{-}^{(s)},\bm{k}_{ \overline{n},j}^{(s)})+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{ \prime},l}^{(s)}\langle\bm{k}_{n^{\prime},l}^{(s)},\bm{k}_{\overline{n},j}^{( s)}\rangle\\ &+\beta_{\overline{n},j,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_ {n,i}^{(s)}\rangle+\beta_{\overline{n},j,-}^{(s)}(\bm{q}_{-}^{(s)},\bm{q}_{n, i}^{(s)})+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{\overline{n},j,n^{ \prime},l}^{(s)}\langle\bm{q}_{n^{\prime},l}^{(s)},\bm{q}_{n,i}^{(s)}\rangle \\ &+\Big{(}\alpha_{n,i,+}^{(s)}\bm{k}_{+}^{(s)}+\alpha_{n,i,-}^{(s)} \bm{k}_{-}^{(s)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l }^{(s)}\bm{k}_{n^{\prime},l}^{(s)}\Big{)}\\ &\cdot\Big{(}\beta_{\overline{n},j,+}^{(s)}\bm{q}_{+}^{(s)\top}+ \beta_{\overline{n},j,-}^{(s)}\bm{q}_{-}^{(s)\top}+\sum_{n^{\prime}=1}^{N} \sum_{l=2}^{M}\beta_{\overline{n},j,n^{\prime},l}^{(s)}\bm{q}_{n^{\prime},l}^ {(s)\top}\Big{)}\Big{|}\\ &\leq|\langle\bm{q}_{n,i}^{(T_{1})},\bm{k}_{\overline{n},j}^{(T_{ 1})}\rangle|\\ &+\sum_{s=T_{1}}^{t}|\alpha_{n,i,+}^{(s)}||\langle\bm{k}_{+}^{(s) },\bm{k}_{\overline{n},j}^{(s)}\rangle|+\sum_{s=T_{1}}^{t}|\alpha_{n,i,-}^{(s) }||\langle\bm{k}_{-}^{(s)},\bm{k}_{\overline{n},j}^{(s)}\rangle|\\ &+\sum_{s=T_{1}}^{t}|\alpha_{n,i,\overline{n},j}^{(s)}||\bm{k}_ {\overline{n},j}^{(s)}||^{2}_{2}+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\alpha_{n,i, n,l}^{(s)}||\langle\bm{k}_{n,l}^{(s)},\bm{k}_{\overline{n},j}^{(s)}\rangle|\\ &+\sum_{n^{\prime}\neq n\wedge(l\neq j\lor n^{\prime}\neq\overline {n})}\sum_{s=T_{1}}^{t}|\alpha_{n,i,n^{\prime},l}^{(s)}||\langle\bm{k}_{n^{ \prime},l}^{(s)},\bm{k}_{\overline{n},j}^{(s)}\rangle|\\ &+\sum_{s=T_{1}}^{t}|\beta_{\overline{n},j,+}^{(s)}||\langle\bm{q }_{+}^{(s)},\bm{q}_{n,i}^{(s)}\rangle|+\sum_{s=T_{1}}^{t}|\beta_{\overline{n}, j,-}^{(s)}||\langle\bm{q}_{-}^{(s)},\bm{q}_{n,i}^{(s)}\rangle|\\ &+\sum_{s=T_{1}}^{t}|\beta_{\overline{n},j,n,i}^{(s)}||\bm{q}_{n,i}^{(s)}||^{2}_{2}+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\beta_{\overline{n},j, \overline{n},l}^{(s)}||\langle\bm{q}_{\overline{n},l}^{(s)},\bm{q}_{n,i}^{(s)}\rangle |\\ &+\sum_{n^{\prime}\neq\overline{n}\wedge(l\neq i\lor n^{ \prime}\neq n)}\sum_{s=T_{1}}^{t}|\beta_{\overline{n},j,n^{\prime},l}^{(s)}| \langle\bm{q}_{n^{\prime},l}^{(s)},\bm{q}_{n,i}^{(s)}\rangle|\\ &+\{lower order term\}\\ &=|\langle\bm{q}_{n,i}^{(T_{1})},\bm{k}_{\overline{n},j}^{(T_{1})}\rangle| \\ &+O\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\cdot o(1)+O\Big{(}d^{- \frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)}\cdot\Theta( \sigma_{p}^{2}\sigma_{h}^{2}dd_{h})\\ &+M\cdot O\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\cdot o(1)+N\cdot M \cdot O\Big{(}d^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/\delta) \Big{)}\cdot o(1)\\ &+O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}\cdot o(1)\\ &=|\langle\bm{q}_{n,i}^{(T_{1})},\bm{k}_{\overline{n},j}^{(T_{1})} \rangle|+o\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\\ &+O\Big{(}d^{-\frac{1}{2}}d_{h}^{\frac{1}{4}}\Big{)}+o\Big{(}Nd^{- \frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)}\\ &=o(1),\end{split}\] (61)

where the first inequality is by triangle inequality, the second inequality is by (196), the second equality is by \(\sigma_{h}^{2}\leq\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d_{h}^{- \frac{1}{2}}\cdot\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{-\frac{3}{2}}\), the last equality is by \(|\langle\bm{q}_{n,i}^{(T_{1})},\bm{k}_{\overline{n},j}^{(T_{1})}\rangle|=o(1)\), \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\) and \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\).

### Stage III

In Stage III, the outputs of ViT grow up and the loss derivatives are no longer at \(o(1)\). We will carefully compute the growth rate of \(V_{\pm}\) and \(V_{n,i}\) while keeping monitoring the monotonicity of \(\langle\bm{q},\bm{k}\rangle\). By substituting \(t=T_{2}=\Theta\Big{(}\frac{1}{\eta\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}} \Big{)}\) into propositions \(\mathcal{B}(t)\), \(\mathcal{C}(t)\), \(\mathcal{D}(t)\), \(\mathcal{E}(t)\) in Stage II, we have the following conditions at the beginning of stage III

\[|V_{+}^{(T_{2})}|,|V_{-}^{(T_{2})}|,|V_{n,i}^{(T_{2})}|=o(1),\]

\[V_{+}^{(T_{2})}\geq 3M\cdot|V_{n,i}^{(T_{2})}|,\]

\[V_{-}^{(T_{2})}\leq-3M\cdot|V_{n,i}^{(T_{2})}|,\]

\[\|\bm{q}_{\pm}^{(T_{2})}\|_{2}^{2},\|\bm{k}_{\pm}^{(T_{2})}\|_{2}^{2}=\Theta \Big{(}\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}\Big{)},\]

\[\|\bm{q}_{n,i}^{(T_{2})}\|_{2}^{2},\|\bm{k}_{n,i}^{(T_{2})}\|_{2}^{2}=\Theta \Big{(}\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}\Big{)},\]

\[|\langle\bm{q}_{+}^{(T_{2})},\bm{q}_{-}^{(T_{2})}\rangle|,|\langle\bm{q}_{\pm }^{(T_{2})},\bm{q}_{n,i}^{(T_{2})}\rangle|,|\langle\bm{q}_{n,i}^{(T_{2})},\bm{ q}_{n,j}^{(T_{2})}\rangle|=o(1),\]

\[|\langle\bm{k}_{+}^{(T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|,|\langle\bm{k}_{\pm }^{(T_{2})},\bm{k}_{n,i}^{(T_{2})}\rangle|,|\langle\bm{k}_{n,i}^{(T_{2})},\bm{ k}_{n,i}^{(T_{2})}\rangle|=o(1),\]

for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],i\neq j\) or \(n\neq n^{\prime}\).

\[\Lambda_{n,\pm,j}^{(T_{2})}\geq\log\Big{(}\exp(\Lambda_{n,\pm,j}^{(T_{1})})+ \Theta\Big{(}\frac{d_{h}^{\frac{1}{\delta}}}{N\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{3}}\Big{)}\Big{)}\]

\[\Lambda_{n,i,\pm,j}^{(T_{2})}\geq\log\Big{(}\exp(\Lambda_{n,i,\pm,j}^{(T_{1})} )+\Theta\Big{(}\frac{\sigma_{p}^{2}dd_{h}^{\frac{1}{\delta}}}{N\|\bm{\mu}\|_{2 }^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}\Big{)}\Big{)}\]

\[|\langle\bm{q}_{\pm}^{(T_{2})},\bm{k}_{\pm}^{(T_{2})}\rangle|,|\langle\bm{q}_{ \pm}^{(T_{2})},\bm{k}_{n,j}^{(T_{2})}\rangle|,|\langle\bm{q}_{n,i}^{(T_{2})}, \bm{k}_{\pm}^{(T_{2})}\rangle|,|\langle\bm{q}_{n,i}^{(T_{2})},\bm{k}_{n,j}^{( T_{2})}\rangle|\leq\log(d_{h}^{\frac{1}{\delta}})\]

\[|\langle\bm{q}_{\pm}^{(T_{2})},\bm{k}_{\mp}^{(T_{2})}\rangle|,|\langle\bm{q}_{ n,i}^{(T_{2})},\bm{k}_{n,j}^{(T_{2})}\rangle|=o(1)\]

for \(i,j\in[M]\backslash\{1\},n,\overline{n}\in[N],n\neq\overline{n}\).

Let \(T_{3}=\Theta\Big{(}\frac{1}{\eta e\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}} \Big{)}\). Next we prove the following four propositions \(\mathcal{F}(t)\), \(\mathcal{G}(t)\), \(\mathcal{H}(t)\), \(\mathcal{I}(t)\) by induction on t for \(t\in[T_{2},T_{3}]\):

* \(\mathcal{F}(t):\) \[V_{+}^{(t)}\geq 3M\cdot|V_{n,i}^{(t)}|,\] \[V_{-}^{(t)}\leq-3M\cdot|V_{n,i}^{(t)}|,\] \[|V_{n,i}^{(t)}|=o(1),\] \[\log\Big{(}\exp(V_{+}^{(T_{2})})+\eta C_{17}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O} \|_{2}^{2}(t-T_{2})\Big{)}\leq V_{+}^{(t)}\leq 2\log\big{(}O(\frac{1}{\epsilon}) \big{)},\] \[-2\log\big{(}O(\frac{1}{\epsilon})\big{)}\leq V_{-}^{(t)}\leq-\log\Big{(} \exp(-V_{-}^{(T_{2})})+\eta C_{17}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(t- T_{2})\Big{)}\] for \(i\in[M]\backslash\{1\},n\in[N]\).
* \(\mathcal{G}(t):\) \[\|\bm{q}_{\pm}^{(t)}\|_{2}^{2},\|\bm{k}_{\pm}^{(t)}\|_{2}^{2}=\Theta(\|\bm{ \mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}),\] \[\|\bm{q}_{n,i}^{(t)}\|_{2}^{2},\|\bm{k}_{n,i}^{(t)}\|_{2}^{2}=\Theta\Big{(} \sigma_{p}^{2}\sigma_{h}^{2}dd_{h}\Big{)},\] \[|\langle\bm{q}_{+}^{(t)},\bm{q}_{-}^{(t)}\rangle|,|\langle\bm{q}_{\pm}^{(t)},\bm{q}_{n,i}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{q}_{n^{\prime},j}^{(t) }\rangle|=o(1),\] \[|\langle\bm{k}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle|,|\langle\bm{k}_{\pm}^{(t)},\bm{k}_{n,i}^{(t)}\rangle|,|\langle\bm{k}_{n,i}^{(t)},\bm{k}_{n^{\prime},j}^{(t) }\rangle|=o(1)\] for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],i\neq j\) or \(n\neq n^{\prime}\).

* \(\mathcal{H}(t):\) \[\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{\pm}^{(t+1)}\rangle\geq\langle\bm{q}_{\pm}^{( t)},\bm{k}_{\pm}^{(t)}\rangle,\] \[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{\pm}^{(t+1)}\rangle\geq\langle\bm{q}_{n,i }^{(t)},\bm{k}_{\pm}^{(t)}\rangle,\] \[\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle\leq\langle\bm{q}_{\pm }^{(t)},\bm{k}_{n,j}^{(t)}\rangle,\] \[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle\leq\langle\bm{q}_{n,i }^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] for \(i,j\in[M]\backslash\{1\},n\in[N]\).
* \(\mathcal{I}(t):\) \[|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{q}_{\pm}^{ (t)},\bm{k}_{n,j}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)} \rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle|\leq\log(\epsilon ^{-1}d_{h}^{\frac{1}{2}}),\] \[|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\mp}^{(t)}\rangle|,|\langle\bm{q}_{n,i }^{(t)},\bm{k}_{\mp,j}^{(t)}\rangle|=o(1)\] for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],n\neq\overline{n}\).

By the results of Stage II, we know that \(\mathcal{F}(T_{1})\), \(\mathcal{G}(T_{2})\), \(\mathcal{I}(T_{2})\) are true. To prove that \(\mathcal{F}(t)\), \(\mathcal{G}(t)\), \(\mathcal{H}(t)\) and \(\mathcal{I}(t)\) are true in stage 3, we will prove the following claims holds for \(t\in[T_{2},T_{3}]\):

**Claim 5**.: \(\mathcal{H}(T_{2}),\ldots,\mathcal{H}(t-1)\Longrightarrow\mathcal{F}(t+1)\)__

**Claim 6**.: \(\mathcal{F}(t),\mathcal{G}(t),\mathcal{H}(T_{2}),\ldots,\mathcal{H}(t-1) \Longrightarrow\mathcal{H}(t)\)__

**Claim 7**.: \(\mathcal{F}(T_{2}),\ldots,\mathcal{F}(t),\mathcal{H}(T_{2}),\ldots,\mathcal{H }(t-1),\mathcal{I}(T_{2}),\ldots,\mathcal{I}(t)\Longrightarrow\mathcal{G}(t+1)\)__

**Claim 8**.: \(\mathcal{F}(T_{2}),\ldots,\mathcal{F}(t),\mathcal{G}(T_{2}),\ldots,\mathcal{G }(t),\mathcal{H}(T_{2}),\ldots,\mathcal{H}(t-1)\Longrightarrow\mathcal{I}(t+1)\)__

#### d.3.1 Proof of Claim 5

The proofs for \(V_{+}^{(t)}\geq 3M\cdot|V_{n,i}^{(t)}|\) and \(V_{-}^{(t)}\leq-3M\cdot|V_{n,i}^{(t)}|\) are the same as for D.2.1. Based on \(\mathcal{H}(T_{2}),\ldots,\mathcal{H}(t)\) where \(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle\) and \(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{\pm}^{(s)}\rangle\) are monotonically non-decreasing and \(\max_{j}\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\), \(\max_{j}\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\) are monotonically non-increasing for \(s\in[T_{2},t-1]\), we have

\[\Lambda_{n,\pm,j}^{(s)}\geq\Lambda_{n,\pm,j}^{(T_{2})}\geq\log\Big{(}\exp( \Lambda_{n,\pm,j}^{(T_{1})})+\Theta\Big{(}\frac{d_{h}^{\frac{1}{2}}}{N\big{(} \log(6N^{2}M^{2}/\delta)\big{)}^{3}}\Big{)}\Big{)},\] (62)

\[\Lambda_{n,i,\pm,j}^{(s)}\geq\Lambda_{n,i,\pm,j}^{(T_{2})}\geq\log\Big{(}\exp (\Lambda_{n,i,\pm,j}^{(T_{1})})+\Theta\Big{(}\frac{\sigma_{p}^{2}dd_{h}^{ \frac{1}{2}}}{N\|\bm{\mu}\|_{2}^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3} }\Big{)}\Big{)}\] (63)

for \(i,j\in[M]\backslash\{1\},n\in[N],s\in[T_{2},t]\). We further get

\[\frac{\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}{ \exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle)+\sum\limits_{j^{ \prime}=2}^{M}\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j^{\prime}}^{(s)}\rangle)}\] \[\leq\frac{\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j}^{(s)} \rangle)}{C\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle)}\] \[=\frac{1}{C\exp(\Lambda_{n,\pm,j}^{(s)})}\] \[\leq\frac{1}{C\exp(\Lambda_{n,\pm,j}^{(T_{1})})+\Theta\Big{(}\frac {d_{h}^{\frac{1}{2}}}{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}\Big{)}}\] (64) \[\leq\frac{1}{\Theta\Big{(}\frac{d_{h}^{\frac{1}{2}}}{N\big{(}\log(6 N^{2}M^{2}/\delta)\big{)}^{3}}\Big{)}}\] \[=O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h} ^{\frac{1}{2}}}\Big{)}.\]For the first inequality, by (62) and the monotonicity of \(\langle\bm{q}^{(s)},\bm{k}^{(s)}\rangle\)\(\langle(\bm{q}^{(s)}_{\pm},\bm{k}^{(s)}_{\pm})\) is increasing and \(\langle\bm{q}^{(s)}_{\pm},\bm{k}^{(s)}_{n,j}\rangle\) is decreasing), there exist a constant \(C\) such that \(C\exp(\langle\bm{q}^{(s)}_{\pm},\bm{k}^{(s)}_{\pm}\rangle)\geq\exp(\langle\bm{ q}^{(s)}_{\pm},\bm{k}^{(s)}_{\pm}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp( \langle\bm{q}^{(s)}_{\pm},\bm{k}^{(s)}_{n,j^{\prime}}\rangle)\). The second inequality is by plugging (62). Similarly, we have

\[\begin{split}&\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n, j}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{\pm}\rangle)+\sum \limits_{j^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,j^{ \prime}}\rangle)}\\ &\leq\frac{1}{C\exp(\Lambda^{(s)}_{n,i,\pm,j})}\\ &=O\Big{(}\frac{N\|\bm{\mu}\|^{2}_{2}\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{3}}{\sigma_{p}^{2}dd_{h}^{\frac{1}{2}}}\Big{)}.\end{split}\] (65)

Plugging (64) and (65) into the update rule of \(\rho_{V,n,i}\) in Lemma B.2 and get

\(|\rho_{V,n,i}^{(s+1)}-\rho_{V,n,i}^{(s)}|\)

\[\begin{split}&\leq\frac{\eta}{NM}|\ell_{n}^{(s)}|\cdot\|\bm{ \xi}_{n,i}\|_{2}^{2}\cdot\Big{(}O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta )\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)}+O\Big{(}\frac{N\|\bm{\mu}\|^{2}_{2} \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\sigma_{p}^{2}dd_{h}^{\frac{1}{2}} }\Big{)}\Big{)}\\ &+\frac{\eta}{NM}\sum_{n^{\prime}\neq n^{\prime}\neq i^{\prime}}| \ell_{n^{\prime}}^{(t)}|\cdot\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime},i^{ \prime}}\rangle\cdot\Big{(}O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)}+O\Big{(}\frac{N\|\bm{\mu}\|^{2}_{2} \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\sigma_{p}^{2}dd_{h}^{\frac{1}{2} }}\Big{)}\Big{)}\Big{)}\\ &\leq\frac{3\eta\tilde{\sigma}_{p}^{2}d}{2NM}\cdot\Big{(}O\Big{(} \frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)} +O\Big{(}\frac{N\|\bm{\mu}\|^{2}_{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3 }}{\sigma_{p}^{2}dd_{h}^{\frac{1}{2}}}\Big{)}\Big{)}\\ &+\frac{\eta}{NM}N\cdot M\cdot 2\tilde{\sigma}_{p}^{2}\sqrt{d \log(4N^{2}M^{2}/\delta)}\sqrt{d}\cdot\Big{(}O\Big{(}\frac{N\big{(}\log(6N^{ 2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)}+O\Big{(}\frac{N\|\bm{ \mu}\|^{2}_{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\sigma_{p}^{2}dd_{h }^{\frac{1}{2}}}\Big{)}\Big{)}\\ &\leq\frac{2\eta}{NM}\cdot\Big{(}O\Big{(}\frac{N\sigma_{p}^{2}d \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)}+O\Big{(} \frac{N\|\bm{\mu}\|^{2}_{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^ {\frac{1}{2}}}\Big{)}\Big{)}\\ &=O\Big{(}\frac{\eta\sigma_{p}^{2}d\big{(}\log(6N^{2}M^{2}/\delta )\big{)}^{3}}{d_{h}^{\frac{1}{2}}}+\frac{\eta\|\bm{\mu}\|^{2}_{2}\big{(}\log(6N ^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)}\end{split}\] (66)

where the second inequality is by Lemma C.4 and \(|\ell_{n}^{(t)}|\leq 1\). For the last inequality, since \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\), we have \(N\cdot M\cdot 2\tilde{\sigma}_{p}^{2}\sqrt{d\log(4N^{2}M^{2}/\delta)}\leq\frac{1} {2}\tilde{\sigma}_{p}^{2}d\). By Definition B.1 and taking a summation we have

\[\begin{split}&|V_{n,i}^{(t+1)}|\leq|V_{n,i}^{(T_{2})}|+\sum \limits_{s=T_{2}}^{t}|\rho_{V,n,i}^{(s+1)}-\rho_{V,n,i}^{(s)}|\cdot\|\bm{w}_{O} \|^{2}_{2}\\ &\leq|V_{n,i}^{(T_{2})}|+T_{3}\cdot|\rho_{V,n,i}^{(t+1)}-\rho_{V,n,i}^{(t)}|\cdot\|\bm{w}_{O}\|^{2}_{2}\\ &\leq o(1)+\Theta\Big{(}\frac{1}{\eta\epsilon\|\bm{\mu}\|^{2}_{2} \|\bm{w}_{O}\|^{2}_{2}}\Big{)}\cdot O\Big{(}\frac{\eta\sigma_{p}^{2}d\big{(} \log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}+\frac{\eta\|\bm{\mu}\|^{ 2}_{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)} \cdot\|\bm{w}_{O}\|^{2}_{2}\\ &=o(1)+O\Big{(}\frac{\sigma_{p}^{2}d\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{3}}{\epsilon\|\bm{\mu}\|^{2}_{2}d_{h}^{\frac{1}{2}}}+\frac{\big{(} \log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)}\\ &=o(1)+o(1)\\ &=o(1),\end{split}\] (67)where the first equality is by \(N\cdot\mathrm{SNR}^{2}\geq\Omega(1)\), the second equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\). Then we have a constant upper bound for the sum of \(V_{n,i}\) as follows:

\[\sum\limits_{i\in[M]\setminus\{1\}}|V_{n,i}^{(s)}|=(M-1)\cdot o(1)\leq C_{15}\]

for \(n\in[N],s\in[T_{2},t]\).

Expanding (64) and (65) we have

\[\frac{\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j}^{(s)}))}{\exp( \langle\bm{q}_{\pm}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j^{\prime}=2} ^{M}\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j^{\prime}}^{(s)}\rangle)}=O\Big{(} \frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)} =o(1),\] (68)

where the equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2 }\epsilon^{-2}\Big{)}\).

\[\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}))}{\exp( \langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j^{\prime}=2} ^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j^{\prime}}^{(s)}\rangle)}=O\Big{(} \frac{N\|\bm{\mu}\|_{2}^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\sigma_ {p}^{2}dd_{h}^{\frac{1}{2}}}\Big{)}=o(1),\] (69)

where the equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2 }\epsilon^{-2}\Big{)}\). Then we have

\[softmax(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}))\geq 1-(M-1) \cdot o(1)\geq 1-o(1),\] (70) \[softmax(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{\pm}^{(s)}))\geq 1-(M-1) \cdot o(1)\geq 1-o(1)\] (71)

for \(i\in[M]\setminus\{1\},n\in[N],s\in[T_{2},t]\). Next we provide the bounds for \(-\ell_{n}^{\prime(s)}\). Note that \(\ell(z)=\log(1+\exp(-z))\) and \(-\ell^{\prime}=\exp(-z)/(1+\exp(-z))\), without loss of generality, we assume \(y_{n}=1\), we have

\[-\ell^{\prime}(f(\bm{X}_{n},\theta(s)))=\frac{1}{1+\exp(\frac{1}{M}\sum \limits_{l=1}^{M}\bm{\varphi}(\bm{x}_{n,l}^{\top}\bm{W}_{Q}^{(s)}\bm{W}_{K}^{( s)\top}(\bm{X}_{n})^{\top})\bm{X}_{n}\bm{W}_{V}^{(s)}\bm{w}_{O})}\] (72)

where \(\frac{1}{M}\sum\limits_{l=1}^{M}\bm{\varphi}(\bm{x}_{n,l}^{\top}\bm{W}_{Q}^{( s)}\bm{W}_{K}^{(s)\top}(\bm{X}_{n})^{\top})\bm{X}_{n}\bm{W}_{V}^{(s)}\bm{w}_{O}\) can be bounded as follows:

\[\begin{split}&\frac{1}{M}\sum\limits_{l=1}^{M}\bm{\varphi}(\bm{x}_ {n,l}^{\top}\bm{W}_{Q}^{(s)}\bm{W}_{K}^{(s)\top}(\bm{X}_{n})^{\top})\bm{X}_{n} \bm{W}_{V}^{(s)}\bm{w}_{O}\\ &=\frac{1}{M}\Big{(}\big{(}softmax(\langle\bm{q}_{\pm}^{(s)}, \bm{k}_{\pm}^{(s)}\rangle)+\sum\limits_{l=2}^{M}softmax(\langle\bm{q}_{n,l}^{( s)},\bm{k}_{\pm}^{(s)}\rangle)\big{)}\cdot\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(s)}\bm{w}_{O} \\ &+\sum\limits_{j\in[M]\setminus\{1\}}\big{(}softmax(\langle\bm{q}_{ \pm}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)+\sum\limits_{l=2}^{M}softmax(\langle \bm{q}_{n,l}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)\big{)}\cdot\bm{\xi}_{n,j}^{\top} \bm{W}_{V}^{(s)}\bm{w}_{O}\Big{)}\\ &=\frac{1}{M}\Big{(}M\cdot\big{(}1-o(1)\big{)}\cdot V_{+}^{(s)}+M \cdot o(1)\cdot\sum\limits_{j\in[M]\setminus\{1\}}V_{n,i}^{(s)}\Big{)}\\ &\geq\frac{V_{+}^{(s)}}{2}\end{split}\] (73)

for \(s\in[T_{2},t]\). The second equality is by plugging (68), (69), (70) and (71), the inequality is by \(V_{+}^{(s)}\geq 3M\cdot|V_{n,i}^{(s)}|\) for \(s\in[T_{2},t]\). Similarly, we have

\[\begin{split}&\frac{1}{M}\sum\limits_{l=1}^{M}\bm{\varphi}(\bm{x}_ {n,l}^{\top}\bm{W}_{Q}^{(s)}\bm{W}_{K}^{(s)\top}(\bm{X}_{n})^{\top})\bm{X}_{n} \bm{W}_{V}^{(s)}\bm{w}_{O}\\ &\leq\max\limits_{i\in[M]\setminus\{1\}}\{V_{+}^{(s)},V_{n,i}^{(s) }\}\\ &=V_{+}^{(s)}\end{split}\] (74)Plugging (73) and (74) into (72) we have

\[\begin{split}-\ell^{\prime}(f(\bm{X}_{n},\theta(s)))&= \frac{1}{1+\exp(\frac{1}{M}\sum\limits_{l=1}^{M}\bm{\varphi}(\bm{x}_{n,l}^{\top} \bm{W}_{Q}^{(s)}\bm{W}_{K}^{(s)\top}(\bm{X}_{n})^{\top})\bm{X}_{n}\bm{W}_{V}^{(s )}\bm{w}_{O})}\\ &\geq\frac{1}{1+\exp(V_{+}^{(s)})}\\ &\geq\frac{C_{16}}{\exp(V_{+}^{(s)})}\end{split}\] (75)

where the first inequality is by plugging (74). For the last inequality, note that \(V_{+}^{(T_{2})}\geq 0\) and \(V_{+}^{(s)}\) is monotonically increasing, so there exist a constant \(C_{16}\) such that \(\frac{1}{1+\exp(V_{+}^{(s)})}\geq\frac{C_{16}}{\exp(V_{+}^{(s)})}\). We also have the upper bound

\[\begin{split}-\ell^{\prime}(f(\bm{X}_{n},\theta(s)))& =\frac{1}{1+\exp(\frac{1}{M}\sum\limits_{l=1}^{M}\bm{\varphi}(\bm{ x}_{n,l}^{\top}\bm{W}_{Q}^{(s)}\bm{W}_{K}^{(s)\top}(\bm{X}_{n})^{\top})\bm{X}_{n} \bm{W}_{V}^{(s)}\bm{w}_{O})}\\ &\leq\frac{1}{1+\exp(V_{+}^{(s)}/2)}\\ &\leq\frac{1}{\exp(V_{+}^{(s)}/2)}\end{split}\] (76)

Plugging (70), (71) and (75) into the update rule of \(\gamma_{V,+}^{(t)}\) and in Lemma B.2 and get

\[\begin{split}&\gamma_{V,+}^{(s+1)}-\gamma_{V,+}^{(s)}\\ &=-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_{n\in S_{+}}\ell _{n}^{\prime(s)}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)} \rangle)}{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{k =2}^{M}\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,k}^{(s)}\rangle)}\\ &+\sum\limits_{j=2}^{M}\frac{\exp(\langle\bm{q}_{n,j}^{(s)},\bm{ k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,j}^{(s)},\bm{k}_{+}^{(s)}\rangle)+ \sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n,j}^{(s)},\bm{k}_{n,k}^{(s)}\rangle) }\\ &\geq-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_{n\in S_{+}} \ell_{n}^{\prime(s)}(M\cdot(1-o(1)))\\ &\geq\frac{\eta\|\bm{\mu}\|_{2}^{2}}{N}\cdot\frac{N}{4}\cdot(1-o( 1))\cdot\frac{C_{16}}{\exp(V_{+}^{(s)})}\\ &\geq\frac{\eta C_{17}\|\bm{\mu}\|_{2}^{2}}{\exp(V_{+}^{(s)})} \end{split}\] (77)

where the second inequality is by (75). Then by definition B.1, we get

\[V_{+}^{(s+1)}-V_{+}^{(s)}=\big{(}\gamma_{V,+}^{(s+1)}-\gamma_{V,+}^{(s)}\big{)} \|\bm{w}_{O}\|_{2}^{2}\geq\frac{\eta C_{17}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_ {2}^{2}}{\exp(V_{+}^{(s)})}\]

Multiply both sides simultaneously by \(\exp(V_{+}^{(s)})\) and get

\[\exp(V_{+}^{(s)})\big{(}V_{+}^{(s+1)}-V_{+}^{(s)}\big{)}\geq\eta C_{17}\|\bm{ \mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}\]Taking a summation from \(T_{2}\) to \(t\) and get

\[\begin{split}&\sum\limits_{s=T_{2}}^{t}\exp(V_{+}^{(s)})\big{(}V_{+}^ {(s+1)}-V_{+}^{(s)}\big{)}\\ &\geq\sum\limits_{s=T_{2}}^{t}\eta C_{17}\|\bm{\mu}\|_{2}^{2}\|\bm {w}_{O}\|_{2}^{2}\\ &\geq\eta C_{17}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{ 2}+1)\end{split}\] (78)

By the property that \(V_{+}^{(s)}\) is monotonically increasing, we have

\[\begin{split}\int_{V_{+}^{(T_{2})}}^{V_{+}^{(t+1)}}\exp(x)dx& \geq\sum\limits_{s=T_{2}}^{t}\exp(V_{+}^{(s)})\big{(}V_{+}^{(s+1)}-V _{+}^{(s)}\big{)}\\ &\geq\eta C_{17}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{ 2}+1)\end{split}\] (79)

By \(\int_{V_{+}^{(T_{2})}}^{V_{+}^{(t+1)}}\exp(x)dx=\exp(V_{+}^{(t+1)})-\exp(V_{+} ^{(T_{2})})\) we get

\[V_{+}^{(t+1)}\geq\log\Big{(}\exp(V_{+}^{(T_{2})})+\eta C_{17}\|\bm{\mu}\|_{2}^{ 2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{2}+1)\Big{)}\] (80)

Similarly, we have

\[V_{-}^{(t+1)}\leq-\log\Big{(}\exp(-V_{-}^{(T_{2})})+\eta C_{17}\|\bm{\mu}\|_{2 }^{2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{2}+1)\Big{)}\] (81)

Next we provide upper bounds for \(V_{+}^{(t+1)}\) and \(V_{-}^{(t+1)}\). By the update rule of \(\gamma_{V,+}^{(t)}\) and in Lemma B.2 we have

\[\begin{split}&\gamma_{V,+}^{(s+1)}=\gamma_{V,+}^{(s)}\\ &-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_{n\in S_{+}}\ell _{n}^{\prime(s)}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)} \rangle)}{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{k =2}^{M}\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,k}^{(s)}\rangle)}\\ &+\sum\limits_{j=2}^{M}\frac{\exp(\langle\bm{q}_{n,j}^{(s)},\bm{ k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,j}^{(s)},\bm{k}_{+}^{(s)}\rangle)+ \sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n,j}^{(s)},\bm{k}_{n,k}^{(s)}\rangle) }\\ &\leq\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_{n\in S_{+}}- \ell_{n}^{\prime(s)}\cdot M\\ &\leq\frac{\eta\|\bm{\mu}\|_{2}^{2}}{N}\cdot\frac{3N}{4}\cdot \frac{1}{\exp(V_{+}^{(s)}/2)}\\ &=\frac{3\eta\|\bm{\mu}\|_{2}^{2}}{4\exp(V_{+}^{(s)}/2)}\end{split}\]

where the second inequality is by (76). Then by definition B.1, we get

\[V_{+}^{(s+1)}-V_{+}^{(s)}=\big{(}\gamma_{V,+}^{(s+1)}-\gamma_{V,+}^{(s)}\big{)} \|\bm{w}_{O}\|_{2}^{2}\leq\frac{3\eta\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}} {4\exp(V_{+}^{(s)}/2)}\] (82)

Further we have

\[\begin{split}\exp(V_{+}^{(s+1)}/2)&\leq\exp(V_{+}^ {(s)}/2+\frac{3\eta\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}}{8\exp(V_{+}^{(s)}/ 2)}\\ &=\exp(V_{+}^{(s)}/2)\cdot\exp(\frac{3\eta\|\bm{\mu}\|_{2}^{2}\| \bm{w}_{O}\|_{2}^{2}}{8\exp(V_{+}^{(s)}/2)})\\ &\leq C_{18}\exp(V_{+}^{(s)}/2).\end{split}\] (83)For the last inequality, by \(\eta\leq\widetilde{O}(\min\{\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d_{ h}^{-\frac{1}{2}})\), \(V_{+}^{(T_{2})}=\Theta(1)\) and the monotonicity of \(V_{+}^{(s)}\), we have \(\exp(\frac{3\eta\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}}{8\exp(V_{+}^{(s)} /2)})\leq\exp(o(1))\leq C_{18}\). Multiplying both sides by \(\Big{(}V_{+}^{(s+1)}/2-V_{+}^{(s)}/2\Big{)}\) simultaneously gives

\[\begin{split}\exp(V_{+}^{(s+1)}/2)\Big{(}V_{+}^{(s+1)}/2-V_{+}^{( s)}/2\Big{)}&\leq C_{18}\exp(V_{+}^{(s)}/2)\Big{(}V_{+}^{(s+1)}/2-V_{+}^{( s)}/2\Big{)}\\ &\leq\frac{3\eta C_{18}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2 }}{8}\end{split}\] (84)

where the last inequality is by plugging (82). Taking a summation we have

\[\begin{split}&\int_{V_{+}^{(T_{2})}/2}^{V_{+}^{(T_{2})}/2}\exp(x)dx \\ &\leq\sum_{s=T_{2}}^{t}\exp(V_{+}^{(s+1)}/2)\Big{(}V_{+}^{(s+1)} /2-V_{+}^{(s)}/2\Big{)}\\ &\leq\sum_{s=T_{2}}^{T_{3}}\frac{3\eta C_{18}\|\bm{\mu}\|_{2}^{2} \|\bm{w}_{O}\|_{2}^{2}}{8}\\ &\leq\Theta\Big{(}\frac{1}{\eta\epsilon\|\bm{\mu}\|_{2}^{2}\|\bm {w}_{O}\|_{2}^{2}}\Big{)}\cdot\frac{3\eta C_{18}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{ O}\|_{2}^{2}}{8}\\ &=O(\frac{1}{\epsilon})\end{split}\] (85)

By \(\int_{V_{+}^{(T_{2})}/2}^{V_{+}^{(t+1)}/2}\exp(x)dx=\exp(V_{+}^{(t+1)}/2)-\exp (V_{+}^{(T_{2})}/2)\) we have

\[V_{+}^{(t+1)}\leq 2\log\big{(}\exp(V_{+}^{(T_{2})}/2)+O(\frac{1}{\epsilon}) \big{)}=2\log\big{(}O(\frac{1}{\epsilon})\big{)}\]

Similarly, we have

\[V_{-}^{(t+1)}\geq-2\log\big{(}O(\frac{1}{\epsilon})\big{)}\]

#### d.3.2 Proof of Claim 6

By \(\mathcal{H}(T_{2}),\ldots,\mathcal{H}(t-1)\), we have \(softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle),softmax(\langle \bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)}\rangle)=1-o(1)\) and \(softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{(t)}\rangle),softmax(\langle \bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)=o(1)\), which have been proved in D.3.1. By the results of F4, we have the signs of \(\alpha\) and \(\beta\) as follows:

\[\alpha_{+,+}^{(t)},\alpha_{-,-}^{(t)},\beta_{+,+}^{(t)},\beta_{-,-}^{(t)}, \alpha_{n,i,+}^{(t)},\alpha_{n,i,-}^{(t)},\beta_{n,+,i}^{(t)},\beta_{n,-,i}^{( t)}\geq 0,\]

\[\alpha_{n,+,i}^{(t)},\alpha_{n,-,i}^{(t)},\alpha_{n,i,n,j}^{(t)},\beta_{n,i,+ }^{(t)},\beta_{n,i,-}^{(t)},\beta_{n,j,n,i}^{(t)}\leq 0.\]Then combined with \(\mathcal{G}(T)\) and we have the dynamics of \(\langle\bm{q},\bm{k}\rangle\) as follows:

\[\begin{split}&\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle -\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\\ &=\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\sum_{n\in S_{+} }\sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle \\ &+\beta_{+,+}^{(t)}\|\bm{q}_{+}^{(t)}\|_{2}^{2}+\sum_{n\in S_{+} }\sum_{i=2}^{M}\beta_{n,+,i}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)} \rangle\\ &+\left(\alpha_{++,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n\in S_{+}} \sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\right)\\ &\cdot\left(\beta_{+,+}^{(t)}\bm{q}_{+}^{(t)\top}+\sum_{n\in S_{+ }}\sum_{i=2}^{M}\beta_{n,+,i}^{(t)}\bm{q}_{n,i}^{(t)\top}\right)\\ &=\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\beta_{+,+}^{(t) }\|\bm{q}_{+}^{(t)}\|_{2}^{2}+\{lower\;order\;term\}\\ &\geq 0\end{split}\] (86)

Similarly, we have

\[\langle\bm{q}_{-}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle-\langle\bm{q}_{-}^{(t)}, \bm{k}_{-}^{(t)}\rangle\geq 0.\] (87)

\[\begin{split}&\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{+}^{(t+1)} \rangle-\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle\\ &=\alpha_{n,i,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\alpha_{n,i,- }^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle+\sum_{n^{\prime}=1}^{N }\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_ {n^{\prime},l}^{(t)}\rangle\\ &+\beta_{+,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)} \rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(t)} \langle\bm{q}_{n,i}^{(t)},\bm{q}_{n^{\prime},l}^{(t)}\rangle\\ &+\left(\alpha_{n,i,+}^{(t)}\bm{k}_{+}^{(t)}+\alpha_{n,i,-}^{(t) }\bm{k}_{-}^{(t)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime },l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)}\right)\\ &\cdot\left(\beta_{+,+}^{(t)}\bm{q}_{+}^{(t)\top}+\sum_{n^{\prime }\in S_{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(t)}\bm{q}_{n^{\prime},l}^{( t)\top}\right)\\ &=\alpha_{n,i,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\beta_{n,+,i}^ {(t)}\|\bm{q}_{n,i}^{(t)}\|_{2}^{2}+\{lower\;order\;term\}\\ &\geq 0.\end{split}\] (88)

Similarly, we have

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle-\langle\bm{q}_{n,i}^{( t)},\bm{k}_{-}^{(t)}\rangle\geq 0.\] (89)\[\langle\bm{q}_{+}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle-\langle\bm{q}_{+} ^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] (90) \[=\alpha_{+,+}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{n,j}^{(t)} \rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\alpha_{n^{\prime},+,l}^{(t)} \langle\bm{k}_{n,j}^{(t)},\bm{k}_{n^{\prime},l}^{(t)}\rangle\] \[+\beta_{n,j,+}^{(t)}\|\bm{q}_{+}^{(t)}\|_{2}^{2}+\beta_{n,j,-}^{( t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{-}^{(t)}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^ {M}\beta_{n,j,n^{\prime},l}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n^{\prime},l }^{(t)}\rangle\] \[+\left(\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n^{\prime}\in S_ {+}}\sum_{l=2}^{M}\alpha_{n^{\prime},+,l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)}\right)\] \[\cdot\left(\beta_{n,j,+}^{(t)}\bm{q}_{+}^{(t)\top}+\beta_{n,j,-} ^{(t)\top}\bm{q}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n, j,n^{\prime},l}^{(t)\top}\bm{q}_{n^{\prime},l}^{(t)\top}\right)\] \[=\alpha_{n,+,j}^{(t)}\|\bm{k}_{n,j}^{(t)}\|_{2}^{2}+\beta_{n,j,+}^ {(t)}\|\bm{q}_{+}^{(t)}\|_{2}^{2}+\{lower\;order\;term\}\] \[\leq 0\]

Similarly, we have

\[\langle\bm{q}_{-}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle-\langle\bm{q}_{-}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\leq 0.\] (91)

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle-\langle\bm{q}_{n,i}^{( t)},\bm{k}_{n,j}^{(t)}\rangle\] (92) \[=\alpha_{n,i,+}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{n,j}^{(t)} \rangle+\alpha_{n,i,-}^{(t)}\langle\bm{k}_{-}^{(t)},\bm{k}_{n,j}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\langle\bm {k}_{n^{\prime},l}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] \[+\beta_{n,j,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)} \rangle+\beta_{n,j,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{q}_{n,i}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(t)}\langle\bm {q}_{n^{\prime},l}^{(t)},\bm{q}_{n,i}^{(t)}\rangle\] \[+\left(\alpha_{n,i,+}^{(t)}\bm{k}_{+}^{(t)}+\alpha_{n,i,-}^{(t)} \bm{k}_{-}^{(t)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l }^{(t)}\bm{k}_{n^{\prime},l}^{(t)}\right)\] \[\cdot\left(\beta_{n,j,+}^{(t)}\bm{q}_{+}^{(t)\top}+\beta_{n,j,-} ^{(t)}\bm{q}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{ \prime},l}^{(t)}\bm{q}_{n^{\prime},l}^{(t)\top}\right)\] \[=\alpha_{n,i,n,j}^{(t)}\|\bm{k}_{n,j}^{(t)}\|_{2}^{2}+\beta_{n,j, n,i}^{(t)}\|\bm{q}_{n,i}^{(t)}\|_{2}^{2}\] \[+\{lower\;order\;term\}\] \[\leq 0,\]

which completes the proof. The proof for Claim 7 is in Section F.11

#### d.3.3 Proof of Claim 8

By the results of F.9, we have

\[\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle\bm{q}_{+}^{(t)}, \bm{k}_{+}^{(t)}\rangle\leq\frac{\eta C_{10}\|\bm{\mu}\|_{2}^{4}\sigma_{7}^{2} d_{h}\log\left(O(\frac{1}{\epsilon})\right)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t) }\rangle)},\] (93)

Further we have

\[\exp(\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle) \leq\exp\left(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle+ \frac{\eta C_{10}\|\bm{\mu}\|_{2}^{4}\sigma_{7}^{2}d_{h}\log\left(O(\frac{1}{ \epsilon})\right)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)}\right)\] (94) \[=\exp\left(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\right) \cdot\exp\left(\frac{\eta C_{10}\|\bm{\mu}\|_{2}^{4}\sigma_{7}^{2}d_{h}\log \left(O(\frac{1}{\epsilon})\right)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t )}\rangle)}\right)\] \[\leq C_{11}\exp\left(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle\right).\]For the last inequality, by \(\eta\leq\widetilde{O}(\min\{\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d_{ h}^{-\frac{1}{2}})\), \(\sigma_{h}^{2}\leq\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d_{ h}^{-\frac{1}{2}}\cdot\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{-\frac{3}{2}}\), \(\langle\bm{q}_{+}^{(T_{1})},\bm{k}_{+}^{(T_{1})}\rangle=o(1)\) and the monotonicity of \(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\) for \(s\in[T_{1},t]\), we have \(\exp\Big{(}\frac{\eta C_{100}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_{h}\log\big{(}O \frac{(1}{\epsilon})\big{)}}{\exp(\langle\bm{q}_{+}^{(t_{1})},\bm{k}_{+}^{(t_ {1})}\rangle)}\Big{)}\leq\exp(o(1))\leq C_{11}\). Multiplying both sides by \(\Big{(}\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle\bm{q}_{+}^ {(t)},\bm{k}_{+}^{(t)}\rangle\Big{)}\) simultaneously gives

\[\begin{split}&\exp(\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)} \rangle)\Big{(}\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle\bm {q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\Big{)}\\ &\leq C_{11}\exp\Big{(}\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle\Big{)}\cdot\Big{(}\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)} \rangle-\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\Big{)}\\ &\leq\eta C_{12}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_{h}\log\big{(} O(\frac{1}{\epsilon})\big{)},\end{split}\] (95)

where the last inequality is by plugging (93). Taking a summation we have

\[\begin{split}&\int_{(\bm{q}_{+}^{(T_{2})},\bm{k}_{+}^{(T_{2})})}^{ \langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle}\exp(x)dx\\ &\leq\sum_{s=T_{2}}^{t}\exp(\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^ {(s+1)}\rangle)\Big{(}\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle- \langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\Big{)}\\ &\leq\sum_{s=T_{2}}^{t}\eta C_{12}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^ {2}d_{h}\log\big{(}O(\frac{1}{\epsilon})\big{)}\\ &\leq T_{3}\cdot\eta C_{12}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_{h }\log\big{(}O(\frac{1}{\epsilon})\big{)}\\ &=O\Big{(}\frac{d_{h}^{\frac{1}{2}}\log\big{(}O(\frac{1}{\epsilon })\big{)}}{\epsilon(\log(6N^{2}M^{2}/\delta))^{\frac{3}{2}}}\Big{)}\end{split}\] (96)

where the first inequality is due to \(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\) is monotone increasing, the last equality is by \(T_{3}=\Theta(\eta^{-1}\epsilon^{-1}\|\bm{\mu}\|_{2}^{-2}\|\bm{w}_{O}\|_{2}^{-2})\), \(\|\bm{w}_{O}\|_{2}^{2}=\Theta(1)\) and \(\sigma_{h}^{2}\leq\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d_ {h}^{-\frac{1}{2}}\cdot\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{-\frac{3}{2}}\). By \(\int_{(\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)})}^{\langle\bm{q}_{+}^{(t+1)},\bm{k} _{+}^{(t+1)}\rangle}\exp(x)dx=\exp(\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1) }\rangle)-\exp(\langle\bm{q}_{+}^{(T_{2})},\bm{k}_{+}^{(T_{2})}\rangle)\), we have

\[\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle\leq\log\Big{(}\exp( \langle\bm{q}_{+}^{(T_{2})},\bm{k}_{+}^{(T_{2})}\rangle)+O\Big{(}\frac{d_{h}^{ \frac{1}{2}}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon(\log(6N^{2}M^{ 2}/\delta))^{\frac{3}{2}}}\Big{)}\Big{)}\leq\log(\epsilon^{-1}d_{h}^{\frac{1} {2}}),\] (97)

where the last inequality is by \(\langle\bm{q}_{+}^{(T_{2})},\bm{k}_{+}^{(T_{2})}\rangle\leq\log\big{(}d_{h}^{ \frac{1}{2}}\big{)}\). By the results of F.9, we also have

\[\langle\bm{q}_{-}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle-\langle\bm{q}_{-}^{(t)}, \bm{k}_{-}^{(t)}\rangle\leq\frac{\eta C_{10}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_ {h}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\exp(\langle\bm{q}_{-}^{(t)},\bm{k }_{-}^{(t)}\rangle)}.\] (98)

\[\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle-\langle\bm{q}_{\pm}^{(t )},\bm{k}_{n,j}^{(t)}\rangle\geq-\frac{\eta C_{10}\sigma_{p}^{2}d\|\bm{\mu}\|_{2} ^{2}\sigma_{h}^{2}d_{h}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{N}\cdot\exp( \langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{(t)}\rangle).\] (99)

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{\pm}^{(t+1)}\rangle-\langle\bm{q}_{n,i}^{(t )},\bm{k}_{\pm}^{(t)}\rangle\leq\frac{\eta C_{10}\sigma_{p}^{2}d\|\bm{\mu}\|_{2} ^{2}\sigma_{h}^{2}d_{h}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{N\exp(\langle \bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)}\rangle)}.\] (100)

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle-\langle\bm{q}_{n,i}^{(t )},\bm{k}_{n,j}^{(t)}\rangle\geq-\frac{\eta C_{10}\sigma_{p}^{4}d^{2}\sigma_{h }^{2}d_{h}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{N}\cdot\exp(\langle\bm{q}_{n,i }^{(t)},\bm{k}_{n,j}^{(t)}\rangle).\] (101)Then using the similar method as for \(\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle\), we get

\[\langle\bm{q}_{-}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle\leq\log(\epsilon^{-1}d_{h}^{ \frac{1}{2}}),\]

\[\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle\geq-\log(\epsilon^{-1}d_ {h}^{\frac{1}{2}}),\]

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{\pm}^{(t+1)}\rangle\leq\log(\epsilon^{-1}d_ {h}^{\frac{1}{2}}),\]

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle\geq-\log(\epsilon^{-1} d_{h}^{\frac{1}{2}}),\]

Next we provide the upper bound for \(|\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{\mp}^{(t+1)}\rangle|,|\langle\bm{q}_{n,i }^{(t+1)},\bm{k}_{n^{\prime},j}^{(t+1)}\rangle|\). By the results of F.10, we have

\[\sum_{s=T_{2}}^{t}|\beta_{n,+,i}^{(t)}|,\sum_{s=T_{2}}^{t}|\beta_{n,-,i}^{(t) }|=O\Big{(}\frac{\text{SNR}^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)},\] (102)

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\begin{split}&\sum_{s=T_{2}}^{t}|\alpha_{n,+,i}^{(t)}|,\sum_{s=T_ {2}}^{t}|\alpha_{n,-,i}^{(t)}|,\sum_{s=T_{2}}^{t}|\beta_{n,i,+}^{(t)}|,\sum_{s =T_{2}}^{t}|\beta_{n,i,-}^{(t)}|,\sum_{s=T_{2}}^{t}|\beta_{n,i,+}^{(t)}|,\sum_ {s=T_{2}}^{t}|\beta_{n,i,-}^{(t)}|\\ &=O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)}, \end{split}\] (103)

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\begin{split}&\sum_{s=T_{2}}^{t}|\alpha_{n,+,i}^{(t)}|,\sum_{s =T_{2}}^{t}|\alpha_{n,-,i}^{(t)}|,\sum_{s=T_{2}}^{t}|\alpha_{n,i,+}^{(t)}|, \sum_{s=T_{2}}^{t}|\alpha_{n,i,-}^{(t)}|,\sum_{s=T_{2}}^{t}|\alpha_{n,i,n,j}^ {(t)}|,\sum_{s=T_{2}}^{t}|\beta_{n,j,n,i}^{(t)}|\\ &=O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)}, \end{split}\] (104)

for \(i,j\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\begin{split}&\sum_{s=T_{2}}^{t}|\alpha_{n,i,n^{\prime},j}^{(t) }|,\sum_{s=T_{2}}^{t}|\beta_{n,j,n^{\prime},i}^{(t)}|=O\Big{(}\frac{\big{(}\log (6N^{2}M^{2}/\delta)\big{)}^{4}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{ \epsilon d^{\frac{1}{2}}d_{h}^{\frac{1}{2}}}\Big{)}\end{split}\] (105)for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],n\neq n^{\prime}\). Plugging these and proposition \(\mathcal{G}(t)\) into the update rule of \(|\langle\bm{q}_{+}^{(t)},\bm{k}_{\mp}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)}, \bm{k}_{\bar{n},j}^{(t)}\rangle|\) and get

\[|\langle\bm{q}_{+}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle|\leq|\langle \bm{q}_{+}^{(T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|+\sum_{s=T_{2}}^{t}|\langle \bm{q}_{+}^{(s+1)},\bm{k}_{-}^{(s+1)}\rangle-\langle\bm{q}_{+}^{(s)},\bm{k}_{ -}^{(s)}\rangle|\] \[\leq|\langle\bm{q}_{+}^{(T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}\Big{|}\alpha_{+,+}^{(s)}\langle\bm{k}_{+}^{( s)},\bm{k}_{-}^{(s)}\rangle+\sum_{n\in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(s)} \langle\bm{k}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle\] \[+\beta_{-,-}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{-}^{(s)}\rangle +\sum_{n\in S_{-}}\sum_{i=2}^{M}\beta_{n,-,i}^{(s)}\langle\bm{q}_{n,i}^{(s)}, \bm{q}_{+}^{(s)}\rangle\] \[+\Big{(}\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n\in S_{+}}\sum_ {i=2}^{M}\alpha_{n,+,i}^{(s)}\bm{k}_{n,i}^{(s)}\Big{)}\] \[\cdot\Big{(}\beta_{-,-}^{(s)}\bm{q}_{-}^{(s)\top}+\sum_{n\in S_{- }}\sum_{i=2}^{M}\beta_{n,-,i}^{(s)\top}\Big{)}\Big{|}\] \[\leq|\langle\bm{q}_{+}^{(T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\alpha_{+,+}^{(t)}||\langle\bm{k}_{+}^{(t)}, \bm{k}_{-}^{(t)}\rangle|+\sum_{n\in S_{+}}\sum_{i=2}^{M}\sum_{s=T_{2}}^{t}| \alpha_{n,+,i}^{(t)}||\langle\bm{k}_{n,i}^{(t)},\bm{k}_{-}^{(t)}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\beta_{-,-}^{(t)}||\langle\bm{q}_{+}^{(t)}, \bm{q}_{-}^{(t)}\rangle|+\sum_{n\in S_{-}}\sum_{i=2}^{M}\sum_{s=T_{2}}^{t}| \beta_{n,-,i}^{(t)}||\langle\bm{q}_{n,i}^{(t)},\bm{q}_{+}^{(t)}\rangle|\] \[+\{lower\ order\ term\}\] \[=|\langle\bm{q}_{+}^{(T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|\] \[+O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot o(1)+N\cdot M\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)} ^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}} \Big{)}\cdot o(1)\] \[+O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot o(1)+N\cdot M\cdot O\Big{(}\frac{\mathrm{SNR}^{2}\big{(}\log(6N^{2}M^{2} /\delta)\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{ \frac{1}{2}}}\Big{)}\cdot o(1)\] \[=|\langle\bm{q}_{+}^{(T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|+o \Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1} {\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)}+o\Big{(}\frac{N\cdot \mathrm{SNR}^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{ 1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)}\] \[=o(1),\] (106)

where the first inequality is by triangle inequality, the second inequality is by (213), the last equality is by \(|\langle\bm{q}_{+}^{(T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|=o(1)\) and \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\). Similarly we have \[|\langle\bm{q}_{-}^{(i+1)},\bm{k}_{+}^{(i+1)}\rangle|=o(1).\] \[|\langle\bm{q}_{n,i}^{(i+1)},\bm{k}_{\bar{n},j}^{(i+1)}\rangle|\leq| \langle\bm{q}_{n,i}^{(T_{2})},\bm{k}_{\bar{n},j}^{(T_{2})}\rangle|+\sum_{s=T_{2 }}^{t}|\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{\bar{n},j}^{(s+1)}\rangle-\langle\bm {q}_{n,i}^{(s)},\bm{k}_{\bar{n},j}^{(s)}\rangle|\] \[\leq|\langle\bm{q}_{n,i}^{(T_{2})},\bm{k}_{\bar{n},j}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}\Big{|}\alpha_{n,i,+}^{(s)}\langle\bm{k}_{+}^ {(s)},\bm{k}_{\bar{n},j}^{(s)}\rangle+\alpha_{n,i,-}^{(s)}\langle\bm{k}_{-}^ {(s)},\bm{k}_{\bar{n},j}^{(s)}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M} \alpha_{n,i,n^{\prime},l}^{(s)}\langle\bm{k}_{n^{\prime},l}^{(s)},\bm{k}_{ \bar{n},j}^{(s)}\rangle\] \[+\beta_{\bar{n},j,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{n,i}^{ (s)}\rangle+\beta_{\bar{n},j,-}^{(s)}\langle\bm{q}_{-}^{(s)},\bm{q}_{n,i}^{(s) }\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{\bar{n},j,n^{\prime},l}^{( s)}\langle\bm{q}_{n^{\prime},l}^{(s)},\bm{q}_{n,i}^{(s)}\rangle\] \[+\Big{(}\alpha_{n,i,+}^{(s)}\bm{k}_{+}^{(s)}+\alpha_{n,i,-}^{(s) }\bm{k}_{-}^{(s)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime}, l}^{(s)}\bm{k}_{n^{\prime},l}^{(s)}\Big{)}\] \[\cdot\Big{(}\beta_{\bar{n},j,+}^{(s)}\bm{q}_{+}^{(s)\top}+\beta_{ \bar{n},j,-}^{(s)\top}\bm{q}_{-}^{(s)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{ M}\beta_{\bar{n},j,n^{\prime},l}^{(s)\top}\bm{q}_{n^{\prime},l}^{(s)\top}\Big{)}\Big{|}\] \[\leq|\langle\bm{q}_{n,i}^{(T_{2})},\bm{k}_{\bar{n},j}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\alpha_{n,i,+}^{(t)}||\langle\bm{k}_{+}^{(t)}, \bm{k}_{\bar{n},j}^{(t)}\rangle|+\sum_{s=T_{2}}^{t}|\alpha_{n,i,-}^{(t)}|| \langle\bm{k}_{-}^{(t)},\bm{k}_{\bar{n},j}^{(t)}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\alpha_{n,i,\bar{n},j}^{(t)}|||\bm{k}_{\bar{n},j}^{(t)}|^{2}_{2}+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\alpha_{n,i,n,l}^{(t)}|| \langle\bm{k}_{n,l}^{(t)},\bm{k}_{\bar{n},j}^{(t)}\rangle|\] \[+\sum_{n^{\prime}\neq n\wedge(l\neq j\lor n^{\prime}\neq\bar{n})} \sum_{t=T_{2}}^{T_{3}}|\alpha_{n,i,n^{\prime},l}^{(t)}||\langle\bm{k}_{n^{ \prime},l}^{(t)},\bm{k}_{\bar{n},j}^{(t)}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\beta_{\bar{n},j,+}^{(t)}||\langle\bm{q}_{+}^ {(t)},\bm{q}_{n,i}^{(t)}\rangle|+\sum_{s=T_{2}}^{t}|\beta_{\bar{n},j,-}^{(t)}|| \langle\bm{q}_{-}^{(t)},\bm{q}_{n,i}^{(t)}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\beta_{\bar{n},j,n,i}^{(t)}|||\bm{q}_{n,i}^{(t )}||^{2}_{2}+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\beta_{\bar{n},j,\bar{n},l}^{(t)} |||\langle\bm{q}_{\bar{n},l}^{(t)},\bm{q}_{n,i}^{(t)}\rangle|\] \[+\sum_{n^{\prime}\neq\bar{n}\wedge(l\neq i\lor n^{\prime}\neq n)} \sum_{s=T_{2}}^{t}|\beta_{\bar{n},j,n^{\prime},l}^{(t)}||\langle\bm{q}_{n^{ \prime},l}^{(t)},\bm{q}_{n,i}^{(t)}\rangle|\] \[+\{lower order~{}term\}\] \[=|\langle\bm{q}_{n,i}^{(T_{2})},\bm{k}_{\bar{n},j}^{(T_{2})}\rangle|\] \[+O\Big{(}\frac{\left(\log(6N^{2}M^{2}/\delta)\right)^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot o(1)+O\Big{(}\frac{\left(\log(6N^{2}M^{2}/\delta)\right)^{4}\log\big{(}O( \frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)}\cdot\Theta( \sigma_{p}^{2}\sigma_{h}^{2}dd_{h})\] \[+M\cdot O\Big{(}\frac{\left(\log(6N^{2}M^{2}/\delta)\right)^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot o(1)+N\cdot M\cdot O\Big{(}\frac{\left(\log(6N^{2}M^{2}/\delta)\right)^{4} \log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d^{\frac{1}{2}}d_{h}^{\frac{1} {2}}}\Big{)}\cdot o(1)\] \[=|\langle\bm{q}_{n,i}^{(T_{2})},\bm{k}_{\bar{n},j}^{(T_{2})}\rangle|+ o\Big{(}N\frac{\left(\log(6N^{2}M^{2}/\delta)\right)^{3}\log\big{(}O(\frac{1}{ \epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)}\] \[+O\Big{(}\frac{\left(\log(6N^{2}M^{2}/\delta)\right)^{3}\log\big{(}O (\frac{1}{\epsilon})\big{)}}{\epsilon d^{\frac{1}{2}}}\Big{)}+o\Big{(}\frac{N \left(\log(6N^{2}M^{2}/\delta)\right)^{4}\log\big{(}O(\frac{1}{\epsilon}) \big{)}}{\epsilon d^{\frac{1}{2}}d_{h}^{\frac{1}{2}}}\Big{)}\] \[=o(1),\] (107)where the first inequality is by triangle inequality, the second inequality is by (196), the second equality is by \(\sigma_{h}^{2}\leq\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d_{h} ^{-\frac{1}{2}}\cdot\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{-\frac{3}{2}}\), the last equality is by \(|\langle\bm{q}_{n,i}^{(T_{2})},\bm{k}_{\overline{n},j}^{(T_{2})}\rangle|=o(1)\), \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\) and \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2 }\epsilon^{-2}\Big{)}\).

**Lemma D.7** (Convergence of Training Loss).: _There exist \(T=\frac{C_{10}}{\eta\epsilon\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{\mathcal{O}}\|_{2}^{ 2}}\) such that_

\[L_{S}(\theta(T))\leq\epsilon\]

Proof of Lemma D.7.: Recall (68), (69), (70) and (68), we have

\[softmax(\langle\bm{q}_{\pm}^{(T)},\bm{k}_{n,j}^{(t)}\rangle)=O\Big{(}\frac{N \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)},\] (108)

\[softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)=O\Big{(}\frac{N \|\bm{\mu}\|_{2}^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\sigma_{p}^{2} dd_{h}^{\frac{1}{2}}}\Big{)}=O\Big{(}\frac{N\cdot\mathrm{SNR}^{2}\cdot\big{(}\log(6N^{2}M ^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)},\] (109)

\[softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle)=1-O\Big{(}\frac{N \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)},\] (110)

\[softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)}\rangle)=1-O\Big{(}\frac{N \cdot\mathrm{SNR}^{2}\cdot\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{ \frac{1}{2}}}\Big{)}\] (111)

Substituting \(t=T=\frac{C_{10}}{\eta\epsilon\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{\mathcal{O}}\|_{2} ^{2}}\) into propositions \(\mathcal{F}(t)\) and get

\[V_{+}^{(t)} \geq\log\Big{(}\exp(V_{+}^{(T_{2})})+\eta C_{1}\tau\|\bm{\mu}\|_{2 }^{2}\|\bm{w}_{\mathcal{O}}\|_{2}^{2}(t-T_{2})\Big{)}\] \[\geq\log\Big{(}\exp(V_{+}^{(T_{2})})+\frac{C_{20}}{\epsilon}\Big{)}\] (112) \[\geq\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)},\]

\[|V_{n,i}^{(t)}|=O(1).\] (113)For \(n\in S_{+}\), we bound \(f(\bm{X}_{n},\theta(t))\) as follows

\[\begin{split} f(\bm{X}_{n},\theta(t))&=\frac{1}{M} \sum\limits_{l=1}^{M}\bm{\varphi}(\bm{x}_{n,l}^{\top}\bm{W}_{Q}\bm{W}_{K}^{\top} (\bm{X}_{n})^{\top})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\\ &\frac{1}{M}\cdot\Big{(}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t) },\bm{k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)} \rangle)}\\ &+\sum\limits_{i=2}^{M}\frac{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{ k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t) }\rangle)}\\ &+\sum\limits_{i=2}^{M}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t) },\bm{k}_{n,i}^{(t)}\rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t) }\rangle)}\\ &+\sum\limits_{l=2}^{M}\frac{\exp(\langle\bm{q}_{n,l}^{(t)},\bm{ k}_{n,i}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,l}^{(t)},\bm{k}_{+}^{(t)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,l}^{(t)},\bm{k}_{n,j}^{(t) }\rangle)}\cdot V_{n,i}^{(t)}\Big{)}\\ &\geq\frac{1}{M}\cdot\Big{(}1-O\Big{(}\frac{N\big{(}\log(6N^{2} M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)}\\ &+(M-1)\cdot\Big{(}1-O\big{(}\frac{N\cdot\text{SNR}^{2}\cdot \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\big{)}\Big{)} \Big{)}\cdot\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)}\\ &-\frac{1}{M}\cdot(M-1)\cdot\Big{(}O\Big{(}\frac{N\big{(}\log(6N^ {2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)}\\ &+(M-1)\cdot O\big{(}\frac{N\cdot\text{SNR}^{2}\cdot\big{(}\log(6N ^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\big{)}\Big{)}\cdot O(1)\\ &=\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)}-O\Big{(}\frac{N\cdot \text{SNR}^{2}\cdot\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\Big{(}\frac{ C_{20}}{\epsilon}\Big{)}}{d_{h}^{\frac{1}{2}}}\Big{)}\\ &\geq\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)}-\log(C_{20})\\ &\geq\log\Big{(}\frac{1}{\epsilon}\Big{)},\end{split}\] (114)

where the first inequality is by plugging (108), (109), (110) and (111). For the second inequality, by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\text{SNR}^{4},\text{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\), we have \(O\Big{(}\frac{N\cdot\text{SNR}^{2}\cdot\big{(}\log(6N^{2}M^{2}/\delta)\big{)} ^{3}\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)}}{d_{h}^{\frac{1}{2}}}\Big{)}=o( 1)=\log(1+o(1))\leq\log(C_{20})\) as long as \(C_{20}\) is sufficiently large. Then we have

\[\begin{split}\ell_{n}^{(t)}&=\log\Big{(}1+\exp(-f( \bm{X}_{n},\theta(t)))\Big{)}\\ &\leq\exp(-f(\bm{X}_{n},\theta(t)))\\ &\leq\exp\Big{(}-\log\Big{(}\frac{1}{\epsilon}\Big{)}\Big{)}\\ &\leq\epsilon.\end{split}\] (115)

Similarly, we have \(\ell_{n}^{(t)}\leq\epsilon\) for \(n\in S_{-}\). Therefore, we have \(L_{S}(\theta(T))=\frac{1}{N}\sum\limits_{n=1}^{N}\ell_{n}^{(t)}\leq\epsilon\).

### Population Loss

Consider a new data point \((\bm{X},y)\) generated from the distribution defined in Definition 3.1. Without loss of generality, we suppose that the signal token is \(\bm{\mu}_{+}\) and the label is 1, i.e. \(\bm{X}=(\bm{\mu}_{+},\bm{\xi}_{2},\ldots,\bm{\xi}_{M}),\,y=1\). We will first give the bounds of attention, then the bounds of the output of ViT, and finally the bound of the expectation of loss.

**Lemma D.8** (Bound of Attention).: _Under the same conditions as Theorem 4.1, with probability at least \(1-\delta/2N^{2}M\), we have that_

\[\frac{\exp(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(T_{3})\top}\bm {\mu}_{+})}{\exp(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(T_{3}) \top}\bm{\mu}_{+})+\sum\limits_{i=2}^{M}\exp(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{( T_{3})}\bm{W}_{K}^{(T_{3})\top}\bm{\xi}_{i})}\geq\frac{1}{2M}.\]

Proof of Lemma D.8.: By the result of D.2 and D.3, \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{1})}\bm{W}_{K}^{(T_{1})\top}\bm{\mu}_{+}=o(1)\), and \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}\bm{\mu}_{+}\) is monotonically non-decreasing for \(t\in[T_{1},T_{3}]\), we have \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(T_{3})\top}\bm{\mu}_{+} \geq-o(1)\).

By position \(\mathcal{G}(t)\) in D.3, we have \(\|\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\|_{2}^{2}=\Theta\big{(}\|\bm{\mu} \|_{2}^{2}\sigma_{n}^{2}d_{h}\big{)}\). In order to give bounds for \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(T_{3})\top}\bm{\xi}_{i}\), we provide a bound for the F-norm of \(\bm{W}_{K}^{(T_{3})}-\bm{W}_{K}^{(0)}\). According to the properties of F-norm, we have

\[\|\bm{W}_{K}^{(T_{3})}-\bm{W}_{K}^{(0)}\|_{F}\leq\|\bm{W}_{K}^{(T_{1})}-\bm{W} _{K}^{(0)}\|_{F}+\|\bm{W}_{K}^{(T_{2})}-\bm{W}_{K}^{(T_{1})}\|_{F}+\|\bm{W}_{ K}^{(T_{3})}-\bm{W}_{K}^{(T_{2})}\|_{F}\] (116)

Next, we will provide the bounds of \(\|\bm{W}_{K}^{(T_{1})}-\bm{W}_{K}^{(0)}\|_{F}\), \(\|\bm{W}_{K}^{(T_{2})}-\bm{W}_{K}^{(T_{1})}\|_{F}\), and \(\|\bm{W}_{K}^{(T_{3})}-\bm{W}_{K}^{(T_{2})}\|_{F}\) respectively.

**Bounding \(\|\bm{W}_{K}^{(T_{1})}-\bm{W}_{K}^{(0)}\|_{F}\):** Recall that

\[\nabla_{\bm{W}_{K}}L_{S}(\theta) =\frac{1}{NM}\sum_{n\in S_{+}}\ell_{n}^{\prime}(\theta)(\bm{X}_{n }^{\top}(diag(\bm{\varphi}_{n,1})-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1 })\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\mu}_{+}^{\top}\] \[+\sum_{i=2}^{M}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{ \varphi}_{n,i}^{\top}\bm{\varphi}_{n,i})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{ \xi}_{n,i}^{\top})\bm{W}_{Q}\] \[-\frac{1}{NM}\sum_{n\in S_{-}}\ell_{n}^{\prime}(\theta)(\bm{X}_{n }^{\top}(diag(\bm{\varphi}_{n,1})-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1 })\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\mu}_{-}^{\top}\] \[+\sum_{i=2}^{M}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{ \varphi}_{n,i}^{\top}\bm{\varphi}_{n,i})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\xi }_{n,i}^{\top})\bm{W}_{Q},\]

so we have

\[\|\bm{W}_{K}^{(t+1)}-\bm{W}_{K}^{(t)}\|_{F}=\|\eta\nabla_{\bm{W}_{ K}}L_{S}(\theta(t))\|_{F}\] \[\leq\frac{\eta}{NM}\sum_{n\in S_{+}}|\ell_{n}^{\prime(t)}\big{(}\| \bm{X}_{n}^{(t)\top}\|_{F}\|(diag(\bm{\varphi}_{n,1}^{(t)})-\bm{\varphi}_{n,1 }^{(t)\top}\bm{\varphi}_{n,1}^{(t)})\|_{F}\|\bm{X}_{n}\bm{W}_{V}^{(t)}\bm{w}_{O }\|_{F}\|\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(t)}\|_{F}\] \[+\sum_{i=2}^{M}\|\bm{X}_{n}^{\top}\|_{F}\|(diag(\bm{\varphi}_{n,i }^{(t)})-\bm{\varphi}_{n,i}^{(t)\top}\bm{\varphi}_{n,i}^{(t)})\|_{F}\|\bm{X}_{ n}\bm{W}_{V}^{(t)}\bm{w}_{O}\|_{F}\|\bm{\xi}_{n,i}^{\top}\bm{W}_{Q}^{(t)}\|_{F}\big{)}\] \[+\frac{\eta}{NM}\sum_{n\in S_{-}}|\ell_{n}^{\prime(t)}\big{(}\| \bm{X}_{n}^{\top}\|_{F}\|(diag(\bm{\varphi}_{n,1}^{(t)})-\bm{\varphi}_{n,1}^{(t) \top}\bm{\varphi}_{n,1}^{(t)})\|_{F}\|\bm{X}_{n}\bm{W}_{V}^{(t)}\bm{w}_{O}\|_{F }\|\bm{\mu}_{-}^{\top}\bm{W}_{Q}^{(t)}\|_{F}\] \[+\sum_{i=2}^{M}\|\bm{X}_{n}^{\top}\|_{F}\|(diag(\bm{\varphi}_{n,i}^ {(t)})-\bm{\varphi}_{n,i}^{(t)\top}\bm{\varphi}_{n,i}^{(t)})\|_{F}\|\bm{X}_{n} \bm{W}_{V}^{(t)}\bm{w}_{O}\|_{F}\|\bm{\xi}_{n,i}^{\top}\bm{W}_{Q}^{(t)}\|_{F}\big{)}\] (117)

We bound \(|\ell_{n}^{\prime(t)}|\), \(\|\bm{X}_{n}\|_{F}\), \(\|(diag(\bm{\varphi}_{n,i}^{(t)})-\bm{\varphi}_{n,i}^{(t)\top}\bm{\varphi}_{n,i}^{(t) })\|_{F}\), \(\|\bm{X}_{n}\bm{W}_{V}^{(t)}\bm{w}_{O}\|_{F}\), \(\|\bm{\mu}_{\pm}^{\top}\bm{W}_{Q}^{(t)}\|_{F}\) and \(\|\bm{\xi}_{n,i}^{\top}\bm{W}_{Q}^{(t)}\|_{F}\) respectively.

* \(|\ell_{n}^{\prime(t)}|\leq 1\)
* By Lemma C.4, \(\|\bm{X}_{n}\|_{F}\leq\sqrt{\|\bm{\mu}\|_{2}^{2}+(M-1)\frac{3\sigma_{p}^{2}d}{2}} =O\big{(}\max\{\|\bm{\mu}\|_{2},\sigma_{p}\sqrt{d}\}\big{)}\)
* Note that each element of vector \(\bm{\varphi}_{n,i}\) has an upper bound 1, and the number of tokens has a constant upper bound (\(M=\Theta(1)\)), then we have \(\|(diag(\bm{\varphi}_{n,i}^{(t)})-\bm{\varphi}_{n,i}^{(t)\top}\bm{\varphi}_{n, i}^{(t)})\|_{F}=O(1)\).
* By Lemma D.3, we have \(|V_{+}^{(t)}|,|V_{-}^{(t)}|,|V_{n,i}^{(t)}|=O(d_{h}^{-\frac{1}{4}})\), so we have \[\|\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\|_{F}=\sqrt{\big{(}V_{+}^{(t)}\big{)}^{2}+ \sum_{i=2}^{M}\big{(}V_{n,i}^{(t)}\big{)}^{2}}=\sqrt{M\cdot\big{(}d_{h}^{- \frac{1}{4}}\big{)}^{2}}=O\big{(}d_{h}^{-\frac{1}{4}}\big{)}\]
* By Lemma D.4, we have \(\|\bm{\mu}_{\pm}^{\top}\bm{W}_{Q}^{(t)}\|_{F}=\Theta\big{(}\|\bm{\mu}\|_{2} \sigma_{h}d_{h}^{\frac{1}{2}}\big{)}\), \(\|\bm{\xi}_{n,i}^{\top}\bm{W}_{Q}^{(t)}\|_{F}=\Theta\big{(}\sigma_{p}\sigma_{h }d^{\frac{1}{2}}d_{h}^{\frac{1}{2}}\big{)}\)

Plugging them into (117) and get

\[\begin{split}\|\bm{W}_{K}^{(t+1)}-\bm{W}_{K}^{(t)}\|_{F}& \leq\frac{\eta}{NM}\cdot NM\cdot O\big{(}\max\{\|\bm{\mu}\|_{2}, \sigma_{p}\sqrt{d}\}\big{)}\cdot O\big{(}d_{h}^{-\frac{1}{4}}\big{)}\cdot O \big{(}\max\{\|\bm{\mu}\|_{2},\sigma_{p}\sqrt{d}\}\sigma_{h}d_{h}^{\frac{1}{2 }}\big{)}\\ &=O\big{(}\eta\cdot\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\} \cdot\sigma_{h}d_{h}^{\frac{1}{4}}\big{)}\end{split}\] (118)

Taking a summation we have

\[\begin{split}\|\bm{W}_{K}^{(T_{1})}-\bm{W}_{K}^{(0)}\|_{F}& \leq\sum_{s=0}^{T_{1}-1}\|\bm{W}_{K}^{(t+1)}-\bm{W}_{K}^{(t)}\|_{F} \\ &\leq O\big{(}\frac{1}{\eta d_{h}^{\frac{1}{4}}\|\bm{\mu}\|_{2}^{ 2}\|\bm{w}_{O}\|_{2}^{2}}\big{)}\cdot O\big{(}\eta\cdot\max\{\|\bm{\mu}\|_{2} ^{2},\sigma_{p}^{2}d\}\cdot\sigma_{h}d_{h}^{\frac{1}{4}}\big{)}\\ &=O\Big{(}\frac{\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}}{\| \bm{\mu}\|_{2}^{2}}\cdot\sigma_{h}\Big{)}\\ &\leq O\Big{(}N\sigma_{h}\Big{)},\end{split}\] (119)

where the last inequality is by \(N\cdot\mathrm{SNR}^{2}=\Omega(1)\).

**Bounding \(\|\bm{W}_{K}^{(T_{2})}-\bm{W}_{K}^{(T_{1})}\|_{F}\):** By F.7, we know that each element of matrix \((diag(\bm{\varphi}_{n,1}^{(t)})-\bm{\varphi}_{n,1}^{(t)\top}\bm{\varphi}_{n,1}^{ (t)})\) have upper bound \(O\Big{(}\frac{1}{1+\frac{\eta^{2}|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h }^{\frac{1}{2}}}{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t -T_{1}-1)}\Big{)}\), and each element of matrix \((diag(\bm{\varphi}_{n,i}^{(t)})-\bm{\varphi}_{n,i}^{(t)\top}\bm{\varphi}_{n,i}^ {(t)})\) have upper bound \(O\Big{(}\frac{1}{1+\frac{\eta^{2}\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h }^{\frac{1}{2}}}{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t -T_{1}-1)}\Big{)}\). Considering that the number of tokens has a constant upper bound (\(M=\Theta(1)\)), we have

\[\|(diag(\bm{\varphi}_{n,1}^{(t)})-\bm{\varphi}_{n,1}^{(t)\top}\bm{ \varphi}_{n,1}^{(t)})\|_{F}=O\Big{(}\frac{1}{1+\frac{\eta^{2}\|\bm{\mu}\|_{2}^{ 4}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{2}}\cdot(t-T_{1})(t-T_{1}-1)}\Big{)}\] (120) \[\|(diag(\bm{\varphi}_{n,i}^{(t)})-\bm{\varphi}_{n,i}^{(t)\top}\bm{ \varphi}_{n,i}^{(t)})\|_{F}=O\Big{(}\frac{1}{1+\frac{\eta^{2}\sigma_{p}^{2}d\| \bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N\big{(}\log(6N^{2} M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}-1)}\Big{)}\] (121)

By \(\mathcal{B}(t)\) in D.2, we have \(|V_{\pm}^{(t)}|,|V_{n,i}^{(t)}|\leq O(d_{h}^{-\frac{1}{4}})+\eta C_{4}\|\bm{\mu}\|_{2 }^{2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{1})\) for \(t\in[T_{1},T_{2}]\). We further have

\[\|\bm{X}_{n}\bm{W}_{V}^{(t)}\bm{w}_{O}\|_{F}=O\Big{(}d_{h}^{-\frac{1}{4}}+\eta\| \bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(t-T_{1})\Big{)}.\]Then we have

\[\|(diag(\boldsymbol{\varphi}_{n,1}^{(t)})-\boldsymbol{\varphi}_{n,1}^ {(t)\top}\boldsymbol{\varphi}_{n,1}^{(t)})\|_{F}\cdot\|\boldsymbol{X}_{n}\boldsymbol {W}_{V}^{(t)}\boldsymbol{w}_{O}\|_{F}\] \[=O\Big{(}\frac{1}{1+\frac{\eta^{2}\|\boldsymbol{\mu}\|_{2}^{2}\| \boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N\left(\log(6N^{2}M^{2}/\delta )\right)^{2}}\cdot(t-T_{1})(t-T_{1}-1)}\Big{)}\cdot O\Big{(}d_{h}^{-\frac{1}{ 4}}+\eta\|\boldsymbol{\mu}\|_{2}^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}(t-T_{1}) \Big{)}\] \[=O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/ \delta)\Big{)},\] (122)

The reason for the last equation is similar to (256). We also have

\[\|(diag(\boldsymbol{\varphi}_{n,i}^{(t)})-\boldsymbol{\varphi}_{n,i}^{(t)\top}\boldsymbol{\varphi}_{n,i}^{(t)})\|_{F}\cdot\|\boldsymbol{X}_{n} \boldsymbol{W}_{V}^{(t)}\boldsymbol{w}_{O}\|_{F}\] \[=O\Big{(}\frac{1}{1+\frac{\eta^{2}\sigma_{p}^{2}d\|\boldsymbol{ \mu}\|_{2}^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N\left(\log(6N ^{2}M^{2}/\delta)\right)^{2}}\cdot(t-T_{1})(t-T_{1}-1)}\Big{)}\cdot O\Big{(}d _{h}^{-\frac{1}{4}}+\eta\|\boldsymbol{\mu}\|_{2}^{2}\|\boldsymbol{w}_{O}\|_{2} ^{2}(t-T_{1})\Big{)}\] \[=O\Big{(}\frac{\|\boldsymbol{\mu}\|_{2}}{\sigma_{p}\sqrt{d}}N^{ \frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)}.\] (123)

Plugging them into (117) and get

\[\|\boldsymbol{W}_{K}^{(t+1)}-\boldsymbol{W}_{K}^{(t)}\|_{F} \leq\frac{\eta}{NM}\cdot N\cdot O\big{(}\max\{\|\boldsymbol{\mu} \|_{2},\sigma_{p}\sqrt{d}\}\big{)}\cdot\Big{(}O\Big{(}N^{\frac{1}{2}}d_{h}^{- \frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)}\cdot O\big{(}\|\boldsymbol{\mu} \|_{2}\sigma_{h}d_{h}^{\frac{1}{4}}\big{)}\] \[+(M-1)\cdot O\Big{(}\frac{\|\boldsymbol{\mu}\|_{2}}{\sigma_{p} \sqrt{d}}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)} \cdot O\big{(}\sigma_{p}\sqrt{d}\sigma_{h}d_{h}^{\frac{1}{4}}\big{)}\Big{)}\] \[=O\Big{(}\eta\cdot\max\{\|\boldsymbol{\mu}\|_{2},\sigma_{p}\sqrt{ d}\}N^{\frac{1}{2}}\|\boldsymbol{\mu}\|_{2}\sigma_{h}d_{h}^{\frac{1}{4}}\log(6N^{2}M^{ 2}/\delta)\Big{)}\]

Taking a summation we have

\[\|\boldsymbol{W}_{K}^{(T_{2})}-\boldsymbol{W}_{K}^{(T_{1})}\|_{F} \leq\sum_{s=T_{1}}^{T_{2}-1}\|\boldsymbol{W}_{K}^{(t+1)}-\boldsymbol {W}_{K}^{(t)}\|_{F}\] \[\leq O\big{(}\frac{1}{\eta\|\boldsymbol{\mu}\|_{2}^{2}\| \boldsymbol{w}_{O}\|_{2}^{2}}\big{)}\cdot O\Big{(}\eta\max\{\|\boldsymbol{\mu} \|_{2},\sigma_{p}\sqrt{d}\}N^{\frac{1}{2}}\|\boldsymbol{\mu}\|_{2}\sigma_{h}d_ {h}^{\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)}\] \[=O\Big{(}\frac{\max\{\|\boldsymbol{\mu}\|_{2},\sigma_{p}\sqrt{d} \}}{\|\boldsymbol{\mu}\|_{2}}\cdot N^{\frac{1}{2}}\sigma_{h}d_{h}^{\frac{1}{ 4}}\log(6N^{2}M^{2}/\delta)\Big{)}\] \[\leq O\Big{(}N^{\frac{1}{2}}(1+\mathrm{SNR}^{-1})\sigma_{h}d_{h}^ {\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)},\] (125)

**Bounding \(\|\boldsymbol{W}_{K}^{(T_{3})}-\boldsymbol{W}_{K}^{(T_{2})}\|_{F}\):** Recall that ((64), (65))

\[\frac{\exp((\boldsymbol{\langle q}_{\pm}^{(t)},\boldsymbol{k}_{n, j}^{(t)}))}{\exp((\boldsymbol{\langle q}_{\pm}^{(t)},\boldsymbol{k}_{n,j^{ (t)}}^{(t)}))+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{\langle q}_{\pm}^{( t)},\boldsymbol{k}_{n,j^{(t)}}^{(t)}\rangle)}=O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)},\] \[\frac{\exp(\langle\boldsymbol{\langle q}_{n,i}^{(t)},\boldsymbol {k}_{n,j}^{(t)}\rangle)}{\exp(\langle\boldsymbol{\langle q}_{n,i}^{(t)}, \boldsymbol{k}_{n,j}^{(t)}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle \boldsymbol{\langle q}_{n,i}^{(t)},\boldsymbol{k}_{n,j^{\prime}}^{(t)}\rangle)}=O \Big{(}\frac{N\|\boldsymbol{\mu}\|_{2}^{2}\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{3}}{\sigma_{p}^{2}dd_{h}^{\frac{1}{2}}}\Big{)}\]

for \(i,j\in[M]\backslash\{1\},n\in\mathbb{S}_{\pm},t\in[T_{2},T_{3}]\). We have

\[\|(diag(\boldsymbol{\varphi}_{n,1}^{(t)})-\boldsymbol{\varphi}_{n,1}^{(t)\top} \boldsymbol{\varphi}_{n,1}^{(t)})\|_{F}=O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)},\] (126)\[\|(diag(\bm{\varphi}_{n,i}^{(t)})-\bm{\varphi}_{n,i}^{(t)\top}\bm{\varphi}_{n,i}^{( t)})\|_{F}=O\Big{(}\frac{N\|\bm{\mu}\|_{2}^{2}\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{3}}{\sigma_{p}^{2}dd_{h}^{\frac{1}{2}}}\Big{)}.\] (127)

By \(\mathcal{F}(t)\) in D.3, we have

\[|V_{\pm}^{(t)}|\leq 2\log\big{(}O(\frac{1}{\epsilon})\big{)}\]

\[|V_{n,i}^{(t)}|=O(1)\]

we further have

\[\|\bm{X}_{n}\bm{W}_{V}^{(t)}\bm{w}_{O}\|_{F}=O\Big{(}\log O\big{(}\frac{1}{ \epsilon}\big{)}\Big{)}.\]

Plugging them into (117) and get

\[\|\bm{W}_{K}^{(t+1)}-\bm{W}_{K}^{(t)}\|_{F} \leq\frac{\eta}{NM}\cdot N\cdot O\big{(}\max\{\|\bm{\mu}\|_{2}, \sigma_{p}\sqrt{d}\}\big{)}\cdot\Big{(}O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2} /\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)}\cdot O\big{(}\|\bm{\mu}\|_{2} \sigma_{h}d_{h}^{\frac{1}{2}}\big{)}\] (128)

Taking a summation we have

\[\|\bm{W}_{K}^{(T_{3})}-\bm{W}_{K}^{(T_{2})}\|_{F}\leq\sum_{s=T_{2 }}^{T_{3}-1}\|\bm{W}_{K}^{(t+1)}-\bm{W}_{K}^{(t)}\|_{F}\] \[\leq O\big{(}\frac{1}{\eta\epsilon\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{ O}\|_{2}^{2}}\big{)}\cdot O\Big{(}\eta\cdot\max\{\|\bm{\mu}\|_{2}^{2}, \sigma_{p}^{2}d\}\cdot\frac{N\|\bm{\mu}\|_{2}\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}\sigma_{h}d_{h}^{\frac{1}{2} }}{\sigma_{p}d^{\frac{1}{2}}d_{h}^{\frac{1}{2}}}\Big{)}\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\frac{N \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon}) \big{)}\sigma_{h}}{\epsilon\|\bm{\mu}\|_{2}\sigma_{p}d^{\frac{1}{2}}}\Big{)}.\] (129)

Combining (116), (119), (125) and (129) and get

\[\|\bm{W}_{K}^{(T_{3})}-\bm{W}_{K}^{(0)}\|_{F}\leq\|\bm{W}_{K}^{(T_ {1})}-\bm{W}_{K}^{(0)}\|_{F}+\|\bm{W}_{K}^{(T_{2})}-\bm{W}_{K}^{(T_{1})}\|_{F} +\|\bm{W}_{K}^{(T_{3})}-\bm{W}_{K}^{(T_{2})}\|_{F}\] \[=O\Big{(}N\sigma_{h}\Big{)}+O\Big{(}N^{\frac{1}{2}}(1+\mathrm{SNR} ^{-1})\sigma_{h}d_{h}^{\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)}\] \[+O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\frac{N \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon}) \big{)}\sigma_{h}}{\epsilon\|\bm{\mu}\|_{2}\sigma_{p}d^{\frac{1}{2}}}\Big{)}\] \[=O\Big{(}\sigma_{h}d_{h}^{\frac{1}{2}}\Big{)},\] (130)

where the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\).

**Bounding \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(T_{3})\top}\bm{\xi}_{i}\):** We decompose \(\bm{W}_{K}^{(T_{3})}\) into \(\bm{W}_{K}^{(0)}+\Big{(}\bm{W}_{K}^{(T_{3})}-\bm{W}_{K}^{(0)}\Big{)}\), then we have

\[\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(T_{3})\top}\bm{\xi}_{i}=\bm {\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(0)\top}\bm{\xi}_{i}+\bm{\mu}_{+ }^{\top}\bm{W}_{Q}^{(T_{3})}\Big{(}\bm{W}_{K}^{(T_{3})\top}-\bm{W}_{K}^{(0)\top }\Big{)}\bm{\xi}_{i}\]

Now we bound \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(0)\top}\bm{\xi}_{i}\) and \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\Big{(}\bm{W}_{K}^{(T_{3})\top}-\bm{W}_{K }^{(0)\top}\Big{)}\bm{\xi}_{i}\) respectively.

**Bounding \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(0)\top}\bm{\xi}_{i}\):** Note that \(\|\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\|_{2}^{2}=\Theta\big{(}\|\bm{\mu}\|_{2} ^{2}\sigma_{h}^{2}d_{h}\big{)}\) and each element of \(\bm{W}_{K}^{(0)\top}\) is sampled from a Gaussian distribution \(\mathcal{N}(0,\sigma_{h}^{2})\), we have that each element of \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(0)\top}\) is a random variable with mean zero and standard deviation smaller than \(C_{21}\|\bm{\mu}\|_{2}\sigma_{h}^{2}d_{h}^{\frac{1}{2}}\). Therefore, for any \(i\in[M]\backslash\{1\}\) by Bernstein's inequality, with probability at least \(1-\delta/3N^{2}M^{2}\),

\[\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(0)\top}\bm{\xi}_{i}=O(\| \bm{\mu}\|_{2}\sigma_{p}\sigma_{h}^{2}d_{h}^{\frac{1}{2}}\sqrt{d\log(6N^{2}M^{2 }/\delta)})=o(1),\]

where the last equality is by \(\sigma_{h}^{2}\leq\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d_{ h}^{-\frac{1}{2}}\cdot\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{-\frac{3}{2}}\).

**Bounding \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\Big{(}\bm{W}_{K}^{(T_{3})\top}-\bm{W}_ {K}^{(0)\top}\Big{)}\bm{\xi}_{i}\):** By (130), we have

\[\begin{split}&\Big{\|}\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})} \Big{(}\bm{W}_{K}^{(T_{3})\top}-\bm{W}_{K}^{(0)\top}\Big{)}\Big{\|}_{2}\leq\| \bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\|_{2}\|\bm{W}_{K}^{(T_{3})\top}-\bm{W} _{K}^{(0)\top}\|_{F}\\ &\leq C_{22}\|\bm{\mu}\|_{2}\sigma_{h}^{2}d_{h}.\end{split}\] (131)

Therefore \(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\Big{(}\bm{W}_{K}^{(T_{3})\top}-\bm{W}_ {K}^{(0)\top}\Big{)}\bm{\xi}_{i}\) is a Gaussian distribution with mean zero and standard deviation smaller than \(C_{22}\|\bm{\mu}\|_{2}\sigma_{p}\sigma_{h}^{2}d_{h}\). By Gaussian tail bound, for any \(i\in[M]\backslash\{1\}\), with probability at least \(1-\delta/6N^{2}M^{2}\),

\[\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\Big{(}\bm{W}_{K}^{(T_{3})\top}-\bm{W}_ {K}^{(0)\top}\Big{)}\bm{\xi}_{i}\leq C_{22}\|\bm{\mu}\|_{2}\sigma_{p}\sigma_{h }^{2}d_{h}\sqrt{2\log\big{(}12N^{2}M^{2}/\delta\big{)}}=o(1),\]

where the last equality is by \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\) and \(\sigma_{h}^{2}\leq\min\{\|\bm{\mu}\|_{2}^{-2},(\sigma_{p}^{2}d)^{-1}\}\cdot d_ {h}^{-\frac{1}{2}}\cdot\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{-\frac{3}{2}}\). Applying a union bound, with probability at least \(1-\delta/2N^{2}M\),

\[\begin{split}\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{ (T_{3})\top}\bm{\xi}_{i}&=\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})} \bm{W}_{K}^{(0)\top}\bm{\xi}_{i}+\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})}\Big{(} \bm{W}_{K}^{(T_{3})\top}-\bm{W}_{K}^{(0)\top}\Big{)}\bm{\xi}_{i}\\ &=o(1)+o(1)\\ &=o(1).\end{split}\]

Therefore, we have

\[\begin{split}&\frac{\exp(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_{3})} \bm{W}_{K}^{(T_{3})\top}\bm{\mu}_{+})}{\exp(\bm{\mu}_{+}^{\top}\bm{W}_{Q}^{(T_ {3})}\bm{W}_{K}^{(T_{3})\top}\bm{\mu}_{+})+\sum\limits_{i=2}^{M}\exp(\bm{\mu}_{+ }^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(T_{3})\top}\bm{\xi}_{i})}\geq\frac{ \exp(-o(1))}{\exp(-o(1))+(M-1)\exp(o(1))}\\ &\geq\frac{1}{1+(M-1)\exp(o(1))}\\ &\geq\frac{1}{1+(M-1)+(M-1)o(1)}\\ &\geq\frac{1}{M}-o(1)\\ &\geq\frac{1}{2M},\end{split}\]

which complete the proof.

**Lemma D.9** (Bound of V).: _Under the same conditions as Theorem 4.1, with probability at least \(1-\delta/2N^{2}M\), we have that_

\[|\bm{\xi}_{i}^{\top}\bm{W}_{V}^{(T_{3})}\bm{w}_{O}|\leq 1\]

Proof of Lemma D.9.: By (9), we have

\[\begin{split}\bm{W}_{V}^{(t+1)}\bm{w}_{O}-\bm{W}_{V}^{(t)}\bm{w}_{ O}&=-\frac{\eta}{NM}\sum\limits_{n=1}^{N}y_{n}\ell_{n}^{\prime(t)}[\bm{w}_{ O}\sum\limits_{l=1}^{M}\bm{\varphi}(\bm{x}_{n,l}\bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t) \top}\bm{X}_{n}^{\top})\bm{X}_{n}]^{\top}\bm{w}_{O}\\ &=-\frac{\eta}{NM}\sum\limits_{n=1}^{N}y_{n}\ell_{n}^{\prime(t)} \sum\limits_{l=1}^{M}\bm{X}_{n}^{\top}\bm{\varphi}(\bm{x}_{n,l}\bm{W}_{Q}^{(t)} \bm{W}_{K}^{(t)\top}\bm{X}_{n}^{\top})^{\top}\|\bm{w}_{O}\|_{2}^{2}.\end{split}\] (132)Then we have

\[\begin{split}\|\bm{W}_{V}^{(t+1)}\bm{w}_{O}-\bm{W}_{V}^{(t)}\bm{w}_{O} \|_{2}&\leq\frac{\eta}{NM}\sum_{n=1}^{N}\big{|}y_{n}\ell_{n}^{ \prime(t)}\big{|}\sum_{l=1}^{M}\big{\|}\bm{X}_{n}\big{\|}_{F}\big{\|}\bm{\varphi }(\bm{x}_{n,l}\bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}\bm{X}_{n}^{\top})\big{\|}_{2 }\|\bm{w}_{O}\|_{2}^{2}\\ &\leq\frac{\eta}{NM}\cdot NM\cdot O\big{(}\max\{\|\bm{\mu}\|_{2}, \sigma_{p}\sqrt{d}\}\big{)}\cdot O(1)\cdot\|\bm{w}_{O}\|_{2}^{2}\\ &=O\Big{(}\eta\cdot\max\{\|\bm{\mu}\|_{2},\sigma_{p}\sqrt{d}\} \cdot\|\bm{w}_{O}\|_{2}^{2}\Big{)},\end{split}\] (133)

where the second inequality is by \(\big{|}y_{n}\ell_{n}^{\prime(t)}\big{|}\leq 1\), \(\|\bm{X}_{n}\|_{F}=O\big{(}\max\{\|\bm{\mu}\|_{2},\sigma_{p}\sqrt{d}\}\big{)}\) and \(\big{\|}\bm{\varphi}(\bm{x}_{n,l}\bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}(\bm{X}_ {n})^{\top})\big{\|}_{2}=O(1)\). Taking a summation and get

\[\begin{split}\|\bm{W}_{V}^{(T_{3})}\bm{w}_{O}\|_{2}& \leq\|\bm{W}_{V}^{(0)}\bm{w}_{O}\|_{2}+\sum_{t=0}^{T_{3}-1}\|\bm{W} _{V}^{(t+1)}\bm{w}_{O}-\bm{W}_{V}^{(t)}\bm{w}_{O}\|_{2}\\ &=\|\bm{W}_{V}^{(0)}\bm{w}_{O}\|_{2}+O\big{(}\frac{1}{\eta\epsilon \|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}}\big{)}\cdot O\Big{(}\eta\cdot\max\{ \|\bm{\mu}\|_{2},\sigma_{p}\sqrt{d}\}\cdot\|\bm{w}_{O}\|_{2}^{2}\Big{)}\\ &=O\Big{(}\sigma_{V}\|\bm{w}_{O}\|_{2}\sqrt{d}\Big{)}+O\Big{(} \frac{\max\{\|\bm{\mu}\|_{2},\sigma_{p}\sqrt{d}\}}{\epsilon\|\bm{\mu}\|_{2}^{2 }}\Big{)}\\ &=O\Big{(}\sigma_{V}\|\bm{w}_{O}\|_{2}\sqrt{d}+\frac{\max\{\|\bm{ \mu}\|_{2},\sigma_{p}\sqrt{d}\}}{\epsilon\|\bm{\mu}\|_{2}^{2}}\Big{)}.\end{split}\] (134)

Then \(\bm{\xi}_{i}^{\top}\bm{W}_{V}^{(T_{3})}\bm{w}_{O}\) is a Gaussian random variable with mean zero and standard deviation smaller than \(O\Big{(}\sigma_{V}\|\bm{w}_{O}\|_{2}\sigma_{p}\sqrt{d}+\frac{\max\{\|\bm{\mu}\|_ {2},\sigma_{p}\sqrt{d}\}\cdot\sigma_{p}}{\epsilon\|\bm{\mu}\|_{2}^{2}}\Big{)}\). By Gaussian tail bound, for any \(i\in[M]\backslash\{1\}\), with probability at least \(1-\delta/2N^{2}M^{2}\),

\[\begin{split}|\bm{\xi}_{i}^{\top}\bm{W}_{V}^{(T_{3})}\bm{w}_{O}|& \leq O\Big{(}\sigma_{V}\|\bm{w}_{O}\|_{2}\sigma_{p}\sqrt{d}+\frac{ \max\{\|\bm{\mu}\|_{2},\sigma_{p}\sqrt{d}\}\cdot\sigma_{p}}{\epsilon\|\bm{\mu} \|_{2}^{2}}\Big{)}\cdot\sqrt{2\log\big{(}4N^{2}M^{2}/\delta\big{)}}\\ &\leq 1,\end{split}\] (135)

where the last inequality is by \(\sigma_{V}\leq\widetilde{O}\big{(}\|\bm{w}_{O}\|_{2}^{-1}\cdot\min\{\|\bm{\mu} \|_{2}^{-1},(\sigma_{p}\sqrt{d})^{-1}\}\cdot d_{h}^{-\frac{1}{4}}\big{)}\), \(N\cdot\mathrm{SNR}^{2}=\Omega(1)\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\) and \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\). Applying a union bound completes the proof.

**Lemma D.10** (Population Loss).: _Under the same conditions as Theorem 4.1, we have_

\[L_{D}\big{(}\theta(T_{3})\big{)}=o(1)\]

Proof of Lemma d.10.: Let event A to be the event that Lemma D.8 holds, and event B to be the event that Lemma D.9 holds. By a union bound, \(\mathbb{P}(A\wedge B)\geq 1-\delta/N^{2}M\). We decompose \(L_{D}\big{(}\theta(T_{3})\big{)}\) into the following form:

\[L_{D}\big{(}\theta(T_{3})\big{)}=\mathbb{E}[1(A\wedge B)\ell(yf(\bm{X},\theta(T_ {3})))]+\mathbb{E}[1((A\wedge B)^{c})\ell(yf(\bm{X},\theta(T_{3})))]\] (136)

Then we bound \(\mathbb{E}[1(A\wedge B)\ell(yf(\bm{X},\theta(T_{3})))]\) and \(\mathbb{E}[1((A\wedge B)^{c})\ell(yf(\bm{X},\theta(T_{3})))]\) respectively.

**Bounding \(\mathbb{E}[1(A\wedge B)\ell(yf(\bm{X},\theta(T_{3})))]\):** When event A and event B hold, we have

\[\begin{split} y(f(\bm{X},\theta(T_{3})))&=\frac{1}{M} \sum_{l=1}^{M}\bm{\varphi}(\bm{x}_{l}^{\top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(T_{3 })\top}\bm{X}^{\top})\bm{X}\bm{W}_{V}^{(T_{3})}\bm{w}_{O}\\ &\geq\frac{1}{2M^{2}}\big{|}\bm{\mu}_{+}^{\top}\bm{W}_{V}^{(T_{3} )}\bm{w}_{O}\big{|}-\big{(}1-\frac{1}{2M^{2}}\big{)}\max\{\big{|}\bm{\xi}_{i}^{ \top}\bm{W}_{V}^{(T_{3})}\bm{w}_{O}\big{|}\}\\ &\geq\frac{1}{2M^{2}}\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)}-1 \\ &\geq\frac{\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)}}{C_{21}}-1, \end{split}\] (137)where the first inequality is by Lemma D.8, the second inequality is by (112) and Lemma D.9, the last inequality is by \(M=\Theta(1)\). Therefore,

\[\begin{split}\mathbb{E}[\mathbbm{1}(A\wedge B)\ell(yf(\bm{X}, \theta(T_{3})))]&\leq\ell(yf(\bm{X},\theta(T_{3})))\\ &\leq\log(1+\exp(-yf(\bm{X},\theta(T_{3}))))\\ &\leq\exp(-yf(\bm{X},\theta(T_{3})))\\ &\leq\exp\Big{(}1-\frac{\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)} }{C_{21}}\Big{)}\\ &\leq 3\exp\Big{(}\frac{\log\Big{(}\frac{\epsilon}{C_{20}}\Big{)} }{C_{21}}\Big{)}\end{split}\] (137)

**Bounding \(\mathbb{E}[\mathbbm{1}((A\wedge B)^{c})\ell(yf(\bm{X},\theta(T_{3})))]\):**

\[\begin{split}\mathbb{E}[\mathbbm{1}((A\wedge B)^{c})\ell(yf( \bm{X},\theta(T_{3})))]&\leq\sqrt{\mathbb{E}[\mathbbm{1}((A \wedge B)^{c})]}\cdot\sqrt{\mathbb{E}[\ell(yf(\bm{X},\theta(T_{3})))^{2}]}\\ &\leq\sqrt{\mathbb{P}((A\wedge B)^{c})}\cdot\sqrt{\mathbb{E}[ \exp(-2yf(\bm{X},\theta(T_{3})))]}\\ &\leq\frac{\delta^{\frac{1}{2}}}{N}\cdot\sqrt{\mathbb{E}[\exp(-2 f(\bm{X},\theta(T_{3})))]},\end{split}\] (138)

where the first inequality is by Cauchy-Schwartz inequality, the second inequality is by \(\log(1+\exp(-z))\leq\exp(-z)\).

Next we provide a bound for \(\mathbb{E}[\exp(-2f(\bm{X},\theta(T_{3})))]\). Note that

\[y(f(\bm{X},\theta(T_{3})))=\frac{1}{M}\sum_{l=1}^{M}\bm{\varphi}(\bm{x}_{l}^{ \top}\bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(T_{3})\top}\bm{X}^{\top})\bm{X}\bm{W}_{V }^{(T_{3})}\bm{w}_{O}\]

and \(\bm{\mu}_{+}\bm{W}_{V}^{T_{3}}\bm{w}_{O}\geq 0\), we further have

\[y(f(\bm{X},\theta(T_{3})))\geq-\sum_{i=2}^{M}|\bm{\xi}_{i}\bm{W}_{V}^{(T_{3}) }\bm{w}_{O}|\]

Therefore,

\[\begin{split}\mathbb{E}[\exp(-2f(\bm{X},\theta(T_{3})))]& \leq\mathbb{E}[\exp(2\sum_{i=2}^{M}|\bm{\xi}_{i}\bm{W}_{V}^{(T_{3}) }\bm{w}_{O}|)]\\ &\leq\mathbb{E}[\prod_{i=2}^{M}\exp(2\cdot|\bm{\xi}_{i}\bm{W}_{V} ^{(T_{3})}\bm{w}_{O}|)]\\ &\leq\mathbb{E}[\prod_{i=2}^{M}\Big{(}\exp(2\bm{\xi}_{i}\bm{W}_{V} ^{(T_{3})}\bm{w}_{O})+\exp(-2\bm{\xi}_{i}\bm{W}_{V}^{(T_{3})}\bm{w}_{O})\Big{)} ]\\ &\leq\prod_{i=2}^{M}\mathbb{E}[\Big{(}\exp(2\bm{\xi}_{i}\bm{W}_{V} ^{(T_{3})}\bm{w}_{O})+\exp(-2\bm{\xi}_{i}\bm{W}_{V}^{(T_{3})}\bm{w}_{O})\Big{)} ]\\ &\leq 2^{M}\prod_{i=2}^{M}\mathbb{E}[\exp(2\bm{\xi}_{i}\bm{W}_{V}^{(T_ {3})}\bm{w}_{O})]\end{split}\] (139)

where the third inequality is by \(|x|\leq\max\{x,-x\}\), The fourth inequality is by the independence of \(\bm{\xi}_{i}\bm{W}_{V}^{(T_{3})}\bm{w}_{O}\).

We denote \(\sigma=\|\bm{W}_{V}^{(T_{3})}\bm{w}_{O}\|_{2}\), then \(\bm{\xi}_{i}\bm{W}_{V}^{(T_{3})}\bm{w}_{O}\) is a Gaussian distribution with mean zero and variance smaller than \(C_{p}^{2}\sigma^{2}\sigma_{p}^{2}\). Therefore,

\[\begin{split}\mathbb{E}[\exp(2\bm{\xi}_{i}\bm{W}_{V}^{(T_{3})}\bm {w}_{O})]&\leq\exp(2C_{p}^{2}\sigma^{2}\sigma_{p}^{2})\\ &=\exp\Big{(}2C_{p}^{2}\sigma_{p}^{2}\cdot O\Big{(}\sigma_{V}^{2} \|\bm{w}_{O}\|_{2}^{2}d+\frac{\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}}{ \epsilon^{2}\|\bm{\mu}\|_{2}^{4}}\Big{)}\Big{)}\\ &=\exp\Big{(}O\Big{(}\sigma_{V}^{2}\|\bm{w}_{O}\|_{2}^{2}\sigma_{ p}^{2}d+\frac{\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_{p}^{2}}{ \epsilon^{2}\|\bm{\mu}\|_{2}^{4}}\Big{)}\Big{)}\\ &=\exp\Big{(}o(1)\Big{)}\\ &=1+o(1),\end{split}\] (140)

where the first equality is by (134), the third equality is by \(\sigma_{V}\leq\widetilde{O}\big{(}\|\bm{w}_{O}\|_{2}^{-1}\cdot\min\{\|\bm{\mu} \|_{2}^{-1},(\sigma_{p}\sqrt{d})^{-1}\}\cdot d_{h}^{-\frac{1}{4}}\big{)}\), \(N\cdot\mathrm{SNR}^{2}\geq\Omega(1)\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\). Then we have

\[\begin{split}\mathbb{E}[\exp(-2f(\bm{X},\theta(T_{3})))]& \leq 2^{M}\prod_{i=2}^{M}\mathbb{E}[\exp(2\bm{\xi}_{i}\bm{W}_{V}^{(T_ {3})}\bm{w}_{O})]\\ &\leq 2^{M}\cdot(1+o(1))^{M}\\ &=O(1).\end{split}\] (141)

We further get

\[\mathbb{E}[\mathbbm{1}((A\wedge B)^{c})\ell(yf(\bm{X},\theta(T_{3})))]\leq \frac{\delta^{\frac{1}{2}}}{N}\cdot\sqrt{\mathbb{E}[\exp(-2f(\bm{X},\theta(T_{ 3})))]}\leq\frac{C_{24}\delta^{\frac{1}{2}}}{N}\] (142)

Plugging (137) and (142) into (135), we have

\[L_{D}\big{(}\theta(T_{3})\big{)}\leq 3\exp\Big{(}\frac{\log\Big{(}\frac{ \epsilon}{C_{20}}\Big{)}}{C_{21}}\Big{)}+\frac{C_{24}\delta^{\frac{1}{2}}}{N}= o(1).\]

## Appendix E Harmful Overfitting

In this section, we consider harmful overfitting case under the condition that \(N^{-1}\cdot\mathrm{SNR}^{-2}\geq\Omega(1)\). The proofs in this section are based on the results in Section C, which hold with high probability.

### Stage I

In Stage I, \(V_{\pm}^{(t)}\), \(V_{n,i}^{(t)}\) begin to pull apart until \(|V_{\pm}^{(t)}|\) is sufficiently larger than \(|V_{n,i}^{(t)}|\). At the same time, the inner products of \(\bm{q}\) and \(\bm{k}\) maintain their magnitude.

**Lemma E.1** (Upper bound of V).: _Let \(T_{0}=O(\frac{N}{\eta d_{h}^{\frac{1}{4}}\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}})\). Then under the same conditions as Theorem 4.2, we have_

\[|V_{+}^{(t)}|,|V_{-}^{(t)}|,|V_{n,i}^{(t)}|=O(d_{h}^{-\frac{1}{4}})\]

_for \(t\in[0,T_{0}]\)._Proof of Lemma e.1.: By Lemma B.2, we have

\[|\gamma_{V,+}^{(t+1)}-\gamma_{V,+}^{(t)}|\] \[\leq-\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{NM}\sum_{n\in S_{+}} \ell_{n}^{\prime(t)}\big{(}\frac{\exp(\langle\boldsymbol{q}_{+}^{(t)}, \boldsymbol{k}_{+}^{(t)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(t)}, \boldsymbol{k}_{+}^{(t)}\rangle)+\sum\limits_{k=2}^{M}\exp(\langle\boldsymbol {q}_{+}^{(t)},\boldsymbol{k}_{n,k}^{(t)}\rangle)}\] \[+\sum\limits_{j=2}^{M}\frac{\exp(\langle\boldsymbol{q}_{n,j}^{(t )},\boldsymbol{k}_{+}^{(t)}\rangle)}{\exp(\langle\boldsymbol{q}_{n,j}^{(t)}, \boldsymbol{k}_{+}^{(t)}\rangle)+\sum\limits_{k=2}^{M}\exp(\langle\boldsymbol {q}_{n,j}^{(t)},\boldsymbol{k}_{n,k}^{(t)}\rangle)}\] \[\leq\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{NM}\cdot\frac{3N}{4} \big{(}1+(M-1)\big{)}\] \[\leq\frac{3\eta\|\boldsymbol{\mu}\|_{2}^{2}}{4},\]

where the second inequality is by Lemma C.1 and \(-\ell_{n}^{\prime(t)}\leq 1\). Similarly, we have

\[|\gamma_{V,-}^{(t+1)}-\gamma_{V,-}^{(t)}|\leq\frac{3\eta\| \boldsymbol{\mu}\|_{2}^{2}}{4}.\]

By Definition B.1, we have

\[|V_{+}^{(t)}| =\big{|}V_{+}^{(0)}+\sum\limits_{s=0}^{t-1}(\gamma_{V,+}^{(s+1)}- \gamma_{V,+}^{(s)})\|\boldsymbol{w}_{O}\|_{2}^{2}|\] \[\leq|V_{+}^{(0)}|+\sum\limits_{s=0}^{T_{0}-1}|\gamma_{V,+}^{(s+1 )}-\gamma_{V,+}^{(s)}|\cdot\|\boldsymbol{w}_{O}\|_{2}^{2}\] \[\leq d_{h}^{-\frac{1}{4}}+\frac{3\eta\|\boldsymbol{\mu}\|_{2}^{2} }{4}\cdot\|\boldsymbol{w}_{O}\|_{2}^{2}\cdot O(\frac{N}{\eta d_{h}^{\frac{1}{ 4}}\sigma_{p}^{2}d\|\boldsymbol{w}_{O}\|_{2}^{2}})\] \[=d_{h}^{-\frac{1}{4}}+O(\frac{N\|\boldsymbol{\mu}\|_{2}^{2}}{d_{h }^{\frac{1}{4}}\sigma_{p}^{2}d})\] \[=O(d_{h}^{-\frac{1}{4}}),\]

where the first inequality is by triangle inequality and \(t\leq T_{0}\), the second inequality is by Lemma C.2, the last equality is by \(N^{-1}\cdot\mathrm{SNR}^{-2}=\Omega(1)\). Similarly, we have \(|V_{-}^{(t)}|=O(d_{h}^{-\frac{1}{4}})\).

By Lemma B.2, we have

\[|\rho^{(t+1)}_{V,n,i}-\rho^{(t)}_{V,n,i}|\] \[\leq\Big{|}-\frac{\eta}{NM}\sum\limits_{n^{\prime}\in S_{+}}\ell^{ \prime(t)}_{n^{\prime}}\big{(}\sum\limits_{i=2}^{M}(\langle\bm{\xi}_{n,i},\bm{ \xi}_{n^{\prime},i^{\prime}}\rangle)\frac{\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^ {(t)}_{n^{\prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t )}_{+}\rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_ {n^{\prime},k}\rangle)}\] \[+\sum\limits_{j=2}^{M}\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime},i^{\prime}}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{n^{\prime},j},\bm{k}^{(t)}_ {n^{\prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n^{\prime},j},\bm{ k}^{(t)}_{+}\rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}^{(t)}_{n^{ \prime},j},\bm{k}^{(t)}_{n^{\prime},k}\rangle)})\big{)}\] \[+\frac{\eta}{NM}\sum\limits_{n^{\prime}\in S_{-}}\ell^{\prime(t)} _{n^{\prime}}\big{(}\sum\limits_{i=2}^{M}(\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{ \prime},i^{\prime}}\rangle)\frac{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n ^{\prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-} \rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n^{ \prime},k}\rangle)}\] (143) \[+\sum\limits_{j=2}^{M}\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime},i^{\prime}}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{n^{\prime},j},\bm{k}^{(t)}_ {n^{\prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n^{\prime},j},\bm{ k}^{(t)}_{-}\rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}^{(t)}_{n^{ \prime},j},\bm{k}^{(t)}_{n^{\prime},k}\rangle)})\Big{|}\] \[\leq\frac{3\eta\tilde{\sigma}_{p}^{2}d}{2NM}\cdot M+\frac{\eta}{ NM}\cdot MN\cdot 2\tilde{\sigma}_{p}^{2}\cdot\sqrt{d\log(4N^{2}M^{2}/\delta)}\] \[\leq\frac{2\eta\tilde{\sigma}_{p}^{2}d}{N}\] \[=\frac{2\eta C_{p}^{2}\sigma_{p}^{2}d}{N}\]

where the second inequality is by Lemma C.4 and \(-\ell^{\prime(t)}_{n}\leq 1\), the third inequality is by \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\), the last inequality is by \(N\cdot\mathrm{SNR}^{2}=\Omega(1)\). Then by Definition B.1, we have

\[|V^{(t)}_{n,i}| =\big{|}V^{(0)}_{n,i}+\sum\limits_{s=0}^{t-1}(\gamma^{(s+1)}_{V,n, i}-\gamma^{(s)}_{V,n,i})\|\bm{w}_{O}\|_{2}^{2}\big{|}\] \[\leq|V^{(0)}_{n,i}|+\sum\limits_{s=0}^{t-1}|\gamma^{(s+1)}_{V,n,i }-\gamma^{(s)}_{V,n,i}|\cdot\|\bm{w}_{O}\|_{2}^{2}\] \[\leq d_{h}^{-\frac{1}{4}}+\frac{2\eta C_{p}^{2}\sigma_{p}^{2}d}{N} \cdot\|\bm{w}_{O}\|_{2}^{2}\cdot O(\frac{N}{\eta d_{h}^{\frac{1}{4}}\sigma_{p }^{2}d\|\bm{w}_{O}\|_{2}^{2}})\] \[=O(d^{-\frac{1}{4}}),\]

where the first inequality is by triangle inequality, the second inequality is by Lemma C.2, which completes the proof.

**Lemma E.2** (Inner Products Hold Magnitude).: _Let \(T_{0}=O(\frac{N}{\eta d_{h}^{\frac{1}{4}}\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}})\). Then under the same conditions as Theorem 4.2, we have_

\[|\langle\bm{q}^{(t)}_{\pm},\bm{k}^{(t)}_{\pm}\rangle|,|\langle\bm {q}^{(t)}_{n,i},\bm{k}^{(t)}_{\pm}\rangle|,|\langle\bm{q}^{(t)}_{\pm},\bm{k}^ {(t)}_{n,j}\rangle|,|\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n^{\prime},j}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_{ h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[|\langle\bm{q}^{(t)}_{\pm},\bm{q}^{(t)}_{\mp}\rangle|,|\langle\bm {q}^{(t)}_{n,i},\bm{q}^{(t)}_{\pm}\rangle|,|\langle\bm{q}^{(t)}_{n,i},\bm{q}^ {(t)}_{n^{\prime},j}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_ {h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[|\langle\bm{k}^{(t)}_{\pm},\bm{k}^{(t)}_{\mp}\rangle|,|\langle\bm {k}^{(t)}_{n,i},\bm{k}^{(t)}_{\pm}\rangle|,|\langle\bm{k}^{(t)}_{n,i},\bm{k}^ {(t)}_{n^{\prime},j}\rangle|\]\[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_{h}^{2}\cdot \sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[\|\bm{q}_{\pm}^{(t)}\|_{2}^{2},\|\bm{k}_{\pm}^{(t)}\|_{2}^{2}=\Theta(\|\bm{\mu} \|_{2}^{2}\sigma_{h}^{2}d_{h}),\]

\[\|\bm{q}_{n,i}^{(t)}\|_{2}^{2},\|\bm{k}_{n,i}^{(t)}\|_{2}^{2}=\Theta(\sigma_{p} ^{2}\sigma_{h}^{2}dd_{h})\]

_for \(i,j\in[M]\backslash\{1\}\), \(n,n^{\prime}\in[N]\) and \(t\in[0,T_{0}]\)._

The proof for Lemma E.2 is similar to D.4.

**Lemma E.3** (V's Beginning of Learning Signals).: _There exist \(T_{1}=\frac{4M(3M+1)N}{\eta d_{h}^{\frac{1}{4}}(C_{p}^{2}-24M)\sigma_{p}^{2}d \|\bm{w}_{O}\|_{2}^{2}}\) such that the second element of vector \(\bm{X}_{n}\bm{W}_{V}^{(T_{1})}\bm{w}_{O}\) dominate its other elements, which means \(V_{n,2}^{(T_{1})}\geq 3M\cdot\max\{|V_{+}^{(T_{1})}|,|V_{n,i}^{(T_{1})}|\}\) for all \(n\in S_{+}\), \(i\in[M]\backslash\{1,2\}\) and \(V_{n,2}^{(T_{1})}\leq-3M\cdot\max\{|V_{-}^{(T_{1})}|,|V_{n,i}^{(T_{1})}|\}\) for all \(n\in S_{-}\), \(i\in[M]\backslash\{1,2\}\)._

Proof of Lemma E.3.: According to Condition 4.1 where \(C_{p}^{2}=25M\), we have Then we have

\[T_{1}=\frac{4M(3M+1)N}{\eta d_{h}^{\frac{1}{4}}(25M-24M)\sigma_{p}^{2}d\|\bm{ w}_{O}\|_{2}^{2}}=\frac{4(3M+1)N}{\eta d_{h}^{\frac{1}{4}}\sigma_{p}^{2}d\|\bm{ w}_{O}\|_{2}^{2}}=O\Big{(}\frac{N}{\eta d_{h}^{\frac{1}{4}}\sigma_{p}^{2}d\|\bm{ w}_{O}\|_{2}^{2}}\Big{)},\]

which satisfies the time condition in Lemma E.1 and Lemma E.2. Then by Lemma D.1 and Lemma D.2 we have

\[-\ell_{n}^{\prime(t)}=\frac{1}{2}\pm o(1),\]

\[\frac{1}{M}-o(1)\leq softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)} \rangle)\leq\frac{1}{M}+o(1),\]

\[\frac{1}{M}-o(1)\leq softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)} \rangle)\leq\frac{1}{M}+o(1),\]

\[\frac{1}{M}-o(1)\leq softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{(t)} \rangle)\leq\frac{1}{M}+o(1),\]

\[\frac{1}{M}-o(1)\leq softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)} \rangle)\leq\frac{1}{M}+o(1)\]

for \(i,j\in[M]\backslash\{1\}\), \(n\in[N]\) and \(t\in[0,T_{1}]\). Plugging them in the update rule for \(\gamma_{V,+}^{(t)}\) showed in Lemma B.2 and we have

\[|\gamma_{V,+}^{(t+1)}-\gamma_{V,+}^{(t)}|\] (144) \[=\Big{|}-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum_{n\in S_{+}} \ell_{n}^{\prime(t)}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{k= 2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)}\] \[+\sum\limits_{j=2}^{M}\frac{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k} _{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{+}^{(t)}\rangle)+ \sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n,j}^{(t)},\bm{k}_{n,k}^{(t)} \rangle)}\Big{)}\Big{|}\] \[\leq\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\cdot\frac{3N}{4}\cdot( \frac{1}{2}\pm o(1))\cdot M(\frac{1}{M}\pm o(1))\] \[\leq\frac{\eta\|\bm{\mu}\|_{2}^{2}}{2M}\] \[\leq\frac{2\eta\sigma_{p}^{2}d}{NM}\]where the last inequality is by \(N^{-1}\cdot\mathrm{SNR}^{-2}=\Omega(1)\).Then by Definition B.1 and taking a summation, we have

\[\begin{split}|V_{+}^{(T_{1})}|&\leq|V_{+}^{(0)}|+T_{1 }\frac{2\eta\sigma_{p}^{2}d}{NM}\|\bm{w}_{O}\|_{2}^{2}\\ &=d_{h}^{-\frac{1}{4}}+\frac{4M(3M+1)N}{\eta d_{h}^{\frac{1}{4}}( C_{p}^{2}-24M)\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}}\cdot\frac{2\eta\sigma_{p}^{ 2}d}{NM}\|\bm{w}_{O}\|_{2}^{2}\\ &=d_{h}^{-\frac{1}{4}}+\frac{8(3M+1)}{d_{h}^{\frac{1}{4}}(C_{p}^{ 2}-24M)}.\end{split}\] (145)

Similarly, we have

\[|V_{-}^{(T_{1})}|\leq d_{h}^{-\frac{1}{4}}+\frac{8(3M+1)}{d_{h}^{\frac{1}{4}}( C_{p}^{2}-24M)}.\] (146)

For \(n\in S_{+}\), by Lemma B.2, we have

\[\begin{split}&\rho_{V,n,2}^{(t+1)}-\rho_{V,n,2}^{(t)}\\ &=-\frac{\eta}{NM}\sum\limits_{n^{\prime}\in S_{+}}\ell_{n^{\prime }}^{(t)}\big{(}\sum\limits_{i^{\prime}=2}^{M}(\langle\bm{\xi}_{n,2},\bm{\xi}_{ n^{\prime},i^{\prime}}\rangle\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n^{ \prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n^{\prime},k}^{(t)}\rangle)}\\ &+\sum\limits_{j=2}^{M}\langle\bm{\xi}_{n,2},\bm{\xi}_{n^{\prime },i^{\prime}}\rangle\frac{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{n^{ \prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{+} ^{(t)}\rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n^{\prime},j}^{(t)}, \bm{k}_{n^{\prime},k}^{(t)}\rangle)})\big{)}\\ &+\frac{\eta}{NM}\sum\limits_{n^{\prime}\in S_{-}}\ell_{n^{\prime }}^{(t)}\big{(}\sum\limits_{i^{\prime}=2}^{M}(\langle\bm{\xi}_{n,2},\bm{\xi}_{ n^{\prime},i^{\prime}}\rangle\frac{\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{n^{ \prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{-}^{(t)} \rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{-}^{(t)},\bm{k}_{n^{\prime},k}^{(t)}\rangle)}\\ &+\sum\limits_{j=2}^{M}\langle\bm{\xi}_{n,2},\bm{\xi}_{n^{\prime },i^{\prime}}\rangle\frac{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{n^{ \prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}_{n^{\prime},j}^{(t)},\bm{k}_{-} ^{(t)}\rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}_{n^{\prime},j}^{(t)}, \bm{k}_{n^{\prime},k}^{(t)}\rangle)})\big{)}\\ &\geq\frac{\eta\tilde{\sigma}_{p}^{2}d}{2NM}\cdot M(\frac{1}{M}-o( 1))-\frac{\eta}{NM}\cdot MN\cdot 2\tilde{\sigma}_{p}^{2}\cdot\sqrt{d\log(4N^{2}M^{2}/ \delta)}\\ &\geq\frac{\eta\tilde{\sigma}_{p}^{2}d}{4NM}\\ &\geq\frac{\eta C_{p}^{2}\sigma_{p}^{2}d}{4NM}\end{split}\] (147)

where the second inequality is by Lemma C.4 and \(-\ell_{n}^{\prime(t)}\leq 1\), the third inequality is by \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\). Then by Definition B.1, we have

\[\begin{split} V_{n,2}^{(T_{1})}&=V_{n,2}^{(0)}+ \sum\limits_{s=0}^{T_{1}-1}(\rho_{V,n,2}^{(s+1)}-\rho_{V,n,2}^{(s)})\|\bm{w}_{O }\|_{2}^{2}\\ &\geq-d_{h}^{-\frac{1}{4}}+\frac{4M(3M+1)N}{\eta d_{h}^{\frac{1}{ 4}}(C_{p}^{2}-24M)\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}}\cdot\frac{\eta C_{p}^ {2}\sigma_{p}^{2}d}{4NM}\|\bm{w}_{O}\|_{2}^{2}\\ &\geq-d_{h}^{-\frac{1}{4}}+\frac{C_{p}^{2}(3M+1)}{d_{h}^{\frac{1} {4}}(C_{p}^{2}-24M)}\end{split}\] (148)

for \(n\in S_{+}\). Similarly, for \(n\in S_{-}\), we have

\[V_{n,2}^{(T_{1})}\leq d_{h}^{-\frac{1}{4}}-\frac{C_{p}^{2}(3M+1)}{d_{h}^{\frac{1} {4}}(C_{p}^{2}-24M)}.\] (149)By Lemma B.2, we have

\[|\rho^{(t+1)}_{V,n,i}-\rho^{(t)}_{V,n,i}|\] (150) \[\leq\Big{|}-\frac{\eta}{NM}\sum_{n^{\prime}\in S_{+}}\ell^{\prime(t) }_{n^{\prime}}\big{(}\sum_{i=2}^{M}(\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime}, i^{\prime}}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n^{\prime},i^{ \prime}}\rangle)}{\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{+}\rangle)+\sum \limits_{k=2}^{M}\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{n^{\prime},k}\rangle)}\] \[+\sum_{j=2}^{M}\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime},i^{ \prime}}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{n^{\prime},j},\bm{k}^{(t)}_{n^ {\prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n^{\prime},j},\bm{k}^ {(t)}_{-}\rangle)+\sum\limits_{k=2}^{M}\exp(\langle\bm{q}^{(t)}_{n^{\prime},j },\bm{k}^{(t)}_{n^{\prime},k}\rangle)})\Big{|}\] \[\leq\frac{3\eta\sigma_{p}^{2}d}{2NM}\cdot M(\frac{1}{M}+o(1))+ \frac{\eta}{NM}\cdot MN\cdot 2\tilde{\sigma}_{p}^{2}\cdot\sqrt{d\log(4N^{2}M^{2}/ \delta)}\] \[\leq\frac{2\eta\sigma_{p}^{2}d}{NM}\]

for \(i\in[M]\backslash\{1,2\}\), where the second inequality is by Lemma C.4 and \(-\ell^{\prime(t)}_{n}\leq 1\), the last inequality is by \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\). Then by Definition B.1, we have

\[|V^{(t)}_{n,i}| =\big{|}V^{(0)}_{n,i}+\sum_{s=0}^{t-1}(\gamma^{(s+1)}_{V,n,i}- \gamma^{(s)}_{V,n,i})\|\bm{w}_{O}\|_{2}^{2}\big{|}\] (151) \[\leq|V^{(0)}_{n,i}|+\sum_{s=0}^{t-1}|\gamma^{(s+1)}_{V,n,i}- \gamma^{(s)}_{V,n,i}|\cdot\|\bm{w}_{O}\|_{2}^{2}\] \[\leq d_{h}^{-\frac{1}{4}}+\frac{4M(3M+1)N}{\eta d_{h}^{\frac{1}{ 4}}(C_{p}^{2}-24M)\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}}\cdot\frac{2\eta\sigma _{p}^{2}d}{NM}\cdot\|\bm{w}_{O}\|_{2}^{2}\] \[\leq d_{h}^{-\frac{1}{4}}+\frac{8(3M+1)}{d_{h}^{\frac{1}{4}}(C_{p} ^{2}-24M)}\]

for \(i\in[M]\backslash\{1,2\}\), where the first inequality is by triangle inequality, the second inequality is by Lemma C.2.

According to (145), (146), (148), (149) and (151), it is easy to verify that \(V^{(T_{1})}_{n,2}\geq 3M\cdot\max\{|V^{(T_{1})}_{+}|,|V^{(T_{1})}_{n,i}|\}\) for all \(n\in S_{+}\), \(i\in[M]\backslash\{1,2\}\) and \(V^{(T_{1})}_{n,2}\leq-3M\cdot\max\{|V^{(T_{1})}_{-}|,|V^{(T_{1})}_{n,i}|\}\) for all \(n\in S_{-}\), \(i\in[M]\backslash\{1,2\}\), which completes the proof.

### Stage II

In stage II, \(\langle\bm{q}_{+},\bm{k}_{+}\rangle\), \(\langle\bm{q}_{n,i},\bm{k}_{+}\rangle\) grows while \(\langle\bm{q}_{+},\bm{k}_{n,j}\rangle\), \(\langle\bm{q}_{n,i},\bm{k}_{n,j}\rangle\) decreases, resulting in attention focusing more and more on the signal and less on the noise. By the results of stage I, we have the following condition at the beginning of stage II

\[V^{(T_{1})}_{n,2}\geq 3M\cdot\max\{|V^{(T_{1})}_{+}|,|V^{(T_{1})}_{n,i}|\}\]

for all \(n\in S_{+}\), \(i\in[M]\backslash\{1,2\}\).

\[V^{(T_{1})}_{n,2}\leq-3M\cdot\max\{|V^{(T_{1})}_{-}|,|V^{(T_{1})}_{n,i}|\}\]for all \(n\in S_{-}\), \(i\in[M]\backslash\{1,2\}\).

\[|V_{+}^{(T_{1})}|,|V_{-}^{(T_{1})}|,|V_{n,i}^{(T_{1})}|=O(d_{h}^{-\frac{1}{4}}),\]

\[|\langle\bm{q}_{\pm}^{(T_{1})},\bm{k}_{\pm}^{(T_{1})}\rangle|,| \langle\bm{q}_{n,i}^{(T_{1})},\bm{k}_{\pm}^{(T_{1})}\rangle|,|\langle\bm{q}_{ \pm}^{(T_{1})},\bm{k}_{n,j}^{(T_{1})}\rangle|,|\langle\bm{q}_{n,i}^{(T_{1})}, \bm{k}_{n^{\prime},j}^{(T_{1})}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_{ h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[|\langle\bm{q}_{\pm}^{(T_{1})},\bm{q}_{\mp}^{(T_{1})}\rangle|,| \langle\bm{q}_{n,i}^{(T_{1})},\bm{q}_{\pm}^{(T_{1})}\rangle|,|\langle\bm{q}_{ n,i}^{(T_{1})},\bm{q}_{n^{\prime},j}^{(T_{1})}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_ {h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[|\langle\bm{k}_{\pm}^{(T_{1})},\bm{k}_{\mp}^{(T_{1})}\rangle|,| \langle\bm{k}_{n,i}^{(T_{1})},\bm{k}_{\pm}^{(T_{1})}\rangle|,|\langle\bm{k}_{ n,i}^{(T_{1})},\bm{k}_{n^{\prime},j}^{(T_{1})}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_ {h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[\|\bm{q}_{\pm}^{(T_{1})}\|_{2}^{2},\|\bm{k}_{\pm}^{(T_{1})}\|_{2}^{2}=\Theta( \|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}),\]

\[\|\bm{q}_{n,i}^{(T_{1})}\|_{2}^{2},\|\bm{k}_{n,i}^{(T_{1})}\|_{2}^{2}=\Theta( \sigma_{p}^{2}\sigma_{h}^{2}dd_{h})\]

for \(i,j\in[M]\backslash\{1\}\), \(n,n^{\prime}\in[N]\).

Some of the proofs at this stage are based on the above conditions.

**Notations.** To better characterize the gap between different inner products, we define the following notations:

* denote \(\Psi_{+}^{(t)}=\langle\bm{q}_{+}^{(t)},\bm{k}_{n,2}^{(t)}\rangle-\langle\bm{q }_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle,\quad n\in S_{+}\).
* denote \(\Psi_{n,+,j}^{(t)}=\langle\bm{q}_{+}^{(t)},\bm{k}_{n,2}^{(t)}\rangle-\langle \bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle,\quad n\in S_{+},j\in[M]\backslash\{ 1,2\}\).
* denote \(\Psi_{-}^{(t)}=\langle\bm{q}_{-}^{(t)},\bm{k}_{n,2}^{(t)}\rangle-\langle\bm{q }_{-}^{(t)},\bm{k}_{-}^{(t)}\rangle,\quad n\in S_{-}\).
* denote \(\Psi_{n,-j}^{(t)}=\langle\bm{q}_{-}^{(t)},\bm{k}_{n,2}^{(t)}\rangle-\langle \bm{q}_{-}^{(t)},\bm{k}_{n,j}^{(t)}\rangle,\quad n\in S_{-},j\in[M]\backslash\{ 1,2\}\).
* denote \(\Psi_{n,i,+}^{(t)}=\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,2}^{(t)}\rangle-\langle \bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle,\quad n\in S_{+},i\in[M]\backslash\{ 1\}\).
* denote \(\Psi_{n,i,-}^{(t)}=\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,2}^{(t)}\rangle-\langle \bm{q}_{n,i}^{(t)},\bm{k}_{-}^{(t)}\rangle,\quad n\in S_{-},i\in[M]\backslash\{ 1\}\).
* denote \(\Psi_{n,i,j}^{(t)}=\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,2}^{(t)}\rangle-\langle \bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle,i\in[M]\backslash\{1\},j\in[M] \backslash\{1,2\}\).

**Lemma E.4** (Upper bound of V).: _Let \(T_{0}=O\Big{(}\frac{N}{\eta\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}\log(6N^{2}M^{2}/ \delta)}\Big{)}\). Then under the same conditions as Theorem 4.2, we have_

\[|V_{+}^{(t)}|,|V_{-}^{(t)}|,|V_{n,i}^{(t)}|=o(1)\]

_for \(t\in[0,T_{0}]\)._

The proof of Lemma E.4 is similar to that of Lemma E.1, except that the time \(T_{0}\) is changed.

Let \(T_{2}=\Theta\Big{(}\frac{N}{\eta\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}\log(6N^ {2}M^{2}/\delta)}\Big{)}\), then by Lemma D.6 and Lemma D.1 we have \(\frac{1}{2}-o(1)\leq-\ell_{n}^{(t)}\leq\frac{1}{2}+o(1)\) for \(n\in[N],t\in[T_{1},T_{2}]\), which can simplify the calculations of \(\alpha\) and \(\beta\) defined in Definition B.3 by replacing \(-\ell_{n}^{(t)}\) by their bounds. Next we prove the following four propositions \(\mathcal{J}(t)\), \(\mathcal{K}(t)\), \(\mathcal{L}(t)\), \(\mathcal{M}(t)\) by induction on t for \(t\in[T_{1},T_{2}]\):

* \(\mathcal{J}(t):\) \[V_{n,2}^{(t)}\geq\frac{\eta C_{3}\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}(t-T_{1})}{N}\] \[V_{n,2}^{(t)}\leq-\frac{\eta C_{3}\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^ {2}(t-T_{1})}{N}\]\[V_{n,2}^{(t)}\geq 3M\cdot\max\{|V_{+}^{(T_{1})}|,|V_{n,i}^{(T_{1})}|\}\] for all \(n\in S_{+},i\in[M]\backslash\{1,2\}\). \[V_{n,2}^{(t)}\leq-3M\cdot\max\{|V_{-}^{(T_{1})}|,|V_{n,i}^{(T_{1})}|\}\] for all \(n\in S_{-},i\in[M]\backslash\{1,2\}\). \[|V_{\pm}^{(t)}|\leq O(d_{h}^{-\frac{1}{4}})+\frac{\eta C_{4}\sigma_{p}^{2}d\| \bm{w}_{0}\|_{2}^{2}(t-T_{1})}{N}\] \[|V_{n,i}^{(t)}|\leq O(d_{h}^{-\frac{1}{4}})+\frac{\eta C_{4}\sigma_{p}^{2}d\| \bm{w}_{0}\|_{2}^{2}(t-T_{1})}{N}\] for \(i\in[M]\backslash\{1\},n\in[N]\).
* \(\mathcal{K}(t):\) \[\|\bm{q}_{\pm}^{(t)}\|_{2}^{2},\|\bm{k}_{\pm}^{(t)}\|_{2}^{2}=\Theta\Big{(}\| \bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}\Big{)},\] \[\|\bm{q}_{+}^{(t)},\bm{q}_{-}^{(t)}\|,|\langle\bm{q}_{\pm}^{(t)},\bm{q}_{n,i }^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{q}_{n,j}^{(t)}\rangle|=o(1),\] \[|\langle\bm{k}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle|,|\langle\bm{k}_{\pm}^{(t) },\bm{k}_{n,i}^{(t)}\rangle|,|\langle\bm{k}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)} \rangle|=o(1),\] for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],i\neq j\) or \(n\neq n^{\prime}\).
* \(\mathcal{L}(t):\) \[\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{n,2}^{(t+1)}\rangle-\langle\bm{q}_{\pm}^ {(t)},\bm{k}_{n,2}^{(t)}\rangle\geq 2\big{(}\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{ \pm}^{(t+1)}\rangle-\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle\big{)}\] \[\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{n,2}^{(t+1)}\rangle-\langle\bm{q}_{ \pm}^{(t)},\bm{k}_{n,2}^{(t)}\rangle\geq 2\big{(}\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{ n,j}^{(t+1)}\rangle-\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\big{)}\] \[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,2}^{(t+1)}\rangle-\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,2}^{(t)}\rangle\geq 2\big{(}\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{ \pm}^{(t+1)}\rangle-\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)}\rangle\big{)}\] \[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,2}^{(t+1)}\rangle-\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,2}^{(t)}\rangle\geq 2\big{(}\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{ n,j}^{(t+1)}\rangle-\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\big{)}\] \[\Psi_{\pm}^{(t+1)}\geq\log\Big{(}\exp(\Psi_{\pm}^{(T_{1})})+\frac{\eta^{2}C _{7}\|\bm{\mu}\|_{2}^{2}\sigma_{p}^{2}d\|\bm{w}_{0}\|_{2}^{2}d_{h}^{\frac{1}{ 2}}}{N^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}+1) \Big{)}\] \[\Psi_{n,\pm,j}^{(t+1)}\geq\log\Big{(}\exp(\Psi_{n,\pm,j}^{(T_{1})})+ \frac{\eta^{2}C_{7}\sigma_{p}^{4}d^{2}\|\bm{w}_{0}\|_{2}^{2}d_{h}^{\frac{1}{ 2}}}{N^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}+1) \Big{)}\] \[\Psi_{n,i,j}^{(t+1)}\geq\log\Big{(}\exp(\Psi_{n,i,j}^{(T_{1})})+ \frac{\eta^{2}C_{7}\sigma_{p}^{4}d^{2}\|\bm{w}_{0}\|_{2}^{2}d_{h}^{\frac{1}{ 2}}}{N^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}+1) \Big{)}\] for \(i\in[M]\backslash\{1\},j\in[M]\backslash\{1,2\},n\in S_{\pm}\).
* \(\mathcal{M}(t):\) \[|\Psi_{\pm}^{(t)}|,|\Psi_{n,\pm,j^{\prime}}^{(t)}|,|\Psi_{n,i}^{(t)},|\Psi_{n, i,j^{\prime}}^{(t)}|\leq\log(d_{h}^{\frac{1}{2}})\] \[|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{q}_{ \pm}^{(t)},\bm{k}_{n,j}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{( t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle|\leq 2\log(d_{h}^{\frac{1}{2}})\] \[|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\mp}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle|=o(1)\] for \(i,j\in[M]\backslash\{1\},j^{\prime}\in[M]\backslash\{1,2\},n,\overline{n} \in[N],n\neq\overline{n}\).

By the results of Stage I, we know that \(\mathcal{J}(T_{1})\), \(\mathcal{K}(T_{1})\), \(\mathcal{M}(T_{1})\) are true. To prove that \(\mathcal{J}(t)\), \(\mathcal{K}(t)\), \(\mathcal{L}(t)\) and \(\mathcal{M}(t)\) are true in stage 2, we give the following claims holds for \(t\in[T_{1},T_{2}]\):

**Claim 9**.: \(\mathcal{L}(T_{1}),\ldots,\mathcal{L}(t-1)\Longrightarrow\mathcal{J}(t+1)\)__

**Claim 10**.: \(\mathcal{J}(T_{1}),\ldots,\mathcal{J}(t),\mathcal{K}(T_{1}),\ldots,\mathcal{K}(t), \mathcal{L}(T_{1}),\ldots,\mathcal{L}(t-1)\Longrightarrow\mathcal{L}(t)\)

**Claim 11**.: \(\mathcal{J}(T_{1}),\ldots,\mathcal{J}(t),\mathcal{L}(T_{1}),\ldots,\mathcal{L}(t-1), \mathcal{M}(T_{1}),\ldots,\mathcal{M}(t)\Longrightarrow\mathcal{K}(t+1)\)__

**Claim 12**.: \(\mathcal{J}(T_{1}),\ldots,\mathcal{J}(t),\mathcal{K}(T_{1}),\ldots,\mathcal{K}( t),\mathcal{L}(T_{1}),\ldots,\mathcal{L}(t-1)\Longrightarrow\mathcal{M}(t+1)\)__

The proofs of Claim 9-12 are similar to the proofs of Claim 5-8. Next, we show some important procedures for proving Claim 10.

**Proof of Claim 10:**

Similar to D.2.2, we have the dynamic of \(\Psi\) as follows

\[\Psi_{\pm}^{(s+1)}-\Psi_{\pm}^{(s)} \geq\frac{\eta^{2}C_{7}\|\boldsymbol{\mu}\|_{2}^{2}\sigma_{p}^{2} d\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}(s-T_{1})}{N^{2}\big{(}\log(6N^{2}M ^{2}/\delta)\big{)}^{2}}\cdot\frac{1}{\exp(\Psi_{\pm}^{(s)})},\] (152) \[\Psi_{n,\pm,j}^{(s+1)}-\Psi_{n,\pm,j}^{(s)} \geq\frac{\eta^{2}C_{7}\|\boldsymbol{\mu}\|_{2}^{2}\sigma_{p}^{2} d\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}(s-T_{1})}{N^{2}\big{(}\log(6N^{2}M ^{2}/\delta)\big{)}^{2}}\cdot\frac{1}{\exp(\Psi_{n,\pm,j}^{(s)})},\] (153) \[\Psi_{n,i,\pm}^{(s+1)}-\Psi_{n,i,\pm}^{(s)} \geq\frac{\eta^{2}C_{7}\sigma_{p}^{4}d^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}d _{h}^{\frac{1}{2}}(s-T_{1})}{N^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}} \cdot\frac{1}{\exp(\Psi_{n,i,\pm}^{(s)})},\] (154) \[\Psi_{n,i,j}^{(s+1)}-\Psi_{n,i,j}^{(s)} \geq\frac{\eta^{2}C_{7}\sigma_{p}^{4}d^{2}\|\boldsymbol{w}_{O}\|_{2 }^{2}d_{h}^{\frac{1}{2}}(s-T_{1})}{N^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)} ^{2}}\cdot\frac{1}{\exp(\Psi_{n,i,j}^{(s)})},\] (155)

for \(n\in[N],i\in[M]\backslash\{1\},j\in[M]\backslash\{1,2\},s\in[T_{1},t]\). Next, we provide the logarithmic increasing lower bounds of \(\Psi\). Recall that

\[\Psi_{+}^{(s+1)}-\Psi_{+}^{(s)} \geq\frac{\eta^{2}C_{7}\|\boldsymbol{\mu}\|_{2}^{2}\sigma_{p}^{2} d\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}(s-T_{1})}{N^{2}\big{(}\log(6N^{2}M ^{2}/\delta)\big{)}^{2}}\cdot\frac{1}{\exp(\Psi_{+}^{(s)})},\] (156)

Multiply both sides simultaneously by \(\exp(\Psi_{+}^{(s)})\) and get

\[\exp(\Psi_{+}^{(s)})\Big{(}\Psi_{+}^{(s+1)}-\Psi_{+}^{(s)}\Big{)} \geq\frac{\eta^{2}C_{7}\|\boldsymbol{\mu}\|_{2}^{2}\sigma_{p}^{2} d\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}(s-T_{1})}{N^{2}\big{(}\log(6N^{2}M ^{2}/\delta)\big{)}^{2}}.\] (157)

Taking a summation from \(T_{1}\) to \(t\) and get

\[\sum_{s=T_{1}}^{t}\exp(\Psi_{+}^{(s)})\Big{(}\Psi_{+}^{(s+1)}-\Psi _{+}^{(s)}\Big{)}\] \[\geq\sum_{s=T_{1}}^{t}\frac{\eta^{2}C_{7}\|\boldsymbol{\mu}\|_{2}^ {2}\sigma_{p}^{2}d\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}(s-T_{1})}{ N^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\] (158) \[\geq\frac{\eta^{2}C_{7}\|\boldsymbol{\mu}\|_{2}^{2}\sigma_{p}^{2} d\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N^{2}\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}+1).\]

By the property that \(\Psi_{+}^{(s)}\) is monotonically increasing, we have

\[\int_{\Psi_{+}^{(T_{1})}}^{\Psi_{+}^{(t+1)}}\exp(x)dx\geq\sum_{s= T_{1}}^{t}\exp(\Psi_{+}^{(s)})\Big{(}\Psi_{+}^{(s+1)}-\Psi_{+}^{(s)}\Big{)}\] (159) \[\geq\frac{\eta^{2}C_{7}\|\boldsymbol{\mu}\|_{2}^{2}\sigma_{p}^{2} d\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N^{2}\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}+1).\]

By \(\int_{\Psi_{+}^{(T_{1})}}^{\Psi_{+}^{(t+1)}}\exp(x)dx=\exp(\Psi_{+}^{(t+1)})- \exp(\Psi_{+}^{(T_{1})})\) we get

\[\Psi_{+}^{(t+1)}\geq\log\Big{(}\exp(\Psi_{+}^{(T_{1})})+\frac{\eta^{2}C_{7}\| \boldsymbol{\mu}\|_{2}^{2}\sigma_{p}^{2}d\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{ \frac{1}{2}}}{N^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})( t-T_{1}+1)\Big{)}.\] (160)Similarly, we have

\[\Psi_{-}^{(t+1)}\geq\log\Big{(}\exp(\Psi_{-}^{(T_{1})})+\frac{\eta^{2}C_{7}\| \boldsymbol{\mu}\|_{2}^{2}\sigma_{p}^{2}d\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{ \frac{1}{2}}}{N^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T _{1}+1)\Big{)}.\] (161)

\[\Psi_{n,\pm,j}^{(t+1)}\geq\log\Big{(}\exp(\Psi_{n,\pm,j}^{(T_{1})})+\frac{\eta^ {2}C_{7}\|\boldsymbol{\mu}\|_{2}^{2}\sigma_{p}^{2}d\|\boldsymbol{w}_{O}\|_{2}^ {2}d_{h}^{\frac{1}{2}}}{N^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t -T_{1})(t-T_{1}+1)\Big{)}.\] (162)

\[\Psi_{n,i,\pm}^{(t+1)}\geq\log\Big{(}\exp(\Psi_{n,i,\pm}^{(T_{1})})+\frac{\eta ^{2}C_{7}\sigma_{p}^{4}d^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}} }{N^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}+1) \Big{)}.\] (163)

\[\Psi_{n,i,j}^{(t+1)}\geq\log\Big{(}\exp(\Psi_{n,i,j}^{(T_{1})})+\frac{\eta^{2} C_{7}\sigma_{p}^{4}d^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N^{2} \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(t-T_{1})(t-T_{1}+1)\Big{)}.\] (164)

for \(n\in[N],i\in[M]\backslash\{1\},j\in[M]\backslash\{1,2\}\).

### Stage III

In Stage III, the outputs of ViT grow up and the loss derivatives are no longer at \(o(1)\). We will carefully compute the growth rate of \(V_{\pm}\) and \(V_{n,i}\) while keeping monitoring the monotonicity of \(\langle\boldsymbol{q},\boldsymbol{k}\rangle\). By substituting \(t=T_{2}=\Theta\Big{(}\frac{N}{\eta\sigma_{p}^{2}d\|\boldsymbol{w}_{O}\|_{2}^{ \frac{1}{2}}}\Big{)}\) into propositions \(\mathcal{J}(t)\), \(\mathcal{K}(t)\), \(\mathcal{L}(t)\), \(\mathcal{M}(t)\) in Stage II, we have the following conditions at the beginning of stage III

\[|V_{+}^{(T_{2})}|,|V_{-}^{(T_{2})}|,|V_{n,i}^{(T_{2})}|=o(1)\]

for all \(n\in[N]\), \(i\in[M]\backslash\{1\}\).

\[V_{n,2}^{(T_{1})}\geq 3M\cdot\max\{|V_{+}^{(T_{1})}|,|V_{n,i}^{(T_{1})}|\}\]

for all \(n\in S_{+}\), \(i\in[M]\backslash\{1,2\}\).

\[V_{n,2}^{(T_{1})}\leq-3M\cdot\max\{|V_{-}^{(T_{1})}|,|V_{n,i}^{(T_{1})}|\}\]

for all \(n\in S_{-}\), \(i\in[M]\backslash\{1,2\}\).

\[\|\boldsymbol{q}_{\pm}^{(T_{2})}\|_{2}^{2},\|\boldsymbol{k}_{\pm}^{(T_{2})}\|_ {2}^{2}=\Theta\Big{(}\|\boldsymbol{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}\Big{)},\]

\[\|\boldsymbol{q}_{n,i}^{(T_{2})}\|_{2}^{2},\|\boldsymbol{k}_{n,i}^{(T_{2})}\|_ {2}^{2}=\Theta\Big{(}\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}\Big{)},\]

\[|\langle\boldsymbol{q}_{+}^{(T_{2})},\boldsymbol{q}_{-}^{(T_{2})}\rangle|,| \langle\boldsymbol{q}_{\pm}^{(T_{2})},\boldsymbol{q}_{n,i}^{(T_{2})}\rangle|,|\langle\boldsymbol{q}_{n,i}^{(T_{2})},\boldsymbol{q}_{n,j}^{(T_{2})}\rangle| =o(1),\]

\[|\langle\boldsymbol{k}_{+}^{(T_{2})},\boldsymbol{k}_{-}^{(T_{2})}\rangle|,| \langle\boldsymbol{k}_{+}^{(T_{2})},\boldsymbol{k}_{n,i}^{(T_{2})}\rangle|,| \langle\boldsymbol{k}_{n,i}^{(T_{2})},\boldsymbol{k}_{n,j}^{(T_{2})}\rangle| =o(1),\]

for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],i\neq j\) or \(n\neq n^{\prime}\).

\[\Psi_{\pm}^{(T_{2})}\geq\log\Big{(}\exp(\Psi_{\pm}^{(T_{1})})+\frac{C_{7}\| \boldsymbol{\mu}\|_{2}^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{ \sigma_{p}^{2}d\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}\Big{)}\]

\[\Psi_{n,\pm,j}^{(T_{2})}\geq\log\Big{(}\exp(\Psi_{n,\pm,j}^{(T_{1})})+\frac{C_{7} \|\boldsymbol{\mu}\|_{2}^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{ \sigma_{p}^{2}d\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}\Big{)}\]

\[\Psi_{n,i,\pm}^{(T_{2})}\geq\log\Big{(}\exp(\Psi_{n,i,\pm}^{(T_{1})})+\frac{C_{7} \|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{3}}\Big{)}\]\[\Psi_{n,i,j}^{(T_{2})}\geq\log\Big{(}\exp(\Psi_{n,i,j}^{(T_{1})})+\frac{C_{\tau} \|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{3}}\Big{)}\]

\[|\Psi_{\pm}^{(t)}|,|\Psi_{n,\pm,j^{\prime}}^{(t)}|,|\Psi_{n,i,\pm}^{(t)}|,|\Psi_ {n,i,j^{\prime}}^{(t)}|\leq\log(d_{h}^{\frac{1}{2}})\]

\[|\langle\bm{q}_{\pm}^{(T_{2})},\bm{k}_{\pm}^{(T_{2})}\rangle|,|\langle\bm{q}_{ \pm}^{(T_{2})},\bm{k}_{\pm}^{(T_{2})}\rangle|,|\langle\bm{q}_{n,i}^{(T_{2})}, \bm{k}_{\pm}^{(T_{2})}\rangle|,|\langle\bm{q}_{n,i}^{(T_{2})},\bm{k}_{n,j}^{( T_{2})}\rangle|\leq 2\log(d_{h}^{\frac{1}{2}})\]

\[|\langle\bm{q}_{\pm}^{(T_{2})},\bm{k}_{\mp}^{(T_{2})}\rangle|,|\langle\bm{q}_{ n,i}^{(T_{2})},\bm{k}_{\mp,j}^{(T_{2})}\rangle|=o(1)\]

for \(i,j\in[M]\backslash\{1\},j^{\prime}\in[M]\backslash\{1,2\},n,\overline{n}\in[ N],n\neq\overline{n}\).

Let \(T_{3}=\Theta\Big{(}\frac{N}{\eta\epsilon\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}} \Big{)}\), Next we prove the following four propositions \(\mathcal{N}(t)\), \(\mathcal{O}(t)\), \(\mathcal{P}(t)\), \(\mathcal{Q}(t)\) by induction on t for \(t\in[T_{2},T_{3}]\):

* \(\mathcal{N}(t):\) \[V_{n,2}^{(t)}\geq 3M\cdot\max\{|V_{+}^{(T_{1})}|,|V_{n,i}^{(T_{1})}|\}\] for all \(n\in S_{+}\), \(i\in[M]\backslash\{1,2\}\). \[V_{n,2}^{(t)}\leq-3M\cdot\max\{|V_{-}^{(T_{1})}|,|V_{n,i}^{(T_{1})}|\}\] for all \(n\in S_{-}\), \(i\in[M]\backslash\{1,2\}\). \[|V_{\pm}^{(t)}|,|V_{n,i}^{(t)}|=o(1),\] for \(i\in[M]\backslash\{1,2\},n\in[N]\). \[\log\Big{(}\exp(V_{n,2}^{(T_{2})})+\frac{\eta C_{17}\sigma_{p}^{2}d\|\bm{w}_{O }\|_{2}^{2}(t-T_{2})}{N}\Big{)}\leq V_{n,2}^{(t)}\leq 2\log\big{(}O(\frac{1}{ \epsilon})\big{)},\] for all \(n\in S_{+}\). \[-2\log\big{(}O(\frac{1}{\epsilon})\big{)}\leq V_{n,2}^{(t)}\leq-\log\Big{(} \exp(-V_{n,2}^{(T_{2})})+\frac{\eta C_{17}\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2} (t-T_{2})}{N}\Big{)}\] for all \(n\in S_{-}\).
* \(\mathcal{O}(t):\) \[\|\bm{q}_{\pm}^{(t)}\|_{2}^{2},\|\bm{k}_{\pm}^{(t)}\|_{2}^{2}=\Theta(\|\bm{\mu} \|_{2}^{2}\sigma_{h}^{2}d_{h}),\] \[\|\langle\bm{q}_{n,i}^{(t)}\|_{2}^{2},\|\bm{k}_{n,i}^{(t)}\|_{2}^{2}=\Theta \Big{(}\sigma_{p}^{2}\sigma_{h}^{2}d_{h}\Big{)},\] \[|\langle\bm{q}_{+}^{(t)},\bm{q}_{-}^{(t)}\rangle|,|\langle\bm{q}_{+}^{(t)}, \bm{q}_{n,i}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{q}_{n,j}^{(t)}\rangle| =o(1),\] \[|\langle\bm{k}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle|,|\langle\bm{k}_{\pm}^{(t) },\bm{k}_{n,i}^{(t)}\rangle|,|\langle\bm{k}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle| =o(1)\] for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],i\neq j\) or \(n\neq n^{\prime}\).
* \(\mathcal{P}(t):\) \[\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{n,2}^{(t+1)}\rangle-\langle\bm{q}_{\pm}^{(t) },\bm{k}_{n,2}^{(t)}\rangle\geq 2\big{(}\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{\pm}^{(t+1)} \rangle-\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle\big{)}\] \[\langle\bm{q}_{\pm}^{(t+1)},\bm{k}_{n,2}^{(t+1)}\rangle-\langle\bm{q }_{\pm}^{(t)},\bm{k}_{n,2}^{(t)}\rangle\geq 2\big{(}\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,j}^{(t+1)} \rangle-\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\big{)}\] \[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,2}^{(t+1)}\rangle-\langle\bm{q }_{n,i}^{(t)},\bm{k}_{n,2}^{(t)}\rangle\geq 2\big{(}\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{n,j}^{(t+1)} \rangle-\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\big{)}\] \[\Psi_{n,i}^{(t+1)}\geq\Psi_{n,\pm,j}^{(t)}\] \[\Psi_{n,i,\pm}^{(t+1)}\geq\Psi_{n,i,\pm}^{(t)}\] \[\Psi_{n,i,\pm}^{(t+1)}\geq\Psi_{n,i,\pm}^{(t)}\] \[\Psi_{n,i,j}^{(t+1)}\geq\Psi_{n,i,j}^{(t)}\] for \(i\in[M]\backslash\{1\},j\in[M]\backslash\{1,2\},n\in S_{\pm}\).
* \(\mathcal{Q}(t):\) \[|\Psi_{\pm}^{(t)}|,|\Psi_{n,\pm,j^{\prime}}^{(t)}|,|\Psi_{n,i,\pm}^{(t)}|,|\Psi_{ n,i,j^{\prime}}^{(t)}|\leq\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{q}_{\pm}^{ (t)},\bm{k}_{n,j}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)} \rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle|\leq 2\log( \epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\mp}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^ {(t)},\bm{k}_{n,j}^{(t)}\rangle|=o(1)\] for \(i,j\in[M]\backslash\{1\},j^{\prime}\in[M]\backslash\{1,2\},n,\overline{n}\in[ N],n\neq\overline{n}\).

By the results of Stage II, we know that \(\mathcal{N}(T_{1})\), \(\mathcal{O}(T_{2})\), \(\mathcal{Q}(T_{2})\) are true. To prove that \(\mathcal{N}(t)\), \(\mathcal{O}(t)\), \(\mathcal{P}(t)\) and \(\mathcal{Q}(t)\) are true in Stage III, we will prove the following claims holds for \(t\in[T_{2},T_{3}]\):

**Claim 13**.: \(\mathcal{P}(T_{2}),\ldots,\mathcal{P}(t-1)\Longrightarrow\mathcal{N}(t+1)\)

**Claim 14**.: \(\mathcal{N}(t),\mathcal{O}(t),\mathcal{P}(T_{2}),\ldots,\mathcal{P}(t-1) \Longrightarrow\mathcal{P}(t)\)

**Claim 15**.: \(\mathcal{N}(T_{2}),\ldots,\mathcal{N}(t),\mathcal{P}(T_{2}),\ldots,\mathcal{ P}(t-1),\mathcal{Q}(T_{2}),\ldots,\mathcal{Q}(t)\Longrightarrow\mathcal{O}(t+1)\)

**Claim 16**.: \(\mathcal{N}(T_{2}),\ldots,\mathcal{N}(t),\mathcal{O}(T_{2}),\ldots,\mathcal{ O}(t),\mathcal{P}(T_{2}),\ldots,\mathcal{P}(t-1)\Longrightarrow\mathcal{Q}(t+1)\)

**Lemma E.5** (Convergence of Training Loss).: _There exist \(T=\frac{C_{2}aN}{\eta\epsilon\sigma_{p}^{2}d|\bm{w}_{O}|_{2}^{2}}\) such that_

\[L_{S}(\theta(T))\geq 0.1\]

_Proof of Lemma E.5._ Similar to D.7, we have

\[softmax(\langle\bm{q}_{\pm}^{(T)},\bm{k}_{\pm}^{(T)}\rangle) =O\Big{(}\frac{\sigma_{p}^{2}d\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{3}}{\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}} \Big{)},\] (165) \[softmax(\langle\bm{q}_{\pm}^{(T)},\bm{k}_{n,j}^{(t)}\rangle) =O\Big{(}\frac{\sigma_{p}^{2}d\big{(}\log(6N^{2}M^{2}/\delta)\big{)} ^{3}}{\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}\Big{)},\] (166) \[softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{\pm}^{(T)}\rangle) =O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\|\bm{w}_{O}\|_{ 2}^{2}d_{h}^{\frac{1}{2}}}\Big{)},\] (167) \[softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle) =O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\|\bm{ w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}\Big{)},\] (168) \[softmax(\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,2}^{(t)}\rangle) =1-O\Big{(}\frac{\sigma_{p}^{2}d\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{3}}{\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}} \Big{)},\] (169) \[softmax(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,2}^{(t)}\rangle) =1-O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\|\bm{w}_{O}\|_{2} ^{2}d_{h}^{\frac{1}{2}}}\Big{)}\] (170)

for \(i\in[M]\backslash\{1\},j\in[M]\backslash\{1,2\},n\in[N]\).

For \(n\in S_{+}\), substituting \(t=T=\frac{C_{1}\eta N}{\eta\epsilon\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}}\) into propositions \(\mathcal{N}(t)\) and get

\[V_{n,2}^{(t)} \geq\log\Big{(}\exp(V_{n,2}^{(T_{2})})+\frac{\eta C_{1}\sigma_{p} ^{2}d\|\bm{w}_{O}\|_{2}^{2}}{N}(t-T_{2})\Big{)}\] \[\geq\log\Big{(}\exp(V_{n,2}^{(T_{2})})+\frac{C_{20}}{\epsilon}\Big{)}\] (171) \[\geq\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)},\] \[|V_{+}^{(t)}|,|V_{n,i}^{(t)}| =o(1)\] (172)we bound \(f(\bm{X}_{n},\theta(t))\) as follows

\[f(\bm{X}_{n},\theta(t)) =\frac{1}{M}\sum_{l=1}^{M}\bm{\varphi}(\bm{x}_{n,l}^{\top}\bm{W}_{Q }^{(t)}\bm{W}_{K}^{(t)\top}\bm{X}_{n}^{\top})\bm{X}_{n}\bm{W}_{V}^{(t)}\bm{w}_{O}\] (173) \[\frac{1}{M}\cdot\Big{(}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t)}, \bm{k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle) +\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)}\] \[+\sum_{i=2}^{M}\frac{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{ (t)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)} \Big{)}\cdot V_{+}^{(T)}\] \[+\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,2}^{(t)})}{ \exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)}\] \[+\sum_{l=2}^{M}\frac{\exp(\langle\bm{q}_{n,l}^{(t)},\bm{k}_{n,2}^ {(t)}\rangle)}{\exp(\langle\bm{q}_{n,l}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\bm{q}_{n,l}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)} \cdot V_{n,2}^{(t)}\] \[+\sum_{i=2}^{M}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{ n,i}^{(t)}\rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)}\] \[+\sum_{l=2}^{M}\frac{\exp(\langle\bm{q}_{n,l}^{(t)},\bm{k}_{n,i}^ {(t)}\rangle)}{\exp(\langle\bm{q}_{n,l}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\bm{q}_{n,l}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)} \cdot V_{n,i}^{(t)}\Big{)}\] \[\geq\frac{1}{M}\cdot\Big{(}1-O\Big{(}\frac{\sigma_{p}^{2}d\big{(} \log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2} d_{h}^{\frac{1}{2}}}\Big{)}\] \[+(M-1)\cdot\Big{(}1-O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta )\big{)}^{3}}{\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}\Big{)}\Big{)}\Big{)} \cdot\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)}\] \[-\frac{1}{M}\cdot(M-1)\cdot\Big{(}O\Big{(}\frac{\sigma_{p}^{2}d \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_ {2}^{2}d_{h}^{\frac{1}{2}}}\Big{)}\] \[+(M-1)\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)} ^{3}}{\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}\Big{)}\Big{)}\cdot O(1)\] \[=\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)}-O\Big{(}\frac{\sigma_{p }^{2}d\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{ O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}\Big{)}\] \[\geq\log\Big{(}\frac{C_{20}}{\epsilon}\Big{)}-\log(C_{20})\] \[\geq\log\Big{(}\frac{1}{\epsilon}\Big{)}.\]

For the second inequality, by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\), we have \(O\Big{(}\frac{\sigma_{p}^{2}d\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\| \bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}\Big{)}=o(1)=\log(1+ o(1))\leq\log(C_{20})\) as long as \(C_{20}\) is sufficiently large.

Then we have

\[\ell_{n}^{(t)} =\log\Big{(}1+\exp(-f(\bm{X}_{n},\theta(t)))\Big{)}\] (174) \[\leq\exp(-f(\bm{X}_{n},\theta(t)))\] \[\leq\exp\Big{(}-\log\Big{(}\frac{1}{\epsilon}\Big{)}\Big{)}\] \[\leq\epsilon.\]

Similarly, we have \(\ell_{n}^{(t)}\leq\epsilon\) for \(n\in S_{-}\). Therefore, we have \(L_{S}(\theta(T))=\frac{1}{N}\sum\limits_{n=1}^{N}\ell_{n}^{(t)}\leq\epsilon\).

### Population Loss

**Lemma E.6** (Population Loss).: _Under the same conditions as Theorem 4.2, let \(T_{3}=\Theta\Big{(}\frac{N}{\eta\epsilon\sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}} \Big{)}\), we have_

\[L_{D}\big{(}\theta(T_{3})\big{)}\geq 0.1\]

Proof of Lemma E.6.: Similar to (133)

\[\|\bm{W}_{V}^{(t+1)}\bm{w}_{O}-\bm{W}_{V}^{(t)}\bm{w}_{O}\|_{2} \leq\frac{\eta}{NM}\sum\limits_{n=1}^{N}\big{|}y_{n}\ell_{n}^{ \prime(t)}|\sum\limits_{l=1}^{M}\big{\|}\bm{X}_{n}\big{\|}_{F}\big{\|}\bm{ \varphi}(\bm{x}_{n,l}\bm{W}_{Q}^{(t)}\bm{W}_{K}^{(t)\top}(\bm{X}_{n})^{\top} )\big{\|}_{2}\|\bm{w}_{O}\|_{2}^{2}\] (175) \[\leq\frac{\eta}{NM}\cdot NM\cdot O\big{(}\max\{\|\bm{\mu}\|_{2}, \sigma_{p}\sqrt{d}\}\big{)}\cdot O(1)\cdot\|\bm{w}_{O}\|_{2}^{2}\] \[=O\Big{(}\eta\cdot\sigma_{p}\sqrt{d}\cdot\|\bm{w}_{O}\|_{2}^{2} \Big{)}.\]

Taking a summation, we have

\[\|\bm{W}_{V}^{(T_{3})}\bm{w}_{O}\|_{2} \leq\|\bm{W}_{V}^{(0)}\bm{w}_{O}\|_{2}+\sum\limits_{t=0}^{T_{3}-1 }\|\bm{W}_{V}^{(t+1)}\bm{w}_{O}-\bm{W}_{V}^{(t)}\bm{w}_{O}\|_{2}\] (176) \[=\|\bm{W}_{V}^{(0)}\bm{w}_{O}\|_{2}+O\big{(}\frac{N}{\eta\epsilon \sigma_{p}^{2}d\|\bm{w}_{O}\|_{2}^{2}}\big{)}\cdot O\Big{(}\eta\cdot\sigma_{p }\sqrt{d}\cdot\|\bm{w}_{O}\|_{2}^{2}\Big{)}\] \[=O\Big{(}\sigma_{V}\|\bm{w}_{O}\|_{2}\sqrt{d}\Big{)}+O\Big{(} \frac{N}{\epsilon\sigma_{p}\sqrt{d}}\Big{)}.\]

Then \(\bm{\xi}_{i}^{\top}\bm{W}_{V}^{(T_{3})}\bm{w}_{O}\) is a Gaussian random variable with mean zero and standard deviation smaller than \(O\Big{(}\sigma_{V}\|\bm{w}_{O}\|_{2}\sigma_{p}\sqrt{d}+\frac{N}{\epsilon\sqrt{ d}}\Big{)}\). By Gaussian tail bound, for any \(i\in[M]\backslash\{1\}\), with probability at least \(1-1/2M\),

\[|\bm{\xi}_{i}^{\top}\bm{W}_{V}^{(T_{3})}\bm{w}_{O}|\leq O\Big{(}\sigma_{V}\| \bm{w}_{O}\|_{2}\sigma_{p}\sqrt{d}+\frac{N}{\epsilon\sqrt{d}}\Big{)}\cdot \sqrt{2\log\big{(}4M\big{)}}\leq 1/2,\]

where the last inequality is by \(\sigma_{V}\leq\widetilde{O}\big{(}\|\bm{w}_{O}\|_{2}^{-1}\cdot\min\{\|\bm{\mu} \|_{2}^{-1},(\sigma_{p}\sqrt{d})^{-1}\}\cdot d_{h}^{-\frac{1}{4}}\big{)}\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\). Applying a union bound, with probability at least \(1-1/2\), \(|\bm{\xi}_{i}^{\top}\bm{W}_{V}^{(T_{3})}\bm{w}_{O}|\leq 1/2\). Recall that \(V_{\pm}^{(T_{3})}=o(1)\), with probability at least \(1-1/2\), we have

\[y(f(\bm{X},\theta(T_{3}))) =\frac{1}{M}\sum\limits_{l=1}^{M}\bm{\varphi}(\bm{x}_{l}^{\top} \bm{W}_{Q}^{(T_{3})}\bm{W}_{K}^{(T_{3})\top}\bm{X}^{\top})\bm{X}\bm{W}_{V}^{(T_ {3})}\bm{w}_{O}\] (177) \[\geq\log(1+e^{-1/2}).\]

Thus, \(L_{D}(\theta(t))\geq\log(1+e^{-1/2})\cdot 0.5\geq 0.1\).

Complete Calculation Process For Benign Overfitting

In this section, we show more calculation process under benign overfitting regime. The calculations for harmful overfitting is similar.

### Calculations for \(\alpha\) and \(\beta\)

In this subsection, we give the calculations for \(\alpha\) and \(\beta\) defined in Definition B.3.

**Restatement of Lemma B.1.** the gradients of loss function \(L_{S}(\theta)\) with respect to \(\bm{W}_{Q},\bm{W}_{K}\) and \(\bm{W}_{V}\) are given by

\[\begin{split}\nabla_{\bm{W}_{Q}}L_{S}(\theta)&= \frac{1}{NM}\sum_{n\in S_{+}}\ell^{\prime}_{n}(\theta)(\bm{\mu}_{+}\bm{w}_{O}^ {\top}\bm{W}_{V}^{\top}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,1})-\bm{\varphi} _{n,1}^{\top}\bm{\varphi}_{n,1})\\ &+\sum_{i=2}^{M}\bm{\xi}_{n,i}\bm{w}_{O}^{\top}\bm{W}_{V}^{\top} \bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{\varphi}_{n,i}^{\top}\bm{\varphi }_{n,i}))\bm{X}_{n}\bm{W}_{K}\\ &-\frac{1}{NM}\sum_{n\in S_{-}}\ell^{\prime}_{n}(\theta)(\bm{\mu} _{-}\bm{w}_{O}^{\top}\bm{W}_{V}^{\top}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,1})-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1})\\ &+\sum_{i=2}^{M}\bm{\xi}_{n,i}\bm{w}_{O}^{\top}\bm{W}_{V}^{\top} \bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{\varphi}_{n,i}^{\top}\bm{\varphi }_{n,i}))\bm{X}_{n}\bm{W}_{K},\end{split}\] (178)

\[\begin{split}\nabla_{\bm{W}_{K}}L_{S}(\theta)&= \frac{1}{NM}\sum_{n\in S_{+}}\ell^{\prime}_{n}(\theta)(\bm{X}_{n}^{\top}(diag( \bm{\varphi}_{n,1})-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1})\bm{X}_{n}\bm {W}_{V}\bm{w}_{O}\bm{\mu}_{+}^{\top}\\ &+\sum_{i=2}^{M}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{ \varphi}_{n,i}^{\top}\bm{\varphi}_{n,i})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\xi }_{n,i}^{\top})\bm{W}_{Q}\\ &-\frac{1}{NM}\sum_{n\in S_{-}}\ell^{\prime}_{n}(\theta)(\bm{X}_{ n}^{\top}(diag(\bm{\varphi}_{n,1})-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1}) \bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\mu}_{-}^{\top}\\ &+\sum_{i=2}^{M}\bm{X}_{n}^{\top}(diag(\bm{\varphi}_{n,i})-\bm{ \varphi}_{n,i}^{\top}\bm{\varphi}_{n,i})\bm{X}_{n}\bm{W}_{V}\bm{w}_{O}\bm{\xi }_{n,i}^{\top})\bm{W}_{Q}.\end{split}\] (179)

We will give expressions for \(\alpha\) and \(\beta\) defined in Definition B.3 based on these two equations above. By (178) and the orthogonal relation between \(\bm{\mu}\) and \(\bm{\xi}\), we have

\[\begin{split}\Delta\bm{q}_{+}^{(t)}=\bm{\mu}_{+}^{\top}\Delta \bm{W}_{Q}^{(t)}&=\frac{\eta}{NM}\sum_{n\in S_{+}}-\ell^{\prime (t)}_{n}\|\bm{\mu}\|_{2}^{2}\bm{w}_{O}^{\top}\bm{W}_{V}^{(t)\top}\bm{X}_{n}^{ \top}(diag(\bm{\varphi}_{n,1})-\bm{\varphi}_{n,1}^{\top}\bm{\varphi}_{n,1}) \bm{X}_{n}\bm{W}_{K}^{(t)}\\ &=\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n\in S_{+}}\sum_{i=2}^ {M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\end{split}\] (180)

where \(\bm{w}_{O}^{\top}\bm{W}_{V}^{(t)\top}\bm{X}_{n}^{\top}\) and \(\bm{X}_{n}\bm{W}_{K}^{(t)}\) can be viewed in the following forms

\[\bm{w}_{O}^{\top}\bm{W}_{V}^{(t)\top}\bm{X}_{n}^{\top}=\big{(}V_{+}^{(t)},V_{n,2}^{(t)},\ldots,V_{n,M}^{(t)}\big{)},\]

\[\bm{X}_{n}\bm{W}_{K}^{(t)}=\big{(}\bm{k}_{+}^{(t)\top},\bm{k}_{n,2}^{(t)\top}, \ldots,\bm{k}_{n,M}^{(t)\top}\big{)}^{\top}.\]Then we can express \(\alpha^{(t)}_{+,+}\) and \(\alpha^{(t)}_{n,+,i}\) as follows

\[\begin{split}&\alpha^{(t)}_{+,+}=\frac{\eta}{NM}\sum\limits_{n\in S_ {+}}-\ell^{\prime(t)}_{n}\|\boldsymbol{\mu}\|_{2}^{2}\\ &\cdot\Big{(}V^{(t)}_{+}\big{(}\frac{\exp(\langle\boldsymbol{q}^{ (t)}_{+},\boldsymbol{k}^{(t)}_{+}\rangle)}{\exp(\langle\boldsymbol{q}^{(t)}_ {+},\boldsymbol{k}^{(t)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}^{(t)}_{+},\boldsymbol{k}^{(t)}_{n,j}\rangle)}\\ &-(\frac{\exp(\langle\boldsymbol{q}^{(t)}_{+},\boldsymbol{k}^{( t)}_{+}\rangle)}{\exp(\langle\boldsymbol{q}^{(t)}_{+},\boldsymbol{k}^{(t)}_{+} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}^{(t)}_{+}, \boldsymbol{k}^{(t)}_{n,j}\rangle)})^{2}\big{)}\\ &-\sum\limits_{i=2}^{M}\big{(}V^{(t)}_{n,i}\cdot\frac{\exp( \langle\boldsymbol{q}^{(t)}_{+},\boldsymbol{k}^{(t)}_{+}\rangle)}{\exp( \langle\boldsymbol{q}^{(t)}_{+},\boldsymbol{k}^{(t)}_{+}\rangle)+\sum\limits _{j=2}^{M}\exp(\langle\boldsymbol{q}^{(t)}_{+},\boldsymbol{k}^{(t)}_{n,j} \rangle)}\\ &\cdot\frac{\exp(\langle\boldsymbol{q}^{(t)}_{+},\boldsymbol{k}^ {(t)}_{n,i}\rangle)}{\exp(\langle\boldsymbol{q}^{(t)}_{+},\boldsymbol{k}^{(t)} _{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}^{(t)}_{+}, \boldsymbol{k}^{(t)}_{n,j}\rangle)})\Big{)},\end{split}\] (181)

Using the similar method as for \(\Delta\boldsymbol{q}^{(t)}_{+}\), we get the other \(\alpha\) and \(\beta\) as follows\[\alpha^{(t)}_{-,-}=\frac{\eta}{NM}\sum\limits_{n\in S_{-}}\ell^{\prime (t)}_{n}\|\boldsymbol{\mu}\|_{2}^{2}\] \[\cdot\Big{(}V_{-}^{(t)}\big{(}\frac{\exp(\langle\boldsymbol{q}_{- }^{(t)},\boldsymbol{k}_{-}^{(t)}\rangle)}{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{-}^{(t)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol {q}_{-}^{(t)},\boldsymbol{k}_{n,j}^{(t)}\rangle)}\] \[-(\frac{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{-}^ {(t)}\rangle)}{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{-}^{(t)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{-}^{(t)}, \boldsymbol{k}_{n,j}^{(t)}\rangle)})^{2}\big{)}\] (183) \[-\sum\limits_{i=2}^{M}\big{(}V_{n,i}^{(t)}\cdot\frac{\exp( \langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{-}^{(t)}\rangle)}{\exp( \langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{-}^{(t)}\rangle)+\sum\limits_ {j=2}^{M}\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{n,j}^{(t)} \rangle)}\] \[\cdot\frac{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{n,i}^{(t)}\rangle)}{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{-}^{( t)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{-}^{(t)}, \boldsymbol{k}_{n,j}^{(t)}\rangle)})\Big{)},\]

\[\alpha^{(t)}_{n,-,i}=\frac{\eta}{NM}\ell^{\prime(t)}_{n}\| \boldsymbol{\mu}\|_{2}^{2}\] \[\cdot\Big{(}-V_{-}^{(t)}\cdot\frac{\exp(\langle\boldsymbol{q}_{- }^{(t)},\boldsymbol{k}_{-}^{(t)}\rangle)}{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{-}^{(t)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol {q}_{-}^{(t)},\boldsymbol{k}_{n,j}^{(t)}\rangle)}\] \[\cdot\frac{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{ n,i}^{(t)}\rangle)}{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{-}^{(t)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{-}^{(t)}, \boldsymbol{k}_{n,j}^{(t)}\rangle)}\] \[+V_{n,i}^{(t)}\big{(}\frac{\exp(\langle\boldsymbol{q}_{-}^{(t)}, \boldsymbol{k}_{n,i}^{(t)}\rangle)}{\exp(\langle\boldsymbol{q}_{-}^{(t)}, \boldsymbol{k}_{-}^{(t)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol {q}_{-}^{(t)},\boldsymbol{k}_{n,j}^{(t)}\rangle)}\] (184) \[-(\frac{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{n,i} ^{(t)}\rangle)}{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{-}^{(t)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{-}^{(t)}, \boldsymbol{k}_{n,j}^{(t)}\rangle)})^{2}\big{)}\] \[-\sum\limits_{k\neq i}\big{(}V_{n,k}^{(t)}\cdot\frac{\exp( \langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{n,i}^{(t)}\rangle)}{\exp( \langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{n,i}^{(t)}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{n,j}^{(t )}\rangle)}\] \[\cdot\frac{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{ n,k}^{(t)}\rangle)}{\exp(\langle\boldsymbol{q}_{-}^{(t)},\boldsymbol{k}_{-}^{(t)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{-}^{(t)}, \boldsymbol{k}_{n,j}^{(t)}\rangle)})\Big{)},\]\[\alpha^{(t)}_{n^{\prime},i^{\prime},-}=\frac{\eta}{NM}\sum_{n\in S_{+}} -\ell^{\prime(t)}_{n}\sum_{i=2}^{M}\langle\boldsymbol{\xi}_{n^{\prime},i^{ \prime}},\boldsymbol{\xi}_{n,i}\rangle\] \[\cdot\Big{(}V^{(t)}_{+}\big{(}\frac{\exp(\langle\boldsymbol{q}^ {(t)}_{n,i},\boldsymbol{k}^{(t)}_{+}\rangle)}{\exp(\langle\boldsymbol{q}^{(t) }_{n,i},\boldsymbol{k}^{(t)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}^{(t)}_{n,i},\boldsymbol{k}^{(t)}_{n,j}\rangle)}\] \[-(\frac{\exp(\langle\boldsymbol{q}^{(t)}_{n,i},\boldsymbol{k}^{( t)}_{+}\rangle)}{\exp(\langle\boldsymbol{q}^{(t)}_{n,i},\boldsymbol{k}^{(t)}_{+} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}^{(t)}_{n,i}, \boldsymbol{k}^{(t)}_{n,j}\rangle)})^{2}\big{)}\] (185) \[-\sum\limits_{k=2}^{M}\big{(}V^{(t)}_{n,i}\cdot\frac{\exp( \langle\boldsymbol{q}^{(t)}_{n,i},\boldsymbol{k}^{(t)}_{+}\rangle)}{\exp( \langle\boldsymbol{q}^{(t)}_{n,i},\boldsymbol{k}^{(t)}_{+}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\boldsymbol{q}^{(t)}_{n,i},\boldsymbol{k}^{(t)}_ {n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\boldsymbol{q}^{(t)}_{n,i},\boldsymbol{k}^ {(t)}_{n,k}\rangle)}{\exp(\langle\boldsymbol{q}^{(t)}_{n,i},\boldsymbol{k}^{( t)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}^{(t)}_{n,i}, \boldsymbol{k}^{(t)}_{n,j}\rangle)}\big{)}\Big{)},\]\[\alpha^{(t)}_{n^{\prime},i^{\prime},n,i}=\frac{\eta}{NM}-\ell^{\prime (t)}_{n}\sum\limits_{k=2}^{M}\langle\boldsymbol{\xi}_{n^{\prime},i^{\prime}}, \boldsymbol{\xi}_{n,k}\rangle\] \[\cdot\Big{(}-V^{(t)}_{+}\cdot\frac{\exp(\langle\boldsymbol{q}^{( t)}_{n,k},\boldsymbol{k}^{(t)}_{+}\rangle)}{\exp(\langle\boldsymbol{q}^{(t)}_{n,k}, \boldsymbol{k}^{(t)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol {q}^{(t)}_{n,k},\boldsymbol{k}^{(t)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k} ^{(t)}_{n,i}\rangle)}{\exp(\langle\boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k}^ {(t)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k}^{(t)}_{n,j}\rangle)}\] \[+V^{(t)}_{n,i}\big{(}\frac{\exp(\langle\boldsymbol{q}^{(t)}_{n,k },\boldsymbol{k}^{(t)}_{n,i}\rangle)}{\exp(\langle\boldsymbol{q}^{(t)}_{n,k}, \boldsymbol{k}^{(t)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol {q}^{(t)}_{n,k},\boldsymbol{k}^{(t)}_{n,j}\rangle)}\] (187) \[-(\frac{\exp(\langle\boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k}^{( t)}_{n,i}\rangle)}{\exp(\langle\boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k}^ {(t)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k}^{(t)}_{n,j}\rangle)})^{2}\big{)}\] \[-\sum\limits_{l\neq i}\big{(}V^{(t)}_{n,l}\cdot\frac{\exp(\langle \boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k}^{(t)}_{n,i}\rangle)}{\exp(\langle \boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k}^{(t)}_{+}\rangle)+\sum\limits_{j=2} ^{M}\exp(\langle\boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k}^{(t)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k} ^{(t)}_{n,l}\rangle)}{\exp(\langle\boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k}^ {(t)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}^{(t)}_{n,k},\boldsymbol{k}^{(t)}_{n,j}\rangle)}\big{)}\Big{)}\]

for \(n\in S_{+}\),\[\alpha^{(t)}_{n^{\prime},i^{\prime},n,i}=\frac{\eta}{NM}\ell^{\prime(t)}_{n }\sum\limits_{k=2}^{M}\langle\bm{\xi}_{n^{\prime},i^{\prime}},\bm{\xi}_{n,k}\rangle\] \[\cdot\Big{(}-V^{(t)}_{-}\cdot\frac{\exp(\langle\bm{q}^{(t)}_{n,k}, \bm{k}^{(t)}_{-}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{-} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{n, j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{n,i} \rangle)}{\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{-}\rangle)+\sum\limits _{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{n,j}\rangle)}\] \[+V^{(t)}_{n,i}\big{(}\frac{\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k} ^{(t)}_{n,i}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{-}\rangle)+ \sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{n,j}\rangle)}\] (188) \[-(\frac{\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{n,i}\rangle) }{\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{-}\rangle)+\sum\limits_{j=2}^{M }\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{n,j}\rangle)})^{2}\big{)}\] \[-\sum\limits_{l\neq i}\big{(}V^{(t)}_{n,l}\cdot\frac{\exp(\langle \bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{n,i}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n,k}, \bm{k}^{(t)}_{-}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,k}, \bm{k}^{(t)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{n,l} \rangle)}{\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{-}\rangle)+\sum\limits _{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,k},\bm{k}^{(t)}_{n,j}\rangle)}\big{)}\]

for \(n\in S_{-}\),

\[\beta^{(t)}_{-,-}=\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\sum\limits_ {n\in S_{-}}\ell^{\prime(t)}_{n}\] \[\cdot\Big{(}V^{(t)}_{-}\big{(}\frac{\exp(\langle\bm{q}^{(t)}_{-}, \bm{k}^{(t)}_{-}\rangle)}{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-}\rangle) +\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,j}\rangle)}\] \[-(\frac{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-}\rangle)}{ \exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,j}\rangle)})^{2}\big{)}\] (189) \[-\sum\limits_{i=2}^{M}\big{(}V^{(t)}_{n,i}\cdot\frac{\exp(\langle \bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-}\rangle)}{\exp(\langle\bm{q}^{(t)}_{-},\bm{k} ^{(t)}_{-}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{( t)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,i}\rangle) }{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,j}\rangle)}\big{)},\]\[\beta_{n,+,i}^{(t)}=-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\ell_{n}^{ \prime(t)}\] \[\cdot\Big{(}V_{+}^{(t)}\big{(}\frac{\exp(\langle\bm{q}_{n,i}^{(t)}, \bm{k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t) }\rangle)}\] \[-(\frac{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle)}{ \exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)})^{2}\big{)}\] (190) \[-\sum\limits_{k=2}^{M}\big{(}V_{n,i}^{(t)}\cdot\frac{\exp(\langle \bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(t)}, \bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(t)}, \bm{k}_{n,j}^{(t)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,k}^{(t)} \rangle)}{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_ {j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)})\Big{)},\]\[\beta^{(t)}_{n^{\prime},i^{\prime},-}=\frac{\eta}{NM}\sum\limits_{n\in S_{-}} \ell^{\prime(t)}_{n}\] \[\Big{(}-V^{(t)}_{-}\sum\limits_{i=2}^{M}\big{(}\langle\bm{\xi}_{n^ {\prime},i^{\prime}},\bm{\xi}_{n,i}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{-}, \bm{k}^{(t)}_{-}\rangle)}{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-}\rangle) +\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{n,i} \rangle)}{\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{+}\rangle)+\sum\limits_{j =2}^{M}\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{n,j}\rangle)}\] \[+\sum\limits_{k=2}^{M}V^{(t)}_{n,k}\big{(}\langle\bm{\xi}_{n^{ \prime},i^{\prime}},\bm{\xi}_{n,k}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{+}, \bm{k}^{(t)}_{n,k}\rangle)}{\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{+} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{n,j} \rangle)}\] (192) \[-\sum\limits_{i=2}^{M}\big{(}\langle\bm{\xi}_{n^{\prime},i^{ \prime}},\bm{\xi}_{n,i}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{ n,k}\rangle)}{\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{+}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{+}\rangle)} {\exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{+}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{n,j}\rangle)})\Big{)},\]

\[\beta^{(t)}_{n^{\prime},i^{\prime},-}=\frac{\eta}{NM}\sum\limits_{n\in S_{-}} \ell^{\prime(t)}_{n}\] \[\Big{(}-V^{(t)}_{-}\sum\limits_{i=2}^{M}\big{(}\langle\bm{\xi}_{ n^{\prime},i^{\prime}},\bm{\xi}_{n,i}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{-}, \bm{k}^{(t)}_{-}\rangle)}{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-}\rangle) +\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,i}\rangle)} {\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,j}\rangle)}\] \[+\sum\limits_{k=2}^{M}V^{(t)}_{n,k}\big{(}\langle\bm{\xi}_{n^{ \prime},i^{\prime}},\bm{\xi}_{n,k}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{-}, \bm{k}^{(t)}_{n,k}\rangle)}{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-}\rangle )+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,j}\rangle)}\] (193) \[-\sum\limits_{i=2}^{M}\big{(}\langle\bm{\xi}_{n^{\prime},i^{ \prime}},\bm{\xi}_{n,i}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{ n,k}\rangle)}{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,i}\rangle)} {\exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{-}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}^{(t)}_{-},\bm{k}^{(t)}_{n,j}\rangle)})\Big{)},\]\[\beta^{(t)}_{n^{\prime},i^{\prime},n,i}=\frac{\eta}{NM}\ell^{\prime(t)}_{n}\] \[\Big{(}-V^{(t)}_{-}\sum\limits_{k=2}^{M}\big{(}\langle\bm{\xi}_{n^ {\prime},i^{\prime}},\bm{\xi}_{n,k}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{n,i}, \bm{k}^{(t)}_{-}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{-} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n, j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,k} \rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{+}\rangle)+\sum\limits _{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,j}\rangle)}\Big{)}\] \[+\sum\limits_{k=2}^{M}V^{(t)}_{n,k}\big{(}\langle\bm{\xi}_{n^{ \prime},i^{\prime}},\bm{\xi}_{n,k}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,k}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{+} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n, j}\rangle)}\] (194) \[-\sum\limits_{l=2}^{M}\big{(}\langle\bm{\xi}_{n^{\prime},i^{ \prime}},\bm{\xi}_{n,l}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t) }_{n,k}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{+}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,l} \rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{+}\rangle)+\sum\limits _{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,j}\rangle)}\big{)}\Big{)}\]

for \(n\in S_{+}\),

\[\beta^{(t)}_{n^{\prime},i^{\prime},n,i}=\frac{\eta}{NM}\ell^{ \prime(t)}_{n}\] \[\Big{(}-V^{(t)}_{-}\sum\limits_{k=2}^{M}\big{(}\langle\bm{\xi}_{ n^{\prime},i^{\prime}},\bm{\xi}_{n,k}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{n,i}, \bm{k}^{(t)}_{-}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{-} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n, j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,k} \rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{-}\rangle)+\sum\limits _{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,j}\rangle)}\Big{)}\] \[+\sum\limits_{k=2}^{M}V^{(t)}_{n,k}\big{(}\langle\bm{\xi}_{n^{ \prime},i^{\prime}},\bm{\xi}_{n,k}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{n,i}, \bm{k}^{(t)}_{n,k}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{-} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n, j}\rangle)}\] (195) \[-\sum\limits_{l=2}^{M}\big{(}\langle\bm{\xi}_{n^{\prime},i^{ \prime}},\bm{\xi}_{n,l}\rangle\frac{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{ n,k}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{-}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,l} \rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{-}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,j}\rangle)}\big{)}\Big{)}\]

for \(n\in S_{-}\).

### Update Rules for Inner Products

In this subsection, we give the update rules for the inner products of \(\bm{q}\) and \(\bm{k}\).

**Restatement of Lemma B.3.** The dynamics of \(\bm{x}\bm{W}_{Q}\bm{W}_{K}\bm{X}^{\top}\) can be characterized as follows\[\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle\bm{q}_{+}^ {(t)},\bm{k}_{+}^{(t)}\rangle\] \[=\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\sum_{n\in S_{+}} \sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\] \[+\beta_{++,+}^{(t)}\|\bm{q}_{+}^{(t)}\|_{2}^{2}+\sum_{n\in S_{+}} \sum_{i=2}^{M}\beta_{n,+,i}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)}\rangle\] \[+\Big{(}\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n\in S_{+}} \sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{++,+}^{(t)}\bm{q}_{+}^{(t)\top}+\sum_{n\in S_{ +}}\sum_{i=2}^{M}\beta_{n^{\prime},+,i}^{(t)\top}\bm{q}_{n^{\prime},l}^{(t) \top}\Big{)},\]

\[\langle\bm{q}_{-}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle-\langle\bm{q} _{-}^{(t)},\bm{k}_{-}^{(t)}\rangle\] \[=\alpha_{-,-}^{(t)}\|\bm{k}_{-}^{(t)}\|_{2}^{2}+\sum_{n\in S_{-}} \sum_{i=2}^{M}\alpha_{n,-,i}^{(t)}\langle\bm{k}_{-}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\] \[+\beta_{-,-}^{(t)}\|\bm{q}_{-}^{(t)}\|_{2}^{2}+\sum_{n\in S_{-}} \sum_{i=2}^{M}\beta_{n,-,i}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)}\rangle\] \[+\Big{(}\alpha_{-,-}^{(t)}\bm{k}_{-}^{(t)}+\sum_{n\in S_{-}}\sum_ {i=2}^{M}\alpha_{n,-,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{-,-}^{(t)}\bm{q}_{-}^{(t)\top}+\sum_{n\in S_{- }}\sum_{i=2}^{M}\beta_{n,-,i}^{(t)}\bm{q}_{n,i}^{(t)\top}\Big{)},\]

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle\bm{ q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle\] \[=\alpha_{n,i,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\alpha_{n,i,-}^ {(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle+\sum_{n^{\prime}=1}^{N} \sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{n ^{\prime},l}^{(t)}\rangle\] \[+\beta_{++,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)} \rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(t)} \langle\bm{q}_{n,i}^{(t)},\bm{q}_{n^{\prime},l}^{(t)}\rangle\] \[+\Big{(}\alpha_{n,i,+}^{(t)}\bm{k}_{+}^{(t)}+\alpha_{n,i,-}^{(t)} \bm{k}_{-}^{(t)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime}, l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{++,+}^{(t)}\bm{q}_{+}^{(t)\top}+\sum_{n^{\prime }\in S_{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(t)}\bm{q}_{n^{\prime},l}^{(t) \top}\Big{)},\]\[\langle\bm{q}_{-}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle-\langle\bm{q}_{ -}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] \[=\alpha_{-,-}^{(t)}\langle\bm{k}_{-}^{(t)},\bm{k}_{n,j}^{(t)} \rangle+\sum_{n^{\prime}\in S_{-}}\sum_{l=2}^{M}\alpha_{n^{\prime},-,l}^{(t)} \langle\bm{k}_{n,j}^{(t)},\bm{k}_{n^{\prime},l}^{(t)}\rangle\] \[+\beta_{n,j,-}^{(t)}\|\bm{q}_{-}^{(t)}\|_{2}^{2}+\beta_{n,j,+}^{( t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{-}^{(t)}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M} \beta_{n,j,n^{\prime},l}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{q}_{n^{\prime},l}^{ (t)}\rangle\] \[+\Big{(}\alpha_{-,-}^{(t)}\bm{k}_{-}^{(t)}+\sum_{n^{\prime}\in S _{-}}\sum_{l=2}^{M}\alpha_{n^{\prime},-,l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)} \Big{)}\] \[\cdot\Big{(}\beta_{n,j,+}^{(t)}\bm{q}_{+}^{(t)\top}+\beta_{n,j,-} ^{(t)\top}\bm{q}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n, j,n^{\prime},l}^{(t)\top}\bm{q}_{n^{\prime},l}^{(t)\top}\Big{)},\]

[MISSING_PAGE_EMPTY:91]

\[\begin{split}&\langle\bm{q}_{+}^{(t+1)},\bm{q}_{-}^{(t+1)}-\langle\bm{q }_{+}^{(t)},\bm{q}_{-}^{(t)}\rangle\\ &=\langle\Delta\bm{q}_{+}^{(t)},\bm{q}_{-}^{(t)}\rangle+\langle \Delta\bm{q}_{+}^{(t)},\Delta\bm{q}_{-}^{(t)}\rangle+\langle\Delta\bm{q}_{+}^{ (t)},\Delta\bm{q}_{-}^{(t)}\rangle\\ &=\alpha_{++}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{+}^{(t)} \rangle+\sum_{n\in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\langle\bm{q}_{-}^ {(t)},\bm{k}_{n,i}^{(t)}\rangle\\ &+\alpha_{-,-}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{-}^{(t)} \rangle+\sum_{n\in S_{-}}\sum_{i=2}^{M}\alpha_{n,-,i}^{(t)}\langle\bm{q}_{+}^ {(t)},\bm{k}_{n,i}^{(t)}\rangle\\ &+\Big{(}\alpha_{++,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n\in S_{+}} \sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\\ &\cdot\Big{(}\alpha_{-,-}^{(t)}\bm{k}_{-}^{(t)\top}+\sum_{n\in S _{-}}\sum_{i=2}^{M}\alpha_{n,-,i}^{(t)}\bm{k}_{n,i}^{(t)\top}\Big{)},\end{split}\] (198)

\[\begin{split}&\|\bm{q}_{+}^{(t+1)}\|_{2}^{2}-\|\bm{q}_{+}^{(t)} \|_{2}^{2}\\ &=2\langle\Delta\bm{q}_{+}^{(t)},\bm{q}_{+}^{(t)}\rangle+\langle \Delta\bm{q}_{+}^{(t)},\Delta\bm{q}_{+}^{(t)}\rangle\\ &=2\alpha_{++}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle+2\sum_{n\in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\langle\bm{q}_{+}^ {(t)},\bm{k}_{n,i}^{(t)}\rangle\\ &+\Big{(}\alpha_{++,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n\in S_{+}} \sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\\ &\cdot\Big{(}\alpha_{++}^{(t)}\bm{k}_{+}^{(t)\top}+\sum_{n\in S_{ +}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)\top}\Big{)},\end{split}\] (199)

\[\begin{split}&\|\bm{q}_{-}^{(t+1)}\|_{2}^{2}-\|\bm{q}_{-}^{(t)} \|_{2}^{2}\\ &=2\langle\Delta\bm{q}_{-}^{(t)},\bm{q}_{-}^{(t)}\rangle+\langle \Delta\bm{q}_{-}^{(t)},\Delta\bm{q}_{-}^{(t)}\rangle\\ &=2\alpha_{-,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{-}^{(t)} \rangle+2\sum_{n\in S_{-}}\sum_{i=2}^{M}\alpha_{n,-,i}^{(t)}\langle\bm{q}_{-}^ {(t)},\bm{k}_{n,i}^{(t)}\rangle\\ &+\Big{(}\alpha_{-,-}^{(t)}\bm{k}_{-}^{(t)}+\sum_{n\in S_{-}} \sum_{i=2}^{M}\alpha_{n,-,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\\ &\cdot\Big{(}\alpha_{-,-}^{(t)}\bm{k}_{-}^{(t)\top}+\sum_{n\in S _{-}}\sum_{i=2}^{M}\alpha_{n,-,i}^{(t)\top}\bm{k}_{n,i}^{(t)\top}\Big{)}, \end{split}\] (200)

\[\begin{split}&\|\bm{q}_{n,i}^{(t+1)}\|_{2}^{2}-\|\bm{q}_{n,i}^{(t)} \|_{2}^{2}\\ &=2\langle\Delta\bm{q}_{n,i}^{(t)},\bm{q}_{n,i}^{(t)}\rangle+ \langle\Delta\bm{q}_{n,i}^{(t)},\Delta\bm{q}_{n,i}^{(t)}\rangle\\ &=2\alpha_{n,i+}^{(t)}\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)} \rangle+2\alpha_{n,i,-}^{(t)}\langle\bm{q}_{n,i}^{(t)},\bm{k}_{-}^{(t)} \rangle+2\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)} \langle\bm{q}_{n,i}^{(t)},\bm{k}_{n^{\prime},l}^{(t)}\rangle\\ &+\Big{(}\alpha_{n,i,+}^{(t)}\bm{k}_{+}^{(t)}+\alpha_{n,i,-}^{(t) }\bm{k}_{-}^{(t)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l }^{(t)}\bm{k}_{n^{\prime},l}^{(t)\top}\Big{)}\\ &\cdot\Big{(}\alpha_{n,i,+}^{(t)\top}\bm{k}_{+}^{(t)\top}+\alpha_ {n,i,-}^{(t)}\bm{k}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)\top}\Big{)},\end{split}\] (201)\[\langle\bm{q}_{+}^{(t+1)},\bm{q}_{n,i}^{(t+1)}\rangle-\langle\bm{q}_{+}^ {(t)},\bm{q}_{n,i}^{(t)}\rangle\] \[=\langle\Delta\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)}\rangle+\langle \bm{q}_{+}^{(t)},\Delta\bm{q}_{n,i}^{(t)}\rangle+\langle\Delta\bm{q}_{+}^{(t)},\Delta\bm{q}_{n,i}^{(t)}\rangle\] \[=\alpha_{+,+}^{(t)}\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)} \rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\alpha_{n^{\prime},+,l}^{(t)} \langle\bm{q}_{n,i}^{(t)},\bm{k}_{n^{\prime},l}^{(t)}\rangle\] \[+\alpha_{n,i,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle+\alpha_{n,i,-}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\langle \bm{q}_{+}^{(t)},\bm{k}_{n^{\prime},l}^{(t)}\rangle\] (202) \[+\Big{(}\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n\in S_{+}}\sum_ {i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\] \[\cdot\Big{(}\alpha_{n,i,+}^{(t)}\bm{k}_{+}^{(t)\top}+\alpha_{n,i, -}^{(t)}\bm{k}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)\top}\Big{)},\]

\[\langle\bm{q}_{-}^{(t+1)},\bm{q}_{n,i}^{(t+1)}\rangle-\langle\bm{ q}_{-}^{(t)},\bm{q}_{n,i}^{(t)}\rangle\] \[=\langle\Delta\bm{q}_{-}^{(t)},\bm{q}_{n,i}^{(t)}\rangle+\langle \bm{q}_{-}^{(t)},\Delta\bm{q}_{n,i}^{(t)}\rangle+\langle\Delta\bm{q}_{-}^{(t)},\Delta\bm{q}_{n,i}^{(t)}\rangle\] \[=\alpha_{-,-}^{(t)}\langle\bm{q}_{n,i}^{(t)},\bm{k}_{-}^{(t)} \rangle+\sum_{n^{\prime}\in S_{-}}\sum_{l=2}^{M}\alpha_{n^{\prime},-,l}^{(t)} \langle\bm{q}_{n,i}^{(t)},\bm{k}_{n^{\prime},l}^{(t)}\rangle\] \[+\alpha_{n,i,+}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{+}^{(t)} \rangle+\alpha_{n,i,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{-}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\langle \bm{q}_{-}^{(t)},\bm{k}_{n^{\prime},l}^{(t)}\rangle\] (203) \[+\Big{(}\alpha_{-,-}^{(t)}\bm{k}_{-}^{(t)}+\sum_{n\in S_{-}}\sum_ {i=2}^{M}\alpha_{n,-,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\] \[\cdot\Big{(}\alpha_{n,i,+}^{(t)}\bm{k}_{+}^{(t)\top}+\alpha_{n,i, -}^{(t)\top}\bm{k}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_ {n,i,n^{\prime},l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)\top}\Big{)},\]

\[\langle\bm{q}_{n,i}^{(t+1)},\bm{q}_{n,j}^{(t+1)}\rangle-\langle \bm{q}_{n,i}^{(t)},\bm{q}_{n,j}^{(t)}\rangle\] \[=\langle\Delta\bm{q}_{n,i}^{(t)},\bm{q}_{n,j}^{(t)}\rangle+ \langle\bm{q}_{n,i}^{(t)},\Delta\bm{q}_{n,j}^{(t)}\rangle+\langle\Delta\bm{q}_ {n,i}^{(t)},\Delta\bm{q}_{n,j}^{(t)}\rangle\] \[=\alpha_{n,i,+}^{(t)}\langle\bm{q}_{n,j}^{(t)},\bm{k}_{+}^{(t)} \rangle+\alpha_{n,i,-}^{(t)}\langle\bm{q}_{n,j}^{(t)},\bm{k}_{-}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(t)}\langle \bm{q}_{n,j}^{(t)},\bm{k}_{n^{\prime},l}^{(t)}\rangle\] \[+\alpha_{n,j,+}^{(t)}\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)} \rangle+\alpha_{n,j,-}^{(t)}\langle\bm{q}_{n,i}^{(t)},\bm{k}_{-}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,j,n^{\prime},l}^{(t)}\langle \bm{q}_{n,i}^{(t)},\bm{k}_{n^{\prime},l}^{(t)}\rangle\] (204) \[+\Big{(}\alpha_{n,i,+}^{(t)}\bm{k}_{+}^{(t)}+\alpha_{n,i,-}^{(t)} \bm{k}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{ \prime},l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)\top}\Big{)}\] \[\cdot\Big{(}\alpha_{n,j,+}^{(t)}\bm{k}_{+}^{(t)\top}+\alpha_{n,j, -}^{(t)\top}\bm{k}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n, j,n^{\prime},l}^{(t)}\bm{k}_{n^{\prime},l}^{(t)\top}\Big{)},\]\[\begin{split}&\|\bm{k}_{-}^{(t+1)}\|_{2}^{2}-\|\bm{k}_{-}^{(t)}\|_{2} ^{2}\\ &=2\langle\Delta\bm{k}_{-}^{(t)},\bm{k}_{-}^{(t)}\rangle+\langle \Delta\bm{k}_{-}^{(t)},\Delta\bm{k}_{-}^{(t)}\rangle\\ &=2\beta_{-,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{-}^{(t)} \rangle+2\sum_{n\in S_{-}}\sum_{i=2}^{M}\beta_{n,-,i}^{(t)}\langle\bm{q}_{n,i} ^{(t)},\bm{k}_{-}^{(t)}\rangle\\ &+\Big{(}\beta_{-,-}^{(t)}\bm{q}_{-}^{(t)}+\sum_{n\in S_{-}}\sum_ {i=2}^{M}\beta_{n,-,i}^{(t)}\bm{q}_{n,i}^{(t)}\Big{)}\\ &\cdot\Big{(}\beta_{-,-}^{(t)}\bm{q}_{-}^{(t)\top}+\sum_{n\in S_{ -}}\sum_{i=2}^{M}\beta_{n,-,i}^{(t)}\bm{q}_{n,i}^{(t)\top}\Big{)},\end{split}\] (207)\[\begin{split}&\langle\bm{k}_{-}^{(t+1)},\bm{k}_{n,i}^{(t+1)}\rangle- \langle\bm{k}_{-}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\\ &=\langle\Delta\bm{k}_{-}^{(t)},\bm{k}_{n,i}^{(t)}\rangle+ \langle\bm{k}_{-}^{(t)},\Delta\bm{k}_{n,i}^{(t)}\rangle+\langle\Delta\bm{k}_{ -}^{(t)},\Delta\bm{k}_{n,i}^{(t)}\rangle\\ &=\beta_{-,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{n,i}^{(t)} \rangle+\sum_{n^{\prime}\in S_{-}}\sum_{l=2}^{M}\beta_{n^{\prime},-l}^{(t)} \langle\bm{q}_{n^{\prime},l}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\\ &+\beta_{n,i,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{-}^{(t)} \rangle+\beta_{n,i,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{-}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{\prime},l}^{(t)}\langle\bm {q}_{n^{\prime},l}^{(t)},\bm{k}_{+}^{(t)}\rangle\\ &+\Big{(}\beta_{-,-}^{(t)}\bm{q}_{-}^{(t)}+\sum_{n^{\prime}\in S _{-}}\sum_{l=2}^{M}\beta_{n^{\prime},-,l}^{(t)}\bm{q}_{n^{\prime},l}^{(t)} \Big{)}\\ &\cdot\Big{(}\beta_{n,i,+}^{(t)}\bm{q}_{+}^{(t)\top}+\beta_{n,i,- }^{(t)}\bm{q}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^ {\prime},l}^{(t)}\bm{q}_{n^{\prime},l}^{(t)\top}\Big{)},\end{split}\] (208)

\[\begin{split}&\langle\bm{k}_{+}^{(t+1)},\bm{k}_{n,i}^{(t+1)} \rangle-\langle\bm{k}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\\ &=\langle\Delta\bm{k}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle+ \langle\bm{k}_{+}^{(t)},\Delta\bm{k}_{n,i}^{(t)}\rangle+\langle\Delta\bm{k}_{ +}^{(t)},\Delta\bm{k}_{n,i}^{(t)}\rangle\\ &=\beta_{+,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{n,i}^{(t)} \rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(t)} \langle\bm{q}_{n^{\prime},l}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\\ &+\beta_{n,i,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle+\beta_{n,i,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{+}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{\prime},l}^{(t)}\langle\bm {q}_{n^{\prime},l}^{(t)},\bm{k}_{+}^{(t)}\rangle\\ &+\Big{(}\beta_{+,+}^{(t)}\bm{q}_{+}^{(t)}+\sum_{n^{\prime}\in S _{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(t)}\bm{q}_{n^{\prime},l}^{(t)} \Big{)}\\ &\cdot\Big{(}\beta_{n,i,+}^{(t)}\bm{q}_{+}^{(t)\top}+\beta_{n,i, -}^{(t)\top}-\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{\prime},l}^{(t )}\bm{q}_{n^{\prime},l}^{(t)\top}\Big{)},\end{split}\] (209)

\[\begin{split}&\langle\bm{k}_{-}^{(t+1)},\bm{k}_{n,i}^{(t+1)} \rangle-\langle\bm{k}_{-}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\\ &=\langle\Delta\bm{k}_{-}^{(t)},\bm{k}_{n,i}^{(t)}\rangle+ \langle\bm{k}_{-}^{(t)},\Delta\bm{k}_{n,i}^{(t)}\rangle+\langle\Delta\bm{k}_{ -}^{(t)},\Delta\bm{k}_{n,i}^{(t)}\rangle\\ &=\beta_{-,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{n,i}^{(t)} \rangle+\sum_{n^{\prime}\in S_{-}}\sum_{l=2}^{M}\beta_{n^{\prime},-l}^{(t)} \langle\bm{q}_{n^{\prime},l}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\\ &+\beta_{n,i,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{-}^{(t)} \rangle+\beta_{n,i,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{-}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{\prime},l}^{(t)}\langle\bm {q}_{n^{\prime},l}^{(t)},\bm{k}_{+}^{(t)}\rangle\\ &+\Big{(}\beta_{-,-}^{(t)}\bm{q}_{-}^{(t)}+\sum_{n^{\prime}\in S _{-}}\sum_{l=2}^{M}\beta_{n^{\prime},-,l}^{(t)}\bm{q}_{n^{\prime},l}^{(t)} \Big{)}\\ &\cdot\Big{(}\beta_{n,i,+}^{(t)}\bm{q}_{+}^{(t)\top}+\beta_{n,i,- }^{(t)}\bm{q}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{ \prime},l}^{(t)}\bm{q}_{n^{\prime},l}^{(t)\top}\Big{)},\end{split}\] (210)\[\langle\bm{k}_{n,i}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle-\langle\bm{k}_ {n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] \[=\langle\Delta\bm{k}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle+\langle \bm{k}_{n,i}^{(t)},\Delta\bm{k}_{n,j}^{(t)}\rangle+\langle\bm{\Delta k}_{n,i}^{( t)},\Delta\bm{k}_{n,j}^{(t)}\rangle\] \[=\beta_{n,i,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)} \rangle+\beta_{n,i,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{n,j}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{\prime},l}^{(t)}\langle\bm {q}_{n^{\prime},l}^{(t)},\bm{k}_{n,j}^{(t)}\rangle\] \[+\beta_{n,j,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{n,i}^{(t)} \rangle+\beta_{n,j,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{n,i}^{(t)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(t)}\langle\bm {q}_{n^{\prime},l}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\] (211) \[+\Big{(}\beta_{n,i,+}^{(t)}\bm{q}_{+}^{(t)}+\beta_{n,i,-}^{(t)} \bm{q}_{-}^{(t)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{\prime},l }^{(t)}\bm{q}_{n^{\prime},l}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{n,j,+}^{(t)}\bm{q}_{+}^{(t)\top}+\beta_{n,j,-}^ {(t)\top}-\bm{q}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(t)}\langle\bm{q}_{n^{\prime},l}^{(t)\top}\Big{)},\]

\[\langle\bm{k}_{n,i}^{(t+1)},\bm{k}_{\overline{n},j}^{(t+1)} \rangle-\langle\bm{k}_{n,i}^{(t)},\bm{k}_{\overline{n},j}^{(t)}\rangle\] \[=\langle\Delta\bm{k}_{n,i}^{(t)},\bm{k}_{\overline{n},j}^{(t)} \rangle+\langle\bm{k}_{n,i}^{(t)},\Delta\bm{k}_{\overline{n},j}^{(t)}\rangle+ \langle\Delta\bm{k}_{n,i}^{(t)},\Delta\bm{k}_{\overline{n},j}^{(t)}\rangle\] \[=\beta_{n,i,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{\overline{n}, j}^{(t)}\rangle+\beta_{n,i,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{\overline{n},j}^{(t)} \rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{\prime},l}^{(t)} \langle\bm{q}_{n^{\prime},l}^{(t)},\bm{k}_{\overline{n},j}^{(t)}\rangle\] \[+\beta_{\overline{n},j,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{n, i}^{(t)}\rangle+\beta_{\overline{n},j,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{n,i}^{(t)} \rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{\overline{n},j,n^{\prime},l}^{(t)}\langle\bm{q}_{n^{\prime},l}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\] (212) \[+\Big{(}\beta_{n,i,+}^{(t)}\bm{q}_{+}^{(t)\top}+\beta_{n,i,-}^{(t )}\bm{q}_{-}^{(t)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{\prime},l }^{(t)}\bm{q}_{n^{\prime},l}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{\overline{n},j,+}^{(t)}\bm{q}_{+}^{(t)\top}+ \beta_{\overline{n},j,-}^{(t)}\bm{q}_{-}^{(t)\top}+\sum_{n^{\prime}=1}^{N} \sum_{l=2}^{M}\beta_{\overline{n},j,n^{\prime},l}^{(t)}\bm{q}_{n^{\prime},l}^{( t)\top}\Big{)}\]

for \(n\neq\overline{n}\),

\[\langle\bm{q}_{+}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle-\langle\bm{q }_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle\] \[=\langle\Delta\bm{q}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle+\langle \bm{q}_{+}^{(t)},\Delta\bm{k}_{-}^{(t)}\rangle+\langle\Delta\bm{q}_{+}^{(t)}, \Delta\bm{k}_{-}^{(t)}\rangle\] \[=\alpha_{+,+}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{-}^{(t)} \rangle+\sum_{n\in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\langle\bm{k}_{n,i }^{(t)},\bm{k}_{-}^{(t)}\rangle\] \[+\beta_{-,-}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{-}^{(t)} \rangle+\sum_{n\in S_{-}}\sum_{i=2}^{M}\beta_{n,-,i}^{(t)}\langle\bm{q}_{n,i}^{( t)},\bm{q}_{+}^{(t)}\rangle\] (213) \[+\Big{(}\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n\in S_{+}}\sum_ {i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{-,-}^{(t)}\bm{q}_{-}^{(t)\top}+\sum_{n\in S_{- }}\sum_{i=2}^{M}\beta_{n,-,i}^{(t)}\bm{q}_{n,i}^{(t)\top}\Big{)},\]\[\langle\bm{k}_{+}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle-\langle\bm{k}_{+}^{(t )},\bm{k}_{-}^{(t)}\rangle\] \[=\langle\Delta\bm{k}_{+}^{(t)},\bm{k}_{-}^{(t)}\rangle+\langle\bm{ k}_{+}^{(t)},\Delta\bm{k}_{-}^{(t)}\rangle+\langle\Delta\bm{k}_{+}^{(t)},\Delta\bm{k}_{- }^{(t)}\rangle\] \[=\beta_{+,+}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{k}_{-}^{(t)} \rangle+\sum_{n\in S_{+}}\sum_{i=2}^{M}\beta_{n,+,i}^{(t)}\langle\bm{q}_{n,i} ^{(t)},\bm{k}_{-}^{(t)}\rangle\] \[+\beta_{-,-}^{(t)}\langle\bm{q}_{-}^{(t)},\bm{k}_{+}^{(t)} \rangle+\sum_{n\in S_{-}}\sum_{i=2}^{M}\beta_{n,-,i}^{(t)}\langle\bm{q}_{n,i} ^{(t)},\bm{k}_{+}^{(t)}\rangle\] (214) \[+\Big{(}\beta_{+,+}^{(t)}\bm{q}_{+}^{(t)}+\sum_{n\in S_{+}}\sum_ {i=2}^{M}\beta_{n,+,i}^{(t)}\bm{q}_{n,i}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{-,-}^{(t)}\bm{q}_{-}^{(t)\top}+\sum_{n\in S_{- }}\sum_{i=2}^{M}\beta_{n,-,i}^{(t)\top}\Big{)}.\]

### Proof of Lemma D.4

Let \(T_{0}=O(\frac{1}{\eta d_{h}^{\frac{1}{4}}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{0}\|_{2 }^{2}})\). By Lemma D.3, we have \(|V_{+}^{(t)}|,|V_{-}^{(t)}|,|V_{n,i}^{(t)}|=O(d_{h}^{-\frac{1}{4}})\) for \(t\in[0,T_{0}]\) by Lemma D.3. Plugging this into the expression for \(\alpha\) and \(\beta\) gives

\[|\alpha_{+,+}^{(t)}|=\Big{|}\frac{\eta}{NM}\sum_{n\in S_{+}}-\ell _{n}^{\prime(t)}\|\bm{\mu}\|_{2}^{2}\] \[\cdot\Big{(}V_{+}^{(t)}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t)}, \bm{k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle) +\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)}\] \[-(\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)}{ \exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)})^{2}\big{)}\] \[-\sum_{i=2}^{M}\big{(}V_{n,i}^{(t)}\cdot\frac{\exp(\langle\bm{q}_ {+}^{(t)},\bm{k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{( t)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)} \rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle) }{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)}\big{)}\Big{|}\] \[\leq\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\cdot\frac{3NM}{4}\cdot O(d_ {h}^{-\frac{1}{4}})\] \[=O(\frac{\eta\|\bm{\mu}\|_{2}^{2}}{d_{h}^{\frac{1}{4}}}),\]

where the inequality is by \(-\ell_{n}^{\prime(t)}\leq 1\) and the property that attention is smaller than 1 ( e.g. \(\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q} _{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_ {+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)}\leq 1\) ). We also have

\[|\alpha_{n,+,i}^{(t)}|=\Big{|}-\frac{\eta}{NM}\ell_{n}^{\prime(t)}\|\bm{\mu}\|_ {2}^{2}\]\[\cdot\Big{(}-V_{+}^{(t)}\cdot\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)} \rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j=2} ^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,i}^{(t)} \rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j =2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)}\] \[+V_{n,i}^{(t)}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{ n,i}^{(t)}\rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+ \sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)}\] \[-(\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle)} {\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)})^{2}\big{)}\] \[-\sum\limits_{k\neq i}\big{(}V_{n,k}^{(t)}\cdot\frac{\exp(\langle \bm{q}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle)}{\exp(\langle\bm{q}_{+}^{(t)},\bm{ k}_{+}^{(t)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{+}^{(t)},\bm{ k}_{n,j}^{(t)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,k}^{(t)}\rangle)} {\exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{+}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)})\Big{|}\] \[\leq\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\cdot M\cdot O(d_{h}^{- \frac{1}{4}})\] \[=O(\frac{\eta\|\bm{\mu}\|_{2}^{2}}{d_{h}^{\frac{1}{4}}N}),\]

where the inequality is by \(-\ell_{n}^{\prime(t)}\leq 1\) and the property that attention is smaller than 1. We also have

\[|\alpha_{n^{\prime},i^{\prime},+}^{(t)}|=\Big{|}\frac{\eta}{NM} \sum\limits_{n\in S_{+}}-\ell_{n}^{\prime(t)}\sum\limits_{i=2}^{M}\langle\bm{ \xi}_{n^{\prime},i^{\prime}},\bm{\xi}_{n,i}\rangle\] \[\cdot\Big{(}V_{+}^{(t)}\big{(}\frac{\exp(\langle\bm{q}_{n,i}^{(t) },\bm{k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t) }\rangle)}\] \[-(\frac{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle)} {\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle))+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)})^{2}\Big{)}\] \[-\sum\limits_{k=2}^{M}\big{(}V_{n,i}^{(t)}\cdot\frac{\exp(\langle \bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(t)}, \bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(t)}, \bm{k}_{n,j}^{(t)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,k}^{(t)} \rangle)}{\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{+}^{(t)}\rangle)+\sum\limits_{j =2}^{M}\exp(\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n,j}^{(t)}\rangle)}\Big{)} \Big{|}\] \[\leq\frac{\eta}{NM}\Big{(}\frac{3\tilde{\sigma}_{p}^{2}d}{2}+ \frac{3N}{4}\cdot\tilde{\sigma}_{p}^{2}\cdot\sqrt{d\log(4N^{2}M^{2}/\delta)} \Big{)}\cdot M\cdot O(d_{h}^{-\frac{1}{4}})\]\[=O(\frac{\eta\sigma_{p}^{2}d}{d_{h}^{\frac{1}{4}}N}),\]

where the inequality is by \(-\ell_{n}^{\prime(t)}\leq 1\), Lemma C.4 and the property that attention is smaller than 1.

Similarly, we have

\[|\alpha_{-,-}^{(t)}|,|\beta_{+,+}^{(t)}|,|\beta_{-,-}^{(t)}|=O\Big{(}\frac{\eta \|\bm{\mu}\|_{2}^{2}}{d_{h}^{\frac{1}{4}}}\Big{)},\]

\[|\alpha_{n,-,l}^{(t)}|,|\beta_{n,+,l}^{(t)}|,|\beta_{n,-,l}^{(t)}|=O\Big{(} \frac{\eta\|\bm{\mu}\|_{2}^{2}}{d_{h}^{\frac{1}{4}}N}\Big{)},\]

\[|\alpha_{n,l,-}^{(t)}|,|\beta_{n,l,+}^{(t)}|,|\beta_{n,l,-}^{(t)}|,|\alpha_{n, l,n^{\prime},l^{\prime}}^{(t)}|,|\beta_{n,l,n^{\prime},l^{\prime}}^{(t)}|=O \Big{(}\frac{\eta\sigma_{p}^{2}d}{d_{h}^{\frac{1}{4}}N}\Big{)}\]

for \(t\in[0,T_{0}]\).

Next we use induction to show that the following proposition \(\mathcal{A}(t)\) holds for \(t\in[0,T_{0}]\)\(\mathcal{A}(t)\) :

\[|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{ q}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{q}_{\pm}^{(t)},\bm{k}_{n,j}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{k}_{n^{\prime},j}^{(t)}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_ {h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[|\langle\bm{q}_{\pm}^{(t)},\bm{q}_{\mp}^{(t)}\rangle|,|\langle\bm {q}_{n,i}^{(t)},\bm{q}_{\pm}^{(t)}\rangle|,|\langle\bm{q}_{n,i}^{(t)},\bm{q}_{ n^{\prime},j}^{(t)}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_ {h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[|\langle\bm{k}_{\pm}^{(t)},\bm{k}_{\mp}^{(t)}\rangle|,|\langle\bm {k}_{n,i}^{(t)},\bm{k}_{\pm}^{(t)}\rangle|,|\langle\bm{k}_{n,i}^{(t)},\bm{k}_ {n^{\prime},j}^{(t)}\rangle|\] \[=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_ {h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)},\]

\[\|\bm{q}_{\pm}^{(t)}\|_{2}^{2},\|\bm{k}_{\pm}^{(t)}\|_{2}^{2}=\Theta(\|\bm{\mu} \|_{2}^{2}\sigma_{h}^{2}d_{h})\]

\[\|\bm{q}_{n,i}^{(t)}\|_{2}^{2},\|\bm{k}_{n,i}^{(t)}\|_{2}^{2}=\Theta(\sigma_ {p}^{2}\sigma_{h}^{2}dd_{h})\]

for \(i,j\in[M]\backslash\{1\}\), \(n,n^{\prime}\in[N]\).

By Lemma C.3 we know that \(\mathcal{A}(0)\) is true. Now we assume \(\mathcal{A}(0),\ldots,\mathcal{A}(T)\) is true, then we need to proof that \(\mathcal{A}(T+1)\) is true. We first proof \(|\langle\bm{q}_{+}^{(T+1)},\bm{k}_{+}^{(T+1)}\rangle|=O\Big{(}\max\{\|\bm{\mu} \|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_{h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2 }/\delta)}\Big{)}\), as an example.

\[\begin{split}&\big{|}\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)} \rangle-\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\big{|}\\ &=\Big{|}\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\sum_{n \in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\\ &+\beta_{+,+}^{(t)}\|\bm{q}_{+}^{(t)}\|_{2}^{2}+\sum_{n\in S_{+} }\sum_{i=2}^{M}\beta_{n,+,i}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)} \rangle\\ &+\Big{(}\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}+\sum_{n\in S_{+}} \sum_{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\\ &\cdot\Big{(}\beta_{+,+}^{(t)}\bm{q}_{+}^{(t)\top}+\sum_{n\in S_{+ }}\sum_{i=2}^{M}\beta_{n,+,i}^{(t)\top}\bm{q}_{n,i}^{(t)\top}\Big{)}\big{|}\\ &\leq O\Big{(}\frac{\eta\|\bm{\mu}\|_{2}^{2}}{d_{h}^{\frac{1}{4} }}\Big{)}\cdot\Theta(\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h})\\ &+NM\cdot O\Big{(}\frac{\eta\|\bm{\mu}\|_{2}^{2}}{d_{h}^{\frac{1} {4}}N}\Big{)}\cdot O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot \sigma_{h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)}\\ &+\{lower\ order\ term\}\\ &=O\Big{(}\eta\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_{h}^{\frac{3}{ 4}}\Big{)}\end{split}\] (215)

Taking a summation, we obtain that

\[\begin{split}&|\langle\bm{q}_{+}^{(T+1)},\bm{k}_{+}^{(T+1)} \rangle|\leq|\langle\bm{q}_{+}^{(0)},\bm{k}_{+}^{(0)}\rangle|+\sum_{t=0}^{T} \big{|}\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle\bm{q}_{+}^{ (t)},\bm{k}_{+}^{(t)}\rangle\big{|}\\ &\leq|\langle\bm{q}_{+}^{(0)},\bm{k}_{+}^{(0)}\rangle|+\sum_{t=0} ^{T_{0}-1}\big{|}\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle \bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\big{|}\\ &\leq O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot \sigma_{h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)}+O\Big{(}\frac{1 }{\eta d_{h}^{\frac{1}{4}}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}}\Big{)} \cdot O\Big{(}\eta\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_{h}^{\frac{3}{4}}\Big{)} \\ &=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_{ h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)}+O\Big{(}\|\bm{\mu}\|_{2}^{2} \sigma_{h}^{2}d_{h}^{\frac{3}{2}}\Big{)}\\ &=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_ {h}^{2}\cdot\sqrt{d_{h}\log(6N^{2}M^{2}/\delta)}\Big{)}\end{split}\] (216)

Similarly to \(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\), it is easy to know that the inner product does not change by a magnitude more than the product of \(\max\{\alpha,\beta\}\) and \(\max\{\langle\bm{q},\bm{q}\rangle,\langle\bm{k},\bm{k}\rangle\}\) in a single iteration, which can be expressed as follows

\[\begin{split}&\big{|}\langle\bm{q}^{(t+1)},\bm{k}^{(t+1)}\rangle- \langle\bm{q}^{(t)},\bm{k}^{(t)}\rangle\big{|}\\ &=O\Big{(}\max\{\frac{\eta\|\bm{\mu}\|_{2}^{2}}{d_{h}^{\frac{1}{4} }},\frac{\eta\sigma_{p}^{2}d}{d_{h}^{\frac{1}{4}}N}\}\Big{)}\cdot\Theta(\max\{ \|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h},\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}\}) \\ &=O\Big{(}\eta\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}^{\frac{3}{4}} \cdot\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\Big{)}\end{split}\] (217)where the second equality is by the condition that \(N\cdot\mathrm{SNR}^{2}=\Omega(1)\).

Taking a summation, we obtain that

\[\begin{split}&\big{|}\langle\bm{q}^{(T+1)},\bm{k}^{(T+1)}\rangle- \langle\bm{q}^{(0)},\bm{k}^{(0)}\rangle\big{|}\leq\sum_{t=0}^{T-1}\big{|} \langle\bm{q}^{(t+1)},\bm{k}^{(t+1)}\rangle-\langle\bm{q}^{(t)},\bm{k}^{(t)} \rangle\big{|}\\ &\leq\sum_{t=0}^{T_{0}-1}O\Big{(}\eta\|\bm{\mu}\|_{2}^{2}\sigma_{ h}^{2}d_{h}^{\frac{3}{4}}\cdot\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\} \Big{)}\\ &=O\Big{(}\frac{1}{\eta d_{h}^{\frac{1}{4}}\|\bm{\mu}\|_{2}^{2}\| \bm{w}_{O}\|_{2}^{2}}\Big{)}\cdot O\Big{(}\eta\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{ 2}d_{h}^{\frac{3}{4}}\cdot\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\Big{)} \\ &=O\Big{(}\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma _{h}^{2}d_{h}^{\frac{1}{4}}\Big{)}.\end{split}\] (218)

It is clear that the magnitude of \(\langle\bm{q}^{(T+1)},\bm{k}^{(T+1)}\rangle-\langle\bm{q}^{(0)},\bm{k}^{(0)}\rangle\) is smaller than \(\max\{\|\bm{\mu}\|_{2}^{2},\sigma_{p}^{2}d\}\cdot\sigma_{h}^{2}\cdot\sqrt{d_ {h}\log(6N^{2}M^{2}/\delta)}\), Thus the magnitude of the bound for \(\langle\bm{q}^{(T+1)},\bm{k}^{(T+1)}\rangle\) is the same as that of \(\langle\bm{q}^{(T)},\bm{k}^{(T)}\rangle\). The proof for \(\langle\bm{q}^{(T+1)},\bm{q}^{(T+1)}\rangle\) and \(\langle\bm{k}^{(T+1)},\bm{k}^{(T+1)}\rangle\) is exactly the same, and we can conclude the proof by an induction.

### Lower Bounds of \(\alpha\) and \(\beta\)

In this subsection, we present some bounds for \(\alpha\) and \(\beta\) which can be used in D.2 and D.3. All the calculations in this subsection are based on the precise expression for \(\alpha\) and \(\beta\) in F.1 and assume that \(\mathcal{B}(T_{1}),\ldots,\mathcal{B}(s),\mathcal{D}(T_{1}),\ldots,\mathcal{ D}(s-1)\) hold (\(s\in[T_{1},t]\)). Then the following propositions hold:

\[V_{+}^{(s)}\geq 3M\cdot|V_{n,i}^{(s)}|,\]

\[V_{-}^{(s)}\leq-3M\cdot|V_{n,i}^{(s)}|,\]

\[softmax(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle), softmax(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{\pm}^{(s)}\rangle)\geq\frac{1}{M}-o(1),\]

\[softmax(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle), softmax(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{\pm}^{(s)}\rangle)\geq\frac{1}{M}+o(1).\]

Now we give the bounds respectively for \(\alpha_{+,+}^{(s)},\alpha_{n,+,i}^{(s)},\alpha_{-,-}^{(s)},\alpha_{n,-,i}^{( s)},\alpha_{n,i,+}^{(s)},\alpha_{n,i,-}^{(s)},\alpha_{n,i,n^{\prime},i^{\prime}}^{(s)}, \beta_{+,+}^{(s)},\)\(\beta_{n,+,i}^{(s)},\beta_{-,-}^{(s)},\beta_{n,i,+}^{(s)},\beta_{n,i,-}^{(s)}, \beta_{n,i,n^{\prime},i^{\prime}}^{(s)}\).

\[\alpha_{+,+}^{(s)}=\frac{\eta}{NM}\sum\limits_{n\in S_{+}}-\ell_{n}^{ \prime(s)}\|\boldsymbol{\mu}\|_{2}^{2}\cdot\frac{\exp(\langle\boldsymbol{q}_{+}^ {(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{ q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\] \[\cdot\Big{(}V_{+}^{(s)}\big{(}1-\frac{\exp(\langle\boldsymbol{q}_ {+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s )},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\big{)}\] \[-\sum\limits_{i=2}^{M}\big{(}V_{n,i}^{(s)}\cdot\frac{\exp( \langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,i}^{(s)}\rangle)}{\exp( \langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits _{j=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)} \rangle)}\big{)}\] \[\geq\frac{\eta}{NM}\sum\limits_{n\in S_{+}}-\ell_{n}^{\prime(s) }\|\boldsymbol{\mu}\|_{2}^{2}\cdot\frac{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{ q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\] (219) \[-\frac{1}{2}\cdot V_{+}^{(s)}\sum\limits_{i=2}^{M}\cdot\frac{ \exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,i}^{(s)}\rangle)}{\exp( \langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits _{j=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)} \rangle)}\Big{)}\] \[\geq\frac{\eta}{2NM}\sum\limits_{n\in S_{+}}-\ell_{n}^{\prime(s) }\|\boldsymbol{\mu}\|_{2}^{2}V_{+}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q} _{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{( s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\] \[\cdot\big{(}1-\frac{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\big{)}\]

where the first inequality is by \(V_{+}^{(s)}\geq 3M\cdot|V_{n,i}^{(s)}|\), the second inequality is by the fact that the sum of attention equal to 1. Similarly, we have

\[\beta_{+,+}^{(s)}\geq\] (220) \[\frac{\eta}{2NM}\sum\limits_{n\in S_{+}}-\ell_{n}^{\prime(s)}\| \boldsymbol{\mu}\|_{2}^{2}V_{+}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{+ }^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+(M-1)\exp(\max\limits_{j}\{\langle\boldsymbol {q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle\})}\] \[\cdot\frac{\exp(\max\limits_{j}\{\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle\})}{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+(M-1)\exp(\max\limits_{j}\{\langle\boldsymbol {q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle\})},\]

\[\alpha_{-,-}^{(s)}\geq\] (221) \[\frac{\eta}{2NM}\sum\limits_{n\in S_{-}}\ell_{n}^{\prime(s)}\| \boldsymbol{\mu}\|_{2}^{2}V_{-}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{-}^ {(s)},\boldsymbol{k}_{-}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{-}^{(s)}, \boldsymbol{k}_{-}^{(s)}\rangle)+(M-1)\exp(\max\limits_{j}\{\langle\boldsymbol {q}_{-}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle\})}\] \[\cdot\frac{\exp(\max\limits_{j}\{\langle\boldsymbol{q}_{-}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle\})}{\exp(\langle\boldsymbol{q}_{-}^{(s)}, \boldsymbol{k}_{-}^{(s)}\rangle)+(M-1)\exp(\max\limits_{j}\{\langle\boldsymbol {q}_{-}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle\})},\]\[\begin{split}&\partial_{-,-}^{(s)}\geq\\ &\frac{\eta}{2NM}\sum_{n\in S_{-}}\ell_{n}^{\prime(s)}\|\boldsymbol{ \mu}\|_{2}^{2}V_{-}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{-}^{(s)}, \boldsymbol{k}_{-}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{-}^{(s)}, \boldsymbol{k}_{-}^{(s)}\rangle)+(M-1)\exp(\max\limits_{j}\langle\langle \boldsymbol{q}_{-}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle\})\\ &\cdot\frac{\exp(\max\limits_{j}\{\langle\boldsymbol{q}_{-}^{(s) },\boldsymbol{k}_{n,j}^{(s)}\rangle\})}{\exp(\langle\boldsymbol{q}_{-}^{(s)}, \boldsymbol{k}_{-}^{(s)}\rangle)+(M-1)\exp(\max\limits_{j}\{\langle \boldsymbol{q}_{-}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle\})}.\end{split}\] (222)

\[\begin{split}&\alpha_{n,+,i}^{(s)}=-\frac{\eta}{NM}\ell_{n}^{ \prime(s)}\|\boldsymbol{\mu}\|_{2}^{2}\cdot\frac{\exp(\langle\boldsymbol{q}_{+ }^{(s)},\boldsymbol{k}_{n,i}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{( s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\\ &\cdot\Big{(}-V_{+}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{ +}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{( s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\\ &+V_{n,i}^{(s)}\big{(}1-\frac{\exp(\langle\boldsymbol{q}_{+}^{(s )},\boldsymbol{k}_{n,i}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\big{)}\\ &-\sum_{k\neq i}\big{(}V_{n,k}^{(s)}\cdot\frac{\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,k}^{(s)}\rangle)}{\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{ M}\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}) \Big{)}\\ &\leq-\frac{\eta}{NM}\ell_{n}^{\prime(s)}\|\boldsymbol{\mu}\|_{2}^ {2}\cdot\frac{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,i}^{(s)} \rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{n,j}^{(s)}\rangle)}\\ &+|V_{n,i}^{(s)}|+\sum_{k\neq i}\big{(}|V_{n,k}^{(s)}|\cdot(\frac {1}{M}+o(1))\big{)}\Big{)}\\ &\leq-\frac{\eta}{NM}\ell_{n}^{\prime(s)}\|\boldsymbol{\mu}\|_{2}^ {2}\cdot\frac{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,i}^{(s)} \rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{n,j}^{(s)}\rangle)}\\ &\cdot\Big{(}-V_{+}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{ +}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}+2M\max\limits_{ l}|V_{n,l}^{(s)}|\Big{)}\\ &\leq-\frac{\eta}{NM}\ell_{n}^{\prime(s)}\|\boldsymbol{\mu}\|_{2}^ {2}\cdot\frac{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,i}^{(s)} \rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{n,j}^{(s)}\rangle)}\\ &\cdot\Big{(}-V_{+}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{+ }^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\\ &\cdot\Big{(}-V_{+}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{+ }^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\\ &\cdot\Big{(}-V_{+}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{+ }^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{+ }^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\\ &\cdot\Big{(}-V_{+}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{+ }^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{+ }^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\\ &\cdot\Big{(}-V_{+}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{+ }^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\\ &\cdot\Big{(}-V_{+}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{+ }^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^\[+\frac{3}{4}V_{+}^{(s)}\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)} \rangle)}{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2} ^{M}\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}\] \[=\frac{\eta}{4NM}\ell_{n}^{\prime(s)}\|\bm{\mu}\|_{2}^{2}V_{+}^{(s )}\cdot\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,i}^{(s)}\rangle)}{\exp( \langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp( \langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)} {\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)},\]

where the third inequality is by \(V_{+}^{(s)}\geq 3M\cdot|V_{n,i}^{(s)}|\) and \(softmax(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)\geq\frac{1}{M}-o(1)\). Similarly, we have

\[\alpha_{n,-,i}^{(s)}\leq\] \[-\frac{\eta}{4NM}\ell_{n}^{\prime(s)}\|\bm{\mu}\|_{2}^{2}V_{-}^{(s )}\cdot\frac{\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{n,i}^{(s)}\rangle)}{\exp( \langle\bm{q}_{-}^{(s)},\bm{k}_{-}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp( \langle\bm{q}_{-}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{-}^{(s)}\rangle)} {\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{-}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}.\]

\[\beta_{n,+,i}^{(s)}=-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\ell_{n}^ {\prime(s)}\] \[\cdot\Big{(}V_{+}^{(s)}\big{(}\frac{\exp(\langle\bm{q}_{n,i}^{(s) },\bm{k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s) }\rangle)}\] \[-(\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)} {\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)})^{2}\Big{)}\] \[-\sum\limits_{k=2}^{M}\big{(}V_{n,i}^{(s)}\cdot\frac{\exp(\langle \bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{n,j}^{(s)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,k}^{(s)}\rangle) }{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}\Big{)}\] \[=-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\ell_{n}^{\prime(s)}\frac{\exp (\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{( s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{n,j}^{(s)}\rangle)}\] \[\cdot\Big{(}V_{+}^{(s)}\big{(}1-\frac{\exp(\langle\bm{q}_{n,i}^{( s)},\bm{k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s) }\rangle)}\big{)}\] \[-\sum\limits_{k=2}^{M}\big{(}V_{n,i}^{(s)}\cdot\frac{\exp(\langle \bm{q}_{n,i}^{(s)},\bm{k}_{n,k}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{n,j}^{(s)}\rangle)}\big{)}\Big{)}\]\[\geq-\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{NM}\ell_{n}^{\prime(s)} \frac{\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{ \exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{n,j}^{( s)}\rangle)}\] \[\cdot\Big{(}V_{+}^{(s)}\big{(}1-\frac{\exp(\langle\boldsymbol{q} _{n,i}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{n,i }^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\big{)}\] \[-\frac{1}{2}V_{+}^{(s)}\sum\limits_{k=2}^{M}.\frac{\exp(\langle \boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{n,k}^{(s)}\rangle)}{\exp(\langle \boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2} ^{M}\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\] \[\geq-\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{2NM}\ell_{n}^{\prime (s)}V_{+}^{(s)}\frac{\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{+}^ {(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{+}^{(s) }\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{n,i}^{(s)}, \boldsymbol{k}_{n,j}^{(s)}\rangle)}\] \[\cdot\big{(}1-\frac{\exp(\langle\boldsymbol{q}_{n,i}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{n,i}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\big{)},\]

where the first inequality is by \(V_{+}^{(s)}\geq 3M\cdot|V_{n,i}^{(s)}|\), the second inequality is by \(\sum\limits_{k=2}^{M}softmax(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k} _{n,k}^{(s)}\rangle)=\big{(}1-softmax(\langle\boldsymbol{q}_{n,i}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)\big{)}\). Similarly, we have

\[\beta_{n,-,i}^{(s)}\geq\] \[\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{2NM}\ell_{n}^{\prime(s)}V _{-}^{(s)}\frac{\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{-}^{(s) }\rangle)}{\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{-}^{(s)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{n,i}^{(s)}, \boldsymbol{k}_{n,j}^{(s)}\rangle)}\] \[\cdot\big{(}1-\frac{\exp(\langle\boldsymbol{q}_{n,i}^{(s)}, \boldsymbol{k}_{-}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{n,i}^{(s)}, \boldsymbol{k}_{-}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\big{)}.\]

\[\alpha_{n^{\prime},i^{\prime},+}^{(s)}=\frac{\eta\|\boldsymbol{ \xi}_{n^{\prime},i^{\prime}}\|_{2}^{2}}{NM}-\ell_{n^{\prime}}^{\prime}(\theta (s))\] \[\cdot\Big{(}V_{+}^{(s)}\big{(}\frac{\exp(\langle\boldsymbol{q}_{n ^{\prime},i^{\prime}},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle \boldsymbol{q}_{n^{\prime},i^{\prime}}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+ \sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{n^{\prime},i^{\prime}}^{(s)}, \boldsymbol{k}_{n^{\prime},j}^{(s)}\rangle)}\] \[-(\frac{\exp(\langle\boldsymbol{q}_{n^{\prime},i^{\prime}}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{n^{\prime},i^{ \prime}}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp( \langle\boldsymbol{q}_{n^{\prime},i^{\prime}}^{(s)},\boldsymbol{k}_{n^{\prime}, j}^{(s)}\rangle)})^{2}\Big{)}\] \[-\sum\limits_{k=2}^{M}\big{(}V_{n^{\prime},i^{\prime}}^{(s)}\cdot \frac{\exp(\langle\boldsymbol{q}_{n^{\prime},i^{\prime}}^{(s)},\boldsymbol{k}_{+}^ {(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{n^{\prime},i^{\prime}}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{n^{\prime},i^{\prime}}^{(s)},\boldsymbol{k}_{n^{\prime},j}^{(s) }\rangle)}\] \[\cdot\frac{\exp(\langle\boldsymbol{q}_{n^{\prime},i^{\prime}}^{(s)}, \boldsymbol{k}_{n^{\prime},k}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{n^{ \prime},i^{\prime}}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\boldsymbol{q}_{n^{\prime},i^{\prime}}^{(s)},\boldsymbol{k}_{n^{ \prime},j}^{(s)}\rangle)}\big{)}\Big{)}\] \[+\{lower\,order\,term\}\]\[\cdot\Big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{\prime}}, \bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{\prime}},\bm{ k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{n^{\prime},j}\rangle)}\] \[\cdot\Big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)})\] \[\cdot\Big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)})\] \[\cdot\Big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)})\] \[\cdot\Big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)})\] \[\cdot\Big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s) }_{n^{\prime},i^{\prime}},\bm{k}^{(s)}_{n^{\prime},j}\rangle)}\] \[\cdot\Big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s) }_{n^{\prime},i^{\prime}},\bm{k}^{(s)}_{n^{\prime},j}\rangle)}\] \[\cdot\Big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s) }_{n^{\prime},i^{\prime}},\bm{k}^{(s)}_{n^{\prime},j}\rangle)}\] \[\cdot\Big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s) }_{n^{\prime},i^{\prime}},\bm{k}^{(s)}_{n^{\prime},j}\rangle)}\] \[\cdot\Big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s) }_{n^{\prime},i^{\prime}},\bm{k}^{(s)}_{n^{\prime},j}\rangle)}\] \[+\{lower\ order\ term\}\] \[\geq\frac{\eta\sigma_{p}^{2}d}{5NM}-\ell_{n^{\prime}}^{\prime}( \theta(s))V_{+}^{(s)}\frac{\exp((\langle\bm{q}^{(s)}_{n^{\prime},i^{\prime}}, \bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{\prime}}, \bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{n^{ \prime},i^{\prime}},\bm{k}^{(s)}_{n^{\prime},j}\rangle)}\] \[\cdot\Big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{ \prime}},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s) }_{n^{\prime},i^{\prime}},\bm{k}^{(s)}_{n^{\prime},j}\rangle)}\Big{)},\]

where the \(\{lower\ order\ term\}\) is by the property that \(\|\bm{\xi}_{n^{\prime},i^{\prime}}\|_{2}^{2}\) is much larger than \(\langle\bm{\xi}_{n^{\prime},i^{\prime}},\bm{\xi}_{n,i}\rangle\) in Lemma C.4 and the condition \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\), the first inequality is by \(V_{+}^{(s)}\geq 3M\cdot|V_{n,i}^{(s)}|\), the last inequality is by \(\|\bm{\xi}_{n^{\prime},i^{\prime}}\|_{2}^{2}\geq\frac{\sigma_{p}^{2}d}{2}\) and we absorb the \(\{lower\ order\ term\}\). Similarly, we have

\[\alpha_{n^{\prime},i^{\prime},-}^{(s)}\geq\] \[\frac{\eta\sigma_{p}^{2}d}{5NM}\ell_{n^{\prime}}^{\prime}(\theta( s))V_{-}^{(s)}\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{\prime}},\bm{k}^{(s)}_{-} \rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{\prime}},\bm{k}^{(s)}_{-} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{\prime}}, \bm{k}^{(s)}_{n^{\prime},j}\rangle)}\]\[\cdot\big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{\prime}},\bm{k}^{(s)}_{ -}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{\prime}},\bm{k}^{(s)}_{-} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{n^{\prime},i^{\prime}}, \bm{k}^{(s)}_{n^{\prime},j}\rangle)}\big{)}.\]

\[\beta^{(s)}_{n^{\prime},i^{\prime},+}=\frac{\eta\|\bm{\xi}_{n^{ \prime},i^{\prime}}\|_{2}^{2}}{NM}-\ell^{\prime}_{n^{\prime}}(\theta(s))\] \[\Big{(}-V^{(s)}_{+}\big{(}\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{ k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+ \sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n^{\prime},j} \rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n^{\prime },i^{\prime}}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+ \sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n^{\prime},j} \rangle)}\big{)}\] \[+V^{(s)}_{n^{\prime},i^{\prime}}\big{(}\frac{\exp(\langle\bm{q}^ {(s)}_{+},\bm{k}^{(s)}_{n^{\prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}^{( s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{n^{\prime},j}\rangle)}\] \[\cdot\big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n^ {\prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n^{ \prime},j}\rangle)}\big{)}\big{)}\Big{)}\] \[+\{lower\ order\ term\}\] \[+V^{(s)}_{n^{\prime},i^{\prime}}\cdot\big{(}1-\frac{\exp(\langle \bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n^{\prime},i^{\prime}}\rangle)}{\exp(\langle\bm {q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q} ^{(s)}_{+},\bm{k}^{(s)}_{n^{\prime},j}\rangle)}\big{)}\Big{)}\] \[+\{lower\ order\ term\}\] \[\leq\frac{\eta\|\bm{\xi}_{n^{\prime},i^{\prime}}\|_{2}^{2}}{NM}- \ell^{\prime}_{n^{\prime}}(\theta(s))\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{n^{\prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{n^{\prime},j}\rangle)}\] \[\Big{(}-V^{(s)}_{+}\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{ k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+ \sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n^{\prime},j} \rangle)}\] \[+\frac{1}{2}V^{(s)}_{+}\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+ \sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n^{\prime},j} \rangle)}\Big{)}\] \[\leq\frac{\eta\|\bm{\xi}_{n^{\prime},i^{\prime}}\|_{2}^{2}}{2NM} \ell^{\prime}_{n^{\prime}}(\theta(s))V^{(s)}_{+}\cdot\frac{\exp(\langle\bm{q}^ {(s)}_{+},\bm{k}^{(s)}_{n^{\prime},i^{\prime}}\rangle)}{\exp(\langle\bm{q}^{( s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{n^{\prime},j}\rangle)}\]\[\cdot\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)}{\exp( \langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp( \langle\bm{q}_{+}^{(s)},\bm{k}_{n^{\prime},j}^{(s)}\rangle)}\] \[+\{lower\ order\ term\}\] \[\leq\frac{\eta\sigma_{p}^{2}d}{5NM}\ell_{n^{\prime}}^{\prime}( \theta(s))V_{+}^{(s)}\cdot\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n^{ \prime},i^{\prime}}^{(s)}\rangle)}{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{( s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n^{ \prime},j}^{(s)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle) }{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n^{\prime},j}^{(s)}\rangle)},\]

where the \(\{lower\ order\ term\}\) is by the property that \(\|\bm{\xi}_{n^{\prime},i^{\prime}}\|_{2}^{2}\) is much larger than \(\langle\bm{\xi}_{n^{\prime},i^{\prime}},\bm{\xi}_{n,i}\rangle\) in Lemma C.4 and the condition \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\), the first inequality is by \(V_{+}^{(s)}\geq 3M\cdot|V_{n,i}^{(s)}|\), the last inequality is by \(\|\bm{\xi}_{n^{\prime},i^{\prime}}\|_{2}^{2}\geq\frac{\sigma_{p}^{2}d}{2}\) and we absorb the \(\{lower\ order\ term\}\). Similarly, we have

\[\beta_{n^{\prime},i^{\prime},-}^{(s)}\leq\] \[-\frac{\eta\sigma_{p}^{2}d}{5NM}\ell_{n^{\prime}}^{\prime}(\theta (s))V_{-}^{(s)}\cdot\frac{\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{n^{\prime},i^{ \prime}}^{(s)}\rangle)}{\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{-}^{(s)}\rangle)+ \sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{n^{\prime},j}^{(s)} \rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{-}^{(s)}\rangle) }{\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{-}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{n^{\prime},j}^{(s)}\rangle)}.\]

\[\alpha_{n,i,n,j}^{(s)}=\frac{\eta}{NM}-\ell_{n}^{\prime(s)}\sum \limits_{k=2}^{M}\langle\bm{\xi}_{n,i},\bm{\xi}_{n,k}\rangle\cdot\frac{\exp( \langle\bm{q}_{n,k}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,k}^ {(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{l^{\prime}=2}^{M}\exp(\langle\bm {q}_{n,k}^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)}\] \[\cdot\Big{(}-V_{+}^{(s)}\cdot\frac{\exp(\langle\bm{q}_{n,k}^{(s)}, \bm{k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,k}^{(s)},\bm{k}_{+}^{(s)} \rangle)+\sum\limits_{l^{\prime}=2}^{M}\exp(\langle\bm{q}_{n,k}^{(s)},\bm{k}_ {n,l^{\prime}}^{(s)}\rangle)}\] \[+V_{n,j}^{(s)}\big{(}1-\frac{\exp(\langle\bm{q}_{n,k}^{(s)},\bm{ k}_{n,j}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,k}^{(s)},\bm{k}_{+}^{(s)} \rangle)+\sum\limits_{l^{\prime}=2}^{M}\exp(\langle\bm{q}_{n,k}^{(s)},\bm{k}_ {n,l^{\prime}}^{(s)}\rangle)}\big{)}\] \[-\sum\limits_{l\neq i}\big{(}V_{n,l}^{(s)}\cdot\frac{\exp(\langle \bm{q}_{n,k}^{(s)},\bm{k}_{n,l}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,k}^{(s)}, \bm{k}_{+}^{(s)}\rangle)+\sum\limits_{l^{\prime}=2}^{M}\exp(\langle\bm{q}_{n,k }^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)}\big{)}\Big{)}\] \[=-\frac{\eta\|\bm{\xi}_{n,i}\|_{2}^{2}}{NM}\ell_{n}^{\prime(s)} \cdot\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}{\exp( \langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{l^{\prime}=2}^ {M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)}\] \[\cdot\Big{(}-V_{+}^{(s)}\cdot\frac{\exp(\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)} \rangle)+\sum\limits_{l^{\prime}=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_ {n,l^{\prime}}^{(s)}\rangle)}\] \[+V_{n,j}^{(s)}\big{(}1-\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{ k}_{n,j}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)} \rangle)+\sum\limits_{l^{\prime}=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_ {n,l^{\prime}}^{(s)}\rangle)}\big{)}\]\[+\frac{\eta\|\boldsymbol{\xi}_{n,i}\|_{2}^{2}}{4NM}\ell_{n}^{\prime(s)}V_{+}^{(s)} \cdot\frac{\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{n,j}^{(s)} \rangle)}{\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{+}^{(s)} \rangle)+\sum\limits_{l^{\prime}=2}^{M}\exp(\langle\boldsymbol{q}_{n,i}^{(s) },\boldsymbol{k}_{n,l^{\prime}}^{(s)}\rangle)}\]\[\cdot\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)}{\exp( \langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{l^{\prime}=2}^{M }\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,l^{\prime}}\rangle)},\]

where the \(\{lower\ order\ term\}\) is by the property that \(\|\bm{\xi}_{n,i}\|_{2}^{2}\) is much larger than \(\langle\bm{\xi}_{n^{\prime},i^{\prime}},\bm{\xi}_{n,i}\rangle\) in Lemma C.4 and the condition \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\), the third inequality is by \(V_{+}^{(s)}\geq 3M\cdot|V_{n,i}^{(s)}|\) and \(softmax(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)\geq\frac{1}{M}-o(1)\), the last inequality is by \(\|\bm{\xi}_{n,i}\|_{2}^{2}\geq\frac{\sigma_{2}^{2}d}{2}\) and we absorb the \(\{lower\ order\ term\}\). Similarly, we have

\[\alpha^{(s)}_{n,i,n,j}\leq\] \[-\frac{\eta\sigma_{p}^{2}d}{9NM}\ell_{n}^{\prime(s)}V_{-}^{(s)} \cdot\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,j}\rangle)}{\exp( \langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{-}\rangle)+\sum\limits_{l^{\prime}=2} ^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,l^{\prime}}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{-} \rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{-}\rangle)+\sum\limits_ {l^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,l^{\prime}} \rangle)}.\]

\[\beta^{(s)}_{n,j,n,i}=\frac{\eta}{NM}-\ell_{n}^{\prime(s)}\] \[\Big{(}-V_{+}^{(s)}\sum\limits_{k=2}^{M}\big{(}\langle\bm{\xi}_{ n,j},\bm{\xi}_{n,k}\rangle\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+} \rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle))+\sum\limits_ {l^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,l^{\prime}} \rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,k} \rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_ {l^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,l^{\prime}} \rangle)}\] \[+\sum\limits_{k=2}^{M}V_{n,k}^{(s)}\big{(}\langle\bm{\xi}_{n,j}, \bm{\xi}_{n,k}\rangle\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,k} \rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_ {l^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,l^{\prime}} \rangle)}\] \[-\sum\limits_{l=2}^{M}\big{(}\langle\bm{\xi}_{n,j},\bm{\xi}_{n,l} \rangle\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,k}\rangle)}{\exp( \langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{l^{\prime}=2} ^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,l^{\prime}}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,l} \rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_ {l^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,l^{\prime}} \rangle)})\Big{)}\] \[=\frac{\eta\|\bm{\xi}_{n,j}\|_{2}^{2}}{NM}-\ell_{n}^{\prime(s)} \cdot\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,j}\rangle)}{\exp( \langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{l^{\prime}=2}^ {M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,l^{\prime}}\rangle)}\] \[\Big{(}-V_{+}^{(s)}\big{(}\frac{\exp(\langle\bm{q}^{(s)}_{n,i}, \bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle) +\sum\limits_{l^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,l ^{\prime}}\rangle)}\big{)}\] \[+V_{n,j}^{(s)}\big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{ k}^{(s)}_{n,j}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+} \rangle)+\sum\limits_{l^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s) }_{n,l^{\prime}}\rangle)}\big{)}\Big{)}\] \[+\{lower\ order\ term\}\]\[\leq\frac{\eta\|\bm{\xi}_{n,j}\|_{2}^{2}}{NM}-\ell_{n}^{(s)}\cdot \frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}{\exp(\langle\bm {q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{l^{\prime}=2}^{M}\exp( \langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)}\] \[\leq\frac{\eta\|\bm{\xi}_{n,j}\|_{2}^{2}}{NM}-\ell_{n}^{(s)}\cdot \frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}{\exp(\langle \bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{l^{\prime}=2}^{M} \exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)}\] \[\leq\frac{\eta\|\bm{\xi}_{n,j}\|_{2}^{2}}{NM}-\ell_{n}^{(s)} \cdot\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}{\exp( \langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{l^{\prime}=2} ^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)}\] \[\Big{(}-V_{+}^{(s)}\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+ }^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum \limits_{l^{\prime}=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{\prime}} ^{(s)}\rangle)}\] \[+\frac{1}{2}V_{+}^{(s)}\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{ k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+ \sum\limits_{l^{\prime}=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{ \prime}}^{(s)}\rangle)}\Big{)}\] \[+\{lower\ order\ term\}\] \[\leq\frac{\eta\|\bm{\xi}_{n,j}\|_{2}^{2}}{2NM}\ell_{n}^{(s)}V_{+}^ {(s)}\cdot\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}{ \exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{l^{\prime }=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle) }{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{l^{ \prime}=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)}\] \[+\{lower\ order\ term\}\] \[\leq\frac{\eta\sigma_{p}^{2}d}{5NM}\ell_{n}^{(s)}V_{+}^{(s)} \cdot\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}{\exp( \langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{l^{\prime}=2} ^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle) }{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{l^{ \prime}=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)},\]

where the \(\{lower\ order\ term\}\) is by the property that \(\|\bm{\xi}_{n,i}\|_{2}^{2}\) is much larger than \(\langle\bm{\xi}_{n^{\prime},i^{\prime}},\bm{\xi}_{n,i}\rangle\) in Lemma C.4 and the condition \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\), the second inequality is by \(V_{+}^{(s)}\geq 3M\cdot|V_{n,i}^{(s)}|\), the last inequality is by \(\|\bm{\xi}_{n,j}\|_{2}^{2}\geq\frac{\sigma_{p}^{2}d}{2}\) and we absorb the \(\{lower\ order\ term\}\). Similarly, we have

\[\beta_{n,j,n,i}^{(s)}\leq\] \[-\frac{\eta\sigma_{p}^{2}d}{5NM}\ell_{n}^{(s)}V_{-}^{(s)}\cdot \frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}{\exp(\langle \bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle)+\sum\limits_{l^{\prime}=2}^{M}\exp( \langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle)} {\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle)+\sum\limits_{l^{ \prime}=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l^{\prime}}^{(s)}\rangle)}.\]

Summarizing the above equations, we have the signs of \(\alpha\) and \(\beta\) as follows:\[\alpha^{(s)}_{+,+},\alpha^{(s)}_{-,-},\beta^{(s)}_{+,+},\beta^{(s)}_{ -,-},\alpha^{(s)}_{n,i,+},\alpha^{(s)}_{n,i,-},\beta^{(s)}_{n,+,i},\beta^{(s)}_{ n,-,i}\geq 0,\] \[\alpha^{(s)}_{n,+,i},\alpha^{(s)}_{n,-,i},\alpha^{(s)}_{n,i,n,j}, \beta^{(s)}_{n,i,+},\beta^{(s)}_{n,i,-},\beta^{(s)}_{n,j,n,i}\leq 0.\]

### Lower Bounds of \(\langle\) q, k \(\rangle\)

In order to give the lower bounds for \(\langle\bm{q},\bm{k}\rangle\), we need to rewrite the bounds of \(\alpha\) and \(\beta\) in a more concise form. We first expand the equations in F.4 under the assumption that \(\mathcal{B}(s)\) holds for \(s\in[T_{1},t]\).

\[\alpha^{(s)}_{+,+}\] \[\geq\frac{\eta}{2NM}\sum_{n\in S_{+}}-\ell^{\prime(s)}_{n}\|\bm{ \mu}\|_{2}^{2}V^{(s)}_{+}\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)} _{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits \limits_{j^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{ \prime}}\rangle)}\] \[\cdot\big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{ +}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits \limits_{j^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{ \prime}}\rangle)}\big{)}\] \[=\frac{\eta}{2NM}\sum_{n\in S_{+}}-\ell^{\prime(s)}_{n}\|\bm{\mu} \|_{2}^{2}V^{(s)}_{+}\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+} \rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j ^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{\prime}} \rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j} \rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j ^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{\prime}} \rangle)}\] (223) \[\geq\frac{\eta}{2NM}-\ell^{\prime(s)}_{n}\|\bm{\mu}\|_{2}^{2}V^{(s )}_{+}\cdot\big{(}\frac{1}{M}-o(1)\big{)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j} \rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j ^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{\prime}} \rangle)}\] \[\geq\frac{\eta}{2NM}-\ell^{\prime(s)}_{n}\|\bm{\mu}\|_{2}^{2}V^{( s)}_{+}\cdot(\frac{1}{M}-o(1))\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{ n,j}\rangle)}{C\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)}\] \[\geq\frac{\eta^{2}C_{5}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}(s -T_{1})}{N}\cdot\frac{1}{\exp(\Lambda^{(s)}_{n,+,j})},\]

where the \(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle\) in the second inequality is a particular choice (we will characterize the dynamic of \(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle-\langle\bm{q}^{(s)}_{+},\bm{k}^ {(s)}_{n,j}\rangle\)), the third inequality is by \(softmax(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)\geq\big{(}\frac{1}{M}- o(1)\big{)}\). In the fourth inequality, by \(\langle\bm{q}^{(T_{1})},\bm{k}^{(T_{1})}\rangle=o(1)\) and the monotonicity of \(\langle\bm{q}^{(s)},\bm{k}^{(s)}\rangle\)\((\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle\) is increasing and \(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle\) is decreasing), there exist a constant C such that \(C\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)\geq\exp(\langle\bm{q}^{( s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle\bm{q}^{( s)}_{+},\bm{k}^{(s)}_{n,j^{\prime}}\rangle)\). In the last inequality, we plugging the lower bounds of \(V_{+}^{(s)}\) and \(-\ell_{n}^{\prime(s)}\) and then absorb all the constant factors. Similarly, we have

\[\beta_{+,+}^{(s)}\geq\frac{\eta^{2}C_{5}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{ 2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,+,j}^{(s)})},\] (224)

\[\alpha_{-,-}^{(s)}\geq\frac{\eta^{2}C_{5}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2} ^{2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,-,j}^{(s)})},\] (225)

\[\beta_{-,-}^{(s)}\geq\frac{\eta^{2}C_{5}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2} ^{2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,-,j}^{(s)})}.\] (226)

\[\alpha_{n,+,j}^{(s)}\] (227) \[\leq\frac{\eta}{4NM}\ell_{n}^{\prime(s)}\|\bm{\mu}\|_{2}^{2}V_{+ }^{(s)}\cdot\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}{ \exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j^{\prime}= 2}^{M}\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j^{\prime}}^{(s)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle) }{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j^{\prime }=2}^{M}\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j^{\prime}}^{(s)}\rangle)}\] \[\leq\frac{\eta}{4NM}\ell_{n}^{\prime(s)}\|\bm{\mu}\|_{2}^{2}V_{+ }^{(s)}\cdot(\frac{1}{M}-o(1))\cdot\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{ n,j}^{(s)}\rangle)}{C\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)}\] \[\leq-\frac{\eta^{2}C_{5}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2 }(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,+,j}^{(s)})},\]

where the inequalities is similar to (223). Similarly, we have

\[\alpha_{n,-,j}^{(s)}\leq-\frac{\eta^{2}C_{5}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\| _{2}^{2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,-,j}^{(s)})}.\]\[\beta^{(s)}_{n,+,i}\] \[\geq-\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{2NM}\ell_{n}^{\prime(s) }V_{+}^{(s)}\cdot\big{(}\frac{1}{M}-o(1)\big{)}\] \[\cdot\frac{\exp(\langle\boldsymbol{q}^{(s)}_{n,i},\boldsymbol{k}^ {(s)}_{n,j^{\prime}}\rangle)}{\exp(\langle\boldsymbol{q}^{(s)}_{n,i}, \boldsymbol{k}^{(s)}_{+}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle \boldsymbol{q}^{(s)}_{n,i},\boldsymbol{k}^{(s)}_{n,j^{\prime}}\rangle)}\] \[\geq-\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{2NM}\ell_{n}^{\prime (s)}V_{+}^{(s)}\cdot\big{(}\frac{1}{M}-o(1)\big{)}\] \[\cdot\frac{\sum\limits_{j^{\prime}=2}^{M}\exp(\langle\boldsymbol{ q}^{(s)}_{n,i},\boldsymbol{k}^{(s)}_{n,j^{\prime}}\rangle)}{\exp(\langle \boldsymbol{q}^{(s)}_{n,i},\boldsymbol{k}^{(s)}_{n,j^{\prime}}\rangle)+\sum \limits_{j^{\prime}=2}^{M}\exp(\langle\boldsymbol{q}^{(s)}_{n,i},\boldsymbol{ k}^{(s)}_{n,j^{\prime}}\rangle)}\] (228) \[\geq-\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{2NM}\ell_{n}^{\prime (s)}V_{+}^{(s)}\cdot\big{(}\frac{1}{M}-o(1)\big{)}\] \[\cdot\frac{\exp(\langle\boldsymbol{q}^{(s)}_{n,i},\boldsymbol{k} ^{(s)}_{n,j}\rangle)}{\exp(\langle\boldsymbol{q}^{(s)}_{n,i},\boldsymbol{k}^{( s)}_{n,j}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle\boldsymbol{q}^{(s)}_{n,i}, \boldsymbol{k}^{(s)}_{n,j^{\prime}}\rangle)}\] \[\geq-\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{2NM}\ell_{n}^{\prime (s)}V_{+}^{(s)}\cdot\big{(}\frac{1}{M}-o(1)\big{)}\cdot\frac{\exp(\langle \boldsymbol{q}^{(s)}_{n,i},\boldsymbol{k}^{(s)}_{n,j}\rangle)}{C\exp(\langle \boldsymbol{q}^{(s)}_{n,i},\boldsymbol{k}^{(s)}_{n}\rangle)}\] \[\geq\frac{\eta^{2}C_{5}\|\boldsymbol{\mu}\|_{2}^{4}\|\boldsymbol{ w}_{O}\|_{2}^{2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda^{(s)}_{n,i,+,j})},\]

where the inequalities is similar to (223), and \(\langle\boldsymbol{q}^{(s)}_{n,i},\boldsymbol{k}^{(s)}_{n,j}\rangle\) is a particular choice (we will characterize the dynamic of \(\langle\boldsymbol{q}^{(s)}_{n,i},\boldsymbol{k}^{(s)}_{+}\rangle-\langle \boldsymbol{q}^{(s)}_{n,i},\boldsymbol{k}^{(s)}_{n,j}\rangle\)). Similarly, we have

\[\beta^{(s)}_{n,-,i}\geq\frac{\eta^{2}C_{5}\|\boldsymbol{\mu}\|_{2}^{4}\| \boldsymbol{w}_{O}\|_{2}^{2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda^{(s)}_{n,i,-,j})},\] (229)

\[\alpha^{(s)}_{n,i+}\geq\frac{\eta^{2}C_{5}\sigma_{p}^{2}d\|\boldsymbol{\mu}\|_ {2}^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda^ {(s)}_{n,i,+,j})},\] (230)

\[\alpha^{(s)}_{n,i-}\geq\frac{\eta^{2}C_{5}\sigma_{p}^{2}d\|\boldsymbol{\mu}\|_ {2}^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda^ {(s)}_{n,i,-,j})},\] (231)

\[\beta^{(s)}_{n,j,+}\leq-\frac{\eta^{2}C_{5}\sigma_{p}^{2}d\|\boldsymbol{\mu}\|_ {2}^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda^{(s )}_{n,+,j})},\] (232)

\[\beta^{(s)}_{n,j,-}\leq-\frac{\eta^{2}C_{5}\sigma_{p}^{2}d\|\boldsymbol{\mu}\|_ {2}^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda^ {(s)}_{n,-,j})},\] (233)

\[\alpha^{(s)}_{n,i,n,j}\leq-\frac{\eta^{2}C_{5}\sigma_{p}^{2}d\|\boldsymbol{\mu}\|_ {2}^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda^ {(s)}_{n,i,\pm,j})},\] (234)

\[\beta^{(s)}_{n,j,n,i}\leq-\frac{\eta^{2}C_{5}\sigma_{p}^{2}d\|\boldsymbol{\mu}\|_ {2}^{2}\|\boldsymbol{w}_{O}\|_{2}^{2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda^ {(s)}_{n,i,\pm,j})}.\] (235)With the concise lower bounds for \(\alpha\) and \(\beta\) above and proposition \(\mathcal{C}(s)\), we will give the lower bounds for the dynamics of \(\langle\bm{q},\bm{k}\rangle\).

\[\begin{split}&\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle- \langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\\ &=\alpha_{+,+}^{(s)}\|\bm{k}_{+}^{(s)}\|_{2}^{2}+\sum_{n\in S_{+} }\sum_{i=2}^{M}\alpha_{n,+,i}^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_{n,i}^{(s)} \rangle\\ &+\beta_{+,+}^{(s)}\|\bm{q}_{+}^{(s)}\|_{2}^{2}+\sum_{n\in S_{+} }\sum_{i=2}^{M}\beta_{n,+,i}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{n,i}^{(s)} \rangle\\ &+\Big{(}\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n\in S_{+}} \sum_{i=2}^{M}\alpha_{n,+,i}^{(s)}\bm{k}_{n,i}^{(s)}\Big{)}\\ &\cdot\Big{(}\beta_{+,+}^{(s)}\bm{q}_{+}^{(s)\top}+\sum_{n\in S_{+ }}\sum_{i=2}^{M}\beta_{n,+,i}^{(s)\top}\bm{q}_{n,i}^{(s)\top}\Big{)}\\ &=\alpha_{+,+}^{(s)}\|\bm{k}_{+}^{(s)}\|_{2}^{2}+\beta_{+,+}^{(s )}\|\bm{q}_{+}^{(s)}\|_{2}^{2}+\{lower\;order\;term\}\\ &\geq\frac{2\eta^{2}C_{5}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{0}\|_{2}^{ 2}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,+,j}^{(s)})}\cdot\Theta(\|\bm{\mu }\|_{2}^{2}\sigma_{h}^{2}d_{h})\\ &+\{lower\;order\;term\}\\ &\geq\frac{\eta^{2}C_{6}\|\bm{\mu}\|_{2}^{6}\|\bm{w}_{0}\|_{2}^{ 2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,+,j}^{(s)})}, \end{split}\] (236)

where the first inequality is by (223), (224), the bound of \(\|\bm{q}_{+}^{(s)}\|_{2}^{2}\), \(\|\bm{k}_{+}^{(s)}\|_{2}^{2}\) in stage II, the second inequality is by absorbing the \(\{lower\;order\;term\}\) and the constant factors. Similarly, we have

\[\begin{split}&\langle\bm{q}_{-}^{(s+1)},\bm{k}_{-}^{(s+1)} \rangle-\langle\bm{q}_{-}^{(s)},\bm{k}_{-}^{(s)}\rangle\\ &\geq\frac{\eta^{2}C_{6}\|\bm{\mu}\|_{2}^{6}\|\bm{w}_{0}\|_{2}^{ 2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,-,j}^{(s)})}. \end{split}\] (237)\[\langle\bm{q}_{-}^{(s+1)},\bm{k}_{n,j}^{(s+1)}\rangle-\langle\bm{q}_{-}^{ (s)},\bm{k}_{n,j}^{(s)}\rangle\] \[\leq-\frac{\eta^{2}C_{6}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{4}\|\bm{w}_ {O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,-,j} ^{(s)})}.\] (239)\[\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{-}^{(s+1)}\rangle-\langle\bm{q}_{n,i }^{(s)},\bm{k}_{-}^{(s)}\rangle\] \[\geq\frac{\eta^{2}C_{6}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{4}\|\bm{w}_ {O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda_{n,i,-, j}^{(s)})}.\]\[\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{n,j}^{(s+1)}\rangle-\langle\bm{q}_ {n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\] \[=\alpha_{n,i,+}^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_{n,j}^{(s)} \rangle+\alpha_{n,i,-}^{(s)}\langle\bm{k}_{-}^{(s)},\bm{k}_{n,j}^{(s)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime}}^{(s)}\langle\bm {k}_{n^{\prime},l}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\] \[+\beta_{n,j,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{n,i}^{(s)} \rangle+\beta_{n,j,-}^{(s)}\langle\bm{q}_{-}^{(s)},\bm{q}_{n,i}^{(s)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(s)}\langle\bm {q}_{n^{\prime},l}^{(s)},\bm{q}_{n,i}^{(s)}\rangle\] \[+\left(\alpha_{n,i,+}^{(s)}\bm{k}_{+}^{(s)}+\alpha_{n,i,-}^{(s)} \bm{k}_{-}^{(s)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime}}^ {(s)}\bm{k}_{n^{\prime},l}^{(s)}\right)\] \[\cdot\left(\beta_{n,j,+}^{(s)}\bm{q}_{+}^{(s)\top}+\beta_{n,j,-}^ {(s)\top}\bm{q}_{-}^{(s)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j, n^{\prime},l}^{(s)\top}\bm{q}_{n^{\prime},l}^{(s)\top}\right)\] (242) \[=\alpha_{n,i,n,j}^{(s)}\|\bm{k}_{n,j}^{(s)}\|_{2}^{2}+\beta_{n,j, n,i}^{(s)}\|\bm{q}_{n,i}^{(s)}\|_{2}^{2}\] \[+\{lower order~{}term\}\] \[\leq-\frac{2\eta^{2}C_{5}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2}\| \bm{w}_{O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda _{n,i,\pm,j}^{(s)})}\cdot\Theta\Big{(}\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}\Big{)}\] \[+\{lower order~{}term\}\] \[\leq-\frac{\eta^{2}C_{6}\sigma_{p}^{4}d^{2}\|\bm{\mu}\|_{2}^{2}\| \bm{w}_{O}\|_{2}^{2}\sigma_{h}^{2}d_{h}(s-T_{1})}{N}\cdot\frac{1}{\exp(\Lambda _{n,i,\pm,j}^{(s)})},\]

where the first inequality is by (234), (235), the bound of \(\|\bm{k}_{n,j}^{(s)}\|_{2}^{2}\), \(\|\bm{q}_{n,i}^{(s)}\|_{2}^{2}\) in stage II, the second inequality is by absorbing the \(\{lower order~{}term\}\) and the constant factors.

### Upper Bounds of \(\langle\) q, k \(\rangle\)

In order to give the upper bounds of \(\langle\bm{q},\bm{k}\rangle\) in stage II, we need to give the upper bounds of \(\alpha\) and \(\beta\) based on the equations in F.1 under the assumption that \(\mathcal{D}(T_{1}),\ldots,\mathcal{D}(s-1)\) hold for \(s\in[T_{1},t]\).

\[\alpha_{+,+}^{(s)}=\frac{\eta}{NM}\sum_{n\in S_{+}}-\ell_{n}^{ \prime(s)}\|\bm{\mu}\|_{2}^{2}\] \[\cdot\Big{(}V_{+}^{(s)}\big{(}\frac{\exp(\langle\bm{q}_{+}^{(s)}, \bm{k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle) +\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}\] \[-(\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)}{ \exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)})^{2}\big{)}\] \[-\sum_{i=2}^{M}\big{(}V_{n,i}^{(s)}\cdot\frac{\exp(\langle\bm{q}_{ +}^{(s)},\bm{k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s) }\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s) }\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,i}^{(s)}\rangle) }{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}\big{)}\] \[=\frac{\eta}{NM}\sum_{n\in S_{+}}-\ell_{n}^{\prime(s)}\|\bm{\mu}\|_ {2}^{2}\frac{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)}{\exp( \langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp( \langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}\]\[\cdot\Big{(}V_{+}^{(s)}\cdot\frac{\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}{\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\] \[-\sum\limits_{i=2}^{M}V_{n,i}^{(s)}\cdot\frac{\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,i}^{(s)}\rangle)}{\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\Big{)}\] \[\leq\frac{\eta}{NM}\sum\limits_{n\in S_{+}}\|\boldsymbol{\mu}\|_ {2}^{2}\cdot\Big{(}V_{+}^{(s)}\cdot\frac{\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}{\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{ M}\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\] \[+\max\limits_{i}|V_{n,i}^{(s)}|\cdot\frac{\sum\limits_{j=2}^{M} \exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}{ \exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum \limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{( s)}\rangle)}\Big{)}\] \[\leq\frac{\eta}{NM}\cdot\frac{3N}{4}\cdot\|\boldsymbol{\mu}\|_{ 2}^{2}\cdot\Big{(}V_{+}^{(s)}\cdot\frac{C}{\exp(\langle\boldsymbol{q}_{+}^{(s) },\boldsymbol{k}_{+}^{(s)}\rangle)}+\max\limits_{i}|V_{n,i}^{(s)}|\cdot\frac{ C}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)} \Big{)}\] \[\leq\frac{\eta C_{9}|\boldsymbol{\mu}\|_{2}^{2}}{\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)},\]

where the first inequality is by \(-\ell_{n}^{(s)}\leq 1\) and \(softmax(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)\leq 1\). For the second inequality, we first consider \(\frac{\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{ k}_{n,j}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{( s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{n,j}^{(s)}\rangle)}\leq\frac{\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}{\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}\), then by the monotonicity of \(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle\) and \(\langle\boldsymbol{q}_{+}^{(T_{1})},\boldsymbol{k}_{n,j}^{(T_{1})}\rangle=o(1)\) we have \(\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j }^{(s)}\rangle)\leq C\) for \(s\in[T_{1},t]\). The last inequality is by \(V_{+}^{(s)},V_{n,i}^{(s)}=o(1)\) for \(s\in[T_{1},t]\) and absorbing the constant factors. Similarly, we have

\[\alpha_{-,-}^{(s)} \leq\frac{\eta C_{9}\|\boldsymbol{\mu}\|_{2}^{2}}{\exp(\langle \boldsymbol{q}_{-}^{(s)},\boldsymbol{k}_{-}^{(s)}\rangle)},\] \[\beta_{+,+}^{(s)} \leq\frac{\eta C_{9}\|\boldsymbol{\mu}\|_{2}^{2}}{\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)},\] \[\beta_{-,-}^{(s)} \leq\frac{\eta C_{9}\|\boldsymbol{\mu}\|_{2}^{2}}{\exp(\langle \boldsymbol{q}_{-}^{(s)},\boldsymbol{k}_{-}^{(s)}\rangle)}.\]

\[\alpha_{n,+,j}^{(s)}=-\frac{\eta}{NM}\ell_{n}^{(s)}\|\boldsymbol{ \mu}\|_{2}^{2}\] \[\cdot\Big{(}-V_{+}^{(s)}\cdot\frac{\exp(\langle\boldsymbol{q}_{+}^ {(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j^{\prime}}^{(s)}\rangle)}\] \[\cdot\frac{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s) }\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{n,j^{\prime}}^{(s)}\rangle)}\] \[+V_{n,i}^{(s)}\big{(}\frac{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{n,j}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j^{\prime}}^{(s)}\rangle)}\]\[-(\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}{\exp( \langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j^{\prime}=2}^{M} \exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{\prime}}\rangle)})^{2})\] \[-\sum\limits_{k\neq j}\big{(}V^{(s)}_{n,k}\cdot\frac{\exp(\langle \bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm {k}^{(s)}_{+}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{ +},\bm{k}^{(s)}_{n,j^{\prime}}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,k} \rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{ j^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{\prime}} \rangle)})\Big{)}\] \[\geq-\frac{\eta}{NM}\ell^{\prime(s)}_{n}\|\bm{\mu}\|_{2}^{2} \cdot\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}{\exp( \langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j^{\prime}=2}^ {M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{\prime}}\rangle)}\] \[\cdot\Big{(}-V^{(s)}_{+}\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+} \rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{( s)}_{n,j^{\prime}}\rangle)}\] \[-|V^{(s)}_{n,i}|\big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{ k}^{(s)}_{n,j}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+ \sum\limits_{j^{\prime}=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^ {\prime}}\rangle)}\big{)}\] \[-\max\limits_{l}|V^{(s)}_{n,l}|\cdot\frac{\sum\limits_{k\neq j} \exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,k}\rangle)}{\exp(\langle\bm{q}^{ (s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle \bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{\prime}}\rangle)}\Big{)}\] \[\geq-\frac{2\eta}{NM}\|\bm{\mu}\|_{2}^{2}V^{(s)}_{+}\cdot\frac{ \exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}{\exp(\langle\bm{q}^{ (s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle \bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{\prime}}\rangle)}\] \[\geq-\frac{2\eta}{NM}\|\bm{\mu}\|_{2}^{2}V^{(s)}_{+}\cdot\frac{ \exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}{C}\] \[\geq-\frac{\eta C_{0}\|\bm{\mu}\|_{2}^{2}}{N}\cdot\exp(\langle\bm{ q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle),\]

where the second inequality is by \(V^{(s)}_{+}\geq 3M\cdot|V^{(s)}_{n,i}|,-\ell^{\prime(s)}_{n}\leq 1\) and the property that attention \(<1\). For the third inequality, we consider \(\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}{\exp(\langle\bm{ q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp( \langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{\prime}}\rangle)}\leq\frac{\exp( \langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j^{\prime}}\rangle)}{\exp(\langle\bm{ q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}\) first, then by the monotonicity of \(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle\) and \(\langle\bm{q}^{(T_{1})}_{+},\bm{k}^{(T_{1})}_{+}\rangle\) we have \(\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)\geq C\) for \(s\in[T_{1},t]\). The last inequality is by \(V^{(s)}_{+}=o(1)\) for \(s\in[T_{1},t]\) and absorbing the constant factors. Similarly, we have

\[\alpha^{(s)}_{n,-,j}\geq-\frac{\eta C_{0}\|\bm{\mu}\|_{2}^{2}}{N} \cdot\exp(\langle\bm{q}^{(s)}_{-},\bm{k}^{(s)}_{n,j}\rangle).\]

\[\beta^{(s)}_{n,+,i}=-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\ell^{\prime (s)}_{n}\] \[\cdot\Big{(}V^{(s)}_{+}\big{(}\frac{\exp(\langle\bm{q}^{(s)}_{n,i}, \bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,j} \rangle)}\]\[-(\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle \bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm {q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,j}\rangle)})^{2})\] \[-\sum\limits_{k=2}^{M}\big{(}V^{(s)}_{n,i}\cdot\frac{\exp(\langle \bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i}, \bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i}, \bm{k}^{(s)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,k} \rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)+\sum\limits _{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,j}\rangle)})\] \[=-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\ell_{n}^{\prime(s)}\frac{ \exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^ {(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^ {(s)}_{n,i},\bm{k}^{(s)}_{n,j}\rangle)}\] \[\cdot\Big{(}V^{(s)}_{+}\big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{ n,i},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n, j}\rangle)}\big{)}\] \[-\sum\limits_{k=2}^{M}\big{(}V^{(s)}_{n,i}\cdot\frac{\exp(\langle \bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,k}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i}, \bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i}, \bm{k}^{(s)}_{n,j}\rangle)})\Big{)}\] \[\leq-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\ell_{n}^{\prime(s)}\frac {\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^ {(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^ {(s)}_{n,i},\bm{k}^{(s)}_{n,j}\rangle)}\] \[\cdot\Big{(}V^{(s)}_{+}\big{(}1-\frac{\exp(\langle\bm{q}^{(s)}_{ n,i},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n, j}\rangle)}\big{)}\] \[+|V^{(s)}_{n,i}|(\frac{\sum\limits_{k=2}^{M}\exp(\langle\bm{q}^ {(s)}_{n,i},\bm{k}^{(s)}_{n,k}\rangle)}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^ {(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^ {(s)}_{n,j}\rangle)})\Big{)}\] \[\leq-\frac{\eta\|\bm{\mu}\|_{2}^{2}}{NM}\ell_{n}^{\prime(s)}\cdot \Big{(}V^{(s)}_{+}\cdot\frac{C}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+} \rangle)}+|V^{(s)}_{n,i}|\cdot\frac{C}{\exp(\langle\bm{q}^{(s)}_{n,i},\bm{k}^ {(s)}_{+}\rangle)}\Big{)}\] \[\leq\frac{\eta C_{9}\|\bm{\mu}\|_{2}^{2}}{N\exp(\langle\bm{q}^{(s) }_{n,i},\bm{k}^{(s)}_{+}\rangle)},\]where the second inequality is by \(softmax(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)\leq 1\). For the second inequality, we first consider \(\frac{\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)} \rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)\sum\limits_{j= 2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)}\), then by the monotonicity of \(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\) and \(\langle\bm{q}_{n,i}^{(T_{1})},\bm{k}_{n,j}^{(T_{1})}\rangle=o(1)\) we have \(\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)\leq C\) for \(s\in[T_{1},t]\). The last inequality is by \(V_{+}^{(s)},V_{n,i}^{(s)}=o(1)\) for \(s\in[T_{1},t]\) and absorbing the constant factors. Similarly, we have

\[\beta_{n,-,i}^{(s)}\leq\frac{\eta C_{0}\|\bm{\mu}\|_{2}^{2}}{N\exp(\langle\bm{ q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle)},\]

\[\alpha_{n,i,+}^{(s)}=\frac{\eta\|\bm{\xi}_{n,i}\|_{2}^{2}}{NM}- \ell_{n}^{\prime(s)}\] \[\cdot\Big{(}V_{+}^{(s)}\big{(}\frac{\exp(\langle\bm{q}_{n,i}^{(s) },\bm{k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)} \rangle)}\] \[-(\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)} {\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)})^{2}\big{)}\] \[-\sum\limits_{k=2}^{M}\big{(}V_{n,i}^{(s)}\cdot\frac{\exp(\langle \bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{n,j}^{(s)}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,k}^{(s)} \rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_ {j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)})\Big{)}\] \[+\{lower\ order\ term\}\] \[-\sum\limits_{k=2}^{M}\big{(}V_{n,i}^{(s)}\cdot\frac{\exp(\langle \bm{q}_{n,i}^{(s)},\bm{k}_{n,k}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{n,j}^{(s)}\rangle)}\big{)}\] \[+\{lower\ order\ term\}\] \[\leq\frac{\eta\|\bm{\xi}_{n,i}\|_{2}^{2}}{NM}\] \[\cdot\Big{(}V_{+}^{(s)}\big{(}1-\frac{\exp(\langle\bm{q}_{n,i}^{( s)},\bm{k}_{+}^{(s)}\rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s) }\rangle)}\big{)}\]\[\beta_{n,i,-}^{(s)}\geq-\frac{\eta C_{9}\sigma_{p}^{2}d}{N}\exp( \langle\bm{q}_{-}^{(s)},\bm{k}_{n,i}^{(s)}\rangle),\]

\[\alpha_{n,i,n,j}^{(s)}\geq-\frac{\eta C_{9}\sigma_{p}^{2}d}{N}\exp( \langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle),\]

\[\beta_{n,j,n,i}^{(s)}\geq-\frac{\eta C_{9}\sigma_{p}^{2}d}{N}\exp( \langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle),\]

Similar to F.5, we apply the bounds of \(\alpha\) and \(\beta\) above to give the upper bounds for the dynamics \(\langle\bm{q},\bm{k}\rangle\).

\[\begin{split}&\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle- \langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle\\ &=\alpha_{+,+}^{(s)}\|\bm{k}_{+}^{(s)}\|_{2}^{2}+\sum_{n\in S_{+} }\sum_{i=2}^{M}\alpha_{n,+,i}^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_{n,i}^{(s)}\rangle \\ &+\beta_{+,+}^{(s)}\|\bm{q}_{+}^{(s)}\|_{2}^{2}+\sum_{n\in S_{+} }\sum_{i=2}^{M}\beta_{n,+,i}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{n,i}^{(s)} \rangle\\ &+\Big{(}\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n\in S_{+}} \sum_{i=2}^{M}\alpha_{n,+,i}^{(s)}\bm{k}_{n,i}^{(s)}\Big{)}\\ &\cdot\Big{(}\beta_{+,+}^{(s)}\bm{q}_{+}^{(s)\top}+\sum_{n\in S_ {+}}\sum_{i=2}^{M}\beta_{n,+,i}^{(s)}\bm{q}_{n,i}^{(s)\top}\Big{)}\\ &=\alpha_{+,+}^{(s)}\|\bm{k}_{+}^{(s)}\|_{2}^{2}+\beta_{+,+}^{(s )}\|\bm{q}_{+}^{(s)}\|_{2}^{2}+\{lower\;order\;term\}\\ &\leq\frac{2\eta C_{0}\|\bm{\mu}\|_{2}^{4}}{\exp(\langle\bm{q}_{ +}^{(s)},\bm{k}_{+}^{(s)}\rangle)}\cdot\Theta(\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{ 2}d_{h})+\{lower\;order\;term\}\\ &\leq\frac{\eta C_{10}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2}d_{h}}{ \exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)},\end{split}\] (243)

similarly, we have

\[\langle\bm{q}_{-}^{(s+1)},\bm{k}_{-}^{(s+1)}\rangle-\langle\bm{q}_{-}^{(s)}, \bm{k}_{-}^{(s)}\rangle\leq\frac{\eta C_{10}\|\bm{\mu}\|_{2}^{4}\sigma_{h}^{2} d_{h}}{\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{-}^{(s)}\rangle)}.\] (244)

\[\begin{split}&\langle\bm{q}_{+}^{(s+1)},\bm{k}_{n,j}^{(s+1)} \rangle-\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\\ &=\alpha_{+,+}^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_{n,j}^{(s)} \rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\alpha_{n^{\prime}+,l}^{(s)} \langle\bm{k}_{n,j}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle\\ &+\beta_{n,j,+}^{(s)}\|\bm{q}_{+}^{(s)}\|_{2}^{2}+\beta_{n,j,-}^ {(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{-}^{(s)}\rangle+\sum_{n^{\prime}=1}^{N} \sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{n^ {\prime},l}^{(s)}\rangle\\ &+\Big{(}\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n^{\prime}\in S _{+}}\sum_{l=2}^{M}\alpha_{n^{\prime}+,l}^{(s)}\bm{k}_{n^{\prime},l}^{(s)} \Big{)}\\ &\cdot\Big{(}\beta_{n,j,+}^{(s)\top}\bm{q}_{+}^{(s)\top}+\beta_{n,j,-}^{(s)\top}\bm{q}_{-}^{(s)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M} \beta_{n,j,n^{\prime},l}^{(s)}\bm{q}_{n^{\prime},l}^{(s)\top}\Big{)}\\ &=\alpha_{n,+,j}^{(s)}\|\bm{k}_{n,j}^{(s)}\|_{2}^{2}+\beta_{n,j,+ }^{(s)\top}\|\bm{q}_{+}^{(s)}\|_{2}^{2}+\{lower\;order\;term\}\\ &\geq-\frac{\eta C_{9}\|\bm{\mu}\|_{2}^{2}}{N}\cdot\exp(\langle \bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)\cdot\Theta\Big{(}\sigma_{p}^{2} \sigma_{h}^{2}dd_{h}\Big{)}\\ &-\frac{\eta C_{9}\sigma_{p}^{2}d}{N}\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)\cdot\Theta\Big{(}\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{ 2}d_{h}\Big{)}\\ &+\{lower\;order\;term\}\\ &\geq-\frac{\eta C_{10}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2} \sigma_{h}^{2}d_{h}}{N}\cdot\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)} \rangle),\end{split}\] (245)

similarly, we have

\[\langle\bm{q}_{-}^{(s+1)},\bm{k}_{n,j}^{(s+1)}\rangle-\langle\bm{q}_{-}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\geq-\frac{\eta C_{10}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^ {2}\sigma_{h}^{2}d_{h}}{N}\cdot\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{n,j}^{(s)} \rangle).\] (246)\[\begin{split}&\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle- \langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle\\ &=\alpha_{n,i,+}^{(s)}\|\bm{k}_{+}^{(s)}\|_{2}^{2}+\alpha_{n,i,- }^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_{-}^{(s)}\rangle+\sum_{n^{\prime}=1}^{N }\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_ {n^{\prime},l}^{(s)}\rangle\\ &+\beta_{++,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{n,i}^{(s)} \rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(s)} \langle\bm{q}_{n,i}^{(s)},\bm{q}_{n^{\prime},l}^{(s)}\rangle\\ &+\left(\alpha_{n,i,+}^{(s)}\bm{k}_{+}^{(s)}+\alpha_{n,i,-}^{(s) }\bm{k}_{-}^{(s)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime },l}^{(s)}\bm{k}_{n^{\prime},l}^{(s)}\right)\\ &\cdot\left(\beta_{++,+}^{(s)}\bm{q}_{+}^{(s)\top}+\sum_{n^{ \prime}\in S_{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(s)}\bm{q}_{n^{\prime},l}^{(s)\top}\right)\\ &=\alpha_{n,i,+}^{(s)}\|\bm{k}_{+}^{(s)}\|_{2}^{2}+\beta_{n,+,i}^ {(s)}\|\bm{q}_{n,i}^{(s)}\|_{2}^{2}+\{lower\;order\;term\}\\ &\leq\frac{\eta C_{9}\sigma_{p}^{2}d}{N\exp(\langle\bm{q}_{n,i}^ {(s)},\bm{k}_{+}^{(s)}\rangle)}\cdot\Theta\Big{(}\|\bm{\mu}\|_{2}^{2}\sigma_{ h}^{2}d_{h}\Big{)}\\ &+\frac{\eta C_{9}\|\bm{\mu}\|_{2}^{2}}{N\exp(\langle\bm{q}_{n,i} ^{(s)},\bm{k}_{+}^{(s)}\rangle)}\cdot\Theta\Big{(}\sigma_{p}^{2}\sigma_{h}^{2} dd_{h}\Big{)}\\ &+\{lower\;order\;term\}\\ &\leq\frac{\eta C_{10}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2}\sigma_{ h}^{2}d_{h}}{N\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)},\end{split}\] (247)

similarly, we have

\[\begin{split}&\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{-}^{(s+1)} \rangle-\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle\leq\frac{\eta C_{10} \sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}}{N\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle)}.\end{split}\] (248)

\[\begin{split}&\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{n,j}^{(s+1)} \rangle-\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\\ &=\alpha_{n,i,+}^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_{n,j}^{(s)} \rangle+\alpha_{n,i,-}^{(s)}\langle\bm{k}_{-}^{(s)},\bm{k}_{n,j}^{(s)}\rangle +\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(s)}\langle\bm {k}_{n^{\prime},l}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\\ &+\beta_{n,j,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{n,i}^{(s)} \rangle+\beta_{n,j,-}^{(s)}\langle\bm{q}_{n,i}^{(s)}\rangle+\sum_{n^{\prime}= 1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(s)}\langle\bm{q}_{n^{\prime},l}^{(s)},\bm{q}_{n,i}^{(s)}\rangle\\ &+\left(\alpha_{n,i,+}^{(s)}\bm{k}_{+}^{(s)}+\alpha_{n,i,-}^{(s) }\bm{k}_{-}^{(s)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime },l}^{(s)}\bm{k}_{n^{\prime},l}^{(s)}\right)\\ &\cdot\left(\beta_{n,j,+}^{(s)\top}\bm{q}_{+}^{(s)\top}+\beta_{ n,j,-}^{(s)\top}\bm{q}_{-}^{(s)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{ \prime},l}^{(s)\top}\bm{q}_{n^{\prime},l}^{(s)\top}\right)\\ &=\alpha_{n,i,n,j}^{(s)}\|\bm{k}_{n,j}^{(s)}\|_{2}^{2}+\beta_{n,j, n,i}^{(s)}\|\bm{q}_{n,i}^{(s)}\|_{2}^{2}\\ &+\{lower\;order\;term\}\\ &\geq-\frac{2\eta C_{9}\sigma_{p}^{2}d}{N}\exp(\langle\bm{q}_{n,i }^{(s)},\bm{k}_{n,j}^{(s)}\rangle)\cdot\Theta\Big{(}\sigma_{p}^{2}\sigma_{h}^{2 }dd_{h}\Big{)}\\ &+\{lower\;order\;term\}\\ &\geq-\frac{\eta C_{10}\sigma_{p}^{4}d^{2}\sigma_{h}^{2}d_{h}}{N} \cdot\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle).\end{split}\] (249)

### Bounds for the Sum of \(\alpha\) and \(\beta\)

The gradients of the inner products of \(\bm{q}\) and \(\bm{k}\) contain a lot of coefficients \(\alpha\) and \(\beta\), and in order to conveniently give the upper bounds of some lower order inner products, we will give upper bounds for the summation of \(\alpha\) and \(\beta\) (e.g. \(\sum\limits_{s=T_{1}}^{t}|\alpha^{(s)}_{+,+}|\)).

Note that in the Jacobi matrix of the Softmax function, the elements on the diagonal are \(softmax(a_{i})\cdot\left(1-softmax(a_{i})\right)\) and the elements on the off-diagonal are \(softmax(a_{i})\cdot softmax(a_{j})\). In Stage II, the attentions on signals \(\bm{\mu}_{\pm}\) increase and the attentions on noises \(\bm{\xi}\) decrease, then we can consider the following cases

* if \(a_{i}=\langle\bm{q}_{+},\bm{k}_{+}\rangle\) or \(a_{i}=\langle\bm{q}_{i},\bm{k}_{+}\rangle\), \(softmax(a_{i})\) has a constant upper bound 1, \(\left(1-softmax(a_{i})\right)\) decreases as \(softmax(a_{i})\) increases. So the upper bound of \(softmax(a_{i})\cdot\left(1-softmax(a_{i})\right)\) decreases as \(softmax(a_{i})\) increases.
* if \(a_{i}=\langle\bm{q}_{+},\bm{k}_{j}\rangle\) or \(a_{i}=\langle\bm{q}_{i},\bm{k}_{j}\rangle\), \(\left(1-softmax(a_{i})\right)\) has a constant upper bound 1. So the upper bound of \(softmax(a_{i})\cdot\left(1-softmax(a_{i})\right)\) decreases as \(softmax(a_{i})\) decreases.
* if \(a_{j}=\langle\bm{q}_{+},\bm{k}_{j}\rangle\) or \(a_{j}=\langle\bm{q}_{i},\bm{k}_{j}\rangle\), \(softmax(a_{i})\) has a constant upper bound 1. So the upper bound of \(softmax(a_{i})\cdot softmax(a_{j})\) decreases as \(softmax(a_{j})\) decreases.

Based on the above cases, we first study the bounds of the following terms

* \(1-softmax(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)\)
* \(1-softmax(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)\)
* \(softmax(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)\)
* \(softmax(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,j}\rangle)\)

Note that \(1-softmax(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)\) = \(\sum\limits_{j}softmax(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)\) and \(1-softmax(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle)\) = \(\sum\limits_{j}softmax(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,j}\rangle)\), we only need to give the upper bounds for \(softmax(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)\) and \(softmax(\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{n,j}\rangle)\).

Assume that the propositions \(\mathcal{B}(T_{1}),\ldots,\mathcal{B}(s)\), \(\mathcal{D}(T_{1}),\ldots,\mathcal{D}(s-1)\) hold (\(s\in[T_{1},t]\)), we have

\[|V^{(s)}_{\pm}|,|V^{(s)}_{n,i}|\leq O(d_{h}^{-\frac{1}{4}})+\eta C_{4}\|\bm{ \mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(s-T_{1}),\] (250)

\[\Lambda^{(s)}_{n,\pm,j}\geq\log\Big{(}\exp(\Lambda^{(T_{1})}_{n,\pm,j})+\frac {\eta^{2}C_{8}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N \big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(s-T_{1})(s-T_{1}-1)\Big{)},\] (251)

\[\Lambda^{(s)}_{n,i,\pm,j}\geq\log\Big{(}\exp(\Lambda^{(T_{1})}_{n,i,\pm,j})+ \frac{\eta^{2}C_{8}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}d_ {h}^{\frac{1}{2}}}{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(s-T_{1})( s-T_{1}-1)\Big{)},\] (252)

for \(i,j\in[M]\backslash\{1\},n\in[N],s\in[T_{1},t]\).

Then we have

\[\begin{split}&\frac{\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j}^{(s)} \rangle)}{\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle)+\sum\limits_ {j^{\prime}=2}^{M}\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j^{\prime}}^{(s)} \rangle)}\\ &\leq\frac{\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j}^{(s)} \rangle)}{C\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle)}\\ &=\frac{1}{C\exp(\Lambda_{n,\pm,j}^{(s)})}\\ &\leq\frac{1}{C\exp(\Lambda_{n,\pm,j}^{(\Lambda_{T_{1}})})+\frac {\eta^{2}C_{8}C\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{0}\|_{2}^{2}d_{h}^{\frac{1}{2}}} {N\left(\log(6N^{2}M^{2}/\delta)\right)^{2}}\cdot(s-T_{1})(s-T_{1}-1)}\\ &\leq\frac{1}{C_{13}+\frac{\eta^{2}C_{13}\|\bm{\mu}\|_{2}^{2}\| \bm{w}_{0}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N\left(\log(6N^{2}M^{2}/\delta)\right) ^{2}}\cdot(s-T_{1})(s-T_{1}-1)}.\end{split}\] (253)

For the first inequality, by \(\langle\bm{q}^{(T_{1})},\bm{k}^{(T_{1})}\rangle=o(1)\) and the monotonicity of \(\langle\bm{q}^{(s)},\bm{k}^{(s)}\rangle\) (\(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle\) is increasing and \(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\) is decreasing), there exist a constant C such that \(C\exp(\langle\bm{q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle)\geq\exp(\langle\bm{ q}_{\pm}^{(s)},\bm{k}_{\pm}^{(s)}\rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp( \langle\bm{q}_{\pm}^{(s)},\bm{k}_{n,j^{\prime}}^{(s)}\rangle)\). The second inequality is by plugging (251). For the last inequality, by \(\Lambda_{n,\pm,j}^{(T_{1})}=o(1)\), there exist a constant \(C_{13}\) such that \(C_{13}\leq C\exp(\Lambda_{n,\pm,j}^{(T_{1})})\) and \(C_{13}\leq C_{8}C\). Similarly, we have

\[\begin{split}&\frac{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)} \rangle)}{\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)+\sum\limits_ {j^{\prime}=2}^{M}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j^{\prime}}^{(s)} \rangle)}\\ &\leq\frac{1}{C\exp(\Lambda_{n,i+,j}^{(s)})}\\ &\leq\frac{1}{C_{13}+\frac{\eta^{2}C_{13}\sigma_{2}^{2}d\|\bm{ \mu}\|_{2}^{2}\|\bm{w}_{0}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N\left(\log(6N^{2}M^ {2}/\delta)\right)^{2}}\cdot(s-T_{1})(s-T_{1}-1)}.\end{split}\] (254)

Plugging (250),(253) and (254) into the expressions of \(\alpha\), \(\beta\) we have \[\begin{split}&\alpha^{(s)}_{+,+}|=\Big{|}\frac{\eta}{NM}\sum\limits_{n \in S_{+}}-\ell^{\prime(s)}_{n}\|\boldsymbol{\mu}\|_{2}^{2}\\ &\cdot\Big{(}V^{(s)}_{+}\big{(}\frac{\exp(\langle\boldsymbol{q}^{ (s)}_{+},\boldsymbol{k}^{(s)}_{+}\rangle)}{\exp(\langle\boldsymbol{q}^{(s)}_ {+},\boldsymbol{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle \boldsymbol{q}^{(s)}_{+},\boldsymbol{k}^{(s)}_{n,j}\rangle)}\\ &-(\frac{\exp(\langle\boldsymbol{q}^{(s)}_{+},\boldsymbol{k}^{( s)}_{+}\rangle)}{\exp(\langle\boldsymbol{q}^{(s)}_{+},\boldsymbol{k}^{(s)}_{+} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}^{(s)}_{+}, \boldsymbol{k}^{(s)}_{n,j}\rangle)})^{2}\big{)}\\ &-\sum\limits_{i=2}^{M}\big{(}V^{(s)}_{n,i}\cdot\frac{\exp( \langle\boldsymbol{q}^{(s)}_{+},\boldsymbol{k}^{(s)}_{+}\rangle)}{\exp( \langle\boldsymbol{q}^{(s)}_{+},\boldsymbol{k}^{(s)}_{+}\rangle)+\sum\limits _{j=2}^{M}\exp(\langle\boldsymbol{q}^{(s)}_{+},\boldsymbol{k}^{(s)}_{n,j} \rangle)}\\ &\cdot\frac{\exp(\langle\boldsymbol{q}^{(s)}_{+},\boldsymbol{k}^{ (s)}_{n,i}\rangle)}{\exp(\langle\boldsymbol{q}^{(s)}_{+},\boldsymbol{k}^{(s)}_ {+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}^{(s)}_{+}, \boldsymbol{k}^{(s)}_{n,j}\rangle)}\\ &\leq\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{NM}\cdot\frac{3N}{4} \cdot\Big{(}O(d_{h}^{-\frac{1}{4}})+\eta C_{4}\|\boldsymbol{\mu}\|_{2}^{2}\| \boldsymbol{w}_{O}\|_{2}^{2}(s-T_{1})\Big{)}\\ &\cdot O\Big{(}\frac{1}{C_{13}+\frac{\eta^{2}C_{13}\|\boldsymbol{ \mu}\|_{2}^{4}\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N\big{(}\log (6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(s-T_{1})(s-T_{1}-1)}\\ &=O\Big{(}\frac{\eta\|\boldsymbol{\mu}\|_{2}^{4}\|\boldsymbol{w} _{O}\|_{2}^{2}d_{h}^{\frac{1}{4}}}{C_{13}+\frac{\eta^{2}C_{13}\|\boldsymbol{ \mu}\|_{2}^{4}\|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N\big{(}\log (6N^{2}M^{2}/\delta)\big{)}^{2}}\cdot(s-T_{1})(s-T_{1}-1)}\\ &+O\Big{(}\frac{\eta^{2}\|\boldsymbol{\mu}\|_{2}^{4}\|\boldsymbol{ w}_{O}\|_{2}^{2}(s-T_{1})}{C_{13}+\frac{\eta^{2}C_{13}\|\boldsymbol{\mu}\|_{2}^{4}\| \boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{2}}\cdot(s-T_{1})(s-T_{1}-1)}\Big{)}\\ &=O\Big{(}\eta\|\boldsymbol{\mu}\|_{2}^{2}d_{h}^{-\frac{1}{4}} \Big{)}+O\Big{(}\frac{\eta^{2}\|\boldsymbol{\mu}\|_{2}^{4}\|\boldsymbol{w}_{O} \|_{2}^{2}(s-T_{1})}{C_{13}+\frac{\eta^{2}C_{13}\|\boldsymbol{\mu}\|_{2}^{4} \|\boldsymbol{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{2}}\cdot(s-T_{1})(s-T_{1}-1)}\Big{)}.\end{split}\] (255)where the third equality is by \(\frac{\eta^{2}C_{13}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}} {N\left(\log(6N^{2}M^{2}/\delta)\right)^{\frac{1}{2}}}\cdot(s-T_{1})(s-T_{1}-1)\geq 0\) for \(s\in[T_{1},t]\). Next, we give an upper bound for \(\frac{\eta^{2}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}(s-T_{1})}{C_{13}+ \frac{\eta^{2}C_{13}\|\bm{\mu}\|_{2}^{2}|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2} }}{N\left(\log(6N^{2}M^{2}/\delta)\right)^{\frac{1}{2}}}\cdot(s-T_{1})(s-T_{1}- 1)}\) as follows:

\[\frac{\eta^{2}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}(s-T_{1})}{ C_{13}+\frac{\eta^{2}C_{13}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{ \frac{1}{2}}}{N\left(\log(6N^{2}M^{2}/\delta)\right)^{\frac{1}{2}}}\cdot(s-T_{ 1})(s-T_{1}-1)}\] \[=\frac{\eta^{2}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}}{\frac {C_{13}}{(s-T_{1})}+\frac{\eta^{2}C_{13}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2} ^{2}d_{h}^{\frac{1}{2}}}{N\left(\log(6N^{2}M^{2}/\delta)\right)}\cdot(s-T_{1})- \frac{\eta^{2}C_{13}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{ 2}}}{N\left(\log(6N^{2}M^{2}/\delta)\right)^{2}}}\] \[\leq\frac{\eta^{2}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}}{2 \sqrt{\frac{\eta^{2}C_{13}^{2}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h}^ {\frac{1}{2}}}{N\left(\log(6N^{2}M^{2}/\delta)\right)^{\frac{1}{2}}}-\frac{ \eta^{2}C_{13}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N \left(\log(6N^{2}M^{2}/\delta)\right)^{\frac{1}{2}}}}}\] (256) \[=\frac{\eta^{2}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2}}{\frac {2\eta C_{13}\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}d_{h}^{\frac{1}{2}}}{N^{ \frac{1}{2}}\left(\log(6N^{2}M^{2}/\delta)\right)}-\frac{\eta^{2}C_{13}\|\bm{ \mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{2}}}{N\left(\log(6N^{2}M^ {2}/\delta)\right)^{\frac{1}{2}}}}\] \[=\frac{\eta^{2}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}}{\Theta \Big{(}\frac{\eta\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}d_{h}^{\frac{1}{2}}}{N^{ \frac{1}{2}}\log(6N^{2}M^{2}/\delta)\Big{)}}}\] \[=O\Big{(}\eta\|\bm{\mu}\|_{2}^{2}N^{\frac{1}{2}}d_{h}^{-\frac{1}{ 4}}\log(6N^{2}M^{2}/\delta)\Big{)},\]

where the inequality is by \(ax+\frac{b}{x}\geq 2\sqrt{ab}\) for \(x>0\), the third equality is by absorbing the lower order term \(\frac{\eta^{2}C_{13}\|\bm{\mu}\|_{2}^{4}\|\bm{w}_{O}\|_{2}^{2}d_{h}^{\frac{1}{ 2}}}{N\left(\log(6N^{2}M^{2}/\delta)\right)^{\frac{1}{2}}}\), the last equality is by \(\|\bm{w}_{O}\|_{2}=\Theta(1)\). Plugging this into (255) and get

\[|\alpha_{+,+}^{(s)}| =O\Big{(}\eta\|\bm{\mu}\|_{2}^{2}d_{h}^{-\frac{1}{4}}\Big{)}+O \Big{(}\eta\|\bm{\mu}\|_{2}^{2}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M ^{2}/\delta)\Big{)}\] (257) \[=O\Big{(}\eta\|\bm{\mu}\|_{2}^{2}N^{\frac{1}{2}}d_{h}^{-\frac{1}{ 4}}\log(6N^{2}M^{2}/\delta)\Big{)}.\]

Similarly, we have

\[|\alpha_{-,-}^{(s)}|,|\beta_{+,+}^{(s)}|,|\beta_{-,-}^{(s)}|=O\Big{(}\eta\|\bm{ \mu}\|_{2}^{2}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/\delta) \Big{)},\] (258)

\[|\alpha_{n,+,i}^{(s)}|,|\alpha_{n,-,i}^{(s)}|=O\Big{(}\eta\|\bm{\mu}\|_{2}^{2}N ^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)},\] (259)

\[|\beta_{n,+,i}^{(s)}|,|\beta_{n,-,i}^{(s)}|=O\Big{(}\frac{\eta\|\bm{\mu}\|_{2}^{ 3}\log(6N^{2}M^{2}/\delta)}{\sigma_{p}d^{\frac{1}{2}}N^{\frac{1}{2}}d_{h}^{ \frac{1}{3}}}\Big{)}=O\Big{(}\eta\|\bm{\mu}\|_{2}^{2}\cdot\text{SNR}\cdot N^{- \frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)},\] (260)

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[|\alpha_{n,i,+}^{(s)}|,|\alpha_{n,i,-}^{(s)}|=O\Big{(}\frac{\eta\|\bm{\mu}\|_{2} \sigma_{p}d^{\frac{1}{2}}\log(6N^{2}M^{2}/\delta)}{N^{\frac{1}{2}}d_{h}^{ \frac{1}{4}}}\Big{)}=O\Big{(}\eta\|\bm{\mu}\|_{2}^{2}d_{h}^{-\frac{1}{4}}\log(6N^ {2}M^{2}/\delta)\Big{)},\] (261)

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\), the last equality is by \(N\cdot\text{SNR}^{2}\geq\Omega(1)\).

\[|\beta_{n,i,+}^{(s)}|,|\beta_{n,i,-}^{(s)}|=O\Big{(}\frac{\eta\sigma_{p}^{2}d \log(6N^{2}M^{2}/\delta)}{N^{\frac{1}{2}}d_{h}^{\frac{1}{4}}}\Big{)}=O\Big{(} \eta\|\bm{\mu}\|_{2}^{2}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/ \delta)\Big{)},\] (262)for \(i\in[M]\backslash\{1\},n\in S_{\pm}\), the last equality is by \(N\cdot\mathrm{SNR}^{2}\geq\Omega(1)\).

\[|\alpha^{(s)}_{n,i,n,j}|,|\beta^{(s)}_{n,j,n,i}|=O\Big{(}\frac{\eta\|\bm{\mu}\|_ {2}\sigma_{p}d^{\frac{1}{2}}\log(6N^{2}M^{2}/\delta)}{N^{\frac{1}{2}}d_{h}^{ \frac{1}{4}}}\Big{)}=O\Big{(}\eta\|\bm{\mu}\|_{2}^{2}d_{h}^{-\frac{1}{4}}\log( 6N^{2}M^{2}/\delta)\Big{)},\] (263)

for \(i,j\in[M]\backslash\{1\},n\in[N]\), the last equality is by \(N\cdot\mathrm{SNR}^{2}\geq\Omega(1)\).

\[|\alpha^{(s)}_{n,i,n^{\prime},j}|,|\beta^{(s)}_{n,j,n^{\prime},i}|=O\Big{(} \frac{\eta\|\bm{\mu}\|_{2}\sigma_{p}\log(6N^{2}M^{2}/\delta)\log(4N^{2}M^{2}/ \delta)}{N^{\frac{1}{2}}d_{h}^{\frac{1}{4}}}\Big{)}=O\Big{(}\eta\|\bm{\mu}\|_ {2}^{2}d^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{2}\Big{)},\] (264)

for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],n\neq n^{\prime}\), the last equality is by \(N\cdot\mathrm{SNR}^{2}\geq\Omega(1)\). Taking a summation we obtain that

\[\begin{split}\sum_{s=T_{1}}^{t}|\alpha^{(s)}_{+,+}|& =O\Big{(}\frac{1}{\eta\|\bm{\mu}\|_{2}^{2}\|\bm{w}_{O}\|_{2}^{2} \log(6N^{2}M^{2}/\delta)}\Big{)}\cdot O\Big{(}\eta\|\bm{\mu}\|_{2}^{2}N^{ \frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)}\\ &=O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)},\end{split}\] (265)

where the last equality is by \(\|\bm{w}_{O}\|=\Theta(1)\). Similarly, we have

\[\sum_{s=T_{1}}^{t}|\alpha^{(s)}_{-,-}|,\sum_{s=T_{1}}^{t}|\beta^{(s)}_{+,+}|, \sum_{s=T_{1}}^{t}|\beta^{(s)}_{-,-}|,\sum_{s=T_{1}}^{t}|\beta^{(s)}_{n,i,+}|,\sum_{s=T_{1}}^{t}|\beta^{(s)}_{n,i,-}|=O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{ 1}{4}}\Big{)},\] (266)

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\sum_{s=T_{1}}^{t}|\alpha^{(s)}_{n,+,i}|,\sum_{s=T_{1}}^{t}|\alpha^{(s)}_{n,- i}|=O\Big{(}N^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)},\] (267)

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\sum_{s=T_{1}}^{t}|\alpha^{(s)}_{n,i,+}|,\sum_{s=T_{1}}^{t}|\alpha^{(s)}_{n,i,-}|,\sum_{s=T_{1}}^{t}|\alpha^{(s)}_{n,i,n,j}|,\sum_{s=T_{1}}^{t}|\beta^{(s)} _{n,j,n,i}|=O\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\] (268)

for \(i,j\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\sum_{s=T_{1}}^{t}|\alpha^{(s)}_{n,i,+}|,\sum_{s=T_{1}}^{t}|\alpha^{(s)}_{n,i,-}|,\sum_{s=T_{1}}^{t}|\alpha^{(s)}_{n,i,n,j}|,\sum_{s=T_{1}}^{t}|\beta^{(s)} _{n,j,n,i}|=O\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\] (269)

for \(i,j\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\sum_{s=T_{1}}^{t}|\alpha^{(s)}_{n,i,n^{\prime},j}|,\sum_{s=T_{1}}^{t}|\beta^{( s)}_{n,j,n^{\prime},i}|=O\Big{(}d^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{ 2}/\delta)\Big{)}\] (270)

for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],n\neq n^{\prime}\).

With these sums of \(\alpha\) and \(\beta\) above, we can easily prove **Claim** 3 and **Claim** 4.

### Proof of Claim 3

In this subsection, we assume that \(\mathcal{E}(T_{1}),\ldots,\mathcal{E}(t)\) hold, and then proof that \(\mathcal{C}(t+1)\) is true with the result of F.7.

\[\Big{|}\|\bm{q}_{+}^{(t+1)}\|_{2}^{2}-\|\bm{q}_{+}^{(T_{1})}\|_{2}^{2}\Big{|} \leq\sum_{s=T_{1}}^{t}\Big{|}\|\bm{q}_{+}^{(s+1)}\|_{2}^{2}-\|\bm{q}_{+}^{(s)} \|_{2}^{2}\Big{|}\]\[\leq\sum_{s=T_{1}}^{t}\left|\alpha_{+,+}^{(s)}\langle\bm{q}_{+}^{(s)}, \bm{k}_{+}^{(s)}\rangle+2\sum_{n\in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(s)} \langle\bm{q}_{+}^{(s)},\bm{k}_{n,i}^{(s)}\rangle\right.\] \[+\left(\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n\in S_{+}}\sum_{ i=2}^{M}\alpha_{n,+,i}^{(s)}\bm{k}_{n,i}^{(s)}\right)\] \[\cdot\left(\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)\top}+\sum_{n\in S_{+ }}\sum_{i=2}^{M}\alpha_{n,+,i}^{(s)\top}\bm{k}_{n,i}^{(s)\top}\right)\Big{|}\] \[\leq 2\sum_{s=T_{1}}^{t}|\alpha_{+,+}^{(s)}||\langle\bm{q}_{+}^{ (s)},\bm{k}_{+}^{(s)}\rangle|+2\sum_{n\in S_{+}}\sum_{i=2}^{M}\sum_{s=T_{1}}^ {t}|\alpha_{n,+,i}^{(s)}||\langle\bm{q}_{+}^{(s)},\bm{k}_{n,i}^{(s)}\rangle|\] \[+\{lower\;order\;term\}\] \[=O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}\cdot O(\log( d_{h}^{\frac{1}{2}}))+N\cdot M\cdot O\Big{(}N^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}} \Big{)}\cdot O(\log(d_{h}^{\frac{1}{2}}))\] \[=O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(d_{h}^{\frac{1} {2}})\Big{)}\]

where the first inequality is by triangle inequality, the second inequality is by 199. Since \(\sigma_{h}^{2}\geq\big{(}\max\{\sigma_{p}^{2}d,\|\mu\|_{2}^{2}\}\big{)}^{-1} \cdot d_{h}^{-\frac{1}{2}}(\log(6N^{2}M^{2}/\delta))^{-2}\) and \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2 }\epsilon^{-2}\Big{)}\), we have \(N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(d_{h}^{\frac{1}{2}})=o(\|\bm{\mu}\|_{ 2}^{2}\sigma_{h}^{2}d_{h})\), so \(\|\bm{q}_{+}^{(t+1)}\|_{2}^{2}=\|\bm{q}_{+}^{(T_{1})}\|_{2}^{2}+o(\|\bm{\mu}\|_ {2}^{2}\sigma_{h}^{2}d_{h})=\Theta(\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h})\). Similarly, we have

\[\Big{|}\|\bm{q}_{-}^{(t+1)}\|_{2}^{2}-\|\bm{q}_{-}^{(T_{1})}\|_{2}^{2}\Big{|}=O \Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(d_{h}^{\frac{1}{2}})\Big{)}=o( \|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}),\]

\[\Big{|}\|\bm{k}_{\pm}^{(t+1)}\|_{2}^{2}-\|\bm{k}_{\pm}^{(T_{1})}\|_{2}^{2}\Big{|} =O\Big{(}(1+\mathrm{SNR})N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(d_{h}^{\frac {1}{2}})\Big{)}=o(\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}),\]

\[\Big{|}\|\bm{q}_{n,i}^{(t+1)}\|_{2}^{2}-\|\bm{q}_{n,i}^{(T_{1})}\|_{2}^{2}\Big{|} =O\Big{(}d_{h}^{-\frac{1}{4}}\log(d_{h}^{\frac{1}{2}})\Big{)}=o(\sigma_{p}^{2} \sigma_{h}^{2}dd_{h}),\]

\[\|\bm{k}_{n,i}^{(t+1)}\|_{2}^{2}-\|\bm{k}_{n,i}^{(T_{1})}\|_{2}^{2}\Big{|}=O \Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(d_{h}^{\frac{1}{2}})\Big{)}=o( \sigma_{p}^{2}\sigma_{h}^{2}dd_{h}),\]

so we have

\[\|\bm{q}_{\pm}^{(t+1)}\|_{2}^{2},\|\bm{k}_{\pm}^{(t+1)}\|_{2}^{2}=\Theta(\|\bm{ \mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}),\]

\[\|\bm{q}_{n,i}^{(t+1)}\|_{2}^{2},\|\bm{k}_{n,i}^{(t+1)}\|_{2}^{2}=\Theta( \sigma_{p}^{2}\sigma_{h}^{2}dd_{h})\]

for \(i\in[M]\backslash\{1\},n\in[N]\).

\[|\langle\bm{q}_{+}^{(t+1)},\bm{q}_{-}^{(t+1)}\rangle|\leq|\langle \bm{q}_{+}^{(T_{1})},\bm{q}_{-}^{(T_{1})}\rangle|+\sum_{s=T_{1}}^{t}\Big{|} \langle\bm{q}_{+}^{(s+1)},\bm{q}_{-}^{(s+1)}\rangle-\langle\bm{q}_{+}^{(s)}, \bm{q}_{-}^{(s)}\rangle\Big{|}\] \[\leq|\langle\bm{q}_{+}^{(T_{1})},\bm{q}_{-}^{(T_{1})}\rangle|\] \[+\sum_{s=T_{1}}^{t}\Big{|}\alpha_{+,+}^{(s)}\langle\bm{q}_{-}^{(s )},\bm{k}_{+}^{(s)}\rangle+\sum_{i=5}\sum_{i=2}^{M}\alpha_{n,+,i}^{(s)}\langle \bm{q}_{-}^{(s)},\bm{k}_{n,i}^{(s)}\rangle\] \[+\alpha_{-,-}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{k}_{-}^{(s)}\rangle+ \sum_{n\in S_{-}}\sum_{i=2}^{M}\alpha_{n,-,i}^{(s)}\langle\bm{q}_{+}^{(s)}, \bm{k}_{n,i}^{(s)}\rangle\] \[+\Big{(}\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n\in S_{+}}\sum_{ i=2}^{M}\alpha_{n,+,i}^{(s)}\bm{k}_{n,i}^{(s)}\Big{)}\] \[\cdot\Big{(}\alpha_{-,-}^{(s)}\bm{k}_{-}^{(s)\top}+\sum_{n\in S_{- }}\sum_{i=2}^{M}\alpha_{n,-,i}^{(s)}\bm{k}_{n,i}^{(s)\top}\Big{)}\Big{|}\]\[\leq|\langle\bm{q}_{+}^{(T_{1})},\bm{q}_{-}^{(T_{1})}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\alpha_{+,+}^{(s)}|||\langle\bm{q}_{-}^{(s)}, \bm{k}_{+}^{(s)}\rangle|+\sum_{n\in S_{+}}\sum_{i=2}^{M}\sum_{s=T_{1}}^{t}| \alpha_{n,+,i}^{(s)}|||\langle\bm{q}_{-}^{(s)},\bm{k}_{n,i}^{(s)}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\alpha_{-,-}^{(s)}|||\langle\bm{q}_{+}^{(s)}, \bm{k}_{-}^{(s)}\rangle|+\sum_{n\in S_{-}}\sum_{i=2}^{M}\sum_{s=T_{1}}^{t}| \alpha_{n,-,i}^{(s)}|||\langle\bm{q}_{+}^{(s)},\bm{k}_{n,i}^{(s)}\rangle|\] \[+\{lower\ order\ term\}\] \[\leq|\langle\bm{q}_{+}^{(T_{1})},\bm{q}_{-}^{(T_{1})}\rangle|\] \[+O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}\cdot o(1)+N \cdot M\cdot O\Big{(}N^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}\cdot\log(d_{ h}^{\frac{1}{2}})\] \[=|\langle\bm{q}_{+}^{(T_{1})},\bm{q}_{-}^{(T_{1})}\rangle|+O \Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(d_{h}^{\frac{1}{2}})\Big{)}\] \[=o(1),\]

where the first inequality is triangle inequality, the second inequality is by (198), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{ 2}\epsilon^{-2}\Big{)}\).

\[|\langle\bm{q}_{+}^{(t+1)},\bm{q}_{n,i}^{(t+1)}\rangle|\leq| \langle\bm{q}_{+}^{(T_{1})},\bm{q}_{n,i}^{(T_{1})}\rangle|+\sum_{s=T_{1}}^{t} \Big{|}\langle\bm{q}_{+}^{(s+1)},\bm{q}_{n,i}^{(s+1)}\rangle-\langle\bm{q}_{+} ^{(s)},\bm{q}_{n,i}^{(s)}\rangle\Big{|}\] \[\leq|\langle\bm{q}_{+}^{(T_{1})},\bm{q}_{n,i}^{(T_{1})}\rangle|\] \[+\sum_{s=T_{1}}^{t}\Big{|}\alpha_{+,+}^{(s)}\langle\bm{q}_{n,i}^{ (s)},\bm{k}_{+}^{(s)}\rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\alpha_{ n^{\prime},+,l}^{(s)}\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle\] \[+\alpha_{n,i,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)} \rangle+\alpha_{n,i,-}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{k}_{-}^{(s)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(s)}\langle\bm {q}_{+}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle\] \[+\Big{(}\alpha_{++,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n\in S_{+}}\sum_ {i=2}^{M}\alpha_{n,+,i}^{(s)}\bm{k}_{n,i}^{(s)}\Big{)}\] \[\cdot\Big{(}\alpha_{n,i,+}^{(s)}\bm{k}_{+}^{(s)\top}+\alpha_{n,i,- }^{(s)}\bm{k}_{-}^{(s)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^ {\prime},l}^{(s)\top}\bm{k}_{n^{\prime},l}^{(s)\top}\Big{)}\Big{|}\] \[\leq|\langle\bm{q}_{+}^{(T_{1})},\bm{q}_{n,i}^{(T_{1})}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\alpha_{+,+}^{(s)}||\langle\bm{q}_{n,i}^{(s) },\bm{k}_{+}^{(s)}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\alpha_{n,i,+,l}^ {(s)}||\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l}^{(s)}\rangle|+\sum_{n^{\prime}\in S _{+}\wedge n^{\prime}\neq n}\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\alpha_{n^{\prime },+,l}^{(s)}||\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\alpha_{n,i,+}^{(s)}||\langle\bm{q}_{+}^{(s)}, \bm{k}_{+}^{(s)}\rangle|+\sum_{s=T_{1}}^{t}|\alpha_{n,i,-}^{(s)}||\langle\bm{q}_ {+}^{(s)},\bm{k}_{-}^{(s)}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\alpha_{n, i,n,l}^{(s)}||\langle\bm{q}_{+}^{(s)},\bm{k}_{n,l}^{(s)}\rangle|\] \[+\sum_{n^{\prime}\neq n}\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\alpha_ {n,i,n^{\prime},l}^{(s)}|||\langle\bm{q}_{+}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle|\] \[+\{lower\ order\ term\}\] \[\leq|\langle\bm{q}_{+}^{(T_{1})},\bm{q}_{n,i}^{(T_{1})}\rangle|\] \[+O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}\cdot\log(d_{h} ^{\frac{1}{2}}\Big{)}+M\cdot O\Big{(}N^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)} \cdot\log(d_{h}^{\frac{1}{2}})\] \[+N\cdot M\cdot O\Big{(}N^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)} \cdot o(1)+O\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\cdot\log(d_{h}^{\frac{1}{2}})\] \[+M\cdot O\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\cdot\log(d_{h}^{\frac {1}{2}}\Big{)}+N\cdot M\cdot O\Big{(}d_{h}^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}} \log(6N^{2}M^{2}/\delta)\Big{)}\cdot\log(d_{h}^{\frac{1}{2}})\]\[=|\langle\bm{q}_{+}^{(T_{1})},\bm{q}_{n,i}^{(T_{1})}\rangle|+O\Big{(}N^{ \frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}+O\Big{(}Nd^{-\frac{1}{2}}d_{h}^{-\frac{1 }{4}}\log(6N^{2}M^{2}/\delta)\log(d_{h}^{\frac{1}{2}})\Big{)}\] \[=o(1),\]

where the first inequality is triangle inequality, the second inequality is by (202), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2 }\epsilon^{-2}\Big{)}\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\). Similarly, we have \(|\langle\bm{q}_{-}^{(t+1)},\bm{q}_{n,i}^{(t+1)}\rangle|=o(1)\).

\[|\langle\bm{q}_{n,i}^{(t+1)},\bm{q}_{n,j}^{(t+1)}\rangle|\leq| \langle\bm{q}_{n,i}^{(T_{1})},\bm{q}_{n,j}^{(T_{1})}\rangle|+\sum_{s=T_{1}}^{ t}\Big{|}\langle\bm{q}_{n,i}^{(s+1)},\bm{q}_{n,j}^{(s+1)}\rangle-\langle\bm{q}_{n,i }^{(s)},\bm{q}_{n,j}^{(s)}\rangle\Big{|}\] \[\leq|\langle\bm{q}_{n,i}^{(T_{1})},\bm{q}_{n,j}^{(T_{1})}\rangle|\] \[+\sum_{s=T_{1}}^{t}\Big{|}\alpha_{n,i,+}^{(s)}\langle\bm{q}_{n,j} ^{(s)},\bm{k}_{+}^{(s)}\rangle+\alpha_{n,i,-}^{(s)}\langle\bm{q}_{n,j}^{(s)}, \bm{k}_{-}^{(s)}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{ \prime},l}^{(s)}\langle\bm{q}_{n,j}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle\] \[+\alpha_{n,j,+}^{(s)}\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)} \rangle+\alpha_{n,j,-}^{(s)}\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,j,n^{\prime},l}^{(s)}\langle\bm {q}_{n,i}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle\] \[+\Big{(}\alpha_{n,i,+}^{(s)}\bm{k}_{+}^{(s)}+\alpha_{n,i,-}^{(s)} \bm{k}_{-}^{(s)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime}, l}^{(s)}\bm{k}_{n^{\prime},l}^{(s)}\Big{)}\] \[\cdot\Big{(}\alpha_{n,j,+}^{(s)}\bm{k}_{+}^{(s)\top}+\alpha_{n,j, -}^{(s)}\bm{k}_{-}^{(s)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,j, n^{\prime},l}^{(s)}\bm{k}_{n^{\prime},l}^{(s)\top}\Big{)}\Big{|}\] \[\leq|\langle\bm{q}_{n,i}^{(T_{1})},\bm{q}_{n,j}^{(T_{1})}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\alpha_{n,i,+}^{(s)}||\langle\bm{q}_{n,j}^{(s )},\bm{k}_{+}^{(s)}\rangle|+\sum_{s=T_{1}}^{t}|\alpha_{n,i,-}^{(s)}||\langle\bm {q}_{n,j}^{(s)},\bm{k}_{-}^{(s)}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}| \alpha_{n,i,n,l}^{(s)}||\langle\bm{q}_{n,j}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle|\] \[+\sum_{n^{\prime}\neq n}\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\alpha_{ n,i,n^{\prime},l}^{(s)}||\langle\bm{q}_{n,j}^{(s)},\bm{k}_{n^{\prime},l}^{(s)} \rangle|+\sum_{s=T_{1}}^{t}|\alpha_{n,j,+}^{(s)}||\langle\bm{q}_{n,i}^{(s)},\bm {k}_{+}^{(s)}\rangle|+\sum_{s=T_{1}}^{t}|\alpha_{n,j,-}^{(s)}||\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle|\] \[+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\alpha_{n,j,n,l}^{(s)}||\langle \bm{q}_{n,i}^{(s)},\bm{k}_{n,l}^{(s)}\rangle|+\sum_{n^{\prime}\neq n}\sum_{l=2} ^{M}\sum_{s=T_{1}}^{t}|\alpha_{n,j,n^{\prime},l}^{(s)}||\langle\bm{q}_{n,i}^{(s) },\bm{k}_{n^{\prime},l}^{(s)}\rangle|\] \[+\{lower\ order\ term\}\] \[\leq|\langle\bm{q}_{n,i}^{(T_{1})},\bm{q}_{n,j}^{(T_{1})}\rangle|\] \[+O\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\cdot\log(d_{h}^{\frac{1}{4}} )+M\cdot O\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\cdot\log(d_{h}^{\frac{1}{4}})\] \[+N\cdot M\cdot O\Big{(}d^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6 N^{2}M^{2}/\delta)\Big{)}\cdot o(1)\] \[=|\langle\bm{q}_{n,i}^{(T_{1})},\bm{q}_{n,j}^{(T_{1})}\rangle|+O \Big{(}d_{h}^{-\frac{1}{4}}\log(d_{h}^{\frac{1}{2}})\Big{)}+o\Big{(}Nd^{-\frac{ 1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)}\] \[=o(1)\]

for \(i,j\in[M]\backslash\{1\},i\neq j,n\in[N]\). The first inequality is triangle inequality, the second inequality is by (204), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\).

\[|\langle\bm{q}_{n,i}^{(t+1)},\bm{q}_{\overline{n},j}^{(t+1)}\rangle| \leq|\langle\bm{q}_{n,i}^{(T_{1})},\bm{q}_{\overline{n},j}^{(T_{1})}\rangle|+ \sum_{s=T_{1}}^{t}\Big{|}\langle\bm{q}_{n,i}^{(s+1)},\bm{q}_{\overline{n},j}^{(s+ 1)}\rangle-\langle\bm{q}_{n,i}^{(s)},\bm{q}_{\overline{n},j}^{(s)}\Big{|}\] \[\leq|\langle\bm{q}_{n,i}^{(T_{1})},\bm{q}_{\overline{n},j}^{(T_{1} )}\rangle|\] \[+\sum_{s=T_{1}}^{t}\Big{|}\alpha_{n,i,+}^{(s)}\langle\bm{q}_{ \overline{n},j}^{(s)},\bm{k}_{+}^{(s)}\rangle+\alpha_{n,i,-}^{(s)}\langle\bm{q}_ {\overline{n},j}^{(s)},\bm{k}_{-}^{(s)}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2} ^{M}\alpha_{n,i,n^{\prime},l}^{(s)}\langle\bm{q}_{\overline{n},j}^{(s)},\bm{k}_{n^{ \prime},l}^{(s)}\rangle\]\[\leq|\langle\bm{k}_{+}^{(T_{1})},\bm{k}_{-}^{(T_{1})}\rangle|\] \[+\sum_{s=T_{1}}^{t}\Big{|}\beta_{+,+}^{(s)}\langle\bm{q}_{+}^{(s)}, \bm{k}_{-}^{(s)}\rangle+\sum_{n\in S_{+}}\sum_{i=2}^{M}\beta_{n,+,i}^{(s)} \langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle\] \[+\beta_{-,-}^{(s)}\langle\bm{q}_{-}^{(s)},\bm{k}_{+}^{(s)}\rangle+ \sum_{n\in S_{-}}\sum_{i=2}^{M}\beta_{n,-,i}^{(s)}\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{+}^{(s)}\rangle\] \[+\Big{(}\beta_{+,+}^{(s)}\bm{q}_{+}^{(s)}+\sum_{n\in S_{+}}\sum_{ i=2}^{M}\beta_{n,+,i}^{(s)}\bm{q}_{n,i}^{(s)}\Big{)}\] \[\cdot\Big{(}\beta_{-,-}^{(s)}\bm{q}_{-}^{(s)\top}+\sum_{n\in S_{-} }\sum_{i=2}^{M}\beta_{n,-,i}^{(s)\top}\Big{)}\Big{|}\] \[\leq|\langle\bm{k}_{+}^{(T_{1})},\bm{k}_{-}^{(T_{1})}\rangle|\]\[+\sum_{s=T_{1}}^{t}|\beta^{(s)}_{+,+}||\langle\bm{q}^{(s)}_{+},\bm{k}^ {(s)}_{-}\rangle|+\sum_{n\in S_{+}}\sum_{i=2}^{M}\sum_{s=T_{1}}^{t}|\beta^{(s)}_ {n,+,i}||\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{-}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\beta^{(s)}_{-,-}||\langle\bm{q}^{(s)}_{-}, \bm{k}^{(s)}_{+}\rangle|+\sum_{n\in S_{-}}\sum_{i=2}^{M}\sum_{s=T_{1}}^{t}| \beta^{(s)}_{n,-,i}||\langle\bm{q}^{(s)}_{n,i},\bm{k}^{(s)}_{+}\rangle|\] \[+\{lower\ order\ term\}\] \[=|\langle\bm{k}^{(T_{1})}_{+},\bm{k}^{(T_{1})}_{-}\rangle|\] \[+O\Big{(}N^{\frac{1}{2}}d^{-\frac{1}{4}}_{h}\Big{)}\cdot\log(d^{ \frac{1}{2}}_{h})+N\cdot M\cdot O\Big{(}\text{SNR}\cdot N^{-\frac{1}{2}}d^{- \frac{1}{4}}_{h}\Big{)}\cdot\log(d^{\frac{1}{2}}_{h})\] \[=|\langle\bm{k}^{(T_{1})}_{+},\bm{k}^{(T_{1})}_{-}\rangle|+O \Big{(}\text{SNR}\cdot N^{\frac{1}{2}}d^{-\frac{1}{4}}_{h}\log(d^{\frac{1}{2} }_{h})\Big{)}\] \[=o(1),\]

where the first inequality is triangle inequality, the second inequality is by (214), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\text{SNR}^{4},\text{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\).

\[|\langle\bm{k}^{(t+1)}_{+},\bm{k}^{(t+1)}_{n,i}\rangle|\leq| \langle\bm{k}^{(T_{1})}_{+},\bm{k}^{(T_{1})}_{n,i}\rangle|+\sum_{s=T_{1}}^{t} \Big{|}\langle\bm{k}^{(s+1)}_{+},\bm{k}^{(s+1)}_{n,i}\rangle-\langle\bm{k}^{( s)}_{+},\bm{k}^{(s)}_{n,i}\rangle\Big{|}\] \[\leq|\langle\bm{k}^{(T_{1})}_{+},\bm{k}^{(T_{1})}_{n,i}\rangle|\] \[+\sum_{s=T_{1}}^{t}\Big{|}\beta^{(s)}_{+,+}\langle\bm{q}^{(s)}_{+ },\bm{k}^{(s)}_{n,i}\rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\beta^{(s) }_{n^{\prime},+,l}\langle\bm{q}^{(s)}_{n^{\prime},l},\bm{k}^{(s)}_{n,i}\rangle\] \[+\beta^{(s)}_{n,i,+}\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+} \rangle+\beta^{(s)}_{n,i,-}\langle\bm{q}^{(s)}_{-},\bm{k}^{(s)}_{+}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{n,i,n^{\prime},l}\langle\bm {q}^{(s)}_{n^{\prime},l},\bm{k}^{(s)}_{+}\rangle\] \[+\Big{(}\beta^{(s)}_{+,+}\bm{q}^{(s)}_{+}+\sum_{n^{\prime}\in S_{ +}}\sum_{l=2}^{M}\beta^{(s)}_{n^{\prime},+,l}\bm{q}^{(s)}_{n^{\prime},l}\Big{)}\] \[\cdot\Big{(}\beta^{(s)}_{n,i,+}\bm{q}^{(s)\top}_{+}+\beta^{(s)}_{ n,i,-}\bm{q}^{(s)\top}_{-}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{n,i,n^{ \prime},l}\bm{q}^{(s)\top}_{n^{\prime},l}\Big{)}\Big{|}\] \[\leq|\langle\bm{k}^{(T_{1})}_{+},\bm{k}^{(T_{1})}_{n,i}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\beta^{(s)}_{++}||\langle\bm{q}^{(s)}_{+},\bm {k}^{(s)}_{n,i}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\beta^{(s)}_{n,+,l}|| \langle\bm{q}^{(s)}_{n,l},\bm{k}^{(s)}_{n,i}\rangle|\] \[+\sum_{n^{\prime}\in S_{+}\wedge n^{\prime}\neq n}\sum_{l=2}^{M} \sum_{s=T_{1}}^{t}|\beta^{(s)}_{n^{\prime},+,l}||\langle\bm{q}^{(s)}_{n^{ \prime},l},\bm{k}^{(s)}_{n,i}\rangle|+\sum_{s=T_{1}}^{t}|\beta^{(s)}_{n,i,+}|| \langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\beta^{(s)}_{n,i,-}||\langle\bm{q}^{(s)}_{-}, \bm{k}^{(s)}_{+}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\beta^{(s)}_{n,i,n,l} |||\langle\bm{q}^{(s)}_{n,l},\bm{k}^{(s)}_{+}\rangle|\] \[+\sum_{n^{\prime}\neq n}^{N}\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}| \beta^{(s)}_{n,i,n^{\prime},l}||\langle\bm{q}^{(s)}_{n^{\prime},l},\bm{k}^{(s)}_ {+}\rangle|\] \[+\{lower\ order\ term\}\] \[=|\langle\bm{k}^{(T_{1})}_{+},\bm{k}^{(T_{1})}_{n,i}\rangle|\] \[+O\Big{(}N^{\frac{1}{2}}d^{-\frac{1}{4}}_{h}\Big{)}\cdot\log(d^{ \frac{1}{2}}_{h})+M\cdot O\Big{(}\text{SNR}\cdot N^{-\frac{1}{2}}d^{-\frac{1}{4} }_{h}\Big{)}\cdot\log(d^{\frac{1}{2}}_{h})\] \[+N\cdot M\cdot O\Big{(}\text{SNR}\cdot N^{-\frac{1}{2}}d^{-\frac{1} {4}}_{h}\Big{)}\cdot o(1)+O\Big{(}N^{\frac{1}{2}}d^{-\frac{1}{4}}_{h}\Big{)} \cdot\log(d^{\frac{1}{2}}_{h})\]\[+M\cdot O\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\cdot\log(d_{h}^{\frac{1}{ 2}})+N\cdot M\cdot O\Big{(}d^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/ \delta)\Big{)}\cdot\log(d_{h}^{\frac{1}{2}})\] \[=|\langle\bm{k}_{+}^{(T_{1})},\bm{k}_{n,i}^{(T_{1})}\rangle|+o \Big{(}\mathrm{SNR}\cdot N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(d_{h}^{\frac{ 1}{2}})\Big{)}\] \[+O\Big{(}Nd^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/ \delta)\cdot\log(d_{h}^{\frac{1}{2}})\Big{)}\] \[=o(1)\]

where the first inequality is triangle inequality, the second inequality is by (209), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2 }\epsilon^{-2}\Big{)}\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\). Similarly, we have \(|\langle\bm{k}_{-}^{(t+1)},\bm{k}_{n,i}^{(t+1)}\rangle|=o(1)\).

\[|\langle\bm{k}_{n,i}^{(t+1)},\bm{k}_{n,j}^{(t+1)}\rangle|\leq| \langle\bm{k}_{n,i}^{(T_{1})},\bm{k}_{n,j}^{(T_{1})}\rangle|+\sum_{s=T_{1}}^{ t}\Big{|}\langle\bm{k}_{n,i}^{(s+1)},\bm{k}_{n,j}^{(s+1)}\rangle-\langle\bm{k}_{n,i}^ {(s)},\bm{k}_{n,j}^{(s)}\rangle\Big{|}\] \[\leq|\langle\bm{k}_{n,i}^{(T_{1})},\bm{k}_{n,j}^{(T_{1})}\rangle|\] \[+\sum_{s=T_{1}}^{t}\Big{|}\beta_{n,i,+}^{(s)}\langle\bm{q}_{+}^{( s)},\bm{k}_{n,j}^{(s)}\rangle+\beta_{n,i,-}^{(s)}\langle\bm{q}_{-}^{(s)},\bm{k}_{n, j}^{(s)}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{\prime},l}^{( s)}\langle\bm{q}_{n^{\prime},l}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\] \[+\beta_{n,j,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{k}_{n,i}^{(s)} \rangle+\beta_{n,j,-}^{(s)}\langle\bm{q}_{-}^{(s)},\bm{k}_{n,i}^{(s)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(s)}\langle\bm{ q}_{n^{\prime},l}^{(s)},\bm{k}_{n,i}^{(s)}\rangle\] \[+\Big{(}\beta_{n,i,+}^{(s)}\bm{q}_{+}^{(s)}+\beta_{n,i,-}^{(s)} \bm{q}_{-}^{(s)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{\prime},l}^ {(s)}\bm{q}_{n^{\prime},l}^{(s)}\Big{)}\] \[\cdot\Big{(}\beta_{n,j,+}^{(s)}\bm{q}_{+}^{(s)\top}+\beta_{n,j,-} ^{(s)\top}-\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(s) \top}\Big{)}\Big{|}\] \[\leq|\langle\bm{k}_{n,i}^{(T_{1})},\bm{k}_{n,j}^{(T_{1})}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\beta_{n,i,+}^{(s)}||\langle\bm{q}_{+}^{(s)}, \bm{k}_{n,j}^{(s)}\rangle|+\sum_{s=T_{1}}^{t}|\beta_{n,i,-}^{(s)}||\langle\bm {q}_{-}^{(s)},\bm{k}_{n,j}^{(s)}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}| \beta_{n,i,n,l}^{(s)}||\langle\bm{q}_{n,l}^{(s)},\bm{k}_{n,j}^{(s)}\rangle|\] \[+\sum_{n^{\prime}\neq n}^{N}\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}| \beta_{n,i,n^{\prime},l}^{(s)}||\langle\bm{q}_{n^{\prime},l}^{(s)},\bm{k}_{n,j} ^{(s)}\rangle|+\sum_{s=T_{1}}^{t}|\beta_{n,j,+}^{(s)}||\langle\bm{q}_{+}^{(s)}, \bm{k}_{n,i}^{(s)}\rangle|+\sum_{s=T_{1}}^{t}|\beta_{n,j,-}^{(s)}||\langle\bm{q} _{-}^{(s)},\bm{k}_{n,i}^{(s)}\rangle|\] \[+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\beta_{n,j,n,l}^{(s)}||\langle \bm{q}_{n,l}^{(s)},\bm{k}_{n,i}^{(s)}\rangle|+\sum_{n^{\prime}\neq n}^{N}\sum_{ l=2}^{M}\sum_{s=T_{1}}^{t}|\beta_{n,j,n^{\prime},l}^{(s)}||\langle\bm{q}_{n^{ \prime},l}^{(s)},\bm{k}_{n,i}^{(s)}\rangle|\] \[+\{lower order\ term\}\] \[=|\langle\bm{k}_{n,i}^{(T_{1})},\bm{k}_{n,j}^{(T_{1})}\rangle|\] \[+O\Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\Big{)}\cdot\log(d_{h}^ {\frac{1}{2}})+M\cdot O\Big{(}d_{h}^{-\frac{1}{4}}\Big{)}\cdot\log(d_{h}^{ \frac{1}{2}})\] \[+N\cdot M\cdot O\Big{(}d^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N ^{2}M^{2}/\delta)\Big{)}\cdot o(1)\] \[=|\langle\bm{k}_{n,i}^{(T_{1})},\bm{k}_{n,j}^{(T_{1})}\rangle|+O \Big{(}N^{\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(d_{h}^{\frac{1}{2}})\Big{)}+o \Big{(}Nd^{-\frac{1}{2}}d_{h}^{-\frac{1}{4}}\log(6N^{2}M^{2}/\delta)\Big{)}\] \[=o(1)\]

for \(i,j\in[M]\backslash\{1\},i\neq j,n\in[N]\). The first inequality is triangle inequality, the second inequality is by (211), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\).

\[|\langle\bm{k}_{n,i}^{(t+1)},\bm{k}_{\overline{n},j}^{(t+1)} \rangle|\leq|\langle\bm{k}_{n,i}^{(T_{1})},\bm{k}_{\overline{n},j}^{(T_{1})} \rangle|+\sum_{s=T_{1}}^{t}\Big{|}\langle\bm{k}_{n,i}^{(s+1)},\bm{k}_{ \overline{n},j}^{(s+1)}\rangle-\langle\bm{k}_{n,i}^{(s)},\bm{k}_{\overline{n},j}^{(s)}\rangle \Big{|}\] \[\leq|\langle\bm{k}_{n,i}^{(T_{1})},\bm{k}_{\overline{n},j}^{(T_{1})}\rangle|\]\[+\sum_{s=T_{1}}^{t}|\beta^{(s)}_{n,i,+}\langle\bm{q}^{(s)}_{+},\bm{k}^{( s)}_{\overline{n},j}\rangle+\beta^{(s)}_{n,i,-}\langle\bm{q}^{(s)}_{-},\bm{k}^{( s)}_{\overline{n},j}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{n,i,n^{ \prime},l}\langle\bm{q}^{(s)}_{n^{\prime},l},\bm{k}^{(s)}_{\overline{n},j}\rangle\] \[+\beta^{(s)}_{\overline{n},j,+}\langle\bm{q}^{(s)}_{+},\bm{k}^{( s)}_{n,i}\rangle+\beta^{(s)}_{\overline{n},j,-}\langle\bm{q}^{(s)}_{-},\bm{k}^{( s)}_{n,i}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{\overline{n},j,n^{ \prime},l}\langle\bm{q}^{(s)}_{n^{\prime},l},\bm{k}^{(s)}_{n,i}\rangle\] \[+\Big{(}\beta^{(s)}_{n,i,+}\bm{q}^{(s)}_{+}+\beta^{(s)}_{n,i,-} \bm{q}^{(s)}_{-}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{n,i,n^{ \prime},l}\bm{q}^{(s)}_{n^{\prime},l}\Big{)}\] \[\cdot\Big{(}\beta^{(s)}_{\overline{n},j,+}\bm{q}^{(s)\top}_{+}+ \beta^{(s)}_{\overline{n},j,-}\bm{q}^{(s)\top}_{-}+\sum_{n^{\prime}=1}^{N}\sum _{l=2}^{M}\beta^{(s)}_{\overline{n},j,n^{\prime},l}\bm{q}^{(s)\top}_{n^{\prime },l}\Big{)}\Big{|}\] \[\leq|\langle\bm{k}^{(T_{1})}_{n,i},\bm{k}^{(T_{1})}_{\overline{n},j}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\beta^{(s)}_{n,i,+}||\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{\overline{n},j}\rangle|+\sum_{s=T_{1}}^{t}|\beta^{(s)}_{n,i,-}|| \langle\bm{q}^{(s)}_{-},\bm{k}^{(s)}_{\overline{n},j}\rangle|+\sum_{l=2}^{M} \sum_{s=T_{1}}^{t}|\beta^{(s)}_{n,i,\overline{n},l}||\langle\bm{q}^{(s)}_{ \overline{n},l},\bm{k}^{(s)}_{\overline{n},j}\rangle|\] \[+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\beta^{(s)}_{n,i,n,l}||\langle \bm{q}^{(s)}_{n,l},\bm{k}^{(s)}_{\overline{n},j}\rangle|+\sum_{n^{\prime}\neq n \wedge n^{\prime}\overline{n}}\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\beta^{(s)}_ {n,i,n^{\prime},l}||\langle\bm{q}^{(s)}_{n^{\prime},l},\bm{k}^{(s)}_{\overline {n},j}\rangle|\] \[+\sum_{s=T_{1}}^{t}|\beta^{(s)}_{\overline{n},j,+}||\langle\bm{q} ^{(s)}_{+},\bm{k}^{(s)}_{n,i}\rangle|+\sum_{s=T_{1}}^{t}|\beta^{(s)}_{\overline {n},j,-}||\langle\bm{q}^{(s)}_{-},\bm{k}^{(s)}_{n,i}\rangle|+\sum_{l=2}^{M} \sum_{s=T_{1}}^{t}|\beta^{(s)}_{\overline{n},j,n,l}||\langle\bm{q}^{(s)}_{n,l}, \bm{k}^{(s)}_{n,i}\rangle|\] \[+\sum_{l=2}^{M}\sum_{s=T_{1}}^{t}|\beta^{(s)}_{\overline{n},j, \overline{n},l}||\langle\bm{q}^{(s)}_{\overline{n},l},\bm{k}^{(s)}_{n,i}\rangle| +\sum_{n^{\prime}\neq n\wedge n^{\prime}\overline{n}}\sum_{l=2}^{M}\sum_{s=T_ {1}}^{t}|\beta^{(s)}_{\overline{n},j,n^{\prime},l}||\langle\bm{q}^{(s)}_{n^{ \prime},l},\bm{k}^{(s)}_{n,i}\rangle|\] \[+\{lower order term\}\] \[=|\langle\bm{k}^{(T_{1})}_{n,i},\bm{k}^{(T_{1})}_{\overline{n},j}\rangle|\] \[+O\Big{(}N^{\frac{1}{2}}d^{-\frac{1}{4}}_{h}\Big{)}\cdot\log(d^{ \frac{1}{2}}_{h})+M\cdot O\Big{(}d^{-\frac{1}{2}}d^{-\frac{1}{4}}_{h}\log(6N^ {2}M^{2}/\delta)\Big{)}\cdot\log(d^{\frac{1}{2}}_{h})\] \[+M\cdot O\Big{(}d^{-\frac{1}{4}}_{h}\Big{)}\cdot o(1)+N\cdot M \cdot O\Big{(}d^{-\frac{1}{2}}d^{-\frac{1}{4}}_{h}\log(6N^{2}M^{2}/\delta) \Big{)}\cdot o(1)\] \[=|\langle\bm{k}^{(T_{1})}_{n,i},\bm{k}^{(T_{1})}_{\overline{n},j}\rangle|\] \[+O\Big{(}N^{\frac{1}{2}}d^{-\frac{1}{4}}_{h}\log(d^{\frac{1}{2}}_{ h})\Big{)}+o\Big{(}Nd^{-\frac{1}{2}}d^{-\frac{1}{4}}_{h}\log(6N^{2}M^{2}/\delta) \Big{)}\] \[=o(1)\]

for \(i,j\in[M]\backslash\{1\},n,\overline{n}\in[N],n\neq\overline{n}\). The first inequality is triangle inequality, the second inequality is by (212), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\).

### Upper Bounds of \(\langle\) q, k \(\rangle\)

In order to give the upper bounds for \(\langle\bm{q},\bm{k}\rangle\) in stage III, we need to give the upper bounds of \(\alpha\) and \(\beta\) based on the equations in F.1. The main difference between this subsection and F.6 is that the bounds of \(|V_{\pm}|,|V_{n,i}|\) is \(\log\big{(}O(\frac{1}{\epsilon})\big{)}\) in this subsection, while the bounds of \(|V_{\pm}|,|V_{n,i}|\) is \(\log\big{(}O(\frac{1}{\epsilon})\big{)}\) in F.6, resulting in different bounds for \(\alpha\) and \(\beta\). Now we take \(\alpha^{(s)}_{+,+}\) as an example

\[\alpha^{(s)}_{+,+}=\frac{\eta}{NM}\sum_{n\in S_{+}}-\ell^{\prime(s)}_ {n}\|\bm{\mu}\|^{2}_{2}\] \[\cdot\Big{(}V^{(s)}_{+}(\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{( s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum_{j=2}^{M}\exp( \langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}\]\[-(\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)}{\exp( \langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp( \langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)})^{2})\] \[-\sum\limits_{i=2}^{M}\big{(}V^{(s)}_{n,i}\cdot\frac{\exp(\langle \bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k }^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^ {(s)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,i} \rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{ j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)})\Big{)}\] \[=\frac{\eta}{NM}\sum\limits_{n\in S_{+}}-\ell^{\prime(s)}_{n}\| \bm{\mu}\|_{2}^{2}\frac{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle) }{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}\] \[\cdot\Big{(}V^{(s)}_{+}\cdot\frac{\sum\limits_{j=2}^{M}\exp( \langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}{\exp(\langle\bm{q}^{(s)}_{ +},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}\] \[-\sum\limits_{i=2}^{M}V^{(s)}_{n,i}\cdot\frac{\exp(\langle\bm{q} ^{(s)}_{+},\bm{k}^{(s)}_{n,i}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s )}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_ {n,j}\rangle)}\Big{)}\] \[\leq\frac{\eta}{NM}\sum\limits_{n\in S_{+}}\|\bm{\mu}\|_{2}^{2} \cdot\Big{(}V^{(s)}_{+}\cdot\frac{\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s) }_{+},\bm{k}^{(s)}_{n,j}\rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}\] \[+\max\limits_{i}|V^{(s)}_{n,i}|\cdot\frac{\sum\limits_{j=2}^{M} \exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}{\exp(\langle\bm{q}^{( s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{n,j}\rangle)}\Big{)}\] \[\leq\frac{\eta}{NM}\cdot\frac{3N}{4}\cdot\|\bm{\mu}\|_{2}^{2} \cdot\Big{(}V^{(s)}_{+}\cdot\frac{C}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_ {+}\rangle)}+\max\limits_{i}|V^{(s)}_{n,i}|\cdot\frac{C}{\exp(\langle\bm{q}^{( s)}_{+},\bm{k}^{(s)}_{+}\rangle)}\Big{)}\] \[\leq\frac{\eta C_{0}\|\bm{\mu}\|_{2}^{2}\log\big{(}O(\frac{1}{ \epsilon})\big{)}}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)},\]

where the first inequality is by \(-\ell^{\prime(s)}_{n}\leq 1\) and \(softmax(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)\leq 1\). For the second inequality, we first consider \(\frac{\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j} \rangle)}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)+\sum\limits_{ j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}\leq\frac{\sum \limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)}{\exp( \langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle)}\), then by the monotonicity of \(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle\) and \(\langle\bm{q}^{(T_{1})}_{+},\bm{k}^{(T_{1})}_{n,j}\rangle=o(1)\) we have \(\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j}\rangle)\leq C\) for \(t\in[T_{1},T_{3}]\). The last inequality is by \(V^{(s)}_{+},|V^{(s)}_{n,i}|\leq 2\log\big{(}O(\frac{1}{\epsilon})\big{)}\) for \(t\in[T_{2},T_{3}]\) and absorbing the constant factors. Similar to F.6, we can give the bounds for the other \(\alpha\) and \(\beta\) as follows:

\[\alpha^{(s)}_{-,-}\leq\frac{\eta C_{0}\|\bm{\mu}\|_{2}^{2}\log\big{(}O(\frac{1}{ \epsilon})\big{)}}{\exp(\langle\bm{q}^{(s)}_{-},\bm{k}^{(s)}_{-}\rangle)},\] \[\beta^{(s)}_{+,+}\leq\frac{\eta C_{0}\|\bm{\mu}\|_{2}^{2}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\exp(\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+} \rangle)},\]\[\beta^{(s)}_{-,-}\leq\frac{\eta C_{9}\|\bm{\mu}\|_{2}^{2}\log\left(O(\frac{1}{ \epsilon})\right)}{N}.\]

[MISSING_PAGE_POST]

Similar to F.5, we apply the bounds of \(\alpha\) and \(\beta\) above to give the upper bounds for the dynamics \(\langle\bm{q},\bm{k}\rangle\).

\[\langle\bm{q}_{+}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle-\langle\bm{q}_{+}^{(s )},\bm{k}_{+}^{(s)}\rangle\] \[=\alpha_{+,+}^{(s)}\|\bm{k}_{+}^{(s)}\|^{2}_{2}+\sum_{n\in S_{+}} \sum_{l=2}^{M}\alpha_{n,+,i}^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_{n,i}^{(s)}\rangle\] \[+\beta_{+,+}^{(s)}\|\bm{q}_{+}^{(s)}\|^{2}_{2}+\sum_{n\in S_{+}} \sum_{i=2}^{M}\beta_{n,+,i}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{n,i}^{(s)}\rangle\] \[+\left(\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n\in S_{+}}\sum_ {i=2}^{M}\alpha_{n,+,i}^{(s)}\bm{k}_{n,i}^{(s)}\right)\] (271) \[\cdot\left(\beta_{+,+}^{(s)}\bm{q}_{+}^{(s)\top}+\sum_{n\in S_{+ }}\sum_{i=2}^{M}\beta_{n,+,i}^{(s)\top}\right)\] \[=\alpha_{+,+}^{(s)}\|\bm{k}_{+}^{(s)}\|^{2}_{2}+\beta_{+,+}^{(s) }\|\bm{q}_{+}^{(s)}\|^{2}_{2}+\{lower\ order\ term\}\] \[\leq\frac{2\eta C_{9}\|\bm{\mu}\|^{2}_{2}\log\left(O\frac{(1}{ \epsilon})\right)}{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle)} \cdot\Theta(\|\bm{\mu}\|^{2}_{2}\sigma_{h}^{2}d_{h})+\{lower\ order\ term\}\] \[\leq\frac{\eta C_{10}\|\bm{\mu}\|^{4}_{2}\sigma_{h}^{2}d_{h}\log \left(O(\frac{1}{\epsilon})\right)}{\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{ (s)}\rangle)},\]

similarly, we have

\[\langle\bm{q}_{-}^{(s+1)},\bm{k}_{-}^{(s+1)}\rangle-\langle\bm{q}_{-}^{(s)}, \bm{k}_{-}^{(s)}\rangle\leq\frac{\eta C_{10}\|\bm{\mu}\|^{4}_{2}\sigma_{h}^{2} d_{h}\log\left(O(\frac{1}{\epsilon})\right)}{\exp(\langle\bm{q}_{-}^{(s)},\bm{k}_{-}^ {(s)}\rangle)}.\] (272)

\[\langle\bm{q}_{+}^{(s+1)},\bm{k}_{n,j}^{(s+1)}\rangle-\langle\bm{q }_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\] \[=\alpha_{+,+}^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_{n,j}^{(s)} \rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\alpha_{n^{\prime},+,l}^{(s)} \langle\bm{k}_{n,j}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle\] \[+\beta_{n,j,+}^{(s)}\|\bm{q}_{+}^{(s)}\|^{2}_{2}+\beta_{n,j,-}^{( s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{-}^{(s)}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2} ^{M}\beta_{n,j,n^{\prime},l}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{n^{\prime},l}^{(s)}\rangle\] \[+\left(\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n^{\prime}\in S_{ +}}\sum_{l=2}^{M}\alpha_{n^{\prime},+,l}^{(s)}\bm{k}_{n^{\prime},l}^{(s)}\right)\] \[\cdot\left(\beta_{n,j,+}^{(s)}\bm{q}_{+}^{(s)\top}+\beta_{n,j,-}^ {(s)\top}\bm{q}_{-}^{(s)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,j, n^{\prime},l}^{(s)}\bm{q}_{n^{\prime},l}^{(s)\top}\right)\] (273) \[=\alpha_{n,+,j}^{(s)}\|\bm{k}_{n,j}^{(s)}\|^{2}_{2}+\beta_{n,j,+}^ {(s)}\|\bm{q}_{+}^{(s)}\|^{2}_{2}+\{lower\ order\ term\}\] \[\geq-\frac{\eta C_{9}\|\bm{\mu}\|^{2}_{2}\log\left(O(\frac{1}{ \epsilon})\right)}{N}\cdot\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)} \rangle)\cdot\Theta\Big{(}\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}\Big{)}\] \[-\frac{\eta C_{9}\sigma_{p}^{2}d\log\left(O(\frac{1}{\epsilon}) \right)}{N}\exp(\langle\bm{q}_{+}^{(s)},\bm{k}_{n,j}^{(s)}\rangle)\cdot\Theta \Big{(}\|\bm{\mu}\|^{2}_{2}\sigma_{h}^{2}d_{h}\Big{)}\] \[+\{lower\ order\ term\}\] \[\geq-\frac{\eta C_{10}\sigma_{p}^{2}d\|\bm{\mu}\|^{2}_{2}\sigma_{h} ^{2}d_{h}\log\left(O(\frac{1}{\epsilon})\right)}{N}\cdot\exp(\langle\bm{q}_{+} ^{(s)},\bm{k}_{n,j}^{(s)}\rangle),\]

similarly, we have

\[\langle\bm{q}_{-}^{(s+1)},\bm{k}_{n,j}^{(s+1)}\rangle-\langle\bm{q}_{-}^{(s)}, \bm{k}_{n,j}^{(s)}\rangle\geq-\frac{\eta C_{10}\sigma_{p}^{2}d\|\bm{\mu}\|^{2}_{2} \sigma_{h}^{2}d_{h}\log\left(O(\frac{1}{\epsilon})\right)}{N}\cdot\exp(\langle\bm{q }_{-}^{(s)},\bm{k}_{n,j}^{(s)}\rangle).\] (274)\[\begin{split}&\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{+}^{(s+1)}\rangle- \langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle\\ &=\alpha_{n,i,+}^{(s)}\|\bm{k}_{+}^{(s)}\|_{2}^{2}+\alpha_{n,i,- }^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_{-}^{(s)}\rangle+\sum_{n^{\prime}=1}^{N }\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_ {n^{\prime},l}^{(s)}\rangle\\ &+\beta_{++,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{n,i}^{(s)} \rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(s)} \langle\bm{q}_{n,i}^{(s)},\bm{q}_{n^{\prime},l}^{(s)}\rangle\\ &+\left(\alpha_{n,i,+}^{(s)}\bm{k}_{+}^{(s)}+\alpha_{n,i,-}^{(s) }\bm{k}_{-}^{(s)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(s)}\bm{k}_{n^{\prime},l}^{(s)}\right)\\ &\cdot\left(\beta_{++,+}^{(s)}\bm{q}_{+}^{(s)\top}+\sum_{n^{ \prime}\in S_{+}}\sum_{l=2}^{M}\beta_{n^{\prime},+,l}^{(s)}\bm{q}_{n^{\prime},l}^{(s)\top}\right)\\ &=\alpha_{n,i,+}^{(s)}\|\bm{k}_{+}^{(s)}\|_{2}^{2}+\beta_{n,+,i}^ {(s)}\|\bm{q}_{n,i}^{(s)}\|_{2}^{2}+\{lower\;order\;term\}\\ &\leq\frac{\eta C_{9}\sigma_{p}^{2}d\log\left(O(\frac{1}{\epsilon })\right)}{N\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)}\cdot \Theta\Big{(}\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}\Big{)}\\ &+\frac{\eta C_{9}\|\bm{\mu}\|_{2}^{2}\log\left(O(\frac{1}{ \epsilon})\right)}{N\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle)} \cdot\Theta\Big{(}\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}\Big{)}\\ &+\{lower\;order\;term\}\\ &\leq\frac{\eta C_{10}\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2}\sigma_{ h}^{2}d_{h}\log\left(O(\frac{1}{\epsilon})\right)}{N\exp(\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{+}^{(s)}\rangle)},\end{split}\] (275)

similarly, we have

\[\begin{split}&\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{n,j}^{(s+1)} \rangle-\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle\leq\frac{\eta C_{10 }\sigma_{p}^{2}d\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}\log\left(O(\frac{1}{ \epsilon})\right)}{N\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle)}. \end{split}\] (276)

\[\begin{split}&\langle\bm{q}_{n,i}^{(s+1)},\bm{k}_{n,j}^{(s+1)} \rangle-\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\\ &=\alpha_{n,i,+}^{(s)}\langle\bm{k}_{+}^{(s)},\bm{k}_{n,j}^{(s)} \rangle+\alpha_{n,i,-}^{(s)}\langle\bm{k}_{-}^{(s)},\bm{k}_{n,j}^{(s)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(s)}\langle \bm{k}_{n^{\prime},l}^{(s)},\bm{k}_{n,j}^{(s)}\rangle\\ &+\beta_{n,j,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{q}_{n,i}^{(s)} \rangle+\beta_{n,j,-}^{(s)}\langle\bm{q}_{n,i}^{(s)}\rangle+\sum_{n^{\prime }=1}^{N}\sum_{l=2}^{M}\beta_{n,j,n^{\prime},l}^{(s)}\langle\bm{q}_{n^{\prime},l}^{(s)},\bm{q}_{n,i}^{(s)}\rangle\\ &+\left(\alpha_{n,i,+}^{(s)}\bm{k}_{+}^{(s)}+\alpha_{n,i,-}^{(s) }\bm{k}_{-}^{(s)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime },l}^{(s)}\bm{k}_{n^{\prime},l}^{(s)}\right)\\ &\cdot\left(\beta_{n,j,+}^{(s)\top}\bm{q}_{+}^{(s)\top}+\beta_{n,j,-}^{(s)\top}\bm{q}_{-}^{(s)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M} \beta_{n,j,n^{\prime},l}^{(s)\top}\bm{q}_{n^{\prime},l}^{(s)\top}\right)\\ &=\alpha_{n,i,n,j}^{(s)}\|\bm{k}_{n,j}^{(s)}\|_{2}^{2}+\beta_{n,j, n,i}^{(s)}\|\bm{q}_{n,i}^{(s)}\|_{2}^{2}\\ &+\{lower\;order\;term\}\\ &\geq-\frac{2\eta C_{9}\sigma_{p}^{2}d\log\left(O(\frac{1}{ \epsilon})\right)}{N}\exp(\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,j}^{(s)}\rangle) \cdot\Theta\Big{(}\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}\Big{)}\\ &+\{lower\;order\;term\}\\ &\geq-\frac{\eta C_{10}\sigma_{p}^{4}d^{2}\sigma_{h}^{2}d_{h}\log \left(O(\frac{1}{\epsilon})\right)}{N}\cdot\exp(\langle\bm{q}_{n,i}^{(s)},\bm{ k}_{n,j}^{(s)}\rangle).\end{split}\] (277)

### Bounds for the Sum of \(\alpha\) and \(\beta\)

Assume that the propositions \(\mathcal{F}(T_{2}),\ldots,\mathcal{F}(s)\), \(\mathcal{H}(T_{2}),\ldots,\mathcal{H}(s-1)\) hold (\(s\in[T_{1},t]\)), we have

\[|V_{\pm}^{(s)}|\leq 2\log\big{(}O(\frac{1}{\epsilon})\big{)},\] (278)

\[|V_{n,i}^{(s)}|=O(1),\] (279)

\[\Lambda_{n,\pm,j}^{(s)}\geq\Lambda_{n,\pm,j}^{(T_{2})}\geq\log\Big{(}\exp( \Lambda_{n,\pm,j}^{(T_{1})})+\Theta\Big{(}\frac{d_{h}^{\frac{1}{2}}}{N\big{(} \log(6N^{2}M^{2}/\delta)\big{)}^{3}}\Big{)}\Big{)},\] (280)

\[\Lambda_{n,i,\pm,j}^{(s)}\geq\Lambda_{n,i,\pm,j}^{(T_{2})}\geq\log\Big{(}\exp( \Lambda_{n,i,\pm,j}^{(T_{1})})+\Theta\Big{(}\frac{\sigma_{p}^{2}dd_{h}^{\frac{ 1}{2}}}{N\|\boldsymbol{\mu}\|_{2}^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{ 3}}\Big{)}\Big{)}\] (281)

for \(i,j\in[M]\backslash\{1\},n\in[N],s\in[T_{2},t]\). Similar to (64) and (65), we have

\[\frac{\exp(\langle\boldsymbol{q}_{\pm}^{(s)},\boldsymbol{k}_{n,j}^{(s)} \rangle)}{\exp(\langle\boldsymbol{q}_{\pm}^{(s)},\boldsymbol{k}_{\pm}^{(s)} \rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle\boldsymbol{q}_{\pm}^{(s)}, \boldsymbol{k}_{n,j^{\prime}}^{(s)}\rangle)}=O\Big{(}\frac{N\big{(}\log(6N^{2 }M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)}\] (282)

\[\frac{\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{n,j^{\prime}}^{( s)}\rangle)}{\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{+}^{(s)} \rangle)+\sum\limits_{j^{\prime}=2}^{M}\exp(\langle\boldsymbol{q}_{n,i}^{(s)},\boldsymbol{k}_{n,j^{\prime}}^{(s)}\rangle)}=O\Big{(}\frac{N\|\boldsymbol{ \mu}\|_{2}^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{\sigma_{p}^{2}dd_{ h}^{\frac{1}{2}}}\Big{)}\] (283)

Plugging (282),(283) into the expressions of \(\alpha\), \(\beta\) and letting \(O\Big{(}\log\big{(}O(\frac{1}{\epsilon})\big{)}\Big{)}\) be the upper bound for \(|V_{\pm}^{(s)}|,|V_{n,i}^{(s)}|\) we have

\[|\alpha_{+,+}^{(s)}|=\Big{|}\frac{\eta}{NM}\sum\limits_{n\in S_{+} }-\ell_{n}^{\prime(s)}\|\boldsymbol{\mu}\|_{2}^{2}\] \[\cdot\Big{(}V_{+}^{(s)}\big{(}\frac{\exp(\langle\boldsymbol{q}_{+ }^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{ q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\] \[-(\frac{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^ {(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol {k}_{n,j}^{(s)}\rangle)})\] \[-\sum\limits_{i=2}^{M}\big{(}V_{n,i}^{(s)}\cdot\frac{\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)}{\exp(\langle \boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)}\rangle)+\sum\limits_{j=2}^{M }\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n,j}^{(s)}\rangle)}\] (284) \[\cdot\frac{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{n, i}^{(s)}\rangle)}{\exp(\langle\boldsymbol{q}_{+}^{(s)},\boldsymbol{k}_{+}^{(s)} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\boldsymbol{q}_{+}^{(s)}, \boldsymbol{k}_{n,j}^{(s)}\rangle)}\Big{)}\Big{|}\] \[\leq\frac{\eta\|\boldsymbol{\mu}\|_{2}^{2}}{NM}\cdot\frac{3N}{4} \cdot O\Big{(}\log\big{(}O(\frac{1}{\epsilon})\big{)}\Big{)}\cdot O\Big{(} \frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}}{d_{h}^{\frac{1}{2}}}\Big{)}\] \[=O\Big{(}\frac{\eta N\|\boldsymbol{\mu}\|_{2}^{2}\big{(}\log(6N^{2 }M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{d_{h}^{ \frac{1}{2}}}\Big{)}\]Similarly, we have

\[|\alpha^{(s)}_{-,-}|,|\beta^{(s)}_{+,+}|,|\beta^{(s)}_{-,-}|=O\Big{(}\frac{\eta N \|\boldsymbol{\mu}\|^{2}_{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(} O(\frac{1}{\epsilon})\big{)}}{d_{h}^{\frac{1}{2}}}\Big{)},\] (285)

\[|\alpha^{(s)}_{n,+,i}|,|\alpha^{(s)}_{n,-,i}|=O\Big{(}\frac{\eta\|\boldsymbol{ \mu}\|^{2}_{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{ \epsilon})\big{)}}{d_{h}^{\frac{1}{2}}}\Big{)},\] (286)

\[|\beta^{(s)}_{n,+,i}|,|\beta^{(s)}_{n,-,i}| =O\Big{(}\frac{\eta\|\boldsymbol{\mu}\|^{4}_{2}\big{(}\log(6N^{2} M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\sigma_{p}^{2}d_{h}^{ \frac{1}{2}}}\Big{)}\] (287) \[=O\Big{(}\frac{\eta\|\boldsymbol{\mu}\|^{2}_{2}\cdot\mathrm{SNR} ^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon}) \big{)}}{d_{h}^{\frac{1}{2}}}\Big{)},\]

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[|\alpha^{(s)}_{n,i,+}|,|\alpha^{(s)}_{n,i,-}|=O\Big{(}\frac{\eta\|\boldsymbol{ \mu}\|^{2}_{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{ \epsilon})\big{)}}{d_{h}^{\frac{1}{2}}}\Big{)},\] (288)

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[|\beta^{(s)}_{n,i,+}|,|\beta^{(s)}_{n,i,-}| =O\Big{(}\frac{\eta\sigma_{p}^{2}d\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{d_{h}^{\frac{1}{2}}}\Big{)}\] (289) \[=O\Big{(}\frac{\eta N\|\boldsymbol{\mu}\|^{2}_{2}\big{(}\log(6N^ {2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{d_{h}^{ \frac{1}{2}}}\Big{)},\]

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\), the last equality is by \(N\cdot\mathrm{SNR}^{2}\geq\Omega(1)\).

\[|\alpha^{(s)}_{n,i,n,j}|,|\beta^{(s)}_{n,j,n,i}|=O\Big{(}\frac{\eta\| \boldsymbol{\mu}\|^{2}_{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{d_{h}^{\frac{1}{2}}}\Big{)},\] (290)

for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],n\neq n^{\prime}\). Taking a summation we obtain that

\[\sum_{s=T_{2}}^{t}|\alpha^{(s)}_{+,+}| =O\Big{(}\frac{1}{\eta\epsilon\|\boldsymbol{\mu}\|^{2}_{2}\| \boldsymbol{w}_{O}\|^{2}_{2}}\Big{)}\cdot O\Big{(}\frac{\eta\|\boldsymbol{\mu} \|^{2}_{2}N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{ \epsilon})\big{)}}{d_{h}^{\frac{1}{2}}}\Big{)}\] (292) \[=O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)},\]

where the last equality is by \(\|\boldsymbol{w}_{O}\|=\Theta(1)\). Similarly, we have

\[\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,+,i}|,\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,-,i} |=O\Big{(}\frac{\mathrm{SNR}^{2}\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)},\] (293)

for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\sum_{s=T_{2}}^{t}|\alpha^{(s)}_{-,-}|,\sum_{s=T_{2}}^{t}|\beta^{(s)}_{+,+}|, \sum_{s=T_{2}}^{t}|\beta^{(s)}_{-,-}|,\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,+,i}|, \sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,-,i}|,\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,+}|,\sum_{s=T_{2}}^{s}|\beta^{(s)}_{n,i,-}|\] (294) \[=O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)},\]for \(i\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\begin{split}&\sum_{s=T_{2}}^{t}|\alpha_{n,+,i}^{(s)}|,\,\sum_{s=T_ {2}}^{t}|\alpha_{n,-,i}^{(s)}|,\,\sum_{s=T_{2}}^{t}|\alpha_{n,i,+}^{(s)}|,\, \sum_{s=T_{2}}^{t}|\alpha_{n,i,-}^{(s)}|,\,\sum_{s=T_{2}}^{t}|\alpha_{n,i,n,j} ^{(s)}|,\,\sum_{s=T_{2}}^{t}|\beta_{n,j,n,i}^{(s)}|\\ &=O\Big{(}\frac{\left(\log(6N^{2}M^{2}/\delta)\right)^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)},\end{split}\] (295)

for \(i,j\in[M]\backslash\{1\},n\in S_{\pm}\).

\[\begin{split}&\sum_{s=T_{2}}^{t}|\alpha_{n,i,n^{\prime},j}^{(s)}|, \,\sum_{s=T_{2}}^{t}|\beta_{n,j,n^{\prime},i}^{(s)}|=O\Big{(}\frac{\left(\log (6N^{2}M^{2}/\delta)\right)^{4}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{ \epsilon d_{h}^{\frac{1}{2}}d_{h}^{\frac{1}{2}}}\Big{)}\end{split}\] (296)

for \(i,j\in[M]\backslash\{1\},n,n^{\prime}\in[N],n\neq n^{\prime}\).

With these sums of \(\alpha\) and \(\beta\) above, we can easily prove **Claim** 7 and **Claim** 8.

### Proof of Claim 7

In this subsection, we assume that \(\mathcal{I}(T_{2}),\ldots,\mathcal{I}(t)\) hold, and then proof that \(\mathcal{G}(t+1)\) is true with the result of F.10.

\[\begin{split}&\Big{|}\|\bm{q}_{+}^{(t+1)}\|_{2}^{2}-\|\bm{q}_{+}^{ (t+1)}\|_{2}^{2}\Big{|}\leq\sum_{s=T_{2}}^{t}\Big{|}\|\bm{q}_{+}^{(s+1)}\|_{2} ^{2}-\|\bm{q}_{+}^{(s)}\|_{2}^{2}\Big{|}\\ &\leq\sum_{s=T_{2}}^{t}\Big{|}2\alpha_{++,+}^{(s)}\langle\bm{q}_{ +}^{(s)},\bm{k}_{+}^{(s)}\rangle+2\sum_{n\in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i }^{(s)}\langle\bm{q}_{+}^{(s)},\bm{k}_{n,i}^{(s)}\rangle\\ &+\Big{(}\alpha_{++,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n\in S_{+}} \sum_{i=2}^{M}\alpha_{n,+,i}^{(s)}\bm{k}_{n,i}^{(s)}\Big{)}\\ &\cdot\Big{(}\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)\top}+\sum_{n\in S_ {+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(s)\top}\Big{)}\Big{|}\\ &\leq 2\sum_{s=T_{2}}^{t}|\alpha_{++,+}^{(s)}||\langle\bm{q}_{+}^{(s )},\bm{k}_{+}^{(s)}\rangle|+2\sum_{n\in S_{+}}\sum_{i=2}^{M}\sum_{s=T_{2}}^{t }|\alpha_{n,+,i}^{(s)}||\langle\bm{q}_{+}^{(s)},\bm{k}_{n,i}^{(s)}\rangle|\\ &+\{lower\;order\;term\}\\ &=O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\\ &+N\cdot M\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2 }}}\Big{)}\cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\\ &=O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})}{ \epsilon d_{h}^{\frac{1}{2}}}\Big{)}\end{split}\]

where the first inequality is by triangle inequality, the second inequality is by 199, the third inequality is by \(t\leq T_{3}\). Since \(\sigma_{h}^{2}\geq\big{(}\max\{\sigma_{p}^{2}d,\|\mu\|_{2}^{2}\}\big{)}^{-1} \cdot d_{h}^{-\frac{1}{2}}(\log(6N^{2}M^{2}/\delta))^{-2}\) and \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{ 2}\epsilon^{-2}\Big{)}\), we have \(o(\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h})\), so \(\|\bm{q}_{+}^{(t+1)}\|_{2}^{2}=\|\bm{q}_{+}^{(T_{2})}\|_{2}^{2}+o(\|\bm{\mu}\|_{2} ^{2}\sigma_{h}^{2}d_{h})=\Theta(\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h})\). Similarly, we have

\[\Big{|}\|\bm{q}_{-}^{(t+1)}\|_{2}^{2}-\|\bm{q}_{-}^{(t+1)}\|_{2}^{2}\Big{|}=O \Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{ \epsilon})\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2}}\big{)}}{\epsilon d_{h}^ {\frac{1}{2}}}\Big{)}=o(\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}),\]

\[\Big{|}\|\bm{k}_{\pm}^{(t+1)}\|_{2}^{2}-\|\bm{k}_{\pm}^{(t+1)}\|_{2}^{2}\Big{|} =O\Big{(}\frac{\big{(}1+SNR^{2}\big{)}N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{ 3}\log\big{(}O(\frac{1}{\epsilon})\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2} }\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)}=o(\|\bm{\mu}\|_{2}^{2}\sigma_{h }^{2}d_{h}),\]

\[\Big{|}\|\bm{q}_{n,i}^{(t+1)}\|_{2}^{2}-\|\bm{q}_{n,i}^{(t+1)}\|_{2}^{2}\Big{|} =O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1} {\epsilon})\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2}}\big{)}}{\epsilon d_{h} ^{\frac{1}{2}}}\Big{)}=o(\sigma_{p}^{2}\sigma_{h}^{2}dd_{h}),\]

so we have

\[\|\bm{q}_{\pm}^{(t+1)}\|_{2}^{2},\|\bm{k}_{\pm}^{(t+1)}\|_{2}^{2}=\Theta(\| \bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h}),\]

\[\|\bm{q}_{n,i}^{(t+1)}\|_{2}^{2},\|\bm{k}_{n,i}^{(t+1)}\|_{2}^{2}=\Theta(\sigma _{p}^{2}\sigma_{h}^{2}dd_{h})\]

for \(i\in[M]\backslash\{1\},n\in[N]\).

\[|\langle\bm{q}_{+}^{(t+1)},\bm{q}_{-}^{(t+1)}\rangle|\leq| \langle\bm{q}_{+}^{(T_{2})},\bm{q}_{-}^{(T_{2})}\rangle|+\sum_{s=T_{2}}^{t} \Big{|}\langle\bm{q}_{+}^{(s+1)},\bm{q}_{-}^{(s+1)}\rangle-\langle\bm{q}_{+}^ {(s)},\bm{q}_{-}^{(s)}\rangle\Big{|}\] \[\leq|\langle\bm{q}_{+}^{(T_{2})},\bm{q}_{-}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}\Big{|}\alpha_{+,+}^{(s)}\langle\bm{q}_{-}^{(s )},\bm{k}_{+}^{(s)}\rangle+\sum_{n\in S_{+}}\sum_{i=2}^{M}\alpha_{n,+,i}^{(s)} \langle\bm{q}_{-}^{(s)},\bm{k}_{n,i}^{(s)}\rangle\] \[+\alpha_{-,-}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{k}_{-}^{(s)}\rangle +\sum_{n\in S_{-}}\sum_{i=2}^{M}\alpha_{n,-,i}^{(s)}\langle\bm{q}_{+}^{(s)}, \bm{k}_{n,i}^{(s)}\rangle\] \[+\Big{(}\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n\in S_{+}}\sum_ {i=2}^{M}\alpha_{n,+,i}^{(s)}\bm{k}_{n,i}^{(s)}\Big{)}\] \[\cdot\Big{(}\alpha_{-,-}^{(s)}\bm{k}_{-}^{(s)\top}+\sum_{n\in S_{- }}\sum_{i=2}^{M}\alpha_{n,-,i}^{(s)}\bm{k}_{n,i}^{(s)\top}\Big{)}\Big{|}\] \[\leq|\langle\bm{q}_{+}^{(T_{2})},\bm{q}_{-}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\alpha_{+,+}^{(s)}||\langle\bm{q}_{-}^{(s)}, \bm{k}_{+}^{(s)}\rangle|+\sum_{n\in S_{+}}\sum_{i=2}^{M}\sum_{s=T_{2}}^{t}| \alpha_{n,+,i}^{(s)}||\langle\bm{q}_{-}^{(s)},\bm{k}_{n,i}^{(s)}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\alpha_{-,-}^{(s)}||\langle\bm{q}_{+}^{(s)}, \bm{k}_{-}^{(s)}\rangle|+\sum_{n\in S_{-}}\sum_{i=2}^{M}\sum_{s=T_{2}}^{t}| \alpha_{n,-,i}^{(s)}||\langle\bm{q}_{+}^{(s)},\bm{k}_{n,i}^{(s)}\rangle|\] \[+\{lower order term\}\] \[\leq|\langle\bm{q}_{+}^{(T_{2})},\bm{q}_{-}^{(T_{2})}\rangle|\] \[+O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot o(1)\] \[+N\cdot M\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)} ^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\]\[=|\langle\bm{q}_{+}^{(T_{2})},\bm{q}_{-}^{(T_{2})}\rangle|+O\Big{(} \frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon} )\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})}{\epsilon d_{h}^{\frac{1}{2}}} \Big{)}\] \[=o(1),\]

where the first inequality is triangle inequality, the second inequality is by (198), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2 }\epsilon^{-2}\Big{)}\).

\[\leq|\langle\bm{q}_{+}^{(T_{2})},\bm{q}_{n,i}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}\Big{|}\alpha_{+,+}^{(s)}\langle\bm{q}_{n,i}^ {(s)},\bm{k}_{+}^{(s)}\rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\alpha_{ n^{\prime},+,l}^{(s)}\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle\] \[+\alpha_{n,i,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)} \rangle+\alpha_{n,i,-}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{k}_{-}^{(s)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(s)}\langle \bm{q}_{+}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle\] \[+\Big{(}\alpha_{+,+}^{(s)}\bm{k}_{+}^{(s)}+\sum_{n\in S_{+}}\sum_ {i=2}^{M}\alpha_{n,+,i}^{(s)}\bm{k}_{n,i}^{(s)}\Big{)}\] \[\cdot\Big{(}\alpha_{n,i+}^{(s)}\bm{k}_{+}^{(s)\top}+\alpha_{n,i,- }^{(s)\top}\bm{k}_{-}^{(s)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{ n,i,n^{\prime},l}^{(s)\top}\bm{k}_{n^{\prime},l}^{(s)\top}\Big{)}\Big{|}\] \[\leq|\langle\bm{q}_{+}^{(T_{2})},\bm{q}_{n,i}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\alpha_{n,+,+}^{(s)}||\langle\bm{q}_{n,i}^{( s)},\bm{k}_{+}^{(s)}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\alpha_{n,+,l}^{(s)}|| \langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l}^{(s)}\rangle|+\sum_{n^{\prime}\in S_{+} \wedge n^{\prime}\neq n}\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\alpha_{n^{\prime},+, l}^{(s)}||\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\alpha_{n,i,+}^{(s)}||\langle\bm{q}_{+}^{(s)}, \bm{k}_{+}^{(s)}\rangle|+\sum_{s=T_{2}}^{t}|\alpha_{n,i,-}^{(s)}||\langle\bm{q }_{+}^{(s)},\bm{k}_{-}^{(s)}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\alpha_{ n,i,n,l}^{(s)}||\langle\bm{q}_{+}^{(s)},\bm{k}_{n,l}^{(s)}\rangle|\] \[+\sum_{n^{\prime}\neq n}\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\alpha_{ n,i,n^{\prime},l}^{(s)}||\langle\bm{q}_{+}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle|\] \[+\{lower\ order\ term\}\] \[\leq|\langle\bm{q}_{+}^{(T_{2})},\bm{q}_{n,i}^{(T_{2})}\rangle|\] \[+O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})+M\cdot O\Big{(}\frac{\big{(}\log(6 N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d _{h}^{\frac{1}{2}}}\Big{)}\cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[+N\cdot M\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)} ^{4}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}d_{h} ^{\frac{1}{2}}}\Big{)}\cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[=|\langle\bm{q}_{+}^{(T_{2})},\bm{q}_{n,i}^{(T_{2})}\rangle|+O \Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{ \epsilon})\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})}{\epsilon d_{h}^{\frac {1}{2}}}\Big{)}\] \[+O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{4}\log \big{(}O(\frac{1}{\epsilon})\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})}{ \epsilon d_{h}^{\frac{1}{2}}d_{h}^{\frac{1}{2}}}\Big{)}\]\[=o(1),\]

where the first inequality is triangle inequality, the second inequality is by (202), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\). Similarly, we have \(|\langle\bm{q}_{-}^{(t+1)},\bm{q}_{n,i}^{(t+1)}\rangle|=o(1)\).

\[|\langle\bm{q}_{n,i}^{(t+1)},\bm{q}_{n,j}^{(t+1)}\rangle|\leq| \langle\bm{q}_{n,i}^{(T_{2})},\bm{q}_{n,j}^{(T_{2})}\rangle|+\sum_{s=T_{2}}^{t }\Big{|}\langle\bm{q}_{n,i}^{(s+1)},\bm{q}_{n,j}^{(s+1)}\rangle-\langle\bm{q}_ {n,i}^{(s)},\bm{q}_{n,j}^{(s)}\rangle\Big{|}\] \[\leq|\langle\bm{q}_{n,i}^{(T_{2})},\bm{q}_{n,j}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}\Big{|}\alpha_{n,i,+}^{(s)}\langle\bm{q}_{n,j }^{(s)},\bm{k}_{+}^{(s)}\rangle+\alpha_{n,i,-}^{(s)}\langle\bm{q}_{n,j}^{(s)}, \bm{k}_{-}^{(s)}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{ \prime},l}^{(s)}\langle\bm{q}_{n,j}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle\] \[+\alpha_{n,j,+}^{(s)}\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)} \rangle+\alpha_{n,j,-}^{(s)}\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,j,n^{\prime},l}^{(s)}\langle\bm {q}_{n,i}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle\] \[+\Big{(}\alpha_{n,i,+}^{(s)}\bm{k}_{+}^{(s)}+\alpha_{n,i,-}^{(s) }\bm{k}_{-}^{(s)}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^{(s)}\bm{k}_{n^{\prime},l}^{(s)}\Big{)}\] \[\cdot\Big{(}\alpha_{n,j,+}^{(s)}\bm{k}_{+}^{(s)\top}+\alpha_{n,j, -}^{(s)\top}-\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,j,n^{\prime},l}^{( s)}\bm{k}_{n^{\prime},l}^{(s)\top}\Big{)}\Big{|}\] \[\leq|\langle\bm{q}_{n,i}^{(T_{2})},\bm{q}_{n,j}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\alpha_{n,i,+}^{(s)}||\langle\bm{q}_{n,j}^{( s)},\bm{k}_{+}^{(s)}\rangle|+\sum_{n^{\prime}\neq n}\sum_{l=2}^{M}\sum_{s=T_{2}}^{t }|\alpha_{n,j,n^{\prime},l}^{(s)}||\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n^{\prime },l}^{(s)}\rangle|\] \[+\sum_{n^{\prime}\neq n}\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\alpha_ {n,i,n^{\prime},l}^{(s)}||\langle\bm{q}_{n,j}^{(s)},\bm{k}_{n^{\prime},l}^{(s )}\rangle|+\sum_{s=T_{2}}^{t}|\alpha_{n,j,+}^{(s)}||\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{+}^{(s)}\rangle|+\sum_{s=T_{2}}^{t}|\alpha_{n,j,-}^{(s)}||\langle\bm{ q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle|\] \[+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\alpha_{n,j,n,l}^{(s)}||\langle \bm{q}_{n,i}^{(s)},\bm{k}_{n,l}^{(s)}\rangle|+\sum_{n^{\prime}\neq n}\sum_{l=2 }^{M}\sum_{s=T_{2}}^{t}|\alpha_{n,j,n^{\prime},l}^{(s)}||\langle\bm{q}_{n,i}^{( s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle|\] \[+\{lower order term\}\] \[\leq|\langle\bm{q}_{n,i}^{(T_{2})},\bm{q}_{n,j}^{(T_{2})}\rangle|\] \[+O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[+N\cdot M\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)} ^{4}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d^{\frac{1}{2}}d_{h}^{ \frac{1}{2}}}\Big{)}\cdot o(1)\] \[=|\langle\bm{q}_{n,i}^{(T_{2})},\bm{q}_{n,j}^{(T_{2})}\rangle|+O \Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{ \epsilon})\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})}{\epsilon d_{h}^{ \frac{1}{2}}}\Big{)}\] \[+o\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{4}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d^{\frac{1}{2}}d_{h}^{\frac{1}{2} }}\Big{)}\] \[=o(1)\]for \(i,j\in[M]\backslash\{1\},i\neq j,n\in[N]\). The first inequality is triangle inequality, the second inequality is by (204), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\).

\[|\langle\bm{q}_{n,i}^{(t+1)},\bm{q}_{\bar{n},j}^{(t+1)}\rangle| \leq|\langle\bm{q}_{n,i}^{(T_{2})},\bm{q}_{\bar{n},j}^{(T_{2})}\rangle|+\sum_ {s=T_{2}}^{t}\Big{|}\langle\bm{q}_{n,i}^{(s+1)},\bm{q}_{\bar{n},j}^{(s+1)} \rangle-\langle\bm{q}_{n,i}^{(s)},\bm{q}_{\bar{n},j}^{(s)}\rangle\Big{|}\] \[\leq|\langle\bm{q}_{n,i}^{(T_{2})},\bm{q}_{\bar{n},j}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}\Big{|}\alpha_{n,i,+}^{(s)}\langle\bm{q}_{\bar {n},j}^{(s)},\bm{k}_{+}^{(s)}\rangle+\alpha_{n,i,-}^{(s)}\langle\bm{q}_{\bar {n},j}^{(s)},\bm{k}_{-}^{(s)}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M} \alpha_{n,i,n^{\prime},l}^{(s)}\langle\bm{q}_{\bar{n},j}^{(s)},\bm{k}_{n^{ \prime},l}^{(s)}\rangle\] \[+\alpha_{\bar{n},j+}^{(s)}\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{ (s)}\rangle+\alpha_{\bar{n},j-}^{(s)}\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s) }\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{\bar{n},j,n^{\prime},l}^ {(s)}\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle\] \[+\Big{(}\alpha_{n,i,+}^{(s)}\bm{k}_{+}^{(s)}+\alpha_{n,i,-}^{(s)} \bm{-}_{+}^{(s)}\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\alpha_{n,i,n^{\prime},l}^ {(s)}\bm{k}_{n^{\prime},l}^{(s)}\Big{)}\] \[\cdot\Big{(}\alpha_{\bar{n},j+}^{(s)}\bm{k}_{+}^{(s)\top}+\alpha_ {\bar{n},j,-}^{(s)\top}\bm{-}_{-}^{(s)\top}+\sum_{n^{\prime}=1}^{N}\sum_{l=2} ^{M}\alpha_{\bar{n},j,n^{\prime},l}^{(s)\top}\bm{k}_{n^{\prime},l}^{(s)\top} \Big{)}\Big{|}\] \[\leq|\langle\bm{q}_{n,i}^{(T_{2})},\bm{q}_{\bar{n},j}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\alpha_{n,i,+}^{(s)}||\langle\bm{q}_{\bar{n}, j}^{(s)},\bm{k}_{+}^{(s)}\rangle|+\sum_{s=T_{2}}^{t}|\alpha_{n,i,-}^{(s)}|| \langle\bm{q}_{\bar{n},j}^{(s)},\bm{k}_{-}^{(s)}\rangle|+\sum_{l=2}^{M}\sum_{ s=T_{2}}^{t}|\alpha_{n,i,\overline{n},l}^{(s)}||\langle\bm{q}_{\bar{n},j}^{(s)},\bm{k}_{ \bar{n},l}^{(s)}\rangle|\] \[+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\alpha_{n,i,n,l}^{(s)}||\langle \bm{q}_{\bar{n},j}^{(s)},\bm{k}_{n,l}^{(s)}\rangle|+\sum_{n^{\prime}\neq n \wedge n^{\prime}\neq\overline{n}}\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\alpha_{n, i,n^{\prime},l}^{(s)}||\langle\bm{q}_{\bar{n},j}^{(s)},\bm{k}_{n^{\prime},l}^{(s)}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\alpha_{\bar{n},j,+}^{(s)}||\langle\bm{q}_{n, i}^{(s)},\bm{k}_{+}^{(s)}\rangle|+\sum_{s=T_{2}}^{t}|\alpha_{\bar{n},j,-}^{(s)}|| \langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{2}} ^{t}|\alpha_{\bar{n},j,n,l}^{(s)}||\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n,l}^{( s)}\rangle|\] \[+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\alpha_{\bar{n},j,\overline{n}, l}^{(s)}||\langle\bm{q}_{n,i}^{(s)},\bm{k}_{\overline{n},l}^{(s)}\rangle|+\sum_{n^{ \prime}\neq n\wedge n^{\prime}\neq\overline{n}}\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}| \alpha_{\bar{n},j,n^{\prime},l}^{(s)}||\langle\bm{q}_{n,i}^{(s)},\bm{k}_{n^{ \prime},l}^{(s)}\rangle|\] \[+\{lower order term\}\] \[=|\langle\bm{q}_{n,i}^{(T_{2})},\bm{q}_{\bar{n},j}^{(T_{2})}\rangle|\] \[+O\Big{(}\frac{\left(\log(6N^{2}M^{2}/\delta)\right)^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[+M\cdot O\Big{(}\frac{\left(\log(6N^{2}M^{2}/\delta)\right)^{4} \log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot o(1)+N\cdot M\cdot O\Big{(}\frac{\left(\log(6N^{2}M^{2}/\delta)\right)^{4} \log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}d_{h}^{ \frac{1}{2}}}\Big{)}\cdot o(1)\] \[=|\langle\bm{q}_{n,i}^{(T_{2})},\bm{q}_{\bar{n},j}^{(T_{2})}\rangle| +O\Big{(}\frac{\left(\log(6N^{2}M^{2}/\delta)\right)^{3}\log\big{(}O(\frac{1}{ \epsilon})\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})}{\epsilon d_{h}^{ \frac{1}{2}}}\Big{)}\Big{)}\] \[+o\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{4}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d^{\frac{1}{2}}d_{h}^{\frac{1}{2}}} \Big{)}\] \[=o(1)\]

for \(i,j\in[M]\backslash\{1\},n,\overline{n}\in[N],n\neq\overline{n}\). The first inequality is triangle inequality, the second inequality is by (205), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\) and \(\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\).

\[|\langle\bm{k}_{+}^{(t+1)},\bm{k}_{-}^{(t+1)}\rangle|\leq|\langle\bm{k}_{+}^{( T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|+\sum_{s=T_{2}}^{t}\Big{|}\langle\bm{k}_{+}^{(s+1)}, \bm{k}_{-}^{(s+1)}\rangle-\langle\bm{k}_{+}^{(s)},\bm{k}_{-}^{(s)}\rangle\Big{|}\] \[\leq|\langle\bm{k}_{+}^{(T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}\Big{|}\beta_{++,+}^{(s)}\langle\bm{q}_{+}^{(s )},\bm{k}_{-}^{(s)}\rangle+\sum_{n\in S_{+}}\sum_{i=2}^{M}\beta_{n_{n}+,i}^{(s )}\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle\] \[+\beta_{-,-}^{(s)}\langle\bm{q}_{-}^{(s)},\bm{k}_{+}^{(s)}\rangle +\sum_{n\in S_{-}}\sum_{i=2}^{M}\beta_{n,-,i}^{(s)}\langle\bm{q}_{n,i}^{(s)}, \bm{k}_{+}^{(s)}\rangle\] \[+\Big{(}\beta_{+,+}^{(s)}\bm{q}_{+}^{(s)}+\sum_{n\in S_{+}}\sum_ {i=2}^{M}\beta_{n,+,i}^{(s)}\bm{q}_{n,i}^{(s)}\Big{)}\] \[\cdot\Big{(}\beta_{-,-}^{(s)}\bm{q}_{-}^{(s)\top}+\sum_{n\in S_{- }}\sum_{i=2}^{M}\beta_{n,-,i}^{(s)\top}\Big{)}\Big{|}\] \[\leq|\langle\bm{k}_{+}^{(T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\beta_{+,+}^{(s)}||\langle\bm{q}_{+}^{(s)}, \bm{k}_{-}^{(s)}\rangle|+\sum_{n\in S_{+}}\sum_{i=2}^{M}\sum_{s=T_{2}}^{t}| \beta_{n,+,i}^{(s)}||\langle\bm{q}_{n,i}^{(s)},\bm{k}_{-}^{(s)}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\beta_{-,-}^{(s)}||\langle\bm{q}_{-}^{(s)}, \bm{k}_{+}^{(s)}\rangle|+\sum_{n\in S_{-}}\sum_{i=2}^{M}\sum_{s=T_{2}}^{t}| \beta_{n,-,i}^{(s)}||\langle\bm{q}_{n,i}^{(s)},\bm{k}_{+}^{(s)}\rangle|\] \[+\{lower\;order\;term\}\] \[=|\langle\bm{k}_{+}^{(T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|+O \Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1} {\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)}\cdot\log(\epsilon^{- 1}d_{h}^{\frac{1}{2}})\] \[+N\cdot M\cdot O\Big{(}\frac{\mathrm{SNR}^{2}\big{(}\log(6N^{2}M ^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h }^{\frac{1}{2}}}\Big{)}\cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[=|\langle\bm{k}_{+}^{(T_{2})},\bm{k}_{-}^{(T_{2})}\rangle|+O \Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1} {\epsilon})\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})}{\epsilon d_{h}^{ \frac{1}{2}}}\Big{)}\] \[+O\Big{(}\frac{N\cdot\mathrm{SNR}^{2}\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}\log(\epsilon^{-1}d_{ h}^{\frac{1}{2}})}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)}\] \[=o(1),\]

where the first inequality is triangle inequality, the second inequality is by (214), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\).

\[|\langle\bm{k}_{+}^{(t+1)},\bm{k}_{n,i}^{(t+1)}\rangle|\leq| \langle\bm{k}_{+}^{(T_{2})},\bm{k}_{n,i}^{(T_{2})}\rangle|+\sum_{s=T_{2}}^{t} \Big{|}\langle\bm{k}_{+}^{(s+1)},\bm{k}_{n,i}^{(s+1)}\rangle\Big{|}\] \[\leq|\langle\bm{k}_{+}^{(T_{2})},\bm{k}_{n,i}^{(T_{2})}\rangle|\] \[+\sum_{s=T_{2}}^{t}\Big{|}\beta_{+,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{k}_{n,i}^{(s)}\rangle+\sum_{n^{\prime}\in S_{+}}\sum_{l=2}^{M}\beta_{n^{ \prime},+,l}^{(s)}\langle\bm{q}_{n^{\prime},l}^{(s)},\bm{k}_{n,i}^{(s)}\rangle\] \[+\beta_{n,i,+}^{(s)}\langle\bm{q}_{+}^{(s)},\bm{k}_{+}^{(s)}\rangle +\beta_{n,i,-}^{(s)}\langle\bm{q}_{-}^{(s)},\bm{k}_{+}^{(s)}\rangle+\sum_{n^{ \prime}=1}^{N}\sum_{l=2}^{M}\beta_{n,i,n^{\prime},l}^{(s)}\langle\bm{q}_{n^{ \prime},l}^{(s)},\bm{k}_{+}^{(s)}\rangle\]\[+\Big{(}\beta^{(s)}_{+,+}\bm{q}^{(s)}_{+}+\sum_{n^{\prime}\in S_{+}} \sum_{l=2}^{M}\beta^{(s)}_{n^{\prime},+,l}\bm{q}^{(s)}_{n^{\prime},l}\Big{)}\] \[\cdot\Big{(}\beta^{(s)}_{n,i,+}\bm{q}^{(s)\top}_{+}+\beta^{(s)}_{n,i,-}\bm{q}^{(s)\top}_{-}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{n, i,n^{\prime},l}\bm{q}^{(s)\top}_{n^{\prime},l}\Big{)}\Big{|}\] \[\leq|\langle\bm{k}^{(T_{2})}_{+},\bm{k}^{(T_{2})}_{n,i}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{+,+}||\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{n,i}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,+,l }|||\langle\bm{q}^{(s)}_{n,l},\bm{k}^{(s)}_{n,i}\rangle|\] \[+\sum_{n^{\prime}\in S_{+}\wedge n^{\prime}\neq n}\sum_{l=2}^{M} \sum_{s=T_{2}}^{t}|\beta^{(s)}_{n^{\prime},+,l}|||\langle\bm{q}^{(s)}_{n^{ \prime},l},\bm{k}^{(s)}_{n,i}\rangle|+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,+}||| \langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{+}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,-}||\langle\bm{q}^{(s)}_{-}, \bm{k}^{(s)}_{+}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,n,l }|||\langle\bm{q}^{(s)}_{n,l},\bm{k}^{(s)}_{+}\rangle|\] \[+\sum_{n^{\prime}\neq n}^{N}\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}| \beta^{(s)}_{n,i,n^{\prime},l}|||\langle\bm{q}^{(s)}_{n^{\prime},l},\bm{k}^{( s)}_{+}\rangle|\] \[+\{lower order term\}\] \[=|\langle\bm{k}^{(T_{2})}_{+},\bm{k}^{(T_{2})}_{n,i}\rangle|\] \[+O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[+M\cdot O\Big{(}\frac{\text{SNR}^{2}\big{(}\log(6N^{2}M^{2}/\delta )\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{ 2}}}\Big{)}\cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[+N\cdot M\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{4}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{ 2}}}\Big{)}\cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[+N\cdot M\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta) \big{)}^{4}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d^{\frac{1}{2}}d _{h}^{\frac{1}{2}}}\Big{)}\cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[=|\langle\bm{k}^{(T_{2})}_{+},\bm{k}^{(T_{2})}_{n,i}\rangle|+O \Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log\big{(}O(\frac{1}{ \epsilon})\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})}{\epsilon d_{h}^{\frac{ 1}{2}}}\Big{)}\] \[+o\Big{(}\frac{N\cdot\text{SNR}^{2}\big{(}\log(6N^{2}M^{2}/\delta )\big{)}^{3}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{ 2}}}\Big{)}\Big{)}\] \[=o(1)\]

where the first inequality is triangle inequality, the second inequality is by (209), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\text{SNR}^{4},\text{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\). Similarly, we have \(|\langle\bm{k}^{(t+1)}_{-},\bm{k}^{(t+1)}_{n,i}\rangle|=o(1)\).

\[|\langle\bm{k}^{(t+1)}_{n,i},\bm{k}^{(t+1)}_{n,j}\rangle|\leq| \langle\bm{k}^{(T_{2})}_{n,i},\bm{k}^{(T_{2})}_{n,j}\rangle|+\sum_{s=T_{2}}^{t }\Big{|}\langle\bm{k}^{(s+1)}_{n,i},\bm{k}^{(s+1)}_{n,j}\rangle-\langle\bm{k}^{ (s)}_{n,i},\bm{k}^{(s)}_{n,j}\rangle\Big{|}\] \[\leq|\langle\bm{k}^{(T_{2})}_{n,i},\bm{k}^{(T_{2})}_{n,j}\rangle|\]\[+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,+}\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,j }\rangle+\beta^{(s)}_{n,i,-}\langle\bm{q}^{(s)}_{-},\bm{k}^{(s)}_{n,j}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{n,i,n^{\prime},l}|\langle\bm {q}^{(s)}_{n^{\prime},l},\bm{k}^{(s)}_{n,j}\rangle\] \[+\beta^{(s)}_{n,j,+}\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,i} \rangle+\beta^{(s)}_{n,j,-}\langle\bm{q}^{(s)}_{-},\bm{k}^{(s)}_{n,i}\rangle+ \sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{n,j,n^{\prime},l}\langle\bm {q}^{(s)}_{n^{\prime},l},\bm{k}^{(s)}_{n,i}\rangle\] \[+\Big{(}\beta^{(s)}_{n,i,+}\bm{q}^{(s)}_{+}+\beta^{(s)}_{n,i,-} \bm{q}^{(s)}_{-}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{n,i,n^{ \prime},l}\bm{q}^{(s)}_{n^{\prime},l}\Big{)}\] \[\cdot\Big{(}\beta^{(s)}_{n,j,+}\bm{q}^{(s)\top}_{+}+\beta^{(s)}_{ n,j,-}\bm{q}^{(s)\top}_{-}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{n,j,n^{ \prime},l}\bm{q}^{(s)\top}_{n^{\prime},l}\Big{)}\Big{|}\] \[\leq|\langle\bm{k}^{(T_{2})}_{n,i},\bm{k}^{(T_{2})}_{n,j}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,+}||\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{n,j}\rangle|+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,-}||\langle\bm {q}^{(s)}_{-},\bm{k}^{(s)}_{n,j}\rangle|+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}| \beta^{(s)}_{n,i,n,l}||\langle\bm{q}^{(s)}_{n,l},\bm{k}^{(s)}_{n,j}\rangle|\] \[+\sum_{n^{\prime}\neq n}^{N}\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}| \beta^{(s)}_{n,i,n^{\prime},l}||\langle\bm{q}^{(s)}_{n^{\prime},l},\bm{k}^{(s )}_{n,j}\rangle|+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,j,+}||\langle\bm{q}^{(s)}_{ +},\bm{k}^{(s)}_{n,i}\rangle|+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,j,-}||\langle \bm{q}^{(s)}_{-},\bm{k}^{(s)}_{n,i}\rangle|\] \[+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,j,n,l}||\langle \bm{q}^{(s)}_{n,l},\bm{k}^{(s)}_{n,i}\rangle|+\sum_{n^{\prime}\neq n}^{N}\sum_ {l=2}^{M}\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,j,n^{\prime},l}||\langle\bm{q}^{(s) }_{n^{\prime},l},\bm{k}^{(s)}_{n,i}\rangle|\] \[+\{lower\;order\;term\}\] \[=|\langle\bm{k}^{(T_{2})}_{n,i},\bm{k}^{(T_{2})}_{n,j}\rangle|\] \[+O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[+M\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3} \log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})\] \[+N\cdot M\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)} ^{4}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d^{\frac{1}{2}}d_{h}^{ \frac{1}{2}}}\Big{)}\Big{)}\cdot o(1)\] \[=o(1)\]

for \(i,j\in[M]\backslash\{1\},i\neq j,n\in[N]\). The first inequality is triangle inequality, the second inequality is by (211), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\).

\[|\langle\bm{k}^{(t+1)}_{n,i},\bm{k}^{(t+1)}_{\bar{n},j}\rangle|\leq| \langle\bm{k}^{(T_{2})}_{n,i},\bm{k}^{(T_{2})}_{\bar{n},j}\rangle|+\sum_{s=T_{2 }}^{t}\Big{|}\langle\bm{k}^{(s+1)}_{n,i},\bm{k}^{(s+1)}_{\bar{n},j}\rangle- \langle\bm{k}^{(s)}_{n,i},\bm{k}^{(s)}_{\bar{n},j}\rangle\Big{|}\] \[\leq|\langle\bm{k}^{(T_{2})}_{n,i},\bm{k}^{(T_{2})}_{\bar{n},j}\rangle|\] \[+\sum_{s=T_{2}}^{t}\Big{|}\beta^{(s)}_{n,i,+}\langle\bm{q}^{(s)}_{+ },\bm{k}^{(s)}_{\bar{n},j}\rangle+\beta^{(s)}_{n,i,-}\langle\bm{q}^{(s)}_{-}, \bm{k}^{(s)}_{\bar{n},j}\rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s) }_{n,i,n^{\prime},l}\langle\bm{q}^{(s)}_{n^{\prime},l},\bm{k}^{(s)}_{\bar{n},j}\rangle\] \[+\beta^{(s)}_{\bar{n},j,+}\langle\bm{q}^{(s)}_{+},\bm{k}^{(s)}_{n,i} \rangle+\beta^{(s)}_{\bar{n},j,-}\langle\bm{q}^{(s)}_{-},\bm{k}^{(s)}_{n,i} \rangle+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{\bar{n},j,n^{\prime},l} \langle\bm{q}^{(s)}_{n^{\prime},l},\bm{k}^{(s)}_{n,i}\rangle\]\[+\Big{(}\beta^{(s)}_{n,i,+}\bm{q}^{(s)}_{+}+\beta^{(s)}_{n,i,-}\bm{q}^ {(s)}_{-}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{n,i,n^{\prime},l}\bm {q}^{(s)}_{n^{\prime},l}\Big{)}\] \[\cdot\Big{(}\beta^{(s)}_{n,j,+}\bm{q}^{(s)\top}_{+}+\beta^{(s)}_{ n,j,-}\bm{q}^{(s)\top}_{-}+\sum_{n^{\prime}=1}^{N}\sum_{l=2}^{M}\beta^{(s)}_{ \overline{n},j,n^{\prime},l}\bm{q}^{(s)\top}_{n^{\prime},l}\Big{)}\Big{|}\] \[\leq|\langle\bm{k}^{(T_{2})}_{n,i},\bm{k}^{(T_{2})}_{\overline{n},j}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,+}||\langle\bm{q}^{(s)}_{+}, \bm{k}^{(s)}_{\overline{n},j}\rangle|+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,-}|| \langle\bm{q}^{(s)}_{-},\bm{k}^{(s)}_{\overline{n},j}\rangle|+\sum_{l=2}^{M} \sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,\overline{n},l}||\langle\bm{q}^{(s)}_{ \overline{n},l},\bm{k}^{(s)}_{\overline{n},j}\rangle|\] \[+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,n,l}||\langle \bm{q}^{(s)}_{n,l},\bm{k}^{(s)}_{\overline{n},j}\rangle|+\sum_{n^{\prime}\neq n \wedge n^{\prime}}\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\beta^{(s)}_{n,i,n^{\prime },l}||\langle\bm{q}^{(s)}_{n^{\prime},l},\bm{k}^{(s)}_{\overline{n},j}\rangle|\] \[+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{\overline{n},j,+}||\langle\bm{q} ^{(s)}_{+},\bm{k}^{(s)}_{n,i}\rangle|+\sum_{s=T_{2}}^{t}|\beta^{(s)}_{ \overline{n},j,-}||\langle\bm{q}^{(s)}_{-},\bm{k}^{(s)}_{n,i}\rangle|+\sum_{l= 2}^{M}\sum_{s=T_{2}}^{t}|\beta^{(s)}_{\overline{n},j,n,l}||\langle\bm{q}^{(s)} _{n,l},\bm{k}^{(s)}_{n,i}\rangle|\] \[+\sum_{l=2}^{M}\sum_{s=T_{2}}^{t}|\beta^{(s)}_{\overline{n},j, \overline{n},l}||\langle\bm{q}^{(s)}_{\overline{n},l},\bm{k}^{(s)}_{n,i} \rangle|+\sum_{n^{\prime}\neq n\wedge n^{\prime}}\overline{n}\sum_{l=2}^{M} \sum_{s=T_{2}}^{t}|\beta^{(s)}_{\overline{n},j,n^{\prime},l}||\langle\bm{q}^{( s)}_{n^{\prime},l},\bm{k}^{(s)}_{n,i}\rangle|\] \[+\{lower order term\}\] \[=|\langle\bm{k}^{(T_{2})}_{n,i},\bm{k}^{(T_{2})}_{\overline{n},j}\rangle|\] \[+O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})+M\cdot O\Big{(}\frac{\big{(}\log(6N ^{2}M^{2}/\delta)\big{)}^{4}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d _{h}^{\frac{1}{2}}d_{h}^{\frac{1}{2}}}\Big{)}\cdot\log(\epsilon^{-1}d_{h}^{ \frac{1}{2}})\] \[+M\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3} \log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}}\Big{)} \cdot o(1)+N\cdot M\cdot O\Big{(}\frac{\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{ 4}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{\frac{1}{2}}d_{h}^ {\frac{1}{2}}}\Big{)}\cdot o(1)\] \[=|\langle\bm{k}^{(T_{2})}_{n,i},\bm{k}^{(T_{2})}_{\overline{n},j}\rangle|\] \[+O\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/\delta)\big{)}^{3}\log \big{(}O(\frac{1}{\epsilon})\big{)}\log(\epsilon^{-1}d_{h}^{\frac{1}{2}})}{ \epsilon d_{h}^{\frac{1}{2}}}\Big{)}+o\Big{(}\frac{N\big{(}\log(6N^{2}M^{2}/ \delta)\big{)}^{4}\log\big{(}O(\frac{1}{\epsilon})\big{)}}{\epsilon d_{h}^{ \frac{1}{2}}d_{h}^{\frac{1}{2}}}\Big{)}\] \[=o(1)\]

for \(i,j\in[M]\backslash\{1\},n,\overline{n}\in[N],n\neq\overline{n}\). The first inequality is triangle inequality, the second inequality is by (212), the last equality is by \(d_{h}=\widetilde{\Omega}\Big{(}\max\{\mathrm{SNR}^{4},\mathrm{SNR}^{-4}\}N^{2} \epsilon^{-2}\Big{)}\) and \(d=\widetilde{\Omega}\Big{(}\epsilon^{-2}N^{2}d_{h}\Big{)}\).

#### F.12 Explanations of Lower Order Terms

In this section, we provide some explanations of lower order terms to demonstrate the rigor of our proof.

To bound the so-call \(\{lower order term\}\), we condition that dimensions \(d\), \(d_{h}\) are sufficiently large and learning rate \(\eta\) is sufficiently small. Next, we show how we utilize these three parameters.

Sufficiently mall learning rate \(\eta\) :Recall the dynamics of QK

\[\langle\bm{q}^{(t+1)},\bm{k}^{(t+1)}\rangle-\langle\bm{q}^{(t)},\bm{k}^{(t)} \rangle=\langle\Delta\bm{q}^{(t)},\bm{k}^{(t)}\rangle+\langle\bm{q}^{(t)}, \Delta\bm{k}^{(t)}\rangle+\langle\Delta\bm{q}^{(t)},\Delta\bm{k}^{(t)}\rangle\]

Note that terms \(\langle\Delta\bm{q}^{(t)},\bm{k}^{(t)}\rangle\), \(\langle\bm{q}^{(t)},\Delta\bm{k}^{(t)}\rangle\) contain factor \(\eta\), and term \(\langle\Delta\bm{q}^{(t)},\Delta\bm{k}^{(t)}\rangle\) contains factor \(\eta^{2}\). Therefore, as long as \(\eta\) is sufficiently small, \(\langle\Delta\bm{q}^{(t)},\Delta\bm{k}^{(t)}\rangle\) is sufficiently small than \(\langle\Delta\bm{q}^{(t)},\bm{k}^{(t)}\rangle\) and \(\langle\bm{q}^{(t)},\Delta\bm{k}^{(t)}\rangle\). Now we take the dynamic of \(\langle\bm{q}^{(t)}_{+},\bm{k}^{(t)}_{+}\rangle\) as an example.

\[\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle\bm{q}_{+}^ {(t)},\bm{k}_{+}^{(t)}\rangle\] (297) \[=\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\sum\limits_{n\in S _{+}}\sum\limits_{i=2}^{M}\alpha_{n,+,i}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_ {n,i}^{(t)}\rangle\] \[+\beta_{++,+}^{(t)}\|\bm{q}_{+}^{(t)}\|_{2}^{2}+\sum\limits_{n\in S _{+}}\sum\limits_{i=2}^{M}\beta_{n,+,i}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_ {n,i}^{(t)}\rangle\] \[+\Big{(}\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}+\sum\limits_{n\in S _{+}}\sum\limits_{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\] \[\cdot\Big{(}\beta_{+,+}^{(t)}\bm{q}_{+}^{(t)\top}+\sum\limits_{n \in S_{+}}\sum\limits_{i=2}^{M}\beta_{n,+,i}^{(t)}\bm{q}_{n,i}^{(t)\top}\Big{)},\]

Under benign overfitting regime, we have \(\|\bm{k}_{+}^{(t)}\|_{2}^{2}=\Theta(\|\bm{\mu}\|_{2}^{2}\sigma_{h}^{2}d_{h})\), \(\langle\bm{q}_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\leq\log(\epsilon^{-1}d_{h}^{ \frac{1}{2}})\) and \(\beta_{++}^{(t)}=O(\eta\|\bm{\mu}\|_{2}^{2})\). Therefore, as long as \(\eta=o(\sigma_{h}^{2}d_{h}(\log(\epsilon^{-1}d_{h}^{\frac{1}{2}}))^{-1})\), we have

\[\big{(}\alpha_{+,+}^{(t)}\bm{k}_{+}^{(t)}\big{)}\big{(}\beta_{+,+}^{(t)}\bm{q }_{+}^{(t)\top}\big{)}=o(\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}).\]

Similar method can be used for other items in \(\Big{(}\alpha_{++,+}^{(t)}\bm{k}_{+}^{(t)}+\sum\limits_{n\in S_{+}}\sum\limits _{i=2}^{M}\alpha_{n,+,i}^{(t)}\bm{k}_{n,i}^{(t)}\Big{)}\cdot\Big{(}\beta_{+,+} ^{(t)}\bm{q}_{+}^{(t)\top}+\sum\limits_{n\in S_{+}}\sum\limits_{i=2}^{M}\beta_ {n,+,i}^{(t)}\bm{q}_{n,i}^{(t)\top}\Big{)}\). At last, we have

\[\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle\bm{q }_{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\] (298) \[=\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\sum\limits_{n\in S _{+}}\sum\limits_{i=2}^{M}\alpha_{n,+,i}^{(t)}\langle\bm{k}_{+}^{(t)},\bm{k}_ {n,i}^{(t)}\rangle\] \[+\beta_{++,+}^{(t)}\|\bm{q}_{+}^{(t)}\|_{2}^{2}+\sum\limits_{n\in S _{+}}\sum\limits_{i=2}^{M}\beta_{n,+,i}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_ {n,i}^{(t)}\rangle\] \[+\{lower\ order\ term\}.\]

Sufficiently large dimension \(d_{h}\) :Take (297) as an example, Noting that \(\|\bm{k}_{+}^{(t)}\|_{2}^{2}=O(\eta\|\bm{\mu}\|_{2}^{2})\) and \(\langle\bm{k}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle=o(1)\). Therefore, as long as \(d_{h}\) is sufficiently large, \(\|\bm{k}_{+}^{(t)}\|_{2}^{2}\) is much larger than \(\langle\bm{k}_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle\). Besides, by the property that the sum of each row and column of matrix \((diag(\bm{\varphi}_{n,1}^{(t)})-\bm{\varphi}_{n,1}^{(t)\top}\bm{\varphi}_{n,1}^{ (t)})\) is 0, we have \(\alpha_{+,+}^{(t)}+\sum\limits_{n\in S_{+}}\sum\limits_{i=2}^{M}\alpha_{n,+,i}^ {(t)}=0\). We also prove \(\alpha_{+,+}^{(t)}\geq 0\) and \(\alpha_{n,+,i}^{(t)}\leq 0\) under benign overfitting regime, thus the magnitude of \(\alpha_{n,+,i}^{(t)}\) is smaller than \(\alpha_{+,+}^{(t)}\). All in all, it can be proved that \(\sum\limits_{n\in S_{+}}\sum\limits_{i=2}^{M}\alpha_{n,+,i}^{(t)}\langle\bm{k }_{+}^{(t)},\bm{k}_{n,i}^{(t)}\rangle=o(\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_ {2}^{2})\). This method can be further applied to \(\beta_{++,+}^{(t)}\|\bm{q}_{+}^{(t)}\|_{2}^{2}+\sum\limits_{n\in S_{+}}\sum \limits_{i=2}^{M}\beta_{n,+,i}^{(t)}\langle\bm{q}_{+}^{(t)},\bm{q}_{n,i}^{(t)}\rangle\). At last, we can simplify (297) as follows:

\[\langle\bm{q}_{+}^{(t+1)},\bm{k}_{+}^{(t+1)}\rangle-\langle\bm{q} _{+}^{(t)},\bm{k}_{+}^{(t)}\rangle\] (299) \[=\alpha_{+,+}^{(t)}\|\bm{k}_{+}^{(t)}\|_{2}^{2}+\beta_{+,+}^{(t)}\| \bm{q}_{+}^{(t)}\|_{2}^{2}+\{lower\ order\ term\}.\]

**Sufficiently large dimension \(d\) :** Take \(\alpha^{(t)}_{n^{\prime},i^{\prime},+}\) as an example:

\[\alpha^{(t)}_{n^{\prime},i^{\prime},+}=\frac{\eta}{NM}\sum_{n\in S_{ +}}-\rho^{\prime(t)}_{n}\sum_{i=2}^{M}\langle\bm{\xi}_{n^{\prime},i^{\prime}}, \bm{\xi}_{n,i}\rangle\] (299) \[\cdot\Big{(}V^{(t)}_{+}\big{(}\frac{\exp(\langle\bm{q}^{(t)}_{n,i },\bm{k}^{(t)}_{+}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{+} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,j }\rangle)}\] \[-(\frac{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{+}\rangle)}{ \exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{+}\rangle)+\sum\limits_{j=2}^{M} \exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,j}\rangle)})^{2}\Big{)}\] \[-\sum\limits_{k=2}^{M}\big{(}V^{(t)}_{n,i}\cdot\frac{\exp(\langle \bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{+}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i}, \bm{k}^{(t)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i}, \bm{k}^{(t)}_{n,j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,k} \rangle)}{\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{+}\rangle)+\sum\limits _{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n,i},\bm{k}^{(t)}_{n,j}\rangle)})\Big{)},\]

Note that \(\langle\bm{\xi}_{n^{\prime},i^{\prime}},\bm{\xi}_{n,i}\rangle\) can be divided into two types: \(\|\bm{\xi}_{n^{\prime},i^{\prime}}\|_{2}^{2}\) and \(\langle\bm{\xi}_{n^{\prime},i^{\prime}},\bm{\xi}_{n,i}\rangle\) for \(i\neq i^{\prime}\ or\ n\neq n^{\prime}\).

By Lemma C.4, we have

\[\tilde{\sigma}_{p}^{2}d/2 \leq\|\bm{\xi}_{n,2}\|_{2}^{2}\leq 3\tilde{\sigma}_{p}^{2}d/2,\] \[\sigma_{p}^{2}d/2 \leq\|\bm{\xi}_{n,i}\|_{2}^{2}\leq 3\sigma_{p}^{2}d/2,\] \[|\langle\bm{\xi}_{n,i},\bm{\xi}_{n^{\prime},i^{\prime}}\rangle| \leq 2\tilde{\sigma}_{p}^{2}\cdot\sqrt{d\log(4N^{2}M^{2}/\delta)}\]

for \(i,i^{\prime}\in[M]\backslash\{1\},n,n^{\prime}\in[N],i\neq i^{\prime}\ or\ n\neq n^{\prime}\).

As long as \(d\) is sufficiently large, \(\|\bm{\xi}_{n^{\prime},i^{\prime}}\|_{2}^{2}\) is much larger than \(\langle\bm{\xi}_{n^{\prime},i^{\prime}},\bm{\xi}_{n,i}\rangle\) for \(i\neq i^{\prime}\ or\ n\neq n^{\prime}\). Therefore, (299) can be further simplified as follows:

\[\alpha^{(t)}_{n^{\prime},i^{\prime},+}=-\frac{\eta}{NM}\ell^{ \prime(t)}_{n^{\prime}}\|\bm{\xi}_{n^{\prime},i^{\prime}}\|_{2}^{2}\] (300) \[\cdot\Big{(}V^{(t)}_{+}\big{(}\frac{\exp(\langle\bm{q}^{(t)}_{n^{ \prime},i^{\prime}},\bm{k}^{(t)}_{+}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n^{ \prime},i^{\prime}},\bm{k}^{(t)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp( \langle\bm{q}^{(t)}_{n^{\prime},i^{\prime}},\bm{k}^{(t)}_{n^{\prime},j}\rangle )}\] \[-(\frac{\exp(\langle\bm{q}^{(t)}_{n^{\prime},i^{\prime}},\bm{k}^{( t)}_{+}\rangle)}{\exp(\langle\bm{q}^{(t)}_{n^{\prime},i^{\prime}},\bm{k}^{(t)}_{+} \rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n^{\prime},i^{\prime} },\bm{k}^{(t)}_{n^{\prime},j}\rangle)})^{2}\Big{)}\] \[-\sum\limits_{k=2}^{M}\big{(}V^{(t)}_{n^{\prime},i^{\prime}}\cdot \frac{\exp(\langle\bm{q}^{(t)}_{n^{\prime},i^{\prime}},\bm{k}^{(t)}_{+}\rangle) }{\exp(\langle\bm{q}^{(t)}_{n^{\prime},i^{\prime}},\bm{k}^{(t)}_{+}\rangle)+ \sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{n^{\prime},i^{\prime}},\bm{k}^{( t)}_{n^{\prime},j}\rangle)}\] \[\cdot\frac{\exp(\langle\bm{q}^{(t)}_{n^{\prime},i^{\prime}},\bm{ k}^{(t)}_{n^{\prime},k}\rangle)}{\exp(\langle\langle\bm{q}^{(t)}_{n^{\prime},i^{ \prime}},\bm{k}^{(t)}_{+}\rangle)+\sum\limits_{j=2}^{M}\exp(\langle\bm{q}^{(t)}_{ n^{\prime},i^{\prime}},\bm{k}^{(t)}_{n^{\prime},j}\rangle)})\Big{)}\] \[+\{lower order term\}.\]

## Appendix G Takeaways for Practitioners

Our theoretical results mainly focuse on the impact of different \(N\) and SNR on the generalization performance. So we can provide guidance from the perspective of increasing \(N\), SNR and \(N\cdot\mathrm{SNR}^{2}\). The following are some practical scenarios

**Data Augmentation:** Researchers sometimes employ the technique of data augmentation by introducing controlled noise into their datasets. From the perspective of our paper's results, this method reduce SNR but improve N because we generate "new" data point by adding noises. As reducing SNR may be harmful to generalization performance, we must make sure that we use enough data points to train the model( enough sample size \(N\) ).

**Semi-Supervised Learning:** Semi-supervised learning is useful when you have a small amount of labeled data and a large amount of unlabeled data. Labeled data can be seen as data with high SNR, while unlabeled data with low SNR because for some unlabeled samples, we may mistake their labels, making them equivalent to noises. In this scenario, we need to ensure that we have sufficient unlabeled data (enough sample size \(N\)) and make full use of labeled data (high SNR data points).

Overall, we need to consider both the sample size \(N\) and the signal-to-noise ratio SNR to train the model.

## Appendix H Broader Impacts

This work focus on theoretically studying the training dynamics and generalization of Transformer in Vision. The techniques used in this paper may be generalized to study other abilities of Transformer or other network models. Besides, the theoretical results in this paper may inspire more attempts at training large foundational models with high-quality data. We do not foresee any form of negative social impact induced by our work.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract reflects the paper's scope, and the introduction reflects the paper's contributions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss our limitation in the conclusion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide the assumptions in Condition 4.1. The detailed proofs are in the appendix. We also provide a proof sketch in the main paper.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide the experiments setting in this paper, and the codes are submitted as supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The codes are submitted as supplementary material, which include a readme file. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide detailed data generation methods and hyperparameters in the experiments setting. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: we plot the 2-sigma error bar for the training dynamics in Appendix A Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the information on the computer resources in experiments setting. All the experiments in this paper can be performed within hours. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This work focuses on theoretical study of ViT, and all the data is synthesized. We see no ethical or immediate negative societal consequence of our work, and it will not violate the Code Of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the societal impacts in Appendix H. There are no negative social impacts in this paper. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: All the data in this paper is synthesized. This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: All the data in this paper is synthesized. This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: All the data in this paper is synthesized. This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.