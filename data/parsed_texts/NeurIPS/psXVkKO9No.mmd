# Self-Predictive Universal AI

 Elliot Catt, Jordi Grau Moya, Marcus Hutter, Matthew Aitchison

Tim Genewein, Gregoire Deletang, Kevin Li Wenliang, Joel Veness

Google DeepMind

ecatt@google.com

###### Abstract

Reinforcement Learning (RL) algorithms typically utilize learning and/or planning techniques to derive effective policies. Integrating both approaches has proven to be highly successful in addressing complex sequential decision-making challenges, as evidenced by algorithms such as AlphaZero and MuZero, which consolidate the planning process into a parametric search-policy. AIXI, the universal Bayes-optimal agent, leverages planning through comprehensive search as its primary means to find an optimal policy. Here we define an alternative universal Bayesian agent, which we call Self-AIXI, that on the contrary to AIXI, maximally exploits learning to obtain good policies. It does so by _self-predicting_ its own stream of action data, which is generated, similarly to other TD(0) agents, by taking an action maximization step over the current on-policy (universal mixture-policy) Q-value estimates. We prove that Self-AIXI converges to AIXI, and inherits a series of properties like maximal Legg-Hutter intelligence and the self-optimizing property.

## 1 Introduction

Reinforcement Learning (RL) [1] algorithms exploit learning, planning 1, or their combination, to obtain good policies from experience. Pure learning consists of using real experience for improving a policy via a (parametric) model, possibly representing an explicit policy-model and/or the Q-values [2; 3; 4; 5; 6]. In a sense, learning stores the computational effort of policy-improvement into the parameters, which makes it a computationally efficient approach when needing to reuse the policy later on. In contrast, pure planning finds good policies via simulated experience using an environment model and a randomized (or exhaustive) search policy [1; 7; 8]. In the case of unknown or stochastic environments, one must re-plan after receiving a new observation, thus wasting all computational-effort from the previous step. This makes pure planning a wasteful approach. Using both, planning and learning, is a good way to improve performance and efficiency as demonstrated by modern high-performant RL algorithms such as MuZero [9; 10; 11; 12]. These algorithms distill the planning effort back into the parametric search-policy by training it to predict the good actions obtained from planning. In a way, these agents are _self-predicting_ their own policy-improvements. Although empirically successful and widely used, this distillation [13] or self-prediction 2 process is motivated in a purely heuristic way without much theoretical understanding on its optimality condition.

Footnote 1: We use the terms planning and search interchangeably.

Footnote 2: Policy distillation usually refers to the process of amortizing one or several policies into another policy model. We view self-prediction as a type of distillation where a search-policy is consolidated into another model.

The AIXI agent [14; 15] is a theoretical universal Bayes-optimal agent obtained through pure planning without relying on distilling the search effort as described above. AIXI learns an environment model via a Solomonoff predictor [16; 17] and uses it for exhaustive (computationally intractable) planning. Thus, although it uses learning for the environment model, we say AIXI adopts a pure planning approach in the context of policy generation. Two desirable properties of Solomonoff prediction are universality--obtained by considering a huge hypothesis class containing all computableenvironments--and fast convergence to the true environment statistics [18; 19]. Unfortunately, universality makes Solomonoff prediction incomputable, a property that AIXI also inherits. Nevertheless, the AIXI agent is considered as the most powerful agent and the gold-standard in decision-making under unknown general environments. An existing gap in the literature, however, is the lack of an alternative universal agent that maximally exploits learning and distillation, instead of the more wasteful planning approach used by AIXI. Understanding this distillation process from a theoretical standpoint can inspire taking new directions in approximating the AIXI agent as done in [20], which is based on a pure planning approach.

The central aim of this paper is to define an agent, which we name Self-AIXI, that maximally exploits _self-prediction_ (or distillation) instead of planning, and to prove that this agent is optimal in the sense that converges to the gold-standard AIXI agent. We define self-prediction as the process of predicting the stream of action-data generated by the agent itself. Self-AIXI generates action-data in a similar fashion as other Temporal Difference TD(0) [1] algorithms in which there is only a single "planning" step when choosing an action i.e. \(\operatorname*{arg\,max}_{a_{t}}\hat{Q}(h_{t},a_{t})\) for history \(h_{t}\), action \(a_{t}\) and current Q-value estimates \(\hat{Q}\). The difference lies in that Self-AIXI does exact Bayesian inference on the policy space by holding a universal mixture \(\zeta\) over policies (in addition to \(\xi\), the universal mixture over environments), and consumes action-data generated by maximizing the Self-AIXI policy i.e. \(\operatorname*{arg\,max}_{a}Q_{\xi}^{\zeta}(h_{t},a)\). In summary our contributions are:

* We define Self-AIXI, a novel universal agent based on self-prediction.
* We prove that Self-AIXI's Q-values converge to AIXI's Q-values asymptotically.
* We show how Self-AIXI leads to maximum Legg-Hutter [21] intelligence and inherits the self-optimization [22] property.

These results provide compelling evidence that self-prediction can effectively serve as a robust alternative to traditional planning methods.

## 2 Background

### General Reinforcement Learning

Let \(\mathcal{A},\mathcal{O},\mathcal{R}\) denote the (finite) action, observation and reward set respectively. Let the set of percepts be defined as \(\mathcal{E}:=\mathcal{O}\times\mathcal{R}\). \(\Delta\mathcal{X}\) denotes a distribution over \(\mathcal{X}\). Define the set of histories \(\mathcal{H}:=(\mathcal{A}\times\mathcal{E})^{*}\). A policy \(\pi:\mathcal{H}\rightarrow\Delta\mathcal{A}\) is a distribution over actions given a history. An environment \(\mu:\mathcal{H}\times\mathcal{A}\rightarrow\Delta\mathcal{E}\) is a distribution over percepts given history and action. We use \(h_{<t}:=a_{1}e_{1}\ldots a_{t-1}e_{t-1}=a_{1}o_{1}r_{1}\ldots a_{t-1}o_{t-1} r_{t-1}\) to denote the history of interactions up to time \(t-1\). Given an environment \(\mu\) and policy \(\pi\) we use \(\mu^{\pi}\) to denote the induced probability measure on histories, i.e. \(h_{<t}\) is assumed to be sampled from \(\mu^{\pi}\). We are interested in how well agents (policies) perform. We measure this performance by the future expected sum of discounted rewards, called value function.

**Definition 1** (Value function and optimal policy).: _The value \(V_{\nu}^{\pi}:\mathcal{H}\rightarrow[0,1]\) of a policy \(\pi\) in an environment \(\nu\), and discount factor \(\gamma\) given a history \(h_{<t}\in\mathcal{H}\) is defined as_

\[V_{\nu}^{\pi}(h_{<t}):=\frac{1}{\Gamma_{t}}\mathbb{E}_{\nu}^{\pi}\left[\sum_ {k=t}^{\infty}\gamma^{k-t}r_{k}\bigg{|}h_{<t}\right]\]

_Where \(\Gamma_{t}=\sum_{k=t}^{\infty}\gamma^{k-t}=\frac{1}{1-\gamma}\) is the normalization factor of the discount. The optimal value is defined as \(V_{\nu}^{*}(h_{<t}):=\operatorname*{sup}_{\pi}V_{\nu}^{\pi}(h_{<t})\), the set of optimal policies with respect to that value is defined as \(\operatorname*{arg\,max}_{\pi}V_{\nu}^{\pi}(h_{<t})\), and an optimal policy with respect to the value is defined as \(\pi_{\nu}^{*}(\cdot|h_{<t})\in\operatorname*{arg\,max}_{\pi}V_{\nu}^{\pi}(h_ {<t})\)._

We can similarly define the action value or Q-value function as \(Q_{\nu}^{\pi}(h_{<t},a_{t}):=V_{\nu}^{\pi}(h_{<t}a_{t})\), which allows to write the pseudo-recursive Bellman forms

\[V_{\nu}^{\pi}(h_{<t})=\sum_{a_{t}\in\mathcal{A}}\pi(a_{t}|h_{<t})Q_{\nu}^{\pi }(h_{<t},a_{t}),\;Q_{\nu}^{\pi}(h_{<t},a_{t})=\sum_{e_{t}\in\mathcal{E}}\nu(e _{t}|h_{<t}a_{t})\left(r_{t}+\gamma V_{\nu}^{\pi}(h_{1:t})\right)\] (1)We use throughout the paper the Total-Variation distance as a convenient way to compare two policies.

**Definition 2** (TV distance).: _We define the Total Variation (TV) distance of two measures \(\nu_{1}^{\pi_{1}},\nu_{2}^{\pi_{2}}\) between timesteps \(t\) and \(t\leq m\leq\infty\) as_

\[D_{m}(\nu_{1}^{\pi_{1}},\nu_{2}^{\pi_{2}}|h_{<t}):=\sup_{H\subseteq(\mathcal{A }\times\mathcal{E})^{m}}|\nu_{1}^{\pi_{1}}(H|h_{<t})-\nu_{2}^{\pi_{2}}(H|h_{<t})| \leq 1\]

The following lemma, useful for our later proofs, shows that TV distance is an upper bound for value function difference in general reinforcement learning.

**Lemma 3** ([23, Lemma 4.17]).: _For any two policies \(\pi_{1},\pi_{2}\) and two environments \(\nu_{1},\nu_{2}\)_

\[|V_{\nu_{1}}^{\pi_{1}}(h_{<t})-V_{\nu_{2}}^{\pi_{2}}(h_{<t})|\leq D_{\infty}( \nu_{1}^{\pi_{1}},\nu_{2}^{\pi_{2}}|h_{<t})\]

### Universal Artificial Intelligence

We achieve universality by considering a Bayesian mixture over a class of potential environments \(\mathcal{M}\) which is large enough so as to include the true environment the agent is interacting with. A typical choice is the class of all computable functions.

**Definition 4** (Bayesian mixture environment).: _The Bayesian mixture over the class of environments \(\mathcal{M}\) is defined as_

\[\xi(e_{t}|h_{<t}a_{t}):=\sum_{\nu\in\mathcal{M}}w(\nu|h_{<t})\nu(e_{t}|h_{<t} a_{t})\quad\text{with posterior}\quad w(\nu|h_{1:t}):=w(\nu|h_{<t})\frac{\nu(e_{t}|h_{<t}a_{t})}{\xi(e_{t}|h_{<t} a_{t})}\]

_and \(w(\nu|\epsilon):=w(\nu)\) is the prior probability of \(\nu\)._

Given this environment mixture \(\xi\), we define AI\(\xi\) (AIXI) as acting optimally with respect to \(\xi\).

**Definition 5** (Al\(\xi\)).: _AIXI, the optimal Bayesian agent is defined as_

\[\pi_{\xi}^{\star}(h_{<t}):=\operatorname*{arg\,max}_{a}Q_{\xi}^{\star}(h_{<t},a)\]

The following results prove that the AIXI agent achieves maximal intelligence in the Legg-Hutter sense and is self-optimizing. These are important to have in mind throughout the paper since we will prove the same type of results for our Self-AIXI agent.

Legg-Hutter Intelligence measures the performance of an agent \(\pi\) in a wide range of environments \(\mathcal{M}\), weighted by their p(oste)prior plausibility \(w(\cdot)\):

**Definition 6** (LH Intelligence [21]).: _The Legg-Hutter Intelligence measure \(\Upsilon\) of a policy \(\pi\) given a history \(h_{<t}\) is defined as_

\[\Upsilon(\pi|h_{<t}):=\sum_{\nu\in\mathcal{M}}w(\nu|h_{<t})V_{\nu}^{\pi}(h_{< t})=V_{\xi}^{\pi}(h_{<t})\]

It has been shown that if \(\mathcal{M}\) is such that there exists a policy (sequence) which is able to achieve strong asymptotic optimality then AIXI can achieve this as well. This property is known as self-optimizing.

**Theorem 7** (Alxi is Self-optimizing [14]).: _Let \(\mu\) be some environment. If there is a policy \(\pi\) and a sequence of policies \(\overline{\pi_{1}},\overline{\pi_{2}}\ldots\) such that for all \(\nu\in\mathcal{M}\)_

\[V_{\nu}^{\star}(h_{<t})-V_{\nu}^{\overline{\pi_{1}}}(h_{<t})\to 0\ \ \text{as}\ \ \ t\to\infty\ \ \mu^{\pi}\text{-almost surely}\] (2)

_then_

\[V_{\nu}^{\star}(h_{<t})-V_{\nu}^{\pi_{\xi}^{\star}}(h_{<t})\to 0\ \ \text{as}\ \ \ t\to\infty\ \ \mu^{\pi}\text{-almost surely}\]

_If \(\pi=\pi_{\xi}^{\star}\) and Equation 2 holds for all \(\mu\in\mathcal{M}\), then \(\pi_{\xi}^{\star}\) is strongly asymptotically optimal in the class \(\mathcal{M}\)._

For a taxonomy of classes that do (not) allow for self-optimizing policies, see [21, Figure 3.1]. A challenge that is difficult to overcome is that for geometric discounting, there may not yet be widely-recognized classes of environments that allow self-optimizing policies.

Self-prediction in General Reinforcement Learning

We are ready to define our Self-AIXI agent that is based on _self-prediction_ instead of pure planning. We start with our definition of the mixture policy \(\zeta\) and discuss the choices of model class and prior.

**Definition 8** (Bayesian mixture policy).: _The Bayesian mixture over the class of policies \(\mathcal{P}\) is defined as_

\[\zeta(a_{t}|h_{<t}):=\sum_{\pi\in\mathcal{P}}\omega(\pi|h_{<t})\pi(a_{t}|h_{<t} )\quad\text{with posterior}\quad\omega(\pi|h_{1:t}):=\omega(\pi|h_{<t})\frac{ \pi(a_{t}|h_{<t})}{\zeta(a_{t}|h_{<t})}\]

_and \(\omega(\pi|\epsilon):=\omega(\pi)\) is the prior probability of \(\pi\). Note \(\omega(\cdot)\) is a prior for the mixture policy, while \(w(\cdot)\) was a prior for the mixture environment._

**Lemma 9** (Linearity of Q-values).: _For all \(\mu\), \(\pi\), \(h_{<t}\) and \(a_{t}\) we have_

\[Q^{\pi}_{\xi}(h_{<t},a_{t})=\sum_{\nu\in\mathcal{M}}w(\nu|h_{<t})Q^{\pi}_{\nu} (h_{<t},a_{t}),\qquad Q^{\zeta}_{\mu}(h_{<t},a_{t})=\sum_{\pi\in\mathcal{P}} \omega(\pi|h_{<t})Q^{\pi}_{\mu}(h_{<t},a_{t})\]

From the linearity of \(Q\)-values we can define the following \(Q\)-value over the Bayesian mixture environment and Bayesian mixture policy.

\[Q^{\zeta}_{\xi}(h_{<t},a_{t}):=\sum_{\pi\in\mathcal{P}}\omega(\pi|h_{<t})\sum _{\nu\in\mathcal{M}}w(\nu|h_{<t})Q^{\pi}_{\nu}(h_{<t},a_{t})\] (3)

**Definition 10** (Self-Aixi).: _The agent Self-AIXI is defined as taking the (one step) optimal action with respect to \(Q^{\zeta}_{\xi}\). Formally,_

\[\pi_{S}(h_{<t}):=\operatorname*{arg\,max}_{a_{t}}Q^{\zeta}_{\xi}(h_{<t},a_{t})\]

**Remark 11** (Self-Aixi does not optimize the future.).: _Importantly, Self-AIXI maximizes the Q-values from the mixture policy instead of the optimal policy, which means that there is no need to optimize the future but it needs to know the Q-values of the current \(\zeta\)-mixture. Pragmatically, these Q-values are on-policy, typically easier to estimate than the optimal off-policy values._

Note how action selection and history interact with the policy-mixture. The action selected by the Self-AIXI agent necessarily improves, by definition, over the current value estimates \(\max_{a_{t}}Q^{\zeta}_{\xi}(h_{<t},a_{t})\geq V^{\zeta}_{\xi}(h_{<t})\). Then, this action is added to the next history \(h_{<t+1}\) which is consumed by the policy-mixture at the next time step i.e., \(\zeta(a_{t+1}|h_{<t+1})\). This leads to the following remark.

**Remark 12** (Self-predicting minimal improvements.).: _The policy-mixture does Bayesian inference over the incoming self-generated action-data and makes better predictions over time. Thus, Self-AIXI is self-predicting its own small improvements made by the \(\operatorname*{arg\,max}\) operation in Definition 10. In the case of using the largest class of policies (all computable functions), the mixture is a Solomonoff predictor with enough power to represent, within itself, the policy evaluation and improvement operation._

Showing that the self-prediction process employed by Self-AIXI converges to AIXI is the aim of our theoretical results in Section 4.

### Choice of class and prior

When choosing an environment class \(\mathcal{M}\) we need to pick one large enough so as to contain the true environment \(\mu\); the larger the class the weaker the assumption that \(\mu\in\mathcal{M}\). However, the larger the class we choose the harder it will be to compute the Bayesian mixture of that class, to the point that \(\xi\) may become incomputable (or at least more incomputable than any \(\nu\in\mathcal{M}\)). With all of this in mind there are two directions that this can be taken: The first is to choose the largest class such that computing \(\xi\) is still tractable and the second is to choose the largest possible class such that it contains all possible environments our universe could contain (or be). In the first case we can choose something like the class of variable-order MDPs with efficient Context Tree Weighting(CTW) algorithm [24] for \(\xi\), as is done in [20]. In the second case we can pick the set of all (cumulatively lower-) semicomputable semimeatures [25]. Without going to much into the choice of prior, a simplicity-based prior is an ideal choice. There is an interesting poly-time computable class of all measures of logarithmic complexity [26] for which \(\xi\in\mathcal{M}\) (\((\alpha,\gamma)\)-simple measures). One easy way of achieving \(\xi\in\mathcal{M}\) is to add \(\xi\) to \(\mathcal{M}\) with some weight \(w_{0}^{\prime}>0\) and renormalize \(w_{i}\sim w_{i}^{\prime}=(1-w_{0}^{\prime})w_{i}\), then \(\xi^{\prime}\equiv\xi\in\mathcal{M}^{\prime}:=\{\xi\}\cup\mathcal{M}\). We can apply the same logic to the policy class \(\mathcal{P}\) and policy prior \(\omega(\pi)\) as we just did for \(\mathcal{M}\) and \(w(\mu)\), similarly we need to have that \(\pi_{S}\in\mathcal{P}\). We leave the full investigation of policy classes to future work. Full discussion on the choice of class \(\mathcal{M}\) and prior \(w(\nu)\) are beyond the scope of this work [14; 27].

### Experimental Results

While our work is mainly theoretical, we also conducted experiments (see Appendix B) comparing self-prediction, using a Self-AIXI approximation, against the pure planning approach, using an AIXI approximation, using Context Tree Weighting as predictor and Monte-Carlo Tree Search for the Q-value estimates. In short, the Self-AIXI approximation outperforms the AIXI approximation in three environments and performs equally in the remaining two.

## 4 Theoretical Results

In this section we will demonstrate the intelligence of Self-AIXI by showing that it asymptotically converges to AIXI in expectation. We specifically chose this criterion over other types of optimality, since Cesaro and Almost-Sure asymptotic optimality (converging to the optimal policy) are too strong [28; 29; 30], and are not satisfied by AIXI. In fact, agents which satisfy these also die with certainty [31].

The goal is to show that Self-AIXI eventually performs as well as the most intelligent agent, and AIXI is the most intelligent agent [21]. We will do this by proving an asymptotic convergence result to the Bayes optimal policy in all environments.

To summarize the results of this section:

* Theorem 18: \(\forall\mu\). \(V_{\xi}^{\pi_{S}}\to V_{\xi}^{s}\) in \(\mu^{\pi_{S}}\)-expectation.
* Theorem 21: \(\forall\mu\). \(V_{\mu}^{\pi_{S}}\to V_{\mu}^{\pi_{\xi}^{s}}\) in \(\mu^{\pi_{S}}\)-expectation.
* Theorem 22: \(V_{\mu}^{\pi_{S}}\to V_{\mu}^{\pi_{S}}\)-almost surely, under similar conditions to the AIXI Self-optimizing result from Theorem 7.

The initial three results successively broaden in scope, with the second being a more general form of the first, and the third further generalizing the second, holding for all \(\mu\) instead of just \(\xi\). All proofs not found in this section can be found in the appendix.

### Expected AIXI-like behavior

To start with, we will present a useful Lemma about the environment mixture \(\xi\):

**Lemma 13** (Convergence of \(\xi\) to \(\mu\) in TV).: _For any environment \(\mu\in\mathcal{M}\) and policy \(\pi\) we have_

\[D_{\infty}(\xi^{\pi},\mu^{\pi}|h_{<t})\to 0\ \ \text{as}\ \ \ t\to\infty\ \ \mu^{\pi}\text{-almost surely}\]

Proof.: Dominance \(\xi^{\pi}(\cdot)\geq w(\mu)\mu^{\pi}(\cdot)\) implies that \(\mu^{\pi}\) is absolutely continuous with respect to \(\xi^{\pi}\) and therefore \(\xi^{\pi}\) merges strongly with \(\mu^{\pi}\)[32]. For full details and definitions of strong merging and absolute continuity see [23]. 

We can derive a dual to Lemma 13 and restate it in terms of asymptotic convergence in expectation:

**Lemma 14** (Convergence of \(\zeta\) to \(\pi\) in expectation).: _If \(\pi\in\mathcal{P}\) then for all \(\nu\) we have_

\[\mathbb{E}_{\nu}^{\pi}[D_{\infty}(\nu^{\pi},\nu^{\zeta}|h_{<t})]\to 0\ \ \text{as}\ \ \ t\to\infty\]

Proof.: The proof is dual to the proof of Lemma 13 with policies and environments exchanged. Finally, almost sure convergence implies convergence in expectation for bounded random variables.

Now we can show that if a policy is close to one-step optimal, then it is also close to optimal (this result is independent of \(\xi\) and will work for any environment, but we are only interested in the \(\xi\) case).

**Lemma 15** (A one-step good policy is close to optimal).: _Let \(\Delta(h):=|V_{\xi}^{*}(h)-V_{\xi}^{\pi}(h)|\) with \(h\in(\mathcal{A}\times\mathcal{E})^{t}\) for \(t\geq t_{0}\in\mathbb{N}\)._

\[\begin{array}{ll}&\mbox{If }\ \ \mathbb{E}_{\xi}^{\pi}|\max_{a}Q_{\xi}^{ \pi}(h,a)-V_{\xi}^{\pi}(h)|<\beta\ \ \forall t\geq t_{0}\\ &\mbox{and }\ \ \mathbb{E}_{\xi}^{\pi}[\max_{a}\sum_{e}\xi(e|ha)\Delta(hae)] \leq(1+\alpha)\mathbb{E}_{\xi}^{\pi}\Delta(hae)\ \ \forall t\geq t_{0}\\ &\mbox{then }\ \ \mathbb{E}_{\xi}^{\pi}\Delta(h)<\frac{\beta}{1-\gamma(1+ \alpha)}\ \ \forall t\geq t_{0}\ \ \mbox{provided }\ \ 1+\alpha<1/\gamma\end{array}\]

Using the previous Lemma we can show our main result about the behavior of Self-AIXI. This result shows that the performance of Self-AIXI converges to that of AIXI on (expected) histories generated by Self-AIXI. We call a policy that satisfied the second condition for some \(1+\alpha<1/\gamma\) for some (large) \(t_{0}\), _sensible off-policy_. We conjecture this plausible condition holds for \(\pi_{S}\), bar some exotic counter-example classes \(\mathcal{M}\).

**Theorem 16** (Self-AIXI converges to AIXI in \(\xi\)-expectation).: _Assuming \(\pi_{S}\) is sensible off-policy,_

\[\mathbb{E}_{\xi}^{\pi_{S}}\left[V_{\xi}^{*}(h_{<t})-V_{\xi}^{\pi_{S}}(h_{<t}) \right]\to 0\ \ \mbox{as }\ \ t\rightarrow\infty.\]

Proof.: \[\begin{array}{ll}0\ \stackrel{{(a)}}{{\leq}}\ \ \max_{a}Q_{\xi}^{\zeta}(h,a)-V_{\xi}^{\zeta}(h)\ \stackrel{{(b)}}{{=}}\ V_{\xi}^{\pi_{S}}(h)-V_{\xi}^{\zeta}(h) \ \stackrel{{(c)}}{{\leq}}\ D_{\infty}(\xi^{\pi_{S}},\xi^{\zeta}|h) \end{array}\]

where (a) and (b) follow from

\[V_{\xi}^{\pi_{S}}(h)\ \equiv\ \max_{a}Q_{\xi}^{\zeta}(h,a)\ \geq\ \sum_{a}\zeta(a|h)Q_{\xi}^{\zeta}(h,a)\ =\ V_{\xi}^{\zeta}(h)\]

and (c) follows from Lemma 3. Let \(\beta>0\) and \(h\in(\mathcal{A}\times\mathcal{E})^{t}\). Now Lemma 14 and the above implies that there exists a \(t_{0}\) such that for all \(t\geq t_{0}\),

\[0\ \leq\ \mathbb{E}_{\xi}^{\pi_{S}}[\max_{a}Q_{\xi}^{\zeta}(h,a)-V_{\xi}^{ \zeta}(h)]\ \leq\ \mathbb{E}_{\xi}^{\pi_{S}}D_{\infty}(\xi^{\pi_{S}},\xi^{\zeta}|h)\ <\ \beta\]

Taking the expectation of \(0\leq V_{\xi}^{*}-V_{\xi}^{\pi_{S}}\leq V_{\xi}^{*}-V_{\xi}^{\zeta}\) (which follows from \(V_{\xi}^{*}\geq V_{\xi}^{\pi_{S}}\geq V_{\xi}^{\zeta}\)), Lemma 15 implies

\[0\ \leq\ \mathbb{E}_{\xi}^{\pi_{S}}[V_{\xi}^{*}(h)-V_{\xi}^{\pi_{S}}(h)]\ \leq\ \mathbb{E}_{\xi}^{\pi_{S}}[V_{\xi}^{*}-V_{\xi}^{\zeta}]\ \leq\ \beta/(1-\gamma(1+\alpha)).\]

Since \(\beta>0\) was arbitrary, we get the \(\mathbb{E}_{\xi}^{\pi_{S}}[V_{\xi}^{*}(h)-V_{\xi}^{\pi_{S}}(h)]\to 0\). 

The previous theorem also holds in expectation over the true environment \(\mu\), instead of in expectation w.r.t. \(\xi\), but first we need a small lemma:

**Lemma 17** (\(\mathbb{E}_{\xi}^{\pi}\to 0\) implies \(\mathbb{E}_{\mu}^{\pi}\to 0\)).: _If \(\pi\) is such that_

\[\mathbb{E}_{\xi}^{\pi}\left[V_{\xi}^{*}(h_{<t})-V_{\xi}^{\pi}(h_{<t})\right] \to 0\ \ \mbox{as }\ \ t\rightarrow\infty.\]

_then for all \(\mu\in\mathcal{M}\) we have_

\[\mathbb{E}_{\mu}^{\pi}\left[V_{\xi}^{*}(h_{<t})-V_{\xi}^{\pi}(h_{<t})\right] \to 0\ \ \mbox{as }\ \ t\rightarrow\infty.\]

We can now present the main Theorem of the paper, this theorem states that as Self-AIXI interacts with the true environment \(\mu\) Self-AIXI will learn to act like the optimal Bayesian agent AIXI.

**Theorem 18** (Main Theorem: Self-AIXI converges to AIXI in \(\mu\)-expectation).: _For all \(\mu\in\mathcal{M}\) we have_

\[\mathbb{E}_{\mu}^{\pi_{S}}\left[V_{\xi}^{*}(h_{<t})-V_{\xi}^{\pi_{S}}(h_{<t}) \right]\to 0\ \ \mbox{as }\ \ t\rightarrow\infty.\]

Proof.: Theorem 16 and Lemma 17. 

We can rewrite the above theorem in the form of the Universal Intelligence measure \(\Upsilon\),

**Corollary 19** (LH Form of Theorem 18).: _For all \(\mu\in\mathcal{M}\)_

\[\mathbb{E}_{\mu}^{\pi_{S}}\left[\max_{\pi}\Upsilon(\pi|h_{<t})-\Upsilon(\pi_{S}| h_{<t})\right]\to 0\ \ \text{as}\ \ t\rightarrow\infty\]

Finally we can generalise our previous result to work for the value function of any \(\mu\). This is important because we are most interested in the performance of our agent on the true environment. We first need to show that convergence of the value function with \(\xi\) implies convergence of the value function with \(\mu\) (in \(\mu\)-expectation).

**Lemma 20** (\(V_{\xi}^{\pi^{\prime}}\to V_{\xi}^{\pi}\) implies \(V_{\mu}^{\pi^{\prime}}\to V_{\mu}^{\pi}\) in \(\mu\)-expectation).: _If \(\pi\) is such that for all \(\mu\in\mathcal{M}\)_

\[\mathbb{E}_{\mu}^{\pi}\left[V_{\xi}^{\pi^{\prime}}(h_{<t})-V_{\xi}^{\pi}(h_{< t})\right]\to 0\ \ \text{as}\ \ t\rightarrow\infty.\]

_and \(D_{\infty}(\mu^{\pi^{\prime}},\xi^{\pi^{\prime}}|h_{<t})\to 0\ \mu^{\pi}\)-almost surely then we have_

\[\mathbb{E}_{\mu}^{\pi}\left[V_{\mu}^{\pi^{\prime}}(h_{<t})-V_{\mu}^{\pi}(h_{< t})\right]\to 0\ \ \text{as}\ \ t\rightarrow\infty.\]

Now we can present a convergence in value functions with the true environment \(\mu\).

**Theorem 21** (Self-AIXI converges to the \(\xi\)-optimal agent in \(\mu\)-expectation).: _For all \(\mu\in\mathcal{M}\) if \(D_{\infty}(\mu^{\pi_{\xi}^{*}},\xi^{\pi_{\xi}^{*}}|h_{<t})\to 0\ \mu^{\pi_{S}}\)-almost surely then we have_

\[\mathbb{E}_{\mu}^{\pi_{S}}\left[|V_{\mu}^{\pi_{\xi}^{*}}(h_{<t})-V_{\mu}^{\pi _{S}}(h_{<t})|\right]\to 0\ \ \text{as}\ \ t\rightarrow\infty.\]

Proof.: Immediate result form Theorem 18 and Lemma 20 

This result states that as Self-AIXI interacts with the true environment \(\mu\), the expected future performance difference between Self-AIXI and AIXI in \(\mu\) will go to zero, that is to say, they will (asymptotically) have equal performance in \(\mu\).

### Self-optimization

We have previously shown the performance of Self-AIXI using convergence of value functions \(\mu\)-expectation. We can also demonstrate the performance of Self-AIXI in a more direct way, using self-optimization. That is, showing that if it is possible for a policy to learn to be optimal in all environments in a given class, then Self-AIXI will learn to be optimal in all environments in this class.

Self-AIXI can self-optimize in the same way as AIXI, with an additional assumption on the self model. The additional assumption is that for all \(t,h_{<t}\) we have \(V_{\xi}^{\zeta}(h_{<t})\geq V_{\xi}^{\overline{\pi_{t}}}(h_{<t})-\epsilon_{t}\).

**Theorem 22** (Self-AIXI is Self-optimizing).: _Let \(\mu\) be some environment. If there is a policy \(\pi\) and a sequence of policies \(\overline{\pi_{1}},\overline{\pi_{2}}\ldots\) all contained within \(\mathcal{P}\) such that for all \(t,h_{<t}\) we have \(V_{\xi}^{\zeta}(h_{<t})\geq V_{\xi}^{\overline{\pi_{t}}}(h_{<t})-\epsilon_{t}\) with \(\epsilon_{t}\to 0\), and for all \(\nu\in\mathcal{M}\)_

\[V_{\nu}^{*}(h_{<t})-V_{\nu}^{\overline{\pi_{t}}}(h_{<t})\to 0\ \ \text{as}\ \ t \rightarrow\infty\ \ \mu^{\pi}\text{-almost surely}\] (4)

_then_

\[V_{\nu}^{*}(h_{<t})-V_{\nu}^{\pi_{S}}(h_{<t})\to 0\ \ \text{as}\ \ t \rightarrow\infty\ \ \mu^{\pi}\text{-almost surely}\]

_If \(\pi=\pi_{S}\) and Equation 4 holds for all \(\mu\in\mathcal{M}\), then \(\pi_{S}\) is strongly asymptotically optimal in the class \(\mathcal{M}\)._

There are known important cases of model classes which admit self-optimising policies such as Ergodic MDPs [33] and Ergodic k-MDPs [34]. However, we have not yet shown there exist (interesting) classes \(\mathcal{M}\) and \(\mathcal{P}\) where the assumption that for all \(t,h_{<t}\) we have \(V_{\xi}^{\zeta}(h_{<t})\geq V_{\xi}^{\overline{\pi_{t}}}(h_{<t})-\epsilon_{t}\) with \(\epsilon_{t}\to 0\) and the other self-optimizing assumptions hold. We leave this to future work.

## 5 Related work

### Universal Artificial Intelligence

There has been much work studying the Bayesian optimal agent AIXI and extending it in various ways [14; 21; 23]. Some of these extensions include: Knowledge seeking agent (KSA), an extension of AIXI which seeks information leading to a form of maximal exploration [35; 36]. BayesExp, which combines the exploiting of AIXI and exploring of KSA in a way that allows the agent to be weakly asymptotically optimal [37; 38]. An AIXI variant that uses Thompson sampling [39] for additional exploration is asymptotically optimal in expectation [28]. Inquisitive Agent (Inq) uses an information gaining strategy based on expeditions to achieve strong asymptotic optimality [40].

AIXI achieves (by definition) Bayesian optimality, and although there are problems with this notion of optimality, it is widely agreed to be the (current) best choice for an optimality criterion. Every notion of optimality possesses inherit difficulties; these difficulties have been extensively studied Leike and Hutter [29], Lattimore and Hutter [30], Cohen et al. [31].

### Comparison with other methods in the literature

To overcome the computational burden of the AIXI's planning phase, approximation techniques usually relax the planning problem into an estimation problem [41; 42]. The main approach for off-the-shelf estimation methods such as Monte Carlo Tree Search (MCTS) is an (optimistic) estimation of the Q-values. Though seemingly different, planning and learning, in the general case, can be seen as following very similar computational processes as shown by [43]. Other practical methods which reduce or remove the planning problem are distributional reinforcement learning [44], Compress and Control (CnC) [45] and feature reinforcement learning (FRL) [46; 47]. In distributional reinforcement learning and CnC powerful sequence prediction methods are utilized to estimate the distribution of returns directly. While it does not remove the planning entirely, FRL adaptively compresses the general RL problem to a more simple (often MDP) problem and solves that problem by traditional methods. The planning required in the simple problem is often far less than in the general problem.

In Table 1 we compare some exemplary agents where each column denotes whether they use some form of learning (or policy distillation), whether they use explicit search/planning, whether they learn an explicit model of the environment, if they are a Bayes-optimal agent, and if there are practical in the sense that there is an actual implementation of the agent. As can be seen, Self-AIXI is the only Bayes-optimal agent that is universal and exploits learning instead of planning.

### Agents modeling themselves

Self-prediction can be thought as an agent modeling itself. Within the universal AI literature, there are other works that go further and drop the dualism of traditional reinforcement learning for the (more realistic) physicalism where agents are contained within their own environment. In this case many complications arise which previous work tries to address. For example, in [48] the behavior of several agents able to modify their own source code is analyzed. In [49], Orseau et. al. presents a space-time embedded definition of the value function and intelligence, and in [50] Leike et. al. aims to solve the grain of truth (a self-referential) problem of Bayesian agents in GRL, showing that there

\begin{table}
\begin{tabular}{|c||c|c|c|c|c|} \hline  & Policy Learning & & Environment & Bayes & \\  & (or distillation) & Planning & Model & Optimal & Practical \\ \hline \hline AIXI & No & Yes & Yes & Yes & No \\ \hline Self-AIXI & Yes & No & Yes & Yes & No \\ \hline MC-AIXI-CTW & No & Yes & Yes & Yes & Yes \\ \hline \hline \(\mu\)-Zero & Yes & Yes & Yes & No & Yes \\ \hline DQN & Yes & No & No & No & Yes \\ \hline CnC & No & No & No & Yes & Yes \\ \hline PhiMDP & No & No & No & No & No \\ \hline \end{tabular}
\end{table}
Table 1: Comparison between different methodsis a non-trivial class of environments for which the optimal Bayesian agent over that class is within the class.

### Self Prediction in Traditional RL

Recently traditional reinforcement learning has begun to embrace self-prediction. In [51] Predictions of Bootstrapped Latents (PBL) is presented as a method which can use predictive representations on multitask environments. [52] used Self-Predictive Representations (SPR) to train agents to anticipate future latent states, which showed improvements on Atari games benchmarks. In [53], exploration in visually complex environments was streamlined with BYOL-Explore using a bootstrapped prediction. Additionally, [54] underscores the challenges of representation collapse in self-predictive learning and introduces bidirectional learning to address this. Together, these studies underscore the evolving landscape of self-predictive strategies in traditional RL.

## 6 Discussion, Future Work and Limitations

Practical considerations for computing the mixture component in each step.Computing the Self-AIXI policy requires estimating Q-values using the Bayesian mixture policy \(\zeta(a_{t}|h_{<t})\). Naively, this means evaluating each policy in the mixture and computing its respective posterior weight. However, more efficient schemes can be derived, e.g., similar to how the CTW algorithm computes the Bayesian mixture without explicitly evaluating each hypothesis and computing the (marginalizing) sum over hypotheses in each step.

Leveraging large sequence models.Powerful sequence predictors like large language models (LLMs) are the cornerstone of modern general AI due to their impressive zero- and few-shot performance [55; 56; 57]. An open question in AGI research is how to leverage the strength of sequence prediction not only for environment modeling, but also to derive goal-directed policies; how to build agents from predictors. We have shown how to shift computational effort for planning (in AIXI) can be transferred into effort devoted to prediction in Self-AIXI, making self-prediction theoretically very well suited to exploit strong predictors such as LLMs. In contrast to similar ad-hoc approaches [58], Self-AIXI is derived from sound theoretical design principles and comes with theoretical guarantees.

Safety implications.Self-AIXI constitutes a theoretical blueprint for building powerful general AI agents that may be very suitable to leverage strong predictors, such as LLMs, in practice. The implications of such agents have recently been discussed in the philosophical and technical literature, and concerns regarding their safety have been raised [59; 60]. Self-AIXI comes with some interesting favorable safety properties. First and foremost, Self-AIXI is derived from sound reasoning and decision-making principles: Bayesian posterior inference and rational optimization of rewards. Self-AIXI comes with a well-understood theoretical model to study abstract properties, guarantees and bounds. An advantage over AIXI, by design, is the self-model of the agent, which can be analyzed on its own. The explicit separation of beliefs over the environment and posterior over the own policy leads to an agent that is more interpretable and can be verified more easily compared to a monolithic agent where components are mixed. If the self-predictor used is not a Bayesian one, but instead some black box predictor, then both the guarantees of Self-AIXI and the guarantees on interpretability are no longer maintained. We strongly encourage an open and differentiated wider discussion on the safety of (universal) RL agents, and believe that theoretical models, like Self-AIXI, can facilitate this discussion by providing a formal and mathematically concrete formulation.

Limitations.Our theoretical results require that the Self-AIXI policy is part of the policy class under consideration, i.e., \(\pi_{S}\in\mathcal{P}\) (meaning that the mixture policy is itself part of the policy class). Generally, if there is a policy in the policy class that is "close" to the optimal policy (in terms of minimal KL divergence), Self-AIXI will still perform well. This result can be stated formally using known formulations and bounds for Bayesian mixtures. We leave a detailed mathematical analysis for future work, since the aim of this work is to introduce the theory of Self-AIXI. Another issue is that Self-AIXI, like the original AIXI, is incomputable for environment classes required for (strict) universality. To tackle this, many practical approximations of AIXI have been proposed in the literature, which provide straightforward starting points for similar approximations for Self-AIXI.

Designing and studying non-trivial practical approximations of Self-AIXI is beyond the scope of this paper.

Future work.On the theoretical side there are several promising directions of future work: exploring which non-trivial policy classes \(\mathcal{P}\) satisfy \(\pi_{S}\in\mathcal{P}\) (or replacing this assumption); extending this work with a sampling-first approach (e.g. Thompson Sampling). On the practical side the first step is to empirically demonstrate the performance of the self-prediction approach proposed in this paper. We believe that especially when coupled with powerful modern predictors, e.g., transformer-based sequence predictors, high-fidelity approximations of \(\xi\) and \(\zeta\) are possible, which can lead to strong, general agents.

## 7 Conclusion

In this paper we presented Self-AIXI, a theoretical framework for universal AI that extends AIXI by self-prediction (the agent performs inference over its own policy). The result is an agent that matches AIXIs performance guarantees, while shifting the computational focus on prediction instead of planning. This makes Self-AIXI interesting for building general agents by leveraging recent breakthroughs in sequence predictors, and combining them with RL in a sound fashion. Current approaches to do this, such as RL from human/artificial feedback (RLHF/RLAF), are quite limited (in terms of generality of the RL formulation) and do not come with optimality or universality guarantees. We believe our results to be an important cornerstone for building agents that go beyond zero- or few-shot transfer, towards rapid and data efficient RL.

#### Acknowledgments

We thank Tor Lattimore, Laurent Orseau and Anian Ruoss for their helpful feedback and insightful discussions.

## References

* [1] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. 2018.
* [2] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Reinforcement learning_, pages 5-32, 1992.
* [3] John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. In Yoshua Bengio and Yann LeCun, editors, _4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings_, 2016. URL http://arxiv.org/abs/1506.02438.
* [4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.
* [5] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In _International conference on machine learning_, pages 1928-1937. PMLR, 2016.
* [6] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* [7] Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of monte carlo tree search methods. _IEEE Transactions on Computational Intelligence and AI in games_, 4(1):1-43, 2012.
* [8] Carlos E Garcia, David M Prett, and Manfred Morari. Model predictive control: Theory and practice--a survey. _Automatica_, 25(3):335-348, 1989.

* Silver et al. [2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. _nature_, 550(7676):354-359, 2017.
* Silver et al. [2018] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. _Science_, 362(6419):1140-1144, 2018.
* Schrittwieser et al. [2020] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020.
* Antonoglou et al. [2022] Ioannis Antonoglou, Julian Schrittwieser, Sherjil Ozair, Thomas K. Hubert, and David Silver. Planning in Stochastic Environments with a Learned Model. In _International Conference on Learning Representations_, March 2022.
* Rusu et al. [2015] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. _arXiv preprint arXiv:1511.06295_, 2015.
* Hutter [2004] Marcus Hutter. _Universal artificial intelligence: Sequential decisions based on algorithmic probability_. Springer Science & Business Media, 2004.
* Everitt and Hutter [2018] Tom Everitt and Marcus Hutter. Universal artificial intelligence: Practical agents and fundamental challenges. In Hussein A. Abbass, Jason Scholz, and Darryn J. Reid, editors, _Foundations of Trusted Autonomy_, chapter 2, pages 15-46. Springer, 2018. ISBN 978-3-319-64815-6. doi: 10.1007/978-3-319-64816-3_2.
* Solomonoff [1964] Ray J Solomonoff. A formal theory of inductive inference. part i. _Information and control_, 7(1):1-22, 1964.
* Rathmanner and Hutter [2011] Samuel Rathmanner and Marcus Hutter. A philosophical treatise of universal induction. _Entropy_, 13(6):1076-1136, 2011. ISSN 1099-4300. doi: 10.3390/e13061076.
* Hutter [2003] Marcus Hutter. Convergence and loss bounds for bayesian sequence prediction. _IEEE Transactions on Information Theory_, 49(8):2061-2067, 2003.
* Hutter [2007] Marcus Hutter. On universal prediction and Bayesian confirmation. _Theoretical Computer Science_, 384(1):33-48, 2007. ISSN 0304-3975. doi: 10.1016/j.tcs.2007.05.016.
* Veness et al. [2010] Joel Veness, Kee Siong Ng, Marcus Hutter, and David Silver. Reinforcement learning via AIXI approximation. In _Proc. 24th AAAI Conference on Artificial Intelligence_, pages 605-611, Atlanta, USA, 2010. AAAI Press. URL http://arxiv.org/abs/1007.2049.
* Legg and Hutter [2007] Shane Legg and Marcus Hutter. Universal intelligence: A definition of machine intelligence. _Minds and machines_, 17(4):391-444, 2007.
* Hutter [2002] Marcus Hutter. Self-optimizing and Pareto-optimal policies in general environments based on Bayes-mixtures. In _Proc. 15th Annual Conf. on Computational Learning Theory (COLT'02)_, volume 2375 of _LNAI_, pages 364-379, Sydney, Australia, 2002. Springer. ISBN 978-3-540-43836-6. doi: 10.1007/3-540-45435-7_25. URL http://arxiv.org/abs/cs.AI/0204040.
* Leike [2016] Jan Leike. _Nonparametric general reinforcement learning_. PhD thesis, The Australian National University (Australia), 2016.
* Willems et al. [1995] Frans MJ Willems, Yuri M Shtarkov, and Tjalling J Tjalkens. The context-tree weighting method: Basic properties. _IEEE transactions on information theory_, 41(3):653-664, 1995. doi: http://dx.doi.org/10.1109/18.382012.
* Schmidhuber [2002] J. Schmidhuber. Hierarchies of generalized Kolmogorov complexities and nonenumerable universal measures computable in the limit. _International Journal of Foundations of Computer Science_, 13(4):587-612, 2002.

* Vovk [1989] Vladimir G Vovk. Prediction of stochastic sequences. _Rossiskaya Akademiya Nauk. Problemy Peredachi Informatsii_, 25(4):35-49, 1989.
* Li et al. [2008] Ming Li, Paul Vitanyi, et al. _An introduction to Kolmogorov complexity and its applications_, volume 3. Springer, 2008.
* Leike et al. [2017] Jan Leike, Tor Lattimore, Laurent Orseau, and Marcus Hutter. On Thompson sampling and asymptotic optimality. In _Proc. 26th International Joint Conf. on Artificial Intelligence (IJCAI'17)_, pages 4889-4893, Melbourne, Australia, 2017. ISBN 978-0-9992411-0-3. doi: 10.24963/ijcai.2017/688. URL http://arxiv.org/abs/1602.07905. Best sister conferences paper track.
* Leike and Hutter [2015] Jan Leike and Marcus Hutter. Bad universal priors and notions of optimality. _Journal of Machine Learning Research, W&CP: COLT_, 40:1244-1259, 2015. ISSN 1532-4435. URL http://arxiv.org/abs/1510.04931. Also presented at EWRL'15. http://ewrl.files.wordpress.com/2015/02/ewrl12_2015_submission_3.pdf.
* Lattimore and Hutter [2011] Tor Lattimore and Marcus Hutter. Asymptotically optimal agents. In _Proc. 22nd International Conf. on Algorithmic Learning Theory (ALT'11)_, volume 6925 of _LNAI_, pages 368-382, Espoo, Finland, 2011. Springer. ISBN 3-642-24411-4. doi: 10.1007/978-3-642-24412-4_29. URL http://arxiv.org/abs/1107.5537.
* Cohen et al. [2021] Michael K Cohen, Elliot Catt, and Marcus Hutter. Curiosity killed or incapacitated the cat and the asymptotically optimal agent. _IEEE Journal on Selected Areas in Information Theory_, 2(2):665-677, 2021.
* Blackwell and Dubins [1962] D. Blackwell and L. Dubins. Merging of opinions with increasing information. _Annals of Mathematical Statistics_, 33:882-887, 1962.
* Legg and Hutter [2004] Shane Legg and Marcus Hutter. Ergodic mdps admit self-optimising policies. Technical report, Technical Report IDSIA-21-04, IDSIA, 2004.
* Veness [2011] Joel Veness. _Approximate universal artificial intelligence and self-play learning for games_. PhD thesis, University of New South Wales, Sydney, Australia, 2011.
* Orseau [2011] L. Orseau. Universal knowledge-seeking agents. In _Proc. 22nd International Conf. on Algorithmic Learning Theory (ALT'11)_, volume 6925 of _LNAI_, pages 353-367, Espoo, Finland, 2011. Springer. doi: 10.1007/978-3-642-24412-4_28.
* Orseau et al. [2013] Laurent Orseau, Tor Lattimore, and Marcus Hutter. Universal knowledge-seeking agents for stochastic environments. In _Proc. 24th International Conf. on Algorithmic Learning Theory (ALT'13)_, volume 8139 of _LNAI_, pages 158-172, Singapore, 2013. Springer. ISBN 978-3-642-40934-9. doi: 10.1007/978-3-642-40935-6_12.
* Lattimore and Hutter [2014] Tor Lattimore and Marcus Hutter. Bayesian reinforcement learning with exploration. In _Proc. 25th International Conf. on Algorithmic Learning Theory (ALT'14)_, volume 8776 of _LNAI_, pages 170-184, Bled, Slovenia, 2014. Springer. ISBN 978-3-319-11661-7. doi: 10.1007/978-3-319-11662-4_13.
* Lattimore [2014] T. Lattimore. _Theory of General Reinforcement Learning_. PhD thesis, Research School of Computer Science, Australian National University, 2014. URL https://library.anu.edu.au/record=b3579030.
* Thompson [1933] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 25(3/4):285-294, 1933.
* Cohen et al. [2019] Michael Cohen, Elliot Catt, and Marcus Hutter. A strongly asymptotically optimal agent in general environments. In _Proc. 28th International Joint Conf. on Artificial Intelligence (IJCAI'19)_, pages 2179-2186, Macao, China, 2019. ISBN 978-0-9992411-4-1. doi: 10.24963/ijcai.2019/302. URL http://arxiv.org/abs/1903.01021.
* Veness et al. [2011] Joel Veness, Kee Siong Ng, Marcus Hutter, William Uther, and David Silver. A monte-carlo aixi approximation. _Journal of Artificial Intelligence Research_, 40:95-142, 2011.

* [42] Samuel Yang-Zhao, Tianyu Wang, and Kee Siong Ng. A direct approximation of aixi using logical state abstractions. In _Advances in Neural Information Processing Systems_.
* [43] Jean-Bastien Grill, Florent Altche, Yunhao Tang, Thomas Hubert, Michal Valko, Ioannis Antonoglou, and Remi Munos. Monte-carlo tree search as regularized policy optimization. In _International Conference on Machine Learning_, pages 3769-3778. PMLR, 2020.
* [44] Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement learning. In _International conference on machine learning_, pages 449-458. PMLR, 2017.
* [45] Joel Veness, Marc Bellemare, Marcus Hutter, Alvin Chua, and Guillaume Desjardins. Compress and control. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 29, 2015.
* [46] Marcus Hutter et al. Feature reinforcement learning: Part i. unstructured mdps. 2009.
* [47] Sultan J Majeed. Abstractions of general reinforcement learning. _arXiv preprint arXiv:2112.13404_, 2021.
* [48] Laurent Orseau and Mark Ring. Self-modification and mortality in artificial agents. In _International Conference on Artificial General Intelligence_, pages 1-10. Springer, 2011.
* [49] Laurent Orseau and Mark Ring. Space-time embedded intelligence. In _International Conference on Artificial General Intelligence_, pages 209-218. Springer, 2012.
* [50] Jan Leike, Jessica Taylor, and Benya Fallenstein. A formal solution to the grain of truth problem. In _Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence_, pages 427-436, 2016.
* [51] Zhaohan Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altche, Remi Munos, and Mohammad Gheshlaghi Azar. Bootstrap latent-predictive representations for multitask reinforcement learning. In _International Conference on Machine Learning_, pages 3875-3886. PMLR, 2020.
* [52] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. _arXiv preprint arXiv:2007.05929_, 2020.
* [53] Zhaohan Guo, Shantanu Thakoor, Miruna Pislar, Bernardo Avila Pires, Florent Altche, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, et al. Byol-explore: Exploration by bootstrapped prediction. _Advances in neural information processing systems_, 35:31855-31870, 2022.
* [54] Yunhao Tang, Zhaohan Daniel Guo, Pierre Harvey Richemond, Bernardo Avila Pires, Yash Chandak, Remi Munos, Mark Rowland, Mohammad Gheshlaghi Azar, Charline Le Lan, Clare Lyle, et al. Understanding self-predictive learning for reinforcement learning. In _International Conference on Machine Learning_, pages 33632-33656. PMLR, 2023.
* [55] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of NAACL-HLT_, pages 4171-4186, 2019.
* [56] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [57] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.
* [58] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.

* [59] Nick Bostrom. _Superintelligence: Paths, Dangers, Strategies_. Oxford University Press, Inc., USA, 1st edition, 2014. ISBN 0199678111.
* [60] Stuart Russell. _Human compatible: Artificial intelligence and the problem of control_. Penguin, 2019.
* [61] Cameron B. Browne, Edward Powley, Daniel Whitehouse, Simon M. Lucas, Peter I. Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of monte carlo tree search methods. _IEEE Transactions on Computational Intelligence and AI in Games_, 4(1):1-43, 2012. doi: 10.1109/TCIAIG.2012.2186810.
* [62] F.M.J. Willems, Y.M. Shtarkov, and T.J. Tjalkens. The context-tree weighting method: basic properties. _IEEE Transactions on Information Theory_, 41(3):653-664, 1995. doi: 10.1109/18.382012.

## Appendix A Proofs

**Lemma 15** (A one-step good policy is close to optimal).: _Let \(\Delta(h):=|V^{*}_{\xi}(h)-V^{\pi}_{\xi}(h)|\) with \(h\in(\mathcal{A}\times\mathcal{E})^{t}\) for \(t\geq t_{0}\in\mathbb{N}\)._

\[\text{If }\ \mathbb{E}^{\pi}_{\xi}|\max_{a}Q^{\pi}_{\xi}(h,a)-V^{\pi}_ {\xi}(h)|<\beta\ \ \forall t\geq t_{0}\] \[\text{and }\ \mathbb{E}^{\pi}_{\xi}[\max_{a}\sum_{\varepsilon} \xi(e|ha)\Delta(hae)]\leq(1+\alpha)\mathbb{E}^{\pi}_{\xi}\Delta(hae)\ \ \forall t\geq t_{0}\] \[\text{then }\ \mathbb{E}^{\pi}_{\xi}\Delta(h)<\frac{\beta}{1- \gamma(1+\alpha)}\ \ \forall t\geq t_{0}\ \ \text{provided }\ 1+\alpha<1/\gamma\]

Proof.: Let \(\delta:=\sup_{t\geq t_{0}}\mathbb{E}\Delta(h)\), where \(h\in(\mathcal{A}\times\mathcal{E})^{t}\) and \(\mathbb{E}\) is short for \(\mathbb{E}^{\pi}_{\xi}\).

\[\mathbb{E}\Delta(h) =\ \big{|}\max_{a}Q^{*}_{\xi}(h,a)-V^{\pi}_{\xi}(h)\big{|}\] \[=\ \mathbb{E}\big{|}\max_{a}Q^{*}_{\xi}(h,a)-\max_{a}Q^{\pi}_{ \xi}(h,a)+\max_{a}Q^{\pi}_{\xi}(h,a)-V^{\pi}_{\xi}(h)\big{|}\] \[\leq\ \mathbb{E}\big{|}\max_{a}Q^{\pi}_{\xi}(h,a)-V^{\pi}_{\xi}(h) \big{|}+\mathbb{E}\big{|}\max_{a}Q^{*}_{\xi}(h,a)-\max_{a}Q^{\pi}_{\xi}(h,a) \big{|}\] \[\stackrel{{(1)}}{{<}}\beta+\mathbb{E}\Big{|}\max_{a} \sum_{\varepsilon}\xi(e|ha)\left(r+\gamma V^{*}_{\xi}(hae)\right)-\max_{a}\sum _{\varepsilon}\xi(e|ha)\left(r+\gamma V^{\pi}_{\xi}(hae)\right)\Big{|}\] \[\leq\ \beta+\gamma\mathbb{E}\max_{a}\sum_{\varepsilon}\xi(e|ha)|V^{*}_ {\xi}(hae)-V^{\pi}_{\xi}(hae)|\] \[\leq\ \beta+\gamma(1+\alpha)\mathbb{E}\Delta(hae)\]

Taking \(\sup_{t\geq t_{0}}\) on both sides implies \(\delta<\beta+\gamma(1+\alpha)\delta\) implies \(\delta<\beta/(1-\gamma(1+\alpha))\). 

**Lemma 17** (\(\mathbb{E}^{\pi}_{\xi}\to 0\) implies \(\mathbb{E}^{\pi}_{\mu}\to 0\)).: _If \(\pi\) is such that_

\[\mathbb{E}^{\pi}_{\xi}\left[V^{*}_{\xi}(h_{<t})-V^{\pi}_{\xi}(h_{<t})\right] \to 0\ \ \text{as }\ \ t\to\infty.\]

_then for all \(\mu\in\mathcal{M}\) we have_

\[\mathbb{E}^{\pi}_{\mu}\left[V^{*}_{\xi}(h_{<t})-V^{\pi}_{\xi}(h_{<t})\right] \to 0\ \ \text{as }\ \ t\to\infty.\]

Proof.: \[\mathbb{E}^{\pi}_{\mu}\left[V^{*}_{\xi}(h_{<t})-V^{\pi}_{\xi}(h_{<t})\right] \leq\frac{1}{w(\mu)}\mathbb{E}^{\pi}_{\xi}\left[V^{*}_{\xi}(h_{<t})-V^{\pi}_{ \xi}(h_{<t})\right]\to 0\]

by the dominance of \(\xi(\cdot)\geq w(\mu)\mu(\cdot)\). 

**Lemma 20** (\(V^{\pi^{\prime}}_{\xi}\to V^{\pi}_{\xi}\) implies \(V^{\pi^{\prime}}_{\mu}\to V^{\pi}_{\mu}\) in \(\mu\)-expectation).: _If \(\pi\) is such that for all \(\mu\in\mathcal{M}\)_

\[\mathbb{E}^{\pi}_{\mu}\left[V^{\pi^{\prime}}_{\xi}(h_{<t})-V^{\pi}_{\xi}(h_{< t})\right]\to 0\ \ \text{as }\ \ t\to\infty.\]

_and \(D_{\infty}(\mu^{\pi^{\prime}},\xi^{\pi^{\prime}}|h_{<t})\to 0\ \mu^{\pi}\)-almost surely then we have_

\[\mathbb{E}^{\pi}_{\mu}\left[V^{\pi^{\prime}}_{\mu}(h_{<t})-V^{\pi}_{\mu}(h_{< t})\right]\to 0\ \ \text{as }\ \ t\to\infty.\]

Proof.: \[\mathbb{E}^{\pi}_{\mu}\left[\left|V^{\pi^{\prime}}_{\mu}(h_{<t} )-V^{\pi}_{\mu}(h_{<t})\right|\right]\] \[=\mathbb{E}^{\pi}_{\mu}\left[\left|V^{\pi^{\prime}}_{\mu}(h_{<t}) -V^{\pi^{\prime}}_{\xi}(h_{<t})+V^{\pi^{\prime}}_{\xi}(h_{<t})-V^{\pi}_{\xi}(h_{ <t})+V^{\pi}_{\xi}(h_{<t})-V^{\pi}_{\mu}(h_{<t})\right|\right]\] \[\leq\mathbb{E}^{\pi}_{\mu}\left[\left|V^{\pi^{\prime}}_{\mu}(h_{< t})-V^{\pi^{\prime}}_{\xi}(h_{<t})\right|\right]+\mathbb{E}^{\pi}_{\mu}\left[\left|V^{ \pi^{\prime}}_{\xi}(h_{<t})-V^{\pi}_{\xi}(h_{<t})\right|\right]+\mathbb{E}^{\pi }_{\mu}\left[\left|V^{\pi}_{\xi}(h_{<t})-V^{\pi}_{\mu}(h_{<t})\right|\right]\]

The second and third term go to 0 as \(t\to\infty\) by the assumptions and Lemma 3 with Lemma 13. The first term goes to 0 as \(D_{\infty}(\mu^{\pi^{\prime}},\xi^{\pi^{\prime}}|h_{<t})\to 0\ \mu^{\pi}\)-almost surely implies \(\mathbb{E}^{\pi}_{\mu}\left[D_{\infty}(\mu^{\pi^{\prime}},\xi^{\pi^{\prime}}|h_{< t})\right]\to 0\) and we have \(\mathbb{E}^{\pi}_{\mu}\left[\left|V^{\pi^{\prime}}_{\mu}(h_{<t})-V^{\pi^{\prime}}_{\xi}(h_{<t}) \right|\right]\leq\mathbb{E}^{\pi}_{\mu}\left[D_{\infty}(\mu^{\pi^{\prime}},\xi^{ \pi^{\prime}}|h_{<t})\right]\).

**Theorem 22** (Self-AXI is Self-optimizing).: _Let \(\mu\) be some environment. If there is a policy \(\pi\) and a sequence of policies \(\overline{\pi_{1}},\overline{\pi_{2}}\ldots\) all contained within \(\mathcal{P}\) such that for all \(t,h_{<t}\) we have \(V_{\xi}^{\zeta}(h_{<t})\geq V_{\xi}^{\overline{\pi_{t}}}(h_{<t})-\epsilon_{t}\) with \(\epsilon_{t}\to 0\), and for all \(\nu\in\mathcal{M}\)_

\[V_{\nu}^{*}(h_{<t})-V_{\nu}^{\overline{\pi_{t}}}(h_{<t})\to 0\ \ \text{as}\ \ t\to\infty\ \ \mu^{\text{\tiny{$\pi$-almost surely}}}\] (4)

_then_

\[V_{\nu}^{*}(h_{<t})-V_{\nu}^{\pi_{S}}(h_{<t})\to 0\ \ \text{as}\ \ t\to\infty\ \ \mu^{\text{\tiny{$\pi$-almost surely}}}\]

_If \(\pi=\pi_{S}\) and Equation 4 holds for all \(\mu\in\mathcal{M}\), then \(\pi_{S}\) is strongly asymptotically optimal in the class \(\mathcal{M}\)._

Proof.: \[0 \leq w(\mu|h_{<t})\left(V_{\mu}^{*}(h_{<t})-V_{\mu}^{\pi_{S}}(h_{ <t})\right)\] (5) \[\leq\sum_{\nu\in\mathcal{M}}w(\nu|h_{<t})\left(V_{\nu}^{*}(h_{<t} )-V_{\nu}^{\pi_{S}}(h_{<t})\right)\] (6) \[=\sum_{\nu\in\mathcal{M}}w(\nu|h_{<t})V_{\nu}^{*}(h_{<t})-V_{\xi} ^{\pi_{S}}(h_{<t})\] (7) \[\leq\sum_{\nu\in\mathcal{M}}w(\nu|h_{<t})V_{\nu}^{*}(h_{<t})-V_{ \xi}^{\zeta}(h_{<t})\] (8) \[\leq\sum_{\nu\in\mathcal{M}}w(\nu|h_{<t})V_{\nu}^{*}(h_{<t})-V_{ \xi}^{\overline{\pi_{t}}}(h_{<t})+\epsilon_{t}\] (9) \[=\sum_{\nu\in\mathcal{M}}w(\nu|h_{<t})\left(V_{\nu}^{*}(h_{<t})- V_{\nu}^{\overline{\pi_{t}}}(h_{<t})\right)+\epsilon_{t}\] (10) \[\to 0\] (11)

Equation 6 comes from adding positive terms. Equations 7 and 10 comes from the linearity of the value function. Equation 8 comes from \(\pi_{S}\) being one step optimal then following \(\zeta\) and. Equation 9 comes from the assumptions. Lastly, 11 comes from Equation 4 and [14, Lem.5.28ii].

\(w(\mu|h_{<t})\nrightarrow 0\) as \(h_{<t}\) is generated from \(\mu^{\pi}\) (for more details see Self-Optimizing proof in [14]). Therefore \(V_{\mu}^{*}(h_{<t})-V_{\mu}^{\pi_{S}}(h_{<t})\to 0\)\(\mu^{\text{\tiny{$\pi$-almost surely}}}\).

[MISSING_PAGE_FAIL:17]

Figure 1: Rolling average reward per step for MC-AIXI (green) and MC-Self-AIXI (red).