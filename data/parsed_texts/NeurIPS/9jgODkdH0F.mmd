# Connectivity Shapes Implicit Regularization in Matrix Factorization Models for Matrix Completion

 Zhiwei Bai\({}^{1}\), Jiajie Zhao\({}^{1}\), Yaoyu Zhang\({}^{1}\)

\({}^{1}\) School of Mathematical Sciences, Institute of Natural Sciences, MOE-LSC,

Shanghai Jiao Tong University, Shanghai 200240, P.R. China.

{bai299, zjj0216, zhyy.sjtu}@sjtu.edu.cn.

Corresponding author: zhyy.sjtu@sjtu.edu.cn.

###### Abstract

Matrix factorization models have been extensively studied as a valuable test-bed for understanding the implicit biases of overparameterized models. Although both low nuclear norm and low rank regularization have been studied for these models, a unified understanding of when, how, and why they achieve different implicit regularization effects remains elusive. In this work, we systematically investigate the implicit regularization of matrix factorization for solving matrix completion problems. We empirically discover that the connectivity of observed data plays a crucial role in the implicit bias, with a transition from low nuclear norm to low rank as data shifts from disconnected to connected with increased observations. We identify a hierarchy of intrinsic invariant manifolds in the loss landscape that guide the training trajectory to evolve from low-rank to higher-rank solutions. Based on this finding, we theoretically characterize the training trajectory as following the hierarchical invariant manifold traversal process, generalizing the characterization of Li et al. (2020) to include the disconnected case. Furthermore, we establish conditions that guarantee minimum nuclear norm, closely aligning with our experimental findings, and we provide a dynamics characterization condition for ensuring minimum rank. Our work reveals the intricate interplay between data connectivity, training dynamics, and implicit regularization in matrix factorization models.

## 1 Introduction

Overparameterized models have the capacity to easily fit data with random labels (Zhang et al., 2017, 2021). However, in real-world applications, models with more parameters than training samples still generalize well. This has led researchers to hypothesize that overparameterized models undergo implicit regularization, favoring certain functions as outputs. Overparameterized matrix factorization models, \(\mathbf{f_{\theta}}=\mathbf{A}\mathbf{B}\) with \(\mathbf{\theta}=(\mathbf{A},\mathbf{B}),\mathbf{A},\mathbf{B}\in\mathbb{R}^{d\times d}\), have served as a simplified test-bed for studying this implicit regularization. In the context of matrix completion problems like the Netflix challenge, these models aim to find a low-rank completion of a partially observed matrix \(\mathbf{M}\in\mathbb{R}^{d\times d}\). Prior works have offered seemingly conflicting perspectives on the implicit regularization at play, with some claiming it promotes low nuclear norm (Gunasekar et al., 2017) and others arguing for low rank (Arora et al., 2019; Li et al., 2020; Razin and Cohen, 2020). However, a unified understanding of when, how, and why they achieve different implicit regularization effects remains elusive.

Unlike previous works that focus on either low rank or low nuclear norm regularization,, we systematically investigate the training dynamics and implicit regularization of matrix factorization for matrix completion. Through extensive experiments, we found that a certain connectivity property of observed data plays a key role in the implicit regularization effects. Data connectivity, in the context of this paper, refers to the way observed entries in the matrix are linked through shared rowsor columns. A set of observations is considered connected if there's a path between any two observed entries via other observed entries in the same rows or columns. This concept plays a crucial role in determining the behavior of matrix factorization models, as we will demonstrate throughout this paper. As shown in Fig. 1, we sample observations randomly from a ground truth matrix \(\mathbf{M}^{*}\in\mathbb{R}^{d\times d}\) with \(\text{rank}(\mathbf{M}^{*})<d\) and train models \(\mathbf{f_{\theta}}=\mathbf{AB},\mathbf{A},\mathbf{B}\in\mathbb{R}^{d\times d}\) from small random initialization without any rank constraints. For each observation set, we calculate the solutions with the minimum nuclear norm and minimum rank, which serve as the ground truth benchmarks. These are then compared with the completion matrix obtained by the model. From Fig. 1, we observe that:

(i) **Low rank bias in connected case:** When the observed entries are connected, the model consistently learns the lowest-rank solution.

(ii) **Low nuclear norm bias in certain disconnected case:** When the observed entries are disconnected, the model generally does not find the minimum nuclear norm or lowest-rank solution. However, in the special case where each connected component is a complete bipartite subgraph, the model consistently finds the minimum nuclear norm solution.

To understand how data connectivity modulates the implicit bias, we analyze the loss landscape and optimization dynamics. We find a hierarchy of intrinsic invariant manifolds \(\mathbf{\Omega}_{k}\) of different ranks in the loss landscape. These manifolds constrain the optimization trajectory, causing the model to learn by incrementally ascending through higher ranks. In the disconnected case, additional sub-\(\mathbf{\Omega}_{k}\) invariant manifolds emerge within the \(\mathbf{\Omega}_{k}\) invariant manifold, preventing the model from reaching the global lowest-rank solution. However, we prove that the minimum nuclear norm solution is guaranteed in the disconnected with complete bipartite subgraph case.

The contributions of our work are summarized as follows:

(i) We systematically investigate the influence of data connectivity on the implicit regularization. Our empirical findings indicate that the connectivity of observed data plays a key role in the implicit bias, leading to a transition from favoring solutions with a low nuclear norm to those with a low rank as the data becomes more connected with an increase in observations (refer to Sec. 4).

(ii) We characterize the training dynamics of matrix factorization theoretically, showing that the optimization trajectory follows a _Hierarchical Invariant Manifold Traversal (HIMT)_ process. This generalizes the characterization of Li et al. (2020), whose proposed _Greedy Low-Rank Learning(GLRL)_ algorithm equivalence only corresponding to the connected case (refer to Sec. 5 and Sec. 6.1).

(iii) Regarding the minimum nuclear norm regularization, we establish conditions that provide guarantees closely aligned with our empirical findings, which complement the results of Gunasekar et al. (2017). For the minimum rank regularization, we present a dynamic characterization condition that assures the attainment of the minimum rank solution (refer to Sec. 6.2).

Figure 1: **The connectivity of observed data affects the implicit regularization. The ground truth matrix \(\mathbf{M}^{*}\in\mathbb{R}^{4\times 4}\) has rank ranging from 1 to 3. The sample size \(n\) covers settings where \(n\) is equal to, smaller than, and larger than the \(2rd-r^{2}\) threshold required for exact reconstruction. Darker scatter points indicate a greater number of samples, while lighter points indicate fewer samples. The positions of observed entries are randomly chosen, and the experiment is repeated 10 times for each sample size. (Please refer to Appendix B for additional experiments and detailed methodology.)**

Related works

Norm minimization and rank minimization.Extensive research has been conducted on the implicit regularization of matrix factorization models, focusing on norm minimization and rank minimization. For norm minimization, Gunasekar et al. (2017) proved that gradient flow with infinitesimal initialization converges to the minimum nuclear norm solution in the special case of commutative observations. Ji and Telgarsky (2019); Gunasekar et al. (2018) studied norm minimization regularization in deep linear networks. For rank minimization, numerous works have shown that matrix factorization models favor low-rank solutions. Arora et al. (2019); Gidel et al. (2019); Gissin et al. (2019); Razin and Cohen (2020); Jiang et al. (2023); Belabbas (2020) investigated how infinitesimal initialization of gradient flow encourages low rank in specific settings. Li et al. (2020) showed that under certain assumptions, matrix factorization dynamics are equivalent to a greedy low-rank learning heuristic. Li et al. (2018); Stoger and Soltanolkotabi (2021); Jin et al. (2023) established low-rank recovery guarantees for matrix sensing problems under the Restricted Isometry Property (RIP) condition. Zhang et al. (2022, 2023) studied a broader class of model rank minimization for nonlinear models, of which the matrix factorization model is a special case.

Nonlinear dynamics.The initialization scale can significantly influence the implicit regularization of neural networks. Large initialization typically leads to linear dynamics (Jacot et al., 2018) and poor generalization (Chizat et al., 2019), while small initialization induces nonlinear dynamics (Luo et al., 2021). In this work, we focus on the case of infinitesimal initialization, which corresponds to highly nonlinear dynamics. An important characteristic of nonlinear neural network dynamics is the phenomenon of condensation (Luo et al., 2021; Zhou et al., 2022), where the network's effective complexity is small. The low-rank \(\mathbf{\Omega}_{k}\) invariant manifolds we propose are essentially a manifestation of condensation. Zhang et al. (2021, 2022); Bai et al. (2022); Fukumizu et al. (2019); Simsek et al. (2021) established the embedding principle of the loss landscape of neural networks and empirically demonstrated that the training process traverses critical points embedded from smaller subnetworks. Jacot et al. (2021) conjectured a saddle to saddle dynamics for deep linear networks, which is conceptually analogous to the dynamics characterization in this work.

## 3 Preliminaries

Matrix completion problem.This study focuses on the matrix completion problem, which involves estimating missing entries within a partially observed matrix. Given an incomplete matrix \(\mathbf{M}\in\mathbb{R}^{d\times d}\), the goal is to predict the entirety of \(\mathbf{M}\) based on its observed elements. The set of observed entries is represented as \(S=\{(i_{k},j_{k}),\mathbf{M}_{i_{k},j_{k}}\}_{k=1}^{n}\), where \((i_{k},j_{k})\) indicates the row and column indices, and \(\mathbf{M}_{i_{k},j_{k}}\) is the corresponding value assumed non-zero in the matrix. The set of observed indices is defined as \(S_{\mathbf{x}}=\{(i_{k},j_{k})\}_{k=1}^{n}\). Entries that are not observed, denoted by \(\star\), are considered missing or unknown. The positions of observed elements in the matrix \(\mathbf{M}\) are defined by a binary observation matrix \(\mathbf{P}\), where \(\mathbf{P}_{ij}=1\) indicates that \(\mathbf{M}_{ij}\) is observed, and \(\mathbf{P}_{ij}=0\) indicates that \(\mathbf{M}_{ij}\) is unobserved.

Matrix factorization model.Matrix factorization is a prevalent approach for addressing the matrix completion problem. It reconstructs the matrix \(\mathbf{W}\in\mathbb{R}^{d\times d}\) through the product \(\mathbf{W}=\mathbf{A}\mathbf{B}\), where \(\mathbf{A}\in\mathbb{R}^{d\times r}\) and \(\mathbf{B}\in\mathbb{R}^{r\times d}\). This work studies the overparameterized scenario with \(r=d\), aiming to understand the implicit regularization effect in the absence of explicit rank restrictions, paralleling prior research (Gunasekar et al., 2017; Arora et al., 2019; Li et al., 2020; Jin et al., 2023). In this work, we focus on the asymmetric factorization, which can be represented as a parametric model:

\[\mathbf{f}_{\mathbf{\theta}}=\mathbf{A}\mathbf{B},\quad\mathbf{A},\mathbf{B}\in\mathbb{R}^{d\times d}. \tag{1}\]

The matrix factorization model parameters are denoted by \(\mathbf{\theta}=(\mathbf{A},\mathbf{B})\), identified with its vectorized form \(\mathrm{vec}(\mathbf{\theta})\in\mathbb{R}^{2d^{2}}\). The augmented matrix is \(\mathbf{W}_{\mathrm{aug}}^{\top}=\begin{bmatrix}\mathbf{A}^{\top}&\mathbf{B}\end{bmatrix} ^{\top}\in\mathbb{R}^{d\times 2d}\), and \(\mathrm{row}(\mathbf{A})\) and \(\mathrm{col}(\mathbf{B})\) denote the row and column spaces of \(\mathbf{A}\) and \(\mathbf{B}\), respectively. The augmented matrix \(\mathbf{W}_{\mathrm{aug}}\) plays a crucial role in our subsequent analysis, particularly in characterizing the intrinsic invariant manifolds \(\mathbf{\Omega}_{k}\) of the optimization process. Specifically, it allows us to establish the relationship \(\text{rank}(\mathbf{A})=\text{rank}(\mathbf{B}^{\top})=\text{rank}(\mathbf{W}_{\mathrm{aug}})\), which is important to understanding the invariance property under gradient flow.

Loss function.The learning process for the parameters \(\mathbf{\theta}=(\mathbf{A},\mathbf{B})\) involves minimizing a loss function that measures the difference between observed and estimated entries. In this work, we focus on the mean squared error, and the empirical risk is thus formulated as

\[R_{S}(\mathbf{\theta})=\frac{1}{n}\|(\mathbf{A}\mathbf{B}-\mathbf{M})_{S_{\mathbf{x}}}\|_{F}^{2}:= \frac{1}{n}\sum_{k=1}^{n}(\mathbf{a}_{i_{k}}\cdot\mathbf{b}_{\cdot,j_{k}}-\mathbf{M}_{i_{k },j_{k}})^{2}, \tag{2}\]

where \(\mathbf{a}_{i}\) and \(\mathbf{b}_{\cdot,j}\) represent the \(i\)-th row and \(j\)-th column of matrix \(\mathbf{A}\) and \(\mathbf{B}\), respectively. The residual matrix \(\delta\mathbf{M}=(\mathbf{A}\mathbf{B}-\mathbf{M})_{S_{\mathbf{x}}}\) has elements \(\delta\mathbf{M}_{ij}=(\mathbf{A}\mathbf{B})_{ij}-\mathbf{M}_{ij}\) for \((i,j)\in S_{\mathbf{x}}\) and \(\delta\mathbf{M}_{ij}=0\) for \((i,j)\notin S_{\mathbf{x}}\). The training dynamics follow the gradient flow of \(R_{S}(\mathbf{\theta})\):

\[\frac{\mathrm{d}\mathbf{\theta}}{\mathrm{d}t}=-\nabla_{\mathbf{\theta}}R_{S}(\mathbf{ \theta}),\quad\mathbf{\theta}(0)=\mathbf{\theta}_{0}. \tag{3}\]

In all experiments, \(\mathbf{\theta}_{0}\sim N(0,\sigma^{2})\) is initialized from a Gaussian distribution with mean 0 and small variance \(\sigma^{2}\). We use gradient descent with a small learning rate to approximate the gradient flow dynamics (Please refer to Appendix B.1 for the detailed experiment setup).

## 4 Connectivity affects implicit regularization

In this section, we define connectivity and present experimental results on implicit regularization for connected and disconnected observational data.

**Definition 1** (**Associated Observation Graph**).: _Given a incomplete matrix \(\mathbf{M}\) to be completed and its observation matrix \(\mathbf{P}\), the associated observation graph \(G_{\mathbf{M}}\) is the bipartite graph with adjacency matrix \(\begin{bmatrix}\mathbf{0}&\mathbf{P}^{\top}\\ \mathbf{P}&\mathbf{0}\end{bmatrix}\), with isolated vertices removed._

**Definition 2** (**Connectivity**).: _Given a incomplete matrix \(\mathbf{M}\) to be completed, it is considered connected if its associated observation graph \(G_{\mathbf{M}}\) is connected; otherwise, it is disconnected. The connected components of \(\mathbf{M}\) are defined as the connected components of \(G_{\mathbf{M}}\)._

The connectivity of the graph, as defined above, reflects the connectivity of the observed data. Appendix A Sec. A.2 provides a detailed discussion on the equivalent definition of connectivity.

In the case of disconnectivity, there is a special case where each connected component has full observations, characterized by disconnectivity with complete bipartite components.

**Definition 3** (**Disconnectivity with Complete Bipartite Components**).: _A incomplete matrix \(\mathbf{M}\) is considered disconnected with complete bipartite components if its associated observation graph \(G_{\mathbf{M}}\) is disconnected and each connected component forms a complete bipartite subgraph._

We present examples to demonstrate how connectivity influences the characteristics of the learned solutions. Consider three matrices to be completed, each obtained by adding one more observation to the previous matrix: \(\mathbf{M}_{1}\) (disconnected), \(\mathbf{M}_{2}\) (disconnected with complete bipartite components), and \(\mathbf{M}_{3}\) (connected). Fig. A1 of Appendix B illustrates the associated graphs \(G_{\mathbf{M}}\).

\[\mathbf{M}_{1}=\begin{bmatrix}1&2&\star\\ 3&\star&\star\\ \star&\star&5\end{bmatrix},\mathbf{M}_{2}=\begin{bmatrix}1&2&\star\\ 3&4&\star\\ \star&\star&5\end{bmatrix},\mathbf{M}_{3}=\begin{bmatrix}1&2&\star\\ 3&4&\star\\ 6&\star&5\end{bmatrix}. \tag{4}\]

Figs. 2(a-b) compare the learned matrices with the ground truth (GT) solutions having the smallest nuclear norm and rank. For disconnected \(\mathbf{M}_{1}\) (blue bars), the learned solution achieves neither the smallest nuclear norm nor rank. For disconnected \(\mathbf{M}_{2}\) with complete bipartite components (green bars), the learned matrix has the smallest nuclear norm but not rank. For connected \(\mathbf{M}_{3}\) (red bars), the lowest rank-2 solution is not unique; the model identifies a particular lowest rank-2 solution, but it does not correspond to the one with the minimum nuclear norm.

To thoroughly study all possible cases, we examine all sampling patterns of the \(3\times 3\) matrix completion. Fig. 2(c) shows that the model consistently learns the lowest-rank solution for connected sampling patterns but fails to do so for disconnected patterns. Fig. 2(d) further verifies the impact of connectivity on low-rank matrix recovery by comparing the reconstruction error for 100 randomly sampled rank-1 matrices using two connected sampling patterns (red and blue dots) and one disconnected sampling pattern (green dots). The model consistently achieves small reconstruction errors under connected sampling patterns, while the error is significantly larger for the disconnected pattern.

These empirical results demonstrate an implicit preference for low rank induced by connectivity and a preference for low nuclear norm in a particular kind of disconnection. In the following section, we will investigate the training dynamics under both connected and disconnected scenarios.

## 5 Training dynamics in connected and disconnected cases

### Connected case

This section empirically demonstrates the detailed dynamics of connected observed data. Fig. 3(a) shows the connected target matrix \(\mathbf{M}\) with a single unknown element denoted by \(\star\). The rank of \(\mathbf{M}\) is at least three and equals three if and only if \(\star=1.2\).

Learning lowest-rank solution.We initialize \(\mathbf{A}\) and \(\mathbf{B}\) with different scales and record the singular values of the learned matrix. As depicted in Fig. 3(b), when starting with larger initialization, the learned solutions are almost always rank-4. Conversely, as the initialization scale decreases, the first three singular values of the learned solution are consistently maintained in magnitude, but the fourth singular value keeps decreasing, resulting in the model learning the lowest rank-3 solution.

Traversing progressive optima at each rank.For a small random initialization (Gaussian distribution with mean 0 and variance \(10^{-16}\)), the loss curves exhibit a steady, stepwise decline (Fig. 3(c)). The flat periods correspond to small gradient norms, indicating potential saddle points (Fig. 3(d)). We compare the matrices learned at these saddle points with the optimal approximation of each rank and plot their difference in Fig. 3(d), which is very small. These findings suggest that the model starts near \(\mathbf{0}\) (rank-0) and progressively finds optimal approximations within rank-1, rank-2, and higher-rank manifolds until reaching a global minimum.

Alignment of the row space of \(\mathbf{A}\) and the column space of \(\mathbf{B}\).Starting with small initialization, we track the rank (number of significantly non-zero singular values) of \(\mathbf{W}=\mathbf{A}\mathbf{B}\), \(\mathbf{A}\), \(\mathbf{B}\), and the augmented matrix \(\mathbf{W}_{\mathrm{aug}}\) during the training process. We observe that the rank gradually increases, with singular values growing rapidly one after another (Fig. 3(e-h)). Throughout the entire process, we consistently find that \(\text{rank}(\mathbf{A})=\text{rank}(\mathbf{B}^{\top})=\text{rank}(\mathbf{W}_{\mathrm{aug}})\), which implies that the row space of \(\mathbf{A}\) and the column space of \(\mathbf{B}\) remain aligned at all times. This alignment corresponds to a special structure that we refer to as the "Hierarchical Intrinsic Invariant Manifold" in Sec. 6.1, which plays a crucial role in the overall dynamics of the system.

Figure 2: (a) Nuclear norms of the learned solutions for \(\mathbf{M}_{1}\), \(\mathbf{M}_{2}\), and \(\mathbf{M}_{3}\). Dashed lines represent theoretically computed smallest nuclear norms. (b) Singular values of the learned matrices for \(\mathbf{M}_{1},\mathbf{M}_{2},\mathbf{M}_{3}\). Each set of three bars represents the singular values of a matrix. The thick vertical lines partition significantly nonzero singular values, which serves as the empirical rank. The text (GT) shows the ground truth minimum rank. Mean and standard deviation are recorded over 100 repetitions. (c) All equivalent sampling patterns of the \(3\times 3\) matrix completion problem (see Appendix B for details). Cyan stars marked the case learning the lowest-rank solution. (d) Reconstruction error of the solutions for a \(10\times 10\) matrix reconstruction problem with \(\mathbf{M}^{*}\) randomly sampled at rank \(r=1\) and sample size set to the minimum reconstruction setting \(n=2rd-r^{2}\).

The dynamics of increasing ranks step by step aligns with the description of _Greedy Low Rank Learning (GLRL)_(Li et al., 2020). However, we will show next that when the observed data are disconnected, the learning process is not equivalent to GLRL.

### Disconnected case

In this section, we present a typical experiment in the disconnected situation. As depicted in Fig. 4(a), the target matrix \(\mathbf{M}\) contains four unknown elements denoted by \(\star\) and is disconnected. The rank of \(\mathbf{M}\) is at least one, and there are infinitely many rank-1 solutions.

Alignment of the row space of \(\mathbf{A}\) and the column space of \(\mathbf{B}\).As shown in Fig. 4(b-e), the learning process in the disconnected case is similar to the previous experiment: the model naturally evolves from low-rank to high-rank, with each step increasing a singular value and satisfying \(\text{rank}(\mathbf{A})=\text{rank}(\mathbf{B}^{\top})=\text{rank}(\mathbf{W}_{\text{aug}})\). Fig. 4(f) illustrates that as the initialization scale decreases, the model tends to learn symmetric solutions. However, unlike the connected case, the output does not approach a particular solution as the initialization decreases. For this specific disconnected \(\mathbf{M}\), we will show that every symmetric solution learned is a minimal nuclear norm solution(see Sec. 6.2 Thm. 4). For fewer observations, the experimental phenomena are similar (see Appendix B Fig. B5).

Lowest-rank solution is not learned.Despite the adaptive learning behavior, the final learned solution has rank 2, as evidenced by the two significantly non-zero singular values in Fig. 4(b-d). Examining the dynamics (3), we find that they decouple into two independent systems: one for the 1st and 3rd rows of \(\mathbf{A}\) and columns of \(\mathbf{B}\), and another for the 2nd row of \(\mathbf{A}\) and column of \(\mathbf{B}\). Fig. 4(g) shows that the model first learns the surrounding elements \(1,3,3,9\) (rank-1 saddle point), then learns the middle element \(5\) in the next stage. The decoupling of dynamics is equivalent to the definition of disconnection (see Appendix A Prop. A.4 for proof). In Fig. 4(e), we fixed a rank-1 matrix and explored all nine disconnected sampling patterns with 5 observations. For each pattern, we conducted experiments with small initializations. The loss curves consistently indicate that in disconnected cases, the model learns a sub-optimal solution in the rank-1 manifold, ultimately resulting in a rank-2 solution. This demonstrates that regardless of the specific disconnected sampling pattern, the model fails to achieve the optimal low-rank solution.

Figure 3: (a) The matrix \(\mathbf{M}\) to be completed, with the \(\star\) position unknown. (b) The four singular values of the learned solution at different initialization scale (Gaussian distribution, mean 0, variance from \(10^{0}\) to \(10^{-16}\)). (c) Training loss for 16 connected sampling patterns in a \(4\times 4\) matrix, each covering 1 element and observing the remaining 15 in a fixed rank-3 matrix. (d) Evolution of the \(l_{2}\)-norm of the gradients throughout the training process. The cyan crosses represent the difference between the matrix corresponding to the saddle point and the optimal approximation at each rank. (e-h) Evolution of singular values for matrices \(\mathbf{W},\mathbf{A},\mathbf{B}\), and \(\mathbf{W}_{\text{aug}}\) during training.

Not equivalent to GLRL in disconnected case.We compare the GLRL algorithm (Li et al., 2020) with the matrix factorization model for solving the same matrix completion problem (Fig. 4). Li et al. (2020) claim that the matrix factorization dynamics is mathematically equivalent to the GLRL algorithm under reasonable assumptions. While GLRL learns the same rank-1 saddle point shown in Fig. 4(g) in the first stage, it then fills unobserved elements with 0, resulting in a unique rank-2 solution (Fig. 4(h)). In contrast, the matrix factorization model learns symmetric solutions with some degree of freedom depending on the random seed (Fig. 4(f)). The key difference is that the first critical point (Fig. 4(g)) reached by the trajectory is a sub-optimal and not a second-order stationary point of the rank-1 manifold as assumed by Li et al. (2020). Therefore, the equivalence assumption between GLRL and matrix factorization does not hold in the disconnected case.

## 6 Theoretical analysis of training dynamics and implicit regularization

### Characterization of training dynamics

Matrix factorization models exhibit a distinctive adaptive learning behavior, progressively evolving from low rank to high rank. Understanding this phenomenon is rooted in grasping the global dynamics of matrix factorization models, where the role of intrinsic invariant manifolds becomes critical.

**Proposition 1** (Hierarchical Intrinsic Invariant Manifold (Hilm)).: _(see Appendix A Prop. A.1 for Proof) Let \(\mathbf{f_{\theta}}=\mathbf{AB}\) be a matrix factorization model and \(\{\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k}\}\) be \(k\) linearly independent vectors. Define the manifold \(\mathbf{\Omega}_{k}\) as \(\mathbf{\Omega}_{k}:=\mathbf{\Omega}_{k}(\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k})=\{ \mathbf{\theta}=(\mathbf{A},\mathbf{B})\mid\operatorname{row}(\mathbf{A})=\operatorname{col}( \mathbf{B})=\operatorname{span}\{\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k}\}\}\). The manifold \(\mathbf{\Omega}_{k}\) possesses the following properties:_

_(i) **Invariance under Gradient Flow:** Given data \(S\) and the gradient flow dynamics \(\dot{\mathbf{\theta}}=-\nabla R_{S}(\mathbf{\theta})\), if the initial point \(\mathbf{\theta}_{0}\in\mathbf{\Omega}_{k}\), then \(\mathbf{\theta}(t)\in\mathbf{\Omega}_{k}\) for all \(t\geq 0\)._

_(ii) **Intrinsic Property:**\(\mathbf{\Omega}_{k}\) is a data-independent invariant manifold, meaning that for any data \(S\), \(\mathbf{\Omega}_{k}\) remains invariant under the gradient flow dynamics._

_(iii) **Hierarchical Structure:** The manifolds \(\mathbf{\Omega}_{k}\) form a hierarchy: \(\mathbf{\Omega}_{0}\subsetneqq\mathbf{\Omega}_{1}\subsetneqq\cdots\subsetneqq\mathbf{ \Omega}_{k-1}\subsetneqq\mathbf{\Omega}_{k}\)._

Figure 4: (a) The matrix to be completed, with unknown entries marked by \(\star\). (b-d) Evolution of singular values for \(\mathbf{A}\), \(\mathbf{B}\), and \(\mathbf{W}_{\mathrm{aug}}\) during training. (e) Training loss for 9 disconnected sampling patterns in a \(3\times 3\) matrix, each covering 4 elements and observing the remaining 5 in a fixed rank-1 matrix. (f) Learned values at symmetric positions \((1,2)\) and \((2,1)\) under varying initialization scales (zero mean, varying variance). Each point represents one of ten random experiments per variance; labels show initialization variance. Other symmetric positions exhibit similar behavior. (g) Learned output at the saddle point corresponding to the red dot in (e). (h) Final learned solution of the GLRL algorithm (Li et al., 2020).

Figs. 3(f-h) and Figs. 4(b-d) show that the training process with small initialization consistently satisfies \(\text{rank}(\mathbf{A})=\text{rank}(\mathbf{B}^{\top})=\text{rank}(\mathbf{W}_{\text{aug}})\), aligning with the \(\mathbf{\Omega}_{k}\) invariant manifold. Since a non-zero initialization in practice, the training trajectory is close to the \(\mathbf{\Omega}_{k}\) invariant manifold, approaches a critical point, and transitions to the next level invariant manifold without getting trapped.

In both connected and disconnected scenarios, we observe a step-by-step hierarchical \(\mathbf{\Omega}_{k}\) invariant manifold traversal. In the connected case, at each level we observe that the model reaches an optimal solution (Fig. 3). However, in the disconnected case, we can prove that each connected component induces a sub-\(\mathbf{\Omega}_{k}\) invariant manifold, leading to the experimentally observed sub-optimal solution (see Fig. 4).

**Proposition 2** (**Intrinsic Sub-\(\mathbf{\Omega}_{k}\) Invariant Manifold**).: _(see Appendix A Prop. A.2 for Proof) Let \(\mathbf{f}_{\mathbf{\theta}}=\mathbf{A}\mathbf{B}\) be a matrix factorization model, \(\mathbf{M}\) be an incomplete matrix and \(\mathbf{\Omega}_{k}\) be an invariant manifold defined in Prop. 1. If \(\mathbf{M}\) is disconnected with \(m\) connected components, then there exist \(m\) sub-\(\mathbf{\Omega}_{k}\) manifolds \(\mathbf{\omega}_{k}\) such that \(\mathbf{\omega}_{k}\subsetneqq\mathbf{\Omega}_{k}\), each possessing the following properties:_

_(i) **Invariance under Gradient Flow:** Given data \(S\) and the gradient flow dynamics \(\dot{\mathbf{\theta}}=-\nabla R_{S}(\mathbf{\theta})\), if the initial point \(\mathbf{\theta}_{0}\in\mathbf{\omega}_{k}\), then \(\mathbf{\theta}(t)\in\mathbf{\omega}_{k}\) for all \(t\geq 0\)._

_(ii) **Intrinsic Property:**\(\mathbf{\omega}_{k}\) _is a data-value-independent invariant manifold, meaning that for a fixed sampling pattern in_ \(\mathbf{M}\) _and any observed values_ \(S\)_,_ \(\mathbf{\omega}_{k}\) _remains invariant under the gradient flow._

_(iii) **Strict Subset Relation:** The output set \(\{\mathbf{f}_{\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\omega}_{k}\}\) is a proper subset of \(\{\mathbf{f}_{\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\Omega}_{k}\}\), namely, \(\{\mathbf{f}_{\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\omega}_{k}\}\subsetneqq\{\mathbf{f}_ {\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\Omega}_{k}\}\)._

Fig. 5(a) illustrates the trajectory of the experiment in Fig. 4. In the disconnected case, sub-\(\mathbf{\Omega}_{k}\) invariant manifolds exist and attract the dynamics, leading the model to learn sub-optimal solutions on the entire \(\mathbf{\Omega}_{k}\) invariant manifold. In fact, we can prove that these sub-optimal solutions are necessarily strict saddle points. This loss landscape result extends Theorem 5.10 from Li et al. (2020), which established the findings for the specific case of symmetric matrix factorization models (see Appendix A Sec. A.3 for a detailed discussion).

**Theorem 1** (**Loss Landscape**).: _(see Appendix A Thm. A.3 for Proof) Given any data \(S\), the critical points of \(R_{S}(\mathbf{\theta})\) are either strict saddle points or global minima._

Gradient descent easily escapes saddle points (Lee et al., 2016, 2019). Fig. 5(b) shows that when the model escapes a saddle point, the parameters initially appear chaotic but align in one direction after some time, consistent with the "condensation" phenomenon in neural networks (Luo et al., 2021; Zhou et al., 2022). For matrix factorization models, by meticulously analyzing the Hessian matrix structure (see Appendix A.5), we find that this alignment corresponds to an \(\mathbf{\Omega}_{1}\) invariant manifold,

Figure 5: (a) Illustrated trajectories for the experiment in Fig. 4. The blue line represents the trajectory converging to the lowest-rank solution, and the red line represents the actual trajectory experienced by the model. (b) The parameter trajectory escaping from a second-order stationary point to reach the next critical point for the experiment in Fig. 3. The 8 scatter points represent the 4 row vectors of matrix \(\mathbf{A}\) and the 4 column vectors of matrix \(\mathbf{B}\). For ease of visualization, we randomly project them onto two dimensions and plot them in polar coordinates.

resulting in a rank increase of one at a time. Under reasonable assumptions, we prove that the training trajectory follows the \(\mathbf{\Omega}_{k}\) invariant manifold step by step.

**Assumption 1** (Unique Top Singular Value).: _Let \(\delta\mathbf{M}=(\mathbf{A}_{c}\mathbf{B}_{c}-\mathbf{M})_{S_{\mathbf{m}}}\) be the residual matrix at the critical point \(\mathbf{\theta}_{c}=(\mathbf{A}_{c},\mathbf{B}_{c})\). Assume that the largest singular value of \(\delta\mathbf{M}\) is unique._

**Assumption 2** (Second-order Stationary Point).: _Let \(\mathbf{\Omega}\) be an \(\mathbf{\Omega}_{k}\) invariant manifold or sub-\(\mathbf{\Omega}_{k}\) invariant manifold defined in Prop. 1 or 2. Assume \(\mathbf{\theta}_{c}\) is a second-order stationary point within \(\mathbf{\Omega}\), i.e., \(\nabla R_{S}(\mathbf{\theta}_{c})=0\) and \(\mathbf{\theta}^{\top}\nabla^{2}R_{S}(\mathbf{\theta}_{c})\mathbf{\theta}\geq 0\) for all \(\mathbf{\theta}\in\mathbf{\Omega}\)._

**Theorem 2** (Transition to the Next Rank-level Invariant Manifold).: _(see Appendix A Thm. A.4 for proof) Consider the dynamics \(\dot{\mathbf{\theta}}=-\nabla R_{S}(\mathbf{\theta})\). Let \(\varphi(\mathbf{\theta}_{0},t)\) denote the value of \(\mathbf{\theta}(t)\) when \(\mathbf{\theta}(0)=\mathbf{\theta}_{0}\). Let \(\mathbf{\Omega}\) be an \(\mathbf{\Omega}_{k}\) or sub-\(\mathbf{\Omega}_{k}\) invariant manifold. Let \(\mathbf{\theta}_{c}\in\mathbf{\Omega}\) be a critical point satisfying Assump. 1 and 2. Then, for randomly selected \(\mathbf{\theta}_{0}\), with probability 1 with respect to \(\mathbf{\theta}_{0}\), the limit_

\[\tilde{\varphi}(\mathbf{\theta}_{c},t):=\lim_{\alpha\to 0}\varphi\left(\mathbf{ \theta}_{c}+\alpha\mathbf{\theta}_{0},t+\frac{1}{\lambda_{1}}\log\frac{1}{\alpha}\right) \tag{5}\]

_exists and falls into an invariant manifold \(\mathbf{\Omega}_{k+1}\). Here \(\lambda_{1}\) is the top eigenvalue of \(-\nabla^{2}R_{S}(\mathbf{\theta}_{c})\)._

Proof sketch.: The main idea is to analyze the local dynamics near the critical point \(\mathbf{\theta}_{c}\). The nonlinear dynamics can be approximated linearly in the vicinity of \(\mathbf{\theta}_{c}\): \(\frac{\mathrm{d}\mathbf{\theta}}{\mathrm{d}t}\approx\mathbf{H}(\mathbf{\theta}_{0}-\mathbf{ \theta}_{c})\), where \(\mathbf{H}=-\nabla^{2}R_{S}(\mathbf{\theta}_{c})\) is the negative Hessian matrix. For exact linear approximation, the solution is: \(\mathbf{\theta}(t)=e^{t\mathbf{H}}(\mathbf{\theta}_{0}-\mathbf{\theta}_{c})+\mathbf{\theta}_{c}\). Let \(\lambda_{1}>\lambda_{2}>...>\lambda_{s}\) be the eigenvalues of \(\mathbf{H}\), with corresponding eigenvectors \(\mathbf{q}_{ij}\). We can express \(\mathbf{\theta}(t)\) as: \(\mathbf{\theta}(t)=\sum_{i=1}^{s}\sum_{j=1}^{l_{i}}e^{\lambda_{i}t}(\mathbf{\theta}_{ 0}-\mathbf{\theta}_{c},\mathbf{q}_{ij})\mathbf{q}_{ij}+\mathbf{\theta}_{c}\). For sufficiently large \(t_{0}\), the dynamics follows a dominant eigenvalue dynamics: \(\mathbf{\theta}(t_{0})=\sum_{j=1}^{l_{1}}e^{\lambda_{1}t_{0}}(\mathbf{\theta}_{0}-\mathbf{ \theta}_{c},\mathbf{q}_{1j})\mathbf{q}_{1j}+O(e^{\lambda_{2}t_{0}})\). Through detailed analysis of the eigenvalues and eigenvectors of the Hessian matrix (please refer to Lemma A.2-A.4 of Appendix A), we show that if the largest singular value of residual matrix \(\delta\mathbf{M}\) at \(\mathbf{\theta}_{c}\) is unique and \(\mathbf{\theta}_{c}\) is a second-order stationary point within \(\mathbf{\Omega}\), the first principal component \(\sum_{j=1}^{l_{1}}e^{\lambda_{1}t_{0}}(\mathbf{\theta}_{0}-\mathbf{\theta}_{c},\mathbf{q}_{ 1j})\mathbf{q}_{1j}\) will happen to be an \(\mathbf{\Omega}_{1}\) invariant manifold. Consequently, escaping \(\mathbf{\theta}_{c}\) increases the rank by 1, entering \(\mathbf{\Omega}_{k+1}\). 

**Remark**.: _Assumption. 1 ensures that upon departing from a critical point \(\mathbf{\theta}_{c}\), the trajectory is constrained to escape along a single dominant eigendirection corresponding to the largest singular value. This assumption holds for randomly generated matrix with probability 1, making it a reasonable condition in most practical scenarios. In Sec A.7 of Appendix A, we provide an special example to illustrate the situation where Assump. 1 does not hold._

**Remark**.: _To ensure the escape direction falls within the \(\mathbf{\Omega}_{k+1}\) invariant manifold, the Hessian's top eigenvectors must satisfy \(rank(\mathbf{A})=rank(\mathbf{B}^{\top})=rank(\mathbf{W}_{\text{aug}})\). The condition that \(\mathbf{\theta}_{c}\) is a second-order stationary point within \(\mathbf{\Omega}\) in Assump. 2 guarantees this Hessian structure. Our Assump. 2 is more general than conditions proposed by Li et al. (2020), as it remains valid across both connected and disconnected configurations. Empirical findings (Figs. 3 and 4) indicate that this assumption consistently holds in practical scenarios._the convergence characteristics within each \(\mathbf{\Omega}_{k}\) invariant manifold, which is an endeavor we leave for future work. Despite this, our insights into the system's dynamics, i.e., hierarchical invariant manifold traversal, allow us to assert that if a trajectory successfully navigates through the optimal on each rank-level invariant manifold \(\mathbf{\Omega}_{k}\), a solution of minimal rank can be achieved naturally.

**Theorem 3** (Minimum Rank).: _(see Appendix A Thm. A.5 for proof) Consider the dynamics \(\dot{\mathbf{\theta}}=-\nabla R_{S}(\mathbf{\theta})\), where \(\mathbf{\theta}(t)=(\mathbf{A}(t),\mathbf{B}(t))\), and denote \(\mathbf{W}_{t}=\mathbf{A}(t)\mathbf{B}(t)\). Assume \(\mathbf{W}_{t}\) achieves an optimal within each invariant manifold \(\mathbf{\Omega}_{k}\). For a full rank initialization \(\mathbf{W}_{0}\), if the limit \(\widehat{\mathbf{W}}=\lim_{\alpha\to 0}\mathbf{W}_{\infty}(\alpha\mathbf{W}_{0})\) exists and is a global optimum with \(\widehat{\mathbf{W}}_{ij}=\mathbf{M}_{ij}\) for all \((i,j)\in S_{\mathbf{x}}\), then_

\[\widehat{\mathbf{W}}\in\operatorname*{argmin}_{\mathbf{W}}\operatorname*{rank}(\mathbf{W} )\quad\text{s.t.}\quad\mathbf{W}_{ij}=\mathbf{M}_{ij},\forall(i,j)\in S_{\mathbf{x}}. \tag{6}\]

For a disconnected matrix \(\mathbf{M}\), our theoretical results (Prop. 2) and experiments (Fig. 4) confirm the existence of sub-\(\mathbf{\Omega}_{k}\) invariant manifolds. These manifolds attract the training trajectory, leading to sub-optimal solutions and preventing convergence to the lowest-rank solution.

However, in a specific disconnected case, such as disconnection with complete bipartite components, as illustrated in Figs. 1 and 2, the minimum nuclear norm may still serve as a characterization. Gunasekar et al. (2017) proved a special case: if the observations are commutative, then the symmetric model will learn the minimum nuclear norm solution. Intriguingly, for the example \(\mathbf{M}_{2}\) in Eq. (4), even though the observations are not commutative, the model still learns a minimum nuclear norm solution. In fact, we can prove the following result, which aligns well with practical experiments.

**Theorem 4** (Minimum Nuclear Norm Guarantee).: _(see Appendix A Thm. A.6 for proof) Consider the dynamics \(\dot{\mathbf{\theta}}=-\nabla R_{S}(\mathbf{\theta})\), where \(\mathbf{\theta}(t)=(\mathbf{A}(t),\mathbf{B}(t))\), and let \(\mathbf{W}_{t}=\mathbf{A}(t)\mathbf{B}(t)\). If the observation graph associated with the incomplete matrix \(\mathbf{M}\) is disconnected with complete bipartite components, and if for a full rank initialization \(\mathbf{W}_{0}\), the limit \(\widehat{\mathbf{W}}=\lim_{\alpha\to 0}\mathbf{W}_{\infty}(\alpha\mathbf{W}_{0})\) exists and is a global optimum with \(\widehat{\mathbf{W}}_{ij}=\mathbf{M}_{ij}\) for all \((i,j)\in S_{\mathbf{x}}\), then_

\[\widehat{\mathbf{W}}\in\operatorname*{argmin}_{\mathbf{W}}\|\mathbf{W}\|_{*}\quad\text{s.t.}\quad\mathbf{W}_{ij}=\mathbf{M}_{ij},\forall(i,j)\in S_{\mathbf{x}}. \tag{7}\]

## 7 Conclusion and future work

This study presents a comprehensive experimental and theoretical investigation of matrix factorization models. The primary objective was to develop a cohesive framework for understanding the conditions, mechanisms, and reasons behind the diverse implicit regularization effects exhibited by matrix factorization models. A key finding of this research is the pivotal role of the connectivity of observed data in shaping the implicit regularization behavior. To elucidate this phenomenon, we identified the significance of hierarchical invariant manifold traversal within the training dynamics.

Our experiments (Figs. 1, 2, 3) provide strong evidence that connected observed data leads to minimum-rank solutions, as the model learns the optimal of the \(\mathbf{\Omega}_{k}\) invariant manifold. However, further investigation is needed to uncover the underlying mechanisms by which connectivity facilitates optimal attainment across different \(\mathbf{\Omega}_{k}\) invariant manifolds. Additionally, the trade-offs between initialization scale and training efficiency warrant further research, as certain cases may require extremely small initialization, potentially impacting training speed (see Appendix B Sec. B.4).

Generalizing the insights gained from matrix factorization models to other architectures is also an important avenue for future work. Our preliminary experiments indicate that the learning phenomenon from low rank to high rank persists in deep multi-layer matrix factorization and the query-key factorization model in Transformer attention mechanisms (see Appendix B Figs. B9, B11). These findings suggest that the hierarchical invariant manifold traversal process uncovered in our study may have broader implications and merit further exploration.

## Acknowledgments and Disclosure of Funding

This work is sponsored by the National Natural Science Foundation of China Grant No. 12101402, the National Key R&D Program of China Grant No. 2022YFA1008200, the Lingang Laboratory Grant No. LG-QS-202202-08, Shanghai Municipal of Science and Technology Major Project No. 2021SHZDZX0102.

## References

* Li et al. (2020) Z. Li, Y. Luo, K. Lyu, Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning, in: International Conference on Learning Representations, 2020.
* Zhang et al. (2017) C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding deep learning requires rethinking generalization. iclr 2017, arXiv preprint arXiv:1611.03530 (2017).
* Zhang et al. (2021) C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding deep learning (still) requires rethinking generalization, Communications of the ACM 64 (2021) 107-115.
* Gunasekar et al. (2017) S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, N. Srebro, Implicit regularization in matrix factorization, Advances in neural information processing systems 30 (2017).
* Arora et al. (2019) S. Arora, N. Cohen, W. Hu, Y. Luo, Implicit regularization in deep matrix factorization, Advances in Neural Information Processing Systems 32 (2019).
* Razin and Cohen (2020) N. Razin, N. Cohen, Implicit regularization in deep learning may not be explainable by norms, Advances in neural information processing systems 33 (2020) 21174-21187.
* Ji and Telgarsky (2019) Z. Ji, M. Telgarsky, Gradient descent aligns the layers of deep linear networks, in: 7th International Conference on Learning Representations, ICLR 2019, 2019.
* Gunasekar et al. (2018) S. Gunasekar, J. D. Lee, D. Soudry, N. Srebro, Implicit bias of gradient descent on linear convolutional networks, Advances in neural information processing systems 31 (2018).
* Gidel et al. (2019) G. Gidel, F. Bach, S. Lacoste-Julien, Implicit regularization of discrete gradient dynamics in linear neural networks, Advances in Neural Information Processing Systems 32 (2019).
* Gissin et al. (2019) D. Gissin, S. Shalev-Shwartz, A. Daniely, The implicit bias of depth: How incremental learning drives generalization, in: International Conference on Learning Representations, 2019.
* Jiang et al. (2023) L. Jiang, Y. Chen, L. Ding, Algorithmic regularization in model-free overparametrized asymmetric matrix factorization, SIAM Journal on Mathematics of Data Science 5 (2023) 723-744.
* Belabbas (2020) M. A. Belabbas, On implicit regularization: Morse functions and applications to matrix factorization, arXiv preprint arXiv:2001.04264 (2020).
* Li et al. (2018) Y. Li, T. Ma, H. Zhang, Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations, in: Conference On Learning Theory, PMLR, 2018, pp. 2-47.
* Stoger et al. (2021) D. Stoger, M. Soltanolkotabi, Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction, Advances in Neural Information Processing Systems 34 (2021) 23831-23843.
* Jin et al. (2023) J. Jin, Z. Li, K. Lyu, S. S. Du, J. D. Lee, Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing, in: International Conference on Machine Learning, PMLR, 2023, pp. 15200-15238.
* Zhang et al. (2022) Y. Zhang, Z. Zhang, L. Zhang, Z. Bai, T. Luo, Z.-Q. J. Xu, Linear stability hypothesis and rank stratification for nonlinear models, arXiv preprint arXiv:2211.11623 (2022).
* Zhang et al. (2023) Y. Zhang, Z. Zhang, L. Zhang, Z. Bai, T. Luo, Z.-Q. J. Xu, Optimistic estimate uncovers the potential of nonlinear models, arXiv preprint arXiv:2307.08921 (2023).
* Jacot and Gabriel (2018) A. Jacot, F. Gabriel, C. Hongler, Neural tangent kernel: Convergence and generalization in neural networks, Advances in neural information processing systems 31 (2018).
* Chizat et al. (2019) L. Chizat, E. Oyallon, F. Bach, On lazy training in differentiable programming, Advances in neural information processing systems 32 (2019).
* Luo et al. (2021) T. Luo, Z.-Q. J. Xu, Z. Ma, Y. Zhang, Phase diagram for two-layer relu neural networks at infinite-width limit, Journal of Machine Learning Research 22 (2021) 1-47.
* Luo et al. (2020)H. Zhou, Z. Qixuan, T. Luo, Y. Zhang, Z.-Q. Xu, Towards understanding the condensation of neural networks at initial training, Advances in Neural Information Processing Systems 35 (2022) 2184-2196.
* Zhang et al. (2021) Y. Zhang, Z. Zhang, T. Luo, Z. J. Xu, Embedding principle of loss landscape of deep neural networks, Advances in Neural Information Processing Systems 34 (2021) 14848-14859.
* Zhang et al. (2022) Y. Zhang, Y. Li, Z. Zhang, T. Luo, Z. J. Xu, Embedding principle: A hierarchical structure of loss landscape of deep neural networks, Journal of Machine Learning 1 (2022) 60-113.
* Bai et al. (2022) Z. Bai, T. Luo, Z.-Q. J. Xu, Y. Zhang, Embedding principle in depth for the loss landscape analysis of deep neural networks, arXiv preprint arXiv:2205.13283 (2022).
* Fukumizu et al. (2019) K. Fukumizu, S. Yamaguchi, Y.-i. Mototake, M. Tanaka, Semi-flat minima and saddle points by embedding neural networks to overparameterization, Advances in Neural Information Processing Systems 32 (2019) 13868-13876.
* Simsek et al. (2021) B. Simsek, F. Ged, A. Jacot, F. Spadaro, C. Hongler, W. Gerstner, J. Brea, Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances, in: Proceedings of the 38th International Conference on Machine Learning, PMLR, 2021, pp. 9722-9732.
* Jacot et al. (2021) A. Jacot, F. Ged, B. Simsek, C. Hongler, F. Gabriel, Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity, arXiv preprint arXiv:2106.15933 (2021).
* Lee et al. (2016) J. D. Lee, M. Simchowitz, M. I. Jordan, B. Recht, Gradient descent only converges to minimizers, in: Conference on learning theory, PMLR, 2016, pp. 1246-1257.
* Lee et al. (2019) J. D. Lee, I. Panageas, G. Piliouras, M. Simchowitz, M. I. Jordan, B. Recht, First-order methods almost always avoid strict saddle points, Mathematical programming 176 (2019) 311-337.
* Du and Lee (2018) S. Du, J. Lee, On the power of over-parametrization in neural networks with quadratic activation, in: International conference on machine learning, PMLR, 2018, pp. 1329-1338.

Proofs of Theoretical Results

In this section, we give all proofs for our theoretical results mentioned in the main text.

### Hierarchical Intrinsic Invariant Manifold and Sub Invariant Manifold

**Proposition A.1** (**Hierarchical Intrinsic Invariant Manifold (Hilm)**).: _Let \(\mathbf{f_{\theta}}=\mathbf{A}\mathbf{B}\) be a matrix factorization model and \(\{\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k}\}\) be \(k\) linearly independent vectors. Define the manifold \(\mathbf{\Omega}_{k}\) as \(\mathbf{\Omega}_{k}:=\mathbf{\Omega}_{k}(\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k})=\{ \mathbf{\theta}=(\mathbf{A},\mathbf{B})\mid\operatorname{row}(\mathbf{A})=\operatorname{col}( \mathbf{B})=\operatorname{span}\{\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k}\}\}\). The manifold \(\mathbf{\Omega}_{k}\) possesses the following properties:_

_(i) **Invariance under Gradient Flow:** Given data \(S\) and the gradient flow dynamics \(\dot{\mathbf{\theta}}=-\nabla R_{S}(\mathbf{\theta})\), if the initial point \(\mathbf{\theta}_{0}\in\mathbf{\Omega}_{k}\), then \(\mathbf{\theta}(t)\in\mathbf{\Omega}_{k}\) for all \(t\geq 0\)._

_(ii) **Intrinsic Property:**\(\mathbf{\Omega}_{k}\) is a data-independent invariant manifold, meaning that for any data \(S\), \(\mathbf{\Omega}_{k}\) remains invariant under the gradient flow dynamics._

_(iii) **Hierarchical Structure:** The manifolds \(\mathbf{\Omega}_{k}\) form a hierarchy: \(\mathbf{\Omega}_{0}\subsetneqq\mathbf{\Omega}_{1}\subsetneqq\cdots\subsetneqq\mathbf{ \Omega}_{k-1}\subsetneqq\mathbf{\Omega}_{k}\)._

Proof.: (i) Invariance under Gradient Flow.

By definition, \(\mathbf{\Omega}_{k}:=\mathbf{\Omega}_{k}(\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k})=\{ \mathbf{\theta}=(\mathbf{A},\mathbf{B})\mid\operatorname{row}(\mathbf{A})=\operatorname{col}( \mathbf{B})=\operatorname{span}\{\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k}\}\}\). Consider the gradient flow dynamics in (8):

\[\left\{\begin{aligned} \dot{\mathbf{a}}_{i}&=-\frac{2}{n} \sum_{j\in I_{i}}(\mathbf{a}_{i}\cdot\mathbf{b}_{\cdot,j}-\mathbf{M}_{ij})\mathbf{b}_{\cdot,j} ^{\top},\\ \dot{\mathbf{b}}_{\cdot,j}&=-\frac{2}{n}\sum_{i\in I_{j}} (\mathbf{a}_{i}\cdot\mathbf{b}_{\cdot,j}-\mathbf{M}_{ij})\mathbf{a}_{i}^{\top},\end{aligned}\right. \tag{8}\]

where \(I_{i}=\{j|\exists i:(i,j)\in S_{\mathbf{x}}\}\), \(I_{j}=\{i|\exists j:(i,j)\in S_{\mathbf{x}}\}\), \(\mathbf{a}_{i}\) and \(\mathbf{b}_{\cdot,j}\) represent the \(i\)-th row and \(j\)-th column of \(\mathbf{A}\) and \(\mathbf{B}\), respectively.

For any \((i,j)\in S_{\mathbf{x}}\), the evolution of \(\mathbf{a}_{i}\) is coupled with \(\mathbf{b}_{\cdot,j}\) for \(j\in I_{i}\). The condition \(\operatorname{row}(\mathbf{A})=\operatorname{col}(\mathbf{B})=\operatorname{span}\{ \mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k}\}\) ensures the existence of \(k\) linearly independent vectors \(\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k}\in\mathbb{R}^{d}\) such that \(\mathbf{a}_{i},\mathbf{b}_{\cdot,j}\in\operatorname{span}\{\mathbf{\alpha}_{1},\cdots,\bm {\alpha}_{k}\}\) for all \(1\leq i,j\leq d\).

Consequently, if \(\mathbf{a}_{i}\) and \(\mathbf{b}_{\cdot,j}\) are initially in \(\operatorname{span}\{\mathbf{\alpha}_{1},\mathbf{\alpha}_{2},\cdots,\mathbf{\alpha}_{k}\}\), they will continue to evolve within this subspace under the gradient flow dynamics. Additionally, for \((i,j)\notin S_{\mathbf{x}}\), the gradients for the corresponding \(\mathbf{a}_{i}\) and \(\mathbf{b}_{\cdot,j}\) will be zero, provided their initial values are zero, maintaining this state throughout the evolution.

(ii) Intrinsic Property.

As demonstrated in part (i), \(\mathbf{\Omega}_{k}\) is invariant under gradient flow dynamics for any dataset \(S\), confirming its status as a data-independent invariant manifold.

(iii) Hierarchical Structure.

Th invariant manifold \(\mathbf{\Omega}_{k}\) encompasses matrices of rank up to \(k\), including those of lower ranks. Consequently, the manifolds exhibit the following hierarchical nesting:

\[\mathbf{\Omega}_{0}\subsetneqq\mathbf{\Omega}_{1}\subsetneqq\cdots\subsetneqq\mathbf{ \Omega}_{k-1}\subsetneqq\mathbf{\Omega}_{k}.\]

**Proposition A.2** (**Intrinsic Sub-\(\mathbf{\Omega}_{k}\) Invariant Manifold**).: _Let \(\mathbf{f_{\theta}}=\mathbf{A}\mathbf{B}\) be a matrix factorization model, \(\mathbf{M}\) be an incomplete matrix and \(\mathbf{\Omega}_{k}\) be an invariant manifold defined in Prop. 1. If \(\mathbf{M}\) is disconnected with \(m\) connected components, then there exist \(m\) sub-\(\mathbf{\Omega}_{k}\) manifolds \(\mathbf{\omega}_{k}\) such that \(\mathbf{\omega}_{k}\subsetneqq\mathbf{\Omega}_{k}\), each possessing the following properties:_

_(i) **Invariance under Gradient Flow:** Given data \(S\) and the gradient flow dynamics \(\dot{\mathbf{\theta}}=-\nabla R_{S}(\mathbf{\theta})\), if the initial point \(\mathbf{\theta}_{0}\in\mathbf{\omega}_{k}\), then \(\mathbf{\theta}(t)\in\mathbf{\omega}_{k}\) for all \(t\geq 0\)._

_(ii) **Intrinsic Property:**\(\mathbf{\omega}_{k}\) is a data-value-independent invariant manifold, meaning that for a fixed sampling pattern in \(\mathbf{M}\) and any observed values \(S\), \(\mathbf{\omega}_{k}\) remains invariant under the gradient flow.__(iii) Strict Subset Relation: The output set \(\{\mathbf{f}_{\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\omega}_{k}\}\) is a proper subset of \(\{\mathbf{f}_{\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\Omega}_{k}\}\), namely, \(\{\mathbf{f}_{\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\omega}_{k}\}\subsetneqq\{\mathbf{f}_{ \mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\Omega}_{k}\}\)._

Proof.: Existence.

Let us consider an incomplete matrix \(\mathbf{M}\) whose associated observational graph is divided into \(m\) connected components, denoted by \(L_{1},L_{2},\ldots,L_{m}\). For each component \(L_{p}\), we define \(S_{\mathbf{x}}^{L_{p}}\) as the subset of observed indices within \(L_{p}\), where \(1\leq p\leq m\) and \(S_{\mathbf{x}}\) is the set of all observed indices.

For each \(L_{p}\), we can identify row indices \(R_{p}\) and column indices \(C_{p}\) corresponding to the observed entries in \(L_{p}\) as follows:

\[R_{p}=\{i|\exists j:(i,j)\in S_{\mathbf{x}}^{L_{p}}\},\quad C_{p}=\{j|\exists i:(i,j)\in S_{\mathbf{x}}^{L_{p}}\}.\]

Here, \(R_{p}\) includes the row indices and \(C_{p}\) includes the column indices of the entries observed in \(L_{p}\).

Define \(\mathbf{A}^{L_{p}}\) and \(\mathbf{B}^{L_{p}}\) as the submatrices of \(\mathbf{A}\) and \(\mathbf{B}\) corresponding to \(R_{p}\) and \(C_{p}\), respectively, and let \(\mathbf{A}^{L_{p}}_{r}\) and \(\mathbf{B}^{L_{p}}_{r}\) be the remaining rows not in \(R_{p}\) and \(C_{p}\).

Let \(\mathbf{\Omega}_{k}:=\mathbf{\Omega}_{k}(\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k})=\{\mathbf{ \theta}=(\mathbf{A},\mathbf{B})\mid\text{row}(\mathbf{A})=\text{col}(\mathbf{B})=\text{span}\{ \mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k}\}\}\) be the given \(\mathbf{\Omega}_{k}\) invariant manifold.

The sub-\(\mathbf{\Omega}_{k}\) invariant manifold associated with the connected component \(L_{p}\) can be defined as

\[\mathbf{\omega}_{k}^{L_{p}}:=\{(\mathbf{\theta}=(\mathbf{A},\mathbf{B}))\mid\text{row}(\mathbf{A} ^{L_{p}})=\text{col}((\mathbf{B}^{L_{p}}))=\text{span}\{\mathbf{\alpha}_{1},\cdots, \mathbf{\alpha}_{k}\},\mathbf{A}^{L_{p}}_{r}=\mathbf{B}^{L_{p}}_{r}=\mathbf{0}\}. \tag{9}\]

It is easy to check \(\mathbf{\omega}_{k}^{L_{p}}\) is a proper subset of \(\mathbf{\Omega}_{k}\).

(i) Invariance under Gradient Flow.

The condition \(\text{row}(\mathbf{A}^{L_{p}})=\text{col}((\mathbf{B}^{L_{p}}))=\text{span}\{\mathbf{ \alpha}_{1},\cdots,\mathbf{\alpha}_{k}\}\) along with \(\mathbf{A}^{L_{p}}_{r}=\mathbf{B}^{L_{p}}_{r}=\mathbf{0}\) guarantees that \(\mathbf{a}_{i},\mathbf{b}_{\cdot,j}\in\text{span}\{\mathbf{\alpha}_{1},\ldots,\mathbf{\alpha} _{k}\}\) for all \((i,j)\in S_{\mathbf{x}}^{L_{p}}\), and \(\mathbf{a}_{i},\mathbf{b}_{\cdot,j}=\mathbf{0}\) for all \((i,j)\notin S_{\mathbf{x}}^{L_{p}}\).

In other words, the sub-\(\mathbf{\Omega}_{k}\) invariant manifold \(\mathbf{\omega}_{k}^{L_{p}}\) is the set of all pairs \((\mathbf{A},\mathbf{B})\) where, for each observed position \((i,j)\) in the connected component \(L_{p}\), the vectors \(\mathbf{a}_{i}\) and \(\mathbf{b}_{\cdot,j}\) lie within the span of \(\{\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k}\}\), and for any position not in \(S_{\mathbf{x}}^{L_{p}}\), the vectors are zero.

Considering the dynamics expressed in equation (8), it is evident that the evolution of \(\mathbf{a}_{i}\) is influenced by \(\mathbf{b}_{\cdot,j}\) for \((i,j)\in S_{\mathbf{x}}^{L_{p}}\). Hence, if \(\mathbf{a}_{i}\) and \(\mathbf{b}_{\cdot,j}\) are initially in the span of \(\{\mathbf{\alpha}_{1},\mathbf{\alpha}_{2},\cdots,\mathbf{\alpha}_{k}\}\), they will continue to evolve within this span under the gradient flow dynamics. Moreover, for positions \((i,j)\notin S_{\mathbf{x}}^{L_{p}}\), we consider the following scenarios:

* For \((i,j)\notin S_{\mathbf{x}}\), since the matrix entry \(\mathbf{M}_{ij}\) does not contribute to the loss \(R_{S}(\mathbf{\theta})\), the gradients for corresponding \(\mathbf{a}_{i}\) and \(\mathbf{b}_{\cdot,j}\) will perpetually be zero. Thus, if their initial values are zero, they will remain zero throughout the evolution.
* For \((i,j)\in S_{\mathbf{x}}\) but not in \(S_{\mathbf{x}}^{L_{p}}\), the dynamics corresponding to different connected components are decoupled. Therefore, if the initial values for \(\mathbf{a}_{i}\) and \(\mathbf{b}_{\cdot,j}\) are zero, they will stay zero during the evolution.

(ii) Intrinsic Property.

As established in (i), the manifold \(\mathbf{\omega}_{k}^{L_{p}}\) is invariant under gradient flow for any data \(S\) with a fixed sampling pattern, qualifying it as a data-value-independent invariant manifold.

(iii) Strict Subset Relation.

The output set \(\{\mathbf{f}_{\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\Omega}_{k}\}\) encompasses all matrices of rank \(k\), whereas \(\{\mathbf{f}_{\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\omega}_{k}^{L_{p}}\}\) is limited to rank-\(k\) matrices with specific row and column indices confined to \(R_{p}\) and \(C_{p}\). Consequently, \(\{\mathbf{f}_{\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\omega}_{k}\}\) forms a strict subset of \(\{\mathbf{f}_{\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\Omega}_{k}\}\), as stated by \(\{\mathbf{f}_{\mathbf{\theta}}\mid\mathbf{\theta}\in\mathbf{\omega}_{k}\}\subsetneqq\{\mathbf{\theta }\mid\mathbf{\theta}\in\mathbf{\Omega}_{k}\}\).

### Connectivity

**Definition A.1** (Associated Observation Graph).: _Given a incomplete matrix \(\mathbf{M}\) to be completed and its observation matrix \(\mathbf{P}\), the associated observation graph \(G_{\mathbf{M}}\) is the bipartite graph with adjacency matrix \(\begin{bmatrix}\mathbf{0}&\mathbf{P}^{\top}\\ \mathbf{P}&\mathbf{0}\end{bmatrix}\), with isolated vertices removed._

**Definition A.2** (Connectivity).: _A matrix \(\mathbf{M}\) to be completed is considered connected if its associated observation graph \(G_{\mathbf{M}}\) is connected, otherwise, we call it disconnected. The connected components of \(\mathbf{M}\) are defined as the connected components of this graph._

**Definition A.3** (Disconnectivity with Complete Bipartite Components).: _A matrix \(\mathbf{M}\) to be completed is considered disconnected with complete bipartite components if its associated observation graph \(G_{\mathbf{M}}\) is disconnected and each connected component forms a complete bipartite subgraph._

**Remark**.: _In the bipartite graph representation of the observed data, isolated vertices correspond to entire rows or columns of the matrix \(\mathbf{M}\) that are not observed. These rows or columns do not contribute to the loss calculation and have no influence on the dynamics of the matrix factorization under infinitesimal initialization. Consequently, when analyzing the connectivity of the observed data and its impact on the learning dynamics, these isolated vertices can be safely disregarded._

**Remark**.: _The disconnectivity of the bipartite graph representing the observed data is equivalent to the reducibility of the adjacency matrix \(\begin{bmatrix}\mathbf{0}&\mathbf{P}^{\top}\\ \mathbf{P}&\mathbf{0}\end{bmatrix}\), where \(\mathbf{P}\) is the binary observation matrix indicating the positions of the observed entries in \(\mathbf{M}\)._

_In the context of matrix completion problems, such as the Netflix problem, connectivity has a practical interpretation. Connected components in the bipartite graph indicate groups of users and movies that are linked by the users' viewing history. Users within the same connected component are related through the movies they have watched in common. Due to this practical significance, we prefer to use the term "connectivity" instead of "reducibility" when discussing the structure of the observed data in matrix completion problems._

**Definition A.4** (Connectivity of Observed Data).: _Given a matrix \(\mathbf{M}\) to be completed, an undirected simple graph \(G\) can be induced from it: the nodes of the graph are the observed elements in the matrix, and two nodes are adjacent if and only if they are in the same row or column of the matrix \(\mathbf{M}\). A matrix \(\mathbf{M}\) to be completed is considered connected if its induced graph \(G\) is connected, otherwise, we call it disconnected._

**Lemma A.1**.: _For any simple graph \(G\), if we remove all isolated vertices from \(G\) to obtain a new graph \(G^{\prime}\), then \(G^{\prime}\) is connected if and only if the line graph of \(G^{\prime}\), denoted as \(L(G^{\prime})\), is connected._

Proof.: \(\implies\) Assume \(G^{\prime}\) is connected. Consider any two nodes in \(L(G^{\prime})\), which correspond to two edges in \(G^{\prime}\), say \(e_{1}\) and \(e_{2}\). Since \(G^{\prime}\) is connected, there exists a path connecting the endpoints of \(e_{1}\) and \(e_{2}\). This path corresponds to a sequence of edges in \(G^{\prime}\), which in turn corresponds to a path connecting the nodes representing \(e_{1}\) and \(e_{2}\) in \(L(G^{\prime})\). Therefore, \(L(G^{\prime})\) is connected.

\(\iff\) Conversely, assume \(L(G^{\prime})\) is connected. Consider any two vertices \(v_{1}\) and \(v_{2}\) in \(G^{\prime}\). Since \(G^{\prime}\) has no isolated vertices, each of \(v_{1}\) and \(v_{2}\) is incident to at least one edge. Let these edges be \(e_{1}\) and \(e_{2}\), respectively. Since \(L(G^{\prime})\) is connected, there exists a path connecting the nodes representing \(e_{1}\) and \(e_{2}\) in \(L(G^{\prime})\). This path corresponds to a sequence of edges in \(G^{\prime}\), which in turn corresponds to a path connecting \(v_{1}\) and \(v_{2}\) in \(G^{\prime}\). Therefore, \(G^{\prime}\) is connected.

In conclusion, we have proven that for any simple graph \(G\), if we remove all isolated vertices from \(G\) to obtain a new graph \(G^{\prime}\), then \(G^{\prime}\) is connected if and only if the line graph of \(G^{\prime}\), denoted as \(L(G^{\prime})\), is connected. 

**Proposition A.3**.: _Given a incomplete matrix \(\mathbf{M}\), the connectivity of \(\mathbf{M}\) defined in Def. A.2 and Def. A.4 is equivalent._

Proof.: By definition, each edge of a bipartite graph corresponds to an observed data item, and two edges in a bipartite graph are adjacent if and only if the two corresponding observed data items are in the same row or column. Therefore, the connectivity of the observed data is equivalent to the connectivity of the edges of the bipartite graph, which is, in turn, equivalent to the connectivity of the line graph of the bipartite graph.

According to Lem. A.1, for any graph \(G\), if we remove all isolated vertices from \(G\) to obtain a new graph \(G^{\prime}\), then \(G^{\prime}\) is connected if and only if the line graph of \(G^{\prime}\), denoted as \(L(G^{\prime})\), is connected.

In the context of the bipartite graph representation of the observed data, removing isolated vertices corresponds to removing rows and columns that contain no observed entries. Thus, the connectivity of the bipartite graph after removing isolated vertices is equivalent to the connectivity of the observed data as defined in Def. A.4.

Consequently, the connectivity of the observed data as defined in Def. A.2 (based on the line graph of the bipartite graph) is equivalent to the connectivity of the observed data as defined in Def. A.4 (based on the connectivity of observed data). 

**Definition A.5** (Decoupling of Dynamics).: _Given an incomplete matrix \(\mathbf{M}\), consider the gradient flow dynamics of matrix factorization models, \(\forall 1\leq i,j\leq d\),_

\[\left\{\begin{aligned} \dot{\mathbf{a}}_{i}&=-\frac{2}{n} \sum_{j\in I_{i}}(\mathbf{a}_{i}\cdot\mathbf{b}_{\cdot,j}-\mathbf{M}_{ij})\mathbf{b}_{\cdot,j }^{\top},\\ \dot{\mathbf{b}}_{\cdot,j}&=-\frac{2}{n}\sum_{i\in I_{j} }(\mathbf{a}_{i}\cdot\mathbf{b}_{\cdot,j}-\mathbf{M}_{ij})\mathbf{a}_{i}^{\top}.\end{aligned}\right. \tag{10}\]

_The dynamics are said to be decoupled if there exist disjoint subsets of indices \(R_{1},R_{2},\ldots,R_{k}\subseteq\{1,2,\ldots,d\}\) for the rows of \(\mathbf{A}\) and \(C_{1},C_{2},\ldots,C_{k}\subseteq\{1,2,\ldots,d\}\) for the columns of \(\mathbf{B}\), such that for each \(l\in\{1,2,\ldots,k\}\), the dynamics of \(\{\mathbf{a}_{i}:i\in R_{l}\}\) and \(\{\mathbf{b}_{\cdot,j}:j\in C_{l}\}\) form an independent system of equations. In other words, the dynamics can be divided into \(k(k>1)\) independent subsystems, each involving a subset of rows of \(\mathbf{A}\) and a subset of columns of \(\mathbf{B}\). If such a division is not possible, the dynamics are said to be coupled._

**Proposition A.4**.: _Given an incomplete matrix \(\mathbf{M}\), if it is disconnected as defined by Def. A.2, then the dynamics are decoupled as defined by Def. A.5; if it is connected as defined by Def. A.2, then the dynamics are coupled as defined by Def. A.5._

Proof.: Consider a matrix \(\mathbf{M}\) to be completed, with its associated observation graph comprising \(m\) connected components, denoted as \(L_{1},L_{2},\cdots,L_{m}\). Let \(S_{\mathbf{x}}^{L_{p}}\subseteq S_{\mathbf{x}}\) represent the subset of observed indices corresponding to the connected component \(L_{p}\), where \(1\leq p\leq m\) and \(S_{\mathbf{x}}\) denotes the complete set of observed indices. If the incomplete matrix \(\mathbf{M}\) is disconnected, then for each connected component \(L_{p}\), the subset \(S_{\mathbf{x}}^{L_{p}}\) can be partitioned into two subsets \(R_{p}\) and \(C_{p}\), \(1\leq p\leq m\), such that

\[R_{p}=\{i|\exists j:(i,j)\in S_{\mathbf{x}}^{L_{p}}\},\quad C_{p}=\{j|\exists i:(i, j)\in S_{\mathbf{x}}^{L_{p}}\}. \tag{11}\]

In other words, \(R_{p}\) contains the row indices and \(C_{p}\) contains the column indices of the observed entries in the connected component \(L_{p}\).

It can be easily verified that the dynamics are decoupled in this case, as the subsets \(\{R_{p},C_{p}\}_{p=1}^{m}\) satisfy the conditions in Def. A.5. Each connected component \(L_{p}\) corresponds to an independent subsystem involving the rows of \(\mathbf{A}\) indexed by \(R_{p}\) and the columns of \(\mathbf{B}\) indexed by \(C_{p}\).

If \(\mathbf{M}\) is connected, then its associated observation graph consists of a single connected component, and the entire dynamics are coupled. 

Examples of connectivity and disconnectivity.Consider three matrices to be completed, each obtained by adding one more observation to the previous matrix: \(\mathbf{M}_{1}\) (disconnected), \(\mathbf{M}_{2}\) (disconnected with complete bipartite components), and \(\mathbf{M}_{3}\) (connected).

\[\mathbf{M}_{1}=\left[\begin{array}{ccc}1&2&\star\\ 3&\star&\star\\ \star&\star&5\end{array}\right],\mathbf{M}_{2}=\left[\begin{array}{ccc}1&2& \star\\ 3&4&\star\\ \star&\star&5\end{array}\right],\mathbf{M}_{3}=\left[\begin{array}{ccc}1&2&\star \\ 3&4&\star\\ 6&\star&5\end{array}\right] \tag{12}\]

The observation matrix P is:

\[\mathbf{P}_{1}=\left[\begin{array}{ccc}1&1&0\\ 1&0&0\\ 0&0&1\end{array}\right],\mathbf{P}_{2}=\left[\begin{array}{ccc}1&1&0\\ 1&1&0\\ 0&0&1\end{array}\right],\mathbf{P}_{3}=\left[\begin{array}{ccc}1&1&0\\ 1&1&0\\ 1&0&1\end{array}\right] \tag{13}\]And the adjacency matrix is:

\[\mathbf{A}_{1}=\left[\begin{array}{cc}\mathbf{0}&\mathbf{P}_{1}^{\top}\\ \mathbf{P}_{1}&\mathbf{0}\end{array}\right],\mathbf{A}_{2}=\left[\begin{array}{cc}\mathbf{0}& \mathbf{P}_{2}^{\top}\\ \mathbf{P}_{2}&\mathbf{0}\end{array}\right],\mathbf{A}_{3}=\left[\begin{array}{cc}\mathbf{0} &\mathbf{P}_{3}^{\top}\\ \mathbf{P}_{3}&\mathbf{0}\end{array}\right] \tag{14}\]

Given the adjacency matrix \(\mathbf{A}\), we can obtain a graph \(G_{\mathbf{M}}\). Fig. A1 illustrates the associated graphs \(G_{\mathbf{M}}\), from which we can see that \(\mathbf{M}_{1}\) is disconnected, with its associated observation graph consisting of two connected components. \(\mathbf{M}_{2}\) is also disconnected, but each connected component of its associated observation graph forms a complete bipartite subgraph. In contrast, \(\mathbf{M}_{3}\) is connected, and its associated observation graph consists of a single connected component.

### Loss Landscape

In this paper, we focus on the problem of asymmetric matrix factorization. Previous literature (Gunasekar et al., 2017; Li et al., 2018, 2020; Jin et al., 2023) has predominantly concentrated on symmetric matrix factorization problems. Although asymmetric matrix factorization models can be transformed into symmetric cases, studying symmetric matrix factorization does not necessarily cover all aspects of the asymmetric scenarios.

Generally, an asymmetric matrix factorization model \(\mathbf{W}=\mathbf{A}\mathbf{B}\) can be transformed into a symmetric situation by setting

\[\mathbf{U}=\begin{bmatrix}\mathbf{A}\\ \mathbf{B}^{\top}\end{bmatrix}\in\mathbb{R}^{2d\times d}.\]

We then consider the model \(\mathbf{W}^{\prime}=\mathbf{U}\mathbf{U}^{\top}\), which corresponds to the following matrix completion problem:

\[\begin{bmatrix}\mathbf{A}\mathbf{A}^{\top}&\mathbf{A}\mathbf{B}\\ \mathbf{B}^{\top}\mathbf{A}^{\top}&\mathbf{B}^{\top}\mathbf{B}\end{bmatrix}.\]

We define the loss as

\[\mathcal{L}^{\prime}\left(\begin{bmatrix}\mathbf{A}&\mathbf{B}\\ \mathbf{C}&\mathbf{D}\end{bmatrix}\right)=\frac{1}{2}\mathcal{L}(\mathbf{B})+\frac{1}{2} \mathcal{L}(\mathbf{C}^{\top}).\]

Li et al. (2020) established the following results:

**Theorem A.1** (Theorem 5.10 in Li et al. (2020)).: _Let \(f:\mathbb{R}^{d\times d}\rightarrow\mathbb{R}\) be a convex \(\mathcal{C}^{2}\)-smooth function. (1). All stationary points of \(\mathcal{L}:\mathbb{R}^{d\times d}\rightarrow\mathbb{R},\mathcal{L}(\mathbf{U})= \frac{1}{2}f\left(\mathbf{U}\mathbf{U}^{\top}\right)\) are either strict saddles or global minimizers; (2). For any random initialization, \(GF(1)\) converges to strict saddles of \(\mathcal{L}(\mathbf{U})\) with probability 0._

The proof of this theorem relies heavily on Theorem A.2 of Du and Lee (2018), which requires the parameter matrix \(\mathbf{U}\in\mathbb{R}^{d\times k}\) to satisfy the condition that \(k\geq d\). In the case of symmetric matrix factorization, where \(\mathbf{U}\in\mathbb{R}^{d\times d}\), this condition is naturally met. However, for asymmetric matrix factorization, where \(\mathbf{U}=\begin{bmatrix}\mathbf{A}\\ \mathbf{B}^{\top}\end{bmatrix}\in\mathbb{R}^{2d\times d}\), this condition is not satisfied, and thus the proof of Theorem A.1 is only applicable to the symmetric case of matrix factorization.

Figure A1: **The associated observation graphs \(G_{\mathbf{M}}\) of the incomplete matrices \(\mathbf{M}_{1}\), \(\mathbf{M}_{2}\), and \(\mathbf{M}_{3}\) in Eq. 4. \(\mathbf{M}_{1}\) is disconnected, with its associated observation graph consisting of two connected components. \(\mathbf{M}_{2}\) is also disconnected, but each connected component of its associated observation graph forms a complete bipartite subgraph. In contrast, \(\mathbf{M}_{3}\) is connected, and its associated observation graph consists of a single connected component.**

**Theorem A.2** (Theorem 3.1 in Du and Lee (2018)).: _Let \(f:\mathbb{R}^{d\times d}\to\mathbb{R}\) be a \(\mathcal{C}^{2}\) convex function. Then \(\mathcal{L}:\mathbb{R}^{d\times k}\to\mathbb{R},\mathcal{L}(\mathbf{U})=f\left(\mathbf{ U}\mathbf{U}^{\top}\right),k\geq d\) satisfies that (1). Every local minimizer of \(\mathcal{L}\) is also a global minimizer; (2). All saddles are strict. Here saddles denote those stationary points whose hessian are not positive semi-definite (thus including local maximizers)._

Below we give a direct proof of the loss landscape of an asymmetric matrix factorization model.

**Theorem A.3** (Loss Landscape).: _For any data \(S\), the critical points of \(R_{S}(\mathbf{\theta})\) are either strict saddle points or global minima._

Proof.: We start by recalling the definition of the loss function:

\[R_{S}(\mathbf{\theta})=\mathcal{L}(\mathbf{A},\mathbf{B})=\frac{1}{2}\|\mathbf{AB}-\mathbf{M}\|_{ S_{\mathbf{x}}}^{2}=\frac{1}{2}\sum_{(i,j)\in S_{\mathbf{x}}}((\mathbf{AB})_{ij}-\mathbf{M}_{ij })^{2}.\]

Let \(\mathbf{\theta}=(\mathbf{A},\mathbf{B})\) denote a critical point. We define a new matrix, \(\delta\mathbf{M}\), as the difference between the product of \(\mathbf{A}\) and \(\mathbf{B}\) and the matrix \(\mathbf{M}\), with this difference being computed only over the indices in the set \(S_{\mathbf{x}}\). More formally, \(\delta\mathbf{M}=(\mathbf{AB}-\mathbf{M})_{S_{\mathbf{x}}}\), where the elements of \(\delta\mathbf{M}\) are given by:

* For \((i,j)\in S_{\mathbf{x}}\), we have \(\delta\mathbf{M}_{ij}=(\mathbf{AB})_{ij}-\mathbf{M}_{ij}\).
* For \((i,j)\notin S_{\mathbf{x}}\), we have \(\delta\mathbf{M}_{ij}=0\).

This definition of \(\delta\mathbf{M}\) ensures that we only consider the differences in the entries that belong to the set \(S_{\mathbf{x}}\), while all other entries are set to zero.

Consider the function:

\[\mathcal{L}(\mathbf{A}+\mathbf{\varepsilon},\mathbf{B}+\mathbf{\eta}) =\frac{1}{2}\|\delta\mathbf{M}+\mathbf{\varepsilon}\mathbf{B}+\mathbf{A}\mathbf{\eta }+\mathbf{\varepsilon}\mathbf{\eta}\|_{S_{\mathbf{x}}}^{2} \tag{15}\] \[=\frac{1}{2}\|\delta\mathbf{M}\|_{S_{\mathbf{x}}}^{2}+\langle\delta\mathbf{M},\mathbf{\varepsilon}\mathbf{B}+\mathbf{A}\mathbf{\eta}\rangle_{S_{\mathbf{x}}}+\frac{1}{2}\|\mathbf{ \varepsilon}\mathbf{B}+\mathbf{A}\mathbf{\eta}\|_{S_{\mathbf{x}}}^{2}\] \[\quad+\langle\delta\mathbf{M},\mathbf{\varepsilon}\mathbf{\eta}\rangle_{S_{ \mathbf{x}}}+o(\|\mathbf{\varepsilon}\|^{2},\|\mathbf{\eta}\|^{2}),\]

where the inner product of two matrices \(\mathbf{A},\mathbf{B}\) is defined as \(\langle\mathbf{A},\mathbf{B}\rangle:=\mathrm{Tr}(\mathbf{A}\mathbf{B}^{\top})\).

At the critical point, the first order term \(\langle\delta\mathbf{M},\mathbf{\varepsilon}\mathbf{B}+\mathbf{A}\mathbf{\eta}\rangle_{S_{\mathbf{x}}}\) equals 0. The Hessian operator, representing the second order term, is given by:

\[h_{\mathbf{A},\mathbf{B}}(\mathbf{\varepsilon},\mathbf{\eta})=\frac{1}{2}\|\mathbf{\varepsilon}\mathbf{ B}+\mathbf{A}\mathbf{\eta}\|_{S_{\mathbf{x}}}^{2}+\langle\delta\mathbf{M},\mathbf{\varepsilon}\mathbf{ \eta}\rangle_{S_{\mathbf{x}}}.\]

Our goal is to demonstrate that if \(\delta\mathbf{M}\neq\mathbf{0}\), there always exists \(\mathbf{\varepsilon},\mathbf{\eta}\) such that \(h_{\mathbf{A},\mathbf{B}}(\mathbf{\varepsilon},\mathbf{\eta})<0\). To this end, we consider the ranks of matrices \(\mathbf{A}\) and \(\mathbf{B}\) in two cases:

(i) \(\text{rank}(\mathbf{A})<d\) or \(\text{rank}(\mathbf{B})<d\):

Without loss of generality, we assume \(\delta\mathbf{M}_{ij}:=\delta\mathbf{M}_{ij}\neq 0\) for some \((i,j)\in S_{\mathbf{x}}\), and \(\text{rank}\mathbf{A}<d\). Under these conditions, there exists a non-zero vector \(\mathbf{v}\) such that \(\mathbf{A}\mathbf{v}=0\).

We set \(\mathbf{\eta}_{:j}^{*}=\mathbf{v}\) and \(\mathbf{\eta}_{,s}^{*}=0\) for \(s\neq j\), where \(\mathbf{\eta}_{,j}^{*}\) denotes the \(j\)-th column of the matrix \(\mathbf{\eta}\). Let \(\mathbf{\varepsilon}_{i}^{*}=\mathbf{w}^{\top}\in\mathbb{R}^{d}\) and \(\mathbf{\varepsilon}_{s}=0\) for \(s\neq i\), where \(\mathbf{\varepsilon}_{i}^{*}\) denotes the \(i\)-th row of the matrix \(\mathbf{\varepsilon}\).

We then have:

\[h_{\mathbf{A},\mathbf{B}}(\mathbf{\varepsilon}^{*},\mathbf{\eta}^{*}) =\We define \(g(\mathbf{w},\mathbf{v})=\frac{1}{2}\mathbf{w}^{\top}\mathbf{B}\mathbf{B}^{\top}\mathbf{w}+\delta\mathbf{M}_{ ij}\mathbf{w}^{\top}\mathbf{v}\) and consider:

\[g(-\alpha\delta\mathbf{M}_{ij}\mathbf{v},\mathbf{v}) =\frac{1}{2}\alpha^{2}\delta\mathbf{M}_{ij}^{2}\mathbf{v}^{\top}\mathbf{B}\bm {B}^{\top}\mathbf{v}-\alpha\delta\mathbf{M}_{ij}^{2}\mathbf{v}^{\top}\mathbf{v}\] \[=\frac{1}{2}\alpha^{2}\delta\mathbf{M}_{ij}^{2}(\mathbf{v}^{\top}\mathbf{B} \mathbf{B}^{\top}\mathbf{v}-2\frac{1}{\alpha}\mathbf{v}^{\top}\mathbf{v}).\]

For \(0<\alpha<\frac{2}{\lambda_{\mathbf{B},\max}}\), where \(\lambda_{\mathbf{B},\max}\) represents the top eigenvalue of \(\mathbf{B}\mathbf{B}^{\top}\), we find \(g(-\alpha\delta\mathbf{M}_{ij}\mathbf{v},\mathbf{v})<0\).

Therefore, when \(\mathbf{w}=-\alpha\delta\mathbf{M}_{ij}\mathbf{v}\), we obtain \(h_{\mathbf{A},\mathbf{B}}(\mathbf{\varepsilon}^{*},\mathbf{\eta}^{*})<0\). This immediately implies the critical point \(\mathbf{\theta}=(\mathbf{A},\mathbf{B})\) is a strict saddle point.

(ii) \(\text{rank}(\mathbf{A})=\text{rank}(\mathbf{B})=d\):

Let \(\mathbf{\varepsilon}=\alpha\delta\mathbf{M}\mathbf{B}^{-1}\) and \(\mathbf{\eta}=0\). In this scenario, the first order term \(\langle\delta\mathbf{M},\mathbf{\varepsilon}\mathbf{B}+\mathbf{A}\mathbf{\eta}\rangle_{S_{\mathbf{a}}}\) in Eq. (15) simplifies to \(\alpha\|\delta\mathbf{M}\|_{S_{\mathbf{a}}}^{2}\). At a critical point, this quantity equals zero, it implies that \(\delta\mathbf{M}=0\). This in turn implies that the critical point \(\mathbf{\theta}=(\mathbf{A},\mathbf{B})\) is a global minimum.

This concludes the proof, establishing that the critical points of \(R_{S}(\mathbf{\theta})\) are either strict saddle points or global minima. 

### Escaping from Top Eigendirection

In this section, we focus on the dynamics of escaping from a critical point. According to Prop. A.3, the loss landscape consists solely of strict saddle points and a global minimum. Consequently, gradient-based methods can readily escape from a critical point that is not a global minimum.

In the following, we will demonstrate that the escaping dynamics near a critical point can be approximated by a linearized version of these dynamics. For this, consider the following:

\[\dot{\mathbf{\theta}}=-\nabla R_{S}(\mathbf{\theta}). \tag{16}\]

Assume \(\mathbf{\theta}_{c}\) is a saddle point for which \(\nabla R_{S}(\mathbf{\theta}_{c})=0\). We can apply a first-order Taylor expansion to the right-hand side of Eq. (16), yielding:

\[-\nabla R_{S}(\mathbf{\theta})=-\nabla R_{S}(\mathbf{\theta}_{c})-\nabla^{2}R_{S}(\mathbf{ \theta}_{c})(\mathbf{\theta}-\mathbf{\theta}_{c})+\mathcal{O}(\|\mathbf{\theta}-\mathbf{\theta }_{c}\|^{2}), \tag{17}\]

where \(\nabla^{2}R_{S}(\mathbf{\theta}_{c})\) represents the Hessian matrix. Given that \(\nabla R_{S}(\mathbf{\theta}_{c})=0\), the gradient flow dynamics around \(\mathbf{\theta}_{c}\) can be approximated as:

\[\dot{\mathbf{\theta}}=\mathbf{H}(\mathbf{\theta}-\mathbf{\theta}_{c}), \tag{18}\]

where \(\mathbf{H}:=-\nabla^{2}R_{S}(\mathbf{\theta}_{c})\). Eq. (18) is a classic linear ordinary differential equation, with the solution:

\[\mathbf{\theta}(t)=\mathrm{e}^{t\mathbf{H}}(\mathbf{\theta}_{0}-\mathbf{\theta}_{c})+\mathbf{ \theta}_{c}. \tag{19}\]

The dynamics near a critical point can be approximated by a linearized version. Hence, in the vicinity of a critical point, we can analyze the linearized dynamics to understand the escape mechanism. In the following, we will show that during this escape process, the dynamics follow a pattern referred to as _dominant eigenvalue dynamics_.

Eq. (19) elucidates that the dynamics near the critical point \(\mathbf{\theta}_{c}\) are predominantly dictated by the properties of \(\mathbf{H}\), a real symmetric matrix in \(\mathbb{R}^{2d^{2}\times 2d^{2}}\). Its eigendecomposition is given by:

\[\mathbf{H}:=-\nabla^{2}R_{S}(\mathbf{\theta}_{c})=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{\top}, \tag{20}\]

where \(\mathbf{\Lambda}\) is a diagonal matrix and \(\mathbf{Q}\) is an orthogonal matrix. Let \(\lambda_{1}>\lambda_{2}>\cdots>\lambda_{s}\in\mathbb{R}\) denote the eigenvalues of \(\mathbf{H}\), and let \(\mathbf{q}_{i1},\mathbf{q}_{i2},\cdots,\mathbf{q}_{il_{i}}\) represent the eigenvectors corresponding to \(\lambda_{i}\).

Given that \(\lambda_{1}>\lambda_{2}\), the ratio \(e^{\lambda_{1}t}/e^{\lambda_{i}t}\) for \(i>1\) grows exponentially fast. Consequently, near \(\mathbf{\theta}_{c}\), the evolution of the system is primarily driven by the eigenvectors \(\mathbf{q}_{11},\mathbf{q}_{12},\cdots,\mathbf{q}_{1l_{1}}\) associated with the largest eigenvalue \(\lambda_{1}\).

[MISSING_PAGE_FAIL:20]

We define \(C=\sum\limits_{i=2}^{s}\sum\limits_{j=1}^{l_{i}}\left\|\frac{\langle\mathbf{\theta}_{0}- \mathbf{\theta}_{c},\mathbf{q}_{ij}\rangle\mathbf{q}_{ij}}{\sum\limits_{j=1}^{l_{1}}\langle \mathbf{\theta}_{0}-\mathbf{\theta}_{c},\mathbf{q}_{1j}\rangle\mathbf{q}_{1j}}\right\|\). By choosing \(t_{0}=\frac{\log\frac{C}{\varepsilon}}{\lambda_{1}-\lambda_{2}}\), we ensure that for all \(t>t_{0}\), the following condition is met:

\[\left\|\frac{\mathbf{\theta}(t)-\mathbf{\theta}_{c}}{\sum\limits_{j=1}^{l_{1}}\mathrm{e }^{\lambda_{1}t}\langle\mathbf{\theta}_{0}-\mathbf{\theta}_{c},\mathbf{q}_{1j}\rangle\mathbf{q }_{1j}}-\mathbf{1}\right\|<\varepsilon. \tag{25}\]

Prop. A.5 describes that under the linearized dynamics, the parameters will escape from the saddle point along a specific direction. However, when considering the original nonlinear dynamics \(\dot{\mathbf{\theta}}(t)=-\nabla R_{S}(\mathbf{\theta})\), we encounter a trade-off: we should choose \(t_{0}\) sufficiently large so that the trajectory can align well with the dominant eigendirection while escaping the saddle point, but if \(t_{0}\) is too large, the linearization approximation will fail as \(\mathbf{\theta}(t_{0})\) moves away from \(\mathbf{\theta}_{c}\). Li et al. (2020) (Theorem 5.3) proved a general dynamical result through careful analysis and error control: assuming the eigenvector corresponding to the maximum eigenvalue is unique and the initialization is sufficiently close to the saddle point, there always exists a suitable \(t_{0}\) such that the linear dynamics can align with the dominant eigendirection before the linearization breaks down. We can generalize this result to the case where the eigenvector corresponding to the largest eigenvalue is not unique:

**Proposition A.6**.: _Consider the dynamics given by \(\dot{\mathbf{\theta}}(t)=-\nabla R_{S}(\mathbf{\theta})\), we use \(\varphi(\mathbf{\theta}_{0},t)\) to denote the value of \(\mathbf{\theta}(t)\) in the case of \(\mathbf{\theta}(0)=\mathbf{\theta}_{0}\). At a critical point \(\mathbf{\theta}_{c}\), we denote the negative Hessian as \(\mathbf{H}:=-\nabla^{2}R_{S}(\mathbf{\theta}_{c})\). Let \(\lambda_{1}\in\mathbb{R}\) be the largest eigenvalue of \(\mathbf{H}\), with corresponding eigenvectors \(\mathbf{q}_{11},\mathbf{q}_{12},\cdots,\mathbf{q}_{1l_{1}}\). Denote \(c_{j}=\langle\mathbf{\theta}_{0}-\mathbf{\theta}_{c},\mathbf{q}_{1j}\rangle,\forall 1\leq j \leq l_{1}\), and \(\mathbf{v}_{1}=\sum_{j=1}^{l_{1}}c_{j}\mathbf{q}_{1j}\). Assume there exists \(j\) such that \(c_{j}\neq 0\). Let \(\mathbf{z}_{\alpha}(t):=\varphi\left(\mathbf{\theta}_{c}+\alpha\mathbf{v}_{1},t+\frac{1}{ \lambda_{1}}\log\frac{1}{\alpha}\right)\) for every \(\alpha>0\), then \(\mathbf{z}(t):=\lim_{\alpha\to 0}\mathbf{z}_{\alpha}(t)\) exists and is also a solution of the given dynamics, i.e., \(\mathbf{z}(t)=\varphi(\mathbf{z}(0),t)\). Furthermore, \(\forall t\in\mathbb{R}\), there exists a constant \(C>0\) such that_

\[\left\|\varphi\left(\mathbf{\theta}_{c}+\alpha\mathbf{\theta}_{0},t+\frac{1}{\lambda_{ 1}}\log\frac{1}{\alpha}\right)-\mathbf{z}(t)\right\|_{2}\leq C\alpha^{\frac{ \lambda_{1}-\lambda_{2}}{2\lambda_{1}-\lambda_{2}}}\]

_for every sufficiently small \(\alpha\), where \(\lambda_{1}-\lambda_{2}>0\) is the eigenvalue gap._

Proof.: By Theorem 5.3 in Section 5.1 of Li et al. (2020), we know that if the eigenspace corresponding to \(\lambda_{1}\) is one-dimensional, i.e., \(l_{1}=1\), then the escaping direction will be the top eigenvector direction, and the convergence rate is \(O(\alpha^{\frac{\lambda_{1}-\lambda_{2}}{2\lambda_{1}-\lambda_{2}}})\). Therefore, Proposition A.6 holds in this case.

Now, if the eigenspace corresponding to \(\lambda_{1}\) is not one-dimensional, we denote \(c_{j}=\langle\mathbf{\theta}_{0}-\mathbf{\theta}_{c},\mathbf{q}_{1j}\rangle,\forall 1\leq j \leq l_{1}\), and \(\mathbf{v}_{1}=\sum_{j=1}^{l_{1}}c_{j}\mathbf{q}_{1j}\) will be the escaping direction. Following the same technique as in Li et al. (2020), we can easily verify that the convergence rate remains \(O(\alpha^{\frac{\lambda_{1}-\lambda_{2}}{2\lambda_{1}-\lambda_{2}}})\). Therefore, Proposition A.6 holds in this case as well.

### Eigenvalues and Eigenvectors of Hessian

Suppose the dominant directions fulfill specific conditions, such as any combination \(c_{1}\mathbf{q}_{11}+c_{2}\mathbf{q}_{12}+\cdots+c_{l_{1}}\mathbf{q}_{1l_{1}}\), leading to rank 1 model parameters \((\mathbf{A},\mathbf{B})\). In such scenarios, we may observe a phenomenon where the rank of the matrix increases incrementally.

Firstly, we analyze the eigenvector structure of the Hessian matrix at the critical point \(\mathbf{\theta}_{c}=(\mathbf{A}_{c},\mathbf{B}_{c})\)to understand why the parameter will enter the rank-1 invariant manifold.

Computation of the Hessian Matrix at a Critical Point.To compute the Hessian matrix, we first consider the gradient:

\[R_{S}(\mathbf{\theta})=\mathbb{E}_{S}\ell\left(f(\mathbf{x},\mathbf{\theta}),f^ {*}(\mathbf{x})\right),\] \[\nabla_{\mathbf{\theta}}R_{S}(\mathbf{\theta})=\mathbb{E}_{S}\nabla\ell \left(\mathbf{f}(\mathbf{x},\mathbf{\theta}),\mathbf{f}^{*}(\mathbf{x})\right)^{\top}\nabla_{\mathbf{ \theta}}\mathbf{f}_{\mathbf{\theta}}(\mathbf{x}),\] \[=\sum_{i=1}^{d^{2}}\mathbb{E}_{S}\partial_{i}\ell\left(\mathbf{f}_{ \mathbf{\theta}},\mathbf{f}^{*}\right)\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{\theta}} \right)_{i},\] \[=\sum_{i=1}^{d^{2}}\mathbb{E}_{S}(\mathbf{f}_{\mathbf{\theta}}-\mathbf{f}^{*} )_{i}\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{\theta}}\right)_{i},\]

where \(\partial_{i}\ell\left(\mathbf{f}_{\mathbf{\theta}},\mathbf{f}^{*}\right)\) is the \(i\)-th element of \(\nabla\ell\left(\mathbf{f}(\mathbf{x},\mathbf{\theta}),\mathbf{f}^{*}(\mathbf{x})\right)\), and \(\left(\mathbf{f}_{\mathbf{\theta}}\right)_{i}\) is the \(i\)-th element of the vectorization of \(\mathbf{f}_{\mathbf{\theta}}\).

For the Hessian matrix \(\mathbf{H}_{S}(\mathbf{\theta})\), we have

\[\mathbf{H}(\mathbf{\theta}) :=\nabla_{\mathbf{\theta}}\nabla_{\mathbf{\theta}}R_{S}(\mathbf{\theta})=\sum_ {i=1}^{d^{2}}\mathbb{E}_{S}\nabla_{\mathbf{\theta}}\left(\partial_{i}\ell\left(\bm {f}_{\mathbf{\theta}},\mathbf{f}^{*}\right)\right)\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{ \mathbf{\theta}}\right)_{i}+\sum_{i=1}^{d^{2}}\mathbb{E}_{S}\partial_{i}\ell\left( \mathbf{f}_{\mathbf{\theta}},\mathbf{f}^{*}\right)\nabla_{\mathbf{\theta}}\nabla_{\mathbf{\theta}} \left(\left(\mathbf{f}_{\mathbf{\theta}}\right)_{i}\right)\] \[=\sum_{i,j=1}^{d^{2}}\mathbb{E}_{S}\partial_{ij}\ell\left(\mathbf{f}_ {\mathbf{\theta}},\mathbf{f}^{*}\right)\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{\theta}} \right)_{i}\left(\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{\theta}}\right)_{j} \right)^{\top}+\sum_{i=1}^{d^{2}}\mathbb{E}_{S}\partial_{i}\ell\left(\mathbf{f}_{ \mathbf{\theta}},\mathbf{f}^{*}\right)\nabla_{\mathbf{\theta}}\nabla_{\mathbf{\theta}}\left( \left(\mathbf{f}_{\mathbf{\theta}}\right)_{i}\right),\] \[=\sum_{i,j=1}^{d^{2}}\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{ \theta}}\right)_{i}\left(\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{\theta}}\right) _{j}\right)^{\top}+\sum_{i=1}^{d^{2}}\mathbb{E}_{S}(\mathbf{f}_{\mathbf{\theta}}-\mathbf{ f}^{*})_{i}\nabla_{\mathbf{\theta}}\nabla_{\mathbf{\theta}}\left(\left(\mathbf{f}_{\mathbf{\theta}} \right)_{i}\right),\]

where \(\partial_{ij}\ell\left(\mathbf{f}_{\mathbf{\theta}},\mathbf{f}^{*}\right)\) is the \((i,j)\)-th element of \(\nabla\nabla\ell\left(\mathbf{f}(\mathbf{x},\mathbf{\theta}),\mathbf{f}^{*}(\mathbf{x})\right)\).

We define matrices \(\mathbf{H}^{(1)}(\mathbf{\theta})\) and \(\mathbf{H}^{(2)}(\mathbf{\theta})\) as follows:

\[\mathbf{H}^{(1)}(\mathbf{\theta}) :=\sum_{i,j=1}^{d^{2}}\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{ \theta}}\right)_{i}\left(\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{\theta}}\right) _{j}\right)^{\top},\] \[\mathbf{H}^{(2)}(\mathbf{\theta}) :=\sum_{i=1}^{d^{2}}\mathbb{E}_{S}(\mathbf{f}_{\mathbf{\theta}}-\mathbf{f}^{*} )_{i}\nabla_{\mathbf{\theta}}\nabla_{\mathbf{\theta}}\left(\left(\mathbf{f}_{\mathbf{\theta}} \right)_{i}\right),\]

We further denote that \(\mathbf{H}(\mathbf{\theta}):=\mathbf{H}^{(1)}(\mathbf{\theta})+\mathbf{H}^{(2)}(\mathbf{\theta})\).

For matrix factorization model, the eigenvectors of \(\mathbf{H}^{(2)}\) has a special structure, as characterized by Lem. A.2.

**Lemma A.2** (**Data-Independent Interleaved Structure of Eigenvectors of \(\mathbf{H}^{(2)}\))**.: _Let \(\mathbf{\theta}_{c}=(\mathbf{A}_{c},\mathbf{B}_{c})\) be any critical point of the matrix factorization model. If \(\lambda\) is an eigenvalue of \(\mathbf{H}^{(2)}(\mathbf{\theta}_{c})\in\mathbb{R}^{2d^{2}\times 2d^{2}}\), then there exist at least \(d\) eigenvectors associated with \(\lambda\). These \(d\) eigenvectors take the form \(\mathbf{v}\otimes\mathbf{e}_{1},\mathbf{v}\otimes\mathbf{e}_{2},\cdots,\mathbf{v}\otimes\mathbf{e}_{d} \in\mathbb{R}^{2d^{2}}\), where \(\mathbf{v}\in\mathbb{R}^{2d}\) is a vector to be determined and \(\mathbf{e}_{i}\) is the unit vector representing the \(i\)-th column of the identity matrix \(\mathbf{I}_{d}\in\mathbb{R}^{d\times d}\)._

Proof.: Let's denote the residual matrix at the critical point as \(\delta\mathbf{M}=(\mathbf{A}_{c}\mathbf{B}_{c}-\mathbf{M})_{S_{\mathbf{\pi}}}\), where \((\mathbf{A}_{c},\mathbf{B}_{c})\) is a critical point. For the vectorized parameter \(\mathbf{\theta}_{c}\), by direct calculation the matrix \(\mathbf{H}^{(2)}:=-\nabla^{2}R_{S}(\mathbf{\theta}_{c})\) can be formulated as a block matrix, with the diagonal blocks being \(0\). The specific format is as follows:

\[\mathbf{H}^{(2)}=\begin{bmatrix}\mathbf{0}&-\mathbf{\delta M}\otimes\mathbf{I}_{d}\\ -\mathbf{\delta M}^{\top}\otimes\mathbf{I}_{\mathbf{d}}&\mathbf{0}\end{bmatrix}. \tag{26}\]

Next, we compute the eigenvectors of \(\mathbf{H}^{(2)}\). Let \(\lambda\) be an eigenvalue of \(\mathbf{H}^{(2)}\). We need to verify that \(\mathbf{v}\otimes\mathbf{e}_{1},\mathbf{v}\otimes\mathbf{e}_{2},\cdots,\mathbf{v}\otimes\mathbf{e}_{d} \in\mathbb{R}^{2d^{2}}\) are the eigenvectors of \(\mathbf{H}^{(2)}\) corresponding to \(\lambda\), for a particular \(\mathbf{v}\in\mathbb{R}^{2d}\) yet to be determined. That is, we need to ensure that for all \(1\leq i\leq d\), the equation \((\mathbf{H}^{(2)}-\lambda\mathbf{I}_{2d^{2}})(\mathbf{v}\otimes\mathbf{e}_{i})=\mathbf{0}\) has a non-zero solution for \(\mathbf{v}\). Notice that

\[(\mathbf{H}^{(2)}-\lambda\mathbf{I}_{2d^{2}})(\mathbf{v}\otimes\mathbf{e}_{i}) =\begin{bmatrix}-\lambda\mathbf{I}_{d}\otimes\mathbf{I}_{d}&-\mathbf{\delta} \mathbf{M}\otimes\mathbf{I}_{d}\\ -\mathbf{\delta}\mathbf{M}^{\top}\otimes\mathbf{I}_{d}&-\lambda\mathbf{I}_{d}\otimes\mathbf{I}_{d} \end{bmatrix}(\mathbf{v}\otimes\mathbf{e}_{i})\] \[=\begin{pmatrix}\begin{bmatrix}-\lambda\mathbf{I}_{d}&-\mathbf{\delta}\mathbf{ M}\\ -\mathbf{\delta}\mathbf{M}^{\top}&-\lambda\mathbf{I}_{d}\end{bmatrix}\otimes\mathbf{I}_{d}\end{pmatrix} (\mathbf{v}\otimes\mathbf{e}_{i}) \tag{27}\] \[=\begin{pmatrix}\begin{bmatrix}-\lambda\mathbf{I}_{d}&-\mathbf{\delta}\mathbf{ M}\\ -\mathbf{\delta}\mathbf{M}^{\top}&-\lambda\mathbf{I}_{d}\end{bmatrix}\mathbf{v}\end{pmatrix} \otimes\mathbf{e}_{i}.\]

Since \(\lambda\) is an eigenvalue of \(\mathbf{H}^{(2)}\), the determinant of the matrix \(\mathbf{H}^{(2)}-\lambda\mathbf{I}_{2d^{2}}\) equals zero. Hence

\[\det\begin{pmatrix}\begin{bmatrix}-\lambda\mathbf{I}_{d}&-\mathbf{\delta}\mathbf{M}\\ -\mathbf{\delta}\mathbf{M}^{\top}&-\lambda\mathbf{I}_{d}\end{bmatrix}\end{bmatrix}^{2d}= \det\begin{pmatrix}\begin{bmatrix}-\lambda\mathbf{I}_{d}&-\mathbf{\delta}\mathbf{M}\\ -\mathbf{\delta}\mathbf{M}^{\top}&-\lambda\mathbf{I}_{d}\end{bmatrix}\otimes\mathbf{I}_{d} \end{pmatrix}=0. \tag{28}\]

Consequently, from Eq. (27), we conclude that there always exists a non-zero vector \(\mathbf{v}\in\mathbb{R}^{2d}\) such that \((\mathbf{H}^{(2)}-\lambda\mathbf{I}_{2d^{2}})(\mathbf{v}\otimes\mathbf{e}_{i})=0\). Since \(\mathbf{v}\neq\mathbf{0}\), it is evident that \(\mathbf{v}\otimes\mathbf{e}_{1},\mathbf{v}\otimes\mathbf{e}_{2},\cdots,\mathbf{v}\otimes\mathbf{e}_{d} \in\mathbb{R}^{2d^{2}}\) are linearly independent, and thus they represent \(d\) eigenvectors corresponding to \(\lambda\). 

**Proposition A.7** (Eigenvectors Structure of \(\mathbf{H}\) at the Origin).: _Consider the dynamics given by Eq. (21), where we denote \(\mathbf{H}:=-\nabla^{2}R_{S}(\mathbf{0})\). If \(\lambda\) is an eigenvalue of \(\mathbf{H}\in\mathbb{R}^{2d^{2}\times 2d^{2}}\), then there exist at least \(d\) eigenvectors associated with \(\lambda\) in \(\mathbf{H}\). These \(d\) eigenvectors take the form \(\mathbf{v}\otimes\mathbf{e}_{1},\mathbf{v}\otimes\mathbf{e}_{2},\cdots,\mathbf{v}\otimes\mathbf{e}_{d} \in\mathbb{R}^{2d^{2}}\), where \(\mathbf{v}\in\mathbb{R}^{2d}\) is a vector to be determined and \(\mathbf{e}_{i}\) is the unit vector representing the \(i\)-th column of the identity matrix \(\mathbf{I}_{d}\in\mathbb{R}^{d\times d}\)._

Proof.: In the matrix factorization model, at the origin the gradient \(\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{\theta}}\right)=\mathbf{0}\) and thus \(\mathbf{H}^{(1)}(\mathbf{0})=0\) and the Hessian matrix reduces to \(\mathbf{H}^{(2)}(\mathbf{0})\), making \(\mathbf{H}=-\nabla^{2}R_{S}(\mathbf{0})=-\mathbf{H}^{(2)}(\mathbf{0})\).

Let's denote the residual matrix at the origin as \(\delta\mathbf{M}=(\mathbf{A}_{c}\mathbf{B}_{c}-\mathbf{M})_{S_{\mathbf{u}}}\), where at the origin \((\mathbf{A}_{c},\mathbf{B}_{c})=(\mathbf{0},\mathbf{0})\). For the vectorized parameter \(\mathbf{\theta}_{c}\), the matrix \(\mathbf{H}:=-\nabla^{2}R_{S}(\mathbf{0})\) can be formulated as a block matrix, with the diagonal blocks being \(0\). The specific format is as follows:

\[\mathbf{H}=\mathbf{H}^{(2)}=\begin{bmatrix}\mathbf{0}&-\mathbf{\delta}\mathbf{M}\otimes\mathbf{I}_{d} \\ -\mathbf{\delta}\mathbf{M}^{\top}\otimes\mathbf{I}_{d}&\mathbf{0}\end{bmatrix}. \tag{29}\]

By Lem. A.2, the proof is completed. 

**Lemma A.3** (Eigenvectors Structure of \(\mathbf{H}\) at Second-order Stationary Point).: _Let \(\mathbf{\Omega}\) denote an \(\mathbf{\Omega}_{k}\) invariant manifold or sub-\(\mathbf{\Omega}_{k}\) invariant manifold defined in Prop. A.1 and A.2, and consider a second-order stationary point \(\mathbf{\theta}_{c}\) within \(\mathbf{\Omega}\), i.e., \(\nabla R_{S}(\mathbf{\theta}_{c})=0\) and \(\mathbf{\theta}^{\top}\nabla^{2}R_{S}(\mathbf{\theta}_{c})\mathbf{\theta}\geq 0\) for all \(\mathbf{\theta}\in\mathbf{\Omega}\). Then, the eigenvectors corresponding to the negative eigenvalues of the Hessian matrix \(\mathbf{H}(\mathbf{\theta}_{c})\) are contained within the span of the eigenvectors corresponding to the negative eigenvalues of \(\mathbf{H}^{(2)}(\mathbf{\theta}_{c})\)._

Proof.: Recall the definitions of \(\mathbf{H}^{(1)}(\mathbf{\theta})\) and \(\mathbf{H}^{(2)}(\mathbf{\theta})\) given by:

\[\mathbf{H}^{(1)}(\mathbf{\theta}) :=\sum_{i,j=1}^{d^{2}}\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{ \theta}}\right)_{i}\left(\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{\theta}}\right) _{j}\right)^{\top},\] \[\mathbf{H}^{(2)}(\mathbf{\theta}) :=\sum_{i=1}^{d^{2}}\mathbb{E}_{S}\left[\left(\mathbf{f}_{\mathbf{ \theta}}-\mathbf{f}^{*}\right)_{i}\nabla_{\mathbf{\theta}}^{2}\left(\mathbf{f}_{\mathbf{\theta} }\right)_{i}\right].\]

The Hessian matrix \(\mathbf{H}(\mathbf{\theta})\) at \(\mathbf{\theta}\) is \(\mathbf{H}(\mathbf{\theta}):=\mathbf{H}^{(1)}(\mathbf{\theta})+\mathbf{H}^{(2)}(\mathbf{\theta})\).

The manifold \(\mathbf{\Omega}\) is an affine subspace with orthogonal complement denoted by \(\mathbf{\Omega}^{\perp}\). Let \(\mathbf{H}\) have the following block representation in the bases of \(\mathbf{\Omega}\) and \(\mathbf{\Omega}^{\perp}\):

\[\mathbf{H}=\begin{bmatrix}\mathbf{H}_{11}&\mathbf{H}_{12}\\ \mathbf{H}_{21}&\mathbf{H}_{22}\end{bmatrix}. \tag{30}\]Since \(\Omega\) is an invariant subspace under the gradient flow, we have \(\mathbf{H}\mathbf{\theta}\in\mathbf{\Omega}\) for all \(\mathbf{\theta}\in\mathbf{\Omega}\), which implies that \(\mathbf{H}_{12}=\mathbf{0}\). Since \(\mathbf{H}\) is symmetry, we have \(\mathbf{H}_{21}=\mathbf{0}\).

Let \(\lambda<0\) be a negative eigenvalue of \(\mathbf{H}:=\mathbf{H}(\mathbf{\theta}_{c})\) with \(\mathbf{v}\) as the corresponding eigenvector. Since \(\mathbf{H}_{11}\) is positive semi-definite, \(\mathbf{v}\) must lie in \(\mathbf{\Omega}^{\perp}\).

At a critical point \(\mathbf{\theta}_{c}=(\mathbf{A}_{c},\mathbf{B}_{c})\), by direct calculation, the gradient \(\nabla_{\mathbf{\theta}}\mathbf{f}_{\mathbf{\theta}_{c}}\) can be structured as:

\[\nabla_{\mathbf{\theta}}\mathbf{f}_{\mathbf{\theta}_{c}}=\left[\begin{array}{cccc}\mathbf{B} _{c}&&&\\ &\ddots&\\ a_{11}\mathbf{I}&a_{21}\mathbf{I}&\cdots&a_{d1}\mathbf{I}\\ a_{12}\mathbf{I}&a_{22}\mathbf{I}&\cdots&a_{d2}\mathbf{I}\\ \vdots&\vdots&\ddots&\vdots\\ a_{1d}\mathbf{I}&a_{2d}\mathbf{I}&\cdots&a_{dd}\mathbf{I}\end{array}\right]=\left[\begin{array} []{c}\mathbf{I}\otimes\mathbf{B}_{c}\\ \mathbf{A}_{c}^{\top}\otimes\mathbf{I}\end{array}\right]_{2d^{2}\times d^{2}}, \tag{31}\]

where \(\otimes\) denotes the Kronecker product.

Note that \(\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{\theta}^{*}}\right)_{j}\) is the \(j\)-th column of matrix \(\nabla_{\mathbf{\theta}}f_{\mathbf{\theta}_{c}}\) and it falls precisely within the defined \(\mathbf{\Omega}_{k}\) invariant manifold or sub-\(\mathbf{\Omega}_{k}\) invariant manifold \(\mathbf{\Omega}\). Therefore, we have:

\[\left(\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{\theta}_{c}}\right)_{j}\right)^{ \top}\mathbf{v}=0\quad\forall 1\leq j\leq d^{2}, \tag{32}\]

which implies that \(\mathbf{v}\) is orthogonal to the image of \(\nabla_{\mathbf{\theta}}\left(\mathbf{f}_{\mathbf{\theta}_{c}}\right)\), placing it in the null space of \(\mathbf{H}^{(1)}(\mathbf{\theta}_{c})\).

As a result, we have:

\[\mathbf{H}(\mathbf{\theta}_{c})\mathbf{v}=\left(\mathbf{H}^{(1)}(\mathbf{\theta}_{c})+\mathbf{H}^{(2)} (\mathbf{\theta}_{c})\right)\mathbf{v}=\mathbf{H}^{(2)}(\mathbf{\theta}_{c})\mathbf{v}=\lambda\mathbf{v}.\]

Thus, the eigenvector \(\mathbf{v}\) of the Hessian \(\mathbf{H}(\mathbf{\theta}_{c})\), corresponding to the negative eigenvalue \(\lambda\), is also an eigenvector of \(\mathbf{H}^{(2)}(\mathbf{\theta}_{c})\), confirming that \(\mathbf{v}\) is within the span of the eigenvectors of \(\mathbf{H}^{(2)}(\mathbf{\theta}_{c})\). 

### Transition to the Next Rank-level Invariant Manifold

**Proposition A.8**.: _The linear combination of the eigenvectors of \(\mathbf{H}^{(2)}\): \(c_{1}(\mathbf{v}\otimes\mathbf{e}_{1})+c_{2}(\mathbf{v}\otimes\mathbf{e}_{2})+\cdots+c_{d}(\bm {v}\otimes\mathbf{e}_{d})\) falls within the invariant manifold \(\mathbf{\Omega}_{1}(\mathbf{c})\), where \(\mathbf{c}=(c_{1},c_{2},\cdots,c_{d})^{\top}\)._

Proof.: Notice that

\[c_{1}(\mathbf{v}\otimes\mathbf{e}_{1})+c_{2}(\mathbf{v}\otimes\mathbf{e}_{2})+ \cdots+c_{d}(\mathbf{v}\otimes\mathbf{e}_{d})\] \[=\mathbf{v}\otimes[c_{1}\mathbf{e}_{1}+c_{2}\mathbf{e}_{2}+\cdots+c_{d}\mathbf{e }_{d}] \tag{33}\] \[=\mathbf{v}\otimes\mathbf{c}.\]

By Definition A.1, the data-independent invariant manifold generated by \(\mathbf{c}\) is \(\mathbf{\Omega}_{1}(\mathbf{c})=\{(\mathbf{A},\mathbf{B})|\mathbf{a}_{i},\mathbf{b}_{j}\in\mathrm{span} \{\mathbf{c}\},\forall 1\leq i,j\leq d\}\). If \(\mathbf{\theta}=(\mathbf{A},\mathbf{B})\in\mathbf{\Omega}_{1}(\mathbf{c})\), then \(\mathbf{A},\mathbf{B}\) must take the form

\[\mathbf{A}=\left[\begin{matrix}\beta_{1}\mathbf{c}\\ \beta_{2}\mathbf{c}\\ \vdots\\ \beta_{d}\mathbf{c}\end{matrix}\right],\quad\mathbf{B}=\left[\begin{matrix}\beta_{d+ 1}\mathbf{c}\\ \beta_{d+2}\mathbf{c}\\ \vdots\\ \beta_{2d}\mathbf{c}\end{matrix}\right], \tag{34}\]

for some \(\mathbf{\beta}=[\beta_{1},\beta_{2},\cdots,\beta_{2d}]^{\top}\in\mathbb{R}^{2d}\), and the vectorized parameter \(\mathbf{\theta}=\mathrm{vec}((\mathbf{A},\mathbf{B}))\in\mathbb{R}^{2d}\) takes the form \(\mathbf{\beta}\otimes\mathbf{c}\). Let \(\mathbf{\beta}=\mathbf{v}\), and the proof is complete. 

**Lemma A.4**.: _Suppose \(\mathbf{\alpha}_{1},\mathbf{\alpha}_{2},\cdots,\mathbf{\alpha}_{k+1}\in\mathbb{R}^{d}\) are linearly independent, the data-independent invariant manifold exhibits the property \(\mathbf{\Omega}_{k}(\mathbf{\alpha}_{1},\mathbf{\alpha}_{2},\cdots,\mathbf{\alpha}_{k})+\mathbf{ \Omega}_{1}(\mathbf{\alpha}_{k+1})=\mathbf{\Omega}_{k+1}(\mathbf{\alpha}_{1},\mathbf{\alpha}_{2 },\cdots,\mathbf{\alpha}_{k+1})\)._

Proof.: Assume that \(\mathbf{\theta}=(\mathbf{A},\mathbf{B})\in\mathbf{\Omega}_{k+1}(\mathbf{\alpha}_{1},\mathbf{\alpha}_{2 },\cdots,\mathbf{\alpha}_{k+1})\). Then \(\mathbf{A},\mathbf{B}\) should adopt the form:

\[A=\sum_{i=1}^{k}\mathbf{\beta}_{i}\mathbf{\alpha}_{i}^{\top}+\mathbf{\beta}_{k+1}\mathbf{ \alpha}_{k+1}^{\top},B=\sum_{i=1}^{k}\mathbf{\gamma}_{i}\mathbf{\alpha}_{i}^{\top}+ \mathbf{\gamma}_{k+1}\mathbf{\alpha}_{k+1}^{\top}. \tag{35}\]Denote \(\mathbf{c}_{i}=\begin{bmatrix}\mathbf{\beta}_{i}\\ \mathbf{\gamma}_{i}\end{bmatrix}\in\mathbb{R}^{2d}\), then the vectorized parameters \(\mathbf{\theta}:=\text{vec}(\mathbf{\theta})\) can be expressed as:

\[\mathbf{\theta}=\sum\limits_{i=1}^{k}\mathbf{c}_{i}\otimes\mathbf{\alpha}_{i}+\mathbf{c}_{k+1} \otimes\mathbf{\alpha}_{k+1}. \tag{36}\]

Since we know that \(\sum\limits_{i=1}^{k}\mathbf{c}_{i}\otimes\mathbf{\alpha}_{i}\in\mathbf{\Omega}_{k}(\mathbf{ \alpha}_{1},\mathbf{\alpha}_{2},\cdots,\mathbf{\alpha}_{k})\) and \(\mathbf{c}_{k+1}\otimes\mathbf{\alpha}_{k+1}\in\mathbf{\Omega}_{1}(\mathbf{\alpha})\), it is straightforward to validate that \(\mathbf{\Omega}_{k+1}(\mathbf{\alpha}_{1},\mathbf{\alpha}_{2},\cdots,\mathbf{\alpha}_{k+1})= \mathbf{\Omega}_{k}(\mathbf{\alpha}_{1},\mathbf{\alpha}_{2},\cdots,\mathbf{\alpha}_{k})+\mathbf{ \Omega}_{1}(\mathbf{\alpha}_{k+1})\). 

**Assumption A.1** (**Unique Top Eigenvalue**).: _Let \(\delta\mathbf{M}=(\mathbf{A}_{c}\mathbf{B}_{c}-\mathbf{M})_{S_{m}}\) be the residual matrix at the critical point \(\mathbf{\theta}_{c}=(\mathbf{A}_{c},\mathbf{B}_{c})\). Assume that the top eigenvalue of the matrix \(\begin{bmatrix}\mathbf{0}&-\delta\mathbf{M}\\ -\delta\mathbf{M}^{\top}&\mathbf{0}\end{bmatrix}\) is unique._

**Assumption A.2** (**Second-order Stationary Point**).: _Let \(\mathbf{\Omega}\) be an \(\mathbf{\Omega}_{k}\) invariant manifold or sub-\(\mathbf{\Omega}_{k}\) invariant manifold defined in Prop. A.1 or A.2. Assume \(\mathbf{\theta}_{c}\) is a second-order stationary point within \(\mathbf{\Omega}\), i.e., \(\nabla R_{S}(\mathbf{\theta}_{c})=0\) and \(\mathbf{\theta}^{\top}\nabla^{2}R_{S}(\mathbf{\theta}_{c})\mathbf{\theta}\geq 0\) for all \(\mathbf{\theta}\in\mathbf{\Omega}\)._

**Theorem A.4** (**Transition to the Next Rank-level Invariant Manifold**).: _Consider the dynamics \(\dot{\mathbf{\theta}}=-\nabla R_{S}(\mathbf{\theta})\). Let \(\varphi(\mathbf{\theta}_{0},t)\) denote the value of \(\mathbf{\theta}(t)\) when \(\mathbf{\theta}(0)=\mathbf{\theta}_{0}\). Let \(\mathbf{\Omega}\) be a \(\mathbf{\Omega}_{k}\) invariant manifold or sub-\(\mathbf{\Omega}_{k}\) invariant manifold. Let \(\mathbf{\theta}_{c}\in\mathbf{\Omega}\) be a critical point satisfying Assump. A.1 and A.2. Then, for randomly selected \(\mathbf{\theta}_{0}\), with probability 1 with respect to \(\mathbf{\theta}_{0}\), the limit_

\[\tilde{\varphi}(\mathbf{\theta}_{c},t):=\lim_{\alpha\to 0}\varphi\left(\mathbf{\theta}_{ c}+\alpha\mathbf{\theta}_{0},t+\frac{1}{\lambda_{1}}\log\frac{1}{\alpha}\right) \tag{37}\]

_exists and falls into an invariant manifold \(\mathbf{\Omega}_{k+1}\). Here \(\lambda_{1}\) is the top eigenvalue of negative Hessian \(-\nabla^{2}R_{S}(\mathbf{\theta}_{c})\)._

Proof.: At the critical point \(\mathbf{\theta}_{c}\), we denote the negative Hessian as \(\mathbf{H}:=-\nabla^{2}R_{S}(\mathbf{\theta}_{c})\). Let \(\lambda_{1}\in\mathbb{R}\) be the largest eigenvalue of \(\mathbf{H}\), with corresponding eigenvectors \(\mathbf{q}_{11},\mathbf{q}_{12},\cdots,\mathbf{q}_{1l_{1}}\).

Denote \(c_{j}=\langle\mathbf{\theta}_{0}-\mathbf{\theta}_{c},\mathbf{q}_{1j}\rangle,\forall 1\leq j \leq l_{1}\), and \(\mathbf{v}_{1}=\sum_{j=1}^{l_{1}}c_{j}\mathbf{q}_{1j}\). For a randomly selected \(\mathbf{\theta}_{0}\), with probability 1, there exists at least one \(j\) such that \(c_{j}\neq 0\).

Consider the path \(\mathbf{z}_{\alpha}(t):=\varphi\left(\mathbf{\theta}_{c}+\alpha\mathbf{v}_{1},t+\frac{1}{ \lambda_{1}}\log\frac{1}{\alpha}\right)\) for every \(\alpha>0\). By Prop. A.6, the limit \(\mathbf{z}(t):=\lim_{\alpha\to 0}\mathbf{z}_{\alpha}(t)\) exists and satisfies the dynamics \(\mathbf{z}(t)=\varphi(\mathbf{z}(0),t)\).

Furthermore, \(\forall t\in\mathbb{R}\), there exists a constant \(C>0\) such that

\[\left\|\varphi\left(\mathbf{\theta}_{c}+\alpha\mathbf{\theta}_{0},t+\frac{1}{\lambda_{ 1}}\log\frac{1}{\alpha}\right)-\mathbf{z}(t)\right\|_{2}\leq C\alpha^{\frac{\lambda _{1}-\lambda_{2}}{2\lambda_{1}-\lambda_{2}}}\]

for every sufficiently small \(\alpha\), where \(\lambda_{1}-\lambda_{2}>0\) is the eigenvalue gap.

This implies that the limit \(\lim_{\alpha\to 0}\varphi\left(\mathbf{\theta}_{c}+\alpha\mathbf{\theta}_{0},t+\frac{1}{ \lambda_{1}}\log\frac{1}{\alpha}\right)\) exists and

\[\tilde{\varphi}(\mathbf{\theta}_{c},t):=\lim_{\alpha\to 0}\varphi\left(\mathbf{\theta}_{ c}+\alpha\mathbf{\theta}_{0},t+\frac{1}{\lambda_{1}}\log\frac{1}{\alpha} \right)=\lim_{\alpha\to 0}\varphi\left(\mathbf{\theta}_{c}+\alpha\mathbf{v}_{1},t+\frac{1}{ \lambda_{1}}\log\frac{1}{\alpha}\right).\]

Assuming \(\mathbf{\theta}_{c}\in\mathbf{\Omega}_{k}\) and satisfies Assumps. A.1 and A.2, we aim to show the existence of a rank-(\(k+1\)) invariant manifold \(\mathbf{\Omega}_{k+1}\) containing \(\mathbf{\theta}_{c}+\alpha\mathbf{v}_{1}\).

Define the following matrices:

\[\mathbf{H}^{(1)}(\mathbf{\theta}_{c}):=\sum\limits_{i,j=1}^{d^{2}}\nabla_{\mathbf{\theta}_{ c}}\left(\mathbf{f}_{\mathbf{\theta}_{c}}\right)_{i}\left(\nabla_{\mathbf{\theta}_{c}}\left( \mathbf{f}_{\mathbf{\theta}_{c}}\right)_{j}\right)^{\top},\]

\[\mathbf{H}^{(2)}(\mathbf{\theta}_{c}):=\sum\limits_{i=1}^{d^{2}}\mathbb{E}_{S}\left[( \mathbf{f}_{\mathbf{\theta}_{c}}-\mathbf{f}^{*})_{i}\,\nabla_{\mathbf{\theta}_{c}}^{2}\left( \mathbf{f}_{\mathbf{\theta}_{c}}\right)_{i}\right].\]The Hessian matrix \(\mathbf{H}(\mathbf{\theta}_{c})\) at \(\mathbf{\theta}_{c}\) can be expressed as \(\mathbf{H}(\mathbf{\theta}_{c}):=\mathbf{H}^{(1)}(\mathbf{\theta}_{c})+\mathbf{H}^{(2)}(\mathbf{\theta}_ {c})\), and we have \(\mathbf{H}=-\mathbf{H}(\mathbf{\theta}_{c})\).

Assump. A.1 and Lem. A.2 imply that there exist exactly \(d\) eigenvectors associated with the top eigenvalue \(\lambda_{1}\) of \(-\mathbf{H}^{(2)}(\mathbf{\theta}_{c})\). These eigenvectors are of the form \(\mathbf{v}\otimes\mathbf{e}_{i}\) for \(i=1,\ldots,d\), where \(\mathbf{v}\in\mathbb{R}^{2d}\) is a vector to be determined and \(\mathbf{e}_{i}\) is the \(i\)-th standard basis vector in \(\mathbb{R}^{d}\). By Assump. A.2 and Lem. A.3, the eigenvectors corresponding to \(\lambda_{1}\) of \(\mathbf{H}\) are contained within the span of the eigenvectors associated with the negative eigenvalues of \(\mathbf{H}^{(2)}(\mathbf{\theta}_{c})\).

Prop. A.8 ensures that the escaping direction \(\mathbf{v}_{1}\) lies within a rank-1 invariant manifold \(\mathbf{\Omega}_{1}\). Lem. A.4 then guarantees the existence of an invariant manifold \(\mathbf{\Omega}_{k+1}\) that includes \(\mathbf{\theta}_{c}+\alpha\mathbf{v}_{1}\). Since \(\mathbf{\Omega}_{k+1}\) is invariant under the gradient flow, the trajectory \(\varphi\left(\mathbf{\theta}_{c}+\alpha\mathbf{v}_{1},t+\frac{1}{\lambda_{1}}\log\frac {1}{\alpha}\right)\) remains within \(\mathbf{\Omega}_{k+1}\).

Finally, since \(\mathbf{\Omega}_{k+1}\) is a closed subspace, the limit \(\tilde{\varphi}(\mathbf{\theta}_{c},t)\) lies in \(\bar{\mathbf{\Omega}}_{k+1}\), concluding the proof. 

### Example of Coincident Top Eigenvalues

Consider the \(2\times 2\) matrix completion problem: \(\mathbf{M}=\begin{bmatrix}2&\star\\ \star&2\end{bmatrix}\). In this case, the two numbers on the diagonal are identical, which causes the maximum singular value of the residual matrix at the origin to be non-unique, violating Assump. 1. Consequently, the training process will jump directly from the rank 0 to the rank 2 invariant manifold, thereby missing the lowest rank solution of rank 1. This behavior is demonstrated in Fig. A2, which shows experimental results for this scenario.

### Minimum Rank

**Theorem A.5** (Minimum Rank).: _Let \(\mathbf{\Omega}\) denote an invariant as defined previously. Assume \(\mathbf{W}_{t}\) achieves a global minimum within each invariant manifold \(\mathbf{\Omega}_{k}\). If the limit \(\widehat{\mathbf{W}}=\lim_{\alpha\to 0}\mathbf{W}_{\infty}(\alpha I)\) exists and is a global optimum with \(\widehat{\mathbf{W}}(i,j)=\mathbf{M}(i,j)\) for all \((i,j)\in S_{\mathbf{x}}\), then_

\[\widehat{\mathbf{W}}\in\operatorname*{argmin}_{\mathbf{W}}\operatorname*{rank}(\mathbf{W} )\quad\text{s.t.}\quad\mathbf{W}(i,j)=\mathbf{M}(i,j),\forall(i,j)\in S_{\mathbf{x}}. \tag{38}\]

Proof.: Consider the invariant manifold \(\mathbf{\Omega}_{k}\), which is defined as follows:

\[\mathbf{\Omega}_{k}:=\mathbf{\Omega}_{k}(\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k})=\{( \mathbf{A},\mathbf{B})|\mathbf{a}_{i},\mathbf{b}_{\cdot,j}\in\operatorname*{span}\{\mathbf{\alpha }_{1},\cdots,\mathbf{\alpha}_{k}\},\forall 1\leq i,j\leq d\},\]

where \(\mathbf{a}_{i}\) denotes the \(i\)-th row of \(\mathbf{A}\), \(\mathbf{b}_{\cdot,j}\) denotes the \(j\)-th column of \(\mathbf{B}\), and \(\mathbf{\alpha}_{1},\ldots,\mathbf{\alpha}_{k}\) are independent vectors that span the invariant subspace associated with \(\mathbf{\Omega}_{k}\).

According to Thm. A.4, the training trajectory adheres to a hierarchical traversal across invariant manifolds. For any matrix \(\mathbf{C}\) with \(\operatorname*{rank}(\mathbf{C})\leq k\), we will show that there always exists \(\mathbf{\theta}=(\mathbf{A},\mathbf{B})\in\mathbf{\Omega}_{k}\) such that \(\mathbf{A}\mathbf{B}=\mathbf{C}\). Therefore, \(\mathbf{\Omega}_{k}\) contains all matrices of rank \(k\).

In fact, Since \(\operatorname*{rank}(\mathbf{C})\leq k\), we can express \(\mathbf{C}\) as a sum of \(k\) rank-one matrices: \(\mathbf{C}=\sum_{i=1}^{k}\mathbf{u}_{i}\mathbf{v}_{i}^{\top}\) where \(\mathbf{u}_{i}\) and \(\mathbf{v}_{i}\) are column vectors. By the definition of \(\mathbf{\Omega}_{k}\), for any \(\mathbf{\theta}=(\mathbf{A},\mathbf{B})\in\mathbf{\Omega}_{k}\), each

Figure A2: **Analysis of matrix completion for \(\mathbf{M}\) with identical diagonal elements. (a) Training loss under small initialization. (b-d) Singular value evolution for \(\mathbf{A},\mathbf{B},\mathbf{W}_{\text{aug}}\). Simultaneous growth of singular values results in direct convergence to a rank-2 invariant manifold.**

row of \(\mathbf{A}\) and each column of \(\mathbf{B}\) can be expressed as a linear combination of \(\{\mathbf{\alpha}_{1},\cdots,\mathbf{\alpha}_{k}\}\): \(\mathbf{a}_{i}=\sum_{j=1}^{k}c_{ij}\mathbf{\alpha}_{j}\,\mathbf{b}_{\cdot,j}=\sum_{i=1}^{k}d _{ij}\mathbf{\alpha}_{i}\) where \(c_{ij}\) and \(d_{ij}\) are scalars. We can write:

\[\mathbf{A}=\begin{bmatrix}\sum_{j=1}^{k}c_{1j}\mathbf{\alpha}_{j}\\ \vdots\\ \sum_{j=1}^{k}c_{dj}\mathbf{\alpha}_{j}\end{bmatrix},\mathbf{B}=\begin{bmatrix}\sum_{i =1}^{k}d_{i1}\mathbf{\alpha}_{i}&\cdots&\sum_{i=1}^{k}d_{id}\mathbf{\alpha}_{i}\end{bmatrix}.\]

Now, we can express the product \(\mathbf{AB}\) as: \(\mathbf{AB}=\sum_{i=1}^{k}\sum_{j=1}^{k}\left(\sum_{l=1}^{d}c_{li}d_{jl}\right)\bm {\alpha}_{i}\mathbf{\alpha}_{j}^{\top}\) By choosing appropriate values for \(c_{ij}\) and \(d_{ij}\), we can make \(\mathbf{AB}=\mathbf{C}\). This is possible because the outer products \(\mathbf{\alpha}_{i}\mathbf{\alpha}_{j}^{\top}\) span the same subspace as the rank-one matrices \(\mathbf{u}_{i}\mathbf{v}_{i}^{\top}\) in the expression of \(\mathbf{C}\).

Therefore, for any matrix \(\mathbf{C}\) with \(\text{rank}(\mathbf{C})\leq k\), there always exists \(\mathbf{\theta}=(\mathbf{A},\mathbf{B})\in\mathbf{\Omega}_{k}\) such that \(\mathbf{AB}=\mathbf{C}\).

If the output matrix \(\mathbf{W}_{t}\) attains optimums within each \(\mathbf{\Omega}_{k}\), it suggests that the optimization process is selecting the best approximation from the set of all possible rank-\(k\) matrices. Provided that each step in the optimization is optimal, the resulting solution will naturally be the matrix with the lowest feasible rank that satisfies the matrix completion criteria, thereby completing the proof. 

### Minimum Nuclear Norm Guarantee

**Lemma A.5** (**Minimal Nuclear Norm Computation**).: _Given a matrix \(\mathbf{M}\) to be completed with observed diagonal entries, i.e., \(\operatorname{diag}(\mathbf{M})=\mathbf{v}\), the minimal nuclear norm solution among all possible completions is \(\|\mathbf{v}\|_{1}\)._

Proof.: The nuclear norm of a matrix is the dual of the spectral norm \(\|\cdot\|_{2}\), defined as:

\[\|\mathbf{A}\|_{*}=\max_{\|\mathbf{X}\|_{2}\leq 1}\langle\mathbf{A},\mathbf{X}\rangle.\]

Given that \(\|\operatorname{diag}(\text{sign}(\mathbf{v}))\|_{2}\leq 1\), for any matrix \(\mathbf{A}\) with \(\operatorname{diag}(\mathbf{A})=\mathbf{v}\), it follows that:

\[\|\mathbf{A}\|_{*}\geq\langle\mathbf{A},\operatorname{diag}(\text{sign}(\mathbf{v})) \rangle=\langle\mathbf{v},\text{sign}(\mathbf{v})\rangle=\|\mathbf{v}\|_{1}.\]

Specifically, the nuclear norm of the diagonal matrix with \(\mathbf{v}\) on its diagonal is \(\|\operatorname{diag}(\mathbf{v})\|_{*}=\|\mathbf{v}\|_{1}\), which establishes that the diagonal matrix with \(\mathbf{v}\) is indeed a minimizer for the nuclear norm. 

**Diagonal Observations**

**Proposition A.9** (**Minimum Nuclear Norm Guarantee in Diagonal Case**).: _Consider the dynamics \(\hat{\mathbf{\theta}}=-\nabla R_{S}(\mathbf{\theta})\), where \(\mathbf{\theta}(t)=(\mathbf{A}(t),\mathbf{B}(t))\) and denote \(\mathbf{W}_{t}=\mathbf{A}(t)\mathbf{B}(t)\). If the observation data is diagonal, and if for a full rank initialization \(\mathbf{W}_{0}\), the limit \(\widehat{\mathbf{W}}=\lim_{\alpha\to 0}\mathbf{W}_{\infty}(\alpha\mathbf{W}_{0})\) exists and is a global optimum with \(\widehat{\mathbf{W}}_{ij}=\mathbf{M}_{ij}\) for all \((i,j)\in S_{\mathbf{x}}\), then_

\[\widehat{\mathbf{W}}\in\operatorname{argmin}_{\mathbf{W}}\|\mathbf{W}\|_{*}\quad\text{s.t. }\quad\mathbf{W}_{ij}=\mathbf{M}_{ij},\forall(i,j)\in S_{\mathbf{x}}. \tag{39}\]

Proof.: Without loss of generality, assume that \(\mathbf{M}\) is a diagonal matrix given by:

\[\mathbf{M}=\begin{bmatrix}\mu_{1}&&&\\ &\mu_{2}&&\\ &&\ddots&\\ &&&\mu_{d}\end{bmatrix}.\]

By Lem. A.5, the minimal nuclear norm among all possible completions is \(|\mu_{1}|+|\mu_{2}|+\cdots+|\mu_{d}|\). When the matrix to be completed is diagonal, the evolution of the \(i\)-th row of \(\mathbf{A}\) is influenced only by the \(i\)-th column of \(\mathbf{B}\). Hence, the dynamics decouple into \(d\) independent parts, each equivalent to learning a scalar \(\mu_{i}\). The learning process thus unfolds in \(d\) stages, with each stage passing through a critical point to learn a respective \(\mu_{i}\).

By Lem. A.2, the second term of the Hessian matrix can be expressed as:

\[\mathbf{H}^{(2)}=\begin{bmatrix}\mathbf{0}&-\mathbf{\delta}\mathbf{M}\otimes\mathbf{I}_{d}\\ -\mathbf{\delta}\mathbf{M}^{\top}\otimes\mathbf{I}_{d}&\mathbf{0}\end{bmatrix}.\]

According to Lem. A.2, the \(d\) eigenvectors of \(\mathbf{H}^{(2)}\) take the form \(\mathbf{v}\otimes\mathbf{e}_{1},\mathbf{v}\otimes\mathbf{e}_{2},\ldots,\mathbf{v}\otimes\mathbf{e}_{d} \in\mathbb{R}^{2d^{2}}\), where \(\mathbf{v}\in\mathbb{R}^{2d}\) is a vector to be determined and \(\mathbf{e}_{i}\) is the unit vector corresponding to the \(i\)-th column of the identity matrix \(\mathbf{I}_{d}\in\mathbb{R}^{d\times d}\).

(i) Suppose \(\mu_{1}>\mu_{2}>\cdots>\mu_{d}>0\).

With a infinitesimal initialization, the training dynamics first focus on the element with the largest singular value, then proceed sequentially to blocks with smaller singular values. This pattern of learning is consistent with the concept of "sequential learning" as reported in the literature (Gidel et al., 2019; Gissin et al., 2019; Jiang et al., 2023).

For a diagonal observation matrix, the residual matrix \(\delta\mathbf{M}\) at any critical point remains a diagonal matrix. While starting to learn \(\mu_{i}\) from a critical point, direct calculation confirms that vector \(\mathbf{v}=[\mathbf{e}_{i},\mathbf{e}_{i}]^{\top}\in\mathbb{R}^{2d}\). Escaping from each saddle point \(\mathbf{\theta}_{c}\), the trajectory \(\mathbf{\theta}(t)-\mathbf{\theta}_{c}\) approximates \(\sum_{i=1}^{d}c_{i}(\mathbf{v}\otimes\mathbf{e}_{i})\), which satisfies \(\mathbf{B}_{,i}=(\mathbf{A}^{\top})_{,i}\). Thus, learning a diagonal matrix \(\mathbf{M}\) using the asymmetric model \(\mathbf{AB}\) is equivalent to using a symmetric model \(\mathbf{AA}^{\top}\). The final outcome ensures that \(\operatorname{diag}(\mathbf{AB})=\operatorname{diag}(\mathbf{AA}^{\top})=\operatorname {diag}(\mathbf{M})\).

The nuclear norm of \(\mathbf{AA}^{\top}\) equals the sum of its eigenvalues, which is precisely the trace of the matrix, and \(\operatorname{tr}(\mathbf{AA}^{\top})=\operatorname{tr}(\mathbf{M})=\mu_{1}+\mu_{2}+ \cdots+\mu_{d}\). Therefore, the nuclear norm of the learned matrix \(\mathbf{W}=\mathbf{AB}=\mathbf{AA}^{\top}\) remains \(|\mu_{1}|+|\mu_{2}|+\cdots+|\mu_{d}|\).

(ii) If some \(\mu_{i}<0\), assume without loss of generality that \(|\mu_{1}|>|\mu_{2}|>\cdots>|\mu_{n}|>0\).

While starting to learn \(\mu_{i}\) from a critical point, direct calculation confirms that \(\mathbf{v}=[\mathbf{e}_{i},\operatorname{sign}(\mu_{i})\mathbf{e}_{i}]^{\top}\in\mathbb{ R}^{2d}\). Escaping from each saddle point \(\mathbf{\theta}_{c}\), the trajectory \(\mathbf{\theta}(t)-\mathbf{\theta}_{c}\) approximates \(\sum_{i=1}^{d}c_{i}(\mathbf{v}\otimes\mathbf{e}_{i})\), satisfying \(\mathbf{B}_{,i}=\operatorname{sign}(\mu_{i})(\mathbf{A}^{\top})_{,i}\). Hence, \(\mathbf{AB}=\mathbf{AA}^{\top}\mathbf{Q}\), where \(\mathbf{Q}\) is an orthogonal matrix given by:

\[\mathbf{Q}=\begin{bmatrix}\operatorname{sign}(\mu_{1})&&&&\\ &\operatorname{sign}(\mu_{2})&&\\ &&\ddots&\\ &&&\operatorname{sign}(\mu_{d})\end{bmatrix}.\]

The final result ensures that \(\operatorname{diag}(\mathbf{AB})=\operatorname{diag}(\mathbf{AA}^{\top}\mathbf{Q})= \operatorname{diag}(\mathbf{M})\), meaning \(\operatorname{diag}(\mathbf{AA}^{\top})=\operatorname{diag}(\mathbf{QM})\). The nuclear norm of \(\mathbf{AA}^{\top}\) equals the sum of its eigenvalues, which is the trace of the matrix, and \(\operatorname{tr}(\mathbf{AA}^{\top})=\operatorname{tr}(\mathbf{QM})=|\mu_{1}|+|\mu_{2 }|+\cdots+|\mu_{d}|\).

Since an orthogonal transformation does not change the nuclear norm of a matrix, the nuclear norm of the final learned matrix \(\mathbf{W}=\mathbf{AB}=\mathbf{AA}^{\top}\mathbf{Q}\) is still \(|\mu_{1}|+|\mu_{2}|+\cdots+|\mu_{d}|\). 

**Disconnected with Complete Bipartite Components Theorem A.6** (**Minimum Nuclear Norm Guarantee)**.: _Consider the dynamics \(\dot{\mathbf{\theta}}=-\nabla R_{S}(\mathbf{\theta})\), where \(\mathbf{\theta}(t)=(\mathbf{A}(t),\mathbf{B}(t))\), and let \(\mathbf{W}_{t}=\mathbf{A}(t)\mathbf{B}(t)\). If the observation graph associated with the matrix \(\mathbf{M}\) to be completed is disconnected with complete bipartite components, and if for a full rank initialization \(\mathbf{W}_{0}\), the limit \(\widehat{\mathbf{W}}=\lim_{\alpha\to 0}\mathbf{W}_{\infty}(\alpha\mathbf{W}_{0})\) exists and is a global optimum with \(\widehat{\mathbf{W}}_{ij}=\mathbf{M}_{ij}\) for all \((i,j)\in S_{\mathbf{x}}\), then_

\[\widehat{\mathbf{W}}\in\operatorname{argmin}_{\mathbf{W}}\|\mathbf{W}\|_{*}\quad\text{s.t. }\quad\mathbf{W}_{ij}=\mathbf{M}_{ij},\forall(i,j)\in S_{\mathbf{x}}. \tag{40}\]

Proof.: Consider a matrix \(\mathbf{M}\in\mathbb{R}^{d\times d}\) composed of \(m\) connected components, with each component forming a complete bipartite subgraph. Since \(\mathbf{M}\) is disconnected, it can be represented in a block diagonal form without loss of generality:

\[\mathbf{M}=\begin{bmatrix}\mathbf{M}_{1}&&&\\ &\mathbf{M}_{2}&&\\ &&\ddots&\\ &&&\mathbf{M}_{m}\end{bmatrix},\]

where each block \(\mathbf{M}_{i}\in\mathbb{R}^{d_{i}\times d_{i}^{\prime}}\), and \(\sum_{i=1}^{m}d_{i}=d,\sum_{i=1}^{m}d_{i}^{\prime}=d\), representing the sum of the dimensions of the blocks.

Each block \(\mathbf{M}_{i}\) corresponds some singular values of the corresponding Hessian matrix at a critical point. With a infinitesimal initialization, the training dynamics first focus on the block with the largest singular value, then proceed sequentially to blocks with smaller singular values. This pattern of learning is consistent with the concept of "sequential learning" as reported in the literature (Gidel et al., 2019; Gissin et al., 2019; Jiang et al., 2023).

Since each connected component of \(\mathbf{M}\) forms a complete bipartite subgraph, the block \(\mathbf{M}_{i}\) is fully observed. We can do singular value decomposition (SVD) on each sub-block \(\mathbf{M}_{i}\) as \(\mathbf{M}_{i}=\mathbf{U}_{i}\Sigma_{i}\mathbf{V}_{i}^{\top}\), where \(\mathbf{U}_{i}\) and \(\mathbf{V}_{i}\) are orthogonal matrices, and \(\Sigma_{i}\) is a diagonal matrix with the singular values of \(\mathbf{M}_{i}\).

Construct block diagonal matrices \(\mathbf{U}\) and \(\mathbf{V}\) as follows:

\[\mathbf{U}=\begin{bmatrix}\mathbf{U}_{1}&&&\\ &\mathbf{U}_{2}&&\\ &&\ddots&\\ &&&\mathbf{U}_{m}\end{bmatrix},\quad\mathbf{V}=\begin{bmatrix}\mathbf{V}_{1}&&&\\ &\mathbf{V}_{2}&&\\ &&\ddots&\\ &&&\mathbf{V}_{m}\end{bmatrix}.\]

This leads to the diagonal matrix:

\[\mathbf{U}\mathbf{M}\mathbf{V}^{\top}=\begin{bmatrix}\mathbf{\Sigma}_{1}&&&\\ &\mathbf{\Sigma}_{2}&&\\ &&\ddots&\\ &&&\mathbf{\Sigma}_{m}\end{bmatrix}=\begin{bmatrix}\mu_{1}&&&\\ &\mu_{2}&&\\ &&\ddots&\\ &&&\mu_{d}\end{bmatrix}.\]

Orthogonal transformations preserve the nuclear norm, so by Lem. A.5, the minimal nuclear norm among all possible completions is the sum of the absolute values of the diagonal entries, i.e., \(|\mu_{1}|+|\mu_{2}|+\cdots+|\mu_{d}|\).

Consider an incomplete matrix \(\mathbf{M}\) whose associated observational graph is divided into \(m\) connected components, denoted by \(L_{1},L_{2},\ldots,L_{m}\). For each component \(L_{p}\), we define \(S_{\mathbf{x}}^{L_{p}}\) as the subset of observed indices within \(L_{p}\), where \(1\leq p\leq m\) and \(S_{\mathbf{x}}\) is the set of all observed indices.

For each \(L_{p}\), we can identify row indices \(R_{p}\) and column indices \(C_{p}\) corresponding to the observed entries in \(L_{p}\) as follows:

\[R_{p}=\{i|\exists j:(i,j)\in S_{\mathbf{x}}^{L_{p}}\},\quad C_{p}=\{j|\exists i:(i,j)\in S_{\mathbf{x}}^{L_{p}}\}.\]

Here, \(R_{p}\) includes the row indices and \(C_{p}\) includes the column indices of the entries observed in \(L_{p}\). Define \(\mathbf{A}^{L_{p}}\) and \(\mathbf{B}^{L_{p}}\) as the submatrices of \(\mathbf{A}\) and \(\mathbf{B}\) corresponding to \(R_{p}\) and \(C_{p}\), respectively. The evolution of \(\mathbf{A}^{L_{p}}\) is influenced only by \(\mathbf{B}^{L_{p}}\). Thus, the dynamics decouple into \(m\) independent parts, each equivalent to learning a fully observed matrix \(\mathbf{M}_{i}\).

Accordingly, we partition \(\mathbf{A}\) and \(\mathbf{B}\) into \(m\) blocks:

\[\mathbf{A}=\begin{bmatrix}\mathbf{A}_{1}\\ \vdots\\ \mathbf{A}_{m}\end{bmatrix},\quad\mathbf{B}=\begin{bmatrix}\mathbf{B}_{1}&\cdots&\mathbf{B}_ {m}\end{bmatrix},\]

where \(\mathbf{A}_{i}=\mathbf{A}^{L_{i}}\) and \(\mathbf{B}_{j}=\mathbf{B}^{L_{j}}\).

Denote \(L_{i}=\|\mathbf{A}_{i}\mathbf{B}_{i}-\mathbf{M}_{i}\|_{2}^{2}\). The overall loss function \(L=\sum_{i=1}^{m}L_{i}\) can be decomposed into \(m\) independent parts. Performing orthogonal transformations \(\tilde{\mathbf{A}}_{i}=\mathbf{U}_{i}^{\top}\mathbf{A}_{i}\) and \(\tilde{\mathbf{B}}_{i}=\mathbf{B}_{i}\mathbf{V}_{i}\), we obtain a diagonal loss \(\tilde{L}_{i}=\|\tilde{\mathbf{A}}_{i}\tilde{\mathbf{B}}_{i}-\mathbf{\Sigma}_{i}\|_{2}^{2}\) for \(1\leq i\leq C\).

Since gradient descent is the steepest descent in the \(l_{2}\) norm and orthogonal transformations preserve this norm, the dynamics of optimizing \(\tilde{L}_{i}\) are equivalent to those of optimizing \(L_{i}\).

Without loss of generality, assume \(\mu_{1}>\mu_{2}>\cdots>\mu_{d}>0\). Otherwise, as with Prop. A.9, a sign orthogonal transformation \(\mathbf{Q}\) can be applied without changing the nuclear norm.

By Prop. A.9, the learning result for a diagonal matrix implies \(\tilde{\mathbf{B}}_{i}=\tilde{\mathbf{A}}_{i}^{\top}\), which means \(\mathbf{B}_{i}\mathbf{V}_{i}=\mathbf{U}_{i}^{\top}\mathbf{A}_{i}^{\top}\).

The final learning result

\[\tilde{\mathbf{A}}=\begin{bmatrix}\tilde{\mathbf{A}}_{1}\\ \vdots\\ \tilde{\mathbf{A}}_{m}\end{bmatrix},\quad\tilde{\mathbf{B}}=\begin{bmatrix}\tilde{\mathbf{ B}}_{1}&\cdots&\tilde{\mathbf{B}}_{m}\end{bmatrix},\]

satisfies \(\tilde{\mathbf{B}}=\tilde{\mathbf{A}}^{\top}\).

The result ensures that \(\mathrm{diag}(\tilde{\mathbf{A}}\tilde{\mathbf{B}})=\mathrm{diag}(\tilde{\mathbf{A}} \tilde{\mathbf{A}}^{\top})=\mathrm{diag}(\mathbf{\Sigma})\). The nuclear norm of \(\tilde{\mathbf{A}}\tilde{\mathbf{A}}^{\top}\) equals the sum of its eigenvalues, which is the trace of the matrix, and \(\text{Tr}(\tilde{\mathbf{A}}\tilde{\mathbf{A}}^{\top})=\text{Tr}(\Sigma)=|\mu_{1}|+| \mu_{2}|+\cdots+|\mu_{d}|\).

Since orthogonal transformations do not alter the nuclear norm of a matrix, the nuclear norm of \(\mathbf{W}=\mathbf{A}\mathbf{B}\) is also \(|\mu_{1}|+|\mu_{2}|+\cdots+|\mu_{d}|\), concluding the proof. 

## Appendix B Experimental Setup and Supplementary Experiments

In this section, we present the supplementary experiments mentioned in the main text and the details of experiments.

### Experimental Setup

For all our experiments, we employ gradient descent with a carefully chosen small learning rate. A learning rate is deemed suitable when it yields a smooth, monotonically decreasing training trajectory for the loss function, free from any abrupt fluctuations or oscillations. We initialize all model parameters using a Gaussian distribution with a mean of zero and a variance that is detailed for each specific experiment. Because of the small size of the experiment, the experiment can be completed on a single CPU.

The criterion for the sufficiency of training in all cases is a training loss that falls below \(10^{-10}\). To ascertain the rank of the matrix produced by the learning process, we utilize a technique of extrapolation with an infinitesimally small initialization. As depicted in Fig. 3(b), if a singular value persistently diminishes in response to decreasing initialization magnitudes, it is then inferred that such a singular value will not contribute to the rank in the context of an infinitesimal initialization.

We have included code in the Supplementary Material that determines the connectivity of a partially observed matrix and provides specific examples illustrating the implicit regularization effects. This code can be used to reproduce our results and explore the relationship between data connectivity and the implicit biases of matrix factorization models in various matrix completion scenarios.

### Connectivity Experiments

In the connectivity experiments corresponding to Fig. 1, we explore the behavior of randomly generated \(4\times 4\) matrices with intrinsic ranks of 1, 2, and 3. To investigate the impact of sampling density on matrix reconstruction, we sample matrices at three different levels: \(2rd-r^{2}\), which meets the threshold for exact reconstruction, \(2rd-r^{2}-1\), which is just below the threshold, and \(2rd-r^{2}+1\), which exceeds the threshold.

For each sampling size, we randomly generate 10 sets of sampling positions. We then assess the connectivity of the sampled positions and compute both the rank and the nuclear norm of the solutions obtained through gradient descent. As an illustration, in Fig. B1, panel (a) presents a scenario with connected sampling positions, panel (b) shows disconnected sampling positions, and panel (c) depicts disconnected sampling with each disconnected component forming a complete bipartite graph.

In the connectivity experiments depicted in Figs. 2(c-d), we examine the behavior of randomly generated matrices of size \(4\times 4\) and \(10\times 10\) with a rank of 1. The matrices are sampled at a size of \(2rd-r^{2}\), which corresponds to the threshold for exact reconstruction. We evaluate two connected and one disconnected sampling patterns.

Fig. B2(a) displays the first connected sampling pattern, where all entries in the first row and the first column are sampled. Fig. B2(b) illustrates the second connected sampling pattern, which forms a "Z" shape across the matrix. Fig. B2(c) shows the disconnected sampling pattern, where the samples are split into two unconnected blocks, one in the top-left and the other in the bottom-right of the matrix. A similar approach is taken for the \(10\times 10\) matrices.

For Figs. 2 (a-b), we performed 100 random initializations for each initialization scale, recorded the mean and standard deviation, and plotted them on the figure. For a sequence \(x_{1},x_{2},\cdots,x_{n}\), the mean is calculated by \(\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\), and the standard deviation is calculated by:

\[\sigma=\sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}.\]

Figs. 2(c-d) demonstrate that when the target matrix has a rank of 1 and the number of samples meets the minimum requirement for reconstruction with connected sampling positions, the matrix factorization model is capable of accurately reconstructing the original target matrix.

In scenarios where the target matrix has a higher rank, we have extended our experiments accordingly. For a randomly chosen \(4\times 4\) matrix with rank 2, we selected a sample count less than the threshold of \(2rd-r^{2}=12\), specifically 10 samples, while ensuring that the sampling pattern is connected, as shown in Fig. B3. The resulting solution from the matrix completion has a rank of 2, which is the minimal rank that fits the sampled data.

Fig. B4 reveals distinct behaviors of the matrix completion depending on the scale of initialization. With a larger initialization, the third and fourth singular values of the completed matrix remain relatively significant, suggesting that the model does not converge to the lowest rank solution. On the other hand, with a smaller initialization, the third and fourth singular values are uniformly small, indicating that the model successfully converges to the lowest rank solution.

### Equivalent Sampling Patterns

For a given sample size, there are different sampling models corresponding to connected or disconnected sampling. As shown in Fig. 11, 7 observations are sampled, but different sampling positions affect connectivity or disconnection. To thoroughly study all possible cases, we examine all sampling cases of a \(3\times 3\) matrix completion, as illustrated in Figure 2(c).

For a \(3\times 3\) matrix, the sample size varies from 1 to 9. When using the matrix decomposition model \(\mathbf{f_{\theta}}=\mathbf{AB}\) for matrix completion, the dynamics obtained by exchanging rows or columns or transposing the matrix to be completed are equivalent. These three operations allow us to divide all sampling patterns equally.

For sample size = 1, there is only one sampling pattern in the equivalent sense, and the observation matrix \(\mathbf{P}\) is:

\[\mathbf{P}_{1}=\begin{bmatrix}1&0&0\\ 0&0&0\\ 0&0&0\end{bmatrix}.\]

where \(1\) indicates that the position is observed and non-zero, and \(0\) means that the position is not observed or is \(0\).

For sample size = 2, there are only 2 sampling patterns in the equivalent sense:

\[\mathbf{P}_{1}=\begin{bmatrix}1&1&0\\ 0&0&0\\ 0&0&0\end{bmatrix},\mathbf{P}_{2}=\begin{bmatrix}1&0&0\\ 0&1&0\\ 0&0&0\end{bmatrix}\]

For sample size = 3, there are only 4 sampling patterns in the equivalent sense:

\[\mathbf{P}_{1}=\begin{bmatrix}1&1&1\\ 0&0&0\\ 0&0&0\end{bmatrix},\mathbf{P}_{2}=\begin{bmatrix}1&1&0\\ 1&0&0\\ 0&0&0\end{bmatrix},\mathbf{P}_{3}=\begin{bmatrix}1&1&0\\ 0&0&1\\ 0&0&0\end{bmatrix},\mathbf{P}_{4}=\begin{bmatrix}1&0&0\\ 0&1&0\\ 0&0&1\end{bmatrix}\]

For sample size = 4, there are only 5 sampling patterns in the equivalent sense:

\[\mathbf{P}_{1}=\begin{bmatrix}1&1&1\\ 1&0&0\\ 0&0&0\end{bmatrix},\mathbf{P}_{2}=\begin{bmatrix}1&1&0\\ 1&1&0\\ 0&0&0\end{bmatrix},\mathbf{P}_{3}=\begin{bmatrix}1&1&0\\ 0&1&1\\ 0&0&1\end{bmatrix},\mathbf{P}_{4}=\begin{bmatrix}1&1&0\\ 1&0&0\\ 0&0&1\end{bmatrix}\]

For sample size = 5, there are only 3 sampling patterns in the equivalent sense:

\[\mathbf{P}_{1}=\begin{bmatrix}1&1&1\\ 1&1&0\\ 0&0&0\end{bmatrix},\mathbf{P}_{2}=\begin{bmatrix}1&1&1\\ 1&0&0\\ 1&0&0\end{bmatrix},\mathbf{P}_{3}=\begin{bmatrix}1&1&0\\ 1&1&0\\ 0&0&1\end{bmatrix}\]

For sample size = 6, there are only 4 sampling patterns in the equivalent sense:

\[\mathbf{P}_{1}=\begin{bmatrix}1&1&1\\ 1&1&1\\ 0&0&0\end{bmatrix},\mathbf{P}_{2}=\begin{bmatrix}1&1&1\\ 1&1&0\\ 0&0&1\end{bmatrix},\mathbf{P}_{3}=\begin{bmatrix}1&1&1\\ 1&1&0\\ 1&0&0\end{bmatrix},\mathbf{P}_{4}=\begin{bmatrix}1&1&0\\ 0&1&1\\ 1&0&1\end{bmatrix}\]

For sample size = 7, there are only 2 sampling patterns in the equivalent sense:

\[\mathbf{P}_{1}=\begin{bmatrix}1&1&1\\ 1&1&1\\ 1&0&0\end{bmatrix},\mathbf{P}_{2}=\begin{bmatrix}1&1&1\\ 1&1&0\\ 0&0&1\end{bmatrix},\mathbf{P}_{3}=\begin{bmatrix}1&1&1\\ 1&1&0\\ 1&0&0\end{bmatrix},\mathbf{P}_{4}=\begin{bmatrix}1&1&0\\ 0&1&1\\ 1&0&1\end{bmatrix}\]

For sample size = 7, there are only 2 sampling patterns in the equivalent sense:

\[\mathbf{P}_{1}=\begin{bmatrix}1&1&1\\ 1&1&1\\ 1&0&0\end{bmatrix},\mathbf{P}_{2}=\begin{bmatrix}1&1&1\\ 1&1&0\\ 0&0&1\end{bmatrix}\]For sample size = 8, there is only 1 sampling pattern in the equivalent sense:

\[\mathbf{P}_{1}=\begin{bmatrix}1&1&1\\ 1&1&1\\ 1&1&0\end{bmatrix}\]

For sample size = 9, there is only 1 sampling pattern in the equivalent sense:

\[\mathbf{P}_{1}=\begin{bmatrix}1&1&1\\ 1&1&1\\ 1&1&1\end{bmatrix}\]

### Initialization Scale Analysis

Our experimental findings indicate that when the observational data is connected, matrix factorization models often learn the lowest-rank solution starting from a small initialization. However, the required scale of initialization is not constant across different instances. We empirically observed that if the magnitude of the numerical values in the matrix to be completed varies significantly, an extremely small initialization is necessary, which, in some cases, can exceed machine precision.

Consider the following two simple \(2\times 2\) matrix completion problems, with the only difference being that the number 3 in the first row is replaced by 20. When training begins from a small initialization, for \(\mathbf{M}_{4}\), the fourth element only needs to learn the value 6 to be a rank-1 solution. However, for \(\mathbf{M}_{5}\), the fourth element needs to learn the value 40 to achieve rank-1.

\[\mathbf{M}_{4}=\begin{bmatrix}1&2\\ 3&\times\end{bmatrix},\qquad\mathbf{M}_{5}=\begin{bmatrix}1&2\\ 20&\times\end{bmatrix}.\]

Fig. B6 illustrates the difficulty in learning these two examples. For \(\mathbf{M}_{4}\), an initialization variance of approximately \(10^{-7}\) is sufficient to learn the lowest-rank solution. In contrast, for \(\mathbf{M}_{5}\), an extremely small initialization variance is required, making it challenging to learn a rank-1 solution. Yet, with an exceedingly small initialization variance of \(10^{-83}\), we can still observe the second singular value plummeting to zero. The origin is a saddle point. The smaller the initialization, the longer it will stay at the origin. If initialization continues to decrease, training will stagnate. Therefore, if the magnitude difference is even greater, such as replacing 20 with 100 in \(\mathbf{M}_{5}\), then with the small initialization allowed by machine precision, it is nearly impossible to learn the value 200 completely.

For the matrix factorization model \(\mathbf{f}_{\theta}=\mathbf{A}\mathbf{B}\), the Hessian matrix at 0 has strictly negative eigenvalues, making the origin a strict saddle point. Under small random initialization, gradient descent escapes this saddle at an exponential rate. Our Theorem 1 ensures that only strict saddle points and global minima exist as critical points in the loss landscape. Subsequent saddle points on invariant manifolds are also strict, with exponential escape speeds, maintaining acceptable optimization process.

Reaching the lowest rank solution requires parameters to escape the saddle point along the unique top eigen-direction. Fig. B6 illustrates the relationship between required initialization scale and observation magnitude differences. For lowest possible rank with large numerical magnitude differences in observations, extremely small initialization is necessary. However, for approximately low rank solutions, a relatively small initialization suffices.

Figure B6: (a, c) The value of the \((2,2)\) element learned by the model as the initialization decreases. (b, d) The singular values of the matrix learned by the model with decreasing initialization.

### High-dimensional Experiments

To validate the scalability of our findings, we extended our experiments to higher-dimensional matrices. We conducted tests on \(20\times 20\) matrices, employing both connected (Fig. 14) and disconnected (Fig. 15) sampling patterns, while monitoring rank evolution during training. Our results consistently corroborated the main findings:

(i) Connected observations converged to optimal low-rank solutions.

(ii) Disconnected observations yielded higher-rank solutions.

(iii) The Hierarchical Invariant Manifold Traversal (HIMT) process was observed in both connected and disconnected scenarios.

### Dynamics of Deep Matrix Factorization

In the context of depth-3 matrix factorization models, we consider the functional form:

\[\mathbf{f}_{\mathbf{\theta}}=\mathbf{ABC},\quad\text{where}\quad\mathbf{A},\mathbf{B},\mathbf{C}\in \mathbb{R}^{d\times d}.\]

Figs. 16 and 17 suggest that even for a depth-3 model, the learning process exhibits a progression from low rank to high rank structures.

### Incorporating Attention Mechanisms

Within the Transformer architecture, the matrix factorization component retains its significance. The attention mechanism is formalized as follows:

\[\mathbf{f}_{\mathbf{\theta}}(\mathbf{X})=\sum_{i=1}^{h}\text{softmax}_{\text{ row}}\left(\frac{\mathbf{X}\mathbf{W}_{\mathbf{Q}_{i}}\mathbf{W}_{\mathbf{K}_{i}}^{\top}\mathbf{X}^{\top}}{ \sqrt{d_{k}}}\right)\mathbf{X}\mathbf{W}_{\mathbf{V}_{i}}\mathbf{W}_{\mathbf{O}_{i}},\]

where the row-wise softmax operation is applied to the attention scores, and the sum is over the \(h\) different attention heads, with \(\mathbf{W}_{\mathbf{Q}_{i}},\mathbf{W}_{\mathbf{K}_{i}},\mathbf{W}_{\mathbf{V}_{i}},\mathbf{W}_{\mathbf{O}_{i}}\) representing the learnable weight matricesfor queries, keys, values, and output transformations, respectively, and \(d_{k}\) is the dimensionality of the key vectors.

The attention module's ability to capture low-rank representations is reflected in the depth-2 matrix factorization model. As illustrated in Fig. 11, the attention models consistently learns representations that evolve from lower to higher ranks.

Figure 11: **The attention modules in Transformer learn adaptively from low rank to high rank with small initialization. (a-d) The evolution of singular values for the matrices \(\mathbf{W_{Q}},\mathbf{W_{K}},\mathbf{W_{V}}\), and \(\mathbf{W_{O}}\) throughout the training process. The number of significantly non-zero singular values suggests the effective rank of each matrix.**

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Please see the Abstract. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please see Section 7 for the limitation discussion and see Section 6.1 and Section 6.2 for the discussion of assumptions. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Please see Section 6.1 and Section 6.2 for assumptions. Please see Appendix A for proofs of all theoretical results.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Please refer to Appendix B for comprehensive details of the experiments. The design ensures that all experiments can be readily replicated. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have included example code in the supplementary material to facilitate replication of the matrix completion experiment. Given the simplicity of both the model and the data in the matrix factorization context, this code enables straightforward verification of our experimental results. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Appendix B for comprehensive details of the experiments. The design ensures that all experiments can be readily replicated. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For information on error bars, please consult Section 4, and for a detailed calculation, refer to Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please see Appendix B for the setup of experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer:[Yes] Justification: Please see Section 7. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.