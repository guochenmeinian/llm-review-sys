# Deep Insights into Noisy Pseudo Labeling

on Graph Data

 Botao Wang\({}^{1,2}\), Jia Li\({}^{1,2}\)1, Yang Liu\({}^{1,2}\), Jiashun Cheng\({}^{1,2}\),

**Yu Rong\({}^{3}\), Wenjia Wang\({}^{1,2}\), Fugee Tsung\({}^{1,2}\)**

\({}^{1}\)Hong Kong University of Science and Technology, Hong Kong SAR, China

\({}^{2}\)Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China

\({}^{3}\)Tencent AI Lab, Shenzhen, China

{bwangbk, yliukj, jchengak}@connect.ust.hk

{jialee, wenjiawang season}@ust.hk, yu.rong@hotmail.com

Footnote 1: corresponding author

###### Abstract

Pseudo labeling (PL) is a wide-applied strategy to enlarge the labeled dataset by self-annotating the potential samples during the training process. Several works have shown that it can improve the graph learning model performance in general. However, we notice that the incorrect labels can be fatal to the graph training process. Inappropriate PL may result in the performance degrading, especially on graph data where the noise can propagate. Surprisingly, the corresponding error is seldom theoretically analyzed in the literature. In this paper, we aim to give deep insights of PL on graph learning models. We first present the error analysis of PL strategy by showing that the error is bounded by the confidence of PL threshold and consistency of multi-view prediction. Then, we theoretically illustrate the effect of PL on convergence property. Based on the analysis, we propose a cautious pseudo labeling methodology in which we pseudo label the samples with highest confidence and multi-view consistency. Finally, extensive experiments demonstrate that the proposed strategy improves graph learning process and outperforms other PL strategies on link prediction and node classification tasks.

## 1 Introduction

Pseudo Labeling (PL) [27, 13] is one of the most popular self-supervised learning approaches and has been widely used to tackle the label sparsity problem. Its core idea is to enlarge the training set by self-labeling. As most self-labeled samples should be consistent with ground truth, the enlarged dataset has a larger sample capacity to improve the model generalization. Plenty of studies have shown the effectiveness of PL [7, 26, 14, 5]. However, there is a trade-off between the benefit of PL and the effect of mislabeled samples. When the benefit of PL outweighs the impact of introduced noise, the performance of the base model (i.e., without PL) can be improved. But for non-i.i.d. condition such as graph data, the introduced noisy labels may transfer among the samples and be amplified, which may degrade the performance of base model. Although several methods have been proposed to alleviate this noisy phenomenon [31, 32, 15], there is still neither a clear explanation nor a quantification of how pseudo labeling affects the graph learning models.

In this study, we attempt to answer questions above. Specifically, we evaluate PL's effect on the prediction error of the base model and the convergence of the empirical loss function. For a graph learning model, the message aggregation process would amplify the noises of incorrect labels introduced by PL. These noises can even accumulate to damage the base model's performance. Forexample, in a two-layer graph neural network (GNN) for node classification, the mislabeled nodes can inappropriately reduce the predicted confidence of their 2-hop neighbors. Moreover, in some circumstances, such as link prediction task, the pseudo labeled sample would not only serve as the label but also as the model inputs in the consequent iterations. This characteristic further aggravates the loss function's convergence due to the noises added to adjacency matrix.

To visualize the side effect of the PL on graph data, we conduct a toy experiment as shown in Figure 1, where the benefit of PL to a popular link prediction model **GAE**[12] depends on the choice of graph dataset, i.e., PL improves the performance of GAE on Actor, but degrades that on WikiCS. Even worse, due to the incorrect label introduced by PL, PL leads to the model's collapse on Amazon_Photo.

To obtain a stable and consistent result by PL strategy, it is necessary to quantify the impact of introduced noisy labels and theoretically analyze how it affects the graph training procedure compared to the procedure without PL. In this paper, we build a theoretical connection between PL strategy and graph learning models with multi-view augmentations. We prove that the error bound of the PL predictor is jointly bounded by a confidence threshold and the prediction consistency over different augmentations. Moreover, we theoretically analyze that PL strategy affects convergence property by the covariance term in the optimization function. Accordingly, we propose the Cautious Pseudo Labeling (CPL) strategy for the graph learning process that maximizes the confidence threshold by committing the PL samples with high prediction probability. We evaluate the CPL on different graph learning tasks, including link prediction and node classification models, where we observe remarkable and consistent improvements over multiple datasets and base models. As shown in Figure 1, compared with the base model GAE, the average AUC improvement of CPL over three datasets is 7.79%, which clearly validates the superiority of our model.

## 2 Model

In this section, we define the general problem of graph learning, including link prediction and node classification tasks. We give the error bound and convergence property analysis associated with the application of PL strategy. Lastly, we propose cautious PL strategy accordingly.

### Problem Definition

In graph learning, given graph \(G=(\mathcal{V},\mathcal{E})\), \(\mathcal{V}=\{v_{i}\}\) is the node set, \(\mathcal{E}=\{(i,j)\}\) is the edge set, and \(|\mathcal{V}|=N\) is the node number. The feature matrix and adjacent matrix are denoted by \(X=[x_{ij}]_{N\times F}\) and \(A=[a_{ij}]_{N\times N}\), respectively, where \((i,j)\in\mathcal{E}\) if and only if \(a_{ij}\neq 0\). A base GNN model \(g\) is trained on the graph \(G\) and the observed labels. It outputs the probability of prediction target. In this work, we adopt the PL scheme which involves a teacher model \(g_{\phi}\) and a student model \(g_{\psi}\). The teacher model calculates confidence of the unlabeled samples and PL a subset of samples using the strategy \(\mathcal{T}\). Subsequently, student model utilizes the enlarged set to fine-tune the base model and becomes the teacher model in the next iteration.

**Node classification task.** The objective of the node classification task is to predict the probabilities of an unlabeled node belonging to different classes \(g:G\rightarrow\mathbb{R}^{N\times M}\), where \(M\) is the number of classes. The original training set comprises the labeled nodes \(\mathcal{V}_{o}\) and their labels \(Y_{o}\). And the model aims to predict the classes of the unlabeled nodes \(\mathcal{V}_{u}\). In the PL scheme, the teacher model calculates the confidence of the unlabeled nodes \(\hat{Y}_{u}\) and assigns pseudo labels to a selected subset of nodes

Figure 1: The performance of GAE, PL and proposed CPL strategy on link prediction w.r.t. Actor, WikiCS and Amazon_Photo dataset. Horizontal axis is #PL samples, and vertical axis is AUC.

\(\{\mathcal{V}_{p},Y_{p}|\mathcal{V}_{p}\subset\mathcal{V}_{u},Y_{p}\subset\hat{Y} _{u}\}\). Then the student model undergoes fine-tuning on the enlarged label set \(g_{\psi}(G)\rightarrow\hat{Y}_{o},\hat{Y}_{o}=Y_{o}\cup Y_{p},\hat{Y}_{o}\) is the enlarged label set.

**Link prediction task.** The prediction targets are the probabilities of edges between unlabeled node pairs. In our scheme, the link prediction model \(g(g_{emb},s)\) consists of an embedding network \(g_{emb}:G\rightarrow\mathbb{R}^{N\times D}\) and a score function \(s:\mathbb{R}^{D}\times\mathbb{R}^{D}\rightarrow\mathbb{R}\) that estimates the probability of the link, where \(D\) is the dimension of the node embeddings. A portion of edges is observed as the training graph \(G=(\mathcal{V},\mathcal{E}_{o}),\mathcal{E}_{o}\subset\mathcal{E}\). The teacher model predicts the confidence of the unobserved edges \(\mathcal{E}_{u}\) and enlarge the observed edge set \(\hat{\mathcal{E}}_{o}=\mathcal{E}_{p}\cup\mathcal{E}_{o}\) for fine-tuning \(g_{\psi}:G(\mathcal{V},\hat{\mathcal{E}}_{o})\rightarrow\mathbb{R}^{N\times N}\).

It is worth to mention that in the link prediction task, the node embeddings undergo a change once pseudo labels are added, even before fine-tuning. Because the enlarged edge set is also utilized as input for the GNN.

The optimization target is formulated as minimizing \(\mathcal{L}=\text{CE}(g_{\psi},Y_{o})/|Y_{o}|\), where \(\text{CE}(\cdot)\) denotes cross entropy, \(Y_{o}\) is the ground truth label of the observed set and represents \(\mathcal{E}_{o}\) for the link prediction task. The overall performance is evaluated by the 0-1 loss: \(\text{Err}(g)=\mathbb{E}[\text{argmax}(g_{\psi})\neq Y]\).

### Pseudo Labeling Error Analysis

We here present the error bound analysis of the graph learning under the PL strategy. To facilitate mathematical treatment, we introduce several assumptions.

#### 2.2.1 Graph perturbation invariant

We assume **graph perturbation invariant** (GPI) property in GNN, which states that the prediction variance is linearly bounded by the difference between the augmented and original inputs.

**Definition 2.1**: _Given a graph \(G\) and its perturbation \(\hat{G}=G(X\odot M_{x},A\odot M_{a})\) by the random feature masks \(M_{x}\in\{1,0\}^{N\times F}\) and adjacent matrix mask \(M_{a}\in\{1,0\}^{N\times N}\) satisfying_

\[\frac{1}{N\cdot F}\|\textbf{1}^{N\times F}-M_{x}\|_{2}^{2}+\frac{1}{N^{2}}\| \textbf{1}^{N\times N}-M_{a}\|_{2}^{2}<\epsilon,\] (1)

_the GNN \(g(\cdot)\) has GPI property if there exists a constant \(C>0\) such that the perturbed prediction confidence satisfies \(\|g(\hat{G})-g(G)\|_{2}^{2}<C\epsilon\). Here, \(\odot\) is element-wise product, and \(\|\cdot\|_{2}\) is the 2-norm of the vector or matrix._

The GPI property guarantees the variation of the output confidence is linearly bounded by the degree of graph perturbation, which is similar to the \(C\)-Lipschitz condition applied to GNN.

#### 2.2.2 Additive expansion property

With Definition 2.1, if we have a convex measurable function \(f(y)\) that satisfies the \(C\)-Lipchitz condition, we can find a probability measure on the output prediction \(p_{f}(y)\) that satisfies the **additive expansion property** as stated in Proposition 2.2.

**Proposition 2.2**: _Define a local optimal subset as \(U\subset Y\), whose probability is higher than a threshold \(p_{f}(y)>1-q,y\in U\), and its perturbation set \(U_{\epsilon}=\{\hat{y}=g(G):\|\hat{y}-y\|_{2}\leq C\epsilon,y\in U\}\), where \(G\in\{\hat{G}\}\) is the space of the perturbed graph. Then, there exists \(\alpha>1,\eta>0\), s.t. the probability measure \(p_{f}\) satisfying following additive expansion property: \(p_{\alpha f}(U_{\epsilon}\setminus U)\geq p_{\alpha f}(U)+\eta\cdot\alpha\)._

The proposition guarantees the continuity of the measurable function in the neighborhood of the local optimal subset \(U\). In practice, \(U\) represents the PL samples, which ideally should be close to the ground truth labels in ideal. However, the correctness of the PL samples is discrete where the continuity condition is hard to be satisfied. To address this issue, we can leverage GPI. By applying multi-view augmentations and calculating average confidence, we canreparameterize the probability measure to be continuous. In this condition, the Proposition 2.2 implies that the probability of the neighborhood under the amplified measure is greater than the original local optimum. This observation opens up potential opportunities for optimization. For a detailed proof of Proposition 2.2, please refer to Appendix A.

#### 2.2.3 Prediction error measurement

Then, we use the additive expansion property to calculate the error bound of the multi-view GNN model under PL strategy. According to Theorem B.2 in [25], when we use 0-1 loss to measure the error of predictor, the error bound can be evaluated by the following theorem.

**Theorem 2.3**: _Let \(q>0\) be a given threshold. For the GNN in the teacher model \(g_{\phi}\), if its corresponding density measure satisfies additive expansion, the error of the student predictor \(g_{\psi}\) is bounded by_

\[\text{Err}(g)\leq 2(q+\mathcal{A}(g_{\psi})),\] (2)

_where \(\mathcal{A}(g_{\psi})=\mathbb{E}_{Y_{\text{test}}}[\bm{I}(\exists g_{\psi}( \hat{G})\neq g_{\psi}(G))]\) measures the inconsistency over differently augmented inputs, \(Y_{\text{test}}\) is the test set for evaluation._

The brief derivation is provided in Appendix.B. From to Eq.2, we observe that the prediction error (0-1 loss) is bounded by the confidence threshold \(q\) and the expectation of the prediction inconsistency \(\mathcal{A}\). Intuitively, if the predictions of multi-view augmented inputs tend to be consistent and the threshold is smaller, the error bound will be smaller, which implies a more accurate predictor.

If \(q\) is a small value, the threshold probability of PL \(1-q\) approaches 1. This cautious approach in self-training leads to a smaller lower bound in the error estimate. When applying random PL, setting the confidence threshold as \(q=0.5\), then the maximum theoretical error rate is 1. It means that there is no guarantee on the performance after applying PL.

The inconsistency term \(\mathcal{A}\) is the expectation of the probability that predictions are inconsistent when input graphs are augmented differently. A small value of \(\mathcal{A}\) indicates consistent prediction across different views. In such cases, we have more confidence in the predictions, leading to a smaller error bound. On the other hand, if the predictions from different views are inconsistent with each other, the model lacks robustness, resulting in a larger error bound.

### Convergence Analysis

In this section, we analyze the influence of the PL strategy on the empirical loss to illustrate its convergence property. First, we assume that optimizing teacher model will not influence the convergence property of PL strategy. This assumption stems from the understanding that the teacher model can converge independently without the incorporation of PL.

**Assumption 2.4**: _The loss function defined in Algorithm 1 is non-increasing during the optimization: \(\text{CE}(g_{\psi}^{(t)},\hat{Y}_{o}^{(t+1)})\leq\text{CE}(g_{\phi}^{(t)}, \hat{Y}_{o}^{(t)}).\)_

Then, we show that the PL sample selection strategy \(\mathcal{T}\) influences the covariance term derived from the empirical loss, then affects the convergence property.

\[\mathcal{L}_{\mathcal{T}}^{(t+1)}\leq\beta\text{Cov}\left[\text{ce}\left(g_{ \psi},Y\right),\mathcal{T}\right]+\mathcal{L}_{\mathcal{T}}^{(t)}\] (3)

where \(\beta=|\hat{Y}_{u}|/(|\hat{Y}_{o}|+k)\), \(ce(\cdot)\) is the element-wise cross entropy. The equality is achieved when the model reaches the global optimal under given training set.

The detailed derivation of the Eq.3 is shown in Appendix.C. From Eq.3, we observe that the effect of PL strategy is decoupled and encapsulated in the covariance term. The covariance sign determines whether the empirical loss will increase or decrease in the next iteration of optimization. For instance, when PL samples are randomly selected, \(\mathcal{T}\) would be independent with \(g_{\psi}\). The covariance becomes 0, indicating that the PL does not influence the loss function. However, a carefully designed PL strategy can accelerate the optimization of empirical loss and yield improved convergence properties.

### Cautious Pseudo Labeling

According to Theorem 2.3, setting a higher confidence threshold for pseudo labeling (PL) can lead to a lower prediction error. This is reflected in a positive covariance term, which accelerates the optimization of the empirical loss and improves the convergence property. In order to satisfy these requirements, we propose the iterative Cautious Pseudo Labeling (CPL) strategy. CPL involves carefully and iteratively pseudo labeling the most confident samples to maximize these improvements in prediction accuracy and convergence.

There are a teacher model and a student model in each iteration. First, we calculate multi-view prediction by the teacher model \(g_{\phi}\) as the confidence measure. Although biased, it is a simple and effective measure in many PL strategies. In node classification, the GNN directly outputs the probability as confidence \(g(G)\). For link prediction, the confidence is determined by the inner product similarity of node embedding \(g(G)=\sigma(E^{T}E)\), where \(E=(e_{1},...,e_{N})^{T}=g_{emb}(G)\) is the node embedding, and \(\sigma(\cdot)\) is the sigmoid function.

Then we select PL samples in unobserved set with the highest confidence, enlarging the training set. We iteratively select top-\(k\) confident samples for PL: \(Y_{p}^{(t)}=\mathcal{T}(\hat{Y}_{u}^{t},\overline{g}_{\phi},k)\), where \(\mathcal{T}\in\{0,1\}^{|\hat{Y}_{u}^{(t)}|},\sum\mathcal{T}=k\). \(\overline{g}_{\phi}^{(t)}\) is the averaged confidence of the multi-view augmentation and is equal to \(g_{\phi}^{(t)}\) for the single input. At the \(t\)-th iteration, the selected PL samples are \(Y_{p}^{(t)}\).

Then we update the observed and the unobserved set. The student model is fine-tuned by the enlarged set and becomes the new teacher model in the next iteration. We take the link prediction task as the example, whose main scheme is in Fig.2. The complete algorithm of CPL is shown in Algorithm 1.

The confidence threshold \(q(t)\) recorded in Algorithm 1 is the lowest confidence among the PL samples \(Y_{p}^{(t)}\). At each iteration, we record the lowest confidence in these \(k\) PL samples as \(q^{(t)}\). We update \(q\) to be \(q^{(t)}\) if \(q^{(t)}\) is smaller. Finally, \(q\) serves as the confidence threshold in Eq.2 for error analysis in Theorem 2.3.

The following theorem states the improvement on convergence property under CPL.

**Theorem 2.5**: _Let \(\mathcal{L}_{\mathcal{T}}^{(t)}\) denote the optimization target at the \(t\)-th iteration. The risk is monotonically decreasing under pseudo labeling strategy, i.e.,_

\[\mathcal{L}_{\mathcal{T}}^{(t+1)}\leq\beta\text{Cov}\left[\text{ ce}\left(g_{\psi},Y\right),\mathcal{T}\right]+\mathcal{L}_{\mathcal{T}}^{(t)} \leq\mathcal{L}_{\mathcal{T}}^{(t)}.\] (4)

_Here \(\mathcal{L}_{\mathcal{T}}^{(t+1)}\) is calculated by \(1/|\hat{Y}_{o}^{(t+1)}|\text{CE}(g_{\psi}^{(t)},\hat{Y}_{o}^{(t+1)})\)._

We have demonstrated the first inequality in Eq.3. The second inequality holds because the covariance between the cross-entropy loss and PL strategy is non-positive. On one hand, each view of \(g_{\psi}\) is positively correlated to the ground-truth label \(Y\) due to the consistency regularization. Then, \(g_{\psi}\) exhibits negative correlation with the cross-entropy loss \(\text{ce}(g_{\psi},Y)\). On the other hand, the PL strategy \(\mathcal{T}\) can be seen as a discrete sign function. In CPL, higher confidence samples are more likely to be pseudo-labeled. Therefore, \(\mathcal{T}\) is positively correlated with \(g_{\psi}\). Consequently, \(\mathcal{T}\) is negatively correlated with the cross-entropy term, resulting in a non-positive covariance term \(\text{Cov}(\cdot)\). This guarantees the monotone decreasing behavior of the loss function, as stated in Theorem 2.5. Furthermore, since the loss function is lower bounded, Theorem 2.5 ensures the convergence of Algorithm 1.

Figure 2: A framework illustration of CPL on link prediction: There is a teacher model and a student model that share parameter. The most confident samples in teacher modelâ€™s prediction are pseudo labeled. Then the student model is fine-tuned on the enlarged dataset and becomes teacher model in the next iteration.

``` Input: Graph \(G\), observed and unobserved label set \(Y_{o},Y_{u}\), iterative and total pseudo labeling  number \(k,K\) Output: Student model \(g_{\psi}(G)\), confidence threshold \(q\) Pre-train teacher model \(g_{\psi}^{(0)}\) on observed set \(\{G,Y_{o}\}\) ;  Initialize student model: \(g_{\psi}^{(0)}=g_{\phi}^{(0)}\); while\(\left|\hat{Y}_{o}^{(t)}\right|\leq K\)do  Calculate the average confidence of the unobserved set by the multi-view teacher model \(g_{\phi}\):  Select pseudo labeling subset \(Y_{p}^{(t)}\) with top-\(k\) confidence in \(\hat{Y}_{u}^{(t)}\);  Update set: \(\hat{Y}_{o}^{(t+1)}=\hat{Y}_{o}^{(t)}\cup Y_{p}^{(t)}\), \(\hat{Y}_{u}^{(t+1)}=\hat{Y}_{u}^{(t)}\setminus Y_{p}^{(t)}\) ;  Update the current confidence threshold \(q\) according to \(q(t)\);  Fine-tune the student model \(g_{\psi}^{(t)}\) by minimizing the cross entropy on \(\hat{Y}_{o}^{(t+1)}\);  Update teacher model \(g_{\phi}^{(t+1)}=g_{\psi}^{(t)}\) and set \(g_{\psi}^{(t+1)}=g_{\phi}^{(t+1)}\); \(t=t+1\);  end while ```

**Algorithm 1**Iterative cautious pseudo labeling.

Time complexityThe computational complexity of CPL depends on the complexity of specific GNN models used. The key operation in CPL is the selection of top-\(k\) values. For node classification, it takes \(O(N\text{log}k)\), while for link prediction, it takes \(O(N^{2}\text{log}k)\). However, since GNN models typically compute probabilities for all samples, the overhead introduced by CPL does not increase the complexity of existing methods significantly. For example, the complexity of GAE [12] for link prediction is \(O(|\mathcal{E}_{o}|D^{2}+N^{2}D)\) (i.e., the complexity of graph convolution operations and inner-products). Integrating CPL into GAE does not increase the complexity since both involve \(O(N^{2})\) operations. In practice, the additional time consumption mainly results from the fine-tuning process.

## 3 Experiments

In this section, we conduct an evaluation to assess the effectiveness of the CPL strategy on both link prediction and node classification tasks. We compare CPL with raw base models as well as other PL strategies. Then, we analyze the impact of CPL capacity, training data ratio, PL strategy, and augmentation methods. Finally, a case study is bridged to the theoretical analysis on convergence and error bound. The implementation is open-sourced at https://github.com/AcEbt/CPL.

### Datasets and Benchmarks

We adopt five public available datasets to evaluate CPL strategy for link prediction, i.e. CiteSeer, Actor, WikiCS, TwitchPT, and Amazon_Photo, and five datasets for node classification, i.e. Cora, CiteSeer, PubMed, Amazon_Photo, and LastFMAsia. Detailed statistics are reported in Table 1.

In link prediction task, as there are few PL-based methods, we apply the CPL strategy on three popular models: **GAE**[12]**,********node2vec**[4]**, **SEAL**[29]. To reserve sufficient candidate unobserved samples for PL, the dataset is randomly split into 10%,40%,50% for training, validation, and testing.

In node classification task, we employ CPL on 4 popular base models: **GCN**, **GraphSAGE**, **GAT**, **APPNP**. CPL is compared with two other PL strategies, namely **M3S**[22] and **DR-GST**[18], using the implementation and parameters provided by the authors 2. We adopt the official split for the citation datasets and 5%,15%,80% split for other datasets in node classification.

Footnote 2: https://github.com/BUPT-GAMMA/DR-GST

We run experiments with 5 random seeds and report the mean and standard deviations of the metrics.

### Performance Comparison

#### 3.2.1 Overall performance comparison

For link prediction, Table 2 lists the AUC and AP of raw baselines and ones with CPL employed. We observe that CPL distinctively increases the performance of baseline models in nearly all cases. Note that the CPL strategy can achieve performance gain under the circumstances of both high and low performance (e.g., the AUC of GAE on Actor improves from 55.34 to 65.58 and the AP of SEAL on CiteSeer improves from 64.38 to 64.94).

For node classification, Table 3 shows the improvement on the base models and comparison with other PL strategies. The CPL strategy can consistently improve the performance of the base models. However, other PL strategy may be ineffective or degrade the base model (e.g., DR-GST on Cora with APPNP, M3S for PubMed with GraphSAGE). CPL also consistently outperforms other PL strategies. The time consumption is also reported in AppendixD.

#### 3.2.2 Impact of pseudo labeling capacity

In the experiment, the number of PL samples in each iteration \(k\) is set from 100 to 1000. We provide a reference interval for the selection of \(k\). Intuitively, a small \(k\) can lead to a reliable result. But a too small \(k\) will unnecessarily increase the training time. And it does not prominently influence the overall performance as we test on different datasets. In the experiment, the predicted confidence distribution is around 1, as there are usually plenty of potential PL samples. \(k\) is much smaller than the total number of unobserved samples. Then the \(k\)-th highest confidence in the unobserved set will not have much difference, as the cases in Table 4.

#### 3.2.3 Impact of training data ratio

PL enlarges the observed dataset and introduces extra information for the training. This effect has a different degree of contribution on different training data. When the training set is partially applied for the training, the ratio of the observed set ranges from 0.1 to 0.9. The variation of AUC and AP on the Amazon_Photo dataset is shown in Fig.3. The CPL method can consistently improve the performance of the raw model even starting from a small training set. It is also worth to mention that CPL is more likely to have a more significant contribution when the training set is small, as there is more introduced information.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline
**Dataset** & **Cora** & **CiteSeer** & **PubMed** & **Actor** & **WikiCS** & **TwitchPT** & **Amazon_Photo** & **LastFMAisa** \\ \hline \# Nodes & 2,078 & 3,327 & 19,717 & 7,600 & 11,701 & 1,912 & 7,650 & 7,624 \\
**\# Links** & 10,556 & 9,104 & 88,648 & 30,019 & 216,123 & 64,510 & 238,162 & 55,612 \\
**\# Features** & 1433 & 3,703 & 500 & 932 & 300 & 128 & 745 & 128 \\
**\# Classes** & 7 & 6 & 3 & 5 & 10 & 2 & 8 & 18 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Dataset statistics.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & **Model** & **Citeseer** & **Actor** & **WikiCS** & **TwitchPT** & **Amazon_Photo** \\ \hline \multirow{5}{*}{**AUC(\%)**} & GAE & 71.10 \(\pm\) 0.56 & 55.34 \(\pm\) 0.57 & 90.81 \(\pm\) 0.69 & 74.48 \(\pm\) 3.03 & 67.92 \(\pm\) 1.31 \\  & GAE+CPL & **72.45 \(\pm\) 0.24** & **65.58 \(\pm\) 104** & **95.56 \(\pm\) 0.24** & **79.67 \(\pm\) 3.77** & **76.30 \(\pm\) 1.84** \\  & node2vec & 52.03 \(\pm\) 0.60 & 53.30 \(\pm\) 0.59 & 88.82 \(\pm\) 0.28 & 79.46 \(\pm\) 0.77 & 89.32 \(\pm\) 0.21 \\  & node2vec+CPL & **55.22 \(\pm\) 1.63** & **65.11 \(\pm\) 2.31** & **91.99 \(\pm\) 0.26** & **84.76 \(\pm\) 3.52** & **89.53 \(\pm\) 0.30** \\  & SEAL & 63.60 \(\pm\) 0.01 & 73.41 \(\pm\) 0.02 & 86.01 \(\pm\) 0.04 & 87.80 \(\pm\) 0.01 & 76.96 \(\pm\) 0.17 \\  & SEAL+CPL & **64.33 \(\pm\) 0.14** & **73.54 \(\pm\) 0.01** & **86.83 \(\pm\) 0.07** & **87.87 \(\pm\) 0.01** & **78.86 \(\pm\) 0.01** \\ \hline \multirow{5}{*}{**AP(\%)**} & GAE & 72.12 \(\pm\) 0.63 & 53.60 \(\pm\) 1.06 & 90.58 \(\pm\) 0.71 & 69.73 \(\pm\) 5.06 & 67.06 \(\pm\) 0.99 \\  & GAE+CPL & **73.54 \(\pm\) 0.20** & **67.65 \(\pm\) 1.06** & **95.58 \(\pm\) 0.29** & **79.09 \(\pm\) 5.48** & **75.52 \(\pm\) 4.23** \\  & node2vec & 52.90 \(\pm\) 0.36 & 55.43 \(\pm\) 0.62 & 92.54 \(\pm\) 0.51 & 83.37 \(\pm\) 0.52 & 91.46 \(\pm\) 0.18 \\  & node2vec+CPL & **56.19 \(\pm\) 1.60** & **68.33 \(\pm\) 2.85** & **93.66 \(\pm\) 0.29** & **85.87 \(\pm\) 2.15** & **91.47 \(\pm\) 0.21** \\  & SEAL & 64.38 \(\pm\) 0.01 & 73.17 \(\pm\) 0.12 & 83.63 \(\pm\) 0.16 & 87.69 \(\pm\) 0.01 & 73.72 \(\pm\) 0.56 \\  & SEAL+CPL & **64.94 \(\pm\) 0.14** & **73.44 \(\pm\) 0.02** & **86.72 \(\pm\) 0.12** & **87.75 \(\pm\) 0.02** & **80.36 \(\pm\) 0.09** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance (AUV%) comparison on link prediction.

#### 3.2.4 Impact of noisy pseudo labeling strategy

As the noise introduced by PL has a large effect in the graph, we compare the CPL with the PL strategy. In PL, the samples are selected randomly in the unobserved set, whose labels are estimated by the teacher model. The comparison of AUC's variation on different datasets is shown in Fig.1. We discover that PL has different effects on the baseline models. On Actor, the PL consistently improves the link predictor, but not as much as the CPL. On WikiCS however, PL keeps weakening the AUC, as the introduced noise outweighs. On Amazon_Photo, PL can improve the performance at the first few iterations. But the AUC suddenly drops from 70% to around 50% after iterations, which illustrates that introducing too much noise may degrade the model. As for the CPL, although it may drop after reaching its best, i.e. for WikiCS, it can distinctively improve the performance compared to the base model and will not lead to failure.

#### 3.2.5 Impact of multi-view augmentation

During the training, we apply multi-view augmented graphs as the input of the teacher model, so that the continuity condition in Theorem 2.3 is easier to be satisfied. We here conduct the ablation experiments that compares multi-view augmentation with different single-view augmentation methods, including drop node (Node view), feature mask (Feature view), and DropEdge (Structure view). In each experiment, a single augmentation method was applied three times. And "Random" refers to the random selection of samples during PL. As shown in Fig.4, all of the single-view augmentation methods have better performance than the base model and their improvements are almost the same. When we use multi-view augmentation, the AUC and AP are further improved. It illustrates that multi-view augmentation contributes to a more robust graph learning and tends to obtain a consistent result, which echoes with our theoretical analysis.

### Case Study

The case study on node classification is conducted on LastFMAisa on different baselines. The detailed intermediate variables about the error analysis are listed in Table 5. We record the inconsistency \(\mathcal{A}\) and confidence \(1-q\) during the CPL as Algorithm.1. Then the theoretical error bound \(\text{Err}_{th}(g)\) is calculated by Theorem 2.3. As \(\text{Err}_{th}(g)>\text{Err}_{exp}(g)\), the experimental error is bounded by the theoretical error, implying that it is an effective bound. We also report the accuracy of PL samples. The CPL has smaller error than other PL strategies and is consistently better than \(\text{Err}_{exp}(g)\).

The case study on link prediction is conducted based on GAE on WikiCS. The relationship between consistency \(1-\mathcal{A}(g)\) and the number of PL samples is shown in Fig.5. The result shows that more PL samples help to increase the consistency of the prediction. We also compare the optimization target of PL \(\mathcal{L}_{\mathcal{R}}^{(t)}\) and CPL \(\mathcal{L}_{\mathcal{T}}^{(t)}\) in Fig.5. We discover that the optimization target of CPL converges faster than PL and is consistently smaller. It implies the covariance term \(\text{Cov}\left[\text{ce},\mathcal{T}\right]\) is negative as illustrated in Theorem 2.5. Thus, the CPL strategy can improve the convergence property of graph learning. A detailed illustration of error bound verification and Knowledge discovery is shown in Appendix E.

## 4 Related Works

Pseudo labeling is a popular approach in self-training (ST), aiming to enlarge the training set by self-annotation. Most studies focus on learning a more accurate PL algorithms to avoid noisy samples. Confidence thresholding is a simple yet effective method in ST, where the similarity with ground truth or consistency is used to measure the confidence, ensuring that flawed samples are excluded from the enlarged dataset [9; 20]. [6] uses the perturbation on the hidden states to yield close predictions for similar unlabeled inputs. [7; 16; 26] rely on the consistency of the unsupervised clustering. [30] utilizes adversarial learning to acquire domain-uninformative representations and train a PL classifier. [3; 17] use a contrastive loss to improve the representation learning before PL. Directly modeling noisy labels can also enhance noise tolerance [31], such as using soft pseudo labels [2], or distilling correct information to mitigate overfitting on noise [32].

The confidence measure plays a crucial role in avoiding the overconfident results [11]. [33] constructs confidence regularizers, expecting a smooth prediction with soft-label and preventing infinite entropy minimization. [10] learns a confidence metric based on the generality and unity of its distribution of pseudo loss. [1] uses the distance between the distributions of positive and negative samples as a confidence measure. [24] applies data augmentation on node features, where consistency is used as a confidence measure for classification. The co-training method constructs multi-view classifiers. It adds pseudo labels in each view to provide complementary information for each other, showing better performance than single-view [5].

Some studies on graph data apply the PL strategy to node classification tasks. M3S [21], IFC-GCN[8] utilize the clustering to PL the unlabeled samples. CaGCN-st [23] is a confidence-calibrated model

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{3}{c}{**CiteSeer**} \\
**k** & Raw & 100 & 500 & 2000 \\ \hline
**AUC(\%)** & 71.10 & 72.25 & 72.45 & 71.98 \\
**AP(\%)** & 72.45 & 73.29 & 73.54 & 73.13 \\ \hline \multicolumn{5}{c}{**Amazon\_Photo**} \\
**k** & Raw & 5 & 50 & 500 \\ \hline
**AUC(\%)** & 67.92 & 76.63 & 76.30 & 75.95 \\
**AP(\%)** & 67.06 & 75.70 & 75.52 & 74.97 \\ \hline \hline \end{tabular}
\end{table}
Table 4: CPL with different \(k\).

Figure 5: Case study of the consistency and convergence on WikiCS: Cautious PL (CPL) improves the prediction consistency within error bound, and converges faster than PL.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & **GCN** & **GraphSAGE** & **GAT** & **APPNP** \\ \hline Inconsistency \(\mathcal{A}\) (\%) & 6.69 & 4.01 & 2.96 & 3.13 \\ Confidence \(1-q\) (\%) & 77.63 & 88.35 & 83.00 & 86.86 \\ Theoretical Err\({}_{th}\) (\%) & 58.12 & 31.32 & 39.92 & 32.54 \\ Experimental Err\({}_{exp}\) (\%) & 20.08 & 17.75 & 17.11 & 16.44 \\ PL error (\%) & 7.78 & 6.43 & 8.18 & 6.02 \\ M3S PL error (\%) & 65.63 & 27.00 & 65.00 & 27.49 \\ DR-GST PL error (\%) & 26.31 & 14.10 & 13.38 & 29.09 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Case analysis of CPL on LastFMAisia.

that utilizes low-confidence but high-accuracy samples. DR-GST [18] employs dropout and edge-drop augmentation to conduct information gain inference for selecting PL samples.

## 5 Conclusion

In this study, we provide deep insights into PL strategy by analyzing its impact on prediction error and the convergence properties. We offer theoretical explanations for the effect of PL strategies on graph learning processes, particularly addressing degradation issues. Based on the theoretical analysis, we introduce the CPL strategy, a plug-in and practical technique that can be generally applied to various baseline models. The experiments demonstrate effectiveness and superiority of CPL in link prediction and node classification tasks.

In future work, we plan to explore a more reliable confidence measures as the PL criteria, such as informativeness in the multi-view network and prediction uncertainty.

## Acknowledgments and Disclosure of Funding

The research of Tsung was supported in part by the Industrial Informatics and Intelligence (Triple-i) Institute at HKUST(GZ).

The research of Jia Li was supported by NSFC Grant No. 62206067, Tencent AI Lab Rhino-Bird Focused Research Program and Guangzhou-HKUST(GZ) Joint Funding Scheme 2023A03J0673.

## References

* [1] Charles Corbiere, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick Perez. "Addressing failure prediction by learning model confidence". In: NIPS 32 (2019).
* [2] Yixiao Ge, Dapeng Chen, and Hongsheng Li. "Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification". In: _arXiv preprint arXiv:2001.01526_ (2020).
* [3] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, et al. "Self-paced contrastive learning with hybrid memory for domain adaptive object re-id". In: _NIPS 33_ (2020), pp. 11309-11321.
* [4] Aditya Grover and Jure Leskovec. "node2vec: Scalable feature learning for networks". In: _Proceedings of the 22nd ACM SIGKDD_. 2016, pp. 855-864.
* [5] Tengda Han, Weidi Xie, and Andrew Zisserman. "Self-supervised co-training for video representation learning". In: _NIPS 33_ (2020), pp. 5679-5690.
* [6] Junxian He, Jiatao Gu, Jiajun Shen, and Marc'Aurelio Ranzato. "Revisiting Self-Training for Neural Sequence Generation". In: _ICLR_. 2020.
* [7] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. "Hubert: Self-supervised speech representation learning by masked prediction of hidden units". In: _IEEE/ACM Transactions on Audio, Speech, and Language Processing_ 29 (2021), pp. 3451-3460.
* [8] Zhihui Hu, Guang Kou, Haoyu Zhang, Na Li, Ke Yang, and Lin Liu. "Rectifying pseudo labels: Iterative feature clustering for graph representation learning". In: _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_. 2021, pp. 720-729.
* [9] Zijian Hu, Zhengyu Yang, Xuefeng Hu, and Ram Nevatia. "Simple: Similar pseudo label exploitation for semi-supervised classification". In: _CVPR_. 2021, pp. 15099-15108.
* [10] Kai Huang, Jie Geng, Wen Jiang, Xinyang Deng, and Zhe Xu. "Pseudo-Loss Confidence Metric for Semi-Supervised Few-Shot Learning". In: _ICCV_. 2021, pp. 8671-8680.
* [11] Susmit Jha, Sunny Raj, Steven Fernandes, Sumit K Jha, Somesh Jha, Brian Jalaian, Gunjan Verma, and Ananthram Swami. "Attribution-based confidence metric for deep neural networks". In: _NIPS_ 32 (2019).
* [12] Thomas N Kipf and Max Welling. "Variational Graph Auto-Encoders". In: _NIPS Workshop on Bayesian Deep Learning_ (2016).
* [13] Dong-Hyun Lee. "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks". In: _Workshop on challenges in representation learning, ICML_. Vol. 3. 2. 2013, p. 896.

* [14] Jia Li, Yongfeng Huang, Heng Chang, and Yu Rong. "Semi-Supervised Hierarchical Graph Classification". In: _IEEE Transactions on Pattern Analysis and Machine Intelligence_ (2022).
* [15] Jia Li, Jiajin Li, Yang Liu, Jianwei Yu, Yueting Li, and Hong Cheng. "Deconvolutional Networks on Graph Data". In: _NeurIPS_. 2021.
* [16] Jia Li, Yu Rong, Hong Cheng, Helen Meng, Wenbing Huang, and Junzhou Huang. "Semi-supervised graph classification: A hierarchical graph perspective". In: _The World Wide Web Conference_. 2019, pp. 972-982.
* [17] Junnan Li, Caiming Xiong, and Steven CH Hoi. "Comatch: Semi-supervised learning with contrastive graph regularization". In: _ICCV_. 2021, pp. 9475-9484.
* [18] Hongrui Liu, Binbin Hu, Xiao Wang, Chuan Shi, Zhiqiang Zhang, and Jun Zhou. "Confidence may cheat: Self-training on graph neural networks under distribution shift". In: _Proceedings of the ACM Web Conference 2022_. 2022, pp. 1248-1258.
* [19] Laszlo Lovasz and Miklos Simonovits. "Random walks in a convex body and an improved volume algorithm". In: _Random structures & algorithms_ 4.4 (1993), pp. 359-412.
* [20] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. "Fixmatch: Simplifying semi-supervised learning with consistency and confidence". In: _NIPS_ 33 (2020), pp. 596-608.
* [21] Ke Sun, Zhouchen Lin, and Zhanxing Zhu. "Multi-stage self-supervised learning for graph convolutional networks on graphs with few labeled nodes". In: _Proceedings of the AAAI conference on artificial intelligence_. Vol. 34. 04. 2020, pp. 5892-5899.
* [22] Xiao Wang, Deyu Bo, Chuan Shi, Shaohua Fan, Yanfang Ye, and S Yu Philip. "A survey on heterogeneous graph embedding: methods, techniques, applications and sources". In: _IEEE Transactions on Big Data_ (2022).
* [23] Xiao Wang, Hongrui Liu, Chuan Shi, and Cheng Yang. "Be confident! towards trustworthy graph neural networks via confidence calibration". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 23768-23779.
* [24] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, Juncheng Liu, and Bryan Hooi. "Nodeaug: Semi-supervised node classification with data augmentation". In: _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. 2020, pp. 207-217.
* [25] Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. "Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data". In: _ICRL_. 2021.
* [26] Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen. "Learning semantic representations for unsupervised domain adaptation". In: _ICML_. PMLR. 2018, pp. 5423-5432.
* [27] David Yarowsky. "Unsupervised word sense disambiguation rivaling supervised methods". In: _33rd annual meeting of the association for computational linguistics_. 1995, pp. 189-196.
* [28] Percy Liang Yuchen Zhang and Moses Charikar. "A hitting time analysis of stochastic gradient Langevin dynamics". In: _Proceedings of the 2017 Conference on Learning Theory_. Vol. 65. Proceedings of Machine Learning Research. PMLR, 2017, pp. 1980-2022.
* [29] Muhan Zhang and Yixin Chen. "Link prediction based on graph neural networks". In: _NIPS_ 31 (2018), pp. 5165-5175.
* [30] Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. "Collaborative and adversarial network for unsupervised domain adaptation". In: _CVPR_. 2018, pp. 3801-3809.
* [31] Zhilu Zhang and Mert Sabuncu. "Generalized cross entropy loss for training deep neural networks with noisy labels". In: _NIPS_ 31 (2018).
* [32] Zizhao Zhang, Han Zhang, Sercan O Arik, Honglak Lee, and Tomas Pfister. "Distilling effective supervision from severe label noise". In: _CVPR_. 2020, pp. 9294-9303.
* [33] Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and Jinsong Wang. "Confidence regularized self-training". In: _ICCV_. 2019, pp. 5982-5991.

Proof of Proposition 2.2: additive expansion proposition

We denote the embedding vector of a node \(v_{i}\) by \(e_{i}\triangleq g_{i}(\hat{G})=\text{GNN}(\hat{G})[i]\). Without loss of generality, we drop the subscript for short. We can also define the density measure \(d_{f}\) as:

\[p_{f}(e)\triangleq\frac{\text{exp}(-d(e))}{\int_{\mathcal{G}}\text{exp}(-d(e)) dg(\mathcal{G})}.\] (5)

For any subset of the embedding space \(E\subset g(\mathcal{X})\), the local probability can be measured by \(p_{f}(E)=\int_{E}p_{f}(e)de\). If we have a local optimal subset \(U\subset E\) with a confidence threshold of \(1-q\), and its perturbation \(U_{\epsilon}\), then the consistency at the boundary of the subset separation problem \(E\to U,E\setminus U\) can be quantified by Cheeger constant. We introduce the continuous Cheeger inequality to elaborate the lower bound of the Cheeger constant under an amplified measure \(\alpha f\), where \(\alpha>1\) is a constant.

We first define the restricted _Cheeger constant_ in the link prediction task. Given the function \(f\) and any subset \(E\), Cheeger constant is calculated by

\[\mathcal{C}_{f}(E)\triangleq\lim_{\epsilon\to 0^{+}}\inf_{A\subset E}\frac{p_{f} (A_{\epsilon})-p_{f}(A)}{\epsilon\min\{p_{f}(A),p_{f}(E\setminus A)\}}.\] (6)

According to the definition, the Cheeger constant is a lower bound of probability density in the neighborhood of the given set. It quantifies the chance of escaping the subset \(A\) under the probability measure \(f\) and reveals the consistency over the set cutting boundary.

Then we prove that for the any subset \(E\subset g(\mathcal{G})\) with its local optimal subset \(U:\{e\in E:p_{f}(e)>1-q\}\), there exists \(\alpha>1\) s.t. \(\mathcal{C}_{\alpha f}(E\setminus U)\geq 1\).

As the measurable function for the link prediction is defined as \(f(e)=-e^{T}e_{a}\). When \(e^{*}=e_{a}\), \(f\) reaches the global minimal. For the embedding vectors outside the local minimal subset \(e_{y}\in g(\mathcal{G})\setminus U\), there exists \(\epsilon>0\) s.t.

\[f(e_{y})\geq f(e^{*})+2\hat{\epsilon},\] (7)

where \(\hat{\epsilon}=C\epsilon\). If we define \(E^{*}_{\epsilon}=\{e^{*}_{\hat{\epsilon}}\}\cap g(\mathcal{G})\), where \(e^{*}_{\hat{\epsilon}}\) is the \(\hat{\epsilon}\) neighbor of \(e^{*}\), according to the Lipchitz condition of \(f\), for \(e\in E^{*}_{\epsilon}\), we have:

\[f(e_{x})\leq f(e^{*})+\hat{\epsilon}\|e_{x}-e^{*}\|_{2}\leq f(e^{*})+\hat{ \epsilon}.\] (8)

Combining Eq.8 and Eq.7 leads to \(f(e_{y})-f(e_{x})\geq\hat{\epsilon}\). Thus, for the amplified probability measure \(p_{\alpha f}\), we have

\[p_{\alpha f}(e_{x})/p_{\alpha f}(e_{y})\geq\text{exp}(\alpha\hat{\epsilon})\] (9)

According to the inequality property from [28] (formula 63), we have

\[\frac{p_{\alpha f}(U)}{p_{\alpha f}(g(\mathcal{G})\setminus U)}\geq\text{ exp}\left(\alpha\hat{\epsilon}-2\text{log}\left(2C^{2}/\hat{\epsilon}\right) \right).\] (10)

As \(p_{\alpha f}(g(\mathcal{G})\setminus U)+p_{\alpha f}(U)=1\). If we select \(\alpha\) large enough s.t. the RHS of Eq.10 is larger than 1, \(p_{\alpha f}(g(\mathcal{G})\setminus U)\leq\frac{1}{2}\). Thus, according to [19] (Theorem 2.6), we have \(\mathcal{C}_{\alpha f}\geq 1\). It guarantees consistency around the perimeter of the \(U\). As \(\alpha>1\) and \(p_{f},p_{\alpha f}\) are bounded on any subsets of embedding space. It implies a probability margin \(\eta>0\) at the neighborhood of the local optimal between two measurable functions \(f,\alpha f\), where

\[\eta=\inf_{\hat{\epsilon}\in U_{*}\setminus U,e\in U}(p_{\alpha f}(\hat{ \epsilon})-p_{f}(e)).\] (11)

which, according to [25], implies additive expansion property of the probability measure in the link prediction, as Proposition 2.2.

Proof of Theorem 2.3: error analysis

In [25], \(\mathcal{M}(g_{\phi})\) is also assumed to satisfy additive-expansion \((q,\epsilon)\), where \(\mathcal{M}(g)\triangleq\{y\in Y:g(y)\neq y\}\) is the set of mis-classified samples, and they give the error bound of the trained classifier \(s\) (Theorem B.2):

\[\text{Err}(g)\leq 2(q+\mathcal{A}(g)).\] (12)

Here in link prediction task, \(\mathcal{M}(g_{\phi})\) is mis-classified samples by the pseudo labeler (teacher model). It can be written by \(\{y_{i}:Y_{p}[i]\neq\mathcal{E}_{\mathcal{T}}[i]\}\), which is intractable during the training. The probability threshold is \(1-q\) and a local optimal subset \(U\) for PL is constructed accordingly. We aim to let \(\mathcal{M}(g_{\phi})\cap U\) be close to \(\emptyset\), so that \(g(\mathcal{G})\setminus U\) can cover \(\mathcal{M}(g_{\phi})\) as much as possible. So we define the robust set \(\mathcal{S}(g)\) as

\[\mathcal{S}(g)=\left\{y:g\left(y\right)=g\left(\hat{y}\right),\hat{y}\in\{y_{ \epsilon}\}\right\},\] (13)

where \(y_{\epsilon}\) is the \(\epsilon\) neighborhood of sample \(y\). Then, according to Proposition 2.1, we have:

\[p_{f}\left(\{y\in Y:g_{\phi}(y)\neq y,y\in\mathcal{S}(g_{\psi})\}\right)\leq p _{\alpha f}\left(g\left(\mathcal{G}\right)\setminus U\right)\leq q,\] (14)

which has similar form with [25] Lemma B.3 for link prediction task. Besides, the analysis of \(p_{f}(\{y\in Y:g_{\phi}(y)=y,g_{\psi}(y)\neq y,y\in\mathcal{S}(g_{\psi})\})\) and \(p_{f}(\overline{\mathcal{S}(g_{\psi})})\) are the same. Thus, the assumption on \(\mathcal{M}(g_{\phi})\) is satisfied. Then, we can draw the same conclusion with Eq.12, and the classifier is the student model \(g_{\psi}\). The theorem is proofed.

## Appendix C Proof of convergence inequality

The PL strategy \(\mathcal{T}\) for the unlabeled data provides a Bayesian prior, from which we formalize the empirical loss defined in Eq.1 as

\[\mathcal{L}_{\mathcal{T}}^{(t+1)}=\frac{1}{\left|\hat{Y}_{o}^{(t)}\right|+k} \left[\text{CE}\left(g_{\psi}^{(t)},\hat{Y}_{o}^{(t)}\right)+\text{CE}\left(g _{\psi}^{(t)},Y_{p}^{(t)}\right)\right].\] (15)

We can decompose the cross-entropy loss of the pseudo labeled samples by:

\[\text{CE}\left(g_{\psi},Y_{p}\right)= \sum_{\hat{Y}_{u}}\text{ce}\left(g_{\psi},Y\right)\cdot\mathcal{T}\] (16) \[= \sum_{\hat{Y}_{u}}\left[\text{ce}\left(g_{\psi},Y\right)-Y\left[ \text{ce}\left(g_{\psi},Y\right)\right]\right]\cdot\left[\mathcal{T}-Y \mathcal{T}\right]\] \[+Y\mathcal{T}\sum_{\hat{Y}_{u}}\text{ce}\left(g_{\psi},Y\right)+ \mathbb{E}\left[\text{ce}\left(g_{\psi},Y\right)\right]\sum_{\hat{Y}_{u}} \mathcal{T}-\left|\hat{Y}_{u}\right|Y\mathcal{T}Y\left[\text{ce}\left(g_{\psi },Y\right)\right]\]

Thus, Eq.16 can be simplified to:

\[\text{CE}\left(g_{\psi},Y_{p}\right)= \left|\hat{Y}_{u}\right|\text{Cov}\left[\text{ce}\left(g_{\psi},Y \right),\mathcal{T}\right]+\mathbb{E}\mathcal{T}\cdot\left|\hat{Y}_{u}\right| \mathbb{E}\left[\text{ce}\left(g_{\psi},Y\right)\right]\] (17) \[+\mathbb{E}\left[\text{ce}\left(g_{\psi},Y\right)\right]\cdot \left|\hat{Y}_{u}\right|\mathbb{E}\mathcal{T}-\left|\hat{Y}_{u}\right|\mathbb{ E}\mathcal{T}\mathbb{E}\left[\text{ce}\left(g_{\psi},Y\right)\right]\] \[= \left|\hat{Y}_{u}\right|\text{Cov}\left[\text{ce}\left(g_{\psi},Y \right),\mathcal{T}\right]+\left|\hat{Y}_{u}\right|\mathbb{E}\mathcal{T} \mathbb{E}\left[\text{ce}\left(g_{\phi},Y\right)\right]\] \[= \left|\hat{Y}_{u}\right|\text{Cov}\left[\text{ce}\left(g_{\psi},Y \right),\mathcal{T}\right]+k\mathbb{E}\left[\text{ce}\left(g_{\phi},Y\right)\right]\]

Note that \(\mathcal{T}\) is the indicator-like function, where we have

\[\mathbb{E}\mathcal{T}=\frac{1}{\left|\hat{Y}_{u}\right|}\sum_{\hat{Y}_{u}} \mathcal{T}=\frac{k}{\left|\hat{Y}_{u}\right|}.\] (18)

[MISSING_PAGE_FAIL:14]

For group 1, 3 nodes are connected by the pseudo links, and they are all linked to a central node whose degree is 321. The metadata information of the nodes are all strongly relevant to "Linux" in the "operating systems" topic. Thus, the PL linked nodes are likely to have common neighbors discovered triangle relationship. In group2, node 3489 has no in/out degree and is pseudo linked to node 7680. Both papers focus on the "malware"/"phishing" under the topic "Computer security". Although they only have one common token, the CPL strategy successfully discovers the correlation and consistently add it to the training set. The detailed result of the case study is shown in Table 8.