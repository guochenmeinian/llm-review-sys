# Learning Invariant Molecular Representation in Latent Discrete Space

 Xiang Zhuang\({}^{1,2,3}\), Qiang Zhang\({}^{1,2,3}\), Keyan Ding\({}^{2}\), Yatao Bian\({}^{4}\),

**Xiao Wang\({}^{5}\), Jingsong Lv\({}^{6}\), Hongyang Chen\({}^{6}\), Huajun Chen\({}^{1,2,3}\)\({}^{\dagger}\)**

\({}^{1}\)College of Computer Science and Technology, Zhejiang University

\({}^{2}\)ZJU-Hangzhou Global Scientific and Technological Innovation Center

\({}^{3}\)Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph

\({}^{4}\)Tencent AI Lab, \({}^{5}\)School of Software, Beihang University, \({}^{6}\)Zhejiang Lab

{zhuangxiang,qiang.zhang.cs,dingkeyan,huajunsir}@zju.edu.cn

yatao.bian@gmail.com,xiao_wang@buaa.edu.cn

{jingsonglv,hongyang}@zhejianglab.com

Equal contribution.Corresponding author.

###### Abstract

Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called "first-encoding-then-separation" to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts. Our code is available at https://github.com/HICAI-ZJU/iMoLD.

## 1 Introduction

Computer-aided drug discovery has played an important role in facilitating molecular design, aiming to reduce costs and alleviate the high risk of experimental failure [1, 2]. In recent years, the emergence of deep learning has led to a growing interest in molecular representation learning, which aims to encode molecules as low-dimensional and dense vectors [3, 4, 5, 6, 7, 8]. These learned representations have demonstrated their availability in various tasks, including target structure prediction [9], binding affinity analysis [10], drug re-purposing [11] and retrosynthesis [12].

Despite the significant progress in molecular representation methods, a prevailing assumption in the traditional approaches is that data sources are independent and sampled from the same distribution. However, in practical drug development, molecules exhibit diverse characteristics and may originate from different distributions [13, 14]. For example, in virtual screening scenarios [15], the distribution shift occurs not only in the molecule itself, e.g., size [13] or scaffold [16] changes, but also in thetarget [14], e.g., the emergent COVID-19 leads to a new target from unknown distributions. This out-of-distribution (OOD) problem poses a challenge to the generalization capability of molecular representation methods and results in the degradation of performance in downstream tasks [17; 18].

Current studies mainly focus on regular Euclidean data for OOD generalization. Most studies [18; 19; 20; 21; 22] adopt the invariance principle [23; 24], which highlights the importance of focusing on the critical causal factors that remain invariant to distribution shifts while overlooking the spurious parts [23; 19; 20]. Although the invariance principle has shown effectiveness on Euclidean data, its application to non-Euclidean data necessitates further investigation and exploration. Molecules are often represented as graphs, a typical non-Euclidean data, where atoms as nodes and bonds as edges, thereby preserving rich structural information [3]. The complicated molecular graph structure makes it challenging to accurately distinguish the invariant causal parts from diverse spurious correlations [25].

Preliminary studies have made some attempts on molecular graphs [26; 25; 27; 28; 29]. They explicitly divide graphs to extract invariant substructures, at the granularity of nodes [28], edges [26; 27; 28; 29], or motifs [25]. These attempts can be summarized as the "first-separation-then-encoding" paradigm (Figure 1 (a)), which first divides the graph into invariant and spurious parts and then encodes each part separately. We argue that this practice is suboptimal for extremely complex and entangled graphs, such as real-world molecules [30; 31], since some intricate properties cannot be readily determined by analyzing a subset of the molecular structure [30; 31]. Besides, some methods [25; 27] require assumptions and inferences about the environmental distribution, which are often untenable for molecules due to the intricate environment. Additionally, the downstream tasks related to molecules are diverse, including regression and classification. However, some methods such as CIGA [26] and DisC [32] can only be applied to single-label classification tasks, due to the constraints imposed by the invariant learning objective function.

To fill these gaps, we present a novel molecule invariant learning framework to effectively capture the invariance of molecular graphs and achieve generalized representation against distribution shifts. In contrast to the conventional approaches, we propose a "first-encoding-then-separation strategy" (Figure 1 (b)). Specifically, we first employ a Graph Neural Network (GNN) [33; 34; 35] to encode the molecule (i.e., encoding GNN), followed by a residual vector quantization module to alleviate the over-fitting to training data distributions while preserving the expressivity of the encoder. We then utilize another GNN to score the molecule representation (i.e., scoring GNN), which measures the contributions of each dimension to the target in latent space, resulting in a clear separation between invariant and spurious representations. Finally, we design a self-supervised learning objective [36; 37; 38] that aims to encourage the identified invariant features to effectively preserve label-related information while discarding environment-related information. It is deserving of note that the objective is task-agnostic, which means that our method can be applied to various tasks, including regression and single- or multi-label classification.

Our main contributions can be summarized as follows:

* We propose a paradigm of "first-encoding-then-separation" using an encoding GNN and a scoring GNN, which enables us to effectively identify invariant features from highly complex graphs.
* We introduce a residual vector quantization module that strikes a balance between model expressivity and generalization. The quantization acts as the bottleneck to enhance generalization, while the residual connection complements the model's expressivity.
* We design a self-supervised invariant learning objective that facilitates the precise capture of invariant features. This objective is versatile, task-agnostic, and applicable to a variety of tasks.

Figure 1: (a) First-Separation-Then-Encoding: the input is separated using a subgraph generator, then each subgraph is encoded respectively. (b) First-Encoding-Then-Separation: the input is encoded, then the representation is separated by a scorer.

* We conduct comprehensive experiments on a diverse set of real-world datasets with various distribution shifts. The experimental results demonstrate the superiority of our method compared to state-of-the-art approaches.

## 2 Related Work

OOD Generalization and Invariance Principle.The susceptibility of deep neural networks to substantial performance degradation under distribution shifts has led to a proliferation of research focused on out-of-distribution (OOD) generalization [39]. Three lines of methods have been studied for OOD generalization on Euclidean data, including group distributionally robust optimization [40; 41; 42], domain adaptation [43; 44; 45] and invariant learning [19; 20; 21]. Group distributionally robust optimization considers groups of distributions and optimize across all groups simultaneously. Domain adaptation aims to align the data distributions but may fail to find an optimal predictor without additional assumptions [19; 20; 46]. Invariant learning aims to learn invariant representation satisfying the invariant principle [20; 24], which includes two assumptions: (1) sufficiency, meaning the representation has sufficient predictive abilities, (2) invariance, meaning representation is invariant to environmental changes. However, most methods require environmental labels, which are expensive to obtain for molecules [26], and direct application of these methods to complicated non-Euclidean molecular structure does not yield promising results [13; 22; 26].

OOD Generalization on Graphs.Recently, there has been growing attention on the graph-level representations under distribution shifts from the perspective of invariant learning. Some methods [26; 32; 27; 28; 25] follows the "first-separation-then-encoding" paradigm, and they make attempt to capture the invariant substructures by dividing nodes and edges in the explicit structural space. However, these methods suffer from the difficulty in dividing molecule graphs in raw space due to the complex and entangled molecular structure [47]. Moreover, MoleOOD [25] and GIL [27] require inference of unavailable environmental labels, which entails prior assumptions on the environmental distribution. And the objective of invariant learning may hinder the application, e.g., CIGA [26] and DisC [32] can only apply to single-label classification rather than to regression and multi-label tasks. Additionally, OOD-GNN [48] does not use the invariance principle. It proposes to learn disentangled graph representations, but requires computing global weights for all data, leading to a high computational cost. OOD generalization on graphs can also be improved by another line of relevant works [29; 49; 47] on GNN explainability [50; 51], which aims to provide a rationale for prediction. However, they may fail in some distribution shift cases [26]. And DIR [29] and GSAT [49] also divide graphs in the raw structural space. Although GREA [47] learns in the latent feature space, it only conducts separation on each node while neglecting the different significance of each representation dimensionality. In this work, we focus on the OOD generalization of molecular graphs, against multi-type of distribution shifts, e.g., scaffold, size, and assay, as shown in Figure 2.

Vector Quantization.Vector Quantization (VQ) [52; 53] acts as a bottleneck of representation learning. It discretizes continuous input data in the hidden space by assigning them to the nearest vectors in a predefined codebook. Some studies [53; 54; 55; 56] have demonstrated its effectiveness to enhance model robustness against data corruptions. Other studies [57; 58; 59] find that taking VQ as an inter-component communication within neural networks can contribute to the model generalization ability. However, we posit that while VQ can improve generalization against distribution shifts, it may also limit the model's expressivity and potentially lead to under-fitting. To address this concern, we propose to equip the conventional VQ with a residual connection to strike a balance between model generalization and expressivity.

Figure 2: An overview of distribution shifts in molecules. Distribution shifts occur when molecules originate from different scaffold, size or assay environments.

Preliminaries

### Problem Definition

We focus on the OOD generalization of molecules. Let \(\mathcal{G}\) be the molecule graph space and \(\mathcal{Y}\) be the label space, the goal is to find a predictor \(f\,:\mathcal{G}\rightarrow\mathcal{Y}\) to map the input \(G\in\mathcal{G}\) into the label \(Y\in\mathcal{Y}\). Generally, we are given a set of datasets collected from multiple environments \(E_{all}:D=\{D^{e}\}_{e\in E_{all}}\). Each \(D^{e}\) contains pairs of an input molecule and its label: \(D^{e}=\{(G_{i},Y_{i})\}_{i=1}^{n_{e}}\) that are drawn from the joint distribution \(P(G,\,Y|E=e)\) of environment \(e\). However, the information of environment \(e\) is always not available for molecules, thus we redefine the training joint distribution as \(P_{train}(G,\,Y)=P(G,\,Y|E=e),\forall e\in E_{train}\), and the testing joint distribution as \(P_{test}(G,\,Y)=P(G,\,Y|E=e),\forall e\in E_{all}\setminus E_{train}\), where \(P_{train}(G,\,Y)\neq P_{test}(G,\,Y)\). We denote joint distribution across all environment as \(P_{all}(G,\,Y)=P(G,\,Y|E=e),\forall e\in E_{all}\). Formally, the goal is to learn an optimal predictor \(f^{*}\) based on training data and can generalize well across all distributions:

\[f^{*}=\arg\min_{f}\mathbb{E}_{(G,\,Y)\sim P_{all}}\left[\ell\left(f\left(G \right),\,Y\right)\right],\] (1)

where \(\ell(\cdot,\cdot)\) is the empirical risk function. Moreover, since the joint distribution \(P(G,\,Y)\) can be written as \(P(Y|G)P(G)\), the OOD problem can be refined into two cases, namely covariate and concept shift [60; 61; 62; 63]. In covariate shift, the distribution of input differs. Formally, \(P_{train}(G)\neq P_{test}(G)\) and \(P_{train}(Y|G)=P_{test}(Y|G)\). While concept shift occurs when the conditional distribution changes as \(P_{train}(Y|G)\neq P_{test}(Y|G)\) and \(P_{train}(G)=P_{test}(G)\). We will consider and distinguish between both cases in our experiments.

### Molecular Representation Learning

We denote a molecule graph by \(G=\{\mathcal{V},\mathcal{E}\}\), where \(\mathcal{V}\) is the set of nodes (e.g., atoms) and \(\mathcal{E}\in\mathcal{V}\times\mathcal{V}\) is the set of edges (e.g., chemical bonds). Generally, the predictor \(f\) can be denoted as \(\rho\circ g\), containing an encoder \(g:\mathcal{G}\rightarrow\mathbb{R}^{d}\) that extracts representation for each molecule and a downstream classifier \(\rho:\mathbb{R}^{d}\rightarrow\mathcal{Y}\) that predicts the label with the molecular representation. In particular, the encoder \(g\) operates in two stages: firstly, by employing a graph neural network [33; 34; 35] to generate node representations \(\mathbf{H}\) according to the following equation:

\[\mathbf{H}=\left[\mathbf{h}_{1},\mathbf{h}_{2},\ldots,\mathbf{h}_{|\mathcal{ V}|}\right]^{\top}=\mathrm{GNN}(G)\in\mathbb{R}^{|\mathcal{V}|\times d},\] (2)

where \(\mathbf{h}_{v}\in\mathbb{R}^{d}\) is the representation of node \(v\). Secondly, the encoder utilizes a readout operator to obtain the overall graph representation \(\mathbf{z}\):

\[\mathbf{z}=\mathrm{READOUT}(\mathbf{H})\in\mathbb{R}^{d}.\] (3)

The readout operator can be implemented using a simple, permutation invariant function such as average pooling.

## 4 Method

This section presents the details of our proposed method that learns invariant **M**olecular representation in **L**atent **D**iscrete space, called **i**MoLD**. Figure 3 shows the overview of iMoLD, which mainly consists of three steps: 1) Using a GNN encoder and a residual vector quantization module to obtain the molecule representation (Section 4.1); 2) Separating the representation into invariant and spurious parts through a GNN scorer (Section 4.2); 3) Optimizing the above process with a task-agnostic self-supervised learning objective (Section 4.3).

### Encoding with Residual Vector Quantization

The current mainstream methods [26; 25; 28; 27; 32] adopt the "first-separation-then-encoding" paradigm, which explicitly divides graphs into invariant and spurious substructures on the granularity of edge, node, or motif, and then encodes each substructure individually. In contrast, we use the opposite paradigm that first encodes the whole molecule followed by separation.

Specifically, given an input molecule \(G\), we first use a GNN to encode it, resulting in node representations \(\mathbf{H}\):

\[\mathbf{H}=\mathrm{GNN}_{E}(G)\in\mathbb{R}^{|\mathcal{V}|\times d},\] (4)

where \(\mathrm{GNN}_{E}\) represents the encoding GNN, \(|\mathcal{V}|\) is the number of nodes in \(G\), and \(d\) is the dimensionality of features. Inspired by the studies [57, 59] that Vector Quantization (VQ) [52, 53] is helpful to improve the model generalization on computer vision tasks, we propose a Residual Vector Quantization (RVQ) module to refine the obtained representations.

In the RVQ module, VQ is used to discretize continuous representations into discrete ones. Formally, it introduces a shared learnable codebook as a discrete latent space: \(\mathcal{C}=\left\{\mathbf{e}_{1},\mathbf{e}_{2},\ldots\mathbf{e}_{|\mathcal{C }|}\right\}\), where each \(\mathbf{e}_{k}\in\mathbb{R}^{d}\). For each node representation \(\mathbf{h}_{v}\) in \(\mathbf{H}\), VQ looks up and fetches the nearest neighbor in the codebook and outputs it as the result. Mathematically,

\[\mathrm{Q}(\mathbf{h}_{v})=\mathbf{e}_{k},\quad\text{where}\quad k=\operatorname* {argmin}_{k\in\{1,\ldots,|\mathcal{C}|\}}\left\|\mathbf{h}_{v}-\mathbf{e}_{k} \right\|_{2},\] (5)

and \(\mathrm{Q}(\cdot)\) denotes the discretization operation which quantizes \(\mathbf{h}_{v}\) to \(\mathbf{e}_{k}\) in the codebook.

The VQ operation acts as a bottleneck to enhance generalization and alleviate the easy-over-fitting issue caused by distribution shifts. However, it also impairs the expressivity of the model by using a limited discrete codebook to replace the original continuous input, suffering from a potential under-fitting issue. Accordingly, we propose to equip the conventional VQ with a residual connection to strike a balance between model generalization and expressivity. In specific, we incorporate both the continuous and discrete representations to update node representations \(\mathbf{H}\) to \(\mathbf{H}^{\prime}\):

\[\mathbf{H}^{\prime}=\left[\mathrm{Q}(\mathbf{h}_{1})+\mathbf{h}_{1},\mathrm{ Q}(\mathbf{h}_{2})+\mathbf{h}_{2},\ldots,\mathrm{Q}(\mathbf{h}_{|\mathcal{V}|})+ \mathbf{h}_{|\mathcal{V}|}\right]^{\top}.\] (6)

Similar to VQ-VAE [52, 53], we employ the exponential moving average updates for the codebook:

\[N_{k}^{(t)}=N_{k}^{(t-1)}*\eta+n_{k}^{(t)}(1-\eta),\quad\mathbf{m}_{k}^{(t)}= \mathbf{m}_{k}^{(t-1)}*\eta+\sum_{v}^{n_{k}^{(t)}}\mathbf{h}_{v}^{(t)}(1-\eta ),\quad\mathbf{e}_{k}^{(t)}=\frac{\mathbf{m}_{k}^{(t)}}{N_{k}^{(t)}},\] (7)

where \(n_{k}^{(t)}\) is the number of node representations in the \(t\)-th mini-batch that are quantized to \(\mathbf{e}_{k}\), and \(\eta\) is a decay parameter between 0 and 1.

### Separation at Nodes and Features

After encoding, we separate the representation into invariant parts and spurious parts. It is worth noting that our separation is not only performed at the node dimension but also takes into account the feature dimension in the latent space. The reasons are two-fold: 1) Distribution shifts on molecules

Figure 3: An overview of iMoLD. Firstly, given a batch of inputs, we learn invariant and spurious representations (\(\mathbf{z}^{\text{Inv}}\) and \(\mathbf{z}^{\text{Spin}}\)) for each input in latent discrete space by a first-encoding-then-separation paradigm. An encoding GNN and an RVQ module are involved to obtain molecule representation, then the representation is separated through a scoring GNN. The invariant \(\mathbf{z}^{\text{Inv}}\) is used to predict the label \(\vec{y}\). Then a task-agnostic self-supervised learning objective across the batch is designed to facilitate the acquisition of reliable invariant \(\mathbf{z}^{\text{Inv}}\).

can occur at both the structure level and the attribute level [26], corresponding to the node dimension \(|\mathcal{V}|\) and the feature dimension \(d\) in \(\mathbf{H^{\prime}}\), respectively. 2) The resulting representation may be highly entangled, thus it is advisable to perform a separation on each dimension in the latent space.

Specifically, we use another GNN as a scorer to obtain the separating score \(\mathbf{S}\):

\[\mathbf{S}=\sigma\left(\mathrm{GNN}_{S}\left(G\right)\right)\in\mathbb{R}^{| \mathcal{V}|\times d},\] (8)

where \(\mathrm{GNN}_{S}\) represents the scoring GNN, \(|\mathcal{V}|\) is the number of nodes in \(G\), and \(d\) is the dimensionality of features. \(\sigma(\cdot)\) denotes the Sigmoid function to constrain each entry in \(\mathbf{S}\) falls into the range of \((0,1)\). Then we can capture the invariant and complementary spurious features at both structure and attribute granularity in the latent representation space by applying the separating scores to node representations:

\[\mathbf{H}^{\text{Inv}}=\mathbf{H^{\prime}}\odot\mathbf{S},\quad\mathbf{H}^{ \text{Spu}}=\mathbf{H^{\prime}}\odot\left(1-\mathbf{S}\right),\] (9)

where \(\mathbf{H}^{\text{Inv}}\) and \(\mathbf{H}^{\text{Spu}}\) denote the invariant and spurious node representations respectively, and \(\odot\) is the element-wise product. Finally, the invariant and spurious representation (denoted as \(\mathbf{z}^{\text{Inv}}\) and \(\mathbf{z}^{\text{Spu}}\) respectively) of \(G\) can be generated by a readout operator:

\[\mathbf{z}^{\text{Inv}}=\mathrm{READOUT}(\mathbf{H}^{\text{Inv}})\in\mathbb{R }^{d},\quad\mathbf{z}^{\text{Spu}}=\mathrm{READOUT}(\mathbf{H}^{\text{Spu}}) \in\mathbb{R}^{d}.\] (10)

### Learning Objective

Our OOD optimization objectives are composed of an invariant learning loss, a task prediction loss, and two additional regularization losses.

Task-agnostic Self-supervised Invariant Learning.Invariant learning aims to optimize the encoding \(\mathrm{GNN}_{E}\) and the scoring \(\mathrm{GNN}_{S}\) to produce precise invariant and spurious representations. In particular, we need \(\mathbf{z}^{\text{Inv}}\) to be invariant under environmental changes. Additionally, we expect the objective to be independent of the downstream task, which allows the method to be not restricted to a specific type of task. To achieve these, we design a task-agnostic and self-supervised invariant learning objective. Specifically, we disturb \(\mathbf{z}^{\text{Inv}}_{i}\) via concatenating a corresponding \(\mathbf{z}^{\text{Spu}}_{j}\) in a shuffled batch, resulting in an augmented representation \(\widetilde{\mathbf{z}}^{\text{Inv}}_{i}\):

\[\widetilde{\mathbf{z}}^{\text{Inv}}_{i}=\mathbf{z}^{\text{Inv}}_{i}\oplus \mathbf{z}^{\text{Spu}}_{j\in[1,B]},\] (11)

where \(\oplus\) denotes concatenation operator and \(B\) is batch size.

Inspired by a simple self-supervised learning framework that takes different augmentation views as similar positive pairs and does not require negative samples [37], we treat \(\mathbf{z}^{\text{Inv}}_{i}\) and \(\widetilde{\mathbf{z}}^{\text{Inv}}_{i}\) as positive pairs and push them to be similar, using an MLP-based predictor (denoted as \(\omega\)) that transforms the output of one view and aligns it to the other view. We minimize their negative cosine similarity:

\[\mathcal{L}_{\text{inv}}=-\sum_{i=1}^{B}\mathrm{sim}(\mathrm{sg}[\mathbf{z}^{ \text{Inv}}_{i}],\omega(\widetilde{\mathbf{z}}^{\text{Inv}}_{i})),\] (12)

where \(\mathrm{sim}(\cdot,\cdot)\) represents the formula of cosine similarity and \(\mathrm{sg}[\cdot]\) denotes stop-gradient operation to prevent collapsing [37]. We employ \(\mathcal{L}_{\text{inv}}\) as our objective for invariant learning to ensure the invariance of \(\mathbf{z}^{\text{Inv}}\) against distribution shifts.

Task Prediction.The objective of task prediction is to provide an invariant presentation \(\mathbf{z}^{\text{Inv}}\) with sufficient predictive abilities. During training, the choice of prediction loss function depends on the type of task. For classification tasks, we employ the cross-entropy loss, while for regression tasks, we use the mean squared error loss. Take the binary classification task as an example, the cross-entropy loss is computed between the predicted \(\widehat{y}_{i}=\rho(\mathbf{z}^{\text{Inv}}_{i})\) and the ground-truth label \(y_{i}\):

\[\mathcal{L}_{\text{pred}}=\sum_{i=1}^{B}\left(y_{i}\log\widehat{y}_{i}+(1-y_{i })\log\left(1-\widehat{y}_{i}\right)\right).\] (13)

[MISSING_PAGE_FAIL:7]

* **DrugOOD**[13], which is a OOD benchmark for AI-aided drug discovery. This benchmark provides three environment-splitting strategies, including assay, scaffold and size, and applies these three splitting to two measurements (IC50 and EC50). As a result, we obtain 6 datasets, and each dataset contains a binary classification task for drug target binding affinity prediction. A detailed description of each environment-splitting strategy is also in Appendix A.

Baselines.We thoroughly compare our method against ERM [65] and two groups of OOD baselines: (1) general OOD algorithms used for Euclidean data, which include domain adaptation methods such as Coral [44] and DANN [43], group distributionally robust optimization method (GroupDRO [41]), invariant learning methods such as IRM [19] and VREx [40] and data augmentation method (Mixup [66]). And (2) graph-specific algorithms, including Graph OOD algorithms such as CAL [28], DisC [32], MoleOOD [25] and CIGA [26], as well as interpretable graph learning methods such as DIR [29], GSAT [49] and GREA [47]. Details of baselines and implementation are in Appendix B.

Evaluation.We report the ROC-AUC score for GOOD-HIV and DrugOOD datasets as the task is binary classification. For GOOD-ZINC, we use the Mean Average Error (MAE) since the task is regression. While for GOOD-PCBA, we use Average Precision (AP) averaged over all tasks as the evaluation metric due to extremely imbalanced classes. We run experiments 10 times with different random seeds, select models based on the validation performance and report the mean and standard deviations on the test set.

### Main Results (RQ1)

Table 1 and Table 2 present the empirical results on GOOD and DrugOOD benchmarks, respectively. Our method iMoLD achieves the best performance on 16 of the 18 datasets and ranks second on the other two datasets. Among the compared baselines, the graph-specific OOD methods perform best on only 11 datasets, and some general OOD methods outperform them on another 7 datasets. This suggests that although some advanced graph-specific OOD methods can achieve superior performance on some synthetic datasets (e.g., to predict whether a specific motif is present in a synthetic graph [29]), they may not perform well on molecules due to the realistic and complex data structures and distribution shifts. In contrast, our method is able to achieve the best performance on most of the datasets, which indicates that the proposed identification of invariant features in the latent space is effective for applying the invariance principle to the molecular structure. We also observe that MoleOOD, a method designed specifically for molecules, does not perform well on GOOD-ZINC and GOOD-PCBA, possibly due to its dependence on inferred environments. Inferring environments may become more challenging for larger-scale datasets, such as GOOD-ZINC and GOOD-PCBA, which contain hundreds of thousands of data and more complex tasks (e.g., PCBA has a total of 128 classification tasks). Our method does not require the inference of environment, and is shown to be effective on datasets of diverse scales and tasks.

\begin{table}
\begin{tabular}{l|c c c|c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{IC50 \(\uparrow\)} & \multicolumn{3}{c}{EC50 \(\uparrow\)} \\ \cline{2-7}  & Assay & Scaffold & Size & Assay & Scaffold & Size \\ \hline ERM & 71.63(0.76) & 68.79(0.47) & 67.50(0.38) & 67.39(2.90) & 64.98(1.29) & 65.10(0.38) \\ IRM & 71.15(0.57) & 67.22(0.62) & 61.58(0.58) & 67.77(2.71) & 63.86(1.36) & 59.19(0.83) \\ Coral & 71.28(0.91) & 68.36(0.61) & 64.53(0.32) & 72.08(2.80) & 64.83(1.64) & 58.47(0.43) \\ MiVUp & 71.49(1.08) & 68.59(0.27) & 67.79(0.39) & 67.81(4.06) & 65.77(1.83) & 65.77(0.60) \\ \hline DIR & 69.84(1.41) & 66.33(0.65) & 62.92(1.89) & 65.81(2.93) & 63.76(3.22) & 61.56(4.23) \\ GSAT & 70.59(0.43) & 66.45(0.50) & 66.70(0.37) & 73.82(2.62) & 64.25(0.63) & 62.65(1.79) \\ GREA & 70.23(1.17) & 67.02(0.28) & 66.59(0.56) & 74.17(1.47) & 64.50(0.78) & 62.81(1.54) \\ CAL & 70.09(1.03) & 65.90(1.04) & 66.42(0.50) & 74.54(4.18) & 65.19(0.87) & 61.21(1.76) \\ DisC & 61.40(2.56) & 62.70(2.11) & 61.43(1.06) & 63.71(5.56) & 60.57(2.27) & 57.38(2.48) \\ MoleOOD & 71.62(0.52) & 68.58(1.14) & 65.62(0.77) & 72.69(1.46) & 65.74(1.47) & 65.51(1.24) \\ CIGA & 71.86(1.37) & **69.14(0.70)** & 66.92(0.54) & 69.15(5.79) & 67.32(1.35) & 65.65(0.82) \\ iMoLD & **72.11(0.51)** & 68.84(0.58) & **67.92(0.43)** & **77.48(1.70)** & **67.79(0.88)** & **67.09(0.91)** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluation performance on DrugOOD benchmark. The best is marked with **boldface** and the second best is with underline.

[MISSING_PAGE_FAIL:9]

when the model achieves the best score on the validation set, using t-SNE [67] on the covariate-shift dataset of GOOD-HIV-Scaffold in Figure 4 (d). We also visualize the results of some baselines, including the vanilla ERM (Figure 4 (a)), the ERM equipped with VQ after encoder (ERM(+VQ), Figure 4 (b)), and with the RVQ module (ERM(+RVQ), Figure 4 (c)). We additionally compute the 1-order Wasserstein distance [68] between the features on the training and validation sets of each class, to quantify the dissimilarity in the feature distribution across varying environments. We find that adding the VQ or RVQ after the encoder results in a more uniform distribution of features and lower feature distances, due to the fact that VQ makes it possible to reuse previously encountered embeddings in new environments by discretizing them. Moreover, the feature distribution of our method is more uniform and the distance is smaller. This suggests that our method is effective in identifying features that are invariant across different environments. Moreover, we also observe that all ERM methods achieve lower validation scores with higher training scores, implying that these methods are prone to overfitting. Our method, on the other hand, achieves not only a higher validation score but also a higher corresponding training score, thereby demonstrating its ability to overcome the problem of easy overfitting and improve the generalization ability effectively.

## 6 Conclusion

In this work, we propose a new framework that learns invariant molecular representation against distribution shifts. We adopt a "first-encoding-then-separation" strategy, wherein a combination of encoding GNN and residual vector quantization is utilized to derive molecular representation in latent discrete space. Then we learn a scoring GNN to identify invariant features from this representation. Moreover, we design a task-agnostic self-supervised learning objective to enable precise invariance identification and versatile applicability to various tasks. Extensive experiments on real-world datasets demonstrate the superiority of our method on the molecular OOD problem. Overall, our proposed framework presents a promising approach for learning invariant molecular representations and offers valuable insights for addressing distribution shifts in molecular data analysis.

## Acknowledgments

This work was supported by the National Key Research and Development Program of China (2022YFB4500300), National Natural Science Foundation of China (NSFCU19B2027, NSFC91846204, NSFC62302433), joint project DH-2022ZY0012 from Donghai Lab, and sponsored by CCF-Tencent Open Fund (CCF-Tencent RAGR20230122). We want to express gratitude to the anonymous reviewers for their hard work and kind comments.

## References

* [1] Eugene N Muratov, Jurgen Bajorath, Robert P Sheridan, Igor V Tetko, Dmitry Filimonov, Vladimir Poroikov, Tudor I Oprea, Igor I Baskin, Alexandre Varnek, Adrian Roitberg, et al. Qsar without borders. _Chemical Society Reviews_, 49(11):3525-3564, 2020.
* [2] Stephani Joy Y Macalino, Vijayakumar Gosu, Sunhye Hong, and Sun Choi. Role of computer-aided drug design in modern drug discovery. _Archives of pharmacal research_, 38:1686-1701, 2015.
* [3] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In _ICML_, volume 70 of _Proceedings of Machine Learning Research_, pages 1263-1272. PMLR, 2017.
* [4] Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et al. Analyzing learned molecular representations for property prediction. _Journal of chemical information and modeling_, 59(8):3370-3388, 2019.
* [5] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. In _NeurIPS_, 2020.

* [6] Yin Fang, Qiang Zhang, Haihong Yang, Xiang Zhuang, Shumin Deng, Wen Zhang, Ming Qin, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Molecular contrastive learning with chemical element knowledge graph. In _AAAI_, pages 3968-3976. AAAI Press, 2022.
* [7] Yin Fang, Qiang Zhang, Ningyu Zhang, Zhuo Chen, Xiang Zhuang, Xin Shao, Xiaohui Fan, and Huajun Chen. Knowledge graph-enhanced molecular contrastive learning with functional prompt. _Nature Machine Intelligence_, pages 1-12, 2023.
* [8] Xiang Zhuang, Qiang Zhang, Bin Wu, Keyan Ding, Yin Fang, and Huajun Chen. Graph sampling-based meta-learning for molecular property prediction. In _IJCAI_, pages 4729-4737. ijcai.org, 2023.
* [9] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* [10] Mostafa Karimi, Di Wu, Zhangyang Wang, and Yang Shen. Deepaffinity: interpretable deep learning of compound-protein affinity through unified recurrent and convolutional neural networks. _Bioinformatics_, 35(18):3329-3338, 2019.
* [11] Deisy Morselli Gysi, Italo Do Valle, Marinka Zitnik, Asher Ameli, Xiao Gan, Onur Varol, Susan Dina Ghiassian, JJ Patten, Robert A Davey, Joseph Loscalzo, et al. Network medicine framework for identifying drug-repurposing opportunities for covid-19. _Proceedings of the National Academy of Sciences_, 118(19):e2025581118, 2021.
* [12] Connor W Coley, Luke Rogers, William H Green, and Klavs F Jensen. Computer-assisted retrosynthesis based on molecular similarity. _ACS central science_, 3(12):1237-1245, 2017.
* A focus on affinity prediction problems with noise annotations. _CoRR_, abs/2201.09637, 2022.
* [14] David Mendez, Anna Gaulton, A Patricia Bento, Jon Chambers, Marleen De Veij, Eloy Felix, Maria Paula Magarinos, Juan F Mosquera, Prudence Mutowo, Michal Nowotka, et al. Chembl: towards direct deposition of bioassay data. _Nucleic acids research_, 47(D1):D930-D940, 2019.
* [15] Douglas B Kitchen, Helene Decornez, John R Furr, and Jurgen Bajorath. Docking and scoring in virtual screening for drug discovery: methods and applications. _Nature reviews Drug discovery_, 3(11):935-949, 2004.
* [16] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.
* [17] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In _COLT_, 2009.
* [18] Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant feature representation. In _ICML (1)_, volume 28 of _JMLR Workshop and Conference Proceedings_, pages 10-18. JMLR.org, 2013.
* [19] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _CoRR_, abs/1907.02893, 2019.
* [20] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. In _NeurIPS_, pages 3438-3450, 2021.
* [21] Elliot Creager, Jorn-Henrik Jacobsen, and Richard S. Zemel. Environment inference for invariant learning. In _ICML_, volume 139 of _Proceedings of Machine Learning Research_, pages 2189-2200. PMLR, 2021.

* [22] Masanori Koyama and Shoichiro Yamaguchi. Out-of-distribution generalization with maximal invariant predictor. _CoRR_, abs/2008.01883, 2020.
* [23] Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. _Journal of the Royal Statistical Society. Series B (Statistical Methodology)_, pages 947-1012, 2016.
* [24] Mateo Rojas-Carulla, Bernhard Scholkopf, Richard E. Turner, and Jonas Peters. Invariant models for causal transfer learning. _J. Mach. Learn. Res._, 19:36:1-36:34, 2018.
* [25] Nianzu Yang, Kaipeng Zeng, Qitian Wu, Xiaosong Jia, and Junchi Yan. Learning substructure invariance for out-of-distribution molecular representations. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [26] Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, Kaili Ma, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-distribution generalization on graphs. In _Advances in Neural Information Processing Systems_, 2022.
* [27] Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning invariant graph representations for out-of-distribution generalization. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 11828-11841. Curran Associates, Inc., 2022.
* [28] Yongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, and Tat-Seng Chua. Causal attention for interpretable and generalizable graph classification. In _KDD_, pages 1696-1705. ACM, 2022.
* [29] Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. In _ICLR_. OpenReview.net, 2022.
* [30] Li-Juan Jiang, Milan Vasak, Bert L Vallee, and Wolfgang Maret. Zinc transfer potentials of the \(\alpha\)-and \(\beta\)-clusters of metallothioneein are affected by domain interactions in the whole molecule. _Proceedings of the National Academy of Sciences_, 97(6):2503-2508, 2000.
* [31] Marie Jose Huron and Pierre Claverie. Calculation of the interaction energy of one molecule with its whole surrounding. i. method and application to pure nonpolar compounds. _The Journal of Physical Chemistry_, 76(15):2123-2133, 1972.
* [32] Shaohua Fan, Xiao Wang, Yanhu Mo, Chuan Shi, and Jian Tang. Debiasing graph neural networks via learning disentangled causal substructure. _CoRR_, abs/2209.14107, 2022.
* [33] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _ICLR (Poster)_. OpenReview.net, 2017.
* [34] William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _NIPS_, pages 1024-1034, 2017.
* [35] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _ICLR_. OpenReview.net, 2019.
* [36] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [37] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 15750-15758, 2021.
* [38] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.

* [39] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. _CoRR_, abs/2108.13624, 2021.
* [40] David Krueger, Ethan Caballero, Jorn-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron C. Courville. Out-of-distribution generalization via risk extrapolation (rex). In _ICML_, volume 139 of _Proceedings of Machine Learning Research_, pages 5815-5826. PMLR, 2021.
* [41] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In _ICLR_. OpenReview.net, 2020.
* [42] Michael Zhang, Nimit Sharad Sohoni, Hongyang R. Zhang, Chelsea Finn, and Christopher Re. Correct-n-contrast: a contrastive approach for improving robustness to spurious correlations. In _ICML_, volume 162 of _Proceedings of Machine Learning Research_, pages 26484-26516. PMLR, 2022.
* [43] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. _J. Mach. Learn. Res._, 17:59:1-59:35, 2016.
* [44] Baochen Sun and Kate Saenko. Deep CORAL: correlation alignment for deep domain adaptation. In _ECCV Workshops (3)_, volume 9915 of _Lecture Notes in Computer Science_, pages 443-450, 2016.
* [45] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In _ECCV (15)_, volume 11219 of _Lecture Notes in Computer Science_, pages 647-663. Springer, 2018.
* [46] Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J. Gordon. On learning invariant representations for domain adaptation. In _ICML_, volume 97 of _Proceedings of Machine Learning Research_, pages 7523-7532. PMLR, 2019.
* [47] Gang Liu, Tong Zhao, Jiaxin Xu, Tengfei Luo, and Meng Jiang. Graph rationalization with environment-based augmentations. In _KDD_, pages 1069-1078. ACM, 2022.
* [48] Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Ood-gnn: Out-of-distribution generalized graph neural network. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* [49] Siqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. In _ICML_, volume 162 of _Proceedings of Machine Learning Research_, pages 15524-15543. PMLR, 2022.
* [50] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks. In _NeurIPS_, pages 9240-9251, 2019.
* [51] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [52] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In _NIPS_, pages 6306-6315, 2017.
* [53] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In _NeurIPS_, pages 14837-14847, 2019.
* [54] Woncheol Shin, Gyubok Lee, Jiyoung Lee, Joonseok Lee, and Edward Choi. Translation-equivariant image quantizer for bi-directional image-text generation. _CoRR_, abs/2112.00384, 2021.
* [55] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural audio codec. _IEEE ACM Trans. Audio Speech Lang. Process._, 30:495-507, 2022.

* [56] Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z. Li. Mole-BERT: Rethinking pre-training graph neural networks for molecules. In _The Eleventh International Conference on Learning Representations_, 2023.
* [57] Dianbo Liu, Alex Lamb, Kenji Kawaguchi, Anirudh Goyal, Chen Sun, Michael C. Mozer, and Yoshua Bengio. Discrete-valued neural communication. In _NeurIPS_, pages 2109-2121, 2021.
* [58] Frederik Trauble, Anirudh Goyal, Nasim Rahaman, Michael Mozer, Kenji Kawaguchi, Yoshua Bengio, and Bernhard Scholkopf. Discrete key-value bottleneck. _CoRR_, abs/2207.11240, 2022.
* [59] Dianbo Liu, Alex Lamb, Xu Ji, Pascal Notsawo, Michael Mozer, Yoshua Bengio, and Kenji Kawaguchi. Adaptive discrete communication bottlenecks with dynamic vector quantization. _CoRR_, abs/2202.01334, 2022.
* [60] Jose G Moreno-Torres, Troy Raeder, Rocio Alaiz-Rodriguez, Nitesh V Chawla, and Francisco Herrera. A unifying view on dataset shift in classification. _Pattern recognition_, 45(1):521-530, 2012.
* [61] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. _Dataset shift in machine learning_. Mit Press, 2008.
* [62] Gerhard Widmer and Miroslav Kubat. Learning in the presence of concept drift and hidden contexts. _Machine learning_, 23:69-101, 1996.
* [63] Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. _CoRR_, abs/2206.08452, 2022.
* [64] Rafael Gomez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose Miguel Hernandez-Lobato, Benjamin Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alan Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. _ACS central science_, 4(2):268-276, 2018.
* [65] Vladimir Cherkassky. The nature of statistical learning theory. _IEEE Trans. Neural Networks_, 8(6):1564, 1997.
* [66] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In _ICLR (Poster)_. OpenReview.net, 2018.
* [67] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* [68] Cedric Villani et al. _Optimal transport: old and new_, volume 338. Springer, 2009.
* [69] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _NeurIPS_, pages 8024-8035, 2019.
* [70] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _CoRR_, abs/1903.02428, 2019.

Details of Datasets

### Dataset

In this paper, we use 18 publicly benchmark datasets, 12 of which are from GOOD [63] benchmark. They are the combination of 3 datasets (GOOD-HIV, GOOD-ZINC and GOOD-PCBA), 2 types of distribution shift (covariate and concept), and 2 environment-splitting strategies (scaffold and size). The rest 6 datasets are from DrugOOD [13] benchmark, including IC50-Assay, IC50-Scaffold, IC50-Size, EC50-Assay, EC50-Scaffold, and EC50-Size. The prefix denotes the measurement and the suffix denotes the environment-splitting strategies. This benchmark exclusively focuses on covariate shift. We use the latest data released on the official webpage3 based on the ChEMBL 30 database4. We use the default dataset split proposed in each benchmark. For covariate shift, the training, validation and testing sets are obtained based on environments without interactions. For concept shift, a screening approach is leveraged to scan and select molecules in the dataset. Statistics of each dataset are in Table 4.

Footnote 3: https://drugood.github.io/

Footnote 4: http://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/releases/chembl_30

### The Cause of Molecular Distribution Shift

The molecule data can be divided according to different environments, and distribution shifts occur when the source environments of data are different during training and testing. In this work, we investigate three types of environment-splitting strategies, i.e., scaffold, size and assay. And the explanation of each environment are in Table 5.

## Appendix B Details of Implementation

### Baselines

We adopt the following methods as baselines for comparison, one group of which are common approaches for non-Euclidean data:

* **ERM**[65] minimizes the empirical loss on the training set.
* **IRM**[19] seeks to find data representations across all environments by penalizing feature distributions that have different optimal classifiers.
* **VREx**[40] reduces the risk variances of training environments to achieve both covariate robustness and invariant prediction.

* **GroupDRO**[41] minimizes the loss on the worst-performing group, subject to a constraint that ensures the loss on each group remains close.
* **Coral**[44] encourages feature distributions consistent by penalizing differences in the means and covariances of feature distributions for each domain.
* **DANN**[43] encourages features from different environments indistinguishable by adversarially training a regular classifier and a domain classifier.
* **Mixup**[66] augments data in training through data interpolation.

And the others are graph-specific methods:

* **DIR5**[29] discovers the subset of a graph as invariant rationale by conducting interventional data augmentation to create multiple distributions. Footnote 5: https://github.com/Wuyxin/DIR-GNN
* **GSAT6**[49] proposes to build an interpretable graph learning method through the attention mechanism and inject stochasticity into the attention to select label-relevant subgraphs. Footnote 6: https://github.com/Graph-COM/GSAT
* **GREA7** identifies subgraph structures called rationales by environment replacement to create virtual data points to improve generalizability and interpretability. Footnote 7: https://github.com/liugangcode/GREA
* **CAL8**[28] proposes a causal attention learning strategy for graph classification to encourage GNNs to exploit causal features while ignoring the shortcut paths. Footnote 8: https://github.com/yongdousui/CAL
* **DisC9**[32] analyzes the generalization problem of GNNs in a causal view and proposes a disentangling framework for graphs to learn causal and bias substructure. Footnote 9: https://github.com/googlebaba/DisC
* **MoleOOD10**[25] investigates the OOD problem on molecules and designs an environment inference model and a substructure attention model to learn environment-invariant molecular substructures. Footnote 10: https://github.com/yangnianzu0515/MoleOOD
* **CIGA11**[26] proposes an information-theoretic objective to extract the desired invariant subgraphs from the lens of causality. Footnote 11: https://github.com/LFPhase/CIGA

### Implementation

Experiments are conducted on one 24GB NVIDIA RTX 3090 GPU.

\begin{table}
\begin{tabular}{p{142.3pt} p{284.5pt}} \hline \hline Environment & Explanation \\ \hline Scaffold & Molecular scaffold is the fundamental structure of a molecule with desirable bioactive properties. Molecules with the same scaffold belong to the same environment. Distribution shift arises when there is a change in the molecular scaffold. \\ \hline Size & The size of a molecule refers to the total number of atoms in the molecule. Molecular size is also an inherent structural characteristic of molecular graphs. The distribution shift occurs when size changes. \\ \hline Assay & Assay is an experimental method as an examination or determination for molecular characteristics. Due to variations in assay environments and targets, activity values measured by different assays often differ significantly. Samples tested within the same assay belong to a single environment, while a change in the assay leads to a distribution shift. \\ \hline \hline \end{tabular}
\end{table}
Table 5: Description of different environment splits leading to molecular distribution shifts.

Baselines.For datasets in the GOOD benchmark, we use the results provided in the official leaderboard12. For datasets in the DrugOOD benchmark, we use the official benchmark code13 to get the results for ERM, IRM, Coral, and Mixup on the latest version of datasets. The results for GroupDRO and DANN are not reported due to an error occurred while the code was running. For some baselines that do not have reported results, we implement them using public codes. All of the baselines are implemented using the GIN-Virtual [3; 35] (on GOOD) or GIN [35] (on DrugOOD) as the GNN backbone that is parameterized according to the guidance of the respective benchmark. And we conduct a grid search to select hyper-parameters for all implemented baselines.

Footnote 12: https://good.readthedocs.io/en/latest/leaderboard.html

Footnote 13: https://github.com/tencent-ailab/DrugOOD

Our method.We implement the proposed iMoLD in Pytorch [69] and PyG [70]. For all the datasets, we select hyper-parameters by ranging the code book size \(|\mathcal{C}|\) from \(\{100,500,1000,4000,10000\}\), threshold \(\gamma\) from \(\{0.1,0.5,0.7,0.9\}\), \(\lambda_{1}\) from \(\{0.001,0.01,0.1,0.5\}\), \(\lambda_{2}\) from \(\{0.01,0.1,0.5,1\}\), \(\lambda_{3}\) from \(\{0.01,0.1,0.3,0.5,1\}\), and batch size from \(\{32,64,128,256,512\}\). For datasets in DrugOOD, we also select dropout rate from \(\{0.1,0.3,0.5\}\). The maximum number of epochs is set to \(200\) and the learning rate is set to \(0.001\). Please refer to Table 6 for a detailed hyper-parameter configuration of various datasets. The hyper-parameter sensitivity analysis is in Appendix C.1.

## Appendix C Additional Experimental Results

### Hyper-parameter Sensitivity Analysis

Take the dataset of covariate-shift split of GOODHIV-Scaffold as an example, we conduct extensive experiments to investigate the hyper-parameter sensitivity, and the results are shown in Figure 5. We observe that the performance tends to improve first and then decrease slightly as the size of the codebook \(|\mathcal{C}|\) increases. This is because a small codebook limits the expressivity of the model, while too large one cuts the advantage of the discrete space. The effect of the threshold \(\gamma\) is insignificant and there is no remarkable trend. As the \(\lambda_{1}\), \(\lambda_{2}\) and \(\lambda_{3}\) increase, the performance shows a tendency to increase first and then decrease, indicating that \(\mathcal{L}_{\text{inv}}\), \(\mathcal{L}_{\text{reg}}\) and \(\mathcal{L}_{\text{cnt}}\) are effective and can improve performance within a reasonable range. We also observe that the standard deviation of performance increases as \(\lambda_{1}\) increases, which may be due to the fact that too much weight on the self-supervised invariant learning objective may enhance or affect the performance. The standard deviation is the smallest when \(\lambda_{2}=0\), suggesting that neural networks may have more stable outcomes by learning adaptively when there are no constraints, but it is difficult to obtain higher performance. While the

Figure 5: Hyper-parameter sensitivity analysis on the covariate-shift dataset of GOODHIV-Scaffold.

standard deviation is the largest when \(\lambda_{3}=0\), indicating that the commitment loss in VQ can increase the performance stability.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline  & & & \(\gamma\) & \(|\mathcal{C}|\) & batch-size & \(\lambda_{1}\) & \(\lambda_{2}\) & \(\lambda_{3}\) & dropout \\ \hline \multirow{6}{*}{GOOD} & \multirow{3}{*}{HIV} & scaffold & covariate & 0.8 & 4000 & 128 & 0.01 & 0.5 & 0.1 & - \\  & & concept & 0.7 & 4000 & 256 & 0.01 & 0.5 & 0.1 & - \\  & & size & covariate & 0.7 & 4000 & 256 & 0.01 & 0.5 & 0.1 & - \\  & & & concept & 0.9 & 4000 & 1024 & 0.01 & 0.5 & 0.1 & - \\ \cline{2-11}  & \multirow{3}{*}{ZINC} & \multirow{3}{*}{scaffold} & covariate & 0.3 & 4000 & 32 & 0.01 & 0.5 & 0.1 & - \\  & & & concept & 0.5 & 4000 & 256 & 0.01 & 0.5 & 0.1 & - \\  & & size & covariate & 0.3 & 4000 & 256 & 0.01 & 0.5 & 0.1 & - \\  & & & concept & 0.3 & 4000 & 64 & 0.0001 & 0.5 & 0.1 & - \\ \cline{2-11}  & \multirow{3}{*}{PCBA} & scaffold & covariate & 0.9 & 10000 & 32 & 0.0001 & 1 & 0.1 & - \\  & & concept & 0.9 & 10000 & 32 & 0.0001 & 1 & 0.1 & - \\  & & size & covariate & 0.9 & 10000 & 32 & 0.0001 & 1 & 0.1 & - \\ \hline \multirow{6}{*}{DrugOOD} & \multirow{3}{*}{IC50} & \multirow{3}{*}{IC50} & assay & 0.7 & 1000 & 128 & 0.001 & 0.5 & 0.1 & 0.5 \\  & & scaffold & 0.9 & 1000 & 128 & 0.0001 & 0.5 & 0.1 & 0.5 \\  & & size & 0.7 & 1000 & 128 & 0.01 & 0.5 & 0.1 & 0.1 \\ \cline{2-11}  & \multirow{3}{*}{EC50} & \multirow{3}{*}{scaffold} & assay & 0.7 & 500 & 128 & 0.01 & 0.5 & 0.1 & 0.5 \\ \cline{1-1}  & & & scaffold & 0.3 & 500 & 128 & 0.001 & 0.5 & 0.1 & 0.3 \\ \cline{1-1}  & & size & 0.7 & 500 & 128 & 0.001 & 0.5 & 0.1 & 0.1 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Hyper-parameter configuration.