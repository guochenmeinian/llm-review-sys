# GraphTrail: Translating Gnn Predictions into Human-Interpretable Logical Rules

 Burouj Armgaan, Manthan Dalmia

Department of Computer Science & Engineering

IIT Delhi, India

csz228001@iitd.ac.in,

manthandalmia2@gmail.com

&Sourav Medya

Department of Computer Science

University of Illinois, Chicago, USA

medya@uic.edu

&Sayan Ranu

Department of Computer Science & Engineering and Yardi School of AI

IIT Delhi, India

sayanranu@cse.iitd.ac.in

###### Abstract

Instance-level explanation of graph neural networks (Gnns) is a well-studied area. These explainers, however, only explain an instance (e.g., a graph) and fail to uncover the combinatorial reasoning learned by a Gnn from the training data towards making its predictions. In this work, we introduce GraphTrail, the first _end-to-end_, post-hoc, global Gnn explainer that translates the functioning of a black-box Gnn model to a boolean formula over the (sub)graph-level concepts without relying on local explainers. GraphTrail is unique in automatically mining the discriminative subgraph-level concepts using _Shapley values_. Subsequently, the Gnn predictions are mapped to a human-interpretable boolean formula over these concepts through _symbolic regression_. Extensive experiments across diverse datasets and Gnn architectures demonstrate significant improvement over existing global explainers in mapping Gnn predictions to faithful logical formulae. The robust and accurate performance of GraphTrail makes it invaluable for improving GNNs and facilitates adoption in domains with strict transparency requirements.

## 1 Introduction and Related Works

Gnns have witnessed widespread adoption for graph-level prediction tasks due to their impressive performance [35, 14, 46, 11, 41, 18, 38, 32, 5, 12, 53, 22]. Unfortunately, like other deep-learning models, Gnns are considered black boxes due to their lack of transparency and interpretability. This lack of interpretability presents a significant barrier to their adoption in critical domains such as healthcare, finance, and law enforcement. Additionally, the ability to explain predictions is crucial for understanding potential flaws in the model and generating insights for further refinement.

**Existing Works:** To introduce interpretability of Gnns, several algorithms have been proposed in the literature [23]. Fig. G in the Appendix, which was originally presented in [23], and now updated by us with more recent works, presents the taxonomy of Gnn explainability research. As observed, a vast majority of explainers focus on _instance-level_ explanations.

Instance-level (or local) explainers [55, 29, 42, 58, 15, 57, 27, 45, 25, 3, 1, 51, 47, 26, 6, 19] take a graph as input and identify components within this graph--such as a subgraph--that maximally influence the prediction made by a model. This instance-level focus limits its ability to extract patterns utilized by Gnns at a global level across a multitude of graphs and how these patterns are combined into a single decision-making rule. The objective of our work is to develop an end-to-endglobal explainer that _(1)_ mines the subgraph concepts used by a black-box Gnn model, and then _(2)_ uncovers the boolean logic used by the Gnn over these concepts to make its predictions.

Works on global Gnn explainers are limited [56, 16, 54, 2]. XGnn [56] and GnnInterpreter [50] are generative-modeling based global explainers. Both generate a graph that maximally aligns with a specified class label. While this generated graph likely contains important features used by the Gnn in making its predictions, it may not actually be present in the dataset, limiting its utility for analyzing specific predictions. Additionally, it does not produce a human-interpretable rule explaining the class attributions made by the Gnn. Finally, XGnn[56] requires domain-specific validity-rules as input, which affects its generalizability and results in inferior performance [2]. Another work [54] evaluates which base _concepts_ are detected by the neurons of a Gnn model during predictions and their relative importance. These concepts can be subgraphs or node-level properties, such as degrees. However, this method lacks the ability to automatically mine the concepts. More importantly, this method also does not generate a human-interpretable rule for the decision-making process of the Gnn.

The closest work to ours is GLGExplainer[2], which shares the objective of providing a global explanation of the Gnn through a boolean formula over subgraph-level concepts. However, there are significant limitations that need to be addressed:

* **Dependency on instance-level explainers:** GLGExplainer does not mine the subgraph concepts in an end-to-end manner. Instead, it relies on an instance-level explainer (e.g. PGExplainer[29]) to provide these concepts, over which it searches for the combinatorial formula mapping to the Gnn predictions. This dependency creates a disconnect with the objective, as instance-level explainers lack a global understanding of the model. Our proposed approach develops an end-to-end pipeline that mines concepts based on global trends.
* **Lack of interpretability due to vector-level concepts:** In GLGExplainer, each concept in the formula corresponds to a feature vector and not a subgraph. These vectors represent the embedding of a cluster of subgraphs generated by the instance explainer. Hence, in its original form, the formula is not human-interpretable. To convert into a human-interpretable formula, GLGExplainer randomly selects a subgraph from the cluster, assuming all subgraphs in a cluster are similar. Our investigation (SS 4) reveals that this assumption is rarely true in practice, compromising both interpretability and efficacy.
* **Lack of robustness:** GLGExplainer shows significant variation in the formula based on the training split used. As our analysis in SS 4 reveals, due to the reliance on instance-level explanations, even when data are drawn from the same distribution, the base concept candidates vary, and consequently so does the eventual formula.

At this juncture, we note that our work is distinct from the line of research on explainable Gnns [61, 10, 34]. Explainable Gnns are designed to make explainable predictions rather than explaining the predictions of a black-box Gnn.

**Contributions:** In this work, we present an end-to-end, post-hoc, global Gnn explainer called GraphTrail (TRAnslating Gnn Prediction into human-Interpretable Logical Rules)1, which addresses the limitations outlined above. Specifically,

Footnote 1: Trail also implies a pathway of clues, which aligns with our problem

* **Problem formulation:** We formulate the problem of translating a message-passing Gnn model for graph classification into a human-interpretable logic formula over subgraph concepts. Unlike existing works, in our formulation, the concepts are not assumed to be an input generated through a decoupled algorithm.
* **Novel methodology:** We develop GraphTrail, which uses a mix of several innovative insights. First, GraphTrail exploits the fact that a message passing Gnn decomposes a graph into a set of computation trees. This enables GraphTrail to limit the exploration of concepts from an exponential subgraph search space to a linear space of computation trees. The global impact of computation trees is assessed using _Shapley values_, and then mapped to a boolean formula over concepts using _symbolic regression_.
* **Empirical analysis:** Extensive experiments across a diverse set of datasets, Gnn architectures and pooling function, demonstrate GraphTrail to significantly surpass existing global explainers in efficacy, human-interpretability, data efficiency, and robustness.

## 2 Preliminaries and Problem Formulation

**Definition 1** (Graph).: _A graph is defined as \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{X})\) over a node set \(\mathcal{V}\), edge set \(\mathcal{E}=\{(u,v)\mid u,v\in\mathcal{V}\}\) and a node feature matrix \(\mathbf{X}=\{\mathbf{x}_{v}\mid v\in\mathcal{V}\}\) where \(\mathbf{x}_{v}\in\mathbb{R}^{d}\) is the set of features characterizing each node._

Two graphs are termed identical if they are _isomorphic_ to each other.

**Definition 2** (Graph Isomorphism).: _Graph \(\mathcal{G}_{1}\) is isomorphic to graph \(\mathcal{G}_{2}\) (denoted as \(\mathcal{G}_{1}\cong\mathcal{G}_{2}\)) if there exists a bijection between their node sets that preserves the edge connectivity and node features. Specifically, \(\mathcal{G}_{1}\cong\mathcal{G}_{2}\iff\exists f:\mathcal{V}_{1}\rightarrow \mathcal{V}_{2}\) such that: \((1)\)\(f\) is a bijection, \((2)\)\(\mathbf{x}_{v}=\mathbf{x}_{f(v)},\) where \(\mathbf{x}_{v}\in\mathcal{V}_{1},\mathbf{x}_{f(v)}\in\mathcal{V}_{2}\) and \((3)\)\((u,v)\in\mathcal{E}_{1}\) if and only if \((f(u),f(v))\in\mathcal{E}_{2}\)._

Graph \(\mathcal{G}_{1}\) is _subgraph isomorphic_ to \(\mathcal{G}_{2}\), denoted as \(\mathcal{G}_{1}\subseteq\mathcal{G}_{2}\), if \(f\) is an _injection_ and condition (3) is modified to \((u,v)\in\mathcal{E}_{1}\) if \((f(u),f(v))\in\mathcal{E}_{2}\).

**Definition 3** (Graph Classification).: _In graph classification, we are given a set of train graphs \(\mathcal{D}_{tr}=\{\mathcal{G}_{1},\cdots,\mathcal{G}_{m}\}\), where each graph \(\mathcal{G}_{i}\) is tagged with a class label \(\mathcal{Y}_{i}\) from the set \(\{\mathcal{Y}_{1},\cdots,\mathcal{Y}_{c}\}\). The objective is to train a Gnn model \(\Phi\) such that given a graph with an unknown class label, the label prediction error is minimized._

Error may be measured using any of the known metrics such as cross-entropy loss, negative log-likelihood, etc. Hereon, we implicitly assume \(\Phi\) to be a _message-passing_Gnn[22, 14, 46, 53]. We assume the Gnn\(\Phi\) returns a \(c\)-dimensional distribution over the class labels, where \(\Phi(\mathcal{G})_{j}\) is the probability of the \(j\)-th class, and \(\Phi(\mathcal{G})^{*}=\arg\max_{j\in\{1,2,\cdots,c\}}\{\Phi(\mathcal{G})_{j}\}\) denotes the predicted class label.

**Definition 4** (Concepts [2, 21]).: _Concepts refer to semantically meaningful units of information within the data that humans use to analyze and make decisions about that dataset. In the context of graph classification, these concepts correspond to subgraphs._

**Problem 1** (Global Gnn explanation through boolean logic over concepts).: _Let \(\mathcal{D}\) be a set of graphs, where each graph is labeled with a class from the set \(\{\mathcal{Y}_{1},\cdots,\mathcal{Y}_{c}\}\). Given a trained Gnn\(\Phi\), our objective is to learn a set of \(c\) boolean formulas \(\{f_{1},\cdots,f_{c}\}\), over subgraph-level concepts such that for any graph \(\mathcal{G}\), if \(\Phi(\mathcal{G})^{*}=\mathcal{Y}_{i}\), then \(f_{i}(\mathcal{G})=\)TRUE and \(\forall j\neq i,\ f_{j}(\mathcal{G})=\)FALSE. The candidate space of concepts includes all unique subgraphs of the dataset, i.e., \(\mathcal{C}=\{\mathcal{S}\mid\mathcal{S}\subseteq\mathcal{G},\mathcal{G}\in \mathcal{D}\}\)._

The proposed problem formulation surfaces two key challenges:

1. _How do we extract the concepts?_ Concept extraction poses a significant challenge due to the exponential size of the candidate space. In the worst case, a graph with \(n\) nodes can have \(2^{n}\) possible subgraphs. Furthermore, if there are \(n\) subgraphs in the dataset, identifying the _unique_ subgraphs requires performing \(\mathcal{O}(n^{2})\) graph isomorphism tests. The computation cost of graph isomorphism grows exponentially with graph size.
2. _How do we uncover the boolean logic over the concepts?_ The number of boolean formulas associated with a set of symbols (concepts) increases exponentially with the size of the set. Hence, the scalability challenge is further exacerbated.

## 3 GraphTrail: Proposed Global Explainer

Fig. 1 presents the pipeline of GraphTrail. GraphTrail builds on the observation that any message passing Gnn decomposes a graph of \(n\) nodes into \(n\)_computation trees_. Consequently, only the subgraphs corresponding to these computation trees are processed by the Gnn and the rest of the subgraphs are irrelevant to the Gnn's predictions. This property allows us to reduce the candidate space size from exponential to linear. Subsequently, the impact of each of the computation trees is assessed through its _Shapley Value_[43], and the top-\(k\) trees are sent to the logic formulator. The Boolean logic is revealed by _the symbolic regression_ over these computation trees. Symbolic regression aims to discover concise closed-form mathematical equations that best fit a given set of data [24, 31]. In our context, we constrain the regressor over computation trees with boolean operators and fit to the predictions of Gnn. We next elaborate on each of these steps.

### Computation Framework of Message-passing GNNs

Gnns aggregate messages layer by layer. If \(\mathbf{x}_{v}\in\mathbb{R}^{|F|}\) is the input feature vector for node \(v\in\mathcal{V}\), then the \(0^{th}\) layer representation of node \(v\) is set to \(\mathbf{h}^{0}_{v}=\mathbf{x}_{v}\). In each of the subsequent layers \(\ell\)

[MISSING_PAGE_EMPTY:4]

**Lemma 1**.: _In an \(L\)-layered Gnn, the computation tree \(\mathcal{T}^{L}_{v}\) is sufficient to compute node embedding \(\mathbf{h}^{L}_{v}\), \(\forall v\in\mathcal{V}\)._

Proof.: In a single layer of message-passing, a node \(v\) receives messages from each of its immediate neighbors. Over \(L\) layers, \(v\) collects messages from nodes within \(L\) hops. The computation tree \(\mathcal{T}^{L}_{v}\) contains all such paths of length up to \(L\) and hence is sufficient to compute \(\mathbf{h}^{L}_{v}\). 

**Lemma 2**.: \(\mathcal{T}^{L}_{v}\cong\mathcal{T}^{L}_{u}\implies\mathbf{h}^{L}_{v}=\mathbf{ h}^{L}_{u}\)_._

Proof.: The expressive power of a message-passing Gnn is upper bounded by the _Weisfeiler-Lehman test (1-WL)_[53]. This implies that if the \(L\)-hop neighborhoods of two nodes are indistinguishable by 1-WL, then their representations will be the same. The 1-WL test cannot differentiate between nodes with identical computation trees [44]. 

Owing to Lemma 1 and Lemma 2, given a graph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{X})\), we can decompose it into a multiset of computation trees \(\mathcal{T}_{\mathcal{G}}=\{\mathcal{T}^{L}_{v}\mid v\in\mathcal{V}\}\), compute node embeddings \(\mathbf{h}^{L}_{v},\ \ \forall v\in\mathcal{V}\), and then aggregate them into graph embedding \(\mathbf{h}_{\mathcal{G}}\) (Eq. 4) without incurring any loss of information. This computation structure enables us to shrink the candidate space of concepts, originally defined in Prob. 1, to the set of unique computation trees, i.e., \(\mathcal{C}=\bigcup_{\mathcal{G}\in\mathcal{D}}\mathcal{T}_{\mathcal{G}}\). The reformulation of the concept space imparts several desirable side-effects.

* First, the size of the candidate space of concepts reduces from exponential to \(\sum_{\forall\mathcal{G}\in\mathcal{D}}|\mathcal{V}|\), which is linear to the graph size (i.e., number of nodes) and the number of graphs in the dataset.
* Second, while graph isomorphism has a computational cost exponential to the graph size, rooted-tree isomorphism can be performed in time linear to the number of edges in the tree (See App. A for details). Therefore, distilling all computation trees to only the unique ones can be done in linear time.
* Third, Gnn may map non-isomorphic \(L\)-hop node neighborhoods (subgraphs) to isomorphic computation trees (See Fig. 2). This further reduces the number of concept candidates and, consequently, the computational burden.

### Mining Concepts

To mine concepts from the candidate space, we assess their _Shapley values_. Shapley value [43] is a co-operative game theoretic technique where players form coalitions and receive payouts based on their contribution to the outcome. Mathematically, the Shapley value \(\psi_{i}(\Phi)\) of player \(i\) is its average marginal contribution across all possible combinations of features.

\[\psi_{i}(V)=\sum_{S\subseteq\{1,\cdots p\}\setminus\{i\}}\frac{|S|!\ (p-|S|-1)!}{p!}(V(S\cup\{i\})-V(S)) \tag{5}\]

Here, \(p\) is the total number of players, \(S\) represents a subset of players excluding player \(i\) and the _value function_\(V(.)\) quantifies the outcome.

With the hypotheses that some combination of computation trees is responsible for Gnn's predictions, we aim to leverage Shapley values in identifying this combination. Consequently, the players correspond to computation trees, and the value function corresponds to the performance of the Gnn when restricted to the chosen subset of computation trees. We set the value function as the _negated cross-entropy error_ of Gnn \(\Phi\)'s prediction (one may use other accuracy measures as well). Thus, Eq. 5 is re-expressed as:

Figure 2: The figure illustrates the process of constructing the computation trees of nodes \(v_{1}\in\mathcal{G}_{1}\) and \(u_{1}\in\mathcal{G}_{2}\) for \(L=2\). The colors of the nodes represent the node labels. Note that although \(v_{1}\) and \(u_{1}\) are embedded in non-isomorphic \(L\)-hop neighborhoods, their computation trees are isomorphic.

\[\psi_{i}(\Phi) =\sum_{\forall S,S\subseteq\mathcal{C}\setminus\{C_{i}\}}\frac{|S|! \ (|\mathcal{C}|-|S|-1)!}{|\mathcal{C}|!}\left(V(S\cup\{C_{i}\})-V(S)\right) \tag{6}\] \[\text{where, }V(S) =\sum_{\forall\mathcal{G}\in\mathcal{D}}\sum_{j=1}^{c}\mathcal{Y }_{j}^{\mathcal{G}}\log\left(\Phi\left(\mathcal{G}^{S}\right)_{j}\right) \tag{7}\]

In Eq. 6, \(\mathcal{C}=\{C_{1},\cdots,C_{m}\}\) is the set of unique computation trees, and \(C_{i}\) denotes the \(i\)-th one. Eq. 7 represents the negated cross-entropy error. In Eq. 7, we slightly overload the notation introduced in Def. 3 as follows: we assume that the class label information is maintained as a \(c\)-dimensional one-hot vector. Thus, \(\mathcal{Y}_{j}^{\mathcal{G}}\) is \(1\) if the true class label of \(\mathcal{G}\) is \(j\)-th label. The Gnn\(\Phi\) returns a distribution over the class labels, where \(\Phi(\mathcal{G}^{S})_{j}\) is the probability of the \(j\)-th class, when \(\mathcal{G}\) is embedded by only considering the computation trees within \(S\). Mathematically, this means modifying Eq. 4 as follows:

\[\mathbf{h}_{\mathcal{G}}^{S}=\textsc{Pool}\left(\left\{\mathbf{h}_{v}^{L}\ | \ \forall v\in\mathcal{V}\text{ and }\mathcal{T}_{v}^{L}\in S\right\}\right) \tag{8}\]

Finally, we select the top-\(k\) computation trees with the highest Shapley value as concepts. We discuss our procedure for selecting \(k\) in the next section (SS 3.4).

#### 3.3.1 Computational Tractability

While we are able to map the problem of concept mining to Shapley Value computations, several computational challenges arise. Firstly, Shapley value computation necessitates sampling all possible subsets of players (computation trees), leading to exponential computation complexity. Secondly, we need to compute \(\mathbf{h}_{\mathcal{G}}^{S}\), i.e., the graph embedding corresponding to each subset of trees \(S\). The naive approach involves: **(1)** enumerating computation trees rooted in each node, which consumes \(\mathcal{O}(|\mathcal{E}|)\) time, **(2)** performing \(\mathcal{O}(|S||\mathcal{V}|)\) tree isomorphisms to detect computation trees contained within \(S\) and then **(3)** computing Eq. 8. Given that Shapley computation requires drawing multiple samples of \(S\) (Eq. 6), the above pipeline for each sample \(S\) is prohibitively slow.

Fortunately, there are works to circumvent the first challenge via sampling a small set of coalitions with probabilistic approximations [59, 33]. We use the sampling strategy outlined in [28] (see App. B for details).

The second challenge is unique since computing Shapley on computation trees of Gnns has not been studied. GraphShap [36], the only other work to study Shapley value in the context of explaining graph classification, looks at the specific class of identity-aware graphs, where nodes with unique identities recur across multiple graph views. More importantly, GraphShap is not customized for Gnns and hence the context of computation tree does not arise.

To address the bottleneck of computing Eq. 8, we project each graph into a _concept vector_.

**Definition 6** (Concept vector).: _Let \(\mathcal{C}=\{C_{1},\cdots,C_{m}\}\) be the set of unique computation trees. The concept vector \(\mathcal{C}_{\mathcal{G}}\in\mathbb{Z}_{\geq 0}^{m}\) of graph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{X})\) is an \(m\)-dimensional vector where the \(i\)-th dimension represents the number of computation trees in \(\mathcal{G}\) that are isomorphic to the \(i\)-th concept candidate \(C_{i}\in\mathcal{C}\). Mathematically, \(\mathbf{C}_{\mathcal{G}}[i]=|\{v\in\mathcal{V}\ |\ \mathcal{T}_{v}^{L}\cong C_{i}\}|\)._

The "concept encoding" box in Fig. 1 illustrates this operation. From Lemma 1, the embedding of the root node of \(C_{i}\) depends solely on \(C_{i}\). Hence, we pre-compute and map each \(C_{i}\in\mathcal{C}\) into an embedding \(\mathbf{h}_{i}\). By combining Lemma 2 with Eq.4, we deduce that for both MeanPool and SumPool, the two most common aggregators can be computed in \(O(|S|)\) time. Specifically,

\[\text{For SumPool, }\ \ \mathbf{h}_{\mathcal{G}}^{S} =\sum_{C_{i}\in S}\mathbf{C}_{\mathcal{G}}[i]\times\mathbf{h}_{i} \tag{9}\] \[\text{For MeanPool, }\ \ \mathbf{h}_{\mathcal{G}}^{S} =\frac{1}{|\mathcal{V}|}\sum_{C_{i}\in S}\mathbf{C}_{\mathcal{G}}[i] \times\mathbf{h}_{i} \tag{10}\]

Owing to Lemma 1 and Lemma 2, computing the graph embedding simply involves \(|S|\) look-ups of vectors instead of fresh rounds of message-passing.

### Generation of Logical Rules through Symbolic Regression

Let us denote the top-\(k\) computation trees with the \(k\) highest Shapley values as \(\mathcal{C}^{*}\). To discover logical rules over \(\mathcal{C}^{*}\), we perform _symbolic regression_. Symbolic regression is a branch of regression analysis that aims to discover symbolic expressions from experimental data [31]. Formally, given a set of input-output pairs \(\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}\), where \(\mathbf{x}_{i}\in\mathbb{R}^{d}\) is an input vector, and \(y_{i}\in\mathbb{R}\) is the output value (or label), and a set of operators, such as addition, subtraction, logarithm, trigonometric functions, boolean logic, etc., the goal is to find a symbolic equation \(e\) and corresponding function \(f_{e}\) such that \(\forall i,\ y_{i}\approx f_{e}(\mathbf{x}_{i})\).

To perform symbolic regression over computation trees, we first project the concept vector of each graph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{X})\) on the top-\(k\) trees \(\mathcal{C}^{*}\). This distilled concept vector is denoted as \(\mathbf{C}^{*}_{\mathcal{G}}\in\mathbb{Z}_{>0}^{k}\), where \(\mathbf{C}^{*}_{\mathcal{G}}[i]=|\{v\in\mathcal{V}\mid\mathcal{T}^{L}_{v}\cong C _{i}\}|\), with \(C_{i}\) representing the \(i^{\text{th}}\) ranked computation tree in \(\mathcal{C}^{*}\) based on Shapley value. While symbolic regression can accommodate a wide array of operators, mathematical operations such as addition or multiplication are not meaningful when applied to computation trees. Hence, to facilitate human interpretability, we restrict to the boolean operators: conjunction (\(\wedge\)), disjunction (\(\vee\)), negation (\(\lnot\)), and XOR (\(\oplus\)). We aim to learn a symbolic function \(f_{c}\) for each class label \(c\) predicted by the Gnn. Let \(\mathcal{D}_{c}=\{\mathcal{G}\in\mathcal{D}\mid\Phi(\mathcal{G})^{*}=c\}\) be the subset of graphs where the predicted label is \(c\). The symbolic function \(f_{c}\) is identified by minimizing a multi-objective loss function that balances prediction error and model complexity. Formally,

\[\mathcal{L}\left(f_{c}\right)=Error(f_{c})+(\alpha)Complexity(f_{c}) \tag{11}\]

\(Complexity(f_{c})\) corresponds to the number of boolean operators and variables in \(f_{c}\). \(\alpha\) is a weighting hyper-parameter. Error corresponds to the number of disagreements with the Gnn. Formally,

\[Error\left(f_{c}\right)=\sum_{\forall\mathcal{G}\in\mathcal{D}_{c}}1-f_{c}( \mathcal{G})+\sum_{\forall\mathcal{G}^{\prime}\in\mathcal{D}\setminus\mathcal{ D}_{e}}f_{c}(\mathcal{G}) \tag{12}\]

Since \(f_{c}\) is a boolean function, it returns either TRUE (equivalently \(1\)) or FALSE (\(0\)).

Symbolic regression poses a combinatorial optimization challenge, as the number of potential functions increases exponentially with the number of symbols, which is \(k\) in our case. Fortunately, the area is rich with several studies [31]. We use [8], which leverages a multi-population evolutionary algorithm to efficiently optimize and identify symbolic expressions (See App. C for details).

As depicted in Fig. 1, following the generation of boolean functions, GraphTrail concludes.

**Identifying \(k\) - the number of concepts:** The higher the value of \(k\), the more expressive symbolic regression is to fit to the prediction data by Gnn. On the other hand, the computation cost grows monotonically with \(k\). To optimize this balance, we start with a small value of \(k\), and incrementally increase it until the accuracy plateaus, similar to using the patience parameter to determine the number of epochs in machine learning model training.

## 4 Experiments

In this section, we evaluate GraphTrail and benchmark its performance in translating predictions of various Gnn architectures across diverse datasets and pooling layers. More experiments can be found in the appendix. The codebase of GraphTrail is shared at [https://github.com/idea-iitd/GraphTrail](https://github.com/idea-iitd/GraphTrail).

### Experimental Setup

**Baselines:** As discussed in SS 1, GLGExplainer[2] is the only existing algorithm that fits a logical formula to Gnn predictions, making it our primary baseline. Recall, GLGExplainer generates a formula over vectors representing embeddings of subgraph clusters rather than individual subgraphs, rendering it non-interpretable. The formula is applied to an input graph by first processing it through an instance-level explainer, embedding the explanation subgraphs into feature space, and assigning them to the closest cluster. These cluster vectors are then considered present in the input graph. To make the formula interpretable, each cluster vector is replaced by the subgraph closest to the cluster representation in the embedding space. A subgraph (concept) \(\mathcal{G}_{S}\) in the formula is considered present in an input graph \(\mathcal{G}\) if \(\mathcal{G}_{S}\subseteq\mathcal{G}\). We term this version GLGExplainer-iso.

We use the authors' implementation of GLGExplainer with the recommended parameters and employ PGExplainer[29] as the instance-level explainer, as suggested. The implementation of PGExplainer is available in Pytorch Geometric.

**GraphTrail-S:** Shapley value computation brings the computational bottleneck in the pipeline, which involves calculating the global Shapley value for each computation tree on each graph. This process can become impractical for datasets with a large number of graphs. To address this issue, we propose GraphTrail-S, an efficient version of GraphTrail. Note that we are interested in the ranking according to the Shapley values and not the values themselves. If a subset of the dataset can emulate the entire dataset, the generated ranking is expected to remain the same. Thus, GraphTrail-S creates a stratified split of the training dataset that maintains the class ratios and computes the global Shapley values using this subset. The rest of the pipeline remains unchanged, allowing for a more efficient yet effective method. This is also shown in the results from Table 1 where GraphTrail-s is slightly inferior to GraphTrail.

**Datasets:** We use four benchmark datasets listed in Table C (App. D). While NCI1 [49], MUTAG [17], and Mutagenicity [39, 20] are collections of molecules, BAMultiShapes [55] is a synthetic dataset specifically curated to benchmark global Gnn explainers and associated with ground truth logical explanations. Further details are presented in App. D.

**Evaluation framework:** While we benchmark against various Gnn architectures and Pool layers (Eq. 4), the default architecture is set to Gat for MUTAG and Mutagenicity and Gin for the other two. We use SumPool as the default across datasets. The accuracies of the Gnn architectures across all Pool choices are presented in Table D in the Appendix. Details on the hardware platform, train-val-test splits and other parameters are provided in App. E. All experiments have been repeated with three seeds and we report the means and standard deviations.

**Metrics:** We evaluate explainer faithfulness using _Fidelity_; the ratio of graphs where the logical formula matches the Gnn class label. Additional metrics can be found in the appendix.

### Fidelity

Is GraphTrail faithful in translating Gnn through logical rules? Table 1 provides the answer with several key observations.

First, both GraphTrail and GraphTrail-s significantly outperform GLGExplainer. GraphTrail computes Shapley values by evaluating all subsets of concepts, while GraphTrail-s employs a faster, sampling-based method. The key reason for the superior performance of GraphTrail and GraphTrail-s over GLGExplainer lies in aligning the concept definitions with the fundamental computational units of a Gnn, namely computational trees. GraphTrail then mines these concepts with a global perspective by analyzing their Shapley values. In contrast, GLGExplainer depends on instance-level explainers, which lack the ability to detect global patterns.

Second, GLGExplainer-iso consistently underperforms compared to GLGExplainer. This is because the clusters of instance-level subgraph explanations in GLGExplainer-iso are not _pure_ (See App. I). Although they are grouped based on embedding distance, their structures vary significantly. Representing a cluster by an exemplar subgraph fails to capture the diversity effectively.

Finally, GLGExplainer results do not match the original results reported in [2]. In App. H, we discuss the causes, including the identification of test data leakage in the Mutagenicity data set.

### Robustness

_Varying architectures and pooling mechanisms._ In Table 2, we present the robustness of GraphTrail and GLGExplainer across various Gnn architectures and Pool layers. GraphTrail achieves

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & BAMultiShapes & MUTAG & Mutagenicity & NCI1 \\ \hline GLG & 0.48 \(\pm\) 0.02 & 0.74 \(\pm\) 0.10 & 0.62 \(\pm\) 0.03 & 0.57 \(\pm\) 0.04 \\ GLG-iso & 0.00 \(\pm\) 0.00 & 0.61 \(\pm\) 0.20 & 0.53 \(\pm\) 0.06 & 0.49 \(\pm\) 0.05 \\ G-Trail-S & 0.86 \(\pm\) 0.01 & 0.78 \(\pm\) 0.08 & 0.72 \(\pm\) 0.01 & 0.72 \(\pm\) 0.02 \\ G-Trail & **0.87 \(\pm\) 0.02** & **0.82 \(\pm\) 0.09** & **0.72 \(\pm\) 0.01** & **0.72 \(\pm\) 0.02** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Average fidelity of the formulae across three seeds. The best and second best results are shown in bold and underlined, respectively. G-Trail and GLG represents GraphTrail and GLGExplainer respectively.

\begin{table}
\begin{tabular}{l|c c c c c|c c c c c c} \hline \hline  & \multicolumn{4}{c|}{Gnn Architecture} & \multicolumn{4}{c}{Pool.} \\  & GCN & GAT & Gin & SUM & MEAN & MAX \\ \hline  & G-Trail & GLG & G-Trail & GLG & G-Trail & GLG & G-Trail & GLG & G-Trail & GLG & G-Trail & GLG \\ \hline BAMultiShapes & **0.81** & 0.49 & **1.00** & 0.93 & **0.87** & 0.48 & **0.87** & 0.48 & **0.88** & 0.50 & **0.89** & 0.52 \\ MUTAG & **0.87** & 0.80 & **0.82** & 0.74 & **0.87** & 0.55 & **0.82** & 0.74 & **0.77** & 0.54 & **0.78** & 0.74 \\ Mutagenicity & **0.72** & 0.63 & **0.72** & 0.62 & **0.73** & 0.64 & **0.72** & 0.62 & **0.70** & 0.62 & **0.69** & 0.68 \\ NCI1 & **0.75** & 0.59 & **0.73** & 0.65 & **0.72** & 0.57 & **0.72** & 0.57 & **0.70** & 0.59 & **0.66** & 0.62 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of Fidelity across Gnn architectures and Pool layers. The best result in each dataset and category is highlighted in bold.

higher fidelity than GLGExplainer across all scenarios. This result further substantiates the superior ability of GraphTrail in faithfully translating Gnn outcomes.

_Varying \(k\)._ Fig. 3 explores the relationship between the number of mined concepts (\(k\)) and the fidelity achieved by GraphTrail across various datasets. Recall that higher values of \(k\) allow for greater expressivity in symbolic regression. Consistent with this, the results exhibit an increasing trend, with fidelity approaching saturation at \(k=50\) for most datasets.

### Data Efficiency

In Fig. 4, we assess the explainers' effectiveness in low-data regimes by varying the train data volume on the \(x\)-axis and measuring Fidelity on the \(y\)-axis. We expect Fidelity to decrease with lesser data. In GraphTrail, the decrease in Fidelity with lesser data is minor, indicating robust efficacy even with limited data and that it can be used in low-resource settings. Notably, the higher average Fidelity and lower standard deviation in GraphTrail across sizes indicate enhanced stability.

### Visual Analysis of Rules

Fig. 5 presents the rules inferred by GraphTrail and GLGExplainer for the datasets MUTAG, Mutagenicity, and BAMultiShapes.

**MUTAG**: According to [9], compounds with rings and electron-attracting group elements conjugated with nitro groups enhance mutagenicity. GraphTrail identifies both the ring structures as well as the nitro group as identifiers of mutagenicity in this dataset. In contrast, GLGExplainer fails to identify the ring structure.

**Mutagenicity:** Prior work [55; 45; 29; 2] often limits the mutagenic behavior in this dataset to \(NO_{2}\) and \(NH_{2}\), i.e., the nitro and amine groups. However, in [20], eight general toxicophores are capable of identifying 75% of all mutagens in the dataset (see Fig. F [20]). As seen in Fig. 5, without any supervision, GraphTrail identifies and incorporates six of them in its formula, viz. aromatic nitro, aromatic amine, nitroso, unsubstituted heteroatom-bonded heteroatom, azo-type, and polycyclic aromatic systems. The azo-type and the unsubstituted heteroatom-bonded heteroatom groups have not been identified explicitly but are potential extensions of two of the identified structures.

**BAMultiShapes**: BAMultiShapes has known ground truth logical rules for its class labels. While GraphTrail does not perfectly recover these rules, GraphTrail's inferred concepts and their logical combination exhibit greater similarity to the ground truth compared to GLGExplainer's results. It should be noted that explainers try to find what the model has learned rather than the ground truth. One striking observation is that GLGExplainer's extracted concepts bear no similarity to the ground truth and two of the prototypes have no graphs in their clusters and are therefore empty. In contrast, GraphTrail identifies concepts that are isomorphic to the grid and house motifs present in the ground truth.

## 5 Conclusions, Limitations and Future Directions

In this work, we have designed an end-to-end, post-hoc, global graph neural network (Gnn) explainer called GraphTrail. We have formulated the problem of translating a message-passing Gnn model

Figure 4: Results on the test set fidelity averaged over three seeds while varying the training data size.

Figure 3: Impact of \(k\) (number of concepts) on Fidelity.

for graph classification into a human-interpretable logic formula over subgraph concepts without assuming the concepts are pre-generated through a separate algorithm. GraphTrail leverages several novel insights such as the decomposition of graphs into computation trees by message-passing Gnns, which reduces the search space for concepts from exponential number of all possible subgraphs to a linear space of computation trees. GraphTrail evaluates the impact of these computation trees using Shapley values and then creates a boolean formula via symbolic regression only with the important ones. Extensive empirical analyses across diverse datasets and different Gnn architectures demonstrate GraphTrail's effectiveness in capturing the underlying logical relationships within complex Gnn models for a wide range of scenarios in a human-interpretable form.

**Limitations:** While both GraphTrail and GLGExplainer express rules as logical combinations of concept's presence or absence, they currently lack the ability to capture the multiplicity of a concept's occurrence within a graph. Finally, we believe attempting to interpret Gnns with subgraph level concepts creates a dilemma between interpretability and faithfulness to the computational mechanism of Gnns. To elaborate, in GraphTrail, since symbolic regression generates the formula over computation trees, we map it back to the graph space, by replacing each computation tree with the most frequent \(L\)-hop ego graph that generates this tree. Note, Gnns operate at the level of computation trees. Given any formula at a subgraph level, a new formula can be generated with identical fidelity, by simply replacing each subgraph concept with one that produces the same set of computation trees (recall Fig. 2). However, when concepts are lowered to computation tree level, human interpretability is compromised. It is worth exploring how the best balance between human interpretability [13] and staying faithful to Gnn computation structure can be obtained.

Figure 5: Visual inspection of the formulae generated by GraphTrail and GLGExplainer. The encircled numbers in Mutagenicity are toxicophore IDs from Fig. F (in Appendix). The structures with two IDs in Mutagenicity can be extended to both toxicophores.

Acknowledgements

Burouj Armgaan acknowledges the support of the Prime Minister's Research Fellowship (PMRF) for funding this research.

## References

* [1] Carlo Abrate and Francesco Bonchi. Counterfactual graphs for explainable classification of brain networks. In _KDD_, page 2495-2504, 2021. (Cited on p. 1 \(\longleftrightarrow\))
* [2] Steve Azzolin, Antonio Longa, Pietro Barbiero, Pietro Lio, and Andrea Passerini. Global explainability of GNNs via logic combination of learned concepts. In _The Eleventh International Conference on Learning Representations_, 2023. (Cited on pp. 2, 3, 7, 8, 9, 15, and 19 \(\longleftrightarrow\))
* [3] Mohit Bajaj, Lingyang Chu, Zi Yu Xue, Jian Pei, Lanjun Wang, Peter Cho-Ho Lam, and Yong Zhang. Robust counterfactual explanations on graph neural networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. (Cited on pp. 1 **and 15 \(\longleftrightarrow\))**
* [4] Federico Baldassarre and Hossein Azizpour. Explainability techniques for graph convolutional networks. _arXiv preprint arXiv:1905.13686_, 2019. (Cited on p. 15 \(\longleftrightarrow\))
* [5] Vaibhav Bihani, Sahil Manchanda, Srikanth Sastry, Sayan Ranu, and NM Krishnan. Stridernet: A graph reinforcement learning approach to optimize atomic structures on rough energy landscapes. In _ICML_, 2023. (Cited on p. 1 \(\longleftrightarrow\))
* [6] Chirag Chhablani, Sarthak Jain, Akshay Channesh, Ian A Kash, and Sourav Medya. Game-theoretic counterfactual explanation for graph neural networks. In _Proceedings of the ACM on Web Conference 2024_, pages 503-514, 2024. (Cited on p. 1 \(\longleftrightarrow\))
* [7] Yun Chi, Yirong Yang, and Richard Muntz. Canonical forms for labelled trees and their applications in frequent subtree mining. _Knowl. Inf. Syst._, 8:203-234, 08 2005. (Cited on p. 15 \(\longleftrightarrow\))
* [8] Miles Cranmer. Interpretable machine learning for science with pysr and symbolicregression. jl. _arXiv preprint arXiv:2305.01582_, 2023. (Cited on pp. 7 **and 17 \(\longleftrightarrow\))**
* [9] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. _Journal of medicinal chemistry_, 34(2):786-797, 1991. (Cited on pp. 9 **and 15 \(\longleftrightarrow\))**
* [10] Dobrik Georgiev, Pietro Barbiero, Dmitry Kazhdan, Petar Velickovic, and Pietro Lio. Algorithmic concept-based explainable reasoning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 6685-6693, 2022. (Cited on p. 2 \(\longleftrightarrow\))
* [11] Mridul Gupta, Hariprasad Kodamana, and Sayan Ranu. FRIGATE: Frugal spatio-temporal forecasting on road networks. In _29th SIGKDD Conference on Knowledge Discovery and Data Mining_, 2023. (Cited on p. 1 \(\longleftrightarrow\))
* [12] Shubham Gupta, Sahil Manchanda, Srikanta Bedathur, and Sayan Ranu. Tigger: Scalable generative modelling for temporal interaction graphs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 6819-6828, 2022. (Cited on p. 1 \(\longleftrightarrow\))
* [13] Pantea Habibi, Peyman Baghershahi, Sourav Medya, and Debaleena Chattopadhyay. Design requirements for human-centered graph neural network explanations. _arXiv preprint arXiv:2405.06917_, 2024. (Cited on p. 10 \(\longleftrightarrow\))
* [14] William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, NIPS'17, page 1025-1035, Red Hook, NY, USA, 2017. Curran Associates Inc. (Cited on pp. 1 **and 3 \(\longleftrightarrow\))*** [15] Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, and Yi Chang. Graphlime: Local interpretable model explanations for graph neural networks. _IEEE Transactions on Knowledge and Data Engineering_, 2022. (Cited on pp. **1 and 15 \(\leftrightarrow\)**)
* [16] Zexi Huang, Mert Kosan, Sourav Medya, Sayan Ranu, and Ambuj Singh. Global counterfactual explainer for graph neural networks. In _Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining_, pages 141-149, 2023. (Cited on pp. **2 and 15 \(\leftrightarrow\)**)
* [17] Sergei Ivanov, Sergei Sviridov, and Evgeny Burnaev. Understanding isomorphism bias in graph data sets. _Geometric Learning and Graph Representations ICLR Workshop_, 2019. (Cited on pp. **8 and 17 \(\leftrightarrow\)**)
* [18] Jayant Jain, Vritika Bagadia, Sahil Manchanda, and Sayan Ranu. Neuromlr: Robust & reliable route recommendation on road networks. _Advances in Neural Information Processing Systems_, 34:22070-22082, 2021. (Cited on p. **1 \(\leftrightarrow\)**)
* [19] Jaykumar Kakkad, Jaspal Jannu, Kartik Sharma, Charu Aggarwal, and Sourav Medya. A survey on explainability of graph neural networks. _arXiv preprint arXiv:2306.01958_, 2023. (Cited on p. **1 \(\leftrightarrow\)**)
* [20] Jeroen Kazius, Ross McGuire, and Roberta Bursi. Derivation and validation of toxicophores for mutagenicity prediction. _Journal of medicinal chemistry_, 48(1):312-320, 2005. (Cited on pp. **8, 9, and 17 \(\leftrightarrow\)**)
* [21] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In _International conference on machine learning_, pages 2668-2677. PMLR, 2018. (Cited on p. **3 \(\leftrightarrow\)**)
* [22] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016. (Cited on pp. **1 and 3 \(\leftrightarrow\)**)
* [23] Mert Kosan, Samidha Verma, Burouj Armgaan, Khushbu Pahwa, Ambuj Singh, Sourav Medya, and Sayan Ranu. GNNX-BENCH: Unravelling the utility of perturbation-based GNN explainers through in-depth benchmarking. In _The Twelfth International Conference on Learning Representations_, 2024. (Cited on p. **1 \(\leftrightarrow\)**)
* [24] John R. Koza. _Genetic programming: on the programming of computers by means of natural selection_. MIT Press, Cambridge, MA, USA, 1992. (Cited on p. **3 \(\leftrightarrow\)**)
* [25] Wanyu Lin, Hao Lan, and Baochun Li. Generative causal explanations for graph neural networks. In _International Conference on Machine Learning_, pages 6666-6679. PMLR, 2021. (Cited on pp. **1 and 15 \(\leftrightarrow\)**)
* [26] Shengyao Lu, Keith G Mills, Jiao He, Bang Liu, and Di Niu. GOAt: Explaining graph neural networks via graph output attribution. In _The Twelfth International Conference on Learning Representations_, 2024. (Cited on pp. **1 and 15 \(\leftrightarrow\)**)
* [27] Ana Lucic, Maartye A Ter Hoeve, Gabriele Tolomei, Maarten De Rijke, and Fabrizio Silvestri. Cf-gnnexplainer: Counterfactual explanations for graph neural networks. In _AISTATS_, pages 4499-4511, 2022. (Cited on pp. **1 and 15 \(\leftrightarrow\)**)
* [28] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. _Advances in neural information processing systems_, 30, 2017. (Cited on pp. **6 and 16 \(\leftrightarrow\)**)
* [29] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Parameterized explainer for graph neural network. _Advances in neural information processing systems_, 33:19620-19631, 2020. (Cited on pp. **1, 2, 7, 9, 15, and 19 \(\leftrightarrow\)**)
* [30] Jing Ma, Ruocheng Guo, Saumitra Mishra, Aidong Zhang, and Jundong Li. Clear: Generative counterfactual explanations on graphs. _arXiv preprint arXiv:2210.08443_, 2022.

* [31] Nour Makke and Sanjay Chawla. Interpretable scientific discovery with symbolic regression: a review. _Artificial Intelligence Review_, 57(1):2, 2024.
* [32] Sahil Manchanda, Akash Mittal, Anuj Dhawan, Sourav Medya, Sayan Ranu, and Ambuj Singh. Gcomb: Learning budget-constrained combinatorial algorithms over billion-sized graphs. _Advances in Neural Information Processing Systems_, 33:20000-20011, 2020.
* [33] Sourav Medya, Tiyani Ma, Arlei Silva, and Ambuj Singh. A game theoretic approach for core resilience. In _International Joint Conferences on Artificial Intelligence Organization_, 2020.
* [34] Peter Muller, Lukas Faber, Karolis Martinkus, and Roger Wattenhofer. Graphchef: Learning the recipe of your dataset. In _ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH)_, 2023.
* [35] Sunil Nishad, Shubhangi Agarwal, Arnab Bhattacharya, and Sayan Ranu. Graphreach: Position-aware graph neural network using reachability estimations. _IJCAI_, 2021.
* [36] Alan Perotti, Paolo Bajardi, Francesco Bonchi, and Andre Panisson. Explaining identity-aware graph classifiers through the language of motifs. In _2023 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2023.
* [37] Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainability methods for graph convolutional neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10772-10781, 2019.
* [38] Rishabh Ranjan, Siddharth Grover, Sourav Medya, Venkatesan Chakaravarthy, Yogish Sabharwal, and Sayan Ranu. Greed: A neural framework for learning graph distance functions. In _Advances in Neural Information Processing Systems_, 2022.
* [39] Kaspar Riesen and Horst Bunke. Iam graph database repository for graph based pattern recognition and machine learning. In _Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)_, pages 287-297. Springer, 2008.
* [40] Thomas Schnake, Oliver Eberle, Jonas Lederer, Shinichi Nakajima, Kristof T Schutt, Klaus-Robert Muller, and Gregoire Montavon. Higher-order explanations of graph neural networks via relevant walks. _IEEE transactions on pattern analysis and machine intelligence_, 44(11):7581-7596, 2021.
* [41] Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, and Sayan Ranu. Neurocut: A neural approach for robust graph partitioning. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '24, page 2584-2595, New York, NY, USA, 2024. Association for Computing Machinery.
* [42] Caihua Shan, Yifei Shen, Yao Zhang, Xiang Li, and Dongsheng Li. Reinforcement learning enhanced explainer for graph neural networks. In _NeurIPS 2021_, December 2021.
* [43] L. Shapley. _7. A Value for n-Person Games. Contributions to the Theory of Games II (1953) 307-317._, pages 69-79. Princeton University Press, Princeton, 1997.
* [44] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M. Borgwardt. Weisfeiler-lehman graph kernels. _J. Mach. Learn. Res._, 12(null):2539-2561, nov 2011.
* [45] Juntao Tan, Shijie Geng, Zuohui Fu, Yingqiang Ge, Shuyuan Xu, Yunqi Li, and Yongfeng Zhang. Learning and evaluating graph neural network explanations based on counterfactual and factual reasoning. In _Proceedings of the ACM Web Conference 2022_, WWW '22, page 1018-1027, 2022.

* [46] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _International Conference on Learning Representations_, 2018. (Cited on pp. **1, 3, and 4 \(\leftrightarrow\)**)
* [47] Samidha Verma, Burouj Armgaan, Sourav Medya, and Sayan Ranu. InduCE: Inductive counterfactual explanations for graph neural networks. _Transactions on Machine Learning Research_, 2024. (Cited on pp. **1 and 15 \(\leftrightarrow\)**)
* [48] Minh Vu and My T Thai. Pgm-explainer: Probabilistic graphical model explanations for graph neural networks. _Advances in neural information processing systems_, 33:12225-12235, 2020. (Cited on p. **15 \(\leftrightarrow\)**)
* [49] Nikil Wale and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. In _ICDM_, 2006. (Cited on pp. **8 and 17 \(\leftrightarrow\)**)
* [50] Xiaoqi Wang and Han Wei Shen. GNNInterpreter: A probabilistic generative model-level explanation for graph neural networks. In _The Eleventh International Conference on Learning Representations_, 2023. (Cited on p. **2 \(\leftrightarrow\)**)
* [51] Geemi P Wellawatte, Aditi Seshadri, and Andrew D White. Model agnostic generation of counterfactual explanations for molecules. _Chemical science_, 13(13):3697-3705, 2022. (Cited on p. **1 \(\leftrightarrow\)**)
* [52] Yaochen Xie, Sumeet Katariya, Xianfeng Tang, Edward Huang, Nikhil Rao, Karthik Subbian, and Shuiwang Ji. Task-agnostic graph explanations. _NeurIPS_, 2022. (Cited on p. **15 \(\leftrightarrow\)**)
* [53] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2019. (Cited on pp. **1, 3, and 5 \(\leftrightarrow\)**)
* [54] Han Xuanyuan, Pietro Barbiero, Dobrik Georgiev, Lucie Charlotte Magister, and Pietro Lio. Global concept-based interpretability for graph neural networks via neuron analysis. In _AAAI_, 2023. (Cited on pp. **2 and 15 \(\leftrightarrow\)**)
* [55] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks. _Advances in neural information processing systems_, 32, 2019. (Cited on pp. **1, 8, 9, 15, 17, and 19 \(\leftrightarrow\)**)
* [56] Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. Xgnn: Towards model-level explanations of graph neural networks. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 430-438, 2020. (Cited on pp. **2 and 15 \(\leftrightarrow\)**)
* [57] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomic survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022. (Cited on p. **1 \(\leftrightarrow\)**)
* [58] Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks via subgraph explorations. In _ICML_, pages 12241-12252. PMLR, 2021. (Cited on pp. **1 and 15 \(\leftrightarrow\)**)
* [59] Jiayao Zhang, Qiheng Sun, Jinfei Liu, Li Xiong, Jian Pei, and Kui Ren. Efficient sampling approaches to shapley value approximation. _Proceedings of the ACM on Management of Data_, 1(1):1-24, 2023. (Cited on p. **6 \(\leftrightarrow\)**)
* [60] Yue Zhang, David Defazio, and Arti Ramesh. Relex: A model-agnostic relational model explainer. In _Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society_, pages 1042-1049, 2021. (Cited on p. **15 \(\leftrightarrow\)**)
* [61] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Cheekong Lee. Protgnn: Towards self-explaining graph neural networks. _AAAI_, 2021. (Cited on p. **2 \(\leftrightarrow\)**)

## Appendix

## Appendix A Rooted Tree Isomorphism

To identify the set of unique computation trees GraphTrail performs rooted tree-isomorphism. An efficient method for this task involves using canonical labels. The canonical label of a graph \(\mathcal{G}\) is a unique representation that remains unchanged under isomorphism, i.e., two graphs \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\) have the same canonical label if and only if they are isomorphic. We construct the canonical label of a rooted computation tree through Depth-First Canonical Labeling (DFCF) [7]. From a labelled rooted unordered tree we can derive many labelled rooted ordered trees as shown in Fig. H. There is a one-to-one correspondence between a labelled rooted ordered tree and its depth-first string encoding. The ordering of the strings orders the trees and the minimum in the ordering is the canonical label. Each string is a depth-first (preorder) traversal that uses \(\$\) to represent a backtrack and \(\#\) to represent

Figure 1: The eight toxicophores in the Mutagenicity dataset that are capable of identifying 75% of all mutagens in the dataset [9]. “aro” indicates an aromatic atom. "arom. rings" indicates an atom that is part of multiple aromatic rings. The toxicophores identified by GraphTrail are colored green. Toxicophores 5 and 6 are underlined as they are not explicitly identified but are potential extensions to two of the identified structures in Fig. 5.

the end of the string encoding. In sorting, \(\#\) is greater than $ and both these symbols are greater than other labels. The Depth-First Canonical Form is constructed by sorting the vertices of a rooted unordered tree level by level. At each level, the vertices are sorted first by their labels and then by the ranks of their children at respective levels. Fig. 1 illustrates the process.

## Appendix B Shapley Values

SHAP [28] represents the Shapley value explanation (SV) [43] as an additive feature attribution method and fits a weighted linear regression model to approximate the model's output. The weights in this model correspond to the SVs (contribution of features). It specifies the explanation as

\[g(z)=\phi_{0}+\sum_{j=1}^{M}\phi_{j}z_{j}\]

where \(g\) is the model, \(z\in\{0,1\}^{M}\) is the coalition vector, \(M\) is the maximum coalition size, and \(\phi_{j}\in\mathbb{R}\) is the SV for feature \(j\). Kernel-SHAP [28] is a SHAP variant to estimate SVs. It calculates SVs via:

* **Coalition Sampling:** It randomly selects subsets of features, coalitions \((z_{k})\). It assigns weights to each coalition using the SHAP kernel where \(M\) = total features, \(|z|\) = coalition size: \[\pi(z)=\frac{(M-1)}{\binom{M}{x}|z|(M-|z|)}\]
* **Predictions:** For each coalition, Kernel-SHAP maps the coalition back to the original features and obtains a prediction afterwards.

Figure 1: Example of constructing a Depth-First-Canonical-Form of a rooted tree. Each vertex is labeled with its identifier and, within parentheses, the ranks of its children, ending with a “hash symbol” to denote the end of the encoding. The rank of the vertex at its level is placed before the parentheses. After sorting all levels, the tree is then scanned from the top down, beginning with the root, and the children of each vertex are rearranged according to the established order.

Figure 2: End of string encoding

* **Fitting:** Using the calculated weights and predictions, Kernel-SHAP fits a linear model.

## Appendix C Symbolic Regression

In symbolic regression, given a set of \(n\) input-output pairs \((x_{i},y_{i})_{i=1}^{n}\), where \(x_{i}\in\mathbb{R}^{d}\) is an input vector, and \(y\in\mathbb{R}\) is the output value (or label), and a set of operators, such as addition, subtraction, etc., the goal is to find a symbolic equation \(e\) and corresponding function \(f_{e}\) such that \(\forall i:y_{i}\approx f_{e}(x_{i})\), while also reducing the complexity (number of operators and variable) of the formula. The loss function is presented in Eq. 11. Finding the optimal formula is not computationally tractable since the number of formulas grows exponentially with the number of variables and operations. Hence, from the various approximation strategies in the literature [31] we use [8], which leverages an evolutionary algorithm.

The process begins by initializing \(n_{p}\) populations, each with \(L\) random expressions of complexity \(C\) (3 in our experiments). For each \(P_{i}\), a set \(M_{i}\) is created to store the expressions with the smallest loss (Eq. 11) at each complexity level within that population. Additionally, a global set \(H\) is maintained to store the expressions with lowest loss at each complexity across all populations. The algorithm iterates over each population \(P_{i}\) for a fixed number of epochs, allowing them to evolve. Evolution is driven by running tournaments within each population, where the expression \(E\) with lowest loss is declared the winner. A copy of the winner is created and chosen for _mutation_ or _crossover_, forming \(E^{*}\). Mutating an expression involves repeatedly applying random operations from a set of operations, e.g., replacing/adding operators and variables. Crossover selects the two best expressions, \(E_{1},E_{2}\) from the population and swapping random sub-expressions between them, forming \(E_{1}^{*},E_{2}^{*}\). The newly formed expression(s) replace the oldest expression(s) in the population.

Once evolution within each population is complete, the sets \(M_{i}\) and \(H\) are updated with the best expressions. When the set number of epochs are complete, the expression from \(H\) with minimum loss is returned.

## Appendix D Dataset details

Table C presents four benchmark datasets used in our experiments. NCI1 [49], MUTAG [17], and Mutagenicity [39, 20] are collections of molecules. The nodes represent different atoms, and the edges are chemical bonds between them. The molecules are classified by whether they are anticancer (NCI1) or mutagenic (MUTAG and Mutagenicity). In MUTAG, Class 1 is the mutagenic class while in Mutagenicity, Class 0 is mutagenic. BAMultiShapes [55] is a synthetic dataset. The dataset is composed of 1,000 Barabasi-Albert (BA) graphs with network motifs attached in random positions. These motifs include _house_, _grid_, and _wheel_ structures. Class 0 includes plain BA graphs as well as BA graphs augmented with a house, a grid, a wheel, or all three motifs combined. Class 1 consists of BA graphs enriched with combinations of two motifs: a house and a grid, a house and a wheel, or a wheel and a grid.

## Appendix E Further details of empirical framework

All experiments are performed on an Intel Xeon Gold 6248 processor with 96 cores and 1 NVIDIA A100 GPU with 40GB of memory and 377 GB RAM with Ubuntu 18.04.

Each dataset is split into train-validation-test sets in the proportion of 70:10:20. The accuracies of all Gnn architectures are reported in Table D. All Gnns have been trained with \(L=3\) layers. We use Adam optimizer with a learning rate set to \(0.001\). Training stops early after a warm-up of 90

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & BAMultiShapes & Mutagenicity & MUTAG & NCI1 \\ \hline \#Graphs & 1000 & 4337 & 188 & 4110 \\ Avg. \(|\mathcal{V}|\) & 40 & 30.32 & 17.90 & 29.87 \\ Avg. \(|\mathcal{E}|\) & 87.00 & 30.77 & 39.60 & 32.30 \\ \#Node features & 10 & 14 & 7 & 37 \\ \hline \hline \end{tabular}
\end{table}
Table C: Statistics of the datasets.

[MISSING_PAGE_FAIL:18]

## Appendix H Reproducibility Analysis of GLGExplainer

Our analysis reveals inconsistencies between the findings reported in [2] and our own observations with GLGExplainer. Specifically, the reported results in [2] for Mutagenicity dataset might be inflated due to test data leakage. In this dataset, the presence of NO2 and/or NH2 motifs is indicative of a positive class label [55]. However, GLGExplainer utilizes PGExplainer[29] to generate instance explanations. Within GLGExplainer, these explanations are created by excluding training graphs that lack these motifs (see Fig. 2 for the relevant code snippet from PGExplainer's repository [https://github.com/flyingdoog/PGExplainer/blob/master/MUTAG.ipynb](https://github.com/flyingdoog/PGExplainer/blob/master/MUTAG.ipynb)). This approach results in biased explanations that overemphasize specific motifs. When all training graphs are included, GLGExplainer produces a wider variety of explanations, leading to a decrease in its overall performance.

For other datasets, GLGExplainer offers instance explanations but lacks transparency regarding the specific PGExplainer settings used to generate them. Despite employing all recommended settings and extensive parameter tuning for PGExplainer, we were unable to replicate the instance explanations presented by the authors of GLGExplainer. Our email to the authors of GLGExplainer pointing out these inconsistencies did not receive a response.

## Appendix I Cluster Impurity of GLGExplainer

In GLGExplainer, the formulae are vectors that are called prototypes. Prototypes are representatives of clusters of local explanations, and it is assumed that the clusters are _pure_. This essentially

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Method** & **BAMultiShapes** & **MUTAG** & **Mutagenicity** & **NCI1** \\ \hline GLG & 0.48 \(\pm\) 0.02 & 0.74 \(\pm\) 0.10 & 0.62 \(\pm\) 0.03 & 0.57 \(\pm\) 0.04 \\ \hline G-Trail & **0.86 \(\pm\) 0.02** & **0.82 \(\pm\) 0.09** & **0.72 \(\pm\) 0.01** & **0.72 \(\pm\) 0.02** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Weighted average recall of the generated formulas against the GNN output

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Method** & **BAMultiShapes** & **MUTAG** & **Mutagenicity** & **NCI1** \\ \hline GLG & 0.46 \(\pm\) 0.03 & 0.68 \(\pm\) 0.00 & 0.55 \(\pm\) 0.03 & 0.56 \(\pm\) 0.04 \\ \hline G-Trail & **0.83 \(\pm\) 0.03** & **0.72 \(\pm\) 0.05** & **0.65 \(\pm\) 0.01** & **0.67 \(\pm\) 0.02** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Accuracy of the generated formulae against ground-truth labels

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Method** & **BAMultiShapes** & **MUTAG** & **Mutagenicity** & **NCI1** \\ \hline GLG & 0.41 \(\pm\) 0.06 & 0.68 \(\pm\) 0.11 & 0.61 \(\pm\) 0.03 & 0.49 \(\pm\) 0.11 \\ \hline G-Trail & **0.86 \(\pm\) 0.02** & **0.82 \(\pm\) 0.08** & **0.72 \(\pm\) 0.02** & **0.70 \(\pm\) 0.03** \\ \hline \hline \end{tabular}
\end{table}
Table 7: The weighted average recall of the generated formulas against the GNN output

[MISSING_PAGE_EMPTY:20]

Figure L: A visual depiction of the impure clusters of GLGExplainer in Mutagenicity.

Figure M: A visual depiction of the impure clusters of GLGExplainer in BAMultiShapes.

Figure N: A visual depiction of the impure clusters of GLGExplainer in NCI1.

## NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Sec. 2, 3, and 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Sec. 5. Guidelines:
* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Sec. 3.2 Guidelines:
* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.

* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Sec 4.1, Appendix B and C Guidelines:
* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Sec. 4 and Appendix B and C. The code has been provided via a github repository. Guidelines:
* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Sec. 4 and Appendix B and C. Guidelines:
* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Section 4 Guidelines:
* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Sec. 4 and Appendix C Guidelines:
* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: All sections.

Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Sec. 1 (positive social impact), Sec. 5 (negative social impact) Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Sec. 4 Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.