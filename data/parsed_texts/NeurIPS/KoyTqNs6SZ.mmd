# Learning to Price Homogeneous Data

Keran Chen

UW-Madison

kchen429@wisc.edu

&Joon Suk Huh

UW-Madison

jhuh23@wisc.edu

&Kirthevasan Kandasamy

UW-Madison

kandasamy@cs.wisc.edu

###### Abstract

We study a data pricing problem, where a seller has access to \(N\) homogeneous data points (e.g. drawn i.i.d. from some distribution). There are \(m\) types of buyers in the market, where buyers of the same type \(i\) have the same valuation curve \(v_{i}:[N]\rightarrow[0,1]\), where \(v_{i}(n)\) is the value for having \(n\) data points. _A priori_, the seller is unaware of the distribution of buyers, but can repeat the market for \(T\) rounds so as to learn the revenue-optimal pricing curve \(p:[N]\rightarrow[0,1]\). To solve this online learning problem, we first develop novel discretization schemes to approximate any pricing curve. When compared to prior work, the size of our discretization schemes scales gracefully with the approximation parameter, which translates to better regret in online learning. Under assumptions like smoothness and diminishing returns which are satisfied by data, the discretization size can be reduced further. We then turn to the online learning problem, both in the stochastic and adversarial settings. On each round, the seller chooses an _anonymous_ pricing curve \(p_{t}\). A new buyer appears and may choose to purchase some amount of data. She then reveals her type _only if_ she makes a purchase. Our online algorithms build on classical algorithms such as UCB and FTPL, but require novel ideas to account for the asymmetric nature of this feedback and to deal with the vastness of the space of pricing curves. Our algorithms achieve \(\widetilde{\mathcal{O}}(m\sqrt{T})\) regret in the stochastic setting and \(\widetilde{\mathcal{O}}(m^{\nicefrac{{3}}{{2}}}\sqrt{T})\) regret in the adversarial setting.

## 1 Introduction

Due to the rise in popularity of machine learning, there is an increased demand for data. However, not all users of data have the wherewithal to collect data on their own, and have to rely on data marketplaces to acquire the data they need. For example, a materials data platform (e.g. [18]), may have collected vast amounts of data from various proprietary sources. Materials scientists in smaller organizations and academia, who do not have large experimental apparatuses, may wish to purchase this data to aid in their research. Similarly, small businesses may wish to purchase customer data for advertising and product recommendations [4, 5], while small technology companies may wish to purchase data about cloud operations to optimize their computing infrastructure [2, 3].

**Model.** Motivated by the emergence of such data marketplaces, we study the following online data pricing problem. A seller has access to \(N\) homogeneous data points, (e.g. drawn i.i.d. from some distribution). He wishes to sell the data to a sequence of distinct buyers over \(T\) rounds, and intends to achieve large revenue. There are \(m\) types of buyers in the data marketplace, with all buyers in type \(i\) having the same valuation curve \(v_{i}:[N]\rightarrow[0,1]\) for the data, where \(v_{i}(n)\) represents the buyer's value for having \(n\) points. As data is homogeneous, we can treat an agent's value as a function of the _amount of data_\(n\) (we will illustrate this in the sequel). Valuation curves are monotone non-decreasing, as more data is better. At each round \(t\), the seller chooses a price curve \(p_{t}:[N]\rightarrow[0,1]\), where \(p_{t}(n)\) is the price for purchasing \(n\) data points. Then, a buyer with type \(i_{t}\) arrives and purchases an amount of data that maximizes her utility (value minus price), provided that she can achieve non-negative utility. A buyer will reveal her type to the seller _only if_ she makes a purchase, and _only after_ shemakes the purchase. The seller has knowledge of valuation curves of the \(m\) types, but does not know the distribution \(q\) over types (stochastic setting), or the buyer sequence (adversarial setting). Moreover, he cannot practice non-anonymous (discriminatory) pricing, as he needs to choose the pricing curve \(p_{t}\) without knowledge of the buyer's type on that round.

While there is extensive research on _revenue-optimal pricing_ and _learning to price_, data marketplaces merit special attention, both due to their recent emergence and the unique characteristics of data. Typically the number of data \(N\) (number of goods) is very large, but data usually satisfies additional properties such as smoothness (an agent's value does not increase significantly with a small amount of additional data) and diminishing returns (additional data is more valuable when a buyer has less data). To illustrate further, note that two steps are essential to develop an effective online learning solution for data pricing. _(1)_ First, we need to solve the _planning problem_, i.e. find a revenue-optimal pricing curve when the type distribution \(q\) is known. _(2)_ Second, when \(q\) is unknown, we need to combine the algorithm in step (1) with estimates for \(q\) to maximize long-term revenue.

Methods in the existing literature fall short in both steps. _(1)_ When the type distribution \(q\) is known, the data pricing problem resembles an _ordered item pricing_ problem, which is known to be NP-hard [13, 25]. Hence, prior work has aimed at approximating the optimal pricing curves via discretization schemes. Unfortunately, existing discretization schemes have poor, often exponential, dependence on the approximation parameter \(\epsilon\). However, achieving sublinear regret in online learning requires choosing \(\epsilon\) that vanishes with longer time horizons, i.e. \(\epsilon\to 0\) as \(T\to\infty\). Therefore, directly using existing discretization schemes in an online setting leads to poor statistical _and_ computational properties of the associated online algorithm. This requires us to leverage the above properties of data to design discretization schemes with better dependence on \(\epsilon\). _(2)_ While there is prior work on learning optimal prices [22, 27, 33], these techniques either fall short of addressing the complexities in our setting, or fail to account for the properties of data, and hence do not scale gracefully when the amount of data \(N\) is very large. Moreover, in our online learning setup, the seller faces a trade-off between setting high prices to maximize instantaneous revenue versus setting low prices so as to guarantee a purchase, which results in the buyer revealing their type, which in turn can be helpful in future rounds. Prior work has studied this asymmetric feedback model _only in single-item markets_ which is significantly simpler, and _only in the stochastic setting_[23, 47].

### Summary of our contributions

Our contributions in this work are threefold: _(1)_ First, in SS3, we develop discretization schemes for revenue-optimal data pricing under a variety of assumptions, which we will use later in our online learning schemes. _(2)_ In SS4, we study learning a revenue-optimal price in a stochastic setting, where the customer types on each round are drawn from a fixed but unknown distribution \(q\). _(3)_ Finally, in SS5, we study online learning when the buyer types are chosen by an oblivious adversary.

**1. Discretization (approximation) schemes for revenue-optimal data pricing.** Assuming only monotonicity, we show that there is a discretization of size \(\widetilde{\mathcal{O}}((N/\epsilon)^{m})\) which is an \(\mathcal{O}(\epsilon)\) additive approximation to any pricing curve. When compared to prior work [14, 25], our discretization scheme has smaller dependence on \(\epsilon^{-1}\) when the number of types \(m\) is small (see Table 1). This will be useful, both statistically and computationally, when we study the online setting, as we need to choose \(\epsilon\to 0\) as \(T\to\infty\) to achieve sublinear regret. This is still quite large in real-world data marketplaces, where \(N\) may be very large. Hence, we also study two other assumptions. First, when valuations are smooth, satisfying an \(L\)-Lipschitz-like condition, we construct a discretization of size \(\widetilde{\mathcal{O}}\left((L/\epsilon^{2})^{m}\right)\), which has no dependence on \(N\). Next, under a diminishing returns condition, we construct a discretization of size \(\mathcal{O}\left(J^{m}\epsilon^{-3m}\log^{m}(N)\right)\), which only has polylogarithmic dependence on \(N\).

_Key insights._ We first show that when there are only \(m\) types, for any price function \(p:[N]\to[0,1]\), there exists an "\(m\)-step" price function \(p^{\prime}\) whose revenue is at least as much as that of \(p\) on any type distribution \(q\). An \(m\)-step function is non-decreasing and changes values at most \(m\) times, allowing us to focus on this restricted class and thereby reduce the search space when \(m\ll N\). We then consider discretizations of the data space \([N]\) and valuations \([0,1]\) which allow us to obtain an \(\mathcal{O}(\epsilon)\)-approximation to any pricing curve, and then apply this insight to construct our discretizations. Finally, we show that with monotonicity and diminishing returns, similarly accurate approximations are attainable with substantially coarser discretizations.

**2. Learning to price in the stochastic setting.** Next, we turn to the online learning problem described in the beginning in a stochastic setting. On each round, our algorithm computes an upper confidence bound (UCB) [8; 38] on the revenue for each price curve in the discretization previously developed; we then choose the price curve with the highest UCB. As summarized in Table 2, this algorithm achieves a \(\widetilde{\mathcal{O}}(m\sqrt{T})\) bound on the regret for _any_ discretization scheme, including those from prior work. In the stochastic setting, the key advantage of our discretization schemes is computational.

_Key insights._ Both the design and the anlaysis of an algorithm is challenging in this setting due to two reasons: _(i)_ the large size of the discretization and _(i)_ the asymmetric nature of feedback. First, naively maintaining UCBs for each price leads to large confidence intervals, and hence large regret as the size of the discretization is large; instead, we construct confidence intervals on estimates of the type distribution, and translate them to UCBs for the revenue. Second, the asymmetric nature of the feedback places us between bandit and full-information settings. Treating this like a bandit setting would lead to poor, exponential dependence on \(m\) in the regret. However, we are unable to treat this as full information since the type distribution is revealed only if there is a purchase. Handling this asymmetry requires a delicate construction of the UCB.

**3. Learning to price in the adversarial setting.** We study learning in an adversarial setting where the types on each round may be chosen adversarially. Table 2 shows the regret and time complexity of our method when paired with various discretization schemes. In the adversarial setting, our discretization schemes offer both computational and statistical advantages compared to prior work.

_Key insights._ Our algorithm builds on the Follow-the-Perturbed-Leader (FTPL) [31], originally designed for full-information settings and not directly applicable here. To handle asymmetric feedback, we use the information we have about the valuation curves to keep track of which customers would not have made a purchase given a price curve. If a purchase is made and we observe feedback, we use the usual FTPL update, but if not, we reward each pricing curve with the sum of revenue of all types that would not purchase in that current round.

### Related work

**Dynamic pricing**. The online posted-price mechanism, also known as dynamic pricing, is a central research area in algorithmic market design [19; 33]. In the most classical setting [33], the seller sets a price for an item in each round, and a buyer purchases the item only if their valuation exceeds the posted price. While several extensions of this setting have been explored for both parametric [12; 20; 28; 29; 32; 46] and non-parametric [11; 17; 39; 40; 44] demands, most focus on single-parameter demands, i.e., selling a single item to buyers. Our data pricing problem is multi-parameter, as demands are parameterized by multiple outcomes, i.e. the number of data points.

**Bayesian unit-demand pricing problem**. Formally, our data pricing problem is a variant of the Bayesian Unit-demand Pricing Problem (BUPP) [13]. BUPP addresses the problem of (offline) revenue maximization over a known distribution of unit-demand buyers, meaning they want to buy at

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Algorithm & Assumptions & Size of discretization & Reference \\ \hline Hartline and Koltun [25] & – & \(\widetilde{\mathcal{O}}(2^{N}\epsilon^{-N})\) & – \\ \hline Chawla et al. [14] & **M** & \(N^{\mathcal{O}\left(\epsilon^{-2}\log\epsilon^{-1}\right)}\) & – \\ \hline Algorithm 1 (ours) & **M, F** & \(\widetilde{\mathcal{O}}(N^{m}\epsilon^{-m})\) & Theorem 3.1 \\ \hline Algorithm 5 (ours) & **M, F, S** & \(\widetilde{\mathcal{O}}\left(L^{m}\epsilon^{-2m}\right)\) & Theorem 3.2 \\ \hline Algorithm 2 (ours) & **M, F, D** & \(\widetilde{\mathcal{O}}\left(J^{m}\epsilon^{-3m}\log^{m}N\right)\) & Theorem 3.3 \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of discretization (approximation) schemes of prior work and our methods under various assumptions. All methods achieve a \(\mathcal{O}(\epsilon)\) additive approximation to any pricing curve. Here, **M** means **M**onotonicity, **F** means that there are a **F**inite (\(m\)) number of types, **S** means that the valuation curves satisfy a \(L\)-Lipschitz-like **S**moothness condition (Assumption 1), and **D** means that they satisfy a **D**iminishing returns condition (Assumption 2). The \(\widetilde{\mathcal{O}}\) notation suppresses log dependencies when there is already a polynomial dependence on a parameter. Prior work has exponential dependence in either \(N\) or \(\epsilon^{-1}\). We wish to do better since _(i)_ typically, the number of data \(N\) is very large and _(ii)_ we need \(\epsilon\to 0\) as \(T\to\infty\) to achieve sublinear regret.

most one item from the inventory. In BUPP, a seller has \(N\) distinct items to sell to a unit-demand buyer whose valuations are \(v=(v_{1},\ldots,v_{N})\), where \(v_{i}\) is the value of the \(i\)th item. Given prices \(p_{i},\ i\in[N]\), the unit-demand buyer purchases a single item \(i\in[N]\) that maximizes their utility: \(v_{i}-p_{i}\). Assuming the valuation profile \(v\) follows a known distribution \(D\), the goal of BUPP is to find the best prices \(\{p_{i}\}_{i\in[N]}\) that maximize the seller's expected revenue.

Our data pricing problem is a variant of BUPP in two ways: _(1)_ We study the sequential setting where type distributions are _unknown_, while valuation profiles for each type are known, and _(2)_ We assume monotonic values \(v_{1}\leq\cdots\leq v_{N}\), which is natural in data pricing. Unfortunately, BUPP is a computationally intractable problem, as is ours. BUPP is known to be \(\NP\)-hard even when \(D\) is a product distribution [16]. Moreover, even assuming that values are monotonic (i.e., \(v_{1}\leq\cdots\leq v_{N}\)), the problem remains (strongly) \(\NP\)-hard [14]. Therefore, we aim to provide a reasonably efficient no-regret algorithm for our problem, especially when the number of types \(m\) is a fixed constant.

The previous works most relevant to our paper are Hartline and Koltun [25] and Chawla et al. [14], which study offline revenue maximization for unit-demand buyers. Buyers in our problem are also unit-demand, as each amount of data points can be seen as an individual item. Revenue maximization for unit-demand buyers is known to be computationally intractable [24], even with ordered (monotonic) buyer values [14], leading these works to focus on approximation algorithms. Hartline and Koltun [25] proposed an approximation algorithm with near-linear runtime in the number of buyers, given a fixed number of items. Chawla et al. [14] introduced a polynomial-time approximation scheme (PTAS) for unit-demand buyers with monotonic values. In this work, we extend the framework to the online setting with partial feedback, which has more practical implications.

In addition, Balcan and Beyhaghi [10] provide new guarantees for learning revenue-maximizing menus of lotteries and two-part tariffs, demonstrating that their discretization technique yields efficient solutions for specific pricing models. Similar discretization methods could be investigated in future work to potentially improve our approach in more complex data pricing scenarios.

**Market design for data-sharing**. In recent years, there has been a plethora of work devoted to algorithmic market design for data sharing [6, 7, 30, 43]. These works provide ingenious solutions to challenges unique to the data market, such as free replicability and the difficulty of valuation due to the combinatorial nature of data. Except for Agarwal et al. [6], the above-cited solutions are inherently _offline or single-shot_. While we focus on a simplified yet relevant setting where data comes from a single source, resulting in monotonic valuations, in this work, we tackle the problem in a sequential, dynamic setting, which has practical importance. In contrast to our approach, Agarwal et al. [6] considered the price to be a constant (i.e., a scalar rather than a price vector) to address the inherent computational intractability of multi-dimensional pricing. Instead, we maintain the price

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Setting & Assumptions & Regret bound & Complexity per iteration & Reference \\ \hline \multirow{3}{*}{Stochastic} & \(\mathbf{M,F}\) & \multirow{3}{*}{\(\widetilde{\mathcal{O}}\left(m\sqrt{T}\right)\)} & \(\widetilde{\mathcal{O}}\left(\left(\frac{N}{m}\right)^{m}T^{\nicefrac{{m}}{{2} }}\right)\) & \multirow{3}{*}{Theorem 4.1} \\ \cline{2-3}  & \(\mathbf{M,F,S}\) & & & \\ \cline{2-3}  & \(\mathbf{M,F,D}\) & & & \\ \hline \multirow{3}{*}{Adversarial} & \(\mathbf{M,F}\) & \multirow{3}{*}{\(\widetilde{\mathcal{O}}\left(m^{\nicefrac{{3}}{{2}}}\sqrt{T}\right)\)} & \(\widetilde{\mathcal{O}}\left(\left(\frac{N}{m}\right)^{m}T^{\nicefrac{{m}}{{2} }}\right)\) & \multirow{3}{*}{Theorem 5.1} \\ \cline{2-3}  & \(\mathbf{M,F,S}\) & & & \\ \cline{1-1} \cline{2-3}  & \(\mathbf{M,F,D}\) & & & \\ \hline \hline Discretization method & Assumptions & Complexity per iteration & Regret (Adversarial) \\ \hline Hartline and Koltun [25] & \(\mathbf{F}\) & \(\widetilde{\mathcal{O}}(2^{N}\epsilon^{-N})\) & \(\widetilde{\mathcal{O}}(m\sqrt{TN})\) \\ \hline Chawla et al. [14] & \(\mathbf{M,F}\) & \(N^{\mathcal{O}\left(\epsilon^{-2}\log\epsilon^{-1}\right)}\) & \(\widetilde{\mathcal{O}}\left(mT^{\nicefrac{{3}}{{4}}}\right)\) \\ \hline \end{tabular}
\end{table}
Table 2: Comparison of regret and time complexity of _our_ online learning methods when paired with our discretization schemes and schemes from prior work. See Table 1 for a description of the assumptions. All methods, including [14, 25] achieve \(\mathcal{O}(m\sqrt{T})\) regret in the stochastic setting.

as a vector (i.e., a price function) but focus on cases where the valuation function satisfies natural properties such as monotonicity, smoothness, and diminishing returns.

## 2 Problem setting, assumptions, and challenges

A seller has \(N\) homogeneous data points. There are \(m\) types of buyers who wish to purchase this data. A buyer of type \(i\in[m]\) has a valuation curve \(v_{i}:[N]\rightarrow[0,1]\), where \(v_{i}(n)\) is her value for \(n\) data points. We will assume \(v_{i}(n)\) is non-decreasing as more data is valuable, and further that \(v_{i}(0)=0\).

**Example 1**.: To motivate this model, consider a seller with \(N\) ordered data points \(\{x_{1},\ldots,x_{N}\}\), drawn i.i.d. from a distribution \(D\). If a buyer purchases \(n\) points, she receives the first \(n\) points, \(X_{n}=\{x_{1},\ldots,x_{n}\}\). Her _ex-post_ value \(\widetilde{v}_{i}(X_{n})\) may represent the accuracy of her ML model trained with \(X_{n}\). However, as the buyer has not seen the data before the purchase, she does not know which specific points she will receive, and hence her (_ex-ante_) value \(v_{i}(n)=\mathbb{E}_{X_{n}}[\widetilde{v}_{i}(X_{n})]\) is the expected model accuracy when \(n\) i.i.d points are drawn from \(D\). The different types could be buyers who use the data for different tasks or models. For instance, with ImageNet's [21], \(N\approx 1.4\) million data points, different types of buyers could perform different learning tasks such as object detection, identification, and segmentation, and/or train different models such as AlexNet [36], ResNet [26], and GoogLeNet [42]. Both empirically and theoretically, for many learning tasks, \(v_{i}(n)\) is non-decreasing, and satisfies additional characteristics such as smoothness and/or diminishing returns.

**Pricing curves, buyer utility, and buyer purchase model.** Let \(p:[N]\rightarrow[0,1]\) be a pricing curve chosen by the seller. Let \(\mathcal{P}\stackrel{{\Delta}}{{=}}\{p:[N]\rightarrow[0,1]:\ p(0)=0\}\) denote the set of all pricing curves. If a buyer purchases \(n\) points, her utility is \(u_{i}(n)=v_{i}(n)-p(n)\). If a buyer can achieve non-negative utility, i.e. \(v_{i}(n)\geq p(n)\) for some \(n\in[N]\), she will purchase an amount of data to maximize her utility. To fully specify the buyer's purchase model, we will assume that when there are multiple \(n\) which maximizes her utility, she will choose the largest such \(n\). Formally, for a given pricing curve \(p\), a buyer of type \(i\) will purchase \(n_{i,p}\) points where,

\[n_{i,p}\stackrel{{\Delta}}{{=}}\left\{\begin{array}{ll}0&\text {if $v_{i}(n)<p(n)$ for all $n\in[N]$,}\\ \max\left\{\,\operatorname{argmax}_{n\in[N]}\left(v_{i}(n)-p(n)\right)\,\right\} &\text{otherwise.}\end{array}\right.\] (1)

_Optimal revenue._ It follows that the revenue from a buyer of type is \(p(n_{i,p})\). Let \(q=(q_{1},\ldots,q_{m})\) be the distribution of the buyers. Under this distribution \(q\), the expected revenue \(\operatorname{rev}(p)\) for a price curve \(p\), the optimal price \(p^{\text{OPT}}\), and the optimal revenue \(\operatorname{OPT}\) as follows:

\[\operatorname{rev}(p)\stackrel{{\Delta}}{{=}}\sum_{i=1}^{m}q_{i} \cdot p(n_{i,p}),\hskip 28.452756ptp^{\text{OPT}}\stackrel{{ \Delta}}{{=}}\operatorname{argmax}_{p\in\mathcal{P}}\operatorname{rev}(p), \hskip 28.452756pt\operatorname{OPT}\stackrel{{\Delta}}{{=}} \operatorname{rev}(p^{\text{OPT}}).\] (2)

We have omitted the dependence on \(q\) in \(\operatorname{rev}\), \(p^{\text{OPT}}\), and \(\operatorname{OPT}\). There is no closed-form solution to finding the optimal pricing curve, even when \(q\) is known. Therefore, in SS3, we explore discretization methods to approximate \(p^{\text{OPT}}\), which will then be used in SS4 and SS5 to develop online learning algorithms. Unfortunately, the size of this discretization can be very large in \(N\) and \(m\) without further assumptions. Therefore, we also consider two additional commonly satisfied conditions by data.

Our first such assumption states that buyer valuation curves satisfy a Lipschitz-like smoothness condition with Lipschitz constant \(L/N\). We use \(L/N\) instead of \(L\) since the number of data has a range \([0,N]\), while the valuations only have a range \([0,1]\). This condition states that a buyer's valuation does not change significantly if she only purchases a few additional points.

**Assumption 1** (Smoothness, **S**).: _For all \(n,n^{\prime}\in[N]\), we have \(v_{i}(n+n^{\prime})-v_{i}(n)\leq\frac{L}{N}n^{\prime}\)._

Our second condition is based on the fact that data typically exhibits diminishing returns [34, 35]. This means that an additional data point is more valuable when there is less data, i.e. \(v_{i}(n+1)-v_{i}(n)\) is decreasing with \(n\). We will in fact make a stronger assumption, and justify it below.

**Assumption 2** (Diminishing returns, **D**).: _There exists some \(J>0\) such that, for all types \(i\in[m]\), and for all \(n\in[N]\), we have \(v_{i}(n+1)-v_{i}(n)\leq\frac{J}{n}\)._

Assumption 2 quantifies the rate of decrease of diminishing returns. Following Example 1, the valuation (accuracy) curves for many learning problems take the form \(v_{i}(n)=\alpha-\beta n^{-\gamma}\); for instance, for binary classification in a VC class \(\mathcal{H}\), \(\alpha\) may be the best accuracy in \(\mathcal{H}\), \(\beta\in\mathcal{O}(\sqrt{d_{\mathcal{H}}})\) where \(d_{\mathcal{H}}\) is the VC dimension, and \(\gamma=1/2\)[41]; similarly, for nonparametric regression of a twice differentiable function, \(\alpha\) and \(\beta\) are constants while \(\gamma=2/5\)[45]. In such cases, Assumption 2 is satisfied with \(J=\beta\gamma\). Note that neither assumption subsumes the other: a non-concave Lipschitz function will not satisfy Assumption 2, while a suitable \(L\) for a function which satisfies Assumption 2 may need to be very large for Assumption 1 to hold for small \(n\).

### Learning to price in online settings

In this work, we will also study how a seller may learn to maximize revenue. In our learning problem, the seller is aware of the valuation curves \(\{v_{i}\}_{i}\) of each type, but does not know the distribution of types (stochastic setting) or there may be no such distribution (adversarial setting).

**Setup.** The seller repeats the data market for \(T\) rounds. At the beginning of each round, he chooses some price curve \(p_{t}\in\mathcal{P}\). _After_ the seller has chosen \(p_{t}\), a new buyer of type \(i_{t}\in[m]\) appears and purchases \(n_{t}=n_{i_{t},p_{t}}\) amount of data (see (1)). The buyer is aware of her own valuation curve. If she makes a purchase, that is if \(n_{t}>0\), she pays \(p_{t}(n_{t})\) to the seller and reveals her type \(i_{t}\). Otherwise, the buyer will make no payment and not reveal her type.

We have assumed that _a priori_, the seller is aware of the buyer valuation curves \(\{v_{i}\}_{i\in[m]}\), and that buyers are aware of their own valuation curves. In Example 1, a seller can profile how different machine learning models perform with different amounts of data and publish them ahead of time. The buyers can also gauge their value from these curves, even though they do not have access to the data. Next, we have also assumed that buyers will reveal their type after the purchase. In modern machine learning as a service platforms [1, 4, 18], buyers directly run their jobs in the seller's computing platform, so the seller can observe the buyers job _type_ directly. Even if this is not the case, sellers can elicit this information via questionnaires and reviews from customers who have made a purchase [23].

**Challenges.** Despite these assumptions, the learning problem remains challenging for two main reasons. First, the space of price curves is vast: discretizing the valuations in \([0,1]\) into \(K\) bins, still leaves \(\mathcal{O}(K^{N})\) possible price curves, which is both statistically and computationally intractable, especially for large \(N\). Second, in addition to the exploration-exploitation trade-off usually encountered in sequential decision-making, the seller faces a tension between high instantaneous revenue and information acquisition: setting high prices can yield high immediate revenue if a purchase occurs, but it also increases the risk of no purchase, resulting in no revenue and crucially no feedback about the buyer type which could help him in future rounds. This trade-off was recently studied for single-item markets in a stochastic setting [23, 47], but is more complex in our multi-item problem. Moreover, to our knowledge, no existing work addresses this asymmetric feedback model in an adversarial setting, even for single-item markets. Next, we describe the buyer arrival model and define the regret for the learning problem in both stochastic and adversarial settings.

**Stochastic setting.** Here, there is some fixed but unknown distribution of types \(q\). On each round, a buyer of type \(i_{t}\sim q\) is drawn independently. The optimal expected revenue \(\mathrm{OPT}\) under type distribution \(q\) is as defined in (2). The regret \(R_{T}\) is as defined below. We wish to design algorithms which have small expected regret \(\mathbb{E}[R_{T}]\), where the expectation accounts for both the sampling of types \(i_{t}\sim q\) and any randomness in the algorithm. We have,

\[R_{T}\ \stackrel{{\Delta}}{{=}}\ T\cdot\mathrm{OPT}\,-\,\sum_{t=1}^ {T}p_{t}(n_{t})\ =\ T\cdot\mathrm{OPT}\,-\,\sum_{t=1}^{T}p_{t}(n_{i_{t},p_{t}}).\] (3)

**Adversarial setting.** Here, the types on each round \(\{i_{t}\}_{t=1}^{T}\) are chosen arbitrarily, possibly by an oblivious adversary, ahead of time. The type on round \(t\) is revealed to the seller only at the end of the round, and only if there is a purchase. In the adversarial setting, we define our regret \(R_{T}\) with respect to the single best price in \(\mathcal{P}\) in hindsight. We wish to design algorithms with small expected regret \(\mathbb{E}[R_{T}]\), where the expectation is with respect to any randomness in the algorithm. We have,

\[R_{T}\ \stackrel{{\Delta}}{{=}}\ \max_{p\in\mathcal{P}}\sum_{t=1}^ {T}p(n_{i_{t},p})\,-\,\sum_{t=1}^{T}p_{t}(n_{i_{t},p_{t}}).\] (4)

**Given:** Approximation parameter \(\epsilon>0\).

Let \(W\) be discretization of the valuation space \([0,1]\) defined as follows,

\[Z_{i} \stackrel{{\Delta}}{{=}}\left\{\epsilon(1+\epsilon)^{i}; \quad\forall\ i\in\left\{0,1,\ldots,\left\lceil\log_{1+\epsilon}\frac{1}{ \epsilon}\right\rceil\right\}\right\},\] \[W_{i} \stackrel{{\Delta}}{{=}}\left\{Z_{i-1}+Z_{i-1}\cdot \frac{\epsilon k}{m};\ \ \forall\ k\in\left\{1,2,...,\left\lceil(2+\epsilon)m\right\rceil\right\},\quad W\stackrel{{\Delta}}{{=}}\bigcup_{i=1}^{\left\lceil\log_{1+ \epsilon}\frac{1}{\epsilon}\right\rceil}W_{i}.\]

Set \(\overline{\mathcal{P}}\) to be the class of all "\(m\)-step" functions mapping \([N]\) to \(W\).

**Algorithm 1** Price discretization scheme under monotonicity

## 3 Efficient discretization of price curves with small errors

We first study the revenue maximization problem in the offline setting, where the seller knows both the valuation curves \(v_{i},i\in[m]\), and the type distribution \(q\). Our goal is to design a discretization so as to achieve revenue within a gap of \(\mathcal{O}(\epsilon)\) from \(\mathrm{OPT}\). Before discussing our discretization algorithms, we first show that the optimal pricing curve is "simple" when there are at most \(m\) types.

**Lemma 3.1**.: _Assume there are \(m\) types with non-decreasing value curves \(\{v_{i}\}_{i\in[m]}\). For any non-decreasing price curve \(p\), there exists an "\(m\)-step" price curve \(\bar{p}\) that yields expected revenue at least that of \(p\) with respect to any distribution over the \(m\) types. Here, \(m\)-step refers to non-decreasing functions \(f:[N]\rightarrow[0,1]\) where \(f(n+1)-f(n)>0\) in at most \(m\) points (i.e., at most \(m\) jumps)._

Lemma 3.1, proven in Appendix A.1, will be an important tool in all three discretization algorithms of this section. It will allow us to reduce the space of pricing curves as we only need to focus on \(m\)-step price curves. Next, we present our first discretization procedure in Algorithm 1, which only assumes the monotonicity of the valuation curves.

**Discretization scheme under monotonic valuations.** Our discretization procedure, outlined in Algorithm 1, adapts the method in Hartline and Koltun [25] using Lemma 3.1. For this, we will first construct a discretization \(W\) of the valuation space as follows. Let \(Z_{i}=\epsilon(1+\epsilon)^{i}\), \(i=0,1,\ldots,\left\lceil\log_{1+\epsilon}\frac{1}{\epsilon}\right\rceil\) be the powers of \((1+\epsilon)\) on price space \([\epsilon,1]\). For each \(i\), we let \(W_{i}\) be a uniform discretization of the interval \([Z_{i-1},Z_{i+1})\) uniformly with gap \(Z_{i-1}\cdot\frac{\epsilon}{m}\). Finally, let \(W\) be the union of all such \(W_{i}\). According to Lemma 3.1, every price function in \(\mathcal{P}\) has the same revenue as an \(m\)-step function. We set \(\overline{\mathcal{P}}\) to be all choices of non-decreasing \(m\)-step functions that take value in \(W\). We have the following theorem about Algorithm 1 which we prove in Appendix A.2.

**Theorem 3.1**.: _Consider the discretization \(\overline{\mathcal{P}}\) as constructed in Algorithm 1. For any type distribution, there exists \(p\in\overline{\mathcal{P}}\) such that \(\mathrm{rev}(p)\geq\mathrm{OPT}-\mathcal{O}(\epsilon)\). Moreover, we have \(\left|\overline{\mathcal{P}}\right|\leq\left(\frac{e(N-1)}{m}\right)^{m}\left(e \lceil(2+\epsilon)\rceil\left\lceil\log_{1+\epsilon}\frac{1}{\epsilon}\right \rceil\right)^{m}\in\widetilde{\mathcal{O}}\left(\left(\frac{N}{\epsilon} \right)^{m}\right)\)._

**Discretization scheme for smooth monotonic valuations.** Due to space constraints, we present our algorithm, under Assumption 1 in Appendix A.4. We have the following theorem about Algorithm 5.

**Theorem 3.2**.: _Consider the discretization \(\overline{\mathcal{P}}\) as constructed in Algorithm 5. Under Assumption 1, for any type distribution, there exists \(p\in\overline{\mathcal{P}}\) such that \(\mathrm{rev}(p)\geq\mathrm{OPT}-\mathcal{O}(\epsilon)\). Moreover, \(\left|\overline{\mathcal{P}}\right|\in\mathcal{O}\left(\log_{1+\epsilon}^{m} \left(1/\epsilon\right)\cdot\left(L/\epsilon\right)^{m}\right)\in\widetilde{ \mathcal{O}}\left(\left(\frac{L}{\epsilon^{2}}\right)^{m}\right)\)._

**Discretization scheme for monotone valuations under diminishing returns.** Finally, we study discretization schemes under the diminishing returns condition. Our procedure, outlined in Algorithm 2 proceeds as follows. We use the same discretization \(W\) of the valuation space from Algorithm 1. Next, we will discretize the dataspace \([N]\). To exploit the structure in the diminishing returns condition, we will need to do so more densely when \(n\) is small. For this, let \(Y_{i}=\frac{2Jm}{\epsilon^{2}}(1+\epsilon^{2})^{i}\), \(i=0,\ldots,\left\lceil\log_{1+\epsilon^{2}}\frac{N\epsilon^{2}}{2Jm}\right\rceil\) be the powers of \((1+\epsilon^{2})\) on data space \(\left[\frac{2Jm}{\epsilon^{2}},N\right]\). For each \(i\), the set \(Q_{i}\) further partitions the interval \([Y_{i},Y_{i+1})\) uniformly with gap \(Y_{i}\cdot\frac{\epsilon^{2}}{2Jm}\). For \(n\) smaller than \(\frac{2Jm}{\epsilon^{2}}\), we do not discretize it as the valuations may change rapidly when \(n\) is small. Let \(N_{\textbf{D}}\) be the union of\(\left\{1,2,\ldots,\left\lfloor\frac{2Jm}{\epsilon^{2}}\right\rfloor\right\}\) and all the set \(Q_{i}\). Therefore, \(N_{\mathbf{D}}\) has a size of at most \(\frac{2Jm}{\epsilon^{2}}+2Jm\lceil\log_{1+\epsilon^{2}}\frac{N\epsilon^{2}}{2Jm}\rceil\). We have the following theorem about Algorithm 2 which we prove in Appendix A.5.

**Theorem 3.3**.: _Consider the discretization \(\overline{\mathcal{P}}\) as constructed in Algorithm 2. Under Assumption 2, for any type distribution, there exists \(p\in\overline{\mathcal{P}}\) such that \(\mathrm{rev}(p)\geq\mathrm{OPT}-\mathcal{O}(\epsilon)\). Moreover,_

\[|\overline{\mathcal{P}}|\in\mathcal{O}\left(\left(\frac{J}{\epsilon^{2}} \right)^{m}\log^{m}\left(\frac{N\epsilon^{2}}{Jm}\right)\cdot\left(\log_{1+ \epsilon}^{m}1/\epsilon\right)\right)\in\widetilde{\mathcal{O}}\left(\left( \frac{J}{\epsilon^{3}}\right)^{m}\right).\]

Proof outline.: By Lemma 3.1, we may assume the optimal price curve \(p^{\star}=\left\{(n_{i}^{\star},p_{i}^{\star})\right\}_{i=1}^{m}\) is an \(m\)-step function, where \(p_{i}^{\star}\) denote the value of \(p\) on step \(i\). We generate an \(m\)-step price curve \(p=\left\{(n_{i},p_{i})\right\}_{i=1}^{m}\) on space \(N_{\mathbf{D}}\to W\) such that \(n_{i}\) is obtained by rounding down \(n_{i}^{\star}\) to the closest value in \(N_{\mathbf{D}}\), and \(p_{i}\geq p_{i}^{\star}/(1+\epsilon)\). We then show that if a buyer purchases at step \(i\) under price \(p^{\star}\), she will not purchase at step \(j<i\) under new price \(p\). Therefore, the revenue from this buyer is at least \(p_{i}\geq p_{i}^{\star}/(1+\epsilon)=p_{i}^{\star}-\mathcal{O}(\epsilon)\), which ensures that \(\mathrm{rev}(p)\geq\mathrm{OPT}-\mathcal{O}(\epsilon)\).

## 4 Online learning in the stochastic setting

We now study the online learning problem outlined in SS2.1 in the stochastic setting. Our Algorithm, outlined in Algorithm 3 is based on the classical upper confidence bound (UCB) algorithm for stochastic bandits [8; 38]. It takes a discretization \(\overline{\mathcal{P}}\) of the pricing curves as input, and on each round chooses a \(p_{t}\in\overline{\mathcal{P}}\) which has the largest UCB on the revenue.

The key challenge lies in constructing an UCB. As \(\overline{\mathcal{P}}\) is large, naively constructing UCB over prices in \(\overline{\mathcal{P}}\) will lead to a \(\sqrt{|\overline{\mathcal{P}}|T\log T}\) upper bound, leading to poor, exponential dependence on \(m\). This is the bound if we only observe the reward for the prices that are actually pulled, but do not observe the types after purchase. Therefore, naively applying UCB is like bandit feedback. On the other extreme, had we been in an alternative setting where we observe the type regardless of purchase, this is like a full information feedback because once observe the type, we know the revenue for all prices. Then UCB gives us \(\sqrt{\log(|\overline{\mathcal{P}}|)T\log T}\) upper bound. We are in an intermediate regime between bandit feedback and full information: The challenge in constructing the UCB arises because we only observe types upon purchase. As the key unknown is the type distribution, we maintain UCBs for it and translate them to UCBs for the revenue. In particular, our UCB depends on how many times a buyer _could_ have purchased at a given round, which is a random quantity depending on the algorithm itself.

Construction of UCB.We will now show how to construct the upper confidence bound \(\widehat{\mathrm{rev}}_{t}\) at the end of round \(t\), which will be used in computing \(p_{t+1}\). For \(\tau\leq t\), let \(S_{\tau}\), defined below in (5), be the set of types who would have purchased in round \(\tau\) at price \(p_{\tau}\) had they appeared in that round. Then, for any type \(i\in[m]\), we define \(T_{i,t}\) to be the number of times that type \(i\) appears in set \(S_{\tau}\) for \(\tau\in\{1,\dots,t\}\). That is, \(T_{i,t}\) measures the number of times a buyer of type \(i\) would have purchased during the first \(t\) rounds. We have,

\[S_{\tau}\stackrel{{\Delta}}{{=}}\big{\{}i\in[m]:\exists n\in[N],v_ {i}(n)-p_{\tau}(n)\geq 0\big{\}},\qquad T_{i,t}\stackrel{{\Delta}}{{=}} \sum_{\tau=1}^{t}\mathbbm{I}(i\in S_{\tau}).\] (5)

Note that as we use the \(0\) price function on round 1, i.e. \(p_{1}(\cdot)=0\), we have \(T_{i,t}>0\) for all \(t>1\). Next, we estimate \(q_{i}\) via the fraction of times that type \(i\) has appeared in the past \(t\) rounds, provided that \(i\in S_{\tau}\) for \(\tau\in\{1,\dots,t\}\). We have defined this quantity, \(\overline{q}_{i,t}\) below in (6). Via a standard application of Hoeffding's inequality, we can show that \(\big{|}q_{i}-\overline{q}_{i,t}\big{|}\leq\sqrt{(\log T)/T_{i,t}}\) with high probability. Using this, we can construct an upper confidence bound \(\widehat{q}_{i,t}\) as follows,

\[\overline{q}_{i,t}\stackrel{{\Delta}}{{=}}\frac{1}{T_{i,t}}\sum _{\tau=1}^{t}\mathbbm{I}(i\in S_{\tau},i_{\tau}=i),\qquad\quad\widehat{q}_{i, t}\stackrel{{\Delta}}{{=}}\overline{q}_{i,t}+\sqrt{\frac{\log T}{T_{i,t}}}.\] (6)

We now translate the UCBs on \(q\) to the UCBs on the revenue. Recall from (1) that a buyer of type \(i\) will purchase \(n_{i,p}\) points at price \(p\) and the revenue from this buyer will be \(p(n_{i,p})\). Note that as the seller has access to the valuation curves, he can compute \(n_{i,p}\) for any \(i\) and price curve \(p\). Since \(\operatorname{rev}(p)=\mathbb{E}_{i\sim q}[p(n_{i,p})]\), we have the following natural UCB for \(\operatorname{rev}(p)\) on round \(t\):

\[\widehat{\operatorname{rev}}_{t}(p)\ \stackrel{{\Delta}}{{=}}\ \sum_{i=1}^{m}\widehat{q}_{i,t}\cdot p(n_{i,p}).\] (7)

This completes the description of our construction. The following theorem bounds the regret for Algorithm 3 when paired with any of the discretization schemes in SS3. While the computational complexity of our method depends on \(|\mathcal{P}|\), there is no dependence on the regret because of the above construction of the UCB. The proof is given in Appendix C.

**Theorem 4.1**.: _Suppose in Algorithm 3 we use a discretization \(\overline{\mathcal{P}}\) which is a \(\mathcal{O}(1/\sqrt{T})\) additive approximation to any price curve. Then, the regret of Algorithm 3 satisfies \(\mathbb{E}[R_{T}]\in\widetilde{\mathcal{O}}(m\sqrt{T})\)._

Proof challenges.: When bounding the regret, we first observe that the subsets \(S\subset[m]\) induces a partitioning of the price curves, where \(p\) belongs to the partition of \(S\), if all types in \(S\) would make a purchase at price \(p\), and all types in \(S^{c}\) would not make a purchase at price \(p\). With this insight, we can view the action of a seller as not just choosing a price curve, but also choosing a set \(S_{t}\subset[n]\). That is, \(S_{t}\) can be viewed as a super-arm in a combinatorial semi-bandit problem [37].

## 5 Online learning in the adversarial setting

We now study the adversarial setting. Similar to the stochastic setting, our algorithm will use a discretization of the price curves from SS3. We will control regret by bounding both the discretization error and the algorithm's regret relative to the best pricing curve in the discretization.

Before proceeding, let us first contextualize our feedback model against prior work. If the buyers do not reveal their types, this becomes an adversarial bandit problem with \(|\overline{\mathcal{P}}|\) arms (pricing curves) [33]. Using an algorithm such as EXP-3 [9] results in large \(\widetilde{\mathcal{O}}(T^{\nicefrac{{1}}{{2}}}|\overline{\mathcal{P}}|^{ \nicefrac{{1}}{{2}}})\) regret, which is not ideal due to \(|\overline{\mathcal{P}}|\)'s exponential dependence in \(m\). Conversely, if buyers reveal their types regardless of purchase,this is equivalent to full information feedback, where algorithms such as Hedge or Follow-the-perturbed-leader (FTPL) [31] yield \(\mathcal{O}(T^{\nicefrac{{1}}{{2}}}\log^{\nicefrac{{1}}{{2}}}|\overline{\mathcal{P}} |)\) regret, translating to \(\widetilde{\mathcal{O}}((mT)^{\nicefrac{{1}}{{2}}})\) with our discretization schemes in SS3. In our intermediate regime, where feedback is only revealed upon purchase, we aim for a middle ground. We show our algorithm, outlined in Algorithm4, achieves \(\widetilde{\mathcal{O}}(m^{3/2}T^{\nicefrac{{1}}{{2}}})\) regret, which is worse than full information, but still depends polynomially on \(m\).

Our algorithm takes a discretization \(\overline{\mathcal{P}}\) and a perturbation parameter \(\theta\) as input. First, it samples a random perturbation \(\theta_{p}\) from an exponential distribution with pdf \(\theta e^{-\theta x}\) for each pricing curve \(p\) in \(\overline{\mathcal{P}}\). It maintains rewards \(\{r_{t}(p)\}_{t,p}\) for each round \(t\) and price curve \(p\). On each round, it chooses the price curve that maximizes the perturbed cumulative reward \(\sum_{\tau=1}^{t}r_{\tau}(p)+\theta_{p}\).

This scheme is similar to FTPL, but the key difference is in how we design the rewards \(\{r_{t}(p)\}_{t,p}\). To describe this, let \(S_{t}\), defined exactly as in (5), be the set of agents who would have purchased in round \(t\) at price \(p_{t}\). At the end of the round, if there was a purchase, for all prices \(p\in\overline{\mathcal{P}}\), we set the reward to be \(r_{t}(p)=p(n_{i_{t},p})\), i.e. the payment we would have received from the buyer at that round, had the price been \(p\) (see (1)). If there was no purchase, we know that \(i_{t}\notin S_{t}\), in which case we set \(r_{t}(p)=\sum_{i\in S_{t}^{c}}p(n_{i,p})\). In this case, \(r_{t}(p)\) is an upper bound on \(p(n_{i_{t},p})\), and this upper bound is tight around prices similar to the chosen price \(p_{t}\); in fact, \(r_{t}(p_{t})=0\) if there was no purchase. Intuitively, \(r_{t}(p)\) deals with the uncertainty of not knowing the type on round \(t\) by providing a large reward (as we are taking the sum) to prices that _could have_ resulted in a purchase, which encourages exploration of such prices in future rounds. This intuition will help us bound the regret.

Theorem5.1 provides a bound on the regret for Algorithm4. Its proof is given in AppendixB. Combining this with the size of \(\overline{\mathcal{P}}\) under the various assumptions in SS3, we obtain \(\widetilde{\mathcal{O}}(m^{\nicefrac{{3}}{{2}}}\sqrt{T})\) regret.

**Theorem 5.1**.: _Suppose in Algorithm4 we use a discretization \(\overline{\mathcal{P}}\) which is a \(\mathcal{O}(1/\sqrt{T})\) additive approximation to any price curve. Let \(R_{T}\) be as defined in (4). Then, for Algorithm4, we have \(\mathbb{E}[R_{T}]\ \in\ \mathcal{O}\left(m^{2}\theta T+\theta^{-1}\left(1+\log \left|\overline{\mathcal{P}}\right|\right)\right)\). Setting \(\theta=\sqrt{\frac{1+\log\left|\overline{\mathcal{P}}\right|}{m^{2}T}}\), we have \(\mathbb{E}[R_{T}]\ \in\ \mathcal{O}\big{(}m\sqrt{T\log\left|\overline{ \mathcal{P}}\right|}\big{)}\)._

## 6 Conclusion and Discussion

We designed revenue-optimal learning algorithms for pricing data. First, we leveraged properties like smoothness and diminishing returns to create novel discretization schemes for approximating any pricing curve. These schemes were then used in our learning algorithms to improve their statistical and computational properties. Our algorithms build on classical methods like UCB and FTPL but required significant adaptations to handle the vast space of pricing curves and the asymmetric feedback. An interesting future direction would be to relax the assumption that the seller knows the valuation curves \(v_{i}\).

Computational complexity.Our algorithm is designed to achieve polynomial computational complexity with respect to the number of data points when the number of types is fixed, making it suitable for practical data pricing scenarios where the type count is typically small or bounded. While the overall computational cost grows exponentially with the number of types due to the problem's strong NP-hardness (see [14]), this design choice ensures computational feasibility in settings with large datasets and a limited number of types.

## References

* [1] AWS Forecast. https://aws.amazon.com/forecast/,. Accessed: 2024-05-12.
* [2] AWS Data Hub. https://aws.amazon.com/blogs/big-data/tag/datahub/,. Accessed: 2024-05-11.
* [3] Azure Data Share. https://azure.microsoft.com/en-us/products/data-share. Accessed: 2024-05-10.
* [4] Delta Sharing. https://docs.databricks.com/en/data-sharing/index.html. Accessed: 2024-05-11.
* [5] Ads Data Hub. https://developers.google.com/ads-data-hub/guides/intro. Accessed: 2022-05-10.
* [6] A. Agarwal, M. Dahleh, and T. Sarkar. A marketplace for data: An algorithmic solution. In _Proceedings of the 2019 ACM Conference on Economics and Computation_, pages 701-726, 2019.
* [7] A. Agarwal, M. Dahleh, T. Horel, and M. Rui. Towards data auctions with externalities. _arXiv preprint arXiv:2003.08345_, 2020.
* [8] P. Auer. Using confidence bounds for exploitation-exploration trade-offs. _Journal of Machine Learning Research_, 3(Nov):397-422, 2002.
* [9] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. _SIAM journal on computing_, 32(1):48-77, 2002.
* [10] M. F. Balcan and H. Beyhaghi. New guarantees for learning revenue maximizing menus of lotteries and two-part tariffs. _Transactions on Machine Learning Research_, 2024.
* [11] O. Besbes and A. Zeevi. Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms. _Operations Research_, 57(6):1407-1420, 2009.
* [12] O. Besbes and A. Zeevi. On the (surprising) sufficiency of linear models for dynamic pricing with demand learning. _Management Science_, 61(4):723-739, 2015.
* [13] S. Chawla, J. D. Hartline, and R. Kleinberg. Algorithmic pricing via virtual valuations. In _Proceedings of the 8th ACM Conference on Electronic Commerce_, pages 243-251, 2007.
* [14] S. Chawla, R. Rezvan, Y. Teng, and C. Tzamos. Pricing ordered items. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 722-735, 2022.
* [15] W. Chen, W. Hu, F. Li, J. Li, Y. Liu, and P. Lu. Combinatorial multi-armed bandit with general reward functions. _Advances in Neural Information Processing Systems_, 29, 2016.
* [16] X. Chen, I. Diakonikolas, D. Paparas, X. Sun, and M. Yannakakis. The complexity of optimal multidimensional pricing. In _Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms_, pages 1319-1328. SIAM, 2014.
* [17] W. C. Cheung, D. Simchi-Levi, and H. Wang. Dynamic pricing and demand learning with limited price experimentation. _Operations Research_, 65(6):1722-1731, 2017.
* Accelerating Materials Innovation. URL: https://citrine.io/, 2024. Accessed: March 9, 2024.
* [19] A. V. Den Boer. Dynamic pricing and learning: Historical origins, current research, and new directions. _Surveys in Operations Research and Management Science_, 20(1):1-18, 2015.
* [20] A. V. den Boer and B. Zwart. Simultaneously learning and optimizing using controlled variance pricing. _Management Science_, 60(3):770-783, 2014.
* [21] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.

* Dudik et al. [2020] M. Dudik, N. Haghtalab, H. Luo, R. E. Schapire, V. Syrgkanis, and J. W. Vaughan. Oracle-efficient online learning and auction design. _Journal of the ACM (JACM)_, 67(5):1-57, 2020.
* Guo et al. [2023] W. Guo, N. Haghtalab, K. Kandasamy, and E. Vitercik. Leveraging reviews: Learning to price with buyer and seller uncertainty. In _Proceedings of the 24th ACM Conference on Economics and Computation_, pages 816-816, 2023.
* Guruswami et al. [2005] V. Guruswami, J. D. Hartline, A. R. Karlin, D. Kempe, C. Kenyon, and F. McSherry. On profit-maximizing envy-free pricing. In _SODA_, volume 5, pages 1164-1173, 2005.
* Hartline and Koltun [2005] J. D. Hartline and V. Koltun. Near-optimal pricing in near-linear time. In _Proceedings of the 9th International Conference on Algorithms and Data Structures_, WADS'05, page 422-431, Berlin, Heidelberg, 2005. Springer-Verlag. ISBN 3540281010. doi: 10.1007/11534273_37. URL https://doi.org/10.1007/11534273_37.
* He et al. [2016] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Jagadeesan et al. [2021] M. Jagadeesan, A. Wei, Y. Wang, M. Jordan, and J. Steinhardt. Learning equilibria in matching markets from bandit feedback. _Advances in Neural Information Processing Systems_, 34:3323-3335, 2021.
* Javanmard [2017] A. Javanmard. Perishability of data: dynamic pricing under varying-coefficient models. _The Journal of Machine Learning Research_, 18(1):1714-1744, 2017.
* Javanmard and Nazerzadeh [2019] A. Javanmard and H. Nazerzadeh. Dynamic pricing in high-dimensions. _The Journal of Machine Learning Research_, 20(1):315-363, 2019.
* Jia et al. [2019] R. Jia, D. Dao, B. Wang, F. A. Hubis, N. Hynes, N. M. Gurel, B. Li, C. Zhang, D. Song, and C. J. Spanos. Towards efficient data valuation based on the Shapley value. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1167-1176. PMLR, 2019.
* Kalai and Vempala [2005] A. Kalai and S. Vempala. Efficient algorithms for online decision problems. _Journal of Computer and System Sciences_, 71(3):291-307, 2005.
* Keskin and Zeevi [2014] N. B. Keskin and A. Zeevi. Dynamic pricing with an unknown demand model: Asymptotically optimal semi-myopic policies. _Operations Research_, 62(5):1142-1167, 2014.
* Kleinberg and Leighton [2003] R. Kleinberg and T. Leighton. The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In _44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings._, pages 594-605. IEEE, 2003.
* Krause and Guestrin [2011] A. Krause and C. Guestrin. Submodularity and its applications in optimized information gathering. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 2(4):1-20, 2011.
* Krause et al. [2008] A. Krause, H. B. McMahan, C. Guestrin, and A. Gupta. Robust submodular observation selection. _Journal of Machine Learning Research_, 9(12), 2008.
* Krizhevsky et al. [2012] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.
* Kveton et al. [2015] B. Kveton, Z. Wen, A. Ashkan, and C. Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. In _Artificial Intelligence and Statistics_, pages 535-543. PMLR, 2015.
* Lai and Robbins [1985] T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. _Advances in applied mathematics_, 6(1):4-22, 1985.
* Misra et al. [2019] K. Misra, E. M. Schwartz, and J. Abernethy. Dynamic online pricing with incomplete information using multiarmed bandit experiments. _Marketing Science_, 38(2):226-252, 2019.
* Perakis and Singhvi [2023] G. Perakis and D. Singhvi. Dynamic pricing with unknown nonparametric demand and limited price changes. _Operations Research_, 2023.

* [41] S. Shalev-Shwartz and S. Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* [42] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1-9, 2015.
* [43] T. Wang, J. Rausch, C. Zhang, R. Jia, and D. Song. A principled approach to data valuation for federated learning. _Federated Learning: Privacy and Incentive_, pages 153-167, 2020.
* [44] Y. Wang, B. Chen, and D. Simchi-Levi. Multimodal dynamic pricing. _Management Science_, 67(10):6136-6152, 2021.
* [45] L. Wasserman. _All of nonparametric statistics_. Springer Science & Business Media, 2006.
* [46] J. Xu and Y.-X. Wang. Logarithmic regret in feature-based dynamic pricing. _Advances in Neural Information Processing Systems_, 34:13898-13910, 2021.
* [47] H. Zhao and W. Chen. Stochastic one-sided full-information bandit. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 150-166. Springer, 2019.

## Appendix A Omitted Details from Section 3

### Proof of Lemma 3.1

**Lemma 3.1**.: _Assume there are \(m\) types with non-decreasing value curves \(\{v_{i}\}_{i\in[m]}\). For any non-decreasing price curve \(p\), there exists an "\(m\)-step" price curve \(\bar{p}\) that yields expected revenue at least that of \(p\) with respect to any distribution over the \(m\) types. Here, \(m\)-step refers to non-decreasing functions \(f:[N]\to[0,1]\) where \(f(n+1)-f(n)>0\) in at most \(m\) points (i.e., at most \(m\) jumps)._

Proof of Lemma 3.1.: Fix a price curve \(p\). Let \(n_{i,p}\) be the amount of data type \(i\) purchase at price curve \(p\), that is

\[n_{i,p}\stackrel{{\Delta}}{{=}}\max\left\{\underset{n\in[N]}{ \operatorname{argmax}}(v_{i}(n)-p(n))\right\}.\]

For \(\{n_{i,p}\}_{i\in[m]}\), let \(\pi:[m]\to[m]\) be a permutation such that \(n_{\pi(1),p}\leq n_{\pi(2),p}\leq\cdots\leq n_{\pi(m),p}\). Let \(n_{(i)}\stackrel{{\Delta}}{{=}}n_{\pi(i),p}\). Then, define a function \(\bar{p}:[N]\to[0,1]\) as follows,

\[\bar{p}(n)\stackrel{{\Delta}}{{=}}\begin{cases}p\left(n_{(1)} \right),&n\leq n_{(1)},\\ p\left(n_{(2)}\right),&n_{(1)}<n\leq n_{(2)},\\ &\vdots\\ p\left(n_{(m-1)}\right),&n_{(m-2)}<n\leq n_{(m-1)},\\ p\left(n_{(m)}\right),&n_{(m-1)}<n\leq N,\end{cases}\]

so that \(\bar{p}\) has at most \(m\) steps. Then, \(\bar{p}\) has following properties,

\[\bar{p}(n) =p(n),\text{ when }n\in\left\{n_{(1)},n_{(2)},\ldots,n_{(m)}\right\},\] \[\bar{p}(n) \leq p(n),\text{ when }n\in[N]\setminus\left\{n_{(1)},n_{(2)}, \ldots,n_{(m)}\right\}.\]

We next prove that for any \(i\in[m]\), after changing the price function from \(p\) to \(\bar{p}\), the type \(i\) buyer either purchases at \((n_{i,p},p(n_{i,p}))\) or at \((N,p(n_{(m)}))\).

For any type \(i\) and any amount of data \(n\leq n_{(m)}\), there exists \(k\) such that \(n_{(k-1)}<n\leq n_{(k)}\) (let \(n_{(0)}=0\)), we then have

\[v_{i}(n)-\bar{p}(n) \leq v_{i}\left(n_{(k)}\right)-\bar{p}\left(n_{(k)}\right)\] (as

\[v_{i}\]

 is non-decreasing and

\[\bar{p}\]

 is a step function.) \[=v_{i}\left(n_{(k)}\right)-p\left(n_{(k)}\right)\] (as

\[\bar{p}\left(n_{(k)}\right)=p\left(n_{(k)}\right)\]\[\leq v_{i}(n_{i,p})-p(n_{i,p})\] (as \[n_{i,p}\] maximizes the buyer's utility.) \[=v_{i}(n_{i,p})-\bar{p}(n_{i,p}).\] (as \[\bar{p}(n_{i,p})=p(n_{i,p})\] )

As shown in the above, type \(i\) still prefers purchasing \(n_{i,p}\) data over all \(n\leq n_{(m)}\) under price \(\bar{p}\).

For \(n\in\left\{n_{(m)}+1,\ldots,N\right\}\), by the monotonicity of value curves, we have

\[N=\max\left\{\operatorname*{arg\,max}_{n\in\left\{n_{(m)}+1,\ldots,N\right\}} \left(v_{i}(n)-\bar{p}(n)\right)\right\}.\]

Therefore, for any \(i\in[m]\), type \(i\) either purchases at \((n_{i,p},p(n_{i,p}))\), or purchases at \((N,\bar{p}(N))=(N,p(n_{(m)}))\) under price \(\bar{p}\). No matter in which case, type \(i\) contributes no less revenue under \(\bar{p}\) than \(p\). It then follows that, for any type distribution \(q\),

\[\operatorname*{rev}(\bar{p})\geq\operatorname*{rev}(p).\]

### Proof of Theorem 3.1

In this subsection, we prove Theorem 3.1 by decomposing it into three technical lemmas (Lemma A.1, A.2 and A.3). In Lemma A.1 and A.2, we prove the approximation guarantee of our discretization scheme and, in Lemma A.3 we provide an upper bound on the size of the discretization.

**Lemma A.1**.: _For any type distribution, there exists a pricing function \(\widetilde{p}:[N]\to[\epsilon,1]\) such that_

\[\operatorname*{rev}(\widetilde{p})\geq\operatorname*{OPT}-\epsilon.\]

Proof of Lemma a.1.: Consider the optimal pricing function \(p^{\star}:[N]\to[0,1]\), i.e., \(\operatorname*{OPT}=\operatorname*{rev}(p^{\star})\). Consider price curve \(\widetilde{p}:[N]\to[\epsilon,1]\) where \(\widetilde{p}(n)=\max\left(\epsilon,p^{\star}(n)\right)\).

Let \(J\stackrel{{\Delta}}{{=}}\{n\in[N]:\widetilde{p}(n)=p^{\star}(n)\}\) be the set of data quantities whose price under \(\widetilde{p}\) are the same as those under \(p\). Any buyer type who would have purchased \(n\in J\) amount of data under \(p^{\star}\) will purchase the same amount of data under \(\widetilde{p}\). On the other hand, for buyer types who would have purchased \(n\notin J\) amount of data under \(p^{\star}\), since \(\widetilde{p}(n)=\epsilon>p^{\star}(n)\) for \(n\notin J\), the expected revenue contribution from such buyers under \(p^{\star}\) is at most \(\epsilon\), hence no matter they purchase or not under \(\widetilde{p}\), we have \(\operatorname*{rev}(\widetilde{p})\geq\operatorname*{OPT}-\epsilon\). 

**Lemma A.2**.: _For any \(\widetilde{p}\in[\epsilon,1]^{N}\) there exists \(p^{\prime}\in\overline{\mathcal{P}}\) such that \(\operatorname*{rev}(p^{\prime})\geq\operatorname*{rev}(\widetilde{p})/(1+\epsilon)\), for any type distribution \(q\)._

Proof of Lemma a.2.: For \(m\) buyer types, by Lemma 3.1, there exists a non-decreasing step function \(\bar{p}\in[\epsilon,1]^{N}\) with at most \(m\) steps, whose expected revenue is at least \(\operatorname*{rev}(\widetilde{p})\). Assume \(\bar{p}\) has \(k\) steps, \(k\leq m\). To simplify the notation, for \(1\leq j\leq k\), let \(\bar{p}_{j}\) denote the price \(\bar{p}\) on \(j\)th step. That is,

\[\bar{p}(n)=\begin{cases}\bar{p}_{1},&n\in(0,i_{1}]\cap\mathbb{Z},\\ \bar{p}_{2},&n\in(i_{1},i_{2}]\cap\mathbb{Z},\\ &\vdots\\ \bar{p}_{k},&n\in(i_{k-1},N]\cap\mathbb{Z}.\end{cases}\]

Where \(i_{1},\ldots,i_{k-1}\in[N]\) are discontinuities in \(\bar{p}\).

Recall the definitions of \(Z\) and \(W\) as stated in Algorithm 1,

\[Z_{i}\stackrel{{\Delta}}{{=}}\left\{\epsilon(1+\epsilon)^{i}: \forall\;i\in\left\{0,1,\ldots,\left\lceil\log_{1+\epsilon}\frac{1}{\epsilon} \right\rceil\right\}\right\},\;Z=\bigcup_{i}Z_{i}.\]

\[W_{i}\stackrel{{\Delta}}{{=}}\left\{Z_{i-1}+Z_{i-1}\cdot\frac{ \epsilon k}{m}:\forall\,k\in\{1,2,...,\left\lceil(2+\epsilon)m\right\rceil\} \right\},\quad W\stackrel{{\Delta}}{{=}}\bigcup_{i=1}^{\lceil \log_{1+\epsilon}\frac{1}{\epsilon}\rceil}W_{i}.\]Let \(i_{k}=N\) and for each \(j\in[k]\), let \(Z_{i_{j}}\) be the price obtained by rounding \(\bar{p}_{j}\) down to the nearest value in \(Z\). By constructions of \(Z\) and \(W\) above, \(W_{i_{j}}\) is a partition of interval \((Z_{i_{j}-1},Z_{i_{j}+1})\). Let \(w_{j}\) be the price obtained by rounding \(\bar{p}_{j}\) down to the nearest value in \(W_{i_{j}}\). Set \(d_{j}\triangleq\frac{\epsilon}{m}\cdot Z_{i_{j}-1}\) and consider \(k\)-step function \(p\) defined by whose price at \(j\)th step (denoted \(p_{j}\)) is \(w_{j}-(j-1)d_{j}\in W_{i_{j}}\), that is

\[p(n)=\begin{cases}p_{1}=w_{1},&\text{for }n\in(0,i_{1}]\cap\mathbb{Z},\\ p_{2}=w_{2}-d_{2},&\text{for }n\in(i_{1},i_{2}]\cap\mathbb{Z},\\ \quad\quad\vdots\\ p_{k}=w_{k}-(k-1)d_{k},&\text{for }n\in(i_{k-1},N]\cap\mathbb{Z}.\end{cases}\]

By the tie-breaking rule and the monotonicity of valuation curves, buyers only purchase among \(0,i_{1},i_{1},\ldots,i_{k}\) number of data under \(p\) and \(\bar{p}\).

_Subclaim._ Then, \(p\) and \(\bar{p}\) satisfies the following

\[\operatorname{rev}(p)\geq\operatorname{rev}(\bar{p})/(1+\epsilon),\] (8)

with respect to any type distribution.

_Proof of the Subclaim._ We prove the above subclaim with two steps.

**Step 1**: No buyer who prefers to purchase \(i_{j}\) data under \(\bar{p}\) would prefer \(i_{j^{\prime}}\) data for some \(j^{\prime}<j\) under \(p\) (i.e., one with a less price). This is because, when going from price \(\bar{p}\) to \(p\), the increase in the buyer's utility for \(i_{j}\) data is \(\bar{p}_{j}-p_{j}\), which is higher than the increase \(\bar{p}_{j^{\prime}}-p_{j^{\prime}}\) for \(i_{j^{\prime}}\) data. Formally, this can be seen as follows: For any \(j^{\prime}<j\) we have,

\[\bar{p}_{j}-p_{j}\geq w_{j}-p_{j}=(j-1)d_{j},\]

as \(\bar{p}_{j}\geq w_{j}\) and \(p_{j}=w_{j}-(j-1)d_{j}\). Moreover,

\[\bar{p}_{j^{\prime}}<w_{j^{\prime}}+d_{j^{\prime}}\implies\bar{p}_{j^{\prime} }-p_{j^{\prime}}<w_{j^{\prime}}+d_{j^{\prime}}-p_{j^{\prime}}=j^{\prime}d_{j^{ \prime}}.\] (9)

The inequality \(\bar{p}_{j^{\prime}}<w_{j^{\prime}}+d_{j^{\prime}}\) holds because \(w_{j^{\prime}}\) is the result of rounding down \(\bar{p}_{j}\) to the nearest value in \(W_{i_{j}}\).

By constructions of sets \(Z\) and \(W\), we have \(d_{j}\geq d_{j^{\prime}}\) which implies \((j-1)d_{j}\geq j^{\prime}d_{j^{\prime}}\). Then, by combining the above inequalities, we obtain

\[\bar{p}_{j}-p_{j}\geq(j-1)d_{j}\geq j^{\prime}d_{j^{\prime}}\geq\bar{p}_{j^{ \prime}}-p_{j^{\prime}}.\] (10)

Consider a buyer with value curve \(v\) who prefers to purchase at \(i_{j}\) under price \(\bar{p}\), then it must be

\[v(i_{j})-\bar{p}_{j}>v(i_{j^{\prime}})-\bar{p}_{j^{\prime}}.\] (11)

Then, by combining (10) and (11), we have

\[v(i_{j})-p_{j}>v(i_{j^{\prime}})-p_{j^{\prime}},\]

therefore the buyer would not purchase at \(i_{j^{\prime}}<i_{j}\) under \(p\).

**Step 2**: Next, we claim that \(p_{j}\geq\bar{p}_{j}/(1+\epsilon)\) for all step \(j\in[k]\). Since \(Z_{i_{j}}\) is obtained by rounding \(\bar{p}_{j}\) down to the nearest value in \(Z\), we have

\[\bar{p}_{j}\geq Z_{i_{j}}=Z_{i_{j}-1}+\epsilon Z_{i_{j}-1}=Z_{i_{j}-1}+md_{j}.\] (12)

By (9) and the above, we have

\[p_{j}\geq\bar{p}_{j}-jd_{j}\geq Z_{i_{j}-1}+(m-j)d_{j}\geq Z_{i_{j}-1},\]

where the first inequality is by (9), the second is by (12), and the third is because \(m\leq j\).

Then, it follows that

\[\bar{p}_{j^{\prime}}-p_{j}\leq j\cdot d_{j}=\epsilon\cdot\frac{j}{m}\cdot Z_{i _{j}-1}\leq\epsilon\cdot Z_{i_{j}-1}\leq\epsilon\cdot p_{j}\implies p_{j} \geq\bar{p}_{j}/(1+\epsilon).\]So far we have proved \(p_{j}\geq\bar{p}_{j}/(1+\epsilon)\) and no type wants to change their preference to a smaller amount of data under \(p\). If one type purchase at \(\bar{p}_{i}\) under \(\bar{p}\) and \(p_{k}\) under \(p\) for \(k\geq i\), then \(p_{k}\geq p_{i}\geq\bar{p}_{i}/(1+\epsilon)\). Therefore, we have

\[\operatorname{rev}(p)\geq\operatorname{rev}(\bar{p})/(1+\epsilon)\geq \operatorname{rev}(\bar{p})/(1+\epsilon).\]

Since the construction of price \(p\) is not relevant to type distribution, the above holds for any type distribution \(q\), which proves the subclaim. \(\square\)

Note that \(p\) constructed in the above subclaim is not necessarily non-decreasing as a larger amount of data surfers more price deduction when going from \(\bar{p}\) to \(p\). In this case, we can directly construct a non-decreasing price curve \(p^{\prime}\in\overline{\mathcal{P}}\) from \(p\) such that

\[\operatorname{rev}(p^{\prime})\geq\operatorname{rev}(\bar{p})/(1+\epsilon).\]

Let \(S\stackrel{{\Delta}}{{=}}\{i\in[k]:\exists j<i,\text{ s.t. }p_{j}>p_{i}\}\). If \(S\) is empty, this implies that \(p\) is non-decreasing, hence setting \(p^{\prime}=p\). If \(S\) is not empty, we define \(p^{\prime}\) as follows: Let \(p^{\prime}\) be a \(k\)-step function with the same jump points \(i_{1},\ldots,i_{k}\) as \(p\). Let \(p^{\prime}_{i}\) be the value of \(p^{\prime}\) on \(i\)th step. Then, for \(i\notin S\), let \(p^{\prime}_{i}=p_{i}\); and for \(i\in S\), let \(p^{\prime}_{i}=\max_{j\notin S,j<i}p_{j}\). By construction, \(p^{\prime}\) is non-decreasing. Moreover, \(p^{\prime}=p\) on set \(S^{c}\) and \(p^{\prime}>p\) on set \(S\).

Next, we claim that \(\bar{p}_{j}-p^{\prime}_{j}\) is non-decreasing for all \(j\in[k]\). Both \((\bar{p}_{j}-p_{j})_{j\in[k]}\) and \(\bar{p}\) are non-decreasing with respect to \(j\) by the previous results. Hence,

\[\bar{p}_{j}-p^{\prime}_{j}<\bar{p}_{j}-p^{\prime}_{j}\leq\bar{p}_ {j+1}-p_{j+1}=\bar{p}_{j+1}-p^{\prime}_{j+1},\quad\text{if }j\in S,j+1\notin S,\] \[\bar{p}_{j}-p^{\prime}_{j}=\bar{p}_{j}-p^{\prime}_{j}\leq\bar{p}_ {j+1}-p_{j+1}=\bar{p}_{j+1}-p^{\prime}_{j+1},\quad\text{if }j\notin S,j+1\notin S,\] \[\bar{p}_{j}-p^{\prime}_{j}=\bar{p}_{j}-p^{\prime}_{j+1}\leq\bar{p }_{j+1}-p^{\prime}_{j+1},\quad\text{if }j\notin S,j+1\in S,\] (as \[p^{\prime}_{j+1}=p^{\prime}_{j}\] ) \[\bar{p}_{j}-p^{\prime}_{j}=\bar{p}_{j}-p^{\prime}_{j+1}\leq\bar{p }_{j+1}-p^{\prime}_{j+1},\quad\text{if }j\in S,j+1\in S.\] (as \[p^{\prime}_{j+1}=p^{\prime}_{j}\] )

Therefore, any type that prefers to purchase at \(j\)th step under \(\bar{p}\) would not prefer purchasing at any step \(j^{\prime}<j\) under \(p^{\prime}\), and since \(p^{\prime}_{j}\geq p_{j}\geq\bar{p}_{j}/(1+\epsilon)\), we have

\[\operatorname{rev}(p^{\prime})\geq\operatorname{rev}(\bar{p})/(1+\epsilon) \geq\operatorname{rev}(\bar{p})/(1+\epsilon).\]

\(\square\)

**Lemma A.3**.: _When \(n>m\), \(\left|\overline{\mathcal{P}}\right|\leq\left(\frac{eN}{m}\right)^{m}\left(e \lceil(2+\epsilon)\rceil\left\lceil\log_{1+\epsilon}\frac{1}{\epsilon}\right\rceil \right)^{m}\)._

Proof of Lemma a.3.: For any integer \(i\leq m\), the number of non-decreasing \(i\)-step price function is \(\binom{N-1}{i}\binom{|W|}{i}\), hence we have

\[\left|\overline{\mathcal{P}}\right| =\sum_{i=1}^{m}\binom{N-1}{i}\binom{|W|}{i}\] \[\leq\left(\sum_{i=1}^{m}\binom{N-1}{i}\right)\left(\sum_{i=1}^{ m}\binom{|W|}{i}\right)\] \[\leq\left(\sum_{i=0}^{m}\binom{N-1}{i}\right)\left(\sum_{i=0}^{ m}\binom{|W|}{i}\right)\] \[\leq\left(\frac{e(N-1)}{m}\right)^{m}\left(\frac{e\left|W\right| }{m}\right)^{m}\] \[\leq\left(\frac{e(N-1)}{m}\right)^{m}\left(e\lceil(2+\epsilon) \rceil\left\lceil\log_{1+\epsilon}\frac{1}{\epsilon}\right\rceil\right)^{m}\]

In the last inequality, we use the fact that \(|W|\leq\lceil(2+\epsilon)m\rceil\left\lceil\log_{1+\epsilon}\frac{1}{\epsilon}\right\rceil\). \(\square\)

Finally, Theorem 3.1 follows directly from the above lemmas.

**Theorem 3.1**.: _Consider the discretization \(\overline{\mathcal{P}}\) as constructed in Algorithm 1. For any type distribution, there exists \(p\in\overline{\mathcal{P}}\) such that \(\operatorname{rev}(p)\geq\operatorname{OPT}-\mathcal{O}(\epsilon)\). Moreover, we have \(\left|\overline{\mathcal{P}}\right|\leq\left(\frac{e(N-1)}{m}\right)^{m}\left(e \lceil(2+\epsilon)\rceil\left\lceil\log_{1+\epsilon}\frac{1}{\epsilon}\right \rceil\right)^{m}\in\widetilde{\mathcal{O}}\left(\left(\frac{N}{\epsilon} \right)^{m}\right)\)._

Proof of Theorem 3.1.: Combining Lemma A.1 and Lemma A.2 together, we conclude that there exists price curve \(p^{\prime}\in\overline{\mathcal{P}}\) such that

\[\operatorname{rev}(p^{\prime})\geq\frac{\operatorname{rev}(\tilde{p})}{1+ \epsilon}\geq\frac{\operatorname{OPT}-\epsilon}{1+\epsilon}\geq\operatorname {OPT}-\frac{2\epsilon}{1+\epsilon}=\operatorname{OPT}-\mathcal{O}(\epsilon).\]

The size of \(\overline{\mathcal{P}}\) follows from Lemma A.3. 

### Price discretization scheme for smooth monotonic valuations

We study discretization schemes to approximate monotone valuations under the smoothness condition in Assumption 1. Our procedure is outlined in Algorithm 5. The discretization \(W\) of the valuation space follows Algorithm 1. Additionally, we uniformly split the data space into multiples of \(\left\lfloor\frac{eN}{mL}\right\rfloor\), denoting them as the set \(N_{\mathbf{S}}\). We then set the discretization \(\overline{\mathcal{P}}\) to be the class of all "\(m\)-step" price curves on the function space \(N_{\mathbf{S}}\to W\). The following theorem, proven in Appendix A.4, outlines the main properties of this discretization scheme: the size of the discretization has no dependence on the number of data \(N\).

``` Given: Smoothness constant \(L\), approximation parameter \(\epsilon>0\). Let \(W\) be discretization of the valuation space \([0,1]\) given in Algorithm 1. Let \(N_{\mathbf{S}}\) be the following discretization of the interval \([0,N]\), \[\delta\stackrel{{\Delta}}{{=}}\left\lfloor\frac{eN}{mL}\right\rfloor, \qquad\quad N_{\mathbf{S}}\stackrel{{\Delta}}{{=}}\left\{\delta k :\;k\in\left\lceil\frac{N}{\delta}\right\rceil\right\}.\] Set \(\overline{\mathcal{P}}\) to be the class of all "\(m\)-step" functions mapping \(N_{\mathbf{S}}\to W\). ```

**Algorithm 5** Price discretization scheme for smooth monotonic valuations

### Proof of Theorem 3.2

**Theorem 3.2**.: _Consider the discretization \(\overline{\mathcal{P}}\) as constructed in Algorithm 5. Under Assumption 1, for any type distribution, there exists \(p\in\overline{\mathcal{P}}\) such that \(\operatorname{rev}(p)\geq\operatorname{OPT}-\mathcal{O}(\epsilon)\). Moreover, \(\left|\overline{\mathcal{P}}\right|\in\mathcal{O}\left(\log_{1+\epsilon}^{m} \left(1/\epsilon\right)\cdot\left(L/\epsilon\right)^{m}\right)\in\widetilde{ \mathcal{O}}\left(\left(\frac{L}{\epsilon^{2}}\right)^{m}\right)\)._

Proof of Theorem 3.2.: By Lemma 3.1, there is a revenue optimal price curve \(p^{\star}:[N]\rightarrow[0,1]\) which is a \(k\)-step function, for some \(k\in[m]\). Where \(p^{\star}\) can be compactly represented as the following set of tuples:

\[\left\{(n_{1}^{\star},p_{1}^{\star}),(n_{2}^{\star},p_{2}^{\star}),\ldots,(n_{ k}^{\star},p_{k}^{\star})\right\},\]

where \(n_{1}^{\star},\ldots,n_{k}^{\star}\) denote the locations of jumps and \(p_{i}^{\star}\) denote the value of \(p^{\star}\) on step \(i\in[k]\) (i.e. \(p^{\star}(n)=p_{i}^{\star}\) for \(n\in(n_{i-1}^{\star},n_{i}^{\star}]\)).

Let \(\bar{\epsilon}:=\frac{\epsilon}{m}\). Next, we generate a price \(p^{\prime}\) using Algorithm 6, which ensures that the price curve \(p\) generated in the following step (13) is non-decreasing. We demonstrate that in each round of Algorithm 6, we incur a revenue loss of at most \(\bar{\epsilon}\). If \(p_{i}^{\prime}>p_{i-1}^{\prime}+\bar{\epsilon}\), everything remains the same and thus does not affect the expected revenue. If not, we combine the price of step \(i\) with step \(i-1\), let \(p_{j}^{\prime}\stackrel{{\Delta}}{{=}}p_{j}^{\prime}-\left(p_{i}^ {\prime}-p_{i-1}^{\prime}\right)\) for \(j=i,\ldots,k\). During this process, buyers either make purchases at the same step, or switch to purchase at a higher step. Note that \(p_{i}^{\prime}-p_{i-1}^{\prime}<\bar{\epsilon}\), so the revenue loss of each type is at most \(\bar{\epsilon}\). This implies that the revenue loss in each round is at most \(\bar{\epsilon}\). As there are \(k\) rounds, we lose expected revenue of at most \(m\bar{\epsilon}\). We conclude that \(\operatorname{rev}(p^{\prime})\) is within a gap of \(\epsilon\) from \(\operatorname{OPT}\), i.e., \(\operatorname{rev}(p^{\prime})\geq\operatorname{OPT}-\epsilon\).

After combining some steps in Algorithm 6, Assume that \(p^{\prime}\) is a \(\bar{k}\)-step function (\(\bar{k}\leq k\)) represented by

\[\left\{(n_{1}^{\prime},p_{1}^{\prime}),(n_{2}^{\prime},p_{2}^{\prime}),\ldots,(n_ {\bar{k}}^{\prime},p_{\bar{k}}^{\prime})\right\}.\]

Then, we define a new price curve \(p\in\overline{\mathcal{P}}\) as follows: let \(\delta:=\left\lfloor\frac{\bar{\epsilon}N}{L}\right\rfloor,\) then \(p\) is a \(\bar{k}\)-step function represented by

\[\left\{(n_{1},p_{1}),(n_{2},p_{2}),\ldots,(n_{\bar{k}},p_{\bar{k}})\right\},\]

where

\[n_{i}\stackrel{{\Delta}}{{=}}\left\lfloor\frac{n_{i}^{\prime}}{ \delta}\right\rfloor\delta,\quad p_{i}\stackrel{{\Delta}}{{=}}p_ {i}^{\prime}-i\bar{\epsilon}.\] (13)

First, we show that no buyer who purchases at step \(i\) under \(p^{\prime}\) would purchase at step \(j<i\) under \(p\). Let the buyer's valuation be \(v\). First, we prove that the buyer's utility is non-negative at \(n_{i}\):

\[v(n_{i})-p_{i} \geq v(n_{i}^{\prime})-\delta\cdot\frac{L}{N}-p_{i}\] (by \[L/N\] -Smoothness of \[v\].) \[=v(n_{i}^{\prime})-\delta\cdot\frac{L}{N}-p_{i}^{\prime}+i\bar{\epsilon}\] \[\geq v(n_{i}^{\prime})-\bar{\epsilon}-p_{i}^{\prime}+i\bar{\epsilon}\] (as \[\delta\cdot\frac{L}{N}\leq\frac{L}{N}\cdot\frac{\bar{\epsilon}N}{L}=\bar{ \epsilon}\].) \[=v(n_{i}^{\prime})-p_{i}^{\prime}+(i-1)\bar{\epsilon}\] \[\geq v(n_{i}^{\prime})-p_{i}^{\prime}\] \[\geq 0.\]

Then, we prove that the buyer's utility at \(n_{i}\) is larger than that of \(n_{j}\) for \(j<i\), therefore, the buyer would not prefer buying at step \(j<i\) under price \(p\).

\[v(n_{i})-p_{i}-(v(n_{j})-p_{j}) \geq v(n_{i}^{\prime})-\delta\cdot\frac{L}{N}-v(n_{j}^{\prime})-(p _{i}-p_{j})\] (by \[L/N\] -Smoothness of \[v\].) \[=v(n_{i}^{\prime})-\delta\cdot\frac{L}{N}-v(n_{j}^{\prime})-(p_{ i}^{\prime}-p_{j}^{\prime}-(i-j)\bar{\epsilon})\] \[\geq v(n_{i}^{\prime})-\bar{\epsilon}-v(n_{j}^{\prime})-(p_{i}^{ \prime}-p_{j}^{\prime}-(i-j)\bar{\epsilon})\] (as \[\delta\cdot\frac{L}{N}\leq\frac{L}{N}\cdot\frac{\bar{\epsilon}N}{L}=\bar{ \epsilon}\] ) \[=(v(n_{i}^{\prime})-p_{i}^{\prime})-(v(n_{j}^{\prime})-p_{j}^{ \prime})+(i-j-1)\bar{\epsilon}\] \[\geq(v(n_{i}^{\prime})-p_{i}^{\prime})-(v(n_{j}^{\prime})-p_{j}^ {\prime})\] (as \[i>j\] ) \[\geq 0.\] (as the buyer prefers \[n_{i}\] than \[n_{k}\] under \[p^{\prime}\].)

Finally, fix the type distribution \((q_{1},\ldots,q_{m})\), then we have

\[\mathrm{rev}(p^{\prime})-\mathrm{rev}(p)\leq\sum_{h=1}^{m}q_{h}\left(\sum_{i=1 }^{k}(p_{i}^{\prime}-p_{i})\cdot\mathbb{I}(\text{Type $j$ purchase at $p_{i}^{\prime}$ under price $p^{\prime}$})\right)\]\[\leq m\tilde{\epsilon}\] \[=\epsilon.\] (as \[\epsilon=m\bar{\epsilon}\].)

Hence, \(\mathrm{rev}(p)\) is within a gap of \(2\epsilon\) from \(\mathrm{OPT}\).

We then apply Theorem 3.1 to price \(p\). Therefore, it is enough to consider price functions from the set \(N_{\mathbf{S}}\stackrel{{\Delta}}{{=}}\left\{k\delta:k=1,\ldots, \lceil\frac{N}{\delta}\rceil\right\}\subseteq[N]\) to \(W\) to approximate the revenue within \(\mathcal{O}(\epsilon)\) gap. Moreover, this discretization is of the size \(\left\lceil\frac{N}{\delta}\right\rceil^{|W|}\in\mathcal{O}\left(\left(\log_{ 1+\epsilon}\left(\frac{1}{\epsilon}\right)\right)^{m}\left(\frac{L}{\epsilon} \right)^{m}\right)\) as \(\left\lceil\frac{N}{\delta}\right\rceil\in\mathcal{O}\left(\frac{Lm}{\epsilon }\right)\). 

### Proof of Theorem 3.3

**Theorem 3.3**.: _Consider the discretization \(\overline{\mathcal{P}}\) as constructed in Algorithm 2. Under Assumption 2, for any type distribution, there exists \(p\in\overline{\mathcal{P}}\) such that \(\mathrm{rev}(p)\geq\mathrm{OPT}-\mathcal{O}(\epsilon)\). Moreover,_

\[|\overline{\mathcal{P}}|\in\mathcal{O}\left(\left(\frac{J}{\epsilon^{2}} \right)^{m}\log^{m}\left(\frac{N\epsilon^{2}}{Jm}\right)\cdot\left(\log_{1+ \epsilon}^{m}1/\epsilon\right)\right)\in\widetilde{\mathcal{O}}\left(\left( \frac{J}{\epsilon^{3}}\right)^{m}\right).\]

Proof of Theorem 3.3.: For each \(i=0,1,\ldots,\left\lceil\log_{1+\epsilon^{2}}\left(\frac{N\epsilon^{2}}{2Jm} \right)\right\rceil\), let \(Y_{i}\stackrel{{\Delta}}{{=}}\left\lfloor\frac{2Jm}{\epsilon^{2} }(1+\epsilon^{2})^{i}\right\rfloor\), and \(Q_{i}\) be the set \(\left\{\left\lfloor Y_{i}+\frac{Y_{i}\epsilon^{2}}{2Jm}k\right\rfloor:k=1, \ldots,\left\lfloor 2Jm\right\rfloor\right\}\), i.e., \(Q_{i}\) splits the interval \([Y_{i},Y_{i+1}]\) equally into \(2mJ\) parts.

The union of \(Q_{i}\)s and the set \(\left\{1,2,\ldots,\left\lfloor\frac{2Jm}{\epsilon^{2}}\right\rfloor\right\}\) form a set of grids on \([0,N]\), denoted by \(N_{\mathbf{D}}\). There are at most \(\frac{2Jm}{\epsilon^{2}}+2Jm\log_{1+\epsilon^{2}}\left(\frac{N\epsilon^{2}}{ 2Jm}\right)\) grids in total.

By Lemma 3.1, there is a revenue optimal price curve \(p^{\star}:[N]\rightarrow[0,1]\) which is a \(k\)-step function, for some \(k\in[m]\). Where \(p^{\star}\) can be compactly represented as the following set of tuples:

\[\left\{(n_{1}^{\star},p_{1}^{\star}),(n_{2}^{\star},p_{2}^{\star}),\ldots,(n_{ k}^{\star},p_{k}^{\star})\right\},\]

where \(n_{1}^{\star},\ldots,n_{k}^{\star}\) denote the locations of jumps and \(p_{i}^{\star}\) denote the value of \(p^{\star}\) on step \(i\in[k]\) (i.e. \(p^{\star}(n)=p_{i}^{\star}\) for \(n\in(n_{i-1}^{\star},n_{i}^{\star}]\)).

Then, define a new \(k\)-step price curve \(p\) via

\[\left\{(n_{1},p_{1}),(n_{2},p_{2}),\ldots,(n_{k},p_{k})\right\},\]

where \(n_{i}\) is given by

\[n_{i}\leftarrow\text{round down }n_{i}^{\star}\text{ to the closest grid in }N_{\mathbf{D}}.\]

Then we define \(p_{i}\) below. If \(p_{i}^{\star}<\epsilon(1+\epsilon)\), let \(p_{i}=\epsilon(1+\epsilon)\); otherwise, let \(Z_{n_{i}^{\star}}\) be the price obtained by rounding \(p_{i}^{\star}\) down to the nearest value in \(Z\). By constructions of \(Z\) and \(W\) above, \(W_{n_{i}^{\star}}\) is a partition of interval \((Z_{n_{i}^{\star}-1},Z_{n_{i}^{\star}+1})\). Let \(w_{i}\) be the price obtained by rounding \(p_{i}^{\star}\) down to the nearest value in \(W_{n_{i}^{\star}}\). Set \(d_{i}\stackrel{{\Delta}}{{=}}\frac{\epsilon}{m}\cdot Z_{n_{i}^{ \star}-1}\). Then define \(p_{i}\stackrel{{\Delta}}{{=}}w_{i}-i\cdot d_{i}\in W_{n_{i}^{\star}}\).

First, we prove for \(i\) satisfying \(p_{i}^{\star}>\epsilon(1+\epsilon)\), if a buyer purchases at \(n_{i}\) under price \(p^{\star}\), she will not purchase at \(n_{j}\), \(j<i\) under new price \(p\). We prove this property separately when \(n_{i}\leq\frac{2Jm}{\epsilon^{2}}\) and \(n_{i}>\frac{2Jm}{\epsilon^{2}}\).

(i) When \(n_{i}>\frac{2Jm}{\epsilon^{2}}\).

The buyer's utility at \(n_{i}\) under price \(p\) is,

\[v(n_{i})-p_{i}=v(n_{j}^{\star})-p_{i}^{\star}+\left(p_{i}^{\star}-p_{i}-(v(n_{i }^{\star})-v(n_{i}))\right).\] (14)

Let \(\delta_{i}\stackrel{{\Delta}}{{=}}v(n_{i}^{\star})-v(n_{i})\). Then \(\delta_{i}\) is upper bounded by,

\[\delta_{i}=\sum_{h=n_{i}}^{n_{i}^{\star}-1}v(h+1)-v(h)\leq\sum_{h=n_{i}}^{n_{i} ^{\star}-1}\frac{J}{h}\leq\frac{J}{n_{i}}(n_{i}^{\star}-n_{i})\]\[v(n_{i})-p_{i}-(v(n_{j})-p_{j}) =v(n_{i}^{\star})-p_{i}^{\star}-(v(n_{j}^{\star})-p_{j}^{\star})+(p_ {i}^{\star}-p_{i}-\delta_{i})-(p_{j}^{\star}-p_{j}-\delta_{j})\] \[=v(n_{i}^{\star})-p_{i}^{\star}-(v(n_{j}^{\star})-p_{j}^{\star})+(p_ {i}^{\star}-p_{i})-(p_{j}^{\star}-p_{j})\] \[\geq v(n_{i}^{\star})-p_{i}^{\star}-(v(n_{j}^{\star})-p_{j}^{\star})\] \[\geq 0,\]

where the first inequality is due to (18), and the second is because the buyer prefers \(n_{i}^{\star}\) over \(n_{j}^{\star}\) under \(p^{\star}\).

So far we have completed the proof that for \(i\) satisfying \(p_{i}^{\star}>\epsilon(1+\epsilon)\), if a buyer purchases at \(n_{i}\) under price \(p^{\star}\), she will not purchase at \(n_{j},\,j<i\) under new price \(p\).

Then, similar to Step 2 in the proof of Lemma A.2, we have \(p\geq\frac{p^{\star}}{1+\epsilon}\) pointwise. We then conclude the proof by observing

\[\operatorname{rev}(p)\geq\frac{\operatorname{rev}(p^{\star})-\mathcal{O}( \epsilon)}{1+\epsilon}=\operatorname{OPT}-\mathcal{O}(\epsilon).\]

**Lemma A.4**.: _When \(n_{i}>\frac{2Jm}{\epsilon^{2}}\), we have \(n_{j}^{\star}-n_{i}\leq n_{i}\cdot\frac{\epsilon^{2}}{2Jm}+1\)._

Proof of Lemma a.4.: By the construction of discretization set, \(n_{i}\) must have the following form,

\[\left\lfloor Y_{i^{\prime}}+Y_{i^{\prime}}\cdot\frac{\epsilon^{2}k^{\prime}}{ 2Jm}\right\rfloor,\text{ where }Y_{i^{\prime}}=\left\lfloor\frac{2Jm}{\epsilon^{2}}(1+ \epsilon^{2})^{i^{\prime}}\right\rfloor\text{ for some }i^{\prime},k^{\prime}\in \mathbb{Z}.\]

Since \(n_{j}^{\prime}\) is obtained by rounding down \(n_{j}\) to the nearest grid in \(N_{\textbf{D}}\), \(n_{j}\) satisfies the following inequality,

\[n_{j}\leq n_{j}^{\star}\leq Y_{i^{\prime}}+Y_{i^{\prime}}\cdot\frac{\epsilon^{ 2}(k^{\prime}+1)}{2Jm}.\]

Therefore, we have

\[n_{i}^{\star}-n_{i} \leq Y_{i^{\prime}}+Y_{i^{\prime}}\cdot\frac{\epsilon^{2}(k^{ \prime}+1)}{2Jm}-n_{i}\] \[=Y_{i^{\prime}}+Y_{i^{\prime}}\cdot\frac{\epsilon^{2}(k^{\prime} +1)}{2Jm}-\left\lfloor Y_{i^{\prime}}+Y_{i^{\prime}}\cdot\frac{\epsilon^{2}k^ {\prime}}{2Jm}\right\rfloor\] \[\leq Y_{i^{\prime}}+Y_{i^{\prime}}\cdot\frac{\epsilon^{2}(k^{ \prime}+1)}{2Jm}-\left(Y_{i^{\prime}}+Y_{i^{\prime}}\cdot\frac{\epsilon^{2}k^ {\prime}}{2Jm}\right)+1\] \[=Y_{i^{\prime}}\cdot\frac{\epsilon^{2}}{2Jm}+1\] \[\leq n_{i}\cdot\frac{\epsilon^{2}}{2Jm}+1.\]

Where in the last inequality, since \(Y_{i^{\prime}}\) is an integer, and we have

\[n_{i}^{\prime}=\left\lfloor Y_{i^{\prime}}+Y_{i^{\prime}}\cdot\frac{\epsilon^ {2}k^{\prime}}{2Jm}\right\rfloor\geq Y_{i^{\prime}},\text{ for }k^{\prime}\geq 0.\]

## Appendix B Proof of Theorem 5.1

**Theorem 5.1**.: _Suppose in Algorithm 4 we use a discretization \(\overline{\mathcal{P}}\) which is a \(\mathcal{O}(1/\sqrt{T})\) additive approximation to any price curve. Let \(R_{T}\) be as defined in (4). Then, for Algorithm 4, we have \(\mathbb{E}[R_{T}]\ \in\ \mathcal{O}\left(m^{2}\theta T+\theta^{-1}\left(1+\log\left| \overline{\mathcal{P}}\right|\right)\right)\). Setting \(\theta=\sqrt{\frac{1+\log\left|\overline{\mathcal{P}}\right|}{m^{2}T}}\), we have \(\mathbb{E}[R_{T}]\ \in\ \mathcal{O}\big{(}m\sqrt{T\log\left|\overline{\mathcal{P}}\right|} \big{)}\)._

Proof of Theorem 5.1.: Recall that the regret \(R_{T}\) for the adversarial setting is

\[R_{T} \ \stackrel{{\Delta}}{{=}}\ \max_{p\in\mathcal{P}}\sum_{t=1}^{T}r(i_{t},p)\,- \,\sum_{t=1}^{T}r(i_{t},p_{t})\] \[\ =\ \underbrace{\max_{p\in\mathcal{P}}\sum_{t=1}^{T}r(i_{t},p)\,- \,\max_{p\in\overline{\mathcal{P}}}\sum_{t=1}^{T}r(i_{t},p)}_{\text{Loss of revenue due to discretization}}\ +\ \underbrace{\max_{p\in\overline{\mathcal{P}}}\sum_{t=1}^{T}r(i_{t},p)\,-\,\sum_{t =1}^{T}r(i_{t},p_{t})}_{\stackrel{{\Delta}}{{=}}\ \overline{R_{T}}\ (\text{ discretization regret})}.\] (19)

We decompose \(R_{T}\) into two regrets. The first term is the sacrifice of revenue on discretization. The second term is the algorithm regret when competing against the optimal price within the discretization set \(\overline{\mathcal{P}}\).

According to Theorem 3.1, our discretization scheme approaches optimal revenue within a gap of \(\frac{2\epsilon}{1+\epsilon}\):

\[\max_{p\in\mathcal{P}}\sum_{t=1}^{T}r(i_{t},p)\,-\,\max_{p\in \mathcal{P}}\sum_{t=1}^{T}r(i_{t},p)\leq\frac{2\epsilon T}{1+\epsilon}<2\epsilon T.\] (20)

Therefore, the first term can be bounded by \(2\epsilon T\).

According to Theorem B.1, the second term discretization regret is upper bounded by

\[\mathbb{E}[\overline{R}_{T}]\leq 3m\sqrt{T\log\left| \overline{\mathcal{P}}\right|}.\] (21)

Combining (20) and (21) together, we have,

\[\mathbb{E}[R_{T}]\leq 2\epsilon T+3m\sqrt{T\log\left| \overline{\mathcal{P}}\right|}=\mathcal{O}\left(m\sqrt{T\log\left|\overline{ \mathcal{P}}\right|}\right).\] (as \[\epsilon=\tfrac{1}{\sqrt{T}}\] )

Plug in the size of discretization set in Section 3, we have,

\[\mathbb{E}[R_{T}]=\widetilde{\mathcal{O}}\left(m^{\nicefrac{{3}} {{2}}}\sqrt{T}\right).\]

**Theorem B.1**.: _The discretization regret \(\overline{R}_{T}\) defined in (19) has upper bound \(\mathcal{O}\left(m\sqrt{T\log\left|\overline{\mathcal{P}}\right|}\right)\)._

Proof of Theorem b.1.: We first claim that \(r_{t}(p_{t})=r(i_{t},p_{t})\) all \(t\). If the buyer make a purchase at round \(t\), \(r_{t}(p_{t})=r(i_{t},p_{t})\) holds by definition. But if the buyer does not purchase at a price \(p_{t}\) on round \(t\), \(r(i_{t},p_{t})=0\). Since \(S_{t}^{c}\) contains all the types that would not make a purchase at \(p_{t}\), we have \(r(i,p_{t})=0,\,\forall i\in S_{t}^{c}\), and

\[r(i_{t},p_{t})=\sum_{i\in S_{t}^{c}}r(i,p_{t})=r_{t}(p_{t})=0.\]

Therefore, \(r_{t}(p_{t})=r(i_{t},p_{t})\) holds for every round \(t\in[T]\). Denote \(p^{\star}\) as,

\[p^{\star}=\operatorname*{argmax}_{p\in\overline{\mathcal{P}}} \,\sum_{t=1}^{T}r(i_{t},p).\]

Then, we decompose the regret as follows,

\[\mathbb{E}[R_{T}] =\sum_{t=1}^{T}r(i_{t},p^{\star})-\mathbb{E}\left[\sum_{t=1}^{T}r (i_{t},p_{t})\right]\] \[=\sum_{t=1}^{T}r(i_{t},p^{\star})-\mathbb{E}\left[\sum_{t=1}^{T}r _{t}(p_{t})\right]\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\left(r(i_{t},p^{\star})-r_{t}(p^ {\star})\right)\right]+\mathbb{E}\left[\sum_{t=1}^{T}r_{t}(p^{\star})-\sum_{t= 1}^{T}r_{t}(p_{t+1})\right]+\mathbb{E}\left[\sum_{t=1}^{T}r_{t}(p_{t+1})-r_{t} (p_{t})\right].\] (22)

We bound three terms in (22) separately.

**The first term.** For any price \(p\) and any round \(t\), we have \(r_{t}(p)\geq r(i_{t},p)\) by definition. Hence,

\[\sum_{t=1}^{T}\left(r(i_{t},p^{\star})-r_{t}(p^{\star})\right) \leq 0.\] (23)

**The second term.** Since \(p^{\star}=\operatorname*{argmax}_{p\in\overline{\mathcal{P}}}\,\sum_{t=1}^{T}r (i_{t},p)\). We apply Lemma B.1 to \(p^{\star}\),

\[\sum_{t=1}^{T}r_{t}(p^{\star})-\sum_{t=1}^{T}r_{t}(p_{t+1})\leq \theta_{p_{1}}-\theta_{p^{\star}}.\]Note that both \(\theta_{p_{1}}\) and \(\theta_{p^{*}}\) are drawn i.i.d. from exponential distribution,

\[\mathbb{E}[\theta_{p_{1}}]\leq\mathbb{E}\left[\max_{p\in\overline{ \mathcal{P}}}\theta_{p}\right]\leq\frac{1+\log\left|\overline{\mathcal{P}} \right|}{\theta},\]

\[\mathbb{E}[\theta_{p^{*}}]\leq\mathbb{E}\left[\max_{p\in\overline{ \mathcal{P}}}\theta_{p}\right]\leq\frac{1+\log\left|\overline{\mathcal{P}} \right|}{\theta}.\]

We have

\[\mathbb{E}\left[\sum_{t=1}^{T}r_{t}(p^{*})-\sum_{t=1}^{T}r_{t}(p_{t+ 1})\right]\leq\mathbb{E}\big{[}\theta_{p_{1}}-\theta_{p^{*}}\big{]}\leq\frac{ 1+\log\left|\overline{\mathcal{P}}\right|}{\theta}.\] (24)

**The third term.** Note that for any price \(p\in\overline{\mathcal{P}}\) and any round \(t\), \(r_{t}(p)\leq m\). Therefore we have,

\[\mathbb{E}\left[r_{t}(p_{t+1})-r_{t}(p_{t})\right]=\mathbb{P} \left(p_{t+1}\neq p_{t}\right)\mathbb{E}\left[r_{t}(p_{t+1})-r_{t}(p_{t})\mid p _{t+1}\neq p_{t}\right]\leq m\cdot\mathbb{P}\left(p_{t+1}\neq p_{t}\right).\]

The price curve on round \(t\) is \(p_{t}\), then by the price updation rule,

\[p_{t}=\operatorname*{argmax}_{p\in\overline{\mathcal{P}}}\sum_{ \tau=1}^{t-1}r_{\tau}(p)+\theta_{p},\]

which is equivalent to,

\[\theta_{p_{t}}\geq\theta_{p}+\sum_{\tau=1}^{t-1}r_{\tau}(p)-\sum _{\tau=1}^{t-1}r_{\tau}(p_{t}),\,\forall p\in\overline{\mathcal{P}}.\]

For all \(p^{\prime}\in\overline{\mathcal{P}}\), let \(c_{t-1,p^{\prime}}\) denote

\[\max_{p\in\overline{\mathcal{P}}}\left(\theta_{p}+\sum_{\tau=1}^ {t-1}r_{\tau}(p)-\sum_{\tau=1}^{t-1}r_{\tau}(p^{\prime})\right)\triangleq c_{ t-1,p^{\prime}},\] (25)

then \(p_{t}=p^{\prime}\) is equivalent to

\[\theta_{p^{\prime}}\geq c_{t-1,p^{\prime}}.\] (26)

**Subclaim.** If \(\theta_{p_{t}}\) also satisfies the following condition (27),

\[\theta_{p_{t}}\geq\theta_{p}+\sum_{\tau=1}^{t-1}r_{\tau}(p)-\sum _{\tau=1}^{t-1}r_{\tau}(p_{t})+m,\,\forall p\in\overline{\mathcal{P}},\] (27)

then \(p_{t+1}=p_{t}\).

_Proof of the Subclaim._ If (27) holds for all \(p\in\overline{\mathcal{P}}\),

\[\theta_{p_{t}} \geq\theta_{p}+\sum_{\tau=1}^{t-1}r_{\tau}(p)-\sum_{\tau=1}^{t-1} r_{\tau}(p_{t})+m\] \[\geq\theta_{p}+\sum_{\tau=1}^{t}r_{\tau}(p)-\sum_{\tau=1}^{t}r_{ \tau}(p_{t}).\] (because

\[\forall p\in\overline{\mathcal{P}}\]

,

\[r_{t}(p)\in[0,m]\]

)

Hence,

\[p_{t}=\operatorname*{argmax}_{p\in\overline{\mathcal{P}}}\sum _{\tau=1}^{t}r_{\tau}(p)+\theta_{p}=p_{t+1}.\]

\(\square\)

Therefore, (27) is a sufficient condition for \(p_{t+1}=p_{t}\). We then bound the probability of \(p_{t+1}=p_{t}\) by computing the probability of (27) happening.

\[\mathbb{P}\left(p_{t}=p_{t+1}\right) =\sum_{p\in\overline{\mathcal{P}}}\mathbb{P}\left(p_{t}=p\right) \mathbb{P}(p_{t+1}=p\mid p_{t}=p)\] \[=\sum_{p\in\overline{\mathcal{P}}}\mathbb{P}\left(p_{t}=p\right) \mathbb{P}\left(p_{t+1}=p\mid\theta_{p}\geq c_{t-1,p}\right)\] ( by ( 26 )) \[\geq\sum_{p\in\overline{\mathcal{P}}}\mathbb{P}\left(p_{t}=p \right)\mathbb{P}\left(\theta_{p}\geq c_{t-1,p}+m\mid\theta_{p}\geq c_{t-1,p}\right)\] \[\geq\sum_{p\in\overline{\mathcal{P}}}\mathbb{P}\left(p_{t}=p \right)e^{-m\theta}\] \[=e^{-m\theta}\] \[\geq 1-m\theta\]

Therefore, \(\mathbb{P}\left(p_{t}\neq p_{t+1}\right)\leq m\theta\). Hence, the third term can be bounded as

\[\mathbb{E}\big{[}r_{t}(p_{t+1})-r_{t}(p_{t})\big{]}\leq m^{2}\theta\implies \sum_{t=1}^{T}\mathbb{E}\big{[}r_{t}(p_{t+1})-r_{t}(p_{t})\big{]}\leq m^{2} \theta T.\] (28)

Set \(\theta=\sqrt{\frac{\log\left|\overline{\mathcal{P}}\right|}{m^{2}T}}\). Combining the upper bounds for three terms (23), (24) and (28) together, we have

\[\mathbb{E}[R_{T}]\leq\frac{1+\log\left|\overline{\mathcal{P}}\right|}{\theta }+m^{2}\theta T\in\mathcal{O}\left(m\sqrt{T\log\left|\overline{\mathcal{P}} \right|}\right).\]

Plugging in the size of the discretization set (Theorem 3.1), we have,

\[\mathbb{E}[R_{T}]\in\widetilde{\mathcal{O}}\left(m^{\nicefrac{{3}}{{2}}} \sqrt{T}\right).\]

**Lemma B.1**.: _For any \(p\in\overline{\mathcal{P}}\),_

\[\sum_{t=1}^{T}r_{t}(p_{t+1})+\theta_{p_{1}}\geq\sum_{t=1}^{T}r_{t}(p)+\theta_{ p}.\] (29)

Proof of Lemma b.1.: We prove this by induction. For \(T=0\), the inequality \(\theta_{p_{1}}\geq\theta_{p}\) holds by definition \(p_{1}=\operatorname*{argmax}_{p\in\overline{\mathcal{P}}}\,\theta_{p}\). Assume that the inequality holds for some \(T\). Then for any \(p\in\overline{\mathcal{P}}\),

\[\sum_{t=1}^{T+1}r_{t}(p_{t+1})+\theta_{p_{1}} =\sum_{t=1}^{T}r_{t}(p_{t+1})+\theta_{p_{1}}+r_{T+1}(p_{T+2})\] \[\geq\sum_{t=1}^{T}r_{t}(p_{T+2})+\theta_{p_{T+2}}+r_{T+1}(p_{T+2})\] \[=\sum_{t=1}^{T+1}r_{t}(p_{T+2})+\theta_{p_{T+2}}\] \[\geq\sum_{t=1}^{T+1}r_{t}(p)+\theta_{p}.\]

Where the first inequality is by the induction hypothesis, and the second inequality is by

\[p_{T+2}=\operatorname*{arg\,max}_{p\in\overline{\mathcal{P}}}\sum_{t=1}^{T+1}r _{t}(p)+\theta_{p}.\]

By the induction, the inequality (29) holds for any \(T\geq 0\)Proof of Theorem 4.1

In this section, we prove, Theorem 4.1, our regret upper bound of Algorithm 3. We prove the theorem by first decomposing the regret into two parts: Regret with respect to the best price in a discretized set (called "discretization regret") and the residual error due to discretization. The residual error is controlled by the approximation guarantees developed in Section 3. Then, the key lemma in this appendix is Lemma C.1 which controls the discretization. We prove Lemma C.1 using a technique adapted from Chen et al. [15].

**Theorem 4.1**.: _Suppose in Algorithm 3 we use a discretization \(\overline{\mathcal{P}}\) which is a \(\mathcal{O}(1/\sqrt{T})\) additive approximation to any price curve. Then, the regret of Algorithm 3 satisfies \(\mathbb{E}[R_{T}]\in\widetilde{\mathcal{O}}(m\sqrt{T})\)._

Proof of Theorem 4.1.: For the sake of simplicity, we define \(r(i,p)\) as the revenue under type \(i\) and price \(p\), i.e, \(r(i,p)\ \stackrel{{\Delta}}{{=}}\ p(n_{i,p})\). Therefore, on every round, we have \(r(i_{t},p_{t})=p_{t}(n_{i_{t},p_{t}})\).

Recall that the regret \(R_{T}\) is

\[R_{T} \ \stackrel{{\Delta}}{{=}}\ \ T\cdot\mathrm{OPT}\,-\, \sum_{t=1}^{T}p_{t}(n_{i_{t},p_{t}})\] \[=\ \ T\cdot\mathrm{OPT}\,-\,\sum_{t=1}^{T}r(i_{t},p_{t})\] \[=\ \underbrace{\ T\cdot\mathrm{OPT}\,-\,T\cdot\max_{p\in \overline{\mathcal{P}}}\mathrm{rev}(p)}_{\text{Loss of revenue due to discretization}}+\underbrace{\ T\cdot\max_{p\in\overline{\mathcal{P}}}\mathrm{rev}(p)\,-\,\sum_{t=1}^{T}r(i_{t},p_{t})}_{ \triangleq\ \overline{R}_{T}\ \text{(discretization regret)}}.\] (30)

We decompose \(R_{T}\) into two parts. The first term is the sacrifice of revenue on discretization. The second term is the algorithm regret when competing against the optimal price within the discretization set \(\overline{\mathcal{P}}\).

According to Theorem 3.1, our discretization scheme approaches \(\mathrm{OPT}\) within a gap of \(\frac{2\epsilon}{1+\epsilon}\),

\[\mathrm{OPT}\,-\,\max_{p\in\overline{\mathcal{P}}}\mathrm{rev}(p)\leq\frac{2 \epsilon}{1+\epsilon}\leq 2\epsilon.\]

Therefore, the first term can be bounded as,

\[T\cdot\mathrm{OPT}\,-\,T\cdot\max_{p\in\overline{\mathcal{P}}}\mathrm{rev}(p )\leq 2\epsilon T.\] (31)

By Lemma C.1, the second term, discretization regret, is upper bounded by

\[\mathbb{E}[\overline{R}_{T}]\leq 93m\sqrt{T\log T}\] (32)

Combining (31) and (32) together, we have,

\[\mathbb{E}[R_{T}]\leq 2\epsilon T+93m\sqrt{T\log T}=\widetilde{\mathcal{O}}(m \sqrt{T})\] ( as \[\epsilon=\tfrac{1}{\sqrt{T}}\] )

**Lemma C.1**.: _The discretization regret \(\overline{R}_{T}\) defined in (30) is at most \(\widetilde{\mathcal{O}}(m\sqrt{T})\)._

Proof of Lemma c.1.: The discretization regret \(\overline{R}_{T}\)

\[\mathbb{E}[\overline{R}_{T}] =\mathbb{E}\left[\,T\cdot\max_{p\in\overline{\mathcal{P}}} \mathrm{rev}(p)\,-\,\sum_{t=1}^{T}r(i_{t},p_{t})\right]\] \[=\mathbb{E}\left[\,\sum_{t=1}^{T}\left(r(p^{\star},i_{t})\,-\,r( p_{t},i_{t})\right)\right]\]\[=\sum_{t=1}^{T}\mathbb{E}\left[r(p^{\star},i_{t})\,-\,r(p_{t},i_{t})\right]\] \[=\sum_{t=1}^{T}\mathbb{E}\left[\operatorname{rev}(p^{\star})\,- \,\operatorname{rev}(p_{t})\right]\] \[=\sum_{t=1}^{T}\mathbb{E}\left[\left(\operatorname{rev}(p^{\star} )\,-\,\operatorname{rev}(p_{t})\right)\cdot\mathbbm{I}(A_{t})\right]\,+\,\sum_ {t=1}^{T}\mathbb{E}\left[\left(\operatorname{rev}(p^{\star})\,-\, \operatorname{rev}(p_{t})\right)\cdot\mathbbm{I}(A_{t}^{c})\right]\] \[\stackrel{{\Delta}}{{=}}\sum_{t=1}^{T}\mathbb{E} \left[\delta_{p_{t}}\cdot\mathbbm{I}(A_{t})\right]\,+\,\sum_{t=1}^{T}\mathbb{ E}\left[\delta_{p_{t}}\cdot\mathbbm{I}(A_{t}^{c})\right].\] (33)

We can further decompose \(\mathbb{E}[\overline{R}_{T}]\) into \(\sum_{t=1}^{T}\mathbb{E}\left[\delta_{p_{t}}\cdot\mathbbm{I}(A_{t})\right]\) and \(\sum_{t=1}^{T}\mathbb{E}\left[\delta_{p_{t}}\cdot\mathbbm{I}(A_{t}^{c})\right]\). Where for any round \(t\), we define the good event \(A_{t}\) as follows,

\[\forall i\in\left[m\right],\quad q_{i}\leq\widehat{q}_{i,t}\leq q_{i}+2\sqrt{ \frac{\log T}{T_{i,t}}}.\]

Define \(\overline{q}_{i,t}\stackrel{{\Delta}}{{=}}\frac{\sum_{t=1}^{t} \mathbbm{I}(i\in S_{\tau},i_{\tau}=i)}{T_{i,t}}=\frac{\sum_{i=1}^{t}\mathbbm{I }(i\in S_{\tau})\cdot\mathbbm{I}(i_{\tau}=i)}{\sum_{\tau=1}^{t}\mathbbm{I}(i \in S_{\tau})}\). Note that \(\mathbbm{I}(i_{\tau}=i)\) is a random variable that follows Bernoulli distribution \(\text{Ber}(q_{i})\), and one can only observe \(\mathbbm{I}(i_{\tau}=i)\) when \(i\in S_{\tau}\), let \(\overline{x}_{i,j}\) denote the mean value of first \(j\) i.i.d. observations of \(\mathbbm{I}(i_{s}=i)\). Then, we have

\[\mathbb{P}\left(\left|\overline{q}_{i,t}-q_{i}\right|>\sqrt{ \frac{\log T}{T_{i,t}}}\right) =\sum_{j=0}^{t}\mathbb{P}\left(\left|\overline{q}_{i,t}-q_{i} \right|>\sqrt{\frac{\log T}{T_{i,t}}},\,\,\,T_{i,t}=j\right)\] \[\leq\sum_{j=0}^{t}\mathbb{P}\left(\left|\overline{x}_{i,j}-q_{i} \right|>\sqrt{\frac{\log T}{j}}\right)\] \[\leq\sum_{j=0}^{t}2\exp(-2\log T)\] \[\leq\frac{2}{T}.\]

Where in the first inequality, the event \(\left\{\left|\overline{q}_{i,t}-q_{i}\right|>\sqrt{\frac{\log T}{T_{i,t}}}, \,\,\,T_{i,t}=j\right\}\) indicates \(\left\{\left|\overline{x}_{i,j}-q_{i}\right|>\sqrt{\frac{\log T}{j}}\right\},\) and the second inequality follows from Hoeffding's inequality.

We then bound the second term in (33)

\[\sum_{t=1}^{T}\mathbb{E}\left[\delta_{p_{t}}\mathbbm{I}(A_{t}^{c})\right] \leq\sum_{t=1}^{T}\mathbb{E}\left[\mathbbm{I}(A_{t}^{c})\right]\] \[\leq\sum_{t=1}^{T}\sum_{i=1}^{m}\mathbb{P}\left(\left|\overline{ q}_{i,t}-q_{i}\right|>\sqrt{\frac{\log T}{T_{i,t}}}\right)\] \[\leq\sum_{t=1}^{T}\sum_{i=1}^{m}\frac{2}{T}\] \[\leq 2m.\]

Define event \(H_{t}\stackrel{{\Delta}}{{=}}\left\{0<\delta_{p_{t}}<2\sum_{i \in S_{t}}\sqrt{\frac{\log T}{T_{i,t-1}}}\right\}\). By Lemma C.3, we know that

\[\mathbbm{I}(A_{t-1},\,\delta_{p_{t}}>0)\implies\mathbbm{I}\left(0<\delta_{p_{ t}}<\sum_{i\in S_{t}}2\sqrt{\frac{\log T}{T_{i,t-1}}}\right)=\mathbbm{I}(H_{T}).\]It remains to prove the upper bound for \(\sum_{t=1}^{T}\mathbb{E}\left[\delta_{p_{t}}\mathbb{I}(A_{T})\right]\).

For \(t\in\{1,\ldots,T\}\) and \(k\in\mathbb{Z}_{+}\), let

\[m_{k,t}\stackrel{{\Delta}}{{=}}\begin{cases}\alpha_{k}\left( \frac{m}{\delta_{p_{t}}}\right)^{2}\log T,&\delta_{p_{t}}>0,\\ +\infty,&\delta_{p_{t}}=0,\end{cases}\]

and

\[A_{k,t}\stackrel{{\Delta}}{{=}}\left\{i\in S_{t}:T_{i,t-1}\leq m _{k,t}\right\}.\]

Then, we define an event

\[\mathcal{G}_{k,t}\stackrel{{\Delta}}{{=}}\left\{|A_{k,t}|\geq \beta_{k}m\right\},\]

which means "In the \(t\)-th round, at least \(\beta_{k}m\) types in \(S_{t}\) has been observed at most \(m_{k,t}\) times".

Then, by Lemma C.5, we have

\[\sum_{t=1}^{T}\mathbb{I}(\mathcal{H}_{t})\cdot\delta_{p_{t}}\leq\sum_{k=1}^{ \infty}\sum_{t=1}^{T}\mathbb{I}\left(\mathcal{G}_{k,t},\delta_{p_{t}}>0\right) \cdot\delta_{p_{t}}.\]

For \(i\in[m],k\in\mathbb{Z}_{+},t\in[T]\), define an event

\[\mathcal{G}_{i,k,t}\stackrel{{\Delta}}{{=}}\mathcal{G}_{k,t} \cap\left\{i\in S_{t},\,T_{i,t-1}\leq m_{k,t}\right\}.\]

Then by the definitions of \(\mathcal{G}_{k,t}\) and \(\mathcal{G}_{i,k,t}\) we have

\[\mathbb{I}\left(\mathcal{G}_{k,t},\,\delta_{p_{t}}>0\right)\leq\frac{1}{\beta _{k}m}\sum_{i\in E_{\mathrm{B}}}\mathbb{I}\left(\mathcal{G}_{i,k,t},\,\delta_{ p_{t}}>0\right).\]

Therefore,

\[\sum_{t=1}^{T}\mathbb{I}(\mathcal{H}_{t})\cdot\delta_{p_{t}}\leq\sum_{i\in E_ {\mathrm{B}}}\sum_{k=1}^{\infty}\sum_{t=1}^{T}\mathbb{I}\left(\mathcal{G}_{i,k,t},\,\delta_{p_{t}}>0\right)\cdot\frac{\delta_{p_{t}}}{\beta_{k}m}.\]

For any price function \(p\), define \(\delta_{p}\stackrel{{\Delta}}{{=}}\mathrm{rev}(p^{*})-\mathrm{rev }(p)\). If \(\delta_{p}>0\), we call it a "bad" price. Let \(E_{B}\stackrel{{\Delta}}{{=}}\left\{i\in[m]:\,\text{type $i$ would make a purchase at least one bad price}\right\}\).

For each type \(i\in E_{\mathrm{B}}\), suppose \(i\) is contained in \(N_{i}\) bad prices \(p_{i,1}^{\mathrm{B}},p_{i,2}^{\mathrm{B}},\ldots,p_{i,N_{i}}^{\mathrm{B}}\). Let \(\delta_{i,l}\stackrel{{\Delta}}{{=}}\delta_{p_{i,l}^{\mathrm{B}} }\left(l\in[N_{i}]\right)\). Without loss of generality, we assume \(\delta_{i,1}\geq\delta_{i,2}\geq\cdots\geq\delta_{i,N_{i}}\). Let \(\delta_{i,\min}\stackrel{{\Delta}}{{=}}\delta_{i,N_{i}}\).

For convenience, we also define \(\delta_{i,0}=+\infty\), i.e., \(\alpha_{k}\left(\frac{2m}{\delta_{i,0}}\right)^{2}=0\). Then, we have

\[\sum_{t=1}^{T}\mathbb{I}\left(\mathcal{H}_{t}\right)\delta_{p_{t}}\] \[\leq \sum_{i\in E_{\mathrm{B}}}\sum_{k=1}^{\infty}\sum_{t=1}^{T} \mathbb{I}\left(\mathcal{G}_{i,k,t},\,\delta_{p_{t}}>0\right)\frac{\delta_{p_{t }}}{\beta_{k}m}\] \[= \sum_{i\in E_{\mathrm{B}}}\sum_{k=1}^{\infty}\sum_{t=1}^{T}\sum_{ l=1}^{N_{i}}\mathbb{I}\left(\mathcal{G}_{i,k,t},\,p_{t}=p_{i,l}^{\mathrm{B}}\right) \frac{\delta_{p_{t}}}{\beta_{k}m}\] \[= \sum_{i\in E_{\mathrm{B}}}\sum_{k=1}^{\infty}\sum_{t=1}^{T}\sum_{ l=1}^{N_{i}}\mathbb{I}\left(\mathcal{G}_{i,k,t},\,p_{t}=p_{i,l}^{\mathrm{B}}\right) \frac{\delta_{i,l}}{\beta_{k}m}\]\[\leq \sum_{i\in E_{\rm B}}\sum_{k=1}^{\infty}\sum_{t=1}^{T}\sum_{l=1}^{N_{ i}}\mathbbm{1}\left(T_{i,t-1}\leq m_{k,t},\,p_{t}=p_{i,l}^{\rm B}\right)\frac{ \delta_{i,l}}{\beta_{k}m}\] \[= \sum_{i\in E_{\rm B}}\sum_{k=1}^{\infty}\sum_{t=1}^{T}\sum_{l=1}^{ N_{i}}\mathbbm{1}\left(T_{i,t-1}\leq\alpha_{k}\left(\frac{2m}{\delta_{i,l}} \right)^{2}\log T,\,p_{t}=p_{i,l}^{\rm B}\right)\frac{\delta_{i,l}}{\beta_{k}m}\] \[= \sum_{i\in E_{\rm B}}\sum_{k=1}^{\infty}\sum_{t=1}^{T}\sum_{l=1}^{ N_{i}}\sum_{j=1}^{l}\mathbbm{1}\left(\alpha_{k}\left(\frac{2m}{\delta_{i,j-1}} \right)^{2}\log T<T_{i,t-1}\leq\alpha_{k}\left(\frac{2m}{\delta_{i,j}}\right) ^{2}\log T,\,p_{t}=p_{i,l}^{\rm B}\right)\frac{\delta_{i,j}}{\beta_{k}m}\] \[\leq \sum_{i\in E_{\rm B}}\sum_{k=1}^{\infty}\sum_{t=1}^{T}\sum_{l=1}^ {N_{i}}\mathbbm{1}\left(\alpha_{k}\left(\frac{2m}{\delta_{i,j-1}}\right)^{2} \log T<T_{i,t-1}\leq\alpha_{k}\left(\frac{2m}{\delta_{i,j}}\right)^{2}\log T,\, p_{t}=p_{i,l}^{\rm B}\right)\frac{\delta_{i,j}}{\beta_{k}m}\] \[\leq \sum_{i\in E_{\rm B}}\sum_{k=1}^{\infty}\sum_{t=1}^{T}\sum_{l=1}^ {N_{i}}\mathbbm{1}\left(\alpha_{k}\left(\frac{2m}{\delta_{i,j-1}}\right)^{2} \log T<T_{i,t-1}\leq\alpha_{k}\left(\frac{2m}{\delta_{i,j}}\right)^{2}\log T, \,p_{t}=p_{i,l}^{\rm B}\right)\frac{\delta_{i,j}}{\beta_{k}m}\] \[\leq \sum_{i\in E_{\rm B}}\sum_{k=1}^{\infty}\sum_{t=1}^{N_{i}}\sum_{j =1}^{N_{i}}\mathbbm{1}\left(\alpha_{k}\left(\frac{2m}{\delta_{i,j-1}}\right)^{ 2}\log T<T_{i,t-1}\leq\alpha_{k}\left(\frac{2m}{\delta_{i,j}}\right)^{2}\log T,\,i\in S_{t}\right)\frac{\delta_{i,j}}{\beta_{k}m}\] \[\leq \sum_{i\in E_{\rm B}}\sum_{k=1}^{\infty}\sum_{j=1}^{N_{i}}\left( \alpha_{k}\left(\frac{2m}{\delta_{i,j}}\right)^{2}\log T-\alpha_{k}\left( \frac{2m}{\delta_{i,j-1}}\right)^{2}\log T\right)\frac{\delta_{i,j}}{\beta_{k}m}\] \[= 4m\left(\sum_{k=1}^{\infty}\frac{\alpha_{k}}{\beta_{k}}\right) \log T\cdot\sum_{i\in E_{\rm B}}\sum_{j=1}^{N_{i}}\left(\frac{1}{\delta_{i,j} ^{2}}-\frac{1}{\delta_{i,j-1}^{2}}\right)\delta_{i,j}\] \[\leq 1068m\log T\cdot\sum_{i\in E_{\rm B}}\sum_{j=1}^{N_{i}}\left( \frac{1}{\delta_{i,j}^{2}}-\frac{1}{\delta_{i,j-1}^{2}}\right)\delta_{i,j},\]

where the last inequality is due to Lemma C.4. Finally, for each \(i\in E_{\rm B}\) we have

\[\sum_{j=1}^{N_{i}}\left(\frac{1}{\delta_{i,j}^{2}}-\frac{1}{ \delta_{i,j-1}^{2}}\right)\delta_{i,j} =\frac{1}{\delta_{i,N_{i}}}+\sum_{j=1}^{N_{i}-1}\frac{1}{\delta_{ i,j}^{2}}\left(\delta_{i,j}-\delta_{i,j+1}\right)\] \[\leq\frac{1}{\delta_{i,N_{i}}}+\int_{\delta_{i,N_{i}}}^{\delta_{i,1 }}\frac{1}{x^{2}}\mathrm{d}x\] \[=\frac{2}{\delta_{i,N_{i}}}-\frac{1}{\delta_{i,1}}\] \[\leq\frac{2}{\delta_{i,\min}}.\]

It follows that

\[\sum_{t=1}^{T}\mathbbm{1}(\mathcal{H}_{t})\cdot\delta_{p_{t}}\leq 1068m\log T\cdot \sum_{i\in E_{\rm B}}\frac{2}{\delta_{i,\min}}=m\sum_{i\in E_{\rm B}}\frac{2136 }{\delta_{i,\min}}\log T\] (34)

So far, the distribution-dependent regret bound is proven. To prove the distribution-independent bound, we decompose \(\sum_{t=1}^{T}\mathbbm{1}(\mathcal{H}_{t})\cdot\delta_{p_{t}}\) into two parts:

\[\sum_{t=1}^{T}\mathbbm{1}(\mathcal{H}_{t})\cdot\delta_{p_{t}} =\sum_{t=1}^{T}\mathbbm{1}\left(\mathcal{H}_{t},\,\delta_{p_{t}}\leq \epsilon\right)\cdot\delta_{p_{t}}+\sum_{t=1}^{T}\mathbbm{1}\left(\mathcal{H}_{t },\,\delta_{p_{t}}>\epsilon\right)\cdot\delta_{p_{t}}\] \[\leq\epsilon T+\sum_{t=1}^{T}\mathbbm{1}\left(\mathcal{H}_{t}, \delta_{p_{t}}>\epsilon\right)\cdot\delta_{p_{t}},\]

where \(\epsilon>0\) is a constant to be determined. The second term can be bounded in the same way as in the proof of the distribution-dependent regret bound, except that we only consider the case \(\delta_{p_{t}}>\epsilon\). (For each type \(i\in E_{\mathrm{B}}\), suppose \(i\) is contained in \(N_{i}\) bad prices \(p_{t,1}^{\mathrm{B}},p_{t,2}^{\mathrm{B}},\ldots,p_{i,N_{i}}^{\mathrm{B}}\). Let \(\delta_{i,l}\stackrel{{\Delta}}{{=}}\delta_{p_{t,l}^{\mathrm{B}}} \left(l\in[N_{i}]\right)\) satisfies \(\delta_{i,1}\geq\delta_{i,2}\geq\ldots\geq\delta_{i,N_{i}}\geq\epsilon\). Also let \(\delta_{i,\min}\stackrel{{\Delta}}{{=}}\delta_{i,N_{i}}\).) Thus, we can replace (34) by

\[\sum_{t=1}^{T}\mathbbm{1}\left(\mathcal{H}_{t},\delta_{p_{t}}> \epsilon\right)\cdot\delta_{p_{t}}\leq m\cdot\sum_{i\in E_{\mathrm{B}},\delta_ {i,\min>\epsilon}}\frac{2136}{\delta_{i,\min}}\log T\leq\frac{2136m^{2}}{ \epsilon}\log T.\]

It follows that

\[\sum_{t=1}^{T}\mathbbm{1}(\mathcal{H}_{t})\cdot\delta_{S_{t}} \leq\epsilon T+\;\frac{2136m^{2}}{\epsilon}\log T.\]

Finally, letting \(\epsilon=\sqrt{\frac{2136m^{2}\log T}{T}}\), we get

\[\sum_{t=1}^{T}\mathbbm{1}(\mathcal{H}_{t})\cdot\delta_{S_{t}} \leq 2\sqrt{2136m^{2}T\log T}\leq 93\sqrt{m^{2}T\log T}.\]

**Lemma C.2**.: _Under good event \(A_{t}\), for any price function \(p\), let \(S_{p}\) denote the set of types who would purchase at price \(p\), then we have_

\[\forall t\in[T],\quad\mathrm{rev}(p)\leq\widehat{\mathrm{rev}}_{t}(p)\leq \mathrm{rev}(p)+\sum_{i\in S_{p}}2\sqrt{\frac{\log T}{T_{i,t}}}.\]

Proof of Lemma c.2.: When \(A_{t}\) happens,

\[q_{i}\leq\widehat{q}_{i,t}\leq q_{i}+2\sqrt{\frac{\log T}{T_{i,t}}},\]

for all \(i\in[m]\).

Therefore, we have

\[\widehat{\mathrm{rev}}_{t}(p)=\sum_{i=1}^{m}\widehat{q}_{i,t}\cdot r(i,p)\geq \sum_{i=1}^{m}q_{i}\cdot r(i,p)=\mathrm{rev}(p)\]

and

\[\widehat{\mathrm{rev}}_{t}(p)=\sum_{i=1}^{m}\widehat{q}_{i,t}\cdot r(i,p)\leq \sum_{i=1}^{m}\left(q_{i}+2\sqrt{\frac{\log T}{T_{i,t}}}\right)\cdot r(i,p) \leq\mathrm{rev}(p)+\sum_{i\in S_{p}}2\sqrt{\frac{\log T}{T_{i,t}}}.\]

The last inequality is by \(r(i,p)\leq 1\). 

**Lemma C.3**.: _For each \(t\in[T]\), under good event \(A_{t-1}\), the following inequality holds,_

\[\delta_{p_{t}}\stackrel{{\Delta}}{{=}}\mathrm{rev}(p^{\star})- \mathrm{rev}(p_{t})\leq 2\sum_{i\in S_{t}}\sqrt{\frac{\log T}{T_{i,t-1}}}.\]

Proof of Lemma c.3.: When \(A_{t-1}\) happens, by Lemma C.2,

\[\mathrm{rev}(p^{\star})\leq\widehat{\mathrm{rev}}_{t-1}(p^{\star}),\]

\[\mathrm{rev}(p_{t})\geq\widehat{\mathrm{rev}}_{t-1}(p_{t})-2\sum_{i\in S_{t} }\sqrt{\frac{\log T}{T_{i,t-1}}}.\]It then follows that,

\[\delta_{p_{t}}=\operatorname{rev}(p^{\star})-\operatorname{rev}(p_{t})\leq \widehat{\operatorname{rev}}_{t-1}(p^{\star})-\left(\widehat{\operatorname{rev }}_{t-1}(p_{t})-2\sum_{i\in S_{t}}\sqrt{\frac{\log T}{T_{i,t-1}}}\right)\]

Since \(p_{t}=\operatorname{argmax}_{p\in\overline{\mathcal{P}}}\widehat{ \operatorname{rev}}_{t-1}(p)\), we have

\[\widehat{\operatorname{rev}}_{t-1}(p_{t})\geq\widehat{\operatorname{rev}}_{ t-1}(p^{\star}).\]

**Lemma C.4** (Theorem 4 of Kveton et al. [37]).: _We can choose \(\left\{\alpha_{k}\right\}_{k\geq 0}\) and \(\left\{\beta_{k}\right\}_{k\geq 0}\), which satisfy the following properties: \(\left\{\alpha_{k}\right\}_{k\geq 0}\) and \(\left\{\beta_{k}\right\}_{k\geq 0}\) are positive and_

\[\alpha_{1}>\alpha_{2}>\ldots\text{ and }1=\beta_{0}>\beta_{1}>\beta_{2}>\ldots,\]

_such that \(\lim_{k\to\infty}\alpha_{k}=\lim_{k\to\infty}\beta_{k}=0\). Moreover,_

\[\sqrt{6}\sum_{k=1}^{\infty}\frac{\beta_{k-1}-\beta_{k}}{\sqrt{\alpha_{k}}} \leq 1,\text{ and }\sum_{k=1}^{\infty}\frac{\alpha_{k}}{\beta_{k}}<267.\]

**Lemma C.5**.: _On round \(t\), if event \(\mathcal{H}_{t}\) happens, then at least one event \(\mathcal{G}_{k,t},\;k\in\mathbb{Z}_{+}\) happens, where_

\[\mathcal{G}_{k,t}\stackrel{{\Delta}}{{=}}\left\{|A_{k,t}|\geq \beta_{k}m\right\},\quad\text{where }A_{k,t}\stackrel{{\Delta}}{{=}} \left\{i\in S_{t}:T_{i,t-1}\leq m_{k,t}\right\},\]

_and \(m_{k,t}=\alpha_{k}\left(\frac{m}{\delta_{p_{t}}}\right)^{2}\log T\) when \(\delta_{p_{t}}>0\) and \(+\infty\) otherwise._

Proof of Lemma C.5.: Assume that \(\mathcal{H}_{t}\) happens and that none of \(\mathcal{G}_{1,t},\mathcal{G}_{2,t},\ldots\) happens. Then \(|A_{k,t}|<\beta_{k}m\) for all \(k\in\mathbb{Z}_{+}\). Let \(A_{0,t}=S_{t}\) and \(\bar{A}_{k,t}=S_{t}\backslash A_{k,t}\) for \(k\in\mathbb{Z}_{+}\cup\left\{0\right\}\). Thus \(\bar{A}_{k-1,t}\subseteq\bar{A}_{k,t}\) for all \(k\in\mathbb{Z}_{+}\). Note that \(\lim_{k\to\infty}m_{k,t}=0\). Thus there exists \(N\in\mathbb{Z}_{+}\)such that \(\bar{A}_{k,t}=S_{t}\) for all \(k\geq N\), and then we have \(S_{t}=\bigcup_{k=1}^{\infty}\left(\bar{A}_{k,t}\backslash\bar{A}_{k-1,t}\right)\). Finally, note that for all \(i\in\bar{A}_{k,t}\), we have \(T_{i,t-1}>m_{k,t}\). Therefore

\[\sum_{i\in S_{t}}\frac{1}{\sqrt{T_{i,t-1}}} =\sum_{k=1}^{\infty}\sum_{i\in\bar{A}_{k,t}\backslash\bar{A}_{k-1,t}}\frac{1}{\sqrt{T_{i,t-1}}}\leq\sum_{k=1}^{\infty}\sum_{i\in\bar{A}_{k,t} \backslash\bar{A}_{k-1,t}}\frac{1}{\sqrt{m_{k,t}}}\] \[=\sum_{k=1}^{\infty}\frac{\left|\bar{A}_{k,t}\backslash\bar{A}_{k -1,t}\right|}{\sqrt{m_{k,t}}}=\sum_{k=1}^{\infty}\frac{|A_{k-1,t}\backslash A _{k,t}|}{\sqrt{m_{k,t}}}=\sum_{k=1}^{\infty}\frac{|A_{k-1,t}|-|A_{k,t}|}{ \sqrt{m_{k,t}}}\] \[=\frac{|S_{t}|}{\sqrt{m_{1,t}}}+\sum_{k=1}^{\infty}|A_{k,t}| \left(\frac{1}{\sqrt{m_{k+1,t}}}-\frac{1}{\sqrt{m_{k,t}}}\right)\] \[<\frac{m}{\sqrt{m_{1,t}}}+\sum_{k=1}^{\infty}\beta_{k}m\left( \frac{1}{\sqrt{m_{k+1,t}}}-\frac{1}{\sqrt{m_{k,t}}}\right)\] \[=\sum_{k=1}^{\infty}\frac{\left(\beta_{k-1}-\beta_{k}\right)m}{ \sqrt{m_{k,t}}}.\]

Under event \(\mathcal{H}_{t}\), we have

\[\delta_{p_{t}} \leq\sum_{i\in S_{t}}2\sqrt{\frac{\log T}{T_{i,t-1}}}=2\sqrt{\log T }\cdot\sum_{i\in S_{t}}\frac{1}{\sqrt{T_{i,t-1}}}\] \[<2\sqrt{\log T}\cdot\sum_{k=1}^{\infty}\frac{\left(\beta_{k-1}- \beta_{k}\right)m}{\sqrt{m_{k,t}}}=2\sum_{k=1}^{\infty}\frac{\beta_{k-1}-\beta_{ k}}{\sqrt{\alpha_{k}}}\cdot\delta_{p_{t}}\leq\delta_{p_{t}},\]

where the last inequality is due to Lemma C.4. We reach a contradiction here, hence the lemma follows.

## Appendix D Miscellaneous

### Notations

The following table contains the notations used in this paper.

\begin{table}
\begin{tabular}{|c|c|} \hline Notation & Meaning \\ \hline \(N\) & The total amount of data. \\ \hline \(n\in[N]\) & The number of data. \\ \hline \(m\) & The number of types. \\ \hline \(p:[N]\rightarrow[0,1]\) & A price curve. \\ \hline \(\overline{\mathcal{P}}\) & A set of discretized price curves. \\ \hline \(v_{i}:[N]\rightarrow[0,1]\) & The valuation curve for type \(i\in[m]\). \\ \hline \(\mathcal{V}=\{v_{i}:i\in[m]\}\) & The set of all valuation curves. \\ \hline \(n_{i,p}\) & The amount of data type \(i\in[m]\) purchases at price curve \(p\). \\ \hline \(r(i,p)=p(n_{i,p})\) & The revenue from type \(i\in[m]\) under price curve \(p\). \\ \hline \(q=(q_{1},q_{2},\ldots,q_{m})\) & The type distribution. \\ \hline \(\operatorname{rev}(p)\) & The expected revenue under price \(p\). \\ \hline \(i_{t}\in[m]\) & The type of buyer on round \(t\in[T]\). \\ \hline \(p_{t}:[N]\rightarrow[0,1]\) & The price curve on round \(t\in[T]\). \\ \hline \(S_{t}\) & The set of types that would make a purchase at price \(p_{t}\). \\ \hline \(S_{p}\) & The set of types that would make a purchase at price \(p\). \\ \hline \(T_{i,t}\stackrel{{\Delta}}{{=}}\sum_{\tau=1}^{t}\mathbbm{1}(i\in S _{\tau})\) & The number of times that type \(i\) appears in set \(S_{\tau}\) for \(\tau\in\{1,\ldots,t\}\). \\ \hline \(\mathcal{P}=\{p\in[N]\rightarrow[0,1]:p(0)=0\}\) & The set of all pricing curves. \\ \hline \(L\) & Smoothness constant of valuation curves. \\ \hline \(J\) & Diminishing return constant of valuation curves. \\ \hline \end{tabular}
\end{table}
Table 3: Table of notations.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We fully included paper's contributions and scope in the appendix. See SS1.1 for the summary of our contributions.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See SS6 for future works on relaxing one of key assumptions of the paper.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provided the full set of assumptions. Moreover, we provided the full proofs of each Lemma and Theorem in this paper both in main text and Appendices. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: This paper does not include experiments.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This paper does not include experiments requiring code.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This paper does not include experiments.
7. **Experiment Statistical Significance**Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This paper does not include experiments.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This paper does not include experiments.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conforms, in every respect, with the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our research is theoretical and have no societal impact in a foreseeable future.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper poses no such risks.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use existing assets.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: This paper does not involve crowdsourcing nor research with human subjects.