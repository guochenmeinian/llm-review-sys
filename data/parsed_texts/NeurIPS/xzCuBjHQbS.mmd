# Random Function Descent

 Felix Benning

University of Mannheim

felix.benning@uni-mannheim.de &Leif Doring

University of Mannheim

leif.doering@uni-mannheim.de

###### Abstract

Classical worst-case optimization theory neither explains the success of optimization in machine learning, nor does it help with step size selection. In this paper we demonstrate the viability and advantages of replacing the classical 'convex function' framework with a 'random function' framework. With complexity \(\mathcal{O}(n^{3}d^{3})\), where \(n\) is the number of steps and \(d\) the number of dimensions, Bayesian optimization with gradients has not been viable in large dimension so far. By bridging the gap between Bayesian optimization (i.e. random function optimization theory) and classical optimization we establish viability. Specifically, we use a'stochastic Taylor approximation' to rediscover gradient descent, which is scalable in high dimension due to \(\mathcal{O}(nd)\) complexity. This rediscovery yields a specific step size schedule we call Random Function Descent (RFD). The advantage of this random function framework is that RFD is scale invariant and that it provides a theoretical foundation for common step size heuristics such as gradient clipping and gradual learning rate warmup.

## 1 Introduction

Cost function minimization is one of the most fundamental mathematical problems in machine learning. Gradient-based methods, popular for this task, require a step size, typically chosen using established heuristics. This article aims to deepen the theoretical understanding of these heuristics and proposes a new algorithm based on this insight.

Classical optimization theory uses \(L\)-smoothness, which limits the rate of change of the gradient by \(L\), to provide some convergence guarantees for learning rates smaller than \(1/L\) [e.g. 38]. As this theory is based on an upper bound (the worst case), the learning rate \(1/L\) is naturally much more conservative than necessary on average. Even if \(L\) was known, this learning rate would therefore be impractical. Since line search algorithms typically require access to full cost function evaluations, the field of machine learning (ML) therefore relies heavily on step size heuristics [e.g. 48, 49, 42, 20]. To investigate these heuristics, we introduce new ideas based on a 'random function' perspective.

While automatic step size selection in the convex function framework is possible [11], convexity is generally only satisfied asymptotically and locally. So the understanding of the initial stages of optimization, which includes the warmup heuristic [20], greatly benefits from a framework which also admits non-convex functions. This objective is achieved by the 'random function' framework we investigate.

Many successful algorithms in computer science are significantly slower in the worst case than in the average case based on a probabilistic framework (e.g. Quicksort [23] or the simplex algorithm [e.g. 6]). On random quadratic functions the average case behavior of first order optimizers is already being investigated by the ML community [e.g. 58, 43, 33, 12, 9, 40, 41]. Interested in the landscape of high dimensional random functions as a model for'spin glasses', the physics community independently started studying the average case of optimization as well [e.g. 4, 15, 37, 51, 24], albeit not geared for ML algorithms.

Average case analysis fundamentally requires a prior distribution over possible cost functions. The evaluations seen so far then result in a posterior over the cost of other parameter inputs. Using this posterior for optimization is called "Bayesian optimization" (BO) (e.g. 32, 47, 16, 2), which is best known in the context of low dimensional optimization (e.g. hyperparameter tuning) in the ML community. BO is treated like a zero order method for low dimensional problems due to the \(\mathcal{O}(n^{3})\) complexity for the covariance matrix inversion of the \(n\) evaluations seen so far, which increases to \(\mathcal{O}(n^{3}d^{3})\) when gradient information is included (e.g. 35, 55), where \(d\) is the input dimension of our cost function. This limits classic BO to relatively small dimensions even under sparsity considerations (e.g. 45, 39).

While the BO algorithms developed in the 'random function framework' might not have been viable in high dimension so far, due to their computational complexity, this framework is already used to explain the high relative frequency of saddle points in high dimension (10) and to explain the highly predictable progress optimizers make on high dimensional cost functions (5).

In this work we bridge the gap between BO and (computationally viable) gradient based methods, derived from the first Taylor approximation, with the introduction of a stochastic Taylor approximation based on a forgetful BO posterior. The optimization method "Random Function Descent" (RFD), resulting from the minimization of this stochastic Taylor approximation, coincides with a specific form of gradient descent which establishes its viability in high dimension. The advantages of its BO heritage are scale invariance and an explicit step size schedule, which illuminates the inner workings of step size heuristics such as gradient clipping (42) and gradual learning rate warmup (20).

Our contributions and outlineThe main goal of this paper is to demonstrate the **viability** and **advantages** of replacing the classical "convex function" framework with a "random function" framework. Theorem 4.2 is the main theoretical result establishing **viability** (computatability and scalable complexity) for a given covariance model. Section 6 is concerned with practical estimation of the covariance model and viability is demonstrated with a practical example in the MNIST case study (Section 7). The **advantages** of this approach are scale invariance (Advantage 2.3) and an explicit step size schedule, which does not require expensive tuning and explains existing ML heuristics such as warmup (cf. Section 5.2). This explanation of the initial stage of optimization could never be delivered by the convex framework, because the convexity assumption is not fulfilled initially so it can at best explain asymptotic behavior.

**Sec. 2**: We motivate a stochastic Taylor approximation and RFD and prove its scale-invariance.
**Sec. 3**: We briefly motivate and discuss the common distributional assumptions in BO.
**Sec. 4**: We establish the connection between RFD and gradient descent.
**Sec. 5**: We investigate the step size schedule suggested by RFD. In particular we

1. calculate explicit formulas for the step size schedules resulting from common covariance models (Table 1, Sec. C),
2. analyze the general asymptotic behavior (Sec. 5.1),
3. discuss how RFD explains gradient clipping and learning rate warmup (Sec. 5.2),
**Sec. 6**: We develop a non-parametric variance estimation method, which is robust with respect to the choice of covariance kernel. Finally, we present an extension of RFD to mini-batch losses.
**Sec. 7**: We conduct a case study on the MNIST dataset.
**Sec. 8**: We discuss extensions (see also Sec. E) and limitations.

## 2 The random function descent algorithm

The classic derivation of gradient descent (e.g. 38, p. 29), adds an \(L\)-smoothness based trust bound to the first Taylor approximation, \(T[J(\theta)\mid J(w),\nabla J(w)]\), of the cost function \(J\) around \(w\) resulting in the gradient step

\[w-\tfrac{1}{L}\nabla J(w)=\operatorname*{argmin}_{\theta}T[J(\theta)\mid J(w),\nabla J(w)]+\tfrac{L}{2}\|\theta-w\|^{2}.\]

Our unusual notation for the Taylor approximation \(T[J(\theta)\mid J(w),\nabla J(w)]\) is meant to highlight the connection to the stochastic Taylor approximation we define below.

**Definition 2.1** (Stochastic Taylor approximation).: We define the first order stochastic Taylor approximation of a random (cost) function1\(\mathbf{J}\) around \(w\) by the conditional expectation

Footnote 1: **Remark on terminology**: “stochastic process” [e.g. 53], “random field” [e.g. 1] and “random function” [e.g. 36] are all synonyms. However the latter seems most descriptive of random variables in the set of functions. “Gaussian processes” are naturally Gaussian stochastic processes, i.e. Gaussian random functions. To better distinguish random functions from deterministic functions, we use bold letters to denote random functions (as the usual convention of capitalizing random variables often clashes with other conventions for functions).

\[\mathbb{E}[\mathbf{J}(\theta)\mid\mathbf{J}(w),\nabla\mathbf{J}(w)].\]

This is the best \(L^{2}\) approximation [30, Cor. 8.17] of \(\mathbf{J}(\theta)\) provided first order knowledge of \(\mathbf{J}\) at \(w\).

We call this the'stochastic Taylor approximation' because this approximation only makes use of derivatives in a single point. While the standard Taylor approximation is a polynomial approximation, the'stochastic Taylor approximation' is the best approximation in an \(L^{2}\) sense and already mean-reverting by itself, i.e. it naturally incorporates covariance-based trust (cf. Figure 1). While \(L\)-smoothness-based trust _guarantees_ that the gradient still points in the direction we are going (for learning rates smaller \(1/L\)), covariance based trust tells us whether the derivative is still negative _on average_. Minimizing the stochastic Taylor approximation is therefore optimized for the average case. Since convergence proofs for gradient descent typically rely on an improvement _guarantee_, proving convergence is significantly harder in the average case and we answer this question only partially in Corollary 5.3.

**Definition 2.2** (Random Function Descent - RFD).: Select \(w_{n+1}\) as the minimizer2 of the first order stochastic Taylor approximation

Footnote 2: we ignore throughout the main body that \(\operatorname{argmin}\) could be set-valued and that the \(w_{n}\) would be random variables (cf. Section D.1.1 for a formal approach).

\[w_{n+1}:=\operatorname*{argmin}_{w}\mathbb{E}[\mathbf{J}(w)\mid\mathbf{J}(w_{n }),\nabla\mathbf{J}(w_{n})].\]

Properties of RFDBefore we make RFD more explicit in Section 4, we discuss some properties which are easier to see in the abstract form.

First, observe that RFD is greedy and forgetful in the same way gradient descent is greedy and forgetful when derived as the minimizer of the regularized first Taylor approximation, or the Newton method as the minimizer of the second Taylor approximation. This is because the Taylor approximation only uses derivatives from the last point \(w_{n}\) (forgetful), and we minimize this approximation (greedy). Since momentum methods retain some information about past gradients, they are not as forgetful. We therefore expect a similar improvement could be made for RFD in the future.

Second, it is well known that classical gradient descent with exogenous step sizes (and most other first order methods) lack the scale invariance property of the Newton method [e.g. 21, 13]. Scale invariance means that scaling the input parameters \(w\) or the cost itself (e.g. by switching from the mean squared error to the sum squared error) does not change the points selected by the optimization method.

**Advantage 2.3** (Scale invariance).: _RFD is invariant to additive shifts and positive scaling of the cost \(\mathbf{J}\). RFD is also invariant with respect to transformations of the parameter input of \(\mathbf{J}\) by differentiable bijections whose Jacobian is invertible everywhere (e.g. invertible linear maps)._

While invariance to bijections of inputs is much stronger than the affine invariance offered by the Newton method, non-linear bijections will typically break the 'isotropy' assumption of the following

Figure 1: The stochastic Taylor approximation naturally contains a trust bound in contrast to the classical one. Here \(\mathbf{J}\) is a Gaussian random function (with covariance as in Equation (11), with length scale \(s=2\) and variance \(\sigma^{2}=1\)). The ribbon represents two conditional standard deviations around the conditional expectation.

section which makes RFD explicit. This invariance should therefore be viewed as an opportunity to look for the bijection of inputs which ensures isotropy (e.g. a whitening transformation). The discussion of geometric anisotropy in Section E.1 is conducive to build an understanding of this.

## 3 A distribution over cost functions

It is impossible to make average case analysis explicit without a distribution over functions, so we use the canonical distributional assumption of Bayesian optimization (e.g. [16; 55; 44]), 'isotropic Gaussian random functions'. This assumption was also used in the high dimensional setting by Dauphin et al. [10] to argue that saddle points are much more common than minima in high dimension, which is often cited to explain why second order methods are uncommon in machine learning.

To motivate isotropy, we note that in average case analysis the uniform distribution is popular, since it weighs all problem instances equally (e.g. all possible permutations in sorting). Isotropy is such a uniformity assumption, which essentially requires "\(\mathbb{P}(\mathbf{J}=J)=\mathbb{P}(\mathbf{J}=J\circ\phi)\)", for all isometries \(\phi\). In other words, the probability that our cost function is equal to \(J\) is equal to the probability that it is equal to a shifted and turned version of \(J\), given by \(J\circ\phi\).

Since the probability of any single realization of a cost function \(J\) is zero, the equation we put in quotes is mathematically unsound. The formal definition follows below.

**Definition 3.1** (Isotropy).: A random function \(\mathbf{J}\) is called isotropic if its distribution stays the same under isometric transformations of its input, i.e. for any isometry \(\phi\) we have

\[\mathbb{P}_{\mathbf{J}}=\mathbb{P}_{\mathbf{J}\circ\phi}.\]

If \(\mathbf{J}\) is Gaussian, isotropy is well known (e.g. 44; 1) to be equivalent to the condition that there exists \(\mu\in\mathbb{R}\) and a function \(C:\mathbb{R}\to\mathbb{R}\) such that for all \(w,\tilde{w}\in\mathbb{R}^{d}\) the expectation and covariance are

\[\mathbb{E}[\mathbf{J}(w)]=\mu,\qquad\mathrm{Cov}(\mathbf{J}(w),\mathbf{J}( \tilde{w}))=C(\tfrac{\|w-\tilde{w}\|^{2}}{2}).\]

For these isotropic Gaussian random functions we use the notation \(\mathbf{J}\sim\mathcal{N}(\mu,C)\).

We discuss generalizations to isotropy in Section F and E.1, but for ease of exposition we retain the (stationary) isotropy assumption throughout the main body. Note that the Gaussian assumption can be statistically tested in practice (cf. Figure 4), but it is also straightforward to reproduce our results with the "best linear unbiased estimator" (BLUE) (Section E.3) in place of the conditional expectation to remove the Gaussian assumption. We finally want to highlight that, in contrast to the uniformity assumption on finite sets, 'isotropic Gaussian random functions' leave us with a family of plausible distributions. It is therefore necessary to estimate \(\mu\) and \(C\), which is the topic of Section 6.

## 4 Relation to gradient descent

While we were able to define RFD abstractly without any assumptions on the distribution \(\mathbb{P}_{\mathbf{J}}\) of the random cost \(\mathbf{J}\), an explicit calculation requires distributional assumptions and we have motivated isotropic Gaussian random functions in Section 3 for this purpose. The assumption of isotropy allows for an explicit version of the stochastic Taylor approximation which then immediately leads to an explicit version of RFD.

**Lemma 4.1** (Explicit first order stochastic Taylor approximation).: _For \(\mathbf{J}\sim\mathcal{N}(\mu,C)\), the first order stochastic Taylor approximation is given by_

\[\mathbb{E}[\mathbf{J}(w-\mathbf{d})\mid\mathbf{J}(w),\nabla\mathbf{J}(w)]= \mu+\frac{C(\tfrac{\|\mathbf{d}\|^{2}}{2})}{C(0)}(\mathbf{J}(w)-\mu)-\frac{C ^{\prime}\big{(}\tfrac{\|\mathbf{d}\|^{2}}{2}\big{)}}{C^{\prime}(0)}\langle \mathbf{d},\nabla\mathbf{J}(w)\rangle.\]

The explicit version of RFD follows by fixing the step size \(\eta=\|\mathbf{d}\|\) and optimizing over the direction first.

**Theorem 4.2** (Explicit RFD).: _Let \(\mathbf{J}\sim\mathcal{N}(\mu,C)\), then RFD coincides with gradient descent_

\[w_{n+1}=w_{n}-\eta_{n}^{*}\tfrac{\nabla\mathbf{J}(w_{n})}{\|\nabla\mathbf{J} (w_{n})\|},\]_where the RFD step sizes are given by_

\[\eta_{n}^{*}:=\operatorname*{argmin}_{\eta\in\mathbb{R}}\frac{C\big{(}\frac{\eta^ {2}}{2}\big{)}}{C(0)}(\mathbf{J}(w_{n})-\mu)-\eta\frac{C^{\prime}\big{(}\frac{ \eta^{2}}{2}\big{)}}{C^{\prime}(0)}\|\nabla\mathbf{J}(w_{n})\|.\] (1)

While the descent direction is a universal property for all isotropic Gaussian random functions, it follows from (1) that the step sizes depend much more on the specific covariance structure. In particular it depends on the decay rate of the covariance acting as the trust bound.

_Remark 4.3_ (Scalable complexity).: While Bayesian optimization typically has computational complexity \(\mathcal{O}(n^{3}d^{3})\) in number of steps \(n\) and dimensions \(d\)[55; 45], RFD under the isotropy assumption has the same computational complexity as gradient descent (i.e. \(\mathcal{O}(nd)\)).

_Remark 4.4_ (Step until the given information is no longer informative).: While \(L\)-smoothness-based trust prescribes step sizes that _guarantee_ the slope to point downwards over the entire step, RFD prescribes steps which are exactly large enough that the gradient is no longer correlated to the previously observed evaluation. This is because the first order condition demands

\[0\stackrel{{!}}{{=}}\nabla\mathbb{E}[\mathbf{J}(w)\mid\mathbf{J} (w_{n}),\nabla\mathbf{J}(w_{n})]=\mathbb{E}[\nabla\mathbf{J}(w)\mid\mathbf{J} (w_{n}),\nabla\mathbf{J}(w_{n})].\]

And for measurable functions \(\phi:\mathbb{R}^{d+1}\to\mathbb{R}\) such that \(\Phi=\phi(\mathbf{J}(w_{n}),\nabla\mathbf{J}(w_{n}))\) is sufficiently integrable, \(\Phi\) is then uncorrelated from \(\partial_{i}\mathbf{J}(w)\) by the first order condition

\[\operatorname{Cov}(\partial_{i}\mathbf{J}(w),\Phi)=\mathbb{E}\Big{[}\underbrace {\mathbb{E}[\partial_{i}\mathbf{J}(w)\mid\mathbf{J}(w_{n}),\nabla\mathbf{J}( w_{n})]}_{=0}(\Phi-\mathbb{E}[\Phi])\Big{]}=0.\]

## 5 The RFD step size schedule

While classical theory leads to 'learning rates', RFD suggests'step sizes' applied to normalized gradients representing the actual length of the step size. In the following we thus make the distinction

\[w_{n+1}=w_{n}-\underbrace{h_{n}}_{\text{learning rate}}\nabla\mathbf{J}(w_{n})=w_{n}- \underbrace{\eta_{n}}_{\text{step size}}\frac{\nabla\mathbf{J}(w_{n})}{\| \nabla\mathbf{J}(w_{n})\|}.\]

To get a better feel for the step sizes suggested by RFD, it is enlightening to divide (1) by \(\mu-\mathbf{J}(w_{n})\) which results in a minimization problem

\[\eta^{*}:=\eta^{*}(\Theta):=\operatorname*{argmin}_{\eta}q_{\Theta}(\eta) \qquad\text{for}\qquad q_{\Theta}(\eta):=-\frac{C\big{(}\frac{\eta^{2}}{2} \big{)}}{C(0)}-\eta\frac{C^{\prime}\big{(}\frac{\eta^{2}}{2}\big{)}}{C^{ \prime}(0)}\Theta,\] (2)

which is only parametrized by the "gradient cost quotient"

\[\Theta_{n}=\frac{\|\nabla\mathbf{J}(w_{n})\|}{\mu-\mathbf{J}(w_{n})},\]

i.e. \(\eta_{n}^{*}=\eta^{*}(\Theta_{n})\). This minimization problem can be solved explicitly for the most common [44; ch. 4] differentiable isotropic covariance models, see Table 1, Figure 2 and Appendix C for details.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline \multicolumn{1}{c}{Model} & \multicolumn{3}{c}{RFD step size \(\eta^{*}\) for \(\mathbf{J}(w)\leq\mu\)} & \multicolumn{1}{c}{A-RFD} \\ \cline{3-5} \multicolumn{1}{c}{} & & General case \(\big{(}\)with \(\Theta=\frac{\|\nabla\mathbf{J}(w)\|}{\mu-\mathbf{J}(w)}\big{)}\) & \(\mathbf{J}(w)=\mu\) & \(\Theta\to 0\) \\ \hline \multicolumn{1}{c}{Matern} & \(\nu\) & & & \\ \hline \multirow{5}{*}{Squared-exponential} & \multirow{2}{*}{\(\infty\)} & \multirow{2}{*}{\(\sqrt{\big{(}\frac{u-y(w)}{2}\big{)}^{2}+s^{2}\|\nabla\mathbf{J}(w)\|^{2}+ \frac{\mu-\mathbf{J}(w)}{2}\|\nabla\mathbf{J}(w)\|\)} & \multirow{2}{*}{\(s\)} & \multirow{2}{*}{\(s^{2}\Theta\)} \\  & & & & \\ \hline Rational & \multirow{2}{*}{\(\beta\)} & \multirow{2}{*}{\(s\sqrt{\beta}\operatorname{Root}\limits_{\eta}\big{(}-1+\frac{\sqrt{\beta}}{s \Theta}\eta+(1+\beta)\eta^{2}+\frac{\sqrt{\beta}}{s\Theta}\eta^{3}\big{)}\)} & \multirow{2}{*}{\(s\sqrt{\frac{\beta}{1+\beta}}\)} & \multirow{2}{*}{\(s^{2}\Theta\)} \\   quadratic & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: RFD step size (cf. Figure 2 and Eq. (11), (13), (14) for the formal definitions of the models). In particular, \(s\) is the length scale in all covariance models.

Figure 2 can be interpreted as follows: At the start of optimization, the cost should be roughly equal to the average cost \(\mu\approx\mathbf{J}(w)\), so the gradient cost quotient \(\Theta\) is infinite and the step sizes are therefore given by \(\eta^{*}(\infty)\) (also listed in its own column in Table 1). As we start minimizing, the difference \(\mu-\mathbf{J}(w)\) becomes positive. Towards the end of minimization this difference no longer changes as the cost no longer decreases. I.e. towards the end the gradient cost quotient \(\Theta\) is roughly linear in the gradient \(\|\nabla\mathbf{J}(w)\|\). The derivative \(\frac{d}{d\Theta}\eta^{*}(0)\) of \(\eta^{*}(\Theta)\) at zero then effectively results in a constant asymptotic learning rate.

### Asymptotic learning rate

To explain the claim above, note that the gradient cost quotient \(\Theta\) converges to zero towards the end of optimization, because the gradient norm converges to zero. A first order Taylor expansion of \(\eta^{*}\) would therefore imply

\[\eta^{*}(\Theta)\approx\eta^{*}(0)+\tfrac{d}{d\Theta}\eta^{*}(0)\Theta= \underbrace{\tfrac{d}{d\Theta}\eta^{*}(0)}_{\text{asymptotic learning rate}}\|\nabla\mathbf{J}(w)\|\]

assuming \(\eta^{*}(0)=0\) and differentiability of \(\eta^{*}\), which is a reasonable educated guess based on the the examples in Figure 2. But since the RFD step sizes \(\eta^{*}\) are abstractly defined as an \(\operatorname*{argmin}\), it is necessary to formalize this intuition for general covariance models. First, we define asymptotic step sizes as an object towards which we can prove convergence. Then we prove convergence, proving they are well defined. In addition, we obtain a more explicit formula for the asymptotic learning rate.

**Definition 5.1** (A-Fd).: We define the step sizes of "asymptotic RFD" (A-RFD) to be the minimizer of the second order Taylor approximation \(T_{2}q_{\Theta}\) of \(q_{\Theta}\) around zero

\[\hat{\eta}(\Theta):=\operatorname*{argmin}_{\eta}T_{2}q_{\Theta}(\eta)=\tfrac {C(0)}{-C^{*}(0)}\Theta=\underbrace{\tfrac{C(0)}{C^{\prime}(0)(\mathbf{J}(w)- \mu)}}_{\text{asymptotic learning rate}}\|\nabla\mathbf{J}(w)\|.\]

In the following we prove that these are truly asymptotically equal to the step sizes \(\eta^{*}\) of RFD.

**Proposition 5.2** (A-RFD is well defined).: _Let \(\mathbf{J}\sim\mathcal{N}(\mu,C)\) and assume there exists \(\eta_{0}>0\) such that the correlation for larger distances \(\eta\geq\eta_{0}\) are bounded smaller than \(1\), i.e. \(\tfrac{C(\eta^{2}/2)}{C(0)}<\rho\in(0,1)\). Then the step sizes of RFD are asymptotically equal to the step sizes of A-RFD, i.e._

\[\hat{\eta}(\Theta)\sim\eta^{*}(\Theta)\quad\text{as}\quad\Theta\to 0.\]

Note that the assumption is essentially always satisfied, since the Cauchy-Schwarz inequality implies

\[C\big{(}\tfrac{\|w-\hat{w}\|^{2}}{2}\big{)}=\operatorname{Cov}(\mathbf{J}(w), \mathbf{J}(\tilde{w}))\leq\sqrt{\operatorname{Var}(\mathbf{J}(w))\operatorname {Var}(\mathbf{J}(\tilde{w}))}=C(0),\]

where equality requires the random variables to be almost surely equal [30]. If the random function is not periodic or constant, this will generally be strict. In the proof, this requirement is only needed to ensure that \(\eta^{*}\) is not very large. The smallest local minimum of \(q_{\Theta}\) is always close to \(\hat{\eta}\) even without this assumption (which ensures it is a global minimum).

Figure 2 illustrates that \(\eta^{*}\to 0\) should imply \(\Theta\to 0\), resulting in a weak convergence guarantee.

**Corollary 5.3**.: _Assume \(\eta^{*}\to 0\) implies \(\Theta\to 0\), the cost \(\mathbf{J}\) is bounded, has continuous gradients and RFD converges to some point \(w_{\infty}\). Then \(w_{\infty}\) is a critical point and the RFD step sizes \(\eta^{*}\) are asymptotically equal to \(\hat{\eta}\)._

For the squared exponential covariance model we formally prove that \(\eta^{*}\) is strictly monotonously increasing in \(\Theta\) and thus \(\eta^{*}\to 0\) implies \(\Theta\to 0\) (Prop. C.3). The 'bounded' and 'continuous gradients' assumptions are almost surely satisfied for all sufficiently smooth covariance functions [cf. 1], where three times differentiable is more than enough smoothness.

### RFD step sizes explain common step size heuristics

Asymptotically, RFD suggests constant learning rates, similar to the classical \(L\)-smooth setting. We thus define these asymptotic learning rates (as the limit of the learning rates \(h_{n}\) of iteration \(n\)) to be

\[h_{\infty}:=\frac{C(0)}{C^{\prime}(0)(\mathbf{J}(w_{\infty})-\mu)},\] (3)

where \(\mathbf{J}(w_{\infty})\) is the cost we reach in the limit. If we used these asymptotic learning rates from the start, step sizes would become too large for large gradients, as RFD step sizes exhibit a plateau (cf. Figure 2). To emulate the behavior of RFD with a piecewise linear function, we could introduce a cutoff whenever our step size exceeds the initial step size \(\eta^{*}(\infty)\), i.e.

\[w_{n+1}=w_{n}-\min\Bigl{\{}h_{\infty},\frac{\eta^{*}(\infty)}{\|\nabla\mathbf{ J}(w_{n})\|}\Bigr{\}}\nabla\mathbf{J}(w_{n}).\] (gradient clipping)

At this point we have rediscovered 'gradient clipping' [42]. Since the rational quadratic covariance has the same asymptotic learning rate \(h_{\infty}\) for every \(\beta\), its parameter \(\beta\) controls the step size bound \(\eta^{*}(\infty)\) of gradient clipping (cf. Table 1, Figure 2).

Pascanu et al. [42] motivated gradient clipping with the geometric interpretation of movement towards a 'wall' placed behind the minimum. This suggests that clipping should happen towards the end of training. This stands in contrast to a more recent step size heuristic, "(linear) warmup" [20], which suggests smaller learning rates at the start (i.e. \(h_{0}=\frac{\eta^{*}(\infty)}{\|\nabla\mathbf{J}(w_{0})\|}\)) and gradual ramp-up to the asymptotic learning rate \(h_{\infty}\). In other words, gradients are not clipped due to some wall next to the minimum, but because the step sizes would be too large at the start otherwise. Goyal et al. [20] further observe that 'constant warmup' (i.e. a step change of learning rates akin to gradient clipping) performs worse than gradual warmup. Since RFD step sizes suggest this gradual increase, we argue that they may have discovered RFD step sizes empirically (also cf. Figure 3).

## 6 Mini-batch loss and covariance estimation

Since we do not have access to evaluations of the cost \(\mathbf{J}\) in practice, we need to prove some results about stochastic losses \(\ell_{i}\) before we can apply RFD in practice. For this, assume that we have independent identically distributed (iid) data \(X_{i}\) independent of the true relationship \(\mathbf{f}\) drawn from \(\mathbb{P}_{\mathbf{f}}\) resulting in labels \(Y_{i}=\mathbf{f}(X_{i})+\varsigma_{i}\), where we have added independent iid noise \(\varsigma_{i}\), resulting in loss and cost

\[\ell_{i}(w):=\ell\bigl{(}w,(X_{i},Y_{i})\bigr{)}\quad\text{and}\quad\mathbf{J }(w):=\mathbb{E}[\ell_{i}(w)\mid\mathbf{f}].\]

In this setting we confirm (cf. Lemma D.9), that the stochastic approximation errors

\[\epsilon_{i}(w):=\ell_{i}(w)-\mathbf{J}(w)\]

are independent conditional on the true relationship \(\mathbf{f}\). In particular they (and all their derivatives) are uncorrelated and also uncorrelated from \(\mathbf{J}\). It follows that mini-batch losses

\[\mathcal{L}_{b}(w):=\frac{1}{b}\sum_{i=1}^{b}\ell_{i}(w)=\mathbf{J}(w)+\frac {1}{b}\sum_{i=1}^{b}\epsilon_{i}(w)\] (4)

have variance

\[\operatorname{Var}(\mathcal{L}_{b}(w))=\operatorname{Var}(\mathbf{J}(w))+ \frac{1}{b}\operatorname{Var}(\epsilon_{1}(w))\overset{\text{isotropy}}{=}C(0 )+\frac{1}{b}C_{\epsilon}(0),\] (5)

where we assume \(\mathbf{J}\sim\mathcal{N}(\mu,C)\) and \(\epsilon_{i}\sim\mathcal{N}(0,C_{\epsilon})\) in the last equation for simplicity. But this step did not yet require the distributional Gaussian assumption beyond the mean and variance.

### Variance estimation

Recall that the asymptotic learning rate \(h_{\infty}\) in Equation (3) only depends on \(C(0)\) and \(C^{\prime}(0)\). So if we estimate these values, we are certain to get the right RFD step sizes asymptotically without knowing the entire covariance kernel \(C\).

Equation (5) reveals that for \(Z_{b}:=(\mathcal{L}_{b}(w)-\mu)^{2}\) we have

\[\mathbb{E}[Z_{b}]=\beta_{0}+\tfrac{1}{b}\beta_{1}\qquad\text{i.e.}\qquad Z_{b}= \beta_{0}+\tfrac{1}{b}\beta_{1}+\text{noise}\]

with bias \(\beta_{0}=C(0)\) and slope \(\beta_{1}=C_{\epsilon}(0)\). So a linear regression on samples \((\tfrac{1}{b_{b}},Z_{b_{k}})_{k\leq n}\) allows for the estimation of \(\beta_{0}\) and \(\beta_{1}\). Using the Gaussian assumption from (5), the variance of \(Z_{b}\) is the (centered) fourth moment of \(\mathcal{L}_{b}\), which is given by

\[\sigma_{b}^{2}:=\operatorname{Var}(Z_{b})=\mathbb{E}[Z_{b}^{4}]-\mathbb{E}[Z_ {b}^{2}]^{2}=2\operatorname{Var}(\mathcal{L}_{b}(w))^{2}=2(\beta_{0}+\tfrac{1} {b}\beta_{1})^{2}.\]

In particular the variance of \(Z_{b}\) depends on the batch size \(b\). The linear regression is therefore heteroskedastic. Weighted least squares (WLS) [e.g. 28, Theorem 4.2] is designed to handle this case, but for its application the variance of \(Z_{b}\) is needed. Since \(\beta_{0},\beta_{1}\) are the parameters we wish to estimate, we find ourselves in the paradoxical situation that we need \(\beta\) to obtain \(\beta\). Our solution to this problem is to start with a guess of \(\beta_{0},\beta_{1}\), apply WLS to obtain a better estimate and repeat this bootstrapping procedure until convergence. Since all \(Z_{b}\) have the same underlying cost \(\mathbf{J}\), we sample the parameters \(w\) randomly to reduce their covariance (details in Sec. B).

The same procedure can be applied to obtain \(C^{\prime}(0)\), where the counterpart of Equation (5) is given by

\[\operatorname{Var}(\partial_{i}\mathcal{L}_{b}(w))=\operatorname{Var}( \partial_{i}\mathbf{J}(w))+\tfrac{1}{b}\operatorname{Var}(\partial_{i} \epsilon_{1}(w))\stackrel{{\text{isotropy}}}{{=}}-(C^{\prime}(0 )+\tfrac{1}{b}C^{\prime}_{\epsilon}(0)).\]

_Remark 6.1_.: Under the isotropy assumption the partial derivatives are iid, so the expectation of \(\|\nabla\mathcal{L}_{b}(w)\|^{2}=\sum_{i=1}^{d}(\partial_{i}\mathcal{L}_{b}(w ))^{2}\) is this variance scaled by \(d\). In particular the variance needs to scale with \(\tfrac{1}{d}\) to keep the gradient norms (and thus the Lipschitz constant of \(\mathbf{J}\)) stable. This observation is closely related to "isoperimetry" [e.g. 7], for details see [5]. Removing the isotropy assumption and estimating the variance component-wise is most likely how "adaptive" step sizes [e.g. 14, 29], like the ones used by Adam, work (cf. Sec. E.1).

Batch size distributionBefore we can apply linear regression to the samples \((\tfrac{1}{b_{k}},Z_{b_{k}})_{k\leq n}\), it is necessary to choose the batch sizes \(b_{k}\). As this choice is left to us, we calculate the variance of our estimator \(\hat{\beta}_{0}\) of \(\beta_{0}\) explicitly (Lemma B.2), in order to minimize this variance subject to a sample budget \(\alpha\) over the selection of batch sizes

\[\min_{n,b_{1},\dots,b_{n}}\operatorname{Var}(\hat{\beta}_{0})\quad\text{s.t.} \qquad\sum_{\begin{subarray}{c}k=1\\ \text{samples used}\end{subarray}}^{n}b_{k}\leq\alpha.\] (6)

Since this optimization problem is very difficult to solve, we rephrase it in terms of the empirical distribution of batch sizes \(\nu_{n}=\tfrac{1}{n}\sum_{i=1}^{n}\delta_{b_{i}}\). Optimizing over distributions is still difficult, but we explain in Section B.1 how to heuristically arrive at the parametrization

\[\nu(b)\propto\exp\bigl{(}\lambda_{1}\tfrac{1}{\sigma_{b}^{2}}-\lambda_{2}b \bigr{)},\qquad b\in\mathbb{N}\]

where the parameters \(\lambda_{1},\lambda_{2}\geq 0\) can then be used to optimize (6). Due to our usage of \(\sigma_{b}^{2}\) this has to be bootstrapped.

Covariance estimationWhile the variance estimates above ensure correct asymptotic learning rates, we motivated in Section 5.2 that asymptotic learning rates alone would result in too large step sizes at the beginning. We therefore use the estimates of \(C(0)\) and \(C^{\prime}(0)\) to fit a covariance model, effectively acting as a gradient clipper while retaining the asymptotic guarantees. Note that covariance models with less than two parameters are generally fully determined by these values.

### Stochastic RFD (S-RFD)

It is reasonable to ask whether there is a'stochastic gradient descent'-like counterpart to the 'gradient descent'-like RFD. The answer is yes, and we already have all the required machinery.

**Extension 6.2** (S-RFD).: _For loss \(\mathbf{J}\sim\mathcal{N}(\mu,C)\) and stochastic errors \(\epsilon_{i}\stackrel{{\text{iid}}}{{\sim}}\mathcal{N}(0,C_{ \epsilon})\) we have_

\[\operatorname*{argmin}_{\mathbf{d}}\mathbb{E}[\mathbf{J}(w-\mathbf{d})\mid \mathcal{L}_{b}(w),\nabla\mathcal{L}_{b}(w)]=\eta^{*}(\Theta)\tfrac{\nabla \mathcal{L}_{b}(w)}{\|\nabla\mathcal{L}_{b}(w)\|}\]_with the same step size function \(\eta^{*}\) as for RFD, but modified \(\Theta\)_

\[\Theta=\frac{C^{\prime}(0)}{C^{\prime}(0)+\frac{1}{b}C^{\prime}_{\epsilon}(0)} \frac{C(0)+\frac{1}{b}C_{\epsilon}(0)}{C(0)}\frac{\|\nabla\mathcal{L}_{b}(w)\|} {\mu-\mathcal{L}_{b}(w)}.\]

Note, that our non-parametric covariance estimation already provides us with estimates of \(C_{\epsilon}(0)\) and \(C^{\prime}_{\epsilon}(0)\), so no further adaptions are needed. The resulting asymptotic learning rate is given by

\[h_{\infty}=\frac{C(0)+\frac{1}{b}C_{\epsilon}(0)}{(C^{\prime}(0)+\frac{1}{b}C ^{\prime}_{\epsilon}(0))(\mathcal{L}_{b}(w_{\infty})-\mu)}.\] (7)

## 7 MNIST case study

For our case study we use the negative log likelihood loss to train a neural network [3, 17] on the MNIST dataset [34]. We choose this model as one of the simplest state-of-the-art models at the time of selection, consisting only of convolutional layers with ReLU activation interspersed by batch normalization layers and a single dense layers at the end with softmax activation. Assuming isotropy, we estimate \(\mu\), \(C(0)\) and \(C^{\prime}(0)\) as described in Section 6.1 and deduce the parameters \(\sigma^{2}\) and \(s\) of the respective covariance model (more details in Section B). We then use the step sizes listed in Table 1 for the'squared exponential' and 'rational quadratic' covariance in our RFD algorithm.

In Figure 3, RFD is benchmarked against step size tuned Adam [29] and stochastic gradient descent (SGD). Even with early stopping, their tuning would typically require more than 1 epoch worth of samples, _in contrast to RFD_ (Section A.1.1). We highlight that A-RFD performs significantly worse than either of the RFD versions which effectively implement some form of learning rate warmup. This is despite the RFD learning rates converging to the asymptotic one within one epoch (ca. \(30\) out of \(60\) steps per epoch). The step sizes on the other hand are (up to noise) monotonously decreasing. This stands in contrast to the "wall next to the minimum" motivation of gradient clipping.

**Code availability:** Our implementation of RFD can be found at https://github.com/FelixBenning/pyrfd and the package can also be installed from PyPI via 'pip install pyrfd'.

## 8 Limitations and extensions

To cover the vast amount of ground that lays between the 'formulation of a general average case optimization problem' and the 'prototype of a working optimizer with theoretical backing',

1. we used the common [16, 52, 55, 10]_isotropic_ and _Gaussian_ distributional assumption for \(\mathbf{J}\),
2. we used very _simple covariance models_ for the actual implementation,
3. we used WLS in our variance estimation procedure despite the _violation of independence_.

Since RFD is defined as the minimizer of an average instead of an upper bound - making it more risk affine - it naturally loses the improvement guarantee driving classical convergence proofs. It is therefore impossible to extend classical optimization proofs and new mathematical theory must be developed. This risk-affinity can also be observed in its comparatively large step sizes (cf. Fig. 3 and Sec. A). On CIFAR-100 [31], the step sizes were _too_ large and it is an open question whether assumptions were violated or whether RFD is simply too risk-affine. But since the variance of random functions vanishes asymptotic with high dimension [5] we highly suspect the former (cf. Remark E.5).

Future work will therefore have to target these assumptions. Some of the assumptions were already simplifications for the sake of exposition, and we deferred their relaxation to the appendix. The Gaussian assumption can be relaxed with a generalization to the 'BLUE' (Sec. E.3), isotropy can be generalized to 'geometric anisotropies' (Sec. E.1) and the risk-affinity of RFD can be reduced with confidence intervals (Sec. E.2). Since simple random linear models already violate stationary isotropy (Sec. F.1), we believe that stationarity is the most important assumption to attack in future work.

## 9 Conclusion

In this paper we have demonstrated the **viability** (computability and scalable complexity) and **advantages** (scale invariance, explainable step size schedule which does not require expensive tuning) of replacing the classical "convex function" framework with the "random function" framework. Along the way we bridged the gap between Bayesian optimization (not scalable so far) and classical optimization methods (scalable). This theoretical framework not only sheds light on existing step size heuristics, but can also be used to develop future heuristics.

We envision the following improvements to RFD in the future:

1. The _reliability_ of RFD can be improved by generalizing the distributional assumptions to cover more real world scenarios. In particular we are interested in the generalization to non-stationary isotropy because we suspect that regularization such as weight and batch normalization [46, 25] are used to patch violations of stationarity (cf. Section F).
2. The _performance_ of RFD can also be improved. Since RFD is forgetful while momentum methods retains some information it is likely fruitful to relax the full forgetfulness. Furthermore, we suspect that adaptive learning rates [e.g. 14, 29], such as those used by Adam, can be incorporated with geometric anisotropies (cf. Sec. E.1). Performance could also be further improved by estimating the covariance (locally) online instead of globally at the start. Finally, the implementation itself can be made more performant.

## Acknowledgement

We extend our sincere gratitude to our colleagues at the University of Mannheim, with special thanks to Rainer Gemulla and Julie Naegelen for insightful discussions and invaluable feedback. The Experiments in this work were partially carried out on the compute cluster of the state of Baden-Wurtemberg (bwHPC).

Figure 3: Training on the MNIST dataset (batch size \(1024\)). Ribbons describe the range between the \(10\%\) and \(90\%\) quantile of \(20\) repeated experiments while lines represent their mean. SE stands for the squared exponential (11) and RQ for the rational quadratic (13) covariance. The validation loss uses the test data set, which provides a small advantage to Adam and SGD, as we also use it for tuning.

## References

* [1] Robert J. Adler and Jonathan E. Taylor. _Random Fields and Geometry_. Springer Monographs in Mathematics. New York, NY: Springer New York, 2007. isbn: 978-0-387-48112-8. doi: 10.1007/978-0-387-48116-6.
* [2] Apoorv Agnihotri and Nipun Batra. "Exploring Bayesian Optimization". In: _Distill_ 5.5 (2020-05-05), e26. issn: 2476-0757. doi: 10.23915/distill.00026.
* [3] Sanghyeon An et al. _An Ensemble of Simple Convolutional Neural Network Models for MNIST Digit Recognition_. 2020-10-04. doi: 10.48550/arXiv.2008.10400. Pre-published.
* [4] Antonio Auffinger and Qiang Zeng. "Complexity of Gaussian Random Fields with Isotropic Increments". In: _Communications in Mathematical Physics_ 402.1 (2023-08-01), pp. 951-993. issn: 1432-0916. doi: 10.1007/s00220-023-04739-0.
* [5] Felix Benning and Leif Doring. _Gradient Span Algorithms Make Predictable Progress in High Dimension_. 2024-10-13. doi: 10.48550/arXiv.2410.09973. Pre-published.
* [6] Karl Heinz Borgwardt. _The Simplex Method: A Probabilistic Analysis_. Softcover reprint of the original 1st ed. 1987 edition. Berlin Heidelberg: Springer, 1986-11-01. 282 pp. isbn: 978-3-540-17096-9.
* [7] Sebastien Bubeck and Mark Sellke. "A Universal Law of Robustness via Isoperimetry". In: _Advances in Neural Information Processing Systems_. Vol. 34. Virtual Event: Curran Associates, Inc., 2021, pp. 28811-28822. arXiv: 2105. 12806 [cs, stat]. url: https: // proceedings. neurips. cc / paper / 2021 / hash / f197002b9a0853eca5e046d9ca4663d5-Abstract.html (visited on 2023-09-22).
* [8] Youngmin Cho and Lawrence Saul. "Kernel Methods for Deep Learning". In: _Advances in Neural Information Processing Systems_. Vol. 22. Curran Associates, Inc., 2009. url: https: // proceedings. neurips. cc / paper / 2009 / hash / 5751ec3e9a4feab575962e78e006250d-Abstract.html (visited on 2023-04-03).
* [9] Leonardo Cunha et al. "Only Tails Matter: Average-Case Universality and Robustness in the Convex Regime". In: _Proceedings of the 39th International Conference on Machine Learning_. PMLR, 2022-06-28, pp. 4474-4491. url: https://proceedings.mlr.press/v162/cunha22a.html (visited on 2023-11-09).
* [10] Yann N Dauphin et al. "Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization". In: _Advances in Neural Information Processing Systems_. Vol. 27. Montreal, Canada: Curran Associates, Inc., 2014. url: https://proceedings.neurips.cc/paper/2014/hash/17e23e50bedc63b4095e3d8204ce063b-Abstract.html (visited on 2022-06-10).
* [11] Aaron Defazio and Konstantin Mishchenko. "Learning-Rate-Free Learning by D-Adaptation". In: _Proceedings of the 40th International Conference on Machine Learning_. International Conference on Machine Learning. PMLR, 2023-07-03, pp. 7449-7479. arXiv: 2301.07733 [cs.LG]. url: https://proceedings.mlr.press/v202/defazio23a.html (visited on 2024-03-31).
* [12] Percy Deift and Thomas Trogdon. "The Conjugate Gradient Algorithm on Well-Conditioned Wishart Matrices Is Almost Deterministic". In: _Quarterly of Applied Mathematics_ 79.1 (2021-03), pp. 125-161. issn: 0033-569X, 1552-4485. doi: 10.1090/qam/1574. arXiv: 1901.09007 [cs, math].
* [13] P. Deuflhard and G. Heindl. "Affine Invariant Convergence Theorems for Newton's Method and Extensions to Related Methods". In: _SIAM Journal on Numerical Analysis_ 16.1 (1979-02), pp. 1-10. issn: 0036-1429. doi: 10.1137/0716001.
* [14] John Duchi, Elad Hazan, and Yoram Singer. "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization". In: _The Journal of Machine Learning Research_ 12 (2011-07-01), pp. 2121-2159. issn: 1532-4435.
* [15] Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. "Optimization of Mean-Field Spin Glasses". In: _The Annals of Probability_ 49.6 (2021-11), pp. 2922-2960. doi: 10.1214/21-AOP1519.
* [16] Peter I. Frazier. "Bayesian Optimization". In: _Recent Advances in Optimization and Modeling of Contemporary Problems_. INFORMS TutORials in Operations Research. Phoenix, Arizona, USA: INFORMS, 2018-10, pp. 255-278. isbn: 978-0-9906153-2-3. doi: 10.1287/educ.2018.0188. arXiv: 1807.02811 [cs, math, stat].

* [17] Fuchang Gao and Lixing Han. "Implementing the Nelder-Mead Simplex Algorithm with Adaptive Parameters". In: _Computational Optimization and Applications_ 51.1 (2012-01-01), pp. 259-277. issn: 1573-2894. doi: 10.1007/s10589-010-9329-3.
* [18] Xavier Glorot and Yoshua Bengio. "Understanding the Difficulty of Training Deep Feed-forward Neural Networks". In: _Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics_. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. Sardinia, Italy: JMLR Workshop and Conference Proceedings, 2010-03-31, pp. 249-256. url: https://proceedings.mlr.press/v9/glorot10a.html (visited on 2023-04-11).
* [19] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, 2016-11-10. 801 pp. isbn: 978-0-262-33737-3. Google Books: omivDQAQBAJ.
* [20] Priya Goyal et al. _Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour_. arXiv, 2018-04-30. arXiv: 1706.02677 [cs]. url: http://arxiv.org/abs/1706.02677 (visited on 2024-04-02).
* [21] Anders Hansson. _Optimization for Learning and Control_. Hoboken, New Jersey: John Wiley & Sons, Inc., 2023. isbn: 978-1-119-80914-2.
* [22] Geoffrey Hinton. "Neural Networks for Machine Learning". Massive Open Online Course (Coursera). 2012. url: https://www.cs.toronto.edu/~hinton/coursera_lectures.html (visited on 2021-11-16).
* [23] C. A. R. Hoare. "Quicksort". In: _The Computer Journal_ 5.1 (1962-01-01), pp. 10-16. issn: 0010-4620. doi: 10.1093/comjnl/5.1.10.
* [24] Brice Huang and Mark Sellke. "Tight Lipschitz Hardness for Optimizing Mean Field Spin Glasses". In: _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_. 2022-10, 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS). 2022-10, pp. 312-322. doi: 10.1109/FOCS54457.2022.00037.
* [25] Sergey Ioffe and Christian Szegedy. "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift". In: _Proceedings of the 32nd International Conference on Machine Learning_. International Conference on Machine Learning. PMLR, 2015-06-01, pp. 448-456. arXiv: 1502.03167 [cs]. url: https://proceedings.mlr.press/v37/ioffe15.html (visited on 2021-10-06).
* [26] E. T. Jaynes. "Information Theory and Statistical Mechanics". In: _Physical Review_ 106.4 (1957-05-15), pp. 620-630. doi: 10.1103/PhysRev.106.620.
* [27] Richard Arnold Johnson and Dean W. Wichern. _Applied Multivariate Statistical Analysis_. 6th ed. Upper Saddle River, N.J: Pearson College Div, 2007. 767 pp. isbn: 978-0-13-187715-3.
* [28] Steven M. Kay. _Fundamentals of Statistical Signal Processing: Estimation Theory_. USA: Prentice-Hall, Inc., 1993-02. 595 pp. isbn: 978-0-13-345711-7.
* [29] Diederik P. Kingma and Jimmy Ba. "Adam: A Method for Stochastic Optimization". In: _Proceedings of the 3rd International Conference on Learning Representations_. ICLR. San Diego, 2015. arXiv: 1412.6980.
* [30] Achim Klenke. _Probability Theory: A Comprehensive Course_. Universitext. London: Springer, 2014. isbn: 978-1-4471-5360-3 978-1-4471-5361-0. doi: 10.1007/978-1-4471-5361-0.
* [31] Alex Krizhevsky. _Learning Multiple Layers of Features from Tiny Images_. 2009. url: https://www.cs.toronto.edu/%20kriz/learning-features-2009-TR.pdf (visited on 2024-05-21).
* [32] H. J. Kushner. "A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise". In: _Journal of Basic Engineering_ 86.1 (1964-03-01), pp. 97-106. issn: 0021-9223. doi: 10.1115/1.3653121.
* [33] Jonathan Lacotte and Mert Pilanci. "Optimal Randomized First-Order Methods for Least-Squares Problems". In: _Proceedings of the 37th International Conference on Machine Learning_. International Conference on Machine Learning. PMLR, 2020-11-21, pp. 5587-5597. url: https://proceedings.mlr.press/v119/lacotte20a.html (visited on 2023-11-09).
* [34] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. _THE MNIST DATABASE of Handwritten Digits_. 2010. url: http://yann.lecun.com/exdb/mnist/ (visited on 2024-01-24).

* [35] Daniel James Lizotte. "Practical Bayesian Optimization". PhD thesis. Alberta, Canada: University of Alberta, 2008. 168 pp.
* [36] G. Matheron. "The Intrinsic Random Functions and Their Applications". In: _Advances in Applied Probability_ 5.3 (1973-12), pp. 439-468. issn: 0001-8678, 1475-6064. doi: 10.2307/1425829.
* [37] Andrea Montanari. "Optimization of the Sherrington-Kirkpatrick Hamiltonian". In: _SIAM Journal on Computing_ (2021-01-07), FOCS19-1. issn: 0097-5397. doi: 10.1137/20M132016X.
* [38] Yurii Evgen'evic Nesterov. _Lectures on Convex Optimization_. Second edition. Springer Optimization and Its Applications; Volume 137. Cham: Springer, 2018. isbn: 978-3-319-91578-4. doi: 10.1007/978-3-319-91578-4.
* [39] Misha Padidar et al. "Scaling Gaussian Processes with Derivative Information Using Variational Inference". In: _Advances in Neural Information Processing Systems_. Vol. 34. Curran Associates, Inc., 2021, pp. 6442-6453. URL: https://proceedings.neurips.cc/paper/2021/hash/32bbf7b2bc4ed14eb1e9c2580056a989-Abstract.html (visited on 2023-05-17).
* [40] Courtney Paquette et al. "Halting Time Is Predictable for Large Models: A Universality Property and Average-Case Analysis". In: _Foundations of Computational Mathematics_ 23.2 (2022-02), pp. 597-673. issn: 1615-3383. doi: 10.1007/s10208-022-09554-y. arXiv: 2006.04299 [math, stat].
* [41] Elliot Paquette and Thomas Trogdon. "Universality for the Conjugate Gradient and MINRES Algorithms on Sample Covariance Matrices". In: _Communications on Pure and Applied Mathematics_ 76.5 (2022-09-01), pp. 1085-1136. issn: 1097-0312. doi: 10.1002/cpa.22081.
* [42] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. "On the Difficulty of Training Recurrent Neural Networks". In: _Proceedings of the 30th International Conference on Machine Learning_. International Conference on Machine Learning. Atlanta: PMLR, 2013-05-26, pp. 1310-1318. url: https://proceedings.mlr.press/v28/pascanu13.html (visited on 2024-04-02).
* [43] Fabian Pedregosa and Damien Scieur. "Acceleration through Spectral Density Estimation". In: _Proceedings of the 37th International Conference on Machine Learning_. International Conference on Machine Learning. Virtual Event (formerly Vienna): PMLR, 2020-11-21, pp. 7553-7562. url: https://proceedings.mlr.press/v119/pedregosa20a.html (visited on 2023-11-09).
* [44] Carl Edward Rasmussen and Christopher K.I. Williams. _Gaussian Processes for Machine Learning_. 2nd ed. Adaptive Computation and Machine Learning 3. Cambridge, Massachusetts: MIT Press, 2006. 248 pp. isbn: 0-262-18253-X. url: http://gaussianprocess.org/gpml/chapters/RW.pdf (visited on 2023-04-06).
* [45] Filip de Roos, Alexandra Gessner, and Philipp Hennig. "High-Dimensional Gaussian Process Inference with Derivatives". In: _Proceedings of the 38th International Conference on Machine Learning_. International Conference on Machine Learning. PMLR, 2021-07-01, pp. 2535-2545. url: https://proceedings.mlr.press/v139/de-roos21a.html (visited on 2023-05-15).
* [46] Tim Salimans and Durk P Kingma. "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks". In: _Advances in Neural Information Processing Systems_. Vol. 29. Barcelona, Spain: Curran Associates, Inc., 2016. url: https: //proceedings.neurips.cc/paper/2016/hash/ed265bc903a5a097f61d3ec06d496d2e-Abstract.html (visited on 2023-10-16).
* [47] Bobak Shahriari et al. "Taking the Human Out of the Loop: A Review of Bayesian Optimization". In: _Proceedings of the IEEE_ 104.1 (2016-01), pp. 148-175. issn: 1558-2256. doi: 10.1109/JPROC.2015.2494218.
* Learning Rate, Batch Size, Momentum, and Weight Decay_. 2018-04-24. doi: 10.48550/arXiv.1803.09820. arXiv: 1803.09820 [cs, stat]. Pre-published.
* [49] Leslie N. Smith. "Cyclical Learning Rates for Training Neural Networks". In: _2017 IEEE Winter Conference on Applications of Computer Vision (WACV)_. 2017 IEEE Winter Conference on Applications of Computer Vision (WACV). 2017-03, pp. 464-472. doi: 10.1109/WACV.2017.58.

* [50] Michael L. Stein. _Interpolation of Spatial Data_. Springer Series in Statistics. New York, NY: Springer, 1999. ISBN: 978-1-4612-7166-6 978-1-4612-1494-6. doi: 10.1007/978-1-4612-1494-6.
* [51] Eliran Subag. "Following the Ground States of Full-RSB Spherical Spin Glasses". In: _Communications on Pure and Applied Mathematics_ 74.5 (2021), pp. 1021-1044. ISSN: 1097-0312. doi: 10.1002/cpa.21922.
* [52] Ziyu Wang et al. "Bayesian Optimization in a Billion Dimensions via Random Embeddings". In: _Journal of Artificial Intelligence Research_ 55 (2016-02-19), pp. 361-387. ISSN: 1076-9757. doi: 10.1613/jair.4806.
* [53] C.K.I. Williams and D. Barber. "Bayesian Classification with Gaussian Processes". In: _IEEE Transactions on Pattern Analysis and Machine Intelligence_ 20.12 (1998-12), pp. 1342-1351. ISSN: 1939-3539. doi: 10.1109/34.735807.
* [54] Christopher K. I. Williams. "Computation with Infinite Neural Networks". In: _Neural Computation_ 10.5 (1998-07-01), pp. 1203-1216. ISSN: 0899-7667. doi: 10.1162 / 089976698300017412.
* [55] Jian Wu et al. "Bayesian Optimization with Gradients". In: _Advances in Neural Information Processing Systems_. Vol. 30. Curran Associates, Inc., 2017. url: https://proceedings.neurips.cc/paper/2017/hash/64a08e5fe1e6c39faeb90108c430eb120-Abstract.html (visited on 2022-06-02).
* [56] Han Xiao, Kashif Rasul, and Roland Vollgraf. _Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms_. 2017-09-15. doi: 10.48550/arXiv.1708.07747. arXiv: 1708.07747 [cs, stat]. Pre-published.
* [57] Matthew D. Zeiler. "ADADELTA: An Adaptive Learning Rate Method". 2012-12-22. arXiv: 1212.5701 [cs].
* [58] Guodong Zhang et al. "Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model". In: _Advances in Neural Information Processing Systems_. Vol. 32. Curran Associates, Inc., 2019. arXiv: 1907. 04164 [cs, stat]. URL: https ://proceedings.neurips.cc / paper / 2019 / hash / e0eacd983971634327ae1819ea8b6214-Abstract.html (visited on 2023-11-09).

## Appendix A Experiments

### Covariance estimation

In Figure 4 we visualize weighted least squares (WLS) regression of the covariance estimation from Section 6.1. Note, that we sampled much more samples per batch size for these plots than RFD would typically require by itself in order to be able to plot batch-wise means and batch-wise QQ-plots. The batch size distribution we described in Section B.1 would avoid sampling the same batch size multiple times to ensure better stability of the regression and generally requires much fewer samples than were used for this visualization (cf. A.1.1)

We can observe from the QQ-plots on the right, that the Gaussian assumption is essentially justified for the losses, resulting in a \(\chi^{2}(1)\) distribution for the squared losses and a \(\chi^{2}(d)\) distribution for the gradient norms squared. The confidence interval estimate for the squared norms appears to be much too small (it is plotted, but too small to be visible). Perhaps this signifies a violation of the isotropy

Figure 4: Visualization of the variance estimation (Section 6.1) with \(95\%\)-confidence intervals based on the assumed distribution. Quantile-quantile (QQ) plots of the losses (against a normal distribution), squared losses (against a \(\chi^{2}(1)\) distribution) and squared gradient norms (against a \(\chi^{2}(d)\)-distribution) are displayed on the right for a selection of batch sizes.

assumption as the variance of

\[\|\nabla\mathcal{L}_{b}(w)\|^{2}=\sum_{i=1}^{d}(\partial_{i}\mathcal{L}_{b}(w))^{2}\]

does not appear to be the variance of independent \(\chi^{2}(d)\) Gaussian random variables, and the independence only follows from the isotropy assumption.

#### a.1.1 Sampling efficiency and stability

To evaluate the sampling efficiency and stability of our variance estimation process, we repeated the covariance estimation of the model model M7 [3] applied to the MNIST dataset \(20\) times (Figure 5). We used a tolerance of \(\mathrm{tol}=0.3\) as a stopping criterion for the estimated relative standard deviation (10).

At this tolerance, the asymptotic learning rate already seems relatively stable (in the same order of magnitude) and the sample cost is quite cheap. The majority of runs (\(16/20\) runs or \(80\%\)) required less than \(60\,000\) samples (1 epoch). There was one large outlier which used \(500\,589\) samples. A closer inspection revealed, that after the initial sample to estimate the optimal batch size distribution, it sampled almost exclusively at batch sizes \(20\) (which was the minimal cutoff to avoid instabilities caused by batch normalization) and batch sizes between \(1700\) and \(1900\). It therefore seems like the initial batch of samples caused a very unfavorable batch size distribution which then required a lot of samples to recover from. Our selection of an initial sample size of \(6000\) might therefore have been too small.

A more extensive empirical study is needed to tune this estimation process, but the process promises to be very sample efficient. Classical step size tuning would train models for a short duration in order to evaluate the performance of a particular learning rate (e.g. 48), but a single epoch worth of samples is very hard to beat.

Our implementation of this process on the other hand is very inefficient as of writing. Piping data of differing batch sizes into a model is not a standard use case. We implement this by repeatedly initializing data loaders, which is anything but performance friendly.

### Other models and datasets

To estimate the effect of the batch size on RFD, we trained the same model (M7 [3]) on MNIST with batch size \(128\) (Figure 6). We can see that the asymptotic learning rate of S-RFD is reduced at a smaller batch size (cf. Equation 7) but the performance is barely different. Overall, RFD seems to be slightly too risk-affine, selecting larger step sizes than the tuned SGD models.

We also trained a different model (M5 [3]) on the Fashion MNIST dataset [56] with batch size \(128\) (Figure 7). Since the validation loss increases after epoch \(5\), early stopping would have been

Figure 5: \(20\) repeated covariance estimations of model M7 [3] applied to the MNIST dataset. On the left are the resulting asymptotic learning rates (assuming a final loss of zero) and on the right are the samples used until the stopping criterion interrupted sampling.

appropriate. We therefore include Adam with learning rate \(10^{-3}\), despite Adam with learning rate \(10^{-4}\) technically performing better at the end of training. We can generally see, that RFD comes very close to tuned performance at the time early stopping would have been appropriate. Again, learning rates seem to be slightly too large (risk-affine) in comparison to tuned SGD.

## Appendix B Variance estimation in detail

Recall that we are interested in the regression

\[Z_{b}(w)=(\mathcal{L}_{b}(w)-\mu)^{2}\sim\beta_{0}+\frac{1}{b}\beta_{1}\]

where the variance of \(Z_{b}\) is given by

\[\sigma_{b}^{2}=2(\beta_{0}+\tfrac{1}{b}\beta_{1})^{2}.\]

under the Gaussian assumption on \(\mathcal{L}_{b}\).

More specifically we for minibatch sizes \((b_{k})_{k\leq n}\) and parameter vectors \((w_{k})_{k\leq n}\) we want to sample mini batch losses

\[\mathcal{L}^{(k)}:=\mathcal{L}_{b_{k}}(w_{k})=\mathbf{J}(w_{k})+\frac{1}{b_{k }}\sum_{i=1}^{b_{k}}\epsilon_{k,i}(w_{k})\]

As the \(\epsilon_{k,i}\) are all conditionally independent and therefore uncorrelated, we have

\[\mathrm{Cov}(\mathcal{L}^{(k)},\mathcal{L}^{(l)})=\mathrm{Cov}(\mathbf{J}(w_{ k}),\mathbf{J}(w_{l}))=C\big{(}\tfrac{\|w_{k}-w_{l}\|^{2}}{2}\big{)}\]

Since the covariance kernel \(C\) is typically monotonously falling in the distance of parameters \(\|w_{k}-w_{l}\|^{2}\), we want to select them as spaced out as possible to minimize the covariance of \(\mathcal{L}^{(k)}\) (which is the next best thing to iid samples). Randomly selecting \(w_{i}\) with Glorot initialization [18] will ensure a good spread.

Note that Glorot initialization places all parameters approximately on the same sphere. This is because Glorot initialization initializes all parameters independently, therefore their norm is the

Figure 6: Training model M7 [3] with batch size \(128\) on MNIST [34].

sum of independent squares, which converges by the law of large numbers due to the normalization Glorot uses. Since stationary isotropy and non-stationary isotropy coincides on the sphere, this is an important effect to consider (cf. Section F).

What is left, is the selection of the batch sizes \(b_{k}\).

### Batch size distribution

Since we plan to use the data set \((\frac{1}{b_{k}},\mathcal{L}^{(k)})_{k\leq n}\) for weighted least squares (WLS) regression and do not have a selection process for the batch sizes \(b_{k}\) yet, it might be appropriate to select the batch sizes \(b_{k}\) in such a way, that the variance of our estimator \(\hat{\beta}_{0}\) of \(\beta_{0}\) is minimized. Here we choose \(\mathrm{Var}(\hat{\beta}_{0})\) and not \(\mathrm{Var}(\hat{\beta}_{1})\) as our optimization target, since \(\beta_{0}=C(0)\) is used to fit the covariance model, while \(\beta_{1}=C_{\epsilon}(0)\) is only required for S-RFD. Without deeper analysis \(\beta_{0}\) therefore seems to be more important.

Optimization over \(n\) parameters \(b_{k}\) is quite difficult, but we can simplify this optimization problem by considering the empirical batch size distribution

\[\nu_{n}=\frac{1}{n}\sum_{k=1}^{n}\delta_{b_{k}}.\]

Using a random variable \(B\) distributed according to \(\nu_{n}\), the total number of sample losses can then be expressed as

\[\sum_{k=1}^{n}b_{k}=n\mathbb{E}[B]=\text{samples used}.\]

Under an (unrealistic) independence assumption, the variance \(\mathrm{Var}(\hat{\beta}_{0})\) also has a simple representation in terms of \(\nu_{n}\) (Lemma B.2). We now want to minimize this variance subject to compute

Figure 7: Model M5 [3] trained on Fashion MNIST [56] with batch size \(128\).

constraint \(\alpha\) limiting the number of sample losses we can use resulting in the optimization problem

\[\mathrm{Var}(\hat{\beta_{0}})=\frac{1}{n}\underbrace{\frac{1}{\mathbb{E}[\frac{1 }{\sigma_{B}^{2}}]}}_{\text{\text{\textless variance of $Z_{B}$}}},\underbrace{\mathbb{E}\Big{[}\frac{1}{\sigma_{B}^{2}} \big{(}\frac{1}{B}-\mathbb{E}[\frac{1}{B\sigma_{B}^{2}\mathbb{E}[1/\sigma_{B}^ {2}]}\big{)}^{2}\Big{]}}_{\text{\textless inverse of the ``spread'' of $\frac{1}{\lambda}$}}\] s.t. \[n\mathbb{E}[B]\leq\alpha.\] (8)

where we recall that \(\sigma_{B}^{2}\) is the variance of \(Z_{B}\). So the inverse of the expectation of its inverse is roughly the average variance of \(Z_{B}\). The second half is the fraction of a weighted second moment divided by the weighted variance. Unless the mean is at zero, the former will be larger. In particular we want a spread of data otherwise the variance would be zero. This is in some conflict with the variance of \(Z_{B}\).

But first, let us get rid of \(n\). Note that we would always increase \(n\) until our compute budget is used up, since this always reduces variance. So we approximately have \(n\mathbb{E}[B]=\alpha\). Thus

\[\mathrm{Var}(\hat{\beta_{0}})=\frac{\mathbb{E}[B]}{\alpha}\frac{1}{\mathbb{E }[\frac{1}{\sigma_{B}^{2}}]}\frac{\mathbb{E}[\frac{1}{\sigma_{B}^{2}}]}{ \mathbb{E}\Big{[}\frac{1}{\sigma_{B}^{2}}\big{(}\frac{1}{B}-\mathbb{E}[\frac {1}{B\sigma_{B}^{2}\mathbb{E}[1/\sigma_{B}^{2}]}\big{)}^{2}\Big{]}}\]

Since \(\alpha\) is now just resulting in a constant factor, it can be assumed to be \(1\) without loss of generality. Over batch size distributions \(\nu\) we therefore want to solve the minimization problem

\[\min_{\nu}\underbrace{\frac{\mathbb{E}[B]}{\mathbb{E}[\frac{1}{\sigma_{B}^{2} }]}}_{\text{\textless moments}}\underbrace{\mathbb{E}[\frac{1}{\sigma_{B}^{2}} \big{(}\frac{1}{B}-\mathbb{E}[\frac{1}{B\sigma_{B}^{2}\mathbb{E}[1/\sigma_{B} ^{2}]}]\big{)}^{2}\Big{]}}_{\text{\textless spread}}\] (9)

**Example B.1** (If we did not require spread).: If we were not concerned with the variance of batch sizes, we could select a constant \(B=b\). Then it is straightforward to minimize the moments factor manually

\[\min_{b}\frac{\mathbb{E}[b]}{\mathbb{E}[\frac{1}{\sigma_{b}^{2}}]}=b\sigma_{ b}^{2}=2b(\beta_{0}+\frac{1}{b}\beta_{1})^{2},\]

resulting in \(\frac{1}{b}=\frac{\beta_{0}}{\beta_{1}}\). In other words: If we did not have to be concerned with the spread of \(B\) there is one optimal selection to minimize the first factor. But in reality we have to trade-off this target with the spread of \(B\).

To ensure a good spread of data, we use the maximum entropy distribution for \(B\), with the moment constraints

\[\mathbb{E}[-B]\geq-\frac{\alpha}{n}\] average sample usage \[\mathbb{E}[\frac{1}{\sigma_{B}^{2}}]\geq\theta Z_{B}\text{ variance}\]

which capture the first factor. Maximizing entropy under moment constraints is known [26] to result in the Boltzmann (a.k.a. Gibbs) distribution

\[\nu(b)=\mathbb{P}(B=b)\propto\exp\Bigl{(}\lambda_{1}\frac{1}{\sigma_{b}^{2}}- \lambda_{2}b\Bigr{)},\]

where \(\lambda_{1},\lambda_{2}\) depend on the momentum constraints. We can now forget the origin of this distribution and use \(\lambda_{1},\lambda_{2}\) as parameters for the distribution \(\nu\) in Equation (9) to get close to its minimum. In practice we use a zero order black box optimizer (Nelder-Mead [17]). One could calculate the expectations of (9) under this distribution explicitly and take manual derivatives with respect to \(\lambda_{i}\) to investigate this further, but we wanted to avoid getting too distracted by this tangent.

We also use the estimated relative standard deviation

\[\mathrm{rel\_std}=\frac{\sqrt{\mathrm{Var}(\hat{\beta}_{0})}}{\hat{\beta}_{0}}\] (10)

as a stopping criterion for sampling. Without extensive testing we found a tolerance of \(\mathrm{rel\_std}<\mathrm{tol}=0.3\) to be reasonable, cf. Section A.1.1.

**Lemma B.2** (Variance of \(\hat{\beta}_{0}\) in terms of the empirical batch size distribution).: _Assuming independence of the samples \(((\frac{1}{b_{k}}),Z_{b_{k}})_{k\leq n}\), the variance of \(\hat{\beta}_{0}\) is given by_

\[\mathrm{Var}(\hat{\beta}_{0})=\frac{1}{n}\frac{1}{\mathbb{E}[\frac{1}{\sigma_{B }^{2}}]}\frac{\mathbb{E}[\frac{1}{\sigma_{B}^{2}B^{2}}]}{\mathbb{E}\Big{[} \frac{1}{\sigma_{B}^{2}}\big{(}\frac{1}{B}-\mathbb{E}[\frac{1}{B\sigma_{B}^{2 }\mathbb{E}[1/\sigma_{B}^{2}]}]\big{)}^{2}\Big{]}}\]

_where \(B\) is distributed according to the empirical batch size distribution \(\nu_{n}=\frac{1}{n}\sum_{k=1}^{n}\delta_{b_{k}}\)._

Proof.: With the notation \(\sigma_{k}^{2}=\sigma_{b_{k}}^{2}\) to describe the variance of \(Z_{b_{k}}\) it follows from (cf. 28, Thm. 4.2) that the variance of the estimator \(\hat{\beta}\) of \(\hat{\beta}\) using \(n\) samples is given by

\[\mathrm{Var}(\hat{\beta}) =(H^{T}C^{-1}H)^{-1}\] \[=\frac{1}{\big{(}\sum_{k}\frac{1}{\sigma_{k}^{2}}\big{)}\big{(} \sum_{k}\frac{1}{(\sigma_{k}b_{k})^{2}}\big{)}-\big{(}\sum_{k}\frac{1}{\sigma _{b}^{2}b_{k}}\big{)}^{2}}\begin{pmatrix}\sum_{k}\frac{1}{(\sigma_{k}b_{k})^{2 }}&-\sum_{k}\frac{1}{\sigma_{b}^{2}b_{k}}\\ -\sum_{k}\frac{1}{\sigma_{b}^{2}b_{k}}&\sum_{k}\frac{1}{\sigma_{k}^{2}}\end{pmatrix}\]

where

\[C:=\begin{pmatrix}\sigma_{1}^{2}&&\\ &\ddots&\\ &&\sigma_{n}^{2}\end{pmatrix}\qquad H:=\begin{pmatrix}1&\frac{1}{b_{1}}\\ \vdots&\\ 1&\frac{1}{b_{n}}\end{pmatrix}.\]

In particular we have

\[\mathrm{Var}(\hat{\beta}_{0})=\frac{\sum_{k}\frac{1}{\sigma_{k}^{2}b_{k}^{2}} }{\big{(}\sum_{k}\frac{1}{\sigma_{k}^{2}}\big{)}\big{(}\sum_{k}\frac{1}{ \sigma_{k}^{2}b_{k}^{2}}\big{)}-(\sum_{k}\frac{1}{\sigma_{k}^{2}b_{k}})^{2}}.\]

With the help of \(\theta:=\sum_{j}\frac{1}{\sigma_{j}^{2}}\) and \(\lambda_{k}:=\frac{1}{\sigma_{k}^{2}\theta}\), we can reorder the divisor. For this note that since the \(\lambda_{k}\) sum to \(1\) we have

\[\sum_{k}\lambda_{k}\Big{(}\frac{1}{b_{k}}-\sum_{j}\lambda_{j} \frac{1}{b_{j}}\Big{)}^{2} =\sum_{k}\lambda_{k}\Big{(}\frac{1}{b_{k}^{2}}-2\frac{1}{b_{k}} \sum_{j}\lambda_{j}\frac{1}{b_{j}}+\Big{(}\sum_{j}\lambda_{j}\frac{1}{b_{j}} \Big{)}^{2}\Big{)}\] \[=\sum_{k}\lambda_{k}\frac{1}{b_{k}^{2}}-2\Big{(}\sum_{k}\lambda_{ k}\frac{1}{b_{k}}\Big{)}+\Big{(}\sum_{k}\lambda_{k}\frac{1}{b_{j}}\Big{)}^{2}\] \[=\sum_{k}\lambda_{k}\frac{1}{b_{k}^{2}}-\Big{(}\sum_{k}\lambda_{ k}\frac{1}{b_{k}}\Big{)}^{2}\]

Where the above is essentially the well known statement \(\mathbb{E}[(Y-\mathbb{E}[Y])^{2}]=\mathbb{E}[Y^{2}]-\mathbb{E}[Y]^{2}\) for an appropriate selection of \(Y\). This implies that our divisor is given by a weighted variance

\[\theta^{2}\sum_{k}\lambda_{k}\Big{(}\frac{1}{b_{k}}-\sum_{j}\lambda_{j}\frac{1 }{b_{j}}\Big{)}^{2}=\theta\sum_{k}\frac{1}{\sigma_{k}^{2}b_{k}^{2}}-\Big{(} \sum_{k}\frac{1}{\sigma_{k}^{2}b_{k}}\Big{)}^{2},\]

where it is only necessary to plug in the definition of \(\theta\) to see the right term is exactly our divisor. Expanding both the enumerator as well as the divisor by \(\frac{1}{n}\), we obtain

\[\mathrm{Var}(\hat{\beta}_{0})=\frac{1}{\theta}\frac{\frac{1}{n}\sum_{k}\frac{1} {\sigma_{k}^{2}b_{k}^{2}}}{\frac{1}{n}\sum_{k}\frac{1}{\sigma_{k}^{2}}\big{(} \frac{1}{b_{k}}-\sum_{j}\lambda_{j}\frac{1}{b_{j}}\big{)}^{2}}\]

Since \(\theta=n\mathbb{E}[1/\sigma_{B}^{2}]\) for \(B\sim\frac{1}{n}\sum_{k=1}^{n}\delta_{b_{k}}\) and \(\lambda_{k}=\frac{1}{n\sigma_{k}^{2}\mathbb{E}[1/\sigma_{B}^{2}]}\), the above can thus be written as

\[\mathrm{Var}(\hat{\beta}_{0})=\frac{1}{n}\frac{1}{\mathbb{E}[1/\sigma_{B}^{2}] }\frac{\mathbb{E}[\frac{1}{\sigma_{B}^{2}B^{2}}]}{\mathbb{E}\Big{[}\frac{1}{ \sigma_{B}^{2}}\big{(}\frac{1}{B}-\mathbb{E}[\frac{1}{B\sigma_{B}^{2}\mathbb{E}[ 1/\sigma_{B}^{2}]}]\big{)}^{2}\Big{]}},\]

which proves our claim.

Covariance models

In this section we calculate the step sizes of the covariance models listed in Table 1 and plotted in Figure 2. Additionally we calculate the asymptotic learning rate of A-RFD and prove an Assumption of Corollary 5.3 for the squared exponential covariance (Prop. C.3).

### Squared exponential

The squared exponential covariance function is given by

\[C\big{(}\tfrac{\|x-y\|^{2}}{2}\big{)}=\sigma^{2}\exp\bigl{(}-\tfrac{\|x-y\|^{2} }{2s^{2}}\bigr{)}.\] (11)

Note that \(\sigma^{2}\) will play no role in the step sizes of RFD due to its scale invariance (cf. Advantage 2.3).

**Theorem C.1**.: _Let \(\mathbf{J}\sim\mathcal{N}(\mu,C)\) where \(C\) is the **squared exponential** covariance function (11), then we have_

\[\eta^{*}\frac{\nabla\mathbf{J}(w)}{\|\nabla\mathbf{J}(w)\|}=\operatorname*{ argmin}_{\mathbf{d}}\mathbb{E}[\mathbf{J}(w-\mathbf{d})\mid\mathbf{J}(w), \nabla\mathbf{J}(w)]\]

_with RFD step size_

\[\eta^{*}=\frac{s^{2}\|\nabla\mathbf{J}(w)\|}{\sqrt{\bigl{(}\tfrac{\mu-\mathbf{ J}(w)}{2}\bigr{)}^{2}+s^{2}\|\nabla\mathbf{J}(w)\|^{2}+\tfrac{\mu-\mathbf{J}(w)} {2}}}.\]

Proof.: The covariance function \(C\) is of the form

\[C(h)=\sigma^{2}e^{-\frac{h}{s^{2}}}.\]

By Equation (2)

\[\eta^{*}=-\operatorname*{argmin}_{\eta}\frac{C(\frac{\eta^{2}}{2})}{C(0)}- \eta\frac{C^{\prime}(\frac{\eta^{2}}{2})}{C^{\prime}(0)}\Theta.\]

where \(\Theta=\frac{\|\nabla\mathbf{J}(w)\|}{\mu-\mathbf{J}(w)}\). We calculate

\[-\frac{C\big{(}\frac{\eta^{2}}{2}\big{)}}{C(0)}-\eta\frac{C^{\prime}\big{(} \frac{\eta^{2}}{2}\big{)}}{C^{\prime}(0)}\Theta=-e^{-\frac{\eta^{2}}{2s^{2}} }(1+\eta\Theta).\]

This results in the first order condition

\[0\stackrel{{!}}{{=}}\frac{\eta}{s^{2}}e^{-\frac{\eta^{2}}{2s^{2} }}(1+\eta\Theta)-e^{-\frac{\eta^{2}}{2s^{2}}}\Theta=\frac{e^{-\frac{\eta^{2}}{ 2s^{2}}}}{s^{2}}(\eta^{2}\Theta+\eta-s^{2}\Theta).\]

Since the exponential can never be zero, we have to solve a quadratic equation. Its solution results in

\[\eta^{*}(\Theta)=\sqrt{\left(\tfrac{1}{26}\right)^{2}+s^{2}}-\tfrac{1}{26}.\] (12)

At this point we could stop, but the result is numerically unstable as it suffers from catastrophic cancellation. To solve this issue we set \(x=\frac{1}{26}\) and reorder

\[\eta^{*}=\sqrt{x^{2}+s^{2}}-x=(\sqrt{x^{2}+s^{2}}-x)\frac{\sqrt{x^{2}+s^{2}}+ x}{\sqrt{x^{2}+s^{2}}+x}=\frac{\varkappa+s^{2}-\varkappa^{2}}{\sqrt{x^{2}+s^{2} }+x}.\]

Re-substituting \(x=\frac{1}{26}=\frac{\mu-\mathbf{J}(w)}{2\|\nabla\mathbf{J}(w)\|}\), we finally get

\[\eta^{*}=\frac{s^{2}}{\sqrt{\left(\tfrac{\mu-\mathbf{J}(w)}{2\|\nabla\mathbf{ J}(w)\|}\right)^{2}+s^{2}}+\tfrac{\mu-\mathbf{J}(w)}{2\|\nabla\mathbf{J}(w)\|}}= \frac{s^{2}\|\nabla\mathbf{J}(w)\|}{\sqrt{\left(\tfrac{\mu-\mathbf{J}(w)}{2} \right)^{2}+s^{2}\|\nabla\mathbf{J}(w)\|^{2}+\tfrac{\mu-\mathbf{J}(w)}{2}}}.\qed\]

**Proposition C.2** (A-RFD for the Squared Exponential Covariance).: _If \(\mathbf{J}\) is isotropic with squared exponential covariance (11), then the step size of A-RFD is given by_

\[\hat{\eta}=\frac{s^{2}}{\mu-\mathbf{J}(w)}\|\nabla\mathbf{J}(w)\|,\]Proof.: By Definition 5.1 of A-RFD and \(\Theta=\frac{\|\nabla\mathbf{J}(w)\|}{\mu-\mathbf{J}(w)}\) we have

\[\hat{\eta}(\Theta)=\frac{C(0)}{-C^{\prime}(0)}\Theta=\frac{\sigma^{2}\exp(0)}{ \frac{\sigma^{2}}{\sigma^{2}}\exp(0)}\frac{\|\nabla\mathbf{J}(w)\|}{\mu- \mathbf{J}(w)}=s^{2}\frac{\|\nabla\mathbf{J}(w)\|}{\mu-\mathbf{J}(w)}.\qed\]

**Proposition C.3**.: _If \(\mathbf{J}\) is isotropic with squared exponential covariance (11), then the RFD step sizes are strictly monotonously increasing in \(\Theta\)._

Proof.: Since we know that \(\Theta\to 0\) implies \(\eta^{*}\sim\hat{\eta}\to 0\) strict monotonicity of \(\eta^{*}\) in \(\Theta\) is sufficient to show that \(\eta^{*}\to 0\) also implies \(\Theta\to 0\). So we take the derivative of (12) resulting in

\[\frac{d}{d\Theta}\eta^{*}=\frac{1-\frac{1}{\sqrt{1+s^{2}(2\Theta)^{2}}}}{2 \Theta^{2}},\]

which is greater zero for all \(\Theta>0\). 

### Rational quadratic

The **rational quadratic** covariance function is given by

\[C\big{(}\tfrac{\|x-y\|}{2}\big{)}=\sigma^{2}\left(1+\frac{\|x-y\|^{2}}{\beta s ^{2}}\right)^{-\beta/2}\quad\beta>0.\] (13)

It can be viewed as a scale mixture of the squared exponential and converges to the squared exponential in the limit \(\beta\to\infty\)[44, p. 87].

**Theorem C.4** (Rational Quadratic).: _For \(\mathbf{J}\sim\mathcal{N}(\mu,C)\) where \(C\) is the **rational quadratic covariance** we have for \(\Theta=\frac{\|\nabla\mathbf{J}(w)\|}{\mu-\mathbf{J}(w)}\geq 0\) that the RFD step size is given by_

\[\eta^{*}=s\sqrt{\beta}\operatorname*{Root}_{\eta}\left(-1+\tfrac{\sqrt{\beta}} {\mathfrak{s}\Theta}\eta+(1+\beta)\eta^{2}+\tfrac{\sqrt{\beta}}{\mathfrak{s} \Theta}\eta^{3}\right).\]

_The unique root of the polynomial in \(\eta\) can be found either directly with a formula for polynomials of third degree (e.g. using Cardano's method) or by bisection as it is contained in \([0,1/\sqrt{1+\beta}]\)._

Proof.: By Theorem 4.2 we have

\[\eta^{*}=\operatorname*{argmin}_{\eta}-\frac{C\big{(}\tfrac{\eta^{2}}{2} \big{)}}{C(0)}-\eta\frac{C^{\prime}\big{(}\tfrac{\eta^{2}}{2}\big{)}}{C^{ \prime}(0)}\Theta\]

for \(C(x)=\sigma^{2}(1+\tfrac{2x}{\beta s^{2}})^{-\beta/2}\). We therefore need to minimize

\[f\big{(}\frac{\eta}{\sqrt{\beta}s}\big{)}:=-\left(1+\frac{\eta^{2}}{\beta s^ {2}}\right)^{-\beta/2}-\eta\left(1+\frac{\eta^{2}}{\beta s^{2}}\right)^{- \beta/2-1}\Theta.\]

Substitute in \(\tilde{\eta}:=\frac{\eta}{\sqrt{\beta}s}\), then the first order condition is

\[0\stackrel{{!}}{{=}}f^{\prime}(\tilde{\eta})=-\frac{d}{d\tilde{ \eta}}\left(1+\tilde{\eta}^{2}\right)^{-\beta/2}+\sqrt{\beta}s\tilde{\eta} \left(1+\tilde{\eta}^{2}\right)^{-\beta/2-1}\Theta\]

Dividing both sides by \(\sqrt{\beta}s\Theta\) we get

\[0=\frac{f^{\prime}(\tilde{\eta})}{\sqrt{\beta}s\Theta} =\tfrac{\beta}{2}(1+\tilde{\eta}^{2})^{-\frac{\beta}{2}-1}2 \tilde{\eta}\frac{1}{\sqrt{\beta}s\Theta}+(1+\tilde{\eta}^{2})^{-\frac{\beta }{2}-2}\left[1+\tilde{\eta}^{2}-(\tfrac{\beta}{2}+1)2\tilde{\eta}^{2}\right]\] \[=(1+\tilde{\eta}^{2})^{-\frac{\beta}{2}-2}\underbrace{\left[ \beta\tilde{\eta}\frac{1}{\sqrt{\beta}s\Theta}(1+\tilde{\eta}^{2})-[1-\tilde {\eta}^{2}(1+\beta)]\right]}_{=-1+\frac{\sqrt{\beta}}{\mathfrak{s}\Theta} \tilde{\eta}+(1+\beta)\tilde{\eta}^{2}+\frac{\sqrt{\beta}}{\mathfrak{s} \Theta}\tilde{\eta}^{3}}\]

Since \(\Theta\geq 0\) and \(\beta>0\) all coefficients of the polynomial are positive except for the shift. The polynomial thus starts out at \(-1\) in zero and only increases from there. Therefore there exists a unique positive critical point which is a minimum.

At the point \(\tilde{\eta}=\sqrt{1+\beta}\) the quadratic term is already larger than \(1\) so the polynomial is positive and we have passed the root. The minimum is therefore contained in the interval \([0,\sqrt{1+\beta}]\).

After finding the minimum in \(\tilde{\eta}\) we return to \(\eta\) by multiplication with \(\sqrt{\beta}s\)

**Proposition C.5** (A-RFD for the Rational Quadratic Covariance).: _If \(\mathbf{J}\) is isotropic with rational quadratic covariance (13), then the step size of A-RFD is given by_

\[\hat{\eta}=\frac{s^{2}}{\mu-\mathbf{J}(w)}\|\nabla\mathbf{J}(w)\|.\]

Proof.: \(C(x)=\sigma^{2}(1+\frac{2\pi}{\beta s^{2}})^{-\beta/2}\) implies by Definition 5.1 of A-RFD and \(\Theta=\frac{\|\nabla\mathbf{J}(w)\|}{\mu-\mathbf{J}(w)}\)

\[\hat{\eta}(\Theta)=\frac{C_{\mathbf{J}}(0)}{-C_{\mathbf{J}}^{\prime}(0)} \Theta=\frac{\sigma^{2}(1+0)^{-\beta/2}}{\frac{\sigma^{2}}{s^{2}}(1+0)^{- \beta/2-1}}\frac{\|\nabla\mathbf{J}(x)\|}{\mu-\mathbf{J}(x)}=s^{2}\frac{\| \nabla\mathbf{J}(x)\|}{\mu-\mathbf{J}(x)}.\qed\]

### Matern

**Definition C.6**.: The Matern model parametrized by \(s>0,\nu\geq 0,\sigma^{2}\geq 0\) is given by

\[C\big{(}\tfrac{\|x-y\|^{2}}{2}\big{)}=\sigma^{2}\frac{2^{1-\nu}}{\Gamma(\nu) }\left(\tfrac{\sqrt{2\nu}\|x-y\|}{s}\right)^{\nu}K_{\nu}\left(\tfrac{\sqrt{2\nu }\|x-y\|}{s}\right)\] (14)

where \(K_{\nu}\) is the modified Bessel function.

For \(\nu=p+\frac{1}{2}\) with \(p\in\mathbb{N}_{0}\), it can be simplified [cf. 44, sec. 4.2.1] to

\[C\big{(}\tfrac{\|x-y\|^{2}}{2}\big{)}=\sigma^{2}e^{-\tfrac{\sqrt{2\nu}\|x-y\|} {s}}\tfrac{p!}{(2p)!}\sum_{k=0}^{p}\tfrac{(2p-k)!}{(p-k)!k!}\left(\tfrac{2\sqrt {2\nu}}{s}\|x-y\|\right)^{k}\]

The Matern model encompasses Rasmussen and Williams [44]

* the **nugget effect** for \(\nu=0\) (independent randomness)
* the **exponential model** for \(\nu=\frac{1}{2}\) (Ornstein-Uhlenbeck process)
* the **squared exponential model** for \(\nu\to\infty\) with the same scale \(s\) and variance \(\sigma^{2}\).

The random functions induced by the Matern model are a.s. \(\lfloor\nu\rfloor\)-times differentiable Rasmussen and Williams [44], i.e. the smoothness of the model increases with increasing \(\nu\). While the exponential covariance model with \(\nu=\frac{1}{2}\) results in a random function which is not yet differentiable, larger \(\nu\) result in increasing differentiability. As differentiability starts with \(\nu=\frac{3}{2}\) and we have a more explicit formula for \(\nu=p+\frac{1}{2}\) the cases \(\nu=\frac{3}{2}\) and \(\nu=\frac{5}{2}\) are of particular interest.

"[F]or \(\nu\geq 7/2\), in the absence of explicit prior knowledge about the existence of higher order derivatives, it is probably very hard from finite noisy training examples to distinguish between values of \(\nu\geq 7/2\) (or even to distinguish between finite values of \(\nu\) and \(\nu\to\infty\), the smooth squared exponential, in this case)" [44, p. 85].

**Theorem C.7**.: _Assuming \(\mathbf{J}\sim\mathcal{N}(\mu,C)\) is a random function where \(C\) is the Matern covariance such that \(\nu=p+\frac{1}{2}\) with \(p\in\{1,2\}\). Then the RFD step is given for \(\Theta:=\frac{\|\nabla\mathbf{J}(w)\|}{\mu-\mathbf{J}(w)}\geq 0\) by_

* \(p=1\)__ \[\eta^{*}=\frac{s}{\sqrt{3}}\frac{1}{\left(1+\frac{\sqrt{3}}{s\Theta}\right)}\]
* \(p=2\)__ \[\eta^{*}=\frac{s}{\sqrt{5}}\frac{(1-\zeta)+\sqrt{4+(1+\zeta)^{2}}}{2(1+\zeta)} \qquad\zeta:=\frac{\sqrt{5}}{3s\Theta}.\]

Proof.: We define \(\mathcal{C}(\eta):=C(\frac{\eta^{2}}{2})\), which implies

\[\mathcal{C}^{\prime}(\eta)=C^{\prime}\big{(}\tfrac{\eta^{2}}{2}\big{)}\eta\]

[MISSING_PAGE_FAIL:24]

**Proposition C.8** (A-RFD for the Matern Covariance).: _If \(\mathbf{J}\) is isotropic with Matern covariance (14) such that \(\nu=p+\frac{1}{2}\), then the step size of A-RFD for \(p\in\{1,2\}\) is given by_

* \(p=1\)__ \[\hat{\eta}=\frac{s^{2}}{3}\frac{\|\nabla\mathbf{J}(x)\|}{\mu-\mathbf{J}(x)}\]
* \(p=2\)__ \[\hat{\eta}=\frac{3s^{2}}{5}\frac{\|\nabla\mathbf{J}(x)\|}{\mu-\mathbf{J}(x)}\]

Proof.: Noting \(\Theta=\frac{\|\nabla\mathbf{J}(x)\|}{\mu-\mathbf{J}(x)}\), we have by Definition 5.1 of A-RFD for \(p=1\)

\[\hat{\eta}=\frac{C(0)}{-C^{\prime}(0)}\Theta\stackrel{{\eqref{eq: Matern covariance}}}{{=}}\frac{s^{2}}{3}\Theta,\]

and in the case \(p=2\)

\[\hat{\eta}=\frac{C(0)}{-C^{\prime}(0)}\Theta\stackrel{{\eqref{eq: Matern covariance}}}{{=}}\frac{3s^{2}}{5}\Theta.\qed\]

## Appendix D Proofs

In this section we prove all the claims made in the main body.

### Section 2: Random function descent

#### d.1.1 Formal RFD

As we mentioned in a footnote at the definition of RFD, the fact that the parameters become random variables as they are selected by random gradients poses some mathematical challenges which would have been distracting to address in the main body. In following paragraphs leading up to Definition D.1 we introduce and discuss the probability theory required to provide a mathematically sound definition.

For a fixed cost distribution \(\mathbb{P}_{\mathbf{J}}\) and any weight vectors \(w\) and \(\tilde{w}\) the conditional distribution

\[\mathbb{E}[\mathbf{J}(\tilde{w})\mid\mathbf{J}(w),\nabla\mathbf{J}(w)]\]

is by its axiomatic definition a \((\mathbf{J}(w),\nabla\mathbf{J}(w))\)-measurable random variable. By the factorization lemma (30, Cor. 1.9.7), there therefore exists a measurable function \((j,g)\mapsto\varphi_{w,\tilde{w}}(j,g)\) such that the following equation holds almost surely

\[\varphi_{w,\theta}(\mathbf{J}(w),\nabla\mathbf{J}(w))=\mathbb{E}[\mathbf{J}( \tilde{w})\mid\mathbf{J}(w),\nabla\mathbf{J}(w)].\] (19)

Since it is possible to calculate \(\varphi_{w,\tilde{w}}\) explicitly in the Gaussian case (cf. G.1), the function

\[\Phi_{\mathbb{P}_{\mathbf{J}}}:\begin{cases}(\mathbb{R}^{d}\times\mathbb{R} \times\mathbb{R}^{d})\rightarrow\mathbb{R}\\ (w,j,g)\mapsto\operatorname*{argmin}_{\tilde{w}}\varphi_{w,\tilde{w}}(j,g), \end{cases}\]

which implements some tie-breaker rules for set valued \(\operatorname*{argmin}\) is measurable when \(\mathbf{J}\) is Gaussian and its covariance function is sufficiently smooth. To prove measurability in the general case is a difficult problem of its own, which we do not attempt to solve here, since we would not utilize the conditional expectation outside of the Gaussian case anyway (cf. Section E.3). For deterministic \(w\), we therefore have

\[\Phi_{\mathbb{P}_{\mathbf{J}}}(w,\mathbf{J}(w),\nabla\mathbf{J}( w)) =\operatorname*{argmin}_{\tilde{w}}\varphi_{w,\tilde{w}}(\mathbf{J} (w),\nabla\mathbf{J}(w))\] \[\stackrel{{\eqref{eq:Matern covariance}}}{{=}} \operatorname*{argmin}_{\tilde{w}}\mathbb{E}[\mathbf{J}(\tilde{w}) \mid\mathbf{J}(w),\nabla\mathbf{J}(w)].\]So if the parameter vectors \(w_{n}\) were deterministic, our formal definition of RFD and our initial definition would coincide. But for random weights \(W\) (19) stops to hold in general3

Footnote 3: E.g. consider the random variable

\[W=\big{(}\mathrm{argmin}\,\mathbf{J}\big{)}\mathbf{1}_{\mathbf{J}(\tilde{w})>0}+ \big{(}\mathrm{argmax}\,\mathbf{J}\big{)}\mathbf{1}_{\mathbf{J}(\tilde{w})<0}.\]

 In this case, \(\mathbf{J}(W)\) is much more informative of \(\mathbf{J}(\tilde{w})\) than \(\mathbf{J}(w)\) at some deterministic \(w\)., i.e.

\[\varphi_{W,\tilde{w}}(\mathbf{J}(W),\nabla\mathbf{J}(W))\neq\mathbb{E}[ \mathbf{J}(\tilde{w})\mid\mathbf{J}(W),\nabla\mathbf{J}(W)].\]

If this equation does not need to hold, we similarly have in general

\[\Phi_{\mathbb{P}_{\mathbf{J}}}(W,\mathbf{J}(W),\nabla\mathbf{J}(W))\neq \mathrm{argmin}\,\mathbb{E}[\mathbf{J}(\tilde{w})\mid\mathbf{J}(W),\nabla \mathbf{J}(W)].\]

So the following definition is not just a restatement of the original definition of RFD.

**Definition D.1** (Formal RFD).: For a Gaussian random cost function \(\mathbf{J}\), we define the RFD algorithm with starting point \(W_{0}=w_{0}\in\mathbb{R}^{d}\) by

\[W_{n+1}:=\Phi_{\mathbb{P}_{\mathbf{J}}}(W_{n},\mathbf{J}(W_{n}),\nabla\mathbf{ J}(W_{n}))\]

This is what we effectively do in Theorem 4.2 under the additional isotropy assumption, where we calculate the \(\mathrm{argmin}\) under the assumption that \(w\) is deterministic (i.e. we determine \(\Phi_{\mathbb{P}_{\mathbf{J}}}\)), before we plug-in the random variables \(W_{n}\) to obtain \(W_{n+1}\). Similarly this is how the step size prescriptions of RFD actually work. We first assume deterministic weights and later plug the random variables into our formulas. For this reason, we avoided large letters indicating random variables for parameters \(w\) in the main body.

#### d.1.2 Scale invariance

**Advantage 2.3** (Scale invariance).: _RFD is invariant to additive shifts and positive scaling of the cost \(\mathbf{J}\). RFD is also invariant with respect to transformations of the parameter input of \(\mathbf{J}\) by differentiable bijections whose Jacobian is invertible everywhere (e.g. invertible linear maps)._

Before we get to the proof, let us quickly formulate the statement in mathematical terms. Let \(w_{n}\) be the parameters selected optimizing \(\mathbf{J}\) starting in \(w_{0}\) and \(\tilde{w}_{n}\) the parameters selected by the same optimizer optimizing \(\tilde{\mathbf{J}}\) starting in \(\tilde{w}_{0}\).

If we apply affine linear scaling to cost \(\mathbf{J}\) such that \(\tilde{\mathbf{J}}(w)=a\mathbf{J}(w)+b\) and start optimization in the same point, i.e. \(w_{0}=\tilde{w}_{0}\), then we expect a scale invariant optimizer to select

\[w_{n}=\tilde{w}_{n}.\]

If we scale inputs on the other hand (or more generally map them with a bijection \(\phi\)), then we expect for \(\tilde{\mathbf{J}}:=\mathbf{J}\circ\phi\) and starting point \(\tilde{w}_{0}=\phi^{-1}(w_{0})\), that this relationship is retained by a scale invariant optimizer, i.e.

\[\tilde{w}_{n}=\phi^{-1}(w_{n}).\]

Why do we use a different starting point? As an illustrating example, assume that \(\phi\) maps miles into kilometers. Then \(\tilde{\mathbf{J}}\) accepts miles, while \(\mathbf{J}\) accepts kilometers. Then we have to map the initial starting point \(w_{0}\) of \(\mathbf{J}\) measured in kilometers into miles \(\tilde{w}_{0}\). \(\phi^{-1}\) is precisely this transformation from kilometers into miles. A scale invariant optimizer should retain this relation, i.e. no matter if the input is measured in miles or kilometers the same points are selected.

Proof.: The following proof will be split into three parts. The first two parts of the proof will address a more general audience and ignore the mathematical subtleties we discussed in Section D.1.1. In the third part we explain to the interested probabilists how to resolve these issues.

1. **Invariance with regard to affine linear scaling**

Let \(\tilde{\mathbf{J}}(w):=a\mathbf{J}(w)+b\) where \(a>0\) and \(b\in\mathbb{R}\) and assume \(\tilde{w}_{0}=w_{0}\). With the induction start given, we only require the induction step to prove \(\tilde{w}_{n}=w_{n}\).

For the induction step, we assume this equation holds up to \(n\). Since \(\phi(x)=ax+b\) is a measurable bijection, the sigma algebra4 generated by

Footnote 4: if you are unfamiliar with sigma algebras read them as “information”.

\[(\tilde{\mathbf{J}}(w_{n}),\nabla\tilde{\mathbf{J}}(w_{n}))=(\phi\circ\mathbf{ J}(w_{n}),a\nabla\mathbf{J}(w_{n}))\]

is therefore equal to the sigma algebra generated by \((\mathbf{J}(w_{n}),\nabla\mathbf{J}(w_{n}))\). This implies

\[\begin{split}\tilde{w}_{n+1}&=\operatorname*{ argmin}_{w}\mathbb{E}[\tilde{\mathbf{J}}(w)\mid\tilde{\mathbf{J}}(\tilde{w}_{n}), \nabla\tilde{\mathbf{J}}(\tilde{w}_{n})]\\ &\stackrel{{\text{induction}}}{{=}}\operatorname*{ argmin}_{w}\mathbb{E}[\tilde{\mathbf{J}}(w)\mid\tilde{\mathbf{J}}(w_{n}),\nabla \tilde{\mathbf{J}}(w_{n})]\\ &\stackrel{{\text{sigma Alg.}}}{{=}}\operatorname*{ argmin}_{w}\mathbb{E}[\tilde{\mathbf{J}}(w)\mid\mathbf{J}(w_{n}),\nabla\mathbf{J}(w_{n})]\\ &\stackrel{{\text{linearity}}}{{=}}\operatorname*{ argmin}_{w}a\mathbb{E}[\mathbf{J}(w)\mid\mathbf{J}(w_{n}),\nabla\mathbf{J}(w_{n})]+b\\ &\stackrel{{\text{monotonicity}}}{{=}}\operatorname*{ argmin}_{w}\mathbb{E}[\mathbf{J}(w)\mid\mathbf{J}(w_{n}),\nabla\mathbf{J}(w_{n})]\\ &\stackrel{{\text{def.}}}{{=}}w_{n+1}\end{split}\] (20)

Where we have used the linearity of the conditional expectation and the strict monotonicity of \(\phi(x)=ax+b\).

## 2 Invariance with regard to certain input bijections

Let \(\phi\) be a differentiable bijection whose jacobian is invertible everywhere and assume \(\tilde{\mathbf{J}}:=\mathbf{J}\circ\phi\). Since \(\phi\) is a bijection, \(\phi(M)\) is the domain of \(\mathbf{J}\) whenever \(M\) is the domain of \(\tilde{\mathbf{J}}\).

For a starting point \(w_{0}\in\phi(M)\) we now assume \(\tilde{w}_{0}=\phi^{-1}(w_{0})\in M\) and are again going to prove the claim

\[\tilde{w}_{n}=\phi^{-1}(w_{n}).\]

by induction. Assume that we have this claim up to \(n\). Then we have by induction

\[\tilde{\mathbf{J}}(\tilde{w}_{n})=\mathbf{J}\circ\phi(\phi^{-1}(w_{n}))= \mathbf{J}(w_{n})\] (21)

and

\[\nabla\tilde{\mathbf{J}}(\tilde{w}_{n})=\nabla_{\tilde{w}_{n}}(\mathbf{J} \circ\phi(\tilde{w}_{n}))=\phi^{\prime}(\tilde{w}_{n})(\nabla\mathbf{J})(\phi (\tilde{w}_{n}))=\phi^{\prime}(\tilde{w}_{n})\nabla\mathbf{J}(w_{n}).\]

Since \(\phi^{\prime}(\tilde{w}_{n})\) is invertible by assumption, the sigma algebras generated by \((\tilde{\mathbf{J}}(\tilde{w}_{n}),\nabla\tilde{\mathbf{J}}(\tilde{w}_{n}))\) and \(\mathbf{J}(w_{n}),\nabla\mathbf{J}(w_{n})\) are identical. But this results in the induction step

\[\begin{split}\tilde{w}_{n+1}&=\operatorname*{ argmin}_{w\in M}\mathbb{E}[\tilde{\mathbf{J}}(w)\mid\tilde{\mathbf{J}}(\tilde{w}_{n}), \nabla\tilde{\mathbf{J}}(\tilde{w}_{n})]\\ &\stackrel{{\text{sigma Alg.}}}{{=}}\operatorname*{ argmin}_{w\in M}\mathbb{E}[\tilde{\mathbf{J}}(w)\mid\mathbf{J}(w_{n}),\nabla \mathbf{J}(w_{n})]\\ &\stackrel{{\text{def.}}}{{=}}\operatorname*{ argmin}_{w\in M}\mathbb{E}[\mathbf{J}\circ\phi(w)\mid\mathbf{J}(w_{n}),\nabla \mathbf{J}(w_{n})]\\ &=\phi^{-1}\Bigl{(}\underbrace{\operatorname*{argmin}_{\theta \in\phi(M)}\mathbb{E}[\mathbf{J}(\theta)\mid\mathbf{J}(w_{n}),\nabla\mathbf{J} (w_{n})]}_{\stackrel{{\text{def.}}}{{=}}w_{n+1}}\Bigr{)}.\end{split}\] (22)

where we simply optimize over \(\theta=\phi(w)\) instead of \(w\) and correct the \(\operatorname*{argmin}\) at the end.

## 3 Addressing the subtleties

In equation (20) we have really proven for deterministic \(w\)

\[\Phi_{\mathbb{P}_{\tilde{\jmath}}}(w,\tilde{\mathbf{J}}(w),\nabla\tilde{ \mathbf{J}}(w))=\Phi_{\mathbb{P}_{\tilde{\jmath}}}(w,\mathbf{J}(w),\nabla \mathbf{J}(w)).\]

But this implies with the induction assumption \(W_{n}=\tilde{W}_{n}\)

\[\tilde{W}_{n+1}=\Phi_{\mathbb{P}_{\tilde{\jmath}}}(\tilde{W}_{n},\tilde{ \mathbf{J}}(\tilde{W}_{n}),\nabla\tilde{\mathbf{J}}(\tilde{W}_{n}))\stackrel{{ \text{ind.}}}{{=}}\Phi_{\mathbb{P}_{\tilde{\jmath}}}(W_{n},\mathbf{J}(W_{n}), \nabla\mathbf{J}(W_{n}))=W_{n+1}.\]Similarly we have proven in (22) that

\[\Phi_{\mathbb{P}_{\tilde{\mathbf{J}}}}\big{(}\phi^{-1}(w),\tilde{\mathbf{J}}(\phi^ {-1}(w)),\nabla\tilde{\mathbf{J}}(\phi^{-1}(w))\big{)}=\phi^{-1}\big{(}\Phi_{ \mathbb{P}_{\tilde{\mathbf{J}}}}(w,\mathbf{J}(w),\nabla\mathbf{J}(w))\big{)}.\]

By the induction assumption \(\tilde{W}=\phi^{-1}(W_{n})\), this implies

\[\tilde{W}_{n+1} =\Phi_{\mathbb{P}_{\tilde{\mathbf{J}}}}(\tilde{W}_{n},\tilde{ \mathbf{J}}(\tilde{W}_{n}),\nabla\tilde{\mathbf{J}}(\tilde{W}_{n}))\] \[\stackrel{{\text{ind}}}{{=}}\Phi_{\mathbb{P}_{ \tilde{\mathbf{J}}}}\big{(}\phi^{-1}(W_{n}),\tilde{\mathbf{J}}(\phi^{-1}(W_{n} )),\nabla\tilde{\mathbf{J}}(\phi^{-1}(W_{n}))\big{)}\] \[=\phi^{-1}\big{(}\Phi_{\mathbb{P}_{\tilde{\mathbf{J}}}}(W_{n}, \mathbf{J}(W_{n}),\nabla\mathbf{J}(W_{n}))\big{)}\] \[=\phi^{-1}(W_{n+1}).\qed\]

### Section 4: Relation to gradient descent

**Lemma 4.1** (Explicit first order stochastic Taylor approximation).: _For \(\mathbf{J}\sim\mathcal{N}(\mu,C)\), the first order stochastic Taylor approximation is given by_

\[\mathbb{E}[\mathbf{J}(w-\mathbf{d})\mid\mathbf{J}(w),\nabla\mathbf{J}(w)]=\mu +\frac{C\big{(}\frac{\|\mathbf{d}\|^{2}}{2}\big{)}}{C(0)}(\mathbf{J}(w)-\mu)- \frac{C^{\prime}\big{(}\frac{\|\mathbf{d}\|^{2}}{2}\big{)}}{C^{\prime}(0)} \langle\mathbf{d},\nabla\mathbf{J}(w)\rangle.\]

Proof.: \((\mathbf{J}(w),\nabla\mathbf{J}(w),\mathbf{J}(w-\mathbf{d}))\) is a Gaussian vector for which the conditional distribution is well known. It is only necessary to calculate the covariance matrix. The key ingredient here is to observe that \(\mathbf{J}(w),\partial_{1}\mathbf{J}(w),\dots,\partial_{d}\mathbf{J}(w)\) are all independent, trivializing matrix inversion.

More formally, by Lemma G.2 we have

\[\mathrm{Cov}\Big{(}\begin{pmatrix}\mathbf{J}(w)\\ \nabla\mathbf{J}(w)\end{pmatrix}\Big{)}=\begin{pmatrix}C(0)&\\ &-C^{\prime}(0)\mathbb{I}_{d\times d}\end{pmatrix}\]

and

\[\mathrm{Cov}\Big{(}\mathbf{J}(w-\mathbf{d}),\begin{pmatrix}\mathbf{J}(w)\\ \nabla\mathbf{J}(w)\end{pmatrix}\Big{)}=\begin{pmatrix}C(\frac{\|\mathbf{d}\|^ {2}}{2})\\ C^{\prime}(\frac{\|\mathbf{d}\|^{2}}{2})\mathbf{d}\end{pmatrix}.\]

By Theorem G.1 we therefore know that

\[\mathbb{E}[\mathbf{J}(w-\mathbf{d})\mid\mathbf{J}(w),\nabla\mathbf{J}(w)]=\mu +\begin{pmatrix}C(\frac{\|\mathbf{d}\|^{2}}{2})\\ C^{\prime}(\frac{\|\mathbf{d}\|^{2}}{2})\mathbf{d}\end{pmatrix}^{T}\begin{pmatrix} C(0)&\\ &-C^{\prime}(0)\mathbb{I}_{d\times d}\end{pmatrix}^{-1}\begin{pmatrix}\mathbf{J}(w)- \mu\\ \nabla\mathbf{J}(w)\end{pmatrix},\]

which immediately yields the claim. 

**Theorem 4.2** (Explicit RFD).: _Let \(\mathbf{J}\sim\mathcal{N}(\mu,C)\), then RFD coincides with gradient descent_

\[w_{n+1}=w_{n}-\eta_{n}^{*}\frac{\nabla\mathbf{J}(w_{n})}{\|\nabla\mathbf{J}(w _{n})\|},\]

_where the RFD step sizes are given by_

\[\eta_{n}^{*}:=\operatorname*{argmin}_{\eta\in\mathbb{R}}\frac{C\big{(}\frac{ \eta^{2}}{2}\big{)}}{C(0)}(\mathbf{J}(w_{n})-\mu)-\eta\frac{C^{\prime}\big{(} \frac{\eta^{2}}{2}\big{)}}{C^{\prime}(0)}\|\nabla\mathbf{J}(w_{n})\|.\] (1)

Proof.: The explicit version of RFD follows essentially by fixing the step size \(\eta=\|\mathbf{d}\|\) and optimizing over the direction first. With Lemma 4.1 we have

\[\min_{\mathbf{d}}\mathbb{E}[\mathbf{J}(w-\mathbf{d})\mid\mathbf{ J}(w),\nabla\mathbf{J}(w)]\] \[=\min_{\eta\geq 0}\min_{\mathbf{d}:\|\mathbf{d}\|=\eta}\mu+\frac{C \big{(}\frac{\eta^{2}}{2}\big{)}}{C(0)}(\mathbf{J}(w)-\mu)-\frac{C^{\prime} \big{(}\frac{\eta^{2}}{2}\big{)}}{C^{\prime}(0)}\langle\mathbf{d},\nabla \mathbf{J}(w)\rangle\] \[=\min_{\eta\geq 0}\mu+\frac{C\big{(}\frac{\eta^{2}}{2}\big{)}}{C(0)}( \mathbf{J}(w)-\mu)-\frac{C^{\prime}\big{(}\frac{\eta^{2}}{2}\big{)}}{C^{ \prime}(0)}\begin{cases}\max_{\mathbf{d}:\|\mathbf{d}\|=\eta}\langle\mathbf{d },\nabla\mathbf{J}(w)\rangle&\frac{C^{\prime}(\frac{\eta^{2}}{2})}{C^{\prime} (0)}\geq 0\\ \min_{\mathbf{d}:\|\mathbf{d}\|=\eta}\langle\mathbf{d},\nabla\mathbf{J}(w) \rangle&\frac{C^{\prime}(\frac{\eta^{2}}{2})}{C^{\prime}(0)}<0.\end{cases}\]

[MISSING_PAGE_FAIL:29]

**Theorem D.3** (Details of Proposition 5.2).: _Let \(\mathbf{J}\sim\mathcal{N}(\mu,C)\) and assume there exists \(\eta_{0}>0\) such that the correlation for larger distances \(\eta\geq\eta_{0}\) are bounded smaller than \(1\), i.e. \(\frac{C(\eta^{2}/2)}{C(0)}<\rho\in(0,1)\). Then there exists \(K,\Theta_{0}>0\) such that for all \(\Theta<\Theta_{0}\)_

\[1-K\Theta\leq\frac{\eta^{*}(\Theta)}{\hat{\eta}(\Theta)}\leq 1+K\Theta.\]

_In particular we have \(\eta^{*}(\Theta)\sim\hat{\eta}(\Theta)\) as \(\Theta\to 0\) or equivalently as \(\hat{\eta}\to 0\)._

Proof.: This follows immediately from Lemma D.4, Lemma D.5 and Lemma D.6. 

**Corollary 5.3**.: _Assume \(\eta^{*}\to 0\) implies \(\Theta\to 0\), the cost \(\mathbf{J}\) is bounded, has continuous gradients and RFD converges to some point \(w_{\infty}\). Then \(w_{\infty}\) is a critical point and the RFD step sizes \(\eta^{*}\) are asymptotically equal to \(\hat{\eta}\)._

Proof.: Assuming RFD converges, its step sizes \(\eta^{*}\) converge to zero. But this implies \(\Theta\to 0\) by assumption, i.e.

\[\Theta=\frac{\|\nabla\mathbf{J}(w)\|}{\mu-\mathbf{J}(w)}\to 0\]

Since \(\mathbf{J}(w)\) is bounded, this implies \(\|\nabla\mathbf{J}(w)\|\to 0\) and by continuity the of the gradient, it is zero in its limit. Thus we converge to a stationary point. The asymptotic equality follows by Lemma D.4 and Lemma D.5, as we know \(\eta^{*}\) converges so we do not require the assumptions of Lemma D.6. 

#### d.3.1 Locating the Minimizer

In the following we want to rule out locations for the RFD step size \(\eta^{*}\) by proving \(q_{\Theta}(\eta)>q_{\Theta}(\hat{\eta})\) for a wide range of \(\eta\). For this endeavour the relative position of the step size \(\eta\) relative to \(\hat{\eta}\) is a useful re-parametrization

\[\eta:=\eta(\lambda)=\lambda\hat{\eta}.\]

Due to \(\hat{\eta}=\frac{C(0)}{-C^{\prime}(0)}\Theta\) we obtain

\[T_{2}q_{\Theta}(\eta)=-1-\eta\Theta+\tfrac{\eta^{2}-C^{\prime}(0)}{C(0)}=-1+ \lambda(\tfrac{\lambda}{2}-1)\hat{\eta}\Theta\]

On the other hand we have for the bound

\[|q_{\Theta}(\eta)-T_{2}q_{\Theta}(\eta)|\leq\lambda^{3}\hat{\eta}^{3}c_{0} \Big{(}\lambda\tfrac{C(0)}{4|C^{\prime}(0)|}+1\Big{)}\Theta\]

Since \(\hat{\eta}=\eta(1)\) we thus obtain

\[\frac{q_{\Theta}(\eta)-q_{\Theta}(\hat{\eta})}{\hat{\eta}\Theta} \geq\frac{T_{2}q_{\Theta}(\eta)-|q_{\Theta}(\eta)-T_{2}q_{\Theta}( \eta)|-T_{2}q_{\Theta}(\hat{\eta})-|q_{\Theta}(\hat{\eta})-T_{2}q_{\Theta}( \hat{\eta})|}{\hat{\eta}\Theta}\] \[=\tfrac{1}{2}(1-\lambda)^{2}-\hat{\eta}^{2}c_{0}\Big{[}\lambda^{3 }\Big{(}\lambda\tfrac{C(0)}{4|C^{\prime}(0)|}+1\Big{)}+\Big{(}\tfrac{C(0)}{4| C^{\prime}(0)|}+1\Big{)}\Big{]}.\] (23)

This equation will be the basis of a number of lemmas ruling out various step sizes as minimizers.

**Lemma D.4** (Ruling out small step sizes).: _If the step size is (much) smaller than the asymptotic step size \(\hat{\eta}=\hat{\eta}(\Theta)\), then it can not be a minimizer. More specifically_

\[\frac{\eta}{\hat{\eta}}\in[0,1-c_{1}\Theta)\implies q_{\Theta}(\eta)>q_{ \Theta}(\hat{\eta})\]

_where \(c_{1}:=2\frac{C(0)}{|C^{\prime}(0)|}\sqrt{c_{0}\big{(}\frac{C(0)}{4|C^{\prime}( 0)|}+1\big{)}}<\infty\)._Proof.: Here we consider the case \(\eta\leq\hat{\eta}\), i.e. \(\lambda\in[0,1]\). By (23) we have

\[\frac{q_{\Theta}(\eta)-q_{\Theta}(\hat{\eta})}{\hat{\eta}\Theta} \geq\tfrac{1}{2}(1-\lambda)^{2}-\hat{\eta}^{2}c_{0}\Big{[}\lambda ^{3}\Big{(}\lambda\frac{C(0)}{4|C^{\prime}(0)|}+1\Big{)}+\Big{(}\frac{C(0)}{4 |C^{\prime}(0)|}+1\Big{)}\Big{]}\] \[\geq\tfrac{1}{2}(1-\lambda)^{2}-2\hat{\eta}^{2}c_{0}\Big{(}\tfrac {C(0)}{4|C^{\prime}(0)|}+1\Big{)}\] \[\overset{!}{>}0\]

for which

\[(1-\lambda)^{2}>4\hat{\eta}^{2}c_{0}\Big{(}\tfrac{C(0)}{4|C^{\prime}(0)|}+1 \Big{)}\]

is sufficient or equivalently

\[\lambda<1-2\hat{\eta}\sqrt{c_{0}\Big{(}\tfrac{C(0)}{4|C^{\prime}(0)|}+1\Big{)} }=1-\Theta\underbrace{2\tfrac{C(0)}{|C^{\prime}(0)|}\sqrt{c_{0}\Big{(}\tfrac{C (0)}{4|C^{\prime}(0)|}+1\Big{)}}}_{=:c_{1}}\]

So for \(\lambda\in[0,1-\Theta c_{1})\) we have \(q_{\Theta}(\eta)>q_{\Theta}(\hat{\eta})\). 

**Lemma D.5** (Ruling out medium sized step sizes as minimizer).: _For \(c_{2}=2c_{1}\) and \(\Theta\leq\Theta_{0}:=\frac{1}{5c_{1}}\), we have_

\[\frac{\eta}{\hat{\eta}}\in\big{(}1+c_{2}\Theta,\tfrac{1}{c_{2}\Theta}\big{)} \implies q_{\Theta}(\eta)>q_{\Theta}(\hat{\eta})\]

Proof.: Here we consider the case \(\lambda\geq 1\), i.e. \(\eta>\hat{\eta}\). Again starting with (23) we get

\[\frac{q(\eta)-q(\hat{\eta})}{\hat{\eta}\Theta} \geq\tfrac{1}{2}(1-\lambda)^{2}-\hat{\eta}^{2}c_{0}\Big{[} \lambda^{3}\Big{(}\lambda\frac{C(0)}{4|C^{\prime}(0)|}+1\Big{)}+\Big{(}\frac{ C(0)}{4|C^{\prime}(0)|}+1\Big{)}\Big{]}\] \[\geq\tfrac{1}{2}(\lambda-1)^{2}-2\lambda^{4}\hat{\eta}^{2}c_{0} \Big{(}\tfrac{C(0)}{4|C^{\prime}(0)|}+1\Big{)}\] \[\overset{!}{>}0,\]

for which

\[\lambda-1>2\lambda^{2}\hat{\eta}\sqrt{c_{0}\Big{(}\tfrac{C(0)}{4|C^{\prime}(0 )|}+1\Big{)}}=c_{1}\Theta\lambda^{2}\]

or equivalently

\[\lambda-1-c_{1}\Theta\lambda^{2}>0\]

is sufficient. Note that this is a concave parabola in \(\lambda\). So it is positive between its zeros which are characterized by

\[c_{1}\Theta\lambda^{2}-\lambda+1=0.\]

They are thus given by

\[\lambda_{1/2}=\frac{1\pm\sqrt{1-4c_{1}\Theta}}{2c_{1}\Theta}.\]

So whenever \(\lambda\in(\lambda_{1},\lambda_{2})\) we have that \(q_{\Theta}(\eta)>q_{\Theta}(\hat{\eta})\). In particular for \(4c_{1}\Theta\leq 1\) or equivalently \(\Theta\leq\frac{1}{4c_{1}}\) we have

\[\lambda_{2}\geq\frac{1}{2c_{1}\Theta}=\frac{1}{c_{2}\Theta}\]

To get a bound on \(\lambda_{1}\) note that the original equation was essentially

\[\lambda\geq 1+c_{1}\Theta\lambda^{2}\]

with equality for \(\lambda=\lambda_{1}\), if \(\Theta\) is reduced, the inequality remains, which implies that \(\lambda_{1}\) is decreasing with \(\Theta\). So assuming the inequality is satisfied for a particular \(\lambda\) e.g. \(\lambda=\sqrt{2}\) which requires

\[\sqrt{2}\geq 1+2c_{1}\Theta\iff\Theta\leq\tfrac{\sqrt{2}-1}{2c_{1}},\]

then we know that \(\lambda_{1}\leq\sqrt{2}\) for all smaller \(\Theta\). This implies for \(\Theta\leq\Theta_{0}=\frac{1}{5c_{1}}\leq\frac{\sqrt{2}-1}{2c_{1}}\)

\[\lambda_{1}=1+c_{1}\Theta\lambda_{1}^{2}\leq 1+\underbrace{2c_{1}}_{c_{2}} \Theta.\qed\]

**Lemma D.6** (Ruling out large step sizes as minimizer).: _If there exists step size \(\eta_{0}>0\) such that the correlation is bounded by some \(\rho<1\), i.e._

\[\frac{C\big{(}\frac{\eta^{2}}{2}\big{)}}{C(0)}\leq\rho\in(0,1),\]

_for larger step sizes \(\eta\geq\eta_{0}\), then there exist \(\Theta_{0}>0\) such that for all \(\Theta<\Theta_{0}\)_

\[\frac{\eta}{\hat{\eta}}\in\big{(}1+c_{2}\Theta,\infty\big{)}\implies q(\eta)>q (\hat{\eta}),\]

_where \(c_{2}\) is the constant from Lemma D.5._

Proof.: The upper bound \(\frac{1}{c_{2}\Theta}\) in Lemma D.5 is only due to the loss of precision of the Taylor approximation. To remove it, we take a closer look at the actual \(q_{\Theta}\) itself. We have the following bound for our asymptotic minimum

\[\frac{q_{\Theta}(\hat{\eta})}{\Theta} \leq\frac{T_{2}q_{\Theta}(\hat{\eta})+|q_{\Theta}(\hat{\eta})-T_ {2}q_{\Theta}(\hat{\eta})|}{\Theta}=-\frac{1}{\Theta}-\frac{1}{2}\hat{\eta}+ \hat{\eta}^{3}\underbrace{c_{0}\Big{(}\frac{C(0)}{4|C^{\prime}(0)|}+1\Big{)}}_ {=:c_{3}}\] \[\leq-\frac{1}{\Theta}+\hat{\eta}^{3}c_{3}\]

Which means we have for

\[\frac{q_{\Theta}(\eta)-q_{\Theta}(\hat{\eta})}{\Theta} \geq\Big{(}1-\frac{C\big{(}\frac{\eta^{2}}{2}\big{)}}{C(0)}\Big{)} \frac{1}{\Theta}-\eta\frac{C^{\prime}\big{(}\frac{\eta^{2}}{2}\big{)}}{C^{ \prime}(0)}-\hat{\eta}^{3}c_{3}\] \[\stackrel{{\text{Lemma D.7}}}{{\geq}}\Big{(}1- \frac{C\big{(}\frac{\eta^{2}}{2}\big{)}}{C(0)}\Big{)}\frac{1}{\Theta}-\frac{ \sqrt{C(0)}}{\sqrt{-C^{\prime}(0)}}-\hat{\eta}^{3}c_{3}\] \[\geq\big{(}1-M\big{)}\frac{1}{\Theta}-\frac{\sqrt{C(0)}}{\sqrt{- C^{\prime}(0)}}-\hat{\eta}^{3}c_{3}\] \[\stackrel{{!}}{{\geq}}0,\]

where we use the assumption that there exists \(\rho\in(0,1)\) such that \(\rho\geq\frac{C\big{(}\frac{\eta^{2}}{2}\big{)}}{C(0)}\) for all \(\eta\geq\eta_{0}\) and the fact that we only need to consider \(\eta\geq\frac{1}{c_{2}\Theta}\) (due to Lemma D.5) which allows a translation of \(\eta_{0}\) into some maximal \(\Theta_{0}\). Note that \(\hat{\eta}\sim\Theta\) vanishes as \(\Theta\to 0\), so eventually the term \((1-M)\frac{1}{\Theta}\) dominates. Selecting \(\Theta_{0}\) small small enough is thus sufficient to cover everything that is not already covered by Lemma D.5. 

#### d.3.2 Technical bounds

**Lemma D.7** (Bound on the first derivative of the covariance).: \[\sup_{\eta\geq 0}|C^{\prime}\big{(}\tfrac{\eta^{2}}{2}\big{)}\eta|\leq\sqrt{-C ^{\prime}(0)C(0)}\]

Proof.: Since we have

\[\operatorname{Cov}(D_{v}\mathbf{J}(x),\mathbf{J}(y))=C^{\prime}\big{(}\tfrac {\|x-y\|^{2}}{2}\big{)}\langle x-y,v\rangle\]

we have for a standardized vector \(\|v\|=1\) and \(x-y=\eta v\) by Cauchy-Schwarz

\[|C^{\prime}\big{(}\tfrac{\eta^{2}}{2}\big{)}\eta|=|\operatorname{Cov}(D_{v} \mathbf{J}(x),\mathbf{J}(y))|\stackrel{{\text{c.s.}}}{{\leq}} \sqrt{\operatorname{Var}(D_{v}\mathbf{J}(x))\operatorname{Var}(\mathbf{J}(y) )}=\sqrt{-C^{\prime}(0)C(0)}.\]

As the bound is independent of \(\eta\) this yields the claim. 

**Lemma D.8** (Bound on the second derivative of the covariance).: \[\sup_{\theta\geq 0}|C^{\prime\prime}(\theta)|\leq\max\Bigl{\{}\sup_{\theta \in[0,1]}|C^{\prime\prime}(\theta)|,|C^{\prime}(0)|\Bigr{\}}\]Proof.: Note that

\[\mathrm{Cov}(D_{v}\mathbf{J}(x),D_{w}\mathbf{J}(y))=-C^{\prime\prime}\big{(}\tfrac{ \|x-y\|^{2}}{2}\big{)}\langle x-y,v\rangle\langle x-y,w\rangle-C^{\prime}\big{(} \tfrac{\|x-y\|^{2}}{2}\big{)}\langle v,w\rangle\]

Selecting \(v,w\) as orthonormal vectors (e.g. \(v=e_{1},w=e_{2}\)) and \(x-y:=\eta(v+w)\) for some \(\eta>0\) results in \(\|x-y\|^{2}=2\eta^{2}\) and thus by the Cauchy-Schwarz inequality

\[\big{|}-C^{\prime\prime}(\eta^{2})\eta^{2}\big{|}=\big{|}\mathrm{Cov}(D_{v} \mathbf{J}(x),D_{w}\mathbf{J}(y))\big{|}\stackrel{{\text{C.s.}}}{{ \leq}}\sqrt{\mathrm{Var}(D_{v}\mathbf{J}(x))\,\mathrm{Var}(D_{w}\mathbf{J}(y) )}=\sqrt{(-C^{\prime}(0))^{2}}\]

This implies the claim. 

### Section 6: Stochastic loss

**Lemma D.9**.: _The stochastic approximation errors_

\[\epsilon_{i}(w):=\ell_{i}(w)-\mathbf{J}(w)\]

_are identically distributed, centered random functions, which are independent conditional on \(\mathbf{f}\). In particular,_

\[\mathbb{E}[\epsilon_{i}(w)\epsilon_{j}(\tilde{w})]=\mathbb{E}[\epsilon_{i}(w) \epsilon_{j}(\tilde{w})\mid\mathbf{f}]=0\quad\forall j\neq i.\]

Proof.: The \(\epsilon_{i}\) are independent random functions conditional on \(\mathbf{f}\), since for any \(n\in\mathbb{N}\), any bounded measurable functions \(h\) and \(g\)

\[\mathbb{E}\Big{[}h\big{(}\epsilon_{i}(w_{1}),\dots,\epsilon_{i}( w_{n})\big{)}g\big{(}\epsilon_{j}(w_{1}),\dots,\epsilon_{j}(w_{n})\big{)}\mid \mathbf{f}\Big{]}\] \[=\Big{[}h\big{(}\epsilon_{i}(w_{1}),\dots,\epsilon_{i}(w_{n}) \big{)}\underset{\langle\epsilon_{n}\rangle\mathbb{E}\Big{[}g(\epsilon_{j}(w_ {1}),\dots,\epsilon_{j}(w_{n}))\mid\mathbf{f}\Big{]}}{\mathbb{E}\Big{[}g\big{(} \epsilon_{j}(w_{1}),\dots,\epsilon_{j}(w_{n})\big{)}\mid\mathbf{f}\Big{]}}\] \[=\mathbb{E}\Big{[}h\big{(}\epsilon_{i}(w_{1}),\dots,\epsilon_{i} (w_{n})\big{)}\mid\mathbf{f}\Big{]}\mathbb{E}\Big{[}g\big{(}\epsilon_{j}(w_{ 1}),\dots,\epsilon_{j}(w_{n})\big{)}\mid\mathbf{f}\Big{]},\]

where \((*)\) uses the fact that \(\epsilon_{j}\) does not depend on the independent \(X_{i},\varsigma_{i}\). Since almost by definition

\[\mathbb{E}[\epsilon_{i}\mid\mathbf{f}]=\mathbb{E}[\ell(\cdot,X_{i},Y_{i}) \mid\mathbf{f}]-\mathbf{J}(\cdot)=0,\]

the stochastic approximation errors are thus uncorrelated

\[\mathbb{E}[\epsilon_{i}\epsilon_{j}]=\mathbb{E}\Big{[}\mathbb{E}[\epsilon_{i} \epsilon_{j}\mid\mathbf{f}]\Big{]}=\mathbb{E}\Big{[}\mathbb{E}[\epsilon_{i} \mid\mathbf{f}]\mathbb{E}[\epsilon_{j}\mid\mathbf{f}]\Big{]}=0.\]

**Extension 6.2** (S-RFD).: _For loss \(\mathbf{J}\sim\mathcal{N}(\mu,C)\) and stochastic errors \(\epsilon_{i}\stackrel{{\text{iid}}}{{\sim}}\mathcal{N}(0,C_{ \epsilon})\) we have_

\[\underset{\mathbf{d}}{\operatorname{argmin}}\,\mathbb{E}[\mathbf{J}(w-\mathbf{ d})\mid\mathcal{L}_{b}(w),\nabla\mathcal{L}_{b}(w)]=\eta^{*}(\Theta)\tfrac{ \nabla\mathcal{L}_{b}(w)}{\|\nabla\mathcal{L}_{b}(w)\|}\]

_with the same step size function \(\eta^{*}\) as for RFD, but modified \(\Theta\)_

\[\Theta=\frac{C^{\prime}(0)}{C^{\prime}(0)+\frac{1}{b}C^{\prime}_{\epsilon}(0)} \frac{C(0)+\frac{1}{b}C_{\epsilon}(0)}{C(0)}\frac{\|\nabla\mathcal{L}_{b}(w)\| }{\mu-\mathcal{L}_{b}(w)}.\]

Proof.: Since \(\epsilon_{i}\) are conditionally independent between each other and to \(\mathbf{J}\), as entire functions, the same holds true for \(\nabla\epsilon_{i}\). As all the mixed covariances disappear we have

\[\mathrm{Cov}\Big{(}\Big{(}\mathcal{L}_{b}(w)\Big{)}\Big{)} =\mathrm{Cov}\Big{(}\Big{(}\mathbf{J}(w)\Big{)}\Big{)}+\frac{1} {b^{2}}\sum_{i=1}^{b}\mathrm{Cov}\Big{(}\Big{(}\frac{\epsilon_{i}(w)}{\nabla \epsilon_{i}(w)}\Big{)}\Big{)}\] \[=\begin{pmatrix}C(0)&-C^{\prime}(0)\mathbb{I}_{d\times d}\end{pmatrix} +\frac{1}{b^{2}}\sum_{i=1}^{b}\begin{pmatrix}C_{\epsilon}(0)&-C^{\prime}_{ \epsilon}(0)\mathbb{I}_{d\times d}\end{pmatrix}\] \[=\begin{pmatrix}C(0)+\frac{1}{b}C_{\epsilon}(0)&-\Big{(}C^{\prime} (0)+\frac{1}{b}C^{\prime}_{\epsilon}(0)\Big{)}\mathbb{I}_{d\times d}.\end{pmatrix}\]by Lemma G.2. If you want to break up the first step we recommend considering individual entries of the covariance matrix to convince yourself that all the mixed covariances disappear. Together with the fact

\[\operatorname{Cov}\Bigl{(}\mathbf{J}(w-\mathbf{d}),\binom{\mathcal{ L}_{b}(w)}{\nabla\mathcal{L}_{b}(w)}\Bigr{)}\] \[=\operatorname{Cov}\Bigl{(}\mathbf{J}(w-\mathbf{d}),\binom{ \mathbf{J}(w)}{\nabla\mathbf{J}(w)}\Bigr{)}+\frac{1}{b^{2}}\sum_{i=1}^{b} \underbrace{\operatorname{Cov}\Bigl{(}\mathbf{J}(w-\mathbf{d}),\binom{ \epsilon_{i}(w)}{\nabla\epsilon_{i}(w)}\Bigr{)}}_{=0}\] \[=\binom{C(\frac{\|\mathbf{d}\|^{2}}{2})}{C^{\prime}(\frac{\| \mathbf{d}\|^{2}}{2})\mathbf{d}}.\]

The rest is analogous to Lemma 4.1 and Theorem 4.2, so we only sketch the remaining steps.

Applying Theorem G.1 as in Lemma 4.1 we obtain a stochastic version of the stochastic Taylor approximation ("stochastic" Taylor approximation" perhaps?)

\[\mathbb{E}[\mathbf{J}(w-\mathbf{d})\mid\mathcal{L}_{b}(w),\nabla\mathcal{L}_ {b}(w)]=\mu+\frac{C(\frac{\|\mathbf{d}\|^{2}}{2})}{C(0)+\frac{1}{b}C_{\epsilon }(0)}(\mathcal{L}_{b}(w)-\mu)-\frac{C^{\prime}\bigl{(}\frac{\|\mathbf{d}\|^{2} }{2}\bigr{)}}{C^{\prime}(0)+\frac{1}{b}C^{\prime}_{\epsilon}(0)}\langle \mathbf{d},\mathcal{L}_{b}(w)\rangle.\]

Minimizing this subject to a constant step size as in Theorem 4.2 results in

\[\eta^{*} =\operatorname*{argmin}_{\eta\in\mathbb{R}}\frac{C(\frac{\| \mathbf{d}\|^{2}}{2})}{C(0)+\frac{1}{b}C_{\epsilon}(0)}(\mathcal{L}_{b}(w)-\mu )-\eta\frac{C^{\prime}\bigl{(}\frac{\|\mathbf{d}\|^{2}}{2}\bigr{)}}{C^{\prime} (0)+\frac{1}{b}C^{\prime}_{\epsilon}(0)}\|\mathcal{L}_{b}(w)\|\] \[=\operatorname*{argmin}_{\eta\in\mathbb{R}}-\frac{C\bigl{(}\frac{ \|\mathbf{d}\|^{2}}{2})}{C(0)}-\eta\frac{C^{\prime}\bigl{(}\frac{\|\mathbf{d} \|^{2}}{2}\bigr{)}}{C^{\prime}(0)+\frac{1}{b}C^{\prime}_{\epsilon}(0)}\frac{C( 0)}{C(0)+\frac{1}{b}C_{\epsilon}(0)}\frac{\|\mathcal{L}_{b}(w)\|}{\mu- \mathcal{L}_{b}(w)},\]

where we divided the term by \(\frac{C(0)}{C(0)+\frac{1}{b}C_{\epsilon}(0)}\frac{1}{\mu-\mathcal{L}_{b}(w)}\geq 0\) to obtain the last equation. The claim follows by definition of \(\eta^{*}(\Theta)\) and our redefinition of \(\Theta\). 

## Appendix E Extensions

In this section we present a few possible extensions to Theorem 4.2, which are all composable, i.e. it is possible to combine these extensions without any major problems (including S-RFD, i.e. Extension 6.2).

### Geometric anisotropy/Adaptive step sizes

In this section, we discuss the generalization of isotropy to "geometric anisotropies" [50, p. 17], which provide good insights into the inner workings of adaptive learning rates (e.g. AdaGrad [14] and Adam [29]).

**Definition E.1** (Geometric Anisotropy).: We say a random function \(\mathbf{J}\) exhibits a "geometric anisotropy", if there exists an invertible matrix \(A\) such that \(\mathbf{J}(x)=\mathbf{g}(Ax)\) for some isotropic random function \(\mathbf{g}\).

This implies that the expectation of \(\mathbf{J}\) is still constant (\(\mathbb{E}[\mathbf{J}(x)]=\mathbb{E}[\mathbf{g}(Ax)]=\mu\)) and the covariance function of \(\mathbf{J}\) is given by

\[\operatorname{Cov}(\mathbf{J}(x),\mathbf{J}(y))=\operatorname{Cov}(\mathbf{g} (Ax),\mathbf{g}(Ay))=C\Bigl{(}\frac{\|A(x-y)\|^{2}}{2}\Bigr{)}=C\Bigl{(}\frac{ \|x-y\|^{2}_{A^{T}A}}{2}\Bigr{)}\] (24)

where \(\|\cdot\|_{\Sigma}\) is the norm induced by \(\langle x,y\rangle_{\Sigma}:=\langle x,\Sigma y\rangle\) for some strictly positive definite matrix \(\Sigma=A^{T}A\). Here (24) characterizes the set of random functions with a geometric anisotropy in the Gaussian case, because for an \(\mathbf{J}\) with such a covariance we can always obtain an isotropic \(\mathbf{g}\) by \(\mathbf{g}(x):=\mathbf{J}(A^{-1}x)\). This is the whitening transformation we suggest looking for in order to ensure isotropy in the context of scale invariance (Section 2).

An important observation is, that Theorem F.2 implies that \(\mathbf{J}\) is still stationary, so the distribution of \(\mathbf{J}\) is still invariant to translations. If stationarity is a problem, this is therefore not the solution. But geometric anisotropies are a beautiful model to explain preconditioning and adaptive step sizes. For this, we first determine the RFD steps.

**Extension E.2** (RFD steps under geometric anisotropy).: _Let \(\mathbf{J}\) be a Gaussian random function which exhibits a "geometric anisotropy" \(A\) and is based on an isotropic random function \(\mathbf{g}\sim\mathcal{N}(\mu,C)\). Then the RFD steps are given by_

\[\eta^{*}\frac{\Sigma^{-1}\nabla\mathbf{J}(w)}{\|\Sigma^{-1}\nabla\mathbf{J}(w) \|_{\Sigma}}=\operatorname*{argmin}_{\mathbf{d}}\mathbb{E}[\mathbf{J}(w- \mathbf{d})\mid\mathbf{J}(w),\nabla\mathbf{J}(w)]\]

_with_

\[\eta^{*}=\operatorname*{argmin}_{\eta}q_{\Theta}(\eta)\quad\text{ where}\quad\Theta=\frac{\|\Sigma^{-1}\nabla\mathbf{J}(w)\|_{\Sigma}}{\mu-\mathbf{J}(w)}.\]

Proofsketch.: There are two ways to see this. Either we apply scale invariance (Advantage 2.3) directly to translate the steps on \(\mathbf{g}\) into steps on \(\mathbf{J}\). Alternatively one can manually retrace the steps of the proof. Details in Subsection E.1.1 

The step direction is therefore

\[\Sigma^{-1}\nabla\mathbf{J}(x)\]

and \(\Sigma^{-1}\) acts as a preconditioner. So how would one obtain \(\Sigma\)? As it turns out the following holds true (by Lemma G.2)

\[\mathbb{E}[\nabla\mathbf{J}(w)\nabla\mathbf{J}(w)^{T}]=A^{T}\mathbb{E}[\nabla \mathbf{g}(w)\nabla\mathbf{g}(w)^{T}]A=A^{T}(-C^{\prime}(0)\mathbb{I})A=-C^{ \prime}(0)\Sigma\]

In their proposal of the first "adaptive" method, AdaGrad, Duchi et al. [14] suggest to use the matrix

\[G_{t}=\sum_{k=1}^{t}\nabla\mathbf{J}(w_{k})\nabla\mathbf{J}(w_{k})^{T},\]

which is basically already looking like an estimation method of \(\Sigma\). They then restrict themselves to \(\operatorname{diag}(G_{t})\) due to the computational costs of a full matrix inversion. This results in entry-wise ("adaptive") learning rates. Later adaptive methods like RMSProp [22], AdaDelta [57] and in particular Adam [29] replace this sum with an exponential mean estimate, i.e. in the case of Adam the decay rate \(\beta_{2}\) is used to get an exponential moving average

\[v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})\operatorname{diag}(\nabla\mathbf{J}(w_{t })\nabla\mathbf{J}(w_{t})^{T})=\beta_{2}v_{t-1}+(1-\beta_{2})(\nabla\mathbf{ J}(w_{t}))^{2}.\]

They then take the expectation

\[\mathbb{E}[v_{t}]=\mathbb{E}\Big{[}(1-\beta_{2})\sum_{k=1}^{t}\beta_{2}^{t-k} \nabla\mathbf{J}(w_{k})^{2}\Big{]}=\mathbb{E}\Big{[}(1-\beta_{2})\sum_{k=1}^{ t}\beta_{2}^{t-k}\nabla\mathbf{J}(w_{k})^{2}\Big{]}=(1-\beta_{2}^{t})\underbrace{ \mathbb{E}[\nabla\mathbf{J}(x_{t})^{2}]}_{\propto\operatorname{diag}(\Sigma)}\]

So \(\hat{v}_{t}=v_{t}/(1-\beta_{2}^{t})\) in the Adam optimizer is essentially an estimator for \(\operatorname{diag}(\Sigma)\). It is noteworthy, that Kingma and Ba [29] already used the expectation symbol. This is despite the fact, that they did not yet model the optimization objective \(\mathbf{J}\) as a random function.

We can not yet explain why they then use the square root of their estimate \(\operatorname{diag}(\Sigma)^{-1/2}\) instead of \(\operatorname{diag}(\Sigma)^{-1}\) itself. This might have something to do with the fact that the estimation of \(G_{t}\) happens online and the \(\mathbf{J}(w_{k})\) are therefore highly correlated. Another reason might be that the inverse of an estimator has different properties than the estimator itself. Finally, the fact that only the diagonal is used might also be the reason, if the preconditioner \(\operatorname{diag}(\Sigma)^{-1/2}\) is simply better when we restrict ourselves to diagonal matrices.

#### e.1.1 Proof of Extension e.2

Since the application of scale invariance provides no intuition, we provide a proof which retraces some of the steps of the original proof.

Recall, that for an isotropic random function \(\mathbf{g}\) we have the stochastic Taylor approximation

\[\mathbb{E}[\mathbf{g}(w-\mathbf{d})\mid\mathbf{g}(x),\nabla\mathbf{g}(x)]=\mu+ \frac{C\big{(}\frac{\|\mathbf{d}\|^{2}}{2}\big{)}}{C(0)}(\mathbf{g}(w)-\mu)+ \frac{C^{\prime}\big{(}\frac{\|\mathbf{d}\|^{2}}{2}\big{)}}{C^{\prime}(0)} \langle\mathbf{d},\nabla\mathbf{g}(w)\rangle\]

This implies for a random function with geometric anisotropy \(\mathbf{J}(w)=\mathbf{g}(Aw)\) that

\[\mathbb{E}[\mathbf{J}(w-\mathbf{d})\mid\mathbf{J}(w),\nabla \mathbf{J}(w)] =\mathbb{E}[\mathbf{g}(A(w-\mathbf{d}))\mid\mathbf{g}(Aw),\nabla \mathbf{g}(Aw)]\] \[=\mu+\frac{C\big{(}\frac{\|\mathbf{d}\|^{2}}{2}\big{)}}{C(0)}( \mathbf{g}(Aw)-\mu)-\frac{C^{\prime}\big{(}\frac{\|\mathbf{d}\|^{2}}{2}\big{)} }{C^{\prime}(0)}\langle\mathbf{d},\nabla\mathbf{g}(Aw)\rangle\] \[=\mu+\frac{C\big{(}\frac{\|\mathbf{d}\|^{2}_{\Sigma}}{2}\big{)}} {C(0)}(\mathbf{J}(w)-\mu)-\frac{C^{\prime}\big{(}\frac{\|\mathbf{d}\|^{2}_{ \Sigma}}{2}\big{)}}{C^{\prime}(0)}\langle\mathbf{d},\underbrace{A^{T}\nabla \mathbf{g}(Aw)}_{=\nabla\mathbf{J}(w)}\rangle\]

with \(\Sigma:=A^{T}A\). As in the original proof, we now optimize over the direction first, while keeping the step size constant, although we now fix the step size with regard to the norm \(\|\cdot\|_{\Sigma}\) (which basically means that we still do the optimization in the isotropic space). Note that

\[\max_{\mathbf{d}}\langle\mathbf{d},\nabla\mathbf{J}(x)\rangle\quad\text{s.t.} \quad\|\mathbf{d}\|_{\Sigma}=\eta\]

is equivalent to

\[\max_{\mathbf{d}}\langle\mathbf{d},\Sigma^{-1}\nabla\mathbf{J}(x)\rangle_{ \Sigma}\quad\text{s.t.}\quad\|\mathbf{d}\|_{\Sigma}=\eta\]

which is solved by

\[\pm\eta\frac{\Sigma^{-1}\nabla\mathbf{J}(x)}{\|\Sigma^{-1}\nabla\mathbf{J}(x) \|_{\Sigma}}\]

The remainder of the proof is exactly the same as in the original.

### Conservative RFD

In the first paragraph of Section 2 we motivated the relation between RFD and classical optimization with the observation, that gradient descent is the minimizer of a regularized first order Taylor approximation

\[\tfrac{1}{L}\nabla\mathbf{J}(w)=\operatorname*{argmin}_{\mathbf{d}}T[\mathbf{ J}(w-\mathbf{d})\mid\mathbf{J}(w),\nabla\mathbf{J}(w)]+\tfrac{L}{2}\|w\|^{2}.\]

This regularized Taylor approximation is in fact an upper bound on our function under the \(L\)-smoothness assumption [38], i.e.

\[\mathbf{J}(w-\mathbf{d})\leq T[\mathbf{J}(w-\mathbf{d})\mid\mathbf{J}(w), \nabla\mathbf{J}(w)]+\tfrac{L}{2}\|\mathbf{d}\|^{2}\]

An improvement on of this upper bound compared to \(\mathbf{J}(x)\) therefore guarantees an improvement of the loss. This guarantee was lost with the conditional expectation (on purpose, as we wanted to consider the average case). Losing this guarantee also makes convergence proofs more difficult as they typically make use of this improvement. In view of the confidence intervals of Figure 1, it is natural to ask for a similar upper bound in the random setting, where this can only be the top of an confidence interval. This is provided in the following theorem

**Lemma E.3** (An \(\gamma\)-upper bound).: _We have_

\[\mathbb{P}\Big{(}\mathbf{J}(w-\mathbf{d})\leq\mathbb{E}[\mathbf{J}(w-\mathbf{ d})\mid\mathbf{J}(w),\nabla\mathbf{J}(w)]+\rho_{\gamma}(\|\mathbf{d}\|^{2}) \Big{)}\geq\gamma\]

_for \(\rho_{\gamma}(\eta^{2}):=\Phi^{-1}(\gamma)\sigma(\eta^{2})\) with_

\[\sigma^{2}(\eta^{2}):=C(0)-\frac{C\big{(}\frac{\eta^{2}}{2}\big{)}^{2}}{C(0)}- \frac{C^{\prime}\big{(}\frac{\eta^{2}}{2}\big{)}^{2}}{-C^{\prime}(0)}\eta^{2}\]

_where \(\Phi\) is the cumulative distribution function (cdf) of the standard normal distribution._Proof.: Note that the conditional variance is with the usual argument about the covariance matrices (cf. the proof of Thoerem 4.2) using Lemma G.2 and an application of Theorem G.1 given by

\[\sigma^{2}(\|w\|^{2}):=\operatorname{Var}[\mathbf{J}(w-\mathbf{d})\mid\mathbf{J} (w),\nabla\mathbf{J}(w)]=C(0)-\frac{C(\frac{\|\mathbf{d}\|^{2}}{2})^{2}}{C(0)}- \frac{C^{\prime}(\frac{\|\mathbf{d}\|^{2}}{2})^{2}}{-C^{\prime}(0)}\|\mathbf{d }\|^{2}.\]

Since the conditional distribution is normal (by Theorem G.1), we have

\[\frac{\mathbf{J}(w-\mathbf{d})-\mathbb{E}[\mathbf{J}(w-\mathbf{d})\mid\mathbf{ J}(w),\nabla\mathbf{J}(w)]}{\sigma(\|w\|^{2})}\sim\mathcal{N}(0,1).\]

But this implies the claim

\[\mathbb{P}\Big{(}\mathbf{J}(w-\mathbf{d})\leq\mathbb{E}[\mathbf{ J}(w-\mathbf{d})\mid\mathbf{J}(w),\nabla\mathbf{J}(w)]+\rho_{\gamma}(\| \mathbf{d}\|^{2})\Big{)}\] \[=\mathbb{P}\Big{(}\frac{\mathbf{J}(w-\mathbf{d})-\mathbb{E}[ \mathbf{J}(w-\mathbf{d})\mid\mathbf{J}(w),\nabla\mathbf{J}(w)]}{\sigma(\|w\| ^{2})}\leq\Phi^{-1}(\gamma)\Big{)}\] \[=\Phi(\Phi^{-1}(\gamma))=\gamma.\]

To avoid the Gaussian assumption, one could apply the Markov inequality instead, or another applicable concentration inequality. 

Using this upper bound, we obtain a natural conservative extension of RFD

**Extension E.4** (\(\gamma\)-conservative RFD).: _Let \(\mathbf{J}\sim\mathcal{N}(\mu,C)\) and \(\rho_{\gamma}(\eta^{2})=\Phi^{-1}(\gamma)\sigma(\eta^{2})\), where \(\sigma\) is the conditional standard deviation as defined in Lemma E.3. Then the conservative RFD step direction is given by_

\[\eta^{*}\frac{\nabla\mathbf{J}(w)}{\|\nabla\mathbf{J}(w)\|}=\operatorname*{ argmin}_{\mathbf{d}}\mathbb{E}[\mathbf{J}(w-\mathbf{d})\mid\mathbf{J}(w), \nabla\mathbf{J}(w)]+\rho_{\gamma}(\|\mathbf{d}\|^{2})\]

_and the \(\gamma\)-conservative RFD step size is given by_

\[\eta^{*}=\operatorname*{argmin}_{\eta}\frac{C\big{(}\frac{\eta^{2}}{2}\big{)}} {C(0)}(\mathbf{J}(w)-\mu)-\eta\frac{C^{\prime}\big{(}\frac{\eta^{2}}{2}\big{)} }{C^{\prime}(0)}\|\nabla\mathbf{J}(w)\|+\rho_{\gamma}(\eta^{2}).\]

Proof.: The proof is the same as in Theorem 4.2 with Lemma 4.1 replaced by Lemma E.3. 

Taking multiple steps should generally have an averaging effect, so we expect faster convergence for almost risk neutral minimization of the conditional expectation (i.e. \(\gamma\approx\frac{1}{2}\)). Here \(\gamma\) is a natural parameter to vary conservatism. In a software implementation it might be a good idea to call this parameter 'conservatism' and rescale it to be in \([0,1]\) instead of \([\frac{1}{2},1]\). But formulas look cleaner with \(\gamma\).

In Bayesian optimization it is much more common to reverse this approach and minimize a lower confidence bound ('conservatism' \(<0\) or \(\gamma<\frac{1}{2}\)) in order to encourage exploration. But since RFD is forgetful, this is not a good idea for RFD.

_Remark E.5_ (Conservative RFD coincides asymptotically with RFD in high dimension).: While conservative RFD might seem like a good approach to fix the instability of RFD under the isotropy assumption on some optimization problems, the variance generally vanishes in high dimension [see 5] and conservative RFD coincides asymptotically with RFD. We therefore believe that the underlying issue is not an overly risk-affine algorithm but rather that distributional assumptions, in particular the stationarity assumption, are violated when instabilities occur (cf. Section F).

Nevertheless, conservative RFD might be a good approach for lower dimensional, risk-sensitive applications.

### Beyond the Gaussian assumption

In this section we sketch how the extension beyond the Gaussian case using the "best linear unbiased estimator" BLUE [e.g. 27, ch. 7] works.

For this we recapitulate what a BLUE is. A **linear estimator**\(\hat{Y}\) of \(Y\) using \(X_{1},\ldots,X_{n}\) is of the form

\[\hat{Y}\in\operatorname{span}\{X_{1},\ldots,X_{n}\}+\mathbb{R}.\]

The set of **unbiased linear estimators** is defined as

\[\operatorname{LUE}=\operatorname{LUE}\left[Y\mid X_{1},\ldots,X_ {n}\right] =\{\hat{Y}\in\operatorname{span}\{X_{1},\ldots,X_{n}\}+\mathbb{R} :\mathbb{E}[\hat{Y}]=\mathbb{E}[Y]\}\] (25) \[=\{\hat{Y}+\mathbb{E}[Y]:\hat{Y}\in\operatorname{span}\{X_{1}- \mathbb{E}[X_{1}],\ldots,X_{n}-\mathbb{E}[X_{n}]\}\}.\]

And the BLUE is the **best linear unbiased estimator**, i.e.

\[\operatorname{BLUE}[Y\mid X_{1},\ldots,X_{n}]:=\operatorname*{argmin}_{\hat{Y }\in\operatorname{LUE}}\mathbb{E}[\|\hat{Y}-Y\|^{2}].\] (26)

Other risk functions to minimize are possible, but this is the usual one.

**Lemma E.6**.: _If \(X,Y_{1},\ldots,Y_{n}\) are multivariate normal distributed, then we have_

\[\operatorname{BLUE}[Y\mid X_{1},\ldots,X_{n}] =\mathbb{E}[Y\mid X_{1},\ldots,X_{n}]\] \[\bigg{(} =\operatorname*{argmin}_{\hat{Y}\in\{f(X_{1},\ldots,X_{n}):f\text{ meas.}\}}\mathbb{E}[\|Y-\hat{Y}\|^{2}]\bigg{)}.\]

Proof.: We observe that the conditional expectation of Gaussian random variables is linear (Theorem G.1). So as a linear function its \(L^{2}\) risk must be larger or equal to that of the BLUE. And as an \(L^{2}\) projection [30, Cor. 8.17] the conditional expectation was already optimal. 

If we now replace the conditional expectation with the BLUE, then all our theory remains the same because the result in Theorem G.1 remains the BLUE for general distributions [27]. Instead of minimizing

\[\min_{\mathbf{d}}\mathbb{E}[\mathbf{J}(w-\mathbf{d})\mid\mathbf{J}(w),\nabla \mathbf{J}(w)]\]

we can therefore always minimize

\[\min_{\mathbf{d}}\operatorname{BLUE}[\mathbf{J}(w-\mathbf{d})\mid\mathbf{J}(w ),\nabla\mathbf{J}(w)]\]

without the Gaussian assumption and all our results can be translated to this case. The reader only needs to replace all mentions of Theorem G.1 with the BLUE equivalent and replace all "dependence" claims with "uncorrelated".

## Appendix F Input invariance

In this section we generalize the notion of isotropy to non-stationary isotropy and discuss why we believe this generalization is necessary. Recall that we motivated isotropy as an invariant distribution with regard to isometric transformations of the input. In particular its distribution stays invariant with regard to translations (also known as stationarity), which we do not believe plausible for cost functions, because the cost at zero \(\mathbf{J}(0)\) behaves fundamentally different from the cost of any other parameter vector.

In the following we will therefore generalize this notion to general input invariant distributions. And we will discuss their applicability to machine learning after we characterize the named categories.

**Definition F.1** (Input Invariance).: A random function \(\mathbf{f}\) is \(\Phi\)-input invariant, if 5

Footnote 5: The input to a random function is somewhat ambiguous since it is a random variable, i.e. function from the probability space \(\Omega\) into function space, so its first input should be \(\omega\in\Omega\). Formally, the definition should therefore be: For all measurable sets of functions \(A\)

\[\mathbb{P}_{\mathbf{f}}(\phi_{*}^{-1}(A))=\mathbb{P}_{\mathbf{f}}(A)\quad \forall\phi\in\Phi\]

 where \(\phi_{*}:f\mapsto f\circ\phi\) denotes the pullback. But this is less helpful for an intuitive understanding.

* If \(\Phi\) is the set of _isometries_, we call \(\mathbf{f}\) (stationary) **isotropic**.
* If \(\Phi\) is the set of _translations_, we call \(\mathbf{f}\)**stationary**.
* If \(\Phi\) is the set of _linear isometries_, we call \(\mathbf{f}\)**non-stationary isotropic**.

We further say a random function \(\mathbf{f}\) is \(n\)-weakly \(\Phi\)-input invariant, if for all \(\phi\in\Phi\), all \(k\leq n\) and all \(x_{i}\)

\[\mathbb{E}[\mathbf{f}(\phi(x_{1}))\cdot\dots\cdot\mathbf{f}(\phi(x_{k}))]= \mathbb{E}[\mathbf{f}(x_{1})\cdot\dots\cdot\mathbf{f}(x_{k})].\]

Since second moments fully determine Gaussian distributions, \(2\)-weakly input invariance is special, because it is equivalent to full input invariance in the Gaussian case. So an omitted \(n\) equals \(2\). "Weakly isometry invariant" naturally becomes "weakly isotropic", etc.

While stationary and stationary isotropic random functions are well known [e.g. 44, 1], we are not aware of research on non-stationary isotropy although we doubt the concept is new. It turns out that the different notions of input isometry have simple characterizations in terms of the covariance functions. We present these in Theorem F.2 of which the stationary isotropic and stationary case are already well known.

**Theorem F.2** (Characterization of Weak Input Invariances).: _Let \(\mathbf{f}:\mathbb{R}^{d}\to\mathbb{R}\) be a random function, then \(\mathbf{f}\) is_

1. _weakly stationary, if and only if there exists_ \(\mu\in\mathbb{R}\) _and function_ \(C:\mathbb{R}^{d}\to\mathbb{R}\) _such that for all_ \(x,y\)__ \[\mu_{\mathbf{f}}(x)=\mu,\qquad\mathcal{C}_{\mathbf{f}}(x,y)=C(x-y).\]
2. _weakly non-stationary isotropic, if and only if there exist functions_ \(\mu:\mathbb{R}_{\geq 0}\to\mathbb{R}\) _and_ \(\kappa:D\to\mathbb{R}\) _with_ \(D=\{\lambda\in\mathbb{R}_{\geq 0}^{2}\times\mathbb{R}:|\lambda_{3}|\leq 2 \sqrt{\lambda_{1}\lambda_{2}}\}\subseteq\mathbb{R}^{3}\)_. such that for all_ \(x,y\)__ \[\mu_{\mathbf{f}}(x) =\mu\big{(}\tfrac{\|x\|^{2}}{2}\big{)}\] \[\mathcal{C}_{\mathbf{f}}(x,y) =\kappa\big{(}\tfrac{\|x\|^{2}}{2},\tfrac{\|y\|^{2}}{2},\langle x,y\rangle\big{)}\]
3. _weakly stationary isotropic, if and only if there exists_ \(\mu\in\mathbb{R}\) _and a function_ \(C:\mathbb{R}_{\geq 0}\to\mathbb{R}\) _such that for all_ \(x,y\)__ \[\mu_{\mathbf{f}}(x)=\mu,\qquad\mathcal{C}_{\mathbf{f}}(x,y)=C\big{(}\tfrac{\|x -y\|^{2}}{2}\big{)}\]

Proof.: The proof essentially follow as a corollary from a characterization of isometries (Proposition F.4). For details see Subsec F.2. 

Non-stationary isotropy is therefore a generalization of stationary isotropy. It allows the zero parameter vector to have special meaning because the distribution is only invariant to linear isometries (i.e. rotations and reflections) which keep the zero in place.

It is important to highlight, that a geometric anisotropy (Section E.1) retains stationarity, while breaking non-stationary isotropy. A similar geometric generalization could also be applied to non-stationary isotropy.

Another important observation is the fact, that non-stationary isotropy coincides with stationary isotropy on the sphere. I.e. when \(\|x\|\) and \(\|y\|\) are constant, the function

\[\kappa\big{(}\tfrac{\|x\|^{2}}{2},\tfrac{\|y\|^{2}}{2},\tfrac{\|x-y\|^{2}}{2} \big{)}\]

only depends on \(\|x-y\|\) and the mean is also constant. In other words, we have stationary isotropy on the sphere.

Isotropy might therefore 'get by' as an assumption in machine learning, as parameters are typically initialized on the sphere. This is because Glorot initialization [18] samples parameter entries independently, so their squared norm

\[\|w\|^{2}=\sum_{i=1}^{d}(w^{(i)})^{2}\]is a sum of independent random variables which are normalized such that a law of large numbers applies. Up to small variance their lengths are therefore all the same, and are placed on a sphere at this radius.

If we leave this sphere, this equivalence stops being true. Weight normalization [46], batch normalization [25], weight decay [e.g. 19] or equivalently \(L^{2}\) regularization, etc. might all contribute to keep this assumption intact.

But in the following section we will see, that even simple linear regressions considered by researches investigating the average case behavior on quadratic functions [e.g. 58, 43, 33, 12, 9, 40, 41], require non-stationary isotropy. Moreover the covariance kernels suggested by investigations into random neuronal networks [e.g. 54, 8] are also non-stationary isotropic but not stationary isotropic.

### Random linear regression

In this section, we determine the distribution of the cost function induced by a simple linear regression. For this we define the mean squared sample loss

\[\ell_{i}(w)=(Y-f_{w}(X))^{2},\]

where the random data \(X\) is mapped by the true relationship \(\mathbf{f}\) to labels \(Y=\mathbf{f}(X)\) and

\[f_{w}(x)=\langle x,w\rangle\]

is a linear model. If the true relationship \(\mathbf{f}\) is also a random linear function \(\mathbf{f}(x)=\langle\theta,x\rangle\) with random signal \(\theta\sim\mathcal{N}(0,\mathbb{I})\) independent of input \(X\sim\mathcal{N}(0,\mathbb{I})\), then the cost function is given by

\[\mathbf{J}(w) =\mathbb{E}[\ell_{i}(w)\mid\mathbf{f}]=\mathbb{E}[\langle\theta- w,X\rangle^{2}\mid\theta]\] \[=(\theta-w)^{T}\mathbb{E}[XX^{T}](\theta-w)\] \[=\|\theta-w\|^{2}\]

**Lemma F.3**.: _The expectation and covariance of \(\mathbf{J}\) are given by_

\[\mathbb{E}[\mathbf{J}(w)] =\text{const.}+\|w\|^{2}\] \[\operatorname{Cov}(\mathbf{J}(w),\mathbf{J}(\tilde{w})) =\text{const.}+4\langle w,\tilde{w}\rangle\]

_In particular, the cost \(\mathbf{J}\) is non-stationary isotropic, but not stationary isotropic._

Proof.: Its expectation is given by

\[\mathbb{E}[\mathbf{J}(w)] =\mathbb{E}[\|\theta-w\|^{2}]=\mathbb{E}[\|\theta\|^{2}]-2 \langle\underbrace{\mathbb{E}[\theta]}_{=0},w\rangle+\|w\|^{2}\] \[=\text{const.}+\|w\|^{2}\]

In particular it is not constant, but dependent on \(\|w\|^{2}\), which means that we do not have stationary isotropy. But there is still hope for non-stationary isotropy, and this is essentially true as can be seen by calculating

\[\operatorname{Cov}(\mathbf{J}(w),\mathbf{J}(\tilde{w})) =\mathbb{E}\Big{[}(\mathbf{J}(w)-\mathbb{E}[\mathbf{J}(w)])( \mathbf{J}(\tilde{w})-\mathbb{E}[\mathbf{J}(\tilde{w})])\Big{]}\] \[=\mathbb{E}\Big{[}(\|\theta\|^{2}-\mathbb{E}\|\theta\|^{2}-2 \langle\theta,w\rangle)(\|\theta\|^{2}-\mathbb{E}\|\theta\|^{2}-2\langle \theta,\tilde{w}\rangle)\Big{]}\] \[=\operatorname{Var}(\|\theta\|^{2}-\mathbb{E}\|\theta\|^{2})\] \[\quad-2\mathbb{E}[(\|\theta\|^{2}-\mathbb{E}\|\theta\|^{2}) \langle\theta,w\rangle]\] \[\quad-2\mathbb{E}[(\|\theta\|^{2}-\mathbb{E}\|\theta\|^{2}) \langle\theta,\tilde{w}\rangle]\] \[\quad+4w^{T}\mathbb{E}[\theta\theta^{T}]\tilde{w}\] \[=\operatorname{Var}(\|\theta\|^{2}-\mathbb{E}\|\theta\|^{2})+4 \langle w,\tilde{w}\rangle\] \[=\text{const.}+4\langle w,\tilde{w}\rangle\]

because the terms in the middle are zero, e.g.

\[\mathbb{E}[(\|\theta\|^{2}-\mathbb{E}\|\theta\|^{2})\langle\theta,w\rangle]= \langle\underbrace{\mathbb{E}[\|\theta\|^{2}\theta]}_{=0},w\rangle-\mathbb{E} \|\theta\|^{2}\langle\underbrace{\mathbb{E}[\theta]}_{=0},w\rangle\]

where the entries of \(\mathbb{E}[\|\theta\|^{2}\theta]\) are zero, because of independence and first moments being zero and third moments being zero.

### Proof of Theorem F.2

**Proposition F.4** (Characterizing isometries).: _Let \(\mathcal{X}\) be a vectorspace and \(x_{i},y_{i}\in\mathcal{X}\) for \(i=1,\ldots,n\), then the following pairs of statements are equivalent_

1. 1. \(x_{i}-x_{j}=y_{i}-y_{j}\) _for all_ \(i,j\)__ 2. _there exists a_ _translation_ \(\phi\) _with_ \(\phi(x_{i})=y_{i}\) _for all_ \(i\)_._

_In the remainder we further assume \(\mathcal{X}\) to be a Hilbertspace,_

1. \(\|x_{i}\|=\|y_{i}\|\) _and_ \(\|x_{i}-x_{j}\|=\|y_{i}-y_{j}\|\) _for all_ \(i,j\)__ 2. _there exists a_ _linear isometry_ \(\phi\) _with_ \(\phi(x_{i})=y_{i}\) _for all_ \(i\)_._
2. \(\|x_{i}-x_{j}\|=\|y_{i}-y_{j}\|\) _for all_ \(i,j\)__ 3. _there exists an_ _(affine) isometry_ \(\phi\) _with_ \(\phi(x_{i})=y_{i}\) _for all_ \(i\)_._

Proof.: (1a)\(\Rightarrow\) (1b)**:**: we define

\[\phi(x):=x+(y_{0}-x_{0}),\]

which implies

\[\phi(x_{i})=x_{i}-x_{0}+y_{0}=(y_{i}-y_{0})+y_{0}=y_{i}.\]

(1b)\(\Rightarrow\) (1a)**:**: Let \(\phi(x)=x+c\) for some \(c\). Then we immediately have

\[y_{i}-y_{j}=\phi(x_{i})-\phi(x_{j})=x_{i}+c-(x_{j}+c)=x_{i}-x_{j}.\]

(2a)\(\Rightarrow\) (2b)**:**: By the polarization formula, for all \(i,j\)

\[\langle x_{i},x_{j}\rangle=\frac{\|x_{i}\|^{2}+\|y_{i}\|^{2}-\|x_{i}-x_{j}\|^ {2}}{2}=\langle y_{i},y_{j}\rangle.\]

We apply the Gram-Schmidt orthonormalization procedure to both \(x_{i}\) and \(y_{i}\) such that

\[U_{k_{n}}=\mathrm{span}(u_{1},\ldots,u_{k_{n}})=\mathrm{span}(x_{1},\ldots,x_ {n})\]

for orthonormal \(u_{i}\) where we skip \(x_{m}\) if it is already in \(U_{k_{m-1}}\) (resulting in \(k_{m}=k_{m-1}\)), and similarly

\[V_{k_{n}}=\mathrm{span}(v_{1},\ldots,v_{k_{n}})=\mathrm{span}(y_{1},\ldots,y_ {n}).\]

Since this procedure only uses scalar products, we inductively get

\[\langle x_{k},u_{j}\rangle=\langle y_{k},v_{j}\rangle\quad\forall k,j\]

We now extend \(u_{i}\) and \(v_{i}\) to orthonormal basis of \(\mathcal{X}\) and define the linear mapping by its behavior on the basis elements \(\phi:u_{i}\mapsto v_{i}\). Mapping an orthonormal basis to an orthonormal basis is an isometry and we have

\[\phi(x_{k}) =\phi\Bigl{(}\sum_{j=1}^{k}\langle x_{k},u_{j}\rangle u_{j}\Bigr{)}\] \[=\sum_{j=1}^{k}\langle x_{k},u_{j}\rangle\phi(u_{j})=\sum_{j=1}^{k} \langle y_{k},v_{j}\rangle v_{j}\] \[=y_{k}.\]

(2b)\(\Rightarrow\) (2a)**:**: Isometries preserve distances by definition. This implies \(\|x_{i}-x_{j}\|=\|y_{i}-y_{j}\|\). And linear functions map \(0\) to \(0\), so we have

\[\|x_{i}\|=\|x_{i}-0\|=\|\phi(x_{i})-\phi(0)\|=\|y_{i}\|.\]

(3a)\(\Rightarrow\) (3b)**:**: We define

\[\tilde{x}_{i}=x_{i}-x_{0}\]

and similarly for \(y\). In particular, \(\tilde{x}_{0}=\tilde{y}_{0}=0\). Since \(\tilde{x}_{i}\) and \(\tilde{y}_{i}\) satisfy the requirements of 2, there exists a linear isometry \(\tilde{\phi}\) with \(\tilde{\phi}(\tilde{x}_{i})=\tilde{y}_{i}\). Then the isometry

\[\phi:x\mapsto\tilde{\phi}(x-x_{0})+y_{0}\]

does the job.

(3b) \(\Rightarrow\) (3a): This is precisely the distance preserving property of Isometries. 

**Theorem F.2** (Characterization of Weak Input Invariances).: _Let \(\mathbf{f}:\mathbb{R}^{d}\to\mathbb{R}\) be a random function, then \(\mathbf{f}\) is_

1. _weakly stationary, if and only if there exists_ \(\mu\in\mathbb{R}\) _and function_ \(C:\mathbb{R}^{d}\to\mathbb{R}\) _such that for all_ \(x,y\)__ \[\mu_{\mathbf{f}}(x)=\mu,\qquad\mathcal{C}_{\mathbf{f}}(x,y)=C(x-y).\]
2. _weakly non-stationary isotropic, if and only if there exist functions_ \(\mu:\mathbb{R}_{\geq 0}\to\mathbb{R}\) _and_ \(\kappa:D\to\mathbb{R}\) _with_ \(D=\{\lambda\in\mathbb{R}_{\geq 0}^{2}\times\mathbb{R}:|\lambda_{3}|\leq 2 \sqrt{\lambda_{1}\lambda_{2}}\}\subseteq\mathbb{R}^{3}\)_. such that for all_ \(x,y\)__ \[\mu_{\mathbf{f}}(x)=\mu\big{(}\tfrac{\|x\|^{2}}{2}\big{)}\] \[\mathcal{C}_{\mathbf{f}}(x,y)=\kappa\big{(}\tfrac{\|x\|^{2}}{2},\tfrac{\|y\|^{ 2}}{2},\langle x,y\rangle\big{)}\]
3. _weakly stationary isotropic, if and only if there exists_ \(\mu\in\mathbb{R}\) _and a function_ \(C:\mathbb{R}_{\geq 0}\to\mathbb{R}\) _such that for all_ \(x,y\)__ \[\mu_{\mathbf{f}}(x)=\mu,\qquad\mathcal{C}_{\mathbf{f}}(x,y)=C\big{(}\tfrac{\|x- y\|^{2}}{2}\big{)}\]

Proof.: Starting from the mean and covariance function it is easy to check \(2\)-weak non-stationary isotropy. So we only need to check the other direction.

The proof is essentially an application of Prop. F.4. For brevity (and since the other two results are well known), we will only prove the weakly non-stationary isotropic case (the other two cases can be proven with minor adjustments to the proof).

Without loss of generality, we will find the slightly different representation

\[\mathbb{E}[\mathbf{f}_{d}(x)]=\tilde{\mu}(\|x\|)\quad\text{and}\quad\mathcal{ C}_{\mathbf{f}_{d}}(x,y)=\tilde{\kappa}(\|x\|,\|y\|,\langle x,y\rangle),\]

where the domain of \(\tilde{\kappa}\) is given by \(\tilde{D}=\{\lambda\in\mathbb{R}_{\geq 0}^{2}\times\mathbb{R}:|\lambda_{3}| \leq\lambda_{1}\lambda_{2}\}\). The representation of the theorem is then equivalent by a change to

\[\mu(\lambda):=\tilde{\mu}\big{(}\tfrac{\lambda^{2}}{2}\big{)}\quad\text{and} \quad\kappa(\lambda_{1},\lambda_{2},\lambda_{2}):=\tilde{\kappa}\big{(}\tfrac {\lambda_{1}^{2}}{2},\tfrac{\lambda_{2}^{2}}{2},\lambda_{3}\big{)}.\]

First we want to find \(\mu\). Let \(v\) be some vector (w.l.o.g. \(\|v\|=1\)). Then we define

\[\mu(r):=\mathbb{E}[\mathbf{f}_{d}(rv)]\]

Now we need to show that this definition of \(\mu\) is an appropriate mean function. For this choose any \(x\in\mathcal{X}\). Then for \(r=\|x\|\) there exists by Prop. F.4 (2.) a non-stationary isometry \(\phi\) such that \(\phi(x)=rv\) (we use \(n=1\)). With \(1\)-weak non-stationary isotropy of \(\mathbf{f}_{d}\) this implies

\[\mathbb{E}[\mathbf{f}_{d}(x)]=\mathbb{E}[\mathbf{f}_{d}(rv)]=\mu(r)=\mu(\|x\|).\]

Next we need to define \(\kappa(r_{x},r_{y},r_{xy})\). For this, choose two orthonormal vectors \(v,w\). For every \(r=(r_{x},r_{y},r_{xy})\in\tilde{D}\) we define

\[x^{*}(r)=r_{x}v\]

\[y^{*}(r)=\frac{r_{xy}}{r_{x}}v+\sqrt{r_{y}^{2}-\frac{r_{xy}^{2}}{r_{x}^{2}}}w.\]

Where \(r\in\tilde{D}\) ensures \(|r_{xy}|\leq r_{x}r_{y}\) and thus \(r_{y}^{2}-\frac{r_{xy}^{2}}{r_{x}^{2}}\geq 0\). Then we have

\[\|x^{*}(r)\|=r_{x},\quad\|y^{*}(r)\|=r_{y},\quad\text{and}\quad\langle x^{*}(r ),x^{*}(y)\rangle=r_{xy},\] (27)

and define

\[\kappa(r_{x},r_{y},r_{xy}):=\mathcal{C}_{\mathbf{f}_{d}}(x^{*}(r),y^{*}(r)).\]

Again, we need to show that this kernel does the job. For this, choose any \(x,y\in\mathcal{X}\). For

\[r:=(\|x\|,\|y\|,\langle x,y\rangle),\]

which is in \(\tilde{D}\) by the Cauchy-Schwarz inequality, the induced \(x^{*}(r)\) and \(y^{*}(r)\) satisfy by (27)

\[\|x^{*}(r)\|=\|x\|,\quad\|y^{*}(r)\|=\|y\|\quad\text{and}\quad\|x^{*}(r)-y^{*} (r)\|=\|x-y\|.\]

By Prop. F.4 (2.) there therefore exists an isometry \(\phi\) such that \(\phi(x)=x^{*}(r)\) and \(\phi(y)=y^{*}(r)\). By \(2\)-weak input isotropy of \(\mathbf{f}_{d}\) we conclude

\[\mathcal{C}_{\mathbf{f}_{d}}(x,y)\stackrel{{\text{isotrop.}}}{{=}} \mathcal{C}_{\mathbf{f}_{d}}(x^{*}(r),y^{*}(r))\stackrel{{\text{ def.}}}{{=}}\kappa\big{(}\|x\|,\|y\|,\langle x,y\rangle\big{)}.\qed\]Technical

### Conditional Gaussian distribution

For the following well known result we found a tidy proof giving insight into the reason it is true, so we wrote it down for your convenience but do not even expect this particular proof to be new.

**Theorem G.1** (Conditional Gaussian distribution).: _Let \(X\sim\mathcal{N}(\mu,\Sigma)\) be a multivariate Gaussian vector where the covariance matrix is a block matrix of the form_

\[\mu=\begin{bmatrix}\mu_{1}\\ \mu_{2}\end{bmatrix}\quad\text{and}\quad\Sigma=\begin{bmatrix}\Sigma_{11}& \Sigma_{12}\\ \Sigma_{21}&\Sigma_{22}\end{bmatrix},\]

_then assuming \(\Sigma_{11}\) is invertible, the conditional distribution of \(X_{2}\) given \(X_{1}\) is_

\[X_{2}\mid X_{1}\sim\mathcal{N}(\mu_{2|1},\Sigma_{2|1}),\]

_with conditional mean and variance_

\[\mu_{2|1} :=\mu_{2}+\Sigma_{21}\Sigma_{11}^{-1}(X_{1}-\mu_{1})\] \[\Sigma_{2|1} :=\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}.\]

Proof.: Let \(\bar{X}:=X-\mu\) be the centered version of \(X\). There exists some lower triangular matrix \(L\) (even if \(\Sigma\) is only positive semidefinite only not uniquely) such that \(\Sigma=LL^{T}\) (i.e. the Cholesky Decomposition). We can then write without loss of generality

\[X-\mu=:\begin{bmatrix}\bar{X}_{1}\\ \bar{X}_{2}\end{bmatrix}=\begin{bmatrix}L_{11}&0\\ L_{21}&L_{22}\end{bmatrix}\begin{bmatrix}Y_{1}\\ Y_{2}\end{bmatrix}=LY\]

with independent standard normal \(Y_{i}\), i.e. \(Y\sim\mathcal{N}(0,\mathbb{I})\). Since \(\Sigma_{11}\) is invertible, so is \(L_{11}\) and therefore the map from \(Y_{1}\) to \(X_{1}\). Conditioning on \(X_{1}\) is therefore equivalent to conditioning on \(Y_{1}\). But we have

\[X_{2}=\mu_{2}+\bar{X}_{2}=\underbrace{\mu_{2}+L_{21}Y_{1}}_{\text{conditional expectation}}+\underbrace{L_{22}Y_{2}}_{\text{conditional distribution}}\]

So it follows that

\[X_{2}\mid X_{1}\sim\mathcal{N}(\mu_{2|1},\Sigma_{2|1})\]

with

\[\mu_{2|1} :=\mu_{2}+L_{21}Y_{1}\] \[\Sigma_{2|1} :=L_{22}L_{22}^{T}.\]

What is left to do, is find a representation for the \(L_{ij}\) using the block matrices of \(\Sigma\). For this note

\[\Sigma=LL^{T}=\begin{bmatrix}L_{11}L_{11}^{T}&L_{11}L_{21}^{T}\\ L_{21}L_{11}^{T}&L_{22}L_{22}^{T}+L_{21}L_{21}^{T}\end{bmatrix}\]

This implies

\[L_{21}Y_{1}=(L_{21}L_{11}^{T}L_{11}^{-T})(L_{11}^{-1}\bar{X}_{1})=\Sigma_{21} \Sigma_{11}^{-1}(X_{1}-\mu_{1})\]

so we have the desired conditional expectation, and finally

\[L_{22}L_{22}^{T} =\Sigma_{22}-L_{21}L_{21}^{T}\] \[=\Sigma_{22}-\underbrace{L_{21}(L_{11}^{T})(L_{11}^{-T})(L_{11}^ {-1})}_{=\Sigma_{11}^{-1}}\underbrace{L_{11})L_{21}^{T}}_{=\Sigma_{12}}.\qed\]

### Covariance of derivatives

By Swapping integration and differentiation we have for a centered random function \(\mathbf{f}\)

\[\operatorname{Cov}(\partial_{x_{i}}\mathbf{f}(x),\mathbf{f}(y)) =\mathbb{E}[\partial_{x_{i}}\mathbf{f}(x)\mathbf{f}(y)]=\partial _{x_{i}}\mathbb{E}[\mathbf{f}(x)\mathbf{f}(y)]\] \[=\partial_{x_{i}}\mathcal{C}_{\mathbf{f}}(x,y)\]So the covariance of a derivative of \(\mathbf{f}\) with \(\mathbf{f}\) is equal to a partial derivative of the covariance function [more details in 1]. Similarly other covariances can be calculated, e.g.

\[\operatorname{Cov}(\partial_{x_{i}}\mathbf{f}(x),\partial_{y_{i}}\mathbf{f}(y)) =\partial_{x_{i}}\partial_{y_{i}}\mathcal{C}_{\mathbf{f}}(x,y).\]

For this reason the derivatives of the covariance function are interesting as they represent the covariance of derivatives.

Applying this observation to isotropic covariance functions

\[\operatorname{Cov}(\mathbf{f}(x),\mathbf{f}(y))=C\big{(}\tfrac{\|x-y\|^{2}}{2} \big{)}\]

we obtain.

**Lemma G.2** (Covariance of derivatives).: _Let \(\mathbf{f}\sim\mathcal{N}(\mu,C)\) and \(\mathbf{d}=x-y\), then_

\[\begin{array}{c|cc}\operatorname{Cov}&\mathbf{f}(y)&\partial_{j}\mathbf{f}( y)\\ \hline\mathbf{f}(x)&C(\tfrac{\|\mathbf{d}\|^{2}}{2})&-C^{\prime}(\tfrac{\| \mathbf{d}\|^{2}}{2})\langle\mathbf{d},e_{j}\rangle\\ \partial_{i}\mathbf{f}(x)&C^{\prime}(\tfrac{\|\mathbf{d}\|^{2}}{2})\langle \mathbf{d},e_{i}\rangle&-\Big{[}C^{\prime\prime}(\tfrac{\|\mathbf{d}\|^{2}}{2 })\langle\mathbf{d},e_{j}\rangle\langle\mathbf{d},e_{i}\rangle+C^{\prime}( \tfrac{\|\mathbf{d}\|^{2}}{2})\langle e_{j},e_{i}\rangle\Big{]}\end{array}\]

### Constrained linear optimization

Let \(U\) be a vectorspace. We define the projection of a vector \(w\) onto \(U\) by

\[P_{U}(w):=\operatorname*{argmin}_{v\in U}\|v-w\|^{2}\]

**Lemma G.3** (Constrained maximiziation of scalar products).: _For a linear subspace \(U\subseteq\mathbb{R}^{d}\), we have_

\[\max_{\begin{subarray}{c}v\in U\\ \|v\|=\lambda\end{subarray}}\langle v,w\rangle =\lambda\|P_{U}(w)\|\] (28) \[\operatorname*{argmax}_{\begin{subarray}{c}v\in U\\ \|v\|=\lambda\end{subarray}}\langle v,w\rangle =\lambda\frac{P_{U}(w)}{\|P_{U}(w)\|}\] (29)

Before we get to the proof let us note that this immediately results in the following corollary about minimization.

**Corollary G.4** (Constrained minimization of scalar products).: \[\min_{\begin{subarray}{c}v\in U\\ \|v\|=\lambda\end{subarray}}\langle v,w\rangle =-\lambda\|P_{U}(w)\|\] (30) \[\operatorname*{argmin}_{\begin{subarray}{c}v\in U\\ \|v\|=\lambda\end{subarray}}\langle v,w\rangle =-\lambda\frac{P_{U}(w)}{\|P_{U}(w)\|}\] (31)

Proof of Corollary G.4.: The trick is to move one '\(-\)' outside from \(w=-(-w)\)

\[\min_{\begin{subarray}{c}v\in U\\ \|v\|=\lambda\end{subarray}}\langle v,w\rangle=-\max_{\begin{subarray}{c}v\in U \\ \|v\|=\lambda\end{subarray}}\langle v,-w\rangle=-\lambda\|P_{U}(w)\|\]

where we have used in the last equation that the projection is linear (we can move the minus sign out) and the norm removes the inner minus sign. The \(\operatorname*{argmin}\) argument is similar. 

Proof of Lemma G.3.: **Step 1:** We claim that

\[v^{*}=\lambda\frac{P_{U}(w)}{\|P_{U}(w)\|}\]

results in the value \(\langle v^{*},w\rangle=\lambda\|P_{U}(w)\|\).

For this we consider

\[P_{U}(w) =\underset{v\in U}{\operatorname{argmin}}\underbrace{\|v-w\|^{2}}_{ =\|v\|^{2}-2\langle v,w\rangle+\|w\|^{2}}\] (32) \[=\underset{v\in U}{\operatorname{argmin}}\underbrace{\|v\|^{2}-2 \langle v,w\rangle}_{=:f(v)}\]

we know that \(t\mapsto f(t\langle w\rangle_{U})\) is minimized at \(t=1\) by the definition of \(\langle w\rangle_{U}\). The first order condition implies

\[0\overset{!}{=}\frac{d}{dt}=2t\|P_{U}(w)\|^{2}-2\langle P_{U}(w),w\rangle\]

and thus

\[1=t^{*}=\frac{\langle P_{U}(w),w\rangle}{\|P_{U}(w)\|^{2}}\]

Multiplying both sides by \(\lambda\|P_{U}(w)\|\) finishes this step

\[\lambda\|P_{U}(w)\|=\Big{\langle}\underbrace{\lambda\frac{P_{U}(w)}{\|P_{U}( w)\|}}_{=v^{*}},w\Big{\rangle}.\] (33)

**Step 2:** By (33), we know that we can achieve the value we claim to be the maximum (and know the location \(v^{*}\) to do so). So if we prove that we can not exceed this value, then it is a maximum and \(v^{*}\) is the \(\operatorname{argmax}\). This would finish the proof. What remains to be shown is therefore

\[\langle v,w\rangle\leq\lambda\|P_{U}(w)\|\qquad\forall v\in U:\|v\|=\lambda.\]

Let \(v\in U\) with \(\|v\|=\lambda\). Then for any \(\mu\in\mathbb{R}\) we can plug \(\mu v\) into \(f\) from (32) to get

\[\mu^{2}\lambda^{2}-2\mu\langle v,w\rangle =f(\mu v)\] \[\overset{\eqref{eq:P_U}}{\geq}f(P_{U}w)=\|P_{U}(w)\|^{2}-2\langle P _{U}w,w\rangle\] \[=-(P_{U}w,w)\]

where the last equation follows from (33) with \(\lambda=\|P_{U}w\|\). Reordering we get for all \(\mu\)

\[\langle P_{U}w,w\rangle+\mu^{2}\lambda^{2}\geq 2\mu\langle v,w\rangle\]

We now select \(\mu=\frac{\|P_{U}w\|}{\lambda}>0\) and divide both sides by \(\mu\) to get

\[2\langle v,w\rangle\leq\big{\langle}\underbrace{\frac{P_{U}(w)}{\mu}}_{=v^{*} },w\big{\rangle}+\lambda\|P_{U}(w)\|=2\lambda\|P_{U}w\|\]

Dividing both sides by \(2\) yields the claim.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section 8 recapitulates all the assumptions made and highlights possible generalizations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Section D provides all proofs of statements made in the main body and follows an identical structure for easier cross reference. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: While we do not have the necessary space to discuss all implementation details, we believe that we have discussed all relevant insights necessary to reproduce our results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code of our algorithm is fairly well commented and passes pylint and flake8 linting. The code to perform the benchmarks is less polished and we have not seeded the covariance estimation sampling process, but, since we obtained similar results over multiple runs (Section A.1.1), we are confident that our results are reproducible. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Training on the MNIST data set is fairly standard, so we feel like our brief outline is sufficient. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Quantiles are plotted in Figure 3 and the figures of Section A and we provided a histogram of asymptotic learning rates resulting from multiple covariance estimation runs (Section A.1.1). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: We have not kept careful track of resources used, as MNIST is a fairly small dataset for machine learning standards. We believe the compute was comparatively negligible, although the use of multiple GPUs was helpful in repeating experiments in parallel. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: See broader impacts. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Since we focus on optimization theory our work has no societal impact beyond the advancement of the the field of Machine Learning, which may have many societal consequences, but none we feel necessary to address.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite datasets and models used. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.