# Verified Safe Reinforcement Learning for Neural Network Dynamic Models

Junlin Wu

Computer Science & Engineering

Washington University in St. Louis

junlin.wu@wustl.edu &Huan Zhang

Electrical & Computer Engineering

University of Illinois Urbana-Champaign

huan@huan-zhang.com &Yevgeniy Vorobeychik

Computer Science & Engineering

Washington University in St. Louis

yvorobeychik@wustl.edu

###### Abstract

Learning reliably safe autonomous control is one of the core problems in trustworthy autonomy. However, training a controller that can be formally verified to be safe remains a major challenge. We introduce a novel approach for learning verified safe control policies in nonlinear neural dynamical systems while maximizing overall performance. Our approach aims to achieve safety in the sense of finite-horizon reachability proofs, and is comprised of three key parts. The first is a novel curriculum learning scheme that iteratively increases the verified safe horizon. The second leverages the iterative nature of gradient-based learning to leverage incremental verification, reusing information from prior verification runs. Finally, we learn multiple verified initial-state-dependent controllers, an idea that is especially valuable for more complex domains where learning a single universal verified safe controller is extremely challenging. Our experiments on five safe control problems demonstrate that our trained controllers can achieve verified safety over horizons that are as much as an order of magnitude longer than state-of-the-art baselines, while maintaining high reward, as well as a perfect safety record over entire episodes. Our code is available at https://github.com/jlwu002/VSRL.

## 1 Introduction

The ability to synthesize safe control policies is one of the core challenges in autonomous systems. This problem has been explored from numerous directions across multiple disciplines, including control theory and AI [1, 17]. While considerable progress has been made, particularly when dynamics are linear [20], the ability to synthesize controllers that can be successfully _verified_ to be safe while maintaining high performance in nonlinear dynamical systems remains a major open problem. Indeed, even the subproblem of safety verification in nonlinear systems is viewed in itself as a major challenge and is an active area of research, particularly for neural network controllers [1, 14, 15]. State-of-the-art approaches for safe control synthesis, including most that leverage reinforcement learning [13], typically only offer empirical evaluation of safety, and rely on safety proofs that hold either asymptotically (rather than for concrete problems) [16] or under idealized assumptions which do not hold in practice [1].

Two common properties are typically leveraged in safety verification: _forward invariance_ and _reachability_. The former aims to identify a set of starting subsets of safe states under which one-step(forward) dynamics remain in this (forward invariant) set. The latter computes the set of states that can possibly be reached after \(K\) steps of the dynamics for a given control policy, and checks whether it intersects with the unsafe set. Approaches for synthesizing (including those that do so using learning) safe policies almost exclusively aim to achieve verified safety through forward invariance. However, this has proved extremely challenging to employ beyond the simplest dynamics.

We propose the first (to our knowledge) approach for learning \(K\)-step verified safe neural network controllers that also aim to maximize efficiency in systems with neural dynamics. While neural dynamics are clearly not universal, they can capture or effectively approximate a broad range of practical dynamical systems (Nagabandi et al., 2018), and have consequently been the focus of much prior work in safe control and verification (Dai et al., 2021). For example, consider the scenario of a drone navigating through a series of obstacles to reach a designated goal, requiring \(K=50\) steps to safely maneuver through the obstacles. We aim to train a controller that can reach the goal as fast as possible, while guaranteeing safety for the initial \(50\) steps, ensuring 1) the drone does not collide with any obstacles and 2) its angle remains within a predefined safe range.

Our approach combines deep reinforcement learning with state-of-the-art differentiable tools for efficient reachability bound computation, and contains two key novel ingredients. The first is a novel curriculum learning scheme for learning a verified safe controller. This scheme takes advantage of the structure of the \(K\)-reachability problem at the root of our safety verification by creating a curriculum sequence with respect to increasing \(K\). An important insight that is specific to the verification setting is that verification must work not merely for a fixed \(K\), but for all steps prior, an issue we address by memorizing subsets of states who either violate, or nearly violate, safety throughout the entire \(K\)-step curriculum learning process. Additionally, to maintain _both_ strong empirical and verified performance, we propose a novel loss function that integrates overall reward, as well as _both_ traditional (empirical) safety loss along with the \(K\)-reachability bound. Our second innovation is to learn a _collection_ of controllers that depend on the initial state, in contrast to typical approaches that focus on learning a single "universal" controller. The ability to allow for learning multiple controllers makes the verified learning problem considerably easier, as we can "save" controllers that work on a subset of initial states, and simply try learning a new controller for the rest, guaranteeing incremental improvement through the learning process. We further improve performance through incremental verification, which leverages information obtained in previous learning iterations.

We evaluate the proposed approach in five control settings. The first two are lane following and obstacle avoidance, both pertaining to autonomous driving. The last three involve drone control with obstacle avoidance. Two of these consider fixed obstacles, while the third aims to avoid even moving obstacles (with known dynamics). We show that the proposed approach outperforms five state-of-the-art safe control baselines in the ability to achieve verified safety without significantly compromising overall reward (efficiency). In particular, our approach learns controllers that can verify \(K\)-step safety for \(K\) up to an order of magnitude larger than the prior art and maintains a perfect safety record for \(K\) far above what we verify, something no baseline can achieve.

In summary, we make the following contributions:

1. A framework for safe optimal control that combines both finite-horizon verified (worst-case) and empirical (average-case) safety constraints.
2. A novel curriculum learning approach that leverages memorization, forward reachability analysis, and differentiable reachability overapproximation for efficiently learning verified safe policies.
3. An approach for learning a _collection_ of control policies that depend on the initial state which enables significant improvements in verified safety horizon over large initial state sets \(S_{0}\).
4. An incremental verification approach that leverages small changes in gradient-based learning to improve verification efficiency during learning.
5. An extensive experimental evaluation that demonstrates the efficacy of the proposed approach in comparison with five state-of-the-art safe RL baselines.

Related Work:Safe reinforcement learning has been extensively studied through the lens of constrained Markov decision process (CMDP)-based approaches, which represent cost functions as constraints and aim to maximize reward while bounding cost, using approaches such as Lagrangian and penalty methods, and constrained policy optimization (Achiam et al., 2017; Stooke et al., 2020; Ma et al., 2022; Jayant and Bhatnagar, 2022; Yu et al., 2022; So and Fan, 2023; Ganai et al., 2024).

An alternative control-theoretic perspective aims to ensure stability or safety using Lyapunov and control barrier functions. For example, Dawson et al. (2022) used a learning-based approach to find robust control Lyapunov barrier functions; Chow et al. (2018) constructed Lyapunov functions to solve CMDPs; (Wang et al., 2023) proposed soft barrier functions for unknown and stochastic environments; and Alshiekh et al. (2018) created safety shielding for safe RL agents. These approaches, however, provide no practical formal safety guarantee for neural network controllers. In addition, some work on provably safe RL focuses on the probabilistic setting (Berkenkamp et al., 2017; Jansen et al., 2020; Xiong et al., 2024) and required statistical assumptions, whereas our work aims for strict deterministic safety guarantees over a finite horizon.

Among existing works focusing on safe RL with formal guarantees, Fulton and Platzer (2018) apply a theorem prover for differential dynamic logic to guarantee safety during runtime. Noren et al. (2021) and Wei et al. (2022) consider forward safety invariance for systems with uncertainty. Kochdumper et al. (2023) propose to project actions to safe subspace using zonotope abstraction and mixed-integer programming (MIP). However, these approaches do not readily apply to neural network controllers. For systems involving neural networks, Wei and Liu (2022) applied integer programming formulation for neural networks to solve an MIP problem to find safe control actions satisfying forward invariance; Bastani et al. (2018) extracted decision-tree-based policies for RL to reduce verification complexity; and Ivanov et al. (2019) used hybrid system verification tools to model deep neural networks. Our work differs from these and similar approaches because we consider forward reachability guarantees for neural network controllers in neural nonlinear systems.

We make extensive use of neural network verification tools. Early work in this vein used SMT (Katz et al., 2017; Huang et al., 2017) or MIP-based (Tjeng et al., 2019) approaches to solve this problem, but their scalability is extremely limited. Significant progress has been made in developing techniques to formally verify the properties of large neural networks through overapproximation, such as bound propagation (Zhang et al., 2018; Gowal et al., 2018; Xu et al., 2021), optimization (Qin et al., 2019; Dvijotham et al., 2018, 2020), and abstract interpretation (Gehr et al., 2018; Singh et al., 2019; Katz et al., 2019; Lopez et al., 2023). Recently, most verifiers have adopted branch-and-bound based approaches to further enhance their performance (Wang et al., 2021; Kouvaros and Lomuscio, 2021; Ferrari et al., 2022; Zhang et al., 2022). Our approach makes use of differentiable overapproximation methods known collectively as \(\alpha\),\(\beta\)-CROWN (Wang et al., 2021; Zhang et al., 2022) (implemented with the auto_LiRPA package), and takes advantage of the particular structure of these verification approaches in applying incremental verification to significantly speed up safe controller learning.

## 2 Preliminaries

**Constrained Markov Decision Process (CMDP):** We consider a deterministic Constrained Markov Decision Process (CMDP) defined by the tuple \((\mathcal{S},\mathcal{A},F,R,\gamma,C_{1},C_{2},\ldots,C_{m},d_{1},d_{2}, \ldots,d_{m})\), where: \(\mathcal{S}\) is a set of states, \(\mathcal{A}\) is a set of actions, \(F:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}\) is the deterministic state transition function, \(R:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is the reward function, \(C_{i}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is the cost function for the \(i\)-th constraint, \(d_{i}\) is the cost limit for the \(i\)-th constraint, and \(\gamma\in[0,1)\) is the discount factor. A policy \(\pi:\mathcal{S}\rightarrow\mathcal{A}\) is a mapping from states to actions. A trajectory is a sequence of states and actions generated by following a policy \(\pi\) from some initial state \(s_{0}\in\mathcal{S}_{0}\subseteq\mathcal{S}\), which can be represented as a sequence \(\tau=(s_{0},a_{0},s_{1},a_{1},s_{2},a_{2},\ldots)\) where \(s_{t}\in\mathcal{S}\), \(a_{t}=\pi(s_{t})\) for all \(t\), \(s_{t+1}=F(s_{t},a_{t})\), a reward \(r_{t}=R(s_{t},a_{t})\) and a cost \(c_{t}=\sum_{i\in[m]}C_{i}(s_{t},a_{t})\) are received after each action.

We denote \(\pi_{\theta}\) as the policy that is parameterized by the parameter \(\theta\). A common goal for CMDP is to learn a policy \(\pi_{\theta}\) that maximizes a discounted sum of rewards \(\mathcal{J}(\pi_{\theta})\) while ensuring that expected discounted costs \(\mathcal{J}_{C_{i}}(\pi_{\theta})\) do not exceed the cost limit \(d_{i}\), \(\forall i\in[m]\). Formally, CMDP is to solve the below optimization problem:

\[\max_{\theta}\mathcal{J}(\pi_{\theta})\quad\text{s.t.}\ \mathcal{J}_{C_{i}}(\pi_{ \theta})\leq d_{i},\forall i\in[m],\] (1)

where \(\mathcal{J}(\pi_{\theta})=\mathbb{E}_{\tau\sim\pi}\left[\sum_{t=0}^{\infty} \gamma^{t}R(s_{t},a_{t})\right]\) and \(\mathcal{J}_{C_{i}}(\pi_{\theta})=\mathbb{E}_{\tau\sim\pi}\left[\sum_{t=0}^{ \infty}\gamma^{t}C_{i}(s_{t},a_{t})\right]\).

**Verified Safe CMDP:** We define the state space as the union of predefined safe and unsafe states, denoted as \(\mathcal{S}=\mathcal{S}_{\text{safe}}\cup\mathcal{S}_{\text{unsafe}}\). We assume that the transition function \(F\) is represented by a ReLU neural network, and is known for verification purposes. This assumption is very general, as many known dynamical systems can be represented exactly or approximately using ReLU neural networks (Gillespie et al., 2018; Pfrommer et al., 2021; Dai et al., 2021; Liu et al., 2024). Our

[MISSING_PAGE_FAIL:4]

typical structure of safety constraints that only pertain to a small subset of state variables. For instance, in drone control for obstacle avoidance, we prioritize splitting the location and angle axes. Next, we design a cost function \(C_{R}\) for regions where \(C_{R}(S)=0\) if \(S\cap\mathcal{S}_{\text{unsafe}}=\emptyset,\text{ and }C_{R}(S)>0\) otherwise. A positive \(C_{R}\) means region \(S\) intersects with \(\mathcal{S}_{\text{unsafe}}\), while \(C_{R}=0\) indicates \(S\) is safe. For example, if the task is to avoid the region \([a,b]\), and the output bounds are given by \(x_{B}=[x_{lb},x_{ub}]\), we can define \(C_{R}(x_{B})=\max(x_{ub}-a,0)\cdot\max(b-x_{lb},0)\). We then calculate the gradient \(\partial C_{R}(F^{t,\pi_{\theta}}_{\text{Bound}}(\mathcal{S}_{0}))/\partial r\) for a chosen value of \(t\) and proceed to split along the dimensions with the largest gradient values, as a larger gradient indicates a higher likelihood of reducing the cost \(C_{R}\). We continue this process, keeping the total number of grid splits within a predetermined budget, and stop splitting once the budget is reached.

For each training phase \(k\), we monitor the training rounds (\(n_{\text{train}}\)) as well as the \(k\)-step forward reachable regions returned by the verifier that are identified as unsafe (\(S_{uc}\)). Each phase is conducted for a maximum of \(n_{\text{max}}\) rounds or until verified \(k\)-step safety is achieved, that is, when \(S_{uc}=\emptyset\) (Line 8). At the end of each training phase, we also filter out regions \(S_{k}\subseteq\mathcal{G}_{0}\) where \(F^{k,\pi_{\theta}}_{\text{Bound}}(S_{k})\) are within \(\epsilon\) distance to the unsafe regions. These regions are then stored in the buffer \(B\) (Line 13). We include these critical regions in the training set for each reinforcement learning update to enhance verified safety across the entire horizon. During this process, we optionally use the Branch-and-Bound algorithm (Everett et al., 2020; Wang et al., 2021) to refine \(\mathcal{G}_{0}\) up to a predetermined branching limit, which helps achieve tighter bounds.

For each RL update, we use a loss function that integrates the standard safe RL loss with a \(k\)-phase loss for bounds (Line 9), where

\[\mathcal{L}(x) =\mathcal{L}_{\text{SafeRL}}(x)+\lambda\mathcal{L}_{\text{Bound} }(S_{uc}\cup B)\] (3a) \[\mathcal{L}_{\text{Bound}}(S_{uc}\cup B) =C_{R}(F^{k,\pi_{\theta}}_{\text{Bound}}(S_{uc}))+\sum_{(S_{i},i) \in B}C_{R}(F^{i,\pi_{\theta}}_{\text{Bound}}(S_{i})).\] (3b)

Here, \(\mathcal{L}_{\text{SafeRL}}\) is the standard safety RL loss, and \(\mathcal{L}_{\text{Bound}}\) denotes the loss that incentivizes ensuring the output bounds returned by the verifier remain within the safe region. If both \(S_{uc}\) is \(k\)-step safe and \(\forall(S_{i},i)\in B\), \(S_{i}\) is \(i\)-step safe, then \(\mathcal{L}_{\text{Bound}}(S_{uc}\cup B)=0\), otherwise, \(\mathcal{L}_{\text{Bound}}(S_{uc}\cup B)>0\). In practice, we clip \(\mathcal{L}_{\text{Bound}}(S_{uc}\cup B)\) to ensure it remains within a reasonable range for training stability. The regularization parameter \(\lambda\) is calculated based on the magnitude of \(\mathcal{L}_{\text{SafeRL}}\) and \(\mathcal{L}_{\text{Bound}}\), with \(\lambda=\min(\lambda_{\text{max}},a_{r}\cdot\mathcal{L}_{\text{SafeRL}}/ \mathcal{L}_{\text{Bound}})\), where \(\lambda_{\text{max}}\) and \(a_{r}\) are hyperparamters. This approach helps maintain the effectiveness of bound training, especially when \(\mathcal{L}_{\text{Bound}}\) is small. Furthermore, we cluster elements in \(B\) into categories so that we do not need to construct a computational graph for all \(i<k\). Specifically, we merge all \(S_{i}\) for \(i_{1}\leq i\leq i_{2}\) into the \(i_{2}\) category, meaning the elements in \(B\) are now \((\cup_{i_{1}\leq i\leq i_{2}}S_{i},i_{1},i_{2})\) instead of \((S_{i},i)\).

It is important to note that while our training scheme targets \(K\)-step verified safety, the policy returned by Algorithm 1 does not necessarily guarantee it. We address this issue by learning initial-state-dependent controllers as described below. Furthermore, the computation of \(\mathcal{L}_{\text{Bound}}\) is computationally intensive. Its backpropagation requires constructing computational graphs for the \(k\)-th step forward NN \(F_{\text{Bound}}^{k,\pi_{\theta}}\), as well as for all \(i\)-th step forward NNs corresponding to each \((S_{i},i)\in B\). These NNs become increasingly deep as \(k\) grows, causing the computational graphs to consume memory beyond the typical GPU memory limits. We will address this next.

**Incremental Verification:** Above we discussed the challenge presented by the backpropagation of \(\mathcal{L}_{\text{bound}}\), which is GPU-memory intensive and does not scale efficiently as the target \(K\)-step horizon increases. To mitigate these issues, we propose the use of incremental verification to enhance computational efficiency and reduce memory consumption. While incremental verification is well-explored in the verification literature Wang et al. (2023); Althoff (2015), to our knowledge, we are the first to apply it in _training_ provably safe controllers.

At a high level, to calculate the reachable region for a \(k_{\text{target}}\) step, we decompose the verification into multiple phases. We begin by splitting the \(k_{\text{target}}\) horizon into intervals defined by \(0<k_{1}<k_{2}<\dots<k_{n}=k_{\text{target}}\). We first calculate the reachability region for the \(k_{i}\) step and then use its output bounds as input to calculate the reachable region for the \(k_{i+1}\) step. This approach ensures that the computational graph is only built for the \((k_{i+1}-k_{i})\) step horizon when using \(\alpha\),\(\beta\)-CROWN.

Unlike traditional incremental verification, which typically calculates the reachable region from \(k\) to \(k+1\), we incrementally verify and backpropagate several steps ahead in a single training iteration (i.e., from \(k_{i}\) to \(k_{i+1}\), where \(k_{i+1}-k_{i}>1\)). This generalized version of incremental verification is essential for training, as it significantly accelerates the process and reduces the likelihood of becoming trapped in "local optima," where inertia from the policy obtained for \(k\) prevents successful verification for \(k+1\) (e.g., due to proximity to the unsafe region with velocity directed toward it).

For the bounds used in neural network training, we effectively build the computational graph and perform backpropagation using a neural network sized for \((k_{n}-k_{n-1})\) steps' reachability, which is independent of \(k_{\text{target}}\). This significantly reduces GPU memory usage. Since \(F^{k,\pi_{\theta}}\) is an iterative composition of \(F\) under the same policy \(\pi_{\theta}\), the bound for \(k_{n-1}\) steps tends to be tight. Moreover, when training \(\pi_{\theta}\) to tighten these bounds, the overall bound for the entire \(k_{\text{target}}\) horizon becomes increasingly tight.

**Initial-State-Dependent Controller:** While curriculum learning above includes verification steps, it does not guarantee verified safety for the controller over the entire \(K\)-step horizon. In this section, we propose using an initial-state-dependent controller to address this issue. For example, in a vehicle avoidance scenario, different initial conditions, such as varying speeds and positions, may correspond to different control strategies. We introduce a mapping function \(h:\mathcal{S}_{0}\rightarrow\Theta\), which maps each initial state \(s_{0}\in\mathcal{S}\) to a specific policy \(\pi_{h(s_{0})}\). The underlying idea is that training a verifiable safe policy \(\pi_{\theta}\) over the entire set of initial states \(\mathcal{S}_{0}\) is inherently challenging. However, by mapping each initial state to a specific set of parameters, we can significantly enhance the expressivity of the policy. This approach is particularly effective in addressing and eliminating corner cases in unverifiable regions.

At a high level, the mapping and parameter set \(\Theta\) are obtained by first performing comprehensive verification for the controller output from Algorithm 1 over the entire \(K\)-step horizon. We then filter unverified regions, cluster them, and fine-tune the controller parameters \(\theta\) for each cluster. We store these fine-tuned parameters in the parameter set \(\Theta\). This iterative refinement process continues until for every \(s_{0}\in\mathcal{S}_{0}\), there exists a \(\theta\in\Theta\) such that \(\pi_{\theta}\) is verified safe for the entire \(K\)-step horizon. The detailed algorithm is presented in Algorithm 2.

The algorithm starts with verifying the policy \(\pi_{\theta}\) obtained from Algorithm 1. The function \(\textsc{VerifySafe}(\pi_{\theta},\mathcal{S}_{0},K)\) (Line 4) performs verification of policy \(\pi_{\theta}\) for initial states \(\mathcal{S}_{0}\) for the entire horizon \(K\). This verification process identifies and categorizes regions into verified safe areas, \(S^{V}_{\text{safe}}\), and areas identified as unsafe, \(S^{V}_{\text{unsafe}}\). Notably, the union of these regions covers all initial states, meaning \(S^{V}_{\text{safe}}\cup S^{V}_{\text{unsafe}}=\mathcal{S}_{0}\). After verifying that any state \(s_{0}\in S^{V}_{\text{safe}}\) is guaranteed to be safe under policy \(\pi_{\theta}\), we record \((S^{V}_{\text{unsafe}},\pi_{\theta})\) in the mapping dictionary \(H\) (Line 5).

Next, we address the unsafe regions \(S^{V}_{\text{unsafe}}\) that lack a corresponding verified safe controller. We first cluster them based on the type of safety violation (Line 7). The reason for clustering is that regions with similar safety violations are more likely to be effectively verified safe by the same controller. For instance, in a scenario involving navigation around two obstacles, we could potentially identify up to three clusters: the first corresponding to grids that can lead to collisions with obstacle 1, the second includes grids associated with collisions with obstacle 2, and the third is the set of grids that may lead to collisions with both. Given the finite number of safety constraints, the number of possible clusters is also finite. Although the theoretical maximum number of clusters grows exponentially with the number of safety constraints, in practice, this number is significantly smaller. This is due to the fact that the controller, being pretrained, is less likely to violate multiple or all constraints simultaneously. We then fine-tune the controller for the initial states in each cluster using Algorithm 1. This fine-tuning process is typically fast, as the initial policy is already well-trained. We store each initial state region and its corresponding verified safe policy in the mapping dictionary \(H\). This clustering and fine-tuning process continues until a verified safe policy exists for every \(s_{0}\in\mathcal{S}_{0}\).

At decision time, given an initial state \(s_{0}\), we first identify the pair \((S_{\text{safe}}^{V},\pi_{\theta})\) in the mapping dictionary \(H\) where \(s_{0}\in S_{\text{safe}}^{V}\), then use the corresponding verified safe controller \(\pi_{\theta}\). Note that the soundness of the algorithm directly follows from our use of the sound verification tool \(\alpha\),\(\beta\)-CROWN.

```
1:Input: target safety horizon \(K\), policy \(\pi_{\theta}\)
2:Output: mapping dictionary \(H\), which includes the mapping \(h\) and parameter set \(\Theta\)
3:Initialize \(H=\{\}\)
4:\((S_{\text{safe}}^{V},S_{\text{unsafe}}^{V})\leftarrow\textsc{VerifySafety}( \pi_{\theta},\mathcal{S}_{0},K)\)
5:Store \((S_{\text{safe}}^{V},\theta)\) in mapping dictionary \(H\)
6:while\(S_{\text{safe}}^{V}\neq\emptyset\)do
7:\(\{S_{1},S_{2},\dots,S_{I}\}\leftarrow\textsc{ClusterRegion}(S_{\text{ unsafe}}^{V})\) // cluster based on safety violation
8:for\(i=1,2,\dots,I\)do
9:\(\pi_{\theta^{\prime}}\leftarrow\textsc{TrainPolicy}(\pi_{\theta},S_{i},K)\)
10:\((S_{\text{safe},i}^{V},S_{\text{unsafe},i}^{V})\leftarrow\textsc{VerifySafety}( \pi_{\theta^{\prime}},S_{i},K)\)
11: Store \((S_{\text{safe},i}^{V},\theta^{\prime})\) in mapping dictionary \(H\)
12:endfor
13:\(S_{\text{unsafe}}^{V}\leftarrow\bigcup_{i}S_{\text{unsafe},i}^{V}\)
14:endwhile ```

**Algorithm 2** Initial-State-Dependent Controller

## 4 Experiments

### Experiment Setup

We evaluate our proposed approach in five control settings: Lane Following, Vehicle Avoidance, 2D Quadrotor (with both fixed and moving obstacles), and 3D Quadrotor (Kong et al., 2015; Dai et al., 2021). The dynamics of these environments are approximated using NN with ReLU activations. We use a continuous action space for those discrete-time systems. In each experiment, we specify the initial region \(\mathcal{S}_{0}\) for which we wish to achieve verified safety. We then aim to achieve the maximum \(K\) for which safety can be verified. We evaluate the approaches using four metrics: 1) _Verified-\(K\)_: the percentage of regions in \(\mathcal{S}_{0}\) that can be verified for safety over \(K\) steps; 2) _Verified-Max_: the maximum number of steps for which all states in \(\mathcal{S}_{0}\) can be verified as safe; 3) _Emp-k_: the percentage of regions in \(\mathcal{S}_{0}\) that are empirically safe for \(k\) steps, obtained by sampling \(10^{7}\) datapoints from the initial state \(\mathcal{S}_{0}\). This is evaluated for both \(k=K\) (the number of steps we are able to verify safety for) and \(k=T\) (total episode length); 4) _Avg Reward_: the average reward over 10 episodes, with both mean and standard deviations reported. Note that the average reward is computed over the entire episode horizon for each environment, independently of the verification horizon, as in conventional reinforcement learning.

We compare the proposed _verified safe RL (VSRL)_ approach to six baselines: 1) PPO-Lag, which utilizes constrained PPO with the standard Lagrangian penalty (Achiam et al., 2017); 2) PPO-PID, which employs constrained PPO with PID Lagrangian methods (Stooke et al., 2020); 3) CAP, which adopts model-based safe RL with an adaptive penalty (Ma et al., 2022); 4) MBPPO, which applies model-based safe RL with constrained PPO (Jayant and Bhatnagar, 2022); 5) CBF-RL, which is a Control Barrier Function (CBF)-based safe reinforcement learning approach (Emam et al., 2022); and 6) RESPO, which implements safe RL using iterative reachability estimation (Ganai et al., 2024).

Next, we describe the four autonomous system environments in which we run our experiments. Further experimental setup details are provided in Appendix A.2.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline  & \multicolumn{5}{c}{Lane Following} \\  & Verified-80(\(\uparrow\)) & Verified-Max(\(\uparrow\)) & Emp-80(\(\uparrow\)) & Emp-500(\(\uparrow\)) & Avg Reward(\(\uparrow\)) \\ \hline PPO-Lag & \(98.6\) & \(7\) & \(99.9\) & \(99.9\) & \(326\pm 6\) \\ PPO-PID & \(88.5\) & \(8\) & \(99.9\) & \(99.9\) & \(327\pm 6\) \\ CAP & \(99.5\) & \(7\) & \(99.9\) & \(99.9\) & \(357\pm 4\) \\ MBPPO & \(99.7\) & \(8\) & \(99.9\) & \(99.9\) & \(382\pm 5\) \\ CBF-RL & \(98.7\) & \(7\) & \(99.9\) & \(99.9\) & \(331\pm 7\) \\ RESPO & \(99.8\) & \(7\) & \(99.9\) & \(99.9\) & \(\mathbf{383\pm 7}\) \\ \hline VSRL & \(\mathbf{100.0}\) & \(\mathbf{80}\) & \(\mathbf{100.0}\) & \(\mathbf{100.0}\) & \(214\pm 5\) \\ \hline \hline \multicolumn{5}{c}{Vehicle Avoidance (Moving Obstacles)} \\  & Verified-50(\(\uparrow\)) & Verified-Max(\(\uparrow\)) & Emp-50(\(\uparrow\)) & Emp-500(\(\uparrow\)) & Avg Reward(\(\uparrow\)) \\ \hline PPO-Lag & \(72.8\) & \(6\) & \(87.8\) & \(87.8\) & \(303\pm 12\) \\ PPO-PID & \(72.0\) & \(6\) & \(89.4\) & \(89.4\) & \(287\pm 22\) \\ CAP & \(73.3\) & \(13\) & \(89.5\) & \(89.5\) & \(393\pm 35\) \\ MBPPO & \(82.6\) & \(6\) & \(94.2\) & \(94.2\) & \(375\pm 10\) \\ CBF-RL & \(73.0\) & \(6\) & \(89.3\) & \(89.3\) & \(301\pm 15\) \\ RESPO & \(74.5\) & \(9\) & \(89.6\) & \(89.6\) & \(391\pm 20\) \\ \hline VSRL & \(\mathbf{100.0}\) & \(\mathbf{50}\) & \(\mathbf{100.0}\) & \(\mathbf{100.0}\) & \(\mathbf{401\pm 4}\) \\ \hline \hline \multicolumn{5}{c}{2D Quadrotor (Fixed Obstacles)} \\  & Verified-50(\(\uparrow\)) & Verified-Max(\(\uparrow\)) & Emp-50(\(\uparrow\)) & Emp-500(\(\uparrow\)) & Avg Reward(\(\uparrow\)) \\ \hline PPO-Lag & \(0.0\) & \(5\) & \(83.4\) & \(83.4\) & \(405\pm 30\) \\ PPO-PID & \(0.0\) & \(4\) & \(99.3\) & \(97.5\) & \(\mathbf{411\pm 25}\) \\ CAP & \(0.0\) & \(3\) & \(99.5\) & \(99.5\) & \(393\pm 12\) \\ MBPPO & \(58.9\) & \(9\) & \(99.9\) & \(84.5\) & \(399\pm 11\) \\ CBF-RL & \(0.0\) & \(5\) & \(89.9\) & \(89.7\) & \(408\pm 17\) \\ RESPO & \(60.4\) & \(14\) & \(99.9\) & \(99.9\) & \(339\pm 19\) \\ \hline VSRL & \(\mathbf{100.0}\) & \(\mathbf{50}\) & \(\mathbf{100.0}\) & \(\mathbf{100.0}\) & \(401\pm 20\) \\ \hline \hline \multicolumn{5}{c}{2D Quadrotor (Moving Obstacles)} \\  & Verified-50(\(\uparrow\)) & Verified-Max(\(\uparrow\)) & Emp-50(\(\uparrow\)) & Emp-500(\(\uparrow\)) & Avg Reward(\(\uparrow\)) \\ \hline PPO-Lag & \(0.0\) & \(3\) & \(99.7\) & \(99.7\) & \(371\pm 7\) \\ PPO-PID & \(0.0\) & \(2\) & \(99.7\) & \(99.7\) & \(371\pm 5\) \\ CAP & \(57.1\) & \(8\) & \(99.2\) & \(99.2\) & \(362\pm 3\) \\ MBPPO & \(0.0\) & \(4\) & \(99.3\) & \(99.3\) & \(\mathbf{374\pm 6}\) \\ CBF-RL & \(0.0\) & \(4\) & \(99.3\) & \(99.3\) & \(369\pm 6\) \\ RESPO & \(0.0\) & \(6\) & \(99.1\) & \(99.1\) & \(373\pm 6\) \\ \hline VSRL & \(\mathbf{100.0}\) & \(\mathbf{50}\) & \(\mathbf{100.0}\) & \(\mathbf{100.0}\) & \(364\pm 4\) \\ \hline \hline \multicolumn{5}{c}{3D Quadrotor (Fixed Obstacles)} \\  & Verified-15(\(\uparrow\)) & Verified-Max(\(\uparrow\)) & Emp-15(\(\uparrow\)) & Emp-500(\(\uparrow\)) & Avg Reward(\(\uparrow\)) \\ \hline PPO-Lag & \(0.0\) & \(3\) & \(85.2\) & \(81.2\) & \(132\pm 11\) \\ PPO-PID & \(0.0\) & \(3\) & \(89.4\) & \(88.3\) & \(\mathbf{145\pm 12}\) \\ CAP & \(0.0\) & \(4\) & \(63.6\) & \(59.2\) & \(141\pm 11\) \\ MBPPO & \(41.1\) & \(1\) & \(75.4\) & \(73.1\) & \(132\pm 9\) \\ CBF-RL & \(0.0\) & \(2\) & \(82.3\) & \(79.2\) & \(140\pm 10\) \\ RESPO & \(0.0\) & \(1\) & \(65.7\) & \(21.3\) & \(79\pm 8\) \\ \hline VSRL & \(\mathbf{100.0}\) & \(\mathbf{15}\) & \(\mathbf{100.0}\) & \(\mathbf{100.0}\) & \(122\pm 14\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results for verified safety, empirical safety and average reward. The percentage results are truncated instead of rounded, to prevent missing unsafe violations.

**Lane Following:** Our lane following environment follows the discrete-time bicycle model [Kong et al., 2015]. The model inputs are 3-dimensional \((x,\theta,v)\), where \(x\) is the lateral distance to the center of the lane, \(\theta\) is the angle relative to the center of the lane, and \(v\) represents the speed. The objective is to maintain a constant speed while following the lane, meaning the system equilibrium point is \((x,\theta,v)=(0,0,v_{\text{target}})\). The safety constraints are 1) \(x\) stays within a maximum distance from the lane center (\(\|x\|\leq d_{\text{max}}\)), 2) \(\theta\) remains within a predefined range \((\|\theta\|\leq\theta_{\text{max}})\), and 3) \(v\) does not exceed the maximum threshold \((v\leq v_{\text{max}})\).

**Vehicle Avoidance:** Our vehicle avoidance environment features a vehicle moving on an \(x\)-\(y\) plane, with 4-dimensional inputs \((x,y,\theta,v)\). Here, \((x,y)\) represents the location of the vehicle on the plane, \(\theta\) is the angle relative to the \(y\)-axis, and \(v\) is the speed. In this setting, we have five moving obstacles, each moving from one point to another at constant speed. Each obstacle is represented as a square. Additionally, safety constraints are set for the speed \((v\leq v_{\text{max}})\) and angle \((\|\theta\|\leq\theta_{\text{max}})\). The task is to navigate the vehicle to a designated location while following safety constraints.

**2D Quadrotor:** For the 2D quadrotor environment, we follow the settings in Dai et al. [2021]. The input is 6-dimensional \((y,z,\theta,\dot{y},\dot{z},\dot{\theta})\), where \((y,z)\) represents the position of the quadrotor on the \(y\)-\(z\) plane, and \(\theta\) represents the angle. The action space is 2-dimensional and continuous; the actions are clipped within a range to reflect motor constraints. Our safety criteria include an angle constraint (\(\|\theta\|\leq\theta_{\text{max}}\)) and a minimum height constraint to prevent collision with the ground \((y\geq y_{\text{min}})\). We consider two scenarios for obstacles: fixed and moving. For fixed obstacles, there are five rectangular obstacles positioned in the \(y\)-\(z\) plane. For moving obstacles, there are five obstacles that moves from one point to another at constant speed, each represented as a square.

**3D Quadrotor:** Our 3D quadrotor environment features a 12-dimensional input space, represented as \((x,y,z,\phi,\theta,\psi,\dot{x},\dot{y},\dot{z},\omega_{x},\omega_{y},\omega_{ z})\). The action space is 4-dimensional and continuous; the actions are clipped within a range to reflect motor constraints. Here, \((x,y,z)\) denotes the location of the quadrotor in space, \(\phi\) is the roll angle, \(\theta\) is the pitch angle, and \(\psi\) is the yaw angle, \(\omega_{x},\omega_{y},\omega_{z}\) represent the angular velocity around the \(x\), \(y\), and \(z\) axes, respectively. The task is to navigating towards the goal while adhering to safety constraints, which include avoiding five obstacles represented as 3D rectangles. The details for the environment settings are deferred to the Appendix.

### Results

As shown in Table 1, our approach significantly outperforms all baselines in terms of verified safety, as well as empirical safety over the entire episode horizon. Furthermore, the only environment in which VSRL exhibits a significant decrease in reward compared to baselines is lane following; for the rest, it achieves reward comparable to, or better than the baselines.

Specifically, in the _lane following_ environment, the proposed VSRL approach achieves verified \(80\)-step safety using a single controller (i.e., \(|\Theta|=1\)). This is an order of magnitude higher \(K\) than all baselines (which only achieve \(K\leq 8\)). While all baselines obtain a safety record of over 99.9% over the entire episode (\(K=500\)), our approach empirically achieves perfect safety.

For vehicle avoidance, we achieve verified \(50\)-step safety using two controllers (i.e., \(|\Theta|=2\)); in contrast, the best baseline yields only \(K=13\). We also observe considerable improvements in both verified and empirical safety over the baseline approaches: for example, the best verified baseline (CAP) violates safety over 10% of the time over the full episode length, whereas VSRL maintains a perfect safety record. In this case, VSRL also achieves the highest reward.

For the 2D Quadrotor environment with fixed and moving obstacles, we are able to achieve verified \(50\)-step safety using four and two controllers, respectively. The best baseline achieves only \(K=14\) in the case of fixed and \(K=8\) in the case of moving obstacles (notably, different baselines are best in these cases).

Finally, in the most complex 3D Quadrotor environment, we achieve verified safety for \(K=15\), but empirically maintain a perfect safety record for the entire episode direction. The best baseline achieves verified safety for only \(K=4\), but is empirically unsafe over 40% of the time during an episode. Even the best safety record of any baseline is unsafe nearly 12% of the time, and we can only verify its safety over a horizon \(K=3\).

**Ablation Study:** We evaluate the importance of both incremental verification and using multiple initial-state-dependent controllers as part of VSRL. As shown in the Appendix (Section A.1), theformer significantly reduces average verification time during training, whereas the latter enables us to greatly boost the size of the initial state region \(\mathcal{S}_{0}\) for which we are able to achieve verify safety.

## 5 Conclusion

We present an approach for learning neural network control policies for nonlinear neural dynamical systems. In contrast to conventional methods for safe control synthesis which rely on forward invariance-based proofs, we opt instead for the more pragmatic finite-step reachability verification. This enables us to make use of state-of-the-art differentiable neural network overapproximation tools that we combine with three key innovations. The first is a novel curriculum learning approach for maximizing safety horizon. The second is to learn multiple initial-state-dependent controllers. The third is to leverage small changes in iterative gradient-based learning to enable incremental verification. We show that the proposed approach significantly outperforms state of the art safe RL baselines on several dynamical system environments, accounting for both fixed and moving obstacles. A key limitation of our approach is the clearly weaker safety guarantees it provides compared to forward invariance. Nevertheless, our results demonstrate that finite-step reachability provides a more pragmatic way of achieving verified safety that effectively achieves safety over the entire episode horizon _in practice_, providing an alternative direction for advances in verified safe RL to the more typical forward-invariance-based synthesis.

## Acknowledgments

This research was partially supported by the NSF (grants IIS-1905558, IIS-2214141, CCF-2403758, and IIS-2331967) and NVIDIA. Huan Zhang is supported in part by the AI2050 program at Schmidt Sciences (AI 2050 Early Career Fellowship).

## References

* Achiam et al. (2017) Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In _International conference on machine learning_, pages 22-31. PMLR, 2017.
* Alshiekh et al. (2018) Mohammed Alshiekh, Roderick Bloem, Rudiger Ehlers, Bettina Konighofer, Scott Niekum, and Ufuk Topcu. Safe reinforcement learning via shielding. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* Althoff (2015) Matthias Althoff. An introduction to CORA 2015. In _Proc. of the 1st and 2nd Workshop on Applied Verification for Continuous and Hybrid Systems_, pages 120-151. EasyChair, December 2015. doi: 10.29007/zbkv.
* Bastani et al. (2018) Osbert Bastani, Yewen Pu, and Armando Solar-Lezama. Verifiable reinforcement learning via policy extraction. _Advances in neural information processing systems_, 31, 2018.
* Bengio et al. (2009) Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In _Proceedings of the 26th annual international conference on machine learning_, pages 41-48, 2009.
* Berkenkamp et al. (2017) Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe model-based reinforcement learning with stability guarantees. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, page 908-919, 2017.
* Chow et al. (2018) Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunov-based approach to safe reinforcement learning. _Advances in neural information processing systems_, 31, 2018.
* Dai et al. (2021) Hongkai Dai, Benoit Landry, Lujie Yang, Marco Pavone, and Russ Tedrake. Lyapunov-stable neural-network control. In Dylan A. Shell, Marc Toussaint, and M. Ani Hsieh, editors, _Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021_, 2021.
* Dawson et al. (2022) Charles Dawson, Zengyi Qin, Sicun Gao, and Chuchu Fan. Safe nonlinear control using robust neural lyapunov-barrier functions. In _Conference on Robot Learning_, pages 1724-1735. PMLR, 2022.
* Duven et al. (2018)Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A. Mann, and Pushmeet Kohli. A dual approach to scalable verification of deep networks. In _Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018_, pages 550-559, 2018.
* Dvijotham et al. [2020] Krishnamurthy Dj Dvijotham, Robert Stanforth, Sven Gowal, Chongli Qin, Soham De, and Pushmeet Kohli. Efficient neural network verification with exactness characterization. In _Uncertainty in artificial intelligence_, pages 497-507. PMLR, 2020.
* Emam et al. [2022] Yousef Emam, Gennaro Notomista, Paul Glotfelter, Zsolt Kira, and Magnus Egerstedt. Safe reinforcement learning using robust control barrier functions. _IEEE Robotics and Automation Letters_, 2022.
* Everett et al. [2020] Michael Everett, Golnaz Habibi, and Jonathan P How. Robustness analysis of neural networks via efficient partitioning with applications in control systems. _IEEE Control Systems Letters_, 5(6):2114-2119, 2020.
* Ferrari et al. [2022] Claudio Ferrari, Mark Niklas Mueller, Nikola Jovanovic, and Martin Vechev. Complete verification via multi-neuron relaxation guided branch-and-bound. In _International Conference on Learning Representations_, 2022.
* Fulton and Platzer [2018] Nathan Fulton and Andre Platzer. Safe reinforcement learning via formal methods: Toward safe control through proof and learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* Ganai et al. [2024] Milan Ganai, Zheng Gong, Chenning Yu, Sylvia Herbert, and Sicun Gao. Iterative reachability estimation for safe reinforcement learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* Gehr et al. [2018] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In _2018 IEEE Symposium on Security and Privacy (SP)_, pages 3-18, 2018.
* Gillespie et al. [2018] Morgan T Gillespie, Charles M Best, Eric C Townsend, David Wingate, and Marc D Killpack. Learning nonlinear dynamic models of soft robots for model predictive control with neural networks. In _2018 IEEE International Conference on Soft Robotics (RoboSoft)_, pages 39-45. IEEE, 2018.
* Gowal et al. [2018] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. _arXiv preprint arXiv:1810.12715_, 2018.
* Gu et al. [2022] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, and Alois Knoll. A review of safe reinforcement learning: Methods, theory and applications. _arXiv preprint arXiv:2205.10330_, 2022.
* Huang et al. [2017] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural networks. In _Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30_, pages 3-29. Springer, 2017.
* Ivanov et al. [2019] Radoslav Ivanov, James Weimer, Rajeev Alur, George J Pappas, and Insup Lee. Verisig: verifying safety properties of hybrid systems with neural network controllers. In _Proceedings of the 22nd ACM International Conference on Hybrid Systems: Computation and Control_, pages 169-178, 2019.
* Jansen et al. [2020] Nils Jansen, Bettina Konighofer, Sebastian Junges, Alex Serban, and Roderick Bloem. Safe reinforcement learning using probabilistic shields. In _31st International Conference on Concurrency Theory (CONCUR 2020)_. Schloss-Dagstuhl-Leibniz Zentrum fur Informatik, 2020.
* Jayant and Bhatnagar [2022] Ashish K Jayant and Shalabh Bhatnagar. Model-based safe deep reinforcement learning via a constrained proximal policy optimization algorithm. _Advances in Neural Information Processing Systems_, 35:24432-24445, 2022.
* Jansen et al. [2018]Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In _International Conference on Computer Aided Verification_, pages 97-117, 2017.
* Katz et al. (2019) Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zeljic, et al. The marabou framework for verification and analysis of deep neural networks. In _Computer Aided Verification: 31st International Conference, CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings, Part I 31_, pages 443-452. Springer, 2019.
* Kochdumper et al. (2023) Niklas Kochdumper, Hanna Krasowski, Xiao Wang, Stanley Bak, and Matthias Althoff. Provably safe reinforcement learning via action projection using reachability analysis and polynomial zonotopes. _IEEE Open Journal of Control Systems_, 2:79-92, 2023.
* Kong et al. (2015) Jason Kong, Mark Pfeiffer, Georg Schildbach, and Francesco Borrelli. Kinematic and dynamic vehicle models for autonomous driving control design. In _2015 IEEE intelligent vehicles symposium (IV)_, pages 1094-1099. IEEE, 2015.
* Kouvaros and Lomuscio (2021) Panagiotis Kouvaros and Alessio Lomuscio. Towards scalable complete verification of relu neural networks via dependency-based branching. In _IJCAI_, pages 2643-2650, 2021.
* Liu et al. (2024) Ziang Liu, Genggeng Zhou, Jeff He, Tobia Marcucci, Fei-Fei Li, Jiajun Wu, and Yunzhu Li. Model-based control with sparse neural dynamics. _Advances in Neural Information Processing Systems_, 36, 2024.
* Lopez et al. (2023) Diego Manzanas Lopez, Sung Woo Choi, Hoang-Dung Tran, and Taylor T Johnson. Nnv 2.0: the neural network verification tool. In _International Conference on Computer Aided Verification_, pages 397-412. Springer, 2023.
* Ma et al. (2022) Yecheng Jason Ma, Andrew Shen, Osbert Bastani, and Jayaraman Dinesh. Conservative and adaptive penalty for model-based safe reinforcement learning. In _Proceedings of the AAAI conference on artificial intelligence_, volume 36, pages 5404-5412, 2022.
* Nagabandi et al. (2018) Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In _2018 IEEE international conference on robotics and automation (ICRA)_, pages 7559-7566. IEEE, 2018.
* Noren et al. (2021) Charles Noren, Weiye Zhao, and Changliu Liu. Safe adaptation with multiplicative uncertainties using robust safe set algorithm. _IFAC-PapersOnLine_, 54(20):360-365, 2021.
* Pfrommer et al. (2021) Samuel Pfrommer, Mathew Halm, and Michael Posa. Contactnets: Learning discontinuous contact dynamics with smooth, implicit representations. In _Conference on Robot Learning_, pages 2279-2291. PMLR, 2021.
* Qin et al. (2019) Chongli Qin, Krishnamurthy (Dj) Dvijotham, Brendan O'Donoghue, Rudy Bunel, Robert Stanforth, Sven Gowal, Jonathan Uesato, Grzegorz Swirszcz, and Pushmeet Kohli. Verification of non-linear specifications for neural networks. In _International Conference on Learning Representations_, 2019.
* Singh et al. (2019) Gagandeep Singh, Timon Gehr, Markus Puschel, and Martin Vechev. An abstract domain for certifying neural networks. _Proceedings of the ACM on Programming Languages_, 3(POPL):41, 2019.
* So and Fan (2023) Oswin So and Chuchu Fan. Solving stabilize-avoid optimal control via epigraph form and deep reinforcement learning. In _Proceedings of Robotics: Science and Systems_, 2023.
* Stooke et al. (2020) Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by grid lagrangian methods. In _International Conference on Machine Learning_, pages 9133-9143. PMLR, 2020.
* Tjeng et al. (2019) Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer programming. In _International Conference on Learning Representations_, 2019.
* Tjeng et al. (2019)Kim P Wabersich and Melanie N Zeilinger. Linear model predictive safety certification for learning-based control. In _2018 IEEE Conference on Decision and Control (CDC)_, pages 7130-7135. IEEE, 2018.
* Wang et al. (2021) Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. Beta-CROWN: Efficient bound propagation with per-neuron split constraints for complete and incomplete neural network verification. _Advances in Neural Information Processing Systems_, 34, 2021.
* Wang et al. (2023a) Yixuan Wang, Simon Sinong Zhan, Ruochen Jiao, Zhilu Wang, Wanxin Jin, Zhuoran Yang, Zhaoran Wang, Chao Huang, and Qi Zhu. Enforcing hard constraints with soft barriers: safe reinforcement learning in unknown stochastic environments. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23. JMLR, 2023a.
* Wang et al. (2023b) Yixuan Wang, Weichao Zhou, Jiameng Fan, Zhilu Wang, Jiajun Li, Xin Chen, Chao Huang, Wenchao Li, and Qi Zhu. Polar-express: Efficient and precise formal reachability analysis of neural-network controlled systems. _IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems_, 2023b.
* Wei and Liu (2022) Tianhao Wei and Changliu Liu. Safe control with neural network dynamic models. In _Learning for Dynamics and Control Conference_, pages 739-750. PMLR, 2022.
* Wei et al. (2022) Tianhao Wei, Shucheng Kang, Weiye Zhao, and Changliu Liu. Persistently feasible robust safe control by safety index synthesis and convex semi-infinite programming. _IEEE Control Systems Letters_, 7:1213-1218, 2022.
* Wu and Vorobeychik (2022) Junlin Wu and Yevgeniy Vorobeychik. Robust deep reinforcement learning through bootstrapped opportunistic curriculum. In _International Conference on Machine Learning_, pages 24177-24211. PMLR, 2022.
* Xiong et al. (2024) Nuoya Xiong, Yihan Du, and Longbo Huang. Provably safe reinforcement learning with step-wise violation constraints. _Advances in Neural Information Processing Systems_, 36, 2024.
* Xu et al. (2021) Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast and Complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. In _International Conference on Learning Representations_, 2021.
* Yu et al. (2022) Dongjie Yu, Haitong Ma, Shengbo Li, and Jianyu Chen. Reachability constrained reinforcement learning. In _International Conference on Machine Learning_, pages 25636-25655. PMLR, 2022.
* Zhang et al. (2018) Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. In _Advances in Neural Information Processing Systems_, pages 4944-4953, 2018.
* Zhang et al. (2022) Huan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, and J. Zico Kolter. General cutting planes for bound-propagation-based neural network verification. In _Proceedings of the 36th International Conference on Neural Information Processing Systems_, 2022.

Appendix

### Ablation Study

In this section, we conduct an ablation study to evaluate the importance of both incremental verification and the use of multiple initial-state-dependent controllers as part of the VSRL approach.

Table 2 presents the ablation study results for incremental verification. To ensure a fair comparison, we record the runtime for \(20\) training epochs with only one region from the grid split for all environments. In practice, this process can be run on GPUs in parallel for multiple regions. Given that the neural network structures for the 2D Quadrotor environment with both moving and fixed obstacles are the same, the runtime results are similar; therefore, we report these collectively as 2D Quadrotor. The results indicate that incremental verification significantly reduces the average verification time during training. Without incremental verification, the verification time increases rapidly as the number of steps increases.

Table 3 shows the ablation study results for using multiple initial-state-dependent controllers. We report results for the Vehicle Avoidance environment (Veh. Avoid.), 2D Quadrotor with fixed obstacles (2D-Quad (F)), moving obstacles (2D-Quad (M)), and 3D Quadrotor (3D-Quad (F)). We exclude the Lane Following environment from this comparison, as only one controller was used there to achieve \(100\%\) verified safety. The results demonstrate that using multiple controllers significantly enhances the ability to achieve verified safety across a larger initial state region \(\mathcal{S}_{0}\).

### Experiment Setup

**Lane Following Our lane following environment follows the discrete-time bicycle model (Kong et al., 2015)**

\[\dot{x}=v\cos(\theta+\beta)\]

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & \multicolumn{4}{c}{Lane Following} \\  & \(5\)-step \((\downarrow)\) & \(10\)-step \((\downarrow)\) & \(15\)-step \((\downarrow)\) & \(20\)-step \((\downarrow)\) \\ \hline w/ Incr. Veri. & \(\mathbf{9.4}\) & \(\mathbf{9.4}\) & \(\mathbf{17.4}\) & \(\mathbf{24.8}\) \\ w/o Incr. Veri. & \(9.6\) & \(38.1\) & \(105.9\) & \(185.5\) \\ \hline \hline \end{tabular} \begin{tabular}{l l l l l} \hline \hline  & \multicolumn{4}{c}{Vehicle Avoidance} \\  & \(5\)-step \((\downarrow)\) & \(10\)-step \((\downarrow)\) & \(15\)-step \((\downarrow)\) & \(20\)-step \((\downarrow)\) \\ \hline w/ Incr. Veri. & \(\mathbf{14.0}\) & \(\mathbf{16.1}\) & \(\mathbf{19.8}\) & \(\mathbf{25.5}\) \\ w/o Incr. Veri. & \(14.1\) & \(47.6\) & \(110.5\) & \(187.1\) \\ \hline \hline \end{tabular} \begin{tabular}{l l l l l} \hline \hline  & \multicolumn{4}{c}{2D Quadrotor} \\  & \(5\)-step \((\downarrow)\) & \(10\)-step \((\downarrow)\) & \(15\)-step \((\downarrow)\) & \(20\)-step \((\downarrow)\) \\ \hline w/ Incr. Veri. & \(\mathbf{8.4}\) & \(\mathbf{13.7}\) & \(\mathbf{15.9}\) & \(\mathbf{19.6}\) \\ w/o Incr. Veri. & \(8.8\) & \(35.8\) & \(86.8\) & \(152.1\) \\ \hline \hline \end{tabular} 
\begin{tabular}{l l l l l} \hline \hline  & \multicolumn{4}{c}{3D Quadrotor} \\  & \(5\)-step \((\downarrow)\) & \(6\)-step \((\downarrow)\) & \(7\)-step \((\downarrow)\) & \(8\)-step \((\downarrow)\) \\ \hline w/ Incr. Veri. & \(31.0\) & \(\mathbf{31.9}\) & \(\mathbf{36.7}\) & \(\mathbf{49.9}\) \\ w/o Incr. Veri. & \(\mathbf{30.9}\) & \(61.6\) & \(149.9\) & \(403.6\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Runtime (in seconds) for \(20\) training epochs with and without incremental verification.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & Veh. Avoid. \((\uparrow)\) & 2D-Quad (F) \((\uparrow)\) & 2D-Quad (M) \((\uparrow)\) & 3D-Quad (F) \((\uparrow)\) \\ \hline Single Ctrl. & \(99.0\) & \(97.6\) & \(96.9\) & \(74.7\) \\ Multi Ctrl. & \(\mathbf{100.0}\) & \(\mathbf{100.0}\) & \(\mathbf{100.0}\) & \(\mathbf{100.0}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Percentage of regions in \(\mathcal{S}_{0}\) that can be verified for safety for \(K\) steps (Verified-\(K\)).

\[\dot{y} =v\sin(\theta+\beta)\] \[\dot{\theta} =\frac{v}{l_{r}}\sin(\beta)\] \[\dot{v} =a\] \[\beta =\tan^{-1}\left(\frac{l_{r}}{l_{f}+l_{r}}\tan(\delta_{f})\right)\]

where we set the wheel base of the vehicle to \(2.9\)m. The model inputs are 3-dimensional \((x,\theta,v)\), where \(x\) is the lateral distance to the center of the lane, \(\theta\) is the angle relative to the center of the lane, and \(v\) represents the speed. The objective is to maintain a constant speed while following the lane, meaning the system equilibrium point is \((x,\theta,v)=(0,0,v_{\text{target}})\). The safety constraints are

1. \(x\) stays within a maximum distance from the lane center (\(\|x\|\leq d_{\text{max}}\)),
2. \(\theta\) remains within a predefined range (\(\|\theta\|\leq\theta_{\text{max}}\)),
3. \(v\) does not exceed the maximum threshold (\(v\leq v_{\text{max}}\)).

The parameters are set as \(d_{\text{max}}=0.7\), \(\theta_{\text{max}}=\pi/4\), and \(v_{\text{max}}=5.0\). The initial regions \(\mathcal{S}_{0}\) is \(x\in[-0.5,0.5],\theta\in[-0.2,0.2],v\in[0.0,0.5]\). The reward received at each step is measured as the distance to the equilibrium point. More specifically, for a state that is of distance \(d\) to the target equilibrium point, the reward is \(e^{-d}\). For VSRL training, our controller is initialized using a controller pretrained with a safe RL algorithm. When training with the bound loss, we add a large penalty on unsafe states to incentivize maintaining safety throughout the entire trajectory. For branch and bound during verification, we set the precision limit as \(0.025\), which means as soon as the precision of the grid region reaches this precision, we stop branching. For the dynamics approximation, we use an NN with two layers of ReLU each of size \(8\).

Vehicle AvoidanceOur vehicle avoidance environment features a vehicle moving on an \(x\)-\(y\) plane, with 4-dimensional inputs \((x,y,\theta,v)\). Here, \((x,y)\) represents the location of the vehicle on the plane, \(\theta\) is the angle relative to the \(y\)-axis, and \(v\) is the speed. In this setting, we have five moving obstacles, each moving from one point to another at a constant speed for the duration of \(500\) steps. The five obstacles are: 1) moving from \((x,y)=(-0.6,1.0)\) to \((x,y)=(-0.35,2.0)\); 2) moving from \((x,y)=(0.6,0.0)\) to \((x,y)=(0.75,1.0)\); 3) moving from \((x,y)=(0.0,1.0)\) to \((x,y)=(0.0,2.0)\); 4) moving from \((x,y)=(-0.85,1.0)\) to \((x,y)=(-1.6,1.5)\); 5) moving from \((x,y)=(0.75,0.0)\) to \((x,y)=(0.85,0.0)\). Each obstacle is represented as a square with a diameter of \(0.1\). Additionally, safety constraints are set for the speed \((v\leq v_{\text{max}})\) and angle (\(\|\theta\|\leq\theta_{\text{max}}\)), where \(v_{\text{max}}=5.0\) and \(\theta_{\text{max}}=\pi/2\). The task is to navigate the vehicle to a designated location while following safety constraints. The agent starts near the origin within an area defined by \(x,y\in[-0.5,0.5]\), \(\theta\in[-0.2,0.2]\), and \(v\in[0,0.1]\), and the goal is \((x_{\text{target}},y_{\text{target}})=(1.0,2.0)\). The branching precision limit is \(0.025\) and for dynamics approximation, we use a NN with two layers of ReLU each of size \(10\).

2D QuadrotorFor the 2D quadrotor environment, we follow the settings in Dai et al. (2021).

\[\dot{x} =-\frac{\sin(\theta)}{m}\cdot(u_{0}+u_{1})\] \[\dot{y} =\frac{\cos(\theta)}{m}\cdot(u_{0}+u_{1})-g\] \[\dot{\theta} =\frac{\text{length}}{I}\cdot(u_{0}-u_{1})\]

We use a timestep \(dt=0.02\), the mass of the quadrotor is set to \(m=0.486\), the length to \(l=0.25\), the inertia to \(I=0.00383\), and gravity to \(g=9.81\). The input is 6-dimensional \((y,z,\theta,\dot{y},\dot{z},\dot{\theta})\), where \((y,z)\) represents the position of the quadrotor on the \(y\)-\(z\) plane, and \(\theta\) represents the angle. The action space is 2-dimensional and continuous; the actions are clipped within a range to reflect motor constraints. Our safety criteria are

1. angle \(\theta\) remains within a predefined range (\(\|\theta\|\leq\theta_{\text{max}}\)),
2. a minimum height constraint to prevent collision with the ground \((y\geq y_{\text{min}})\),
3. avoid obstacles.

Here we set \(\theta_{\text{max}}=\pi/3\) and \(y_{\text{min}}=-0.2\). The task is for the quadrotor to navigate towards the goal while following safety constraints. We consider two scenarios for obstacles: fixed and moving. For fixed obstacles, there are five rectangular obstacles positioned in the \(y\)-\(z\) plane. We use \((x_{l},x_{u},y_{l},y_{u})\) to represent the two dimensional box, and the obstacles are: \((x_{l},x_{u},y_{l},y_{u})=(-0.3,-0.1,0.4,0.6)\), \((x_{l},x_{u},y_{l},y_{u})=(-1.2,-0.8,0.2,0.4)\), \((x_{l},x_{u},y_{l},y_{u})=(0.0,0.1,0.5,1.0)\), \((x_{l},x_{u},y_{l},y_{u})=(0.6,0.7,0.0,0.2)\), \((x_{l},x_{u},y_{l},y_{u})=(-0.8,-0.7,0.7,0.9)\). For moving obstacles, there are five obstacles that moves from one point to another at constant speed for the duration of \(500\) steps, each represented as a square of diameter \(0.1\). The obstacles are: 1) moving from \((x,y)=(0.6,0.0)\) to \((x,y)=(0.6,0.1)\); 2) moving from \((x,y)=(-0.5,0.2)\) to \((x,y)=(-0.4,0.3)\); 3) moving from \((x,y)=(-0.3,0.4)\) to \((x,y)=(-0.4,0.5)\); 4) moving from \((x,y)=(-0.1,0.3)\) to \((x,y)=(0.0,0.4)\); 5) moving from \((x,y)=(-0.7,0.5)\) to \((x,y)=(-0.4,0.6)\). The initial region for the quadrotor is defined with \(x\in[-0.5,0.5]\) and the remaining state variables within \([-0.1,0.1]\). The target goal is set to \((x,y)=(0.6,0.6)\). We set the branching limit to \(0.0125\) and for dynamics approximation we use a NN with two layers of ReLU each of size \(6\).

3D QuadrotorOur 3D quadrotor environment features a 12-dimensional input space, represented as \((x,y,z,\phi,\theta,\psi,\dot{x},\dot{y},\dot{z},\omega_{x},\omega_{y},\omega_{ z})\). The action space is 4-dimensional and continuous; the actions are clipped within a range to reflect motor constraints. Here, \((x,y,z)\) denotes the location of the quadrotor in space, \(\phi\) is the roll angle, \(\theta\) is the pitch angle, and \(\dot{\psi}\) is the yaw angle, \(\omega_{x},\omega_{y},\omega_{z}\) represent the angular velocity around the \(x\), \(y\), and \(z\) axes, respectively. The environment setting and neural network dynamics approximation follows the setup in Dai et al. (2021), with the modification of using ReLU activations instead of LeakyReLU. The system dynamics is:

plant_input \[=\begin{bmatrix}1&1&1&1\\ 0&L&0&-L\\ -L&0&L&0\\ \kappa_{z}&-\kappa_{z}&\kappa_{z}&-\kappa_{z}\end{bmatrix}\cdot u\] \[R =\text{rpy2rotant}(\phi,\theta,\psi)\] \[\ddot{\mathbf{p}} =\begin{bmatrix}0\\ 0\\ -g\end{bmatrix}+R\cdot\begin{bmatrix}0\\ 0\\ \text{plant\_input}[0]/m\end{bmatrix}\] \[\dot{\bm{\omega}} =\frac{-\bm{\omega}\times(I\cdot\bm{\omega})+\text{plant\_input }[1:]}{I}\] \[\begin{bmatrix}\dot{\phi}\\ \dot{\theta}\\ \dot{\psi}\end{bmatrix} =\begin{bmatrix}1&\sin(\phi)\cdot\tan(\theta)&\cos(\phi)\cdot \tan(\theta)\\ 0&\cos(\phi)&-\sin(\phi)\\ 0&\frac{\sin(\phi)}{\cos(\phi)}&\frac{\cos(\phi)}{\cos(\theta)}\end{bmatrix} \cdot\bm{\omega}\]

The dynamics neural network has two ReLU layers, each with a size of \(16\) and \(dt=0.02\). We set the branching precision limit to \(0.00625\). The task is to navigating towards the goal while avoiding five obstacles represented as 3D rectangles. The locations of the obstacles are \((-0.5,0.5,-0.2,0.2,-0.65,-0.55),(-0.7,-0.6,-0.1,0.1,-0.5,-0.4),(0.5,0.6,-0.2,0.2, -0.4,-0.3),(-0.8,-0.6,0.2,0.4,-0.3,-0.2),(-0.8,-0.6,-0.4,-0.2,-0.2,-0.1)\), where the first obstacle is to avoid controller collide with the ground. We set the goal at \((x,y,z)=(0.0,0.0,0.0)\), and the initial region is defined with \(x\in[-0.5,0.5]\), \(y\in[-0.1,0.1]\), and \(z\in[-0.5,-0.3]\), with the remaining variables confined to the range \([-0.05,0.05]\). The reward is calculated based on the distance to the goal, where the agent receives a higher reward for being closer to the goal. The environment episodes end if either the magnitude of \(\phi\) or \(\theta\) exceeds \(\pi/3\).

### Compute Resources

Our code runs on an AMD Ryzen 9 5900X CPU with a 12-core processor and an NVIDIA GeForce RTX 3090 GPU.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Abstract and Introduction Section Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: we discuss the primary limitation of the work in the conclusion. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide details of the experimental setup both in Section A.2 and in the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We will provide all code and data on github, along with documentation, to facilitate reproduction of the experiments if the paper is accepted for publication. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide this information in Section 4 and the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report confidence intervals in the results table in the experiments (see Section 4). Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Justification: We provide this information in the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: All aspects of the paper and research conform in all respects to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: As mentioned in the introduction, ability to synthesize safe controllers is a core challenge in autonomous systems (such as autonomous cars). As such, our work is expected to have positive broader impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper is about safe reinforcement learning, and poses no such risk. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper references the Auto_Lirpa codebase and associated research papers describing it. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.