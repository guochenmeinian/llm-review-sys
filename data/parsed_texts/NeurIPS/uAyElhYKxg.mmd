# (S)GD over Diagonal Linear Networks:

Implicit Bias, Large Stepsizes and Edge of Stability

 Mathieu Even

Inria - ENS Paris

&Scott Pesme

EPFL

&Suriya Gunasekar

Microsoft Research

&Nicolas Flammarion

EPFL

Denotes equal contribution

###### Abstract

In this paper, we investigate the impact of stochasticity and large stepsizes on the implicit regularisation of gradient descent (GD) and stochastic gradient descent (SGD) over \(2\)-layer diagonal linear networks. We prove the convergence of GD and SGD with macroscopic stepsizes in an overparametrised regression setting and provide a characterisation of their solution through an implicit regularisation problem. Our characterisation provides insights on how the choice of minibatch sizes and stepsizes lead to qualitatively distinct behaviors in the solutions. Specifically, we show that for sparse regression learned with \(2\)-layer diagonal linear networks, large stepsizes consistently benefit SGD, whereas they can hinder the recovery of sparse solutions for GD. These effects are amplified for stepsizes in a tight window just below the divergence threshold, known as the "edge of stability" regime.

## 1 Introduction

The stochastic gradient descent algorithm (SGD) [51] is the foundational algorithm for almost all neural network training. Though a remarkably simple algorithm, it has led to many impressive empirical results and is a key driver of deep learning. However the performances of SGD are quite puzzling from a theoretical point of view as (1) its convergence is highly non-trivial and (2) there exist many global minimums for the training objective which generalise very poorly [66].

To explain this second point, the concept of implicit regularisation has emerged: if overfitting is harmless in many real-world prediction tasks, it must be because the optimisation process is _implicitly favoring_ solutions that have good generalisation properties for the task. The canonical example is overparametrised linear regression with more trainable parameters than number of samples: although there are infinitely many solutions that fit the samples, GD and SGD explore only a small subspace of all the possible parameters. As a result, it can be shown that they implicitly converge to the closest solution in terms of the \(\ell_{2}\) distance, and this without explicit regularisation [66; 24].

Currently, most theoretical works on implicit regularisation have primarily focused on continuous time approximations of (S)GD where the impact of crucial hyperparameters such as the stepsize and the minibatch size are ignored. One such common simplification is to analyse gradient flow, which is a continuous time limit of GD and minibatch SGD with an infinitesimal stepsize. By definition, this analysis does not capture the effect of stepsize or stochasticity. Another approach is to approximate SGD by a stochastic gradient flow [60; 48], which tries to capture the noise and the stepsize using an appropriate stochastic differential equation. However, there are no theoretical guarantees that these results can be transferred to minibatch SGD as used in practice. This is a limitation in our understanding since the performances of most deep learning models are often sensitive to the choice of stepsize and minibatch size. The importance of stepsize and SGD minibatch size is common knowledge in practice and has also been systematically established in controlled experiments [36; 42; 20].

In this work, we aim to expand our understanding of the impact of stochasticity and stepsizes by analysing the (S)GD trajectory in \(2\)-layer diagonal networks (DLNs). In Fig. 1, we show that even in our simple network, there are significant differences between the nature of the solutions recovered by SGD and GD at macroscopic stepsizes. We discuss this behavior further in the later sections.

The \(2\)-layer diagonal linear network which we consider is a simplified neural network that has received significant attention lately [61; 57; 26; 50]. Despite its simplicity, it surprisingly reveals training characteristics which are observed in much more complex architectures, such as the role of the initialisation [61], the role of noise [48; 50], or the emergence of saddle-to-saddle dynamics [6; 49]. It therefore serves as an ideal proxy model for gaining a deeper understanding of complex phenomenons such as the roles of stepsizes and of stochasticity as highlighted in this paper. We also point out that implicit bias and convergence for more complex architectures such as 2-layer ReLU networks, matrix multiplication are not yet fully understood, even for the simple gradient flow. Therefore studying the subtler effects of large stepsizes and stochasticity in these settings is currently out of reach.

### Main results and paper organisation

The overparametrised regression setting and diagonal linear networks are introduced in Section 2. We formulate our theoretical results (Theorems 1 and 2) in Section 3: we prove that for **macroscopic stepsizes**, gradient descent and stochastic gradient descent over \(2\)-layer diagonal linear networks converge to a zero-training loss solution \(\beta_{\infty}^{*}\). We further provide a refined characterization of \(\beta_{\infty}^{*}\) through a trajectory-dependent implicit regularisation problem, that captures the effects of hyperparameters of the algorithm, such as stepsizes and batchsizes, in useful and analysable ways. In Section 4 we then leverage this crisp characterisation to explain the influence of crucial parameters such as the stepsize and batch-size on the recovered solution. Importantly **our analysis shows a stark difference between the generalisation performances of GD and SGD for large stepsizes**, hence explaining the numerical results seen in Fig. 1 for the sparse regression setting. Finally, in Section 5, we use our results to shed new light on the _Edge of Stability_ (_EoS_) phenomenon [14].

### Related works

**Implicit bias.** The concept of implicit bias from optimization algorithm in neural networks has been studied extensively in the past few years, starting with early works of Telgarsky [55], Neyshabur et al. [45], Keskar et al. [36], Soudry et al. [53]. The theoretical results on implicit regularisation have been extended to multiplicative parametrisations [23; 25], linear networks [34], and homogeneous networks [40; 35; 13]. For regression loss on diagonal linear networks studied in this work, Woodworth et al. [61] demonstrate that the scale of the initialisation determines the type of solution obtained, with large initialisations yielding minimum \(\ell_{2}\) norm solutions--the neural tangent kernel regime [30] and small initialisation resulting in minimum \(\ell_{1}\) norm solutions--the _rich regime_[13]. The analysis relies on the link between gradient descent and mirror descent established by Ghai et al. [21] and further explored by Vaskevicius et al. [56], Wu and Rebeschini [62]. These works focus on full batch gradient, and often in the inifitesimal stepsize limit (gradient flow), leading to general insights and results that do not take into account the effects of stochasticity and large stepsizes.

**The effect of stochasticity in SGD on generalisation.** The relationship between stochasticity in SGD and generalisation has been studied in various works [41; 29; 11; 38; 64]. Empirically, models generated by SGD exhibit better generalisation performance than those generated by GD [37; 31; 27].

Figure 1: Noiseless sparse regression with a diagonal linear network using SGD and GD, with parameters initialized at the scale of \(\alpha=0.1\) (Section 2). The test losses at convergence for various stepsizes are plotted for GD and SGD. Small stepsizes correspond to gradient flow (GF) performance. We see that increasing the stepsize improves the generalisation properties of SGD, but deteriorates that of GD. The dashed vertical lines at stepsizes \(\tilde{\gamma}_{\max}^{\mathrm{GD}}\) and \(\tilde{\gamma}_{\max}^{\mathrm{GD}}\) denote the largest stepsizes for which SGD and GD, respectively, converge. See Section 2 for the precise experimental setting.

Explanations related to the flatness of the minima picked by SGD have been proposed [28]. Label noise has been shown to influence the implicit bias of SGD [26; 8; 15; 50] by implicitly regularising the sharp minimisers. Recently, studying a _stochastic gradient flow_ that models the noise of SGD in continuous time with Brownian diffusion, Pesme et al. [48] characterised for diagonal linear networks the limit of their stochastic process as the solution of an implicit regularisation problem. However similar explicit characterisation of the implicit bias remains unclear for SGD with large stepsizes.

**The effect of stepsizes in GD and SGD.** Recent efforts to understand how the choice of stepsizes affects the learning process and the properties of the recovered solution suggest that larger stepsizes lead to the minimisation of some notion of flatness of the loss function [52; 37; 44; 33; 64; 43], backed by empirical evidences or stability analyses. Larger stepsizes have also been proven to be beneficial for specific architectures or problems: two-layer network [39], regression [63], kernel regression [7] or matrix factorisation [59]. For large stepsizes, it has been observed that GD enters an _Edge of Stability (EoS)_ regime [32; 14], in which the iterates and the train loss oscillate before converging to a zero-training error solution; this phenomenon has then been studied on simple toy models [1; 67; 12; 16] for GD. Recently, [2] presented empirical evidence that large stepsizes can lead to loss stabilisation and towards simpler predictors.

## 2 Setup and preliminaries

**Overparametrised linear regression.** We consider a linear regression over inputs \(X=(x_{1},\ldots,x_{n})\in(\mathbb{R}^{d})^{n}\) and outputs \(y=(y_{1},\ldots,y_{n})\in\mathbb{R}^{n}\). We consider _overparametrised_ problems where input dimension \(d\) is (much) larger than the number of samples \(n\). In this case, there exists infinitely many linear predictors \(\beta^{\star}\in\mathbb{R}^{d}\) which perfectly fit the training set, _i.e._, \(y_{i}=\langle\beta^{\star},x_{i}\rangle\) for all \(1\leqslant i\leqslant n\). We call such vectors _interpolating predictors_ or _interpolators_ and we denote by \(\mathcal{S}\) the set of all interpolators \(\mathcal{S}=\{\beta^{\star}\in\mathbb{R}^{d}\ {\rm s.t.}\ \langle\beta^{\star},x_{i} \rangle=y_{i},\forall i\in[n]\}\). Note that \(\mathcal{S}\) is an affine space of dimension greater than \(d-n\) and equal to \(\beta^{\star}+{\rm span}(x_{1},\ldots,x_{n})^{\perp}\) for any \(\beta^{\star}\in\mathcal{S}\). We consider the following quadratic loss: \(\mathcal{L}(\beta)=\frac{1}{2n}\sum_{i=1}^{n}(\langle\beta,x_{i}\rangle-y_{i} )^{2}\), for \(\beta\in\mathbb{R}^{d}\).

**2-layer linear diagonal network.** We parametrise regression vectors \(\beta\) as functions \(\beta_{w}\) of trainable parameters \(w\in\mathbb{R}^{p}\). Although the final prediction function \(x\mapsto\langle\beta_{w},x\rangle\) is linear in the input \(x\), the choice of the parametrisation drastically changes the solution recovered by the optimisation algorithm [25]. In the case of the linear parametrisation \(\beta_{w}=w\) many first-order methods (SGD, GD, with or without momentum) converge towards the same solution and the choice of stepsize does not impact the recovered solution beyond convergence. In an effort to better understand the effects of stochasticity and large stepsize, we consider the next simple parametrisation, that of a \(2\)-layer diagonal linear neural network given by:

\[\beta_{w}=u\odot v\text{ where }w=(u,v)\in\mathbb{R}^{2d}\,.\] (1)

This parametrisation can be viewed as a simple neural network \(x\mapsto\langle u,\sigma({\rm diag}(v)x)\rangle\) where the output weights are represented by \(u\), the inner weights is the diagonal matrix \({\rm diag}(v)\), and the activation \(\sigma\) is the identity function. In this spirit, we refer to the entries of \(w=(u,v)\in\mathbb{R}^{2d}\) as the _weights_ and to \(\beta\coloneqq u\odot v\in\mathbb{R}^{d}\) as the _prediction parameter_. Despite the simplicity of the parametrisation (1), the loss function \(F\) over parameters \(w=(u,v)\in\mathbb{R}^{2d}\) is **non-convex** (and thus the corresponding optimization problem is challenging to analyse), and is given by:

\[F(w)\coloneqq\mathcal{L}(u\odot v)=\frac{1}{2n}\sum_{i=1}^{n}(y_{i}-\langle u \odot v,x_{i}\rangle)^{2}\,.\] (2)

**Mini-batch SGD.** We minimise \(F\) using mini-batch SGD: let \(w_{0}=(u_{0},v_{0})\) and for \(k\geqslant 0\),

\[w_{k+1}=w_{k}-\gamma_{k}\nabla F_{\mathcal{B}_{k}}(w_{k})\,,\quad\text{where} \quad F_{\mathcal{B}_{k}}(w)\coloneqq\frac{1}{2b}\sum_{i\in\mathcal{B}_{k}}(y_ {i}-\langle u\odot v,x_{i}\rangle)^{2}\,,\] (3)

where \(\gamma_{k}\) are stepsizes, \(\mathcal{B}_{k}\subset[n]\) are mini-batches of \(b\in[n]\) distinct samples sampled uniformly and independently, and \(\nabla F_{\mathcal{B}_{k}}(w_{k})\) are minibatch gradients of partial loss over \(\mathcal{B}_{k}\), \(F_{\mathcal{B}_{k}}(w)\coloneqq\mathcal{L}_{\mathcal{B}_{k}}(u\odot v)\) defined above. Classical SGD and full-batch GD are special cases with \(b=1\) and \(b=n\), respectively. For \(k\geqslant 0\), we consider the successive prediction parameters \(\beta_{k}\coloneqq u_{k}\odot v_{k}\) built from the weights\(w_{k}=(u_{k},v_{k})\). We analyse SGD initialised at \(u_{0}=\sqrt{2}\bm{\alpha}\in\mathbb{R}^{d}_{>0}\) and \(v_{0}=\bm{0}\in\mathbb{R}^{d}\), resulting in \(\beta_{0}=\bm{0}\in\mathbb{R}^{d}\) independently of the chosen weight initialisation \(\bm{\alpha}^{2}\).

**Experimental details.** We consider the noiseless sparse regression setting where \((x_{i})_{i\in[n]}\sim\mathcal{N}(0,I_{d})\) and \(y_{i}=\langle\beta_{\ell_{1}}^{\star},x_{i}\rangle\) for some \(s\)-sparse vector \(\beta_{\ell_{1}}^{\star}\). We perform (S)GD over the DLN with a uniform initialisation \(\bm{\alpha}=\alpha\bm{1}\in\mathbb{R}^{d}\) where \(\alpha>0\). Fig. 1 and Fig. 2 (left) correspond to the setup \((n,d,s,\alpha)=(20,30,3,0.1)\), Fig. 2 (right) to \((n,d,s,\alpha)=(50,100,4,0.1)\) and Fig. 3 to \((n,d,s,\alpha)=(50,100,2,0.1)\).

**Notations.** Let \(H\coloneqq\nabla^{2}\mathcal{L}=\frac{1}{n}\sum_{i}x_{i}x_{i}^{\top}\) denote the Hessian of \(\mathcal{L}\), and for a batch \(\mathcal{B}\subset[n]\) let \(H_{\mathcal{B}}\coloneqq\nabla^{2}\mathcal{L}_{\mathcal{B}}=\frac{1}{| \mathcal{B}|}\sum_{i\in\mathcal{B}}x_{i}x_{i}^{\top}\) denote the Hessian of the partial loss over the batch \(\mathcal{B}\). Let \(L\) denote the "smoothness" such that \(\forall\beta\), \(\left\|H_{\mathcal{B}}\beta\right\|_{2}\leqslant L\|\beta\|_{2}\), \(\left\|H_{\mathcal{B}}\beta\right\|_{\infty}\leqslant L\|\beta\|_{\infty}\) for all batches \(\mathcal{B}\subset[n]\) of size \(b\). A real function (e.g, \(\log,\exp\)) applied to a vector must be understood as element-wise application, and for vectors \(u,v\in\mathbb{R}^{d}\), \(u^{2}=(u_{i}^{2})_{i\in[d]}\), \(u\odot v=(u_{i}v_{i})_{i\in[d]}\) and \(u/v=(u_{i}/v_{i})_{i\in[d]}\). We write \(\bm{1}\), \(\bm{0}\) for the constant vectors with coordinates \(1\) and \(0\) respectively. The Bregman divergence [9] of a differentiable convex function \(h:\mathbb{R}^{d}\to\mathbb{R}\) is defined as \(D_{h}(\beta_{1},\beta_{2})=h(\beta_{1})-(h(\beta_{2})+\langle\nabla h(\beta_{ 2}),\beta_{1}-\beta_{2}\rangle)\).

## 3 Implicit bias of SGD and GD

We start by recalling some known results on the implicit bias of gradient flow on diagonal linear networks before presenting our main theorems on characterising the (stochastic) gradient descent solutions (Theorem 1) as well as proving the convergence of the iterates (Theorem 2).

### Warmup: gradient flow

We first review prior findings on gradient flow on diagonal linear neural networks. Woodworth et al. [61] show that the limit \(\beta_{\bm{\alpha}}^{*}\) of the _gradient flow_\(\mathrm{d}w_{t}=-\nabla F(w_{t})\mathrm{d}t\) initialised at \((u_{0},v_{0})=(\sqrt{2}\bm{\alpha},\bm{0})\) is the solution of the minimal interpolation problem:

\[\beta_{\bm{\alpha}}^{*}=\operatorname*{argmin}_{\beta^{*}\in\mathcal{S}}\, \psi_{\bm{\alpha}}(\beta^{*})\,,\quad\text{where}\quad\psi_{\bm{\alpha}}( \beta)=\frac{1}{2}\sum_{i=1}^{d}\Big{(}\beta_{i}\mathrm{arcsinh}(\frac{\beta_{ i}}{\alpha_{i}^{2}})-\sqrt{\beta_{i}^{2}+\alpha_{i}^{4}}+\alpha_{i}^{2} \Big{)}\,.\] (4)

The convex potential \(\psi_{\bm{\alpha}}\) is the **hyperbolic entropy function** (or **hyper entropy**) [21]. Depending on the structure of the vector \(\bm{\alpha}\), the generalisation properties of \(\beta_{\bm{\alpha}}^{\star}\) highly vary. We point out the two main characteristics of \(\bm{\alpha}\) that affect the behaviour of \(\psi_{\bm{\alpha}}\) and therefore also the solution \(\beta_{\bm{\alpha}}^{\star}\).

**1.** The **Scale** of \(\bm{\alpha}\). For an initialisation vector \(\bm{\alpha}\) we call the \(\ell_{1}\)-norm \(\|\bm{\alpha}\|_{1}\) the **scale** of the initialisation. It is an important quantity affecting the properties of the recovered solution \(\beta_{\bm{\alpha}}^{\star}\). To see this let us consider a uniform initialisation of the form \(\bm{\alpha}=\alpha\bm{1}\) for a scalar value \(\alpha>0\). In this case the potential \(\psi_{\bm{\alpha}}\) has the property of resembling the \(\ell_{1}\)-norm as the scale \(\alpha\) vanishes: \(\psi_{\bm{\alpha}}\sim\ln(1/\alpha)\|_{1}\) as \(\alpha\to 0\). Hence, a small initialisation results in a low \(\ell_{1}\)-norm solution which is known to induce sparse recovery guarantees [10]. This setting is often referred to as the "rich" regime [61]. In contrast, using a large initialisation scale leads to solutions with low \(\ell_{2}\)-norm: \(\psi_{\bm{\alpha}}\sim\|_{1}\|_{2}^{2}/(2\alpha^{2})\) as \(\alpha\to\infty\), a setting known as the "kernel" or "lazy" regime. Overall, to retrieve the minimum \(\ell_{1}\)-norm solution, one should use a uniform initialisation with small scale \(\alpha\), see Fig. 7 in Appendix D for an illustration and [61, Theorem 2] for a precise characterisation.

**2.** The **Shape** of \(\bm{\alpha}\). In addition to the scale of the initialisation \(\bm{\alpha}\), a lesser studied aspect is its "shape", which is a term we use to refer to the relative distribution of \(\{\alpha_{i}\}_{i}\) along the \(d\) coordinates [3]. It is a crucial property because having \(\bm{\alpha}\to\bm{0}\)**does not** necessarily lead to the potential \(\psi_{\bm{\alpha}}\) being close to the \(\ell_{1}\)-norm. Indeed, we have that \(\psi_{\bm{\alpha}}(\beta)\stackrel{{\bm{\alpha}\to\bm{0}}}{{ \sim}}\sum_{i=1}^{d}\ln(\frac{1}{\alpha_{i}})|\beta_{i}|\) (see Appendix D), therefore if the vector \(\ln(1/\bm{\alpha})\) has entries changing at different rates, then \(\psi_{\bm{\alpha}}(\beta)\) is a **weighted**\(\ell_{1}\)-norm. In words, if the entries of \(\bm{\alpha}\)_do not go to zero "uniformly"_, then the resulting implicit bias minimizes a weighed \(\ell_{1}\)-norm. This phenomenon can lead to solutions with vastly different sparsity structure than the minimum \(\ell_{1}\)-norm interpolator. See Fig. 7 and Example 1 in Appendix D.

### Implicit bias of (stochastic) gradient descent

In Theorem 1, we prove that for an initialisation \(\sqrt{2}\bm{\alpha}\in\mathbb{R}^{d}\) and for **arbitrary** stepsize sequences \((\gamma_{k})_{k\geqslant 0}\)**if the iterates converge to an interpolator**, then this interpolator is the solution of a constrained minimisation problem which involves the hyperbolic entropy \(\psi_{\bm{\alpha}_{\infty}}\) defined in (4), where \(\bm{\alpha}_{\infty}\in\mathbb{R}^{d}\) is an effective initialisation which depends on the trajectory and on the stepsize sequence. Later, **we prove the convergence of iterates for macroscopic step sizes** in Theorem 2.

**Theorem 1** (Implicit bias of (S)GD).: _Let \((u_{k},v_{k})_{k\geqslant 0}\) follow the mini-batch SGD recursion (3) initialised at \((u_{0},v_{0})=(\sqrt{2}\bm{\alpha},\bm{0})\) and with stepsizes \((\gamma_{k})_{k\geqslant 0}\). Let \((\beta_{k})_{k\geqslant 0}=(u_{k}\odot v_{k})_{k\geqslant 0}\) and assume that they converge to some interpolator \(\beta_{\infty}^{*}\in\mathcal{S}\). Then, \(\beta_{\infty}^{*}\) satisfies:_

\[\beta_{\infty}^{*}=\operatorname*{argmin}_{\beta^{*}\in\mathcal{S}}D_{\psi_{ \bm{\alpha}_{\infty}}}(\beta^{*},\tilde{\beta}_{0})\,,\] (5)

_where \(D_{\psi_{\bm{\alpha}_{\infty}}}\) is the Bregman divergence with hyperentropy potential \(\psi_{\bm{\alpha}_{\infty}}\) of the **effective initialisation \(\bm{\alpha}_{\infty}\)**, and \(\tilde{\beta}_{0}\) is a small **perturbation term**. The **effective initialisation \(\bm{\alpha}_{\infty}\)** is given by,_

\[\bm{\alpha}_{\infty}^{2}=\bm{\alpha}^{2}\odot\exp\left(-\sum_{k=0}^{\infty}q \big{(}\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\big{)}\right)\,,\] (6)

_where \(q(x)=-\frac{1}{2}\ln((1-x^{2})^{2})\) satisfies \(q(x)\geqslant 0\) for \(|x|\leqslant\sqrt{2}\), with the convention \(q(1)=+\infty\)._

_The **perturbation term**\(\tilde{\beta}_{0}\in\mathbb{R}^{d}\) is explicitly given by \(\tilde{\beta}_{0}=\frac{1}{2}\big{(}\bm{\alpha}_{+}^{2}-\bm{\alpha}_{-}^{2} \big{)}\), where \(q_{\pm}(x)=\mp 2x-\ln((1\mp x)^{2})\), and \(\bm{\alpha}_{\pm}^{2}=\bm{\alpha}^{2}\odot\exp\left(-\sum_{k=0}^{\infty}q_{ \pm}(\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}))\right)\)._

**Trajectory-dependent characterisation.** The characterisation of \(\beta_{\infty}^{*}\) in Theorem 1 holds for any stepsize schedule such that the iterates converge and goes beyond the continuous-time frameworks previously studied [61; 48]. The result even holds for adaptive stepsize schedules which keep the stepsize scalar such as AdaDelta [65]. An important aspect of our result is that \(\bm{\alpha}_{\infty}\) and \(\tilde{\beta}_{0}\) depend on the iterates' trajectory. Nevertheless, we argue that our formulation provides useful ingredients for understanding the implicit regularisation effects of (S)GD for this problem compared to trivial characterisations (such as _e.g._, \(\min_{\beta}\|\beta-\beta_{\infty}^{*}\|\)). Importantly, **the key parameters \(\bm{\alpha}_{\infty},\tilde{\beta}_{0}\) depend on crucial parameters such as the stepsize and noise in a useful and analysable manner**: understanding how they affect \(\bm{\alpha}_{\infty}\) and \(\tilde{\beta}_{0}\) coincides with understanding how they affect the recovered solution \(\beta_{\infty}^{*}\) and its generalisation properties. This is precisely the object of Sections 4 and 5 where we discuss the qualitative and quantitative insights from Theorem 1 in greater detail.

**The perturbation \(\tilde{\beta}_{0}\) can be ignored.** We show in Proposition 16, under reasonable assumptions on the stepsizes, that \(|\tilde{\beta}_{0}|\leqslant\bm{\alpha}^{2}\) and \(\bm{\alpha}_{\infty}\leqslant\bm{\alpha}\) (component-wise). The magnitude of \(\tilde{\beta}_{0}\) is therefore negligible in front of the magnitudes of \(\beta^{*}\in S\) and one can roughly ignore the term \(\tilde{\beta}_{0}\). Hence, the implicit regularisation Eq. (5) can be thought of as \(\beta_{\infty}^{*}\approx\operatorname*{argmin}_{\beta^{*}\in S}D_{\psi_{\bm {\alpha}_{\infty}}}(\beta^{*},0)=\psi_{\bm{\alpha}_{\infty}}(\beta^{*})\), and thus _the solution \(\beta_{\infty}^{*}\) minimises the same potential function that the solution of gradient flow (see Eq. (4)), but with an effective initialisation \(\bm{\alpha}_{\infty}\)_. Also note that for \(\gamma_{k}\equiv\gamma\to 0\) we have \(\bm{\alpha}_{\infty}\to\bm{\alpha}\) and \(\tilde{\beta}_{0}\to\bm{0}\) (Proposition 19), recovering the previously known result for gradient flow (4).

**Deviation from gradient flow.** The difference with gradient flow is directly associated with the quantity \(\sum_{k}q(\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}))\). Also, as the (stochastic) gradients converge to 0 and \(q(x)\overset{x\to 0}{\sim}x^{2}\), one should think of this sum as roughly being \(\sum_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})^{2}\): the larger this sum, the more the recovered solution differs from that of gradient flow. The full picture of how large stepsizes and stochasticity impact the generalisation properties of \(\beta_{\infty}^{*}\) and the recovery of minimum \(\ell_{1}\)-norm solution is nuanced as clearly seen in Fig. 1.

### Convergence of the iterates

Theorem 1 provides the implicit minimisation problem but says nothing about the convergence of the iterates. Here we show under very reasonable assumptions on the stepsizes that the iterates indeed converge towards a global optimum. Note that since the loss \(F\) is non-convex, such a convergence result is non-trivial and requires an involved analysis.

**Theorem 2** (Convergence of the iterates).: _Let \((u_{k},v_{k})_{k\geqslant 0}\) follow the mini-batch SGD recursion (3) initialised at \(u_{0}=\sqrt{2}\boldsymbol{\alpha}\in\mathbb{R}_{>0}^{d}\) and \(v_{0}=\boldsymbol{0}\), and let \((\beta_{k})_{k\geqslant 0}=(u_{k}\odot v_{k})_{k\geqslant 0}\). Recall the "smoothness" parameter \(L\) on the minibatch loss defined in the notations. There exist \(B>0\) verifying \(B=\tilde{\mathcal{O}}(\min_{\beta^{*}\in\mathcal{S}}\left\|\beta^{*}\right\|_ {\infty})\) and a numerical constant \(c>0\) such that for stepsizes satisfying \(\gamma_{k}\leqslant\frac{c}{LB}\), the iterates \((\beta_{k})_{k\geqslant 0}\) converge almost surely to the interpolator \(\beta^{*}_{\infty}\) solution of Eq. (5)._

In fact, we can be more precise by showing an exponential rate of convergence of the losses as well as characterise the rate of convergence of the iterates as follows.

**Proposition 1** (Quantitative convergence rates).: _For a uniform initialisation \(\boldsymbol{\alpha}=\alpha\boldsymbol{1}\) and under the assumptions of Theorem 2, we have:_

\[\mathbb{E}\left[\mathcal{L}(\beta_{k})\right]\leqslant\left(1-\frac{1}{2} \gamma\alpha^{2}\lambda_{b}\right)^{k}\mathcal{L}(\beta_{0})\quad\text{and} \quad\mathbb{E}\left[\left\|\beta_{k}-\beta^{*}_{\alpha_{k}}\right\|^{2} \right]\leqslant C\left(1-\frac{1}{2}\gamma\alpha^{2}\lambda_{b}\right)^{k}\,,\]

_where \(\lambda_{b}>0\) is the largest value such that \(\lambda_{b}H\preceq\mathbb{E}_{\mathcal{B}}[H_{\mathcal{B}}]\), \(C=2B(\alpha^{2}\lambda_{\min}^{+})^{-1}\left(1+(4B\lambda_{\max})(\alpha^{2} \lambda_{\min}^{+})^{-1}\right)\mathcal{L}(\beta_{0})\) and \(\lambda_{\min}^{+},\lambda_{\max}>0\) are respectively the smallest non-null and the largest eigenvalues of \(H\), and \(\beta^{*}_{\alpha_{k}}\) is the interpolator that minimises the perturbed hypertnopy \(h_{k}\) of parameter \(\alpha_{k}\), as defined in Eq. (7) in the next subsection._

The convergence of the losses is proved directly using the time-varying mirror structure that we exhibit in the next subsection, the convergence of the iterates is proved by studying the curvature of the mirror maps on a small neighborhood around the affine interpolation space.

### Sketch of proof through a time varying mirror descent

As in the continuous-time framework, our results heavily rely on showing that the iterates \((\beta_{k})_{k}\) follow a mirror descent recursion with time-varying potentials on the convex loss \(\mathcal{L}(\beta)\). To show this, we first define the following quantities:

\[\boldsymbol{\alpha}_{k}^{2}\coloneqq\boldsymbol{\alpha}_{+,k}\odot \boldsymbol{\alpha}_{-,k}\qquad\text{and}\qquad\phi_{k}\coloneqq\frac{1}{2} \operatorname{arcsinh}\left(\frac{\boldsymbol{\alpha}_{+,k}^{2}-\boldsymbol{ \alpha}_{-,k}^{2}}{2\boldsymbol{\alpha}_{k}^{2}}\right)\in\mathbb{R}^{d}\,,\]

where \(\boldsymbol{\alpha}_{\pm,k}\coloneqq\boldsymbol{\alpha}\exp\left(-\frac{1}{2 }\sum_{i=0}^{k-1}q_{\pm}\big{(}\gamma_{\ell}\nabla\mathcal{L}_{\mathcal{B}_{ \ell}}(\beta_{\ell})\big{)}\right)\in\mathbb{R}^{d}\). Finally for \(k\geqslant 0\), we define the potentials \((h_{k}:\mathbb{R}^{d}\to\mathbb{R})_{k\geqslant 0}\) as:

\[h_{k}(\beta)=\psi_{\boldsymbol{\alpha}_{k}}(\beta)-\langle\phi_{k},\beta\rangle.\] (7)

Where \(\psi_{\boldsymbol{\alpha}_{k}}\) is the hyperbolic entropy function defined Eq. (4). Now that all the relevant quantities are defined, we can state the following proposition which explicits the time-varying stochastic mirror descent.

**Proposition 2**.: _The iterates \((\beta_{k}=u_{k}\odot v_{k})_{k\geqslant 0}\) from Eq. (3) satisfy the Stochastic Mirror Descent recursion with varying potentials \((h_{k})_{k}\):_

\[\nabla h_{k+1}(\beta_{k+1})=\nabla h_{k}(\beta_{k})-\gamma_{k}\nabla \mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\,,\]

_where \(h_{k}:\mathbb{R}^{d}\to\mathbb{R}\) for \(k\geqslant 0\) are defined Eq. (7). Since \(\nabla h_{0}(\beta_{0})=0\) we have:_

\[\nabla h_{k}(\beta_{k})\in\operatorname{span}(x_{1},\ldots,x_{n}).\] (8)

Theorem 1 and 2 and Proposition 1 follow from this key proposition: by suitably modifying classical convex optimization techniques to account for the time-varying potentials, we can prove the convergence of the iterates towards an interpolator \(\beta^{*}_{\infty}\) along with that of the relevant quantities \(\boldsymbol{\alpha}_{\pm,k}\), \(\boldsymbol{\alpha}_{k}\) and \(\phi_{k}\). The implicit regularisation problem then directly follows from: (1) the limit condition \(\nabla h_{\infty}(\beta_{\infty})\in\operatorname{Span}(x_{1},\ldots,x_{n})\) as seen from Eq. (8) and (2) the interpolation condition \(X\beta^{*}_{\infty}=y\). Indeed, these two conditions exactly correspond to the KKT conditions of the convex problem Eq. (5).

Analysis of the impact of the stepsize and stochasticity on \(\alpha_{\infty}\)

In this section, we analyse the effects of large stepsizes and stochasticity on the implicit bias of (S)GD. We focus on how these factors influence the effective initialisation \(\bm{\alpha}_{\infty}\), which plays a key role as shown in Theorem 1. From its definition in Eq. (6), we see that \(\bm{\alpha}_{\infty}\) is a function of the vector \(\sum_{k}q(\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}))\). We henceforth call this quantity the _gain vector_. For simplicity of the discussions, from now on, we consider constant stepsizes \(\gamma_{k}=\gamma\) for all \(k\geqslant 0\) and a uniform initialisation of the weights \(\bm{\alpha}=\alpha\bm{1}\) with \(\alpha>0\). We can then write the gain vector as:

\[\mathrm{Gain}_{\gamma}\coloneqq\ln\left(\frac{\bm{\alpha}^{2}}{\bm{\alpha}_{ \infty}^{2}}\right)=\sum_{k}q(\gamma\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta _{k}))\in\mathbb{R}^{d}\,.\]

Following our discussion in section 3.1 on the scale and the shape of \(\bm{\alpha}_{\infty}\), we recall the link between the scale and shape of \(\mathrm{Gain}_{\gamma}\) and the recovered solution:

**1.** The **scale** of \(\mathrm{Gain}_{\gamma}\), i.e. the magnitude of \(\|\mathrm{Gain}_{\gamma}\|_{1}\) indicates how much the implicit bias of (S)GD differs from that of gradient flow: \(\|\mathrm{Gain}_{\gamma}\|_{1}\sim 0\) implies that \(\bm{\alpha}_{\infty}\sim\bm{\alpha}\) and therefore the recovered solution is close to that of gradient flow. On the contrary, \(\|\mathrm{Gain}_{\gamma}\|_{1}\gg\ln(1/\alpha)\) implies that \(\bm{\alpha}_{\infty}\) has effective scale much smaller than \(\bm{\alpha}\) thereby changing the implicit regularisation Eq. (5).

**2.** The **shape** of \(\mathrm{Gain}_{\gamma}\) indicates which coordinates of \(\beta\) in the associated minimum weighted \(\ell_{1}\) problem are most penalised. First recall from Section 3.1 that a uniformly large \(\mathrm{Gain}_{\gamma}\) leads to \(\psi_{\bm{\alpha}_{\infty}}\) being closer to the \(\ell_{1}\)-norm. However, with small weight initialisation \(\alpha\to 0\), we have,

\[\psi_{\bm{\alpha}_{\infty}}(\beta)\sim\ln(\frac{1}{\alpha})\|\beta\|_{1}+\sum _{i=1}^{d}\mathrm{Gain}_{\gamma}(\mathrm{i})|\beta_{i}|\,,\] (9)

In this case, having a heterogeneously large vector \(\mathrm{Gain}_{\gamma}\) leads to a weighted \(\ell_{1}\) norm as the effective implicit regularisation, where the coordinates of \(\beta\) corresponding to the largest entries of \(\mathrm{Gain}_{\gamma}\) are less likely to be recovered.

### The scale of \(\mathrm{Gain}_{\gamma}\) is increasing with the stepsize

The following proposition highlights the dependencies of the scale of the gain \(\|\mathrm{Gain}_{\gamma}\|_{1}\) in terms of various problem constants.

**Proposition 3**.: _Let \(\Lambda_{b},\lambda_{b}>0\)3 be the largest and smallest values, respectively, such that \(\lambda_{b}H\preceq\mathbb{E}_{\mathcal{B}}\big{[}H_{\mathcal{B}}^{2}\big{]} \preceq\Lambda_{b}H\). For any stepsize \(\gamma>0\) satisfying \(\gamma\leqslant\frac{c}{BL}\) (as in Theorem 2), initialisation \(\alpha\bm{1}\) and batch size \(b\in[n]\), the magnitude of the gain satisfies:_

Footnote 3: \(\Lambda_{b},\lambda_{b}>0\) are data-dependent constants; for \(b=n\), we have \((\lambda_{n},\Lambda_{n})=(\lambda_{\min}^{+}(H),\lambda_{\max}(H))\) where \(\lambda_{\min}^{+}(H)\) is the smallest non-null eigenvalue of \(H\); for \(b=1\), we have \(\min_{i}\|x_{i}\|_{2}^{2}\leqslant\lambda_{1}\leqslant\Lambda_{1}\leqslant \max_{i}\|x_{i}\|_{2}^{2}\).

\[\lambda_{b}\gamma^{2}\sum_{k}\mathbb{E}\mathcal{L}(\beta_{k})\leqslant\mathbb{ E}\left[\|\mathrm{Gain}_{\gamma}\|_{1}\right]\leqslant 2\Lambda_{b}\gamma^{2} \sum_{k}\mathbb{E}\mathcal{L}(\beta_{k})\,,\] (10)

_where the expectation is over a uniform and independent sampling of the batches \((\mathcal{B}_{k})_{k\geqslant 0}\)._

**The slower the training, the larger the gain.** Eq. (10) shows that the slower the training loss converges to \(0\), the larger the sum of the loss and therefore the larger the scale of \(\mathrm{Gain}_{\gamma}\). This means that the (S)GD trajectory deviates from that of gradient flow if the stepsize and/or noise slows down the training. This supports observations previously made from stochastic gradient flow [48] analysis.

**The bigger the stepsize, the larger the gain.** The effect of the stepsize on the magnitude of the gain is not directly visible in Eq. (10) because a larger stepsize tends to speed up the training. For stepsize \(0<\gamma\leqslant\gamma_{\max}=\frac{c}{BL}\) as in Theorem 2 we have that (see Appendix G.1):

\[\sum_{k}\gamma^{2}\mathcal{L}(\beta_{k})=\Theta\left(\gamma\ln\left(\frac{1}{ \alpha}\right)\left\|\beta_{\ell_{1}}^{*}\right\|_{1}\right)\,.\] (11)

Eq. (11) clearly shows that increasing the stepsize **boosts** the magnitude \(\|\mathrm{Gain}_{\gamma}\|_{1}\) up until the limit of \(\gamma_{\max}\). Therefore, the larger the stepsize the smaller is the effective scale of \(\bm{\alpha}_{\infty}\). In turn, larger gap between \(\bm{\alpha}_{\infty}\) and \(\bm{\alpha}\) leads to a larger deviation of (S)GD from the gradient flow.

**Large stepsizes and Edge of Stability.** The previous paragraph holds for stepsizes smaller than \(\gamma_{\max}\) for which we can theoretically prove convergence. But what if we use even bigger stepsizes? Let \((\beta_{k}^{\gamma})_{k}\) denote the iterates generated with stepsize \(\gamma\) and let us define \(\tilde{\gamma}_{\max}\coloneqq\sup_{\gamma\geqslant 0}\{\gamma\text{ s.t. }\forall \gamma^{\prime}\in(0,\gamma),\ \sum_{k}\mathcal{L}(\beta_{k}^{\gamma^{\prime}})<\infty\}\), which corresponds to the largest stepsize such that the iterates still converge for a given problem (even if not provably so). From Proposition 3 we have that \(\gamma_{\max}\leq\tilde{\gamma}_{\max}\). As we approach this upper bound on convergence \(\gamma\to\tilde{\gamma}_{\max}\), the sum \(\sum_{k}\mathcal{L}(\beta_{k}^{\gamma})\) diverges. For such large stepsizes, the iterates of gradient descent tend to "bounce" and this regime is commonly referred to as the _Edge of Stability_. In this regime, the convergence of the loss can be made arbitrarily slow due to these bouncing effects. As a consequence, as seen through Eq. (10), the magnitude of \(\operatorname{Gain}_{\gamma}\) can be become arbitrarily big as observed in Fig. 2 (left). In this regime, the recovered solution tends to dramatically differ from the gradient flow solution, as seen in Fig. 1.

**Impact of stochasticity and linear scaling rule.** Assuming inputs \(x_{i}\) sampled from \(\mathcal{N}(0,\sigma^{2}I_{d})\) with \(\sigma^{2}>0\), we obtain, w.h.p. over the dataset (see Appendix G.3, Proposition 17). The scale of \(\operatorname{Gain}_{\gamma}\) decreases with batch size and there exists a factor \(n\) between that of SGD and that of GD. Additionally, the magnitude of \(\operatorname{Gain}_{\gamma}\) depends on \(\frac{\gamma}{b}\), resembling the **linear scaling rule** commonly used in deep learning [22].

By analysing the magnitude \(\|\operatorname{Gain}_{\gamma}\|_{1}\), we have explained **the distinct behavior of (S)GD with large stepsizes compared to gradient flow**. However, our current analysis does not qualitatively distinguish the behavior between SGD and GD beyond the linear stepsize scaling rules, in contrast with Fig. 1. A deeper understanding of the shape of \(\operatorname{Gain}\gamma\) is needed to explain this disparity.

### The shape of \(\operatorname{Gain}_{\gamma}\) explains the differences between GD and SGD

In this section, we restrict our presentation to single batch SGD (\(b=1\)) and full batch GD (\(b=n\)). When visualising the typical shape of \(\operatorname{Gain}_{\gamma}\) for large stepsizes (see Fig. 2 - right), we note that GD and SGD behave very differently. For GD, the magnitude of \(\operatorname{Gain}_{\gamma}\) is higher for coordinates in the support of \(\beta_{\ell_{1}}^{\star}\) and thus these coordinates are adversely weighted in the asymptotic limit of \(\psi_{\mathbf{\alpha}_{\infty}}\) (per (9)). This explains the distinction seed in Fig. 1, where GD in this regime has poor sparse recovery despite having a small scale of \(\mathbf{\alpha}_{\infty}\), as opposed to SGD that behaves well.

The **shape** of \(\operatorname{Gain}_{\gamma}\) is determined by the sum of the squared gradients \(\sum_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})^{2}\), and in particular by the degree of heterogeneity among the coordinates of this sum. Precisely analysing the sum over the whole trajectory of the iterates \((\beta_{k})_{k}\) is technically out of reach. However, we empirically observe for the trajectories shown in Fig. 2 that the shape is largely determined within the first few iterates as formalized in the observation below.

**Observation 1**.: \(\sum_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})^{2}\propto\mathbb{E}[ \nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{0})^{2}]\)_._

Figure 2: _Left:_ the scale of \(\operatorname{Gain}_{\gamma}\) explodes as \(\gamma\to\tilde{\gamma}_{\max}\) for both GD and SGD. _Right:_\(\beta_{\mathrm{sparse}}^{\star}\) is fixed, we perform \(100\) runs of GD and SGD with different feature matrices, and we plot the \(d\) coordinates of \(\operatorname{Gain}_{\gamma}\) (for GD and SGD) on the \(x\)-axis (which is in log scale for better visualisation). The shape of \(\operatorname{Gain}_{\gamma}^{\mathrm{SGD}}\) is homogeneous whereas that of GD is heterogeneous with much higher magnitude on the support of \(\beta_{\mathrm{sparse}}^{\star}\). The shape of \(\operatorname{Gain}_{\gamma}^{\mathrm{GD}}\) is proportional to the expected gradient at initialisation which is \((\beta_{\mathrm{sparse}}^{\star})^{2}\).

In the simple case of a Gaussian noiseless sparse recovery problem (where \(y_{i}=\langle\beta^{\star}_{\mathrm{sparse}},x_{i}\rangle\) for some sparse vector \(\beta^{\star}_{\mathrm{sparse}}\)), we can control these gradients for GD and SGD (Appendix G.4) as:

\[\nabla\mathcal{L}(\beta_{0})^{2}=(\beta^{\star}_{\mathrm{sparse}} )^{2}+\varepsilon\,,\text{ for some }\varepsilon\text{ verifying }\|\varepsilon\| _{\infty}<<\left\|\beta^{\star}_{\mathrm{sparse}}\right\|_{\infty}^{2},\] (12) \[\mathbb{E}_{i_{0}}[\nabla\mathcal{L}_{i_{0}}(\beta_{0})^{2}]= \Theta\Big{(}\|\beta^{\star}_{\mathrm{sparse}}\|_{2}^{2}\mathbf{1}\Big{)}\,.\] (13)

The gradient of GD is heterogeneous.Since \(\beta^{\star}_{\mathrm{sparse}}\) is sparse by definition, we deduce from Eq. (25) that \(\nabla\mathcal{L}(\beta_{0})\) is heterogeneous with larger values corresponding to the support of \(\beta^{\star}_{\mathrm{sparse}}\). Along with observation 1, this means that \(\mathrm{Gain}_{\gamma}\)**has much larger values on the support of \(\beta^{\star}_{\mathrm{sparse}}\)**. The corresponding weighted \(\ell_{1}\)-norm therefore penalises the coordinates belonging to the support of \(\beta^{\star}_{\mathrm{sparse}}\), which hinders the recovery of \(\beta^{\star}_{\mathrm{sparse}}\) (as explained in Example 1, Appendix D).

The stochastic gradient of SGD is homogeneous.On the contrary, from Eq. (26), we have that the initial stochastic gradients are homogeneous, leading to a weighted \(\ell_{1}\)-norm where the weights are roughly balanced. The corresponding weighted \(\ell_{1}\)-norm is therefore close to the uniform \(\ell_{1}\)-norm and the classical \(\ell_{1}\) recovery guarantees are expected.

Overall summary of the joint effects of the scale and shape.In summary we have the following trichotomy which fully explains Fig. 1:

1. for small stepsizes, the scale is small, and (S)GD solutions are close to that of gradient flow;
2. for large stepsizes the scale is significant and the recovered solutions differ from GF: * for SGD the shape of \(\boldsymbol{\alpha}_{\infty}\) is uniform, the associated norm is closer to the \(\ell_{1}\)-norm and the recovered solution is closer to the sparse solution; * for GD, the shape is heterogeneous, the associated norm is weighted such that it hinders the recovery of the sparse solution.

In this last section, we relate heuristically these findings to the _Edge of Stability_ phenomenon.

## 5 Edge of Stability: the neural point of view

In recent years it has been noticed that when training neural networks with 'large' stepsizes at the limit of divergence, GD enters the _Edge of Stability (EoS)_ regime. In this regime, as seen in Fig. 3, the iterates of GD 'bounce' / 'oscillate'. In this section, we come back to the point of view of the weights \(w_{k}=(u_{k},v_{k})\in\mathbb{R}^{2d}\) and make the connection between our previous results and the common understanding of the _EoS_ phenomenon. The question we seek to answer is: in which case does GD enter the _EoS_ regime, and if so, what are the consequences on the trajectory? _Keep in mind that this section aims to provide insights rather than formal statements._ We study the GD trajectory starting from a small initialisation \(\boldsymbol{\alpha}=\alpha\mathbf{1}\) where \(\alpha\ll 1\) such that we can consider that gradient flow converges close to the sparse interpolator \(\beta^{\star}_{\mathrm{sparse}}=\beta_{w^{\star}_{\mathrm{sparse}}}\) corresponding to the weights \(w^{\star}_{\mathrm{sparse}}=(\sqrt{|\beta^{\star}_{\mathrm{sparse}}|}, \mathrm{sign}(\beta^{\star}_{\mathrm{sparse}})\sqrt{|\beta^{\star}_{\mathrm{ sparse}}|})\) (see Lemma 1 in [49] for the mapping from the predictors to weights for gradient flow). The trajectory of GD as seen in Fig. 3 (left) can be decomposed into up to \(3\) phases.

**First phase: gradient flow.** The stepsize is appropriate for the local curvature (as seen in Fig. 3, lower right) around initialisation and the iterates of GD remain close to the trajectory of gradient flow (in black in Fig. 3). If the stepsize is such that \(\gamma<\frac{1}{\lambda_{\mathrm{max}}(\nabla^{2}F(w^{\star}_{\mathrm{sparse}}))}\), then it is compatible with the local curvature and the iterates can converge: in this case GF and GD converge to the same point (as seen in Fig. 1 for small stepsizes). For larger \(\gamma>\frac{2}{\lambda_{\mathrm{max}}(\nabla^{2}F(w^{\star}_{\mathrm{sparse}}))}\) (as is the case for \(\gamma_{\mathrm{GD}}\) in Fig. 3, lower right), the iterates cannot converge to \(\beta^{\star}_{\mathrm{sparse}}\) and we enter the oscillating phase.

**Second phase: oscillations.** The iterates start oscillating. The gradient of \(F\) writes \(\nabla_{(u,v)}F(w)\sim(\nabla\mathcal{L}(\beta)\odot v,\nabla\mathcal{L}( \beta)\odot u)\) and for \(w\) in the vicinity of \(w^{\star}_{\mathrm{sparse}}\) we have that \(u_{i}\approx v_{i}\approx 0\) for \(i\notin\mathrm{supp}(\beta^{\star}_{\mathrm{sparse}})\). Therefore for \(w\sim w^{\star}_{\mathrm{sparse}}\) we have that \(\nabla_{u}F(w)_{i}\approx\nabla_{v}F(w)_{i}\approx 0\) for \(i\notin\mathrm{supp}(\beta^{\star}_{\mathrm{sparse}})\) and the gradients roughly belong to \(\mathrm{Span}(e_{i},e_{i+d})_{i\in\mathrm{supp}(\beta^{\star}_{\mathrm{sparse}})}\). This meansthat only the coordinates of the weights \((u_{i},v_{i})\) for \(i\in\mathrm{supp}(\beta^{*}_{\mathrm{sparse}})\) can oscillate and similarly for \((\beta_{i})_{i\in\mathrm{supp}(\beta^{*}_{\mathrm{sparse}})}\) (as seen Fig. 3 left).

**Last phase: convergence.** Due to the oscillations, the iterates gradually drift towards a region of lower curvature (Fig. 3, lower right, the sharpness decreases) where they may (potentially) converge. Theorem 1 enables us to understand where they converge: the coordinates of \(\beta_{k}\) that have oscillated significantly along the trajectory belong to the support of \(\beta^{*}_{\mathrm{sparse}}\), and therefore \(\mathrm{Gain}_{\gamma}(\mathrm{i})\) becomes much larger for \(i\in\mathrm{supp}(\beta^{*}_{\mathrm{sparse}})\) than for the other coordinates. Thus, the coordinates of the solution recovered in the _EoS_ regime are heavily penalised on the support of the sparse solution. This is observed in Fig. 3 (left): the oscillations of \((\beta_{i})_{i\in\mathrm{supp}(\beta^{*}_{\mathrm{sparse}})}\) lead to a gradual shift of these coordinates towards \(0\), hindering an accurate recovery of the solution \(\beta^{*}_{\mathrm{sparse}}\).

**SGD in the _EoS_ regime.** In contrast to the behavior of GD where the oscillations primarily occur on the non-sparse coordinates of ground truth sparse model, for SGD we see a different behavior in Fig. 6 (Appendix A). For stepsizes in the _EoS_ regime, just below the non-convergence threshold: the fluctuation of the coordinates occurs evenly over all coordinates, leading to a uniform \(\bm{\alpha}_{\infty}\). These fluctuations are reminiscent of label-noise SGD [2], that have been shown to recover the sparse interpolator in diagonal linear networks [50].

## 6 Conclusion

We study the effect of stochasticity along with large stepsizes when training DLNs with (S)GD. We prove convergence of the iterates as well as explicitly characterise the recovered solution by exhibiting an implicit regularisation problem which depends on the iterates' trajectory. In essence the impact of stepsize and minibatch size are captured by the effective initialisation parameter \(\bm{\alpha}_{\infty}\) that depends on these choices in an informative way. We then use our characterisation to explain key empirical differences between SGD and GD and provide further insights on the role of stepsize and stochasticity. In particular, our characterisation explains the fundamentally different generalisation properties of SGD and GD solutions at large stepsizes as seen in Fig. 1: without stochasticity, the use of large stepsizes can prevent the recovery of the sparse interpolator, even though the effective scale of the initialization decreases with larger stepsize for both SGD and GD. We also provide insights on the link between the _Edge of Stability_ regime and our results.

### Aknowledgements

M. Even deeply thanks Laurent Massoulie for making it possible to visit Microsoft Research and the Washington state during an internship supervised by Suriya Gunasekar, the MSR Machine Learning Foundations group for hosting him, and Martin Jaggi for inviting him for a week in Lausanne at EPFL, making it possible to meet and discuss with Scott Pesme and Nicolas Flammarion.

Figure 3: GD at the _EoS. Left_: For GD, the coordinates on the support of \(\beta^{*}_{\mathrm{sparse}}\) oscillate and drift towards \(0\). _Right, top:_ The GD train losses saturate before eventually converging. _Bottom:_ GF converges towards a solution that has a high hessian maximum eigenvalue. GD cannot converge towards this solution because of its large stepsize: it therefore drifts towards a solution that has a curvature just below \(2/\gamma\).

## References

* [1] Kwangjun Ahn, Sebastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, and Yi Zhang. Learning threshold neurons via the "edge of stability". _arXiv preprint_, 2022.
* [2] M. Andriushchenko, A. Varre, L. Pillaud-Vivien, and N. Flammarion. SGD with large step sizes learns sparse features. _arXiv preprint_, 2022.
* [3] Shahar Azulay, Edward Moroshko, Mor Shpigel Nacson, Blake E Woodworth, Nathan Srebro, Amir Globerson, and Daniel Soudry. On the implicit bias of initialization shape: Beyond infinitesimal mirror descent. In _International Conference on Machine Learning_, pages 468-477. PMLR, 2021.
* [4] R. Baraniuk, M. Davenport, R. DeVore, and M. Wakin. A simple proof of the restricted isometry property for random matrices. _Constructive Approximation_, 28(3):253-263, January 2008.
* [5] H. H Bauschke, J. Bolte, and M. Teboulle. A descent lemma beyond Lipschitz gradient continuity: first-order methods revisited and applications. _Mathematics of Operations Research_, 42(2):330-348, 2017.
* [6] Raphael Berthier. Incremental learning in diagonal linear networks. _arXiv preprint arXiv:2208.14673_, 2022.
* [7] G. Beugnot, J. Mairal, and A. Rudi. On the benefits of large learning rates for kernel methods. In _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 254-282. PMLR, 02-05 Jul 2022.
* [8] G. Blanc, N. Gupta, G. Valiant, and P. Valiant. Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process. In _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pages 483-513. PMLR, 09-12 Jul 2020.
* [9] L.M. Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. _USSR Computational Mathematics and Mathematical Physics_, 7(3):200-217, 1967. ISSN 0041-5553.
* [10] E. Candes, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements. _Communications on Pure and Applied Mathematics_, 59(8):1207-1223, 2006.
* [11] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In _International Conference on Learning Representations_, 2018.
* [12] Lei Chen and Joan Bruna. On gradient descent convergence beyond the edge of stability, 2022.
* [13] Lenaic Chizat, Edouard Oyallon, and Francis Bach. _On Lazy Training in Differentiable Programming_. 2019.
* [14] Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In _International Conference on Learning Representations_, 2021.
* [15] Alex Damian, Tengyu Ma, and Jason D. Lee. Label noise SGD provably prefers flat global minimizers. In _Advances in Neural Information Processing Systems_, 2021.
* [16] Alex Damian, Eshaan Nichani, and Jason D. Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability. In _International Conference on Learning Representations_, 2023.
* [17] J. L. Doob. _Stochastic Processes_. John Wiley & Sons, 1990.
* [18] Radu Alexandru Dragomir, Mathieu Even, and Hadrien Hendrikx. Fast stochastic Bregman gradient methods: Sharp analysis and variance reduction. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 2815-2825. PMLR, 18-24 Jul 2021.

* [19] Mathieu Even and Laurent Massoulie. Concentration of non-isotropic random tensors with applications to learning and empirical risk minimization. In _Proceedings of Thirty Fourth Conference on Learning Theory_, volume 134 of _Proceedings of Machine Learning Research_, pages 1847-1886. PMLR, 15-19 Aug 2021.
* [20] Jonas Geiping, Micah Goldblum, Phillip E Pope, Michael Moeller, and Tom Goldstein. Stochastic training is not necessary for generalization. In _International Conference on Learning Representations_, 2022.
* [21] Udaya Ghai, Elad Hazan, and Yoram Singer. Exponentiated gradient meets gradient descent. In _Proceedings of the 31st International Conference on Algorithmic Learning Theory_, volume 117 of _Proceedings of Machine Learning Research_, pages 386-407. PMLR, 08 Feb-11 Feb 2020.
* [22] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training imagenet in 1 hour. _arXiv preprint arXiv:1706.02677_, 2017.
* [23] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. _Advances in Neural Information Processing Systems_, 30, 2017.
* [24] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. In _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 1832-1841. PMLR, 10-15 Jul 2018.
* [25] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. In _Advances in Neural Information Processing Systems_, volume 31, 2018.
* [26] Jeff Z. HaoChen, Colin Wei, Jason Lee, and Tengyu Ma. Shape matters: Understanding the implicit bias of the noise covariance. In _Proceedings of Thirty Fourth Conference on Learning Theory_, volume 134 of _Proceedings of Machine Learning Research_, pages 2315-2357. PMLR, 15-19 Aug 2021.
* [27] Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize well: Theoretical and empirical evidence. In _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [28] Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. _Neural Computation_, 9(1):1-42, January 1997.
* [29] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: Closing the generalization gap in large batch training of neural networks. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, page 1729-1739, 2017.
* [30] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, page 8580-8589, 2018.
* [31] Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in SGD, 2017.
* [32] Stanislaw Jastrzebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amost Storkey. On the relation between the sharpest directions of DNN loss and the SGD step length. In _International Conference on Learning Representations_, 2019.
* ICANN 2018_, pages 392-402, 2018.
* [34] Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In _International Conference on Learning Representations_, 2019.

* [35] Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In _Advances in Neural Information Processing Systems_, volume 33, pages 17176-17186, 2020.
* [36] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2017.
* [37] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2017.
* [38] Bobby Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape local minima? In _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 2698-2707. PMLR, 10-15 Jul 2018.
* [39] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large learning rate in training neural networks. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems_, 2019.
* [40] Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks. _arXiv preprint arXiv:1906.05890_, 2019.
* Volume 48_, page 354-363, 2016.
* [42] Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks. _arXiv preprint arXiv:1804.07612_, 2018.
* [43] Rotem Mulayoff, Tomer Michaeli, and Daniel Soudry. The implicit bias of minima stability: A view from function space. In _Advances in Neural Information Processing Systems_, 2021.
* [44] Mor Shpigel Nacson, Kavya Ravichandran, Nathan Srebro, and Daniel Soudry. Implicit bias of the step size in linear diagonal neural networks. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 16270-16295. PMLR, 17-23 Jul 2022.
* [45] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. _arXiv preprint arXiv:1412.6614_, 2014.
* [46] Ryan O'Donnell. Analysis of boolean functions, 2021.
* [47] Francesco Orabona, Koby Crammer, and Nicolo Cesa-Bianchi. A generalized online mirror descent with applications to classification and regression. _Mach. Learn._, 99(3):411-435, jun 2015.
* [48] S. Pesme, L. Pillaud-Vivien, and N. Flammarion. Implicit bias of SGD for diagonal linear networks: a provable benefit of stochasticity. In _Advances in Neural Information Processing Systems_, 2021.
* [49] Scott Pesme and Nicolas Flammarion. Saddle-to-saddle dynamics in diagonal linear networks. _arXiv preprint arXiv:2304.00488_, 2023.
* [50] L. Pillaud-Vivien, J. Reygner, and N. Flammarion. Label noise (stochastic) gradient descent implicitly solves the lasso for quadratic parametrisation. In _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 2127-2159. PMLR, 2022.
* [51] H. Robbins and S. Monro. A stochastic approxiation method. _Ann. Math. Statist_, 22(3):400-407, 1951.
* [52] Samuel L. Smith and Quoc V. Le. A Bayesian perspective on generalization and stochastic gradient descent. In _International Conference on Learning Representations_, 2018.

* [53] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. _J. Mach. Learn. Res._, 19(1):2822-2878, jan 2018.
* [54] Terrence Tao. Concentration of measure. _254A, Notes 1, Blogpost_, 2010.
* [55] Matus Telgarsky. Margins, shrinkage, and boosting. In _International Conference on Machine Learning_, pages 307-315. PMLR, 2013.
* [56] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. The statistical complexity of early-stopped mirror descent. In _Advances in Neural Information Processing Systems_, volume 33, pages 253-264, 2020.
* [57] Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal sparse recovery. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems_, 2019.
* [58] Roman Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
* [59] Yuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao. Large learning rate tames homogeneity: Convergence and balancing effect. In _International Conference on Learning Representations_, 2022.
* [60] Stephan Wojtowytsch. Stochastic gradient descent with noise of machine learning type. part II: Continuous time analysis. _arXiv preprint arXiv:2106.02588_, 2021.
* [61] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pages 3635-3673. PMLR, 09-12 Jul 2020.
* [62] Fan Wu and Patrick Rebeschini. A continuous-time mirror descent approach to sparse phase retrieval. In _Advances in Neural Information Processing Systems_, volume 33, pages 20192-20203, 2020.
* [63] Jingfeng Wu, Difan Zou, Vladimir Braverman, and Quanquan Gu. Direction matters: On the implicit bias of stochastic gradient descent with moderate learning rate. In _International Conference on Learning Representations_, 2021.
* [64] Lei Wu, Chao Ma, and Weinan E. How SGD selects the global minima in over-parameterized learning: A dynamical stability perspective. In _Advances in Neural Information Processing Systems_, volume 31, 2018.
* [65] Matthew D Zeiler. Adadelta: an adaptive learning rate method. _arXiv preprint arXiv:1212.5701_, 2012.
* [66] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In _International Conference on Learning Representations_, 2017.
* [67] Xingyu Zhu, Zixuan Wang, Xiang Wang, Mo Zhou, and Rong Ge. Understanding edge-of-stability training dynamics with a minimalist example. _International Conference on Learning Representations_, 2023.

Organisation of the Appendix.
1. In Appendix A, we provide additional experiments for uncentered data as well as on the behaviour of the sharpness and trace of the Hessian along the trajectory of the iterates. We finally provide an experiment highlighting the EoS regime for SGD.
2. In Appendix B, we prove that \((\beta_{k})\) follows a Mirror descent recursion with varying potentials. We explicit these potentials and discuss some consequences.
3. In Appendix C we prove that (S)GD on the \(\frac{1}{2}(w_{+}^{2}-w_{-}^{2})\) and \(u\odot v\) parametrisations with suitable initialisations lead to the same sequence \((\beta_{k})\).
4. In Appendix D, we show that the hyperentropy \(\psi_{\bm{\alpha}}\) converges to a **weighted**-\(\ell_{1}\)-norm when \(\bm{\alpha}\) converges to \(0\) non-uniformly. We then discuss the effects of this **weighted**\(\ell_{1}\)-norm for sparse recovery.
5. In Appendix E, we provide our descent lemmas for mirror descent with varying potentials and prove the boundedness of the iterates.
6. In Appendix F, we prove our main results: Theorem 1 and Theorem 2, as well as quantitative convergence (Proposition 1).
7. In Appendix G, we prove the lemmas and propositions given in the main text.
8. In Appendix H, we provide technical lemmas used throughout the proof of Theorem 1 and Theorem 2.
9. In Appendix I, we provide concentration results for random matrices and random vectors, used to estimate with high probability (w.r.t. the dataset) quantities related to the data.

Additional experiments and results

### Uncentered data

When the data is uncentered, the discussion and the conclusion for GD are somewhat different. This paragraph is motivated by the observation of Nacson et al. [44] who notice that GD with large stepsizes helps to recover low \(\ell_{1}\) solutions for uncentered data (Fig. 4). We make the following assumptions on the uncentered inputs.

**Assumption 1**.: _There exist \(\mu\in\mathbb{R}^{d}\) and \(\delta,c_{0},c_{1},c_{2}>0\) such that for all \(s\)-sparse vectors \(\beta\) verifying \(\langle\mu,\beta\rangle\geqslant c_{0}\|\beta\|_{\infty}\|\mu\|_{\infty}\), there exists \(\varepsilon\in\mathbb{R}^{d}\) such that \((X^{\top}X)\beta=\langle\beta,\mu\rangle\mu+\varepsilon\) where \(\|\varepsilon\|_{2}\leqslant\delta\|\beta\|_{2}\) and \(c_{1}\langle\beta,\mu\rangle^{2}\mu^{2}\leqslant\frac{1}{n}\sum_{i}x_{i}^{2} \langle x_{i},\beta\rangle^{2}\leqslant c_{2}\langle\beta,\mu\rangle^{2}\mu^{2}\)._

Assumption 1 is not restrictive and holds with high probability for \(\mathcal{N}(\mu\mathbf{1},\sigma^{2}I_{d})\) inputs when \(\mu\gg\sigma\mathbf{1}\) (see Lemma 9 in Appendix). The following lemma characterises the initial shape of SGD and GD gradients for uncentered data.

**Proposition 4** (Shape of the (stochastic) gradient at initialisation).: _Under Assumption 1 and if \(\langle\mu,\beta_{\mathrm{sparse}}^{\star}\rangle\geqslant c_{0}\|\beta\|_{ \infty}\|\mu\|_{\infty}\), the squared full batch gradient and the expected stochastic gradient descent at initialisation satisfy, for some \(\varepsilon\) satisfying \(\left\|\varepsilon\right\|_{\infty}\ll\left\|\beta_{\mathrm{sparse}}\right\|_ {2}\):_

\[\nabla\mathcal{L}(\beta_{0})=\langle\beta_{\mathrm{sparse}}^{\star},\mu \rangle^{2}\mu^{2}+\varepsilon\,,\] (14)

\[\mathbb{E}_{i\sim\mathrm{Unif}([n])}[\nabla\mathcal{L}_{i}(\beta_{0})^{2}]= \Theta\Big{(}\langle\beta_{\mathrm{sparse}}^{\star},\mu\rangle^{2}\mu^{2} \Big{)}\,.\] (15)

In this case the initial gradients of SGD and of GD are both homogeneous, explaining the behaviours of gradient descent in Fig. 4 (App. A): large stepsizes help in the recovery of the sparse solution in the presence of uncentered data, as opposed to centered data. Note that for decentered data with a \(\mu\in\mathbb{R}^{d}\) orthogonal to \(\beta_{\mathrm{sparse}}^{\star}\), there is no effect of decentering on the recovered solution. If the support of \(\mu\) is the same as that of \(\beta_{\mathrm{sparse}}^{\star}\), the effect is detrimental and the same discussion as in the centered data case applies.

### Behaviour of the maximal value and trace of the hessian

Here in Fig. 5, we provide some additional experiments on the behaviour of: (1) the maximum eigenvalue of the hessian \(\nabla^{2}F(w_{\infty}^{\gamma})\) at the convergence of the iterates of SGD and GD (2) the trace

Figure 4: Noiseless sparse regression with a \(2\)-layer DLN with uncentered data \(x_{i}\sim\mathcal{N}(\mu\mathbf{1},I_{d})\) where \(\mu=5\). All the stepsizes lead to convergence to a global solution and the solutions of SGD and GD have similar behaviours, corroborating Proposition 4. The setup corresponds to \((n,d,s,\alpha)=(20,30,3,0.1)\).

of hessian at the convergence of the iterates. As is clearly observed, increasing the stepsize for GD leads to a 'flatter' minimum in terms of the maximum eigenvalue of the hessian, while increasing the stepsize for SGD leads to a 'flatter' minimum in terms of its trace. These two solutions have very different structures. Indeed from the value of the hessian Eq. (22) at a global solution, and (very) roughly assuming that '\(X^{\top}X=I_{d}\)' and that '\(\boldsymbol{\alpha}\sim 0\)' (pushing the EOS phenomenon), one can see that minimising \(\lambda_{\max}(\nabla^{2}F(w))\) under the constraints \(X(w_{+}^{2}-w_{-}^{2})=y\) and \(w_{+}\odot w_{-}=0\) is equivalent to minimising \(\|\beta\|_{\infty}\) under the constant \(X\beta=y\). On the other hand minimising the trace of the hessian is equivalent to minimising the \(\ell_{1}\)-norm.

### Edge of Stability for SGD

Figure 5: Noiseless sparse regression setting. Diagonal linear network. Centered data. Behaviour of \(2\) different types of flatness of the recovered solution by SGD and GD depending on the stepsize. The setup corresponds to \((n,d,s,\alpha)=(20,30,3,0.1)\).

Figure 6: SGD at the edge of stability: all coordinates fluctuate, and the sparse solution is recovered. As opposed to GD at the EoS, since all coordinates fluctuate, the coordinates to recover are not more penalised than the others.

[MISSING_PAGE_FAIL:18]

where \(\psi_{\bm{\alpha}_{k}}\) is the hyperbolic entropy defined in (4) of scale \(\bm{\alpha}_{k}\):

\[\psi_{\bm{\alpha}_{k}}(\beta)=\frac{1}{2}\sum_{i=1}^{d}\Big{(}\beta_{i}\mathrm{ arcsinh}(\frac{\beta_{i}}{\alpha_{k,i}^{2}})-\sqrt{\beta_{i}^{2}+\alpha_{k,i}^{4}}+ \alpha_{k,i}^{2}\Big{)}\]

where \(\alpha_{k,i}\) corresponds to the \(i^{th}\) coordinate of the vector \(\bm{\alpha}_{k}\).

Now that all the relevant quantities are define, we can state the following proposition which explicits the time-varying stochastic mirror descent followed by \((\beta_{k})_{k}\)

**Proposition 5**.: _The iterates \((\beta_{k}=u_{k}\odot v_{k})_{k\geqslant 0}\) from Eq. (3) satisfy the Stochastic Mirror Descent recursion with varying potentials \((h_{k})_{k}\):_

\[\nabla h_{k+1}(\beta_{k+1})=\nabla h_{k}(\beta_{k})-\gamma_{k}\nabla\mathcal{ L}_{\mathcal{B}_{k}}(\beta_{k})\,,\] (19)

_where \(h_{k}:\mathbb{R}^{d}\to\mathbb{R}\) for \(k\geqslant 0\) are defined Eq. (18). Since \(\nabla h_{0}(\beta_{0})=0\) we have:_

\[\nabla h_{k}(\beta_{k})\in\mathrm{span}(x_{1},\ldots,x_{n})\]

Proof.: Using Proposition 6, we study the \(\frac{1}{2}(w_{+}^{2}-w_{-}^{2})\) parametrisation instead of the \(u\odot v\), indeed this is the natural parametrisation to consider when doing the calculations as it "separates" the recursions on \(w_{+}\) and \(w_{-}\).

Let us focus on the recursion of \(w_{+}\):

\[w_{+,k+1}=(1-\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}))\cdot w _{+,k}\,.\]

We have:

\[w_{+,k+1}^{2} =(1-\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}))^{ 2}\cdot w_{+,k}^{2}\] \[=\exp\left(\ln((1-\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}( \beta_{k}))^{2})\right)\cdot w_{+,k}^{2}\,,\]

with the convention that \(\exp(\ln(0))=0\). This leads to:

\[w_{+,k+1}^{2} =\exp\big{(}-2\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(w_{k })+2\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})+\ln((1-\gamma_{k }\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}))^{2})\big{)}\cdot w_{+,k}^{2}\] \[=\exp\big{(}-2\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta _{k})-q_{+}(\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}))\big{)} \cdot w_{+,k}^{2}\,,\]

since \(q_{+}(x)=-2x-\ln((1-x)^{2})\). Expanding the recursion and using that \(w_{+,k=0}\) is initialised at \(w_{+,k=0}=\bm{\alpha}\), we thus obtain:

\[w_{+,k}^{2} =\bm{\alpha}^{2}\exp(-\sum_{\ell=0}^{k-1}q_{+}(\gamma_{\ell} \nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})))\exp\left(-2\sum_{\ell=0 }^{k-1}\gamma_{\ell}\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\right)\] \[=\bm{\alpha}_{+,k}^{2}\exp\left(-2\sum_{\ell=0}^{k-1}\gamma_{\ell }\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\right),\]

where we recall that \(\bm{\alpha}_{\pm,k}^{2}=\bm{\alpha}^{2}\exp(-\sum_{\ell=0}^{k-1}q_{\pm}(\gamma_ {\ell}g_{\ell}))\). One can easily check that we similarly get:

\[w_{-,k}^{2}=\bm{\alpha}_{-,k}^{2}\exp\left(+2\sum_{\ell=0}^{k-1}\gamma_{\ell} \nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\right),\]

leading to:

\[\beta_{k} =\frac{1}{2}(w_{+,k}^{2}-w_{-,k}^{2})\] \[=\frac{1}{2}\bm{\alpha}_{+,k}^{2}\exp\left(-2\sum_{\ell=0}^{k-1} \gamma_{\ell}\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\right)-\frac{ 1}{2}\bm{\alpha}_{-,k}^{2}\exp\left(+2\sum_{\ell=0}^{k-1}\gamma_{\ell}\nabla \mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\right).\]

Using Lemma 4, the previous equation can be simplified into:

\[\beta_{k}=\bm{\alpha}_{+,k}\bm{\alpha}_{-,k}\sinh\Big{(}-2\sum_{\ell=0}^{k-1} \gamma_{\ell}\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})+\mathrm{ arcsinh}\left(\frac{\bm{\alpha}_{+,k}^{2}-\bm{\alpha}_{-,k}^{2}}{2\bm{ \alpha}_{+,k}\bm{\alpha}_{-,k}}\right)\Big{)}\,,\]which writes as:

\[\frac{1}{2}\operatorname{arcsinh}\big{(}\frac{\beta_{k}}{\bm{\alpha}_{k}^{2}} \big{)}-\phi_{k}=-\sum_{\ell=0}^{k-1}\gamma_{\ell}\nabla\mathcal{L}_{\mathcal{B} _{\ell}}(\beta_{\ell})\in\operatorname{span}(x_{1},\ldots,x_{n})\,,\]

where \(\phi_{k}=\frac{1}{2}\operatorname{arcsinh}\big{(}\frac{\bm{\alpha}_{+,k}^{2}- \bm{\alpha}_{-,k}^{2}}{2\bm{\alpha}_{k}^{2}}\big{)}\), \(\bm{\alpha}_{k}^{2}=\bm{\alpha}_{+,k}\odot\bm{\alpha}_{-,k}\) and since the potentials \(h_{k}\) are defined in Eq. (18) as \(h_{k}=\psi_{\bm{\alpha}_{k}}-\langle\phi_{k},\cdot\rangle\) with

\[\psi_{\bm{\alpha}}(\beta)=\frac{1}{2}\sum_{i=1}^{d}\Big{(}\beta_{i} \operatorname{arcsinh}(\frac{\beta_{i}}{\bm{\alpha}_{i}^{2}})\,-\sqrt{\beta_{ i}^{2}+\bm{\alpha}_{i}^{4}}+\bm{\alpha}_{i}^{2}\Big{)}\] (20)

specifically such that \(\nabla h_{k}(\beta_{k})=\frac{1}{2}\operatorname{arcsinh}\big{(}\frac{\beta _{k}}{\bm{\alpha}_{k}^{2}}\big{)}-\phi_{k}\). Hence,

\[\nabla h_{k}(\beta_{k})=\sum_{\ell<k}\gamma_{\ell}\nabla\mathcal{L}_{\mathcal{ B}_{\ell}}(\beta_{\ell})\,,\]

so that:

\[\nabla h_{k+1}(\beta_{k+1})=\nabla h_{k}(\beta_{k})-\gamma_{k}\nabla\mathcal{L }_{\mathcal{B}_{k}}(\beta_{k})\,,\]

which corresponds to a Mirror Descent with varying potentials \((h_{k})_{k}\). 

Appendix C Equivalence of the \(u\odot v\) and \(\frac{1}{2}(w_{+}^{2}-w_{-}^{2})\) parametrisations

We here prove the equivalence between the \(\frac{1}{2}(w_{+}^{2}-w_{-}^{2})\) and \(u\odot v\) parametrisations, **that we use throughout the proofs in the Appendix.**

**Proposition 6**.: _Let \((\beta_{k})_{k\geqslant 0}\) and \((\beta_{k}^{\prime})_{k\geqslant 0}\) be respectively generated by stochastic gradient descent on the \(u\odot v\) and \(\frac{1}{2}(w_{+}^{2}-w_{-}^{2})\) parametrisations:_

\[(u_{k+1},v_{k+1})=(u_{k},v_{k})-\gamma_{k}\nabla_{u,v}\big{(}\mathcal{L}_{ \mathcal{B}_{k}}(u\odot v)\big{)}(u_{k},v_{k})\,,\]

_and_

\[w_{\pm,k+1}=w_{\pm,k}-\gamma_{k}\nabla_{w_{\pm}}\big{(}\mathcal{L}_{\mathcal{ B}_{k}}(\frac{1}{2}(w_{+}^{2}-w_{-}^{2}))\big{)}(w_{+,k},w_{-,k})\,,\]

_initialised as \(u_{0}=\sqrt{2}\bm{\alpha},v_{0}=0\) and \(w_{+,0}=w_{-,0}=\bm{\alpha}\). Then for all \(k\geqslant 0\), we have \(\beta_{k}=\beta_{k}^{\prime}\)._

Proof.: We have:

\[w_{\pm,0}=\bm{\alpha}\,,\quad w_{\pm,k+1}=(1\mp\gamma_{k}\nabla\mathcal{L}_{ \mathcal{B}_{k}}(\beta_{k}^{\prime}))w_{\pm,k}\,,\]

and

\[u_{0}=\sqrt{2}\bm{\alpha}\,,\quad v_{0}=0\,,\quad u_{k+1}=u_{k}-\gamma_{k} \nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})v_{k}\,,\quad v_{k+1}=v_{k}- \gamma_{k}\nabla\mathcal{L}(\beta_{k})u_{k}\,.\]

Hence,

\[\beta_{k+1}=(1+\gamma_{k}^{2}\nabla\mathcal{L}(\beta_{k})^{2})\beta_{k}-\gamma _{k}(u_{k}^{2}+v_{k}^{2})\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\,,\]

and

\[\beta_{k+1}^{\prime}=(1+\gamma_{k}^{2}\nabla\mathcal{L}_{\mathcal{B}_{k}}( \beta_{k}^{\prime})^{2})\beta_{k}^{\prime}-\gamma_{k}(w_{+,k}^{2}+w_{-,k}^{2} )\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}^{\prime})\,.\]

Then, let \(z_{k}=\frac{1}{2}(u_{k}^{2}-v_{k}^{2})\) and \(z_{k}^{\prime}=w_{+,k}w_{-k}\). We have \(z_{0}=\bm{\alpha}^{2}\), \(z_{0}^{\prime}=\bm{\alpha}^{2}\) and:

\[z_{k+1}=(1-\gamma_{k}^{2}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})^{2})z_ {k}\,,\quad z_{k+1}^{\prime}=(1-\gamma_{k}^{2}\nabla\mathcal{L}_{\mathcal{B}_{k }}(\beta_{k}^{\prime})^{2})z_{k}^{\prime}\,.\]

Using \(a^{2}+b^{2}=\sqrt{(2ab)^{2}+(a^{2}-b^{2})^{2}}\) for \(a,b\in\mathbb{R}\), we finally obtain that:

\[u_{k}^{2}+v_{k}^{2}=\sqrt{(2\beta_{k})^{2}+(2z_{k})^{2}}\,,\quad w_{+,k}^{2}+w_ {-,k}^{2}=\sqrt{(2\beta_{k}^{\prime})^{2}+(2z_{k}^{\prime})^{2}}\,.\]

We conclude by observing that \((\beta_{k},z_{k})\) and \((\beta_{k}^{\prime},z_{k}^{\prime})\) follow the exact same recursions, initialised at the same value \((0,\bm{\alpha}^{2})\).

Convergence of \(\psi_{\alpha}\) to a weighted \(\ell_{1}\) norm and harmful behaviour

We show that when taking the scale of the initialisation to \(0\), one must be careful in the characterisation of the limiting norm, indeed if each entry does not go to zero "at the same speed", then the limit norm is a **weighted**\(\ell_{1}\)-norm rather than the classical \(\ell_{1}\) norm.

**Proposition 7**.: _For \(\alpha\geqslant 0\) and a vector \(h\in\mathbb{R}^{d}\), let \(\tilde{\alpha}=\alpha\exp(-h\ln(1/\alpha))\in\mathbb{R}^{d}\). Then we have that for all \(\beta\in\mathbb{R}^{d}\)_

\[\psi_{\tilde{\alpha}}(\beta)\underset{\alpha\to 0}{\sim}\ln(\frac{1}{ \alpha})\cdot\sum_{i=1}^{d}(1+h_{i})|\beta_{i}|.\]

Proof.: Recall that

\[\psi_{\tilde{\alpha}}(\beta)=\frac{1}{2}\sum_{i=1}^{d}\left(\beta_{i}\mathrm{ arcsinh}(\frac{\beta_{i}}{\tilde{\alpha}_{i}^{2}})\,-\sqrt{\beta_{i}^{2}+ \tilde{\alpha_{i}}^{4}}+\tilde{\alpha}_{i}^{2}\right)\]

Using that \(\mathrm{arcsinh}(x)\underset{|x|\rightarrow\infty}{\sim}\mathrm{sgn}(x)\ln(| x|)\), and that \(\ln(\frac{1}{\tilde{\alpha}_{i}^{2}})=(1+h_{i})\ln(\frac{1}{\alpha^{2}})\) we obtain that

\[\psi_{\tilde{\alpha}}(\beta) \underset{\alpha\to 0}{\sim}\frac{1}{2}\sum_{i=1}^{d} \mathrm{sgn}(\beta_{i})\beta_{i}(1+h_{i})\ln(\frac{1}{\alpha^{2}})\] \[=\frac{1}{2}\ln(\frac{1}{\alpha^{2}})\sum_{i=1}^{d}(1+h_{i})| \beta_{i}|.\]

The following Fig. 7 illustrates the effect of the non-uniform shape \(\bm{\alpha}\) on the corresponding potential \(\psi_{\bm{\alpha}}\).

More generally, for \(\alpha\) such that \(\alpha_{i}\to 0\) for all \(i\in[d]\) at rates such that \(\ln(1/\alpha_{i})\sim q_{i}\ln(1/\max_{i}\alpha_{i})\), we retrieve a weighted \(\ell_{1}\) norm:

\[\frac{\psi_{\alpha}(\beta)}{\ln(1/\alpha^{2})}\rightarrow\sum_{i=1}^{d}q_{i} |\beta_{i}|\,.\]

Figure 7: _Left_: Uniform \(\bm{\alpha}=\alpha\bm{1}\): a smaller scale \(\alpha\) leads to the potential \(\psi_{\alpha}\) being closer to the \(\ell_{1}\)-norm. _Right_: A non uniform \(\bm{\alpha}\) can lead to the recovery of a solution which is very far from the minimum \(\ell_{1}\)-norm solution. The affine line corresponds to the set of interpolators when \(n=1\), \(d=2\) and \(s=1\).

Hence, even for arbitrary small \(\max_{i}\alpha_{i}\), if the _shape_ of \(\alpha\) is 'bad', the interpolator \(\beta_{\alpha}\) that minimizes \(\psi_{\alpha}\) can be arbitrary far away from \(\beta_{\ell_{1}}^{\star}\) the interpolator of minimal \(\ell_{1}\) norm.

We illustrate the importance of the previous proposition in the following example.

**Example 1**.: _We illustrate how, even for arbitrary small \(\max_{i}\alpha_{i}\), the interpolator \(\beta_{\alpha}^{\star}\) that minimizes \(\psi_{\alpha}\) can be far from the minimum \(\ell_{1}\) norm solution, due to the shape of \(\boldsymbol{\alpha}\) that is not uniform. The message of this example is that for \(\boldsymbol{\alpha}\to 0\) non-uniformly across coordinates, if the coordinates of \(\alpha\) that go slowly to \(0\) coincide with the non-null coordinates of the sparse interpolator we want to retrieve, then \(\beta_{\alpha}^{\star}\) will be far from the sparse solution._

_A simple counterexample can be built: let \(\beta_{\mathrm{sparse}}^{\star}=(1,\ldots,1,0,\ldots,0)\) (with only the \(s=o(d)\) first coordinates that are non-null), and let \((x_{i}),(y_{i})\) be generated as \(y_{i}=\langle\beta_{\mathrm{sparse}}^{\star},x_{i}\rangle\) with \(x_{i}\sim\mathcal{N}(0,1)\). For \(n\) large enough (\(n\) of order \(s\ln(d)\) where \(s\) is the sparsity), the design matrix \(X\) is RIP [10], so that the minimum \(\ell_{1}\) norm interpolator \(\beta_{\ell_{1}}^{\star}\) is exactly equal to \(\beta_{\mathrm{sparse}}^{\star}\)._

_However, if \(\alpha\) is such that \(\max_{i}\alpha_{i}\to 0\) with \(h_{i}>>1\) for \(j\leqslant s\) and \(h_{i}=1\) for \(i\geqslant s+1\) (\(h_{i}\) as in Proposition 7), \(\beta_{\alpha}^{\star}\) will be forced to verify \(\beta_{\alpha,i}^{\star}=0\) for \(i\leqslant s\) and hence \(\|\beta_{\alpha,1}^{\star}-\beta_{\ell_{1}}^{\star}\|_{1}\geqslant s\)._

## Appendix E Main descent lemma and boundedness of the iterates

The goal of this section is to prove the following proposition, our main descent lemma: for well-chosen stepsizes, the Bregman divergences \((D_{h_{k}}(\beta^{\star},\beta_{k}))_{k\geqslant 0}\) decrease. We then use this proposition to bound the iterates for both SGD and GD.

**Proposition 8**.: _There exist a constant \(c>0\) and \(B>0\) such that \(B=\mathcal{O}(\inf_{\beta^{\star}\in\mathcal{S}}\|\beta^{\star}\|_{\infty})\) for GD and \(B=\mathcal{O}(\ln(1/\alpha)\inf_{\beta^{\star}\in\mathcal{S}}\|\beta^{\star}\| _{\infty})\) for SGD, such that if \(\gamma_{k}\leqslant\frac{c}{LB}\) for all \(k\), then we have, for all \(k\geqslant 0\) and any interpolator \(\beta^{\star}\in\mathcal{S}\):_

\[D_{h_{k+1}}(\beta^{\star},\beta_{k+1})\leqslant D_{h_{k}}(\beta^{\star},\beta _{k})-\gamma_{k}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\,.\]

To prove this result, we first provide a general descent lemma for time-varying mirror descent (Proposition 9, appendix E.1), before proving the proposition for fixed iteration \(k\) and bound \(B>0\) on the iterates infinity norm in Appendix E.2 (Proposition 10). We finally use this to prove a bound on the iterates infinity norm in appendix E.3.

### Descent lemma for (stochastic) mirror descent with varying potentials

In the following we adapt a classical mirror descent equality but for time varying potentials, that differentiates from Orabona et al. [47] in that it enables us to prove the decrease of the Bregman divergences of the iterates. Moreover, as for classical MD, it is an equality.

**Proposition 9**.: _For \(h,g:\mathbb{R}^{d}\to\mathbb{R}\) functions, let \(D_{h,g}(\beta,\beta^{\prime})=h(\beta)-g(\beta^{\prime})-\langle\nabla g( \beta^{\prime}),\beta-\beta^{\prime}\rangle\)4 for \(\beta,\beta^{\prime}\in\mathbb{R}^{d}\). Let \((h_{k})\) strictly convex functions defined \(\mathbb{R}^{d}\)\(\mathcal{L}\) a convex function defined on \(\mathbb{R}^{d}\). Let \((\beta_{k})\) defined recursively through \(\beta_{0}\in\mathbb{R}^{d}\), and_

Footnote 4: for \(h=g\), we recover the classical Bregman divergence that we denote \(D_{h}=D_{h,h}\)

\[\beta_{k+1}\in\operatorname*{argmin}_{\beta\in\mathbb{R}^{d}}\left\{\gamma_{k} \langle\nabla\mathcal{L}(\beta_{k}),\beta-\beta_{k}\rangle+D_{h_{k+1},h_{k}}( \beta,\beta_{k})\right\},\]

_where we assume that the minimum is unique and attained in \(\mathbb{R}^{d}\). Then, \((\beta_{k})\) satisfies_

\[\nabla h_{k+1}(\beta_{k+1})=\nabla h_{k}(\beta_{k})-\gamma_{k}\nabla\mathcal{L} (\beta_{k})\,,\]

_and for any \(\beta\in\mathbb{R}^{d}\),_

\[D_{h_{k+1}}(\beta,\beta_{k+1}) =D_{h_{k}}(\beta,\beta_{k})-\gamma_{k}\langle\nabla\mathcal{L}( \beta_{k}),\beta_{k}-\beta\rangle+D_{h_{k+1}}(\beta_{k},\beta_{k+1})\] \[\quad-\big{(}h_{k+1}-h_{k}\big{)}(\beta_{k})+\big{(}h_{k+1}-h_{k} \big{)}(\beta)\,.\]

Proof.: Let \(\beta\in\mathbb{R}^{d}\). Since we assume that the minimum through which \(\beta_{k+1}\) is computed is attained in \(\mathbb{R}^{d}\), the gradient of the function \(V_{k}(\beta)=\gamma_{k}\langle\nabla\mathcal{L}(\beta_{k}),\beta-\beta_{k} \rangle+D_{h_{k+1},h_{k}}(\beta,\beta_{k})\) evaluated at \(\beta_{k+1}\) is null, leading to \(\nabla h_{k+1}(\beta_{k+1})=\nabla h_{k}(\beta_{k})-\gamma_{k}\nabla\mathcal{L} (\beta_{k})\).

Then, since \(\nabla V_{k}(\beta_{k+1})=0\), we have \(D_{V_{k}}(\beta,\beta_{k+1})=V_{k}(\beta)-V_{k}(\beta_{k+1})\). Using \(\nabla^{2}V_{k}=\nabla^{2}h_{k+1}\), we also have \(D_{V_{k}}=D_{h_{k+1}}\). Hence:

\[D_{h_{k+1}}(\beta,\beta_{k+1})=\gamma_{k}\langle\nabla\mathcal{L}(\beta_{k}), \beta-\beta_{k+1}\rangle+D_{h_{k+1},h_{k}}(\beta,\beta_{k})-D_{h_{k+1},h_{k}}( \beta_{k+1},\beta_{k})\,.\]

We write \(\gamma_{k}\langle\nabla\mathcal{L}(\beta_{k}),\beta-\beta_{k+1}\rangle=\gamma_ {k}\langle\nabla\mathcal{L}(\beta_{k}),\beta-\beta^{k}\rangle+\gamma_{k} \langle\nabla\mathcal{L}(\beta_{k}),\beta_{k}-\beta_{k+1}\rangle\). We also have \(\gamma_{k}\langle\nabla\mathcal{L}(\beta_{k}),\beta_{k}-\beta_{k+1}\rangle= \langle\nabla h_{k}(\beta_{k})-\nabla h_{k+1}(\beta_{k+1}),\beta_{k}-\beta_{k +1}\rangle=D_{h_{k},h_{k+1}}(\beta_{k},\beta_{k+1})+D_{h_{k+1},h_{k}}(\beta_{k +1},\beta^{k})\), so that \(\gamma_{k}\langle\nabla\mathcal{L}(\beta_{k}),\beta_{k}-\beta_{k+1}\rangle-D_{ h_{k+1},h_{k}}(\beta_{k+1},\beta^{k})=D_{h_{k},h_{k+1}}(\beta_{k},\beta_{k+1})\). Thus,

\[D_{h_{k+1}}(\beta,\beta_{k+1})=D_{h_{k+1},h_{k}}(\beta,\beta_{k})-\gamma_{k} \big{(}D_{f}(\beta,\beta_{k})+D_{f}(\beta_{k},\beta)\big{)}+D_{h_{k},h_{k+1}}( \beta_{k},\beta_{k+1})\,,\]

and writing \(D_{h,g}(\beta,\beta^{\prime})=D_{g}(\beta,\beta^{\prime})+h(\beta)-g(\beta)\) concludes the proof. 

### Proof of Proposition 10

In next proposition, we use Proposition 9 to prove our main descent lemma. To that end, we bound the error terms that appear in Proposition 9 as functions of \(\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\) and norms of \(\beta_{k},\beta_{k+1}\), so that for explicit stepsizes, the error terms can be cancelled by half of the negative quantity \(-2\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\).

**Additional notation:** let \(L_{2},L_{\infty}>0\) such that \(\forall\beta\), \(\left\|H_{\mathcal{B}}\beta\right\|_{2}\leqslant L\|\beta\|_{2}\), \(\left\|H_{\mathcal{B}}\beta\right\|_{\infty}\leqslant L\|\beta\|_{\infty}\) for all batches \(\mathcal{B}\subset[n]\) of size \(b\).

**Proposition 10**.: _Let \(k\geqslant 0\) and \(B>0\). Provided that \(\left\|\beta_{k}\right\|_{\infty},\left\|\beta_{k+1}\right\|_{\infty},\left\| \beta^{\star}\right\|_{\infty}\leqslant B\) and \(\gamma_{k}\leqslant\frac{c}{LB}\) where \(c>0\) is some numerical constant, we have:_

\[D_{h_{k+1}}(\beta^{\star},\beta_{k+1})\leqslant D_{h_{k}}(\beta^{\star},\beta _{k})-\gamma_{k}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\,.\] (21)

Proof.: Let \(\beta^{\star}\in\mathcal{S}\) be any interpolator. From Proposition 9:

\[D_{h_{k+1}}(\beta^{\star},\beta_{k+1})=D_{h_{k}}(\beta^{\star},\beta_{k})-2 \gamma_{k}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})+D_{h_{k+1}}(\beta_{k+1}, \beta_{k})-(h_{k+1}-h_{k})(\beta_{k})+(h_{k+1}-h_{k})(\beta^{\star}).\]

We want to bound the last three terms of this equality. First, to bound the last two we apply Lemma 7 assuming that \(\|\beta^{\star}\|_{\infty},\|\beta_{k+1}\|_{\infty}\leqslant B\):

\[-(h_{k+1}-h_{k})(\beta_{k})+(h_{k+1}-h_{k})(\beta^{\star})\leqslant 24BL_{2} \gamma_{k}^{2}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\]

We now bound \(D_{h_{k+1}}(\beta_{k},\beta_{k+1})\). Classical Bregman manipulations provide that

\[D_{h_{k+1}}(\beta_{k},\beta_{k+1}) =D_{h_{k+1}^{\star}}(\nabla h_{k+1}(\beta_{k+1}),\nabla h_{k+1}( \beta_{k}))\] \[=D_{h_{k+1}^{\star}}(\nabla h_{k}(\beta^{k})-\gamma_{k}\nabla \mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}),\nabla h_{k+1}(\beta_{k}))\,.\]

From Lemma 6 we have that \(h_{k+1}\) is \(\min(1/(4\alpha_{k+1}^{2}),1/(4B))\) strongly convex on the \(\ell^{\infty}\)-centered ball of radius \(B\) therefore \(h_{k+1}^{\ast}\) is \(\max(4\alpha_{k+1}^{2},4B)=4B\) (for \(\alpha\) small enough or \(B\) big enough) smooth on this ball, leading to:

\[D_{h_{k+1}}(\beta_{k},\beta_{k+1}) \leqslant 2B\|\nabla h_{k}(\beta_{k})-\gamma_{k}\nabla\mathcal{L}_{ \mathcal{B}_{k}}(\beta_{k})-\nabla h_{k+1}(\beta_{k})\|_{2}^{2}\] \[\leqslant 4B\big{(}\|\nabla h_{k}(\beta_{k})-\nabla h_{k+1}(\beta_{k}) \|_{2}^{2}+\|\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\|_{2}^{2} \big{)}\,.\]

Using \(|\nabla h_{k}(\beta)-\nabla h_{k+1}(\beta)|\leqslant 2\delta_{k}\) where \(\delta_{k}=q(\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}))\), we get that:

\[D_{h_{k+1}}(\beta_{k},\beta_{k+1})\leqslant 8B\|\delta_{k}\|_{2}^{2}+4BL\gamma_{k}^{2} \mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\,.\]

Now, \(\left\|\delta_{k}\right\|_{2}^{2}\leqslant\left\|\delta_{k}\right\|_{1}\left\| \delta_{k}\right\|_{\infty}\) and using Lemma 5, \(\left\|\delta_{k}\right\|_{1}\|\delta_{k}\|_{\infty}\leqslant 4\|\gamma_{k}\nabla \mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\|_{2}^{2}\|\gamma_{k}\nabla\mathcal{L}_{ \mathcal{B}_{k}}(\beta_{k})\|_{\infty}^{2}\leqslant 2\|\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\|_{2}^{2}\) since \(\left\|\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\right\|_{\infty} \leqslant\gamma_{k}L_{\infty}\|\beta_{k}-\beta_{\infty}\|\leqslant\gamma_{k} \times 2LB\leqslant 1/2\) is verified for \(\gamma_{k}\leqslant 1/(4LB)\). Thus,

\[D_{h_{k+1}}(\beta_{k},\beta_{k+1})\leqslant 40BL_{2}\gamma_{k}^{2}\mathcal{L}_{\mathcal{B}_{k}}( \beta_{k})\,.\]

Hence, provided that \(\left\|\beta_{k}\right\|_{\infty}\leqslant B\), \(\left\|\beta_{k+1}\right\|_{\infty}\leqslant B\) and \(\gamma_{k}\leqslant 1/(4LB)\), we have:

\[D_{h_{k+1}}(\beta^{\star},\beta_{k+1})\leqslant D_{h_{k}}(\beta^{\star},\beta_{k })-2\gamma_{k}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})+64L_{2}\gamma_{k}^{2}B \mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\,,\]

and thus

\[D_{h_{k+1}}(\beta^{\star},\beta_{k+1})\leqslant D_{h_{k}}(\beta^{\star},\beta_{ k})-\gamma_{k}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\,.\]

if \(\gamma_{k}\leqslant\frac{c}{BL}\), where \(c=\frac{1}{64}\).

### Bound on the iterates

We now bound the iterates \((\beta_{k})\) by an explicit constant \(B\) that depends on \(\left\lVert\beta^{\star}\right\rVert_{1}\) (for any fixed \(\beta^{\star}\in\mathcal{S}\)).

The first bound we prove holds for both SGD and GD, and is of the form \(\mathcal{O}(\left\lVert\beta^{\star}\right\rVert_{1}\ln(1/\alpha^{2})\) while the second bound, that holds only for GD (\(b=n\)) is of order \(\mathcal{O}(\left\lVert\beta^{\star}\right\rVert_{1})\) (independent of \(\alpha\)). While a bound independent of \(\alpha\) is only proved for GD, we believe that such a result also holds for SGD, and in both cases \(B\) should be thought of order \(\mathcal{O}(\left\lVert\beta^{\star}\right\rVert_{1})\).

#### e.3.1 Bound that depends on \(\alpha\) for GD and SGD

A consequence of Proposition 10 is the boundedness of the iterates, as shown in next corollary. Hence, Proposition 10 can be applied using \(B\) a uniform bound on the iterates \(\ell^{\infty}\) norm.

**Corollary 1**.: _Let \(B=3\left\lVert\beta^{\star}\right\rVert_{1}\ln\left(1+\frac{\left\lVert\beta^{ \star}\right\rVert_{1}}{\alpha^{2}}\right)\). For stepsizes \(\gamma_{k}\leqslant\frac{c}{BL}\), we have \(\left\lVert\beta_{k}\right\rVert_{\infty}\leqslant B\) for all \(k\geqslant 0\)._

Proof.: We proceed by induction. Let \(k\geqslant 0\) such that \(\left\lVert\beta_{k}\right\rVert_{\infty}\leqslant B\) for some \(B>0\) and \(D_{h_{k}}(\beta^{\star},\beta_{k})\leqslant D_{h_{0}}(\beta^{\star},\beta_{0})\) (note that these two properties are verified for \(k=0\), since \(\beta_{0}=0\)). For \(\gamma_{k}\) sufficiently small (_i.e._, that satisfies \(\gamma_{k}\leqslant\frac{c}{BL}\) where \(B^{\prime}\geqslant\left\lVert\beta_{k+1}\right\rVert_{\infty},\left\lVert \beta_{k}\right\rVert_{\infty},\left\lVert\beta^{\star}\right\rVert_{\infty}\)), using Proposition 10, we have \(D_{h_{k+1}}(\beta^{\star},\beta_{k+1})\leqslant D_{h_{k}}(\beta^{\star},\beta_{ k})\) so that \(D_{h_{k+1}}(\beta^{\star},\beta_{k+1})\leqslant D_{h_{0}}(\beta^{\star},\beta_{0})\), which can be rewritten as:

\[\sum_{i=1}^{d}\alpha_{k+1,i}^{2}(\sqrt{1+(\frac{\beta_{k+1,i}}{\alpha_{k+1,i} ^{2}})^{2}}-1)\leqslant\sum_{i=1}^{d}\beta_{i}^{\star}\operatorname{arcsinh}( \frac{\beta_{k+1,i}}{\alpha^{2}})\,.\]

Hence, \(\left\lVert\beta_{k+1}\right\rVert_{1}\leqslant\left\lVert\beta^{\star} \right\rVert_{1}\ln(1+\frac{\left\lVert\beta_{k+1}\right\rVert_{1}}{\alpha^{ 2}})\). We then notice that for \(x,y>0\), \(x\leqslant y\ln(1+x)\implies x\leqslant 3y\ln(1+y)\): if \(x>y\ln(1+y)\) and \(x>y\), we have that \(y\ln(1+y)<y\ln(1+x)\), so that \(1+y<1+x\), which contradicts our assumption. Hence, \(x\leqslant\max(y,y\ln(1+y))\). In our case, \(x=\left\lVert\beta^{k+1}\right\rVert_{1}/\alpha^{2}\), \(y=\left\lVert\beta^{\star}\right\rVert_{1}/\alpha^{2}\) so that for small alpha, \(\ln(1+y)\geqslant 1\).

Hence, we deduce that \(\left\lVert\beta_{k+1}\right\rVert_{1}\leqslant B\), where \(B=\left\lVert\beta^{\star}\right\rVert_{1}\ln(1+\frac{\left\lVert\beta^{\star} \right\rVert_{1}}{\alpha^{2}})\).

This is true as long as \(\gamma_{k}\) is tuned using \(B^{\prime}\) a bound on \(\max(\left\lVert\beta_{k}\right\rVert_{\infty},\left\lVert\beta_{k+1}\right \rVert_{\infty})\). Using the continuity of \(\beta_{k+1}\) as a function of \(\gamma_{k}\) (\(\beta_{k}\) being fixed), we show that \(\gamma_{k}\leqslant\frac{1}{2}\times\frac{c}{BL}\) can be used using this \(B\). Indeed, let \(\phi:\mathbb{R}^{+}\rightarrow\mathbb{R}^{d}\) be the function that takes as entry \(\gamma_{k}\geqslant 0\) and outputs the corresponding \(\left\lVert\beta_{k+1}\right\rVert_{\infty}\): \(\phi\) is continuous. Let \(\gamma_{r}=\frac{1}{2}\times\frac{c}{rL}\) for \(r>0\) and \(\bar{r}=\sup\left\{r\geqslant 0:B<\phi(\gamma_{r})\right\}\) (the set is upper-bounded; if is is empty, we do not need what follows since it means that any stepsize leads to \(\left\lVert\beta_{k+1}\right\rVert_{\infty}\leqslant B\)). By continuity of \(\phi\), \(\phi(\gamma_{\bar{r}})=B\). Furthermore, for all \(r\) that satisfies \(r\geqslant\max(\phi(\gamma_{r}),B)\geqslant\max(\phi(\gamma_{r}),\left\lVert \beta_{k}\right\rVert_{\infty},\left\lVert\beta^{\star}\right\rVert_{\infty})\), we have, using what is proved just above, that \(\left\lVert\beta_{k+1}\right\rVert_{\infty}\leqslant B\) and thus \(\phi(\gamma_{r})\leqslant B\) for such a \(r\):

**Lemma 1**.: _For \(r>0\) such that \(r\geqslant\max(\phi(\gamma_{r}),B)\), we have \(\phi(\gamma_{r})\leqslant B\)._

Now, if \(\bar{r}>B\), by definition of \(\bar{r}\) and by continuity of \(\phi\), since \(\phi(\bar{r})=B\), there exists some \(B<r<\bar{r}\) such that \(\phi(\gamma_{r})>B\) (definition of the supremum) and \(\phi(\gamma_{r})\leqslant 2B\) (continuity of \(\phi\)). This particular choice of \(r\) thus satisfies \(r>B\) and and \(\phi(\gamma_{r})\leqslant 2B\leqslant 2r\), leading to \(\phi(\gamma_{r})\leqslant B\), using Lemma 1, hence a contradiction: we thus have \(\bar{r}\leqslant B\).

This concludes the induction: for all \(r\geqslant B\), we have \(r\geqslant\bar{r}\) so that \(\phi(\gamma_{r})\leqslant B\) and thus for all stepsizes \(\gamma\leqslant\frac{c}{2LB}\), we have \(\left\lVert\beta_{k+1}\right\rVert_{\infty}\leqslant B\).

#### e.3.2 Bound independent of \(\alpha\)

We here assume in this subsection that \(b=n\). We prove that for gradient descent, the iterates are bounded by a constant that does not depend on \(\alpha\).

**Proposition 11**.: _Assume that \(b=n\) (full batch setting). There exists some \(B=\mathcal{O}(\left\lVert\beta^{\star}\right\rVert_{1})\) such that for stepsizes \(\gamma_{k}\leqslant\frac{c}{BL}\), we have \(\left\lVert\beta_{k}\right\rVert_{\infty}\leqslant B\) for all \(k\geqslant 0\)._Proof.: We first begin by proving the following proposition: for sufficiently small stepsizes, the loss values decrease. In the following lemma we provide a bound on the gradient descent iterates \((w_{+,k},w_{-,k})\) which will be useful to show that the loss is decreasing.

**Proposition 12**.: _For \(\gamma_{k}\leqslant\frac{c}{LB}\) where \(B\geqslant\max(\left\lVert\beta_{k}\right\rVert_{\infty},\left\lVert\beta_{k+1 }\right\rVert_{\infty})\), we have \(\mathcal{L}(\beta_{k+1})\leqslant\mathcal{L}(\beta_{k})\)_

Proof.: Oddly, using the time-varying mirror descent recursion is not the easiest way to show the decrease of the loss, due to the error terms which come up. Therefore to show that the loss is decreasing we use the gradient descent recursion. Recall that the iterates \(w_{k}=(w_{+,k},w_{-,k})\in\mathbb{R}^{2d}\) follow a gradient descent on the non convex loss \(F(w)=\frac{1}{2}\|y-\frac{1}{2}X(w_{+}^{2}-w_{-}^{2})\|_{2}\).

For \(k\geqslant 0\), using the Taylor formula we have that \(F(w_{k+1})\leqslant F(w_{k})-\gamma_{k}(1-\frac{\gamma_{k}L_{k}}{2})\|\nabla F (w_{k})\|^{2}\) with the local smoothness \(L_{k}=\sup_{w\in[w_{k},w_{k+1}]}\lambda_{\max}(\nabla^{2}F(w))\). Hence if \(\gamma_{k}\leqslant 1/L_{k}\) for all \(k\) we get that the loss is non-increasing. We now bound \(L_{k}\). Computing the heasian ot \(F\), we obtain that:

\[\begin{split}\nabla^{2}F(w_{k})&=\begin{pmatrix} \operatorname{diag}(\nabla\mathcal{L}(\beta_{k}))&0\\ 0&-\operatorname{diag}(\nabla\mathcal{L}(\beta_{k}))\end{pmatrix}\\ &+\begin{pmatrix}\operatorname{diag}(w_{+,k})H\operatorname{diag}(w_{+,k} )&-\operatorname{diag}(w_{-,k})H\operatorname{diag}(w_{+,k})\\ -\operatorname{diag}(w_{+,k})H\operatorname{diag}(w_{-,k})&\operatorname{ diag}(w_{-,k})H\operatorname{diag}(w_{-,k})\end{pmatrix}\,.\end{split}\] (22)

Let us denote by \(M=\begin{pmatrix}M_{+}&M_{+,-}\\ M_{+,-}&M_{-}\end{pmatrix}\in\mathbb{R}^{2d\times 2d}\) the second matrix in the previous equality. With this notation \(\|\nabla^{2}F(w_{k})\|\leqslant\|\nabla\mathcal{L}(\beta_{k})\|_{\infty}+2\|M\|\) (where the norm corresponds to the Schatten \(2\)-norm which is the largest eigenvalue for symmetric matrices). Now, notice that:

\[\begin{split}\|M\|^{2}&=\sup_{u\in\mathbb{R}^{2d},\|u\|=1} \|Mu\|^{2}\\ &=\sup_{\begin{subarray}{c}u_{+}\in\mathbb{R}^{d},\|u_{+}\|=1\\ u_{-}=\mathbb{R}^{d},\|u_{-}\|=1\\ (a,b)\in\mathbb{R}^{2},a^{2}+b^{2}=1\end{subarray}}\left\|M\begin{pmatrix}a \cdot u_{+}\\ b\cdot u_{-}\end{pmatrix}\right\|^{2}.\\ \end{split}\]

We have:

\[\begin{split}\left\|M\begin{pmatrix}a\cdot u_{+}\\ b\cdot u_{-}\end{pmatrix}\right\|^{2}&=\left\|\begin{pmatrix}aM_{+}u_{+}+bM_{ +-}u_{-}\\ aM_{+-}u_{+}+bM_{-}u_{-}\end{pmatrix}\right\|^{2}\\ &=\|aM_{+}u_{+}+bM_{+-}u_{-}\|^{2}+\|aM_{+-}u_{+}+bM_{-}u_{-}\|^{2}\\ &\leqslant 2\Big{(}a^{2}\|M_{+}u_{+}\|^{2}+b^{2}\|M_{+-}u_{-}\|^{2}+a^{2 }\|M_{+-}u_{+}\|^{2}+b^{2}\|M_{-}u_{-}\|^{2}\Big{)}\\ &\leqslant 2\Big{(}\|M_{+}\|^{2}+\|M_{+-}\|^{2}+\|M_{-}\|^{2}\Big{)} \,.\end{split}\]

Since \(\|M_{\pm}\|\leqslant\lambda_{max}\cdot\|w_{\pm}\|_{\infty}^{2}\) and \(\|M_{+-}\|\leqslant\lambda_{max}\|w_{+}\|_{\infty}\|w_{-}\|_{\infty}\) we finally get that

\[\begin{split}\|M\|^{2}&\leqslant 6\lambda_{max}^{2} \cdot\max(\|w_{+}\|_{\infty}^{2},\|w_{-}\|_{\infty}^{2})^{2}\\ &\leqslant 6\lambda_{max}^{2}(\|w_{+}^{2}\|_{\infty}+\|w_{-}^{2}\|_{ \infty})^{2}\\ &\leqslant 12\lambda_{max}^{2}\|w_{+}^{2}+w_{-}^{2}\|_{\infty} ^{2}\,.\end{split}\]

We now upper bound this quantity in the following lemma.

**Lemma 2**.: _For all \(k\geqslant 0\), the following inequality holds component-wise:_

\[w_{+,k}^{2}+w_{-,k}^{2}=\sqrt{4\boldsymbol{\alpha}_{k}^{4}+\beta_{k}^{2}}\,.\]

Proof.: Notice from the definition of \(w_{+,k}\) and \(w_{-,k}\) given in the proof of Proposition 5 that:

\[|w_{+,k}||w_{-,k}|=\boldsymbol{\alpha}_{-,k}\boldsymbol{\alpha}_{+,k}= \boldsymbol{\alpha}_{k}^{2}.\] (23)And \(\bm{\alpha}_{0}=\bm{\alpha}^{2}\). Now since \(\bm{\alpha}_{k}\) is decreasing coordinate-wise (under our assumptions on the stepsizes, \(\gamma_{k}^{2}\nabla\mathcal{L}(\beta_{k})^{2}\leqslant(1/2)^{2}<1\)), we get that.:

\[w_{+,k}^{2}+w_{-,k}^{2}=2\sqrt{\bm{\alpha}_{k}^{4}+\beta_{k}^{2}}\leqslant 2 \sqrt{\bm{\alpha}^{4}+\beta_{k}^{2}}\]

leading to \(w_{+,k}^{2}+w_{-,k}^{2}\leqslant\sqrt{4\bm{\alpha}^{4}+B^{2}}\). 

From Lemma 2, \(w_{+,k}^{2}+w_{-,k}^{2}\) is bounded by \(2\sqrt{\bm{\alpha}^{4}+B^{2}}\). Putting things together we finally get that \(\|\nabla^{2}F(w)\|\leqslant\|\nabla\mathcal{L}(\beta)\|_{\infty}+8\lambda_{ max}\sqrt{4\|\bm{\alpha}\|_{\infty}^{4}+B^{2}}\). Hence,

\[L_{k}\leqslant\sup_{\|\beta\|_{\infty}\leqslant B}\|\nabla\mathcal{L}(\beta) \|_{\infty}+8\lambda_{\max}\sqrt{\|\bm{\alpha}\|_{\infty}^{4}+B^{2}} \leqslant LB+8\lambda_{\max}\sqrt{\|\bm{\alpha}\|_{\infty}^{4}+B^{2}} \leqslant 10LB\,,\]

for \(B\geqslant\|\bm{\alpha}\|_{\infty}^{2}\). 

We finally prove the bound on \(\left\|\beta_{k}\right\|_{\infty}\) independent of \(\alpha\) for a uniform initialisation \(\bm{\alpha}=\alpha\bm{1}\), using the monotonic property of \(\mathcal{L}\).

**Proposition 13**.: _Assume that \(b=n\) (full batch setting). There exists some \(B=\mathcal{O}(\left\|\beta^{\star}\right\|_{1})\) such that for stepsizes \(\gamma_{k}\leqslant\frac{c}{BL}\), we have \(\left\|\beta_{k}\right\|_{\infty}\leqslant B\) for all \(k\geqslant 0\)._

Proof.: In this proof, we first let \(B\) be a bound on the iterates. Tuning stepsizes using this bound, we prove that the iterates are bounded by a some \(B^{\prime}=\mathcal{O}(\left\|\beta^{\star}\right\|_{1})\). Finally, we conclude by using the continuity of the iterates (at a finite horizon) that this explicit bound can be used to tune the stepsizes. Writing the mirror descent with varying potentials, we have, since \(\nabla h_{0}(\beta_{0})=0\),

\[\nabla h_{k}(\beta_{k})=-\sum_{\ell<k}\gamma_{\ell}\nabla\mathcal{L}(\beta_{ \ell})\,,\]

leading to, by convexity of \(h_{k}\):

\[h_{k}(\beta_{k})-h_{k}(\beta^{\star})\leqslant\langle\nabla h_{k}(\beta_{k}),\beta_{k}-\beta^{\star}\rangle=-\sum_{\ell<k}\langle\gamma_{\ell}\nabla \mathcal{L}(\beta_{\ell}),\beta_{k}-\beta^{\star}\rangle\,.\]

We then write, using \(\nabla\mathcal{L}(\beta)=H(\beta-\beta^{\star})\) for \(H=XX^{\top}\), that \(-\sum_{\ell<k}\langle\gamma_{\ell}\nabla\mathcal{L}(\beta_{\ell}),\beta_{k}- \beta^{\star}\rangle=-\sum_{\ell<k}\gamma_{\ell}\langle X^{\top}(\bar{\beta}_{ k}-\beta^{\star}),X^{\top}(\beta_{k}-\beta^{\star})\rangle\leqslant\sum_{\ell<k} \gamma_{\ell}\sqrt{\mathcal{L}(\bar{\beta}_{k})\mathcal{L}(\bar{\beta}_{k})}\), leading to:

\[h_{k}(\beta_{k})-h_{k}(\beta^{\star})\leqslant 2\sqrt{\sum_{\ell<k} \gamma_{\ell}\mathcal{L}(\bar{\beta}_{k})\sum_{\ell<k}\gamma_{\ell}\mathcal{L} (\beta_{k})}\leqslant 2\sum_{\ell<k}\gamma_{\ell}\mathcal{L}(\bar{\beta}_{k}) \leqslant 2D_{h_{0}}(\beta^{\star},\beta^{0})\,,\]

where the last inequality holds provided that \(\gamma_{k}\leqslant\frac{1}{CLB}\). Thus,

\[\psi_{\bm{\alpha}_{k}}(\beta_{k})\leqslant\psi_{\bm{\alpha}_{k}}(\beta^{ \star})+2\psi_{\bm{\alpha}_{0}}(\beta^{\star})+\langle\phi_{k},\beta_{k}-\beta^ {\star}\rangle\,.\]

Then, \(\langle\phi_{k},\beta_{k}-\beta^{\star}\rangle\leqslant\left\|\phi_{k}\right\| _{1}\left\|\beta_{k}-\beta^{\star}\right\|_{\infty}\) and \(\left\|\phi_{k}\right\|_{1}\leqslant C\lambda_{\max}\sum_{k<K}\gamma_{k}^{2} \mathcal{L}(\beta^{k})\leqslant C\lambda_{\max}\gamma_{\max}h_{0}(\beta^{\star})\). Then, using

\[\|\beta\|_{\infty}-\frac{1}{\ln(1/\alpha^{2})}\leqslant\frac{\psi_{\alpha}( \beta)}{\ln(1/\alpha^{2})}\leqslant\|\beta\|_{1}\big{(}1+\frac{\ln(\|\beta\|_{1 }+\alpha^{2})}{\ln(1/\alpha^{2})}\big{)}\,,\]

we have:

\[\left\|\beta_{k}\right\|_{\infty} \leqslant\frac{1}{\ln(1/\alpha^{2})}+\|\beta^{\star}\|_{1}\big{(}1 +\frac{\ln(\|\beta^{\star}\|_{1}+\alpha^{2})}{\ln(1/\alpha^{2})}\big{)}+\| \beta^{\star}\|_{1}\big{(}1+\frac{\ln(\|\beta^{\star}\|_{1}+\alpha^{2})}{\ln(1/ \alpha^{2})}\big{)}\] \[\quad+B_{0}C\lambda_{\max}\gamma_{\max}h_{0}(\beta^{\star})/\ln(1/ \alpha^{2})\] \[\leqslant R+B_{0}C\lambda_{\max}\gamma_{\max}h_{0}(\beta^{\star})/ \ln(1/\alpha^{2})\,,\]

where \(R=\mathcal{O}(\left\|\beta^{\star}\right\|_{1})\) is independent of \(\alpha\). Hence, since \(B_{0}=\sup_{k<\infty}\left\|\beta_{k}\right\|_{\infty}<\infty\), we have:

\[B_{0}(1-C\lambda_{\max}\gamma_{\max}h_{0}(\beta^{\star})/\ln(1/\alpha^{2})) \leqslant R\implies B_{0}\leqslant 2R\,,\]provided that \(\gamma_{\max}\leqslant 1/(2C\lambda_{\max}h_{0}(\beta^{\star})/\ln(1/\alpha^{2}))\) (note that \(h_{0}(\beta^{\star})/\ln(1/\alpha^{2})\) is independent of \(\alpha^{2}\)).

Hence, if for all \(k\) we have \(\gamma_{k}\leqslant\frac{1}{C^{\prime}L\bar{B}}\) where \(B\) bounds all \(\left\lVert\beta_{k}\right\rVert_{\infty}\), we have \(\left\lVert\beta_{k}\right\rVert_{\infty}\leqslant 2R\) for all \(k\), where \(R=\mathcal{O}(\left\lVert\beta^{\star}\right\rVert_{1})\) is independent of \(\alpha\) and stepsizes \(\gamma_{k}\).

Let \(K>0\) be fixed, and

\[\bar{\gamma}=\inf\left\{\gamma>0\quad\text{s.t.}\quad\sup_{k\leqslant K}\left \lVert\beta_{k}\right\rVert_{\infty}>2R\right\}.\]

For \(\gamma\geqslant 0\) a constant stepsize, let

\[\varphi(\gamma)=\sup_{k\leqslant K}\left\lVert\beta_{k}\right\rVert_{\infty},\]

which is a continuous function of \(\gamma\). For \(r>0\), let \(\gamma_{r}=\frac{1}{C^{\prime}Lr}\).

An important feature to notice is that if \(\gamma<\gamma_{r}\) and \(r\) bounds all \(\left\lVert\beta_{k}\right\rVert_{\infty},k\leqslant K\), then \(\varphi(\gamma)\leqslant R\), as shown above. We will show that we have \(\bar{\gamma}\geqslant\gamma_{2R}\). Reasoning by contradiction, if \(\bar{\gamma}<\gamma_{2R}\): by continuity of \(\varphi\), we have \(\varphi(\bar{\gamma})\leqslant R\) and thus, there exists some small \(0<\varepsilon<\gamma_{2R}-\bar{\gamma}\) such that for all \(\gamma\in[\bar{\gamma},\bar{\gamma}+\varepsilon]\), we have \(\varphi(\bar{\gamma})\leqslant 2R\).

However, such \(\gamma\)'s verify both \(\varphi(\gamma)\leqslant 2R\) (since \(\gamma\in[\bar{\gamma},\bar{\gamma}+\varepsilon]\) and by definition of \(\varepsilon\)) and \(\gamma\leqslant\gamma_{2R}\) (by definition of \(\varepsilon\)), and hence \(\varphi(\gamma)\leqslant R\). This contradicts the infimum of \(\bar{\gamma}\), and hence \(\bar{\gamma}\geqslant\gamma_{2R}\). Thus, for \(\gamma\leqslant\gamma_{2R}=\frac{1}{2C^{\prime}LR}\), we have \(\left\lVert\beta_{k}\right\rVert_{\infty}\leqslant R\).

## Appendix F Proof of Theorem 1 and 2, and of Proposition 1

### Proof of Theorem 1 and 2

We are now equipped to prove Theorem 1 and Theorem 2, condensed in the following Theorem.

**Theorem 3**.: _Let \((u_{k},v_{k})_{k\geqslant 0}\) follow the mini-batch SGD recursion (3) initialised at \(u_{0}=\sqrt{2}\boldsymbol{\alpha}\in\mathbb{R}_{>0}^{d}\) and \(v_{0}=\boldsymbol{0}\), and let \((\beta_{k})_{k\geqslant 0}=(u_{k}\odot v_{k})_{k\geqslant 0}\). There exists and explicit \(B>0\) and a numerical constant \(c>0\) such that:_

1. _For stepsizes satisfying_ \(\gamma_{k}\leqslant\frac{c}{LB}\)_, the iterates satisfy_ \(\left\lVert\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\right\rVert _{\infty}\leqslant 1\) _and_ \(\left\lVert\beta_{k}\right\rVert_{\infty}\leqslant B\) _for all_ \(k\)_;_
2. _For stepsizes satisfying_ \(\gamma_{k}\leqslant\frac{c}{LB}\)_,_ \((\beta_{k})_{k\geqslant 0}\) _converges almost surely to some_ \(\beta_{\infty}^{\star}\in\mathcal{S}\)_,_
3. _If_ \((\beta_{k})_{k}\) _and the neurons_ \((u_{k},v_{k})_{k}\) _respectively converge to a model_ \(\beta_{\infty}^{\star}\) _and neurons_ \((u_{\infty},v_{\infty})\) _satisfying_ \(\beta_{\infty}^{\star}\in\mathcal{S}\) _(and_ \(\beta_{\infty}^{\star}=u_{\infty}\odot v_{\infty}\)_), then for almost all stepsizes (with respect to the Lebesgue measure), the limit_ \(\beta_{\infty}^{\star}\) _satisfies:_ \[\beta_{\infty}^{\star}=\operatorname*{argmin}_{\beta^{\star}\in\mathcal{S}}D_ {\psi_{\boldsymbol{\alpha}_{\infty}}}(\beta^{\star},\tilde{\beta}_{0})\,,\] _for_ \(\boldsymbol{\alpha}_{\infty}\in\mathbb{R}_{>0}^{d}\) _and_ \(\tilde{\beta}_{0}\in\mathbb{R}^{d}\) _satisfying_ \[\boldsymbol{\alpha}_{\infty}^{2}=\boldsymbol{\alpha}^{2}\odot\exp\left(-\sum_ {k=0}^{\infty}q\big{(}\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k })\big{)}\right)\,,\] _where_ \(q(x)=-\frac{1}{2}\ln((1-x^{2})^{2})\geqslant 0\) _for_ \(|x|\leqslant\sqrt{2}\)_, and_ \(\tilde{\beta}_{0}\) _is a perturbation term equal to:_ \[\tilde{\beta}_{0}=\frac{1}{2}\big{(}\boldsymbol{\alpha}_{+}^{2}-\boldsymbol{ \alpha}_{-}^{2}\big{)},\] _where,_ \(q_{\pm}(x)=\mp 2x-\ln((1\mp x)^{2})\)_, and_ \(\boldsymbol{\alpha}_{\pm}^{2}=\boldsymbol{\alpha}^{2}\odot\exp\left(-\sum_{k=0} ^{\infty}q_{\pm}(\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}))\right)\)_._

Proof.: **Point 1.** The first point of the Theorem is a direct consequence of Corollary 1 and the bounds proved in appendix E.3.

**Point 2.** Then, for stepsizes \(\gamma_{k}\leqslant\frac{c}{LB}\), using Proposition 8 for any interpolator \(\beta^{\star}\in\mathcal{S}\):

\[D_{h_{k+1}}(\beta^{\star},\beta_{k+1})\leqslant D_{h_{k}}(\beta^{\star},\beta_{ k})-\gamma_{k}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\,.\] (24)

Hence, summing:

\[\sum_{k}\gamma_{k}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\leqslant D_{h_{0}} (\beta^{\star},\beta_{0})\,,\]

so that the series converges.

Under our stepsize rule, \(\left\|\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\right\|_{ \infty}\leqslant\frac{1}{2}\), leading to \(\left\|q(\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\right\|_{ \infty}\leqslant 3\left\|\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}) \right\|_{\infty}^{2}\) by Lemma 5. Using \(\left\|\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\right\|^{2}\leqslant 2 L_{2}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\), we have that \(\ln(\boldsymbol{\alpha}_{\pm,k})\), \(\ln(\boldsymbol{\alpha}_{k})\) all converge.

We now show that \(\sum_{k}\gamma_{k}\mathcal{L}(\beta_{k})<\infty\). We have:

\[\sum_{\ell<k}\mathcal{L}(\beta_{k})=\sum_{\ell<k}\gamma_{k}\mathcal{L}_{ \mathcal{B}_{k}}(\beta_{k})+M_{k}\,,\]

where \(M_{k}=\sum_{\ell<k}\gamma_{k}(\mathcal{L}(\beta_{k})-\mathcal{L}_{\mathcal{B} _{k}}(\beta_{k}))\). We have that \((M_{k})\) is a martingale with respect to the filtration \((\mathcal{F}_{k})\) defined as \(\mathcal{F}_{k}=\sigma(\beta_{\ell},\ell\leqslant k)\). Using our upper-bound on \(\sum_{\ell<k}\gamma_{k}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\), we have:

\[M_{k}\geqslant\sum_{\ell<k}\gamma_{k}\mathcal{L}(\beta_{k})-\sum_{\ell<k} \gamma_{k}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\geqslant-D_{h_{0}}(\beta^ {\star},\beta_{0})\,,\]

and hence \((M_{k})\) is a lower bounded martingale. Using Doob's first martingale convergence theorem (a lower bounded super-martingale converges almost surely, Doob [17]), \((M_{k})\) converges almost surely. Consequently, since \(\sum_{\ell<k}\gamma_{k}\mathcal{L}(\beta_{k})=\sum_{\ell<k}\gamma_{k} \mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})+M_{k}\), we have that \(\sum_{\ell<k}\gamma_{k}\mathcal{L}(\beta_{k})\) converges almost surely (the first term is upper bounded, the second converges almost surely).

We now prove the convergence of \((\beta_{k})\). Since it is a bounded sequence, let \(\beta_{\sigma(k)}\) be a convergent sub-sequence and let \(\beta^{\star}_{\infty}\) denote its limit: \(\beta_{\sigma(k)}\to\beta^{\star}_{\infty}\).

Almost surely, \(\sum_{k}\gamma_{k}\mathcal{L}(\beta_{k})<\infty\) and so \(\gamma_{k}\mathcal{L}(\beta_{k})\to 0\), leading to \(\mathcal{L}(\beta_{k})\to 0\) since the stepsizes are lower bounded, so that \(\mathcal{L}(\beta_{\sigma(k)})\to 0\), and hence \(\mathcal{L}(\beta^{\star}_{\infty})=0\): this means that \(\beta^{\star}_{\infty}\) is an interpolator.

Since the quantities \((\boldsymbol{\alpha}_{k})_{k}\), \((\boldsymbol{\alpha}_{\pm,k})_{k}\) and \((\phi_{k})_{k}\) converge almost surely to \(\boldsymbol{\alpha}_{\infty}\), \(\boldsymbol{\alpha}_{\pm}\) and \(\phi_{\infty}\), we get that the potentials \(h_{k}\) uniformly converge to \(h_{\infty}=\psi_{\boldsymbol{\alpha}_{\infty}}-\langle\phi_{\infty},\cdot\rangle\) on all compact sets. Now notice that we can decompose \(\nabla h_{\infty}(\beta^{\star}_{\infty})\) as:

\[\nabla h_{\infty}(\beta^{\star}_{\infty})=\big{(}\nabla h_{\infty}(\beta^{ \star}_{\infty})-\nabla h_{\infty}(\beta_{\sigma(k)})\big{)}+\big{(}\nabla h_ {\infty}(\beta_{\sigma(k)})-\nabla h_{\sigma(k)}(\beta_{\sigma(k)})\big{)}+ \nabla h_{\sigma(k)}(\beta_{\sigma(k)}).\]

The first two terms converge to \(0\): the first is a direct consequence of the convergence of the extracted subsequence, the second is a consequence of the uniform convergence of \(h_{\sigma(k)}\) to \(h_{\infty}\) on compact sets. Finally the last term is always in \(\mathrm{Span}(x_{1},\ldots,x_{n})\) due to Proposition 5, leading to \(\nabla h_{\infty}(\beta^{\star}_{\infty})\in\mathrm{Span}(x_{1},\ldots,x_{n})\). Consequently, \(\nabla h_{\infty}(\beta^{\star}_{\infty})\in\mathrm{Span}(x_{1},\ldots,x_{n})\). Notice that from the definition of \(h_{\infty}\), we have that \(\nabla h_{\infty}(\beta^{\star}_{\infty})=\nabla\psi_{\boldsymbol{\alpha}_{ \infty}}(\beta^{\star}_{\infty})-\phi_{\infty}\). Now since \(\phi_{\infty}=\frac{1}{2}\arcsinh(\frac{\boldsymbol{\alpha}_{\pm}^{2}- \alpha_{\infty}^{2}}{2\boldsymbol{\alpha}_{\infty}^{2}})\), one can notice that \(\tilde{\beta}_{0}\) is precisely defined such that \(\nabla\psi_{\alpha_{\infty}}(\tilde{\beta}_{0})=\phi_{\infty}\). Therefore \(\nabla\psi_{\boldsymbol{\alpha}_{\infty}}(\beta^{\star}_{\infty})-\nabla\psi_ {\boldsymbol{\alpha}_{\infty}}(\tilde{\beta}_{0})\in\mathrm{Span}(x_{1},\ldots, x_{n})\). This condition along with the fact that \(\beta^{\star}_{\infty}\) is an interpolator are exactly the optimality conditions of the convex minimisation problem:

\[\min_{\beta^{\star}\in\mathcal{S}}\,D_{\psi_{\boldsymbol{\alpha}_{\infty}}}( \beta^{\star},\tilde{\beta}_{0})\]

Therefore \(\beta^{\star}_{\infty}\) must be equal to the unique minimiser of this problem. Since this is true for any sub-sequence we get that \(\beta_{k}\) converges almost surely to:

\[\beta^{\star}_{\infty}=\operatorname*{argmin}_{\tilde{\beta}\in\mathcal{S}}\,D_{ \psi_{\boldsymbol{\alpha}_{\infty}}}(\beta^{\star},\tilde{\beta}_{0}).\]

**Point 3.** From what we just proved, note that it is sufficient to prove that \(\boldsymbol{\alpha}_{k}\), \(\boldsymbol{\alpha}_{\pm,k},\phi_{k}\) converge to limits \(\boldsymbol{\alpha}_{\infty},\boldsymbol{\alpha}_{\pm,\infty},\phi_{\infty}\) satisfying \(\boldsymbol{\alpha}_{\infty},\boldsymbol{\alpha}_{\pm,\infty}\in\mathbb{R}^{d}_ {>0}\) (with positive and non-null coordinates) and \(\phi_{\infty}\in\mathbb{R}^{d}\). Indeed, if this holds and since we assume that the iterates converge to some interpolator, we proved just above that this interpolator is uniquely defined through the desired implicit regularization problem. We thus prove the convergence of \(\boldsymbol{\alpha}_{k},\boldsymbol{\alpha}_{\pm,k},\phi_{k}\).

Note that the convergence of \(u_{k},v_{k}\) is equivalent to the convergence of \(w_{\pm,k}\) in the \(w_{+}^{2}-w_{-}^{2}\) parameterisation used in our proofs, that we use there too. We have:

\[w_{\pm,k+1}=(1\mp\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{k})) \odot w_{\pm,k}\,,\]

so that

\[\ln(w_{\pm,k}^{2})=\sum_{\ell<k}\ln((1\mp\gamma_{\ell}\nabla\mathcal{L}_{ \mathcal{B}_{\ell}}(\beta_{\ell}))^{2})\,.\]

We now assume that stepsizes are such that for all \(\ell\geqslant 0\) and \(i\in[d]\), stepsizes are such that we have \(|\gamma_{\ell}\nabla_{i}\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})|\neq 1\): this is true for all stepsizes except a countable number of stepsizes, and so this is true for almost all stepsizes. Since we assume that the iterates \(\beta_{k}\) converge to some interpolator, this leads to \(\gamma_{\ell}\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\to 0\) if we assume that stepsizes do not diverge.

Taking the limit, we have

\[\ln(w_{\pm,\infty}^{2})=\sum_{\ell<\infty}\ln((1\mp\gamma_{\ell}\nabla \mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell}))^{2})\,.\]

This limit is in \((\{-\infty\}\cup\mathbb{R})^{d}\) (since \(w_{\pm,\infty}\in\mathbb{R}^{d}\)), and a coordinate of the limit is equal to \(-\infty\) if and only if the sum on the RHS diverges to \(-\infty\) (note that from our assumption just above, no term of the sum can be equal to \(-\infty\)).

We have \(\ln((1\mp\gamma_{\ell}\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell}))^{ 2})\sim\mp 2\gamma_{\ell}\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\) as \(\ell\to\infty\), so that if for some coordinate \(i\) we have \(\sum_{\ell}\gamma_{\ell}\nabla_{i}\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{ \ell})=\mp\infty\), then the coordinate \(i\) of the limit satisfies \(\ln(w_{i,\pm,\infty}^{2})=+\infty\), which is impossible. Hence, the sum \(\sum_{\ell}\gamma_{\ell}\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\) is in \(\mathbb{R}^{d}\) (and is thus converging); consequently, \(\sum_{\ell}\gamma_{\ell}^{2}\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{ \ell})^{2}\) converges and thus \(\sum_{\ell}q(\gamma_{\ell}\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell}))\) and \(\sum_{\ell}q_{\pm}(\gamma_{\ell}\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_ {\ell}))\) all converge: the sequences \(\boldsymbol{\alpha}_{k},\boldsymbol{\alpha}_{\pm,k}\) thus converge to limits in \(\mathbb{R}^{d}_{>0}\), and \(\phi_{k}\) converges, concluding our proof.

### Proof of Proposition 1

We begin with the following Lemma, that explicits the curvature of \(D_{h}\) around the set of interpolators.

**Lemma 3**.: _For all \(k\geqslant 0\), if \(\mathcal{L}(\beta_{k})\leqslant\frac{1}{2\lambda_{\max}}(\alpha^{2}\lambda_{ \min}^{+})^{2}\), we have \(\left\|\beta_{k}-\beta_{\alpha_{k}}^{\star}\right\|^{2}\leqslant 2B(\alpha^{2} \lambda_{\min}^{+})^{-1}\mathcal{L}(\beta_{k})\)._

Proof.: Recall that the sequence \(\mathbf{z}^{k}=\nabla h_{k}(\beta^{k})\) satisfies \(\mathbf{z}^{0}=0\) and \(\mathbf{z}^{k+1}=\mathbf{z}^{k}-\gamma_{k}\mathcal{L}(\beta^{k})\), so that we have that \(\mathbf{z}^{k}\in V=\mathrm{Im}(\mathbf{X}\mathbf{X}^{\top})\) for all \(k\geqslant 0\). Then, let \(\beta_{k}^{\alpha}\) be the unique minimizer of \(h_{k}\) over \(\mathcal{S}\) the space of interpolators: \(\beta_{k}^{\alpha}\) is exactly characterized by \(\mathbf{X}^{\top}\beta_{k}^{\alpha}=\mathbf{Y}\) and \(\nabla h_{k}(\beta_{k}^{\alpha})\in V\). We define \(\mathbf{z}_{k}^{\alpha}\in V\) as \(\mathbf{z}_{k}^{\alpha}=\nabla h_{k}(\beta_{k}^{\alpha})\).

Now, fix \(\mathbf{z}^{\alpha}=\mathbf{z}_{k}^{\alpha}\) and \(h=h_{k}\), and let us define \(\psi:\mathbf{z}\in V\to D_{h^{*}}(\mathbf{z},\mathbf{z}^{\alpha})\) and \(\phi:\mathbf{z}\in V\to\mathcal{L}(\nabla h^{*}(\mathbf{z}))\). We next show that for all \(\mathbf{z}\in V\), there exists \(\mu_{z}\) such that \(\nabla^{2}\phi(\mathbf{z})\geqslant\mu_{z}\nabla^{2}\psi(\mathbf{z})\), and that \(\mu_{z}\geqslant\mu\) for \(\mathbf{z}\) in an open convex set of \(V\) around \(\mathbf{z}^{\alpha}\), for some \(\mu>0\). For \(A\in\mathbb{R}^{d\times d}\) an operator/matrix on \(\mathbb{R}^{d}\), let us denote \(A_{V}\) its restriction/co-restriction to \(V\).

First, for \(\mathbf{z}\in V\), we have \(\nabla^{2}\psi(\mathbf{z})=\nabla^{2}(h^{*}(\mathbf{z})-h^{*}(\mathbf{z})- \langle\nabla h^{*}(\mathbf{z}^{\alpha}),z-z^{\alpha}\rangle)(\mathbf{z})= \nabla^{2}h^{*}(\mathbf{z})_{V}\). Then, \(\nabla\phi(\mathbf{z})=\nabla^{2}h^{*}(\mathbf{z})\nabla\mathcal{L}(\nabla h^{* }(\mathbf{z}))\), so that \(\nabla^{2}\phi(\mathbf{z})=\left(\nabla^{2}h^{*}(\mathbf{z})\nabla^{2} \mathcal{L}(\nabla h^{*}(\mathbf{z}))\nabla^{2}h^{*}(\mathbf{z})\right)_{V}+ \nabla^{3}h^{*}(\mathbf{z})(\nabla\mathcal{L}(\nabla h^{*}(\mathbf{z})),\cdot,\cdot)_{V}\).

Since \(h\) is \(1/(2\alpha^{2})\) smooth (on \(\mathbb{R}^{d}\) and thus on \(V\)), \(h^{*}\) is \(2\alpha^{2}\) strongly convex (on \(V\) and on \(\mathbb{R}^{d}\)). Using \(V=\mathrm{Im}(\mathbf{X}\mathbf{X}^{\top})\) and \(\nabla^{2}\mathcal{L}\equiv\mathbf{X}\mathbf{X}^{\top}\), we have \(\left(\nabla^{2}h^{*}(\mathbf{z})\nabla^{2}\mathcal{L}(\nabla h^{*}(\mathbf{z})) \nabla^{2}h^{*}(\mathbf{z})\right)_{V}=\nabla^{2}h^{*}(\mathbf{z})_{V}\nabla^{ 2}\mathcal{L}(\nabla h^{*}(\mathbf{z}))_{V}\nabla^{2}h^{*}(\mathbf{z})_{V}\), and thus \(\left(\nabla^{2}h^{*}(\mathbf{z})\nabla^{2}\mathcal{L}(\nabla h^{*}(\mathbf{z})) \nabla^{2}h^{*}(\mathbf{z})\right)_{V}\succeq 2\alpha^{2}\lambda_{\min}^{+}\nabla^{2}h^{*}( \nabla^{2})_{V}\).

For the other term of \(\nabla^{2}\phi\), namely \(\nabla^{3}h^{*}(\mathbf{z})(\nabla\mathcal{L}(\nabla h^{*}(\mathbf{z})),\cdot, \cdot)_{V}\), we compute \(\nabla^{3}_{ijk}h^{*}(\mathbf{z})=\mathbf{1}_{i=j=k}2\alpha^{2}_{i,k}\sinh( \mathbf{z}_{i})\), leading to: \(\nabla^{3}h^{*}(\mathbf{z})(\nabla\mathcal{L}(\nabla h^{*}(\mathbf{z})),\cdot \cdot,\cdot)_{V}=\mathrm{diag}(2\alpha^{2}\sinh(\mathbf{z})\odot(\mathbf{X} \mathbf{X}^{\top}(2\alpha^{2}\sinh(\mathbf{z})-\beta^{\alpha})))_{V}\). Thus, writing \(\beta_{\mathbf{z}}=2\alpha^{2}_{i,k}\sinh(\mathbf{z})=\nabla h^{*}(\mathbf{z})\) the primal surrogate of 

[MISSING_PAGE_FAIL:30]

Proof of Proposition 1.: We apply Proposition 14 for \(\beta=\beta_{k}\), with \(L_{r}=4BL\) (using Lemma 6) and replacing \(\mathcal{L}\) by \(\mathcal{L}_{\mathcal{B}_{k}}\), to obtain:

\[\gamma_{k}(\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k+1})-\mathcal{L}_ {\mathcal{B}_{k}}(\beta_{k})) \leqslant-D_{h_{k+1}}(\beta_{k},\beta_{k+1})-(1-\gamma_{k}L_{r})D_ {h_{k}}(\beta_{k+1},\beta_{k})\] \[\quad+(h_{k+1}-h_{k})(\beta_{k})-(h_{k+1}-h_{k})(\beta_{k+1})\,,\]

and thus, taking the mean wrt \(\mathcal{B}_{k}\),

\[\gamma_{k}(\mathbb{E}_{\mathcal{B}_{k}}\mathcal{L}(\beta_{k+1})- \mathcal{L}(\beta_{k})) \leqslant-\mathbb{E}_{\mathcal{B}_{k}}D_{h_{k+1}}(\beta_{k}, \beta_{k+1})-(1-\gamma_{k}L_{r})\mathbb{E}_{\mathcal{B}_{k}}D_{h_{k}}(\beta_{k +1},\beta_{k})\] \[\quad+\mathbb{E}_{\mathcal{B}_{k}}(h_{k+1}-h_{k})(\beta_{k})- \mathbb{E}_{\mathcal{B}_{k}}(h_{k+1}-h_{k})(\beta_{k+1})\] \[\leqslant-(1-\gamma_{k}L_{r})\mathbb{E}_{\mathcal{B}_{k}}D_{h_{k }}(\beta_{k+1},\beta_{k})\] \[\quad+\mathbb{E}_{\mathcal{B}_{k}}(h_{k+1}-h_{k})(\beta_{k})- \mathbb{E}_{\mathcal{B}_{k}}(h_{k+1}-h_{k})(\beta_{k+1})\,.\]

First, as in the proof of Proposition 10, using the fact that \(h_{k}\) is \(\ln(1/\alpha_{k})\) smooth,

\[D_{h_{k}}(\beta_{k+1},\beta_{k}) \geqslant\frac{1}{2\ln(1/\alpha_{k})}\|\nabla h_{k}(\beta_{k})- \gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})-\nabla h_{k}(\beta_{k })+\nabla h_{k+1}(\beta_{k+1})-\nabla h_{k}(\beta_{k+1})\|_{2}^{2}\] \[\geqslant-\frac{1}{2\ln(1/\alpha_{k})}\|\nabla h_{k}(\beta_{k})- \nabla h_{k+1}(\beta_{k})\|_{2}^{2}+\frac{1}{4\ln(1/\alpha_{k})}\|\gamma_{k} \nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\|_{2}^{2}\,,\]

and thus

\[\mathbb{E}D_{h_{k}}(\beta_{k+1},\beta_{k})\geqslant\mathbb{E}\left[-\frac{1}{ 2\ln(1/\alpha_{k})}\|\nabla h_{k}(\beta_{k})-\nabla h_{k+1}(\beta_{k})\|_{2}^ {2}+\frac{\lambda_{b}}{2\ln(1/\alpha_{k})}\gamma_{k}^{2}\mathcal{L}_{\mathcal{ B}}(\beta_{k})\right]\,.\]

Now, we apply Lemma 7 assuming that \(\|\beta^{\star}\|_{\infty},\|\beta_{k+1}\|_{\infty}\leqslant B\) (which is satisfied since we are under the assumption of Theorem 2):

\[(h_{k+1}-h_{k})(\beta_{k})-(h_{k+1}-h_{k})(\beta^{\star})\leqslant 24BL \gamma_{k}^{2}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\,.\]

Using \(|\nabla h_{k}(\beta)-\nabla h_{k+1}(\beta)|\leqslant 2\delta_{k}\) where \(\delta_{k}=q(\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}))\) as in Proposition 10, we have:

\[\mathbb{E}\|\nabla h_{k}(\beta_{k})-\nabla h_{k+1}(\beta_{k})\|_{2}^{2} \leqslant 16B\gamma_{k}^{2}\mathbb{E}\|\nabla\mathcal{L}_{\mathcal{B}_{k}} (\beta_{k})\|^{2}\leqslant 32BL\gamma_{k}^{2}\mathbb{E}\mathcal{L}(\beta_{k})\,.\]

Wrapping everything together,

\[\mathbb{E}\left[\mathcal{L}(\beta_{k+1})-\mathcal{L}(\beta_{k})\right] \leqslant-(1-\gamma_{k}4BL)\frac{\lambda_{b}}{2\ln(1/\alpha_{k})} \gamma_{k}\mathbb{E}\mathcal{L}(\beta_{k})\] \[\quad+\big{(}\gamma_{k}^{2}(1-4\gamma_{k}BL)24BL+\frac{32BL}{\ln( 1/\alpha_{k})}\big{)}\gamma_{k}^{2}\mathbb{E}\mathcal{L}(\beta_{k})\,.\]

Thus, for \(\gamma_{k}\leqslant\frac{c^{\prime}}{LB\ln(1/(\min_{i}\alpha_{k,i}))}\), we have the first part of Proposition 1.

Using Lemma 3, we then have:

\[\mathbb{E}\left[\big{\|}\beta_{k}-\beta_{\alpha_{k}}^{\star}\big{\|}^{2}\right] =\mathbb{E}\left[\mathbf{1}_{\{\mathcal{L}(\beta_{k})\leqslant \frac{1}{2\max}(\alpha^{2}\lambda_{\min}^{+})^{2}\}}\big{\|}\beta_{k}-\beta_{ \alpha_{k}}^{\star}\big{\|}^{2}\right]\] \[\leqslant\mathbb{E}\left[\mathbf{1}_{\{\mathcal{L}(\beta_{k}) \leqslant\frac{1}{2\max}(\alpha^{2}\lambda_{\min}^{+})^{2}\}}2B(\alpha^{2} \lambda_{\min}^{+})^{-1}\mathcal{L}(\beta_{k})\right]\] \[\quad+\mathbb{P}\left(\mathcal{L}(\beta_{k})>\frac{1}{2\lambda_{ \max}}(\alpha^{2}\lambda_{\min}^{+})^{2}\right)\times 4B^{2}\] \[\leqslant 2B(\alpha^{2}\lambda_{\min}^{+})^{-1}\mathbb{E}\left[ \mathcal{L}(\beta_{k})\right]\] \[\quad+\frac{\mathbb{E}\left[\mathcal{L}(\beta_{k})\right]}{\frac{ 2\lambda_{\max}}{(\alpha^{2}\lambda_{\min}^{+})^{2}}}\times 4B^{2}\] \[=2B(\alpha^{2}\lambda_{\min}^{+})^{-1}\Big{(}1+\frac{4B\lambda_{ \max}}{\alpha^{2}\lambda_{\min}^{+}}\Big{)}\mathbb{E}\left[\mathcal{L}(\beta_{ k})\right]\,.\]Proof of miscellaneous results mentioned in the main text

In this section, we provide proofs for results mentioned in the main text and that are not directly directed to the proof of Theorem 3.

### Proof of Proposition 3 and the sum of the losses

We start by proving the following proposition, present as is in the first 9 pages of this paper. We then continue with upper and lower bounds (of similar magnitude) on the sum of the losses.

**Proposition 3**.: _Let \(\Lambda_{b},\lambda_{b}>0\)5 be the largest and smallest values, respectively, such that \(\lambda_{b}H\preceq\mathbb{E}_{\mathcal{B}}\left[H_{\mathcal{B}}^{2}\right] \preceq\Lambda_{b}H\). For any stepsize \(\gamma>0\) satisfying \(\gamma\leqslant\frac{c}{BL}\) (as in Theorem 2), initialisation \(\alpha\mathbf{1}\) and batch size \(b\in[n]\), the magnitude of the gain satisfies:_

Footnote 5: \(\Lambda_{b},\lambda_{b}>0\) are data-dependent constants; for \(b=n\), we have \((\lambda_{n},\Lambda_{n})=(\lambda_{\min}^{+}(H),\lambda_{\max}(H))\) where \(\lambda_{\min}^{+}(H)\) is the smallest non-null eigenvalue of \(H\); for \(b=1\), we have \(\min_{i}\|x_{i}\|_{2}^{2}\leqslant\lambda_{1}\leqslant\Lambda_{1}\leqslant \max_{i}\|x_{i}\|_{2}^{2}\).

\[\lambda_{b}\gamma^{2}\sum_{k}\mathbb{E}\mathcal{L}(\beta_{k})\leqslant\mathbb{ E}\left[\|\mathrm{Gain}_{\gamma}\|_{1}\right]\leqslant 2\Lambda_{b}\gamma^{2} \sum_{k}\mathbb{E}\mathcal{L}(\beta_{k})\,,\] (10)

_where the expectation is over a uniform and independent sampling of the batches \((\mathcal{B}_{k})_{k\geqslant 0}\)._

Proof.: From Lemma 5, for all \(-1/2\leqslant x\leqslant 1/2\), it holds that \(x^{2}\leqslant q(x)\leqslant 2x^{2}\). We have, using \(\left\|\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\right\|_{\infty} \leqslant 1/2\) (which holds under the stepsize assumption):

\[\mathbb{E}\|\mathrm{Gain}_{\gamma}\|_{1} =-\mathbb{E}\sum_{i}\ln\Big{(}\frac{\alpha_{\infty,i}}{\alpha} \Big{)}\] \[=\sum_{\ell<\infty}\sum_{i}\mathbb{E}q\big{(}\gamma_{\ell}\nabla_{ i}\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\big{)}\] \[\leqslant 2\sum_{\ell<\infty}\sum_{i}\mathbb{E}\big{(}\gamma_{\ell} \nabla_{i}\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\big{)}^{2}\] \[=\sum_{\ell<\infty}\gamma_{\ell}^{2}\mathbb{E}\|\nabla\mathcal{L }_{\mathcal{B}_{\ell}}(\beta_{\ell})\|_{2}^{2}\] \[\leqslant 4\Lambda_{b}\sum_{\ell<\infty}\gamma_{\ell}^{2}\mathbb{E} \mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\,,\]

since \(\mathbb{E}\|\nabla\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\|_{2}^{2} \leqslant 2\Lambda_{b}\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})\). For the left handside we use \(q(x)\geqslant x^{2}\) for \(|x|\leqslant 1/2\) and \(\mathbb{E}\|\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{\ell})\|_{2}^{2} \geqslant 2\lambda_{b}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{\ell})\). Finally, since \(\mathcal{B}_{\ell}\) independent from \(\beta_{\ell}\), we have \(\mathbb{E}\mathcal{L}_{\mathcal{B}_{\ell}}(\beta_{\ell})=\mathbb{E}\mathcal{L }(\beta_{\ell})\). 

**Proposition 15**.: _For stepsizes \(\gamma_{k}\equiv\gamma\leqslant\frac{c}{LB}\) (as in Theorem 2), we have:_

\[\sum_{k\geqslant 0}\gamma^{2}\mathbb{E}\mathcal{L}(\beta_{k})=\Theta\left( \gamma\|\beta^{\star}\|_{1}\ln(1/\alpha)\right)\,.\]

Proof.: We first lower bound \(\sum_{k<\infty}\gamma_{k}^{2}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\). We have the following equality, that holds for any \(k\):

\[D_{h_{k+1}}(\beta^{\star},\beta_{k+1}) =D_{h_{k}}(\beta^{\star},\beta_{k})-2\gamma\mathcal{L}_{\mathcal{ B}_{k}}(\beta_{k})+D_{h_{k+1}}(\beta_{k},\beta_{k+1})\] \[\quad+\big{(}h_{k}-h_{k+1})(\beta_{k})-\big{(}h_{k}-h_{k+1}\big{)} (\beta^{\star})\,,\]

leading to, by summing for \(k\in\mathbb{N}\):

\[\sum_{k<\infty}2\gamma\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})=D_{h_{0}}(\beta ^{\star},\beta_{0})-\lim_{k\to\infty}D_{h_{k}}(\beta^{\star},\beta_{k})+\sum_ {k<\infty}D_{h_{k+1}}(\beta_{k},\beta_{k+1})+\sum_{k<\infty}\big{(}h_{k}-h_{k+1 }\big{)}(\beta_{k})-\big{(}h_{k}-h_{k+1}\big{)}(\beta^{\star})\,.\]

First, since \(h_{k}\to h_{\infty}\), \(\beta_{k}\to\beta_{\infty}\), we have \(\lim_{k\to\infty}D_{h_{k}}(\beta^{\star},\beta_{k})=0\). Then, \(D_{h_{k+1}}(\beta_{k},\beta_{k+1})\geqslant 0\).

Finally, \(|\big{(}h_{k}-h_{k+1}\big{)}(\beta_{k})-\big{(}h_{k}-h_{k+1}\big{)}(\beta^{ \star})|\leqslant 16BL_{2}\gamma^{2}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\). Hence :

\[\sum_{k<\infty}2\gamma(1+16\gamma BL_{2})\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k} )\geqslant D_{h_{0}}(\beta^{\star},\beta_{0})\,,\]and thus \(\sum_{k<\infty}\gamma\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\geqslant D_{h_{0}}( \beta^{\star},\beta_{0})/4\) for \(\gamma\leqslant c/(BL)\) (with \(c\geqslant 16\)). This gives the RHS inequality. The LHS is a direct consequence of bounds proved in previous subsections.

Hence, we have that

\[\gamma^{2}\sum_{k}\mathcal{L}(\beta_{k})=\Theta\left(\gamma D_{h_{0}}(\beta^{ \star},\beta_{0})\right)\,.\]

Noting that \(D_{h_{0}}(\beta^{\star},\beta_{0})=h_{0}(\beta^{\star})=\Theta\big{(}\ln(1/ \alpha)\|\beta^{\star}\|_{1}\big{)}\) concludes the proof. 

### \(\tilde{\beta}_{0}\) is negligible

In the following proposition we show that \(\tilde{\beta}_{0}\) is close to \(\mathbf{0}\) and therefore one should think of the implicit regularization problem as \(\beta^{\star}_{\infty}=\operatorname*{argmin}_{\beta^{\star}\in S}\psi_{ \alpha_{\infty}}(\beta^{\star})\)

**Proposition 16**.: _Under the assumptions of Theorem 2,_

\[|\tilde{\beta}_{0}|\leqslant\alpha^{2},\]

_where the inequality must be understood coordinate-wise._

Proof.: \[|\tilde{\beta}_{0}| =\frac{1}{2}|\alpha_{+}^{2}-\alpha_{-}^{2}|\] \[=\frac{1}{2}\alpha^{2}\big{|}\exp(-\sum_{k}q_{+}(\gamma_{k} \nabla\mathcal{L}(\beta_{k}))-\exp(-\sum_{k}q_{-}(\gamma_{k}\nabla\mathcal{L }(\beta_{k}))\big{|}\] \[\leqslant\alpha^{2},\]

where the inequality is because \(q_{+}(\gamma_{k}\nabla\mathcal{L}(\beta_{k}))\geqslant 0\), \(q_{-}(\gamma_{k}\nabla\mathcal{L}(\beta_{k}))\geqslant 0\) for all \(k\).

### Impact of stochasticity and linear scaling rule

**Proposition 17**.: _With probability \(1-2ne^{-d/16}-3/n^{2}\) over the \(x_{i}\sim_{\mathrm{iid}}\mathcal{N}(0,\sigma^{2}I_{d})\), \(c_{1}\frac{d\sigma^{2}}{b}(1+o(1))\leqslant\lambda_{b}\leqslant\Lambda_{b} \leqslant c_{2}\frac{d\sigma^{2}}{b}(1+o(1))\),_

so that under these assumptions,

\[\sum_{k}\gamma_{k}\mathbb{E}\mathcal{L}(\beta_{k})=\Theta\left(\frac{\gamma} {b}\sigma^{2}\|\beta^{\star}\|_{1}\ln(1/\alpha)\right)\,.\]

Proof.: The bound on \(\lambda_{b},\Lambda_{b}\) is a direct consequence of the concentration bound provided in Lemma 13. 

### (Stochastic) gradients at the initialisation

To understand the behaviour and the effects of the stochasticity and the stepsize on the shape of \(\operatorname*{Gain}_{\gamma}\), we analyse a noiseless sparse recovery problem under the following standard assumption 2[10] and as common in the sparse recovery literature, we make the following assumption 3 on the inputs.

**Assumption 2**.: _There exists an \(s\)-sparse ground truth vector \(\beta^{\star}_{\mathrm{sparse}}\) where \(s\) verifies \(n=\Omega(s\ln(d))\), such that \(y_{i}=\langle\beta^{\star}_{\mathrm{sparse}},x_{i}\rangle\) for all \(i\in[n]\)._

**Assumption 3**.: _There exists \(\delta,c_{1},c_{2}>0\) such that for all \(s\)-sparse vectors \(\beta\), there exists \(\varepsilon\in\mathbb{R}^{d}\) such that \((X^{\top}X)\beta=\beta+\varepsilon\) where \(\|\varepsilon\|_{\infty}\leqslant\delta\|\beta\|_{2}\) and \(c_{1}\|\beta\|_{2}^{2}\mathbf{1}\leqslant\frac{1}{n}\sum_{i}x_{i}^{2}\langle x _{i},\beta\rangle^{2}\leqslant c_{2}\|\beta\|_{2}^{2}\mathbf{1}\)._

The first part of Assumption 3 closely resembles the classical restricted isometry property (RIP) and is relevant for GD while the second part is relevant for SGD. Such an assumption is not restrictive and holds with high probability for Gaussian inputs \(\mathcal{N}(0,\sigma^{2}I_{d})\) (see Lemma 10 in Appendix).

Based on the claim above, we analyse the shape of the (stochastic) gradient at initialisation. For GD and SGD, it respects writes, where \(g_{0}=\nabla\mathcal{L}_{i_{0}}(\beta_{0})^{2}\), \(i_{0}\sim\mathrm{Unif}([\mathrm{n}])\):

\[\nabla\mathcal{L}(\beta_{0})^{2}=[X^{\top}X\beta^{\star}]^{2}\,,\,\,\,\mathbb{ E}_{i_{0}}[g_{0}]=\frac{1}{n}\sum_{i}x_{i}^{2}\langle x_{i},\beta^{\star} \rangle^{2}.\]

The following lemma then shows that while the initial stochastic gradients of SGD are homogeneous, it is not the case for that of GD.

**Proposition 18**.: _Under Assumption 3, the squared full batch gradient and the expected stochastic gradient at initialisation satisfy, for some \(\varepsilon\) verifying \(\left\|\varepsilon\right\|_{\infty}<\!<\left\|\beta_{\mathrm{sparse}}^{\star} \right\|_{\infty}^{2}\):_

\[\nabla\mathcal{L}(\beta_{0})^{2}=(\beta_{\mathrm{sparse}}^{\star })^{2}+\varepsilon\,,\] (25) \[\mathbb{E}_{i_{0}}[\nabla\mathcal{L}_{i_{0}}(\beta_{0})^{2}]= \Theta\Big{(}\|\beta^{\star}\|_{2}^{2}\mathbf{1}\Big{)}\,.\] (26)

Proof of Proposition 18.: Under Assumption 3, we have using:

\[\nabla\mathcal{L}(\beta_{0})^{2} =(X^{\top}X\beta_{\mathrm{sparse}}^{\star})\] \[=(\beta_{\mathrm{sparse}}^{\star}+\varepsilon)^{2}\] \[=\beta_{\mathrm{sparse}}^{\star}{}^{2}+\varepsilon^{2}+2 \varepsilon\beta_{\mathrm{sparse}}^{\star}\,.\]

We have \(\left\|\varepsilon^{2}+2\varepsilon\beta_{\mathrm{sparse}}^{\star}\right\|_ {\infty}\leqslant\left\|\varepsilon\right\|_{\infty}^{2}+2\|\varepsilon\|_{ \infty}\big{\|}\beta_{\mathrm{sparse}}^{\star}\big{\|}_{\infty}\), and we conclude by using \(\left\|\varepsilon\right\|_{\infty}\leqslant\delta\big{\|}\beta_{\mathrm{ sparse}}^{\star}\big{\|}_{2}\).

Then,

\[\mathbb{E}_{i\sim\mathrm{Unif}([\mathrm{n}])}[\nabla\mathcal{L}_{i}(\beta_{0}) ^{2}]=\frac{1}{n}x_{i}^{2}\langle x_{i},\beta_{\mathrm{sparse}}^{\star} \rangle\,,\]

and we conclude using Assumption 3.

Proof of Proposition 4.: The proof proceeds as that of Proposition 18. 

### Convergence of \(\alpha_{\infty}\) and \(\tilde{\beta}_{0}\) for \(\gamma\to 0\)

**Proposition 19**.: _Let \(\tilde{\beta}_{0}(\gamma),\alpha_{\infty}(\gamma)\) be as defined in Theorem 1, for constant stepsizes \(\gamma_{k}\equiv\gamma\). We have:_

\[\tilde{\beta}_{0}(\gamma)\to 0\,,\quad\bm{\alpha}_{\infty}\to\alpha\mathbf{1}\,,\]

_when \(\gamma\to 0\)._

Proof.: We have, as proved previoulsy, that

\[\left\|\sum_{k}\gamma^{2}\nabla\mathcal{L}_{\mathcal{B}_{k}}( \beta_{k})^{2}\right\|_{1} \leqslant\sum_{k}\gamma^{2}\big{\|}\nabla\mathcal{L}_{\mathcal{B}_ {k}}(\beta_{k})^{2}\big{\|}_{1}\] \[=\sum_{k}\gamma^{2}\|\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k })\|_{2}^{2}\] \[\leqslant 2L\gamma^{2}\sum_{k}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k })\] \[\leqslant 2L\gamma D_{h_{0}}(\beta^{\star},\beta_{0})\,,\]

for \(\gamma\leqslant\frac{c}{B\ell}\). Thus, \(\sum_{k}\gamma^{2}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})^{2}\to 0\) as \(\gamma\to 0\) (note that \(\beta_{k}\) implicitly depends on \(\gamma\), so that this result is not immediate).

Then, for \(\gamma\leqslant\frac{c}{LB}\),

\[\left\|\ln(\bm{\alpha}_{\infty}^{2}/\alpha^{2})\right\|_{1}\leqslant\sum_{k} \left\|q(\gamma\mathcal{L}(\beta_{k})\right\|_{1}\leqslant 2\sum_{k}\gamma^{2} \big{\|}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})^{2}\big{\|}_{1}\,,\]which tends to \(0\) as \(\gamma\to 0\). Similarly, \(\left\|\ln(\bm{\alpha}_{+,\infty}^{2}/\alpha^{2})\right\|_{1}\to 0\) and \(\left\|\ln(\bm{\alpha}_{-,\infty}^{2}/\alpha^{2})\right\|_{1}\to 0\) as \(\gamma\to 0\), leading to \(\tilde{\beta}_{0}(\gamma)\to 0\) as \(\gamma\to 0\).

## Appendix H Technical lemmas

In this section we present a few technical lemmas, used and referred to throughout the proof of 1.

**Lemma 4**.: _Let \(\alpha_{+},\alpha_{-}>0\) and \(x\in\mathbb{R}\), and \(\beta=\alpha_{+}^{2}e^{x}-\alpha_{-}^{2}e^{-x}\). We have:_

\[\operatorname{arcsinh}\left(\frac{\beta}{2\alpha_{+}\alpha_{-}}\right)=x+\ln \left(\frac{\alpha_{+}}{\alpha_{-}}\right)=x+\operatorname{arcsinh}\left( \frac{\alpha_{+}^{2}-\alpha_{-}^{2}}{2\alpha_{+}\alpha_{-}}\right).\]

Proof.: First,

\[\frac{\beta}{2\alpha_{+}\alpha_{-}} =\frac{1}{2}\left(\frac{\alpha_{+}}{\alpha^{-}}e^{x}-\left( \frac{\alpha_{+}}{\alpha^{-}}\right)^{-1}e^{-x}\right)\] \[=\frac{e^{x+\ln(\alpha_{+}/\alpha_{-})}-e^{-x-\ln(\alpha_{+}/ \alpha_{-})}}{2}\] \[=\sinh(x+\ln(\alpha_{+}/\alpha_{-}))\,,\]

hence the result by taking the \(\operatorname{arcsinh}\) of both sides. Note also that we have \(\ln(\alpha_{+}/\alpha_{-})=\operatorname{arcsinh}(\frac{\alpha_{+}^{2}- \alpha_{-}^{2}}{2\alpha_{+}\alpha_{-}})\). 

**Lemma 5**.: _If \(|x|\leqslant 1/2\) then \(x^{2}\leqslant q(x)\leqslant 2x^{2}\)_

**Lemma 6**.: _On the \(\ell_{\infty}\) ball of radius \(B\), the quadratic loss function \(\beta\mapsto\mathcal{L}(\beta)\) is \(4\lambda_{\max}\max(B,\alpha^{2})\)-relatively smooth w.r.t all the \(h_{k}\)'s._

Proof.: We have:

\[\nabla^{2}h_{k}(\beta)=\operatorname{diag}\left(\frac{1}{2\sqrt{\alpha_{k}^{ 4}+\beta^{2}}}\right)\succeq\operatorname{diag}\left(\frac{1}{2\sqrt{\alpha ^{4}+\beta^{2}}}\right),\]

since \(\alpha_{k}\leqslant\alpha\) component-wise. Thus, \(\nabla^{2}h_{k}(\beta)\succeq\frac{1}{2}\min\big{(}\min_{1\leqslant i\leqslant d }\frac{1}{2|\beta_{i}|},\frac{1}{2\alpha^{2}}\big{)}I_{d}=\frac{1}{\max(4\| \beta\|_{\infty},4\alpha^{2})}I_{d}\), and \(h_{k}\) is \(\frac{1}{\max(4B,4\alpha^{2})}\)-strongly convex on the \(\ell^{\infty}\) norm of radius \(B\). Since \(\mathcal{L}\) is \(\lambda_{\max}\)-smooth over \(\mathbb{R}^{d}\), we have our result. 

**Lemma 7**.: _For \(k\geqslant 0\) and for all \(\beta\in\mathbb{R}^{d}\):_

\[|h_{k+1}(\beta)-h_{k}(\beta)|\leqslant 8L_{2}\gamma_{k}^{2}\mathcal{L}_{\mathcal{B} _{k}}(\beta_{k})\|\beta\|_{\infty}.\]

Proof.: We have \(\alpha_{+,k+1}^{2}=\alpha_{+,k}^{2}e^{-\delta_{+,k}}\) and \(\alpha_{-,k+1}^{2}=\alpha_{-,k}^{2}e^{-\delta_{-,k}}\), for \(\delta_{+,k}=\tilde{q}(\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k }))\) and \(\delta_{-,k}=\tilde{q}(-\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k }))\). And \(\alpha_{k+1}=\alpha_{k}\exp(-\delta_{k})\) where \(\delta_{k}\coloneqq\delta_{+,k}+\delta_{-,k}=q(\gamma_{k}\nabla\mathcal{L}_{ \mathcal{B}_{k}}(\beta_{k}))\).

To prove the result we will use that for \(\beta\in\mathbb{R}^{d}\), we have \(|(h_{k+1}-h_{k})(\beta)|\leqslant\sum_{i=1}^{d}\int_{0}^{|\beta_{i}|}|\nabla_{i} h_{k+1}(x)-\nabla_{i}h_{k}(x)|\mathrm{d}x\).

First, using that\(|\operatorname{arcsinh}(a)-\operatorname{arcsinh}(b)|\leqslant|\ln(a/b)|\) for \(ab>0\). We have that

\[\Big{|}\operatorname{arcsinh}\left(\frac{x}{\alpha_{k+1}^{2}} \right)-\operatorname{arcsinh}\left(\frac{x}{\alpha_{k}^{2}}\right)\Big{|} \leqslant\ln\left(\frac{\alpha_{k}^{2}}{\alpha_{k+1}^{2}}\right)\] \[=\delta_{k}\,,\]

since \(\delta_{k}\geqslant 0\) due to our stepsize condition.

We now prove that \(|\phi_{k+1}-\phi_{k}|\leqslant\frac{|\delta_{+,k}-\delta_{-,k}|}{2}\). We have \(\phi_{k}=\operatorname{arcsinh}\left(\frac{\alpha_{+,k}^{2}-\alpha_{-,k}^{2}}{2 \alpha_{+,k}\alpha_{-,k}}\right)\) and hence,

\[|\phi_{k+1}-\phi_{k}|=\left|\operatorname{arcsinh}\left(\frac{\alpha_{+,k}^{2}- \alpha_{-,k}^{2}}{2\alpha_{+,k}\alpha_{-,k}}\right)-\operatorname{arcsinh} \left(\frac{\alpha_{+,k+1}^{2}-\alpha_{-,k+1}^{2}}{2\alpha_{+,k+1}\alpha_{-,k +1}}\right)\right|\,.\]Then, assuming that \(\alpha_{+,k,i}\geqslant\alpha_{-,k,i}\), we have:

\[\frac{\alpha_{+,k+1,i}^{2}-\alpha_{-,k+1,i}^{2}}{2\alpha_{+,k+1,i} \alpha_{-,k+1,i}} =e^{\delta_{k,i}/2}\frac{\alpha_{+,k,i}^{2}e^{-\delta_{+,k,i}}- \alpha_{-,k,i}^{2}e^{-\delta_{-,k,i}}}{2\alpha_{+,k,i}\alpha_{-,k,i}}\] \[\left\{\begin{aligned} &\leqslant\begin{cases}e^{\frac{\delta_{+,k,i}- \delta_{-,k,i}}{2}}\frac{\alpha_{+,k,i}^{2}-\alpha_{-,k,i}^{2}}{2\alpha_{+,k,i} \alpha_{-,k,i}}&\text{if}\quad\delta_{+,k,i}\geqslant\delta_{-,k,i}\\ & e^{\frac{\delta_{-,k,i}-\delta_{-,k,i}}{2}}\frac{\alpha_{+,k,i}^{2}- \alpha_{-,k,i}^{2}}{\alpha_{+,k,i}\alpha_{-,k,i}}&\text{if}\quad\delta_{ -,k,i}\geqslant\delta_{+,k,i}\\ &\geqslant\begin{cases}e^{-\frac{\delta_{+,k,i}-\delta_{-,k,i}}{2}} \frac{\alpha_{+,k,i}^{2}-\alpha_{-,k,i}^{2}}{2\alpha_{+,k,i}\alpha_{-,k,i}}& \text{if}\quad\delta_{+,k,i}\geqslant\delta_{-,k,i}\\ & e^{-\frac{\delta_{-,k,i}-\delta_{+,k,i}}{2}}\frac{\alpha_{+,k,i}^{2}- \alpha_{-,k,i}^{2}}{2\alpha_{+,k,i}\alpha_{-,k,i}}&\text{if}\quad\delta_{-,k,i}\geqslant\delta_{+,k,i}\end{cases}\end{aligned}\right..\]

We thus have \(\frac{\alpha_{+,k+1,i}^{2}-\alpha_{-,k+1,i}^{2}}{2\alpha_{+,k+1,i}\alpha_{-,k +1,i}}\in\left[e^{-\frac{\left|\delta_{+,k,i}-\delta_{-,k,i}\right|}{2}},e^{ \frac{\left|\delta_{+,k,i}-\delta_{-,k,i}\right|}{2}}\right]\times\frac{\alpha_ {+,k,i}^{2}-\alpha_{-,k,i}^{2}}{2\alpha_{+,k,i}\alpha_{-,k,i}}\), and this holds similarly if \(\alpha_{+,k,i}\leqslant\alpha_{-,k,i}\). Then, using \(|\operatorname{arcsinh}(a)-\operatorname{arcsinh}(b)|\leqslant|\ln(a/b)|\) we obtain that:

\[|\phi_{k+1}-\phi_{k}| =\left|\operatorname{arcsinh}\left(\frac{\alpha_{+,k}^{2}- \alpha_{-,k}^{2}}{2\alpha_{+,k}\alpha_{-,k}}\right)-\operatorname{arcsinh} \left(\frac{\alpha_{+,k+1}^{2}-\alpha_{-,k+1}^{2}}{2\alpha_{+,k+1}\alpha_{-,k +1}}\right)\right|\] \[\leqslant\frac{\left|\delta_{+,k}-\delta_{-,k}\right|}{2}\,.\]

Wrapping things up, we have:

\[|\nabla h_{k}(\beta)-\nabla h_{k+1}(\beta)|\leqslant\delta_{k}+\frac{\left| \delta_{+,k}-\delta_{-,k}\right|}{2}\leqslant 2\delta_{k}\,,\]

This leads to the following bound:

\[|h_{k+1}(\beta)-h_{k}(\beta)| \leqslant\langle|2\delta_{k}|,|\beta|\rangle\] \[\leqslant 2\|\delta_{k}\|_{1}\|\beta\|_{\infty}.\]

Recall that \(\delta_{k}=q(\gamma_{k}\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\), hence from Lemma 5 if \(\gamma_{k}\|\nabla\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k})\|_{\infty}\leqslant 1/2\), we get that

\[\|\delta_{k}\|_{1}\leqslant 2\gamma_{k}^{2}\|\nabla\mathcal{L}_{\mathcal{B}_{k}} (\beta_{k})\|_{2}^{2}\leqslant 4L_{2}\gamma_{k}^{2}\mathcal{L}_{\mathcal{B}_{k}} (\beta_{k}).\]

Putting things together we obtain that

\[|h_{k+1}(\beta)-h_{k}(\beta)| \leqslant\langle|2\delta_{k}|,|\beta|\rangle\] \[\leqslant 8L_{2}\gamma_{k}^{2}\mathcal{L}_{\mathcal{B}_{k}}(\beta_{k}) \|\beta\|_{\infty}.\]

## Appendix I Concentration inequalities for matrices

In this last section of the appendix, we provide and prove several concentration bounds for random vectors and matrices, with (possibly uncentered) isotropic gaussian inputs. These inequalities can easily be generalized to subgaussian random variables via more refined concentration bounds, and to non-isotropic subgaussian random variables [19], leading to a dependence on an effective dimension and on the subgaussian matrix \(\Sigma\). We present these lemmas before proving them in a row.

The next two lemmas closely resemble the RIP assumption, for centered and then for uncentered gaussians.

**Lemma 8**.: _Let \(x_{1},\dots,x_{n}\in\mathbb{R}^{d}\) be i.i.d. random variables of law \(\mathcal{N}(0,I_{d})\) and \(H=\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{\top}\). Then, denoting by \(\mathcal{C}\) the set of all \(s\)-sparse vector \(\beta\in\mathbb{R}^{d}\) satisfying \(\left\|\beta\right\|_{2}\leqslant 1\), there exist \(C_{4},C_{5}>0\) such that for any \(\varepsilon>0\), if \(n\geqslant C_{4}s\ln(d)\varepsilon^{-2}\),_

\[\mathbb{P}\left(\sup_{\beta\in\mathcal{S}}\left\|H\beta-\beta\right\|_{\infty} \geqslant\varepsilon\right)\leqslant e^{-C_{5}n}\,.\]

**Lemma 9**.: _Let \(x_{1},\ldots,x_{n}\in\mathbb{R}^{d}\) be i.i.d. random variables of law \(\mathcal{N}(\mu,\sigma^{2}I_{d})\) and \(H=\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{\top}\). Then, denoting by \(\mathcal{C}\) the set of all \(s\)-sparse vector \(\beta\in\mathbb{R}^{d}\) satisfying \(\left\|\beta\right\|_{2}\leqslant 1\), there exist \(C_{4},C_{5}>0\) such that for any \(\varepsilon>0\), if \(n\geqslant C_{4}s\ln(d)\varepsilon^{-2}\),_

\[\mathbb{P}\left(\sup_{\beta\in\mathcal{S}}\left\|H\beta-\mu\langle\mu,\beta \rangle-\sigma^{2}\beta\right\|_{\infty}\geqslant\varepsilon\right) \leqslant e^{-C_{5}n}\,.\]

We then provide two lemmas that estimate the mean Hessian of SGD.

**Lemma 10**.: _Let \(x_{1},\ldots,x_{n}\) be i.i.d. random variables of law \(\mathcal{N}(0,I_{d})\). Then, there exist \(c_{1},c_{2}>0\) such that with probability \(1-\frac{1}{d^{2}}\) and if \(n=\Omega(s^{5/4}\ln(d))\), we have for all \(s\)-sparse vectors \(\beta\):_

\[c_{1}\|\beta\|_{2}^{2}\mathbf{1}\leqslant\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2} \langle x_{i},\beta\rangle^{2}\leqslant c_{2}\|\beta\|_{2}^{2}\mathbf{1}\,,\]

_where the inequality is meant component-wise._

**Lemma 11**.: _Let \(x_{1},\ldots,x_{n}\) be i.i.d. random variables of law \(\mathcal{N}(\mu,\sigma^{2}I_{d})\). Then, there exist \(c_{0},c_{1},c_{2}>0\) such that with probability \(1-\frac{c_{0}}{d^{2}}-\frac{1}{nd}\) and if \(n=\Omega(s^{5/4}\ln(d))\) and \(\mu\geqslant 4\sigma\sqrt{\ln(d)}\mathbf{1}\), we have for all \(s\)-sparse vectors \(\beta\):_

\[\frac{\mu^{2}}{2}\left(\langle\mu,\beta\rangle^{2}+\frac{1}{2}\sigma^{2}\| \beta\|_{2}^{2}\right)\leqslant\frac{1}{n}\sum_{i}x_{i}^{2}\langle x_{i}, \beta\rangle^{2}\leqslant 4\mu^{2}\left(\langle\mu,\beta\rangle^{2}+2 \sigma^{2}\|\beta\|_{2}^{2}\right)\,.\]

_where the inequality is meant component-wise._

Finally, next two lemmas are used to estimate \(\lambda_{b},\Lambda_{b}\) in our paper.

**Lemma 12**.: _Let \(x_{1},\ldots,x_{n}\in\mathbb{R}^{d}\) be i.i.d. random variables of law \(\mathcal{N}(\mu\mathbf{1},\sigma^{2}I_{d})\). Let \(H=\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{\top}\) and \(\tilde{H}=\frac{1}{n}\sum_{i=1}^{n}\|x_{i}\|^{2}x_{i}x_{i}^{\top}\). There exist numerical constants \(C_{2},C_{3}>0\) such that_

\[\mathbb{P}\Big{(}C_{2}\big{(}\mu^{2}+\sigma^{2}\big{)}dH\preceq\tilde{H} \preceq C_{3}\big{(}\mu^{2}+\sigma^{2}\big{)}dH\Big{)}\geqslant 1-2ne^{-d/16}\,.\]

**Lemma 13**.: _Let \(x_{1},\ldots,x_{n}\in\mathbb{R}^{d}\) be i.i.d. random variables of law \(\mathcal{N}(\mu\mathbf{1},\sigma^{2}I_{d})\) for some \(\mu\in\mathbb{R}\). Let \(H=\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{\top}\) and for \(1\leqslant b\leqslant n\) let \(\tilde{H}_{b}=\mathbb{E}_{\mathcal{B}}\left[\left(\frac{1}{b}\sum_{i\in \mathcal{B}}x_{i}x_{i}^{\top}\right)^{2}\right]\) where \(\mathcal{B}\subset[n]\) is sampled uniformly at random in \(\{\mathcal{B}\subset[n]\) s.t. \(|\mathcal{B}|=b\}\). With probability \(1-2ne^{-d/16}-3/n^{2}\), we have, for some numerical constants \(c_{1},c_{2},c_{3},C>0\):_

\[\left(c_{1}\frac{d(\mu^{2}+\sigma^{2})}{b}-c_{2}\frac{(\sigma^{2}+\mu^{2})\ln( n)}{\sqrt{d}}-c_{3}\frac{\mu^{2}d}{n}\right)H\preceq\tilde{H}_{b}\preceq C \left(\frac{d(\mu^{2}+\sigma^{2})}{b}+\frac{(\sigma^{2}+\mu^{2})\ln(n)}{\sqrt{ d}}+\mu^{2}d\right)\]

Proof of Lemma 8.: For \(j\in[d]\), we have:

\[(H\beta)_{j} =\frac{1}{n}\sum_{i=1}^{n}x_{ij}\langle x_{i},\beta\rangle\] \[=\frac{1}{n}\sum_{i=1}^{n}\sum_{j^{\prime}=1}^{d}x_{ij}x_{ij^{ \prime}}\beta_{j^{\prime}}\] \[=\frac{1}{n}\sum_{i=1}^{n}x_{ij}^{2}\beta_{j}+\frac{1}{n}\sum_{i =1}^{n}\sum_{j^{\prime}\neq j}x_{ij}x_{ij^{\prime}}\beta_{j^{\prime}}\] \[=\frac{\beta_{j}}{n}\sum_{i=1}^{n}x_{ij}^{2}+\frac{1}{n}\sum_{i =1}^{n}x_{ij}\sum_{j^{\prime}\neq j}x_{ij^{\prime}}\beta_{j^{\prime}}\,.\]

We thus notice that \(\mathbb{E}\left[H\beta\right]=\beta\), and

\[(H\beta)_{j}=\beta_{j}+\frac{\beta_{j}}{n}\sum_{i=1}^{n}(x_{ij}^{2}-1)+\frac{1 }{n}\sum_{i=1}^{n}z_{i}\,,\]where \(z_{i}=x_{ij}\sum_{j^{\prime}\neq j}x_{ij^{\prime}}\beta_{j^{\prime}}\), and \(\sum_{j^{\prime}\neq j}x_{ij^{\prime}}\beta_{j^{\prime}}\sim\mathcal{N}(0,\|\beta \|^{2}-\beta_{j}^{2})\) and \(\|\beta\|^{2}-\beta_{j}^{2}\leqslant 1\). Hence, \(z_{j}+x_{ij}^{2}-1\) is a centered subexponential random variables (with a subexponential parameter of order 1). Thus, for \(t\leqslant 1\):

\[\mathbb{P}\left(\left|\frac{\beta_{j}}{n}\sum_{i=1}^{n}(x_{ij}^{2}-1)+\frac{1 }{n}\sum_{i=1}^{n}z_{i}\right|\geqslant t\right)\leqslant 2e^{-cnt^{2}}\,.\]

Hence, using an \(\varepsilon\)-net of \(\mathcal{C}=\big{\{}\beta\in\mathbb{R}^{d}:\,\|\beta\|_{2}\leqslant 1\,,\| \beta\|_{0}\big{\}}\) (of cardinality less than \(d^{s}\times(C/\varepsilon)^{s}\), and for \(\varepsilon\) of order 1), we have, using the classical \(\varepsilon\)-net trick explained in [Chapt. 9, 58] or [App. C, Even and Massoulie [19]]:

\[\mathbb{P}\left(\sup_{\beta\in\mathcal{C},\,j\in[d]}|(H\beta)_{j}-\beta_{j}| \geqslant t\right)\leqslant d\times d^{s}(C/\varepsilon)^{s}\times 2e^{-cnt^{2}}= \exp\left(-c\ln(2)nt^{2}+(s+1)\ln(d)+s\ln(C/\varepsilon)\right)\,.\]

Consequently, for \(t=\varepsilon\) and if \(n\geqslant C_{4}s\ln(d)/\varepsilon^{2}\), we have:

\[\mathbb{P}\left(\sup_{\beta\in\mathcal{C},\,j\in[d]}|(H\beta)_{j}-\beta_{j}| \geqslant t\right)\leqslant\exp\left(-C_{5}nt^{2}\right)\,.\]

\(\Box\)

Proof of Lemma 9.: We write \(x_{i}=\sigma z_{i}+\mu\) where \(z_{i}\sim\mathcal{N}(0,I_{d})\). We have:

\[X^{\top}X\beta =\frac{1}{n}\sum_{i=1}^{n}(\mu+\sigma z_{i})\langle\mu+\sigma z_{i },\beta\rangle\] \[=\mu\langle\mu,\beta\rangle+\frac{\sigma^{2}}{n}\sum_{i=1}^{n}z_{ i}\langle z_{i},\beta\rangle+\frac{\sigma}{n}\sum_{i=1}^{n}\mu\langle z_{i}, \beta\rangle+\frac{\sigma}{n}\sum_{i=1}^{n}z_{i}\langle\mu,\beta\rangle\] \[=\mu\langle\mu,\beta\rangle+\frac{\sigma^{2}}{n}\sum_{i=1}^{n}z_{ i}\langle z_{i},\beta\rangle+\sigma\mu\langle\frac{1}{n}\sum_{i=1}^{n}z_{i}, \beta\rangle+\frac{\sigma\langle\mu,\beta\rangle}{n}\sum_{i=1}^{n}z_{i}\,.\]

The first term is deterministic and is to be kept. The second one is of order \(\sigma^{2}\beta\) whp using Lemma 8.

Then, \(\frac{1}{n}\sum_{i=1}^{n}z_{i}\sim\mathcal{N}(0,I_{d}/n)\), so that

\[\mathbb{P}\left(\left|\langle\frac{1}{n}\sum_{i=1}^{n}z_{i},\beta\rangle \right|\geqslant t\right)\leqslant 2e^{-nt^{2}/(2\|\beta\|_{2}^{2})}\,,\]

and

\[\mathbb{P}\left(\left|\frac{1}{n}\sum_{i=1}^{n}z_{ij}\right|\geqslant t \right)\leqslant 2e^{-nt^{2}/2}\,.\]

Hence,

\[\mathbb{P}\left(\sup_{\beta\in\mathcal{C}}\left\|\frac{1}{n}\sum_{i=1}^{n}z_{ ij}\right\|_{\infty}\geqslant t\,,\,\sup_{\beta\in\mathcal{C}}\left|\langle \frac{1}{n}\sum_{i=1}^{n}z_{i},\beta\rangle\right|\geqslant t\right) \leqslant 4e^{cs\ln(d)}e^{-nt^{2}/2}\,.\]

Thus, with probability \(1-Ce^{-n\varepsilon^{2}}\) and under the assumptions of Lemma 8, we have \(\left\|X^{\top}X\beta-\mu\langle\mu,\beta\rangle-\sigma^{2}\beta\right\|_{\infty}\leqslant\varepsilon\)

Proof of Lemma 10.: To ease notations, we assume that \(\sigma=1\). We remind (O'Donnell [46], Chapter 9 and Tao [54]) that for _i.i.d._ real random variables \(a_{1},\ldots,a_{n}\) that satisfy a tail inequality of the form

\[\mathbb{P}\big{(}|a_{1}-\mathbb{E}a_{1}|\geqslant t\big{)}\leqslant Ce^{-ct^{ p}}\,,\] (27)

for \(p<1\), then for all \(\varepsilon>0\) there exists \(C^{\prime},c^{\prime}\) such that for all \(t\),

\[\mathbb{P}\big{(}|\frac{1}{n}\sum_{i=1}^{n}a_{i}-\mathbb{E}a_{1}|\geqslant t \big{)}\leqslant C^{\prime}e^{-c^{\prime}nt^{p-\varepsilon}}\,.\]We now expand \(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\langle x_{i},\beta\rangle^{2}\):

\[\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\langle x_{i},\beta\rangle^{2} =\frac{1}{n}\sum_{i\in[n],k,\ell\in[d]}x_{i}^{2}x_{ik}x_{i\ell} \beta_{k}\beta_{\ell}\] \[=\frac{1}{n}\sum_{i\in[n],k\in[d]}x_{i}^{2}x_{ik}^{2}\beta_{k}^{2 }+\frac{1}{n}\sum_{i\in[n],k\neq\ell\in[d]}x_{i}^{2}x_{ik}x_{i\ell}\beta_{k} \beta_{\ell}\,.\]

Thus, for \(j\in[d]\),

\[\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\langle x_{i},\beta\rangle^{2} \right)_{j}=\sum_{k\in[d]}\frac{\beta_{k}^{2}}{n}\sum_{i\in[n]}x_{ij}^{2}x_{ ik}^{2}+\sum_{k\neq\ell\in[d]}\frac{\beta_{k}\beta_{\ell}}{n}\sum_{i\in[n]}x_{ij}^{2 }x_{ik}x_{i\ell}\,.\]

We notice that for all indices, all \(x_{ij}^{2}x_{ik}x_{i\ell}\) and \(x_{ij}^{2}x_{ik}^{2}\) satisfy the tail inequality Eq.27 for \(C=8\), \(c=1/2\) and \(p=1/2\), so that for \(\varepsilon=1/4\):

\[\mathbb{P}\big{(}|\frac{1}{n}\sum_{i=1}^{n}x_{ij}^{2}x_{ik}x_{i\ell}|\geqslant t \big{)}\leqslant C^{\prime}e^{-c^{\prime}nt^{1/4}}\quad,\quad\mathbb{P}\big{(} |\frac{1}{n}\sum_{i=1}^{n}x_{ij}^{2}x_{ik}^{2}-\mathbb{E}\left[x_{ij}^{2}x_{ik }^{2}\right]|\geqslant t\big{)}\leqslant C^{\prime}e^{-c^{\prime}nt^{1/4}}\,.\]

For \(j\neq k\), we have \(\mathbb{E}\left[x_{ij}^{2}x_{ik}^{2}\right]=1\) while for \(j=k\), we have \(\mathbb{E}\left[x_{ij}^{2}x_{ik}^{2}\right]=\mathbb{E}\left[x_{ij}^{4}\right]=3\). Hence,

\[\mathbb{P}\left(\exists j,k\neq\ell\,,\,|\frac{1}{n}\sum_{i=1}^{n}x_{ij}^{2}x_ {ik}x_{i\ell}|\geqslant t\,,\,|\frac{1}{n}\sum_{i=1}^{n}x_{ij}^{2}x_{ik}^{2}- \mathbb{E}\left[x_{ij}^{2}x_{ik}^{2}\right]|\geqslant t\right)\leqslant C^{ \prime}d^{2}e^{-c^{\prime}nt^{1/4}}\,.\]

Thus, with probability \(1-C^{\prime}d^{2}e^{-c^{\prime}nt^{1/4}}\), for all \(j\in[d]\),

\[\left|\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\langle x_{i},\beta\rangle^{2} \right)_{j}-2\beta_{j}^{2}-\|\beta\|_{2}^{2}\right|\leqslant t\sum_{k,\ell}| \beta_{k}||\beta_{\ell}|=t\|\beta\|_{1}^{2}\,.\]

Using the classical technique of Baraniuk et al. [4], to make a union bound on all \(s\)-sparse vectors, we consider an \(\varepsilon\)-net of the set of \(s\)-sparse vectors of \(\ell^{2}\)-norm smaller than 1. This \(\varepsilon\)-net is of cardinality less than \((C_{0}/\varepsilon)^{s}d^{s}\), and we only need to take \(\varepsilon\) of order 1 to obtain the result for all \(s\)-sparse vectors. This leads to:

\[\mathbb{P}\left(\exists\beta\in\mathbb{R}^{d}\,\,\text{$s$-sparse and }\|\beta\|_{2}\leqslant 1\,,\,\exists j\in\mathbb{R}^{d}\,, \quad\left|\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\langle x_{i},\beta\rangle^ {2}\right)_{j}-2\beta_{j}^{2}-\|\beta\|_{2}^{2}\right|\geqslant t\|\beta\|_{1} ^{2}\right)\] \[\leqslant C^{\prime}d^{2}e^{c_{1}s+s\ln(d)}e^{-c^{\prime}nt^{1/4}}\,.\]

This probability is equal to \(C^{\prime}/d^{2}\) for \(t=\left(\frac{(s+4)\ln(d)+c_{1}s}{c^{\prime}n}\right)^{4}\). We conclude that with probability \(1-C^{\prime}/d^{2}\), all \(s\)-sparse vectors \(\beta\) satisfy:

\[\left|\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\langle x_{i},\beta\rangle^{2} \right)_{j}-2\beta_{j}^{2}-\|\beta\|_{2}^{2}\right|\leqslant\left(\frac{(s+4) \ln(d)+c_{1}s}{c^{\prime}n}\right)^{4}\|\beta\|_{1}^{2}\leqslant\left(\frac{(s +4)\ln(d)+c_{1}s}{c^{\prime}n}\right)^{4}s\|\beta\|_{2}^{2}\,,\]

and the RHS is smaller than \(\|\beta\|_{2}^{2}/2\) for \(n\geqslant\Omega(s^{5/4}\ln(d))\). 

Proof of Lemma11.: We write \(x_{i}=\mu+\sigma z_{i}\) where \(x_{i}\sim\mathcal{N}(0,1)\). We have:

\[\mathbb{P}\big{(}\forall i\in[n],\forall j\in[d],\,|z_{ij}|\geqslant t\big{)} \leqslant e^{\ln(nd)-t^{2}/2}=\frac{1}{nd}\,,\]

for \(t=2\sqrt{\ln(nd)}\). Thus, if \(\mu\geqslant 4\sigma\sqrt{\ln(nd)}\) we have \(\frac{\mu}{2}\leqslant x_{i}\leqslant 2\mu\), so that

\[\frac{\mu^{2}}{2n}\sum_{i}\langle x_{i},\beta\rangle^{2}\leqslant\frac{1}{n} \sum_{i}x_{i}^{2}\langle x_{i},\beta\rangle^{2}\leqslant\frac{4\mu^{2}}{n} \sum_{i}\langle x_{i},\beta\rangle^{2}\,.\]Then, \(\langle x_{i},\beta\rangle\sim\mathcal{N}(\langle\mu,\beta\rangle,\sigma^{2}\| \beta\|_{2}^{2})\). For now, we assume that \(\left\|\beta\right\|_{2}=1\). We have \(\mathbb{P}(|\langle x_{i},\beta\rangle^{2}-\langle\mu,\beta\rangle^{2}-\sigma ^{2}\|\beta\|_{2}^{2}|\geqslant t)\leqslant Ce^{-ct/\sigma^{2}}\), and for \(t\leqslant 1\), using concentration of subexponential random variables [58]:

\[\mathbb{P}\left(\left|\frac{1}{n}\sum_{i}\langle x_{i},\beta\rangle^{2}- \langle\mu,\beta\rangle^{2}-\sigma^{2}\|\beta\|_{2}^{2}\right|\geqslant t \right)\leqslant C^{\prime}e^{-nc^{\prime}t^{2}/\sigma^{4}}\,,\]

and using the \(\varepsilon\)-net trick of Baraniuk et al. [4],

\[\mathbb{P}\left(\sup_{\beta\in\mathcal{C}}\left|\frac{1}{n}\sum_{i}\langle x_ {i},\beta\rangle^{2}-\langle\mu,\beta\rangle^{2}-\sigma^{2}\|\beta\|_{2}^{2} \right|\geqslant t\right)\leqslant C^{\prime}e^{s\ln(d)-nc^{\prime}t^{2}/ \sigma^{4}}=\frac{C^{\prime}}{d^{2}}\,,\]

for \(t=\sigma^{2}\|\beta\|_{2}^{2}\sqrt{\frac{2(cs+2)\ln(d)}{n}}\). Consequently, we have, with probability \(1-\frac{C^{\prime}}{d^{2}}-\frac{1}{nd}\):

\[\frac{\mu^{2}}{2}\left(\langle\mu,\beta\rangle^{2}+\frac{1}{2}\sigma^{2}\| \beta\|_{2}^{2}\right)\leqslant\frac{1}{n}\sum_{i}x_{i}^{2}\langle x_{i},\beta \rangle^{2}\leqslant 4\mu^{2}\left(\langle\mu,\beta\rangle^{2}+2\sigma^{2}\| \beta\|_{2}^{2}\right)\,.\]

Proof of Lemma 12.: First, we write \(x_{i}=\mu\mathbf{1}+\sigma z_{i}\), where \(z_{i}\sim\mathcal{N}(0,I)\), leading to:

\[\frac{1}{n}\sum_{i\in[n]}\|x_{i}\|_{2}^{2}x_{i}x_{i}^{\top}=\frac{1}{n}\sum_{ i\in[n]}\big{(}\sigma^{2}\|z_{i}\|_{2}^{2}+d\mu^{2}+2\sigma\mu(\mathbf{1},z_{i} )\big{)}x_{i}x_{i}^{\top}\]

We use concentration of \(\chi_{d}^{2}\) random variables around \(d\):

\[\mathbb{P}(\chi_{d}^{2}>d+2t+2\sqrt{dt})\geqslant t)\leqslant e^{-t}\quad \text{and}\quad\mathbb{P}(\chi_{d}^{2}>d-2\sqrt{dt})\leqslant t)\leqslant e^{- t}\,,\]

so that for all \(i\in[n]\),

\[\mathbb{P}(\left\|z_{i}\right\|_{2}^{2}\notin[d-2\sqrt{dt},d+2t+2\sqrt{dt}]) \leqslant 2e^{-t}\,.\]

Thus,

\[\mathbb{P}(\forall i\in[n],\,\left\|z_{i}\right\|_{2}^{2}\in[d-2\sqrt{dt},d+2t +2\sqrt{dt}])\geqslant 1-2ne^{-t}\,.\]

Taking \(t=d/16\),

\[\mathbb{P}(\forall i\in[n],\,\left\|z_{i}\right\|_{2}^{2}\in[\frac{d}{2},13d/8] )\geqslant 1-2ne^{-d/16}\,.\]

Then, for all \(i\), \(\langle\mathbf{1},z_{i}\rangle\) is of law \(\mathcal{N}(0,d)\), so that \(\mathbb{P}(|\langle\mathbf{1},z_{i}\rangle|\geqslant t)\leqslant 2e^{-t^{2}/(2d)}\) and

\[\mathbb{P}\big{(}\forall i\in[n],\,|\langle\mathbf{1},z_{i}\rangle|\geqslant t \big{)}\leqslant 2ne^{-\frac{t^{2}}{2d}}\,.\]

Taking \(t=\sqrt{2}d^{3/4}\),

\[\mathbb{P}\big{(}\forall i\in[n],\,|\langle\mathbf{1},z_{i}\rangle|\geqslant d ^{3/4}\big{)}\leqslant 2ne^{-d^{1/2}}\,.\]

Thus, with probability \(1-2n(e^{-d/16}+e^{-\sqrt{d}}\), we have \(\forall i\in[n]\), \(|\langle\mathbf{1},z_{i}\rangle|\geqslant d^{3/4}\) and \(\|z_{i}\|_{2}^{2}\in[\frac{d}{2},13d/8]\), so that

\[\big{(}\frac{d}{2}\sigma^{2}+d\mu^{2}-2\mu\sigma d^{3/4}\big{)}H\preceq\tilde{ H}\preceq\big{(}\frac{13d}{8}\sigma^{2}+d\mu^{2}+2\mu\sigma d^{3/4}\big{)}H\,,\]

leading to the desired result. 

Proof of Lemma 13.: We have:

\[\tilde{H}_{b} =\mathbb{E}\left[\frac{1}{b^{2}}\sum_{i,j\in\mathcal{B}}\langle x _{i},x_{j}\rangle x_{i}x_{j}^{\top}\right]\] \[=\mathbb{E}\left[\frac{1}{b^{2}}\sum_{i\in\mathcal{B}}\|x_{i}\|_ {2}^{2}x_{i}x_{i}^{\top}+\frac{1}{b^{2}}\sum_{i,j\in\mathcal{B},\,i\neq j} \langle x_{i},x_{j}\rangle x_{i}x_{j}^{\top}\right]\] \[=\frac{1}{b^{2}}\sum_{i\in[n]}\mathbb{P}(i\in\mathcal{B})\|x_{i} \|_{2}^{2}x_{i}x_{i}^{\top}+\frac{1}{b^{2}}\sum_{i\neq j}\mathbb{P}(i,j\in \mathcal{B})\langle x_{i},x_{j}\rangle x_{i}x_{j}^{\top}\,.\]Then, since \(\mathbb{P}(i\in\mathcal{B})=\frac{b}{n}\) and \(\mathbb{P}(i,j\in\mathcal{B})=\frac{b(b-1)}{n(n-1)}\) for \(i\neq j\), we get that:

\[\tilde{H}_{b}=\frac{1}{bn}\sum_{i\in[n]}\left\|x_{i}\right\|_{2}^{2}x_{i}x_{i}^ {\top}+\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\langle x_{i},x_{j}\rangle x_{i}x_{j }^{\top}\,.\]

Using Lemma 12, the first term satisfies:

\[\mathbb{P}\Big{(}\frac{d(\mu^{2}+\sigma^{2})}{b}C_{2}H\preceq\frac{1}{bn}\sum _{i\in[n]}\left\|x_{i}\right\|_{2}^{2}x_{i}x_{i}^{\top}\preceq\frac{d(\mu^{2}+ \sigma^{2})}{b}C_{3}H\Big{)}\geqslant 1-2ne^{-d/16}\,.\]

We now show that the second term is of smaller order. Writing \(x_{i}=\mu\mathbf{1}+\sigma z_{i}\) where \(z_{i}\sim\mathcal{N}(0,I_{d})\), we have:

\[\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\langle x_{i},x_{j}\rangle x_{i}x_{j}^{ \top}=\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\langle x_{i},x_{j}\rangle x_{i}x_{j }^{\top}\]

For \(i\neq j\), \(\langle x_{i},x_{j}\rangle=\sum_{k=1}^{d}x_{ik}x_{jk}=\sum_{k=1}^{d}a_{k}\) where \(a_{k}=x_{ik}x_{jk}\) satisfies \(\mathbb{E}a_{k}=0\), \(\mathbb{E}a_{k}^{2}=1\) and \(\mathbb{P}(a_{k}\geqslant t)\leqslant 2\mathbb{P}(|x_{ik}|\geqslant\sqrt{t}) \leqslant 4e^{-t/2}\). Hence, \(a_{k}\) is a centered subexponential random variables. Using concentration of subexponential random variables [58], for \(t\leqslant 1\),

\[\mathbb{P}\left(\frac{1}{d}|\langle x_{i},x_{j}\rangle|\geqslant t\right) \leqslant 2e^{-cdt^{2}}\,.\]

Thus,

\[\mathbb{P}\left(\forall i\neq j,\,\frac{1}{d}|\langle x_{i},x_{j}\rangle| \leqslant t\right)\geqslant 1-n(n-1)e^{-cdt^{2}}\,.\]

Then, taking \(t=d^{-1/2}4\ln(n)/c\), we have:

\[\mathbb{P}\left(\forall i\neq j,\,\frac{1}{d}|\langle x_{i},x_{j}\rangle| \leqslant\frac{4\ln(n)}{c\sqrt{d}}\right)\geqslant 1-\frac{1}{n^{2}}\,.\]

Going back to our second term,

\[\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\langle x_{i},x_{j}\rangle x_{ i}x_{j}^{\top} =\frac{(b-1)}{bn(n-1)}\sum_{i<j}\langle x_{i},x_{j}\rangle\big{(}x _{i}x_{j}^{\top}+x_{j}x_{i}^{\top}\big{)}\] \[\preceq\frac{(b-1)}{bn(n-1)}\sum_{i<j}\big{|}\langle x_{i},x_{j} \rangle\big{|}\big{(}x_{i}x_{i}^{\top}+x_{j}x_{j}^{\top}\big{)}\,,\]

where we used \(x_{i}x_{j}^{\top}+x_{j}x_{i}^{\top}\preceq x_{i}x_{i}^{\top}+x_{j}x_{j}^{\top}\). Thus,

\[\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\langle x_{i},x_{j}\rangle x_ {i}x_{j}^{\top} \preceq\sup_{i\neq j}|\langle x_{i},x_{j}\rangle|\times\frac{(b-1) }{bn(n-1)}\sum_{i<j}\big{(}x_{i}x_{i}^{\top}+x_{j}x_{j}^{\top}\big{)}\] \[=\sup_{i\neq j}|\langle x_{i},x_{j}\rangle|\times\frac{b-1}{b} \frac{1}{n-1}\sum_{i=1}^{n}x_{i}x_{i}^{\top}\] \[=\sup_{i\neq j}|\langle x_{i},x_{j}\rangle|\times\frac{b-1}{b} \frac{n}{n-1}H\,.\]

Similarly, we have

\[\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\langle x_{i},x_{j}\rangle x_{i}x_{j}^{ \top}\succeq-\sup_{i\neq j}|\langle x_{i},x_{j}\rangle|\times\frac{b-1}{b} \frac{n}{n-1}H\,.\]

Hence, with probability \(1-1/n^{2}\),

\[-\frac{4\ln(n)}{c\sqrt{d}}\times\frac{b-1}{b}\frac{n}{n-1}H\preceq\frac{(b-1) }{bn(n-1)}\sum_{i\neq j}\langle x_{i},x_{j}\rangle x_{i}x_{j}^{\top}\preceq \frac{4\ln(n)}{c\sqrt{d}}\times\frac{b-1}{b}\frac{n}{n-1}H\,.\]Wrapping things up, with probability \(1-1/n^{2}-2ne^{-d/16}\),

\[\left(-\frac{4\ln(n)}{c\sqrt{d}}\frac{b-1}{b}\frac{n}{n-1}+C_{2}\frac{d}{b} \right)\times H\preceq\tilde{H}_{b}\preceq\left(\frac{4\ln(n)}{c\sqrt{d}}\frac{ b-1}{b}\frac{n}{n-1}+C_{3}\frac{d}{b}\right)\times H\,.\]

Thus, provided that \(\frac{4\ln(n)}{c\sqrt{d}}\leqslant\frac{C_{3}d}{2b}\) and \(d\geqslant 48\ln(n)\), we have with probability \(1-3/n^{2}\):

\[C_{2}^{\prime}\frac{d}{b}\times H\preceq\tilde{H}_{b}\preceq C_{3}^{\prime} \frac{d}{b}\times H\,.\]

Proof of Lemma 13.: We have:

\[\tilde{H}_{b} =\mathbb{E}\left[\frac{1}{b^{2}}\sum_{i,j\in\mathcal{B}}\langle x _{i},x_{j}\rangle x_{i}x_{j}^{\top}\right]\] \[=\mathbb{E}\left[\frac{1}{b^{2}}\sum_{i\in\mathcal{B}}\|x_{i}\|_ {2}^{2}x_{i}x_{i}^{\top}+\frac{1}{b^{2}}\sum_{i,j\in\mathcal{B},\,i\neq j} \langle x_{i},x_{j}\rangle x_{i}x_{j}^{\top}\right]\] \[=\frac{1}{b^{2}}\sum_{i\in[n]}\mathbb{P}(i\in\mathcal{B})\|x_{i}\| _{2}^{2}x_{i}x_{i}^{\top}+\frac{1}{b^{2}}\sum_{i\neq j}\mathbb{P}(i,j\in \mathcal{B})\langle x_{i},x_{j}\rangle x_{i}x_{j}^{\top}\,.\]

Then, since \(\mathbb{P}(i\in\mathcal{B})=\frac{b}{n}\) and \(\mathbb{P}(i,j\in\mathcal{B})=\frac{b(b-1)}{n(n-1)}\) for \(i\neq j\), we get that:

\[\tilde{H}_{b}=\frac{1}{bn}\sum_{i\in[n]}\|x_{i}\|_{2}^{2}x_{i}x_{i}^{\top}+ \frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\langle x_{i},x_{j}\rangle x_{i}x_{j}^{\top}\,.\]

Using Lemma 12, the first term satisfies:

\[\mathbb{P}\Big{(}\frac{d(\mu^{2}+\sigma^{2})}{b}C_{2}H\preceq\frac{1}{bn} \sum_{i\in[n]}\|x_{i}\|_{2}^{2}x_{i}x_{i}^{\top}\preceq\frac{d(\mu^{2}+\sigma ^{2})}{b}C_{3}H\Big{)}\geqslant 1-2ne^{-d/16}\,.\]

We now show that the second term is of smaller order. Writing \(x_{i}=\mu\mathbf{1}+\sigma z_{i}\) where \(z_{i}\sim\mathcal{N}(0,I_{d})\), we have:

\[\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\langle x_{i},x_{j}\rangle x_{i }x_{j}^{\top} =\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\big{(}\sigma^{2}\langle z_{i},z_{j}\rangle+\sigma\mu\langle\mathbf{1},z_{i}+z_{j}\rangle+\mu^{2}d\big{)}x_{ i}x_{j}^{\top}\] \[=\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\big{(}\sigma^{2}\langle z_{i },z_{j}\rangle+\sigma\mu\langle\mathbf{1},z_{i}+z_{j}\rangle\big{)}x_{i}x_{j}^ {\top}+\frac{(b-1)}{bn(n-1)}\mu^{2}d\sum_{i\neq j}x_{i}x_{j}^{\top}\]

For \(i\neq j\), \(\langle z_{i},z_{j}\rangle=\sum_{k=1}^{d}z_{ik}z_{jk}=\sum_{k=1}^{d}a_{k}\) where \(a_{k}=z_{ik}z_{jk}\) satisfies \(\mathbb{E}a_{k}=0\), \(\mathbb{E}a_{k}^{2}=1\) and \(\mathbb{P}(a_{k}\geqslant t)\leqslant 2\mathbb{P}(|z_{ik}|\geqslant\sqrt{t}) \leqslant 4e^{-t/2}\). Hence, \(a_{k}\) is a centered subexponential random variables.

Using concentration of subexponential random variables [58], for \(t\leqslant 1\),

\[\mathbb{P}\left(\frac{1}{d}|\langle x_{i},x_{j}\rangle|\geqslant t\right) \leqslant 2e^{-cdt^{2}}\,.\]

Thus,

\[\mathbb{P}\left(\forall i\neq j,\,\frac{1}{d}|\langle x_{i},x_{j}\rangle| \leqslant t\right)\geqslant 1-n(n-1)e^{-cdt^{2}}\,.\]

Then, taking \(t=d^{-1/2}4\ln(n)/c\), we have:

\[\mathbb{P}\left(\forall i\neq j,\,\frac{1}{d}|\langle x_{i},x_{j}\rangle| \leqslant\frac{4\ln(n)}{c\sqrt{d}}\right)\geqslant 1-\frac{1}{n^{2}}\,.\]

For \(i\in[n]\), \(\langle\mathbf{1},z_{i}\rangle\sim\mathcal{N}(0,d)\) so that \(\mathbb{P}(|\langle\mathbf{1},z_{i}\rangle|\geqslant t)\leqslant 2e^{-t^{2}/(2d)}\), and

\[\mathbb{P}(\forall i\in[n],\,|\langle\mathbf{1},z_{i}\rangle|\leqslant t) \geqslant 1-2ne^{-t^{2}/(2d)}=1-\frac{2}{n^{2}}\,,\]for \(t=3\sqrt{d}\ln(n)\). Hence, with probability \(1-3/n^{2}\), for all \(i\neq j\) we have \(|\sigma^{2}\langle z_{i},z_{j}\rangle+\sigma\mu\langle\mathbf{1},z_{i}+z_{j} \rangle|\leqslant(\sigma^{2}+\sigma\mu)C\ln(n)/\sqrt{d}\).

Now,

\[\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\big{(}\sigma^{2}\langle z_{i},z_{j}\rangle+\sigma\mu\langle\mathbf{1},z_{i}+z_{j}\rangle\big{)}x_{i}x_{j}^{\top} =\frac{(b-1)}{bn(n-1)}\sum_{i<j}\big{(}\sigma^{2}\langle z_{i},z_{ j}\rangle+\sigma\mu\langle\mathbf{1},z_{i}+z_{j}\rangle\big{)}(x_{i}x_{j}^{ \top}+x_{j}x_{i}^{\top})\] \[\preceq\frac{(b-1)}{bn(n-1)}\sum_{i<j}\big{|}\sigma^{2}\langle z _{i},z_{j}\rangle+\sigma\mu\langle\mathbf{1},z_{i}+z_{j}\rangle\big{)}\big{|} \big{(}x_{i}x_{i}^{\top}+x_{j}x_{j}^{\top}\big{)}\,,\]

where we used \(x_{i}x_{j}^{\top}+x_{j}x_{i}^{\top}\preceq x_{i}x_{i}^{\top}+x_{j}x_{j}^{\top}\). Thus,

\[\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\big{(}\sigma^{2}\langle z_{i},z_{j}\rangle+\sigma\mu\langle\mathbf{1},z_{i}+z_{j}\rangle\big{)}x_{i}x_{j}^{\top} \preceq\sup_{i\neq j}\big{|}\sigma^{2}\langle z_{i},z_{j}\rangle+ \sigma\mu\langle\mathbf{1},z_{i}+z_{j}\rangle\big{)}|\times\frac{(b-1)}{bn(n- 1)}\sum_{i<j}\big{(}x_{i}x_{i}^{\top}+x_{j}x_{j}^{\top}\big{)}\] \[=\sup_{i\neq j}\big{|}\sigma^{2}\langle z_{i},z_{j}\rangle+\sigma \mu\langle\mathbf{1},z_{i}+z_{j}\rangle\big{|}\times\frac{b-1}{b}\frac{1}{n-1} \sum_{i=1}^{n}x_{i}x_{i}^{\top}\] \[=\sup_{i\neq j}\big{|}\sigma^{2}\langle z_{i},z_{j}\rangle+\sigma \mu\langle\mathbf{1},z_{i}+z_{j}\rangle\big{|}\times\frac{b-1}{b}\frac{n}{n-1} H\,.\]

Similarly, we have

\[\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\big{(}\sigma^{2}\langle z_{i},z_{j}\rangle+\sigma\mu\langle\mathbf{1},z_{i}+z_{j}\rangle\big{)}x_{i}x_{j}^{ \top}\succeq-\sup_{i\neq j}\big{|}\sigma^{2}\langle z_{i},z_{j}\rangle+\sigma \mu\langle\mathbf{1},z_{i}+z_{j}\rangle\big{)}|\times\frac{b-1}{b}\frac{n}{n-1} H\,.\]

Hence, with probability \(1-3/n^{2}\),

\[-\frac{(\sigma^{2}+\sigma\mu)C\ln(n)}{\sqrt{d}}\times\frac{b-1}{b }\frac{n}{n-1}H\preceq\frac{(b-1)}{bn(n-1)}\sum_{i\neq j}\big{(}\sigma^{2} \langle z_{i},z_{j}\rangle+\sigma\mu\langle\mathbf{1},z_{i}+z_{j}\rangle\big{)} x_{i}x_{j}^{\top}\] \[\preceq\frac{(\sigma^{2}+\sigma\mu)C\ln(n)}{\sqrt{d}}\times\frac {b-1}{b}\frac{n}{n-1}H\,.\]

We thus have shown that this term (the one in the middle of the above inequality) is of smaller order.

We are hence left with \(\frac{(b-1)}{bn(n-1)}\mu^{2}d\sum_{i\neq j}x_{i}x_{j}^{\top}\). Denoting \(\bar{x}=\frac{1}{n}\sum_{i}x_{i}\), we have \(\frac{1}{n^{2}}\sum_{i\neq j}x_{i}x_{j}^{\top}=\frac{1}{n^{2}}\sum_{i,j}x_{i}x_{ j}^{\top}-\frac{1}{n^{2}}\sum_{i}x_{i}x_{i}^{\top}\), so that:

\[\frac{(b-1)}{bn(n-1)}\mu^{2}d\sum_{i\neq j}x_{i}x_{j}^{\top}=\frac{(b-1)n}{b(n- 1)}\mu^{2}d\left(\bar{x}\bar{x}^{\top}-\frac{1}{n}H\right)\,.\]

We note that we have \(H=\frac{1}{n}\sum_{i}x_{i}x_{i}^{\top}=\frac{1}{n^{2}}\sum_{i<j}x_{i}x_{i}x_{j}^ {\top}+x_{j}x_{j}^{\top}\succeq\frac{1}{n^{2}}\sum_{i<j}x_{i}x_{j}^{\top}+x_{j} x_{i}^{\top}=\bar{x}\bar{x}^{\top}\) using \(x_{i}x_{i}^{\top}+x_{j}x_{j}^{\top}\succeq x_{i}x_{j}^{\top}+x_{j}x_{i}^{\top}\). Thus, \(H\succeq\bar{x}\bar{x}^{\top}\succeq 0\), and:

\[-\frac{(b-1)n}{b(n-1)}\mu^{2}d\frac{1}{n}H\preceq\frac{(b-1)}{bn(n-1)}\mu^{2}d \sum_{i\neq j}x_{i}x_{j}^{\top}\preceq\frac{(b-1)n}{b(n-1)}\mu^{2}d(1-1/n)H\,.\]

We are now able to wrap everything together. With probability \(1-2ne^{-d/16}-3/n^{2}\), we have, for some numerical constants \(c_{1},c_{2},c_{3},C>0\):

\[\left(c_{1}\frac{d(\mu^{2}+\sigma^{2})}{b}-c_{2}\frac{(\sigma^{2}+\mu^{2})\ln(n) }{\sqrt{d}}-c_{3}\frac{\mu^{2}d}{n}\right)H\preceq\tilde{H}_{b}\preceq C \left(\frac{d(\mu^{2}+\sigma^{2})}{b}+\frac{(\sigma^{2}+\mu^{2})\ln(n)}{\sqrt{d}} +\mu^{2}d\right)\]