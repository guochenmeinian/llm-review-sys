# A Regularized Conditional GAN for Posterior Sampling in Image Recovery Problems

 Matthew C. Bendel

Dept. ECE

The Ohio State University

Columbus, OH 43210

bendel.8@osu.edu

&Rizwan Ahmad

Dept. BME

The Ohio State University

Columbus, OH 43210

ahmad.46@osu.edu

&Philip Schniter

Dept. ECE

The Ohio State University

Columbus, OH 43201

schniter.1@osu.edu

###### Abstract

In image recovery problems, one seeks to infer an image from distorted, incomplete, and/or noise-corrupted measurements. Such problems arise in magnetic resonance imaging (MRI), computed tomography, deblurring, super-resolution, inpainting, phase retrieval, image-to-image translation, and other applications. Given a training set of signal/measurement pairs, we seek to do more than just produce one good image estimate. Rather, we aim to rapidly and accurately sample from the posterior distribution. To do this, we propose a regularized conditional Wasserstein GAN that generates dozens of high-quality posterior samples per second. Our regularization comprises an \(\ell_{1}\) penalty and an adaptively weighted standard-deviation reward. Using quantitative evaluation metrics like conditional Frechet inception distance, we demonstrate that our method produces state-of-the-art posterior samples in both multicoil MRI and large-scale inpainting applications. The code for our model can be found here: https://github.com/matt-bendel/rcGAN.

## 1 Introduction

We consider image recovery, where one observes measurements \(\bm{y}=\mathcal{M}(\bm{x})\) of the true image \(\bm{x}\) that may be masked, distorted, and/or corrupted \(\bm{x}\) with noise, and the goal is to infer \(\bm{x}\) from \(\bm{y}\). This includes linear inverse problems arising in, e.g., deblurring, super-resolution, inpainting, colorization, computed tomography (CT), and magnetic resonance imaging (MRI), where \(\bm{y}=\bm{A}\bm{x}+\bm{w}\) with known linear operator \(\bm{A}\) and noise \(\bm{w}\). But it also includes non-linear inverse problems like those arising in phase-retrieval and dequantization, as well as image-to-image translation problems. In all cases, it is impossible to perfectly infer \(\bm{x}\) from \(\bm{y}\).

Image recovery is often posed as finding the single "best" recovery \(\widehat{\bm{x}}\), which is known as a _point estimate_ of \(\bm{x}\)[1]. But point estimation is problematic due to the _perception-distortion tradeoff_[2], which establishes a fundamental tradeoff between distortion (defined as some distance between \(\widehat{\bm{x}}\) and \(\bm{x}\)) and perceptual quality (defined as some distance between \(\widehat{\bm{x}}\) and the set of clean images). For example, the minimum mean-squared error (MMSE) recovery \(\widehat{\bm{x}}_{\text{mmse}}\) is optimal in terms of \(\ell_{2}\) distortion, but can be unrealistically smooth. Although one could instead compute an approximation of the maximum a posteriori (MAP) estimate [3] or minimize some combination of perceptual and distortion losses, it's unclear which combination would be most appropriate.

Another major limitation with point estimation is that it's unclear how certain one can be about the recovered \(\widehat{\bm{x}}\). For example, with deep-learning-based recovery, it's possible to hallucinate a nice-looking \(\widehat{\bm{x}}\), but is it correct? Quantifying the uncertainty in \(\widehat{\bm{x}}\) is especially important in medical applications such as MRI, where a diagnosis must be made based on the measurements \(\bm{y}\). Rather than simply reporting our best guess of whether a pathology is present or absent based on \(\widehat{\bm{x}}\), we might want to report the probability that the pathology is present (given all available data).

Yet another problem with point estimation is that the estimated \(\widehat{\bm{x}}\) could pose issues with fairness [4]. For example, say we are inpainting a face within an image. With a racially heterogeneous prior distribution, \(\widehat{\bm{x}}_{\mathsf{mmse}}\) (being the posterior mean) will be biased towards the most predominant race. The same will be true of many other point estimates \(\widehat{\bm{x}}\).

To address the aforementioned limitations of point estimation, we focus on _generating samples from the posterior distribution_\(p_{\kappa|\mathsf{y}}(\cdot|\bm{y})\), which represents the complete state-of-knowledge about \(\bm{x}\) given the measurements \(\bm{y}\). The posterior correctly fuses prior knowledge with measurement knowledge, thereby alleviating any concerns about fairness (assuming the data used to represent the prior is fairly chosen [4]). Furthermore, the posterior directly facilitates uncertainty quantification via, e.g., pixel-wise standard deviations or pathology detection probabilities (see Appendix A). Also, if it was important to report a single "good" recovery, then posterior sampling leads to an easy navigation of the perception-distortion tradeoff. For example, averaging \(P\geq 1\) posterior samples gives a close approximation to the less-distorted-but-oversmooth \(\widehat{\bm{x}}_{\mathsf{mmse}}\) with large \(P\) and sharp-but-more-distorted \(\widehat{\bm{x}}\) with small \(P\). Additionally, posterior sampling unlocks other important capabilities such as adaptive acquisition [5] and counterfactual diagnosis [6].

Concretely, given a training dataset of image/measurement pairs \(\{(\bm{x}_{t},\bm{y}_{t})\}_{t=1}^{T}\), our goal is to learn a generating function \(G_{\bm{\theta}}\) that, for a new \(\bm{y}\), maps random code vectors \(\bm{z}\sim\mathcal{N}(\bm{0},\bm{I})\) to posterior samples \(\widehat{\bm{x}}=G_{\bm{\theta}}(\bm{z},\bm{y})\sim p_{\kappa|\mathsf{y}}( \cdot|\bm{y})\). There exist several well-known approaches to this task, with recent literature focusing on conditional generative adversarial networks (cGANs) [7; 8; 9; 10], conditional variational autoencoders (cVAEs) [11; 12; 13], conditional normalizing flows (cNFs) [14; 15; 16], and score/diffusion/Langevin-based generative models [17; 18; 19; 20; 21]. Despite it being a long-standing problem, posterior image sampling remains challenging. Although score/diffusion/Langevin approaches have dominated the recent literature due to advances in accuracy and diversity, their sample-generation speeds remain orders-of-magnitude behind those of cGANs, cVAEs, and cNFs.

We choose to focus on cGANs, which are typically regarded as generating samples of high quality but low diversity. Our proposed cGAN tackles the lack-of-diversity issue using a novel regularization that consists of supervised-\(\ell_{1}\) loss plus an adaptively weighted standard-deviation (SD) reward. This is not a heuristic choice; we prove that our regularization enforces consistency with the true posterior mean and covariance under certain conditions.

Experimentally, we demonstrate our regularized cGAN on accelerated MRI and large-scale face completion/inpainting. We consider these applications for three main reasons. First, uncertainty quantification in MRI, and fairness in face-generation, are both of paramount importance. Second, posterior-sampling has been well studied for both applications, and fine-tuned cGANs [9] and score/Langevin-based approaches [19; 20] are readily available. Third, the linear operator "\(\bm{A}\)" manifests very differently in these two applications,1 which illustrates the versatility of our approach. To quantify performance, we focus on conditional Frechet inception distance (CFID) [22], which is a principled way to quantify the difference between two high-dimensional posterior distributions, although we also report other metrics. Our results show the proposed regularized cGAN (rcGAN) outperforming existing cGANs [8; 23; 9] and the score/diffusion/Langevin approaches from [19] and [20] in all tested metrics, while generating samples \(\sim 10^{4}\) times faster than [19; 20].

Footnote 1: In MRI, the forward operator acts locally in the frequency domain but globally in the pixel domain, while in inpainting, the operator acts locally in the pixel domain but globally in the frequency domain.

## 2 Problem formulation and background

We build on the Wasserstein cGAN framework from [8]. The goal is to design a generator network \(G_{\bm{\theta}}:\mathcal{Z}\times\mathcal{Y}\rightarrow\mathcal{X}\) such that, for fixed \(\bm{y}\), the random variable \(\widehat{\bm{x}}=G_{\bm{\theta}}(\bm{z},\bm{y})\) induced by \(\bm{z}\sim p_{\bm{z}}\) has a distribution that best matches the posterior \(p_{\kappa|\mathsf{y}}(\cdot|\bm{y})\) in Wasserstein-1 distance. Here, \(\mathcal{X}\), \(\mathcal{Y}\), and \(\mathcal{Z}\) denote the sets of \(\bm{x}\), \(\bm{y}\), and \(\bm{z}\), respectively, and \(\bm{z}\) is drawn independently of \(\bm{y}\).

The Wasserstein-1 distance can be expressed as

\[W_{1}(p_{\kappa|\mathsf{y}}(\cdot,\bm{y}),p_{\kappa|\mathsf{y}}(\cdot,\bm{y}))= \sup_{D\in L_{1}}\mathrm{E}_{\kappa|\mathsf{y}}\{D(\bm{x},\bm{y})\}-\mathrm{E} _{\kappa|\mathsf{y}}\{D(\widehat{\bm{x}},\bm{y})\},\] (1)

where \(L_{1}\) denotes functions that are 1-Lipschitz with respect to their first argument and \(D:\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}\) is a "critic" or "discriminator" that tries to distinguish between true \(\bm{x}\) and generated \(\widehat{\bm{x}}\) given \(\bm{y}\)Since we want the method to work for typical values of \(\bm{y}\), we define a loss by taking an expectation of (1) over \(\bm{y}\sim p_{\gamma}\). Since the expectation commutes with the supremum in (1), we have [8]

\[\operatorname{E_{\gamma}}\{W_{1}(p_{\operatorname{xyl}}(\cdot,\bm{ y}),p_{\operatorname{xyl}}(\cdot,\bm{y}))\} =\sup_{D\in L_{1}}\operatorname{E_{\text{x,y}}}\{D(\bm{x},\bm{y}) \}-\operatorname{E_{\text{x,y}}}\{D(\widehat{\bm{x}},\bm{y})\}\] (2) \[=\sup_{D\in L_{1}}\operatorname{E_{\text{x,z,y}}}\{D(\bm{x},\bm{ y})-D(G_{\bm{\theta}}(\bm{z},\bm{y}),\bm{y})\}.\] (3)

In practice, \(D\) is implemented by a neural network \(D_{\bm{\phi}}\) with parameters \(\bm{\phi}\), and \((\bm{\theta},\bm{\phi})\) are trained by alternately minimizing

\[\mathcal{L}_{\text{adv}}(\bm{\theta},\bm{\phi})\triangleq\operatorname{E_{ \text{x,z,y}}}\{D_{\bm{\phi}}(\bm{x},\bm{y})-D_{\bm{\phi}}(G_{\bm{\theta}}( \bm{z},\bm{y}),\bm{y})\}\] (4)

with respect to \(\bm{\theta}\) and minimizing \(-\mathcal{L}_{\text{adv}}(\bm{\theta},\bm{\phi})+\mathcal{L}_{\text{gp}}(\bm {\phi})\) with respect to \(\bm{\phi}\), where \(\mathcal{L}_{\text{gp}}(\bm{\phi})\) is a gradient penalty that is used to encourage \(D_{\bm{\phi}}\in L_{1}\)[24]. Furthermore, the expectation over \(\bm{x}\) and \(\bm{y}\) in (4) is replaced in practice by a sample average over the training examples \(\{(\bm{x}_{t},\bm{y}_{t})\}_{t=1}^{T}\).

One of the main challenges with the cGAN framework in image recovery problems is that, for each measurement example \(\bm{y}_{t}\), there is only a single image example \(\bm{x}_{t}\). Thus, with the previously described training methodology, there is no incentive for the generator to produce diverse samples \(G(\bm{z},\bm{y})|_{\bm{z}\sim p_{z}}\) for a fixed \(\bm{y}\). This can lead to the generator learning to ignore the code vector \(\bm{z}\), which causes a form of "mode collapse."

Although issues with stability and mode collapse are also present in _unconditional_ GANs (uGANs) or discretely conditioned cGANs [25], the causes are fundamentally different than in continuously conditioned cGANs like ours. With continuously conditioned cGANs, there is only _one_ example of a valid \(\bm{x}_{t}\) for each given \(\bm{y}_{t}\), whereas with uGANs there are many \(\bm{x}_{t}\) and with discretely conditioned cGANs there are many \(\bm{x}_{t}\) for each conditioning class. As a result, most strategies that are used to combat mode-collapse in uGANs [26; 27; 28] are not well suited to cGANs. For example, mini-batch discrimination strategies like MBSD [29], where the discriminator aims to distinguish a mini-batch of true samples \(\{\bm{x}_{t}\}\) from a mini-batch of generated samples \(\{\widehat{\bm{x}}_{t}\}\), don't work with cGANs because the posterior statistics are very different than the prior statistics.

To combat mode collapse in cGANs, Adler & Oktem [8] proposed to use a three-input discriminator \(D^{\text{adler}}_{\bm{\phi}}:\mathcal{X}\times\mathcal{X}\times\mathcal{Y}\to \mathbb{R}\) and replace \(\mathcal{L}_{\text{adv}}\) from (4) with the loss

\[\mathcal{L}^{\text{adler}}_{\text{adv}}(\bm{\theta},\bm{\phi}) \triangleq\operatorname{E_{\text{x,z,z,y}}}\big{\{}\tfrac{1}{2}D^{ \text{adler}}_{\bm{\phi}}(\bm{x},G_{\bm{\theta}}(\bm{z}_{1},\bm{y}),\bm{y})+ \tfrac{1}{2}D^{\text{adler}}_{\bm{\phi}}(G_{\bm{\theta}}(\bm{z}_{2},\bm{y}), \bm{x},\bm{y})\] \[\quad-D^{\text{adler}}_{\bm{\phi}}(G_{\bm{\theta}}(\bm{z}_{1},\bm {y}),G_{\bm{\theta}}(\bm{z}_{2},\bm{y}),\bm{y})\big{\}},\] (5)

which rewards variation between the first and second inputs to \(D^{\text{adler}}_{\bm{\phi}}\). They then proved that minimizing \(\mathcal{L}^{\text{adler}}_{\text{adv}}\) in place of \(\mathcal{L}_{\text{adv}}\) does not compromise the Wasserstein cGAN objective, i.e., \(\operatorname{arg\,min}_{\bm{\theta}}\mathcal{L}^{\text{adler}}_{\text{adv}}( \bm{\theta},\bm{\phi})=\operatorname{arg\,min}_{\bm{\theta}}\mathcal{L}_{\text{ adv}}(\bm{\theta},\bm{\phi})\). As we show in Section 4, this approach does prevent complete mode collapse, but it leaves much room for improvement.

## 3 Proposed method

### Proposed regularization: supervised-\(\ell_{1}\) plus SD reward

We now propose a novel cGAN regularization framework. To train the generator, we propose to solve

\[\operatorname{arg\,min}_{\bm{\theta}}\{\beta_{\text{adv}}\mathcal{L}_{\text{adv }}(\bm{\theta},\bm{\phi})+\mathcal{L}_{1,\text{SD},P_{\text{train}}}(\bm{ \theta},\beta_{\text{SD}})\}\] (6)

with appropriately chosen \(\beta_{\text{adv}},\beta_{\text{SD}}>0\) and \(P_{\text{train}}\geq 2\), where the regularizer

\[\mathcal{L}_{1,\text{SD},P_{\text{train}}}(\bm{\theta},\beta_{\text{SD}}) \triangleq\mathcal{L}_{1,P_{\text{train}}}(\bm{\theta})-\beta_{\text{SD}} \mathcal{L}_{\text{SD},P_{\text{train}}}(\bm{\theta})\] (7)

is constructed from the \(P_{\text{train}}\)-sample supervised-\(\ell_{1}\) loss and standard-deviation (SD) reward terms

\[\mathcal{L}_{1,P_{\text{train}}}(\bm{\theta}) \triangleq\operatorname{E_{\text{x,z,1}}}_{\text{x,z,y}}\big{\{} \|\bm{x}-\widehat{\bm{x}}_{(P_{\text{train}})}\|_{1}\big{\}}\] (8) \[\mathcal{L}_{\text{SD},P_{\text{train}}}(\bm{\theta}) \triangleq\sqrt{\tfrac{\pi}{2P_{\text{train}}(P_{\text{train}}-1)} }\sum_{i=1}^{P_{\text{train}}}\operatorname{E_{\text{z}_{1},\dots,\bm{z}_{p}, \gamma}}\big{\{}\|\widehat{\bm{x}}_{i}-\widehat{\bm{x}}_{(P_{\text{train}})}\| _{1}\big{\}},\] (9)

and where \(\{\widehat{\bm{x}}_{i}\}\) denote the generated samples and \(\widehat{\bm{x}}_{(P)}\) their \(P\)-sample average:

\[\widehat{\bm{x}}_{i}\triangleq G_{\bm{\theta}}(\bm{z}_{i},\bm{y}),\qquad \widehat{\bm{x}}_{(P)}\triangleq\tfrac{1}{P}\sum_{i=1}^{P}\widehat{\bm{x}}_{i}.\] (10)

The use of supervised-\(\ell_{1}\) loss and SD reward in (7) is not heuristic. As shown in Proposition 3.1, it encourages the samples \(\{\widehat{\bm{x}}_{i}\}\) to match the true posterior in both mean and covariance.

**Proposition 3.1**.: _Suppose \(P_{\text{train}}\geq 2\) and \(\bm{\theta}\) has complete control over the \(\bm{y}\)-conditional mean and covariance of \(\widehat{\bm{x}}_{i}\). Then the parameters \(\bm{\theta}_{*}=\arg\min_{\bm{\theta}}\mathcal{L}_{1,\mathsf{SD},P_{\text{train} }}(\bm{\theta},\beta_{\mathsf{SD}}^{\mathcal{N}})\) with_

\[\beta_{\mathsf{SD}}^{\mathcal{N}}\triangleq\sqrt{\tfrac{2}{\pi P_{\text{train }}(P_{\text{train}}+1)}}\] (11)

_yield generated statistics_

\[\operatorname{E}_{\bm{z}_{i}|\{\bm{\widehat{x}}_{i}(\bm{\theta}_{* })|\bm{y}\}} =\operatorname{E}_{\bm{x}|\{\bm{y}\}}\{\bm{x}|\bm{y}\}=\widehat{\bm{x}}_{ \text{rmse}}\] (12a) \[\operatorname{Cov}_{\bm{z}_{i}|\{\bm{\widehat{x}}_{i}(\bm{\theta}_ {*})|\bm{y}\}} =\operatorname{Cov}_{\bm{x}|\{\bm{y}\}}\{\bm{x}|\bm{y}\}\] (12b)

_when the elements of \(\widehat{\bm{x}}_{i}\) and \(\bm{x}\) are independent Gaussian conditioned on \(\bm{y}\). Thus, minimizing \(\mathcal{L}_{1,\mathsf{SD},P_{\text{train}}}\) encourages the \(\bm{y}\)-conditional mean and covariance of \(\widehat{\bm{x}}_{i}\) to match those of the true \(\bm{x}\)._

See Appendix B for a proof. In imaging applications, \(\widehat{\bm{x}}_{i}\) and \(\bm{x}\) may not be independent Gaussian conditioned on \(\bm{y}\), and so the value of \(\beta_{\mathsf{SD}}\) in (11) may not be appropriate. Thus we propose a method to automatically tune \(\beta_{\mathsf{SD}}\) in Section 3.2.

Figure 1 shows a toy example with parameters \(\bm{\theta}=[\mu,\sigma]^{\top}\), generator \(G_{\bm{\theta}}(z,y)=\mu+\sigma z\), and \(z\sim\mathcal{N}(0,1)\), giving generated posterior \(p_{\text{\tiny Syl}}(x|y)=\mathcal{N}(x;\mu,\sigma^{2})\). Assuming the true \(p_{\text{\tiny Syl}}(x|y)=\mathcal{N}(x;\mu_{0},\sigma_{0}^{2})\), Figs. 1(a)-(b) show that, by minimizing the proposed \(\mathcal{L}_{1,\mathsf{SD},P_{\text{train}}}(\bm{\theta},\beta_{\mathsf{SD}}^ {\mathcal{N}})\) regularization over \(\bm{\theta}=[\mu,\sigma]^{\top}\) for any \(P_{\text{train}}\geq 2\), we recover the true \(\bm{\theta}_{0}=[\mu_{0},\sigma_{0}]^{\top}\). They also show that the cost function steepens as \(P_{\text{train}}\) decreases, with agrees with our empirical finding that \(P_{\text{train}}=2\) tends to work best in practice.

We note that regularizing a cGAN with supervised-\(\ell_{1}\) loss alone is not new; see, e.g., [7]. In fact, the use of supervised-\(\ell_{1}\) loss is often preferred over \(\ell_{2}\) in image recovery because it results in sharper, more visually pleasing results [30]. But regularizing a cGAN using supervised-\(\ell_{1}\) loss _alone_ can push the generator towards mode collapse, for reasons described below. For example, in [7], \(\ell_{1}\)-induced mode collapse led the authors to use dropout to induce generator variation, instead of random \(\bm{z}_{i}\).

Why not supervised-\(\ell_{2}\) regularization?One may wonder: Why regularize using supervised-\(\ell_{1}\) loss plus an SD reward in (7) and not a more conventional choice like supervised-\(\ell_{2}\) loss plus a variance reward, or even supervised-\(\ell_{2}\) loss alone? We start by discussing the latter.

The use of supervised-\(\ell_{2}\) regularization in a cGAN can be found in [7]. In this case, to train the generator, one would aim to solve \(\arg\min_{\bm{\theta}}\{\mathcal{L}_{\text{adv}}(\bm{\theta},\bm{\phi})+ \lambda\mathcal{L}_{2}(\bm{\theta})\}\) with some \(\lambda>0\) and

\[\mathcal{L}_{2}(\bm{\theta})\triangleq\operatorname{E}_{\bm{x},\bm{y}}\big{\{} \|\bm{x}-\operatorname{E}_{\bm{z}}\{G_{\bm{\theta}}(\bm{z},\bm{y})\|_{2}^{2}\}. \] (13)

Ohayon et al. [23] revived this idea for the explicit purpose of fighting mode collapse. In practice, however, the \(\operatorname{E}_{\bm{z}}\) term in (13) must be implemented by a finite-sample average, giving

\[\mathcal{L}_{2,P_{\text{train}}}(\bm{\theta})\triangleq\operatorname{E}_{\bm{ x},\bm{z}_{1},\ldots,\bm{x},\bm{y}}\big{\{}\|\bm{x}-\tfrac{1}{P_{\text{train}}} \sum_{i=1}^{P_{\text{train}}}G_{\bm{\theta}}(\bm{z}_{i},\bm{y})\|_{2}^{2}\big{\}}\] (14)

for some \(P_{\text{train}}\geq 2\). For example, Ohayon's implementation [31] used \(P_{\text{train}}=8\). As we show in Proposition 3.2, \(\mathcal{L}_{2,P_{\text{train}}}\)_induces_ mode collapse rather than prevents it.

**Proposition 3.2**.: _Say \(P_{\text{train}}\) is finite and \(\bm{\theta}\) has complete control over the \(\bm{y}\)-conditional mean and covariance of \(\widehat{\bm{x}}_{i}\). Then the parameters \(\bm{\theta}_{*}=\arg\min_{\bm{\theta}}\mathcal{L}_{2,P_{\text{train}}}(\bm{ \theta})\) yield generated statistics_

\[\operatorname{E}_{\bm{z}_{i}|\{\bm{\widehat{x}}_{i}(\bm{\theta}_{* })|\bm{y}\}} =\operatorname{E}_{\bm{x}|\{\bm{y}\}}\{\bm{x}|\bm{y}\}=\widehat{\bm{x}}_{ \text{rmse}}\] (15a) \[\operatorname{Cov}_{\bm{z}_{i}|\{\bm{\widehat{x}}_{i}(\bm{\theta}_{* })|\bm{y}\}} =\bm{0}.\] (15b)

_Thus, minimizing \(\mathcal{L}_{2,P_{\text{train}}}\) encourages mode collapse._The proof (see Appendix C) follows from the bias-variance decomposition of (14), i.e.,

\[\mathcal{L}_{2,P_{\text{train}}}(\bm{\theta})\] \[=\mathds{E}_{\nu}\left\{\|\widehat{\bm{x}}_{\text{rmse}}-\mathds{E }_{\bm{x}\mid\nu}\{\widehat{\bm{x}}_{i}(\bm{\theta})|\bm{y}\}\|_{2}^{2}+\tfrac{ 1}{P_{\text{train}}}\operatorname{tr}[\operatorname{Cov}_{\bm{x}\mid\nu}\{ \widehat{\bm{x}}_{i}(\bm{\theta})|\bm{y}\}]+\mathds{E}_{\nu\mid\nu}\{\|\bm{e} _{\text{rmse}}\|_{2}^{2}|\bm{y}\}\right\},\] (16)

where \(\bm{e}_{\text{rmse}}\triangleq\bm{x}-\bm{x}_{\text{rmse}}\) is the MMSE error. Figure 1(c) shows that \(\mathcal{L}_{2,P_{\text{train}}}\) regularization causes mode collapse in the toy example, and Section 4.2 shows that it causes mode collapse in MRI.

Why not supervised \(\ell_{2}\) plus a variance reward?To mitigate \(\mathcal{L}_{2,P_{\text{train}}}\)'s incentive for mode-collapse, the second term in (16) could be canceled using a variance reward, giving

\[\mathcal{L}_{2,\text{var},P_{\text{train}}}(\bm{\theta}) \triangleq\mathcal{L}_{2,P_{\text{train}}}(\bm{\theta})-\tfrac{1}{P_{ \text{train}}}\mathcal{L}_{\text{var},P_{\text{train}}}(\bm{\theta})\] (17) \[\text{with }\mathcal{L}_{\text{var},P_{\text{train}}}(\bm{\theta}) \triangleq\tfrac{1}{P_{\text{train}}-1}\sum_{i=1}^{P_{\text{train}}} \mathds{E}_{\bm{x}_{1},\dots,\bm{x}_{\text{p}},\nu}\{\|\widehat{\bm{x}}_{i}( \bm{\theta})-\widehat{\bm{x}}_{{}_{(P)}}(\bm{\theta})\|_{2}^{2}\}.\] (18)

since Appendix D shows that \(\mathcal{L}_{\text{var},P_{\text{train}}}(\bm{\theta})\) is an unbiased estimator of the posterior trace-covariance:

\[\mathcal{L}_{\text{var},P_{\text{train}}}(\bm{\theta})=\mathds{E}_{\nu}\{ \operatorname{tr}[\operatorname{Cov}_{\bm{x}\mid\nu}\{\widehat{\bm{x}}_{i}( \bm{\theta})|\bm{y}\}]\}\;\text{ for any }P_{\text{train}}\geq 2.\] (19)

However, the resulting \(\mathcal{L}_{2,\text{var},P_{\text{train}}}\) regularizer in (17) does not encourage the generated covariance to match the _true_ posterior covariance, unlike the proposed \(\mathcal{L}_{1,\text{SD},P_{\text{train}}}\) regularizer in (7) (recall Proposition 3.1). For the toy example, this behavior is visible in Fig. 1(d).

### Auto-tuning of SD reward weight \(\beta_{\text{SD}}\)

We now propose a method to auto-tune \(\beta_{\text{SD}}\) in (7) for a given training dataset. Our approach is based on the observation that larger values of \(\beta_{\text{SD}}\) tend to yield samples \(\widehat{\bm{x}}_{i}\) with more variation. But more variation is not necessarily better; we want samples with the correct amount of variation. To assess the correct amount of variation, we compare the expected \(\ell_{2}\) error of the \(P\)-sample average \(\widehat{\bm{x}}_{{}_{(P)}}\) to that of \(\widehat{\bm{x}}_{{}_{(1)}}\). When \(\{\widehat{\bm{x}}_{i}\}\) are true-posterior samples, these errors follow a particular relationship, as established by Proposition 3.3 below (see Appendix E for a proof).

**Proposition 3.3**.: _Say \(\widehat{\bm{x}}_{i}\sim p_{\text{x}\mid\nu}(\cdot|\bm{y})\) are independent samples of the true posterior and, for any \(P\geq 1\), their \(P\)-sample average is \(\widehat{\bm{x}}_{{}_{(P)}}\triangleq\tfrac{1}{P}\sum_{i=1}^{P}\widehat{\bm{ x}}_{i}\). Then_

\[\mathcal{E}_{P}\triangleq\mathds{E}\{\|\widehat{\bm{x}}_{{}_{(P)}}-\bm{x}\|_{ 2}^{2}|\bm{y}\}=\tfrac{P+1}{P}\mathcal{E}_{\text{rmse}},\quad\text{which implies}\quad\tfrac{\mathcal{E}_{1}}{\mathcal{E}_{P}}=\tfrac{2P}{P+1}.\] (20)

Experimentally we find that \(\mathcal{E}_{1}/\mathcal{E}_{P}\) grows with the SD reward weight \(\beta_{\text{SD}}\). (See Fig. 2.) Thus, we propose to adjust \(\beta_{\text{SD}}\) so that the observed SNR-gain ratio \(\mathcal{E}_{1}/\mathcal{E}_{P_{\text{val}}}\) matches the correct ratio \((2P_{\text{val}})/(P_{\text{val}}+1)\) from (20), for some \(P_{\text{val}}\geq 2\) that does not need to match \(P_{\text{train}}\). (We use \(P_{\text{val}}=8\) in Section 4.) In particular, at each training epoch \(\tau\), we approximate \(\mathcal{E}_{P_{\text{val}}}\) and \(\mathcal{E}_{1}\) as follows:

\[\widehat{\mathcal{E}}_{P_{\text{val}},\tau} \triangleq\tfrac{1}{V}\sum_{\nu=1}^{V}\|\tfrac{1}{P_{\text{val} }}\sum_{i=1}^{P_{\text{val}}}G_{\bm{\theta}_{\tau}}(\bm{z}_{i,v},\bm{y}_{v})- \bm{x}_{v}\|_{2}^{2}\] (21) \[\widehat{\mathcal{E}}_{1,\tau} \triangleq\tfrac{1}{V}\sum_{\nu=1}^{V}\|G_{\bm{\theta}_{\tau}}(\bm{ z}_{1,v},\bm{y}_{v})-\bm{x}_{v}\|_{2}^{2},\] (22)

with validation set \(\{(\bm{x}_{v},\bm{y}_{v})\}_{v=1}^{V}\) and i.i.d. codes \(\{\bm{z}_{i,v}\}_{i=1}^{P_{\text{val}}}\). We update \(\beta_{\text{SD}}\) using gradient descent:

\[\beta_{\text{SD},\tau+1} =\beta_{\text{SD},\tau}-\mu_{\text{SD}}\cdot([\widehat{\mathcal{ E}}_{1,\tau}/\widehat{\mathcal{E}}_{P_{\text{val}},\tau}]_{\text{dB}}-[2P_{\text{val}}/(P_{ \text{val}}+1)]_{\text{dB}})\beta_{\text{SD}}^{\text{K}}\;\text{ for }\;\tau=0,1,2,\dots\] (23)

with \(\beta_{\text{SD},0}=\beta_{\text{SD}}^{\text{K}}\), some \(\mu_{\text{SD}}>0\), and \([x]_{\text{dB}}\triangleq 10\log_{10}(x)\).

Figure 2: Example PSNR of \(\widehat{\bm{x}}_{{}_{(P)}}\) versus \(P\), the number of averaged outputs, for several training \(\beta_{\text{SD}}\) and MRI recovery at \(R=4\). Also shown is the theoretical behavior for true-posterior samples.

Numerical experiments

### Conditional Frechet inception distance

As previously stated, our goal is to train a generator \(G_{\bm{\theta}}\) so that, for typical fixed values of \(\bm{y}\), the generated distribution \(p_{\mathrm{x|y}}(\cdot|\bm{y})\) matches the true posterior \(p_{\mathrm{x|y}}(\cdot|\bm{y})\). It is essential to have a quantitative metric for evaluating performance with respect to this goal. For example, it is not enough that the generated samples are "accurate" in the sense of being close to the ground truth, nor is it enough that they are "diverse" according to some heuristically chosen metric.

We quantify posterior-approximation quality using the conditional Frechet inception distance (CFID) [22], a computationally efficient approximation to the conditional Wasserstein distance

\[\mathrm{CWD}\triangleq\mathrm{E}_{\bm{\gamma}}\{W_{2}(p_{\mathrm{x|y}}(\cdot, \bm{y}),p_{\mathrm{x|y}}(\cdot,\bm{y}))\}.\] (24)

In (24), \(W_{2}(p_{\mathrm{a}},p_{\mathrm{b}})\) denotes the Wasserstein-2 distance between distributions \(p_{\mathrm{a}}\) and \(p_{\mathrm{b}}\), defined as

\[W_{2}(p_{\mathrm{a}},p_{\mathrm{b}})\triangleq\min_{p_{\mathrm{a},\mathrm{b}} \in\Pi(p_{\mathrm{a}},p_{\mathrm{b}})}\mathrm{E}_{\mathrm{a},\mathrm{b}}\{\| \bm{a}-\bm{b}\|_{2}^{2}\},\] (25)

where \(\Pi(p_{\mathrm{a}},p_{\mathrm{b}})\triangleq\left\{p_{\mathrm{a},\mathrm{b}} :p_{\mathrm{a}}=\int p_{\mathrm{a},\mathrm{b}}\,\mathrm{d}\bm{b}\text{ and }p_{\mathrm{b}}=\int p_{\mathrm{a},\mathrm{b}}\,\mathrm{d}\bm{a}\right\}\) denotes the set of joint distributions \(p_{\mathrm{a},\mathrm{b}}\) with prescribed marginals \(p_{\mathrm{a}}\) and \(p_{\mathrm{b}}\). Similar to how FID [32]--a popular uGAN metric--is computed, CFID approximates CWD (24) as follows: i) the random vectors \(\bm{x}\), \(\widehat{\bm{x}}\), and \(\bm{y}\) are replaced by low-dimensional embeddings \(\underline{\bm{x}}\), \(\widehat{\underline{\bm{x}}}\), and \(\underline{\bm{y}}\), generated by the convolutional layers of a deep network, and ii) the embedding distributions \(p_{\mathrm{x|y}}\) and \(p_{\mathrm{x|y}}\) are approximated by multivariate Gaussians. More details on CFID are given in Appendix F.

### MRI experiments

We consider parallel MRI recovery, where the goal is to recover a complex-valued multicoil image \(\bm{x}\) from zero-filled measurements \(\bm{y}\) (see Appendix G for details).

Data.For training data \(\{\bm{x}_{t}\}\), we used the first 8 slices of all fastMRI [33] T2 brain training volumes with at least 8 coils, cropping to \(384\times 384\) pixels and compressing to 8 virtual coils [34], yielding 12 200 training images. Using the same procedure, 2 376 testing and 784 validation images were obtained from the fastMRI T2 brain testing volumes. From the 2 376 testing images, 72 were randomly selected to evaluate the Langevin technique [19] in order to limit its sample generation to 6 days. To create the measurement \(\bm{y}_{t}\), we transformed \(\bm{x}_{t}\) to the Fourier domain, sampled using pseudo-random GRO patterns [35] at acceleration \(R=4\) and \(R=8\), and Fourier-transformed the zero-filled k-space measurements back to the (complex, multicoil) image domain.

Architecture.We use a UNet [36] for our generator and a standard CNN for our generator, along with data-consistency as in Appendix H. Architecture and training details are given in Appendix I.

Competitors.We compare our cGAN to the Adler and Oktem's cGAN [8], Ohayon et al.'s cGAN [23], Jalal et al.'s fastMRI Langevin approach [19], and Sriram et al.'s E2E-VarNet [37]. The cGAN from [8] uses generator loss \(\beta_{\mathsf{adv}}\mathcal{L}_{\mathsf{adv}}^{\mathsf{adler}}(\bm{\theta}, \bm{\phi})\) and discriminator loss \(-\mathcal{L}_{\mathsf{adv}}^{\mathsf{adler}}(\bm{\theta},\bm{\phi})+\alpha_{1} \mathcal{L}_{\mathsf{gp}}(\bm{\phi})+\alpha_{2}\mathcal{L}_{\mathsf{drift}}( \bm{\phi})\), while the cGAN from [23] uses generator loss \(\beta_{\mathsf{adv}}\mathcal{L}_{\mathsf{adv}}(\bm{\theta},\bm{\phi})+ \mathcal{L}_{2,P}(\bm{\theta})\) and discriminator loss \(-\mathcal{L}_{\mathsf{adv}}(\bm{\theta},\bm{\phi})+\alpha_{1}\mathcal{L}_{ \mathsf{gp}}(\bm{\phi})+\alpha_{2}\mathcal{L}_{\mathsf{drift}}(\bm{\phi})\). Each used the value of \(\beta_{\mathsf{adv}}\) specified in the original paper. All cGANs used the same generator and discriminator architectures, except that [8] used extra discriminator input channels to facilitate the 3-input loss (5). For the fastMRI Langevin approach [19], we did not modify the authors' implementation in [38] except to use the GRO sampling mask. For the E2E-VarNet [37], we use the same training procedure and hyperparameters outlined in [19] except that we use the GRO sampling mask.

Testing.To evaluate performance, we converted the multicoil outputs \(\widehat{\bm{x}}_{i}\) to complex-valued images using SENSE-based coil combining [39] with ESPIRiT-estimated [40] coil sensitivity maps, as described in Appendix G. Magnitude images were used to compute performance metrics.

Results.Table 1 shows CFID, FID \(\underline{\mathsf{APSD}}\triangleq(\frac{1}{NP}\sum_{i=1}^{P}\|\widehat{ \bm{x}}_{(p_{\mathrm{f}})}-\widehat{\bm{x}}_{i}\|^{2})^{1/2}\), and 4-sample generation time at \(R\in\{4,8\}\). (C)FID was computed using VGG-16 (not Inception-v3) to better align with radiologists' perceptions [41]. As previously described, the Langevin method was evaluated using only 72 test images. Because CFID is biased at small sample sizes [22], we re-evaluated the other methods using all \(2\,376\) test images, and again using all \(14\,576\) training and test images. Table 1 shows that our approach gave significantly better CFID and FID than the competitors. Also, the APSD of Ohayon et al.'s cGAN was an order-of-magnitude smaller than the others, indicating mode collapse. The cGANs generated samples \(3\,800\) times faster than the Langevin approach from [19].

Tables 2 and 3 show PSNR, SSIM, LPIPS [42], and DISTS [43] for the \(P\)-sample average \(\widehat{\bm{x}}_{{}_{(P)}}\) at \(P\in\{1,2,4,8,16,32\}\) and \(R\in\{4,8\}\), respectively. While the E2E-VarNet achieves the best PSNR at \(R\in\{4,8\}\) and the best SSIM at \(R=4\), the proposed cGAN achieves the best LPIPS and DISTS performances at \(R\in\{4,8\}\) when \(P=2\) and the best SSIM at \(R=8\) when \(P=8\). The \(P\) dependence can be explained by the perception-distortion tradeoff [2]: as \(P\) increases, \(\widehat{\bm{x}}_{{}_{(P)}}\) transitions from better perceptual quality to lower \(\ell_{2}\) distortion. PSNR favors \(P\to\infty\) (e.g., \(\ell_{2}\) optimality) while the other metrics favor particular combinations of perceptual quality and distortion. The plots in Appendices K.1 and K.2 show zoomed-in versions of \(\widehat{\bm{x}}_{{}_{(P)}}\) that visually demonstrate the perception-distortion tradeoff at \(P\in\{1,2,4,32\}\): smaller \(P\) yield sharper images with more variability from the ground truth, while larger \(P\) yield smoother reconstructions.

Figure 3 shows zoomed versions of two posterior samples \(\widehat{\bm{x}}_{i}\), as well as \(\widehat{\bm{x}}_{{}_{(P)}}\), at \(P=32\) and \(R=8\). The posterior samples show meaningful variations for the proposed method, essentially no variation for Ohayon et al.'s cGAN, and vertical or horizontal reconstruction artifacts for Adler & Oktem's cGAN and the Langevin method, respectively. The \(\widehat{\bm{x}}_{{}_{(P)}}\) plots show that these artifacts are mostly suppressed by sample averaging with large \(P\).

Figure 4 shows examples of \(\widehat{\bm{x}}_{{}_{(P)}}\), along with the corresponding pixel-wise absolute errors \(|\widehat{\bm{x}}_{{}_{(P)}}-\bm{x}|\) and pixel-wise SD images \((\frac{1}{P}\sum_{i=1}^{P}(\widehat{\bm{x}}_{{}_{(P)}}-\widehat{\bm{x}}_{i})^ {2})^{1/2}\), for \(P=32\) and \(R=8\). The absolute-error image for the Langevin technique looks more diffuse than those of the other methods in the brain region. The fact that it is brighter in the air region (i.e., near the edges) is a consequence of minor differences in sensitivity-map estimation. The pixel-wise SD images show a lack of variability for the E2E-VarNet, which does not generate posterior samples, as well as Ohayon et al.'s cGAN, due to mode collapse. The Langevin pixel-wise SD images show localized hot-spots that appear to be reconstruction artifacts.

Appendices K.1 and K.2 show other example MRI recoveries with zoomed pixel-wise SD images at \(R=4\) and \(R=8\), respectively. Notably, Figures K.10 and K.11 show strong hallucinations for Langevin recovery at \(R=8\), as highlighted by the red arrows.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multicolumn{11}{c}{PSNR\(\uparrow\)} & \multicolumn{6}{c}{SSIM\(\uparrow\)} \\ \cline{2-11} Model & \(P\!=\!1\) & \(P\!=\!2\) & \(P\!=\!4\) & \(P\!=\!8\) & \(P\!=\!16\) & \(P\!=\!32\) & \(P\!=\!1\) & \(P\!=\!2\) & \(P\!=\!4\) & \(P\!=\!8\) & \(P\!=\!16\) & \(P\!=\!32\) \\ \hline E2E-VarNet (Sriram et al. [37]) & **39.93** & - & - & - & - & - & **0.9641** & - & - & - & - & - \\ Langgina (Jalal et al. [19]) & 36.04 & 37.02 & 37.65 & 37.99 & 38.17 & 38.27 & 0.8989 & 0.913 & 0.9218 & 0.9260 & 0.9281 & 0.9292 \\ eGAN (Adler \& Oktem) [8] & 35.63 & 36.64 & 37.24 & 37.56 & 37.73 & 37.82 & 0.8936 & 0.945 & 0.9478 & 0.9480 & 0.9477 & 0.9473 \\ eGAN (Ohayon et al. [23]) & 39.44 & 39.46 & 39.46 & 39.47 & 39.47 & 39.47 & 0.9585 & 0.9546 & 0.9539 & 0.9535 & 0.9533 \\ cGAN (Ours) & 36.96 & 38.14 & 38.84 & 39.24 & 39.44 & 39.55 & 0.9440 & 0.9526 & 0.9544 & 0.9524 & 0.9537 & 0.9533 \\  & & LPIPS\(\uparrow\) & & & & & & & & & & & \\ \cline{2-11} Model & \(P\!=\!1\) & \(P\!=\!2\) & \(P\!=\!4\) & \(P\!=\!8\) & \(P\!=\!16\) & \(P\!=\!32\) & \(P\!=\!1\) & \(P\!=\!2\) & \(P\!=\!4\) & \(P\!=\!8\) & \(P\!=\!16\) & \(P\!=\!32\) \\ \hline E2E-VarNet (Sriram et al. [37]) & 0.0316 & - & - & - & - & - & - & 0.0859 & - & - & - & - & - \\ Langgina (Jalal et al. [19]) & 0.0545 & 0.0934 & 0.0336 & 0.0320 & 0.0317 & 0.0316 & 0.1116 & 0.0921 & 0.0828 & 0.0793 & 0.0781 & 0.0777 \\ eGAN (Adler \& Oktem) [8] & 0.0285 & 0.0255 & 0.0273 & 0.0298 & 0.0316 & 0.0237 & 0.0972 & 0.0857 & 0.0878 & 0.0930 & 0.0967 & 0.0990 \\ eGAN (Ohayon et al. [23]) & 0.0245 & 0.0247 & 0.0248 & 0.0249 & 0.0249 & 0.0249 & 0.0767 & 0.0790 & 0.0801 & 0.0807 & 0.0810 & 0.0811 \\ eGAN (Ours) & 0.0175 & **0.0164** & 0.0188 & 0.0216 & 0.0235 & 0.0245 & **0.0546** & 0.0563 & 0.0667 & 0.0755 & 0.0809 & 0.0837 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Average PSNR, SSIM, LPIPS, and DISTS of \(\widehat{\bm{x}}_{{}_{(P)}}\) versus \(P\) for \(R=4\) MRI

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multicolumn{11}{c}{PSNR\(\uparrow\)} & \multicolumn{6}{c}{SSIM\(\uparrow\)} \\ \cline{2-11} Model & \(P\!=\!1\) & \(P\!=\!2\) & \(P\!=\!4\) & \(P\!=\!8\) & \(P\!=\!16\) & \(P\!=\!32\) & \(P\!=\!1\) & \(P\!=\!2\) & \(P\!=\!4\) & \(P\!=\!8\) & \(P\!=\!16\) & \(P\!=\!32\) \\ \hline E2E-VarNet (Sriram et al. [37]) & **39.93** & - & - & - & - & - & **0.9641** & - & - & - & - & - \\ Langgina (Jalal et al. [19]) & 36.04 & 37.02 & 37.65 & 37.99 & 38.17 & 38.27 & 0.8989 & 0.9138 & 0.9218 & 0.9260 & 0.9281 & 0.9292 \\ eGAN (Adler \& Oktem) [8] & 35.63 & 36.64 & 37.24 & 37.56 & 37.73 & 37.37 & 37.82 & 0.8936 & 0.945 & 0.9478 & 0.9480 & 0.9477 & 0.9473 \\ eGAN (Ohayon et al. [23]) & 39.44 & 39.46 & 39.46 & 39.47 & 39.47 & 39.47 & 39.47 & 0.9585 & 0.9546 & 0.9539 & 0.9535 & 0.953 \\ eGAN (Ours) & 36.96 & 38.14 & 38.84 & 39.24

### Inpainting experiments

In this section, our goal is to complete a large missing square in a face image.

**Data.** We used \(256\times 256\) CelebA-HQ face images [29] and a centered \(128\times 128\) missing square. We randomly split the dataset, yielding \(27\,000\) training, \(2\,000\) validation, and \(1\,000\) testing images.

**Architecture.** For our cGAN, we use the CoModGAN generator and discriminator from [9] with our proposed \(\mathcal{L}_{1,\text{SD},P_{\text{train}}}\) regularization. Unlike [9], we do not use MBSD [29] at the discriminator.

**Training/validation/testing.** We use the same general training and testing procedure described in Section 4.2, but with \(\beta_{\text{adv}}=\) 5e-3, \(n_{\text{batch}}=100\), and 110 epochs of cGAN training. Running PyTorch on a server with 4 Tesla A100 GPUs, each with 82 GB of memory, the cGAN training takes approximately 2 days. FID was evaluated on \(1\,000\) test images using \(P=32\) samples per measurement. To avoid the bias that would result from evaluating CFID on only \(1\,000\) images (see Appendix J.1), CFID was evaluated on all \(30\,000\) images with \(P=1\).

**Competitors.** We compare with the state-of-the-art CoModGAN [9] and Score-based SDE [20] approaches. For CoModGAN, we use the PyTorch implementation from [44]. CoModGAN differs from our cGAN only in its use of MBSD and lack of \(\mathcal{L}_{1,\text{SD},P_{\text{train}}}\) regularization. For Song et al.'s SDE, we use the authors' implementation from [45] with their pretrained weights and the settings they suggested for the \(256\times 256\) CelebA-HQ dataset.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline  & \multicolumn{6}{c}{PSNR!} & \multicolumn{6}{c}{SSIM!} \\ \cline{2-13} Model & \(P\)=\(1\) & \(P\)=\(2\) & \(P\)=\(4\) & \(P\)=\(8\) & \(P\)=\(16\) & \(P\)=\(32\) & \(P\)=\(1\) & \(P\)=\(2\) & \(P\)=\(4\) & \(P\)=\(8\) & \(P\)=\(16\) & \(P\)=\(32\) \\ \hline E2E-VarNet (Sriram et al. [37]) & **36.49** & - & - & - & - & - & 0.9220 & - & - & - & - & - \\ Langgino (Malal et al. [19]) & 32.17 & 32.83 & 33.55 & 33.74 & 33.83 & 33.39 & 30.8725 & 0.8919 & 0.9031 & 0.9910 & 0.9120 & 0.9317 \\ cGAN (Adler \& Göken [8]) & 31.31 & 32.31 & 32.92 & 33.26 & 33.42 & 33.51 & 0.8865 & 0.9405 & 0.9103 & 0.9111 & 0.9102 & 0.9095 \\ cGAN (Oluyap et al. [23]) & 34.89 & 34.90 & 34.90 & 34.90 & 34.91 & 34.92 & 0.9222 & 0.9217 & 0.9213 & 0.9211 & 0.9211 & 0.9210 \\ cGAN (Ours) & 32.32 & 33.67 & 34.53 & 35.01 & 35.27 & 35.42 & 0.9030 & 0.9199 & 0.9252 & **0.9257** & 0.9251 & 0.9246 \\ \hline  & \multicolumn{6}{c}{IPIPJS!} & \multicolumn{6}{c}{DIS!} \\ \cline{2-13} Model & \(P\)=\(1\) & \(P\)=\(2\) & \(P\)=\(4\) & \(P\)=\(8\) & \(P\)=\(16\) & \(P\)=\(32\) & \(P\)=\(1\) & \(P\)=\(2\) & \(P\)=\(4\) & \(P\)=\(8\) & \(P\)=\(16\) & \(P\)=\(32\) \\ \hline E2E-VarNet (Sriram et al. [37]) & 0.0575 & - & - & - & - & - & 0.1253 & - & - & - & - & - \\ Langgino (Malal et al. [19]) & 0.0769 & 0.0619 & 0.0579 & 0.0589 & 0.0611 & 0.0611 & 0.1341 & 0.1136 & 0.1086 & 0.1119 & 0.1175 & 0.1212 \\ cGAN (Adler \& Göken [8]) & 0.0698 & 0.0614 & 0.0623 & 0.0667 & 0.074 & 0.0727 & 0.1407 & 0.1262 & 0.1252 & 0.1291 & 0.1334 & 0.1361 \\ cGAN (Oluyap et al. [23]) & 0.0532 & 0.0536 & 0.0539 & 0.0540 & 0.0534 & 0.0540 & 0.1128 & 0.1143 & 0.1151 & 0.1155 & 0.1157 & 0.1158 \\ cGAN (Ours) & 0.0418 & **0.0379** & 0.0421 & 0.0476 & 0.0516 & 0.0539 & 0.0906 & **0.0877** & 0.0965 & 0.1063 & 0.1135 & 0.1177 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average PSNR, SSIM, LPIPS, and DISTS of \(\widehat{\bm{x}}_{(P)}\) versus \(P\) for \(R=8\) MRI

Figure 3: Example \(R=8\) MRI reconstructions. Arrows show meaningful variations across samples.

**Results.** Table 4 shows test CFID, FID, and 128-sample generation time. The table shows that our approach gave the best CFID and FID, and that the cGANs generated samples 13 000 times faster than the score-based method. Figure 5 shows an example of five generated samples for the three methods under test. The samples are all quite good, although a few generated by CoModGAN and the score-based technique have minor artifacts. Some samples generated by our technique show almond-shaped eyes, demonstrating fairness. Additional examples are given in Appendix K.3.

## 5 Conclusion

We propose a novel regularization framework for image-recovery cGANs that consists of supervised-\(\ell_{1}\) loss plus an appropriately weighted standard-deviation reward. For the case of an independent Gaussian posterior, we proved that our regularization yields generated samples that agree with the true-posterior samples in both mean and covariance. We also established limitations for alternatives based on supervised-\(\ell_{2}\) regularization with or without a variance reward. For practical datasets, we proposed a method to auto-tune our standard-deviation reward weight.

Experiments on parallel MRI and large-scale face inpainting showed our proposed method outperforming all cGAN and score-based competitors in CFID, which measures posterior-approximation quality, as well as other metrics like FID, PSNR, SSIM, LPIPS, and DISTS. Furthermore, it generates samples thousands of times faster than Langevin/score-based approaches.

In ongoing work, we are extending our approach so that it can be trained to handle a wide range of recovery tasks, such as MRI with a generic acceleration and sampling mask [46], or inpainting with a generic mask. We are also extending our approach to other applications, such as super-resolution, deblurring, compressive sensing, denoising, and phase retrieval.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Model & CFID\(\downarrow\) & FID\(\downarrow\) & Time (128)\(\downarrow\) \\ \hline Score-SDE (Song et al. [20]) & 5.11 & 7.92 & 48 min \\ CoModGAN (Zhao et al. [9]) & 5.29 & 8.50 & **217 ms** \\ cGAN (ours) & **4.69** & **7.45** & **217 ms** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Average results for inpainting: FID was computed from \(1\,000\) test images with \(P\!=\!32\), while CFID was computed from all 30 000 images with \(P\!=\!1\)

#### Limitations.

We acknowledge several limitations of our work. First, while our current work focuses on how to build a fast and accurate posterior sampler, it's not yet clear how to best exploit the resulting posterior samples in each given application. For example, in MRI, where the posterior distribution has the potential to help assess uncertainty in image recovery, it's still not quite clear how to best convey uncertainty information to radiologists (e.g., they may not gain much from pixel-wise SD images). More work is needed on this front. Second, we acknowledge that, because radiologists are risk-averse, more studies are needed before they will feel comfortable incorporating generative deep-learning-based methods into the clinical workflow. Third, we acknowledge that the visual quality of our \(R=8\) MRI reconstructions falls below clinical standards. Fourth, some caution is needed when interpreting our CFID, FID, and DISTS perceptual metrics because the VGG-16 backbone used to compute them was trained on ImageNet data. Although there is some evidence that the resulting DISTS metric correlates well with radiologists' perceptions [41], there is also evidence that ImageNet-trained features may discard information that is diagnostically relevant in medical imaging [47]. Thus our results need to be validated with a pathology-centric radiologist study before they can be considered relevant to clinical practice.

## Acknowledgments and Disclosure of Funding

The authors are funded in part by the National Institutes of Health under grant R01-EB029957 and the National Science Foundation under grant CCF-1955587.

## References

* [1] E. L. Lehmann and G. Casella, _Theory of point estimation_. Springer Science & Business Media, 2006.
* [2] Y. Blau and T. Michaeli, "The perception-distortion tradeoff," in _Proc. IEEE Conf. Comp. Vision Pattern Recog._, pp. 6228-6237, 2018.
* [3] C. K. Sonderby, J. Caballero, L. Theis, W. Shi, and F. Huszar, "Amortised MAP inference for image super-resolution," in _Proc. Int. Conf. on Learn. Rep._, 2017.
* [4] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, "A survey on bias and fairness in machine learning," _ACM Comput. Surveys_, vol. 54, no. 6, pp. 1-35, 2021.
* [5] T. Sanchez, I. Krawczuk, Z. Sun, and V. Cevher, "Uncertainty-driven adaptive sampling via GANs," in _Proc. Neural Inf. Process. Syst. Workshop_, 2020.
* [6] C.-H. Chang, E. Creager, A. Goldenberg, and D. Duvenaud, "Explaining image classifiers by counterfactual generation," in _Proc. Int. Conf. on Learn. Rep._, 2019.
* [7] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, "Image-to-image translation with conditional adversarial networks," in _Proc. IEEE Conf. Comp. Vision Pattern Recog._, pp. 1125-1134, 2017.
* [8] J. Adler and O. Oktem, "Deep Bayesian inversion," _arXiv:1811.05910_, 2018.

Figure 5: Example of inpainting a \(128\!\times\!128\) square on a \(256\!\times\!256\) resolution CelebA-HQ image.

* [9] S. Zhao, J. Cui, Y. Sheng, Y. Dong, X. Liang, E. I.-C. Chang, and Y. Xu, "Large scale image completion via co-modulated generative adversarial networks," in _Proc. Int. Conf. on Learn. Rep._, 2021.
* [10] H. Zhao, H. Li, S. Maurer-Stroh, and L. Cheng, "Synthesizing retinal and neuronal images with generative adversarial nets," _Med. Image Analysis_, vol. 49, 07 2018.
* [11] V. Edupuganti, M. Mardani, S. Vasanawala, and J. Pauly, "Uncertainty quantification in deep MRI reconstruction," _IEEE Trans. Med. Imag._, vol. 40, pp. 239-250, Jan. 2021.
* [12] F. Tonolini, J. Radford, A. Turpin, D. Faccio, and R. Murray-Smith, "Variational inference for computational imaging inverse problems," _J. Mach. Learn. Res._, vol. 21, no. 179, pp. 1-46, 2020.
* [13] K. Sohn, H. Lee, and X. Yan, "Learning structured output representation using deep conditional generative models," in _Proc. Neural Inf. Process. Syst. Conf._, 2015.
* [14] L. Ardizzone, C. Luth, J. Kruse, C. Rother, and U. Kothe, "Guided image generation with conditional invertible neural networks," _arXiv:1907.02392_, 2019.
* [15] C. Winkler, D. Worrall, E. Hoogeboom, and M. Welling, "Learning likelihoods with conditional normalizing flows," _arXiv preprint arXiv:1912.00042_, 2019.
* [16] H. Sun and K. L. Bouman, "Deep probabilistic imaging: Uncertainty quantification and multi-modal solution characterization for computational imaging," in _Proc. AAAI Conf. Artificial Intell._, vol. 35, pp. 2628-2637, 2021.
* [17] M. Welling and Y. W. Teh, "Bayesian learning via stochastic gradient Langevin dynamics," in _Proc. Int. Conf. Mach. Learn._, pp. 681-688, 2011.
* [18] Y. Song and S. Ermon, "Improved techniques for training score-based generative models," in _Proc. Neural Inf. Process. Syst. Conf._, 2020.
* [19] A. Jalal, M. Arvinte, G. Daras, E. Price, A. Dimakis, and J. Tamir, "Robust compressed sensing MRI with deep generative priors," in _Proc. Neural Inf. Process. Syst. Conf._, 2021.
* [20] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, "Score-based generative modeling through stochastic differential equations," in _Proc. Int. Conf. on Learn. Rep._, 2021.
* [21] Y. Song, L. Shen, L. Xing, and S. Ermon, "Solving inverse problems in medical imaging with score-based generative models," in _Proc. Int. Conf. on Learn. Rep._, 2022.
* [22] M. Soloveitchik, T. Diskin, E. Morin, and A. Wiesel, "Conditional Frechet inception distance," _arXiv:2103.11521_, 2021.
* [23] G. Ohayon, T. Adrai, G. Vaksman, M. Elad, and P. Milanfar, "High perceptual quality image denoising with a posterior sampling CGAN," in _Proc. IEEE Int. Conf. Comput. Vis. Workshops_, vol. 10, pp. 1805-1813, 2021.
* [24] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville, "Improved training of Wasserstein GANs," in _Proc. Neural Inf. Process. Syst. Conf._, p. 5769-5779, 2017.
* [25] M. Mirza and S. Osindero, "Conditional generative adversarial nets," _arXiv:1411.1784_, 2014.
* [26] E. Schonfeld, B. Schiele, and A. Khoreva, "A U-Net based discriminator for generative adversarial networks," in _Proc. IEEE Conf. Comp. Vision Pattern Recog._, pp. 8207-8216, 2020.
* [27] T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and T. Aila, "Training generative adversarial networks with limited data," in _Proc. Neural Inf. Process. Syst. Conf._, vol. 33, pp. 12104-12114, 2020.
* [28] Z. Zhao, S. Singh, H. Lee, Z. Zhang, A. Odena, and H. Zhang, "Improved consistency regularization for GANs," in _Proc. AAAI Conf. Artificial Intell._, vol. 35, pp. 11033-11041, 2021.
* [29] T. Karras, T. Aila, S. Laine, and J. Lehtinen, "Progressive growing of GANs for improved quality, stability, and variation," in _Proc. Int. Conf. on Learn. Rep._, 2018.
* [30] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, "Loss functions for image restoration with neural networks," _IEEE Trans. Comput. Imag._, vol. 3, pp. 47-57, Mar. 2017.
* [31] G. Ohayon, T. Adrai, G. Vaksman, M. Elad, and P. Milanfar, "High perceptual quality image denoising with a posterior sampling CGAN." Downloaded from https://github.com/theoad/pscgan, July 2021.
* [32] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, "GANs trained by a two time-scale update rule converge to a local Nash equilibrium," in _Proc. Neural Inf. Process. Syst. Conf._, vol. 30, 2017.
* [33] J. Zbontar, F. Knoll, A. Sriram, M. J. Muckley, M. Bruno, A. Defazio, M. Parente, K. J. Geras, J. Katsnelson, H. Chandarana, Z. Zhang, M. Drozdzal, A. Romero, M. Rabbat, P. Vincent, J. Pinkerton, D. Wang, N. Yakubova, E. Owens, C. L. Zitnick, M. P. Recht, D. K. Sodickson, and Y. W. Lui, "fastMRI: An open dataset and benchmarks for accelerated MRI," _arXiv:1811.08839_, 2018.

* [34] T. Zhang, J. M. Pauly, S. S. Vasanawala, and M. Lustig, "Coil compression for accelerated imaging with Cartesian sampling," _Magn. Reson. Med._, vol. 69, no. 2, pp. 571-582, 2013.
* [35] M. Joshi, A. Pruitt, C. Chen, Y. Liu, and R. Ahmad, "Technical report (v1.0)-pseudo-random cartesian sampling for dynamic MRI," _arXiv:2206.03630_, 2022.
* [36] O. Ronneberger, P. Fischer, and T. Brox, "U-Net: Convolutional networks for biomedical image segmentation," in _Proc. Intl. Conf. Med. Image Comput. Comput. Assist. Intervent._, pp. 234-241, 2015.
* [37] A. Sriram, J. Zbontar, T. Murrell, A. Defazio, C. L. Zitnick, N. Yakubova, F. Knoll, and P. Johnson, "End-to-end variational networks for accelerated MRI reconstruction," in _Proc. Intl. Conf. Med. Image Comput. Assist. Intervent._, pp. 64-73, 2020.
* [38] A. Jalal, M. Arvinte, G. Daras, E. Price, A. Dimakis, and J. Tamir, "csgm-mri-langevin." https://github.com/utcsilab/csgm-mri-langevin, 2021. Accessed: 2021-12-05.
* [39] K. P. Pruessmann, M. Weiger, M. B. Scheidegger, and P. Boesiger, "SENSE: Sensitivity encoding for fast MRI," _Magn. Reson. Med._, vol. 42, no. 5, pp. 952-962, 1999.
* [40] M. Uecker, P. Lai, M. J. Murphy, P. Virtue, M. Elad, J. M. Pauly, S. S. Vasanawala, and M. Lustig, "ESPIRiT-an eigenvalue approach to autocalibrating parallel MRI: Where SENSE meets GRAPPA," _Magn. Reson. Med._, vol. 71, no. 3, pp. 990-1001, 2014.
* [41] S. Kastryulin, J. Zakirov, N. Pezzotti, and D. V. Dylov, "Image quality assessment for magnetic resonance imaging," _arXiv:2203.07809_, 2022.
* [42] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, "The unreasonable effectiveness of deep features as a perceptual metric," in _Proc. IEEE Conf. Comp. Vision Pattern Recog._, pp. 586-595, 2018.
* [43] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli, "Image quality assessment: Unifying structure and texture similarity," _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 44, no. 5, pp. 2567-2581, 2020.
* [44] Y. Zeng, "co-mod-gan-pytorch." Downloaded from https://github.com/zengxianyu/co-mod-gan-pytorch, Sept. 2022.
* [45] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, "Score-based generative modeling through stochastic differential equations." Downloaded from https://github.com/yang-song/score_sde_pytorch, Oct. 2022.
* [46] M. C. Bendel, R. Ahmad, and P. Schniter, "Mask-agnostic posterior sampling MRI via conditional GANs with guided reconstruction," in _Proc. NeurIPS Workshop on Deep Inverse Problems_, 2023.
* [47] T. Kynkaanniemi, T. Karras, M. Aittala, T. Aila, and J. Lehtinen, "The role of imagenet classes in frechet inception distance," _arXiv:2203.06026_, 2022.
* [48] F. C. Leone, L. S. Nelson, and R. Nottingham, "The folded normal distribution," _Technometrics_, vol. 3, no. 4, pp. 543-550, 1961.
* [49] K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition," _arXiv:1409.1556_, 2014.
* [50] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, "Rethinking the inception architecture for computer vision," in _Proc. IEEE Conf. Comp. Vision Pattern Recog._, 2016.
* [51] F. Ong and M. Lustig, "SigPy: A python package for high performance iterative reconstruction," in _Proc. Annu. Meeting ISMRM_, vol. 4819, 2019.
* [52] D. Chen and M. E. Davies, "Deep decomposition learning for inverse imaging problems," in _Proc. European Conf. Comp. Vision_, pp. 510-526, 2020.
* [53] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in _Proc. Int. Conf. on Learn. Rep._, 2015.
* [54] P. Deora, B. Vasudeva, S. Bhattacharya, and P. M. Pradhan, "Structure preserving compressive sensing MRI reconstruction using generative adversarial networks," in _Proc. IEEE Conf. Comp. Vision Pattern Recog. Workshop_, pp. 2211-2219, June 2020.