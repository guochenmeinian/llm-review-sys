# Optimistic Active Exploration of Dynamical Systems

Bhavya Sukhija\({}^{1}\)  Lenart Treven\({}^{1}\)  Cansu Sancaktar\({}^{2}\)  Sebastian Blaes\({}^{2}\)

**Stelian Coros\({}^{1}\) Andreas Krause\({}^{1}\)**

ETH Zurich \({}^{1}\) MPI for Intelligent Systems\({}^{2}\)

{sukhijab,trevenl,scoros,krausea}@ethz.ch

{cansu.sancaktar,sebastian.blae}@tuebingen.mpg.de

###### Abstract

Reinforcement learning algorithms commonly seek to optimize policies for solving one particular task. How should we explore an unknown dynamical system such that the estimated model globally approximates the dynamics and allows us to solve multiple downstream tasks in a zero-shot manner? In this paper, we address this challenge, by developing an algorithm - OpAx- for active exploration. OpAx uses well-calibrated probabilistic models to quantify the epistemic uncertainty about the unknown dynamics. It optimistically--w.r.t. to plausible dynamics--maximizes the information gain between the unknown dynamics and state observations. We show how the resulting optimization problem can be reduced to an optimal control problem that can be solved at each episode using standard approaches. We analyze our algorithm for general models, and, in the case of Gaussian process dynamics, we give a first-of-its-kind sample complexity bound and show that the epistemic uncertainty _converges to zero_. In our experiments, we compare OpAx with other heuristic active exploration approaches on several environments. Our experiments show that OpAx is not only theoretically sound but also performs well for zero-shot planning on novel downstream tasks.

## 1 Introduction

Most reinforcement learning (RL) algorithms are designed to maximize cumulative rewards for a single task at hand. Particularly, model-based RL algorithms, such as (Chua et al., 2018; Kakade et al., 2020; Curi et al., 2020), excel in efficiently exploring the dynamical system as they direct the exploration in regions with high rewards. However, due to the directional bias, their underlying learned dynamics model fails to generalize in other areas of the state-action space. While this is sufficient if only one control task is considered, it does not scale to the setting where the system is used to perform several tasks, i.e., under the same dynamics optimized for different reward functions. As a result, when presented with a new reward function, they often need to relearn a policy from scratch, requiring many interactions with the system, or employ multi-task (Zhang and Yang, 2021) or transfer learning (Weiss et al., 2016) methods. Traditional control approaches such as trajectory optimization (Biagiotti and Melchiorri, 2008) and model-predictive control (Garcia et al., 1989) assume knowledge of the system's dynamics. They leverage the dynamics model to solve an optimal control problem for each task. Moreover, in the presence of an accurate model, important system properties such as stability and sensitivity can also be studied. Hence, knowing an accurate dynamics model bears many practical benefits. However, in many real-world settings, obtaining a model using just physics' first principles is very challenging. A promising approach is to leverage data for learning the dynamics, i.e., system identification or active learning. To this end, the key question we investigate in this work is: _how should we interact with the system to learn its dynamics efficiently?_

While active learning for regression and classification tasks is well-studied, active learning in RL is much less understood. In particular, active learning methods that yield strong theoretical and practical results, generally query data points based on information-theoretic criteria (Krause et al., 2008; Settles, 2009; Balcan et al., 2010; Hanneke et al., 2014; Chen et al., 2015). In the context of

[MISSING_PAGE_FAIL:2]

and efficient w.r.t. rate of convergence of \(\bm{\mu}_{n}\) to \(\bm{f}^{*}\).

To devise such an algorithm, we take inspiration from Bayesian experiment design (Chaloner and Verdinelli, 1995). In the Bayesian setting, given a prior over \(\bm{f}^{*}\), a natural objective for active exploration is the mutual information (Lindley, 1956) between \(\bm{f}^{*}\) and observations \(\bm{y}_{\mathcal{D}_{n}}\).

**Definition 1** (Mutual Information, Cover and Thomas (2006)).: _The mutual information between \(\bm{f}^{*}\) and its noisy measurements \(\bm{y}_{\mathcal{D}_{n}}\) for points in \(\mathcal{D}_{n}\), where \(\bm{y}_{\mathcal{D}_{n}}\) is the concatenation of \((\bm{y}_{\mathcal{D}_{n},i})_{i<T}\) is defined as,_

\[F(\mathcal{D}_{n}):=I\left(\bm{f}^{*};\bm{y}_{\mathcal{D}_{n}}\right)=H\left( \bm{y}_{\mathcal{D}_{n}}\right)-H\left(\bm{y}_{\mathcal{D}_{n}}\mid\bm{f}^{*} \right),\] (3)

_where \(H\) is the Shannon differential entropy._

The mutual information quantifies the reduction in entropy of \(\bm{f}^{*}\) conditioned on the observations. Hence, maximizing the mutual information w.r.t. the dataset \(\mathcal{D}_{n}\) leads to the maximal entropy reduction of our prior. Accordingly, a natural objective for active exploration in RL can be the mutual information between \(\bm{f}^{*}\) and the collected transitions over a budget of \(N\) episodes, i.e., \(I\left(\bm{f}^{*};\bm{y}_{\mathcal{D}_{1:N}}\right)\). This requires maximizing the mutual information over a sequence of policies, which is a challenging planning problem even in settings where the dynamics are known (Mutny et al., 2023). A common approach is to greedily pick a policy that maximizes the information gain conditioned on the previous observations at each episode:

\[\max_{\bm{\pi}\in\Pi}\mathbb{E}_{\tau^{\bm{\pi}}}\left[I\left(\bm{f}^{*}_{\tau ^{\bm{\pi}}};\bm{y}_{\tau^{\bm{\pi}}}\mid\mathcal{D}_{1:n-1}\right)\right].\] (4)

Here \(\bm{f}^{*}_{\tau^{\bm{\pi}}}=\left(\bm{f}^{*}(\bm{z}_{n,0}),\ldots,\bm{f}^{*}( \bm{z}_{n,T-1})\right)\), \(\bm{y}_{\tau^{\bm{\pi}}}=\left(\bm{y}_{n,0},\ldots,\bm{y}_{n,T-1}\right)\), \(\tau^{\bm{\pi}}\) is the trajectory under the policy \(\bm{\pi}\), and the expectation is taken w.r.t. the process noise \(\bm{w}\).

Interpretation in frequentist settingWhile information gain is Bayesian in nature (requires a prior over \(\bm{f}^{*}\)), it also has a frequentist interpretation. In particular, later in Section 3 we relate it to the epistemic uncertainty of the learned model. Accordingly, while this notion of information gain stems from Bayesian literature, we can use it to motivate our objective in both Bayesian and frequentist settings.

### Assumptions

In this work, we learn a probabilistic model of the function \(\bm{f}^{*}\) from data. Moreover, at each episode \(n\), we learn the mean estimator \(\bm{\mu}_{n}(\bm{x},\bm{u})\) and the epistemic uncertainty \(\bm{\sigma}_{n}(\bm{x},\bm{u})\), which quantifies our uncertainty on the mean prediction. To this end, we use Bayesian models such as Gaussian processes (GPs, Rasmussen and Williams, 2005) or Bayesian neural networks (BNNs, Wang and Yeung, 2020). More generally, we assume our model is _well-calibrated_:

**Definition 2** (All-time calibrated statistical model of \(\bm{f}^{*}\), Rothfuss et al. (2023)).: _Let, \(\bm{z}=(\bm{x},\bm{u})\) and \(\mathcal{Z}:=\mathcal{X}\times\mathcal{U}\). An all-time calibrated statistical model of the function \(\bm{f}^{*}\) is a sequence \(\left(\bm{\mu}_{n},\bm{\sigma}_{n},\beta_{n}(\delta)\right)_{n\geq 0}\), such that_

\[\Pr\left(\forall\bm{z}\in\mathcal{Z},\forall l\in\left\{1,\ldots,d_{x}\right\},\forall n\in\mathbb{N}:\left|\mu_{n,l}(\bm{z})-f_{l}(\bm{z})\right|\leq\beta_{ n}(\delta)\sigma_{n,l}(\bm{z}))\geq 1-\delta\]

_Here \(\mu_{n,l}\) and \(\sigma_{n,l}\) are the l-th element in the vector valued functions \(\bm{\mu}_{n}\) and \(\bm{\sigma}_{n}\) respectively. The scalar function, \(\beta_{n}(\delta)\in\mathbb{R}_{\geq 0}\) quantifies the width of the \(1-\delta\) confidence intervals. We assume w.l.o.g. that \(\beta_{n}\) monotonically increases with \(n\), and that \(\sigma_{n,l}(\bm{z})\leq\sigma_{\max}\) for all \(\bm{z}\in\mathcal{Z}\), \(n\geq 0\), and \(l\in\left\{1,\ldots,d_{x}\right\}\)._

**Assumption 1** (Well calibration assumption).: _Our learned model is an all-time-calibrated statistical model of \(\bm{f}^{*}\), i.e., there exists a sequence of \(\left(\beta_{n}(\delta)\right)_{n\geq 0}\) such that our model satisfies the well-calibration condition, c.f., Definition 2._

This is a natural assumption on our modeling. It states that we can make a mean prediction and also quantify how far it is off from the true one with high probability. A GP model satisfies this requirement for a very rich class of functions, c.f., Lemma 3. For BNNs, calibration methods (Kuleshov et al., 2018) are often used and perform very well in practice. Next, we make a simple continuity assumption on our function \(\bm{f}^{*}\).

**Assumption 2** (Lipschitz Continuity).: _The dynamics model \(\bm{f}^{*}\) and our epistemic uncertainty prediction \(\bm{\sigma}_{n}\) are \(L_{\bm{f}}\) and \(L_{\bm{\sigma}}\) Lipschitz continuous, respectively. Moreover, we define \(\Pi\) to be the policy class of \(L_{\bm{\pi}}\) Lipschitz continuous functions._The Lipschitz continuity assumption on \(\bm{f}^{*}\) is quite common in control theory (Khalil, 2015) and learning literature (Curi et al., 2020; Pasztor et al., 2021; Sussex et al., 2023). Furthermore, the Lipschitz continuity of \(\bm{\sigma}_{n}\) also holds for GPs with common kernels such as the linear or radial basis function (RBF) kernel (Rothfuss et al., 2023).

Finally, we reiterate the assumption of the system's stochasticity.

**Assumption 3** (Process noise distribution).: _The process noise is i.i.d. Gaussian with variance \(\sigma^{2}\), i.e., \(\bm{w}_{k}\overset{i.i.d}{\sim}\mathcal{N}(\bm{0},\sigma^{2}\bm{I})\)._

We focus on the setting where \(\bm{w}\) is homoscedastic for simplicity. However, our framework can also be applied to the more general heteroscedastic and sub-Gaussian case (c.f., Theorem 2).

## 3 Optimistic Active Exploration

In this section, we propose our _optimistic active exploration_ (OpAx) algorithm. The algorithm consists of two main contributions: _(i)_ First we reformulate the objective in Equation (4) to a simple optimal control problem, which suggests policies that visit states with high epistemic uncertainty. _(ii)_ We leverage the optimistic planner introduced by Curi et al. (2020) to efficiently plan a policy under _unknown_ dynamics. Moreover, we show that the optimistic planner is crucial in giving theoretical guarantees for the algorithm.

### Optimal Exploration Objective

The objective in Equation (4) is still difficult and expensive to solve in general. However, since in this work, we consider Gaussian noise, c.f., Assumption 3, we can simplify this further.

**Lemma 1** (Information gain is upper bounded by sum of epistemic uncertainties).: _Let \(\bm{y}=\bm{f}^{*}(\bm{z})+\bm{w}\), with \(\bm{w}\sim\mathcal{N}(0,\sigma^{2}\bm{I})\) and let \(\bm{\sigma}_{n-1}\) be the epistemic uncertainty after episode \(n-1\). Then the following holds for all \(n\geq 1\) and dataset \(\mathcal{D}_{1:n-1}\),_

\[I\left(\bm{f}^{*}_{\bm{\tau}^{*}};\bm{y}_{\bm{\tau}^{*}}\mid\mathcal{D}_{1:n-1 }\right)\leq\frac{1}{2}\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}}\log\left(1+\frac{ \sigma_{n-1,j}^{2}(\bm{z}_{t})}{\sigma^{2}}\right).\] (5)

We prove Lemma 1 in Appendix A. The information gain is non-negative (Cover and Thomas, 2006). Therefore, if the right-hand side of Equation (5) goes to zero, the left-hand side goes to zero as well. Lemma 1 relates the information gain to the model epistemic uncertainty. Therefore, it gives a tractable objective that also has a frequentist interpretation - collect points with the highest epistemic uncertainty. We can use it to plan a trajectory at each episode \(n\), by solving the following optimal control problem:

\[\bm{\pi}_{n}^{*}=\operatorname*{argmax}_{\bm{\pi}\in\Pi}J_{n}(\bm{\pi})= \operatorname*{argmax}_{\bm{\pi}\in\Pi}\ \mathbb{E}_{\bm{\tau}^{\bm{\pi}}}\left[\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}}\log \left(1+\frac{\sigma_{n-1,j}^{2}(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))}{\sigma^{2} }\right)\right],\] (6)

\[\bm{x}_{t+1}=\bm{f}^{*}(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))+\bm{w}_{t}.\]

The problem in Equation (6) is closely related to previous literature in active exploration for RL. For instance, some works consider different geometries such as the sum of epistemic uncertainties (Pathak et al. (2019); Sekar et al. (2020), c.f., appendix C for more detail).

### Optimistic Planner

The optimal control problem in Equation (6) requires knowledge of the dynamics \(\bm{f}^{*}\) for planning, however, \(\bm{f}^{*}\) is unknown. A common choice is to use the mean estimator \(\bm{\mu}_{n-1}\) in Equation (6) instead of \(\bm{f}^{*}\) for planning (Buisson-Fenet et al., 2020). However, in general, using the mean estimator is susceptible to model biases (Chua et al., 2018) and is provably optimal only in the case of linear systems (Simchowitz and Foster, 2020). To this end, we propose using an optimistic planner, as suggested in Curi et al. (2020), instead. Accordingly, given the mean estimator \(\bm{\mu}_{n-1}\) and the epistemic uncertainty \(\bm{\sigma}_{n-1}\), we solve the following optimal control problem

\[\bm{\pi}_{n},\bm{\eta}_{n}=\operatorname*{argmax}_{\bm{\pi}\in\Pi,\bm{\eta} \in\mathbb{Z}}J_{n}(\bm{\pi},\bm{\eta})=\operatorname*{argmax}_{\bm{\pi}\in \Pi,\bm{\eta}\in\mathbb{Z}}\mathbb{E}_{\bm{\tau}^{\bm{\pi},\bm{\eta}}}\left[ \sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}}\log\left(1+\frac{\sigma_{n-1,j}^{2}(\hat{ \bm{x}}_{t},\bm{\pi}(\hat{\bm{x}}_{t}))}{\sigma^{2}}\right)\right],\] (7)

\[\hat{\bm{x}}_{t+1}=\bm{\mu}_{n-1}(\hat{\bm{x}}_{t},\bm{\pi}(\hat{\bm{x}}_{t})) +\beta_{n-1}(\delta)\bm{\sigma}_{n-1}(\hat{\bm{x}}_{t},\bm{\pi}(\hat{\bm{x}}_{ t}))\bm{\eta}(\hat{\bm{x}}_{t})+\bm{w}_{t},\]

**OPAx: Optimistic Active Exploration**

``` **Init:** Aleatoric uncertainty \(\sigma\), Probability \(\delta\), Statistical model \((\bm{\mu}_{0},\bm{\sigma}_{0},\beta_{0}(\delta))\) for episode \(n=1,\ldots,N\)do \[\bm{\pi}_{n}=\operatorname*{argmax}_{\bm{\pi}\in\Omega}\max_{\bm{ \eta}\in\Xi}\mathbb{E}\left[\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}}\log\left(1+ \frac{\sigma_{n-1,j}^{2}(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))}{\sigma^{2}}\right)\right] \quad\blacktriangleright\text{Prepare policy}\] \[\mathcal{D}_{n}\leftarrow\textsc{Rollout}(\bm{\pi}_{n}) \quad\blacktriangleright\text{Collect measurements}\] \[\text{Update }(\bm{\mu}_{n},\bm{\sigma}_{n},\beta_{n}(\delta)) \leftarrow\mathcal{D}_{1:n} \quad\blacktriangleright\text{Update model}\]

where \(\Xi\) is the space of policies \(\bm{\eta}:\mathcal{X}\rightarrow[-1,1]^{d_{x}}\). Therefore, we use the policy \(\bm{\eta}\) to "hallucinate" (pick) transitions that give us the most information. Overall, the resulting formulation corresponds to a simple optimal control problem with a larger action space, i.e., we increase the action space by another \(d_{x}\) dimension. A natural consequence of Assumption 1 is that \(J_{n}(\bm{\pi}_{n}^{*})\leq J_{n}(\bm{\pi}_{n},\bm{\eta}_{n})\) with high probability (c.f., Corollary 1 in Appendix A). That is by solving Equation (7), we get an optimistic estimate on Equation (6). Intuitively, the policy \(\bm{\pi}_{n}\) that OpAx suggests, behaves optimistically with respect to the information gain at each episode.

## 4 Theoretical Results

We theoretically analyze the convergence properties of OpAx. We first study the regret of planning under unknown dynamics. Specifically, since we cannot evaluate the optimal exploration policy from eq. (6) and use the optimistic one, i.e., eq. (7) instead, we incur a regret. We show that due to the optimism in the face of uncertainty paradigm, we can give sample complexity bounds for the Bayesian and frequentist settings. All the proofs are presented in Appendix A.

**Lemma 2** (Regret of optimistic planning under unknown dynamics).: _Let Assumption 1 hold. Furthermore, define \(J_{n,k}(\bm{\pi}_{n},\bm{\eta}_{n},\bm{x})\) as_

\[J_{n,k}(\bm{\pi}_{n},\bm{\eta}_{n},\bm{x}) =\mathbb{E}_{\bm{\pi}^{*}\bm{\pi}_{n},\bm{\eta}_{n}}\left[\sum_{t =k}^{T-1}\sum_{j=1}^{d_{x}}\log\left(1+\frac{\sigma_{n-1,j}^{2}(\hat{\bm{x}}_{ t},\bm{\pi}_{n}(\hat{\bm{x}}_{t}))}{\sigma^{2}}\right)\right],\] \[\text{s.t. }\hat{\bm{x}}_{t+1}=\bm{\mu}_{n-1}(\hat{\bm{x}}_{t},\bm{ \pi}_{n}(\hat{\bm{x}}_{t}))+\beta_{n-1}(\delta)\bm{\sigma}_{n-1}(\hat{\bm{x}}_ {t},\bm{\pi}_{n}(\hat{\bm{x}}_{t}))\bm{\eta}_{n}(\hat{\bm{x}}_{t})+\bm{w}_{t}\] \[\text{and }\hat{\bm{x}}_{0}=\bm{x}.\]

_Then, for all \(n\geq 1\), with probability at least \(1-\delta\),_

\[J_{n}(\bm{\pi}_{n}^{*})-J_{n}(\bm{\pi}_{n})\leq\sum_{t=0}^{T-1} \mathbb{E}_{\bm{\tau}^{*}\bm{\pi}_{n}}\left[J_{n,t+1}(\bm{\pi}_{n},\bm{\eta}_{ n},\bm{x}_{t+1}^{\prime})-J_{n,t+1}(\bm{\pi}_{n},\bm{\eta}_{n},\bm{x}_{t+1}) \right],\] \[\text{with }\bm{x}_{t+1}=\bm{f}^{*}(\bm{x}_{t},\bm{\pi}_{n}(\bm{x}_{t}))+ \bm{w}_{t},\] \[\text{and }\bm{x}_{t+1}^{\prime}=\bm{\mu}_{n-1}(\bm{x}_{t},\bm{\pi}_{n}( \bm{x}_{t}))+\beta_{n-1}(\delta)\bm{\sigma}_{n-1}(\bm{x}_{t},\bm{\pi}_{n}(\bm{x }_{t}))\bm{\eta}_{n}(\bm{x}_{t})+\bm{w}_{t}.\]

Lemma 2 gives a bound on the regret of planning optimistically under unknown dynamics. The regret is proportional to the difference in the expected returns for \(\bm{x}_{t}\) and \(\bm{x}_{t}^{\prime}\). Note, \(\left\lVert\bm{x}_{t}-\bm{x}_{t}^{\prime}\right\rVert\propto\beta_{n}(\delta) \bm{\sigma}_{n-1}(\bm{x}_{t-1},\bm{\pi}_{n}(\bm{x}_{t-1}))\). Hence, when we have low uncertainty in our predictions, planning optimistically suffers smaller regret. Next, we leverage Lemma 2 to give a sample complexity bound for the Bayesian and frequentist setting.

Bayesian SettingWe start by introducing a measure of model complexity as defined by Curi et al. (2020).

\[\mathcal{MC}_{N}(\bm{f}^{*}):=\max_{\mathcal{D}_{1},\ldots,\mathcal{D}_{N} \subset\mathcal{Z}\times\mathcal{X}}\sum_{n=1}^{N}\sum_{\bm{z}\in\mathcal{D}_ {n}}\left\lVert\bm{\sigma}_{n-1}(\bm{z})\right\rVert_{2}^{2}.\] (8)

This complexity measure captures the difficulty of learning \(\bm{f}^{*}\) given \(N\) trajectories. Mainly, the more complicated \(\bm{f}^{*}\), the larger the epistemic uncertainties \(\bm{\sigma}_{n}\), and in turn, the larger corresponding \(\mathcal{MC}_{N}(\bm{f}^{*})\). Moreover, if the model complexity measure is sublinear in \(N\), i.e. \(\mathcal{MC}_{N}(\bm{f}^{*})/N\to 0\) for \(N\rightarrow\infty\), then the epistemic uncertainties also converge to zero in the limit, which impliesconvergence to the true function \(\bm{f}^{*}\). We present our main theoretical result, in terms of the model complexity measure.

**Theorem 1**.: _Let Assumption 1 and 3 hold. Then, for all \(N\geq 1\), with probability at least \(1-\delta\),_

\[\mathbb{E}_{\mathcal{D}_{1:N-1}}\left[\max_{\bm{\pi}\in\Pi}\mathbb{E}_{\bm{\tau }^{\bm{\pi}}}\left[I\left(\bm{f}^{*}_{\bm{\tau}^{\bm{\pi}}};\bm{y}_{\bm{\tau}^{ \bm{\pi}}}\mid\mathcal{D}_{1:N-1}\right)\right]\right]\leq\mathcal{O}\left( \beta_{N}T^{\nicefrac{{3}}{{2}}}\sqrt{\frac{\mathcal{MC}_{N}(\bm{f}^{*})}{N}}\right)\] (9)

Theorem 1 relates the maximum expected information gain at iteration \(N\) to the model complexity of our problem. For deterministic systems, the expectation w.r.t. \(\bm{\tau}^{\bm{\pi}}\) is redundant. The bound in Equation (9) depends on the Lipschitz constants, planning horizon, and dimensionality of the state space (captured in \(\beta_{N}\) and \(\mathcal{MC}_{N}(\bm{f}^{*})\)). If the right-hand side is monotonically decreasing with \(N\), Theorem 1 guarantees that the information gain at episode \(N\) is also shrinking with \(N\), and the algorithm is converging. Empirically, Pathak et al. (2019) show that the epistemic uncertainties go to zero as more data is acquired. In general, deriving a worst-case bound on the model complexity is a challenging and active open research problem. However, in the case of GPs, convergence results can be shown for a very rich class of functions. We show this in the following for the frequentist setting.

Frequentist Setting with Gaussian Process ModelsWe extend our analysis to the frequentist kernelized setting, where \(\bm{f}^{*}\) resides in a Reproducing Kernel Hilbert Space (RKHS) of vector-valued functions.

**Assumption 4**.: _We assume that the functions \(f^{*}_{j}\), \(j\in\{1,\ldots,d_{x}\}\) lie in a RKHS with kernel \(k\) and have a bounded norm \(B\), that is \(\bm{f}^{*}\in\mathcal{H}^{d_{x}}_{k,B}\), with \(\mathcal{H}^{d_{x}}_{k,B}=\{\bm{f}\mid\|f_{j}\|_{k}\leq B,j=1,\ldots,d_{x}\}\)._

In this setting, we model the posterior mean and epistemic uncertainty of the vector-valued function \(\bm{f}^{*}\) with \(\bm{\mu}_{n}(\bm{z})=[\mu_{n,j}(\bm{z})]_{j\leq d_{x}}\), and \(\bm{\sigma}_{n}(\bm{z})=[\sigma_{n,j}(\bm{z})]_{j\leq d_{x}}\), where,

\[\mu_{n,j}(\bm{z}) =\bm{k}_{n}^{\top}(\bm{z})(\bm{K}_{n}+\sigma^{2}\bm{I})^{-1}\bm{y }_{1:n}^{j},\] (10) \[\sigma_{n,j}^{2}(\bm{z}) =k(\bm{x},\bm{x})-\bm{k}_{n}^{\top}(\bm{z})(\bm{K}_{n}+\sigma^{ 2}\bm{I})^{-1}\bm{k}_{n}(\bm{x}),\]

Here, \(\bm{y}_{1:n}^{j}\) corresponds to the noisy measurements of \(f^{*}_{j}\), i.e., the observed next state from the transitions dataset \(\mathcal{D}_{1:n}\), \(\bm{k}_{n}=[k(\bm{z},\bm{z}_{i})]_{i\leq nT},\bm{z}_{i}\in\mathcal{D}_{1:n}\), and \(\bm{K}_{n}=[k(\bm{z}_{i},\bm{z}_{l})]_{i,l\leq nT},\bm{z}_{i},\bm{z}_{l}\in \mathcal{D}_{1:n}\) is the data kernel matrix. It is known that if \(\bm{f}^{*}\) satisfies Assumption 4, then Equation (10) yields well-calibrated confidence intervals, i.e., that Assumption 1 is satisfied.

**Lemma 3** (Well calibrated confidence intervals for RKHS, Rothfuss et al. (2023)).: _Let \(\bm{f}^{*}\in\mathcal{H}^{d_{x}}_{k,B}\). Suppose \(\bm{\mu}_{n}\) and \(\bm{\sigma}_{n}\) are the posterior mean and variance of a GP with kernel \(k\), c.f., Equation (10). There exists \(\beta_{n}(\delta)\), for which the tuple \((\bm{\mu}_{n},\bm{\sigma}_{n},\beta_{n}(\delta))\) satisfies Assumption 1 w.r.t. function \(\bm{f}^{*}\)._

Theorem 2 presents our convergence guarantee for the kernelized case to the \(T\)-step reachability set \(\mathcal{R}\) for the policy class \(\pi\in\Pi\). In particular, \(\mathcal{R}\) is defined as

\[\mathcal{R}=\{\bm{z}\in\mathcal{Z}\mid\exists(\bm{\pi}\in\Pi,t\leq T),\ \text{s.t.},p(\bm{z}_{t}=\bm{z}|\bm{\pi},\bm{f}^{*})>0\}\]

There are two key differences from Theorem 1; (_i_) we can derive an upper bound on the epistemic uncertainties \(\bm{\sigma}_{n}\), and (_ii_) we can bound the model complexity \(\mathcal{MC}_{N}(\bm{f}^{*})\), with the _maximum information gain_ of kernel \(k\) introduced by Srinivas et al. (2012), defined as

\[\gamma_{N}(k)=\max_{\mathcal{D}_{1},\ldots,\mathcal{D}_{N}:|\mathcal{D}_{n}| \leq T}\frac{1}{2}\log\det(\bm{I}+\sigma^{-2}\bm{K}_{N}).\]

**Theorem 2**.: _Let Assumption 3 and 4 hold, Then, for all \(N\geq 1\), with probability at least \(1-\delta\),_

\[\max_{\bm{\pi}\in\Pi}\mathbb{E}_{\bm{\tau}^{\bm{\pi}}}\left[\max_{\bm{z}\in \bm{\tau}^{\bm{\pi}}}\sum_{j=1}^{d_{x}}\frac{1}{2}\sigma_{N,j}^{2}(\bm{z}) \right]\leq\mathcal{O}\left(\beta_{N}T^{\nicefrac{{3}}{{2}}}\sqrt{\frac{ \gamma_{N}(k)}{N}}\right).\] (11)

_If we relax noise Assumption 3 to \(\sigma\)-sub Gaussian. Then, if Assumption 2 holds, we have for all \(N\geq 1\), with probability at least \(1-\delta\),_

\[\max_{\bm{\pi}\in\Pi}\mathbb{E}_{\bm{\tau}^{\bm{\pi}}}\left[\max_{\bm{z}\in \bm{\tau}^{\bm{\pi}}}\sum_{j=1}^{d_{x}}\frac{1}{2}\sigma_{N,j}^{2}(\bm{z}) \right]\leq\mathcal{O}\left(\beta_{N}^{T}T^{\nicefrac{{3}}{{2}}}\sqrt{\frac{ \gamma_{N}(k)}{N}}\right).\] (12)_Moreover, if \(\gamma_{N}(k)=\mathcal{O}\left(\text{polylog}(N)\right)\), then for all \(\bm{z}\in\mathcal{R}\), and \(1\leq j\leq d_{x}\),_

\[\sigma_{N,j}(\bm{z})\xrightarrow{\text{a.s.}}0\text{ for }N\to\infty.\] (13)

We only state Theorem 2 for the expected epistemic uncertainty along the trajectory at iteration \(N\). For deterministic systems, the expectation is redundant and for stochastic systems, we can leverage concentration inequalities to give a bound without the expectation (see Appendix A for more detail).

For the Gaussian noise case, we obtain a tighter bound by leveraging the change of measure inequality from Kakade et al. (2020, Lemma C.2.) (c.f., Lemma 6 in Appendix A for more detail). In the more general case of sub-Gaussian noise, we cannot use the same analysis. To this end, we use the Lipschitz continuity assumptions (Assumption 2) similar to Curi et al. (2020). This results in comparing the deviation between two trajectories under the same policy and dynamics but different initial states (see Lemma 2). For many systems (even linear) this can grow exponentially in the horizon \(T\). Accordingly, we obtain a \(\beta_{N}^{T}\) term in our bound (Equation (12)). Nonetheless, for cases where the RKHS is of a kernel with maximum information gain \(\gamma_{N}(k)=\mathcal{O}\left(\text{polylog}(N)\right)\), we can give sample complexity bounds and an almost sure convergence result in the reachable set \(\mathcal{R}\) (Equation (13)). Kernels such as the RBF kernel or the linear kernel (kernel with a finite-dimensional feature map \(\phi(x)\)) have maximum information gain which grows polylogarithmically with \(n\)(Vakili et al. (2021)). Therefore, our convergence guarantees hold for a very rich class of functions. The exponential dependence of our bound on \(T\) imposes the restriction on the kernel class. For the case of Gaussian noise, we can include a richer class of kernels, such as Matern.

In addition to the convergence results above, we also give guarantees on the zero-shot performance of Opax in Appendix A.5.

## 5 Experiments

We evaluate Opax on the Pendulum-v1 and MountainCar environment from the OpenAI gym benchmark suite (Brockman et al., 2016), on the Reacher, Swimmer, and Cheetah from the deep mind control suite (Tassa et al., 2018), and a high-dimensional simulated robotic manipulation task introduced by Li et al. (2020). See Appendix B for more details on the experimental setup.

BaselinesWe implement four baselines for comparisons. To show the benefit of our intrinsic reward, we compare OpAx to (_1_) a random exploration policy (Random) which randomly samples actions from the action space. As we discuss in Section 3 our choice of objective in Equation (6) is in essence similar to the one proposed by Pathak et al. (2019) and Sekar et al. (2020). Therefore, in our experiments, we compare the optimistic planner with other planning approaches. Moreover, most work on active exploration either uses the mean planner or does not specify the planner (c.f., Section 6). We use the most common planners: (2) mean (Mean-AE), and (_3_) trajectory sampling (TS-1) scheme proposed in Chua et al. (2018) (PETS-AE) as our baselines. The mean planner simply uses the mean estimate \(\bm{\mu}_{n}\) of the well-calibrated model. This is also used in Buisson-Fenet et al. (2020). Finally, we compare OpAx to (_4_) H-UCRL (Curi et al., 2020), a single-task model-based RL algorithm. We investigate the following three aspects: (_i_) _how fast does active exploration reduce model's epistemic uncertainty \(\bm{\sigma}_{n}\) with increasing \(n\)_, (_ii_) _can we solve downstream tasks with_ OpAx, and (_iii_) _does_ OpAx _scale to high-dimensional and challenging object manipulation tasks_? For our experiments, we use GPs and probabilistic ensembles (PE, Lakshminarayanan et al. (2017)) for modeling the dynamics. For the planning, we either the soft actor-critic (SAC, Haarnoja et al. (2018)) policy optimizer, which takes simulated trajectories from our learned model to train a policy, or MPC with the iEEM optimizer (Pinneri et al., 2021).

How fast does active exploration reduce the epistemic uncertainty?For this experiment, we consider the Pendulum-v1 environment. We sample transitions at random from the pendulum's _reachable_ state-action space and evaluate our model's epistemic uncertainty for varying episodes and baselines. We model the dynamics with both GPs and PE. We depict the result in Figure 1. We conclude that the Random agent is slower in reducing the uncertainty compared to other active exploration methods for both GP and PE models. In particular, from the experiment, we empirically validate Theorem 2 for the GP case and also conclude that empirically even when using PE models, we find convergence of epistemic uncertainty. Moreover, we notice for the PE case that OpAx reaches smaller uncertainties slightly faster than Mean-AE and PETS-AE. We believe this is due to the additional exploration induced by the optimistic planner.

Can the model learnt through OpAx solve downstream tasks?We use OpAx and other active exploration baselines to actively learn a dynamics model and then evaluate the learned model on downstream tasks. We consider several tasks, (_i_) Pendulum-v1 swing up, (_ii_) Pendulum-v1 keep down (keep the pendulum at the stable equilibria), (_iii_) MountainCar, (_iv_) Reacher - go to target, (_v_) Swimmer - go to target, (_vi_) Swimmer - go away from target (quickly go away from the target position), (_vii_) Cheetah - run forward, (_viii_) Cheetah - run backward. For all tasks, we consider PEs, except for (_i_) where we also use GPs. Furthermore, for the MountainCar and Reacher, we give a reward once the goal is reached. Since this requires long-term planning, we use a SAC policy for these tasks. We use MPC with iCEM for the remaining tasks. We also train H-UCRL on tasks (_i_) with GPs, and (_ii_), (_iii_), (_iv_), (_v_), (_vii_) with PEs. We report the best performance across all episodes.

To make a fair comparison, we use the following evaluation procedure; first, we perform active exploration for each episode on the environment, and then after every few episodes we use the mean estimate \(\bm{\mu}_{n}\) to evaluate our learned model on the downstream tasks.

Figure 2 shows that all active exploration variants perform considerably better than the Random agent. In particular, for the MountainCar, the Random agent is not able to solve the task. Moreover, PETS-AE performs slightly worse than the other exploration baselines in this environment. In general, we notice that OpAx always performs well and is able to achieve H-UCRL's performance on all the tasks for which H-UCRL is trained. However, on tasks that are new/unseen for H-UCRL, active exploration algorithms outperform H-UCRL. From this experiment, we conclude two things (_1_) apart from providing theoretical guarantees, the model learned through OpAx also performs well in downstream tasks, and (2) active exploration agents generalize well to downstream tasks, whereas H-UCRL performs considerably worse on new/unseen tasks. We believe this is because, unlike active exploration agents, task-specific model-based RL agents only explore the regions of the state-action space that are relevant to the task at hand.

Does OpAx scale to high-dimensional and challenging object manipulation tasks?To answer this question, we consider the Fetch Pick & Place Construction environment (Li et al., 2020). We again use the active exploration agents to learn a model and then evaluate the success rate of the learned model in three challenging downstream tasks: (_i_) Pick & Place, (_ii_) Throw, and (_iii_) Flip (see Figure 4). The environment contains a \(7\)-DoF robot arm and four \(6\)-DoF blocks that can be manipulated. In total, the state space is \(58\)-dimensional. The \(4\)-dimensional actions control the end-effector of the robot in Cartesian space as well as the opening/closing of the gripper. We compare OpAx to PETS-AE, Mean-AE, a random policy as well as CEE-US (Sancaktar et al., 2022). CEE-US is a model-based active exploration algorithm, for which Sancaktar et al. (2022) reports state-of-the-art performance compared to several other active exploration methods. In all three tasks, OpAx is at least on par with the best-performing baselines, including CEE-US. We run OpAx and all baselines with the same architecture and hyperparameter settings. See Appendix B for more details.

Figure 1: Reduction in maximum epistemic uncertainty in _reachable_ state-action space for the Pendulum-v1 environment over 10 different random seeds. We evaluate OpAx with both GPs and PE and plot the mean performance with two standard error confidence intervals. For both, active exploration reduces epistemic uncertainty faster compared to random exploration. All active exploration baselines perform well for the GP case, whereas for the PE case OpAx gives slightly lower uncertainties.

Figure 3: Fetch Pick & Place Construction environment.

## 6 Related Work

System identification is a broadly studied topic (Astrom and Eykhoff, 1971; Schoukens and Ljung, 2019; Schon et al., 2011; Ziemann et al., 2022; Ziemann and Tu, 2022). However, system identification from the perspective of experiment design for nonlinear systems is much less understood (Chiuso and Pillonetto, 2019). Most methods formulate the identification task through the maximization of intrinsic rewards. Common choices of intrinsic rewards are (_i_) model prediction error or "Curiosity" (Schmidhuber, 1991; Pathak et al., 2017), (_ii_) novelty of transitions (Stadie et al., 2015), and (_iii_) diversity of skills (Eysenbach et al., 2018).

A popular choice for intrinsic rewards is mutual information or entropy (Jain et al., 2018; Buisson-Fenet et al., 2020; Shyam et al., 2019; Pathak et al., 2019; Sekar et al., 2020). Jain et al. (2018) propose an approach to maximize the information gain greedily wrt the immediate next transition, i.e., one-step greedy, whereas Buisson-Fenet et al. (2020) consider planning full trajectories. Shyam et al. (2019); Pathak et al. (2019); Sekar et al. (2020) and Sancaktar et al. (2022) consider general Bayesian models, such as BNNs, to represent a probabilistic distribution for the learned model. Shyam et al. (2019) propose using the information gain of the model with respect to observed transition as the intrinsic reward. To this end, they learn an ensemble of Gaussian neural networks and represent the distribution over models with a Gaussian mixture model (GMM). A similar approach is also proposed in Pathak et al. (2019); Sekar et al. (2020); Sancaktar et al. (2022). The main difference between Shyam et al. (2019) and Pathak et al. (2019) lies in how they represent mutual information. Moreover, Pathak et al. (2019) use the model's epistemic uncertainty, that is the

Figure 2: We evaluate the downstream performance of our agents over 10 different random seeds and plot the mean performance with two standard error confidence intervals. For all the environments we use PE as models, except plot (1), for which we use a GP model (see plot (2) in the figure above). For tasks (1)-(6), we also train H-UCRL, a model-based RL algorithm. Tasks (7)-(9) are new/unseen for H-UCRL. From the Figure, we conclude that (_i_) compared to other active exploration baselines, OPAX constantly performs well and is on par with H-UCRL, and (_ii_) on the new/unseen tasks the active exploration baselines and OPAX outperform H-UCRL by a large margin.

disagreement between the ensemble models as an intrinsic reward. Sekar et al. (2020) link the model disagreement (epistemic uncertainty) reward to maximizing mutual information and demonstrate state-of-the-art performance on several high-dimensional tasks. Similarly, Sancaktar et al. (2022), use the disagreement in predicted trajectories of an ensemble of neural networks to direct exploration. Since trajectories can diverge due to many factors beyond just the model epistemic uncertainty, e.g., aleatoric noise, this approach is restricted to deterministic systems and susceptible to systems with unstable equilibria. Our approach is the most similar to Pathak et al. (2019); Sekar et al. (2020) since we also propose the model epistemic uncertainty as the intrinsic reward for planning. However, we thoroughly and theoretically motivate this choice of reward from a Bayesian experiment design perspective. Furthermore, we induce additional exploration in OpAx through our optimistic planner and rigorously study the theoretical properties of the proposed methods. On the contrary, most of the prior work discussed above either uses the mean planner (Mean-AE) or does not discuss the planner thoroughly or provide any theoretical results. In general, theoretical guarantees for active exploration algorithms are rather immature (Chakraborty et al., 2023; Wagenmaker et al., 2023) and mostly restrictive to a small class of systems (Simchowitz et al., 2018; Tarbouriech et al., 2020; Wagenmaker and Jamieson, 2020; Mania et al., 2020). To the best of our knowledge, we are the first to give convergence guarantees for a rich class of nonlinear systems.

While our work focuses on the active learning of dynamics, there are numerous works that study exploration in the context of reward-free RL (Jin et al., 2020; Kaufmann et al., 2021; Wagenmaker et al., 2022; Chen et al., 2022). However, most methods in this setting give guarantees for special classes of MDPs (Jin et al., 2020; Kaufmann et al., 2021; Wagenmaker et al., 2022; Qiu et al., 2021; Chen et al., 2022) and result in practical algorithms. On the contrary, we focus on solely learning the dynamics. While a good dynamics model may be used for zero-shot planning, it also exhibits more relevant knowledge about the system such as its stability or sensitivity to external effects. Furthermore, our proposed method is not only theoretically sound but also practical.

## 7 Conclusion

We present OpAx, a novel model-based RL algorithm for the active exploration of unknown dynamical systems. Taking inspiration from Bayesian experiment design, we provide a comprehensive explanation for using model epistemic uncertainty as an intrinsic reward for exploration. By leveraging the _optimistic in the face of uncertainty_ paradigm, we put forth first-of-their-kind theoretical results on the convergence of active exploration agents in reinforcement learning. Specifically, we study convergence properties of general Bayesian models, such as BNNs. For the frequentist case of RKHS dynamics, we established sample complexity bounds and convergence guarantees for OpAx for a rich class of functions. We evaluate the efficacy of OpAx across various RL environments with state space dimensions from two to 58. The empirical results corroborate our theoretical findings, as OpAx displays systematic and effective exploration across all tested environments and exhibits strong performance in zero-shot planning for new downstream tasks.

Figure 4: Success rates for pick & place, throwing and flipping tasks with four objects in the Fetch Pick & Place Construction environment for OpAx and baselines. We evaluate task performance via planning zero-shot with models learned using different exploration strategies. We report performance on three independent seeds. OpAx is on par with the best-performing baselines in all tasks.

## Acknowledgments and Disclosure of Funding

We would like to thank Jonas Hubotter for the insightful discussions and his feedback on this work. Furthermore, we also thank Alex Hagele, Parnian Kassraie, Scott Sussex, and Dominik Baumann for their feedback.

This project has received funding from the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, and the Microsoft Swiss Joint Research Center.

## References

* Balcan et al. (2010) Balcan, M.-F., Hanneke, S., and Vaughan, J. W. (2010). The true sample complexity of active learning. _Machine learning_, 80:111-139.
* Berkenkamp et al. (2017) Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A. (2017). Safe model-based reinforcement learning with stability guarantees. _NeurIPS_, 30.
* Biagiotti and Melchiorri (2008) Biagiotti, L. and Melchiorri, C. (2008). _Trajectory Planning for Automatic Machines and Robots_. Springer Publishing Company, Incorporated, 1st edition.
* Bradbury et al. (2018) Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. (2018). JAX: composable transformations of Python+NumPy programs.
* Brockman et al. (2016) Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. (2016). Openai gym. _arXiv preprint arXiv:1606.01540_.
* Buisson-Fenet et al. (2020) Buisson-Fenet, M., Solowjow, F., and Trimpe, S. (2020). Actively learning gaussian process dynamics. In Bayen, A. M., Jadbabaie, A., Pappas, G., Parrilo, P. A., Recht, B., Tomlin, C., and Zeilinger, M., editors, _L4DC_.
* Chakraborty et al. (2023) Chakraborty, S., Bedi, A., Koppel, A., Wang, M., Huang, F., and Manocha, D. (2023). STEERING : Stein information directed exploration for model-based reinforcement learning. pages 3949-3978.
* Chaloner and Verdinelli (1995) Chaloner, K. and Verdinelli, I. (1995). Bayesian experimental design: A review. _Statistical science_, pages 273-304.
* Chen et al. (2022) Chen, J., Modi, A., Krishnamurthy, A., Jiang, N., and Agarwal, A. (2022). On the statistical efficiency of reward-free exploration in non-linear rl. _Advances in Neural Information Processing Systems_, 35:20960-20973.
* Chen et al. (2015) Chen, Y., Hassani, S. H., Karbasi, A., and Krause, A. (2015). Sequential information maximization: When is greedy near-optimal? In _COLT_.
* Chiuso and Pillonetto (2019) Chiuso, A. and Pillonetto, G. (2019). System identification: A machine learning perspective. _Annual Review of Control, Robotics, and Autonomous Systems_, 2:281-304.
* Chowdhury and Gopalan (2017) Chowdhury, S. R. and Gopalan, A. (2017). On kernelized multi-armed bandits. In _ICML_, pages 844-853. PMLR.
* Chua et al. (2018) Chua, K., Calandra, R., McAllister, R., and Levine, S. (2018). Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In _NeurIPS_.
* Cover and Thomas (2006) Cover, T. M. and Thomas, J. A. (2006). _Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)_. Wiley-Interscience.
* Curi et al. (2020) Curi, S., Berkenkamp, F., and Krause, A. (2020). Efficient model-based reinforcement learning through optimistic policy search and planning. _NeurIPS_, 33:14156-14170.
* Eysenbach et al. (2018) Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. (2018). Diversity is all you need: Learning skills without a reward function. _arXiv preprint arXiv:1802.06070_.
* a survey. _Automatica_, pages 335-348.
* Gopalan (2017)Haznoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _ICML_, pages 1861-1870.
* Hanneke et al. (2014) Hanneke, S. et al. (2014). Theory of disagreement-based active learning. _Foundations and Trends(r) in Machine Learning_, 7(2-3):131-309.
* Jain et al. (2018) Jain, A., Nghiem, T., Morari, M., and Mangharam, R. (2018). Learning and control using gaussian processes. In _ICCPS_, pages 140-149.
* Jin et al. (2020) Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T. (2020). Reward-free exploration for reinforcement learning. In _ICML_, pages 4870-4879. PMLR.
* Kakade et al. (2020) Kakade, S., Krishnamurthy, A., Lowrey, K., Ohnishi, M., and Sun, W. (2020). Information theoretic regret bounds for online nonlinear control. _NeurIPS_, 33:15312-15325.
* Kaufmann et al. (2021) Kaufmann, E., Menard, P., Domingues, O. D., Jonsson, A., Leurent, E., and Valko, M. (2021). Adaptive reward-free exploration. In _ALT_, pages 865-891. PMLR.
* Khalil (2015) Khalil, H. K. (2015). _Nonlinear control_, volume 406. Pearson New York.
* Krause et al. (2008) Krause, A., Singh, A., and Guestrin, C. (2008). Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. _Journal of Machine Learning Research_, 9(2).
* Kuleshov et al. (2018) Kuleshov, V., Fenner, N., and Ermon, S. (2018). Accurate uncertainties for deep learning using calibrated regression. In _ICML_, pages 2796-2804. PMLR.
* Lakshminarayanan et al. (2017) Lakshminarayanan, B., Pritzel, A., and Blundell, C. (2017). Simple and scalable predictive uncertainty estimation using deep ensembles. _NeurIPS_, 30.
* Li et al. (2020) Li, R., Jabri, A., Darrell, T., and Agrawal, P. (2020). Towards practical multi-object manipulation using relational reinforcement learning. In _ICRA_, pages 4051-4058. IEEE.

* Mania et al. (2020) Mania, H., Jordan, M. I., and Recht, B. (2020). Active learning for nonlinear system identification with guarantees. _arXiv preprint arXiv:2006.10277_.
* Mehta et al. (2021) Mehta, V., Paria, B., Schneider, J., Ermon, S., and Neiswanger, W. (2021). An experimental design perspective on model-based reinforcement learning. _arXiv preprint arXiv:2112.05244_.
* Mutny et al. (2023) Mutny, M., Janik, T., and Krause, A. (2023). Active exploration via experiment design in markov chains. In _AISTATS_.
* Pasztor et al. (2021) Pasztor, B., Bogunovic, I., and Krause, A. (2021). Efficient model-based multi-agent mean-field reinforcement learning. _arXiv preprint arXiv:2107.04050_.
* Pathak et al. (2017) Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven exploration by self-supervised prediction. In _ICML_, pages 2778-2787. PMLR.
* Pathak et al. (2019) Pathak, D., Gandhi, D., and Gupta, A. (2019). Self-supervised exploration via disagreement. In _ICML_, pages 5062-5071. PMLR.
* Pinneri et al. (2021) Pinneri, C., Sawant, S., Blaes, S., Achterhold, J., Stueckler, J., Rolinek, M., and Martius, G. (2021). Sample-efficient cross-entropy method for real-time planning. In _CORL_, Proceedings of Machine Learning Research, pages 1049-1065.
* Qiu et al. (2021) Qiu, S., Ye, J., Wang, Z., and Yang, Z. (2021). On reward-free rl with kernel and neural function approximations: Single-agent mdp and markov game. In _ICML_, pages 8737-8747. PMLR.
* Rasmussen and Williams (2005) Rasmussen, C. E. and Williams, C. K. I. (2005). _Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)_. The MIT Press.
* a survey. _Automatica_, 7(2):123-162.
* Sukhukhukh et al. (2017)Rothfuss, J., Sukhija, B., Birchler, T., Kassraie, P., and Krause, A. (2023). Hallucinated adversarial control for conservative offline policy evaluation. _UAI_.
* Sancaktar et al. (2022) Sancaktar, C., Blaes, S., and Martius, G. (2022). Curious exploration via structured world models yields zero-shot object manipulation. In _NeurIPS 35 (NeurIPS 2022)_.
* Schmidhuber (1991) Schmidhuber, J. (1991). A possibility for implementing curiosity and boredom in model-building neural controllers. In _Proc. of the international conference on simulation of adaptive behavior: From animals to animats_, pages 222-227.
* Schon et al. (2011) Schon, T. B., Wills, A., and Ninness, B. (2011). System identification of nonlinear state-space models. _Automatica_, pages 39-49.
* Schoukens and Ljung (2019) Schoukens, J. and Ljung, L. (2019). Nonlinear system identification: A user-oriented road map. _IEEE Control Systems Magazine_, 39(6):28-99.
* Sekar et al. (2020) Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., and Pathak, D. (2020). Planning to explore via self-supervised world models. In _ICML_, pages 8583-8592. PMLR.
* Settles (2009) Settles, B. (2009). Active learning literature survey.
* Shyam et al. (2019) Shyam, P., Jaskowski, W., and Gomez, F. (2019). Model-based active exploration. In _ICML_, pages 5779-5788. PMLR.
* Simchowitz and Foster (2020) Simchowitz, M. and Foster, D. (2020). Naive exploration is optimal for online LQR. In _ICML_, Proceedings of Machine Learning Research, pages 8937-8948. PMLR.
* Simchowitz et al. (2018) Simchowitz, M., Mania, H., Tu, S., Jordan, M. I., and Recht, B. (2018). Learning without mixing: Towards a sharp analysis of linear system identification. In _COLT_, pages 439-473. PMLR.
* Srinivas et al. (2012) Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M. W. (2012). Information-theoretic regret bounds for gaussian process optimization in the bandit setting. _IEEE Transactions on Information Theory_.
* Stadie et al. (2015) Stadie, B. C., Levine, S., and Abbeel, P. (2015). Incentivizing exploration in reinforcement learning with deep predictive models. _arXiv preprint arXiv:1507.00814_.
* Sussex et al. (2023) Sussex, S., Makarova, A., and Krause, A. (2023). Model-based causal bayesian optimization. In _ICLR_.
* Tarbouriech et al. (2020) Tarbouriech, J., Shekhar, S., Pirotta, M., Ghavamzadeh, M., and Lazaric, A. (2020). Active model estimation in markov decision processes. In _UAI_, pages 1019-1028. PMLR.
* Tassa et al. (2018) Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. (2018). Deepmind control suite. _arXiv preprint arXiv:1801.00690_.
* Vakili et al. (2021) Vakili, S., Khezeli, K., and Picheny, V. (2021). On information gain and regret bounds in gaussian process bandits. In _AISTATS_.
* Wagenmaker and Jamieson (2020) Wagenmaker, A. and Jamieson, K. (2020). Active learning for identification of linear dynamical systems. In _COLT_, pages 3487-3582. PMLR.
* Wagenmaker et al. (2023) Wagenmaker, A., Shi, G., and Jamieson, K. (2023). Optimal exploration for model-based rl in nonlinear systems. _arXiv preprint arXiv:2306.09210_.
* Wagenmaker et al. (2022) Wagenmaker, A. J., Chen, Y., Simchowitz, M., Du, S., and Jamieson, K. (2022). Reward-free rl is no harder than reward-aware rl in linear markov decision processes. In _ICML_, pages 22430-22456. PMLR.
* Wang and Yeung (2020) Wang, H. and Yeung, D.-Y. (2020). A survey on bayesian deep learning. _ACM computing surveys (csur)_, pages 1-37.
* Weiss et al. (2016) Weiss, K., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer learning. _Journal of Big data_, pages 1-40.
* Wagenmaker et al. (2017)Zhang, Y. and Yang, Q. (2021). A survey on multi-task learning. _IEEE Transactions on Knowledge and Data Engineering_, pages 5586-5609.
* Ziemann and Tu (2022) Ziemann, I. and Tu, S. (2022). Learning with little mixing. _NeurIPS_, 35:4626-4637.
* Ziemann et al. (2022) Ziemann, I. M., Sandberg, H., and Matni, N. (2022). Single trajectory nonparametric learning of nonlinear dynamics. In _COLT_, pages 3333-3364. PMLR.

###### Contents of Appendix

* A Proofs for section 4
* A.1 Proof of Lemma 2
* A.2 Analyzing regret of optimistic planning
* A.3 Proof for general Bayesian models
* A.4 Proof of GP results
* A.5 Zero-shot guarantees
* B Experiment Details
* B.1 Environment Details
* B.2 Opax in the High-dimensional Fetch Pick & Place Environment
* C Study of exploration intrinsic rewards
Proofs for section 4

We first prove some key properties of our active exploration objective in Equation (6). Then, we prove Theorem 1 which holds for general Bayesian models, and finally we prove Theorem 2, which guarantees convergence for the frequentist setting where the dynamics are modeled using a GP.

**Lemma 4** (Properties of OpAx's objective).: _Let Assumption 1 and 2 hold, then the following is true for all \(n\geq 0\),_

\[\log\left(1+\frac{\sigma_{n-1,j}^{2}(\bm{x}_{t},\bm{\pi}(\bm{x}_{ t}))}{\sigma^{2}}\right) \geq 0\] (1) \[\frac{1}{2}\sup_{\bm{\pi}\in\bm{\Pi},\bm{\eta}\in\Xi}\,\mathbb{E} \left[\left(\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}}\log\left(1+\frac{\sigma_{n-1,j}^ {2}(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))}{\sigma^{2}}\right)\right)^{2}\right] \leq\frac{1}{2}T^{2}d_{x}^{2}\log^{2}\left(1+\frac{\sigma_{\max}^{2}}{ \sigma^{2}}\right),\] (2) \[\left|\frac{1}{2}\sum_{j=1}^{d_{x}}\log\left(1+\frac{\sigma_{n,j }^{2}(\bm{z})}{\sigma^{2}}\right)-\log\left(1+\frac{\sigma_{n,j}^{2}(\bm{z}^{ \prime})}{\sigma^{2}}\right)\right| \leq\frac{d_{x}\sigma_{\max}L_{\bm{\sigma}}}{\sigma^{2}}\left\|\bm{z}- \bm{z}^{\prime}\right\|.\] (3)

_where \(\sigma_{\max}=\sup_{\bm{z}\in\mathcal{Z};i\geq 0;1\leq j\leq d_{x}}\sigma_{i,j}( \bm{z})\)._

Proof.: The positivity of the reward follows from the positive definiteness of the epistemic uncertainty \(\sigma_{n-1,j}\). For (2), the following holds

\[\mathbb{E}\left[\left(\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}}\log \left(1+\frac{\sigma_{n-1,j}^{2}(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))}{\sigma^{2}} \right)\right)^{2}\right]\] \[\leq\mathbb{E}\left[\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}}Td_{x}\log ^{2}\left(1+\frac{\sigma_{n-1,j}^{2}(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))}{\sigma ^{2}}\right)\right]\] \[\leq\mathbb{E}\left[\left(\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}}Td_{ x}\log^{2}\left(1+\frac{\sigma_{\max}^{2}}{\sigma^{2}}\right)\right)\right]\] \[\leq T^{2}d_{x}^{2}\log^{2}\left(1+\frac{\sigma_{\max}^{2}}{ \sigma^{2}}\right)\]

From hereon, let \(J_{\max}=\nicefrac{{1}}{{2}}T^{2}d_{x}^{2}\log^{2}\left(1+\sigma^{-2}\sigma_{ \max}^{2}\right)\).

Finally, we show that this reward is Lipschitz continuous.

\[\left|\sigma_{n,j}^{2}(\bm{z})-\sigma_{n,j}^{2}(\bm{z}^{\prime})\right| =\left|\sigma_{n,j}(\bm{z})\sigma_{n,j}(\bm{z})-\sigma_{n,j}(\bm {z})\sigma_{n,j}(\bm{z}^{\prime})+\sigma_{n,j}(\bm{z})\sigma_{n,j}(\bm{z}^{ \prime})-\sigma_{n,j}(\bm{z}^{\prime})\sigma_{n,j}(\bm{z}^{\prime})\right|\] \[\leq L_{\sigma}\left\|\bm{z}-\bm{z}^{\prime}\right\|\sigma_{n,j}( \bm{z})+L_{\sigma}\left\|\bm{z}-\bm{z}^{\prime}\right\|\sigma_{n,j}(\bm{z}^{ \prime})\] \[\leq 2\sigma_{\max}L_{\sigma}\left\|\bm{z}-\bm{z}^{\prime}\right\|.\]

\[\left|\frac{1}{2}\sum_{j=1}^{d_{x}}\log\left(1+\frac{\sigma_{n,j }^{2}(\bm{z})}{\sigma^{2}}\right)-\log\left(1+\frac{\sigma_{n,j}^{2}(\bm{z}^{ \prime})}{\sigma^{2}}\right)\right| =\frac{1}{2}\left|\sum_{j=1}^{d_{x}}\log\left(1+\frac{\frac{\sigma _{n,j}^{2}(\bm{z})-\sigma_{n,j}^{2}(\bm{z}^{\prime})}{\sigma^{2}}}{1+\frac{ \sigma_{n,j}^{2}(\bm{z}^{\prime})}{\sigma^{2}}}\right)\right|\] \[\leq\frac{1}{2}\left|\sum_{j=1}^{d_{x}}\log\left(1+\frac{\left| \sigma_{n,j}^{2}(\bm{z})-\sigma_{n,j}^{2}(\bm{z}^{\prime})\right.\right|}{ \sigma^{2}}\right)\right|\] \[\leq\frac{1}{2\sigma^{2}}\sum_{j=1}^{d_{x}}\left|\sigma_{n,j}^{2 }(\bm{z})-\sigma_{n,j}^{2}(\bm{z}^{\prime})\right|\] (*) \[\leq\frac{d_{x}\sigma_{\max}L_{\bm{\sigma}}}{\sigma^{2}}\left\| \bm{z}-\bm{z}^{\prime}\right\|.\]

Where (*) is true because for all \(x\geq 0\), \(\log(1+x)\leq x\)

**Corollary 1** (OpAx gives an optimistic estimate on Equation (6)).: _Let Assumption 1 hold and \(\bm{\pi}_{n}^{*}\) denote the solution to Equation (6) and \(J_{n}(\bm{\pi}_{n}^{*})\) the resulting objective. Similarly, let \(\bm{\pi}_{n}\) and \(\bm{\eta}_{n}\) be the solution to Equation (7) and \(J_{n}(\bm{\pi}_{n},\bm{\eta}_{n})\) the corresponding value of the objective. Then with probability at least \(1-\delta\) we have for every episode \(n\in\{1,\ldots,N\}\):_

\[J_{n}(\bm{\pi}_{n}^{*})\leq J_{n}(\bm{\pi}_{n},\bm{\eta}_{n}).\]

Proof.: Follows directly from Assumption 1. 

### Proof of Lemma 2

**Lemma 5** (Difference in Policy performance).: _Let \(J_{r,k}(\bm{\pi},\bm{x}_{k})=\mathbb{E}_{\bm{\tau}^{\bm{\pi}}}\left[\sum_{t=k}^ {T-1}r(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))\right]\) and \(A_{r,k}(\bm{\pi},\bm{x},\bm{a})=\mathbb{E}_{\bm{\tau}^{\bm{\pi}}}\left[r(\bm{x },\bm{a})+J_{r,k+1}(\bm{\pi},\bm{x}^{\prime})-J_{r,k}(\bm{\pi},\bm{x})\right]\) with \(\bm{x}^{\prime}=\bm{\tilde{f}}^{*}(\bm{x},\bm{a})+\bm{w}\). For simplicity we refer to \(J_{r,0}(\bm{\pi},\bm{x}_{0})=J_{r}(\bm{\pi},\bm{x}_{0})\). The following holds for all \(\bm{x}_{0}\in\mathcal{X}\):_

\[J_{r}(\bm{\pi}^{\prime},\bm{x}_{0})-J_{r}(\bm{\pi},\bm{x}_{0})=\mathbb{E}_{\bm {\tau}^{\bm{\pi}}^{\prime}}\left[\sum_{t=0}^{T-1}A_{r,t}(\bm{\pi},\bm{x}_{t}^{ \prime},\bm{\pi}^{\prime}(\bm{x}_{t}^{\prime}))\right]\]

Proof.: \[J_{r}(\bm{\pi}^{\prime},\bm{x}_{0}) =\mathbb{E}_{\bm{\tau}^{\bm{\pi}}^{\prime}}\left[\sum_{t=0}^{T-1} r(\bm{x}_{t}^{\prime},\bm{\pi}^{\prime}(\bm{x}_{t}^{\prime}))\right]=\mathbb{E}_{ \bm{\tau}^{\bm{\pi}}^{\prime}}\left[r(\bm{x}_{0},\bm{\pi}^{\prime}(\bm{x}_{0}) )+J_{r,1}(\bm{\pi}^{\prime},\bm{x}_{1}^{\prime})\right]\] \[=\mathbb{E}_{\bm{\tau}^{\bm{\pi}}^{\prime}}\left[r(\bm{x}_{0},\bm {\pi}^{\prime}(\bm{x}_{0}))+J_{r,1}(\bm{\pi},\bm{x}_{1}^{\prime})-J_{r}(\bm{ \pi},\bm{x}_{0})\right]\] \[+J_{r}(\bm{\pi},\bm{x}_{0})+\mathbb{E}_{\bm{\tau}^{\bm{\pi}}^{ \prime}}\left[J_{r,1}(\bm{\pi}^{\prime},\bm{x}_{1}^{\prime})-J_{r,1}(\bm{\pi },\bm{x}_{1}^{\prime})\right]\] \[=\mathbb{E}_{\bm{\tau}^{\bm{\pi}}^{\prime}}\left[A_{r,0}(\bm{\pi },\bm{x}_{0},\bm{\pi}^{\prime}(\bm{x}_{0}))\right]+J_{r}(\bm{\pi},\bm{x}_{0}) +\mathbb{E}_{\bm{\tau}^{\bm{\pi}}^{\prime}}\left[J_{r,1}(\bm{\pi}^{\prime}, \bm{x}_{1})-J_{r,1}(\bm{\pi},\bm{x}_{1})\right]\]

Therefore we obtain

\[J_{r}(\bm{\pi}^{\prime},\bm{x}_{0})-J_{r}(\bm{\pi},\bm{x}_{0})=\mathbb{E}_{ \bm{\tau}^{\bm{\pi}}^{\prime}}\left[A_{0}(\bm{\pi},\bm{x}_{0},\bm{\pi}^{\prime }(\bm{x}_{0}))\right]+\mathbb{E}_{\bm{\tau}^{\bm{\pi}}^{\prime}}\left[J_{r,1} (\bm{\pi}^{\prime},\bm{x}_{1}^{\prime})-J_{r,1}(\bm{\pi},\bm{x}_{1}^{\prime}) \right].\]

Using the same argument for \(J_{r,1}\), \(J_{r,2}\),..., \(J_{r,T-1}\) and that \(J_{r,T}(\bm{\pi},\bm{x})=0\) for all \(\bm{\pi}\in\Pi\) and \(\bm{x}\in\mathcal{X}\) completes the proof. 

Assume a policy \(\bm{\pi}\) is fixed and dynamics are of the form:

\[\bm{x}^{\prime}=\bm{\mu}_{n}(\bm{x},\bm{\pi}(\bm{x}))+\beta_{n}(\delta)\bm{ \sigma}(\bm{x},\bm{\pi}(\bm{x}))\bm{u}+\bm{w}.\] (14)

Here \(\bm{u}\in[-1,1]^{d_{x}}\). Furthermore, assume that the associated running rewards do not depend on \(\bm{u}\), that is, \(r(\bm{x}_{t})\), and let \(\bm{\eta}\in\Xi\) denote the policy, i.e., \(\bm{\eta}:\mathcal{X}\to[-1,1]^{d_{x}}\).

**Corollary 2**.: _The following holds for all \(\bm{x}_{0}\in\mathcal{X}\) and policy \(\bm{\pi}\):_

\[J_{r}(\bm{\pi},\bm{\eta}^{\prime},\bm{x}_{0})-J_{r}(\bm{\pi},\bm{\eta},\bm{x}_{ 0})=\mathbb{E}_{\bm{\tau}^{\bm{\eta}}^{\prime}}\left[\sum_{t=0}^{T-1}J_{r,t+1}( \bm{\pi},\bm{\eta},\bm{x}_{t+1}^{\prime})-J_{r,t+1}(\bm{\pi},\bm{\eta},\bm{x} _{t+1})\right],\]

_with \(\bm{x}_{t+1}=\bm{\mu}_{n}(\bm{x}_{t}^{\prime},\bm{\pi}(\bm{x}_{t}^{\prime}))+ \beta_{n}(\delta)\bm{\sigma}(\bm{x}_{t}^{\prime},\bm{\pi}(\bm{x}_{t}^{\prime})) \bm{\eta}(\bm{x}_{t}^{\prime})+\bm{w}_{t}\), and \(\bm{x}_{t+1}^{\prime}=\bm{\mu}_{n}(\bm{x}_{t}^{\prime},\bm{\pi}(\bm{x}_{t}^{ \prime}))+\beta_{n}(\delta)\bm{\sigma}(\bm{x}_{t}^{\prime},\bm{\pi}(\bm{x}_{t}^{ \prime}))\bm{\eta}^{\prime}(\bm{x}_{t}^{\prime})+\bm{w}_{t}\)._

Proof.: From Lemma 5 we have

\[J_{r}(\bm{\pi},\bm{\eta}^{\prime},\bm{x}_{0})-J_{r}(\bm{\pi},\bm{\eta},\bm{x}_{ 0})=\mathbb{E}_{\bm{\tau}^{\bm{\eta}}^{\prime}}\left[\sum_{t=0}^{T-1}A_{r,t}( \bm{\eta},\bm{x}_{t}^{\prime},\bm{\eta}^{\prime}(\bm{x}_{t}^{\prime}))\right].\]

Furthermore,

\[\mathbb{E}_{\bm{\tau}^{\bm{\eta}}^{\prime}}\left[A_{r,t}(\bm{\eta},\bm{x}_{t}^{ \prime},\bm{\eta}^{\prime}(\bm{x}_{t}^{\prime}))\right] =\mathbb{E}_{\bm{\tau}^{\bm{\eta}}^{\prime}}\left[r(\bm{x}_{t}^{ \prime})+J_{r,t+1}(\bm{\pi},\bm{\eta},\bm{x}_{t+1}^{\prime})-J_{r,t}(\bm{\pi}, \bm{\eta},\bm{x}_{t}^{\prime})\right]\] \[=\mathbb{E}_{\bm{\tau}^{\bm{\eta}}^{\prime}}\left[r(\bm{x}_{t}^{ \prime})+J_{r,t+1}(\bm{\pi},\bm{\eta},\bm{x}_{t+1}^{\prime})-r(\bm{x}_{t}^{ \prime})-J_{r,t+1}(\bm{\pi},\bm{\eta},\bm{x}_{t+1})\right]\] \[=\mathbb{E}_{\bm{\tau}^{\bm{\eta}}^{\prime}}\left[J_{r,t+1}(\bm{ \pi},\bm{\eta},\bm{x}_{t+1}^{\prime})-J_{r,t+1}(\bm{\pi},\bm{\eta},\bm{x}_{t+ 1})\right].\]Proof of Lemma 2.: From Assumption 1 we know that with probability at least \(1-\delta\) there exists a \(\bar{\bm{\eta}}\) such that \(\bm{f}^{*}(\bm{z})=\bm{\mu}_{n}(\bm{z})+\beta_{n}(\delta)\bm{\sigma}(\bm{z})\bar {\bm{\eta}}(\bm{x})\) for all \(\bm{z}\in\mathcal{Z}\).

\[J_{n}(\bm{\pi}_{n}^{*})-J_{n}(\bm{\pi}_{n}) \leq J_{n}(\bm{\pi}_{n},\bm{\eta}_{n})-J_{n}(\bm{\pi}_{n})\] (Corollary 1) \[=J_{n}(\bm{\pi}_{n},\bm{\eta}_{n})-J_{n}(\bm{\pi}_{n},\bar{\bm{\eta }})\] \[=\mathbb{E}_{\bm{\tau}^{\bm{\eta}}}\left[\sum_{t=0}^{T-1}J_{n,t+1} (\bm{\pi}_{n},\bm{\eta}_{n},\bm{x}_{t+1}^{\prime})-J_{n,t+1}(\bm{\pi}_{n},\bm {\eta}_{n},\bm{x}_{t+1})\right]\] (Corollary 2) \[=\mathbb{E}_{\bm{\tau}^{\bm{\pi}_{n}}}\left[\sum_{t=0}^{T-1}J_{n,t +1}(\bm{\pi}_{n},\bm{\eta}_{n},\bm{x}_{t+1}^{\prime})-J_{n,t+1}(\bm{\pi}_{n}, \bm{\eta}_{n},\bm{x}_{t+1})\right],\] (Expectation wrt \[\bm{\pi}_{n}\] under true dynamics \[\bm{f}^{*}\] ) \[\text{and }\bm{x}_{t+1}^{\prime}=\bm{\mu}_{n-1}(\bm{x}_{t},\bm{\pi}_ {n}(\bm{x}_{t}))+\beta_{n-1}(\delta)\bm{\sigma}_{n-1}(\bm{x}_{t},\bm{\pi}_{n}( \bm{x}_{t}))\bm{\eta}_{n}(\bm{x}_{t})+\bm{w}_{t}.\]

### Analyzing regret of optimistic planning

In the following, we analyze the regret of optimistic planning for both \(\sigma\)-Gaussian noise and \(\sigma\)-sub Gaussian noise case. We start with the Gaussian case.

**Lemma 6** (Absolute expectation Difference Under Two Gaussians (Lemma C.2. Kakade et al. (2020))).: _For Gaussian distribution \(\mathcal{N}(\mu_{1},\sigma^{2}\mathbb{I})\) and \(\mathcal{N}(\mu_{2},\sigma^{2}\mathbb{I})\), and for any (appropriately measurable) positive function \(g\), it holds that:_

\[\left|\mathbb{E}_{z\sim\mathcal{N}_{1}}[g(z)]-\mathbb{E}_{z\sim\mathcal{N}_{ 2}}[g(z)]\right|\leq\min\left\{\frac{\left\|\mu_{1}-\mu_{2}\right\|}{\sigma^ {2}},1\right\}\sqrt{\mathbb{E}_{z\sim\mathcal{N}_{1}}[g^{2}(z)]}\]

Proof.: \[\left|\mathbb{E}_{z\sim\mathcal{N}_{1}}[g(z)]-\mathbb{E}_{z\sim \mathcal{N}_{2}}[g(z)]\right| =\left|\mathbb{E}_{z\sim\mathcal{N}_{1}}\left[g(z)\left(1-\frac{ \mathcal{N}_{2}}{\mathcal{N}_{1}}\right)\right]\right|\] \[\leq\left|\sqrt{\mathbb{E}_{z\sim\mathcal{N}_{1}}[g^{2}(z)]} \sqrt{\mathbb{E}_{z\sim\mathcal{N}_{1}}\left[\left(1-\frac{\mathcal{N}_{2}}{ \mathcal{N}_{1}}\right)^{2}\right]}\right|\] \[=\sqrt{\mathbb{E}_{z\sim\mathcal{N}_{1}}[g^{2}(z)]}\sqrt{ \mathbb{E}_{z\sim\mathcal{N}_{1}}\left[\left(1-\frac{\mathcal{N}_{2}}{ \mathcal{N}_{1}}\right)^{2}\right]}\] \[\leq\sqrt{\mathbb{E}_{z\sim\mathcal{N}_{1}}[g^{2}(z)]}\min\left\{ \frac{\left\|\mu_{1}-\mu_{2}\right\|}{\sigma^{2}},1\right\}\] (Lemma C.2. Kakade et al. (2020))

**Corollary 3** (Regret of optimistic planning for Gaussian noise).: _Let \(\bm{\pi}_{n}^{*}\), \(\bm{\pi}_{n}\) denote the solution to Equation (6) and Equation (7) respectively, and \(\bm{z}_{n,t}^{*}\), \(\bm{z}_{n,t}\), the corresponding state-action pairs visited during their respective trajectories. Furthermore, let Assumption 1 - 3 hold. Then, the following is true for all \(n\geq 0\), \(t\in[0,T-1]\), with probability at least \(1-\delta\)_

\[J_{n}(\bm{\pi}_{n}^{*})-J_{n}(\bm{\pi}_{n})\leq\mathcal{O}\left(T\mathbb{E}_{ \bm{\tau}^{\bm{\pi}_{n}}}\left[\sum_{t=0}^{T-1}\frac{(1+\sqrt{d_{x}})\beta_{n- 1}(\delta)\left\|\bm{\sigma}_{n-1}(\bm{z}_{n,t})\right\|}{\sigma^{2}}\right]\right)\]Proof.: For simplicity, define \(g_{n}(\bm{x})=J_{n,t+1}(\bm{\pi}_{n},\bm{\eta}_{n},\bm{x})\). Note since \(\bm{w}_{t}\sim\mathcal{N}(0,\sigma^{2}\mathbb{I})\) (Assumption 3), we have that \(\bm{x}_{t+1}^{\prime}\) and \(\bm{x}_{t+1}\) are also Gaussians. Therefore, we can leverage Lemma 6.

\[\mathbb{E}_{\bm{\tau}^{\bm{\pi}_{n}}} \left[J_{n,t+1}(\bm{\pi}_{n},\bm{\eta}_{n},\bm{x}_{t+1}^{\prime}) -J_{n,t+1}(\bm{\pi}_{n},\bm{\eta}_{n},\bm{x}_{t+1})\right]=\mathbb{E}\left[g_{n }(\bm{x}_{t+1}^{\prime})-g_{n}(\bm{x}_{t+1})\right]\] \[\leq\sqrt{\mathbb{E}[g_{n}^{2}(\bm{x}_{t+1})]}\min\left\{\frac{ \left\|\bm{x}_{t+1}^{\prime}-\bm{x}_{t+1}\right\|}{\sigma^{2}},1\right\}\] (Lemma 6) \[\leq\sqrt{J_{\max}}\min\left\{\frac{\left\|\bm{x}_{t+1}^{\prime}- \bm{x}_{t+1}\right\|}{\sigma^{2}},1\right\}.\] (Lemma 4)

Furthermore,

\[\left\|\bm{x}_{t+1}^{\prime}-\bm{x}_{t+1}\right\| =\left\|\bm{\mu}_{n-1}(\bm{x}_{t},\bm{\pi}_{n}(\bm{x}_{t}))+ \beta_{n-1}(\delta)\bm{\sigma}_{n-1}(\bm{x}_{t},\bm{\pi}_{n}(\bm{x}_{t}))\bm{ \eta}_{n}(\bm{x}_{t})-\bm{f}^{*}(\bm{x}_{t},\bm{\pi}_{n}(\bm{x}_{t}))\right\|\] \[\leq\left\|\bm{\mu}_{n-1}(\bm{x}_{t},\bm{\pi}_{n}(\bm{x}_{t}))- \bm{f}^{*}(\bm{x}_{t},\bm{\pi}_{n}(\bm{x}_{t}))\right\|\] \[+\beta_{n-1}(\delta)\left\|\bm{\sigma}_{n-1}(\bm{x}_{t},\bm{\pi}_{ n}(\bm{x}_{t}))\right\|\left\|\bm{\eta}_{n}(\bm{x}_{t})\right\|\] \[\leq(1+\sqrt{d_{x}})\beta_{n-1}(\delta)\left\|\bm{\sigma}_{n-1}( \bm{x}_{t},\bm{\pi}_{n}(\bm{x}_{t}))\right\|.\] (Assumption 1)

Next, we use Lemma 2

\[J_{n}(\bm{\pi}_{n}^{*})-J_{n}(\bm{\pi}_{n}) \leq\mathbb{E}_{\bm{\tau}^{\bm{\pi}_{n}}}\left[\sum_{t=0}^{T-1}J_ {n,t+1}(\bm{\pi}_{n},\bm{\eta}_{n},\bm{x}_{t+1}^{\prime})-J_{n,t+1}(\bm{\pi}_{ n},\bm{\eta}_{n},\bm{x}_{t+1})\right],\] \[\leq\mathbb{E}_{\bm{\tau}^{\bm{\pi}_{n}}}\left[\sum_{t=0}^{T-1} \sqrt{J_{\max}}\min\left\{\frac{(1+\sqrt{d_{x}})\beta_{n-1}(\delta)\left\|\bm{ \sigma}_{n-1}(\bm{x}_{t},\bm{\pi}_{n}(\bm{x}_{t}))\right\|}{\sigma^{2}},1 \right\}\right],\] \[\leq\sqrt{J_{\max}}\mathbb{E}_{\bm{\tau}^{\bm{\pi}_{n}}}\left[\sum _{t=0}^{T-1}\frac{(1+\sqrt{d_{x}})\beta_{n-1}(\delta)\left\|\bm{\sigma}_{n-1}( \bm{x}_{t},\bm{\pi}_{n}(\bm{x}_{t}))\right\|}{\sigma^{2}}\right],\] \[=\mathcal{O}\left(T\mathbb{E}_{\bm{\tau}^{\bm{\pi}_{n}}}\left[\sum _{t=0}^{T-1}\frac{(1+\sqrt{d_{x}})\beta_{n-1}(\delta)\left\|\bm{\sigma}_{n-1} (\bm{x}_{t},\bm{\pi}_{n}(\bm{x}_{t}))\right\|}{\sigma^{2}}\right]\right).\]

**Lemma 7** (Regret of planning optimistically for sub-Gaussian noise).: _Let \(\bm{\pi}_{n}^{*}\), \(\bm{\pi}_{n}\) denote the solution to Equation (6) and Equation (7) respectively, and \(\bm{z}_{n,t}^{*}\), \(\bm{z}_{n,t}\) the corresponding state-action pairs visited during their respective trajectories. Furthermore, let Assumption 1 and 2 hold, and relax Assumption 3 to \(\sigma\)-sub Gaussian noise. Then, the following is true for all \(n\geq 0\) with probability at least \(1-\delta\)_

\[J_{n}(\bm{\pi}_{n}^{*})-J_{n}(\bm{\pi}_{n})\leq\mathcal{O}\left(L_{\bm{\sigma}} ^{T-1}\beta_{n-1}^{T}(\delta)T\mathbb{E}_{\bm{\tau}^{\bm{\pi}_{n}}}\left[\sum_ {t=0}^{T-1}\left\|\bm{\sigma}_{n-1,j}(\bm{z}_{n,t})\right\|\right]\right)\]

Proof.: Curi et al. (2020, Lemma 5) bound the regret with the sum of epistemic uncertainties for Lipschitz continuous reward functions, under Assumption 1 and 2 for sub-Gaussian noise (c.f., Rothfuss et al. (2023, Theorem 3.5) for a more rigorous derivation). For the active exploration setting, the reward in episode \(n+1\) is

\[r(\bm{z})=\frac{1}{2}\sum_{j=1}^{d_{x}}\log\left(1+\frac{\bm{\sigma}_{n-1,j}^{2} (\bm{z})}{\sigma^{2}}\right).\]

We show in Lemma 4 that our choice of exploration reward is Lipschitz continuous. Thus, can use the regret bound from Curi et al. (2020). 

Compared to the Gaussian case, \(\sigma\)-sub Gaussian noise has the additional exponential dependence on the horizon \(T\), i.e., the \(\beta_{n}^{T}\) term. This follows from the analysis through Lipschitz continuity. Moreover, as we show in Lemma 2, the regret of planning optimistically is proportional to the change in value under the same optimistic dynamics and policy, but different initial states. The Lipschitz continuity property of our objective allows us to relate the difference in values to the discrepancy in the trajectories. Even for linear systems, trajectories under the same dynamics and policy but different initial states can deviate exponentially in the horizon.

### Proof for general Bayesian models

In this section, we analyze the information gain for general Bayesian models and prove Theorem 1.

**Theorem 3** (Entropy of a RV with finite second moment is upper bounded by Gaussian entropy (Theorem 8.6.5 Cover and Thomas (2006))).: _Let the random vector \(\bm{x}\in\mathbb{R}^{n}\) have covariance \(\bm{K}=\mathbb{E}\left[\bm{x}\bm{x}^{\top}\right]\) (i.e., \(\bm{K}_{ij}=E\left[\bm{x}_{i}\bm{x}_{j}\right],1\leq i,j\leq n\)). Then_

\[H(X)\leq\frac{1}{2}\log((2\pi e)^{n}|\bm{K}|)\]

_with equality if and only if \(\bm{x}\sim\mathcal{N}(\bm{\mu},\bm{K})\) for \(\bm{\mu}=E\left[\bm{x}\right]\)._

**Lemma 8** (Monotonocity of information gain).: _Let \(\bm{\tau}^{\bm{\pi}}\) denote the trajectory induced by the policy \(\bm{\pi}\). Then, the following is true for all \(n\geq 0\), policies \(\bm{\pi}\)_

\[\mathbb{E}_{\mathcal{D}_{1:n}}\left[I\left(\bm{f}_{\bm{\tau}^{ \bm{\pi}}}^{*};\bm{y}_{\bm{\tau}^{\bm{\pi}}}\mid\mathcal{D}_{1:n}\right) \right]\leq\mathbb{E}_{\mathcal{D}_{1:n-1}}\left[I\left(\bm{f}_{\bm{\tau}^{ \bm{\pi}}}^{*};\bm{y}_{\bm{\tau}^{\bm{\pi}}}\mid\mathcal{D}_{1:n-1}\right) \right].\]

Proof.: \[\mathbb{E}_{\mathcal{D}_{1:n}} \left[I\left(\bm{f}_{\bm{\tau}^{\bm{\pi}}}^{*};\bm{y}_{\bm{\tau}^ {\bm{\pi}}}\mid\mathcal{D}_{1:n-1}\right)-I\left(\bm{f}_{\bm{\tau}^{\bm{\pi}}} ^{*};\bm{y}_{\bm{\tau}^{\bm{\pi}}}\mid\mathcal{D}_{1:n}\right)\right]\] \[=\mathbb{E}_{\mathcal{D}_{1:n}}\left[H\left(\bm{y}_{\bm{\tau}^{ \bm{\pi}}}\mid\mathcal{D}_{1:n-1}\right)-H\left(\bm{y}_{\bm{\tau}^{\bm{\pi}}} \mid\bm{f}_{\bm{\tau}^{\bm{\pi}}}^{*},\mathcal{D}_{1:n-1}\right)\right.\] \[-\left.\left(H\left(\bm{y}_{\bm{\tau}^{\bm{\pi}}}\mid\mathcal{D} _{1:n}\right)-H\left(\bm{y}_{\bm{\tau}^{\bm{\pi}}}\mid\bm{f}_{\bm{\tau}^{\bm{ \pi}}}^{*},\mathcal{D}_{1:n}\right)\right]\right.\] \[=\mathbb{E}_{\mathcal{D}_{1:n}}\left[H\left(\bm{y}_{\bm{\tau}^{ \bm{\pi}}}\mid\mathcal{D}_{1:n-1}\right)-H\left(\bm{y}_{\bm{\tau}^{\bm{\pi}}} \mid\bm{D}_{1:n}\right)\right]\] \[+\mathbb{E}_{\mathcal{D}_{1:n}}\left[H\left(\bm{y}_{\bm{\tau}^{ \bm{\pi}}}\mid\bm{f}_{\bm{\tau}^{\bm{\pi}}}^{*}\right)-H\left(\bm{y}_{\bm{\tau} ^{\bm{\pi}}}\mid\bm{f}_{\bm{\tau}^{\bm{\pi}}}^{*}\right)\right]\] \[\geq 0\] (information never hurts)

A direct consequence of Lemma 8 is the following corollary.

**Corollary 4** (Information gain at \(N\) is less than the average gain till \(N\)).: _Let \(\bm{\tau}^{\bm{\pi}}\) denote the trajectory induced by the policy \(\bm{\pi}\). Then, the following is true for all \(N\geq 1\), policies \(\bm{\pi}\),_

\[\mathbb{E}_{\mathcal{D}_{1:N-1}}\left[I\left(\bm{f}_{\bm{\tau}^{ \bm{\pi}}}^{*};\bm{y}_{\bm{\tau}^{\bm{\pi}}}\mid\mathcal{D}_{1:N-1}\right) \right]\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1}}\left[I \left(\bm{f}_{\bm{\tau}^{\bm{\pi}}}^{*};\bm{y}_{\bm{\tau}^{\bm{\pi}}}\mid \mathcal{D}_{1:n-1}\right)\right].\]

Next, we prove Lemma 1, which is central to our proposed active exploration objective in Equation (6).

Proof of Lemma 1.: Let \(\bm{y}_{\bm{\tau}^{\bm{\pi}}}=\left\{\bm{y}_{t}\right\}_{t=0}^{T-1}=\left\{\bm {f}_{t}^{*}+\bm{w}_{t}\right\}_{t=0}^{T-1}\), where \(\bm{f}_{t}^{*}=\bm{f}^{*}(\bm{z}_{t})\). Furthermore, denote with \(\Sigma_{n}(\bm{f}_{0:T-1}^{*})\) the covariance of \(\bm{f}_{0:T-1}^{*}\).

\[I\left(\bm{f}_{\bm{\tau}^{\bm{\pi}}}^{*};\bm{y}_{\bm{\tau}^{\bm{ \pi}}}\mid\mathcal{D}_{1:n}\right) =I\left(\bm{f}_{0:T-1}^{*};\bm{y}_{0:T-1}\mid\mathcal{D}_{1:n}\right)\] \[=H\left(\bm{y}_{0:T-1}\mid\mathcal{D}_{1:n}\right)-H\left(\bm{y} _{0:T-1}\mid\bm{f}_{0:T-1}^{*},\mathcal{D}_{1:n}\right)\] \[\leq\frac{1}{2}\log\left(\left|\sigma^{2}\mathbb{I}+\Sigma_{n}( \bm{f}_{0:T-1}^{*})\right|\right)-\frac{1}{2}\log\left(\left|\sigma^{2}\mathbb{ I}\right|\right)\] (Theorem 3) \[\leq\frac{1}{2}\log\left(\left|\mathrm{diag}\left(\mathbb{I}+ \sigma^{-2}\Sigma_{n}(\bm{f}_{0:T-1}^{*})\right)\right|\right)\] (Hadamard's inequality) \[=\frac{1}{2}\sum_{t=0}^{T-1}\sum_{j=1}^{d_{e}}\log\left(1+\frac{ \bm{\sigma}_{n,j}^{2}(\bm{z}_{t})}{\sigma^{2}}\right).\]

We can leverage the result from Lemma 1 to bound the average mutual information with the sum of epistemic uncertainties.

**Lemma 9** (Average information gain is less than sum of average epistemic uncertainties).: _Let Assumption 3 hold and denote with \(\bm{\bar{\pi}}_{N}\) be the solution of Equation (4). Then, for all \(N\geq 1\) and dataset \(\mathcal{D}_{1:N}\) the following is true_

\[\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1},\bm{\bar{ \tau}}^{\bm{\pi}_{n}}}\left[I\left(\bm{f}^{*}_{\bm{\bar{\tau}}^{\bm{\pi}_{N}}}; \bm{y}_{\bm{\bar{\tau}}^{\bm{\pi}_{N}}}\mid\mathcal{D}_{1:n-1}\right)\right]\] \[\quad\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1},\bm{\bar{\tau}}^{\bm{\pi}_{n}^{*}}}\left[\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}} \left(\frac{1}{2}\log\left(1+\frac{\bm{\sigma}^{2}_{n,j}(\bm{z}^{*}_{n,t})}{ \sigma^{2}}\right)\right)\right],\]

_where \(z^{*}_{n,t}\) are the state-action tuples visited by the solution of Equation (6), i.e., \(\bm{\pi}^{*}_{n}\)._

Proof.: \[\frac{1}{N}\sum_{n=1}^{N} \mathbb{E}_{\mathcal{D}_{1:n-1},\bm{\bar{\tau}}^{\bm{\pi}_{N}}} \left[I\left(\bm{f}^{*}_{\bm{\bar{\tau}}^{\bm{\pi}_{N}}};\bm{y}_{\bm{\bar{\tau }}^{\bm{\pi}_{N}}}\mid\mathcal{D}_{1:n-1}\right)\right]\] \[\quad\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1},\bm{\bar{\tau}}^{\bm{\pi}_{N}}}\left[\left(\sum_{\bm{z}_{t}\in\bm{\bar{\tau}} ^{\bm{\pi}_{N}}}\frac{1}{2}\sum_{j=1}^{d_{x}}\log\left(1+\frac{\bm{\sigma}^{2 }_{n,j}(\bm{z}_{t})}{\sigma^{2}}\right)\right)\right]\] (Lemma 1) \[\quad\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1} }\left[\max_{\bm{\bar{\tau}}\in\Pi}\mathbb{E}_{\bm{\bar{\tau}}^{\bm{\pi}}} \left[\sum_{\bm{z}_{t}\in\bm{\bar{\tau}}_{\bm{\pi}}}\frac{d_{x}}{\sum_{j=1}^{ d_{x}}}\left(\frac{1}{2}\log\left(1+\frac{\bm{\sigma}^{2}_{n,j}(\bm{z}_{t})}{ \sigma^{2}}\right)\right)\right]\right]\] (1) \[\quad=\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1}, \bm{\bar{\tau}}^{\bm{\pi}_{n}}}\left[\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}}\left( \frac{1}{2}\log\left(1+\frac{\bm{\sigma}^{2}_{n,j}(\bm{z}^{*}_{n,t})}{\sigma^{ 2}}\right)\right)\right].\]

Here (1) follows from the tower property. Note that the second expectation in (1) is wrt \(\bm{\bar{\tau}}^{\bm{\pi}}\) conditioned on a realization of \(\mathcal{D}_{1:n-1}\), where the conditioning is captured in the epistemic uncertainty \(\bm{\sigma}_{n}(\cdot)\). 

We use the results from above, to prove Theorem 1.

Proof of theorem 1.: Let \(\bm{\bar{\pi}}_{n}\) denote the solution to Equation (4) at iteration \(n\geq 1\). We first relate the information gain from OpAx to the information gain of \(\bm{\bar{\pi}}_{n}\).

\[\mathbb{E}_{\mathcal{D}_{1:N-1},\bm{\bar{\tau}}^{\bm{\pi}_{N}}} \left[I\left(\bm{f}^{*}_{\bm{\bar{\tau}}^{\bm{\pi}_{N}}};\bm{y}_{ \bm{\bar{\tau}}^{\bm{\pi}_{N}}}\mid\mathcal{D}_{1:N-1}\right)\right]\] (Corollary 4) \[\quad\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1},\bm{\bar{\tau}}^{\bm{\pi}_{n}}}\left[\left(\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}} \frac{1}{2}\log\left(1+\frac{\bm{\sigma}^{2}_{n-1,j}(\bm{z}^{*}_{n,t})}{\sigma^ {2}}\right)\right)\right]\] (Lemma 9) \[\quad=\frac{1}{N}\sum_{n=1}^{N}\left(\mathbb{E}_{\mathcal{D}_{1:n-1 },\bm{\bar{\tau}}_{n}}\left[\sum_{t=0}^{T-1}\frac{1}{2}\sum_{j=1}^{d_{x}}\log \left(1+\frac{\bm{\sigma}^{2}_{n-1,j}(\bm{z}_{n,t})}{\sigma^{2}}\right)\right]+ J_{n}(\bm{\pi}^{*}_{n})-J_{n}(\bm{\pi}_{n})\right)\] \[\quad\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1 }}\left[\mathbb{E}_{\bm{\bar{\tau}}_{\bm{\pi}_{n}}}\left[\sum_{t=0}^{T-1}\frac{ 1}{2}\sum_{j=1}^{d_{x}}\log\left(1+\frac{\bm{\sigma}^{2}_{n-1,j}(\bm{z}_{n,t})}{ \sigma^{2}}\right)\right]\right.\] \[\quad\quad\left.+\mathcal{O}\left(\beta_{n-1}(\delta)T\mathbb{E}_{ \bm{\bar{\tau}}_{\bm{\pi}_{n}}}\left[\sum_{t=0}^{T-1}\left\|\bm{\sigma}_{n-1}( \bm{z}_{n,t})\right\|_{2}\right]\right)\right]\] (Corollary 3)

In summary, the maximum expected mutual information at episode \(N\) is less than the mutual information of OpAx and the sum of model epistemic uncertainties. Crucial to the proof is the regretbound for optimistic planning from Corollary 3.

\[\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1}}\left[ \mathbb{E}_{\bm{\tau}_{\bm{\tau}_{n}}}\left[\sum_{t=0}^{T-1}\frac{1}{2}\sum_{j=1} ^{d_{x}}\log\left(1+\frac{\bm{\sigma}_{n-1,j}^{2}(\bm{z}_{n,t})}{\sigma^{2}} \right)\right]\right.\] \[\qquad\left.+\mathcal{O}\left(T\beta_{n-1}(\delta)\mathbb{E}_{ \bm{\tau}_{\bm{\tau}_{n}}}\left[\sum_{t=0}^{T-1}\|\bm{\sigma}_{n-1}(\bm{z}_{n,t })\|_{2}\right]\right)\right]\] \[=\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1}}\left[ \mathbb{E}_{\bm{\tau}_{\bm{\tau}_{n}}}\left[\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}} \log\left(\sqrt{1+\frac{\bm{\sigma}_{n-1,j}^{2}(\bm{z}_{n,t})}{\sigma^{2}}} \right)\right]\right.\] \[\qquad\left.+\mathcal{O}\left(T\beta_{n-1}(\delta)\mathbb{E}_{ \bm{\tau}_{\bm{\tau}_{n}}}\left[\sum_{t=0}^{T-1}\|\bm{\sigma}_{n-1}(\bm{z}_{n, t})\|_{2}\right]\right)\right]\] \[\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1}} \left[\mathbb{E}_{\bm{\tau}_{\bm{\tau}_{n}}}\left[\sum_{t=0}^{T-1}\sum_{j=1}^{d _{x}}\log\left(1+\frac{\bm{\sigma}_{n-1,j}(\bm{z}_{n,t})}{\sigma}\right)\right]\right.\] \[\qquad\left.+\mathcal{O}\left(T\beta_{n-1}(\delta)\mathbb{E}_{ \bm{\tau}_{\bm{\tau}_{n}}}\left[\sum_{t=0}^{T-1}\|\bm{\sigma}_{n-1}(\bm{z}_{n, t})\|_{2}\right]\right)\right]\] \[\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D}_{1:n-1}} \left[\mathbb{E}_{\bm{\tau}_{\bm{\tau}_{n}}}\left[\sum_{t=0}^{T-1}\sum_{j=1}^ {d_{x}}\frac{\bm{\sigma}_{n-1,j}(\bm{z}_{n,t})}{\sigma}\right]\right.\] \[\qquad\left.+\mathcal{O}\left(T\beta_{n-1}(\delta)\mathbb{E}_{ \bm{\tau}_{\bm{\tau}_{n}}}\left[\sum_{t=0}^{T-1}\|\bm{\sigma}_{n-1}(\bm{z}_{n, t})\|_{2}\right]\right)\right]\qquad\qquad(\log(1+x)\leq x\text{ for }x\geq 0.)\] \[\leq\mathcal{O}\left(\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{ \mathcal{D}_{1:n-1}}\left[T\beta_{n-1}(\delta)\mathbb{E}_{\bm{\tau}_{\bm{\tau }_{n}}}\left[\sum_{t=0}^{T-1}\|\bm{\sigma}_{n-1}(\bm{z}_{n,t})\|_{2}\right] \right]\right)\]

Above, we show that the maximum expected mutual information can be upper bounded with the sum of epistemic uncertainties for the states OpAx visits during learning. Finally, we further upper bound this with the model complexity measure.

\[\mathcal{O}\left(\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\mathcal{D} _{1:n-1}}\left[\beta_{n-1}(\delta)T\mathbb{E}_{\bm{\tau}_{\bm{\tau}_{n}}}\left[ \sum_{t=0}^{T-1}\|\bm{\sigma}_{n-1}(\bm{z}_{n,t})\|_{2}\right]\right]\right)\] \[=\mathcal{O}\left(\frac{1}{N}\sqrt{\left(\mathbb{E}_{\mathcal{D} _{1:N}}\left[\sum_{n=1}^{N}(T\beta_{n-1}(\delta))\mathbb{E}_{\bm{\tau}_{\bm{ \tau}_{n}}}\left[\sum_{t=0}^{T-1}\|\bm{\sigma}_{n-1}(\bm{z}_{n,t})\|_{2}\right] \right]\right)^{2}}\right)\] \[\leq\mathcal{O}\left(\frac{1}{N}T\beta_{N}(\delta)\right)\sqrt{ TN\mathbb{E}_{\mathcal{D}_{1:N}}\left[\sum_{n=1}^{N}\mathbb{E}_{\bm{\tau}_{\bm{ \tau}_{n}}}\left[\sum_{t=0}^{T-1}\|\bm{\sigma}_{n}(\bm{z}_{n,t})\|_{2}^{2} \right]\right]}\right)\] \[\leq\mathcal{O}\left(\beta_{N}(\delta)T^{\nicefrac{{3}}{{2}}} \sqrt{\frac{\mathcal{M}\mathcal{C}_{N}(\bm{f}^{*})}{N}}\right)\]

Theorem 1 gives a bound on the maximum expected mutual information w.r.t. the model complexity. We can use concentration inequalities such as Markov, to give a high probability bound on the information gain. In particular, we have for all \(\epsilon>0\)

\[\Pr\left(I\left(\bm{f}_{\bm{\tau}^{*}\pi_{{}_{N}}}^{*};\bm{y}_{\bm{ \tau}^{*}\pi_{{}_{N}}}\mid\mathcal{D}_{1:N-1}\right)\geq\epsilon\right) \leq\frac{\mathbb{E}_{\mathcal{D}_{1:N-1},\bm{\tau}^{*}\pi_{{}_{N} }}\left[I\left(\bm{f}_{\bm{\tau}^{*}\pi_{{}_{N}}}^{*};\bm{y}_{\bm{\tau}^{*}\pi_ {{}_{N}}}\mid\mathcal{D}_{1:N-1}\right)\right]}{\epsilon}\] \[\leq\mathcal{O}\left(T^{\nicefrac{{3}}{{2}}}\beta_{N}(\delta) \sqrt{\frac{\mathcal{MC}_{N}(\bm{f}^{*})}{N\epsilon^{2}}}\right).\]

### Proof of GP results

This section presents our results for the frequentist setting where the dynamics are modeled using GPs. Since the information gain has no meaning in the frequentist setting, we study the epistemic uncertainty of the GP models.

**Corollary 5** (Monotonicity of the variance).: _For all \(n\geq 0\), and policies \(\bm{\pi}\) the following is true._

\[\sum_{t=0}^{T-1}\sum_{j=1}^{d_{s}}\frac{1}{2}\log\left(1+\frac{ \sigma_{N-1,j}^{2}(\bm{z}_{t})}{\sigma^{2}}\right)\leq\frac{1}{N}\sum_{n=1}^{N }\sum_{t=0}^{T-1}\sum_{j=1}^{d_{s}}\frac{1}{2}\log\left(1+\frac{\bm{\sigma}_{n -1,j}^{2}(\bm{z}_{t})}{\sigma^{2}}\right)\]

Proof.: Follows directly due to the monotonicity of GP posterior variance. 

Next, we prove that the trajectory of Equation (6) at iteration \(n\) is upper-bounded with the maximum information gain.

**Lemma 10**.: _Let Assumption 2 - 4 hold Then, for all \(N\geq 1\), with probability at least \(1-\delta\), we have_

\[\max_{\bm{\pi}\in\Pi}\mathbb{E}_{\bm{\tau}_{\bm{\pi}}}\left[\left( \sum_{t=0}^{T-1}\sum_{j=1}^{d_{s}}\frac{1}{2}\log\left(1+\frac{\sigma_{N,j}^{2 }(\bm{z}_{t})}{\sigma^{2}}\right)\right)\right]\leq\mathcal{O}\left(\beta_{N} (\delta)T^{\nicefrac{{3}}{{2}}}\sqrt{\frac{\gamma_{N}}{N}}\right).\]

_Moreover, relax noise Assumption 3 to \(\sigma\)-sub Gaussian. Then, for all \(N\geq 1\), with probability at least \(1-\delta\), we have_

\[\max_{\bm{\pi}\in\Pi}\mathbb{E}_{\bm{\tau}_{\bm{\pi}}}\left[\left( \sum_{t=0}^{T-1}\sum_{j=1}^{d_{s}}\frac{1}{2}\log\left(1+\frac{\sigma_{N,j}^{2 }(\bm{z}_{t})}{\sigma^{2}}\right)\right)\right]\leq\mathcal{O}\left(L_{\bm{ \sigma}}^{T}\beta_{N}^{T}(\delta)T^{\nicefrac{{3}}{{2}}}\sqrt{\frac{\gamma_{N }}{N}}\right)\]

Proof.: **Gaussian noise case**: Let \(\bm{z}_{n,t}^{*}\) denote the state-action pair at time \(t\) for the trajectory of Equation (6) at iteration \(n\geq 1\) and \(\bm{\pi}_{n}^{*}\) the corresponding policy.

\[\mathbb{E}_{\bm{\tau}_{\bm{\pi}_{n}^{*}}}\left[\left(\sum_{t=0}^{ T-1}\sum_{j=1}^{d_{s}}\frac{1}{2}\log\left(1+\frac{\sigma_{N,j}^{2}(\bm{z}_{N,t}^{* })}{\sigma^{2}}\right)\right)\right]\] \[\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\bm{\tau}_{\bm{\pi}_{n}^ {*}}}\left[\sum_{t=0}^{T-1}\sum_{j=1}^{d_{s}}\frac{1}{2}\log\left(1+\frac{ \sigma_{n,j}^{2}(\bm{z}_{N,t}^{*})}{\sigma^{2}}\right)\right]\] (Corollary 5) \[\leq\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}_{\bm{\tau}_{\bm{\pi}_{n}^ {*}}}\left[\sum_{t=0}^{T-1}\sum_{j=1}^{d_{s}}\frac{1}{2}\log\left(1+\frac{ \sigma_{n,j}^{2}(\bm{z}_{n,t}^{*})}{\sigma^{2}}\right)\right]\] (By definition of \[\bm{\pi}_{n}^{*}\] ) \[\leq\mathcal{O}\left(\beta_{N}(\delta)T^{\nicefrac{{3}}{{2}}} \sqrt{\frac{\mathcal{MC}_{N}(\bm{f}^{*})}{N}}\right)\] (See proof of Theorem 1) \[\leq\mathcal{O}\left(\beta_{N}(\delta)T^{\nicefrac{{3}}{{2}}} \sqrt{\frac{\gamma_{N}}{N}}\right)\] (Curi et al., 2020, Lemma 17)

**Sub-Gaussian noise case**: The only difference between the Gaussian and sub-Gaussian case is the regret term (c.f., Corollary 3 and Lemma 7). In particular, the regret for the sub-Gaussian case leverages the Lipschitz continuity properties of the system (Assumption 2). This results in an exponential dependence on the horizon for our bound. We refer the reader to Curi et al. (2020); Rothfuss et al. (2023) for a more detailed derivation.

Lemma 10 gives a sample complexity bound that holds for a richer class of kernels. Moreover, for GP models, \(\beta_{N}\propto\sqrt{\gamma_{N}}\)(Chowdhury and Gopalan, 2017). Therefore, for kernels, where \(\lim_{N\to\infty}\gamma_{N}^{\nicefrac{{2}}{{3}}/N}\to 0\), we can show convergence (for the Gaussian case). We summarize bounds on \(\gamma_{N}\) from Vakili et al. (2021) in Table 1.

From hereon, we focus on deriving the results for the case of Gaussian noise case. All our results can be easily extended for the sub-Gaussian setting by considering its corresponding bound.

**Lemma 11**.: _The following is true for all \(N\geq 0\) and policies \(\boldsymbol{\pi}\in\Pi\),_

\[\mathbb{E}_{\boldsymbol{\tau_{\boldsymbol{\pi}}}}\left[\max_{ \boldsymbol{z}\in\boldsymbol{\tau^{\boldsymbol{\pi}}}}\sum_{j=1}^{d_{x}}\frac{ 1}{2}\boldsymbol{\sigma}_{N,j}^{2}(\boldsymbol{z})\right]\leq C_{\boldsymbol{ \sigma}}\mathbb{E}_{\boldsymbol{\tau_{\boldsymbol{\pi}}}}\left[\sum_{t=0}^{T-1 }\sum_{j=1}^{d_{x}}\frac{1}{2}\log\left(1+\frac{\boldsymbol{\sigma}_{N,j}^{2} (\boldsymbol{z}_{t})}{\sigma^{2}}\right)\right],\]

_with \(C_{\boldsymbol{\sigma}}=\frac{\boldsymbol{\sigma}_{\max}}{\log(1+\sigma^{ \nicefrac{{2}}{{3}}}\boldsymbol{\sigma}_{\max})}\)._

Proof.: \[C_{\boldsymbol{\sigma}}\mathbb{E}_{\boldsymbol{\tau_{\boldsymbol{ \pi}}}}\left[\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}}\frac{1}{2}\log\left(1+\frac{ \boldsymbol{\sigma}_{N,j}^{2}(\boldsymbol{z}_{t})}{\sigma^{2}}\right)\right]\] \[\geq\mathbb{E}_{\boldsymbol{\tau_{\boldsymbol{\pi}}}}\left[\sum_{ t=0}^{T-1}\sum_{j=1}^{d_{x}}\frac{1}{2}\boldsymbol{\sigma}_{N,j}^{2}( \boldsymbol{z}_{t})\right],\] (Curi et al., 2020, Lemma. 15) \[\geq\mathbb{E}_{\boldsymbol{\tau_{\boldsymbol{\pi}}}}\left[\max_{ \boldsymbol{z}\in\boldsymbol{\tau^{\boldsymbol{\pi}}}}\sum_{j=1}^{d_{x}}\frac{ 1}{2}\boldsymbol{\sigma}_{N,j}^{2}(\boldsymbol{z})\right].\]

**Corollary 6**.: _Let Assumption 2 and 4 hold, and relax noise Assumption 3 to \(\sigma\)-sub Gaussian. Then, for all \(N\geq 1\), with probability at least \(1-\delta\), we have_

\[\max_{\boldsymbol{\pi}\in\Pi}\mathbb{E}_{\boldsymbol{\tau_{\boldsymbol{\pi}}}} \left[\max_{\boldsymbol{z}\in\boldsymbol{\tau^{\boldsymbol{\pi}}}}\sum_{j=1}^ {d_{x}}\frac{1}{2}\boldsymbol{\sigma}_{N,j}^{2}(\boldsymbol{z})\right]\leq \mathcal{O}\left(\beta_{N}(\delta)T^{\nicefrac{{3}}{{2}}}\sqrt{\frac{\gamma_{ N}}{N}}\right).\]

**Lemma 12**.: _Let Assumption 2 and 4 hold, and relax noise Assumption 3 to \(\sigma\)-sub Gaussian. Furthermore, assume \(\lim_{N\to\infty}\beta_{N}^{2}(\delta)\nicefrac{{\gamma_{N}}}{{(k)}}/N\to 0\). Then for all \(N\geq 1\), \(\boldsymbol{z}\in\mathcal{R}\), and \(1\leq j\leq d_{x}\), with probability at least \(1-\delta\), we have_

\[\boldsymbol{\sigma}_{n,j}(\boldsymbol{z})\xrightarrow{\text{a.s.}}0\text{ for }n\to\infty.\]

Proof.: We first show that the expected epistemic uncertainty along a trajectory converges to zero almost surely. Then we leverage this result to show almost sure convergence of all trajectories induced

\begin{table}
\begin{tabular}{l l l} \hline \hline Kernel & \(k(\boldsymbol{x},\boldsymbol{x}^{\prime})\) & \(\gamma_{n}\) \\ \hline Linear & \(\boldsymbol{x}^{\top}\boldsymbol{x}^{\prime}\) & \(\mathcal{O}\left(d\log(n)\right)\) \\ RBF & \(e^{-\frac{\left\|\boldsymbol{x}-\boldsymbol{x}^{\prime}\right\|^{2}}{2l^{2}}}\) & \(\mathcal{O}\left(\log^{d+1}(n)\right)\) \\ Matern & \(\frac{1}{\Gamma(\nu)2^{\nu-1}}\left(\frac{\sqrt{2\nu}\left\|\boldsymbol{x}- \boldsymbol{x}^{\prime}\right\|}{l}\right)^{\nu}B_{\nu}\left(\frac{\sqrt{2\nu} \left\|\boldsymbol{x}-\boldsymbol{x}^{\prime}\right\|}{l}\right)\) & \(\mathcal{O}\left(n^{\frac{d}{2\nu+d}}\log^{\frac{2\nu}{d+d}}(n)\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Maximum information gain bounds for common choice of kernels.

by \(\bm{\pi}\in\Pi\). To this end, let \(S_{n}=\mathbb{E}_{\bm{\tau}_{\bm{\pi}_{n}^{*}}}\left[\left(\sum_{t=0}^{T-1}\sum_{j= 1}^{d_{x}}\frac{1}{2}\log\left(1+\frac{\sigma_{n,j}^{2}(\bm{\pi}_{n,t}^{*})}{ \sigma^{2}}\right)\right)\right]\) for all \(n\geq 0\). So far we have,

\[\Pr\left(S_{n}\leq\mathcal{O}\left(\beta_{N}(\delta)T^{3/2}\sqrt{\frac{\gamma_ {n}}{n}}\right)\right)\geq 1-\delta\]

Consider a sequence \(\{\delta_{n}\}_{n\geq 0}\) such that \(\lim_{n\to\infty}\delta_{n}=0\), and \(\lim_{n\to\infty}\beta_{n}(\delta_{n})T^{3/2}\sqrt{\frac{\gamma_{n}}{n}}\to 0\). Note, for GP models with \(\lim_{N\to\infty}\beta_{N}^{3}(\delta)\gamma_{N}(k)/N\to 0\), such a sequence of \(\delta_{n}\) exists (Chowdhury and Gopalan, 2017). Consider any \(\epsilon>0\) and let \(N^{*}(\epsilon)\) be the smallest integer such that

\[\mathcal{O}\left(\beta_{N^{*}(\epsilon)}(\delta)T^{3/2}\sqrt{\frac{\gamma_{N ^{*}(\epsilon)}}{N^{*}(\epsilon)}}\right)<\epsilon.\]

Then, we have

\[\sum_{n=0}^{\infty}\Pr(S_{n}>\epsilon) =\sum_{n=0}^{N^{*}(\epsilon)-1}\Pr(S_{n}>\epsilon)+\sum_{n=N^{*} (\epsilon)}^{\infty}\Pr(S_{n}>\epsilon)\] \[=\sum_{n=0}^{N^{*}(\epsilon)-1}\Pr(S_{n}>\epsilon)+\sum_{n=N^{*} (\epsilon)}^{\infty}\delta_{n}\] \[\leq N^{*}(\epsilon)+\sum_{n=N^{*}(\epsilon)}^{\infty}\delta_{n}.\]

Note, since \(\lim_{n\to\infty}\delta_{n}=0\), we have

\[\sum_{n=N^{*}(\epsilon)}^{\infty}\delta_{n}<\infty.\]

In particular, \(\sum_{n=0}^{\infty}\Pr(S_{n}>\epsilon)<\infty\) for all \(\epsilon>0\). Therefore, we obtain

\[S_{n}\xrightarrow{\text{a.s.}}0\text{ for }n\to\infty.\]

Define the random variable \(V=\lim_{n\to\infty}\left(\sum_{t=0}^{T-1}\sum_{j=1}^{d_{x}}\frac{1}{2}\log \left(1+\frac{\sigma_{n,j}^{2}(\bm{\pi}_{n,t}^{*})}{\sigma^{2}}\right)\right)\), with \(\bm{z}_{n,t}^{*}\in\bm{\tau}\) and \(\bm{\tau}\sim\bm{\tau}^{\bm{\pi}_{n}^{*}}\). \(V\) represents the sum of epistemic uncertainties of a random trajectory induced by the sequence of policies \(\{\bm{\pi}_{n}\}_{n\geq 0}\). Note \(V\geq 0\), therefore we apply Markov's inequality. Moreover, for all \(\epsilon>0\), we have

\[\Pr\left(V>\epsilon\right)\leq\frac{\mathbb{E}[V]}{\epsilon}=0.\]

Hence, we have

\[\Pr\left(V=0\right)=1\implies V\xrightarrow{\text{a.s.}}0\text{ for }n\to\infty\]

Accordingly, we get for all \(\pi\in\Pi\).

\[\Pr\left(\lim_{n\to\infty}\sum_{\bm{z}_{t}\in\bm{\tau}_{\bm{\pi}}}\sum_{j=1}^ {d_{x}}\frac{1}{2}\log\left(1+\frac{\sigma_{n,j}^{2}(\bm{z}_{t})}{\sigma^{2}} \right)\to 0\right)=1.\] (15)

Assume there exists a \(\bm{z}\in\mathcal{R}\), such that for some \(\epsilon\), \(\sigma_{n,j}^{2}(\bm{z})>\epsilon\) for all \(n\geq 0\). Since, \(\bm{z}\in\mathcal{R}\), there exists a \(t\) and \(\pi\in\Pi\) such that \(p(\bm{z}_{t}=\bm{z}|\pi,\bm{f}^{*})>0\). This implies that \(\Pr(\bm{z}\in\bm{\tau}_{\bm{\pi}})>0\). However, from Equation (15), we have that \(\sigma_{n,j}^{2}(\bm{z})\to 0\) for \(N\to\infty\) almost surely, which is a contradiction. 

Finally, we leverage the results from above to prove Theorem 2.

Proof of Theorem 2.: The proof follows directly from Corollary 6 and Lemma 12.

### Zero-shot guarantees

In this section, we give guarantees on the zero-shot performance of OpAx for a bounded cost function. We focus this section on the case of Gaussian noise. However, a similar analysis can be performed for the sub-Gaussian case and Lipschitz continuous costs. Since the analysis for both cases is similar, we only present the Gaussian case with bounded costs here.

**Corollary 7**.: _Consider the following optimal control problem_

\[\operatorname*{arg\,min}_{\bm{\pi}\in\Pi}J_{c}(\bm{\pi},\bm{f}^ {*})=\operatorname*{arg\,min}_{\bm{\pi}\in\Pi}\mathbb{E}_{\bm{\tau}^{\bm{\pi}}} \left[\sum_{t=0}^{T-1}c(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))\right],\] (16) \[\bm{x}_{t+1}=\bm{f}^{*}(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))+\bm{w}_ {t}\quad\forall 0\leq t\leq T,\]

_with bounded and positive costs. Then we have for all policies \(\bm{\pi}\) with probability at least \(1-\delta\)_

\[J_{c}(\bm{\pi},\bm{\eta}^{P})-J_{c}(\bm{\pi})\leq\mathcal{O}\left(T\mathbb{E} _{\bm{\tau}^{\bm{\pi}_{n}}}\left[\sum_{t=0}^{T-1}\frac{(1+\sqrt{d_{x}})\beta_{n -1}(\delta)\left\|\bm{\sigma}_{n-1}(\bm{z}_{t})\right\|}{\sigma^{2}}\right] \right),\]

_where \(J_{c}(\bm{\pi},\bm{\eta}^{P})=\max_{\bm{\eta}\in\mathbb{R}}J_{c}(\bm{\pi},\bm{ \eta})\)._

Proof.: From Corollary 2 we get

\[J_{c}(\bm{\pi},\bm{\eta}^{P})-J_{c}(\bm{\pi})=\mathbb{E}_{\bm{\tau}^{\bm{\pi}} }\left[\sum_{t=0}^{T-1}J_{r,t+1}(\bm{\pi},\bm{\eta}^{P},\bm{x}_{t+1}^{P})-J_{r,t+1}(\bm{\pi},\bm{\eta}^{P},\bm{x}_{t+1})\right],\]

with \(\bm{x}_{t+1}=\bm{f}^{*}(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))+\bm{w}_{t}\), and \(\bm{x}_{t+1}^{P}=\bm{\mu}_{n}(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))+\beta_{n}(\delta )\bm{\sigma}(\bm{x}_{t},\bm{\pi}(\bm{x}_{t}))\bm{\eta}^{P}(\bm{x}_{t})+\bm{w}_ {t}\). Furthermore, the cost is positive and bounded. Therefore,

\[J_{c}^{2}(\bm{\pi},\bm{\eta},\bm{x})\leq T^{2}c_{\max}^{2},\]

for all \(\bm{x},\bm{\eta},\bm{\pi}\). Accordingly, we can now use the same analysis as in Lemma 2 and get

\[J_{c}(\bm{\pi},\bm{\eta}^{P})-J_{c}(\bm{\pi})\leq\mathcal{O}\left(T\mathbb{E} _{\bm{\tau}^{\bm{\pi}_{n}}}\left[\sum_{t=0}^{T-1}\frac{(1+\sqrt{d_{x}})\beta_{ n-1}(\delta)\left\|\bm{\sigma}_{n-1}(\bm{z}_{t})\right\|}{\sigma^{2}}\right] \right),\]

**Lemma 13**.: _Consider the control problem in Equation (16) and let Assumption 3 and 4 hold. Furthermore, assume for every \(\epsilon>0\), there exists a finite integer \(n^{*}\) such that_

\[\forall n\geq n^{*};\beta_{n}^{\nicefrac{{3}}{{2}}}(\delta)T^{\nicefrac{{1}} {{1}}}\!\left(\frac{\gamma_{n}(k)}{n}\right)^{\frac{1}{4}}\leq\epsilon,\] (17)

_and denote with \(\hat{\bm{\pi}}_{n}\) the minimax optimal policy, i.e., the solution to \(\min_{\bm{\pi}\in\Pi}\max_{\bm{\eta}\in\mathbb{R}}J_{c}(\bm{\pi},\bm{\eta})\). Then for all \(n\geq n^{*}\), we have probability at least \(1-\delta\), \(J_{c}(\hat{\bm{\pi}}_{N})-J_{c}(\bm{\pi}^{*})\leq\mathcal{O}(\epsilon)\)._

Proof of Zero-shot performance.: In Theorem 2 we give a rate at which the maximum uncertainty along a trajectory decreases:

\[\max_{\bm{\pi}\in\Pi}\mathbb{E}_{\bm{\tau}^{\bm{\pi}}}\left[\max_{\bm{z}\in \bm{\tau}^{\bm{\pi}}}\frac{1}{2}\left\|\sigma_{N,j}(\bm{z})\right\|^{2}\right] \leq\mathcal{O}\left(\beta_{N}T^{\nicefrac{{3}}{{2}}}\!\sqrt{\frac{\gamma_{N}( k)}{N}}\right).\]Combining this with Corollary 7 we get

\[J_{c}(\bm{\pi},\bm{\eta}^{P})-J_{c}(\bm{\pi}) \leq\mathcal{O}\left(T\mathbb{E}_{\bm{\tau}^{\bm{\pi}_{n}}}\left[ \sum_{t=0}^{T-1}\frac{(1+\sqrt{d_{x}})\beta_{n-1}(\delta)\left\|\bm{\sigma}_{n-1 }(\bm{z}_{t})\right\|}{\sigma^{2}}\right]\right)\] \[\leq\mathcal{O}\left(T^{2}\beta_{n-1}(\delta)\mathbb{E}_{\bm{ \tau}^{\bm{\pi}_{n}}}\left[\max_{\bm{z}\in\bm{\tau}^{\bm{\pi}_{n}}}\left\|\bm{ \sigma}_{n-1}(\bm{z})\right\|\right]\right)\] \[\leq\mathcal{O}\left(\sqrt{\mathbb{E}_{\bm{\tau}^{\bm{\pi}_{n}}} \left[\max_{\bm{z}\in\bm{\tau}^{\bm{\pi}_{n}}}T^{4}\beta_{n-1}^{2}(\delta) \left\|\bm{\sigma}_{n-1}(\bm{z})\right\|^{2}\right]}\right)\] \[\leq\mathcal{O}\left(\sqrt{\beta_{n}^{3}T^{1/2}\sqrt{\frac{\gamma _{n}(k)}{n}}}\right)\] \[=\mathcal{O}\left(\beta_{n}^{3/2}(\delta)T^{1/4}\left(\frac{ \gamma_{n}(k)}{n}\right)^{\frac{1}{4}}\right)\] \[=\mathcal{O}(\epsilon).\] ( \[\forall n\geq n^{*}\] )

Hence, we have that for each policy \(\bm{\pi}\), our upper bound \(\max_{\bm{\eta}}J_{c}(\bm{\pi},\bm{\eta}^{P})\) is \(\epsilon\) precise, i.e.,

\[\max_{\bm{\eta}\in\Xi}J_{c}(\bm{\pi},\bm{\eta})-J_{c}(\bm{\pi})\leq\mathcal{O }(\epsilon),\forall\bm{\pi}\in\Pi.\] (18)

We leverage this to prove optimality for the minimax solution. For the sake of contradiction, assume that

\[J_{c}(\hat{\bm{\pi}}_{n})>J_{c}(\bm{\pi}^{*})+\mathcal{O}(\epsilon).\] (19)

Then we have,

\[\max_{\bm{\eta}\in\Xi}J_{c}(\bm{\pi}^{*},\bm{\eta}) \geq\min_{\bm{\pi}\in\Pi}\max_{\bm{\eta}\in\Xi}J_{c}(\bm{\pi}, \bm{\eta})\] \[=J_{c}(\hat{\bm{\pi}}_{n},\hat{\bm{\eta}}^{P})\] \[=\max_{\bm{\eta}\in\Xi}J_{c}(\hat{\bm{\pi}}_{n},\bm{\eta})\] \[\geq J_{c}(\hat{\bm{\pi}}_{n})\] \[>J_{c}(\bm{\pi}^{*})+\mathcal{O}(\epsilon)\] (Equation ( 19 )) \[\geq J_{c}(\bm{\pi}^{*},\bm{\eta}^{*,P})\] (Equation ( 18 )) \[=\max_{\bm{\eta}\in\Xi}J_{c}(\bm{\pi}^{*},\bm{\eta}).\]

This is a contradiction, which completes the proof. 

Lemma 13 shows that OpAx also results in nearly-optimal zero-shot performance. The convergence criteria in Equation (17) is satisfied for kernels \(k\) that induce a very rich class of RKHS (c.f., Table 1).

## Appendix B Experiment Details

The environment details and hyperparameters used for our experiments are presented in this section. In Appendix B.2 we discuss the experimental setup of the Fetch Pick & Construction environment (Li et al., 2020) in more detail.

### Environment Details

Table 2 lists the rewards used for the different environments. We train the dynamics model after each episode of data collection. For training, we fix the number of epochs to determine the number of gradient steps. Furthermore, for computational reasons, we upper bound the number of gradient steps by "maximum number of gradient steps". The hyperparameters for the model-based agent are presented in Table 3. Furthermore, we present the iCEM hyperparameters in Table 4. For more detail on the iCEM hyperparameters see Pinneri et al. (2021).

Model-based SAC optimizerFor the reacher and MountainCar environment we use scarce rewards (c.f., Table 2), which require long-term planning. Therefore, a receding horizon MPC approach is less suitable for these tasks. Accordingly, we use a policy network which we train using SAC (Haarnoja et al., 2018). Moreover, our model-based SAC uses transitions simulated through the learned model to train a policy. Accordingly, given a dataset of transitions from the true environment, we sample \(P\) initial states from the dataset. For each of the states, we simulate a trajectory of \(H\) steps using our learned model and the SAC policy. We collect the simulated transitions into a simulation data buffer \(\mathcal{D}_{\text{SIM}}\), which we then use to perform \(G\) gradient steps as suggested by Haarnoja et al. (2018) to train the policy. The algorithm is summarized below, and we provide the SAC hyperparameters in Table 5.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Hyperparameters** & **Pendulum-v1 - GP** & **Pendulum-v1** & **Swimmer** & **Cheetah** \\ \hline Number of samples \(P\) & 500 & 500 & 250 & 200 \\ Horizon \(H\) & 20 & 20 & 15 & 10 \\ Size of elite-set \(K\) & 50 & 50 & 25 & 20 \\ Colored-noise exponent \(\beta\) & 0.25 & 0.25 & 0.25 & 0.25 \\ Number of particles & 10 & 10 & 10 & 10 \\ _CEM-iterations_ & 10 & 10 & 5 & 5 \\ Fraction of elites reused \(\xi\) & 0.3 & 0.3 & 0.3 & 0.3 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Parameters of iCEM optimizer for experiments in Section 5.

\begin{table}
\begin{tabular}{c c} \hline
**Environment** & **Reward \(r_{t}\)** \\ \hline Pendulum-v1 - swing up & \(\theta_{t}^{2}+0.1\dot{\theta}_{t}+0.001u_{t}^{2}\) \\ Pendulum-v1 - keep down & \((\theta_{t}-\pi)^{2}+0.1\dot{\theta}_{t}+0.001u_{t}^{2}\) \\ MountainCar & \(-0.1u_{t}^{2}+100(1\{\bm{x}_{t}\in\bm{x}_{\text{goal}}\})\) \\ Reacher - go to target & \(100(1\{\|\bm{x}_{t}-\bm{x}_{\text{target}}\|_{2}\leq 0.05\}\) \\ Swimmer - go to target & \(-||\bm{x}_{t}-\bm{x}_{\text{target}}||_{2}\) \\ Swimmer - go away from target & \(||\bm{x}_{t}-\bm{x}_{\text{target}}||_{2}\) \\ Cheetah - run forward & \({}^{\upsilon_{\text{forward},t}}\) \\ Cheetah - run backward & \({}^{-\upsilon_{\text{forward},t}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Downstream task rewards for the environments presented in Section 5.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline
**Hyperparameters** & **Pendulum-v1 - GP** & **Pendulum-v1** & **MountainCar** & **Reacher** & **Swimmer** & **Cheetah** \\ \hline Action repeat & N/A & N/A & N/A & N/A & 4 & 4 \\ Exploration horizon & \(100\) & \(200\) & \(200\) & \(50\) & \(1000\) & \(1000\) \\ Downstream task horizon & \(200\) & \(200\) & \(200\) & \(50\) & \(1000\) & \(1000\) \\ Hidden layers & N/A & \(2\) & \(2\) & \(2\) & \(4\) & \(4\) \\ Neurons per layers & N/A & \(256\) & \(128\) & \(256\) & \(256\) & \(256\) \\ Number of ensembles & N/A & \(7\) & \(5\) & \(5\) & \(5\) & \(5\) \\ Batch size & N/A & \(64\) & \(64\) & \(64\) & \(64\) & \(64\) \\ Learning rate & \(0.1\) & \(5\times 10^{-4}\) & \(5\times 10^{-4}\) & \(10^{-3}\) & \(5\times 10^{-4}\) & \(5\times 10^{-5}\) \\ Number of epochs & \(50\) & \(50\) & \(50\) & \(50\) & \(50\) & \(50\) \\ Maximum number of gradient steps & N/A & \(5000\) & \(5000\) & \(6000\) & \(7500\) & \(7500\) \\ \(\beta_{n}\) & \(2.0\) & \(2.0\) & \(1.0\) & \(1.0\) & \(2.0\) & \(2.0\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyperparameters for results in Section 5.

### Opax in the High-dimensional Fetch Pick & Place Environment

#### b.2.1 Environment and Model Details

In our experiments, we use an extension of the Fetch Pick & Place environment to multiple objects as proposed in Li et al. (2020) and further modified in Sancaktar et al. (2022) with the addition of a large table. This is a compositional object manipulation environment with an end-effector-controlled robot arm. The robot actions \(\bm{u}\in\mathbb{R}^{4}\) correspond to the gripper movement in Cartesian coordinates and the gripper opening/closing. The robot state \(\bm{x}_{\text{robot}}\in\mathbb{R}^{10}\) contains positions and velocities of the end-effector as well as the gripper-state (open/close) and gripper-velocity. Each object's state \(\bm{x}_{\text{obj}}\in\mathbb{R}^{12}\) is given by its position, orientation (in Euler angles), and linear and angular velocities.

We follow the free-play paradigm as used in CEE-US (Sancaktar et al., 2022). At the beginning of free play, an ensemble of world models is randomly initialized with an empty replay buffer. At each iteration of free play, a certain number of rollouts (here: 20 rollouts with 100 timesteps each) are collected and then added to the replay buffer. Afterwards, the models in the ensemble are trained for a certain number of epochs on the collected data so far. Note that unlike in the original proposal by Sancaktar et al. (2022), we use Multilayer Perceptrons (MLP) as world models instead of Graph Neural Networks (GNN), for the sake of reducing run-time. This corresponds to the ablation MLP + iCEM presented in Sancaktar et al. (2022), which was shown to outperform all the baselines other than CEE-US with GNNs. As we are interested in exploring the difference in performance via injection of optimism into active exploration, we use the computationally less heavy MLPs in our work. This is reflected in the downstream task performance we report compared to the original CEE-US with GNNs. Details for the environment and models are summarized in Table 6.

After the free-play phase, we use the trained models to solve downstream tasks zero-shot via model-based planning with iCEM. We test for the tasks pick & place, throwing and flipping with 4 objects. The rewards used for these tasks are the same as presented in Sancaktar et al. (2022).

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Parameters** & **MountainCar** & **Reacher** \\ \hline Discount factor & \(0.99\) & \(0.99\) \\ Learning rate actor & \(5\times 10^{-4}\) & \(5\times 10^{-4}\) \\ Learning rate critic & \(5\times 10^{-4}\) & \(5\times 10^{-4}\) \\ Learning rate entropy coefficient & \(5\times 10^{-4}\) & \(5\times 10^{-4}\) \\ Actor architecture & \([64,64]\) & \([250,250]\) \\ Critic architecture & \([256,256]\) & \([250,250]\) \\ Batch size & 32 & 64 \\ Gradient steps \(G\) & 350 & 500 \\ Simulation horizon \(H\) & 4 & 10 \\ Initial state sample size \(P\) & 500 & 2000 \\ Total SAC training steps \(K\) & 35 & 350 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Parameters of model-based SAC optimizer for experiments in Section 5.

#### b.2.2 OpAx Heuristic Variant

In the case of Fetch Pick & Place with an high-dimensional observation space, we implement a heuristic of OpAx. Note that as Fetch Pick & Place is a deterministic environment without noise, we only model epistemic uncertainty.

**OpAx (Heuristic Variant)**

**Input:** Ensemble \(\{\bm{f}_{i}\}_{k=1}^{K}\), \(\epsilon\ll 1\)

**for**\(n\in 1,\dots,N_{\text{max}}\)**do**

Solve optimal control problem till convergence for the system: \(\bm{x}_{t+1}=\bm{f}_{j_{t}}(\bm{x}_{t},\bm{u}_{t})\).

\[\bm{u}_{0:T-1}^{\star},j_{0:T-1}^{\star}=\operatorname*{argmax}_{\bm{u}_{0:T -1},j_{0:T-1}}\sum_{t=0}^{T-1}\sum_{i=1}^{d_{x}}\log\big{(}\epsilon^{2}+\sigma_ {n,i}^{2}(\bm{x}_{t},\bm{u}_{t})\big{)}\] Estimate \[\sigma_{n,i}(\bm{x}_{t},\bm{u}_{t})\text{ with ensemble disagreement.}\]

Rollout \(\bm{u}_{0:T_{1}}^{\star}\) on the system and collect data \(\mathcal{D}_{n}\).

Update models \(\{\bm{f}_{i}\}_{k=1}^{K}\).

#### b.2.3 Controller Parameters

The set of default hyperparameters used for the iCEM controller are presented in Table 7, as they are used during the intrinsic phase for OpAx, CEE-US, and other baselines. The controller settings used for the extrinsic phase are given in Table 8. For more information on the hyperparameters and the iCEM algorithm, we refer the reader to Pinneri et al. (2021).

\begin{table}

\end{table}
Table 6: Environment and model settings used for the experiment results shown in Figure 4.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Parameter** & **Value** \\ \hline Number of samples \(P\) & \(128\) \\ Horizon \(H\) & \(30\) \\ Size of elite-set \(K\) & \(10\) \\ Colored-noise exponent \(\beta\) & \(3.5\) \\ _CEM-iterations_ & \(3\) \\ Noise strength \(\sigma_{\text{init}}\) & \(0.5\) \\ Momentum \(\alpha\) & \(0.1\) \\ use\_mean\_actions & Yes \\ shift\_elites & Yes \\ keep\_elites & Yes \\ Fraction of elites reused \(\xi\) & \(0.3\) \\ Cost along trajectory & sum \\ \hline \hline \end{tabular}
\end{table}
Table 7: Base settings for iCEM as they are used in the intrinsic phase. Same settings are used for all methods.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline
**Task** & \multicolumn{5}{c}{**Controller Parameters**} \\  & Horizon & Colored-noise exponent & use\_mean\_actions & Noise strength & Cost Along \\  & \(h\) & \(\beta\) & & \(\sigma_{\text{init}}\) & Trajectory \\ \hline Pick \& Place & 30 & 3.5 & Yes & 0.5 & best \\ Throwing & 35 & 2.0 & Yes & 0.5 & sum \\ Flipping & 30 & 3.5 & No & 0.5 & sum \\ \hline \hline \end{tabular}
\end{table}
Table 8: iCEM hyperparameters used for zero-shot generalization in the extrinsic phase. Any settings not specified here are the same as the general settings given in Table 7.

## Appendix C Study of exploration intrinsic rewards

The intrinsic reward suggested in Equation (6) takes the log of the model epistemic uncertainty. Another common choice for the intrinsic reward is the epistemic uncertainty or model disagreement without the log (Pathak et al., 2019; Sekar et al., 2020). In the following Lemma, we show that these rewards are closely related.

**Lemma 14**.: _Let \(\sigma_{\max}=\sup_{\bm{z}\in\mathcal{Z};i\geq 0;1\leq j\leq d_{x}}\sigma_{i,j}( \bm{z})\) and \(\sigma>0\). Then for all \(i\geq 0\) and \(j\in\{1,\dots,d_{x}\}\)_

\[\sigma_{i,j}^{2}(\bm{z})\leq\frac{\sigma_{\max}}{\log(1+\sigma^{-2}\sigma_{ \max})}\log(1+\sigma^{-2}\sigma_{i,j}^{2}(\bm{z}))\leq\frac{\sigma^{-2}\sigma_{ \max}}{\log(1+\sigma^{-2}\sigma_{\max})}\sigma_{i,j}^{2}(\bm{z}).\]

Proof.: Curi et al. (2020) derive the first inequality on the left. The second inequality follows since \(\log(1+x)\leq x\) for all \(x\geq 0\). 

Due to this close relation between the two objectives, our theoretical findings also apply to the intrinsic reward proposed by (Pathak et al., 2019). Moreover, empirically we notice in Figure 5 that OpAx performs similarly when the sum of epistemic uncertainties is used instead of the objective in Equation (6). However, our objective can naturally be extended to the case of heteroscedastic aleatoric noise, since it trades off the ratio of epistemic and aleatoric uncertainty.

Figure 5: Comparison of OpAx between intrinsic reward proposed in Equation (6) and the sum of epistemic uncertainties proposed by (Pathak et al., 2019), i.e., OpAx-sum. For both choices of intrinsic rewards, OpAx reduces the epistemic uncertainty and performs well on downstream tasks.