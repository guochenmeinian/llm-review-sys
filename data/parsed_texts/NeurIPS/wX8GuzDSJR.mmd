# What can a Single Attention Layer Learn?

A Study Through the Random Features Lens

 Hengyu Fu

Peking University

2100010881@stu.pku.edu.cn

&Tianyu Guo

UC Berkeley

tianyu_guo@berkeley.edu

&Yu Bai

Salesforce AI Research

yu.bai@salesforce.com

&Song Mei

UC Berkeley

songmei@berkeley.edu

Equal contributions.

###### Abstract

Attention layers--which map a sequence of inputs to a sequence of outputs--are core building blocks of the Transformer architecture which has achieved significant breakthroughs in modern artificial intelligence. This paper presents a rigorous theoretical study on the learning and generalization of a single multi-head attention layer, with activation function replaced by ReLU, which have recently shown comparable performance with the original Softmax activation. We consider the _random feature_ setting where the attention layer has a large number of heads, with randomly sampled frozen query and key matrices, and trainable value matrices. We show that such a random-feature attention layer can express a broad class of target functions that are permutation invariant to the key vectors. We further provide quantitative excess risk bounds for learning these target functions from finite samples, using random feature attention with finitely many heads.

Our results feature several implications unique to the attention structure compared with existing random features theory for neural networks, such as (1) Advantages over standard fully connected random-feature models; (2) Concrete and natural classes of functions that can be learned efficiently by a random-feature attention layer. Additionally, we show that the sampling distribution of the _query-key_ matrix (the product of the query and key matrix) matters--A _biased_ Gaussian random matrix results in better sample complexities over the standard zero-mean counterpart for learning certain natural target functions.Experiments on simulated data corroborate our theoretical findings and further illustrate the interplay between the sample size and the complexity of the target function.

## 1 Introduction

The transformer architecture [86] has achieved remarkable recent successes in many areas of artificial intelligence (AI) such as vision, language, speech, graph processing, reinforcement learning, and more recently general AI capabilities [29, 30, 31, 19, 73, 101, 22, 77, 65, 20]. A central building block in transformers is the _attention layers_[10]--sequence-to-sequence mappings that allow each token within the input sequence to "attend to" other tokens that are most relevant to the present token, and produce outputs based on those tokens. Attention layers implement this mechanism in a compact way that allows them to handle sequences of arbitrary length using a fixed set of parameters, a crucial reason behind their success in handling long input sequences.

Despite its wide applicability, the theoretical properties of attention layers are less well understood. While multi-layer attention networks (transformers) have been shown to be universal approximators for certain classes of functions, such as equivariant sequence-to-sequence functions [102], their results only focus on the expressive power and do not account for learning from finite samples. Another line of work derives generalization bounds for learning with _multi-layer_ transformers in terms of the number of layers, heads, and weight norms [91, 35], yet the results are either instantiated on specific target functions such as sparse boolean functions [35], or generic but arguably elusive function classes such as Turing machines [91]. Understandings about the more basic building block--a _single_ attention layer-- remain largely open. This is in stark contrast with the situation for fully connected neural networks, where there is by now a decent understanding of the learning and generalization in the important basic case of _two-layer_ neural networks on generic and natural function classes (e.g., [8, 51, 7, 40] and many other results along the line). This motivates the following open question:

_What function classes can be learned by a **single attention layer** with benign sample complexities?_

This work makes progress on this problem by studying the learning and generalization with a single attention layer in the _random feature_ setting [74, 76, 28, 100], in which the query and key matrices are frozen at their random initialization, and the value matrices remain to be learnable parameters. Motivated by the attention structure in practical architectures, we consider attention layers that take in a single _query token_\(\mathbf{x}_{0}\in\mathbb{R}^{d}\) and \(N\)_key tokens_\(\{\mathbf{x}_{i}\}_{i\in[N]}\) as the input, and produce a scalar-valued output--A simplified setting capturing the essence (the interaction between the query and keys) of attention models. We study the sample complexity of learning certain target functions (of \(\mathbf{x}_{0:N}\)) using an attention layer with a large but finite number of heads, and finitely many samples.

Our contributions are summarized as follows.

* We show that a Random Feature Attention layer (the RFA model) with a sufficiently large number of heads can express a broad class of target functions that are averages over a generic function of two tokens, which are in particular permutation invariant with respect to the key tokens (Section 3.1). We give several natural examples of target functions in this class (Section 3.3) with concrete bounds on the number of heads and weight norms.
* We derive an \(\widetilde{\mathcal{O}}(\sqrt{B(f_{\star})/n})\) excess risk bound for learning with the RFA model with sufficiently many heads, where \(B(f_{\star})\) is an inherent complexity measure of the target function \(f_{\star}\) and \(n\) is the sample size (Section 3.2). When instantiated on the aforementioned examples, the bounds only depend on the input dimension and not the number of key tokens, improving over a naive two-layer random feature neural network model (RFMLP). Such improvement is expected due to the permutation invariance structure of target functions, aligning with the attention mechanism.
* Towards moving beyond standard random feature settings, we study a _biased_ RFA model where the query-key matrices (product of transposed query matrices and key matrices) are drawn from a distribution with a non-zero mean (more precisely the identity matrix as the mean), motivated by a similar observation on learned attention layers in practice. We show that this model achieves provably superior sample complexity than the standard zero-mean RFA for learning certain functions of the _correlations_ between the query token and key tokens (Section 4).
* Experiments on simulated data verify our theoretical findings in realistic settings of learning from finite samples using a RFA layer with a mild number of heads, and characterize the interplay between the complexity of the target function and the sample size (Section 5).

### Related work

Transformers The Transformer architecture, initially proposed by [86], brought about a revolutionary change in natural language processing and has been widely adopted in large language models such as GPT and BERT [72, 29, 19]. At the core of transformers lie the _attention layers_, which were originally introduced as neural network modules for machine translation tasks [10, 49, 68].

A recent line of work investigated the capabilities of transformers by viewing transformers to be function approximators [102], computational models [91, 70, 99, 15, 56], or algorithms [82], and using transformers to perform synthetic reasoning tasks [103]. Among these works, the closest to our work is [102], which shows that multi-layer transformers can approximate any permutation-equivariant sequence-to-sequence function, and any function if positional encodings are added. Ourpaper instead uses a single attention layer to approximate sequence-to-scalar functions and focuses on the generalization property with quantitative bounds.

In terms of generalization properties of transformers, [35] analyzed the generalization bound of a single attention network through the Rademacher complexity and showed that a single self-attention head could efficiently represent a sparse function of the input sequence. Besides, several works studied the sample complexities of vision transformers [48; 50], and prompt-tuning using attention [67] with special target function classes. Our paper also studies the generalization bound of a single attention network, but from the different perspective of kernel methods, and for a more general class of target functions. The kernel limit of transformers was derived in [43; 98], which shows that multi-head attention architectures behave as Gaussian processes as the number of heads tends to infinity. However, they do not study the representation power of the limiting kernel.

Besides approximation and generalization capabilities, recent work also studied the limitations [42; 14], internal working mechanisms [36; 81; 94; 64], and in-context learning capabilities [19; 96; 37; 88; 3; 26; 41; 52] of Transformer models.

Theory of random features and neural tangent kernelsA recent line of work [28; 51; 33; 32; 6; 7; 104; 66; 25] studied the training dynamics of overparametrized neural networks under certain random initialization, and showed that it converges to a kernel estimator, which corresponds to the "neural tangent kernel" (NTK). These works suggested that one could use kernel or random-feature models [74] to study the properties of deep neural networks.

For NTK of MLPs and their corresponding random-feature models, there is a vast number of literature that studies their approximation power [12; 71; 8], as well as their generalization properties [13; 21; 93; 92; 54; 55; 76; 79; 100; 57; 60; 40; 39]. More recently, a line of work studies the NTK beyond MLPs, including convolution networks [53; 17; 59; 62; 18; 16], residual networks [45; 83; 4], graph networks [97; 47], and transformers [43; 98].

Although the kernel approach is a powerful tool for studying neural networks, it received criticism since it does not capture the feature learning of neural networks. Going beyond the kernel regime, a series of works used the mean field method to establish the evolution of the network parameters via a Wasserstein gradient flow [58; 9; 24; 78]. Several other mechanisms have been proven to obtain superior results over the NTK, including the Quadratic NTK [5; 11; 23; 63], regularization [90], Neural Tangent Hierarchy [34; 44], representation learning [27], and staircase-like mechanisms [1; 2].

## 2 Preliminaries

We consider a sequence of \(N+1\) input tokens \(\mathbf{x}_{0:N}=(\mathbf{x}_{0},\{\mathbf{x}_{i}\}_{i\in[N]})\in\mathcal{X}= (\mathbb{R}^{d})^{N+1}\), where each \(\{\mathbf{x}_{i}\}_{i\in[N]}\subseteq\mathbb{R}^{d}\) represents a sequence of _key vectors_, and \(\mathbf{x}_{0}\) represents the _query vector_. This model simplifies standard self-attention, which maps \(N\) input tokens to \(N\) output tokens, where the output \(i\) only uses input \(i\) as the query token and all of \([N]\) as key tokens. Results obtained in this model can be directly mapped back to full self-attention, by simply concatenating \(N\) outputs generated by our model with \(\mathbf{x}_{0}\) ranging over \(\{\mathbf{x}_{i}\}_{i\in[N]}\). In addition, throughout the paper, we consider scalar-valued attention models, which take the sequence \(\mathbf{x}_{0:N}\) as input and give a scalar output in \(\mathbb{R}\).

Attention layerWe consider a scalar-valued, \(M\)-head, multiplicative attention layer that takes \(\mathbf{x}_{0:N}=(\mathbf{x}_{0},\{\mathbf{x}_{i}\}_{i\in[N]})\) as the input. The attention layer first applies affine (linear with bias) transformations to the input vectors to obtain {query, keys, values} at each head \(m\in[M]\):

\[\begin{split}\mathbf{q}_{m,0}&=\mathbf{Q}_{m}[ \mathbf{x}_{0};1]=:\mathbf{Q}_{m}\widetilde{\mathbf{x}}_{0}\in\mathbb{R}^{d}, \qquad\mathbf{k}_{m,i}=\mathbf{K}_{m}[\mathbf{x}_{i};1]=:\mathbf{K}_{m} \widetilde{\mathbf{x}}_{i}\in\mathbb{R}^{d},\\ v_{m,i}&=\mathbf{v}_{m}^{\top}[\mathbf{x}_{i};1]= \mathbf{v}_{m}^{\top}\widetilde{\mathbf{x}}_{i}\in\mathbb{R},\ \ i\in[N],\end{split}\] (1)

where \(\mathbf{Q}_{m},\mathbf{K}_{m}\in\mathbb{R}^{(d+1)\times d}\), \(\mathbf{v}_{m}\in\mathbb{R}^{d+1}\) are the parameters of the attention layer, and \(\widetilde{\mathbf{x}}_{i}:=[\mathbf{x}_{i};1]\) for a more compact display. Then, it computes the output value by an attention mechanism

\[f(\mathbf{x}_{0:N})=\sum_{m=1}^{M}\frac{1}{N}\sum_{i=1}^{N}f_{m,i}(\mathbf{x}_ {0},\mathbf{x}_{i}),\ \ \ \ \ f_{m,i}(\mathbf{x}_{0},\mathbf{x}_{i})=\sigma(\langle\mathbf{q}_{m,0},\mathbf{k }_{m,i}\rangle)\cdot v_{m,i}\in\mathbb{R}.\] (2)

Above, \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) is an activation function applied entry-wisely to each attention score \(\langle\mathbf{q}_{m,0},\mathbf{k}_{m,i}\rangle\). We choose \(\sigma\) to be the ReLU activation \(\sigma(t)=\max\left\{t,0\right\}\) throughout this paper. Notice that this choice of the attention non-linearity is different from standard transformers [86] with softmax-attention. We remark that we choose to study the (normalized) ReLU attention for theoretical convenience, and this replacement does not change the essence of the attention mechanism. Such a choice is also recently explored in the literatures such as [80] and [95], which show that transformers with ReLU-attention perform as well as standard softmax-attention transformers in certain NLP and CV tasks. Moreover, our results can extend to other activation functions such as the exponential activation \(\sigma(t)=\exp(t)\), which is more similar to the standard Softmax activation. We refer to Section B.5 for a short discussion.

Simplifying the expression, we reparametrize the attention layer (1) and (2) using parameters \(\left\{(\mathbf{W}_{m},\mathbf{v}_{m})\right\}_{m\in[M]}\subseteq\mathbb{R}^{( d+1)\times(d+1)}\times\mathbb{R}^{d+1}\):

\[f_{i,m}(\mathbf{x}_{0:N})=\sigma\Big{(}\widetilde{\mathbf{x}}_{0}^{\top} \mathbf{Q}_{m}\mathbf{K}_{m}\widetilde{\mathbf{x}}_{i}\Big{)}\cdot\langle \mathbf{v}_{m},\widetilde{\mathbf{x}}_{i}\rangle=\sigma\big{(}\langle \mathbf{W}_{m},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top} \rangle\big{)}\cdot\langle\mathbf{v}_{m},\widetilde{\mathbf{x}}_{i}\rangle\,.\] (3)

For technical convenience, we assume all input tokens have unit norm throughout the rest of the paper: \(\|\mathbf{x}_{i}\|_{2}\equiv 1\) so that \(\|\widetilde{\mathbf{x}}_{i}\|_{2}\equiv\sqrt{2}\), for all \(i\in\{0\}\cup[N]\).

Random-feature attention modelsWe consider a random-feature version2 of the multiplicative attention mechanism (3), where the weight matrices \(\left\{\mathbf{W}_{m}\right\}_{m\in[M]}\) have i.i.d. Gaussian entries3:

Footnote 2: A different and closely related model is the Neural Tangent Kernel [46; 32], which is however similar in essence to the random feature model in terms of the sample complexity of learning, e.g. [40].

Footnote 3: Another feasible choice for (4) is to sample the key matrix and the query matrix separately with independent Gaussian entries, which however will produce a mean-zero product matrix similar to (4) in many aspects.

\[(\mathbf{W}_{m})_{ij}\mathbf{\sim_{iid}N}(0,1/4),\ \ \ (m,i,j)\in[M]\times[d+1]^{2}.\] (4)

The variance is chosen to be \(1/4\) without loss of generality: this choice of variance is such that \(\langle\mathbf{W}_{m},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{ \top}\rangle\sim\mathsf{N}(0,1)\) has a unit variance. The weight matrices \(\left\{\mathbf{W}_{m}\right\}_{m\in[M]}\) are then held to be fixed during the entire learning process, whereas the value vectors \(\left\{\mathbf{v}_{m}\right\}_{m\in[M]}\) are the learnable parameters. The random-feature attention model with input \(\mathbf{x}_{0:N}\) is thus given by

\[f_{M}^{\mathbf{W}}(\mathbf{x}_{0:N};\mathbf{V})=\sum_{m=1}^{M}\tfrac{1}{N} \sum_{i=1}^{N}\sigma\big{(}\left\langle\mathbf{W}_{m},\widetilde{\mathbf{x}}_ {0}\widetilde{\mathbf{x}}_{i}^{\top}\right\rangle\big{)}\,\langle\mathbf{v}_{ m},\widetilde{\mathbf{x}}_{i}\rangle\,.\] (5)

Notice that random-feature attention model is linear in the parameter \(\mathbf{V}\), so training this model with a convex loss function gives a convex optimization problem.

Additional notationFor any \(\mathbf{x}\in\mathbb{R}^{d_{1}}\) and \(\mathbf{y}\in\mathbb{R}^{d_{2}}\), let \(\mathbf{x}\otimes\mathbf{y}\in\mathbb{R}^{d_{1}\times d_{2}}\) denote their tensor product (outer product), and \(\mathbf{x}^{\otimes n}:=\mathbf{x}\otimes\cdots\otimes\mathbf{x}\) denote the \(n\)-fold self tensor product of \(\mathbf{x}\). For a tensor \(\mathbf{A}\), we use \(\|\mathbf{A}\|_{\mathsf{F}_{t}}\) to denote its Frobenius norm. For a function \(f:\mathcal{X}\rightarrow\mathbb{R}\), we use \(\|f\|_{\infty}\) to denote its \(L^{\infty}\) norm. We use \(\mathcal{O}(\cdot)\) (resp. \(\Theta(\cdot)\)) for standard Big-O (resp. Big-Theta) relations. We use \(\widetilde{\mathcal{O}}(\cdot)\) for hiding the multiplicative terms that are logarithmic in problem parameters, including \((M,d,n,N,\delta^{-1})\). We use \(\mathrm{Poly}(p)\) to denote a polynomial of \(p\) that is less than \(p^{\mathcal{C}}\) for some universal constant \(0<C<\infty\).

## 3 Learning with random-feature attention models

In this section, we study the expressivity and generalization of random-feature attention models. We will consider a broad class of target functions that can be well approximated and is efficiently learnable by random-feature attention models.

### Expressivity of random-feature attention

Consider a broad class of permutation invariant4 target functions \(f_{\star}:\mathcal{X}\rightarrow\mathbb{R}\) that takes form

Footnote 4: A function \(f(\mathbf{x}_{0},\mathbf{x}_{1},...,\mathbf{x}_{N})\) is permutation invariant (with respect to \(\mathbf{x}_{1:N}\)) if \(f(\mathbf{x}_{0},\mathbf{x}_{1},...,\mathbf{x}_{N})=f(\mathbf{x}_{0}, \mathbf{x}_{\sigma(1)},...,\mathbf{x}_{\sigma(N)})\) for any permutation \(\sigma:[N]\rightarrow[N]\). We consider permutation invariant target functions since attention layers can only fit these functions due to the structure of attention models.

\[f_{\star}(\mathbf{x}_{0:N})=\tfrac{1}{N}\sum_{i=1}^{N}F(\mathbf{x}_{0},\mathbf{ x}_{i}).\] (6)Assume that there exists symmetric tensors \(\{\mathbf{f}_{\star}\in\mathbb{R}^{d^{\prime+1}}\}_{r,s\geq 0}\) such that \(F:\mathbb{R}^{2d}\to\mathbb{R}\) admits representation

\[F(\mathbf{x}_{0},\mathbf{x}_{i})=\sum_{r,s\geq 0}^{\infty}\left\langle\mathbf{x}_{0 }^{\otimes r}\otimes\mathbf{x}_{i}^{\otimes s},\mathbf{f}_{rs}\right\rangle.\] (7)

Note that such an expression allows \(F(\mathbf{x}_{0},\mathbf{x}_{i})\) to be any general nonlinear function that admits convergent Taylor expansions. In particular, any polynomials of \([\mathbf{x}_{0},\mathbf{x}_{i}]\) (e.g., \(\boldsymbol{\beta}^{\top}\mathbf{x}_{0}\), \(\boldsymbol{\beta}^{\top}\mathbf{x}_{i}\), and \(\left\langle\mathbf{x}_{0},\mathbf{S}\mathbf{x}_{i}\right\rangle\) for some \(\boldsymbol{\beta}\in\mathbb{R}^{d}\) and \(\mathbf{S}\in\mathbb{R}^{d^{2}}\)) are within this function class. We will discuss more specific target functions in Section 3.3.

**Theorem 1** (Expressivity of RFA model).: _Suppose function \(f_{\star}:\mathcal{X}\to\mathbb{R}\) takes form (6). Then for any input distribution \(P\) on \(\mathcal{X}\), with probability at least \(1-\delta\) (over \(\left\{\mathbf{W}_{m}\right\}_{m\in[M]}\) sampled from (4)), there exists an \(M\)-head RFA model (5) with coefficients \(\mathbf{V}=\left\{\mathbf{v}_{m}\right\}_{m\in[M]}\subseteq\mathbb{R}^{d+1}\) that approximates \(f_{\star}\) in \(L^{2}(P)\) up to error_

\[\mathbb{E}_{\mathbf{x}_{0:N}\sim P}\Big{[}\big{(}f_{\star}(\mathbf{x}_{0:N})- f_{M}^{\mathbf{W}}(\mathbf{x}_{0:N};\mathbf{V})\big{)}^{2}\Big{]}\leq\mathcal{O} \Big{(}\frac{(d^{2}+\log M)B(f_{\star})\delta^{-1}}{M}\Big{)}.\] (8)

_In addition, the norms of the weight of this random-feature attention model are bounded as_

\[\sum_{m=1}^{M}\left\|\mathbf{v}_{m}\right\|_{2}\leq\mathcal{O}\bigg{(}\sqrt{B( f_{\star})}+\sqrt{\frac{B(f_{\star})\delta^{-1}}{M}}\bigg{)},\qquad\sum_{m=1}^{M} \left\|\mathbf{v}_{m}\right\|_{2}^{2}\leq\mathcal{O}\bigg{(}\frac{B(f_{\star} )\delta^{-1}}{M}\bigg{)}.\] (9)

_Here \(B(f_{\star})\) is a complexity measure of \(f_{\star}\) defined as_

\[B(f_{\star})=\sum_{k=0}^{\infty}C_{k}\sum_{\max\{r,s\}=k}\left\|\mathbf{f}_{rs }\right\|_{\mathsf{Fr}}^{2},\qquad C_{k}=k^{4.5}4^{k}\lor 1.\] (10)

_In case where \(f_{\star}\) admits multiple representations of the form (7), \(B(f_{\star})\) is the infimum of the right-hand-side over all such representations._

The proof of Theorem 1 is contained in Appendix B.1. Our proof relies on standard analyses of infinite-width random feature model with ReLU-Gaussian kernel, combined with a sampling argument to obtain approximation with finite-width.

This theorem is applicable to general functions with a finite \(B(f_{\star})\) norm. The \(4^{k}\) scaling of \(C_{k}\) in the summand of equation (10) seemingly confines the target function class to those with exponentially fast decaying \(\left\|\mathbf{f}_{rs}\right\|_{\mathsf{Fr}}\), which suggests a relatively narrow target function class. However, as we will demonstrate in the forthcoming examples, this class includes a diverse range of functions.

### Generalization and sample complexity of learning

Given \(n\) samples \(\{\mathbf{x}_{0:N}^{(j)},y_{j}\}_{j\in[n]}\sim_{\mathrm{iid}}\mathsf{P}\), where \(\mathbf{x}_{0:N}^{(j)}=\{\mathbf{x}_{i}^{(j)}\}_{0\leq i\leq N}\) is the \(j\)-th token sequence with length \(N+1\), and \(y_{j}\) is the label corresponding to the \(i\)-th token sequence. Assume that we are given a loss function \(\ell(\widehat{y},y)\) that is 1-Lipschitz in \(\widehat{y}\), and \(\ell(0,y)\leq 1\) for any \(y\). The population risk is then given by \(L_{D}(f)=\mathbb{E}_{(\mathbf{x}_{0:N},y)\sim\mathsf{P}}[\ell(f(\mathbf{x}_{0:N} ),y)]\). We consider the empirical risk minimization (ERM) over the RFA model (5),

\[\widehat{\mathbf{V}}=\arg\min_{\mathbf{V}\in\mathcal{V}_{M}}\widehat{L}_{D}(f_ {M}^{\mathbf{W}}(\cdot;\mathbf{V})),\qquad\widehat{L}_{D}(f)=\tfrac{1}{n}\sum_ {j=1}^{n}\ell(f(\mathbf{x}_{0:N}^{(j)}),y_{j}),\] (11)

where the constrained class \(\mathcal{V}_{M}\) is given by

\[\mathcal{V}_{M}=\left\{\mathbf{V}=\left\{\mathbf{v}_{m}\right\}_{m=1}^{M}:\ \sum_{m=1}^{M}\left\|\mathbf{v}_{m}\right\|_{2}\leq K_{1},\sum_{m=1}^{M} \left\|\mathbf{v}_{m}\right\|_{2}^{2}\leq K_{2}/M\right\},\] (12)

with \(K_{1}\) and \(K_{2}\) being two constants. Theorem 2 below provides the excess risk bound for the empirical risk minimizer.

**Theorem 2**.: _Assume \(M>\delta^{-1}\) and \(n>\log(dM)\). Let \(f_{\star}\) be the minimizer of the population risk \(L_{D}(f)\) within the target function class (6) (7). Let \(\widehat{f}_{M}^{\mathbf{W}}=f_{M}^{\mathbf{W}}(\cdot;\widehat{\mathbf{V}})\) be the empirical risk minimizer given by (11), where in (12) we choose \(K_{1}=C\sqrt{B(f_{\star})}\) and \(K_{2}=CB(f_{\star})\delta^{-1}\), with \(C\) being a constant. Then for any joint distribution \(\mathsf{P}\), with probability at least \(1-\delta\) over \(\left\{\mathbf{W}_{m}\right\}_{m\in[M]}\) sampled according to (4) and \(\left\{(\mathbf{x}_{0:N}^{(j)},y_{j})\right\}_{j\in[n]}\sim_{\mathrm{iid}} \mathsf{P}\), the excess risk is bounded by_

\[L_{D}(\widehat{f}_{M}^{\mathbf{W}})-L_{D}(f_{\star})\leq\widetilde{\mathcal{O}} \Bigg{(}\sqrt{B(f_{\star})}\Bigg{[}\sqrt{\frac{1}{n}}+\sqrt{\frac{d^{2}\delta^{ -1}}{M}}\Bigg{]}\Bigg{)}.\] (13)The proof of Theorem 2 is contained in Appendix B.2. The proof mostly uses the Rademacher complexity bound for the supremum of empirical process. The main non-trivial technical challenge lies in showing the concentration of \(\sup_{f\in\mathcal{V}_{M}}|\widehat{L}_{D}(f)-L_{D}(f)|\), which cannot be simply controlled due to the unboundedness of the infinity norm of functions in the target function class \(\mathcal{V}_{M}\). We dealt with this subtlety by a carefully decomposition of \(\sup_{f\in\mathcal{V}_{M}}|\widehat{L}_{D}(f)-L_{D}(f)|.\) The seemingly unnatural constraint set (12) is used in bounding different terms in this decomposition.

### Examples and comparison

We next give the sample complexity for learning several examples of target functions using the random-feature attention model. We will compare its sample complexity for learning these functions with that of the standard random-feature model [74] (thereafter, we call it the random-feature MLP model, in short RFMLP model). In the RFMLP model, we view \(\mathbf{x}_{0:N}\) as an input vector instead of a sequence of vectors denoted as \(\mathrm{vec}(\mathbf{x}_{0:N})=[\mathbf{x}_{0};\mathbf{x}_{1};\ldots;\mathbf{ x}_{N};1]\in\mathbb{R}^{d(N+1)+1}\). The RFMLP is given by

\[f_{M}^{\text{MLP}}(\mathbf{x}_{0:N};\mathbf{v})=\sum_{m=1}^{M} \sigma\big{(}\left<\mathbf{w}_{m},\mathrm{vec}(\mathbf{x}_{0:N})\right>\big{)} \cdot v_{m},\quad\{\mathbf{w}_{m}\}_{m\in[M]\sim\mathrm{iid}}\mathsf{N}( \mathbf{0},\mathbf{I}/(N+2)).\] (14)

We choose the variance of random weights \(\mathbf{w}_{m}\) to be \(1/(N+2)\) to ensure that \(\langle\mathbf{w}_{m},\mathrm{vec}(\mathbf{x}_{0:N})\rangle\sim\mathsf{N}(0,1)\) has unit variance. The generalization and approximation properties of the random-feature MLP model have been well-studied in the literature, for example, [7, 8, 60].

We instantiate Theorem 2 on three concrete examples of target functions (calculations of the excess risks in Appendix B.4, where the result for RFMLP are adapted5 from Arora et al. [7]). In all three cases, the target functions are permutation invariant with respect to \(\left\{\mathbf{x}_{i}\right\}_{i\in[N]}\), by which we naturally expect RFA to achieve better sample complexity than RFMLP in accordance with this structure. Although the comparsion between RFA and RFMLP is based on comparing upper bounds on the sample complexity of both models, existing work has also derived lower bounds on the sample complexity of RFMLP, which aligns with the upper bound for RFMLP we used. We do not invoke these lower bounds, as they apply to a special case with a uniform distributional assumption on the input tokens.

Footnote 5: By deriving the corresponding results for Random Features instead of Neural Tangent Kernels.

**Example 1** (Functions of \(\mathbf{x}_{0}\)): We consider functions of \(\mathbf{x}_{0}\) (no dependence on \(\mathbf{x}_{1:N}\)) of the form

\[f_{\star}(\mathbf{x}_{0:N})=\sum_{k=0}^{\infty}\left<\mathbf{x}_{0}^{\otimes k },\mathbf{A}_{k}\right>,\ \ \mathbf{A}_{k}\in\mathbb{R}^{d^{k}},\quad\text{ with }B(f_{\star})=\sum_{k=0}^{ \infty}C_{k}\left\|\mathbf{A}_{k}\right\|_{\mathsf{Fr}}^{2}\text{ by (\ref{eq:def}).}\]

By Theorem 2, setting \(M=\Theta(d^{2}n)\), the excess risk bound gives \(\widetilde{\mathcal{O}}(\sqrt{\sum_{k=0}^{\infty}k^{4.5}4^{k}\|\mathbf{A}_{k} \|_{\mathsf{Fr}}^{2}/n})\). \(\Diamond\)

As a special case, consider \(f_{\star}(\mathbf{x}_{0:N})=(\boldsymbol{\beta}^{\top}\mathbf{x}_{0})^{p}\), which corresponds to taking \(\mathbf{A}_{k}=\boldsymbol{\beta}^{\otimes p}\) for \(k=p\) and \(\mathbf{A}_{k}=\mathbf{0}\) for \(k\neq p\). The above excess risk of RFA model and the RFMLP model scales as

\[\texttt{RFA}:\widetilde{\mathcal{O}}\Big{(}\mathrm{Poly}(p)\sqrt{4^{p}\| \boldsymbol{\beta}\|_{2}^{2p}/n}\Big{)},\qquad\texttt{RFMLP}:\widetilde{ \mathcal{O}}\Big{(}\mathrm{Poly}(p)\sqrt{(N+2)^{p}\|\boldsymbol{\beta}\|_{2}^{2 p}/n}\Big{)}.\]

Compared to the RFMLP model, the RFA model significantly reduces the necessary sample size by a factor of \((N/4)^{p}\).

**Example 2** (Average of functions of \(\mathbf{x}_{i}\)): We consider average of functions of \(\mathbf{x}_{i}\) of the form

\[f_{\star}(\mathbf{x}_{0:N})=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=0}^{ \infty}\langle\mathbf{x}_{i}^{\otimes k},\mathbf{A}_{k}\rangle,\ \mathbf{A}_{k}\in\mathbb{R}^{d^{k}},\quad\text{with }B(f_{ \star})=\sum_{k=0}^{\infty}C_{k}\left\|\mathbf{A}_{k}\right\|_{\mathsf{Fr}}^{2} \text{ by (\ref{eq:def}).}\]

Theorem 2 then gives an \(\widetilde{\mathcal{O}}(\sqrt{\sum_{k=0}^{\infty}k^{4.5}4^{k}\|\mathbf{A}_{k} \|_{\mathsf{Fr}}^{2}/n})\) excess risk, same as Example 1. \(\Diamond\)

As a specific example, consider \(f_{\star}=\frac{1}{N}\sum_{i=1}^{N}\psi(\langle\boldsymbol{\beta},\mathbf{x}_ {i}\rangle)\) with \(\psi(z)=z\arctan(z/\eta)\) for some \(\eta>2\), \(\|\boldsymbol{\beta}\|_{2}=1\). Using the power series expansion of \(\psi\), the excess risk bound of RFA model and the RFMLP model scale as

\[\texttt{RFA}:\widetilde{\mathcal{O}}\Big{(}\sqrt{\sum_{k=1}^{ \infty}k^{4.5}(2/\eta)^{2k}/n}\Big{)}=\widetilde{\mathcal{O}}(\sqrt{1/n}), \quad\texttt{RFMLP}:\widetilde{\mathcal{O}}\Big{(}\sqrt{\sum_{k=1}^{\infty}k^ {4.5}[(N+2)/(2\eta)]^{2k}/n}\Big{)}.\]The latter diverges whenever \(\eta\leq(N+2)/2\), in which case the bound is meaningless.

**Example 3** (Correlation-weighted functions): \(f_{\star}\) is the following function:

\[f_{\star}(\mathbf{x}_{0:N})=\frac{1}{N}\sum_{i=1}^{N}F(\langle\mathbf{x}_{0}, \mathbf{S}\mathbf{x}_{i}\rangle)G(\mathbf{x}_{i}),\quad F(t)=\sum_{k=0}^{ \infty}a_{k}\cdot t^{k},\quad G(\mathbf{x}_{i})=\sum_{k=0}^{\infty}\left\langle \mathbf{x}_{i}^{\otimes k},\mathbf{G}_{k}\right\rangle,\]

for \(\mathbf{S}\in\mathbb{R}^{d\times d}\), \(\{a_{k}\}_{k\geq 0}\subseteq\mathbb{R}\), \(\mathbf{G}_{k}\in\mathbb{R}^{d^{k}}\). This target function fully exploits the representation power of the attention layer. Eq. (10) gives \(B(f_{\star})=\mathcal{O}(\sum_{k=0}^{\infty}C_{k}(\sum_{r+s=k}a_{r}^{2}|| \mathbf{S}||_{\mathbf{r}}^{2r}||\mathbf{G}_{s}||_{\mathbf{F}_{\star}}^{2}))\). \(\Diamond\)

As a specific example, consider \(f_{1,\star}=\frac{1}{N}\sum_{i=1}^{N}\left\langle\mathbf{x}_{0},\mathbf{x}_{i }\right\rangle^{p}\), corresponding to taking \(\mathbf{S}=\mathbf{I}_{d}\), \(F(t)=t^{p}\), and \(G\equiv 1\). The excess risk bound of RFA (by Theorem 2) and RFMLP scale as

\[\texttt{RFA}:\widetilde{\mathcal{O}}\Big{(}\mathrm{Poly}(p)\sqrt{(4d)^{p}/n} \Big{)},\qquad\texttt{RFMLP}:\widetilde{\mathcal{O}}\Big{(}\mathrm{Poly}(p) \sqrt{[(N+2)d]^{p}/n}\Big{)}.\]

As another example, consider \(f_{2,\star}=\frac{1}{N}\sum_{i=1}^{N}\cos(\langle\mathbf{x}_{0},\mathbf{x}_{i }\rangle)\langle\mathbf{x}_{i}^{\otimes p},\mathbf{G}\rangle\) with \(\|\mathbf{G}\|_{\mathbf{F}_{\star}}=1\). Then the excess risk bound of RFA and RFMLP scale as

\[\texttt{RFA}:\widetilde{\mathcal{O}}\bigg{(}\mathrm{Poly}(pd)\sqrt{e^{4\sqrt{ d}}4^{p}/n}\bigg{)},\qquad\texttt{RFMLP}:\widetilde{\mathcal{O}}\bigg{(} \mathrm{Poly}(pNd)\sqrt{e^{2(N+2)\sqrt{d}}N^{p}/n}\bigg{)}.\]

RFA reduces the required sample size by factors of \((N/4)^{p}\) for \(f_{1,\star}\) and \(\exp(N\sqrt{d})\) for \(f_{2,\star}\).

## 4 Expressivity of biased random-feature attention model

We now move beyond the Gaussian weight assumption by exploring alternative possibilities for the weight distribution in the attention heads. We observe empirically that the weight matrices in transformer architectures learned in practice are often more similar to the identity matrix than a mean-zero matrix (Figure 1; see the details in Appendix D.1. This is also observed in a recent and concurrent work [84]).

Towards understanding this effect, we consider an alternative attention model with _biased_ random weights, where the bias is a fixed matrix \(\mathbf{W}_{0}\in\mathbb{R}^{(d+1)\times(d+1)}\):

\[f_{M}^{\mathbf{W},\mathbf{W}_{0}}(\mathbf{x}_{0:N};\mathbf{V})=\sum_{m=1}^{M }\frac{1}{N}\sum_{i=1}^{N}\sigma\big{(}\langle\mathbf{W}_{0}+\mathbf{W}_{m}, \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\rangle\big{)} \left\langle\mathbf{v}_{m},\widetilde{\mathbf{x}}_{i}\right\rangle.\] (15)

Here \(\{\mathbf{W}_{m}\}_{m\in[M]}\) are again Gaussian random matrices sampled according to (4). The biased random-feature attention model is similar to (5) except that a bias weight \(\mathbf{W}_{0}\) is added. Motivated by our observation, we choose \(\mathbf{W}_{0}=[\mathbf{I}_{d\times d},\mathbf{0}_{d\times 1};\mathbf{0}_{1 \times d},0]\in\mathbb{R}^{(d+1)\times(d+1)}\), so that the diagonal elements of \(\mathbf{W}_{0}+\mathbf{W}_{m}\) will be on average larger than the off-diagonal elements.

Figure 1: Visualization of weight matrices of the 2nd, 5th, 8th, and 11th layers of the BERT-Base model. Each row contains weight matrices of a layer. All matrices are clipped to the top-left \(32\times 32\) block. Lighter color indicates a larger absolute value.

### Expressivity of biased random-feature attention

Given the formulation of biased random-feature attention models (thereafter, we call it the biased random-feature attention model, in short BRFA model), a natural conjecture is that this model can better fit functions that are the average of function of \(\langle\mathbf{x}_{0},\mathbf{x}_{i}\rangle\). We here show that this is indeed the case. In particular, we consider a broad class of target functions \(g_{\star}:\mathcal{X}\rightarrow\mathbb{R}\) that take forms

\[g_{\star}(\mathbf{x}_{0:N})=\tfrac{1}{N}\sum_{i=1}^{N}F(\langle \mathbf{x}_{0},\mathbf{x}_{i}\rangle)G(\mathbf{x}_{0},\mathbf{x}_{i})\] \[F(t)=\sum_{k=0}^{\infty}a_{k}t^{k},\quad G(\mathbf{x}_{0}, \mathbf{x}_{i})=\left\langle\widetilde{\mathbf{x}}_{i}^{\otimes 3}\otimes \widetilde{\mathbf{x}}_{0}^{\otimes 2},\mathbf{A}_{\star}\right\rangle.\] (16)

Here the scalars \(\{a_{k}\}_{k\geq 0}\subseteq\mathbb{R}\) and the tensor \(\mathbf{A}_{\star}\in\mathbb{R}^{d^{8}}\) parameterizes \(g_{\star}\). As we will explain in Section 4.3, confining \(G\) to be a degree-\((3,2)\) polynomial in \((\mathbf{x}_{i},\mathbf{x}_{0})\) is essential to our theoretical results. Our next theorem provides the excess risk of learning target function \(g_{\star}\) using the BRFA model (15).

**Theorem 3**.: _Given the same setting and assumptions as in Theorem 2, when the population risk minimizer gives \(f_{\star}=g_{\star}\), with probability at least \(1-\delta\), we have_

\[L_{D}(\widehat{f}_{M}^{\mathbf{W},\mathbf{W}_{0}})-L_{D}(g_{\star})=\widetilde {\mathcal{O}}\Bigg{(}\inf_{L}\bigg{[}\sqrt{B(g_{\star},L)}\Big{(}\sqrt{\frac{1 }{n}}+\sqrt{\frac{d^{2}\delta^{-1}}{M}}\Big{)}+\varepsilon_{L}\left\|g_{\star} \right\|_{\infty}\bigg{]}\Bigg{)},\] (17)

_where \(\varepsilon_{L}=1/[2^{L+1}(L+1)!]\) and_

\[B(g_{\star},L)=\left\|\mathbf{A}_{\star}\right\|_{\mathsf{F}_{\mathsf{r}}}^{2 }\cdot(\sum_{k=0}^{\infty}|a_{k}|\cdot C_{k})^{2},\quad\text{with }C_{k}=(2L+k)^{(k+3)/2}8^{L+k/2}.\] (18)

The proof of Theorem 3 is contained in Appendix C. We provide the intuitions of the result and an overview of the proof technique in Section 4.3.

### Examples and comparison

Compared to the target functions (7) discussed in Section 3.1, functions in (16) may not express the average of arbitrary functions of \(\mathbf{x}_{0}\) and \(\mathbf{x}_{i}\), but are well-suited to express functions of correlations. Consequently, we anticipate that the BRFA model will outperform the RFA model in learning functions of correlations. We will now present three concrete examples of target functions (16), and compare the excess risk of the BRFA model to that of the RFA model. The proof of excess risk is contained in Appendix C.3.

**Example 4** (Low degree polynomials): Consider average of polynomials of \(\mathbf{x}_{i}\) and \(\mathbf{x}_{0}\),

\[g_{\star}=\frac{1}{N}\sum_{i=1}^{N}\langle\mathbf{x}_{i}^{\otimes 3}\otimes \mathbf{x}_{0}^{\otimes 2},\mathbf{A}\rangle,\quad\text{with }B(g_{\star},L)=\left\|\mathbf{A} \right\|_{\mathsf{F}_{\mathsf{r}}}^{2}L^{3}8^{2L}\quad\text{by \eqref{eq:def}}.\]

For any \(\eta>0\), if we take \(n\geq\exp(\exp(\Theta(1/\eta)))\), \(L=\Theta((1+\log\log n)^{-1}\log n)\), and \(M=\Theta(d^{2}n)\), the excess risk will scale as \(\widetilde{\mathcal{O}}(\sqrt{\|\mathbf{A}\|_{\mathsf{F}_{\mathsf{r}}}^{2}/n^{ 1-\eta}})\). \(\Diamond\)

Compared with the excess risk of the RFA model as detailed in Example 2, the excess risk bound of the BRFA model loses a factor of \(n^{-\eta/2}\).

**Example 5** (Functions of correlations): Consider a special case of functions of correlations,

\[g_{\star}=\frac{1}{N}\sum_{i=1}^{N}\left\langle\mathbf{x}_{0},\mathbf{x}_{i} \right\rangle^{p}\left\langle\boldsymbol{\beta},\mathbf{x}_{i}\right\rangle, \quad\boldsymbol{\beta}\in\mathbb{S}^{d-1},\ \text{with }B(g_{\star},L)=(2L+p)^{p+3}8^{2L+p}\ \text{ by \eqref{eq:def}}.\]

For any \(\eta>0\), choosing the same parameters \((n,L,M)\) as Example 4, the excess risk bound scales as \(\widetilde{\mathcal{O}}(\sqrt{(\log n+p)^{(p+3)}8^{p}/n^{1-\eta}})\). \(\Diamond\)

Consider the required sample size \(n_{\star}\) to reach an accuracy of \(0.01\). The BRFA model requires \(n_{\star}=\widetilde{\mathcal{O}}((8p+48)^{p+3})\), whereas the RFA model requires \(n_{\star}=\widetilde{\mathcal{O}}((4d)^{p})\). Thus, in comparison to the RFA model, the BRFA model can reduce the required sample size by a factor of \(\widetilde{\mathcal{O}}([d/(2p+12)]^{p})\).

**Example 6** (Correlation-weighted functions): Consider the function

\[g_{\star}=\frac{1}{N}\sum_{i=1}^{N}\cos(\left\langle\mathbf{x}_{0},\mathbf{x}_ {i}\right\rangle)\left\langle\mathbf{x}_{i}^{\otimes 3},\mathbf{G}\right\rangle, \quad\text{with }\left\|\mathbf{G}\right\|_{\mathsf{F}_{\mathsf{r}}}^{2}\leq 1 \text{ and }B(g_{\star},L)=\Theta((8e)^{2L}),\]where \(B(g_{\star},L)\) is bounded through the Taylor expansion of \(\cos(t)\) and (18). For any \(\eta>0\), choosing the same parameters as Example 4, the excess risk bound scales as \(\widetilde{\mathcal{O}}(\sqrt{1/n^{1-\eta}})\). \(\Diamond\)

Consider the required sample size \(n_{\star}\) to reach an accuracy of \(0.01\). The BRFA model requires \(n_{\star}=\widetilde{\mathcal{O}}(1)\), whereas the RFA model requires \(n_{\star}=\widetilde{\mathcal{O}}(\mathrm{Poly}(d)\exp(\sqrt{d}))\). Thus, in comparison to the RFA model, the BRFA model can reduce the required sample size by a factor of \(\widetilde{\mathcal{O}}(\mathrm{Poly}(d)\exp(\sqrt{d}))\).

### Overview of techniques

Here we provide the intuition and an overview of the technique of Theorem 3, with the proof details in Appendix C. To show the sample complexity of learning with the BRFA model, the first step is to derive the kernel \(K_{\texttt{BBFA}}(\mathbf{x}_{0:N},\mathbf{x}_{0,N}^{\prime})\) associated with the infinite-width BRFA model. This kernel has a natural feature map, given by \(\{\Psi_{k}:\mathcal{X}\rightarrow\mathbb{R}^{d^{2k+1}}\}_{k\geq 0}\), where

\[\Psi_{k}(\mathbf{x}_{0:N})=\sum_{i=1}^{N}\phi(\langle\mathbf{x}_{0},\mathbf{ x}_{i}\rangle)\cdot\mathrm{He}_{k-2}(\langle\mathbf{x}_{0},\mathbf{x}_{i} \rangle)\cdot\widetilde{\mathbf{x}}_{i}^{\otimes k+1}\otimes\widetilde{ \mathbf{x}}_{0}^{\otimes k},\quad\forall k\geq 2.\]

Here \(\phi(t)=(2\pi)^{-1/2}e^{-t^{2}/2}\) is the Gaussian density function, and \(\mathrm{He}_{k}(z)\) denotes the \(k\)-th probabilist's Hermite polynomial, with detailed expression and properties given in Appendix A.1. This feature map implies the learnability of the following target function class by the BRFA model,

\[\widetilde{g}_{\star}(\mathbf{x}_{0:N})=\tfrac{1}{N}\sum_{i=1}^{N}\phi( \langle\mathbf{x}_{0},\mathbf{x}_{i}\rangle)\sum_{k=2}^{\infty}\mathrm{He}_{k -2}(\langle\mathbf{x}_{0},\mathbf{x}_{i}\rangle)\left\langle\widetilde{ \mathbf{x}}_{i}^{\otimes k+1}\otimes\widetilde{\mathbf{x}}_{0}^{\otimes k}, \mathbf{A}_{k}\right\rangle,\] (19)

whose RKHS norm associated with kernel \(K_{\texttt{BBFA}}\) is bounded by \(B(\widetilde{g}_{\star})=\sum_{k=2}^{\infty}(k-2)!k^{2}4^{k}\|\mathbf{A}_{k} \|_{\texttt{fr}}^{2}\).

Notice that \(\widetilde{g}_{\star}\) bears similarities to, but also distinct differences from, \(g_{\star}\) as presented in (16). The key difference lies in the \(\phi(\langle\mathbf{x}_{0},\mathbf{x}_{i}\rangle)\) factor in \(\widetilde{g}_{\star}\), which is hard to interpret and analyze. To obtain the excess risk bound for learning \(g_{\star}\), we can use \(\widetilde{g}_{\star}\) to approximate \(g_{\star}\) in the \(L^{\infty}\) norm. The excess risk for learning \(g_{\star}\) can be bounded by the summation of the excess risk for learning \(\widetilde{g}_{\star}\) and the approximation error. Acquiring this approximation error bound necessitates a truncation argument of the Taylor expansion of \(1/\phi(\cdot)\).

## 5 Numerical experiments

We test our theory by experimentally approximating two types of target functions using the three models under investigation RFA (5), BRFA (15), and RFMLP (14). We choose the target functions to be of form

\[f_{1,p}(\mathbf{x}_{0:N}) =\tfrac{1}{N}\sum_{i=1}^{N}\left\langle\boldsymbol{\beta},\mathbf{ x}_{i}\right\rangle^{p}, p\in\mathbb{N},\quad\boldsymbol{\beta}\in\mathbb{S}^{d-1},\] (20) \[f_{2,q}(\mathbf{x}_{0:N}) =\tfrac{1}{N}\sum_{i=1}^{N}\left\langle\mathbf{x}_{0},\mathbf{x} _{i}\right\rangle^{q}\left\langle\boldsymbol{\beta},\mathbf{x}_{i}\right\rangle, q\in\mathbb{N},\quad\boldsymbol{\beta}\in\mathbb{S}^{d-1}.\] (21)

The first target function (20) is a specific instance of Example 2, whereas the second target function (21) has been considered in both Example 3 and 5.

In our experimental setup, we set the input dimension as \(d=16\) and the number of tokens as \(N=16\). We fix the width of RFA and BRFA to be \(M_{\texttt{BFFA}}=M_{\texttt{BBFA}}=M=1000\), whereas the width of RFMLP is set as \(M_{\texttt{RFMLP}}=M(d+1)=17000\). This configuration ensures an equal number of parameters across all three models. To further accentuate the test risk difference between the BRFA and RFA, in BRFA we use a bias matrix of \(\mathbf{W}_{0}=4[\mathbf{I}_{d\times d},\mathbf{0}_{d\times 1};\mathbf{0}_{1 \times d},0]\in\mathbb{R}^{(d+1)\times(d+1)}\), which is four times the matrix investigated in our theory. The input distribution is selected as \(\{\mathbf{x}_{i}\}_{0\leq i\leq N}\sim_{\text{iid}}\)Unif\((\mathbb{S}^{d-1})\), and we take \(y=f_{\star}(\mathbf{x}_{0:N})\) without any noise. We consider three representative target functions: \(f_{1,p}\) for \(p=2,4\), and \(f_{2,p}\) for \(p=3\), as per (20) and (21). We examine a list of sample sizes \(n\) from \(2^{4}\) to \(2^{12}\). Prior to training with RF models, we standardize the \(y_{i}\)'s to have zero mean and unit standard deviation, ensuring that the trivial risk equals \(1\). We train the RF models using square loss with ridge regularization, selecting the ridge parameter to minimize the test error. The experimental results are displayed in Figure 2.

The left and middle panels of Figure 2 demonstrate a noticeable separation between RFMLP and the other two random-feature attention models for learning these target functions. RFMLP can hardly approximate the target function, whereas RFA and BRFA exhibit significantly better performance. This observation is consistent with our sample complexity analysis detailed in Example 2, where the sample complexity bound of RFMLP for learning average of functions of \(\mathbf{x}_{i}\) is found to be \(\mathcal{O}((N/4)^{p})\) times greater than that of RFA.

The performance comparison between RFA and BRFA depends on the target functions. RFA outperforms BRFA in learning \(f_{1,2}\) and \(f_{1,4}\), whereas BRFA outperforms RFA in learning \(f_{2,3}\). The latter phenomenon is as we expected: as demonstrated in Example 3 and 5, BRFA is more powerful than RFA in approximating the correlation-weighted functions.

We have conducted further experiments with various other target functions, detailed in Appendix D.

## 6 Conclusion

In this work, we introduced and examined the expressivity of two random-feature attention models, namely RFA (5) and BRFA (15). For general classes of functions that are invariant to the permutation of key tokens \(\mathbf{x}_{1:N}\), the excess risk of RFA (5) can avoid the dependence on sequence length, in contrast to the standard random-feature model RFMLP (14). Moreover, for specific functions that adopt the form of correlation-weighted polynomials (6), the excess risk of BRFA can avoid the polynomial dependence on the dimension. These insights enhance our understanding of the attention mechanism within a simplified context. Finally, our work left open many interesting questions for future work, such as the expressivity of softmax attention, the influence of positional encoding in expressivity, and the expressivity of multi-layer transformers.

## Acknowledgment

S. Mei is supported in part by NSF DMS-2210827 and NSF CCF-2315725.

## References

* [1] E. Abbe, E. B. Adsera, and T. Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In _Conference on Learning Theory_, pages 4782-4887. PMLR, 2022.
* [2] E. Abbe, E. Boix-Adsera, and T. Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. _arXiv preprint arXiv:2302.11055_, 2023.
* [3] E. Akyirek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-context learning? investigations with linear models. _arXiv preprint arXiv:2211.15661_, 2022.
* [4] Z. Allen-Zhu and Y. Li. What can resnet learn efficiently, going beyond kernels? _Advances in Neural Information Processing Systems_, 32, 2019.

Figure 2: Test error of three RF models for learning \(f_{1,2}\) (left), \(f_{1,4}\) (mid), and \(f_{2,3}\) (right), as per (20) and (21). We set \(d,N=16\), \(M_{\texttt{RFA}}=M_{\texttt{BRFA}}=1000\), and \(M_{\texttt{RFMLP}}=17000\). We train the RF models using square loss with ridge regularization, with the ridge parameter selected to minimize test error. The test error is calculated using \(n_{\mathrm{test}}=1000\) fresh samples. The figure reports the mean and normalized standard error of the test error, based on \(5\) independent experimental instances.

* [5] Z. Allen-Zhu, Y. Li, and Y. Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. _Advances in neural information processing systems_, 32, 2019.
* [6] Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization. In _International Conference on Machine Learning_, pages 242-252. PMLR, 2019.
* [7] S. Arora, S. Du, W. Hu, Z. Li, and R. Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning_, pages 322-332. PMLR, 2019.
* [8] F. Bach. Breaking the curse of dimensionality with convex neural networks. _The Journal of Machine Learning Research_, 18(1):629-681, 2017.
* [9] F. Bach and L. Chizat. Gradient descent on infinitely wide neural networks: Global convergence and generalization. _arXiv preprint arXiv:2110.08084_, 2021.
* [10] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. _arXiv preprint arXiv:1409.0473_, 2014.
* [11] Y. Bai and J. D. Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks. _arXiv preprint arXiv:1910.01619_, 2019.
* [12] A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. _IEEE Transactions on Information theory_, 39(3):930-945, 1993.
* [13] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. _Journal of Machine Learning Research_, 3(Nov):463-482, 2002.
* [14] S. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to recognize formal languages. _arXiv preprint arXiv:2009.11264_, 2020.
* [15] S. Bhattamishra, A. Patel, and N. Goyal. On the computational power of transformers and its implications in sequence modeling. _arXiv preprint arXiv:2006.09286_, 2020.
* [16] A. Bietti. Approximation and learning with deep convolutional models: a kernel perspective. _stat_, 1050:18, 2022.
* [17] A. Bietti and J. Mairal. On the inductive bias of neural tangent kernels. _Advances in Neural Information Processing Systems_, 32, 2019.
* [18] A. Bietti, L. Venturi, and J. Bruna. On the sample complexity of learning under geometric stability. _Advances in neural information processing systems_, 34:18673-18684, 2021.
* [19] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [20] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* [21] A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. _Foundations of Computational Mathematics_, 7:331-368, 2007.
* [22] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* [23] M. Chen, Y. Bai, J. D. Lee, T. Zhao, H. Wang, C. Xiong, and R. Socher. Towards understanding hierarchical learning: Benefits of neural representations. _Advances in Neural Information Processing Systems_, 33:22134-22145, 2020.

* [24] L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. _Advances in neural information processing systems_, 31, 2018.
* [25] L. Chizat, E. Oyallon, and F. Bach. On lazy training in differentiable programming. _Advances in neural information processing systems_, 32, 2019.
* [26] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. _arXiv preprint arXiv:2212.10559_, 2022.
* [27] A. Damian, J. Lee, and M. Soltanolkotabi. Neural networks can learn representations with gradient descent. In _Conference on Learning Theory_, pages 5413-5452. PMLR, 2022.
* [28] A. Daniely. Sgd learns the conjugate kernel class of the network. _Advances in Neural Information Processing Systems_, 30, 2017.
* [29] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [30] L. Dong, S. Xu, and B. Xu. Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition. In _2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)_, pages 5884-5888. IEEE, 2018.
* [31] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [32] S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pages 1675-1685. PMLR, 2019.
* [33] S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized neural networks. _arXiv preprint arXiv:1810.02054_, 2018.
* [34] E. Dyer and G. Gur-Ari. Asymptotics of wide networks from feynman diagrams. _arXiv preprint arXiv:1909.11304_, 2019.
* [35] B. L. Edelman, S. Goel, S. Kakade, and C. Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_, pages 5793-5831. PMLR, 2022.
* [36] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021.
* [37] S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022.
* [38] R. Ge, J. D. Lee, and T. Ma. Learning one-hidden-layer neural networks with landscape design. _arXiv preprint arXiv:1711.00501_, 2017.
* [39] B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. When do neural networks outperform kernel methods? _Advances in Neural Information Processing Systems_, 33:14820-14830, 2020.
* [40] B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. Linearized two-layers neural networks in high dimension. 2021.
* [41] A. Giannou, S. Rajput, J.-y. Sohn, K. Lee, J. D. Lee, and D. Papailiopoulos. Looped transformers as programmable computers. _arXiv preprint arXiv:2301.13196_, 2023.
* [42] M. Hahn. Theoretical limitations of self-attention in neural sequence models. _Transactions of the Association for Computational Linguistics_, 8:156-171, 2020.

* [43] J. Hron, Y. Bahri, J. Sohl-Dickstein, and R. Novak. Infinite attention: Nngp and ntk for deep attention networks. In _International Conference on Machine Learning_, pages 4376-4386. PMLR, 2020.
* [44] J. Huang and H.-T. Yau. Dynamics of deep neural networks and neural tangent hierarchy. In _International conference on machine learning_, pages 4542-4551. PMLR, 2020.
* [45] K. Huang, Y. Wang, M. Tao, and T. Zhao. Why do deep residual networks generalize better than deep feedforward networks?--a neural tangent kernel perspective. _Advances in neural information processing systems_, 33:2698-2709, 2020.
* [46] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* [47] S. Jegelka. Theory of graph neural networks: Representation and learning. _arXiv preprint arXiv:2204.07697_, 2022.
* [48] S. Jelassi, M. Sander, and Y. Li. Vision transformers provably learn spatial structure. _Advances in Neural Information Processing Systems_, 35:37822-37836, 2022.
* [49] Y. Kim, C. Denton, L. Hoang, and A. M. Rush. Structured attention networks. _arXiv preprint arXiv:1702.00887_, 2017.
* [50] H. Li, M. Wang, S. Liu, and P.-Y. Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. _arXiv preprint arXiv:2302.06015_, 2023.
* [51] Y. Li and Y. Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. _Advances in neural information processing systems_, 31, 2018.
* [52] Y. Li, M. E. Idiz, D. Papailiopoulos, and S. Oymak. Transformers as algorithms: Generalization and implicit model selection in in-context learning. _arXiv preprint arXiv:2301.07067_, 2023.
* [53] Z. Li, R. Wang, D. Yu, S. S. Du, W. Hu, R. Salakhutdinov, and S. Arora. Enhanced convolutional neural tangent kernels. _arXiv preprint arXiv:1911.00809_, 2019.
* [54] T. Liang and A. Rakhlin. Just interpolate: Kernel "ridgeless" regression can generalize. 2020.
* [55] T. Liang, A. Rakhlin, and X. Zhai. On the risk of minimum-norm interpolants and restricted lower isometry of kernels. _arXiv preprint arXiv:1908.10292_, 27, 2019.
* [56] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts to automata. _arXiv preprint arXiv:2210.10749_, 2022.
* [57] S. Mei and A. Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. _Communications on Pure and Applied Mathematics_, 75(4):667-766, 2022.
* [58] S. Mei, A. Montanari, and P.-M. Nguyen. A mean field view of the landscape of two-layer neural networks. _Proceedings of the National Academy of Sciences_, 115(33):E7665-E7671, 2018.
* [59] S. Mei, T. Misiakiewicz, and A. Montanari. Learning with invariances in random features and kernel models. In _Conference on Learning Theory_, pages 3351-3418. PMLR, 2021.
* [60] S. Mei, T. Misiakiewicz, and A. Montanari. Generalization error of random feature and kernel methods: hypercontractivity and kernel matrix concentration. _Applied and Computational Harmonic Analysis_, 59:3-84, 2022.
* [61] H. Q. Minh, P. Niyogi, and Y. Yao. Mercer's theorem, feature maps, and smoothing. In _Learning Theory: 19th Annual Conference on Learning Theory, COLT 2006, Pittsburgh, PA, USA, June 22-25, 2006. Proceedings 19_, pages 154-168. Springer, 2006.

* [62] T. Misiakiewicz and S. Mei. Learning with convolution and pooling operations in kernel methods. _arXiv preprint arXiv:2111.08308_, 2021.
* [63] E. Nichani, Y. Bai, and J. D. Lee. Identifying good directions to escape the ntk regime and efficiently learn low-degree plus sparse polynomials. _arXiv preprint arXiv:2206.03688_, 2022.
* [64] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022.
* [65] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [66] S. Oymak and M. Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. _IEEE Journal on Selected Areas in Information Theory_, 1(1):84-105, 2020.
* [67] S. Oymak, A. S. Rawat, M. Soltanolkotabi, and C. Thrampoulidis. On the role of attention in prompt-tuning. _arXiv preprint arXiv:2306.03435_, 2023.
* [68] A. P. Parikh, O. Tackstrom, D. Das, and J. Uszkoreit. A decomposable attention model for natural language inference. _arXiv preprint arXiv:1606.01933_, 2016.
* [69] K. Y. Patarroyo. A digression on hermite polynomials. _arXiv preprint arXiv:1901.01648_, 2019.
* [70] J. Perez, J. Marinkovic, and P. Barcelo. On the turing completeness of modern neural network architectures. _arXiv preprint arXiv:1901.03429_, 2019.
* [71] A. Pinkus. Approximation theory of the mlp model in neural networks. _Acta numerica_, 8:143-195, 1999.
* [72] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018.
* [73] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [74] A. Rahimi and B. Recht. Random features for large-scale kernel machines. _Advances in neural information processing systems_, 20, 2007.
* [75] A. Rahimi and B. Recht. Uniform approximation of functions with random bases. In _2008 46th annual allerton conference on communication, control, and computing_, pages 555-561. IEEE, 2008.
* [76] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. _Advances in neural information processing systems_, 21, 2008.
* [77] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. _arXiv preprint arXiv:2205.06175_, 2022.
* [78] G. M. Rotskoff and E. Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error. _stat_, 1050:22, 2018.
* [79] A. Rudi and L. Rosasco. Generalization properties of learning with random features. _Advances in neural information processing systems_, 30, 2017.
* [80] K. Shen, J. Guo, X. Tan, S. Tang, R. Wang, and J. Bian. A study on relu and softmax in transformer. _arXiv preprint arXiv:2302.06461_, 2023.
* [81] C. Snell, R. Zhong, D. Klein, and J. Steinhardt. Approximating how single head attention learns. _arXiv preprint arXiv:2103.07601_, 2021.

* [82] D. A. Tarzanagh, Y. Li, X. Zhang, and S. Oymak. Max-Margin Token Selection in Attention Mechanism, Sept. 2023. URL http://arxiv.org/abs/2306.13596. arXiv:2306.13596 [cs, math].
* [83] T. Tirer, J. Bruna, and R. Giryes. Kernel-based smoothness analysis of residual networks. In _Mathematical and Scientific Machine Learning_, pages 921-954. PMLR, 2022.
* [84] A. Trockman and J. Z. Kolter. Mimetic initialization of self-attention layers. _arXiv preprint arXiv:2305.09828_, 2023.
* [85] J. A. Tropp et al. An introduction to matrix concentration inequalities. _Foundations and Trends(r) in Machine Learning_, 8(1-2):1-230, 2015.
* [86] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [87] R. Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [88] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent. _arXiv preprint arXiv:2212.07677_, 2022.
* [89] M. J. Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* [90] C. Wei, J. D. Lee, Q. Liu, and T. Ma. Regularization matters: Generalization and optimization of neural nets vs their induced kernel. _Advances in Neural Information Processing Systems_, 32, 2019.
* [91] C. Wei, Y. Chen, and T. Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. _Advances in Neural Information Processing Systems_, 35:12071-12083, 2022.
* [92] E. Weinan, C. Ma, and L. Wu. Barron spaces and the compositional function spaces for neural network models. _arXiv preprint arXiv:1906.08039_, 2019.
* [93] E. Weinan, C. Ma, and L. Wu. A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics. _Sci. China Math_, 2019.
* [94] G. Weiss, Y. Goldberg, and E. Yahav. Thinking like transformers. In _International Conference on Machine Learning_, pages 11080-11090. PMLR, 2021.
* [95] M. Wortsman, J. Lee, J. Gilmer, and S. Kornblith. Replacing softmax with relu in vision transformers, 2023.
* [96] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian inference. _arXiv preprint arXiv:2111.02080_, 2021.
* [97] K. Xu, M. Zhang, J. Li, S. S. Du, K.-i. Kawarabayashi, and S. Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. _arXiv preprint arXiv:2009.11848_, 2020.
* [98] G. Yang. Tensor programs ii: Neural tangent kernel for any architecture. _arXiv preprint arXiv:2006.14548_, 2020.
* [99] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process bounded hierarchical languages. _arXiv preprint arXiv:2105.11115_, 2021.
* [100] G. Yehudai and O. Shamir. On the power and limitations of random features for understanding neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.

* [101] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu. Do transformers really perform badly for graph representation? _Advances in Neural Information Processing Systems_, 34:28877-28888, 2021.
* [102] C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal approximators of sequence-to-sequence functions? _arXiv preprint arXiv:1912.10077_, 2019.
* [103] Y. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner. Unveiling transformers with lego: a synthetic reasoning task. _arXiv preprint arXiv:2206.04301_, 2022.
* [104] D. Zou, Y. Cao, D. Zhou, and Q. Gu. Gradient descent optimizes over-parameterized deep relu networks. _Machine learning_, 109:467-492, 2020.

## Appendix A Technical tools

### Basics on Hermite Polynomials

In this section, we briefly review Hermite polynomials and their properties. Let \(\mathrm{He}_{n}\) be the probabilists' Hermite polynomial of degree \(n\):

\[\mathrm{He}_{n}(x):=(-1)^{n}e^{\frac{x^{2}}{2}}\frac{d^{n}}{dx^{n}}e^{-\frac{x ^{2}}{2}}.\]

Here are some basic properties of Hermite polynomials \(\{\mathrm{He}_{n}\}_{n\geq 0}\):

* Let \((x,y)\sim\mathsf{N}(\mathbf{0},[1,\rho;\rho,1])\). Then \(\mathbb{E}[\mathrm{He}_{m}(x)\mathrm{He}_{n}(y)]=n!\rho^{n}1_{m=n}\) for \(m,n\geq 0\).
* \(\mathrm{He}_{n}(-x)=(-1)^{n}\mathrm{He}_{n}(x)\);
* \(\mathrm{He}_{n+1}(x)=x\mathrm{He}_{n}(x)-\mathrm{He}^{\prime}_{n}(x)\).
* \(\mathrm{He}^{\prime}_{n}(x)=n\mathrm{He}_{n-1}(x)\).

It can be shown that Hermite polynomials are a complete orthogonal basis of the following Hilbert space with \(\phi(x)=(2\pi)^{-1/2}e^{-x^{2}/2}\):

\[L^{2}(\mathbb{R},\phi(x)):=\left\{f:\mathbb{R}\to\mathbb{R}:\quad\int_{- \infty}^{\infty}f(x)^{2}\phi(x)\mathrm{d}x<\infty\right\}.\]

For two functions \(f,g:\mathbb{R}\to\mathbb{R}\), we define their inner product \(\langle f,g\rangle\) as:

\[\langle f,g\rangle:=\mathbb{E}_{x\sim\mathsf{N}(0,1)}[f(x)g(x)]=\int_{-\infty }^{\infty}f(x)g(x)\phi(x)\mathrm{d}x.\]

For any function \(f\in L^{2}(\mathbb{R},\phi(x))\), we can derive its Hermite expansion:

\[f(x)=\sum_{n=0}^{\infty}\frac{a_{n}}{n!}\mathrm{He}_{n}(x),\]

where \(a_{n}=\langle f,\mathrm{He}_{n}\rangle=\int_{-\infty}^{\infty}f(x)\mathrm{ He}_{n}(x)\phi(x)\mathrm{d}x\). Then for another function \(g(x)=\sum_{n=0}^{\infty}\frac{b_{n}}{n!}\mathrm{He}_{n}(x)\), the inner product of \(f\) and \(g\) gives:

\[\langle f,g\rangle=\sum_{n=0}^{\infty}\frac{a_{n}b_{n}}{n!}.\]

Here are some formulae for Hermite expansion of certain functions.

**Lemma A.1** (See e.g., [38]).: _Let \(f,g\in L^{2}(\mathbb{R},\phi(x))\). Then, for any unit vectors \(u,v\in\mathbb{R}^{d}\), we have that_

\[\mathbb{E}_{x\sim\mathcal{N}(0,\mathbf{I}_{d})}\big{[}f(u^{\top}x)g(v^{\top}x )\big{]}=\sum_{n=0}^{\infty}\frac{a_{n}b_{n}}{n!}\left\langle u,v\right\rangle ^{n},\]

_where \(f(x)=\sum_{n=0}^{\infty}\frac{a_{n}}{n!}\mathrm{He}_{n}(x)\) and \(g(x)=\sum_{n=0}^{\infty}\frac{b_{n}}{n!}\mathrm{He}_{n}(x)\)._

**Lemma A.2** (Inverse explicit expression [69]).: _Hermite expansion of \(x^{n}\) gives:_

\[x^{n}=n!\sum_{m=0}^{\lfloor\frac{n}{2}\rfloor}\frac{1}{2^{m}m!(n-2m)!}\mathrm{He} _{n-2m}(x).\]

**Lemma A.3**.: _Let \(\sigma_{c}(x):=\sigma(x+c)\) be the shifted ReLU function. Then the Hermite expansion of \(\sigma_{c}\) gives:_

\[\sigma_{c}(x)=c\Phi(c)+\phi(c)+\Phi(c)\mathrm{He}_{1}(x)+\sum_{n=2}^{\infty} \frac{(-1)^{n}}{n!}\phi(c)\mathrm{He}_{n-2}(c)\mathrm{He}_{n}(x),\]

_where \(\phi(x),\Phi(x)\) are the PDF and CDF of standard Gaussian._

Proof of Lemma a.3.: Denote \(a_{n,i}:=\int_{-c}^{\infty}(x+c)^{i}\mathrm{He}_{n}(x)\phi(x)dx\). It is easy to obtain that

\[a_{0,0}=\Phi(c),\ \ \ \ a_{0,1}=c\Phi(c)+\phi(c),\ \ \ \ \ a_{1,0}=\phi(c),\ \ \ \text{and}\ \ \ a_{1,1}=\Phi(c).\]

Then using the two different formulae for \(a_{n,0}\),

\[a_{n,0} =\int_{-c}^{\infty}\mathrm{He}_{n}(x)\phi(x)\mathrm{d}x=-\frac{ \mathrm{He}_{n+1}(-c)}{n+1}\phi(-c)+\int_{-c}^{\infty}\frac{\mathrm{He}_{n+1}( x)}{n+1}x\phi(x)\mathrm{d}x\] \[=-\frac{\mathrm{He}_{n+1}(-c)}{n+1}\phi(-c)+\frac{1}{n+1}a_{n+1,1 }-\frac{c}{n+1}a_{n+1,0},\ \ n\geq 0,\ \ \text{and}\] \[a_{n,0} =\int_{-c}^{\infty}(x\mathrm{He}_{n-1}(x)-(n-1)\mathrm{He}_{n-2} (x))\phi(x)\mathrm{d}x\] \[=a_{n-1,1}-ca_{n-1,0}-(n-1)a_{n-2,0},\ \ n\geq 2,\]

we obtain that

\[a_{n,1} =(-1)^{n}\mathrm{He}_{n}(c)\phi(c)+ca_{n-1,1}+(n-c^{2})a_{n-1,0}-( n-1)ca_{n-2,0},\ \ \text{and}\] \[a_{n,0} =a_{n-1,1}-ca_{n-1,0}-(n-1)a_{n-2,0},\ \ n\geq 2.\]

Then it is easy to prove by induction that

\[a_{n,1} =(-1)^{n}\mathrm{He}_{n-2}(c)\phi(c),\ \ n\geq 2,\] \[a_{n,0} =(-1)^{n-1}\mathrm{He}_{n-1}(c)\phi(c),\ \ n\geq 1.\]

This completes the proof. 

### Basics on Random Features

In this section, we give some basic properties of the random feature model considered in our work, which can be seen as an extension of the standard random feature model (e.g. of Rahimi and Recht [74; 75]) to the _vector-valued case_.

Given a functional \(\boldsymbol{\sigma}(x;w):\mathcal{X}\times\mathcal{W}\to\mathbb{R}^{d}\). Denote \(\mu\) as a probability measure on \(\mathcal{W}\). We define the (infinite-width) random feature model as:

\[\mathcal{F}=\{f:f(x)=\left\langle\mathbf{v},\boldsymbol{\sigma}(x;\cdot) \right\rangle_{\mathcal{H}_{\mathcal{W}}},\ \ \ \mathbf{v}\in\mathcal{H}_{\mathcal{W}}\},\] (22)

where \(\mathcal{H}_{\mathcal{W}}=\left\{\mathbf{v}(w):\int_{\mathcal{W}}\mathbf{v}(w) ^{\top}\mathbf{v}(w)\mu(\mathrm{d}w)<\infty\right\}\) is a Hilbert space with norm \(\|\mathbf{v}\|_{\mathcal{H}_{\mathcal{W}}}^{2}=\int_{\mathcal{W}}\mathbf{v}(w )^{\top}\mathbf{v}(w)\mu(\mathrm{d}w)\) and inner product \(\left\langle\mathbf{v},\mathbf{u}\right\rangle_{\mathcal{H}_{\mathcal{W}}}= \int_{\mathcal{W}}\mathbf{v}(w)^{\top}\mathbf{u}(w)\mu(\mathrm{d}w)\). Besides, we endow \(\mathcal{F}\) with a norm \(\|\cdot\|_{\mathcal{F}}\) and the corresponding inner product \(\left\langle\cdot,\cdot\right\rangle_{\mathcal{F}}\) defined as:

\[\|f\|_{\mathcal{F}}=\inf_{f=\left\langle\mathbf{v},\boldsymbol{\sigma}(\cdot )\right\rangle_{\mathcal{H}_{\mathcal{W}}}}\|\mathbf{v}\|_{\mathcal{H}_{ \mathcal{W}}},\ \ \ \left\langle f,g\right\rangle_{\mathcal{F}}=\frac{\|f+g\|_{\mathcal{F}}^{2}-\|f-g \|_{\mathcal{F}}^{2}}{4}.\]

We further define the corresponding reproducing kernel \(K:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\), s.t.

\[K(x,y)=\int_{\mathcal{W}}\boldsymbol{\sigma}(x;w)^{\top}\boldsymbol{\sigma}(y; w)\mu(\mathrm{d}w),\]

which is positive definite. Define the RKHS induced by this kernel as \(\mathcal{H}_{K}\) with corresponding norm \(\|\cdot\|_{\mathcal{H}_{K}}\) and the inner product \(\left\langle\cdot,\cdot\right\rangle_{\mathcal{H}_{K}}\). Then we have the following proposition according to [61]:

**Proposition A.1**.: _Given the above definition of \(\mathcal{F}\) and \(\mathcal{H}_{K}\), we have that \(\left(\mathcal{F},\left\|\cdot\right\|_{\mathcal{F}}\right)=\left(\mathcal{H}_{K },\left\|\cdot\right\|_{\mathcal{H}_{K}}\right)\)._

More generally [11], for any feature map \(\phi:\mathcal{X}\to\mathcal{H}\) (where \(\mathcal{H}\) is a Hilbert space) that induces the kernel \(K\), i.e., \(K(x,y)=\left\langle\phi(x),\phi(y)\right\rangle_{\mathcal{H}}\), we have for any function \(f\) that

\[\left\|f\right\|_{\mathcal{H}_{K}}=\inf_{f=\left(\mathbf{u},\phi(\cdot) \right)_{\mathcal{H}}}\left\|\mathbf{u}\right\|_{\mathcal{H}},\] (23)

which shows the equivalence among different feature maps that generate the same kernel.

### Concentration inequalities

**Definition A.1** (Sub-Gaussian and Sub-Exponential random variables [87]).: _For a random variable \(X\), its sub-gaussian norm, denoted \(\left\|X\right\|_{\psi_{2}}\), is defined as_

\[\left\|X\right\|_{\psi_{2}}=\inf\left\{t>0:\mathbb{E}\exp\left(X^{2}/t^{2} \right)\leq 2\right\}.\]

_If \(\sigma\equiv\left\|X\right\|_{\psi_{2}}<\infty\), we say that \(X\) is \(\sigma\)-sub-Gaussian._

_For a random variable \(X\), its sub-exponential norm, denoted \(\|X\|_{\psi_{1}}\), is defined as_

\[\left\|X\right\|_{\psi_{1}}=\inf\left\{t>0:\mathbb{E}\exp\left(|X|/t\right)\leq 2 \right\}.\]

_If \(\sigma\equiv\|X\|_{\psi_{1}}<\infty\), we say that \(X\) is \(\sigma\)-sub-exponential._

**Theorem A.1** (Gaussian concentration inequality (e.g., [89])).: _Let \((X_{1},\ldots,X_{n})\) be a vector of i.i.d. standard Gaussian variables, and let \(f:\mathbb{R}^{n}\to\mathbb{R}\) be L-Lipschitz with respect to the Euclidean norm. Then the random variable \(f(X)-\mathbb{E}[f(X)]\) is L-sub-Gaussian, and hence_

\[\mathbb{P}\big{(}|f(X)-\mathbb{E}[f(X)]|\geq t\big{)}\leq 2e^{-\frac{t^{2}}{2L ^{2}}}\quad\text{ for all }t\geq 0.\]

**Theorem A.2** (Bounded difference inequality (e.g., [89])).: _Consider a function \(f(X):\mathbb{R}^{n}\to\mathbb{R}\). Assume that for any \(X=(X_{1},\ldots,X_{n})\) and \(X^{i,\prime}=(X_{1},\ldots,X_{i}^{\prime},\ldots,X_{n})\), we have difference bound \(|f(X)-f(X^{i,\prime})|\leq L_{i}\). We further assume that the random vector \(X=(X_{1},X_{2},\ldots,X_{n})\) has independent components. Then_

\[\mathbb{P}\big{(}|f(X)-\mathbb{E}[f(X)]|\geq t\big{)}\leq 2e^{-\frac{t^{2}}{ \Sigma k_{k}-1}\frac{t^{2}}{k}}\quad\text{ for all }t\geq 0.\]

**Theorem A.3** (Matrix Bernstein Inequality (e.g., [85])).: _Consider a sequence \(\left\{\bm{S}_{k}\right\}_{k\in[n]}\) of independent random matrices with common dimension \(d_{1}\times d_{2}\). Assume that_

\[\mathbb{E}\bm{S}_{k}=\bm{0}\quad\text{ and }\left\|\bm{S}_{k}\right\|_{ \mathrm{op}}\leq L\text{ almost surely, }\quad\text{for each index }k.\]

_Introduce the random matrix_

\[\bm{Z}=\sum_{k=1}^{n}\bm{S}_{k}.\]

_Let \(v(\bm{Z})\) be the matrix variance statistic of the sum_

\[v(\bm{Z}) =\max\left\{\left\|\mathbb{E}\left(\bm{Z}\bm{Z}^{\top}\right) \right\|_{\mathrm{op}},\left\|\mathbb{E}\left(\bm{Z}^{\top}\bm{Z}\right)\right\| _{\mathrm{op}}\right\}\] \[=\max\left\{\left\|\sum_{k=1}^{n}\mathbb{E}\left(\bm{S}_{k}\bm{S} _{k}^{\top}\right)\right\|_{\mathrm{op}},\left\|\sum_{k=1}^{n}\mathbb{E}\left( \bm{S}_{k}^{\top}\bm{S}_{k}\right)\right\|_{\mathrm{op}}\right\}.\]

_Then we have_

\[\mathbb{E}\|\bm{Z}\|_{\mathrm{op}}\leq\sqrt{2v(\bm{Z})\log\left(d_{1}+d_{2} \right)}+\frac{1}{3}L\log\left(d_{1}+d_{2}\right),\]

_and for all \(t\geq 0\),_

\[\mathbb{P}\big{(}\|\bm{Z}\|_{\mathrm{op}}\geq t\big{)}\leq(d_{1}+d_{2})\exp \left(\frac{-t^{2}/2}{v(\bm{Z})+Lt/3}\right).\]

**Theorem A.4** (Ledoux-Talagrand contraction inequality (e.g., [89])).: _Let \(\left\{\xi_{i}\right\}_{i\in[n]}\sim_{\mathrm{iid}}\mathsf{Unif}(\left\{\pm 1 \right\})\) be independent Rademacher random variables. For any set \(\mathbb{T}\subset\mathbb{R}^{n}\) and any family of L-Lipschitz functions \(\left\{\phi_{j}\right\}_{j\in[n]}\) with \(\phi_{i}(0)=0\), we have_

\[\mathbb{E}\Big{[}\sup_{\theta\in\mathbb{T}}\Big{|}\sum_{i=1}^{n}\xi_{i}\phi_{i} (\theta_{i})\Big{|}\Big{]}\leq 2L\cdot\mathbb{E}\Big{[}\sup_{\theta\in\mathbb{T}} \Big{|}\sum_{i=1}^{n}\xi_{i}\theta_{i}\Big{|}\Big{]}.\] (24)Proofs for Section 3

Throughout this section, we use the notation \(\lesssim\) to hide a universal constant \(C\). Also, we use \(\overline{\sigma}(\mathbf{W},\mathbf{X})\) to denote a function of \(\mathbf{W}\in\mathbb{R}^{(d+1)\times(d+1)}\) and \(\mathbf{X}\in\{\overline{\mathbf{X}}\in\mathbb{R}^{(d+1)\times(d+1)}:\| \overline{\mathbf{X}}\|_{\mathsf{Fr}}^{2}\leq 4\}\equiv\mathcal{D}\) that satisfies the following properties:

1. For any \(\mathbf{X}\in\mathcal{D}\), we have that \(\overline{\sigma}(\mathbf{W},\mathbf{X})\) is \(L_{1}\)-Lipschitz with respect to \(\mathbf{W}\).
2. For any \(\mathbf{X}\in\mathcal{D}\), the expectation \(\mathbb{E}_{\mathbf{W}}[\overline{\sigma}(\mathbf{W},\mathbf{X})]\leq L_{2}\).
3. For any \(\mathbf{X}\in\mathcal{D}\), we have \(\overline{\sigma}(\mathbf{0},\mathbf{X})\leq L_{3}\).

Here \(L_{1}\), \(L_{2}\), and \(L_{3}\) are universal constants. Let \(\{\mathbf{W}_{m}\}_{m\in[M]}\) be sampled from Eq. (4). Let \(\mathbf{v}:\mathbb{R}^{(d+1)\times(d+1)}\to\mathbb{R}\) with \(\mathbb{E}_{\mathbf{W}}[\|\mathbf{v}(\mathbf{W})\|_{2}^{2}]\leq R^{2}\). Consider a random feature model associated with \(\overline{\sigma}\) (hereafter, we will refer to it as RF model)

\[f_{M}^{\mathbf{W}}(\mathbf{x}_{0:N};\mathbf{V})=\sum_{m=1}^{M}\frac{1}{N}\sum_ {i=1}^{N}\overline{\sigma}\big{(}\mathbf{W}_{m},\widetilde{\mathbf{x}}_{0} \widetilde{\mathbf{x}}_{i}^{\top}\big{)}\left\langle\mathbf{v}_{m},\widetilde {\mathbf{x}}_{i}\right\rangle,\] (25)

as well as the infinite-width version of the random feature model,

\[f_{\mathbf{v}}(\mathbf{x}_{0:N})=\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{\mathbf{ W}}\Big{[}\overline{\sigma}(\mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{ \mathbf{x}}_{i}^{\top})\left\langle\mathbf{v}(\mathbf{W}),\widetilde{\mathbf{ x}}_{i}\right\rangle\Big{]}.\] (26)

Note that both model RFA and BRFA correspond to special choices of \(\overline{\sigma}\). Thus, all lemmas and theorems in this section are applicable to both model RFA and model BRFA.

### Proof of Theorem 1

To prove Theorem 1, we first state two auxilliary lemmas, Lemma B.1 and B.2.

**Lemma B.1** (From infinite-width RF model to finite-width RF model).: _Consider \(f_{\mathbf{v}}\) that takes the form as Eq. (26), with \(\mathbb{E}_{\mathbf{W}}[\|\mathbf{v}(\mathbf{W})\|_{2}^{2}]\leq R^{2}\). Let \(\mathbf{x}_{0:N}\sim P\). Define the \(\|\cdot\|_{L^{2}(P)}^{2}\) norm by_

\[\|g\|_{L^{2}(P)}^{2}=\int g(\mathbf{x}_{0:N})^{2}P(\mathrm{d}\mathbf{x}_{0:N}).\]

_Then with probability at least \(1-\delta\), there exists a sequence of vectors \(\left\{\mathbf{v}_{m}\right\}_{m=1}^{M}\subseteq\mathbb{R}^{d+1}\) and constant universal \(C<\infty\) that only depends on \(L_{1}\), \(L_{2}\), and \(L_{3}\) s.t._

\[\Big{\|}f_{\mathbf{v}}-\frac{1}{N}\sum_{i=1}^{N}\sum_{m=1}^{M} \overline{\sigma}(\mathbf{W}_{m},\widetilde{\mathbf{x}}_{0}\widetilde{ \mathbf{x}}_{i}^{\top})\left\langle\mathbf{v}_{m},\widetilde{\mathbf{x}}_{i} \right\rangle\Big{\|}_{L^{2}(P)}^{2}\leq\frac{C(d^{2}+\log M)R^{2}\delta^{-1}}{M}\] (27) \[\sum_{m=1}^{M}\Big{\|}\mathbf{v}_{m}\Big{\|}_{2}\leq\sqrt{2}R+ \sqrt{\frac{CR^{2}\delta^{-1}}{M}}\quad\text{ and }\quad\sum_{m=1}^{M}\Big{\|}\mathbf{v}_{m}\Big{\|}_{2}^{2}\leq \frac{CR^{2}\delta^{-1}}{M}.\] (28)

**Lemma B.2**.: _Under the setting of Theorem 1, let \(f_{\star}\) be a target function of form (6) and (7). Then there exists an infinite-width RFA model (26) with \(\mathbf{v}:\mathbb{R}^{(d+1)\times(d+1)}\to\mathbb{R}\) such that_

\[f_{\star}=\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{\mathbf{W}}[\overline{\sigma}( \mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top})\left \langle\mathbf{v}(\mathbf{W}),\widetilde{\mathbf{x}}_{i}\right\rangle],\] (29)

_with_

\[\mathbb{E}_{\mathbf{W}}[\|\mathbf{v}(\mathbf{W})\|_{2}^{2}]\leq B(f_{\star}),\]

_where \(B(f_{\star})\) is as defined in (10)._

The proofs of Lemma B.1 and B.2 are given in Section B.1.1. Now we assume these two lemmas hold, and use them to prove Theorem 1.

Proof of Theorem 1.: For any function that takes form (6) and (7), by Lemma B.2, it admits representation (29) with \(\mathbb{E}_{\mathbf{W}}[\|\mathbf{v}(\mathbf{W})\|_{2}^{2}]\leq B(f_{\star})\). Then by Lemma B.1, since RFA model is a special case of the RF model, there exists \(\left\{\mathbf{v}_{m}\right\}_{m\in[M]}\) such that Eq. (27) and Eq. (28) hold with probability larger than \(1-\delta\). This proves Theorem 1.

#### b.1.1 Proof of auxiliary lemmas

Proof of Lemma b.1.: **Step 1. Proof of Eq. (27).** To prove Eq. (27), we use a truncation argument.

Fix a \(R_{\mathbf{W}}>0\) which we will choose its value later in the proof. Recall that we have \(\mathbf{W}_{1},\dots,\mathbf{W}_{m}\) i.i.d. with \(\mathbf{W}_{m,i,j}\mathbf{\sim_{\mathrm{iid}}N}(0,1/4)\). Define \(\mathbf{v}_{m}=\mathbf{v}(\mathbf{W}_{m})1\{\|\mathbf{W}_{m}\|_{\mathbf{Fr}} \leq R_{\mathbf{W}}/2\}/M\) for \(m=1,\dots,M\). Consider the truncated infinite-width random feature model \(f_{\mathbf{v}}^{R_{\mathbf{W}}}=\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{\mathbf{W }}[\overline{\sigma}(\mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{ \mathbf{x}}_{i}^{\top})\,\langle\mathbf{v}(\mathbf{W}),\widetilde{\mathbf{x}}_ {i}\rangle\,1\,\{\|\mathbf{W}\|_{\mathbf{Fr}}\leq R_{\mathbf{W}}/2\}]\), we have

\[\mathbb{E}_{\mathbf{W}}\Big{[}\Big{\|}f_{\mathbf{v}}^{R_{\mathbf{ W}}}-\frac{1}{N}\sum_{i=1}^{N}\sum_{m=1}^{M}\overline{\sigma}(\mathbf{W}, \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top})\,\langle \mathbf{v}_{m},\widetilde{\mathbf{x}}_{i}\rangle\,\Big{\|}_{L^{2}(P)}^{2} \Big{]}\] \[\leq\mathbb{E}_{\mathbf{X}}\Big{(}\mathbb{E}_{\mathbf{W}}\Big{[} \Big{\|}\frac{1}{N}\sum_{i=1}^{N}\overline{\sigma}(\mathbf{W},\widetilde{ \mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top})\widetilde{\mathbf{x}}_{i} \Big{\|}_{2}^{2}\Big{\|}\mathbf{v}(\mathbf{W})\Big{\|}_{2}^{2}\mathbf{1}\,\{\| \mathbf{W}\|_{\mathbf{Fr}}\leq R_{\mathbf{W}}/2\}\,\Big{]}\Big{)}/M\] \[\leq\mathbb{E}_{\mathbf{X}}\Big{(}\mathbb{E}_{\mathbf{W}}\Big{[} \frac{2}{N}\sum_{i=1}^{N}\|\widetilde{\mathbf{x}}_{i}\|_{2}^{2}(L_{1}^{2}\| \mathbf{W}\|_{2}^{2}+L_{3}^{2})\|\mathbf{v}(\mathbf{W})\|_{2}^{2}\mathbf{1}\, \{\|\mathbf{W}\|_{\mathbf{Fr}}\leq R_{\mathbf{W}}/2\}\,\Big{]}\Big{)}/M\] \[\leq\frac{\widetilde{C}R_{\mathbf{W}}^{2}R^{2}}{M},\]

where \(\widetilde{C_{1}}\) only depends on \(L_{1}\), \(L_{2}\), and \(L_{3}\). Then using Markov's inequality,

\[\Big{\|}f_{\mathbf{v}}^{R_{\mathbf{W}}}-\frac{1}{N}\sum_{i=1}^{N}\sum_{m=1}^{ M}\overline{\sigma}(\mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x }}_{i}^{\top})\,\langle\mathbf{v}_{m},\widetilde{\mathbf{x}}_{i}\rangle\, \Big{\|}_{L^{2}(P)}^{2}\leq\frac{3\widetilde{C}R_{\mathbf{W}}^{2}R^{2}}{ \delta M}\]

holds with probability at least \(1-\delta/3\). Next for the difference between \(f_{\mathbf{v}}^{R_{\mathbf{W}}}\) and \(f_{\mathbf{v}}\), we have

\[\|f_{\mathbf{v}}-f_{\mathbf{v}}^{R_{\mathbf{W}}}\|_{L^{2}(P)}^{2} \leq\mathbb{E}_{\mathbf{X}}\Big{[}\Big{(}\mathbb{E}_{\mathbf{W}} \Big{[}\frac{1}{N}\sum_{i=1}^{N}\overline{\sigma}(\mathbf{W},\widetilde{ \mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top})\,\langle\mathbf{v}, \widetilde{\mathbf{x}}_{i}\rangle\,\mathbf{1}\,\{\|\mathbf{W}\|_{\mathbf{Fr}} \geq R_{\mathbf{W}}/2\}\,\Big{]}\Big{)}^{2}\Big{]}\] \[\leq\widetilde{C_{2}}R^{2}\mathbb{P}(\|\mathbf{W}\|_{\mathbf{Fr} }>R_{\mathbf{W}}/2)^{\frac{1}{2}}.\]

Here \(\widetilde{C_{2}}\) is a constant that only depends on \(L_{1}\) and \(L_{2}\). By concentration of functions of Gaussian random vectors (Theorem A.1), \(\|\overline{\sigma}(\mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x }}_{i}^{\top})\|_{\psi_{2}}\leq L_{1}+L_{2}\) for any \(i\). So in the last inequality, we used the bound \((\mathbb{E}_{\mathbf{W}}[\overline{\sigma}(\mathbf{W},\widetilde{\mathbf{x}}_{0 }\widetilde{\mathbf{x}}_{i}^{\top})^{4}])^{1/2}\) by \(\Theta((L_{1}+L_{2})^{2})\). To bound \(\mathbb{P}(\|\mathbf{W}\|_{\mathbf{Fr}}>R_{\mathbf{W}}/2)\), we use concentration of functions of Gaussian random vectors (Theorem A.1) again, and get that

\[\mathbb{P}\big{(}\|\mathbf{W}\|_{\mathbf{Fr}}-\mathbb{E}(\|\mathbf{W}\|_{ \mathbf{Fr}})\geq t/2\big{)}\leq\exp\big{(}-t^{2}/2\big{)}.\]

Take \(C=max(\widetilde{C_{1}},\widetilde{C_{2}})\). Since \(\mathbb{E}(\|\mathbf{W}\|_{\mathbf{Fr}})\leq(\mathbb{E}\|\mathbf{W}\|_{ \mathbf{Fr}}^{2})^{1/2}\leq d+1\), by choosing \(R_{\mathbf{W}}=d+1+C\sqrt{\log M}\), the above probability is less than \(1/M^{2}\). Then

\[\Big{\|}f_{\mathbf{v}}-\frac{1}{N}\sum_{i=1}^{N}\sum_{m=1}^{M} \overline{\sigma}(\mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_ {i}^{\top})\,\langle\mathbf{v}_{m},\widetilde{\mathbf{x}}_{i}\rangle\,\Big{\|}_{L ^{2}(P)}^{2} \leq\frac{3CR_{\mathbf{W}}^{2}R^{2}}{\delta M}+CR^{2}\mathbb{P}(\|\mathbf{W}\|_{ \mathbf{Fr}}>R_{\mathbf{W}}/2)^{\frac{1}{2}}\] \[\leq\frac{C(\log M+d^{2})R^{2}}{\delta M}\]

with probability larger than \(1-\delta/3\). This proves Eq. (27).

**Step 2. Proof of Eq. (28).** By Chebyshev's inequality,

\[\mathbb{P}\Bigg{(}\sum_{m=1}^{M}\|\mathbf{v}_{m}\|_{2}-\mathbb{E}\Big{[}\,\| \mathbf{v}(\mathbf{W})\|_{2}\,\mathbf{1}\,\{\|\mathbf{W}_{m}\|_{\mathbf{Fr}}\leq R _{\mathbf{W}}/2\}\,\Big{]}\geq\sqrt{\frac{6R^{2}}{\delta M}}\Bigg{)}\]\[\leq\frac{\mathbb{E}\Big{[}\left\|\mathbf{v}(\mathbf{W})\right\|_{2}^{2} \mathbf{1}\left\{\|\mathbf{W}\|_{\mathsf{Fr}}\leq R_{\mathbf{W}}/2\right\}\Big{]} \delta M}{6R^{2}M}\leq\frac{\delta}{3}.\]

Combining with the fact that \(\mathbb{E}\Big{[}\left\|\mathbf{v}(\mathbf{W})\right\|_{2}\mathbf{1}\left\{\| \mathbf{W}\|_{\mathsf{Fr}}\leq R_{\mathbf{W}}/2\right\}\Big{]}\leq\sqrt{ \mathbb{E}\Big{[}\left\|\mathbf{v}(\mathbf{W})\right\|_{2}^{2}\Big{]}}\leq R\), we have

\[\mathbb{P}\Bigg{(}\sum_{m=1}^{M}\left\|\mathbf{v}_{m}\right\|_{2}\geq R+\sqrt{ \frac{6R^{2}}{\delta M}}\Bigg{)}\leq\frac{\delta}{3}.\]

For the second part of (28), Markov inequality gives

\[\mathbb{P}\Bigg{(}\sum_{m=1}^{M}\left\|\mathbf{v}_{m}\right\|_{2}^{2}\geq\frac {6R^{2}}{\delta M}\Bigg{)}\leq\frac{\mathbb{E}\Big{[}\left\|\mathbf{v}( \mathbf{W})\right\|_{2}^{2}\mathbf{1}\left\{\|\mathbf{W}_{m}\|_{\mathsf{Fr}} \leq R_{\mathbf{W}}/2\right\}\Big{]}\delta M}{6R^{2}M}\leq\frac{\delta}{3}.\]

This proves Eq. (28) and completes the proof of Lemma B.1. 

Proof of Lemma b.2.: To get the kernel of the RFA model, we have

\[K(\mathbf{x}_{0:N},\mathbf{x}_{0:N}^{\prime})=\frac{1}{N^{2}}\mathbb{E}_{ \mathbf{W}}\Big{[}\sum_{i,j=1}^{N}\sigma(\langle\mathbf{W},\widetilde{\mathbf{ x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\rangle)\sigma(\langle\mathbf{W}, \widetilde{\mathbf{x}}_{0}^{\prime}(\widetilde{\mathbf{x}}_{j}^{\prime})^{ \top}\rangle)\langle\widetilde{\mathbf{x}}_{i},\widetilde{\mathbf{x}}_{j}^{ \prime}\rangle\Big{]}.\]

We first consider a single component in the sum, which is

\[\mathbb{E}_{\mathbf{W}}\Big{[}\sigma\big{(}\langle\mathbf{W},\widetilde{ \mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\rangle\big{)}\sigma(\langle \mathbf{W},\widetilde{\mathbf{x}}_{0}^{\prime}(\widetilde{\mathbf{x}}_{j})^{ \prime}\rangle)\Big{]}\langle\widetilde{\mathbf{x}}_{i},\widetilde{\mathbf{x}}_{ j}^{\prime}\rangle.\] (30)

Let \(\mathsf{N}_{2}(\rho)\) denote a bivariate normal distribution with marginals are \(\mathsf{N}(0,1)\) and the correlation is \(\rho\in[-1,1]\). Then (30) can be expanded as follows:

\[\mathbb{E}_{\mathbf{W}}\Big{[}\sigma\big{(}\langle\mathbf{W}, \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\rangle\big{)} \sigma\big{(}\langle\mathbf{W},\widetilde{\mathbf{x}}_{0}^{\prime}(\widetilde{ \mathbf{x}}_{j}^{\prime})^{\top}\rangle\big{)}\Big{]}\langle\widetilde{ \mathbf{x}}_{i},\widetilde{\mathbf{x}}_{j}^{\prime}\rangle\] \[=\mathbb{E}_{Z_{1},Z_{2}\sim\mathsf{N}_{2}(u_{i,j})}\Big{[}\sigma( Z_{1})\sigma(Z_{2})\Big{]}\langle\widetilde{\mathbf{x}}_{i},\widetilde{\mathbf{x}}_{j}^{ \prime}\rangle\] \[=\frac{1}{2\pi}\Big{(}u_{i,j}(\pi/2-\arccos u_{i,j})+\sqrt{1-u_{i, j}^{2}}\Big{)}\langle\widetilde{\mathbf{x}}_{i},\widetilde{\mathbf{x}}_{j}^{ \prime}\rangle\] \[=\frac{1}{2\pi}\Bigg{(}1+\frac{\pi}{2}u_{i,j}+\sum_{\ell=1}^{ \infty}\frac{(2\ell-3)!!}{(2\ell)!!(2\ell-1)}u_{i,j}^{2\ell}\Bigg{)}\langle \widetilde{\mathbf{x}}_{i},\widetilde{\mathbf{x}}_{j}^{\prime}\rangle\] \[=\sum_{\ell\in\{0,1\}\cup\{2k\}_{k\geq 1}}c_{\ell}\left\langle \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top},\widetilde{ \mathbf{x}}_{0}^{\prime}(\widetilde{\mathbf{x}}_{j}^{\prime})^{\top}\right\rangle ^{\ell}4^{-\ell}\langle\widetilde{\mathbf{x}}_{i},\widetilde{\mathbf{x}}_{j}^{ \prime}\rangle\] \[=\sum_{\ell\in\{0,1\}\cup\{2k\}_{k\geq 1}}c_{\ell}\Big{\langle}2^{- \ell}(\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top})^{\otimes \ell}\otimes\widetilde{\mathbf{x}}_{i},2^{-\ell}(\widetilde{\mathbf{x}}_{0}^{ \prime}(\widetilde{\mathbf{x}}_{j}^{\prime})^{\top})^{\otimes\ell}\otimes \widetilde{\mathbf{x}}_{j}^{\prime}\Big{\rangle}.\]

Here the coefficients \(\{c_{\ell}\}\) satisfy

\[c_{0}=1/(2\pi),\ \ c_{1}=1/4,\ \ \ \text{and}\ \ \ c_{2\ell}=\frac{(2\ell-3)!!}{2\pi(2 \ell)!!(2\ell-1)}=O(\ell^{-\frac{5}{2}})\ \ \text{for}\ \ell\geq 1.\]

Therefore, the kernel can be expressed as:

\[K(\mathbf{x}_{0:N},\mathbf{x}_{0:N}^{\prime})\] \[=\frac{1}{N^{2}}\sum_{1\leq i,j\leq N}\sum_{\ell\in\{0,1\}\cup\{2k \}_{k\geq 1}}c_{\ell}\Big{\langle}2^{-\ell}(\widetilde{\mathbf{x}}_{0}\widetilde{ \mathbf{x}}_{i}^{\top})^{\otimes\ell}\otimes\widetilde{\mathbf{x}}_{i},2^{-\ell }(\widetilde{\mathbf{x}}_{0}^{\prime}(\widetilde{\mathbf{x}}_{j}^{\prime})^{ \top})^{\otimes\ell}\otimes\widetilde{\mathbf{x}}_{j}^{\prime}\Big{\rangle}\] \[=\sum_{\ell\in\{0,1\}\cup\{2k\}_{k\geq 1}}\Big{\langle}\frac{\sqrt{c_{ \ell}}}{N}\sum_{i=1}^{N}2^{-\ell}(\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x} }_{i}^{\top})^{\otimes\ell}\otimes\widetilde{\mathbf{x}}_{i},\frac{\sqrt{c_{ \ell}}}{N}\sum_{j=1}^{N}2^{-\ell}(\widetilde{\mathbf{x}}_{0}^{\prime}(\widetilde{ \mathbf{x}}_{j}^{\prime})^{\top})^{\otimes\ell}\otimes\widetilde{\mathbf{x}}_{j}^{ \prime}\Big{\rangle}.\]Now we reformulate the target function as

\[f_{\star} =\frac{1}{N}\sum_{i=1}^{N}\sum_{\ell=0}^{\infty}\sum_{\max\{r,s\}= \ell}\langle\widetilde{\mathbf{x}}_{0}^{\otimes r}\otimes\widetilde{\mathbf{x}} _{i}^{\otimes s},\mathbf{f}_{rs}\rangle\] \[=\frac{1}{N}\sum_{i=1}^{N}\sum_{\ell=2k,k\geq 0}^{\infty}\sum_{ \max\{r,s\}=\ell\,\operatorname*{or}\ell-1}\Big{\langle}(\widetilde{\mathbf{ x}}_{0}\otimes\widetilde{\mathbf{x}}_{i})^{\otimes\ell}\otimes\widetilde{\mathbf{x}} _{i},\widetilde{\mathbf{f}}_{rs}\Big{\rangle}\] \[=\sum_{\ell=2k,k\geq 0}^{\infty}\Big{\langle}\frac{2^{\ell}}{ \sqrt{c_{\ell}}}\sum_{\max\{r,s\}=\ell\,\operatorname*{or}\ell-1}\widetilde{ \mathbf{f}}_{rs},\frac{\sqrt{c_{\ell}}}{N}\sum_{i=1}^{N}2^{-\ell}(\widetilde{ \mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top})^{\otimes\ell}\otimes \widetilde{\mathbf{x}}_{i}\Big{\rangle},\]

where \(\widetilde{\mathbf{f}}_{rs}\) is a transpose of \(\mathbf{f}_{rs}\otimes\mathbf{1}_{d+1}^{\otimes(2\ell-r-s+1)}\) with \(\mathbf{1}_{d+1}=(0,\ldots,0,1)\), such that \(\langle\widetilde{\mathbf{x}}_{0}^{\otimes r}\otimes\widetilde{\mathbf{x}}_{ i}^{\otimes s},\mathbf{f}_{rs}\rangle=\langle(\widetilde{\mathbf{x}}_{0} \otimes\widetilde{\mathbf{x}}_{i})^{\otimes\ell}\otimes\widetilde{\mathbf{x}} _{i},\widetilde{\mathbf{f}}_{rs}\rangle\) for any \(r\geq 0\) and \(s\geq 0\). Then by the feature map equivalence property (23), the RKHS norm of \(f^{\star}\) can be bounded as

\[\|f^{\star}\|_{\mathcal{H}_{K}}^{2} \leq\Big{\|}\sum_{\ell=2k,k\geq 0}^{\infty}\Big{\langle}\frac{2^{ \ell}}{\sqrt{c_{\ell}}}\sum_{\max\{r,s\}=\ell\,\operatorname*{or}\ell-1} \widetilde{\mathbf{f}}_{rs},\frac{\sqrt{c_{\ell}}}{N}\sum_{i=1}^{N}2^{-\ell}( \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top})^{\otimes\ell} \otimes\widetilde{\mathbf{x}}_{i}\Big{\rangle}\Big{\|}_{\mathcal{H}_{K}}^{2}\] \[=\sum_{\ell=2k,k\geq 0}^{\infty}\Big{\langle}\frac{2^{\ell}}{ \sqrt{c_{\ell}}}\sum_{\max\{r,s\}=\ell\,\operatorname*{or}\ell-1}\widetilde{ \mathbf{f}}_{rs},\frac{2^{\ell}}{\sqrt{c_{\ell}}}\sum_{\max\{r,s\}=\ell\, \operatorname*{or}\ell-1}\widetilde{\mathbf{f}}_{rs}\Big{\rangle}\] \[=\sum_{\ell=2k,k\geq 0}^{\infty}4^{\ell}c_{\ell}^{-1}\Big{\|} \sum_{\max\{r,s\}=\ell\,\operatorname*{or}\ell-1}\widetilde{\mathbf{f}}_{rs} \Big{\|}_{\operatorname*{Fr}}^{2}\] \[\leq\,\sum_{k=0}^{\infty}4^{k}k^{4.5}\sum_{\max\{r,s\}=k}\left\| \widetilde{\mathbf{f}}_{rs}\right\|_{\mathbf{Fr}}^{2}.\]

Thus, using again the property (23) with the original feature map of the random feature model, there exists \(\mathbf{v}:\mathbb{R}^{(d+1)\times(d+1)}\rightarrow\mathbb{R}^{d+1}\) such that

\[f_{\star} =\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{\mathbf{W}}[\overline{ \sigma}(\mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{ \top})\,\langle\mathbf{v}(\mathbf{W}),\widetilde{\mathbf{x}}_{i}\rangle],\ \ \text{ with}\] \[\mathbb{E}_{\mathbf{W}}[\|\mathbf{v}(\mathbf{W})\|_{2}^{2}] \leq\,\sum_{k=0}^{\infty}4^{k}k^{4.5}\sum_{\max\{r,s\}=k}\| \widetilde{\mathbf{f}}_{rs}\|_{\mathbf{Fr}}^{2}.\]

Notice that \(\|\widetilde{\mathbf{f}}_{rs}\|_{\mathbf{Fr}}^{2}=\|\mathbf{f}_{rs}\|_{ \mathbf{Fr}}^{2}\) by our construction of \(\widetilde{\mathbf{f}}_{rs}\), so that the right-hand-side of the equation above coincides with Eq. (10). This proves Lemma B.2. 

### Preliminary proposition for Theorem 2

To prove Theorem 2, we first present and prove the following proposition that gives a high probability bound for the difference between the empirical risk and the population risk. In the proposition and lemmas below, we denote \(\mathbf{X}=\{\mathbf{x}_{0:N}^{(j)}\}_{j\in[n]}\) and \(\mathbf{y}=\{y_{j}\}_{j\in[n]}\).

**Proposition B.1**.: _Under the setting of Theorem 2. Consider the finite width RF model (25):_

\[f_{M}^{\mathbf{W}}(\mathbf{x}_{0:N};\mathbf{V})=\sum_{m=1}^{M}\frac{1}{N}\sum _{j=1}^{n}\overline{\sigma}\big{(}\mathbf{W}_{m},\widetilde{\mathbf{x}}_{0} \widetilde{\mathbf{x}}_{i}^{\top}\big{)}\,\langle\mathbf{v}_{m},\widetilde{ \mathbf{x}}_{i}\rangle\,.\]

_Then with probability at least \(1-\delta\) (w.r.t. \(\mathbf{W}\), \(\mathbf{y}\), and \(\mathbf{X}\)), we have_

\[\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\frac{1}{n}\sum_{j=1}^{ n}\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y_{j} \Big{)}-\mathbb{E}_{\mathbf{x}_{0:N},y}\ell\big{(}f_{M}^{\mathbf{W}}(\mathbf{x }_{0:N};\mathbf{V}),y\big{)}\Big{|}\] \[\lesssim K_{1}\sqrt{\frac{\log(dM)\log\left(nNM\right)}{n}}+\sqrt{ \log\left(\frac{6}{\delta}\right)}\Bigg{(}\frac{K_{1}}{\sqrt{n}}+\sqrt{\frac{K_{2 }}{M}}\Bigg{)}.\] (31)The main difficulty of the proof of Proposition B.1 comes from that \(\ell\big{(}f_{M}^{\mathbf{W}},y\big{)}\) might be unbounded and that \(\ell(f_{M}^{\mathbf{W}}(\mathbf{x}_{0:N}^{(j)}),y_{j})\) are not independent across \(j\) (since they share the same \(\left\{\mathbf{W}_{m}\right\}_{m\in[M]}\)). So we begin with several lemmas below.

**Lemma B.3**.: _Let \(\left\{\xi_{j}\right\}_{j\in[n]}\) be a set of \(i.i.d.\) Rademacher random variables. Under the setting of Proposition B.1,_

\[\mathbb{E}_{\mathbf{X},\mathbf{y},\mathbf{W},\boldsymbol{\xi}} \Big{[}\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\frac{1}{n}\sum_{j=1}^{n}\xi_ {j}\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y_{j}\Big{)}\Big{|}\Big{]}\lesssim K_{1}\sqrt{\frac{\log(dM)\log\left(nNM \right)}{n}}.\] (32)

_Furthermore, any fixed \(\mathbf{X}\) and \(\mathbf{y}\),_

\[\mathbb{E}_{\mathbf{W},\boldsymbol{\xi}}\Big{[}\sup_{\mathbf{V}\in\mathcal{V} _{M}}\Big{|}\frac{1}{n}\sum_{j=1}^{n}\xi_{j}\ell\Big{(}f_{M}^{\mathbf{W}} \Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y_{j}\Big{)}\Big{|}\Big{]} \lesssim K_{1}\sqrt{\frac{\log(dM)\log\left(nNM\right)}{n}}.\] (33)

**Lemma B.4**.: _Under the setting of Proposition B.1. With probability at least \(1-\delta/3\) over \(\mathbf{X}\), \(\mathbf{y}\), and \(\mathbf{W}\),_

\[\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\frac{1}{n}\sum_{j=1}^{ n}\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y_{j}\Big{)}-\mathbb{E}_{\mathbf{X},\mathbf{y},\mathbf{W}}\Big{[}\frac{1}{n} \sum_{j=1}^{n}\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)}; \mathbf{V}\Big{)},y_{j}\Big{)}\Big{]}\Big{|}\] \[\lesssim K_{1}\sqrt{\frac{\log(dM)\log\left(nNM\right)}{n}}+K_{1} \sqrt{\frac{\log\left(6/\delta\right)}{n}}.\] (35)

**Lemma B.5**.: _Under the setting of Proposition B.1. With probability at least \(1-\delta/3\) over \(\mathbf{X}\) and \(\mathbf{y}\),_

\[\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\mathbb{E}_{\mathbf{W}} \Big{[}\frac{1}{n}\sum_{j=1}^{n}\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x }_{0:N}^{(j)};\mathbf{V}\Big{)},y_{j}\Big{)}\Big{]}-\mathbb{E}_{\mathbf{X}, \mathbf{y},\mathbf{W}}\Big{[}\frac{1}{n}\sum_{j=1}^{n}\ell\Big{(}f_{M}^{ \mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y_{j}\Big{)}\Big{]} \Big{|}\] \[\lesssim\sqrt{\frac{K_{2}\log(6/\delta)}{M}}+K_{1}\sqrt{\frac{ \log(dM)\log\left(nNM\right)}{n}}.\] (36)

The proofs of Lemma B.3, B.4, B.5, and B.6 are contained in section B.2.1. Now assuming they hold, we proceed to prove Proposition B.1.

Proof of Proposition b.1.: Split the left-hand side of inequality (31), we have

\[\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\frac{1}{n}\sum_{j=1}^ {n}\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y_{j}\Big{)}-\mathbb{E}_{\mathbf{X},\mathbf{y}}\Big{(}\ell\Big{(}f_{M}^{ \mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y\Big{)}\Big{)} \Big{|}\] \[\leq\] \[\lesssim K_{1}\sqrt{\frac{\log(dM)\log\left(nNM\right)}{n}}+\sqrt{ \log\left(\frac{6}{\delta}\right)}\Bigg{(}\frac{K_{1}}{\sqrt{n}}+\sqrt{\frac{K _{2}}{M}}\Bigg{)}\]

with probability at least \(1-\delta\). Here the last inequality uses Lemma B.4, B.5, and B.6. This proves Proposition B.1.

#### b.2.1 Proof of auxiliary lemmas

Proof of Lemma b.3.: First using Rademacher contraction inequality, since \(\ell(0,y)\leq 1\), we can center it and only pay an extra term \(1/\sqrt{n}\) in the Rademacher complexity. Then by the Rademacher contraction property (Theorem A.4), the problem boils down to bounding the Rademacher complexity of \(f_{M}^{\mathbf{W}}\), which is

\[\mathbb{E}_{\mathbf{X},\mathbf{y},\mathbf{W},\boldsymbol{\xi}}\Big{[}\sup_{ \mathbf{V}\in\mathcal{V}_{M}}\Big{|}\frac{1}{n}\sum_{j=1}^{n}\xi_{j}f_{M}^{ \mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)}\Big{|}\Big{]}.\]

Fix \(\mathbf{X}\), \(\mathbf{y}\), and \(\mathbf{W}\), we have

\[\mathbb{E}_{\boldsymbol{\xi}}\Big{[}\sup_{\mathbf{V}\in\mathcal{ V}_{M}}\Big{|}\frac{1}{n}\sum_{j=1}^{n}\xi_{j}f_{M}^{\mathbf{W}}\Big{(} \mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)}\Big{|}\Big{]}\] \[=\] \[\leq K_{1}\mathbb{E}_{\boldsymbol{\xi}}\Big{[}\max_{m}\Big{\|} \frac{1}{n}\sum_{j=1}^{n}\xi_{j}\Big{[}\frac{1}{N}\sum_{i=1}^{N}\overline{ \sigma}(\mathbf{W}_{m},\widetilde{\mathbf{x}}_{0}^{(j)}\widetilde{\mathbf{x} }_{i}^{(j)\top})\widetilde{\mathbf{x}}_{i}^{(j)}\Big{]}\Big{\|}_{2}\Big{]}.\] (37)

By matrix Bernstein inequality (Theorem A.3), for any fixed \(m\),

\[\mathbb{P}\Big{[}\Big{\|}\frac{1}{n}\sum_{j=1}^{n}\xi_{j}\Big{[}\frac{1}{N} \sum_{i=1}^{N}\overline{\sigma}(\mathbf{W}_{m},\widetilde{\mathbf{x}}_{0}^{(j )}\widetilde{\mathbf{x}}_{i}^{(j)\top})\widetilde{\mathbf{x}}_{i}^{(j)}\Big{]} \Big{\|}_{2}\geq\varepsilon\Big{]}\leq 2d\exp\bigg{(}-\frac{n\varepsilon^{2}/2}{A^{2}+K \varepsilon/3}\bigg{)},\]

where

\[A = \max_{m}\sqrt{\frac{1}{nN^{2}}\sum_{i,j,k}\Big{[}\overline{\sigma }(\mathbf{W}_{m},\widetilde{\mathbf{x}}_{0}^{(j)}\widetilde{\mathbf{x}}_{i}^{ (j)\top})\overline{\sigma}(\mathbf{W}_{m},\widetilde{\mathbf{x}}_{0}^{(j)} \widetilde{\mathbf{x}}_{k}^{(j)})\left\langle\widetilde{\mathbf{x}}_{i}^{(j)},\widetilde{\mathbf{x}}_{k}^{(j)}\right\rangle\Big{]}}\] \[\lesssim \max_{m,i,j}\overline{\sigma}(\mathbf{W}_{m},\widetilde{\mathbf{ x}}_{0}^{(j)}\widetilde{\mathbf{x}}_{i}^{(j)\top}),\text{ and }\] \[K = \max_{i,m}\Big{\|}\frac{1}{N}\sum_{i=1}^{N}\overline{\sigma}( \mathbf{W}_{m},\widetilde{\mathbf{x}}_{0}^{(j)}\widetilde{\mathbf{x}}_{i}^{(j) \top})\widetilde{\mathbf{x}}_{i}^{(j)}\Big{\|}_{2}.\]

Using the union bound, we have

\[\mathbb{P}\Big{[}\max_{m\in[M]}\Big{\|}\frac{1}{n}\sum_{j=1}^{n}\xi_{j}\Big{[} \frac{1}{N}\sum_{i=1}^{N}\overline{\sigma}(\mathbf{W}_{m},\widetilde{\mathbf{ x}}_{0}^{(j)}\widetilde{\mathbf{x}}_{i}^{(j)\top})\widetilde{\mathbf{x}}_{i}^{(j)} \Big{]}\Big{\|}_{2}\geq\varepsilon\Big{]}\leq 2dM\exp\bigg{(}-\frac{n\varepsilon^{2}}{A^{2}+K \varepsilon/3}\bigg{)}.\]

Therefore, we can bound its expectation with

\[\mathbb{E}_{\boldsymbol{\xi}}\Big{[}\max_{m}\Big{\|}\frac{1}{n}\sum_{j=1}^{n} \xi_{j}\Big{[}\frac{1}{N}\sum_{i=1}^{N}\overline{\sigma}(\mathbf{W}_{m}, \widetilde{\mathbf{x}}_{0}^{(j)}\widetilde{\mathbf{x}}_{i}^{(j)\top}) \widetilde{\mathbf{x}}_{i}^{(j)}\Big{]}\Big{\|}_{2}\Big{]}\lesssim\Big{[} \sqrt{\frac{\log(dM)}{n}}A+\frac{\log(dM)}{n}K\Big{]}.\] (38)

Now take expectation over \(\mathbf{X}\), \(\mathbf{y}\), and \(\mathbf{W}\). Since \(n\geq\log(dM)\), we have

\[\mathbb{E}_{\mathbf{X},\mathbf{y},\mathbf{W}}\Big{[}\sqrt{\frac{ \log(dM)}{n}}A+\frac{\log(dM)}{n}K\Big{]}\] \[\lesssim \sqrt{\frac{\log(dM)\log\left(nNM\right)}{n}}.\]

Combine this with Eq. (37) and (38), we prove (32).

Fixing any \(\mathbf{X}\) and \(\mathbf{y}\), only taking expectation over \(\mathbf{W}\), we get

\[\mathbb{E}_{\mathbf{W}}\Big{[}\sqrt{\frac{\log(dM)}{n}}A+\frac{\log(dM)}{n}K \Big{]}\lesssim\sqrt{\frac{\log(dM)\log\left(nNM\right)}{n}}.\]

Combine this with Eq. (37) and (38), we prove (33). This finishes the proof of Lemma B.3.

Proof for Lemma b.4.: Denote \(\mathbf{X}=\left\{\mathbf{x}_{0:N}^{(j)}\right\}_{j\in[n]}\) and \(\mathbf{y}=\left\{y_{j}\right\}_{j\in[n]}\) and denote

\[g(\mathbf{W}_{1:M};\mathbf{X},\mathbf{y})=\sup_{\mathbf{V}\in\mathcal{V}_{M}} \Big{|}\frac{1}{n}\sum_{j=1}^{n}\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x }_{0:N}^{(j)};\mathbf{V}\Big{)},y_{j}\Big{)}-\mathbb{E}_{\mathbf{W}}\Big{[} \frac{1}{n}\sum_{j=1}^{n}\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x}_{0:N}^ {(j)};\mathbf{V}\Big{)},y_{j}\Big{)}\Big{]}\Big{|}.\]

Given \(\mathbf{W}_{1:M}=\left\{\mathbf{W}_{m}\right\}_{m=1}^{M}\) and \(\mathbf{W}_{1:M}^{\prime}=\left\{\mathbf{W}_{m}^{\prime}\right\}_{m=1}^{M}\), we define \(\|\mathbf{W}_{1:M}-\mathbf{W}_{1:M}^{\prime}\|_{\mathbf{F}_{\mathbf{f}}}=( \sum_{m\in[M]}\|\mathbf{W}_{m}-\mathbf{W}_{m}^{\prime}\|_{\mathbf{F}_{ \mathbf{f}}}^{2})^{1/2}\). Then we have

\[g(\mathbf{W}_{1:M};\mathbf{X},\mathbf{y})-g(\mathbf{W}_{1:M}^{ \prime};\mathbf{X},\mathbf{y})\] \[\leq\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\frac{1}{n}\sum_{j =1}^{n}\Big{[}\ell\big{(}f_{M}^{\mathbf{W}},y_{j}\big{)}-\ell\Big{(}f_{M}^{ \mathbf{W}^{\prime}},y_{j}\big{)}\Big{]}\Big{|}\] \[\leq\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\frac{1}{Nn}\sum_{i,j,m}\Big{[}\overline{\sigma}(\mathbf{W},\widetilde{\mathbf{x}}_{0}^{(j)} \widetilde{\mathbf{x}}_{i}^{(j)\top})-\overline{\sigma}(\mathbf{W}^{\prime}, \widetilde{\mathbf{x}}_{0}^{(j)}\widetilde{\mathbf{x}}_{i}^{(j)\top})\Big{]} \left\langle\mathbf{v}_{m},\widetilde{\mathbf{x}}_{i}\right\rangle\Big{|}\] \[\lesssim\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\sum_{m}\| \mathbf{W}_{m}-\mathbf{W}_{m}^{\prime}\|_{\mathbf{F}_{\mathbf{f}}}\|_{ \mathbf{v}_{m}}\|_{2}\Big{|}\] \[\lesssim\|\mathbf{W}_{1:M}-\mathbf{W}_{1:M}^{\prime}\|_{\mathbf{ F}_{\mathbf{f}}}\sqrt{\frac{K_{2}}{M}}.\]

Since \(\mathbf{W}_{m}\) has independent standard Gaussian entries, by Gaussian concentration inequality (Theorem A.1), we have that

\[\mathbb{P}\Big{[}\Big{|}g(\mathbf{W}_{1:M};\mathbf{X},\mathbf{y})-\mathbb{E}_ {\mathbf{W}}\Big{[}g(\mathbf{W}_{1:M};\mathbf{X},\mathbf{y})\mid\mathbf{X}, \mathbf{y}\Big{]}\Big{|}\geq\varepsilon\mid\mathbf{X},\mathbf{y}\Big{]}\leq 2 \exp\bigg{(}-\frac{M\varepsilon^{2}}{4K_{2}}\bigg{)}.\] (39)

For the conditional expectation, by symmetrization, we have

\[\mathbb{E}_{\mathbf{W}}\Big{[}g(\mathbf{W}_{1:M};\mathbf{X}, \mathbf{y})\mid\mathbf{X},\mathbf{y}\Big{]}\] \[=\mathbb{E}_{\mathbf{W}}\Big{[}\sup_{\mathbf{V}\in\mathcal{V}_{M} }\Big{|}\sum_{j=1}^{n}\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{( j)};\mathbf{V}\Big{)},y_{j}\Big{)}-\mathbb{E}_{\mathbf{W}}\Big{[}\ell\Big{(}f_{M}^{ \mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y_{j}\Big{)}\Big{]} \Big{|}\mid\mathbf{X},\mathbf{y}\Big{]}\] \[\leq 2\mathbb{E}_{\mathbf{W}:\boldsymbol{\xi}}\Big{[}\sup_{ \mathbf{V}\in\mathcal{V}_{M}}\Big{|}\sum_{j=1}^{n}\ell\Big{(}f_{M}^{\mathbf{W}} \Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y_{j}\Big{)}\Big{|}\mid \mathbf{X},\mathbf{y}\Big{]}\] \[\lesssim K_{1}\sqrt{\frac{\log(dM)\log\left(nNM\right)}{n}}\] (40)

for any \(\mathbf{X}\) and \(\mathbf{y}\), where the last inequality uses Lemma B.3. Combining (39) and (40) and taking \(\varepsilon=2\sqrt{K_{2}\log\left(6/\delta\right)/M}\), we have

\[\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\frac{1}{n}\sum_{j=1}^{n }\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y _{j}\Big{)}-\mathbb{E}_{\mathbf{W}}\Big{[}\frac{1}{n}\sum_{j=1}^{n}\ell\Big{(}f_ {M}^{\mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y_{j}\Big{)} \Big{]}\Big{|}\] \[\lesssim\sqrt{\frac{K_{2}}{M}}\sqrt{\log\frac{6}{\delta}}+K_{1} \sqrt{\frac{\log(dM)\log\left(nNM\right)}{n}},\] (41)

for any \(\mathbf{X}\) and \(\mathbf{y}\). Since the right-hand side is irrelevant to \(\mathbf{X}\) and \(\mathbf{y}\), we get (34). This proves Lemma B.4. 

Proof of Lemma b.5.: Let

\[h(\mathbf{x}_{0:N},\mathbf{y})=\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|} \mathbb{E}_{\mathbf{W}}\Big{[}\frac{1}{n}\sum_{j=1}^{n}\ell\big{(}f_{M}^{ \mathbf{W}},y_{j}\big{)}\Big{]}-\mathbb{E}_{\mathbf{X},\mathbf{y},\mathbf{W}} \Big{[}\frac{1}{n}\sum_{j=1}^{n}\ell\big{(}f_{M}^{\mathbf{W}},y_{j}\big{)} \Big{]}\Big{|}.\] (42)For each \(i\in[n]\), let \(\{\mathbf{X}^{\prime},\mathbf{y}^{\prime}\}\) differs with \(\{\mathbf{X},\mathbf{y}\}\) only on \(i\)-th data point. We have

\[h(\mathbf{X},\mathbf{y})-h(\mathbf{X}^{\prime},\mathbf{y}^{\prime})\] \[\leq\] \[\leq\] \[\leq\] \[\lesssim\] \[\lesssim\] \[\lesssim\] \[\lesssim\]

Therefore, \(h(\mathbf{x}_{0:N},\mathbf{y})\) satisfies the bounded difference property with the parameter \(\left\{L_{i}\right\}_{i=1}^{n}\) uniformly bounded by \(\Theta(K_{1}/n)\). By bounded difference inequality (Theorem A.2), there's a constant \(\widetilde{C}\) such that

\[\mathbb{P}\Big{[}\Big{|}h(\mathbf{X},\mathbf{y})-\mathbb{E}_{\mathbf{X}}\Big{[} h(\mathbf{X},\mathbf{y})\Big{]}\Big{|}\geq\varepsilon\Big{]}\leq 2\exp \left(-\frac{\widetilde{C}n\varepsilon^{2}}{{K_{1}}^{2}}\right).\]

Combining with Lemma B.3, we have

\[\mathbb{E}_{\mathbf{X},\mathbf{y}}\Big{[}h(\mathbf{X},\mathbf{y}) \Big{]}\] \[\leq 2\mathbb{E}_{\mathbf{X},\mathbf{y},\mathbf{\xi}}\Big{[}\sup_{ \mathbf{V}\in\mathcal{V}_{M}}\Big{|}\frac{1}{n}\sum_{j=1}^{n}\xi_{j}\mathbb{E} _{\mathbf{W}}\Big{[}\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j) };\mathbf{V}\Big{)},y_{j}\Big{)}\Big{]}\Big{|}\Big{]}\] \[\lesssim K_{1}\sqrt{\frac{\log(dM)\log\left(nNM\right)}{n}}.\]

Therefore, by taking \(\varepsilon=2K_{1}[\log{(6/\delta)}/(n\widetilde{C})]^{1/2}\), we have

\[\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\mathbb{E}_{\mathbf{W}} \Big{[}\frac{1}{n}\sum_{j=1}^{n}\ell\big{(}f_{M}^{\mathbf{W}},y_{j}\big{)} \Big{]}-\mathbb{E}_{\mathbf{X},\mathbf{y},\mathbf{W}}\Big{[}\frac{1}{n}\sum_{ j=1}^{n}\ell\big{(}f_{M}^{\mathbf{W}},y_{j}\big{)}\Big{]}\Big{|}\] \[\lesssim K_{1}\sqrt{\frac{\log(dM)\log\left(nNM\right)}{n}}+K_{1} \sqrt{\frac{\log\left(1/\delta\right)}{n}}\]

with probability at least \(1-\delta\). This proves Lemma B.5. 

Proof of Lemma b.6.: Denote

\[\varphi(\mathbf{W}_{1:M})=\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\mathbb{ E}_{\mathbf{X},\mathbf{y}}\Big{[}\frac{1}{n}\sum_{j=1}^{n}\ell\big{(}f_{M}^{ \mathbf{W}},y_{j}\big{)}\Big{]}-\mathbb{E}_{\mathbf{X},\mathbf{y},\mathbf{W}} \Big{[}\frac{1}{n}\sum_{j=1}^{n}\ell\big{(}f_{M}^{\mathbf{W}},y_{j}\big{)} \Big{]}\Big{|}.\]

Similar to the proof of Lemma B.4. Given \(\mathbf{W}_{1:M}=\{\mathbf{W}_{m}\}_{m=1}^{M}\) and \(\mathbf{W}_{1:M}^{\prime}=\{\mathbf{W}_{m}^{\prime}\}_{m=1}^{M}\), define \(\|\mathbf{W}_{1:M}-\mathbf{W}_{1:M}^{\prime}\|_{\mathsf{Fr}}=\sqrt{\sum_{m}\| \mathbf{W}_{m}-\mathbf{W}_{m}^{\prime}\|_{\mathsf{Fr}}^{2}}\). We have

\[\varphi(\mathbf{W}_{1:M})-\varphi(\mathbf{W}_{1:M}^{\prime})\] \[\leq \sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\frac{1}{n}\sum_{i,j,m} \mathbb{E}_{\mathbf{X},\mathbf{y}}\Big{\{}\Big{[}\overline{\sigma}(\mathbf{W},\widetilde{\mathbf{x}}_{0}^{(j)}\widetilde{\mathbf{x}}_{i}^{(j)\top})- \overline{\sigma}(\mathbf{W}^{\prime},\widetilde{\mathbf{x}}_{0}^{(j)} \widetilde{\mathbf{x}}_{i}^{(j)\top})\Big{]}\,\langle\mathbf{v}_{m}, \widetilde{\mathbf{x}}_{i}\rangle\Big{\}}\,\Big{|}\]\[\leq L_{D}\big{(}f_{\mathbf{v},M}^{\mathbf{W}}\big{)}-\widehat{L}_{D} \big{(}f_{\mathbf{v},M}^{\mathbf{W}}\big{)}+\widehat{L}_{D}\big{(}f_{\mathbf{v},M}^{\mathbf{W}}\big{)}-\widehat{L}_{D}\big{(}f_{\mathbf{v}^{\star},M}^{\mathbf{W }}\big{)}+\widehat{L}_{D}\big{(}f_{\mathbf{v}^{\star},M}^{\mathbf{W}}\big{)}\] \[\quad-L_{D}\big{(}f_{\mathbf{v}^{\star},M}^{\mathbf{W}}\big{)}+L_ {D}\big{(}f_{\mathbf{v}^{\star},M}^{\mathbf{W}}\big{)}-L_{D}\big{(}f_{\mathbf{ v},M}^{\mathbf{W}}\big{)}+L_{D}\big{(}f_{\mathbf{v},M}^{\mathbf{W}}\big{)}-L_{D}(f ^{\star})\] (48) \[\leq L_{D}\big{(}f_{\mathbf{v},M}^{\mathbf{W}}\big{)}-\widehat{L}_ {D}\big{(}f_{\mathbf{v},M}^{\mathbf{W}}\big{)}+\widehat{L}_{D}\big{(}f_{ \mathbf{v}^{\star},M}^{\mathbf{W}}\big{)}-L_{D}\big{(}f_{\mathbf{v}^{\star},M }^{\mathbf{W}}\big{)}+L_{D}\big{(}f_{\mathbf{v},M}^{\mathbf{W}}\big{)}-L_{D}( f^{\star})\] (49) \[\leq 2\sup_{f\in\mathcal{V}_{M}}\Big{|}L_{D}(f)-\widehat{L}_ {D}(f)\Big{|}+\|f_{\mathbf{v},M}^{\mathbf{W}}-f_{\star}\|_{L^{2}(P)}\] (50)\[\lesssim K_{1}\sqrt{\frac{\log(dM)\log(nNM)}{n}}+\sqrt{\log\left(\frac{1}{ \delta}\right)}\Bigg{(}\frac{K_{1}}{\sqrt{n}}+\sqrt{\frac{K_{2}}{M}}\Bigg{)}+ \sqrt{\frac{(d^{2}+\log M)B(f_{\star})\delta^{-1}}{M}}\] (51) \[\lesssim\sqrt{\frac{B(f_{\star})}{n}}\Big{(}\sqrt{\log(dM)\log( nNM)}+\sqrt{\log\left(\delta^{-1}\right)}\Big{)}+\sqrt{\frac{(d^{2}+\log M)B(f_{ \star})\delta^{-1}}{M}}\] (52) \[\lesssim\sqrt{\frac{B(f_{\star})[\log(dM)\log(nNM)+\log(\delta^{ -1})]}{n}}+\sqrt{\frac{(d^{2}+\log M)B(f_{\star})\delta^{-1}}{M}},\] (53)

where from (48) to (49) we use the definition of \(f_{\mathbf{\psi},M}^{\mathbf{W}}\) and \(f_{\mathbf{\psi}^{*},M}^{\mathbf{W}}\). From (49) to (50), we bound \(\ll\)\(\mathtt{N}\) notes: \(\mathtt{fc\gg}\)\(L_{D}(f_{\mathbf{\psi},M}^{\mathbf{W}})-\widehat{L}_{D}(f_{\mathbf{\psi},M}^{ \mathbf{W}})\) and \(L_{D}(f_{\mathbf{\psi}^{*},M}^{\mathbf{W}})-\widehat{L}_{D}(f_{\mathbf{\psi} ^{*},M}^{\mathbf{W}})\) by \(\sup_{f}|L_{D}(f)-\widehat{L}_{D}(f)|\) and use the Lipschitzness of \(\ell(f,y)\). From (50) to (51), we use Proposition B.1 and Lemma B.1. From (51) to (52), we insert the value of \(K_{1}\) and \(K_{2}\) into the equation. This proves Theorem 2. 

### Proof of Examples in Section 3

#### b.4.1 Excess risk of \(\mathtt{RFMLP}\)

Denote \(\mathcal{F}\) to be the set of all functions in the function class (6) and (7), i.e.,

\[\mathcal{F}=\Big{\{}f_{\star}(\mathbf{x}_{0:N})=\frac{1}{N}\sum_{i=1}^{N}\sum _{r,s\geq 0}^{\infty}\big{\langle}\mathbf{x}_{0}^{\otimes r}\otimes\mathbf{x}_{i}^{ \otimes s},\mathbf{f}_{rs}\big{\rangle}:\;\mathbf{f}_{rs}\in\mathbb{R}^{d^{r+ s}}\text{ symmetric },r,s\geq 0\Big{\}}.\] (54)

Consider the \(\mathtt{RFMLP}\) model

\[f_{M}^{\text{MLP}}(\mathbf{x}_{0:N};\mathbf{v})=\sum_{m=1}^{M}\sigma\big{(} \left\langle\mathbf{w}_{m},\operatorname{vec}(\mathbf{x}_{0:N})\right\rangle \big{)}\cdot v_{m},\quad\quad\{\mathbf{w}_{m}\}_{m\in[M]\cap\mathrm{iid}} \mathsf{N}(\mathbf{0},\mathbf{I}/(N+2)),\] (55)

where \(\operatorname{vec}(\mathbf{x}_{0:N})=[\mathbf{x}_{0};\mathbf{x}_{1};\ldots; \mathbf{x}_{N};1]\in\mathbb{R}^{dN+d+1}\). For target functions that take forms in (6) and (7), define \(B_{\mathrm{MLP}}(f_{\star})=\sum_{k=0}^{\infty}\widetilde{C}_{k}\sum_{r+s=k} \|\mathbf{f}_{rs}\|_{\mathbf{f}_{\star}}^{2}\) with \(\widetilde{C}_{k}=k^{3.5}(N+2)^{2k}\). In case where \(f_{\star}\) admits multiple representations of the form (7), \(B_{\mathrm{MLP}}(f_{\star})\) is the infimum of the right-hand-side over all such representations.

Then we consider the empirical risk minimizer over the \(\mathtt{RFMLP}\) model:

\[\widehat{\mathbf{v}}=\arg\min_{\mathbf{v}\in\mathcal{V}_{M}}\widehat{L}_{D}(f _{M}^{\text{MLP}}(\cdot;\mathbf{v})),\qquad\widehat{L}_{D}(f)=\tfrac{1}{n} \sum_{j=1}^{n}\ell(f(\mathbf{x}_{0:N}^{(j)}),y_{j}),\] (56)

where the constrained class \(\mathcal{V}_{M}^{\text{MLP}}\) gives

\[\mathcal{V}_{M}^{\text{MLP}}=\Big{\{}\mathbf{v}=\{v_{m}\}_{m=1}^{M}:\;\sum_{ m=1}^{M}|v_{m}|\leq K_{1},\sum_{m=1}^{M}v_{m}^{2}\leq K_{2}/M\Big{\}}\,.\] (57)

**Proposition B.2** (The sample complexity of \(\mathtt{RFMLP}\)).: _Let \(f_{\star}=\arg\min_{f\in\mathcal{F}}L_{D}(f)\) be the population risk minimizer within the target function class (54). Assume \(M>\delta^{-1}\) and \(n>\log(dM)\). Take \(K_{1}=C\sqrt{B_{\mathrm{MLP}}(f_{\star})}\) and \(K_{2}=CB_{\mathrm{MLP}}(f_{\star})\delta^{-1}\) in (57), with \(C\) being a constant. Let \(\widehat{f}_{M}^{\text{MLP}}=f_{M}^{\text{MLP}}(\cdot;\widehat{\mathbf{v}})\) be empirical risk minimizer of model \(\mathtt{RFMLP}\). Then for any joint distribution \(\mathtt{P}\), with probability at least \(1-\delta\) over \(\{\mathbf{w}_{m}\}_{m\in[M]}\) sampled according to (55) and \(\{(\mathbf{x}_{0:N}^{(j)},y_{j})\}_{j\in[n]}\sim_{iid}\mathtt{P}\), the excess risk is bounded by_

\[L_{D}(\widehat{f}_{M}^{\text{MLP}})-L_{D}(f_{\star})\leq\widetilde{\mathcal{O} }\Bigg{(}\sqrt{B_{\mathrm{MLP}}(f_{\star})}\bigg{[}\sqrt{\frac{1}{n}}+\sqrt{ \frac{d^{2}\delta^{-1}}{M}}\bigg{]}\Bigg{)}.\] (58)

Proof of Proposition B.2.: The proof is basically the same as that of \(\mathtt{RFA}\) model. We only give a sketch of the proof. Firstly, with a few modifications of the proof, we can show that Lemma B.1 also holds for \(\mathtt{RFMLP}\), with \(B(f_{\star})\) replaced with \(B_{\mathrm{MLP}}(f_{\star})\). Lemma B.2 is slightly different, we have the kernel expansion:

\[K(\mathbf{x}_{0:N},\mathbf{x}_{0:N}^{\prime})=\sum_{\ell\in\{0,1\}\cup\{2k\}_{k \geq 1}}\Big{\langle}\sqrt{c_{\ell}}(N+2)^{-\ell}\Big{[}\operatorname{ vec}(\mathbf{x}_{0:N})\Big{]}^{\otimes\ell},\sqrt{c_{\ell}}(N+2)^{-\ell}\Big{[} \operatorname{vec}(\mathbf{x}_{0:N}^{\prime})\Big{]}^{\otimes\ell}\Big{\rangle}.\]Therefore we can rewrite \(f_{\star}\) as

\[f_{\star}=\sum_{\ell\in\{0,1\}\cup\{2k\}_{k\geq 1}}\Big{\langle}\frac{(N+2)^{ \ell}}{N\sqrt{c_{\ell}}}\sum_{i=1}^{N}\sum_{r+s=\ell}^{N}\widetilde{\mathbf{f}} _{rs,i},\sqrt{c_{\ell}}(N+2)^{-\ell}\Big{[}\operatorname{vec}(\mathbf{x}_{0:N }^{\prime})\Big{]}^{\otimes\ell}\Big{\rangle},\] (59)

where \(\langle\widetilde{\mathbf{f}}_{rs,i},[\operatorname{vec}(\mathbf{x}_{0:N}^{ \prime})]^{\otimes\ell}\rangle=\langle\mathbf{f}_{rs},\mathbf{x}_{0}^{\otimes r }\otimes\mathbf{x}_{i}^{\otimes s}\rangle\). Thus,

\[\|f_{\star}\|_{\mathcal{H}_{K}}^{2}\leq\sum_{k=0}^{\infty}\widetilde{C}_{k}\sum _{r+s=k}\|\mathbf{f}_{rs}\|_{\widetilde{\mathbf{f}}_{\mathbf{r}}}^{2}\quad \text{with}\quad\widetilde{C}_{k}=k^{3.5}(N+2)^{2k}.\]

The right-hand-side gives the formulation of \(B_{\mathrm{MLP}}(f_{\star})\). Then, a similar version of Proposition B.1 holds for RFMLP model \(f_{M}^{\mathbf{W}}\), i.e., with probability at least \(1-\delta\) (w.r.t. \(\mathbf{W}\), \(\mathbf{y}\), and \(\mathbf{X}\)),

\[\sup_{\mathbf{V}\in\mathcal{V}_{M}}\Big{|}\frac{1}{n}\sum_{j=1}^{ n}\ell\Big{(}f_{M}^{\mathbf{W}}\Big{(}\mathbf{x}_{0:N}^{(j)};\mathbf{V}\Big{)},y_{ j}\Big{)}-\mathbb{E}_{(\mathbf{x}_{0:N},y)\sim\mathrm{p}}\ell\big{(}f_{M}^{ \mathbf{W}}(\mathbf{x}_{0:N};\mathbf{V}),y\big{)}\Big{|}\] \[\lesssim K_{1}\sqrt{\frac{(N+2)\log(dM)\log(nNM)}{n}}+\sqrt{(N+2) \log(1/\delta)}\Bigg{(}\frac{K_{1}}{\sqrt{n}}+\sqrt{\frac{K_{2}}{M}}\Bigg{)}.\] (60)

Then combining all of the above equations and following the proof in Section B.2, we get (58). 

**Remark 1**.: _The representation in (59) is not unique. With a more careful choice of the representation of the target function \(f_{\star}\), we can get a better bound for \(B_{\mathrm{MLP}}(f_{\star})\), which is given by_

\[B_{\mathrm{MLP}}(f_{\star})=\sum_{k=0}^{\infty}\widetilde{C}_{k}\sum_{r+s=k}\| \mathbf{f}_{rs}\|_{\widetilde{\mathbf{f}}_{\mathbf{r}}}^{2}\quad\text{with} \quad\widetilde{C}_{k}=k^{3.5}[(N+2)/2]^{2k}.\] (61)

#### b.4.2 Proofs of Examples

**Proposition B.3** (Restatement of Example 1).: _For functions of \(\mathbf{x}_{0}\) of the form_

\[f_{\star}(\mathbf{x}_{0:N})=\sum_{k=0}^{\infty}\left\langle\mathbf{x}_{0}^{ \otimes k},\mathbf{A}_{k}\right\rangle,\ \ \mathbf{A}_{k}\in\mathbb{R}^{d^{k}},\]

_we have that \(B(f_{\star})=\sum_{k=0}^{\infty}C_{k}\left\|\mathbf{A}_{k}\right\|_{\widetilde {\mathbf{f}}_{\mathbf{r}}}^{2}\). Setting \(M=\Theta(d^{2}n)\), the excess risk bound gives \(\widetilde{\mathcal{O}}(\sqrt{\sum_{k=0}^{\infty}k^{4.5}4^{k}\|\mathbf{A}_{k} \|_{\widetilde{\mathbf{f}}_{\mathbf{r}}}^{2}/n})\). Moreover, consider \(f_{\star}(\mathbf{x}_{0:N})=(\boldsymbol{\beta}^{\top}\mathbf{x}_{0})^{p}\). The above excess risk of RFA model and the RFMLP model scales as_

\[\mathtt{RFA}:\widetilde{\mathcal{O}}\Big{(}\mathrm{Poly}(p)\sqrt{4^{p}\| \boldsymbol{\beta}\|_{2}^{2p}/n}\,\Big{)},\quad\quad\mathtt{RFMLP}:\widetilde{ \mathcal{O}}\Big{(}\mathrm{Poly}(p)\sqrt{((N+2))^{p}\|\boldsymbol{\beta}\|_{2} ^{2p}/n}\Big{)}.\]

Proof of Proposition b.3.: This follows by direct calculation. 

**Proposition B.4** (Restatement of Example 2).: _For \(f_{\star}=\frac{1}{N}\sum_{i=1}^{N}\psi(\langle\boldsymbol{\beta},\mathbf{x}_{i }\rangle)\) with \(\psi(z)=z\arctan(z/\eta)\) with \(\eta>2\) and \(\|\boldsymbol{\beta}\|_{2}=1\). The excess risk bound of RFA model and the RFMLP model are_

\[\mathtt{RFA}:\widetilde{\mathcal{O}}\Big{(}\sqrt{\sum_{k=1}^{\infty}k^{4.5}(2 /\eta)^{2k}/n}\Big{)}=\widetilde{\mathcal{O}}(\sqrt{1/n}),\quad\mathtt{RFMLP}: \widetilde{\mathcal{O}}\Big{(}\sqrt{\sum_{k=1}^{\infty}k^{4.5}[(N+2)/2\eta]^ {2k}/n}\Big{)}.\]

Proof of Proposition b.4.: Use the power series expansion of \(\psi\), we have

\[f_{\star}=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{\infty}(-1)^{k}\frac{\langle \boldsymbol{\beta},\mathbf{x}_{i}\rangle^{2k}}{(2k-1)\eta^{2k-1}}.\]

Plug it into the formula of \(B(f_{\star})\) (10) and \(B_{\mathrm{MLP}}(f_{\star})\) (61), we get

\[B(f_{\star})=\sum_{k=1}^{\infty}(2k)^{4.5}4^{2k}\|\boldsymbol{\beta}/\eta\|_{2} ^{4k}=\Theta\Big{(}\sum_{k=1}^{\infty}k^{4.5}(2/\eta)^{2k}\Big{)},\]

and \(B_{\mathrm{MLP}}(f_{\star})=\Theta(\sum_{k=1}^{\infty}k^{4.5}[(N+2)/2\eta]^{2k})\). Therefore, by Theorem 2 and Proposition B.2, we get their excess risk. This proves Proposition B.4.

**Proposition B.5** (Restatement of Example 3).: _For \(f_{1,\star}=\frac{1}{N}\sum_{i=1}^{N}\left\langle\mathbf{x}_{0},\mathbf{x}_{i} \right\rangle^{p}\), the excess risk bound of_ RFA _(by Theorem 2) and_ RFMLP _scale as_

\[\mathtt{RFA}:\widetilde{\mathcal{O}}\Big{(}\mathrm{Poly}(p)\sqrt{(4d)^{p}/n} \Big{)},\qquad\mathtt{RFMLP}:\widetilde{\mathcal{O}}\Big{(}\mathrm{Poly}(p) \sqrt{[(N+2)d]^{p}/n}\Big{)}.\]

_For \(f_{2,\star}=\frac{1}{N}\sum_{i=1}^{N}\cos(\left\langle\mathbf{x}_{0},\mathbf{x }_{i}\right\rangle)\left\langle\mathbf{x}_{i}^{\otimes p},\mathbf{G}\right\rangle\) with \(\|\mathbf{G}\|_{\mathbf{Fr}}=1\). Then the excess risk bound of_ RFA _and_ RFMLP _scale as_

\[\mathtt{RFA}:\widetilde{\mathcal{O}}\bigg{(}\mathrm{Poly}(pd)\sqrt{e^{4\sqrt{ d}}4^{p}/n}\bigg{)},\quad\mathtt{RFMLP}:\widetilde{\mathcal{O}}\Big{(} \mathrm{Poly}(pNd)\sqrt{e^{2(N+2)d}(N+2)^{p}/n}\Big{)}.\]

Proof of Proposition B.5.: For \(f_{1,\star}\), direct calculation gives the value of \(B(f_{1,\star})\) and the excess risk follows. For \(f_{2,\star}\), using the Taylor expansion of \(\cos(z)\), we get

\[f_{2,\star}=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=0}^{\infty}(-1)^{k}\frac{\left\langle \mathbf{x}_{0},\mathbf{x}_{i}\right\rangle^{2k}}{(2k)!}\left\langle\mathbf{x }_{i}^{\otimes p},\mathbf{G}\right\rangle=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=0}^ {\infty}\left\langle(\mathbf{x}_{0}\mathbf{x}_{i}^{\top})^{\otimes 2k}\otimes \mathbf{x}_{i}^{\otimes p},\frac{(-1)^{k}}{(2k)!}\mathbf{I}_{d}^{\otimes 2k} \otimes\mathbf{G}\right\rangle.\]

Plug it into the formula of \(B(f_{\star})\) (10) and \(B_{\mathrm{MLP}}(f_{\star})\) (61), we get

\[B(f_{2,\star})=4^{p}\mathrm{Poly}(p)\sum_{k=0}^{\infty}(2k)^{4.5}4^{2k}\frac{d ^{2k}}{((2k)!)^{2}}=4^{p}\mathrm{Poly}(pd)\Theta\Big{(}\sum_{k=0}^{\infty}\frac {(4d)^{2k}}{((2k)!)^{2}}\Big{)}.\]

Note that for any \(z>0\),

\[\sum_{k=0}^{\infty}\frac{(z)^{2k}}{((2k)!)^{2}}\leq\Big{[}\sum_{k=0}^{\infty} \frac{(\sqrt{z})^{2k}}{(2k)!}\Big{]}^{2}=\Theta\Big{(}e^{2\sqrt{z}}\Big{)}.\] (62)

Plug \(z=4d\) into (62) for \(B(f_{2,\star})\) gives the excess risk for RFA model. As for \(B_{\mathrm{MLP}}(f_{2,\star})\), we have

\[B_{\mathrm{MLP}}(f_{2,\star})=\mathrm{Poly}(p)\sum_{k=0}^{\infty}(N+2)^{p}(4k) ^{4.5}(N+2)^{4k}\frac{d^{2k}}{((2k)!)^{2}}=4^{p}\mathrm{Poly}(pdN)\Theta\Big{(} \sum_{k=0}^{\infty}\frac{((N+2)\sqrt{d})^{4k}}{((2k)!)^{2}}\Big{)}.\]

Using similar argument, we get that \(B_{\mathrm{MLP}}(f_{2,\star})\) is bounded by \(\Theta(\mathrm{Poly}(pdN)4^{p}\exp(2(N+2)\sqrt{d}))\). Using Theorem 2 and Proposition B.2, we can get the excess risk bound. This proves Proposition B.5. 

### Random Feature Attention with Exponential Activation

We give a brief discussion on our results with activation function replaced by the exponential function \(\sigma^{\prime}(t)=\exp(t)\). The random feature attention model with exponential activation (we call it \(\mathtt{ERFA}\)) is given as follows.

\[f_{M}^{\mathbf{W}}(\mathbf{x}_{0:N};\mathbf{V})=\sum_{m=1}^{M}\frac{1}{N}\sum_ {i=1}^{N}\sigma^{\prime}\big{(}\left\langle\mathbf{W}_{m},\widetilde{\mathbf{ x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\right\rangle\big{)}\left\langle\mathbf{v}_{m}, \widetilde{\mathbf{x}}_{i}\right\rangle.\] (63)

We consider deriving the explicit form of the Kernel \(K_{\mathtt{ERFA}}(\mathbf{x}_{0:N},\mathbf{x}_{0:N}^{\prime})\) associated with \(\mathtt{ERFA}\) model

\[K_{\mathtt{ERFA}}(\mathbf{x}_{0:N},\mathbf{x}_{0:N}^{\prime})=\frac{1}{N^{2}} \mathbb{E}_{\mathbf{W}}\Big{[}\sum_{i,j=1}^{N}\exp((\mathbf{W},\widetilde{ \mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}+\widetilde{\mathbf{x}}_{0}^{ \prime}(\widetilde{\mathbf{x}}_{j}^{\prime})^{\top}))(\widetilde{\mathbf{x}}_{ i},\widetilde{\mathbf{x}}_{j}^{\prime})\Big{]}\] (64)

For any \(i,j\in[N]\), let \(u_{i,j}=\big{\langle}\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}, \widetilde{\mathbf{x}}_{0}^{\prime}(\widetilde{\mathbf{x}}_{j}^{\prime})^{\top} \big{\rangle}\) /4. For a single component in (64), we have

\[\mathbb{E}_{\mathbf{W}\sim\mathsf{N}(0,1/4)}\Big{[}\exp(\langle \mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}+ \widetilde{\mathbf{x}}_{0}^{\prime}(\widetilde{\mathbf{x}}_{j}^{\prime})^{\top} ))\Big{]}\big{\langle}\widetilde{\mathbf{x}}_{i},\widetilde{\mathbf{x}}_{j}^{ \prime}\big{\rangle}\] \[= \exp\bigg{(}\frac{1}{8}\left\|\widetilde{\mathbf{x}}_{0}\widetilde{ \mathbf{x}}_{i}^{\top}+\widetilde{\mathbf{x}}_{0}^{\prime}(\widetilde{\mathbf{x}}_{ j}^{\prime})^{\top}\big{\|}_{\mathcal{F}}^{2}\right\rangle(\widetilde{\mathbf{x}}_{i}, \widetilde{\mathbf{x}}_{j}^{\prime})\] \[= \exp{(1+u_{i,j})}\langle\widetilde{\mathbf{x}}_{i},\widetilde{ \mathbf{x}}_{j}^{\prime}\rangle\] \[= e\Bigg{(}\sum_{\ell=0}^{\infty}\frac{1}{\ell!}u_{i,j}^{\ell} \bigg{)}\langle\widetilde{\mathbf{x}}_{i},\widetilde{\mathbf{x}}_{j}^{\prime}\rangle\]\[= \sum_{\ell=0}^{\infty}d_{\ell}\left\langle\widetilde{\mathbf{x}}_{0} \widetilde{\mathbf{x}}_{i}^{\top},\widetilde{\mathbf{x}}_{0}^{\prime}( \widetilde{\mathbf{x}}_{j}^{\prime})^{\top}\right\rangle^{\ell}4^{-\ell} \langle\widetilde{\mathbf{x}}_{i},\widetilde{\mathbf{x}}_{j}^{\prime}\rangle\] \[= \sum_{\ell=0}^{\infty}d_{\ell}\Big{\langle}2^{-\ell}(\widetilde{ \mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top})^{\otimes\ell}\otimes \widetilde{\mathbf{x}}_{i},2^{-\ell}(\widetilde{\mathbf{x}}_{0}^{\prime}( \widetilde{\mathbf{x}}_{j}^{\prime})^{\top})^{\otimes\ell}\otimes\widetilde{ \mathbf{x}}_{j}^{\prime}\Big{\rangle}.\]

Here \(d_{\ell}=e/\ell!\) for any \(\ell\geq 0\). Therefore, the kernel can be expressed as:

\[K_{\mathtt{EBFA}}(\mathbf{x}_{0:N},\mathbf{x}_{0:N}^{\prime})=\sum_{\ell=0}^ {\infty}\Big{\langle}\frac{\sqrt{d_{\ell}}}{N}\sum_{i=1}^{N}2^{-\ell}( \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top})^{\otimes\ell} \otimes\widetilde{\mathbf{x}}_{i},\frac{\sqrt{d_{\ell}}}{N}\sum_{j=1}^{N}2^{- \ell}(\widetilde{\mathbf{x}}_{0}^{\prime}(\widetilde{\mathbf{x}}_{j}^{\prime })^{\top})^{\otimes\ell}\otimes\widetilde{\mathbf{x}}_{j}^{\prime}\Big{\rangle}.\]

Thus, for any target function \(f_{\star}\) that in the form of (6), we can redefine its complexity measure as

\[B(f_{\star})=\sum_{k=0}^{\infty}C_{k}\sum_{\max\{r,s\}=k}\left\|\mathbf{f}_{rs }\right\|_{\mathtt{Fr}}^{2},\qquad C_{k}=k!4^{k}k^{2}\lor 1\] (65)

and obtain the corresponding results of Theorem 1 and Theorem 2 by repeating the proof of these two theorems.

## Appendix C Proofs for Section 4

We consider the empirical risk minimizer over the \(\mathtt{BRFA}\) model (15),

\[\widehat{\mathbf{V}}=\arg\min_{\mathbf{V}\in\mathcal{V}_{M}}\widehat{L}_{D}( f_{M}^{\mathbf{W},\mathbf{W}_{0}}(\cdot;\mathbf{V})),\qquad\widehat{L}_{D}(f)= \tfrac{1}{n}\sum_{j=1}^{n}\ell(f(\mathbf{x}_{0:N}^{(j)}),y_{j}),\] (66)

where the constrained class \(\mathcal{V}_{M}\) gives (12), copied here for reader's convenience,

\[\mathcal{V}_{M}=\Big{\{}\mathbf{V}=\{\mathbf{v}_{m}\}_{m=1}^{M}:\ \sum_{m=1}^{M}\left\|\mathbf{v}_{m}\right\|_{2}\leq K_{1},\sum_{m=1}^{M} \left\|\mathbf{v}_{m}\right\|_{2}^{2}\leq K_{2}/M\Big{\}}\,.\] (67)

Denote \(\mathcal{G}\) to be the set of all functions in the function class (16), i.e.,

\[\mathcal{G}=\Big{\{}g_{\star}=\frac{1}{N}\sum_{i=1}^{N}F(\langle\mathbf{x}_{0 },\mathbf{x}_{i}\rangle)G(\mathbf{x}_{0},\mathbf{x}_{i}):F(t)=\sum_{k=0}^{ \infty}a_{k}t^{k},G=\langle\widetilde{\mathbf{x}}_{i}^{\otimes 3}\otimes \widetilde{\mathbf{x}}_{0}^{\otimes 2},\mathbf{A}_{\star}\rangle\,\Big{\}}.\] (68)

We restate Theorem 3 in Theorem C.1 with detailed assumptions.

**Theorem C.1** (Restatement of Theorem 3).: _Assume \(M>\delta^{-1}\), \(n>\log(dM)\) and let \(L\in\mathbb{Z}_{\geq 1}\). Let \(g_{\star}=\arg\min_{g\in\mathcal{G}}L_{D}(g)\) be the population risk minimizer within the target function class \(\mathcal{G}\) (68). Take \(K_{1}=C\sqrt{B(g_{\star},L)}\) and \(K_{2}=CB(g_{\star},L)\delta^{-1}\) in (67), with \(C\) being a constant. Let \(\widehat{f}_{M}^{\mathbf{W},\mathbf{W}_{0}}=f_{M}^{\mathbf{W},\mathbf{W}_{0}} (\cdot;\widehat{\mathbf{V}})\) be empirical risk minimizer given by (66). Then for any joint distribution \(\mathsf{P}\), with probability at least \(1-\delta\) over \(\{\mathbf{W}_{m}\}_{m\in[M]}\) sampled according to (4) and \(\{(\mathbf{x}_{0:N}^{(j)},y_{j})\}_{j\in[n]}\sim_{iid}\mathsf{P}\), the excess risk is bounded by_

\[L_{D}(\widehat{f}_{M}^{\mathbf{W},\mathbf{W}_{0}})-L_{D}(g_{ \star})\] \[\lesssim \inf_{L}\Bigg{\{}\sqrt{B(g_{\star},L)}\Big{[}\sqrt{\frac{\log( dM)\log(nNM)+\log(\delta^{-1})}{n}}+\sqrt{\frac{(d^{2}+\log M)\delta^{-1}}{M}} \Big{]}+\varepsilon_{L}\left\|g_{\star}\right\|_{\infty}\Bigg{\}},\]

_where \(\varepsilon_{L}=1/[2^{L+1}(L+1)!]\) and_

\[B(g_{\star},L)=\left\|\mathbf{A}_{\star}\right\|_{\mathtt{Fr}}^{2}\cdot(\sum_{k =0}^{\infty}|a_{k}|\cdot C_{k})^{2},\quad\text{with }C_{k}=(2L+k)^{(k+3)/2}8^{L+k/2}.\] (69)

### Auxiliary results for the proof of Theorem 3

In this section, we give some auxiliary results used in the proof of Theorem 3. The proof will be given in Section C.2. We define the biased transformer with infinite width (informally corresponding to Eq. (15) with \(M\to\infty\)), given by

\[f_{\mathbf{v}}^{\mathbf{W}_{0}}(\mathbf{x}_{0:N})=\frac{1}{N}\sum_{i=1}^{N} \mathbb{E}_{\mathbf{W}}\big{[}\sigma\big{(}\langle\mathbf{W}+\mathbf{W}_{0}, \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\rangle\big{)} \,\langle\mathbf{v}(\mathbf{W}),\widetilde{\mathbf{x}}_{i}\rangle\big{]}\] (70)\[=\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{\mathbf{W}}\big{[}\sigma\big{(} \big{\langle}\mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{ \top}\big{\rangle}+h_{i}\big{)}\,\langle\mathbf{v}(\mathbf{W}),\widetilde{ \mathbf{x}}_{i}\rangle\big{]},\]

where \(h_{i}:=\langle\mathbf{W}_{0},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_ {i}^{\top}\rangle\) and \(\mathbf{W}\) is sampled according to (4). Here we set \(\mathbf{W}_{0}=[\mathbf{I}_{d\times d},\mathbf{0}_{d\times 1};\mathbf{0}_{1 \times d},0]\). Then \(h_{i}=\langle\mathbf{x}_{0},\mathbf{x}_{i}\rangle\).

We consider a class of target functions \(\widetilde{g}_{\star}:\mathcal{X}\to\mathbb{R}\) that takes form

\[\widetilde{g}_{\star}(\mathbf{x}_{0:N})=\frac{1}{N}\sum_{i=1}^{N}\sum_{\ell=0 }^{\infty}\left\langle u_{\ell}(\widetilde{\mathbf{x}}_{0},\widetilde{ \mathbf{x}}_{i}),\mathbf{D}_{\ell}\right\rangle,\] (71)

for some coefficients \(\{\mathbf{D}_{\ell}\in\mathbb{R}^{(d+1)^{2\ell+1}}\}_{\ell\geq 0}\) and

\[u_{\ell}(\widetilde{\mathbf{x}}_{0},\widetilde{\mathbf{x}}_{i})=\begin{cases} \begin{array}{ll}[h_{i}\Phi(h_{i})+\phi(h_{i})]\widetilde{\mathbf{x}}_{i}&( \ell=0),\\ \Phi(h_{i})\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\otimes \widetilde{\mathbf{x}}_{i}&(\ell=1),\\ \phi(h_{i})\mathrm{He}_{\ell-2}(h_{i})(\widetilde{\mathbf{x}}_{0}\widetilde{ \mathbf{x}}_{i}^{\top})^{\otimes\ell}\otimes\widetilde{\mathbf{x}}_{i}&(\ell \geq 2),\end{array}\end{cases}\]

where \(\phi\) and \(\Phi\) are the PDF and CDF of the standard Gaussian random variable, respectively. Lemma C.1 below provides a counterpart of Lemma B.2 for the BRFA model and the target function (71).

**Lemma C.1**.: _Any function \(\widetilde{g}_{\star}\) of form (71) can be expressed exactly as an infinite-head random feature attention model (70)_

\[\widetilde{g}_{\star}(\mathbf{x}_{0:N})=f_{\mathbf{v}}^{\mathbf{W}_{0}}( \mathbf{x}_{0:N})=\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{\mathbf{W}}\big{[} \sigma\big{(}\big{\langle}\mathbf{W}+\mathbf{W}_{0},\widetilde{\mathbf{x}}_{0 }\widetilde{\mathbf{x}}_{i}^{\top}\big{\rangle}\big{)}\,\langle\mathbf{v}( \mathbf{W}),\widetilde{\mathbf{x}}_{i}\rangle\big{]},\] (72)

_where the coefficients \(\mathbf{v}(\cdot)\) satisfy_

\[\mathbb{E}_{\mathbf{W}}\Big{[}\big{\|}\mathbf{v}(\mathbf{W})\|_{2}^{2}\Big{]} \leq\sum_{\ell\geq 0}4^{\ell}\ell\ell!\,\big{\|}\mathbf{D}_{\ell}\big{\|}_{ \mathsf{Fr}}^{2}\,.\]

Given Lemma C.1, we can get a counterpart of Theorem 1 for the BRFA model as Proposition C.1 below.

**Proposition C.1** (Counterpart of Theorem 1 for BRFA).: _Suppose function \(\widetilde{g}_{\star}:\mathcal{X}\to\mathbb{R}\) takes form (71). Then for any input distribution \(P\) on \(\mathcal{X}\), with probability at least \(1-\delta\) (over \(\{\mathbf{W}_{m}\}_{m\in[M]}\) sampled from (4)), there exists an \(M\)-head BRFA model (15) with coefficients \(\mathbf{V}=\{\mathbf{v}_{m}\}_{m\in[M]}\subseteq\mathbb{R}^{d+1}\) that approximates \(\widetilde{g}_{\star}\) in \(L^{2}(P)\) up to error_

\[\mathbb{E}_{\mathbf{x}_{0:N}\sim P}\Big{[}\big{(}\widetilde{g}_{\star}( \mathbf{x}_{0:N})-f_{M}^{\mathbf{W}}(\mathbf{x}_{0:N};\mathbf{V})\big{)}^{2} \Big{]}\leq\mathcal{O}\bigg{(}\frac{(d^{2}+\log M)B(\widetilde{g}_{\star}) \delta^{-1}}{M}\bigg{)}.\] (73)

_In addition, the norms of the weight of this random-feature attention model are bounded as_

\[\sum_{m=1}^{M}\big{\|}\mathbf{v}_{m}\big{\|}_{2}\leq\mathcal{O}\bigg{(}\sqrt{B( \widetilde{g}_{\star})}+\sqrt{\frac{B(\widetilde{g}_{\star})\delta^{-1}}{M}} \bigg{)},\qquad\sum_{m=1}^{M}\big{\|}\mathbf{v}_{m}\big{\|}_{2}^{2}\leq\mathcal{ O}\bigg{(}\frac{B(\widetilde{g}_{\star})\delta^{-1}}{M}\bigg{)}.\] (74)

_Here \(B(\widetilde{g}_{\star})\) is defined alternatively as_

\[B(\widetilde{g}_{\star})=\sum_{k=0}^{\infty}C_{k}\|\mathbf{D}_{k}\|_{\mathsf{ Fr}}^{2},\quad\text{with}\quad C_{k}=k!4^{k}\lor 1.\] (75)

Furthermore, we could approximate the target function in the form (16) to any precision by a function that takes form (72), which is discussed in Lemma C.2.

**Lemma C.2**.: _For any target function \(g_{\star}\) in the form of (16), and for any precision \(\varepsilon_{\ell}:=\frac{1}{2^{\ell+1}(\ell+1)!}\), there exists a function \(f_{\mathbf{v}}^{\mathbf{W}_{0}}\) in the form of (72) such that_

\[\big{\|}g_{\star}-f_{\mathbf{v}}^{\mathbf{W}_{0}}\big{\|}_{\infty}\leq\big{\|}g _{\star}\big{\|}_{\infty}\,\varepsilon_{\ell},\]

_and_

\[\mathbb{E}_{\mathbf{W}}\Big{[}\big{\|}\mathbf{v}(\mathbf{W})\|_{2}^{2}\Big{]} \leq\left(\sum_{p=0}^{\infty}a_{p}8^{\ell+\frac{p+3}{2}}(2\ell+p)^{\frac{p+3}{2} }\right)^{2}\|\mathbf{A}_{\star}\|_{\mathsf{Fr}}^{2}\,,\]

_where the tensor \(\mathbf{A}_{\star}\in\mathbb{R}^{d^{5}}\) parameterizes \(g_{\star}\)._

#### c.1.1 Proof of auxiliary lemmas

_Proof of Lemma c.1._

The function (72) can be interpreted as a linear function of the feature map

\[\Psi(\mathbf{x}_{0:N})=\frac{1}{N}\sum_{i=1}^{N}\sigma\big{(}\langle\mathbf{W}, \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\rangle+h_{i}\big{)} \widetilde{\mathbf{x}}_{i}.\]

So the kernel w.r.t. the feature map takes the form that

\[K(\mathbf{x}_{0:N},\mathbf{x}_{0:N}^{\prime})\] \[=\mathbb{E}_{\mathbf{W}}[\langle\Psi(\mathbf{x}_{0:N}),\Psi( \mathbf{x}_{0:N}^{\prime})\rangle]\] \[=\frac{1}{N^{2}}\sum_{1\leq i,j\leq N}\mathbb{E}_{\mathbf{W}} \big{[}\sigma\big{(}\langle\mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{ \mathbf{x}}_{i}^{\top}\rangle+h_{i}\big{)}\sigma\big{(}\langle\mathbf{W}, \widetilde{\mathbf{x}}_{0}^{\prime}\widetilde{\mathbf{x}}_{j}^{\prime\top} \rangle+h_{j}^{\prime}\big{)}\big{]}\langle\widetilde{\mathbf{x}}_{i}, \widetilde{\mathbf{x}}_{j}^{\prime}\rangle,\]

where \(h_{i}=\langle\mathbf{x}_{0},\mathbf{x}_{i}\rangle\) and \(h_{j}^{\prime}=\langle\mathbf{x}_{0}^{\prime},\mathbf{x}_{j}^{\prime}\rangle\). Similar to the proof of Lemma B.2, we also consider a single component of the equation above first, which has the form of

\[\mathbb{E}_{\mathbf{W}}\big{[}\sigma\big{(}\langle\mathbf{W}, \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\rangle+h_{i}\big{)} \sigma\big{(}\langle\mathbf{W},\widetilde{\mathbf{x}}_{0}^{\prime}\widetilde{ \mathbf{x}}_{j}^{\prime\top}\rangle+h_{j}^{\prime}\big{)}\big{]}\langle \widetilde{\mathbf{x}}_{i},\widetilde{\mathbf{x}}_{j}^{\prime}\rangle.\] (76)

We expand \(\sigma\big{(}\langle\mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x }}_{i}^{\top}\rangle+h_{i}\big{)}\) by Hermite polynomials in the space \(L^{2}(\mathbb{R},\phi)\) using Lemma A.3,

\[\mathbb{E}_{\mathbf{W}}\big{[}\sigma\big{(}\langle\mathbf{W}, \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\rangle+h_{i} \big{)}\mathrm{He}_{\ell}\big{(}\langle\mathbf{W},\widetilde{\mathbf{x}}_{0} \widetilde{\mathbf{x}}_{i}^{\top}\rangle\big{)}\big{]}\] \[=\mathbb{E}_{z\sim\mathbf{N}(0,1)}[\sigma(z+h_{i})\mathrm{He}_{ \ell}(z)]\] \[=\left\{\begin{array}{cl}h_{i}\Phi(h_{i})+\phi(h_{i})&(\ell=0), \\ \Phi(h_{i})&(\ell=1),\\ (-1)^{\ell}\phi(h_{i})\mathrm{He}_{\ell-2}(h_{i})&(\ell\geq 2).\end{array}\right.\]

Therefore, we have

\[\sigma\big{(}\langle\mathbf{W},\widetilde{\mathbf{x}}_{0} \widetilde{\mathbf{x}}_{i}^{\top}\rangle+h_{i}\big{)} =h_{i}\Phi(h_{i})+\phi(h_{i})+\Phi(h_{i})\mathrm{He}_{1}\big{(} \big{\langle}\mathbf{W},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^ {\top}\rangle\big{)}\] \[\quad+\sum_{\ell=2}^{\infty}\frac{(-1)^{\ell}}{\ell!}\phi(h_{i}) \mathrm{He}_{\ell-2}(h_{i})\mathrm{He}_{\ell}\big{(}\langle\mathbf{W}, \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\rangle\big{)}.\]

Using Lemma A.1, we obtain the expansion:

\[\mathbb{E}_{\mathbf{W}}\big{[}\sigma\big{(}\langle\mathbf{W}, \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\rangle+h_{i}\big{)} \sigma\big{(}\langle\mathbf{W},\widetilde{\mathbf{x}}_{0}^{\prime}\widetilde{ \mathbf{x}}_{j}^{\prime\top}\rangle+h_{j}^{\prime}\big{)}\big{]}\langle \widetilde{\mathbf{x}}_{i},\widetilde{\mathbf{x}}_{j}^{\prime}\rangle\] \[=\]

where

\[\varphi_{\ell}(\widetilde{\mathbf{x}}_{0},\widetilde{\mathbf{x}}_{i})=\sqrt{ \frac{1}{\ell!}}2^{-\ell}u_{\ell}(\widetilde{\mathbf{x}}_{0},\widetilde{ \mathbf{x}}_{i})=\left\{\begin{array}{cl}[h_{i}\Phi(h_{i})+\phi(h_{i})] \widetilde{\mathbf{x}}_{i}&(\ell=0),\\ 2^{-1}\Phi(h_{i})\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top} \otimes\widetilde{\mathbf{x}}_{i}&(\ell=1),\\ \sqrt{\frac{1}{\ell!}}2^{-\ell}\phi(h_{i})\mathrm{He}_{\ell-2}(h_{i})( \widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top})^{\otimes\ell} \otimes\widetilde{\mathbf{x}}_{i}&(\ell\geq 2).\end{array}\right.\]

Then by taking summation with respect to \(i\) and \(j\), we derive the expansion of the kernel:

\[K(\mathbf{x}_{0:N},\mathbf{x}_{0:N}^{\prime})=\sum_{\ell=0}^{\infty}\Big{\langle} \frac{1}{N}\sum_{i=1}^{N}\varphi_{\ell}(\mathbf{x}_{0},\mathbf{x}_{i}),\frac{1} {N}\sum_{j=1}^{N}\varphi_{\ell}(\mathbf{x}_{0}^{\prime},\mathbf{x}_{j}^{\prime}) \Big{\rangle}.\]Then for the target function that takes the form in (71), we have the RKHS norm of \(\widetilde{g}_{\star}\) bounded by:

\[\|\widetilde{g}_{\star}\|_{\mathcal{H}_{K}}^{2}\leq\sum_{\ell=0}^{\infty}\left(2 ^{\ell}\sqrt{\ell!}\right)^{2}\|\mathbf{D}_{\ell}\|_{\mathsf{Fr}}^{2}=\sum_{ \ell=0}^{\infty}4^{\ell}\ell!\,\|\mathbf{D}_{\ell}\|_{\mathsf{Fr}}^{2}\]

by the feature equivalence property (23). Thus, there exists coefficients \(\mathbf{v}(\mathbf{W})\) such that

\[f_{\mathbf{v}}^{\mathbf{W}_{0}}(\mathbf{x}_{0:N})=\frac{1}{N}\sum_{i=1}^{N} \mathbb{E}_{\mathbf{W}}\big{[}\sigma\big{(}\big{\langle}\mathbf{W}+\mathbf{W}_ {0},\widetilde{\mathbf{x}}_{0}\widetilde{\mathbf{x}}_{i}^{\top}\big{\rangle} \big{)}\,\langle\mathbf{v}(\mathbf{W}),\widetilde{\mathbf{x}}_{i}\rangle\big{]}\]

and

\[\mathbb{E}_{\mathbf{W}}\Big{[}\big{\|}\mathbf{v}(\mathbf{W})\|_{2}^{2}\Big{]} \leq\sum_{\ell\geq 0}4^{\ell}\ell!\,\|\mathbf{D}_{\ell}\|_{\mathsf{Fr}}^{2}\,,\]

which proves Lemma C.1. 

Proof of Proposition c.1.: Note that Lemma B.1 is also applicable to the model BRFA. Combining it with Lemma C.1 proves Proposition C.1. 

Proof of Lemma c.2.: Firstly, we consider approximating \(x^{p}\) using the function class \(\left\{\phi(x)\mathrm{He}_{\ell}(x)\right\}_{n\geq 0}\), which is equivalent to approximating \(x^{p}\phi^{-1}(x)\) using Hermite polynomials. We take \(p=0\) first and compute the Hermite expansion of the \(2\ell\)-th-order term in the Taylor expansion of \(\phi^{-1}(x)=e^{x^{2}/2}\), i.e. \(\psi_{\ell}(x)=\sum_{n=0}^{\ell}\frac{x^{2n}}{2^{n}n!}\). Using Lemma A.2, we have that:

\[\psi_{\ell}(x) =\sum_{n=0}^{\ell}\frac{x^{2n}}{2^{n}n!}=\sum_{n=0}^{\ell}\frac{ (2n)!}{2^{n}n!}\sum_{m=0}^{n}\frac{\mathrm{He}_{2n-2m}(x)}{2^{m}m!(2n-2m)!}\] \[=\sum_{k=0}^{\ell}\mathrm{He}_{2k}(x)\sum_{m=0}^{\ell-k}\frac{(2 m+2k)!}{2^{m+k}(m+k)!}\cdot\frac{1}{2^{m}m!(2k)!}\] \[=:\sum_{k=0}^{\ell}c_{\ell,k}\mathrm{He}_{2k}(x),\]

where \(c_{\ell,k}=\frac{2^{k}}{(2k)!}\sum_{m=0}^{\ell-k}\frac{(2m+2k)!}{2^{2m+2k}(m+ k)!m!}\). Similarly, for any \(p\geq 0\), define \(p_{0}=\lfloor\frac{p}{2}\rfloor\) with \(r=p-2p_{0}\). We have that

\[\psi_{\ell}(x)x^{p} =\sum_{n=0}^{\ell}\frac{x^{2n+2p_{0}+r}}{2^{n}n!}=\sum_{n=0}^{\ell }\frac{(2n+2p_{0}+r)!}{2^{n}n!}\sum_{m=0}^{n+p_{0}}\frac{\mathrm{He}_{2n+2p_{0 }+r-2m}(x)}{2^{m}m!(2n+2p_{0}+r-2m)!}\] \[=\sum_{k=0}^{\ell+p_{0}}\mathrm{He}_{2k+r}(x)\sum_{m=\max(p_{0}- k,0)}^{\ell+p_{0}-k}\frac{(2m+2k+r)!}{2^{m+k-p_{0}}(m+k-p_{0})!}\cdot\frac{1}{2^{m}m! (2k+r)!}\] \[=:\sum_{k=0}^{\ell+p_{0}}c_{\ell,k,p_{0}}\mathrm{He}_{2k+r}(x),\]

where \(c_{\ell,k,p_{0}}=\sum_{m=\max(p_{0}-k,0)}^{\ell+p_{0}-k}\frac{(2m+2k+r)!}{2^{m +k-p_{0}}(m+k-p_{0})!}\cdot\frac{1}{2^{m}m!(2k+r)!}\). Then we can bound \(c_{\ell,k,p_{0}}\) as follows:

\[c_{\ell,k,p_{0}} =\sum_{m=\max(p_{0}-k,0)}^{\ell+p_{0}-k}\frac{(2m+2k+r)!}{2^{m+k- p_{0}}(m+k-p_{0})!}\cdot\frac{1}{2^{m}m!(2k+r)!}\] \[\leq\sum_{m=0}^{\ell+p_{0}-k}2^{p_{0}}\frac{(m+k)!}{(m+k-p_{0})!} \frac{(2m+2k+r)!}{2^{m+k}(m+k)!}\cdot\frac{1}{2^{m}m!(2k+r)!}\]\[\leq 2^{p_{0}}(\ell+p_{0})^{p_{0}}(2l+p)^{r}\sum_{m=0}^{\ell+p_{0}-k} \frac{(2m+2k)!}{2^{m+k}(m+k)!}\cdot\frac{1}{2^{m}m!(2k)!}\leq(2\ell+p)^{p_{0}+r}c _{\ell+p_{0},k}.\]

Next, we give an upper bound of \(c_{\ell,k}\):

\[c_{\ell,k} =\frac{2^{k}}{(2k)!}\sum_{m=0}^{\ell-k}\frac{(2m+2k)!}{2^{2m+2k}(m +k)!m!}\] \[=\frac{2^{k}}{(2k)!}\sum_{m=0}^{\ell-k}\frac{(2m+2k)!}{2^{2m+2k}( m+k)!(m+k)!}\cdot\frac{(m+k)!}{m!}\] \[\leq\frac{2^{k}}{(2k)!}\sum_{m=0}^{\ell-k}\sqrt{\frac{1}{m+k}} \cdot\frac{(m+k)!}{m!}\] \[=\frac{2^{k}}{(2k)!}\sum_{m=0}^{\ell-k}\sqrt{m+k}\cdot\frac{(m+k -1)!}{m!}\] \[\leq\frac{2^{k}}{(2k)!}\sqrt{\ell}\sum_{m=0}^{\ell-k}\frac{(m+k- 1)!}{m!}\] \[=\frac{2^{k}}{(2k)!}\sqrt{\ell}\frac{\ell!}{k(\ell-k)!}.\]

Here we use the inequality that

\[\sqrt{\frac{2}{\pi(2n+1)}}\leq\frac{(2n)!}{2^{2n}n!n!}\leq\sqrt{\frac{1}{2n}},\forall n\geq 1.\]

and we will use it again in the following proof. Since the function

\[\frac{1}{N}\phi(\left\langle\mathbf{x}_{0},\mathbf{x}_{i}\right\rangle)\psi_{ \ell}(\left\langle\mathbf{x}_{0},\mathbf{x}_{i}\right\rangle)\sum_{i=1}^{N} \left\langle\mathbf{x}_{0},\mathbf{x}_{i}\right\rangle^{p}\left\langle \widetilde{\mathbf{x}}_{i}^{\otimes 3}\otimes\widetilde{\mathbf{x}}_{0}^{\otimes 2}, \mathbf{A}_{\star}\right\rangle\]

can be written as

\[\frac{1}{N}\sum_{i=1}^{N}\sum_{k=0}^{\ell+p_{0}}\left\langle u_{2k+r+2}( \widetilde{\mathbf{x}}_{0},\widetilde{\mathbf{x}}_{i}),c_{\ell,k,p_{0}} \mathbf{A}_{\star}\otimes\mathbf{e}^{\otimes(4k+2r)}\right\rangle,\]

where \(\mathbf{e}=\mathrm{diag}(0,0,...,1)\). By Lemma C.1, there exists \(\mathbf{v}_{p,\ell}(\mathbf{W})\) s.t.

\[f_{\mathbf{v}_{p,\ell}}^{\mathbf{W}_{0}}(\mathbf{x}_{0:N})=\phi(\left\langle \mathbf{x}_{0},\mathbf{x}_{i}\right\rangle)\psi_{\ell}(\left\langle\mathbf{x} _{0},\mathbf{x}_{i}\right\rangle)\frac{1}{N}\sum_{i=1}^{N}\left\langle\mathbf{x }_{0},\mathbf{x}_{i}\right\rangle^{p}\left\langle\widetilde{\mathbf{x}}_{i}^{ \otimes 3}\otimes\widetilde{\mathbf{x}}_{0}^{\otimes 2},\mathbf{A}_{\star}\right\rangle\]

with

\[\mathbb{E}_{\mathbf{W}}\Big{[}\|\mathbf{v}_{p,\ell}(\mathbf{W})\|_{2}^{2} \Big{]}\leq\|\mathbf{A}_{\star}\|_{\mathrm{Fr}}^{2}\sum_{k=0}^{\ell+p_{0}}4^{2k +r+2}(2k+r+2)!c_{\ell,k,p_{0}}^{2}.\]

Notice that

\[\sum_{k=0}^{\ell+p_{0}}4^{2k+r+2}(2k+r+2)!c_{\ell,k,p_{0}}^{2}\] \[\leq\sum_{k=0}^{\ell+p_{0}}4^{2k+r+2}(2k+r+2)!\big{[}(2\ell+p)^{p _{0}+r}c_{\ell+p_{0},k}\big{]}^{2}\] \[\leq(2\ell+p)^{2p_{0}+2r}\sum_{k=0}^{\ell+p_{0}}4^{2k+r+2}(2k+r+2)!\bigg{[}\frac{2^{k}}{(2k)!}\sqrt{\ell+p_{0}}\frac{(\ell+p_{0})!}{k(\ell+p_{0} -k)!}\bigg{]}^{2}\]\[\leq 4(2\ell+p)^{2p_{0}+2r}(2\ell+2p_{0}+r+2)\sum_{k=0}^{\ell_{p}}4^{2k+2} (2k+2)!\bigg{[}\frac{2^{k}}{(2k)!}\sqrt{\ell_{p}}\frac{(\ell_{p})!}{k(\ell_{p})! }\bigg{]}^{2},\]

where \(\ell_{p}=\ell+p_{0}\), and by the inequality that

\[\sum_{k=0}^{\ell_{p}}4^{2k+2}(2k+2)!\bigg{[}\frac{2^{k}}{(2k)!} \sqrt{\ell_{p}}\frac{(\ell_{p})!}{k(\ell_{p})!}\bigg{]}^{2}\] \[= \sum_{k=0}^{\ell_{p}}2^{4k+4}\ell_{p}\frac{(2k+2)(2k+1)}{k^{2}} \frac{2^{2k}}{(2k)!}\bigg{[}\frac{\ell_{p}!}{(\ell_{p}-k)!}\bigg{]}^{2}\] \[\leq \sum_{k=0}^{\ell_{p}}2^{4k+4}\ell_{p}\frac{(2k+2)(2k+1)}{k^{2}} \sqrt{\frac{\pi}{2}(2k+1)}\bigg{[}\frac{\ell_{p}!}{k!(\ell_{p}-k)!}\bigg{]}^{2}\] \[\leq 8\times 2^{4\ell_{p}+4}\ell_{p}^{\frac{3}{2}}\sum_{k=0}^{ \ell}\bigg{[}\frac{\ell_{p}!}{k!(\ell_{p}-k)!}\bigg{]}^{2}\] \[= 4\sqrt{2}\times 2^{4\ell_{p}+4}\ell_{p}^{\frac{3}{2}} \bigg{(}\frac{2\ell_{p}}{\ell_{p}}\bigg{)}\] \[\leq 4\sqrt{2}\times 2^{4}\ell_{p}\cdot(64)^{\ell_{p}},\]

we obtain an upper bound:

\[\mathbb{E}_{\mathbf{W}}\Big{[}\|\mathbf{v}_{p,\ell}(\mathbf{W}) \|_{2}^{2}\Big{]} \leq (2\ell+p)^{2p_{0}+2r}\,\|\mathbf{A}_{\star}\|_{\mathsf{Fr}}^{2} \sum_{k=0}^{\ell+p_{0}}4^{2k+r+2}(2k+r+2)!c_{\ell+p_{0},k}^{2}\] \[\leq \|\mathbf{A}_{\star}\|_{\mathsf{Fr}}^{2}\times 4(2\ell+p)^{2p_{0}+2r }(2\ell+2p_{0}+r+2)\times 4\sqrt{2}\times 2^{4}\ell_{p}\cdot(64)^{\ell_{p}}\] \[\leq \|\mathbf{A}_{\star}\|_{\mathsf{Fr}}^{2}\,(2\ell+p)^{p+r+1}8^{2 \ell+2p_{0}+3}.\]

Finally, we consider the target function (16)

\[g_{\star}(\mathbf{x}_{0:N})=\frac{1}{N}\sum_{i=1}^{N}F(\left\langle\mathbf{x} _{0},\mathbf{x}_{i}\right\rangle)G(\mathbf{x}_{0},\mathbf{x}_{i}),\quad F(t)= \sum_{p=0}^{\infty}a_{p}t^{p},\quad G(\mathbf{x}_{0},\mathbf{x}_{i})=\left\langle \widetilde{\mathbf{x}}_{i}^{\otimes 3}\otimes\widetilde{\mathbf{x}}_{0}^{\otimes 2}, \mathbf{A}_{\star}\right\rangle.\]

By approximating \(\frac{1}{N}\sum_{i=1}^{N}a_{p}\left\langle\mathbf{x}_{0},\mathbf{x}_{i} \right\rangle^{p}G(\mathbf{x}_{0},\mathbf{x}_{i})\) separately and adding the approximation functions together, we obtain a \(\mathbf{v}_{\ell}\) s.t. \(f_{\mathbf{v}_{\ell}}^{\mathbf{W}_{0}}(\mathbf{x}_{0:N})=\phi(\left\langle \mathbf{x}_{0},\mathbf{x}_{i}\right\rangle)\psi_{\ell}(\left\langle\mathbf{x}_ {0},\mathbf{x}_{i}\right\rangle)g_{\star}(\mathbf{x}_{0:N})\) and

\[\mathbb{E}_{\mathbf{W}}\Big{[}\|\mathbf{v}_{p,\ell}(\mathbf{W}) \|_{2}^{2}\Big{]}\leq\left(\sum_{p=0}^{\infty}a_{p}8^{\ell+p/2+3/2}(2\ell+p)^{ \frac{p+3}{2}}\right)^{2}\|\mathbf{A}_{\star}\|_{\mathsf{Fr}}^{2}=8^{3}B(g_{ \star},L)\]

and \(\big{\|}g_{\star}-f_{\mathbf{v}_{\ell}}^{\mathbf{W}_{0}}\big{\|}_{\infty}=\left\| (\phi\psi_{\ell}-1)g_{\star}\right\|_{\infty}\leq\left\|\phi\psi_{\ell}-1\right\| _{\infty}\left\|g_{\star}\right\|_{\infty}\leq\frac{e}{2^{\ell+1}(\ell+1)!} \left\|g_{\star}\right\|_{\infty}=\left\|g_{\star}\right\|_{\infty}\varepsilon_{\ell}\). This finishes the proof of Lemma C.2. 

### Proof of Theorem c.1

Proof.: For any \(L>0\), using Lemma C.2, we can find a function \(f_{\mathbf{v}_{L}}^{\mathbf{W}_{0}}\) such that \(\big{\|}g_{\star}-f_{\mathbf{v}_{L}}^{\mathbf{W}_{0}}\big{\|}_{\infty}\leq \left\|g_{\star}\right\|_{\infty}\varepsilon_{L}\) with \(B(f_{\mathbf{v}_{L}}^{\mathbf{W}_{0}})\leq 8^{3}B(g_{\star},L)\). Follow the same manner as the proof of Theorem 2. We can get that

\[L_{D}(\widehat{f}_{M}^{\mathbf{W},\mathbf{W}_{0}})- L_{D}(f_{\mathbf{v}_{L}}^{\mathbf{W}_{0}})\] \[\lesssim\sqrt{B(g_{\star},L)}\Big{[}\sqrt{\frac{\log(dM)\log(nNM)+ \log(\delta^{-1})}{n}}+\sqrt{\frac{(d^{2}+\log M)\delta^{-1}}{M}}\Big{]}.\]

Therefore, we have that

\[L_{D}(\widehat{f}_{M}^{\mathbf{W},\mathbf{W}_{0}})-L_{D}(g_{\star})\] \[=L_{D}(\widehat{f}_{M}^{\mathbf{W},\mathbf{W}_{0}})-L_{D}(f_{ \mathbf{v}_{L}}^{\mathbf{W}_{0}})+L_{D}(f_{\mathbf{v}_{L}}^{\mathbf{W}_{0}})-L_ {D}(g_{\star})\] \[\lesssim\sqrt{B(g_{\star},L)}\Big{[}\sqrt{\frac{\log(dM)\log(nNM)+ \log(\delta^{-1})}{n}}+\sqrt{\frac{(d^{2}+\log M)\delta^{-1}}{M}}\Big{]}+ \varepsilon_{L}\left\|g_{\star}\right\|_{\infty}.\] (77)

Taking infimum over \(L\) proves Theorem 3.

### Proof of Examples in Section 4

**Proposition C.2** (Restatement of Example 4).: _Consider the target function_

\[g_{\star}=\frac{1}{N}\sum_{i=1}^{N}\langle\mathbf{x}_{i}^{\otimes 3}\otimes \mathbf{x}_{0}^{\otimes 2},\mathbf{A}\rangle.\]

_It has norm bound \(B(g_{\star},L)=\|\mathbf{A}\|_{\mathsf{Fr}}^{2}\,L^{3}8^{2L}\). So for any \(\eta>0\), if we take \(n\geq\exp(\exp(\Theta(1/\eta)))\), \(L=\Theta((1+\log\log n)^{-1}\log n)\), and \(M=\Theta(d^{2}n)\), the excess risk will scale as \(\widetilde{\mathcal{O}}(\sqrt{\|\mathbf{A}\|_{\mathsf{Fr}}^{2}/n^{1-\eta}})\)._

Proof of Proposition C.2.: The value of \(B(g_{\star},L)\) can be obtained by definition and by direct calculation. As for the second part of the proposition, take \(L=r\log n\), where \(r>0\) is a parameter that is to be chosen to minimize the excess risk. Eq. (17) becomes

\[\widetilde{\mathcal{O}}(\|\mathbf{A}\|_{\mathsf{Fr}}\,r^{3/2}n^{3r-1/2}+n^{-r \log(2/e)-r\log r-r\log\log n}),\] (78)

where \(\widetilde{\mathcal{O}}\) hides all the logarithm factors and constants of \(n,d\), and \(M\). To minimize the excess risk, we need to make the two terms in (78) have the same scale. So we set \(3r-1/2=-r\log(2/e)-r\log r-r\log\log n\). Denoting the solution as \(r_{\star}\), we get that

\[r_{\star}=\frac{1/2-r_{\star}\log r_{\star}}{\log(2e^{2})+\log\log n}.\]

So this gives \(r_{\star}<(1/2+1/e)(\log(2e^{2})+\log\log n)^{-1}\). Assume \(r_{\star}<1\) (otherwise \(3r_{\star}>1/2\) and the excess risk is meaningless), then \(r_{\star}>(2\log(2e^{2})+2\log\log n)^{-1}\). Therefore, we get that \(r_{\star}=C(1+\log\log n)^{-1}\), with \(C\) being a constant. As a result, when choosing \(L=r_{\star}\), \(\log n=\Theta((1+\log\log n)^{-1}\log n)\), the excess risk Eq. (78) scales as \(\widetilde{\mathcal{O}}(\|\mathbf{A}\|_{\mathsf{Fr}}\,n^{3r_{\star}-\frac{1}{ 2}})=\widetilde{\mathcal{O}}(\|\mathbf{A}\|_{\mathsf{Fr}}\,n^{C/(1+\log\log n) -\frac{1}{2}})\). As a result, let \(n>\exp(\exp(C/\eta-1))\) where \(C\) is a constant, we get an excess risk that scales as \(\widetilde{\mathcal{O}}(\sqrt{\|\mathbf{A}\|_{\mathsf{Fr}}^{2}/n^{1-\eta}})\). This finishes the proof of Proposition C.2. 

**Proposition C.3** (Restatement of Example 5).: _Consider the target function_

\[g_{\star}=\frac{1}{N}\sum_{i=1}^{N}\left\langle\mathbf{x}_{0},\mathbf{x}_{i} \right\rangle^{p}\left\langle\boldsymbol{\beta},\mathbf{x}_{i}\right\rangle, \;\;\;\boldsymbol{\beta}\in\mathbb{S}^{d-1}.\]

_It has \(B(g_{\star},L)=(2L+p)^{p+3}8^{2L+p}\). For any \(\eta>0\), choosing the same parameters \((n,L,M)\) as Example 4, the excess risk bound scales as \(\widetilde{\mathcal{O}}(\sqrt{(\log n+p)^{(p+3)}8^{p}/n^{1-\eta}})\)._

_Furthermore, to reach an accuracy of \(0.01\), the \(\mathsf{BRFA}\) model requires \(n_{\star}=\widetilde{\mathcal{O}}((8p+48)^{p+3})\), whereas the \(\mathsf{RFA}\) model requires \(n_{\star}=\widetilde{\mathcal{O}}((4d)^{p})\)._

Proof of Proposition C.3.: The value of \(B(g_{\star},L)\) can be obtained by direct calculation, and we use the same method as the proof of Proposition C.2 to get the \(\widetilde{\mathcal{O}}(\sqrt{1/n^{1-\eta}})\) excess risk. To reach an accuracy of \(0.01\), we can set \(L=3\), note that \(\varepsilon_{L}\left\|g_{\star}\right\|_{\infty}<0.006\). Therefore we just need to choose \(n_{\star}\) s.t. \(n>\widetilde{\mathcal{O}}((6+p)^{p+3}8^{6+p})\), and this gives \(n_{\star}=\widetilde{\mathcal{O}}((8p+48)^{p+3})\) for the \(\mathsf{BRFA}\) model. Direct calculation using Theorem 2 gives the value of \(n_{\star}\) for the model \(\mathsf{RFA}\). This finishes the proof of Proposition C.3. 

**Proposition C.4** (Restatement of Example 6).: _Consider the target function that has the form_

\[g_{\star}=\frac{1}{N}\sum_{i=1}^{N}\cos(\langle\mathbf{x}_{0},\mathbf{x}_{i} \rangle)\left\langle\mathbf{x}_{i}^{\otimes 3},\mathbf{G}\right\rangle.\]

_It has \(B(g_{\star},L)=\Theta((8e)^{2L})\). For any \(\eta>0\), choosing the same parameters as Example 4, the excess risk bound scales as \(\widetilde{\mathcal{O}}(\sqrt{1/n^{1-\eta}})\)._

_Furthermore, to reach an accuracy of \(0.01\). The \(\mathsf{BRFA}\) model requires \(n_{\star}=\widetilde{\mathcal{O}}(1)\), whereas the \(\mathsf{RFA}\) model requires \(n_{\star}=\widetilde{\mathcal{O}}(\operatorname{Poly}(d)\exp(\sqrt{d}))\)._Proof of Proposition c.4.: We use the expansion of the \(\cos\) function,

\[g_{\star}=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=0}^{\infty}\frac{(-1)^{k}}{(2k)!}\left< \mathbf{x}_{0},\mathbf{x}_{i}\right>^{2k}\left<\mathbf{x}_{i}^{\otimes 3}, \mathbf{G}\right>.\]

Then use the Eq. (17), we get that

\[B(g_{\star},L)=\sum_{k=0}^{\infty}\frac{(2L+2k)^{k+3/2}8^{L+k}}{(2k)!}\leq\sum_ {k}\frac{(2k+3)^{k+3/2}8^{k}}{(2k)!}(8e)^{L}=\Theta((8e)^{L}).\]

This gives the formula for \(B(g_{\star},L)\). To reach an accuracy of 0.01, we set \(L=3\). Then \(B(g_{\star},L)=\Theta(1)\). So BRFA needs \(n_{\star}=\widetilde{\mathcal{O}}(1)\). And Theorem 2 and Example 3 show that RFA model would require \(n_{\star}=\widetilde{\mathcal{O}}(\operatorname{Poly}(d)\exp(\sqrt{d}))\). This finishes the proof of Proposition C.4. 

## Appendix D Further experiments

In addition to Section 5, we perform further simulations to examine our theory upon the approximation power of RFA (5), BRFA (15), and RFMLP (14). Besides the target function (21), we consider two additional target functions of the form

\[f_{3,p}(\mathbf{x}_{0:N}) =\left<\boldsymbol{\beta},\mathbf{x}_{0}\right>^{p}, p\in\mathbb{N}, \quad\boldsymbol{\beta}\in\mathbb{S}^{d-1},\] (79) \[f_{4,\gamma}(\mathbf{x}_{0:N}) =\tfrac{1}{N}\sum_{i=1}^{N}\left<\mathbf{x}_{0},\mathbf{S} \mathbf{x}_{i}\right>^{3}\left<\boldsymbol{\beta},\mathbf{x}_{i}\right>, \quad\boldsymbol{\beta}\in\mathbb{S}^{d-1}, \quad\mathbf{S}=\boldsymbol{Z}+\gamma I_{d}.\] (80)

The target function (79) is a specific instance of Example 1, and the target function (80) is a specific instance of Example 3. In (80), we sample \(Z_{ij}\gamma_{\text{iid}}\mathsf{N}(0,1/d)\) for \((i,j)\in[d]^{2}\) and vary \(\gamma\) in the experiment. Other experimental settings are the same as in Section 5.

Figure 3 demonstrates the effect of sequence length \(N\) on the performance of three RF models when fitting the target function \(f_{3,2}\) (79), which solely depends on \(x_{0}\). We observe that a larger sequence length \(N\) leads to a larger separation of the test error between RFMLP and the other two random-feature attention models. This result aligns with our sample complexity analysis detailed in Example 1, where the sample complexity bound of RFMLP for learning average of functions of \(\mathbf{x}_{i}\) is found to be \(\mathcal{O}((N/4)^{p})\) times greater than that of RFA.

Figure 4 demonstrates the performance comparison between RFA and BRFA under different choices of the token dimension \(d\). As we can see from the left panel of Figure 4, in the case of \(d=4\), RFA outperforms BRFA for large sample size, which may result from that BRFA has slower convergence rate with respect to the sample size \(n\) as we state in Example 5. In the middle and the right panel (i.e., when \(d\) is larger), BRFA exceeds RFA, and the largest separation lies in the case of \(d=32\). This result is consistent with our analyses that BRFA saves a Poly(d) factor in the sample complexity for \(d\gg p\).

Figure 5 demonstrates that BRFA has no advantage in approximating the target function \(f_{4,\gamma}\) for \(\gamma=0\) (left panel; i.e., \((S_{ij}\gamma_{\text{iid}}\mathsf{N}(0,1/d)\), for \((i,j)\in[d]^{2}\)). However, as \(\gamma\) increases, BRFA outperforms RFA and their separation increases with a larger \(\gamma\) (middle and right panel). Notice that \(\lim_{\gamma\to\infty}\left<\mathbf{x}_{0},\mathbf{S}\mathbf{x}_{i}\right>/ \gamma=\left<\mathbf{x}_{0},\mathbf{x}_{i}\right>\). So this result also conforms to our analysis that BRFA is adept at approximating functions of correlations as in Example 5.

In conclusion, RFA and BRFA have similar performance in fitting functions without correlation structure, such as (20) and (79), and RFA may behave even better in some cases. However, BRFA is presumably more powerful than RFA in approximating functions of correlations.

### Weight matrices in BERT

As noted in Section 4, we plot the query-key matrices (weight matrices) of the BERT-Base model [29]6 and show that many query-key matrices are diagonally dominated. The BERT-Base model has 12 attention layers with 12 heads in each layer. Denote the query matrix in the \(i\)-th head of \(j\)-th layer as \(Q_{ij}\in\mathbb{R}^{768\times 64}\) and the key matrix as \(K_{ij}\in\mathbb{R}^{768\times 64}\). We compute \(W_{ij}=\sqrt{768}\cdot Q_{ij}K_{ij}^{\top}\) for \(i,j\) from 1 to 12, and then take the absolute value for each entry of the weight matrices \(W_{ij}\). Figure 1Figure 4: Test error of three RF models for learning \(f_{2,3}\) (21). We fix \(N=16\) while vary \(d=8\) (left), \(16\) (middle), and \(32\) (right). For a fair comparison among RFA, BRFA, and RFMLP, the number of heads of RFMLP is taken to be \(M_{\text{RFMLP}}=9000\) (left), \(17000\) (middle), and \(33000\) (right). The other settings are the same as in Figure 2.

Figure 5: Test error of three RF models for learning \(f_{4,\gamma}\) (80). We choose \(\gamma=0\) (left), \(2\) (middle), and \(8\) (right). The other settings are the same as in Figure 2.

Figure 3: Test error of three RF models for learning \(f_{3,2}\) (79). We fix \(d=16\) while varying \(N=4\) (left), \(8\) (middle), and \(16\) (right). The other settings are the same as in Figure 2.

shows the heat maps of weight matrices of the 2nd, 5th, 8th, and 11th layers, where all matrices are clipped to the top-left \(32\times 32\) block. As we can see, a lot of weight matrices are diagonally dominated. We remark that a very recent and concurrent paper [84] observed similar phenomena for the ViT-Tiny model. They further show that diagonally dominated weight initialization of self-attention layers allows training transformers faster and obtaining higher final accuracies on CIFAR-10 and ImageNet datasets.