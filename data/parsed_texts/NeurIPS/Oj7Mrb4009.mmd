# Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples Regularization

Runqi Lin Chaojian Yu Tongliang Liu

Sydney AI Centre, The University of Sydney

{rlin0511, chyu8051, tongliang.liu}@sydney.edu.au

Corresponding author

###### Abstract

Single-step adversarial training (SSAT) has demonstrated the potential to achieve both efficiency and robustness. However, SSAT suffers from catastrophic overfitting (CO), a phenomenon that leads to a severely distorted classifier, making it vulnerable to multi-step adversarial attacks. In this work, we observe that some adversarial examples generated on the SSAT-trained network exhibit anomalous behaviour, that is, although these training samples are generated by the inner maximization process, their associated loss decreases instead, which we named abnormal adversarial examples (AAEs). Upon further analysis, we discover a close relationship between AAEs and classifier distortion, as both the number and outputs of AAEs undergo a significant variation with the onset of CO. Given this observation, we re-examine the SSAT process and uncover that before the occurrence of CO, the classifier already displayed a slight distortion, indicated by the presence of few AAEs. Furthermore, the classifier directly optimizing these AAEs will accelerate its distortion, and correspondingly, the variation of AAEs will sharply increase as a result. In such a vicious circle, the classifier rapidly becomes highly distorted and manifests as CO within a few iterations. These observations motivate us to eliminate CO by hindering the generation of AAEs. Specifically, we design a novel method, termed _Abnormal Adversarial Examples Regularization_ (AAER), which explicitly regularizes the variation of AAEs to hinder the classifier from becoming distorted. Extensive experiments demonstrate that our method can effectively eliminate CO and further boost adversarial robustness with negligible additional computational overhead. Our implementation can be found at https://github.com/tmllab/2023_NeurIPS_AAER.

## 1 Introduction

In recent years, deep neural networks (DNNs) have demonstrated impressive performance in various decision-critical domains, such as autonomous driving [25, 12], face recognition [31, 3] and medical imaging diagnosis [8]. However, DNNs were found to be vulnerable to adversarial examples [35, 9, 16]. Although these adversarial perturbations are imperceptible to human eyes, they can lead to a completely different prediction in DNNs. To this end, many adversarial defence strategies have been proposed, such as pre-processing techniques [13], detection algorithms [27], verification and provable defence [20, 44], and adversarial training (AT) [11, 26, 46]. Among them, AT is considered to be the most effective method against adversarial attacks [2, 4].

Despite the notable progress in improving model robustness, the standard multi-step AT significantly increases the computational overhead due to the iterative steps of forward and backward propagation [26, 40, 41, 42, 48, 47]. In light of this, several works have attempted to use single-step adversarial training (SSAT) as a more efficient alternative to achieve robustness. Unfortunately, aserious problem catastrophic overfitting (CO) has been identified in SSAT [39], manifesting as a sharp decline in the model's robust accuracy against multi-step adversarial attacks, plummeting from a peak to nearly 0% within a few iterations, as shown in Figure 1. This intriguing phenomenon has been widely investigated and prompted numerous efforts to resolve it. Recently, Kim [21] pointed out that the SSAT-trained classifiers are typically accompanied by highly distorted decision boundaries, which will lead to the model manifestation as CO. However, the underlying process of the classifier's gradual distortion, as well as the factor inducing rapid distortion, remains unclear.

In this study, we identify some adversarial examples generated by the distorted classifier exhibiting anomalous behaviour, wherein the loss associated with them decreases despite being generated by the inner maximization process. We refer to these anomalous training samples as abnormal adversarial examples (AAEs). Upon further investigation of the training process, we observe that both the number and outputs of AAEs undergo a significant variation during CO. This observation suggests a strong correlation between the variation of AAEs and the gradually distorted classifier. By utilizing AAEs as the indicator, we re-evaluate the process of SSAT and uncover that the classifier already exhibits slight distortions even before the onset of CO, which is evidenced by the presence of few AAEs. To make matters worse, directly optimizing the model based on these AAEs will further accelerate the distortion of the decision boundaries. Furthermore, in response to this more distorted classifier, the variation in AAE will dramatically increase as a result. This interaction leads to a vicious circle between the variation of AAEs and the decision boundaries distortion, ultimately leading to the model rapidly manifesting as CO. All these atypical findings raise a question:

_Can CO be prevented by hindering the generation of abnormal adversarial examples?_

To answer the above question, we design a novel method, called _Abnormal Adversarial Examples Regularization_ (AAER), which prevents CO by incorporating a regularizer term designed to suppress the generation of AAEs. Specifically, to achieve this objective, AAER consists of two components: the number and the outputs variation of AAEs. The first component identifies and counts the number of AAEs in the training samples through anomalous loss decrease behaviour. The second component calculates the outputs variation of AAEs by combining the prediction confidence and logits distribution. Subsequently, AAER explicitly regularizes both the number and the outputs variation of AAEs to prevent the model from being distorted. It is worth noting that our method does not involve any extra example generation or backward propagation processes, making it highly efficient in terms of computational overhead. Our major contributions are summarized as follows:

* We identify a particular behaviour in SSAT, in which some AAEs generated by the distorted classifier have an opposite objective to the maximization process, and their number and outputs variation are highly correlated with the classifier distortion.
* We discover that the classifier exhibits initial distortion before CO, manifesting as a small number of AAEs. Besides, the model decision boundaries will be further exacerbated by directly optimizing the classifier on these AAEs, leading to a further increase in their number, which ultimately manifests as CO within a few iterations.
* _Abnormal Adversarial Examples Regularization_ (AAER), which explicitly regularizes the number and outputs variation of AAEs to hinder the classifier from becoming distorted. We evaluate the effectiveness of our method across different adversarial budgets, adversarial attacks, datasets and network architectures, showing that our proposed method can consistently prevent CO even with extreme adversaries and boost robustness with negligible additional computational overhead.

Figure 1: The test accuracy of RS-FGSM [39] (red line) and RS-AAER (green line) with 16/255 noise magnitude. The dashed and solid lines denote natural and robust (PGD-7-1) accuracy, respectively. The dashed black line corresponds to the 9th epoch, which is the point that RS-FGSM occurs CO.

Related Work

### Adversarial Training

DNNs are known to be vulnerable to adversarial attacks [35], and AT has been demonstrated to be the most effective defence method [2]. AT is generally formulated as a min-max optimization problem [26; 4]. The inner maximization problem tries to generate the strongest adversarial examples to maximize the loss, and the outer minimization problem tries to optimize the network to minimize the loss on adversarial examples, which can be formalized as follows:

\[\min_{\theta}\mathbb{E}_{(x,y)\sim\mathcal{D}}\left[\max_{\delta\in\Delta} \ell(x+\delta,y;\theta)\right],\] (1)

where \((x,y)\) is the training dataset from the distribution \(D\), \(\ell(x,y;\theta)\) is the loss function parameterized by \(\theta\), \(\delta\) is the perturbation confined within the boundary \(\epsilon\) shown as: \(\Delta=\{\delta:\|\delta\|_{p}\leq\epsilon\}\).

### Catastrophic Overfitting

Since the intriguing phenomenon of CO was identified [39], there has been a line of work trying to explore and mitigate this problem. [39] first suggested using a random initialization and early stopping to avoid CO. Furthermore, [36] empirically showed that using a dynamic dropout schedule can avoid early overfitting to adversarial examples, and [6; 45] found that incorporating a stronger data augmentation is effective in avoiding CO. Another alternative approach imports partial multi-step AT, for example, [38] periodically trained the model on natural, single-step and multi-step adversarial examples, and [37] built a regularization term by comparing with the multi-step adversarial examples.

However, the above methods have not provided a deeper insight into the essence of CO. Separate works found that CO is closely related to anomalous gradient updates. [24] constrained the training samples to a carefully extracted subspace to avoid abrupt gradient growth. [10] ignored the small gradient adversarial perturbations to mitigate substantial weight updates in the network. [15] proposed an instance-adaptive SSAT approach where the perturbation size is inversely proportional to the gradient. [29] leveraged the latent representation of gradients as the adversarial perturbation to compensate for local linearity. [34] introduced a relaxation term to find more suitable gradient directions by smoothing the loss surface. [1] proposed a regularization term to avoid the non-linear surfaces around the samples. More recently, [21] introduced a new perspective that CO is a manifestation of highly distorted decision boundaries. Accordingly, they proposed to reduce the perturbation size for the already misclassified adversarial examples.

Unfortunately, the aforementioned methods tend to either suffer from CO with strong adversaries or significantly increase the computational overhead. In this work, we delve into the interaction between AAEs and distorted decision boundaries, revealing a close relationship between them. Based on this insight, we propose a novel approach, AAER, that eliminates CO by explicitly hindering the generation of AAEs, thereby achieving both efficiency and robustness.

## 3 Methodology

In this section, we first define the abnormal adversarial example (AAE) and show how their numbers change throughout the training process (Section 3.1). We further compare the outputs variation of normal adversarial examples (NAEs) and AAEs and find that their outputs exhibit significantly different behaviour after CO (Section 3.2). Building upon our observations, we propose a novel regularization term, _Abnormal Adversarial Examples Regularization_ (AAER), that uses the number and outputs variation of AAEs to explicitly suppress the generation of them to eliminate catastrophic overfitting (CO) (Section 3.3).

### Definition and Counting of AAE

Adversarial training employs the most adversarial data to reduce the sensitivity of the network's outputs w.r.t. adversarial perturbation of the natural data. Consequently, the inner maximization process is expected to generate effective adversarial examples that maximize the classification loss. As empirically demonstrated by [21], the decision boundaries of the classifier become highly distorted after the occurrence of CO. In this study, we find that after adding the adversarial perturbation generated by the distorted classifier, the loss of certain training samples unexpectedly decreases. This particular behaviour is illustrated in Figure 2, we can observe that the NAEs (blue) can either lead to the model misclassifications or position themselves closer to the decision boundary after the inner maximization process. In contrast, the AAEs (red) will be located further away from the decision boundary and fail to mislead the classifier after adding the perturbation generated by the distorted classifier. Therefore, we introduce the following formula to define AAEs:

\[\begin{split}\delta=\operatorname{sign}\left(\nabla_{x+\eta} \ell(x+\eta,y;\theta)\right),\\ x^{AAE}\stackrel{{ def}}{{=}}\ell\left(x+\eta,y; \theta\right)>\ell\left(x+\eta+\delta,y;\theta\right),\end{split}\] (2)

where \(\eta\) is the random initialization.

Next, we observe the variation in the number of AAEs throughout the model training process, and the corresponding statistical results are presented in Figure 3 (left). It can be observed that before the occurrence of CO, a small number of AAEs already existed, indicating the presence of slight initial distortion in the classifier. To further validate this point, we visualize the loss surface of both AAEs and NAEs using the method proposed by [23] as shown in Figure 4 (left and middle). It's evident that before CO, the classifier showcases a more nonlinear loss surface around AAEs in comparison to NAEs. This empirical observation strongly suggests that the generation of AAEs is directly influenced by the distorted classifier.

Besides, the number of AAEs experiences a dramatic surge during CO occurrences. For example, the number of AAEs (red line) exploded 19 times at the onset of CO (9th epoch), as shown in Figure 3 (left). Importantly, this rapid increase in the AAEs number implies a continuous deterioration in the classifier's boundaries, which in turn leads to a further increase in their number. The number of AAEs reaches its peak at the 10th epoch, surging to approximately 66 times than that before CO. To meticulously analyze the interaction between AAEs and CO, we delve into their relationship at the iteration level, as shown in Figure 4 (right). We can observe that the robustness accuracy from peak sharply drops to nearly 0% within 18 iterations, simultaneously, the number of AAEs rises from 0 to 70. Remarkably, the trends in robustness accuracy and the number of AAEs display a completely opposite relationship, suggesting a vicious cycle between optimizing AAEs and CO. Lastly, the number of AAEs consistently maintains a high level until the end of the training. Given this empirical and statistical observation, we can infer that there is a close correlation between the number of AAEs and the CO phenomenon, which also prompts us to wonder (Q1): _whether CO can be mitigated by reducing the number of abnormal adversarial examples_.

### Outputs Variation of NAE and AAE

The above observations indicate the close relationship between CO and AAEs. In this part, we further analyze the outputs variation of AAEs during CO. Specifically, we discover that CO has a

Figure 2: A conceptual diagram of the classifierâ€™s decision boundary and training samples. The training samples belonging to NAE (blue) can effectively mislead the classifier, while AAE (red) cannot. The left panel shows the decision boundary before optimizing AAEs, which only has a slight distortion. The middle panel shows the decision boundary after optimizing AAEs, which exacerbates the distortion and generates more AAEs.

significant impact on both the prediction confidence and the logits distribution of AAEs. To quantify the variation in prediction confidence, we utilize the cross-entropy (CE) to calculate the change in loss during the inner maximization process, which is formulated as follows:

\[\ell\left(x+\eta+\delta,y;\theta\right)-\ell\left(x+\eta,y;\theta\right).\] (3)

We investigate the prediction confidence of NAEs and AAEs variation during the model training. From Figure 3 (middle), we can observe that the change in prediction confidence of NAEs is consistently greater than 0, indicating that their lead to a worse prediction in the classifier. On the contrary, this variation in AAEs is atypical negative implying that the associated adversarial perturbation has an unexpected opposite effect. Moreover, we delve into the impact of CO on the variation in prediction confidence. Before CO, we note a slight negative variation in the AAEs' prediction confidence, which has an insignificant impact on all training samples (blue line). However, during CO, the prediction confidence of AAEs undergoes a rapid and substantial drop, reaching a decline of 17 times at the 9th epoch. After CO, the prediction confidence of AAEs is 43 times (10th epoch) smaller than before and significantly impacts all training samples.

In addition to the inability to mislead the classifier, the logits distribution of AAE is also disturbed during the CO process. To analyze the variation in logits distribution, we employ the Euclidean (L2) distance to quantify the impact of adversarial perturbation, which is formulated as follows:

\[\left\|f_{\theta}\left(x+\eta+\delta\right)-f_{\theta}\left(x+\eta\right) \right\|_{2}^{2},\] (4)

where \(f_{\theta}\) is the DNN classifier parameterized by \(\theta\) and \(\|\cdot\|_{2}^{2}\) is the L2 distance.

The logits distribution variation of both NAEs and AAEs are illustrated in Figure 3 (right). Comparing the logits distribution variation between NAEs and AAEs, we can find that their magnitudes are similar before CO. However, it becomes evident that the logits distribution variation of AAEs increases dramatically during CO, being 13 times larger than before. After further optimization on AAEs, the variation in logits distribution reaches the peak, approximately 62 times larger than before. This observation highlights that even a small adversarial perturbation can cause a substantial variation in the logits distribution, this phenomenon typically happens on the highly distorted decision boundaries. Additionally, it's worth noting that the increase in logits distribution variation for NAEs (green line) occurs one epoch later than that of AAEs, indicating that the primary cause of decision boundary distortion lies within the AAEs. In other words, directly optimizing the network using these AAEs exacerbates the distortion of decision boundaries, resulting in a significant change in the logits distribution for NAEs. Even after CO, the logits distribution variance of AAEs remains twice as large as NAEs. The significant difference between NAEs and AAEs in the variation of prediction confidence and logits distribution inspires us to wonder (Q2): _whether CO can be mitigated by constraining the outputs variation of abnormal adversarial examples._

### Abnormal Adversarial Examples Regularization

Recognizing the strong correlation between CO and AAEs, we first attempt a passive approach by removing AAEs and training solely on NAEs. This simple approach demonstrates the capability

Figure 3: The number, the variation of prediction confidence and logits distribution (from left to right) for NAEs, AAEs and training samples in RS-FGSM with 16/255 noise magnitude. The dashed black line corresponds to the 9th epoch, which is the point that the model occurs CO.

to delay the onset of CO, thereby confirming that direct optimization of AAEs will accelerate the classifier's distortion. However, it is important to note that the generation of AAEs is caused by the distorted classifier. Passively removing AAEs cannot provide the necessary constraints to promote smoother classifiers, thereby only delaying the onset of CO but not preventing it.

To truly relieve this problem, we design a novel regularization term, _Abnormal Adversarial Examples Regularization_ (AAER), which aims to hinder the classifier from becoming distorted by explicitly reducing the number and constraining the outputs variation of AAEs. Specifically, part (_i_) categorizes the training samples into NAEs and AAEs according to the definition in Eq. 2, and then penalizes the number of AAEs. The AAEs' outputs variation is simultaneously constrained by part (_ii_) prediction confidence and part (_iii_) logits distribution. In terms of prediction confidence, we penalize the anomalous variation in AAEs that should not be negative during the inner maximization process, which is formalized as follows:

\[AAE\_CE=\frac{1}{n}\sum_{i=1}^{n}\left(\ell\left(x_{i}^{AAE}+\eta,y_{i};\theta \right)-\ell\left(x_{i}^{AAE}+\eta+\delta,y_{i};\theta\right)\right),\] (5)

where \(n\) is the number of abnormal adversarial examples.

For logits distribution, we first calculate the logits distribution variation of AAEs and NAEs separately, as shown in Eq. (6) and Eq. (7):

\[AAE\_L2=\frac{1}{n}\sum_{i=1}^{n}\left(\left\|f_{\theta}\left(x_{i}^{AAE}+\eta +\delta\right)-f_{\theta}\left(x_{i}^{AAE}+\eta\right)\left\|_{2}^{2}\right);\right.\] (6)

\[\left.NAE\_L2=\frac{1}{m-n}\sum_{j=1}^{m-n}\left(\left\|f_{\theta}\left(x_{j}^ {NAE}+\eta+\delta\right)\right.\right.\left.-f_{\theta}\left(x_{j}^{NAE}+\eta \right)\left\|_{2}^{2}\right),\right.\] (7)

where \(m\) is the number of training samples.

Then, we use the logits distribution variation of NAEs as a reference to constrain the variation in AAEs. It's essential to emphasize that our optimization objective is to make the logits distribution variation of AAEs closer to that of NAEs, rather than less. To achieve this, we use the max function to limit the minimum value, which is formalized as follows:

\[Constrained\_Variation=max\left(AAE\_L2\left(6\right)-NAE\_L2\left(7\right),0 \right),\] (8)

where \(max(,)\) is the max function.

Although Figure 3 (right) illustrates that the logits distribution variation of NAEs will significantly increase and instability after CO. However, that is a natural consequence of the highly distorted classifier which disrupted the logits distribution of NAEs. In contrast, after using the AAER to hinder the classifier from becoming distorted, the NAEs can be used as a stable standard throughout the training, as shown in Figure 5 (a:right).

Based on the optimization objectives described above, we can build a novel regularization term - AAER, which aims to suppress the AAEs by the number, the variation of prediction confidence

Figure 4: Left/Middle panel: The visualization of AAEs/NAEs loss surface before CO (8th epoch). Right panel: The number of AAEs and the test robustness within each iteration at CO (9th epoch). The green and red lines represent the robust accuracy and number of AAEs, respectively.

and logits distribution, ultimately achieving the purpose of preventing CO, which is shown in the following formula:

\[AAER=(\lambda_{1}\cdot\frac{n}{m})\cdot\left(\lambda_{2}\cdot AAE\_CE\left(5 \right)+\lambda_{3}\cdot Constrained\_Variation\left(8\right)\right),\] (9)

where \(\lambda_{1}\), \(\lambda_{2}\) and \(\lambda_{3}\) are the hyperparameters to control the strength of the regularization term.

AAER can effectively hinder the generation of AAEs that are highly correlated with the distorted classifier and CO, thereby encouraging training for a smoother classifier that can effectively defend against adversarial attacks. By considering both the number and output variation of AAEs, we establish a more adaptable and comprehensive measure of classifier distortion. Importantly, our method does not require any additional generation or backward propagation processes, making it highly convenient in terms of computational overhead. The proposed algorithm AAER realization is summarized in Algorithm 1.

```
0: Network \(f_{\theta}\), epochs T, mini-batch M, perturbation radius \(\epsilon\), step size \(\alpha\), initialization term \(\eta\).
0: Adversarially robust model \(f_{\theta}\)
1:for\(t=1\dots T\)do
2:for\(k=1\dots M\)do
3:\(\delta=\alpha\cdot\operatorname{sign}\left(\nabla_{x+\eta}\ell(x_{k}+\eta,y_ {k};\theta)\right)\)
4:\(CE=\frac{1}{m}\sum_{k=1}^{m}\ell\left(x_{k}+\eta+\delta,y_{k};\theta\right)\)
5:\(AAER\) = Eq. (9)
6:\(\theta=\theta-\nabla_{\theta}\left(CE+AAER\right)\)
7:endfor
8:endfor ```

**Algorithm 1**_Abnormal Adversarial Examples Regularization_ (AAER)

## 4 Experiment

In this section, we provide a comprehensive evaluation to verify the effectiveness of AAER, including experiment settings (Section 4.1), performance evaluation (Section 4.2), ablation studies (Section 4.3) and time complexity study (Section 4.4).

### Experiment Settings

**Baselines.** We compare our method with other SSAT methods, including RS-FGSM [39], FreeAT [30], N-FGSM [6], Grad Align [1], ZeroGrad and MultiGrad [10]. We also compare our method with multi-step AT, PGD-2 and PGD-10 (PGD-20 with 32/255 noise magnitude) [26], providing a reference for the ideal performance. The results of other competing baselines, including GAT [33], NuAT [34], PGI-FGSM [17], SDI-FGSM [18] and Kim [21], can be found in Appendix F. We report both the natural and robust accuracy results of the final model, which are obtained without early stopping and using the hyperparameters provided in the official repository. Please note that for FreeAT, we did not use the subset of training samples to keep the same training epochs across different methods.

**Attack Methods.** To report the robust accuracy of models, we attack these methods using the standard PGD adversarial attack with \(\alpha=\epsilon/4\) step size, 50 attack steps and 10 restarts. We also evaluate our methods on Auto Attack [5] as shown in Appendix C.

**Datasets and Model Architectures.** We evaluate our method on several benchmark datasets, including Cifar-10/100 [22], SVHN [28], Tiny-ImageNet [28] and Imagenet-100 [7]. The standard data augmentation random cropping and horizontal flipping are applied for these datasets. The settings and results on SVHN, Tiny-ImageNet and Imagenet-100 are provided in Appendix E. We use the PreactResNet-18 [14] and WideResNet-34 [43] architectures on these datasets to evaluate results. The results of WideResNet-34 can be found in Appendix D.

**Setup for Our Proposed Method.** In this work, we use the SGD optimizer with a momentum of 0.9, weight decay of 5 x \(10^{-4}\) and \(L_{\infty}\) as the threat model. For the learning rate schedule, we use the cyclical learning rate schedule [32] with 30 epochs, which reaches its maximum learning rate 

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

(_iii_) logits distribution can partially mitigate CO. However, solely relying on part (_iii_) cannot accurately reflect the degree of classifier distortion as it lacks a comprehensive measure, resulting in poor performance. Additionally, part (_i_) also plays an important role in performance, without it, both natural and robust accuracy significantly drop. Meanwhile, part (_ii_) contributes to the stability and natural accuracy of the method. Therefore, to effectively and stably eliminate CO, all parts of the regularization term are necessary and critical. Further ablation studies on other regularization methods and \(\lambda\) selection can be found in Appendix B.

### Time Complexity Study

Efficiency is a key advantage of SSAT over multi-step AT, as it can be readily scaled to large networks and datasets. Consequently, the computational overhead plays an important role in the SSAT overall performance. In Table 4, we present a time complexity comparison among various SSAT methods. It can be seen that AAER only imposes a minor training cost of 0.2 seconds, representing a mere 1.8% increase compared to FGSM. In contrast, Grad Align and PGD-10 are 3.2 and 5.3 times slower than our method.

## 5 Conclusion

In this paper, we find that the abnormal adversarial examples exhibit anomalous behaviour, i.e. they are further to the decision boundaries after adding perturbations generated by the inner maximization process. We empirically show the abnormal adversarial examples are closely related to the classifier distortion and catastrophic overfitting, by analyzing their number and outputs variation during the training process. Motivated by this, we propose a novel and effective method, _Abnormal Adversarial Examples Regularization_ (AAER), through a regularizer to eliminate catastrophic overfitting by suppressing the generation of abnormal adversarial examples. Our approach can successfully resolve the catastrophic overfitting with different noise magnitudes and further boost adversarial robustness with negligible additional computational overhead.

## Acknowledgments and Disclosure of Funding

Tongliang Liu is partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, and IC190100031.

\begin{table}
\begin{tabular}{c|c c c c c c|c c} \hline \hline Method & FreeAT & ZeroGrad & MultiGrad & Grad Align & RS/N-FGSM & RS/N-AAER & PGD-2 & PGD-10 \\ \hline Training Time (S) & 43.8 & 11.0 & 21.7 & 36.1 & 11.0 & 11.2 & 16.4 & 59.1 \\ \hline \hline \end{tabular}
\end{table}
Table 4: CIFAR10 training time on a single NVIDIA RTX 4090 GPU using PreactResNet-18. The results are averaged over 30 epochs.

## References

* [1]M. Andriushchenko and N. Flammarion (2020) Understanding and improving fast adversarial training. Advances in Neural Information Processing Systems33, pp. 16048-16059. Cited by: SS1.
* [2]A. Athalye, N. Carlini, and D. Wagner (2018) Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples. In International conference on machine learning, pp. 274-283. Cited by: SS1.
* [3]S. Balaban (2015) Deep learning and face recognition: the state of the art. Biometric and surveillance technology for human and activity identification XII, pp. 68-75. Cited by: SS1.
* [4]F. Croce, S. Gowal, T. Brunner, E. Shelhamer, M. Hein, and T. Cemgil (2022) Evaluating the adversarial robustness of adaptive test-time defenses. In International Conference on Machine Learning, pp. 4421-4435. Cited by: SS1.
* [5]F. Croce and M. Hein (2020) Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206-2216. Cited by: SS1.
* [6]P. de Jorge, A. Bibi, R. Volpi, A. Sanyal, P. H. Torr, G. Rogez, and P. K. Dokania (2022) Make some noise: reliable and efficient single-step adversarial training. arXiv preprint arXiv:2202.01181. Cited by: SS1.
* [7]J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009) Imagenet: a large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255. Cited by: SS1.
* [8]B. J. Erickson, P. Korfiatis, Z. Akkus, and T. L. Kline (2017) Machine learning for medical imaging. Radiographics37 (2), pp. 505-515. Cited by: SS1.
* [9]F. E. Fernandes and G. G. Yen (2020) Automatic searching and pruning of deep neural networks for medical imaging diagnostic. IEEE Transactions on Neural Networks and Learning Systems32 (12), pp. 5664-5674. Cited by: SS1.
* [10]Z. Gologoni, M. Saberi, M. Eskandar, and M. R. Rohban (2021) Zerograd: mitigating and explaining catastrophic overfitting in fgsm adversarial training. arXiv preprint arXiv:2103.15476. Cited by: SS1.
* [11]I. J. Goodfellow, J. Shlens, and C. Szegedy (2014) Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572. Cited by: SS1.
* [12]S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu (2020) A survey of deep learning techniques for autonomous driving. Journal of Field Robotics37 (3), pp. 362-386. Cited by: SS1.
* [13]C. Guo, M. Rana, M. Cisse, and L. Van Der Maaten (2017) Countering adversarial images using input transformations. arXiv preprint arXiv:1711.00117. Cited by: SS1.
* [14]K. He, X. Zhang, S. Ren, and J. Sun (2016) Identity mappings in deep residual networks. In European conference on computer vision, pp. 630-645. Cited by: SS1.
* [15]Z. Huang, Y. Fan, C. Liu, W. Zhang, Y. Zhang, M. Salzmann, S. Susstrunk, and J. Wang (2022) Fast adversarial training with adaptive step size. arXiv preprint arXiv:2206.02417. Cited by: SS1.
* [16]Z. Huang, M. Zhu, X. Xia, L. Shen, J. Yu, C. Gong, B. Han, B. Du, and T. Liu (2023) Robust generalization against photon-limited corruptions via worst-case sharpness minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16175-16185. Cited by: SS1.
* [17]X. Jia, Y. Zhang, X. Wei, B. Wu, K. Ma, J. Wang, and X. Cao (2022) Prior-guided adversarial initialization for fast adversarial training. In European Conference on Computer Vision, pp. 567-584. Cited by: SS1.

[MISSING_PAGE_POST]

* [20] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In _International conference on computer aided verification_, pages 97-117. Springer, 2017.
* [21] Hoki Kim, Woojin Lee, and Jaewook Lee. Understanding catastrophic overfitting in single-step adversarial training. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 8119-8127, 2021.
* [22] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [23] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. _Advances in neural information processing systems_, 31, 2018.
* [24] Tao Li, Yingwen Wu, Sizhe Chen, Kun Fang, and Xiaolin Huang. Subspace adversarial training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13409-13418, 2022.
* [25] Todd Litman. _Autonomous vehicle implementation predictions_. Victoria Transport Policy Institute Victoria, BC, Canada, 2017.
* [26] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_, 2017.
* [27] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial perturbations. _arXiv preprint arXiv:1702.04267_, 2017.
* [28] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.
* [29] Geon Yeong Park and Sang Wan Lee. Reliably fast adversarial training via latent adversarial perturbation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7758-7767, 2021.
* [30] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! _Advances in Neural Information Processing Systems_, 32, 2019.
* [31] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In _Proceedings of the 2016 acm sigsac conference on computer and communications security_, pages 1528-1540, 2016.
* [32] Leslie N Smith. Cyclical learning rates for training neural networks. In _2017 IEEE winter conference on applications of computer vision (WACV)_, pages 464-472. IEEE, 2017.
* [33] Gaurang Sriramanan, Savanti Addepalli, Arya Baburaj, et al. Guided adversarial attack for evaluating and enhancing adversarial defenses. _Advances in Neural Information Processing Systems_, 33:20297-20308, 2020.
* [34] Gaurang Sriramanan, Savanti Addepalli, Arya Baburaj, et al. Towards efficient and effective adversarial training. _Advances in Neural Information Processing Systems_, 34:11821-11833, 2021.
* [35] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _arXiv preprint arXiv:1312.6199_, 2013.
* [36] BS Vivek and R Venkatesh Babu. Single-step adversarial training with dropout scheduling. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 947-956. IEEE, 2020.
* [37] BS Vivek, Ambareesh Revanur, Naveen Venkat, and R Venkatesh Babu. Plug-and-pipeline: Efficient regularization for single-step adversarial training. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 138-146. IEEE, 2020.
* [38] Xiaosen Wang, Chuanbiao Song, Liwei Wang, and Kun He. Multi-stage optimization based adversarial training. _arXiv preprint arXiv:2106.15357_, 2021.
* [39] Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training. _arXiv preprint arXiv:2001.03994_, 2020.
* [40] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. _Advances in Neural Information Processing Systems_, 33:2958-2969, 2020.

* [41] Chaojian Yu, Bo Han, Mingming Gong, Li Shen, Shiming Ge, Bo Du, and Tongliang Liu. Robust weight perturbation for adversarial training. _arXiv preprint arXiv:2205.14826_, 2022.
* [42] Chaojian Yu, Bo Han, Li Shen, Jun Yu, Chen Gong, Mingming Gong, and Tongliang Liu. Understanding robust overfitting of adversarial training and beyond. In _International Conference on Machine Learning_, pages 25595-25610. PMLR, 2022.
* [43] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. _arXiv preprint arXiv:1605.07146_, 2016.
* [44] Bohang Zhang, Du Jiang, Di He, and Liwei Wang. Rethinking lipschitz neural networks and certified robustness: A boolean function perspective. _Advances in Neural Information Processing Systems_, 35:19398-19413, 2022.
* [45] Chaoning Zhang, Kang Zhang, Axi Niu, Chenshuang Zhang, Jiu Feng, Chang D Yoo, and In So Kweon. Noise augmentation is all you need for fgsm fast adversarial training: Catastrophic overfitting and robust overfitting require different augmentation. _arXiv preprint arXiv:2202.05488_, 2022.
* [46] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In _International conference on machine learning_, pages 7472-7482. PMLR, 2019.
* [47] Dawei Zhou, Nannan Wang, Xinbo Gao, Bo Han, Xiaoyu Wang, Yibing Zhan, and Tongliang Liu. Improving adversarial robustness via mutual information estimation. In _International Conference on Machine Learning_, pages 27338-27352. PMLR, 2022.
* [48] Dawei Zhou, Nannan Wang, Heng Yang, Xinbo Gao, and Tongliang Liu. Phase-aware adversarial defense for improving adversarial robustness. 2023.

Vanilla-AAER

To further validate the effectiveness of our method, we implement Vanilla-AAER to prevent CO. The Vanilla-AAER method follows the settings of Vanilla-FGSM [11], which does not use random initialization and sets the step size as \(\alpha=1.0\cdot\epsilon\). The Vanilla-AAER hyperparameters setting is shown in Table 5.

Based on the results presented in Table 6, we can observe that Vanilla-AAER achieves comparable or even superior robustness compared to RS-AAER. This outcome may be attributed to the fact that Vanilla-FGSM has a higher expectation of adversarial perturbation, as demonstrated in prior work [1]. However, the absence of random initialization in Vanilla-AAER may reduce the diversity of adversarial examples, potentially impacting the natural accuracy of the model. Nonetheless, the most significant finding is that Vanilla-AAER effectively eliminates CO across various noise magnitudes, which cannot be accomplished by Vanilla-FGSM.

## Appendix B Impacts of Regularization Term

To showcase the distinct effectiveness of our method in eliminating CO, we conducted a comparison with other regularization methods used in multi-step AT, such as TRADES [46] and ALP [19]. In order to ensure a fair comparison, we set the iteration times for TRADES and ALP as 1, and the step size as \(\alpha=1.25\cdot\epsilon\) and \(\alpha=1.0\cdot\epsilon\) for superimposing RS-FGSM and N-FGSM, respectively.

From Table 7, we can observe that TRADES and ALP methods may improve adversarial robustness when CO is not present in the baseline methods. However, these methods are not effective in eliminating CO. As demonstrated by the RS-TRADES and RS-ALP methods, CO still occurs with larger noise magnitudes, similar to RS-FGSM. Furthermore, these methods can even harm the baseline method, as they break the N-FGSM robustness with 32/255 noise magnitude. Therefore, we conclude that the TRADES and ALP methods are not suitable for eliminating CO. Additionally, other multi-step and robust overfitting methods also prove ineffective against CO. Hence, CO has been identified as an independent phenomenon requiring distinct solutions. In contrast to these regularization methods, AAER explicitly reflects and prevents distortion of the classifier from the perspective of AAEs, which is the key factor enabling AAER to effectively eliminate CO.

We further investigate the impact of hyperparameters \(\lambda_{1}\), \(\lambda_{2}\), and \(\lambda_{3}\) on the performance of AAER. Figure 7 (left) shows the effect of \(\lambda_{1}\) on the performance. It can be observed that when \(\lambda_{1}\) is small, AAER is unable to effectively suppress CO, and increasing \(\lambda_{1}\) improves both natural and robust accuracy. However, when \(\lambda_{1}\) increases from 1.0 to 2.0, the robust accuracy remains unchanged while the natural accuracy decreases. Therefore, we choose \(\lambda_{1}=1\) to balance both natural and robust

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline noise magnitude & 8/255 & 12/255 & 16/255 & 32/255 \\ \hline \hline Vanilla-FGSM & \(84.16\pm 4.68\) & \(79.86\pm 2.05\) & \(72.51\pm 3.79\) & \(64.29\pm 3.83\) \\  & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) \\ \hline Vanilla-AAER & \(80.45\pm 0.25\) & \(64.97\pm 3.14\) & \(51.92\pm 2.90\) & \(18.78\pm 2.45\) \\  & \(\mathbf{46.66\pm 0.74}\) & \(\mathbf{32.44\pm 1.18}\) & \(\mathbf{24.12\pm 0.76}\) & \(\mathbf{12.19\pm 0.40}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: CIFAR10: Accuracy of Vanilla-FGSM and Vanilla-AAER with different noise magnitude using PreActResNet-18 under \(L_{\infty}\) threat model. The top number is the natural accuracy (%), while the bottom number is the PGD-50-10 accuracy (%).

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Vanilla-AAER & 8/255 & 12/255 & 16/255 & 32/255 \\ \hline \(\lambda_{2}\) & 5.5 & 6.5 & 7.0 & 4.8 \\ \hline \(\lambda_{3}\) & 2.0 & 3.5 & 3.5 & 0.7 \\ \hline \hline \end{tabular}
\end{table}
Table 5: The hyperparameters setting for different noise magnitudes. The top number is \(\lambda_{2}\) while the bottom number is \(\lambda_{3}\). The \(\lambda_{1}\) is fixed as 1.0 in all settings.

accuracy. Figure 7 (middle) demonstrates the impact of \(\lambda_{2}\). It can be observed that when \(\lambda_{2}\) is small, increasing \(\lambda_{2}\) is beneficial for both natural and robust accuracy. However, when \(\lambda_{2}\) increases from 7 to 14, the robust accuracy becomes flat while the natural accuracy decreases. Therefore, we select \(\lambda_{2}=7\) considering both natural and robust accuracy. Figure 7 (right) shows the effect of \(\lambda_{3}\). It can be observed that when \(\lambda_{3}\) is small (2.25), the model experiences CO. Increasing \(\lambda_{3}\) reduces the natural accuracy. From the variation of \(\lambda_{3}\) between 2.25 and 4.75, we choose \(\lambda_{3}=3.25\) to balance the elimination of CO with natural and robustness performance.

## Appendix C Evaluation Based on Auto Attack

Auto Attack [5] is regarded as the most reliable robustness evaluation to date. It is an ensemble of complementary attacks, consisting of three white-box attacks (APGD-CE, APGD-DLR, and FAB) and a black-box attack (Square Attack). In order to avoid the pseudo-robustness brought by gradient masking or gradient obfuscation, we report the Auto Attack results on Cifar10/100 in Table 8 and Table 9.

In Table 8 and Table 9, we observe that our method, AAER, consistently improves adversarial robustness under Auto Attack on both the CIFAR-10 and CIFAR-100 datasets. It demonstrates that

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline noise magnitude & 8/255 & 12/255 & 16/255 & 32/255 \\ \hline \hline RS-TRADES (\(\beta\) = 1.0) & 89.03 & 91.41 & 92.11 & 90.95 \\  & 35.56 & 0.86 & 0.00 & 0.00 \\ \hline RS-TRADES (\(\beta\) = 6.0) & 90.72 & 92.19 & 91.50 & 88.69 \\  & 11.29 & 0.03 & 0.01 & 0.00 \\ \hline RS-ALP (logit pairing weight=0.5) & 86.75 & 92.18 & 91.14 & 81.03 \\  & 43.96 & 0.04 & 0.00 & 0.00 \\ \hline RS-ALP (logit pairing weight=1.0) & 85.15 & 92.14 & 90.48 & 81.03 \\  & 46.59 & 0.02 & 0.00 & 0.00 \\ \hline N-TRADES (\(\beta\) = 1.0) & 86.82 & 81.12 & 85.62 & 81.81 \\  & 41.24 & 16.40 & 0.12 & 0.00 \\ \hline N-TRADES (\(\beta\) = 6.0) & 83.23 & 74.77 & 68.16 & 85.97 \\  & 49.56 & 35.98 & 26.13 & 0.24 \\ \hline N-ALP (logit pairing weight=0.5) & 84.63 & 81.57 & 72.05 & 79.41 \\  & 46.03 & 27.85 & 0.32 & 0.00 \\ \hline N-ALP (logit pairing weight=1.0) & 82.60 & 76.84 & 69.32 & 82.98 \\  & 48.32 & 33.36 & 23.46 & 0.00 \\ \hline \hline \end{tabular}
\end{table}
Table 7: CIFAR10: Accuracy of TRADES and ALP methods with different noise magnitude using PreActResNet-18 under \(L_{\infty}\) threat model. The top number is the natural accuracy (%), while the bottom number is the PGD-50-10 accuracy (%).

AAER is effective in preventing CO and enhancing robustness under various adversarial attacks. The results validate the robustness and comprehensiveness of our proposed method.

## Appendix D Experiment with WideResNet Architecture

We also compare the performance of our method using WideResNet-34, which is more complex than PreActResNet. Since the baselines cannot adapt well to WideResNet-34, we also need to correspondingly adjust the hyperparameters. The \(\lambda_{1}\) is fixed as 1.0 in all settings. For CIFAR10, we set RS-AAER \(\lambda_{2}=4.0\) and \(\lambda_{3}=2.0\), N-AAER \(\lambda_{2}=2.5\) and \(\lambda_{3}=0.6\). For CIFAR100, we set RS-AAER \(\lambda_{2}=2.5\) and \(\lambda_{3}=1.0\), N-AAER \(\lambda_{2}=1.0\) and \(\lambda_{3}=0.2\). We report the results on Cifar10/100 in Table 10 and Table 11.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline noise magnitude & 8/255 & 12/255 & 16/255 & 32/255 \\ \hline \hline FreeAT & \(40.23\pm 0.33\) & \(28.04\pm 0.73\) & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) \\ \hline ZeroGrad & 43.48 & - & - & - \\ \hline MultiGrad & 44.39 & - & - & - \\ \hline Grad Align & \(44.82\pm 0.09\) & \(30.05\pm 0.17\) & \(19.60\pm 0.47\) & \(7.89\pm 2.62\) \\ \hline RS-FGSM & \(43.17\pm 0.34\) & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) \\ \hline N-FGSM & \(44.43\pm 0.24\) & \(30.32\pm 0.08\) & \(19.06\pm 1.81\) & \(6.78\pm 0.75\) \\ \hline RS-AAER & \(43.22\pm 0.20\) & \(26.75\pm 0.21\) & \(17.03\pm 0.51\) & \(5.37\pm 0.67\) \\ \hline N-AAER & \(44.79\pm 0.23\) & \(30.76\pm 0.17\) & \(20.18\pm 0.15\) & \(8.46\pm 0.74\) \\ \hline \hline PGD-2 & \(42.97\pm 0.65\) & \(28.63\pm 0.38\) & \(18.52\pm 0.55\) & \(3.77\pm 0.02\) \\ \hline PGD-10 (20) & \(46.95\pm 0.54\) & \(33.30\pm 0.20\) & \(22.29\pm 0.27\) & \(11.48\pm 0.43\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: CIFAR10: Accuracy of different methods and different noise magnitudes using PreactResNet-18 under \(L_{\infty}\) threat model. We only report the robust accuracy (%) under Auto Attack while the natural accuracy is same as Table 2. The results are averaged over 3 random seeds and reported with the standard deviation.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline noise magnitude & 8/255 & 12/255 & 16/255 & 32/255 \\ \hline \hline FreeAT & \(18.28\pm 0.20\) & \(12.37\pm 0.14\) & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) \\ \hline ZeroGrad & 21.15 & - & - & - \\ \hline MultiGrad & 21.62 & - & - & - \\ \hline Grad Align & \(21.87\pm 0.13\) & \(13.78\pm 0.11\) & \(9.64\pm 0.12\) & \(1.76\pm 0.70\) \\ \hline RS-FGSM & \(7.98\pm 11.91\) & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) & \(0.00\pm 0.00\) \\ \hline N-FGSM & \(22.68\pm 0.25\) & \(14.57\pm 0.09\) & \(10.30\pm 0.14\) & \(0.00\pm 0.00\) \\ \hline RS-AAER & \(21.41\pm 0.01\) & \(12.31\pm 0.28\) & \(8.56\pm 0.02\) & \(2.93\pm 0.17\) \\ \hline N-AAER & \(22.93\pm 0.10\) & \(14.73\pm 0.24\) & \(10.35\pm 0.11\) & \(3.46\pm 0.14\) \\ \hline \hline PGD-2 & \(22.52\pm 0.14\) & \(13.69\pm 0.02\) & \(9.56\pm 0.07\) & \(1.76\pm 0.22\) \\ \hline PGD-10 (20) & \(23.78\pm 0.08\) & \(15.61\pm 0.09\) & \(10.93\pm 0.05\) & \(4.13\pm 0.10\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: CIFAR100: Accuracy of different methods and different noise magnitudes using PreactResNet-18 under \(L_{\infty}\) threat model. We only report the robust accuracy (%) under Auto Attack while the natural accuracy is same as Table 2. The results are averaged over 3 random seeds and reported with the standard deviation.

In Table 10 and Table 11, we observe that when using the WideResNet-34 architecture, RS-FGSM suffers from CO with a noise magnitude of 8/255, which is different from the results obtained with the PreActResNet-18 architecture. However, our method, AAER, can successfully prevent CO and achieve high robustness even with complex network architectures. This demonstrates the reliability of AAER in preventing CO and improving robustness across different network architectures. It is worth noting that complex networks can better reflect the efficiency of our method in terms of training time, while our method can achieve comparable robustness to multi-step AT.

## Appendix E Settings and Results on SVHN, Tiny-ImageNet and Imagenet-100

**SVHN Settings and Results.** For experiments on SVHN, we use the cyclical learning rate schedule with 15 epochs that reaches its maximum learning rate (0.05) when 40% (6) epochs are passed. In the meantime, we uniformly increase the step size between 0 and 5 epochs, which follow the settings of [6]. We show the hyperparameters setting on SVHN in Table 12. In Table 13, we present the performance of AAER on the SVHN dataset, along with the results of the competing baseline taken from [6]. It is evident that our method can successfully prevent CO and improve robust accuracy across different noise magnitudes. This demonstrates the effectiveness of AAER in enhancing the robustness of models trained on the SVHN dataset.

**Tiny-ImageNet Settings and Results.** We also scale our method to a medium-sized dataset Tiny-ImageNet to showcase its effectiveness. We utilized the cyclical learning rate schedule with 30 epochs, reaching the maximum learning rate of 0.2 at the midpoint of 15 epochs. For RS-AAER, we set \(\lambda_{2}=0.75\) and \(\lambda_{3}=0.15\), while for N-AAER, we set \(\lambda_{2}=0.25\) and \(\lambda_{3}=0.05\). The value of \(\lambda_{1}\) remained fixed at 1.0 in all settings. Table 14 presents the performance of AAER on the Tiny-ImageNet dataset. We can observe that our method effectively prevents CO and improves robust accuracy in this medium-scale dataset.

\begin{table}
\begin{tabular}{c|c c c|c c} \hline \hline method & RS-FGSM & N-FGSM & RS-AAER & N-AAER & PGD-2 & PGD-10 \\ \hline \hline natural accuracy (\%) & 84.41 \(\pm\) 0.45 & 84.67 \(\pm\) 0.32 & 87.39 \(\pm\) 0.14 & 84.47 \(\pm\) 0.23 & 88.68 \(\pm\) 0.14 & 85.53 \(\pm\) 0.22 \\ \hline robust accuracy (\%) & 0.00 \(\pm\) 0.00 & 49.72 \(\pm\) 0.25 & 47.58 \(\pm\) 0.42 & **50.07 \(\pm\) 0.53** & 47.32 \(\pm\) 0.50 & **53.70 \(\pm\) 0.53** \\ \hline training time (S) & \multicolumn{2}{c}{98.2} & \multicolumn{2}{c}{98.6} & \multicolumn{2}{c}{147.1} & \multicolumn{2}{c}{536.2} \\ \hline \hline \end{tabular}
\end{table}
Table 10: CIFAR10: Accuracy of different methods with 8/255 noise magnitude using WideResNet-34 under \(L_{\infty}\) threat model. The results are averaged over 3 random seeds and reported with the standard deviation.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline SVHN & 4/255 & 8/255 & 12/255 \\ \hline RS-AAER & 0.5 & 0.6 & 0.45 \\  & 1.25 & 0.85 & 0.55 \\ \hline N-AAER & 0.75 & 1.0 & 1.0 \\  & 0.25 & 1.0 & 0.75 \\ \hline \hline \end{tabular}
\end{table}
Table 12: SVHN: The hyperparameters setting for different noise magnitudes. The top number is \(\lambda_{2}\) while the bottom number is \(\lambda_{3}\). The \(\lambda_{1}\) is fixed as 1.0 in all settings.

\begin{table}
\begin{tabular}{c|c c c|c c} \hline \hline method & RS-FGSM & N-FGSM & RS-AAER & N-AAER & PGD-2 & PGD-10 \\ \hline \hline natural accuracy (\%) & 55.04 \(\pm\) 1.24 & 59.02 \(\pm\) 0.63 & 59.81 \(\pm\) 0.38 & 57.76 \(\pm\) 0.36 & 64.64 \(\pm\) 0.27 & 60.34 \(\pm\) 0.34 \\ \hline robust accuracy (\%) & 0.00 \(\pm\) 0.00 & 28.49 \(\pm\) 0.54 & 26.88 \(\pm\) 0.30 & **29.09 \(\pm\) 0.66** & 26.47 \(\pm\) 0.10 & **30.02 \(\pm\) 0.09** \\ \hline \hline \end{tabular}
\end{table}
Table 11: CIFAR100: Accuracy of different methods with 8/255 noise magnitude using WideResNet-34 under \(L_{\infty}\) threat model. The results are averaged over 3 random seeds and reported with the standard deviation.

\begin{table}
\begin{tabular}{c|c c c|c} \hline \hline \multicolumn{1}{c|}{\multirow{2}{*}{method}} & RS-FGSM & N-FGSM & RS-AAER & N-AAER \\ \hline \hline natural accuracy (\%) & \(27.10\pm 11.44\) & \(38.87\pm 0.17\) & \(32.28\pm 1.52\) & \(39.52\pm 0.42\) \\ \hline robust accuracy (\%) & \(0.00\pm 0.00\) & \(20.71\pm 0.74\) & \(14.22\pm 0.96\) & \(\mathbf{20.90\pm 0.34}\) \\ \hline \hline \end{tabular}
\end{table}
Table 15: ImageNet-100: Accuracy of different methods with 8/255 noise magnitude using PreActResNet-18 under \(L_{\infty}\) threat model. The results are averaged over 3 random seeds and reported with the standard deviation.

\begin{table}
\begin{tabular}{c|c c c|c} \hline \hline \multicolumn{1}{c|}{\multirow{2}{*}{method}} & RS-FGSM & N-FGSM & RS-AAER & N-AAER \\ \hline \hline natural accuracy (\%) & \(27.10\pm 11.44\) & \(38.87\pm 0.17\) & \(32.28\pm 1.52\) & \(39.52\pm 0.42\) \\ \hline robust accuracy (\%) & \(0.00\pm 0.00\) & \(20.71\pm 0.74\) & \(14.22\pm 0.96\) & \(\mathbf{20.90\pm 0.34}\) \\ \hline \hline \end{tabular}
\end{table}
Table 13: SVHN: Accuracy of different methods and different noise magnitudes using PreActResNet-18 under \(L_{\infty}\) threat model. The baseline results are taken from [6]. The top number is the natural accuracy (%), while the bottom number is the PGD-50-10 accuracy (%). The results are averaged over 3 random seeds and reported with the standard deviation.

\begin{table}
\begin{tabular}{c|c c c|c} \hline \hline \multicolumn{1}{c|}{\multirow{2}{*}{method}} & RS-FGSM & N-FGSM & RS-AAER & N-AAER \\ \hline \hline natural accuracy (\%) & \(27.10\pm 11.44\) & \(38.87\pm 0.17\) & \(32.28\pm 1.52\) & \(39.52\pm 0.42\) \\ \hline robust accuracy (\%) & \(0.00\pm 0.00\) & \(20.71\pm 0.74\) & \(14.22\pm 0.96\) & \(\mathbf{20.90\pm 0.34}\) \\ \hline \hline \end{tabular}
\end{table}
Table 15: ImageNet-100: Accuracy of different methods with 8/255 noise magnitude using PreActResNet-18 under \(L_{\infty}\) threat model. The results are averaged over 3 random seeds and reported with the standard deviation.

ImageNet-100 Settings and Results.We have also extended our method to a large-sized dataset, ImageNet-100, to demonstrate its effectiveness. We utilized the cyclical learning rate schedule with 30 epochs, reaching the maximum learning rate of 0.2 at the midpoint of 15 epochs. For RS-AAER, we set \(\lambda_{2}=3.0\) and \(\lambda_{3}=2.5\), while for N-AAER, we set \(\lambda_{2}=1.25\) and \(\lambda_{3}=0.25\). The value of \(\lambda_{1}\) remained fixed at 1.0 in all settings. From Table 15, it is evident that our method effectively prevents CO and enhances robust accuracy in this large-scale dataset. It needs to be highlighted that the results in the above table may not be optimal, which can be further improved by increasing training epochs or adjusting hyperparameters and learning rate. However, they do establish the effectiveness of our method in trustworthy eliminating CO on a larger-scale dataset. The above outcomes underscore the scalability and effectiveness of AAER in fortifying the robustness of models trained.

## Appendix F More Competing Baselines

We also compare the performance of our method with other SSAT methods, including GAT [33], NuAT [34], PGI-FGSM [17], SDI-FGSM [18] and Kim [21]. The results for GAT, NuAT and Kim are directly taken from [6], while the reported PGI-FGSM and SDI-FGSM results are based on the official code after searching the hyperparameters across different noise magnitudes.

In Table 16, we can observe that GAT, NuAT, PGI-FGSM and SDI-FGSM demonstrate superior performance under 8/255 noise magnitude. However, it becomes apparent that these methods still suffer from CO with strong adversaries, resulting in nearly zero robustness under 16/255 noise magnitude, let alone the more challenging 32/255. In contrast, our proposed method exhibits consistent effects across different settings with negligible computational overhead, demonstrating its trustworthy effectiveness in preventing CO. We would like to emphasize that while achieving excellent performance under 8/255 noise magnitude is certainly gratifying, but can reliable defence against CO is more critical to a successful SSAT method.

## Appendix G Long Training Schedule

We also compare the performance of our method using the standard robust overfitting training schedule. This training schedule consists of 200 epochs with an initial learning rate of 0.1. The learning rate is divided by 10 at the 100th and 150th epochs, respectively. During the long training

\begin{table}
\begin{tabular}{c|c c c} \hline \hline noise magnitude & 8/255 & 12/255 & 16/255 \\ \hline \hline \multirow{2}{*}{GAT} & \(76.75\pm 0.38\) & \(80.44\pm 5.08\) & \(82.17\pm 2.47\) \\  & \(50.98\pm 0.12\) & \(14.93\pm 9.26\) & \(1.25\pm 0.51\) \\ \hline \multirow{2}{*}{NuAT} & \(73.22\pm 0.34\) & \(74.38\pm 7.32\) & \(80.1\pm 1.08\) \\  & \(50.10\pm 0.33\) & \(17.54\pm 8.82\) & \(3.29\pm 0.87\) \\ \hline \multirow{2}{*}{PGI-FGSM} & \(77.94\pm 0.26\) & \(83.82\pm 0.86\) & \(83.42\pm 0.24\) \\  & \(\mathbf{52.86\pm 0.34}\) & \(5.19\pm 0.59\) & \(4.16\pm 0.31\) \\ \hline \multirow{2}{*}{SDI-FGSM} & \(79.28\pm 0.08\) & \(70.07\pm 0.84\) & \(81.09\pm 0.13\) \\  & \(49.26\pm 0.10\) & \(\mathbf{36.56\pm 1.34}\) & \(0.05\pm 0.01\) \\ \hline \multirow{2}{*}{Kim} & \(89.02\pm 0.10\) & \(88.35\pm 0.31\) & \(90.45\pm 0.08\) \\  & \(33.01\pm 0.09\) & \(13.11\pm 0.63\) & \(1.88\pm 0.05\) \\ \hline \multirow{2}{*}{RS-AAER} & \(83.83\pm 0.27\) & \(74.40\pm 0.79\) & \(64.56\pm 1.45\) \\  & \(46.14\pm 0.02\) & \(32.17\pm 0.16\) & \(23.87\pm 0.36\) \\ \hline \multirow{2}{*}{N-AAER} & \(80.56\pm 0.35\) & \(71.15\pm 0.18\) & \(61.84\pm 0.43\) \\  & \(48.31\pm 0.23\) & \(\mathbf{36.52\pm 0.10}\) & \(\mathbf{28.20\pm 0.71}\) \\ \hline \hline \end{tabular}
\end{table}
Table 16: CIFAR10: Accuracy of different methods and different noise magnitudes using PreActResNet-18 under \(L_{\infty}\) threat model. GAT, NuAT and Kim results are taken from [6]. The top number is the natural accuracy (%), while the bottom number is the PGD-50-10 accuracy (%). The results are averaged over 3 random seeds and reported with the standard deviation.

schedule, we employ a warm-up strategy in the first 20 epochs, which uniformly rises the strength of AAER from 0% to 100%.

In Table 17, we observe that our method, AAER, consistently improves adversarial robustness under the long training schedule, underscoring its consistently reliable and effective performance in preventing CO.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline method & RS-FGSM & N-FGSM & RS-AAER & N-AAER \\ \hline \hline natural accuracy (\%) & \(91.21\pm 0.26\) & \(83.25\pm 0.04\) & \(85.69\pm 0.20\) & \(83.23\pm 0.25\) \\ \hline robust accuracy (\%) & \(0.00\pm 0.00\) & \(36.98\pm 0.34\) & \(36.05\pm 0.17\) & \(\bm{37.38\pm 0.16}\) \\ \hline \hline \end{tabular}
\end{table}
Table 17: CIFAR10: Accuracy of long training schedule with 8/255 noise magnitude using PreActResNet-18 under \(L_{\infty}\) threat model. The results are averaged over 3 random seeds and reported with the standard deviation.