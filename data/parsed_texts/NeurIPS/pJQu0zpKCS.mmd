# Optimal Exploration for Model-Based RL

in Nonlinear Systems

 Andrew Wagenmaker

Paul G. Allen School of Computer Science & Engineering

University of Washington

Seattle, WA 98195

ajwagen@cs.washington.edu

&Guanya Shi

Robotics Institute

Carnegie Mellon University

Pittsburgh, PA 15213

&Kevin Jamieson

Paul G. Allen School of Computer Science & Engineering

University of Washington

Seattle, WA 98195

###### Abstract

Learning to control unknown nonlinear dynamical systems is a fundamental problem in reinforcement learning and control theory. A commonly applied approach is to first explore the environment (exploration), learn an accurate model of it (system identification), and then compute an optimal controller with the minimum cost on this estimated system (policy optimization). While existing work has shown that it is possible to learn a uniformly good model of the system [1], in practice, if we aim to learn a good controller with a low cost on the actual system, certain system parameters may be significantly more critical than others, and we therefore ought to focus our exploration on learning such parameters.

In this work, we consider the setting of nonlinear dynamical systems and seek to formally quantify, in such settings, (a) which parameters are most relevant to learning a good controller, and (b) how we can best explore so as to minimize uncertainty in such parameters. Inspired by recent work in linear systems [2], we show that minimizing the controller loss in nonlinear systems translates to estimating the system parameters in a particular, task-dependent metric. Motivated by this, we develop an algorithm able to efficiently explore the system to reduce uncertainty in this metric, and prove a lower bound showing that our approach learns a controller at a near-instance-optimal rate. Our algorithm relies on a general reduction from policy optimization to optimal experiment design in arbitrary systems, and may be of independent interest. We conclude with experiments demonstrating the effectiveness of our method in realistic nonlinear robotic systems1.

Footnote 1: Code: https://github.com/ajwagen/nonlinear_sysid_for_control

## 1 Introduction

Controlling nonlinear dynamical systems is a core problem in robotics, cyber-physical systems, and beyond, and a significant body of work in both the control theory and reinforcement learning communities has sought to address this challenge [3, 4, 5]. In many real-world scenarios [6, 7, 8, 9], the dynamics of the system of interest is unknown, or only a coarse model of them is available, whichsignificantly increases the challenge of control--not only must we control such systems, we must _learn_ to control them. While a variety of methods exist to address this challenge, a commonly applied approach is to first perform _system identification_, learning an accurate model of the system's dynamics, and then use this model to obtain a controller. Despite its promising potential, there are still several fundamental questions that must be answered to make this approach practically effective.

_Which parameters are most relevant to learning a good controller?_ Beyond some special cases, little work has been done characterizing how the estimation error from system identification translates to end-to-end suboptimality in the resulting controller of our nonlinear systems. In particular, certain parameters of the system or regions of the state space may be irrelevant to learning a good controller, and coarse estimates of these parameters would suffice, while other parameters may be critical to learning a good controller, and we must therefore estimate these parameters very accurately in order to effectively control the system. In the context of this work, where nonlinearities are considered, the heterogeneity of the parameters is further accentuated. For instance, around a point of equilibrium, some system parameters might be completely inactive, having no impact on the dynamics (see the example in Section 1.1 for an illustration of this).

_How can we best explore so as to minimize uncertainty in relevant parameters?_ Even if we are able to determine which parameters are most important for obtaining a good controller on the true system, it is not obvious how to use this information. How can we direct our system identification phase in order to focus on learning these parameters as quickly as possible, without spending time estimating the parameters of the system less critical for control? This is fundamentally a question of _exploration_. While it is known in linear systems that random excitation will efficiently explore [10], exploration in nonlinear systems is significantly more challenging since, in order to excite all parameters of interest, non-trivial planning may be required to ensure all relevant states are reached (as is the case in the example considered in Section 1.1).

We address both these questions in a particular class of nonlinear systems parameterized as:

\[\bm{x}_{h+1}=A_{\star}\bm{\phi}(\bm{x}_{h},\bm{u}_{h})+\bm{w}_{h}.\] (1.1)

Here \(\bm{x}_{h}\in\mathbb{R}^{d_{\bm{x}}}\) denotes the state of the system, \(\bm{u}_{h}\in\mathcal{U}\subseteq\mathbb{R}^{d_{\bm{u}}}\) the input, \(\bm{w}_{h}\sim\mathcal{N}(0,\sigma_{\bm{w}}^{2}\cdot I)\) random noise, \(\bm{\phi}(\cdot,\cdot)\in\mathbb{R}^{d_{\bm{\phi}}}\) a (possibly nonlinear, known) feature map, and \(A_{\star}\in\mathbb{R}^{d_{\bm{w}}\times d_{\bm{\phi}}}\) the (unknown) system parameter. Systems of this form are able to model a variety of real-world settings [11, 12, 13, 14, 15]2, and have been the subject of recent attention in the reinforcement learning community [1, 17, 14], yet the aforementioned questions have remained unanswered. Towards addressing this, in this work we make the following contributions:

Footnote 2: In real-world settings, \(\bm{\phi}\) is typically (1) from physics (i.e., the system structure is known but some parameters such as drag coefficient are unknown [3]), (2) learned using representation learning or meta-learning [12, 15], and/or (3) from random features (e.g., any sufficiently regular, smooth nonlinear system \(\bm{f}(\bm{x},\bm{u})\) can be modeled by (1.1) using \(N\) random features up to a \(1/\sqrt{N}\) error [16]).

1. For systems of the form (1.1), given some cost of interest which we wish to find a controller to minimize, we (a) formally characterize how estimation error translates into suboptimality in the learned controller, under the _certainty equivalent_ control rule and (b) provide a lower bound on the loss of _any_ (sufficiently regular) control rule learned from \(T\) rounds of interaction with (1.1).
2. Motivated by this characterization, we present an algorithm which achieves the _instance-optimal_ rate, with controller loss matching our lower bound. To the best of our knowledge, this is the first statistically optimal algorithm in the setting of nonlinear dynamical systems. Our algorithm relies on a generic reduction from policy optimization to optimal exploration in _arbitrary_ dynamical systems (not necessarily of the form (1.1)), which may be of independent interest.
3. We present numerical experiments on several realistic nonlinear systems which illustrate that our approach--efficiently exploring to reduce uncertainty in parameters most relevant to learning a controller--yields significant gains in practice.

To further motivate our approach, we consider the following example.

### Motivating Example

To motivate the need for effective exploration, we consider a simple 1-D system with nonlinear dynamics given by:

\[\bm{x}_{h+1}=a_{1}\bm{x}_{h}+a_{2}\bm{u}_{h}+\sum_{i=1}^{10}a_{i+2}\bm{\phi}_ {i}(\bm{x}_{h})+\bm{w}_{h}\]where \(\bm{\phi}_{i}(\bm{x})=\max\{1-100(\bm{x}-c_{i})^{2},0\}\) for some \(c_{i}\). We choose \(a_{1}=0.8,a_{2}=1\), and \(a_{3}=\ldots=a_{12}=-3\). We assume \(a_{1:12}\) are unknown, \((\bm{\phi}_{i})_{i=1}^{10}\) is known, and set

\[\mathrm{cost}(\bm{x},\bm{u})=(\bm{x}-c_{1})^{2}+100^{-1}\cdot\bm{u}^{2}.\]

With this choice of cost, the optimal controller will attempt to direct the state \(\bm{x}\) to the equilibrium point \(c_{1}\) and maintain this position. Note that, with our choice of \(\bm{\phi}_{i}\), \(\bm{\phi}_{i}(\bm{x})\neq 0\) only when \(\bm{x}\) is very close to \(c_{i}\). This renders the parameters \(a_{4:12}\) irrelevant to learning the optimal controller, since \(\bm{\phi}_{2},\ldots,\bm{\phi}_{10}\) will be inactive if we are playing optimally, but learning \(a_{1:3}\) is critical to performing optimally. In particular, the coefficient of the first nonlinearity, \(a_{3}\), must be learned, as its value significantly changes the dynamics at the goal state.

We illustrate the result of running on this system in Figure 1, comparing our proposed approach (Task-Driven Exploration, Algorithm 1) to the approach which chooses \(\bm{u}_{h}\sim\mathcal{N}(0,\sigma_{\bm{u}}^{2})\) (Random Exploration), and the approach proposed in [1] (Uniform Exploration) which seeks to explore so as to estimate \(a_{1:12}\) uniformly well. As can be seen, neither of these latter two approaches are able to learn a good controller, while our approach easily finds a near-optimal controller. The failure modes of each of these approaches is somewhat different. Here Random Exploration fails since the chance of reaching the point \(\bm{x}_{h}\sim c_{1}\) is extremely small if the input is random noise--reaching \(c_{1}\) requires playing a particular sequence of actions which are very unlikely to be played if \(\bm{u}_{h}\) is chosen randomly. The Uniform Exploration approach does, in contrast, plan and, given enough time, is guaranteed to estimate all parameters accurately. However, as it aims to estimate all parameters uniformly well, it will attempt to estimate \(a_{4:12}\) accurately despite their irrelevance to control, which will slow down the rate at which it is able to estimate \(a_{3}\). Only our approach, which both plans and takes into account the cost while exploring, is able to reach \(a_{3}\) enough times to efficiently estimate it, and learn a good controller.

This example illustrates that it is critical both to explore efficiently, and also to let the objective--learning a good controller--guide this exploration. We emphasize that the behavior in this example is only exhibited in nonlinear systems--though taking into account the task while exploring in linear systems is known to yield provable improvements [2], even playing random noise allows every direction to be learned in such systems. In nonlinear systems, however, this is not the case--one may fail to learn completely unless careful planning is performed.

## 2 Related Work

Learning for control.Recently, there has been increased interest in studying control problems from a learning-theoretic perspective, largely for linear system settings such as online LQR or LQG with unknown dynamics [18; 19; 20; 21; 22; 23; 24; 25; 26]. In the nonlinear setting, [27; 28; 29] provide formal guarantees on system identification in several different classes of nonlinear systems, yet they only consider noiseless systems, or systems that are significantly easier to excite than (1.1). [17] study systems of the form (1.1), but consider only the regret minimization problem. While their bounds would yield a polynomial complexity via an online-to-batch conversion, the resulting guarantee would scale at a \(\mathcal{O}(1/\sqrt{T})\) rate, significantly slower than our \(\mathcal{O}(1/T)\) rate. Several additional works in reinforcement with general function approximation encompass nonlinear systems of the form (1.1) [30; 31; 32], yet these results also achieve a slow \(\mathcal{O}(1/\sqrt{T})\) rate. The most relevant work [1] proposes an active learning approach to identify unknown parameters in (1.1), with the goal of minimizing the Euclidean distance in the parameter space. However, they do not provide end-to-end guarantees on learning controllers and, as shown in Section 1.1, this approach could be significantly worse than learning a model with the goal task in mind. Also very related to our work is [2], which seeks to answer a similar set of questions: performing system identification in order to learn a good controller. This work is restricted to the setting of linear dynamics, however, and does not address the additional complexities of nonlinear systems.

System identification, dual control, and iterative learning control.There is a large body of classical work in system identification [7], and our work can be seen as an instance of _active_ system

Figure 1: Performance on Motivating Example

identification. While a variety of approaches have been proposed for active system identification [33; 34; 35; 36; 37; 38; 39; 40], these tend to only consider linear systems, or lack rigorous theoretical guarantees. Recently deep learning approaches have also been applied in system identification [6; 8; 9; 41; 42]. In these works, the system identification phase is separate from the downstream controller design. Instead, in the control community, estimating parameters while simultaneously optimizing for performance has been formulated as a dual or iterative learning control problem [43; 44; 45], yet these settings focus on stability or asymptotic convergence whereas our work quantifies the end-to-end suboptimality gap.

**Model-based reinforcement learning.** This paper falls into the broad category of model-based reinforcement learning (MBRL), where an agent explores the environment to learn a model and then computes an optimal policy using the learned model. On the empirical side, deep MBRL has made exciting progress in many domains [46; 47; 48], and several task-aware methods have been designed to improve MBRL's performance [47; 48; 49], yet these works lack formal guarantees. On the theoretical side, a variety of different model-based approaches exist [50; 51; 52; 53; 14]; however, the majority of these consider restricted settings such as tabular or linear MDPs. Of particular interest is the work of [14] which presents a result in systems of the form (1.1). While they show that polynomial sample complexity is possible, their guarantee scales at a \(\mathcal{O}(1/T^{1/6})\) rate compared to our much faster \(\mathcal{O}(1/T)\) rate.

**Adaptive nonlinear control.** Adaptive nonlinear control also seeks to control an unknown nonlinear system with parametric uncertainties [3; 4]. In particular, the key idea of model-reference adaptive control (MRAC) bears affinity to this paper, in that the adaptation law in MRAC adapts unknown parameters in a task-aware manner. There are two main differences between MRAC and our work. First, adaptive control does not explicitly optimize a cost function--the objective is typically tracking error convergence and Lyapunov stability, whereas our framework allows general cost functions--and the focus is typically on asymptotic convergence, while we give non-asymptotic optimality guarantees. Second, adaptive control has by and large been limited to specific system classes (e.g., fully-actuated systems [4; 15]) and policy classes (e.g., policy to directly cancel out the matched uncertainty [12; 13]), whereas our framework allows more general systems and policy classes.

## 3 Preliminaries

**Notation.**\(\|\cdot\|_{\mathrm{op}}\) denotes the operator norm (matrix 2-norm), \(\|\cdot\|_{\mathrm{F}}\) the Frobenius norm, and \(\|\cdot\|_{M}\) the Mahalanobis norm, defined as \(\|\bm{x}\|_{M}:=\sqrt{\bm{x}^{\top}M\bm{x}}\) for \(M\succeq 0\). \(\mathrm{vec}(A)\) denotes the vectorization of matrix \(A\). \(\mathcal{B}_{p}(A;r):=\{A^{\prime}:\|A-A^{\prime}\|_{p}\leq r\}\). \([H]=\{1,2,\ldots,H\}\). We let \(\mathbb{E}_{A}[\cdot]\) denote the expectation over trajectories induced on system with parameter \(A\), and \(\mathbb{E}_{A,\pi}[\cdot]\) the expectation induced when policy \(\pi\) is played. \(\mathrm{poly}(\cdot)\) denotes some term that is polynomial in its arguments, with exponents absolute constants. We use \(\lesssim\) informally to highlight key parameters in an inequality.

**Setting.** In this work, we are interested in systems of the form (1.1). We consider the episodic setting, where episodes are of length \(H\), and assume that each episodes starts from a given state \(\bm{x}_{1}\). We also assume \(\|A_{\star}\|_{\mathrm{op}}\leq B_{A}\) for some known \(B_{A}>0\). We note that the setting considered here encompasses many real-world systems of interest in robotics and control (e.g., [12; 14; 15; 11] and Section 6).

The goal of the learner is to find a policy (controller) \(\pi=(\pi_{h})_{h=1}^{H}\) which achieves minimal cost on (1.1), for the cost defined by some (known) function \((\mathrm{cost}_{h}(\cdot,\cdot))_{h=1}^{H}\), with \(\mathrm{cost}_{h}:\mathbb{R}^{d_{\bm{x}}}\times\mathcal{U}\to\mathbb{R}_{+}\). For a given policy \(\pi\), we define the expected cost on system \(A\) as

\[\mathcal{J}(\pi;A):=\mathbb{E}_{A,\pi}\left[\sum_{h=1}^{H}\mathrm{cost}_{h}( \bm{x}_{h},\bm{u}_{h})\right].\]

We consider the following interaction protocol:

1. Learner interacts with (1.1) for \(T\) episodes, at each episode playing a policy \(\pi_{\mathrm{exp}}\in\Pi_{\mathrm{exp}}\).
2. After \(T\) episodes, the learner proposes a policy \(\widehat{\pi}_{T}\in\Pi^{\star}\).
3. The learner suffers cost \(\mathcal{J}(\widehat{\pi}_{T};A_{\star})\).

The goal of the learner is therefore first to explore and, after \(T\) episodes of exploration, to propose its best guess at the optimal controller for (1.1), \(\widehat{\pi}_{T}\). Here we take \(\Pi_{\mathrm{exp}}\) to be a (known) set of admissible exploration policies (for example, policies with bounded input power), and \(\Pi^{\star}\) a (known) set of admissible control policies. We assume that policies in \(\Pi^{\star}\) are deterministic, but allow for randomized policies in \(\Pi_{\mathrm{exp}}\). Policies may be either open- or closed-loop.

System Notation.We let \(\mathcal{T}\) denote the space of all possible state trajectories, \(\mathcal{T}\subseteq\mathbb{R}^{d_{\bm{x}}\times(H+1)}\), and, for any \(\bm{\tau}\in\mathcal{T}\), let \(\bm{\tau}_{1:h}\) denote the first \(h\) states and inputs in \(\bm{\tau}\). For any policy \(\pi\), we denote

\[\bm{\Lambda}_{A,\pi}:=\mathbb{E}_{A,\pi}\left[\sum_{h=1}^{H}\bm{\phi}(\bm{x}_{h },\bm{u}_{h})\bm{\phi}(\bm{x}_{h},\bm{u}_{h})^{\top}\right]\]

the expected covariance induced by playing \(\pi\) on system \(A\), \(\bm{\Lambda}_{\pi}:=\bm{\Lambda}_{A_{*},\pi}\), and \(\check{\bm{\Lambda}}:=I_{d_{\bm{x}}}\otimes\bm{\Lambda}\) the Kronecker product of \(I_{d_{\bm{x}}}\) and \(\bm{\Lambda}\). Finally, we let \(\bm{\Omega}\) denote the convex hull of covariance matrices induced by \(\Pi_{\mathrm{exp}}\), \(\bm{\Omega}:=\{\mathbb{E}_{\pi\sim\omega}[\bm{\Lambda}_{\pi}]\ :\ \omega\in\triangle_{\Pi_{\mathrm{exp}}}\}\), for \(\triangle_{\Pi_{\mathrm{exp}}}\) the set of distributions over \(\Pi_{\mathrm{exp}}\).

### Regularity Assumptions

In order to make learning in (1.1) tractable, we need several regularity assumptions.

**Assumption 1** (Bounded Features).: _For all \(\bm{x}\in\mathbb{R}^{d_{\bm{x}}}\) and \(\bm{u}\in\mathcal{U}\), we have \(\|\bm{\phi}(\bm{x},\bm{u})\|_{2}\leq B_{\bm{\phi}}\)._

**Assumption 2** (Bounded Cost).: _There exists some \(r_{\mathrm{cost}}(A_{\star})>0\) such that, for all \(A\in\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\mathrm{cost}}(A_{\star}))\) and all \(\pi\in\Pi^{\star}\), we have \(\mathbb{E}_{A,\pi}[(\sum_{h=1}^{H}\mathrm{cost}_{h}(\bm{x}_{h},\bm{u}_{h}))^{2 }]\leq L_{\mathrm{cost}}\)._

**Assumption 3** (Uniform Feature Excitation).: _There exists \(\omega\in\triangle_{\Pi_{\mathrm{exp}}}\) such that \(\lambda_{\min}(\mathbb{E}_{\pi_{\mathrm{exp}}\sim\omega}[\bm{\Lambda}_{\pi_{ \mathrm{exp}}}])\geq\lambda_{\min}^{\star}\) for some \(\lambda_{\min}^{\star}>0\)._

We remark that these assumptions have appeared before in work on systems of the form (1.1) [1, 17]. In particular, Assumption 3 implies that every direction of \(A_{\star}\) can, in principle, be excited, allowing it to be learned. In order to precisely quantify the optimal rates of learning, we require that our system satisfy certain smoothness assumptions. First, we require that \(\bm{\phi}(\cdot,\cdot)\) is differentiable in its second argument.

**Assumption 4** (Smooth Nonlinearity).: _For all \(\bm{x}\in\mathbb{R}^{d_{\bm{x}}}\) and \(\bm{u}\in\mathcal{U}\), \(\bm{\phi}(\bm{x},\bm{u})\) is four-times differentiable in \(\bm{u}\). Furthermore, \(\|\nabla_{\bm{u}}^{(i)}\bm{\phi}(\bm{x},\bm{u})\|_{\mathrm{op}}\leq L_{\bm{ \phi}}\), \(\forall i\in\{1,2,3,4\}\), \(\bm{x}\in\mathbb{R}^{d_{\bm{x}}}\), and \(\bm{u}\in\mathcal{U}\)._

We also require that the class of admissible control policies, \(\Pi^{\star}\), has a parametric form, \(\Pi^{\star}=\{\pi^{\bm{\theta}}\ :\ \bm{\theta}\in\mathbb{R}^{d_{\bm{\theta}}}\}\), and that the parameterization is smooth in the following sense.

**Assumption 5** (Smooth Controller Class).: \(\pi_{h}^{\bm{\theta}}(\bm{\tau}_{1:h})\) _is four-times differentiable in \(\bm{\theta}\) for all \(\bm{\tau}\in\mathcal{T}\) and \(h\in[H]\). Furthermore, \(\|\nabla_{\bm{\theta}}^{(i)}\pi_{h}^{\bm{\theta}}(\bm{\tau}_{1:h})\|_{\mathrm{ op}}\leq L_{\bm{\theta}}\) for \(\forall i\in\{1,2,3,4\}\), \(\bm{\theta}\in\mathbb{R}^{d_{\bm{\theta}}}\), and \(\bm{\tau}\in\mathcal{T}\)._

Assumption 5 is satisfied for commonly considered classes of controllers, such as linear controllers, but is also satisfied by more complex classes such as neural network controllers. While the learner may propose any \(\widehat{\pi}_{T}\in\Pi^{\star}\), we are particularly interested in the _certainty equivalence_ decision rule (i.e., the learner decides \(\widehat{\pi}_{T}\) as if the estimated system is the actual one), defined as:

\[\pi_{\star}(A):=\pi^{\bm{\theta}_{\star}(A)}\quad\text{for}\quad\bm{\theta}_{ \star}(A):=\arg\min_{\bm{\theta}\in\mathbb{R}^{d_{\bm{\theta}}}}\mathcal{J}( \pi^{\bm{\theta}};A).\] (3.1)

To ensure that \(\pi_{\star}(A)\) is well-defined and sufficiently regular, we make the following assumption.

**Assumption 6** (Unique Optimal Controller).: _We assume that the global minimum of \(\mathcal{J}(\pi^{\bm{\theta}};A_{\star})\), \(\bm{\theta}_{\star}(A_{\star})\), is unique, and that \(\nabla_{\bm{\theta}}^{2}\mathcal{J}(\pi^{\bm{\theta}};A_{\star})|_{\bm{\theta} =\bm{\theta}_{\star}(A_{\star})}\succ 0\)._

In general, the policy optimization problem in (3.1) may not be computationally tractable. As we show in Appendix D, the globally optimal decision rule of (3.1) can be replaced with a locally optimal decision rule (i.e. \(\pi_{\star}(A)\) a local minimum of \(\mathcal{J}(\pi;A)\)). Furthermore, Assumption 6 can be replaced by assuming the differentiability of \(\bm{\theta}_{\star}(A)\) with respect to \(A\) for \(A\) near \(A_{\star}\). For ease of exposition, in the main text we assume that Assumption 6 holds and that \(\pi_{\star}(A)\) is defined as in (3.1). With these definitions and under Assumptions 1, 2, 4 and 5, we can show that \(\mathcal{J}(\pi^{\bm{\theta}};A_{\star})\) is differentiable in \(\bm{\theta}\) and, combined with Assumption 6, that \(\bm{\theta}_{\star}(A)\) is differentiable in \(A\), for \(A\in\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\bm{\theta}}(A_{\star}))\) and some \(r_{\bm{\theta}}(A_{\star})>0\). We let \(L_{\pi_{\star}}\) denote an upper bound on the norm of the derivatives of \(\bm{\theta}_{\star}(A)\).

## 4 Optimal Exploration in Nonlinear Systems

In this work, we are interested in characterizing the instance-optimal rates of learning a controller \(\pi\in\Pi^{\star}\) which minimizes the loss \(\mathcal{J}(\pi;A_{\star})\). The following result, a generalization of Proposition 8.2 of [2] to nonlinear systems, is the starting point of our analysis.

**Proposition 1** (Informal).: _Under Assumptions 1, 2 and 4 to 6 and on the system (1.1), we have_

\[\mathcal{J}(\pi_{\star}(\widehat{A});A_{\star})-\mathcal{J}(\pi_{ \star}(A_{\star});A_{\star})=\|\mathrm{vec}(A_{\star}-\widehat{A})\|_{\mathcal{ H}(A_{\star})}^{2}+\mathcal{O}^{\star}(\|A_{\star}-\widehat{A}\|_{\mathrm{F}}^{3})\]

_for_

\[\mathcal{H}(A_{\star}):=\nabla_{A}^{2}\mathcal{J}(\pi_{\star}(A); A_{\star})|_{A=A_{\star}}\]

_and where \(\mathcal{O}^{\star}(\cdot)\) hides factors polynomial in the regularity parameters of Assumptions 1 to 6._

The quantity \(\mathcal{H}(A_{\star}):=\nabla_{A}^{2}\mathcal{J}(\pi_{\star}(A);A_{\star})|_{ A=A_{\star}}\), referred to as the _model-task Hessian_ in [2], corresponds to the _curvature_ of the loss of the certainty-equivalence controller \(\pi_{\star}(A)\) around \(A\gets A_{\star}\). It precisely quantifies how estimation error in each coordinate of \(A_{\star}\) translates into suboptimality of the controller--providing an answer to our question of which parameters are most relevant to learning a good controller--and reduces the problem of minimizing the controller loss to estimating \(A_{\star}\) in a particular norm. The following result gives a bound on this estimation error, \(\|\mathrm{vec}(A_{\star}-\widehat{A})\|_{\mathcal{H}(A_{\star})}^{2}\).

**Proposition 2** (Informal).: _Consider interacting with (1.1) for \(T\) episodes, and let \(\boldsymbol{\Lambda}_{T}=\sum_{t=1}^{T}\sum_{h=1}^{H}\boldsymbol{\phi}( \boldsymbol{x}_{h}^{t},\boldsymbol{u}_{h}^{t})\boldsymbol{\phi}(\boldsymbol{x }_{h}^{t},\boldsymbol{u}_{h}^{t})^{\top}\) denote the observed covariates and_

\[\widehat{A}=\arg\min_{A}\sum_{t=1}^{T}\!\sum_{h=1}^{H}\!\| \boldsymbol{x}_{h+1}^{t}-A\boldsymbol{\phi}(\boldsymbol{x}_{h}^{t}, \boldsymbol{u}_{h}^{t})\|_{2}^{2}\]

_the least-squares estimate of \(A_{\star}\). Recalling that \(\hat{\boldsymbol{\Lambda}}_{T}=I_{d_{\boldsymbol{\infty}}}\otimes \boldsymbol{\Lambda}_{T}\), we have, with high probability:_

\[\|\mathrm{vec}(A_{\star}-\widehat{A})\|_{\mathcal{H}(A_{\star}) }^{2}\lesssim\sigma_{\boldsymbol{w}}^{2}\cdot\mathrm{tr}(\mathcal{H}(A_{\star })\hat{\boldsymbol{\Lambda}}_{T}^{-1}).\]

### Algorithm and Upper Bound

Proposition 2 motivates our algorithmic approach: explore to collect covariates \(\boldsymbol{\Lambda}_{T}\) minimizing \(\mathrm{tr}(\mathcal{H}(A_{\star})\hat{\boldsymbol{\Lambda}}_{T}^{-1})\). There are two primary challenges to achieving this: we do not know \(\mathcal{H}(A_{\star})\), as it depends on the (unknown) parameter \(A_{\star}\) and, even if we did know \(\mathcal{H}(A_{\star})\), it is not clear how to explore so as to collect data minimizing \(\mathrm{tr}(\mathcal{H}(A_{\star})\hat{\boldsymbol{\Lambda}}_{T}^{-1})\). We address both of these challenges in Algorithm 1.

```
1:inputs: episodes \(T\), \((\mathrm{cost}_{h})_{h=1}^{H}\), confidence \(\delta\), control policies \(\Pi^{\star}\), exploration policies \(\Pi_{\mathrm{exp}}\)
2:\(\widehat{A}^{1}\gets 0\), \(\ell_{T}\leftarrow\lceil\log_{2}T/8\rceil\), \(T_{\ell}\gets 2^{\ell}\)
3:for\(\ell=1,2,3,\ldots,\ell_{T}\)do
4: Compute estimate of model-task Hessian: \(\mathcal{H}_{\ell}\leftarrow\mathcal{H}(\widehat{A}^{\ell})\)
5: Run DynamicOED on \(\Phi_{\ell}(\boldsymbol{\Lambda})\leftarrow\mathrm{tr}(\mathcal{H}_{\ell} \cdot\boldsymbol{\Lambda}^{-1})\) to learn exploration policies \(\Pi_{\ell}\subseteq\Pi_{\mathrm{exp}}\)
6: Rerun each policy in \(\Pi_{\ell}\)\(N_{\ell}=\lceil T_{\ell}/\|\Pi_{\ell}\rceil\) times, denote collected data \(\mathfrak{D}_{\ell}\)
7: Estimate \(A_{\star}\): \(\widehat{A}^{\ell+1}=\arg\min_{A}\sum_{h=1}^{H}\sum_{(\boldsymbol{x}_{h+1}, \boldsymbol{u}_{h},\boldsymbol{x}_{h})\in\mathfrak{D}_{\ell}}\|\boldsymbol{x }_{h+1}-A\boldsymbol{\phi}(\boldsymbol{x}_{h},\boldsymbol{u}_{h})\|_{2}^{2}\)
8:return\(\widehat{\pi}_{T}\leftarrow\pi_{\star}(\widehat{A}^{\ell+1})\in\Pi^{\star}\) ```

**Algorithm 1** Optimal Exploration in Nonlinear Systems (informal)

Algorithm 1 proceeds in epochs of exponentially increasing length. At each epoch it first approximates \(\mathcal{H}(A_{\star})\) by computing the model-task Hessian of the estimated system, \(\widehat{A}^{\ell}\). Using this approximatiom of \(\mathcal{H}(A_{\star})\), it seeks to explore to minimize \(\mathrm{tr}(\mathcal{H}(\widehat{A}^{\ell})\hat{\boldsymbol{\Lambda}}_{T}^{-1})\). This exploration routine is encapsulated in the DynamicOED (dynamic optimal experiment design) function, an adaptive experiment-design routine inspired by recent work in reinforcement learning [56] and described in more detail in Section 5. DynamicOED returns a set of exploration policies, \(\Pi_{\ell}\), which we run to collect data \(\mathfrak{D}_{\ell}\). As we will show, the collected covariates, \(\boldsymbol{\Lambda}_{\ell}:=\sum_{h=1}^{H}\sum_{(\boldsymbol{u}_{h}, \boldsymbol{x}_{h})\in\mathfrak{D}_{\ell}}\boldsymbol{\phi}(\boldsymbol{x}_{h}, \boldsymbol{u}_{h})\boldsymbol{\phi}(\boldsymbol{x}_{h},\boldsymbol{u}_{h})^{\top}\), satisfy

\[\mathrm{tr}(\mathcal{H}(\widehat{A}^{\ell})\hat{\boldsymbol{\Lambda}}_{\ell}^{ -1})\lesssim T_{\ell}^{-1}\cdot\min_{\boldsymbol{\Lambda}\in\boldsymbol{ \Omega}}\mathrm{tr}(\mathcal{H}(\widehat{A}^{\ell})\hat{\boldsymbol{\Lambda}}^{ -1}),\]

which implies that DynamicOED collects data minimizing \(\mathrm{tr}(\mathcal{H}(\widehat{A}^{\ell})\hat{\boldsymbol{\Lambda}}_{\ell}^{ -1})\) at a near-optimal rate. Given the data \(\mathfrak{D}_{\ell}\), we form the least-squares estimate of \(A_{\star}\), \(\widehat{A}^{\ell+1}\), and the process repeats. After running for \(T\) episodes, the certainty-equivalence controller on the last estimate obtained, \(\widehat{\pi}_{T}=\pi_{\star}(\widehat{A}^{\ell}x^{+1})\), is returned. The following result bounds the suboptimality of \(\widehat{\pi}_{T}\) as compared to \(\pi_{\star}(A_{\star})\)

**Theorem 1**.: _Under Assumptions 1 to 6, if \(T\geq C_{\mathrm{poly}}\cdot\max\{1,r_{\mathrm{cost}}(A_{\star})^{-2},r_{\bm{ \theta}}(A_{\star})^{-2}\}\), then with probability at least \(1-\delta\), Algorithm 1 explores with policies in \(\Pi_{\mathrm{exp}}\) at every episode, runs for at most \(T\) episodes, and returns \(\widehat{\pi}_{T}\in\Pi^{\star}\) satisfying:_

\[\mathcal{J}(\widehat{\pi}_{T};A_{\star})-\mathcal{J}(\pi_{\star}(A_{\star});A _{\star})\leq\frac{\sigma_{\bm{w}}^{2}}{T}\cdot\min_{\bm{\Lambda}\in\bm{\Omega} }\mathrm{tr}\left(\mathcal{H}(A_{\star})\tilde{\bm{\Lambda}}^{-1}\right)\cdot C \log\frac{6d_{\bm{x}}d_{\bm{\phi}}}{\delta}+\frac{C_{\mathrm{poly}}}{T^{3/2}}\]

_where we recall \(\bm{\Omega}\) is the set of possible expected covariates on (1.1), \(C\) is a universal constant, and_

\[C_{\mathrm{poly}}=\mathrm{poly}(d_{\bm{\phi}},d_{\bm{x}},H,B_{A},B_{\bm{\phi} },L_{\bm{\phi}},L_{\bm{\theta}},L_{\mathrm{cost}},L_{\pi_{\star}},\sigma_{\bm {w}},\sigma_{\bm{w}}^{-1},\tfrac{1}{\lambda_{\min}},\log\tfrac{T}{\delta}).\]

Theorem 1 shows that Algorithm 1 is able to explore so as to optimally minimize the exploration loss \(\mathrm{tr}(\mathcal{H}(A_{\star})\tilde{\bm{\Lambda}}_{T}^{-1})\), up to a lower-order term scaling as \(T^{-3/2}\) and polynomially in system parameters. While Propositions 1 and 2 show that collecting data which minimizes \(\mathrm{tr}(\mathcal{H}(A_{\star})\tilde{\bm{\Lambda}}_{T}^{-1})\) is in some sense fundamental, it is not clear it is necessary. We next show that it is indeed necessary.

### Lower Bounds on Learning Controllers

Our goal is to show that, up to constants and lower-order terms, the bound given in Theorem 1 is not improvable, regardless of which controller estimate we use. To obtain such lower bounds, we need several additional assumptions. In particular, we require that the loss \(\mathcal{J}(\pi^{\bm{\theta}};A)\) grows quadratically in the distance \(\bm{\theta}\) is from \(\bm{\theta}_{\star}(A)\), and strengthen Assumption 3 to ensure (1.1) is sufficiently easy to excite. Formal statements of these conditions are given in Appendix F. Our lower bound is as follows.

**Theorem 2** (Informal).: _Under Assumptions 1 to 6 and the additional regularity assumptions mentioned above, as long as \(T\geq C_{\mathrm{lb}}\), for any \(\omega_{\mathrm{exp}}\in\triangle_{\Pi_{\mathrm{exp}}}\), we have_

\[\min_{\widehat{\pi}}\max_{A\in\mathcal{B}_{T}}\mathbb{E}_{\mathfrak{D}_{T} \sim A,\omega_{\mathrm{exp}}}[\mathcal{J}(\widehat{\pi}(\mathfrak{D}_{T});A) -\mathcal{J}(\pi_{\star}(A);A)]\geq\frac{\sigma_{\bm{w}}^{2}}{3T}\cdot\min_{ \bm{\Lambda}\in\bm{\Omega}}\mathrm{tr}(\mathcal{H}(A_{\star})\tilde{\bm{ \Lambda}}^{-1})-\frac{C_{\mathrm{lb}}}{T^{5/4}}\]

_for \(\mathcal{B}_{T}:=\mathcal{B}_{F}(A_{\star};\mathcal{O}(T^{-5/6}))\), \(\mathbb{E}_{\mathfrak{D}_{T}\sim A,\omega_{\mathrm{exp}}}[\cdot]=\mathbb{E}_{ \pi_{\mathrm{exp}}\sim\omega_{\mathrm{exp}}}[\mathbb{E}_{\mathfrak{D}_{T} \sim A,\pi_{\mathrm{exp}}}[\cdot]]\) the expectation over trajectories generated by running policies \(\pi\sim\omega_{\mathrm{exp}}\) on system \(A\) for \(T\) episodes, \(\widehat{\pi}\) any mapping from observations to policies in \(\Pi^{\star}\), and \(C_{\mathrm{lb}}\) some value scaling polynomially in problem parameters._

Note that this lower bound holds for _any_\(A_{\star}\) and mapping \(\bm{\phi}\), as long as our assumptions are met. Up to constants and lower-order terms, the scaling of Theorem 2 matches that of Theorem 1--both scale with \(\min_{\bm{\Lambda}\in\bm{\Omega}}\mathrm{tr}(\mathcal{H}(A_{\star})\tilde{\bm{ \Lambda}}^{-1})\)--which implies that Algorithm 1 is indeed optimal (under certain additional regularity conditions). To the best of our knowledge, this is the first result characterizing the optimal statistical rates for learning in nonlinear dynamical systems. We emphasize that Theorem 2 holds for _any_ decision rule \(\widehat{\pi}\)--it does not require that we use the certainty equivalence decision rule. As Algorithm 1 does rely on certainty equivalence, this result also implies that the certainty equivalence decision rule is optimal for (certain classes of) nonlinear dynamical systems.

The proof of Theorem 2 builds on the work [2], which shows a similar result for linear dynamical systems. It critically relies on our quadratic decomposition of the controller loss in Proposition 1, which reduces the problem of obtaining a lower bound on controller loss to a lower bound on estimating \(A_{\star}\) in the \(\mathcal{H}(A_{\star})\) norm. Given this, the result can be obtained by applying lower bounds on regression in general norms.

## 5 Optimal Experiment Design in Arbitrary Dynamical Systems

We turn now to the DynamicOED routine, which is the key algorithmic tool we use to prove Theorem 1. DynamicOED is a general reduction from policy optimization to optimal experiment design in arbitrary dynamical systems, and is an extension of a recently proposed approach for experiment design in linear MDPs [56]. This section may be of independent interest.

To illustrate the generality of this reduction, in this section we consider the following system:

\[\bm{x}_{h+1}=f_{h}(\bm{x}_{h},\bm{u}_{h},\bm{w}_{h}),\quad h=1,2,\ldots,H,\] (5.1)

where \(\bm{x}_{h}\in\mathcal{X}\subseteq\mathbb{R}^{d_{\bm{w}}}\) denotes the state, \(\bm{u}_{h}\in\mathcal{U}\subseteq\mathbb{R}^{d_{\bm{w}}}\) the input, and \(\bm{w}_{h}\in\mathbb{R}^{d_{\bm{w}}}\) the noise. We take the dynamics \((f_{h})_{h=1}^{H}\) to be unknown and arbitrary. We assume there is some known featurization ofour system that is of interest, \(\bm{\phi}(\bm{x},\bm{u})\rightarrow\mathbb{R}^{d_{\bm{\phi}}}\), and an experiment design object on this featurization, \(\Phi:\mathbb{R}^{d_{\bm{\phi}}\times d_{\bm{\phi}}}\rightarrow\mathbb{R}\). Our goal is to collect some set of trajectories \(\{\bm{\tau}_{t}\}_{t=1}^{T}\) which minimizes \(\Phi\):

\[\Phi\big{(}\tfrac{1}{TH}\cdot\sum_{t=1}^{T}\sum_{h=1}^{H}\bm{\phi}(\bm{x}_{h}^ {t},\bm{u}_{h}^{t})\bm{\phi}(\bm{x}_{h}^{t},\bm{u}_{h}^{t})^{\top}\big{)}.\]

As an example, if \(\Phi(\bm{\Lambda})=\log\det(\bm{\Lambda})\), this reduces to \(D\)-optimal design, and if \(\Phi(\bm{\Lambda})=\operatorname{tr}(\mathcal{H}\cdot\bm{\Lambda}^{-1})\), the setting considered in Section 4, this reduces to weighted \(A\)-optimal design. As before, we assume we have access to some set of exploration policies \(\Pi_{\exp}\), and define \(\bm{\Lambda}_{\pi}\) and \(\bm{\Omega}\) as in Section 3, but with respect to this new feature map \(\bm{\phi}\) and system (5.1). We also define \(\widehat{\bm{\Omega}}\) to be the space of all possible covariance matrices:

\[\widehat{\bm{\Omega}}:=\big{\{}\sum_{h=1}^{H}\bm{\phi}(\bm{x}_{h},\bm{u}_{h}) \bm{\phi}(\bm{x}_{h},\bm{u}_{h})^{\top}\ :\ \bm{x}_{h}\in\mathcal{X},\bm{u}_{h}\in\mathcal{U}, \forall h\in[H]\big{\}}.\]

To facilitate efficient experiment design in this setting, we will make the following assumption on \(\Phi\).

**Assumption 7** (Regularity of \(\Phi\)).: \(\Phi\) _is regular in the following sense:_

1. \(\Phi\) _is convex, differentiable, and_ \(\beta\)_-smooth in the norm_ \(\|\cdot\|\) _(with dual-norm_ \(\|\cdot\|_{\star}\)_):_ \[\|\nabla_{\bm{\Lambda}}\Phi(\bm{\Lambda})-\nabla_{\bm{\Lambda}^{\prime}}\Phi( \bm{\Lambda}^{\prime})\|_{\star}\leq\beta\cdot\|\bm{\Lambda}-\bm{\Lambda}^{ \prime}\|,\quad\forall\bm{\Lambda},\bm{\Lambda}\in\widehat{\bm{\Omega}}.\]
2. _There exists some_ \(M<\infty\) _satisfying_ \(\sup_{\bm{\Lambda}\in\widehat{\bm{\Omega}}}\sup_{\bm{x}\in\mathcal{X},\bm{u} \in\mathcal{U}}|\bm{\phi}(\bm{x},\bm{u})^{\top}\nabla_{\bm{\Lambda}}\Phi(\bm {\Lambda})\bm{\phi}(\bm{x},\bm{u})|\leq M\)_._

The key algorithmic assumption we make is access to a regret minimization oracle on (5.1).

**Assumption 8** (Regret Minimization Oracle).: _Let \(\operatorname{cost}_{h}(\bm{x},\bm{u})=\bm{\phi}(\bm{x},\bm{u})^{\top}Q_{h}\bm {\phi}(\bm{x},\bm{u})\) for some \(Q_{h}\in\mathbb{R}^{d_{\bm{\phi}}\times d_{\bm{\phi}}}\) such that \(|\sum_{h}\operatorname{cost}_{h}(\bm{x}_{h},\bm{u}_{h})|\leq 1\) for all \(\bm{x}_{h}\in\mathcal{X},\bm{u}_{h}\in\mathcal{U}\). We assume we have access to some learner \(\mathbb{A}_{\mathcal{R}}\) which is able to achieve low regret on costs \(\{\operatorname{cost}_{h}(\cdot,\cdot)\}_{h=1}^{H}\) with respect to policy class \(\Pi_{\exp}\). That is, with probability at least \(1-\delta\):_

\[\sum_{t=1}^{T}\mathbb{E}_{f,\pi_{t}}[\sum_{h=1}^{H}\operatorname{cost}_{h}(\bm {x}_{h}^{t},\bm{u}_{h}^{t})]-T\cdot\min_{\pi\in\Pi_{\exp}}\mathbb{E}_{f,\pi}[ \sum_{h=1}^{H}\operatorname{cost}_{h}(\bm{x}_{h},\bm{u}_{h})]\leq C_{\mathcal{ R}}\cdot\log^{p_{\mathcal{R}}}\tfrac{T}{\delta}\cdot T^{\alpha}\]

_for some \(C_{\mathcal{R}}>0\), \(p_{\mathcal{R}}>0\), and \(\alpha\in(0,1)\), and where \(\pi_{t}\) is the policy \(\mathbb{A}_{\mathcal{R}}\) plays at episode \(t\)._

Note that the regret minimization algorithm satisfying Assumption 8 may be arbitrary. For example, for linear systems, we could apply provably efficient algorithms for the Linear Quadratic Regulator [25, 20]; for nonlinear systems of the form (1.1) we could apply the LC\({}^{3}\) algorithm of [17]; for more general settings of reinforcement learning with function approximation, algorithms such as BiLin-UCB [30] or E2D [31] could be applied. In practice, though they may not formally satisfy the guarantee of Assumption 8, deep RL approaches could be used. We have the following result.

**Theorem 3**.: _Fix \(T>0\) and denote \(R:=\sup_{\bm{\Lambda},\bm{\Lambda}^{\prime}\in\widehat{\bm{\Omega}}}\|\bm{ \Lambda}-\bm{\Lambda}^{\prime}\|\). Under Assumption 7, and assuming we have access to a learner \(\mathbb{A}_{\mathcal{R}}\) satisfying Assumption 8 with \(\alpha=1/2\), DynamicOED runs for \(T\) episodes on (5.1), and with probability at least \(1-\delta\) collects data \(\{(\bm{x}_{h}^{t},\bm{u}_{h}^{t})\}_{h\in[H],t\in[T]}\) satisfying_

\[\Phi\bigg{(}\frac{1}{T}\cdot\sum_{t=1}^{T}\sum_{h=1}^{H}\bm{\phi}_{h}^{t}(\bm{ \phi}_{h}^{t})^{\top}\bigg{)}-\min_{\bm{\Lambda}\in\bm{\Omega}}\Phi(\bm{\Lambda} )\leq\frac{\beta R^{2}\log T+HM(C_{\mathcal{R}}\log^{p_{\mathcal{R}}}\tfrac{2T}{ \delta}+3\log^{1/2}\tfrac{4T}{\delta})}{T^{1/3}}\]

_where \(\bm{\phi}_{h}^{t}:=\bm{\phi}(\bm{x}_{h}^{t},\bm{u}_{h}^{t})\), and we recall \(\bm{\Omega}\) is the set of possible expected covariates on (5.1)._

Theorem 3 shows that, given access only to a regret minimization oracle, it is possible to solve experiment design problems on arbitrary dynamical systems. The requirement that \(\alpha=1/2\) is for expositional purposes only--we generalize this result to arbitrary \(\alpha\) (and more general feature maps) in Appendix C. Under certain conditions, it can be shown that, if the exploration policies DynamicOED runs to collect \(\mathfrak{D}\) are _rerun_, the newly collected data satisfies a similar guarantee as Theorem 3. This lets us run DynamicOED to learn an approximate solution of \(\min_{\bm{\Lambda}\in\bm{\Omega}}\Phi(\bm{\Lambda})\), and then rerun the learned policies as many times as desired to collect additional data approximately minimizing \(\Phi\).

### Overview of DynamicOED Algorithm

DynamicOED is inspired by recent work on experiment design in reinforcement learning [57, 58, 56, 59], and can be seen as an extension of the FWRegret algorithm of [56] to arbitrary systems. Werefer the reader to [56] for a more in-depth discussion of the FWRegret algorithm, and briefly sketch its extension to arbitrary systems here (see Appendix C and Algorithm 4 for precise definitions).

```
1:input: objective \(\Phi\), episodes \(T\), confidence \(\delta\), regret algorithm \(\mathbb{A}_{\mathcal{R}}\), exploration policies \(\Pi_{\exp}\)
2:Set \(K\leftarrow\mathcal{O}(T^{2/3}),N\leftarrow\mathcal{O}(T^{1/3})\), \(\gamma_{n}\leftarrow\frac{1}{n+1}\)
3:// \(\phi_{h}^{k,n}:=\boldsymbol{\phi}(\boldsymbol{x}_{h}^{k,n},\boldsymbol{u}_{h}^ {k,n})\) for \((\boldsymbol{x}_{h}^{k,n},\boldsymbol{u}_{h}^{k,n})\) the state-input at step \(h\) of episode \(k\) of iteration \(n\)
4:Play any \(\pi_{\exp}\in\Pi_{\exp}\) for \(K\) episodes, set \(\boldsymbol{\Lambda}_{0}\leftarrow\frac{1}{K}\sum_{k=1}^{K}\sum_{h=1}^{H} \boldsymbol{\phi}_{h}^{k,0}(\boldsymbol{\phi}_{h}^{k,0})^{\top}\)
5:for\(n=1,2,\ldots,N\)do
6: Compute derivative of \(\Phi(\boldsymbol{\Lambda}_{n-1})\), \(\Xi_{n}\leftarrow\nabla_{\boldsymbol{\Lambda}}\Phi(\boldsymbol{\Lambda})|_{ \boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{n-1}}\)
7:Run \(\mathbb{A}_{\mathcal{R}}\) on cost \(\mathrm{cost}^{n}_{h}(\boldsymbol{x},\boldsymbol{u})\leftarrow\frac{1}{M} \cdot\boldsymbol{\phi}(\boldsymbol{x},\boldsymbol{u})^{\top}(\Xi_{n}) \boldsymbol{\phi}(\boldsymbol{x},\boldsymbol{u})\) for \(K\) episodes
8:\(\boldsymbol{\Lambda}_{n}\leftarrow(1-\gamma_{n})\boldsymbol{\Lambda}_{n-1}+ \frac{\gamma_{n}}{K}\cdot\sum_{k=1}^{K}\sum_{h=1}^{H}\boldsymbol{\phi}_{h}^{k,n}(\boldsymbol{\phi}_{h}^{k,n})^{\top}\)
9:return\(\frac{1}{T}\sum_{n=0}^{N}\sum_{k=1}^{K}\sum_{h=1}^{H}\boldsymbol{\phi}_{h}^{k,n}(\boldsymbol{\phi}_{h}^{k,n})^{\top}\) ```

**Algorithm 2** Dynamic Optimal Experiment Design (DynamicOED, Informal)

Conceptually, DynamicOED runs a variant of conditional gradient descent on the objective \(\Phi(\boldsymbol{\Lambda})\). At each iteration, \(n\), it computes the gradient of the loss at the current iterate, \(\Xi_{n}\leftarrow\nabla_{\boldsymbol{\Lambda}}\Phi(\boldsymbol{\Lambda})|_{ \boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{n-1}}\). To run a standard gradient descent algorithm on this objective, we would simply update \(\boldsymbol{\Lambda}_{n-1}\) by taking a step in the direction \(\Xi_{n}\). However, our objective is to minimize \(\Phi\) over the constraint set, \(\boldsymbol{\Omega}\). Thus, rather than taking a step in the direction \(\Xi_{n}\), we wish to take a step in the direction of steepest descent _within the constraint set_.

The challenge is that the constraint set in our setting, \(\boldsymbol{\Omega}\), is _unknown_, as it depends on the expectation over trajectories induced on the unknown dynamics \((f_{h})_{h=1}^{H}\), and therefore we cannot directly compute this steepest descent direction. The key observation is that the computation of this steepest descent direction is equivalent to solving:

\[\arg\min_{\pi_{\exp}\in\Pi_{\exp}}\mathbb{E}_{f,\pi_{\exp}}[\sum_{h=1}^{H} \boldsymbol{\phi}(\boldsymbol{x}_{h},\boldsymbol{u}_{h})^{\top}(\Xi_{n}) \boldsymbol{\phi}(\boldsymbol{x}_{h},\boldsymbol{u}_{h})].\]

This is simply a policy optimization problem, however, and can be solved approximately by \(\mathbb{A}_{\mathcal{R}}\) under Assumption 8. Thus, in the call to \(\mathbb{A}_{\mathcal{R}}\) on Line 6, we approximate the steepest descent direction, and on Line 7 update \(\boldsymbol{\Lambda}_{n-1}\) in this direction. Convergence of this procedure to the optimal value, \(\min_{\boldsymbol{\Lambda}\in\boldsymbol{\Omega}}\Phi(\boldsymbol{\Lambda})\), can then be shown by the standard analysis of conditional gradient descent. We remark that, under Assumption 7 and Assumption 8, this argument is completely generic and does not require that our system, (5.1), exhibit any additional properties.

### From Theorem 3 to Theorem 1

In Algorithm 1, our goal is to collect covariates, \(\tilde{\boldsymbol{\Lambda}}_{T_{\ell}}^{-1}\), on (1.1) such that \(\mathrm{tr}(\mathcal{H}(\widehat{A}^{\ell})\tilde{\boldsymbol{\Lambda}}_{T_{ \ell}}^{-1})\) is as small as possible. To achieve this, we apply DynamicOED to the dynamics (1.1) and objective \(\Phi_{\ell}(\boldsymbol{\Lambda})=\mathrm{tr}(\mathcal{H}(\widehat{A}^{\ell} )\tilde{\boldsymbol{\Lambda}}^{-1})\), with Assumption 8 instantiated by the LC\({}^{3}\) algorithm of [17]. By the guarantee given in Theorem 3, after running for a number of episodes \(N\) which scales polynomially in problem parameters, DynamicOED will collect covariates \(\boldsymbol{\Lambda}_{N}\) such that \(\Phi_{\ell}(\frac{1}{N}\boldsymbol{\Lambda}_{N})\leq 2\cdot\min_{ \boldsymbol{\Lambda}\in\boldsymbol{\Omega}}\Phi_{\ell}(\boldsymbol{\Lambda})\), which implies \(\mathrm{tr}(\mathcal{H}(\widehat{A}^{\ell})\tilde{\boldsymbol{\Lambda}}_{N} ^{-1})\leq\frac{2}{N}\cdot\min_{\boldsymbol{\Lambda}\in\boldsymbol{\Omega}} \mathrm{tr}(\mathcal{H}(\widehat{A}^{\ell})\tilde{\boldsymbol{\Lambda}}^{-1})\). By rerunning the policies DynamicOED used to collect this \(\boldsymbol{\Lambda}_{N}\) for \(T_{\ell}/N\) additional times, we can ensure \(\mathrm{tr}(\mathcal{H}(\widehat{A}^{\ell})\tilde{\boldsymbol{\Lambda}}_{T_{ \ell}}^{-1})\lesssim\frac{1}{T_{\ell}}\cdot\min_{\boldsymbol{\Lambda}\in \boldsymbol{\Omega}}\mathrm{tr}(\mathcal{H}(\widehat{A}^{\ell})\tilde{ \boldsymbol{\Lambda}}^{-1})\) as desired.

We remark that the LC\({}^{3}\) algorithm requires access to a computation oracle. As the focus of this work is primarily statistical, we leave addressing this computational challenge for future work. Furthermore, as we show in the following section, computationally efficient, sampling-based implementations of our approach are very effective in practice. We remark as well that the objective we ultimately care about minimizing is \(\mathrm{tr}(\mathcal{H}(A_{*})\tilde{\boldsymbol{\Lambda}}^{-1})\). As we show, by including a small amount of uniform exploration, we can ensure that the suboptimality incurred optimizing \(\mathrm{tr}(\mathcal{H}(\widehat{A}^{\ell})\tilde{\boldsymbol{\Lambda}}^{-1})\) instead of \(\mathrm{tr}(\mathcal{H}(A_{*})\tilde{\boldsymbol{\Lambda}}^{-1})\) only contributes to the lower-order terms of the final guarantee in Theorem 1.

## 6 Experimental Results

Finally, we demonstrate the effectiveness of our proposed approach (Algorithm 1, the Task-Driven Exploration method in Figures 1 to 3) on several systems motivated by robotic applications. Wecompare Algorithm 1 with an approach that plays \(\bm{u}_{h}\sim\mathcal{N}(0,\sigma_{\bm{u}}^{2}\cdot I)\) (Gaussian Exploration), and an approach inspired by [1] (Uniform Exploration), which seeks to estimate \(A_{\star}\) uniformly well, playing inputs that reduce \(\|\widehat{A}-A_{\star}\|_{\mathrm{op}}\).

To benchmark the performance of these approaches, we consider an affine system with dynamics corresponding to that of a simplified 3-D drone (i.e., 3-D double integrator with a gravity term), and a nonlinear system with dynamics corresponding to that of a 2-D car. For both systems, we choose \(H=50\), and plot the value of \(\mathcal{J}(\widehat{\pi}_{t};A_{\star})-\mathcal{J}(\pi_{\star}(A_{\star});A_ {\star})\) for \(\widehat{\pi}_{t}\) the certainty-equivalence controller computed on the estimate of the system obtained at time \(t\). For the drone, we let \(\Pi^{\star}\) be the class of linear-affine feedback controllers, and for the car, \(\Pi^{\star}\) is a set of nonlinear controllers with dimension 4. While the optimal controller for the drone can be computed in closed-form, for the car we rely on a sampling-based routine to find an approximately optimal controller. The model-task hessian \(\mathcal{H}(\widehat{A}^{\ell})\) is computed via automatic differentiation. For the exploration policies of Task-Driven Exploration and Uniform Exploration, \(\Pi_{\mathrm{exp}}\), we rely on MPC-style sampling based methods. For all approaches, we require that \(\mathbb{E}_{A,\pi_{\mathrm{exp}}}[\sum_{h=1}^{H}\|\bm{u}_{h}\|_{2}^{2}]\leq \gamma^{2}\) for some \(\gamma^{2}>0\) and all \(\pi_{\mathrm{exp}}\in\Pi_{\mathrm{exp}}\). On all examples, we implement DynamicOED with \(\mathbb{A}_{\mathcal{R}}\) a posterior sampling-inspired version of the LC\({}^{3}\) of [17]. Figures 1 and 3 shows performance averaged over 100 trials, and Figure 2 over 200 trials. Additional experimental details can be found in Appendix G.

As illustrated in Figures 1 to 3, our approach yields a non-trivial gain over existing approaches on all systems. In particular, in Figures 2 and 3 it improves on the sample complexity of existing approaches by roughly a factor of 2--for example, in the drone system, reaching excess controller cost of \(10\) after less than \(20\) episodes, as compared to over \(40\) episodes for existing approaches.

Our implementation is very modular, and any piece (for example, the parameterization of \(\Pi_{\mathrm{exp}}\) and \(\Pi^{\star}\), the policy optimizer, or the exploration routine) can be easily replaced with other procedures. Our results highlight that, even when using, for example, a possibly suboptimal policy optimizer, exploring so as to minimize uncertainty in the model-task hessian yields a non-trivial gain. We expect that this would hold true regardless of the policy optimizer used--the model-task hessian will adapt to the structure of the policy optimizer. Integration of our approach with deep model-based RL approaches is an interesting direction for future work, but we believe the approach will scale to these settings as well.

## 7 Conclusion

In this work, we have characterized the instance-optimal rate of learning controllers in nonlinear dynamical systems. To the best of our knowledge, this is the first work to obtain an optimal sample complexity for learning in nonlinear dynamical systems. Furthermore, our experimental results demonstrate the effectiveness of our proposed algorithm in realistic nonlinear systems. This work opens the door for several interesting directions for future work. First, it is not clear that the assumption on uniform feature excitation, Assumption 3, is necessary. Can this be removed with a more refined analysis (or shown to be necessary)? Second, while in many settings \(\bm{\phi}\) is known or can be effectively represented by random features, in some settings it is helpful to learn it [12]. Can we obtain end-to-end guarantees on learning both \(\bm{\phi}\) and \(A_{\star}\)? Finally, it is of much interest to extend our experimental results to larger-scale systems for real-world deployment.

Figure 2: Performance on Drone Figure 3: Performance on Car

## Acknowledgements

AW would like to thank Kevin Tully for helpful discussions. The work of AW is supported by NSF HDR 62-0221. The work of KJ is supported in part by NSF TRIPODS 2023166 and CIF 2007036.

## References

* Mania et al. [2022] Horia Mania, Michael I Jordan, and Benjamin Recht. Active learning for nonlinear system identification with guarantees. _J. Mach. Learn. Res._, 23:32-1, 2022.
* Wagenmaker et al. [2021] Andrew J Wagenmaker, Max Simchowitz, and Kevin Jamieson. Task-optimal exploration in linear dynamical systems. In _International Conference on Machine Learning_, pages 10641-10652. PMLR, 2021.
* Slotine et al. [1991] Jean-Jacques E Slotine, Weiping Li, et al. _Applied nonlinear control_, volume 199. Prentice hall Englewood Cliffs, NJ, 1991.
* Astrom and Wittenmark [2013] Karl J Astrom and Bjorn Wittenmark. _Adaptive control_. Courier Corporation, 2013.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Shi et al. [2019] Guanya Shi, Xichen Shi, Michael O'Connell, Rose Yu, Kamyar Azizzadenesheli, Animashree Anandkumar, Yisong Yue, and Soon-Jo Chung. Neural lander: Stable drone landing control using learned dynamics. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 9784-9790. IEEE, 2019.
* Ljung [1998] Lennart Ljung. _System identification_. Springer, 1998.
* Nguyen-Tuong and Peters [2011] Duy Nguyen-Tuong and Jan Peters. Model learning for robot control: a survey. _Cognitive processing_, 12:319-340, 2011.
* Brunke et al. [2022] Lukas Brunke, Melissa Greeff, Adam W Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati, and Angela P Schoellig. Safe learning in robotics: From learning-based control to safe reinforcement learning. _Annual Review of Control, Robotics, and Autonomous Systems_, 5:411-444, 2022.
* Simchowitz et al. [2018] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. _arXiv preprint arXiv:1802.08334_, 2018.
* Shi et al. [2021] Guanya Shi, Kamyar Azizzadenesheli, Michael O'Connell, Soon-Jo Chung, and Yisong Yue. Meta-adaptive nonlinear control: Theory and algorithms. _Advances in Neural Information Processing Systems_, 34:10013-10025, 2021.
* O'Connell et al. [2022] Michael O'Connell, Guanya Shi, Xichen Shi, Kamyar Azizzadenesheli, Anima Anandkumar, Yisong Yue, and Soon-Jo Chung. Neural-fly enables rapid learning for agile flight in strong winds. _Science Robotics_, 7(66):eabm6597, 2022.
* Boffi et al. [2021] Nicholas M Boffi, Stephen Tu, and Jean-Jacques E Slotine. Regret bounds for adaptive nonlinear control. In _Learning for Dynamics and Control_, pages 471-483. PMLR, 2021.
* Song and Sun [2021] Yuda Song and Wen Sun. Pc-mlp: Model-based reinforcement learning with policy cover guided exploration. In _International Conference on Machine Learning_, pages 9801-9811. PMLR, 2021.
* Richards et al. [2021] Spencer M Richards, Navid Azizan, Jean-Jacques Slotine, and Marco Pavone. Adaptive-control-oriented meta-learning for nonlinear systems. _arXiv preprint arXiv:2103.04490_, 2021.
* Rahimi and Recht [2008] Ali Rahimi and Benjamin Recht. Uniform approximation of functions with random bases. In _2008 46th annual allerton conference on communication, control, and computing_, pages 555-561. IEEE, 2008.
* Kakade et al. [2020] Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Information theoretic regret bounds for online nonlinear control. _Advances in Neural Information Processing Systems_, 33:15312-15325, 2020.

* [18] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24:2312-2320, 2011.
* [19] Max Simchowitz, Ross Boczar, and Benjamin Recht. Learning linear dynamical systems with semi-parametric least squares. _arXiv preprint arXiv:1902.00768_, 2019.
* [20] Horia Mania, Stephen Tu, and Benjamin Recht. Certainty equivalence is efficient for linear quadratic control. _Advances in Neural Information Processing Systems_, 32, 2019.
* [21] Alon Cohen, Tomer Koren, and Yishay Mansour. Learning linear-quadratic regulators efficiently with only \(\sqrt{T}\) regret. _arXiv preprint arXiv:1902.06223_, 2019.
* [22] Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample complexity of the linear quadratic regulator. _Foundations of Computational Mathematics_, 20(4):633-679, 2020.
* [23] Chenkai Yu, Guanya Shi, Soon-Jo Chung, Yisong Yue, and Adam Wierman. The power of predictions in online control. _Advances in Neural Information Processing Systems_, 33:1994-2004, 2020.
* [24] Andrew Wagenmaker and Kevin Jamieson. Active learning for identification of linear dynamical systems. In _Conference on Learning Theory_, pages 3487-3582. PMLR, 2020.
* [25] Max Simchowitz and Dylan Foster. Naive exploration is optimal for online lqr. In _International Conference on Machine Learning_, pages 8937-8948. PMLR, 2020.
* [26] Max Simchowitz, Karan Singh, and Elad Hazan. Improper learning for non-stochastic control. In _Conference on Learning Theory_, pages 3320-3436. PMLR, 2020.
* [27] Dylan Foster, Tuhin Sarkar, and Alexander Rakhlin. Learning nonlinear dynamical systems from a single trajectory. In _Learning for Dynamics and Control_, pages 851-861. PMLR, 2020.
* [28] Samet Oymak. Stochastic gradient descent learns state equations with nonlinear activations. In _conference on Learning Theory_, pages 2551-2579. PMLR, 2019.
* [29] Yahya Sattar and Samet Oymak. Non-asymptotic and accurate learning of nonlinear dynamical systems. _The Journal of Machine Learning Research_, 23(1):6248-6296, 2022.
* [30] Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang. Bilinear classes: A structural framework for provable generalization in rl. In _International Conference on Machine Learning_, pages 2826-2836. PMLR, 2021.
* [31] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* [32] Zixiang Chen, Chris Junchi Li, Angela Yuan, Quanquan Gu, and Michael I Jordan. A general framework for sample-efficient function approximation in reinforcement learning. _arXiv preprint arXiv:2209.15634_, 2022.
* [33] Raman Mehra. Optimal input signals for parameter estimation in dynamic systems-survey and new results. _IEEE Transactions on Automatic Control_, 19(6):753-768, 1974.
* [34] Laszlo Gerencser and Hakan Hjalmarsson. Adaptive input design in system identification. In _Proceedings of the 44th IEEE Conference on Decision and Control_, pages 4988-4993. IEEE, 2005.
* [35] Dimitrios Katselis, Cristian R Rojas, Hakan Hjalmarsson, and Mats Bengtsson. Application-oriented finite sample experiment design: A semidefinite relaxation approach. _IFAC Proceedings Volumes_, 45(16):1635-1640, 2012.
* [36] Ian R Manchester. Input design for system identification via convex relaxation. In _49th IEEE Conference on Decision and Control (CDC)_, pages 2041-2046. IEEE, 2010.
* [37] Cristian R Rojas, James S Welsh, Graham C Goodwin, and Arie Feuer. Robust optimal experiment design for system identification. _Automatica_, 43(6):993-1008, 2007.

* [38] Graham Clifford Goodwin and Robert L Payne. _Dynamic system identification: experiment design and data analysis_. Academic press, 1977.
* [39] Kristian Lindqvist and Hakan Hjalmarsson. Identification for control: Adaptive input design using convex optimization. In _Proceedings of the 40th IEEE Conference on Decision and Control (Cat. No. 01CH37228)_, volume 5, pages 4326-4331. IEEE, 2001.
* [40] Laszlo Gerencser, Jonas Martensson, and Hakan Hjalmarsson. Adaptive input design for arx systems. In _2007 European Control Conference (ECC)_, pages 5707-5714. IEEE, 2007.
* [41] Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James M Rehg, Byron Boots, and Evangelos A Theodorou. Information theoretic mpc for model-based reinforcement learning. In _2017 IEEE International Conference on Robotics and Automation (ICRA)_, pages 1714-1721. IEEE, 2017.
* [42] Guanya Shi, Wolfgang Honig, Xichen Shi, Yisong Yue, and Soon-Jo Chung. Neural-swarm2: Planning and control of heterogeneous multirotor swarms using learned interactions. _IEEE Transactions on Robotics_, 38(2):1063-1079, 2021.
* [43] Aleksandr Aronovich Feldbaum. Dual control theory. i. _Avtomatika i Telemekhanika_, 21(9):1240-1249, 1960.
* [44] Ali Mesbah. Stochastic model predictive control with active uncertainty learning: A survey on dual control. _Annual Reviews in Control_, 45:107-117, 2018.
* [45] Douglas A Bristow, Marina Tharayil, and Andrew G Alleyne. A survey of iterative learning control. _IEEE control systems magazine_, 26(3):96-114, 2006.
* [46] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learning for atari. _arXiv preprint arXiv:1903.00374_, 2019.
* [47] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. _Advances in Neural Information Processing Systems_, 33:14129-14142, 2020.
* [48] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. _Advances in neural information processing systems_, 31, 2018.
* [49] Yashwanth Kumar Nakka, Anqi Liu, Guangya Shi, Anima Anandkumar, Yisong Yue, and Soon-Jo Chung. Chance-constrained trajectory optimization for safe exploration and learning of nonlinear systems. _IEEE Robotics and Automation Letters_, 6(2):389-396, 2020.
* [50] Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension. _Advances in Neural Information Processing Systems_, 27, 2014.
* [51] Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches. In _Conference on learning theory_, pages 2898-2933. PMLR, 2019.
* [52] Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a generative model is minimax optimal. In _Conference on Learning Theory_, pages 67-83. PMLR, 2020.
* [53] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_, pages 4532-4576. PMLR, 2021.
* [54] Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In _International Conference on Machine Learning_, pages 7304-7312. PMLR, 2019.

* [55] Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272. PMLR, 2017.
* [56] Andrew Wagenmaker and Kevin Jamieson. Instance-dependent near-optimal policy identification in linear mdps via online experiment design. _arXiv preprint arXiv:2207.02575_, 2022.
* [57] Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy exploration. In _International Conference on Machine Learning_, pages 2681-2691. PMLR, 2019.
* [58] Tom Zahavy, Brendan O'Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough for convex mdps. _Advances in Neural Information Processing Systems_, 34:25746-25759, 2021.
* [59] Andrew Wagenmaker and Aldo Pacchiano. Leveraging offline data in online reinforcement learning. _arXiv preprint arXiv:2211.04974_, 2022.
* [60] Jean Dieudonne. _Foundations of modern analysis_. Read Books Ltd, 2011.
* [61] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* [62] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.

Technical Tools

Additional Notation.We let \(\mathcal{S}^{d-1}\) refer to the unit ball in \(d\) dimensions and \(\mathbb{S}^{d}_{+}\) (resp. \(\mathbb{S}^{d}_{++}\)) the set of positive semi-definite matrices (resp. positive definite matrices) in \(\mathbb{R}^{d\times d}\). Throughout, \(\mathcal{O}(\cdot)\) denotes standard big-O notation, \(\widetilde{\mathcal{O}}(\cdot)\) hides additional logarithmic factors. \(\mathrm{poly}(\cdot)\) denotes some term that is polynomial in its arguments, with exponents absolute constants. \(\triangle_{\mathcal{X}}\) denotes the set of distributions over set \(\mathcal{X}\).

**Lemma A.1**.: _Let \(\bm{w}_{i}\sim\mathcal{N}(0,I_{d_{\bm{\alpha}}})\) for all \(i\in\{1,2,\ldots,n\}\). Then,_

\[\mathbb{E}\left[\left(\sum_{i=1}^{n}\|\bm{w}_{i}\|_{2}\right)^{c}\right]\leq n ^{2}\cdot\mathrm{poly}(d_{\bm{\alpha}}).\]

_for \(c\) an absolute constant._

Proof.: We first bound

\[(\sum_{i=1}^{n}\|\bm{w}_{i}\|_{2})^{c}\leq n\cdot\max_{i}\|\bm{w}_{i}\|_{2}^{c }\leq n\cdot\sum_{i}\|\bm{w}_{i}\|_{2}^{c}.\]

The result then follows since we can bound the \(\mathbb{E}[\|\bm{w}_{i}\|_{2}^{c}]\leq\mathrm{poly}(d)\) for \(\bm{w}_{i}\sim\mathcal{N}(0,I)\) and \(c\) an absolute constant. 

**Lemma A.2** (Lemma I.4 of [2]).: _Assume \(A,B\in\mathbb{S}^{d}_{++}\), \(\|A-B\|_{\mathrm{op}}\leq\epsilon\), and \(\epsilon<\lambda_{\min}(B)\). Then_

\[\|A^{-1}-B^{-1}\|_{\mathrm{op}}\leq\frac{\epsilon}{\lambda_{\min}(B)(\lambda_ {\min}(B)-\epsilon)}.\]

### Martingale Regression in General Norms

For the following two results, we consider the martingale regression setting of [2] (referred to as the MDM setting). In particular, we consider observations of the form

\[y_{\mathsf{t}}=\langle\bm{\mu}^{\star},\bm{z}_{\mathsf{t}}\rangle+w_{\mathsf{ t}},\] (A.1)

for \(y_{\mathsf{t}}\in\mathbb{R}\), unknown parameter \(\bm{\mu}^{\star}\in\mathbb{R}^{d_{\bm{\mu}}}\), \(w_{\mathsf{t}}\mid\mathcal{F}_{\mathsf{t}-1}\sim\mathcal{N}(0,\sigma_{\bm{w} }^{2})\), and \(\bm{z}_{\mathsf{t}}\), \(\mathcal{F}_{\mathsf{t}-1}\)-measurable, for a filtration \((\mathcal{F}_{\mathsf{t}})_{\mathsf{t}\geq 1}\). This setting therefore encompasses general stochastic processes where the observations are linear--the evolution of \(\bm{z}_{\mathsf{t}}\) could be arbitrary.

We consider the setting where we interact with (A.1) for \(\mathsf{T}\) steps, collecting observations \(\{(\bm{z}_{\mathsf{t}},y_{\mathsf{t}})\}_{\mathsf{t}=1}^{\mathsf{T}}\), and then form the least-squares estimate of \(\bm{\mu}^{\star}\):

\[\widehat{\bm{\mu}}=\left(\sum_{\mathsf{t}=1}^{\mathsf{T}}\bm{z}_{\mathsf{t}} \bm{z}_{\mathsf{t}}^{\top}\right)^{-1}\sum_{\mathsf{t}=1}^{\mathsf{T}}\bm{z}_{ \mathsf{t}}y_{\mathsf{t}}.\]

We also denote \(\bm{\Sigma}_{\mathsf{T}}:=\sum_{\mathsf{t}=1}^{\mathsf{T}}\bm{z}_{\mathsf{t}} \bm{z}_{\mathsf{t}}^{\top}\). The following results characterize the estimation error of \(\widehat{\bm{\mu}}\) in the \(M\)-norm and 2-norm.

**Proposition 3** (Theorem 7.2 of [2]).: _Fix any matrices \(\bm{\Gamma}\in\mathbb{S}^{d_{\bm{\mu}}}_{++},M\in\mathbb{S}^{d_{\bm{\mu}}}_{+}\), with \(M\neq 0\). Given a parameter \(\beta\in(0,1/4)\), define the event_

\[\mathcal{E}:=\{\|\bm{\Sigma}_{\mathsf{T}}-\bm{\Gamma}\|_{\mathrm{op}}\leq \beta\lambda_{\min}(\bm{\Gamma})\}.\]

_Then, if \(\mathcal{E}\) holds, the following holds with probability at least \(1-\delta\):_

\[\|\widehat{\bm{\mu}}-\bm{\mu}^{\star}\|_{M}^{2}\leq 5(1+\zeta)\cdot\sigma_{\bm{w} }^{2}\log\frac{6d_{\bm{\mu}}}{\delta}\cdot\mathrm{tr}(M\bm{\Gamma}^{-1})\]

_where \(\zeta=26\beta^{2}\lambda_{\max}(\bm{\Gamma})\mathrm{tr}(\bm{\Gamma}^{-1})\)._

**Proposition 4** (Lemma E.1 of [2]).: _On the event_

\[\mathcal{E}_{\mathrm{op}}:=\{\lambda_{\min}(\bm{\Sigma}_{\mathsf{T}})\geq \underline{\lambda},\bm{\Sigma}_{\mathsf{T}}\preceq\mathsf{T}\bar{\bm{\Gamma}}_{ \mathsf{T}}\},\]

_then we have that with probability at least \(1-\delta\):_

\[\|\widehat{\bm{\mu}}-\bm{\mu}^{\star}\|_{2}\leq C\cdot\sigma_{\bm{w}}\sqrt{ \frac{\log 1/\delta+d_{\bm{\mu}}+\log\det(\bar{\bm{\Gamma}}_{\mathsf{T}}/ \underline{\lambda}+I)}{\underline{\lambda}\mathsf{T}}}.\]

#### a.1.1 Connection Between (1.1) and (A.1)

We will apply the results Proposition 3 and Proposition 4 in the setting of (1.1) in order to obtain estimation bounds on \(A_{*}\). As the setting of (1.1) has vector observations, we briefly describe here how it can be mapped into the setting described above.

Recall that (1.1) evolves as

\[\bm{x}_{h+1}=A_{*}\bm{\phi}(\bm{x}_{h},\bm{u}_{h})+\bm{w}_{h},\quad h=1,\dots,H,\]

for \(\bm{x}_{h}\in\mathbb{R}^{d_{\bm{x}}}\), \(\bm{\phi}(\bm{x},\bm{u})\in\mathbb{R}^{d_{\bm{\phi}}}\), and \(A_{*}\in\mathbb{R}^{d_{\bm{x}}\times d_{\bm{\phi}}}\). We assume that \(\bm{x}_{1}\) is some fixed starting state. Assume that we have run for \(T\) episodes, and collected observations \(\{(\bm{x}_{1}^{t},\bm{u}_{1}^{t},\bm{x}_{2}^{t},\dots,\bm{x}_{h}^{t},\bm{u}_{h }^{t},\bm{x}_{h+1}^{t})\}_{t=1}^{T}\). Now let \(\bm{\mu}^{*}:=\mathrm{vec}(A_{*})\). Furthermore, for any \(t,h\), and \(i\in[d_{\bm{x}}]\), let \(\mathsf{t}=(t,h,i)\) and \(\bm{z}_{\mathsf{t}}=[\bm{0}_{d_{\bm{\phi}}(i-1)},\bm{\phi}(\bm{x}_{h}^{t}, \bm{u}_{h}^{t}),\bm{0}_{d_{\bm{\phi}}(d_{\bm{x}}-i)}]\in\mathbb{R}^{d_{\bm{x} }d_{\bm{\phi}}}\) where \(\bm{0}_{d}\) denotes the zero vector of length \(d\). Then we see that

\[[\bm{x}_{h+1}^{t}]_{i}=\langle\bm{\mu}^{*},\bm{z}_{\mathsf{t}}\rangle+[\bm{w}_ {h}^{t}]_{i}.\]

Setting \(y_{\mathsf{t}}=[\bm{x}_{h+1}^{t}]_{i}\) and \(w_{\mathsf{t}}=[\bm{w}_{h}^{t}]_{i}\), it is clear that this follows the observation model of (A.1) with \(d_{\bm{\mu}}=d_{\bm{\phi}}d_{\bm{x}}\) and \(\mathsf{T}=d_{\bm{x}}TH\). It is also straightforward to see that the measurability assumptions of the setting of (A.1) are satisfied by this.

## Appendix B Proof of Main Result

```
1:inputs: number of episodes to run \(T\), cost function \((\mathrm{cost}_{h})_{h=1}^{H}\), confidence \(\delta\), control policies \(\Pi^{*}\), exploration policies \(\Pi_{\mathrm{exp}}\)
2:\(\widehat{A}^{1}\leftarrow\) anything, \(\ell_{T}\leftarrow\lceil\log_{2}T/8\rceil\)
3:for\(\ell=1,2,3,\dots,\ell_{T}\)do
4:\(T_{\ell}\gets 2^{\ell},\delta_{\ell}\leftarrow\delta/12\ell^{2}\)
5: Compute estimate of cost matrix: \(\mathcal{H}_{\ell}\leftarrow\mathcal{H}(\widehat{A}^{\ell})\)
6:\(\Pi_{\ell}\leftarrow\textsc{LearnExp}(\mathcal{H}_{\ell},T_{\ell},\delta_{\ell },\mathbb{A}_{\mathcal{R}},\Pi_{\mathrm{exp}})\) (Algorithm 7), with \(\mathbb{A}_{\mathcal{R}}\) the LC\({}^{3}\) algorithm [17]
7: Rerun each policy in \(\Pi_{\ell}\)\(N_{\ell}=\lceil T_{\ell}/|\Pi_{\ell}|\rceil\) times, denote collected data \(\mathfrak{D}_{\ell}\)
8: Estimate system parameters
9: \[\widehat{A}^{\ell+1}=\arg\min_{A}\sum_{h=1}^{H}\sum_{(\bm{x}_{h+1},\bm{u}_{h}, \bm{x}_{h})\in\mathfrak{D}_{\ell}}\|\bm{x}_{h+1}-A\bm{\phi}(\bm{x}_{h},\bm{u}_{ h})\|_{2}^{2}\]
10:return\(\widehat{\pi}_{T}\leftarrow\pi_{*}(\widehat{A}^{\ell_{T}+1})\) ```

**Algorithm 3** Optimal Exploration in Nonlinear Systems (Full Version of Algorithm 1)

**Theorem 4** (Full Version of Theorem 1).: _Assume Assumptions 1 to 5 and 13 hold. Then if_

\[T\geq\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},H,B_{A},B_{\bm{\phi}},L_{\bm{ \phi}},L_{\bm{\theta}},L_{\mathrm{cost}},L_{\pi_{*}},\sigma_{\bm{w}},\sigma_{ \bm{w}}^{-1},\tfrac{1}{\lambda_{\min}^{1}},\log\tfrac{T}{\delta}\right)\cdot \max\left\{1,\tfrac{1}{r_{\mathrm{cost}}(A_{*})^{2}},\tfrac{1}{r_{\bm{\theta}}( A_{*})^{2}}\right\},\] (B.1)

_with probability at least \(1-\delta\), Algorithm 3 plays exploration policies \(\pi_{\mathrm{exp}}\in\Pi_{\mathrm{exp}}\) at every episode, runs for at most \(T\) episodes, and the controller \(\widehat{\pi}_{T}\) returned Algorithm 3 satisfies, with probability at least \(1-\delta\):_

\[\mathcal{J}(\widehat{\pi}_{T};A_{*})-\mathcal{J}(\pi_{*}(A_{*});A_{*})\leq \frac{\sigma_{\bm{w}}^{2}}{T}\cdot\min_{\bm{\Lambda}\in\bm{\Omega}}\mathrm{tr} \left(\mathcal{H}(A_{*})\bm{\Lambda}^{-1}\right)\cdot C\log\frac{6d_{\bm{x}}d_{ \bm{\phi}}}{\delta}+\frac{C_{\mathrm{tot}}}{T^{3/2}}\]

_for \(C\) a universal constant and_

\[C_{\mathrm{poly}}:=\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},H,B_{A},B_{\bm{ \phi}},L_{\bm{\phi}},L_{\bm{\theta}},L_{\mathrm{cost}},L_{\pi_{*}},\sigma_{\bm{ w}},\sigma_{\bm{w}}^{-1},\tfrac{1}{\lambda_{\min}^{T}},\log\tfrac{T}{\delta}\right).\]

Proof.: Let \(\mathcal{E}_{\ell}\) denote that the good event of Lemma B.3 holds at round \(\ell\) which, by Lemma B.3, occurs with probability at least \(1-6\delta_{\ell}\). By our setting of \(\delta_{\ell}=\delta/12\ell^{2}\), we have that the total failureprobability of \(\mathcal{E}_{\ell}\) for all \(\ell\) is bounded as

\[\sum_{\ell=1}^{\infty}6\cdot\frac{\delta}{12\ell^{2}}\leq\delta.\]

Henceforth we assume that \(\mathcal{E}:=\cap_{\ell}\mathcal{E}_{\ell}\) holds. Let \(\widehat{A}:=\widehat{A}^{\ell_{\mathcal{T}}+1}\), and \(\widehat{A}^{-}:=\widehat{A}^{\ell_{\mathcal{T}}}\).

Before proceeding to the main proof, we note that the conclusion that Algorithm 3 only explores with policies in \(\Pi_{\mathrm{exp}}\) follows from the definition of LearnExpII and LC\({}^{3}\). Note that LearnExpII only interacts with (1.1) through calls to DynamicOED, which itself only interacts with (1.1) through calls to \(\mathbb{A}_{\mathcal{R}}\), instantiated in Algorithm 3 by LC\({}^{3}\). Inspection of the LC\({}^{3}\) algorithm in [17] reveals that LC\({}^{3}\) only interacts with (1.1) by playing policies in \(\Pi_{\mathrm{exp}}\), from which the conclusion follows.

Bounding the Number of Episodes.Denote \(T_{\ell}^{\mathrm{oed}}=|\Pi_{\ell}|\). Note that by construction we always have \(N_{\ell}T_{\ell}^{\mathrm{oed}}\geq T_{\ell}\). By Lemma B.2, as long as (B.1) is met, we can bound the total number of episodes collected up to and including round \(\ell\) by \(4T_{\ell}\) for \(\ell\in\{\ell_{T},\ell_{T}-1\}\). We therefore, in the following, will make use of the fact that

\[c\cdot T\leq N_{\ell}K_{\ell}\leq c^{\prime}\cdot T\]

for \(\ell\in\{\ell_{T},\ell_{T}-1\}\) and absolute constants \(c,c^{\prime}\). Furthermore, it also follows from this that the total number of episodes run by Algorithm 3 is bounded by

\[4T_{\ell_{T}}=4\cdot 2^{\lceil\log T/8\rceil}\leq 8\cdot\frac{T}{8}=T,\]

so Algorithm 3 runs for at most \(T\) episodes.

Approximating the Controller Loss.Let \(r_{\mathrm{est}}(A_{\star}):=\min\{1,r_{\mathrm{cost}}(A_{\star}),r_{\bm{ \theta}}(A_{\star})\}\), for \(r_{\mathrm{cost}}(A_{\star})\) as in Assumption 2 and \(r_{\bm{\theta}}(A_{\star})\) as in Assumption 13. By Lemma D.2, under Assumptions 1, 2, 4, 5 and 13, as long as \(\widehat{A}\in\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\mathrm{est}}(A_{\star}))\), we have

\[\mathcal{J}(\widehat{\pi}_{T};A_{\star})-\mathcal{J}(\pi_{\star} (A_{\star});A_{\star})\leq\|\mathrm{vec}(\widehat{A}-A_{\star})\|_{\mathcal{H }(A_{\star})}^{2}\] \[\qquad+\mathrm{poly}(L_{\pi_{\star}},\|A_{\star}\|_{\mathrm{op}}, L_{\bm{\phi}},L_{\bm{\theta}},L_{\mathrm{cost}},\sigma_{\bm{w}}^{-1},H,d_{\bm{w}} )\cdot\|\widehat{A}-A_{\star}\|_{\mathrm{op}}^{3}.\]

Furthermore, we can bound

\[\|\mathrm{vec}(\widehat{A}-A_{\star})\|_{\mathcal{H}(A_{\star})}^{ 2} =\|\mathrm{vec}(\widehat{A}-A_{\star})\|_{\mathcal{H}(\widehat{A}^{-})}^{2}+ \mathrm{vec}(\widehat{A}-A_{\star})^{\top}(\mathcal{H}(A_{\star})-\mathcal{H }(\widehat{A}^{-}))\mathrm{vec}(\widehat{A}-A_{\star})\] \[\leq(\widehat{A}-A_{\star})^{\top}\mathcal{H}(\widehat{A}^{-})( \widehat{A}-A_{\star})+\|\widehat{A}^{-}-A_{\star}\|_{\mathrm{op}}^{2}\| \mathcal{H}(A_{\star})-\mathcal{H}(\widehat{A}^{-})\|_{\mathrm{op}}.\]

Bounding the Hessian Estimation Error.On \(\mathcal{E}\), by Lemma B.3 and as long as (B.1) is met, we have (note that at the final epoch, the plug-in estimator \(\mathcal{H}(\widehat{A}^{-})\) is given as input to DynamicOED):

\[\|\mathrm{vec}(\widehat{A}-A_{\star})\|_{\mathcal{H}(\widehat{A}^ {-})}^{2} \leq\frac{60}{N_{\ell_{T}}T_{\ell_{T}}^{\mathrm{oed}}}\cdot\min_{ \mathbf{\Lambda}\in\mathbf{\Omega}}\mathrm{tr}\left(\mathcal{H}(\widehat{A}^{- })\check{\mathbf{\Lambda}}^{-1}\right)\cdot\sigma_{\bm{w}}^{2}\log\frac{6d_{ \bm{w}}d_{\bm{\phi}}}{\delta}\] \[\qquad+\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{ \lambda_{\min}^{\star}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}}},H, \|\mathcal{H}(\widehat{A}^{-})\|_{\mathrm{op}},\log\frac{T_{\ell_{T}}}{\delta} \right)\cdot\frac{1}{N_{\ell_{T}}^{2}}\] \[\leq\frac{C}{T}\cdot\min_{\mathbf{\Lambda}\in\mathbf{\Omega}} \mathrm{tr}\left(\mathcal{H}(\widehat{A}^{-})\check{\mathbf{\Lambda}}^{-1} \right)\cdot\sigma_{\bm{w}}^{2}\log\frac{6d_{\bm{x}}d_{\bm{\phi}}}{\delta}\] \[\qquad+\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{ \lambda_{\min}^{\star}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}}},H, \|\mathcal{H}(\widehat{A}^{-})\|_{\mathrm{op}},\log\frac{T}{\delta}\right) \cdot\frac{1}{T^{2}}\]

where the last line uses that \(N_{\ell_{T}}T_{\ell_{T}}^{\mathrm{oed}}\) is within a constant of \(T\), and that \(T_{\ell_{T}}^{\mathrm{oed}}\) can be bounded by

\[\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{\lambda_{\min}^{\star}},B_ {A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}}},H,\log\frac{T}{\delta}\right)\]

by Lemma B.3. By Lemma B.4 we can bound

\[\min_{\mathbf{\Lambda}\in\mathbf{\Omega}}\mathrm{tr}\left(\mathcal{H}(\widehat{A }^{-})\check{\mathbf{\Lambda}}^{-1}\right)\leq\min_{\mathbf{\Lambda}\in\mathbf{ \Omega}}2\mathrm{tr}\left(\mathcal{H}(A_{\star})\check{\mathbf{\Lambda}}^{-1} \right)+\frac{2d_{\bm{x}}d_{\bm{\phi}}}{\lambda_{\min}^{\star}}\cdot\| \mathcal{H}(A_{\star})-\mathcal{H}(\widehat{A}^{-})\|_{\mathrm{op}}\]and by Lemma D.3, under Assumptions 1, 2, 4, 5 and 13, and as long as \(\widehat{A}^{-}\in\mathcal{B}_{\mathrm{F}}(A_{*};r_{\mathrm{est}}(A_{*}))\), we can bound

\[\|\mathcal{H}(A_{\star})-\mathcal{H}(\widehat{A}^{-})\|_{\mathrm{op}}\leq \mathrm{poly}(L_{\pi_{\star}},\|A_{\star}\|_{\mathrm{op}},L_{\bm{\phi}},L_{\bm {\theta}},L_{\mathrm{cost}},\sigma_{\bm{w}}^{-1},H,d_{\bm{x}})\cdot\|\widehat{A }^{-}-A_{\star}\|_{\mathrm{op}}.\]

Let

\[C_{\mathrm{tot}}:=\mathrm{poly}\left(L_{\pi_{\star}},B_{A},B_{\bm{\phi}},L_{\bm {\phi}},L_{\bm{\theta}},L_{\mathrm{cost}},d_{\bm{x}},d_{\bm{\phi}},\frac{1}{ \lambda_{\mathrm{min}}^{\star}},\sigma_{\bm{w}},\sigma_{\bm{w}}^{-1},H,\| \mathcal{H}(A_{\star})\|_{\mathrm{op}},\log\frac{T}{\delta}\right)\]

denote some lower-order constant, whose precise polynomial dependence may change from line to line. On \(\mathcal{E}\), by Lemma B.3 we can bound

\[\|\widehat{A}-A_{\star}\|_{\mathrm{op}},\|\widehat{A}^{-}-A_{\star}\|_{\mathrm{ op}}\leq C_{\mathrm{tot}}\cdot\frac{1}{\sqrt{T}},\] (B.2)

and so assuming the burn-in (B.1) is met, we can bound \(\|\widehat{A}^{-}-A_{\star}\|_{\mathrm{op}}\leq\frac{1}{2}r_{\mathrm{est}}(A_ {\star})\) and \(\|\widehat{A}-A_{\star}\|_{\mathrm{op}}\leq\frac{1}{2}r_{\mathrm{est}}(A_{ \star})\). This then implies that

\[\|\mathcal{H}(A_{\star})-\mathcal{H}(\widehat{A}^{-})\|_{\mathrm{op}}\leq C_ {\mathrm{tot}}\cdot\frac{1}{\sqrt{T}},\]

so in particular we can bound

\[\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{\lambda_{ \mathrm{min}}^{\star}},B_{A},B_{\bm{\phi}},H,\log\frac{1}{\sigma_{\bm{w}}},\| \mathcal{H}(\widehat{A}^{-})\|_{\mathrm{op}},\log\frac{T}{\delta}\right)\] \[\leq\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{\lambda_{ \mathrm{min}}^{\star}},B_{A},B_{\bm{\phi}},H,\log\frac{1}{\sigma_{\bm{w}}},\| \mathcal{H}(A_{\star})\|_{\mathrm{op}},\log\frac{T}{\delta}\right).\]

We have therefore shown that

\[\mathcal{J}(\tilde{\pi}_{T};A_{\star})-\mathcal{J}(\pi_{\star}(A_ {\star});A_{\star}) \leq\frac{C}{T}\cdot\min_{\bm{\Lambda}\in\bm{\Omega}}\mathrm{tr} \left(\mathcal{H}(A_{\star})\tilde{\bm{\Lambda}}^{-1}\right)\cdot\sigma_{\bm{ w}}^{2}\log\frac{6d_{\bm{x}}d_{\bm{\phi}}}{\delta}\] \[\qquad+C_{\mathrm{tot}}\cdot\left(\|\widehat{A}-A_{\star}\|_{ \mathrm{op}}^{3}+\|\widehat{A}^{-}-A_{\star}\|_{\mathrm{op}}^{3}+\frac{1}{T^{2 }}+\frac{1}{T}\cdot\|\widehat{A}^{-}-A_{\star}\|_{\mathrm{op}}\right)\] \[\leq\frac{C}{T}\cdot\min_{\bm{\Lambda}\in\bm{\Omega}}\mathrm{tr} \left(\mathcal{H}(A_{\star})\tilde{\bm{\Lambda}}^{-1}\right)\cdot\sigma_{\bm {w}}^{2}\log\frac{6d_{\bm{x}}d_{\bm{\phi}}}{\delta}+\frac{C_{\mathrm{tot}}}{ T^{3/2}},\]

The final result follows from using Lemma D.4 to bound

\[\|\mathcal{H}(A_{\star})\|_{\mathrm{op}}\leq\mathrm{poly}(\|A_{\star}\|_{ \mathrm{op}},B_{\bm{\phi}},L_{\bm{\phi}},L_{\bm{\theta}},L_{\mathrm{cost}},L_{ \pi_{\star}},\sigma_{\bm{w}}^{-1},H,d_{\bm{x}}).\]

Proof of Theorem 1.: The proof of Theorem 1 is identical to that of Theorem 4, the only difference being that we replace Assumption 13 with Assumption 6. However, by Proposition 6, the conditions of Assumption 13 are met when Assumption 6 holds. 

### Supporting Lemmas

**Lemma B.1**.: _Under Assumptions 1 and 3, the system (1.1) satisfies Assumptions 11 and 12 with_

\[\bm{\psi}(\bm{\tau})\leftarrow I_{d_{\bm{x}}}\otimes\sum_{h=1}^{H}\bm{\phi}( \bm{x}_{h}^{\bm{\tau}},\bm{u}_{h}^{\bm{\tau}})\bm{\phi}(\bm{x}_{h}^{\bm{\tau} },\bm{u}_{h}^{\bm{\tau}})^{\top},\quad D=d_{\bm{x}}HB_{\bm{\phi}}^{2},\]

\(d_{\bm{\psi}}\gets d_{\bm{\phi}}d_{\bm{x}}\)_, and where \(\bm{x}_{h}^{\bm{\tau}}\) (resp. \(\bm{u}_{h}^{\bm{\tau}}\)) denotes the state (resp. input) at step \(h\) of trajectory \(\bm{\tau}\). Furthermore, it satisfies Assumption 10 with \(\mathbb{A}_{\mathcal{R}}\) instantiated with the \(\mathrm{LC}^{3}\) algorithm of [17] and_

\[C_{\mathcal{R}}=C\cdot H\sqrt{d_{\bm{\phi}}(d_{\bm{\phi}}+d_{\bm{x}}+B_{A})} \cdot\log(1+B_{\bm{\phi}}H/\sigma_{\bm{w}}),\quad p_{\mathcal{R}}=3/2,\quad \alpha=1/2\]

_for a universal constant \(C\)._Proof.: That Assumption 12 is satisfied is immediate under Assumption 3. It is clear that \(\bm{\psi}(\bm{\tau})\in\mathcal{S}_{+}^{d_{\bm{\phi}}d_{\bm{\alpha}}}\). To obtain a bound on \(D\), we only need to bound the trace of \(\bm{\psi}(\bm{\tau})\):

\[\operatorname{tr}(\bm{\psi}(\bm{\tau})) =\sum_{h=1}^{H}\operatorname{tr}(I_{d_{\bm{\alpha}}})\cdot \operatorname{tr}(\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\bm{u}_{h}^{\bm{\tau}})) \bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\bm{u}_{h}^{\bm{\tau}}))^{\top})\] \[=d_{\bm{x}}\sum_{h=1}^{H}\|\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\bm{u }_{h}^{\bm{\tau}}))\|_{2}^{2}\] \[\leq d_{\bm{x}}HB_{\bm{\phi}}^{2}\]

where the inequality holds under Assumption 1.

To show that Assumption 10 is satisfied in this setting, we have by Theorem 6 that with probability at least \(1-\delta\), LC\({}^{3}\) has regret bounded as (using that \(c_{\max}\leq 1\) in the setting of Assumption 10):

\[\mathcal{R}_{T}\leq C\cdot H\sqrt{d_{\bm{\phi}}\cdot(d_{\bm{\phi}}+d_{\bm{x} }+B_{A}+\log\frac{1}{\delta})\cdot T}\cdot\log\left(1+B_{\bm{\phi}}HT/\sigma_{ \bm{w}}\right)\]

for \(C\) a universal constant. We can therefore take \(\alpha=1/2\), \(p_{\mathcal{R}}=3/2\), and

\[C_{\mathcal{R}}=C^{\prime}\cdot H\sqrt{d_{\bm{\phi}}(d_{\bm{\phi}}+d_{\bm{x} }+B_{A})}\cdot\log(1+B_{\bm{\phi}}H/\sigma_{\bm{w}}).\]

**Lemma B.2**.: _Let \(\bar{T}_{\ell}\) denote the total number of episodes collected by Algorithm 3 at round \(\ell\). For_

\[T_{\ell}\geq\operatorname{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{\lambda _{\min}^{\star}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}}},H,\log \frac{T_{\ell}}{\delta}\right),\] (B.3)

_on the success event of Lemma B.3, we have \(2T_{\ell}\geq\bar{T}_{\ell}\) and_

\[2T_{\ell}\geq\sum_{i=1}^{\ell-1}\bar{T}_{i}.\]

Proof.: Recall that \(T_{\ell}=2^{\ell}\) and \(N_{\ell}=\lceil T_{\ell}/T_{\ell}^{\mathrm{oed}}\rceil\) for \(T_{\ell}^{\mathrm{oed}}=|\Pi_{\ell}|\). By Lemma B.3, \(\bar{T}_{\ell}\) can be bounded as

\[\bar{T}_{\ell}\leq N_{\ell}T_{\ell}^{\mathrm{oed}}+(16+2\log T_{\ell}^{ \mathrm{oed}})T_{\ell}^{\mathrm{oed}}\]

and \(T_{\ell}^{\mathrm{oed}}\) can be bounded as

\[T_{\ell}^{\mathrm{oed}}\leq\operatorname{poly}\left(d_{\bm{\phi}},d_{\bm{x}}, \frac{1}{\lambda_{\min}^{\star}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{ w}}},H,\log\frac{T_{\ell}}{\delta}\right).\] (B.4)

Note that we can bound

\[\bar{T}_{i} \leq N_{i}T_{i}^{\mathrm{oed}}+(16+2\log T_{i}^{\mathrm{oed}})T_ {i}^{\mathrm{oed}}\] \[\leq T_{i}+(17+2\log T_{i}^{\mathrm{oed}})T_{i}^{\mathrm{oed}}\] \[\leq T_{i}+\operatorname{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac {1}{\lambda_{\min}^{\star}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}}},H,\log\frac{T_{\ell}}{\delta}\right).\]

From this it is immediately obvious that \(2T_{\ell}\geq\bar{T}_{\ell}\) as long as (B.3) is satisfied.

To show the second conclusion note that, by our choice of \(T_{i}=2^{i}\), we have that \(T_{\ell}\geq\sum_{i=1}^{\ell-1}T_{i}\), so it therefore remains to show that

\[T_{\ell}\geq\sum_{i=1}^{\ell-1}\operatorname{poly}\left(d_{\bm{\phi}},d_{\bm{x }},\frac{1}{\lambda_{\min}^{\star}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{ \bm{w}}},H,\log\frac{T_{i}}{\delta}\right).\]However, we can bound

\[\sum_{i=1}^{\ell-1}\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{\lambda_{ \min}^{\star}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}}},H,\log\frac{T_ {i}}{\delta}\right)\leq\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{ \lambda_{\min}^{\star}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}}},H, \log\frac{T_{\ell}}{\delta}\right)\cdot\log T_{\ell},\]

so a sufficient condition is

\[T_{\ell}\geq\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{\lambda_{ \min}^{\star}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}}},H,\log\frac{T _{\ell}}{\delta}\right)\cdot\log T_{\ell}\]

which we see is met when (B.3) holds. 

**Lemma B.3**.: _Consider running Algorithm 7 with weight matrix \(\mathcal{H}\), parameter \(\widetilde{N}\), and confidence \(\delta\), and rerunning each policy in \(\Pi_{\mathrm{out}}\)\(N\leq\widetilde{N}\) times. Then, under Assumptions 1 and 3, with probability at least \(1-6\delta\):_

\[\|\mathrm{vec}(\widehat{A}-A_{\star})\|_{\mathcal{H}}^{2}\leq \frac{60}{NT_{\mathrm{out}}}\cdot\min_{\bm{\Lambda}\in\bm{\Omega}} \mathrm{tr}\left(\mathcal{H}\hat{\bm{\Lambda}}^{-1}\right)\cdot\sigma_{\bm{w} }^{2}\log\frac{6d_{\bm{x}}d_{\bm{\phi}}}{\delta}\] \[\qquad\qquad+\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1} {\lambda_{\min}^{\star}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}}},H, \|\mathcal{H}\|_{\mathrm{op}},\log\frac{\widetilde{N}}{\delta}\right)\cdot \frac{1}{N^{2}}\]

_where \(\widehat{A}\) denotes the least-squares estimate of \(A_{\star}\) obtained on the data generated by rerunning \(\Pi_{\mathrm{out}}\). In addition, we have_

\[\|\widehat{A}-A_{\star}\|_{\mathrm{F}}\leq\mathrm{poly}\left(d_{\bm{\phi}},d_ {\bm{x}},\frac{1}{\lambda_{\min}^{\star}},B_{A},B_{\bm{\phi}},\sigma_{\bm{w}},H,\log\frac{\widetilde{N}}{\delta}\right)\cdot\frac{1}{\sqrt{N}}.\]

_Furthermore, we have_

\[T_{\mathrm{out}}\leq\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{ \lambda_{\min}^{\star}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}}},H, \log\frac{\widetilde{N}}{\delta}\right)\]

_and the total number of episodes collected by this procedure is bounded by \(NT_{\mathrm{out}}+(16+2\log(T_{\mathrm{out}}))\cdot T_{\mathrm{out}}\), for \(T_{\mathrm{out}}=|\Pi_{\mathrm{out}}|\)._

Proof.: By Lemma B.1, the assumptions of Lemma C.6 and Lemma C.7 are met, so we can therefore apply these results in our setting. By Lemma C.6, the event \(\mathcal{E}_{\mathrm{exp}}\) occurs with probability at least \(1-\delta\). Throughout the remainder of the proof we union bound over the success event of Lemma C.7 and \(\mathcal{E}_{\mathrm{exp}}\), which together occur with probability at least \(1-4\delta\).

Let

\[\widetilde{\bm{\Lambda}}:=\sum_{t=1}^{NT_{\mathrm{out}}}\bm{\psi}(\bm{\tau}^{t })=I_{d_{\bm{x}}}\otimes\sum_{t=1}^{NT_{\mathrm{out}}}\sum_{h=1}^{H}\bm{\phi}( \bm{x}_{h}^{t},\bm{u}_{h}^{t})\bm{\phi}(\bm{x}_{h}^{t},\bm{u}_{h}^{t})^{\top}\]

denote the features returned by rerunning every policy in \(\Pi_{\mathrm{out}}\)\(N\) times. By Lemma C.7, we then have that:

\[\left\|\widetilde{\bm{\Lambda}}-N\cdot\sum_{\pi\in\Pi_{\mathrm{ out}}}\tilde{\bm{\Lambda}}_{\pi}\right\|_{\mathrm{op}}\leq\underbrace{\frac{ \sqrt{T_{\mathrm{out}}}\cdot\sqrt{8d_{\bm{\phi}}d_{\bm{x}}\log(1+8\sqrt{NT_{ \mathrm{out}}})+8\log 1/\delta}}{\sqrt{N}\cdot 6272d_{\bm{\phi}}d_{\bm{x}}\log\frac{68 \widetilde{N}}{\delta}}}_{=:\beta}\cdot\lambda_{\min}\left(N\cdot\sum_{\pi\in \Pi_{\mathrm{out}}}\tilde{\bm{\Lambda}}_{\pi}\right).\]

Applying Proposition 3 with \(\mathcal{E}\) the event that the above conclusion holds and \(\bm{\Gamma}:=N\cdot\sum_{\pi\in\Pi_{\mathrm{out}}}\tilde{\bm{\Lambda}}_{\pi}\), we obtain that, with probability at least \(1-\delta\) (using the mapping to the martingale regression setting described in Appendix A.1.1):

\[\|\mathrm{vec}(\widehat{A}-A_{\star})\|_{\mathcal{H}}^{2}\leq 5(1+\zeta)\cdot\sigma_{\bm{w}}^{2}\log\frac{6d_{\bm{x}}d_{\bm{ \phi}}}{\delta}\cdot\mathrm{tr}(\mathcal{H}\bm{\Gamma}^{-1})\]for \(\zeta=26\beta^{2}\lambda_{\max}(\mathbf{\Gamma})\mathrm{tr}(\mathbf{\Gamma}^{-1})\) and \(\beta\) as defined above. Since \(\|\bm{\phi}(\bm{x},\bm{u})\|_{2}\leq B_{\bm{\phi}}\) under Assumption 1, we have \(\|\tilde{\bm{\Lambda}}_{\pi}\|_{2}\leq B_{\bm{\phi}}^{2}\), so we can upper bound \(\lambda_{\max}(\mathbf{\Gamma})\leq NT_{\mathrm{out}}B_{\bm{\phi}}^{2}\). By Lemma C.7, we can also bound (using that \(D=d_{\bm{x}}HB_{\bm{\phi}}^{2}\) by Lemma B.1):

\[\mathrm{tr}(\mathbf{\Gamma}^{-1})\leq\frac{1}{N\cdot 6272d_{\bm{x}}HB_{\bm{ \phi}}^{2}\log\frac{68\widetilde{N}}{\delta}}.\]

Combining these and using that

\[T_{\mathrm{out}}\leq\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}}, \frac{1}{\lambda_{\min}^{*}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}} },H,\log\frac{\widetilde{N}}{\delta}\right)\] (B.5)

as shown in Lemma C.8 (and using our bounds on \(C_{\mathcal{R}}\) and \(p_{\mathcal{R}}\) in Lemma B.1), we can therefore bound

\[\zeta\leq\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{ \lambda_{\min}^{*}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}}},H,\log \frac{\widetilde{N}}{\delta}\right)\cdot\frac{1}{N}.\]

Using that \(\mathrm{tr}(\mathcal{H}\mathbf{\Gamma}^{-1})\leq\|\mathcal{H}\|_{\mathrm{op}} \cdot\mathrm{tr}(\mathbf{\Gamma}^{-1})\), and the bound on \(\mathrm{tr}(\mathbf{\Gamma}^{-1})\) given above, it follows that

\[\|\mathrm{vec}(\widehat{A}-A_{\star})\|_{\mathcal{H}}^{2}\leq 5 \sigma_{\bm{w}}^{2}\log\frac{6d_{\bm{x}}d_{\bm{\phi}}}{\delta}\cdot\mathrm{tr} (\mathcal{H}\mathbf{\Gamma}^{-1})+\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}}, \frac{1}{\lambda_{\min}^{*}},B_{A},B_{\bm{\phi}},\log\frac{1}{\sigma_{\bm{w}} },H,\|\mathcal{H}\|_{\mathrm{op}},\log\frac{\widetilde{N}}{\delta}\right) \cdot\frac{1}{N^{2}}.\]

Finally, by Lemma C.7, we can bound

\[\mathrm{tr}(\mathcal{H}\mathbf{\Gamma}^{-1})\leq\frac{12}{NT_{ \mathrm{out}}}\cdot\min_{\bm{\Lambda}\in\bm{\Omega}}\mathrm{tr}(\mathcal{H} \mathbf{\check{\Lambda}}^{-1}).\]

By Lemma C.8, we can bound the total number of episodes collected by Algorithm 7 by \((16+2\log(T_{\mathrm{out}}))\cdot T_{\mathrm{out}}\).

Bound on Frobenius Norm Error.By Lemma C.7, we can lower bound

\[\lambda_{\min}(\widetilde{\bm{\Lambda}})\geq N\cdot 6272d_{\bm{x}}d_{\bm{ \phi}}HB_{\bm{\phi}}^{2}\log\frac{68\widetilde{N}}{\delta}.\]

Furthermore, since \(\|\bm{\phi}(\bm{x},\bm{u})\|_{2}\leq B_{\bm{\phi}}\), we always have \(\|\widetilde{\bm{\Lambda}}\|_{\mathrm{op}}\leq NT_{\mathrm{out}}B_{\bm{\phi}}^ {2}\), which implies \(\widetilde{\bm{\Lambda}}\preceq NT_{\mathrm{out}}B_{\bm{\phi}}^{2}\cdot I\). By Proposition 4, we then have that with probability at least \(1-\delta\) (again using the mapping to the martingale regression setting described in Appendix A.1.1):

\[\|\widehat{A}-A_{\star}\|_{\mathrm{F}} \leq C\cdot\sigma_{\bm{w}}\sqrt{\frac{\log 1/\delta+d_{\bm{x}}d_{\bm{ \phi}}+\log\det(\frac{T_{\mathrm{out}}}{6272d_{\bm{x}}d_{\bm{\phi}}H\log\frac{68 \widetilde{N}}{\delta}\cdot I+I})}{N\cdot 6272d_{\bm{x}}d_{\bm{\phi}}HB_{\bm{\phi}}^{2} \log\frac{68\widetilde{N}}{\delta}}}\] \[\leq\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},\frac{1}{\lambda_{ \min}^{*}},B_{A},B_{\bm{\phi}},\sigma_{\bm{w}},H,\log\frac{\widetilde{N}}{ \delta}\right)\cdot\frac{1}{\sqrt{N}}.\]

**Lemma B.4**.: _Under Assumption 3, for any \(\mathcal{H},\mathcal{H}^{\prime}\), we can bound_

\[\min_{\tilde{\bm{\Lambda}}\in\bm{\Omega}}\mathrm{tr}(\mathcal{H }\mathbf{\check{\Lambda}}^{-1})\leq\min_{\tilde{\bm{\Lambda}}\in\bm{\Omega}} 2\mathrm{tr}(\mathcal{H}^{\prime}\mathbf{\check{\Lambda}}^{-1})+\frac{2d_{\bm{x }}d_{\bm{\phi}}}{\lambda_{\min}^{*}}\cdot\|\mathcal{H}-\mathcal{H}^{\prime}\|_{ \mathrm{op}}.\]

Proof.: We have

\[\min_{\tilde{\bm{\Lambda}}\in\bm{\Omega}}\mathrm{tr}(\mathcal{H }\mathbf{\check{\Lambda}}^{-1}) =\min_{\tilde{\bm{\Lambda}}\in\bm{\Omega}}\mathrm{tr}(\mathcal{H }^{\prime}\mathbf{\check{\Lambda}}^{-1})+\mathrm{tr}((\mathcal{H}-\mathcal{H}^{ \prime})\mathbf{\check{\Lambda}}^{-1})\] \[\leq\min_{\tilde{\bm{\Lambda}}\in\bm{\Omega}}\mathrm{tr}( \mathcal{H}^{\prime}\mathbf{\check{\Lambda}}^{-1})+\|\mathcal{H}-\mathcal{H}^{ \prime}\|_{\mathrm{op}}\cdot\mathrm{tr}(\mathbf{\check{\Lambda}}^{-1})\]

[MISSING_PAGE_FAIL:22]

We define DynamicOED as in Algorithm 4. We then have the following generalization of Theorem 3.

**Theorem 5** (Full Version of Theorem 3).: _Let Assumption 7 hold, and assume that we have access to a learner \(\mathbb{A}_{\mathcal{R}}\) satisfying Assumption 8. Fix \(N,K>0\). Then, with probability at least \(1-\delta\), DynamicOED runs for at most \((N+1)K\) episodes, and collects a dataset satisfying \(\mathfrak{D}=\{\{\bm{\tau}_{k}^{n}\}_{k=1}^{K}\}_{n=0}^{N}\) satisfying_

\[\Phi\left(\frac{1}{K(N+1)}\sum_{n=0}^{N}\sum_{k=1}^{K}\bm{\psi}( \bm{\tau}_{k}^{n})\right)-\min_{\bm{\Gamma}\in\bm{\Omega}_{\bm{\psi}}}\Phi(\bm {\Gamma})\leq\frac{\beta R^{2}(\log N+1)}{2(N+1)}+M\cdot\left(C_{\mathcal{R}} \log^{p_{\mathcal{R}}}\frac{2NK}{\delta}\cdot K^{\alpha-1}\right.\] \[\left.\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+ \sqrt{\frac{8\log(4N/\delta)}{K}}\right)\]

_where \(R=\sup_{\bm{\Gamma},\bm{\Gamma}^{\prime}\in\widehat{\bm{\Omega}}_{\bm{\psi}}} \|\bm{\Gamma}-\bm{\Gamma}^{\prime}\|\)._

In this work we are particularly interested in the case where \(\bm{\psi}(\bm{\tau})\in\mathcal{S}_{+}^{d_{\bm{\psi}}}\). We encapsulate this in the following assumption.

**Assumption 11** (Matrix Experiment Design).: _We assume that \(\bm{\psi}(\bm{\tau})\in\mathcal{S}_{+}^{d_{\bm{\psi}}}\) and that, for all \(\bm{\tau}\in\mathcal{T}\), \(\operatorname{tr}(\bm{\psi}(\bm{\tau}))\leq D\) for some \(D>0\)._

The following corollary instantiates Theorem 3 under Assumption 11 with objective \(\Phi(\bm{\Gamma})=\operatorname{tr}\big{(}\mathcal{H}(\bm{\Gamma}+\bm{\Gamma} _{0})^{-1}\big{)}\), the objective considered in Algorithm 1.

**Corollary 1**.: _Consider the objective_

\[\Phi(\bm{\Gamma})=\operatorname{tr}\big{(}\mathcal{H}\cdot(\bm{\Gamma}+\bm{ \Gamma}_{0})^{-1}\big{)}\]

_and assume that \(\mathcal{H}\succeq 0\) and Assumption 10 holds with \(\alpha=1/2\) and Assumption 11 holds. Fix \(N,K\), let \(T:=(N+1)K\), and consider running Algorithm 4 on this objective and with these choices of \(N\) and \(K\). Then Algorithm 4 will run for at most \(T\) episodes, and, with probability at least \(1-\delta\), will return data satisfying_

\[\operatorname{tr}\left(\mathcal{H}\left(\sum_{t=1}^{T}\bm{\psi}( \bm{\tau}_{t})+T\bm{\Gamma}_{0}\right)^{-1}\right)\leq\frac{1}{T}\cdot\min_{ \bm{\Gamma}\in\bm{\Omega}_{\bm{\psi}}}\operatorname{tr}\big{(}\mathcal{H}( \bm{\Gamma}+\bm{\Gamma}_{0})^{-1}\big{)}+\frac{8D^{4}\|\mathcal{H}\|_{\rm op} \|\bm{\Gamma}_{0}^{-1}\|_{\rm op}^{3}}{T(N+1)}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\frac{8D \|\mathcal{H}\|_{\rm op}\|\bm{\Gamma}_{0}^{-1}\|_{\rm op}^{2}(\log^{1/2}\frac{ 4T}{\delta}+C_{\mathcal{R}}\log^{p_{\mathcal{R}}}\frac{2T}{\delta})}{T\sqrt{K}}\]

```
1:input: objective \(\Phi\), number of episodes \(T\) (OR number of iterates \(N\), episodes per iterate \(K\)), confidence \(\delta\), regret minimization algorithm \(\mathbb{A}_{\mathcal{R}}\), exploration policies \(\Pi_{\rm exp}\)
2:Play any policy \(\pi_{\rm exp}\in\Pi_{\rm exp}\) for \(K\) episodes, collect trajectories \(\mathfrak{D}_{0}=\{\bm{\tau}_{k}^{0}\}_{k=1}^{K}\), set \(\bm{\Gamma}_{0}\gets K^{-1}\sum_{k=1}^{K}\bm{\psi}(\bm{\tau}_{k}^{0})\)
3:for\(n=1,2,\ldots,N\)do
4: Set \(\gamma_{n}\leftarrow\frac{1}{n+1}\)
5: Run \(\mathbb{A}_{\mathcal{R}}\) on cost \[\operatorname{cost}_{h}^{n}(\bm{\tau})\leftarrow\frac{1}{M}\langle\Xi_{n},\bm{ \psi}_{h}(\bm{x}_{h},\bm{u}_{h})\rangle\quad\text{for}\quad\Xi_{n}\leftarrow \nabla_{\bm{\Gamma}}\Phi(\bm{\Gamma})|_{\bm{\Gamma}=\bm{\Gamma}_{n-1}}\] for \(K\) episodes, collect trajectories \(\mathfrak{D}_{n}=\{\bm{\tau}_{k}^{n}\}_{k=1}^{K}\), denote policies run as \(\Pi_{n}\)
6:\(\bm{\Gamma}_{n}\leftarrow(1-\gamma_{n})\bm{\Gamma}_{n-1}+\gamma_{n}K^{-1} \sum_{k=1}^{K}\bm{\psi}(\bm{\tau}_{k}^{n})\)
7:return\((N+1)K\bm{\Gamma}_{N},\cup_{n=0}^{N}\Pi_{n}\), \(\cup_{n=0}^{N}\mathfrak{D}_{n}\) ```

**Algorithm 4** Dynamic Optimal Experiment Design (DynamicOED)

### Proof of Theorem 3 and Theorem 5

**Lemma C.1** (Lemma C.1 of [56]).: _Consider running Algorithm 5 with some convex function \(f\) that is \(\beta\)-smooth with respect to some norm \(\|\cdot\|\), assume that \(\bm{y}_{n}\in\mathcal{Y}\) for some \(\mathcal{Y}\) and all \(n\), and let \(R:=\sup_{\bm{z},\bm{y}\in\mathcal{Z}\cup\mathcal{Y}}\|\bm{z}-\bm{y}\|\). Then for \(N\geq 2\), we have_

\[f(\bm{z}_{N+1})-\min_{\bm{z}\in\mathcal{Z}}f(\bm{z})\leq\frac{\beta R^{2}(\log N +1)}{2(N+1)}+\frac{1}{N+1}\sum_{n=1}^{N}\epsilon_{n}.\]

**Lemma C.2** (Lemma C.2 of [56]).: _When running Algorithm 5, we have_

\[\bm{z}_{N+1}=\frac{1}{N+1}\left(\sum_{n=1}^{N}\bm{y}_{n}+\bm{z}_{1}\right).\]

Proof of Theorem 3.: By our assumption on \(\mathbb{A}_{\mathcal{R}}\), Assumption 8, we have that, at round \(n\), with probability at least \(1-\delta/2N\),

\[\sum_{k=1}^{K}\mathbb{E}_{\pi_{k}}[\cos^{n}(\bm{\tau}_{k})]-K\cdot\inf_{\pi\in \Pi_{\exp}}\mathbb{E}_{\pi}\left[\cos^{n}(\bm{\tau})\right]\leq C_{\mathcal{ R}}\log^{p_{\mathcal{R}}}\frac{2NK}{\delta}\cdot K^{\alpha}\]

where we have used that, under Assumption 9 and by the definition of \(\cos^{n}_{h}(\bm{\tau})\), \(|\cos^{n}(\bm{\tau})|\leq 1\) for all \(\bm{\tau}\in\mathcal{T}\). This implies that

\[\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}_{\pi_{k}}[\langle\Xi_{n},\bm{\psi}(\bm{ \tau}_{k})\rangle]\leq M\cdot\inf_{\pi\in\Pi_{\exp}}\mathbb{E}_{\pi}\left[\cos ^{n}(\bm{\tau})\right]+MC_{\mathcal{R}}\log^{p_{\mathcal{R}}}\frac{2NK}{\delta }\cdot K^{\alpha-1}.\]

Furthermore, by Azuma-Hoeffding and under Assumption 9, we have that, with probability at least \(1-\delta/2N\),

\[\left|\frac{1}{K}\sum_{k=1}^{K}\langle\Xi_{n},\bm{\psi}(\bm{\tau}_{k}) \rangle-\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}_{\pi_{k}}[\langle\Xi_{n},\bm{\psi }(\bm{\tau}_{k})\rangle]\right|\leq\sqrt{\frac{8M^{2}\log(4N/\delta)}{K}}.\]

This implies that

\[\frac{1}{K}\sum_{k=1}^{K}\langle\Xi_{n},\bm{\psi}(\bm{\tau}_{k})\rangle\leq M \cdot\inf_{\pi\in\Pi_{\exp}}\mathbb{E}_{\pi}\left[\cos^{n}(\bm{\tau})\right] +M\cdot\left(C_{\mathcal{R}}\log^{p_{\mathcal{R}}}\frac{K}{\delta}\cdot K^{ \alpha-1}+\sqrt{\frac{8\log(4N/\delta)}{K}}\right).\] (C.1)

Note that

\[\langle\Xi_{n},\bm{\psi}(\bm{\tau}_{k})\rangle=\langle\nabla_{\bm{\Gamma}}\Phi (\bm{\Gamma})|_{\bm{\Gamma}=\bm{\Gamma}_{n}},\bm{\psi}(\bm{\tau})\rangle,\]

and that for any \(\bm{\Gamma}\in\bm{\Omega}_{\bm{\psi}}\), we have

\[\langle\nabla_{\bm{\Gamma}}\Phi(\bm{\Gamma})|_{\bm{\Gamma}=\bm{\Gamma}_{n}}, \bm{\Gamma}\rangle=\mathbb{E}_{\pi\sim\omega}[\mathbb{E}_{\pi}[\langle\nabla_ {\bm{\Gamma}}\Phi(\bm{\Gamma})|_{\bm{\Gamma}=\bm{\Gamma}_{n}},\bm{\psi}(\bm{ \tau})\rangle]]\]

for some \(\omega\). This implies that

\[\inf_{\bm{\Gamma}\in\bm{\Omega}_{\bm{\psi}}}\langle\nabla_{\bm{ \Gamma}}\Phi(\bm{\Gamma})|_{\bm{\Gamma}=\bm{\Gamma}_{n}},\bm{\Gamma}\rangle =\inf_{\omega\in\Delta_{\Pi}}\mathbb{E}_{\pi\sim\omega}[\mathbb{ E}_{\pi}[\langle\nabla_{\bm{\Gamma}}\Phi(\bm{\Gamma})|_{\bm{\Gamma}=\bm{ \Gamma}_{n}},\bm{\psi}(\bm{\tau})\rangle]]\] \[=\inf_{\pi\in\Pi_{\exp}}\mathbb{E}_{\pi}[\langle\Xi_{n},\bm{\psi }(\bm{\tau})\rangle]\]\[=M\cdot\inf_{\pi\in\Pi_{\exp}}\mathbb{E}_{\pi}[\mathrm{cost}^{n}( \boldsymbol{\tau})].\]

By (C.1) above, we have that

\[\frac{1}{K}\sum_{k=1}^{K}\boldsymbol{\psi}(\boldsymbol{\tau}_{k})\]

is an approximate minimizer of \(M\cdot\sup_{\pi\in\Pi_{\exp}}\mathbb{E}_{\pi}[\mathrm{cost}^{n}(\boldsymbol{ \tau})]\), with approximation tolerance \(M(C_{\mathcal{R}}\log^{p_{\mathcal{R}}}\frac{2NK}{\delta}\cdot K^{\alpha-1}+ \sqrt{\frac{8\log(4N/\delta)}{K}})\). We can therefore apply Lemma C.1 with

\[\epsilon_{n}=M\cdot\left(C_{\mathcal{R}}\log^{p_{\mathcal{R}}}\frac{K}{\delta} \cdot K^{\alpha-1}+\sqrt{\frac{8\log(4N/\delta)}{K}}\right)\]

to get that

\[\Phi(\boldsymbol{\Gamma}_{N+1})-\min_{\boldsymbol{\Gamma}\in \boldsymbol{\Omega}_{\boldsymbol{\psi}}}\Phi(\boldsymbol{\Gamma})\leq\frac{ \beta R^{2}(\log N+1)}{2(N+1)}+M\cdot\left(C_{\mathcal{R}}\log^{p_{\mathcal{R }}}\frac{2NK}{\delta}\cdot K^{\alpha-1}+\sqrt{\frac{8\log(4N/\delta)}{K}} \right).\]

The result then follows since \(\boldsymbol{\Gamma}_{N+1}=\frac{1}{K(N+1)}\sum_{n=0}^{N}\sum_{k=1}^{K} \boldsymbol{\psi}(\boldsymbol{\tau}_{k}^{n})\) by Lemma C.2.

Proof of Corollary 1.: By Theorem 5, for any setting of \(N\) and \(K\), we have that with probability at least \(1-\delta\):

\[\mathrm{tr}\left(\mathcal{H}\left(\frac{1}{K(N+1)}\sum_{n=0}^{N} \sum_{k=1}^{K}\boldsymbol{\psi}(\boldsymbol{\tau}_{k}^{n})+\boldsymbol{\Gamma }_{0}\right)^{-1}\right)-\min_{\boldsymbol{\Gamma}\in\boldsymbol{\Omega}} \mathrm{tr}\left(\mathcal{H}(\boldsymbol{\Gamma}+\boldsymbol{\Gamma}_{0})^{-1}\right)\] \[\qquad\leq\frac{\beta R^{2}\log N}{N+1}+\frac{MC_{\mathcal{R}} \log^{p_{\mathcal{R}}}\frac{2NK}{\delta}}{K^{1-\alpha}}+M\sqrt{\frac{8\log\frac {4N}{\delta}}{K}}\]

which implies

\[\mathrm{tr}\left(\mathcal{H}\left(\sum_{n=0}^{N}\sum_{k=1}^{K} \boldsymbol{\psi}(\boldsymbol{\tau}_{k}^{n})+T\boldsymbol{\Gamma}_{0}\right)^ {-1}\right)-\frac{\min_{\boldsymbol{\Gamma}\in\boldsymbol{\Omega}}\mathrm{tr} \left(\mathcal{H}(\boldsymbol{\Gamma}+\boldsymbol{\Gamma}_{0})^{-1}\right)}{T}\] \[\qquad\leq\frac{\beta R^{2}\log N}{T(N+1)}+\frac{MC_{\mathcal{R} }\log^{p_{\mathcal{R}}}\frac{2NK}{\delta}}{T\sqrt{K}}+M\frac{1}{T}\sqrt{\frac{ 8\log\frac{4N}{\delta}}{K}}\]

This gives

\[\mathrm{tr}\left(\mathcal{H}\left(\sum_{n=0}^{N}\sum_{k=1}^{K} \boldsymbol{\psi}(\boldsymbol{\tau}_{k}^{n})+T\boldsymbol{\Gamma}_{0}\right)^ {-1}\right) \leq\frac{\min_{\boldsymbol{\Gamma}\in\boldsymbol{\Omega}}\mathrm{ tr}\left(\mathcal{H}(\boldsymbol{\Gamma}+\boldsymbol{\Gamma}_{0})^{-1}\right)}{T}\] \[\qquad+\frac{\beta R^{2}\log T}{T(N+1)}+\frac{M(3\log^{1/2}\frac{ 4T}{\delta}+C_{\mathcal{R}}\log^{p_{\mathcal{R}}}\frac{2T}{\delta})}{T\sqrt{K }}.\]

It then remains to bound \(R,\beta,\) and \(M\). By Lemma D.6 of [56], we have that

\[\nabla_{\boldsymbol{\Gamma}}\Phi(\boldsymbol{\Gamma})[\widetilde{ \boldsymbol{\Gamma}}] =-\mathrm{tr}\left(\mathcal{H}(\boldsymbol{\Gamma}+\boldsymbol{ \Gamma}_{0})^{-1}\widetilde{\boldsymbol{\Gamma}}(\boldsymbol{\Gamma}+ \boldsymbol{\Gamma}_{0})^{-1}\right).\]

We can then compute the second derivative as, using Lemma D.6 of [56]:

\[\nabla_{\boldsymbol{\Gamma}}^{2}\Phi(\boldsymbol{\Gamma})[ \widetilde{\boldsymbol{\Gamma}},\bar{\boldsymbol{\Gamma}}] =\frac{\mathrm{d}}{\mathrm{d}t}\left[-\mathrm{tr}\left(\mathcal{H}( \boldsymbol{\Gamma}+\boldsymbol{\Gamma}_{0}+t\bar{\boldsymbol{\Gamma}})^{-1} \widetilde{\boldsymbol{\Gamma}}(\boldsymbol{\Gamma}+\boldsymbol{\Gamma}_{0}+t \bar{\boldsymbol{\Gamma}})^{-1}\right)\right]\] \[=\mathrm{tr}\left(\mathcal{H}(\boldsymbol{\Gamma}+\boldsymbol{ \Gamma}_{0})^{-1}\bar{\boldsymbol{\Gamma}}(\boldsymbol{\Gamma}+\boldsymbol{ \Gamma}_{0})^{-1}\widetilde{\boldsymbol{\Gamma}}(\boldsymbol{\Gamma}+ \boldsymbol{\Gamma}_{0})^{-1}\right)\]\[+\operatorname{tr}\left(\mathcal{H}(\boldsymbol{\Gamma}+\boldsymbol{ \Gamma}_{0})^{-1}\widetilde{\boldsymbol{\Gamma}}(\boldsymbol{\Gamma}+\boldsymbol{ \Gamma}_{0})^{-1}\widetilde{\boldsymbol{\Gamma}}(\boldsymbol{\Gamma}+ \boldsymbol{\Gamma}_{0})^{-1}\right).\]

Recall that \(M\) is any bound on

\[\sup_{\boldsymbol{\Gamma}\in\widehat{\boldsymbol{\Omega}}_{\boldsymbol{\psi}}} \sup_{\boldsymbol{\tau}\in\mathcal{T}}|\langle\nabla_{\boldsymbol{\Gamma}} \Phi(\boldsymbol{\Gamma}),\boldsymbol{\psi}(\boldsymbol{\tau})\rangle|.\]

By the above computation of the gradient, we can bound this as

\[\begin{split}\sup_{\boldsymbol{\Gamma}\in\widehat{\boldsymbol{ \Omega}}_{\boldsymbol{\psi}}}\sup_{\boldsymbol{\tau}\in\mathcal{T}}|\langle \nabla_{\boldsymbol{\Gamma}}\Phi(\boldsymbol{\Gamma}),\boldsymbol{\psi}( \boldsymbol{\tau})\rangle|&\leq\sup_{\boldsymbol{\Gamma}\in \widehat{\boldsymbol{\Omega}}_{\boldsymbol{\psi}}}\sup_{\boldsymbol{\tau}\in \mathcal{T}}\left|\operatorname{tr}\left(\mathcal{H}(\boldsymbol{\Gamma}+ \boldsymbol{\Gamma}_{0})^{-1}\boldsymbol{\psi}(\boldsymbol{\tau})( \boldsymbol{\Gamma}+\boldsymbol{\Gamma}_{0})^{-1}\right)\right|\\ &\leq\|\mathcal{H}\|_{\mathrm{op}}\|\boldsymbol{\Gamma}_{0}^{-1} \|_{\mathrm{op}}^{2}\cdot\sup_{\boldsymbol{\tau}\in\mathcal{T}}\operatorname{ tr}(\boldsymbol{\psi}(\boldsymbol{\tau}))\\ &\leq D\|\mathcal{H}\|_{\mathrm{op}}\|\boldsymbol{\Gamma}_{0}^{-1 }\|_{\mathrm{op}}^{2}.\end{split}\] (C.2)

To bound \(\beta\), by the Mean Value Theorem it suffices to bound the operator norm of \(\nabla_{\boldsymbol{\Gamma}}^{2}\Phi(\boldsymbol{\Gamma})\). Using the expression above, we can bound this as

\[\begin{split}\sup_{\widetilde{\boldsymbol{\Gamma}},\widetilde{ \boldsymbol{\Gamma}}\in\widehat{\boldsymbol{\Omega}}_{\boldsymbol{\psi}}}| \nabla_{\widetilde{\boldsymbol{\Gamma}}}^{2}\Phi(\boldsymbol{\Gamma})[ \boldsymbol{\psi}(\boldsymbol{\tau}_{1}),\boldsymbol{\psi}(\boldsymbol{\tau}_ {2})]|&\leq 2\|\mathcal{H}\|_{\mathrm{op}}\|\boldsymbol{\Gamma}_{0}^{-1}\|_{ \mathrm{op}}^{3}\cdot\sup_{\widetilde{\boldsymbol{\Gamma}},\widetilde{ \boldsymbol{\Gamma}}\in\widehat{\boldsymbol{\Omega}}_{\boldsymbol{\psi}}} \operatorname{tr}(\widetilde{\boldsymbol{\Gamma}}\widetilde{\boldsymbol{ \Gamma}})\\ &\leq 2D^{2}\|\mathcal{H}\|_{\mathrm{op}}\|\boldsymbol{\Gamma}_{0}^{-1} \|_{\mathrm{op}}^{3}.\end{split}\]

Finally, it's straightforward to bound \(R\leq 2D\). Putting all of this together gives the result. 

### Collecting Full-Rank Data

```
1:input: scale \(N\), confidence \(\delta\), regret minimization algorithm \(\mathbb{A}_{\mathcal{R}}\), exploration policies \(\Pi_{\mathrm{exp}}\)
2:for\(j=1,2,3,\ldots\)do
3:\(N_{j}\leftarrow\lceil 2^{j/3}\rceil-1,K_{j}\leftarrow\lceil 2^{2j/3}\rceil,T_{j} \leftarrow(N_{j}+1)K_{j},\lambda_{j}\gets T_{j}^{-1/18},\delta_{j} \leftarrow\frac{\delta}{4j^{2}}\)
4:\(\boldsymbol{\Sigma}_{j},\Pi_{j}\leftarrow\textsc{DynamicOED}(\Phi,N_{j},K_{j}, \delta_{j},\mathbb{A}_{\mathcal{R}},\Pi_{\mathrm{exp}})\) for \(\Phi(\boldsymbol{\Gamma})=\operatorname{tr}((\boldsymbol{\Gamma}+\lambda_{j} \cdot I)^{-1})\)
5:if\(\lambda_{\min}(\boldsymbol{\Sigma}_{j})\geq 12544D_{\boldsymbol{\phi}}\log\frac{2N(2+32T_{j})}{\delta}\)then
6:break
7:return\(\Pi_{j}\) ```

**Algorithm 6** Minimum Eigenvalue Maximization (MinEig)

In this section, we consider the setting where \(\boldsymbol{\psi}(\boldsymbol{\tau})\in\mathcal{S}_{+}^{d_{\boldsymbol{\psi}}}\), and our goal is to collect \(\{\boldsymbol{\tau}_{t}\}_{t=1}^{T}\) such that \(\boldsymbol{\psi}(\frac{1}{T}\sum_{t=1}^{T}\boldsymbol{\psi}(\boldsymbol{\tau}_ {t}))>0\). For this to be achievable, we need the following assumption, a generalization of Assumption 3.

**Assumption 12** (Full-Rank Data).: _Consider \(\boldsymbol{\psi}(\boldsymbol{\tau})\) such that \(\boldsymbol{\psi}(\boldsymbol{\tau})\in\mathbb{S}_{+}^{d_{\boldsymbol{\psi}}}\). Then we have \(\sup_{\boldsymbol{\Gamma}\in\boldsymbol{\Omega}_{\boldsymbol{\psi}}}\lambda_{ \min}(\boldsymbol{\Gamma})\geq\lambda_{\min}^{\star}\) for some \(\lambda_{\min}^{\star}>0\)._

Throughout this section we also assume that Assumption 10 is satisfied with \(\alpha=1/2\) (though all results generalize in a straightforward way for \(\alpha\neq 1/2\)). We have the following result.

**Lemma C.3**.: _Under Assumptions 10 to 12, running Algorithm 6 we have that with probability at least \(1-\delta\), it will terminate after collecting at most_

\[\mathrm{poly}\left(d_{\boldsymbol{\psi}},\frac{1}{\lambda_{\min}^{\star}},D,C_{ \mathcal{R}},\log^{p_{\mathcal{R}}}\frac{N}{\delta}\right)\]

_episodes, and return policy set \(\Pi\) such that_

\[\lambda_{\min}\left(\sum_{\pi\in\Pi}\boldsymbol{\Gamma}_{\pi}\right)\geq 6272Dd_{ \boldsymbol{\psi}}\log\frac{68N}{\delta}.\]

_Furthermore, if we rerun each policy in \(\Pi\) once, the resulting features \(\boldsymbol{\Sigma}\) will satisfy, with probability at least \(1-\delta/N\):_

\[\lambda_{\min}\left(\boldsymbol{\Sigma}\right)\geq 6272Dd_{\boldsymbol{\psi}} \log\frac{68N}{\delta}.\]Proof.: By Lemma C.4 and our choice of \(N_{j}\) and \(K_{j}\) in Algorithm 6, we have that if \(\lambda_{j}\leq\frac{\lambda_{\min}^{*}}{4d_{\bm{\psi}}}\) and

\[T_{j}^{1/3}\geq\widetilde{\Omega}\left(\left(D\lambda_{j}^{-2}(D^{3}\lambda_{j }^{-1}+C_{\mathcal{R}}\cdot\log^{p_{\mathcal{R}}}\frac{1}{\delta_{j}})\right) \cdot\frac{\lambda_{\min}^{*}}{d_{\bm{\psi}}}\right),\] (C.3)

then \(\lambda_{\min}(\bm{\Sigma}_{j})\geq\frac{\lambda_{\min}^{*}}{4d_{\bm{\psi}}} \cdot T_{j}\) with probability at least \(1-\delta_{j}\). It follows that, with probability at least \(1-\delta_{j}\), the if statement on Line 5 will be true once \(\lambda_{j}\leq\frac{\lambda_{\min}^{*}}{4d_{\bm{\psi}}}\), (C.3) holds, and

\[\frac{\lambda_{\min}^{*}}{4d_{\bm{\psi}}}\cdot T_{j}\geq 12544Dd_{\bm{\psi} }\log\frac{2N(2+32T_{j})}{\delta}.\] (C.4)

By our choice of \(\lambda_{j}=T_{j}^{-1/18}\), a sufficient condition to ensure \(\lambda_{j}\leq\frac{\lambda_{\min}^{*}}{4d_{\bm{\psi}}}\), (C.3), and (C.4) is

\[T_{j}\geq\widetilde{\Omega}\left(\max\left\{\left(\frac{d_{\bm{\psi}}}{ \lambda_{\min}^{*}}\right)^{18},\left(\frac{D^{4}\lambda_{\min}^{*}}{d_{\bm{ \psi}}}\right)^{6},\left(DC_{\mathcal{R}}\cdot\log^{p_{\mathcal{R}}}\frac{1}{ \delta_{j}}\cdot\frac{\lambda_{\min}^{*}}{d_{\bm{\psi}}}\right)^{9/2},\frac{Dd _{\bm{\psi}}^{2}}{\lambda_{\min}^{*}}\cdot\log\frac{NT_{j}}{\delta}\right\} \right).\]

Since \(T_{j}=\lceil 2^{j/3}\rceil\lceil 2^{2j/3}\rceil\in[2^{j},4\cdot 2^{j}]\), it follows that the if statement on Line 5 will be met after running for at most

\[\widetilde{\mathcal{O}}\left(\max\left\{\left(\frac{d_{\bm{\psi}}}{ \lambda_{\min}^{*}}\right)^{18},\left(\frac{D^{4}\lambda_{\min}^{*}}{d_{\bm{ \psi}}}\right)^{6},\left(DC_{\mathcal{R}}\cdot\log^{p_{\mathcal{R}}}\frac{1}{ \delta_{j}}\cdot\frac{\lambda_{\min}^{*}}{d_{\bm{\psi}}}\right)^{9/2},\frac{Dd _{\bm{\psi}}^{2}}{\lambda_{\min}^{*}}\cdot\log\frac{N}{\delta}\right\}\right)\] (C.5)

episodes.

By Lemma C.5, if \(\lambda_{\min}(\bm{\Sigma}_{j})\geq 12544Dd_{\bm{\psi}}\log\frac{2N(2+32T_{j})} {\delta}\) and we rerun all policies in \(\Pi_{j}\), then we will collect data \(\widetilde{\bm{\Sigma}}\) such that \(\lambda_{\min}(\widetilde{\bm{\Sigma}})\geq\frac{1}{2}\lambda_{\min}(\bm{ \Sigma}_{j})\), with probability at least \(1-\delta/2N\). As the if statement on Line 5 will only be true once this is met, it follows that, with probability at least \(1-\delta/2N\), rerunning all policies in \(\Pi_{j}\) once, we will collect data \(\bm{\Sigma}\) which satisfies

\[\lambda_{\min}(\bm{\Sigma})\geq\frac{1}{2}\lambda_{\min}(\bm{ \Sigma}_{j})\geq 6272Dd_{\bm{\psi}}\log\frac{2N(2+32T_{j})}{\delta}\geq 6272Dd_{\bm{ \psi}}\log\frac{68N}{\delta}.\]

The lower bound on \(\lambda_{\min}(\sum_{\pi\in\Pi}\bm{\Gamma}_{\pi})\) follows analogously from Lemma C.5.

The result then follows noting that the failure probability of running DynamicOED is at most

\[\sum_{j=1}^{\infty}\frac{\delta}{4j^{2}}\leq\delta/2.\]

#### c.2.1 Supporting Lemmas

**Lemma C.4**.: _Under Assumptions 10 to 12, consider running DynamicOED on the objective_

\[\Phi(\bm{\Gamma})=\mathrm{tr}((\bm{\Gamma}+\lambda\cdot I)^{-1})\]

_with \(N=\lceil 2^{i/3}\rceil-1\) and \(K=\lceil 2^{2i/3}\rceil\), for some \(\lambda>0\) and \(i\). Let \(T:=(N+1)K\). Then if \(\lambda\leq\frac{\lambda_{\min}^{*}}{4d_{\bm{\psi}}}\) and_

\[T^{1/3}\geq\widetilde{\Omega}\left(\left(D\lambda^{-2}(D^{3} \lambda^{-1}+C_{\mathcal{R}}\cdot\log^{p_{\mathcal{R}}}\frac{1}{\delta}) \right)\cdot\frac{\lambda_{\min}^{*}}{d_{\bm{\psi}}}\right),\] (C.6)

_with probability at least \(1-\delta\),_

\[\lambda_{\min}\left(\sum_{t=1}^{T}\bm{\psi}(\bm{\tau}_{t})\right) \geq\frac{\lambda_{\min}^{*}}{4d_{\bm{\psi}}}\cdot T.\]Proof.: Applying Corollary 1 with \(\mathcal{H}=I\) and \(\boldsymbol{\Gamma}_{0}=\lambda\cdot I\), we have that, with probability at least \(1-\delta\):

\[\operatorname{tr}\left(\left(\sum_{t=1}^{T}\boldsymbol{\psi}( \boldsymbol{\tau}_{t})+T\lambda\cdot I\right)^{-1}\right) \leq\frac{1}{T}\cdot\min_{\boldsymbol{\Gamma}\in\boldsymbol{ \Omega}_{\boldsymbol{\psi}}}\operatorname{tr}((\boldsymbol{\Gamma}+\lambda \cdot I)^{-1})+\frac{8D^{4}\lambda^{-3}}{T(N+1)}+\frac{8D\lambda^{-2}(\log^{1 /2}\frac{T}{\delta}+C_{\mathcal{R}}\log^{p_{\mathcal{R}}}\frac{T}{\delta})}{T \sqrt{K}}\] \[\leq\frac{1}{T}\cdot\min_{\boldsymbol{\Gamma}\in\boldsymbol{ \Omega}_{\boldsymbol{\psi}}}\operatorname{tr}((\boldsymbol{\Gamma}+\lambda \cdot I)^{-1})+\frac{24D^{4}\lambda^{-3}}{T^{4/3}}+\frac{24D\lambda^{-2}( \log^{1/2}\frac{T}{\delta}+C_{\mathcal{R}}\log^{p_{\mathcal{R}}}\frac{T}{ \delta})}{T^{4/3}}\]

where the second inequality follows since

\[T=\lceil 2^{i/3}\rceil\lceil 2^{2i/3}\rceil\leq 4\cdot 2^{i}\]

which implies \(N+1=\lceil 2^{i/3}\rceil\geq T^{1/3}/4^{1/3}\) and \(K=\lceil 2^{2i/3}\rceil\geq T^{2/3}/4^{2/3}\). If \(T\) satisfies (C.6), then we can bound

\[\frac{24D^{4}\lambda^{-3}}{T^{4/3}}+\frac{24D\lambda^{-2}(\log^{1/2}\frac{T}{ \delta}+C_{\mathcal{R}}\log^{p_{\mathcal{R}}}\frac{T}{\delta})}{T^{4/3}}\leq \frac{1}{T}\cdot\frac{d_{\boldsymbol{\psi}}}{\lambda_{\min}^{\star}}.\]

Furthermore, under Assumption 3 there exists some \(\boldsymbol{\Gamma}\in\boldsymbol{\Omega}_{\boldsymbol{\psi}}\) such that \(\boldsymbol{\Gamma}\succeq\lambda_{\min}^{\star}\cdot I\), so we can upper bound

\[\min_{\boldsymbol{\Gamma}\in\boldsymbol{\Omega}_{\boldsymbol{\psi}}} \operatorname{tr}((\boldsymbol{\Gamma}+\lambda\cdot I)^{-1})\leq\frac{d_{ \boldsymbol{\psi}}}{\lambda_{\min}^{\star}}\]

and we can lower bound

\[\operatorname{tr}\left(\left(\sum_{t=1}^{T}\boldsymbol{\psi}( \boldsymbol{\tau}_{t})+T\lambda\cdot I\right)^{-1}\right)\geq\frac{1}{\lambda _{\min}(\sum_{t=1}^{T}\boldsymbol{\psi}(\boldsymbol{\tau}_{t}))+T\lambda}.\]

Thus,

\[\frac{1}{\lambda_{\min}(\sum_{t=1}^{T}\boldsymbol{\psi}( \boldsymbol{\tau}_{t}))+T\lambda}\leq\frac{1}{T}\cdot\frac{2d_{\boldsymbol{ \psi}}}{\lambda_{\min}^{\star}}\implies\lambda_{\min}\left(\sum_{t=1}^{T} \boldsymbol{\psi}(\boldsymbol{\tau}_{t})\right)\geq\frac{T\lambda_{\min}^{ \star}}{2d_{\boldsymbol{\psi}}}-T\lambda.\]

It follows that if \(\lambda\leq\frac{\lambda_{\min}^{\star}}{4d_{\boldsymbol{\psi}}}\), then we have

\[\lambda_{\min}\left(\sum_{t=1}^{T}\boldsymbol{\psi}(\boldsymbol{ \tau}_{t})\right)\geq T\cdot\frac{\lambda_{\min}^{\star}}{4d_{\boldsymbol{ \psi}}}\]

which proves the result. 

**Lemma C.5**.: _Consider running some policies \((\pi_{\tau})_{\tau=1}^{T}\), for \(\pi_{\tau}\)\(\mathcal{F}_{\tau-1}\)-measurable, and collecting covariance \(\boldsymbol{\Sigma}_{T}=\sum_{t=1}^{T}\boldsymbol{\psi}(\boldsymbol{\tau}_{t})\). Then under Assumption 11, as long as_

\[\lambda_{\min}(\boldsymbol{\Sigma}_{T})\geq 12544Dd_{\boldsymbol{\psi}} \log\frac{2+32T}{\delta}\]

_with probability at least \(1-\delta\), if we rerun each \((\pi_{\tau})_{\tau=1}^{T}\), we will collect features \(\widetilde{\boldsymbol{\Sigma}}_{T}\) such that_

\[\lambda_{\min}(\widetilde{\boldsymbol{\Sigma}}_{T})\geq\frac{1} {2}\lambda_{\min}(\boldsymbol{\Sigma}_{T}).\]

_Furthermore,_

\[\lambda_{\min}\left(\sum_{\tau=1}^{T}\boldsymbol{\Gamma}_{\pi_{ \tau}}\right)\geq\frac{1}{2}\lambda_{\min}(\boldsymbol{\Sigma}_{T}).\]

Proof.: This follows from applying Lemma D.7 of [56] to the matrix \(\frac{1}{D}\boldsymbol{\Sigma}_{T}\). Note that while [56] considers the setting of linear MDPs, the proof of Lemma D.7 of [56] does not make use of the linear MDP assumption, and the proof therefore extends immediately to our setting. Furthermore, though it is not explicitly stated, the lower bound on \(\lambda_{\min}(\sum_{\tau=1}^{T}\boldsymbol{\Gamma}_{\pi_{\tau}})\) is also proved in Lemma D.7 of [56].

```
1:input:\(\mathcal{H}\), iterates bound \(\widetilde{N}\), confidence \(\delta\), regret minimization algorithm \(\mathbb{A}_{\mathcal{R}}\), exploration policies \(\Pi_{\mathrm{exp}}\)
2:for\(i=1,2,3,\ldots\)do
3:\(N_{i}\leftarrow[2^{i/3}]-1,K_{i}\leftarrow[7^{2i/3}],T_{i}\leftarrow(N_{i}+1)K_{i}, \delta_{i}\leftarrow\delta/4i^{2}\)
4:\(\Pi_{\mathrm{MinEq}}^{i}\leftarrow\textsc{MinEq}(\widetilde{N}T_{i},\delta_ {i},\mathbb{A}_{\mathcal{R}},\Pi_{\mathrm{exp}})\)
5: Run policies in \(\Pi_{\mathrm{MinEq}}^{i}\left[T_{i}/|\Pi_{\mathrm{MinEq}}^{i}|\right]\) times, set \(\bm{\Gamma}_{0}^{i}\) to collected features
6:\(\Phi(\bm{\Gamma})\leftarrow\mathrm{tr}(\mathcal{H}(\bm{\Gamma}+T_{i}^{-1} \bm{\Gamma}_{0}^{i})^{-1})\)
7:\(\bm{\Gamma}_{\mathrm{fw}}^{i},\Pi_{\mathrm{fw}}^{i}\leftarrow\textsc{DynamicOED}(\Phi,N_{i},K_{i},\delta, \mathbb{A}_{\mathcal{R}},\Pi_{\mathrm{exp}})\)
8:if \[\max_{j=1,\ldots,i}|\Pi_{\mathrm{MinEq}}^{j}|\leq T_{i}\] (C.7) \[\frac{16D^{4}\|\mathcal{H}\|_{\mathrm{op}}\|(T_{i}^{-1}\bm{\Gamma }_{0})^{-1}\|_{\mathrm{op}}^{3}}{T_{i}(N_{i}+1)}+\frac{16D\|\mathcal{H}\|_{ \mathrm{op}}\|(T_{i}^{-1}\bm{\Gamma}_{0})^{-1}\|_{\mathrm{op}}^{2}(\log^{1/2} \frac{4T_{i}}{\delta}+C_{\mathcal{R}}\log^{p_{\mathcal{R}}}\frac{2T_{i}}{ \delta})}{T_{i}\sqrt{K_{i}}}\leq\mathrm{tr}\left(\mathcal{H}\left(\bm{\Gamma }_{\mathrm{fw}}^{i}+\bm{\Gamma}_{0}^{i}\right)^{-1}\right)\] (C.8) \[\mathrm{tr}(\mathcal{H})\cdot D\sqrt{2T_{i}}\sqrt{8d_{\bm{\psi}} \log(1+8\sqrt{2T_{i}})+8\log 1/\delta}\cdot\frac{2}{\lambda_{\min}\left(\bm{\Gamma }_{\mathrm{fw}}^{i}+\bm{\Gamma}_{0}^{i}\right)^{2}}\leq\mathrm{tr}\left( \mathcal{H}\left(\bm{\Gamma}_{\mathrm{fw}}^{i}+\bm{\Gamma}_{0}^{i}\right)^{-1}\right)\] (C.9) \[D\sqrt{2T_{i}}\sqrt{8d_{\bm{\psi}}\log(1+8\sqrt{2T_{i}})+8\log 1/ \delta}\leq\frac{1}{2}\lambda_{\min}\left(\bm{\Gamma}_{\mathrm{fw}}^{i}+\bm{ \Gamma}_{0}^{i}\right)\] (C.10)
9:then
10:\(\bm{\Gamma}_{\mathrm{out}}\leftarrow\bm{\Gamma}_{\mathrm{fw}}^{i}+\bm{\Gamma} _{0}^{i},\Pi_{\mathrm{out}}\leftarrow\Pi_{\mathrm{MinEq}}^{i}\cup(\cup_{j=1} ^{\left\lceil T_{i}/|\Pi_{\mathrm{MinEq}}\right\rceil}\Pi_{\mathrm{fw}}^{i})\)
11:return\(\bm{\Gamma}_{\mathrm{out}},\Pi_{\mathrm{out}}\) ```

**Algorithm 7** Learn Minimizing Exploration Policies (LearnExpII)

### Rerunning Policies

In this section, we build on the analysis of the DynamicOED algorithm to show that, not only do the features collected by DynamicOED approximately minimize \(\Phi\), but that, under certain conditions, if we rerun the policies that DynamicOED ran to collect this data, we will collect a new set of features which also approximately minimizes \(\Phi\).

In particular, we specialize this argument to objectives of the form \(\Phi(\bm{\Gamma})=\mathrm{tr}(\mathcal{H}\bm{\Gamma}^{-1})\). LearnExpII (Algorithm 7) proceeds by first calling MinEq to collect full-rank data, using this data as a regularizer of \(\Phi(\bm{\Gamma})\), and the running DynamicOED on this objective. After meeting a certain termination criteria, it terminates, and returns the policies it has run over its operation.

**Lemma C.6**.: _Let \(\mathcal{E}_{\mathrm{exp}}\) denote the event that, for all \(i=1,2,3,\ldots\), the success event of MinEig and DynamicOED occur, and_

\[\lambda_{\min}(\bm{\Gamma}_{0}^{i})\geq\lceil T_{i}/|\Pi_{\mathrm{ MinEq}}^{i}|\rceil\cdot 6272Dd_{\bm{\psi}}\log\frac{68\widetilde{N}}{\delta}.\]

_Then if Assumptions 10 to 12 hold, \(\mathbb{P}[\mathcal{E}_{\mathrm{exp}}]\geq 1-\delta\)._

**Lemma C.7**.: _Consider rerunning each policy in \(\Pi_{\mathrm{out}}\)\(N\leq\widetilde{N}\) times, and let \(\widetilde{\bm{\Gamma}}\) denote the obtained features. Then, if Assumptions 10 to 12 hold, with probability at least \(1-3\delta\), on the event \(\mathcal{E}_{\mathrm{exp}}\):_

\[\left\|\widetilde{\bm{\Gamma}}-N\cdot\sum_{\pi\in\Pi_{\mathrm{out}}}\bm{\Gamma} _{\pi}\right\|_{\mathrm{op}}\leq\frac{\sqrt{T_{\mathrm{out}}}\cdot\sqrt{8d_{ \bm{\psi}}\log(1+8\sqrt{NT_{\mathrm{out}}})+8\log 1/\delta}}{\sqrt{N}\cdot 6272d_{\bm{\psi}}\log\frac{68 \widetilde{N}}{\delta}}\cdot\lambda_{\min}\left(N\cdot\sum_{\pi\in\Pi_{ \mathrm{out}}}\bm{\Gamma}_{\pi}\right),\] (C.11)

\[N\cdot 6272Dd_{\bm{\psi}}\log\frac{68\widetilde{N}}{\delta}\leq\min\left\{ \lambda_{\min}\left(N\cdot\sum_{\pi\in\Pi_{\mathrm{out}}}\bm{\Gamma}_{\pi} \right),\lambda_{\min}(\widetilde{\bm{\Gamma}})\right\},\] (C.12)

_and_

\[\mathrm{tr}\left(\mathcal{H}\left(\sum_{\pi\in\Pi_{\mathrm{out}}}\bm{\Gamma}_{ \pi}\right)^{-1}\right)\leq\frac{12}{T_{\mathrm{out}}}\cdot\min_{\widetilde{ \bm{\Gamma}}\in\widetilde{\bm{\Omega}}}\mathrm{tr}\left(\mathcal{H}\bm{\Gamma} ^{-1}\right)\] (C.13)_for \(T_{\rm out}:=|\Pi_{\rm out}|\)._

**Lemma C.8**.: _On the event \(\mathcal{E}_{\rm exp}\), under Assumptions 10 to 12, we can bound_

\[T_{\rm out}\leq{\rm poly}\left(d_{\bm{\psi}},\frac{1}{\lambda_{ \rm min}^{\star}},D,C_{\mathcal{R}},\log^{p_{\bm{\kappa}}}\frac{\widetilde{N}}{ \delta}\right).\]

_Furthermore, the total number of episodes collected by Algorithm 7 is bounded by \((16+2\log(T_{\rm out}))\cdot T_{\rm out}\)._

#### c.3.1 Supporting Lemmas and Proofs

**Lemma C.9**.: _Under Assumption 11, for any \(\bm{\Gamma}=\mathbb{E}_{\bm{\tau}\sim\omega}[\bm{\psi}(\bm{\tau})]\) and \(\mathcal{H}\succeq 0\) we can bound_

\[{\rm tr}(\mathcal{H}\bm{\Gamma}^{-1})\geq D^{-1}\cdot{\rm tr}( \mathcal{H}).\]

Proof.: By Von Neumann's Trace Inequality we can lower bound

\[{\rm tr}(\mathcal{H}\bm{\Gamma}^{-1})\geq\lambda_{\rm min}(\bm{ \Gamma}^{-1})\cdot{\rm tr}(\mathcal{H})=\|\bm{\Gamma}\|_{\rm op}^{-1}\cdot{ \rm tr}(\mathcal{H}).\]

By our assumption that \({\rm tr}(\bm{\psi}(\bm{\tau}))\leq D\), we can bound \(\|\bm{\Gamma}\|_{\rm op}\leq D\), which proves the result. 

**Lemma C.10**.: _Assume \({\rm tr}(\bm{\psi}(\bm{\tau}))\leq D\) for all \(\bm{\tau}\). Let \(\bm{\Gamma}_{K}\) denote the time-normalized features obtained by playing policies \(\{\pi_{k}\}_{k=1}^{K}\), where \(\pi_{k}\) is \(\mathcal{F}_{k-1}\)-measurable. Then, with probability at least \(1-\delta\),_

\[\left\|\frac{1}{K}\sum_{k=1}^{K}\bm{\Gamma}_{\pi_{k}}-\bm{\Gamma }_{K}\right\|_{\rm op}\leq D\sqrt{\frac{8d_{\bm{\psi}}\log(1+8\sqrt{K})+8\log 1/ \delta}{K}}.\]

Proof.: This follows from an argument identical to the proof of Lemma C.4 of [56]. While [56] considers the setting of linear MDPs, we note that the proof of Lemma C.4 of [56] nowhere relies on the linear MDP assumption. The result stated here then follows identically as Lemma C.4 of [56], after normalizing \(\bm{\psi}(\bm{\tau})\) by \(D\). 

Proof of Lemma c.6.: By Lemma C.3, the failure probability of running MinEig at round \(i\) is \(\delta_{i}=\delta/8i^{2}\), and by Corollary 1 the failure probability of DynamicOED at round \(i\) is also bounded by \(\delta_{i}=\delta/8i^{2}\). It follows that the total failure probability of running MinEig and DynamicOED is bounded by

\[\sum_{i=1}^{\infty}\frac{2\cdot\delta}{8i^{2}}\leq\frac{\delta}{ 2}.\]

Furthermore, by Lemma C.3, we have that rerunning all policies in \(\Pi_{\text{MnEig}}^{i}\), we will obtain features \(\bm{\Gamma}\) satisfying, with probability at least \(1-\delta_{i}/\widetilde{N}T_{i}\):

\[\lambda_{\rm min}(\bm{\Gamma})\geq 6272Dd_{\bm{\psi}}\log\frac{68 \widetilde{N}}{\delta_{i}}.\]

Repeating this \(\lceil T_{i}/|\Pi_{\text{MnEig}}^{i}|\rceil\) times and union bounding, we have that

\[\lambda_{\rm min}(\bm{\Gamma}_{0}^{i})\geq\lceil T_{i}/|\Pi_{\text {MnEig}}^{i}|\rceil\cdot 6272Dd_{\bm{\psi}}\log\frac{68\widetilde{N}}{\delta_{i}}\]

with probability at least \(1-\delta/\widetilde{N}T_{i}\cdot\lceil T_{i}/|\Pi_{\text{MnEig}}^{i}|\rceil \geq 1-\delta/\widetilde{N}\). 

Proof of Lemma c.7.: Let \(\Pi_{\text{MnEig}},\bm{\Gamma}_{0},\Pi_{\text{fw}},\bm{\Gamma}_{\text{fw}}\) denote the policies and features obtained on the round at which Algorithm 7 terminates. Let \(T_{\rm fw}=|\Pi_{\text{fw}}|\) denote the number of episodes of DynamiCOED on the terminating round, and \(N_{\rm fw},K_{\rm fw}\) the corresponding values of \(N_{i}\) and \(K_{i}\). Throughout the proof we make use of the fact that at termination of Algorithm 7, all of (C.7)-(C.10) are met.

Proof of (C.11) and (C.12).By Lemma C.10 we have that, with probability at least \(1-\delta\):

\[\left\|\widetilde{\mathbf{\Gamma}}-N\cdot\sum_{\pi\in\Pi_{\mathrm{out}}} \mathbf{\Gamma}_{\pi}\right\|_{\mathrm{op}}\leq D\sqrt{NT_{\mathrm{out}}}\cdot \sqrt{8d_{\boldsymbol{\psi}}\log(1+8\sqrt{NT_{\mathrm{out}}})+8\log 1/\delta}.\]

On \(\mathcal{E}_{\mathrm{exp}}\), by Lemma C.3, we can bound

\[\left|\Pi_{\mathrm{MinEq}}\right|\leq\mathrm{poly}\left(d_{\boldsymbol{\psi}},\frac{1}{\lambda_{\min}^{*}},D,C_{\mathcal{R}},\log^{p_{\mathcal{R}}}\frac{ \widetilde{N}T_{\mathrm{out}}}{\delta}\right)\]

and, furthermore, we can lower bound

\[\lambda_{\min}\left(\sum_{\pi\in\Pi_{\mathrm{MinEq}}}\mathbf{\Gamma}_{\pi} \right)\geq 6272Dd_{\boldsymbol{\psi}}\log\frac{68\widetilde{N}}{\delta}.\]

Since \(\Pi_{\mathrm{MinEq}}\subseteq\Pi_{\mathrm{out}}\), it follows that

\[\lambda_{\min}\left(N\cdot\sum_{\pi\in\Pi_{\mathrm{out}}}\mathbf{\Gamma}_{\pi }\right)\geq N\cdot 6272Dd_{\boldsymbol{\psi}}\log\frac{68\widetilde{N}}{\delta}.\]

Combining these, we therefore have that, with probability at least \(1-\delta\):

\[\left\|\widetilde{\mathbf{\Gamma}}-N\cdot\sum_{\pi\in\Pi_{\mathrm{out}}} \mathbf{\Gamma}_{\pi}\right\|_{\mathrm{op}}\leq\frac{\sqrt{NT_{\mathrm{out}}} \cdot\sqrt{8d_{\boldsymbol{\psi}}\log(1+8\sqrt{NT_{\mathrm{out}}})+8\log 1/ \delta}}{N\cdot 6272d_{\boldsymbol{\psi}}\log\frac{68\widetilde{N}}{\delta}} \cdot\lambda_{\min}\left(N\cdot\sum_{\pi\in\Pi_{\mathrm{out}}}\mathbf{\Gamma}_ {\pi}\right).\]

In addition, also by Lemma C.3, we have that with probability at least \(1-\delta/\widetilde{N}T_{\mathrm{out}}\cdot N\geq 1-\delta\), that

\[\lambda_{\min}(\widetilde{\mathbf{\Gamma}})\geq N\cdot 6272Dd_{\boldsymbol{ \psi}}\log\frac{68\widetilde{N}}{\delta}.\]

Proof of (C.13).By Corollary 1, on \(\mathcal{E}_{\mathrm{exp}}\) we have that:

\[\Phi(\mathbf{\Gamma}_{\mathrm{fw}})=\mathrm{tr}\left(\mathcal{H} \left(\mathbf{\Gamma}_{\mathrm{fw}}+\mathbf{\Gamma}_{0}\right)^{-1}\right) \leq\frac{\min_{\mathbf{\Gamma}\in\mathbf{\Omega}}\mathrm{tr}\left(\mathcal{H }(\mathbf{\Gamma}+T_{\mathrm{fw}}^{-1}\mathbf{\Gamma}_{0})^{-1}\right)}{T_{ \mathrm{fw}}}\\ +\frac{8D^{4}\|\mathcal{H}\|_{\mathrm{op}}\|(T_{\mathrm{fw}}^{-1} \mathbf{\Gamma}_{0})^{-1}\|_{\mathrm{op}}^{3}}{T_{\mathrm{fw}}(N_{\mathrm{fw}} +1)}+\frac{8D\|\mathcal{H}\|_{\mathrm{op}}\|(T_{\mathrm{fw}}^{-1}\mathbf{ \Gamma}_{0})^{-1}\|_{\mathrm{op}}^{2}(\log^{1/2}\frac{4T_{\mathrm{fw}}}{ \delta}+C_{\mathcal{R}}\log^{p_{\mathcal{R}}}\frac{2T_{\mathrm{fw}}}{\delta}) }{T_{\mathrm{fw}}\sqrt{K_{\mathrm{fw}}}}.\]

Since \(T_{\mathrm{fw}}\) satisfies (C.8), we can bound

\[\frac{8D^{4}\|\mathcal{H}\|_{\mathrm{op}}\|(T_{\mathrm{fw}}^{-1}\mathbf{ \Gamma}_{0})^{-1}\|_{\mathrm{op}}^{3}}{T_{\mathrm{fw}}(N_{\mathrm{fw}}+1)}+ \frac{8D\|\mathcal{H}\|_{\mathrm{op}}\|(T_{\mathrm{fw}}^{-1}\mathbf{\Gamma}_ {0})^{-1}\|_{\mathrm{op}}^{2}(\log^{1/2}\frac{4T_{\mathrm{fw}}}{\delta}+C_{ \mathcal{R}}\log^{p_{\mathcal{R}}}\frac{2T_{\mathrm{fw}}}{\delta})}{T_{ \mathrm{fw}}\sqrt{K_{\mathrm{fw}}}}\leq\frac{1}{2}\Phi(\mathbf{\Gamma}_{ \mathrm{fw}}).\]

It follows that

\[\Phi(\mathbf{\Gamma}_{\mathrm{fw}})\leq\frac{\min_{\mathbf{ \Gamma}\in\mathbf{\Omega}}\mathrm{tr}\left(\mathcal{H}(\mathbf{\Gamma}+T_{ \mathrm{fw}}^{-1}\mathbf{\Gamma}_{0})^{-1}\right)}{T_{\mathrm{fw}}}+\frac{1}{ 2}\Phi(\mathbf{\Gamma}_{\mathrm{fw}})\] \[\implies \Phi(\mathbf{\Gamma}_{\mathrm{fw}})\leq 2\cdot\frac{\min_{\mathbf{ \Gamma}\in\mathbf{\Omega}}\mathrm{tr}\left(\mathcal{H}(\mathbf{\Gamma}+T_{ \mathrm{fw}}^{-1}\mathbf{\Gamma}_{0})^{-1}\right)}{T_{\mathrm{fw}}}.\] (C.14)

By Lemma C.10, we have that, with probability at least \(1-\delta\):

\[\left\|\sum_{\pi\in\Pi_{\mathrm{out}}}\mathbf{\Gamma}_{\pi}-\left(\mathbf{ \Gamma}_{\mathrm{fw}}+\mathbf{\Gamma}_{0}\right)\right\|_{\mathrm{op}}\leq D \sqrt{T_{\mathrm{out}}}\sqrt{8d_{\boldsymbol{\psi}}\log(1+8\sqrt{T_{\mathrm{ out}}})+8\log 1/\delta}.\]

Since (C.10) is satisfied and \(|\Pi_{\mathrm{MinEq}}^{i}|\leq T_{i}\), we have

\[D\sqrt{T_{\mathrm{out}}}\sqrt{8d_{\boldsymbol{\psi}}\log(1+8\sqrt{T_{\mathrm{ out}}})+8\log 1/\delta}\leq\frac{1}{2}\lambda_{\min}\left(\mathbf{\Gamma}_{\mathrm{fw}}+ \mathbf{\Gamma}_{0}\right).\]

By Lemma A.2 it follows that

\[\left\|\left(\sum_{\pi\in\Pi_{\mathrm{out}}}\mathbf{\Gamma}_{\pi}\right)^{-1}- \left(\mathbf{\Gamma}_{\mathrm{fw}}+\mathbf{\Gamma}_{0}\right)^{-1}\right\|_ {\mathrm{op}}\leq D\sqrt{T_{\mathrm{out}}}\sqrt{8d_{\boldsymbol{\psi}}\log(1+8 \sqrt{T_{\mathrm{out}}})+8\log 1/\delta}\cdot\frac{2}{\lambda_{\min}\left(\mathbf{\Gamma}_{ \mathrm{fw}}+\mathbf{\Gamma}_{0}\right)^{2}}.\]This implies that

\[\operatorname{tr}\left(\mathcal{H}\left(\sum_{\pi\in\Pi_{\mathrm{ our}}}\boldsymbol{\Gamma}_{\pi}\right)^{-1}\right) \leq\operatorname{tr}\left(\mathcal{H}\left(\boldsymbol{\Gamma}_ {\mathrm{fw}}+\boldsymbol{\Gamma}_{0}\right)^{-1}\right)\] \[+\operatorname{tr}(\mathcal{H})\cdot D\sqrt{T_{\mathrm{out}}} \sqrt{8d_{\boldsymbol{\psi}}\log(1+8\sqrt{T_{\mathrm{out}}})+8\log 1/\delta}\cdot \frac{2}{\lambda_{\min}\left(\boldsymbol{\Gamma}_{\mathrm{fw}}+\boldsymbol{ \Gamma}_{0}\right)^{2}}.\]

Now if

\[\operatorname{tr}(\mathcal{H})\cdot D\sqrt{T_{\mathrm{out}}}\sqrt{8d_{ \boldsymbol{\psi}}\log(1+8\sqrt{T_{\mathrm{out}}})+8\log 1/\delta}\cdot\frac{2}{ \lambda_{\min}\left(\boldsymbol{\Gamma}_{\mathrm{fw}}+\boldsymbol{\Gamma}_{0} \right)^{2}}\leq\operatorname{tr}\left(\mathcal{H}\left(\boldsymbol{\Gamma}_ {\mathrm{fw}}+\boldsymbol{\Gamma}_{0}\right)^{-1}\right),\] (C.15)

we can bound this all by

\[\leq 2\operatorname{tr}\left(\mathcal{H}\left(\boldsymbol{\Gamma}_ {\mathrm{fw}}+\boldsymbol{\Gamma}_{0}\right)^{-1}\right)\leq\frac{4}{T_{ \mathrm{fw}}}\cdot\min_{\boldsymbol{\Gamma}\in\boldsymbol{\Omega}} \operatorname{tr}\left(\mathcal{H}(\boldsymbol{\Gamma}+T_{\mathrm{fw}}^{-1} \boldsymbol{\Gamma}_{0})^{-1}\right)\]

where the last inequality follows from (C.14). However, note that (C.15) since (C.9) holds. Finally, note that

\[T_{\mathrm{out}}=T_{\mathrm{fw}}+\lceil T_{\mathrm{fw}}/\rceil\Pi_{\mathrm{ MinEq}}\rceil\lvert\lvert\Pi_{\mathrm{MinEq}}\rvert\leq 2T_{\mathrm{fw}}+ \lvert\Pi_{\mathrm{MinEq}}\rvert\leq 3T_{\mathrm{fw}}\]

where the last inequality follows since (C.7) holds. We can therefore upper bound \(\frac{4}{T_{\mathrm{fw}}}\leq\frac{12}{T_{\mathrm{out}}}\). Putting this together proves the result.

Proof of Lemma c.8.: To bound \(T_{\mathrm{out}}\), it suffices to show that (C.7)-(C.10) are satisfied for sufficiently large \(T_{i}\).

On \(\mathcal{E}_{\mathrm{exp}}\), by Lemma C.3, we can bound

\[\lvert\Pi_{\mathrm{MinEq}}^{i}\rvert\leq\operatorname{poly}\left(d_{ \boldsymbol{\psi}},\frac{1}{\lambda_{\min}^{\star}},D,C_{\mathcal{R}},\log^{p_ {\mathcal{R}}}\frac{\widetilde{N}T_{i}}{\delta}\right),\]

so to ensure (C.7) is met it suffices that

\[T_{i}\geq\operatorname{poly}\left(d_{\boldsymbol{\psi}},\frac{1}{\lambda_{ \min}^{\star}},D,C_{\mathcal{R}},\log^{p_{\mathcal{R}}}\frac{\widetilde{N}}{ \delta}\right).\]

On \(\mathcal{E}_{\mathrm{exp}}\), we have

\[\lambda_{\min}(\boldsymbol{\Gamma}_{\mathrm{fw}}^{i}+\boldsymbol{\Gamma}_{0} ^{i})\geq\lambda_{\min}(\boldsymbol{\Gamma}_{0}^{i})\geq\lceil T_{i}/\lvert \Pi_{\mathrm{MinEq}}^{i}\rvert\rceil\cdot 6272Dd_{\boldsymbol{\psi}}\log\frac{68 \widetilde{N}}{\delta}.\]

Which also implies

\[\lVert(T_{i}^{-1}\boldsymbol{\Gamma}_{0}^{i})^{-1}\rVert_{\mathrm{op}}=\frac{ T_{i}}{\lambda_{\min}(\boldsymbol{\Gamma}_{0}^{i})}\leq\frac{T_{i}}{\lceil T_{i}/ \lvert\Pi_{\mathrm{MinEq}}^{i}\rvert}\cdot\frac{1}{6272Dd_{\boldsymbol{\psi}} \log\frac{68\widetilde{N}}{\delta}}\leq\frac{\lvert\Pi_{\mathrm{MinEq}}^{i} \rvert}{6272Dd_{\boldsymbol{\psi}}\log\frac{68\widetilde{N}}{\delta}}\]

Furthermore, by Lemma C.9 we can lower bound

\[\operatorname{tr}\left(\mathcal{H}\left(\boldsymbol{\Gamma}_{ \mathrm{fw}}^{i}+\boldsymbol{\Gamma}_{0}^{i}\right)^{-1}\right)\geq\frac{ \operatorname{tr}(\mathcal{H})}{D(T_{i}+\lceil T_{i}/\lvert\Pi_{\mathrm{MinEq }}^{i}\rvert\lvert\lvert\Pi_{\mathrm{MinEq}}^{i}\rvert)}\geq\frac{ \operatorname{tr}(\mathcal{H})}{3DT_{i}}.\]

Combining these and using that \(N_{i}=\mathcal{O}(T_{i}^{1/3})\) and \(K_{i}=\mathcal{O}(T_{i}^{2/3})\), it is easy to see that (C.8)-(C.10) will be met once

\[T_{i}\geq\operatorname{poly}\left(d_{\boldsymbol{\psi}},\frac{1}{\lambda_{ \min}^{\star}},D,C_{\mathcal{R}},\log^{p_{\mathcal{R}}}\frac{\widetilde{N}}{ \delta}\right).\]

The bound on \(T_{\mathrm{out}}\) then follows since \(T_{i}=\lceil 2^{i/3}\rceil\lceil 2^{2i/3}\rceil\in[2^{i},4\cdot 2^{i}]\), so it can be at most a constant larger than the sufficient condition before terminating.

Let \(i^{*}\) denote the round that Algorithm 7 terminates on. Note that at round \(i\), MinEig runs for at most \(|\Pi^{i}_{\text{MnEig}}|\), DynamicOED runs for at most \(T_{i}\) episodes, and we run for an additional \(\lceil T_{i}/|\Pi^{i}_{\text{MnEig}}|\rceil\cdot|\Pi^{i}_{\text{MnEig}}|\) episodes on Line 5. In total, then, the number of episodes Algorithm 7 runs for is bounded by

\[\sum_{i=1}^{i^{*}}(T_{i}+|\Pi^{i}_{\text{MnEig}}|+\lceil T_{i}/| \Pi^{i}_{\text{MnEig}}|\rceil\cdot|\Pi^{i}_{\text{MnEig}}|) \leq 2\sum_{i=1}^{i^{*}}(T_{i}+|\Pi^{i}_{\text{MnEig}}|)\] \[\leq 16T_{i^{*}}+2\sum_{i=1}^{i^{*}}|\Pi^{i}_{\text{MnEig}}|\]

where the last inequality follows since \(T_{i}\in[2^{i},4\cdot 2^{i}]\). Now note that, since Algorithm 7 only terminates once (C.7) is met, we will have \(\max_{j=1,\ldots,i^{*}}|\Pi^{j}_{\text{MnEig}}|\leq T_{i^{*}}\). This implies that \(2\sum_{i=1}^{i^{*}}|\Pi^{i}_{\text{MnEig}}|\leq 2i^{*}T_{i^{*}}\leq 2\log(T_{i^{*}} )\cdot T_{i^{*}}\). Bounding \(T_{i^{*}}\leq T_{\text{out}}\) gives the result.

## Appendix D Smooth Nonlinear Systems

In this section we restrict to the nonlinear regulator system of (1.1). Our goal will be to show that, under our assumptions, the nonlinear regulator system exhibits certain smooth behavior. As we have assumed

\[\Pi^{*}=\{\pi^{\boldsymbol{\theta}}\;:\;\boldsymbol{\theta}\in\mathbb{R}^{d_{ \boldsymbol{\theta}}}\},\]

it will be convenient to define \(\mathcal{J}(\boldsymbol{\theta};A):=\mathcal{J}(\pi^{\boldsymbol{\theta}};A)\) and \(\boldsymbol{\theta}_{*}(A)=\boldsymbol{\theta}_{*}\). For the remainder of this section, we will typically use \(\boldsymbol{\theta}\) in place of \(\pi^{\boldsymbol{\theta}}\). In addition, when considering radius terms such as \(r_{\boldsymbol{\theta}}(A_{*})\) and \(r_{\mathrm{cost}}(A_{*})\), to simplify results we assume that \(r_{\boldsymbol{\theta}}(A_{*})\leq 1\) and \(r_{\mathrm{cost}}(A_{*})\leq 1\). Note that this does not change the validity of the result since, for example, if a result holds with \(A\in\mathcal{B}_{\mathrm{F}}(A_{*};r)\) for some \(r>r_{\boldsymbol{\theta}}(A_{*})\), it also holds for \(A\in\mathcal{B}_{\mathrm{F}}(A_{*};r_{\boldsymbol{\theta}}(A_{*}))\). Throughout this section, we let \(\nabla_{x}f(x)[\Delta]\) refer to the directional gradient of \(f(x)\) in direction \(\Delta\).

We first have the following result, which shows that under our assumptions, the controller loss is differentiable.

**Lemma D.1**.: _Under Assumptions 1, 2, 4 and 5, for any \(A\) satisfying \(A\in\mathcal{B}_{\mathrm{F}}(A_{*};r_{\boldsymbol{\theta}}(A_{*}))\), the controller loss \(\mathcal{J}(\boldsymbol{\theta};A)\) is four-times differentiable in \(\boldsymbol{\theta}\) and \(A\). Furthermore, we can bound_

\[\|\nabla_{A}^{(i)}\nabla_{\boldsymbol{\theta}}^{(j)}\mathcal{J}(\boldsymbol{ \theta};A)\|_{\mathrm{op}}\leq\mathrm{poly}(\|A\|_{\mathrm{op}},B_{\boldsymbol {\phi}},L_{\boldsymbol{\theta}},L_{\boldsymbol{\theta}},L_{\mathrm{cost}}, \sigma_{\boldsymbol{w}}^{-1},H,d_{\boldsymbol{x}})\]

_for \(i,j\in\{0,1,2,3,4\}\) satisfying \(1\leq i+j\leq 3\)._

In this section, we generalize Assumption 6 to the following.

**Assumption 13**.: _We assume there exists some \(r_{\boldsymbol{\theta}}(A_{\star})>0\) such that, for all \(A\in\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\boldsymbol{\theta}}(A_{\star}))\), \(\boldsymbol{\theta}_{\star}(A)\) satisfies:_

* \(\nabla_{\boldsymbol{\theta}}\mathcal{J}(\boldsymbol{\theta};A)|_{\boldsymbol{ \theta}=\boldsymbol{\theta}_{\star}(A)}=0\)_,_
* \(\boldsymbol{\theta}_{\star}(A)\) _is three-times differentiable in_ \(A\)_, and we can bound_ \(\|\nabla_{A}^{(i)}\boldsymbol{\theta}_{\star}(A)\|_{\mathrm{op}}\leq L_{\pi_{ \star}}\) _for some_ \(L_{\pi_{\star}}>0\) _and_ \(i\in\{1,2,3\}\)_._

The first condition requires that \(\boldsymbol{\theta}_{\star}(A)\) corresponds to a stationary point of the loss. This will be met, for example, by choosing \(\boldsymbol{\theta}_{\star}(A)\) to be a minima (local or global) of \(\mathcal{J}(\boldsymbol{\theta};A)\). It is not obvious, however, that the first and second condition can be simultaneously satisfied. In the following we show that, assuming \(\nabla_{\boldsymbol{\theta}}^{2}\mathcal{J}(\boldsymbol{\theta};A_{\star})|_{ \boldsymbol{\theta}=\boldsymbol{\theta}_{\star}(A_{\star})}\) is full-rank (which will be the case, for example, when \(\boldsymbol{\theta}_{\star}(A_{\star})\) is a strict local minimum of \(\mathcal{J}(\pi^{\boldsymbol{\theta}};A_{\star})\)), there always exists some \(\boldsymbol{\theta}_{\star}(A)\) satisfying both conditions of Assumption 13, with \(L_{\pi_{\star}}\) scaling polynomially in problem parameters, and \(r_{\boldsymbol{\theta}}(A_{\star})\) scaling inverse polynomially in problem parameters. Note that this definition of \(\pi_{\star}(A)\) is general enough to capture settings where the global minimum of \(\mathcal{J}(\pi;A)\) cannot be efficiently computed--it suffices to take \(\pi_{\star}(A)\) a local minimum of the loss.

**Proposition 5**.: _Assume that Assumptions 1, 2, 4 and 5 hold and that \(\lambda_{\min}(\nabla^{2}_{\bm{\theta}}\mathcal{J}(\bm{\theta};A_{\star})|_{\bm {\theta}=\bm{\theta}_{\star}(A_{\star})})>0\). Let \(r_{\bm{\theta}}(A_{\star})>0\) be some value satisfying_

\[r_{\bm{\theta}}(A_{\star})=\min\left\{r_{\mathrm{cost}}(A_{\star}),\mathrm{poly }\left(\frac{1}{\lambda_{\min}(\nabla^{2}_{\bm{\theta}}\mathcal{J}(\bm{ \theta};A_{\star})|_{\bm{\theta}=\bm{\theta}_{\star}(A_{\star})})},\|A_{\star }\|_{\mathrm{op}},B_{\bm{\phi}},L_{\bm{\phi}},L_{\bm{\theta}},L_{\mathrm{cost}},\sigma^{-1}_{\bm{w}},H,d_{\bm{x}}\right)^{-1}\right\}.\]

_Then there exists some function \(\bm{\theta}_{\star}(A)\) such that, for all \(A\in\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\bm{\theta}}(A_{\star}))\):_

* \(\nabla\bm{\theta}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta}_{\star} (A)}=0\)_,_
* \(\bm{\theta}_{\star}(A)\) _is three-times differentiable in_ \(A\)_,_

_and it suffices that we take_

\[L_{\pi_{\star}}=\mathrm{poly}\left(\frac{1}{\lambda_{\min}(\nabla^{2}_{\bm{ \theta}}\mathcal{J}(\bm{\theta};A_{\star})|_{\bm{\theta}=\bm{\theta}_{\star}(A _{\star})})},\|A_{\star}\|_{\mathrm{op}},B_{\bm{\phi}},L_{\bm{\phi}},L_{\bm{ \theta}},L_{\mathrm{cost}},\sigma^{-1}_{\bm{w}},H,d_{\bm{x}}\right).\]

While Proposition 5 shows that there exists some \(\bm{\theta}_{\star}(A)\) satisfying Assumption 13, it does not directly give a recipe for constructing such a map. The following result shows that under a mild additional assumption, the minimizer of the loss satisfies Assumption 13.

**Proposition 6**.: _Let_

\[\bm{\theta}_{\star}(A):=\operatorname*{arg\,min}_{\bm{\theta}\in\mathbb{R}^{ d_{\bm{\theta}}}}\mathcal{J}(\bm{\theta};A).\]

_Then under Assumptions 1, 2 and 4 to 6, there exists some \(r_{\bm{\theta}}(A_{\star})>0\) and \(L_{\pi_{\star}}<\infty\) such that \(\bm{\theta}_{\star}(A)\) satisfies Assumption 13._

The scaling of \(L_{\pi_{\star}}\) in Proposition 6 can be shown to match that of Proposition 5, but in general \(r_{\bm{\theta}}(A_{\star})\) could be smaller than the value of \(r_{\bm{\theta}}(A_{\star})\) given in Proposition 5. In particular, in the setting of Proposition 6, we can only show that \(r_{\bm{\theta}}(A_{\star})\) scales with \(\min_{\bm{\theta}\not\in\mathcal{B}_{2}(\bm{\theta}_{\star},(A_{\star});r)} \mathcal{J}(\bm{\theta};A_{\star})-\mathcal{J}(\bm{\theta}_{\star}(A_{\star}); A_{\star})\) for some \(r>0\) which scales inverse polynomially in problem parameters. While we can show that \(\mathcal{J}(\bm{\theta};A_{\star})-\mathcal{J}(\bm{\theta}_{\star}(A_{\star}); A_{\star})\) scales inverse polynomially in problem parameters, including in \(\lambda_{\min}(\nabla^{2}_{\bm{\theta}}\mathcal{J}(\bm{\theta};A_{\star})|_{\bm {\theta}=\bm{\theta}_{\star}(A_{\star})})\), for \(\bm{\theta}\) approximately a distance of \(r\) from \(\bm{\theta}_{\star}(A_{\star})\), it is possible \(\mathcal{J}(\bm{\theta};A_{\star})\) has some local minimizer \(\bm{\theta}^{\prime}\) arbitrarily far away from \(\bm{\theta}_{\star}(A_{\star})\), such that \(\mathcal{J}(\bm{\theta}^{\prime};A_{\star})\) and \(\mathcal{J}(\bm{\theta}_{\star}(A_{\star});A_{\star})\) are arbitrarily close, in which case \(\Delta^{\star}\), and therefore \(r_{\bm{\theta}}(A_{\star})\), could be arbitrarily small. The failure mode here is that, while \(\bm{\theta}_{\star}(A_{\star})\) may be the global minimum of \(\mathcal{J}(\bm{\theta};A_{\star})\), for \(A\) arbitrarily close to \(A_{\star}\), the global minimum of \(\mathcal{J}(\bm{\theta};A)\) could instead be near \(\bm{\theta}^{\prime}\), which would render the map \(\bm{\theta}_{\star}(A)\) discontinuous.

By making further assumptions on \(\mathcal{J}(\bm{\theta};A_{\star})\) which exclude this case, we can obtain a value of \(r_{\bm{\theta}}(A_{\star})\) scaling similarly to in Proposition 5. For example, in the following, we show that under the assumption that \(\mathcal{J}(\bm{\theta};A)\) is convex, this holds.

**Proposition 7**.: _Assume that there exists some \(r_{\mathrm{conv}}(A_{\star})>0\) such that, for all \(A\in\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\mathrm{conv}}(A_{\star}))\), \(\mathcal{J}(\bm{\theta};A)\) is convex in \(\bm{\theta}\), and set_

\[\bm{\theta}_{\star}(A)=\operatorname*{arg\,min}_{\bm{\theta}\in\mathbb{R}^{d_{ \bm{\theta}}}}\mathcal{J}(\bm{\theta};A).\]

_Then we have that \(\bm{\theta}_{\star}\) satisfies Assumption 13 with_

\[r_{\bm{\theta}}(A_{\star})=\min\left\{r_{\mathrm{conv}}(A_{\star}),r_{ \mathrm{cost}}(A_{\star}),\right.\] \[\left.\mathrm{poly}\left(\frac{1}{\lambda_{\min}(\nabla^{2}_{\bm{ \theta}}\mathcal{J}(\bm{\theta};A_{\star})|_{\bm{\theta}=\bm{\theta}_{\star}(A_{ \star})})},\|A_{\star}\|_{\mathrm{op}},B_{\bm{\phi}},L_{\bm{\phi}},L_{\bm{\theta} },L_{\mathrm{cost}},\sigma^{-1}_{\bm{w}},H,d_{\bm{x}}\right)^{-1}\right\}\]

_and it suffices that we take_

\[L_{\pi_{\star}}=\mathrm{poly}\left(\frac{1}{\lambda_{\min}(\nabla^{2}_{\bm{ \theta}}\mathcal{J}(\bm{\theta};A_{\star})|_{\bm{\theta}=\bm{\theta}_{\star}(A _{\star})})},\|A_{\star}\|_{\mathrm{op}},B_{\bm{\phi}},L_{\bm{\phi}},L_{\bm {\theta}},L_{\mathrm{cost}},\sigma^{-1}_{\bm{w}},H,d_{\bm{x}}\right).\]

Note that, if \(\mathcal{J}(\bm{\theta};A)\) is \(\mu\)-strongly convex in \(\bm{\theta}\) for all \(A\) near \(A_{\star}\), we can lower bound \(\lambda_{\min}(\nabla^{2}_{\bm{\theta}}\mathcal{J}(\bm{\theta};A_{\star})|_{\bm {\theta}=\bm{\theta}_{\star}(A_{\star})})\geq\mu\).

Approximating the Controller Loss.In order to efficiently direct our exploration, it is convenient to derive a quadratic approximation to the controller loss. The following result shows that, under our assumptions, this is indeed possible.

**Lemma D.2** (Formal Version of Proposition 1).: _Under Assumptions 1, 2, 4 to 5 and 13, for \(\widehat{A}\in\mathcal{B}_{\mathrm{F}}(A_{\star};\min\{r_{\mathrm{cost}}(A_{ \star}),r_{\bm{\theta}}(A_{\star})\})\), we have_

\[\mathcal{J}(\bm{\theta}_{\star}(\widehat{A});A_{\star})-\mathcal{J}(\bm{ \theta}_{\star}(A_{\star});A_{\star})\leq\mathrm{vec}(\widehat{A}-A_{\star}) ^{\top}\mathcal{H}(A_{\star})\mathrm{vec}(\widehat{A}-A_{\star})+M[\widehat{A} -A_{\star},\widehat{A}-A_{\star},\widehat{A}-A_{\star}].\]

_for some tensor \(M\) such that_

\[\|M[\widehat{A}-A_{\star},\widehat{A}-A_{\star},\widehat{A}-A_{\star}]\|_{ \mathrm{op}}\leq\mathrm{poly}(L_{\pi_{\star}},\|A_{\star}\|_{\mathrm{op}},B_{ \bm{\phi}},L_{\bm{\phi}},L_{\bm{\theta}},L_{\mathrm{cost}},\sigma_{\bm{w}}^{- 1},H,d_{\bm{x}})\cdot\|\widehat{A}-A_{\star}\|_{\mathrm{op}}^{3}.\]

In practice we do not know \(\mathcal{H}(A_{\star})\) and must estimate it. The following result shows that the distance between \(\mathcal{H}(A_{\star})\) and \(\mathcal{H}(\widehat{A})\) can be bounded.

**Lemma D.3**.: _Under Assumptions 1, 2, 4, 5 and 13, and if \(\widehat{A}\in\mathcal{B}_{\mathrm{F}}(A_{\star};\min\{r_{\mathrm{cost}}(A_{ \star}),r_{\bm{\theta}}(A_{\star})\})\), we can bound_

\[\|\mathcal{H}(A_{\star})-\mathcal{H}(\widehat{A})\|_{\mathrm{op}}\leq\mathrm{ poly}(L_{\pi_{\star}},\|A_{\star}\|_{\mathrm{op}},B_{\bm{\phi}},L_{\bm{\phi}},L_{ \bm{\theta}},L_{\mathrm{cost}},\sigma_{\bm{w}}^{-1},H,d_{\bm{x}})\cdot\| \widehat{A}-A_{\star}\|_{\mathrm{op}}.\]

### Proof of Smoothness of Nonlinear System

We let \(f_{\bm{w}}(\cdot)\) denote the density of the noise (which, by assumption, is simply an isotropic Gaussian density). We let \(f_{A,\bm{\theta}}(\cdot)\) denote the density over trajectories induced by playing controller \(\bm{\theta}\) on system \(A\). We will overload notation somewhat and let \(f_{A,\bm{\theta}}(\bm{x}_{h+1}\mid\bm{\tau}_{1:h})\) denote the density over \(\bm{x}_{h+1}\) induced by playing controller \(\bm{\theta}\) given trajectory \(\bm{\tau}_{1:h}\). Note that \(f_{A,\bm{\theta}}(\bm{x}_{h+1}\mid\bm{\tau}_{1:h})=f_{\bm{w}}(\bm{x}_{h+1}-A \bm{\phi}(\bm{x}_{h}^{\intercal},\pi_{h}^{\bm{\theta}}(\bm{\tau}_{1:h})))\) and

\[f_{A,\bm{\theta}}(\bm{\tau})=\prod_{h=1}^{H}f_{A,\bm{\theta}}(\bm{x}_{h+1} \mid\bm{\tau}_{1:h}).\]

Throughout this section we let \(\bm{x}_{h}^{\intercal}\) (resp. \(\bm{u}_{h}^{\intercal}\)) denote the state (resp. input) at step \(h\) of trajectory \(\bm{\tau}\). Under our regularity assumptions (Assumptions 1, 2, 4 and 5) and since the noise is Gaussian, we can swap derivatives and integrals, which we make use of throughout the following proofs.

Proof of Lemma D.1.: Let \(\mathrm{cost}(\bm{\tau})\) denote the cost of trajectory \(\bm{\tau}\). Then we have

\[\mathcal{J}(\bm{\theta};A)=\int\mathrm{cost}(\bm{\tau})f_{A,\bm{\theta}}(\bm{ \tau})\mathrm{d}\bm{\tau}.\]

Let \(A_{\bm{t}}:=A+t_{1}\Delta_{1}^{A}+t_{2}\Delta_{2}^{A}+t_{3}\Delta_{3}^{A}\) and \(\bm{\theta}_{\star}:=\bm{\theta}+s_{1}\Delta_{1}^{\bm{\theta}}+s_{2}\Delta_{2}^ {\bm{\theta}}+s_{3}\Delta_{3}^{\bm{\theta}}\), for some \(\Delta_{i}^{A}\) and \(\Delta_{j}^{\bm{\theta}}\), which we assume satisfy \(\|\Delta_{i}^{A}\|_{\mathrm{op}},\|\Delta_{j}^{\bm{\theta}}\|_{\mathrm{op}}\leq 1\). Rather than differentiating \(\mathcal{J}(\bm{\theta};A)\) with respect to \(\bm{\theta}\) or \(A\), we will differentiate \(\mathcal{J}(\bm{\theta}_{\star};A_{\bm{t}})\) with respect to some \(x_{1},x_{2},x_{3},x_{4}\in\{t_{1},t_{2},t_{3},s_{1},s_{2},s_{3}\}\). Note that, for example,

\[\frac{\mathrm{d}}{\mathrm{d}t_{1}}\mathcal{J}(\bm{\theta}_{\star};A_{\bm{t}})| _{\bm{t}=\bm{s}=0}=\nabla_{A}\mathcal{J}(\bm{\theta},A)[\Delta_{1}^{A}],\]

i.e. the directional gradient of \(\mathcal{J}(\bm{\theta},A)\) with respect to \(A\) in direction \(\Delta_{1}^{A}\), and that this similarly holds for gradients with respect to other \(t_{i},s_{j}\), or higher-order derivatives. Thus, if we can show that \(\mathcal{J}(\bm{\theta}_{\star};A_{\bm{t}})\) is differentiable with respect to any \(x_{1},x_{2},x_{3},x_{4}\in\{t_{1},t_{2},t_{3},s_{1},s_{2},s_{3}\}\), and this holds for any choice of \(\Delta_{i}^{A},\Delta_{j}^{\bm{\theta}}\), then we have that \(\mathcal{J}(\bm{\theta},A)\) is four-times differentiable with respect to \(\bm{\theta}\) and \(A\). Furthermore, we can bound the operator norm of \(\nabla_{A}\mathcal{J}(\bm{\theta},A)\), by bounding the value of \(\frac{\mathrm{d}}{\mathrm{d}t_{1}}\mathcal{J}(\bm{\theta}_{\star};A_{\bm{t}}) |_{\bm{t}=\bm{s}=0}\) for all \(\Delta_{1}^{A}\) satisfying \(\|\Delta_{1}^{A}\|_{\mathrm{op}}\leq 1\) (and we can similarly bound the operator norm of the higher order derivatives of \(\mathcal{J}(\bm{\theta},A)\)).

\(\mathcal{J}(\bm{\theta};A)\) is Differentiable.Let \(x_{1},x_{2},x_{3},x_{4}\in\{t_{1},t_{2},t_{3},s_{1},s_{2},s_{3}\}\). We have

\[\frac{\mathrm{d}}{\mathrm{d}x_{1}}\mathcal{J}(\bm{\theta}_{\star};A_{\bm{t}})= \frac{\mathrm{d}}{\mathrm{d}x_{1}}\int\mathrm{cost}(\bm{\tau})f_{A_{\bm{t}}, \bm{\theta}_{\star}}(\bm{\tau})\mathrm{d}\bm{\tau}\]\[=\int\frac{f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})}{f_{A_{\bm{t}},\bm{ \theta}_{s}}(\bm{\tau})}\frac{\mathrm{d}}{\mathrm{d}x_{1}}f_{A_{\bm{t}},\bm{ \theta}_{s}}(\bm{\tau})\mathrm{cost}(\bm{\tau})\mathrm{d}\bm{\tau}\] \[=\int\frac{\mathrm{d}}{\mathrm{d}x_{1}}\log f_{A_{\bm{t}},\bm{ \theta}_{s}}(\bm{\tau})\cdot\mathrm{cost}(\bm{\tau})f_{A_{\bm{t}},\bm{\theta}_ {s}}(\bm{\tau})\mathrm{d}\bm{\tau}.\]

Differentiating this gives

\[\frac{\mathrm{d}}{\mathrm{d}x_{2}}\frac{\mathrm{d}}{\mathrm{d}x_{ 1}}\mathcal{J}(\bm{\theta}_{s};A_{\bm{t}}) =\frac{\mathrm{d}}{\mathrm{d}x_{2}}\int\frac{\mathrm{d}}{\mathrm{ d}x_{1}}\log f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\cdot\mathrm{cost}( \bm{\tau})f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\mathrm{d}\bm{\tau}\] \[=\int\left(\frac{\mathrm{d}}{\mathrm{d}x_{1}}\log f_{A_{\bm{t}}, \bm{\theta}_{s}}(\bm{\tau})\right)\left(\frac{\mathrm{d}}{\mathrm{d}x_{2}} \log f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\right)\cdot\mathrm{cost}(\bm{ \tau})f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\mathrm{d}\bm{\tau}\] \[\qquad+\int\frac{\mathrm{d}}{\mathrm{d}x_{2}}\frac{\mathrm{d}}{ \mathrm{d}x_{1}}\log f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\cdot\mathrm{cost }(\bm{\tau})f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\mathrm{d}\bm{\tau},\]

and

\[\frac{\mathrm{d}}{\mathrm{d}x_{3}}\frac{\mathrm{d}}{\mathrm{d}x_{ 2}}\frac{\mathrm{d}}{\mathrm{d}x_{1}} \mathcal{J}(\bm{\theta}_{s};A_{\bm{t}}) =\int\left(\frac{\mathrm{d}}{\mathrm{d}x_{1}}\log f_{A_{\bm{t}}, \bm{\theta}_{s}}(\bm{\tau})\right)\left(\frac{\mathrm{d}}{\mathrm{d}x_{2}} \log f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\right)\left(\frac{\mathrm{d}}{ \mathrm{d}x_{3}}\log f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\right)\cdot \mathrm{cost}(\bm{\tau})f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\mathrm{d} \bm{\tau}\] \[+\int\left(\frac{\mathrm{d}}{\mathrm{d}x_{3}}\frac{\mathrm{d}}{ \mathrm{d}x_{1}}\log f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\right)\left( \frac{\mathrm{d}}{\mathrm{d}x_{2}}\log f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{ \tau})\right)\cdot\mathrm{cost}(\bm{\tau})f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{ \tau})\mathrm{d}\bm{\tau}\] \[+\int\left(\frac{\mathrm{d}}{\mathrm{d}x_{1}}\log f_{A_{\bm{t}}, \bm{\theta}_{s}}(\bm{\tau})\right)\left(\frac{\mathrm{d}}{\mathrm{d}x_{3}} \log f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\right)\cdot\mathrm{cost}(\bm{ \tau})f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\mathrm{d}\bm{\tau}.\]

The fourth derivative of \(\mathcal{J}(\bm{\theta};A)\) can be similarly calculated by differentiating \(\frac{\mathrm{d}}{\mathrm{d}x_{3}}\frac{\mathrm{d}}{\mathrm{d}x_{2}}\frac{ \mathrm{d}}{\mathrm{d}x_{1}}\mathcal{J}(\bm{\theta}_{s};A_{\bm{t}})\); we omit it for brevity. We have

\[\log f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau}) =\log\prod_{h=1}^{H}f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{x}_{h+1}^{ \bm{\tau}}\mid\bm{\tau}_{1:h})\] \[=\sum_{h=1}^{H}\log f_{\bm{w}}(\bm{x}_{h+1}^{\bm{\tau}}-A_{\bm{t} }\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\pi^{\bm{\theta}_{s}}(\bm{\tau}_{1:h})))\] \[=\sum_{h=1}^{H}-\frac{1}{2\sigma_{\bm{w}}^{2}}\|\bm{x}_{h+1}^{\bm {\tau}}-A_{\bm{t}}\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\pi^{\bm{\theta}_{s}}(\bm{ \tau}_{1:h}))\|_{2}^{2}+C\]

for some \(C\) which does not depend on \(\bm{t}\) or \(\bm{s}\). Given that \(\bm{\phi}(\bm{x},\bm{u})\) is four-times differentiable in \(\bm{u}\) and \(\pi_{h}^{\bm{\theta}_{s}}(\bm{\tau}_{1:h})\) is four-times differentiable in \(\bm{x}\) (which hold by Assumption 4 and Assumption 5), it is clear that \(\log f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{\tau})\) is four-times differentiable in \(t_{i}\) or \(s_{i}\), regardless of the choice of \(\Delta_{i}^{A}\) or \(\Delta_{i}^{\bm{\theta}}\). This proves the first result.

Norm Bounds on Gradient.Note that

\[\frac{\mathrm{d}}{\mathrm{d}t_{i}}\log f_{A_{\bm{t}},\bm{\theta}_{s}}(\bm{ \tau})|_{\bm{t}=\bm{s}=0} =\sum_{h=1}^{H}\frac{1}{\sigma_{\bm{w}}^{2}}(\bm{x}_{h+1}^{\bm{\tau}}-A\bm{ \phi}(\bm{x}_{h}^{\bm{\tau}},\pi_{h}^{\bm{\theta}}(\bm{\tau}_{1:h})))^{\top} \cdot\Delta_{i}^{A}\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\pi_{h}^{\bm{\theta}}(\bm{ \tau}_{1:h})),\] \[\frac{\mathrm{d}}{\mathrm{d}s_{i}}\log f_{A_{\bm{t}},\bm{\theta}_ {s}}(\bm{\tau})|_{\bm{t}=\bm{s}=0} =\sum_{h=1}^{H}\frac{1}{\sigma_{\bm{w}}^{2}}(\bm{x}_{h+1}^{\bm{\tau}}-A\bm{ \phi}(\bm{x}_{h}^{\bm{\tau}},\pi_{h}^{\bm{\theta}}(\bm{\tau}_{1:h})))^{\top} \cdot A\nabla_{\bm{u}}\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\pi_{h}^{\bm{\theta}}( \bm{\tau}_{1:h}))\cdot\nabla_{\bm{\theta}}\pi_{h}^{\bm{\theta}}(\bm{\tau}_{1:h })\cdot\Delta_{i}^{\bm{\theta}}.\]

Furthermore, differentiating these expressions further with respect to \(t_{j}\) or \(s_{j}\) will simply yield higher-order derivates of \(\bm{\phi}(\bm{x},\bm{u})\) and \(\pi_{h}^{\bm{\theta}}(\bm{\tau}_{1:h})\). Using the norm bounds on the gradient of \(\bm{\phi}(\bm{x},\bm{u})\) and \(\pi_{h}^{\bm{\theta}}(\bm{\tau}_{1:h})\) given in Assumption 4 and Assumption 5, and the norm bound of \(\bm{\phi}(\bm{x},\bm{u})\) given in Assumption 1, we can then bound

\[\|\nabla_{A}^{(i)}\nabla_{\bm{\theta}}^{(j)}\log f_{A,\bm{\theta}}(\bm{\tau})\|_ {\mathrm{op}}\leq\mathrm{poly}(\|A\|_{\mathrm{op}},B_{\bm{\phi}},L_{\bm{ \phi}},L_{\bm{\theta}},\sigma_{\bm{w}}^{-1})\cdot\sum_{h=1}^{H}(1+\|\bm{x}_{h+ 1}^{\bm{\tau}}-A\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\pi_{h}^{\bm{\theta}}(\bm{ \tau}_{1:h}))\|_{2})\]for \(i,j\in\{0,1,2,3,4\}\) satisfying \(1\leq i+j\leq 4\) (where we have used the fact noted above that, to bound the operator norm of \(\nabla_{A}^{(i)}\nabla_{\bm{\theta}}^{(j)}\log f_{A,\bm{\theta}}(\bm{\tau})\), it suffices to bound the directional gradient in every direction). It follows that we can bound

\[\|\nabla_{A}^{(i)}\nabla_{\bm{\theta}}^{(j)}\mathcal{J}(\bm{\theta };A)\|_{\mathrm{op}}\] \[\qquad\leq\mathrm{poly}(\|A\|_{\mathrm{op}},B_{\bm{\phi}},L_{\bm {\phi}},L_{\bm{\theta}},\sigma_{\bm{w}}^{-1})\cdot\int\left(\sum_{h=1}^{H}(1+\| \bm{x}_{h+1}^{\bm{\tau}}-A\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\pi_{h}^{\bm{\theta }}(\bm{\tau}_{1:h}))\|_{2})\right)^{4}\cdot\mathrm{cost}(\bm{\tau})f_{A,\bm{ \theta}}(\bm{\tau})\mathrm{d}\bm{\tau}\] \[\qquad\qquad\cdot\sqrt{\int\left(\sum_{h=1}^{H}(1+\|\bm{x}_{h+1} ^{\bm{\tau}}-A\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\pi_{h}^{\bm{\theta}}(\bm{\tau }_{1:h}))\|_{2})\right)^{8}f_{A,\bm{\theta}}(\bm{\tau})\mathrm{d}\bm{\tau}}\] \[\qquad\qquad\overset{(b)}{\leq}\mathrm{poly}(\|A\|_{\mathrm{op}}, B_{\bm{\phi}},L_{\bm{\phi}},L_{\bm{\theta}},L_{\mathrm{cost}},\sigma_{\bm{w}}^{-1},H,d_{ \bm{x}})\]

where \((a)\) follows from Cauchy-Schwarz, and \((b)\) follows from Lemma A.1 and Assumption 2, since we have assumed \(A\in\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\mathrm{cost}}(A_{\star}))\). 

Proof of Proposition 5.: **Existence and Differentiability of \(\bm{\theta}_{\star}\).** By Lemma D.1 we have that \(\mathcal{J}(\bm{\theta};A)\) is four-times differentiable in its arguments. By the Implicit Function Theorem, since \(\lambda_{\min}(\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A_{\star})|_{ \bm{\theta}=\bm{\theta}_{\star}(A_{\star})})>0\) by assumption, we have that there exists some \(r_{\bm{\theta}}^{\prime}(A_{\star})>0\) and unique function \(\bm{\theta}_{\star}(A)\) defined on \(\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\bm{\theta}}^{\prime}(A_{\star}))\) such that \(\nabla_{\bm{\theta}}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta}_{ \star}(A)}=0\), and \(\bm{\theta}_{\star}(A)\) is three-times differentiable (note that, while the Implicit Function Theorem is typically stated to give that the resulting function is only one-time differentiable, it can be extended to \(k\)-times differentiable, assuming the implicit equation is \(k\)-times differentiable [60]).

By Lemma D.1 and the continuity of eigenvalues, it follows that for \(A\) close enough to \(A_{\star}\), we have \(\lambda_{\min}(\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)|_{\bm{ \theta}=\bm{\theta}_{\star}(A)})\geq\frac{1}{2}\lambda_{\min}(\nabla_{\bm{ \theta}}^{2}\mathcal{J}(\bm{\theta};A_{\star})|_{\bm{\theta}=\bm{\theta}_{ \star}(A_{\star})})>0\). We can therefore apply the Implicit Function Theorem as above to any \(A\) satisfying this, to get that there exists some unique \(\bar{\bm{\theta}}_{\star}(A^{\prime})\) defined for all \(A^{\prime}\) near \(A\) such that \(\nabla_{\bm{\theta}}\mathcal{J}(\bm{\theta};A^{\prime})|_{\bm{\theta}=\bar{\bm {\theta}}_{\star}(A^{\prime})}=0\) and \(\bar{\bm{\theta}}_{\star}(A^{\prime})\) is differentiable.

By the uniqueness of \(\bm{\theta}_{\star}(A)\) on \(\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\bm{\theta}}^{\prime}(A_{\star}))\), it follows that any \(\bar{\bm{\theta}}_{\star}(A^{\prime})\) defined in this way must be identical to \(\bm{\theta}_{\star}(A)\) on \(\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\bm{\theta}}^{\prime}(A_{\star}))\) (assuming the regions on which they are defined overlaps). We can therefore define \(\bm{\theta}_{\star}(A)\) to simply be the extension of \(\bm{\theta}_{\star}(A)\) to all such \(\bar{\bm{\theta}}_{\star}(A)\), defined for all \(A\) near \(A_{\star}\) such that \(\lambda_{\min}(\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)|_{\bm{ \theta}=\bm{\theta}_{\star}(A)})\geq\frac{1}{2}\lambda_{\min}(\nabla_{\bm{ \theta}}^{2}\mathcal{J}(\bm{\theta};A_{\star})|_{\bm{\theta}=\bm{\theta}_{ \star}(A_{\star})})\), and will have that \(\bm{\theta}_{\star}(A)\) is three-times differentiable and satisfies \(\nabla_{\bm{\theta}}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta}_{ \star}(A)}=0\) for all such \(A\).

We then choose \(r_{\bm{\theta}}(A_{\star})\) to be defined such that, for all \(A\in\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\bm{\theta}}(A_{\star}))\), we have \(\lambda_{\min}(\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)|_{\bm{ \theta}=\bm{\theta}_{\star}(A)})\geq\frac{1}{2}\lambda_{\min}(\nabla_{\bm{ \theta}}^{2}\mathcal{J}(\bm{\theta};A_{\star})|_{\bm{\theta}=\bm{\theta}_{ \star}(A_{\star})})\). By Lemma D.1, we know that \(\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)\) is continuous and furthermore we know that eigenvalues are continuous. Using the gradient bounds given in Lemma D.1 to bound the Lipschitz constant of \(\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)\), it follows that we can take

\[r_{\bm{\theta}}(A_{\star})=\mathrm{poly}\left(\frac{1}{\lambda_{\min}(\nabla_{ \bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A_{\star})|_{\bm{\theta}=\bm{\theta}_{ \star}(A_{\star})})},\|A_{\star}\|_{\mathrm{op}},B_{\bm{\phi}},L_{\bm{\phi}},L _{\bm{\theta}},L_{\mathrm{cost}},\sigma_{\bm{w}}^{-1},H,d_{\bm{x}}\right)^{-1}.\]

Bounding Norm of Gradients.Fix \(A\in\mathcal{B}_{\mathrm{F}}(A_{\star};r_{\bm{\theta}}(A_{\star}))\). We know that \(\bm{\theta}_{\star}(A)\) satisfies

\[\nabla_{\bm{\theta}}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta}_{\star}(A) }=0.\]

We wish to differentiate \(\bm{\theta}_{\star}(A)\) with respect to \(A\), and bound the magnitude of up to the third derivative. Similar to the proof of Lemma D.1, we let \(A_{\bm{t}}:=A+t_{1}\Delta_{1}^{A}+t_{2}\Delta_{2}^{A}+t_{3}\Delta_{3}^{A}\) for some \(\Delta_{i}^{A}\) satisfying \(\|\Delta_{i}^{A}\|_{\mathrm{op}}\leq 1\). As noted in the proof of Lemma D.1, we have

\[\frac{\mathrm{d}}{\mathrm{d}t_{i}}\bm{\theta}_{\star}(A_{\bm{t}})|_{\bm{t}=0}= \nabla_{A}\bm{\theta}_{\star}(A)[\Delta_{i}^{A}]\](and similarly for higher-order derivatives). Thus, to show the result, it suffices to show that \(\bm{\theta}_{\star}(A_{\bm{t}})\) is differentiable in \(t_{1},t_{2},t_{3}\) for all \(\Delta_{i}^{A}\), and to bound the magnitude of this derivative for all \(\Delta_{i}^{A}\) with \(\|\Delta_{i}^{A}\|_{\mathrm{op}}\leq 1\). We have

\[\frac{\mathrm{d}}{\mathrm{d}t_{1}}\nabla_{\bm{\theta}}\mathcal{J} (\bm{\theta};A_{\bm{t}})|_{\bm{\theta}=\bm{\theta}_{\star}(A_{\bm{t}})}|_{\bm {t}=0}=0\] \[\implies\underbrace{\nabla_{A^{\prime}}\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta};A^{\prime})|_{\bm{\theta}=\bm{\theta}_{\star}(A),A^{ \prime}=A}[\Delta_{1}^{A}]}_{=:G_{1}(A,\Delta_{1}^{A})}+\nabla_{\bm{\theta}}^{2 }\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta}_{\star}(A)}\cdot\nabla_{ A}\bm{\theta}_{\star}(A)[\Delta_{1}^{A}]=0\] (D.1)

which implies

\[\nabla_{A}\bm{\theta}_{\star}(A)[\Delta_{1}^{A}]=-\big{(}\nabla_{\bm{\theta}} ^{2}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta}_{\star}(A)}\big{)}^{ -1}\cdot G_{1}(A,\Delta_{1}^{A})\]

which is well-defined since we have assumed that \(\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta} _{\star}(A)}\) is full-rank, and \(\mathcal{J}\) is differentiable in both its arguments by Lemma D.1. To compute the second derivative of \(\bm{\theta}_{\star}\), we differentiate through (D.1) which gives

\[\frac{\mathrm{d}}{\mathrm{d}t_{2}}\left(G_{1}(A_{\bm{t}},\Delta_ {1}^{A})+\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A_{\bm{t}})|_{\bm{ \theta}=\bm{\theta}_{\star}(A_{\bm{t}})}\cdot\nabla_{A}\bm{\theta}_{\star}(A_ {\bm{t}})[\Delta_{1}^{A}]\right)\big{|}_{\bm{t}=0}=0\] \[\implies\underbrace{\frac{\mathrm{d}}{\mathrm{d}t_{2}}\left(G_{1} (A_{\bm{t}},\Delta_{1}^{A})+\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta}; A_{\bm{t}})|_{\bm{\theta}=\bm{\theta}_{\star}(A_{\bm{t}})}\cdot\nabla_{A}\bm{ \theta}_{\star}(A)[\Delta_{1}^{A}]\right)\big{|}_{\bm{t}=0}}_{=:G_{2}(A, \Delta_{1}^{A},\Delta_{2}^{A})}\] \[\qquad\qquad+\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)|_ {\bm{\theta}=\bm{\theta}_{\star}(A)}\cdot\nabla_{A}^{2}\bm{\theta}_{\star}(A) [\Delta_{1}^{A},\Delta_{2}^{A}]=0.\]

Note that \(G_{2}(A,\Delta_{1}^{A},\Delta_{2}^{A})\) involves at most a third-order derivative of \(\mathcal{J}(\bm{\theta};A)\) and first-order derivative of \(\bm{\theta}_{\star}(A)\), both of which we know exist by Lemma D.1 and what we showed above. This then further implies

\[\nabla_{A}^{2}\bm{\theta}_{\star}(A)[\Delta_{1}^{A},\Delta_{2}^{A}]=-\big{(} \nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta} _{\star}(A)}\big{)}^{-1}\cdot G_{2}(A,\Delta_{1}^{A},\Delta_{2}^{A}),\]

which is well-defined since we have assumed that \(\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta} _{\star}(A)}\) is full-rank. Finally, we compute

\[\frac{\mathrm{d}}{\mathrm{d}t_{3}}\left(G_{2}(A_{\bm{t}},\Delta_{ 1}^{A},\Delta_{2}^{A})+\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A_{\bm{ t}})|_{\bm{\theta}=\bm{\theta}_{\star}(A_{\bm{t}})}\cdot\nabla_{A}^{2}\bm{\theta}_{ \star}(A_{\bm{t}})[\Delta_{1}^{A},\Delta_{2}^{A}]\right)\big{|}_{\bm{t}=0}=0\] \[\implies\underbrace{\frac{\mathrm{d}}{\mathrm{d}t_{3}}\left(G_{2 }(A_{\bm{t}},\Delta_{1}^{A},\Delta_{2}^{A})+\nabla_{\bm{\theta}}^{2}\mathcal{ J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta}_{\star}(A_{\bm{t}})}\cdot\nabla_{A}^{2}\bm{ \theta}_{\star}(A)[\Delta_{1}^{A},\Delta_{2}^{A}]\right)\big{|}_{\bm{t}=0}}_{=:G _{3}(A,\Delta_{1}^{A},\Delta_{2}^{A},\Delta_{3}^{A})}\] \[\qquad\qquad+\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)|_ {\bm{\theta}=\bm{\theta}_{\star}(A)}\cdot\nabla_{A}^{3}\bm{\theta}_{\star}(A)[ \Delta_{1}^{A},\Delta_{2}^{A},\Delta_{3}^{A}]=0.\]

Note that \(G_{3}(A,\Delta_{1}^{A},\Delta_{2}^{A},\Delta_{3}^{A})\) involves at most a fourth-order derivative of \(\mathcal{J}(\bm{\theta};A)\) and second-order derivative of \(\bm{\theta}_{\star}(A)\), both of which we know exist by Lemma D.1 and what we showed above. We therefore have

\[\nabla_{A}^{3}\bm{\theta}_{\star}(A)[\Delta_{1}^{A},\Delta_{2}^{A},\Delta_{3}^{ A}]=-\big{(}\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{ \theta}_{\star}(A)}\big{)}^{-1}\cdot G_{3}(A,\Delta_{1}^{A},\Delta_{2}^{A}, \Delta_{3}^{A})\]

which is well-defined since we have assumed that \(\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta}_{ \star}(A)}\) is full-rank. As each of these expressions is defined for all choice of \(\Delta_{i}^{A}\), the differentiability of \(\bm{\theta}_{\star}(A)\) follows.

Note that the above expressions for \(\nabla_{A}\bm{\theta}_{\star}(A)[\Delta_{1}^{A}],\nabla_{A}^{2}\bm{\theta}_{ \star}(A)[\Delta_{1}^{A},\Delta_{2}^{A},\Delta_{3}^{A}]\), and \(\nabla_{A}^{3}\bm{\theta}_{\star}(A)[\Delta_{1}^{A},\Delta_{2}^{A},\Delta_{3}^{ A}]\) all depend on at most a fourth derivative of \(\mathcal{J}(\bm{\theta};A)\), as well as \((\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\bm{\theta}_{ \star}(A)})^{-1}\). The norm bounds are then a direct consequence of Lemma D.1.

Proof of Proposition 6.: By Lemma D.1 we have that \(\mathcal{J}(\bm{\theta};A)\) is four-times differentiable in its arguments. Since we have assumed \(\nabla_{\bm{\theta}}^{2}\mathcal{J}(\bm{\theta};A_{\star})|_{\bm{\theta}=\bm{ \theta}_{\star}(A_{\star})}\succ 0\), by the Implicit Function Theorem [60], it follows that there exists some \(r^{\prime}_{\bm{\theta}}>0\) and mapping \(\widetilde{\bm{\theta}}(A)\) such that, for all \(A\in\mathcal{B}_{\mathrm{F}}(A_{\star};r^{\prime}_{\bm{\theta}})\), \(\nabla_{\bm{\theta}}\mathcal{J}(\bm{\theta};A)|_{\bm{\theta}=\widetilde{\bm{ \theta}}(A)}=0\), and \(\widetilde{\bm{\theta}}(A)\) is three-times differentiable.

[MISSING_PAGE_FAIL:39]

We can bound

\[|\nabla^{3}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);A_{\star})|_{A=A^{\prime}}[ \widehat{A}-A_{\star},\widehat{A}-A_{\star},\widehat{A}-A_{\star}]|\leq\|\nabla^ {3}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);A_{\star})|_{A=A^{\prime}}\|_{\rm op }\cdot\|\widehat{A}-A_{\star}\|_{\rm op}^{3}.\]

The expression for \(\nabla^{3}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);A_{\star})\) contains up to the third derivative of both \(\mathcal{J}(\bm{\theta};A_{\star})\) and \(\bm{\theta}_{\star}(A)\). By Lemma D.1 and under Assumption 13, since \(A^{\prime}\in\mathcal{B}_{\rm F}(A_{\star};\min\{r_{\rm cost}(A_{\star}),r_{ \bm{\theta}}(A_{\star})\})\) by construction, we can then bound

\[\|\nabla^{3}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);A_{\star})|_{A=A^{\prime}} \|_{\rm op}\leq{\rm poly}(L_{\pi_{\star}},\|A_{\star}\|_{\rm op},B_{\bm{\phi}}, L_{\bm{\phi}},L_{\bm{\theta}},L_{\rm cost},\sigma_{\bm{w}}^{-1},H,d_{\bm{a}}).\]

The result follows by the definition of \(\mathcal{H}(A_{\star})\). 

Proof of Lemma D.3.: Recall that \(\mathcal{H}(\widehat{A})=\nabla^{2}_{A}\mathcal{J}(\bm{\theta}_{\star}(A); \widehat{A})|_{A=\widehat{A}}\). To prove this, we will use that this is differentiable by Lemma D.1, and will apply Taylor's Theorem.

First, note that by Taylor's Theorem we have

\[\nabla^{2}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);\widehat{A})|_{A=\widehat{A }}=\nabla^{2}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);\widehat{A})|_{A=A_{\star }}+\nabla^{3}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);\widehat{A})|_{A=A^{\prime }}[\widehat{A}-A_{\star}]\]

for \(A^{\prime}=t\widehat{A}+(1-t)A_{\star}\) for some \(t\in[0,1]\). The third derivative of \(\mathcal{J}(\bm{\theta}_{\star}(A);\widehat{A})\) will involve up to the third derivative of both \(\mathcal{J}(\bm{\theta};\widehat{A})\) and \(\bm{\theta}_{\star}(A)\), so using Lemma D.1 and Assumption 13, since \(A^{\prime}\in\mathcal{B}_{\rm F}(A_{\star};\min\{r_{\rm cost}(A_{\star}),r_{ \bm{\theta}}(A_{\star})\})\) by assumption, we can bound

\[\|\nabla^{3}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);\widehat{A})|_{A=A^{\prime }}[\widehat{A}-A_{\star}]\|_{\rm op}\leq{\rm poly}(L_{\pi_{\star}},\|A_{\star} \|_{\rm op},B_{\bm{\phi}},L_{\bm{\phi}},L_{\bm{\theta}},L_{\rm cost},\sigma_{ \bm{w}}^{-1},H,d_{\bm{x}})\cdot\|\widehat{A}-A_{\star}\|_{\rm op}.\]

Next, we wish to relate \(\nabla^{2}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);\widehat{A})|_{A=A_{\star}}\) to \(\nabla^{2}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);A_{\star})|_{A=A_{\star}}= \mathcal{H}(A_{\star})\). Again applying Taylor's Theorem, we have

\[\nabla^{2}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);\widehat{A})|_{A=A_{\star}}= \nabla^{2}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);A_{\star})|_{A=A_{\star}}+ \nabla_{A^{\prime}}\nabla^{2}_{A}\mathcal{J}(\bm{\theta}_{\star}(A);A^{\prime })|_{A=A_{\star},A^{\prime}=A^{\prime\prime}}[\widehat{A}-A_{\star}]\]

for \(A^{\prime\prime}=t\widehat{A}+(1-t)A_{\star}\) for some \(t\in[0,1]\). By Lemma D.1 and Assumption 13, we can bound

\[\|\nabla_{A^{\prime}}\nabla^{2}_{A}\mathcal{J}(\bm{\theta}_{\star} (A);A^{\prime})|_{A=A_{\star},A^{\prime}=A^{\prime\prime}}[\widehat{A}-A_{ \star}]\|_{\rm op}\] \[\leq{\rm poly}(L_{\pi_{\star}},\|A_{\star}\|_{\rm op},L_{\bm{\phi}},L_{\bm{\theta}},L_{\rm cost},\sigma_{\bm{w}}^{-1},H,d_{\bm{x}})\cdot\|\widehat {A}-A_{\star}\|_{\rm op}.\]

The result follows. 

**Lemma D.4**.: _Under Assumptions 1, 2, 4, 5 and 13, for all \(A\in\mathcal{B}_{\rm F}(A_{\star};\min\{r_{\rm cost}(A_{\star}),r_{\bm{\theta}} (A_{\star})\})\), we can bound_

\[\|\mathcal{H}(A)\|_{\rm op}\leq{\rm poly}(\|A_{\star}\|_{\rm op},B_{\bm{\phi}},L_{\bm{\phi}},L_{\bm{\theta}},L_{\rm cost},L_{\pi_{\star}},\sigma_{\bm{w}}^{-1 },H,d_{\bm{x}})\]

Proof.: Recall that \(\mathcal{H}(\widehat{A})=\nabla^{2}_{A}\mathcal{J}(\bm{\theta}_{\star}(A); \widehat{A})|_{A=\widehat{A}}\). The bound then follows from Lemma D.1 and Assumption 13. 

## Appendix E High-Probability Regret Bounds in Nonlinear Systems

In this section, we modify the proof the main result of [17] slightly to show a high probability regret bound for LC\({}^{3}\). For the sake of brevity, we omit details that are identical to the proof given in [17]. We will need the following assumption.

**Assumption 14** (Bounded Cost).: _We assume that, for all trajectories \(\bm{\tau}\), we have \({\rm cost}(\bm{\tau})\leq c_{\max}\)._

We adopt the notation used in this work, modifying somewhat the notation from [17]. In particular, we let \(\mathcal{J}(\pi;A)\) denote the expected cost of playing policy \(\pi\) under system \(A\), and we set

\[\bm{\Sigma}_{t}=\sum_{s=1}^{t}\sum_{h=1}^{H}\bm{\phi}(\bm{x}_{h}^{t},\bm{u}_{h} ^{t})\bm{\phi}(\bm{x}_{h}^{t},\bm{u}_{h}^{t})^{\top}+\lambda I\]

denote the covariates obtained by the first \(t\) episodes of LC\({}^{3}\) (plus a regularizer). We let \(\pi^{t}\) denote the policy played at episode \(t\) of LC\({}^{3}\). For a policy set \(\Pi\), we define regret as

\[\mathcal{R}_{T}(\Pi):=\sum_{t=1}^{T}\mathcal{J}(\pi^{t};A_{\star})-T\cdot\min_{ \pi\in\Pi}\mathcal{J}(\pi;A_{\star}).\]We will also denote \(\pi_{\star}:=\arg\min_{\pi\in\Pi}\mathcal{J}(\pi;A_{\star})\).

In addition to these notational changes, we modify LC\({}^{3}\) slightly to use the parameter

\[\beta^{t}:=\sqrt{\lambda}B_{A}+\sqrt{8d_{\boldsymbol{x}}\log 5+8\log(T\det( \boldsymbol{\Sigma}_{t})\det(\boldsymbol{\Sigma}_{0})^{-1}/\delta)}\]

in the construction of the confidence set, \(\textsc{Ball}^{t}\).

Besides the aforementioned changes, in the following proofs we adopt the same notation as [17]. We have the following result.

**Theorem 6**.: _Under Assumptions 1 and 14 and with any policy class \(\Pi\), with probability at least \(1-\delta\), LC\({}^{3}\) has regret bounded as_

\[\mathcal{R}_{T}(\Pi)\leq C\cdot c_{\max}H\sqrt{d_{\boldsymbol{\phi}}\cdot(d_ {\boldsymbol{\phi}}+d_{\boldsymbol{x}}+B_{A}+\log\frac{1}{\delta})\cdot T} \cdot\log\left(1+B_{\boldsymbol{\phi}}HT/\sigma_{\boldsymbol{w}}\right)\]

_for a universal constant \(C\)._

Proof of Theorem 6.: By Lemma E.2, we have that the event \(\mathcal{E}_{1}\) holds with probability at least \(1-\delta\). We therefore assume \(\mathcal{E}_{1}\) holds for the remainder of the proof.

By the definition of the confidence set in LC\({}^{3}\), on \(\mathcal{E}_{1}\) we have that \(A_{\star}\) is the in confidence set for all \(t\leq T\). It follows that on \(\mathcal{E}_{1}\),

\[\mathcal{R}_{T} =\sum_{t=1}^{T}\left[\mathcal{J}(\pi^{t};A_{\star})-\mathcal{J}( \pi_{\star};A_{\star})\right]\] \[\overset{(a)}{\leq}\sum_{t=1}^{T}\left[\mathcal{J}(\pi^{t};A_{ \star})-\mathcal{J}(\pi_{\star};\widehat{A}^{t})\right]\] \[\overset{(b)}{\leq}\sum_{t=1}^{T}c_{\max}\cdot\mathbb{E}_{A_{ \star},\pi^{t}}\left[\sum_{h=1}^{H}\min\left\{\frac{1}{\sigma_{\boldsymbol{w}}} \|(A_{\star}-\widehat{A}^{t})\cdot\boldsymbol{\phi}(\boldsymbol{x}_{h}, \boldsymbol{u}_{h})\|_{2},1\right\}\right]\] (E.1)

where \((a)\) follows from the optimistic property of LC\({}^{3}\) when \(A_{\star}\in\textsc{Ball}^{t}\), and \((b)\) follows from Lemma E.1. On \(\mathcal{E}_{1}\), we have

\[\|(A_{\star}-\widehat{A}^{t})\boldsymbol{\phi}(\boldsymbol{x}_{h },\boldsymbol{u}_{h})\|_{2} \leq\|(A_{\star}-\widehat{A}^{t})\boldsymbol{\Sigma}_{t}^{1/2}\| _{2}\|\boldsymbol{\Sigma}_{t}^{-1/2}\boldsymbol{\phi}(\boldsymbol{x}_{h}, \boldsymbol{u}_{h})\|_{2}\] \[\leq\left(\|(A_{\star}-\bar{A}^{t})\boldsymbol{\Sigma}_{t}^{1/2} \|_{2}+\|(\bar{A}^{t}-\widehat{A}^{t})\boldsymbol{\Sigma}_{t}^{1/2}\|_{2} \right)\cdot\|\boldsymbol{\Sigma}_{t}^{-1/2}\boldsymbol{\phi}(\boldsymbol{x}_ {h},\boldsymbol{u}_{h})\|_{2}\] \[\leq 2\beta^{t}\|\boldsymbol{\phi}(\boldsymbol{x}_{h},\boldsymbol {u}_{h})\|_{\boldsymbol{\Sigma}_{t}^{-1}}\]

where the last inequality follows from the definition of \(\textsc{Ball}^{t}\) since \(\widehat{A}^{t}\in\textsc{Ball}^{t}\) by construction, and by the definition of \(\mathcal{E}_{1}\). This gives

\[\text{(\ref{eq:Ball})}\leq\sum_{t=1}^{T}c_{\max}\cdot\mathbb{E}_{A_{\star}, \pi^{t}}\left[\sum_{h=1}^{H}\min\left\{\frac{2\beta^{t}}{\sigma_{\boldsymbol{w} }}\|\boldsymbol{\phi}(\boldsymbol{x}_{h},\boldsymbol{u}_{h})\|_{\boldsymbol{ \Sigma}_{t}^{-1}},1\right\}\right].\]

By Lemma E.3, with probability \(1-\delta\) we can bound this as

\[\leq\underbrace{\frac{2c_{\max}\beta^{T}}{\sigma_{\boldsymbol{w}}}\cdot\sum_ {t=1}^{T}\sum_{h=1}^{H}\min\left\{\|\boldsymbol{\phi}(\boldsymbol{x}_{h}^{t}, \boldsymbol{u}_{h}^{t})\|_{\boldsymbol{\Sigma}_{t}^{-1}},1\right\}}_{(a)}+4c_{ \max}H\sqrt{T\log 1/\delta}.\]

By Cauchy-Schwarz, we can bound \((a)\) as

\[(a)\leq\frac{2c_{\max}\beta^{T}}{\sigma_{\boldsymbol{w}}}\cdot\sqrt{T}\sqrt{ \sum_{t=1}^{T}\sum_{h=1}^{H}\min\left\{\|\boldsymbol{\phi}(\boldsymbol{x}_{h}^ {t},\boldsymbol{u}_{h}^{t})\|_{\boldsymbol{\Sigma}_{t}^{-1}}^{2},1\right\}}.\]We have

\[\sum_{t=1}^{T}\sum_{h=1}^{H}\min\Big{\{}\|\bm{\phi}(\bm{x}_{h}^{t}, \bm{u}_{h}^{t})\|_{\bm{\Sigma}_{t}^{-1}}^{2},1\Big{\}} H\leq\sum_{t=1}^{T}\min\left\{\sum_{h=1}^{H}\|\bm{\phi}(\bm{x}_{h}^ {t},\bm{u}_{h}^{t})\|_{\bm{\Sigma}_{t}^{-1}}^{2},1\right\}\] \[\leq 2H\log(\det(\bm{\Sigma}_{T})\det(\bm{\Sigma}_{0})^{-1})\]

where the last inequality uses Lemma B.6 of [17]. Putting all of this together, we have shown that with probability at least \(1-2\delta\), we have

\[\mathcal{R}_{T}\leq\frac{2c_{\max}\beta^{T}}{\sigma_{\bm{w}}}\cdot\sqrt{T} \cdot\sqrt{2H\log(\det(\bm{\Sigma}_{T})\det(\bm{\Sigma}_{0})^{-1})}+4c_{\max}H \sqrt{T\log 1/\delta}.\]

It remains to bound \(\beta^{T}\) and \(\log(\det(\bm{\Sigma}_{T})\det(\bm{\Sigma}_{0})^{-1})\). We have \(\bm{\Sigma}_{0}=\lambda I\), so \(\det(\bm{\Sigma}_{0})=\lambda^{d_{\bm{\phi}}}\). Furthermore, if \(\|\bm{\phi}(\bm{x},\bm{u})\|_{2}\leq B_{\bm{\phi}}\), then we can bound \(\det(\bm{\Sigma}_{T})\leq(\lambda+B_{\bm{\phi}}^{2}TH)^{d_{\bm{\phi}}}\). Putting this together we have

\[\log(\det(\bm{\Sigma}_{T})\det(\bm{\Sigma}_{0})^{-1})\leq d_{\bm{\phi}}\cdot \log(1+B_{\bm{\phi}}^{2}TH/\lambda).\]

Recalling that

\[\beta^{T}=\sqrt{\lambda}B_{A}+\sigma_{\bm{w}}\sqrt{8d_{\bm{x}}\log 5+8\log(T \det(\bm{\Sigma}_{T})\det(\bm{\Sigma}_{0})^{-1}/\delta)}\]

we can similarly bound

\[\beta^{T}/\sigma_{\bm{w}} \leq\sqrt{\lambda}B_{A}/\sigma_{\bm{w}}+\sqrt{8d_{\bm{x}}\log 5+8d_{\bm{ \phi}}\cdot\log(1+B_{\bm{\phi}}^{2}TH/\lambda)+8\log(T/\delta)}\] \[\leq\sqrt{\lambda}B_{A}/\sigma_{\bm{w}}+c\sqrt{d_{\bm{x}}+d_{\bm {\phi}}\log(1+B_{\bm{\phi}}TH/\lambda)+\log 1/\delta}.\]

Choosing \(\lambda=\sigma_{\bm{w}}^{2}\) completes the proof. 

### Supporting Lemmas

**Lemma E.1**.: _Under Assumption 14, we can bound_

\[\mathcal{J}(\pi;A_{\star})-\mathcal{J}(\pi;A)\leq c_{\max}\cdot\mathbb{E}_{A_ {\star},\pi}\left[\sum_{h=1}^{H}\min\left\{\frac{1}{\sigma_{\bm{w}}}\|(A_{ \star}-A)\bm{\phi}(\bm{x}_{h},\bm{u}_{h})\|_{2},1\right\}\right].\]

Proof.: Following the proof of Lemma B.3 of [17], and adopting the same notation, we have

\[\mathcal{J}(\pi;A_{\star})-\mathcal{J}(\pi;A)\leq\sum_{h=1}^{H}\mathbb{E}_{A_ {\star},\pi}\left[\sqrt{A_{h}}\min\left\{\frac{1}{\sigma_{\bm{w}}}\|(A_{\star} -A)\bm{\phi}(\bm{x}_{h},\bm{u}_{h})\|_{2},1\right\}\right].\]

Under Assumption 14 we have \(A_{h}\leq c_{\max}^{2}\). Plugging this in gives the result. 

**Lemma E.2**.: _Let \(\beta^{t}:=\sqrt{\lambda}B_{A}+\sigma_{\bm{w}}\sqrt{8d_{\bm{x}}\log 5+8\log(T \det(\bm{\Sigma}_{t})\det(\bm{\Sigma}_{0})^{-1}/\delta)}\) and let \(\mathcal{E}_{1}\) denote the event_

\[\mathcal{E}_{1}:=\left\{\forall t\leq T\;:\;\left\|\left(\bar{A}^{t}-A_{\star }\right)\bm{\Sigma}_{t}^{1/2}\right\|_{\mathrm{op}}\leq\beta^{t}\right\}.\]

_Then running \(\mathrm{LC}^{3}\) we have \(\mathbb{P}_{A_{\star}}[\mathcal{E}_{1}]\geq 1-\delta\)._

Proof.: The proof of Lemma B.S of [17] shows that with probability at least \(1-\delta\),

\[\left\|\left(\bar{A}^{t}-A_{\star}\right)\bm{\Sigma}_{t}^{1/2}\right\|_{ \mathrm{op}}\leq\sqrt{\lambda}\|A_{\star}\|_{\mathrm{op}}+\sigma_{\bm{w}} \sqrt{8d_{\bm{x}}\log 5+8\log(\det(\bm{\Sigma}_{t})\det(\bm{\Sigma}_{0})^{-1}/\delta)}.\]

The result then follows from this, since \(\|A_{\star}\|_{\mathrm{op}}\leq B_{A}\), and a union bound.

**Lemma E.3**.: _With probability \(1-\delta\), we have_

\[\sum_{t=1}^{T}\mathbb{E}_{A_{*},\pi^{t}}\left[\sum_{h=1}^{H}\min \left\{\frac{2\beta^{t}}{\sigma_{\bm{w}}}\|\bm{\phi}(\bm{x}_{h},\bm{u}_{h})\|_{ \bm{\Sigma}_{t}^{-1}},1\right\}\right] \leq\frac{2\beta^{T}}{\sigma_{\bm{w}}}\sum_{t=1}^{T}\sum_{h=1}^{H} \min\left\{\|\bm{\phi}(\bm{x}_{h}^{t},\bm{u}_{h}^{t})\|_{\bm{\Sigma}_{t}^{-1}},1\right\}\] \[\quad+4H\sqrt{T\log 1/\delta}.\]

Proof.: This is an immediate consequence of Azuma-Hoeffding, since \(\sum_{h=1}^{H}\min\left\{\frac{2\beta^{t}}{\sigma_{\bm{w}}}\|\bm{\phi}(\bm{x}_ {h},\bm{u}_{h})\|_{\bm{\Sigma}_{t}^{-1}},1\right\}\leq H\) almost surely, and from upper bounding

\[\min\left\{\frac{2\beta^{t}}{\sigma_{\bm{w}}}\|\bm{\phi}(\bm{x}_{h}^{t},\bm{u} _{h}^{t})\|_{\bm{\Sigma}_{t}^{-1}},1\right\}\leq\frac{2\beta^{T}}{\sigma_{\bm {w}}}\min\left\{\|\bm{\phi}(\bm{x}_{h}^{t},\bm{u}_{h}^{t})\|_{\bm{\Sigma}_{t}^ {-1}},1\right\}.\]

## Appendix F Lower Bounds on Learning in Nonlinear Systems

In this section, we assume that \(\bm{\theta}_{\star}\) and \(\pi_{\star}\) correspond to the global minimizer:

\[\bm{\theta}_{\star}(A):=\operatorname*{arg\,min}_{\bm{\theta}\in\mathbb{R}^{d _{\bm{\theta}}}}\mathcal{J}(\bm{\theta};A),\quad\pi_{\star}(A):=\operatorname* {arg\,min}_{\pi\in\Pi^{\star}}\mathcal{J}(\pi;A).\] (F.1)

Here we formally state the additional assumptions needed in Section 4.2, and provide a formal version of Theorem 2.

**Assumption 15**.: _There exists some \(r_{\mu}(A_{*})>0\) such that, for all \(A\in\mathcal{B}_{\mathrm{F}}(A_{\star},r_{\mu}(A_{\star}))\), \(\pi_{\star}(A)\) is unique and, furthermore, there exists some \(\mu>0\) such that_

\[\mathcal{J}(\bm{\theta};A)\geq\mathcal{J}(\bm{\theta}_{\star}(A);A)+\tfrac{ \mu}{2}\|\bm{\theta}-\bm{\theta}^{\star}(A)\|_{2}^{2}.\]

Assumption 15 will be satisfied in cases where \(\mathcal{J}(\pi^{\bm{\theta}};A)\) is strongly convex in \(\bm{\theta}\), but may hold even when this is not the case. Intuitively, it requires that our controller class is not overparameterized--moving \(\bm{\theta}\) away from its optimal value will cause the loss to increase. We will additionally make the following regularity assumptions on policies in \(\Pi_{\mathrm{exp}}\) and their induced covariates set, \(\bm{\Omega}\).

**Assumption 16**.: _There exists some \(\underline{\lambda}>0\) such that, for each \(\bm{\Lambda}\in\bm{\Omega}\), we have \(\lambda_{\min}(\bm{\Lambda})\geq\underline{\lambda}\)._

Assumption 16 requires that _every_ exploration policy we consider excites all directions in \(\bm{\phi}\) space (in contrast, Assumption 3 only assumes there _exists_ some distribution over policies in \(\Pi_{\mathrm{exp}}\) which excite all directions). We remark that this assumption is relatively mild if Assumption 3 holds. As we show in Appendix C.2, under Assumption 3, a mixture over policies, \(\omega\), satisfying \(\lambda_{\min}(\mathbb{E}_{\pi\sim\omega}[\bm{\Lambda}_{\pi}])>0\) can be learned using only a number of samples scaling polynomially in problem parameters. Given \(\omega\), a policy class \(\Pi_{\mathrm{exp}}\) satisfying Assumption 16 can be obtained by simply mixing \(\omega\) with every other exploration policy. We are now ready to state our main lower bound.

**Theorem 7** (Formal Version of Theorem 2).: _Under Assumptions 1, 2, 4, 5, 13, 15 and 16 and if \(\pi_{\star}\) is defined as in (F.1), as long as \(T\geq C_{\mathrm{lb}}\), for any \(\omega_{\mathrm{exp}}\in\triangle_{\Pi_{\mathrm{exp}}}\), we have_

\[\min_{\pi}\max_{A\in\mathcal{B}_{T}}\mathbb{E}_{\mathfrak{D}_{T}\sim A,\omega_ {\mathrm{exp}}}[\mathcal{J}(\widehat{\pi}(\mathfrak{D}_{T});A)-\mathcal{J}( \pi_{\star}(A);A)]\geq\frac{\sigma_{\bm{w}}^{2}}{3T}\cdot\min_{\bm{\Lambda}\in \bm{\Omega}}\operatorname*{tr}(\mathcal{H}(A_{*})\hat{\bm{\Lambda}}^{-1})-\frac {C_{\mathrm{lb}}}{T^{5/4}}\]

_for \(\mathcal{B}_{T}:=\{A\ :\ \|A-A_{\star}\|_{\mathrm{F}}^{2}\leq 5d_{\bm{x}}d_{\bm{ \phi}}/(\lambda d_{\bm{x}}TH)^{5/6}\}\), \(\mathbb{E}_{\mathfrak{D}_{T}\sim A,\omega_{\mathrm{exp}}}[\cdot]=\mathbb{E}_{ \pi\sim\omega_{\mathrm{exp}}}[\mathbb{E}_{\mathfrak{D}_{T}\sim A,\pi}[\cdot]]\) denotes the expectation over trajectories generated by running policies \(\pi\) drawn according to \(\omega_{\mathrm{exp}}\) on system \(A\) for \(T\) episodes, \(\widehat{\pi}\) any mapping from observations to policies in \(\Pi^{\star}\), and_

\[C_{\mathrm{lb}}:=\mathrm{poly}\left(d_{\bm{\phi}},d_{\bm{x}},H,\|A_{\star}\|_{ \mathrm{op}},B_{\bm{\phi}},L_{\bm{\phi}},L_{\bm{\theta}},L_{\mathrm{cost}},L_{ \pi_{\star}},\sigma_{\bm{w}},\sigma_{\bm{w}}^{-1},\tfrac{1}{\underline{\lambda} },\tfrac{1}{\mu},\tfrac{1}{r_{\mathrm{cost}}(A_{\star})},\tfrac{1}{r_{\bm{ \theta}}(A_{\star})}\right).\]

Proof of Theorem 7.: This proof follows immediately from Lemma F.1, by lower bounding the right-hand side of (F.2) by the min over all policies in \(\Pi\)

**Lemma F.1**.: _Under Assumptions 1, 2, 4, 5, 13, 15 and 16 and if \(\bm{\theta}_{\star}\) is defined as in (F.1), as long as_

\[T\geq\mathrm{poly}(\|A_{\star}\|_{\mathrm{op}},L_{\bm{\phi}},L_{ \bm{\theta}},L_{\bm{\theta}_{\star}},L_{\mathrm{cost}},\sigma_{\bm{w}},\sigma_{ \bm{w}}^{-1},B_{\bm{\phi}},H,d_{\bm{x}},d_{\bm{\phi}},\lambda^{-1},\mu^{-1},r_ {\mathrm{cost}}(A_{\star})^{-1},r_{\bm{\theta}}(A_{\star})^{-1},r_{\mu}(A_{ \star})^{-1}),\]

_for any \(\omega_{\mathrm{exp}}\in\triangle_{\Pi}\), we have_

\[\min_{\widehat{\bm{\theta}}}\max_{A\in\mathcal{B}_{T}}\mathbb{E} _{\mathfrak{D}_{T}\sim A,\omega_{\mathrm{exp}}}[\mathcal{J}(\widehat{\bm{ \theta}}(\mathfrak{D}_{T});A)-\mathcal{J}(\bm{\theta}_{\star}(A);A)]\geq\frac {\sigma_{\bm{w}}^{2}}{3T}\cdot\mathrm{tr}(\mathcal{H}(A_{\star})\mathbb{E}_{ \pi\sim\omega_{\mathrm{exp}}}[\hat{\bm{\Lambda}}_{\pi}]^{-1})-\frac{C_{\mathrm{ lb}}}{T^{5/4}}\] (F.2)

_for \(\mathcal{B}_{T}:=\{A\,:\,\|A-A_{\star}\|_{\mathrm{F}}^{2}\leq 5d_{\bm{a}}d_{ \bm{\phi}}/(\lambda TH)^{5/6}\}\), where \(\mathbb{E}_{\mathfrak{D}_{T}\sim A,\omega_{\mathrm{exp}}}[:]=\mathbb{E}_{\pi \sim\omega_{\mathrm{exp}}}[\mathbb{E}_{\mathfrak{D}_{T}\sim A,\pi}[:]]\) denotes the expectation over trajectories generated by running policies \(\pi\) drawn according to \(\omega_{\mathrm{exp}}\) on system \(A\) for \(T\) episodes, and_

\[C_{\mathrm{lb}}:=\mathrm{poly}(\|A_{\star}\|_{\mathrm{op}},L_{ \bm{\phi}},L_{\bm{\theta}},L_{\bm{\pi}_{\star}},L_{\mathrm{cost}},\sigma_{\bm{ w}},\sigma_{\bm{w}}^{-1},B_{\bm{\phi}},H,d_{\bm{x}},d_{\bm{\phi}},\lambda^{-1},\mu^{-1},r_ {\mathrm{cost}}(A_{\star})^{-1},r_{\bm{\theta}}(A_{\star})^{-1},r_{\mu}(Ast)^{ -1}).\]

Proof.: This result is a direct consequence of Theorem 6.1 of [2]--to obtain the result we must only verify that the assumptions of this result are met. We verify each assumption below.

Verifying Assumption 3 of [2].Part 1 of Assumption 3 of [2] is met by Assumption 15 within diameter \(r_{\mu}(A_{\star})\). Furthermore, under Assumptions 1, 2, 4, 5 and 13 and by Lemma D.1, the additional parts of Assumption 3 of [2] are also met with diameter \(\min\{r_{\mathrm{cost}}(A_{\star}),r_{\bm{\theta}}(A_{\star})\}\) and smoothness constant \(\mathrm{poly}(\|A_{\star}\|_{\mathrm{op}},L_{\bm{\phi}},L_{\bm{\theta}},L_{\bm{ \pi}_{\star}},L_{\mathrm{cost}},\sigma_{\bm{w}}^{-1},H,d_{\bm{x}})\).

Verifying Assumption 4 and Assumption 5 of [2].Assumption 4 of [2] is immediately met by Assumption 16. Furthermore, Assumption 5 is met by Lemma F.2 with \(c_{\mathrm{cov}}=1,L_{\mathrm{cov}}(\theta_{\star},\gamma^{2})=\frac{H^{2}B_{ \bm{\phi}}^{3}}{\sigma_{\bm{w}}^{2}}\cdot\mathrm{poly}(d_{\bm{x}})\), and \(C_{\mathrm{cov}}=0\).

Given that these assumptions are met, the result follows noting that, if we run for \(T\) episodes, then the effective horizon is \(d_{\bm{x}}TH\) (using the mapping from the setting of (1.1) to the martingale regression setting described in Appendix A.1.1). Note that the final bound scales with \(\frac{1}{T}\) instead of \(\frac{1}{d_{\bm{x}}TH}\) as we are able to bring the \(d_{\bm{x}}H\) factor into the \(\hat{\bm{\Lambda}}_{\pi_{\mathrm{exp}}}\) term, since \(\bm{\Lambda}_{\pi_{\mathrm{exp}}}\) is not normalized by \(d_{\bm{x}}H\). 

**Lemma F.2**.: _Under Assumption 1, for any policy distribution \(\omega\in\triangle_{\Pi_{\mathrm{exp}}}\) and \(A,A^{\prime}\), we have_

\[\mathbb{E}_{\pi\sim\omega}[\bm{\Lambda}_{A,\pi}]\preceq\mathbb{E}_ {\pi\sim\omega}[\bm{\Lambda}_{A^{\prime},\pi}]+\frac{H^{2}B_{\bm{\phi}}^{3}}{ \sigma_{\bm{w}}^{2}}\cdot\mathrm{poly}(d_{\bm{x}})\cdot\|A-A^{\prime}\|_{ \mathrm{F}}\cdot I.\]

Proof.: We will prove that the desired bound follows for a particular \(\pi\in\Pi_{\mathrm{exp}}\), which immediately implies that it holds for \(\omega\in\triangle_{\Pi_{\mathrm{exp}}}\). By definition we have

\[\bm{\Lambda}_{A,\pi}=\int\left(\sum_{h=1}^{H}\bm{\phi}(\bm{x}_{h }^{\bm{\tau}},\bm{u}_{h}^{\bm{\tau}})\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\bm{u}_{h }^{\bm{\tau}})^{\top}\right)\cdot f_{A,\pi}(\bm{\tau})\mathrm{d}\bm{\tau}\]

and

\[f_{A,\pi}(\bm{\tau})=\prod_{h=1}^{H}f_{A}(\bm{x}_{h+1}^{\bm{\tau} }\mid\bm{x}_{h}^{\bm{\tau}},\bm{u}_{h}^{\bm{\tau}})\pi_{h}(\bm{u}_{h}^{\bm{ \tau}}\mid\bm{x}_{h}^{\bm{\tau}}).\]

Fix some \(\bm{v}\in\mathcal{S}^{d_{\bm{\phi}}-1}\), and note that, given the expression above, we have

\[\bm{v}^{\top}\bm{\Lambda}_{A,\pi}\bm{v}=\int\sum_{h=1}^{H}(\bm{v}^{\top}\bm{ \phi}(\bm{x}_{h}^{\bm{\tau}},\bm{u}_{h}^{\bm{\tau}}))^{2}\cdot f_{A,\pi}(\bm{ \tau})\mathrm{d}\bm{\tau}.\]

It follows that

\[\nabla_{A}\bm{v}^{\top}\bm{\Lambda}_{A,\pi}\bm{v}=\int\sum_{h=1}^{H}(\bm{v}^{ \top}\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\bm{u}_{h}^{\bm{\tau}}))^{2}\cdot\nabla_{A} f_{A,\pi}(\bm{\tau})\mathrm{d}\bm{\tau}\]\[=\int\sum_{h=1}^{H}(\bm{v}^{\top}\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\bm{u}_{h}^{ \bm{\tau}}))^{2}\cdot f_{A,\pi}(\bm{\tau})\nabla_{A}\log f_{A,\pi}(\bm{\tau}) \mathrm{d}\bm{\tau}.\]

As in the proof of Lemma D.1, we have, for any \(\Delta\),

\[\nabla_{A}\log f_{A,\pi}(\bm{\tau})[\Delta]=\sum_{h=1}^{H}\frac{1}{\sigma_{\bm {w}}^{2}}(\bm{x}_{h+1}^{\bm{\tau}}-A\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\bm{u}_{h} ^{\bm{\tau}}))\cdot\Delta\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\bm{u}_{h}^{\bm{\tau }}),\]

so we can bound

\[\|\nabla_{A}\log f_{A,\pi}(\bm{\tau})\|_{\mathrm{op}}\leq\frac{B_{\bm{\phi}}}{ \sigma_{\bm{w}}^{2}}\sum_{h=1}^{H}\|\bm{x}_{h+1}^{\bm{\tau}}-A\bm{\phi}(\bm{x }_{h}^{\bm{\tau}},\bm{u}_{h}^{\bm{\tau}})\|_{2}.\]

Furthermore, we can also bound \((\bm{v}^{\top}\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\bm{u}_{h}^{\bm{\tau}}))^{2} \leq B_{\bm{\phi}}^{2}\). We therefore have

\[\|\nabla_{A}\bm{v}^{\top}\bm{\Lambda}_{A,\pi}\bm{v}\|_{\mathrm{op}} \leq HB_{\bm{\phi}}^{3}\int\sum_{h=1}^{H}\|\bm{x}_{h+1}^{\bm{\tau} }-A\bm{\phi}(\bm{x}_{h}^{\bm{\tau}},\bm{u}_{h}^{\bm{\tau}})\|_{2}\cdot f_{A, \pi}(\bm{\tau})\mathrm{d}\bm{\tau}\] \[\leq\frac{H^{2}B_{\bm{\phi}}^{3}}{\sigma_{\bm{w}}^{2}}\cdot \mathrm{poly}(d_{\bm{w}})\]

where the last inequality follows from Lemma A.1. It follows from the Mean Value Theorem that

\[|\bm{v}^{\top}\bm{\Lambda}_{A,\pi}\bm{v}-\bm{v}^{\top}\bm{\Lambda}_{A^{\prime },\pi}\bm{v}|\leq\frac{H^{2}B_{\bm{\phi}}^{3}}{\sigma_{\bm{w}}^{2}}\cdot \mathrm{poly}(d_{\bm{w}})\cdot\|A-A^{\prime}\|_{\mathrm{F}}.\]

As this holds for all \(\bm{v}\in\mathcal{S}^{d-1}\), it follows that

\[\|\bm{\Lambda}_{A,\pi}-\bm{\Lambda}_{A^{\prime},\pi}\|_{\mathrm{op}}\leq\frac {H^{2}B_{\bm{\phi}}^{3}}{\sigma_{\bm{w}}^{2}}\cdot\mathrm{poly}(d_{\bm{x}}) \cdot\|A-A^{\prime}\|_{\mathrm{F}}.\]

## Appendix G Additional Experimental Details

In this section, we provide additional details on our experimental results presented in Section 6. All experiments were run on a machine with 56 Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz CPUs, and 64GB RAM. All code was implemented in PyTorch.

### Details on Problem Settings and Controller Parameterizations

We first expand on the precise definitions of the systems considered. As noted in Section 6, for the drone and car examples we set \(H=50\), and for the system of Section 1.1 we set \(H=10\). In addition, for all examples the noise is distributed as \(\bm{w}_{h}\sim\mathcal{N}(0,0.1\cdot I)\). In all cases we set \(\gamma^{2}=10H\) (where \(\gamma^{2}\) is a bound on \(\mathbb{E}_{\pi_{\mathrm{exp}}}[\sum_{h=1}^{H}\bm{u}_{h}^{\top}\bm{u}_{h}]\)), and we therefore let \(\Pi_{\mathrm{exp}}\) denote the set of all policies satisfying \(\mathbb{E}_{\pi_{\mathrm{exp}}}[\sum_{h=1}^{H}\bm{u}_{h}^{\top}\bm{u}_{h}]\leq \gamma^{2}\).

#### g.1.1 System of Section 1.1 (Figure 1)

The dynamics for this system are given by

\[\bm{x}_{h+1}=0.8\bm{x}_{h}+\bm{u}_{h}-\sum_{i=1}^{10}3\bm{\phi}_{i}(\bm{x}_{h })+\bm{w}_{h}\]

for \(\bm{\phi}_{i}(\bm{x})=\max\{1-100(\bm{x}-c_{i})^{2},0\}\), and \(\mathrm{cost}(\bm{x},\bm{u})=(\bm{x}-c_{1})^{2}+100^{-1}\cdot\bm{u}^{2}\). We set

\[c_{1}=10,c_{2}=-14,c_{3}=-11,c_{4}=-8,c_{5}=-5,c_{6}=-2,c_{7}=1,c_{8}=4,c_{9}=7.\]

This then corresponds to a system in the form (1.1) with

\[A_{*}=[0.8,1,-3,\ldots,-3],\quad\bm{\phi}(\bm{x},\bm{u})=[\bm{x},\bm{u},\bm{ \phi}_{1}(\bm{x}),\ldots,\bm{\phi}_{10}(\bm{x})].\]For this system, we parameterize our controller class \(\Pi^{\star}\) as, for any \(\pi^{\bm{\theta}}\in\Pi^{\star}\) with parameter \(\bm{\theta}\),

\[\pi^{\bm{\theta}}(\bm{x})=\bm{\theta}_{1}\bm{x}+\sum_{i=1}^{10}\bm{\theta}_{i+1} \bm{\phi}_{i}(\bm{x})+\bm{\theta}_{12}.\]

Note that the form of this controller lets us simply "match" the parameters of the system, and cancel undesirable parameters. Given this, for this system we let \(\pi_{\star}(A)\) be the controller which sets \(\bm{\theta}_{1:11}\) to cancel the dynamics of the system \(A\), and set \(\bm{\theta}_{12}=c_{1}=10\).

See Appendix G.1.3 for details on the computation of \(\mathcal{H}(A)\) on this system.

#### g.1.2 Drone System (Figure 2)

The dynamics of this system are given by

\[\bm{x}_{h+1}=\begin{bmatrix}1&0&0&0.1&0&0\\ 0&1&0&0&0.1&0\\ 0&0&1&0&0&0.1\\ 0&0&0&1&0&0\\ 0&0&0&0&1&0\\ 0&0&0&0&1\end{bmatrix}\bm{x}_{h}+\begin{bmatrix}0&0&0\\ 0&0&0\\ 0&0&0\\ 0.1&0&0\\ 0&0.1&0\\ 0&0&0.1\end{bmatrix}\bm{u}_{h}+\begin{bmatrix}0\\ 0\\ 0\\ 0\\ 0\\ -0.98\end{bmatrix}+\bm{w}_{h}.\] (G.1)

Here we interpret \([\bm{x}]_{1:3}\) as the \(x,y,\) and \(z\) positions, respectively, and \([\bm{x}]_{4:6}\) as the \(x,y,z\) velocities. This system is therefore equivalent to three double integrator systems, with an affine term (which we interpret as "gravity") affecting only the \(z\) coordinate. We set the cost to

\[\cos(\bm{x},\bm{u})=\frac{0.1}{5}\cdot\sum_{i=1}^{d_{\bm{x}}}[\bm{x}]_{i}^{2}+ \frac{1}{5}\cdot[\bm{u}]_{1}^{2}+[\bm{u}]_{2}^{2}+[\bm{u}]_{3}^{2}\]

This then corresponds to a system in the form (1.1) with

\[A_{\star}=\begin{bmatrix}1&0&0&0.1&0&0&0&0&0&0\\ 0&1&0&0&0.1&0&0&0&0&0\\ 0&0&1&0&0&0.1&0&0&0&0\\ 0&0&0&1&0&0.1&0&0&0\\ 0&0&0&0&1&0&0&0.1&-0.98\end{bmatrix},\quad\bm{\phi}(\bm{x},\bm{u})=[\bm{x}, \bm{u},1].\]

For this system, we parameterize our controller class \(\Pi^{\star}\) as, for any \(\pi^{\bm{\theta}}\in\Pi^{\star}\) with parameter \(\bm{\theta}\),

\[\pi^{\bm{\theta}}_{h}(\bm{x})=\bm{\theta}^{\mathrm{fb}}_{h}\bm{x}+\bm{\theta} ^{\mathrm{offset}}_{h}\]

where \(\bm{\theta}^{\mathrm{fb}}_{h}\in\mathbb{R}^{3\times 6}\) is the state-feedback portion of the controller, and \(\bm{\theta}^{\mathrm{offset}}_{h}\in\mathbb{R}^{3}\) is an offset term. It can be shown that the optimal controller for a system of the form (G.1) can be parameterized in this way [23]. Furthermore, the optimal parameters can be computed in closed-form. As such, for this system we set \(\pi_{\star}(A)\) to be with the optimal parameters, computed using this closed-form solution.

In addition to computing the optimal controller in closed-form, we can also compute the cost of a controller, \(\mathcal{J}(\pi;A)\), in closed-form. To compute \(\mathcal{H}(A)\) in this example, we then simply apply the torch.autograd.functional.hessian function to \(\mathcal{J}(\pi_{\star}(A);A)\).

#### g.1.3 Car System (Figure 3)

The dynamics of this system are given by

\[\bm{x}_{h+1}=\begin{bmatrix}1&0&0.1&0&0&0\\ 0&1&0&0.1&0&0\\ 0&0&1&0&0&0\\ 0&0&0&1&0&0\\ 0&0&0&0&1&0\\ 0&0&0&0&0&1\end{bmatrix}\bm{x}_{h}+\begin{bmatrix}0&0&0\\ 0&0&0\\ 0.1\cdot\cos([\bm{x}_{h}]_{5})&0\\ 0.1\cdot\sin([\bm{x}_{h}]_{5})&0\\ 0&0\\ 0&0.1\end{bmatrix}\bm{u}_{h}+\bm{w}_{h}\] (G.2)where \([\bm{x}_{h}]_{5}\) denotes the 5th element of \(\bm{x}_{h}\). Here we interpret \([\bm{x}_{h}]_{1}\) as the \(x\) position, \([\bm{x}_{h}]_{2}\) as the \(y\) position, \([\bm{x}_{h}]_{3}\) as the \(x\) velocity, \([\bm{x}_{h}]_{4}\) as the \(y\) velocity, \([\bm{x}_{h}]_{5}\) as the angle of orientation (that is, the direction the car is facing), and \([\bm{x}_{h}]_{6}\) as the angular velocity. The first control dimension, then, corresponds to the "gas", the power given to the car to move forward or backward, and the second control dimension corresponds to altering the direction of the steering wheel. Similar to the drone system, we set the cost to

\[\mathrm{cost}(\bm{x},\bm{u})=\begin{bmatrix}\bm{x}\\ \bm{u}\end{bmatrix}^{\top}Q\begin{bmatrix}\bm{x}\\ \bm{u}\end{bmatrix}\quad\text{with}\quad Q=\frac{0.1\cdot I+\bm{v}_{1}\bm{v}_{ 1}^{\top}+\bm{v}_{2}\bm{v}_{2}^{\top}}{\|0.1\cdot I+\bm{v}_{1}\bm{v}_{1}^{\top }+\bm{v}_{2}\bm{v}_{2}^{\top}\|_{\mathrm{op}}}\]

for some \(\bm{v}_{1},\bm{v}_{2}\). To write this in the form of (1.1), in order to make the problem more challenging we choose an overparameterized \(\bm{\phi}(\bm{x},\bm{u})\):

\[\bm{\phi}(\bm{x},\bm{u})=\begin{bmatrix}\bm{x},\bm{u},\mathrm{cos}([\bm{x}]_{5 }),\sin([\bm{x}]_{5}),[\bm{u}]_{1}\cdot\mathrm{cos}([\bm{x}]_{5}),[\bm{u}]_{1} \cdot\mathrm{sin}([\bm{x}]_{5}),[\bm{u}]_{2}\cdot\mathrm{cos}([\bm{x}]_{5}),[ \bm{u}]_{2}\cdot\mathrm{sin}([\bm{x}]_{5})\end{bmatrix}\]

and set

\[A_{\star}=\begin{bmatrix}1&0&0.1&0&0&0&0&0&0&0&0&0&0&0\\ 0&1&0&0.1&0&0&0&0&0&0&0&0&0&0\\ 0&0&1&0&0&0&0&0&0&0&0.1&0&0&0\\ 0&0&0&1&0&0&0&0&0&0&0&0.1&0&0\\ 0&0&0&0&1&0.1&0&0&0&0&0&0&0&0\\ 0&0&0&0&0&1&0&0.1&0&0&0&0&0&0&0\end{bmatrix}.\]

For the car system, the controller class \(\Pi^{\star}\) is a hierarchical controller parameterized by some \(\bm{\theta}\in\mathbb{R}^{4}\). This controller first uses PD control to compute a "goal input", the direction we would like to modify the state in, as:

\[\bm{u}_{\mathrm{goal}}(\bm{x})=-\bm{\theta}_{1}[\bm{x}]_{1:2}-\bm{\theta}_{2 }[\bm{x}]_{3:4}.\]

Given the underactuated structure of the system in (G.2), we cannot directly push the state in the direction of \(\bm{u}_{\mathrm{goal}}(\bm{x})\). Instead, we set \(\bm{u}\) to the following:

\[\begin{bmatrix}\bm{u}_{\mathrm{goal}}(\bm{x})^{\top}\begin{bmatrix}\mathrm{ cos}([\bm{x}]_{5})\\ \mathrm{sin}([\bm{x}]_{5})\end{bmatrix}\\ -\bm{\theta}_{3}([\bm{x}]_{5}-\beta_{\mathrm{goal}}(\bm{x}))-\bm{\theta}_{4} [\bm{x}]_{6}\end{bmatrix}\quad\text{for}\quad\beta_{\mathrm{goal}}(\bm{x})= \mathrm{tan}^{-1}([\bm{u}_{\mathrm{goal}}(\bm{x})]_{2}/[\bm{u}_{\mathrm{goal }}(\bm{x})]_{1}).\]

Given the complex form of this controller and the dynamics, there does not exist a closed-form way to set \(\bm{\theta}\) optimally. Instead, for this system, we rely on a simple random search procedure to compute \(\pi_{\star}(A)\). To find an optimal controller for system \(A\), we randomly sample parameters \(\bm{\theta}\), compute the cost they incur on system \(A\), and then set \(\pi_{\star}(A)\) to the randomly generated controller with lowest cost. Note that this procedure is not differentiable, but we require \(\pi_{\star}(A)\) is differentiable. To remedy this, in situations where a differentiable \(\pi_{\star}(A)\) is needed (in particular, in the computation of \(\mathcal{H}(A)\)), rather than returning a single controller, we return the softmin distribution over all controllers sampled, weighting each controller by its estimated cost. As the softmin distribution can be differentiated, this parameterization of \(\pi_{\star}(A)\) is differentiable.

For this system, there does not exist a closed-form expression for \(\mathcal{J}(\pi;A)\) and, as such, to compute \(\mathcal{J}(\pi;A)\), we simply perform many roll-outs of policy \(\pi\) on system \(A\) and average the cost. Given this and the search-based implementation of \(\pi_{\star}(A)\) outlined above, we found that computing the hessian \(\mathcal{H}(A)\) using the torch.autograd.functional.hessian as in Appendix G.1.2 was very memory-intensive. Instead, we computed the Jacobian \(G(A):=\nabla_{A^{\prime}}\mathcal{J}(\pi_{\star}(A^{\prime});A)|_{A^{\prime}=A}\), and then, in place of \(\mathcal{H}(A)\), we use \(G(A)G(A)^{\top}\). To compute \(G(A)\), we use the torch.autograd.functional.jacobian function. While using \(G(A)G(A)^{\top}\) in place of \(\mathcal{H}(A)\) is not justified by our theoretical analysis, if we are in settings where \(\pi_{\star}(A)\) is not precisely the minimum of \(\mathcal{J}(\pi;A)\) (which will likely be the case here since we are relying on a sampling-based implementation of \(\pi_{\star}(A)\), which will incur some small error), then we argue that this is a reasonable metric to use. In particular, in this setting, the approximation of \(\mathcal{J}(\pi_{\star}(\widehat{A});A_{\star})\) given in Proposition 1 should have an additional first-order term of the form \(G(A)^{\top}\mathrm{vec}(A_{\star}-\widehat{A})\). As we can upper bound

\[G(A)^{\top}\mathrm{vec}(A_{\star}-\widehat{A})\leq\sqrt{\|\mathrm{vec}(A_{ \star}-\widehat{A})\|^{2}_{G(A_{\star})G(A_{\star})^{\top}}},\]optimizing for the metric \(G(A)G(A)^{\top}\) instead of \(\mathcal{H}(A)\) can be seen as minimizing the first-order Taylor-approximation of the excess loss. Intuitively, this metric quantifies the sensitivity of the loss to particular parameters in \(A_{\star}\), and in practice we found that optimizing this metric produced significant improvements over existing methods. The implementation of the example from Section 1.1 relied on this same approximation.

### Implementation Details

For all methods considered, our implementation follows the basic structure of Algorithm 1: at every epoch, we explore so as to minimize some exploration objective, form an estimate of \(A_{\star}\) on the collected data, and then compute \(\pi_{\star}(\widehat{A}_{t})\) on our estimate. Our main experimental results (Figures 1 to 3) show the loss of \(\pi_{\star}(\widehat{A}_{t})\) as the time horizon \(t\) increases. For each method, to collect an initial set of data, we begin each trial by exploring randomly for some fixed number of episodes (10 for the drone example, 100 for the car example). The first point in each plot then corresponds to the performance after this initial random exploration. Each aspect of our implementation is modular, and any given component can be easily replaced. Below we highlight our implementation of the exploration routine, and choice of exploration objective, for the various approaches we consider.

#### g.2.1 Implementation of DynamicOED

Implementing the exploration procedure, DynamicOED, requires access to a regret minimization oracle. While in principle the LC\({}^{3}\) algorithm of [17] could be applied to this problem to give such an oracle, the LC\({}^{3}\) algorithm requires access to a computation oracle which is not clear how to implement in practice. To remedy this, we implement a Thompson Sampling-inspired modification to the LC\({}^{3}\) algorithm of [17].

The primary computational challenge of implementing the LC\({}^{3}\) algorithm is the computation of the _optimistic_ policy:

\[\operatorname*{arg\,min}_{\pi\in\Pi_{\mathrm{exp}}}\min_{A\in\operatorname{ BAL}^{t}}\mathcal{J}^{\mathrm{exp}}(\pi;A)\]

where \(\mathcal{J}^{\mathrm{exp}}(\pi;A)\) denotes the exploration cost that is minimized in LC\({}^{3}\) (i.e. the expected cost on the cost function \(\mathrm{cost}^{n}_{h}(\bm{x},\bm{u})\leftarrow\frac{1}{M}\cdot\bm{\phi}(\bm{x },\bm{u})^{\top}(\Xi_{n})\bm{\phi}(\bm{x},\bm{u})\) set in DynamicOED), and Ball\({}^{t}\) the confidence set for \(A_{\star}\) at iteration \(t\).

To avoid solving this optimization, we adopt a Thompson Sampling-inspired variation of this procedure. In particular, at iteration \(t\), we sample \(\widetilde{A}_{t}\sim\mathcal{N}(\widehat{A}_{t},\bm{\Lambda}_{t}^{-1})\). Standard Thompson Sampling would then compute \(\operatorname*{arg\,min}_{\pi\in\Pi_{\mathrm{exp}}}\mathcal{J}^{\mathrm{exp}} (\pi;\widetilde{A}_{t})\), but even this can be challenging, so we instead rely on a sampling MPC-inspired approach. Given that we are at state \(\bm{x}_{h}\) and have played inputs \(\bm{u}_{1},\dots,\bm{u}_{h-1}\), we aim to approximately solve the following optimization:

\[\min_{\bm{u}_{h},\bm{u}_{h+1},\dots,\bm{u}_{H}\in\mathbb{R}^{d_{ u}}}\sum_{h^{\prime}=h}^{H}\mathrm{cost}^{n}_{h}(\bm{x}_{h^{\prime}},\bm{u}_{h^{ \prime}})\] (G.3) \[\text{s.t.}\quad\bm{x}_{h+1}=\widetilde{A}_{t}\bm{\phi}(\bm{x}_{h },\bm{u}_{h}),\sum_{h=1}^{H}\bm{u}_{h}^{\top}\bm{u}_{h}\leq\gamma^{2}.\]

To solve this approximately, we sample many possible \(\bm{u}\) randomly, compute the value of the objective of (G.3) on the trajectories induced by these \(\bm{u}\), and finally choose the input that minimizes this objective. Rather than playing the entire sequence of chosen inputs, however, we simply play the first input in the sequence, observe the new state on the actual system, and re-solve (G.3) on this new state. Note that the implementation of LC\({}^{3}\) used for the experiments given in [17] relies on a similar Thompson Sampling-based approximation to the LC\({}^{3}\) algorithm.

#### g.2.2 Implementation of Uniform Exploration

The goal of the procedure we have referred to as Uniform Exploration is to collect data which will result in the estimation error, \(\lVert A_{\star}-\widehat{A}\rVert_{\mathrm{op}}\), being minimized, the goal of the method given in [1]. It can be shown that this is equivalent to maximizing \(\lambda_{\min}(\bm{\Lambda}_{T})\), so this method reduces to choosing inputs that maximize \(\lambda_{\min}(\bm{\Lambda}_{T})\). To implement this procedure, we rely on the same sampling-based MPC approach as we outlined above, with the primary difference being that instead of minimizing the objective of (G.3), we choose the inputs that maximize

\[\lambda_{\min}\left(\bm{\Lambda}_{t}+\sum_{h=1}^{H}\bm{\phi}(\bm{x}_{h},\bm{u}_ {h})\bm{\phi}(\bm{x}_{h},\bm{u}_{h})^{\top}\right),\]

where \(\bm{\Lambda}_{t}\) denote the covariates we have obtained so far at iteration \(t\). While very similar in spirit to the algorithm of [1], the implementation details are somewhat different than the algorithm proposed in that work. We found that in practice our implementation performed better than directly implementing (a sampling-based variant of) the algorithm from [1], and all reported results for Uniform Exploration are therefore on this version.

#### g.2.3 Exploring via Cost Minimization

A natural point of comparison to our methods would be to forsake the system identification phase entirely, and simply run standard policy optimization algorithms such as TRPO or PPO [61, 62], to obtain a controller \(\widehat{\pi}_{T}\). The primary difficulty with these approaches in the settings we consider is that these algorithms are _on-policy_, meaning that they primarily roll out trajectories using their current estimate of the optimal policy, \(\widehat{\pi}_{t}\), and using the collected data to do policy improvement on \(\widehat{\pi}_{t}\). In contrast, our setting is _off-policy_, in the sense that the learner must explore by playing policies in \(\Pi_{\exp}\), but return some policy in \(\Pi^{\star}\). Since in the settings we consider \(\Pi_{\exp}\neq\Pi^{\star}\), on-policy approaches are simply exploring very differently, and therefore cannot be compared with directly.

This is particularly an issue in our setting where _stability_ may come into play. Indeed, it may be the case that some controller in \(\Pi^{\star}\) will destabilize the system, and cause the norm of the state to increase exponentially in \(h\). Inducing such trajectories significantly improves one's ability to perform system identification as the signal-to-noise ratio also then increases exponentially. However, to induce this trajectory with a state-feedback controller, the power of the input played by this controller will also increase exponentially in \(h\). Since we choose \(\Pi_{\exp}\) to include only policies with bounded power, this is not a fair comparison (and, furthermore, is likely not an algorithm one would want to run in practice).

While direct comparison with such approaches is therefore not possible, it is possible to compare against algorithms that, instead of collecting data that minimizes or maximizes objectives such as \(\operatorname{tr}(\mathcal{H}\hat{\bm{\Lambda}}^{-1})\) or \(\lambda_{\min}(\bm{\Lambda})\), instead simply aims to play policies minimizing \(\mathcal{J}(\pi;A)\). In principle, such algorithms are similar to approaches such as TRPO in how they perform their exploration--both collect data by aiming to minimize the actual cost we are attempting to find a controller to minimize.

To implement this approach, we rely on a sampling-based MPC algorithm similar to that described in Appendix G.2.1, but where the goal is now to solve

\[\min_{\pi\in\Pi_{\exp}}\mathcal{J}(\pi;A).\]

Note that the key difference between this approach and approaches such as TRPO is that we still only play \(\pi\in\Pi_{\exp}\). Using this objective to induce exploration, we then simply estimate \(A_{\star}\) on this collected data, and return \(\pi_{\star}(\widehat{A})\). The results of this approach on the drone system are given in Figure 4 (with this cost minimization approach denoted as Cost Minimization Exploration). As this illustrates, this approach is significantly worse than Algorithm 1, and is also outperformed by Uniform Exploration or Random Exploration when the number of episodes is large enough.

### Additional Results

Finally, in this section we present versions of Figures 1 to 3 with error bars in Figures 5 to 7. In all figures, errors bars denote one standard error.

Figure 4: Performance on drone with LC\({}^{3}\) Exploration

Figure 5: Mean excess controller loss on instance of Section 1.1 with error bars

Figure 6: Mean excess controller loss on drone with error bars

Figure 7: Mean excess controller loss on car with error bars