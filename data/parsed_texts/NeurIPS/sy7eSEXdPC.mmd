# Multi-LLM Debate: Framework, Principals, and Interventions

Andrew Estornell

ByteDance Research

andrew.estornell@bytedance.com &Yang Liu

University of California, Santa Cruz

yangliu@ucsc.edu

###### Abstract

The flexible and generalized nature of large language models has allowed for their application in a wide array of language-based domains. Much like their human contemporaries, these models are capable of engaging in discussions and debates as a means of improving answer quality. We first take a theoretical approach to analyzing debate and provide a framework through which debate can be mathematically examined. Building on this framework, we provide several theoretical results for multi-agent debate. In particular, we demonstrate that similar model capabilities, or similar model responses, can result in static debate dynamics where the debate procedure simply converges to the majority opinion. When this majority opinion is the result of a common misconception (possibly ingrained in the models through shared training data) debate is likely to converge to answers associated with that common misconception. Using insights from our theoretical results, we then propose three interventions that improve the efficacy of debate. For each intervention, we provide theoretical results demonstrating how debate is improved. We also demonstrate that these interventions result in better performance on four common benchmark tasks.

## 1 Introduction

Large language models (LLMs) have demonstrated a remarkable ability to perform unseen tasks with high efficacy. This behavior, often referred to as emergent, allows LLMs to serve as general-purpose tools for a wide array of language-based functions. One such behavior of particular interest is the ability of LLMs to intake and process opinions from other models (or humans). As shown in several previous works, this ability allows LLMs to collaboratively solve tasks by engaging in _debate_ Chan et al. (2023); Liang et al. (2023); Du et al. (2023). For a given task, multi-agent debate operates by eliciting responses from each model, distributing those responses among the models, and then eliciting updated responses from each model.

In this work, we aim to explore the debate procedure, by first providing a theoretical framework through which debate can be better understood. This framework draws inspiration from Bayesian inference and in-context learning, showing that debate can be partially viewed as a special type of in-context learning. Through this framework, we then provide several theoretical insights into the debate procedure. In particular, we demonstrate the susceptibility of multi-agent debate to echo-chamber effects. These echo-chamber effects are especially consequential when they stem from a shared misconception between a majority of models, which can arise from circumstances such as highly correlated training data between each model.

We then leverage results from our theoretical framework to improve the efficacy of the debate procedure. In particular, we propose three _interventions_ (modifications to the debate procedure). First, _diversity-pruning_ which aims to maximize the information entropy in model responses at each round of debate; this intervention has the added benefit of reducing the severity of the echo chambereffect. Second, _quality-pruning_ which aims to maximize the relevance of each model's response. We demonstrate that this intervention improves the likelihood that the debate procedure converges to correct answers. Lastly, _misconception-refitation_ which directly identifies, and attempts to refute misconceptions in model responses. This intervention takes inspiration from works such as Robinson et al. (2022) which demonstrate that LLMs are often more skilled at _evaluating_ answers, compared with directly providing answers. For each of our interventions, we provide theoretical results outlining the way in which each improves debate. We also conduct experiments on four common benchmarks demonstrating that these interventions improve debate efficacy in practice.

**Our contributions** 1) We propose a theoretical framework for multi-LLM debate that draws on connections from in-context learning and Bayesian inference. 2) We provide theoretical insights on several key principles of multi-LLM debate. 3) Using these insights, we design three debate interventions which result in consistent improvement to debate, across four language benchmarks (BoolQ, MMLU, MathQ, TruthfuQA) and three families of models (GPT, Lama, and Mistral).

## 2 Related Work

Our work is closely related to multi-agent debate, which focuses on iterative collaboration between agents to make a decision Chan et al. (2023); Liang et al. (2023); Du et al. (2023); Khan et al. (2024); Irving et al. (2018); Michael et al. (2023); Rasal (2024); Pham et al. (2023); Chang (2024). These works often focus on multi-agent debate in the context of question-answering tasks and aim to provide higher quality answers (compared to those of a single model) by engaging multiple models in discussion. The preliminary debate framework, proposed by Du et al. (2023), facilitates debate by first asking each model a question, and then iteratively re-asking agents that same question contextualized by the responses of all models in the previous round. Different variants of this procedure have been proposed: debate where models have different functionality Liang et al. (2023), round-robin style debate Chan et al. (2023), dynamically controlling the level of disagreement between agents in debate Chang (2024), or using judges to assess the correctness of debaters Khan et al. (2024). Other techniques for iteratively improving the quality of answers have also been proposed, e.g., chain-of-thought Wei et al. (2022); Kojima et al. (2022), self-consistency Wang et al. (2022); Singhal et al. (2023), and self-reflection Ren et al. (2023).

Similar to debate, there have been investigations into the use of different LLMs to engage with one-another Liu et al. (2023); Abdelnabi et al. (2023); Zhang et al. (2023); Li et al. (2023); Park et al. (2023), explain their rational to others Wang et al. (2023), or collaboratively engage in general tasks Li et al. (2023); Wang et al. (2023); Park et al. (2023); Wu et al. (2023); Hong et al. (2023); Li et al. (2023, 2023); Tsao and AILAB (2023). While debate has shown promise in a wide range of domains, several works have also demonstrated that the debate process can be unstable and can lead to worse performance than using just a single model Wang et al. (2024); Smit et al. (2023).

Our work is also related to in-context learning and Bayesian inference. The former, Brown et al. (2020); Min et al. (2022); Lampinen et al. (2022) demonstrates that LLMs can perform unseen tasks when provided only a few examples of that task. Other works Xie et al. (2021); Jiang (2023) have shown a connection between in-context learning and Bayesian inference; the additional examples provided to the model can be viewed as updates to the model's posterior distribution over tokens.

## 3 Preliminaries

DebateLet \(\mathbf{x}\) be a given question, with associated answer \(\mathbf{y}\), for example \(\mathbf{x}=\)_"What color is the sky?"_ and \(\mathbf{y}=\)_"Blue"_. Following the debate procedure proposed by Du et al. (2023), a collection of \(n\) LLMs (also referred to as agents) collaborate to infer the correct answer \(\mathbf{y}\) by iteratively engaging in discussion over \(T\) rounds, as described next:

* At round \(t=0\) each agent \(i\) observes task \(\mathbf{x}\), then provides response \(\mathbf{z}_{i}^{(0)}\).
* At rounds \(t>0\) each agent \(i\) observes task \(\mathbf{x}\) and the outputs of the \(n\) agents at the previous timestep \(Z^{(t-1)}=(\mathbf{z}_{1}^{(t-1)},\dots,\mathbf{z}_{n}^{(t-1)})\), then provides response \(\mathbf{z}_{i}^{(t)}\).
* The debate process ends if \(t=T\) or if agents reach a consensus.

To measure if consensus is reached a function \(a\) extracts an _answer_1 from a given response \(\mathbf{z}\). Suppose \(\mathbf{z}=\)_"During the day, the sky is blue"_, then \(a(\mathbf{z})=\)_"Blue"_. At round \(t\), the probability that agent \(i\) provides response \(\mathbf{z}_{i}^{(t+1)}\), is given by

Footnote 1: In practice, \(a\) can be a regular-expression checker or an LLM-based judge such as Liang et al. (2023)

\[\mathbb{P}_{\text{model}}\bigg{(}\underbrace{\mathbf{z}_{i}^{(t+1)}}_{\text{ updated model response}}\big{|}\underbrace{\mathbf{x}}_{\text{task}},\underbrace{Z^{(t)}=( \mathbf{z}_{1}^{(t)},\ldots,\mathbf{z}_{n}^{(t)})}_{\text{all model responses from previous round}},\underbrace{\phi_{i}}_{\text{model parameters}}\bigg{)}\] (1)

where \(\phi_{i}\) captures model hyperparameters (such as training data, architecture, etc.). Both the input \((Z^{(t)},\mathbf{x})\) as well as the hyperparameters \(\bm{\phi}_{i}\), ultimately influence the output \(\mathbf{z}_{i}^{(t+1)}\). Note that on each round, all agents observe the _same_ input, namely \((Z^{(t)},\mathbf{x})\). Thus, differences in agent output \(\mathbf{z}_{i}^{(t+1)}\) are determined by both the stochastic nature of output generations, and the unique parameters \(\bm{\phi}_{i}\) of each model. For notational convenience, we drop the subscript in \(\mathbb{P}_{\text{model}}\) when the parameters \(\bm{\phi}_{i}\) are given, and simply write \(\mathbb{P}(\cdot|\bm{\phi}_{i})\).

The key distinction between our approach and "vanilla" debate, is that we will leverage latent concepts (discussed next) to modify the responses in \(Z^{(t)}\) in between each round of debate.

Latent ConceptsCentral to our investigation is the idea of _latent concepts_ in language generation. As outlined by Xie et al. (2021); Jiang (2023) latent concepts capture the idea that language is not generated at random (either by humans or by models). Rather, when generating language, we first have an idea or an intention form in our minds; we then select words that we believe will convey that underlying idea or intention. More formally, let \(\Theta\) be a _latent concept space_ and let \(\bm{\theta}\in\Theta\) be a _concept_. Following Xie et al. (2021), tasks \(\mathbf{x}\), and their associated answer \(\mathbf{y}\) are generated by first selecting a vector of latent concept \(\bm{\theta}\in\Theta\) and then sampling \((\mathbf{x},\mathbf{y})\sim D(\bm{\theta})\), where \(D\) is some distribution mapping concepts to task-answer pairs. Similarly, when providing responses, models will observe \(\mathbf{x}\), and infer the latent concept \(\bm{\theta}\), or more generally a distribution over the latent concept space, and then generate a response according to those inferred concepts, i.e., the model's generation probability in Equation 1 can be expressed as

\[\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}\big{|}\;\mathbf{x},Z^{(t)},\bm{\phi}_ {i}\big{)}=\prod_{\bm{\theta}\in\Theta}\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1) }\big{|}\;\bm{\theta},\mathbf{x},Z^{(t)},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(} \bm{\theta}|\;\mathbf{x},Z^{(t)},\bm{\phi}_{i}\big{)}\] (2)

Note that the above holds by the law of total probability for any choice of latent concept space.

To provide a more concrete example of latent concepts, consider the question-answering task in the BoolQ dataset. One of the questions in this dataset is "Did Abraham Lincoln write the letter in the film Saving Private Ryan?" to which the correct answer is "Yes". The latent concept, in this case, corresponds to the actual scene in the movie where the Bixby letter (written by Lincoln) is read to a group of soldiers. Just as in our case, first a concept \(\theta\) is drawn, e.g., the film is made; then from the film, a string \(\mathbf{x}\) is sampled, i.e., the previous question about the film.

For another example of latent concepts, we can think of arithmetic calculations such as multiplication. When we wish to express multiplication through language, we may write something like "\(4*4\)". The latent concepts behind this string are the mechanisms of multiplication (e.g., multiplication is just iterative addition, and addition itself is simply increasing the value of a number by one iteratively). These examples are intended to be easily digestible. However, latent concepts can also be significantly more abstract, such as a vector in some unknown embedding space.

## 4 A Theoretical Formulation of Multi-Agent Debate

We begin by providing a theoretical formulation of multi-agent debate. This formulation will provide key insights into the inner workings of the debate procedure, which we will use to improve debate.

The key behind our framework is to use the idea of latent concepts and expansion of each model's generation probability (Equation 2) in order to better understand debate. Prior to presenting our theoretical framework, we first state an important assumption.

**Assumption 4.1**.: For a given latent concept space \(\Theta\), the probability of generating response \(\mathbf{z}_{i}^{(t+1)}\) is conditionally independent of both the responses \(Z^{(t)}\) and the task \(\mathbf{x}\), given concept \(\bm{\theta}\in\Theta\) and model parameters \(\bm{\phi}_{i}\), i.e., \(\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}\big{|}\;\bm{\theta},\mathbf{x},Z^{(t)},\bm{\phi}_{i}\big{)}=\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\;\bm{\theta}, \bm{\phi}_{i}\big{)}\).

Assumption 4.1 can be interpreted as saying that a model's generation \(\mathbf{z}_{i}\), is uniquely determined by the model's parameters \(\bm{\phi}_{i}\) and the concepts identified by a model, namely \(\bm{\theta}\). In the case of encoder-decoder-based models, one can conceptualize the joint between \(\bm{\phi}\) and \(\bm{\theta}\) as corresponding to the embedding produced by the encoder. With this embedding in hand, the original input (\(\mathbf{x}\), \(Z^{(t)}\)) no longer influences the model's output, rather the embedding and model parameters will uniquely determine the model's output.

Next, we derive the following lemma which will be useful in examining the way that model responses evolve debate rounds.

**Lemma 4.2**.: _The generation of model \(i\) at time \(t+1\) can be expressed as,_

\[\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\;Z^{(t)},\mathbf{x},\bm{ \phi}_{i}\big{)}\propto\sum_{\bm{\theta}\in\Theta}\underbrace{\mathbb{P}\big{(} \mathbf{z}_{i}^{(t+1)}|\bm{\theta},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\bm{x} |\bm{\theta},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\bm{\theta}|\bm{\phi}_{i} \big{)}}_{\text{generation without other agents}}\;\underbrace{\prod_{j=1}^{n} \mathbb{P}\big{(}\mathbf{z}_{j}^{(t)}|\;\bm{\theta},\bm{\phi}_{i}\big{)}}_{ \text{skew caused by other agents}}\]

The significance of this lemma is that we are able to express the probability of generating a given response \(\mathbf{z}_{i}^{(t+1)}\)_with_ the other model responses \(Z^{(t)}\) in terms of the probability of generating \(\mathbf{z}_{i}^{(t+1)}\)_without_ the other model responses and a skew term caused by those model responses. Note that,

\[\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\mathbf{x},\bm{\phi}_{i} \big{)}\propto\sum_{\bm{\theta}\in\Theta}\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1 )}|\bm{\theta},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\bm{x}|\bm{\theta},\bm{ \phi}_{i}\big{)}\mathbb{P}\big{(}\bm{\theta}|\bm{\phi}_{i}\big{)}\]

Thus, we can think of \(\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\;Z^{(t)},\mathbf{x},\bm{\phi}_{i} \big{)}\) as a weighted version of \(\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\;\mathbf{x},\bm{\phi}_{i}\big{)}\), where the weights are given by the skew term \(\prod_{j=1}^{n}\mathbb{P}\big{(}\mathbf{z}_{j}^{(t)}|\;\bm{\theta},\bm{\phi}_ {i}\big{)}\).

Debate and In-Context LearningTo help conceptualize the role of latent concepts in debate, we discuss the work of Xie et al. (2021), which uses Bayesian inference over latent concepts to understand in-context learning. There are natural connections between in-context learning and multi-agent debate. In-context learning works as follows: given a task \(\mathbf{x}\) and a model \(f\), select several examples of task-answers pairs \((\mathbf{x}_{1},y_{1})\ldots(\mathbf{x}_{m},y_{m})\) which are _similar_ to \(\mathbf{x}\). Then prompt the model \(f\) for an answer to task \(\mathbf{x}\), given examples \((\mathbf{x}_{j},y_{j})\), i.e. \(\mathbf{z}=f\big{(}\mathbf{x}\big{|}\;(\mathbf{x}_{1},y_{1})\ldots(\mathbf{x} _{m},y_{m})\big{)}\). A key result of Xie et al. (2021) is that latent concepts in the examples \((\mathbf{x}_{j},y_{j})\), particularly concepts shared between many examples, influence the model's answer \(\mathbf{z}\). For any concept where \(\mathbb{P}\big{(}\bm{\theta}|\;(\mathbf{x}_{1},y_{1})\ldots(\mathbf{x}_{m},y _{m})\big{)}\) is large relative to other concepts (i.e., there is a shared concept \(\bm{\theta}\) between the examples), the model is more likely to give response \(\mathbf{z}\) which also share that concept. Model responses at the previous round \(Z^{(t)}\) serve a similar function to the examples of in-context learning. The model's updated response at time \(t+1\), namely \(\mathbf{z}_{i}^{(t)}\), is influenced by concepts shared between the responses in \(Z^{(t)}\). The skew term in Lemma 4.2 provides a glimpse of how latent concepts conveyed by \(Z^{(t)}\) will influence the generation of \(\mathbf{z}_{i}^{(t)}\), namely that \(\prod_{j=1}^{n}\mathbb{P}\big{(}\mathbf{z}_{j}^{(t)}|\;\bm{\theta},\bm{\phi}_ {i}\big{)}\) reweighs the model's generation.

### Debate Objective

Through this perspective of debate we can more effectively design debate procedures by leveraging the concept space \(\Theta\). To do this, we will first formulate debate as an optimization problem where the _skew term_, described in Lemma 4.2, corresponds to the optimization variables. For a given task \(\mathbf{x}\) and answer \(y\), each round of debate can be formulated as the following optimization problem.

\[\arg\max_{Z^{(t)}}\sum_{i=0}^{n}\mathbb{P}\big{(}a(\mathbf{z}_{i}^{(t+1)})=y| \;Z^{(t)},\bm{\phi}_{i}\big{)}\]

At time \(t\) we aim to craft responses \(Z^{(t)}\) such that they maximize the probability of providing the correct answer at the next time step. Expanding this objective over the latent concept space \(\Theta\), yields

\[\arg\max_{Z^{(t)}}\;\sum_{i=1}^{n}\sum_{\bm{\theta}\in\Theta} \bigg{(}\mathbb{P}\big{(}a(\mathbf{z}^{(t+1)})=y|\;\bm{\theta},\bm{\phi}_{i} \big{)}\mathbb{P}\big{(}\bm{\theta}|\;\bm{\phi}_{i}\big{)}\mathbb{P}\big{(} \mathbf{x}|\;\bm{\theta},\bm{\phi}_{i}\big{)}\prod_{j=1}^{n}\mathbb{P}\big{(} \mathbf{z}_{j}^{(t)}|\;\bm{\theta},\bm{\phi}_{i}\big{)}\bigg{)}\] (3)The key challenges with directly optimizing this objective are: 1) the true concept \(\bm{\theta}^{*}\) from which \(\mathbf{x}\) and \(y\) originate, as well as the relationship between \(\mathbf{z}_{j}^{(t)}\) and the underlying concepts, is unknown, 2) the responses in \(Z^{(t)}\)) are natural language. However, will allow us to design several approaches within the concept space to better optimize debate. To motivate these approaches, we first need to make several observations about the debate procedure as a whole.

## 5 Debate Principals

In this section, we discuss factors that affect the efficacy of LLMs debate. In particular, we look at the role of information diversity, both in terms of the diversity of responses in \(Z^{(t)}\) as well as diversity in model capabilities. We see that a lack of diversity in either aspect is detrimental to the debate process. Lastly, we study a particular type of homogeneity in debate, namely shared misconceptions in which a large portion of models all share a similar erroneous belief about the task \(x\).

### Information Diversity

We begin by examining how the debate procedure is affected by both the diversity of model abilities and the diversity of model responses. Homogeneity in either ability or responses will bias the debate procedure towards certain latent concepts.

Similar Model CapabilitiesSuppose the debate process is conducted with only one type of model (in effect \(n\) copies of the same model). That is, \(\bm{\phi}_{i}\equiv\bm{\phi}\) for all \(i\in[n]\). Then, as the number of agents increases, the debate procedure is more greatly impacted by the echo chamber effect, i.e., the probability that a round of debate results in a change to the most likely concept, perceived by agents, approaches \(0\). That is, a greater number of similar agents results in static debate dynamics, in essence defeating the purpose of debate.

**Theorem 5.1**.: _Suppose all \(n\) agents have identical configurations (\(\bm{\phi}_{i}\equiv\bm{\phi}\) for all \(i\)). For round \(t>0\) let \(\bm{\theta}^{\prime(t)}=\big{(}\arg\max_{\bm{\theta}\in\Theta}\mathbb{P}\big{(} \bm{\theta}\,|\,\mathbf{x},Z^{(t)},\bm{\phi}\big{)}\big{)}\) and \(\bm{\theta}^{\prime(t+1)}=\big{(}\arg\max_{\bm{\theta}\in\Theta}\mathbb{P} \big{(}\bm{\theta}\,|\,\mathbf{x},Z^{(t+1)},\bm{\phi}\big{)}\big{)}\), i.e., \(\bm{\theta}^{\prime(t)}\) and \(\bm{\theta}^{\prime(t+1)}\) are the concepts most likely to be inferred by a model with parameters \(\bm{\phi}\) when given task \(\mathbf{x}\) and responses \(Z^{(t)}\), \(Z^{(t+1)}\) respectively. Then \(\mathbb{P}\big{(}\bm{\theta}^{(t)}=\bm{\theta}^{\prime(t+1))}\big{)}\to 1\) as \(n\rightarrow\infty\)._

We defer a full proof to the Supplement, Section A. Theorem 5.1 implies that when debate is conducted with multiple copies of the same model (or very similar models), increasing the number of models results in debate centering on a single (unchanging) concept, rather than a balanced distribution over multiple concepts.

Similar Model OpinionsNext, we examine how similar responses impact the collaboration process. At time \(t\) suppose that there are \(n\) responses \(Z^{(t)}\) and at least \(m\) of those responses are _similar_, i.e., there exists some concept \(\bm{\theta}^{\prime}\) such that \(\bm{\theta}^{\prime}=\arg\max_{\bm{\theta}\in\Theta}\mathbb{P}\big{(}\bm{ \theta}\,|\,\mathbf{z}_{j}^{(t)},\bm{\phi}_{i}\big{)}\) for all \(j\leq m\). That is, each of the \(m\) responses has a shared "most likely" concept when viewed by a model with parameters \(\bm{\phi}_{i}\).

**Theorem 5.2**.: _Suppose that \(Z^{(t)}\) contains at least \(m\) responses with the property that \(\bm{\theta}^{\prime}=\arg\max_{\bm{\theta}\in\Theta}\mathbb{P}\big{(}\mathbf{ z}_{i}^{(t)}\,|\,\bm{\theta},\bm{\phi}_{i}\big{)}\). Then, as \(m\rightarrow\infty\) the model's generation at the next round (\(t+1\)) becomes uniquely determined by a single concept \(\bm{\theta}^{\prime}\) i.e. \(\frac{\mathbb{P}\big{(}\mathbf{z}_{i,1}^{(t+1)}\,|\,Z^{(t)},\mathbf{x},\bm{ \phi}_{i}\big{)}}{\mathbb{P}\big{(}\mathbf{z}_{i,2}^{(t+1)}\,|\,Z^{(t)}, \mathbf{x},\bm{\phi}_{i}\big{)}}\rightarrow\frac{\mathbb{P}\big{(}\mathbf{z}_ {i,1}^{(t+1)}\,|\,\bm{\theta}^{\prime},\bm{\phi}_{i}\big{)}}{\mathbb{P}\big{(} \mathbf{z}_{i,2}^{(t+1)}\,|\,\bm{\theta}^{\prime},\bm{\phi}_{i}\big{)}}\) for all response pairs \(\mathbf{z}_{(i,1)}^{(t+1)},\mathbf{z}_{(i,2)}^{(t+1)}\)._

We defer a full proof to the Supplement Section A. Theorem 5.2 indicates the susceptibility that LLM debate has towards tyranny of the majority. If a large number of models provide similar responses to a task \(\mathbf{x}\), then those repeated answers will drown out the single provided by the other models' responses, as well as the task \(\mathbf{x}\) itself. In Section 7 we demonstrate that this occurs in practice.

### Shared Misconceptions

Next, we study a particular type of homogeneity in model capabilities and responses, namely shared misconceptions. When a common misconception is shared among the models, debate is less effective and is likely to converge to erroneous concepts associated with the shared misconception. We now formalize the notion of misconceptions.

**Definition 5.3**.: **(Misconception)**: For a given concept \(\bm{\theta}^{*}\), a model with parameters \(i\) is said to have a _misconception_ regarding \(\bm{\theta}^{*}\) if there exists another concept \(\bm{\theta}^{\prime}\) s.t.,

That is, for tasks generated from the concept \(\bm{\theta}^{*}\), the model believes that the erroneous concept \(\bm{\theta}^{\prime}\) explains more than half of the tasks better than the true concept \(\bm{\theta}^{*}\).

There is said to be a _shared misconception_ if \(m\) of agents have a misconception and share the same erroneous concept \(\bm{\theta}^{\prime}\). When the models share a common misconception the responses produced by those models are biased towards the erroneous concept \(\bm{\theta}^{\prime}\).

**Theorem 5.4**.: _Let the true concept be \(\bm{\theta}^{*}\) and suppose that \(m\) of the \(n\) agents have a shared misconception for erroneous concept \(\bm{\theta}^{\prime}\). Then, task and answer \((\mathbf{x},y)\sim D(\bm{\theta}^{*})\) expected average correctness of the debate procedure at the final round \(T\) is monotonically decreasing with \(m\), i.e., \(\frac{1}{n}\sum_{i}^{n}\mathbb{P}\big{(}a(\mathbf{z}_{i}^{(T)})=y\big{)}\) is decreasing with \(m\)._

We defer the full proof to the supplement Section A. It should be noted that the phenomenon of converging to erroneous concepts is unlikely to be mitigated by adding more models. When the misconceptions of one model are formed through training data, it is likely that other models will possess the same misconception unless specifically trained to avoid such errors due to the high correlation in training data between models.

## 6 Interventions

In this section, we discuss several modifications to the debate procedure, referred to as _interventions_. We break our interventions into two categories: **Pruning** which focuses on choosing which responses to keep in \(Z^{(t)}\), and **Modifying** which focuses on changing or editing the responses \(Z^{(t)}\).

### Pruning Interventions

At round \(t\) of debate, running interventions work by selecting only a subset of responses \(Z^{\prime(t)}\) from \(Z^{(t)}\) before starting the next round \(t+1\). When using a pruning intervention, the models at round \(t+1\) will see only the pruned response set \(Z^{\prime(t)}\), rather than the full response set \(Z^{(t)}\).

Diversity PruningLet KL represent Kullback-Leibler divergence. The diversity pruning intervention selects \(k\) of the \(n\) responses in \(Z^{(t)}\) which maximizes information entropy, i.e.,

\[Z^{\prime(t)}=\underset{Y\subset Y^{(t)}\left[\mathbf{z}_{i},\mathbf{z}_{j} \in Z\right.}{\text{\text{\text{KL}}}}\big{(}D(\bm{\theta}\left|\,\mathbf{z}_ {i}),\ D(\bm{\theta}\left|\,\mathbf{z}_{j}\right.)\right)\] \[\text{s.t.}\ |Z|=k\]

Quality PruningQuality pruning aims to select the \(k\) responses in \(Z^{(t)}\) with the highest similarity to the task \(x\). Similar to diversity pruning, quality pruning selects \(k\) of the \(n\) responses at time \(t\). Rather than selecting for diversity, quality pruning aims to select the \(k\) highest question responses. This is done by selecting the \(k\) responses which maximize

\[Z^{\prime(t)}=\underset{Z\subset Z^{(t)}}{\text{\text{\text{ argmin}}}}\sum_{\mathbf{z}_{i},\in Z}\text{\text{\text{\text{KL}}}}\big{(}D(\bm{ \theta}\left|\,\mathbf{x}\right.),\ D(\bm{\theta}\left|\,\mathbf{z}_{i})\big{)}\] \[\text{s.t.}\ |Z|=k\]

In practice computing \(\text{\text{\text{KL}}}\big{(}D(\bm{\theta}\left|\mathbf{x}\right.),D(\bm{ \theta}\left|\mathbf{z}_{i}\right.)\big{)}\) or \(\text{\text{\text{KL}}}\big{(}D(\bm{\theta}\left|\mathbf{z}_{i}\right.),D(\bm {\theta}\left|\mathbf{z}_{j}\right.)\big{)}\) is intractable. However, sentence embedding can be used as a proxy for these values. Section C discusses this in further detail.

Next, we show that when models have a shared misconception, diversity pruning decreases the likelihood that the debate procedure will converge to the erroneous concept corresponding to the shared misconception.

**Theorem 6.1**.: _Let the true concept be \(\bm{\theta}^{*}\) and suppose that at least \(n/2\) agents have a shared misconception for erroneous concept \(\bm{\theta}^{\prime}\). Then diversity pruning decreases the probability that debate converges to an answer \(y^{\prime}\) which is sourced from the erroneous concept \(\bm{\theta}^{\prime}\), i.e. \(y^{\prime}\sim D(\bm{\theta}^{\prime})\)._We defer the full proof to the Supplement, Section A.

**Theorem 6.2**.: _For a given task-answer pair \((\mathbf{x},y)\) quality pruning increases the probability that debate converges to the correct answer, i.e. let \(Z^{(t)}\) be the set of all responses at time \(t\) and \(Z^{\prime(t)}\) be the result of quality pruning, then \(\sum_{i=1}^{n}\mathbb{P}(a(\mathbf{z}_{i}^{(t+1)})=y|\mathbf{x},Z^{\prime(t)}, \boldsymbol{\phi}_{i})>\sum_{i=1}^{n}\mathbb{P}(a(\mathbf{z}_{i}^{(t+1)}=y| \mathbf{x},Z^{(t)},\boldsymbol{\phi}_{i})\)._

We defer the full proof to the Supplement, Section A.

**Remark 6.3**.: As shown by Theorems 6.1 and 6.2, diversity pruning decreases the probability that debate converges to incorrect answers sourced from a particular concept, while quality pruning increases the probability that debate converges to a correct answer sourced from the true concept. Both interventions can be used simultaneously to guide the debate procedure more effectively away from wrong answers and towards correct answers.

### Modification Interventions

Misconception RefutationIn addition to selecting which responses in \(Z^{(t)}\) will be used in the next round of debate, we can also modify the responses in \(Z^{(t)}\). Misconception refutation aims to do precisely this by updating response \(\mathbf{z}_{j}^{(t)}\) to be more relevant to the task \(\mathbf{x}\).

\[\mathbf{z}_{j}^{*}=\arg\min_{\mathbf{z}}\text{KL}\big{(}D(\boldsymbol{\theta} |\mathbf{x}),\ D(\boldsymbol{\theta}|\ \mathbf{z})\big{)}-\text{KL}\big{(}D(\boldsymbol{\theta}| \mathbf{z}_{j}^{(t)}),\ D(\boldsymbol{\theta}|\ \mathbf{z})\big{)}\]

Similar to Diversity Pruning and Quality Pruning, the distributions in the above objective are intractable in practice. As such, we use a proxy to update each response \(\mathbf{z}_{j}^{(t)}\), specifically produce \(\mathbf{z}_{j}^{*}\) by having an LLM minimally modify the given response \(\mathbf{z}_{j}^{(t)}\). The model is first prompted for a list of misconceptions and errors identified in the response. Given the list of misconceptions, the model is asked for both a refutation of the misconception and a corrected version of the response. For more details, see Section C of the Supplement.

**Theorem 6.4**.: _For task-answer pair \((\mathbf{x},y)\), misconception refutation increases the probability of debate converging to the correct answer, i.e. let \(Z^{(t)},Z^{*(t)}\) be the responses before and after misconception refutation, then \(\sum_{i=1}^{n}\mathbb{P}(a(\mathbf{z}_{i}^{(t+1)})=y|\mathbf{x},Z^{*(t)}, \boldsymbol{\phi}_{i})>\sum_{i=1}^{n}\mathbb{P}(a(\mathbf{z}_{i}^{(t+1)}=y| \mathbf{x},Z^{(t)},\boldsymbol{\phi}_{i})\)._

## 7 Experiments

Experimental DesignWe conduct experiments on four common language model benchmarks. **BoolQ**Clark et al. (2019), which consists of \(3,270\) yes-no questions, **MMLU**Hendrycks et al. (2020) which consists of \(13,869\) multiple-choice questions (we use the \(3,406\) high-school-level questions), **TruthfulQA**Lin et al. (2021) which consists of \(817\) open-ended questions, and **MathQ** which consists of \(3,000\) arithmetic questions of the from \(a\cdot b\cdot c+d\cdot e\cdot f\). In the BoolQ, MMLU, MathQ, datasets model correctness is measured through regular expression matching. In the TruthfulQA dataset, model correctness is measured via an LLM judge (we use GPT-4 as the judge in all experiments)

We use four LLMs of increasing capability, **GPT-3.5** (GPT-3.5 Turbo) OpenAI (2022), **Llama-2** (Llama-2 7B Chat) Touvron et al. (2023), **Llama-3** (Llama-3 8B Instruct) Meta AI (2024), and **Mistral** (Mistral 7B Instruct v0.2) Jiang et al. (2023). For sentence embeddings (which serve as a proxy of the latent concepts \(\Theta\)), we use sentence embeddings from **ADA-2**OpenAI (2022). We compare a combination of our three interventions **Ours** (see Algorithm 1 full details) with the debate paradigm of Du et al. (2023) (Society of Minds) **SoM**.

We begin by making several empirical observations about the multi-agent debate process.

Tyranny of the MajorityFirst, we examine the susceptibility of models towards agreement with the majority opinion. That is, how likely are models to give a specific answer at round \(t+1\) when \(m\) of the models provided that specific answer at round the previous round (round \(t\))? For example, in BoolQ suppose the specific answer is "Yes", then we want to know: how likely is a model to give a "Yes"-answer at round \(t+1\) if that model observes \(m\) "Yes" answers at round \(t\).

To measure this, we first select a random target answer, e.g., "Yes", and then prompt \(m\) of the models (out of 11) to provide responses "Yes"-answers2 (while the other 11-\(m\) models are prompted to provide a different randomly selected answer). These 11 responses make up \(Z^{(t)}\), we then test each model's likelihood of providing the target at round \(t+1\) when observing \(Z^{(t)}\) before diversity pruning (solid) and after diversity running (hatched).

Footnote 2: Any model _not_ providing the target answer is re-prompted until the target answer is provided.

In Figure 1, we see models are susceptible to echo chamber effects (this phenomenon is predicted by Theorem 5.1). The likelihood of providing the majority answer increases when \(Z^{(t)}\) contains more instances of the majority answer (i.e., as \(m\) increases). Figure 1 also demonstrates that diversity pruning (with \(k=5\)) reduces this echo chamber effect. See the Supplement for details.

Diversity of OpinionsNext, we examine the effectiveness of SoM and our method as a function of opinion diversity. Figure 2 shows the average accuracy improvement of SoM (dashed) and our method (solid) over single model performance (i.e., average performance at round \(t=0\)), as a function of the similarity between all responses at round \(t=0\) of debate (measured via pairwise cosine similarity). We see that for BoolQ, MMLU, and TruthfulQA, SoM is less effective when the similarity between responses increases. This observation is predicted by Theorems 5.1 and 5.2, which show that debate, without interventions, is less effective when model responses are too similar.

We see that our method's improvement compared to SoM is greatest when model opinions are more similar (cosine similarity close to 1). Note that the MathQ benchmark, where responses consist primarily of arithmetic, serves as a counter-example to these observations. This is due to the fact that sentence embedding of any two arithmetic expansions will be similar, regardless of their _true_ similarity; as such, the cosine similarity between embedding is less meaningful on this benchmark.

Debate InterventionsNow, we examine the effectiveness of a combination of our three debate interventions (see Algorithm 1 for full details of how the interventions are combined). We begin with a per-round performance of our method and SoM, as shown in Figure 3. We see that typically, the advantage of our method over debate arises in the later rounds of debate. Next, in Table 1, we present a full set of results for single models, SoM, and a combination of our three interventions. In all cases, our method is either competitive with, or superior to, SoM.

Figure 1: Probability that each model echoes the majority answer at round \(t=11\), as the number of responses at time \(t=0\) gives that majority answer (debate between 12 models are used).

Figure 2: Average accuracy improvement as a function of response diversity at round \(0\) of debate.

In addition to providing results for the combination of our interventions, we also investigate the effectiveness of each intervention applied individually (see Table 3 of the Supplement). These results indicate that our method is most successful when applying all three interventions simultaneously. In fact, some interventions can be detrimental to the debate process when applied in isolation. This is expected as each intervention is inherently designed to be complementary.

## 8 Limitations

While we aim to address some of the fundamental issues of multi-LLM debate, such as tyranny of the majority, there are several factors that need to be considered when adopting our framework. Firstly, our theoretical results leverage a latent concept space, which may not be accessible in practice, necessitating the use of proxies such as sentence embeddings. Reliance on proxies is particularly consequential for quality and diversity pruning; these interventions are less effective in domains where sentence embeddings are less meaningful, e.g., arithmetic questions. Additionally, our interventions can increase the inference time of the debate procedure. Increased inference time stems primarily from misconception refutation, as this intervention requires re-prompting each debater multiple times.

## 9 Conclusion

Multi-agent debate is an effective tool for improving the efficacy of LLM responses. However, debate is naturally susceptible to issues such as tyranny of the majority and shared misconceptions between models. By making use of our theoretical framework for debate, we are able to establish interventions for the debate procedure which help to alleviate these issues and improve the general performance of multi-agent debate. We saw that diversity pruning reduces the influence of similar responses. This is especially helpful in settings where the majority of agents provide incorrect responses that share a common error. A combination of all three interventions consistently leads to better debate.

\begin{table}
\begin{tabular}{||c||c|c|c||c|c||} \hline  & **Single** & **SoM** & **Ours** & **Single** & **SoM** & **Ours** \\ \hline \hline  & \multicolumn{3}{c||}{**BoolQ**} & \multicolumn{3}{c||}{**MMLU**} \\ \hline \(6\times\) GPT-3.5 & \(.80_{\pm 0.14}\) & \(.84_{\pm 0.12}\) & \(.85_{\pm 0.12}\) & \(.73_{\pm 0.14}\) & \(.74_{\pm 0.16}\) & \(.79_{\pm 0.014}\) \\ \hline \(6\times\) Llama-3 & \(.76_{\pm 0.14}\) & \(.78_{\pm 0.13}\) & \(.78_{\pm 0.13}\) & \(.67_{\pm 0.16}\) & \(.70_{\pm 0.015}\) & \(.75_{\pm 0.014}\) \\ \hline \(6\times\) Llama-2 & \(.67_{\pm 0.17}\) & \(.68_{\pm 0.17}\) & \(.73_{\pm 0.16}\) & \(.41_{\pm 0.17}\) & \(.47_{\pm 0.18}\) & \(.52_{\pm 0.18}\) \\ \hline \(6\times\) Mistral & \(.80_{\pm 0.14}\) & \(.82_{\pm 0.13}\) & \(.85_{\pm 0.12}\) & \(.66_{\pm 0.16}\) & \(.65_{\pm 0.16}\) & \(.66_{\pm 0.16}\) \\ \hline \(3\times\) GPT-3.5 \(+3\times\) Llama-3 & - & \(.82_{\pm 0.14}\) & \(.84_{\pm 0.13}\) & - & \(.73_{\pm 0.16}\) & \(.78_{\pm 0.16}\) \\ \hline \(3\times\) GPT-3.5 \(+3\times\) Mistral & - & \(.83_{\pm 0.13}\) & \(\mathbf{.87}_{\pm 0.12}\) & - & \(.69_{\pm 0.17}\) & \(.72_{\pm 0.16}\) \\ \hline \(3\times\) Llama-3 \(+3\times\) Mistral & - & \(.80_{\pm 0.14}\) & \(.80_{\pm 0.14}\) & - & \(.69_{\pm 0.17}\) & \(.74_{\pm 0.16}\) \\ \hline \hline  & \multicolumn{3}{c||}{**TruthfulQA**} & \multicolumn{3}{c||}{**Math**} \\ \hline \(6\times\) GPT-3.5 & \(.61_{\pm 0.03}\) & \(.63_{\pm 0.02}\) & \(\mathbf{.69}_{\pm 0.30}\) & \(.53_{\pm 0.35}\) & \(.88_{\pm 0.16}\) & \(.93_{\pm 0.01}\) \\ \hline \(6\times\) Llama-2 & \(.47_{\pm 0.34}\) & \(.52_{\pm 0.35}\) & \(.55_{\pm 0.34}\) & \(.11_{\pm 0.013}\) & \(.13_{\pm 0.014}\) & \(.19_{\pm 0.015}\) \\ \hline \(6\times\) Llama-3 & \(.53_{\pm 0.35}\) & \(.55_{\pm 0.032}\) & \(.55_{\pm 0.032}\) & \(.25_{\pm 0.16}\) & \(.33_{\pm 0.17}\) & \(.48_{\pm 0.018}\) \\ \hline \(6\times\) Mistral & \(.48_{\pm 0.34}\) & \(.51_{\pm 0.035}\) & \(.53_{\pm 0.034}\) & \(.13_{\pm 0.13}\) & \(.19_{\pm 0.14}\) & \(.18_{\pm 0.014}\) \\ \hline \(3\times\) GPT-3.5 \(+3\times\) Llama-3 & - & \(.56_{\pm 0.035}\) & \(.62_{\pm 0.031}\) & - & \(.76_{\pm 0.015}\) & \(.82_{\pm 0.014}\) \\ \hline \(3\times\) GPT-3.5 \(+3\times\) Mistral & - & \(.52_{\pm 0.035}\) & \(.56_{\pm 0.035}\) & - & \(.56_{\pm 0.018}\) & \(.68_{\pm 0.17}\) \\ \hline \(3\times\) Llama-3 \(+3\times\) Mistral & - & \(.49_{\pm 0.036}\) & \(.53_{\pm 0.035}\) & - & \(.22_{\pm 0.015}\) & \(.23_{\pm 0.015}\) \\ \hline \end{tabular}
\end{table}
Table 1: Accuracy of a solo model, debate, and our debate interventions: 10 rounds, 6 models.

Figure 3: Accuracy per round, our method and SoM when combing GPT-3.5 with Llama-3 or Mistral.

## References

* Abdelnabi et al. (2023) Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schonherr, and Mario Fritz. Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games. _arXiv preprint arXiv:2309.17234_, 2023.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Chan et al. (2023) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. _arXiv preprint arXiv:2308.07201_, 2023.
* Chang (2024a) Edward Y Chang. Evince: Optimizing adversarial llm dialogues via conditional statistics and information theory. _arXiv preprint arXiv:2408.14575_, 2024a.
* Chang (2024b) Edward Y. Chang. Llm collaborative intelligence: The path to artificial general intelligence. _SocraSynth.com_, 2024b.
* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. _arXiv preprint arXiv:1905.10044_, 2019.
* Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. _arXiv preprint arXiv:2305.14325_, 2023.
* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.
* Hong et al. (2023) Siriu Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. _arXiv preprint arXiv:2308.00352_, 2023.
* Irving et al. (2018) Geoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. _arXiv preprint arXiv:1805.00899_, 2018.
* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* Jiang (2023) Hui Jiang. A latent space theory for emergent abilities in large language models. _arXiv preprint arXiv:2304.09960_, 2023.
* Khan et al. (2024) Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Khitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R Bowman, Tim Rocktaschel, and Ethan Perez. Debating with more persuasive lms leads to more truthful answers. _arXiv preprint arXiv:2402.06782_, 2024.
* Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213, 2022.
* Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In _Proceedings of the 29th Symposium on Operating Systems Principles_, pages 611-626, 2023.
* Lampinen et al. (2022) Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. Can language models learn from explanations in context? _arXiv preprint arXiv:2204.02329_, 2022.
* Liu et al. (2020)Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for" mind" exploration of large scale language model society. _arXiv preprint arXiv:2303.17760_, 2023a.
* Li et al. (2023b) Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, and Katia Sycara. Theory of mind for multi-agent collaboration via large language models. _arXiv preprint arXiv:2310.10701_, 2023b.
* Li et al. (2023c) Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah. Tradinggt: Multi-agent system with layered memory and distinct characters for enhanced financial trading performance. _arXiv preprint arXiv:2309.03736_, 2023c.
* Li et al. (2023d) Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. _arXiv preprint arXiv:2310.06500_, 2023d.
* Liang et al. (2023) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. _arXiv preprint arXiv:2305.19118_, 2023.
* Lin et al. (2021) Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. _arXiv preprint arXiv:2109.07958_, 2021.
* Liu et al. (2023) Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. _arXiv preprint arXiv:2310.02170_, 2023.
* AI (2024) Meta AI. Meta lama 3. https://ai.meta.com/blog/meta-llama-3/, 2024.
* Michael et al. (2023) Julian Michael, Salsabila Mahdi, David Rein, Jackson Petty, Julien Dirani, Vishakh Padmakumar, and Samuel R Bowman. Debate helps supervise unreliable experts. _arXiv preprint arXiv:2311.08702_, 2023.
* Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? _arXiv preprint arXiv:2202.12837_, 2022.
* OpenAI (2022) OpenAI, 2022. URL https://openai.com/blog/chatgpt/.
* Park et al. (2023) Jeongeon Park, Bryan Min, Xiaojuan Ma, and Juho Kim. Choicemates: Supporting unfamiliar online decision-making with multi-agent conversational interactions. _arXiv preprint arXiv:2310.01331_, 2023a.
* Park et al. (2023b) Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, pages 1-22, 2023b.
* Pham et al. (2023) Chau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo Yuan, Bryan A Plummer, Zhaoran Wang, and Hongxia Yang. Let models speak ciphers: Multiagent debate through embeddings. _arXiv preprint arXiv:2310.06272_, 2023.
* Rasal (2024) Sumedh Rasal. Llm harmony: Multi-agent communication for problem solving. _arXiv preprint arXiv:2401.01312_, 2024.
* Ren et al. (2023) Jie Ren, Yao Zhao, Tu Vu, Peter J Liu, and Balaji Lakshminarayanan. Self-evaluation improves selective generation in large language models. _arXiv preprint arXiv:2312.09300_, 2023.
* Robinson et al. (2022) Joshua Robinson, Christopher Michael Rytting, and David Wingate. Leveraging large language models for multiple choice question answering. _arXiv preprint arXiv:2210.12353_, 2022.
* Singhal et al. (2023) Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. _arXiv preprint arXiv:2305.09617_, 2023.
* Shi et al. (2023)Andries Smit, Paul Duckworth, Nathan Grinsztajn, Kale-ab Tessera, Thomas D Barrett, and Arnu Pretorius. Are we going mad? benchmarking multi-agent debate between language models for medical q&a. _arXiv preprint arXiv:2311.17371_, 2023.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Tsao and AILAB (2023) Wen-Kwang Tsao and TrendMicro AILAB. Multi-agent reasoning with large language models for effective corporate planning. In _The 10th International Conf. on Computational Science and Computational Intelligence_, 2023.
* Wang et al. (2023a) Boshi Wang, Xiang Yue, and Huan Sun. Can chatgpt defend its belief in truth? evaluating llm reasoning via debate. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 11865-11881, 2023a.
* Wang et al. (2024) Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? _arXiv preprint arXiv:2402.18272_, 2024.
* Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdherey, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_, 2022.
* Wang et al. (2023b) Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-person self-collaboration. _arXiv preprint arXiv:2307.05300_, 2023b.
* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.
* Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. _arXiv preprint arXiv:2308.08155_, 2023.
* Xie et al. (2021) Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. _arXiv preprint arXiv:2111.02080_, 2021.
* Zhang et al. (2023) Jintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration mechanisms for llm agents: A social psychology view. _arXiv preprint arXiv:2310.02124_, 2023.

## Appendix A Theoretical Results

Proof of Lemma 4.2.: This result holds via marginalization of the posterior predictive distribution over the latent concepts \(\Theta\), namely

\[\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\ Z^{(t)},\mathbf{x}, \boldsymbol{\phi}_{i}\big{)} =\sum_{\boldsymbol{\theta}\in\Theta}\mathbb{P}\big{(}\mathbf{z}_{i }^{(t+1)}|\ \boldsymbol{\theta},Z^{(t)},\mathbf{x},\boldsymbol{\phi}_{i}\big{)} \mathbb{P}\big{(}\boldsymbol{\theta}|\ Z^{(t)},\mathbf{x},\boldsymbol{\phi}_{i }\big{)}\] \[=\sum_{\boldsymbol{\theta}\in\Theta}\mathbb{P}\big{(}\mathbf{z}_{ i}^{(t+1)}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\mathbb{P}\big{(}Z^{(t)}, \mathbf{x}|\boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\frac{\mathbb{P}( \boldsymbol{\theta}|\ \boldsymbol{\phi}_{i})}{\mathbb{P}(Z^{(t)},\mathbf{x})| \boldsymbol{\phi}_{i}\big{)}}\] \[\propto \sum_{\boldsymbol{\theta}\in\Theta}\mathbb{P}\big{(}\mathbf{z}_{ i}^{(t+1)}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\mathbb{P}\big{(}\mathbf{x}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\mathbb{P}(\boldsymbol{\theta }|\ \boldsymbol{\phi}_{i}\big{)}\prod_{j=1}^{n}\mathbb{P}\big{(}\mathbf{z}_{j}^{(t) }|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\]

Proof of Theorem 5.1.: Since each model has an identical configuration, i.e. \(\boldsymbol{\phi}_{i}=\boldsymbol{\phi}_{j}\) for all \(i,j\in[n]\), we simply refer to the configuration as \(\boldsymbol{\phi}\). For the given task \(\mathbf{x}\) let \(\boldsymbol{\theta}^{*(t)}\) be the realization of \(\boldsymbol{\theta}\) at time step \(t\). Let \(Z^{(t)}=\big{(}\mathbf{z}_{1}^{(t)},\ldots,\mathbf{z}_{n}^{(t)}\big{)}\), where each \(\mathbf{z}_{j}^{(t)}\sim q(\mathbf{z}|\ \mathbf{x},Z^{(t)},\boldsymbol{\phi})\). Then the conditional density for each concept \(\boldsymbol{\theta}\) can be written as,

\[\mathbb{P}\big{(}\boldsymbol{\theta}|\ Z^{(t)},\mathbf{x}, \boldsymbol{\phi}\big{)}\] \[\propto \mathbb{P}\big{(}Z^{(t)},\mathbf{x}|\ \boldsymbol{\theta}, \boldsymbol{\phi}\big{)}\mathbb{P}\big{(}\boldsymbol{\theta}|\ \boldsymbol{\phi}\big{)}\] \[= \mathbb{P}\big{(}\mathbf{x}|\ \boldsymbol{\theta}, \boldsymbol{\phi}\big{)}\mathbb{P}\big{(}\theta|\ \boldsymbol{\phi}\big{)}\]

where the term \(\mathbb{P}\big{(}\mathbf{x}|\ \boldsymbol{\theta},\boldsymbol{\phi}\big{)} \big{(}\theta|\ \boldsymbol{\phi}\big{)}\) is a constant with respect to the number of agents \(n\). Since \(\mathbb{P}\big{(}\boldsymbol{\theta}|\boldsymbol{\phi}\big{)}>0\) for all \(\boldsymbol{\theta}\in\theta\), each \(\mathbf{z}_{j}^{(t)}\) is an i.i.d. draw from \(q(\mathbf{z}|\ \mathbf{x},Z^{(t-1)},\boldsymbol{\phi})\), then

\[\boldsymbol{\theta}^{*(t)}=\lim_{n\to\infty}\bigg{(}\arg\max_{ \boldsymbol{\theta}\in\Theta}\mathbb{P}\big{(}\boldsymbol{\theta}|\ Z^{(t)}, \mathbf{x},\boldsymbol{\phi}\big{)}\bigg{)}\]

Thus, as \(n\to\infty\), all models predict the same concept, namely \(\boldsymbol{\theta}^{*(t)}\), at timestep \(t\) with probability \(1\). 

Proof of Theorem 5.2.: Consider any two responses from agent \(i\) at time \(t+1\), namely \(\mathbf{z}_{(i,1)}^{(t+1)},\mathbf{z}_{(i,2)}^{(t+1)}\). When there are \(n\) duplicate messages, the ratio between the conditional generation probabilities of both responses can be written as

\[\frac{\mathbb{P}\big{(}\mathbf{z}_{(i,1)}^{(t+1)}|\ Z^{(t)}, \mathbf{x},\boldsymbol{\phi}_{i}\big{)}}{\mathbb{P}\big{(}\mathbf{z}_{(i,2)}^{ (t+1)}|\ Z^{(t)},\mathbf{x},\boldsymbol{\phi}_{i}\big{)}}\] \[= \frac{\sum_{\boldsymbol{\theta}\in\Theta}\mathbb{P}\big{(} \mathbf{z}_{(i,1)}^{(t+1)}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\mathbb{P}\big{(} \boldsymbol{\theta}|\ Z^{(t)},\mathbf{x},\boldsymbol{\phi}_{i}\big{)}}{\sum_{ \boldsymbol{\theta}\in\Theta}\mathbb{P}\big{(}\mathbf{z}_{(i,1)}^{(t+1)}|\ \boldsymbol{\theta}, \boldsymbol{\phi}_{i}\big{)}\mathbb{P}\big{(}\boldsymbol{\theta}|\ \boldsymbol{\phi}_{i}\big{)} \mathbb{P}\big{(}\boldsymbol{\theta}|\ \boldsymbol{\phi}_{i}\big{)}}\] \[= \frac{\sum_{\boldsymbol{\theta}\in\Theta}\mathbb{P}\big{(} \mathbf{z}_{(i,1)}^{(t+1)}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\mathbb{P}\big{(} \mathbf{x}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\mathbb{P}\big{(} \boldsymbol{\theta}|\ \boldsymbol{\phi}_{i}\big{)}\bigg{(}\prod_{j=1}^{n}\mathbb{P}\big{(} \mathbf{z}_{j}^{(t)}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\bigg{)}\bigg{(} \mathbb{P}\big{(}\mathbf{z}^{(t)}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)} \bigg{)}^{m}}\] \[= \frac{\sum_{\boldsymbol{\theta}\in\Theta}\mathbb{P}\big{(} \mathbf{z}_{(i,2)}^{(t+1)}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\mathbb{P}\big{(} \mathbf{x}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\mathbb{P}( \boldsymbol{\theta}|\ \boldsymbol{\phi}_{i}\big{)}\bigg{(}\prod_{j=1}^{k}\mathbb{P}\big{(} \mathbf{z}_{j}^{(t)}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\bigg{)}\bigg{(} \mathbb{P}\big{(}\mathbf{z}^{(t)}|\ \boldsymbol{\theta},\boldsymbol{\phi}_{i}\big{)}\bigg{)}^{m}}\]

[MISSING_PAGE_FAIL:14]

where \(\mathbf{x}^{\prime}\) is a message which conveys the erroneous concept \(\bm{\theta}^{\prime}\). Using this change of model parameters, we can express the condition generation probability as,

\[\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\mathbf{x},\bm{\phi}_{i} \big{)}\] \[\propto \sum_{\bm{\theta}}\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\,\bm{ \theta},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\mathbf{x},\mathbf{x}^{\prime}|\, \bm{\theta},\bm{\phi}_{i}^{\prime}\big{)}\] \[= \sum_{\bm{\theta}}\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\,\bm{ \theta},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\mathbf{x}|\,\bm{\theta},\bm{\phi }_{i}^{\prime}\big{)}\mathbb{P}\big{(}\mathbf{x}^{\prime}|\,\bm{\theta},\bm{ \phi}_{i}^{\prime}\big{)}\]

With this formulation of the conditional generation probability, we can the ratio between the true concept \(\bm{\theta}^{*}\) and the erroneous concept \(\bm{\theta}^{\prime}\).

\[\frac{\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\,\bm{\theta}^{*}, \bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\mathbf{x}|\,\bm{\theta}^{*},\bm{\phi}_{i }^{\prime}\big{)}\mathbb{P}\big{(}\mathbf{x}^{\prime}|\,\bm{\theta}^{*},\bm{ \phi}_{i}^{\prime}\big{)}}{\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\,\bm{ \theta}^{\prime},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\mathbf{x}|\,\bm{\theta }^{\prime},\bm{\phi}_{i}^{\prime}\big{)}}\] \[< \frac{\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\,\bm{\theta}^{*}, \bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\mathbf{x}|\,\bm{\theta}^{*},\bm{\phi}_{i }^{\prime}\big{)}}{\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\,\bm{\theta}^{ \prime},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\mathbf{x}|\,\bm{\theta}^{\prime},\bm{\phi}_{i}^{\prime}\big{)}}\] (4)

which follows directly from \(\mathbb{P}\big{(}\mathbf{x}^{\prime}|\,\bm{\theta}^{\prime},\bm{\phi}_{i}^{ \prime}\big{)}>\mathbb{P}\big{(}\mathbf{x}^{\prime}|\,\bm{\theta}^{*},\bm{\phi }_{i}^{\prime}\big{)}\). When models have a shared misconception (left side of Equation 4),

\[\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\mathbf{x},\bm{\phi}_{i}\big{)}< \mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\mathbf{x},\bm{\phi}_{i}^{\prime} \big{)}\]

for any \(\mathbf{z}_{i}\) with

\[\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\,\bm{\theta}^{*},\bm{\phi}_{i}\big{)}< \mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\,\bm{\theta}^{\prime},\bm{\phi}_{i} \big{)}\]

Therefore, at timestep \(t=0\), models with the shared misconception are more likely to yield responses which correlate with \(\bm{\theta}^{\prime}\).

For rounds \(t>0\), we can express the conditional probability as,

\[\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\,\mathbf{x},Z^{(t)},\bm{ \phi}_{i}\big{)}\] \[\propto \sum_{\bm{\theta}}\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\,\bm{ \theta},\bm{\phi}_{i}\big{)}\prod_{j=m}^{n}\mathbb{P}\big{(}\mathbf{z}_{j}^{(t)}| \,\bm{\theta},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\mathbf{x},\mathbf{x}^{ \prime}|\,\bm{\theta},\bm{\phi}_{i}^{\prime}\big{)}\] \[= \sum_{\bm{\theta}}\bigg{(}\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}| \,\bm{\theta},\bm{\phi}_{i}\big{)}\prod_{j\leq m}^{n}\mathbb{P}\big{(}\mathbf{ z}_{j}^{(t)}|\,\bm{\theta},\bm{\phi}_{i}\big{)}\] \[\prod_{j>m}^{n}\mathbb{P}\big{(}\mathbf{z}_{j}^{(t)}|\,\bm{\theta}, \bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\mathbf{x}|\,\bm{\theta},\bm{\phi}_{i}^{ \prime}\big{)}\mathbb{P}\big{(}\mathbf{x}^{\prime}|\,\bm{\theta},\bm{\phi}_{i} ^{\prime}\big{)}\bigg{)}\]

For a given \(\mathbf{x}\), the terms

\[\prod_{j\leq m}^{n}\mathbb{P}\big{(}\mathbf{z}_{j}^{(t)}|\,\bm{\theta},\bm{ \phi}_{i}\big{)}\big{(}\mathbf{x}^{\prime}|\,\bm{\theta},\bm{\phi}_{i}^{\prime} \big{)}\]

are maximized, in expectation, for \(\bm{\theta}=\bm{\theta}^{\prime}\). Moreover, for the any concept \(\theta\neq\bm{\theta}^{\prime}\), the ratio

\[\frac{\prod_{j\leq m}^{n}\mathbb{P}\big{(}\mathbf{z}_{j}^{(t)}|\,\bm{\theta}^{ \prime},\bm{\phi}_{i}\big{)}\big{(}\mathbf{x}^{\prime}|\,\bm{\theta}^{\prime}, \bm{\phi}_{i}^{\prime}\big{)}}{\prod_{j\leq m}^{n}\mathbb{P}\big{(}\mathbf{z}_{j }^{(t)}|\,\bm{\theta},\bm{\phi}_{i}\big{)}\big{(}\mathbf{x}^{\prime}|\,\bm{ \theta},\bm{\phi}_{i}^{\prime}\big{)}}\]

is monotonically increasing as \(m\) increases. Therefore, for any round, a model with the shared misconception is more likely to generate answers correlating with \(\bm{\theta}^{\prime}\), than those without the shared misconception. Further, the likelihood of generating such answers increases with \(m\).

Proof of Theorem 6.1.: As shown in the proof of Theorem 5.4, we can express each model's conditional generation probability as

\[\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\:\mathbf{x},Z^{(t)},\bm{ \phi}_{i}\big{)}\] \[\propto \sum_{\bm{\theta}}\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\:\bm{ \theta},\bm{\phi}_{i}\big{)}\prod_{j=1}^{n}\mathbb{P}\big{(}\mathbf{z}_{j}^{(t )}|\:\bm{\theta},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(}\mathbf{x},\mathbf{x}^{ \prime}|\:\bm{\theta},\bm{\phi}_{i}^{\prime}\big{)}\] \[= \sum_{\bm{\theta}}\bigg{(}\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)} |\:\bm{\theta},\bm{\phi}_{i}\big{)}\prod_{j\leq m}^{n}\mathbb{P}\big{(} \mathbf{z}_{j}^{(t)}|\:\bm{\theta},\bm{\phi}_{i}\big{)}\]

where \(\mathbf{x}^{\prime}\) is a response which conveys the erroneous concept \(\bm{\theta}^{\prime}\). Under this expression, each response \(\mathbf{z}_{i}^{(t)}\) is generated according to \(D(\mathbf{x},Z^{(t)},\bm{\phi}_{i})\) where each distribution differs only by the model parameters \(\bm{\phi}_{i}\). For \(i\leq m\), the model parameters \(\bm{\phi}_{i}\) posses the common misconception, i.e., each distribution \(D(\mathbf{z}_{j}^{(t)}|\:\mathbf{x},Z^{(t)},\bm{\phi}_{i})\) has a common scaling factor \(\mathbb{P}\big{(}\mathbf{x}^{\prime}|\:\bm{\theta},\bm{\phi}_{i}^{\prime}\big{)}\) which is maximized at \(\bm{\theta}=\bm{\theta}^{\prime}\). Since only these models share this scaling factor,

whenever \(i_{1},i_{2}\leq m<j\). Hence, diversity pruning is more likely to select terms from agents with \(m<j\) (i.e., those without the shared misconception), then agents with \(j\leq m\). Since this holds true on every round of debate, all responses in the debate process place a lower weight on responses associated with \(\bm{\theta}^{\prime}\), i.e., a lower weight is placed on responses which have a higher chance of being incorrect. 

Proof of Theorem 6.2.: At each round \(t\), quality pruning selection a set of \(k\) responses,

\[Z^{\prime(t)}= \underset{Y\subset Z^{(t)}}{\text{argmin}}\sum_{\mathbf{z}_{i} \in Y}\text{KL}\big{(}D(\bm{\theta}|\:\mathbf{x}),\:D(\bm{\theta}|\:\mathbf{z} _{i})\big{)}\] \[\text{s.t.}\:\:|Y|=k\]

As such, for any set of responses \(Z^{(t)}\)

\[\sum_{\mathbf{z}_{i}^{\prime}\in Z^{\prime(t)}}\text{KL}\big{(}D(\bm{\theta} |\:\mathbf{x}),\:D(\bm{\theta}|\:\mathbf{z}_{i}^{\prime})\big{)}\leq\sum_{ \mathbf{z}_{i}\in Z^{(t)}}\text{KL}\big{(}D(\bm{\theta}|\:\mathbf{x}),\:D(\bm {\theta}|\:\mathbf{z}_{i})\big{)}\]

Using the fact that \(Z^{\prime(t)}\subset Z^{(t)}\), we can write,

\[\sum_{\mathbf{z}_{i}^{\prime}\in Z^{\prime(t)}}\text{KL}\big{(}D( \bm{\theta}|\:\mathbf{x}),\:D(\bm{\theta}|\:\mathbf{z}_{i}^{\prime})\big{)}\] \[\leq \sum_{\mathbf{z}_{i}^{\prime}\in Z^{\prime(t)}}\text{KL}\big{(}D( \bm{\theta}|\:\mathbf{x}),\:D(\bm{\theta}|\:\mathbf{z}_{i}^{\prime})\big{)}+ \sum_{\mathbf{z}_{i}\in Z^{(t)}\setminus Z^{\prime(t)}}\text{KL}\big{(}D(\bm{ \theta}|\:\mathbf{x}),\:D(\bm{\theta}|\:\mathbf{z}_{i})\big{)}\] \[\implies\] \[\quad 0\leq\sum_{\mathbf{z}_{i}\in Z^{(t)}\setminus Z^{\prime(t)}} \text{KL}\big{(}D(\bm{\theta}|\:\mathbf{x}),\:D(\bm{\theta}|\:\mathbf{z}_{i}) \big{)}\]

Thus, for a random task and answer pair \((\mathbf{x},y)\sim D(\bm{\theta}^{*})\), the relationship between the correctness of answers in \(Z^{(t)}\) and \(Z^{\prime(t)}\) is

\[\mathbb{E}_{(\mathbf{x},y)\sim D(\bm{\theta}^{*})}\bigg{[}\frac{1}{|Z^{\prime( t)}|}\sum_{\mathbf{z}_{i}\in Z^{\prime(t)}}\mathbb{P}\big{(}a(\mathbf{z}_{i})=y \big{)}\bigg{]}\geq\mathbb{E}_{(\mathbf{x},y)\sim D(\bm{\theta}^{*})}\bigg{[} \frac{1}{|Z^{(t)}\setminus Z^{\prime(t)}|}\sum_{\mathbf{z}_{i}\in Z^{(t)} \setminus Z^{\prime(t)}}\mathbb{P}\big{(}a(\mathbf{z}_{i})=y\big{)}\bigg{]}.\]

That is, in expectation, the responses which are selected for quality pruning are at least as correct as those which are removed by quality pruning. Therefore, in expectation across all possible generations\(\mathbf{z}_{i}^{(t+1)}\) with \(a(\mathbf{z}_{i}^{(t)})=y\),

\[\mathbb{E}_{\mathbf{z}_{i}^{(t+1)}}\bigg{[}\sum_{\bm{\theta}\in \Theta}\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\:\bm{\theta},\bm{\phi}_{i}\big{)} \mathbb{P}\big{(}\mathbf{x}|\:\bm{\theta},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(} \bm{\theta}|\:\bm{\phi}_{i}\big{)}\prod_{\mathbf{z}_{j}^{\prime}\in Z^{(t)}}^{ n}\mathbb{P}\big{(}\mathbf{z}_{j}^{\prime(t)}|\:\bm{\theta},\bm{\phi}_{i}\big{)} \bigg{]}\] \[\geq\mathbb{E}_{\mathbf{z}_{i}^{(t+1)}}\bigg{[}\sum_{\bm{\theta} \in\Theta}\mathbb{P}\big{(}\mathbf{z}_{i}^{(t+1)}|\:\theta,\bm{\phi}_{i}\big{)} \mathbb{P}\big{(}\mathbf{x}|\:\bm{\theta},\bm{\phi}_{i}\big{)}\mathbb{P}\big{(} \bm{\theta}|\:\bm{\phi}_{i}\big{)}\prod_{\mathbf{z}_{j}\in Z^{(t)}}^{n}\mathbb{ P}\big{(}\mathbf{z}_{j}^{(t)}|\:\bm{\theta},\bm{\phi}_{i}\big{)}\bigg{]}\] \[\implies\mathbb{E}_{\mathbf{z}_{i}^{(t+1)}}\bigg{[}\mathbb{P} \big{(}\mathbf{z}_{i}^{(t+1)}|\:\mathbf{x},Z^{(t)},\bm{\phi}_{i}\big{)}\bigg{]} \geq\mathbb{E}_{\mathbf{z}_{i}^{(t+1)}}\bigg{[}\mathbb{P}\big{(}\mathbf{z}_{i }^{(t+1)}|\:\mathbf{x},Z^{\prime(t)},\bm{\phi}_{i}\big{)}\bigg{]}\]

Therefore, the probability of model \(i\) generating a response \(\mathbf{z}_{i}^{(t)}\) at time \(t\), which has \(a(\mathbf{z}_{i}^{(t)})=y\) is greater when conditioning only on the responses selected by quality pruning, i.e., \(Z^{\prime(t)}\). 

Proof of Theorem 6.4.: Let \(\mathbf{z}\) be a given response and \(\mathbf{z}^{\prime}\) be a corrected version of that response after misconception refutation, then

\[\text{KL}\big{(}D(\bm{\theta}|\:\mathbf{x},y),D(\bm{\theta}|\:\mathbf{z}^{ \prime})\big{)}\leq\text{KL}\big{(}D(\bm{\theta}|\:\mathbf{x},y),D(\bm{\theta} |\:\mathbf{z})\big{)}\]

That is, the distribution over concepts given the corrected response \(\mathbf{z}^{\prime}\) is more similar to the true distribution over concepts given the task \(\mathbf{x}\) and answer \(y\) compared to the original answer \(\mathbf{z}\).

Similar to the case of quality pruning, we can then express the KL divergence of all responses before refutation \(Z^{(t)}\), and after refutation \(Z^{\prime(t)}\), as

\[\sum_{\mathbf{z}_{i}^{\prime}\in Z^{\prime(t)}}\text{KL}\big{(}D (\bm{\theta}|\:\mathbf{x},y),D(\bm{\theta}|\:\mathbf{z}_{i}^{\prime})\big{)} \leq\sum_{\mathbf{z}_{i}\in Z^{(t)}}\text{KL}\big{(}D(\bm{\theta}|\:\mathbf{x},y),D(\bm{\theta}|\:\mathbf{z}_{i})\big{)}\] \[\implies\sum_{i=1}^{n}\text{KL}\big{(}D(\bm{\theta}|\:\mathbf{x},y),D(\bm{\theta}|\:\mathbf{z}_{i}^{\prime})\big{)}-\text{KL}\big{(}D(\bm{ \theta}|\:\mathbf{x},y),D(\bm{\theta}|\:\mathbf{z}_{i})\big{)}\leq 0\]

Thus, in aggregate, misconception refutation results in all responses in \(Y(t)\) inducing a distribution over concepts which is more similar to the distribution over concepts given the task \(\mathbf{x}\) and answer \(y\). As shown in the case of quality pruning, this relationship implies that at the next step of generation, model \(i\) is more likely to generate \(\mathbf{z}_{i}^{(t+1)}\) with \(a(\mathbf{z}_{i}^{(t+1)})=y\) when conditioning on \(Z^{\prime(t)}\) compared with \(Z^{(t)}\). 

## Appendix B Shared Misconceptions

Latent concepts, and by extension, shared misconceptions, are quite general and may not always be human-interpretable. To better elucidate what is meant by a shared misconception, we provide an example of one possible type of misconception. In Figure 4 we see model responses to a question regarding the song "Take Me Home, Country Roads", which is a well-known song about the state West Virginia3. While the models identify the connection between the song and West Virginia, they each erroneously equate West Virginia and Virginia, ultimately leading to each model providing the wrong answer. In situations such as this, debate will converge to an incorrect answer due to each model sharing the same false belief.

Footnote 3: Models are given a question and a passage (the passage is omitted from Figure 4)

Misconceptions can also be viewed through the lens of hallucinations. As a byproduct of erroneous training, the models in the above example have learned a false connection between two topics (Virginia and West Virginia).

## Appendix C Experiments

InterventionsWhen combining our three interventions together, we find that first applying quality pruning, then diversity pruning, then misconception refutation, results in the best performance. In practice, we consider all previous responses up to the current round \(t\) when applying pruning, i.e.,\(Z^{(0)}\cup\ldots\cup Z^{(t)}\) results in better performance. When considering all past responses during pruning, there is a potentential misconception refutation, then al that the same set of responses is picked at each round. We simply prevent the methods from selecting the same response during two consecutive rounds to avoid this issue.

Application of Interventions in PracticeFor approximate the KL-divergence between distributions of concepts needed for our running interventions, i.e.

\[\text{KL}\big{(}D(\bm{\theta}|\mathbf{x}),D(\bm{\theta}|\mathbf{z}_{i})\big{)} \quad\text{or}\quad\text{KL}\big{(}D(\bm{\theta}|\mathbf{z}_{i}),D(\bm{\theta }|\mathbf{z}_{j})\big{)}\]

we use the distance between sentence embeddings of each string, \(\mathbf{x}\), \(\mathbf{z}_{i}\), and \(\mathbf{z}_{j}\), i.e., given sentence embedding model \(g\), we to approximate the above in practice via,

\[\text{KL}\|g(\mathbf{x})-g(\mathbf{z}_{i})\|\quad\text{or}\quad\text{KL}\|g( \mathbf{z}_{i})-g(\mathbf{z}_{j})\|\]

```
1:Input: task \(\mathbf{x}\),
2:\(Z_{\text{all}}=\{\}\) // Set of all responses to consider when applying interventions
3:for\(j=1\ldots n\)do
4: get response \(\mathbf{z}_{j}^{(0)}\) by prompting \(\text{LLM}_{j}\), i.e. sample \(\mathbf{z}_{j}^{(0)}\) according to \(\mathbb{P}\big{(}\mathbf{z}|\ \mathbf{x},\bm{\phi}_{j}\big{)}\)
5:\(Z_{\text{all}}\text{add}(\mathbf{z}_{j}^{(0)})\) // Get an initial set of responses from each LLM
6:endfor
7:for\(t=1\ldots T\)do
8:\(Z^{\prime(t)}=\text{QualityPrune}\big{(}Z_{\text{all}},k=1/2|Z_{\text{all}}| \big{)}\) // Prune half the current responses
9:\(Z^{\prime(t)}=\text{DiversityPrune}\big{(}Z^{\prime(t)},k=n\big{)}\)
10:\(Z^{\prime(t)}=\text{MisconceptionRefutation}\big{(}Z^{\prime(t)}\big{)}\) // Set of responses to use agents to consider
11:for\(j=1\ldots n\)do
12: get \(\mathbf{z}_{j}^{(t)}\) by sampling according to \(\mathbb{P}\big{(}\mathbf{z}|\ \mathbf{x},Z^{\prime(t)},\bm{\phi}_{j}\big{)}\), for each \(j\) // Updated responses
13:\(Z_{\text{all}}\text{add}\big{(}\mathbf{z}_{j}^{(t)}\big{)}\)
14:endfor
15:endfor
16:Return\(Z_{\text{all}}[-n:]\) // Each model's response on the last round of debate ```

**Algorithm 1** Application of Combined Interventions

ModelsFor our experiments we make use of four models: GPT-3.5, Llama-2, Llama-3, and Mistral. For certain tasks such as BoolQ or MMLU, such as only providing "Yes" or "No" to BoolQ questions. Justifications for answers are important for both our method and regular debate, as such we set the minimum token for Flan-T5 to be \(10\), and we set the repetition penalty to be \(1.5\).

Measuring AccuracyIn the BoolQ, MMLU, and Math datasets, we extract the model's answers through regular expression checking and compare these extracted answers to the true answer; models are prompted to provide their final answer in the form _"Final Answer: \(X\)"_. In the TruthfulQA dataset

Figure 4: Example of a common misconception between models and a refutation of that misconception.

model answers are taken to be their entire response, which is then judged as being correct or incorrect by a GPT-4 judge. This judge is prompted to provide a yes-no answer to the question _"Dose the answer {answer {answer } accurately answer the question {question }?"_. We allow for models to provide answers of abstention, e.g., responding _"I do not know"_. Abstentions correspond to an accuracy of \(.5\) in BoolQ, \(.25\) in MMLU, \(0\) in Math, and are directly scored by the LLM judge in TruthfulQA.

Target AnswIn Figure 1 we measure the likelihood that a given model will echo a _target answer_ as a function of how many other model select that target answer. Target answers are Yes in BoolQ, option A in MMLU, _correctAnswer_\(-30\) in Math, and a false answer in TruthfulQA. In our experiment, we elicit 20 answers from each model (GPT-3.5, Llama-2, Flan-T5) and then downsample these answers to ensure that each model receives a specific number of target answers.

Abletion of InterventionsHere we provide an ablation of each of our three interventions: Misconception Refutation, Diversity Pruning, and Quality Pruning. Results are shown in Table 3. From this table, we see two key takeaways. First, a combination of all three interventions achieves the highest performance in almost all cases. Second, applying intervention individually can result in worse performance (even when compared with a single model). This is expected as our interventions are designed to work together, rather than separately. Recall that when combining the interventions we first do Quality Pruning, then Diversity Pruning, and then Misconception Refutation. This ordering of interventions ensures that we first select sufficiently relevant responses (Quality Pruning), among those relevant responses we then ensure that the distribution of opinions within these responses is well balanced (Diversity Pruning), and then we lastly ensure that none of the responses contain errors or misconceptions (Misconception Refutation).

\begin{table}
\begin{tabular}{||c|c|c||} \hline
**Model Name** & **Model Version** & **Library** \\ \hline \hline GPT-3.5 & GPT-3.5 Turbo & openai \\ \hline Llama-2 & Llama-2 7B Chat & huggingface \\ \hline Llama-3 & Llama-3 8B Instruct & huggingface \\ \hline Mistral & Mistral 7B Instruct v02 & huggingface \\ \hline \end{tabular}
\end{table}
Table 2: List of specific types of models used in experiments

\begin{table}
\begin{tabular}{||c||c|c|c|c|c|c||} \hline \hline  & **Single** & **SoM** & **Ours** & **MR** & **DP** & **QP** \\ \hline \hline  & \multicolumn{6}{c||}{**BoolQ**} \\ \hline \(6\times\) GPT-3.5 & \(.80_{\pm.014}\) & \(.84_{\pm.012}\) & \(.85_{\pm.012}\) & \(.83_{\pm.013}\) & \(.84_{\pm.012}\) & \(.84_{\pm.012}\) \\ \hline \(6\times\) Llama-3 & \(.76_{\pm.014}\) & \(.78_{\pm.013}\) & \(.78_{\pm.013}\) & \(.78_{\pm.013}\) & \(.76_{\pm.013}\) & \(.77_{\pm.013}\) \\ \hline \(6\times\) Llama-2 & \(.67_{\pm.017}\) & \(.68_{\pm.017}\) & \(.70_{\pm.016}\) & \(.67_{\pm.017}\) & \(.69_{\pm.016}\) & \(.73_{\pm.016}\) \\ \hline \(6\times\) Mistral & \(.80_{\pm.014}\) & \(.82_{\pm.013}\) & \(.85_{\pm.012}\) & \(.83_{\pm.013}\) & \(.81_{\pm.014}\) & \(.82_{\pm.013}\) \\ \hline \hline  & \multicolumn{6}{c||}{**MMLU**} \\ \hline \(6\times\) GPT-3.5 & \(.73_{\pm.014}\) & \(.74_{\pm.015}\) & \(.79_{\pm.014}\) & \(.75_{\pm.015}\) & \(.72_{\pm.016}\) & \(.73_{\pm.016}\) \\ \hline \(6\times\) Llama-3 & \(.67_{\pm.016}\) & \(.70_{\pm.015}\) & \(.75_{\pm.014}\) & \(.71_{\pm.015}\) & \(.68_{\pm.016}\) & \(.68_{\pm.016}\) \\ \hline \(6\times\) Llama-2 & \(.41_{\pm.017}\) & \(.47_{\pm.018}\) & \(.52_{\pm.018}\) & \(.50_{\pm.018}\) & \(.40_{\pm 0.17}\) & \(.43_{\pm 0.17}\) \\ \hline \(6\times\) Mistral & \(.66_{\pm.016}\) & \(.65_{\pm.016}\) & \(.66_{\pm.016}\) & \(.66_{\pm.016}\) & \(.58_{\pm.018}\) & \(.62_{\pm.017}\) \\ \hline \hline  & \multicolumn{6}{c||}{**TruthfulQA**} \\ \hline \(6\times\) GPT-3.5 & \(.61_{\pm.033}\) & \(.63_{\pm.032}\) & \(.69_{\pm.030}\) & \(.65_{\pm.032}\) & \(.58_{\pm.034}\) & \(.62_{\pm.034}\) \\ \hline \(6\times\) Llama-2 & \(.47_{\pm.034}\) & \(.52_{\pm.035}\) & \(.55_{\pm.034}\) & \(.53_{\pm.034}\) & \(.46_{\pm.034}\) & \(.44_{\pm.034}\) \\ \hline \(6\times\) Llama-3 & \(.53_{\pm.035}\) & \(.55_{\pm.032}\) & \(.55_{\pm.032}\) & \(.55_{\pm.032}\) & \(.53_{\pm.032}\) & \(.54_{\pm.032}\) \\ \hline \(6\times\) Mistral & \(.48_{\pm.034}\) & \(.51_{\pm.035}\) & \(.53_{\pm.034}\) & \(.53_{\pm.034}\) & \(.49_{\pm.034}\) & \(.47_{\pm.034}\) \\ \hline \hline  & \multicolumn{6}{c||}{**MathQ**} \\ \hline \(6\times\) GPT-3.5 & \(.53_{\pm.035}\) & \(.88_{\pm.016}\) & \(.93_{\pm.01}\) & \(.92_{\pm.01}\) & \(.85_{\pm.016}\) & \(.86_{\pm.016}\) \\ \hline \(6\times\) Llama-2 & \(.11_{\pm.013}\) & \(.13_{\pm.014}\) & \(.19_{\pm.015}\) & \(.18_{\pm.015}\) & \(.13_{\pm.014}\) & \(.13_{\pm.014}\) \\ \hline \(6\times\) Llama-3 & \(.25_{\pm.016}\) & \(.33_{\pm.017}\) & \(.48_{\pm.018}\) & \(.49_{\pm.018}\) & \(.32_{\pm.017}\) & \(.32_{\pm.017}\) \\ \hline \(6\times\) Mistral & \(.13_{\pm.013}\) & \(.19_{\pm.014}\) & \(.18_{\pm.014}\) & \(.17_{\pm.014}\) & \(.15_{\pm.014}\) & \(.16_{\pm.014}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average accuracy for each intervention: Misconception Refutation (MR). Diversity Pruning (DP), Quality Pruning (QP), a combination of all three (Ours), and vanilla debate (Debate), for 10 rounds and 6 models. Note that the “Single”, “Debate”, and “Ours” columns correspond to the same columns in Table 3Resource DetailsFor all experiments, we use one Nvdata Tesla V100 GPU and one Intel 32-core CPU. For inference with non-API models (i.e., Llama-2, Llama-3, and Mistral, we use the VLLM library Kwon et al. [2023]). Ten rounds of debate with six models and 3,000 questions has a mean completion time of 4 hours for Llama-2, Llama-3, and Mistral. For GPT-3.5, we use the open library, ten rounds of debate with six models and 3,000 questions, has a mean completion time of 12 hours.

### Prompt Examples

We provide example prompt templates for the BoolQ dataset. Prompt templates for other datasets are similar but have changes to reflect the different task types (e.g., multiple-choice answers in MMLU compared to yes-no answers in BoolQ).

**Round 0, or No Debate:**

prompt = You will be given a yes-no question which is based on a passage.  You should use the passage to help you answer the question.  You should give a brief justification for your answer,  and you must provide a final answer of either Yes or No. \n Question: {_QUESTION_} \n Passage: {_PASSAGE_}

**Debate with Round \(>0\)**

prompt = Several other models have provided responses to a yes-no question,  below are their responses: \n Model 1: {_RESPONSE[1]_}..... \n Model n: {_RESPONSE[n]_} \n You should consider these responses when answering the following yes-no question which is based on a passage.  You should use the given responses and the passage to help you answer the question.  You should give a brief justification for your answer, and you must provide a  final answer of either Yes or No. \n Question: {_QUESTION_} \n Passage: {_PASSAGE_}

**Misconception Refutation (Identifying Misconceptions)**

prompt = I would like you to evaluate an answer to a question based on a passage.  Please evaluate this answer and identify any errors, misconceptions,  or inconsistencies with the passage.  If you identify any such errors, please provide  a short list of specific details and briefly discuss how the misconceptions  can be fixed. \n Question: {_QUESTION_} \n Passage: {_PASSAGE_} \n Answer to Evaluate Answer: {_GIVEN_ANSWER_}

**Misconception Refutation (Fixing Misconceptions)**

prompt = I would like you to make corrections to a response.  You will be given a yes-no question based on a passage, a response to  that question, and a list of possible issues with the response.  I want you to provide a corrected version of the response based on the  list of possible issues.  You should make as few changes as possible. \n Question: {_QUESTION_}

\n Passage: {_PASSAGE_}  \n Response to Correct: {_RESPONSE_}  \n Possible Issues: {_LIST_OF_ISSUES_}

**Targeted Answer (Advocating for a Specific Answer)**

prompt = You will be given a yes-no question which is based on a passage.  You should use the passage to provide an answer of {_TARGET_ANSWER_}.  You should give a brief justification for that answer,  and you must provide a final answer {_TARGET_ANSWER_}.  \n Question: {_QUESTION_}  \n Passage: {_PASSAGE_}

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims made in the abstract match theoretical and experimental results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We mention the limitation of our approach in the conclusion section and experiment section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]All of our theorems state the assumption, and we provide complete proofs in the appendix. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We include the details of the experimental results and setup. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We cite all data used in the paper and we will release our code publicly upon publication. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide full information about all expeirments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide confidence intervals for experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report the GPU and CPU types and mounts. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] We have read the code of ethics and are adhering to it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA] Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **License for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All data that we use is publicly available and properly cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.