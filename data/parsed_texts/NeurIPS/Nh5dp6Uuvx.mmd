[MISSING_PAGE_FAIL:1]

Although these representation learning strategies are effective, their pretraining objectives do not directly constrain the global structure of the learned representations. To classify ImageNet images into 1000 classes, networks' penultimate layers must learn representations that allow examples of a given class to be linearly separated from representations of other classes, but the classes themselves could be arranged in any fashion. The objective itself does not directly encourage images of tabby cats to be closer to images of other breeds of cats than to images of raccoons. Similarly, contrastive objectives force related embeddings to be close, and unrelated embeddings to be far, but if a cluster of data points is sufficiently far from all other data points, then the location of this cluster in the embedding space has minimal impact on the value of the loss [62, 35, 9].

Even without explicit global supervision, however, networks implicitly learn to organize high-level concepts somewhat coherently. For example, ImageNet models' representations roughly cluster according to superclasses [19], and the similarity structure of neural network representation spaces is non-trivially similar to similarity structures inferred from brain data or human judgments [40, 92, 66, 80, 49]. The structure that is learned likely reflects a combination of visual similarity between images from related classes and networks' inductive biases. However, there is little reason to believe that this implicitly-learned global structure should be optimal.

Human and machine vision differ in many ways. Some differences relate to sensitivity to distortions and out-of-distribution generalization [21, 22, 25]. For example, ImageNet models are more biased by texture than humans in their decision process [22, 34]. However, interventions that reduce neural networks' sensitivity to image distortions are not enough to make their representations as robust and transferable as those of humans [23]. Experiments inferring humans' object representations from similarity judgments suggest that humans use a complex combination of semantics, texture, shape, and color-related concepts for performing similarity judgments [31, 60].

Various strategies have been proposed to improve _representational alignment_ between neural nets and humans [e.g., 66, 67, 2, 61], to improve robustness against image distortions [e.g., 34, 21], or for obtaining models that make more human-like errors [e.g., 22, 26]. Muttenthaler et al. [61] previously found that a linear transformation learned to maximize alignment on one dataset of human similarity judgments generalized to different datasets of similarity judgments. However, it remains unclear whether better representational alignment can improve networks' generalization on vision tasks.

Here, we study the impact of aligning representations' global structure with human similarity judgments on downstream tasks. These similarity judgments, collected by asking subjects to choose the _odd-one-out_ in a triplet of images [32], provide explicit supervision for relationships among disparate concepts. Although the number of images we use for alignment is three to six orders of magnitude smaller than the number of images in pretraining, we observe significant improvements. Specifically, our contributions are as follows:

Figure 1: Global-local (gLocal) transforms yield a _best-of-both-worlds_ representation space, which improves overall performance. (a) The original representations capture local structure, such as that different trees are similar, but have poor global structure. The gLocal transform preserves local structure, while integrating global information from human knowledge; e.g., unifying superordinate categories, organizing by “animacy”, or connecting semantically-related categories like “food” and “drink”. (b) The gLocal transforms improve both human alignment and downstream task performance compared to original and naively aligned representations for image/text models. We report mean accuracies on anomaly detection and (5-shot) few-shot learning tasks.

* We introduce the _gLocal transform_, a linear transformation that minimizes a combination of a _global alignment loss_, which aligns the representation with human similarity judgments, and a _local contrastive loss_ that maintains the local structure of the original representation space.
* We show that the gLocal transform preserves the local structure of the original space but captures the same global structure as a _naive transform_ that minimizes only the global alignment loss.
* We demonstrate that the gLocal transform substantially increases performance on a variety of few-shot learning and anomaly detection tasks. By contrast, the naive transform impairs performance.
* We compare the human alignment of gLocal and naively transformed representations on four human similarity judgment datasets, finding that the gLocal transform yields only marginally worse alignment than the naive transform.

## 2 Related work

How can we build models which learn representations that support performance on variable downstream tasks? This question has been a core theme of computer vision research [9; 27; 58], but the impact of pretraining on feature representations is complex, and better performance does not necessarily yield more transferable representations [e.g., 44; 45]. For example, some pretraining leads to shortcut learning [23; 50; 1; 57; 22; 34; 3; 91]. Because the relationship between training methods and representations is complicated, it is useful to study how datasets and training shape representations [33]. Standard training objectives do not explicitly constrain the global structure of representations; nevertheless, these objectives yield representations that capture some aspects of the higher-order category structure [e.g., 40] and neural unit activity [e.g., 92; 80] of human and animal representations of the same images. Some models progressively differentiate hierarchical structure over the course of learning [5] in a similar way to how humans learn semantic features [73; 18; 78; 5; 79]. Even so, learned representations still fail to capture important aspects of the structure that humans learn [6]. Human representations capture both perceptual and semantic features [7; 69], including many levels of semantic hierarchy (e.g. higher-level: "animate" [11], superordinate: "mammal", basic: "dog", subordinate: "dashund"), with a bias towards the basic level [74; 55; 38], as well as cross-cutting semantic features [84; 56].

While models may implicitly learn to represent some of this structure, these implicit representations may have shortcomings. For example, Huh et al. [37] suggest that ImageNet models capture only higher-level categories where the sub-categories are visually similar. Similarly, Peterson et al. [66] show that model representations do not natively fully capture the structure of human similarity judgments, though they can be transformed to align better [66; 2]. In some cases, language provides more accurate estimates of human similarity judgments than image representations [54], and image-text models can have more human-aligned representations [61]. How does human alignment affect downstream performance? Sucholutsky & Griffiths [87] show that models which are more human-aligned (but not specifically optimized for alignment) are more robust on few-shot learning tasks. Other work shows benefits of incorporating higher-level category structure [51; 83]. Here, we ask whether transforming model representations to align with human knowledge can improve transfer.

## 3 Methods

**Data.** For measuring the degree of alignment between human and neural network similarity spaces, we use the things dataset, which is a large behavioral dataset of \(4.70\) million unique triplet responses crowdsourced from \(12{,}340\) human participants for \(1854\) natural object images [32]. Images used for collecting human responses in the triplet odd-one-out task were taken from the things object concept and image database [30], which is a collection of natural object images.

**Odd-one-out accuracy.** The triplet odd-one-out task is a commonly used task in the cognitive sciences to measure human notions of object similarity without biasing a participant into a specific direction [20; 72; 31; 60]. To measure the degree of alignment between human and neural network similarity judgments in the things triplet task, we examine the extent to which the odd-one-out can be identified directly from the similarities between images in models' representation spaces. Given representations \(\bm{x}_{1}\), \(\bm{x}_{2}\), and \(\bm{x}_{3}\) of the three images in a triplet, we first construct a similarity matrix \(\bm{S}\in\mathbb{R}^{3\times 3}\) where \(S_{i,j}:=\bm{x}_{i}^{\top}\bm{x}_{j}/(\|\bm{x}_{i}\|_{2}\|\bm{x}_{j}\|_{2})\), the cosine similarity between a pair of representations.3 We identify the closest pair of images in the triplet as \(\arg\max_{i,j>i}S_{i,j}\) with the remaining image being the odd-one-out. We define odd-one-out accuracy as the proportion of triplets where the odd-one-out matches the human odd-one-out choice.

**Alignment loss.** Given an image similarity matrix \(\bm{S}\) and a triplet \(\{i,j,k\}\) (here, images are indexed by natural numbers), the likelihood of a particular pair, \(\{a,b\}\subset\{i,j,k\}\), being most similar, and hence the remaining image being the odd-one-out, is modeled by the softmax of the object similarities,

\[p(\{a,b\}|\{i,j,k\},\bm{S})\coloneqq\exp(S_{a,b})/\left(\exp(S_{i,j})+\exp(S_ {i,k})+\exp(S_{j,k})\right).\] (1)

For \(n\) triplet responses we use the following negative log-likelihood, precisely defined in [60],

\[\mathcal{L}_{\text{global}}(\bm{S})\coloneqq-\frac{1}{n}\sum_{s=1}^{n}\log \underbrace{p\left(\{a_{s},b_{s}\}|\{i_{s},j_{s},k_{s}\},\bm{S}\right)}_{ \text{odd-one-out prediction}}.\] (2)

Since the triplets in [32] consist of randomly selected images, the concepts that humans use for their similarity judgments in the things triplet odd-one-out task primarily reflect superordinate categories rather than fine-grained object features [31, 60], the above alignment loss can be viewed as a loss function whose objective is to transform the representations into a globally-restructured human similarity space where superordinate categories are emphasized over subordinate categories.

**Naive transform.** We first investigate a linear transformation that naively maximizes alignment between neural network representations and human similarity judgments with \(L_{2}\) regularization. This transformation consists of a square matrix \(\bm{W}\) obtained as the solution to

\[\operatorname*{arg\,min}_{\bm{W},\bm{b}}\ \mathcal{L}_{\text{global}}(\bm{S})+ \lambda||\bm{W}||_{\text{F}}^{2},\] (3)

where \(S_{ij}=\left(\bm{W}\bm{x}_{i}+\bm{b}\right)^{\top}\left(\bm{W}\bm{x}_{j}+\bm{ b}\right)\). We call this transformation the _naive transform_ because the regularization term helps prevent overfitting to the training set, but does not encourage the transformed representation space to resemble the original space. Mutenthaler et al. [61] previously investigated a similar transformation. We determine \(\lambda\) via grid-search using \(k\)-fold cross-validation (CV). To obtain a minimally biased estimate of the odd-one-out accuracy of the transform, we partition the \(1854\) objects in things into two disjoint sets, following the procedure of Mutenthaler et al. [61].

**Global transform.** The naive transform does not preserve representational structure that is irrelevant to the odd-one-out task. The global transform instead shrinks \(\bm{W}\) toward a scaled identity matrix by penalizing \(\min_{\alpha}||\bm{W}-\alpha I||_{\text{F}}^{2}\), thus regularizing the transformed representation toward the original. The global transform solves the following minimization problem,

\[\operatorname*{arg\,min}_{\bm{W},\bm{b}}\mathcal{L}_{\text{global}}(\bm{S})+ \lambda\min_{\alpha}\left\lVert\bm{W}-\alpha I\right\rVert_{\text{F}}^{2}= \operatorname*{arg\,min}_{\bm{W},\bm{b}}\mathcal{L}_{\text{global}}(\bm{S})+ \lambda\left\lVert\bm{W}-\left(\sum_{j=1}^{p}\bm{W}_{jj}/p\right)I\right\rVert _{\text{F}}^{2}.\] (4)

We derive the above equality in Appx. F. Again, we select \(\lambda\) via grid-search with \(k\)-fold CV.

**gLocal transform.** In preliminary experiments, we observed a trade-off between alignment and the transferability of a neural network's human-aligned representation space to downstream tasks. Maximizing alignment appears to slightly worsen downstream task performance, whereas using a large value of \(\lambda\) in Eq. 4 leads to a representation that closely resembles the original (since \(\lim_{\lambda\to\infty}\bm{W}=\sigma I\)). Thus, we add an additional regularization term to the objective with the goal of preserving the local structure of the network's original representation space. This loss term can be seen as an additional constraint on the transformation matrix \(\bm{W}\).

We call this loss term _local loss_ and the transform that optimizes this full objective the _gLocal transform_, where global and local representations structures are jointly optimized. For this loss function, we embed all images of the ImageNet train and validation sets [17] in a neural network's \(p\)-dimensional penultimate layer space or image encoder space of image/text models. Let \(\bm{Y}\in\mathbb{R}^{m\times p}\) be a neural network's feature matrix for all \(m\) images in the ImageNet train set. Let \(\bm{S}^{*}\) be the cosine similarity matrix using untransformed representations where \(S^{*}_{ij}=\left(\bm{y}_{i}^{\top}\bm{y}_{j}\right)/\big{(}||\bm{y}_{i}||_{2} ||\bm{y}_{j}||_{2}\big{)}\) and let \(\bm{S}^{\dagger}\) be the cosine similarity matrix of the transformed representations where

\[S^{\dagger}_{ij}=\left(\left(\bm{W}\bm{y}_{i}+\bm{b}\right)^{\top}\left(\bm{W} \bm{y}_{j}+\bm{b}\right)\right)/\big{(}||\bm{W}\bm{y}_{i}+\bm{b}||_{2}||\bm{W} \bm{y}_{j}+\bm{b}||_{2}\big{)}.\]

Let \(\sigma\) be a softmax function that transforms a similarity matrix into a probability distribution,

\[\sigma(\bm{S},\tau)_{ij}\coloneqq\frac{\exp(\bm{S}_{ij}/\tau)}{\sum_{k}^{m} \mathbbm{1}_{\left\{k\neq j\right\}}\exp(\bm{S}_{ik}/\tau)},\]where \(\tau\) is a temperature and \(\sigma(\bm{S},\tau)_{ij}\in(0,1)\). We can then define the local loss as the following contrastive objective between untransformed and transformed neural network similarity spaces,

\[\mathcal{L}_{\text{local}}(\bm{W},\bm{b},\tau)\coloneqq-\frac{1}{m^{2}-m}\sum_{i }^{m}\sum_{j}^{m}\mathbbm{1}_{[i\neq j]}\sigma(\bm{S}^{*},\tau)_{ij}\log\left[ \sigma(\bm{S}^{\dagger},\tau)_{ij}\right].\] (5)

To avoid distributions that excessively emphasize the self-similarity of objects for small \(\tau\), we exclude elements on the diagonal of the similarity matrices. The final _gLocal transform_ is then,

\[\operatorname*{arg\,min}_{\bm{W},\bm{b}}\ \underbrace{(1-\alpha)\,\mathcal{L}_{ \text{global}}(\bm{W},\bm{b})}_{\text{alignment}}+\underbrace{\alpha\mathcal{L} _{\text{local}}(\bm{W},\bm{b},\tau)}_{\text{locality-preserving}}+\lambda\left\| \bm{W}-\left(\sum_{j=1}^{p}\bm{W}_{jj}/p\right)I\right\|_{\text{F}}^{2},\] (6)

where \(\alpha\) is a hyperparameter that balances the trade-off between human alignment and preserving the local structure of a neural network's original representation space. We select values of \(\alpha\) and \(\lambda\) that give the lowest alignment loss via grid search; see details in Appx. A.2.

### Downstream tasks

**Few-shot learning.** In general, few-shot learning (FS) is used to measure the transferability of neural network representations to different downstream tasks [86]. Here, we use FS to investigate whether the gLocal transform, as defined in SS3, can improve downstream task performance and, hence, a network's transferability, compared to the original representation spaces. Specifically, we perform FS with and without applying the gLocal transforms. For all few-shot experiments, we use multinomial logistic regression, which has previously been shown to achieve near-optimal performance when paired with a good representation [88]. The regularization parameter is selected by \(n_{s}\)-fold cross-validation, with \(n_{s}\) being the number of shots per class (more details in Appx. A.3).

**Anomaly detection.** Anomaly detection (AD) is a task where one has a collection of data considered "nominal" and would like to detect if a test sample is different from nominal data. For semantic AD tasks, e.g. nominal images contain a cat, it has been observed that simple AD methods using a pretrained neural network perform best [53; 4; 16]. In this work, we apply \(k\)-nearest neighbor AD to representations from a neural network, a method which has been found to work well [4; 53; 71]. We use the standard one-vs.-rest AD benchmark on classification datasets, where a model is trained using "one" training class as nominal data and performs inference with the full test set with the "rest" classes being anomalous [75].

## 4 Experimental results

In this section, we report experimental results for different FS and AD tasks. In addition, we analyze the effect of the gLocal transforms on both local and global similarity structures and report changes in representational alignment of image/text models for four human similarity judgment datasets. We start this section by introducing the different tasks and datasets and continue with the analyses.

### Experimental setup

**CIFAR-100 coarse** The 100 classes in CIFAR-100 can be grouped into 20 semantically meaningful superclasses. Here, we use these superclasses as targets for which there exist 100 test images each.

**CIFAR-100 shift** simulates a distribution shift between the normal distribution of the training and testing images in CIFAR-100. For each of the 20 superclasses, there exist five subclasses. We use the first three subclasses for training and the last two subclasses for testing.

**Entity-13 and Entity-30** are datasets derived from ImageNet [17; 76]. They have been defined as part of the BREEDS dataset for subpopulation shift [77]. In BREEDS, ImageNet classes are grouped into superclasses based on a modified version of the WordNet hierarchy. Specifically, one starts at the _Entity_ node of that hierarchy and traverses the tree in a breadth-first manner until the desired level of granularity (three steps from the root for Entity-13 and four for Entity 30). The classes residing at that level are considered the new coarse class labels and a fixed number of subclasses are sampled from the trees rooted at these superclass nodes -- twenty for Entity-13 and eight for Entity-30. Through this procedure, Entity-13 results in more coarse-grained labels than Entity-30. The subclasses of each superclass are partitioned into source (training) and target (test) classes, introducing a subpopulation shift. For testing, we use all 50 validation images for each subclass.

### Impact of transforms on global and local structure

Our goal is to use human similarity judgments to correct networks' global representational structure without affecting local representational structure. Here, we study the extent to which our method succeeds at this goal. To quantify distortion of local structure, we first find the nearest neighbor of each ImageNet validation image among the remaining validation 49,999 images in the original representation space. We then measure the proportion of images for which the nearest neighbor in the untransformed representation space is among the closest \(k\) images in the transformed representation space. As shown in Fig. 2, both the gLocal and global transforms generally preserve nearest neighbors, although the gLocal transform is slightly more effective, preserving the closest neighbor of 76.3% of images vs. 73.7% for the global transform. By contrast, the naive, unregularized transform preserves the closest neighbor in only 12.2% of images. For further intuition, we show the neighbors of anchor images in the untransformed, gLocal, and naively transformed representation spaces in Appx. C.

Whereas the local structure of gLocal and global representations closely resembles that of the original representation, the global structure instead more closely resembles the naive transformed representation. We quantify global representational similarity using linear centered kernel alignment (LCKA) [43; 14]. LCKA can be thought of as measuring the similarity between principal components (PCs) of two representations weighted by the amount of variance they explain; see further discussion in Appx. G. It thus primarily reflects similarity between the large PCs that define global representational structure. As shown in Fig. 3 (left), LCKA indicates that the gLocal/global representations are more similar to the naive transformed representation than to the untransformed representation, suggesting that the gLocal, global, and naive transforms induce similar changes in global structure. We further measure LCKA between representations obtained by setting all singular values to zero except for those corresponding to either largest 10 PCs or the remaining 758 PCs. LCKA between representations retaining the largest 10 PCs resembles LCKA between the full representations (Fig. 3 middle). However, when retaining only the remaining PCs, the gLocal/global representations are more similar to the untransformed representation than to the naive-transformed representation (Fig. 3 right).

Figure 4: The gLocal transformed representation captures global structure of the naive transformed representation, as shown by PCA, but local structure of the untransformed representation, as shown by t-SNE with perplexity 10. Visualizations reflect embeddings of 10 images from each of the 260 Entity-13 ImageNet subclasses obtained from CLIP ViT-L and are colored by the superclasses. 2D embeddings are related to align with the gLocal representation using orthogonal Procrustes. The t-SNE fitting process is initialized using PCA; we pick the embedding with the lowest loss from 10 runs.

Figure 3: The top principal components (PCs) of gLocal and global representations of ViT-L on the ImageNet validation set resemble those of the naive transformed representation, indicating that they share similar global structure. The remaining PCs more closely match the untransformed representation. **Left:** CKA between full representations. **Middle:** CKA after setting singular values of each representation to zero for all but the largest 10 PCs. **Right:** CKA after setting singular values to zero for the largest 10 PCs, but retaining smaller PCs.

Figure 2: gLocal and global but not naive transforms preserve nearest neighbors in CLIP ViT-L representations. y-axis indicates the percentage of ImageNet validation images for which the closest image in the untransformed space is among the \(k\) closest after transformation.

In Fig. 4, we visualize the effects of different transformations using PCA, which preserves global structure, and t-SNE, which preserves local structure. As described by Van der Maaten & Hinton [89], PCA "focus[es] on keeping the low-dimensional representations of dissimilar datapoints far apart," whereas t-SNE tries to "keep the low-dimensional representations of very similar datapoints close together." In line with the results above, the global structure of the gLocal representation revealed by PCA closely resembles that of the naive transformed representation, whereas the local structure of the gLocal representation revealed by t-SNE closely resembles that of the untransformed representation.

To complement these analyses, in Appx. B we explore what the alignment transforms alter about the global category structure of the model representations. Generally, categories within a single superordinate category move more closely together, while different superordinate categories move apart, but with sensible exceptions -- e.g. food and drink move closer together.

### Few-shot learning

In this section, we examine the impact of the gLocal transform on few-shot classification accuracy on downstream datasets. We consider a standard _fine-grained_ few-shot learning setup on CIFAR-100 [48], SUN397 [90], and the Describable Textures Dataset [DTD, 12], as well as a _coarse-grained_ setup on Entity-{13,30} of BREEDS [77].

In coarse-grained FS, classes are grouped into semantically meaningful superclasses. This is a more challenging setting than the standard fine-grained scenario, for which there does not exist a superordinate grouping. In fine-grained FS, training examples are uniformly drawn from all (sub-)classes. For coarse-grained FS, we classify according to superclasses rather than fine-grained classes, and choose \(k\) training examples uniformly at random for each superclass. This implies that not every subclass is contained in the train set if the number of training samples is smaller than the number of subclasses. On Entity-{13,30}, superclasses between train and test sets are guaranteed to be disjoint due to a subpopulation shift. To achieve high accuracy, models must consider examples from unseen members of a superclass similar to the few examples it has seen. Task performance is dependent on how well the partial information contained in the few examples of a superclass can be exploited by a model to characterize the entire superclass with its subclasses. Hence, global similarity structure is more crucial than local similarity structure to perform well on this task. We calculate classification accuracy across all available test images of all subclasses, using coarse labels.

We find that transforming the representations via gLocal transforms substantially improves performance over the untransformed representations across both coarse-grained and fine-grained FS tasks for all image/text models considered (Tab. 1). For CLIP models trained on LAION, however, we do not observe improvements for CIFAR-100 and DTD, which are the most fine-grained datasets. For ImageNet models, the gLocal transform improves performance on Entity-{13,30}, but has almost no impact on the performance for the remaining datasets (see Appx. D.1).

### Anomaly detection

Here, we evaluate the performance of representations on \(k\)-nearest neighbor AD with and without using the gLocal transform. AD methods typically return an anomaly score for which a detection threshold has to be chosen. Our method is evaluated using each training class as the nominal class and we report the average AUROC. Following SS3.1, we compute the representations for each normal sample of the training set and then evaluate the model with representations from the test set. We set \(k=5\) for our experiments but found that performance is fairly insensitive to the choice of \(k\) (see Appx. D.2). For measuring the distance between representations, we use cosine similarity.

In Tab. 2, we show that the gLocal transform substantially improves AD performance across all datasets considered in our analyses, for almost every image/text model. However, as in the few-shot setting, we observe no improvements over the untransformed representation space for ImageNet models (see Appx. D.2). In Tab. 3, we further investigate performance on distribution shift datasets,

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c|c} \hline \hline \multirow{2}{*}{Model \textbackslash{}Transform} & \multicolumn{2}{c|}{Entity-13} & \multicolumn{2}{c|}{Entity-30} & \multicolumn{2}{c|}{CIFAR100-Course} & \multicolumn{2}{c|}{CIFAR100} & \multicolumn{2}{c}{SUN397} & \multicolumn{2}{c}{DTD} \\  & original & gLocal & original & gLocal & original & gLocal & original & gLocal & original & gLocal & original & gLocal \\ \hline CLIP-RN50 (WT) & 63.99 & **67.36** & 57.86 & **59.80** & 44.27 & **47.43** & 38.77 & **39.59** & 57.21 & **68.79** & 54.32 & **54.95** \\ CLIP-VL144 (WT) & 65.34 & **71.94** & 66.92 & **69.97** & 66.67 & **68.48** & 72.22 & **73.03** & 69.87 & **71.13** & 62.84 & **63.27** \\ CLIP-VL14 (LAION-400R) & 63.33 & **69.02** & 62.93 & **65.99** & 66.88 & **69.58** & **73.57** & 72.98 & 70.25 & **71.08** & **67.61** & 66.71 \\ CLIP-VIF-L14 (LAION-28B) & 65.98 & **71.24** & 65.64 & **67.93** & 72.43 & **73.48\({}^{\dagger}\)** & **79.01\({}^{\dagger}\)** & 78.48 & 71.62 & **72.62\({}^{\dagger}\)** & **69.05\({}^{\dagger}\)** & 68.44 \\ \hline \hline \end{tabular}
\end{table}
Table 1: 5-shot FS results using the original or transformed representations. \(\dagger\) indicates the highest accuracy for each dataset. Results are averaged over 5 runs.

where improvements are particularly striking. Here, global similarity structure appears to be crucial for generalizing between the superclasses. Tab. 2 additionally reports current state-of-the-art (SOTA) results on the standard benchmarks; SOTA results are not available for the distribution shift datasets. SOTA approaches generally use additional data relevant to the AD task, such as outlier exposure data or textual supervision for the normal class [53; 13; 36], whereas we use only human similarity judgments. Our transformation also works well in non-standard AD settings (see Appx. D.2).

### Representational alignment

We have shown above that the gLocal transform provides performance gains on FS and AD tasks. Here, we examine whether these performance gains come at the cost of alignment with human similarity judgments as compared to the naive transform, which does not preserve local structure. Thus, we examine the impact of the gLocal transform on human alignment, using the same human similarity judgment datasets evaluated in Mutenthaler et al. [61] plus an additional dataset.4 Specifically, we perform representational similarity analysis [RSA; 46] between representations of two image/text models -- CLIP RN50 and CLIP ViT-L/14 -- and human behavior for four different human similarity judgment datasets [31; 65; 66; 10; 41]. RSA is a method for comparing neural network representations to representations obtained from human behavior [46]. In RSA, one first obtains representational similarity matrices (RSMs) for the human behavioral judgments and for the neural network representations (more details in Appx. E). These RSMs measure the similarity between pairs of examples according to each source. As in previous work [10; 41; 61], we use the Spearman rank correlation coefficient to quantify the similarity of these RSMs. We find that there is almost no trade-off in representational alignment for the gLocal transform compared to the naively transformed representations (see Tab. 4). Hence, the gLocal transform can improve representational alignment while preserving local similarity structure.

Footnote 4: Human similarity judgments were collected by either asking participants to arrange natural object images from different categories on a computer screen [66; 10; 41] or in the form of triplet odd-one-out choices [31].

In Fig. 5, we further demonstrate how the representational changes introduced by the linear and gLocal transforms lead to greater alignment with human similarity judgments by visualizing the RSMs on each dataset. The global similarity structure captured by the RSMs is qualitatively identical between naive and gLocal transforms, and both of these transforms lead to RSMs that closely resemble human RSMs (see Fig. 5). Human similarity judgments for data from Hebart et al. [31] were collected in the form of triplet odd-one-out choices. Therefore, we used VICE [60] -- an approximate Bayesian method for inferring mental representations of object concepts from human behavior -- to obtain an RSM for those judgments. Human RSMs are sorted into five different concept clusters using \(k\)-means for datasets from King et al. [41] and Cichy et al. [10] and using the things concept hierarchy [30] for RSMs from Hebart et al. [31]. A visualization for RSMs obtained from CLIP RN50 representations and a more detailed discussion on RSA can be found in Appx. E.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c} \hline \multirow{2}{*}{Model \(\backslash\) Transform} & \multicolumn{2}{c|}{CIFAR10} & \multicolumn{2}{c|}{CIFAR100} & \multicolumn{2}{c|}{CIFAR100-Course} & \multicolumn{2}{c}{ImageNet30} & \multicolumn{2}{c}{DTD} \\  & original & gLocal & original & gLocal & original & gLocal & original & gLocal & original & gLocal \\ \hline CLIP-RN50 (WIT) & 89.44 & **91.9**\(\downarrow\) & 90.83 & **92.92** & 86.47 & **89.29** & 98.98 & **90.95** & 90.67 & **92.29** \\ CLIP-ViT-L/14 (A-VIT) & 95.14 & **98.16**\(\downarrow\) & 91.41 & **97.19** & 85.85 & **95.83** & 98.91 & **97.85\(\uparrow\)** & 92.02 & **94.9** \\ CLIP-ViT-L/14 (A-LION-400M) & **98.8** & 95.39 & **98.66** & 98.53 & **97.86** & 97.77 & 99.51 & **99.69** & 95.54 & **96.41** \\ CLIP-ViT-L/14 (A-LION-28) & 95.97 & **99.11**\(\uparrow\) & 98.76 & **98.97\(\uparrow\)** & 98.05 & **98.5\(\uparrow\)** & 99.29 & **99.74** & 94.87 & **96.78\(\uparrow\)** \\ \hline SOTA & 99.6 [53] & - & - & 97.34 [13] & 99.9 [53] & 94.6 [53] \\ \hline \end{tabular}
\end{table}
Table 2: One-vs-rest nearest neighbor based AD results; with and without transformation. \(\uparrow\) indicates the highest accuracy for each dataset.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c} \hline \multirow{2}{*}{Model \(\backslash\) Transform} & \multicolumn{2}{c|}{Entity-13} & \multicolumn{2}{c|}{Entity-30} & \multicolumn{2}{c}{Living-17} & \multicolumn{2}{c}{Nonliving-26} & \multicolumn{2}{c}{CIFAR100-high} \\  & original & gLocal & original & gLocal & original & gLocal & original & gLocal & original & gLocal \\ \hline CLIP-RN50 (WIT) & 90.22 & **92.03** & 91.64 & **93.71** & **94.62** & 93.28 & 87.49 & **91.09** & 76.27 & **80.33** \\ CLIP-ViT-L/14 (WIT) & 88.54 & **93.21** & 91.31 & **95.63** & 97.31 & 97.27 & 87.14 & **92.53** & 73.60 & **87.11** \\ CLIP-ViT-L/14 (A-LION-400M) & 90.79 & **92.71** & 92.49 & **95.04** & **96.56** & 96.32 & 90.09 & **93.18** & 91.09 & **92.7** \\ CLIP-ViT-L/14 (A-LION-28) & 90.33 & **93.08** & 92.1 & **95.84\(\uparrow\)** & 96.96 & **97.37** & 88.82 & **93.54\(\uparrow\)** & 89.73 & **93.04\(\uparrow\)** \\ \hline \end{tabular}
\end{table}
Table 3: One-vs-rest AD with a class distribution shift between train and test sets; with and without transformation. \(\uparrow\) indicates the highest accuracy for each dataset.

## 5 Discussion

Although neural networks achieve near-human-level performance on a variety of computer vision tasks, they may not optimally capture _global_ object similarities. By contrast, humans represent concepts using rich semantic features -- including superordinate categories and other global constraints -- for performing object similarity judgments [66; 41; 10; 31; 60]. These representations may contribute to humans' strong generalization capabilities [21; 24]. Here, we investigated the impact of aligning neural network representations with human similarity judgments on different FS and AD tasks.

We find that naively aligning neural network representations, without regularizing the learned transformations to preserve structure in the original representation space, can impair downstream task performance. However, our _gLocal transform_, which combines an _alignment loss_ that optimizes for representational alignment with a _local loss_ that preserves the nearest neighbor structure from the original representation, can substantially improve downstream task performance while increasing representational alignment. The transformed representation space transfers surprisingly well across different human similarity judgment datasets and achieves almost equally strong alignment as the naive approach [61], indicating that it captures human notions of object similarity. In addition, it substantially improves downstream task performance compared to both original and naively aligned representations across a variety of FS and AD tasks. The gLocal transform yields state-of-the-art (SOTA) performance on the CIFAR-100 coarse AD task, and approaches SOTA on other AD benchmarks.

**Ablations.** In addition, we show that the gLocal transform can readily be used in combination with other approaches that are specifically designed for few-shot learning with pretrained representations of image/text models -- such as Tip-Adapter [93]. We observe considerable improvements compared to using Tip-Adapter without applying the gLocal transform (see Appx. D.1). This indicates that

\begin{table}
\begin{tabular}{l c c c|c c c|c c c} \hline \hline  & \multicolumn{3}{c|}{CLIP-RN50 (WIT)} & \multicolumn{3}{c|}{CLIP-ViT-L/14 (WIT)} & \multicolumn{3}{c}{CLIP-ViT-L/14 (LAION-2B)} \\ Dataset \(\backslash\) Transform & original & naive & gLocal & original & naive & gLocal & original & naive & gLocal \\ \hline _Human similarity datasets:_ & & & & & & & & & \\ Hebart et al. [31] & 52.78\% & 59.92\% & 59.89\% & 46.71\% & 60.13\% & 60.05\% & 47.50\% & 60.47\% & 60.38\% \\ King et al. [41] & 0.386 & 0.650 & 0.645 & 0.355 & 0.638 & 0.637 & 0.292 & 0.620 & 0.613 \\ Cichy et al. [10] & 0.557 & 0.721 & 0.716 & 0.363 & 0.732 & 0.732 & 0.395 & 0.735 & 0.718 \\ Peterson et al. [65; 66] & 0.364 & 0.701 & 0.705 & 0.260 & 0.688 & 0.688 & 0.314 & 0.689 & 0.660 \\ \hline _Dunstream task:_ & & & & & & & & & \\ \(5\)-shot FS (avg.) & 53.05\% & 41.20\% & **55.01\%** & 67.71\% & 47.62\% & **69.91\%** & 71.10\% & 50.71\% & **72.27\%** \\ Anomaly detection (avg.) & 89.65\% & 90.75\% & **91.52\%** & 90.16\% & 92.79\% & **95.19\%** & 94.79\% & 93.09\% & **96.65\%** \\ \hline \hline \end{tabular}
\end{table}
Table 4: The gLocal transform yields both a high degree of alignment with datasets of human similarity judgments and good performance on FS/AD tasks. Performance on human similarity datasets is measured as odd-one-out accuracy on a held-out test set of things or Spearman’s \(\rho\) for the other three datasets, using either the original representations, the naively transformed representations, or representations transformed via the gLocal transform. For 5-shot FS and AD, we report the average performance across all tasks in Tab. (1, 2, 3).

Figure 5: RSMs for human behavior and CLIP ViT-L/14 [WIT; 70] for four different human similarity judgment datasets [31; 66; 10; 41]. We contrast RSMs obtained from the network’s original representations (second column), the naively aligned representations [61] (third column), and the representations + gLocal transform (rightmost column) against RDMs directly constructed from human similarity judgments (leftmost column). Yellower colors indicate greater similarity; bluer colors indicate greater dissimilarity.

methods designed to improve the transferability of pretrained representations without incorporating knowledge about human global similarity structure can additionally benefit from the gLocal transform. Using a human-adversarial triplet dataset, where each odd-one-out choice is determined to be an object that is different from the human choice, for optimizing the gLocal transform either yields no change in a model's representation space and thus no difference in downstream task performance or changes a model's representation space in a way that substantially deteriorates its downstream task performance (see Appx. D.4). These observations corroborate our findings and suggest that the benefits of the gLocal transform most likely stem from including an inductive bias about global object similarity.

**Limitations.** Our work has some limitations. First, as we show in Appx. D, the gLocal transform fails to consistently improve downstream task performance on ImageNet. We conjecture that the gLocal transform can succeed only if representations capture the concepts by which human representations are organized, and ImageNet representations may not. Second, human similarity judgments are more expensive to acquire than other forms of supervision, and there may be important human concepts that are captured neither by the 1854 images we use for alignment nor by the pretrained representations.

**Conclusion.** Our results imply that even with hundreds of millions of image/text pairs, image/text contrastive learning does not learn a representation space with human-like global organization. However, since the gLocal transform successfully aligns contrastive representations' global structure using only a small number of images, these representations do seem to have a pre-existing representation of the concepts by which human representations are globally organized. Why does this happen? One possibility is that image/text pairs do not provide adequate global supervision, and thus contrastive representation learning is (near-)optimal given the data. Alternatively, contrastive learning may not incorporate signals that exist in the data into the learned representation because it imposes only local constraints. Previous work has shown that t-SNE and UMAP visualizations reflect global structure only if they are carefully initialized [42]. Given the similarity between contrastive representation learning and t-SNE/UMAP [15] and the known sensitivity of contrastive representations to initialization [52], it is plausible contrastive representations also inherit their global structure from their initialization. Although our gLocal transform provides a way to perform post-hoc alignment of representations from image/text models using human similarity judgments, there may be alternative initialization strategies or objectives that can provide the same benefits during training, using only image/text pairs.

## Acknowledgements

LM, LL, JD, and RV acknowledge funding from the German Federal Ministry of Education and Research (BMBF) for the grants BIFOLD22B and BIFOLD23B. LM acknowledges support through the Google Research Collabs Programme. We thank Pieter-Jan Kindermans for helpful comments on an earlier version of the manuscript.

## References

* [1] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* [2] Maria Attarian, Brett D Roads, and Michael C Mozer. Transforming neural network visual representations to predict human judgments of similarity. In _NeurIPS 2020 Workshop SVRHM_, 2020.
* [3] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In _Proceedings of the European conference on computer vision (ECCV)_, pp. 456-473, 2018.
* [4] Liron Bergman, Niv Cohen, and Yedid Hoshen. Deep nearest neighbor anomaly detection. _CoRR_, abs/2002.10445, 2020.
* [5] Alsallakh Bilal, Amin Jourabloo, Mao Ye, Xiaoming Liu, and Liu Ren. Do convolutional neural networks learn class hierarchy? _IEEE transactions on visualization and computer graphics_, 24(1):152-162, 2017.
* [6] Jeffrey S Bowers, Gaurav Malhotra, Marin Dujmovic, Milton Llera Montero, Christian Tsvetkov, Valerio Biscione, Guillermo Puebla, Federico Adolfi, John E Hummel, Rachel F Heaton, et al.

Deep problems with neural network models of human vision. _Behavioral and Brain Sciences_, pp. 1-74, 2022.
* Bracci and Beeck [2016] Stefania Bracci and Hans Op de Beeck. Dissociations and associations between shape and category representations in the two visual pathways. _Journal of Neuroscience_, 36(2):432-444, 2016.
* Caron et al. [2021] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pp. 9650-9660, October 2021.
* Chen et al. [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Hal Daume III and Aarti Singh (eds.), _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pp. 1597-1607. PMLR, 13-18 Jul 2020.
* Cichy et al. [2019] Radoslaw M. Cichy, Nikolaus Kriegeskorte, Kamila M. Jozwik, Jasper J.F. van den Bosch, and Ian Charest. The spatiotemporal neural dynamics underlying perceived similarity for real-world objects. _NeuroImage_, 194:12-24, 2019. ISSN 1053-8119. doi: https://doi.org/10.1016/j.neuroimage.2019.03.031.
* Cichy et al. [2014] Radoslaw Martin Cichy, Dimitrios Pantazis, and Aude Oliva. Resolving human object recognition in space and time. _Nature neuroscience_, 17(3):455-462, 2014.
* Cimpoi et al. [2014] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed,, and A. Vedaldi. Describing textures in the wild. In _Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2014.
* Cohen and Avidan [2022] Matan Jacob Cohen and Shai Avidan. Transformaly-two (feature spaces) are better than one. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 4060-4069, 2022.
* Cortes et al. [2012] Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based on centered alignment. _The Journal of Machine Learning Research_, 13(1):795-828, 2012.
* Damrich et al. [2022] Sebastian Damrich, Niklas Bohm, Fred A Hamprecht, and Dmitry Kobak. From \(t\)-sne to umap with contrastive learning. In _The Eleventh International Conference on Learning Representations_, 2022.
* Deecke et al. [2021] Lucas Deecke, Lukas Ruff, Robert A. Vandermeulen, and Hakan Bilen. Transfer-based semantic anomaly detection. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pp. 2546-2558. PMLR, 18-24 Jul 2021.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pp. 248-255, 2009. doi: 10.1109/CVPR.2009.5206848.
* Deng et al. [2010] Jia Deng, Alexander C Berg, Kai Li, and Li Fei-Fei. What does classifying more than 10,000 image categories tell us? In _Computer Vision-ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part V 11_, pp. 71-84. Springer, 2010.
* Donahue et al. [2014] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In _Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014_, volume 32 of _JMLR Workshop and Conference Proceedings_, pp. 647-655. JMLR.org, 2014.
* Fukuzawa et al. [1988] Kazuyoshi Fukuzawa, Motonobu Itoh, Sumiko Sasanuma, Tsutomu Suzuki, Yoko Fukusako, and Tohru Masui. Internal representations and the conceptual operation of color in pure alexia with color naming defects. _Brain and Language_, 34(1):98-126, 1988. ISSN 0093-934X. doi: https://doi.org/10.1016/0093-934X(88)90126-5.

* Geirhos et al. [2018] Robert Geirhos, Carlos R. M. Temme, Jonas Rauber, Heiko H. Schutt, Matthias Bethge, and Felix A. Wichmann. Generalisation in humans and deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* Geirhos et al. [2019] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In _7th International Conference on Learning Representations, ICLR 2019_. OpenReview.net, 2019.
* Geirhos et al. [2020] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A. Wichmann. Shortcut learning in deep neural networks. _Nature Machine Intelligence_, 2(11):665-673, November 2020. doi: 10.1038/s42256-020-00257-z.
* Geirhos et al. [2020] Robert Geirhos, Katharaju Narayanappa, Benjamin Mitzkus, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. On the surprising similarities between supervised and self-supervised models. In _NeurIPS 2020 Workshop SVRIM_, 2020.
* Geirhos et al. [2021] Robert Geirhos, Katharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Partial success in closing the gap between human and machine vision. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 23885-23899. Curran Associates, Inc., 2021.
* Geirhos et al. [2021] Robert Geirhos, Katharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Partial success in closing the gap between human and machine vision. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 23885-23899. Curran Associates, Inc., 2021.
* Grill et al. [2020] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2016.
* He et al. [2020] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised Visual Representation Learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2020.
* Hebart et al. [2019] Martin N. Hebart, Adam H. Dickter, Alexis Kidder, Wan Y. Kwok, Anna Corriveau, Caitlin Van Wicklin, and Chris I. Baker. THINGS: A database of 1,854 object concepts and more than 26,000 naturalistic object images. _PLOS ONE_, 14(10):1-24, 2019. doi: 10.1371/journal.pone.0223792.
* Hebart et al. [2020] Martin N. Hebart, Charles Y. Zheng, Francisco Pereira, and Chris I. Baker. Revealing the multi-dimensional mental representations of natural objects underlying human similarity judgements. _Nature Human Behaviour_, 4(11):1173-1185, October 2020. doi: 10.1038/s41562-020-00951-3.
* Hebart et al. [2023] Martin N Hebart, Oliver Contier, Lina Teichmann, Adam H Rockter, Charles Y Zheng, Alexis Kidder, Anna Corriveau, Maryam Vaziri-Pashkam, and Chris I Baker. Things-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior. _eLife_, 12:e82580, feb 2023. ISSN 2050-084X. doi: 10.7554/eLife.82580.
* Hermann and Lampinen [2020] Katherine Hermann and Andrew Lampinen. What shapes feature representations? exploring datasets, architectures, and training. _Advances in Neural Information Processing Systems_, 33:9995-10006, 2020.

* Hermann et al. [2020] Katherine Hermann, Ting Chen, and Simon Kornblith. The origins and prevalence of texture bias in convolutional neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 19000-19015. Curran Associates, Inc., 2020.
* Hinton and Roweis [2002] Geoffrey E. Hinton and Sam T. Roweis. Stochastic neighbor embedding. In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer (eds.), _Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British Columbia, Canada]_, pp. 833-840. MIT Press, 2002.
* Hu et al. [2022] Shell Xu Hu, Da Li, Jan Stuhmer, Minyoung Kim, and Timothy M. Hospedales. Pushing the limits of simple pipelines for few-shot learning: External data and fine-tuning make a difference. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 9058-9067, 2022. doi: 10.1109/CVPR52688.2022.00886.
* Huh et al. [2016] Mi-Young Huh, Pulkit Agrawal, and Alexei A. Efros. What makes ImageNet good for transfer learning? _CoRR_, abs/1608.08614, 2016.
* Jordan et al. [2015] Marius Catalin Jordan, Michelle R Greene, Diane M Beck, and Li Fei-Fei. Basic level category structure emerges gradually across human ventral visual cortex. _Journal of cognitive neuroscience_, 27(7):1427-1446, 2015.
* Jia et al. [2021] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pp. 4904-4916. PMLR, 18-24 Jul 2021.
* Khaligh-Razavi and Kriegeskorte [2014] Seyed-Mahdi Khaligh-Razavi and Nikolaus Kriegeskorte. Deep supervised, but not unsupervised, models may explain it cortical representation. _PLoS computational biology_, 10(11):e1003915, 2014.
* King et al. [2019] Marcie L. King, Iris I.A. Groen, Adam Steel, Dwight J. Kravitz, and Chris I. Baker. Similarity judgments and cortical visual responses reflect different properties of object and scene categories in naturalistic images. _NeuroImage_, 197:368-382, 2019. ISSN 1053-8119. doi: https://doi.org/10.1016/j.neuroimage.2019.04.079.
* Kobak and Linderman [2021] Dmitry Kobak and George C Linderman. Initialization is critical for preserving global data structure in both t-sne and umap. _Nature biotechnology_, 39(2):156-157, 2021.
* Kornblith et al. [2019] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. Similarity of neural network representations revisited. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pp. 3519-3529. PMLR, 2019.
* Kornblith et al. [2019] Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do Better ImageNet Models Transfer Better? In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019_, pp. 2661-2671. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00277.
* Kornblith et al. [2021] Simon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi. Why Do Better Loss Functions Lead to Less Transferable Features? In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, volume 34, pp. 28648-28662, 2021.
* connecting the branches of systems neuroscience. _Frontiers in Systems Neuroscience_, 2, 2008. ISSN 1662-5137. doi: 10.3389/neuro.06.004.2008.
* Krizhevsky [2014] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. _arXiv e-prints_, art. arXiv:1404.5997, April 2014.
* Krizhevsky and Hinton [2009] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009.
* Kumar et al. [2022] Manoj Kumar, Neil Houlsby, Nal Kalchbrenner, and Ekin D Cubuk. Do better ImageNet classifiers assess perceptual similarity better? _Transactions on Machine Learning Research_, 2022.
* Lapuschkin et al. [2019] Sebastian Lapuschkin, Stephan Waldchen, Alexander Binder, Gregoire Montavon, Wojciech Samek, and Klaus-Robert Muller. Unmasking clever hans predictors and assessing what machines really learn. _Nature communications_, 10(1):1096, 2019.
* Li et al. [2019] Aoxue Li, Tiange Luo, Zhiwu Lu, Tao Xiang, and Liwei Wang. Large-scale few-shot learning: Knowledge transfer with class hierarchy. In _Proceedings of the ieee/cvf conference on computer vision and pattern recognition_, pp. 7212-7220, 2019.
* Liang et al. [2022] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. _Advances in Neural Information Processing Systems_, 35:17612-17625, 2022.
* Lizerski et al. [2022] Philipp Lizerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Klaus-Robert Muller, and Marius Kloft. Exposing outlier exposure: What can be learned from few, one and zero outlier images. _CoRR_, abs/2205.11474, 2022. doi: 10.48550/arXiv.2205.11474.
* Marjieh et al. [2022] Raja Marjieh, Pol van Rijn, Ilia Sucholutsky, Theodore R. Sumers, Harin Lee, Thomas L. Griffiths, and Nori Jacoby. Words are all you need? Capturing human sensory similarity with textual descriptors, 2022.
* Markman [1989] Ellen M Markman. _Categorization and naming in children: Problems of induction_. mit Press, 1989.
* McClelland et al. [2017] JL McClelland, Z Sadeghi, and AM Saxe. A critique of pure hierarchy: Uncovering cross-cutting structure in a natural dataset. In _Neurocomputational Models of Cognitive Development and Processing: Proceedings of the 14th Neural Computation and Psychology Workshop_, pp. 51-68. World Scientific, 2017.
* McCoy et al. [2019] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. _arXiv preprint arXiv:1902.01007_, 2019.
* Mitrovic et al. [2021] Jovana Mitrovic, Brian McWilliams, Jacob C Walker, Lars Holger Buesing, and Charles Blundell. Representation learning via invariant causal mechanisms. In _International Conference on Learning Representations_, 2021.
* Mutenthaler and Hebart [2021] Lukas Mutenthaler and Martin N. Hebart. Thingsvision: A python toolbox for streamlining the extraction of activations from deep neural networks. _Frontiers in Neuroinformatics_, 15:45, 2021. ISSN 1662-5196. doi: 10.3389/fninf.2021.679838.
* Mutenthaler et al. [2022] Lukas Mutenthaler, Charles Y Zheng, Patrick McClure, Robert A Vandermeulen, Martin N Hebart, and Francisco Pereira. VICE: Variational Interpretable Concept Embeddings. _Advances in Neural Information Processing Systems_, 35:33661-33675, 2022.
* Mutenthaler et al. [2023] Lukas Mutenthaler, Jonas Dippel, Lorenz Linhardt, Robert A Vandermeulen, and Simon Kornblith. Human alignment of neural network representations. In _11th International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, Mai 01-05, 2023_. OpenReview.net, 2023.
* Paccanaro and Hinton [2001] A. Paccanaro and G.E. Hinton. Learning distributed representations of concepts using linear relational embedding. _IEEE Transactions on Knowledge and Data Engineering_, 13(2):232-244, 2001. doi: 10.1109/69.917563.

* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pp. 8024-8035, 2019.
* Pedregosa et al. [2011] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. _Journal of machine learning research_, 12(Oct):2825-2830, 2011.
* Peterson et al. [2016] Joshua C. Peterson, Joshua T. Abbott, and Thomas L. Griffiths. Adapting deep network features to capture psychological representations. In Anna Papafragou, Daniel Grodner, Daniel Mirman, and John C. Trueswell (eds.), _Proceedings of the 38th Annual Meeting of the Cognitive Science Society, Recogbizing and Representing Events, CogSci 2016, Philadelphia, PA, USA, August 10-13, 2016_. cognitivesciencesociety.org, 2016.
* Peterson et al. [2018] Joshua C. Peterson, Joshua T. Abbott, and Thomas L. Griffiths. Evaluating (and improving) the correspondence between Deep Neural Networks and Human Representations. _Cogn. Sci._, 42(8):2648-2669, 2018. doi: 10.1111/cogs.12670.
* November 2, 2019_, pp. 9616-9625. IEEE, 2019. doi: 10.1109/ICCV.2019.00971.
* Pham et al. [2022] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, and Quoc V. Le. Combined scaling for open-vocabulary image classification. _arXiv e-prints_, art. arXiv:2111.10050, 2022.
* Proklova et al. [2016] Daria Proklova, Daniel Kaiser, and Marius V Peelen. Disentangling representations of object shape and object category in human visual cortex: The animate-inanimate distinction. _Journal of cognitive neuroscience_, 28(5):680-692, 2016.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pp. 8748-8763. PMLR, 18-24 Jul 2021.
* Reiss et al. [2021] Tal Reiss, Niv Cohen, Liron Bergman, and Yedid Hoshen. Panda: Adapting pretrained features for anomaly detection and segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 2806-2814, June 2021.
* Robilotto and Zaidi [2004] Rocco Robilotto and Qasim Zaidi. Limits of lightness identification for real objects under natural viewing conditions. _Journal of Vision_, 4(9):9-9, 09 2004. ISSN 1534-7362. doi: 10.1167/4.9.9.
* Rogers et al. [2004] Timothy T Rogers, James L McClelland, et al. _Semantic cognition: A parallel distributed processing approach_. MIT press, 2004.
* Rosch et al. [1976] Eleanor Rosch, Carolyn B Mervis, Wayne D Gray, David M Johnson, and Penny Boyes-Braem. Basic objects in natural categories. _Cognitive psychology_, 8(3):382-439, 1976.
* Ruff et al. [2018] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In _International conference on machine learning_, pp. 4393-4402. PMLR, 2018.

* Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li. ImageNet large scale visual recognition challenge. _Int. J. Comput. Vis._, 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
* Santurkar et al. [2020] Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation shift. _arXiv preprint arXiv:2008.04859_, 2020.
* Saxe et al. [2013] Andrew M Saxe, James L McClellans, and Surya Ganguli. Learning hierarchical categories in deep neural networks. In _Proceedings of the Annual Meeting of the Cognitive Science Society_, volume 35, 2013.
* Saxe et al. [2019] Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic development in deep neural networks. _Proceedings of the National Academy of Sciences_, 116(23):11537-11546, 2019.
* Schrimpf et al. [2020] Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J. Majaj, Rishi Rajalingham, Elias B. Issa, Kohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, Kailyn Schmidt, Daniel L. K. Yamins, and James J. DiCarlo. Brain-score: Which artificial neural network for object recognition is most brain-like? _bioRxiv_, 2020. doi: 10.1101/407007.
* Schuhmann et al. [2021] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset of clip-filtered 400 million image-text pairs. _CoRR_, abs/2111.02114, 2021.
* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an open large-scale dataset for training next generation image-text models. In _NeurIPS_, 2022.
* Schwartz et al. [2022] Eli Schwartz, Leonid Karlinsky, Rogerio Feris, Raja Giryes, and Alex Bronstein. Baby steps towards few-shot learning with multiple semantics. _Pattern Recognition Letters_, 160:142-147, 2022.
* Shafto et al. [2006] Patrick Shafto, Charles Kemp, Vikash Mansinghka, Matthew Gordon, and Joshua B Tenenbaum. Learning cross-cutting systems of categories. In _Proceedings of the 28th annual conference of the Cognitive Science Society_, volume 2146, pp. 2151, 2006.
* Simonyan and Zisserman [2015] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun (eds.), _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015.
* Snell et al. [2017] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Sucholutsky and Griffiths [2023] Ilia Sucholutsky and Thomas L Griffiths. Alignment with human representations supports robust few-shot learning. _arXiv preprint arXiv:2301.11990_, 2023.
* Tian et al. [2020] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking few-shot image classification: a good embedding is all you need? In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIV 16_, pp. 266-282. Springer, 2020.
* Van der Maaten and Hinton [2008] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* Xiao et al. [2010] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 3485-3492. IEEE, 2010.

* Xiao et al. [2020] Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. _arXiv preprint arXiv:2006.09994_, 2020.
* Yamins et al. [2014] Daniel L. K. Yamins, Ha Hong, Charles F. Cadieu, Ethan A. Solomon, Darren Seibert, and James J. DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex. _Proceedings of the National Academy of Sciences_, 111(23):8619-8624, 2014. doi: 10.1073/pnas.1403112111.
* ECCV 2022_, pp. 493-510, Cham, 2022. Springer Nature Switzerland. ISBN 978-3-031-19833-5.
Experimental details

### Model features

We extract penultimate layer features of four different ImageNet-models -- AlexNet [47], VGG-16 [85], ResNet-18, and ResNet-50 [28] -- and image encoder features of four different image/text models -- CLIP RN50 and CLIP ViT-L/14 trained on WIT [70]; CLIP ViT-L/14 trained on Laion-400M [81] and Laion-2B [82] respectively. For extracting the model features, we use the Python library thingsvision[59].

### gLocal probing

To optimize the gLocal transforms, we use standard SGD with momentum and perform cross-validation according to the procedure proposed in Muttenthaler et al. [61]. For finding the optimal gLocal transform, we perform an extensive grid search over four different hyperparameter values -- the learning rate, \(\eta\), the strength of the regularization term \(\lambda\), the global-local trade-off parameter \(\alpha\), and the temperature parameter, \(\tau\), used in the softmax expression for the local contrastive loss term (see Eq. 5). Specifically, we perform an extensive grid search over the Cartesian product of the following sets of hyperparameters:

* \(\eta\in\{0.0001,0.001,0.01,0.1\}\),
* \(\lambda\in\{0.01,0.1,1.0,10.0\}\),
* \(\alpha\in\{0.05,0.1,0.25,0.5,1.0\}\),
* \(\tau\in\{0.1,0.25,0.5,1.0\}\).

We use the same \(\eta\) and \(\lambda\) grids for global probing. We use PyTorch[63] for implementing the probes and PyTorch lightning to accelerate training. We choose the gLocal transform that achieves the lowest alignment loss (see alignment term in Eq. 6). Among the values in the above grid, we find that a combination of \((\alpha=0.1,\lambda=0.1,\eta=0.001)\) yields the lowest alignment loss/highest probing odd-one-out accuracy for both CLIP RN50 (WIT) and CLIP ViT-L/14 (WIT) (see Fig. 6). A combination of \((\alpha=0.25,\lambda=0.1,\eta=0.001)\) gives the second lowest alignment loss/highest probing odd-one-out accuracy for CLIP RN50 (WIT) and CLIP ViT-L/14 (WIT).

For each \((\alpha,\lambda)\) combination we select that combination with the best probing odd-one-out accuracy on a held-out test among the set of possible learning rate, \(\eta\), and temperature value, \(\tau\), combinations determined by the above grid. We observe that \(\eta=0.001\) generally gives the best results across the different \((\alpha,\lambda)\) combinations, whereas performance is fairly insensitive to the value of \(\tau\). Since neither \((\alpha=0.1,\lambda=0.1)\) nor \((\alpha=0.25,\lambda=0.1)\) are values at the edges of the hyperparameter

Figure 6: Among all hyperparameter combinations considered in our grid search, a combination of (\(\alpha=0.1,\lambda=0.1,\eta=0.001\)) for Eq. 6 in §3 yields the best odd-one-out accuracy on a held-out test set for both CLIP RN50 and CLIP ViT-L/14 (WIT).

grid, it is plausible to assume that both the contrastive local loss and the regularization term in Eq. 6 in SS3 are necessary to obtain a transformation that leads to a _best-of-both-worlds_ representation.

Although our goal has been to find a transform that induces both increased representational alignment and improved downstream task performance, we considered \(\alpha=1.0\) to examine whether downstream task performance can potentially be improved by excluding the alignment loss. Note that \(\alpha=1.0\) causes the optimization process to ignore the alignment loss. Unsurprisingly we did not find that to be the case. We remark that minimizing both the local contrastive loss and the regularization preserves the local similarity structure of the original representation space but does not inject any additional information into the representations. Moreover, it is non-trivial to choose a transform that works well across all downstream tasks without including the alignment loss. Therefore, we exclude \(\alpha=1.0\) in Fig. 6.

**Compute**. We used a compute time of approximately 400 hours on a single Nvidia A100 GPU with 40GB VRAM for all linear probing experiments -- including the hyperparameter sweep. The computations were performed on a standard, large-scale academic SLURM cluster.

### Few-shot learning

Here, we use \(n_{s}\)-fold cross-validation for finding the optimal \(\ell_{2}\)-regularization parameter, where \(n_{s}\) refers to the number of shots per class. We select the parameter from the following set of values, \(\{1\mathrm{e}+6,1,1\mathrm{e}+5,1\mathrm{e}+4,1\mathrm{e}+3,1,1\mathrm{e}+2, 1\mathrm{e}+1,1,1\mathrm{e}-1,1\mathrm{e}-2,1\mathrm{e}-3,1\mathrm{e}-4\}\). We use the scikit-learn[64] implementation of (multinomial) logistic regression and refit the regression after selecting the optimal regularization parameter.

**Compute**. We used a compute time of approximately 5600 CPU-hours of 2.90GHz Intel Xeon Gold 6326 CPUs for all few-shot experiments. Computations were performed on a standard, large-scale academic SLURM cluster.

### Anomaly Detection

In this section, we outline our anomaly detection experimental setting in more detail. In the anomaly detection settings that we consider in our analyses _normal/anomaly_ classes are determined via the original classes in the data. Here, each of the original classes is once selected as a normal class with the remaining classes being anomalous and, vice versa, each class in the data is once selected as an anomalous class with the other classes being normal. After embedding the training images from either the normal or the anomalous class in a model's representation space, at inference time a model must classify whether a new image belongs to the normal data or whether it deviates from it and is thus considered an anomaly. For each example in the test set, a model yields an anomaly score where higher scores indicate more probability of an example being anomalous. Using the binary anomaly labels and the anomaly scores for each of the examples, we can then compute the _area-under-the-receiver-operating-characteristic-curve_ (AUROC) to quantify the performance of the model.

**One-vs-rest**. Given a dataset (e.g., CIFAR-10) with \(C\) classes, one class (e.g., "airplane") is chosen to be the normal class and the remaining \(C-1\) classes of the dataset are considered anomalies. Each of the \(C\) classes is once selected as a normal class and the AUROC is averaged across the classes.

**Leave-one-out (LOO)**. In contrast to the "one-vs-rest" setting, in LOO we define one class of the dataset as an anomaly and the remaining classes as normal. Similarly to the "one-vs-rest" setting, this results in \(C\) evaluations for a dataset with \(C\) classes.

In both "one-vs-rest" and LOO AD settings, we evaluate model representations in the following way: First, we compute the representations \(\bm{X}_{\text{train}}\) of the normal samples in the train set. Then, we compute the representations of all test set examples \(\bm{X}_{\text{test}}\). For each test set representation, we compute the cosine similarity to all normal train set representations, \(\bm{X}_{\text{train}}\), and select the \(k\) nearest neighbor samples that have the highest cosine similarity.

The anomaly score of a test set representation is then defined as the average cosine distance to the \(k\) nearest train representations. \(k\) is a hyperparameter that determines the number of nearest neighbors over which the anomaly score is computed. We choose \(k=5\) for our experiments but show that performance is fairly insensitive to the value of \(k\) (see Tab. 12).

[MISSING_PAGE_FAIL:20]

## Appendix C Visualization of neighboring images

To provide further insight into the difference between the effects of the naive and gLocal transforms, in Fig. 8 we visualize the neighbors of nine anchor images. In order to show a diverse set of images, we pick the nearest neighbors in the CLIP ViT-L/14 (WIT) embedding space subject to the constraint that each neighbor comes from a different class from the original images and the nearer neighbors. In accordance with the results in SS4.2, we find that the neighbors in the untransformed and gLocal spaces are generally similar, whereas neighbors in the naive representation space are frequently different. The naive transform appears to discard all non-semantic properties of images, whereas the untransformed and gLocal representation spaces are sensitive to pose, color, and numerosity. In cases where the closest neighbor differs between the naive and gLocal representations (third and fourth row), the neighbors in the gLocal representation are arguably better matches to the anchor.

Figure 7: How does the global structure of the representations change after alignment? Here, we analyze the movements of the representations of pairs of items from different superordinate categories from the THINGS dataset. The squares on the diagonal indicate the change in distance between items within a superordinate category, while the squares off the diagonal indicate changes between pairs of items from the corresponding pair of superordinate categories. A red color indicates the items from the categories move farther apart from each other after alignment, blue indicates moving closer together. Generally, items within a superordinate category move slightly closer together under naive alignment, while those in different categories move farther apart. A similar overall pattern is reflected in both the naively-aligned transform (left) and gLocal (right) ones, though under gLocal alignment there is a greater overall spreading of the representations. (All results are from CLIP-ViT-L/14.)Figure 8: Comparison of neighbors in the ImageNet validation set for representations with different transforms. We visualize the 10 closest images subject to the constraint that each comes from a unique class. The anchor images are shown in the leftmost column. The three rows corresponding to each anchor image show their nearest neighbors in the untransformed, gLocal transformed, and naively transformed representations.

Additional results on downstream tasks

In this section, we provide additional few-shot learning and anomaly detection results for all ImageNet and image/text models that we considered in our analyses (see SSA.1). We start this section by demonstrating a strong relationship between the performances of the different downstream tasks.

**Downstream task relationship**. We observe a strong positive relationship (\(r=0.8524,p\leq 10^{-6}\)) between the average few-shot learning and the average anomaly detection performance for all ImageNet and image/text models that we considered in our analyses (see Fig. 9). This observation holds for both the original representation space and the representations transformed via the naive or gLocal transformations. This indicates that both downstream tasks require similar representations for similarly strong performance.

### Few-shot learning

In the following section, we show additional few-shot learning results. Specifically, we report 5-shot performance of ImageNet models and show few-shot results as a function of the number of samples used during fitting.

**Results for ImageNet models**. In Tab. 6 we report additional 5-shot results for ImageNet models. The gLocal transforms improve few-shot accuracy on Entity-{13,30} from BREEDS but the impact on few-shot performance is either inconsistent or negative for CIFAR-100 coarse, CIFAR-100, SUN397, and DTD of which the latter three are more fine-grained datasets than the other three.

**Results for Tip-Adapter [93]**. To investigate whether the gLocal transforms improve downstream task performance of image/text models in few-shot learning settings in combination with techniques that were specifically designed for this task, we evaluate the CLIP models combined with Tip-Adapter [93]. Tip-Adapter is designed to produce effective few-show classifiers from pretrained image/text models by linearly combining the outputs of two modules:

1. a _zero-shot classifier_, where the similarity of the input in embedding space with the embedded textual description of each class determines the output

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c|c c} \hline \hline  & \multicolumn{2}{c|}{Entity-13} & \multicolumn{2}{c|}{Entity-30} & \multicolumn{2}{c|}{CIFAR100-Coarse} & \multicolumn{2}{c|}{CIFAR100} & \multicolumn{2}{c}{SUN397} & \multicolumn{2}{c}{DTD} \\ Model \(\backslash\) Transform & original & gLocal & original & gLocal & original & gLocal & original & gLocal & original & gLocal & original & gLocal \\ \hline AlexNet & 37.49 & **42.13** & **27.73** & 26.93 & **32.33** & 31.44 & **28.70** & 23.29 & **26.43** & 19.12 & **37.05** & 30.54 \\ ResNet-18 & 56.71 & **58.19** & **52.94** & 51.27 & **42.97** & 41.84 & **38.12** & 35.92 & **37.26** & 34.52 & **48.53** & 45.90 \\ ResNet-50 & 49.27 & **59.18** & 50.16 & **54.11** & **50.99** & 49.48 & **47.99** & 44.75 & **47.71** & 44.26 & **54.02** & 51.49 \\ VGG-16 & 51.78 & **56.49** & 44.69 & **45.97** & **39.08** & 37.03 & **34.54** & 28.08 & **37.12** & 29.89 & **45.39** & 38.01 \\ \hline \hline \end{tabular}
\end{table}
Table 6: 5-shot FS results using the original or transformed representations.

Figure 9: Here, we show anomaly detection AUROC averaged across all tasks reported in Tab. {2, 3} as a function of the average 5-shot classification performance for all ImageNet and CLIP models (see SSA.1), using either the original representations or the representations transformed via the naive or gLocal transformations.

2. a _key-value cache model_ that computes its output by summing the one-hot labels of all stored few-shot data, weighted by the similarities of the associated image embeddings to the embedded input sample

For the evaluation of gLocal transforms, we transform both the text embedding and any image embedding involved in the inference part. The hyper-parameters that we use are \(\alpha=1\) and \(\beta=5.5\) which have been identified as optimal for ImageNet classification by Zhang et al. [93].

We observe that even in conjunction with a method specifically designed for few-shot learning, gLocal transforms improve few-shot accuracy in most scenarios (see Tab. 7), in particular on Entity-{13,30} from BREEDS. Furthermore, the performance increase appears to be stronger for the CLIP models trained on the OpenAI WIT datasets compared to LAION (see Fig. 10). For the smaller of the two LAION datasets [LAION-400M; 81], the effect of the gLocal transform on CLIP with Tip-Adapter is negligible compared to its effect on models trained on the larger LAION-2B [82] or OpenAI's WIT dataset [70].

**Effect of transforms for different numbers of training samples**. When varying the number of training samples for the few-shot experiments described in SS4.3 we observe consistent improvements of the gLocal transforms across shots. Excluding the high-variance setting of \(2\)-shot learning, we either find stable improvements in accuracy for image/text models, or a downward trend for ImageNet models on some tasks. This corroborates our findings from SS4.3. Results appear to be robust to changes in the training set size, in particular for the CLIP models. Yet, we observe the most substantial benefits in low data regimes. See Fig. 11 for more details.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{Model \(\backslash\) Transform} & \multicolumn{2}{c|}{Entity-13} & \multicolumn{2}{c|}{Entity-30} & \multicolumn{2}{c|}{CIFAR100-Course} & \multicolumn{2}{c}{CIFAR100} & \multicolumn{2}{c}{SUN397} & \multicolumn{2}{c}{DTD} \\  & original & gLocal & original & gLocal & original & gLocal & original & gLocal & original & gLocal & original & gLocal \\ \hline CLIP-R8N50 (WIT) & 66.29 & **60.95** & 59.24 & **64.76** & 31.41 & **33.69** & 38.53 & **44.82** & 53.42 & **55.75** & **47.98** & 47.91 \\ CLIP-VV1/14 (WIT) & 68.17 & **77.15** & 67.50 & **73.52** & 55.59 & **66.73** & 45.79 & **64.51** & 64.77 & **69.80** & 57.96 & **59.72** \\ CLIP-VV1/14 (LAION-400M) & **72.87** & 72.04 & **69.60** & 62.21 & **78.77** & 71.38 & **69.65** & 68.27 & **71.24** & 70.30 & **68.96** & 65.69 \\ CLIP-VV1/14 (LAION-2B) & 73.19 & **75.54** & 70.50 & **71.79** & **77.45** & 76.81 & 71.87 & **73.33** & 70.24 & **71.17** & **66.10** & 64.43 \\ \hline \hline \end{tabular}
\end{table}
Table 7: 5-shot FS results using the original or transformed representations in combination with Tip-Adapter.

Figure 10: 5-shot FS results using the original or transformed representations in combination with Tip-Adapter.

**Few-shot learning on ImageNet-1K**. In addition to the few-shot learning results that we presented in SS4.3, here we report few-shot classification results on ImageNet-1K [17]. For both the original and gLocal-transformed CLIP representations, we trained a linear classifier with different numbers of shots, \(k\), i.e., the number of training examples per class. We find that the gLocal-transformed representation achieves better ImageNet validation set accuracy compared to the original representation for both CLIP ViT-L/14 (WIT) and CLIP ViT-L/14 (LAION-2B) in small training data settings where \(k>5\). We don't observe a significant difference between the two representation spaces for CLIP RN50 (WIT) and CLIP ViT-L/14 (LAION-400 M) which generally perform worse than the other two models across all training settings (see Fig. 12).

### Anomaly detection

In addition to the results of image/text models for the "one-vs-rest" anomaly detection (AD) setting that we presented in SS4.4, here we show "one-vs-rest" AD performance of ImageNet models. While the gLocal transform considerably improves AD performance over the untransformed representations

Figure 11: Change in average accuracy for different numbers of training samples per super-class (top row) or (sub-)class (bottom row) used for few-shot learning. Error bands depict 95% Confidence Intervals (CIs), computed over 5 different runs.

Figure 12: ImageNet validation set accuracy as a function of the number of training examples per class used for training a linear classifier on the original and the gLocal-transformed representations. For all CLIP models, we report the mean validating set accuracy across three different seeds.

[MISSING_PAGE_FAIL:26]

### Global versus gLocal transform

Aside from the AD performance of CLIP RN50 and CLIP ViT-L/14 (WIT), the gLocal transform leads to more substantial improvements on downstream tasks than the global transform. In Tab 13, we report the average few-shot and anomaly detection performances using the global or gLocal transforms. For FS, we average performance over all results reported in Tab. 1, and for AD we average performance across all results reported in Tab. 1.

### Human-adversarial experiments

To rule out confounding variables other than global similarity structure for improved downstream task performance of the gLocal transform, we created a new triplet dataset where for each triplet in the data we choose an object that is different from the original human choice to be the new odd-one-out choice. Note that this is not a random choice over all objects but a random choice over the set of two objects not chosen by a human participant, i.e., human-adversarial choices (albeit random and not deterministic). Using this human-adversarial triplet dataset, we use the same gLocal optimization as we do for the original task (see Eq. 6). We minimize a bounded cross-entropy error and determine convergence on a held-out, human-adversarial validation set as we have done for the original task. This allows us to use the same hyperparameter setting that we found is optimal for the gLocal transform (see Appx. A.2 for further details). Across the different few-shot learning and anomaly detection tasks, we find the adversarial transform performing slightly worse than the original transform and substantially worse than the gLocal transform for all CLIP models (see Fig. 13).

## Appendix E Representational alignment

### Human similarity judgments and RSMs

**Multi-arrangement task**. Human similarity judgments for King et al. [41] and [10] were obtained by using a multi-arrangement task. In a multi-arrangement task, participants are presented with a computer screen showing images of several different objects. The participants are asked to arrange the images into semantically meaningful clusters, given the instruction that images of objects that lie close together are considered more similar. From this arrangement, one can infer pairwise (dis-)similarities of the objects and average those across all participants to obtain a representative (dis-)similarity matrix.

**Ordinal scale**. In Peterson et al. [65; 66], pairwise similarity judgments were obtained by asking human participants to rate the similarity of pairs of objects on an ordinal scale that ranges from 0 ("not similar at all") to 10 ("very similar"). The pairwise similarity ratings can be averaged across the different participants which in turn yields a matrix of similarities between pairs of objects.

\begin{table}
\begin{tabular}{l|c c|c c} \hline \hline  & \multicolumn{2}{c|}{AD} & \multicolumn{2}{c}{FS} \\ Model \(\setminus\) Transform & global & gLocal & global & gLocal \\ \hline AlexNet & 81.16 & **81.76** & 28.43 & **28.91** \\ ResNet-18 & 84.62 & **86.91** & 43.42 & **44.61** \\ ResNet-50 & **93.19** & 89.56 & **51.32** & 50.55 \\ VGG-16 & 87.32 & **88.36** & 37.23 & **39.25** \\ \hline CLIP-RN50 (WIT) & **92.12** & 91.52 & 54.63 & **55.00** \\ CLIP-ViT-L/14 (WIT) & **95.49** & 95.19 & **69.92** & 69.91 \\ CLIP-ViT-L/14 (LAION-400M) & 95.72 & **96.08** & 68.80 & **69.40** \\ CLIP-ViT-L/14 (LAION-2B) & 96.33 & **96.65** & 72.00 & **72.27** \\ \hline \hline \end{tabular}
\end{table}
Table 13: Comparison of the average downstream task performance global and gLocal transforms.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c} \hline \hline \multicolumn{1}{c}{\(k\)} & \multicolumn{2}{c}{2} & \multicolumn{2}{c}{5} & \multicolumn{2}{c}{10} & \multicolumn{2}{c}{20} \\ Dataset \(\setminus\) Transform & original & gLocal & original & gLocal & original & gLocal & original & gLocal \\ \hline CIFAR-10 & 95.37 & **98.16** & 95.14 & **98.16** & 94.86 & **98.11** & 94.50 & **98.03** \\ CIFAR-100 & 91.90 & **97.22** & 91.41 & **97.19** & 90.93 & **97.08** & 90.39 & **96.92** \\ CIFAR-100-coarse & 89.28 & **95.9** & 88.50 & **95.83** & 87.73 & **95.66** & 86.81 & **95.41** \\ CIFAR-100-xshift & 74.48 & **87.06** & 73.69 & **87.11** & 73.00 & **87.01** & 72.29 & **86.85** \\ ImageNet30 & 98.95 & **99.74** & 98.91 & **99.75** & 98.85 & **99.77** & 98.78 & **99.77** \\ Entity-13 & 88.37 & **92.5** & 88.54 & **93.23** & 88.45 & **93.63** & 88.28 & **93.95** \\ Entity-30 & 91.26 & **95.11** & 91.31 & **95.53** & 91.22 & **95.74** & 91.03

**Triplet odd-one-out choices**. The triplet odd-one-out task is a commonly used task in the cognitive sciences to infer pairwise object similarity ratings [72; 31; 60]. The triplet odd-one-out task is a _three-alternative-forced-choice_ task where participants are presented with three objects and have to select the one that does not fit. In contrast to the multi-arrangement task or an ordinal scale, the triplet odd-one-out task does not naturally yield a similarity matrix. A similarity matrix can be obtained, however, by learning representations for the objects being used in the task from the human responses. Variational Interpretable Concept Embeddings (VICE) -- an approximate Bayesian method for inferring mental representations of object concepts from triplet odd-one-out choices -- is a method that was specifically developed for that purpose. VICE uses variational inference to learn representations for the objects in the triplets by fitting the human responses via stochastic gradient descent. The method minimizes \(\mathcal{L}_{\text{global}}\) with additional non-negativity and sparsity constraints on the representations. More details about the optimization can be found in Muttenthaler et al. [60]. From the VICE solution, one can easily compute a representational similarity matrix (RSM). Specifically, given learned object representations \(\bm{V}\in\mathbb{R}^{n\times d}\), one first computes the dot-product similarity matrix \(\bm{S}_{h}\coloneqq\bm{V}\bm{V}^{\top}\) and then exponentiate this matrix elementwise, \(\bm{S}^{\prime}_{h}\coloneqq\exp(\bm{S}_{h})\). One can then apply the softmax function defined in Eq. 1 to every combination of triplets in the exponentiated similarity matrix which yields the final RSM for triplet odd-one-out choices from Hebart et al. [32]. The last step is performed to guarantee that the pairwise similarities are modeled according to the triplet odd-one-out objective function that was used to learn the human object representations \(\bm{V}\) (see Eq. 2).

### Neural network representations and RSMs

**Neural network representations**. RSMs for neural network representations are obtained by first embedding the same set of images that were presented to the human participants in the \(p\)-dimensional latent space of a neural net. The latent space could be any layer of a neural network. Here we use the penultimate layer space for ImageNet models and the image encoder space for image/text models. We do this because previous work has shown that the penultimate layer space of ImageNet models and the image encoder space of image/text models respectively yield the highest similarity to human behavior [66; 67; 61]. After embedding the images into the neural net's latent space, one obtains a representation matrix \(\bm{X}\in\mathbb{R}^{n\times p}\) for the \(n\) images in the data. Instead of simply computing the dot-product similarity matrix \(\bm{S}\coloneqq\bm{X}\bm{X}^{\top}\), in RSA one typically uses either a cosine similarity or a Pearson correlation kernel to compute the affinity matrix,

\[\text{cos}(\bm{x}_{i},\bm{x}_{j})\coloneqq\frac{\bm{x}_{i}^{\top}\bm{x}_{j}}{ ||\bm{x}_{i}||_{2}||\bm{x}_{j}||_{2}};\qquad\qquad\phi(\bm{x}_{i},\bm{x}_{j}) \coloneqq\frac{\left(\bm{x}_{i}-\bar{\bm{x}}_{i}\right)^{\top}\left(\bm{x}_{j} -\bar{\bm{x}}_{j}\right)}{||\left(\bm{x}_{i}-\bar{\bm{x}}_{i}\right)||_{2}|| \left(\bm{x}_{j}-\bar{\bm{x}}_{j}\right)||_{2}},\]

where the cosine similarity kernel function \(\text{cos}(\bm{x}_{i},\bm{x}_{j})\) or the Pearson correlation kernel function \(\phi(\bm{x}_{i},\bm{x}_{j})\) is applied to every \((\bm{x}_{i},\bm{x}_{j})\) vector pair of the matrix \(\bm{X}\) for obtaining the final representational similarity matrix \(\bm{S}^{\prime}\in\mathbb{R}^{n\times n}\). Here, we use the Pearson correlation kernel function \(\phi(\bm{x}_{i},\bm{x}_{j})\) to obtain a neural net's RSM. Pearson correlation is the centered version of cosine similarity and the ranking of the obtained similarities does not differ between the two kernel functions but Pearson correlation first centers the vectors to have zero mean and is therefore a more robust measure. For obtaining RSMs with transformed representations, the transforms are first applied to \(\bm{X}\) before computing \(\bm{S}^{\prime}\).

Figure 13: Few-shot learning and anomaly detection performances as a function of the degree of alignment with human similarity judgments for the original, naive, gLocal, and human-adversarial transforms.

### Representational Similarity Analysis (RSA)

**Additional RSMs**. To corroborate our findings from SS4.5, here we additionally show RSMs for CLIP RN50 and CLIP ViT-L/14 (Laion 2B). In accordance with the different RSMs obtained from the representation space of CLIP ViT-L/14 (WIT), there does not appear to be a qualitative difference in the global similarity structure between the RSMs obtained from applying either the naive or the gLocal transforms to CLIP RN50 or CLIP ViT-L/14 (Laion 2B) (see Fig. 14). Hence, the gLocal transform improves representational alignment while preserving the local similarity structure of the original representation equally well for the different CLIP models, as we show in Tab. 4.

## Appendix F Global transform derivation

Here we derive that

\[\min_{\alpha}\left\|\bm{W}-\alpha I\right\|_{\mathrm{F}}^{2}=\left\|\bm{W}- \left(\sum_{i=1}^{p}\bm{W}_{ii}/p\right)I\right\|_{\mathrm{F}}^{2}.\]

First, observe that

\[\min_{\alpha}\left\|\bm{W}-\alpha I\right\|_{\mathrm{F}}^{2} =\min_{\alpha}\sum_{i=1}^{p}\sum_{j=1}^{p}\left(\bm{W}_{ij}- \alpha\mathbbm{1}_{[i=j]}\right)^{2}\] \[=\min_{\alpha}\sum_{i=1}^{p}\sum_{j=1,j\neq i}^{p}\bm{W}_{ij}^{2} +\sum_{k=1}^{p}\left(\bm{W}_{kk}-\alpha\right)^{2}\] \[=\sum_{i=1}^{p}\sum_{j=1,j\neq i}^{p}\bm{W}_{ij}^{2}+\min_{\alpha} \sum_{k=1}^{p}\left(\bm{W}_{kk}-\alpha\right)^{2}.\]

Figure 14: Here, we show representational similarity matrices (RSMs) for human behavior and CLIP RN50 [WIT; 70] and CLIP ViT-L/14 [Laion 2B; 82] for four different human similarity judgment datasets [31; 66; 10; 41]. We contrast RSMs obtained from the network’s original representation space (second column), the naively transformed representation space [61] (third column), and the representation space obtained by using the gLocal transform (rightmost column) against RSMs directly constructed from human similarity judgments (leftmost column).

The minimizer of \(\min_{\alpha}\sum_{k=1}^{p}\left(\bm{W}_{kk}-\alpha\right)^{2}\) is attained with \(\alpha=\sum_{\ell=1}^{p}\bm{W}_{\ell\ell}/p\). Substituting this back into the last equality and reversing the steps from before we have

\[\sum_{i=1}^{p}\sum_{j=1,j\neq i}^{p}\bm{W}_{ij}^{2}+\min_{\alpha} \sum_{k=1}^{p}\left(\bm{W}_{kk}-\alpha\right)^{2} =\sum_{i=1}^{p}\sum_{j=1,j\neq i}^{p}\bm{W}_{ij}^{2}+\sum_{k=1}^{p }\left(\bm{W}_{kk}-\sum_{\ell=1}^{p}\bm{W}_{\ell\ell}/p\right)^{2}\] \[=\sum_{i=1}^{p}\sum_{j=1}^{p}\left(\bm{W}_{ij}-\left(\sum_{\ell=1 }^{p}\bm{W}_{\ell\ell}/p\right)1_{[i=j]}\right)^{2}\] \[=\left\|\bm{W}-\left(\sum_{\ell=1}^{p}\bm{W}_{\ell\ell}/p\right) I\right\|_{\mathrm{F}}^{2},\]

which finishes our derivation.

## Appendix G Properties of LCKA

Kornblith et al. [43] previously validated linear centered kernel alignment (LCKA) as a way to measure similarity between neural network representations. Given representations \(\bm{X}\in\mathbb{R}^{n\times p}\) and \(\bm{Y}\in\mathbb{R}^{n\times p_{2}}\) containing embeddings of the same \(n\) images stacked row-wise, LCKA is:

\[\mathrm{LCKA}(\bm{X},\bm{Y})=\frac{\langle\tilde{\bm{X}}\tilde{\bm{X}}^{ \top},\tilde{\bm{Y}}\tilde{\bm{Y}}^{\top}\rangle_{\mathrm{F}}}{\|\tilde{\bm{X} }\tilde{\bm{X}}^{\top}\|_{\mathrm{F}}\|\tilde{\bm{Y}}\tilde{\bm{Y}}^{\top}\|_{ \mathrm{F}}}=\frac{\|\tilde{\bm{X}}^{\top}\tilde{\bm{Y}}\|_{\mathrm{F}}^{2}}{ \|\tilde{\bm{X}}^{\top}\tilde{\bm{X}}\|_{\mathrm{F}}\|\tilde{\bm{Y}}^{\top} \tilde{\bm{Y}}\|_{\mathrm{F}}},\] (7)

where \(\tilde{\bm{X}}\) and \(\tilde{\bm{Y}}\) are equal to \(\bm{X}\) and \(\bm{Y}\) with column means subtracted. (Formally, \(\tilde{\bm{X}}=\bm{H}\bm{X}\) and \(\tilde{\bm{Y}}=\bm{H}\bm{Y}\) and \(\bm{H}=\bm{I}-\frac{1}{n}\bm{1}\bm{1}^{\top}\) is the centering matrix, which is a matrix representation of the linear operator that subtracts column means.)

As Kornblith et al. [43] note, linear CKA can be thought of as measuring the cosine similarity between all pairs of principal components (PCs) of \(\tilde{\bm{X}}\) and \(\tilde{\bm{Y}}\), weighted by the products of the proportions of variance these PCs explain in each representation. Formally, let \(\tilde{\bm{X}}=\bm{U}\bm{\Sigma}\bm{V}^{\top}\) and \(\tilde{\bm{Y}}=\bm{U}^{\prime}\bm{\Sigma}^{\prime}\bm{V}^{\prime}{}^{\top}\) be the singular value decompositions of \(\tilde{\bm{X}}\) and \(\tilde{\bm{Y}}\). The left-singular vectors \(\bm{u}_{i}=\bm{U}_{:,i}\) are the (unit-norm) PCs of \(\bm{X}\), and the squared singular values \(\lambda_{i}=\Sigma_{ii}^{2}\) are the amount of variance that those PCs explain (up to a factor of \(1/n\)). Given these singular value decompositions, linear CKA is:

\[\mathrm{LCKA}(\bm{X},\bm{Y})=\frac{\sum_{i=1}^{p_{1}}\sum_{j=1}^{p_{2}}\lambda _{i}\lambda_{j}^{\prime}\left(\bm{u}_{i}^{\top}\bm{u}_{j}^{\prime}\right)^{2 }}{\sqrt{\sum_{i=1}^{p_{1}}\lambda_{i}^{2}}\sqrt{\sum_{j=1}^{p_{2}}\lambda_{j }^{\prime}{}^{2}}}.\] (8)