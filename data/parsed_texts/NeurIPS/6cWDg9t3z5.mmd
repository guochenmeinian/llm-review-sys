# Universal Rates of Empirical Risk Minimization

Steve Hanneke

Department of Computer Science

Purdue University

steve.hanneke@gmail.com &Mingyue Xu

Department of Computer Science

Purdue University

xu1864@purdue.edu

###### Abstract

The well-known _empirical risk minimization_ (ERM) principle is the basis of many widely used machine learning algorithms, and plays an essential role in the classical PAC theory. A common description of a learning algorithm's performance is its so-called "learning curve", that is, the decay of the expected error as a function of the input sample size. As the PAC model fails to explain the behavior of learning curves, recent research has explored an alternative universal learning model and has ultimately revealed a distinction between optimal universal and uniform learning rates (Bousquet et al., 2021). However, a basic understanding of such differences with a particular focus on the ERM principle has yet to be developed.

In this paper, we consider the problem of universal learning by ERM in the realizable case and study the possible universal rates. Our main result is a fundamental _tetrachotomy_: there are only four possible universal learning rates by ERM, namely, the learning curves of any concept class learnable by ERM decay either at \(e^{-n},1/n,\)\(\log{(n)}/n\), or arbitrarily slow rates. Moreover, we provide a complete characterization of which concept classes fall into each of these categories, via new complexity structures. We also develop new combinatorial dimensions which supply sharp asymptotically-valid constant factors for these rates, whenever possible.

## 1 Introduction

The classical statistical learning theory mainly focuses on the celebrated PAC (Probably Approximately Correct) model (Vapnik and Chervonenkis, 1974; Valiant, 1984) with emphasis on supervised learning. A particular setting therein, called the _realizable_ case, has been extensively studied. Complemented by the "no-free-lunch" theorem (Antos and Lugosi, 1996), the PAC framework, which adopts a minimax perspective, can only explain the best _worst-case_ learning rate by a learning algorithm over all realizable distributions. Such learning rates are thus also called the _uniform_ rates. However, the uniform rates can only capture the upper envelope of all learning curves, and are too coarse to explain practical machine learning performance. This is because real-world data is rarely worst-case, and the data source is typically fixed in a given learning scenario. Indeed, Cohn and Tesauro (1990, 1992) observed from experiments that practical learning rates can be much faster than is predicted by PAC theory. Moreover, many theoretical works (Schuurmans, 1997; Koltchinskii and Beznosova, 2005; Audibert and Tsybakov, 2007, etc.) were able to prove faster-than-uniform rates for certain learning problems, though requiring additional modelling assumptions. To distinguish from the uniform rates, these rates are named the _universal_ rates and was formalized recently by Bousquet et al. (2021) via a distribution-dependent framework. Unlike the simple _dichotomy_ of the optimal uniform rates: every concept class \(\mathcal{H}\) has a uniform rate being either linear \(\text{VC}(\mathcal{H})/n\) or "bounded away from zero", the optimal universal rates are captured by a _richotomy_: every concept class \(\mathcal{H}\) has a universal rate being either exponential, linear or arbitrarily slow (see Thm.1.6 Bousquet et al., 2021).

In supervised learning, a family of successful learners called the _empirical risk minimization_ (ERM) consist of all learning algorithms that output a sample-consistent classifier. In other words, an ERMalgorithm is any learning rule, which outputs a concept in \(\mathcal{H}\) that minimizes the empirical error (see Appendix A for a formal definition). For notation simplicity, we first introduce

**Definition 1** (**Version space, Mitchell, 1977**).: _Let \(\mathcal{H}\) be a concept class and \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\) be a dataset, the version space induced by \(S_{n}\), denoted by \(V_{S_{n}}(\mathcal{H})\) (or \(V_{n}(\mathcal{H})\) for short), is defined as \(V_{S_{n}}(\mathcal{H}):=\{\hat{h}\in\mathcal{H}:h(x_{i})=y_{i},\forall i\in[n]\}\)._

Now given labeled samples \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\), an ERM algorithm is any learning algorithm that outputs a concept in the sample-induced _version space_, that is, a sequence of universally measurable functions \(\mathcal{A}_{n}:S_{n}\rightarrow\hat{h}_{n}\in V_{S_{n}}(\mathcal{H}),n\in \mathbb{N}\). Throughout this paper, we will simply denote an ERM algorithm by its output predictors \(\{\hat{h}_{n}\}_{n\in\mathbb{N}}\).

It is well-known that the ERM principle plays an important role in understanding general uniform learnability: a concept class is uniformly learnable if and only if it can be learned by ERM. However, while the optimal \(\text{VC}(\mathcal{H})/n\) rate is achievable by some improper learner (Hanneke, 2016a), ERM algorithms can at best achieve a uniform rate of \((\text{VC}(\mathcal{H})/n)\log{(n/\text{VC}(\mathcal{H}))}\). Moreover, such a gap has been shown to be unavoidable in general (Auer and Ortner, 2007), which leaves a challenging question to study: what are the sufficient and necessary conditions on \(\mathcal{H}\) for the entire family of ERM algorithms to achieve the optimal error? Indeed, many subsequent works have devoted to improving the logarithmic factor in specific scenarios. The work of Gine and Koltchinskii (2006) refined the bound by replacing \(\log{(n/\text{VC}(\mathcal{H}))}\) with \(\log{(\theta(\text{VC}(\mathcal{H})/n))}\), where \(\theta(\cdot)\) is called the _disagreement coefficient_. Based on this, Hanneke and Yang (2015) proposed a new data-dependent bound with \(\log{(\hat{n}_{1:n}/\text{VC}(\mathcal{H}))}\), where \(\hat{n}_{1:n}\) is a quantity related to the _version space compression set size_ (a.k.a. the _empirical teaching dimension_). As a milestone, the work of Hanneke (2016b) proved an upper bound \((\text{VC}(\mathcal{H})/n)\log{(\mathfrak{s}_{\mathcal{H}}/\text{VC}( \mathcal{H}))}\) and a lower bound \((\text{VC}(\mathcal{H})+\log{(\mathfrak{s}_{\mathcal{H}})})/n\), where \(\mathfrak{s}_{\mathcal{H}}\) is called the _star number_ of \(\mathcal{H}\) (see Definition 4 in Section 2). Though not quite matching, these two bounds together yield an optimal linear rate when \(\mathfrak{s}_{\mathcal{H}}<\infty\). Thereafter, the uniform rates by ERM can be described as a _trichotomy_, namely, every concept class \(\mathcal{H}\) has a uniform rate by ERM being exactly one of the following: \(1/n\), \(\log{(n)/n}\) and "bounded away from zero".

From a practical perspective, many ERM-based algorithms are designed and are widely applied in different areas of machine learning, such as the logistic regression and SVM, the CAL algorithm in active learning, the gradient descent (GD) algorithm in deep learning. Since the worst-case nature of the PAC model is too pessimistic to reflect the practice of machine learning, understanding the distribution-dependent performance of ERM algorithms is of great significance. However, unlike that a distinction between the optimal uniform and universal rates has been fully understood, how fast universal learning can outperform uniform learning in particular by ERM remains unclear. Furthermore, we are lacking a complete theory to the characterization of the universal rates by ERM, though certain specific scenarios that admit faster rates by ERM have been discovered (Schuurmans, 1997; van Handel, 2013). In this paper, we aim to answer the following fundamental question:

**Question 1**.: _Given a concept class \(\mathcal{H}\), what are the possible rates at which \(\mathcal{H}\) can be universally learned by ERM?_

We start with some basic preliminaries of this paper. We consider an _instance space_\(\mathcal{X}\) and a _concept class_\(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\). Given a probability distribution \(P\) on \(\mathcal{X}\times\{0,1\}\), the _error rate_ of a classifier \(h:\mathcal{X}\rightarrow\{0,1\}\) is defined as \(\text{er}_{P}(h):=P((x,y)\in\mathcal{X}\times\{0,1\}:h(x)\neq y)\). A distribution \(P\) is called _realizable_ with respect to \(\mathcal{H}\), denoted by \(P\in\text{RE}(\mathcal{H})\), if \(\inf_{h\in\mathcal{H}}\text{er}_{P}(h)=0\). Note that in this definition, \(h^{*}\) satisfying \(\text{er}_{P}(h^{*})=\inf_{h\in\mathcal{H}}\text{er}_{P}(h)\) is called the _target concept_ of the learning problem, and is not necessary in \(\mathcal{H}\). We may also say that \(P\) is a realizable distribution centered at \(h^{*}\). Given an integer \(n\), we denote by \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\) a i.i.d. \(P\)-distributed dataset. In the universal learning framework, the performance of a learning algorithm is commonly measured by its _learning curve_(Bousquet et al., 2021; Hanneke et al., 2022; Bousquet et al., 2023), that is, the decay of the _expected error rate_\(\mathbb{E}[\text{er}_{P}(\hat{h}_{n})]\) as a function of sample size \(n\). With these settings settled, we are now able to formalize the problem of universal learning by ERM.

**Definition 2** (**Universal learning by ERM**).: _Let \(\mathcal{H}\) be a concept class, and \(R(n)\to 0\) be a rate function. We say_

* \(\mathcal{H}\) _is universally learnable at rate_ \(R\) _by ERM, if for every distribution_ \(P\in\text{RE}(\mathcal{H})\)_, there exist parameters_ \(C,c>0\) _such that for every ERM algorithm,_ \(\mathbb{E}[\text{er}_{P}(\hat{h}_{n})]\leq CR(cn)\)_, for all_ \(n\in\mathbb{N}\)_._

[MISSING_PAGE_FAIL:3]

learnable but necessarily with arbitrarily slow rates, or is not universally learnable at all. Throughout this paper, we only consider the case where the given concept class is universally learnable by ERM. We leave it an open question whether there exists a nice characterization that determines the universal learnability by ERM.

### Main results

In this section, we summarize the main results of this paper. The examples in Section 1.1 reveal that there are at least four possible universal rates by ERM. Interestingly, we find that these are also the only possibilities. The following two theorems consist of the main results of this work. In particular, Theorem 1 gives out a complete answer to Question 1. It expresses a fundamental _tetrachotomy_: there are exactly four possibilities for the universal learning rates by ERM: being either exponential, or linear, or \(\log\left(n\right)/n\), or arbitrarily slow rates. Moreover, Theorem 2 specifies the answer by pointing out for what realizable distributions (targets), those universal rates are sharp.

**Theorem 1** (**Universal rates for ERM)**.: _For every class \(\mathcal{H}\) with \(|\mathcal{H}|\geq 3\), the following hold:_

* \(\mathcal{H}\) _is universally learnable by ERM with exact rate_ \(e^{-n}\) _if and only if_ \(|\mathcal{H}|<\infty\)_._
* \(\mathcal{H}\) _is universally learnable by ERM with exact rate_ \(1/n\) _if and only if_ \(|\mathcal{H}|=\infty\) _and_ \(\mathcal{H}\) _does not have an infinite star-eluder sequence._
* \(\mathcal{H}\) _is universally learnable by ERM with exact rate_ \(\log\left(n\right)/n\) _if and only if_ \(\mathcal{H}\) _has an infinite star-eluder sequence and_ \(\text{VC}(\mathcal{H})<\infty\)_._
* \(\mathcal{H}\) _requires at least arbitrarily slow rates to be learned by ERM if and only if_ \(\text{VC}(\mathcal{H})=\infty\)_._

**Remark 3**.: _The formal definition of the star-eluder sequence can be found in Section 2. Unlike the separation between exact \(e^{-n}\) and \(1/n\) rates is determined by the cardinality of the class, and the separation between exact \(\log\left(n\right)/n\) and arbitrarily slow rates is determined by the VC dimension of the class, whether there exists a simple combinatorial quantity that determines the separation between exact \(1/n\) and \(\log\left(n\right)/n\) rates is unclear and might be an interesting direction for future work. We thought that it is likely the star number \(\mathfrak{s}_{\mathcal{H}}\) (Definition 4) is the correct characterization here, but it turns out not unfortunately (see details in Section 4 and Appendix B.3)._

Based on Theorem 1, a distinction between the performance of ERM algorithms and the optimal universal learning algorithms can be revealed, which we present in the following table (the required definitions in "Case" are deferred to Section 2, and examples can be found in Appendix B.2).

\begin{tabular}{|l|l|l|l|} \hline Optimal rate & Exact rate by ERM & Case & Example \\ \hline \(e^{-n}\) & \(1/n\) & infinite eluder sequence but no infinite Littestone tree & Example 12 \\ \hline \(e^{-n}\) & \(\log\left(n\right)/n\) & infinite star-eluder sequence but no infinite Littestone tree & Example 13 \\ \hline \(e^{-n}\) & arbitrarily slow & infinite VC-eluder sequence but no infinite Littestone tree & Example 15 \\ \hline \(1/n\) & \(\log\left(n\right)/n\) & infinite star-eluder sequence but no infinite VCL tree & Example 14 \\ \hline \(1/n\) & arbitrarily slow & infinite VC-eluder sequence but no infinite VCL tree & Example 16 \\ \hline \end{tabular} Furthermore, the distinction between the universal rates and the uniform rates by ERM can also be fully captured, and are depicted schematically in Figure 1 as an analogy to the Fig.4 of Bousquet et al. (2021). Besides the examples in Section 1.1, we also need the following additional example concerning the Littlestone dimension to appear in the diagram.

**Example 6** (\(\log\left(n\right)/n\) learning rate and unbounded Littlestone dimension).: _We consider here the class of two-dimensional halfspaces, that is, \(\mathcal{X}:=\mathbb{R}^{2}\) and \(\mathcal{H}_{\text{halfspaces},\mathbb{R}}:=\{\mathbbm{1}(\bm{w}\cdot\bm{x}+b \geq 0):\bm{w}\in\mathbb{R}^{2},b\in\mathbb{R}\}\). It is a classical fact that for any integer \(d\), the class of halfspaces on \(\mathbb{R}^{d}\) has a finite VC dimension \(d\), but has an infinite Littlestone tree, and thus having unbounded Littlestone dimension (Shalev-Shwartz and Ben-David, 2014). Finally, to show that this class is universally learnable by ERM at exact \(\log\left(n\right)/n\) rate, we simply consider the subspace \(\mathbb{S}^{1}\subset\mathcal{X}\), this is indeed an infinite star set of \(\mathcal{H}_{\text{halfspaces},\mathbb{R}}\) centered at \(h_{\text{half-0}\times}\) and thus an infinite star-eluder sequence._

As a complement to Theorem 1, the following Theorem 2 gives out target-specified universal rates. We say a target concept \(h^{*}\) is universally learnable by ERM with exact rate \(R\) if all realizable distribution \(P\) considered in Definition 2 are centered at \(h^{*}\). In other words, \(\mathcal{H}\) is universally learnable with exact rate \(R\) is equivalent to say all realizable target concepts are universally learnable with exact rate \(R\). Concretely, for each of the four possible rates stated in Theorem 1, Theorem 2 specifies the target concepts that can be learned at such exact rate by ERM.

**Theorem 2** (**Target specified universal rates**).: _For every class \(\mathcal{H}\) with \(|\mathcal{H}|\geq 3\), and a target concept \(h^{*}\), the following hold:_

* \(h^{*}\) _is universally learnable by ERM with exact rate_ \(e^{-n}\) _if and only if_ \(\mathcal{H}\) _does not have an infinite eluder sequence centered at_ \(h^{*}\)_._
* \(h^{*}\) _is universally learnable by ERM with exact rate_ \(1/n\) _if and only if_ \(\mathcal{H}\) _has an infinite eluder sequence centered at_ \(h^{*}\)_, but does not have an infinite star-eluder sequence centered at_ \(h^{*}\)_._
* \(h^{*}\) _is universally learnable by ERM with exact rate_ \(\log{(n)}/n\) _if and only if_ \(\mathcal{H}\) _has an infinite star-eluder sequence centered at_ \(h^{*}\)_, but does not have an infinite VC-eluder sequence centered at_ \(h^{*}\)_._
* \(h^{*}\) _requires at least arbitrarily slow rates to be universally learned by ERM if and only if_ \(\mathcal{H}\) _has an infinite VC-eluder sequence centered at_ \(h^{*}\)_._

All detailed proofs appear in Appendix D. We also provide a brief overview of the main idea of each proof as well as some related concepts in Section 2.

An additional part of this work presents a _fine-grained analysis_(Bousquet et al., 2023) of the universal rates by ERM, which complements the _coarse rates_ used in Theorem 1. Concretely, we provide a characterization of sharp distribution-free constant factors of the ERM universal rates, whenever possible. The characterization is based on two newly-developed combinatorial dimensions, called the _star-eluder dimension_ (or SE dimension) and the _VC-eluder dimension_ (or VCE dimension) (Definition 9). We say "whenever possible" because distribution-free constants are unavailable for certain cases (Remark 16). Such a characterization can also be considered as a refinement to the classical PAC theory, in a sense that it is sometimes better but only asymptotically-valid. Due to space limitation, we defer the definition of fine-grained rates and related results to Appendix C.

### Related works

**PAC learning by ERM.** The performance of consistent learning rules (including the ERM algorithm) in the PAC (distribution-free) framework has been extensively studied. For VC classes, Blumer et al. (1989) gave out a \(\log{(n)}/n\) upper bound of the uniform learning rate. Despite the well-known equivalence between uniform learnability and uniform learnability by the ERM principle (Vapnik and Chervonenkis, 1971), the best upper bounds for general ERM algorithms differ from the optimal sample complexity by an unavoidable logarithmic factor (Auer and Ortner, 2007). By analyzing the _disagreement coefficient_ of the version space, the work of Gine and Koltchinskii (2006); Hanneke (2009) refined the logarithmic factor in certain scenarios. Furthermore, not only being a relevant

Figure 1: A Venn diagram depicting the tetrachotomy of the universal rates by ERM and its relation with the uniform rates characterized by the VC dimension and the star number.

measure in the context of active learning (Cohn et al., 1994; El-Yaniv and Wiener, 2012; Hanneke, 2011, 2014), the _region of disagreement_ of the version space was found out to have an interpretation of _sample compression scheme_ with its size known as the _version space compression set size_(Wiener et al., 2015; Hanneke and Yang, 2015). Based on this, the label complexity of the CAL algorithm can be converted into a bound on the error rates of all consistent PAC learners (Hanneke, 2016b). Finally, Hanneke and Yang (2015); Hanneke (2016b) introduced a simple combinatorial quantity named the _star number_, and guaranteed that a concept class with finite star number can be uniformly learned at linear rate.

**Universal Learning.** Observed from empirical experiments, the actual learning rates on real-world data can be much faster than the one described by the PAC theory (Cohn and Tesauro, 1990, 1992). The work of Benedek and Itai (1988) considered a setting lies in between the PAC setting and the universal setting called _nonuniform learning_, in which the learning rate may depend on the target concept but still uniform over the marginal distributions. After that, Schuurmans (1997) studied classes of _concept chains_ and revealed a distinction between exponential and linear rates along with a theoretical guarantee. Later, more improved learning rates have been obtained for various practical learning algorithms such as stochastic gradient decent and kernel methods (Koltchinskii and Beznosova, 2005; Audibert and Tsybakov, 2007; Pillaud-Vivien et al., 2018, etc.). Additionally, van Handel (2013) studied the uniform convergence property from a universal perspective, and proposed the _universal Glivenko-Cantelli property_. Until very recently, the universal (distribution-dependent) learning framework was formalized by Bousquet et al. (2021), in which a complete theory of the (optimal) universal learnability was obtained as well. After that, Bousquet et al. (2023) carried out a fine-grained analysis on the "distribution-free tail" of the universal _learning curves_ by characterizing the optimal constant factor. As generalizations, Kalavasis et al. (2022); Hanneke et al. (2023) studied the universal rates for multiclass classification, and Hanneke et al. (2022) studied the universal learning rates under an interactive learning setting.

## 2 Technical overview

In this section, we discuss some technical aspects in the derivation of our main results in Section 1.2. Our analysis to the universal learning rates by ERM is based on three new types of complexity structures named the _eluder sequence_, the _star-eluder sequence_ and the _VC-eluder sequence_. More details can be found in Sections 3-4 and all technical proofs are deferred to Appendix D.

**Definition 3** (**Realizable data)**.: _Let \(\mathcal{H}\) be a concept class on an instance space \(\mathcal{X}\), we say that a (finite or infinite) data sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\in(\mathcal{X}\times\{0,1\})^{\infty}\) is realizable (with respect to \(\mathcal{H}\)), if for every \(n\in\mathbb{N}\), there exists \(h_{n}\in\mathcal{H}\) such that \(h_{n}(x_{i})=y_{i}\), for all \(i\in[n]\)._

**Definition 4** (**Star number)**.: _Let \(\mathcal{X}\) be an instance space and \(\mathcal{H}\) be a concept class. We define the region of disagreement of \(\mathcal{H}\) as DIS\((\mathcal{H}):=\{x\in\mathcal{X}:\exists h,g\in\mathcal{H}\text{ s.t. }h(x)\neq g(x)\}\). Let \(h\) be a classifier, the star number of \(h\), denoted by \(\mathfrak{s}_{h}(\mathcal{H})\) or \(\mathfrak{s}_{h}\) for short, is defined to be the largest integer \(s\) such that there exist distinct points \(x_{1},\ldots,x_{s}\in\mathcal{X}\) and concepts \(h_{1},\ldots,h_{s}\in\mathcal{H}\) satisfying DIS\((\{h,h_{i}\})\cap\{x_{1},\ldots,x_{s}\}=\{x_{i}\}\), for every \(1\leq i\leq s\). (We say \(\{x_{1},\ldots,x_{s}\}\) is a star set of \(\mathcal{H}\) centered at \(h\).) If no such largest integer \(s\) exists, we define \(\mathfrak{s}_{h}=\infty\). The star number of \(\mathcal{H}\), denoted by \(\mathfrak{s}(\mathcal{H})\) or \(\mathfrak{s}_{\mathcal{H}}\), is defined to be the maximum possible cardinality of a star set of \(\mathcal{H}\), or \(\mathfrak{s}_{\mathcal{H}}=\infty\) if no such maximum exists._

**Remark 4**.: _From this definition, it is clear that the star number \(\mathfrak{s}_{\mathcal{H}}\) of \(\mathcal{H}\) satisfies \(\mathfrak{s}_{\mathcal{H}}\geq\text{VC}(\mathcal{H})\). Indeed, any set \(\{x_{1},\ldots,x_{d}\}\) that gathered by \(\mathcal{H}\) is also a star set of \(\mathcal{H}\) based on the following reasoning: Since \(\{x_{1},\ldots,x_{d}\}\) is shattered by \(\mathcal{H}\), there exists \(h\in\mathcal{H}\) such that \(h(x_{1})=\cdots=h(x_{d})=0\). Moreover, for any \(i\in[d]\), there exists \(h_{i}\in\mathcal{H}\) satisfying \(h_{i}(x_{i})=1\) and \(h_{i}(x_{j})=0\) for all \(j\neq i\). An immediate implication is that a VC-eluder sequence is always a star-eluder sequence (see Definition 6 and Definition 7 below)._

With these basic definitions in hand, we next define the three aforementioned sequences:

**Definition 5** (**Eluder sequence)**.: _Let \(\mathcal{H}\) be a concept class, we say that \(\mathcal{H}\) has an eluder sequence \(\{(x_{1},y_{1}),\ldots,(x_{d},y_{d})\}\), if it is realizable and for every integer \(k\in[d]\), there exists \(h_{k}\in\mathcal{H}\) such that \(h_{k}(x_{i})=y_{i}\) for all \(i<k\) and \(h_{k}(x_{k})\neq y_{k}\). The eluder dimension of \(\mathcal{H}\), denoted by \(E(\mathcal{H})\), is defined to be the largest integer \(d\geq 1\) such that \(\mathcal{H}\) has an eluder sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots,(x_{d},y_{d})\}\). If no such largest \(d\) exists, we say \(\mathcal{H}\) has an infinite eluder sequence and define \(E(\mathcal{H})=\infty\). We say an infinite eluder sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) is centered at \(h\), if \(h(x_{i})=y_{i}\) for all \(i\in\mathbb{N}\)._

**Remark 5**.: _It has been proved that \(\max\{\mathfrak{s}_{\mathcal{H}},\log{(\textit{LD}(\mathcal{H}))}\}\leq\textit{E} (\mathcal{H})\leq 4^{\max\{\mathfrak{s}_{\mathcal{H}},2^{\textit{LD}( \mathcal{H})}\}}\)[Li et al., 2022, Thm.8], where \(\textit{LD}(\mathcal{H})\) is the Littestone dimension of \(\mathcal{H}\). Moreover, the very recent work of Hanneke (2024) proved that \(\textit{E}(\mathcal{H})\leq|\mathcal{H}|\leq 2^{\mathfrak{s}_{\mathcal{H}} \textit{LD}(\mathcal{H})}\), which implies that any concept class with finite star number and finite Littlestone dimension must be a finite class._

Before proceeding to the next two definitions, we define a sequence of integers \(\{n_{k}\}_{k\in\mathbb{N}}\) as \(n_{1}=0\), \(n_{k}:=\binom{k}{2}\) for all \(k>1\), which satisfies \(n_{k+1}-n_{k}=k\) for all \(k\in\mathbb{N}\).

**Definition 6** (Star-eluder sequence).: _Let \(\mathcal{H}\) be a concept class and \(h\) be a classifier. We say that \(\mathcal{H}\) has an infinite star-eluder sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) centered at \(h\), if it is realizable and for every integer \(k\geq 1\), \(\{x_{n_{k}+1},\ldots,x_{n_{k}+k}\}\) is a star set of \(V_{n_{k}}(\mathcal{H})\) centered at \(h\)._

**Definition 7** (VC-eluder sequence).: _Let \(\mathcal{H}\) be a concept class and \(h\) be a classifier. We say that \(\mathcal{H}\) has an infinite VC-eluder sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) centered at \(h\), if it is realizable and labelled by \(h\), and for every integer \(k\geq 1\), \(\{x_{n_{k}+1},\ldots,x_{n_{k}+k}\}\) is a shattered set of \(V_{n_{k}}(\mathcal{H})\)._

**Remark 6**.: _In the definitions of eluder sequence and VC-eluder sequence, "\(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) centered at \(h\)" simply means the sequence is labelled by \(h\). However, the words "centered at" in the definition of star-eluder sequence is more meaningful. In this paper, we give them a uniform name in order to make Theorem 2 look consistent._

**Remark 7**.: _An infinite star-eluder (VC-eluder) sequence requires the version space to keep on having star (shattered) sets with infinitely increasing sizes. If the size cannot grow infinitely, the largest possible size of the star (shattered) set is called the star-eluder (VC-eluder) dimension (Definition 9), which plays an important role in our fine-grained analysis (Appendix C). To distinguish the notion of star-eluder (VC-eluder) sequence here from the \(d\)-star-eluder (\(d\)-VC-eluder) sequence defined in Appendix C, we may call the construction in Definition 6 an infinite strong star-eluder sequence, and the construction in Definition 7 an infinite strong VC-eluder sequence._

Proof Sketch of Theorem 1 and 2.: The proof of Theorem 1 is devided into two parts (Section 3 and Section 4). Roughly speaking, for each equivalence therein, we first characterize the exact universal rates by ERM via the three aforementioned sequences (see Theorems 3-6 in Section 3). We have to prove a lower bound together with an upper bound for the sufficiency since we are showing the "exact" universal rates. The lower bound is established by constructing a realizable distribution on the existent infinite sequence, and the derivation of upper bound is strongly related to the classical PAC theory. To prove the necessity, we will use the method of contradiction. Then in Section 4, we establish equivalent characterizations via those well-known complexity measures, whenever possible. Theorem 2 is an associated target-dependent version, and is directly proved by those corresponding lemmas in Section 3. The complete proof structure for Theorem 1 can be summarized as follow:

**For the first bullet**, we start by proving that \(\mathcal{H}\) is universally learnable with exact rate \(e^{-n}\) if and only if \(\mathcal{H}\) does not have an infinite eluder sequence (Theorem 3), and then we extend the equivalence by showing that \(\mathcal{H}\) does not have an infinite eluder sequence if and only if \(\mathcal{H}\) is a finite class (Lemma 8). **For the second bullet**, we prove that \(\mathcal{H}\) is universally learnable with exact rate \(1/n\) if and only if \(\mathcal{H}\) has an infinite eluder sequence but does not have an infinite star-eluder sequence (Theorem 4). Then the desired equivalence follows immediately from the first bullet. **For the third bullet**, we prove that \(\mathcal{H}\) is universally learnable with exact rate \(\log{(n)}/n\) if and only if \(\mathcal{H}\) has an infinite star-eluder sequence but does not have an infinite VC-eluder sequence (Theorem 5). The desired equivalence comes in conjunction with the claim that \(\mathcal{H}\) has an infinite VC-eluder sequence if and only if \(\mathcal{H}\) has infinite VC dimension (Lemma 9). Finally, **for the last bullet**, it suffices to prove that \(\mathcal{H}\) requires at least arbitrarily slow rates to be universally learned by ERM if and only if \(\mathcal{H}\) has an infinite VC-eluder sequence (Theorem 6). 

## 3 Exact universal rates

Sections 3 and 4 of this paper are devoted to the proof ideas of Theorems 1 and 2 with further details. In this section, we give a complete characterization of the four possible exact universal rates by ERM (\(e^{-n},1/n,\log{(n)}/n\) and arbitrarily slow rates) via the existence/nonexistence of the three combinatorial sequences defined in Section 2. For each of the following "if and only if" results (Theorems 3-6), we are required to prove both the sufficiency and the necessity. The sufficiency consists of both an upper bound and a lower bound since we are proving the exact universal rates. Thenecessity also follows simply by the method of contradiction, given the rates are exact. All technical proofs are deferred to Appendix D.1.

### Exponential rates

**Theorem 3**.: \(\mathcal{H}\) _is universally learnable by ERM with exact rate \(e^{-n}\) if and only if \(\mathcal{H}\) does not have an infinite eluder sequence._

The lower bound for sufficiency is straightforward and was established by Schuurmans (1997).

**Lemma 1** (\(e^{-n}\) **lower bound**).: _Given a concept class \(\mathcal{H}\), for any learning algorithm \(\hat{h}_{n}\), there exists a realizable distribution \(P\) with respect to \(\mathcal{H}\) such that \(\mathbb{E}[er_{P}(\hat{h}_{n})]\geq 2^{-(n+2)}\) for infinitely many \(n\), which implies that \(\mathcal{H}\) is not universally learnable at rate faster than exponential rate \(e^{-n}\)._

**Remark 8**.: _Note that this lower bound is actually stronger than desired in a sense that it holds for any learning algorithm (not necessarily for ERM algorithms)._

**Lemma 2** (\(e^{-n}\) **upper bound**).: _If \(\mathcal{H}\) does not have an infinite eluder sequence (centered at \(h^{*}\)), then \(\mathcal{H}\) (\(h^{*}\)) is universally learnable by ERM at rate \(e^{-n}\)._

Proof of Theorem 3.: The sufficiency follows directly from the lower bound in Lemma 1 together with the upper bound in Lemma 2. Furthermore, Lemma 3 in Section 3.2 proves that the existence of an infinite eluder sequence leads to a linear lower bound of the ERM universal rates. Therefore, the necessity follows by using the method of contradiction. 

### Linear rates

**Theorem 4**.: \(\mathcal{H}\) _is universally learnable by ERM with exact rate \(1/n\) if and only if \(\mathcal{H}\) has an infinite eluder sequence but does not have an infinite star-eluder sequence._

**Lemma 3** (\(1/n\) **lower bound**).: _If \(\mathcal{H}\) has an infinite eluder sequence centered at \(h^{*}\), then \(h^{*}\) is not universally learnable by ERM at rate faster than \(1/n\)._

**Lemma 4** (\(1/n\) **upper bound**).: _If \(\mathcal{H}\) does not have an infinite star-eluder sequence (centered at \(h^{*}\)), then \(\mathcal{H}\) (\(h^{*}\)) is universally learnable by ERM at rate \(1/n\)._

Proof of Theorem 4.: To prove the sufficiency, on one hand, the existence of an infinite eluder sequence implies a linear lower bound based on Lemma 3. On the other hand, if \(\mathcal{H}\) does not have an infinite star-eluder sequence, Lemma 4 yields a linear upper bound. The necessity can be proved by the method of contradiction. Concretely, if either of the two conditions fail, the universal rates will be either \(e^{-n}\) or at least \(\log{(n)}/n\), based on Lemma 2 in Section 3.1 and Lemma 5 in Section 3.3. 

### \(\log{(n)}/n\) rates

**Theorem 5**.: \(\mathcal{H}\) _is universally learnable by ERM with exact rate \(\log{(n)}/n\) if and only if \(\mathcal{H}\) has an infinite star-eluder sequence but does not have an infinite VC-eluder sequence._

**Lemma 5** (\(\log{(n)}/n\) **lower bound**).: _If \(\mathcal{H}\) has an infinite star-eluder sequence centered at \(h^{*}\), then \(h^{*}\) is not universally learnable by ERM at rate faster than \(\log{(n)}/n\)._

**Remark 9**.: _Note that the conclusion in Remark 5 explains why the intersection of "infinite Littlestone classes" and "classes with finite star number" is empty in Figure 1. However, we mention in Remark 3 that infinite star number does not guarantee an infinite star-eluder sequence (see Appendix B.3 for details). Hence, Remark 5 cannot explain why the intersection of "infinite Littlestone classes" and "classes that are learnable at linear rate by ERM" is also empty. To address this problem, we give out the following additional result:_

**Proposition 1**.: _Any infinite concept class \(\mathcal{H}\) has either an infinite star-eluder sequence or infinite Littlestone dimension._

**Lemma 6** (\(\log{(n)}/n\) **upper bound**).: _If \(\mathcal{H}\) does not have an infinite VC-eluder sequence (centered at \(h^{*}\)), then \(\mathcal{H}\) (\(h^{*}\)) is universally learnable by ERM at \(\log{(n)}/n\) rate._

Proof of Theorem 5.: To prove the sufficiency, on one hand, if \(\mathcal{H}\) has an infinite star-eluder sequence, the universal rates have a \(\log{(n)}/n\) lower bound based on Lemma 5. On the other hand, if \(\mathcal{H}\) does not have an infinite VC-eluder sequence, then Lemma 6 yields a \(\log{(n)}/n\) upper bound. The necessity can be proved using the method of contradiction based on Lemma 4 in Section 3.2 and Lemma 7 in Section 3.4 below. 

### Arbitrarily slow rates

**Theorem 6**.: \(\mathcal{H}\) _requires at least arbitrarily slow rates to be learned by ERM if and only if \(\mathcal{H}\) has an infinite VC-eluder sequence._

Proof of Theorem 6.: Given the necessity proved by Lemma 6 in Section 3.3, it suffices to prove the sufficiency, which is completed by the following Lemma 7. 

**Lemma 7** (**Arbitrarily slow rates**).: _If \(\mathcal{H}\) has an infinite VC-eluder sequence centered at \(h^{*}\), then \(h^{*}\) requires at least arbitrarily slow rates to be universally learned by ERM._

## 4 Equivalent characterizations

In Section 3, it has been shown that the eluder sequence, the star-eluder sequence and the VC-eluder sequence are the correct characterizations of the exact universal learning rates by ERM. However, the definitions to them are somewhat non-intuitive. Therefore, in this section, we aim to build connections between these combinatorial sequences and some well-understood complexity measures, which will then give rise to our Theorem 1. Concretely, we have the following two equivalences (see Appendix D.2 for their complete proofs).

**Lemma 8**.: \(\mathcal{H}\) _has an infinite eluder sequence if and only if \(|\mathcal{H}|=\infty\)._

**Lemma 9**.: \(\mathcal{H}\) _has an infinite VC-eluder sequence if and only if \(\text{VC}(\mathcal{H})=\infty\)._

Maybe surprisingly, unlike the above two equivalences, \(\mathfrak{s}_{\mathcal{H}}=\infty\) is inequivalent to the existence of an infinite star-eluder sequence. Indeed, it is straightforward from definition that if \(\mathcal{H}\) has an infinite star-eluder sequence, then it must have \(\mathfrak{s}_{\mathcal{H}}=\infty\). However, the converse is not true.

**Proposition 2**.: \(\mathfrak{s}_{\mathcal{H}}=\infty\) _if \(\mathcal{H}\) has an infinite star-eluder sequence. Moreover, there exist concept classes \(\mathcal{H}\) with \(\mathfrak{s}_{\mathcal{H}}=\infty\) but does not have any infinite star-eluder sequence._

**Remark 10**.: _Based on the results in Section 3, the proposition essentially states that the gap between \(1/n\) and \(\log{(n)}/n\) exact universal rates by ERM is not characterized by the star number \(\mathfrak{s}_{\mathcal{H}}\). We wonder whether there is some other simple combinatorial quantity that is determinant to this gap, which would be an valuable direction for future work._

Why is the case of star-eluder sequence different from the other two structures? We suspect that such a distinction may arise from the following: unlike the eluder sequence and the VC-eluder sequence, the centered concept of a star-eluder sequence is much more meaningful (see Remark 6). Concretely, within its definition, the set of the following \(k\) points is not only required to be a star set of the version space \(V_{n_{k}}(\mathcal{H})\), but is required to be centered at the same labelling target. This intuitively implies that there might exists a class such that for arbitrarily large integer \(k\), it can witness a star set of size \(k\), but with a \(k\)-specified center (for different \(k\)). Such a class (e.g. Examples 19, 20 in Appendix B.3) does have infinite star number but will not have an infinite star-eluder sequence. Indeed, the relations between those star-related notions (star number, star-eluder dimension, star set and star eluder sequence) turn out to be more complicated than expected, and we leave it to Appendix B.3.

## 5 Appendix Summary

Due to page limitation, we leave some interesting results as well as all the proofs to Appendices, which are briefly summarized below. Given extra required notations and definitions in Appendix A and related technical lemmas in Appendix E, the main body of supplements consists of three parts, namely, Appendices B, C and D.

Specifically, Appendix B contains three sub-parts. In Appendix B.1, we provide direct mathematical analysis (without using our characterization in Section 1.2) for those basic examples in Section 1.1. In Appendix B.2, we provide details of examples that appeared in Section 1.2. These examples are carefully constructed, providing evidence that ERM algorithms cannot guarantee the optimal universal rates (Bousquet et al., 2021). In Appendix B.3, we construct nuanced examples to distinguish between the following notions: star number \(\mathfrak{s}_{\mathcal{H}}\) (Definition 4), the star-eluder dimension \(\text{SE}(\mathcal{H})\) (Definition 9), star set (Definition 4) and star eluder sequence (Definition 6). These examples will convince the readers why our characterization in Theorem 1 uses the star eluder sequence rather than the star number (see our discussions in Remarks 3 and 10).

Appendix C presents a fine-grained analysis of the asymptotic rate of decay of the universal learning curves by ERM, whenever possible. This will be an analogy to the optimal fine-grained universal learning curves studied in Bousquet et al. (2023). Concretely, we provide a characterization of sharp distribution-free constant factors of the ERM universal rates. Our characterization of these constant factors is based on two newly-developed combinatorial dimensions, namely, the _star-eluder dimension_ (or SE dimension) and the _VC-eluder dimension_ (or VCE dimension) (Definition 9). We say "whenever possible" because distribution-free constants are unavailable for certain cases (see our discussion in Remark 16). Such a characterization can be considered as a refinement to the classical PAC theory, in a sense that it is sometimes better but only asymptotically-valid.

Finally, Appendix D includes all the missing proofs for the theorems and lemmas that have shown up in previous sections.

## 6 Conclusion and Future Directions

In this paper, we reveal a fundamental tetrachotomy of the universal learning rates by the ERM principle and provide a complete characterization of the exact universal rates via certain appropriate complexity structures. Additionally, by introducing new combinatorial dimensions, we are able to characterize sharp asymptotically-valid constant factors for these rates, whenever possible. While only the realizable case is considered in this paper, we believe analogous results can be extend to different learning scenarios such as the agnostic case. Generalizing the results from binary classification to multiclass classification would be another valuable future direction. Moreover, since this paper considers the "worst-case" ERM in its nature, studying the universal rates of the "best-case" ERM is also an interesting problem which we leave for future work.

## References

* Antos and Lugosi (1996) Antos, A. and Lugosi, G. (1996), "Strong minimax lower bounds for learning," in _Proceedings of the 9th Annual Conference on Computational Learning Theory_, pp. 303-309.
* Audibert and Tsybakov (2007) Audibert, J.-Y. and Tsybakov, A. B. (2007), "Fast learning rates for plug-in classifiers," _The Annals of Statistics_, 35, 608-633.
* Auer and Ortner (2007) Auer, P. and Ortner, R. (2007), "A new PAC bound for intersection-closed concept classes," _Machine Learning_, 66, 151-163.
* Benedek and Itai (1988) Benedek, G. M. and Itai, A. (1988), "Nonuniform learnability," in _Automata, Languages and Programming: 15th International Colloquium Tampere, Finland, July 11-15, 1988 Proceedings 15_, Springer, pp. 82-92.
* Blumer et al. (1989) Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. (1989), "Learnability and the Vapnik-Chervonenkis dimension," _Journal of the ACM (JACM)_, 36, 929-965.
* Bousquet et al. (2023) Bousquet, O., Hanneke, S., Moran, S., Shafer, J., and Tolstikhin, I. (2023), "Fine-grained distribution-dependent learning curves," in _Proceedings of the 36th Annual Conference on Learning Theory_, PMLR, pp. 5890-5924.
* Bousquet et al. (2021) Bousquet, O., Hanneke, S., Moran, S., Van Handel, R., and Yehudayoff, A. (2021), "A theory of universal learning," in _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pp. 532-541.
* Cohn et al. (1994) Cohn, D., Atlas, L., and Ladner, R. (1994), "Improving generalization with active learning," _Machine Learning_, 15, 201-221.
* Cohn et al. (2012)Cohn, D. and Tesauro, G. (1990), "Can neural networks do better than the Vapnik-Chervonenkis bounds?" _Advances in Neural Information Processing Systems_, 3.
* Cohn and Tesauro (1992) -- (1992), "How tight are the Vapnik-Chervonenkis bounds?" _Neural Computation_, 4, 249-269.
* El-Yaniv and Wiener (2012) El-Yaniv, R. and Wiener, Y. (2012), "Active Learning via Perfect Selective Classification." _Journal of Machine Learning Research_, 13, 255-279.
* Gine and Koltchinskii (2006) Gine, E. and Koltchinskii, V. (2006), "Concentration inequalities and asymptotic results for ratio type empirical processes," _The Annals of Probability_, 34, 1143-1216.
* Hanneke (2009) Hanneke, S. (2009), _Theoretical foundations of active learning_, Carnegie Mellon University.
* Hanneke (2011) -- (2011), "Rates of convergence in active learning," _The Annals of Statistics_, 39, 333-361.
* Hanneke (2014) -- (2014), "Theory of disagreement-based active learning," _Foundations and Trends in Machine Learning_, 7, 131-309.
* Hanneke (2016a) -- (2016a), "The optimal sample complexity of PAC learning," _Journal of Machine Learning Research_, 17, 1-15.
* Hanneke (2016b) -- (2016b), "Refined error bounds for several learning algorithms," _Journal of Machine Learning Research_, 17, 1-55.
* Hanneke (2024) -- (2024), "The Star Number and Eluder Dimension: Elementary Observations About the Dimensions of Disagreement," in _Proceedings of the 37th Annual Conference on Learning Theory_.
* Hanneke et al. (2022) Hanneke, S., Karbasi, A., Moran, S., and Velegkas, G. (2022), "Universal rates for interactive learning," _Advances in Neural Information Processing Systems_, 35, 28657-28669.
* Hanneke et al. (2023) Hanneke, S., Moran, S., and Zhang, Q. (2023), "Universal Rates for Multiclass Learning," in _Proceedings of the 36th Annual Conference on Learning Theory_, PMLR, pp. 5615-5681.
* Hanneke and Yang (2015) Hanneke, S. and Yang, L. (2015), "Minimax analysis of active learning." _Journal of Machine Learning Research_, 16, 3487-3602.
* Kalavasis et al. (2022) Kalavasis, A., Velegkas, G., and Karbasi, A. (2022), "Multiclass learnability beyond the pac framework: Universal rates and partial concept classes," _Advances in Neural Information Processing Systems_, 35, 20809-20822.
* Koltchinskii and Beznosova (2005) Koltchinskii, V. and Beznosova, O. (2005), "Exponential convergence rates in classification," in _Proceedings of International Conference on Computational Learning Theory_, Springer, pp. 295-307.
* Li et al. (2022) Li, G., Kamath, P., Foster, D. J., and Srebro, N. (2022), "Understanding the eluder dimension," _Advances in Neural Information Processing Systems_, 35, 23737-23750.
* Littlestone and Warmuth (1986) Littlestone, N. and Warmuth, M. (1986), "Relating data compression and learnability," _Unpublished manuscript_.
* Mitchell (1977) Mitchell, T. M. (1977), "Version spaces: A candidate elimination approach to rule learning," in _Proceedings of the 5th International Joint Conference on Artificial Intelligence_, vol. 1, pp. 305-310.
* Pillaud-Vivien et al. (2018) Pillaud-Vivien, L., Rudi, A., and Bach, F. (2018), "Exponential convergence of testing error for stochastic gradient methods," in _Conference on Learning Theory_, PMLR, pp. 250-296.
* Ramsey (1987) Ramsey, F. P. (1987), "On a problem of formal logic," in _Classic Papers in Combinatorics_, Springer, pp. 1-24.
* Sauer (1972) Sauer, N. (1972), "On the density of families of sets," _Journal of Combinatorial Theory, Series A_, 13, 145-147.
* Schuurmans (1997) Schuurmans, D. (1997), "Characterizing rational versus exponential learning curves," _Journal of Computer and System Sciences_, 55, 140-160.
* Schuurmans (2015)Shalev-Shwartz, S. and Ben-David, S. (2014), _Understanding machine learning: From theory to algorithms_, Cambridge university press.
* Valiant (1984) Valiant, L. G. (1984), "A theory of the learnable," _Communications of the ACM_, 27, 1134-1142.
* van Handel (2013) van Handel, R. (2013), "The universal Glivenko-Cantelli property," _Probability and Related Fields_, 155, 911-934.
* Vapnik and Chervonenkis (1974) Vapnik, V. and Chervonenkis, A. (1974), "Theory of pattern recognition," _Nauka, Moscow_.
* Vapnik and Chervonenkis (1971) Vapnik, V. N. and Chervonenkis, A. Y. (1971), "On uniform convergence of the frequencies of events to their probabilities," _Theory of Probability and its Applications_, 16, 264-280.
* Wiener et al. (2015) Wiener, Y., Hanneke, S., and El-Yaniv, R. (2015), "A compression technique for analyzing disagreement-based active learning." _Journal of Machine Learning Research_, 16, 713-745.

Preliminaries

**Notation 1**.: _We denote by \(\mathbb{N}\) the set of all natural numbers \(\{0,1,\ldots\}\). For any \(n\in\mathbb{N}\), we denote \([n]:=\{1,\ldots,n\}\)._

**Notation 2**.: _For any \(x>0\), we redefine \(\ln{(x)}:=\ln{(x\lor e)}\) and \(\log{(x)}:=\log_{2}{(x\lor 2)}\). Moreover, for correctness, we also adopt the conventions that \(\ln{(0)}=\log{(0)}=0\), \(0\ln{(\infty)}=0\log{(\infty)}=0\). After then, it is reasonable to define \(0\ln{(0/0)}=0\log{(0/0)}=0\)._

**Notation 3**.: _For any \(\mathbb{R}\)-valued functions \(f\) and \(g\), we write \(f(x)\lesssim g(x)\) if there exists a finite numerical constant \(c>0\) such that \(f(x)\leq c\cdot g(x)\) for all \(x\in\mathbb{R}\). For example, \(\ln{(x)}\lesssim\log{(x)}\) and \(\log{(x)}\lesssim\ln{(x)}\)._

**Notation 4**.: _Let \(\mathcal{X}\) be an instance space, we write \(h_{\text{all-}\partial_{\mathcal{X}}}\) and \(h_{\text{all-}\varmid\varmathbb{x}}\) to denote the hypotheses that output all zero labels and all one labels, respectively, that is, \(h_{\text{all-}\partial_{\mathcal{X}}}(x)=0,h_{\text{all-}\varmathbb{x}}(x)=1, \forall x\in\mathcal{X}\)._

**Notation 5**.: _For an infinite union of spaces \((\mathcal{X}_{1}\cup\mathcal{X}_{2}\cup\cdots)\) and an integer \(k\), we write \(\mathcal{X}_{<k}\) to denote the finite union of prefix \((\mathcal{X}_{1}\cup\cdots\cup\mathcal{X}_{k-1})\) and write \(\mathcal{X}_{>k}\) to denote the infinite union of suffix \((\mathcal{X}_{k+1}\cup\mathcal{X}_{k+2}\cup\cdots)\)._

**Definition 8** (**Empirical risk minimization**).: _Let \(\mathcal{H}\) be a concept class on an instance space \(\mathcal{X}\). For every \(n\in\mathbb{N}\), let \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\in(\mathcal{X}\times\{0,1\})^{n}\) be a set of samples. A learning algorithm that outputs \(\{\hat{h}_{n}\}_{n\in\mathbb{N}}\) is called an Empirical Risk Minimization (ERM) algorithm, if it satisfies \(\hat{h}_{n}\in\arg\min_{h\in\mathcal{H}}\hat{\text{er}}_{S_{n}}(h):=\arg\min_{ h\in\mathcal{H}}\{\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}(h(x_{i})\neq y_{i})\}\) for all \(n\in\mathbb{N}\), where \(\hat{\text{er}}_{S_{n}}(\hat{h}_{n})\) is called the empirical error rate of \(\hat{h}_{n}\) on \(S_{n}\). It is clear that \(\hat{\text{er}}_{S_{n}}(\hat{h}_{n})=0\) when \(P\in\text{RE}(\mathcal{H})\)._

## Appendix B Detailed examples

In this appendix section, we provide further examples. Specifically, in Appendix B.1, we present direct analysis (without using our newly-developed characterization) of each example illustrated in Section 1.1. The aim of the examples in Appendix B.2 is to reveal that ERM algorithms can sometimes be optimal but sometimes not in a universal learning framework, and compare their performance with the optimal universal learning algorithms. Finally, we also provide additional examples related to the star number in Appendix B.3 as complements to Proposition 2 in Section 4.

### Details of examples in Section 1.1

We provide direct analysis to the examples illustrated in Section 1.1 without using the characterization in Theorem 1. Concretely, Examples 8 and 9 illustrate scenarios where linear universal rates occur. Example 10 specifies a case where universal rate matches uniform rate by ERM as \(\log{(n)}/n\). Example 11 specifies a case where extremely fast universal learning is achievable, but where some bad ERM algorithms can give rise to arbitrarily slow rates.

**Example 7** (**Example 1 restated**).: _Any finite class \(\mathcal{H}\) is universally learnable at exponential rate by ERM. To show this, for any realizable distribution \(P\) with respect to \(\mathcal{H}\), we have_

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right] \leq \mathbb{P}\left(\exists h\in\mathcal{H}:\text{er}_{P}(h)>0,\hat{ \text{er}}_{S_{n}}(h)=0\right)\] \[\leq \sum_{h\in\mathcal{H}:\text{er}_{P}(h)>0}\mathbb{P}\left(\hat{ \text{er}}_{S_{n}}(h)=0\right)\] \[= \sum_{h\in\mathcal{H}:\text{er}_{P}(h)>0}(1-\text{er}_{P}(h))^{n}\] \[\leq |\mathcal{H}|\cdot\left(1-\min_{h\in\mathcal{H}:\text{er}_{P}(h)> 0}\text{er}_{P}(h)\right)^{n}\] \[\overset{1-t}{\leq} \overset{\leq e^{-t}}{\leq} |\mathcal{H}|\cdot\exp\left\{-\left(\min_{h\in\mathcal{H}:\text{er}_{P }(h)>0}\text{er}_{P}(h)\right)\cdot n\right\}.\]

**Example 8** (**Example 2 restated**).: _Let \(\mathcal{H}_{\text{thresh},\mathbb{N}}:=\{h_{t}:t\in\mathbb{N}\}\) be the class of all threshold classifiers on the space of natural numbers defined by \(h_{t}(x):=\mathbbm{1}(x\geq t)\). \(\mathcal{H}_{\text{thresh},\mathbb{N}}\) is universally learnable at exponential rate since this concept class does not have an infinite Littlestone tree (Bousquet et al., 2021). In the following part, we show that the worst-case ERM cannot achieve such exponential rate, but has a rate \(1/n\)._

_Let \(h_{t^{*}}\in\mathcal{H}_{\text{thresh},\mathbb{N}}\) be the target hypothesis. Given a dataset \(S_{n}\), let \(h_{\hat{t}}=\text{ERM}(S_{n})\) be the output of an ERM algorithm. For any realizable distribution \(P\) satisfying \(P\{(t,0)\}=1\) for all \(t<t^{*}\) and \(P\{(t,1)\}=1\) for all \(t\geq t^{*}\), we define_

\[t_{l}:=\max\left\{t<t^{*}:P(t)>0\right\}.\]

_According to the definition of threshold classifiers, if the dataset \(S_{n}\) contains at least a copy of both \(t_{l}\) and \(t^{*}\), then \(\text{er}_{P}(h_{\hat{t}})=0\). Therefore, we have_

\[\mathbb{E}\left[\text{er}_{P}(h_{\hat{t}})\right]\leq\mathbb{P}\left(\text{ er}_{P}(h_{\hat{t}})>0\right)\leq(1-P(t^{*}))^{n}+(1-P(t_{l}))^{n}.\]

_Note that for any \(\epsilon\in(0,1)\), \(1-\epsilon\leq e^{-\epsilon}\), it follows immediately that_

\[\mathbb{E}\left[\text{er}_{P}(h_{\hat{t}})\right]\leq(1-P(t^{*}))^{n}+(1-P(t _{l}))^{n}\leq e^{-nP(t^{*})}+e^{-nP(t_{l})}\leq 2e^{-n\cdot\min\{P(t^{*}),P(t_{l}) \}}.\]

_However, let us consider a distribution \(P\) satisfying \(P\{(t,0)\}=2^{-t}\) and \(P\{(t,1)\}=0\) for all \(t\in\mathbb{N}\). Note that \(P\) is also realizable with respect to \(\mathcal{H}_{\text{thresh},\mathbb{N}}\) according to the definition, that is_

\[\inf_{h\in\mathcal{H}_{\text{thresh},\mathbb{N}}}\text{er}_{P}(h)=\inf_{t\in \mathbb{N}}\text{er}_{P}(h_{t})=\inf_{t\in\mathbb{N}}2^{-t}=0.\]

_Given a dataset \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\), let \(t_{n}:=\max_{i\in[n]}x_{i}\) be the largest point in the dataset, it is straightforward that the worst-case ERM outputs \(h_{\widehat{t_{n}+1}}\), and we have that_

\[\mathbb{E}\left[\text{er}_{P}\left(h_{\widehat{t_{n}+1}}\right)\right]=\sum_{ t\geq 1}2^{-t-1}\cdot\mathbb{P}\left(\max_{i\in[n]}x_{i}=t\right)=\sum_{t\geq 1}2^{-t-1 }\left[\left(1-2^{-t}\right)^{n}-\left(1-2^{-(t-1)}\right)^{n}\right].\]

_On one hand, we can lower bound the above infinite series by_

\[\sum_{t\geq 1}2^{-t-1}\left[\left(1-2^{-t}\right)^{n}-\left(1-2^{-( t-1)}\right)^{n}\right]\] \[\geq \sum_{t=1}^{\lfloor\log n\rfloor}2^{-t-1}\left[\left(1-2^{-t} \right)^{n}-\left(1-2^{-(t-1)}\right)^{n}\right]\] \[\geq \frac{1}{2n}\sum_{t=1}^{\lfloor\log n\rfloor}\left[\left(1-2^{-t }\right)^{n}-\left(1-2^{-(t-1)}\right)^{n}\right]\] \[\geq \frac{1}{2n}\left(1-\frac{2}{n}\right)^{n}\geq\frac{1}{18n},\text { for infinitely many }n.\]

_This implies that \(\mathcal{H}_{\text{thresh},\mathbb{N}}\) is not learnable by ERM at rate faster than \(1/n\). On the other hand, we have the following upper bound for all \(n\):_

\[\sum_{t\geq 1}2^{-t-1}\left[\left(1-2^{-t}\right)^{n}-\left(1-2^{-(t-1)} \right)^{n}\right]\leq\sum_{t\geq 1}2^{-t}\left(1-2^{-t}\right)^{n}\leq\int_{0}^{1} \epsilon(1-\epsilon)^{n}d\epsilon=\frac{1}{n+1},\]

_which implies that \(\mathcal{H}_{\text{thresh},\mathbb{N}}\) is indeed learnable by ERM at linear rate \(1/n\). In conclusion, \(\mathcal{H}_{\text{thresh},\mathbb{N}}\) is universally learnable by ERM with exact \(1/n\) rate._

**Example 9** (**Threshold classifier on \(\mathbb{R}\)**).: _This example serves as a complement to Example 8. Here, we show that ERM algorithms can sometimes be optimal for universal learning. Specifically, let \(\mathcal{H}_{\text{thresh},\mathbb{R}}:=\{h_{t}:t\in\mathbb{R}\}\) be the class of all threshold classifiers on the real line defined by \(h_{t}(x):=\mathbbm{1}(x\geq t),\forall x\in\mathbb{R}\). It has been shown that \(\mathcal{H}_{\text{thresh},\mathbb{R}}\) is universally learnable with optimal linear rate (Schuurmans, 1997)._

_To show that \(\mathcal{H}_{\text{thresh},\mathbb{R}}\) is also universally learnable with exact linear rate by ERM, we only need to prove an upper bound. To this end, let \(h_{t^{*}}\in\mathcal{H}_{\text{thresh},\mathbb{R}}\) be the target hypothesis. For any realizable distribution \(P\) satisfying \(P\{(t,0)\}=1\) for all \(t<t^{*}\) and \(P\{(t,1)\}=1\) for all \(t\geq t^{*}\), a dataset \(S_{n}\) and \(\epsilon\in(0,1)\), let \(h_{\hat{t}}=\text{ERM}(S_{n})\) be the output of an ERM algorithm. Now we define \(A\) and \(B\) be the minimal regions left and right to \(t^{*}\in\mathbb{R}\) such that \(\mathbb{P}(A)=\mathbb{P}(B)=\epsilon\). If at least one of _and \(B\) does not contain any sample, then the worst-case ERM can output \(h_{\hat{t}}\in\mathcal{H}_{\text{thresh},\mathbb{R}}\) such that \(\text{er}_{P}(h_{\hat{t}})\geq\epsilon\). Therefore, it follows that_

\[\mathbb{E}\left[\text{er}_{P}(h_{\hat{t}})\right]=\int_{0}^{1}\mathbb{P}\left( \text{er}_{P}(h_{\hat{t}})\geq\epsilon\right)d\epsilon\leq\int_{0}^{1}2(1- \epsilon)^{n}d\epsilon\leq\int_{0}^{1}2e^{-n\epsilon}d\epsilon\leq\frac{2}{n}.\]

_Note that such analysis is also applicable to the realizable distribution with the target concept \(h_{\text{all-0}\gamma_{*}}\). Therefore, we have that \(\mathcal{H}_{\text{thresh},\mathbb{R}}\) is universally learnable by ERM with exact rate \(1/n\)._

**Example 10** (**Example 3 restated**).: _Let \(\mathcal{X}=\mathbb{N}\) and define \(\mathcal{H}_{\text{singleton},\mathbb{N}}:=\{h_{t}:t\in\mathcal{X}\}\) be the class of all singletons on \(\mathcal{X}\), where \(h_{t}(x):=1(x=t)\), for all \(x\in\mathcal{X}\). It is clear that \(\text{VC}(\mathcal{H}_{\text{singleton},\mathbb{N}})=1\). Note that \(\mathcal{H}_{\text{singleton},\mathbb{N}}\) is universally learnable at exponential rate since it does not have an infinite Littlestone tree (Actually, we have \(\text{LD}(\mathcal{H}_{\text{singleton},\mathbb{N}})=1\)). In the following part, we show that the worst-case ERM algorithm has an exact universal rate \(\log{(n)}/n\)._

_To get the exact rate by ERM on universally learning \(\mathcal{H}_{\text{singleton},\mathbb{N}}\), we consider a marginal uniform distribution over \(\{1,2,\ldots,1/\epsilon\}\) with all zero labels with \(\epsilon\in(0,1)\), if the dataset \(S_{n}\) does not have a copy of a point \(1\leq x\leq 1/\epsilon\), the worst-case ERM can label 1 at \(x\), and thus has an error rate \(\text{er}_{P}(\hat{h}_{n})\geq\epsilon\). Based on the Coupon Collector's Problem, we know that to have \(\mathbb{E}[\text{er}_{P}(\hat{h}_{n})]\leq\epsilon\), we need \(n=\Omega(\epsilon^{-1}\log(1/\epsilon))\). In other words, \(\mathbb{E}[\text{er}_{P}(\hat{h}_{n})]\geq\Omega(\frac{\log n}{n})\), that is, \(\mathcal{H}_{\text{singleton},\mathbb{N}}\) is not universally learnable by ERM at rate faster than \(\log{(n)}/n\). Finally, the classical PAC theory yields the same upper bound, and thus \(\log{(n)}/n\) is tight._

**Example 11** (**Example 4 restated**).: _Let \(\mathcal{X}=\bigcup_{i\in\mathbb{N}}\mathcal{X}_{i}\) be the disjoint union of finite sets with \(|\mathcal{X}_{i}|=2^{i}\). For each \(i\in\mathbb{N}\), let_

\[\mathcal{H}_{i}:=\left\{h_{S}:=1_{S}:S\subseteq\mathcal{X}_{i},|S|\geq 2^{i-1} \right\},\]

_and consider the concept class \(\mathcal{H}=\bigcup_{i\in\mathbb{N}}\mathcal{H}_{i}\). In the following part, we show that the worst-case ERM can be arbitrarily slow in learning this class._

_Given any rate function \(R(n)\to 0\), let \(\{n_{t}\}_{t\geq 1}\) and \(\{i_{t}\}_{t\geq 1}\) be two strictly increasing sequences such that \(\{p_{t}:=2^{i_{t}-2}/n_{t},\forall t\geq 1\}\) satisfies_

\[\{p_{t}\}_{t\geq 1}\text{ is decreasing },\sum_{t\geq 1}p_{t}\leq 1\text{ and }p_{t}\geq 4R(n_{t}).\]

_We consider any ERM algorithm with the following property: if the data \(S_{n}=\{(x_{i},y_{i})\}_{i=1}^{n}\) satisfies \(y_{i}=0\) for all \(i\in[n]\), outputs \(\hat{h}_{n}\in\mathcal{H}_{i\tau_{n}}\) with_

\[T_{n}:=\min\left\{t:\exists h\in\mathcal{H}_{i}\text{, s.t. }h(x_{1})=\cdots=h(x_{n})=0\right\}.\]

_We construct the following distribution \(P\):_

\[P\left\{(x,0)\right\}=2^{-i_{t}}p_{t},\text{ for all }x\in\mathcal{X}_{i_{t}},t \in\mathbb{N},\]

_where we set \(P\{(x^{{}^{\prime}},0)\}=1-\sum_{t\geq 1}p_{t}\) for some arbitrary choice of \(x^{{}^{\prime}}\notin\bigcup_{t\in\mathbb{N}}\mathcal{X}_{i_{t}}\). Since_

\[\inf_{h\in\mathcal{H}}\text{er}_{P}(h)=\inf_{i\in\mathbb{N}}\inf_{h\in \mathcal{H}_{i}}\text{er}_{P}(h)\leq\inf_{i\in\mathbb{N}}\text{er}_{P}(h_{ \mathcal{X}_{i}})\leq\inf_{i_{t}:t\in\mathbb{N}}\text{er}_{P}(h_{\mathcal{X}_{ i_{t}}})=\inf_{i_{t}:t\in\mathbb{N}}P\left\{(x,0):x\in\mathcal{X}_{i_{t}}\right\}=0,\]

_we know that \(P\) is realizable with respect to \(\mathcal{H}\). Finally, we claim that the ERM defined above behave poorly on \(P\) by showing \(\mathbb{E}[\text{er}_{P}(\hat{h}_{n})]\geq R(n)\) for infinitely many \(n\). To this end, note that for a dataset \(S_{n_{t}}=\{(x_{i},y_{i})\}_{i=1}^{n_{t}}\sim P^{n_{t}}\), and for any \(t\in\mathbb{N}\), it holds_

\[\mathbb{P}\left(T_{n_{t}}\leq t\right)\geq\mathbb{P}\left(\left|\{j\in[n_{t}]:x _{j}\in\mathcal{X}_{i_{t}}\}\right|\leq 2^{i_{t}-1}\right)=\mathbb{P}\left(\sum_{j=1}^{n_{t}} \mathbbm{1}\left\{x_{j}\in\mathcal{X}_{i_{t}}\right\}\leq 2^{i_{t}-1}\right)\geq\frac{1}{2},\]_where the last inequality follows from the Markov's inequality. Therefore,_

\[\mathbb{E}\left[er_{P}(\hat{h}_{n_{t}})\right] \geq 2R(n_{t})\cdot\mathbb{P}\left(er_{P}(\hat{h}_{n_{t}})\geq 2R(n_{t})\right)\] \[\stackrel{{ pt\geq 4R(n_{t})}}{{\geq}} 2R(n_{t})\cdot\mathbb{P}\left(er_{P}(\hat{h}_{n_{t}})\geq\frac{1}{2}p_{t}\right)\] \[\stackrel{{\text{Leff}}}{{\geq}} 2R(n_{t})\cdot\mathbb{P}\left(er_{P}(\hat{h}_{n_{t}})\geq\frac{1}{2}p_ {t}\Big{|}T_{n_{t}}\leq t\right)\mathbb{P}\left(T_{n_{t}}\leq t\right)\] \[\geq 2R(n_{t})\cdot\mathbb{P}\left(er_{P}(\hat{h}_{n_{t}})\geq\frac{1 }{2}p_{T_{n_{t}}}\Big{|}T_{n_{t}}\leq t\right)\mathbb{P}\left(T_{n_{t}}\leq t\right)\] \[\geq 2R(n_{t})\cdot\mathbb{P}\left(T_{n_{t}}\leq t\right)\] \[\geq R(n_{t}).\]

### Optimal universal rates versus exact universal rates by ERM

In this section, we provide evidence that ERM algorithms cannot guarantee the best achievable universal learning rates. Recall that the optimal universal learning rates and the associated characterization have been fully understood by Bousquet et al. (2021), which we present first as follow:

**Theorem 7** (Bousquet et al., 2021, Theorem 1.9).: _For every concept class \(\mathcal{H}\) with \(|\mathcal{H}|\geq 3\), the following hold:_

* \(\mathcal{H}\) _is universally learnable with optimal rate_ \(e^{-n}\) _if_ \(\mathcal{H}\) _does not have an infinite Littlestone tree._
* \(\mathcal{H}\) _is universally learnable with optimal rate_ \(1/n\) _if_ \(\mathcal{H}\) _has an infinite Littlestone tree but does not have an infinite (strong) VCL tree._
* \(\mathcal{H}\) _requires arbitrarily slow rates if_ \(\mathcal{H}\) _has an infinite (strong) VCL tree._

Based on our Theorem 1, to distinguish the optimal universal rates from the exact universal rates by ERM, we have to distinguish their corresponding characterizations. Indeed, those sequences we defined in Section 2 are strongly related to the Littlestone tree and the VCL tree in Theorem 7. According to the definitions, it is not hard to figure out all the following relations:

* Every branch of a Littlestone tree is an eluder sequence. Hence, if \(\mathcal{H}\) does not have an infinite eluder sequence, then \(\mathcal{H}\) must not have an infinite Littlestone tree, and thus can be universally learned with optimal exponential rate. However, there exists a class \(\mathcal{H}\) having an infinite eluder sequence but no infinite Littlestone tree (see Example 12 below). This implies that such a class cannot be universally learned by ERM at rate faster than \(1/n\), but can be learned by some other "optimal" learning algorithms at \(e^{-n}\) rate.
* Every branch of a (strong) VCL tree is a VC-eluder sequence, and also a star-eluder sequence. Therefore, if \(\mathcal{H}\) does not have an infinite star-eluder sequence, then it must not have an infinite VCL tree, and thus can be universally learned with optimal linear rate. However, there exists a concept class that has an infinite star-eluder sequence, but does not have an infinite VCL tree (see Example 14 below). Furthermore, there also exists a concept class that has an infinite star-eluder sequence, but does not even have an infinite Littlestone tree (see Example 13 below). These two examples imply that there exist classes that can not be universally learned by ERM at rate faster than \(\log{(n)}/n\), but can be learned by some other "optimal" learning algorithms at \(1/n\) or even \(e^{-n}\) rates.
* Moreover, there exists a concept class that has an infinite VC-eluder sequence, but does not have an infinite VCL tree (see Example 16 below), or even no infinite Littlestone tree (see Example 15 below). Such examples imply that there exist classes that require arbitrarily slow rates to be universally learned by ERM, but can be learned by some other "optimal" learning algorithms at \(1/n\) or even \(e^{-n}\) rates.

To summarize, we are able to illustrate all the distinctions as in the following table.

**Example 12** (**Infinite elder sequence but no infinite Littlestone tree)**.: _A simple example is given in Example 8, where \(\mathcal{H}=\mathcal{H}_{\text{thresh},\mathbb{N}}\) is the class of all threshold classifiers on \(\mathbb{N}\). Note that \(\mathcal{H}\) does not have an infinite Littlestone tree, but any infinite sequence \(\{(x_{1},0),(x_{2},0),\ldots\}\) with \(x_{1}<x_{2}<\ldots\) is an infinite eluder sequence of \(\mathcal{H}\) centered at \(h_{\text{all-}0\text{'s}}\). In particular, \(h_{\text{all-}0\text{'s}}\) is the only realizable target that allows an infinite eluder sequence. In other words, for \(\mathcal{H}\), all the realizable distribution with target concept \(h^{*}\in\mathcal{H}\) is universally learnable by ERM at exponential rate, except that special one \(h_{\text{all-}0\text{'s}}\), which matches our analysis within Example 8._

**Example 13** (**Infinite star-eluder sequence but no infinite Littlestone tree)**.: _Let \(\mathcal{X}:=\bigcup_{k\in\mathbb{N}}\mathcal{X}_{k}\) be the disjoint union of finite sets with \(|\mathcal{X}_{k}|=k\) and \(\mathcal{H}:=\bigcup_{k\geq 1}\mathcal{H}_{k}\), where \(\mathcal{H}_{k}:=\{\mathbbm{1}_{x}:x\in\mathcal{X}_{k}\}\). Note that this is exactly singletons on an infinite domain and we have the following hold:_

1. \(\mathcal{H}\) _does not have an infinite Littlestone tree since for any root_ \(x\in\mathcal{X}\)_, the subclass_ \(\{h\in\mathcal{H}:h(x)=1\}\) _has only size 1, and thus the corresponding subtree of the Littlestone tree must be finite._
2. \(\mathcal{H}\) _has an infinite star-eluder sequence. Indeed, any infinite sequence_ \(\{(x_{1},0),(x_{2},0),\ldots\}\) _with_ \(x_{k}\in\mathcal{X}_{k}\) _for all_ \(k\geq 1\)_, is an infinite star-eluder sequence. To see this, note that for any_ \(k\in\mathbb{N}\)_, and any_ \(n_{k}\)_, the version space_ \(V_{n_{k}}(\mathcal{H})\) _contains_ \(\bigcup_{j>n_{k}}\mathcal{H}_{j}\)_. Therefore,_ \(\{(x_{n_{k}+1},0),(x_{n_{k}+2},0),\ldots,(x_{n_{k}+k},0)\}\) _is a star set of_ \(V_{n_{k}}(\mathcal{H})\) _centered at_ \(h_{\text{all-}0\text{'s}}\)_, witnessed by concepts_ \(\{\mathbbm{1}_{\{x_{n_{k}+1}\}},\mathbbm{1}_{\{x_{n_{k}+2}\}},\ldots,\mathbbm{ 1}_{\{x_{n_{k}+k}\}}\}\)_._

**Example 14** (**Infinite star-eluder sequence but no infinite VCL tree)**.: _Let \(\mathcal{X}_{1}\) and \(\mathcal{H}_{1}\) be defined in Example 13, let \(\mathcal{X}_{2}=\mathbb{R}\) and \(\mathcal{H}_{2}=\mathcal{H}_{\text{thresh},\mathbb{R}}\) be the class of all threshold classifiers on \(\mathbb{R}\). Note that \(\mathcal{H}_{2}\) has an infinite Littlestone tree. Now we define \(\mathcal{X}:=\mathcal{X}_{1}\cup\mathcal{X}_{2}\) and \(\mathcal{H}:=\mathcal{H}_{1}\cup\mathcal{H}_{2}\), and have the following hold:_

1. \(\mathcal{H}\) _does not have an infinite VCL tree since for any fixed root_ \(x\in\mathcal{X}\)_, the subclass_ \(\{h\in\mathcal{H}:h(x)=1\}\) _has a VC dimension only 1, and thus the corresponding subtree of the VCL tree must be finite._
2. \(\mathcal{H}\) _has an infinite star-eluder sequence (see Example_ 13_)._

**Example 15** (**Infinite VC-eluder sequence but no infinite Littlestone tree)**.: _Let \(\mathcal{X}:=\bigcup_{k\in\mathbb{N}}\mathcal{X}_{k}\) be the disjoint union of finite sets with \(|\mathcal{X}_{k}|=k\) and \(\mathcal{H}:=\bigcup_{k\geq 1}\mathcal{H}_{k}\), where \(\mathcal{H}_{k}:=\{\mathbbm{1}_{S}:S\subseteq\mathcal{X}_{k}\}\). We have the following hold:_

1. \(\mathcal{H}\) _does not have an infinite Littlestone tree since for any root_ \(x\in\mathcal{X}\)_, the subclass_ \(\{h\in\mathcal{H}:h(x)=1\}\) _is finite, and thus the corresponding subtree of the Littlestone tree must be finite._
2. \(\mathcal{H}\) _has an infinite VC-eluder sequence. Indeed, any sequence_ \(\{(x_{1},0),(x_{2},0),(x_{3},0),\ldots\}\) _with_ \(x_{n_{k}+1},\ldots,x_{n_{k}+k}\in\mathcal{X}_{k}\) _for all_ \(k\geq 1\)_, is an infinite VC-eluder sequence. Furthermore, it has been argued that_ \(\text{VC}(\mathcal{H})=\infty\) _(_Ex.2.3 Bousquet et al._,_ 2021_)__, which is consistent with our Lemma 9 in Section 4._

**Example 16** (**Infinite VC-eluder sequence but no infinite VCL tree)**.: _Let \(\mathcal{X}_{1}\) and \(\mathcal{H}_{1}\) be defined in Example 15, let \(\mathcal{X}_{2}=\mathbb{R}\) and \(\mathcal{H}_{2}=\mathcal{H}_{\text{thresh},\mathbb{R}}\) be the class of all threshold classifiers on \(\mathbb{R}\). Note that \(\mathcal{H}_{2}\) has an infinite Littlestone tree. Now we define \(\mathcal{X}:=\mathcal{X}_{1}\cup\mathcal{X}_{2}\) and \(\mathcal{H}:=\mathcal{H}_{1}\cup\mathcal{H}_{2}\), and have the following hold:_

1. \(\mathcal{H}\) _does not have an infinite VCL tree since for any fixed root_ \(x\in\mathcal{X}\)_, the subclass_ \(\{h\in\mathcal{H}:h(x)=1\}\) _has a bounded VC dimension, and thus the corresponding subtree of the VCL-tree must be finite._
2. \(\mathcal{H}\) _has an infinite VC-eluder sequence (see Example_ 15_)._

### Star-related notions

In this section, we provide examples to distinguish between the following star-related notions: star number \(\mathfrak{s}_{\mathcal{H}}\) (Definition 4), the star-eluder dimension \(\operatorname{SE}(\mathcal{H})\) (Definition 9), star set (Definition 4) and star eluder sequence (Definition 6).

In particular, Example 17 reveals that having an infinite star number of \(h^{*}\) does not guarantee that \(\mathcal{H}\) has an infinite star-eluder sequence centered at the same target \(h^{*}\). Note that if \(\mathfrak{s}_{\mathcal{H}}=\infty\) always yields an infinite star set, then we can simply choose this infinite star set to be an infinite star-eluder sequence. Unfortunately, Example 18 fails the conjecture. Furthermore, Proposition 2 in Section 4 is convinced by Example 19. Finally, Example 20 gives an instance that \(\text{SE}(\mathcal{H})=\infty\) and infinite star-eluder sequence are not equivalent as well. For comparison, we recall that \(\text{E}(\mathcal{H})=\infty\) is equivalent to an infinite eluder sequence, and \(\text{VCE}(\mathcal{H})=\infty\) is equivalent to an infinite VC-eluder sequence (see a discussion in Appendix C).

**Example 17** (**Infinite star number and infinite star-eluder sequence with different centers**).: _Let us recall Example 3, where \(\mathcal{H}_{\text{singleton},\mathbb{N}}\) is the class of singletons on natural numbers. According to the analysis in Example 13, we know that \(\mathcal{H}_{\text{singleton},\mathbb{N}}\) has an infinite star number of \(h_{\text{all-0-3}}\), and also an infinite star-eluder sequence centered at \(h_{\text{all-0-3}}\)._

_Now we slightly change the setting: Let \(\mathcal{X}:=\bigcup_{k\in\mathbb{N}}\mathcal{X}_{k}\) be the disjoint union of finite sets with \(|\mathcal{X}_{k}|=k\) (one may simply assume \(\mathcal{X}:=\mathbb{N}\)). Denote \(\mathcal{X}_{k}:=\{x_{k,1},\ldots,x_{k,k}\}\) and define \(h_{k,i}(x):=\mathbbm{1}\{x=x_{k,i}\text{ or }x\notin\mathcal{X}_{k}\}\), for all \(1\leq i\leq k\). We let \(\mathcal{H}:=\{h_{k,i},k\in\mathbb{N},1\leq i\leq k\}\), and have the following hold:_

1. \(\mathfrak{s}_{h_{\text{all-0-3}}}=\infty\)_: Given arbitrarily large integer_ \(k\)_,_ \(\{(x_{k,1},0),(x_{k,2},0),\ldots,(x_{k,k},0)\}\) _is a star set centered at_ \(h_{\text{all-0-3}}\)_, witnessed by hypotheses_ \(\{h_{k,i},1\leq i\leq k\}\)_._
2. \(\mathfrak{s}_{h_{\text{all-3}}}=\infty\)_: Given arbitrarily large integer_ \(k\)_,_ \(\{(x_{1,1},1),(x_{2,1},1),\ldots,(x_{k,1},1)\}\) _is a star set centered at_ \(h_{\text{all-1-3}}\)_, witnessed by hypotheses_ \(\{h_{i,2},1\leq i\leq k\}\)_._
3. _It has an infinite star-eluder sequence centered at_ \(h_{\text{all-1-3}}\)_: Indeed,_ \(\{(x_{1,1},1),(x_{2,1},1),\ldots\}\) _is an example of infinite star-eluder sequence._
4. \(\mathcal{H}\) _does not have an infinite star-eluder sequence centered at_ \(h_{\text{all-0-3}}\)_._

**Example 18** (**Infinite star number but no infinite star set**).: _We slightly change the setting in Example 17: Let \(\mathcal{X}:=\bigcup_{k\in\mathbb{N}}\mathcal{X}_{k}\) be the disjoint union of finite sets with \(|\mathcal{X}_{k}|=k\) (one may again simply assume \(\mathcal{X}:=\mathbb{N}\)). Denote \(\mathcal{X}_{k}:=\{x_{k,1},\ldots,x_{k,k}\}\), let \(h_{k,i}(x):=\mathbbm{1}\{x=x_{k,i}\text{ or }x\in\mathcal{X}_{>k}\}\), for all \(1\leq i\leq k\) and \(k\in\mathbb{N}\), and let \(\mathcal{H}:=\{h_{k,i},1\leq i\leq k,k\in\mathbb{N}\}\). We have the following hold:_

1. \(\mathfrak{s}_{\mathcal{H}}=\infty\) _since_ \(\mathcal{H}\) _has a star set of arbitrarily large finite size._
2. \(\mathcal{H}\) _does not have an infinite star set._

_It is worthwhile to mention that in this example, \(\mathcal{H}\) does have an infinite star-eluder sequence \(\{(x_{1,1},0),(x_{2,1},0),(x_{2,2},0),\ldots\}\) centered at \(h_{\text{all-0-3}}\). Hence, an infinite star set is an infinite star-eluder sequence, but not the only possibility._

**Example 19** (**Infinite star number but no infinite star-eluder sequence**).: _We slightly change the setting in Example 18 as follow: Let \(\mathcal{X}:=\bigcup_{k\in\mathbb{N}}\mathcal{X}_{k}\) be the disjoint union of finite sets with \(|\mathcal{X}_{k}|=k\) (one may again simply assume \(\mathcal{X}:=\mathbb{N}\)). Denote \(\mathcal{X}_{k}:=\{x_{k,1},\ldots,x_{k,k}\}\), let \(h_{k,i}(x):=\mathbbm{1}\{x=x_{k,i}\text{ or }x\in\mathcal{X}_{<k}\}\), for all \(1\leq i\leq k\) and \(k\in\mathbb{N}\), and let \(\mathcal{H}:=\{h_{k,i},1\leq i\leq k,k\in\mathbb{N}\}\). Then the following hold:_

1. \(\mathfrak{s}_{\mathcal{H}}=\infty\) _since_ \(\mathcal{H}\) _has a star set of arbitrarily large finite size._
2. \(\mathcal{H}\) _does not have an infinite star-eluder sequence, and_ \(\text{SE}(\mathcal{H})<\infty\)_._

**Example 20** (**Infinite star-eluder dimension but no infinite star-eluder sequence**).: _For any \(k\in\mathbb{N}\), let \(\mathcal{X}_{k}:=\bigcup_{t\in\mathbb{N}}\mathcal{X}_{k,t}\) be disjoint union of finite sets with \(|\mathcal{X}_{k,t}|=k\) for all \(t\in\mathbb{N}\). Let \(\mathcal{X}:=\bigcup_{k\in\mathbb{N}}\mathcal{X}_{k}\) also with disjoint subspaces \(\{\mathcal{X}_{k}\}_{k\in\mathbb{N}}\). For notation simplicity, let us denote \(\mathcal{X}_{k,t}:=\{x_{k,t},1,\ldots,x_{k,t}\}\) for all \(k,t\in\mathbb{N}\). Now we can define a hypothesis class as follow: let \(h_{k,t,j}(x):=\mathbbm{1}\{(x=x_{k,t,j})\vee(x\in\mathcal{X}_{k,>t})\vee(x \in\mathcal{X}_{<k})\}\), for all \(k,t\in\mathbb{N}\) and \(1\leq j\leq k\), and let \(\mathcal{H}:=\{h_{k,t,j},1\leq j\leq k,k,t\in\mathbb{N}\}\). We have the following hold:_

1. \(\text{SE}(\mathcal{H})=\infty\) _since for arbitrarily large_ \(k\in\mathbb{N}\)_,_ \(\mathcal{H}\) _has an infinite_ \(k\)_-star-eluder sequence_ \(\mathcal{X}_{k}\) _with all labels 0._
2. \(\mathcal{H}\) _does not have an infinite (strong) star-eluder sequence._

**Remark 11**.: _Altogether, we have the follow relations_

_Remarkably, a complete theory to the relations between these notions is still lacking here, which might be of independent interests._

## Appendix C Fine-grained analysis

In this appendix section, we provide a fine-grained analysis of the asymptotic rate of decay of the universal learning curves by ERM, whenever possible. This will be an analogy to the optimal fine-grained universal learning curves studied in Bousquet et al. (2023). Our characterization of the sharp distribution-free constant factors is based on two newly-introduced combinatorial dimensions named the _star-eluder dimension_ or SE dimension and the _VC-eluder dimension_ or VCE dimension. We present their formal definitions first.

**Definition 9** (**Star(VC)-eluder dimension**).: _Let \(\mathcal{H}\) be a concept class, we say \(\mathcal{H}\) has an infinite_

* \(d\)_-star-eluder sequence_ \(\{(x_{1},y_{1}),(x_{2},y_{2}),\dots\}\) _centered at_ \(h\)_, if it is realizable and for every_ \(k\in\mathbb{N}\)_,_ \(\{x_{kd+1},\dots,x_{kd+d}\}\) _is a star set of_ \(V_{kd}(\mathcal{H})\) _centered at_ \(h\)_. Furthermore, the star-eluder dimension of_ \(\mathcal{H}\)_, denoted by_ \(\text{SE}(\mathcal{H})\)_, is defined to be the largest integer_ \(d\geq 0\) _such that_ \(\mathcal{H}\) _has an infinite_ \(d\)_-star-eluder sequence. If_ \(\mathcal{H}\) _does not have any infinite 1-star-eluder sequence, we define_ \(\text{SE}(\mathcal{H})=0\)_. If for arbitrarily large integer_ \(d\)_,_ \(\mathcal{H}\) _has an infinite_ \(d\)_-star-eluder sequence, we define_ \(\text{SE}(\mathcal{H})=\infty\)_._
* \(d\)_-VC-eluder sequence_ \(\{(x_{1},y_{1}),(x_{2},y_{2}),\dots\}\) _centered at_ \(h\)_, if it is realizable, and for every_ \(k\in\mathbb{N}\)_,_ \(h(x_{k})=y_{k}\) _and_ \(\{x_{kd+1},\dots,x_{kd+d}\}\) _is a shattered set of_ \(V_{kd}(\mathcal{H})\)_. Furthermore, the VC-eluder dimension of_ \(\mathcal{H}\)_, denoted by_ \(\text{VCE}(\mathcal{H})\)_, is defined to be the largest integer_ \(d\geq 0\) _such that_ \(\mathcal{H}\) _has an infinite_ \(d\)_-VC-eluder sequence. If_ \(\mathcal{H}\) _does not have any infinite 1-VC-eluder sequence, we define_ \(\text{VCE}(\mathcal{H})=0\)_. If for arbitrarily large integer_ \(d\)_,_ \(\mathcal{H}\) _has an infinite_ \(d\)_-VC-eluder sequence, we define_ \(\text{VCE}(\mathcal{H})=\infty\)_._

**Remark 12**.: _We recall that the eluder dimension \(\text{E}(\mathcal{H})\) in Definition 5 represents the length of the longest eluder sequence that exists in \(\mathcal{H}\). Indeed, an eluder sequence is exactly one branch of a Littestone tree, and thus \(\text{E}(\mathcal{H})<\infty\) implies that \(\mathcal{H}\) has no infinite Littestone tree. The converse is not true, because \(\mathcal{H}\) may have a finite Littestone tree with some of the branches being infinitely long (see Example 12). Similarly, the star-eluder dimension \(\text{SE}(\mathcal{H})\) and the VC-eluder dimension \(\text{VCE}(\mathcal{H})\) here are also strongly related to certain combinatorial structures that have been studied before. In particular for \(\text{VCE}(\mathcal{H})\), one may refer to the concepts of the (strong) VCL tree, \(d\)-VCL tree and the VCL dimension introduced by Bousquet et al. (2021, 2023). Indeed, an infinite (strong) VC-eluder sequence is exactly one branch of a strong VCL tree, and an infinite \(d\)-VC-eluder sequence is exactly one branch of an infinite \(d\)-VCL tree. Since an infinite 1-VCL-tree is exactly an infinite Littestone tree, an infinite 1-VC-eluder sequence is thus exactly an infinite eluder sequence. Moreover, recall that \(\text{VCL}(\mathcal{H})=0\) implies that \(\mathcal{H}\) does not have an infinite Littestone tree, and similarly, here we have \(\text{VCE}(\mathcal{H})=0\) implies that \(\mathcal{H}\) does not have an infinite eluder sequence._

**Remark 13**.: _For any concept class \(\mathcal{H}\), the following hold:_

1. \(\text{E}(\mathcal{H})\geq\text{SE}(\mathcal{H})\geq\text{VCE}(\mathcal{H})\)_._
2. \(\text{VCE}(\mathcal{H})\geq 1\iff\text{SE}(\mathcal{H})\geq 1\iff\text{E}( \mathcal{H})=\infty\)_._
3. \(\text{VCE}(\mathcal{H})=0\iff\text{SE}(\mathcal{H})=0\iff\text{E}(\mathcal{H} )<\infty\)_._We then state the formal definition of the fine-grained universal rates by ERM.

**Definition 10** (**Fine-grained universal rates by ERM**).: _Let \(\mathcal{H}\) be a concept class and \(R(n)\to 0\) be a distribution-free rate function. We say_

* \(\mathcal{H}\) _is universally learnable at fine-grained rate_ \(R\) _by ERM, if for every distribution_ \(P\in\text{RE}(\tilde{\mathcal{H}})\)_, there exists a distribution-dependent rate_ \(\lambda(n)=o\left(R(n)\right)\) _such that for every ERM algorithm,_ \(\mathbb{E}[er_{P}(\hat{h}_{n})]\leq R(n)+\lambda(n)\)_, for all_ \(n\in\mathbb{N}\)_._
* \(\mathcal{H}\) _is not universally learnable at fine-grained rate faster than_ \(R\) _by ERM, if there exists a distribution_ \(P\in\text{RE}(\mathcal{H})\) _such that there is an ERM algorithm satisfying_ \(\mathbb{E}[er_{P}(\hat{h}_{n})]\geq R(n)\)_, for infinitely many_ \(n\in\mathbb{N}\)_._
* \(\mathcal{H}\) _is universally learnable with exact fine-grained rate_ \(R\) _by ERM, if_ \(\mathcal{H}\) _is universally learnable at fine-grained rate_ \(R\) _by ERM, and is not universally learnable at fine-grained rate faster than_ \(R\) _by ERM._

Note that the crucial difference between this definition and Definition 2 is that here \(R(n)\) is independent of the data distribution \(P\). In other words, the fine-grained rates provide optimal distribution-free upper and lower envelopes of the universal learning curves up to numerical constant factors.

**Remark 14**.: _Definition 10 describes special cases of Definition 2 in the following sense: If \(\mathcal{H}\) is universally learnable at fine-grained rate (no faster than) \(R\) by ERM, then it is universally learnable at rate (no faster than) \(R\) by ERM as well. Briefly speaking, the fine-grained analysis aims to find the correct characterization that captures the optimal distribution-free upper envelope and lower envelope of all the distribution-dependent learning curves, tight up to numerical constant factors._

We now turn to state our results of fine-grained universal rates by ERM. All technical aspects of the proofs are deferred to Appendix D.3.

**Theorem 8** (**Fine-grained learning rates**).: _For every class \(\mathcal{H}\) with \(|\mathcal{H}|\geq 3\), the following hold:_

* _If_ \(\text{VCE}(\mathcal{H})<\infty\)_, then_ \(\mathcal{H}\) _is universally learnable at fine-grained rate_ \(\frac{\text{VCE}(\mathcal{H})\log n}{n}\)_, and is not universally learnable at fine-grained rate faster than_ \(\frac{\text{VCE}(\mathcal{H})}{n}\)_, by ERM._
* _If_ \(\text{SE}(\mathcal{H})<\infty\)_, then_ \(\mathcal{H}\) _is universally learnable at fine-grained rate_ \(\frac{\text{VCE}(\mathcal{H})}{n}\log\left(\frac{\text{SE}(\mathcal{H})}{ \text{VCE}(\mathcal{H})}\right)\)_, but is not universally learnable at fine-grained rate faster than_ \(\frac{\text{VCE}(\mathcal{H})+\log\left(\text{SE}(\mathcal{H})\right)}{n}\)_, by ERM._

_or equivalently, there exist finite numerical constants \(\alpha,\beta>0\) such that_

* _If_ \(\text{VCE}(\mathcal{H})<\infty\)_, then_ \[\mathbb{E}\left[er_{P}(\hat{h}_{n})\right] \geq\alpha\cdot\frac{\text{VCE}(\mathcal{H})}{n},\ \text{for infinitely many }n\in\mathbb{N},\] (1) \[\mathbb{E}\left[er_{P}(\hat{h}_{n})\right] \leq\beta\cdot\frac{\text{VCE}(\mathcal{H})\log n}{n}+2^{-\lfloor n /2\kappa\rfloor},\ \forall n\in\mathbb{N},\] (2) _where_ \(\kappa=\kappa(P)\) _is a distribution-dependent constant._
* _If_ \(\text{SE}(\mathcal{H})<\infty\)_, then_ \[\mathbb{E}\left[er_{P}(\hat{h}_{n})\right] \geq\alpha\cdot\frac{\text{VCE}(\mathcal{H})+\log\left(\text{SE} (\mathcal{H})\right)}{n},\ \text{for infinitely many }n\in\mathbb{N},\] (3) \[\mathbb{E}\left[er_{P}(\hat{h}_{n})\right] \leq\beta\cdot\frac{\text{VCE}(\mathcal{H})}{n}\log\left(\frac{ \text{SE}(\mathcal{H})}{\text{VCE}(\mathcal{H})}\right)+2^{-\lfloor n/2\hat{ \kappa}\rfloor},\ \forall n\in\mathbb{N},\] (4) _where_ \(\hat{\kappa}=\hat{\kappa}(P)\) _is a distribution-dependent constant._

**Remark 15**.: _Our proofs use \(\alpha=1/20\) and \(\beta=160\)._

**Remark 16**.: _When \(\text{SE}(\mathcal{H})=\text{VCE}(\mathcal{H})=0\), \(\text{E}(\mathcal{H})<\infty\), or \(\text{SE}(\mathcal{H})=\text{VCE}(\mathcal{H})=1\), \(\text{E}(\mathcal{H})=\infty\), we still have the bounds (3) and (4) since we define \(\log\left(0\right)=0\), \(0\log\left(0/0\right)=0\) and \(\log\left(x\right):=\log\left(x\lor 2\right)\) for any \(x>0\). Moreover, we remark that neither \(\text{VCE}(\mathcal{H})=\infty\) nor \(\text{SE}(\mathcal{H})=\infty\) is considered in these fine-grained rates. This is because when \(\text{VCE}(\mathcal{H})=\infty\), arbitrarily slow rates cannot admit distribution-free constants. And when \(\text{SE}(\mathcal{H})=\infty\), it is still impossible because it does not guarantee an infinite star-eluder sequence, that is, the lower bound of (1) cannot be increased to \(\log\left(n\right)/n\), and is the sharpest one we can have here._

**Remark 17**.: _It is not hard to understand that a target-specified version of fine-grained universal rates by ERM is also derivable, based on a centered version of the star-eluder dimension \(\text{SE}_{h^{*}}\) and VC-eluder dimension \(\text{VCE}_{h^{*}}\)._

**Remark 18**.: _It is worth noting that, when \(\text{SE}(\mathcal{H})<\infty\), there is a mismatch between the lower bound and the upper bound. This serves as an analogy to the mismatch between Cor.12 and Thm.13 in Hanneke (2016b), and certain demonstrating examples have been exhibited in Hanneke and Yang (2015). In the following two examples, we provide evidence that such a gap does exist, in a sense that both the upper bound and the lower bound can sometimes be tight for some classes. Roughly speaking, if an infinite \(\text{SE}(\mathcal{H})\)-star-eluder sequence in \(\mathcal{H}\) is also an infinite \(\text{VCE}(\mathcal{H})\)-VC-eluder sequence (see Definition 9), then \(\frac{\text{VCE}(\mathcal{H})}{n}\log{(\frac{\text{SE}(\mathcal{H})}{\text{ VCE}(\mathcal{H})})}\) is the optimal rate, otherwise \(\frac{\text{VCE}(\mathcal{H})+\log{(\frac{\text{SE}(\mathcal{H})}{\mathcal{H} })}}{n}\) is optimal._

**Example 21** (**Optimal \((\text{VCE}(\mathcal{H})+\log{(\text{SE}(\mathcal{H}))})/n\) rate rate)**.: _We construct a concept class \(\mathcal{H}\) such that an infinite VCE\((\mathcal{H})\)-VC-eluder sequence and an infinite \(\text{SE}(\mathcal{H})\)-star-eluder sequence cannot be realized by an infinite sequence. To this end, we slightly change the example presented in Appendix D.2 of Hanneke and Yang (2015), which yields the tightness of a lower bound \((\text{VC}(\mathcal{H})+\log{(\mathcal{H}_{\mathcal{H}})})/n\)._

_Specifically, let \(d,s>0\) be two integers satisfying \(d\leq s\). Let \(\mathcal{X}:=\mathbb{Z}\setminus\{0\}:=\mathcal{X}_{1}\cup\mathcal{X}_{2}\), where \(\mathcal{X}_{1}:=\mathbb{N}\setminus\{0\}\) and \(\mathcal{X}_{2}:=-\mathbb{N}\setminus\{0\}=-\mathcal{X}_{1}\). We can also write_

\[\mathcal{X}_{1}=(\mathcal{X}_{1,0}\cup\mathcal{X}_{1,1}\cup\cdots)\text{, where }\mathcal{X}_{1,k}:=\{ks+1,\ldots,(k+1)s\}\text{ for all }k\in\mathbb{N}\text{,}\]

\[\mathcal{X}_{2}=(\mathcal{X}_{2,0}\cup\mathcal{X}_{2,1}\cup\cdots)\text{, where }\mathcal{X}_{2,k}:=\{-(k+1)d,\ldots,-kd-1\}\text{ for all }k\in\mathbb{N}\text{.}\]

_Now we let \(\mathcal{H}:=\mathcal{H}_{1}\cup\mathcal{H}_{2}\) satisfying \(\text{VCE}(\mathcal{H})=d\) and \(\text{SE}(\mathcal{H})=s\), where_

\[\mathcal{H}_{1}:=\{h_{k,j}:\forall j\in\mathcal{X}_{1,k},\forall k\in\mathbb{N}\} \text{, where }h_{k,j}(x):=\mathbb{1}(x=j\text{ or }x\in\mathcal{X}_{1,>k}).\]

\[\mathcal{H}_{2}:=\{h_{k,S}:\forall S\subseteq\mathcal{X}_{2,k},\forall k\in \mathbb{N}\}\text{, where }h_{k,S}(x):=\mathbb{1}(x\in S\text{ or }x\in\mathcal{X}_{2,>k}).\]

_In particular, \(\mathcal{X}_{1}\) itself is an infinite \(s\)-star-eluder sequence centered at \(h_{\text{all}\cdot 0\text{'}}\), and \(\mathcal{X}_{2}\) itself is an infinite \(d\)-VC-eluder sequence, but they do not intersect. To show that the upper bound can be decreased to match the lower bound, we simply note that for any infinite \(s\)-star-eluder sequence, its associated VC-eluder dimension is exactly 1, resulting in a \(\log{(s)}/n\) upper bound. For any infinite \(d\)-VC-eluder sequence, its associated star-eluder dimension is also \(d\), resulting in a \(d/n\) upper bound. The maximum of the two upper bounds yields the desired one._

**Example 22** (**Optimal \((\text{VCE}(\mathcal{H})/n)\log{(\text{SE}(\mathcal{H})/\text{VCE}( \mathcal{H}))}\) rate)**.: _We construct a concept class \(\mathcal{H}\) such that there exists an infinite sequence in \(\mathcal{H}\) which is both an infinite VCE\((\mathcal{H})\)-VC-eluder sequence and an infinite \(\text{SE}(\mathcal{H})\)-star-eluder sequence. To this end, we slightly change the example presented in Appendix D.1 of Hanneke and Yang (2015), which yields the tightness of an upper bound \((\text{VC}(\mathcal{H})/n)\log{(\mathfrak{s}_{\mathcal{H}}/\text{VC}( \mathcal{H}))}\)._

_Specifically, let \(d,s>0\) be two integers satisfying \(d\leq s\). Let \(\mathcal{X}:=\mathbb{N}\) and for every \(k\in\mathbb{N}\), define \(h_{k,S}(x):=\mathbb{1}(x\in S\text{ or }x>(k+1)s)\) for every subset \(S\subseteq\{ks+1,\ldots,(k+1)s\}\) with \(|S|\leq d\). Let \(\mathcal{H}:=\{h_{k,S}:S\subseteq\{ks+1,\ldots,(k+1)s\},|S|\leq d,k\in\mathbb{N}\}\). Note that for this class, we have \(\text{VCE}(\mathcal{H})=d\), \(\text{SE}(\mathcal{H})=s\) and there exists an infinite sequence serving as an infinite \(d\)-VC-eluder sequence as well as an infinite \(s\)-star-eluder sequence. To show in this case that the lower bound can be increased to match the upper bound, the realizable distribution that witnesses this rate is referred to Appendix D.1.1 of Hanneke and Yang (2015)._

Now let us turn to the proof of Theorem 8, which is based on the following two lemmas and within each an upper bound as well as a lower bound are established.

**Lemma 10**.: _For every concept class \(\mathcal{H}\) with \(|\mathcal{H}|\geq 3\), if \(\text{VCE}(\mathcal{H})<\infty\), then the following hold:_

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right] \geq\frac{\text{VCE}(\mathcal{H})}{18n},\ \text{ for infinitely many }n\in\mathbb{N},\] \[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right] \leq\frac{28\text{VCE}(\mathcal{H})\log{n}}{n}+2^{-\lfloor n/2 \kappa\rfloor},\ \forall n\in\mathbb{N},\]

_where \(\kappa=\kappa(P)\) is a distribution-dependent constant._

**Remark 19**.: _Note that a concept class with its VCE dimension finite can either have an infinite star-eluder sequence or not, which results in a difference (of a logarithmic factor) in the upper bound and lower bound stated in Lemma 10._Recall that \(\text{VC}(\mathcal{H})<\infty\) yields a uniform upper bound \(\text{VC}(\mathcal{H})\log{(n)}/n\). On one hand, \(\text{VCE}(\mathcal{H})=\infty\) implies that \(\mathcal{H}\) has a shattered set of arbitrarily large size, which further implies an unbounded VC dimension. On the other hand, according to Lemma 9, \(\text{VC}(\mathcal{H})=\infty\) implies that \(\mathcal{H}\) has an infinite VC-elnder sequence, and thus \(\text{VCE}(\mathcal{H})=\infty\) holds as well. Therefore, \(\text{VCE}(\mathcal{H})=\infty\) if and only if \(\text{VC}(\mathcal{H})=\infty\) if and only if \(\mathcal{H}\) has an infinite VC-elnder sequence. Moreover, when \(\text{VCE}(\mathcal{H})<\infty\), a trivial observation is \(\text{VCE}(\mathcal{H})\leq\text{VC}(\mathcal{H})<\infty\). However, the following example reveals that VCE and VC are not the same dimension, namely, there exists a class \(\mathcal{H}\) having strictly \(\text{VCE}(\mathcal{H})<\text{VC}(\mathcal{H})\) (see the following Example 23). Therefore, Lemma 10 sometimes reflects an improvement over the classical uniform bound.

**Example 23** (\(\text{VCE}(\mathcal{H})<\text{VC}(\mathcal{H})<\infty\)).: _To make it more convincing, we provide an example of infinite classes here. Let \(\mathcal{X}_{1}\) be a finite set of size \(d\), and \(\mathcal{X}_{2}\) be an infinite instance space that is disjoint with \(\mathcal{X}_{1}\). For simplicity, one may assume that \(\mathcal{X}_{1}:=\{-d,-(d-1),\ldots,-1\}\) and \(\mathcal{X}_{2}:=\mathbb{N}\). We define \(\mathcal{X}:=\mathcal{X}_{1}\cup\mathcal{X}_{2}\) and let \(\mathcal{H}:=\{h_{S,k}:=\mathbb{I}_{S\cup\{k\}},\forall S\subseteq\mathcal{X}_ {1},\forall k\in\mathbb{N}\}\). This class has \(\text{VC}(\mathcal{H})=(d+1)\) but \(\text{VCE}(\mathcal{H})=1\) since there is no infinite 2-VC-elnder sequence. Similarly, we can also construct an example that witnesses strictly \(\text{SE}(\mathcal{H})<\mathfrak{s}_{\mathcal{H}}<\infty\)._

**Lemma 11**.: _For every concept class \(\mathcal{H}\) with \(|\mathcal{H}|\geq 3\), if \(\text{SE}(\mathcal{H})<\infty\), then the following hold:_

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right] \geq\frac{\log{(\text{SE}(\mathcal{H}))}}{12n},\ \text{ for infinitely many }n\in\mathbb{N},\] \[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right] \leq\frac{160\text{VCE}(\mathcal{H})}{n}\log{\left(\frac{\text{SE }(\mathcal{H})}{\text{VCE}(\mathcal{H})}\right)}+2^{-\lfloor n/2\hat{k}\rfloor },\ \forall n\in\mathbb{N},\]

_where \(\hat{k}=\hat{k}(P)\) is a distribution-dependent constant._

**Remark 20**.: _Note that in Theorem 8, the lower bound appears as \(\frac{\text{VCE}(\mathcal{H})+\log{(\text{SE}(\mathcal{H}))}}{n}\). Indeed, \(\text{SE}(\mathcal{H})<\infty\) immediately implies \(\text{VCE}(\mathcal{H})<\infty\) and then the lower bound in Lemma 10 holds. Combining with the lower bound in Lemma 11 will give us the desired result in Theorem 8 with some sufficiently small constant, e.g. \(\alpha=1/20\)._

_Remarkably, when both \(\text{SE}(\mathcal{H})\) and \(\text{VCE}(\mathcal{H})\) are finite, either of \(\text{VCE}(\mathcal{H})\) and \(\log{(\text{SE}(\mathcal{H}))}\) can be larger than the other, and we provide the following examples for evidence. Therefore, none of the quantities can be removed in the lower bound._

**Example 24** (\(\text{VCE}(\mathcal{H})<\log{(\text{SE}(\mathcal{H}))}<\infty\)).: _Let \(\mathcal{X}:=\bigcup_{k\in\mathbb{N}}\mathcal{X}_{k}\) be the disjoint union of finite sets with \(|\mathcal{X}_{k}|=d<\infty\). We denote \(\mathcal{X}_{k}:=\{x_{k,1},\ldots,x_{k,d}\}\), for every \(k\in\mathbb{N}\), let \(h_{k,j}(x):=\mathbbm{1}\{x=x_{k,j}\text{ or }x\in\mathcal{X}_{>k}\}\), for every \(k\in\mathbb{N}\) and every \(1\leq j\leq d\), and finally let \(\mathcal{H}:=\{h_{k,j},1\leq j\leq d,k\in\mathbb{N}\}\). For this class, we have \(\text{VCE}(\mathcal{H})=2<\log{(\text{SE}(\mathcal{H}))}=\log{d}\) for a sufficiently large \(d\)._

**Example 25** (\(\log{(\text{SE}(\mathcal{H}))}<\text{VCE}(\mathcal{H})<\infty\)).: _Let \(\mathcal{X}:=\bigcup_{k\in\mathbb{N}}\mathcal{X}_{k}\) be the disjoint union of finite sets with \(|\mathcal{X}_{k}|=d<\infty\). We denote \(\mathcal{X}_{k}:=\{x_{k,1},\ldots,x_{k,d}\}\), for every \(k\in\mathbb{N}\), let \(h_{k,S}(x):=\mathbbm{1}\{x\in S\text{ or }x\in\mathcal{X}_{>k}\}\), for every \(k\in\mathbb{N}\) and every subset \(S\subseteq\mathcal{X}_{k}\), and finally let \(\mathcal{H}:=\{h_{k,S},S\subseteq\mathcal{X}_{k},k\in\mathbb{N}\}\). For this class, we have \(\log{(\text{SE}(\mathcal{H}))}=\log{d}<\text{VCE}(\mathcal{H})=d\)._

## Appendix D Proofs

### Omitted Proofs in Section 3

**Proposition 3** (**Proposition 1 restated**).: _Any infinite concept class \(\mathcal{H}\) has either an infinite star-elnder sequence or infinite Littlestone dimension._

To prove the proposition, we first introduce a new complexity structure named the _threshold sequence_.

**Definition 11** (**Threshold sequence**).: _Let \(\mathcal{H}\) be a concept class, we say that \(\mathcal{H}\) has an infinite threshold sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\), if it is realizable and for every integer \(k\), there exists \(\overline{h_{k}}\in\mathcal{H}\) such that \(h_{k}(x_{i})=y_{i}\) for all \(i<k\) and \(h_{k}(x_{i})\neq y_{i}\) for all \(i\geq k\). We say an infinite threshold sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) is centered at \(h\), if \(h(x_{i})=y_{i}\) for all \(i\in\mathbb{N}\)._

The following claim turns out to be an alternative result to the proposition.

**Claim 1**.: _Any infinite class \(\mathcal{H}\) has either an infinite star set or an infinite threshold sequence._

Given that the claim holds, the proof to the proposition is then straightforward. This is because an infinite star set itself is an infinite star-eluder sequence. Moreover, an infinite threshold sequence gives rise to infinite Littlestone dimension since \(\mathcal{H}\) can have a Littlestone tree of arbitrarily large depth (an easy example is \(\mathcal{H}_{\text{thresh},\mathbb{N}}\)). Therefore, it suffices to prove Claim 1. The remaining proof relies on a connection to the classical Ramsey theory, which we briefly introduced as follow.

The classical Ramsey's theorem states that one will find monochromatic cliques in any edge labelling (with colors) of a sufficiently large complete graph. Specifically, let \(r\) be an positive integer, a simple 2-colors version of the Ramsey's theorem states that there exists a smallest positive integer \(R(r,r)\), named the (diagonal) Ramsey number, such that every red-blue edge coloring of the complete graph on \(R(r,r)\) vertices contains either a red clique on \(r\) vertices or a blue clique on \(r\) vertices. However, we will need the following extension of the theorem to an infinite graph.

**Theorem 9** (Infinite Ramsey's theorem, Ramsey, 1987).: _For any countably infinite set, if its induced complete graph is colored with finitely many colors, then there is an infinite monochromatic clique._

Proof of Claim 1.: Based on Lemma 8, we know that any infinite class \(\mathcal{H}\) has an infinite eluder sequence. Let \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) be an infinite eluder sequence centered at \(h^{*}\), that is, for any \(j\in\mathbb{N}\), there exists \(h_{j}\in\mathcal{H}\) such that \(h_{j}(x_{i})=y_{i}=h^{*}(x_{i})\) for all \(i<j\) and \(h_{j}(x_{j})\neq y_{j}=h^{*}(x_{j})\). We aim to show that there exists an infinite subsequence \(\{(x_{i_{1}},y_{i_{1}}),(x_{i_{2}},y_{i_{2}}),\ldots\}\) that is either an infinite star set centered at \(h^{*}\) or an infinite threshold sequence centered at \(h^{*}\). To this end, we consider the infinite eluder sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) as a red-blue coloring of an infinite complete graph according to the following: let the vertices be indexed by \(\mathbb{N}\), then for every edge \(e_{i,j}\) with integers \(i>j\), we color it red if \(h_{j}(x_{i})=y_{i}\) and blue otherwise.

Note that for any infinite subsequence \(\{(x_{i_{1}},y_{i_{1}}),(x_{i_{2}},y_{i_{2}}),\ldots\}\), if the infinite subgraph comprised of the vertices \(\{i_{1},i_{2},\ldots\}\) is monochromatically red, then \(h_{i_{j}}(x_{i_{k}})=y_{i_{k}}=h^{*}(x_{i_{k}})\) for all integers \(k>j\). Since \(h_{i_{j}}(x_{i_{k}})=y_{i_{k}}=h^{*}(x_{i_{k}})\) for all integers \(k<j\) and \(h_{i_{j}}(x_{i_{j}})\neq y_{i_{j}}=h^{*}(x_{i_{j}})\), it implies that \(\{(x_{i_{1}},y_{i_{1}}),(x_{i_{2}},y_{i_{2}}),\ldots\}\) is an infinite star set centered at \(h^{*}\), witnessed by \(\{h_{i_{j}}\}_{j\in\mathbb{N}}\). Moreover, if the infinite subgraph comprised of the vertices \(\{i_{1},i_{2},\ldots\}\) is monochromatically blue, it is not hard to verify that \(\{(x_{i_{1}},y_{i_{1}}),(x_{i_{2}},y_{i_{2}}),\ldots\}\) is an infinite threshold sequence centered at \(h^{*}\). The proof is completed by applying the infinite Ramsey's theorem. 

**Lemma 12** (Lemma 1 restated).: _Given a concept class \(\mathcal{H}\), for any learning algorithm \(\hat{h}_{n}\), there exists a realizable distribution \(P\) with respect to \(\mathcal{H}\) such that \(\mathbb{E}[\text{er}_{P}(\hat{h}_{n})]\geq 2^{-(n+2)}\) for infinitely many \(n\), which implies that \(\mathcal{H}\) is not universally learnable at rate faster than exponential \(e^{-n}\)._

Proof of Lemma 12.: We prove the lemma by using the "probabilistic method". Let us consider non-trivially that \(|\mathcal{H}|>2\), let \(h_{1},h_{2}\in\mathcal{H}\) and \(x,x^{{}^{\prime}}\in\mathcal{X}\) such that \(h_{1}(x)=h_{2}(x)=y\) and \(h_{1}(x^{{}^{\prime}})\neq h_{2}(x^{{}^{\prime}})\). Now for any learning algorithm \(\hat{h}_{n}\), we define the following two realizable distributions \(P_{0}\) and \(P_{1}\), where \(P_{i}\{(x,y)\}=0.5\) and \(P_{i}\{(x^{{}^{\prime}},i)\}=0.5\), \(i\in\{0,1\}\). Let \(I\sim\text{Bernoulli}(0.5)\), and conditioned on \(I\), let \(S_{n}:=\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots,(x_{n},y_{n})\}\) and \((x_{n+1},y_{n+1})\) be i.i.d. samples from \(P_{I}\) that the learning algorithm \(\hat{h}_{n}\) is trained on. We note that

\[\mathbb{E}\left[\mathbb{P}\left(\hat{h}_{n}(x_{n+1})\neq y_{n+1}\big{|}S_{n},I \right)\right]\geq\frac{1}{2}\mathbb{P}\left(x_{1}=\ldots=x_{n}=x,x_{n+1}=x^{{} ^{\prime}}\right)=2^{-(n+2)}.\]

Furthermore, by the law of total probability, we have

\[\mathbb{E}\left[\mathbb{P}\left(\hat{h}_{n}(x_{n+1})\neq y_{n+1} \big{|}S_{n},I\right)\right]= \frac{1}{2}\sum_{i\in\{0,1\}}\mathbb{E}\left[\mathbb{P}\left( \hat{h}_{n}(x_{n+1})\neq y_{n+1}\big{|}S_{n},I=i\right)\big{|}I=i\right]\] \[\leq \max_{i\in\{0,1\}}\mathbb{E}\left[\mathbb{P}\left(\hat{h}_{n}(x_{ n+1})\neq y_{n+1}\big{|}S_{n},I=i\right)\big{|}I=i\right].\]

The above two inequalities imply that for every \(n\), there exists \(i_{n}\in\{0,1\}\) such that

\[\mathbb{E}\left[\mathbb{P}\left(\hat{h}_{n}(x_{n+1})\neq y_{n+1}\big{|}S_{n},I= i_{n}\right)\big{|}I=i_{n}\right]=\mathbb{E}[\text{er}_{P_{i_{n}}}(\hat{h}_{n})]\geq 2^{-(n+2)}.\]

In particular, by the pigeonhole principle, there exists \(i\in\{0,1\}\) such that \(i_{n}=i\) infinitely often, which completes the proof. 

**Lemma 13** (Lemma 2 restated).: _If \(\mathcal{H}\) does not have an infinite eluder sequence centered at \(h^{*}\), then \(h^{*}\) is universally learnable by ERM at rate \(e^{-n}\)._Proof of Lemma 13.: Since \(\mathcal{H}\) does not have an infinite eluder sequence centered at \(h^{*}\), then for any realizable distribution \(P\) centered at \(h^{*}\) and data sequence \(S:=\{(x_{1},h^{*}(x_{1})),(x_{2},h^{*}(x_{2})),\ldots\}\sim P^{\mathbb{N}}\), we have

\[\#\left\{t\in\mathbb{N}:\exists t^{{}^{\prime}}>t\ \text{ s.t. }\ \exists h\in V_{S_{t}}( \mathcal{H}):h(x_{t^{{}^{\prime}}})\neq h^{*}(x_{t^{{}^{\prime}}})\right\}<\infty.\]

For the largest such integer \(t\), we further have \(\mathbb{P}(\exists h\in V_{S_{t}}(\mathcal{H}):h(x_{t^{{}^{\prime}}})\neq h^{* }(x_{t^{{}^{\prime}}}))=1\) for some \(t^{{}^{\prime}}:=t^{{}^{\prime}}(S)>t\). This is true because the probability decays exponentially. Therefore, we have

\[\lim_{n\to\infty}\mathbb{P}_{S\sim P^{\mathbb{N}}}\left(P\left(x\in\mathcal{X }:\exists h\in V_{n}(\mathcal{H})\text{ s.t. }h(x)\neq h^{*}(x)\right)=0\right)=1,\]

which implies that there is a distribution-dependent positive integer \(k:=k(P)<\infty\) such that

\[\mathbb{P}\left(P\left(x\in\mathcal{X}:\exists h\in V_{k}(\mathcal{H})\text{ s.t. }h(x)\neq h^{*}(x)\right)=0\right)\geq 1/2.\]

Now for any integer \(n>k\), we split the dataset \(S_{n}\sim P^{n}\) into \(\lfloor n/k\rfloor\) parts with each one sized at least \(k\), denoted by \(S_{n,1},\ldots,S_{n,\lfloor n/k\rfloor}\). It holds then

\[\mathbb{P}\left(P\left(x\in\mathcal{X}:\exists h\in V_{n}( \mathcal{H})\text{ s.t. }h(x)\neq h^{*}(x)\right)\neq 0\right)\] \[\leq \mathbb{P}\left(\forall i\in\{1,\ldots,\lfloor n/k\rfloor\}:P \left(x\in\mathcal{X}:\exists h\in V_{S_{n,i}}(\mathcal{H})\text{ s.t. }h(x)\neq h^{*}(x)\right)\neq 0\right)\] \[= \prod_{i=1}^{\lfloor n/k\rfloor}\mathbb{P}\left(P\left(x\in \mathcal{X}:\exists h\in V_{S_{n,i}}(\mathcal{H})\text{ s.t. }h(x)\neq h^{*}(x)\right)\neq 0\right)\leq 2^{- \lfloor n/k\rfloor},\]

which also holds for \(n\leq k\). Finally, it follows that

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right]\leq \mathbb{P}\left(\exists h\in V_{n}(\mathcal{H}):\text{er}_{P}(h)>0\right)\] \[= \mathbb{P}\left(P\left(x\in\mathcal{X}:\exists h\in V_{n}( \mathcal{H})\text{ s.t. }h(x)\neq h^{*}(x)\right)\neq 0\right)\leq 2^{- \lfloor n/k\rfloor},\ \ \forall n\in\mathbb{N}.\]

**Lemma 14** (Lemma 3 restated).: _If \(\mathcal{H}\) has an infinite eluder sequence centered at \(h^{*}\), then \(h^{*}\) is not universally learnable by ERM at rate faster than \(1/n\)._

Proof of Lemma 14.: Let \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) be an infinite eluder sequence centered at \(h^{*}\), we consider the following distribution \(P\): \(P\{(x_{i},y_{i})\}=2^{-i}\) and \(P\{(x_{i},1-y_{i})\}=0\) for all \(i\in\mathbb{N}\). Note that \(P\) is realizable (with respect to \(\mathcal{H}\)) with target \(h^{*}\). Given a dataset \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\), let the worst-case ERM outputs \(\hat{h}_{n}:=\text{ERM}(S_{n})\). For any \(t\in\mathbb{N}\), if \(S_{n}\) does not contain any copy of the points in \(\{x_{i},i>t\}\), we have \(\text{er}_{P}(\hat{h}_{n})\geq 2^{-t}\). The probability of such event is

\[\mathbb{P}\left(\sum_{i=1}^{n}\mathbbm{1}\left\{X_{i}\in\{x_{t+1},x_{t+2}, \ldots\}\right\}=0\right)=\prod_{i=1}^{n}\mathbb{P}\left(X_{i}\in\{x_{1}, \ldots,x_{t}\}\right)=\left(1-2^{-t}\right)^{n}.\]

Therefore, it follows immediately that

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right]\geq\sum_{t=1}^{\infty}2^{-t }\left(1-2^{-t}\right)^{n}\geq\frac{1}{n}\left(1-\frac{2}{n}\right)^{n}\geq \frac{1}{9n},\]

where the second inequality follows from choosing \(t=\lfloor\log n\rfloor\). 

**Lemma 15** (Lemma 4 restated).: _If \(\mathcal{H}\) does not have an infinite star-eluder sequence centered at \(h^{*}\), then \(h^{*}\) is universally learnable by ERM at rate \(1/n\)._

Before proceeding to the proof of Lemma 15, we first introduce several useful tools. The following definition of the sample compression scheme was originally stated in Littlestone and Warmuth (1986).

**Definition 12** (**Sample compression scheme**).: _Let \(\mathcal{H}\) be a concept class and \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\). A sample compression scheme for \(\mathcal{H}\) consists of two maps \((\kappa,\rho)\) such that the following hold:_

* _The compression map_ \(\kappa\) _takes_ \(S_{n}\) _to_ \(T:=\kappa(S_{n})\)_, for some_ \(T\in\bigcup_{t=0}^{\infty}\{(x,y)\in S_{n}\}^{t}\)_._
* _The reconstruction function_ \(\rho\) _takes_ \(T\) _to_ \(\rho(T):\mathcal{X}\to\{0,1\}\)_.__The size of the sample compression scheme \((\kappa,\rho)\) is defined as \(\max_{S_{n}\in(\mathcal{X}\times\{0,1\})^{n}}|\kappa(S_{n})|.\) A sample compression scheme \((\kappa,\rho)\) is called sample-consistent for \(\mathcal{H}\), if for any realizable distribution \(P\) with respect to \(\mathcal{H}\) and \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\), it holds that \(\hat{\text{er}}_{S_{n}}(\rho(\kappa(S_{n})))=0\). A sample compression scheme \((\kappa,\rho)\) is called stable if for every subsequence \(S^{{}^{\prime}}\) satisfying \(\kappa(S_{n})\subseteq S^{{}^{\prime}}\subset S_{n}\), it holds that \(\rho(\kappa(S^{{}^{\prime}}))=\rho(\kappa(S_{n}))\), that is, removing any non-compression point from \(S_{n}\) does not change the classifier returned by the sample compression scheme._

**Definition 13** (Version space compression set).: _Let \(\mathcal{H}\) be a concept class and \(P\) be a realizable distribution with respect to \(\mathcal{H}\). For any \(n\in\mathbb{N}\), and any dataset \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\), the version space compression set \(\hat{\mathcal{C}}_{n}\) is defined to be the smallest subset of \(S_{n}\) satisfying \(V_{S_{n}}(\mathcal{H})=\overline{V_{\hat{\mathcal{C}}_{n}}(\mathcal{H})}\). Furthermore, we define the version space compression set size as \(\hat{n}(S_{n}):=|\hat{\mathcal{C}}_{n}|,\) which is a data-dependent quantity. Finally, we define \(\hat{n}_{1:n}:=\max_{1\leq m\leq n}\hat{m}(S_{m})\), which is also data-dependent. However, \(\hat{n}_{1:n}\) is not only just dependent on the full sample, but is also dependent on any prefix of the sample (that is, the order of the sample)._

**Remark 21**.: _It has been argued in Wiener et al. (2015) that the region of disagreement of the version space \(\text{DIS}(V_{n}(\mathcal{H}))\) can be described as a compression scheme, where the size of the compression scheme is exactly the version space compression set size \(\hat{n}(S_{n})\)._

With these definitions in hand, we are now able to prove the lemma.

Proof of Lemma 15.: Let \(P\) be a realizable distribution with respect to \(\mathcal{H}\) centered at \(h^{*}\), let \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\) be a dataset and \(\hat{C}_{n}\subseteq S_{n}\) be the corresponding version space compression set with size \(|\hat{C}_{n}|=\hat{n}(S_{n})\). We let \((\kappa,\rho)\) be a sample compression scheme of size \(\hat{n}(S_{n})\) defined by \(\kappa(S_{n})=\hat{C}_{n}\) and \(\rho(\hat{C}_{n})=\hat{h}_{n}\). Since any ERM algorithm will output predictors \(\{\hat{h}_{n}\}_{n\in\mathbb{N}}\) satisfying \(\hat{h}_{n}\in V_{n}(\mathcal{H})\), it is clear that

\[\hat{\text{er}}_{S_{n}}\left(\rho\left(\kappa\left(S_{n}\right)\right)\right) =\sum_{i=1}^{n}\mathbbm{1}\left\{\rho\left(\kappa\left(S_{n}\right)\right)(x_ {i})\neq y_{i}\right\}=\sum_{i=1}^{n}\mathbbm{1}\left\{\hat{h}_{n}(x_{i})\neq y _{i}\right\}=0,\]

and thus it is sample-consistent. Furthermore, let \(S^{{}^{\prime}}\) be any subsequence satisfying \(\hat{C}_{n}\subseteq S^{{}^{\prime}}\subset S_{n}\). On one hand, we have \(V_{S_{n}}(\mathcal{H})\subseteq V_{S^{{}^{\prime}}}(\mathcal{H})\). On the other hand, we also have \(V_{S^{{}^{\prime}}}(\mathcal{H})\subseteq V_{\hat{C}_{n}}(\mathcal{H})=V_{S_{n }}(\mathcal{H})\). Therefore, we conclude \(V_{S_{n}}(\mathcal{H})=V_{S^{{}^{\prime}}}(\mathcal{H})\), and thus \(\rho(\kappa(S^{{}^{\prime}}))=\rho(\kappa(S_{n}))\), that is, the compression scheme \((\kappa,\rho)\) is also stable. Now we can apply Lemma 24, and then obtain

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right]=\mathbb{E}\left[\text{er}_{P }\left(\rho\left(\kappa\left(S_{n}\right)\right)\right)\right]\leq\frac{ \mathbb{E}[\hat{n}(S_{n})]}{n+1}.\]

Indeed, it has been proved that \(\hat{n}(S_{n})\leq\mathfrak{s}_{h^{*}}\)(Thm.13 Hanneke and Yang, 2015), and for completeness, we prove it as in Lemma 25 in Appendix E. The only remaining concern is that the fact "\(\mathcal{H}\) does not have an infinite star-eluder sequence centered at \(h^{*}\)" does not guarantee \(\mathfrak{s}_{h^{*}}<\infty\). However, it essentially states that the version space will eventually have a bounded star number centered at \(h^{*}\). Since \(\mathcal{H}\) does not have an infinite star-eluder sequence centered at \(h^{*}\), for any sequence \(S:=\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\sim P^{\mathbb{N}}\), there exists a data-dependent integer \(\bar{k}:=\bar{k}(S)<\infty\) such that \(\mathfrak{s}_{h^{*}}(V_{n_{\bar{k}}}(\mathcal{H}))<\bar{k}\). Moreover, we know there exists a distribution-dependent constant factor \(k:=k(P)<\infty\) such that \(\bar{k}(S)\leq k(P)\) with probability at least \(1/2\). By using a similar argument in the proof of Lemma 15, we have

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right]\lesssim\frac{k}{n}+2^{-\lfloor n /k\rfloor},\forall n\in\mathbb{N},\]

which proves a target-specified linear upper bound. 

**Lemma 16** (Lemma 5 restated).: _If \(\mathcal{H}\) has an infinite star-eluder sequence centered at \(h^{*}\), then \(h^{*}\) is not universally learnable by ERM at rate faster than \(\log{(n)}/n\)._

Proof of Lemma 16.: Suppose that \(S:=\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) is an infinite star-eluder sequence in \(\mathcal{H}\) centered at \(h^{*}\), that is \(h^{*}(x_{i})=y_{i}\) for all \(i\in\mathbb{N}\). For notation simplicity, let \(\mathcal{X}_{1}:=\{x_{1}\},\mathcal{X}_{2}:=\{x_{2},x_{3}\},\mathcal{X}_{3}:=\{x_ {4},x_{5},x_{6}\},\ldots,\mathcal{X}_{k}:=\{x_{n_{k}+1},\ldots,x_{n_{k}+k}\},\ldots\), with \(n_{k}:=\binom{k}{2}\). We consider a strictly increasing sequence \(\{k_{t}\}_{t\in\mathbb{N}}\) that will be specified later, and only put non-zero probability masses on these disjoint sets \(\mathcal{X}_{k_{t}}\) with \(t\in\mathbb{N}\). Then, let \(\mathcal{X}:=\bigcup_{t\in\mathbb{N}}\mathcal{X}_{k_{t}}\) be a union of disjoint finite sets with \(|\mathcal{X}_{k_{t}}|=k_{t}\), and consider the following marginal distribution \(P_{\mathcal{X}}\) on \(\mathcal{X}\):

\[P_{\mathcal{X}}\left\{x\in\mathcal{X}_{k_{t}}\right\}=2^{-t}\text{ and }P_{\mathcal{X}}(x)=2^{-t}/k_{t},\ \forall x\in\mathcal{X}_{k_{t}}.\]

It immediately implies the joint distribution \(P:=P(P_{\mathcal{X}},h^{*})\) that is realizable with respect to \(\mathcal{H}\):

\[P\left\{(x,h^{*}(x))\right\}=2^{-t}/k_{t},\ P\left\{(x,1-h^{*}(x))\right\}=0, \ \forall x\in\mathcal{X}_{k_{t}},\ \forall k\in\mathbb{N}.\]

Now for any \(n\in\mathbb{N}\), we let \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\) and consider the event \(\mathcal{E}:=\mathcal{E}_{1}\cap\mathcal{E}_{2}\), where

\[\mathcal{E}_{1} :=\left\{S_{n}\text{ does not contain a copy of any point in }\mathcal{X}_{k_{>t}}\right\},\] \[\mathcal{E}_{2} :=\left\{S_{n}\text{ does not contain a copy of at least one point in }\mathcal{X}_{k_{t}}\right\}.\]

If \(\mathcal{E}\) happens, the worst-case ERM can output some \(\hat{h}_{n}\in V_{S_{n}}(\mathcal{H})\) such that \(\text{er}_{P}(\hat{h}_{n})\geq 2^{-t}/k_{t}\). This is because: \(V_{n_{k_{t}}}(\mathcal{H})\subseteq V_{S_{n,k_{<t}}}(\mathcal{H})\), where \(S_{n,k_{<t}}\) contains the samples of \(S_{n}\) that falling into \(\mathcal{X}_{k_{1}}\cup\cdots\cup\mathcal{X}_{k_{t-1}}\), and then \(\mathcal{X}_{k_{t}}=\{x_{n_{k_{t}}+1},\ldots,x_{n_{k_{t}}+k_{t}}\}\) is a star set of \(V_{S_{n,k_{<t}}}(\mathcal{H})\) witnessed by a set of functions, denoted by \(\{h_{n_{k_{t}}+1},\ldots,h_{n_{k_{t}}+k_{t}}\}\). In other words, \(V_{S_{n,k_{<t}}}(\mathcal{H})\) contains a size-\(k_{t}\) "singletons" with point-wise probability mass \(2^{-t}/k_{t}\). However, the remaining samples \(S_{n}\cap\mathcal{X}_{k_{t}}\) does not contain a copy of every point in \(\mathcal{X}_{k_{t}}\), which results in an error rate \(\text{er}_{P}(\hat{h}_{n})\geq 2^{-t}/k_{t}\), with \(\hat{h}_{n}:=h_{n_{k_{t}}+j}\) for some \(1\leq j\leq k_{t}\).

Hence, it remains to characterize the probability of \(\mathcal{E}\). To this end, we refer to the so-called _Coupon Collector's Problem_, and define a random variable

\[\hat{n}_{k_{t}}:=\min\left\{n\in\mathbb{N}:\mathcal{X}_{k_{t}}\subseteq S_{n }\right\}.\]

Note that \(\hat{n}_{k_{t}}\) can be represented as a sum \(\sum_{j=1}^{k_{t}}G_{j}\) of independent geometric random variables \(G_{j}\sim\text{Geometric}(\frac{k_{t}+1-j}{k_{t}}2^{-t})\) for \(1\leq j\leq k_{t}\), with

\[\begin{cases}&\mathbb{E}\left[\hat{n}_{k_{t}}\right]=\sum_{j=1}^{k_{t}} \mathbb{E}\left[G_{j}\right]=\sum_{j=1}^{k_{t}}\frac{k_{t}\cdot 2^{t}}{k_{t}+1-j}=k_{t} \cdot 2^{t}\left(\sum_{j=1}^{k_{t}}\frac{1}{k_{t}+1-j}\right)=k_{t}\cdot 2^{t} \cdot H_{k_{t}}\\ &\text{Var}\left[\hat{n}_{k_{t}}\right]=\sum_{j=1}^{k_{t}}\text{Var}\left[G_{j }\right]<\sum_{j=1}^{k_{t}}\left(\frac{k_{t}+1-j}{k_{t}}2^{-t}\right)^{-2}< \frac{\pi^{2}\cdot k_{t}^{2}\cdot 2^{2t}}{6}\end{cases},\]

where \(H_{m}\) is \(m^{th}\) harmonic number satisfying \(H_{m}\gtrsim\log\left(m\right)\). Then the standard Chebyshev's inequality implies that \(\mathbb{P}(|\hat{n}_{k_{t}}-\mathbb{E}[\hat{n}_{k_{t}}]|>z)\leq\text{Var}[\hat {n}_{k_{t}}]\cdot z^{-2}\). By choosing \(z=\sqrt{2\text{Var}[\hat{n}_{k_{t}}]}\), we have with probability at least \(1/2\),

\[\hat{n}_{k_{t}}>\mathbb{E}\left[\hat{n}_{k_{t}}\right]-\sqrt{2\text{Var}\left[ \hat{n}_{k_{t}}\right]}\geq k_{t}\cdot 2^{t}\cdot\left(\log k_{t}-\frac{\pi}{\sqrt{3}} \right).\]

In particular, when \(k_{t}\geq 38\), it holds that \(\log k_{t}\geq 2\pi/\sqrt{3}\), and thus \(\hat{n}_{k_{t}}>2^{t-1}k_{t}\log k_{t}\) with probability at least \(1/2\). Altogether, we have for any \(n\leq 2^{t-1}k_{t}\log k_{t}\),

\[\mathbb{P}\left(\mathcal{E}_{2}\right)\geq\mathbb{P}\left(n<\hat{n}_{k_{t}} \right)\geq\mathbb{P}\left(n\leq 2^{t-1}k_{t}\log k_{t},\ 2^{t-1}k_{t}\log k_{t}<\hat{n}_{k_{t}}\right)\geq 1/2.\]

Moreover, to characterize the probability of \(\mathcal{E}_{1}\), note that for any \(x\sim P_{\mathcal{X}}\), \(\mathbb{P}(x\in\mathcal{X}_{k_{>t}})=2^{-k_{t}}\), which implies immediately that for any \(n\in\mathbb{N}\), \(\mathbb{P}(\mathcal{E}_{1})=(1-2^{-k_{t}})^{n}\). Now for any integer \(k_{t}\geq 38\), we let \(n=2^{t-1}k_{t}\log k_{t}\), and have (for infinitely many \(n\)) that

\[\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})\geq\frac{2^{-t}}{k_{t}}\right)\geq \mathbb{P}\left(\mathcal{E}\right)\geq\mathbb{P}\left(\mathcal{E}_{1}\right) \mathbb{P}\left(\mathcal{E}_{2}\right)\geq\frac{1}{2}\left(1-2^{-k_{t}}\right)^ {n},\]

which implies further

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right]\geq\frac{2^{-t}}{k_{t}} \mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})\geq\frac{2^{-t}}{k_{t}}\right)\geq \frac{1}{k_{t}2^{t+1}}\left(1-2^{-k_{t}}\right)^{n}=:\eta_{n,t}.\]

Finally, by choosing \(k_{t}=\Omega(2^{t})\), we can guarantee that \(n\geq\eta_{n,t}^{-1}\log\eta_{n,t}^{-1}\). Applying Lemma 29, we have \(\mathbb{E}[\text{er}_{P}(\hat{h}_{n})]\geq\eta_{n,t}\geq\log\left(n\right)/n\), for infinitely many \(n\). 

**Lemma 17** (**Lemma 6 restated)**.: _If \(\mathcal{H}\) does not have an infinite VC-eluder sequence centered at \(h^{*}\), then \(h^{*}\) is universally learnable by ERM at \(\log\left(n\right)/n\) rate._Proof of Lemma 17.: We first prove that any class \(\mathcal{H}\) is universally learnable by ERM at \(\log\left(n\right)/n\) rate if \(\text{VC}(\mathcal{H})<\infty\). For any realizable distribution \(P\) with respect to \(\mathcal{H}\), we let \(S_{2n}:=\{(x_{i},y_{i})\}_{i=1}^{2n}\sim P^{2n}\), and denote \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\) and \(T_{n}:=\{(x_{i},y_{i})\}_{i=n+1}^{2n}\), namely, the "ghost samples". Given \(\epsilon\in(0,1)\), Lemma 31 states that for any \(n\geq 8/\epsilon\),

\[\mathbb{P}\left(\exists h\in\mathcal{H}:\hat{\text{er}}_{S_{n}}(h)=0\text{ and }\text{er}_{P}(h)>\epsilon\right)\leq 2\mathbb{P}\left(\exists h\in \mathcal{H}:\hat{\text{er}}_{S_{n}}(h)=0\text{ and }\hat{\text{er}}_{T_{n}} \left(h\right)>\epsilon/2\right).\]

Moreover, Lemma 32 states that for any \(n\geq\text{VC}(\mathcal{H})/2\),

\[\mathbb{P}\left(\exists h\in\mathcal{H}:\hat{\text{er}}_{S_{n}}(h)=0\text{ and }\hat{\text{er}}_{T_{n}}\left(h\right)>\epsilon/2\right)\leq\left(\frac{2en}{ \text{VC}(\mathcal{H})}\right)^{\text{VC}(\mathcal{H})}2^{-n\epsilon/2}.\]

Altogether, we have

\[\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})>\epsilon\right)\leq\mathbb{P}\left( \exists h\in\mathcal{H}:\hat{\text{er}}_{S_{n}}(h)=0\text{ and }\text{er}_{P}(h)>\epsilon\right)\leq 2\left(\frac{2en}{ \text{VC}(\mathcal{H})}\right)^{\text{VC}(\mathcal{H})}2^{-\frac{n\epsilon}{2 }},\] (5)

for any \(n\geq\max\{8/\epsilon,\text{VC}(\mathcal{H})/2\}\). Finally, the upper bound on the expectation can be derived via the follow analysis (which will be used several times later): let

\[\epsilon_{n}:=\frac{2}{n}\left(\text{VC}(\mathcal{H})\log\left(\frac{2en}{ \text{VC}(\mathcal{H})}\right)+1\right),\]

and then by letting the RHS of (5) \(=:\delta\), we have

\[\epsilon=\frac{2}{n}\left(\text{VC}(\mathcal{H})\log\left(\frac{2en}{\text{ VC}(\mathcal{H})}\right)+\log\left(\frac{2}{\delta}\right)\right)>\epsilon_{n}.\]

When \(\epsilon\leq\epsilon_{n}\), we of course still have \(\mathbb{P}(\text{er}_{P}(\hat{h}_{n})>\epsilon)\leq 1\). It follows that for all \(n\geq\text{VC}(\mathcal{H})/2\),

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right] =\int_{0}^{1}\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})>\epsilon \right)d\epsilon\] \[=\int_{\frac{8}{n}}^{1}\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})> \epsilon\right)d\epsilon+\int_{0}^{\frac{8}{n}}\mathbb{P}\left(\text{er}_{P}( \hat{h}_{n})>\epsilon\right)d\epsilon\] \[=\int_{\frac{8}{n}}^{\epsilon_{n}}\mathbb{P}\left(\text{er}_{P}( \hat{h}_{n})>\epsilon\right)d\epsilon+\int_{\epsilon_{n}}^{1}\mathbb{P}\left( \text{er}_{P}(\hat{h}_{n})>\epsilon\right)d\epsilon+\int_{0}^{\frac{8}{n}} \mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})>\epsilon\right)d\epsilon\] \[\stackrel{{\eqref{eqof the uniform rate \(\log\left(n\right)/n\) can be applied since the version space has a bounded VC dimension \(d\). Finally, we have that for all \(n\in\mathbb{N}\),

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right]= \int_{0}^{\infty}\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})>\epsilon \right)d\epsilon\] \[\leq \int_{0}^{\infty}\left(\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n}) >\epsilon\Big{|}\mathcal{E}_{n}\right)+\mathbb{P}\left(\neg\mathcal{E}_{n} \right)\right)d\epsilon\lesssim\frac{d}{n}\log\left(\frac{n}{d}\right)+2^{- \lfloor n/k\rfloor},\]

where both \(d:=d(h^{*})\) and \(k:=k(P)\) are distribution-dependent constants. In conclusion, \(h^{*}\) is universally learnable by ERM at \(\log\left(n\right)/n\) rate. 

**Lemma 18** (**Lemma 7 restated**).: _If \(\mathcal{H}\) has an infinite VC-eluder sequence centered at \(h^{*}\), then \(h^{*}\) requires at least arbitrarily slow rates to be universally learned by ERM._

Proof of Lemma 18.: Let \(S:=\left\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\right\}\) be an infinite VC-eluder sequence in \(\mathcal{H}\) centered at \(h^{*}\), that is, \(h^{*}(x_{i})=y_{i}\) for all \(i\in\mathbb{N}\). We inherit the notations used in Lemma 5 by letting \(\mathcal{X}_{k}:=\{x_{n_{k}+1},\ldots,x_{n_{k}+k}\}\) with \(n_{k}:=\binom{k}{2}\), for all \(k\in\mathbb{N}\). Let \(\mathcal{X}:=\bigcup_{k\in\mathbb{N}}\mathcal{X}_{k}\) be a union of disjoint finite sets with \(|\mathcal{X}_{k}|=k\), and consider the following marginal distribution \(P_{\mathcal{X}}\) on \(\mathcal{X}\):

\[P_{\mathcal{X}}\left\{x\in\mathcal{X}_{k}\right\}=p_{k}\text{ and }P_{ \mathcal{X}}(x)=p_{k}/k,\ \forall x\in\mathcal{X}_{k},\]

where \(\{p_{k}\}_{k\in\mathbb{N}}\) is a sequence of probabilities satisfying \(\sum_{k\geq 1}p_{k}\leq 1\) that will be specified later. It implies immediately the following realizable (joint) distribution \(P:=P(P_{\mathcal{X}},h^{*})\):

\[P\left\{(x,h^{*}(x))\right\}=p_{k}/k,\ P\left\{(x,1-h^{*}(x))\right\}=0,\ \forall x\in\mathcal{X}_{k},\ \forall k\in\mathbb{N}.\]

Our remaining target is to show that for any rate function \(R(n)\to 0\), \(\mathcal{H}\) cannot be universally learned by the worst-case ERM at rate faster than \(R(n)\) under the distribution \(P\). To this end, we let \(S_{n}:=\left\{(x_{i},y_{i})\right\}_{i=1}^{n}\sim P^{n}\). For any \(t\in\mathbb{N}\) and any \(j\in[k_{t}]\), we consider the following event

\[\mathcal{E}_{n,k,t,j}:=\left\{S_{n}\text{ does not contain a copy of any point in }\mathcal{X}_{k_{>t}}\cup\left\{x_{n_{k_{t}}+j}\right\}\right\},\]

where \(\{k_{t}\}_{t\in\mathbb{N}}\) is an increasing sequence of integers that will be specified later. If \(\mathcal{E}_{n,k,t,j}\) happens, then the worst-case ERM algorithm can output some \(\hat{h}_{n}\in V_{n_{k_{t}}}(\mathcal{H})\) such that \(\text{er}_{P}(\hat{h}_{n})\geq p_{k_{t}}/k_{t}\), that is the classifier that predicts incorrectly on the "missing" point in \(\mathcal{X}_{k_{t}}\). Moreover, to characterize the probability of event \(\mathcal{E}_{n,k,t,j}\), we have \(\mathbb{P}(\mathcal{E}_{n,k,t,j})=(1-\sum_{k\geq k_{t}}p_{k}-p_{k_{t}}/k_{t})^ {n}\). Therefore, we make a construction by applying Lemma 30, and finally get for all \(t\in\mathbb{N}\),

\[\mathbb{E}\left[\text{er}_{P}\left(\hat{h}_{n_{t}}\right)\right]\geq\sum_{j\in [k_{t}]}p_{k_{t}}\cdot\mathbb{P}\left(\mathcal{E}_{n_{t},k,t,j}\right)\geq p _{k_{t}}\left(1-\sum_{k>k_{t}}p_{k}-\frac{p_{k_{t}}}{k_{t}}\right)^{n_{t}} \geq p_{k_{t}}\left(1-\frac{2}{n_{t}}\right)^{n_{t}}\gtrsim R\left(n_{t} \right).\]

### Omitted Proofs in Section 4

**Lemma 19** (**Lemma 8 restated**).: \(\mathcal{H}\) _has an infinite eluder sequence if and only if \(|\mathcal{H}|=\infty\)._

Proof of Lemma 19.: The necessity is straightforward. To show the sufficiency, we construct such an infinite eluder sequence via the following procedure: pick some \(x_{1}\in\mathcal{X}\) such that both \(V_{(x_{1},0)}(\mathcal{H}):=\{h\in\mathcal{H}:h(x_{1})=0\}\) and \(V_{(x_{1},1)}(\mathcal{H}):=\{h\in\mathcal{H}:h(x_{1})=1\}\) are non-empty. Such a point \(x_{1}\) must exist since otherwise we will have \(|\mathcal{H}|=1\). Furthermore, we know that at least one of them is infinite, since otherwise we will have \(|\mathcal{H}|<\infty\). We assume, without loss of generality, that \(|V_{(x_{1},0)}(\mathcal{H})|=\infty\). Then we pick some \(x_{2}\in\mathcal{X}\) such that both \(V_{(x_{1},0),(x_{2},0)}(\mathcal{H}):=\{h\in\mathcal{H}:h(x_{1})=0,h(x_{2})=0\}\) and \(V_{((x_{1},0),(x_{2},1))}(\mathcal{H}):=\{h\in\mathcal{H}:h(x_{1})=0,h(x_{2})=1\}\) are non-empty. Note that such an \(x_{2}\) exists, because otherwise we will have \(|V_{(x_{1},0)}(\mathcal{H})|<\infty\). Again, at least one of them is infinite for the same reason, and then we choose \(x_{3}\) from that infinite one. Following a similar procedure, we can get an infinite sequence \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\), where \(\{x_{1},x_{2},\ldots\}\) are chosen to keep the separates of version space non-empty, and \(\{y_{1},y_{2},\ldots\}\) are chosen to keep the version space infinite. Now note that for any \(i\in\mathbb{N}\), we can find some \(h_{i}\in V_{S_{i}}(\mathcal{H})\neq\emptyset\), where \(S_{i}=\{(x_{1},1-y_{1}),(x_{2},1-y_{2}),\ldots,(x_{i-1},1-y_{i-1}),(x_{i},y_{i})\}\). According to Definition 5, we know that \(\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) is an infinite eluder sequence consistent with \(\mathcal{H}\)

**Lemma 20** (Lemma 9 restated).: \(\mathcal{H}\) _has an infinite VC-eluder sequence if and only if \(\text{VC}(\mathcal{H})=\infty\)._

Proof of Lemma 20.: According to Definition 7, the necessity is straightforward, i.e. we must have \(\text{VC}(\mathcal{H})=\infty\) if \(\mathcal{H}\) has an infinite VC-eluder sequence. It remains to prove the sufficiency, that is, \(\text{VC}(\mathcal{H})=\infty\) yields the existence of an infinite VC-eluder sequence.

We construct an infinite VC-eluder sequence via the following procedure: Let \(x_{1}\in\mathcal{X}\) be any point, then at least one of \(V_{\{(x_{1},0)\}}(\mathcal{H})\) and \(V_{\{(x_{1},1)\}}(\mathcal{H})\) has an infinite VC dimension, which is because otherwise \(\mathcal{H}=V_{\{(x_{1},y_{1})\}}(\mathcal{H})\cup V_{\{(x_{1},1-y_{1})\}}( \mathcal{H})\) will have a finite VC dimension based on Lemma 28. Let \(y_{1}\in\{0,1\}\) such that \(V_{\{(x_{1},y_{1})\}}(\mathcal{H})\) has an infinite VC dimension, and let \(\{x_{2},x_{3}\}\) be a shattered set of \(V_{\{(x_{1},y_{1})\}}(\mathcal{H})\). Similarly, we know least one of the following four subclasses \(V_{\{(x_{1},y_{1}),(x_{2},0),(x_{3},0)\}}(\mathcal{H})\), \(V_{\{(x_{1},y_{1}),(x_{2},0),(x_{3},1)\}}(\mathcal{H})\), \(V_{\{(x_{1},y_{1}),(x_{2},1),(x_{3},0)\}}(\mathcal{H})\), \(V_{\{(x_{1},y_{1}),(x_{2},1),(x_{3},1)\}}(\mathcal{H})\) has an infinite VC dimension since otherwise \(\text{VC}(V_{\{(x_{1},y_{1})\}}(\mathcal{H}))<\infty\) will lead to a contradiction. We then pick labels \(y_{2},y_{3}\in\{0,1\}\) such that \(V_{\{(x_{1},y_{1}),(x_{2},y_{2}),(x_{3},y_{3})\}}(\mathcal{H})\) has an infinite VC dimension. For notation simplicity, let \(S:=\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) and let \(n_{k}:=\binom{k}{2}\). Inductively, for any \(k\in\mathbb{N}\), if \(V_{S_{1+2+\cdots+(k-1)}}(\mathcal{H})=V_{S_{n_{k}}}(\mathcal{H})\) has an infinite VC dimension, let \(\{x_{n_{k}+1},\ldots,x_{n_{k}+k}\}\) be a shattered set of \(V_{S_{n_{k}}}(\mathcal{H})\). Lemma 28 yields the existence of a set of labels \(\{y_{n_{k}+1},\ldots,y_{n_{k}+k}\}\in\{0,1\}^{k}\) such that \(V_{S_{n_{k+1}}}(\mathcal{H})\) has an infinite VC dimension. Otherwise,

\[\text{VC}\left(V_{S_{n_{k}}}(\mathcal{H})\right)=\text{VC}\left(\bigcup_{(y_{n _{k}+1},\ldots,y_{n_{k}+k})\in\{0,1\}^{k}}V_{S_{n_{k+1}}}(\mathcal{H})\right) \leq 2k+4\max\text{VC}\left(V_{S_{n_{k+1}}}(\mathcal{H})\right)<\infty,\]

which leads us to a contradiction! By such a construction, the returned infinite sequence \(S:=\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) is an infinite VC-eluder sequence consistent with \(\mathcal{H}\). 

### Omitted Proofs in Appendix C

**Lemma 21** (Lemma 10 restated).: _For every concept class \(\mathcal{H}\) with \(|\mathcal{H}|\geq 3\), if \(\text{VCE}(\mathcal{H})<\infty\), then the following hold:_

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right] \geq\frac{\text{VCE}(\mathcal{H})}{18n},\ \text{ for infinitely many }n\in\mathbb{N},\] \[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right] \leq\frac{28\text{VCE}(\mathcal{H})\log n}{n}+2^{-\lfloor n/2 \kappa\rfloor},\ \forall n\in\mathbb{N},\]

_where \(\kappa=\kappa(P)\) is a distribution-dependent constant._

Proof of Lemma 21.: Let \(\mathcal{H}\) be a concept class with \(\text{VCE}(\mathcal{H})=d<\infty\). Note that when \(\text{VCE}(\mathcal{H})=0\), the results hold trivially, and hence we consider only \(d\geq 1\) in the remaining part of the proof.

To show the upper bound, let \(P\) be a realizable distribution with respect to \(\mathcal{H}\), and for any \(n\in\mathbb{N}\), let \(S_{n}:=\{(x_{1},y_{1}),\ldots,(x_{n},y_{n})\}\sim P^{n}\) be a dataset. Since \(\text{VCE}(\mathcal{H})=d\), for any infinite sequence \(S:=\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\sim P^{\mathbb{N}}\), there exists a (smallest) non-negative integer \(k=k(S)<\infty\) such that \(\text{VC}(V_{k}(\mathcal{H}))\leq d\). For any ERM algorithm \(\mathcal{A}\), let \(\hat{h}_{n}:=\mathcal{A}_{\mathcal{H}}(S_{n})\in V_{n}(\mathcal{H})\), which can also be written as \(\hat{h}_{n}:=\hat{h}_{n,k}:=\mathcal{A}_{V_{k}(\mathcal{H})}\left(S_{k+1:n}\right)\), for every \(k\in[n]\). Recall that for any \(\epsilon\in(0,1)\), using the same argument as in the proof of Lemma 6, we can get

\[\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n,k})>\epsilon\right)\leq 2\left(\frac{2e(n-k)}{ d}\right)^{d}2^{-(n-k)\epsilon/2},\ \forall n\geq k+\max\left\{8/\epsilon,d/2\right\}.\] (6)

Now for any \(n\in\mathbb{N}\), we consider the event \(\mathcal{E}_{n}:=\{\text{VC}(V_{\lfloor n/2\rfloor}(\mathcal{H}))\leq d\}\). Applying the inequality \(\mathbb{P}(A)\leq\mathbb{P}(A|B)+\mathbb{P}(-B)\), we have

\[\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})>\epsilon\right)\leq\mathbb{P}\left( \text{er}_{P}(\hat{h}_{n,\lfloor n/2\rfloor})>\epsilon\Big{|}\mathcal{E}_{n} \right)+\mathbb{P}\left(\neg\mathcal{E}_{n}\right).\] (7)

Let the RHS of (6) \(=:\delta\in(0,1)\), then for the first probability in (7), we have

\[\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})>\epsilon\Big{|}\mathcal{E}_{n}\right)= \mathbb{P}\left(\text{er}_{P}(\hat{h}_{n,\lfloor n/2\rfloor})>\epsilon\Big{|} \text{VC}(V_{\lfloor n/2\rfloor}(\mathcal{H}))\leq d\right)\leq\delta,\]for all \(n\geq\max\left\{16/\epsilon,d\right\}\geq\left\lfloor n/2\right\rfloor+\max \left\{8/\epsilon,d/2\right\}\), and also

\[\epsilon=\frac{2}{n-\lfloor\frac{n}{2}\rfloor}\left(d\log\left(\frac{2e(n- \lfloor\frac{n}{2}\rfloor)}{d}\right)+\log\left(\frac{2}{\delta}\right)\right) \gtrsim\frac{4}{n}\left(d\log\left(\frac{ne}{d}\right)+1\right)=:\epsilon_{n}.\] (8)

To characterize the second probability in (7), we define \(\kappa=\kappa(P)\), a distribution-dependent quantity, to be the smallest integer such that \(k(S)\leq\kappa\) with probability at least \(1/2\), where the randomness is from the data sequence \(S\). Note that such an integer \(\kappa\) exists since otherwise there will exist at least an infinite \((d+1)\)-VC-eluder sequence. We then prove:

**Claim 2**.: _For any \(n\in\mathbb{N}\), \(\mathbb{P}(\neg\mathcal{E}_{n})\leq 2^{-\lfloor n/2\kappa\rfloor}\)._

Proof of Claim 2.: For any \(n\in\mathbb{N}\), assume first that \(n\geq\kappa\), we then split the dataset \(S_{n}\sim P^{n}\) into \(\lfloor n/\kappa\rfloor\) parts with each one sized \(\kappa\), denoted by \(S_{n,1},\ldots,S_{n,\lfloor n/\kappa\rfloor}\). According to the definition, for every \(1\leq i\leq\lfloor n/\kappa\rfloor\), we know that the induced version space has VC dimension \(\text{VC}(V_{S_{n,i}}(\mathcal{H}))\leq d\) with probability at least \(1/2\). Note that \(V_{n}(\mathcal{H})=\bigcap_{1\leq i\leq\lfloor n/\kappa\rfloor}V_{S_{n,i}}( \mathcal{H})\) satisfies \(\text{VC}(V_{n}(\mathcal{H}))\leq\text{VC}(V_{S_{n,i}}(\mathcal{H}))\), for all \(1\leq i\leq\lfloor n/\kappa\rfloor\). Therefore, we have

\[\mathbb{P}\left(\text{VC}(V_{n}(\mathcal{H}))>d\right)\leq\mathbb{P}\left( \forall 1\leq i\leq\lfloor n/\kappa\rfloor:\text{VC}(V_{S_{n,i}}(\mathcal{H}))>d \right)\leq 2^{-\lfloor n/\kappa\rfloor},\]

which also holds when \(n<\kappa\). Finally, \(\mathbb{P}(\neg\mathcal{E}_{n})=\mathbb{P}(\text{VC}(V_{\lfloor n/2\rfloor}( \mathcal{H}))>d)\leq 2^{-\lfloor n/2\kappa\rfloor}\). 

Putting together, we have that for all \(n\geq d\),

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right]=\int_{16/n}^{1 }\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})>\epsilon\right)d\epsilon+\int_{0} ^{16/n}\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})>\epsilon\right)d\epsilon\] \[\overset{\eqref{eq:v_1}}{\leq} \int_{16/n}^{1}\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n,\lfloor n/ 2\rfloor})>\epsilon\Big{|}\mathcal{E}_{n}\right)d\epsilon+\int_{0}^{16/n} \mathbb{P}\left(\text{er}_{P}(\hat{h}_{n,\lfloor n/2\rfloor})>\epsilon\Big{|} \mathcal{E}_{n}\right)d\epsilon+\int_{0}^{1}\mathbb{P}\left(\neg\mathcal{E}_{ n}\right)d\epsilon\] \[\overset{\eqref{eq:v_2}}{\leq} \epsilon_{n}+\int_{\epsilon_{n}}^{1}\mathbb{P}\left(\text{er}_{P} (\hat{h}_{n,\lfloor n/2\rfloor})>\epsilon\Big{|}\mathcal{E}_{n}\right)d \epsilon+\int_{0}^{1}\mathbb{P}\left(\neg\mathcal{E}_{n}\right)d\epsilon\] \[\overset{\eqref{eq:v_3}}{\leq} \epsilon_{n}+3\int_{\epsilon_{n}}^{\infty}2\left(\frac{ne}{d}\right) ^{d}2^{-n\epsilon/4}d\epsilon+\int_{0}^{1}\mathbb{P}\left(\neg\mathcal{E}_{n} \right)d\epsilon\] \[\leq \frac{4d}{n}\log\left(\frac{ne}{d}\right)+\frac{4+12\ln\left(2 \right)}{n}+\int_{0}^{1}\mathbb{P}\left(\neg\mathcal{E}_{n}\right)d\epsilon\] \[\overset{\text{claum}}{\leq} \frac{2d}{n}\log\left(\frac{ne}{d}\right)+\frac{4+12\ln\left(2 \right)}{n}+2^{-\lfloor n/2\kappa\rfloor}\leq\frac{28d\log n}{n}+2^{-\lfloor n /2\kappa\rfloor}.\]

When \(n\leq d\), the upper bound is trivial.

To show the lower bound, let \(S:=\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) be any infinite \(d\)-VC-eluder sequence that is consistent with \(\mathcal{H}\). We denote \(\mathcal{X}_{k}:=\{x_{kd-d+1},\ldots,x_{kd}\}\) for all integers \(k\geq 1\), and consider the following realizable distribution \(P\):

\[P\left\{(x_{kd-d+j},y_{kd-d+j})\right\}=p_{k}/d,\ P\left\{(x_{kd-d+j},1-y_{kd- d+j})\right\}=0,\ \forall 1\leq j\leq d,\ \forall k\geq 1,\]

where \(\{p_{k}\}_{k\geq 1}\) is a sequence of probabilities satisfying \(\sum_{k\geq 1}p_{k}\leq 1\), which will be specified later.

We use a similar argument in the proof of Lemma 7, but instead of considering an arbitrarily slow rate function \(R(n)\to 0\), we consider here \(R(n):=d/n\). Specifically, let \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\) be a dataset. Note that for any \(k\in\mathbb{N}\) and any \(j\in[d]\), if the dataset \(S_{n}\) does not contain any copy of the points in \(\mathcal{X}_{>k}\cup\{x_{kd-d+j}\}:=\bigcup_{t>k}\mathcal{X}_{t}\cup\{x_{kd-d+j}\}\), the worst-case ERM can have an error rate \(\text{er}_{P}(\hat{h}_{n})\geq p_{k}/d\). The probability of such event is \(\mathbb{P}(\sum_{i=1}^{n}\mathbbm{1}\{X_{i}\in\mathcal{X}_{>k}\cup\{x_{kd-d+j} \}\}=0)=\prod_{i=1}^{n}\mathbb{P}(X_{i}\notin\mathcal{X}_{>k}\cup\{x_{kd-d+j} \})=(1-\sum_{t>k}p_{t}-p_{k}/d)^{n}\).

Based on Lemma 30, for the rate function \(R(n):=d/n\), there exist probabilities \(\{p_{k}\}_{k\geq 1}\) satisfying \(\sum_{k\geq 1}p_{k}=1\), two increasing sequences of integers \(\{k_{t}\}_{t\geq 1}\) and \(\{n_{t}\}_{t\geq 1}\), and a constant \(1/2\leq C\leq 1\) such that \(\sum_{k\geq k_{t}}p_{k}\leq 1/n_{t}\) and \(p_{k_{t}}=C\cdot d/n_{t}\). Therefore, it follows that for all integers \(t\geq 1\) (and thus for infinitely many \(n\in\mathbb{N}\)),

\[\mathbb{E}\left[\text{er}_{P}\left(\hat{h}_{n_{t}}\right)\right]\geq C\cdot \frac{d}{n_{t}}\sum_{t\geq 1}\left(1-\frac{C+1}{n_{t}}\right)^{n_{t}}\geq\frac{d}{2n_{t}}\sum _{t\geq 1}\left(1-\frac{2}{n_{t}}\right)^{n_{t}}\geq\frac{d}{18n_{t}}.\]

**Lemma 22** (Lemma 11 restated).: _For every concept class \(\mathcal{H}\) with \(|\mathcal{H}|\geq 3\), if \(1\leq\text{SE}(\mathcal{H})<\infty\), then the following hold:_

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right] \geq\frac{\log\left(\text{SE}(\mathcal{H})\right)}{12n},\ \text{ for infinitely many }n\in\mathbb{N},\] \[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right] \leq\frac{160\text{VC}\text{E}(\mathcal{H})}{n}\log\left(\frac{ \text{SE}(\mathcal{H})}{\text{VC}\text{E}(\mathcal{H})}\right)+2^{-\lfloor n /2\hat{\kappa}\rfloor},\ \forall n\in\mathbb{N},\]

_where \(\hat{\kappa}=\hat{\kappa}(P)\) is a distribution-dependent constant._

Proof of Lemma 22.: Let \(\mathcal{H}\) be a concept class with \(\text{SE}(\mathcal{H})=s<\infty\) and \(\text{VCE}(\mathcal{H})=d<\infty\).

To prove the upper bound, let \(P\) be a realizable distribution with respect to \(\mathcal{H}\) centered at \(h\), and for any \(n\in\mathbb{N}\), let \(S_{n}:=\{(x_{1},y_{1}),\ldots,(x_{n},y_{n})\}\sim P^{n}\) be a dataset. Indeed, a distribution-free upper bound for any consistent learning rule has been proved in Hanneke (2016b) (see Lemma 26 in Appendix E), which states that for any \(\delta\in(0,1)\) and any \(n\in\mathbb{N}\), with probability at least \(1-\delta\),

\[\sup_{h\in V_{n}(\mathcal{H})}\text{er}_{P}(h)\leq\frac{8}{n}\left(\text{VC}( \mathcal{H})\ln\left(\frac{49\text{es}_{h}}{\text{VC}(\mathcal{H})}+37\right) +8\ln\left(\frac{6}{\delta}\right)\right).\] (9)

Since \(\mathcal{H}\) does not have an infinite \((d+1)\)-VC-eluder sequence, and also does not have an infinite \((s+1)\)-star-eluder sequence, for any infinite sequence \(S:=\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\sim P^{\mathbb{N}}\), there exists a (smallest) non-negative integer \(k=k(S)<\infty\) such that \(\text{VC}(V_{k}(\mathcal{H}))\leq d\) and \(\mathfrak{s}_{h}(V_{k}(\mathcal{H}))\leq s\). Following a similar argument in the proof of Lemma 10, we define \(\hat{\kappa}=\hat{\kappa}(P)\), a distribution-dependent quantity, to be the smallest integer such that \(k(S)\leq\hat{\kappa}\) with probability at least \(1/2\), and then consider the following event \(\hat{\mathcal{E}}_{n}:=\{\text{VC}(V_{\lfloor n/2\rfloor}(\mathcal{H}))\leq d, \mathfrak{s}_{h}(V_{\lfloor n/2\rfloor}(\mathcal{H}))\leq s\}\) with probability \(\mathbb{P}(\neg\hat{\mathcal{E}}_{n})\leq 2^{-\lfloor n/2\hat{\kappa}\rfloor}\). For notation simplicity, let us denote by \(\epsilon_{n}:=\frac{8}{n}(d\ln\left(\frac{49\text{es}}{d}+37\right)+8\ln\left( 6\right))\), then conditioning on \(\hat{\mathcal{E}}_{n}\), we have that for all \(n\in\mathbb{N}\),

\[\mathbb{E}\left[\sup_{\hat{h}_{n}\in V_{n}(\mathcal{H})}\text{er }_{P}(\hat{h}_{n})\right]=\int_{0}^{1}\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n })>\epsilon\right)d\epsilon\] \[\leq\,\int_{0}^{1}\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})> \epsilon\Big{|}\hat{\mathcal{E}}_{n}\right)d\epsilon+\int_{0}^{1}\mathbb{P} \left(\neg\hat{\mathcal{E}}_{n}\right)d\epsilon\] \[=\,\int_{0}^{\epsilon_{n}}\mathbb{P}\left(\text{er}_{P}(\hat{h}_ {n})>\epsilon\Big{|}\hat{\mathcal{E}}_{n}\right)d\epsilon+\int_{\epsilon_{n}}^ {1}\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})>\epsilon\Big{|}\hat{\mathcal{E} }_{n}\right)d\epsilon+\int_{0}^{1}\mathbb{P}\left(-\hat{\mathcal{E}}_{n} \right)d\epsilon\] \[\stackrel{{\eqref{eq:vCE}}}{{\leq}}\epsilon_{n}+\int_ {\epsilon_{n}}^{1}6\exp\left(\frac{d}{8}\ln\left(\frac{49\text{es}}{d}+37 \right)-\frac{n\epsilon_{n}}{64}\right)d\epsilon+2^{-\lfloor n/2\hat{\kappa} \rfloor}\] \[=\epsilon_{n}+\frac{384}{n}\exp\left(\frac{d}{8}\ln\left(\frac{49 \text{es}}{d}+37\right)-\frac{n\epsilon_{n}}{64}\right)+2^{-\lfloor n/2\hat{ \kappa}\rfloor}\] \[\leq\frac{8}{n}\left(d\ln\left(\frac{49\text{es}}{d}+37\right)+8 \ln\left(6\right)\right)+\frac{64}{n}+2^{-\lfloor n/2\hat{\kappa}\rfloor}\] \[\leq\frac{8d}{n}\log\left(\frac{(49\text{e}+37)s}{d}\right)+\frac {64\ln\left(6\right)+64}{n}+2^{-\lfloor n/2\hat{\kappa}\rfloor}\leq\frac{160d} {n}\log\left(\frac{s}{d}\right)+2^{-\lfloor n/2\hat{\kappa}\rfloor}.\]

To show the lower bound, let \(S:=\{(x_{1},y_{1}),(x_{2},y_{2}),\ldots\}\) be any infinite \(d\)-star-eluder sequence that is consistent with \(\mathcal{H}\). We denote \(\mathcal{X}_{k}:=\{x_{kd-d+1},\ldots,x_{kd}\}\) for every integer \(k\geq 1\), and consider the following realizable distribution \(P\) (with the same center of \(S\)):

\[P\left\{(x_{kd-d+j},y_{kd-d+j})\right\}=p_{k}/d,\ P\left\{(x_{kd-d+j},1-y_{kd-d+j })\right\}=0,\ \forall 1\leq j\leq d,\ \forall k\geq 1,\]

where \(\{p_{k}\}_{k\geq 1}\) is a sequence of probabilities satisfying \(\sum_{k\geq 1}p_{k}\leq 1\), which will be specified later. Let \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\) be a dataset and consider the event \(\mathcal{E}:=\mathcal{E}_{1}\cap\mathcal{E}_{2}\), where

\[\mathcal{E}_{1}:=\{S_{n}\text{ does not contain a copy of any point in }\mathcal{X}_{>k}\}\,,\] \[\mathcal{E}_{2}:=\{S_{n}\text{ does not contain a copy of at least one point in }\mathcal{X}_{k}\}\,.\]If the event \(\mathcal{E}\) happens, the worst-case ERM can have an error rate \(\text{er}_{P}(\hat{h}_{n})\geq p_{k}/d\). This is because \(\{x_{kd-d+1},\ldots,x_{kd}\}\) is a star set (with the same center) of \(V_{n}(\mathcal{H})\supseteq V_{kd-d}(\mathcal{H})\), and so we know that for any \(1\leq j\leq d\), there exists \(h_{k,j}\in V_{n}(\mathcal{H})\) such that \(h_{k,j}(x_{kd-d+j})\neq y_{kd-d+j}\), and the ERM outputting \(h_{k,j}\) will have such an error rate. The probability of \(\mathcal{E}_{1}\) follows simply as \(\mathbb{P}(\mathcal{E}_{1})=(1-\sum_{t>k}p_{t})^{n}\). Moreover, characterizing the probability of \(\mathcal{E}_{2}\) can be approached as an instance of the so-called _Coupon Collector's Problem_. Specifically, we let

\[\hat{n}_{k}:=\min\left\{n\in\mathbb{N}:\mathcal{X}_{k}\subseteq S_{n}\right\},\]

which can be represented as a sum \(\sum_{j=1}^{d}G_{j}\) of independent geometric random variables \(G_{j}\sim\text{Geometric}(\frac{d+1-j}{d}p_{k})\) for \(1\leq j\leq d\), with the following properties

\[\begin{cases}&\mathbb{E}\left[\hat{n}_{k}\right]=\sum_{j=1}^{d}\mathbb{E} \left[G_{j}\right]=\sum_{j=1}^{d}\frac{d\cdot p_{k}^{-1}}{d+1-j}=\frac{d}{p_{k} }\left(\sum_{j=1}^{d}\frac{1}{d+1-j}\right)=\frac{d}{p_{k}}\cdot H_{d}\\ &\text{Var}\left[\hat{n}_{k}\right]=\sum_{j=1}^{d}\text{Var}\left[G_{j}\right] <\sum_{j=1}^{d}\left(\frac{d+1-j}{d}p_{k}\right)^{-2}<\frac{\pi^{2}d^{2}}{6p_{ k}^{2}}\end{cases},\]

where \(H_{d}\) is \(d^{th}\) harmonic number satisfying \(H_{d}\geq\log d\), for all \(d\geq 1\). Then the standard Chebyshev's inequality implies that \(\mathbb{P}(|\hat{n}_{k}-\mathbb{E}[\hat{n}_{k}]|>z)\leq\text{Var}[\hat{n}_{k} ]\cdot z^{-2}\). By choosing \(z=\sqrt{2\text{Var}[\hat{n}_{k}]}\), we have with probability at least \(1/2\),

\[\hat{n}_{k}>\mathbb{E}\left[\hat{n}_{k}\right]-\sqrt{2\text{Var}\left[\hat{n} _{k}\right]}\geq\frac{d}{p_{k}}\left(\log d-\frac{\pi}{\sqrt{3}}\right).\]

In particular, when \(d\geq 38\), it holds that \(\log d\geq 2\pi/\sqrt{3}\), and thus \(\hat{n}_{k}>p_{k}^{-1}d\log d/2\), with probability at least \(1/2\). Altogether, we have for any \(n\leq p_{k}^{-1}d\log d/2\),

\[\mathbb{P}\left(\mathcal{E}_{2}\right)\geq\mathbb{P}\left(n<\hat{n}_{k}\right) \geq\mathbb{P}\left(n\leq p_{k}^{-1}d\log d/2,\;p_{k}^{-1}d\log d/2<\hat{n}_{ k}\right)\geq 1/2.\]

Now for all \(d\geq 38\), it follows from the proceeding analysis that

\[\mathbb{P}\left(\text{er}_{P}(\hat{h}_{n})\geq\frac{p_{k}}{d}\right)\geq \mathbb{P}\left(\mathcal{E}\right)\geq\mathbb{P}\left(\mathcal{E}_{1}\right) \mathbb{P}\left(\mathcal{E}_{2}\right)\geq\frac{1}{2}\left(1-\sum_{t>k}p_{t} \right)^{n},\]

which further implies that for all \(d\geq 38\),

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right]\geq\frac{p_{k}}{d}\mathbb{P }\left(\text{er}_{P}(\hat{h}_{n})\geq\frac{p_{k}}{d}\right)\geq\frac{p_{k}}{2d }\left(1-\sum_{t>k}p_{t}\right)^{n},\]

for all \(n\leq p_{k}^{-1}d\log d/2\). By letting \(n_{k}=p_{k}^{-1}d\log d/2\), we have for all \(k\in\mathbb{N}\) (infinitely many \(n\in\mathbb{N}\)),

\[\mathbb{E}\left[\text{er}_{P}\left(\hat{h}_{n_{k}}\right)\right]\geq\frac{p_{k }}{2d}\left(1-\sum_{t>k}p_{t}\right)^{n_{k}}=\frac{\log d}{4n_{k}}\left(1- \sum_{t>k}p_{t}\right)^{\frac{d\log d}{2p_{k}}}\geq\frac{\log d}{4en_{k}},\]

where the last inequality follows from choosing probabilities \(\{p_{k}\}_{k\geq 1}\) satisfying \(\sum_{t>k}p_{t}\leq 1/n_{k}\). When \(1\leq d<38\), the result is trivial. 

## Appendix E Technical lemmas

**Lemma 23** (Chernoff's bound).: _Let \(Z_{1},\ldots,Z_{n}\) be independent random variables in \(\{0,1\}\), let \(\bar{Z}:=\frac{1}{n}\sum_{i=1}^{n}Z_{i}\). For all \(t\in(0,1)\), we have_

\[\mathbb{P}\left(\bar{Z}\leq(1-t)\mathbb{E}[\bar{Z}]\right)\leq e^{-\frac{n \mathbb{E}[\bar{Z}]t^{2}}{2}}.\]

**Lemma 24**.: _Let \(\mathcal{H}\) be a concept class, and \((\kappa,\rho)\) be a stable sample compression scheme of size \(\hat{n}(S_{n})<n\) that is sample-consistent for \(\mathcal{H}\) given data \(S_{n}\). For any realizable distribution \(P\) with respect to \(\mathcal{H}\) and \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\), let \(\hat{h}_{n}:=\rho(\kappa(S_{n}))\). Then it holds_

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right]\leq\frac{\mathbb{E}[\hat{n}(S _{n})]}{n+1}.\]Proof of Lemma 24.: We claim that if \((x_{n+1},y_{n+1})\) satisfies \(\rho(\kappa(S_{n}))(x_{n+1})\neq y_{n+1}\), we must have \((x_{n+1},y_{n+1})\in\kappa(S_{n+1})\). Suppose not, then there is a subsequence \(S_{n}:=S_{n+1}\setminus\{(x_{n+1},y_{n+1})\}\) satisfying \(\kappa(S_{n+1})\subseteq S_{n}\subset S_{n+1}\) and \(\rho(\kappa(S_{n}))(x_{n+1})\neq y_{n+1}=\rho(\kappa(S_{n+1}))(x_{n+1})\) based on the sample-consistency of the compression scheme, which contradicts to our assumption that \((\kappa,\rho)\) is stable. Now by the exchangeability of random variables \(\{x_{i}\}_{i\geq 1}\), we have

\[\mathbb{E}\left[\text{er}_{P}(\hat{h}_{n})\right]= \mathbb{E}_{S_{n}}\left[\mathbb{P}\left\{\rho\left(\kappa\left(S_ {n}\right)\right)(x_{n+1})\neq y_{n+1}\right\}\right]\] \[= \mathbb{E}_{S_{n+1}}\left[\mathbb{1}\left\{\rho\left(\kappa \left(S_{n}\right)\right)(x_{n+1})\neq y_{n+1}\right\}\right]\] \[= \frac{1}{n+1}\sum_{i=1}^{n+1}\mathbb{E}\left[\mathbb{1}\left\{ \rho\left(\kappa\left(S_{n+1}\setminus\{(x_{i},y_{i})\}\right)\right)(x_{i}) \neq y_{i}\right\}\right]\] \[\leq \frac{1}{n+1}\sum_{i=1}^{n+1}\mathbb{E}\left[\mathbb{1}\left\{(x_ {i},y_{i})\in\kappa\left(S_{n+1}\right)\right\}\right]\leq\frac{\mathbb{E}[ \hat{n}(S_{n})]}{n+1}.\]

**Lemma 25** (**Hanneke and Yang**, 2015, Lemma 44).: _Let \(\mathcal{H}\) be a concept class, and \(P\) be a realizable distribution centered at the target \(h\). For any \(n\in\mathbb{N}\), let \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\) be a dataset, and let \(\hat{\mathcal{C}}_{n}\) be the version space compression set (Definition 13), that is, the smallest subset of \(S_{n}\) such that \(V_{\hat{\mathcal{C}}_{n}}(\mathcal{H})=V_{S_{n}}(\mathcal{H})\). We have \(|\hat{\mathcal{C}}_{n}|\leq\mathfrak{s}_{h}\)._

Proof of Lemma 25.: We assume that the dataset \(S_{n}\) is consistent with the target \(h\), i.e. \(h(x_{j})=y_{j}\) for all \(j\in[n]\). Note that, if there exists \((x_{j},y_{j})\in\hat{\mathcal{C}}_{n}\) such that every hypothesis \(g\in V_{\hat{\mathcal{C}}_{n}\setminus\{(x_{j},y_{j})\}}(\mathcal{H})\) satisfies \(g(x_{j})=h(x_{j})\), then we have \(V_{\hat{\mathcal{C}}_{n}\setminus\{(x_{j},y_{j})\}}(\mathcal{H})=V_{\hat{ \mathcal{C}}_{n}}(\mathcal{H})=V_{S_{n}}(\mathcal{H})\), which contradicts the definition of the version space compression set as the smallest subset. Therefore, for any \((x_{j},y_{j})\in\hat{\mathcal{C}}_{n}\), there exists \(g\in V_{\hat{\mathcal{C}}_{n}\setminus\{(x_{j},y_{j})\}}(\mathcal{H})\) such that \(g(x_{j})\neq h(x_{j})\). Moreover, note that "\(g\in V_{\hat{\mathcal{C}}_{n}\setminus\{(x_{j},y_{j})\}}(\mathcal{H})\)" is equivalent to saying "\(g(x)=y=h(x)\), for all \((x,y)\in\hat{\mathcal{C}}_{n}\setminus\{(x_{j},y_{j})\}\)", which precisely matches the definition of a star set centered at \(h\), that is, \(\hat{\mathcal{C}}_{n}\) is a star set for \(\mathcal{H}\) centered at \(h\), witnessed by those hypotheses \(g\)'s. We must have \(|\hat{\mathcal{C}}_{n}|\leq\mathfrak{s}_{h}\). 

**Lemma 26** (**Hanneke**, 2016b, Theorem 11).: _Let \(\mathcal{H}\) be a concept class, and \(P\) be a realizable distribution with respect to \(\mathcal{H}\) centered at \(h\), let \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\sim P^{n}\) be a dataset, for any \(n\in\mathbb{N}\). Then for any \(\delta\in(0,1)\) and any \(n\in\mathbb{N}\), we have with probability at least \(1-\delta\),_

\[\sup_{h\in V_{n}(\mathcal{H})}\text{er}_{P}(h)\leq\frac{8}{n}\left(\text{VC}( \mathcal{H})\ln\left(\frac{49e\hat{n}_{1:n}}{\text{VC}(\mathcal{H})}+37\right) +8\ln\left(\frac{6}{\delta}\right)\right),\]

_where the data-dependent quantity \(\hat{n}_{1:n}\) is defined in Definition 13 satisfying \(\hat{n}_{1:n}\leq\mathfrak{s}_{h}\)._

**Lemma 27** (**Sauer's lemma**, 1972).: _Let \(\mathcal{H}\) be a concept class with VC\((\mathcal{H})<\infty\) defined on \(\mathcal{X}\) and \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\in(\mathcal{X}\times\{0,1\})^{n}\). Then for all \(n\in\mathbb{N}\), it holds that_

\[\big{|}\mathcal{H}(S_{n})\big{|}\leq\sum_{i=0}^{\text{VC}(\mathcal{H})}\binom{ n}{i}.\]

_In particular, if \(n\geq\text{VC}(\mathcal{H})\),_

\[\big{|}\mathcal{H}(S_{n})\big{|}\leq\left(\frac{en}{\text{VC}(\mathcal{H})} \right)^{\text{VC}(\mathcal{H})}.\]

**Lemma 28** (**VC dimension of unions**).: _Let \(N,T\in\mathbb{N}\) and \(\mathcal{H}_{1},\dots,\mathcal{H}_{N}\) be concept classes with \(\max_{1\leq i\leq N}\text{VC}(\mathcal{H}_{i})\leq T\), then it holds_

\[\text{VC}\left(\bigcup_{i=1}^{N}\mathcal{H}_{i}\right)\leq 2\log N+4T.\]Proof of Lemma 28.: According to Sauer's lemma (Lemma 27), for any \(i\leq N\) and \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\) with \(n\geq T\), we have

\[\big{|}\mathcal{H}_{i}(S_{n})\big{|}\leq\sum_{i=0}^{\text{VC}(\mathcal{H}_{i})} \binom{n}{i}\leq\sum_{i=0}^{T}\binom{n}{i}\leq\left(\frac{en}{T}\right)^{T}.\] (10)

Then we can upper bound the number of possible classifications (of \(S_{n}\)) by the union \(\bigcup_{i=1}^{N}\mathcal{H}_{i}\) as

\[\Bigg{|}\left(\bigcup_{i=1}^{N}\mathcal{H}_{i}\right)(S_{n})\Bigg{|}\leq\sum_ {i=1}^{N}\big{|}\mathcal{H}_{i}(S_{n})\big{|}\overset{\eqref{eq:S_n}}{\leq} \sum_{i=1}^{N}\left(\frac{en}{T}\right)^{T}=N\left(\frac{en}{T}\right)^{T}.\] (11)

Let \(n=\text{VC}(\bigcup_{i=1}^{N}\mathcal{H}_{i})\) and \(S_{n}\) be a set shattered by \(\bigcup_{i=1}^{N}\mathcal{H}_{i}\), the LHS of (11) is exactly \(2^{n}\), and thus

\[2^{n}\leq N\left(\frac{en}{T}\right)^{T}\Rightarrow n\leq\log N+T\log\left( \frac{en}{T}\right)\Rightarrow n\leq 2\log N+4T,\]

where the last step follows from the fact that \(m\leq s+q\log\left({em}/{q}\right)\) implies \(m\leq 2s+4q\), for any \(s\geq 0\) and \(m\geq q\geq 1\). 

**Lemma 29** (Shalev-Shwartz and Ben-David, 2014, Lemma A.1).: _Let \(a>0\), then \(x\geq 2a\log a\) implies \(x\geq a\log x\). Conversely, \(x<a\log x\) implies \(x<2a\log a\)._

**Lemma 30** (Bousquet et al., 2021, Lemma 5.12).: _For any function \(R(n)\to 0\), there exist probabilities \(\{p_{t}\}_{t\in\mathbb{N}}\) satisfying \(\sum_{t\geq 1}p_{t}=1\), two increasing sequences of integers \(\{n_{t}\}_{t\in\mathbb{N}}\) and \(\{k_{t}\}_{t\in\mathbb{N}}\), and a constant \(1/2\leq C\leq 1\) such that the following hold for all \(t\in\mathbb{N}\):_

1. \(\sum_{k>k_{t}}p_{k}\leq\frac{1}{n_{t}}\)_._
2. \(n_{t}p_{k_{t}}\leq k_{t}\)_._
3. \(p_{k_{t}}=CR(n_{t})\)_._

**Lemma 31** (Ghost samples).: _Let \(\mathcal{H}\) be a concept class and \(P\) be a realizable distribution with respect to \(\mathcal{H}\). Let \(S_{2n}:=\{(x_{i},y_{i})\}_{i=1}^{2n}\sim P^{2n}\), \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\) and \(T_{n}:=\{(x_{i},y_{i})\}_{i=n+1}^{2n}\). Then for any \(\epsilon\in(0,1)\) and \(n\geq 8/\epsilon\), it holds_

\[\mathbb{P}\left(\exists h\in\mathcal{H}:\hat{\text{er}}_{S_{n}}(h)=0\text{ and }\text{er}_{P}(h)>\epsilon\right)\leq 2\mathbb{P}\left(\exists h\in \mathcal{H}:\hat{\text{er}}_{S_{n}}(h)=0\text{ and }\hat{\text{er}}_{T_{n}}\left(h \right)>\epsilon/2\right).\]

Proof of Lemma 31.: If there exists \(h\in\mathcal{H}\) such that \(\hat{\text{er}}_{S_{n}}(h)=0\) and \(\text{er}_{P}(h)>\epsilon\), since \(T_{n}\) is independent of \(S_{n}\), by applying the Chernoff's bound (Lemma 23), we have

\[\mathbb{P}\left(\hat{\text{er}}_{T_{n}}\left(h\right)\leq\epsilon/2\big{|}h \right)=\mathbb{P}\left(\frac{1}{n}\sum_{i=n+1}^{2n}\mathbbm{1}\left\{h(x_{i })\neq y_{i}\right\}\leq\frac{\epsilon}{2}\Big{|}\text{er}_{P}(h)>\epsilon \right)<\exp\left\{-\frac{n\epsilon}{8}\right\}.\]

Then for any \(n\geq 8/\epsilon\), it follows

\[\mathbb{P}\left(\hat{\text{er}}_{T_{n}}\left(h\right)>\epsilon/2\big{|}h \right)=1-\mathbb{P}\left(\hat{\text{er}}_{T_{n}}\left(h\right)\leq\epsilon/ 2\big{|}h\right)>1-\exp\left\{-\frac{n\epsilon}{8}\right\}\geq\frac{1}{2},\]

which completes the proof. 

**Lemma 32** (Random swaps).: _Let \(\mathcal{H}\) be a concept class with VC\((\mathcal{H})<\infty\) and \(P\) be a realizable distribution with respect to \(\mathcal{H}\). Let \(S_{2n}:=\{(x_{i},y_{i})\}_{i=1}^{2n}\sim P^{2n}\), \(S_{n}:=\{(x_{i},y_{i})\}_{i=1}^{n}\) and \(T_{n}:=\{(x_{i},y_{i})\}_{i=n+1}^{2n}\). Then for any \(\epsilon\in(0,1)\) and \(n\geq\text{VC}(\mathcal{H})/2\), it holds_

\[\mathbb{P}\left(\exists h\in\mathcal{H}:\hat{\text{er}}_{S_{n}}(h)=0\text{ and }\hat{\text{er}}_{T_{n}}\left(h\right)>\epsilon/2\right)\leq\left(\frac{2en}{\text{ VC}(\mathcal{H})}\right)^{\text{VC}(\mathcal{H})}2^{-n\epsilon/2}.\]

Proof of Lemma 32.: We prove the lemma by using the "random swaps" technique. Specifically, we define \(\sigma_{1},\ldots,\sigma_{n}\) to be independent random variables with \(\sigma_{i}\sim\text{Unif}(\{i,n+i\})\) for all \(1\leq i\leq n\), which are also independent of \(S_{2n}\). For notation simplicity, we denote by \(\sigma_{n+i}\) to be the remaining element in \(\{i,n+i\}\setminus\{\sigma_{i}\}\). Now we let \(S_{\sigma}:=\{(x_{\sigma_{1}},y_{\sigma_{1}}),\ldots,(x_{\sigma_{n}},y_{\sigma_{n}})\}\) and \(T_{\sigma}:=\{(x_{\sigma_{n+1}},y_{\sigma_{n+1}}),\ldots,(x_{\sigma_{2n}},y_{\sigma _{2n}})\}\), and note that \(S_{\sigma}\cup T_{\sigma}\) follows the same distribution as \(S_{2n}\). Hence, we have

\[\mathbb{P}\left(\exists h\in\mathcal{H}:\hat{\mathsf{er}}_{S_{n}}( h)=0\text{ and }\hat{\mathsf{er}}_{T_{n}}\left(h\right)>\epsilon/2\right)\] (12) \[= \mathbb{P}\left(\exists h\in\mathcal{H}:\hat{\mathsf{er}}_{S_{ \sigma}}\left(h\right)=0\text{ and }\hat{\mathsf{er}}_{T_{\sigma}}\left(h\right)> \epsilon/2\right)\] \[= \mathbb{P}\left(\exists(Y_{1},\ldots,Y_{2n})\in\mathcal{H}(S_{2n }):\begin{array}{c}\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}\left\{y_{\sigma_{i} }\neq Y_{\sigma_{i}}\right\}=0,\\ \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}\left\{y_{\sigma_{n+i}}\neq Y_{\sigma_{n+ i}}\right\}>\epsilon/2\end{array}\right)\] \[\stackrel{{\text{L\'{o}T}}}{{=}} \mathbb{E}\left[\mathbb{P}\left(\exists(Y_{1},\ldots,Y_{2n})\in \mathcal{H}(S_{2n}):\begin{array}{c}\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1} \left\{y_{\sigma_{i}}\neq Y_{\sigma_{i}}\right\}=0,\\ \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}\left\{y_{\sigma_{n+i}}\neq Y_{\sigma_{n+ i}}\right\}>\epsilon/2\end{array}\left|S_{2n}\right)\right]\] \[\stackrel{{\text{Union bound}}}{{\leq}} \mathbb{E}\left[\sum_{(Y_{1},\ldots,Y_{2n})\in\mathcal{H}(S_{2n})}\mathbb{P} \left(\begin{array}{c}\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}\left\{y_{\sigma_{ i}}\neq Y_{\sigma_{i}}\right\}=0,\\ \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}\left\{y_{\sigma_{n+i}}\neq Y_{\sigma_{n+ i}}\right\}>\epsilon/2\end{array}\left|S_{2n}\right)\right].\]

Next, we consider that given \(S_{2n}\), how possibly that the following event happens

\[\mathcal{E}_{Y}:=\left\{\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}\left\{y_{\sigma_ {i}}\neq Y_{\sigma_{i}}\right\}=0\text{ and }\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}\left\{y_{\sigma_{n+i}}\neq Y_{\sigma_{n+ i}}\right\}>\frac{\epsilon}{2}\right\},\]

for a given labeling \(Y:=(Y_{1},\ldots,Y_{2n})\in\mathcal{H}(S_{2n})\). Indeed, if \(\mathcal{E}_{Y}\) happens, then there must exist at least \(\lceil n\epsilon/2\rceil\) indices \(i\leq n\) such that either \(y_{i}=Y_{i},y_{n+i}\neq Y_{n+i}\) or \(y_{i}\neq Y_{i},y_{n+i}=Y_{n+i}\), otherwise, the difference between \(\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}\left\{y_{\sigma_{i}}\neq Y_{\sigma_{i}}\right\}\) and \(\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}\left\{y_{\sigma_{n+i}}\neq Y_{\sigma_{n+ i}}\right\}\) is less than \(\epsilon/2\). Based on this and the distribution of \(\sigma_{i}\)'s, we have

\[\mathbb{P}\left(\begin{array}{c}\frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}\left\{y _{\sigma_{i}}\neq Y_{\sigma_{i}}\right\}=0,\\ \frac{1}{n}\sum_{i=1}^{n}\mathbbm{1}\left\{y_{\sigma_{n+i}}\neq Y_{\sigma_{n+ i}}\right\}>\epsilon/2\end{array}\left|S_{2n}\right)\leq 2^{-\left\lceil\frac{n \epsilon}{2}\right\rceil}.\]

Plugging into (12), we finally get for all \(n\geq\text{VC}(\mathcal{H})/2\),

\[\mathbb{P}\left(\exists h\in\mathcal{H}:\hat{\mathsf{er}}_{S_{n}}( h)=0\text{ and }\hat{\mathsf{er}}_{T_{n}}\left(h\right)>\epsilon/2\right) \leq \mathbb{E}\left[\sum_{(Y_{1},\ldots,Y_{2n})\in\mathcal{H}(S_{2n}) }2^{-\left\lceil\frac{n\epsilon}{2}\right\rceil}\right]\] \[\leq \mathbb{E}\left[\mathcal{H}(S_{2n})\right]\cdot 2^{-n\epsilon/2}\] \[\stackrel{{\text{Lemma \ref{lem:2011}}}}{{\leq}}\left(\frac{2en}{ \text{VC}(\mathcal{H})}\right)^{\text{VC}(\mathcal{H})}\cdot 2^{-n\epsilon/2}.\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main results made in the abstract are discussed with details in Section 1.2. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Certain limitations are discussed in the paper (see e.g. Sections 1.1, 2). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The theoretical model that we are working on is formalized in Section 1. Due to space limitations, the complete proofs are postponed to the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems.

* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper is theoretical in its nature and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper is theoretical in its nature and does not include experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).

* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper is theoretical in its nature and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper is theoretical in its nature and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper is theoretical in its nature and does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in this paper follows the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper is theoretical in its nature and holds no foreseeable societal impacts as far as we can discern. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper is theoretical in its nature and poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.

* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **License for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper is theoretical in its nature and does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper is theoretical in its nature and does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.