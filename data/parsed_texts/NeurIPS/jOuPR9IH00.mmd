# Pessimistic Nonlinear Least-Squares Value Iteration

for Offline Reinforcement Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Offline reinforcement learning, where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, the theoretical understanding of offline RL with non-linear function approximation is still limited. Specifically, most existing works on offline RL with non-linear function approximation either have a poor dependency on the function class complexity or require an inefficient planning phase. In this paper, we propose an oracle-efficient algorithm PNLSVI for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. Our algorithm enjoys a regret bound that has a tight dependency on the function class complexity and achieves minimax optimal problem-dependent regret when specialized to linear function approximation. Our theoretical analysis introduces a new coverage assumption for nonlinear Q function, bridging the minimum-eigenvalue assumption and the uncertainty measure widely used in online nonlinear RL. To the best of our knowledge, this is the first statistically optimal algorithm for nonlinear offline RL.

## 1 Introduction

Offline reinforcement learning (RL), also known as batch RL, is a learning paradigm where an agent learns to make decisions based on a set of pre-collected data, instead of interacting with the environment in real-time like online RL. The goal of offline RL is to learn a policy that performs well in a given task, based on historical data that was collected from an unknown environment. Recent years have witnessed significant progress in developing offline RL algorithms that can leverage large amounts of data to learn effective policies. These algorithms often incorporate powerful function approximation techniques, such as deep neural networks, to generalize across large state-action spaces. They have achieved excellent performances in a wide range of domains, including the games of Go and chess (Silver et al., 2017; Schrittwieser et al., 2020), robotics (Gu et al., 2017; Levine et al., 2018), and control systems (Degrave et al., 2022).

Several studies have studied the theoretical guarantees of tabular offline RL and proved near-optimal sample complexities in this setting (Xie et al., 2021; Shi et al., 2022; Li et al., 2022). However, these algorithms cannot handle numerous real-world applications with large state spaces. Consequently, a significant body of research has shifted its focus to offline RL with function approximation. For example, several works have analyzed the sample efficiency of offline RL with linear function approximation under different MDP models, including linear MDPs and their variants (Jin et al., 2021; Zanette et al., 2021; Min et al., 2021; Yin et al., 2022a). To handle nonlinear function class, a recent line of research considered offline RL with general function approximation (Chen and Jiang,2019; Xie et al., 2021; Zhan et al., 2022). While these algorithms have sample efficiency guarantees, they often require an inefficient planning phase or have a poor dependency on the function class complexity. For example, Xie et al. (2021) proposed an information-theoretic algorithm that requires solving an optimization problem over all potential policy and corresponding version space, which includes all functions with lower Bellman error. To overcome this limitation, Xie et al. (2021) proposed a practical implementation, as a cost, the algorithm have a poor dependency on the function class complexity. Recently, (Yin et al., 2022) studied the general differentiable function class and propose a computation efficient algorithm (PFQL). However, their result also have an addition dependence on the dimension \(d\) of the parameter.

Therefore, a natural question arises:

_Can we design a computationally efficient algorithm that achieves the minimax optimality with respect to the complexity of nonlinear function class?_

We give an affirmative answer to the above question in this work. Our contributions are listed as follows:

* We propose a pessimism-based algorithm PNLSVI designed for nonlinear function approximation, which strictly generalizes the existing pessimism-based algorithms for both linear and differentiable function approximation (Xiong et al., 2022; Yin et al., 2022). Our algorithm is oracle-efficient, i.e., it is computationally efficient when there exists an efficient regression oracle and bonus oracle for the function class (e.g., generalized linear function class).
* We prove a data-dependent regret bound with the widely used \(D^{2}\)-divergence in online nonlinear RL regime, which is optimal with respect to the function class complexity. Our analysis closes the gap to optimality for differentiable function approximation, which was previously an open problem (Yin et al., 2022).
* We introduce a novel uniform coverage assumption for general function approximation that is generalized over the assumption in Yin et al. (2022). Our assumption bridges between the minimum-eigenvalue assumption used in linear models and the generalized dimension for nonlinear function class, offering new insights into the function approximation problem in RL.

**Notation:** In this work, we use lowercase letters to denote scalars and use lower and uppercase boldface letters to denote vectors and matrices respectively. For a vector \(\mathbf{x}\in\mathbb{R}^{d}\) and matrix \(\mathbf{\Sigma}\in\mathbb{R}^{d\times d}\), we denote by \(\|\mathbf{x}\|_{2}\) the Euclidean norm and \(\|\mathbf{x}\|_{\mathbf{\Sigma}}=\sqrt{\mathbf{x}^{\top}\mathbf{\Sigma}\mathbf{ x}}\). For two sequences \(\{a_{n}\}\) and \(\{b_{n}\}\), we write \(a_{n}=O(b_{n})\) if there exists an absolute constant \(C\) such that \(a_{n}\leq Cb_{n}\), and we write \(a_{n}=\Omega(b_{n})\) if there exists an absolute constant \(C\) such that \(a_{n}\geq Cb_{n}\). We use \(\widetilde{O}(\cdot)\) and \(\widetilde{\Omega}(\cdot)\) to further hide the logarithmic factors. For any \(a\leq b\in\mathbb{R}\), \(x\in\mathbb{R}\), let \([x]_{[a,b]}\) denote the truncate function \(a\cdot\mathds{1}(x\leq a)+x\cdot\mathds{1}(a\leq x\leq b)+b\cdot\mathds{1}(b \leq x)\), where \(\mathds{1}(\cdot)\) is the indicator function. For a positive integer \(n\), we use \([n]=\{1,2,..,n\}\) to denote the set of integers from \(1\) to \(n\).

## 2 Related Work

**RL with function approximation.** As one of the simplest function approximation classes, linear representation in RL has been extensively studied in recent years (Jiang et al., 2017; Dann et al., 2018; Yang and Wang, 2019; Jin et al., 2020; Wang et al., 2020; Du et al., 2019; Sun et al., 2019; Zanette et al., 2020, 2020; Weisz et al., 2021; Yang and Wang, 2020; Modi et al., 2020; Ayoub et al., 2020; Zhou et al., 2021; He et al., 2021). Several assumptions on the linear structure of the underlying MDPs have been made in these works, ranging from the _linear MDP_ assumption (Yang and Wang, 2019; Jin et al., 2020; Hu et al., 2022; He et al., 2022; Agarwal et al., 2022) to the _low Bellman-rank_ assumption (Jiang et al., 2017) and the _low inherent Bellman error_ assumption (Zanette et al., 2020). Extending the previous theoretical guarantees to more general problem classes, RL with nonlinear function classes has garnered increased attention in recent years (Wang et al., 2020; Jin et al., 2021; Foster et al., 2021; Du et al., 2021; Agarwal and Zhang, 2022; Agarwal et al., 2022). Various complexity measures of function classes have been studied including Bellman rank (Jiang et al., 2017), Bellman-Eluder dimension (Jin et al., 2021), Decision-Estimation Coefficient (Foster et al., 2021) and generalized Eluder dimension (Agarwal et al., 2022). Among these works, the setting in our paper is most related to Agarwal et al. (2022) where \(D^{2}\)-divergence (Gentile et al., 2022) was introduced in RL to indicate the uncertainty of a sample with respect to a particular sample batch.

Offline tabular RL.There is a line of works integrating the principle of pessimism to develop statistically efficient algorithms for offline tabular RL setting (Rashidinejad et al., 2021; Yin and Wang, 2021; Xie et al., 2021; Shi et al., 2022; Li et al., 2022). More specifically, Xie et al. (2021b) utilized the variance of transition noise and proposed a nearly optimal algorithm based on pessimism and Bernstein-type bonus. Subsequently, Li et al. (2022) proposed a model-based approach that achieves minimax-optimal sample complexity without burn-in cost for tabular MDPs. Shi et al. (2022) also contributed by proposing the first nearly minimax-optimal model-free offline RL algorithm.

Offline RL with linear function approximation.Jin et al. (2021b) presented the initial theoretical results on offline linear MDPs. They introduced a pessimism-principelied algorithmic framework for offline RL and proposed an algorithm based on LSVI (Jin et al., 2020). Min et al. (2021) subsequently considered offline policy evaluation (OPE) in linear MDPs, assuming independence between data samples across time steps to obtain tighter confidence sets and proposed an algorithm with optimal \(d\) dependence. Yin et al. (2022a) took one step further and considered the policy optimization in linear MDPs, which implicitly requires the same independence assumption. Zanette et al. (2021) proposed an actor-critic-based algorithm that establishes pessimism principle by directly perturbing the parameter vectors in a linear function approximation framework. Recently, Xiong et al. (2022) proposed a novel uncertainty decomposition technique via a reference function, which leads to a minimax-optimal sample complexity bound for offline linear MDPs without additional assumptions.

Offline RL with general function approximation.Chen and Jiang (2019) critically examined the assumptions underlying value-function approximation methods and established an information-theoretic lower bound. Xie et al. (2021a) introduced the concept of Bellman-consistent pessimism, which enables sample-efficient guarantees by relying solely on the Bellman-completeness assumption. Uehara and Sun (2021) focused on model-based offline RL with function approximation under partial coverage, demonstrating that realizability in the function class and partial coverage are sufficient for policy learning. Zhan et al. (2022) proposed an algorithm that achieves polynomial sample complexity under the realizability and single-policy concentrationly assumptions. Nguyen-Tang and Arora (2023) proposed a method of random perturbations and pessimism for neural function approximation. For differentiable function classes, Yin et al. (2022b) made advancements by improving the sample complexity with respect to the stage \(H\). However, their result had an additional dependence on the dimension \(d\) of the parameter space, whereas in linear function approximation, the dependence is typically on \(\sqrt{d}\).

## 3 Preliminaries

In our work, we consider the inhomogeneous episodic Markov Decision Processes (MDP), which can be denoted by a tuple of \(\mathcal{M}(\mathcal{S},\mathcal{A},H,\{r_{h}\}_{h=1}^{H},\{\mathbb{P}_{h}\}_ {h=1}^{H})\). In specific, \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the finite action space, \(H\) is the length of each episode. For each stage \(h\in[H],r_{h}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) is the reward function1 and \(\mathbb{P}_{h}(s^{\prime}|s,a)\) is the transition probability function, which denotes the probability for state \(s\) to transfer to next state \(s^{\prime}\) with current action \(a\). A policy \(\pi:=\{\pi_{h}\}_{h=1}^{H}\) is a collection of mappings \(\pi_{h}\) from a state \(s\in\mathcal{S}\) to the simplex of action space \(\mathcal{A}\). For simplicity, we denote the state-action pair as \(z:=(s,a)\). For any policy \(\pi\) and stage \(h\in[H]\), we define the value function \(V_{h}^{\pi}(s)\) and the action-value function \(Q_{h}^{\pi}(s,a)\) as the expected cumulative rewards starting at stage \(h\), which can be denoted as follows:

Footnote 1: While we study the deterministic reward functions for simplicity, it is not difficult to generalize our results to stochastic reward functions.

\[Q_{h}^{\pi}(s,a)=r_{h}(s,a)+\mathbb{E}\bigg{[}\sum_{h^{\prime}=h+1}^{H}r_{h^{ \prime}}\big{(}s_{h^{\prime}},\pi_{h^{\prime}}(s_{h^{\prime}})\big{)}\big{|}s_ {h}=s,a_{h}=a\bigg{]},\;V_{h}^{\pi}(s)=Q_{h}^{\pi}\big{(}s,\pi_{h}(s)\big{)},\]

where \(s_{h^{\prime}+1}\sim\mathbb{P}_{h}(\cdot|s_{h^{\prime}},a_{h^{\prime}})\) denotes the observed state at stage \(h^{\prime}+1\). By this definition, the value function \(V_{h}^{\pi}(s)\) and action-value function \(Q_{h}^{\pi}(s,a)\) are bounded in \([0,H]\). In addition, we define the optimal value function \(V_{h}^{\pi}\) and the optimal action-value function \(Q_{h}^{\pi}\) as \(V_{h}^{\ast}(s)=\max_{\pi}V_{h}^{\pi}(s)\) and \(Q_{h}^{\ast}(s,a)=\max_{\pi}Q_{h}^{\pi}(s,a)\). We denote the corresponding optimal policy by \(\pi^{\ast}\). For any function \(V:\mathcal{S}\rightarrow\mathbb{R}\), we denote \([\mathbb{P}_{h}V](s,a)=\mathbb{E}_{s^{\prime}\sim\mathbb{P}_{h}(\cdot|s,a)}V(s ^{\prime})\) and \([\mathrm{Var}_{h}V](s,a)=[\mathbb{P}_{h}V^{2}](s,a)-\big{(}[\mathbb{P}_{h}V](s,a)\big{)}^{2}\) for simplicity. For any function \(f:\mathcal{S}\rightarrow\mathbb{R}\), we define the Bellman operator \(\mathcal{T}_{h}\) as \(\mathcal{T}_{h}f(s_{h},a_{h})=\mathbb{E}_{s_{h+1}\sim\mathbb{P}_{h}(\cdot|s_{h},a_{h})}\left[r_{h}(s_{h},a_{h})+f(s_{h+1})\right]\), where we use the shorthand \(f(s)=\max_{a\in\mathcal{A}}f(s,a)\) for simplicity. Based on this definition, for every stage \(h\in[H]\) and policy \(\pi\), we have the following Bellman equation for value functions \(Q_{h}^{\pi}(s,a)\) and \(V_{h}^{\pi}(s)\), as well as the Bellman optimality equation for optimal value functions:

\[Q_{h}^{\pi}(s_{h},a_{h})=\mathcal{T}_{h}V_{h+1}^{\pi}(s_{h},a_{h}),\;Q_{h}^{*}(s_ {h},a_{h})=\mathcal{T}_{h}V_{h+1}^{*}(s_{h},a_{h}),\]

where \(V_{H+1}^{\pi}(s)=V_{H+1}^{*}(s)=0\). We also define the Bellman operator for second moment as \(\mathcal{T}_{2,h}f(s_{h},a_{h})=\mathbb{E}_{s_{h+1}\sim\mathcal{P}_{h}(\cdot| s_{h},a_{h})}\left[\left(r_{h}(s_{h},a_{h})+f(s_{h+1})\right)^{2}\right]\). For simplicity, we omit the subscripts \(h\) in the Bellman operator without causing confusion.

Offline Reinforcement Learning:In offline RL, the agent only have access to a batch-dataset \(D=\{s_{h}^{k},a_{h}^{k},r_{h}^{k}:h\in[H],k\in[K]\}\), which is collected by a behavior policy \(\mu\), and the agent cannot interact with the environment. Given the batch dataset, the goal of offline RL is finding a near-optimal policy \(\pi\) that minimize the sub-optimality \(V_{1}^{*}(s)-V_{1}^{\pi}(s)\). In addition, for each stage \(h\) and behavior policy \(\mu\), we denote the induced distribution of the state-action pair as \(d_{h}^{\mu}\).

General Function Approximation:In this work, we focus on a special class of episodic MDPs, where the value function satisfies the following completeness assumption.

**Assumption 3.1** (\(\epsilon\)-completeness under general function approximation, Agarwal et al. 2022).: Given a general function class \(\{\mathcal{F}_{h}\}_{h\in[H]}\), where each function class \(\mathcal{F}_{h}\) is composed of functions \(f_{h}:\mathcal{S}\times\mathcal{A}\rightarrow[0,L]\). We assume for each stage \(h\in[H]\), and any function \(V:\mathcal{S}\rightarrow[0,H]\), there exists functions \(f_{h},f_{2,h}\in\mathcal{F}_{h}\) such that

\[\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}|f_{h}(s,a)-\mathcal{T}_{h}V(s,a) |\leq\epsilon,\text{ and }\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}|f_{2,h}(s,a)-\mathcal{T}_{2,h}V( s,a)|\leq\epsilon.\]

In addition, for each stage \(h\in[H]\), we assume there exist a function \(f_{h}^{*}\in\mathcal{F}_{h}\) closed to the optimal value function such that \(\|f_{h}^{*}-Q_{h}^{*}\|_{\infty}\leq\epsilon\). For simplicity, we assume \(L=O(H)\) throughout the paper and denote \(\mathcal{N}=\max_{h\in[H]}|\mathcal{F}_{h}|\).

To deal with general function class \(\mathcal{F}\), Agarwal et al. (2022) introduce the following measure to capture the function class complexity for online learning.

**Definition 3.2** (Generalized Eluder dimension, Agarwal et al. 2022).: Given \(\lambda>0\), a sequence of state-action pairs \(Z=\{z_{i}\}_{i\in[K]}\) and a sequence of non-negative weights \(\boldsymbol{\sigma}=\{\sigma_{i}\}_{i\in[K]}\). Let \(\mathcal{F}\) be a function class consisting of functions \(f:\mathcal{S}\times\mathcal{A}\rightarrow[0,L]\). The generalized Eluder dimension of \(\mathcal{F}\) is given by \(\text{dim}_{\alpha,K}(\mathcal{F}):=\sup_{Z,\boldsymbol{\sigma}:|Z|=K, \boldsymbol{\sigma}\geq\alpha}\text{dim}(\mathcal{F},Z,\boldsymbol{\sigma})\), where

\[\text{dim}(\mathcal{F},Z,\boldsymbol{\sigma}) :=\sum_{i=1}^{K}\min\left(1,\frac{1}{\sigma_{i}^{2}}D_{\mathcal{F }}^{2}(z_{i};z_{[i-1]},\sigma_{[i-1]})\right),\] \[D_{\mathcal{F}}^{2}(z;z_{[k-1]},\sigma_{[k-1]}) :=\sup_{f_{1},f_{2}\in\mathcal{F}}\frac{(f_{1}(z)-f_{2}(z))^{2}} {\sum_{s\in[k-1]}\frac{1}{\sigma_{s}^{2}}(f_{1}(z_{s})-f_{2}(z_{s}))^{2}+ \lambda}.\]

Here, the inequality \(\boldsymbol{\sigma}\geq\alpha\) represents that \(\sigma_{i}\geq\alpha\) holds for all \(i\in[K]\) and we use the notation \(z_{[i-1]}\), \(\sigma_{[i-1]}\) to represent the sequences \(\{z_{s}\}_{s=1}^{i-1}\), \(\{\sigma_{s}\}_{s=1}^{i-1}\).

However, in offline RL, the proposed Generalized Eluder dimension fails to capture the relationship between function class \(\mathcal{F}\) and the pre-collected dataset \(\mathcal{D}\). To generalize this definition to offline environment, for a batch dataset \(\mathcal{D}=\{(s_{h}^{k},a_{h}^{k},r_{h}^{k})\}_{h,k=1}^{H,K}\) and a function class \(\mathcal{F}_{h}\) consisting of functions \(f:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\). We denote \(\mathcal{D}_{h}=\{(s_{h}^{k},a_{h}^{k},r_{h}^{k})\}_{k\in[K]}\) as the subset of the dataset \(D\) that corresponds to the observations collected up to stage \(h\) in the MDP. Then for any weight function \(\sigma_{h}(\cdot,\cdot):\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\), we introduce the following \(D^{2}\)-divergence:

\[D_{\mathcal{F}_{h}}^{2}(z;\mathcal{D}_{h};\sigma_{h})=\sup_{f_{1},f_{2}\in \mathcal{F}_{h}}\frac{(f_{1}(z)-f_{2}(z))^{2}}{\sum_{k\in[K]}\frac{1}{( \sigma_{h}(z_{h}^{k}))^{2}}(f_{1}(z_{h}^{k})-f_{2}(z_{h}^{k}))^{2}+\lambda}.\]

Data Coverage Assumption:In offline RL, there exist a discrepancy between the state-action distribution generated by the behavior policy and the distribution from the learned policy. Under this situation, the distribution shift problem can cause the learned policy to perform poorly or even fail in offline RL. Therefore, we propose the following data coverage assumption to control the distribution shift.

**Assumption 3.3** (Uniform Data Coverage).: There exists a constant \(\kappa>0\), such that for any stage \(h\) and functions \(f_{1},f_{2}\in\mathcal{F}_{h}\), the following inequality holds,

\[\mathbb{E}_{\mu,h}\left[\left(f_{1}(s_{h},a_{h})-f_{2}(s_{h},a_{h})\right)^{2} \right]\geq\kappa\|f_{1}-f_{2}\|_{\infty}^{2},\]

where the state-action pair \((\)at stage \(h)\((s_{h},a_{h})\) is stochastic generated from behavior policy \(\mu\).

**Remark 3.4**.: Data coverage assumption is widely used in offline RL to guarantee that the collected dataset contains enough information of the state-action space to learn an effective policy. In Yin et al. (2022b), they studied the general differentiable function, where the function class is defined as

\[\mathcal{F}:=\Big{\{}f\big{(}\boldsymbol{\theta},\boldsymbol{\phi}(\cdot, \cdot)\big{)}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R},\boldsymbol{ \theta}\in\Theta\Big{\}}.\]

Under this definition, Yin et al. (2022b) introduce the following coverage assumption (Assumption 2.3) such that for all stage \(h\in[H]\), there exists a constant \(\kappa\),

\[\mathbb{E}_{\mu,h}\left[\big{(}f(\boldsymbol{\theta}_{1}, \boldsymbol{\phi}(s,a))-f(\boldsymbol{\theta}_{2},\boldsymbol{\phi}(s,a)) \big{)}^{2}\right]\geq\kappa\|\boldsymbol{\theta}_{1}-\boldsymbol{\theta}_{2} \|_{2}^{2},\forall\boldsymbol{\theta}_{1},\boldsymbol{\theta}_{2}\in\Theta; \quad(*)\] \[\mathbb{E}_{\mu,h}\left[\nabla f(\boldsymbol{\theta},\boldsymbol{ \phi}(s,a))\nabla f(\boldsymbol{\theta},\boldsymbol{\phi}(s,a))^{\top}\right] \succ\kappa I,\forall\boldsymbol{\theta}\in\Theta.\quad(**)\]

We can prove that our assumption is weaker than the first assumption (*). For the second assumption (**), there is no direct counterpart in the general setting.

In addition, for the linear function class, the coverage assumption in Yin et al. (2022b) will reduce to the following linear function coverage assumption (Wang et al., 2020; Min et al., 2021; Yin et al., 2022a; Xiong et al., 2022).

\[\lambda_{\text{min}}(\mathbb{E}_{\mu,h}[\phi(s,a)\phi(s,a)^{\top}])=\kappa>0, \ \forall h\in[H].\]

Therefore, our assumption is also weaker than the linear function coverage assumption when dealing with the linear function class. Due to space limitations, we provide the detailed proof in the appendix.

## 4 Algorithm

```
0: Input confidence parameters \(\beta_{1,h}^{\prime},\beta_{2,h}^{\prime},\beta_{h}\) and \(\epsilon>0\).
1:Initialize: Split the input dataset into \(\mathcal{D}=\{s_{h}^{k},a_{h}^{k},r_{h}^{k}\}_{k,h=1}^{K,H},\mathcal{D}^{\prime} =\{\bar{s}_{h}^{k},\bar{a}_{h}^{k},\bar{r}_{h}^{k}\}_{k,h=1}^{K,H}\) ; Set the value function \(\widehat{f}_{H+1}(\cdot)=\widehat{f}_{H+1}(\cdot)=0\).
2:for stage \(h=H,\ldots,1\)do
3:\(\widehat{f}_{h}^{\prime}=\operatorname*{argmin}_{f_{h}\in\mathcal{F}_{h}} \sum_{k\in[K]}\left(f_{h}(\bar{s}_{h}^{k},\bar{a}_{h}^{k})-\bar{r}_{h}^{k}- \widehat{f}_{h+1}(\bar{s}_{h+1}^{k})\right)^{2}\).
4:\(\widehat{g}_{h}^{\prime}=\operatorname*{argmin}_{g_{h}\in\mathcal{F}_{h}} \sum_{k\in[K]}\left(g_{h}(\bar{s}_{h}^{k},\bar{a}_{h}^{k})-\left(\bar{r}_{h}^{ k}+\widehat{f}_{h+1}^{\prime}(\bar{s}_{h+1}^{k})\right)^{2}\right)^{2}\).
5: Use the bonus oracle (Definition 4.1) to calculate the bonus function \(b_{h}^{\prime}=\mathcal{B}(1,\mathcal{D}_{h}^{\prime},\mathcal{F}_{h},\widehat {f}_{h}^{\prime},\beta_{1,h}^{\prime}+\beta_{2,h}^{\prime},\lambda,\epsilon)\),
6:\(\widehat{f}_{h}^{\prime}\leftarrow\{\widehat{f}_{h}^{\prime}-b_{h}^{\prime}- \epsilon\}_{[0,H-h+1]}\);
7: Construct the variance estimator \(\widehat{\sigma}_{h}^{2}(s,a)=\max\left\{1,\widehat{g}_{h}^{\prime}(s,a)-( \widehat{f}_{h}^{\prime}(s,a))^{2}-O\left(\frac{\sqrt{\log\mathcal{N}}\mathcal{ N}_{h}H^{3}}{\sqrt{K\kappa}}\right)\right\}\).
8:endfor
9:for stage \(h=H,\ldots,1\)do
10:\(\widehat{f}_{h}=\operatorname*{argmin}_{f_{h}\in\mathcal{F}_{h}}\sum_{k\in[K]} \frac{1}{\widehat{g}_{h}^{2}(s_{h}^{k},a_{h}^{k})}\left(f_{h}(s_{h}^{k},a_{h}^ {k})-r_{h}^{k}-\widehat{f}_{h+1}(s_{h+1}^{k})\right)^{2}\)
11: Use the bonus oracle (Definition 4.1) to calculate the bonus function \(b_{h}=\mathcal{B}(\widehat{\sigma}_{h},\mathcal{D}_{h},\mathcal{F}_{h},\widehat {f}_{h},\beta_{h},\lambda,\epsilon)\);
12:\(\widehat{f}_{h}\leftarrow\{\widehat{f}_{h}-b_{h}-\epsilon\}_{[0,H-h+1]}\);
13:\(\widehat{\pi}_{h}(\cdot|s)=\operatorname*{argmax}_{a}\widehat{f}_{h}(s,a)\).
14:endfor
15:Output:\(\widehat{\pi}=\{\widehat{\pi}_{h}\}_{h=1}^{H}\). ```

**Algorithm 1** Pessimistic Nonlinear Least-Squares Value Iteration (PNLSVI)

In this section, we provide a comprehensive and detailed description of our algorithm (PNLSVI), as displayed in Algorithm 1. In the sequel, we introduce the key ideas of the proposed algorithm.

### Pessimistic Value Iteration Based Planning

Our algorithm operates in two distinct phases, Variance Estimate Phase and Pessimistic Planning Phase. At the beginning of the algorithm, the data-set is divided into two disjoint subsets \(\mathcal{D},\mathcal{D}^{\prime}\), and each assigned to a specific phase.

The basic framework of our algorithm follows the pessimistic value iteration, which was initially introduced by Jin et al. (2021b). In details, for each stage \(h\in[H]\), we construct the estimator value function \(\widetilde{f}_{h}\) by solving the following variance-weighted ridge regression (Line 11):

\[\widetilde{f}_{h}=\operatorname*{argmin}_{f_{h}\in\mathcal{F}_{h}}\sum_{k\in[ K]}\frac{1}{\widehat{\sigma}_{h}^{2}(s_{h}^{k},a_{h}^{k})}\left(f_{h}(s_{h}^{k},a_{h }^{k})-r_{h}^{k}-\widehat{f}_{h+1}(s_{h+1}^{k})\right)^{2},\]

where \(\widehat{\sigma}_{h}^{2}\) is the estimated variance and will be discussed in section 4.2. In Line 12, we subtract the confidence bonus function \(b_{h}\) from the estimator value function \(\widetilde{f}_{h}\) to construct the pessimistic value function \(\widehat{f}_{h}\). With the help of the confidence bonus function \(b_{h}\), the pessimistic value function \(\widehat{f}_{h}\) is almost a lower bound for the optimal value function \(f_{h}^{s}\). The details of the bonus function and bonus oracle will be discussed in section 4.3.

Based on the pessimistic value function \(\widehat{f}_{h}\) for stage \(h\), we recursively perform the value iteration for the stage \(h-1\). Finally, we use the pessimistic value function \(\widehat{f}_{h}\) to do planning and output the greedy policy with respect to the pessimistic value function \(\widehat{f}_{h}\) (Line 13 - Line 15).

### Variance Estimate Phase

In this phase, we provide a estimator for the variance \(\widehat{\sigma}_{h}\) in the weighted ridge regression. According to the definition of Bellman operators \(\mathcal{T}\) and \(\mathcal{T}_{2}\), the variance of the function \(\widehat{f}_{h+1}^{\prime}\) for each state-action pair \((s,a)\) can be denoted by

\[[\text{Var}_{h}\widehat{f}_{h+1}](s,a)=\mathcal{T}_{2,h}\widehat{f}_{h+1}^{ \prime}(s,a)-\left(\mathcal{T}_{h}\widehat{f}_{h+1}^{\prime}(s,a)\right)^{2}.\]

Therefore, we need the evaluate the first-order and second-order moments for \(\widehat{f}_{h}^{\prime}\). We perform nonlinear least-squares regression separately for each of these moments. Specifically, in Line 3, we conduct regression to estimate the first-order moment.

\[\widetilde{f}_{h}^{\prime}=\operatorname*{argmin}_{f_{h}\in\mathcal{F}_{h}} \sum_{k\in[K]}\left(f_{h}(\bar{s}_{h}^{k},\bar{a}_{h}^{k})-\bar{r}_{h}^{k}- \widehat{f}_{h+1}^{\prime}(\bar{s}_{h+1}^{k})\right)^{2}.\]

In Line 4, we perform regression for the second-order moment.

\[\widetilde{g}_{h}^{\prime}=\operatorname*{argmin}_{g_{h}\in\mathcal{F}_{h}} \sum_{k\in[K]}\left(g_{h}(\bar{s}_{h}^{k},\bar{a}_{h}^{k})-\left(\bar{r}_{h}^{ k}+\widehat{f}_{h+1}^{\prime}(\bar{s}_{h+1}^{k})\right)^{2}\right)^{2}.\]

In this phase, we set the variance function to \(1\) for each state-action pair \((s,a)\) and derive an estimator with confidence radius \(\beta_{1,h}^{\prime},\beta_{2,h}^{\prime}\). Combing these two regression results and subtracting a confidence bonus function \(b_{h}^{\prime}\), we create a pessimistic estimator for the variance function (Lines 6 to 7).

### Nonlinear Bonus Oracle

As we discussed in sections 4.1 and 4.2, we introduce a uncertainty bonus function to construct a pessimistic estimate of the value function. Unfortunately, for a general class, the uncertainty bonus may varies greatly across different state-action pair. Under this situation, the addition uncertainty bonus function will highly increase the complexity of the pessimistic function class, which make it difficult to construct a accurate estimation and may significant deteriorate the final performance. To address this issue, we assume there exist a function class \(\mathcal{W}\) with cardinally \(|\mathcal{W}|=\mathcal{N}_{b}\) and can approximate the bonus function well. In addition, we assume there exist a nonlinear bonus oracle (Agarwal and Zhang, 2022), which can output the approximate bonus function in the class \(\mathcal{W}\) for each dataset \(\mathcal{D}_{h}\).

**Definition 4.1** (Oracle for bonus function).: For an offline dataset \(\mathcal{D}=\{s_{h}^{k},a_{h}^{k},r_{h}^{k}\}_{h,k=1}^{H,K}\), given index \(h\in[H]\), let \(\mathcal{D}_{h}=\{(s_{h}^{k},a_{h}^{k},r_{h}^{k})\}_{k\in[K]}\) denote the subset of the dataset \(D\) that corresponds to the observations collected up to stage in the MDP. \(\widehat{\sigma}_{h}(\cdot,\cdot):\mathcal{S}\times\mathcal{A}\to\mathbb{R}\) is a variance function. \(\mathcal{F}_{h}\) is a function class such that \(\widehat{f}_{h}\in\mathcal{F}_{h}\). The parameters \(\beta_{h}\), \(\lambda\geq 0\), error parameter \(\epsilon\geq 0\). The bonus oracle \(\mathcal{B}(\widehat{\sigma},\mathcal{D}_{h},\mathcal{F}_{h},\widehat{f}_{h}, \beta_{h},\lambda,\epsilon)\) outputs a bonus function \(b_{h}(\cdot)\) such that

* \(b_{h}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}_{\geq 0}\) belongs to function class \(\mathcal{W}\).
* \(b_{h}(z_{h})\geq\max\left\{|f_{h}(z_{h})-\widehat{f}_{h}(z_{h})|,f_{h}\in \mathcal{F}_{h}:\sum_{k\in[K]}\frac{(f_{h}(z_{h}^{k})-\widehat{f}_{h}(z_{h}^{ k}))^{2}}{(\widehat{g}_{h}(z_{h}^{k},a_{h}^{k}))^{2}}\leq(\beta_{h})^{2}\right\}\) for any \(z_{h}\in\mathcal{S}\times\mathcal{A}\).
* \(b_{h}(z_{h})\leq C\cdot\left(D_{\mathcal{F}_{h}}(z_{h};\mathcal{D}_{h};\widehat {\sigma}_{h})\cdot\sqrt{(\beta_{h})^{2}+\lambda}+\epsilon\beta_{h}\right)\) for all \(z_{h}\in\mathcal{S}\times\mathcal{A}\) with constant \(0<C\leq\infty\).

**Remark 4.2**.: To address the concern of function class complexity, some previous studies (Xie et al., 2021a) have approached the problem differently. Instead of introducing a pointwise bonus in the estimated value function, they solve a complicated optimization problem to guarantee the optimism solely in the intial state. This method can prevent the complexity from bonus function, as a cost, they requires solving an optimization problem over all potential policy and corresponding version space, which includes all functions with lower Bellman error.

## 5 Main Results

In this section we prove an problem-dependent regret bound of Algorithm 1.

**Theorem 5.1**.: Under Assumption 3.3, for \(K\geq\widetilde{\Omega}\left(\frac{\log(\mathcal{N}\mathcal{N}_{b})H^{6}}{ \kappa^{2}}\right)\), if we set the parameters \(\beta_{1,h}^{\prime},\beta_{2,h}^{\prime}=\widetilde{O}(\sqrt{\log\mathcal{N }\mathcal{N}_{b}}H^{2})\) and \(\beta_{h}=\widetilde{O}(\sqrt{\log\mathcal{N}})\) in Algorithm 1, then with the probability of at least \(1-\delta\), for any state \(s\in\mathcal{S}\), we have

\[V_{1}^{*}(s)-V_{1}^{\widehat{\tau}}(s)\leq\widetilde{O}(\sqrt{\log\mathcal{N }})\sum_{h=1}^{H}\mathbb{E}_{\pi^{*}}\left[D_{\mathcal{F}_{h}}(z_{h};\mathcal{ D}_{h};[\mathbb{V}_{h}V_{h+1}^{*}](\cdot,\cdot))|s_{1}=s\right],\]

where \([\mathbb{V}_{h}V_{h+1}^{*}](s,a)=\max\{1,[\text{Var}_{h}V_{h+1}^{*}](s,a)\}\) is the truncated conditional variance.

**Remark 5.2**.: When reduce to the linear MDP environment, the following function classes

\[\mathcal{F}_{h}^{\text{lin}}=\{\langle\bm{\phi}_{h}(\cdot,\cdot),\bm{\theta}_{ h}\rangle:\bm{\theta}_{h}\in\mathbb{R}^{d},\|\bm{\theta}_{h}\|_{2}\leq B_{h}\} \text{ for any }h\in[H],\]

satisfy the completeness assumption (Assumption 3.1) (Jin et al., 2020). Let \(\mathcal{F}_{h}^{\text{lin}}(\epsilon)\) be a \(\epsilon\)-net of the linear function class \(\mathcal{F}_{h}^{\text{lin}}\). In this case, the covering number satisfies \(\log|\mathcal{F}_{h}^{\text{lin}}(\epsilon)|=\widetilde{O}(d)\) and the dependency of function class will reduce to \(\widetilde{O}(\sqrt{\log\mathcal{N}})=\widetilde{O}(\sqrt{d})\). For linear function class, Xiong et al. (2022) proposed the following regret guarantee,

\[V_{1}^{*}(s)-V_{1}^{\widehat{\tau}}(s)\leq\widetilde{O}(\sqrt{d})\cdot\sum_{h =1}^{H}\mathbb{E}_{\pi^{*}}\left[\|\bm{\phi}(s_{h},a_{h})\|_{\bm{\Sigma}_{h}^ {*-1}}|s_{1}=s\right],\]

where \(\Sigma_{h}^{*}=\sum_{k\in[K]}\bm{\phi}(s_{h}^{k},a_{h}^{k})\bm{\phi}(s_{h}^{k },a_{h}^{k})^{\top}/[\mathbb{V}_{h}V_{h+1}^{*}](s_{h}^{k},a_{h}^{k})+\lambda\bm{I}\). In comparison, we can prove the following inequality:

\[D_{\mathcal{F}_{h}^{\text{lin}}(\epsilon)}(z;\mathcal{D}_{h};[\mathbb{V}_{h}V_{ h+1}^{*}](\cdot,\cdot))\leq\|\bm{\phi}_{h}(z)\|_{\Sigma_{h}^{*-1}}.\]

This shows that Theorem 5.1 matches the optimal result in Xiong et al. (2022) for linear function class.

## 6 Key Techniques

In this section, we provide an overview of the key techniques in our algorithm design and analysis.

### Variance Estimator with Nonlinear Function Class

The technique of variance-weighted ridge regression, first introduced in Zhou et al. (2021), has demonstrated its effectiveness in the online RL setting with linear function approximation. For offlinesetting, Xiong et al. (2022) modified the variance-weighted ridge regression technique, and showed that using an accurate and independent variance estimator can improves the performance of the pessimistic value iteration (PEVI) algorithm (Jin et al., 2021b).

In our work, we extend this technique to general nonlinear function class \(\mathcal{F}\), and use the following nonlinear least-squares regression to estimate the underlying value function:

\[\widetilde{f}_{h}=\operatorname*{argmin}_{f_{h}\in\mathcal{F}_{h}}\sum_{k\in[ K]}\frac{1}{\widehat{\sigma}_{h}^{2}(s_{h}^{k},a_{h}^{k})}\left(f_{h}(s_{h}^{k},a_{h }^{k})-r_{h}^{k}-\widehat{f}_{h+1}(s_{h+1}^{k})\right)^{2}.\]

For this regression, it is crucial to obtain a reliable evaluation for the variance of the estimated cumulative reward \(r_{h}^{k}+\widehat{f}_{h+1}(s_{h+1}^{k})\). According to the definition of Bellman operators \(\mathcal{T}\) and \(\mathcal{T}_{2}\), the variance of the function \(\widehat{f}_{h+1}^{\prime}\) for each state-action pair \((s,a)\) can be denoted by

\[[\text{Var}_{h}\widehat{f}_{h+1}^{\prime}](s,a)=\mathcal{T}_{2,h}\widehat{f}_ {h+1}^{\prime}(s,a)-\left(\mathcal{T}_{h}\widehat{f}_{h+1}^{\prime}(s,a) \right)^{2}.\]

To evaluate the first and second moment for the Bellman operator, we perform nonlinear least-squares regression on a separate dataset \(\mathcal{D}^{\prime}\) with uniform weight (\(\widehat{\sigma}_{h}(s,a)=1\) for all state-action pair \((s,a)\)).

For simplicity, we denote the empirical variance as \(\mathbb{B}_{h}(s,a)=\widetilde{g}_{h}^{\prime}(s,a)-\left(\widetilde{f}_{h}^{ \prime}(s,a)\right)^{2}\), and the difference between empirical variance \(\mathbb{B}_{h}(s,a)\) with actually variance \([\text{Var}_{h}\widehat{f}_{h+1}^{\prime}](s,a)\) is upper bound by

\[\left|\mathbb{B}_{h}(s,a)-[\text{Var}_{h}\widehat{f}_{h+1}](s,a)\right|\leq \left|\widetilde{g}_{h}(s,a)-\mathcal{T}_{2,h}\widehat{f}_{h+1}(s,a)\right|+ \left|\left(\widetilde{f}_{h}(s,a)\right)^{2}-\left(\mathcal{T}_{h}\widehat{ f}_{h+1}(s,a)\right)^{2}\right|.\]

For these nonlinear function estimator, the following Lemmas provide coarse concentration properties for the first and second order Bellman operators.

**Lemma 6.1**.: Given a stage \(h\in[H]\), let \(\widehat{f}_{h+1}^{\prime}(\cdot,\cdot)\leq H\) be the estimated value function constructed in Algorithm 1 Line 6. By utilizing Assumption 3.1, there exists a function \(\bar{f}_{h}^{\prime}\in\mathcal{F}_{h}\), such that \(|\bar{f}_{h}^{\prime}(z_{h})-\mathcal{T}_{h}\widehat{f}_{h+1}^{\prime}(z_{h}) |\leq\epsilon\) holds for all state-action pair \(z_{h}=(s_{h},a_{h})\). Then with the probability of at least \(1-\delta/4H\), it holds that \(\sum_{k\in[K]}\left(\bar{f}_{h}^{\prime}(z_{h}^{k})-\widetilde{f}_{h}^{\prime} (\bar{z}_{h}^{k})\right)^{2}\leq(\beta_{1,h}^{\prime})^{2},\) where \(\beta_{1,h}^{\prime}=\widetilde{O}\left(\sqrt{\log\mathcal{N}\mathcal{N}_{b}} H^{2}\right)\), and \(\widetilde{f}_{h}^{\prime}\) is the estimated function for first-moment Bellman operator (Line 3 in Algorithm 1).

**Lemma 6.2**.: Given a stage \(h\in[H]\), let \(\widehat{f}_{h+1}^{\prime}(\cdot,\cdot)\leq H\) be the estimated value function constructed in Algorithm 1 Line 6. By utilizing Assumption 3.1, there exists a function \(\bar{g}_{h}^{\prime}\in\mathcal{F}_{h}\), such that \(|\bar{g}_{h}^{\prime}(z_{h})-\mathcal{T}_{2,h}\widehat{f}_{h+1}^{\prime}(z_{h} )|\leq\epsilon\) holds for all state-action pair \(z_{h}=(s_{h},a_{h})\). Then with the probability of at least \(1-\delta/4H\), it holds that \(\sum_{k\in[K]}\left(\bar{g}_{h}^{\prime}(z_{h}^{k})-\widetilde{g}_{h}^{\prime }(\bar{z}_{h}^{k})\right)^{2}\leq(\beta_{2,h}^{\prime})^{2},\) where \(\beta_{2,h}^{\prime}=\widetilde{O}\left(\sqrt{\log\mathcal{N}\mathcal{N}_{b}} H^{2}\right)\), and \(\widetilde{g}_{h}^{\prime}\) is the estimated function for second-moment Bellman operator (Line 4 in Algorithm 1).

Notice that all of the previous analysis focuses on the estimated function \(\widehat{f}_{h+1}^{\prime}\). By leveraging an induction procedure similar to existing works in the linear case (Jin et al., 2021b; Xiong et al., 2022), we can control the distance between the estimated function \(\widehat{f}_{h+1}^{\prime}\) and the optimal value function \(f_{h}^{*}\). In details, with high probability, for all stage \(h\in[H]\), the distance is upper bounded by \(O\left(\sqrt{\log\mathcal{N}\mathcal{N}_{b}}H^{3}/\sqrt{K\kappa}\right)\). This result allows us to further bound \([\text{Var}_{h}\widehat{f}_{h+1}^{\prime}](s,a)\) and \([\text{Var}_{h}f_{h+1}^{*}](s,a)\).

Therefore, the concentration properties in Lemmas 6.1 and 6.2 enable us to construct the pessimistic variance estimator, which satisfies the following property:

\[[\mathbb{V}_{h}V_{h+1}^{*}](s,a)-\widetilde{O}\bigg{(}\frac{\sqrt{\log \mathcal{N}\mathcal{N}_{b}}H^{3}}{\sqrt{K\kappa}}\bigg{)}\leq\widehat{\sigma}_ {h}^{2}(s,a)\leq[\mathbb{V}_{h}V_{h+1}^{*}](s,a).\] (6.1)

where \([\mathbb{V}_{h}V_{h+1}^{*}](s,a)=\max\{1,[\text{Var}_{h}V_{h+1}^{*}](s,a)\}\) is the truncated conditional variance. Compared with the results in the linear function class, we utilize the logarithm of the covering number of the function class as a substitute for the linear dimension \(d\), which is a common technique in nonlinear function approximation.

### Reference-Advantage Decomposition

The reference-advantage decomposition is a powerful technique to tackle the challenge of additional error from uniform concentration over whole function class \(\mathcal{F}_{h}\). Such an analysis approach has been first studied in the online RL setting Azar et al. (2017); Zhang et al. (2021); Hu et al. (2022); He et al. (2022); Agarwal et al. (2022) and later in the offline environment by Xiong et al. (2022).

For offline RL, in the context of nonlinear function classes, without a explicit linear expression, the increased complexity of the function class structure poses a significant obstacle to effectively utilizing this technique. Previous works, such as Yin et al. (2022b), have struggled to adapt the reference-advantage decomposition to their nonlinear function class, resulting in a parameter space dependence that scales with \(d\), instead of the optimal \(\sqrt{d}\). We provide detailed insights into this approach as follows:

\[r_{h}(s,a)+\widehat{f}_{h+1}(s,a)-\mathcal{T}_{h}\widehat{f}_{h+1}(s,a)= \underbrace{r_{h}(s,a)+f_{h+1}^{*}(s,a)-\mathcal{T}_{h}f_{h+1}^{*}(s,a)}_{ \text{Reference uncertainty}}\]

\[+\underbrace{\widehat{f}_{h+1}(s,a)-f_{h+1}^{*}(s,a)-([\mathbb{P}_{h}\widehat {f}_{h+1}](s,a)-[\mathbb{P}_{h}f_{h+1}^{*}](s,a))}_{\text{Advantage uncertainty}}.\]

We decompose the Bellman error into two parts: the Reference uncertainty and the Advantage uncertainty. For the first term, the optimal value function \(f_{h+1}^{*}\) is fixed and not related to the pre-collected dataset, which circumvents additional uniform concentration over the whole function class and avoid any dependence on the function class size. For the second term, it is worth to notice that the distance between the estimated function \(\widehat{f}_{h+1}^{\prime}\) and the optimal value function \(f_{h}^{*}\) is decreased as \(O(1/\sqrt{K\kappa})\). Though, we still need to maintain the uniform convergence guarantee, the Advantage uncertainty is dominated by the Reference uncertainty when the number of episode \(K\) is large enough. By integrating these results, we can prove a variance-weighted concentration inequality for Bellman operators.

**Lemma 6.3**.: For each stage \(h\in[H]\), assuming the variance estimator \(\widehat{\sigma}_{h}\) satisfies (6.1), let \(\widehat{f}_{h+1}(\cdot,\cdot)\leq H\) be the estimated value function constructed in Algorithm 1 Line 12. By utilizing Assumption 3.1, there exist a function \(\bar{f}_{h}\in\mathcal{F}_{h}\), such that \(|\bar{f}_{h}(z_{h})-\mathcal{T}_{h}\widehat{f}_{h+1}(z_{h})|\leq\epsilon\) holds for all state-action pair \(z_{h}=(s_{h},a_{h})\). Then with the probability of at least \(1-\delta/4H\), it holds that \(\sum_{k\in[K]}\frac{1}{(\widehat{\sigma}_{h}(z_{h}^{k}))^{2}}\left(\bar{f}_{ h}(z_{h}^{k})-\widetilde{f}_{h}(z_{h}^{k})\right)^{2}\leq(\beta_{h})^{2},\) where \(\beta_{h}=\widetilde{O}(\sqrt{\log\mathcal{N}})\) and \(\widetilde{f}_{h}\) is the estimated function from the weighted ridge regression (Line 10 in Algorithm 1).

After controlling the Bellman error, with a similar argument to Jin et al. (2021b); Xiong et al. (2022), we obtain the following lemma, which provide an upper bound for the regret.

**Lemma 6.4** (Regret Decomposition Property).: If \(|\mathcal{T}_{h}\widehat{f}_{h+1}(z)-\widetilde{f}_{h}(z)|\leq b_{h}(z)\) holds for all stage \(h\in[H]\) and state-action pair \(z=(s,a)\in\mathcal{S}\times\mathcal{A}\), then the regret of Algorithm 1 can be bounded as

\[V_{1}^{*}(s)-V_{1}^{\widehat{\pi}}(s)\leq 2\sum_{h=1}^{H}\mathbb{E}_{\pi^{*}} \left[b_{h}\left(s_{h},a_{h}\right)\mid s_{1}=s\right].\]

Here, the expectation \(\mathbb{E}_{\pi^{*}}\) is with respect to the trajectory induced by \(\pi^{*}\) in the underlying MDP.

Combing the results in Lemmas 6.3 and 6.4, we have proved Theorem 5.1.

## 7 Conclusion and Future Work

In this paper, we present PNLSVI, an oracle-efficient algorithm for offline RL with non-linear function approximation. It achieves minimax optimal problem-dependent regret when specialized to linear function approximation.

Regarding future work, we observe that instead of using the uniform coverage assumption, a series of works, such as (Liu et al., 2020; Xie et al., 2021a; Uehara and Sun, 2021; Zhan et al., 2022), only relies on partial coverage assumption. In these works, the offline data distribution only encompasses the state-action distribution of a select high-quality comparator policy \(\pi^{*}\). It would be of significant interest to investigate whether it's possible to design practical algorithms for nonlinear function classes under this weaker partial coverage assumption, while still preserving the inherent efficiency found in linear function approximation.

## References

* Agarwal et al. (2022)Agarwal, A., Jin, Y. and Zhang, T. (2022). Vo \(q\) l: Towards optimal regret in model-free rl with nonlinear function approximation. _arXiv preprint arXiv:2212.06069_.
* Agarwal and Zhang (2022)Agarwal, A. and Zhang, T. (2022). Model-based rl with optimistic posterior sampling: Structural conditions and sample complexity. _arXiv preprint arXiv:2206.07659_.
* Ayoub et al. (2020)Ayoub, A., Jia, Z., Szepesvari, C., Wang, M. and Yang, L. (2020). Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_. PMLR.
* Azar et al. (2017)Azar, M. G., Osband, I. and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_. PMLR.
* Chen and Jiang (2019)Chen, J. and Jiang, N. (2019). Information-theoretic considerations in batch reinforcement learning. In _International Conference on Machine Learning_. PMLR.
* Dann et al. (2018)Dann, C., Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J. and Schapire, R. E. (2018). On oracle-efficient pac rl with rich observations. _Advances in neural information processing systems_**31**.
* Degrave et al. (2022)Degrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B., Carpanese, F., Ewalds, T., Hafner, R., Abdolmaleki, A., de Las Casas, D. et al. (2022). Magnetic control of tokamak plasmas through deep reinforcement learning. _Nature_**602** 414-419.
* Du et al. (2021)Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W. and Wang, R. (2021). Bilinear classes: A structural framework for provable generalization in rl. In _International Conference on Machine Learning_. PMLR.
* Du et al. (2019)Du, S. S., Kakade, S. M., Wang, R. and Yang, L. F. (2019). Is a good representation sufficient for sample efficient reinforcement learning? _arXiv preprint arXiv:1910.03016_.
* Foster et al. (2021)Foster, D. J., Kakade, S. M., Qian, J. and Rakhlin, A. (2021). The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_.
* Gentile et al. (2022)Gentile, C., Wang, Z. and Zhang, T. (2022). Achieving minimax rates in pool-based batch active learning. In _International Conference on Machine Learning_. PMLR.
* Gu et al. (2017)Gu, S., Holly, E., Lillicrap, T. and Levine, S. (2017). Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In _2017 IEEE international conference on robotics and automation (ICRA)_. IEEE.
* He et al. (2022)He, J., Zhao, H., Zhou, D. and Gu, Q. (2022). Nearly minimax optimal reinforcement learning for linear markov decision processes. _arXiv preprint arXiv:2212.06132_.
* He et al. (2021)He, J., Zhou, D. and Gu, Q. (2021). Logarithmic regret for reinforcement learning with linear function approximation. In _International Conference on Machine Learning_. PMLR.
* Hu et al. (2022)Hu, P., Chen, Y. and Huang, L. (2022). Nearly minimax optimal reinforcement learning with linear function approximation. In _International Conference on Machine Learning_. PMLR.
* Jiang et al. (2017)Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J. and Schapire, R. E. (2017). Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_. PMLR.
* Jin et al. (2021)Jin, C., Liu, Q. and Miryoosefi, S. (2021a). Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. _Advances in neural information processing systems_**34** 13406-13418.
* Jin et al. (2020)Jin, C., Yang, Z., Wang, Z. and Jordan, M. I. (2020). Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_. PMLR.
* Jin et al. (2021)Jin, Y., Yang, Z. and Wang, Z. (2021b). Is pessimism provably efficient for offline rl? In _International Conference on Machine Learning_. PMLR.

Levine, S., Pastor, P., Krizhevsky, A., Ibarz, J. and Quillen, D. (2018). Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. _The International journal of robotics research_**37** 421-436.
* Li et al. (2022)Li, G., Shi, L., Chen, Y., Chi, Y. and Wei, Y. (2022). Settling the sample complexity of model-based offline reinforcement learning. _arXiv preprint arXiv:2204.05275_.
* Liu et al. (2020)Liu, Y., Swaminathan, A., Agarwal, A. and Brunskill, E. (2020). Provably good batch off-policy reinforcement learning without great exploration. _Advances in neural information processing systems_**33** 1264-1274.
* Min et al. (2021)Min, Y., Wang, T., Zhou, D. and Gu, Q. (2021). Variance-aware off-policy evaluation with linear function approximation. _Advances in neural information processing systems_**34** 7598-7610.
* Modi et al. (2020)Modi, A., Jiang, N., Tewari, A. and Singh, S. (2020). Sample complexity of reinforcement learning using linearly combined model ensembles. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Nguyen-Tang and Arora (2023)Nguyen-Tang, T. and Arora, R. (2023). Viper: Provably efficient algorithm for offline rl with neural function approximation. In _The Eleventh International Conference on Learning Representations_.
* Rashidinejad et al. (2021)Rashidinejad, P., Zhu, B., Ma, C., Jiao, J. and Russell, S. (2021). Bridging offline reinforcement learning and imitation learning: A tale of pessimism. _Advances in Neural Information Processing Systems_**34** 11702-11716.
* Schrittwieser et al. (2020)Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T. et al. (2020). Mastering atari, go, chess and shogi by planning with a learned model. _Nature_**588** 604-609.
* Shi et al. (2022)Shi, L., Li, G., Wei, Y., Chen, Y. and Chi, Y. (2022). Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity. In _International Conference on Machine Learning_. PMLR.
* Silver et al. (2017)Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A. et al. (2017). Mastering the game of go without human knowledge. _nature_**550** 354-359.
* Sun et al. (2019)Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A. and Langford, J. (2019). Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches. In _Conference on learning theory_. PMLR.
* Uehara and Sun (2021)Uehara, M. and Sun, W. (2021). Pessimistic model-based offline reinforcement learning under partial coverage. _arXiv preprint arXiv:2107.06226_.
* Wang et al. (2020a)Wang, R., Foster, D. P. and Kakade, S. M. (2020a). What are the statistical limits of offline rl with linear function approximation? _arXiv preprint arXiv:2010.11895_.
* Wang et al. (2020b)Wang, R., Salakhutdinov, R. R. and Yang, L. (2020b). Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. _Advances in Neural Information Processing Systems_**33** 6123-6135.
* Wang et al. (2020c)Wang, Y., Wang, R., Du, S. S. and Krishnamurthy, A. (2020c). Optimism in reinforcement learning with generalized linear function approximation. In _International Conference on Learning Representations_.
* Weisz et al. (2021)Weisz, G., Amortila, P. and Szepesvari, C. (2021). Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions. In _Algorithmic Learning Theory_. PMLR.
* Xie et al. (2021a)Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P. and Agarwal, A. (2021a). Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_**34** 6683-6694.

Xie, T., Jiang, N., Wang, H., Xiong, C. and Bai, Y. (2021b). Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. _Advances in neural information processing systems_**34** 27395-27407.
* Xiong et al. (2022)Xiong, W., Zhong, H., Shi, C., Shen, C., Wang, L. and Zhang, T. (2022). Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent mdp and markov game. _arXiv preprint arXiv:2205.15512_.
* Yang and Wang (2019)Yang, L. and Wang, M. (2019). Sample-optimal parametric q-learning using linearly additive features. In _International Conference on Machine Learning_.
* Yang and Wang (2020)Yang, L. and Wang, M. (2020). Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. In _International Conference on Machine Learning_. PMLR.
* Yin et al. (2022a)Yin, M., Duan, Y., Wang, M. and Wang, Y.-X. (2022a). Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism. _arXiv preprint arXiv:2203.05804_.
* Yin et al. (2022b)Yin, M., Wang, M. and Wang, Y.-X. (2022b). Offline reinforcement learning with differentiable function approximation is provably efficient. _arXiv preprint arXiv:2210.00750_.
* Yin and Wang (2021)Yin, M. and Wang, Y.-X. (2021). Towards instance-optimal offline reinforcement learning with pessimism. _Advances in neural information processing systems_**34** 4065-4078.
* Zanette et al. (2020a)Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M. and Lazaric, A. (2020a). Frequentist regret bounds for randomized least-squares value iteration. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Zanette et al. (2020b)Zanette, A., Lazaric, A., Kochenderfer, M. and Brunskill, E. (2020b). Learning near optimal policies with low inherent bellman error. In _International Conference on Machine Learning_. PMLR.
* Zanette et al. (2021)Zanette, A., Wainwright, M. J. and Brunskill, E. (2021). Provable benefits of actor-critic methods for offline reinforcement learning. _Advances in neural information processing systems_**34** 13626-13640.
* Zhan et al. (2022)Zhan, W., Huang, B., Huang, A., Jiang, N. and Lee, J. (2022). Offline reinforcement learning with realizability and single-policy concentrability. In _Conference on Learning Theory_. PMLR.
* Zhang et al. (2021)Zhang, Z., Ji, X. and Du, S. (2021). Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon. In _Conference on Learning Theory_. PMLR.
* Zhou et al. (2021)Zhou, D., Gu, Q. and Szepesvari, C. (2021). Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_. PMLR.