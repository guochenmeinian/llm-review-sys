# Loss landscape Characterization of

Neural Networks without Over-Parametrization

Rustem Islamov\({}^{1}\)  Niccolo Ajroldi\({}^{2}\)  Antonio Orvieto\({}^{2,3,4}\)  Aurelien Lucchi\({}^{1}\)

\({}^{1}\)University of Basel \({}^{2}\)Max Planck Institute for Intelligent Systems \({}^{3}\) ELLIS Institute Tubingen

\({}^{4}\)Tubingen AI Center

###### Abstract

Optimization methods play a crucial role in modern machine learning, powering the remarkable empirical achievements of deep learning models. These successes are even more remarkable given the complex non-convex nature of the loss landscape of these models. Yet, ensuring the convergence of optimization methods requires specific structural conditions on the objective function that are rarely satisfied in practice. One prominent example is the widely recognized Polyak-Lojasiewicz (PL) inequality, which has gained considerable attention in recent years. However, validating such assumptions for deep neural networks entails substantial and often impractical levels of over-parametrization. In order to address this limitation, we propose a novel class of functions that can characterize the loss landscape of modern deep models without requiring extensive over-parametrization and can also include saddle points. Crucially, we prove that gradient-based optimizers possess theoretical guarantees of convergence under this assumption. Finally, we validate the soundness of our new function class through both theoretical analysis and empirical experimentation across a diverse range of deep learning models.

## 1 Introduction

The strides in empirical progress achieved by deep neural networks over the past decade have been truly remarkable. Central to the triumph of these techniques lies the effectiveness of optimization methods, which is particularly noteworthy given the non-convex nature of the objective functions under consideration. Worst-case theoretical results point to a pessimistic view since even a degree four polynomial can be NP-hard to optimize [30] and the loss landscape of some neural networks are known to include saddle points or bad local minima [6, 70, 88].

Yet, empirical evidence has shown that gradient-based optimizers - including SGD, AdaGrad[21] and Adam[39] among many others - can effectively optimize the loss of modern deep-learning-based models. While some have pointed to the ability of gradient-based optimizers to deal with potentially complex landscapes, e.g. escaping saddle points [36, 17], another potential explanation is that the loss landscape itself is less complex than previously assumed [27, 51].

Some key factors in this success include the choice of architecture [5, 48, 40, 18], as well as the over-parametrization [73, 12, 50, 51]. In the well-known infinite-width limit [86, 35, 32], neural networks are known to exhibit simple landscapes [49]. However, practical networks operate in a finite range, which still leaves a lot of uncertainty regarding the nature of the loss landscape. This is especially important given that the convergence guarantees of gradient-based optimizers are derived by assuming some specific structure on the objective function [37, 79, 71]. Consequently, an essential theoretical endeavor involves examining the class of functions that neural networks can represent.

In this work, we present a new class of functions that satisfy a newly proposed \(\alpha\)-\(\beta\)-condition (see Eq. (2)). We theoretically and empirically demonstrate that these functions effectively characterizethe loss landscape of neural networks. Furthermore, we derive theoretical convergence guarantees for commonly used gradient-based optimizers under the \(\alpha\)-\(\beta\)-condition.

In summary, we make the following contributions:

1. We introduce the \(\alpha\)-\(\beta\)-condition and theoretically demonstrate its applicability to a wide range of complex functions, notably those that include local saddle points and local minima.
2. We empirically validate that the \(\alpha\)-\(\beta\)-condition is a meaningful assumption that captures a wide range of practical functions, including matrix factorization and neural networks (ResNet, LSTM, GNN, Transformer, and other architectures).
3. We analyze the theoretical convergence of several optimizers under \(\alpha\)-\(\beta\)-condition, including vanilla SGD (Stochastic Gradient Descent), \(\mathsf{SPS}_{\max}\) (Stochastic Polyak Stepsize) [53], and NGN[63] (Non-negative Gauss-Newton).
4. We provide empirical and theoretical counter-examples where the weakest assumptions, such as the PL and Aiming conditions, do not hold, but the \(\alpha\)-\(\beta\)-condition does.

## 2 Related work

### Function classes in optimization

Studying the convergence properties of gradient-based optimizers has a long history in the field of optimization and machine learning. Notably, one of the fundamental observations is the linear and sub-linear convergence exhibited by GD for _strongly convex_ (SCvx) and general _convex_ (Cvx) functions [61]. However, most modern Machine Learning models have non-convex loss landscapes, for which the existing convex theory is not applicable. Without assumptions on the loss functions (other than smoothness), one can only obtain weak convergence guarantees to a first-order critical point. This situation has led to the derivation of assumptions that are weaker than convexity but that are sufficient to guarantee convergence of GD-based optimizers. The list includes _error bounds_ (EB) [54], _essential strong convexity_ (ESC) [52], weak strong convexity (WSC) [60], the restricted secant inequality (RSI) [87], and the quadratic growth (QG) condition [1]. In the neighborhood of the minimizer set \(\mathcal{S}\), EB, PL, and QG are equivalent if the objective is twice differentiable [68]. All of them, except QG, are sufficient to guarantee a global linear convergence of GD. However, among these less stringent conditions, the Polyak-Lojasiewicz (PL) condition stands out as particularly renowned. Initially demonstrated by Polyak [66] to ensure linear convergence, it has recently experienced a resurgence of interest, in part because it accurately characterizes the loss landscape of heavilly over-parametrized neural networks [50]. It was also shown to be one of the weakest assumptions among the other known conditions outlined so far [37]. A generalized form of the PL condition for non-smooth optimization is the Kurdyka-Lojasiewicz (KL) condition [43; 10] which is satisfied for a much larger class of functions [14; 77] than PL. The KL inequality has been employed to analyze the convergence of the classic proximal-point algorithm [3; 11; 47] and other optimization methods [4; 45].

More recently, some new convex-like conditions have appeared in the literature such as _star-convex_ (StarCvx) [62], _quasar-convex_ (QCvx) [28], and _Aiming_[51]. These conditions are relaxations of

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Condition** & **Definition** & **Comments** \\ \hline QCvx [28] & \(\langle\nabla f(x),x-x^{\ast}\rangle\geq\theta(f(x)-f(x^{\ast}))\) & - excludes saddle points and local minima \\  & for some fixed \(x^{\ast}\in\mathcal{S}\) & - includes saddle points and local minima \\ \hline Aiming [51] & \(\langle\nabla f(x),x-\operatorname{Proj}(x,\mathcal{S})\rangle\geq\theta f(x)\) & - theoretically holds for NN in the presence of \\  & & - does not always hold in practice [Fig. 1 a-b] \\ \hline PL (\({}^{\alpha}\)) [66] & \(\|\nabla f(x)\|^{2}\geq 2\mu(f(x)-f^{\ast})\) & - excludes saddle points and local minima \\  & & - theoretically holds for NN in the presence of \\  & & - impractical over-parameterization [50] \\ \hline \(\alpha\)-\(\beta\)-condition & \(\langle\nabla f_{i}(x),x-\operatorname{Proj}(x,\mathcal{S})\rangle\geq\alpha(f _{i}(x)-f_{i}(\operatorname{Proj}(x,\mathcal{S})))\) & - might have saddles [Ex. 2] and local minima [Ex. 3] - in practice does not require \\
**[This work]** & \(-\beta(f_{i}(x)-f_{i}^{\ast})\) & over-parameterization [Ex. 5] \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of existing assumptions on the problem (1) and their limitations. Here \(\mathcal{S}\) denotes the set of minimizers of \(f\) and \(f_{i}^{\ast}\coloneqq\operatorname*{argmin}_{x}f_{i}(x)\). Unlike earlier conditions, the \(\alpha\)-\(\beta\)-condition is specifically designed to capture local minima and saddle points. NN = Neural Network.

convexity and include non-convex functions. Within the domain of reinforcement learning, several works [84, 23] have also considered relaxations of the gradient domination condition, although these analyses are conducted specifically within the context of policy gradient methods, and therefore less relatable to StarCvx, QCVx or the Aiming condition.

We present a summary of some of these conditions in Table 1. There is no general implication between already existing assumptions such as QCVx, Aiming, PL, and the \(\alpha\)-\(\beta\)-condition. However, as we will later see, the \(\alpha\)-\(\beta\)-condition can more generally characterize the landscape of neural networks without requiring unpractical amounts of over-parametrization. Notably, the \(\alpha\)-\(\beta\)-condition is a condition that applies globally to the loss. However, we will demonstrate that convergence guarantees can still be established for commonly-used gradient-based optimizers, although these guarantees are weaker than those derived under the PL condition, which relies on much stronger assumptions.

### Limitations of existing conditions

Next, we discuss the limitations of previous conditions to characterize the loss landscape of complex objective functions such as the ones encountered when training deep neural networks.

Necessity of Over-parameterization.When considering deep models, the theoretical justification of conditions such as Aiming [51] and PL [50] require a significant amount of over-parameterization. This implies that the neural network must be considerably large, often with the minimum layer's width scaling with the size of the dataset \(n\). However, various studies suggest that this setup may not always accurately model real-world training dynamics [13, 2]. To the best of our knowledge, the weakest requirements on the width of a network are sub-quadratic \(\widetilde{\Omega}(n^{3/2})\)[20, 74], where \(n\) is the size of a dataset. This implies that, even for a small dataset such as MNIST, a network should have billions of parameters which is not a realistic setting in practice. In contrast, the \(\alpha\)-\(\beta\)-condition condition does not require such an over-parametrization condition to hold (e.g., see Example 5). In Section 5 we provide empirical results showing how our condition is affected by over-parameterization.

Necessity of Invexity.One limitation of prior assumptions is their inability to account for functions containing local minima or saddle points. Indeed, many of the weakest conditions, such as QCVx, PL, KL, and Aiming, require that any point where the gradient is zero must be deemed a global minimizer. However, such conditions are not consistent with the loss landscapes observed in practical neural networks. For example, finite-size MLPs can have spurious local points or saddle points [88, 80]. Another known example is the half-space learning problem which is known to have saddle points [17]. We refer the reader to Figure 2-a that illustrates this claim (it showcases the surface of the problem

Figure 1: Training of \(3\) layer LSTM model that shows Aiming condition does not always hold. The term “Angle” in the figures refers to the angle \(\angle(\nabla f(x^{k}),x^{k}-x^{K})\), and it should be positive if Aiming holds, while in a-b we observe that it is negative during the first part of the training. Figures c-d demonstrate that possible constant \(\mu\) in PL condition should be small which makes theoretical convergence slow.

Figure 2: Training for half-space learning problem with SGD. The term “Angle” in the figures refers to the angle \(\angle(\nabla f(x^{k}),x^{k}-x^{K})\).

fixing all parameters except first \(2\)), and also demonstrates that the Aiming and PL conditions fail to hold in such a setting. We present the results in Figure 21 where we observe that \((i)\) the angle between the full gradient and direction to the minimizer \(\angle(\nabla f(x^{k}),x^{k}-x^{*})\) can be negative implying that the Aiming condition does not hold in this case (since the angle should remain positive); \((ii)\) the gradient norm can become zero while we did not reach minimum (loss is still large) implying that the PL condition does not hold as well (since the inequality \(\|\nabla f(x^{k})\|^{2}\geq 2\mu(f(x^{k})-f^{*})\) is not true for any positive \(\mu\)). These observations suggest that the Aiming and PL conditions do not characterize well a landscape in the absence of invexity property. In contrast, we demonstrate in Example 2 and Figure 9 that our proposed assumption is preferable in this scenario.

Footnote 1: Our implementation is based on the open source repository [https://github.com/archana95in/Escaping_saddles_using-Stochastic-Gradient-Descent](https://github.com/archana95in/Escaping_saddles_using-Stochastic-Gradient-Descent) from [17] with small changes to track necessary quantities.

Lack of Theory.As previously mentioned, most theoretical works apply to some infinite limit or neural networks of impractical sizes. In contrast, several works [89, 27, 75] have studied the empirical properties of the loss landscape of neural networks during training. They have shown that gradient-based optimization methods do not encounter significant obstacles that impede their progress. However, these studies fall short of providing theoretical explanations for this observed phenomenon.

Lack of Empirical Evidence.Several theoretical works [49, 51] prove results on the loss landscape of neural networks without supporting their claims using experimental validation on deep learning benchmarks. We demonstrate some practical counter-examples to these conditions proved in prior work. We train LSTM-based model2 with standard initialization on Wikitext-2 [57] and Penn Treebank (PTB) [58] datasets. In Figure 1 (a-b), we show that the angle between the full gradient and direction to the minimizer \(\angle(\nabla f(x^{k}),x^{k}-x^{*})\) can be negative in the first part of the training. This result implies that the Aiming condition does not hold in this setting (either we do not have enough over-parameterization or the initialization does not lie in the locality region where Aiming holds). Moreover, for the same setting in Figure 1 (c-d) we plot \(2\log(\|\nabla f(x^{k})\|)-\nicefrac{{2}}{{8}}\log(f(x^{k})-f(x^{K}))\)3 to measure the empirical value of PL constant \(\log(2\mu)\) (see derivations in Appendix A). We observe that the value of \(\mu\) that might satisfy PL condition should be of order \(10^{-8}-10^{-7}\) and leads to slow theoretical convergence [37]. These observations contradict with practical results. We defer to Appendix A for a more detailed discussion. In contrast, we demonstrate in Figure 7-(g-h) that the proposed \(\alpha\)-\(\beta\)-condition can be verified in this setting.

Footnote 2: Our implementation is based on the open source repository [https://github.com/fhueb/parameter-agnostic-lzlo](https://github.com/fhueb/parameter-agnostic-lzlo) from [33] with small changes to track necessary quantities. The detailed experiment description is given in Appendix D.2.

Footnote 3: We use \(x^{K}\) (for a large value of \(K\)) to approximate the minimizer \(x^{*}\).

## 3 The proposed \(\alpha\)-\(\beta\)-condition

Setting.We consider the following Empirical Risk Minimization (ERM) problem that typically appears when training machine learning models:

\[\min_{x\in\mathbb{R}^{d}}\left[f(x)\coloneqq\tfrac{1}{n}\sum_{i=1}^{n}f_{i}(x )\right]. \tag{1}\]

Here \(x\in\mathbb{R}^{d}\) denotes the parameters of a model we aim to train, \(d\) is the number of parameters, and \(n\) is the number of samples in the training dataset. Each \(f_{i}(x)\) is the loss associated with the \(i\)-th data point. We denote the minimum of the problem (1) by \(f^{*}\) and the minimum of each individual function by \(f^{*}_{i}\coloneqq\min_{x}f_{i}(x)\), which we assume to be finite. Besides, the set \(\mathcal{S}\) denotes the set of all minimizers of \(f\).

A new class of functions.Next, we present a new condition that characterizes the interplay between individual losses \(f_{i}\) and the set of minimizers of the global loss function \(f\).

**Definition 1** (\(\alpha\)-\(\beta\)-condition ).: Let \(\mathcal{X}\subseteq\mathbb{R}^{d}\) be a set and consider a function \(f:\mathcal{X}\rightarrow\mathbb{R}\) as defined in (1). Then \(f\) satisfies the \(\alpha\)-\(\beta\)-condition with positive parameters \(\alpha\) and \(\beta\) such that \(\alpha>\beta\) if for any \(x\in\mathcal{X}\) there exists \(x_{p}\in\mathrm{Proj}(x,\mathcal{S})\) such that for all \(i\),

\[\langle\nabla f_{i}(x),x-x_{p}\rangle\geq\alpha(f_{i}(x)-f_{i}(x_{p}))-\beta(f_{i }(x)-f_{i}^{*}). \tag{2}\]

The \(\alpha\)-\(\beta\)-condition recovers several existing assumptions as special cases. For example, the proposed assumption reduces to QCvx around \(x^{*}\) if \(\alpha>0\), \(\beta=0\), and \(\mathcal{S}\) is a singleton \(\{x^{*}\}\). Importantly, the \(\alpha\)-\(\beta\)-condition is also applicable when the set \(\mathcal{S}\) contains multiple elements.

One can easily check that the pair of parameters \((\alpha,\beta)\) in (2) can not be unique. Indeed, if the assumption is satisfied for some \(\alpha\) and \(\beta\), then due to inequality \(f_{i}(x_{p})\geq f_{i}^{*}\) it will be also satisfied for \((\alpha+\delta,\beta+\delta)\) for any \(\delta\geq 0\).

### Theoretical verification of the \(\alpha\)-\(\beta\)-condition

To demonstrate the significance of Definition 1 as a meaningful condition for describing structural non-convex functions, we provide several examples below that satisfy (2). We do not aim to provide the tightest possible choice of \(\alpha\) and \(\beta\) such that Definition 1 holds. Instead, this section aims to offer a variety of examples that demonstrate specific desired characteristics when \(\alpha\)-\(\beta\)-condition holds, encompassing a broad range of functions.

The initial example illustrates that \(\mathcal{S}\) could potentially be infinite for the class of functions satisfying Definition 1.

**Example 1**.: Let \(f,f_{1},f_{2}\colon\mathbb{R}^{2}\to\mathbb{R}\) be such that

\[f=\tfrac{1}{2}(f_{1}+f_{2})\quad\text{with}\quad f_{1}(x,y)=\tfrac{(x+y)^{2}}{ (x+y)^{2}+1},\quad f_{2}(x,y)=\tfrac{(x+y+1)^{2}}{(x+y+1)^{2}+1}, \tag{3}\]

then Definition 1 holds with \(\alpha\in[\nicefrac{{\alpha}}{{2}},+\infty)\) and \(\beta\in[\nicefrac{{4\alpha}}{{5}},\alpha)\).

Next, we provide an example where \(f\) satisfies Definition 1 even in the presence of saddle points.

**Example 2**.: Let \(f,f_{1},f_{2}\colon\mathbb{R}^{2}\to\mathbb{R}\) be such that

\[f=\tfrac{1}{2}(f_{1}+f_{2})\quad\text{with}\quad f_{1}(x,y)=1-e^{-x^{2}-y^{2} },\quad f_{2}(x,y)=1-e^{-(x-2)^{2}-(y-2)^{2}}, \tag{4}\]

then Definition 1 holds for some \(\alpha\) and \(\beta=\alpha-8\).

**Remark 1**.: Examples 1 and 2 can be generalized for any number of functions \(n\) and dimension \(d\) as follows

\[f_{i}(x)=\tfrac{\left(\sum_{j=1}^{d}x_{j}+a_{i}\right)^{2}}{\left(\sum_{j=1}^ {d}x_{j}+a_{i}\right)^{2}+1},\quad f_{i}(x)=\tfrac{\sum_{j=1}^{d}(x_{j}-b_{ij })^{2}}{1+\sum_{j=1}^{d}(x_{j}-b_{ij})^{2}}, \tag{5}\]

for some properly chosen \(\{a_{i}\}\) and \(\{b_{ij}\},i\in[n],j\in[d]\).

**Example 3**.: Let \(f,f_{1},f_{2}\colon\mathbb{R}^{2}\to\mathbb{R}\) be such that

\[f=\tfrac{1}{2}(f_{1}+f_{2})\quad\text{with}\quad f_{1}(x,y)=\tfrac{1+x^{2}+y^ {2}}{4+x^{2}+y^{2}},\quad f_{2}(x,y)=\tfrac{(x-2.5)^{2}+(y-2.5)^{2}}{4+(y-2.5) ^{2}+(y-2.5)^{2}}, \tag{6}\]

then Definition 1 holds for some \(\alpha\) and \(\beta=\alpha-1\).

The three examples above demonstrate that functions satisfying Definition 1 can potentially be non-convex with an unbounded set of minimizers \(\mathcal{S}\) (Example 1) and can have saddle points (Example 2) and local minima (Example 3). In contrast, the PL and Aiming conditions are not met in cases where a problem exhibits saddle points. For illustration purposes, we plot the loss landscapes of \(f\) in Figure 3.

So far, we have presented simple examples to verify Definition 1. Next, we turn our attention to more practical examples in the field of machine learning. We start with the matrix factorization problem that is known to have saddle points [76] but can be shown to be PL after a sufficiently large number of iterations of alternating gradient descent and under a specific random initialization [81].

**Example 4**.: Let \(f_{i},f_{ij}\) be such that

\[f(W,S)=\tfrac{1}{2nm}\|X-W^{\top}S\|_{\mathrm{F}}^{2}=\tfrac{1}{2nm}\sum_{i,j}( X_{ij}-w_{i}^{\top}s_{j})^{2},\quad f_{ij}(W,S)=\tfrac{1}{2}(X_{ij}-w_{i}^{\top}s_{j} )^{2}, \tag{7}\]

where \(X\in\mathbb{R}^{n\times m},W=(w_{i})_{i=1}^{n}\in\mathbb{R}^{k\times n},S=(s_{ j})_{j=1}^{m}\in\mathbb{R}^{k\times m},\) and \(\mathrm{rank}(X)=r\geq k\). We assume that \(X\) is generated using matrices \(W^{*}\) and \(S^{*}\) with non-zero additive noise that minimize empirical loss, namely, \(X=(W^{*})^{\top}S^{*}+(\varepsilon_{ij})_{i\in[n],j\in[m]}\quad\text{where} \quad W^{*},S^{*}=\operatorname{argmin}_{W,S}f(W,S)\). Let \(\mathcal{X}\) be any bounded set that contains \(\mathcal{S}\). Then Definition 1 is satisfied with \(\alpha=\beta+1\) and some \(\beta>0\).

**Example 5**.: Consider training a two-layer neural network with a logistic loss

\[f=\tfrac{1}{n}\sum_{i=1}^{n}f_{i},\quad f_{i}(W,v)=\phi(y_{i}\cdot v^{\top}\sigma (Wx_{i}))+\lambda_{1}\|v\|^{2}+\lambda_{2}\|W\|_{\mathbb{F}}^{2} \tag{8}\]

for a classification problem where \(\phi(t)\coloneqq\log(1+\exp(-t)),\,W\in\mathbb{R}^{k\times d},v\in\mathbb{R}^{ k},\,\sigma\) is a ReLU function applied coordinate-wise, \(y_{i}\in\{-1,+1\}\) is a label and \(x_{i}\in\mathbb{R}^{d}\) is a feature vector. Let \(\mathcal{X}\) be any bounded set that contains \(\mathcal{S}\). Then the \(\alpha\)-\(\beta\)-condition holds in \(\mathcal{X}\) for some \(\alpha\geq 1\) and \(\beta=\alpha-1\).

**Remark 2**.: The previous examples can be extended to any positive and convex function \(\phi\) (e.g., square loss) with the additional assumption that each individual loss \(f_{i}\) does not have minimizers in \(\mathcal{S}\), i.e. \(\nexists(W^{*},v^{*})\in\mathcal{S}\) such that \(f_{i}(W^{*},v^{*})=f_{i}^{*}\) for some \(i\in[n]\).

We highlight that Example 5 is applicable for a bounded set \(\mathcal{X}\) of an _arbitrary_ size. Moreover, in practice, we typically add \(\ell_{1}\) or \(\ell_{2}\) regularization which can be equivalently written as a constrained optimization problem, and therefore, Example 5 holds in this scenario. In comparison, the results from previous works do not hold for an arbitrary bounded set around \(\mathcal{S}\) requiring initialization to be close enough to the solution set [51, 50]. The proofs for all examples are given in Appendix C.1.

## 4 Theoretical convergence of algorithms

We conduct our analysis under the following smoothness assumption that is standard in the optimization literature.

**Assumption 1**.: _We assume that each \(f_{i}\) is \(L\)-smooth, i.e. for all \(x,y\in\mathbb{R}^{d}\) it holds \(\|\nabla f_{i}(x)-\nabla f_{i}(y)\|\leq L\|x-y\|\)._

The next assumption, which is sometimes called functional dissimilarity [56], is standard in the analysis of SGD with adaptive stepsizes [53, 26, 24].

**Assumption 2**.: _We assume that the interpolation error \(\sigma_{\mathrm{int}}^{2}\coloneqq\mathbb{E}_{i}[f^{*}-f_{i}^{*}]\) is finite, where the expectation is taken concerning the randomness of indices \(i\) for a certain algorithm._

### Convergence under the \(\alpha\)-\(\beta\)-condition

Now we demonstrate that the \(\alpha\)-\(\beta\)-condition is sufficient for common optimizers to converge up to a neighbourhood of the set of minimizers \(\mathcal{S}\). We provide convergence guarantees for SGD-based algorithms with fixed and adaptive stepsize (i.e., the update direction is of the form \(-\gamma_{k}\nabla f_{ik}(x^{k})\)). In this section, we only present the main statements about convergence while the algorithms' description and the proofs are deferred to Appendix C.2.

Convergence of SGD.We start with the results for vanilla SGD with constant stepsize.

**Theorem 1**.: _Assume that Assumptions 1-2 hold. Then the iterates of SGD (Alg. 1) with stepsize \(\gamma\leq\frac{\alpha-\beta}{2L}\) satisfy_

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right]\leq\frac{\mathbb{E}\left[ \operatorname{dist}(x^{0},\mathcal{S})^{2}\right]}{K}\frac{1}{\gamma(\alpha- \beta)}+\frac{2L\gamma}{\alpha-\beta}\sigma_{\mathrm{int}}^{2}+\frac{2\beta}{ \alpha-\beta}\sigma_{\mathrm{int}}^{2}. \tag{9}\]

Figure 3: Loss landscape of \(f\) that satisfy Definition 1. The analytical form of \(f_{i}\) is given in Section 3.1. These examples demonstrate that the problem (1) that satisfies \(\alpha\)-\(\beta\)-condition might have an unbounded set of minimizers \(\mathcal{S}\) (Example 1), a saddle point (Example 2), and local minima (Example 3) in contrast to the PL and Aiming conditions. The contour plots are presented in 19.

Theorem 1 shows that under the \(\alpha\)-\(\beta\)-condition, SGD converges with a rate \(\mathcal{O}(K^{-1/2})\) (the same rate obtained by SGD for convex functions [24]) up to a ball of size \(\mathcal{O}(\beta\sigma_{\mathrm{int}}^{2})\). We argue that the non-vanishing term \(\mathcal{O}(\beta\sigma_{\mathrm{int}}^{2})\) must appear in the convergence rate for non-convex optimization for several reasons: \((i)\) This term arises directly from the use of the \(\alpha\)-\(\beta\) condition in the analysis, without resorting to additional upper bounds or approximations. It reflects the potential existence of local minima that the \(\alpha\)-\(\beta\) condition is designed to model. In the worst-case scenario, if SGD is initialized near local minima and uses a sufficiently small step size, it may fail to converge to the exact minimizer and can become trapped in suboptimal minima. This sub-optimality is modeled in the upper bound by the stepsize-independent quantity \(\mathcal{O}(\beta\sigma_{\mathrm{int}}^{2})\) since we provide convergence guarantees for the function value sub-optimality rather than the squared gradient norm, which is more typical in the non-convex setting. \((ii)\) We also observe that the last term in (9) shrinks as a model becomes more over-parameterized (which is consistent with prior works such as [50] that require large amounts of over-parametrization); see Sections 5.1, 5.2, and D.7 for experimental validation of this claim. Further empirical observations are summarized in Table 2 and will be discussed in Section 5. Theoretically, if a model is sufficiently over-parameterized such that the interpolation condition \(f_{i}^{*}=f^{*}\) holds, then the non-vanishing term is not present in the bound. \((iii)\) The presence of a non-vanishing error term in the rate with the \(\alpha\)-\(\beta\) condition is consistent with empirical observation as it is frequently observed when training neural networks (see Figure \(8.1\) in [25]). This is also observed during the training of language models where the loss is significantly larger than \(0\) (see Figure 19). This phenomenon suggests that reaching a critical point, which is a global minimizer, is not commonly observed practically. \((iv)\) Finally, we note a potential similarity with prior works that propose other conditions to describe the loss landscape of deep neural networks (e.g. gradient confusion [71]), and also obtain a non-vanishing term in the convergence rate (see Theorem \(3.2\) in [71]).

Convergence of \(\mathsf{SPS}_{\max}\).Next, we consider the \(\mathsf{SPS}_{\max}\) algorithm. \(\mathsf{SPS}_{\max}\) stepsize is given by \(\gamma_{k}\coloneqq\min\left\{\frac{f_{i_{k}}(x^{k})-f_{i_{k}}^{*}}{c\|\nabla f _{i_{k}}(x^{k})\|^{2}},\gamma_{b}\right\}\) where \(c\) and \(\gamma_{b}\) are the stepsize hyperparameters.

**Theorem 2**.: _Assume that Assumptions 1-2 hold. Then the iterates of \(\mathsf{SPS}_{\max}\) (Alg. 2) with a stepsize hyperparameter \(c>\frac{1}{2(\alpha-\beta)}\) satisfy_

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right]\leq\frac{c_{1}}{K} \mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right]+2\alpha c_{1} \gamma_{b}\sigma_{\mathrm{int}}^{2}, \tag{10}\]

_where \(\gamma_{\min}\coloneqq\min\{\nicefrac{{1}}{{2cL}},\gamma_{b}\}\) and \(c_{1}\coloneqq\frac{c}{\gamma_{\min}(2(\alpha-\beta)e-1)}\)._

In the convex case, i.e. \(\alpha\)-\(\beta\)-condition holds with \(\alpha=1,\beta=0\), we recover the rate of Loizou et al. [53].

Convergence of Ngn.Finally, we turn to the analysis of NGN. This algorithm is proposed for minimizing positive functions \(f_{i}\) which is typically the case for many practical choices. Its stepsize \(\gamma_{k}\coloneqq\frac{\gamma}{1+\frac{\gamma}{2f_{i_{k}}^{*}\|\nabla f_{ i_{k}}(x^{k})\|^{2}}}\) where \(\gamma\) is the stepsize hyperparameter. NGN stepsize differs from that of \(\mathsf{SPS}_{\max}\) by replacing \(\min\) operator by softer harmonic averaging of \(\mathsf{SPS}\) stepsize and a constant \(\gamma\). In addition to the already mentioned assumptions, we make a mild assumption that the positivity error \(\sigma_{\mathrm{pos}}^{2}\coloneqq\mathbb{E}\left[f_{i}^{*}\right]\) is finite.

**Theorem 3**.: _Assume that Assumptions 1 with \(\alpha\geq\beta+1\) and 1-2 hold. Assume that each function \(f_{i}\) is positive and \(\sigma_{\mathrm{pos}}^{2}<\infty\). Then the iterates of NGN (Alg. 3) with a stepsize parameter \(\gamma>0\) satisfy_

\[\min_{0\leq k\leq K-1}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{\mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right] \frac{(1+2\gamma L)^{2}}{2\gamma K}+\frac{3L\gamma\alpha(1+\gamma L)\sigma_{ \mathrm{int}}^{2}}{c_{2}}}{+\ \ \frac{\gamma L}{a}\max\left\{2\gamma L-1,0\right\}\sigma_{ \mathrm{pos}}^{2}+\frac{2\beta\sigma_{\mathrm{int}}^{2}}{c_{2}}}, \tag{11}\]

_where \(c_{2}\coloneqq 2\gamma L(\alpha-\beta-1)+\alpha-\beta\)._

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & **Model’s width**\(\nearrow\) & **Model’s depth**\(\nearrow\) & **Batch-size**\(\nearrow\) \\ \hline Change in \(\beta\sigma_{\mathrm{int}}^{2}\) & \(\searrow\) & \(\searrow\) & \(\searrow\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Summary of how the non-vanishing term \(\beta\sigma_{\mathrm{int}}^{2}\) (as appearing e.g. in Eq. (9)) increases (\(\nearrow\)) or decreases (\(\searrow\)) as a function of specific quantities of interest.

One of the main properties of NGN is its robustness to the choice of stepsize \(\gamma\). Theorem 3 can be seen as an extension of this feature from the set of convex functions originally analyzed in [63] to the class of structured non-convex satisfying \(\alpha\)-\(\beta\)-condition.

Comparing the results of Theorems 2 and 3 we highlight several important differences. \((i)\) There is no restriction on the stepsize parameter \(\gamma\) for NGN. Conversely, \(\mathsf{SPS}_{\max}\) requires \(c\) to be lower bounded. \((ii)\) Both algorithms converge to a neighborhood of the solution with a fixed stepsize hyperparameter. However, the neighborhood size of \(\mathsf{SPS}_{\max}\) is not controllable by the stepsize hyperparameter and remains constant even in the convex setting when \(\beta=0\). In contrast, NGN converges to a ball whose size can be made smaller by choosing a small stepsize parameter, and the "non-vanishing" term disappears in the convex setting \(\beta=0\).

We note that our goal was not to achieve the tightest convergence guarantees for each algorithm, but rather to underscore the versatility of the \(\alpha\)-\(\beta\)-condition in deriving convergence guarantees for SGD-type algorithms, both for constant or adaptive stepsizes. In addition to the results of this section, we demonstrate the convergence guarantees for SGD, \(\mathsf{SPS}_{\max}\), and NGN with decreasing with \(k\) stepsizes in Appendix C.2. Besides, in Appendix C.2.4 we present a convergence of a slightly modified version of Adagrad-norm method [82] under \(\alpha\)-\(\beta\)-condition.

## 5 Experimental validation of the \(\alpha\)-\(\beta\)-condition

In this section, we provide extensive numerical results supporting that the \(\alpha\)-\(\beta\)-condition does hold in many practical applications for various tasks, model architectures, and datasets. The detailed experimental setting is described in Appendix D.

In all cases, we approximate \(\mathrm{Proj}(x^{k},\mathcal{S})\) as the last iterate \(x^{K}\) in a run. After finding such an approximation, we start a second training run with the same random seed to measure all necessary quantities. To guarantee that the second training trajectory follows the same path as the first run, we disable non-deterministic CUDA operations while training on a GPU. For each task, we demonstrate possible values of pairs of \((\alpha,\beta)\) that work across all runs (might differ from one experiment to another) with different random seeds and satisfy \(\alpha\geq\beta+0.1\).

### MLP architecture

First, we test MLP neural networks with 3 fully connected layers on Fashion-MNIST [83] dataset. We fix the second layer of the network to be a square matrix and vary its dimension layer to investigate the effect of over-parameterization on \(\alpha\)-\(\beta\)-condition. We test it for dimensions \(\{32,128,2049,4096\}\), and for each case, we run experiments for \(4\) different random seeds. In Figure 4 we demonstrate possible values of pairs of \((\alpha,\beta)\) that work across all \(4\) runs. We observe that minimum possible values of \(\alpha\) and \(\beta\) increase from small size to medium, and then tend to decrease again as the model becomes more over-parameterized. We defer more experimental results for MLP to Appendix D.3 to showcase this phenomenon. This observation leads to the fact that the neighborhood of convergence \(\mathcal{O}(\beta\sigma_{\mathrm{int}}^{2})\) of SGD eventually becomes smaller with the size of the model as we expect (since it becomes more over-parameterized).

### CNN architecture

In our next experiment, we test convolutional neural networks with \(2\) convolution layers and \(1\) fully connected layer on CIFAR10 dataset [41]. We vary the number of convolutions in the second convolution layer to investigate the effect of over-parameterization on \(\alpha\)-\(\beta\)-condition. We test it

Figure 4: \(\alpha\)-\(\beta\)-condition in the training of 3 layer MLP model on Fashion-MNIST dataset varying the size of the second layer. Here \(T(x_{k})=\langle\nabla f_{i_{k}}(x^{k}),x^{k}-x^{K}\rangle-\alpha(f_{i_{k}}(x ^{k})-f_{i_{k}}(x^{K}))-\beta f_{i_{k}}(x^{k})\) assuming that \(f_{i}^{*}=0\). Minimum is taken across all runs and iterations for given pair of \((\alpha,\beta)\).

for \(\{32,128,512,2048\}\) number of convolutions in the second layer, and for each case, we run experiments for \(4\) different random seeds. In Figure 5, we observe that the smallest possible values of \((\alpha,\beta)\) increase till \(64\) convolutions, and then decrease back. Second, the difference \(\alpha-\beta\) for possible choice of \(\alpha\) and \(\beta\) decreases from Figure 5-a to Figure 5-b, but then it increases again.

### Resnet architecture

Next, we switch to the Resnet architecture [29] with batch sizes in \(\{64,128,256,512\}\) trained on CIFAR100 [41]. For each batch size, we run experiments for \(4\) different random seeds. In Figure 6, we plot the possible choice of pairs \((\alpha,\beta)\) that works across all runs. We observe that \(\alpha\)-\(\beta\)-condition holds in all cases. Besides, there is a tendency for the minimum possible choice of \(\alpha\) and \(\beta\) to decrease with batch size. Moreover, for larger batches, the difference between \(\alpha\) and \(\beta\) also increases. From Theorem 1, this result suggests that we can use bigger stepsizes with larger batches.

### Training of AlgoPerf workloads and transformers for language modeling

We are now interested in assessing the validity of \(\alpha\)-\(\beta\)-condition on modern Deep Learning architectures. Thereby, we consider tasks from the AlgoPerf benchmarking suite [16]. We consider four workloads from the competition: (i) DLRMsmall model [59] on Criteo 1TB dataset [44]; (ii) U-Net model [69] on FastMRI dataset [85] (iii) GNN model [7] on OGBG dataset [31]; (iv) Transformer-big [78] on WMT dataset [9]. To train the models, we use NAdamW optimizer [19]4, which achieves state-of-the-art performances on the current version of the benchmark. The hyperparameters of the optimizer are chosen to reach the validation target threshold set by the original competition. Moreover, we consider the pretraining of a decoder-only transformer architecture [78] for causal language modeling. We conduct our evaluation on two publicly available Pythia models [8], of sizes \(70\)M and \(160\)M, trained on 1.25B and 2.5B tokens respectively. For this study, we use the SlimPajama [72] dataset. Following the original Pythia recipes, we fix a sequence length of 2048 and train the language model to predict the next token in a self-supervised fashion. We refer to section Appendices D and D.8 for additional details.

Footnote 4: Our implementation is based on open source AlgoPerf code [https://github.com/mlcommons/algorithmic-efficiency](https://github.com/mlcommons/algorithmic-efficiency) with minimal changes to track necessary quantities.

The results are presented in Figure 7. We observe that \(\alpha\)-\(\beta\)-condition holds for a wide range of values of \(\alpha\) and \(\beta\) which demonstrates that our condition can be seen as a good characterization of the training of modern large models as well. One can notice that the possible values of \(\alpha\) and \(\beta\) for AlgoPerf workloads are higher than those for smaller models discussed in previous sections.

Figure 5: \(\alpha\)-\(\beta\)-condition in the training of CNN model on CIFAR10 dataset varying the number of convolutions in the second layer. Here \(T(x_{k})=\langle\nabla f_{i_{k}}(x^{k}),x^{k}-x^{K}\rangle-\alpha(f_{i_{k}}(x^ {k})-f_{i_{k}}(x^{K}))-\beta f_{i_{k}}(x^{k})\) assuming that \(f_{i}^{*}=0\). Minimum is taken across all runs and iterations for a given pair of \((\alpha,\beta)\).

This difference is attributable to smaller models interpolating the training data more effectively, resulting in significantly lower training losses compared to those observed in AlgoPerf experiments (see Appendix D for detailed results). However, we highlight that the convergence guarantees of the optimizers depend on a term \(\mathcal{O}(\beta\sigma_{\text{int}}^{2})\) which is stable across all experiments we provide.

### Additional experiments

We defer the verification of \(\alpha\)-\(\beta\)-condition by Adam and SGDM to Appendix D.6. The results in Figure 16 suggest that Adam explores the part of the landscape with smaller values of \(\alpha\) and \(\beta\) than those for SGDM. The same conclusions can be drawn when comparing SGDM and SGD. These observations might be of the reasons why diagonal preconditioning and momentum are helpful in the training of DL models. The verification of the proposed condition varying the depth of Resnet model can be found in Appendix D.7. The results from Figure 17 demonstrate that the values of \(\alpha\) and \(\beta\) decrease with the depth of Resnet model, i.e. the model becomes more over-parametrized.

## 6 Conclusion, potential extensions, and limitations.

In this work, we introduce a new class of functions that more accurately characterize loss landscapes of neural networks. In particular, we provide several examples that satisfy the proposed condition, including \(2\)-layer ReLU-neural networks. Additionally, we prove that several optimization algorithms converge under our condition. Finally, we provide extensive empirical verification showing that the proposed \(\alpha\)-\(\beta\)-condition holds along the optimization trajectories of various large deep learning models.

It is also possible to further expand convergence guarantees upon the ones presented in Section 4, for instance, by considering momentum [67] which is widely used in practice. However, we defer the exploration of other possible extensions to future research endeavors.

One of the limitations of this work is the empirical validation of the \(\alpha\)-\(\beta\)-condition on neural networks. We are only able to verify this condition along the trajectories of specific optimizers. Even when performing checks with many random seeds, we cannot fully observe the entire loss landscape. Additionally, while our theoretical examples demonstrate that the proposed condition holds, we do not provide the most precise theoretical values of \(\alpha\) and \(\beta\) that satisfy Definition 1. Therefore, in future work, we aim to obtain stronger theoretical guarantees demonstrating that the \(\alpha\)-\(\beta\)-condition holds for neural networks with more precise values of \(\alpha\) and \(\beta\). We also intend to explore how the \(\alpha\)-\(\beta\)-condition varies when changing the architecture or the number of parameters (i.e., theoretical exploration of over-parameterization).

Figure 7: \(\alpha\)-\(\beta\)-condition in the training of some large models from AlgoPerf, 3-layer LSTM, and Transformers for language modeling. Here \(T(x_{k})=\langle\nabla f_{i_{k}}(x^{k}),x^{k}-x^{K}\rangle-\alpha(f_{i_{k}}(x^ {k})-f_{i_{k}}(x^{K}))-\beta f_{i_{k}}(x^{k})\) assuming that \(f_{i}^{*}=0\). Minimum is taken across all runs and iterations for a given pair of \((\alpha,\beta)\).

## Acknowledgement

Rustem Islamov and Aurelien Lucchi acknowledge the financial support of the Swiss National Foundation, SNF grant No 207392. Antonio Orvieto acknowledges the financial support of the Hector Foundation. The authors thank the anonymous reviewers for their valuable comments and suggestions on improving the paper.

## References

* [1] Mihai Anitescu. Degenerate nonlinear programming with a quadratic growth condition. _SIAM Journal on Optimization_, 2000.
* [2] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. In _Advances in Neural Information Processing Systems_, 2019.
* [3] Hedy Attouch and Jerome Bolte. On the convergence of the proximal algorithm for nonsmooth functions involving analytic features. _Mathematical Programming_, 116:5-16, 2009.
* [4] Hedy Attouch, Jerome Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized gauss-seidel methods. _Mathematical Programming_, 2013.
* [5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. _arXiv preprint arXiv: 1607.06450_, 2016.
* [6] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. _Neural networks_, 1989.
* [7] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks. _arXiv preprint arXiv: 1806.01261_, 2018.
* [8] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In _International Conference on Machine Learning_, 2023.
* [9] Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, et al. Findings of the 2017 conference on machine translation (wmt17). Association for Computational Linguistics, 2017.
* [10] Jerome Bolte, Aris Daniilidis, Olivier Ley, and Laurent Mazet. Characterizations of lojasiewicz inequalities and applications. _arXiv preprint arXiv:0802.0826_, 2008.
* [11] Jerome Bolte, Trong Phong Nguyen, Juan Peypouquet, and Bruce W. Suter. From error bounds to the complexity of first-order descent methods for convex functions. _Mathematical Programming_, 2017.
* [12] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. In _Advances in Neural Information Processing Systems_, 2018.
* [13] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS 2019)_, 2019.

* [14] Michel Coste. _An introduction to o-minimal geometry_. Istituti editoriali e poligrafici internazionali Pisa, 2000.
* [15] Ashok Cutkosky and Harsh Mehta. Momentum improves normalized SGD. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* [16] George E. Dahl, Frank Schneider, Zachary Nado, Naman Agarwal, Chandramouli Shama Sastry, Philipp Hennig, Sourabh Medapati, Runa Eschenhagen, Priya Kasimbeg, Daniel Suo, Juhan Bae, Justin Gilmer, Abel L. Peirson, Bilal Khan, Rohan Anil, Mike Rabbat, Shankar Krishnan, Daniel Snider, Ehsan Amid, Kongtao Chen, Chris J. Maddison, Rakshith Vasudev, Michal Badura, Ankush Garg, and Peter Mattson. Benchmarking neural network training algorithms. _arXiv preprint arXiv: 2306.07179_, 2023.
* [17] Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with stochastic gradients. In _Proceedings of the 35th International Conference on Machine Learning_, 2018.
* [18] Hadi Daneshmand, Amir Joudaki, and Francis Bach. Batch normalization orthogonalizes representations in deep random networks. In _Advances in Neural Information Processing Systems_, 2021.
* [19] Timothy Dozat. Incorporating nesterov momentum into adam. 2016. URL [https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ](https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ).
* [20] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _Proceedings of the 36th International Conference on Machine Learning_, 2019.
* [21] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. _Journal of Machine Learning Research_, 2011.
* [22] Ilyas Fatkhullin, Jalal Etesami, Niao He, and Negar Kiyavash. Sharp analysis of stochastic optimization under global kurdyka-lojasiewicz inequality. In _Advances in Neural Information Processing Systems_, 2022.
* [23] Ilyas Fatkhullin, Anas Barakat, Anastasia Kireeva, and Niao He. Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies. In _International Conference on Machine Learning_, 2023.
* [24] Guillaume Garrigos and Robert Gower. Handbook of convergence theorems for (stochastic) gradient methods. _arXiv preprint arXiv: 2301.11235_, 2024.
* [25] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, 2016.
* [26] Robert Gower, Othmane Sebbouh, and Nicolas Loizou. Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. In _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, 2021.
* [27] Charles Guille-Escuret, Hiroki Naganuma, Kilian Fatras, and Ioannis Mitliagkas. No wrong turns: The simple geometry of neural networks optimization paths. _arXiv preprint arXiv: 2306.11922_, 2023.
* [28] Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. _Journal of Machine Learning Research_, 2018.
* [29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. _arXiv preprint arXiv:1512.03385_, 2015.
* [30] Christopher J Hillar and Lek-Heng Lim. Most tensor problems are np-hard. _Journal of the ACM (JACM)_, 2013.
* [31] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_, 2020.

* Huang et al. [2019] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In _Proceedings of Advances in Neural Information Processing Systems_, 2019.
* Hubler et al. [2024] Florian Hubler, Junchi Yang, Xiang Li, and Niao He. Parameter-agnostic optimization under relaxed smoothness. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, 2024.
* Islamov et al. [2024] Rustem Islamov, Mher Safaryan, and Dan Alistarh. AsGrad: A sharp unified analysis of asynchronous-SGD algorithms. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_, volume 238, pages 649-657. PMLR, 2024.
* Jacot et al. [2018] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 2018.
* Jin et al. [2017] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In _International conference on machine learning_, 2017.
* Karimi et al. [2016] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient methods under the polyak-lojasiewicz condition. _Machine Learning and Knowledge Discovery in Databases_, 2016.
* Khirirat et al. [2023] Sarit Khirirat, Eduard Gorbunov, Samuel Horvath, Rustem Islamov, Fakhri Karray, and Peter Richtarik. Clip21: Error feedback for gradient clipping. _arXiv preprint: arXiv 2305.18929_, 2023.
* Kingma and Ba [2017] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv: 1412.6980_, 2017.
* Kohler et al. [2019] Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann, Ming Zhou, and Klaus Neymeyr. Exponential convergence rates for batch normalization: The power of length-direction decoupling in non-convex optimization. In _The 22nd International Conference on Artificial Intelligence and Statistics_, 2019.
* Krizhevsky [2009] Alex Krizhevsky. Learning multiple layers of features from tiny images. _Lecture Notes_, 2009.
* Kumar [2023] Shailesh Kumar. Classifying cifar100 images using resnets, regularization and data augmentation in pytorch. [https://jovian.com/kumar-shailesh1597/cifar100-resnet18](https://jovian.com/kumar-shailesh1597/cifar100-resnet18), 2023.
* Kurdyka [1998] Krzysztof Kurdyka. On gradients of functions definable in o-minimal structures. In _Annales de l'institut Fourier_, volume 48, pages 769-783, 1998.
* Lab [2014] Criteo A. I. Lab. Criteo 1tb click logs dataset. 2014. URL [https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/).
* Lageman [2007] Christian Lageman. _Convergence of gradient-like dynamical systems and optimization algorithms_. PhD thesis, Universitat Wurzburg, 2007.
* Levy [2017] Kfir Levy. Online to offline conversions, universality and adaptive minibatch sizes. In _Advances in Neural Information Processing Systems_. Curran Associates, Inc., 2017.
* Li and Pong [2018] Guoyin Li and Ting Kei Pong. Calculus of the exponent of kurdyka-lojasiewicz inequality and its applications to linear convergence of first-order methods. _Foundations of computational mathematics_, 2018.
* Li et al. [2018] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. _Advances in neural information processing systems_, 31, 2018.

* Liu et al. [2020] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning. _arXiv preprint arXiv:2003.00307_, 2020.
* Liu et al. [2022] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. _Applied and Computational Harmonic Analysis_, 2022.
* Liu et al. [2023] Chaoyue Liu, Dmitriy Drusvyatskiy, Misha Belkin, Damek Davis, and Yian Ma. Aiming towards the minimizers: fast convergence of SGD for overparametrized problems. In _Proceedings of Advances in Neural Information Processing Systems_, 2023.
* Liu et al. [2014] Ji Liu, Steve Wright, Christopher Re, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. In _Proceedings of the 31st International Conference on Machine Learning_, 2014.
* Loizou et al. [2021] Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence. In _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, 2021.
* Luo and Tseng [1993] Zhi-Quan Luo and Paul Tseng. Error bounds and convergence analysis of feasible descent methods: a general approach. _Annals of Operations Research_, 1993.
* Makarenko et al. [2023] Maksim Makarenko, Elnur Gasanov, Abdurakhmon Sadiev, Rustem Islamov, and Peter Richtarik. Adaptive compression for communication-efficient distributed training. _Transactions on Machine Learning Research_, 2023.
* Malinovsky et al. [2022] Grigory Malinovsky, Konstantin Mishchenko, and Peter Richtarik. Server-side stepsizes and sampling without replacement provably help in federated optimization. _OPT2021: 13th Annual Workshop on Optimization for Machine Learning_, 2022.
* Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In _Proceedings of 4th International Conference on Learning Representations (ICLR 2016)_, 2016.
* Mikolov et al. [2010] Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. _Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010_, 2010.
* Naumov et al. [2019] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization and recommendation systems. _arXiv preprint arXiv:1906.00091_, 2019.
* Necoara et al. [2019] I. Necoara, Yu. Nesterov, and F. Glineur. Linear convergence of first order methods for non-strongly convex optimization. _Mathematical Programming_, 2019.
* Nesterov [2004] Yurii Nesterov. _Introductory Lectures on Convex Optimization: A Basic Course_. Kluwer Academic Publishers, Dordrecht, The Netherlands, 2004.
* Nesterov and Polyak [2006] Yurii Nesterov and Boris T. Polyak. Cubic regularization of newton method and its global performance. _Mathematical Programming_, 2006.
* Orvieto and Xiao [2024] Antonio Orvieto and Lin Xiao. An adaptive stochastic gradient method with non-negative gauss-newton stepsizes. _arXiv preprint arXiv:2407.04358_, 2024.
* Orvieto et al. [2022] Antonio Orvieto, Simon Lacoste-Julien, and Nicolas Loizou. Dynamics of sgd with stochastic polyak stepsizes: Truly adaptive variants and convergence to exact solution. In _Advances in Neural Information Processing Systems_, 2022.

* [65] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Proceedings of Advances in Neural Information Processing Systems_, 2019.
* [66] Boris T. Polyak. Gradient methods for the minimisation of functionals. _USSR Computational Mathematics and Mathematical Physics_, 1963.
* [67] B.T. Polyak. Some methods of speeding up the convergence of iteration methods. _USSR Computational Mathematics and Mathematical_, 1964.
* [68] Quentin Rebjock and Nicolas Boumal. Fast convergence to non-isolated minima: four equivalent conditions for c2 functions. _arXiv preprint arXiv:2303.00096_, 2023.
* MICCAI 2015_, Cham, 2015.
* [70] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In _Proceedings of the 35th International Conference on Machine Learning_, 2018.
* [71] Karthik A. Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, and Tom Goldstein. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In _Proceedings of the 37st International Conference on Machine Learning_, 2020.
* [72] Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. Slimpajama-dc: Understanding data combinations for llm training. _arXiv preprint arXiv:2309.10818_, 2023.
* [73] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv: 1409.1556_, 2015.
* [74] Chaewhan Song, Ali Ramezani-Kebrya, Thomas Pethick, Armin Eftekhari, and Volkan Cevher. Subquadratic overparameterization for shallow neural networks. In _Advances in Neural Information Processing Systems_, 2021.
* [75] Hoang Tran, Qinzi Zhang, and Ashok Cutkosky. Empirical tests of optimization assumptions in deep learning. _arXiv preprint arXiv:2407.01825_, 2024.
* [76] Hossein Valavi, Sulin Liu, and Peter Ramadge. Revisiting the landscape of matrix factorization. In _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, 2020.
* [77] Lou Van den Dries and Chris Miller. Geometric categories and o-minimal structures. 1996.
* [78] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 2017.
* [79] Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of sgd for over-parameterized models (and an accelerated perceptron). In _Proceedings of 22nd International Conference on Artificial Intelligence and Statistics (AISTATS 2019)_, 2019.
* [80] Luca Venturi, Afonso S. Bandeira, and Joan Bruna. Spurious valleys in one-hidden-layer neural network optimization landscapes. _Journal of Machine Learning Research_, 2019.
* [81] Rachel Ward and Tamara Kolda. Convergence of alternating gradient descent for matrix factorization. _Advances in Neural Information Processing Systems_, 36:22369-22382, 2023.

* [82] Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad stepsizes: Sharp convergence over nonconvex landscapes. In _Proceedings of the 36th International Conference on Machine Learning_, 2019.
* [83] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv: 1708.07747_, 2017.
* [84] Rui Yuan, Robert M Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policy gradient. In _International Conference on Artificial Intelligence and Statistics_, 2022.
* [85] Jure Zbontar, Florian Knoll, Anuroop Sriram, Matthew J. Muckley, Mary Bruno, Aaron Defazio, Marc Parente, Krzysztof J. Geras, Joe Katsnelson, Hersh Chandarana, Zizhao Zhang, Michal Drozdzal, Adriana Romero, Michael G. Rabbat, Pascal Vincent, James Pinkerton, Duo Wang, Nafissa Yakubova, Erich Owens, C. Lawrence Zitnick, Michael P. Recht, Daniel K. Sodickson, and Yvonne W. Lui. fastmri: An open dataset and benchmarks for accelerated MRI. _arXiv preprint arXiv: 1811.08839_.
* [86] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. _Communications of the ACM_, 2016.
* [87] Hui Zhang and Wotao Yin. Gradient methods for convex minimization: better rates under weaker conditions. _arXiv preprint arXiv: 1303.4645_, 2013.
* [88] Yi Zhou and Yingbin Liang. Critical points of linear neural networks: Analytical forms and landscape properties. In _Proceedings of International Conference on Learning Representations (ICLR 2018)_, 2018.
* [89] Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to global minimum in deep learning via star-convex path. In _Proceedings of International Conference on Learning Representations (ICLR 2019)_, 2019.

###### Contents

* 1 Introduction
* 2 Related work
	* 2.1 Function classes in optimization
	* 2.2 Limitations of existing conditions
* 3 The proposed \(\alpha\)-\(\beta\)-condition
	* 3.1 Theoretical verification of the \(\alpha\)-\(\beta\)-condition
* 4 Theoretical convergence of algorithms
	* 4.1 Convergence under the \(\alpha\)-\(\beta\)-condition
* 5 Experimental validation of the \(\alpha\)-\(\beta\)-condition
	* 5.1 MLP architecture
	* 5.2 CNN architecture
	* 5.3 Resnet architecture
	* 5.4 Training of AlgoPerf workloads and transformers for language modeling
	* 5.5 Additional experiments
* 6 Conclusion, potential extensions, and limitations.
* A Additional explanation on PL assumption
* B Additional examples
* C Missing proofs
* C.1 Proofs of examples satisfying definition 1
* C.1.1 Proof of example 1
* C.1.2 Proof of example 2
* C.1.3 Proof of Example 3
* C.1.4 Proof of example 6
* C.1.5 Proof of example 4
* C.1.6 Proof of example 5
* C.2 Convergence of optimization algorithms under \(\alpha\)-\(\beta\)-condition
* C.2.1 Convergence of SGD
* C.2.2 Convergence of SGD with Polyak Stepsize
* C.2.3 Convergence of NGN
* C.2.4 Convergence of AdaGrad-norm-max
* D Additional experiments
* D.1 Half space learning

* 2 Experiment setup from section 2.2
	* 2.3 MLP architecture
	* 2.4 CNN architecture
	* 2.5 Resnet architecture
	* 2.6 Verification of \(\boldsymbol{\alpha}\)-\(\beta\)-condition by different optimizers
	* 2.7 Increasing the depth of Resnet architecture
	* 2.8 AlgoPerf experiments
	* 2.9 Pythia experiments
* 3
* 4
* 5 ChecklistAdditional explanation on PL assumption

Let us consider the relaxation of PL condition for some \(\delta\in[1,2]\) and \(\mu>0\) (Assumption 3 in [22])

\[\|\nabla f(x)\|^{\delta}\geq(2\mu)^{\delta/2}(f(x)-f^{*})\quad\text{ for all }x\in\mathbb{R}^{d}. \tag{12}\]

Taking logarithms of both sides we continue

\[2\log(\|\nabla f(x)\|)-\frac{2}{\delta}\log(f(x)-f^{*})\geq\log(2\mu). \tag{13}\]

To satisfy PL condition for some \(\delta\), we need to find \(\mu\) that satisfies (13) for all iterations. In Figure 8 we plot LHS of (13). We observe that the values of \(\mu\) that satisfy (13) for all iterations should be of order \(10^{-9}-10^{-7}\) that leads to slow theoretical convergence [37]. Therefore, we claim that PL condition does not hold globally for neural networks. Nevertheless, it might hold locally around the solution (the value of \(\mu\) closer to the end of the training are large enough).

## Appendix B Additional examples

In this section, we list additional examples that satisfy Definition 1 and might have non-optimal stationary points. The functions of this form are typically used to simulate non-convex optimization problems and test the performance of algorithms on small or synthetic datasets [34, 55, 38].

**Example 6**.: Let \(f_{1},f_{2}\colon\mathbb{R}^{2}\to\mathbb{R}\) be such that

\[f_{1}(x,y)=\frac{x^{2}+y^{2}}{1+x^{2}+y^{2}},\quad f_{2}(x,y)=\frac{(x-1)^{2}+ (y-1)^{2}}{1+(x-1)^{2}+(y-1)^{2}}, \tag{14}\]

then Definition 1 holds with \(\alpha\geq\frac{2}{\min_{i}\min_{(x,i)\in\mathbb{R}}f_{i}(z,t)}\approx 41.369325\) and \(\beta=\alpha-1\). Besides, Definition 1 does not hold with \(\beta=0\) not satisfy Definition 1 with \(\beta=0\).

## Appendix C Missing proofs

### Proofs of examples satisfying definition 1

We highlight that we only show that \(\alpha\)-\(\beta\)-condition holds for some \(\alpha,\beta\) without giving all possible values of them.

Figure 8: Training of \(3\) layer LSTM model that shows Aiming condition does not always hold. We plot possible values of PL constant for different powers \(\delta\) in (12). We observe that possible values of \(\mu\) are of order \(10^{-9}-10^{-7}\) which implies slow theoretical convergence and contradicts practical observations.

#### c.1.1 Proof of example 1

**Example 1**.: Let \(f,f_{1},f_{2}\colon\mathbb{R}^{2}\to\mathbb{R}\) be such that

\[f=\tfrac{1}{2}(f_{1}+f_{2})\quad\text{with}\quad f_{1}(x,y)=\tfrac{(x+y)^{2}}{(x +y)^{2}+1},\quad f_{2}(x,y)=\tfrac{(x+y+1)^{2}}{(x+y+1)^{2}+1}, \tag{3}\]

then Definition 1 holds with \(\alpha\in[\nicefrac{{5}}{{2}},+\infty)\) and \(\beta\in[\nicefrac{{4\alpha}}{{5}},\alpha)\).

Proof.: Let \((z,t)=\operatorname{Proj}((x,y),\mathcal{S})\). One can show that the set of minimizers of \(f\) in this example is \(\mathcal{S}=\{(x,y)\colon y=-x-\nicefrac{{1}}{{2}}\}\). Moreover, \(f_{1}^{*}=0\) for \(x=-y\). We will show that \(\alpha\)-\(\beta\)-condition holds for \(i=1\). For \(i=2\) it can be shown in similar way with change of variables. We have

\[\partial_{x}f_{1}(x,y)=\partial_{y}f_{1}(x,y)=\frac{2(x+y)}{(1+(x+y)^{2})^{2}}.\]

Therefore, we need to show that

\[\frac{2(x+y)}{(1+(x+y)^{2})}(x-z+y-t)\geq(\alpha-\beta)\frac{(x+y)^{2}}{1+(x+y )^{2}}-\alpha f_{i}(z,t).\]

Note that \(f_{i}(z,t)=\frac{(z+t)^{2}}{1+(z+t)^{2}}=\frac{(-\nicefrac{{1}}{{2}})^{2}}{1+( -\nicefrac{{1}}{{2}})^{2}}=\frac{1}{5}\). Moreover, we can compute the projection operator onto \(\mathcal{S}\). After simple derivations, the projection can be expressed as

\[\operatorname{Proj}((x,y),\mathcal{S})=\frac{1}{2}\begin{pmatrix}x-y-\nicefrac {{1}}{{2}}\\ -x+y-\nicefrac{{1}}{{2}}\end{pmatrix}.\]

Therefore, we need to satisfy

\[\frac{2(x+y)}{(1+(x+y)^{2})^{2}}(x+y+\nicefrac{{1}}{{2}})\geq(\alpha-\beta) \frac{(x+y)^{2}}{1+(x+y)^{2}}-\frac{\alpha}{5}.\]

This is equivalent to

\[2(x+y)(x+y+\nicefrac{{1}}{{2}})\geq(\alpha-\beta)(x+y)^{2}(1+(x+ y)^{2})-\frac{\alpha}{5}(1+(x+y)^{2})^{2}\] \[\Leftrightarrow 2(x+y)^{2}+\frac{\alpha}{5}(1+2(x+y)^{2}+(x+y)^{4}) \geq(\alpha-\beta)((x+y)^{2}+(x+y)^{4})-(x+y). \tag{15}\]

We should satisfy the above for all values of \(x+y\). First, we need the coefficient next to \((x+y)^{4}\) in LHS to be larger or equal of the corresponding coefficient in RHS. This gives us \(\frac{\alpha}{5}\geq\alpha-\beta\). This shows that \(\beta\) can not be zero. Using \(2ab\leq a^{2}+b^{2}\), (15) is satisfied as long as we have

\[2(x+y)^{2}+\frac{\alpha}{5}(1+2(x+y)^{2}+(x+y)^{4})\geq(\alpha-\beta)((x+y)^{ 2}+(x+y)^{4})+\frac{1}{2}(x+y)^{2}+\frac{1}{2}.\]

Therefore, we should satisfy

\[\begin{cases}\text{coefficient next to }(x+y)^{4}:\frac{\alpha}{5}\geq\alpha- \beta\\ \text{coefficient next to }(x+y)^{2}:2+\frac{2\alpha}{5}\geq\alpha-\beta+\frac{1}{2} \\ \text{coefficient next to }1:\frac{\alpha}{5}\geq\frac{1}{2}\end{cases}\]

Note that if the first inequality holds, then the second one is true as well. Therefore, from the first and third inequalities we get that the solution of the system of inequalities is \(\alpha\geq\frac{5}{2}\) and \(\beta\in[\nicefrac{{4\alpha}}{{5}},\alpha]\). 

#### c.1.2 Proof of example 2

**Example 2**.: Let \(f,f_{1},f_{2}\colon\mathbb{R}^{2}\to\mathbb{R}\) be such that

\[f=\tfrac{1}{2}(f_{1}+f_{2})\quad\text{with}\quad f_{1}(x,y)=1-e^{-x^{2}-y^{2} },\quad f_{2}(x,y)=1-e^{-(x-2)^{2}-(y-2)^{2}}, \tag{4}\]

then Definition 1 holds for some \(\alpha\) and \(\beta=\alpha-8\).

Proof.: Let \((z,t)=\operatorname{Proj}((x,y),\mathcal{S})\). Again, we show that \(\alpha\)-\(\beta\)-condition holds for \(f_{1}\); for \(f_{2}\) it can be proved with the same arguments with change of variables. Note that \(f_{i}^{*}=0\) and \(f^{*}>0\). We also observe that \(\mathcal{S}\) contains two points in total, and both of them of the form \((t,t)\) where for one \(t\) is close to \(0\) (it is around \(t\approx 0.00067\)) and for second \(t\) is close to \(2\) (it is around \(t\approx 1.99932\)). Besides, note that \(c\coloneqq\min_{i}\min_{(t,t)\in\mathcal{S}}f_{i}(t,t)\in(0,1)\) and \(c\approx 9\cdot 10^{-7}\), which means that we are close to interpolation.

We have in this case

\[\partial_{x}f_{1}(x,y)=2x\exp(-x^{2}-y^{2}),\quad\partial_{y}f_{1}(x,y)=2y\exp (-x^{2}-y^{2}).\]

Therefore, we need to satisfy

\[\exp(-x^{2}-y^{2})(2x(x-t)+2y(y-t))\geq(\alpha-\beta)(1-\exp(-x^{2}-y^{2}))- \alpha f_{1}(z,t). \tag{16}\]

Note that if \(\beta=0\) and \(x,y\to+\infty,\) then the inequality is obviously can not be satisfied for all \(x,y\in\mathbb{R}\). Indeed, assume \(\beta=0\), then since \(f_{1}(z,t)<1\) and LHS goes to \(0\) while RHS to \(\alpha-\alpha f_{1}(z,t)>0\).

Rearranging terms in (16), we should satisfy

\[\exp(-x^{2}-y^{2})(2x^{2}+2y^{2}+\alpha-\beta)+\alpha f_{1}(z,t)\geq\exp(-x^{2 }-y^{2})(2xt+2yt)+(\alpha-\beta).\]

Using \(2ab\leq a^{2}+b^{2}\), the above is satisfied if the following inequality holds

\[\exp(-x^{2}-y^{2})(2x^{2}+2y^{2}+\alpha-\beta)+\alpha c\geq\exp(- x^{2}-y^{2})(x^{2}+y^{2}+2t^{2})+(\alpha-\beta)\] \[\Leftrightarrow \exp(-x^{2}-y^{2})(x^{2}+y^{2}+\alpha-\beta)+\alpha c\geq\exp(-x^ {2}-y^{2})\cdot 2t^{2}+(\alpha-\beta).\]

Since \(t\in(0,2)\) we can choose \(\alpha-\beta=8\), so that \(x^{2}+y^{2}-2t^{2}+\alpha-\beta\geq x^{2}+y^{2}\geq 0.\) Finally, we choose \(\alpha\geq\frac{8}{c}\), so that \(\alpha c\geq\alpha-\beta=8\). We can estimate that \(\alpha\gtrsim 72\cdot 10^{7}\). 

#### c.1.3 Proof of Example 3

**Example 3**.: Let \(f,f_{1},f_{2}\colon\mathbb{R}^{2}\to\mathbb{R}\) be such that

\[f=\tfrac{1}{2}(f_{1}+f_{2})\quad\text{with}\quad f_{1}(x,y)=\tfrac{1+x^{2}+y^{ 2}}{4+x^{2}+y^{2}},\quad f_{2}(x,y)=\tfrac{(x-2.5)^{2}+(y-2.5)^{2}}{4+(y-2.5)^ {2}+(y-2.5)^{2}}, \tag{6}\]

then Definition 1 holds for some \(\alpha\) and \(\beta=\alpha-1\).

Proof.: Let \((z,t)=\operatorname{Proj}((x,y),\mathcal{S})\). For this example, we have \(f_{1}^{*}=\frac{1}{4}\) (achieved at \((0,0)\)), \(f_{2}^{*}=0\) (achieved at \((2.5,2.5)\)), and \(f^{*}\approx=0.408\) (achieved at \((2.471,2.471)\)). Besides, we have \(f(0,0)=0.587963\) and \(f(2.5,2.5)=0.409091\). Besides, \(c\coloneqq\min_{i}\min_{(z,t)\in\mathcal{S}}f_{i}(z,t)=f_{2}(2.471,2.471)=0.00167 918\). Let us show that \(\alpha\)-\(\beta\)-condition holds for \(f_{1}\); for \(f_{2}\) it can be shown similarly. We have

\[\partial_{x}f_{1}(x,y)=\frac{6x}{(4+x^{2}+y^{2})^{2}},\quad\partial_{y}f_{1}(x,y)=\frac{6y}{(4+x^{2}+y^{2})^{2}}.\]

Therefore, we need to satisfy for some \(\alpha\) and \(\beta\)

\[\frac{6}{(4+x^{2}+y^{2})^{2}}(x(x-z)+y(y-t))\geq(\alpha-\beta) \frac{1+x^{2}+y^{2}}{4+x^{2}+y^{2}}-f_{1}(z,t)\] \[\Leftrightarrow 6x^{2}+6y^{2}-6xz-6yt\geq(\alpha-\beta)(1+x^{2}+y^{2})(4+x^{2}+y ^{2})-\alpha f_{1}(z,t)(4+x^{2}+y^{2})^{2}\] \[\Leftrightarrow 6(x^{2}+y^{2})+\alpha f_{1}(z,t)(16+8(x^{2}+y^{2})+(x^{2}+y^{2})^ {2})\geq 6xz+6yt\] \[+(\alpha-\beta)(4+5(x^{2}+y^{2})+(x^{2}+y^{2})^{2}).\]

The above is satisfied for all \(x,y\in\mathbb{R}\) and \(\alpha-\beta=1\) if we have

\[6(x^{2}+y^{2})+\alpha f_{1}(z,t)(16+8(x^{2}+y^{2})+(x^{2}+y^{2}) ^{2})\geq 3(x^{2}+y^{2})+3z^{2}+3t^{2}\] \[+(4+5(x^{2}+y^{2})+(x^{2}+y^{2})^{2}).\]

To satisfy the above for all \(x,y\in\mathbb{R}\) we should have

\[\begin{cases}16\alpha f_{1}(z,t)&\geq 4+3(z^{2}+t^{2})\\ 6+8\alpha f_{1}(z,t)&\geq 3+5\\ \alpha f_{1}(z,t)&\geq 1,\end{cases}\]

where \(z=t=2.471\), and \(f_{1}(z,t)=0.0016718\). We can satisfy \(\alpha\)-\(\beta\)-condition with \(\alpha\gtrsim 1512.4586\)

#### c.1.4 Proof of example 6

**Example 6**.: Let \(f_{1},f_{2}\colon\mathbb{R}^{2}\to\mathbb{R}\) be such that

\[f_{1}(x,y)=\frac{x^{2}+y^{2}}{1+x^{2}+y^{2}},\quad f_{2}(x,y)=\frac{(x-1)^{2}+(y -1)^{2}}{1+(x-1)^{2}+(y-1)^{2}}, \tag{14}\]

then Definition 1 holds with \(\alpha\geq\frac{2}{\min_{i}\min_{(x,t)\in\mathcal{S}}f_{i}(z,t)}\approx 41.369325\) and \(\beta=\alpha-1\). Besides, Definition 1 does not hold with \(\beta=0\) not satisfy Definition 1 with \(\beta=0\).

Proof.: Let \((z,t)=\operatorname{Proj}((x,y),\mathcal{S})\). We show that \(\alpha\)-\(\beta\)-condition holds for \(f_{1}\); for \(f_{2}\) the proof is similar with change of variables. In this case, \(f_{i}^{*}=0\), while \(f^{*}>0\) (\(f^{*}\approx 0.316988\) at points \((0.159375,0.159375)\) and \((0.840625,0.840625)\)). Besides, we have \(c\coloneqq\min_{i}\min_{(z,t)\in\mathcal{S}}f_{i}(z,t)\approx 0.048345>0\). Moreover, We have

\[\partial_{x}f_{1}(x,y)=\frac{2x}{(1+x^{2}+y^{2})^{2}},\quad\partial_{y}f_{1}(x,y)=\frac{2y}{(1+x^{2}+y^{2})^{2}}.\]

Therefore, we need to satisfy for some \(\alpha\) and \(\beta\)

\[\frac{1}{(1+x^{2}+y^{2})^{2}}(2x(x-z)+2y(y-t))\geq(\alpha-\beta) \frac{x^{2}+y^{2}}{1+x^{2}+y^{2}}-\alpha f_{1}(z,t)\] \[\Leftrightarrow 2x^{2}+2y^{2}-2xz-2yt\geq(\alpha-\beta)(x^{2}+y^{2})(1+x^{2}+y^{ 2})-\alpha f_{1}(z,t)(1+x^{2}+y^{2})^{2}\] \[\Leftrightarrow 2x^{2}+2y^{2}+\alpha f_{1}(z,t)(1+2(x^{2}+y^{2})+(x^{2}+y^{2})^{ 2})\geq 2xz+2yt+\] \[(\alpha-\beta)(x^{2}+y^{2})(1+x^{2}+y^{2}).\]

Note that if we take \(\beta=0\) and \(x,y\to+\infty\), then LHS grows as \(\alpha f_{1}(z,t)(x^{2}+y^{2})^{2}\) and RHS grows as \(\alpha(x^{2}+y^{2})^{2}\), therefore in the limit the RHS is larger. This implies that \(\beta\neq 0\). Using \(2ab\leq a^{2}+b^{2}\) the above is satisfied if we have

\[(2+2\alpha f_{1}(z,t))(x^{2}+y^{2})+\alpha f_{1}(z,t)(x^{2}+y^{2 })^{2}+\alpha f_{1}(z,t)\geq(x^{2}+y^{2})(1+\alpha-\beta)+\] \[z^{2}+t^{2}+(\alpha-\beta)(x^{2}+y^{2})^{2}. \tag{17}\]

Let us take \(\alpha-\beta=1\) and \(\alpha\geq\frac{2}{\min_{i}\min_{(z,t)\in\mathcal{S}}f_{i}(z,t)}\approx 41.369325\). With this choice, we have

\[z^{2}+t^{2}\leq 2\leq\alpha f_{1}(z,t),\] \[1+\alpha-\beta=2\leq 2+2\alpha f_{1}(z,t),\] \[\alpha-\beta=1<2\leq\alpha f_{1}(z,t).\]

Therefore, LHS is always larger than RHS in (17). 

#### c.1.5 Proof of example 4

**Example 4**.: Let \(f_{i},f_{ij}\) be such that

\[f(W,S)=\frac{1}{2nm}\|X-W^{\top}S\|_{\mathrm{F}}^{2}=\frac{1}{2nm}\sum_{i,j}(X_ {ij}-w_{i}^{\top}s_{j})^{2},\quad f_{ij}(W,S)=\tfrac{1}{2}(X_{ij}-w_{i}^{\top }s_{j})^{2}, \tag{7}\]

where \(X\in\mathbb{R}^{n\times m},W=(w_{i})_{i=1}^{n}\in\mathbb{R}^{k\times n},S=(s_{j })_{j=1}^{m}\in\mathbb{R}^{k\times m},\) and \(\operatorname{rank}(X)=r\geq k\). We assume that \(X\) is generated using matrices \(W^{*}\) and \(S^{*}\) with non-zero additive noise that minimize empirical loss, namely, \(X=(W^{*})^{\top}S^{*}+(\varepsilon_{ij})_{i\in[n],j\in[m]}\quad\text{where} \quad W^{*},S^{*}=\operatorname{argmin}_{W,S}f(W,S)\). Let \(\mathcal{X}\) be any bounded set that contains \(\mathcal{S}\). Then Definition 1 is satisfied with \(\alpha=\beta+1\) and some \(\beta>0\).

Proof.: Since \(k\leq r\), then the matrix factorization problem has a unique solution which can be found from SVD decomposition of \(X\)[76].

We have

\[\nabla_{w_{i}}f_{ij}(W,S)=(w_{i}^{\top}s_{j}-X_{ij})s_{j},\quad\nabla_{s_{j}}f_ {ij}(W,S)=(w_{i}^{\top}s_{j}-X_{ij})w_{i}.\]Therefore, we need to show that

\[\big{\langle}(w_{i}^{\top}s_{j}-X_{ij})s_{j},w_{i}-w_{i}^{*}\big{\rangle}+\big{ }\big{\langle}(w_{i}^{\top}s_{j}-X_{ij})w_{i},s_{j}-s_{j}^{*}\big{\rangle}\geq \frac{\alpha-\beta}{2}(X_{ij}-w_{i}^{\top}s_{j})^{2}\]

\[-\frac{\alpha}{2}(X_{ij}-(w_{i}^{*})^{\top}s_{j}^{*})^{2},\]

since \(f_{ij}^{*}=0\). Since \(f_{ij}\) is convex w.r.t. \(w_{i}\), the above holds if we have

\[\frac{1}{2}(w_{i}^{\top}s_{j}-X_{ij})^{2}-\frac{1}{2}(s_{j}^{\top }w_{i}^{*}-X_{ij})^{2}+\frac{1}{2}(w_{i}^{\top}s_{j}-X_{ij})^{2}-\frac{1}{2}(w _{i}^{\top}s_{j}^{*}-X_{ij})^{2}\geq\] \[\frac{\alpha-\beta}{2}(X_{ij}-w_{i}^{\top}s_{j})^{2}-\frac{\alpha }{2}(X_{ij}-(w_{i}^{*})^{\top}s_{j}^{*})^{2}.\]

Let us take \(\alpha=\beta+1\), then we can simplify the above as follows

\[\frac{1}{2}(w_{i}^{\top}s_{j}-X_{ij})^{2}+\frac{\alpha}{2}(X_{ij}-(w_{i}^{*})^ {\top}s_{j}^{*})^{2}\geq\frac{1}{2}(s_{j}^{\top}w_{i}^{*}-X_{ij})^{2}+\frac{1}{ 2}(w_{i}^{\top}s_{j}^{*}-X_{ij})^{2} \tag{18}\]

Since we consider \(\mathcal{X}\) to be bounded, then there exist \(r\geq 0\) such that \(\|s_{j}\|,\|w_{i}\|\leq r\) for all \(i\) and \(j\). Therefore, the RHS in (18) is bounded by some constant \(C\). From the data generation, we have that \(c=\min_{ij}(X_{ij}-(w_{i}^{*})^{\top}s_{j}^{*})^{2}=\min_{ij}\varepsilon_{ij}^ {2}>0\). Therefore, we can take \(\alpha\geq\frac{2C}{c}\) to verify (18).

#### c.1.6 Proof of example 5

**Example 5**.: Consider training a two-layer neural network with a logistic loss

\[f=\frac{1}{n}\sum_{i=1}^{n}f_{i},\quad f_{i}(W,v)=\phi(y_{i}\cdot v^{\top} \sigma(Wx_{i}))+\lambda_{1}\|v\|^{2}+\lambda_{2}\|W\|_{\mathbb{F}}^{2} \tag{8}\]

for a classification problem where \(\phi(t)\coloneqq\log(1+\exp(-t)),\,W\in\mathbb{R}^{k\times d},v\in\mathbb{R}^{k}\), \(\sigma\) is a ReLU function applied coordinate-wise, \(y_{i}\in\{-1,+1\}\) is a label and \(x_{i}\in\mathbb{R}^{d}\) is a feature vector. Let \(\mathcal{X}\) be any bounded set that contains \(\mathcal{S}\). Then the \(\alpha\)-\(\beta\)-condition holds in \(\mathcal{X}\) for some \(\alpha\geq 1\) and \(\beta=\alpha-1\).

Proof.: Let \((Z,z)\coloneqq\operatorname{Proj}((W,v),\mathcal{S})\). We have the following derivations for gradients

\[\mathbb{R}^{k\times d}\ni\nabla_{W}f_{i}(W,v)=\phi^{\prime}(y_{i} v^{\top}\sigma(Wx_{i}))\cdot y_{i}(v\circ\mathbb{1}_{Wx_{i}\geq 0})x_{i}^{ \top}+2\lambda_{2}W,\] \[\mathbb{R}^{k}\ni\nabla_{v}f_{i}(W,v)=\phi^{\prime}(y_{i}v^{\top} \sigma(Wx_{i}))\cdot y_{i}\sigma(Wx_{i})=\phi^{\prime}(y_{i}v^{\top}\sigma(Wx _{i}))\cdot y_{i}(Wx_{i})\circ\mathbb{1}_{Wx_{i}\geq 0}\] \[+2\lambda_{1}v\] \[=\phi^{\prime}(y_{i}v^{\top}\sigma(Wx_{i}))\cdot y_{i}(Wx_{i}) \circ e_{w}+2\lambda_{1}v,\]

where we denote \(e_{w}\coloneqq\mathbb{1}_{Wx_{i}\geq 0}\in\mathbb{R}^{k}\). Besides, we denote \(e_{z}\coloneqq\mathbb{1}_{Zx_{i}\geq 0}\in\mathbb{R}^{k}\). Note that the optimal value of \(f_{i}^{*}>0\) because of the L2 regularization.

Note that we have the following relations

\[v^{\top}\sigma(Wx_{i})=\langle v,(Wx_{i})\circ\mathbb{1}_{Wx_{i}\geq 0}\rangle= \langle v\circ\mathbb{1}_{Wx_{i}\geq 0},Wx_{i}\rangle=\langle v\circ e_{w},Wx_{i}\rangle. \tag{19}\]

Note that \(\mathcal{S}\) is bounded because of the L2 regularization. Since we assume that the interpolation does not hold, then \(f\) is always strictly larger than \(f_{i}^{*}\) in \(\mathcal{X}\), and due to continuity there exists \(c\coloneqq\min_{i\in[n]}\min_{(Z,z)\in\mathcal{S}}f_{i}(Z,z)>0\).

We need to show that there exist some \(\alpha\) and \(\beta\) such that

\[\langle\nabla_{W}f_{i}(W,v),W-Z\rangle_{\mathrm{F}}+\langle\nabla _{v}f_{i}(W,v),v-z\rangle\] \[\geq\alpha(f_{i}(W,v)-f_{i}(Z,z))-\beta(f_{i}(W,v)-f_{i}^{*})\] \[\Leftrightarrow\phi^{\prime}(y_{i}(v\circ e_{w})^{\top}Wx_{i})y _{i}\left(\big{\langle}(v\circ e_{w})x_{i}^{\top},W-Z\big{\rangle}+\langle(Wx_{ i})\circ e_{w},v-z\rangle\right)+2\lambda_{1}\langle v,v-z\rangle\] \[+2\lambda_{2}\langle W,W-Z\rangle\] \[\geq\alpha\left[\phi(y_{i}(v\circ e_{w})^{\top}Wx_{i})+\lambda_{1} \|v\|^{2}+\lambda_{2}\|W\|_{\mathrm{F}}^{2}-\phi(y_{i}(z\circ e_{z})^{\top}Zx_{i })-\lambda_{1}\|z\|^{2}-\lambda_{2}\|Z\|_{\mathrm{F}}^{2}\right]\] \[-\beta\left[\phi(y_{i}(v\circ e_{w})^{\top}Wx_{i})+\lambda_{1}\|v \|^{2}+\lambda_{2}\|W\|_{\mathrm{F}}^{2}-f_{i}^{*}\right]\] \[\Leftrightarrow\phi^{\prime}(y_{i}(v\circ e_{w})^{\top}Wx_{i}) \left([y_{i}(v\circ e_{w})^{\top}Wx_{i}-y_{i}(v\circ e_{w})^{\top}Zx_{i}]+[y_{i }(v\circ e_{w})^{\top}Wx_{i}-y_{i}(z\circ e_{w})^{\top}Wx_{i}]\right)\] \[+2\lambda_{1}\langle v,v-z\rangle+2\lambda_{2}\langle W,W-Z\rangle\] \[\geq\alpha\left[\phi(y_{i}(v\circ e_{w})^{\top}Wx_{i})+\lambda_{1} \|v\|^{2}+\lambda_{2}\|W\|_{\mathrm{F}}^{2}-\phi(y_{i}(z\circ e_{z})^{\top}Zx_{i })-\lambda_{1}\|z\|^{2}-\lambda_{2}\|Z\|_{\mathrm{F}}^{2}\right]\] \[-\beta\left[\phi(y_{i}(v\circ e_{w})^{\top}Wx_{i})+\lambda_{1}\|v \|^{2}+\lambda_{2}\|W\|_{\mathrm{F}}^{2}-f_{i}^{*}\right], \tag{20}\]where we use (19). Since \(\phi\) is convex, then we have \(\phi^{\prime}(x)(x-y)\geq\phi(x)-\phi(y)\). Therefore, using (19) again, we get that (20) is satisfied if we have

\[2\phi(y_{i}(v\circ e_{w})^{\top}Wx_{i})-\phi(y_{i}(v\circ e_{w}) ^{\top}Zx_{i})-\phi(y_{i}(z\circ e_{w})^{\top}Wx_{i})+\lambda_{1}\langle v,v-z \rangle+\lambda_{2}\langle W,W-Z\rangle\] \[\geq\alpha\left[\phi(y_{i}(v\circ e_{w})^{\top}Wx_{i})+\lambda_{1} \|v\|^{2}+\lambda_{2}\|W\|_{\mathrm{F}}^{2}-\phi(y_{i}(z\circ e_{z})^{\top}Zx_ {i})-\lambda_{1}\|z\|^{2}-\lambda_{2}\|Z\|_{\mathrm{F}}^{2}\right]\] \[-\beta\left[\phi(y_{i}(v\circ e_{w})^{\top}Wx_{i})+\lambda_{1}\|v \|^{2}+\lambda_{2}\|W\|_{\mathrm{F}}^{2}-f_{i}^{*}\right].\]

Now we take \(\alpha=\beta+1\) and simplify the above as follows

\[\phi(y_{i}(v\circ e_{w})^{\top}Wx_{i})+\alpha\phi(y_{i}(z\circ e_ {w})^{\top}Zx_{i}))+\lambda_{1}(2\langle v,v-z\rangle-\|v\|^{2}-\|z\|^{2})\] \[+\lambda_{2}(2(W,W-Z)-\|W\|_{\mathrm{F}}^{2}-\|Z\|_{\mathrm{F}}^{ 2})+(\alpha-1)(\lambda_{1}\|z\|^{2}+\lambda_{2}\|Z\|_{\mathrm{F}}^{2})\] \[\geq\phi(y_{i}(v\circ e_{w})^{\top}Zx_{i})+\phi(y_{i}(z\circ e_{w })^{\top}Wx_{i})+(\alpha-1)f_{i}^{*}. \tag{21}\]

The above is satisfied if we have

\[\phi(y_{i}(v\circ e_{w})^{\top}Wx_{i})+\alpha c+\lambda_{1}\|v-z \|^{2}+\lambda_{2}\|W-Z\|_{\mathrm{F}}^{2}+(\alpha-1)(\|z\|^{2}+\|Z\|_{\mathrm{ F}}^{2})\] \[\geq\phi(y_{i}(v\circ e_{w})^{\top}Zx_{i})+\phi(y_{i}(z\circ e_{w })^{\top}Wx_{i})+(\alpha-1)f_{i}^{*}, \tag{22}\]

because we also have \(\phi(y_{i}(z\circ e_{z})^{\top}Zx_{i})\geq c\) for all \(i\) and \((Z,z)\in\mathcal{S}\). Since \((W,v)\in\mathcal{X}\), there exist constants \(R,r\geq 0\) such that \(\|v\|\leq r\) and \(\|W\|\leq R\), and RHS in (22) is bounded by

\[2\max_{i\in[n]}\log(1+\exp(Rr\|x_{i}\|))\geq\phi(y_{i}(v\circ e_{w})^{\top}Zx_ {i})+\phi(y_{i}(z\circ e_{w})^{\top}Wx_{i}).\]

Therefore, we can take \(\alpha\geq\left\{\frac{2\max_{i\in[n]}\log(1+\exp(Rr\|x_{i}\|))}{c-f_{i}^{*}},1 \right\}\) and \(\beta=\alpha-1\). 

**Remark 3**.: The result suggest that the choice \(\beta=0\) is possible only if constants \(r\) and \(R\) are small, i.e. locally around \(\mathcal{S}\) only. In order to satisfy \(\alpha\)-\(\beta\)-condition in larger set \(\mathcal{X}\), one needs to choose \(\beta>0\).

### Convergence of optimization algorithms under \(\alpha\)-\(\beta\)-condition

#### c.2.1 Convergence of SGD

Constant stepsize.In this section, we present the proof of convergence of SGD with constant stepsize under \(\alpha\)-\(\beta\)-condition for completeness of the presentation.

**Theorem 1**.: _Assume that Assumptions 1-2 hold. Then the iterates of SGD (Alg. 1) with stepsize \(\gamma\leq\frac{\alpha-\beta}{2L}\) satisfy_

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right]\leq\frac{\mathbb{E}\left[ \mathrm{dist}(x^{0},\mathcal{S})^{2}\right]}{K}\frac{1}{\gamma(\alpha-\beta) }+\frac{2L\gamma}{\alpha-\beta}\sigma_{\mathrm{int}}^{2}+\frac{2\beta}{\alpha -\beta}\sigma_{\mathrm{int}}^{2}. \tag{9}\]

Proof.: Let \(x_{p}^{k}\in\mathrm{Proj}(x^{k},\mathcal{S})\) satisfies Definition 1. Using smoothness we have

\[\mathbb{E}_{k}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2}\right] \leq \mathbb{E}_{k}\left[\|x^{k+1}-x_{p}^{k}\|^{2}\right]\] \[= \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\gamma\mathbb{E}_{k}\left[ \left\langle\nabla f_{i_{k}}(x^{k}),x^{k}-x_{p}^{k}\right\rangle\right]+ \gamma^{2}\mathbb{E}_{k}\left[\|\nabla f_{i_{k}}(x^{k})\|^{2}\right]\] \[\stackrel{{(i)}}{{\leq}} \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\gamma\mathbb{E}_{k} \left[f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\right]+2\beta\gamma\mathbb{E}_{k} \left[f_{i_{k}}(x^{k})-f_{i_{k}}^{*}\right]\] \[+\ 2L\gamma^{2}\mathbb{E}_{k}\left[f_{i_{k}}(x^{k})-f_{i_{k}}^{* }\right]\] \[= \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\gamma\mathbb{E}_{k} \left[f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k})\right]\] \[+\ 2\gamma(\beta+L\gamma)\mathbb{E}_{k}\left[\left(f_{i_{k}}(x^{ k})-f_{i_{k}}(x_{p}^{k})\right)+\left(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}\right) \right],\]where \((i)\) holds because of the \(\alpha\)-\(\beta\)-condition and smoothness. Now we need to choose a stepsize \(\gamma\leq\frac{\alpha-\beta}{2L}\) to get

\[\mathbb{E}_{k}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2}\right] \leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-\gamma(\alpha-\beta)\mathbb{ E}_{k}\left[f_{i_{k}}(x^{k})-f_{i_{k}}(x^{k}_{p})\right]\] \[+\ 2\gamma(\beta+L\gamma)\mathbb{E}_{k}\left[f_{i_{k}}(x^{k}_{p}) -f^{*}_{i_{k}}\right].\]

Taking full expectation, noting that \(x^{k}_{p}\) is independent of the randomness of \(i_{k}\), and performing simple derivations, we get

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right]\leq\frac{\mathbb{E}\left[ \mathrm{dist}(x^{0},\mathcal{S})^{2}\right]}{K}\frac{1}{\gamma(\alpha-\beta)}+ \frac{2L\gamma}{\alpha-\beta}\sigma^{2}_{\mathrm{int}}+\frac{2\beta}{\alpha- \beta}\sigma^{2}_{\mathrm{int}}. \tag{23}\]

Decreasing stepsize.Now we present the results with decreasing stepsize.

**Theorem 4**.: _Assume that Assumptions 1-2 hold. Then the iterates of_ SGD _with decreasing stepsize \(\gamma_{k}=\frac{\gamma_{0}}{\sqrt{k+1}}\) where \(\gamma_{0}\leq\frac{\alpha-\beta}{2L}\) satisfy_

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{5\mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right] }{4(\alpha-\beta)\gamma_{0}\sqrt{K}}+\frac{5\gamma_{0}L\sigma^{2}_{\mathrm{ int}}}{\alpha-\beta}\frac{\log(K+1)}{\sqrt{K}}+\frac{2\beta}{\alpha-\beta} \sigma^{2}_{\mathrm{int}} \tag{24}\] \[= \widetilde{\mathcal{O}}\left(\frac{1}{\sqrt{K}}+\beta\sigma^{2}_{ \mathrm{int}}\right). \tag{25}\]

Proof.: Similarly to the proof of constant stepsize SGD we can obtain

\[\mathbb{E}_{k}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2}\right] \leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-\gamma_{k}(\alpha-\beta) \mathbb{E}_{k}\left[f_{i_{k}}(x^{k})-f_{i_{k}}(x^{k}_{p})\right]\] \[+\ 2\gamma_{k}(\beta+L\gamma_{k})\mathbb{E}_{k}\left[f_{i_{k}}(x ^{k}_{p})-f^{*}_{i_{k}}\right],\]

since \(\gamma_{k}\leq\gamma_{0}\leq\frac{\alpha-\beta}{2L}\) for all \(k\). Now we follow standard proof techniques [24] for decreasing stepsize. Taking full expectation and dividing both sides by \(\alpha-\beta\) we get

\[\gamma_{k}\mathbb{E}\left[f(x^{k})-f^{*}\right]\leq\frac{\mathbb{E}\left[ \mathrm{dist}(x^{k},\mathcal{S})^{2}\right]-\mathbb{E}\left[\mathrm{dist}(x^{k +1},\mathcal{S})^{2}\right]}{\alpha-\beta}+\frac{2\beta}{\alpha-\beta}\gamma_{ k}\sigma^{2}_{\mathrm{int}}+\frac{2L}{\alpha-\beta}\gamma_{k}^{2}\sigma^{2}_{ \mathrm{int}}.\]

Summing the above from iteration \(0\) to \(K-1\) leads to

\[\sum_{k=0}^{K-1}\gamma_{k}\mathbb{E}\left[f(x^{k})-f^{*}\right]\leq\frac{ \mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right]}{\alpha-\beta}+ \frac{2\beta}{\alpha-\beta}\sigma^{2}_{\mathrm{int}}\sum_{k=0}^{K-1}\gamma_{k} +\frac{2L}{\alpha-\beta}\sigma^{2}_{\mathrm{int}}\sum_{k=0}^{K-1}\gamma_{k}^{2}.\]

Therefore, we get

\[\min_{0\geq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right]\leq\frac{\mathbb{E} \left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right]}{(\alpha-\beta)\sum_{k=0}^{K -1}\gamma_{k}}+\frac{2\beta}{\alpha-\beta}\sigma^{2}_{\mathrm{int}}+\frac{2L} {\alpha-\beta}\sigma^{2}_{\mathrm{int}}\frac{\sum_{k=0}^{K-1}\gamma_{k}^{2}}{ \sum_{k=0}^{K_{1}}\gamma_{k}}.\]

Using the results of Theorem 5.7[24] we get

\[\sum_{k=0}^{K-1}\gamma_{k}^{2}\leq 2\gamma_{0}^{2}\log(K+1),\quad\sum_{k=0}^{K -1}\gamma_{k}\geq\frac{4\gamma_{0}}{5}\sqrt{K}.\]

Therefore, the final rate we get is

\[\min_{0\geq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{\mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right] }{(\alpha-\beta)\frac{4}{5}\gamma_{0}\sqrt{K}}+\frac{2\beta}{\alpha-\beta} \sigma^{2}_{\mathrm{int}}+\frac{2L}{\alpha-\beta}\sigma^{2}_{\mathrm{int}}\frac{ 2\gamma_{0}^{2}\log(K+1)}{\frac{4\gamma_{0}}{5}\sqrt{K}}\] \[= \frac{5\mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right] }{4(\alpha-\beta)\gamma_{0}\sqrt{K}}+\frac{5\gamma_{0}L\sigma^{2}_{\mathrm{int}} }{\alpha-\beta}\frac{\log(K+1)}{\sqrt{K}}+\frac{2\beta}{\alpha-\beta}\sigma^{2} _{\mathrm{int}}.\]

#### c.2.2 Convergence of \(\mathsf{SGD}\) with Polyak Stepsize

Constant stepsize parameter.In this section, we present the proof of convergence of \(\mathsf{SGD}\) with Polyak stepsize (with constant stepsize parameters) under \(\alpha\)-\(\beta\)-condition for completeness of the presentation.

**Lemma 1** (Lemma from [53]).: The \(\mathsf{SPS}_{\max}\) stepsize satisfy

\[\gamma_{k}^{2}\|\nabla f_{i_{k}}(x^{k})\|^{2}\leq\frac{\gamma_{k}}{c}(f_{i_{k} }(x^{k})-f_{i_{k}}^{*}). \tag{26}\]

**Lemma 2** (Lemma from [53]).: Assume each \(f_{i}\) is \(L\)-smooth, then \(\mathsf{SPS}_{\max}\) stepsize satisfy

\[\gamma_{\min}\coloneqq\min\left\{\frac{1}{2cL},\gamma_{\mathrm{b}}\right\} \leq\gamma_{k}\leq\gamma_{\mathrm{b}}. \tag{27}\]

Now we present the proof of Theorem 2 with constant stepsize parameters.

**Theorem 2**.: _Assume that Assumptions 1-2 hold. Then the iterates of \(\mathsf{SPS}_{\max}\) (Alg. 2) with a stepsize hyperparameter \(c>\frac{1}{2(\alpha-\beta)}\) satisfy_

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right]\leq\frac{c_{1}}{K} \mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right]+2\alpha c_{1} \gamma_{\mathrm{b}}\sigma_{\mathrm{int}}^{2}, \tag{10}\]

_where \(\gamma_{\min}\coloneqq\min\{\nicefrac{{1}}{{2cL}},\gamma_{\mathrm{b}}\}\) and \(c_{1}\coloneqq\frac{c}{\gamma_{\min}(2(\alpha-\beta)c-1)}\)._

Proof.: Let \(x_{p}^{k}\in\mathrm{Proj}(x^{k},\mathcal{S})\) that satisfies Definition 1, then we have

\[\mathbb{E}_{k}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2}\right] \leq \mathbb{E}_{k}\left[\|x^{k+1}-x_{p}^{k}\|^{2}\right]\] \[= \|x^{k}-x_{p}^{k}\|^{2}-2\mathbb{E}_{k}\left[\gamma_{k}\left\langle \nabla f_{i_{k}}(x^{k}),x^{k}-x_{p}^{k}\right\rangle\right]+\mathbb{E}_{k} \left[\gamma_{k}^{2}\|\nabla f_{i_{k}}(x^{k})\|^{2}\right]\] \[\stackrel{{(i)}}{{\leq}} \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\mathbb{E}_{k}\left[ \gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k}))\right]\] \[+\ 2\beta\mathbb{E}_{k}\left[\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k }}^{*})\right]+\frac{1}{c}\mathbb{E}_{k}\left[\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_ {k}}^{*})\right]\] \[= \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\mathbb{E}_{k}\left[ \gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})-[f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*} ]\right]\] \[+\ 2\beta\mathbb{E}_{k}\left[\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k }}^{*})\right]+\frac{1}{c}\mathbb{E}_{k}\left[\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_ {k}}^{*})\right]\] \[= \mathrm{dist}(x^{k},\mathcal{S})^{2}-\mathbb{E}_{k}\left[\gamma_{ k}\left(2\alpha-2\beta-\frac{1}{c}\right)(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})\right]\] \[+\ 2\alpha\mathbb{E}_{k}\left[\gamma_{k}(f_{i_{k}}(x_{p}^{k})-f_{ i_{k}}^{*})\right],\]

[MISSING_PAGE_FAIL:27]

Proof.: From the definition of the stepsize we have

\[\gamma_{k}\leq\frac{1}{c_{k}}\frac{f_{i_{k}}(x^{k})-f_{i_{k}}^{*}}{\|\nabla f_{i_{ k}}(x^{k})\|^{2}}.\]

Therefore, we have

\[\gamma_{k}^{2}\|\nabla f_{i_{k}}(x^{k})\|^{2}\leq\frac{\gamma_{k}}{c_{k}}(f_{i_{ k}}(x^{k})-f_{i_{k}}^{*}).\]

Let \(x_{p}^{k}=\mathrm{Proj}(x^{k},\mathcal{S})\) which satisfies Definition 1. Now we have

\[\mathrm{dist}(x^{k+1},\mathcal{S})^{2} \leq \|x^{k+1}-x_{p}^{k}\|^{2}\] \[\leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\gamma_{k}\left\langle \nabla f_{i_{k}}(x^{k}),x^{k}-x_{p}^{k}\right\rangle+\frac{\gamma_{k}}{c_{k}}(f _{i_{k}}(x^{k})-f_{i_{k}}^{*}).\]

Using \(\alpha\)-\(\beta\)-condition we get

\[\mathrm{dist}(x^{k+1},\mathcal{S})^{2} \leq \|x^{k+1}-x_{p}^{k}\|^{2}\] \[\leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\gamma_{k}(f_{i_{k}}(x ^{k})-f_{i_{k}}(x_{p}^{k}))+2\beta\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})\] \[+\ \frac{\gamma_{k}}{c_{k}}(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})\] \[= \mathrm{dist}(x^{k},\mathcal{S})^{2}-\gamma_{k}\left(2\alpha-2 \beta-\nicefrac{{1}}{{c_{k}}}\right)(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k}))\] \[+\ \gamma_{k}(2\beta+\nicefrac{{1}}{{c_{k}}})(f_{i_{k}}(x_{p}^{k}) -f_{i_{k}}^{*}).\]

Let us divide both sides by \(\gamma_{k}>0\)

\[\frac{\mathrm{dist}(x^{k+1},\mathcal{S})^{2}}{\gamma_{k}} \leq \frac{\mathrm{dist}(x^{k},\mathcal{S})^{2}}{\gamma_{k}}-\left(2 \alpha-2\beta-\nicefrac{{1}}{{c_{k}}}\right)(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^ {k}))\] \[+\ (2\beta+\nicefrac{{1}}{{c_{k}}})(f_{i_{k}}(x_{p}^{k})-f_{i_{k} }^{*}).\]

Since by hypothesis \(c_{k}\geq\frac{1}{\alpha-\beta}\), we get \(2\alpha-2\beta-\nicefrac{{1}}{{c_{k}}}\geq\alpha-\beta\). Therefore, we get

\[f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k}) \leq \frac{\mathrm{dist}(x^{k},\mathcal{S})^{2}}{(\alpha-\beta)\gamma_ {k}}-\frac{\mathrm{dist}(x^{k+1},\mathcal{S})^{2}}{(\alpha-\beta)\gamma_{k}}+ \frac{(2\beta+\nicefrac{{1}}{{c_{k}}})}{\alpha-\beta}(f_{i_{k}}(x_{p}^{k})-f_ {i_{k}}^{*}).\]

Summing from \(k=0\) to \(K-1\) we get

\[\sum_{k=0}^{K-1}f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k}) \leq \sum_{k=0}^{K-1}\frac{\mathrm{dist}(x^{k},\mathcal{S})^{2}}{( \alpha-\beta)\gamma_{k}}-\sum_{k=0}^{K-1}\frac{\mathrm{dist}(x^{k+1}, \mathcal{S})^{2}}{(\alpha-\beta)\gamma_{k}}\] \[+\ \sum_{k=0}^{K-1}\frac{2\beta}{\alpha-\beta}(f_{i_{k}}(x_{p}^ {k})-f_{i_{k}}^{*})+\sum_{k=0}^{K-1}\frac{1}{(\alpha-\beta)c_{k}}(f_{i_{k}}(x_ {p}^{k})-f_{i_{k}}^{*})\] \[\leq \frac{\mathrm{dist}(x^{0},\mathcal{S})^{2}}{(\alpha-\beta)\gamma _{0}}+\sum_{k=1}^{K-1}\frac{\mathrm{dist}(x^{k},\mathcal{S})^{2}}{(\alpha- \beta)\gamma_{k}}-\sum_{k=0}^{K-2}\frac{\mathrm{dist}(x^{k+1},\mathcal{S})^{2 }}{(\alpha-\beta)\gamma_{k}}\] \[-\ \frac{\mathrm{dist}(x^{K},\mathcal{S})^{2}}{(\alpha-\beta) \gamma_{K-2}}+\sum_{k=0}^{K-1}\frac{2\beta}{\alpha-\beta}(f_{i_{k}}(x_{p}^{k} )-f_{i_{k}}^{*})\] \[+\ \sum_{k=0}^{K-1}\frac{1}{(\alpha-\beta)c_{k}}(f_{i_{k}}(x_{p}^ {k})-f_{i_{k}}^{*}).\]Here we use the fact that \(\gamma_{k}\) is a non-increasing sequence with \(k\). We continue as follows

\[\sum_{k=0}^{K-1}f_{i_{k}}(x^{k})-f_{i_{k}}(x^{k}_{p}) \leq \frac{\mathrm{dist}(x^{0},\mathcal{S})^{2}}{(\alpha-\beta)\gamma_{ 0}}+\sum_{k=0}^{K-2}\left(\frac{1}{\gamma_{k+1}}-\frac{1}{\gamma_{k}}\right) \frac{\mathrm{dist}(x^{k+1},\mathcal{S})^{2}}{\alpha-\beta}\] \[+\ \sum_{k=0}^{K-1}\frac{2\beta}{\alpha-\beta}(f_{i_{k}}(x^{k}_{p })-f^{*}_{i_{k}})+\sum_{k=0}^{K-1}\frac{1}{(\alpha-\beta)c_{k}}(f_{i_{k}}(x^{k }_{p})-f^{*}_{i_{k}})\] \[\leq \frac{D^{2}}{\alpha-\beta}\left(\frac{1}{\gamma_{0}}+\sum_{k=0}^{ K-2}\left(\frac{1}{\gamma_{k+1}}-\frac{1}{\gamma_{k}}\right)\right)\] \[+\ \sum_{k=0}^{K-1}\frac{2\beta}{\alpha-\beta}(f_{i_{k}}(x^{k}_{p })-f^{*}_{i_{k}})+\sum_{k=0}^{K-1}\frac{1}{(\alpha-\beta)c_{k}}(f_{i_{k}}(x^{k }_{p})-f^{*}_{i_{k}})\] \[\leq \frac{D^{2}}{(\alpha-\beta)\gamma_{K-1}}+\sum_{k=0}^{K-1}\frac{2 \beta}{\alpha-\beta}(f_{i_{k}}(x^{k}_{p})-f^{*}_{i_{k}})\] \[+\ \sum_{k=0}^{K-1}\frac{1}{(\alpha-\beta)c_{k}}(f_{i_{k}}(x^{k}_ {p})-f^{*}_{i_{k}}).\]

Here we use the fact that \(\nicefrac{{1}}{{\gamma_{k+1}}}-\nicefrac{{1}}{{\gamma_{k}}}\geq 0\). From Lemma 3 we have

\[\frac{1}{\gamma_{k}}\leq c_{k}\underbrace{\max\left\{2L,\frac{1}{c_{0}\gamma_{k }}\right\}}_{=\widetilde{L}}.\]

Therefore, we continue as follows

\[\frac{1}{K}\sum_{k=0}^{K-1}f_{i_{k}}(x^{k})-f_{i_{k}}(x^{k}_{p}) \leq \frac{2\widetilde{L}D^{2}c_{K-1}}{K(\alpha-\beta)}+\frac{1}{K} \sum_{k=0}^{K-1}\frac{2\beta}{\alpha-\beta}(f_{i_{k}}(x^{k}_{p})-f^{*}_{i_{k}})\] \[+\ \frac{1}{K}\sum_{k=0}^{K-1}\frac{1}{(\alpha-\beta)c_{k}}(f_{i _{k}}(x^{k}_{p})-f^{*}_{i_{k}}).\]

Taking expectation we get

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{2\widetilde{L}D^{2}c_{K-1}}{K(\alpha-\beta)}+\frac{2\beta} {\alpha-\beta}\sigma_{\mathrm{int}}^{2}+\frac{1}{K}\sum_{k=0}^{K-1}\frac{ \sigma_{\mathrm{int}}^{2}}{(\alpha-\beta)c_{k}}.\]

**Corollary 1**.: Let \(c_{k}=\sqrt{k+1}\) with \(c_{-1}=c_{0}\), then iterates of DecSPS satisfy

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{2\widetilde{L}D^{2}+2\sigma_{\mathrm{int}}^{2}}{\sqrt{K}( \alpha-\beta)}+\frac{2\beta}{\alpha-\beta}\sigma_{\mathrm{int}}^{2} \tag{30}\] \[= \widetilde{\mathcal{O}}\left(\frac{1}{\sqrt{K}}+\beta\sigma_{ \mathrm{int}}^{2}\right). \tag{31}\]

Proof.: The proof directly follows from Theorem 4 using the choice \(c_{k}=\sqrt{k+1}\) and the fact that \(\sum_{k=0}^{K-1}k^{-1/2}\leq 2\sqrt{K}\). 

**Remark 4**.: It turned out that removing the bounded iterates assumption for DecSPS is a challenging task. Nevertheless, we believe that this is rather the technicalities of Polyak stepsize, but not of our assumption. Moreover, we highlight that [64] removed bounded iterates assumption in restrictive strongly convex setting.

#### c.2.3 Convergence of NGN

Constant stepsize parameter.In this section we present the proof of convergence of NGN with constant stepsize parameter \(\gamma\) under \(\alpha\)-\(\beta\)-condition for completeness of the presentation.

**Lemma 4** (Lemma from [63]).: Let \(f_{i}\) be \(L\)-smooth for all \(i\), then the stepsize of NGN satisfies

\[\gamma_{k}\in\left[\frac{\gamma}{1+\gamma L},\gamma\right]. \tag{32}\]

**Lemma 5** (Lemma from [63]).: Let \(f_{i}\) be \(L\)-smooth for all \(i\), then the iterates of NGN satisfy

\[\gamma_{k}^{2}\|\nabla f_{i_{k}}(x^{k})\|^{2}\leq\frac{4\gamma L}{1+2\gamma L} \gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})+\frac{2\gamma^{2}L}{1+\gamma L}\max \left\{\frac{2\gamma L-1}{2\gamma L+1},0\right\}f_{i_{k}}^{*}. \tag{33}\]

**Theorem 3**.: _Assume that Assumptions 1 with \(\alpha\geq\beta+1\) and 1-2 hold. Assume that each function \(f_{i}\) is positive and \(\sigma_{\mathrm{pos}}^{2}<\infty\). Then the iterates of NGN (Alg. 3) with a stepsize parameter \(\gamma>0\) satisfy_

\[\min_{0\leq k\leq K-1}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{\mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right] }{2\gamma K}\frac{(1+2\gamma L)^{2}}{c_{2}}+\frac{3L\gamma\alpha(1+\gamma L) \sigma_{\mathrm{int}}^{2}}{c_{2}} \tag{11}\] \[+\ \frac{\gamma L}{a}\max\left\{2\gamma L-1,0\right\}\sigma_{ \mathrm{pos}}^{2}+\frac{2\beta\sigma_{\mathrm{int}}^{2}}{c_{2}},\]

_where \(c_{2}\coloneqq 2\gamma L(\alpha-\beta-1)+\alpha-\beta\)._

Proof.: Let \(x_{p}^{k}\in\mathrm{Proj}(x^{k},\mathcal{S})\) satisfying Definition 1. Then we have

\[\mathbb{E}_{k}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2}\right] \leq \mathbb{E}_{k}\left[\|x^{k+1}-x_{p}^{k}\|^{2}\right] \tag{34}\] \[= \|x^{k}-x_{p}^{k}\|^{2}-2\mathbb{E}_{k}\left[\gamma_{k}(\nabla f_ {i_{k}}(x^{k}),x^{k}-x_{p}^{k})\right]+\mathbb{E}_{k}\left[\gamma_{k}^{2}\| \nabla f_{i_{k}}(x^{k})\|^{2}\right]\] \[\leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\mathbb{E}_{k}\left[ \gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k}))\right]\] \[+\ 2\beta\mathbb{E}_{k}\left[\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k }}^{*})\right]+\mathbb{E}_{k}\left[\gamma_{k}^{2}\|\nabla f_{i_{k}}(x^{k})\|^ {2}\right].\]

From Lemma 5

\[\gamma_{k}^{2}\|\nabla f_{i_{k}}(x^{k})\|^{2}\leq\frac{4\gamma L}{1+2\gamma L} \gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})+\frac{2\gamma^{2}L}{1+\gamma L}\max \left\{\frac{2\gamma L-1}{2\gamma L+1},0\right\}f_{i_{k}}^{*}. \tag{35}\]

Plugging (35) in (34) we get

\[\mathbb{E}_{k}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2}\right] \leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\mathbb{E}_{k}\left[ \gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k}))\right]\] \[+\ 2\beta\mathbb{E}_{k}\left[\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k }}^{*})\right]\frac{4\gamma L}{1+2\gamma L}+\mathbb{E}_{k}\left[\gamma_{k}(f_{ i_{k}}(x^{k})-f_{i_{k}}^{*})\right]\] \[+\ \frac{2\gamma^{2}L}{1+\gamma L}\max\left\{\frac{2\gamma L-1}{2 \gamma L+1},0\right\}\mathbb{E}_{k}\left[f_{i_{k}}^{*}\right].\]We have \(f_{i_{k}}(x^{k})-f_{i_{k}}(x^{k}_{p})=(f_{i_{k}}(x^{k})-f^{*}_{i_{k}})-(f_{i_{k}}(x ^{k}_{p})-f^{*}_{i_{k}})\). Now we write \(\gamma_{k}=\rho+\epsilon_{k}\) where \(\rho\) is a constant independent of \(i_{k}\). Therefore, we have

\[\mathbb{E}_{k}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2}\right] \leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\rho\mathbb{E}_{k} \left[f_{i_{k}}(x^{k})-f_{i_{k}}(x^{k}_{p})\right] \tag{36}\] \[-2\alpha\mathbb{E}_{k}\left[\epsilon_{k}(f_{i_{k}}(x^{k})-f^{*}_{ i_{k}})\right]+2\alpha\mathbb{E}_{k}\left[\epsilon_{k}(f_{i_{k}}(x^{k}_{p})-f^{*}_{ i_{k}})\right]\] \[+2\beta\mathbb{E}_{k}\left[\gamma_{k}(f_{i_{k}}(x^{k})-f^{*}_{i_{ k}})\right]+\frac{4\gamma L}{1+2\gamma L}\mathbb{E}_{k}\left[\gamma_{k}(f_{i_{ k}}(x^{k})-f^{*}_{i_{k}})\right]\] \[+\left.\frac{2\gamma^{2}L}{1+\gamma L}\max\left\{\frac{2\gamma L- 1}{2\gamma L+1},0\right\}\mathbb{E}_{k}\left[f^{*}_{i_{k}}\right]\] \[= \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\rho\mathbb{E}_{k} \left[f_{i_{k}}(x^{k})-f_{i_{k}}(x^{k}_{p})\right]\] \[-2\mathbb{E}_{k}\left[\left(\alpha\epsilon_{k}-\left(\beta+\frac{ 2\gamma L}{1+2\gamma L}\right)\gamma_{k}\right)(f_{i_{k}}(x^{k})-f^{*}_{i_{k} })\right]\] \[+2\alpha\mathbb{E}_{k}\left[\epsilon_{k}(f_{i_{k}}(x^{k}_{p})-f^{ *}_{i_{k}})\right]\] \[+\left.\frac{2\gamma^{2}L}{1+\gamma L}\max\left\{\frac{2\gamma L- 1}{2\gamma L+1},0\right\}\mathbb{E}_{k}\left[f^{*}_{i_{k}}\right].\]

We need to find \(\rho\) such that

\[\alpha\epsilon_{k}-\left(\beta+\frac{2\gamma L}{1+2\gamma L} \right)\gamma_{k}\geq 0\] \[\alpha(\gamma_{k}-\rho)-\left(\beta+\frac{2\gamma L}{1+2\gamma L} \right)\gamma_{k}\geq 0\] \[\gamma_{k}\left(\alpha-\beta-\frac{2\gamma L}{1+2\gamma L}\right) \geq\alpha\rho.\]

Note that since \(\gamma_{k}\geq\frac{\gamma}{1+\gamma L}\) from Lemma 4, then it is enough if \(\rho\) satisfies

\[\frac{\gamma}{1+\gamma L}\left(\alpha-\beta-\frac{2\gamma L}{1+2 \gamma L}\right)\geq\alpha\rho\] \[\frac{\gamma(2\gamma L(\alpha-\beta-1)+\alpha-\beta)}{\alpha(1+ \gamma L)(1+2\gamma L)}\geq\rho.\]

Let us take this bound as a value of \(\rho\). Note that since \(\alpha\geq\beta+1\), then \(\rho\geq 0\). Therefore, the bound for \(\epsilon_{k}\) is the following

\[\epsilon_{k} = \gamma_{k}-\rho\] \[\leq \gamma-\frac{\gamma(2\gamma L(\alpha-\beta-1)+\alpha-\beta)}{ \alpha(1+\gamma L)(1+2\gamma L)}\] \[= \gamma\left(\frac{\alpha(1+\gamma L)(1+2\gamma L)-2\gamma L( \alpha-\beta-1)-(\alpha-\beta)}{\alpha(1+\gamma L)(1+2\gamma L)}\right)\] \[= L\gamma^{2}\frac{\alpha+2\alpha\gamma L+2(\beta+1)}{\alpha(1+ \gamma L)(1+2\gamma L)}+\gamma\frac{\beta}{\alpha(1+\gamma L)(1+2\gamma L)}\] \[\leq \underbrace{\frac{3L\gamma^{2}}{1+2\gamma L}}_{:=\frac{1}{2}T_{2 }(\gamma^{2})}+\frac{\beta\gamma}{\alpha(1+\gamma L)(1+2\gamma L)}_{:=\frac{ 1}{2}\beta T_{1}(\gamma)}.\]Therefore, we get the following descent inequality

\[\mathbb{E}_{k}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2}\right] \leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-\underbrace{2\alpha\rho}_{=T_ {0}(\gamma)}\ \mathbb{E}_{k}\left[f_{i_{k}}(x^{k})-f_{i_{k}}(x^{k}_{p})\right] \tag{37}\] \[+\ 2\alpha\mathbb{E}_{k}\left[\epsilon_{k}(f_{i_{k}}(x^{k}_{p})- f^{*}_{i_{k}})\right]+\underbrace{\frac{2\gamma^{2}L}{1+\gamma L}\max\left\{\frac{2 \gamma L-1}{2\gamma L+1},0\right\}}_{=T_{3}(\gamma^{2})}\mathbb{E}_{k}\left[f^{ *}_{i_{k}}\right]\] \[\leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-T_{0}(\gamma)\mathbb{E}_{k} \left[f_{i_{k}}(x^{k})-f_{i_{k}}(x^{k}_{p})\right]\] \[+\ T_{2}(\gamma^{2})\alpha\mathbb{E}_{k}\left[f_{i_{k}}(x^{k}_{p} )-f^{*}_{i_{k}}\right]+\beta T_{1}(\gamma)\alpha\mathbb{E}_{k}\left[f_{i_{k}}( x^{k}_{p})-f^{*}_{i_{k}}\right]\] \[+\ T_{3}(\gamma^{2})\mathbb{E}_{k}\left[f^{*}_{i_{k}}\right]\] \[= \mathrm{dist}(x^{k},\mathcal{S})^{2}-T_{0}(\gamma)(f(x^{k})-f^{*} )+T_{2}(\gamma^{2})\alpha\mathbb{E}_{k}\left[f^{*}-f^{*}_{i_{k}}\right]\] \[+\ \beta T_{1}(\gamma)\alpha\mathbb{E}_{k}\left[f^{*}-f^{*}_{i_{k }}\right]+T_{3}(\gamma^{2})\mathbb{E}_{k}\left[f^{*}_{i_{k}}\right].\]

Here we use the fact that \(x^{k}_{p}\) is a minimizer of \(f\) and independent of \(i_{k}\). Unrolling the recursion, we get

\[\frac{1}{K}\left(\mathbb{E}\left[\mathrm{dist}(x^{K},\mathcal{S} )^{2}\right]-\mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right]\right) \leq -\frac{T_{0}(\gamma)}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[f(x_{k})- f^{*}\right] \tag{38}\] \[+\ E_{1}(\gamma)+E_{2}(\gamma^{2}),\]

where

\[E_{1}(\gamma)\coloneqq\beta T_{1}(\gamma)\alpha\underbrace{\mathbb{E}\left[f^{ *}-f^{*}_{i}\right]}_{\sigma^{2}_{\mathrm{int}}},\quad E_{2}(\gamma^{2}) \coloneqq T_{2}(\gamma^{2})\alpha\underbrace{\mathbb{E}\left[f^{*}-f^{*}_{i} \right]}_{\sigma^{2}_{\mathrm{int}}}+T_{3}(\gamma^{2})\underbrace{\mathbb{E} \left[f^{*}_{i}\right]}_{\sigma^{2}_{\mathrm{pos}}}. \tag{39}\]

Therefore, we get

\[\min_{0\leq k\leq K-1}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{\mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right] }{KT_{0}(\gamma)}+\frac{E_{1}(\gamma)}{T_{0}(\gamma)}+\frac{E_{2}(\gamma^{2}) }{T_{0}(\gamma)} \tag{40}\] \[\leq \frac{\mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right] }{2\gamma K}\frac{(1+2\gamma L)^{2}}{2\gamma L(\alpha-\beta-1)+\alpha-\beta}\] \[+\ \frac{3L\gamma\alpha}{2\gamma L(\alpha-\beta-1)+\alpha-\beta}(1 +\gamma L)\sigma^{2}_{\mathrm{int}}\] \[+\ \frac{\gamma L}{2\gamma L(\alpha-\beta-1)+\alpha-\beta}\max \left\{2\gamma L-1,0\right\}\sigma^{2}_{\mathrm{pos}}\] \[+\ \frac{\beta\sigma^{2}_{\mathrm{int}}}{2\gamma L(\alpha-\beta-1)+ \alpha-\beta}.\]

This finished the proof. 

**Remark 5**.: If all \(f_{i}\) are convex, i.e. we can take \(\alpha=1,\beta=0\) in \(\alpha\)-\(\beta\)-condition, then we get

\[\min_{0\leq k\leq K-1}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{\mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right] }{2\gamma K}(1+2\gamma L)^{2}+3L\gamma(1+\gamma L)\sigma^{2}_{\mathrm{int}}\] \[+\ \gamma L\max\left\{2\gamma L-1,0\right\}\sigma^{2}_{\mathrm{pos}},\]

that coincides with the results in [63].

Decreasing stepsize parameter.Now we present the results with decreasing stepsize parameter.

**Theorem 6**.: _Assume that Assumptions 1 with \(\alpha\geq\beta+1\) and 1-2 hold. Assume that each function \(f_{i}\) is positive and \(\sigma^{2}_{\mathrm{pos}}<\infty\). Then the iterates of_ NGN _with decreasing stepsize of the form_

\[\gamma_{k}=\frac{\widetilde{\gamma}_{k}}{1+\frac{\widetilde{\gamma}_{k}}{2f_{i_{ k}}(x^{k})}\|\nabla f_{i_{k}}(x^{k})\|^{2}},\quad\widetilde{\gamma}_{k} \coloneqq\frac{\gamma}{\sqrt{k+1}}\]satisfy_

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{C_{1}}{\sqrt{K}}\mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S })^{2}\right]+\frac{C_{2}\log(K+1)}{\sqrt{K}}\alpha\sigma_{\mathrm{int}}^{2}+ \frac{C_{3}}{\alpha-\beta}\beta\sigma_{\mathrm{int}}^{2} \tag{41}\] \[+\ \frac{C_{4}\log(K+1)}{\sqrt{K}}\sigma_{\mathrm{pos}}^{2}\] \[= \widetilde{\mathcal{O}}\left(\frac{1}{\sqrt{K}}+\beta\sigma_{ \mathrm{int}}^{2}\right). \tag{42}\]

_where \(C_{1},C_{2},C_{3}\), and \(C_{4}\) are defined in (43)._

Proof.: Similarly to the proof with constant stepsize parameter we can obtain (similar to (37))

\[\mathbb{E}_{k}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2}\right] \leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-T_{0}(\widetilde{\gamma}_{k} )(f(x^{k})-f^{*})+T_{2}(\widetilde{\gamma}_{k}^{2})\alpha\sigma_{\mathrm{int}} ^{2}\] \[+\ \beta T_{1}(\widetilde{\gamma}_{k})\alpha\sigma_{\mathrm{int}} ^{2}+T_{3}(\widetilde{\gamma}_{k}^{2})\sigma_{\mathrm{pos}}^{2}.\]

Note that we have

\[T_{0}(\widetilde{\gamma}_{k}) = \frac{2\widetilde{\gamma}_{k}(2\widetilde{\gamma}_{k}L(\alpha- \beta-1)+\alpha-\beta)}{(1+\widetilde{\gamma}_{k}L)(1+2\widetilde{\gamma}_{k} L)}\] \[\geq \frac{2\widetilde{\gamma}_{k}(\alpha-\beta)}{(1+\widetilde{\gamma} _{0}L)(1+2\widetilde{\gamma}_{0}L)}=:\widetilde{T}_{0}(\widetilde{\gamma}_{k}),\] \[T_{1}(\widetilde{\gamma}_{k}) = \frac{2\widetilde{\gamma}_{k}}{\alpha(1+\widetilde{\gamma}_{k}L)( 1+2\widetilde{\gamma}_{k}L)}\] \[\leq \frac{2\widetilde{\gamma}_{k}}{\alpha}=:\widetilde{T}_{1}( \widetilde{\gamma}_{k}),\] \[T_{2}(\widetilde{\gamma}_{k}^{2}) = \frac{6L\widetilde{\gamma}_{k}^{2}}{1+2\widetilde{\gamma}_{k}L}\] \[\leq 6L\widetilde{\gamma}_{k}^{2}=:\widetilde{T}_{2}(\widetilde{\gamma }_{k}^{2}),\] \[T_{3}(\widetilde{\gamma}_{k}^{2}) = \frac{2\widetilde{\gamma}_{k}^{2}L}{1+\widetilde{\gamma}_{k}L} \max\left\{\frac{2\widetilde{\gamma}_{k}L-1}{2\widetilde{\gamma}_{k}L+1},0\right\}\] \[\leq 2\widetilde{\gamma}_{k}^{2}L\max\{2\gamma L-1,0\}=:\widetilde{T }_{3}(\widetilde{\gamma}_{k}^{2}).\]

Therefore, we can continue as follows

\[\mathbb{E}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2}\right] \leq \mathbb{E}\left[\mathrm{dist}(x^{k},\mathcal{S})^{2}\right]- \widetilde{T}_{0}(\widetilde{\gamma}_{k})\mathbb{E}\left[f(x^{k})-f^{*}\right] +\widetilde{T}_{2}(\widetilde{\gamma}_{k}^{2})\alpha\sigma_{\mathrm{int}}^{2}\] \[+\ \beta\widetilde{T}_{1}(\widetilde{\gamma}_{k})\alpha\sigma_{ \mathrm{int}}^{2}+\widetilde{T}_{3}(\widetilde{\gamma}_{k}^{2})\sigma_{ \mathrm{pos}}^{2}.\]

This leads to

\[\sum_{k=0}^{K-1}\widetilde{T}_{0}(\widetilde{\gamma}_{k})\mathbb{ E}\left[f(x^{k})-f^{*}\right] \leq \mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right]+ \sum_{k=0}^{K-1}\widetilde{T}_{2}(\widetilde{\gamma}_{k}^{2})\alpha\sigma_{ \mathrm{int}}^{2}+\sum_{k=0}^{K-1}\beta\widetilde{T}_{1}(\widetilde{\gamma}_{ k})\alpha\sigma_{\mathrm{int}}^{2}\] \[+\ \sum_{k=0}^{K-1}\widetilde{T}_{3}(\widetilde{\gamma}_{k}^{2}) \sigma_{\mathrm{pos}}^{2}.\]

Therefore, we have

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{1}{\sum_{k=0}^{K-1}\widetilde{T}_{0}(\widetilde{\gamma}_{ k})}\mathbb{E}\left[\mathrm{dist}(x^{0},\mathcal{S})^{2}\right]+\frac{\sum_{k=0}^{K-1} \widetilde{T}_{2}(\widetilde{\gamma}_{k}^{2})}{\sum_{k=0}^{K-1}\widetilde{T}_{ 0}(\widetilde{\gamma}_{k})}\alpha\sigma_{\mathrm{int}}^{2}\] \[+\ \frac{\beta\sum_{k=0}^{K-1}\widetilde{T}_{1}(\widetilde{\gamma }_{k})}{\sum_{k=0}^{K-1}\widetilde{T}_{0}(\widetilde{\gamma}_{k})}\alpha\sigma _{\mathrm{int}}^{2}+\frac{\sum_{k=0}^{K-1}\widetilde{T}_{3}(\widetilde{\gamma}_ {k}^{2})}{\sum_{k=0}^{K-1}\widetilde{T}_{0}(\widetilde{\gamma}_{k})}\sigma_{ \mathrm{pos}}^{2}.\]Following the results of [64] and [24] we get

\[\sum_{k=0}^{K-1}\widetilde{T}_{0}(\widetilde{\gamma}_{k}) = \sum_{k=0}^{K-1}\frac{2\widetilde{\gamma}_{k}}{(1+\gamma L)(1+2 \gamma L)}\] \[\geq \frac{8\gamma\sqrt{K}}{5(1+\gamma L)(1+2\gamma L)},\] \[\sum_{k=0}^{K-1}\widetilde{T}_{2}(\widetilde{\gamma}_{k}^{2}) = \sum_{k=0}^{K-1}6L\widetilde{\gamma}_{k}^{2}\] \[\leq 12L\gamma^{2}\log(K+1),\] \[\sum_{k=0}^{K-1}\widetilde{T}_{3}(\widetilde{\gamma}_{k}^{2}) = \sum_{k=0}^{K-1}2\widetilde{\gamma}_{k}^{2}L\max\{2\gamma L-1,0\}\] \[\leq 4\gamma^{2}L\max\{2\gamma L-1,0\}\log(K+1),\] \[\frac{\sum_{k=0}^{K-1}\widetilde{T}_{1}(\widetilde{\gamma}_{k})} {\sum_{k=0}^{K-1}\widetilde{T}_{0}(\widetilde{\gamma}_{k})} = \frac{\sum_{k=0}^{K-1}\frac{2\widetilde{\gamma}_{k}}{\alpha}}{ \frac{\sum_{k=0}^{K-1}\frac{2\widetilde{\gamma}_{k}}{(1+\gamma L)(1+2\gamma L) }}{(1+\gamma L)(1+2\gamma L)}}\] \[= \frac{(1+\gamma L)(1+2\gamma L)}{\alpha(\alpha-\beta)}.\]

Thus, the final result can be written as follows

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{5(1+\gamma L)(1+2\gamma L)}{8\gamma\sqrt{K}}\mathbb{E}\left[ \operatorname{dist}(x^{0},\mathcal{S})^{2}\right]+\frac{12L\gamma^{2}\log(K+1)} {\frac{8\gamma\sqrt{K}}{5(1+\gamma L)(1+2\gamma L)}}\alpha\sigma_{\text{int}}^ {2}\] \[+\ \frac{\sum_{k=0}^{K-1}\widetilde{T}_{1}(\widetilde{\gamma}_{k} )}{\sum_{k=0}^{K-1}\widetilde{T}_{0}(\widetilde{\gamma}_{k})}\alpha\sigma_{ \text{int}}^{2}+\frac{\sum_{k=0}^{K-1}\widetilde{T}_{3}(\widetilde{\gamma}_{k} ^{2})}{\sum_{k=0}^{K-1}\widetilde{T}_{0}(\widetilde{\gamma}_{k})}\sigma_{\text {pos}}^{2}\] \[= \frac{5(1+\gamma L)(1+2\gamma L)}{8\gamma\sqrt{K}}\mathbb{E}\left[ \operatorname{dist}(x^{0},\mathcal{S})^{2}\right]\] \[+\ \frac{15L\gamma(1+\gamma L)(1+2\gamma L)\log(K+1)}{2\sqrt{K}} \alpha\sigma_{\text{int}}^{2}\] \[+\ \frac{(1+\gamma L)(1+2\gamma L)}{(\alpha-\beta)}\beta\sigma_{ \text{int}}^{2}\] \[+\ \frac{5(1+\gamma L)(1+2\gamma L)\gamma L\max\{2\gamma L-1,0\} \log(K+1)}{2\sqrt{K}}\sigma_{\text{pos}}^{2}.\]

Now it is left to use the definitions of constants \(C_{1},C_{2},\) and \(C_{3}\)

\[C_{1}\coloneqq\frac{5(1+\gamma L)(1+2\gamma L)}{8\gamma}, \tag{43}\] \[C_{2}\coloneqq\frac{15L\gamma(1+\gamma L)(1+2\gamma L)}{2},\] \[C_{3}\coloneqq(1+\gamma L)(1+2\gamma L),\] \[C_{4}\coloneqq\frac{5(1+\gamma L)(1+2\gamma L)\gamma L\max\{2 \gamma L-1,0\}}{2}.\]

#### c.2.4 Convergence of \(\mathsf{AdaGrad}\)-norm-max

**Theorem 7**.: _Assume that Assumptions 1-2 hold. Assume that for all \(k\geq 0\) stochastic gradients satisfy \(\|\nabla f_{i_{k}}(x^{k})\|^{2}\leq G^{2}\) for some \(G>0\) and \(b_{-1}\geq\frac{2L\gamma}{\alpha-\beta}.\) Let \(D^{2}=\max_{i}\|\nabla f_{i}(x^{0})\|^{2}.\) Then the iterates of \(\mathsf{AdaGrad}\)-norm-max (Alg. 4) satisfy_

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{\mathrm{dist}(x^{k},\mathcal{S})^{2}}{\gamma K(\alpha-\beta) }\sqrt{b_{-1}^{2}+G^{2}K} \tag{44}\] \[+\ \frac{2\alpha}{(\alpha-\beta)KD^{2}}\sigma_{\mathrm{int}}^{2} \sqrt{b_{-1}^{2}+G^{2}K}\sqrt{b_{-1}^{2}+D^{2}(K+1)}\] \[= \mathcal{O}\left(\frac{1}{\sqrt{K}}+\alpha\sigma_{\mathrm{int}}^{2 }\right).\]

We see that if \(K\) is large enough, then we recover the standard convex rate of Adagrad of order \(\widetilde{\mathcal{O}}(K^{-1/2})\)[46].

Proof.: Let \(x_{p}^{k}=\mathrm{Proj}(x^{k},\mathcal{S})\) satysfying Definition 1. Then we have

\[\mathrm{dist}(x^{k+1},\mathcal{S})^{2} \leq \|x^{k+1}-x_{p}^{k}\|^{2} \tag{45}\] \[= \|x^{k}-x_{p}^{k}\|^{2}-2\gamma_{k}\langle\nabla f_{i_{k}}(x^{k} ),x^{k}-x_{p}^{k}\rangle+\gamma_{k}^{2}\|\nabla f_{i_{k}}(x^{k})\|^{2}\] \[\leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\gamma_{k}(f_{i_{k}}( x^{k})-f_{i_{k}}(x_{p}^{k}))+2\beta\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})\] \[+\ \gamma_{k}^{2}\|\nabla f_{i_{k}}(x^{k})\|^{2}\] \[\leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\gamma_{k}(f_{i_{k}}( x^{k})-f_{i_{k}}(x_{p}^{k}))+2\beta\gamma_{k}(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})\] \[+\ 2\gamma_{k}^{2}L(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})\] \[= \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\alpha\gamma_{k}(f_{i_{k}}( x^{k})-f_{i_{k}}^{*}+f_{i_{k}}^{*}-f_{i_{k}}(x_{p}^{k}))+2\beta\gamma_{k}(f_{i_{k}} (x^{k})-f_{i_{k}}^{*})\] \[+\ 2\gamma_{k}^{2}L(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})\] \[= \mathrm{dist}(x^{k},\mathcal{S})^{2}-2\gamma_{k}(\alpha-\beta-L \gamma_{k})(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})\] \[+\ 2\alpha\gamma_{k}(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}).\]

Note that we choose \(b_{-1}\geq\frac{2L\gamma}{\alpha-\beta}\). Since \(b_{k}\) is increasing sequence, then we have for any \(k\) that \(b_{k}\geq\frac{2L\gamma}{\alpha-\beta}\). Therefore,

\[L\gamma_{k}=L\frac{\gamma}{b_{k}}\leq L\frac{\gamma}{b_{-1}}\leq L\frac{\gamma} {2L\gamma/\alpha-\beta}=\frac{\alpha-\beta}{2}.\]

This means that

\[-2\gamma_{k}(\alpha-\beta-L\gamma_{k})\leq-2\gamma_{k}(\alpha-\beta-\nicefrac {{\alpha}}{{2}})=-\gamma_{k}(\alpha-\beta).\]

Thus, we can continue (45) as follows

\[\mathrm{dist}(x^{k+1},\mathcal{S})^{2} \leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-\frac{\gamma}{b_{k}}(\alpha- \beta)(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})+2\alpha\frac{\gamma}{b_{k}}(f_{i_{k}}(x ^{k}_{p})-f_{i_{k}}^{*}).\]We know that \(b_{k}\) is increasing sequence that satisfy

\[\sqrt{b_{-1}^{2}+D^{2}(k+1)}\leq b_{k}\leq b_{K-1}\leq\sqrt{b_{-1}^{2}+G^{2}K}.\]

This leads together with the fact that both \(f_{i_{k}}(x^{k})-f_{i_{k}}^{*}\) and \(f_{i_{k}}(x_{p}^{k})-f_{i_{k}}^{*}\) are non-negative to

\[\mathrm{dist}(x^{k+1},\mathcal{S})^{2} \leq \mathrm{dist}(x^{k},\mathcal{S})^{2}-\frac{\gamma}{\sqrt{b_{-1}^{ 2}+G^{2}K}}(\alpha-\beta)(f_{i_{k}}(x^{k})-f_{i_{k}}^{*})\] \[+\ 2\alpha\frac{\gamma}{\sqrt{b_{-1}^{2}+D^{2}(k+1)}}(f_{i_{k}}( x_{p}^{k})-f_{i_{k}}^{*})\] \[= \mathrm{dist}(x^{k},\mathcal{S})^{2}-\frac{\gamma(\alpha-\beta)}{ \sqrt{b_{-1}^{2}+G^{2}K}}(f_{i_{k}}(x^{k})-f_{i_{k}}(x_{p}^{k}))\] \[+\ \frac{2\alpha\gamma}{\sqrt{b_{-1}^{2}+D^{2}(k+1)}}(f_{i_{k}}( x_{p}^{k})-f_{i_{k}}^{*}).\]

Taking the conditional expectation we get

\[\frac{\mathbb{E}_{k}\left[f(x^{k})-f^{*}\right]}{\sqrt{b_{-1}^{2}+ G^{2}K}} \leq \frac{\mathrm{dist}(x^{k},\mathcal{S})^{2}}{\gamma(\alpha-\beta)}- \frac{\mathbb{E}_{k}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2}\right]}{\gamma (\alpha-\beta)}+\frac{2\alpha}{\sqrt{b_{-1}^{2}+D^{2}(k+1)}(\alpha-\beta)} \sigma_{\mathrm{int}}^{2},\]

which leads to the following bound

\[\mathbb{E}_{k}\left[f(x^{k})-f^{*}\right] \leq \left(\frac{\mathrm{dist}(x^{k},\mathcal{S})^{2}}{\gamma(\alpha- \beta)}-\frac{\mathbb{E}_{k}\left[\mathrm{dist}(x^{k+1},\mathcal{S})^{2} \right]}{\gamma(\alpha-\beta)}\right)\sqrt{b_{-1}^{2}+G^{2}K}\] \[+\ \frac{2\alpha}{\sqrt{b_{-1}^{2}+D^{2}(k+1)}(\alpha-\beta)} \sigma_{\mathrm{int}}^{2}\sqrt{b_{-1}^{2}+G^{2}K}.\]

Averaging over \(K\) iterations we get

\[\min_{0\leq k<K}\mathbb{E}\left[f(x^{k})-f^{*}\right] \leq \frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[f(x^{k})-f^{*}\right]\] \[\leq \frac{\mathrm{dist}(x^{k},\mathcal{S})^{2}}{\gamma K(\alpha-\beta) }\sqrt{b_{-1}^{2}+G^{2}K}\] \[+\ \frac{2\alpha}{(\alpha-\beta)K}\sigma_{\mathrm{int}}^{2}\sqrt {b_{-1}^{2}+G^{2}K}\sum_{k=0}^{K-1}\frac{1}{\sqrt{b_{-1}^{2}+D^{2}(k+1)}}\] \[\leq \frac{\mathrm{dist}(x^{k},\mathcal{S})^{2}}{\gamma K(\alpha-\beta )}\sqrt{b_{-1}^{2}+G^{2}K}\] \[+\ \frac{2\alpha}{(\alpha-\beta)KD^{2}}\sigma_{\mathrm{int}}^{2} \sqrt{b_{-1}^{2}+G^{2}K}\sqrt{b_{-1}^{2}+D^{2}(K+1)}\] \[= \mathcal{O}\left(\frac{1}{\sqrt{K}}+\alpha\sigma_{\mathrm{int}}^ {2}\right).\]

## Appendix D Additional experiments

For all the experiments, we make use of PyTorch [65] package. LSTM, MLP, CNN and Resnet experiments are performed using one NVIDIA GeForce RTX \(3090\) GPU with a memory of \(24\) GB. For training Algoperf and Pythia language models, we resort instead to 4xA100-SXM4 GPUs, with a memory of \(40\) GB each, and employ data parallelism for efficient distributed training. When necessary, we disable CUDA non-deterministic operations, to allow consistency between runs with the same random seed.

### Half space learning

Half Space Learning problem corresponds to the following optimization problem

\[f(x)=\frac{1}{n}\sum_{i=1}^{n}\sigma(-b_{i}x^{\top}a_{i})+\frac{\lambda}{2}\|x\|^ {2},\]

where \(\{a_{i},b_{i}\}_{i=1}^{n},a_{i}\in\mathbb{R}^{d},y_{i}\in\{0,1\}\) is a given dataset, \(\lambda=10^{-5},\) and \(\sigma\) is a sigmoid function. For the test, we create a synthetic dataset that contains \(20\) samples for both classes. We sample data points from normal distribution where each class has its own mean and variance value of \(2\). We use SGD with learning rate \(\frac{1}{4}\) and batch size \(1\) for minimization task.5

Footnote 5: We use the implementation from [17] that can be found [https://github.com/archana95in/Escaping-saddles-using-Stochastic-Gradient-Descent](https://github.com/archana95in/Escaping-saddles-using-Stochastic-Gradient-Descent).

We observe that the gradient norm becomes zero quite which means that SGD trajectory goes through saddle point. This leads to possible negative angle between full gradient and direction to minimizer. Nevertheless, we demonstrate that even for such highly non-convex landscape with many saddle points our \(\alpha\)-\(\beta\)-condition holds for large enough values of \(\alpha\) and \(\beta\).

### Experiment setup from section 2.2

We use 3 layer LSTM based model from Hubler et al. [33]6. The model for PTB dataset has \(35441600\) parameters while for Wikitext-2 dataset the model has \(44798534\) parameters. To train the model, we choose NSGD with momentum [15] with decaying stepsize and momentum parameters according to the experiment section from [33]. We train the model for \(1000\) epochs with initial stepsize values \(158\) and \(900\) for PTB and Wikitext-2 datasets respectively. We switch off dropout during measuring stochastic gradients and losses for \(\alpha\)-\(\beta\)-condition. We run the experiments for \(7\) different random seeds and plot the mean with maximum and minimum fluctuations.

Footnote 6: The implementation can be found following the link [https://github.com/fhueb/parameter-agnostic-lzlo](https://github.com/fhueb/parameter-agnostic-lzlo)

In Figure 10, we plot empirical aiming coefficient \(\frac{\langle\nabla f(x^{k}),x^{k}-x^{K}\rangle}{f(x^{k})}\) for full loss \(f\); mean of stochastic losses across \(7\) runs along with maximum and minimum fluctuations from the mean; pairs of \((\alpha,\beta)\) that satisfy \(\alpha\)-\(\beta\)-condition across all \(7\) runs. We observe that for both datasets, there is a plateau at the beginning of the training. This part of the training corresponds to possible negative values of the empirical coefficient Aiming condition. After this, it becomes stable and positive.

Besides, we demonstrate that possible values of pairs of \((\alpha,\beta)\) that satisfy \(\alpha\)-\(\beta\)-condition are large. We believe, this happens because the full loss \(f\) has a minimum value of around \(3.5\) while individual losses have \(f_{i}^{*}\), i.e. the model is far from the interpolation regime (when \(f^{*}=f_{i}^{*}\)).

### MLP architecture

We use MLP model with \(3\) fully connected layers. We fix the dimensions of the second layer to be equal, i.e. the parameter matrix of this layer is square. After the first and second fully connected layers we use ReLU activation function. We train the model in all cases with fixed learning rate \(0.09\) for \(1500\) epochs and batch size \(64\) on Fashion-MNIST [83] dataset. In Figure 12, we plot themean stochastic loss across \(4\) runs along with maximum and minimum fluctuations, and in Figure 11 possible values of \(\alpha\) and \(\beta\) that work over all random seeds and iterations satisfying \(\alpha\geq\beta+0.1\).

We observe that the magnitude of the smallest possible values of \(\alpha\) and \(\beta\) increase till up to the second layer size \(512\), but then it starts decreasing as the model becomes more over-parameterized. This leads to smaller values of neighborhood \(\mathcal{O}(\beta\sigma_{\mathrm{int}}^{2})\) as we expect in this setting.

### CNN architecture

We use CNN model with \(2\) convolution layers followed by a fully connected one. After each convolution layer, we use max-pooling and ReLU activation functions. We train the model with a cosine annealing learning rate scheduler with a maximum value \(0.01\) and batch size \(64\). We train the model on CIFAR10 dataset [41] for \(1500\) epochs. We run the experiments for \(4\) different random seeds. In Figure 13 we plot possible values of \(\alpha\) and \(\beta\) satisfying \(\alpha\geq\beta+0.1\) that work across all runs and iterations.

We observe that increasing the dimension of the second layer of the model makes the model closer to over-parameterization: values of stochastic losses decrease. We observe the same phenomenon as in Appendix D.3: minimum possible values of \(\alpha\) and \(\beta\) increase up to \(128\) number of convolutions, but then it decreases for a larger number of convolutions. This happens because the model becomes more over-parameterized. Moreover, the possible difference between \(\alpha\) and \(\beta\) tends to increase with number of convolutions starting from \(128\) convolutions.

### Resnet architecture

We use the implementation from Kumar [42]. We train the model on CIFAR100 dataset [41] for \(1000\) epochs. We use one-cycle scheduler with a maximum learning rate \(0.01\). To compute dense stochastic gradients, we switch off dropout during evaluations of stochastic gradients and losses for \(\alpha\)-\(\beta\)-condition. We run the experiments for \(4\) different random seeds and plot the mean along with maximum and minimum fluctuations.

In Figure 15 we observe that the minimum value of stochastic loss increases with batch size which means that the model becomes further from over-parameterization.

### Verification of \(\alpha\)-\(\beta\)-condition by different optimizers

Now we turn to another interesting question: how does the choice of an optimizer affect the practical verification of the \(\alpha\)-\(\beta\)-condition? To explore this question, we train Resnet9 model with SGD, SGDM, and Adam. We report the results in Figure 16 varying the batch size used in the training. Comparing the values of \(\alpha\) and \(\beta\) for SGD (from Figure 6), SGDM, and Adam, we observe that the loss landscape explored by the Adam optimizer achieves smaller values of \(\alpha\) and \(\beta\). Moreover, the values of \(\alpha\) and \(\beta\) found by SGDM are typically smaller than those found by SGD. This result may shed light on why momentum (from a comparison of SGDM against SGD) and adaptive stepsize (from a comparison of SGDM against Adam) are typically beneficial in practice: these more advanced algorithms explore better part of a loss landscape from the \(\alpha\)-\(\beta\)-condition point of view.

Figure 12: Values of stochastic loss during the training of \(3\) layer MLP model on Fashion-MNIST dataset varying the dimension of the second layer.

### Increasing the depth of Resnet architecture

In our next experiment, we aim to investigate how the \(\alpha\)-\(\beta\)-condition behaves increasing the depth of a model. To do so, in addition to training Resnet9 model, we test Resnet18 and Resnet34 models on CIFAR100 dataset. The results in Figure 17 suggest that the values of \(\alpha\) and \(\beta\) tend to decrease with the depth of a model. These observations align with those from Section 5.1 and Section 5.2 as the constants \(\alpha\) and \(\beta\) decrease as the depth of the model increases.

### AlgoPerf experiments

For each of all aforementioned tasks, we repeat the training with \(3\) random seeds to create more stable results. The detailed model architectures are given in [16]. In Table 3 we provide the parameters of optimizers we use for each task. The loss curves are presented in Figure 18. For each workload, we run the experiments for \(3\) different random seeds to obtain more stable results. The hyperparameters of optimizer NadamW are chosen such that we can reach the validation threshold set by the organizers7.

Footnote 7: The quality of performance is measured differently from one task to another; we defer to the [16] for a more detailed description of the competition.

We employ a cosine annealing learning rate schedule that reduces the learning to \(1e-10\), with an initial linear warm-up. For each workload, we run the experiments sufficiently enough so that we reach the validation target threshold and the stochastic loss becomes sufficiently stable.

Figure 14: Values of stochastic loss during the training of CNN model on CIFAR10 dataset varying the number of convolutions in the second layer.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline
**Dataset** & **Model** & **Batch Size** & **LR** & \(\beta_{1}\) & \(\beta_{2}\) & **Weight Decay** & **Warmup** \\ \hline Criteo 1TB & DLRMsmall & 262144 & 0.0017 & 0.93 & 0.995 & 0.08 & 0.02 \\ \hline Fastmri & U-Net & 32 & 0.001 & 0.9 & 0.998 & 0.15 & 0.1 \\ \hline OBGB & GNN & 512 & 0.0017 & 0.93 & 0.995 & 0.08 & 0.02 \\ \hline WMT & Transformer & 128 & 0.001 & 0.97 & 0.999 & 0.15 & 0.1 \\ \hline Slim-Pajama-627B & Pythia-70M & 256 & 0.01 & 0.9 & 0.95 & 0.1 & 0.1 \\ \hline Slim-Pajama-627B & Pythia-160M & 256 & 0.006 & 0.9 & 0.95 & 0.1 & 0.1 \\ \hline \end{tabular}
\end{table}
Table 3: Training details of large models from Appendix D.8 and Appendix D.9

Figure 15: Training of Resnet9 model on CIFAR100 dataset varying the batch size.

### Pythia experiments

For each of all aforementioned tasks, we repeat the training with \(3\) random seeds to create more stable results. We train Pythia \(70\)M and Pythia \(160\)M [8] on publicly available Slim-Pajama-627B dataset [72]. Both models are trained on sequences of length 2048, and makes use of a batch size of 0.5M tokens, which amounts to a batch size of 256 samples. We use AdamW optimizer and a cosine annealing with linear warmup, with hyperparameters specified in Table 3. The stochastic loss and training perplexity are reported in Figure 19.

Figure 16: Training of ResNet9 model on CIFAR100 dataset varying the batch size with SGDM (stepsize \(0.01\) and momentum \(0.9\) with OneCycle learning rate scheduler) and Adam (stepsize \(0.0001\) with default momentum parameters with OneCycle learning rate scheduler) optimizers. Here \(T(x_{k})=\langle\nabla f_{i_{k}}(x^{k}),x^{k}-x^{K}\rangle-\alpha(f_{i_{k}}(x^ {k})-f_{i_{k}}(x^{K}))-\beta f_{i_{k}}(x^{k})\) assuming that \(f_{i}^{*}=0\). Minimum is taken across all runs and iterations for a given pair of \((\alpha,\beta)\). We plot values of \(\alpha\),\(\beta\) in \(\alpha\)-\(\beta\)-condition for SGDM (**first row**) and Adam (**second row**).

Figure 17: Training of ResNet18 and Resnet34 model on CIFAR100 dataset varying the batch size SGD (stepsize \(0.01\) and momentum \(0.9\) with OneCycle learning rate scheduler). Here \(T(x_{k})=\langle\nabla f_{i_{k}}(x^{k}),x^{k}-x^{K}\rangle-\alpha(f_{i_{k}}(x ^{k})-f_{i_{k}}(x^{K}))-\beta f_{i_{k}}(x^{k})\) assuming that \(f_{i}^{*}=0\). Minimum is taken across all runs and iterations for a given pair of \((\alpha,\beta)\).

## Appendix E Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract, we shortly list all the main results of the paper. In the introduction and related works, we demonstrate the limitations of previous works and explicitly list the contributions of our work. We support theoretical claims by extensive experimental results on various deep learning benchmarks. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We provide a section in the end of the main body where we discuss several limitations of our work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.

Figure 19: Training statistics for Pythia language models.

Figure 18: Training of large models from AlgoPerf benchmark.

* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We explicitly state a new class of functions we propose in the paper as well as other assumptions on the smoothness and noise of ERM problem. We explicitly state all convergence guarantees for 4 different optimization algorithms. The proofs are deferred to the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: For most of the experiments we use existing open source Github repositories making small changes in the code to track necessary quantities during the training. We provide the links to all used repositories. Besides, we list all parameters of optimizers to reproduce the results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.

* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: For each open source data and code we attach the link. All codes can be used almost immediately with small changes that are needed to track necessary quantities for validation of the proposed condition. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details**Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all hyperparameters and training details in the corresponding section of the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For each instance of the experiment section, we run tests for several random seeds (exact numbers are indicated in the experiment section of the appendix) to get more stable results. For each figure with training curves, we plot the mean and maximum and minimum fluctuations around it. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide information about GPU that we use for our experiments in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We follow all the rules listed following the link above. In particular, we anonymized the paper and attached the code. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: we provide an impact statement in.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines:* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use open-source datasets and code. For all used codes we explicitly cite the corresponding paper or Github repository. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.