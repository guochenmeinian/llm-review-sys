# Unsupervised Anomaly Detection in The Presence of Missing Values

 Feng Xiao\({}^{1}\) &Jicong Fan \({}^{1,2}\)

\({}^{1}\)The Chinese University of Hong Kong, Shenzhen, China

\({}^{2}\)Shenzhen Research Institute of Big Data, Shenzhen, China

xiaofeng.cs.ds@gmail.com &fanjicong@cuhk.edu.cn

Corresponding author

###### Abstract

Anomaly detection methods typically require fully observed data for model training and inference and cannot handle incomplete data, while the missing data problem is pervasive in science and engineering, leading to challenges in many important applications such as abnormal user detection in recommendation systems and novel or anomalous cell detection in bioinformatics, where the missing rates can be higher than 30% or even 80%. In this work, first, we construct and evaluate a straightforward strategy, "impute-then-detect", via combining state-of-the-art imputation methods with unsupervised anomaly detection methods, where the training data are composed of normal samples only. We observe that such two-stage methods frequently yield imputation bias from normal data, namely, the imputation methods are inclined to make incomplete samples "normal", where the fundamental reason is that the imputation models learned only on normal data and cannot generalize well to abnormal data in the inference stage. To address this challenge, we propose an end-to-end method that integrates data imputation with anomaly detection into a unified optimization problem. The proposed model learns to generate well-designed pseudo-abnormal samples to mitigate the imputation bias and ensure the discrimination ability of both the imputation and detection processes. Furthermore, we provide theoretical guarantees for the effectiveness of the proposed method, proving that the proposed method can correctly detect anomalies with high probability. Experimental results on datasets with manually constructed missing values and inherent missing values demonstrate that our proposed method effectively mitigates the imputation bias and surpasses the baseline methods significantly. The source code of our method is available at https://github.com/jicongfan/ImAD-Anomaly-Detection-With-Missing-Data.

## 1 Introduction

Anomaly detection (AD) [Breunig et al., 2000, Scholkopf et al., 2001, Liu et al., 2008, Pevny, 2016, Zong et al., 2018, Ruff et al., 2018, Cai and Fan, 2022, Fu et al., 2024, Zhang et al., 2024, Xiao et al., 2025], aiming at identifying anomalous or novel samples in data, is a crucial machine learning problem. It finds extensive applications in many high-stakes fields such as biology, healthcare, finance, and cybersecurity. Data missing or incompleteness, a persistent and unavoidable issue in many real-world situations, often arises during the processes of data collection, transmission, and storage. Moreover, in fields like bioinformatics (e.g. single-cell RNA sequencing) [Zhang and Zhang, 2018], psychology (e.g. questionnaire data) [Schlomer et al., 2010], and recommendation systems (e.g. user-item interaction data) [Shani and Gunawardana, 2011, Fan et al., 2024], the data missing rates are often higher than \(30\%\) or even \(80\%\). Indeed, the missing data problems lead tomany challenges for anomaly detection, such as detecting anomalous cells or rare cell types based on incomplete single-cell RNA sequencing data [14] and identifying abnormal users in recommendation systems [21]. Regrettably, most existing AD methods necessitate complete data in both the training and test sets, rendering them ill-equipped to handle datasets with missing values. Consequently, addressing the AD challenge in the context of incomplete data becomes both necessary and inevitable.

A naive strategy is filling the missing values by statistical characteristics such as mean or median and then performing anomaly detection. Taking two real-world datasets "Adult" and "KDD" as examples, we consider the mechanism missing completely at random and fill the missing entries with the variable means and then perform a classical AD methods Isolation Forest [15]) and two deep learning based AD methods (Deep SVDD [19] and NeutraL AD [17]). The results are shown in Figure 1. The detection accuracies of the four methods degrade significantly with the missing rate increases. This verified the failure of the naive strategy and the difficulty of unsupervised anomaly detection with missing values. Besides the naive imputation, one may consider using more powerful imputation algorithms [1, 13, 14, 15] to recover the missing values and subsequently implementing AD algorithms on imputed data. We refer to this strategy as "impute-then-detect".

It is worth noting that, for unsupervised anomaly detection, where the training set is composed of only normal samples, such "impute-then-detect" methods would yield imputation bias for normal data, i.e., the imputation methods are inclined to recover an abnormal sample with missing values as "normal" as possible during the inference, which leads to lower recall or higher false negative rate. Figure 2 clearly shows the negative impacts of the imputation bias, which is worth studying and addressing. The main challenge is that the training set and test set do not satisfy the condition of identical distribution and the imputation model trained only on incomplete normal data does not generalize well to incomplete abnormal data. In Section 4.2, we quantitatively and comprehensively evaluate the "impute-then-detect" methods using state-of-the-art imputation algorithms and AD algorithms.

To tackle the aforementioned problem, in this paper, we propose an end-to-end method, called ImAD, for unsupervised anomaly detection on incomplete data. The main idea of ImAD is to integrate data **im**putation and **a**anomaly **d**etection into a unified optimization objective and alleviate imputation bias by automatically learning to generate pseudo-abnormal samples. Note that the pseudo-abnormal samples are by-products of the training process and we do not use any extra data in all experiments. Our contributions are summarized as follows.

* We study the imputation bias problem of the "impute-then-detect" pipeline and quantitatively evaluate their detection performance.
* We propose a novel method ImAD for AD on incomplete data. To the best of our knowledge, it is the first end-to-end unsupervised AD method in the presence of missing value.
* We provide theoretical guarantees for ImAD, proving that it can correctly detect anomalies with high probability.
* We compare ImAD with more than 9 baselines on 11 real datasets of various domains, covering datasets with manually constructed missing values and datasets with inherent missing values.

Figure 1: Performance (AUROC) degradation of anomaly detection methods with increasing missing rate on Adult and KDD datasets.

Figure 2: The degradation of recall rate of abnormal data on “impute-then-detect” methods.

Related Work

Missing Data ImputationData imputation fills missing data with plausible values and provides imputed data for downstream tasks such as classification, clustering, and visualization. As the missing data problem is prevalent in many fields, the study on missing data imputation is extensive, and many algorithms have been proposed in the past decades. Mayer et al. (2019) pointed out that there are approximately 150 implementations available to handle missing data. These methods can be roughly organized into three categories. The first category is based on the iterative regression model, such as well-known methods Multiple Imputation by Chained Equations (MICE) (Royston and White, 2011) and MissForest (Stekhoven and Buhlmann, 2012) that trains random forests on observed data through an iterative imputation scheme. The second category is the matrix completion methods (Candes and Recht, 2012; Mazumder et al., 2010; Fan and Chow, 2018; Fan et al., 2019, 2020). The third category is based on deep learning especially deep generative models (Fan and Chow, 2017; Yoon et al., 2018; Li et al., 2019; Muzellec et al., 2020). For instance, Yoon et al. (2018) proposed generative adversarial imputation network (GAIN) based on generative adversarial network (GAN) (Goodfellow et al., 2014) and (Tashiro et al., 2021) proposed conditional score-based diffusion models for probabilistic time-series imputation (CSDI) based diffusion model (Sohl-Dickstein et al., 2015). Indeed, these deep imputation methods often achieve state-of-the-art performance in the tasks of missing data imputation, when the distributions of the training data and testing data are identical. However, their performance in recovering the missing values for unsupervised AD is rarely studied.

Anomaly detection on incomplete dataThe research on anomaly detection in the presence of missing values is very limited. To the best of the authors' knowledge, (Zemicheal and Dietterich, 2019) is the first work evaluating the detection performance of anomaly detection methods combined with different data imputation techniques. Their experiments of anomaly detection on a few UCI datasets with missing values showed that implementations of unsupervised anomaly detection methods such as Isolation Forest (Liu et al., 2008) on incomplete data should always include algorithms for handling missing values and the imputation contributes to improving the detection performance of anomaly methods. Fan et al. (2022) studied the problem of statistical process monitoring with missing values and proposed a fast incremental nonlinear matrix completion method for online and sequential imputation. Sarda et al. (2023) provided a study of existing unsupervised anomaly detection methods on GAN-imputed data.

It's worth noting that the strategies used in (Zemicheal and Dietterich, 2019; Fan et al., 2022; Sarda et al., 2023) are two-stage methods, where the imputation models are trained on the training dataset that does not contain any abnormal data or only contains very few unlabeled outliers. As a result, the imputation model will not generalize well on abnormal data during the inference and will use the learned pattern of normal data to fill the missing values of abnormal data, which makes the abnormal data similar to normal data and hence lowers detection accuracy. In contrast, our method integrates data imputation and anomaly detection into a unified process, and alleviates the imputation bias via introducing pseudo-abnormal samples, and hence achieves superior detection accuracy.

## 3 Proposed Method

### Problem Formulation and Our Motivation

Given \(n\) samples \(\mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{n}\) drawn from an unknown distribution \(\mathcal{D}_{\mathbf{x}}\subseteq\mathbb{R}^{m}\), the goal of unsupervised AD is to learn a decision function \(f:\mathbb{R}^{m}\rightarrow\{0,1\}\) by utilizing only these \(n\) samples, such that \(f(\mathbf{x})=0\) if \(\mathbf{x}\in\mathcal{D}_{\mathbf{x}}\) and \(f(\mathbf{x})=1\) if \(\mathbf{x}\notin\mathcal{D}_{\mathbf{x}}\). We consider the scenario that \(\mathbf{X}:=[\mathbf{x}_{1}^{\top},\mathbf{x}_{2}^{\top},\cdots,\mathbf{x}_{ n}^{\top}]^{\top}\in\mathbb{R}^{n\times m}\) contains missing values. Let \(\mathbf{M}\in\{0,1\}^{n\times m}\) be a mask matrix determined by some missing mechanism \(\mathcal{M}\) such as MCAR, MAR, or MNAR, where \(M_{i,j}=1\) means \(X_{i,j}\) is observed and \(M_{i,j}=0\) means \(X_{i,j}\) is missing. Then the observed incomplete matrix is

\[\tilde{\mathbf{X}}=[\tilde{\mathbf{x}}_{1}^{\top},\tilde{\mathbf{x}}_{2}^{ \top},\cdots,\tilde{\mathbf{x}}_{n}^{\top}]^{\top}=\mathcal{M}(\mathbf{X})= \mathbf{X}\odot\mathbf{M}\] (1)

where \(\odot\) is the Hadamard product. Equation (1) implies that the missing values of \(\mathbf{X}\) are temporarily filled with zeros. In many scenarios such as gene expression data analysis, recommendation systems, and questionnaire surveys, the data missing rate in \(\tilde{\mathbf{X}}\) is often high. Training an anomaly detection model \(f\) on \(\tilde{\mathbf{X}}\) and using it to detect anomalies in new incomplete data has practical significance such as detecting anomalous cells or rare cell types in bioinformatics, identifying abnormal users in recommendation systems, and recognizing unusual subjects using questionnaires of psychology.

As mentioned before, conventional AD methods are vulnerable to missing values and a good imputation algorithm can raise the detection accuracy of an anomaly detection method to some extent. However, the strategy "impute-then-detect" is inclined to make incomplete abnormal samples normal and hence cannot provide satisfactory detection performance. Therefore, in this work, we aim to provide an end-to-end unsupervised anomaly detection method in the presence of missing values to mitigate the imputation bias and improve the detection accuracy. The most challenging problem is that the imputation model (denoted as \(\mathcal{I}\)) trained only on incomplete normal data cannot generalize well to incomplete abnormal data. To solve the challenge, we take the following strategy and consideration.

We propose to learn a model that can generate some pseudo-abnormal samples, and then learn an imputation model from both the original normal data and the generated pseudo-abnormal samples. Thus, the learned imputation model can generalize well to incomplete abnormal data during inference and recover the missing values with high accuracy, which further improves the accuracy of anomaly detection. However, we encounter the following issues.

* It is non-trivial to generate meaningful pseudo-abnormal samples that are similar enough to real ones. The reason is that the distribution (i.e., \(\mathcal{D}_{\mathbf{x}}\)) of training data is unknown and the data dimension \(m\) is often high.
* The incompleteness of \(\mathbf{X}\) further increases the difficulty of generating pseudo-abnormal samples.
* On the other hand, the generated pseudo-abnormal samples should not be too far from the normal data, where a large gap will make the learned imputation model fail to impute the abnormal samples close to normal data and cause the abnormal samples to be hard to detect.
* The generating model, imputation model, and detection model should be coordinated with each other and as a whole to ensure the reliability of the inference.

### Learning Framework of ImAD

To address the aforementioned challenges, we propose to find a \(d\)-dimensional latent space \(\mathcal{Z}\) where the normal data are lying and then generate pseudo-abnormal samples around the normal samples in \(\mathcal{Z}\). The samples in \(\mathcal{Z}\) will be mapped back by a neural network to the original data space, yielding reliable pseudo-abnormal data.

We define \(\mathcal{D}_{\mathbf{z}}\) as the latent distribution of the normal data in \(\mathcal{Z}\) and define \(\mathcal{D}_{\mathbf{\hat{z}}}\) as the latent distribution of pseudo-abnormal data in \(\mathcal{Z}\). Since the patterns of normality are limited and the patterns of abnormality are unlimited, we let \(\mathcal{D}_{\mathbf{z}}\) be a truncated Gaussian distribution (a hyperball denoted by \(\mathcal{B}\), with radius \(r_{1}\)) in \(\mathcal{Z}\) and assume that the remaining region of \(\mathcal{Z}\) is the abnormal region, denoted as \(\mathcal{Z}\setminus\mathcal{B}\). It should be pointed out that there is no need to define \(\mathcal{D}_{\mathbf{\hat{z}}}\) in the entire space \(\mathcal{Z}\setminus\mathcal{B}\), which will be explained in the discussion for Theorem 3.2(b) and further supported by Theorem 3.4 in Section 3.4. Instead, we only need to define \(\mathcal{D}_{\mathbf{\hat{z}}}\) in a small region of \(\mathcal{Z}\setminus\mathcal{B}\) that encloses \(\mathcal{B}\), which will reduce the uncertainty of random sampling (or samples size equivalently) and make it easier for mapping the samples back to the original data space. Thus, we define \(\mathcal{D}_{\mathbf{\hat{z}}}\) as a hypershell surrounding \(\mathcal{B}\) and let \(\mathcal{D}_{\mathbf{\hat{z}}}\) be a truncated Gaussian. The radii of the two hyperspheres forming the hypershell are \(r_{1}\) and \(r_{2}\) respectively, where \(r_{2}>r_{1}\). An illustration of \(\mathcal{D}_{\mathbf{z}}\) and \(\mathcal{D}_{\mathbf{\hat{z}}}\) in 2-D space is shown in Figure 3, where \(\mathcal{D}_{\mathbf{z}}\) and \(\mathcal{D}_{\mathbf{\hat{z}}}\) are truncated Gaussian from \(\mathcal{N}(\mathbf{0},0.5^{2}\cdot\mathbf{I}_{2})\) and \(\mathcal{N}(\mathbf{0},\mathbf{I}_{2})\) respectively. The theoretical analysis for sampling from \(\mathcal{D}_{\mathbf{z}}\) and \(\mathcal{D}_{\mathbf{\hat{z}}}\) is in Appendix A. We learn a reconstructor \(\mathcal{R}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}\) to transform the samples drawn from \(\mathcal{D}_{\mathbf{z}}\) to the original data distribution \(\mathcal{D}_{\mathbf{x}}\), i.e.,

\[\mathcal{D}_{\mathbf{x}}\approx\mathcal{R}(\mathcal{D}_{\mathbf{z}}).\] (2)

\(\mathcal{R}\) is actually a reconstruction model that recovers the original data from the latent space \(\mathcal{Z}\). With \(\mathcal{D}_{\mathbf{\hat{z}}}\) and \(\mathcal{R}\), we can obtain a distribution \(\mathcal{D}_{\mathbf{\hat{x}}}\) of pseudo-abnormal data in the original data space as

\[\mathcal{D}_{\mathbf{\hat{x}}}:=\mathcal{R}(\mathcal{D}_{\mathbf{\hat{z}}}).\] (3)

The samples (denoted by \(\mathbf{\tilde{x}}\)) drawn from \(\mathcal{D}_{\mathbf{\hat{x}}}\) are reasonable pseudo-abnormal samples, which will be explained by the discussion for Theorem 3.2(a) in Section 3.4.

Now we use a projector \(\mathcal{P}:\mathbb{R}^{m}\rightarrow\mathbb{R}^{d}\) to transform \(\mathcal{D}_{\mathbf{x}}\) and \(\mathcal{D}_{\tilde{\mathbf{x}}}\) into \(\mathcal{D}_{\mathbf{z}}\) and \(\mathcal{D}_{\tilde{\mathbf{z}}}\) respectively, i.e.,

\[\mathcal{D}_{\mathbf{z}}\approx\mathcal{P}(\mathcal{D}_{\mathbf{x}}),\quad \mathcal{D}_{\tilde{\mathbf{z}}}\approx\mathcal{P}(\mathcal{D}_{\tilde{\mathbf{ x}}}).\] (4)

However, the training set \(\tilde{\mathbf{X}}=\mathcal{M}(\mathbf{X})\) is incomplete, and we need to learn an imputation model \(\mathcal{I}\) to recover the missing values, i.e., \(\hat{\mathbf{X}}=\mathcal{I}(\tilde{\mathbf{X}})\). More generally, we denote

\[\mathcal{D}_{\tilde{\mathbf{x}}}=\mathcal{I}(\mathcal{D}_{\tilde{\mathbf{x}}}).\] (5)

We hope that the imputation model is also able to recover the missing values of the generated pseudo-abnormal samples if they have, though they are complete. We thus remove some values of the generated pseudo-abnormal samples \(\tilde{\mathbf{x}}\sim\mathcal{D}_{\tilde{\mathbf{x}}}\) using missing mechanism \(\tilde{\mathcal{M}}\) and let \(\mathcal{D}_{\tilde{\mathbf{x}}}=\tilde{\mathcal{M}}(\mathcal{D}_{\tilde{ \mathbf{x}}})\).

The missing values are then recovered by

\[\mathcal{D}_{\tilde{\mathbf{x}}}=\mathcal{I}(\mathcal{D}_{\tilde{\mathbf{x}}}).\] (6)

This step mitigates the problem of imputation bias encountered by the "impute-then-detect" methods. Let \(\mathcal{E}_{I}\), \(\mathcal{E}_{P}\), and \(\mathcal{E}_{R}\) denote some distance or discrepancy measure between distributions. We here show how to achieve the goals of (2), (3), (4), (5), and (6) in a unified optimization problem. First, for normal data, we solve

\[\underset{\mathcal{I},\mathcal{P},\mathcal{R}}{\text{minimize}}\ \mathcal{E}_{I}( \mathcal{I}(\mathcal{D}_{\tilde{\mathbf{x}}}),\mathcal{D}_{\tilde{\mathbf{x}}} \mid\mathcal{M})+\mathcal{E}_{P}(\mathcal{P}(\mathcal{D}_{\tilde{\mathbf{x}}} ),\mathcal{D}_{\mathbf{z}})+\mathcal{E}_{R}(\mathcal{R}(\mathcal{P}(\mathcal{ D}_{\tilde{\mathbf{x}}})),\mathcal{D}_{\tilde{\mathbf{x}}}\mid\mathcal{M})\] (7)

For the generated pseudo-abnormal data, we solve

\[\underset{\mathcal{I},\mathcal{P},\mathcal{R}}{\text{minimize}}\ \mathcal{E}_{I} \big{(}\mathcal{I}(\tilde{\mathcal{M}}(\mathcal{R}(\mathcal{D}_{\tilde{ \mathbf{z}}}))),\tilde{\mathcal{M}}(\mathcal{R}(\mathcal{D}_{\tilde{\mathbf{z} }}))\mid\tilde{\mathcal{M}}\big{)}+\mathcal{E}_{P}\big{(}\mathcal{P}( \mathcal{I}(\tilde{\mathcal{M}}(\mathcal{R}(\mathcal{D}_{\tilde{\mathbf{z}}})) )),\mathcal{D}_{\tilde{\mathbf{z}}})\] (8)

Let \(\widehat{\mathcal{E}}\). be a finite-sample estimation of \(\mathcal{E}\).. Combining (7) and (8), we obtain the objective of ImAD:

\[\underset{\mathcal{I},\mathcal{P},\mathcal{R}}{\text{minimize}}\ \ \frac{\widehat{\mathcal{E}}_{I}( \mathcal{I}([\tilde{\mathbf{X}};\tilde{\mathbf{X}}]),[\tilde{\mathbf{X}};\tilde {\mathbf{X}}]\mid[\mathbf{M},\tilde{\mathbf{M}}])}{\mathcal{L}^{(\text{disc})}} \ +\underbrace{\widehat{\mathcal{E}}_{P}(\mathcal{P}([\tilde{\mathbf{X}};\tilde{ \mathbf{X}}]),[\mathbf{Z};\tilde{\mathbf{Z}}])}_{\mathcal{L}^{(\text{disc})}} \ +\underbrace{\widehat{\mathcal{E}}_{R}(\mathcal{R}(\mathcal{P}(\tilde{ \mathbf{X}})),\tilde{\mathbf{X}}\mid\mathbf{M})}_{\mathcal{L}^{(\text{disc})}}\] (9)

where \(\tilde{\mathbf{X}}=\mathcal{R}(\tilde{\mathbf{Z}})\odot\tilde{\mathbf{M}}\), \(\hat{\mathbf{X}}=\mathcal{I}(\tilde{\mathbf{X}})\), and \([\cdot;\cdot]\) denotes the row-wise concatenation of two matrices. In (9), the samples in \(\mathbf{Z}\) are drawn from \(\mathcal{D}_{\mathbf{z}}\) and the samples in \(\tilde{\mathbf{Z}}\) are drawn from \(\mathcal{D}_{\tilde{\mathbf{z}}}\). The roles of the three parts of the objective function in (9) are analyzed as follows.

* \(\mathcal{L}^{(\text{DI})}\) denotes the data imputation loss. With this loss, the imputation model will be able to recover the missing values of normal data and abnormal data.

* \(\mathcal{L}^{(\text{AD})}\) denotes the anomaly detection loss. With this loss, the anomaly detection model will be discriminative and be able to project normal data and abnormal data into different regions in \(\mathcal{Z}\).
* \(\mathcal{L}^{(\text{RE})}\) denotes the reconstruction loss. This loss is to ensure that \(\mathcal{D}_{\mathbf{z}}\) and \(\mathcal{D}_{\tilde{\mathbf{z}}}\) are meaningful.

We see that our method ImAD couples data imputation with anomaly detection to a unified optimization objective. Figure 4 depicts the overall framework of ImAD, where the green and red arrows show the flow paths of normal data (starting from \(\tilde{\mathbf{X}}\)) and pseudo-abnormal data (starting from \(\tilde{\mathbf{Z}}\)) respectively. The reconstructors in Figure 4 share parameters.

### Specific Implementation of ImAD

We use three neural networks \(h_{\psi}\), \(f_{\theta}\) and \(g_{\phi}\) with parameters \(\psi,\theta,\phi\) to model \(\mathcal{I},\mathcal{P}\) and \(\mathcal{R}\) respectively. For \(\mathcal{E}\), we consider two different cases. If the samples are pair-wise, we directly use the square loss, which is simple and efficient. Thus, in \(\mathcal{L}^{\text{DI}}\) and \(\mathcal{L}^{\text{RE}}\), we use the square loss, and the square loss for \(\mathcal{L}^{\text{RE}}\) is masked by \(\mathbf{M}\). When the samples are not pair-wise, we take advantage of the Sinkhorn distance [Cuturi, 2013] derived from the optimal transport theory. The Sinkhorn distance between two distributions \(\mathcal{D}_{\mathcal{U}}\) and \(\mathcal{D}_{\mathcal{V}}\) supported by their finite samples \(\mathcal{U}=\{\mathbf{u}_{1},\mathbf{u}_{2},\cdots,\mathbf{u}_{n_{u}}\}\sim \mathcal{D}_{u}\) and \(\mathcal{V}=\{\mathbf{v}_{1},\mathbf{v}_{2},\cdots,\mathbf{v}_{n_{v}}\}\sim \mathcal{D}_{v}\) is defined as

\[\text{Sinkhorn}(\mathcal{U},\mathcal{V}):=\min_{\mathbf{P}}~{}\langle\mathbf{P },\mathbf{C}\rangle_{F}+\eta\sum_{i,j}\mathbf{P}_{ij}\log(\mathbf{P}_{ij}), \qquad\text{s.t.}~{}\mathbf{P}\mathbf{1}=\mathbf{a},\mathbf{P}^{T}\mathbf{1}= \mathbf{b},\mathbf{P}\geq 0\] (10)

where \(\mathbf{P}\in\mathbb{R}^{n_{u}\times n_{v}}\) is the transport plan and \(\mathbf{C}\in\mathbb{R}^{n_{u}\times n_{v}}\) is the metric cost matrix. The two probability vectors \(\mathbf{a}\) and \(\mathbf{b}\) satisfy \(\mathbf{a}^{T}\mathbf{1}=1,\mathbf{b}^{T}\mathbf{1}=1\), and \(\eta\geq 0\) is a trade-off between the Wasserstein distance and entropy regularization.

By applying \(h_{\psi},f_{\theta},g_{\phi}\), square loss, and Sinkhorn distance to (9), we obtain the following problem:

\[\begin{split}\underset{\psi,\theta,\phi}{\text{minimize}}& \underbrace{\text{Sinkhorn}(f_{\theta}(h_{\psi}(\tilde{\mathbf{X}})), \mathbf{Z})+\alpha\|\tilde{\mathbf{Z}}-f_{\theta}(h_{\psi}(g_{\phi}(\tilde{ \mathbf{Z}})\odot\tilde{\mathbf{M}}))\|_{F}^{2}}_{\mathcal{L}^{(\text{AD})}}\\ &+\underbrace{\beta\|([\tilde{\mathbf{X}};\tilde{\tilde{\mathbf{X }}}]-h_{\psi}([\tilde{\mathbf{X}};\tilde{\tilde{\mathbf{X}}}])\odot[\mathbf{M };\tilde{\mathbf{M}}]\|_{F}^{2}}_{\mathcal{L}^{(\text{DI})}}+\underbrace{ \lambda\|(\tilde{\mathbf{X}}-g_{\phi}(f_{\theta}(h_{\psi}(\tilde{\mathbf{X}})) ))\odot\mathbf{M}\|_{F}^{2}}_{\mathcal{L}^{(\text{DI})}}\end{split}\] (11)

Solving the problem (11), we get well trained imputer \(h_{\psi^{*}}\) and projector \(f_{\theta^{*}}\). For a new sample \(\tilde{\mathbf{x}}_{\text{new}}\) containing missing values, we define an anomaly score \(s(\cdot)\) by

\[s(\tilde{\mathbf{x}}_{\text{new}})=\|f_{\theta^{*}}(h_{\psi^{*}}(\tilde{ \mathbf{x}}_{\text{new}}))\|,\] (12)

which is the distance to the origin in the latent space. If \(s(\tilde{\mathbf{x}}_{\text{new}})>r_{1}\), \(\tilde{\mathbf{x}}_{\text{new}}\) is detected as abnormal. Otherwise, \(\tilde{\mathbf{x}}_{\text{new}}\) is treated as a normal sample.

### Theoretical Guarantees for ImAD

WLOG, we assume \(f_{\theta}\), \(g_{\phi}\), and \(h_{\psi}\) all have \(L\) layers, where \(\theta=\{\mathbf{W}_{1}^{f},\mathbf{W}_{2}^{f},\ldots,\mathbf{W}_{L}^{f}\}\), \(\phi=\{\mathbf{W}_{1}^{g},\mathbf{W}_{2}^{g},\ldots,\mathbf{W}_{L}^{g}\}\), and \(\psi=\{\mathbf{W}_{1}^{h},\mathbf{W}_{2}^{h},\ldots,\mathbf{W}_{L}^{h}\}\). Denote the spectral norm and \(\ell_{2,1}\)-norm of a matrix as \(\|\cdot\|_{\sigma}\) and \(\|\cdot\|_{2,1}\) respectively. We also make the following assumptions.

**Assumption 3.1**.: For \(f_{\theta}\), \(g_{\phi}\), and \(h_{\psi}\), the following conditions hold: 1) \(\|\mathbf{W}_{l}^{f}\|_{\sigma}\leq\alpha_{f}\), \(\|\mathbf{W}_{l}^{g}\|_{\sigma}\leq\alpha_{g}\), \(\|\mathbf{W}_{l}^{h}\|_{\sigma}\leq\alpha_{h}\), \(\forall l\in[L]\); 2) \(\|\mathbf{W}_{l}^{f}\|_{2,1}\leq b_{f}\), \(\|\mathbf{W}_{l}^{g}\|_{2,1}\leq b_{g}\), \(\|\mathbf{W}_{l}^{h}\|_{2,1}\leq b_{h}\), \(\forall l\in[L]\); 3) all activation functions in \(f_{\theta}\), \(g_{\phi}\), and \(h_{\psi}\) are \(\rho\)-Lipschitz continuous; 4) the maximum width of the layers in \(f_{\theta}\), \(g_{\phi}\), and \(h_{\psi}\) is \(\bar{d}\).

The following theorem can be used to obtain some deterministic guarantee for ImAD.

**Theorem 3.2**.: _Under Assumption 3.1, we have: (a) \(\|g_{\phi}(\mathbf{z})-g_{\phi}(\tilde{\mathbf{z}})\|\leq\rho^{L}\alpha_{f}^{L} \|\mathbf{z}-\tilde{\mathbf{z}}\|\) holds for any \(\mathbf{z}\), \(\tilde{\mathbf{z}}\); (b) \(\|f_{\theta}(h_{\psi}(\tilde{\mathbf{x}}))-f_{\theta}(h_{\psi}(\tilde{\tilde{ \mathbf{x}}}))\|\leq\rho^{2L}\alpha_{f}^{L}\alpha_{h}^{L}\|\tilde{\mathbf{x}}-\tilde{ \tilde{\mathbf{x}}}\|\) holds for any \(\tilde{\mathbf{x}}\) and \(\tilde{\tilde{\mathbf{x}}}\)._

Theorem 3.2(a) indicates that in the latent space \(\mathcal{Z}\), if an abnormal sample \(\tilde{\mathbf{z}}\sim\mathcal{D}_{\tilde{\mathbf{z}}}\) is close to a normal sample \(\mathbf{z}\sim\mathcal{D}_{\mathbf{z}}\), in the original data space, the corresponding abnormal sample \(\tilde{\mathbf{x}}\) is still close to the normal sample \(\mathbf{x}\) provided that \(\alpha_{g}\) is not too large. This means the generated pseudo-abnormalsamples are practical and useful. For Theorem 3.2(b), let's consider an incomplete abnormal sample \(\tilde{\tilde{\mathbf{x}}}\) and assume that its closest incomplete pseudo-abnormal sample generated by the \(\tilde{\mathbf{z}}\) on the outer hypersphere (shown in Figure 3) is \(\tilde{\tilde{\mathbf{x}}}^{*}\), where \(\|\tilde{\tilde{\mathbf{x}}}-\tilde{\tilde{\mathbf{x}}}^{*}\|=\beta\). Then in the latent space, we have \(\|\tilde{\mathbf{z}}-\tilde{\mathbf{z}}^{*}\|\leq\rho^{2L}\alpha_{f}^{L} \alpha_{h}^{L}\beta\). Let the radii of the inner and outer hyperspheres be \(r_{1}\) and \(r_{2}\) respectively. Now we can conclude that if \(r_{2}-r_{1}>\rho^{2L}\alpha_{f}^{L}\alpha_{h}^{L}\beta\), \(\tilde{\mathbf{z}}\) is outside the decision region given by the inner hypersphere and hence \(\tilde{\tilde{\mathbf{x}}}\) is successfully detected as an abnormal sample.

Now we study the theoretical guarantees for our ImAD in the sense of expectation. Let \(r_{1}\) be the thresholds for the anomaly score defined by (12) to determine whether a sample is normal or not. Let \(r_{2}\) be the radius of the outer hypersphere enclosing \(\mathcal{D}_{\tilde{\mathbf{z}}}\). Let \(\tilde{s}_{\tilde{\mathbf{x}}}\) be the average anomaly score of the (incomplete) normal training data, i.e., \(\tilde{s}_{\tilde{\mathbf{x}}}^{2}=\frac{1}{n}\sum_{i=1}^{n}s(\tilde{\mathbf{ x}}_{i})^{2}\). Let \(\tilde{\varepsilon}_{\tilde{\tilde{\mathbf{x}}}}^{2}:=\frac{1}{n}\sum_{i=1}^{n}| r_{2}^{2}-s(\tilde{\tilde{\mathbf{x}}}_{i})^{2}|\), where \(\tilde{\tilde{\mathbf{x}}}_{i}\) are the (incomplete) pseudo-abnormal samples generated during the training stage. With these definitions and Assumption 3.1, the following (proved in Appendix C) presents the theoretical generalization ability of our ImAD.

**Theorem 3.3**.: _Suppose the squared anomaly score \(s(\tilde{\mathbf{x}})^{2}\) of normal data is always upper-bounded by \(\gamma\), \(|r_{2}^{2}-s(\tilde{\tilde{\mathbf{x}}})^{2}|\) of the pseudo-abnormal data is always upper-bounded by \(\tilde{\gamma}\), and the absolute output of \(f_{\theta}\) is always upper-bounded by \(\vartheta\). Suppose the samples in \(\tilde{\mathbf{X}}\) and \(\tilde{\tilde{\mathbf{X}}}\) are independently drawn \(\mathcal{D}_{\tilde{\mathbf{x}}}\) and \(\mathcal{D}_{\tilde{\mathbf{x}}}\) respectively. Define \(\kappa=\alpha_{f}^{L}\alpha_{h}^{L}\), \(\zeta=\left(1+L\big{(}\frac{b_{f}}{\alpha_{f}}\big{)}^{2/3}+L\big{(}\frac{b_{ h}}{\alpha_{h}}\big{)}^{2/3}\right)^{3/2}\), \(\Delta=r_{1}^{2}-\tilde{s}_{\tilde{\mathbf{x}}}^{2}\), and \(\tilde{\Delta}=r_{2}^{2}-r_{1}^{2}-\tilde{\varepsilon}_{\tilde{\mathbf{x}}}^ {2}\)._

_(a) For normal data from_ \(\mathcal{D}_{\tilde{\mathbf{x}}}\)_, over the randomness of_ \(\tilde{\mathbf{X}}\)_,_

\[\mathbb{P}\left[\mathbb{E}_{\mathcal{D}_{\tilde{\mathbf{x}}}}[s(\tilde{ \mathbf{x}})]>r_{1}\right]\leq\delta,\] (13)

_where \(\delta=2\exp\Big{(}-2n\big{(}\Delta-\frac{8\gamma+48R\ln n}{n}\big{)}^{2}/(9 \gamma^{2})\Big{)}\) and \(R=\rho^{2L-1}\vartheta\kappa\zeta\|\tilde{\mathbf{X}}\|_{F}\sqrt{d\ln(2d^{2})}\)._

_(b) For abnormal data from_ \(\mathcal{D}_{\tilde{\mathbf{x}}}\)_, over the randomness of_ \(\tilde{\mathbf{X}}\)_,_

\[\mathbb{P}\left[\mathbb{E}_{\mathcal{D}_{\tilde{\mathbf{x}}}}[s(\tilde{\tilde{ \mathbf{x}}})]\geq r_{1}\right]\geq 1-\tilde{\delta},\] (14)

_where \(\tilde{\delta}=2\exp\left(-2n\left(\tilde{\Delta}-\frac{8\gamma+48R\ln n}{n} \right)^{2}/(9\tilde{\gamma}^{2})\right)\) and \(\tilde{R}=\rho^{2L-1}\vartheta\kappa\zeta\|\tilde{\mathbf{X}}\|_{F}\sqrt{d\ln(2 d^{2})}\)._

Theorem 3.3(a) means that a normal sample, in expectation, is detected as anomalous with probability at almost \(\delta\), where \(\delta\) is close to zero under some mild conditions such as \(L\) is not too large and \(n\) is not too small. In other words, a false alarm happens with low probability. Theorem 3.3 (b) means that an abnormal sample drawn from \(\mathcal{D}_{\tilde{\mathbf{x}}}\), in expectation, can be successfully detected with probability at least \(1-\delta\), where \(\delta\) is close to zero under some mild conditions. Theorem 3.3(b) also indicates that a larger \(r_{2}\) is better. It is worth noting that here we only focus on \(\mathcal{D}_{\tilde{\mathbf{x}}}\), which is defined by \(\mathcal{D}_{\tilde{\mathbf{x}}}\), \(\tilde{\mathbf{M}}\), and \(g_{\phi}\). \(\mathcal{D}_{\tilde{\mathbf{x}}}\) can be regarded as a distribution of difficult anomalous data that are close to normal data. The anomalous samples drawn from space out of \(\mathcal{D}_{\tilde{\mathbf{x}}}\) are much easier to detect, which is further supported by the following theorem (proved in Appendix D).

**Theorem 3.4**.: _Let \(c\) be a constant satisfying \(\|f_{\theta}\circ h_{\psi}\circ g_{\phi}(\mathbf{z})-f_{\theta}\circ h_{\psi} \circ g_{\phi}(\mathbf{z}^{\prime})\|\geq c\|\mathbf{z}-\mathbf{z}^{\prime}\|\) for any \(\mathbf{z},\mathbf{z}^{\prime}\) and assume that \(\|f_{\theta}\circ h_{\psi}\circ g_{\phi}(\mathbf{0})-\mathbf{0}\|\leq\varepsilon\). Any samples drawn from the space out of \(\mathcal{D}_{\tilde{\mathbf{x}}}\) can be correctly detected if \(cr_{2}-\varepsilon>r_{1}\)._

## 4 Experiments

### Datasets, Baselines, and Implementation Details

We compare ImAD with "impute-then-detect" methods on 11 publicly available tabular datasets from various fields, including seven datasets with manually constructed missing values and four datasets with inherent missing values. In all experiments, only incomplete normal data are used in the training stage, but there are both incomplete normal and abnormal data during the inference. The statistics of all datasets are in Table 1 and a detailed description of all datasets is in Appendix J. Considering the "impute-then-detect" strategy, for data imputation, we use MissForest (Stekhoven and Buhlmann, 2012) and GAIN (Yoon et al., 2018). For anomaly detection, we use Isolation Forest (Liu et al.,2008), Deep SVDD (Ruff et al., 2018), Neutral AD (Qiu et al., 2021) and DPAD (Fu et al., 2024). The pairwise combination between the imputation and anomaly detection methods yields eight "impute-then-detect" baselines.

We use MLPs to construct the three modules of ImAD, Adam (Kingma and Ba, 2015) as the optimizer and set coefficient \(\eta\) of entropy regularization term in Sinkhorn distance to 0.1 in all experiments. Other experimental hyper-parameters are provided in Appendix J. Sensitivity analysis of hyper-parameters is provided in Appendix I. A detailed description of distinct missing mechanisms, including MCAR, MAR, and MNAR, is provided in Appendix J. In this study, we let the missing rate mr be 0.2 or 0.5, which is consistent with the previous data imputation works (Yoon et al., 2018; Muzzle et al., 2020). We use the AUROC (Area Under the Receiver Operating Characteristic curve) and AUPRC (Area Under the Precision-Recall curve) to evaluate the detection performance. ALL experiments were conducted on 20 Cores Intel(R) Xeon(R) Gold 6248 CPU with one NVIDIA Tesla V100 GPU, CUDA 12.0. We report the average results of five runs.

### Experimental Results on Datasets with Manually Constructed Missing Values

Before presenting the numerical results, we show the effectiveness of the generated pseudo-abnormal samples learned for the Botnet dataset in Figure 5, where we directly let the latent space \(\mathcal{Z}\) be 2-D for convenient visualization. We see that the pseudo-abnormal samples cover the region of real abnormal samples, which matches our motivation and expectation.

The results of anomaly detection with missing data under the setting of MCAR are shown in Table 2 and more results under MCAR are provided in Appendix K. In Table 2, "Mean-Filling" denotes that the missing values are filled with feature means.

We have the following observations from the Table 2:

* The detection performance of "impute-then-detect" methods does not decrease with the increasing of missing rate from \(0.2\) to \(0.5\) in some cases (emphasized by underline), which indicates the adverse impact of imputation bias for the detection algorithm. The main reason is that a

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline  & dataset & field & features & instances & normal & abnormal \\ \hline  & Adult & income census & 14 & 30,162 & 22,658 & 7,508 \\ without & Botnet & cybersecurity & 115 & 40,607 & 13,113 & 27,494 \\ inherent & KDD & cybersecurity & 121 & 494,021 & 396,743 & 97,278 \\ missing & Arrhythmia & medical diagnosis & 274 & 452 & 320 & 132 \\ values & Speech & speech recognition & 400 & 3,686 & 3,625 & 61 \\  & Segertolge & cell analysis & 1,000 & 702 & 329 & 372 \\  & Uosskin & cell analysis & 25,334 & 610 & 323 & 378 \\ \hline \hline  & dataset & field & features & instances & missing samples rate & missing entries rate \\ \hline with & Titanic & pattern recognition & 9 & 891 & 79.46\% & 10.79\% \\ inherent & MovieLens1M & recommendation system & 498 & 6,040 & 100\% & 82.41\% \\ missing & Bladder & cell analysis & 23,341 & 2,500 & 100\% & 86.93\% \\ values & Seq2-Heart & cell analysis & 23,341 & 4,365 & 100\% & 88.51\% \\ \hline \end{tabular}
\end{table}
Table 1: Statistics of datasets. The “normal” and “abnormal” denote the number of normal and abnormal samples, respectively. “missing samples rate” means the proportion of samples with missing values and “missing entries rate” means the proportion of all missing values.

Figure 5: Two-dimensional visualization on Botnet.

lower missing rate implies a simpler imputation task, leading to a more pronounced imputation bias from normal data, which makes the abnormal data more "normal", thereby increasing the difficulty of detection for such two-stage methods.
* The "impute-then-detect" methods with "MissForest" (simple and shallow imputation algorithms) achieve better detection performance than those with "GAIN" (generative and deep imputation model) in most cases, suggesting that a sophisticated imputation module may not contribute positively to subsequent anomaly detection because the identical distribution assumption does not hold here. The outstanding recovery ability leads to a pronounced imputation bias and further affects the detection task.
* Compared with all baselines, ImAD achieves better detection performance in almost all cases. Besides, different from the "impute-then-detect" methods, the performance of ImAD increases with the changes of missing rate from \(0.5\) to \(0.2\) in all cases. This indicates that the imputation module of ImAD generalizes well on incomplete abnormal data and the generated pseudo-abnormal samples can alleviate the bias.

### Experimental Results on Datasets with Inherent Missing Values

We report experimental results on the four datasets with inherent missing values in Table 3, where the naive imputation methods "Zero-Filling" and 'Mean-Filling" are also considered. Observing Table 3, we notice that the naive imputation methods are insufficient for subsequent detection tasks when facing high missing rates and the imputation bias impacts the detection accuracy of "impute-then-detect" methods. Our ImAD outperforms all baselines in all cases. It indicates that our proposed method is practical and effective for real-world anomaly detection with missing data.

### Impact of Different Missing Mechanisms

Given a real dataset, the missing mechanism is usually unknown and difficult to estimate. It is expected that when the missing mechanism \(\mathbf{\tilde{M}}\) in generating (incomplete) pseudo-normal samples is closer to the missing mechanism \(\mathbf{M}\) in the real data, the performance of ImAD should be better. In this section, we analyze the impact of different \(\mathbf{\tilde{M}}\) on the detection performance of ImAD. Note that

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \hline \multirow{2}{*}{DI Methods} & \multirow{2}{*}{AD Methods} & \multicolumn{3}{c|}{KDD} & \multicolumn{3}{c|}{ADROC(\%)} & \multicolumn{3}{c|}{ADROC(\%)} & \multicolumn{3}{c}{ADROC(\%)} \\ \cline{3-10}  & & \(\mathrm{mr}=0.2\) & \(\mathrm{mr}=0.5\) & \(\mathrm{mr}=0.2\) & \(\mathrm{mr}=0.5\) & \(\mathrm{mr}=0.2\) & \(\mathrm{mr}=0.5\) & \(\mathrm{mr}=0.2\) & \(\mathrm{mr}=0.2\) & \(\mathrm{mr}=0.5\) \\ \hline \multirow{4}{*}{Mean-Filling} & 1-Forest & 74.1(1.92) & 57.44(4.791) & 81.19(1.31) & 64.35(5.95) & 96.63(0.56) & 56.01(0.84) & 57.99(0.59) & 52.95(0.72) \\  & Deep SVDD & 95.44(1.17) & 85.44(3.26) & 59.09(1.36) & 84.91(4.31) & 62.53(5.35) & 57.72(5.25) & 62.86(8.61) & 59.03(0.19) \\  & Neural.M & 96.63(1.65) & 87.39(1.27) & 88.69(1.36) & 88.10(2.87) & 60.37(2.50) & 54.64(1.08) & 63.20(2.86) & 57.43(1.16) \\  & DAD & 52.55(2.41) & 53.73(2.45) & 55.21(1.03) & 53.51(2.46) & 61.39(0.17) & 59.17(0.35) & 66.72(0.10) & 62.40(2.13) \\ \hline \multirow{4}{*}{MissForest} & 1-Forest & 94.90(1.95) & **93.73(1.15)** & 93.24(2.13) & 93.21(1.29) & 60.06(1.69) & 69.00(0.69) & 57.12(1.26) & 56.80(1.27) \\  & Deep SVDD & 93.58(2.46) & 91.85(4.59) & 85.72(7.95) & 88.79(1.29) & 62.33(4.86) & 61.21(2.24) & 58.31(2.91) & 55.85(1.72) \\  & NetraL.M & 94.01(2.72) & 92.68(2.49) & 83.71(7.95) & **94.88(2.86)** & 58.71(1.58) & 51.23(2.41) & 50.67(2.50) & 52.72(2.61) \\  & DAD & 70.65(4.61) & 60.80(1.08) & 76.37(2.58) & 62.07(0.08) & 64.39(1.5) & 63.02(2.02) & 68.68(0.31) & 64.80(1.03) \\ \hline \multirow{4}{*}{GAIN} & 1-Forest & 82.78(3.80) & 99.40(3.90) & 90.33(1.58) & 89.52(1.07) & 59.53(0.91) & 61.18(1.61) & 57.05(1.02) & 56.87(1.09) \\  & Deep SVDD & 88.68(4.87) & 84.45(4.98) & 88.36(3.42) & 85.55(6.74) & 88.65(4.34) & 65.44(2.40) & 57.61(4.24) & 59.55(2.34) \\  & Neural.M & 90.94(3.28) & 84.10(9.91) & 84.61(1.30) & 84.08(1.03) & 85.01(8.61) & 85.04(2.13) & 53.06(8.06) & 59.06(3.97) \\  & DAD & 70.34(8.20) & 90.80(0.09) & 72.29(1.99) & 94.49(0.44) & 62.10(8.05) & 62.60(0.18) & 68.39(0.42) & 68.48(0.21) \\ \hline \multirow{4}{*}{ImAD (Ours)} & **97.01**(0.33) & 90.78(1.35) & **95.96**(0.18) & 91.58(0.32) & **76.51**(1.22) & **71.19**(1.63) & **73.42**(2.08) & **71.50**(2.02) \\ \cline{2-10}  & \multirow{2}{*}{AD Methods} & \multicolumn{3}{c|}{ADROC(\%)} & \multicolumn{3}{c|}{ADROC(\%)} & \multicolumn{3}{c|}{ADROC(\%)} & \multicolumn{3}{c}{ADROC(\%)} \\ \cline{3-10}  & & \(\mathrm{mr}=0.2\) & \(\mathrm{mr}=0.5\) & \(\mathrm{mr}=0.2\) & \(\mathrm{mr}=0.5\) & \(\mathrm{mr}=0.2\) & \(\mathrm{mr}=0.5\) & \(\mathrm{mr}=0.2\) & \(\mathrm{mr}=0.5\) \\ \hline \multirow{4}{*}{Mean-Filling} & 1-Forest & 78.38(1.48) & 76.29(1.62) & 75.83(0.83) & 76.65(1.26) & 26.28(1.45) & 34.54(2.03) & 36.10(4.07) & 38.98(0.79) \\  & Deep SVDD & 66.22(1.89) & 62.06(4.67) & 77.16(1.86) & 66.36(0.39) & 53.90(5.43) & 52.80(1.57) & 57.25(2.42) & 53.75(2.67) \\  & Neutral.M & 79.02(1.91) & 74.87(2.51) & 81.02(1.97) & 76.79(2.34) & 94.54(3.91) & 50.65(4.58) & 59.70(2.26) & 51.81(2.87) \\  & DAD & 78.33(1.47) & 73.37(2.35) & 87.97(1.76) & 74.63for the synthetic incomplete data, we accurately know the missing mechanism. The experimental results are reported in Table 4. On real incomplete data, our method is robust to the setting of missing mechanism \(\tilde{\mathbf{M}}\) and has better overall performance when \(\tilde{\mathbf{M}}\) is MCAR. Therefore, based on Occam's Razor principle and the empirical results, we recommend using MCAR as the missing mechanism for the generated pseudo-abnormal samples when the real missing mechanism is unknown. On the other hand, as shown in Table 4, on synthetic incomplete data, detection performance degrades when \(\tilde{\mathbf{M}}\) is different from \(\mathbf{M}\).

### More Experimental Results

The appendices contain the following additional results: I. Performance gain from pseudo-abnormal samples (Appendix G); II. Influence of the constrained radii \(r_{1},r_{2}\) (Appendix H); III. Sensitivity analysis of hyperparameters (Appendix I); IV. Impact of different missing rates for training and test set(Appendix K); V. Results of MAR and MNAR (Appendix K).

## 5 Conclusion

This paper proposed ImAD, the first end-to-end unsupervised anomaly detection method on incomplete data. ImAD integrates data imputation with anomaly detection into a unified optimization objective and automatically generates pseudo-abnormal samples to alleviate the imputation bias. We theoretically proved the effectiveness of ImAD and empirically evaluated ImAD on multiple real-world datasets. The results showed that ImAD mitigates imputation bias from normal data and provides an effective solution for unsupervised anomaly detection in the presence of missing values. One limitation of this work is that we haven't considered the applications on incomplete image data and incomplete time series.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline \multirow{3}{*}{Dataset} & \multirow{3}{*}{Missing Mechanism M} & \multicolumn{5}{c|}{Missing Mechanism of Pseudo-Abnormal Samples \(\tilde{\mathbf{M}}\)} \\ \cline{3-8}  & & MCAR & \multicolumn{1}{c|}{MAR} & \multicolumn{1}{c|}{MNAR} \\ \cline{3-8}  & & AUROC(\%) & AUPRC(\%) & AUROC(\%) & AUROC(\%) & AUROC(\%) & AUPRC(\%) \\ \hline Titanic & Unknown & 82.09 & 81.39 & 79.06 & 77.08 & 80.50 & 79.17 \\ Movie\_ens1M & Unknown & 66.32 & 65.34 & 63.14 & 63.39 & 61.44 & 60.91 \\ Bladder & Unknown & 100.00 & 100.00 & 99.95 & 99.95 & 100.00 & 100.00 \\ Seq2\_Heart & Unknown & 96.62 & 96.40 & 96.79 & 96.60 & 95.56 & 94.41 \\ Adult & MCAR & 71.19 & 71.50 & 64.11 & 66.44 & 67.28 & 66.72 \\ Adult & MAR & 65.66 & 67.23 & 74.61 & 70.74 & 71.14 & 69.69 \\ Adult & MNAR & 70.69 & 69.17 & 68.35 & 68.78 & 71.60 & 68.97 \\ \hline \end{tabular}
\end{table}
Table 4: Performance comparison of different missing mechanisms \(\tilde{\mathbf{M}}\).

\begin{table}
\begin{tabular}{l|l|l|c|c|c|c|c|c|c} \hline \multirow{2}{*}{DI Methods} & \multirow{2}{*}{AD Methods} & \multirow{2}{*}{ATOBCC(\%)} & \multirow{2}{*}{AUPRC(\%)} & \multirow{2}{*}{AUPRC(\%)} & \multirow{2}{*}{AUPRC(\%)} & \multirow{2}{*}{AUPRC(\%)} & \multirow{2}{*}{AUROC(\%)} & \multirow{2}{*}{AUPRC(\%)} \\ \cline{3-8}  & & & & & & & & & \\ \hline \multirow{4}{*}{Zero-Filling} & \multirow{4}{*}{L-Forest} & 77.44(0.29) & 77.74(0.27) & 73.55(0.43) & 4.087(0.32) & 3.22(0.72) & 39.23(0.41) & 4.788(0.63) & 46.250(5.4) \\  & Deep SVDD & 54.03(0.18) & 53.49(3.21) & 34.30(8.98) & 43.44(0.58) & 68.30(2.73) & 36.22(4.24) & 71.12(9.09) & 65.18(0.95) \\  & Neutral. AD & 94.90(5.81) & 47.10(1.31) & 39.29(2.79) & 44.50(7.26) & 63.20(3.93) & 54.86(0.84) & 82.84(3.37) & 78.762(1.71) \\  & DPA & 79.50(9.91) & 79.07(1.40) & 44.10(4.36) & 46.12(2.29) & 99.90(0.0) & 99.90(0.0) & 99.51(0.32) & 93.80(0.69) \\ \hline \multirow{4}{*}{Mean-Filling} & \multirow{4}{*}{L-Forest} & 79.60(0.65) & 78.64(0.94) & 36.30(0.75) & 41.47(0.46) & 44.06(3.12) & 46.63(3.88) & 54.69(2.32) & 51.98(2.07) \\  & Deep SVDD & 53.87(0.098) & 52.41(0.36) & 48.18(0.26) & 46.64(0.93) & 81.97(3.44) & 78.93(7.38) & 75.16(1.51) & 71.12(0.65) \\  & Neutral. AD & 65.16(0.20) & 63.63(0.23) & 63.74(0.86) & 42.15(0.46) & 99.10(3.94) & 99.82(2.64) & 89.78(3.66) & 66.75(75.80) \\  & DPAD & 67.02(0.66) & 69.85(0.85) & 47.74(0.48) & 48.52(2.89) & 97.52(1.43) & 97.88(1.15) & 77.93(10.20) & 76.37(9.95) \\ \hline \multirow{4}{*}{MisForest} & \multirow{4}{*}{L-Forest} & 72.70(2.82) & 78.50(0.30) & 33.64(1.04) & 41.45(0.25) & 44.53(2.84) & 26.84(1.94) & 46.54(3.88) & 58.63(1.0) \\  & Deep SVDD & 60.46(8.95) & 69.78(3.73) & 56.40(4.85) & 53.70(4.92) & 94.53(5.91) & 97.94(9.20) & 94.20(4.29) & 32.30(0.45) \\ \cline{1-1}  & \multirow{4}{*}{Deep SVDD} & \multirow{4}{*}{L-Forest} & 54.63(4.42) & 52.13(13.16) & 57.14(1.18) & 55.07(1.72) & 66.41(4.66) & 68.01(5.25) & 91.80(1.18) & 90.87(1.16) \\ \cline{1-1}  & & DPAD & 88.18(1.45) & 70.01(1.03) & 47.50(4.40) & 48.49(3.03) & 96.96(1.30) & 79.21(0.71) & 78.03(5.20) & 4.675(3.33) \\ \hline \multirow{4}{*}{GAN} & \multirow{4}{*}{L-Forest} & 79.46(0.79) & 78.69(0.96) & 64.84(1.01) & 62.61(0.99) & 45.77(2.39) & 46.72(1.91) & 46.62(3.31) & 58.82(3.23) \\ \cline{1-1}  & & Deep SVDD & 70.59(5.44) & 66.43(1.74) & 58.99(1.81) & 56.68(2.11) & 95.43(1.18) & 96.78(0.47) & 93.93(0.37) & 91.54(0.67) \\ \cline{1-1}  & & Neural. AD & 53.71(2.74) & 51.55(2.25) & 57.72(2.81) & 51.42(2.38) & 65.30(3.39) & 65.68(4.69) & 94.18(0.76) & 90.79(1.36) \\ \cline{1-1}  & & DPAD & 78.12(0.97) & 77.11(1.04) & 59.98(1.86) & 58.98(1.08) & 59.72(1.56) & 74.99(2.76) & 73.78(1.61) \\ \hline \multirow{2}{*}{ImAD (Ours)} & **82.09**(0.99) & **81.39**(0.84) & **66.32**(1.36) & **65.34**(1.35) & **100.00** & **1000.00** & **96.62**(0.11) & **96.40**(0.19) \\ \hline \end{tabular}
\end{table}
Table 3: Detection accuracy (AUROC and AUPRC (%, mean and std)

## Acknowledgments

This work was supported by the General Program of Natural Science Foundation of Guangdong Province under Grant No.2024A1515011771, the National Natural Science Foundation of China under Grant No.62376236, the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (2023B1212010001), Shenzhen Science and Technology Program ZDSYS20230626091302006, Shenzhen Stability Science Program 2023, and Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone Project (No.HZQSWS-KCCYB-2024016). The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## References

* Bartlett et al. (2017) Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. _Advances in neural information processing systems_, 30, 2017.
* Becker and Kohavi (1996) Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20.
* Breunig et al. (2000) Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jorg Sander. Lof: identifying density-based local outliers. In _Proceedings of the 2000 ACM SIGMOD international conference on Management of data_, pages 93-104, 2000.
* Cai and Fan (2022) Jinyu Cai and Jicong Fan. Perturbation learning based anomaly detection. _Advances in Neural Information Processing Systems_, 35, 2022.
* Candes and Recht (2012) Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization. _Communications of the ACM_, 55(6):111-119, 2012.
* Chen et al. (2023) Zhenpeng Chen, Jie M Zhang, Max Hort, Mark Harman, and Federica Sarro. Fairness testing: A comprehensive survey and analysis of trends. _ACM Transactions on Software Engineering and Methodology_, 2023.
* Cuturi (2013) Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in neural information processing systems_, 26, 2013.
* Dempster et al. (1977) Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. _Journal of the Royal Statistical Society: Series B (Methodological)_, 39(1):1-22, 1977.
* Fa et al. (2021) Botao Fa, Ting Wei, Yuan Zhou, Luke Johnston, Xin Yuan, Yanran Ma, Yue Zhang, and Zhangsheng Yu. Gapclust is a light-weight approach distinguishing rare cells from voluminous single cell expression profiles. _Nature Communications_, 12(1):4197, 2021.
* Fan and Chow (2017) Jicong Fan and Tommy Chow. Deep learning based matrix completion. _Neurocomputing_, 266:540-549, 2017.
* Fan and Chow (2018) Jicong Fan and Tommy WS Chow. Non-linear matrix completion. _Pattern Recognition_, 77:378-394, 2018.
* Fan et al. (2019) Jicong Fan, Lijun Ding, Yudong Chen, and Madeleine Udell. Factor group-sparse regularization for efficient low-rank matrix recovery. _Advances in neural information processing Systems_, 32, 2019.
* Fan et al. (2020) Jicong Fan, Yuqian Zhang, and Madeleine Udell. Polynomial matrix completion for missing data imputation and transductive learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 3842-3849, 2020.
* Fan et al. (2022) Jicong Fan, Tommy W. S. Chow, and S. Joe Qin. Kernel-based statistical process monitoring and fault detection in the presence of missing data. _IEEE Transactions on Industrial Informatics_, 18(7):4477-4487, 2022. doi: 10.1109/TII.2021.3119377.
* Fan et al. (2024) Jicong Fan, Rui Chen, Zhao Zhang, and Chris H.Q. Ding. Neuron-enhanced autoencoder matrix completion: Theory and practice. In _The Twelfth International Conference on Learning Representations_, 2024.
* Fan et al. (2019)Dazhi Fu, Zhao Zhang, and Jicong Fan. Dense projection for anomaly detection. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 8398-8408, 2024.
* Gondara and Wang (2018) Lovedeep Gondara and Ke Wang. Mida: Multiple imputation using denoising autoencoders. In _Advances in Knowledge Discovery and Data Mining: 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne, VIC, Australia, June 3-6, 2018, Proceedings, Part III 22_, pages 260-272. Springer, 2018.
* Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.
* Han et al. (2021) Soyeon Caren Han, Taejun Lim, Siqi Long, Bernd Burgstaller, and Josiah Poon. Glocal-k: Global and local kernels for recommender systems. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 3063-3067, 2021.
* Han et al. (2023) Xiao Han, Lu Zhang, Yongkai Wu, and Shuhan Yuan. Achieving counterfactual fairness for anomaly detection. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_, pages 55-66. Springer, 2023.
* Kingma and Ba (2015) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _Proceedings of the International Conference on Learning Representations_, 2015.
* Li et al. (2019) Steven Cheng-Xian Li, Bo Jiang, and Benjamin Marlin. Misgan: Learning from incomplete data with generative adversarial networks. _arXiv preprint arXiv:1902.09599_, 2019.
* Lichman (2013) M. Lichman. Uci machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.
* Liu et al. (2008) Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In _2008 eighth ieee international conference on data mining_, pages 413-422. IEEE, 2008.
* Mayer et al. (2019) Imke Mayer, Aude Sportisse, Julie Josse, Nicholas Tierney, and Nathalie Vialaneix. R-miss-tastic: a unified platform for missing values methods and workflows. _arXiv preprint arXiv:1908.04822_, 2019.
* Mazumder et al. (2010) Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. _The Journal of Machine Learning Research_, 11:2287-2322, 2010.
* Meidan et al. (2018) Yair Meidan, Michael Bohadana, Yael Mathov, Yisroel Mirsky, Dominik Breitenbacher, Asaf, and Asaf Shabtai. detection_of_tot_botnet_attacks_n_baiot. UCI Machine Learning Repository, 2018. DOI: https://doi.org/10.24432/C5RC8J.
* Muzellec et al. (2020) Boris Muzellec, Julie Josse, Claire Boyer, and Marco Cuturi. Missing data imputation using optimal transport. In _International Conference on Machine Learning_, pages 7130-7140. PMLR, 2020.
* Pevny (2016) Tomas Pevny. Loda: Lightweight on-line detector of anomalies. _Machine Learning_, 102:275-304, 2016.
* Pigott (2001) Therese D Pigott. A review of methods for missing data. _Educational research and evaluation_, 7(4):353-383, 2001.
* Qiu et al. (2021) Chen Qiu, Timo Pfrommer, Marius Kloft, Stephan Mandt, and Maja Rudolph. Neural transformation learning for deep anomaly detection beyond images. In _International Conference on Machine Learning_, pages 8703-8714. PMLR, 2021.
* Rayana (2016) Shebuti Rayana. Odds library, 2016. URL https://odds.cs.stonybrook.edu.
* Royston and White (2011) Patrick Royston and Ian R White. Multiple imputation by chained equations (mice): implementation in Stata. _Journal of statistical software_, 45:1-20, 2011.
* Rubin (1976) Donald B Rubin. Inference and missing data. _Biometrika_, 63(3):581-592, 1976.
* Rubin et al. (2018)Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In _International conference on machine learning_, pages 4393-4402. PMLR, 2018.
* Sarda et al. (2023) Kisan Sarda, Amol Yerudkar, and Carmen Del Vecchio. Unsupervised anomaly detection for multivariate incomplete data using gan-based data imputation: A comparative study. In _2023 31st Mediterranean Conference on Control and Automation (MED)_, pages 55-60. IEEE, 2023.
* Schaum et al. (2018) Nicholas Schaum, Jim Karkanias, Norma F Neff, Andrew P May, Stephen R Quake, Tony Wyss-Coray, Spyros Darmanis, Joshua Batson, Olga Botvinnik, Michelle B Chen, et al. Single-cell transcriptomics of 20 mouse organs creates a tabula muris: The tabula muris consortium. _Nature_, 562(7727):367, 2018.
* Schlomer et al. (2010) Gabriel L Schlomer, Sheri Bauman, and Noel A Card. Best practices for missing data management in counseling psychology. _Journal of Counseling psychology_, 57(1):1, 2010.
* Scholkopf et al. (2001) Bernhard Scholkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. Estimating the support of a high-dimensional distribution. _Neural computation_, 13(7):1443-1471, 2001.
* Segerstolpe et al. (2016) Asa Segerstolpe, Athanasia Palasantza, Pernilla Eliasson, Eva-Marie Andersson, Anne-Christine Andreasson, Xiaoyan Sun, Simone Picelli, Alan Sabirsh, Maryam Clausen, Magnus K Bjursell, et al. Single-cell transcriptome profiling of human pancreatic islets in health and type 2 diabetes. _Cell metabolism_, 24(4):593-607, 2016.
* Shani and Gunawardana (2011) Guy Shani and Asela Gunawardana. Evaluating recommendation systems. _Recommender systems handbook_, pages 257-297, 2011.
* Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.
* Stekhoven and Buhlmann (2012) Daniel J Stekhoven and Peter Buhlmann. Missforest--non-parametric missing value imputation for mixed-type data. _Bioinformatics_, 28(1):112-118, 2012.
* Tashiro et al. (2021) Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. _Advances in Neural Information Processing Systems_, 34:24804-24816, 2021.
* Usoskin et al. (2015) Dmitry Usoskin, Alessandro Furlan, Saiful Islam, Hind Abdo, Peter Lonnerberg, Daohua Lou, Jens Hjerling-Leffler, Jesper Haeggsrom, Olga Kharchenko, Peter V Kharchenko, et al. Unbiased classification of sensory neuron types by large-scale single-cell rna sequencing. _Nature neuroscience_, 18(1):145-153, 2015.
* Xiao et al. (2025) Feng Xiao, Jianfeng Zhou, Kunpeng Han, Haoyuan Hu, and Jicong Fan. Unsupervised anomaly detection using inverse generative adversarial networks. _Information Sciences_, 689:121435, 2025. ISSN 0020-0255. doi: https://doi.org/10.1016/j.ins.2024.121435.
* Yang and Cai (2017) Zhihai Yang and Zhongmin Cai. Detecting abnormal profiles in collaborative filtering recommender systems. _Journal of Intelligent Information Systems_, 48:499-518, 2017.
* Yoon et al. (2018) Jinsung Yoon, James Jordon, and Mihaela Schaar. Gain: Missing data imputation using generative adversarial nets. In _International conference on machine learning_, pages 5689-5698. PMLR, 2018.
* Zemicheal and Dietterich (2019) Tadesse Zemicheal and Thomas G. Dietterich. Anomaly detection in the presence of missing values for weather data quality control. In _Proceedings of the 2nd ACM SIGCAS Conference on Computing and Sustainable Societies_, COMPASS '19, page 65-73, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367141. doi: 10.1145/3314344.3332490. URL https://doi.org/10.1145/3314344.3332490.
* Zhang and Zhang (2018) Lihua Zhang and Shihua Zhang. Comparison of computational methods for imputing single-cell rna-sequencing data. _IEEE/ACM transactions on computational biology and bioinformatics_, 17(2):376-389, 2018.
* Zhang et al. (2018)Yunhe Zhang, Yan Sun, Jinyu Cai, and Jicong Fan. Deep orthogonal hypersphere compression for anomaly detection. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=cJ84oE4m9Q.
* Zong et al. (2018) Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In _International conference on learning representations_, 2018.

Analysis for Sampling in Latent Space

To project the normal data onto target distribution and generate pseudo-abnormal data, our proposed method involves sampling from two latent distributions \(\mathcal{D}_{\mathbf{z}},\mathcal{D}_{\mathbf{\hat{z}}}\sim\mathcal{N}(\mathbf{0},\sigma^{2}\mathbf{I}_{d})\). In this section, we provide a lower bound for the constrained sampling radius \(r\), given a sampling probability \(p\) that provides a probabilistic guarantee, namely, **to obtain \(N\) points from a truncated Gaussian, we need to sample \(N/p\) times from a Gaussian distribution**. Subsequently, we perform sampling within the truncated Gaussian distribution with a constrained radius \(r\).

For target distribution \(\mathcal{D}_{\mathbf{z}}\), we expect that it is compact and can be easily sampled, in which the compactness is to ensure a clear and reliable decision boundary between normal and abnormal data. Therefore, we select truncated Gaussian from \(\mathcal{N}(\mathbf{0},\sigma^{2}\mathbf{I}_{d})\) as target distribution \(\mathcal{D}_{\mathbf{z}}\) and bound \(\mathcal{D}_{\mathbf{z}}\) in a \(d\)-dimensional radius \(r\) hyperball centering at origin. For radius \(r\), we have the following proposition.

**Proposition A.1**.: _Let \(F_{d}\) denote the cumulative distribution function (CDF) of the chi-square distribution \(\chi^{2}(d)\). For a given probability \(0<p<1\), when \(r\geq\sigma\sqrt{F_{d}^{-1}(p)}\), the sampling probability in \(\mathcal{D}_{\mathbf{z}}\) satisfies \(P(\|\mathbf{z}\|^{2}<r^{2})\geq p\) where \(\mathbf{z}=[z_{1},z_{2},\cdots,z_{d}]\) and \(z_{1},\ldots,z_{d}\stackrel{{ i.i.d.}}{{\sim}}\mathcal{N}(0, \sigma^{2})\)._

Proof.: We have

\[z_{1},\ldots,z_{d}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathcal{N} (0,\sigma^{2})\Longrightarrow\frac{z_{1}}{\sigma},\ldots,\frac{z_{d}}{\sigma} \stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathcal{N}(0,1) \Longrightarrow\frac{\sum_{i=1}^{d}z_{i}^{2}}{\sigma^{2}}\sim\chi^{2}(d).\] (15)

Let \(Y=\frac{\sum_{i=1}^{d}z_{i}^{2}}{\sigma^{2}},\ \text{we get}\)

\[\begin{split}& P\left(Y<F_{d}^{-1}(p)\right)=p\\ &\Longrightarrow P\left(\frac{\sum_{i=1}^{d}z_{i}^{2}}{\sigma^{2}}<F_{d}^{-1}(p) \right)=p\\ &\Longrightarrow P\left(\sum_{i=1}^{d}z_{i}^{2}<\sigma^{2}\cdot F_{d}^{-1}(p) \right)=p\\ &\Longrightarrow P\left(\|\mathbf{z}\|^{2}<\left(\sigma\sqrt{F_{d}^{-1}(p)} \right)^{2}\right)=p.\end{split}\] (16)

Therefore, \(r\geq\sigma\sqrt{F_{d}^{-1}(p)}\Longrightarrow P\left(\|\mathbf{z}\|^{2}<r^{2} \right)\geq p\). 

According to the analysis in Section 3.2, we select truncated Gaussian from \(\mathcal{N}(\mathbf{0},\tilde{\sigma}^{2}\mathbf{I}_{d})\) as target distribution \(\mathcal{D}_{\mathbf{\hat{z}}}\) and bound \(\mathcal{D}_{\mathbf{\hat{z}}}\) between two \(d\)-dimensional hyperspheres with radii \(r_{1},r_{2}\) respectively, centering at origin, where \(r_{2}>r_{1}\). For radius \(r_{1},r_{2}\), we have the following proposition.

**Proposition A.2**.: _Let \(F_{d}\) denote the cumulative distribution function (CDF) of the chi-square distribution \(\chi^{2}(d)\). For a given probability \(0<p<1\), when \(r_{1}\leq\tilde{\sigma}\sqrt{F_{d}^{-1}(p_{1})},r_{2}\geq\tilde{\sigma}\sqrt{F _{d}^{-1}(p_{2})}\) and satisfies \(p=p_{2}-p_{1}\), the sampling probability in \(\mathcal{D}_{\mathbf{\hat{z}}}\) satisfies \(P(r_{1}^{2}<\|\mathbf{z}\|^{2}<r_{2}^{2})\geq p\) where \(\mathbf{z}=[z_{1},z_{2},\cdots,z_{d}]\) and \(z_{1},\ldots,z_{d}\stackrel{{ i.i.d.}}{{\sim}}\mathcal{N}(0, \tilde{\sigma}^{2})\)._

Proof.: According the proof for Proposition A.1, we have

\[r\geq\tilde{\sigma}\sqrt{F_{d}^{-1}(p)}\implies P(\|\mathbf{z}\|^{2}<r^{2}) \geq p.\] (17)

Therefore,

\[\begin{split}& r_{1}\leq\tilde{\sigma}\sqrt{F_{d}^{-1}(p_{1})} \implies P(\|\mathbf{z}\|^{2}<r_{1}^{2})\leq p_{1},\text{and}\\ & r_{2}\geq\tilde{\sigma}\sqrt{F_{d}^{-1}(p_{2})}\implies P(\| \mathbf{z}\|^{2}<r_{2}^{2})\geq p_{2}.\end{split}\] (18)

Therefore, we get \(P\left(\|\mathbf{z}\|^{2}<r_{2}^{2}\right)-P(\|\mathbf{z}\|^{2}<r_{1}^{2})=P \left(r_{1}^{2}<\|\mathbf{z}\|^{2}<r_{2}^{2}\right)\geq p_{2}-p_{1}=p\).

As shown in Figure 3, we set radius \(r_{1}\) of \(\mathcal{D}_{\tilde{\mathbf{z}}}\) equals to radius \(r\) of \(\mathcal{D}_{\mathbf{z}}\). Also, we maintain the settings \(r_{1}=r\) in all experiments to make the introduced pseudo-abnormal samples are not far from the normal data.

## Appendix B Proof for Theorem 3.2

Proof.: Recall that \(g_{\phi}\) was defined as

\[g_{\phi}(\mathbf{z})=\sigma_{L}(\mathbf{W}_{L}^{g}(\cdots\sigma_{2}(\mathbf{W }_{2}^{g}(\sigma_{1}(\mathbf{W}_{1}^{g}\mathbf{z})))\cdots)).\] (19)

Then for any \(\mathbf{z},\tilde{\mathbf{z}}\in\mathbb{R}^{d}\), we have

\[\|g_{\phi}(\mathbf{z})-g_{\phi}(\tilde{\mathbf{z}})\|\] (20) \[= \|\sigma_{L}(\mathbf{W}_{L}^{g}(\cdots\sigma_{2}(\mathbf{W}_{2}^{ g}(\sigma_{1}(\mathbf{W}_{1}^{g}\mathbf{z})))\cdots)-\sigma_{L}(\mathbf{W}_{L}^{g}( \cdots\sigma_{2}(\mathbf{W}_{2}^{g}(\sigma_{1}(\mathbf{W}_{1}^{g}\tilde{ \mathbf{z}})))\cdots))\|\] \[\leq \rho\|\mathbf{W}_{L}^{g}(\cdots\sigma_{2}(\mathbf{W}_{2}^{g}( \sigma_{1}(\mathbf{W}_{1}^{g}\mathbf{z})))\cdots)-\mathbf{W}_{L}^{g}(\cdots \sigma_{2}(\mathbf{W}_{2}^{g}(\sigma_{1}(\mathbf{W}_{1}^{g}\tilde{\mathbf{z}}) ))\cdots)\|\] \[\leq \rho\|\mathbf{W}_{L}^{g}\|_{\sigma}\|\sigma_{L-1}(\cdots\sigma_{2 }(\mathbf{W}_{2}^{g}(\sigma_{1}(\mathbf{W}_{1}^{g}\mathbf{z})))\cdots)-\sigma _{L-1}(\cdots\sigma_{2}(\mathbf{W}_{2}^{g}(\sigma_{1}(\mathbf{W}_{1}^{g} \tilde{\mathbf{z}})))\cdots)\|\] \[\leq \rho^{L}\left(\prod_{l=1}^{L}\|\mathbf{W}_{l}^{g}\|_{\sigma} \right)\|\mathbf{z}-\tilde{\mathbf{z}}\|\] \[\leq \rho^{L}\alpha_{g}^{L}\|\mathbf{z}-\tilde{\mathbf{z}}\|.\]

This finished the proof for part (a) of the theorem. The proof for part (b) is similar and omitted here for simplicity. 

## Appendix C Proof for Theorem 3.3

We define the following model class

\[\mathcal{F}=\{\pi\circ f_{\theta}\circ h_{\psi}:\mathbb{R}^{d}\to\mathbb{R}\}\] (21)

where \(f_{\theta}\) and \(h_{\psi}\) satisfy the Assumption 3.1 and \(\pi\) is the sum of squares of the outputs of \(f_{\theta}\circ h_{\psi}\), corresponding to the definition of the anomaly score, meaning \(s(\mathbf{x})^{2}=\pi(f_{\theta}(h_{\psi}(\mathbf{x})))\). The following lemma (proved by Appendix E) provides the covering number bound of \(\mathcal{F}\).

**Lemma C.1**.: _Under Assumption 3.1, for any \(\epsilon>0\), it holds that_

\[\ln\mathcal{N}\left(\epsilon,\mathcal{F}_{\tilde{\mathbf{X}}},\|\cdot\|_{F} \right)\leq\frac{\|\tilde{\mathbf{X}}\|_{F}^{2}\ln(\bar{d}^{2})\rho^{4L-2}(2 \vartheta)^{2}d\kappa^{2}\zeta^{2}}{\epsilon^{2}}\]

_where \(\kappa^{2}=\alpha_{f}^{2L}\alpha_{h}^{2L}\) and \(\zeta^{2}=\left(1+L\left(\frac{b_{f}}{\alpha_{f}}\right)^{2/3}+L\left(\frac{ b_{h}}{\alpha_{h}}\right)^{2/3}\right)^{3}\)._

Suppose the loss function is \(\mu\)-Lipschitz, it follows from Lemma C.1 that

\[\ln\mathcal{N}\left(\epsilon,\ell\circ\mathcal{F}_{\tilde{\mathbf{ X}}},\|\cdot\|_{F}\right)\leq \ln\mathcal{N}\left(\frac{\epsilon}{\mu},\mathcal{F}_{\tilde{ \mathbf{X}}},\|\cdot\|_{F}\right)\] (22) \[\leq \frac{\mu^{2}\|\tilde{\mathbf{X}}\|_{F}^{2}\ln(\bar{d}^{2})\rho^{ 4L-2}(2\vartheta)^{2}d\kappa^{2}\zeta^{2}}{\epsilon^{2}}\]

With the covering number, we can bound the Rademacher complexity by the following Dudley entropy integral:

**Lemma C.2** (Lemma A.5 of (Bartlett et al., 2017), reformulated).: _Let \(\mathcal{F}_{\gamma}:=\ell\circ\mathcal{F}_{\tilde{\mathbf{X}}}\) be a real-valued function class taking values in \([0,\gamma]\), and assume that \(\mathbf{0}\in\mathcal{F}_{\gamma}\). Then_

\[\mathcal{R}_{\tilde{\mathbf{X}}}(\mathcal{F}_{\gamma})\leq\inf_{\alpha>0} \left(\frac{4\alpha\gamma}{\sqrt{n}}+\frac{12}{n}\int_{\gamma\alpha}^{\gamma \sqrt{n}}\sqrt{\ln\mathcal{N}\left(\epsilon,\mathcal{F}_{\gamma},\|\cdot\| \right)}\,d\epsilon\right).\]Combining (22) and Lemma C.2, and letting \(R^{2}:=\mu^{2}\|\dot{\mathbf{X}}\|_{F}^{2}\ln(2\bar{d}^{2})\rho^{4L}(2\vartheta) ^{2}d\kappa^{2}\zeta^{2}\), we obtain

\[\mathcal{R}_{\mathcal{G}}(\mathcal{F}_{\gamma}) \leq\inf_{\alpha>0}\left(\frac{4\alpha\gamma}{\sqrt{n}}+\frac{12 }{n}\int_{\gamma\alpha}^{\gamma\sqrt{n}}\frac{R}{\epsilon}d\epsilon\right)\] \[=\inf_{\alpha>0}\left(\frac{4\alpha\gamma}{\sqrt{n}}+\frac{12R}{n }\ln\left(\frac{\sqrt{n}}{\alpha}\right)\right)\] \[\leq\frac{4\gamma}{n}+\frac{12R\ln n}{n}\] (23)

where we have chosen \(\alpha=\frac{1}{\sqrt{n}}\).

The following lemma is the classical generalization error bound based on the Rademacher complexity.

**Lemma C.3**.: _Given hypothesis function space \(\mathcal{F}\) mapping \(\mathbf{x}\in\mathcal{X}\) to \(\mathbb{R}^{d}\) and \(\gamma>0\), define \(\mathcal{F}_{\gamma}:=\{(\mathbf{x},y)\mapsto l_{\gamma}(f(\mathbf{x},y):f\in \mathcal{F}\}\), where \(l_{\gamma}(\hat{y},y)\leq\gamma\). Then, with probability at least \(1-\delta\) over a sample \(\mathbf{X}\) of size \(n\), every \(f\in\mathcal{F}\) satisfies \(L_{\gamma}(f)\leq\hat{L}_{\gamma}(f)+2\mathcal{R}_{\mathbf{X}}(\mathcal{F}_{ \gamma})+3\gamma\sqrt{\frac{\ln\left(2/\delta\right)}{2n}}\)._

Now using Lemma C.3 and inequality (23), we have

\[L_{\gamma}(f)\leq\hat{L}_{\gamma}(f)+\frac{8\gamma+24R\ln n}{n}+3\gamma\sqrt{ \frac{\ln\left(2/\delta\right)}{2n}}.\] (24)

For Theorem 3.3(a), the loss function is \(\ell(\hat{y},y)=|\hat{y}-y|=\hat{y}\), where \(y\equiv 0\) and \(\hat{y}=s(\hat{\mathbf{x}})^{2}\) due to the definition of the anomaly score. This also means that the Lipschitz constant \(\mu\) of \(\ell\) is \(1\). We assume that the squared anomaly scores on the normal training data are upper bounded by \(\gamma\). Let \(\hat{L}_{\gamma}(f)=\frac{1}{n}\sum_{i=1}^{n}s(\hat{\mathbf{x}}_{i})^{2}\) and \(L_{\gamma}(f)=\mathbb{E}_{\mathcal{D}_{\mathbf{x}}}[s(\mathbf{x})^{2}]\). It follows from (24) that

\[\mathbb{E}_{\mathcal{D}_{\mathbf{x}}}[s(\hat{\mathbf{x}})^{2}]\leq\frac{1}{n} \sum_{i=1}^{n}s(\hat{\mathbf{x}}_{i})^{2}+\frac{8\gamma+24R\ln n}{n}+3\gamma \sqrt{\frac{\ln\left(2/\delta\right)}{2n}}.\] (25)

Let \(r_{1}\) be the threshold determined by the training data to judge whether a sample is anomalous or not. We let

\[\frac{1}{n}\sum_{i=1}^{n}s(\hat{\mathbf{x}}_{i})^{2}+\frac{8\gamma+24R\ln n}{ n}+3\gamma\sqrt{\frac{\ln\left(2/\delta\right)}{2n}}=r_{1}^{2}\] (26)

and solve for \(\delta\):

\[\delta=2\exp\left(-\frac{2n\left(r_{1}^{2}-\bar{s}^{2}-\frac{8\gamma+24R\ln n }{n}\right)^{2}}{9\gamma^{2}}\right)\] (27)

where \(\bar{s}^{2}=\frac{1}{n}\sum_{i=1}^{n}s(\hat{\mathbf{x}}_{i})^{2}\). Then we rewrite (25) as

\[\mathbb{E}_{\mathcal{D}_{\mathbf{x}}}[s(\hat{\mathbf{x}})^{2}]\leq r_{1}^{2},\] (28)

which holds with probability at least \(1-\delta\). In other words,

\[\mathbb{P}\left[\mathbb{E}_{\mathcal{D}_{\mathbf{x}}}[s(\hat{\mathbf{x}})^{2} ]>r_{1}^{2}\right]\leq\delta,\] (29)

which implies

\[\mathbb{P}\left[\mathbb{E}_{\mathcal{D}_{\mathbf{x}}}[s(\hat{\mathbf{x}})]>r_{ 1}\right]\leq\delta,\] (30)

because both \(s(\hat{\mathbf{x}})\) and \(r_{1}\) are nonnegative. We complete the proof for Theorem 3.3(a).

For Theorem 3.3(b), \(R^{2}:=\mu^{2}\|\ddot{\hat{\mathbf{X}}}\|_{F}^{2}\ln(2\bar{d}^{2})\rho^{4L}(2 \vartheta)^{2}d\kappa^{2}\zeta^{2}\). We consider the following loss function

\[\ell(\ddot{\hat{\mathbf{x}}})=\left|r_{2}^{2}-s(\ddot{\hat{\mathbf{x}}})^{2} \right|.\] (31)

The Lipshitz constant of this loss is \(1\). Suppose the loss is upper bounded by \(\tilde{\gamma}\). Similar to the proof for Theorem 3.3 (a), we have

\[\mathbb{E}_{\mathcal{D}_{\hat{\mathbf{x}}}}[\ell(\ddot{\hat{\mathbf{x}}})]\leq \frac{1}{n}\sum_{i=1}^{n}\ell(\ddot{\hat{\mathbf{x}}}_{i})+\frac{8\tilde{ \gamma}+24R\ln n}{n}+3\tilde{\gamma}\sqrt{\frac{\ln\left(2/\delta\right)}{2n}}.\] (32)Let

\[\frac{1}{n}\sum_{i=1}^{n}\ell(\tilde{\mathbf{x}}_{i})+\frac{8\tilde{\gamma}+24R \ln n}{n}+3\tilde{\gamma}\sqrt{\frac{\ln\left(2/\delta\right)}{2n}}=\tau\] (33)

and solve for \(\delta\):

\[\delta=2\exp\left(-\frac{2n\left(\tau-\bar{\varepsilon}-\frac{8\tilde{\gamma}+ 24R\ln n}{n}\right)^{2}}{9\tilde{\gamma}^{2}}\right)\] (34)

where \(\bar{\varepsilon}=\frac{1}{n}\sum_{i=1}^{n}\ell(\tilde{\mathbf{x}}_{i})\). Then we rewrite (32) as

\[\mathbb{E}_{\mathcal{D}_{\tilde{\mathbf{k}}}}[|r_{2}^{2}-s(\tilde{\mathbf{x}}) ^{2}|]\leq\tau,\] (35)

which holds with probability at least \(1-\delta\). In other words,

\[\mathbb{P}\left[\mathbb{E}_{\mathcal{D}_{\tilde{\mathbf{k}}}}[|r_{2}^{2}-s( \tilde{\mathbf{x}})^{2}|]\leq\tau\right]\geq 1-\delta.\] (36)

Since \(|r_{2}^{2}-s(\tilde{\mathbf{x}})^{2}|\leq\tau\) implies that \(s(\tilde{\mathbf{x}})^{2}\geq r_{2}^{2}-\tau\), letting \(\tau=r_{2}^{2}-r_{1}^{2}\), we arrive at

\[\mathbb{P}\left[\mathbb{E}_{\mathcal{D}_{\tilde{\mathbf{k}}}}[s(\tilde{ \mathbf{x}})^{2}]\geq r_{1}^{2}\right]\geq 1-\delta,\] (37)

which also means

\[\mathbb{P}\left[\mathbb{E}_{\mathcal{D}_{\tilde{\mathbf{k}}}}[s(\tilde{ \mathbf{x}})]\geq r_{1}\right]\geq 1-\delta,\] (38)

because both \(s(\tilde{\mathbf{x}})\) and \(r_{1}\) are nonnegative. Renaming \(\delta\) as \(\tilde{\delta}\), we finish the proof. Note that in the theorem of the main paper, we have put outside the constant \(4\) in \(R^{2}\). That's why the constant \(24\) becomes \(48\).

## Appendix D Proof for Theorem 3.4

Proof.: The anomalous samples (denoted by \(\tilde{\mathbf{x}}\)) drawn from space out of \(\mathcal{D}_{\tilde{\mathbf{x}}}\) are much easier to detect. The reason is that, in the latent space, these anomalous samples (denoted by \(\tilde{\mathbf{z}}^{\prime}\)) are sufficiently far from the normal region (\(\mathcal{D}_{\mathbf{z}}\)). According to the definition of \(\mathcal{D}_{\tilde{\mathbf{x}}}\), we have \(\tilde{\mathbf{z}}^{\prime}=f\circ h(\tilde{\mathbf{x}})=f\circ h\circ g( \tilde{\mathbf{z}})\). According to the definition of the anomaly score, we need to measure \(\|\tilde{\mathbf{z}}^{\prime}-0\|\). We have

\[\|\tilde{\mathbf{z}}^{\prime}-0\|\] \[= \|f\circ h\circ g(\tilde{\mathbf{z}})-f\circ h\circ g(0)+f\circ h \circ g(0)-0\|\] \[\geq \|f\circ h\circ g(\tilde{\mathbf{z}})-f\circ h\circ g(0)\|-\|f \circ h\circ g(0)-0\|\] \[\geq c|\tilde{\mathbf{z}}-0\|-\|f\circ h\circ g(0)-0\|\] \[\geq cr_{2}-\varepsilon\]

where \(c\) is some constant depending on the networks and we have assumed that \(\|f\circ h\circ g(0)-0\|\leq\varepsilon\). Now suppose that \(r_{2}\) is sufficiently large such that \(cr_{2}-\varepsilon>r_{1}\), then \(\|\tilde{\mathbf{z}}^{\prime}-0\|>r_{1}\), meaning that \(\tilde{\mathbf{z}}^{\prime}\) is outside the inner hypersphere and hence can be detected successfully. Nevertheless, determining an exact \(c\) is still an open problem for neural networks. 

## Appendix E Proof for Lemma C.1

Proof.: In \(\mathcal{F}\), \(\pi\) can be regarded as an additional layer of the neural network, where the activation function for each element of \(\mathbf{z}\) is square, the weight matrix for the output is a vector consisting of \(d\) ones, and the activation function for the final output is linear. Thus, \(\pi\circ f_{\theta}\circ g_{\psi}\) has \(2L+1\) layers. For the square activation function, the Lipschitz constant is \(2\vartheta\). For the final output layer, the spectral norm of the weights is \(\sqrt{d}\), which is equal to the \(\ell_{2.1}\) norm because it is a vector.

**Lemma E.1** (Theorem 3.3 of [1]).: _Let fixed nonlinearities \((\sigma_{1},\ldots,\sigma_{L})\) and reference matrices \((M_{1},\ldots,M_{L})\) be given, where \(\sigma_{i}\) is \(\rho_{i}\)-Lipschitz and \(\sigma_{i}(0)=0\). Let spectral norm bounds \((s_{1},\ldots,s_{L})\), and matrix \((2,1)\) norm bounds \((b_{1},\ldots,b_{L})\) be given. Let data matrix \(X\in\mathbb{R}^{n\times d}\) be given, where the \(n\) rows correspond to data points. Let \(\mathcal{H}_{X}\) denote the family of matrices obtained by evaluating \(X\) with all choices of network \(F_{\mathcal{A}}:\mathcal{H}_{X}:=\left\{F_{\mathcal{A}}\left(X^{T}\right):\mathcal{ A}=\left(A_{1},\ldots,A_{L}\right),\left\|A_{i}\right\|_{\sigma}\leq s_{i}, \left\|A_{i}^{\top}-M_{i}^{\top}\right\|_{2,1}\leq b_{i}\right\}\), where each matrix has dimension at most \(W\) along each axis. Then for any \(\epsilon>0\),

\[\ln\mathcal{N}\left(\mathcal{H}_{X},\epsilon,\|\cdot\|_{F}\right)\leq\frac{\| X\|_{F}^{2}\ln\left(2W^{2}\right)}{\epsilon^{2}}\left(\prod_{j=1}^{L}s_{j}^{2} \rho_{j}^{2}\right)\left(\sum_{i=1}^{L}\left(\frac{b_{i}}{s_{i}}\right)^{2/3} \right)^{3}.\]

Then using Lemma E.1 and Assumption 3.1, we can obtain

\[\ln\mathcal{N}\left(\epsilon,\mathcal{F}_{\tilde{\mathbf{X}}},\|\cdot\|_{F} \right)\leq\frac{\|\tilde{\mathbf{X}}\|_{F}^{2}\ln(2\bar{d}^{2})\rho^{4L-2}(2 \vartheta)^{2}d\kappa\zeta}{\epsilon^{2}}\]

where \(\kappa=\alpha_{f}^{2L}\alpha_{h}^{2L}\) and \(\zeta=\left(1+L\left(\frac{b_{f}}{\alpha_{f}}\right)^{2/3}+L\left(\frac{b_{h} }{\alpha_{h}}\right)^{2/3}\right)^{3}\). This finished the proof. 

## Appendix F Time Complexity Analysis

The notations used in the complexity analysis are explained as follows:

* \(n,m\) to denote the number of samples of the training phase and inference phase, respectively.
* Missforest is a well-known data imputation algorithm based on random forest (\(\mathcal{O}(t_{1}\cdot v\cdot n\log n)\)) where \(t_{1}\) denotes the number of trees, \(v\) denotes the number of attributes.
* \(T,T_{g},T_{d},T_{ae},T_{oc}\) denote the iterations of corresponding methods.
* \(\bar{L}\) and \(\bar{d}\) denote the number of layers of the neural network and the maximum width of the layers of the corresponding models, respectively.
* \(t_{2}\) denotes the number of trees of I-Forest and \(t\) is the maximum iterations of the Sinkhorn algorithm.
* \(p,\psi,K\) denote the key parameters of the corresponding methods.

## Appendix G Gain of Detection Performance from Pseudo-Abnormal Samples

### ImAD Benefits from Learning Pseudo-abnormal Samples

In this section, we explore the influences of introduced pseudo-abnormal samples for detection performance. On all datasets used in our experiments, we remove the pseudo-abnormal samples in the training process and only use incomplete normal data to train ImAD. The experimental results are shown in Table 6 and Table 7, where the detection performance of ImAD is improved in all the cases when introducing pseudo-abnormal samples into the training process. This indicates that the generated pseudo-abnormal samples are practical and effective for anomaly detection on incomplete data.

### ImAD's Pseudo-abnormal Samples Can Improve "impute-then-detect" Methods

Furthermore, we save the generated pseudo-abnormal data from the training process of ImAD on Titanic and Bladder, and then we add them into the training set for data imputation of "impute-then-detect" methods. The related results are provided in Table 8. We see that the pseudo-abnormal

\begin{table}
\begin{tabular}{c|c|c|c} \hline DI Methods & AD Methods & Time Complexity (Training) & Time Complexity (Inference) \\ \hline \multirow{5}{*}{\(\mathcal{O}(T\cdot p(t_{1}\cdot v\cdot n\log n)+t_{2}\cdot\psi\log v)\)} & \(\mathcal{O}(p(t_{1}\cdot v\cdot n\log n)+t_{2}\cdot m\log v)\) \\  & Deep SVDD & \(\mathcal{O}(T\cdot p(t_{1}\cdot v\cdot n\log n)+(T_{ae}+T_{oc})(nd^{2}L+n))\) & \(\mathcal{O}(p(t_{1}\cdot v\cdot m\log n)+(md^{2}L+m))\) \\  & Neutral, AD & \(\mathcal{O}(T\cdot p(t_{1}\cdot v\cdot n\log n)+T(nd^{2}L+n\cdot K))\) & \(\mathcal{O}(p(t_{1}\cdot v\cdot m\log n)+(md^{2}L+m\cdot K))\) \\  & IPAD & \(\mathcal{O}(T\cdot p(t_{1}\cdot v\cdot n\log n)+T(nd^{2}L+n^{2}))\) & \(\mathcal{O}(p(t_{1}\cdot v\cdot m\log n)+(md^{2}L+m))\) \\ \hline \multirow{5}{*}{\(\mathcal{O}(T_{g}+T_{d})n^{2}L\)} & I-Forest & \(\mathcal{O}((T_{g}+T_{d})n^{2}L+t_{2}\cdot\log v)\) & \(\mathcal{O}(md^{2}L+(md^{2}L+m))\) \\  & Deep SVDD & \(\mathcal{O}((T_{g}+T_{d})nd^{2}L+(T_{ac}+T_{oc})(nd^{2}L+n))\) & \(\mathcal{O}(md^{2}L+(md^{2}L+m))\) \\  & Neutral, AD & \(\mathcal{O}((T_{g}+T_{d})nd^{2}L+T(nd^{2}L+n\cdot K))\) & \(\mathcal{O}(md^{2}L+(md^{2}L+nm))\) \\ \hline \multicolumn{2}{c|}{ImAD (Ours)} & \(\mathcal{O}(T(nd^{2}L+t\cdot n^{2}))\) & \(\mathcal{O}(md^{2}L+m)\) \\ \hline \end{tabular}
\end{table}
Table 5: The time complexity of training and inference.

samples learned by our ImAD can improve the performance of "impute-then-detect" methods. The reason is that with the pseudo-abnormal data, the imputation algorithms, i.e., MissForest and GAIN, generalize better on the test data. These results further confirm the effectiveness of the generative module of our ImAD.

## Appendix H Influence of Constrained Radii \(r_{1},r_{2}\) for Detection Performance

In this section, we explore the influences of constrained radii \(r_{1},r_{2}\) for detection performance. We change the latent dimension \(d=\{4,8,16,32,64,128,256,512\}\) and carry out related experiments. Detailed experimental settings and results are provided in Table 9 and Figure 6 and Figure 7, respectively.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{Settings} & \multicolumn{2}{c|}{AUROC(\%)} & \multicolumn{2}{c}{AUPRC(\%)} \\ \cline{3-5}  & & mrr=0.2 & mr=0.5 & mrr=0.2 & mrr=0.5 \\ \hline \multirow{2}{*}{Adult} & ImAD w/o pseudo-abnormal samples & 65.30 (4.36) & 68.15 (5.66) & 67.49 (3.98) & 69.50 (4.35) \\  & ImAD & 76.51 (2.12) & 71.19 (1.63) & 73.42 (2.08) & 71.50 (2.02) \\ \hline \multirow{2}{*}{KDD} & ImAD w/o pseudo-abnormal samples & 96.00 (2.03) & 91.50 (1.59) & 94.70 (1.03) & 92.09 (1.14) \\  & ImAD & 97.01 (0.33) & 90.78 (1.35) & 95.96 (0.18) & 91.58 (0.32) \\ \hline \multirow{2}{*}{Botnet} & ImAD w/o pseudo-abnormal samples & 99.78 (0.05) & 99.38 (0.16) & 99.78 (0.05) & 99.40 (0.15) \\  & ImAD & 99.71 (0.22) & 99.53 (0.25) & 99.68 (0.24) & 99.58 (0.20) \\ \hline \multirow{2}{*}{Arrhythmia} & ImAD w/o pseudo-abnormal samples & 78.28 (4.03) & 79.04 (0.73) & 76.98 (3.55) & 78.27 (1.88) \\  & ImAD & 82.24 (1.76) & 81.76 (1.19) & 83.74 (1.85) & 83.37 (1.36) \\ \hline \multirow{2}{*}{Speech} & ImAD w/o pseudo-abnormal samples & 53.22 (3.62) & 47.28 (4.27) & 53.47 (4.66) & 49.92(3.13) \\  & ImAD & 61.94 (2.77) & 58.66 (1.40) & 60.43 (3.33) & 58.13 (1.48) \\ \hline \multirow{2}{*}{Segersolope} & ImAD w/o pseudo-abnormal samples & 97.29 (0.93) & 96.53 (1.15) & 96.62 (0.90) & 96.60 (1.16) \\  & ImAD & 99.14 (0.88) & 96.86 (0.67) & 98.98 (1.18) & 96.85 (0.54) \\ \hline \multirow{2}{*}{Usoskin} & ImAD w/o pseudo-abnormal samples & 79.07 (2.13) & 76.67 (1.27) & 79.43 (0.94) & 78.57 (2.48) \\  & ImAD & 84.95 (1.29) & 79.23 (2.49) & 85.48 (2.34) & 80.06 (3.40) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Gain of detection performance of ImAD from pseudo-abnormal samples on datasets with manually constructed missing values.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Datasets & Settings & AUROC(\%) & AUPRC(\%) \\ \hline \multirow{2}{*}{Titanic} & ImAD w/o pseudo-abnormal samples & 76.14 (1.66) & 75.46 (1.32) \\  & ImAD & 82.09 (0.99) & 81.39 (0.84) \\ \hline \multirow{2}{*}{MovieLens1M} & ImAD w/o pseudo-abnormal samples & 60.62(1.35) & 60.29(1.44) \\  & ImAD & 66.32 (1.36) & 65.34 (1.35) \\ \hline \multirow{2}{*}{Bladder} & ImAD w/o pseudo-abnormal samples & 99.90 (0.21) & 99.87 (0.29) \\  & ImAD & 1.00 (0.00) & 1.00 (0.00) \\ \hline \multirow{2}{*}{Seq2-Heart} & ImAD w/o pseudo-abnormal samples & 95.19 (0.71) & 94.27 (1.18) \\  & ImAD & 96.62 (0.11) & 96.40 (0.19) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Gain of detection performance of ImAD from pseudo-abnormal samples on datasets with inherent missing values.

[MISSING_PAGE_EMPTY:21]

Figure 8: Sensitivity analysis of hyperparameters \(\alpha,\beta,\lambda\) on Adult dataset.

Figure 7: The detection performance on Arrhythmia, Speech, Segerstolpe and Usoskin with different latent dimension.

Detailed Experimental Implementations

### Dataset Description

* **Adult2**[Becker and Kohavi, 1996] is from the 1994 Census Income database with 14 variables including both categorical and continuous variables. The samples of income \(\leq\) 50K are regarded as normal data, and the samples of income \(>\) 50K are regarded as abnormal data. Data preparation follows the previous work [Han et al., 2023].
* **KDD3[Lichman, 2013]** is the KDDCUP99 10 percent dataset from the UCI repository and contains 121 variables including both categorical and continuous variables. The attack samples are regarded as normal data, and the non-attack samples are regarded as abnormal data.
* **Arrhythmia4**[Rayana, 2016] is an ECG dataset. It was used to identify arrhythmic samples in five classes and contains 452 instances with 274 attributes. Footnote 2: https://archive.ics.uci.edu/dataset/2/adult
* **Speech5**[Rayana, 2016] consists of 3686 segments of English speech spoken with different accents and is represented by 400-dimensional so-called i-vectors which are widely used state-of-the-art features for speaker and language recognition. Footnote 3: https://kdd.ics.uci.edu/databases/kddcup99/
* **Segerstolpe6**[Segerstolpe et al., 2016] is a scRNA-seq dataset of human pancreas islets which includes six cell types: "alpha", "beta", "delta", "ductal", "endotheliali" and "gamma". In our experiments, "alpha" is regarded as normal data and "beta" is regarded as abnormal data. Footnote 3: http://odds.cs.stonybrook.edu/arrhythmia-dataset/
* **Usoskin7**[Usoskin et al., 2015] is a dataset employed for the analysis of sensory neuron cells, specifically originating from the mouse lumbar dorsal root ganglion. The dataset encompasses four distinct cell types: non-peptideocicceptor cells (NP), peptidergic nocicceptor cells (PEP), neurofilament-containing cells (NF), and tyrosine hydroxylase-containing cells (TH). In our experiment, TH is regarded as normal data and PEP is abnormal data. Footnote 7: https://dchast.gao-lab.org/download
* **Botnet8**[Meidan et al., 2018] is a public botnet datasets for the IoT: it was gathered from 9 commercial IoT devices authentically infected by Mirai and BASHLITE. There are 7,062,606 instances in the original datasets. In our experiments, we use "Ecobee_Thermostat" subset of the original data, in which "benign_traffic" is regarded as normal data and "gafgyt_attacks" is regarded as abnormal data. The "gafgyt_attacks" has five attack types and we randomly select 1,000 samples from each type as abnormal data of the test set. Footnote 8: https://archive.ics.uci.edu/dataset/442/detection+of+iot+botnet+attacks+n+baiot
* **Titanic9**[Chen et al., 2023] is a classification dataset to detect the survival on the Titanic. In our experiments, we use nine features including "gender, ticket, cabin, age, sibsp, parch, fare, embarked and pclass". The instances that did not survive are considered normal samples and those that survived are considered abnormal(or unusual) samples. Footnote 9: https://www.kaggle.com/c/titanic/data
* **MovieLens1M10** Han et al. [2021] contains 1,000,209 anonymous rating of approximately 3,900 movies made by 6,040 MovieLens users who joined MovieLens in 2000. Due to the missing rate of the original dataset is near 95%, we remove some columns with quite high missing rate and obtain a new dataset with 82% missing rate. Since the age of all samples is divided into seven groups, we chose the middle five groups (18 < age < 56) as normal samples and the remaining as abnormal samples. Footnote 10: https://grouplens.org/datasets/movieLens/1m/
* **Bladder11** is a cell transcriptome data from the model organism Mus musculus, in which contains 4 cell types (bladder cell, bladder urothelial cell, endothelial cell and leukocyte). We use the instance from "bladder cell" as normal samples and those from " leukocyte" as abnormal samples. Footnote 11: https://cblast.gao-lab.org/download
* **Seq2-Heart12**[Schaum et al., 2018] is a single cell transcriptome data from the model organism Mus musculus, containing nearly 100,000 cells from 20 organs and tissues. There are 8 cell types in this data. We use the instances with "fibroblast" type as normal samples and those with "myofibroblast" type as abnormal samples. Footnote 12: https://cblast.gao-lab.org/download

### Missing Mechanisms

In this work, we evaluate the detection performance of all the baselines under the three distinct missing mechanisms and we follow the previous work [Muzellec et al., 2020] to set the missing value generation mechanism.

A detailed explanation of our implementation is provided as follows.

* **MCAR**: missing completely at random if the missingness is independent of the data. In our implementation, each entry is masked according to the realization of a Bernoulli random variable with parameter \(p=\{0.2,0.5\}\).
* **MAR**: missing at random if the missingness depends only on the observed values. In the MAR setting, for all experiments, a fixed subset of variables that cannot have missing values is sampled. Then, the entries from the remaining variables are masked according to a logistic model with random weights, which takes the non-missing variables as inputs. A bias term is fitted using line search to attain the desired proportion of missing values.
* **MNAR**: missing not at random if the missingness depends on both the observed values and the unobserved values. In the MNAR setting, first, we sample a subset of variables whose values in the lower and upper p-th percentiles are masked according to a Bernoulli random variable, and the values in-between are left not missing.

### Sampling in Target Distribution

In our experiments, we select two truncated Gaussian distribution \(\mathcal{N}(\mathbf{0},\sigma^{2}\mathbf{I}_{d})\) with different \(\sigma\) as target distribution \(\mathcal{D}_{\mathbf{z}},\mathcal{D}_{\mathbf{\bar{z}}}\) and set \(\sigma=0.5,\sigma=1.0\) respectively. For target distribution \(\mathcal{D}_{\mathbf{z}}\sim\mathcal{N}(\mathbf{0},0.5^{2}\cdot\mathbf{I}_{d})\), according to the Proposition A.1, we set constrained radius \(r=0.5\sqrt{F_{d}^{-1}(p)}\) where \(d\) denotes the latent dimension and set \(p=0.9\). Similarity, for target distribution \(\mathcal{D}_{\mathbf{\bar{z}}}\sim\mathcal{N}(\mathbf{0},\mathbf{I}_{d})\), we set \(r_{1}=r\) and \(r_{2}=\sqrt{F_{d}^{-1}(p)}\) and set \(p=0.9\).

### All Baselines

For the data imputation method used in our experiments, GAIN 13, MissOT 14, we use official code and the hyperparameters are fine-tuned as suggested in the original paper. For MissForest, we use _missingpy_15 which is a library for missing data imputation in Python to implement the MissForest [Stekhoven and Buhlmann, 2012] algorithm. For anomaly detection method, Deep SVDD 16[Rubin, 1976], Neutral AD 17[Qiu et al., 2021] and DPAD [Fu et al., 2024], we use official code and the hyperparameters are fine-tuned as suggested in the original paper. For Isolation Forest, we use _scikit-learn_18 to implement the Isolation Forest [Liu et al., 2008] algorithm.

Footnote 13: https://github.com/jsyoon0823/GAIN

Footnote 14: https://github.com/BorisMuzellec/MissingDataOT

Footnote 15: https://pypi.org/project/missingpy/

Footnote 16: https://github.com/lukasruff/Deep-SVDD-PyTorch

Footnote 17: https://github.com/boschresearch/NeuTraL-AD

### Hyper-parameter Settings

The hyperparameters used in our experiments are provided in Table 10.

## Appendix K More Experimental Results

In this section, we conduct experiments on the Speech dataset with a missing rate \(\text{mr}\in\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8\}\). The related results are visualized in Figure 9, where the detection performance of "impute-then-detect" methods does not degrade and some of them even improve with the increasing of missing rate from 0.1 to 0.8. Moreover, our proposed method outperforms all baselines in almost all cases.

In some real scenarios, it is possible that the missing rates of training and test sets are not equal. In this section, we conduct related experiments on the Speech dataset. In these experiments, we keep the missing rate \(\text{mr}=0.5\) on the training set and change the missing rate from 0.2 to 0.8 on the test set. We visualize the experimental results in Figure 10.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline Datasets & Missing rate & Latent dimension & Learning rate & \(\alpha\) & \(\beta\) & \(\lambda\) \\ \hline Adult & \(\begin{matrix}\text{mr}=0.2\\ \text{mr}=0.5\end{matrix}\) & 4 & 0.0002 & 5 & 20 & 1 \\ \hline Botnet & \(\begin{matrix}\text{mr}=0.2\\ \text{mr}=0.5\end{matrix}\) & 32 & 0.0001 & 1 & 1 & 1 \\ \hline KDD & \(\begin{matrix}\text{mr}=0.2\\ \text{mr}=0.5\end{matrix}\) & 32 & 0.0001 & 1 & 5 & 1 \\ \hline Arrhythmia & \(\begin{matrix}\text{mr}=0.2\\ \text{mr}=0.5\end{matrix}\) & 128 & 0.0001 & 1 & 1 & 1 \\ \hline Speech & \(\begin{matrix}\text{mr}=0.2\\ \text{mr}=0.5\end{matrix}\) & 128 & 0.0005 & 0.2 & 0.1 & 1 \\ \hline Segerstolpe & \(\begin{matrix}\text{mr}=0.2\\ \text{mr}=0.5\end{matrix}\) & 128 & 0.0001 & 1 & 1 & 1 \\ \hline Usoskin & \(\begin{matrix}\text{mr}=0.2\\ \text{mr}=0.5\end{matrix}\) & 128 & 0.0001 & 0.2 & 0.2 & 0.2 \\ \hline Titanic & - & 4 & 0.0001 & 0.1 & 0.1 & 0.01 \\ \hline MovieLens1M & - & 128 & 0.005 & 1 & 1 & 1 \\ \hline Bladder & - & 128 & 0.0001 & 1 & 0.01 & 1 \\ \hline Seq2-Heart & - & 128 & 0.0001 & 1 & 1 & 1 \\ \hline \end{tabular}
\end{table}
Table 10: Hyperparameters settings of the proposed method on all datasets.

Figure 10: The performance fluctuation with the changes of missing rate from 0.2 to 0.8 on the test set of Speech. “MeanF” and “MissF” denotes Mean-Filling and MissForest, respectively.

Figure 9: The performance fluctuation with the changes of missing rate from 0.1 to 0.8 on the Speech dataset. “MeanF” and “MissF” denotes Mean-Filling and MissForest, respectively.

[MISSING_PAGE_FAIL:26]

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline \multirow{2}{*}{DI Methods} & \multirow{2}{*}{AD Methods} & \multicolumn{2}{c|}{AUROC} & \multicolumn{2}{c}{AUPRC} \\ \cline{3-6}  & & \(\mathrm{mr}\) = 0.2 & \(\mathrm{mr}\) = 0.5 & \(\mathrm{mr}\) = 0.2 & \(\mathrm{mr}\) = 0.5 \\ \hline \multirow{4}{*}{MissForest} & I-Forest & 60.53(1.40) & 60.24(1.05) & 56.84(0.62) & 57.54(1.07) \\  & Deep SVDD & 54.90(7.71) & 57.54(4.33) & 63.74(2.27) & 62.48(2.19) \\ \cline{1-1}  & Neutral AD & 53.07(1.26) & 50.82(2.35) & 55.41(1.01) & 52.52(1.01) \\ \cline{1-1}  & DPAD & 65.36(0.44) & 63.23(0.22) & 69.19(0.30) & 64.50(0.25) \\ \hline \multirow{4}{*}{MissOT\({}_{\mathrm{(MLP)}}\)} & I-Forest & 44.78(2.68) & 38.62(1.52) & 45.72(0.95) & 43.23(1.01) \\ \cline{1-1}  & Deep SVDD & 45.77(8.47) & 50.29(6.28) & 54.04(2.57) & 50.90(2.43) \\ \cline{1-1}  & Neutral AD & 49.87(1.07) & 49.38(1.07) & 50.20(0.42) & 48.37(0.81) \\ \cline{1-1}  & DPAD & 54.96(3.19) & 54.10(2.50) & 53.49(1.85) & 51.98(1.80) \\ \hline \multicolumn{1}{c}{} & ImAD (Ours) & **73.73**(3.57) & **72.35**(1.53) & **71.60**(0.74) & **68.97**(0.31) \\ \hline \hline \end{tabular}
\end{table}
Table 14: Detection performance in terms of AUROC and AUPRC (% mean and std) on Adult with MNAR. The best result in each case is marked in **bold**.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline \multirow{2}{*}{DI Methods} & \multirow{2}{*}{AD Methods} & \multicolumn{2}{c|}{AUROC} & \multicolumn{2}{c}{AUPRC} \\ \cline{3-6}  & & \(\mathrm{mr}\) = 0.2 & \(\mathrm{mr}\) = 0.5 & \(\mathrm{mr}\) = 0.2 & \(\mathrm{mr}\) = 0.5 \\ \hline \multirow{4}{*}{MissForest} & I-Forest & 60.53(1.40) & 60.24(1.05) & 56.84(0.62) & 57.54(1.07) \\  & Deep SVDD & 54.90(7.71) & 57.54(4.33) & 63.74(2.27) & 62.48(2.19) \\ \cline{1-1}  & Neutral AD & 53.07(1.26) & 50.82(2.35) & 55.41(1.01) & 52.52(1.01) \\ \cline{1-1}  & DPAD & 65.36(0.44) & 63.23(0.22) & 69.19(0.30) & 64.50(0.25) \\ \hline \multirow{4}{*}{MissOT\({}_{\mathrm{(MLP)}}\)} & I-Forest & 44.78(2.68) & 38.62(1.52) & 45.72(0.95) & 43.23(1.01) \\ \cline{1-1}  & Deep SVDD & 45.77(8.47) & 50.29(6.28) & 54.04(2.57) & 50.90(2.43) \\ \cline{1-1}  & Neutral AD & 49.87(1.07) & 49.38(1.07) & 50.20(0.42) & 48.37(0.81) \\ \cline{1-1}  & DPAD & 54.96(3.19) & 54.10(2.50) & 53.49(1.85) & 51.98(1.80) \\ \hline \multicolumn{1}{c}{} & ImAD (Ours) & **73.73**(3.57) & **72.35**(1.53) & **71.60**(0.74) & **68.97**(0.31) \\ \hline \hline \end{tabular}
\end{table}
Table 13: Detection performance in terms of AUROC and AUPRC (% mean and std) on Adult with MNAR. The best result in each case is marked in **bold**.

## NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See the abstract part. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Appendix K. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: See Section 3.4 and Appendix A,B,C. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems.

* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Section 4.1 and Appendix I. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All the datasets used are publicly available and we provide the download URLs (see Appendix I) for each dataset. We provide experimental code in the supplementary materials. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 4.1 and Appendix I. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Section 4.2 and 4.3. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All details are included in the paper. Guidelines: ** The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.

* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See the Section 4.1 and Appendix I. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification:

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.