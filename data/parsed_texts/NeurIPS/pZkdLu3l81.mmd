# Toward Information Theoretic Active Inverse Reinforcement Learning

 Ondrej Bajgar

University of Oxford

&Dewi Sid William Gould

Alan Turing Institute

&Jonathon Liu

Alan Turing Institute

&Oliver Newcombe

Independent

&Oliver Newcombe

University of Oxford

&Rohan Narayan Langford Mitta

Independent

&Jack Golden

University of Oxford

###### Abstract

As AI systems become increasingly autonomous, aligning their decision-making to human preferences is essential. In domains like autonomous driving or robotics, it is impossible to write down the reward function representing these preferences by hand. Inverse reinforcement learning (IRL) offers a promising approach to infer the unknown reward from demonstrations. However, obtaining human demonstrations can be costly. Active IRL addresses this challenge by strategically selecting the most informative scenarios for human demonstration, reducing the amount of required human effort. Where most prior work allowed querying the human for an action at one state at a time, we motivate and analyse scenarios where we collect longer trajectories. We provide an information-theoretic acquisition function, propose an efficient approximation scheme, and illustrate its performance through a set of gridworld experiments as groundwork for future work expanding to more general settings.

## 1 Introduction

Stuart Russell suggested three principles for the development of beneficial artificial intelligence: its only objective is realizing human preferences, it is initially uncertain about these preferences, and its ultimate source of information about them is human behavior [1]. _Apprenticeship learning_ via Bayesian _inverse reinforcement learning_ (IRL) can be understood as a possible operationalization of these principles: Bayesian IRL starts with a prior distribution over reward functions representing initial uncertainty about human preferences. It then combines this prior with _demonstration_ data from a human expert acting approximately optimally with respect to the unknown reward, to produce a posterior distribution over rewards. In apprenticeship learning, this posterior over rewards is then used to produce a policy that should perform well with respect to the unknown reward function.

However, getting human demonstrations requires scarce human time. Also, many risky situations where we would wish AI systems to behave especially reliably may be rare in these demonstration data. Bayesian active learning can help with both by giving queries to a human demonstrator that are likely to bring the most information about the reward. Most prior methods for active IRL [2, 3, 4] queried the expert for action annotations of particular isolated states. However, in domains such as autonomous driving with a high frequency of actions, it can be much more natural for the human to provide whole trajectories - say, to drive for a while in a simulator - than to annotate a large collection of unrelated snapshots. There is one previous paper on _active IRL with full trajectories_[5] suggesting a heuristic acquisition function whose shortcomings can, however, completely prevent learning. We instead suggest using the principled tools of Bayesian active learning for the task.

The article provides the following contributions: we formulate the problem of active IRL with full expert trajectories and adapt the expected information gain (EIG) acquisition function to this setting. We then provide an algorithm approximating the EIG and present experiments showing its superior performance relative to random sampling and two other baselines in gridworlds. We consider this initial investigation in tabular settings a stepping stone toward algorithms for more general settings.

## 2 Task formulation

Let \(\mathcal{M}(\xi)=(\mathcal{S},\mathcal{A},p_{\xi},r,\gamma,t_{\max},\rho_{\xi})\) be a parameterized Markov decision process (MDP), where \(\mathcal{S}\) and \(\mathcal{A}\) are finite state and action spaces respectively, \(p_{\xi}:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{P}(\mathcal{S})\) is the transition function where \(\mathcal{P}(\mathcal{S})\) is a set of probability measures over \(\mathcal{S}\), \(r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is an (expected) reward function,1\(\gamma\in(0,1)\) is a discount rate, \(t_{\max}\in\mathbb{N}\cup\{\infty\}\) is the time horizon, and \(\rho_{\xi}\) is the initial state distribution. The parameter \(\xi\) will be used to set up the environment in active learning. Due to space limitations here, we present experiments where \(\xi=s_{0}\) deterministically chooses an initial state, but our method can be used also for choosing the transition dynamics.

Footnote 1: Our formulation permits for the reward to be stochastic. However, our expert model (1) depends on the rewards only via the optimal Q-function, which in turn depends only on the expectated reward. Thus, the demonstrations can only ever give us information about the expectation. Throughout the paper, the learnt reward function can be interpreted either as modeling a deterministic reward, or an expectation of a stochastic reward.

We assume we are initially uncertain about the reward \(r\), and our initial knowledge is captured by a prior distribution \(p(r)\) over rewards, which is a distribution over \(\mathbb{R}^{|\mathcal{S}||\mathcal{A}|}\) - a space of vectors representing the reward associated with each state-action pair. We also have access to an expert that, given an instance \(\mathcal{M}(\xi_{i})\) of the MDP, can produce a trajectory \(\tau_{i}=\big{(}(s_{0}^{i},a_{0}^{i}),\dots,(s_{n_{i}}^{i},a_{n_{i}}^{i})\big{)}\), where \(s_{0}^{i}\sim\rho_{\xi_{i}}\), \(s_{t+1}\sim p_{\xi_{i}}(\cdot|s_{t},a_{t})\), and

\[\pi_{E}^{\xi}(a_{t}|s_{t})=\frac{\exp(\beta Q_{\xi}^{*}(s_{t},a_{t}))}{\sum_{ a^{\prime}\in\mathcal{A}}\exp(\beta Q_{\xi}^{*}(s_{t},a^{\prime}))}\,\] (1)

which is called a _Boltzmann-rational_ policy, given the optimal Q function \(Q_{\xi_{i}}^{*}\) and a hyperparameter \(\beta\) expressing how close to optimal the expert behaviour is (where \(\beta=0\) corresponds to fully random behaviour and \(\beta\rightarrow+\infty\) would yield the optimal policy).

Figure 1: (a) shows an illustrative gridworld and its true rewards. The lower left corner has a ”jail” state with negative reward from which an agent cannot leave. The starred green state is the terminal ”goal” state with a large positive reward. The brown, blue, and red states are ”mud”, ”water”, and ”lava” type states respectively, whose rewards are unknown to the IRL agent. The IRL agent tries to learn the rewards of these three state types from expert demonstrations. (b) shows the learned distributions over the rewards of the ”mud”, ”water”, and ”lava” state types respectively, at some particular step of the active learning process. These learned reward distributions are used to calculate the EIG of obtaining another expert demonstration starting from each given state, shown in (c). In this case, a demonstration starting in the bottom right state gives the most information about the unknown reward parameters.

The task of _Bayesian active inverse reinforcement learning_ is to sequentially query the expert to provide demonstrations in environments \(\xi_{1},\dots,\xi_{N}\) to gain maximum information about the unknown reward. We start with a (possibly empty) set of expert trajectories \(\mathcal{D}_{0}\) and then, at each step of active learning, we choose a parameter \(\xi_{i}\) for the MDP, from which we get the corresponding expert trajectory \(\tau_{i}\). We then update our demonstration dataset to \(\mathcal{D}_{i}=\mathcal{D}_{i-1}\cup\tau_{i}\), and the distribution over rewards to \(p(r|\mathcal{D}_{i})\), which we again use to select the most informative environment setup \(\xi_{i+1}\) in the next step. We repeat until we exhaust our limited demonstration budget \(N\).

Our goal can be operationalized as minimizing the entropy of the posterior distribution over rewards, once all expert demonstrations have been observed. This is equivalent to maximizing the log likelihood of the true parameter value in expectation, or to maximizing the mutual information between the demonstrations and the reward. We call this the _information-theoretic objective_.

For the _apprenticeship-learning objective_, we use the final posterior \(p(r|\mathcal{D}_{N})\) to produce an _apprentice policy_\(\pi^{A}:=\operatorname*{argmax}_{\pi}\mathbb{E}_{r}[\mathbb{E}_{r}[\sum_{s_{ i},a_{t}\in\tau}\gamma^{t}r(s_{t},a_{t})]]\) maximizing the expected return, where \(\tau\) is a trajectory on a known target setup \(\xi_{\text{target}}\) with \(s_{0}\sim\rho_{\xi_{\text{target}}}\), \(s_{t+1}\sim p_{\xi_{\text{target}}}(\cdot|s_{t},a_{t})\) and \(a_{t}=\pi(s_{t})\).

## 3 Method

Our goal at each step is to select an environment setup \(\xi\) that will produce the most information in expectation. In _Bayesian experimental design_ (BED) [6], especially Bayesian optimization [7], this is often framed in terms of an _acquisition function_ that for each \(\xi\) estimates how useful it would be to select, i.e. we would like to select \(\xi\) that maximizes the acquisition function.

We use the acquisition function most common in BED, the _expected information gain_ (EIG):

\[EIG_{n}(\xi)=\mathbb{E}_{r|\mathcal{D}_{n}}\big{[}\mathbb{E}_{\tau|r,\xi}[ \log p(r|\tau,\xi)-\log p(r)]\big{]}=\mathbb{E}_{r|\mathcal{D}_{n}}\big{[} \mathbb{E}_{\tau|r,\xi}[\log p(\tau|r,\xi)-\log p(\tau|\xi)]\big{]},\]

where the expectation over trajectories is taken with respect to \(\rho_{\xi}\), \(p_{\xi}\), and an expert policy that would correspond to the reward \(r\) from the outer expectation, taken with respect to the current posterior.

In general, the expectations cannot be calculated analytically. A basic way to approximate the EIG would be using the following nested Monte Carlo estimator for each candidate environment setup \(\xi\):

1. Sample \(N_{r}\) reward functions \(r_{i}\) from the current posterior \(p(r|\mathcal{D}_{n})\). For each \(r_{i}\): 1. Sample \(N_{\tau}\) trajectories \(\tau_{ij}\) from the estimated expert policy \(\hat{\pi}_{E}^{r_{i},\xi}\) given the environment parameters \(\xi\), where \(\hat{\pi}_{E}^{r_{i},\xi}\) would be the Boltzmann-rational policy corresponding to \(r_{i}\). 2. Estimate2\(p(\tau_{ij}|r_{i},\xi)=\prod_{s_{t},a_{t}\in\tau}\hat{\pi}_{E}^{r_{i},\xi}(a_ {t}|s_{t})\) and \(p(\tau_{ij}|\xi)=\frac{1}{N_{r}}\sum_{k}p(\tau_{ij}|r_{k},\xi)\). Footnote 2: Note that we can omit the probabilities due to the initial state and transitions since these cancel out in Eq. 2.
2. Approximate EIG using the Monte Carlo estimate: \[\widehat{EIG}(\xi)=\frac{1}{N_{r}}\sum_{i=1}^{N_{r}}\frac{1}{N_{r}}\sum_{j=1} ^{N_{\tau}}\left[\log p(\tau_{ij}|r_{i},\xi)-\log p(\tau_{ij}|\xi)\right].\] (2)

While conceptually simple, the computational demands of this grow quickly with the size of the state space. Thus, in the next section, we discuss a method based on Bayesian optimization to allocate any computational budget we may have more efficiently.

### Efficient sampling with Bayesian optimization

We propose to use _Bayesian optimization_, in particular the _upper confidence bound_ (UCB) algorithm, to adaptively choose from which initial states to sample additional hypothetical trajectories to efficiently estimate the EIG. We still use the basic structure of (2), but instead of using the same number of samples in each initial state, we dynamically choose where to add additional samples to best improve our chance of identifying the state maximizing the EIG.

We model the information gain from each hypothetical trajectory \(\tau_{si}\) starting in state \(s\) as a Gaussian noisy observation of the true EIG value:

\[e_{si}(s)\sim\mathcal{N}(\mu_{s},\epsilon_{s}^{2})\,,\] (3)where we assume \(\mu_{s}=\text{EIG}(s)\). We also assume we have a prior on the mean and noise,

\[\mu_{s}\sim\mathcal{N}(\mu_{\text{prior}},\sigma_{\text{prior}}^{2})\,,\quad \epsilon_{s}\sim p_{\phi}(\epsilon_{s})\,.\] (4)

We first collect a fixed initial number of samples for each state. Then, we repeat the following until we have exhausted a budget of trajectories \(T\). Following standard Gaussian updating, after an observation of a new hypothetical trajectory from \(s\), we update the parameters

\[\mu_{s}=\Big{(}\tfrac{\mu_{\text{prior}}}{\sigma_{\text{prior}}^{2}}+\tfrac{n_ {s}\widehat{\text{EIG}}(s)}{\epsilon_{s}^{2}}\Big{)}\Big{(}\tfrac{1}{\sigma_{ \text{prior}}^{2}}+\tfrac{n_{s}}{\epsilon_{s}^{2}}\Big{)}^{-1}\,,\quad\sigma_{ s}^{2}=\Big{(}\tfrac{1}{\sigma_{\text{prior}}^{2}}+\tfrac{n_{s}}{\epsilon_{s}^{2}} \Big{)}^{-1}\,,\] (5)

where \(n_{s}\) is the number of observed trajectories from \(s\), and \(\widehat{\text{EIG}}(s)=\tfrac{1}{n_{s}}\sum_{i=1}^{n_{s}}e_{si}\) is the average of the corresponding EIG estimates. We then update \(\epsilon_{s}\) using maximum a posteriori estimation:

\[\epsilon_{s}=\operatorname*{arg\,max}_{\epsilon_{s}}\Big{[}p_{\phi}(\epsilon_ {s})\cdot\mathcal{N}\left(\widehat{\text{EIG}}(s)\mid\mu_{s}(\epsilon_{s}), \sigma_{s}(\epsilon_{s})\right)\Big{]}\,.\] (6)

and compute a new EIG estimate for the value \(s^{*}\) maximizing the upper confidence bound:

\[s^{*}=\operatorname*{arg\,max}_{s}\text{UCB}(s):=\operatorname*{arg\,max}_{s} \mu_{s}+\kappa\sigma_{s}\,,\] (7)

where \(\kappa\) is a UCB hyperparameter (we use \(\kappa=3\)).

## 4 Experiments

We evaluated our EIG-based methods with full trajectories on two randomized gridworld setups against several simpler baselines: (1) uniform random sampling, (2) selecting the state with maximum entropy in Q-values, (3) querying just a single state (to measure the benefits of whole trajectories), and (4) selecting the starting state leading to trajectories with maximum posterior predictive entropy over the optimal policy. The last one is an acquisition function from [5], which is the only previous work on active IRL over trajectories that we are aware of.

We use two main metrics: the entropy of the posterior distribution over reward parameters after a given number of steps of active learning and the expected return (with respect to the initial state distribution and environment dynamics) of an apprentice policy maximizing this expected return (also with respect to the posterior over rewards).

We test on two kinds of gridworld environments: one with fewer state types (and thus reward parameters) than states, which gives the algorithm a known environment structure to exploit, and one with a single random reward per state. Full details on our experiments and additional results (including the efficiency gains from Bayesian optimization) are provided in Appendix C.

Structured gridworldWe begin with the \(6\times 6\) gridworld shown in Figure 0(a). This environment is deterministic with 5 actions corresponding to moving in the four directions and staying in place. The agent can move freely, except for the bottom-left "jail" state, which is non-terminal, has a negative reward, and traps the agent permanently upon entry. In terms of the state rewards, there are five different state types and both the apprentice and the expert know the type of each state a priori. The rewards associated with two state types are known: "path" type, with a reward of \(-1\), and a "goal" type with reward 100, which is also terminal. There are 3 state types, which we refer to as "water", "mud", and "lava", which have unknown negative reward. We place an independent uniform prior in the interval \([-100,0]\) on the reward of each state type. Our goal is to infer the reward of these three state types.

Fully random gridworldWe also performed experiments on a \(7\times 7\) gridworld with each state's reward drawn from \(\mathcal{N}(0,3)\). Each state furthermore has a 10% probability of being terminal. States with reward above the \(0.9\) quantile of rewards are also terminal.

ResultsFigure 2 shows results for structured environment (the results for the fully random environment can be found in Appendix C), comparing active methods with randomly choosing trajectories. We observe that the performance of EIG in terms of posterior entropy and in terms of apprentice performance is superior to the baselines. The Q-Entropy does better than random initially, but then starts to do worse due to repeatedly sampling from states with irreducible uncertainty. Notably, on the structured environment, the posterior predictive action entropy acquisition function from [5] breaks entirely, as it only ever queries for demonstration trajectories that start in the jail state, as this state trivially has a uniform action distribution, and demonstrations starting in the jail state deterministically remain in the jail state. Thus these demonstrations offer no useful information about the expert reward or the policy. Based on these results, we believe that our information-theoretically derived acquisition function is more principled and robust.

## 5 Discussion and conclusion

We have provided a preliminary study of the problem of active IRL with full trajectories in tabular environments. We have shown that an information theoretic acquisition function provides improvements both in terms of achieving lower posterior entropy, and in terms of apprentice performance. It thus allows using the scarce time of demonstrators more efficiently. We see this preliminary study with synthetic gridworlds and demonstrations as a stepping stone toward an extension to continuous state spaces and more realistic settings.

## References

* Russell [2019] Stuart Russell. _Human Compatible: Artificial Intelligence and the Problem of Control_. Penguin Random House, 2019.
* Lopes et al. [2009] Manuel Lopes, Francisco Melo, and Luis Montesano. Active learning for reward estimation in inverse reinforcement learning. In Wray Buntine, Marko Grobelnik, Dunja Mladenic, and John Shawe-Taylor, editors, _Machine learning and knowledge discovery in databases_, pages 31-46, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg. ISBN 978-3-642-04174-7.
* Brown et al. [2018] Daniel S. Brown, Yuchen Cui, and Scott Niekum. Risk-Aware Active Inverse Reinforcement Learning. In _Proceedings of The 2nd Conference on Robot Learning_, pages 362-372. PMLR, October 2018. URL https://proceedings.mlr.press/v87/brown18a.html. ISSN: 2640-3498.
* Metelli et al. [2021] Alberto Maria Metelli, Giorgia Ramponi, Alessandro Concetti, and Marcello Restelli. Provably Efficient Learning of Transferable Rewards. In _Proceedings of the 38th International Conference on Machine Learning_, pages 7665-7676. PMLR, July 2021. URL https://proceedings.mlr.press/v139/metelli21a.html. ISSN: 2640-3498.

Figure 2: Entropy of the posterior the regret of the apprentice policy on the structured environment. NMC stands for the naive nested Monte Carlo estimation, while BO stands for Bayesian optimization. “S.s. EIG (x\(\times\)8.8)” denotes single-state EIG with the x axis scaled by 8.8 - the mean length of trajectories collected by the full trajectory EIG variants.

* Kweon et al. [2023] Sehee Kweon, Himchan Hwang, and Frank C Park. Trajectory-based active inverse reinforcement learning for learning from demonstration. In _2023 23rd International Conference on Control, Automation and Systems (ICCAS)_, pages 1807-1812. IEEE, 2023.
* Rainforth et al. [2023] Tom Rainforth, Adam Foster, Desi R. Ivanova, and Freddie Bickford Smith. Modern Bayesian Experimental Design, February 2023. URL http://arxiv.org/abs/2302.14545. arXiv:2302.14545 [cs, stat].
* Frazier [2018] Peter I. Frazier. A Tutorial on Bayesian Optimization. _arXiv:1807.02811_, July 2018. URL http://arxiv.org/abs/1807.02811. arXiv: 1807.02811.
* Russell [1998] Stuart Russell. Learning agents for uncertain environments (extended abstract). In _Proceedings of the eleventh annual conference on Computational learning theory_, pages 101-103, Madison Wisconsin USA, July 1998. ACM. ISBN 978-1-58113-057-7. doi: 10.1145/279943.279964. URL https://dl.acm.org/doi/10.1145/279943.279964.
* Kalman [1964] R. E. Kalman. When Is a Linear Control System Optimal? _Journal of Basic Engineering_, 86(1):51-60, March 1964. ISSN 0021-9223. doi: 10.1115/1.3653115. URL https://doi.org/10.1115/1.3653115.
* Arora and Doshi [2021] Saurabh Arora and Prashant Doshi. A survey of inverse reinforcement learning: Challenges, methods and progress. _Artificial Intelligence_, 297:103500, August 2021. ISSN 00043702. doi: 10.1016/j.artint.2021.103500. URL https://linkinghub.elsevier.com/retrieve/pii/S0004370221000515.
* Adams et al. [2022] Stephen Adams, Tyler Cody, and Peter A. Beling. A survey of inverse reinforcement learning. _Artificial Intelligence Review_, February 2022. ISSN 0269-2821, 1573-7462. doi: 10.1007/s10462-021-10108-x. URL https://link.springer.com/10.1007/s10462-021-10108-x.
* Ramachandran and Amir [2007] Deepak Ramachandran and Eyal Amir. Bayesian Inverse Reinforcement Learning. In _Proceedings of the Twentieth International Joint Conference on Artificial Intelligence_, 2007.
* Buening et al. [2024] Thomas Kleine Buening, Victor Villin, and Christos Dimitrakakis. Environment design for inverse reinforcement learning. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, _Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine Learning Research_, pages 24808-24828. PMLR, 21-27 Jul 2024. URL https://proceedings.mlr.press/v235/kleine-buening24a.html.
* Buning et al. [2022] Thomas Kleine Buning, Anne-Marie George, and Christos Dimitrakakis. Interactive inverse reinforcement learning for cooperative games. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 2393-2413. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/buning22a.html.
* Sadigh et al. [2017] Dorsa Sadigh, Anca Dragan, Shankar Sastry, and Sanjit Seshia. Active Preference-Based Learning of Reward Functions. In _Robotics: Science and Systems Foundation_, July 2017. ISBN 978-0-9923747-3-0. doi: 10.15607/RSS.2017.XIII.053. URL http://www.roboticsproceedings.org/rss13/p53.pdf.
* Losey and O'Malley [2018] Dylan P. Losey and Marcia K. O'Malley. Including Uncertainty when Learning from Human Corrections. In _Proceedings of The 2nd Conference on Robot Learning_, pages 123-132. PMLR, October 2018. URL https://proceedings.mlr.press/v87/losey18a.html. ISSN: 2640-3498.
* Lindner et al. [2022] David Lindner, Andreas Krause, and Giorgia Ramponi. Active Exploration for Inverse Reinforcement Learning. _Advances in Neural Information Processing Systems_, 35:5843-5853, December 2022. URL https://proceedings.neurips.cc/paper/2022/hash/26d01e5ed42d8dcedd6aa0e3e99cffc4-Abstract-Conference.html.

* [18] Ondrej Bajgar, Konstantinos Gatsis, Alessandro Abate, and Michael A. Osborne. Walking the Values in Bayesian Inverse Reinforcement Learning. In _Proceedings of the 40th Conference on Uncertainty in Artificial Intelligence_, 2024.
* [19] Alex J Chan and Mihaela van der Schaar. Scalable Bayesian Inverse Reinforcement Learning. _ICLR 2021_, 2021.
* [20] Simon Duane, A. D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid Monte Carlo. _Physics Letters B_, 195(2):216-222, September 1987. doi: 10.1016/0370-2693(87)91197-X.
* [21] Matthew D Hoffman and Andrew Gelman. The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. _Journal of Machine Learning Research_, 15:1593-1623, 2014.

## Appendix A Related work

Our work builds on two strands of work: inverse reinforcement learning (IRL) and active learning, which we will address in turn. The IRL problem itself was introduced by Russell [8], preceded by the closely related problem of _inverse optimal control_ formulated by Kalman [9]. See Arora and Doshi [10] and Adams et al. [11] for recent reviews of the already extensive literature on IRL. Ramachandran and Amir [12] introduced the Bayesian formulation of the problem that we build on here.

Active learning has first been introduced into IRL by Lopes et al. [2] who collect action annotations in states where the current posterior over reward functions implies high ambiguity about the expert's action. A key limitation of this approach is that the action ambiguity can come from several actions being equally good according to the true reward (such as in the jail state in our structured environment). This ambiguity remains even when we already have plenty of expert data for a given state, which can result in queries that do not bring any additional value. Our approach focuses on states that actually bring extra information about the uknown reward. A second limitation - common with other methods discussed below - is that the expert provides single action annotations. This is not practical in settings such as autonomous driving where actions are sampled with high frequency, and it may be more natural for a human demonstrator to provide longer trajectories (i.e. drive for a while) rather than give annotations for unrelated individual time frames.

Bueuning et al. [13] query full trajectories in the context of IRL, where the active component arises in a choice of transition function from a set of transition functions at each step of learning. Buening et al [14] also query full trajectories in a different context involving two cooperating autonomous agents.

Our work addresses the setting of identifying an optimal strategy for choosing trajectories within a fixed environment.

Instead of directly providing demonstrations, in Sadigh et al. [15], the human expert is asked to provide a relative preference between two sample trajectories synthesized by the algorithm. While this generally provides less information per query than our formulation, it is a useful alternative for situations where providing high-quality demonstrations is difficult for humans.

Brown et al. [3] present a risk-aware approach, which queries individual states with the highest \(\alpha\)-quantile policy loss, i.e. the states with a high risk that the apprentice action could be much worse than the expert's action.

Instead of querying at arbitrary states, Losey and O'Malley [16] and Lindner et al. [17] synthesize a policy that explores the environment to produce a trajectory which subsequently gets annotated by the expert. We instead let the expert produce the trajectory.

The closest baseline for our work, and the only existing work we are aware of that deals with full trajectories in active IRL, comes from Kweon et al. [5]. Like in our experimental setup, they propose an acquisition function for querying for expert trajectories starting from a given initial state. Their acquisition function is based on maximizing the posterior predictive action entropy along the demonstration trajectory \(\tau\). That is, maximizing

\[\mathbb{E}_{\tau\sim\hat{\pi}_{E}^{D}}\left[\sum_{t=0}^{|\tau|}\mathcal{A}(s_{t}) \Bigg{|}s_{0}\right]\] (8)

where

\[\mathcal{A}(s)=\sum_{a}-\hat{\pi}_{E}^{D}(a|s)\log\hat{\pi}_{E}^{D}(a|s),\]

i.e. the entropy of the estimated expert policy \(\hat{\pi}_{E}^{D}\) at state \(s\), estimated from demonstration data \(D\).

## Appendix B Scaling Properties

We also provide a brief view of the scaling properties of the nested Monte Carlo estimation of the EIG with respect to increasing sizes of a scaled-up version of the structured environment. We ran 3 repeated trials with adaptive step sizes, 50 warmup steps and 200 samples, for 5 active learning steps, then timed the computational time for the EIG calculation as well as the associated PolicyWalk algorithm for Bayesian IRL. The results are displayed in Figure 3. They suggest that the Bayesian IRL algorithm may be the limiting factor in scaling up, though the ValueWalk algorithm (which we found not suitable for the structured environment, but performing well on the fully random one) generally displays better scaling properties [18]. That said, for scaling the algorithms further, especially to continuous spaces, we expect to need to resort to methods based on variational inference.

On the other hand, we observed the Bayesian-optimization-based method of EIG calculation to scale more favourably, since it does not need to assign a uniform budget to all \(n^{2}\) squares, but can focus only on the most promising ones based on an initial estimate. In our initial investigation, we found that assigning a quarter of the budget across initial squares (which still scales quadratically) and scaling the rest linearly tended to preserve performance on par with nested Monte Carlo with a fully quadratic scaling of the budget, but we are leaving a fuller analysis to a future version of this paper.

## Appendix C Experiment details

### Bayesian IRL methods

Our active learning uses a Bayesian IRL method as a key component. In our experiments, we used two methods based on Markov chain Monte Carlo (MCMC) sampling: on the structured environment, we used PolicyWalk [12], while on the environment with a different random reward in every state, we used the faster ValueWalk [18], which performs the sampling primarily in the space of Q-functions

Figure 3: These plots show that the EIG calculation step scales approximately quadratically in \(n\), or linearly in the number of steps, and is very consistent. For comparison, the plot also shows the scaling of the time required to run the PolicyWalk algorithm for Bayesian IRL

before converting into rewards. We also tried a method based on variational inference [19], but we found its uncertainty estimates unreliable for the purposes of active learning.

For MCMC sampling, we used Hamiltonian Monte Carlo [20] with the no-U-turns (NUTS) sampler [21] and automatic step size selection during warm-up (starting with a step size of 0.1). At every step of active learning, we ran the MCMC sampling from scratch using all demonstrations available up to that point. We ran for 100 warm-up steps and then 200 collected samples. For subsequent usage, we generally thin the sample to 50 samples to reduce autocorrelation.

### Baselines

We compare our methods against various baseline approaches. To evaluate the value of using _full trajectories_ in our EIG estimate, we also give results for experiments where \(\widehat{\text{EIG}}\) is computed after querying a single state only (equivalently a unit-length trajectory), and the returned demonstration has unit length. Relatedly, we consider a baseline experiment in which \(N=8\) single states are queried at each active step, where \(N\) equals the average length of demonstrations in the active setting. In a sense, the latter baseline serves as an "upper bound" of performance we could hope to achieve with a fixed budget of trajectory lengths. For these length-one trajectories, we otherwise use the same EIG calculation as for our longer trajectories.

For the Q-entropy baseline, we calculate the optimal Q-value corresponding to each reward sample (this is in fact produced as a byproduct of both Bayesian IRL algorithms) and estimate its entropy using the \(k\)-nearest-neighbours method with \(k=5\). We then select the state with maximum entropy as the next initial state. The reasoning behind this is that uncertainty in the expert policy is directly dependent on the uncertainty in the Q-value. Furthermore, the uncertainty in the Q-value captures not only uncertainty about the reward in the given state, but also about the rewards and Q-values of states that are expected to follow.

For the posterior predictive action entropy baseline [5], we use the acquisition function (8), while adapting everything else to be consistent with our experiments. Specifically, this means calculating the estimated expert policy \(\hat{\pi}_{E}^{D}\) directly from samples of the expert Q-values and assuming a Boltzmann rational expert. The expectation in Eq. 8 is again approximated by sampling a number of trajectories starting from \(s_{0}\), according to policy \(\hat{\pi}_{E}^{D}\). For the structured environment, this acquisition function only queries trivial trajectories remaining on the jail state which did not terminate, so it was necessary to truncate these trajectories a maximum length. We chose 15 for this maximum.

### Experimental setup

In sampling the hypothetical trajectories, we cap their length at 15 in the structured environment and 10 in the fully random one. In approximations, we used 20 reward samples and 2 sampled trajectories per reward.

#### c.3.1 Structured environment

In the structured environment, we use an expert rationality coefficient of \(\beta=1\). We do not provide any initial demonstrations. All experiments were run with 10 random reward assignments, consistent across all tested methods. The reward was drawn from the same prior as was used by the Bayesian IRL method, i.e. independent \(\text{Uniform}[0,100]\) for the 3 rewards associated with the 3 state types.

#### c.3.2 Fully random environment

In the fully random environment, we use an expert rationality coefficient of \(\beta=1\) and provide 1 initial trajectory starting in the top left corner. Each method was run with 16 random reward and terminal-state assignments. The reward was drawn from the same prior as was used by the Bayesian IRL method, i.e. \(\mathcal{N}(0,3)\), i.i.d. for each state.

### Additional results

In Figure 4 we provide the results on the fully random environment. To make the entropy plot more legible, we aggregate the results for 10 steps subsequent steps, i.e. the result at 10 steps is the mean and standard error across steps 1-10, at 20 across 11-20 and so on. The result at 0 is the performance based on only the initial trajectory (before the first active step).

The results are mostly consistent with the results on the structured environment with several differences. Here we do not observe any advantage to sampling using the Q-entropy baseline even in the early steps. We also observe an advantage of the Bayesian-optimization calculation for EIG - with the same budget, the method is able to achieve better posterior entropy - in fact, a similar entropy is achieved by the naive nested Monte Carlo estimation only with about double the budget. With the same budget of trajectory samples, the Bayesian optimization method incurs a less than 10% increase in computation time, taking \(6.6\) instead of \(6.2\) seconds per step on average. On top of this, the Bayesian IRL method takes about 20 seconds to collect the 200 reward samples (+ 100 warm up steps).

Figure 4: Results on the fully random environment. NMC stands for the naive nested Monte Carlo estimation, while BO stands for Bayesian optimization.