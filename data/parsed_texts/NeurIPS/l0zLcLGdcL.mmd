# Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.

## 1 Introduction

Unsupervised Domain Adaptation (UDA) [15; 22; 41; 44; 9; 19; 21] has become a crucial research area of transfer learning, as it allows models trained on a specific dataset to be applied to related but distinct domains. However, traditional UDA methods are limited by the assumption that the source and target domains have to share the same label space. This assumption is problematic in real-world scenarios where the target distribution is complex, open, and diverse. Universal Domain Adaptation (UniDA) represents a strategy to address the limitations of traditional unsupervised domain adaptation methods. In the UniDA, the target domain have a different label set than the source domain. The goal is to correctly classify target domain samples belonging to the shared classes in the source label set, while any samples not conforming to the source label set are treated as "unknown". The term "universal" characterizes UniDA as not relying on prior knowledge about the label sets of the target domain. UniDA relaxes the assumption of a shared class space while aims to learn domain-invariant features across a more broad range of domains.

Despite being widely explored, most existing universal domain adaptation methods [24; 47; 40; 39; 6; 34; 8; 26] overlook the internal structure intrinsically presented within each image category. These methods aim to align the common classes between the source and target domains for adaptation, butusually train a model to learn the class "prototype" representing each annotated category. This is particularly controversial when significant concept shift exists between samples belonging to the same category. These differences can lead to sub-optimal feature learning and adaptation if the intra-class structure is neglected during training. Since such kind of semantic ambiguity without fine-grained category labels almost happens in all the DA benchmarks, all the methods will encounter this issue.

In this paper, we aim to propose a method to learn the detailed intra-class distinction and mine "sub-prototypes" for better alignment and adaptation. This kind of sub-prototype is the further subdivision of each category-level prototype, which represents the "sub-class" of the annotated categories. The main idea of our proposed approach lies in its utilization of a learnable memory structure to learn sub-prototypes for their corresponding sub-classes. This can optimize the construction and refinement of the feature space, bolstering the classifier's ability to distinguish class-wise relationships and improve the model's transferability across domains. A comparison between our proposed sub-prototypes mining approach and previous methods is illustrated in Figure 1. In previous methods, samples within a category were forced to be aligned together in the feature space regardless of whether there exist significant differences among them because the labels were one-hot encoded. Contrastively, our sub-prototypes' feature space distinguishes sub-classes with apparent differences within the category, thus improving the model's accuracy of domain adaption and interpretability.

Our proposed approach, named memory-assisted sub-prototype mining (MemSPM), is inspired by the memory mechanism works [17; 10; 45; 36]. In our approach, the memory generates sub-prototypes that embody sub-classes learned from the source domain. During testing of the target samples, the encoder produces embedding that are compared to source domain sub-prototypes learned in the memory. Subsequently, a embedding for the query sample is generated through weighted sub-prototype sampling in the memory. This results in reduced domain shifts before the embedding give into the classifier. Our proposal of sub-prototypes mining, which are learned from the source domain

Figure 1: Illustration of our motivation. (a) Examples of concept shift and intra-class diversity in DA benchmarks. For the class of alarm clock, we find that digital clock, pointer clock and alarm bell should be set in different sub-classes. For the class of airplane, we find that images containing more than one plane, single jetliner, and turbopro aircraft should be differently treated for adaptation. (b) Previous methods utilize one-hot labels to guide classifying without considering the intra-class distinction. Consequently, the model forces all samples from the same class to converge towards a single center, disregarding the diversity in the class. Our method clusters samples with large intra-class difference into separate sub-class, providing a more accurate representation. (c) During domain adaptation by our design, the samples in the target domain can also be aligned near the sub-class centers with similar features rather than just the class centers determined by labels.

memory, improves the universal domain adaptation performance by promoting more refined visual concept alignment.

MemSPM approach has been evaluated on four benchmark datasets (Office-31 [37], Office-Home [46], VisDA [33],and Domain-Net [32]), under various category shift scenarios, including PDA, OSDA, and UniDA. Our MemSPM method achieves state-of-the-art performance in most cases. Moreover, we design a visualization module for the sub-prototype learned by our memory to demonstrate the interpretability of MemSPM. Our contributions can be highlighted as follows:

* We study the UniDA problem from a new aspect, which focuses on the negative impacts caused by overlooking the intra-class structure within a category when simply adopting one-hot labels.
* We propose Memory-Assisted Sub-Prototype Mining(MemSPM), which explores the memory mechanism to learn sub-prototypes for improving the model's adaption performance and interpretability. Meanwhile, visualizations reveal the sub-prototypes stored in memory, which demonstrate the interpretability of MemSPM approach.
* Extensive experiments on four benchmarks verify the superior performance of our proposed MemSPM compared with previous works.

## 2 Related Work

**Closed-Set Domain Adaptation (CSDA).** To mitigate the performance degradation caused by the closed-set domain shift, [16; 29; 48] introduce adversarial learning methods with the domain discriminator, aiming to minimize the domain gap between source and target domains. Beyond the use of the additional domain discriminator, some studies [41; 23; 50; 30; 13] have explored the use of two task-specific classifiers, otherwise referred to as bi-classifier, to implicitly achieve the adversarial learning. However, the previously mentioned methods for CSDA cannot be directly applied in scenarios involving the category shift.

**Partial Domain Adaptation (PDA).** PDA posits that private classes are exclusive to the source domain. Representative PDA methods, such as those discussed in [3; 49], employ domain discriminators with weight adjustments or utilize source samples based on their resemblance to the target domain [5]. Methods incorporating residual correction blocks in PDA have been introduced by Li et al. and Liang et al. [25; 27]. Other research [7; 11; 38] explores the use of Reinforcement Learning for source data selection within the context of PDA.

**Open-Set Domain Adaptation (OSDA).** Saito et al. [42] developed a classifier inclusive of an additional 'unknown' class intended to differentiate categories unique to the target domain. Liu et al. [28] and Shermin et al. [43] propose assigning individual weights to each sample depending on their importance during domain adaptation. Jang et al. [20] strive to align the source and target-known distributions, while concurrently distinguishing the target-unknown distribution within the feature alignment process. The above PDA and OSDA methods are limited to specific category shift.

**Universal Domain Adaptation (UniDA)** You et al. [47] proposed Universal Adaptation Network (UAN) to deal with the UniDA setting that the label set of target domain is unknown. Li et al. [24] proposed Domain Consensus Clustering to differentiate the private classes rather than treat the unknow classes as one class. Saito et al. [40] suggested that using the minimum inter-class distance in the source domain as a threshold can be an effective approach for distinguishing between "known" and "unknown" samples in the target domain. However, most existing methods [24; 47; 40; 39; 6; 34; 8; 26] overlook the intra-class distinction within one category, especially in cases where there exists significant concept shift between the samples belonging to the same category.

## 3 Proposed Methods

### Preliminaries

In unsupervised domain adaptation, we are provided with labeled source samples \(\mathcal{D}^{s}=\{x_{i}^{s},y_{i}^{s}\}_{i=1}^{n^{s}}\) and unlabeled target samples \(\mathcal{D}^{t}=\{(x_{i}^{t})\}_{i=1}^{n^{t}}\). As the label set for each domain in UniDA setting may not be identical, we use \(C_{s}\) and \(C_{t}\) to represent label sets for the two domains, respectively.

Then, we denote \(C=C_{s}\cap C_{t}\) as the common label set. \(\hat{C}_{s}\), \(\hat{C}_{t}\) are denoted as the private label sets of the source domain and target domain, respectively. We aim to train a model on \(\mathcal{D}^{s}\) and \(\mathcal{D}^{t}\) to classify target samples into \(|C|+1\) classes, where private samples are treated as unknown class.

Our method aims to address the issue of intra-class concept shift that often exists within the labeled categories in most datasets, which is overlooked by previous methods. Our method enables the model to learn an adaptive feature space that better aligns fine-grained sub-class concepts, taking into account the diversity present within each category. Let \(X\) denotes the input query, \(Z\) denotes the embedding extracted by the encoder, \(L\) denotes the data labels, \(\hat{Z}\) denotes the embedding obtained from the memory, \(\hat{X}\) denotes the visualization of the memory, \(\hat{L}\) denotes the prediction of the input query, and the \(K\) denotes the top-K relevant sub-prototypes, respectively. The overall pipeline is presented in Figure 2. More details will be described in the following sub-sections.

### Input-Oriented Embedding vs. Task-Oriented Embedding

Usually, the image feature extracted by a visual encoder is directly used for learning downstream tasks. We call this kind of feature as input-oriented embedding. However, it heavily relys on the original image content. Since different samples of the same category always varies significantly in their visual features, categorization based on the input-oriented embedding sometimes is unattainable. In our pipeline, we simply adopt a CLIP-based[35] pre-trained visual encoder to extract the input-oriented embeddings, which is not directly used for learning our downstream task.

In our MemSPM, we propose to generate task-oriented embedding, which is obtained by serving input-oriented embedding as a query to retrieve the sub-prototypes from our memory unit. We define \(f^{fixed}_{encode}(\cdot):X\to Z\) to represent the fixed pre-trained encoder and \(f^{UniDA}_{class}(\cdot):\hat{Z}\rightarrow\hat{L}\) to represent the UniDA classifier. The input-oriented embedding \(Z\) is used to retrieve the relevant sub-prototypes from the memory. The task-oriented embedding \(\hat{Z}\) is obtained using the retrieved sub-prototypes for classification tasks. In conventional ways, \(\hat{Z}=Z\), which means the \(\hat{Z}\) is obtained directly from \(Z\). Our method obtains the \(\hat{Z}\) by retrieving the sub-prototypes from the memory, which differencates \(\hat{Z}\) with \(Z\), and eliminates the domain-specific information from the target domain during the testing phase. As a result, it improves the performance of \(f^{UniDA}_{class}(\cdot)\) when performing UniDA.

### Memory-Assisted Sub-Prototype Mining

The memory module proposed in MemSPM consists of two key components: a memory unit responsible for learning sub-prototypes, and an attention-based addressing [18] operator to obtain better task-oriented representation \(\hat{Z}\) for the query, which is more domain-invariant.

Figure 2: Our model first utilizes a fixed pre-trained model as the encoder to extract input-oriented embedding given an input sample. The extracted input-oriented embedding is then compared with sub-prototypes learned in memory to find the closest \(K\). These \(K\) are then weighted-averaged into a task-oriented embedding to represent the input, and used for learning downstream tasks. During the UniDA process, we adopt the cycle-consistent matching method on the task-oriented embedding \(\hat{Z}\) generated from the memory. Moreover, a decoder is designed to reconstruct the image, allowing for visualizing of the sub-prototypes in memory and verifying of the effectiveness of sub-class learning.

#### 3.3.1 Memory Structure with Partitioned Sub-Prototype

The memory in MemSPM is represented as a matrix, denoted by \(M\in\mathbb{R}^{N\times S\times D}\), where \(N\) indicates the number of memory items stored, \(S\) refers to the number of sub-prototypes partitioned in each memory item, and \(D\) represents the dimension of each sub-prototype. For convenience, we assume \(D\) is the same to the dimension of \(Z\in\mathbb{R}^{C}\) ( \(\mathbb{R}^{D}\)=\(\mathbb{R}^{C}\)). Let the vector \(m_{i,j}\), \(\forall i\in[N]\) denote the \(i\)-th row of \(M\), where \([N]\) denotes the set of integers from 1 to \(N\), \(\forall j\in[S]\) denote the \(j\)-th sub-prototype of \(M\) items, where \([S]\) denotes the set of integers from 1 to \(S\). Each \(m_{i}\) denotes a memory item. Given a embedding \(Z\in\mathbb{R}^{D}\), the memory module obtains \(\hat{Z}\) through a soft addressing vector \(W\in\mathbb{R}^{1\times 1\times N}\) as follows:

\[\hat{Z}=W\cdot M=\Sigma_{i=1}^{N}w_{i,j=s_{i}}\cdot m_{i,j=s_{i}},\] (1) \[w_{i,j=s_{i}}=\text{argmax}(w_{i,j},dim=1),\] (2)

where \(W\) is a vector with non-negative entries that indicate the max attention weight of each item's sub-prototype, \(s_{i}\) denotes the index of the sub-prototype in the \(i\)-th item and \(w_{i,j=s_{i}}\) denotes the \(i,j=s_{i}\)-th entry of \(W\). The hyperparameter \(N\) determines the maximum capacity for memory items and the hyper-parameter \(S\) defines the number of sub-prototypes in each memory item. The effect of different setting of hyper-parameters is evaluated in Section 4.

#### 3.3.2 Sub-Prototype Addressing and Retrieving

In MemSPM, the memory \(M\) is designed to learn the sub-prototypes to represent the input-oriented embedding \(Z\). We define the memory as a content addressable memory [17; 10; 45; 36] that allows for direct referencing of the content of the memory being matched. The sub-prototype is retrieved by attention weights \(W\) which are computed based on the similarity between the sub-prototypes in the memory items and the input-oriented embedding \(Z\). To calculate the weight \(w_{i,j}\), we use a softmax operation:

\[w_{i,j}=\frac{\exp(d(z,m_{i,j}))}{\Sigma_{n=1}^{N}\Sigma_{s=1}^{S}\exp(d(z,m_{ n,s}))},\] (3)

where \(d(\cdot,\cdot)\) denotes cosine similarity measurement. As indicated by Eq. 1 and 3, the memory module retrieves the sub-prototype that is most similar to \(Z\) from each memory item in order to obtain the new representation embedding \(\hat{Z}\). As a consequence of utilizing the adaptive threshold addressing technique(Section 3.3.3), only the \(K\) can be utilized to obtain a task-oriented embedding \(\hat{Z}\), that serves to represent the encoded embedding \(Z\).

#### 3.3.3 Adaptive Threshold Technique for More Efficient Memory

Limiting the amount of sub-prototypes retrieved can enhance memory utilization and avoid negative impacts on unrelated sub-prototypes during model parameter updates. Despite the natural reduction in the number of selected memory items, the attention-based addressing mechanism may still lead to the combination of small attention weight items into the output embedding \(\hat{Z}\), which have negative impact on the classifier and sub-prototypes in the memory. Therefore, it is necessary to impose a mandatory quantity limit on the amount of the relevant sub-prototypes retrieved. To address this issue, we apply a adaptive threshold operation to restrict the amount of sub-prototypes retrieved in a forward process.

\[\hat{w}_{i,j=s_{i}}=\begin{cases}w_{i,j=s_{i}},&w_{i,j=s_{i}}>\lambda\\ 0,&\text{other}\end{cases}\] (4)

where \(\hat{w}_{i,j=s_{i}}\) denotes the \(i,j=s_{i}\)-th entry of \(\hat{w}\), the \(\lambda\) denotes the adaptive threshold:

\[\lambda=\text{argmin}(topk(w)).\] (5)

Directly implementing the backward for the discontinuous function in Eq. 4 is not a easy task. For simplicity, we use the method [17]that rewrites the operation using the continuous ReLU activation function as:\[\hat{w}_{i,j=s_{i}}=\frac{\max(w_{i,j=s_{i}}-\lambda)\cdot w_{i,j=s_{i}}}{|w_{i,j= s_{i}}-\lambda|+\epsilon},\] (6)

where \(max(\cdot,0)\) is commonly referred to as the ReLU activation function, and \(\epsilon\) is a small positive scalar. The prototype \(\hat{Z}\) will be obtained by \(\hat{Z}=\hat{W}\cdot M\). The adaptive threshold addressing encourages the model to represent embedding \(Z\) using fewer but more relevant sub-prototypes, leading to learning more effective feature in memory and reducing the impact on irrelevant sub-prototypes.

### Visualization and Interpretability

We denote \(f^{unfixed}_{decode}(\cdot):\hat{Z}\rightarrow\hat{X}\) to represent the decoder. The decoder is trained to visualize what has been learned in the memory by taking the retrieved sub-prototype as input. From an interpretability perspective, each encoded embedding \(Z\) calculates the cosine similarity to find the top-\(K\) fitting sub-prototype representation for the given input-oriented embedding. Then, these sub-prototypes are combined to represent the \(Z\) in \(\hat{Z}\). The sub-prototype in this process can be regarded as the visual description for the input embedding \(Z\). In other word, the input image is much like the sub-classes represented by these sub-prototypes. In this way, samples with significant intra-class differences will be matched to different sub-prototypes, thereby distinguishing different sub-classes. The use of a reconstruction auxiliary task can visualize the sub-prototypes in memory to confirm whether our approach has learned intra-class differences for the annotated category. The results of this visualization are demonstrated in Figure 3.

### Cycle-Consistent Alignment and Adaption

Once the sub-prototypes are mined through memory learning, the method of cycle-consistent matching, inspired by DCC [24], is employed to align the embedding \(\hat{Z}\). The cycle-consistent matching is preferred due to it can provides a better fit to the memory structure compared to other UniDA methods. The other method, One-vs-All Network (OVANet), proposed by Saito et al. [40], needs to train the memory multiple times, which can lead to a significant computational overhead. In brief, the Cycle-Consistent Alignment provides a solution by iteratively learning a consensus set of clusters between the two domains. The consensus clusters are identified based on the similarity of the prototypes, which is measured using a similarity metric. The similarity metric is calculated on the feature representations of the prototypes. For unknown classes, we set the size \(N\) of our memory during the initial phase to be larger than the number of possible sub-classes that may be learned in the source domain. This size is a hyperparameter that is adjusted based on the dataset size. Redundant sub-prototypes are invoked to represent the \(\hat{Z}\), when encountering unknown classes, allowing for an improved distance separation between unknown and known classes in the feature space.

**Training Objective**. The adaptation loss in our training is similar to that of DCC, as \(\mathcal{L}_{DA}\):

\[\mathcal{L}_{DA}=\mathcal{L}_{ce}+\lambda_{1}\mathcal{L}_{cdd}+\lambda_{2} \mathcal{L}_{reg},\] (7)

where the \(\mathcal{L}_{ce}\) denotes the cross-entropy loss on source samples, \(\mathcal{L}_{cdd}\) denotes the domain alignment loss and \(\mathcal{L}_{reg}\) denotes the regularizer. For the auxiliary reconstruction task, we add a mean-squared-error (MSE) loss function, denoted as \(\mathcal{L}_{rec}\). Thus, the model is optimized with:

\[\mathcal{L}=\mathcal{L}_{DA}+\lambda_{3}\mathcal{L}_{rec}=\mathcal{L}_{ce}+ \lambda_{1}\mathcal{L}_{cdd}+\lambda_{2}\mathcal{L}_{reg}+\lambda_{3}\mathcal{ L}_{rec}.\] (8)

## 4 Experiments

### Datasets and Evaluation Metrics

We first conduct the experiments in the UniDA setting [47] where private classes exist in both domains. Moreover, we also evaluate our approach on two other sub-cases, namely Open-Set Domain Adaptation (OSDA) and Partial Domain Adaptation (PDA).

**Datasets**. Our experiments are conducted on four datasets: Office-31 [37], which contains 4652 images from three domains (DSLR, Amazon, and Webcam); OfficeHome

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

**Effect of Memory-Assisted Sub-Prototype Mining**. As the results shown in table 2, table 3, table 4 and table 5, the MemSPM+DCC evaluted on four benchmarks has surpassed the DCC on UniDA, OSDA and PDA scenarios. The MemSPM can significantly improve the performance of the DCC when using ViT-B/16 as backbone. The reason for utilizing the ViT-B/16 is that the memory module of the MemSPM with huge latent space is initialized by randomly normal distribution, which make it hard to retrieve the different sub-prototypes at early stages of training. So, we need ViT as backbone, which have learned a more global feature space.

**Sensitivity to Hyper-parameters**. We conducted experiments on the VisDA dataset under the UniDA setting to demonstrate the impact of hyperparameters \(S\) and \(N\) on the performance of our method. The impact of \(S\) are shown in Figure 3. When \(S\geq 20\), the performance achieve a comparable level. At the same time, the performance of the model is not sensitive to the value of \(N\), when \(S=30\).

## 5 Conclusion

In this paper, we propose the Memory-Assisted Sub-Prototype Mining (MemSPM) method, which can learn the intra-class diversity by mining the sub-prototypes to represent the sub-classes. Compared with the previous methods, which overlook the intra-class structure by using one-hot label, our MemSPM can learn the class feature from a more subdivided sub-class perspective to improve adaptation performance. At the same time, the visualization of the tSNE and reconstruction demonstrates the sub-prototypes have been well learned as we expected. Our MemSPM method exhibits superior performance in most cases compared with previous state-of-the-art methods on four benchmarks.

Figure 3: (a) The tSNE visualization shows the feature space of the sub-classes belonging to the each category, which demonstrate the MemSPM mining the sub-prototypes successfully. (b) The results of different values of \(S\) and \(N\). (c) The reconstruction visualization shows what have been learned in the memory, which demonstrate the intra-class diversity have been learned by MemSPM.

## References

* [1] Silvia Bucci, Mohammad Reza Loghmani, and Tatiana Tommasi. On the effectiveness of image rotation for open set domain adaptation. In _Proceedings of the European Conference on Computer Vision_, pages 422-438, 2020.
* [2] Pau Panareda Busto, Ahsan Iqbal, and Juergen Gall. Open set domain adaptation for image and action recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 42(2):413-429, 2018.
* [3] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Partial transfer learning with selective adversarial networks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 2724-2732, 2018.
* [4] Zhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang. Partial adversarial domain adaptation. In _Proceedings of the European Conference on Computer Vision_, pages 135-150, 2018.
* [5] Zhangjie Cao, Kaichao You, Mingsheng Long, Jianmin Wang, and Qiang Yang. Learning to transfer examples for partial domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2985-2994, 2019.
* [6] Wanxing Chang, Ye Shi, Hoang Tuan, and Jingya Wang. Unified optimal transport framework for universal domain adaptation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 29512-29524. Curran Associates, Inc., 2022.
* [7] Jin Chen, Xinxiao Wu, Lixin Duan, and Shenghua Gao. Domain adversarial reinforcement learning for partial domain adaptation. _IEEE Transactions on Neural Networks and Learning Systems_, 33(2):539-553, 2020.
* [8] Liang Chen, Yihang Lou, Jianzhong He, Tao Bai, and Minghua Deng. Geometric anchor correspondence mining with uncertainty modeling for universal domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16134-16143, 2022.
* [9] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Deep reconstruction-classification networks for unsupervised domain adaptation. In _Proceedings of the European Conference on Computer Vision_, pages 597-613, 2016.
* [10] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Semi-supervised deep learning with memory. In _Proceedings of the European Conference on Computer Vision_, pages 268-283, 2018.
* [11] Zhihong Chen, Chao Chen, Zhaowei Cheng, Boyuan Jiang, Ke Fang, and Xinyu Jin. Selective transfer with reinforced transfer network for partial domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12706-12714, 2020.
* [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [13] Zhekai Du, Jingjing Li, Hongzu Su, Lei Zhu, and Ke Lu. Cross-domain gradient discrepancy minimization for unsupervised domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3937-3946, 2021.
* [14] Bo Fu, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Learning to detect open classes for universal domain adaptation. In _Proceedings of the European Conference on Computer Vision_, pages 567-583, 2020.
* [15] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Francis Bach and David Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 1180-1189. PMLR, 2015.
* [16] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. _The journal of machine learning research_, 17(1):2096-2030, 2016.
* [17] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1705-1714, 2019.
* [18] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. _arXiv preprint arXiv:1410.5401_, 2014.
* [19] Tzu Ming Harry Hsu, Wei Yu Chen, Cheng-An Hou, Yao-Hung Hubert Tsai, Yi-Ren Yeh, and Yu-Chiang Frank Wang. Unsupervised domain adaptation with imbalanced cross-domain data. In _Proceedings of the IEEE International Conference on Computer Vision_, 2015.
* [20] JoonHo Jang, Byeonghu Na, Dong Hyeok Shin, Mingi Ji, Kyungwoo Song, and Il-Chul Moon. Unknown-aware domain adversarial learning for open-set domain adaptation. _Advances in Neural Information Processing Systems_, 35:16755-16767, 2022.

* [21] Tarun Kalluri, Astuti Sharma, and Manmohan Chandraker. Memsac: Memory augmented sample consistency for large scale domain adaptation. In _Proceedings of the European Conference on Computer Vision_, pages 550-568, 2022.
* [22] Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G. Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, June 2019.
* [23] Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy for unsupervised domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10285-10295, 2019.
* [24] Guangrui Li, Guoliang Kang, Yi Zhu, Yunchao Wei, and Yi Yang. Domain consensus clustering for universal domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9757-9766, 2021.
* [25] Shuang Li, Chi Harold Liu, Qiuxia Lin, Qi Wen, Limin Su, Gao Huang, and Zhengming Ding. Deep residual correction network for partial domain adaptation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(7):2329-2344, 2020.
* [26] Jian Liang, Dapeng Hu, Jiashi Feng, and Ran He. Umad: Universal model adaptation under domain and category shift. _arXiv preprint arXiv:2112.08553_, 2021.
* [27] Jian Liang, Yunbo Wang, Dapeng Hu, Ran He, and Jiashi Feng. A balanced and uncertainty-aware approach for partial domain adaptation. In _Proceedings of the European Conference on Computer Vision_, pages 123-140, 2020.
* [28] Hong Liu, Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Qiang Yang. Separate to adapt: Open set domain adaptation via progressive separation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2927-2936, 2019.
* [29] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. _Advances in Neural Information Processing Systems_, 31, 2018.
* [30] Zhihe Lu, Yongxin Yang, Xiatian Zhu, Cong Liu, Yi-Zhe Song, and Tao Xiang. Stochastic classifiers for unsupervised domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9111-9120, 2020.
* [31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [32] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019.
* [33] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge. _arXiv preprint arXiv:1710.06924_, 2017.
* [34] Sanqing Qu, Tianpei Zou, Florian Rohrbein, Cewu Lu, Guang Chen, Dacheng Tao, and Changjun Jiang. Upcycling models under domain and category shift. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023.
* [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR, 2021.
* [36] Jack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex Graves, and Timothy Lillicar. Scaling memory-augmented neural networks with sparse reads and writes. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.
* [37] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In _Proceedings of the European Conference on Computer Vision_, pages 213-226, 2010.
* [38] Aadarsh Sahoo, Rameswar Panda, Rogerio Feris, Kate Saenko, and Abir Das. Select, label, and mix: Learning discriminative invariant feature representations for partial domain adaptation. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 4210-4219, 2023.
* [39] Kuniaki Saito, Donghyun Kim, Stan Sclaroff, and Kate Saenko. Universal domain adaptation through self supervision. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 16282-16292. Curran Associates, Inc., 2020.

* [41] Kuniaki Saito and Kate Saenko. Ovanet: One-vs-all network for universal domain adaptation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9000-9009, 2021.
* [42] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2018.
* [43] Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Open set domain adaptation by backpropagation. In _Proceedings of the European Conference on Computer Vision_, pages 153-168, 2018.
* [44] Tasfa Shermin, Guojun Lu, Shyh Wei Teng, Manzur Murshed, and Ferdous Sohel. Adversarial network with multiple classifiers for open set domain adaptation. _IEEE Transactions on Multimedia_, 23:2732-2744, 2020.
* [45] Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. _arXiv preprint arXiv:1802.08735_, 2018.
* [46] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015.
* [47] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2017.
* [48] Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Universal domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2720-2729, 2019.
* [49] Chaohui Yu, Jindong Wang, Yiqiang Chen, and Meiyu Huang. Transfer learning with dynamic adversarial adaptation network. In _2019 IEEE International Conference on Data Mining_, pages 778-786. IEEE, 2019.
* [50] Jing Zhang, Zewei Ding, Wanqing Li, and Philip Ogunbona. Importance weighted adversarial nets for partial domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 8156-8164, 2018.
* [51] Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan. Domain-symmetric networks for adversarial domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5031-5040, 2019.