# Feature Dropout: Revisiting the Role of Augmentations in Contrastive Learning

 Alex Tamkin, Margalit Glasgow, Xiluo He, Noah Goodman

Stanford University

###### Abstract

What role do augmentations play in contrastive learning? Recent work suggests that good augmentations are _label-preserving_ with respect to a specific downstream task. We complicate this picture by showing that label-destroying augmentations can be useful in the foundation model setting, where the goal is to learn diverse, general-purpose representations for _multiple_ downstream tasks. We perform contrastive learning experiments on a range of image and audio datasets with multiple downstream tasks (e.g. synthetic datasets combining two classes, such as images and digits, and naturalistic datasets labeled with dozens of attributes). In controlled experiments where we destroy features at different rates, we find that destroying one feature a modest fraction of the time can improve learning of other features, while still enabling the dropped out feature to be learned well. Additionally, we show how this hypothesis can help explain the success of Viewmaker Networks, which generate augmentations that appear to target and destroy different features in the input examples, yet often result in better performance than standard augmentations across tasks. To support our empirical results, we theoretically analyze a simple contrastive learning setting with a linear model. In this setting, we show that label-destroying augmentations are crucial for preventing one set of features from suppressing the learning of features useful for another downstream task. Our results highlight the need for analyzing the interaction between _multiple_ downstream tasks when trying to explain the success of foundation models.

## 1 Introduction

In recent years, foundation models [5] have exhibited remarkable progress on a range of AI tasks [13; 31; 37; 36; 6; 11; 25; 1; 38]. A crucial characteristic of foundation models is that they can be adapted for a range of downstream tasks. For example, a foundation model trained on ImageNet should ideally not only perform well at object classification, but should also have learned general features useful for localization, segmentation, and other visual tasks. Indeed, this is borne out by recent work showing the high accuracy of foundation models on a range of downstream tasks [9], as well as a range of analysis work showing that models learn high-level semantic features including texture, color, pose, and style [19].

One popular strategy for training foundation models involves training models to match transformed versions (known as _views_ or _augmentations_) of the same input. For example, image views might include common data augmentations such as cropping or color jitter [9], while views for speech might include pitch modulation or spectrogram masking [27; 35]. This family of objectives includes contrastive approaches such as SimCLR and MoCo, as well as non-contrastive approaches such as BYOL and SwAV [9; 23; 20; 7].

Given the central importance of these views for defining the self-supervised task, much work has focused on the question of what views lead to high-quality representations. The prevailing consensus, exemplified by Tian et al. [52], holds that views should be _label-preserving_ with respectto a downstream task. In other words, because the contrastive loss will produce representations which are _invariant_ to features that vary across views, any information we wish to preserve in the representations should not be altered by such views. As Tian et al. [52] write: "_A good set of views are those that share the minimal information necessary to perform well at the downstream task._"

Here, we question whether this assumption--in particular, with its focus on a single task--is enough to explain why contrastive foundation models succeed on a _range_ of downstream tasks. In Section 2, we observe that the actual choice and application of **views in practice** does not align with this prevailing consensus. For example, complete invariance to several common augmentations (e.g. shifts in brightness or cropping) is undesirable since augmentations of inputs from different classes can collide. Furthermore, in many cases there are explicit ways to specify invariances (e.g. converting images to grayscale) that researchers avoid, instead specifying them indirectly via augmentations (e.g. hue shifts). These observations suggest that specifying invariances is not the sole role of these views.

Instead, we suspect that augmentations serve as a form of **feature dropout**--preventing any one feature from becoming a shortcut feature and suppressing the learning of other features. We study this idea empirically with a set of synthetic datsets constructed by overlaying a simple element (e.g. a digit, shape, letter, or speech snippet) on an image or audio recording. We find that adding such a simple feature can dramatically decrease how well the other feature is learned, but that stochastically "dropping out" the simple feature can enable both features to be learned well. Next, we use this perspective to explain the success of Viewmaker Networks, a recently proposed method that generates augmentations for contrastive learning via adversarial training. We apply viewmaker and expert views to these same synthetic datasets, as well as a naturalistic dataset of facial images annotated with 40 different attributes (e.g. "wearing lipstick" or "blond hair"). Across these settings, we find that viewmaker augmentations learn to selectively obscure various features in the input. Despite this, the viewmaker representations still learn the downstream tasks well, while expert views often struggle on one or more of the attributes. This further suggests that being label-preserving is not a necessary property of good views, as long as the label information is still _sometimes_ accessible.

Finally, we formalize the intuition that feature dropout can aid learning with a theoretical analysis of a simple linear contrastive setting. In this setting, we characterize how the noisiness of each feature directly determines how quickly features are learned, and uncover an **interaction between features** governing how fast they are learned. In particular, we show how learning one feature quickly can suppress the learning of other features, and show that adding noise to the "easiest" feature can _increase_ the rate at which other features are learned. This further indicates that _label-destroying_ augmentations may have a direct role in ensuring that contrastive models learn a broad range of features for downstream tasks.

Overall, these findings suggest the need to revisit common assumptions about the role of augmentations for contrastive learning in the foundation model setting, and move towards a better understanding of how to train generalist models that learn diverse features from unlabeled data.

## 2 Common practices are at odds with the "invariance" explanation

We begin by briefly exploring several common augmentations used in contrastive learning for natural images, and explore how they come into conflict with the common assumption described above. First, we observe that many common augmentations can affect the label of the input, depending on the downstream task. For example, many downstream image recognition tasks require color information (e.g. identifying bird species) or brightness (e.g. scene or time-of-day classification), implying that invariance to these characteristics would be undesirable. Yet hue shifts, greyscaling, and brightness shifts are common augmentations used in contrastive learning [9; 23].

Second, repeated application of some augmentations causes challenges for _all_ downstream tasks. For example, applying brightness shifts repeatedly results in any image turning completely black or completely white. Thus the class label cannot be truly invariant to this augmentation, since inputs from different classes can experience an "augmentation collision" at this black or white image (this is formalized in Appendix B).1 This argument also applies to other augmentations, including shifts in contrast2 and random masking.

Footnote 1: Note that invariance is not to be confused with the related but distinct property of equivariance, often discussed as a desirable property of network architectures (e.g. see Fukushima and Miyake [17]; Chen et al. [8]).

Footnote 2: Continuous reduction in contrast eventually produces single-color images, given finite precision images.

Third, some augmentations are commonly used _despite_ ways of explicitly encoding invariance to them. For example, two image augmentations are _hue shifts_ and _greyscaling_. Invariance to both of these augmentations can be explicitly encoded by always converting an image to greyscale. Yet doing so is not common practice because color information is still desirable for many downstream tasks.

The contradictions between the invariance rationale for augmentations in contrastive learning and these common practices suggest the need for additional explanations for the role of augmentations.

Controlled experiments demonstrate the benefits of feature dropout in settings with multiple features

In this section, we present controlled experiments on synthetic data demonstrating how label-destroying augmentations can balance the learning of multiple features during contrastive learning. Our core toolkit is to overlay images with a set of synthetic features. As we will show, the presence of these synthetic features causes the network to learn the synthetic features very well at the expense of the image features, as measured by downstream classification accuracy. However, "dropping out" these features some fraction of the time during contrastive learning enables us to trade-off how well each feature is learned, while not resulting in complete invariance to either set of features.

### Datasets

We consider the behavior of viewmaker networks on four synthetic datasets, including three image and one audio dataset. Each dataset is constructed in such a way as to support two distinct downstream classification tasks, enabling us to examine precisely how well each downstream task is learned. The presence of two downstream tasks enables us to analyze the foundation model setting where we wish to learn features relevant for multiple downstream tasks, as opposed to one set or the other.

Image datasetsThe three image datasets are based on the canonical CIFAR-10 image-recognition dataset [28] (MIT-License). One task is always to predict the CIFAR-10 object label (e.g. airplane or bird). The other task is dependent on an additional feature overlaid on the image: **C+Shapes:** The CIFAR-10 image is overlaid with one of three randomly-colored shapes: a square, a triangle, or a circle. The second task is to predict what shape was overlaid (N=3 classes). **C+Digits:** The CIFAR-10 images are overlaid with four copies of a randomly-sampled digit from the MNIST dataset. The second task is to predict the digit class (N=10 classes). **C+Letters:** The CIFAR-10 images are overlaid with four copies of a randomly-colored English letter. The second task is to predict the class of the letter (N=26 classes).

Audio datasetThe audio dataset is created by overlaying the audio of a spoken digit (from the AudioMNIST dataset [3], MIT License) with a random background sound (collected from one of three possible classes: cafe, machinery, and traffic) [43; 42]. The tasks are to predict the digit class (N=10 classes) and to predict the sound class (N=3 classes). Inputs are presented to the network as log mel spectrograms.

### Experiments

PretrainingWe pretrain with the SimCLR algorithm for 200 epochs with a batch size of 256 and a temperature of 0.1. We use a ResNet-18 model with standard modifications for smaller inputs (including a smaller stride and no initial maxpool) as used in Tamkin et al. [49]. We use the standard SimCLR augmentations for the image datasets [9], with the exception of gaussian blurring, which we found to have no impact on downstream performance (Appendix C.4). For audio, we use the SpecAug [35] augmentations, which randomly mask out different frequency and time bands, as well as the WaveAug [27] augmentations, which alter various properties of the waveform such as the pitch and speed.

Linear EvaluationWe evaluate the quality of the learned representations by training a linear softmax classifier on top of the prepool representations. We train for 100 epochs, using the same parameters as Tamkin et al. [49], using SGD with learning rate 0.01, momentum 0.9, weight decay 0, and batch size 128. We train separate linear classifiers using the same pretrained network for each downstream task [9]. Augmentations are applied during training but not evaluation.

ResultsAs shown in Figure 1, we see an interaction between the two features, where dropping out the synthetic feature improves learning of the main image or audio class. Across settings, we see regions where both features are still learned well, providing a concrete example of how feature dropout can be useful when learning multiple features during contrastive learning.

## 4 Viewmaker Networks Succeed Despite Destroying Label Information

As another point of evidence that good views need not be label-preserving, we consider viewmaker networks [49], a generative model which produces augmentations for contrastive learning. Intuitively, viewmakers learn a stochastic augmentation policy that makes the contrastive task as hard as possible for the encoder. The stochastic augmentations are parameterized as additive perturbations bounded by an \(L_{1}\) norm, meaning the viewmaker can alter but not completely destroy the original image.

Formally, given an input \(x\in\mathbb{N}\), a viewmaker network \(V_{\psi}\) is trained jointly with an encoder \(E_{\theta}\) to optimize the minimax expression:

\[\max_{\psi}\min_{\theta}\mathcal{L}\left(E_{\theta}\left(x+\epsilon\frac{V_{ \psi}(x,\delta_{1})}{||V_{\psi}(x,\delta_{1})||_{1}}\right),\ E_{\theta}\left(x+ \epsilon\frac{V_{\psi}(x,\delta_{2})}{||V_{\psi}(x,\delta_{2})||_{1}}\right)\right)\]

Here \(\mathcal{L}\) is a multiview loss function (e.g. [9; 23]), \(x\) is a minibatch of inputs, \(||\cdot||_{1}\) is the \(L_{1}\) norm, \(\epsilon\) is the _distortion budget_ controlling the strength of the views, and \(\delta_{1},\delta_{2}\sim N(0,I)\) are random inputs that enable the viewmaker to learn a stochastic augmentation policy. We clamp the output of the viewmaker for images to \([0,1]\) as in Tamkin et al. [49].

Viewmaker networks learn to stochastically alter different parts of the input, including task-relevant features, meaning that these augmentations are not label-preserving. Nevertheless, as we will see shortly, viewmaker networks enable strong performance on multiple downstream tasks, including often better performance than expert-designed augmentations. Moreover, this **feature dropout** capability of viewmaker networks may help them learn many features well rather than just the easiest.

Figure 1: Linear probing accuracy (y-axis) after contrastive learning with varying rates of dropout of the synthetic feature (x-axis). In all cases, we see a clear tradeoff between features, where dropping out the synthetic feature improves learning of the object class.

### Experiments and Results

Experimental SettingsWe use the same experimental settings as Section 3, however without manual dropout of the synthetic features. In one set of experiments, we use the standard augmentations from Chen et al. [9], which we henceforth refer to as the _expert augmentations_. For the experiments with _viewmaker augmentations_, we use a budget of \(\epsilon=0.05P\) for the image datasets, and \(\epsilon=0.125P\) for the audio datasets, where \(P\) is the number of pixels in the input.

Additional naturalistic dataset with 40 attributesTo further validate the behavior of viewmaker on realistic multi-feature datasets, we consider the CelebA [32] dataset, a large database of faces annotated with 40 different features. These features cover a wide spectrum of facial attributes, such as "Has Bangs" "Wearing Lipstick" and "Smiling," and enable us to further analyze whether viewmaker networks learn a broader range of features than commonly-used augmentations.

Figure 2: **Comparison of viewmaker and expert augmentations on datasets with multiple features.** The viewmaker augmentations adapt to the particular semantics of the input data, and make targeted perturbations which remove the class-relevant information of the synthetic features (e.g. occluding the digit, shape, letter, or speech). Despite this, the encoder network is still able to learn strong representations. _Rows_ (from top): Digits, Shapes, Letters, Audio. _Columns_ (from left): Expert augmentations, viewmaker augmentations, difference between original and viewmaker augmentation, rescaled to [0,1]. Center image in each grid is the original. Audio Expert views shown are Spectral views.

### Results on two-feature datasets

Qualitative evidence of feature dropoutVisually, the viewmaker augmentations seem to stochastically alter different aspects of the input, as shown in Figure 2. In addition to modifying the background of each input, the viewmaker also selectively modifies the additional synthetic features added to each domain: **C+Digits:** The viewmaker augmentations selectively add pixels to the MNIST digits, making it difficult to distinguish which number is present. **C+Shapes:** The viewmaker augmentations sometimes draw squares around the shape in the center, making it difficult to determine the shape class. **C+Letters:** The viewmaker draws letter-like markings on top of the letters, obscuring the letter identity and color. **Audio:** The viewmaker identifies the narrow band corresponding to the speech and applies perturbations to it. As can be seen in Figure 2, these label-destroying augmentations are quite common, occuring in a sizable fraction of the sampled views.

Quantitative evidence of feature dropoutWe also measure this selectivity of features quantitatively in Appendix C.2 and Figure 6. We augment images 1,200 times and observe the resulting probability assigned to the correct object class. Two clear modes appear for viewmaker, but not expert, augmentations. This corresponds to the fraction of time the viewmaker destroys the overlaid feature information (low P(correct object class)) and preserves it (high P(correct object class)).

Viewmaker succeeds despite destroying label informationAs shown in Table 1 and Table 2, viewmaker networks achieve good accuracy on both tasks, while expert augmentations frequently achieve lower performance on one or both. On image tasks, for example, while expert views achieve slightly higher performance when classifying the image only, they see a large drop in accuracy when

\begin{table}
\begin{tabular}{l c c|c c} \hline \hline  & VM (CIFAR-10) & Expert (CIFAR-10) & VM (Object) & Expert (Object) \\ \hline CIFAR-10 Only & 84.5 & **86.2** & - & - \\ \hline C+Shape & **79.8** & 76.0 & **100.0** & **100.0** \\ C+Digit & **69.3** & 58.8 & **94.3** & 86.7 \\ C+Letter & 71.9 & **74.8** & **96.9** & 94.1 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Transfer accuracy on different features. Viewmaker (VM) networks are able to achieve good performance across multiple downstream tasks, while expert views sometimes falter. Networks are pretrained on the datasets on the left, and transfer accuracy is reported for the different conditions on the columns. Runs are averages of three seeds (with the exception of CIFAR-10 Only, from [49]).**

Figure 3: **Viewmaker networks capture a broader range of features on a naturalistic dataset. Linear evaluation F1 score on CelebA for viewmaker and expert views. Attributes are sorted from lowest to highest accuracy within each augmentation type.**the synthetic feature is added. In two of these cases (Shape and Digit) the viewmaker models are able to achieve a higher accuracy on both the image and the synthetic feature, while on the third (Letters) they achieve slightly lower accuracy on the images but achieve half the error on the synthetic object. For the audio experiments the picture is similar--viewmaker avoids catastrophic drops in performance learning both features together, achieving the highest accuracy on both, while the expert views see larger drops and worse overall performance. Note that the high performance of expert views for our control tasks (CIFAR-10/Speech/Sound Only) indicates that the viewmaker views are not merely better all-around views, but that they specifically help the model learn multiple features.

These results provide additional evidence that label-preserving views are not necessary for learning good representations--and that feature dropout may improve learning of multiple tasks.

### Results on naturalistic dataset

We observe similar qualitative and quantitative results for the CelebA dataset. We train models using the same settings in Section 3.2, using a budget of 0.01, and indeed find that viewmakers capture a much broader range of features, achieving an average F1 Score of **0.509** over the 40 features, compared to **0.334** for the SimCLR augmentations. In addition, the viewmaker augmentations clearly capture a wider range of features, as can be seen in Figure 3, especially at the tail of the distribution. Furthermore, we see bimodal disruption patterns in over two-thirds of the CelebA features, as shown in Figure 9, indicating significant feature dropout across in most attributes. We also show qualitative results in Figure 8 demonstrating that the viewmaker alters attributes such as facial features, hair color, and background elements in the scene. These results further support the hypothesis that viewmaker networks exhibit feature dropout, yet capture a broader range of features than expert views.

## 5 Theoretical Analysis of Feature Interactions in Linear Contrastive Setting

In this section, we analyze a simple linear model that captures the essence of how label-destroying augmentations can improve downstream accuracy. We study a setting where the data contains many underlying features that are relevant to downstream classification tasks, and where these features are preserved to varying degrees across augmentations. We will show that a linear model trained with a contrastive objective learns these features, and that adding noise to one feature can speed learning of others during gradient descent. One difference between the linear setting we analyze and Section 4 is that here add stochastic Gaussian noise to destroy features across augmentations, as opposed to the bimodal feature dropout behavior of viewmaker networks seen in Figure 2.

Figure 4: We show how label-destroying augmentations can aid learning of other features in a linear contrastive setting: (a) The correlation of the \(k\)th feature of an augmentation pair, shown for \(d=2\). Each pair \(u_{k}^{(i)}\) and \(v_{k}^{(i)}\) have correlated projections onto the ground truth \(\mu_{k}\) direction, representing the feature conserved across augmentations. (b) Feedforward linear network which computes the representation \(f_{\Theta}(w)\). As each feature \(\mu_{k}\) is learned (\(\theta_{k}\rightarrow\mu_{k}\)) the representations of the two views \(f_{\Theta}(u^{(i)})\), \(f_{\Theta}(v^{(i)})\) become more similar, decreasing the contrastive loss.

### Data Model and Setting

We study a model which consists of data with \(K\) distinct features, each corresponding to some ground truth unit-vector directions \(\mu_{1},\dots,\mu_{K}\in\mathbb{R}^{d}\). We sample each data point \(u\in\mathbb{R}^{K\times d}\) and its _augmentation_ (a.k.a. its _positive pair_ or its _view_) \(v\in\mathbb{R}^{K\times d}\) as follows. For \(k\in 1,\dots,K\), the \(k\)th row of \(u\), which we denote \(u_{k}\), is drawn from the Gaussian distribution \(\mathcal{N}(0,I_{d})\). The \(k\)th row of the augmentation, \(v_{k}\), is drawn from the same distribution, but is correlated with \(u_{k}\) in the \(\mu_{k}\)-direction (and is otherwise independent in the other directions). The strength of the correlation is governed by parameter \(\alpha_{k}\in[0,1]\) in the following sense: \(v_{k}^{T}\mu_{k}=\alpha_{k}u_{k}^{T}\mu_{k}+\sqrt{1-\alpha_{k}^{2}}\xi\), where \(\xi\sim\mathcal{N}(0,1)\). Thus the larger \(\alpha_{k}\), the stronger the correlation in that feature across the two views. Figure 4(a) visualizes the correlation of \((u_{k},v_{k})\) in an augmented pair. Formally, we can write that \((u_{k},v_{k})\sim\mathcal{N}\left(0,\begin{pmatrix}I_{d}&\alpha_{k}\mu_{k}\mu_ {k}^{T}\\ \alpha_{k}\mu_{k}\mu_{k}^{T}&I_{d}\end{pmatrix}\right)\), for a vector \(\boldsymbol{\alpha}\in[0,1]^{k}\).

We will learn a model \(\Theta\in\mathbb{R}^{K\times d}\), which represents a collection of \(K\) feature extractors, as pictured in Figure 4(b). The model \(\Theta\), with rows \(\{\theta_{k}\}_{k\in[K]}\), maps a data point \(w\in\mathbb{R}^{K\times d}\) to a representation \(f_{\Theta}(w)\in\mathbb{R}^{K}\) by computing a score \(w_{k}^{T}\theta_{k}\) for each element in the representation. That is, \((f_{\Theta}(w))_{k}=w_{k}^{T}\theta_{k}\). Our goal is that the model \(\Theta\) will be useful for a downstream classification task which depends on the ground truth features. A good representation will capture ground truth features that are correlated across augmentations, such that \(\theta_{k}\) is aligned with \(\mu_{k}\) or \(-\mu_{k}\).

Training.We will study the the evolution of \(\Theta\) as we optimize a standard constrastive learning objective using gradient descent [14; 9]. At each round of gradient descent, we sample a fresh batch of \(m\) data points and their augmentations, \((U,V):=\{(u^{(i)},v^{(i)})_{i\in[m]}\). For each \(i,j\in[m]\), we compute a similarity score \(z_{ij}:=\langle f_{\Theta}(u^{(i)}),f_{\Theta}(v^{(j)})\rangle=\sum_{k}(\theta _{k}^{T}u_{k}^{(i)})(\theta_{k}^{T}v_{k}^{(j)})\) using the dot product of their \(K\)-dimensional representations. We then compute the logits \(p_{ij}:=\frac{\exp(z_{ij})}{\sum_{j^{\prime}}\exp(z_{ij^{\prime}})}\) using the softmax function, and use the classwise cross entropy loss function \(\mathcal{L}(\Theta;U,V):=-\log(p_{ii})\).

### Main Result

We will study gradient descent (GD) on the cross entropy loss, and consider how adding noise to one feature affects the learning of the other features. As suggested earlier, we can measure how well we learn the \(k\)th feature by measuring the alignment of \(\theta_{k}\) with \(\mu_{k}\) or \(-\mu_{k}\). A natural way to measure this alignment is the acute angle between \(\pm\mu_{k}\) and \(\theta_{k}\), given by \(\arccos\left(\frac{\left|\mu_{k}^{T}\theta_{k}\right|}{\left\|\theta_{k} \right\|_{2}}\right)\). Lemma E.1 in Appendix E proves that this quantity directly determines the test accuracy \(\Theta\) on a natural downstream linear classification task.

Formally, we say we _add noise_ to some feature \(k^{\prime}\) of a data point \(v\), if for some \(\beta\in[0,1)\), we define the new noisy data point \(\tilde{v}\) to have coordinates \(\tilde{v}_{k^{\prime}}=\beta v_{k^{\prime}}+\sqrt{1-\beta^{2}}\xi\), where \(\xi\sim\mathcal{N}(0,I_{d})\), and \(\tilde{v}_{k}=v_{k}\) for \(k\neq k^{\prime}\). Thus if \((u,v)\) were a pair generated with the correlation coefficients \(\{\alpha_{k}\}_{k\in[K]}\), then the distribution of \((u,\tilde{v})\) comes from the modified correlation coefficients \(\{\tilde{\alpha}\}_{k\in[K]}\) with the single modification \(\tilde{\alpha}_{k^{\prime}}=\beta\alpha_{k}\). We now present our main theorem:

**Theorem 5.1** (Noise improves feature learning).: _There exists a universal constant \(C\), such that the following holds. Let \(\Theta^{(t+1)}=\Theta^{(t)}-\eta(\nabla\mathcal{L}(U,V;\Theta)+\lambda\Theta^{(t )})\), and \(\tilde{\Theta}^{(t+1)}=\Theta^{(t)}-\eta(\nabla\mathcal{L}(U,\tilde{V};\Theta)+ \lambda\Theta^{(t)})\), where \(\tilde{V}\) is \(V\) with any amount of added noise in the \(k^{\prime}\) feature. This has the effect of changing \(\alpha_{k^{\prime}}\) to \(\tilde{\alpha}_{k^{\prime}}\) for any \(\tilde{\alpha}_{k^{\prime}}<\alpha_{k^{\prime}}\). Then for any \(k\neq k^{\prime}\), if \(|\theta_{k}^{T}\mu_{k}|\leq\frac{1-\alpha_{k^{\prime}}^{2}}{C}\|\theta_{k}\|\), \(\|\theta_{k^{\prime}}\|^{3}\leq|\theta_{k^{\prime}}^{T}\mu_{k}|\), and \(\|\theta_{k}\|^{2}\leq\frac{\alpha_{k}(1-\alpha_{k^{\prime}}^{2})}{C}\), then for a small enough step size \(\eta\), \(\mathbb{E}_{U,V}\left[\arccos\left(\frac{\left|\mu_{k}^{T}\tilde{\theta}_{k}^{( t+1)}\right|}{\left\|\theta_{k}^{(t+1)}\right\|_{2}}\right)\right]> \mathbb{E}_{U,\tilde{V}}\left[\arccos\left(\frac{\left|\mu_{k}^{T}\tilde{ \theta}_{k}^{(t+1)}\right|}{\left\|\tilde{\theta}_{k}^{(t+1)}\right\|_{2}} \right)\right]\)._

We briefly comment on the three assumptions on \(\Theta\) in the theorem. The first assumption, \(|\theta_{k}^{T}\mu_{k}|\leq\frac{1-\alpha_{k^{\prime}}^{2}}{C}\|\theta_{k}\|\) requires that \(\theta_{k}\) is not too aligned with \(\mu_{k}\) - that is, the result applies to all features \(k\) that are not already learned too well. The second two assumptions are satisfied if the \(k^{\prime}\)th feature has been learned to some extent, and the norm of \(\theta_{k}\) and \(\theta_{k^{\prime}}\) are small, which can be enforced throughout training with \(\ell_{2}\) regularization.

The theorem guarantees that at any point in training, if we add noise to the \(k\)'th feature, the next step of GD learns all other features _better_ than if we did not add noise. To validate the implication of this result for the complete trajectory of GD, we include simulations in Appendix D. Our experiments show that introducing noise part-way through training to dominant features can significantly speed the alignment of weak features, with only a small cost to the alignment of the dominant features. We prove our result in Appendix E, including intuition and an overview of the ideas in Section E.3.

## 6 Related work

Understanding contrastive and multiview learningMany prior works have laid the foundations for current contrastive and multiview learning algorithms [4; 21; 14; 55; 2; 34; 23; 9]. Several works analyze contrastive learning to identify important factors [12; 58] or how contrastive models differ from supervised learning [57; 15; 26]. HaoChen et al. [22] study contrastive learning using the concept of an augmentation graph. This model assumes the fraction of non-label preserving augmentations is "extremely small"; interestingly, we show in practice this can be quite large and still yield good performance. Wang et al. [54] theoretically study contrastive learning under an assumption of label-preserving augmentations, though they show that such an assumption alone does not suffice to learn. Most relevant to our work, Tian et al. [52]; Ericsson et al. [16] study how the information shared between different views impacts learning of downstream tasks. We complicate this picture by analyzing the foundation model setting where a single model must learn features for multiple tasks that are not known in advance. In this setting, we find that label-destroying perturbations, thought to be harmful by Tian et al. [52], are useful for preventing one feature from suppressing others.

Feature suppressionOur work is closely connected to the notion of _feature suppression_[24], where the presence of one feature can suppress the learning of others. Several works explore this concept in contrastive learning. For example, the original SimCLR paper [9] noted that color jitter augmentation was necessary to prevent the network from using only the color profile of the input to solve the contrastive task. Followup work [10] characterizes how hyperparameters and dataset features affect feature suppression, including through a range of synthetic experiments that vary the amount of information shared between views. Other works have attempted to address feature suppression in contrastive learning, either via auxiliary losses [29] or by modifying representations in the latent space [39]. Our builds upon these works by empirically and theoretically investigating feature suppression as an alternate rationale for the role of augmentations, as opposed to invariance. We also show that an existing method, viewmaker networks [49], can identify and potentially neutralize suppressing features in an interpretable way, resulting in better performance than expert augmentations. These insights may also generalize to other self-supervised settings, such as language modeling, where multiple features may compete [47].

Spurious correlations and shortcut featuresOutside the framing of feature suppression, several other works explore how classifiers can learn or make use of unwanted features. Shortcut features [18] describe often-simple features (e.g. the average color of an input) which are learned by networks at the expense of more salient features (e.g. the object class). This notion is connected to spurious correlations [45] in deep learning which have been explored extensively [40; 41; 46; 53; 56], including in the context of self-supervised learning [33; 51]. Other works have also performed theoretical analysis of how related dynamics affect learning in the supervised setting [30; 44]. In general, if such spurious correlations are known in advance, one can often design augmentations to remove such correlations and improve learning. However, our work suggests that viewmaker networks may be a useful tool in cases where such features are not known a priori--both as an interpretability tool to visualize the different features a network relies on, and as a way to reduce reliance on particular features without completely destroying the information.

## 7 Discussion and Conclusion

We explore the idea that augmentations in contrastive learning function as a sort of "feature dropout." First, we show that in datasets with multiple features, dropping out one set of features improves learning of the other features. Second, feature dropout may help explain how viewmaker networks can learn a wide range of features well, despite producing augmentations which appear to destroy different features in the input. Finally, we theoretically analyze a linear contrastive setting where we prove that label-destroying views have a positive effect on contrastive learning if the goal is to avoid learning one feature at the expense of others.

Our work has limitations: for example, while our experiments consider image and audio data, self-supervised learning may be applied to a much wider range of modalities [48; 50]. In addition, our theoretical analysis considers a linear contrastive setting, whereas current neural networks are highly nonlinear. Improving upon both of these fronts is an exciting area for future work. On the other hand, understanding augmentations as dropping out easy features suggests possible ways of improving the performance of self-supervised learning. For example, a guided version of viewmaker might enable prioritizing a subset of important features for learning, or might enable dropping out unwanted features such as watermarks, sensitive information, other image artifacts.

The challenge of learning a broad range of useful features lies at the heart of self-supervised learning. We hope our work sheds light on this challenge in contrastive learning, especially as these objectives continue to develop and are applied more broadly and at larger scale.

## References

* [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. _ArXiv_, abs/2204.14198, 2022.
* [2] Philip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In _NeurIPS_, 2019.
* [3] Soren Becker, Marcel Ackermann, Sebastian Lapuschkin, Klaus-Robert Muller, and Wojciech Samek. Interpreting and explaining deep neural networks for classification of audio signals. _ArXiv_, abs/1807.03418, 2018.
* [4] Suzanna Becker and Geoffrey E. Hinton. Self-organizing neural network that discovers surfaces in random-dot stereograms. _Nature_, 355:161-163, 1992.
* [5] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyususha Kalluri, Siddharth Karamcheit, Geoff Keeling, Fershe Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Benjamin Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R'e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramer, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaavan You, Matei A. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. _ArXiv_, abs/2108.07258, 2021.
* [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.

* [7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. _Advances in Neural Information Processing Systems_, 33:9912-9924, 2020.
* [8] Shuxiao Chen, E. Dobriban, and Jane Lee. A group-theoretic framework for data augmentation. _arXiv: Machine Learning_, 2020.
* [9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. _ArXiv_, abs/2002.05709, 2020.
* [10] Ting Chen, Calvin Luo, and Lala Li. Intriguing properties of contrastive losses. _Advances in Neural Information Processing Systems_, 34, 2021.
* [11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Baindoor Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanulayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. _ArXiv_, abs/2204.02311, 2022.
* [12] Elijah Cole, Xuan S. Yang, Kimberly Wilber, Oisin Mac Aodha, and Serge J. Belongie. When does contrastive visual representation learning work? _ArXiv_, abs/2105.05837, 2021.
* [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _ArXiv_, abs/1810.04805, 2019.
* [14] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In _NIPS_, 2014.
* [15] Linus Ericsson, Henry Gouk, and Timothy M. Hospedales. How well do self-supervised models transfer? _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5410-5419, 2021.
* [16] Linus Ericsson, Henry Gouk, and Timothy M. Hospedales. Why do self-supervised models transfer? investigating the impact of invariance on downstream tasks. _ArXiv_, abs/2111.11398, 2021.
* [17] Kunihiko Fukushima and Sei Miyake. Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition. In _Competition and cooperation in neural nets_, pages 267-285. Springer, 1982.
* [18] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard S. Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. _ArXiv_, abs/2004.07780, 2020.
* [19] Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Christopher Olah. Multimodal neurons in artificial neural networks. 2021.
* [20] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in Neural Information Processing Systems_, 33:21271-21284, 2020.
* [21] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. _2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)_, 2:1735-1742, 2006.

* [22] Jeff Z. HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised deep learning with spectral contrastive loss. In _NeurIPS_, 2021.
* [23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9726-9735, 2020.
* [24] Katherine L. Hermann and Andrew Kyle Lampinen. What shapes feature representations? exploring datasets, architectures, and training. _ArXiv_, abs/2006.12433, 2020.
* [25] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. _ArXiv_, abs/2203.15556, 2022.
* [26] A. Tarun Karthik, Mike Wu, Noah D. Goodman, and Alex Tamkin. Tradeoffs between contrastive and supervised learning: An empirical study. _ArXiv_, abs/2112.05340, 2021.
* [27] Eugene Kharitonov, Morgane Riviere, Gabriel Synnaeve, Lior Wolf, Pierre-Emmanuel Mazare, Matthijs Douze, and Emmanuel Dupoux. Data augmenting contrastive learning of speech representations in the time domain. In _2021 IEEE Spoken Language Technology Workshop (SLT)_, pages 215-222. IEEE, 2021.
* [28] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
* [29] Tianhong Li, Lijie Fan, Yuan Yuan, Hao He, Yonglong Tian, Rogerio Schmidt Feris, Piotr Indyk, and Dina Katabi. Addressing feature suppression in unsupervised visual representations. 2020.
* [30] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large learning rate in training neural networks. _ArXiv_, abs/1907.04595, 2019.
* [31] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _ArXiv_, abs/1907.11692, 2019.
* [32] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _ICCV_, pages 3730-3738. IEEE Computer Society, 2015. ISBN 978-1-4673-8391-2. URL http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15.
* [33] Matthias Minderer, Olivier Bachem, Neil Houlsby, and Michael Tschann. Automatic shortcut removal for self-supervised representation learning. In _ICML_, 2020.
* [34] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6706-6716, 2020.
* [35] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition. _arXiv preprint arXiv:1904.08779_, 2019.
* [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* [37] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. _ArXiv_, abs/2102.12092, 2021.
* [38] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Manfred Otto Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. _ArXiv_, abs/2205.06175, 2022.

* Robinson et al. [2021] Joshua Robinson, Li Sun, Ke Yu, K. Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive learning avoid shortcut solutions? _Advances in neural information processing systems_, 34:4974-4986, 2021.
* Sagawa et al. [2019] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. _ArXiv_, abs/1911.08731, 2019.
* Sagawa et al. [2020] Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameterization exacerbates spurious correlations. _ArXiv_, abs/2005.04345, 2020.
* Saki and Kehtaravaz [2016] Fatemeh Saki and Nasser Kehtaravaz. Automatic switching between noise classification and speech enhancement for hearing aid devices. In _2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)_, pages 736-739, 2016. doi: 10.1109/EMBC.2016.7590807.
* Saki et al. [2016] Fatemeh Saki, Abhishek Sehgal, Issa Panahi, and Nasser Kehtaravaz. Smartphone-based real-time classification of noise signals using subband features and random forest classifier. In _2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 2204-2208, 2016. doi: 10.1109/ICASSP.2016.7472068.
* Shah et al. [2020] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. _ArXiv_, abs/2006.07710, 2020.
* Simon [1954] Herbert A Simon. Spurious correlation: A causal interpretation. _Journal of the American statistical Association_, 49(267):467-479, 1954.
* Srivastava et al. [2020] Megha Srivastava, Tatsunori B. Hashimoto, and Percy Liang. Robustness to spurious correlations via human annotations. In _ICML_, 2020.
* Tamkin et al. [2020] Alex Tamkin, Dan Jurafsky, and Noah Goodman. Language through a prism: A spectral approach for multiscale language representations. _Advances in Neural Information Processing Systems_, 33:5492-5504, 2020.
* Tamkin et al. [2021] Alex Tamkin, Vincent Liu, Rongfei Lu, Daniel E Fein, Colin Schultz, and Noah D. Goodman. Dabs: A domain-agnostic benchmark for self-supervised learning. _ArXiv_, abs/2111.12062, 2021.
* Tamkin et al. [2021] Alex Tamkin, Mike Wu, and Noah D. Goodman. Viewmaker networks: Learning views for unsupervised representation learning. _ArXiv_, abs/2010.07432, 2021.
* Tamkin et al. [2022] Alex Tamkin, Gaurab Banerjee, Mohamed Owda, Vincent Liu, Shashank Rammoorthy, and Noah Goodman. Dabs 2.0: Improved datasets and algorithms for universal self-supervision. 2022.
* Tamkin et al. [2022] Alex Tamkin, Dat Nguyen, Salil Deshpande, Jesse Mu, and Noah Goodman. Active learning helps pretrained models learn the intended task. _arXiv preprint arXiv:2204.08491_, 2022.
* Tian et al. [2020] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning. _ArXiv_, abs/2005.10243, 2020.
* Tu et al. [2020] Liffu Tu, Garima Lalwani, Spandana Gella, and He He. An empirical study on robustness to spurious correlations using pre-trained language models. _Transactions of the Association for Computational Linguistics_, 8:621-633, 2020.
* Wang et al. [2022] Yifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Chaos is a ladder: A new theoretical understanding of contrastive learning via augmentation overlap. _arXiv preprint arXiv:2203.13457_, 2022.
* Wu et al. [2018] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3733-3742, 2018.
* Xiao et al. [2021] Kai Y. Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. _ArXiv_, abs/2006.09994, 2021.

* [57] Xingyi Yang, Xuehai He, Yuxiao Liang, Yue Yang, Shanghang Zhang, and Pengtao Xie. Transfer learning or self-supervised learning? a tale of two pretraining paradigms. _ArXiv_, abs/2007.04234, 2020.
* [58] Nanxuan Zhao, Zhirong Wu, Rynson W. H. Lau, and Stephen Lin. What makes instance discrimination good for transfer learning? _ArXiv_, abs/2006.06606, 2021.

Code release

Our code is open-sourced at https://github.com/xiluohe/feature-dropout.

## Appendix B Formalization of observation in Section 2

**Definition B.1** (Invariance).: _A function \(f:\mathbb{R}^{m}\rightarrow\mathbb{R}^{n}\) is invariant to a set of transformations \(G\) if and only if \(f\circ g(x)=f(x)\) for all \(x\in\mathbb{R}^{m}\) and for all \(g\in G\)._

**Definition B.2** (Augmentation collision).: _An augmentation collision occurs if, for two inputs \(x_{a},x_{b}\) and set of transformations \(G\), there exist \(g_{a}^{(1)},\ldots,g_{a}^{(n_{a})},g_{b}^{(1)},\ldots,g_{b}^{(n_{b})}\in G\) for some \(n_{a},n_{b}\in\mathbb{N}\) such that \(g_{a}^{(1)}\circ\ldots\circ g_{a}^{(n_{a})}(x_{a})=g_{b}^{(1)}\circ\ldots\circ g _{a}^{(n_{b})}(x_{b})\)._

**Observation B.3**.: _If there exists an augmentation collision for inputs \(x_{a},x_{b}\) and transformation set \(G\), and \(f\) is invariant to \(G\), then \(f(x_{a})=f(x_{b})\)._

Proof.: By the definition of an augmentation collision, \(g_{a}^{(1)}\circ\ldots\circ g_{a}^{(n_{a})}(x_{a})=g_{b}^{(1)}\circ\ldots \circ g_{a}^{(n_{b})}(x_{b})\). By the definition of a function, we have \(f\circ g_{a}^{(1)}\circ\ldots\circ g_{a}^{(n_{a})}(x_{a})=f\circ g_{b}^{(1)} \circ\ldots\circ g_{a}^{(n_{b})}(x_{b})\). Applying invariance, we obtain \(f(x_{a})=f(x_{b})\). 

Applying this observation, we observe that if the downstream labeling function \(f\) is invariant to a class of augmentations, then there cannot be an augmentation collision for inputs with different labels. However, common augmentations such as brightness shifts can reduce any image to a black or white image, resulting in an augmentation collapse between any two inputs.

## Appendix C Additional feature dropout experiments

### Quantifying the importance of feature dropout

To assess the importance of label-destroying augmentations to the success of the viewmaker, we experiment with a setup where the viewmaker cannot destroy the information in the object class. To do this, we compute a mask around the object and zero out any perturbation from the viewmaker within that mask. We then perform pretraining and transfer as usual.

As we report in Table 3, the accuracy of the CIFAR-10 class label drops precipitously, as expected. At the same time, the accuracy of two of the other objects remains mostly constant (shape and digits), while the accuracy for letters declines modestly (perhaps because the color of the letter is now able to suppress the learning of the letter class.

### Quantifying the degree of feature dropout

We perform an exploratory analysis to testing how well different views drop out the features in an input. We augment a 1,200 examples (CIFAR-10 image plus an overlaid object) using a given augmentation policy (either the expert or viewmaker augmentations). We then encode the model with a classifier trained off of the other augmentation policy (i.e. expert for viewmaker augmentations

\begin{table}
\begin{tabular}{l c c|c c} \hline \hline  & Viewmaker (C-10) & Mask-Viewmaker (C-10) & Viewmaker (Object) & Mask-Viewmaker (Object) \\ \hline C+Shape & 79.8 & 26.0 & 100.0 & 95.8 \\ C+Digit & 69.3 & 50.7 & 94.3 & 95.0 \\ C+Letter & 71.9 & 23.2 & 96.9 & 71.8 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Experiments with a masked viewmaker which is unable to destroy the object class.** Transfer accuracy on CIFAR-10 (C-10) and the object task (Shape, Digit, or Letter). The Mask-Viewmaker has its perturbation masked such that it cannot destroy the label of the object. This results in the features in the object suppressing the CIFAR-10 accuracy, while leaving the object accuracy relatively unscathed.

or the reverse) in order to test how well the augmentations drop out the features. We use a different encoder to see the effects of the augmentations prior to the encoder having a chance to adapt to them.

We observe a bimodal behavior for the viewmaker views, shown in Figure 6, suggesting that the model is adapting to the semantics of the input and has learned to stochastically drop out the simple feature some fraction of the time. By contrast, the expert views display no such structure. Using the corresponding encoder and views leads to models performing uniformly well, as shown in Figure 7.

### Additional visualizations for CelebA

We show feature dropout histograms for CelebA for each of the 40 features in Figure 9. The prevalence of bimodal distributions demonstrates a high degree of feature dropout across attributes. Histograms shown are viewmaker augmentations on an encoder trained with expert views.

Figure 5: **Non-label destroying Viewmaker perturbation examples.**

Figure 6: **Viewmaker augmentations stochastically drop out simple features added to the input.** Probability of the correct answer for different augmentations (Viewmaker or Expert) and different examples from different datasets (Shapes, Letters, Digits). Each histogram shows a single example from each dataset randomly augmented 1200 times, and the corresponding probabilities of the correct answer. The viewmaker augmentations display a bimodal structure, indicating that the simple feature is selectively either destroyed or preserved. The expert augmentations by contrast lack such structure, reflecting their lack of adaptation to the structure of each input.

We also show views and diffs for CelebA in Figure 8. These views show a high degree of sensitivity to the input semantics, and appear to modify characteristics such as the background, hair color, and facial features.

### Importance of Gaussian Blur Transformation

We show experiments for CIFAR-10 (with no overlaid features) on SimCLR (expert views) with and without Gaussian blur augmentations. We find that the inclusion of the Gaussian blur augmentation does not significantly impact CIFAR-10 accuracy.

## Appendix D End-to-end Simulations of Linear Setting

We empirically test the performance of the full trajectory of gradient descent when we add noise to the data. We study a setting with one weak feature with correlations coefficient \(\alpha_{1}\leq 0.5\), and 50 dominant features with \(\alpha_{k}=1\) for \(k=2,\cdots,51\). We compare two approaches run on the same data: in the first approach, we run 150 iterations of GD without adding noise. In the second, we run 50 iterations of GD without noise, and then add noise to the dominant features for the remaining 100 iterations.

In Figure 10(top), we compare the alignment of Feature 1 (the weak feature) and Feature 2 (one of the dominant features) to the ground truth in the two approaches. We observe that adding noise consistently accelerates the learning of the weak feature (blue), with little cost to the dominant features (red). The affect is consistent among many choices for \(\alpha_{1}\), the correlation coefficient of the weak feature. We also plot in Figure 10(bottom) the probability of predicting the correct class (pair)

\begin{table}
\begin{tabular}{l l} \hline \hline  & CIFAR-10 Accuracy \\ \hline Blur & 86.3\% \\ No Blur & 86.1\% \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Experiments with Gaussian blur augmentation.** Transfer accuracy on CIFAR-10 using expert (SimCLR) views with and without Gaussian blur.

Figure 7: **Evaluating views with their respective encoder does not reveal bimodal structure for viewmaker or expert views.** Details are the same as in Figure 6, with the exception that views are evaluated on their corresponding encoder.

Figure 8: **Comparison of viewmaker and expert augmentations on CelebA.** The viewmaker augmentations adapt to the particular semantics of the input data, and make targeted perturbations that alter features such as facial features, hair color, background, and skin tone. _Columns_ (from left): Expert augmentations, viewmaker augmentations, difference between original and viewmaker augmentation, rescaled to [0,1]. Center image in each grid is the original.

of the view under both approaches. We observe that this probability drops sharply when we add noise, which we believe is the mechanism for faster learning with noise.

We remark that we chose to add noise to all the dominant features (instead of a single \(k^{\prime}\) a in our theorem) to accentuate the effect of adding noise. We observed a similar effect, but smaller, when we added noise to fewer features, or when there were fewer than \(50\) dominant features.

## Appendix E Full proofs of propositions and theorems

We begin by stating and proving Lemma E.1 on the downstream classification accuracy.

**Lemma E.1** (Downstream classification accuracy).: _Suppose we draw labeled data points \((u,y)\in\mathbb{R}^{K\times d}\times\{+1,1\}\), where as before, \(u_{k}\sim\mathcal{N}(0,I_{d})\) for \(k\in[K]\), and the label is given by \(\operatorname{sign}(u_{k}^{T}\mu_{k})\). Then the best linear classifier \(\bm{a}\in\mathbb{R}^{K}\) on the representations \(f_{\Theta}(u)\in\mathbb{R}^{K}\) achieves an test error of \(\frac{1}{\pi}\arccos\left(\frac{\left|\mu_{k}^{T}\theta_{\theta_{1}}\right|}{ \left\|\theta_{k}\right\|_{2}}\right)\). That is_

\[\min_{\bm{a}\in\mathbb{R}^{K}}\Pr_{u}[\operatorname{sign}(\bm{a}^{T}f_{\Theta }(u))\neq\operatorname{sign}(\mu_{k}^{T}u_{k})]=\frac{\arccos\left(\frac{ \left|\mu_{k}^{T}\theta_{\theta_{1}}\right|}{\left\|\theta_{k}\right\|_{2}} \right)}{\pi}.\] (1)

Figure 9: **Most Celeb-A features are dropped out by viewmaker. Accuracy of a linear classifier trained on expert images and evaluated on 2000 different augmentations of different images. Over two-thirds of the features exhibit a bimodal structure, indicating feature dropout by the viewmaker augmentations.**

_Thus if \(\theta_{k}\) and \(\mu_{k}\) are orthogonal, then the test error is \(50\%\). If the angle between \(\theta_{k}\) and the \(\pm\mu_{k}\) is zero, then we achieve perfect classification accuracy._

Proof.: It is easy to see that the best linear classifier \(\bm{a}\) will (up to scaling) be equal to the vector \(\operatorname{sign}(\mu_{k}^{T}\theta_{k})e_{k}\). Such a classifier predicts the correct sign whenever \(\operatorname{sign}(\bm{a}^{T}f_{\Theta}(u))=\operatorname{sign}(\mu_{k}^{T} \theta_{k})\operatorname{sign}(\theta_{k}^{T}u_{k})\) equals \(\operatorname{sign}(\mu_{k}^{T}u_{k})\), which occurs exactly a \(1-\frac{\arccos\left(\frac{|\mu_{k}^{T}\theta_{k}|}{|\theta_{k}|\|_{2}}\right)} {\pi}\) fraction of the time. 

In the rest of this section, we prove our main theoretical result, Theorem 5.1, which shows that \(\arccos\left(\frac{|\mu_{k}^{T}\theta_{k}|}{\|\theta_{k}\|_{2}}\right)\) decreases faster in expectation during gradient descent if we add noise to the \(k^{\prime}\) feature.

### Notation.

We let \(\delta_{ij}\) denote the \(\delta\)-function which equals \(1\) if \(i=j\) and \(0\) otherwise. For a parameter \(\Theta=\{\theta_{k}\}_{k\in[K]}\), we let \(\theta_{k}^{\parallel}\coloneqq\mu_{k}\mu_{k}^{T}\theta_{k}\) be the projection of \(\theta_{k}\) in the \(\mu_{k}\) direction. We let \(\theta_{k}^{\perp}=\theta_{k}-\theta_{k}^{\parallel}\) be the projection of \(\theta_{k}\) orthogonal to the feature \(\mu_{k}\).

Throughout this section, we consider the ground truth directions to be fixed, and we fix some initial correlation vector \(\bm{\alpha}\). We let \(\mathbb{P}_{\bm{\alpha}}\) denote the distribution from which the pair \((u,v)\) is drawn from the Gaussian distribution described in Section 5 with correlation coefficients \(\bm{\alpha}\). When unspecified, the variables \(U,V\) are drawn from the distribution \(\mathbb{P}_{\bm{\alpha}}^{m}\). Since we study what happens when we vary \(\alpha_{k^{\prime}}\), for \(x\in[0,1]\), we use the shorthand \(\mathbb{P}_{x}\) to denote the distribution \(\mathbb{P}_{\bm{\alpha}(x)}^{m}\), where \(\alpha(x)_{k^{\prime}}=x\), and \(\alpha(x)_{k}=\alpha_{k}\) for all other \(k\).

We denote \(\mathcal{L}_{i}(\Theta;U,V)=\operatorname{CE}(\{p_{ij}\}_{j\in[m]},e_{i})=- \log(p_{ii})\), which we abbreviate by \(\mathcal{L}_{i}\). When it is clear that we are considering \(\mathcal{L}_{i}\) for some fixed \(i\), we omit the superscripts on the \(i\)th data point or its pair. That is, we denote \(u_{k}\coloneqq u_{k}^{(i)}\) and \(v_{k}\coloneqq v_{k}^{(i)}\).

### Preliminaries

The following facts about of the derivative of the cross entropy loss are easy derived.

**Lemma E.2**.: \[\frac{\partial\mathcal{L}_{i}}{\partial\Theta}=\sum_{j}\left(p_{ij}-\delta_{ ij}\right)\frac{\partial z_{ij}}{\partial\Theta}=\sum_{i}\sum_{j\neq i}p_{ij} \left(\frac{\partial z_{ij}}{\partial\Theta}-\frac{\partial z_{ii}}{\partial \Theta}\right),\] (2)

_where_

\[\frac{\partial z_{ij}}{\partial\theta_{k}}=(u_{k}^{(i)}{v_{k}^{(j)}}^{T}+{v_{k }^{(j)}}{u_{k}^{(i)}}^{T})\theta_{k}.\] (3)

We will also need the following facts on Gaussian random variables. The first, Stein's Lemma, is well known.

Figure 10: Alignment of features with verses without added noise. From left to right: \(\alpha_{1}=0.125,0.25,0.375,0.5\). The top plots show the alignment of Features 1 (weak) and 2 (dominant) to the ground truth; the bottom plots shows the probability of predicting the correct augmentation pair from the batch. Standard deviation bars are shown for the mean alignment over \(200\) runs. We used dimension \(d=5\), and a batch size of \(m=25\).

**Lemma E.3** (Stein's Lemma).: \[\mathbb{E}_{X\sim\mathcal{N}(0,\sigma^{2})}[Xf(X)]=\sigma^{2}\mathbb{E}_{X\sim \mathcal{N}(0,\sigma^{2})}[f^{\prime}(X)].\] (4)

The next two lemmas are proved in Section E.4.

**Lemma E.4**.: _There exists some constant \(C\) such that following holds. If \(\sigma\leq\frac{1}{C}\), and \(0\leq t\leq\frac{1}{\sigma}\), then for any \(c\in\{0,1,2,3\}\), and \(X\sim\mathcal{N}(0,\sigma^{2})\) we have_

\[\mathbb{E}_{X}\left[|X|^{c}\exp(t|X|)\exp(tX^{2})\right]\leq C\sigma^{c}.\] (5)

_If additionally \(d\in\{0,1,2,3\}\), \(\rho\leq\frac{1}{C}\) and \(Y\sim\mathcal{N}(0,\rho^{2})\), then_

\[\mathbb{E}_{X}\left[|X|^{c}|Y|^{d}\exp(t|X|)\exp(|XY|)\right]\leq C\sigma^{c} \rho^{d}.\] (6)

**Lemma E.5**.: _For some universal constant \(C\), for any \(\sigma\in[0,1]\), \(t\geq 0\), \(c\in\{0,1,2,3,4\}\), we have_

\[\mathbb{E}_{X\sim\mathcal{N}(0,\sigma^{2})}\left[\left(\exp(t|X|)-1\right)|X|^ {c}\right]\leq Ct\sigma^{c}.\]

### Approach and Lemmas

Intuition for proof of Theorem 5.1.Our proof involves comparing the gradient of the loss in the \(\theta_{k}\) direction, \(\nabla_{k}:=\frac{\partial}{\partial\theta_{k}}\mathcal{L}\) in the setting with noise to the setting without noise. Loosely, our goal is to show that for any \(k\), the projection of the this gradient onto the ground truth direction, \(\mu_{k}^{T}\nabla_{k}\operatorname{sign}(\mu_{k}^{T}\theta_{k})\), increases when when increase the noise. The main intuition comes from an expansion of this gradient in Lemma E.7, which shows that \(\mathbb{E}\mu_{k}^{T}\nabla_{k}\operatorname{sign}(\mu_{k}^{T}\theta_{k})\) approximately scales with \(\sum_{i}(1-p_{ii})\). Now observe that \(p_{ii}\), the probability of correctly matching the \(i\)th view to its pair, decreases when we add noise to feature \(k^{\prime}\). Thus adding noise will increase \(\mu_{k}^{T}\nabla_{k}\operatorname{sign}(\mu_{k}^{T}\theta_{k})\), thereby improving the alignment.

In the remainder of this section, we outline our proof of Theorem 5.1 in this section. We prove all the lemmas below in Section E.4.

To understand \(\mathbb{E}_{U,V}\left[\arccos\left(\frac{|\mu_{k}^{T}\theta_{k}^{(t+1)}|}{\| \theta_{k}^{c}\|_{2}}\right)\right]\) for a small enough step size, we first claim that it suffices to understand the expected projection of the gradient with respect to \(\theta_{k}\) in the \(\mu_{k}\) direction and in the \(\theta_{k}\) direction. We use the notation \(\nabla_{k}=\frac{\partial\mathcal{L}(\Theta;U,V)}{\partial\theta_{k}}\).

**Lemma E.6**.: _Let \(\theta_{k}^{+}=\theta_{k}-\eta(\nabla_{k}+\lambda\theta_{k})\). Then_

\[\lim_{\eta\to 0}\frac{1}{\eta}\left(\mathbb{E}_{U,V}\left[\arccos\left( \frac{|\mu_{k}^{T}\theta_{k}^{+}|}{\|\theta_{k}^{+}\|_{2}}\right)\right]- \arccos\left(\frac{|\mu_{k}^{T}\theta_{k}|}{\|\theta_{k}\|_{2}}\right) \right)=N\mathbb{E}_{U,V}\left[-(\mu_{k}^{T}\theta_{k})(\mu_{k}^{T}\nabla_{k})+ \frac{\theta_{k}^{T}\nabla_{k}(\mu_{k}^{T}\theta_{k})^{2}}{\|\theta_{k}\|_{2 }^{2}}\right],\] (7)

_where \(N\) is some negative value that depends only on \(\theta_{k}\)._

Now, since we care about the quantity \(\mathbb{E}_{U,V}\left[\arccos\left(\frac{|\mu_{k}^{T}\theta_{k}^{(t+1)}|}{\| \theta_{k}^{(t+1)}\|_{2}}\right)\right]-\mathbb{E}_{U,\bar{V}}\left[\arccos \left(\frac{|\mu_{k}^{T}\bar{\theta}_{k}^{(t+1)}|}{\|\theta_{k}^{(t+1)}\|_{2} }\right)\right]\) being positive, it suffices to show that _derivative_

\[\frac{d}{dx}\mathbb{E}_{U,V\sim\mathbb{P}_{x}}\left[-(\mu_{k}^{T}\theta_{k})( \mu_{k}^{T}\nabla_{k})+\frac{\theta_{k}^{T}\nabla_{k}(\mu_{k}^{T}\theta_{k})^{ 2}}{\|\theta_{k}\|_{2}^{2}}\right],\]

is negative for all \(x\in[\tilde{\alpha}_{k^{\prime}},\alpha_{k^{\prime}}]\). Indeed, from Lemma E.6, we have that

\[\lim_{\eta\to 0}\frac{1}{\eta}\left(\mathbb{E}_{U,V\sim\mathbb{P}_{ \alpha_{k^{\prime}}}}\left[\arccos\left(\frac{|\mu_{k}^{T}\theta_{k}^{+}|}{\| \theta_{k}^{+}\|_{2}}\right)\right]-\mathbb{E}_{U,V\sim\mathbb{P}_{\tilde{ \alpha}_{k^{\prime}}}}\left[\arccos\left(\frac{|\mu_{k}^{T}\theta_{k}|}{\| \theta_{k}\|_{2}}\right)\right]\right)\] (8) \[\qquad\qquad=N\int_{\tilde{\alpha}_{k^{\prime}}}^{\alpha_{k^{ \prime}}}\frac{d}{dx}\mathbb{E}_{U,V\sim\mathbb{P}_{x}}\left[-(\mu_{k}^{T} \theta_{k})(\mu_{k}^{T}\nabla_{k})+\frac{\theta_{k}^{T}\nabla_{k}(\mu_{k}^{T} \theta_{k})^{2}}{\|\theta_{k}\|_{2}^{2}}\right]dx,\] (9)

so if the derivative is negative for the full range, then the difference in arccosines is positive.

In the following lemma we compute the derivative of \(\mathbb{E}[\nabla_{k}]\) with respect to \(x\).

**Lemma E.7**.: \[\frac{d}{dx}\mathbb{E}_{U,V\sim\mathbb{P}_{x}}\left[\nabla_{k}\right] =m\frac{d}{dx}\mathbb{E}_{U,V\sim\mathbb{P}_{x}}\left[\frac{\partial \mathcal{L}_{i}}{\partial\theta_{k}}\right]\] \[=\frac{-m}{1-x^{2}}\theta_{k^{\prime}}^{T}\mu_{k^{\prime}}\]

We will analyze this quantity by explicitly taking the expectation with respect to some set of random variables. Let \(S=\{U_{k},V_{k},U_{k^{\prime}},V_{k^{\prime}}\}\) consist of the random variables \(u_{k^{\prime}}^{(i)}\), \(u_{k}^{(i)}\), and \(v_{k^{\prime}}^{(i)}\), \(v_{k}^{(i)}\) for all \(i\in[m]\). Define \(q_{ij}\) to be the logits when all variables in \(S\) are set to \(0\) (Thus explicitly, \(q_{ij}=\frac{\exp\left(\sum_{k\neq k,k^{\prime}}\theta_{k}^{T}u_{k}^{(i)} \theta_{k}^{T}v_{k}^{(j)}\right)}{\sum_{j^{\prime}}\exp\left(\sum_{k\neq k,k^ {\prime}}\theta_{k}^{T}u_{k}^{(i)}\theta_{k}^{T}v_{k}^{(j^{\prime})}\right)}\)). We will use the notation \(j\sim q\) to denote the distribution on \([m]\) with mass \(q_{ij}\) on \(j\).

Let

\[h(S):=\left(\theta_{k^{\prime}}^{T}u_{k^{\prime}}\right)\left(\mu_{k^{\prime} }^{T}u_{k^{\prime}}^{(i)}-x\mu_{k^{\prime}}^{T}v_{k^{\prime}}^{(i)}\right) \left(\frac{\partial(z_{ij}-z_{ii})}{\partial\theta_{k}}\right),\] (10)

and

\[h_{1}(S)=\left(\theta_{k^{\prime}}^{T}u_{k^{\prime}}\right)\left((1-x^{2})\mu _{k^{\prime}}^{T}u_{k^{\prime}}^{(i)}\right)2\alpha_{k}\left((\mu_{k}^{T}u_{k} )(\theta_{k}^{\parallel}u_{k})\mu_{k}^{T}\right),\] (11)

which are the terms that appear in the right hand side of Lemma E.7 after \(p_{ii}p_{ij}\). Observe that

\[\mathbb{E}_{S}[h(S)-h_{1}(S)]=0.\]

The following four lemmas serve to bound \(\frac{d}{dx}\mathbb{E}_{S}\left[\mu_{k}^{T}\nabla_{k}\right]\) and \(\frac{d}{dx}\mathbb{E}_{S}\left[\theta_{k}^{T}\nabla_{k}\right]\). We call the terms of the form \(\mathbb{E}p_{ii}p_{ij}(h(S)-h_{1}(S))\) "junk" terms, and our goal will be to show that these terms are small. We will control more closely the terms of the form \(\mathbb{E}p_{ii}p_{ij}(h_{1}(S))\).

**Lemma E.8** (Junk Terms for \(\mu_{k}\) term.).: _If \(\|\theta_{k}\|\leq 1\) and \(\|\theta_{k^{\prime}}\|\leq 1\), then for some universal constant \(C\)_

\[\left|\mathbb{E}_{S}\left[p_{ii}p_{ij}\mu_{k}^{T}(h(S)-h_{1}(S))\right]\right| \leq Cq_{ii}q_{ij}\left(\|\theta_{k^{\prime}}\|^{3}\|\theta_{k}\|^{3}+\| \theta_{k^{\prime}}^{\parallel}\|\|\theta_{k}\|^{3}+\alpha_{k}\left(\|\theta_ {k^{\prime}}\|^{3}\|\theta_{k}^{\parallel}\|\right)\right).\]

**Lemma E.9** (Good Term for \(\mu_{k}\) term.).: _If \(\|\theta_{k}\|\leq 1\) and \(\|\theta_{k^{\prime}}\|\leq 1\), then for some universal constant \(C\)_

\[\left|\mathbb{E}_{S}\left[p_{ii}p_{ij}\mu_{k}^{T}h_{1}(S)\right]\right|\geq 2 \alpha_{k}(1-x^{2})q_{ii}q_{ij}\left(\|\theta_{k^{\prime}}^{\parallel}\|\| \theta_{k}^{\parallel}\|\right)\left(1-C(\|\theta_{k^{\prime}}\|^{2}+\|\theta_ {k}\|^{2})\right).\]

Plugging these two lemmas into Lemma E.7 yields the following corollary.

**Corollary E.9.1** (Total \(\mu_{k}\) term.).: _If for a sufficiently large constant \(C\), \(|\theta_{k}^{T}\mu_{k}|\leq\frac{1-\alpha_{k^{\prime}}^{2}}{C}\|\theta_{k}\|\), \(\|\theta_{k^{\prime}}\|^{3}\leq|\theta_{k^{\prime}}^{T}\mu_{k}|\), and \(\|\theta_{k}\|^{2}\leq\frac{\alpha_{k}(1-\alpha_{k^{\prime}}^{2})}{C}\), then_

\[(\mu_{k}^{T}\theta_{k})\frac{d}{dx}\mathbb{E}_{\mathbb{P}_{x}}\left[\mu_{k}^{T} \nabla_{k}\right]\geq\frac{m}{2}\mathbb{E}_{U,V\setminus S}\left[\sum_{i,j}q_ {ii}q_{ij}2\alpha_{k}\|\theta_{k^{\prime}}^{\parallel}\|^{2}\|\theta_{k}^{ \parallel}\|^{2}\right].\]

**Lemma E.10** (Junk Terms for \(\theta_{k}\) term.).: _If \(\|\theta_{k}\|\leq 1\) and \(\|\theta_{k^{\prime}}\|\leq 1\), then for some universal constant \(C\)_

\[\left|\mathbb{E}_{S}\left[p_{ii}p_{ij}\theta_{k}^{T}(h(S)-h_{1}(S))\right] \right|\leq Cq_{ii}q_{ij}\left(\|\theta_{k^{\prime}}\|^{3}\|\theta_{k}\|^{4}+ \|\theta_{k^{\prime}}^{\parallel}\|\|\theta_{k}\|^{4}+\alpha_{k}\left(\|\theta_ {k^{\prime}}\|^{3}\|\theta_{k}\|\|\theta_{k}^{\parallel}\|+\|\theta_{k^{\prime}} ^{\parallel}\|\|\theta_{k}\|^{3}\|\theta_{k}^{\parallel}\|\right)\right).\]

**Lemma E.11** (Good Term for \(\theta_{k}\) term.).: _If \(\|\theta_{k}\|\leq 1\) and \(\|\theta_{k^{\prime}}\|\leq 1\), then for some universal constant \(C\)_

\[\left|\mathbb{E}_{S}\left[p_{ii}p_{ij}\theta_{k}^{T}h_{1}(S)\right]\right|\leq(1 -x^{2})2\alpha_{k}q_{ii}q_{ij}\left(\|\theta_{k^{\prime}}^{\parallel}\|\|\theta_{k} ^{\parallel}\|^{2}\right)\left(1+C(\|\theta_{k^{\prime}}\|^{2}+\|\theta_{k}\|^{2} )\right).\]

Plugging these two lemmas into Lemma E.7 yields the following corollary.

**Corollary E.11.1** (Total \(\theta_{k}\) term.).: _If for a sufficiently large constant \(C\), \(\|\theta_{k}^{\|}\|\leq\frac{1-x^{2}}{C}\|\theta_{k}\|\), \(\|\theta_{k^{\prime}}\|^{3}\leq\|\theta_{k^{\prime}}^{\|}\|\), \(\|\theta_{k}\|^{2}\leq\frac{\alpha_{k}(1-x^{2})}{C}\), then_

\[\frac{(\mu_{k}^{T}\theta_{k})^{2}}{\|\theta_{k}\|^{2}}\left|\frac{d}{dx} \mathbb{E}_{\mathbb{P}_{x}}\left[\theta_{k}^{T}\nabla_{k}\right]\right|\leq \frac{m}{2}\mathbb{E}_{U,V\setminus S}\left[\sum_{i,j}q_{ii}q_{ij}\alpha_{k} \|\theta_{k^{\prime}}^{\|}\|^{2}\|^{2}\right].\]

Combining Corollaries E.9.1 and E.11.1, we obtain the following lemma.

**Lemma E.12**.: _If for a sufficiently large constant \(C\), \(\|\theta_{k}^{\|}\|\leq\frac{1-x^{2}}{C}\|\theta_{k}\|\), \(\|\theta_{k^{\prime}}\|^{3}\leq\|\theta_{k^{\prime}}^{\|}\|\), \(\|\theta_{k}\|^{2}\leq\frac{\alpha_{k}(1-x^{2})}{C}\), then_

\[\mathbb{E}_{U,V\sim\mathbb{P}_{x}}\left[-(\mu_{k}^{T}\theta_{k})(\mu_{k}^{T} \nabla_{k})+\frac{\theta_{k}^{T}\nabla_{k}(\mu_{k}^{T}\theta_{k})^{2}}{\| \theta_{k}\|_{2}^{2}}\right]<0.\] (12)

Theorem 5.1 now follows.

### Proofs of Lemmas

To prove the Lemmas E.4 and E.5, we will use the following well-known formula for the moment generating function (MGF) of the half-normal distribution.

**Lemma E.13** (MGF of half-normal distribution).: _The MGF of the half-normal distribution is_

\[\mathbb{E}_{X\sim\mathcal{N}(0,1)|X>0}[e^{t|X|}]=2e^{t^{2}/2}\Phi(t),\]

_where \(\Phi(t)\) is the cumulative distribution of a normal random variable._

Proof of Lemma E.4.: \[\mathbb{E}_{X}\left[|X|^{c}\exp(t|X|)\exp(tX^{2})\right] =\frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\infty}|x|^{c}\exp(t |x|)\exp(tx^{2})\exp\left(-\frac{x^{2}}{2\sigma^{2}}\right)dx\] \[=\frac{\sqrt{1-2\sigma^{2}t}}{\left(\frac{\sigma}{\sqrt{1-2 \sigma^{2}t}}\right)\sqrt{2\pi}}\int_{-\infty}^{\infty}|x|^{c}\exp(t|x|)\exp \left(-\frac{x^{2}}{2\left(\frac{\sigma}{\sqrt{1-2\sigma^{2}t}}\right)^{2}} \right)dx\] \[=\sqrt{1-2\sigma^{2}t}\mathbb{E}_{Z\sim\mathcal{N}(0,r)|Z\geq 0 }[Z^{c}\exp(tZ)],\]

where \(r=\frac{\sigma}{\sqrt{1-2\sigma^{2}t}}\). To evaluate this, we use the MGF of the half-normal distribution in Lemma E.13. Thus for some constant \(C\), for all \(c\in\{1,2,3,4\}\),

\[\mathbb{E}_{X\sim\mathcal{N}(0,1)|X>0}\left[c|X|^{c}e^{t|X|}\right] \leq\mathbb{E}_{X\sim\mathcal{N}(0,1)|X>0}\left[\frac{d^{c}}{dt^ {c}}e^{t|X|}\right]\] \[\leq C\left(1+t^{c}\right)e^{t^{2}/2}.\]

So for some constant \(C\) (whose value changes throughout this equation), so long as \(\sigma\leq\frac{1}{C}\),

\[\sqrt{1-2\sigma^{2}t}\mathbb{E}_{Z\sim\mathcal{N}(0,r)|Z\geq 0}[Z^{c} \exp(tZ)] =\sqrt{1-2\sigma^{2}t}\mathbb{E}_{X\sim\mathcal{N}(0,1)|Z\geq 0}[r^{c}Z ^{c}\exp(rtZ)]\] \[\leq\sqrt{1-2\sigma^{2}t}Cr^{c}\left(1+(tr)^{c}\right)e^{(tr)^{2} /2}\] \[\leq C\sigma^{c}.\]

This proves the first statement in the lemma. To prove the second, we first take the expectation over \(X\), and using the half-Gaussian MGF as before, we obtain

\[\mathbb{E}_{X}\mathbb{E}_{Y}\left[|X|^{c}|Y|^{d}\exp(t|X|)\exp(|XY|)\right] \leq C\mathbb{E}_{Y}\left[|Y|^{d}\sigma^{c}(1+(t+|Y|)^{c})e^{(t+|Y|)^{2}/2}\right]\]

Now applying the first statement to take the expectation over \(Y\), we obtain

\[\mathbb{E}_{Y}\left[|Y|^{d}(1+(t+|Y|)^{c})e^{(t+|Y|)^{2}/2}\right]\leq C \sigma^{c}\rho^{d}.\]Proof of Lemma e.5.: We prove the lemma by induction on \(c\). Suppose \(c=0\). Then by plugging in the MGF for the half-normal distribution from Lemma E.13, for some constant \(C\), we have

\[\mathbb{E}_{X\sim\mathcal{N}(0,1)|X>0}[(e^{t|X|}-1)] =2e^{t^{2}/2}\Phi(t)-1\] (13) \[\leq 2e^{t^{2}/2}\left(\frac{1+t}{2}\right)-1\] (14) \[\leq\left(e^{t^{2}/2}-1\right)+te^{t^{2}/2}\] (15) \[\leq Ct,\] (16)

thus

\[\mathbb{E}_{X\sim\mathcal{N}(0,\sigma^{2})}[(e^{t|X|}-1)]=\mathbb{E}_{X\sim \mathcal{N}(0,\sigma^{2})|X>0}[(e^{\sigma t|X|}-1)]\leq Ct\sigma.\]

Now for \(c\geq 1\), by Stein's Lemma, we have (for a new constant \(C\)),

\[\mathbb{E}_{X\sim\mathcal{N}(0,\sigma^{2})}[|X|^{c}(e^{t|X|}-1)] =\mathbb{E}_{X\sim\mathcal{N}(0,\sigma^{2})}[X|X|^{c-1}\operatorname {sign}(X)(e^{t|X|}-1)]\] (17) \[=\sigma^{2}\mathbb{E}_{X\sim\mathcal{N}(0,\sigma^{2})}\left[ \frac{d}{dX}\left(|X|^{c-1}\operatorname{sign}(X)(e^{t|X|}-1)\right)\right]\] (18) \[=\sigma^{2}\mathbb{E}_{X\sim\mathcal{N}(0,\sigma^{2})}\left[(c-2) \left(|X|^{c-2}(e^{t|X|}-1)\right)+\left(|X|^{c-1}(te^{t|X|})\right)\right]\] (19) \[\leq Ct\sigma^{c+1}.\] (20)

where in the last step we used the inductive hypothesis and Lemma E.4. 

Proof of Lemma e.6.: First observe that

\[\lim_{\eta\to 0}\frac{1}{\eta}\left(\mathbb{E}_{U,V}\left[ \arccos\left(\frac{|\mu_{k}^{T}\theta_{k}^{+}|}{\|\theta_{k}^{+}\|_{2}}\right) \right]-\arccos\left(\frac{|\mu_{k}^{T}\theta_{k}|}{\|\theta_{k}\|_{2}}\right)\right)\] \[=\lim_{\eta\to 0}\frac{1}{\eta}\left(\mathbb{E}_{U,V}\left[ \arccos\left(\frac{|\mu_{k}^{T}(\theta_{k}(1-\eta\lambda)-\eta\nabla_{k})|}{ \|\theta_{k}(1-\eta\lambda)-\eta\nabla_{k}\|_{2}}\right)\right]-\arccos\left( \frac{|\mu_{k}^{T}\theta_{k}|}{\|\theta_{k}\|_{2}}\right)\right)\] \[=\lim_{\eta\to 0}\frac{1}{\eta}\left(\mathbb{E}_{U,V}\left[ \arccos\left(\frac{|\mu_{k}^{T}(\theta_{k}-\frac{\eta}{1-\eta\lambda}\nabla_{ k})|}{\|\theta_{k}-\frac{\eta}{1-\eta\lambda}\nabla_{k}\|_{2}}\right)\right]- \arccos\left(\frac{|\mu_{k}^{T}\theta_{k}|}{\|\theta_{k}\|_{2}}\right)\right)\] \[=\mathbb{E}_{U,V}\left[\frac{d}{d\eta}\arccos\left(\frac{|\mu_{k} ^{T}(\theta_{k}-\eta\nabla_{k})|}{\|\theta_{k}-\eta\nabla_{k}\|_{2}}\right)(0 )\right],\]

since \(\lim_{\eta\to 0}\frac{\eta}{1-\eta\lambda}=0\). Now

\[\frac{d}{d\eta}\arccos\left(\frac{|\mu_{k}^{T}(\theta_{k}-\eta \nabla_{k})|}{\|\theta_{k}-\eta\nabla_{k}\|_{2}}\right)(0) =\arccos^{\prime}\left(\frac{|\mu_{k}^{T}\theta_{k}|}{\|\theta_{k }\|_{2}}\right)\frac{d}{d\eta}\left(\frac{|\mu_{k}^{T}(\theta_{k}-\eta\nabla _{k})|}{\|\theta_{k}-\eta\nabla_{k}\|_{2}}\right)(0)\] \[=\arccos^{\prime}\left(\frac{|\mu_{k}^{T}\theta_{k}|}{\|\theta_{ k}\|_{2}}\right)\left(\frac{-\operatorname{sign}(\mu_{k}^{T}\theta_{k})\mu_{k}^{T} \nabla_{k}\|\theta_{k}\|+|\mu_{k}^{T}\theta_{k}|\frac{\theta_{k}^{T}\nabla_{k} }{\|\theta_{k}\|}}{\|\theta_{k}\|_{2}^{2}}\right)\] \[=N\left(-\mu_{k}^{T}\theta_{k}\mu_{k}^{T}\nabla_{k}+(\mu_{k}^{T} \theta_{k})^{2}\frac{\theta_{k}^{T}\nabla_{k}}{\|\theta_{k}\|^{2}}\right),\]

where \(N=\arccos^{\prime}\left(\frac{|\mu_{k}^{T}\theta_{k}|}{\|\theta_{k}\|_{2}} \right)\frac{1}{\|\theta_{k}\|_{2}^{2}\theta_{k}\|}\). The lemma follows by taking the expectation over \(U,V\), and observing derivative of \(\arccos(x)\) is negative whenever \(x\) is positive. 

Proof of Lemma e.7.: First observe that by symmetry, we have

\[\frac{d}{dx}\mathbb{E}_{U,V\sim\mathbb{P}_{x}}\left[\nabla_{k}\right]=m\frac{d }{dx}\mathbb{E}_{U,V\sim\mathbb{P}_{x}}\left[\frac{\partial\mathcal{L}_{i}}{ \partial\theta_{k}}\right].\]To make this expectation easier to analyze, we express the random variable \((U(x),V(x))\sim\mathbb{P}_{x}\) as an interpolation of Gaussians in the coordinate \(\mu_{k^{\prime}}^{T}v_{k^{\prime}}^{(i)}\). Let \(\xi\sim\mathcal{N}(0,1)\), and define \((U,V)\sim\mathbb{P}_{1}\), such that \(\mu_{k^{\prime}}^{T}v_{k^{\prime}}^{(i)}=\mu_{k^{\prime}}^{T}u_{k^{\prime}}^{(i)}\). For \(x\in[0,1)\), define \((U(x),V(x))\) to have

\[\mu_{k^{\prime}}^{T}v_{k^{\prime}}^{(i)}(x)=x\mu_{k^{\prime}}^{T}u_{k^{\prime}} ^{(i)}+\sqrt{1-x^{2}}\xi,\] (21)

and otherwise be the same as \((U,V)\). It is easy to check that \((U(x),V(x))\sim\mathbb{P}_{x}\).

Now

\[\frac{d}{dx}\mathbb{E}_{U,V\sim\mathbb{P}_{x}}\left[\frac{\partial\mathcal{L}_ {i}(\Theta;U,V)}{\partial\theta_{k}}\right]=\mathbb{E}_{U,V\sim\mathbb{P}_{1},\xi}\left[\frac{d}{dx}\frac{\partial\mathcal{L}_{i}(\Theta;U(x),V(x))}{ \partial\theta_{k}}\right].\]

Taking the derivative of the cross-entropy loss, we have

\[\frac{d}{dx}\frac{\partial\mathcal{L}_{i}(\Theta;U(x),V(x))}{ \partial\theta_{k}} =\frac{d}{dx}\left(\sum_{j\neq i}p_{ij}\left(\frac{\partial(z_{ij} -z_{ii})}{\partial\theta_{k}}\right)\right)\] \[=\sum_{j\neq i}\frac{dp_{ij}}{d\mu_{k^{\prime}}^{T}v_{k^{\prime}}^ {(i)}(x)}\frac{d\mu_{k^{\prime}}^{T}v_{k^{\prime}}^{(i)}(x)}{dx}\frac{\partial (z_{ij}-z_{ii})}{\partial\theta_{k}}\] \[=\sum_{j\neq i}-p_{ij}p_{ij}\frac{dz_{ii}}{d\mu_{k^{\prime}}^{T}v _{k^{\prime}}^{(i)}(x)}\left(\mu_{k^{\prime}}^{T}u_{k^{\prime}}^{(i)}-\frac{x }{\sqrt{1-x^{2}}}\xi\right)\left(\frac{\partial(z_{ij}-z_{ii})}{\partial\theta _{k}}\right)\]

where the variables \(z_{ij}\) and \(p_{ij}\) are the similarity scores and the softmaxes from the data \((U(x),V(x))\). Here the first line is by Lemma E.2, and the second line holds by chain rule since \(\frac{\partial z_{ij}}{\partial\theta_{k}}-\frac{\partial z_{ii}}{\partial \theta_{k}}\) does not depend on \(v_{k^{\prime}}^{(i)}\). The third line uses the proof of Claim E.14 to take the derivative of \(p_{ij}\), and Equation 21 to take the derivative of \(\mu_{k^{\prime}}^{T}v_{k^{\prime}}^{(i)}(x)\).

Now we reparameterize \(\mu_{k^{\prime}}^{T}u_{k^{\prime}}^{(i)}-\frac{x}{\sqrt{1-x^{2}}}\xi\) as follows:

\[\mu_{k^{\prime}}^{T}u_{k^{\prime}}^{(i)}-\frac{x}{\sqrt{1-x^{2}}}\xi=\left( \frac{1}{1-x^{2}}\right)\mu_{k^{\prime}}^{T}u_{k^{\prime}}^{(i)}-\frac{x}{1-x ^{2}}\mu_{k^{\prime}}^{T}v_{k^{\prime}}^{(i)}(x).\]

Plugging in this reparameterization and \(\frac{dz_{ii}}{d\mu_{k^{\prime}}^{T}v_{k^{\prime}}^{(i)}(x)}=\theta_{k^{\prime }}^{T}\mu_{k^{\prime}}\theta_{k^{\prime}}^{T}u_{k^{\prime}}\), we obtain

\[\frac{d}{dx}\mathbb{E}_{U,V\sim\mathbb{P}_{x}}\left[\frac{\partial\mathcal{L}_ {i}(\Theta;U,V)}{\partial\theta_{k}}\right]=\frac{-1}{1-x^{2}}\sum_{j\neq i} \mathbb{E}_{U,V\sim\mathbb{P}_{x}}\left[p_{ij}p_{ii}\left(\theta_{k^{\prime}}^ {T}\mu_{k^{\prime}}\theta_{k^{\prime}}^{T}u_{k^{\prime}}\right)\left(\mu_{k^{ \prime}}^{T}u_{k^{\prime}}^{(i)}-x\mu_{k^{\prime}}^{T}v_{k^{\prime}}^{(i)} \right)\left(\frac{\partial(z_{ij}-z_{ii})}{\partial\theta_{k}}\right)\right].\]

We now prove Lemmas E.8, E.9, E.10, and E.11.

Notation.Since \(i\) is fixed throughout, we drop the \((i)\) superscripts and let \(u_{k}=u_{k}^{(i)}\) and \(v_{k}=v_{k}^{(i)}\). We will introduce the following random variables, which are all independent, to simplify the exposition:

* \(\xi_{j}:=\theta_{k}^{T}v_{k}^{(j)}\) for \(j\neq i\). Thus \(\xi_{j}\sim\mathcal{N}(0,\|\theta_{k}\|^{2})\).
* \(\xi_{j}^{\prime}:=\theta_{k^{\prime}}^{T}v_{k^{\prime}}^{(j)}\) for \(j\neq i\). Thus \(\xi_{j}^{\prime}\sim\mathcal{N}(0,\|\theta_{k^{\prime}}\|^{2})\).
* \(\xi_{i}:=(\theta_{k}^{\perp})^{T}v_{k}+(\theta_{k}^{\parallel})^{T}(v_{k}-\alpha _{k}u_{k})\). Thus \(\xi_{i}\sim\mathcal{N}(0,\|\theta_{k}^{\perp}\|^{2}+(1-\alpha_{k}^{2})\|\theta _{k}^{\parallel}\|^{2})\).
* \(\xi_{i}^{\prime}:=(\theta_{k^{\prime}}^{\perp})^{T}v_{k^{\prime}}\). Thus \(\xi_{i}^{\prime}\sim\mathcal{N}(0,\|\theta_{k^{\prime}}^{\perp}\|^{2}\|\theta _{k^{\prime}}^{\parallel}\|^{2})\).
* \(\zeta_{i}^{\prime}:=(\theta_{k^{\prime}}^{\parallel})^{T}(v_{k^{\prime}}-\alpha _{k^{\prime}}u_{k^{\prime}})\). Thus \(\zeta_{i}^{\prime}\sim\mathcal{N}(0,(1-\alpha_{k^{\prime}}^{2})\|\theta_{k^{ \prime}}^{\parallel}\|^{2})\).
* \(y=(\theta_{k}^{\parallel})^{T}u_{k}\). Thus \(y\sim\mathcal{N}(0,\|\theta_{k}^{\parallel}\|^{2})\).

* \(y^{\prime}=(\theta^{1}_{k^{\prime}})^{T}u_{k^{\prime}}\). Thus \(y^{\prime}\sim\mathcal{N}(0,\|\theta^{1}_{k^{\prime}}\|^{2})\).
* \(\eta_{i}:=(\theta^{\perp}_{k})^{T}u_{k}\). Thus \(\eta_{i}\sim\mathcal{N}(0,\|\theta^{1}_{k^{\prime}}\|^{2})\).
* \(\eta^{\prime}_{i}:=(\theta^{1}_{k^{\prime}})^{T}u_{k^{\prime}}\). Thus \(\eta^{\prime}_{i}\sim\mathcal{N}(0,\|\theta^{1}_{k^{\prime}}\|^{2})\).

For any such random variable \(X\), we use \(\sigma^{2}_{X}\) to denote its variance. Observe that

\[\frac{p_{ii}p_{ij}}{q_{ii}q_{ij}}=\frac{\exp\left(\theta^{T}_{k}u_{k}\theta^{T }_{k^{\prime}}v_{k}\right)\exp\left(\theta^{T}_{k^{\prime}}u_{k^{\prime}} \theta^{T}_{k^{\prime}}v_{k^{\prime}}\right)}{\mathbb{E}_{j^{\prime}\sim q} \exp\left(\theta^{T}_{k}u_{k}\theta^{T}_{k^{\prime}}v^{(j^{\prime})}_{k^{ \prime}}\right)}\frac{\exp\left(\theta^{T}_{k}u_{k}\theta^{T}_{k}v^{(j^{ \prime})}_{k}\right)\exp\left(\theta^{T}_{k^{\prime}}u_{k^{\prime}}\theta^{T}_ {k^{\prime}}v^{(j^{\prime})}_{k^{\prime}}\right)}{\mathbb{E}_{j^{\prime}\sim q }\exp\left(\theta^{T}_{k}u_{k}\theta^{T}_{k}v^{(j^{\prime})}_{k}\right)\exp \left(\theta^{T}_{k^{\prime}}u_{k^{\prime}}\theta^{T}_{k^{\prime}}v^{(j^{ \prime})}_{k^{\prime}}\right)}.\]

We will use the following two claims in the proofs of all four lemmas.

**Claim E.14**.: _For \(\beta\in\{\xi_{j},\xi^{\prime}_{j},\xi_{i},\xi^{\prime}_{i},\zeta^{\prime}_{i},\eta_{i},\eta^{\prime}_{i},x,\tau^{\prime}\}\), let \(\bar{\beta}_{j^{\prime}}:=\frac{\partial}{\partial\beta}\left(\theta^{T}_{k}u _{k}\theta^{T}_{k}v^{(j^{\prime})}_{k}+\theta^{T}_{k^{\prime}}u_{k^{\prime}} \theta^{T}_{k^{\prime}}v^{(j^{\prime})}_{k^{\prime}}\right)\). Then_

\[\left|\frac{\partial p_{ii}p_{ij}}{\partial\beta}\right|\leq p_{ii}p_{ij} \left(|\bar{\beta}_{j}|+|\bar{\beta}_{i}|+2\mathbb{E}_{j^{\prime}\sim q}|\bar {\beta}_{j^{\prime}}|\right).\]

_If additionally \(\gamma\in\{\xi_{j},\xi^{\prime}_{j},\xi_{i},\xi^{\prime}_{i},\zeta^{\prime}_{i },\eta_{i},\eta^{\prime}_{i}\}\) and \(\gamma\perp\{\bar{\beta}_{j^{\prime}}\}_{j^{\prime}\in[m]}\), then_

\[\left|\frac{\partial}{\partial\gamma}\frac{\partial p_{ii}p_{ij} }{\partial\beta}\right| \leq p_{ii}p_{ij}\left((|\beta_{j}|+|\beta_{i}|+2\mathbb{E}_{j^{ \prime}\sim q}|\bar{\beta}_{j^{\prime}}|\right)(|\bar{\gamma}_{j}|+|\bar{ \gamma}_{i}|+2\mathbb{E}_{j^{\prime}\sim q}|\bar{\gamma}_{j^{\prime}}|)\right)\] \[\quad+p_{ii}p_{ij}\left(2\mathbb{E}_{j^{\prime}\sim q}|\bar{ \beta}_{j^{\prime}}\bar{\gamma}_{j^{\prime}}|+2(\mathbb{E}_{j^{\prime}\sim q }|\bar{\beta}_{j^{\prime}}|)(\mathbb{E}_{j^{\prime}\sim q}|\bar{\gamma}_{j^{ \prime}}|)\right).\]

Proof.: By a straightforward quotient-rule computation of the derivative of \(\frac{p_{ii}}{q_{ij}}\), recalling that \(q_{ij}\) is independent of \(S\), we obtain

\[\frac{\partial p_{ij}}{\partial\beta}=p_{ij}\left(\bar{\beta}_{j}-\mathbb{E}_ {j^{\prime}\sim q}\bar{\beta}_{j^{\prime}}p_{ij^{\prime}}\right).\]

By applying product to the expression above, we obtain

\[\frac{\partial p_{ii}p_{ij}}{\partial\beta}=p_{ii}p_{ij}\left(\bar{\beta}_{j}+ \bar{\beta}_{i}-2\mathbb{E}_{j^{\prime}\sim q}\bar{\beta}_{j^{\prime}}p_{ij^{ \prime}}\right).\]

Taking absolute values and using the fact that \(p_{ij^{\prime}}\leq 1\), we obtain the first result.

Next we take the derivative of \(p_{ij}\) with respect to both \(\beta\) and \(\gamma\). Using the expression above for \(\frac{\partial p_{ij}}{\partial\beta}\), we obtain

\[\frac{\partial}{\partial\gamma}\frac{\partial p_{ij}}{\partial\beta}=p_{ij} \left(\left(\bar{\beta}_{j}-\mathbb{E}_{j^{\prime}\sim q}\bar{\beta}_{j^{\prime} }p_{ij^{\prime}}\right)(\bar{\gamma}_{j}-\mathbb{E}_{j^{\prime}\sim q}\bar{ \gamma}_{j^{\prime}}p_{ij^{\prime}})-\mathbb{E}_{j^{\prime}\sim q}\bar{\beta}_{j^ {\prime}}\bar{\gamma}_{j^{\prime}}p_{ij^{\prime}}+(\mathbb{E}_{j^{\prime}\sim q }\bar{\beta}_{j^{\prime}}p_{ij^{\prime}})(\mathbb{E}_{j^{\prime}\sim q}\bar{ \gamma}_{j^{\prime}}p_{ij^{\prime}})\right),\]

and

\[\frac{\partial}{\partial\gamma}\frac{\partial p_{ii}p_{ij}}{ \partial\beta}=p_{ii}p_{ij}\left(\left(\bar{\beta}_{j}+\bar{\beta}_{i}-2 \mathbb{E}_{j^{\prime}\sim q}\bar{\beta}_{j^{\prime}}p_{ij^{\prime}}\right)( \bar{\gamma}_{j}+\bar{\gamma}_{i}-2\mathbb{E}_{j^{\prime}\sim q}\bar{\gamma}_{j ^{\prime}}p_{ij^{\prime}})\right)\] \[\quad\quad\quad+p_{ii}p_{ij}\left(-2\mathbb{E}_{j^{\prime}\sim q} \bar{\beta}_{j^{\prime}}\bar{\gamma}_{j^{\prime}}p_{ij^{\prime}}+2(\mathbb{E}_{j^{ \prime}\sim q}\bar{\beta}_{j^{\prime}}p_{ij^{\prime}})(\mathbb{E}_{j^{\prime}\sim q }\bar{\gamma}_{j^{\prime}}p_{ij^{\prime}})\right).\]

The second result follows by taking absolute values and the fact that \(p_{ij^{\prime}}\leq 1\). 

**Claim E.15**.: \(\frac{p_{ij}}{q_{ij}}\leq\exp\left(|\theta^{T}_{k}u_{k}\theta^{T}_{k}v^{(j)}_{k} |\right)\exp\left(|\theta^{T}_{k^{\prime}}u_{k^{\prime}}\theta^{T}_{k^{\prime}}v^{(j )}_{k^{\prime}}|\right)\mathbb{E}_{j^{\prime}\sim q}\left[\exp\left(|\theta^{T}_{k }u_{k}\theta^{T}_{k}v^{(j^{\prime})}_{k}|\right)\exp\left(|\theta^{T}_{k^{ \prime}}u_{k^{\prime}}\theta^{T}_{k^{\prime}}v^{(j^{\prime})}_{k^{\prime}}| \right)\right].\)__

Proof.: This follows directly from using Jenson's inequality on the distribution \(j^{\prime}\sim q\) to show that

\[\leq\mathbb{E}_{j^{\prime}\sim q}\left[\exp\left(-\theta^{T}_{k}u_{k} \theta^{T}_{k}v^{(j^{\prime})}_{k}\right)\exp\left(-\theta^{T}_{k^{\prime}}u_{k} \theta^{T}_{k^{\prime}}v^{(j^{\prime})}_{k}\right)\right]\] \[\leq\mathbb{E}_{j^{\prime}\sim q}\left[\exp\left(|\theta^{T}_{k}u_{k} \theta^{T}_{k}v^{(j^{\prime})}_{k}|\right)\exp\left(|\theta^{T}_{k^{\prime}}u_{k^{ \prime}}\theta^{T}_{k^{\prime}}v^{(j^{\prime})}_{k^{\prime}}|\right)\right].\]

[MISSING_PAGE_FAIL:27]

[MISSING_PAGE_FAIL:28]

We use the same approach as before. For the terms (1a) and (2a) we apply Stein's Lemma to \((\eta_{i}^{\prime},\xi_{j})\) and \((\eta_{i}^{\prime},\xi_{i})\) respectively. For (1b), (2b), (3a) and (3b) and (4), we apply Stein's Lemma to \(\xi_{j}\), \(\xi_{i}\), \(\eta_{i}^{\prime}\), \(\eta_{i}\), and \(\xi_{i}^{\prime}\) respectively. Using Claim E.15 and then Lemma E.4 as before, we obtain the following result:

1. \(\left\|\mathbb{E}_{S}\left[p_{ii}p_{ij}\left(\eta_{i}^{\prime}\left(\mu_{k^{ \prime}}^{T}u_{k^{\prime}}-x\mu_{k^{\prime}}^{T}v_{k^{\prime}}\right)\right) \left(\theta_{k}^{T}u_{k}\xi_{j}\right)\right\|\right.\leq\)\(Cq_{ii}q_{ij}\sigma_{\eta_{i}^{\prime}}^{2}\sigma_{\xi_{j}}^{2}\|\theta_{k^{ \prime}}\|\|\theta_{k}\|\|\theta_{k}\|=Cq_{ii}q_{ij}\|\theta_{k^{\prime}}\|^{2} \|\theta_{k^{\prime}}\|\|\theta_{k}\|^{4}\leq Cq_{ii}q_{ij}\|\theta_{k^{ \prime}}\|^{3}\|\theta_{k}\|^{4}\).
2. \(\left\|\mathbb{E}_{S}\left[p_{ii}p_{ij}\left(\eta_{i}^{\prime}\left(\mu_{k^{ \prime}}^{T}u_{k^{\prime}}-x\mu_{k^{\prime}}^{T}v_{k^{\prime}}\right)\right) \left(\theta_{k}^{T}u_{k}\xi_{i}\right)\right]\right\|\leq Cq_{ii}q_{ij}\sigma_{ \eta_{i}^{\prime}}^{2}\sigma_{\xi_{i}}^{2}\|\theta_{k^{\prime}}\|\|\theta_{k} \|\|\theta_{k}\|\leq Cq_{ii}q_{ij}\|\theta_{k^{\prime}}\|^{3}\|\theta_{k}\|^{4}\).
3. \(\left\|\mathbb{E}_{S}\left[p_{ii}p_{ij}\left(y^{\prime}\left(\mu_{k^{\prime}}^{T }u_{k^{\prime}}-x\mu_{k^{\prime}}^{T}v_{k^{\prime}}\right)\right)\left(\theta _{k}^{T}u_{k}\xi_{j}\right)\right]\right\|\leq Cq_{ii}q_{ij}\sigma_{\xi_{j}}^{2}\| \theta_{k}\|\|\|\theta_{k}\|\|\|\theta_{k^{\prime}}\|=Cq_{ii}q_{ij}\|\theta_{k }\|^{4}\|\theta_{k^{\prime}}\|\|\]
4. \(\left\|\mathbb{E}_{S}\left[p_{ii}p_{ij}\left(y^{\prime}\left(\mu_{k^{\prime}}^{ T}u_{k^{\prime}}-x\mu_{k^{\prime}}^{T}v_{k^{\prime}}\right)\right)\left(\theta _{k}^{T}u_{k}\xi_{i}\right)\right]\right\|\leq Cq_{ii}q_{ij}\sigma_{\xi_{i}}^{2}\| \theta_{k}\|\|\theta_{k}\|\|\|\theta_{k^{\prime}}\|\leq Cq_{ii}q_{ij}\|\theta_{k }\|^{4}\|\theta_{k^{\prime}}\|\|\]
5. \(\left|\alpha_{k}\mathbb{E}_{S}\left[p_{ii}p_{ij}\left(\eta_{i}^{\prime}\left( \mu_{k^{\prime}}^{T}u_{k^{\prime}}-x\mu_{k^{\prime}}^{T}v_{k^{\prime}}\right) \right)\left(\theta_{k}^{T}u_{k}y\right)\right]\right|\leq C\alpha_{k}q_{ii}q_{ij} \sigma_{\eta_{i}^{\prime}}^{2}\|\theta_{k^{\prime}}\|\|\theta_{k}\|\|\|\theta_ {k}\|=C\alpha_{k}q_{ii}q_{ij}\|\theta_{k^{\prime}}\|^{2}\|\|\theta_{k^{\prime}}\| \|\theta_{k}\|\|\theta_{k}\|\|\theta_{k}\|=\)
6. \(\left|\alpha_{k}\mathbb{E}_{S}\left[p_{ii}p_{ij}\left(y^{\prime}\left(\mu_{k^{ \prime}}^{T}u_{k^{\prime}}-x\mu_{k^{\prime}}^{T}v_{k^{\prime}}\right)\right) \left(n_{i}y\right)\right]\right|\leq C\alpha_{k}q_{ii}q_{ij}\sigma_{\eta_{i}}^{2}\| \theta_{k}\|\|\theta_{k^{\prime}}\|\|\theta_{k}\|\|\theta_{k}\|=C\alpha_{k}q_{ii }q_{ij}\|\theta_{k}^{\perp}\|^{2}\|\theta_{k}\|\|\theta_{k}\|\|\theta_{k}^{\|}\|\).
7. \(\left|\alpha_{k}\mathbb{E}_{S}\left[p_{ii}p_{ij}\left(y^{\prime}\left(-x\zeta_{ i}^{\prime}\right)\left(\theta_{k}^{T}u_{k}y\right)\right)\right]\right|\leq C\alpha_{k}q_{ii}q_{ij}\sigma_{\zeta_{i}}^{2}\| \theta_{k^{\prime}}\|\|\theta_{k}\|\|\theta_{k}\|\|\leq C\alpha_{k}q_{ii}q_{ij} \|\theta_{k}\|\|\|\theta_{k}\|\|\theta_{k}^{\|}\|\).

Combining the bounds on these 7 terms, proves the lemma:

\[\left|\mathbb{E}_{S}\left[p_{ii}p_{ij}\theta_{k}^{T}(h(S)-h_{1}(S))\right] \right|\leq Cq_{ii}q_{ij}\left(\|\theta_{k^{\prime}}\|^{3}\|\theta_{k}\|^{4}+ \|\theta_{k^{\prime}}^{\|}\|\|\theta_{k}\|^{4}+\alpha_{k}\left(\|\theta_{k^{ \prime}}\|^{3}\|\theta_{k}\|\|\theta_{k}^{\|}+\|\theta_{k^{\prime}}^{\|}\|\| \theta_{k}\|^{3}\|\theta_{k}^{\|}\|\right)\right).\]

\(\Box\)

We now prove the lemmas on the non-junk terms.

_Proof of Lemma E.9._

Now by Claim E.16, we have \(\left|\frac{p_{ii}p_{ij}}{q_{ii}q_{ij}}-1\right|\leq Z_{i}Z_{j}-1\) (where the variable's \(Z_{i},Z_{j}\) are defined in the Claim E.16) so

\[\left|\mathbb{E}_{S}\left[\left(\frac{p_{ii}p_{ij}}{q_{ii}q_{ij}}-1 \right)\left((\theta_{k^{\prime}}^{\|})^{T}u_{k^{\prime}}u_{k^{\prime}}^{T}\mu_{ k^{\prime}}\right)\left(\mu_{k}^{T}u_{k}(\theta_{k}^{\|})^{T}u_{k}\right)\right]\right| \leq\mathbb{E}_{S}\left[\left(Z_{i}Z_{j}-1\right)\left|(\theta_{k ^{\prime}}^{\|})^{T}u_{k^{\prime}}u_{k^{\prime}}^{T}\mu_{k^{\prime}}\right| \left|\mu_{k}^{T}u_{k}(\theta_{k}^{\|})^{T}u_{k}\right|\right]\] \[\leq C\left(\|\theta_{k}\|^{2}+\|\theta_{k^{\prime}}\|^{2}\right)\| \theta_{k^{\prime}}^{\|}\|\|\theta_{k}\|^{6}\|.\]

Here the second inequality follows from applying Lemma E.5 first, and then Lemma E.4 repeatedly for the remainder of the variables in \(S\). This proves the lemma. Note that we need to apply Lemma E.5 several times to a single variable \(X\in S\). Indeed we can write

\[\left(Z_{i}Z_{j}-1\right)\left|(\theta_{k^{\prime}}^{\|})^{T}u_{k^{\prime}}u_{ k^{\prime}}^{T}\mu_{k^{\prime}}\right|\left|\mu_{k}^{T}u_{k}(\theta_{k}^{\|})^{T}u_{k}\right| \right| =\left(\mathbb{E}_{\ell}\exp(|t_{\ell}X|)S_{\ell}-1\right)B|X|^{c}\] \[=\left(\mathbb{E}_{\ell}S_{\ell}(\exp(|t_{\ell}X|)-1)\right)B|X|^{ c}+\left(\mathbb{E}_{\ell}S_{\ell}-1\right)\right)B|X|^{c}\]for some distribution on \(\ell\), and for some terms \(S_{\ell},t_{\ell}\), and \(B\) that are independent of \(X\), and \(c\in\{0,1,2\}\). Then to take the expectation of this term over \(X\), we first apply Lemma E.5 to on \(X\) to the first term, and iteratively apply Lemma E.5 to the random variables appearing in the next terms. 

Proof of Lemma e.11.: \[\frac{1}{1-x^{2}}\mathbb{E}_{S}\left[p_{ii}p_{ij}\theta_{k}^{T}h_ {1}(S)\right] =\mathbb{E}_{S}\left[p_{ii}p_{ij}\left((\theta_{k^{\prime}}^{ \parallel})^{T}u_{k^{\prime}}u_{k^{\prime}}^{T}\mu_{k^{\prime}}\right)\left(2 \theta_{k}^{\parallel})^{T}u_{k}\alpha_{k}(\theta_{k}^{\parallel})^{T}u_{k} \right)\right]\] \[=\mathbb{E}_{S}\left[q_{ii}q_{ij}\left((\theta_{k^{\prime}}^{ \parallel})^{T}u_{k^{\prime}}u_{k^{\prime}}^{T}\mu_{k^{\prime}}\right)\left(2 \alpha_{k}((\theta_{k}^{\parallel})^{T}u_{k})^{2}\right)\right]\] \[\qquad+\mathbb{E}_{S}\left[(p_{ii}p_{ij}-q_{ii}q_{ij})\left(( \theta_{k^{\prime}}^{\parallel})^{T}u_{k^{\prime}}u_{k^{\prime}}^{T}\mu_{k^{ \prime}}\right)\left(2\alpha_{k}((\theta_{k}^{\parallel})^{T}u_{k})^{2}\right)\right]\] \[=2\alpha_{k}q_{ii}q_{ij}\theta_{k^{\prime}}^{T}\mu_{k^{\prime}} \|\theta_{k}^{\parallel}|^{2}+2\alpha_{k}q_{ii}q_{ij}\mathbb{E}_{S}\left[\left( \frac{p_{ii}p_{ij}}{q_{ii}q_{ij}}-1\right)\left((\theta_{k^{\prime}}^{ \parallel})^{T}u_{k^{\prime}}u_{k^{\prime}}^{T}\mu_{k^{\prime}}\right)\left(( \theta_{k}^{\parallel})^{T}u_{k}\right)^{2}\right].\]

Now by Claim E.16, we have \(\left|\frac{p_{ii}p_{ij}}{q_{ii}q_{ij}}-1\right|\leq Z_{i}Z_{j}-1\), so

\[\left|\mathbb{E}_{S}\left[\left(\frac{p_{ii}p_{ij}}{q_{ii}q_{ij}} -1\right)\left((\theta_{k^{\prime}}^{\parallel})^{T}u_{k^{\prime}}u_{k^{ \prime}}^{T}\mu_{k^{\prime}}\right)\left((\theta_{k}^{\parallel})^{T}u_{k} \right)^{2}\right]\right| \leq\mathbb{E}_{S}\left[\left(Z_{i}Z_{j}-1\right)\left|(\theta_{ k^{\prime}}^{\parallel})^{T}u_{k^{\prime}}u_{k^{\prime}}^{T}\mu_{k^{\prime}} \right|\left((\theta_{k}^{\parallel})^{T}u_{k}\right)^{2}\right]\] \[\leq C\left(\|\theta_{k}\|^{2}+\|\theta_{k^{\prime}}\|^{2}\right) \|\theta_{k^{\prime}}^{\parallel}\theta_{k}^{\parallel}|^{2},\]

Again the second inequality follows from applying Lemma E.5 first (several times as described in the previous lemma), and then Lemma E.4 repeatedly for the remainder of the variables in \(S\). Taking absolute values proves the lemma.