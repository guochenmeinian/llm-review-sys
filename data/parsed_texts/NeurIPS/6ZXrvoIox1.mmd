# Beating Adversarial Low-Rank MDPs with Unknown Transition and Bandit Feedback

 Haolin Liu

University of Virginia

srs8rh@virginia.edu

&Zakaria Mhammedi

Google Research

mhammedi@google.com

&Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

&Julian Zimmert

Google Research

zimmert@google.com

The authors are listed in alphabetical order.

###### Abstract

We consider regret minimization in low-rank MDPs with fixed transition and adversarial losses. Previous work has investigated this problem under either full-information loss feedback with unknown transitions (Zhao et al., 2024), or bandit loss feedback with known transition (Foster et al., 2022). First, we improve the \(\mathrm{poly}(d,A,H)T^{\nicefrac{{5}}{{6}}}\) regret bound of Zhao et al. (2024) to \(\mathrm{poly}(d,A,H)T^{\nicefrac{{2}}{{3}}}\) for the full-information unknown transition setting, where \(d\) is the rank of the transitions, \(A\) is the number of actions, \(H\) is the horizon length, and \(T\) is the number of episodes. Next, we initiate the study on the setting with bandit loss feedback and unknown transitions. Assuming that the loss has a linear structure, we propose both model-based and model-free algorithms achieving \(\mathrm{poly}(d,A,H)T^{\nicefrac{{2}}{{3}}}\) regret, though they are computationally inefficient. We also propose oracle-efficient model-free algorithms with \(\mathrm{poly}(d,A,H)T^{\nicefrac{{4}}{{5}}}\) regret. We show that the linear structure is necessary for the bandit case--without structure on the reward function, the regret has to scale polynomially with the number of states. This is contrary to the full-information case (Zhao et al., 2024), where the regret can be independent of the number of states even for unstructured reward function.

## 1 Introduction

We study online reinforcement learning (RL) in low-rank Markov Decision Processes (MDPs). Low-rank MDPs is a class of MDPs where the transition probability can be decomposed as an inner product between two low-dimensional features, i.e., \(P(x^{\prime}\mid x,a)=\phi^{\star}(x,a)^{\top}\mu^{\star}(x^{\prime})\), where \(P(x^{\prime}\mid x,a)\) is the probability of transitioning to state \(x^{\prime}\) when the learner takes action \(a\) on state \(x\), and \(\phi^{\star}\), \(\mu^{\star}\) are two feature mappings. The ground truth features \(\phi^{\star}\) and \(\mu^{\star}\) are unknown to the learner. This setting has recently caught theoretical attention due to its simplicity and expressiveness (Agarwal et al., 2020; Uehara et al., 2021; Zhang et al., 2022; Cheng et al., 2023; Modi et al., 2024; Zhang et al., 2022; Mhammedi et al., 2024; Huang et al., 2023). In particular, since the learner does not know the features, it is necessary for the learner to perform _feature learning_ (or _representation learning_) to approximate them. This allows low-rank MDPs to model the additional difficulty not present in traditional linear function approximation schemes where the features are given, such as in linear MDPs (Jin et al., 2020) and in linear mixture MDPs (Ayoub et al., 2020). Since feature learning is an indispensable part of modern deep RL pipelines, low-rank MDP is a model that is closer to practice than traditional linear function approximation.

Most prior theoretical work on low-rank MDPs focuses on reward-free learning; this is a setting where instead of focusing on a particular reward function, the goal is to learn a model for the transitions (or, in the model-free setting, a small set of policies with good state cover), that enables policy optimization for _any_ downstream reward functions. While this is a reasonable setup in some cases, in other applications, the learner can only obtain loss information from interactions with the environment, and only observes the loss on the state-actions that have been visited (i.e., bandit feedback). This introduces additional challenges to the learner.

Furthermore, in many online learning scenarios, the loss function may change over time, reflecting the non-stationary nature of the environment or task switches (Padakandla et al., 2020). This could be modeled by the _adversarial_ MDP setting, where the loss function changes arbitrarily from one episode to the next, and the changes might even depend on the behavior of the learner. This setting is also extensively studied, but mostly restricted to tabular MDPs (Rosenberg and Mansour, 2019; Jin et al., 2020; Shani et al., 2020; Luo et al., 2021) or traditional linear function approximation schemes (Cai et al., 2020; Luo et al., 2021; He et al., 2022; Zhao et al., 2022; Sherman et al., 2023; Dai et al., 2023; Liu et al., 2023). The work by Zhao et al. (2024) initiated the study on adversarial MDPs in low-rank MDPs, but their work is restricted to full-information loss feedback.

When feature learning, bandit feedback, and adversarial losses are combined, the problem becomes highly challenging, and to the best of our knowledge their are no provably efficient algorithms to tackle this setting. In this work, we provide the first result for this combination. We hope that our result would bring new ideas to RL in practice, where all three elements are usually present simultaneously. We give several main results, targeting at either tighter regret (i.e., the performance gap between the optimal policy and the learner) or computational efficiency, as summarized in Table 1. Below we give a brief introduction for each of them. A more thorough related work review is in Appendix A.

* \(T^{\nicefrac{{2}}{{3}}}\)**-regret algorithm under full-information feedback (Algorithm 1).** This setting is studied by the only prior work in adversarial low-rank MDPs (Zhao et al., 2024), and we greatly improve their \(T^{\nicefrac{{5}}{{6}}}\) regret bound to \(T^{\nicefrac{{2}}{{3}}}\). Our algorithm begins with a model-based initial exploration phase to estimate the transition. It then performs policy optimization where the critic is the \(Q\) value induced by the estimated transition and the full information loss.
* \(T^{\nicefrac{{2}}{{3}}}\)**-regret model-based/model-free inefficient algorithm under bandit feedback (Algorithm 2, Algorithm 5).** Algorithm 2 starts with a model-based initial exploration phase to learn an estimated transition, and then runs exponential weights over policy space for regret minimization in the second phase. To tackle bandit feedback, we construct a novel loss estimator that leverages the structure of low-rank MDP to perform accurate off-policy evaluation. Algorithm 5 starts with a different exploration phase, where it calls VoX(Mhammedi et al., 2023) to learn a policy cover; VoX is a model-free, reward-free exploration algorithm. After this initial exploratory phase, the algorithm also applies exponential weights and utilizes the same loss estimator as in Algorithm 2. However, due to its model-free nature, certain components of the estimator cannot be directly accessed and must be derived through specific optimizations.
* \(T^{4/5}\)**-regret model-free oracle-efficient algorithm under bandit feedback (Algorithm 3, Algorithm 4).** Algorithm 3 also starts with the model-free exploration algorithm VoX(Mhammedi et al., 2023) to learn a policy cover. After that, the algorithm operates in epochs; during epoch \(k\), the algorithm commits to a fixed mixture of policies. This mixture consists of certain exploratory policies (based on the policy cover from the initial phase) and a policy computed using an online learning algorithm based on estimated \(Q\)-functions from previous epochs (these serve as loss functions). Algorithm 4 deals with the much more challenging setting of an _adaptive_ adversary with bandit feedback. Here, we make the additional assumption that the loss feature, which may be different from the feature of the low-rank decomposition, is given. The algorithm is similar to Algorithm 3 with key differences outlined in Section 4.3.

## 2 Preliminaries

We study the episodic online reinforcement learning setting with horizon \(H\). We consider an MDP \(\mathcal{M}=(\mathcal{X},\mathcal{A},P^{*}_{1:H})\), where \(\mathcal{X}\) represents a countable (possibly infinite) state space2, \(\mathcal{A}\) is a finite action space, and \(P^{*}_{h}:\mathcal{X}\times\mathcal{A}\rightarrow\Delta(\mathcal{X})\) denotes the transition kernel from layer \(h\) to \(h+1\). We assume that the initial state \(x_{1}\in\mathcal{X}\) is fixed for simplicity without loss of generality. For any policy \(\pi:\mathcal{X}\mapsto\Delta(\mathcal{A})\) and arbitrary set of transition kernels \(\{P_{h}\}_{h\in[H]}\), we let \(\mathbb{P}^{P,\pi}\) denote the law over \((\bm{x}_{1},\bm{a}_{1},\dots,\bm{x}_{H},\bm{a}_{H})\) induced by the process of setting \(\bm{x}_{1}=x_{1}\), sampling \(\bm{a}_{1}\sim\pi_{1}(\cdot\mid\bm{x}_{1})\), then for \(h=2,\dots,H\), \(\bm{x}_{h}\sim P_{h-1}(\cdot\mid\bm{x}_{h-1},\bm{a}_{h-1})\) and \(\bm{a}_{h}\sim\pi_{h}(\cdot\mid\bm{x}_{h})\). We let \(\mathbb{E}^{P,\pi}\) denote the corresponding expectations. Further, we let \(d^{P,\pi}_{h}(x)\coloneqq\mathbb{P}^{P,\pi}[\bm{x}_{h}=x]\) denote the _occupancy_ of \(x\in\mathcal{X}\). We also let \(d^{P,\pi}_{h}(x,a)\coloneqq\mathbb{P}^{P,\pi}[\bm{x}_{h}=x,\bm{a}_{h}=a]\). Further, we let \(\mathbb{E}^{\pi}=\mathbb{E}^{P^{*},\pi}\), \(\mathbb{P}^{\pi}=\mathbb{P}^{P^{*},\pi}\), and \(d^{\pi}_{h}=d^{P^{*},\pi}_{h}\). We use \(\pi\circ_{h}\pi^{\prime}\) to denote a policy that follows \(\pi_{k}(\cdot\mid\cdot)\) for \(k<h\) and \(\pi^{\prime}_{k}(\cdot\mid\cdot)\) for \(k\geq h\). Similarly, \(\pi\circ_{h}\pi^{\prime}\circ_{h^{\prime}}\pi^{\prime\prime}\) denotes a policy that follows \(\pi_{k}\) for \(k<h\), \(\pi^{\prime}_{k}\) for \(h\leq k<h^{\prime}\) and \(\pi^{\prime\prime}_{k}\) for \(k>h^{\prime}\).

Footnote 2: We assume that \(\mathcal{X}\) is countable only to simplify the presentation. Our results can easily be extended to a continuous state space with an appropriate measure-theoretic treatment (see e.g. Mhammedi et al. (2023)).

We consider a learner interacting with the MDP \(\mathcal{M}\) for \(T\) episodes with adversarial loss functions. Before the game starts, an oblivious adversary chooses the loss functions for all episodes \(\big{(}\ell^{t}_{1:H}:\mathcal{X}\times\mathcal{A}\rightarrow[0,1]\big{)}_{t=1 }^{T}\). For each episode \(t\in[T]\), the learner starts at state \(\bm{x}^{t}_{1}=x_{1}\), then for each step \(h\in[H]\) within episode \(t\), the learner observes state \(\bm{x}^{t}_{h}\in\mathcal{X}_{h}\), chooses an action \(\bm{a}^{t}_{h}\in\mathcal{A}\), then suffers loss \(\ell^{t}_{h}(\bm{x}^{t}_{h},\bm{a}^{t}_{h})\). The state \(\bm{x}^{t}_{h+1}\) at the next step is drawn from transition \(P^{*}_{h}(\cdot\mid\bm{x}^{t}_{h},\bm{a}^{t}_{h})\). We consider _bandit feedback_ setting where the learner could only observe the losses \(\ell^{t}_{1}(\bm{x}^{t}_{1},\bm{a}^{t}_{1}),\dots,\ell^{t}_{H}(\bm{x}^{t}_{H},\bm{a}^{t}_{H})\) at the visited state-action pairs.

We let \(\Pi\coloneqq\{\pi:\mathcal{X}\rightarrow\Delta(\mathcal{A})\}\) denote the set of Markovian policies. For policy \(\pi\in\Pi\), loss \(\ell\) and transition kernels \(P_{1:H}\), we denote by \(Q^{P,\pi}_{h}(\cdot,\cdot;\ell)\) the _state-action_ value function (a.k.a. \(Q\)-function) at step \(h\in[H]\) with respect to the transitions \(P_{1:H}\) and loss \(\ell\); that is

\[Q^{P,\pi}_{h}(x,a;\ell)\coloneqq\mathbb{E}^{P,\pi}\left[\sum_{s=h}^{H}\ell_{s} (\bm{x}^{t}_{s},\bm{a}^{t}_{s})\mid\bm{x}_{h}=x,\bm{a}_{h}=a\right],\] (1)

for all \((x,a)\in\mathcal{X}\times\mathcal{A}\). We let \(V^{P,\pi}_{h}(x;\ell)=\max_{a\in\mathcal{A}}Q^{P,\pi}_{h}(x,a;\ell)\) be the corresponding _state_ value function at layer \(h\). Further, we write \(Q^{\pi}_{h}(\cdot,\cdot;\ell)\coloneqq Q^{P,\pi}_{h}(\cdot,\cdot;\ell)\) and \(V^{\pi}_{h}(\cdot;\ell)\coloneqq V^{P^{*},\pi}(\cdot;\ell)\).

For all of our algorithms except for Algorithm 4, we aim to construct (possibly randomized) policies \(\{\bm{\pi}^{t}\}_{t\in[T]}\) that ensure a sublinear _pseudo-regret_ with respect to the best-fixed policy; that is,

\[\operatorname{Reg}_{T}\coloneqq\min_{\pi\in\Pi}\operatorname{Reg}_{T}(\pi) \quad\text{where}\quad\operatorname{Reg}_{T}(\pi)\coloneqq\mathbb{E}\left[\sum_ {t=1}^{T}V^{\bm{\pi}^{t}}_{1}(x_{1};\ell^{t})-\sum_{t=1}^{T}V^{\pi}_{1}(x_{1}; \ell^{t})\right].\] (2)

ForAlgorithm 4, we bound the standard _regret_

\[\overline{\operatorname{Reg}}_{T}\coloneqq\sum_{t=1}^{T}V^{\pi^{t}}_{1}(x_{1}; \ell^{t})-\min_{\pi\in\Pi}\sum_{t=1}^{T}V^{\pi}_{1}(x_{1};\ell^{t})\] (3)with high probability. This allows it to handle adaptive adversary.

Throughout, we will assume that the MDP \(\mathcal{M}\) is low-rank with _unknown_ feature maps \(\phi^{\star}_{h}\) and \(\mu^{\star}_{h}\).

**Assumption 2.1** (Low-Rank MDP).: _There exist (unknown) features maps \(\phi^{\star}_{1:H}:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\) and \(\mu^{\star}_{1:H}:\mathcal{X}\rightarrow\mathbb{R}^{d}\), such that for all \(h\in[H-1]\) and \((x,a,x^{\prime})\in\mathcal{X}\times\mathcal{A}\times\mathcal{X}\):_

\[\mathbb{P}[\bm{x}_{h+1}=x^{\prime}\mid\bm{x}_{h}=x,\bm{a}_{h}=a]=\phi^{\star}_ {h}(x,a)^{\top}\mu^{\star}_{h+1}(x^{\prime}).\] (4)

_Furthermore, for all \(h\in[H]\), the feature maps \(\mu^{\star}_{h}\) and \(\phi^{\star}_{h}\) are such that \(\sup_{(x,a)\in\mathcal{X}\times\mathcal{A}}\left\lVert\phi^{\star}_{h}(x,a)\right\rVert\leq 1\) and \(\|\sum_{x\in\mathcal{X}}g(x)\cdot\mu^{\star}_{h}(x)\|\leq\sqrt{d}\), for all \(g:\mathcal{X}\rightarrow[0,1]\)._

Loss function under bandit feedback.For bandit feedback setting, we make the following additional linear assumption on the losses; in the sequel, we will argue that this is necessary to avoid a sample complexity scaling with the number of states. This linear loss assumption also appears in Ren et al. (2022); Zhang et al. (2022) for stochastic low-rank MDPs. Note that for the full-information feedback setting, such an assumption is not required.

**Assumption 2.2** (Loss Representation).: _For any \(t\in[T]\) and layer \(h\), there is a vector \(g^{t}_{h}\in\mathbb{B}_{d}(1)\) such that the loss \(\ell^{t}_{h}(x,a)\) at round \(t\) satisfies:_

\[\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad\ell^{t}_{h}(x,a)=\phi^{ \star}_{h}(x,a)^{\top}g^{t}_{h}.\] (5)

We note that there is no loss of generality in assuming that the losses are expressed using the same features \(\phi^{\star}_{1:H}\) as the low-rank structure in (4). This is because if the losses have different features, we can simply combine these features with the low-rank features, and redefine \(\phi^{\star}\) accordingly. For the bulk of our results (and as stated in the prequel), we will assume that the losses \(\{\ell^{t}_{h}(\cdot,\cdot)\}_{h\in[H],t\in[T]}\) (or equivalently \(\{g^{t}_{h}\}_{h\in[H],t\in[T]}\) under Assumption 2.2) are chosen by an adversary before the start of the game (i.e. oblivious adversary). In Section 4.3, we will present a model-free, oracle-efficient algorithm for an adaptive adversary.

Function approximation.So far, Assumption 2.1 and Assumption 2.2 are in line with assumptions made in the linear MDP setting (Jin et al., 2020). However, unlike in linear MDPs, we do not assume that the feature maps \(\phi^{\star}_{1:H}\) are known. To facilitate representation learning and ultimately a sublinear regret, we need to make _realizability_ assumptions. In particular, in the model-free setting, we assume we have a function class \(\Phi\) that contains the true features \(\phi^{\star}_{1:H}\). In the model-based setting, we additionally assume access to a function class \(\Upsilon\) that contains the feature maps \(\mu^{\star}_{1:H}\)3. We will formalize these assumptions in their corresponding sections in the sequel.

Footnote 3: The setting where we assume access to function classes that realize both \(\phi^{\star}_{1:H}\) and \(\mu^{\star}_{1:H}\) is called _model-based_ because it allows one to model the transition probabilities, thanks to the low-rank structure in (4).

Other notation.For \(\psi:\Pi\rightarrow\mathbb{R}^{d}\), we define \(\texttt{John}(\psi,\Pi)\) as a distribution \(\mu\in\Delta(\Pi)\) such that \(\left\lVert\psi(\pi)\right\rVert^{2}_{G^{-1}}\leq d\) for all \(\pi\in\Pi\), where \(G=\sum_{\pi\in\Pi}\mu(\pi)\cdot\psi(\pi)\psi(\pi)^{\top}\). This is the standard John's exploration or \(G\)-optimal design, which always exists.

## 3 Model-based Algorithms for Adversarial Low-rank MDPs

In this section, we discuss adversarial low-rank MDPs under model-based assumption. The model-based assumption is formalized in the Assumption 3.1 below. This assumption is standard which also appears in prior works on model-based learning in low-rank MDPs (Agarwal et al., 2020; Uehara et al., 2021; Zhang et al., 2022; Cheng et al., 2023; Zhao et al., 2024).

**Assumption 3.1** (Model-based assumption).: _The learner has access to two model spaces \(\Phi\) and \(\Upsilon\) such that \(\phi^{\star}\in\Phi\) and \(\mu^{\star}\in\Upsilon\). Moreover, for any \(\phi\in\Phi\), \(\mu\in\Upsilon\), and \(h\in[2\mathinner{.}H]\), we have \(\sup_{(x,a)\in\mathcal{X}\times\mathcal{A}}\left\lVert\phi_{h-1}(x,a)\right\rVert\leq 1\), \(\sum_{x^{\prime}\in\mathcal{X}}\phi_{h-1}(x,a)^{\top}\mu_{h}(x^{\prime})=1\) and \(|\sum_{x\in\mathcal{X}}g(x)\cdot\mu_{h}(x)\|\leq\sqrt{d}\), for all \(g:\mathcal{X}\rightarrow[0,1]\)._

### Adversarial Low-rank MDPs with Full Information

We first discuss learning adversarial low-rank MDPs with full information and model-based assumption. This setting aligns with Zhao et al. (2024), and our Algorithm 1 successfully improves their regret from \(T^{\nicefrac{{5}}{{6}}}\) to \(T^{\nicefrac{{2}}{{3}}}\).

As argued in Zhao et al. (2024), the challenge of learning adversarial low-rank MDPs lies in the need for balancing exploration and exploitation both in representation learning and policy optimization over adversarial losses. To tackle this doubled exploration and exploitation challenge, the algorithm of Zhao et al. (2024) performs _simultaneous_ representation learning and policy optimization. With a closer look at their analysis, we find that there is a drawback of this approach: because their algorithm handles the two tasks at the same time, it spends less exploration for representation learning in the early phase of the algorithm. This results in larger error in the estimated Q-values (i.e., critic) fed to policy optimization, and worsens the overall regret.

To address this issue, we design Algorithm 1 as a simple two-phase algorithm that separates representation learning and policy optimization. In the first phase, following Cheng et al. (2023), we perform optimal reward-free exploration for low-rank MDPs to estimate the transition. The resulted estimator, \(\widetilde{P}\), is able to accurately approximate the true transition and give accurate Q-value estimators for any policy. The more accurate Q-estimator allows for more effective policy optimization in the second phase. Theorem 3.1 shows the guarantee of Algorithm 1 where \(\widetilde{\mathcal{O}}\) hides logarithmic factors of \(d,H,T,|\Phi||\Upsilon|\).

**Theorem 3.1**.: _Algorithm 1 ensures \(\operatorname{Reg}_{T}\leq\widetilde{\mathcal{O}}\left(H^{3}\left(d^{2}+| \mathcal{A}|\right)T^{\frac{2}{3}}\right)\)._

The proof for Theorem 3.1 is given in Appendix B. Zhao et al. (2024) also constructs a lower bound \(\Omega\left(H\sqrt{d|\mathcal{A}|T}\right)\) for this settings. Thus, the \(\operatorname{poly}(|\mathcal{A}|)\)-dependence is unavoidable.

### Model-Based, Computationally Inefficient Algorithm for Bandit Feedback

In this section, following Assumption 3.1, we introduce the first (model-based) algorithm (Algorithm 2) for adversarial low-rank MDPs with bandit feedback and sublinear regret. Compared with linear MDPs, the key challenge for more general low-rank MDPs is to construct a proper loss estimator. For linear MDP, since the feature is known, the loss estimator closely resembles that of linear bandits. However, low-rank MDPs lack such structural simplicity, making standard loss estimators invalid. To overcome this challenge, we propose a new loss estimator that works for any loss function based on off-policy evaluation and the low-rank structure of transition. In this section, \(\widetilde{\mathcal{O}}\) hides logarithmic factors of \(d,H,T,|\Phi||\Upsilon|\).

In Algorithm 2, we first conduct an initial representation learning phase to establish accurate transition estimator \(\widetilde{P}\) and its corresponding features \(\hat{\phi}\) and \(\hat{\mu}\) based on reward-free exploration algorithms in Cheng et al. (2023). Then, in the second phase, we use exponential weights to maintain a distribution over the policy space \(\Pi^{\prime}\) where we mix a uniform policy with \(\Pi\) to enhance exploration. At every round \(t\), a behavior policy \(\pi^{t}\) is chosen from the current policy distribution, and we use the data collected by \(\pi^{t}\) to estimate the value for every \(\pi\in\Pi^{\prime}\). The success of such off-policy evaluation is based on the following observations of low-rank MDP. Using the low-rank transition structure, for \(h\geq 2\), we have

\[\forall\pi\in\Pi,\quad d^{\pi}_{h}(x)=\phi^{*}_{h-1}(\pi)^{\top}\mu^{*}_{h}(x ),\quad\text{where}\quad\phi^{*}_{h-1}(\pi)\coloneqq\mathbb{E}^{\pi}\big{[} \phi^{*}_{h-1}(\bm{x}_{h-1},\bm{a}_{h-1})\big{]}.\] (6)Thus, using the definition of the \(V\)-function from Section 2, we have for any loss function \(\ell\) and \(\pi\):

\[V_{1}^{\pi}(x_{1};\ell)-\mathbb{E}[\ell_{1}(\bm{x}_{1},\bm{a}_{1})]=\sum_{h\geq 2 }^{H}\sum_{(x,a)\in\mathcal{X}\times\mathcal{A}}\phi_{h-1}^{\star}(\pi)^{\top} \mu_{h}^{\star}(x)\pi_{h}(a\mid x)\cdot\ell_{h}(x,a).\] (7)

Letting \(\Lambda_{h}^{t}\coloneqq(\mathbb{E}_{\pi^{t}\sim p^{t}}\left[\phi_{h}^{\star}( \pi^{t})\phi_{h}^{\star}(\pi^{t})^{\top}\right])^{-1}\) and ignoring the loss term \(\mathbb{E}[\ell_{1}(\bm{x}_{1},\bm{a}_{1})]\) from the first step (this term can easily be treated as in a bandit setting with \(H=1\)), we have for all \(\pi\in\Pi^{\prime}\):

\[V_{1}^{\pi}(x_{1};\ell) =\mathbb{E}_{\pi^{t}\sim p^{t}}\left[\sum_{h=2}^{H}\sum_{(x,a)\in \mathcal{X}\times\mathcal{A}}\phi_{h-1}^{\star}(\pi)^{\top}\Lambda_{h-1}^{t} \phi_{h-1}^{\star}(\pi^{t})\phi_{h-1}^{\star}(\pi^{t})^{\top}\mu_{h}^{\star}(x )\pi_{h}(a\mid x)\cdot\ell_{h}(x,a)\right],\] \[=\mathbb{E}_{\pi^{t}\sim p^{t}}\left[\sum_{h=2}^{H}\sum_{x,a}\phi_ {h-1}^{\star}(\pi)^{\top}\Lambda_{h-1}^{t}\phi_{h-1}^{\star}(\pi^{t})\cdot d_{ h}^{\pi^{t}}(x)\pi_{h}^{t}(a\mid x)\frac{\pi_{h}(a\mid x)}{\pi_{h}^{t}(a\mid x )}\cdot\ell_{h}(x,a)\right],\] \[=\mathbb{E}_{\pi^{t}\sim p^{t}}\mathbb{E}^{\pi^{t}}\left[\sum_{h=2 }^{H}\phi_{h-1}^{\star}(\pi)^{\top}\Lambda_{h-1}^{t}\phi_{h-1}^{\star}(\pi^{t} )\cdot\frac{\pi_{h}(\bm{a}_{h}\mid\bm{x}_{h})}{\pi_{h}^{t}(\bm{a}_{h}\mid\bm{x }_{h})}\cdot\ell_{h}(\bm{x}_{h},\bm{a}_{h})\right].\]

Thus, for all \(\ell\) and \(\pi\), \(\sum_{h=2}^{H}\phi_{h-1}^{\star}(\pi)^{\top}\Lambda_{h-1}^{t}\phi_{h-1}^{ \star}(\pi^{t})\cdot\pi_{h}(\bm{a}_{h}\mid\bm{x}_{h})\pi_{h}^{t}(\bm{a}_{h} \mid\bm{x}_{h})^{-1}\cdot\ell_{h}(\bm{x}_{h},\bm{a}_{h})\) for \(\pi^{t}\sim p^{t}\) and \((\bm{x}_{h},\bm{a}_{h})\sim d_{h}^{\pi^{t}}\) is an unbiased estimator of \(V_{1}^{\pi}(x_{1},\ell)\). However, \(\phi_{h-1}^{\star}(\pi)\) is not accessible because both the true feature \(\phi^{\star}\) and occupancy \(d^{\pi}\) for the true transition are unknown. Thus, our estimator incorporates the learned feature \(\hat{\phi}\) and the occupancy of \(\widetilde{P}\) instead as shown in Line 4 and Line 10. Utilizing estimated features and transition could introduce additional bias but the initial representation learning already ensures such bias is small enough to tackle. We compensate for the bias by incorporating an exploration bonus \(b^{t}(\pi)\) in exponential weights. To further encourage exploration, we additionally perform John's exploration together with exponential weights when selecting behavior policies. The main guarantee of Algorithm 2 is given in Theorem 3.2.

**Theorem 3.2**.: _Algorithm 2 achieves \(\operatorname{Reg}_{T}(\pi)\leq\widetilde{\mathcal{O}}\left(d^{2}H^{3}| \mathcal{A}|(d^{2}+|\mathcal{A}|)T^{\nicefrac{{2}}{{3}}}\log|\Pi|\right)\) for any \(\pi\in\Pi\)._

Note that the guarantee in Theorem 3.2 only holds for policy \(\pi\in\Pi\). To ensure our regret bound is meaningful, at least a near-optimal policy should be contained in the given policy set \(\Pi\). In general, the size of such a policy set would grow exponentially with the number of states (e.g. covering of all Markovian policies), making the regret have polynomial dependence on the number of states. In Theorem 3.3, we show that even for low-rank MDPs, if the loss function lacks structure, the regret cannot avoid polynomial dependence on the number of states. The detailed construction for this lower-bound is given in Appendix H.

**Theorem 3.3**.: _There exists a low-rank MDP with \(|\mathcal{X}|\) states, \(|\mathcal{A}|\) actions and sufficiently large \(T\) with unstructured losses such that any agent suffers at least regret of \(\Omega(\sqrt{|\mathcal{X}||\mathcal{A}|T})\)._

Theorem 3.3 shows that under bandit feedback, in general, we could not gain too much from low-rank transition structure compared with tabular MDPs. This contrasts with the \(\Omega(\sqrt{|\mathcal{A}|T})\) lower bound in the full information settings (Zhao et al. (2024)). To get rid of any dependence on the number of states, we additionally introduce Assumption 2.2 to impose linear structure on the loss function. Unlike linear MDPs that require the loss feature to be known, our algorithm can even handle linear loss with unknown feature, since our Algorithm 2 never explicitly uses the loss feature. The linear structure is only used to control the size of the candidate policy class in the analysis (i.e., making \(\log|\Pi|\) irrelevant to the number of states). Specifically, when both loss and transition are linear, the Q-function is also linear, making it sufficient to consider the following linear policy space:

\[\Pi_{\text{lin}}=\left\{\pi:\mathcal{X}\to\mathcal{A}\;\middle|\;\pi_{h}(a\; \middle|\;x)=\mathbb{I}\big{\{}a=\operatorname*{argmin}_{a\in\mathcal{A}} \phi_{h}(x,a)^{\top}\theta_{h}\big{\}},\;h\in[H],\;\middle|\theta_{h}\right|_{2 }\leq\sqrt{d}HT,\;\phi\in\Phi\right\}.\]

The \(\frac{1}{T}\)-cover of \(\Pi_{\text{lin}}\) only have size \(|\Phi|\cdot T^{\mathcal{O}(d)}\) following standard arguments (e.g, Exercise 27.6 of Lattimore and Szepesvari (2020)) and if we feed it into Algorithm 2, our regret could avoid dependece on the size of state space as shown in Corollary 3.1.

**Corollary 3.1**.: _If the loss function satisfies Assumption 2.2, applying Algorithm 2 with \(\Pi\) as the \(\frac{1}{T}\)-cover of \(\Pi_{\text{lin}}\) ensures \(\operatorname{Reg}_{T}\leq\widetilde{\mathcal{O}}\left(d^{3}H^{3}|\mathcal{A}| (d^{2}+|\mathcal{A}|)T^{\nicefrac{{2}}{{3}}}\right)\)._

## 4 Model-free Algorithms for Adversarial Low-rank MDPs

In this section, we consider the model-free setting, where we only assume access to a feature class \(\Phi\) that contains the true feature map \(\phi^{\star}\).

**Assumption 4.1** (Model-free realizability).: _The learner has access to a function class \(\Phi\) such that_

\[\phi^{\star}\in\Phi\quad\quad\text{and}\quad\quad\sup_{\phi\in\Phi}\sup_{(x,a )\in\mathcal{X}\times\mathcal{A}}\|\phi(x,a)\|\leq 1.\] (8)

This is a standard assumption in the context of model-free RL (Modi et al., 2024; Zhang et al., 2022b; Mhammedi et al., 2024a). We note that having access to the function class \(\Phi\) alone (instead of both \(\Phi\) and \(\Upsilon\) as in Assumption 3.1) is not sufficient to model the transition probabilities (unlike in the model-based case). This makes the model-free setting much more challenging; in fact, until the recent work by Mhammedi et al. (2023) there were no model-free, oracle-efficient algorithms for this setting that do not require any additional structural assumptions on the MDP.

### Model-free, Inefficient Algorithm for Bandit Feedback

Our first algorithm follows the same structure as Algorithm 2 but incorporates a model-free initial exploration phase introduced by Mhammedi et al. (2023). Unlike the model-based exploration phase, which directly provides an estimated transition, the model-free exploration phase outputs a policy cover. This policy cover can be combined with the optimization in Algorithm 1 of Liu et al. (2023) to solve the expected feature \(\hat{\phi}\), which is then used in the loss estimator in Line 10 of Algorithm 2. The algorithm, summarized in Algorithm 5, is inefficient but achieves \(T^{\nicefrac{{2}}{{3}}}\) regret. More details and proofs can be found in Appendix D.

### Model-free, Oracle Efficient Algorithm for Bandit Feedback (Oblivious Adversary)

We now descibe the key component of our efficient model-free algorithm (Algorithm 3).

Exploration phase and policy cover.Similar to algorithms in the previous section, Algorithm 3 begins with a reward-free exploratorion phase; Line 2 of Algorithm 3. However, unlike in the previous sections where the role of this exploration phase was to learn a model for the transition probabilities, here the goal is to compute a, so called, _policy cover_ which is a small set of policies that can be used to effectively explore the state space.

**Definition 4.1** (Approximate policy cover).: _For \(\alpha,\varepsilon\in(0,1]\) and \(h\in[H]\), a subset \(\Psi\subseteq\Pi\) is an \((\alpha,\varepsilon)\)-policy cover for layer \(h\) if_

\[\max_{\pi\in\Psi}d_{h}^{\pi}(x)\geq\alpha\cdot\max_{\pi^{\prime}\in\Pi}d_{h}^ {\pi^{\prime}}(x),\quad\text{for all }x\in\mathcal{X}\text{ such that }\max_{\pi^{\prime}\in\Pi}d_{h}^{\pi^{\prime}}(x)\geq \varepsilon\cdot\|u_{h}^{\star}(x)\|.\] (10)In Line 2, Algorithm 3 calls Vox(Mhammedi et al., 2023), a reward-free and model-free exploration algorithm to compute \((1/(8Ad),\varepsilon)\)-policy covers \(\Psi_{1}^{\mathrm{cov}},\ldots,\Psi_{H}^{\mathrm{cov}}\) for layers \(1,\ldots,H\), respectively, with \(|\Psi_{h}|=d\) for all \(h\in[H]\). This call to Vox requires \(O(1/\varepsilon^{2})\) episodes; see the guarantee of Vox in Lemma G.1. After this initial phase, the algorithm operates in epochs, each consisting of \(N_{\mathrm{reg}}\in\mathbb{N}\) episodes, where in each epoch \(k\in[K]\), the algorithm commits to executing policies sampled from a fixed policy distribution \(\rho^{(k)}\in\Delta(\Pi)\) with support on the policy covers \(\Psi_{1:H}^{\mathrm{cov}}\) and a policy \(\widetilde{\pi}^{(k)}\) specified by an online learning algorithm. Next, we describe in more detail how \(\rho^{(k)}\) is constructed and motivate the elements of its construction starting with the online learning policies \(\{\widetilde{\pi}^{(k)}\}_{k\in[K]}\).

Online learning policies.Given estimates \(\{\widetilde{Q}_{1:H}^{(s)}\}_{s<k}\) of the average \(Q\)-functions

\[\left\{\frac{1}{N_{\mathrm{reg}}}\sum_{t\text{ in epoch }s}Q_{1:H}^{\overline{ \pi}^{(k)}}(\cdot,\cdot;\ell^{t})\right\}_{s<k}\] (11)

from the previous epoch (we will describe how these estimates are computed in the sequel), Algorithm 3 computes policy \(\widetilde{\pi}^{(k)}\) for epoch \(k\) according to

\[\widetilde{\pi}_{h}^{(k)}(a\mid x)\propto\exp\left(-\eta\sum_{s<k}\widetilde{ Q}_{h}^{(s)}(x,a)\right)\!,\] (12)

for all \(h\in[H]\). Given a state \(x\in\mathcal{X}\), such exponential weight update ensures a sublinear regret with respect to the sequence of loss functions given by \(\{\pi(\cdot\mid x)\mapsto\sum_{h\in[H]}\widetilde{Q}_{h}^{(k)}(x,\pi_{h}(\cdot \mid x))\}_{k\in[K]}\). Thanks to the performance difference lemma, and as shown in Luo et al. (2021), a sublinear regret with respect to these "surrogate" loss functions translates into a sublinear regret in the low-rank MDP game we are interested in, granted that \(\{\widetilde{Q}_{1:H}^{(k)}\}_{k\in[K]}\) are good estimates of the average \(Q\)-functions (Luo et al., 2021). In line with previous analyses, we require the \(Q\)-function estimates to ensure that the following bias term

\[\mathbb{E}^{\pi}\left[\max_{a\in\mathcal{A}}\left(\frac{1}{N_{\mathrm{reg}}} \sum_{t\text{ in epoch }k}Q_{h}^{\overline{\pi}^{(k)}}(\bm{x}_{h},a;\ell^{t})- \widetilde{Q}_{h}^{(k)}(\bm{x}_{h},a)\right)^{2}\right]\] (13)

is small for all \(h\in[H]\), \(k\in[K]\), and \(\pi\in\Pi\).

\(Q\)-function estimates.Thanks to the low-rank MDP structure and the linear loss representation assumption (Assumption 2.2), the average \(Q\)-functions in (11) are linear in the feature maps \(\phi^{\star}\). Thus, using the function class \(\Phi\) in Assumption 2.1 we can estimate these average \(Q\)-functions by regressing the sum of losses \(\sum_{s=h}^{H}\ell_{s}^{\epsilon}\) onto \(\left(\bm{x}_{h}^{t},\bm{a}_{h}^{t}\right)\) for \(t\) in the \(k\)th epoch (as in (9)). However, naively doing this using only trajectories generated by \(\widehat{\pi}^{(k)}\) would only ensure that the bias term in (13) is small for \(\pi=\widehat{\pi}^{(k)}\). To ensure that it is small for all possible policies \(\pi\)'s, we need to estimate the \(Q\)-functions on the trajectories of policies that are guaranteed to have good state coverage; this is where we use the policy cover from the initial phase.

Mixture of policies.At episode \(t\) in each epoch \(k\in[K]\), we execute policy \(\widetilde{\bm{\pi}}^{t}\) sampled from \(\rho^{(k)}\), where \(\rho^{(k)}\) is the distribution of the random policy:

\[\mathbb{I}\{\bm{\zeta}^{t}=0\}\cdot\widehat{\pi}^{(k)}+\mathbb{I}\{\bm{\zeta} ^{t}=1\}\cdot\bm{\pi}^{t}\circ_{\bm{h}^{t}}\pi_{\texttt{unif}}\circ_{\bm{h}^{t }+1}\widehat{\pi}^{(k)},\] (14)

with \(\bm{\zeta}^{t}\sim\mathrm{Ber}(\nu)\), \(\bm{h}^{t}\sim\texttt{unif}\big{(}[H]\big{)}\), and \(\bm{\pi}^{t}\sim\texttt{unif}\big{(}\Psi^{\mathrm{cov}}_{\bm{h}^{t}}\big{)}\). In words, at the start of each episode of any epoch \(k\), we execute \(\widehat{\pi}^{(k)}\) (see (12)) with probability \(1-\nu\); and with probability \(\nu\), we execute a policy in \(\Psi^{\mathrm{cov}}_{1:H}\) selected uniformly at random. As explained in the previous paragraph, this ensures a small bias for all choices of \(\pi\) in (13) thanks the policy cover property of \(\Psi^{\mathrm{cov}}_{1:H}\). We now state the guarantee of Algorithm 3.

**Theorem 4.1**.: _Let \(\delta\in(0,1)\) be given and suppose Assumption 2.1 and Assumption 2.2 hold. This, for \(T=\mathrm{poly}(A,d,H,\log(|\Phi|/\delta))\) sufficiently large, Algorithm 3 guarantees \(\mathrm{Reg}_{T}\leq\mathrm{poly}(A,d,H,\log(|\Phi|/\delta))\cdot T^{4/5}\) regret against an oblivious adversary._

The proof is in Appendix E. Note that the \(T\)-dependence in this regret even outperforms that of the previous best bound by Zhao et al. (2024) (see Table 1). Compared to their algorithm, Algorithm 3 is model-free and only requires bandit feedback. This makes the result in Theorem 4.1 rather surprising.

### Model-free, Oracle Efficient Algorithm (Adaptive Adversary)

In this section, we present a variant of Algorithm 3 (Algorithm 4) that guarantees a sublinear regret against an adaptive adversary. Given the difficulty of this setting, we make the additional assumption that the algorithm has access to the loss feature \(\phi^{\mathrm{loss}}\), which may be different than the low-rank MDP feature \(\phi^{\star}\) (unlike in Assumption 2.2).

**Assumption 4.2** (Loss Representation).: _There is a (known) feature map \(\phi^{\mathrm{loss}}\) satisfying \(\sup_{t\in[H],(x,a)\in\mathcal{X}\times\mathcal{A}}\big{|}\phi^{\mathrm{loss }}_{h}(x,a)\big{\|}\leq 1\) and such that for any round \(h\in[H]\), \(t\in[T]\), and history \(\mathcal{H}^{t-1}=(x_{1:H}^{1:t-1},a_{1:H}^{1:t-1})\), the loss function at round \(t\) satisfies_

\[\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad\ell_{h}(x,a;\mathcal{H}^{t -1})=\phi^{\mathrm{loss}}_{h}(x,a)^{\top}g^{t}_{h},\] (15)

_for some \(g^{t}_{h}\in\mathbb{B}_{d}(1)\)._

Note that Assumption 4.2 asserts that the loss at round \(t\) depends only on the history \(\mathcal{H}^{t-1}\) and the current state action pair. Before moving forward, we introduce some additional notation we will use throughout this section.

Additional notation.For any two feature maps \(\phi,\psi:\mathcal{X}\times\mathcal{A}\to\mathbb{R}^{d}\), we denote by \([\phi,\psi]:\mathcal{X}\times\mathcal{A}\to\mathbb{R}^{2d}\) the vertical concatenation of the two feature maps. For any \(h\in[H]\), \(t\in[T]\), policy \(\pi\in\Pi\), and history \(\mathcal{H}^{t-1}=(x_{1:H}^{1:t-1},a_{1:H}^{1:t-1})\), we denote by \(Q^{\pi}_{h}(\cdot,\cdot;\mathcal{H}^{t-1})\) the \(Q\)-function at layer \(h\) corresponding to rollout policy \(\pi\); that is,

\[Q^{\pi}_{h}(x,a;\mathcal{H}^{t-1})\coloneqq\mathbb{E}^{\pi}\left[\sum_{s=h}^{H }\ell_{s}(\bm{x}_{s},\bm{a}_{s};\mathcal{H}^{t-1})\mid\bm{x}_{h}=x,\bm{a}_{h}= a\right].\] (16)

Finally, we let \(V^{\pi}_{h}(x;\mathcal{H}^{t-1})\coloneqq\max_{a\in\mathcal{A}}Q^{\pi}_{h}(x,a ;\mathcal{H}^{t-1})\) denote the corresponding \(V\)-function.

Algorithm 4 is similar to Algorithm 3 with the following key differences; after computing a policy cover, the algorithm calls RepLearn (a representation learning algorithm initially introduced by Modi et al. (2024) and subsequently refined by Mhammedi et al. (2023)) to compute a feature map \(\phi^{\mathrm{rep}}\). Then, for every \(h\in[H]\), the algorithm computes a _spanner_; a set of policies \(\Psi^{\mathrm{span}}_{h}=\{\pi_{h,1},\ldots,\pi_{h,2d}\}\) that act as an approximate spanner for the set \(\{\mathbb{E}^{\pi}[\phi^{\mathrm{rep}}_{h}(\bm{x}_{h},\bm{a}_{h}),\phi^{ \mathrm{loss}}_{h}(\bm{x}_{h},\bm{a}_{h})]:\pi\in\Pi\}\subseteq\mathbb{R}^{2d}\), where we use \([\cdot,\cdot]\) to denote the vertical concatenation of vectors. These spanner policies are then used as the exploratory policies after the initial phase; that is, at episode \(t\) in each epoch \(k\in[K]\), we execute policy \(\bm{\pi}^{(k)}\) sampled from \(\rho^{(k)}\), where \(\rho^{(k)}\) is set to be the distribution of the random policy: \(\mathbb{I}[\bm{\zeta}^{t}=0\}\cdot\widehat{\bm{\pi}}^{(k)}+\mathbb{I}[\bm{\zeta} ^{t}=1\}\cdot\bm{\pi}^{t}\circ_{\bm{h}^{t+1}}\widehat{\bm{\pi}}^{(k)}\), with \(\bm{\zeta}^{t}\sim\mathrm{Ber}(\nu)\), \(\bm{h}^{t}\sim\texttt{unif}([H])\), and \(\bm{\pi}^{t}\sim\texttt{unif}(\Psi_{\bm{h}^{\texttt{sel}}}^{\texttt{span}})\). Here, the main difference to Algorithm 3 (see also (47)) is that we use \(\bm{\pi}^{t}\sim\texttt{unif}(\Psi_{\bm{h}^{\texttt{sel}}}^{\texttt{span}})\) instead of \(\bm{\pi}^{t}\sim\texttt{unif}(\Psi_{\bm{h}^{\texttt{sel}}}^{\texttt{cov}})\). We require these spanner policies instead of policies in the policy cover, as an adaptive adversary's history-dependent losses prevent standard least squares regression due to the lack of permutation invariance of state-action pairs across episodes within an epoch. Estimating the Q-functions is thus more complex, and we approach it in expectation over roll-ins using policies in \(\Psi^{\texttt{span}}\), the "in-expectation" estimation task is in a sense easier.

We now state the guarantee of Algorithm 4.

**Theorem 4.2**.: _Let \(\delta\in(0,1)\) be given and suppose that Assumption 2.1 and Assumption 4.2 hold. Then, for \(T=\mathrm{poly}(A,d,H,\log(|\Phi|/\delta))\) sufficiently large, Algorithm 4 guarantees with probability at least \(1-\delta\),_

\[\sum_{t\in[T]}V_{h}^{\bm{\pi}^{t}}(x_{1};\bm{\mathcal{H}}^{t-1})-\min_{\pi\in \mathbb{I}}\sum_{t\in[T]}V_{h}^{\pi}(x_{1};\bm{\mathcal{H}}^{t-1})\leq\mathrm{ poly}(A,d,H,\log(|\Phi|/\delta))\cdot T^{\nicefrac{{\epsilon}}{{5}}},\] (17)

_where \(\bm{\pi}^{t}\) is the policy that Algorithm 4 executes at episode \(t\in[T]\)._

```
1:Number of rounds \(T\), feature class \(\Phi\), loss feature \(\phi^{\mathrm{loss}}\), confidence parameter \(\delta\in(0,1)\).
2:Set \(\varepsilon\gets T^{-1/3}\), \(N_{\mathrm{reg}}\gets T^{2/3}\), \(\nu\gets N_{\mathrm{reg}}^{-1/4}\), and \(\alpha\leftarrow(8Ad)^{-1}\), \(T_{\mathrm{cov}}\leftarrow\varepsilon^{-2}Ad^{13}H^{6}\log(\Phi/\delta)\).
3:Set \(T_{\mathrm{rep}}\leftarrow\alpha^{-1}\varepsilon^{-2}AH\log(|\Phi|/\delta)\), \(T_{\mathrm{span}}\leftarrow\alpha^{-2}\varepsilon^{-2}A\log(dH|\Phi|\varepsilon ^{-1}\delta^{-1})\).
4:Define \(\mathcal{F}_{h}=\big{\{}(x,a)\mapsto\max_{a\in\mathcal{A}}\bar{\phi}_{h}(x,a)^ {\tau}\bar{\theta}\mid\bar{\phi}_{h}=[\phi_{h}^{\mathrm{loss}},\phi_{h}], \phi\in\Phi,\bar{\theta}\in\mathbb{B}_{2d}(1)\big{\}}\), \(\forall h\in[H]\).
5:Get \(\Psi_{\mathrm{Lf}}^{\mathrm{cov}}\leftarrow\texttt{Vol}(\Phi,\varepsilon, \delta/4)\).
6:Get \(\Psi_{h}^{\mathrm{rep}}\leftarrow\texttt{RepLearn}(h,\mathcal{F}_{h+1},\Phi, \texttt{unif}(\Psi_{h}^{\mathrm{cov}}),T_{\mathrm{rep}})\), for all \(h\in[H-1]\). //RepLearn as in Mhammedi et al. (2023)
7:For all \(h\in[H]\), set \(\bar{\Phi}_{h}^{\mathrm{rep}}\leftarrow[\phi_{h}^{\mathrm{loss}},\phi_{h}^{ \mathrm{rep}}]\in\mathbb{R}^{2d}\).
8:For \(h\in[H]\), set \(\Psi_{h}^{\texttt{span}}\leftarrow\texttt{Spanner}(h,\Phi,\Psi_{1:h}^{\mathrm{ cov}},\bar{\Psi}_{h}^{\mathrm{rep}},T_{\mathrm{span}})\). //Algorithm 7
9:Set \(T_{0}\gets T_{\mathrm{cov}}+T_{\mathrm{rep}}+T_{\mathrm{span}}\).
10:for\(k=1,\ldots,(T-T_{0})/N_{\mathrm{reg}}\)do
11:Define \(\widehat{\pi}_{h}^{(k)}(a\mid x)\propto\exp\left(-\eta\sum_{s<k}\widehat{Q}_{ h}^{(s)}(x,a)\right)\) for \(h\in[H]\).
12:for\(t=T_{0}+(k-1)\cdot N_{\mathrm{reg}}+1,\ \ldots,\ T_{0}+k\cdot N_{\mathrm{reg}}\)do
13:Define the random variables \(\bm{\zeta}^{t}\sim\mathrm{Ber}(\nu)\), \(\bm{h}^{t}\sim\texttt{unif}([H])\), and \(\bm{\pi}^{t}\sim\texttt{unif}(\Psi_{\bm{h}^{\texttt{span}}}^{\texttt{span}})\).
14:Set \(\widehat{\bm{\pi}}^{t}=\mathbb{I}[\bm{\zeta}^{t}=0\}\cdot\widehat{\bm{\pi}}^{( k)}+\mathbb{I}[\bm{\zeta}^{t}=1\}\cdot\bm{\pi}^{t}\circ_{\bm{h}^{t+1}} \widehat{\bm{\pi}}^{(k)})\).
15:Execute \(\widehat{\bm{\pi}}^{t}\), and observe trajectory \((\bm{x}_{1}^{t},\bm{a}_{1}^{t},\ldots,\bm{x}_{H}^{t},\bm{a}_{H}^{t})\).
16:For \(h\in[H]\), observe loss \(\bm{\ell}_{h}^{t}\coloneqq\ell_{h}(\bm{x}_{h}^{t},\bm{a}_{h}^{t};\bm{\mathcal{H }}^{t-1})\), where \(\bm{\mathcal{H}}^{t-1}\coloneqq(\bm{x}_{1:H}^{1:t-1},\bm{a}_{1:H}^{1:t-1})\).
17:endfor
18:For \(h\in[H]\) and \(\mathcal{I}^{(k)}=\{T_{0}+(k-1)\cdot N_{\mathrm{reg}}+1,\ \ldots,\ T_{0}+k\cdot N_{\mathrm{reg}}\}\), compute \(\hat{\theta}_{h}^{(k)}\) such that \[\hat{\theta}_{h}^{(k)}\leftarrow\operatorname*{argmin}_{\theta\in\mathbb{B}_{2 d}(4Hd^{2})}\sum_{\pi\in\Psi_{h}^{\texttt{sel}}}\left|\sum_{t\in\mathcal{I}^{(k)}} \mathbb{I}\{\bm{h}^{t}=h,\bm{\pi}^{t}=\pi,\bm{\zeta}^{t}=1\}\cdot\left(\bar{ \phi}_{h}^{\mathrm{rep}}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})^{\top}\theta-\sum_{s=h}^ {H}\bm{\ell}_{s}^{t}\right)\right|\] (19)
19:Set \(\widehat{Q}_{h}^{(k)}(x,a)=\bar{\phi}_{h}^{\mathrm{rep}}(x,a)^{\tau}\hat{ \theta}_{h}^{(k)}\), for all \((x,a)\in\mathcal{X}\times\mathcal{A}\).
20:endfor ```

**Algorithm 4** Oracle Efficient Algorithm for Adversarial Low-Rank MDPs (Adaptive Adversary).

## 5 Conclusion

In this paper, we focus on learning low-rank MDPs with unknown transitions and adversarial losses. For the full-information setting, we improve upon previous regret bounds. More importantly, we initiate the study of the challenging bandit feedback setting, developing various algorithms that achieve sublinear regret under different assumptions. However, the optimal \(\sqrt{T}\) regret remains out of reach due to the limitations of our two-phase design. An interesting direction for future work is to perform on-the-fly representation learning to adapt to adversarial losses and achieve optimal regret.

## References

* Abbeel and Ng (2005) Abbeel, P. and Ng, A. Y. (2005). Exploration and apprenticeship learning in reinforcement learning. In _Proceedings of the 22nd international conference on Machine learning_, pages 1-8.
* Agarwal et al. (2020) Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W. (2020). Flambe: Structural complexity and representation learning of low rank mdps. _Advances in neural information processing systems_, 33:20095-20107.
* Ayoub et al. (2020) Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L. (2020). Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_, pages 463-474. PMLR.
* Cai et al. (2020) Cai, Q., Yang, Z., Jin, C., and Wang, Z. (2020). Provably efficient exploration in policy optimization. In _International Conference on Machine Learning_, pages 1283-1294. PMLR.
* Chen et al. (2022a) Chen, F., Mei, S., and Bai, Y. (2022a). Unified algorithms for rl with decision-estimation coefficients: No-regret, pac, and reward-free learning. _arXiv preprint arXiv:2209.11745_.
* Chen et al. (2022b) Chen, J., Modi, A., Krishnamurthy, A., Jiang, N., and Agarwal, A. (2022b). On the statistical efficiency of reward-free exploration in non-linear rl. _Advances in Neural Information Processing Systems_, 35:20960-20973.
* Chen and Luo (2021) Chen, L. and Luo, H. (2021). Finding the stochastic shortest path with low regret: The adversarial cost and unknown transition case. In _International Conference on Machine Learning_, pages 1651-1660. PMLR.
* Cheng et al. (2023) Cheng, Y., Huang, R., Yang, J., and Liang, Y. (2023). Improved sample complexity for reward-free reinforcement learning under low-rank mdps. _arXiv preprint arXiv:2303.10859_.
* Dai et al. (2022) Dai, Y., Luo, H., and Chen, L. (2022). Follow-the-perturbed-leader for adversarial markov decision processes with bandit feedback. _Advances in Neural Information Processing Systems_, 35:11437-11449.
* Dai et al. (2023) Dai, Y., Luo, H., Wei, C.-Y., and Zimmert, J. (2023). Refined regret for adversarial mdps with linear function approximation. In _International Conference on Machine Learning_, pages 6726-6759. PMLR.
* Dann et al. (2023) Dann, C., Wei, C.-Y., and Zimmert, J. (2023). Best of both worlds policy optimization. In _International Conference on Machine Learning_.
* Du et al. (2021) Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W., and Wang, R. (2021). Bilinear classes: A structural framework for provable generalization in rl. In _International Conference on Machine Learning_, pages 2826-2836. PMLR.
* Foster et al. (2021) Foster, D. J., Kakade, S. M., Qian, J., and Rakhlin, A. (2021). The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_.
* Foster et al. (2022) Foster, D. J., Rakhlin, A., Sekhari, A., and Sridharan, K. (2022). On the complexity of adversarial decision making. _Advances in Neural Information Processing Systems_, 35:35404-35417.
* He et al. (2022) He, J., Zhou, D., and Gu, Q. (2022). Near-optimal policy optimization algorithms for learning adversarial linear mixture mdps. In _International Conference on Artificial Intelligence and Statistics_, pages 4259-4280. PMLR.
* Huang et al. (2023) Huang, A., Chen, J., and Jiang, N. (2023). Reinforcement learning in low-rank mdps with density features. In _International Conference on Machine Learning_, pages 13710-13752. PMLR.
* Jiang et al. (2017) Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. (2017). Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_, pages 1704-1713. PMLR.
* Jin et al. (2020a) Jin, C., Jin, T., Luo, H., Sra, S., and Yu, T. (2020a). Learning adversarial markov decision processes with bandit feedback and unknown transition. In _International Conference on Machine Learning_, pages 4860-4869. PMLR.
* Jin et al. (2020b)Jin, C., Liu, Q., and Miryoosefi, S. (2021a). Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms. _Advances in neural information processing systems_, 34:13406-13418.
* Jin et al. (2020) Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2020b). Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR.
* Jin et al. (2021b) Jin, T., Huang, L., and Luo, H. (2021b). The best of both worlds: stochastic and adversarial episodic mdps with unknown transition. _Advances in Neural Information Processing Systems_, 34:20491-20502.
* Kakade and Langford (2002) Kakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning. In _Proceedings of the Nineteenth International Conference on Machine Learning_, pages 267-274.
* Kong et al. (2023) Kong, F., Zhang, X., Wang, B., and Li, S. (2023). Improved regret bounds for linear adversarial mdps via linear optimization. _arXiv preprint arXiv:2302.06834_.
* Lattimore and Szepesvari (2020) Lattimore, T. and Szepesvari, C. (2020). _Bandit algorithms_. Cambridge University Press.
* Lee et al. (2020) Lee, C.-W., Luo, H., Wei, C.-Y., and Zhang, M. (2020). Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps. In _Advances in Neural Information Processing Systems_.
* Liu et al. (2023) Liu, H., Wei, C.-Y., and Zimmert, J. (2023). Towards optimal regret in adversarial linear mdps with bandit feedback. _arXiv preprint arXiv:2310.11550_.
* Luo et al. (2021) Luo, H., Wei, C.-Y., and Lee, C.-W. (2021). Policy optimization in adversarial mdps: Improved exploration via dilated bonuses. _Advances in Neural Information Processing Systems_, 34:22931-22942.
* Mhammedi et al. (2023) Mhammedi, Z., Block, A., Foster, D. J., and Rakhlin, A. (2023). Efficient model-free exploration in low-rank mdps. _arXiv preprint arXiv:2307.03997_.
* Mhammedi et al. (2024a) Mhammedi, Z., Block, A., Foster, D. J., and Rakhlin, A. (2024a). Efficient model-free exploration in low-rank mdps. _Advances in Neural Information Processing Systems_, 36.
* Mhammedi et al. (2024b) Mhammedi, Z., Foster, D. J., and Rakhlin, A. (2024b). The power of resets in online reinforcement learning. _arXiv preprint arXiv:2404.15417_.
* Modi et al. (2024) Modi, A., Chen, J., Krishnamurthy, A., Jiang, N., and Agarwal, A. (2024). Model-free representation learning and exploration in low-rank mdps. _Journal of Machine Learning Research_, 25(6):1-76.
* Padakandla et al. (2020) Padakandla, S., KJ, P., and Bhatnagar, S. (2020). Reinforcement learning algorithm for non-stationary environments. _Applied Intelligence_, 50(11):3590-3606.
* Ren et al. (2022) Ren, T., Zhang, T., Szepesvari, C., and Dai, B. (2022). A free lunch from the noise: Provable and practical exploration for representation learning. In _Uncertainty in Artificial Intelligence_, pages 1686-1696. PMLR.
* Rosenberg and Mansour (2019) Rosenberg, A. and Mansour, Y. (2019). Online stochastic shortest path with bandit feedback and unknown transition function. _Advances in Neural Information Processing Systems_, 32.
* Shani et al. (2020) Shani, L., Efroni, Y., Rosenberg, A., and Mannor, S. (2020). Optimistic policy optimization with bandit feedback. In _International Conference on Machine Learning_, pages 8604-8613. PMLR.
* Sherman et al. (2023a) Sherman, U., Cohen, A., Koren, T., and Mansour, Y. (2023a). Rate-optimal policy optimization for linear markov decision processes. _arXiv preprint arXiv:2308.14642_.
* Sherman et al. (2023b) Sherman, U., Koren, T., and Mansour, Y. (2023b). Improved regret for efficient online reinforcement learning with linear function approximation. In _International Conference on Machine Learning_.
* Uehara et al. (2021) Uehara, M., Zhang, X., and Sun, W. (2021). Representation learning for online and offline rl in low-rank mdps. _arXiv preprint arXiv:2110.04652_.
* Wainwright et al. (2020)Xie, T., Foster, D. J., Bai, Y., Jiang, N., and Kakade, S. M. (2022). The role of coverage in online reinforcement learning. _arXiv preprint arXiv:2210.04157_.
* Zhang et al. (2022a) Zhang, T., Ren, T., Yang, M., Gonzalez, J., Schuurmans, D., and Dai, B. (2022a). Making linear mdps practical via contrastive representation learning. In _International Conference on Machine Learning_, pages 26447-26466. PMLR.
* Zhang et al. (2022b) Zhang, X., Song, Y., Uehara, M., Wang, M., Agarwal, A., and Sun, W. (2022b). Efficient reinforcement learning in block mdps: A model-free representation learning approach. In _International Conference on Machine Learning_, pages 26517-26547. PMLR.
* Zhao et al. (2022) Zhao, C., Yang, R., Wang, B., and Li, S. (2022). Learning adversarial linear mixture markov decision processes with bandit feedback and unknown transition. In _The Eleventh International Conference on Learning Representations_.
* Zhao et al. (2024) Zhao, C., Yang, R., Wang, B., Zhang, X., and Li, S. (2024). Learning adversarial low-rank markov decision processes with unknown transition and full-information feedback. _Advances in Neural Information Processing Systems_, 36.
* Zhong et al. (2022) Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z., and Zhang, T. (2022). Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond. _arXiv preprint arXiv:2211.01962_.

## Appendix A Related Work

* [1] B Proof of Theorem 3.1 (Model-Based, Full Information)
* [2] C Proof of Theorem 3.2 (Model-Based, Bandit Feedback)
* [3] D More Details of Inefficient Model-Free Algorithm in Section 4.1
* [4] D.1 Algorithm Description
* D.2 Analysis of Occupancy Estimation from Algorithm 6
* D.3 Regret Analysis
* [5] E Proof of Theorem 4.1 (Model-Free, Banfit Feedback)
* E.1 Bounding the Regret Term
* E.2 Bounding the Bias Term
* E.3 Bounding the Regression Error
* E.4 Putting It All Together
* [6] F Proof of Theorem 4.2 (Model-Free, Bandit Feedback, Adaptive Adversary)
* F.1 Bounding the Regret Term
* F.2 Bounding the Bias Term
* F.3 Bound the Regression Error
* F.4 Putting It All Together
* F.5 Spanner Guarantee
* F.6 Representation + Spanner
* F.7 Martingal Concentration
* [7] G Policy Cover and Representation Learning Algorithms
* G.1 Policy Cover
* G.2 Representation Learning
* [8] H Lower Bound for Bandit feedback with Unstructured Losses
* [9] I Helper Results
	* I.1 Martingale Concentration and Regression Results
	* I.2 Online Learning
	* I.3 Reinforcement Learning
Related Work

Learning low-rank MDPs in the stochastic setting.In the absent of adversarial losses, several general learning frameworks have been developed for super classes of low-rank MDPs which offer tight sample complexity for either reward-based (Jiang et al., 2017; Jin et al., 2021; Du et al., 2021; Foster et al., 2021; Zhong et al., 2022) or reward-free (Chen et al., 2022, 2022; Xie et al., 2022) settings. However, these algorithms require solving non-convex optimization problems on non-convex version spaces, making them computationally inefficiency. Oracle-efficient algorithms for low-rank MDPs are first obtained by Agarwal et al. (2020) using a model-based approach, and the sample complexity bound has been largely improved in subsequent works (Uehara et al., 2021; Zhang et al., 2022; Cheng et al., 2023). The model-based approach, however, necessitates the function class to accurately model the transition, which is a strong requirement. To relax it, Modi et al. (2024); Zhang et al. (2022) developed oracle-efficient model-free algorithms, but both of them require additional assumptions on the MDP structure. Recently, Mhammedi et al. (2024) proposed a satisfactory model-free algorithm that removes all these assumptions. Our work leverages their techniques to tackle the more challenging adversarial setting.

Learning adversarial MDPs.Learning adversarial tabular MDPs under bandit feedback and unknown transition has been extensively studied (Rosenberg and Mansour, 2019; Jin et al., 2020; Lee et al., 2020; Jin et al., 2021; Shani et al., 2020; Chen and Luo, 2021; Luo et al., 2021; Dai et al., 2022; Dann et al., 2023). This line of work has demonstrated not only \(\sqrt{T}\) regret bounds but also several data-dependent bounds.

For adversarial MDPs with a large state space which necessitates the use of function approximation, if the transition is known, Foster et al. (2022) shows that adversarial setting is as easy as the stochastic setting even under general function approximation. For full-information loss feedback with unknown transition, \(\sqrt{T}\) bound is derived for both linear mixture MDPs (Cai et al., 2020; He et al., 2022) and linear MDPs (Sherman et al., 2023). For more challenging low-rank MDPs with unknown features, the best result only achieves \(T^{5/6}\) regret (Zhao et al., 2024).

For function approximation with bandit feedback and unknown transition, Zhao et al. (2022) provides \(\sqrt{T}\) bound for linear mixture MDPs, but their regret has polynomial dependence on the size of the state space due to the lack of structure on the loss function. For linear MDPs, a series of recent work has made significant progress in improving the regret bound (Luo et al., 2021; Dai et al., 2023; Sherman et al., 2023; Kong et al., 2023; Liu et al., 2023). The state-of-the-art result by Liu et al. (2023) gives an inefficient algorithm with \(\sqrt{T}\) regret and an efficient algorithm with \(T^{5/4}\) regret. These regret bounds for linear MDPs do not depend on the state space size because of the linear loss assumption. We show in Appendix H that cross-state structure on the losses is necessary for low-rank MDPs with bandit feedback to achieve regret bound that do not scale with the number of states.

Proof of Theorem 3.1 (Model-Based, Full Information)

**Theorem B.1** (Theorem 3 in Cheng et al. (2023)).: _With probability \(1-\delta\), for any policy \(\pi\) and layer \(h\), Algorithm 1 in Cheng et al. (2023) outputs transition \(\widehat{P}_{1:H}\) and features \(\hat{\phi}_{h},\hat{\mu}_{h}\) such that \(\widehat{P}_{h}(x^{\prime}\mid x,a)=\hat{\phi}_{h}(x,a)^{\top}\hat{\mu}_{h+1}(x ^{\prime})\) and_

\[\mathbb{E}^{\pi}\big{[}\|\widehat{P}_{h}\left(\cdot\mid\bm{x}_{h},\bm{a}_{h} \right)-P_{h}^{*}\left(\cdot\mid\bm{x}_{h},\bm{a}_{h}\right)\|_{1}\big{]}\leq\epsilon,\]

_if the number of collected trajectories is at least \(\mathcal{O}\left(\frac{H^{3}d^{2}|\mathcal{A}|(d^{2}+|\mathcal{A}|)}{\epsilon^ {2}}\log^{2}\left(TdH|\Phi||\Upsilon|\right)\right)\)._

Define \(\widehat{V}_{1}^{\pi}(x_{1};\ell)\) as the value function of policy \(\pi\) under transition \(\{\widehat{P}_{h}\}_{h=1}^{H}\) and loss \(\ell\). We have

\[\text{Reg}_{T}(\pi^{*}) =\sum_{t=1}^{T}{V_{1}^{\pi^{t}}(x_{1};\ell^{t})}-\sum_{t=1}^{T}{V _{1}^{\pi^{*}}(x_{1};\ell^{t})}\] \[=\underbrace{\sum_{t=1}^{T}\left({V_{1}^{\pi^{t}}(x_{1};\ell^{t}) -\widehat{V}_{1}^{\pi^{t}}(x_{1};\ell^{t})}\right)}_{\text{{Bias1}}}+\underbrace{ \sum_{t=1}^{T}\left(\widehat{V}_{1}^{\pi^{*}}(x_{1};\ell^{t})-{V_{1}^{\pi^{*}} (x_{1};\ell^{t})}\right)}_{\text{{Bias2}}}\] \[\quad+\underbrace{\sum_{t=1}^{T}\left(\widehat{V}_{1}^{\pi^{t}}(x _{1};\ell^{t})-\widehat{V}_{1}^{\pi^{*}}(x_{1};\ell^{t})\right)}_{\text{{FTRL} }}+\mathcal{O}\left(\frac{H^{3}d^{2}|\mathcal{A}|(d^{2}+|\mathcal{A}|)}{\epsilon ^{2}}\log^{2}\left(TdH|\Phi||\Upsilon|\right)\right)\]

Bounding the bias term.By Theorem B.1 and Lemma I.6, we have

\[\text{{Bias1}}+\text{{Bias2}}\leq 2H^{2}\epsilon T.\]

Bounding the FTRL term.Since \(\eta=\frac{1}{H\sqrt{T}}\), we have

\[\text{{FTRL}} \leq\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}^{\widetilde{P},\pi^{*} }\left[\left(\widetilde{Q}_{h}^{t}(\bm{x}_{h},\cdot),\pi_{h}^{t}(\cdot\mid\bm{ x}_{h})-\pi_{h}^{*}(\cdot\mid\bm{x}_{h})\right)\right]\] (Lemma I.7) \[\leq\frac{H\log|\mathcal{A}|}{\eta}+\eta\sum_{h=1}^{H}\sum_{t=1}^ {T}\mathbb{E}^{\widetilde{P},\pi^{*}o_{h}\pi^{t}}\left[\left(\widetilde{Q}_{h} ^{t}(\bm{x}_{h},\bm{a}_{h})\right)^{2}\right],\] (Lemma I.5) \[\leq\frac{H\log|\mathcal{A}|}{\eta}+2H^{3}\eta T\] ( \[\widetilde{Q}_{h}^{t}(\bm{x}_{h},\bm{a}_{h})\leq H\] ) \[=\mathcal{O}\left(H^{2}\sqrt{T}\log|\mathcal{A}|\right).\]

Thus, by setting \(\epsilon=(Hd^{2}|\mathcal{A}|(d^{2}+|\mathcal{A}|))^{\frac{1}{3}}T^{-\frac{1}{ 3}}\), we have

\[\text{Reg}_{T} \leq\mathcal{O}\left(\frac{H^{3}d^{2}|\mathcal{A}|(d^{2}+|\mathcal{ A}|)}{\epsilon^{2}}\log^{2}\left(TdH|\Phi||\Upsilon|\right)+2H^{2}\epsilon T+H^{2} \sqrt{T}\log|\mathcal{A}|\right)\] \[\leq\mathcal{O}\left(H^{3}\left(d^{2}+|\mathcal{A}|\right)T^{\frac {3}{2}}\log\left(|\mathcal{A}|+dH|\Phi||\Upsilon|T\right)\right).\]Proof of Theorem 3.2 (Model-Based, Bandit Feedback)

**Lemma C.1**.: _With \(\widehat{P}_{1:H}\) as in Theorem B.1, we have for any \(h\in[H]\) and any policy \(\pi\),_

\[\sum_{x\in\mathcal{X}}\big{|}\hat{d}_{h}^{\pi}(x)-d_{h}^{\pi}(x)\big{|}\leq\sum_ {i=1}^{h-1}\mathbb{E}^{\pi}\left[\big{\|}\widehat{P}_{i}(\cdot\mid\bm{x}_{i}, \bm{a}_{i})-P_{i}(\cdot\mid\bm{x}_{i},\bm{a}_{i})\big{\|}_{1}\right]\leq(h-1) \cdot\epsilon.\]

_where \(\hat{d}_{h}^{\pi}=d_{h}^{\overline{P},\pi}\)._

**Proof.** We prove the claim by induction. When \(h=1\), given that the \(\|d_{1}^{\pi}-\hat{d}_{1}^{\pi}\|_{1}=0\). Assume

\[\sum_{x\in\mathcal{X}}\big{|}\hat{d}_{h}^{\pi}(x)-d_{h}^{\pi}(x)\big{|}\leq\sum _{i=1}^{h-1}\mathbb{E}^{\pi}\left[\big{\|}\widehat{P}_{i}(\cdot\mid\bm{x}_{i},\bm{a}_{i})-P_{i}(\cdot\mid\bm{x}_{i},\bm{a}_{i})\big{\|}_{1}\right].\]

We have

\[\sum_{x\in\mathcal{X}_{h+1}}\big{|}\hat{d}_{h+1}^{\pi}(x)-d_{h+1}^ {\pi}(x)\big{|}\] \[=\sum_{x\in\mathcal{X}}\sum_{a\in\mathcal{A}}\sum_{x^{\prime}\in \mathcal{X}_{h+1}}\big{|}\hat{d}_{h}^{\pi}(x)\pi_{h}(a\mid x)\cdot\widehat{P}_{ h}(x^{\prime}|x,a)-d_{h}^{\pi}(x)\pi_{h}(a\mid x)\cdot P_{h}^{\star}(x^{\prime}|x,a) \big{|},\] \[\leq\sum_{x\in\mathcal{X}}\sum_{a\in\mathcal{A}}\sum_{x^{\prime} \in\mathcal{X}_{h+1}}\big{|}\hat{d}_{h}^{\pi}(x)\pi_{h}(a\mid x)\cdot\widehat{P }_{h}(x^{\prime}|x,a)-d_{h}^{\pi}(x)\pi_{h}(a\mid x)\cdot\widehat{P}_{h}(x^{ \prime}|x,a)\big{|}\] \[\leq\sum_{x\in\mathcal{X}}\sum_{a\in\mathcal{A}}\sum_{x^{\prime} \in\mathcal{X}_{h+1}}\big{|}\hat{d}_{h}^{\pi}(x)\pi_{h}(a\mid x)\cdot\widehat{P }_{h}(x^{\prime}|x,a)-d_{h}^{\pi}(x)\pi_{h}(a\mid x)\cdot P_{h}^{\star}(x^{ \prime}|x,a)\big{|},\] \[\leq\sum_{x\in\mathcal{X}}\sum_{a\in\mathcal{A}}\sum_{x^{\prime} \in\mathcal{X}_{h+1}}\widehat{P}_{h}(x^{\prime}|x,a)\pi_{h}(a\mid x)\cdot|\hat {d}_{h}^{\pi}(x)-d_{h}^{\pi}(x)|\] \[\quad+\sum_{x\in\mathcal{X}}\sum_{a\in\mathcal{A}}\sum_{x^{\prime} \in\mathcal{X}_{h+1}}d_{h}^{\pi}(x)\pi_{h}(a\mid x)\cdot|\widehat{P}_{h}(x^{ \prime}|x,a)-P_{h}(x^{\prime}|x,a)|,\] \[\leq\sum_{x\in\mathcal{X}}\big{|}\hat{d}_{h}^{\pi}(x)-d_{h}^{\pi} (x)\big{|}+\mathbb{E}^{\pi}\left[\big{\|}\widehat{P}_{h}(\cdot\mid\bm{x}_{h}, \bm{a}_{h})-P_{h}(\cdot\mid\bm{x}_{h},\bm{a}_{h})\big{\|}_{1}\right],\] \[\leq\sum_{i=1}^{h}\mathbb{E}^{\pi}\left[\big{\|}\widehat{P}_{i}( \cdot\mid\bm{x}_{i},\bm{a}_{i})-P_{i}(\cdot\mid\bm{x}_{i},\bm{a}_{i})\big{\|}_{ 1}\right],\]

where the last step follows by the induction hypothesis. The second inequality of Lemma C.1 directly comes from Theorem B.1. \(\square\)

Our candidate policy space \(\Pi^{\prime}\) has a \(\beta\) mixture of the random policy. For any deterministic policy \(\pi_{0}^{*}\), define policy \(\pi^{*}\) such that for any state \(x\in\mathcal{X}\), we have \(\pi^{*}(\cdot\mid x)=(1-\beta)\pi_{0}^{*}(\cdot\mid x)+\frac{\beta}{|\mathcal{A }|}\). We have \(\pi^{*}\in\Pi^{\prime}\). Define \(\widetilde{V}_{1}^{\pi}(x_{1};\ell)\) as the value function of policy \(\pi\) under transition \(\{\widehat{P}_{h}\}_{h=1}^{H}\) and loss \(\ell\).

For any policy \(\pi_{0}^{*}\), we have

\[\text{Reg}_{T}(\pi_{0}^{*})\] (19) \[=\mathbb{E}\left[\sum_{t=1}^{T}\sum_{\pi}p^{t}(\pi)V_{1}^{\pi}( \bm{x}_{1};\ell^{t})-V_{1}^{\pi_{0}^{*}}(\bm{x}_{1};\ell^{t})\right]+\mathcal{O} \left(\frac{H^{3}d^{2}|\mathcal{A}|(d^{2}+|\mathcal{A}|)}{\epsilon^{2}}\log^{2 }\left(TdH|\Phi||\Upsilon|\right)\right),\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\sum_{\pi}p^{t}(\pi)V_{1}^{\pi}(\bm {x}_{1};\ell^{t})-V_{1}^{\pi^{*}}(\bm{x}_{1};\ell^{t})\right]+\mathcal{O} \left(\frac{H^{3}d^{2}|\mathcal{A}|(d^{2}+|\mathcal{A}|)}{\epsilon^{2}}\log^{2 }\left(TdH|\Phi||\Upsilon|\right)\right)\] (20) \[\qquad+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\sum_{\pi}\left( \rho^{t}(\pi)-p^{t}(\pi)\right)V_{1}^{\pi}(\bm{x}_{1};\ell^{t})\right]}_{\text{ Error1}}+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}V_{1}^{\pi^{*}}(\bm{x}_{1};\ell^{t})-V_{1}^{ \pi_{0}^{*}}(\bm{x}_{1};\ell^{t})\right]}_{\text{Error2}}\] \[=\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\sum_{\pi}p^{t}(\pi)\left( V_{1}^{\pi}(\bm{x}_{1};\ell^{t})-\bar{V}_{1}^{\pi}(\bm{x}_{1};\ell^{t})\right) \right]}_{\text{Bias1}}+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\bar{V}_{1}^{ \pi^{*}}(\bm{x}_{1};\ell^{t})-V_{1}^{\pi^{*}}(\bm{x}_{1};\ell^{t})\right]}_{ \text{Bias2}}\]\[+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\sum_{\pi}p^{t}(\pi)\widehat{V _{1}^{\pi}}(\bm{x}_{1};\ell^{t})-\widehat{V_{1}^{\pi^{*}}}(\bm{x}_{1};\ell^{t}) \right]}_{\text{EXP}}+\text{Error1}+\text{Error2}\] \[+\mathcal{O}\left(\frac{H^{3}d^{2}|\mathcal{A}|(d^{2}+|\mathcal{A} |)}{\epsilon^{2}}\log^{2}\left(TdH|\Phi||\Upsilon|\right)\right).\] (21)

Recall that \(J=\text{John}(\hat{\phi}_{h}(\pi))_{\pi\in\Pi^{\prime},h\in[H]}\in\Delta(\Pi^{ \prime}\times H)\), and we have \(\rho^{t}(\pi)=(1-\gamma)p^{t}(\pi)+\gamma\sum_{h=1}^{H}J(\pi,h)\) where \(p^{t}(\pi)\) is defined in Line 7 of Algorithm 2.

**Lemma C.2**.: _We have_

\[\text{Error1}+\text{Error2}\leq H\gamma T+2H^{2}\beta T.\]

Proof.: \[\text{Error1} =\gamma\mathbb{E}\left[\sum_{t=1}^{T}\sum_{\pi}\left(\sum_{h=1} ^{H}J(\pi,h)-p^{t}(\pi)\right)\cdot V_{1}^{\pi}(\bm{x}_{1};\ell^{t})\right] \leq H\gamma T.\] \[\text{Error2} =\sum_{t=1}^{T}\sum_{h=1}^{H}\mathbb{E}^{\pi^{*}}\left[\sum_{a \in\mathcal{A}}|\pi_{0}^{*}(a\mid\bm{x}_{h})-\pi^{*}(a\mid\bm{x}_{h})|\cdot Q _{h}^{\pi_{0}^{*}}(\bm{x}_{h},a;\ell^{t})\right]\leq 2H^{2}\beta T,\]

where the last step follows by Lemma I.7. 

**Lemma C.3**.: _We have_

\[\text{{Bias1}}+\text{{Bias2}}\leq H^{2}T\epsilon.\]

Proof.: This is a direct result combing Theorem B.1 and Lemma I.6. 

For \((x_{1:H},a_{1:H})\in\mathcal{X}^{H}\times\mathcal{A}^{H}\) and \(\pi\in\Pi^{\prime}\) where \(\Pi^{\prime}\) defined in Line 5 in Algorithm 2 is the mix of a given policy class \(\Pi\) and a uniform policy, and is also the policy class we play with. Recall that the loss estimator

\[\hat{\ell}^{t}(\pi;\pi^{t},x_{1:H},a_{1:H}) \coloneqq\frac{\pi_{1}(a_{1}\mid x_{1})}{\pi_{1}^{t}(a_{1}\mid x_ {1})}\ell_{1}^{t}(x_{1},a_{1})\] \[\quad+\sum_{h=2}^{H}\hat{\phi}_{h-1}(\pi)^{\top}\left(\Sigma_{h- 1}^{t}\right)^{-1}\hat{\phi}_{h-1}(\pi^{t})\frac{\pi_{h}(a_{h}\mid x_{h})}{ \pi_{h}^{t}(a_{h}\mid x_{h})}\ell_{h}^{t}(x_{h},a_{h}).\] (22)

defined in Line 10 of Algorithm 2.

**Lemma C.4**.: _For any episode \(t\in[T]\), for any policy \(\pi\) we have_

\[\widehat{V}_{1}^{\pi}(x_{1};\ell^{t})\leq\mathbb{E}_{\bm{\pi}^{t -\rho^{t}}}\mathbb{E}^{\pi^{*}}\left[\hat{\ell}^{t}(\pi;\bm{\pi}^{t},\bm{x}_{1 :H},\bm{a}_{1:H})\right]+\sqrt{d}He\sum_{h=2}^{H}\left\|\hat{\phi}_{h-1}(\pi) \right\|_{\left(\Sigma_{h-1}^{t}\right)^{-1}},\] \[\widehat{V}_{1}^{\pi}(x_{1};\ell^{t})\geq\mathbb{E}_{\bm{\pi}^{t -\rho^{t}}}\mathbb{E}^{\pi^{*}}\left[\hat{\ell}^{t}(\pi;\bm{\pi}^{t},\bm{x}_{1 :H},\bm{a}_{1:H})\right]-\sqrt{d}H\epsilon\sum_{h=2}^{H}\left\|\hat{\phi}_{h-1 }(\pi)\right\|_{\left(\Sigma_{h-1}^{t}\right)^{-1}}.\]

Proof.: First, from the definition in Line 4 of Algorithm 2, for any \(x\in\mathcal{X}\) with \(h\geq 2\), we have

\[\hat{d}_{h}^{\pi}(x)=\sum_{x^{\prime},a^{\prime}}\hat{d}_{h-1}^{ \pi}(x^{\prime},a^{\prime})\cdot\widehat{P}_{h-1}(x\mid x^{\prime},a^{\prime} )=\hat{\phi}_{h-1}(\pi)^{\top}\hat{\mu}_{h}(x)\]

where \(\hat{\phi}_{h-1}(\pi)=\sum_{x^{\prime},a^{\prime}}\hat{d}_{h-1}^{\pi}(x^{ \prime},a^{\prime})\hat{\phi}_{h-1}(x^{\prime},a^{\prime})\).

We now prove the first inequality:

\[\widehat{V}_{1}^{\pi}(x_{1};\ell^{t})\]\[=\sum_{h=1}^{H}\sum_{x_{h}\in\mathcal{X}}\sum_{a_{h}\in\mathcal{A}} \hat{d}_{h}^{\pi}(x_{h})\pi_{h}(a_{h}\mid x_{h})\cdot\ell_{h}^{t}(x_{h},a_{h}),\] \[=\underbrace{\sum_{a_{1}\in\mathcal{A}}\pi_{1}(a_{1}\mid x_{1}) \cdot\ell_{1}^{t}(x_{1},a_{1})}_{\textbf{First}}+\underbrace{\sum_{h=2}^{H} \sum_{x_{h}\in\mathcal{X}}\sum_{a_{h}\in\mathcal{A}}\hat{\phi}_{h-1}(\pi)^{ \top}\hat{\mu}_{h}(x_{h})\pi_{h}(a_{h}\mid x_{h})\cdot\ell_{h}^{t}(x_{h},a_{h} )}_{\textbf{Remain}}\] (23)

Through importance sampling, we have

\[\textbf{First}=\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{t}}\mathbb{E}^{ \boldsymbol{\pi}^{t}}\Bigg{[}\frac{\pi_{1}(\boldsymbol{a}_{1}\mid\boldsymbol{x }_{1})}{\pi_{1}^{t}(\boldsymbol{a}_{1}\mid\boldsymbol{x}_{1})}\cdot\ell_{1}^{ t}(\boldsymbol{x}_{1},\boldsymbol{a}_{1})\Bigg{]}.\]

We now bound the remaining term in (42) Since \(\Sigma_{h-1}^{t}=\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{t}}\big{[}\hat{ \phi}_{h-1}(\boldsymbol{\pi}^{t})\hat{\phi}_{h-1}(\boldsymbol{\pi}^{t})^{\top }\big{]}\), we have

**Remain**

\[=\sum_{h=2}^{H}\sum_{x_{h}\in\mathcal{X}}\sum_{a_{h}\in\mathcal{A} }\hat{\phi}_{h-1}(\pi)^{\top}\left(\Sigma_{h-1}^{t}\right)^{-1}\mathbb{E}_{ \boldsymbol{\pi}^{t}\sim\rho^{t}}\big{[}\hat{\phi}_{h-1}(\boldsymbol{\pi}^{t} )\hat{\phi}_{h-1}(\boldsymbol{\pi}^{t})^{\top}\big{]}\hat{\mu}_{h}(x_{h})\pi_{ h}(a_{h}\mid x_{h})\ell_{h}^{t}(x_{h},a_{h}),\] \[=\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{t}}\Bigg{[}\sum_{h=2}^ {H}\sum_{x_{h}\in\mathcal{X}}\sum_{a_{h}\in\mathcal{A}}\hat{\phi}_{h-1}(\pi)^{ \top}\left(\Sigma_{h-1}^{t}\right)^{-1}\hat{\phi}_{h-1}(\boldsymbol{\pi}^{t}) \underbrace{\hat{\phi}_{h-1}(\boldsymbol{\pi}^{t})^{\top}\hat{\mu}_{h}(x_{h}) }_{\hat{d}_{h}^{\pi^{t}}(x_{h})}\pi_{h}(a_{h}\mid x_{h})\ell_{h}^{t}(x_{h},a_{ h})\Bigg{]},\] \[=\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{t}}\Bigg{[}\sum_{h=2}^ {H}\sum_{x_{h}\in\mathcal{X}}\sum_{a_{h}\in\mathcal{A}}\hat{\phi}_{h-1}(\pi)^{ \top}\left(\Sigma_{h-1}^{t}\right)^{-1}\hat{\phi}_{h-1}(\boldsymbol{\pi}^{t}) \hat{d}_{h}^{\pi^{t}}(x_{h})\pi_{h}(a_{h}\mid x_{h})\ell_{h}^{t}(x_{h},a_{h}) \Bigg{]},\] (24) \[\leq\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{t}}\Bigg{[}\sum_{h=2 }^{H}\sum_{x_{h}\in\mathcal{X}}\sum_{a_{h}\in\mathcal{A}}\hat{\phi}_{h-1}(\pi)^ {\top}\left(\Sigma_{h-1}^{t}\right)^{-1}\hat{\phi}_{h-1}(\boldsymbol{\pi}^{t}) d_{h}^{\pi^{t}}(x_{h},a_{h})\frac{\pi_{h}(a_{h}\mid x_{h})}{\boldsymbol{\pi}_{h}^{t}(a_{h} \mid x_{h})}\ell_{h}^{t}(x_{h},a_{h})\Bigg{]}\] \[\qquad+\sum_{h=2}^{H}\big{\|}\hat{\phi}_{h-1}(\pi)\big{\|}_{ \left(\Sigma_{h-1}^{t}\right)^{-1}}\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{t }}\Big{[}\big{\|}\hat{\phi}_{h-1}(\boldsymbol{\pi}^{t})\big{\|}_{\left(\Sigma_{ h-1}^{t}\right)^{-1}}\Big{]}\sum_{x_{h}\in\mathcal{X}}\Big{[}\hat{d}_{h}^{\pi^{t}}(x_{h})-d_{h}^{ \pi^{t}}(x_{h})\Big{|},\] (25) \[\leq\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{t}}\mathbb{E}^{ \boldsymbol{\pi}^{t}}\Bigg{[}\sum_{h=2}^{H}\hat{\phi}_{h-1}(\pi)^{\top}\left( \Sigma_{h-1}^{t}\right)^{-1}\hat{\phi}_{h-1}(\boldsymbol{\pi}^{t})\frac{\pi_{h }(\boldsymbol{a}_{h}\mid\boldsymbol{x}_{h})}{\boldsymbol{\pi}_{h}^{t}( \boldsymbol{a}_{h}\mid\boldsymbol{x}_{h})}\ell_{h}^{t}(\boldsymbol{x}_{h}, \boldsymbol{a}_{h})\Bigg{]}\] \[\qquad+\sqrt{d}H\epsilon\sum_{h=2}^{H}\big{\|}\hat{\phi}_{h-1}(\pi) \big{\|}_{\left(\Sigma_{h-1}^{t}\right)^{-1}}\,,\] (26)

where (25) follows by Cauchy-Schwarz, and the last inequality uses Lemma C.1. Adding up **First** and **Remain**, and using the defintion of \(\hat{\ell}\) in (22) implies the first inequality of the lemma.

The second inequality follows the same procedure except for applying Cauchy-Schwarz inequality in the opposite direction in Eq. (24). 

**Lemma C.5**.: _If \(\eta\leq\left(\frac{2|\mathcal{A}|Hd}{\beta\gamma}+dH^{2}\frac{\epsilon}{\sqrt{ \gamma}}\right)^{-1}\), then_

\[\textbf{EXP}\leq\frac{\log|\Pi|}{\eta}+\frac{6dH^{2}|\mathcal{A}|T}{\beta}+4 \eta d^{2}H^{4}\epsilon^{2}T+4dH^{2}\epsilon T,\]

_where **EXP** is as in (21)._

**Proof.** Recall in Line 10 of Algorithm 2, \(b^{t}(\pi)\coloneqq\sqrt{d}H\epsilon\sum_{h=2}^{H}\big{\|}\hat{\phi}_{h-1}(\pi) \big{\|}_{\left(\Sigma_{h-1}^{t}\right)^{-1}}\), for all \(\pi\in\Pi^{\prime}\). By Lemma C.4, we have

\[\textbf{EXP}=\mathbb{E}\left[\sum_{t=1}^{T}\sum_{\pi}p^{t}(\pi)\widehat{V}_{1}^{ \pi}(\boldsymbol{x}_{1};\ell^{t})-\widehat{V}_{1}^{\pi^{*}}(\boldsymbol{x}_{1 };\ell^{t})\right]\]\[\leq\frac{|\mathcal{A}|}{\beta}+\frac{|\mathcal{A}|}{\beta\gamma}\sum _{h=2}^{H}\left|\hat{\phi}_{h-1}(\pi)\right|_{\sum_{j}^{-1}}\left\|\hat{\phi}_{h -1}(\pi^{t})\right\|_{\sum_{j}^{-1}},\] \[\leq\frac{|\mathcal{A}|}{\beta}+\frac{|\mathcal{A}|}{\beta\gamma},\] \[\leq\frac{2|\mathcal{A}|Hd}{\beta\gamma},\]

and\[\left|b^{t}(\pi)\right|=\sqrt{dH}\epsilon\sum_{h=2}^{H}\left\|\hat{\phi}_{h-1}( \pi)\right\|_{\left(\Sigma_{h-1}^{t}\right)^{-1}}\bigg{|}\leq dH^{2}\frac{ \epsilon}{\sqrt{\gamma}}.\]

To ensure \(\eta\left|\hat{\ell}^{t}(\pi;\pi^{t},x_{1:H},a_{1:H})-b^{t}(\pi)\right|\leq 1\), it suffices to set \(\eta\leq\left(\frac{2|\mathcal{A}|Hd}{\beta\gamma}+dH^{2}\frac{\epsilon}{ \sqrt{\gamma}}\right)^{-1}\). Under this constraint, from Lemma 1.5, we have

\[\textbf{FTRL}\leq\frac{\log|\Pi|}{\eta}+\underbrace{2\eta\sum_{t=1}^{T}\sum_{ \pi\in\Pi^{\prime}}p^{t}(\pi)\cdot\mathbb{E}_{\pi^{t-\rho^{t}}}\mathbb{E}^{ \pi^{t}}\left[\hat{\ell}^{t}(\pi;\pi^{t},\bm{x}_{1:H},\bm{a}_{1:H})^{2}\right]}_ {\textbf{Stability-1}}\]

For any \(t\in[T]\), we have

\[\sum_{\pi\in\Pi^{\prime}}p^{t}(\pi)\cdot\mathbb{E}_{\pi^{t-\rho^ {t}}}\mathbb{E}^{\pi^{t}}\left[\hat{\ell}^{t}(\pi;\pi^{t},\bm{x}_{1:H},\bm{a}_ {1:H})^{2}\right]\] (29) \[\leq H\mathbb{E}_{\pi^{t-\rho^{t}}}\mathbb{E}^{\pi^{t}}\left[ \frac{\pi_{1}(\bm{a}_{1}\mid\bm{x}_{1})^{2}}{\bm{\pi}_{1}^{t}(\bm{a}_{1}\mid \bm{x}_{1})^{2}}\ell_{1}^{t}(\bm{x}_{1},\bm{a}_{1})^{2}\right]\] \[+H\sum_{h=2}^{H}\mathbb{E}_{\pi^{t-\rho^{t}}}\mathbb{E}^{\pi^{t}} \left[\sum_{\pi\in\Pi^{\prime}}p^{t}(\pi)\hat{\phi}_{h-1}(\pi)^{\top}\left( \Sigma_{h-1}^{t}\right)^{-1}\hat{\phi}_{h-1}(\pi^{t})\hat{\phi}_{h-1}(\pi^{t}) ^{\top}\left(\Sigma_{h-1}^{t}\right)^{-1}\hat{\phi}_{h-1}(\pi)\frac{\pi_{h}( \bm{a}_{h}\mid\bm{x}_{h})^{2}}{\bm{\pi}_{h}^{t}(\bm{a}_{h}\mid\bm{x}_{h})^{2} }\ell_{h}^{t}(\bm{x}_{h},\bm{a}_{h})^{2}\right]\!,\] \[\leq\frac{H|\mathcal{A}|}{\beta}+2H\sum_{h=2}^{H}\mathbb{E}_{\pi^ {t-\rho^{t}}}\mathbb{E}^{\pi^{t}}\left[\text{Tr}\left(\hat{\phi}_{h-1}(\pi^{t })\hat{\phi}_{h-1}(\pi^{t})^{\top}\left(\Sigma_{h-1}^{t}\right)^{-1}\right) \right]\sum_{a\in\mathcal{A}}\frac{\pi_{h}(a\mid\bm{x}_{h})^{2}}{\bm{\pi}_{h}^ {t}(a\mid\bm{x}_{h})}\ell_{h}^{t}(\bm{x}_{h},a)^{2}\right]\!,\] \[\leq\frac{H|\mathcal{A}|}{\beta}+\frac{2H|\mathcal{A}|}{\beta}\sum _{h=2}^{H}\mathbb{E}_{\pi^{t-\rho^{t}}}\left[\text{Tr}\left(\hat{\phi}_{h-1}( \pi^{t})\hat{\phi}_{h-1}(\pi^{t})^{\top}\left(\Sigma_{h-1}^{t}\right)^{-1} \right)\right]\!,\] \[\leq\frac{3dH^{2}|\mathcal{A}|}{\beta}.\]

where (29) follows by the fact that \(p^{t}(\pi)\leq\frac{1}{1-\gamma}\rho^{t}(\pi)\) and \(\frac{1}{1-\gamma}\leq 2\). Thus,

\[\textbf{Stability-1}=2\eta\sum_{\pi\in\Pi^{\prime}}p^{t}(\pi)\cdot\mathbb{E}_{ \pi^{t-\rho^{t}}}\mathbb{E}^{\pi^{t}}\left[\hat{\ell}^{t}(\pi;\pi^{t},\bm{x}_{ 1:H},\bm{a}_{1:H})^{2}\right]\leq\frac{6d\eta H^{2}|\mathcal{A}|T}{\beta}.\]

Moreover,

\[\textbf{Stability-2} =2\eta\mathbb{E}\left[\sum_{t=1}^{T}\sum_{\pi\in\Pi^{\prime}}p^{ t}(\pi)b^{t}(\pi)^{2}\right]\!,\] \[=2\eta dH^{3}\epsilon^{2}\mathbb{E}\left[\sum_{t=1}^{T}\sum_{h=2} ^{H}\sum_{\pi\in\Pi^{\prime}}p^{t}(\pi)\left\|\hat{\phi}_{h-1}(\pi)\right\|_{ \left(\Sigma_{h-1}^{t}\right)^{-1}}^{2}\right]\!,\] \[\leq 4\eta dH^{3}\epsilon^{2}\mathbb{E}\left[\sum_{t=1}^{T}\sum_{h=1 }^{H-1}\sum_{\pi\in\Pi^{\prime}}\rho^{t}(\pi)\left\|\hat{\phi}_{h-1}(\pi) \right\|_{\left(\Sigma_{h-1}^{t}\right)^{-1}}^{2}\right]\!,\] \[\leq 4\eta d^{2}H^{4}\epsilon^{2}T.\]

Combing Lemma C.2, Lemma C.3 and Lemma C.5, if \(\eta\leq\left(\frac{2|\mathcal{A}|Hd}{\beta\gamma}+dH^{2}\frac{\epsilon}{\sqrt{ \gamma}}\right)^{-1}\), we have

\[\text{Reg}_{T}(\pi_{0}^{*})\leq H\gamma T+2H^{2}\beta T+H^{2}T\epsilon+\frac{ \log|\Pi|}{\eta}+\frac{6d\eta H^{2}|\mathcal{A}|T}{\beta}\]\[+4\eta d^{2}H^{4}\epsilon^{2}T+4dH^{2}\epsilon T+\mathcal{O}\left(\frac{H^{3}d^{2}| \mathcal{A}|(d^{2}+|\mathcal{A}|)}{\epsilon^{2}}\log^{2}\left(TdH|\Phi||\Upsilon |\right)\right).\]

By setting \(\epsilon=T^{-\frac{1}{3}}\), \(\gamma=T^{-\frac{1}{3}}\), \(\beta=T^{-\frac{1}{3}}\), \(\eta=\frac{1}{4dH|\mathcal{A}|}T^{-\frac{2}{3}}\), we have for any \(\pi_{0}^{*}\in\Pi\),

\[\text{Reg}_{T}(\pi_{0}^{*})\leq\mathcal{O}\left(d^{2}H^{3}|\mathcal{A}|(d^{2}+ |\mathcal{A}|)T^{\frac{2}{3}}\log|\Pi|\log^{2}\left(TdH|\Phi||\Upsilon|\right) \right).\]More Details of Inefficient Model-Free Algorithm in Section 4.1

### Algorithm Description

In this section, we give a more detailed introduction of the algorithm mentioned in Section 4.1. This algorithm is model-free and achieves \(T^{\frac{2}{3}}\) regret, but it is computationally inefficient. We consider the low-rank MDPs with linear losses that satisfies Assumption 2.1 and Assumption 2.2.

Let \(\mathcal{C}\left(S,\epsilon^{\prime}\right)\) be \(\epsilon^{\prime}\)-net of space \(S\). We define necessary policy and function classes in Definition D.1.

**Definition D.1**.: _We define linear policy class and its discretization as_

\[\Pi_{\mathrm{lin}}=\left\{\pi:\mathcal{X}\to\Delta(\mathcal{A}) \;\middle|\;\pi_{h}(a\;|\;x)=\mathbb{I}\!\left\{a=\operatorname*{argmin}_{a \epsilon\mathcal{A}}\phi_{h}(x,a)^{\top}\theta_{h}\right\},\;h\in[H],\;\theta_ {h}\in\mathbb{B}_{d}\left(\sqrt{d}HT\right),\;\phi\in\Phi\right\}.\] \[\Pi_{\mathrm{lin}}^{\mathrm{cov}}(\epsilon^{\prime})=\left\{\pi: \mathcal{X}\to\Delta(\mathcal{A})\;\middle|\;\pi_{h}(a\;|\;x)=\mathbb{I}\! \left\{a=\operatorname*{argmin}_{a\epsilon\mathcal{A}}\phi_{h}(x,a)^{\top} \theta_{h}\right\},\;h\in[H],\;\theta_{h}\in\mathcal{C}\left(\mathbb{B}_{d} \left(\sqrt{d}HT\right),\epsilon^{\prime}\right),\;\phi\in\Phi\right\}.\]

_Define corresponding function class as follows_

\[\mathcal{F}^{\pi}=\left\{f:\mathcal{X}\to[-1,1]\;\middle|\;f(x)= \sum_{a}\pi(a|x)\phi(x,a)^{\top}\theta,\;\text{for}\;\;\theta\in\mathbb{B}^{d} (\sqrt{d})\;\text{and}\;\;\phi\in\Phi\right\}\] \[\mathcal{F}=\left\{f:\mathcal{X}\to[-1,1]\;\middle|\;f\in\bigcup _{\pi\in\Pi_{\mathrm{lin}}}\mathcal{F}^{\pi}\right\}.\]

Our main algorithm is given in Algorithm 5, which shares the same structure as Algorithm 2, but with a different initial phase to learn expected feature estimator \(\hat{\phi}_{h}(\pi)=\sum_{(x,a)\in\mathcal{X}\times\mathcal{A}}\hat{d}_{h}^{ \pi}(x)\pi(a|x)\hat{\phi}_{h}(x,a)\) to approximate \(\phi_{h}^{*}(\pi)=\sum_{(x,a)\in\mathcal{X}\times\mathcal{A}}\hat{d}_{h}^{\pi }(x)\pi(a|x)\phi_{h}^{*}(x,a)\) for every \(h\in[H]\). In Algorithm 2, under Assumption 3.1, it is feasible to use established model-based approach to learn an accurate estimated transition \(\widetilde{P}\) together with its feature \(\hat{\phi}\). The occupancy estimator \(\hat{d}_{1:H}^{\pi}\) is induced by \(\widetilde{P}\) which also enjoy small errors. However, when we move to model-free settings with Assumption 4.1, there is no existing approach that could guarantee a good estimation for \(\hat{d}_{h}^{\pi}(x)\) and \(\hat{\phi}\).

To tackle this challenge, we first call VoX (Mhammedi et al., 2023) to construct a policy cover \(\Psi_{1:H}^{\mathrm{cov}}\), and then play every policy in \(\Psi_{1:H}^{\mathrm{cov}}\) for \(n\) episodes to collect data. Subsequently, these data are fed into Algorithm 6 to jointly solve estimated occupancy \(\hat{d}_{1:H}^{\pi}\) and feature \(\hat{\phi}_{1:H}\). Algorithm 6 is similar to (Liu et al., 2023, Algorithm 1), which is used to estimate occupancy on the fly for linear MDP. In Algorithm 6, given a target policy \(\pi\), we jointly solve \(\hat{\phi}_{1:H}\in\Phi\), \(\hat{d}_{1:H}^{\pi}\in[0,1]^{|\mathcal{X}|}\), and \((\hat{\xi}_{1:H,f})_{f\in\mathcal{F}^{\pi}}\subset\mathbb{B}^{d}\left(\sqrt{ d}\right)\) that satisfies four constrains, where \(\hat{\xi}_{h,f}\) is the estimation of \(\xi_{h,f}^{*}:=\sum_{x^{\prime}\in\mathcal{X}}\mu_{h+1}^{*}(x^{\prime})f(x^{ \prime})\). The first constraint Eq. (30) ensures the estimated occupancy \(\hat{d}_{1:H}^{\pi}\) are valid distributions. The second constraint Eq. (31) enforces the estimated values to follow the dynamic programming relationship between the occupancy of layer \(h\) and layer \(h+1\), which helps to control the propagation of estimation errors across layers through the bias of \(\hat{\phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f}\). The third constrain Eq. (32) and fourth constrain Eq. (33) are then used to bound the estimated bias of \(\hat{\phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f}\) by utilizing the data collected from policies in policy covers \(\Psi_{1:H}^{\mathrm{cov}}\). Note that applying Eq. (33) requires access to the whole state space, which is an additional assumption not needed in previous algorithms. The grantee of Algorithm 5 is given in Theorem D.1, where the \(\widetilde{O}\) hides the logarithmic dependence on \(d,H,|\mathcal{A}|,T\).

**Theorem D.1**.: _Algorithm 5 ensures \(\mathrm{Reg}_{T}\preceq\widetilde{O}\left(d^{8}H^{6}|\mathcal{A}|T^{\frac{2}{3} }\log(|\Phi|)\right)\)._```
0: Policy class \(\Pi=\Pi^{\mathrm{cov}}_{\mathrm{lin}}(\frac{1}{T})\).
1: Set \(\epsilon=18^{-1}d^{\frac{5}{2}}T^{-\frac{1}{3}}\), \(\gamma=T^{-\frac{1}{3}}\), \(\beta=T^{-\frac{1}{3}}\), \(\eta=(4Hd[\mathcal{A}])^{-1}T^{-\frac{2}{3}}\), \(n=11250d^{\frac{5}{2}}|\mathcal{A}|T^{\frac{2}{3}}\log\frac{3dnHT|\Phi|}{\delta}\) and \(T_{0}=\widetilde{\mathcal{O}}\left(\epsilon^{-2}|\mathcal{A}|d^{13}H^{6}\log (|\Phi|/\delta)\right)\).
2: Get \(\Psi^{\mathrm{cov}}_{1:H}\leftarrow\mathtt{Vox}(\Phi,\varepsilon,\delta)\) using \(T_{0}\) episodes.
3: For every policy \(\pi^{\prime}\in\Psi^{\mathrm{cov}}_{1:H}\), play it for \(n\) episodes and get the data set \(\left(\mathcal{D}^{\pi^{\prime}}_{h}\right)_{h\in[H]}\) where \(\mathcal{D}^{\pi^{\prime}}_{h}\) consists of tuples \((x,a,x^{\prime})\) such that \((x,a)\sim d^{\pi^{\prime}}_{h}\) and \(x^{\prime}\sim P^{*}(\cdot\mid x,a)\).
4: Define the policy space \(\Pi^{\prime}=\{\pi^{\prime}:\ \exists\pi\in\Pi,\ \ \pi^{\prime}_{h}(\cdot\mid x)=(1- \beta)\pi_{h}(\cdot\mid x)+\beta/|\mathcal{A}|,\ \ \forall x,h\}\).
5: Get \(\hat{\phi}_{h}(\cdot)\leftarrow\) EOM-PC\(\left(\Pi^{\prime},\left(\mathcal{D}^{\pi^{\prime}}_{h}\right)_{h\in[H],\pi^{\prime} \in\Psi^{\mathrm{cov}}_{1:H}}\right)\) from Algorithm 6.
6:for\(t=T_{0}+1\), \(T_{0}+2,\ldots,T\)do
7: Define \(p^{t}(\pi)\propto\exp\left(-\eta\sum_{i=1}^{t-1}\left(\hat{\ell}^{i}(\pi)-b^{ i}(\pi)\right)\right),\) for all \(\pi\in\Pi^{\prime}\).
8: Let \(\rho^{t}(\pi)=(1-\gamma)p^{t}(\pi)+\frac{\gamma}{H-1}\sum_{h=1}^{H-1}J_{h}\), where \(J_{h}=\mathtt{John}(\hat{\phi}_{h}(\cdot),\Pi^{\prime})\). // John as in SS2
9: Execute policy \(\pi^{t}\sim\rho^{t}\) and observe trajectory \((\bm{x}^{t}_{1:H},\bm{a}^{t}_{1:H})\) and losses \(\bm{\ell}^{t}_{h}=\ell^{t}_{h}(\bm{x}^{t}_{h},\bm{a}^{t}_{h})\).
10: Define \(\Sigma^{t}_{h}=\sum_{\pi\in\Pi^{\prime}}\rho^{t}(\pi)\cdot\hat{\phi}_{h}(\pi) \hat{\phi}_{h}(\pi)^{\top}\), \(b^{t}(\pi)=d^{\frac{1}{2}}HT^{-\frac{1}{3}}\cdot\sum_{h=1}^{H-1}\|\hat{\phi}_{h }(\pi)\|_{(\Sigma^{t}_{h})^{-1}}\), and \[\hat{\ell}^{t}(\pi)=\frac{\pi_{1}(\bm{a}^{t}_{1}\mid\bm{x}^{t}_{1})}{\bm{\pi}^ {t}_{1}(\bm{a}^{t}_{1}\mid\bm{x}^{t}_{1})}\bm{\ell}^{t}_{1}+\sum_{h=2}^{H}\hat {\phi}_{h-1}(\pi)^{\top}\left(\Sigma^{t}_{h-1}\right)^{-1}\hat{\phi}_{h-1}(\bm{ \pi}^{t})\frac{\pi_{h}(\bm{a}^{t}_{h}\mid\bm{x}^{t}_{h})}{\bm{\pi}^{t}_{h}(\bm {a}^{t}_{h}\mid\bm{x}^{t}_{h})}\bm{\ell}^{t}_{h}.\]
11:endfor ```

**Algorithm 6** EOM-PC\(\left(\Pi,\left(\mathcal{D}^{\pi^{\prime}}_{h}\right)_{h\in[H],\pi^{\prime}\in\Psi^{ \mathrm{cov}}_{1:H}}\right)\) (Estimate Occupancy Measure with Policy Cover)

```
0: The policy class \(\Pi\), datasets \(\left(\mathcal{D}^{\pi^{\prime}}_{h}\right)_{h\in[H]}\) for every \(\pi^{\prime}\in\Psi^{\mathrm{cov}}_{1:H}\)
0: Jointly find \(\hat{\phi}_{h}\in\Phi\), \((\hat{d}^{\pi}_{h})_{\pi\in\Pi}\in[0,1]^{|\mathcal{X}|}\), and \((\hat{\xi}_{h,f})_{f\in\mathcal{F}}\in\mathbb{B}^{d}\left(\sqrt{d}\right)\) for any \(h\in[H]\) such that for all \(\pi\in\Pi\), \[\sum_{x\in\mathcal{X}}\hat{d}^{\pi}_{h}(x)=1,\qquad\forall h\in[H]\] (30) \[\sum_{x^{\prime}\in\mathcal{X}}\hat{d}^{\pi}_{h+1}(x^{\prime})f(x^ {\prime})=\sum_{x\in\mathcal{X}}\sum_{a\in\mathcal{A}}\hat{d}^{\pi}_{h}(x)\pi(a |x)\hat{\phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f},\qquad\forall f\in\mathcal{F},h\in[H]\] (31) \[\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left(f(x^ {\prime})-\hat{\phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f}\right)^{2}-\min_{(\phi, \xi)\in\Phi\times\mathbb{B}_{\delta}(\sqrt{d})}\sum_{x,a,x^{\prime}\in\mathcal{ D}^{\pi^{\prime}}_{h}}\left(f(x^{\prime})-\phi(x,a)^{\top}\xi\right)^{2}\] \[\leq 132d^{\frac{1}{3}}\log(3dnHT|\Phi|/\delta),\qquad\forall\pi^{ \prime}\in\Psi^{\mathrm{cov}}_{1:H},\ f\in\mathcal{F},h\in[H]\] (32) \[\max_{x,a}\left|\hat{\phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f}\right| \leq 1\qquad\forall f\in\mathcal{F},h\in[H]\] (33)
0:\(\hat{\phi}_{h}:\Pi\rightarrow\mathbb{R}^{d},\hat{\phi}_{h}(\pi)=\sum_{(x,a) \in\mathcal{X}\times\mathcal{A}}\hat{d}^{\pi}_{h}(x)\pi(a|x)\hat{\phi}_{h}(x,a)\), \(\forall h\in[H]\). ```

**Algorithm 7** EOM-PC\(\left(\Pi,\left(\mathcal{D}^{\pi^{\prime}}_{h}\right)_{h\in[H],\pi^{\prime}\in\Psi^{ \mathrm{cov}}_{1:H}}\right)\) (Estimate Occupancy Measure with Policy Cover)

### Analysis of Occupancy Estimation from Algorithm 6

**Lemma D.1**.: _With probability \(1-\delta\), \(\phi^{*}_{1:H},(d^{\pi}_{1:H})_{\pi\in\Pi^{\prime}}\), and \(\xi^{*}_{h,f}:=\sum_{x^{\prime}\in\mathcal{X}}\mu^{*}_{h+1}(x^{\prime})f(x^{ \prime}),\forall f\in\mathcal{F},\forall h\in[H]\) is a solution to Algorithm 6._

**Proof.** Since for any policy \(\pi\) and any \(h\in[H]\), \(\sum_{x\in\mathcal{X}}d_{h}^{\pi}(x)=1\), Eq. (30) holds. For any policy \(\pi\), any \(f\in\mathcal{F}^{\pi}\) and any \(h\in[H]\), we have

\[\sum_{x^{\prime}\in\mathcal{X}}d_{h+1}^{\pi}(x^{\prime})f(x^{\prime}) =\sum_{x^{\prime}\in\mathcal{X}}\sum_{x\in\mathcal{X}}\sum_{a\in \mathcal{A}}d_{h}^{\pi}(x)\pi(a|x)P_{h}^{\star}(x^{\prime}\mid x,a)f(x^{\prime})\] \[=\sum_{x\in\mathcal{X}_{h}}\sum_{a\in\mathcal{A}}d_{h}^{\pi}(x) \pi(a|x)\phi_{h}^{\star}(x,a)^{\top}\sum_{x^{\prime}\in\mathcal{X}_{h+1}}\mu_{h +1}^{\star}(x^{\prime})f(x^{\prime})\] \[=\sum_{x\in\mathcal{X}_{h}}\sum_{a\in\mathcal{A}}d_{h}^{\pi}(x) \pi(a|x)\xi_{h,f}^{\star}.\]

Thus, Eq. (31) holds. From Exercise 27.6 of Lattimore and Szepesvari (2020), the \(\epsilon\)-net of \(\mathbb{B}_{d}(R)\) is \(\left(\frac{3R}{\epsilon}\right)^{d}\). Thus. \(|\Pi^{\prime}|=\left|\Pi_{\text{in}}^{\text{cov}}(\frac{1}{T})\right|=|\Phi| \left(3\sqrt{d}HT^{2}\right)^{d}\). We also have \(\left|\mathcal{C}\left(\mathbb{B}_{d}(\sqrt{d}),\frac{1}{T}\right)\right|= \left(3\sqrt{d}T\right)^{d}\) and for any policy \(\pi\), \(\left|\mathcal{C}\left(\mathcal{F}^{\pi},\frac{1}{T}\right)\right|=|\Phi| \left(3\sqrt{d}T\right)^{d}\). To consider all possible instances, define

\[\mathcal{N}_{T}:=|\Pi^{\prime}|\left|\mathcal{C}\left(\mathbb{B}_{d}(\sqrt{d} ),\epsilon\right)\right||\mathcal{C}\left(\mathcal{F}^{\pi},\epsilon\right)| |\Psi_{1:H}^{\text{cov}}||\Phi|H\leq dH^{2}|\Phi|^{3}\left(3\sqrt{d}HT^{2} \right)^{3d}\]

Thus, by union bound, with probability of \(1-\delta\), for every \(\pi\in\Pi^{\prime}\), every \(\pi^{\prime}\in\Psi_{1:H}^{\text{cov}}\), every \(f\in\mathcal{C}\left(\mathcal{F}^{\pi},\epsilon\right)\), every \(\xi\in\mathcal{C}\left(\mathbb{B}_{d}(\sqrt{d}),\epsilon\right)\), every \(\phi\in\Phi\) and every \(h\in[H]\), we have

\[\sum_{x,a,x^{\prime}\in\mathcal{D}_{h}^{\pi^{\prime}}}\left(f(x^{ \prime})-\phi_{h}^{\star}(x,a)^{\top}\xi_{h,f}^{\star}\right)^{2}-\sum_{x,a,x^{ \prime}\in\mathcal{D}_{h}^{\pi^{\prime}}}\left(f(x^{\prime})-\phi(x,a)^{\top} \xi\right)^{2}\] \[=-2\sum_{x,a,x^{\prime}\in\mathcal{D}_{h}^{\pi^{\prime}}}\left(f( x^{\prime})-\phi_{h}^{\star}(x,a)^{\top}\xi_{h,f}^{\star}\right)\left(\phi_{h}^{ \star}(x,a)^{\top}\xi_{h,f}^{\star}-\phi(x,a)^{\top}\xi\right)\] \[\qquad-\sum_{x,a,x^{\prime}\in\mathcal{D}_{h}^{\pi^{\prime}}} \left(\phi_{h}^{\star}(x,a)^{\top}\xi_{h,f}^{\star}-\phi(x,a)^{\top}\xi\right) ^{2}\] \[=-2\sum_{x,a,x^{\prime}\in\mathcal{D}_{h}^{\pi^{\prime}}}\left(f(x^ {\prime})-\mathbb{E}_{x^{\prime}\cap P^{\star}(\{x,a\})}[f(x^{\prime})]\right) \left(\phi_{h}^{\star}(x,a)^{\top}\xi_{h,f}^{\star}-\phi(x,a)^{\top}\xi\right)\] \[\qquad-\sum_{x,a,x^{\prime}\in\mathcal{D}_{h}^{\pi^{\prime}}} \left(\phi_{h}^{\star}(x,a)^{\top}\xi_{h,f}^{\star}-\phi(x,a)^{\top}\xi\right) ^{2}\] \[\leq 8\sqrt{\sum_{x,a,x^{\prime}\in\mathcal{D}_{h}^{\pi^{\prime}}} \left(\phi_{h}^{\star}(x,a)^{\top}\xi_{h,f}^{\star}-\phi(x,a)^{\top}\xi\right) ^{2}\log\frac{|\mathcal{N}_{T}|}{\delta}}+4\sqrt{d}\log\frac{|\mathcal{N}_{T} |}{\delta}\] \[\qquad-\sum_{x,a,x^{\prime}\in\mathcal{D}_{h}^{\pi^{\prime}}} \left(\phi_{h}^{\star}(x,a)^{\top}\xi_{h,f}^{\star}-\phi(x,a)^{\top}\xi\right) ^{2}\] (Freedman's Inequality) \[\leq 20\sqrt{d}\log\frac{|\mathcal{N}_{T}|}{\delta}\] (AM-GM) \[\leq 120d^{\frac{3}{2}}\log\frac{3dHT|\Phi|}{\delta}\]

Bounding the distance through \(\frac{1}{T}\)-net, we have with probability of \(1-\delta\), for every \(\pi\in\Pi^{\prime}\), every \(\pi^{\prime}\in\Psi_{1:H}^{\text{cov}}\), every \(f\in\mathcal{F}^{\pi}\), every \(\xi\in\mathbb{B}_{d}(\sqrt{d})\), every \(\phi\in\Phi\) and every \(h\in[H]\),

\[\sum_{x,a,x^{\prime}\in\mathcal{D}_{h}^{\pi^{\prime}}}\left(f(x^{ \prime})-\phi_{h}^{\star}(x,a)^{\top}\xi_{h,f}^{\star}\right)^{2}-\sum_{x,a,x^{ \prime}\in\mathcal{D}_{h}^{\pi^{\prime}}}\left(f(x^{\prime})-\phi(x,a)^{\top} \xi\right)^{2} \leq 120d^{\frac{3}{2}}\log\frac{3dHT|\Phi|}{\delta\epsilon}+\frac{12 \sqrt{d}n}{T}\] \[\leq 132d^{\frac{3}{2}}\log\frac{3dnHT|\Phi|}{\delta}\]

Thus, Eq. (32) also holds. Finally, for all \(x,a\), we have

\[\left|\phi_{h}^{\star}(x,a)^{\top}\xi_{h,f}^{\star}\right|=\left|\sum_{x^{ \prime}\in\mathcal{X}}\phi_{h}^{\star}(x,a)^{\top}\mu_{h+1}^{\star}(x^{\prime})f( x^{\prime})\right|=\left|\sum_{x^{\prime}\in\mathcal{X}}P^{\star}\left(x^{\prime} \mid\ x,a\right)f(x^{\prime})\right|\leq 1.\]

Thus, \(\phi_{h}^{\star},d_{h}^{\pi}\), and \(\xi_{h,f}^{\star}:=\sum_{x^{\prime}\in\mathcal{X}}\mu_{h+1}^{\star}(x^{\prime})f( x^{\prime})\), \(\forall f\in\mathcal{F}^{\pi}\), \(h\in[H]\) satisfy all Eq. (30) - Eq. (33) and is a solution to Algorithm 6.

**Lemma D.2**.: _With probability \(1-\delta\), for all \(f\in\mathcal{F}\), any solution \(\hat{d}^{\pi}\) from Algorithm 6 for any \(\pi\in\Pi^{\prime}\) satisfies_

\[\left|\sum_{x}(\hat{d}^{\pi}_{h}(x)-d^{\pi}_{h}(x))f(x)\right|\leq d ^{5}HT^{-\frac{1}{3}}\]

**Proof.**

For every solution \(\hat{\phi}_{1:H},\hat{\xi}_{1:H,f\in\mathcal{F}^{\pi^{\prime}}},\left(\hat{d}^ {\pi}_{1:H}\right)_{\pi\in\Pi^{\prime}}\) of Algorithm 6, we have

\[\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left(f(x^{ \prime})-\hat{\phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f}\right)^{2}-\sum_{x,a,x^{ \prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left(f(x^{\prime})-\phi^{*}_{h}(x,a )^{\top}\xi^{*}_{h,f}\right)^{2}\] \[=\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left(f(x ^{\prime})-\hat{\phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f}\right)^{2}-\min_{\{\phi, \xi\}}\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left(f(x^{\prime })-\phi(x,a)^{\top}\xi\right)^{2}\] \[\qquad+\min_{\{\phi,\xi\}}\sum_{x,a,x^{\prime}\in\mathcal{D}^{ \pi^{\prime}}_{h}}\left(f(x^{\prime})-\phi(x,a)^{\top}\xi\right)^{2}-\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left(f(x^{\prime})-\phi^{*}_{h}( x,a)^{\top}\xi^{*}_{h,f}\right)^{2}\] \[\leq 132d^{\frac{3}{2}}\log\frac{3dnHT|\Phi|}{\delta}\] (34)

where the last step comes from the constrain Eq. (32). On the other hand

\[2\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left(f(x ^{\prime})-\hat{\phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f}\right)^{2}-2\sum_{x,a,x^{ \prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left(f(x^{\prime})-\phi^{*}_{h}(x,a )^{\top}\xi^{*}_{h,f}\right)^{2}\] \[=4\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left(f(x ^{\prime})-\phi^{*}_{h}(x,a)^{\top}\xi^{*}_{h,f}\right)\left(\phi^{*}_{h}(x,a )^{\top}\xi^{*}_{h,f}-\hat{\phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f}\right)\] \[\qquad\qquad+2\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{ h}}\left(\phi^{*}_{h}(x,a)^{\top}\xi^{*}_{h,f}-\hat{\phi}_{h}(x,a)^{\top}\hat{ \xi}_{h,f}\right)^{2}\] \[=4\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left(f(x ^{\prime})-\mathbb{E}_{x^{\prime}\sim P^{*}(\cdot|x,a)}[f(x^{\prime})]\right) \left(\phi^{*}_{h}(x,a)^{\top}\xi^{*}_{h,f}-\hat{\phi}_{h}(x,a)^{\top}\hat{ \xi}_{h,f}\right)\] \[\qquad\qquad+2\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{ h}}\left(\phi^{*}_{h}(x,a)^{\top}\xi^{*}_{h,f}-\hat{\phi}_{h}(x,a)^{\top}\hat{ \xi}_{h,f}\right)^{2}\] (35)

Combing Eq. (34) and Eq. (35), we have

\[\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left(\phi^ {*}_{h}(x,a)^{\top}\xi^{*}_{h,f}-\hat{\phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f} \right)^{2}\] \[\leq-4\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{h}}\left( f(x^{\prime})-\mathbb{E}_{x^{\prime}\sim P^{*}(\cdot|x,a)}[f(x^{\prime})] \right)\left(\phi^{*}_{h}(x,a)^{\top}\xi^{*}_{h,f}-\hat{\phi}_{h}(x,a)^{\top} \hat{\xi}_{h,f}\right)\] \[\qquad-\sum_{x,a,x^{\prime}\in\mathcal{D}^{\pi^{\prime}}_{h}} \left(\phi^{*}_{h}(x,a)^{\top}\xi^{*}_{h,f}-\hat{\phi}_{h}(x,a)^{\top}\hat{\xi} _{h,f}\right)^{2}\] \[\qquad+264d^{\frac{3}{2}}\log\frac{3dnHT|\Phi|}{\delta}\] \[\leq 288d^{\frac{3}{2}}\log\frac{3dnHT|\Phi|}{\delta}\] (Lemma I.2 with \[\lambda=\frac{1}{8}\] )

Since for every data tuple \((x,a,x^{\prime})\in\mathcal{D}^{\pi^{\prime}}_{h}\), \((x,a)\sim d^{\pi^{\prime}}_{h}\) independently, by Lemma I.3, for every \(\pi^{\prime}\in\Psi^{\rm cov}_{1:H}\) we have

\[\mathbb{E}^{\pi^{\prime}}\left[\left(\phi^{*}_{h}(x,a)^{\top}\xi^{*}_{h,f}-\hat{ \phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f}\right)^{2}\right]\leq\frac{2}{n}\sum_{x,a \in\mathcal{D}^{\pi^{\prime}}_{h}}\left(\phi^{*}_{h}(x,a)^{\top}\xi^{*}_{h,f}- \hat{\phi}_{h}(x,a)^{\top}\hat{\xi}_{h,f}\right)^{2}+\frac{24d\log\left(\frac{6 \sqrt{d}|\Phi|}{\delta}\right)}{n}\]\[\leq\frac{600d^{\frac{3}{2}}}{n}\log\frac{3dnHT|\Phi|}{\delta}\]

This implies there exists a representation \(\hat{\phi}_{1:H}(x,a)\) such that for every \(f\in\mathcal{F}\) and any \(h\in[H]\), there exists \(\hat{\xi}_{h,f}\) such that

\[\max_{\pi^{\prime}\epsilon\Psi_{1:H}^{\epsilon^{\prime}}}\mathbb{E}^{\pi^{ \prime}}\bigg{[}\big{(}\phi_{h}^{\star}(x,a)^{\intercal}\xi_{h,f}^{\star}-\hat {\phi}_{h}(x,a)^{\intercal}\hat{\xi}_{h,f}\big{)}^{2}\bigg{]}\leq\frac{600d^{ \frac{3}{2}}}{n}\log\frac{3dnHT|\Phi|}{\delta}.\] (36)

Eq. (36) matches Eq. (99) with a different error bound on the right hand. From Lemma G.1, we have \(\Psi_{h}^{\text{cov}}\) is a \((\frac{1}{4Nd},\varepsilon)\)-policy cover for layer \(h\), following the rest of the proof in Lemma G.2, for every \(\pi\) and every \(f\in\mathcal{F},h\in[H]\), we have

\[\big{|}\mathbb{E}^{\pi}\big{[}\phi_{h}^{\star}(x,a)^{\intercal}\xi_{h,f}^{ \star}-\hat{\phi}_{h}(x,a)^{\intercal}\hat{\xi}_{h,f}\big{]}\big{|}\leq\frac{7 5d^{\frac{5}{4}}}{\sqrt{n}}\sqrt{|\mathcal{A}|\log\frac{3dnHT|\Phi|}{\delta}} +9d^{\frac{5}{2}}\epsilon.\] (37)

The \(d^{\frac{5}{2}}\) in the second term of Eq. (37) improves the \(d^{\frac{7}{2}}\) term in Eq. (98) because \(\hat{\xi}_{h,f}\in\mathbb{B}_{d}(\sqrt{d})\) rather than \(\mathbb{B}_{d}(d^{\frac{3}{2}})\). Putting the choice \(n=11250d^{\frac{5}{2}}|\mathcal{A}|T^{\frac{7}{3}}\log\frac{3dnHT|\Phi|}{\delta}\) and \(\epsilon=\frac{d^{\frac{5}{2}}}{18}T^{-\frac{1}{3}}\) into Eq. (37), we have for every \(\pi\) and every \(f\in\mathcal{F},h\in[H]\), we have

\[\big{|}\mathbb{E}^{\pi}\big{[}\phi_{h}^{\star}(x,a)^{\intercal}\xi_{h,f}^{ \star}-\hat{\phi}_{h}(x,a)^{\intercal}\hat{\xi}_{h,f}\big{]}\big{|}\leq d^{5}T ^{-\frac{1}{3}}\] (38)

Utilizing above results, for every \(\pi\), every \(f\in\mathcal{F}\) and any \(h\in[H-1]\), we have

\[\begin{split}&\left|\sum_{x}(\hat{d}_{h+1}^{\pi}(x)-d_{h+1}^{\pi}(x ))f(x)\right|\\ &=\left|\sum_{x,a}\hat{d}_{h}^{\pi}(x)\pi_{h}(a\mid x)\hat{\phi} _{h}(x,a)^{\intercal}\hat{\xi}_{h,f}-\sum_{x,a}d_{h}^{\pi}(x)\pi(a\mid x)\phi_{ h}^{\star}(x,a)^{\intercal}\xi_{h,f}^{\star}\right|\\ &\leq\left|\sum_{x,a}d_{h}^{\pi}(x)\pi_{h}(a\mid x)\big{(}\phi_{ h}^{\star}(x,a)^{\intercal}\xi_{h,f}^{\star}-\hat{\phi}_{h}(x,a)^{\intercal}\hat{ \xi}_{h,f}\big{)}\right|\\ &\qquad+\left|\sum_{x}\big{(}\hat{d}_{h}^{\pi}(x)-d_{h}^{\pi}(x )\big{)}\underbrace{\sum_{a}\pi_{h}(a|x)\hat{\phi}_{h}(x,a)^{\intercal}\hat{ \xi}_{h,f}}_{\epsilon\mathcal{F}}\right|\\ &\leq d^{5}T^{-\frac{1}{3}}+\left|\sum_{x}\big{(}\hat{d}_{h}^{ \pi}(x)-d_{h}^{\pi}(x)\big{)}f^{\prime}(x)\right|\end{split}\] (Eq. (38))

where in the last step, we define \(f^{\prime}(x)=\sum_{a}\pi_{h}(a|x)\hat{\phi}_{h}(x,a)^{\intercal}\hat{\xi}_{h,f}\in\mathcal{F}\). This allow us to use recursion to finish the proof.

### Regret Analysis

We begin by proving Lemma D.3, showing that the policy class \(\Pi_{\text{lin}}^{\text{cov}}(\epsilon^{\prime})\) suffices to approximate all policies with small error.

**Lemma D.3**.: _For any policy \(\pi\), there exists a policy \(\pi^{\prime}\in\Pi_{\text{lin}}^{\text{cov}}(\epsilon^{\prime})\) such that_

\[\sum_{t=1}^{T}V_{1}^{\pi^{\prime}}(x_{1};\ell^{t})-\sum_{t=1}^{T}V_{1}^{\pi}(x _{1};\ell^{t})\leq H\epsilon^{\prime}\]

**Proof.** Let \(\theta_{h}^{t,\pi}\in\mathbb{B}_{d}(H\sqrt{d})\) be such that

\[\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad Q_{h}^{\pi}(x,a;\ell^{t})= \phi_{h}^{\star}(x,a)^{\intercal}\theta_{h}^{\pi,t}.\]

[MISSING_PAGE_EMPTY:28]

**Lemma D.4**.: \[\textbf{Bias1}+\textbf{Bias2}\leq 2d^{5}H^{2}T^{\frac{2}{3}}\]

**Proof.** For any policy \(\pi\) and any \(t\in[T]\),

\[\left|V_{1}^{\pi}(\boldsymbol{x}_{1};\ell^{t})-\widehat{V}_{1}^{ \pi}(\boldsymbol{x}_{1};\ell^{t})\right| =\sum_{h=1}^{H}\sum_{x_{h}\in\mathcal{A}}\left|d_{h}^{\pi}(x_{h} )-\hat{d}_{h}^{\pi}(x_{h})\right|\sum_{a_{h}\in\mathcal{A}}\pi_{h}(a_{h}\mid x _{h})\cdot\ell_{h}^{t}(x_{h},a_{h})\] \[\leq d^{5}H^{2}T^{-\frac{1}{3}}\] (Lemma D.2)

Thus,

\[\textbf{Bias1}\leq d^{5}H^{2}T^{\frac{2}{3}}\qquad\text{and}\qquad\textbf{ Bias2}\leq d^{5}H^{2}T^{\frac{2}{3}}\]

\(\Box\)

We now prove a mode-free counterpart of Lemma C.4 in Lemma D.5.

**Lemma D.5**.: _For any episode \(t\in[T]\), for any policy \(\pi\) we have_

\[\widehat{V}_{1}^{\pi}(x_{1};\ell^{t}) \leq\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{\prime}}\mathbb{E} ^{\boldsymbol{\pi}^{t}}\left[\hat{\ell}^{t}(\pi;\boldsymbol{\pi}^{t}, \boldsymbol{x}_{1:H},\boldsymbol{a}_{1:H})\right]+d^{\frac{11}{3}}HT^{-\frac{1 }{3}}\sum_{h=2}^{H}\left\|\hat{\phi}_{h-1}(\pi)\right\|_{\left(\Sigma_{h-1}^{t }\right)^{-1}},\] \[\widehat{V}_{1}^{\pi}(x_{1};\ell^{t}) \geq\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{\prime}}\mathbb{E} ^{\boldsymbol{\pi}^{t}}\left[\hat{\ell}^{t}(\pi;\boldsymbol{\pi}^{t}, \boldsymbol{x}_{1:H},\boldsymbol{a}_{1:H})\right]-d^{\frac{11}{3}}HT^{-\frac{1 }{3}}\sum_{h=2}^{H}\left\|\hat{\phi}_{h-1}(\pi)\right\|_{\left(\Sigma_{h-1}^{t }\right)^{-1}}.\]

**Proof.**

We now prove the first inequality:

\[\widehat{V}_{1}^{\pi}(x_{1};\ell^{t})\] \[=\sum_{h=1}^{H}\sum_{x_{h}\in\mathcal{A}}\sum_{a_{h}\in\mathcal{A }}\hat{d}_{h}^{\pi}(x_{h})\pi_{h}(a_{h}\mid x_{h})\cdot\ell_{h}^{t}(x_{h},a_{h }),\] \[=\underbrace{\sum_{a_{1}\in\mathcal{A}}\pi_{1}(a_{1}\mid x_{1}) \cdot\ell_{1}^{t}(x_{1},a_{1})}_{\textbf{First}}+\underbrace{\prod_{h=2}^{H} \sum_{x_{h}\in\mathcal{A}}\sum_{a_{h}\in\mathcal{A}}\hat{d}_{h}^{\pi}(x_{h}) \pi_{h}(a_{h}\mid x_{h})\phi_{h}(x_{h},a_{h})^{\top}g_{h}^{t}}_{\textbf{Remain}}\] (42)

Through importance sampling, we have

\[\textbf{First}=\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{t}}\mathbb{E}^{ \boldsymbol{\pi}^{t}}\left[\frac{\pi_{1}(\boldsymbol{a}_{1}\mid\boldsymbol{x }_{1})}{\pi_{1}^{t}(\boldsymbol{a}_{1}\mid\boldsymbol{x}_{1})}\cdot\ell_{1}^{t }(\boldsymbol{x}_{1},\boldsymbol{a}_{1})\right].\]

We now bound the remaining term in (42) Since \(\Sigma_{h-1}^{t}=\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{t}}\left[\hat{\phi} _{h-1}(\boldsymbol{\pi}^{t})\hat{\phi}_{h-1}(\boldsymbol{\pi}^{t})^{\top}\right]\), we have

**Remain**

\[=\sum_{h=2}^{H}\sum_{x_{h}\in\mathcal{A}}\hat{d}_{h}^{\pi}(x_{h })\underbrace{\sum_{a_{h}\in\mathcal{A}}\pi_{h}(a_{h}\mid x_{h})\phi_{h}(x_{h },a_{h})^{\top}g_{h}^{t}}_{:=f(x_{h})},\] \[=\sum_{h=2}^{H}\sum_{x_{h-1}\in\mathcal{A}_{h-1}}\sum_{a_{h-1}} \hat{d}_{h-1}^{\pi}(x)\pi_{h-1}(a_{h-1}|x_{h-1})\hat{\phi}(x_{h-1},a_{h-1})^{ \top}\hat{\xi}_{h-1,f}\] (by Eq. ( 31 )) \[=\sum_{h=2}^{H}\hat{\phi}_{h-1}(\pi)^{\top}\hat{\xi}_{h-1,f}\] \[=\sum_{h=2}^{H}\hat{\phi}_{h-1}(\pi)^{\top}(\Sigma_{h-1}^{t})^{-1 }\mathbb{E}_{\boldsymbol{\pi}^{t}\sim\rho^{t}}[\hat{\phi}_{h-1}(\boldsymbol{ \pi}^{t})\hat{\phi}_{h-1}(\boldsymbol{\pi}^{t})^{\top}]\hat{\xi}_{h-1,f}\]\[=\mathbb{E}_{\bm{\pi}^{t}\sim\rho^{t}}\left[\sum_{h=2}^{H}\hat{\phi}_{h-1 }(\pi)^{\top}(\Sigma_{h-1}^{t})^{-1}\hat{\phi}_{h-1}(\bm{\pi}^{t})\hat{\phi}_{h- 1}(\bm{\pi}^{t})^{\top}\hat{\zeta}_{h-1,f}\right]\] \[=\mathbb{E}_{\bm{\pi}^{t}\sim\rho^{t}}\left[\sum_{h=2}^{H}\hat{ \phi}_{h-1}(\pi)^{\top}(\Sigma_{h-1}^{t})^{-1}\hat{\phi}_{h-1}(\bm{\pi}^{t}) \sum_{x_{h}\in\mathcal{X}_{h}}\sum_{a_{h}}\left(\hat{d}_{h}^{\bm{\pi}^{t}}(x_{h })-d_{h}^{\bm{\pi}^{t}}(x_{h})\right)\pi_{h}(a_{h}|x_{h})\phi_{h}(x_{h},a_{h})^ {\top}g_{h}^{t}\right]\] \[=\mathbb{E}_{\bm{\pi}^{t}\sim\rho^{t}}\left[\sum_{h=2}^{H}\hat{ \phi}_{h-1}(\pi)^{\top}(\Sigma_{h-1}^{t})^{-1}\hat{\phi}_{h-1}(\bm{\pi}^{t}) \sum_{x_{h}\in\mathcal{X}_{h}}\sum_{a_{h}}\left(\hat{d}_{h}^{\bm{\pi}^{t}}(x_{ h})-d_{h}^{\bm{\pi}^{t}}(x_{h})\right)\pi_{h}(a_{h}|x_{h})\phi_{h}(x_{h},a_{h})^ {\top}g_{h}^{t}\right]\] \[=\mathbb{E}_{\bm{\pi}^{t}\sim\rho^{t}}\left[\sum_{h=2}^{H}\hat{ \phi}_{h-1}(\pi)^{\top}(\Sigma_{h-1}^{t})^{-1}\hat{\phi}_{h-1}(\bm{\pi}^{t}) \sum_{x_{h}\in\mathcal{X}_{h}}\sum_{a_{h}}\left(\hat{d}_{h}^{\bm{\pi}^{t}}(x_{ h})-d_{h}^{\bm{\pi}^{t}}(x_{h})\right)\pi_{h}(a_{h}|x_{h})\phi_{h}(x_{h},a_{h})^ {\top}g_{h}^{t}\right]\] \[=\mathbb{E}_{\bm{\pi}^{t}\sim\rho^{t}}\left[\sum_{h=2}^{H}\sum_{x_{ h}\in\mathcal{X}}\sum_{a_{h}\in\mathcal{A}}\hat{\phi}_{h-1}(\pi)^{\top}\left( \Sigma_{h-1}^{t}\right)^{-1}\hat{\phi}_{h-1}(\bm{\pi}^{t})d_{h}^{\bm{\pi}^{t}} (x_{h})\pi_{h}(a_{h}|\,x_{h})\ell_{h}^{t}(x_{h},a_{h})\right]\] \[\qquad+d^{5}HT^{-\frac{1}{3}}\mathbb{E}_{\bm{\pi}^{t}\sim\rho^{t} }\left[\sum_{h=2}^{H}\sum_{x_{h}\in\mathcal{X}}\sum_{a_{h}\in\mathcal{A}}\left| \hat{\phi}_{h-1}(\pi)^{\top}\left(\Sigma_{h-1}^{t}\right)^{-1}\hat{\phi}_{h-1}( \bm{\pi}^{t})\right|\right],\qquad\qquad\text{(by Lemma D.2)}\] \[\leq\mathbb{E}_{\bm{\pi}^{t}\sim\rho^{t}}\left[\sum_{h=2}^{H}\sum_ {x_{h}\in\mathcal{X}}\sum_{a_{h}\in\mathcal{A}}\hat{\phi}_{h-1}(\pi)^{\top} \left(\Sigma_{h-1}^{t}\right)^{-1}\hat{\phi}_{h-1}(\bm{\pi}^{t})d_{h}^{\bm{ \pi}^{t}}(x_{h},a_{h})\frac{\pi_{h}(a_{h}|\,x_{h})}{\bm{\pi}_{h}^{t}(a_{h}|\,x_ {h})}\ell_{h}^{t}(x_{h},a_{h})\right]\] \[\qquad+d^{5}HT^{-\frac{1}{3}}\sum_{h=2}^{H}\left\|\hat{\phi}_{h-1 }(\pi)\right\|_{\left(\Sigma_{h-1}^{t}\right)^{-1}}\mathbb{E}_{\bm{\pi}^{t}\sim \rho^{t}}\left[\left\|\hat{\phi}_{h-1}(\bm{\pi}^{t})\right\|_{\left(\Sigma_{h-1 }^{t}\right)^{-1}}\right],\] (43) \[\leq\mathbb{E}_{\bm{\pi}^{t}\sim\rho^{t}}\mathbb{E}^{\bm{\pi}^{t} }\left[\sum_{h=2}^{H}\hat{\phi}_{h-1}(\pi)^{\top}\left(\Sigma_{h-1}^{t}\right)^ {-1}\hat{\phi}_{h-1}(\bm{\pi}^{t})\frac{\pi_{h}(\bm{a}_{h}^{t}|\,\bm{x}_{h}^{t })}{\bm{\pi}_{h}^{t}(\bm{a}_{h}^{t}|\,\bm{x}_{h}^{t})}\ell_{h}^{t}(\bm{x}_{h}^ {t},\bm{a}_{h}^{t})\right]\] \[\qquad+d^{\frac{11}{2}}HT^{-\frac{1}{3}}\sum_{h=2}^{H}\left\| \hat{\phi}_{h-1}(\pi)\right\|_{\left(\Sigma_{h-1}^{t}\right)^{-1}},\]

Adding up **First** and **Remain**, and using the defintion of \(\hat{\ell}\) in (22) implies the first inequality of the lemma.

The second inequality follows the same procedure except for applying Cauchy-Schwarz inequality in the opposite direction in Eq. (43). 

Given Lemma D.5, we could follow the same procedure in Lemma C.5 except replacing the factor of bonus \(b^{t}(\pi)\) from \(\sqrt{d}H\epsilon\) to \(d^{\frac{11}{2}}HT^{-\frac{1}{3}}\). This leads to the following changes. Firstly, by a similar argument as Eq. (27), we have

\[\textbf{EXP}=\textbf{FTRL}+4d^{6}H^{2}T^{\frac{2}{3}}\]

where

\[\textbf{FTRL}\coloneqq \sum_{t=1}^{T}\sum_{\pi}p^{t}(\pi)\cdot\left(\mathbb{E}_{\bm{\pi}^ {t}\sim\rho^{t}}\mathbb{E}^{\bm{\pi}^{t}}\left[\hat{\ell}^{t}(\pi;\bm{\pi}^{t}, \bm{x}_{1:H},\bm{a}_{1:H})\right]-b^{t}(\pi)\right)\] \[-\sum_{t=1}^{T}\left(\mathbb{E}_{\bm{\pi}^{t}\sim\rho^{t}} \mathbb{E}^{\bm{\pi}^{t}}\left[\hat{\ell}^{t}(\pi^{*};\bm{\pi}^{t},\bm{x}_{1:H}, \bm{a}_{1:H})\right]-b^{t}(\pi^{*})\right).\]

Secondly, now we have

\[\left|b^{t}(\pi)\right|=\left|d^{\frac{11}{2}}HT^{-\frac{1}{3}}\sum_{h=2}^{H} \left\|\hat{\phi}_{h-1}(\pi)\right\|_{\left(\Sigma_{h-1}^{t}\right)^{-1}} \right|\leq d^{6}H^{2}\frac{T^{-\frac{1}{3}}}{\sqrt{\gamma}}=d^{6}H^{2}T^{-\frac{1 }{4}}.\qquad\text{($\gamma=T^{-\frac{1}{3}}$)}\]To ensure \(\eta\big{|}\hat{\ell}^{t}(\pi;\pi^{t},x_{1:H},a_{1:H})-b^{t}(\pi)\big{|}\leq 1\), it suffices to set \(\eta\leq\left(\frac{2|\mathcal{A}|Hd}{\beta\gamma}+d^{6}H^{2}T^{-\frac{1}{6}} \right)^{-1}=\left(2|\mathcal{A}|HdT^{\frac{2}{3}}+d^{6}H^{2}T^{-\frac{1}{6}} \right)^{-1}\) from \(\beta=\gamma=T^{-\frac{1}{3}}\). Thus our choice \(\eta=(4Hd|\mathcal{A}|)^{-1}T^{-\frac{2}{3}}\) satisfies the condition if we assume \(T\geq d^{6}H^{2}\). Moreover, now we have

\[\textbf{Stability-2} =2\eta\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\sum_{\tau\in\Pi^{\prime}}p ^{t}(\pi)b^{t}(\pi)^{2}\Bigg{]},\] \[=2\eta d^{11}H^{3}T^{-\frac{2}{3}}\mathbb{E}\Bigg{[}\sum_{t=1}^{ T}\sum_{h=2}^{H}\sum_{\tau\in\Pi^{\prime}}p^{t}(\pi)\left\|\hat{\phi}_{h-1}( \pi)\right\|_{\left(\Sigma_{h-1}^{t}\right)^{-1}}^{2}\Bigg{]},\] \[\leq 4\eta d^{11}H^{3}T^{-\frac{2}{3}}\mathbb{E}\Bigg{[}\sum_{t=1 }^{T}\sum_{h=1}^{H-1}\sum_{\tau\in\Pi^{\prime}}\rho^{t}(\pi)\left\|\hat{\phi} _{h-1}(\pi)\right\|_{\left(\Sigma_{h-1}^{t}\right)^{-1}}^{2}\Bigg{]},\] \[\leq 4\eta d^{11}H^{4}T^{\frac{1}{3}}\] \[\leq d^{10}H^{3}T^{-\frac{1}{3}}\] \[\leq d^{4}HT^{\frac{2}{3}}.\] (Assume \[T\geq d^{6}H^{2}\] )

Putting these two changes back to the proof into Lemma C.5, given \(\eta=(4Hd|\mathcal{A}|)^{-1}T^{-\frac{2}{3}}\) we have

\[\textbf{EXP} =\frac{\log|\Pi_{\text{lin}}^{\text{cov}}(\frac{1}{T})|}{\eta}+ \frac{6d\eta H^{2}|\mathcal{A}|T}{\beta}+4\eta d^{2}H^{4}\epsilon^{2}T+d^{4} HT^{\frac{2}{3}}+4d^{6}H^{2}T^{\frac{2}{3}}\] \[=\widetilde{\mathcal{O}}\left(d^{6}|\mathcal{A}|H^{2}T^{\frac{2}{ 3}}\log(|\Phi|)\right)\] (44)

Putting Eq. (41), Lemma D.4, Eq. (44) into Eq. (40) together with \(\epsilon=18^{-1}d^{\frac{5}{2}}T^{-\frac{1}{3}}\), we have

\[\operatorname{Reg}_{T}\leq\widetilde{O}\left(d^{8}H^{6}|\mathcal{A}|T^{\frac{ 2}{3}}\log(|\Phi|)\right)\]Proof of Theorem 4.1 (Model-Free, Banfit Feedback)

We start by introducing some notation. We let \(\mathcal{I}^{(k)}\) denote the rounds in the \(k\)-th epoch:

\[\mathcal{I}^{(k)}\coloneqq\{T_{0}+(k-1)\cdot N_{\mathrm{reg}}+1,\;\ldots,\;T_{0 }+k\cdot N_{\mathrm{reg}}\},\] (45)

where \(T_{0}\) is as in Line 1 of Algorithm 3. Throughout the analysis, we condition on the event

\[\mathcal{E}\coloneqq\mathcal{E}^{\mathrm{cov}},\] (46)

where \(\mathcal{E}^{\mathrm{cov}}\) is as in Lemma G.1. Further, for any \(k\in[K]\), let \(\rho^{(k)}\) be the distribution of the random policy:

\[\mathbb{I}\{\boldsymbol{\zeta}=0\}\cdot\widehat{\pi}^{(k)}+\mathbb{I}\{ \boldsymbol{\zeta}=1\}\cdot\boldsymbol{\pi}\circ_{\boldsymbol{h}}\pi_{\mathtt{ unif}}\circ_{\boldsymbol{h}+1}\widehat{\pi}^{(k)},\] (47)

with \(\boldsymbol{\zeta}\sim\mathrm{Ber}(\nu)\), \(\boldsymbol{h}\sim\mathtt{unif}\{[H]\}\), and \(\boldsymbol{\pi}\sim\mathtt{unif}\{\Psi_{\boldsymbol{h}}^{\mathsf{cov}}\}\).

We start our analysis by applying the performance difference lemma.

Applying the performance difference lemma.For any \(k\in[K]\), \(t\in\mathcal{I}^{(k)}\), and \(\rho^{(k)}\) as just defined, we have

\[\mathbb{E}_{\boldsymbol{\pi}\sim\rho^{(k)}}\mathbb{E}\big{[}V_{1} ^{\boldsymbol{\pi}}(x_{1};\ell^{t})\big{]}-V_{1}^{\boldsymbol{\pi}^{*}}(x_{1}; \ell^{t})\] \[=(1-\nu)\cdot\Big{(}V_{1}^{\boldsymbol{\pi}^{(k)}}(x_{1};\ell^{ t})-V_{1}^{\boldsymbol{\pi}^{*}}(x_{1};\ell^{t})\Big{)}\] \[\quad+\frac{\nu}{Hd}\sum_{h\in[H]}\sum_{\pi\in\Psi_{h}^{\mathsf{ cov}}}\Big{(}V_{1}^{\pi\circ_{h}\pi_{\mathtt{unif}}\circ_{h+1}\overline{\pi}^{(k)}}(x_ {1};\ell^{t})-V_{1}^{\boldsymbol{\pi}^{*}}(x_{1};\ell^{t})\Big{)},\] \[\leq(1-\nu)\cdot\sum_{h=1}^{H}\mathbb{E}^{\pi^{*}}\bigg{[}\sum_{a \in\mathcal{A}}\Big{(}\overline{\pi}_{h}^{(k)}(a\mid\boldsymbol{x}_{h})-\pi_{ h}^{*}(a\mid\boldsymbol{x}_{h})\Big{)}\cdot Q_{h}^{\mathfrak{h}^{(k)}}( \boldsymbol{x}_{h},a;\ell^{t})\bigg{]}+H\nu,\] \[=(1-\nu)\cdot\sum_{h=1}^{H}\mathbb{E}^{\pi^{*}}\bigg{[}\sum_{a \in\mathcal{A}}\Big{(}\overline{\pi}_{h}^{(k)}(a\mid\boldsymbol{x}_{h})-\pi_{ h}^{*}(a\mid\boldsymbol{x}_{h})\Big{)}\cdot\widehat{Q}_{h}^{(k)}(\boldsymbol{x}_{h},a )\bigg{]}+H\nu\] \[\quad+(1-\nu)\cdot\sum_{h=1}^{H}\mathbb{E}^{\pi^{*}}\bigg{[}\sum_ {a\in\mathcal{A}}\overline{\pi}_{h}^{(k)}(a\mid\boldsymbol{x}_{h})\cdot\Big{(} Q_{h}^{\mathfrak{F}^{(k)}}(\boldsymbol{x}_{h},a;\ell^{t})-\widehat{Q}_{h}^{(k)}( \boldsymbol{x}_{h},a)\Big{)}\bigg{]}\] \[\quad+(1-\nu)\cdot\sum_{h=1}^{H}\mathbb{E}^{\pi^{*}}\bigg{[}\sum_ {a\in\mathcal{A}}\pi_{h}^{*}(a\mid\boldsymbol{x}_{h})\cdot\Big{(}\widetilde{Q }_{h}^{(k)}(\boldsymbol{x}_{h},a)-Q_{h}^{\mathfrak{F}^{(k)}}(\boldsymbol{x}_{h },a;\ell^{t})\Big{)}\bigg{]}.\]

Thus, by the triangle inequality, we have

\[\sum_{t\in\mathcal{I}^{(k)}}\Big{(}\mathbb{E}_{\boldsymbol{\pi} \sim\rho^{(k)}}\mathbb{E}\big{[}V_{1}^{\boldsymbol{\pi}}(x_{1};\ell^{t})\big{]} -V_{1}^{\boldsymbol{\pi}^{*}}(x_{1};\ell^{t})\Big{)}\] \[\leq(1-\nu)\cdot\sum_{t\in\mathcal{I}^{(k)}}\sum_{h=1}^{H} \mathbb{E}^{\pi^{*}}\bigg{[}\sum_{a\in\mathcal{A}}\Big{(}\overline{\pi}_{h}^{( k)}(a\mid\boldsymbol{x}_{h})-\pi_{h}^{*}(a\mid\boldsymbol{x}_{h})\Big{)}\cdot \widehat{Q}_{h}^{(k)}(\boldsymbol{x}_{h},a)\bigg{]}+HN_{\mathrm{reg}}\nu,\] \[\quad+2(1-\nu)\cdot\sum_{h=1}^{H}\max_{\pi^{*}\in\mathbb{I}}\sum_ {t\in\mathcal{I}^{(k)}}\mathbb{E}^{\pi^{*}}\bigg{[}\sum_{a\in\mathcal{A}}\pi_{ h}^{*}(a\mid\boldsymbol{x}_{h})\cdot\Big{(}Q_{h}^{\mathfrak{F}^{(k)}}( \boldsymbol{x}_{h},a)-\widehat{Q}_{h}^{(k)}(\boldsymbol{x}_{h},a)\Big{)}\bigg{]} \bigg{]}.\] (48)

We start by bound the first term in the right-hand side of (4.2) (the regret term).

### Bounding the Regret Term

Fix \(h\in[H]\) and define \(\tilde{\pi}_{h}^{\star}(\cdot\mid x)\coloneqq(1-\gamma/T)\cdot\pi_{h}^{\star}( \cdot\mid x)+\gamma\pi_{\mathtt{unif}}(\cdot\mid x)/T\) for all \(x\), where \(\gamma\) is as in Algorithm 3. We have that \(|\widehat{Q}_{h}^{(k)}(x,a)|=|\hat{\phi}_{h}^{(k)}(x,a)^{\intercal}\tilde{\theta }_{h}^{(k)}|\leq H\sqrt{d}\) (since \(\hat{\theta}_{h}^{(k)}\in\mathbb{E}_{d}(H\sqrt{d})\) and \(|\hat{\phi}_{h}^{k}(x,a)|\leq 1\)). By applying Lemma I.5, we have that for any \(\eta\leq\frac{1}{H\sqrt{d}}\) and \(x\in\mathcal{X}\):

\[\sum_{k\in[K]}\sum_{t\in\mathcal{I}^{(k)}}\sum_{a\in\mathcal{A}} \Big{(}\overline{\pi}_{h}^{(k)}(a\mid x)-\pi_{h}^{\star}(a\mid x)\Big{)}\cdot \widetilde{Q}_{h}^{(k)}(x,a)\] \[\leq\sum_{k\in[K]}\sum_{t\in\mathcal{I}^{(k)}}\sum_{a\in\mathcal{ A}}\Big{(}\overline{\pi}_{h}^{(k)}(a\mid x)-\tilde{\pi}_{h}^{\star}(a\mid x) \Big{)}\cdot\widetilde{Q}_{h}^{(k)}(x,a)+H\gamma,\]\[\leq N_{\mathrm{reg}}Hd^{2}\varepsilon.\]

Thus, by letting

\[\overline{Q}_{h}^{\overline{\pi}^{(k)}}(,\cdot):=\frac{1}{N_{\mathrm{reg}}}\sum_ {t\in\mathcal{I}^{(k)}}Q_{h}^{\overline{\pi}^{(k)}}(\cdot,\cdot;\ell^{t}),\] (51)

and using Jensen's inequality (twice), we get

\[\left|\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}^{\pi^{*}}\left[\sum_ {a\in\mathcal{A}}\pi^{\prime}_{h}(a\mid\bm{x}_{h})\cdot\left(Q_{h}^{ \overline{\pi}^{(k)}}(\bm{x}_{h},a;\ell^{t})-\widehat{Q}_{h}^{(k)}(\bm{x}_{h},a)\right)\right]\right|\] \[\leq N_{\mathrm{reg}}\cdot\mathbb{E}^{\pi^{*}}\left[\mathbb{I}\{ \bm{x}_{h}\in\mathcal{X}_{h,\varepsilon}\}\sum_{a\in\mathcal{A}}\pi^{\prime} _{h}(a\mid\bm{x}_{h})\cdot\left|\overline{Q}_{h}^{\overline{\pi}^{(k)}}(\bm{x} _{h},a)-\widehat{Q}_{h}^{(k)}(\bm{x}_{h},a)\right|\right]+2N_{\mathrm{reg}}Hd ^{2}\varepsilon,\] \[\leq N_{\mathrm{reg}}\cdot\sqrt{\mathbb{E}^{\pi^{*}}\left[ \mathbb{I}\{\bm{x}_{h}\in\mathcal{X}_{h,\varepsilon}\}\sum_{a\in\mathcal{A}} \pi^{\prime}_{h}(a\mid\bm{x}_{h})\cdot\left(\overline{Q}_{h}^{\overline{\pi}^ {(k)}}(\bm{x}_{h},a)-\widehat{Q}_{h}^{(k)}(\bm{x}_{h},a)\right)^{2}\right]}+2 N_{\mathrm{reg}}Hd^{2}\varepsilon,\] \[\leq N_{\mathrm{reg}}\cdot\sqrt{\mathbb{E}^{\pi^{*}}\left[ \mathbb{I}\{\bm{x}_{h}\in\mathcal{X}_{h,\varepsilon}\}\cdot\max_{a\in \mathcal{A}}\left(\overline{Q}_{h}^{\overline{\pi}^{(k)}}(\bm{x}_{h},a)- \widehat{Q}_{h}^{(k)}(\bm{x}_{h},a)\right)^{2}\right]}+2N_{\mathrm{reg}}Hd^{2}\varepsilon,\]

and now by the fact that \(\Psi_{h}^{\mathrm{cov}}\) is an \((\frac{1}{8Ad},\varepsilon)\) policy cover for layer \(h\) (see Lemma G.1 and Definition 4.1):

\[\leq N_{\mathrm{reg}}\cdot\sqrt{8A^{2}d\sum_{\pi\in\Psi_{h}^{ \mathrm{cov}}}\mathbb{E}^{\pi\circ_{h}\pi_{\mathrm{unit}}}\left[\left(\overline {Q}_{h}^{\overline{\pi}^{(k)}}(\bm{x}_{h},a)-\widehat{Q}_{h}^{(k)}(\bm{x}_{h},a)\right)^{2}\right]}+2N_{\mathrm{reg}}Hd^{2}\varepsilon.\] (52)

Next, we bound the regression error term in the right-hand side of (52).

### Bounding the Regression Error

**Lemma E.1**.: _Let \(\delta\in(0,1)\) and \(\nu\in(0,1/4)\) be given. There is an event \(\mathcal{E}^{\mathrm{reg}}\) of probability at least \(1-2\delta\) under which_

\[\sum_{\pi\in\Psi_{h}}\mathbb{E}^{\pi\circ_{h}\pi_{\mathrm{out}}} \left[\left(\hat{\phi}_{h}^{(k)}(\bm{x}_{h},\bm{a}_{h})^{\top}\hat{\theta}_{h}^ {(k)}-\overline{Q}_{h}^{\overline{\pi}^{(k)}}(\bm{x}_{h},\bm{a}_{h})\right)^{ 2}\right]\leq\varepsilon_{\mathrm{reg}}^{2}\coloneqq\frac{40H^{3}d\log(2| \mathcal{F}|/\delta)}{\nu N_{\mathrm{reg}}}.\] (53)

**Proof.** Fix \(\delta\in(0,1)\), \(h\in[H]\) and \(k\in[K]\), and let \((\bm{x}_{h}^{t},\bm{a}_{h}^{t},\bm{\zeta}^{t},\bm{h}^{t})\) be as in Algorithm 3. With this, define

\[\bm{I}_{h}^{t}\coloneqq\mathbb{I}\{\bm{\zeta}^{t}=0\text{ or }\bm{h}^{t}\leq h\},\] (54)

and note that \((\bm{x}_{h}^{t},\bm{a}_{h}^{t},\bm{I}_{h}^{t})_{t\in\mathcal{I}^{(k)}}\) are identically and independently distributed. Further, for \(t\in\mathcal{I}^{(k)}\) and \(\pi\in\Pi\), let \(\theta_{h}^{t,\pi}\in\mathbb{B}_{d}(H\sqrt{d})\) be such that

\[\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad Q_{h}^{\pi}(x,a;\ell^{t})= \phi_{h}^{\star}(x,a)^{\top}\theta_{h}^{\pi,t}.\]

Such a \(\theta_{h}^{\pi,t}\) is guaranteed to exist by the low-rank MDP structure (Assumption 2.1) and Assumption 2.2. With this, note that for \(\overline{Q}_{h}^{\overline{\pi}^{(k)}}\) as in (51), we have

\[\overline{Q}_{h}^{\overline{\pi}^{(k)}}(x,a)=\phi_{h}^{\star}(x,a)^{\top} \theta_{h}^{(k)},\quad\text{where}\quad\theta_{h}^{(k)}\coloneqq\frac{1}{N_{ \mathrm{reg}}}\sum_{t\in\mathcal{I}^{(k)}}\theta_{h}^{\overline{\pi}^{(k)},t}.\] (55)

For the rest of this proof, we let \(\mathcal{F}\) be the function class

\[\mathcal{F}\coloneqq\{f:(x,a)\mapsto\phi_{h}(x,a)^{\top}\theta\,|\,\theta\in \mathcal{C}\cup\{\theta_{h}^{(k)}\},\phi\in\Phi\},\]

where \(\mathcal{C}\) is a minimal \((N_{\mathrm{reg}})^{-1}\)-cover of \(\mathbb{B}(H\sqrt{d})\) in \(\|\cdot\|\) distance. Further, for \(\tau\in\mathcal{I}^{(k)}\) we let

\[\bm{z}_{h}^{\tau}\coloneqq\sum_{l=h}^{H}\ell_{l}^{\tau}(\bm{x}_{l}^{\tau},\bm{ a}_{l}^{\tau});\]

\[\bm{\varepsilon}_{h}^{\tau}\coloneqq\sum_{l=h}^{H}\ell_{l}^{\tau}(\bm{x}_{l}^{ \tau},\bm{a}_{l}^{\tau})-\frac{1}{N_{\mathrm{reg}}}\sum_{t\in\mathcal{I}^{(k)} }Q_{h}^{\overline{\pi}^{(k)}}(\bm{x}_{h}^{\tau},\bm{a}_{h}^{\tau};\ell^{t});\] (56)

and

\[\widehat{\mathcal{L}}(f)\coloneqq\sum_{t\in\mathcal{I}^{(k)}}\bm{I}_{h}^{t} \cdot(f(\bm{x}_{h}^{t},\bm{a}_{h}^{t})-\bm{z}_{h}^{t})^{2},\] (57)

for \(f\in\mathcal{F}\). Finally, let \(f_{\star}(x,a)\coloneqq\phi_{h}^{\star}(x,a)^{\top}\theta_{h}^{(k)}\), where \(\theta_{h}^{(k)}\) is as in (55) and \(\hat{f}(x,a)\coloneqq\overline{Q}_{h}^{(k)}(x,a)\). With this, note that \(f_{\star}\) and \(\hat{f}\) satisfy \(f_{\star}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})=\bm{z}_{h}^{t}-\bm{\varepsilon}_{h}^ {t}\) and \(\hat{f}\in\operatorname*{argmin}_{f\in\mathcal{F}}\widetilde{\mathcal{L}}(f)\).

Now, since \(\hat{f}\in\operatorname*{argmin}_{f\in\mathcal{F}}\widetilde{\mathcal{L}}(f)\), we have

\[0\geq\widehat{\mathcal{L}}(\hat{f})-\widehat{\mathcal{L}}(f_{\star})=\nabla \widehat{\mathcal{L}}(f_{\star})[\hat{f}-f_{\star}]+\|\hat{f}-f_{\star}\|^{2},\] (58)

where \(\nabla\) denotes directional derivative and

\[\|\hat{f}-f_{\star}\|^{2}\coloneqq\sum_{t\in\mathcal{I}^{(k)}}\bm{I}_{h}^{t} \cdot(\hat{f}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})-f_{\star}(\bm{x}_{h}^{t},\bm{a}_{h }^{t}))^{2}.\]

Rearranging (58) and using that \(f_{\star}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})=\bm{z}_{h}^{t}-\bm{\varepsilon}_{h}^ {t}\), we get that

\[\|\hat{f}-f_{\star}\|^{2} \leq-\nabla\widehat{\mathcal{L}}(f_{\star})[\hat{f}-f_{\star}],\] \[=2\sum_{t\in\mathcal{I}^{(k)}}\bm{I}_{h}^{t}\cdot(\bm{z}_{h}^{t}-f_ {\star}(\bm{x}_{h}^{t},\bm{a}_{h}^{t}))(\hat{f}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})- f_{\star}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})),\] \[=2\sum_{t\in\mathcal{I}^{(k)}}\bm{I}_{h}^{t}\cdot\bm{\varepsilon}_{h}^ {t}\cdot(\hat{f}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})-f_{\star}(\bm{x}_{h}^{t},\bm{a} _{h}^{t})).\] (59)

We now bound the right-hand side of (59). For any \(h\in[H]\), \(k\in[K]\) and any \(\hat{f},f_{\star}\in\mathcal{F}\), we apply Lemma I.2 with* \(\mathfrak{F}^{i}=\sigma(\{(\bm{x}_{h}^{j},\bm{a}_{h}^{j},\bm{\varepsilon}_{h}^{j}, \bm{\zeta}^{j}):j\leq t_{i}\})\) where \(t_{i}\coloneqq(k-1)\cdot N_{\mathrm{reg}}+i\);
* The random variable \(\bm{w}^{i}\) set as the difference \[\bm{w}^{i}=\mathbb{I}[\{\bm{\zeta}^{t_{i}}=0\text{ or }\bm{h}^{t_{i}}\leq h \}\cdot\bm{\varepsilon}_{h}^{t_{i}}\cdot(\hat{f}(\bm{x}_{h}^{t_{i}},\bm{a}_{h }^{t_{i}})-f_{*}(\bm{x}_{h}^{t_{i}},\bm{a}_{h}^{t_{i}}))\] \[\qquad\qquad-\mathbb{E}\big{[}\mathbb{I}[\{\bm{\zeta}^{t_{i}}=0 \text{ or }\bm{h}^{t_{i}}\leq h\}\cdot\bm{\varepsilon}_{h}^{t_{i}}\cdot(\hat{f}(\bm{x}_ {h}^{t_{i}},\bm{a}_{h}^{t_{i}})-f_{*}(\bm{x}_{h}^{t_{i}},\bm{a}_{h}^{t_{i}})) \,|\,\mathfrak{F}^{i-1}\big{]}\] where \(t_{i}\coloneqq(k-1)\cdot N_{\mathrm{reg}}+i\);
* \(n=N_{\mathrm{reg}}=|\mathcal{I}^{(k)}|\).
* \(R=4H^{2}\); and
* \(\lambda=1/(16H^{2})\);

to get that there is an event \(\mathcal{E}\) of probability at least \(1-\delta\) under which

\[\sum_{t\in\mathcal{I}^{(k)}}\bm{I}_{h}^{t}\cdot\bm{\varepsilon}_{ h}^{t}\cdot(\hat{f}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})-f_{*}(\bm{x}_{h}^{t},\bm{a}_{h }^{t}))\] \[\leq\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}_{t}[\bm{I}_{h}^{t} \cdot\bm{\varepsilon}_{h}^{t}\cdot(\hat{f}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})-f_{ *}(\bm{x}_{h}^{t},\bm{a}_{h}^{t}))]\] \[\quad+\frac{1}{8H^{2}}\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}_{t}[ \bm{I}_{h}^{t}\cdot(\bm{\varepsilon}_{h}^{t})^{2}\cdot(\hat{f}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})-f_{*}(\bm{x}_{h}^{t},\bm{a}_{h}^{t}))^{2}]+16H^{2}\log(| \mathcal{F}|/\delta),\] \[\leq\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}_{t}[\bm{I}_{h}^{t} \cdot\bm{\varepsilon}_{h}^{t}\cdot(\hat{f}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})-f_{ *}(\bm{x}_{h}^{t},\bm{a}_{h}^{t}))]\] \[\quad+\frac{1}{4}\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}_{t}[\bm{I }_{h}^{t}\cdot(\hat{f}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})-f_{*}(\bm{x}_{h}^{t},\bm {a}_{h}^{t}))^{2}]+16H^{2}\log(|\mathcal{F}|/\delta),\] (60)

where \(\mathbb{E}_{t}[\cdot]\) is defined as \(\mathbb{E}\big{[}\cdot|\,\mathfrak{F}^{t-1}\big{]}\) and the last step uses that \(|\bm{\varepsilon}_{h}^{t}|\leq 2H\), for all \(t\in\mathcal{I}^{(k)}\). For the rest of the proof, we condition on \(\mathcal{E}\) and to simplify notation let

\[\bm{\Delta}_{h}^{t}\coloneqq\hat{f}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})-f_{*}(\bm{x }_{h}^{t},\bm{a}_{h}^{t}).\] (61)

Using the expression of \(\bm{\varepsilon}_{h}^{t}\) in (56), we have

\[\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}_{t}[\bm{I}_{h}^{t}\cdot \bm{\varepsilon}_{h}^{t}\cdot\bm{\Delta}_{h}^{t}]\] \[=\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}_{t}\left[\bm{I}_{h}^{t} \cdot\left(\sum_{i=h}^{H}\ell_{i}^{t}(\bm{x}_{l}^{t},\bm{a}_{l}^{t})-\frac{1}{ N_{\mathrm{reg}}}\sum_{\tau\in\mathcal{I}^{(k)}}Q_{h}^{\mathbb{R}^{(k)}}(\bm{x}_{h}^{t },\bm{a}_{h}^{t};\ell^{\tau})\right)\cdot\bm{\Delta}_{h}^{t}\right].\] (62)

Now, since \(\bm{I}_{h}^{t}=\mathbb{I}[\bm{\zeta}^{t}=0\text{ or }\bm{h}^{t}\leq h]\), we have

\[\mathbb{E}_{t}\left[\sum_{l=h}^{H}\ell_{l}^{t}(\bm{x}_{l}^{t},\bm{a}_{l}^{t}) \mid\bm{x}_{h}^{t},\bm{a}_{h}^{t},\bm{I}_{h}^{t}=1\right]=Q_{h}^{\mathbb{R}^{(k) }}(\bm{x}_{h}^{t},\bm{a}_{h}^{t};\ell^{t}).\]

Plugging this into (62) and using the law of total expectation, we have

\[\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}_{t}[\bm{I}_{h}^{t}\cdot\bm{ \varepsilon}_{h}^{t}\cdot\bm{\Delta}_{h}^{t}]\] \[=\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}_{t}\Bigg{[}\bm{I}_{h}^{t} \cdot\left(Q_{h}^{\mathbb{R}^{(k)}}(\bm{x}_{h}^{t},\bm{a}_{h}^{t};\ell^{t})- \frac{1}{N_{\mathrm{reg}}}\sum_{\tau\in\mathcal{I}^{(k)}}Q_{h}^{\mathbb{R}^{(k)}}( \bm{x}_{h}^{t},\bm{a}_{h}^{t};\ell^{\tau})\right)\cdot\bm{\Delta}_{h}^{t}\Bigg{]},\] \[=\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}_{t}\Big{[}\bm{I}_{h}^{t} \cdot Q_{h}^{\mathbb{R}^{(k)}}(\bm{x}_{h}^{t},\bm{a}_{h}^{t};\ell^{t})\cdot\bm{ \Delta}_{h}^{t}\Big{]}-\frac{1}{N_{\mathrm{reg}}}\sum_{\tau\in\mathcal{I}^{(k)}} \sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}_{t}\Big{[}\bm{I}_{h}^{t}\cdot Q_{h}^{ \mathbb{R}^{(k)}}(\bm{x}_{h}^{t},\bm{a}_{h}^{t};\ell^{\tau})\cdot\bm{\Delta}_{h}^{ t}\Big{]}.\] (63)

On the other hand, since \((\bm{x}_{h}^{t},\bm{a}_{h}^{t},\bm{I}_{h}^{t})_{t\in\mathcal{I}^{(k)}}\) are i.i.d. and \(\ell^{t}\) is chosen by an oblivious adversary, we have

\[\forall t\in\mathcal{I}^{(k)},\quad\mathbb{E}_{t}\Big{[}\bm{I}_{h}^{t}\cdot Q_{h}^{ \mathbb{R}^{(k)}}(\bm{x}_{h}^{t},\bm{a}_{h}^{t};\ell^{t})\cdot\bm{\Delta}_{h}^{ t}\Big{]}=\frac{1}{N_{\mathrm{reg}}}\sum_{\tau\in\mathcal{I}^{(k)}}\mathbb{E}_{\tau}\Big{[}\bm{I}_{h}^{ \tau}\cdot Q_{h}^{\mathbb{R}^{(k)}}(\bm{x}_{h}^{\tau},\bm{a}_{h}^{\tau};\ell^{t}) \cdot\bm{\Delta}_{h}^{\tau}\Big{]}.\]

[MISSING_PAGE_FAIL:36]

Proof of Theorem 4.2 (Model-Free, Bandit Feedback, Adaptive Adversary)

We let \(\mathcal{I}^{(k)}\) denote the rounds in the \(k\)th epoch:

\[\mathcal{I}^{(k)}\coloneqq\{T_{0}+(k-1)\cdot N_{\mathrm{reg}}+1,\;\ldots,\;T_{0 }+k\cdot N_{\mathrm{reg}}\},\] (71)

where \(T_{0}\) is as in Line 8 of Algorithm 4. Throughout the analysis, we condition on the event

\[\mathcal{E}\coloneqq\mathcal{E}^{\mathrm{cov}}\cap\mathcal{E}^{\mathrm{rep+ span}}\cap\mathcal{E}^{\mathrm{freed}},\] (72)

where \(\mathcal{E}^{\mathrm{cov}}\), \(\mathcal{E}^{\mathrm{rep+span}}\), and \(\mathcal{E}^{\mathrm{freed}}\) are as in Lemma G.1, Corollary F.1, and Lemma F.3, respectively. We start our analysis by applying the performance difference lemma.

Applying the performance difference lemma.For any \(k\in[K]\), \(t\in\mathcal{I}^{(k)}\), and let \(\rho^{(k)}\) be the distribution of the random policy:

\[\mathbb{I}\{\boldsymbol{\zeta}^{t}=0\}\cdot\widetilde{\pi}^{(k)}+\mathbb{I} \{\boldsymbol{\zeta}^{t}=1\}\cdot\boldsymbol{\pi}^{t}\circ_{\boldsymbol{h}^ {t+1}}\widetilde{\pi}^{(k)},\] (73)

with \(\boldsymbol{\zeta}^{t}\sim\mathrm{Ber}(\nu)\), \(\boldsymbol{h}^{t}\sim\mathtt{unif}([H])\), and \(\boldsymbol{\pi}^{t}\sim\mathtt{unif}(\Psi^{\mathtt{span}}_{\boldsymbol{h}^ {t}})\).

\[\mathbb{E}_{\boldsymbol{\pi}\sim\rho^{(k)}}\mathbb{E}\big{[}V_{1} ^{\boldsymbol{\pi}}(x_{1};\boldsymbol{\mathcal{H}}^{t-1})\big{]}-V_{1}^{\pi^{ *}}(x_{1};\boldsymbol{\mathcal{H}}^{t-1})\] \[=(1-\nu)\cdot\Big{(}V_{1}^{\mathbb{P}^{(k)}}(x_{1};\boldsymbol {\mathcal{H}}^{t-1})-V_{1}^{\pi^{*}}(x_{1};\boldsymbol{\mathcal{H}}^{t-1}) \Big{)}\] \[\quad+\frac{\nu}{Hd}\sum_{h\in[H]}\sum_{\pi\in\Psi^{\mathtt{cov}} _{\mathrm{inv}}}\Big{(}V_{1}^{\pi\circ_{h+1}\widetilde{\pi}^{(k)}}(x_{1}; \boldsymbol{\mathcal{H}}^{t-1})-V_{1}^{\pi^{*}}(x_{1};\boldsymbol{\mathcal{H} }^{t-1})\Big{)},\] \[=(1-\nu)\cdot\sum_{h=1}^{H}\mathbb{E}^{\pi^{*}}\Bigg{[}\sum_{a \in\mathcal{A}}\Big{(}\widetilde{\pi}^{(k)}_{h}(a\mid\boldsymbol{x}_{h})- \pi^{*}_{h}(a\mid\boldsymbol{x}_{h})\Big{)}\cdot Q_{h}^{\widetilde{\pi}^{(k)} }(\boldsymbol{x}_{h},a;\boldsymbol{\mathcal{H}}^{t-1})\mid\boldsymbol{ \mathcal{H}}^{t-1}\Bigg{]}+HN_{\mathrm{reg}}\nu,\] \[=(1-\nu)\cdot\sum_{h=1}^{H}\mathbb{E}^{\pi^{*}}\Bigg{[}\sum_{a \in\mathcal{A}}\Big{(}\widetilde{\pi}^{(k)}_{h}(a\mid\boldsymbol{x}_{h})- \pi^{*}_{h}(a\mid\boldsymbol{x}_{h})\Big{)}\cdot\widetilde{Q}_{h}^{(k)}( \boldsymbol{x}_{h},a)\mid\boldsymbol{\mathcal{H}}^{t-1}\Bigg{]}+HN_{\mathrm{ reg}}\nu\] \[\quad+(1-\nu)\cdot\sum_{h=1}^{H}\mathbb{E}^{\pi^{*}}\Bigg{[}\sum_ {a\in\mathcal{A}}\widetilde{\pi}^{(k)}_{h}(a\mid\boldsymbol{x}_{h})\cdot \Big{(}\widetilde{Q}_{h}^{(k)}(\boldsymbol{x}_{h},a;\boldsymbol{\mathcal{H}}^ {t-1})-\widetilde{Q}_{h}^{(k)}(\boldsymbol{x}_{h},a)\Big{)}\mid\boldsymbol{ \mathcal{H}}^{t-1}\Bigg{]}\] \[\quad+(1-\nu)\cdot\sum_{h=1}^{H}\mathbb{E}^{\pi^{*}}\Bigg{[}\sum_ {a\in\mathcal{A}}\pi^{*}_{h}(a\mid\boldsymbol{x}_{h})\cdot\Big{(}\widetilde{Q }_{h}^{(k)}(\boldsymbol{x}_{h},a)-Q_{h}^{\widetilde{\pi}^{(k)}}(\boldsymbol{x} _{h},a;\boldsymbol{\mathcal{H}}^{t-1})\Big{)}\mid\boldsymbol{\mathcal{H}}^{t- 1}\Bigg{]}.\]

Thus, by the triangle inequality, we have

\[\sum_{t\in\mathcal{I}^{(k)}}\Big{(}\mathbb{E}_{\boldsymbol{\pi}\sim \rho^{(k)}}\mathbb{E}\big{[}V_{1}^{\boldsymbol{\pi}}(x_{1};\boldsymbol{ \mathcal{H}}^{t-1})\big{]}-V_{1}^{\pi^{*}}(x_{1};\boldsymbol{\mathcal{H}}^{t-1 })\Big{)}\] \[\leq(1-\nu)\cdot\sum_{t\in\mathcal{I}^{(k)}}\sum_{h=1}^{H} \mathbb{E}^{\pi^{*}}\Bigg{[}\sum_{a\in\mathcal{A}}\Big{(}\widetilde{\pi}^{(k)} _{h}(a\mid\boldsymbol{x}_{h})-\pi^{*}_{h}(a\mid\boldsymbol{x}_{h})\Big{)} \cdot\widetilde{Q}_{h}^{(k)}(\boldsymbol{x}_{h},a)\mid\boldsymbol{\mathcal{H}} ^{t-1}\Bigg{]}+HN_{\mathrm{reg}}\nu\] \[\quad+2(1-\nu)\cdot\sum_{h=1}^{H}\max_{\pi^{*}\in\Pi}\Bigg{|}\sum_ {t\in\mathcal{I}^{(k)}}\mathbb{E}^{\pi^{*}}\Bigg{[}\sum_{a\in\mathcal{A}}\pi^{ \prime}_{h}(a\mid\boldsymbol{x}_{h})\cdot\Big{(}Q_{h}^{\widetilde{\pi}^{(k)} }(\boldsymbol{x}_{h},a;\boldsymbol{\mathcal{H}}^{t-1})-\widetilde{Q}_{h}^{(k)} (\boldsymbol{x}_{h},a)\Big{)}\mid\boldsymbol{\mathcal{H}}^{t-1}\Bigg{]}\Bigg{|}.\] (74)

We start by bounding the first term on the right-hand side of (74) (the regret term).

### Bounding the Regret Term

Fix \(h\in[H]\) and define \(\widetilde{\pi}^{*}_{h}(\cdot\mid x)\coloneqq(1-\gamma/T)\cdot\pi^{*}_{h}(\cdot \mid x)+\gamma\pi_{\mathtt{unif}}(\cdot\mid x)/T\) for all \(x\), where \(\gamma\) is as in Algorithm 4. We have that \(|\widetilde{Q}_{h}^{(k)}(x,a)|=|\widetilde{\phi}_{h}^{\mathrm{rep}}(x,a)^{ \intercal}\hat{\theta}_{h}^{(k)}|\leq 8Hd^{2}\) (since \(\hat{\theta}_{h}^{(k)}\in\mathbb{B}_{2d}(4Hd^{2})\) and \(|\widetilde{\phi}_{h}^{\mathrm{rep}}(x,a)|\leq|\phi_{h}^{\mathrm{loss}}(x,a)|+| \phi_{h}^{\mathrm{rep}}(x,a)|\leq 2\)). By applying I.5, we have that for any \(\eta\leq 1\frac{1}{8Hd^{2}}\) and \(x\in\mathcal{X}\):

\[\sum_{k\in[\mathcal{K}]}\sum_{t\in\mathcal{I}^{(k)}}\sum_{a\in \mathcal{A}}\Big{(}\widetilde{\pi}^{(k)}_{h}(a\mid x)-\pi^{*}_{h}(a\mid x)\Big{)} \cdot\widetilde{Q}_{h}^{(k)}(x,a)\] \[\leq\sum_{k\in[K]}\sum_{t\in\mathcal{I}^{(k)}}\sum_{a\in \mathcal{A}}\Big{(}\widetilde{\pi}^{(k)}_{h}(a\mid x)-\widetilde{\pi}^{*}_{h}(a \mid x)\Big{)}\cdot\widetilde{Q}_{h}^{(k)}(x,a)+H\gamma,\]\[\leq\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}^{\pi^{*}}\left[\sum_{a\in \mathcal{A}}\pi^{\prime}_{h}(a\mid\bm{x}_{h})\cdot\left(\bar{\phi}^{\mathrm{rep} }_{h}(\bm{x}_{h},a)^{\top}\bar{\phi}^{t,\overline{\pi}^{(k)}}_{h}-\bar{Q}^{(k)} _{h}(\bm{x}_{h},a)\right)\mid\bm{\mathcal{H}}^{t-1}\right]\right]\] \[\quad+\left|\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}^{\pi^{*}}\left[ \sum_{a\in\mathcal{A}}\pi^{\prime}_{h}(a\mid\bm{x}_{h})\cdot\left(\bar{Q}^{ \overline{\pi}^{(k)}}_{h}(\bm{x}_{h},a;\bm{\mathcal{H}}^{t-1})-\bar{\phi}^{ \mathrm{rep}}_{h}(\bm{x}_{h},a)^{\top}\bar{\phi}^{t,\overline{\pi}^{(k)}}_{h} \right)\mid\bm{\mathcal{H}}^{t-1}\right]\right|,\]

and by Corollary F.1 (in particular (92))

\[\leq\left|\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}^{\pi^{*}}\left[ \sum_{a\in\mathcal{A}}\pi^{\prime}_{h}(a\mid\bm{x}_{h})\cdot\left(\bar{\phi}^ {\mathrm{rep}}_{h}(\bm{x}_{h},a)^{\top}\bar{\phi}^{t,\overline{\pi}^{(k)}}_{h} -\bar{\phi}^{\mathrm{rep}}_{h}(\bm{x}_{h},a)^{\top}\bar{\theta}^{(k)}_{h} \right)\mid\bm{\mathcal{H}}^{t-1}\right]\right|+N_{\mathrm{reg}}\cdot\varepsilon _{\mathrm{span}}+N_{\mathrm{reg}}\cdot\varepsilon_{\mathrm{rep}}\]

and by Corollary F.1 again (in particular (93)) and the triangle inequality

\[\leq 2\sum_{\pi\in\bar{\psi}^{\mathrm{rep}}_{h}}\left|\sum_{t\in \mathcal{I}^{(k)}}\mathbb{E}^{\pi}\left[\bar{\phi}^{\mathrm{rep}}_{h}(\bm{x}_ {h},a)^{\top}\bar{\phi}^{t,\overline{\pi}^{(k)}}_{h}-\bar{\phi}^{\mathrm{rep} }_{h}(\bm{x}_{h},a)^{\top}\bar{\theta}^{(k)}_{h}\mid\bm{\mathcal{H}}^{t-1} \right]\right|+N_{\mathrm{reg}}\cdot\varepsilon_{\mathrm{span}}+N_{\mathrm{reg }}\cdot\varepsilon_{\mathrm{rep}}\] \[\leq 2\sum_{\pi\in\bar{\psi}^{\mathrm{rep}}_{h}}\left|\sum_{t\in \mathcal{I}^{(k)}}\mathbb{E}^{\pi}\left[\bar{Q}^{\overline{\pi}^{(k)}}_{h}(\bm {x}_{h},a;\bm{\mathcal{H}}^{t-1})-\bar{\phi}^{\mathrm{rep}}_{h}(\bm{x}_{h},a)^ {\top}\bar{\theta}^{(k)}_{h}\mid\bm{\mathcal{H}}^{t-1}\right]\right|+N_{ \mathrm{reg}}\cdot\varepsilon_{\mathrm{span}}+N_{\mathrm{reg}}\cdot \varepsilon_{\mathrm{rep}}\]\[+2\sum_{\pi\in\Psi_{h}^{\rm span}}\left|\sum_{t\in\mathcal{L}^{(k)}} \mathbb{E}^{\pi}\left[\bar{\phi}_{h}^{\rm rep}(\bm{x}_{h},\bm{a}_{h})^{\top} \bar{\bm{\vartheta}}_{h}^{t,\overline{\pi}^{(k)}}-Q_{h}^{\overline{\pi}^{(k)}} (\bm{x}_{h},a;\bm{\mathcal{H}}^{t-1})\mid\bm{\mathcal{H}}^{t-1}\right]\right|, \quad\text{(triangle inequality)}\] \[\leq 2\sum_{\pi\in\Psi_{h}^{\rm span}}\left|\sum_{t\in\mathcal{L}^{(k )}}\mathbb{E}^{\pi}\left[Q_{h}^{\overline{\pi}^{(k)}}(\bm{x}_{h},a;\bm{ \mathcal{H}}^{t-1})-\bar{\phi}_{h}^{\rm rep}(\bm{x}_{h},\bm{a}_{h})^{\top}\bar {\theta}_{h}^{(k)}\mid\bm{\mathcal{H}}^{t-1}\right]\right|+N_{\rm reg}\cdot \varepsilon_{\rm span}+3dN_{\rm reg}\cdot\varepsilon_{\rm rep},\] (79)

where the last inequality follows by Corollary F.1 (in particular (92)).

Next, we bound the estimation error term in the right-hand side of (79).

### Bound the Regression Error

By Lemma F.3 (Freedman's inequality), we have that for all \(\pi\in\Psi_{h}^{\rm span}\) and \(\bar{\theta}\in\mathbb{B}_{2d}(4Hd^{2})\):

\[N_{\rm reg}\cdot\varepsilon_{\rm fred}\geq\left|\sum_{t\in \mathcal{L}^{(k)}}\mathbb{I}\{\bm{h}^{t}=h,\bm{\pi}^{t}=\pi,\bm{\zeta}^{t}=1 \}\cdot\left(\bar{\phi}_{h}^{\rm rep}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})^{\top} \bar{\theta}-\sum_{s=h}^{H}\bm{\ell}_{s}^{t}\right)\right.\\ \left.-\sum_{t\in\mathcal{L}^{(k)}}\mathbb{E}\left[\mathbb{I}\{ \bm{h}^{t}=h,\bm{\pi}^{t}=\pi,\bm{\zeta}^{t}=1\}\cdot\left(\bar{\phi}_{h}^{\rm rep }(\bm{x}_{h}^{t},\bm{a}_{h}^{t})^{\top}\bar{\theta}-\sum_{s=h}^{H}\bm{\ell}_{s }^{t}\right)\mid\bm{\mathcal{H}}^{t-1}\right]\right|.\] (80)

On the other hand, we have

\[\sum_{t\in\mathcal{L}^{(k)}}\mathbb{E}\left[\mathbb{I}\{\bm{h}^{ t}=h,\bm{\pi}^{t}=\pi,\bm{\zeta}^{t}=1\}\cdot\left(\bar{\phi}_{h}^{\rm rep }(\bm{x}_{h}^{t},\bm{a}_{h}^{t})^{\top}\bar{\theta}-\sum_{s=h}^{H}\bm{\ell}_{s }^{t}\right)\mid\bm{\mathcal{H}}^{t-1}\right]\] \[=\frac{\nu}{Hd}\sum_{t\in\mathcal{L}^{(k)}}\mathbb{E}^{\pi^{0_{h+ 1}}\overline{\pi}^{(k)}}\left[\bar{\phi}_{h}^{\rm rep}(\bm{x}_{h},\bm{a}_{h})^{ \top}\bar{\theta}-\sum_{s=h}^{H}\bm{\ell}_{s}^{t}\mid\bm{\mathcal{H}}^{t-1} \right],\] \[=\frac{\nu}{Hd}\sum_{t\in\mathcal{L}^{(k)}}\mathbb{E}^{\pi}\left[ \bar{\phi}_{h}^{\rm rep}(\bm{x}_{h},\bm{a}_{h})^{\top}\bar{\theta}-Q_{h}^{ \overline{\pi}^{(k)}}(\bm{x}_{h},a;\bm{\mathcal{H}}^{t-1})\mid\bm{\mathcal{H}} ^{t-1}\right].\] (81)

Now, by Corollary F.1 (in particular (92)) and the triangle inequality, we have

\[N_{\rm reg}\cdot\varepsilon_{\rm rep}\geq\left|\sum_{t\in\mathcal{ L}^{(k)}}\mathbb{E}^{\pi}\left[\bar{\phi}_{h}^{\rm rep}(\bm{x}_{h},\bm{a}_{h})^{ \top}\bar{\theta}-Q_{h}^{\overline{\pi}^{(k)}}(\bm{x}_{h},a;\bm{\mathcal{H}}^ {t-1})\mid\bm{\mathcal{H}}^{t-1}\right]\right.\\ \left.-\sum_{t\in\mathcal{L}^{(k)}}\mathbb{E}^{\pi}\left[\bar{ \phi}_{h}^{\rm rep}(\bm{x}_{h},\bm{a}_{h})^{\top}\bar{\theta}-\bar{\phi}_{h}^{ \rm rep}(\bm{x}_{h},\bm{a}_{h})^{\top}\bar{\bm{\vartheta}}_{h}^{t,\overline{ \pi}^{(k)}}\mid\bm{\mathcal{H}}^{t-1}\right]\right|,\] \[=\left|\sum_{t\in\mathcal{L}^{(k)}}\mathbb{E}^{\pi}\left[\bar{\phi }_{h}^{\rm rep}(\bm{x}_{h},\bm{a}_{h})^{\top}\bar{\theta}-Q_{h}^{\overline{\pi} ^{(k)}}(\bm{x}_{h},a;\bm{\mathcal{H}}^{t-1})\mid\bm{\mathcal{H}}^{t-1}\right]\right.\] \[\left.-\mathbb{E}^{\pi}\left[\bar{\phi}_{h}^{\rm rep}(\bm{x}_{h}, \bm{a}_{h})^{\top}\right]\left(N_{\rm reg}\cdot\bar{\theta}-\sum_{t\in\mathcal{ L}^{(k)}}\bar{\bm{\vartheta}}_{h}^{t,\overline{\pi}^{(k)}}\right)\right|.\] (82)

Thus, by combining (80), (81), and (82), we have

\[\sum_{\pi\in\Psi_{h}^{\rm span}}\left|\sum_{t\in\mathcal{L}^{(k)}} \mathbb{I}\{\bm{h}^{t}=h,\bm{\pi}^{t}=\pi,\bm{\zeta}^{t}=1\}\cdot\left(\bar{ \phi}_{h}^{\rm rep}(\bm{x}_{h}^{t},\bm{a}_{h}^{t})^{\top}\bar{\theta}-\sum_{s=h}^ {H}\bm{\ell}_{s}^{t}\right)\right|\] \[\leq dN_{\rm reg}\cdot\varepsilon_{\rm fred}+\frac{\nu N_{\rm reg} \varepsilon_{\rm rep}}{H}+\frac{\nu}{Hd}\sum_{\pi\in\Psi_{h}^{\rm span}}\left| \sum_{t\in\mathcal{L}^{(k)}}\mathbb{E}^{\pi}\left[\bar{\phi}_{h}^{\rm rep}(\bm{x }_{h},\bm{a}_{h})^{\top}\right]\left(N_{\rm reg}\cdot\bar{\theta}-\sum_{t\in \mathcal{L}^{(k)}}\bar{\bm{\vartheta}}_{h}^{t,\overline{\pi}^{(k)}}\right)\right|.\]

Using that \(\hat{\theta}_{h}^{(k)}\) is the minimizer over \(\mathbb{B}_{2d}(4Hd^{2})\) of the left-hand side, and the right-hand side evaluated to \(dN_{\rm reg}\cdot\varepsilon_{\rm fred}+\frac{\nu N_{\rm reg}\varepsilon_{\rm rep }}{H}\) with \(\bar{\theta}=\frac{1}{N_{\rm reg}}\sum_{t\in\mathcal{L}^{(k)}}\bar{\bm{\vartheta}}_{h }^{t,\overline{\pi}^{(k)}}\in\mathbb{B}_{2d}(4Hd^{2})\), we have that

\[\sum_{\pi\in\Psi_{h}^{\rm span}}\left|\sum_{t\in\mathcal{L}^{(k)}}\mathbb{I}\{\bm{h}^{ t}=h,\bm{\pi}^{t}=\pi,\bm{\zeta}^{t}=1\}\cdot\left(\bar{\phi}_{h}^{\rm rep}(\bm{x}_{h}^{t}, \bm{a}_{h}^{t})^{\top}\hat{\theta}_{h}^{(k)}-\sum_{s=h}^{H}\bm{\ell}_{s}^{t} \right)\right|\leq dN_{\rm reg}\cdot\varepsilon_{\rm fred}+\frac{\nu N_{\rm reg} \varepsilon_{\rm rep}}{H}.\]

[MISSING_PAGE_EMPTY:40]

\(\texttt{Spanner}(h,\Phi,\Psi_{1:h},\phi_{h},n)\) (Algorithm 7) is such that \(|\Psi_{h}^{\mathrm{span}}|=2d\) and, with probability at least \(1-\delta\), for all \(\pi^{\prime}\in\Pi\), there exist \(\{\beta_{\pi}\in[-2,2]:\pi\in\Psi_{h}^{\mathrm{span}}\}\) such that_

\[\left\|\mathbb{E}^{\pi^{\prime}}[\bar{\phi}_{h}(\bm{x}_{h},\bm{a}_{h})]-\sum_{ \pi\in\Psi_{h}^{\mathrm{span}}}\beta_{\pi}\cdot\mathbb{E}^{\pi}[\bar{\phi}_{h}( \bm{x}_{h},\bm{a}_{h})]\right\|\leq\varepsilon_{\mathrm{span}}(n,\alpha,\delta),\] (88)

_where_

\[\varepsilon_{\mathrm{span}}(n,\alpha,\delta)=cH^{2}d\sqrt{\frac{dA\cdot(d\log (2n\sqrt{d}H)+\log(n|\Phi|/\delta))}{\alpha n}}+H^{2}d^{5/2}\varepsilon.\] (89)

_where \(c>0\) is a large enough absolute constant. Furthermore, the number of episodes \(T_{\mathrm{span}}(n)\) used by the call to_ Spanner _is at most \(\widetilde{O}(H^{2}d^{2}n)\)._

**Proof.** To derive the desired bound, we will use the generic guarantee of RobustSpanner from (Mhammedi et al., 2023, Proposition E.1). To invoke this result, we first need to derive guarantees for the optimization and estimation subroutines LinOpt and LinEst within the Spanner algorithm (Algorithm 7). In particular, we need to show that there is some \(\varepsilon^{\prime}\in(0,1)\) such that (with high probability) for any \(\bar{\theta}\in\mathbb{R}^{2d}\times\{0\}\) and \(\pi\in\Pi\), the outputs \(\hat{\pi}_{\bar{\theta}}\coloneqq\texttt{LinOpt}(\bar{\theta}/\|\bar{\theta}\|)\) and \(\hat{\phi}^{\pi}\coloneqq\texttt{LinEst}(\pi)\) satisfy

\[\sup_{\pi\in\Pi}\bar{\theta}^{\top}\mathbb{E}^{\pi}[\bar{\phi}_{h}(\bm{x}_{h},\bm{a}_{h})]\leq\bar{\theta}^{\top}\mathbb{E}^{\hat{\pi}_{\bar{\theta}}}[\bar {\phi}_{h}(\bm{x}_{h},\bm{a}_{h})]+\varepsilon^{\prime}\cdot\|\bar{\theta}\| \quad\text{and}\quad\|\hat{\phi}^{\pi}-\mathbb{E}^{\pi}[\bar{\phi}_{h}(\bm{x} _{h},\bm{a}_{h})]\|\leq\varepsilon^{\prime}.\] (90)

With this, we can apply (Mhammedi et al., 2023, Proposition E.1) to get that the output

\[\pi_{1:2d}=\texttt{RobustSpanner}(\texttt{LinOpt}(\cdot),\texttt{LinEst}( \cdot),2,\varepsilon)\]

for \(\varepsilon\leq 2\varepsilon^{\prime}\) is such that for all \(\pi\in\Pi\), there exist \(\beta_{1},\ldots,\beta_{d}\in[-2,2]\) satisfying

\[\left\|\mathbb{E}^{\pi}[\bar{\phi}_{h}(\bm{x}_{h},\bm{a}_{h})]-\sum_{i=1}^{d} \beta_{i}\cdot\mathbb{E}^{\pi_{i}}[\bar{\phi}_{h}(\bm{x}_{h},\bm{a}_{h})]\right\| \leq 6d\varepsilon^{\prime}.\] (91)

Since LinOpt is based on PSDP as in Line 5 of Algorithm 7 and \(\Psi_{1},\ldots,\Psi_{h}\) are \((\alpha,\varepsilon)\)-policy covers for layers \(1\) to \(h\), respectively, (Mhammedi et al., 2023, Corollary H.1) implies that there is an event \(\mathcal{E}^{\texttt{PSDP}}\) of probability at least \(1-\delta/2\) under which for any \(\bar{\theta}\in\mathbb{R}^{d}\setminus\{0\}\), the output \(\hat{\pi}_{\bar{\theta}}=\texttt{LinOpt}(\bar{\theta})\) satisfies

\[\sup_{\pi\in\Pi}\bar{\theta}^{\top}\mathbb{E}^{\pi}[\bar{\phi}_{h}(\bm{x}_{h},\bm{a}_{h})]\leq\bar{\theta}^{\top}\mathbb{E}^{\hat{\pi}_{\bar{\theta}}}[ \bar{\phi}_{h}(\bm{x}_{h},\bm{a}_{h})]\] \[\quad+|\bar{\theta}|\cdot\left(cH^{2}\sqrt{\frac{dA\cdot(d\log(2n \sqrt{d}H)+\log(n|\Phi|/\delta))}{\alpha n}}+H^{2}d^{3/2}\varepsilon\right),\]

for a large enough absolute constant \(c>0\). On the other hand, since LinEst is based on EstVec as in Line 6, (Mhammedi et al., 2023, Lemma G.3) implies that there is an event \(\mathcal{E}^{\texttt{EstVec}}\) of probability at least \(1-\delta/2\) under which for all \(\pi\in\Pi\), the output \(\hat{\phi}^{\pi}\coloneqq\texttt{LinEst}(\pi)\) satisfies

\[\left\|\hat{\phi}^{\pi}-\mathbb{E}^{\pi}[\bar{\phi}_{h}(\bm{x}_{h},\bm{a}_{h})] \right\|\leq c\cdot\sqrt{\frac{\log(2/\delta)}{n}},\]

for a large enough absolute constant \(c>0\). Therefore, under \(\mathcal{E}^{\texttt{PSDP}}\cap\mathcal{E}^{\texttt{EstVec}}\), LinOpt and LinEst satisfy (90) with

\[\varepsilon^{\prime}\coloneqq cH^{2}\sqrt{\frac{dA\cdot(d\log(2n\sqrt{d}H)+ \log(n|\Phi|/\delta))}{\alpha n}}+H^{2}d^{3/2}\varepsilon.\]

Therefore, by (Mhammedi et al., 2023, Proposition) and the fact that \(d\sqrt{\frac{A\log(ndH|\Phi|/\delta)}{\alpha n}}\leq\varepsilon^{\prime}\), the output

\[\pi_{1:2d}=\texttt{RobustSpanner}\left(\texttt{LinOpt}(\cdot),\texttt{LinEst }(\cdot),2,d\sqrt{\frac{A\log(ndH|\Phi|/\delta)}{\alpha n}}\right)\]

is such that for all \(\pi\in\Pi\), there exist \(\beta_{1},\ldots,\beta_{2d}\in[-2,2]\) satisfying

\[\left\|\mathbb{E}^{\pi}[\bar{\phi}_{h}(\bm{x}_{h},\bm{a}_{h})]-\sum_{i=1}^{2d} \beta_{i}\cdot\mathbb{E}^{\pi_{i}}[\bar{\phi}_{h}(\bm{x}_{h},\bm{a}_{h})] \right\|\leq\varepsilon_{\mathrm{span}}(n,\alpha,\delta),\]

where \(\varepsilon_{\mathrm{span}}(n,\alpha,\delta)\) is as in (89).

Bounding the number of episodesBy (Mhammedi et al., 2023, Proposition E.1), RobustSpanner calls LinOpt and LinEst as most \(\widetilde{O}(d^{2})\) times. Each call to LinOpt [resp. LinEst] requires \(H^{2}n\) episodes. This implies the desired bound on the number of iterations. 

### Representation + Spanner

**Corollary F.1**.: _Let \(\varepsilon,\delta\in(0,1)\), \(\phi_{1;H}^{\mathrm{rep}}\), and \(\Psi_{h}^{\mathrm{span}}\) be as in Algorithm 4. Then, for all \(h\in[H]\), \(\phi_{h}^{\mathrm{rep}}\in\Phi\) and \(|\Psi_{h}^{\mathrm{span}}|=2d\) and there is an event \(\mathcal{E}^{\mathrm{rep+span}}\) of probability \(1-3\delta/4\) under which for all \(h\in[H]\):_

* _For_ \(f\in\mathcal{F}_{h+1}\)_, with_ \(\mathcal{F}_{h+1}\) _as in (_97_), there exists_ \(w_{h+1}^{f}\in\mathbb{B}_{d}(3d^{3/2})\) _such that:_ \[\forall\pi\in\Pi,\quad\left|\mathbb{E}^{\pi}\left[\phi_{h}^{\mathrm{rep}}( \boldsymbol{x}_{h},\boldsymbol{a}_{h})^{\top}w_{h+1}^{f}-\mathbb{E}[f( \boldsymbol{x}_{h+1})\mid\boldsymbol{x}_{h},\boldsymbol{a}_{h}]\right]\right| \leq\varepsilon_{\mathrm{rep}}\coloneqq 10d^{7/2}\varepsilon;\] (92)
* _For all_ \(\pi^{\prime}\in\Pi\)_, there exist_ \(\{\beta_{\pi}\in[-2,2]:\pi\in\Psi_{h}^{\mathrm{span}}\}\) _such that for_ \(\bar{\phi}_{h}^{\mathrm{rep}}\coloneqq[\phi_{h}^{\mathrm{loss}},\phi_{h}^{ \mathrm{rep}}]\)__ \[\left\|\mathbb{E}^{\pi^{\prime}}[\bar{\phi}_{h}^{\mathrm{rep}}(\boldsymbol{x}_ {h},\boldsymbol{a}_{h})]-\sum_{\pi\in\Psi_{h}^{\mathrm{span}}}\beta_{\pi}\cdot \mathbb{E}^{\pi}[\bar{\phi}_{h}^{\mathrm{rep}}(\boldsymbol{x}_{h},\boldsymbol{ a}_{h})]\right\|\leq\varepsilon_{\mathrm{span}}\coloneqq 2H^{2}d^{5/2}\varepsilon.\] (93)

Proof.: From Algorithm 4, we have

\[\phi_{h}^{\mathrm{rep}}=\texttt{RepLearn}(h,\mathcal{F}_{h+1},\Phi,\texttt{ unif}(\Psi_{h}^{\mathrm{cov}}),T_{\mathrm{rep}})\quad\text{and}\quad\Psi_{h}^{ \mathrm{span}}=\texttt{Spanner}(h,\Phi,\Psi_{1:h}^{\mathrm{cov}},\bar{\phi}_{h }^{\mathrm{rep}},T_{\mathrm{span}}),\]

where

\[\Psi_{1:H}^{\mathrm{cov}}=\texttt{Vox}(\Phi,\varepsilon,\delta/4),\qquad T_{ \mathrm{rep}}\coloneqq\frac{AH\log(|\Phi|/\delta)}{\alpha\varepsilon^{2}}, \qquad T_{\mathrm{span}}=\frac{A\log(dH|\Phi|^{-1}\delta^{-1})}{\alpha \varepsilon^{2}},\] (94)

and \(\alpha\coloneqq\frac{1}{A\delta d}\). By Lemma G.1, there is an event \(\mathcal{E}^{\mathrm{cov}}\) of probability at least \(1-\delta/4\) under which, for all \(h\in[H]\), \(\Psi_{h}^{\mathrm{cov}}\) is an \((\alpha,\varepsilon)\)-policy cover for layer \(h\) with \(|\Psi_{h}^{\mathrm{cov}}|=d\). In what follows, we condition on \(\mathcal{E}^{\mathrm{cov}}\). By Lemma G.2 and Lemma F.2, there are events \(\mathcal{E}^{\mathrm{rep}}\) and \(\mathcal{E}^{\mathrm{span}}\) of probability at least \(1-\delta/4\) each such that under \(\mathcal{E}^{\mathrm{rep}}\cap\mathcal{E}^{\mathrm{span}}\) (92) and (93) hold; this follows from (98) and (88) and the choices of \(T_{\mathrm{rep}}\) and \(T_{\mathrm{span}}\) in (94). Finally, by the union bound, we have \(\mathbb{P}\big{[}\mathcal{E}^{\mathrm{cov}}\cap\mathcal{E}^{\mathrm{rep}} \cap\mathcal{E}^{\mathrm{span}}\big{]}\geq 1-\delta\) which completes the proof. 

### Martingal Concentration

**Lemma F.3**.: _Let \(K\), \(N_{\mathrm{reg}}\), \(\bar{\phi}_{1:H}^{\mathrm{rep}}\), and \(\mathcal{I}^{(k)}\) be as in Algorithm 4 for \(k\in[K]\). There is an event \(\mathcal{E}^{\mathrm{fred}}\) of probability at least \(1-\delta/4\) under which for all \(\bar{\theta}\in\mathbb{B}_{2d}(4Hd^{2})\), \(h\in[H]\), \(k\in[K]\), and \(\pi\in\Psi_{h}^{\mathrm{span}}\):_

\[\frac{1}{N_{\mathrm{reg}}}\left|\sum_{t\in\mathcal{I}^{(k)}} \mathbb{I}\{\boldsymbol{h}^{t}=h,\boldsymbol{\pi}^{t}=\pi,\boldsymbol{\zeta}^{t }=1\}\cdot\left(\bar{\phi}_{h}^{\mathrm{rep}}(\boldsymbol{x}_{h}^{t}, \boldsymbol{a}_{h}^{t})^{\top}\bar{\theta}-\sum_{s=h}^{H}\boldsymbol{\ell}_{s} ^{t}\right)\right.\] \[\left.-\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}\left[\mathbb{I}\{ \boldsymbol{h}^{t}=h,\boldsymbol{\pi}^{t}=\pi,\boldsymbol{\zeta}^{t}=1\}\cdot \left(\bar{\phi}_{h}^{\mathrm{rep}}(\boldsymbol{x}_{h}^{t},\boldsymbol{a}_{h}^{t })^{\top}\bar{\theta}-\sum_{s=h}^{H}\boldsymbol{\ell}_{s}^{t}\right)\mid \boldsymbol{\mathcal{H}}^{t-1}\right]\right|\leq\varepsilon_{\mathrm{fred}} \coloneqq 4Hd^{2}\sqrt{\frac{\nu\log(dKHN_{\mathrm{reg}}/\delta)}{N_{\mathrm{reg}}}},\]

_where the random variables \(\boldsymbol{h}^{t},\boldsymbol{\zeta}^{t},\boldsymbol{\pi}^{t}\), and \(\boldsymbol{\mathcal{H}}^{t-1}\) are as in Algorithm 7._

Proof.: Fix \(\bar{\theta}\in\mathbb{B}_{2d}(4d^{2})\), \(h\in[H]\), \(k\in[K]\), and \(\pi\in\Psi_{h}^{\mathrm{span}}\). We apply Lemma I.2 (Freedman's inequality) with

* \(R=4Hd^{2}\);
* \(n=N_{\mathrm{reg}}\);
* The random variable \(\boldsymbol{w}^{i}\) set as the difference \[\boldsymbol{w}^{i}\coloneqq\mathbb{I}\{\boldsymbol{h}^{t_{i}}=h, \boldsymbol{\pi}^{t_{i}}=\pi,\boldsymbol{\zeta}^{t_{i}}=1\}\cdot\left(\bar{\phi} _{h}^{\mathrm{rep}}(\boldsymbol{x}_{h}^{t_{i}},\boldsymbol{a}_{h}^{t_{i}})^{\top} \bar{\theta}-\sum_{s=h}^{H}\boldsymbol{\ell}_{s}^{t_{i}}\right)\] \[\qquad\qquad-\mathbb{E}\left[\mathbb{I}\{\boldsymbol{h}^{t_{i}}=h, \boldsymbol{\pi}^{t_{i}}=\pi,\boldsymbol{\zeta}^{t_{i}}=1\}\cdot\left(\bar{\phi} _{h}^{\mathrm{rep}}(\boldsymbol{x}_{h}^{t_{i}},\boldsymbol{a}_{h}^{t_{i}})^{\top} \bar{\theta}-\sum_{s=h}^{H}\boldsymbol{\ell}_{s}^{t_{i}}\right)\mid \boldsymbol{\mathcal{H}}^{t-1}\right],\] (95) where \(t_{i}\coloneqq(k-1)\cdot N_{\mathrm{reg}}+i\);* The filtration \(\mathfrak{F}^{i}\) set as the \(\sigma\)-algebra \(\sigma(\boldsymbol{\mathcal{H}}^{t_{i}-1})\);
* The variance term \(V_{n}\) has the following upper bound \[V_{n}=\sum_{i=1}^{N_{\mathrm{reg}}}\mathbb{E}\left[(\boldsymbol{ w}^{i})^{2}\mid\mathfrak{F}^{i-1}\right] \leq\sum_{i=1}^{N_{\mathrm{reg}}}\mathbb{E}\left[\mathbb{I}\{ \boldsymbol{h}^{t_{i}}=h,\boldsymbol{\pi}^{t_{i}}=\pi,\boldsymbol{\zeta}^{t_{i }}=1\}\cdot\left(\bar{\phi}_{h}^{\mathrm{rep}}(\boldsymbol{x}_{h}^{t_{i}}, \boldsymbol{a}_{h}^{t_{i}})^{\top}\bar{\theta}-\sum_{s=h}^{H}\boldsymbol{ \ell}_{s}^{t_{s}}\right)^{2}\mid\mathfrak{F}^{i-1}\right]\] \[\leq 8Hd^{3}\nu N_{\mathrm{reg}};\]
* \(\lambda=H^{-1}\left(\frac{d^{2}\nu N_{\mathrm{reg}}}{\log(KHN_{\mathrm{reg}}/ \delta)}\right)^{-1/2}\);

to get that there is an event \(\mathcal{E}^{\mathrm{freed}}_{h,k,\pi}(\bar{\theta})\) of probability at last \(1-(N_{\mathrm{reg}})^{-d}H^{-1}K^{-1}d^{-1}\delta/8\) under which

\[\frac{1}{N_{\mathrm{reg}}}\left|\sum_{t\in\mathcal{I}^{(k)}} \mathbb{I}\{\boldsymbol{h}^{t}=h,\boldsymbol{\pi}^{t}=\pi,\boldsymbol{\zeta}^ {t}=1\}\cdot\left(\bar{\phi}_{h}^{\mathrm{rep}}(\boldsymbol{x}_{h}^{t}, \boldsymbol{a}_{h}^{t})^{\top}\bar{\theta}-\sum_{s=h}^{H}\boldsymbol{\ell}_{s} ^{t}\right)\right.\] \[\left.\qquad-\sum_{t\in\mathcal{I}^{(k)}}\mathbb{E}\left[\mathbb{I }\{\boldsymbol{h}^{t}=h,\boldsymbol{\pi}^{t}=\pi,\boldsymbol{\zeta}^{t}=1\} \cdot\left(\bar{\phi}_{h}^{\mathrm{rep}}(\boldsymbol{x}_{h}^{t},\boldsymbol{a }_{h}^{t})^{\top}\bar{\theta}-\sum_{s=h}^{H}\boldsymbol{\ell}_{s}^{t}\right) \mid\boldsymbol{\mathcal{H}}^{t-1}\right]\right|\] \[\leq 4Hd^{2}\sqrt{\frac{\nu\log(dKHN_{\mathrm{reg}}/\delta)}{N_{ \mathrm{reg}}}}.\] (96)

Let \(\mathcal{C}\) be a minimal \((dHN_{\mathrm{reg}})^{-1}\)-cover of \(\mathbb{B}_{2d}(4Hd^{2})\) with respect to the \(\|\cdot\|\) distance. Under the event

\[\mathcal{E}^{\mathrm{freed}}\coloneqq\bigcap_{h\in[H],k\in[K],\pi\in\mathfrak{ F}_{h}^{\mathrm{pos}},\bar{\theta}\in\mathbb{B}_{2d}(4Hd^{2})}\mathcal{E}^{ \mathrm{freed}}_{h,k,\pi}(\bar{\theta}),\]

Eq. (96) holds for all \(h\in[H],k\in[K]\), and \(\bar{\theta}\in\mathbb{B}_{2d}(4Hd^{2})\) up to an additive \(O(1/N_{\mathrm{reg}})\) error. By the union bound, we have \(\mathbb{P}[\mathcal{E}^{\mathrm{freed}}]\geq 1-\delta/4\) which completes the proof.

Policy Cover and Representation Learning Algorithms

In this section, we present guarantees for VoX, RepLearn, and RobustSpanner which we need in the analysis of our oracle efficient algorithm. The results are based on (Mhammedi et al., 2023).

### Policy Cover

The following result is a restatement of (Mhammedi et al., 2023, Theorem 12).

**Lemma G.1** (VoX Guarantee).: _Let \(\varepsilon,\delta\in(0,1)\) be given. Suppose Assumption 2.1 and Assumption 4.1 hold. Then, there is an event \(\mathcal{E}^{\mathrm{cov}}\) of probability at least \(1-\delta\) under which the output \(\bar{\Psi}^{\mathrm{cov}}_{1:H}=\mathtt{VoX}(\Phi,\varepsilon,\delta)\) is such that for all \(h\in[H]\):_

* \(\Psi^{\mathrm{cov}}_{h}\) _is a_ \(\big{(}\frac{1}{8Ad},\varepsilon\big{)}\)_-policy cover for layer_ \(h\)_;_
* \(|\Psi^{\mathrm{cov}}_{h}|\leq d\)_._

_Furthermore, the number of episodes \(T_{\mathrm{cov}}(\varepsilon)\) used by the call to_ \(\mathtt{VoX}\) _is bounded by \(\widetilde{O}(Ad^{13}H^{6}\log(\Phi/\delta))/\varepsilon^{2}\)_._

### Representation Learning

**Lemma G.2** (Representation Learning Guarantee).: _Let \(\varepsilon,\alpha,\delta\in(0,1)\), \(h\in[H-1]\), and \(n\geq 1\) be given and define the function class_

\[\mathcal{F}_{h+1}\coloneqq\left\{f:(x,a)\mapsto\max_{a\in\mathcal{A}}\bar{ \phi}_{h+1}(x,a)^{\top}\bar{\theta}\,\big{|}\,\bar{\phi}_{h+1}=[\phi^{\mathrm{ loss}}_{h+1},\phi_{h+1}],\phi\in\Phi,\bar{\theta}\in\mathbb{B}_{2d}(1)\right\}.\] (97)

_Further, let \(\Psi\) be an \((\alpha,\varepsilon)\)-policy cover for layer \(h\) with \(|\Psi|=d\), and suppose Assumption 2.1 and Assumption 4.1 hold. Then, with probability at least \(1-\delta\), the output \(\bar{\phi}^{\mathrm{rep}}_{h}=\mathtt{RepLearn}(h,\mathcal{F}_{h+1},\Phi, \mathtt{unif}(\Psi),n)\) is such that for all \(f\in\mathcal{F}_{h+1}\) there exists \(w^{f}_{h+1}\in\mathbb{B}_{d}(3d^{3/2})\) such that:_

\[\forall\pi\in\Pi,\quad\Big{|}\mathbb{E}^{\pi}\Big{[}\phi^{\mathrm{ rep}}_{h}(\bm{x}_{h},\bm{a}_{h})^{\top}w^{f}_{h+1}-\mathbb{E}[f(\bm{x}_{h+1}) \,|\,\bm{x}_{h},\bm{a}_{h}]\Big{]}\Big{|}\leq c\cdot\sqrt{\frac{A{H}d^{5}\log (|\Phi|/\delta)}{\alpha n}}+9d^{7/2}\varepsilon,\] (98)

_where \(c>0\) is a large enough absolute constant. Furthermore, the number of episodes \(T_{\mathrm{rep}}(\varepsilon)\) used by the call to_ \(\mathtt{RepLearn}\) _is equal to \(n\)._

**Proof.** By (Mhammedi et al., 2023, Theorem F.1) and the assumption that \(|\Psi|=d\), there is an event \(\mathcal{E}\) of probability at least \(1-\delta/2\) under which \(\phi^{\mathrm{rep}}_{h}\) satisfies:

\[\sup_{f\in\mathcal{F}_{h+1}}\inf_{\mathtt{VoX}\in\mathcal{B}_{d}(3d^{3/2})} \max_{\pi^{\prime}\in\Psi}\mathbb{E}^{\pi^{\prime}}\big{[}(\phi^{\mathrm{rep} }_{h}(\bm{x}_{h},\bm{a}_{h})^{\top}w-\mathbb{E}[f(\bm{x}_{h+1})\,|\,\bm{x}_{h}, \bm{a}_{h}])^{2}\big{]}\leq c\cdot\frac{Ad^{5}\log(|\Phi|/\delta)}{n},\] (99)

where \(c\) is a large enough absolute constant. We use this to show (98). In what follows, we condition on \(\mathcal{E}\). Fix \(\pi\in\Pi\) and \(f\in\mathcal{G}\) and let \(w^{f}_{h+1}\) be the vector \(w\in\mathbb{B}_{d}(3d^{3/2})\) achieving the infimum in (99) for the given choice of \(f\). Let \(\mathcal{X}_{h,\varepsilon}\) be the set of \(\varepsilon\)-reachable states at layer \(h\) as defined in (50). With this this, we have for all \(h\in[H]\),

\[\Big{|}\mathbb{E}^{\pi}\Big{[}\phi^{\mathrm{rep}}_{h}(\bm{x}_{h}, \bm{a}_{h})^{\top}w^{f}_{h+1}-\mathbb{E}[f(\bm{x}_{h+1})\,|\,\bm{x}_{h},\bm{ a}_{h}]\Big{]}\Big{|}\] \[\leq\Big{|}\mathbb{E}^{\pi}\Big{[}\mathbb{E}\{\bm{x}_{h}\not\in \mathcal{X}_{h,\varepsilon}\}\cdot(\phi^{\mathrm{rep}}_{h}(\bm{x}_{h},\bm{a} _{h})^{\top}w^{f}_{h+1}-\mathbb{E}[f(\bm{x}_{h+1})\,|\,\bm{x}_{h},\bm{a}_{h}]) \Big{]}\Big{|}\] \[\leq\Big{|}\mathbb{E}^{\pi}\Big{[}\mathbb{E}\{\bm{x}_{h}\in \mathcal{X}_{h,\varepsilon}\}\cdot(\phi^{\mathrm{rep}}_{h}(\bm{x}_{h},\bm{a} _{h})^{\top}w^{f}_{h+1}-\mathbb{E}[f(\bm{x}_{h+1})\,|\,\bm{x}_{h},\bm{a}_{h}]) \Big{]}+9d^{7/2}\varepsilon,\] (100)

where the last inequality follows by Lemma I.1.

We now bound the first term on the right-hand side of (100). By Jensen's inequality, we have

\[\Big{|}\mathbb{E}^{\pi}\Big{[}\mathbb{I}\{\bm{x}_{h}\in\mathcal{X}_{h, \varepsilon}\}\cdot(\phi^{\mathrm{rep}}_{h}(\bm{x}_{h},\bm{a}_{h})^{\top}w^{f}_ {h+1}-\mathbb{E}[f(\bm{x}_{h+1})\,|\,\bm{x}_{h},\bm{a}_{h}])\Big{]}\Big{|}\]\[\leq\sqrt{\mathbb{E}^{\pi}\left[\mathbb{I}\{\bm{x}_{h}\in\mathcal{X}_{h, \varepsilon}\}\cdot(\phi_{h}^{\mathrm{rep}}(\bm{x}_{h},\bm{a}_{h})^{\intercal}w _{h+1}^{f}-\mathbb{E}[f(\bm{x}_{h+1})\mid\bm{x}_{h},\bm{a}_{h}])^{2}\right]},\]

and so using that \(\Psi\) is a \((\alpha,\varepsilon^{\prime})\)-policy cover (see Definition 4.1), we have

\[\leq\sqrt{\alpha^{-1}\cdot\max_{\pi^{\prime}\in\Psi}\mathbb{E}^{ \pi^{\prime}}\left[(\phi_{h}^{\mathrm{rep}}(\bm{x}_{h},\bm{a}_{h})^{\intercal} w_{h+1}^{f}-\mathbb{E}[f(\bm{x}_{h+1})\mid\bm{x}_{h},\bm{a}_{h}])^{2}\right]},\] \[\leq\sqrt{c\cdot\frac{AHd^{5}\log(|\Phi|/\delta)}{\alpha n}},\]

where the last step follows by (99). Combining this with (100) yields (98).

[MISSING_PAGE_FAIL:46]

### Martingale Concentration and Regression Results

**Lemma I.2**.: _Let \(R>0\) be given and let \(\bm{w}^{1},\ldots\bm{w}^{n}\) be a sequence of real-valued random variables adapted to filtration \(\mathfrak{F}^{1},\cdots,\mathfrak{F}^{n}\). Assume that for all \(i\in[n]\), \(\bm{w}^{i}\leq R\) and \(\mathbb{E}[\bm{w}^{i}\mid\mathfrak{F}^{i-1}]=0\). Define \(\bm{S}_{n}\coloneqq\sum_{i=1}^{n}\bm{w}^{i}\) and \(V_{n}\coloneqq\sum_{i=1}^{n}\mathbb{E}[(\bm{w}^{i})^{2}\mid\mathfrak{F}^{i-1}]\). Then, for any \(\delta\in(0,1)\) and \(\lambda\in[0,1/R]\), with probability at least \(1-\delta\),_

\[\bm{S}_{n}\leq\lambda V_{n}+\ln(1/\delta)/\lambda.\] (104)

We now state two helpful results from Mhammedi et al. (2024b) without a proof.

**Lemma I.3**.: _Let \(B>0\) and \(n\in\mathbb{N}\) be given. abstract set. Further, let \(\mathcal{Q}\subseteq\{g:\mathcal{X}\times\mathcal{A}\rightarrow[0,B]\}\) be a finite function class and \((\bm{x}^{1},\bm{a}^{1},\bm{\varepsilon}^{1}),\ldots,(\bm{x}^{n},\bm{a}^{n}, \bm{\varepsilon}^{n})\) be a sequence of i.i.d. random variables in \(\mathcal{X}\times\mathcal{A}\times\mathbb{R}\). Then, for any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have_

\[\forall g\in\mathcal{Q},\quad\frac{1}{2}\|g\|^{2}-2B^{2}\log(2|\mathcal{Q}|/ \delta)\leq\|g\|_{n}^{2}\leq 2\|g\|^{2}+2B^{2}\log(2|\mathcal{Q}|/\delta),\] (105)

_where \(\|g\|^{2}\coloneqq\sum_{i\in[n]}\mathbb{E}[g(\bm{x}^{i},\bm{a}^{i})^{2}\mid \mathfrak{F}^{i-1}]\) and \(\|g\|_{n}^{2}\coloneqq\sum_{i=1}^{n}g(\bm{x}^{i},\bm{a}^{i})^{2}\)._

**Lemma I.4** (Generic regression guarantee).: _Let \(B>0\), \(n\in\mathbb{N}\), and \(f_{\star}:\mathcal{X}\times\mathcal{A}\rightarrow[0,B]\) be given. Further, let \(\mathcal{F}\subseteq\{f:\mathcal{X}\times\mathcal{A}\rightarrow[0,B]\}\) be a finite function class and \((\bm{x}^{1},\bm{a}^{1},\bm{\varepsilon}^{1}),\ldots,(\bm{x}^{n},\bm{a}^{n}, \bm{\varepsilon}^{n})\) be a sequence of i.i.d. random variables in \(\mathcal{X}\times\mathcal{A}\times\mathbb{R}\). Suppose that_

* \(f_{\star}\in\mathcal{F}\)_;_
* \(\bm{z}^{i}=f_{\star}(\bm{x}^{i},\bm{a}^{i})+\bm{\varepsilon}^{i}+\bm{b}^{i}\)_, for all_ \(i\in[n]\)_;_
* \(\bm{b}^{1},\ldots,\bm{b}^{n}\in\mathbb{R}\) _(not necessarily i.i.d.);_
* \(\bm{\varepsilon}^{i}\in[-B,B]\)_, for all_ \(i\in[n]\)_; and_
* \(\mathbb{E}[\bm{\varepsilon}^{i}\mid\bm{x}^{i},\bm{a}^{i}]=0\)_._

_Then, for \(\hat{f}\in\operatorname*{argmin}_{f\in\mathcal{F}}\sum_{i=1}^{n}(f(\bm{x}^{i}, \bm{a}^{i})-\bm{z}^{i})^{2}\) and any \(\delta\in(0,1)\), with probability at least \(1-\delta/2\),_

\[\|\hat{f}-f_{\star}\|_{n}^{2}\leq 8B^{2}\log(2|\mathcal{F}|/\delta)+8\sum_{i=1}^{n }(\bm{b}^{i})^{2},\] (106)

_where \(\|\hat{f}-f_{\star}\|_{n}^{2}\coloneqq\sum_{i=1}^{n}(\hat{f}(\bm{x}^{i},\bm{a }^{i})-f^{\star}(\bm{x}^{i},\bm{a}^{i}))^{2}\)._

**Proof.** Fix \(\delta\in(0,1)\) and let \(\widehat{L}_{n}(f)\coloneqq\sum_{i=1}^{n}(f(\bm{x}^{i},\bm{a}^{i})-\bm{z}^{i} )^{2}\), for \(f\in\mathcal{F}\), and note that since \(\hat{f}\in\operatorname*{argmin}_{f\in\mathcal{F}}\widehat{L}_{n}(f)\), we have

\[0\geq\widehat{L}_{n}(\hat{f})-\widehat{L}_{n}(f_{\star})=\nabla\widehat{L}_{n} (f_{\star})[\hat{f}-f_{\star}]+\|\hat{f}-f_{\star}\|_{n}^{2},\] (107)

where \(\nabla\) denotes directional derivative. Rearranging, we get that

\[\begin{split}\|\hat{f}-f_{\star}\|_{n}^{2}&\leq-2 \nabla\widehat{L}_{n}(f_{\star})[\hat{f}-f_{\star}]-\|\hat{f}-f_{\star}\|_{n}^{ 2},\\ &=4\sum_{i=1}^{n}(\bm{z}^{i}-f_{\star}(\bm{x}^{i},\bm{a}^{i}))( \hat{f}(\bm{x}^{i},\bm{a}^{i})-f_{\star}(\bm{x}^{i},\bm{a}^{i}))-\|\hat{f}-f_{ \star}\|_{n}^{2},\\ &=4\sum_{i=1}^{n}(\bm{\varepsilon}^{i}+\bm{b}^{i})(\hat{f}(\bm{x} ^{i},\bm{a}^{i})-f_{\star}(\bm{x}^{i},\bm{a}^{i}))-\|\hat{f}-f_{\star}\|_{n}^{2},\\ &=4\sum_{i=1}^{n}\bm{\varepsilon}^{i}\cdot(\hat{f}(\bm{x}^{i},\bm {a}^{i})-f_{\star}(\bm{x}^{i},\bm{a}^{i}))-\|\hat{f}-f_{\star}\|_{n}^{2}+4 \sum_{i=1}^{n}\bm{b}^{i}\cdot(\hat{f}(\bm{x}^{i},\bm{a}^{i})-f_{\star}(\bm{x}^{i },\bm{a}^{i})),\\ &\leq 4\sum_{i=1}^{n}\bm{\varepsilon}^{i}\cdot(\hat{f}(\bm{x}^{i},\bm {a}^{i})-f_{\star}(\bm{x}^{i},\bm{a}^{i}))-\|\hat{f}-f_{\star}\|_{n}^{2}+4 \sum_{i=1}^{n}(\bm{b}^{i})^{2}+\frac{1}{2}\sum_{i=1}^{n}(\hat{f}(\bm{x}^{i},\bm {a}^{i})-f_{\star}(\bm{x}^{i},\bm{a}^{i}))^{2},\\ &=4\sum_{i=1}^{n}\bm{\varepsilon}^{i}\cdot(\hat{f}(\bm{x}^{i},\bm {a}^{i})-f_{\star}(\bm{x}^{i},\bm{a}^{i}))-\|\hat{f}-f_{\star}\|_{n}^{2}+4 \sum_{i=1}^{n}(\bm{b}^{i})^{2}+\frac{1}{2}\|\hat{f}-f_{\star}\|_{n}^{2}.\end{split}\] (109)

Thus, rearranging, we get

\[\|\hat{f}-f_{\star}\|_{n}^{2}\leq 8\sum_{i=1}^{n}\bm{\varepsilon}^{i}\cdot(\hat{f}(\bm{x}^{i}, \bm{a}^{i})-f_{\star}(\bm{x}^{i},\bm{a}^{i}))-2\|\hat{f}-f_{\star}\|_{n}^{2}+8 \sum_{i=1}^{n}(\bm{b}^{i})^{2}.\] (110)We now bound the first term on the right-hand side of (110). For this, we apply Lemma I.2 with \(\bm{w}^{i}=\bm{\varepsilon}^{i}\cdot(\hat{f}(\bm{x}^{i},\bm{a}^{i})-f_{*}(\bm{x} ^{i},\bm{a}^{i}))\), \(R=B^{2}\), \(\lambda=1/(8B^{2})\), and \(\mathfrak{F}^{i}=\varnothing\), and use

1. the union bound over \(f\in\mathcal{F}\); and
2. the facts that \(\mathbb{E}[\bm{\varepsilon}^{i}\mid\bm{x}^{i},\bm{a}^{i}]=0\),

to get that with probability at least \(1-\delta/2\),

\[\sum_{i=1}^{n}\bm{\varepsilon}^{i}\cdot(\hat{f}(\bm{x}^{i},\bm{a}^{i})-f_{*}( \bm{x}^{i},\bm{a}^{i}))\leq\frac{1}{4}\|\hat{f}-f_{*}\|_{n}^{2}+B^{2}\log(2| \mathcal{F}|/\delta).\] (111)

Combining this with (110), we get that with probability at least \(1-\delta/2\),

\[\|\hat{f}-f_{*}\|_{n}^{2}\leq 8B^{2}\log(2|\mathcal{F}|/\delta)+8\sum_{i=1}^{ n}(\bm{b}^{i})^{2}.\] (112)

This completes the proof. 

### Online Learning

The following is the standard guarantee of exponential weights (e.g. Lemma F.4 of Sherman et al. (2023b)).

**Lemma I.5** (Exponential Weights).: _Given a sequence of loss functions \(\{g^{t}\}_{t=1}^{T}\) over a decision set \(\Pi\), \(\{p^{t}\}_{t=1}^{T}\) is a distribution sequence with \(p^{t}\in\Delta\left(\Pi\right),\;\forall t\in[T]\) such that_

\[p^{t+1}(\pi)\propto\exp\left(-\eta\sum_{i=1}^{T}g^{t}(\pi)\right).\]

_If \(p^{1}\) is a uniform distribution over \(|\Pi|\) and \(\eta g^{t}(\pi)\geq-1\) for all \(t\in[T]\) and \(\pi\in\Pi\). Then_

\[\max_{p\in\Delta\left(\Pi\right)}\left\{\sum_{t=1}^{T}\left(g^{t},p^{t}-p \right)\right\}\leq\frac{\log(|\Pi|)}{\eta}+\eta\sum_{t=1}^{T}\sum_{\pi\in\Pi }p^{t}(\pi)g^{t}(\pi)^{2}\]

### Reinforcement Learning

The following is standard simulation lemma which is first proposed by Abbeel and Ng (2005).

**Lemma I.6** (Simulation Lemma).: _For two finite-horizon MDPs \(\widehat{M}=\{\mathcal{X},\mathcal{A},\ell,\{\widehat{P}_{h}\}_{h=1}^{H}\}\) and \(M=\{\mathcal{X},\mathcal{A},\ell,\{P_{h}\}_{h=1}^{H}\}\) with horizon \(H\) and \(\|\ell\|_{\infty}\leq 1\). Let the corresponding value function be \(\widehat{V}_{h}^{\pi}(x;\ell)\) and \(V_{h}^{\pi}(x;\ell)\) for step \(h\in[H]\). For any policy \(\pi:\mathcal{X}\rightarrow\Delta(\mathcal{A})\), we have_

\[\left|\widehat{V}_{1}^{\pi}(x_{1};\ell)-V_{1}^{\pi}(x_{1};\ell)\right|\leq H \sum_{h=1}^{H}\mathbb{E}_{x,a\sim d_{h}^{\pi}}\left[\left\|\widehat{P}_{h} \left(\cdot\mid x,a\right)-P_{h}\left(\cdot\mid x,a\right)\right\|_{1}\right].\]

The following is the standard performance difference lemma which is first proposed by Kakade and Langford (2002).

**Lemma I.7** (Performance Difference Lemma).: _For a finite-horizon MDPs \(M=\{\mathcal{X},\mathcal{A},\ell,\{P_{h}\}_{h=1}^{H}\}\) starting at \(x_{1}\), and two policies \(\pi,\pi^{\prime}:\mathcal{X}\rightarrow\Delta(\mathcal{A})\), we have_

\[V_{1}^{\pi^{\prime}}(x_{1};\ell)-V_{1}^{\pi}(x_{1};\ell)=\sum_{h=1}^{H} \mathbb{E}_{x\sim d_{h}^{\pi}}\left[\sum_{a\in\mathcal{A}}\left(\pi_{h}^{ \prime}(a|x)-\pi_{h}(a|x)\right)Q_{h}^{\pi^{\prime}}(x,a;\ell)\right]\]

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes],[No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes]" is generally preferable to "[No]", it is perfectly acceptable to answer "[No]" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No]" or "[NA]" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes]. Justification: The main claims made in the abstract and introductio accurately reflect the paper's contributions and scope. The claims are validated by detailed proofs. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]. Justification: The paper discuss the limitations of the work in the discussion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.

* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]. Justification: The paper provides detailed assumptions and proofs. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA]. Justification: This is a theoretical paper. Guidelines:* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This paper does not include experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA]. Justification: This paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA]. Justification: This paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA]. Justification: This paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes]. Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a theoretical work. There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA]. Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]. Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]. Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.

* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.