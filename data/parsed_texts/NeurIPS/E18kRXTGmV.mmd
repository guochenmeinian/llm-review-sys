# CVQA: Culturally-diverse Multilingual

Visual Question Answering Benchmark

 David Romero\({}^{*}\)

Chenyang Lyu\({}^{*}\)

Haryo Akbarianto Wibowo\({}^{\bullet}\)

Teresa Lynn

Injy Hamed

Aditya Nanda Kishore

Aishik Mandal

Alina Dragonetti

Artem Abzaliev

Atnafu Lambebo Tonja

Bonut Fufa Balcha

Chenxi Whitehouse

Christian Salamea

Dan John Velasco

David Ifeoluwa Adelani

David Le Meur

Emilio Villa-Cueva

Fajri Koto

Fauzan Farooqui

Frederico Belcavello

Ganzorig Batnasan

Gisela Vallejo

Grainne Caulfield

Guido Ivetta

Haiyue Song

Henok Biadglign Ademtew

Hernan Maina

Holy Lovenia

Israel Abebe Azime

Jan Christian Blaise Cruz

Jay Gala

Jiahui Geng

Jesus-German Ortiz-Barajas

Jinheon Bae

Jocelyn Dunstan

Laura Alonso Alemany

Kumaranage Ravindu Yasas Nagasinghe

Luciana Benotti

Luis Fernando D'Haro

Marcelo Viridiano

Marcos Estecha-Garitagoitia

Maria Camila Buitrago Cabrera

Mario Rodriguez-Cantelar

Melanie Jouitteau

Mihail Mihaylov

Naome Etori

Mohamed Fazli Mohamed Imam

Muhammad Farid Adilazuarda

MunkhiJargal Gochoo

Munkh-Erdene Otgonbold

Olivier Niyomugisha

Paula Monica Silva

Pranjal Chitale

Raj Dabre

Rendi Chevi

Ruochen Zhang

Ryanidio Diandaru

Samuet Calbyawijaya

Santiago Gongora

Soveong Jeong

Sukannua Purkayasnha

Tatsuki Kuribayashi

Teresa Clifford

Thamma Jayakumar

Tiamg Timpont Torrent

Tqoeger Ehsan

Vladimir Araujo

Yova Kenentchedjhieva

Zara Burzo

Zheng Wei Lim

Zheng Xin Yong

Oana Ignat

Joan Nwatu

Rada Mihalcea

Thamar Solorio\({}^{\bullet}\)

and Alham Fikri Aji\({}^{\bullet}\)

###### Abstract

Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA

Figure 1: We propose CVQA, a large-scale multilingual VQA benchmark, representing the cultures of 30 countries and 31 different languages across 10 diverse categories, comprising 10k samples.

datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA2, a new Culturally-diverse multilingual **V**isual **Q**uestion Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 30 countries on four continents, covering 31 languages with 13 scripts, providing a total of 10k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.

Footnote 2: https://huggingface.co/datasets/afaji/cvqa

## 1 Introduction

Visual Question Answering (VQA) [2; 43; 50] is a task that requires AI systems to answer textual questions based on a given context image. VQA serves as an essential measure for assessing the understanding and reasoning capabilities of Multimodal Large Language Models (MLLMs) across diverse images and texts. With the rapid development of MLLMs, significant improvements have been observed, including support for multiple languages [12; 5; 27; 45; 53]. However, there is still a lack of VQA benchmarks that capture a diverse set of languages and cultural contexts. Specifically, most VQA benchmarks only cover the English language [2; 33]. While some work has been undertaken on multilingual VQA, it either covers a limited set of popular languages or is producing questions via translation/generation of text from the original Western-centric images, thus failing to capture cultural nuances inherent in different languages [6; 44].

To address these limitations, we propose CVQA: a novel, large-scale, multilingual, culturally nuanced VQA benchmark that includes a diverse set of languages, including many that are underrepresented and understudied. CVQA follows the grassroots crowd-sourcing collaboration approaches taken by Masakhane for Africa [37], NusaCrowd for Indonesia [4], and AI4Bharat for India [20]. In our case, however, we collaborate across communities, rather than within one particular community, in order to maximize cultural and linguistic representation. Consequently, our data consists of 10k questions across 30 countries, covering 31 languages. We also sub-categorize CVQA based on Country-Language pairs, resulting in 39 distinct pairs, which is substantially more extensive than existing VQA benchmarks. Furthermore, each sample in CVQA falls into one of 10 diverse categories (see Table 1) and is annotated and validated by fluent speakers and those familiar with the respective cultures, ensuring high quality and diversity. Lastly, CVQA is written in both English and local languages, enabling us to benchmark multilingual MLLMs and English-only MLLMs.

In this study, we benchmark CVQA across various MLLMs and find that it presents a significant challenge for open MLLMs, which most of the time achieve no more than 50% accuracy. Additionally, we observe a notable degradation in model performance when questions are asked in native languages, particularly those in understudied languages such as Breton from France and Javanese from Indonesia, highlighting a significant gap in understanding multilingual prompts. We further conduct several ablation studies to analyze the models' performance across different question categories, regions, languages, and image sources. Our contributions can be summarised as follows:

* First, we introduce CVQA, a new culturally diverse multilingual visual question answering dataset consisting of over 10,000 questions from across 30 countries and 31 languages.
* Second, we provide extensive documentation on our process to crowdsource such large dataset across numerous communities, including annotation guidelines.
* Finally, we provide an initial set of evaluations on this benchmark, to serve as a baseline for future research on vision-language models that are culturally diverse.

We note that efforts to enhance cultural awareness in models are increasingly gaining attention. As such, our work contributes to the growing interest within the community and can encourage further initiatives to broaden the limited world view currently captured by MLLMs.

## 2 CVQA Data Collection

The construction of our CVQA dataset involved a detailed annotation process that aims at creating a culturally diverse and linguistically comprehensive dataset for Visual Question Answering. It is worth noting that, while defining culture is challenging, we follow Adilazuarda et al. [1] by using common-ground knowledge (e.g., information surrounding local dishes, history, places, etc. that is generally shared by the people within the region) as a proxy of culture. In this section, we now turn to outline the detailed procedures followed during the data collection and annotation phases.

### Dataset Collection Design

Country-Language Pair SubsetCVQA is a multilingual, multiple-choice locally-nuanced visual question-answering dataset. The format is similar to commonly used visual QA data such as VQA [2], VQA-2 [13] or GQA [17]. Yet, in contrast to them, we gathered images and created question-answer pairs based on the cultures of various locations. Moreover, for each location, the question-answer pairs were created in their respective local languages, along with parallel English translations. Some languages are shared across different locations (e.g., Mexico-Spanish vs Spain-Spanish), and vice-versa, different languages are shared across the same location (e.g., Indonesia-Indonesian vs Indonesia-Javanese). Therefore, to capture them, we group our CVQA dataset into several subsets based on this Country-Language pair, rather than simply on language or location only.

AnnotatorsTo elicit image collectors and annotation contributions to this project, we reached out to our network, which included both linguistic groups and NLP communities. Annotators needed to be fluent speakers of the language in question and be accustomed to the cultures of the locations for which they provided data. To promote data collection, contributors with significant contributions, either by contributing at least 100 validated question-answer pairs and/or managing several subsets, are rewarded as co-authors in this paper. The annotator demographic statistics can be seen in Figure 8, Appendix D. Our annotators are predominantly native speakers, with around 89% residing in the respective country for over 16 years. The age group distribution shows a significant concentration in the 18-30 age bracket, with about one-third female representation. Overall, the demographic profile highlights diversity in terms of age, with high levels of cultural familiarity and language proficiency.

CategoriesFor the categorization of questions of our CVQA dataset, we incorporate 10 diverse categories to ensure a culturally-comprehensive representative set of visual questions, which are shown in Table 1. We mainly adopt the categorization from the OK-VQA dataset [33], with some modifications to fit the theme of our project. Specifically, the categories from the OK-VQA dataset used in our CVQA dataset are 1) to 7). We split the original category of _Geography_, _History_, _Language and Culture_ into 2 separate categories of 8) and 9). In addition, we added a new category of 10) considering the effect that cultural icons and media have on everyday life.

### Annotation Process

We developed concise annotation guidelines (in English) that are suitable for all Country-Language subset teams. Here we provide an overview of the key steps that annotators followed during the dataset creation process. The full guidelines are provided in Appendix A.

Image Selection and PreparationFor each Country-Language pair, annotators were instructed to select images that depict diverse cultural aspects pertinent to their cultural backgrounds among one of

\begin{table}
\begin{tabular}{l} \hline \hline
**Category** \\ \hline
1. Vehicles and Transportation (Vehicles) \\
2. Cooking and Food (Food) \\
3. People and Everyday Life (People) \\
4. Sports and Recreation (Sports) \\
5. Plants and Animals (Plants \& Animals) \\
6. Objects, Materials, and Clothing (Objects) \\
7. Brands and Products (Brands) \\
8. Geography, Buildings, and Landmarks (Geography) \\
9. Tradition, Art, and History (Tradition) \\
10. Public Figure and Pop-Culture (Pop Culture) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Categories in our Dataset. To save space in some of our results, we might refer them by shorthand version in brackets.

the 10 categories. We did not enforce balance across categories considering the different variations of cultural knowledge. We strongly recommend that annotators use their own personal images to avoid accidental data leakage from existing online sources. However, we noted that this request was not always possible, since some images are extremely hard to come by (e.g., photos of public figures or landmarks that are far from the annotator's location). Therefore, we also allowed them to use images from our pre-defined list of open-use licensing sources3. For self-made images, we asked the annotators whether they were willing to make the image available for commercial or research purposes. For images from existing online sources, we applied the original license.

Footnote 3: common.wikimedia.org, Flickr, GapMinder, Unsplash, Pixabay

We requested annotators to avoid using sensitive images that would perpetuate stereotypes. In addition, the annotators were also requested to anonymize faces that were not public figures or fictional characters, as well as text that could reveal the answer to the accompanying questions. We also post-processed all images to remove all metadata such as geo-location, device type, and so on.

Question CreationThe questions associated with each image had to be culturally relevant and formulated such that they would require the context of the image in order to be answerable. A maximum of three question-answer pairs could be provided for each image. Each question was accompanied by one correct answer and three distractors that were reasonably plausible, yet incorrect, thus forming a multiple-choice format.

While we follow the existing VQA benchmarks in terms of using a multiple-choice format, we are also aware that multiple-choice has some flaws when used to measure a model's performance [41]. Hence, we made sure that CVQA is also convertible into free-text open-ended QA, by instructing the annotators to ensure that the question would be answerable even without the accompanying multiple choices (i.e., not through a deductive method). Moreover, to accommodate the multilingual aspect of the benchmark, each question-answer pair was created in the local language and manually translated into English.

Annotators were advised to create questions that promoted an understanding and appreciation of different cultures without perpetuating stereotypes. Typical questions ranged from simple identification queries (e.g., "What is the name of this food?") to more complex ones involving multi-hop reasoning or local common-sense knowledge (e.g., "What is the color of the t-shirt the youngest member of this group is wearing?").

Annotation Examples and TrainingThe annotation guidelines provided multiple examples of well-formulated questions and answers to help guide annotator efforts (See Appendix A). These examples helped clarify the level of specificity and cultural relevance expected in the annotations.

Figure 2: Statistics of the CVQA Benchmark

We provided a tutorial to annotators on how to edit and blur sensitive information in the images. To confirm understanding, we spot-checked the annotators' collected data throughout the annotation period and informed them if some of their data did not follow the guidelines.

ValidationThe last step in the CVQA data creation was the validation process. Each entry was validated by another annotator of the same Country-Language pair. The validators were instructed to ensure that each question followed the guidelines. Based on our spot-checking, common mistakes that we encouraged the validators to check were typos and grammatical mistakes, non-cultural questions, questions that could be answered without the image, as well as incorrectly-sourced images. More information on the annotation platform is provided in Appendix B.

### Data Statistics

To ensure sufficient question variation, we set the minimum number of questions to be included in CVQA to be at least 200 questions per Country-Language subset. In the end, we gathered 10,374 total questions across all subsets. Some statistics of our collected data are shown in Table 2. Our CVQA covers a diverse set of languages and locations spread across the globe. We also capture languages written in various scripts. While Latin is the dominant script (used in 22 Country-Language pairs), the remaining scripts are diverse; covering Arabic, Amharic, Bengali, Chinese, Cyrillic, Devanagari, Hangul, Japanese, Perso-Arabic, Sinhalese, Tamil, and Telugu. The Country-Language pairs and corresponding scripts are shown in Appendix E. CVQA covers several less commonly studied languages and regions, such as Ireland-Irish, Indonesia-Minangkabau, Indonesia-Javanese, France-Breton, Nigeria-Igbo and Mongolia-Mongolian.

Question distribution across the subset and categories are shown in Figure 2. Whether the image is coming from an external or personal source varies depending on the subset. We also note that the category with the most personal images is _Cooking and Food_, which we assume is due to the ease of obtaining such images. In contrast, the category with the least amount of personal images is _Public Figures and Pop Culture_, as it is less likely for people to have personal photos under this category.

To investigate the question variations, we categorize each question into question types of "what", "how", "why", "where", "who", and "which" questions. We categorize the questions by simple string-matching performed on the English questions. While not perfect, we argue that this method should be able to capture the trend of the questions. As shown in Figure 2, the majority of the questions fall into "what" questions. Question distribution across different Country-Language pairs varies, with an interesting finding that India-Bengali has a lot of "how" questions. Across categories, perhaps unsurprisingly, the _Geography and Landmark_ category has noticeably more "where" and "which" (e.g., in which city) questions, whereas the _Public Figure and Pop Culture_ category has more "who" questions. By looking at the most frequently used words (Figure 7) across each category, we can see the general theme of the types of questions being asked. For example, questions in the _Cooking and Food_ category often enquire about dish names, ingredients, or tastes.

## 3 Experimental Setup

Models.To evaluate performance on our CVQA benchmark, we select a range of multimodal vision-language models with multilingual and monolingual English-only capabilities. For monolingual English-only models, we test CLIP [40] a contrastive-learning-based model, trained with approximately 400 million images and English-only text pairs from the web, where we use its _vit-large-patch14-336_ version. We also use InstructBLIP(4.1B) [8], an English-only instruction-aware vision model based on BLIP-2 [24], trained with 13 held-in datasets covering different tasks in English. For multilingual models, we evaluate LLaVA-1.5 (7B) [22] based on LLama-2 [46], and mBLIP [12] a BLIP-2 based model that covers 96 languages (where we evaluate two model variations, mBLIP mT0-XL (4.9B) and mBLIP BLOOMZ (8.3B)). Lastly, we employ M-CLIP [5] a multilin

\begin{table}
\begin{tabular}{l r} \hline \hline No. of images & 5,239 \\ No. of questions & 10,374 \\ No. of countries & 30 \\ No. of languages & 31 \\ No. of country-language pairs & 39 \\ Avg. questions per image & 1.98 \\ Avg. words per question & 7.6 \\ Avg. words per option & 1.80 \\ \hline \hline \end{tabular}
\end{table}
Table 2: CVQA Data Statisticsgual CLIP-based model that supports 68 languages, where we use its _XLM-Roberta-Large-Vit-B-32_ version. We also evaluate the most advanced closed-source MLLMs, such as GPT-4o [36] and Gemini-1.5-Flash [45].

Evaluation Framework.We perform a zero-shot evaluation with two types of prompts, as follows: a location-aware prompt, which specifies the country, the question, and the options, (e.g., _"Location: [country]. Question: [question] Options: [options] Short Answer:"_); and a location-agnostic prompt, which follows the same template but does not specify the country in the prompt (e.g., _"Question: [question] Options: [options] Short Answer:"_). Additionally, due to the multilingual nature of CVQA, for each prompt, we evaluate using the English-only and local language question-option pairs. For the generative-based models, LLaVA, mBLIP and InstructBLIP, the image and the prompts are used as the input. The models then produce output probabilities and we treat the highest probability for the options (A,B,C,D) as the prediction (following MMLU [16]). On the other hand, for embedding-based models like CLIP and M-CLIP, we use the embedding-level similarity between the image and the combination of question and each answer candidate texts (_Question+Option-1,...,Question+Option-4_) to select the one with the highest similarity as the correct answer. We use accuracy to measure the performance, following the existing multiple-choice VQA tasks [2; 55].

## 4 Results

In this section, we discuss the performance of existing MLLMs on the CVQA benchmark.

Main ResultsThe overall performance on our CVQA dataset of various open and closed-source MLLMs are shown in Table 3. Among open models, LLaVA-1.5-7B achieves the best performance, but still significantly lagging behind closed models by more than 10%. However, Table 4 shows that LLaVA-1.5-7B indeed achieves better performance on other established English VQA benchmarks, highlighting that culturally-specific questions that we collect in CVQA are challenging even for the best-performing open model (LLaVA-1.5-7B). The performance is even worse when the question is asked in local languages, emphasizing the models' lower capability in handling non-English prompts.

The experimental results also highlight a substantial performance gap between open and closed-source MLLMs. Closed models like GPT-4o and Gemini-1.5-Flash demonstrate superior performance, with GPT-4o achieving the highest accuracy in both English (75.4%) and local language (74.3%) prompts. In contrast, open models like InstructBLIP and mBLIP-mT0 exhibit lower performance, particularly in local language prompts, indicating a need for more diverse training data and refined fine-tuning processes. While proprietary models show superior performance, it is hard to fully explain why, due to their closed nature. Additionally, their results are not reproducible. Therefore, we use open models in the rest of our experiments.

Performance per Country-Language.To see the capability of MLLMs in solving questions for each country and language, we report accuracy performance for Country-Language pairs in Figure 3. From this, we observe that all models struggle with questions in local languages, demonstrating the challenges for current MLLMs. In other words, across all models, their performance drops in local language questions compared to their performance in English questions. For instance, in the

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline
**Model** & **VQA\(\bm{v2}\)[13]** & **GQA**[17] & **VizWiz**[15] & **SciQA-IMG**[28] & **TextVQA**[43] & **CVQA (EN)** & **CVQA (LOC)** \\ \hline LLaVA-1.5-7B & 78.5 & 62.0 & 50.0 & 66.8 & 58.2 & 48.9 & 36.5 \\ InstructBLIP & - & 49.2 & 34.5 & 60.5 & 50.1 & 47.8 & 32.7 \\ \hline \hline \end{tabular}
\end{table}
Table 4: LLaVA-1.5-7B and InstructBLIP results on various VQA datasets, where the results on the other datasets are taken from Liu et al. [26].

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c} \hline \hline
**LLaVA-1.5-7B** & **M-CLIP** & **CLIP** & **mBLIP-mT0** & **mBLIP-BLOOMZ** & **InstructBLIP** & **Gemini-1.5-Flash** & **GPT-4o** \\
**EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** \\ \hline
49.6 & 35.5 & 38.0 & 33.7 & 42.7 & 30.6 & 31.3 & 30.9 & 39.3 & 32.7 & 49.0 & 31.9 & 66.9 & 68.5 & 75.4 & 74.3 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average performance of MLLMs on our CVQA dataset with English prompts (EN) and local language prompts (LOC).

case of Brazil-Portuguese, LLaVA-1.5-7B achieved a score of 60.73% for English and 51.16% for Portuguese. Moreover, in Mongolia-Mongolian, all models struggled, with LLaVA-1.5-7B reaching only 40% for English and 27.62% for Mongolian, suggesting challenges in less resource-rich language environments. It is worth noting that, these multilingual MLLMs do not originally support some of the languages, which also explains the significant performance drop for those languages. In contrast, in languages that are more frequently studied in NLP and have more abundant training resources, the performance gap between English and local languages, such as Spanish, tends to be smaller [3].

Performance Across Categories.We show the breakdown performances of models per category in Table 5, where the categories themselves are described in Section 2.1. Note that the category _People and Everyday Life_ consistently achieves the highest accuracy across most models, with InstructBLIP obtaining 59.8% in English prompts. This can be possibly attributed to the extensive training data available for everyday human activity and interaction, which widely existed in many visual-related datasets. Conversely, the _Cooking & Food_ and _Pop Culture_ categories exhibit lower accuracy across models, especially in local language prompts. This demonstrates that the high diversity in food and pop culture across different cultures poses a great challenge for the generalization of MLLMs.

Impact of External Image Source.The performance of various models on self-made versus web images is shown in Table 6. One of the interesting findings is the performance variability across

Figure 3: Model performance per Country-Language pair. The blue lines indicate separation by continent. All models show similar behaviour in the majority of cases, despite having different sizes.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline
**Categories** & \multicolumn{2}{c}{**LLaVA-1.5-7B**} & \multicolumn{2}{c}{**M-CLIP**} & \multicolumn{2}{c}{**CLIP**} & \multicolumn{2}{c}{**mBLIP-mT0**} & \multicolumn{2}{c}{**mBLIP-BLOOMZ**} & \multicolumn{2}{c}{**InstructBLIP**} \\  & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** \\ \hline Brands & **49.9** & 36.5 & 37.2 & 35.7 & 36.6 & 29.7 & 33.7 & 30.8 & 40.5 & 35.1 & 48.4 & 32.6 \\ Food & **45.4** & 31.9 & 34.5 & 29.1 & 39.2 & 30.4 & 28.1 & 27.6 & 37.7 & 29.8 & 44.4 & 30.6 \\ Geography & **47.1** & 38.2 & 37.1 & 34.2 & 41.8 & 31.9 & 30.6 & 31.6 & 35.0 & 32.3 & 45.3 & 33.2 \\ Objects & 51.8 & 33.0 & 39.4 & 34.5 & 39.7 & 25.4 & 34.3 & 33.0 & 43.1 & 34.0 & **52.3** & 29.1 \\ People & 58.9 & 38.1 & 45.0 & 37.8 & 46.8 & 30.9 & 35.3 & 34.7 & 46.3 & 36.7 & **59.8** & 34.0 \\ Plants \& Animals & **55.7** & 37.5 & 43.7 & 32.0 & 48.0 & 27.2 & 35.2 & 35.5 & 46.0 & 36.0 & 55.4 & 35.1 \\ Pop Culture & 44.5 & 36.3 & 33.7 & 31.5 & **46.1** & 36.3 & 28.8 & 29.9 & 35.7 & 30.7 & 45.1 & 34.6 \\ Sports & **50.7** & 39.1 & 39.3 & 33.3 & 43.5 & 32.4 & 32.6 & 31.4 & 40.1 & 34.9 & 50.5 & 34.7 \\ Tradition & **50.4** & 35.8 & 37.0 & 35.2 & 41.9 & 32.2 & 31.6 & 31.5 & 39.0 & 32.2 & 47.9 & 30.8 \\ Vehicles & 50.6 & 41.4 & 39.5 & 41.1 & 44.6 & 30.5 & 35.6 & 33.9 & 42.0 & 34.0 & **55.0** & 33.0 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Accuracy of models across categories. Per category, the best performing models on English (EN) and local language (LOC) question-option pairs are bolded and underlined, respectively.

image sources for different models. For self-made images, the accuracy of some models such as LLaVA-1.5-7B and CLIP tends to be lower compared to web images. For instance, LLaVA-1.5-7B achieves a 48.8% accuracy in English prompts on self-made images but slightly higher at 49.7% on web images. CLIP shows an accuracy of 43.1% in English prompts on web images compared to 41.2% on self-made images. While this trend is not consistent across the other models, the results still indicate that web images might be more representative of the data these models (such as LLaVA-1.5-7B and CLIP) were trained on, leading to better performance.

Location-Aware vs Location-Agnostic Prompt.The performance of the various models on location-aware versus location-agnostic prompts is shown in Table 7. While the inclusion of location information has a varied impact on different models, the overall difference between both prompt options is marginal, suggesting no significant effect of including location information on MLLMs.

Performance without Multiple Choice Options.Most of the evaluations we conduct on CVQA are under a multiple-choice setting. However, the multiple-choice setting is often brittle towards option ordering [38; 54], and not very natural with respect to real-world scenarios [30]. In this paragraph, we explore the model's performance on CVQA in an open-ended QA setting. To evaluate in this setting, we prompt the models without giving them the options (e.g., "In which city is this monument located?"). Then, the answer is selected by choosing the model's highest probability of generating the full answer phrase of one of the options [11] (e.g., Jakarta, Bandung, Bali, Surabaya). This way, it is robust towards ordering unlike predicting the answer letter (e.g., A), while also not giving the model multiple-choice options that can be indirectly used for deductive reasoning. Our result shows that LLaVA-1.5-7B achieved a noticeable performance drop when prompted without multiple choice, from 49.6% to just 30% average performance. This notes that in a more practical scenario, these models might be even more unreliable in cultural understanding.

## 5 Limitations

Our new benchmark dataset represents a diverse worldview through the inclusion of different languages and regions not covered in previous datasets. But we acknowledge that even CVQA is not comprehensive, as it covers only a fraction of the world's languages and regions. CVQA also lacks an English-centric baseline, which could arguably provide an interesting comparison with the rest of the regions. Additionally, our data scale prevents using CVQA to train new models, limiting its use for benchmarking purposes only.

We note that each region has different characteristics of questions and difficulty--some regions are more likely to provide simpler, identity "what is" questions, whereas other regions might use questions that require deeper cultural knowledge. Therefore, comparing performance across languages/countries might not always be fair.

Culture is hard to define, and our CVQA ultimately serves only as a proxy to benchmark the model's understanding of culture through local common knowledge. However, this by no means captures all cultural nuances [1]. Additionally, our location granularity captures country-level cultural knowledge. However, it might be interesting to capture cultural awareness at a more granular level, such as city-level, since each city might have variations in cultural common knowledge. Similarly, other demographic factors such as age might play a role in common knowledge.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline
**Image Source** & **LLaVA-1.5-7B** & \multicolumn{2}{c}{**M-CLIP**} & \multicolumn{2}{c}{**CLIP**} & \multicolumn{2}{c}{**mBLIP-m70**} & \multicolumn{2}{c}{**mBLIP-BLOOMZ**} & \multicolumn{2}{c}{**InstructBLIP**} \\  & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** \\ \hline Self-made Image & 48.8 & 34.2 & 38.1 & 34.3 & 41.2 & 30.1 & 31.2 & 31.5 & 40.1 & 33.4 & 48.3 & 31.5 \\ Web Image & 49.7 & 37.4 & 37.4 & 33.3 & 43.1 & 31.8 & 31.9 & 31.2 & 38.7 & 32.3 & 49.1 & 33.1 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Accuracy of different models divided by image source

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline
**Prompt type** & **LLaVA-1.5-7B** & \multicolumn{2}{c}{**M-CLIP**} & \multicolumn{2}{c}{**CLIP**} & \multicolumn{2}{c}{**mBLIP-m70**} & \multicolumn{2}{c}{**mBLIP-BLOOMZ**} & \multicolumn{2}{c}{**InstructBLIP**} \\  & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** & **EN** & **LOC** \\ \hline Location-aware & 49.6 & 35.5 & 38.0 & 33.7 & 42.7 & 30.6 & 31.3 & 30.9 & 39.3 & 32.7 & 49.0 & 31.9 \\ Location-agnostic & 48.3 & 34.7 & 38.1 & 33.8 & 43.8 & 30.8 & 34.1 & 31.8 & 39.8 & 33.6 & 48.7 & 31.1 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Location-aware and location-agnostic resultsIn this section we discussed the following aspects: 1) the fact that this dataset cannot be considered as a comprehensive representation of the world languages and regions; 2) the different levels of question complexity; 3) a bounded definition of culture. While these limitations might be relevant, we consider them as plausible lines for future work and outside the scope of this initial effort.

## 6 Related Work

Substantial progress has been made in recent years on both datasets and methodologies for VQA [42; 23]. Since the introduction of early open-ended VQA datasets [2; 13], various formats like multiple-choice [49; 28], span extraction [34], and free-text generation [27] have been developed. Among these, multiple-choice datasets [28; 29; 52] are the most commonly used, likely due to their simplicity in evaluation and comparison. The development of these datasets has significantly accelerated research progress, serving as both training data and testbeds, especially the recently introduced ScienceQA [28] and MathVista [29] designed for evaluating MLLMs. The evolution of VQA methodologies has been revolutionary, transitioning from statistical machine learning [31] to neural-based methods [32; 17; 40], and advanced MLLMs [27; 36; 45] trained on massive multimodal data. Early VQA systems often required supervised learning and were limited to specific domains, but recent models like CLIP [40], LLaVA [27], and GPT-4V [36] are capable of zero-shot or few-shot learning, demonstrating strong performance. Despite this progress, significant limitations remain. Most VQA datasets focus primarily on English and a few major world languages [28; 29; 51], leading to language bias and under-representation of many languages and cultures. Additionally, the images in these datasets predominantly reflect Western scenes and styles, lacking the diversity needed to represent real-world scenarios across different cultures [7].

Some efforts have been made to create multilingual VQA datasets, such as FM-IQA [10], MCVQA [14], xGQA [39], MaXM [6], MTVQA [44], and MaRVL [25]. However, these datasets are still limited in terms of the number of languages and the cultural diversity of the images and questions, or being a translation of existing English data. On the other hand, there have been initiatives to create culturally-diverse datasets and benchmarks under text-only modality [35; 19; 48; 18; 9; 47; 21]. Our proposed benchmark aims to fill the gap that covers both textual and visual modality by creating a large-scale, culturally-and-linguistically diverse dataset that will enable the development of more inclusive and robust VQA models.

## 7 Conclusion

We proposed CVQA, a novel, human-written visual QA benchmark dataset that captures cultural nuances across a diverse set of languages and locations. CVQA encompasses 10 question categories, with each question written in both English and the native language. This allowed us to benchmark both multilingual visual models and English-only models. We provided insights into our dataset's question types and commonly used terms for each category.

We then performed benchmarks on various visual models, including both multilingual and English-only models. Our benchmark demonstrated that CVQA presented challenges for open-source models. These models generally performed worse when queried in local languages compared to English, indicating poorer performance in handling multilingual queries. The performance is also considerably lower when we do not provide the multiple choice setting, which is a more realistic use case for this technology. We hope that publishing CVQA encourages the AI community to pay more attention to non-English-centric models and benchmarking, thereby advancing progress in multilingual, multimodal research.

## Acknowledgments

We thank Ananjay Goel, Aditya Rachman Putra, Radityo Eko Prasojo, Amr Keleg, and Mostafa Awad for the contributions in creating the dataset. Tiago Torrent was partially funded by CNPq Research Productivity Grant number 315749/2021-0. Luis Fernando D'Haro, Mario Rodriguez-Cantelar and Marcos Estecha-Garitagoitia were supported by the European Commission through Project ASTOUND (101071191 -- HORIZON-EIC-2021- PATHFINDERCHALLENGES-01), and by project BEWORD (PID2021-126061OB-C43) funded by MCIN/AEI/10.13039/501100011033and, as appropriate, by "ERDF A way of making Europe", by the "European Union". We also thank the anonymous reviewers for their valuable feedback and suggestions that helped improve this paper.

## References

* Adilazuarda et al. [2024] M. F. Adilazuarda, S. Mukherjee, P. Lavania, S. Singh, A. Dwivedi, A. F. Aji, J. O'Neill, A. Modi, and M. Choudhury. Towards measuring and modeling "culture" in LLMs: A survey. _arXiv preprint arXiv:2403.15412_, 2024.
* Antol et al. [2015] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. VQA: Visual Question Answering. In _Proceedings of the International Conference on Computer Vision (ICCV)_, pages 2425-2433, 2015.
* Bang et al. [2023] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. _arXiv preprint arXiv:2302.04023_, 2023.
* Chayawijaya et al. [2022] S. Chayawijaya, H. Lovenia, A. F. Aji, G. Winata, B. Wilie, F. Koto, R. Mahendra, C. Wibisono, A. Romadhony, K. Vincentio, J. Santoso, D. Moeljadi, C. Wirawan, F. Hudi, M. S. Wicaksono, I. Parmonangan, I. Alfina, I. F. Putra, S. Rahmadani, Y. Oenang, A. Septiandri, J. Jaya, K. Dhole, A. Suryani, R. A. Putri, D. Su, K. Stevens, M. N. Nityasya, M. Adilazuarda, R. Hadjivaya, R. Diandaru, T. Yu, V. Ghifari, W. Dai, Y. Xu, D. Damangupta, H. Wibowo, C. Tho, I. Karo Karo, T. Faiyanosa, Z. Ji, G. Neubig, T. Baldwin, S. Ruder, P. Fung, H. Sujaini, S. Sakti, and A. Purwarianti. NusaCrowd: Open source initiative for Indonesian NLP resources. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, _Findings of the Association for Computational Linguistics: ACL 2023_, pages 13745-13818, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.868. URL https://aclanthology.org/2023.findings-acl.868.
* Carlsson et al. [2022] F. Carlsson, P. Eisen, F. Rekathati, and M. Sahlgren. Cross-lingual and multilingual CLIP. In _Proceedings of the 13th Language Resources and Evaluation Conference (LREC)_, pages 6848-6854, 2022.
* Changpinyo et al. [2023] S. Changpinyo, L. Xue, M. Yarom, A. Thapliyal, I. Szpektor, J. Amelot, X. Chen, and R. Soricut. MaXM: Towards multilingual visual question answering. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 2667-2682, 2023.
* Chen et al. [2015] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollar, and C. L. Zitnick. Microsoft COCO captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_, 2015.
* Dai et al. [2023] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In _Proceedings of NeurIPS_, 2023.
* Dwivedi et al. [2023] A. Dwivedi, P. Lavania, and A. Modi. Eticor: Corpus for analyzing llms for etiquettes. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 6921-6931, 2023.
* Gao et al. [2015] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu. Are you talking to a machine? dataset and methods for multilingual image question answering. In _Proceedings of NeurIPS_, 2015.
* Gao et al. [2021] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell, N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 2021. URL https://doi.org/10.5281/zenodo.5371628.
* Geigle et al. [2023] G. Geigle, A. Jain, R. Timofte, and G. Glavas. mBLIP: Efficient bootstrapping of multilingual vision-LLMs. _arXiv preprint arXiv:2307.06930_, 2023.
* Goyal et al. [2017] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6904-6913, 2017.
* Gupta et al. [2020] D. Gupta, P. Lenka, A. Ekbal, and P. Bhattacharyya. A unified framework for multilingual and code-mixed visual question answering. In _Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing_, pages 900-913, 2020.
* Gurari et al. [2018] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3608-3617, 2018.

* Hendrycks et al. [2021] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2021.
* Hudson and Manning [2019] D. A. Hudson and C. D. Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6700-6709, 2019.
* Jha et al. [2023] A. Jha, A. M. Davani, C. K. Reddy, S. Dave, V. Prabhakaran, and S. Dev. Seegull: A stereotype benchmark with broad geo-cultural coverage leveraging generative models. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 9851-9870, 2023.
* Kabra et al. [2023] A. Kabra, E. Liu, S. Khanuja, A. F. Aji, G. Winata, S. Chaywijaya, A. Aremu, P. Ogayo, and G. Neubig. Multi-lingual and multi-cultural figurative language understanding. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 8269-8284, 2023.
* Kunchukuttan et al. [2020] A. Kunchukuttan, D. Kakwani, S. Golla, G. N. C., A. Bhattacharyya, M. M. Khapra, and P. Kumar. Ai4bharat-indicnlp corpus: Monolingual corpora and word embeddings for indic languages, 2020.
* Leong et al. [2023] W. Q. Leong, J. G. Ngui, Y. Susanto, H. Rengarajan, K. Sarveswaran, and W. C. Tjhi. Bhasa: A holistic southeast asian linguistic and cultural evaluation suite for large language models, 2023.
* Li et al. [2023] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao. LLaVA-Med: Training a large language-and-vision assistant for biomedicine in one day. In _Proceedings of NeurIPS_, 2023.
* Li et al. [2024] C. Li, Z. Gan, Z. Yang, J. Yang, L. Li, L. Wang, and J. Gao. Multimodal foundation models: From specialists to general-purpose assistants. _Foundations and Trends(r) in Computer Graphics and Vision_, 16(1-2):1-214, 2024.
* Li et al. [2023] J. Li, D. Li, S. Savarese, and S. Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _Proceedings of the 40th International Conference on Machine Learning (ICML)_, pages 19730-19742, 2023.
* Liu et al. [2021] F. Liu, E. Bugliarello, E. M. Ponti, S. Reddy, N. Collier, and D. Elliott. Visually grounded reasoning across languages and cultures. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 10467-10485, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-man.818. URL https://aclanthology.org/2021.emnlp-man.818.
* Liu et al. [2023] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In _Proceedings of NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_, 2023.
* Liu et al. [2022] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In _Proceedings of NeurIPS_, 2023.
* Lu et al. [2022] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In _Proceedings of NeurIPS_, 2022.
* Lu et al. [2023] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. In _Proceedings of the 12th International Conference on Learning Representations (ICLR)_, 2023.
* Lyu et al. [2024] C. Lyu, M. Wu, and A. F. Aji. Beyond probabilities: Unveiling the misalignment in evaluating large language models. _arXiv preprint arXiv:2402.13887_, 2024.
* Malinowski and Fritz [2014] M. Malinowski and M. Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input. In _Proceedings of NeurIPS_, 2014.
* Malinowski et al. [2015] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neurons: A neural-based approach to answering questions about images. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, pages 1-9, 2015.
* Marino et al. [2019] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi. OK-VQA: A visual question answering benchmark requiring external knowledge. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3195-3204, 2019.
* Mathew et al. [2022] M. Mathew, V. Bagal, R. P. Tito, D. Karatzas, E. Valveny, and C. V. Jawahar. InfographicVQA. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 1697-1706, 2022.

* [35] A. Mukherjee, C. Raj, Z. Zhu, and A. Anastasopoulos. Global voices, local biases: Socio-cultural prejudices across languages. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 15828-15845, 2023.
* [36] OpenAI. GPT-4 Technical Report. _arXiv preprint arXiv:2303.08774_, 2023.
* machine translation for africa, 2020.
* [38] P. Pezeshkpour and E. Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. _arXiv preprint arXiv:2308.11483_, 2023.
* [39] J. Pfeiffer, G. Geigle, A. Kamath, J.-M. Steitz, S. Roth, I. Vulic, and I. Gurevych. xGQA: Cross-lingual visual question answering. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 2497-2511, 2022.
* [40] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 8748-8763, 2021.
* [41] J. Robinson and D. Wingate. Leveraging large language models for multiple choice question answering. In _Proceedings of the 11th International Conference on Learning Representations (ICLR)_, 2023.
* [42] H. Sharma and A. S. Jalal. A survey of methods, datasets and evaluation metrics for visual question answering. _Image and Vision Computing_, 116:104327, 2021.
* [43] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8317-8326, 2019.
* [44] J. Tang, Q. Liu, Y. Ye, J. Lu, S. Wei, C. Lin, W. Li, M. F. F. B. Mahmood, H. Feng, Z. Zhao, Y. Wang, Y. Liu, H. Liu, X. Bai, and C. Huang. MTVQA: Benchmarking multilingual text-centric visual question answering. _arXiv preprint arXiv:2405.11985_, 2024.
* [45] G. Team. Gemini: A family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [46] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiob, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [47] B. Wang, Z. Liu, X. Huang, F. Jiao, Y. Ding, A. Aw, and N. F. Chen. Saeaval for multilingual foundation models: From cross-lingual alignment to cultural reasoning, 2024.
* [48] H. A. Wibowo, E. H. Fuadi, M. N. Nityasya, R. E. Prasojo, and A. F. Aji. Copal-id: Indonesian language reasoning with local culture and nuances. _arXiv preprint arXiv:2311.01012_, 2023.
* [49] L. Xu, H. Huang, and J. Liu. SUTD-TrafficQA: A question answering benchmark and an efficient network for video reasoning over traffic events. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9878-9888, 2021.
* [50] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid. Just ask: Learning to answer questions from millions of narrated videos. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 1686-1697, 2021.
* [51] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. MM-Vet: Evaluating large multimodal models for integrated capabilities. _arXiv preprint arXiv:2308.02490_, 2023.
* [52] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. _arXiv preprint arXiv:2311.16502_, 2023.

* [53] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. GPT-4V(ision) is a generalist web agent, if grounded. _arXiv preprint arXiv:2401.01614_, 2024.
* [54] C. Zheng, H. Zhou, F. Meng, J. Zhou, and M. Huang. Large language models are not robust multiple choice selectors. In _Proceedings of the 12th International Conference on Learning Representations (ICLR)_, 2024.
* [55] Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Visual7W: Grounded Question Answering in Images. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2016.

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Section A.
* Did you include the license to the code and datasets? [No] The code and the data are proprietary.
* Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Section 5. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Section 5. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [Yes] See Section 3. 2. Did you include complete proofs of all theoretical results? [Yes] See Section 3.
3. If you ran experiments (e.g., for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See Appendix A and B 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? All the major participants are co-authors of this paper. Data collected by participants are anonymous and PIIs are removed. See Section 2. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] The project is voluntary and all collaborators have been notified of the mechanism of being co-authors before joining.

Annotation Guidelines

## 11 Multilingual Multimodal Visual Question Answering Benchmark: Annotation Guidelines

### Introduction

This document provides guidelines for annotating images and corresponding questions and answers in multiple languages to create a culturally diverse and linguistically comprehensive multimodal QA benchmark.

### Objective

To build a benchmark that represents a wide range of cultures and languages, to measure potential bias in visual AI models.

### Guidelines for Contributors

Each region and language (eg. Ecuador-Spanish) will be represented by **at most 3 annotators, in which 1 will be the team lead**. Each person is expected to provide at least 100 visual questions to be considered as a co-author. The team lead will still have to provide questions, the only difference is that the team lead is responsible to find and to organize more annotators and will manage to contact and brief that annotator, if needed.

### Image Selection:

* [noitemsep]
* Contribute images that represent diverse cultural aspects that represent the specific cultural background you're contributing to. The image must fall into one of the categories below. **Pick one of the most relevant category (more later)**:

Image Category * Vehicles and Transportation Objects, materials, clothing Cooking and food Geography, buildings, and landmarks Plants and animal Other
* Images should be relevant to your culture/country. Ensure that images are relevant to the questions being posed. In other words, the image is needed to answer the question. If the image contains the answer's text, you can blur/crop the image so that the image does not contain the answer.

Image source:

* 1. Self/personal picture (**highly preferable**). You may ask your family/friend to donate their photos, if possible.
* 2. We also accept external images from:
* Flickr: https://www.flickr.com/explore (**please make sure the associated license to the image is Creative Commons**), this can be selected at the the top left of Flickr ("Any License").
* WikimediaCommons: https://commons.wikimedia.org/wiki/Main_Page (**here you do not need to select any license for the images**),
* Unsplash: https://unsplash.com/ (**please make sure to search the image first and they select the license**: **Free**). More details (Tutorial) at the end of this document.
* Dollar Street: https://www.gapminder.org/dollar-street (**here you do not need to select any license for the images**), this webpage has images only from some countries, please make sure to select your country to find images if applicable. More detailed instructions for each web page are shown at the end of this document.

If you use an external image, you'll need to put the url of the original image.

The image must be reasonable quality (not pixelated or blurry, can be understandable). You can upload images of any ratio as long as it is not too tall or wide (e.g.: don't submit panorama pictures). Do not show personally identifiable information (PII) such as faces, car plates, house addresses, Faces of public figures or fictional characters are ok. Also, **please be sure to blur text in the image that will leak the answer**. "PicdeFace" can be used for blurring: https://picdefacer.com/en/. Tutorial on using PicdeFace is shown at the end of this document.

**Question and Answer Creation:**

After finding the image, you must now formulate 1-3 questions + answers from that image. Specifically:

* The question must be answerable **only by looking at the image.**
* Ensure that the questions are culturally relevant and specific to the image content.
* Provide answers that are concise, accurate, and directly related to the question.
* You will also need to provide 1 correct option and 3 other incorrect options (distractors). For the distractors, choose options that are relevant, not obvious wrong answers.

**The question must be answerable even without the multiple-choice.**

Example of the invalid question: ("What song is not performed by this musician" - not answerable if you don't know the choices)

Make sure the questions are **written fluently in both the local language and English.** Use a grammar checker if needed i.e. if you are not fluent in English.

Be mindful of cultural sensitivities and avoid stereotyping or misrepresenting cultural aspects.

Ensure there are **variations on your question**. Identity questions are fine, eg "What is this", or "where is this". But additionally adding more complex/difficult questions would be great. For example, multi-hop reasoning, counting, referencing, or questions that require local commonsense knowledge to be answered.

**Category Definition**

When selecting a category, pick one of the most relevant. Please follow the guideline:

**- Vehicles and Transportation:** Local public transport, local vehicles.

**- Objects, Materials, Clothing:** Questions about local/traditional clothes. Unique/local tools or items.

**- Cooking and Food:** Local dishes and food/drink. This category includes native fruits in the context of the image if that fruit is served as a food/drink.

**- Geography, Buildings, Landmarks:** Popular/common landmarks, local architecture/buildings. Local monuments.

**- Plants and Animals:** Plants and animals commonly found in the region.

**- Brands, Products, and Companies:** Questions about understanding local yet popular brands or companies. Even if the brand is about food/transportation, if the main focus of the question is the brand recognition itself, then it should be under this category.

**- Sports & Recreation:** Local sports and fun activities. Focuses on the activity itself rather than the location (in that case, it goes to the 'landmark' category).

**- Tradition, Art, History:** Local ceremonies/festivals/events, local dance/music, folklores. Historical artifacts.

**- People & Everyday Life:** Focuses on the people themselves: i.e., common habits/customs, common occupations and jobs, routine religious activities, everyday activities/routines.

- **Public Figures & Pop Culture:** Questions on the understanding of common public figures (e.g., politicians, artists, musicians, etc.). Common pop culture such as movies and games. If the category is still ambiguous to you, pick the one you think is the most appropriate.

Examples that can be improved

Make sure the image is needed to respond the question, example:

1. \(\lambda\)En quo mes se celebra esta fiesta? (In which month is this celebration held?)

Gerner

2. \(\lambda\)En quo mes se celebra la fiesta de la "Mama Negra"? (In which month is the celebration of the "Mama Negra" held?)

Wrong-As this question can be answered without looking at the image.

Make sure the question is not ambiguous:

1. Where is this monument located?

Wrong-Not specific, the answer could be a city,country, province,etc
2. In which city is this monument located?

Correct-specifically asking about the city

Make sure the question is not too vague:

1. What is this?

Question wording can be more specific
2. What is the name of this vehicle?

Correct-specifically asking about the vehicle name

Acceptable examples

Category:Tradition / Art / History - Spanish/Mexico

Quo se muestra en la imagen? (What is shown in the image?)

A. **el calcalendar aizeca/ pleda del sol (the aztec calendar/ aztec sun stone)**

B. **un serpentier azteca** (an aztec serpent)

C. **coatilcou** (coalitcou)

D. **tialoc** (dialoc)

\(\lambda\)En donde se exhibe esta pieza? (Where is this piece exhibited?)

A. **En el mueste nacional de entropologia (in the National Museum of Atrology)**

B. **en el castillo de Chapuliepec (in the Chapuliepec Castle)**

C. **En el zocalo de la ciudad de Mexico (in the Mexico City zocalo)**

D. **En Teothuacan (In Teothuacan)*** [15] Kedu mnemme ndi a na-eme? (Which ceremony are they doing?)
* [16] A. I [jba nkwy (Traditional marriage)
* [17] B. Ncheta Ommux (Birthday)
* [18] C. Emunne cheffiancy (Chiefatincy ceremony)
* [19] D. Emunne ir j bi dhtvu (New yam festival)
* [20] Kedu ebe a na-eme mnemme a? (Where is this ceremony held?)
* [21] A. Uio nna nwunye (The home of the bride's father)
* [22] B. Ahia (The market)
* [23] C. Uio nna di (The home of the groom's father)
* [24] D. Uio nse (The church)

* [25] Pada tahun berapakah foto ini diambi? (In what year is this photo taken?)

* [26] A. 2015 (2015)
* [27] B. 2020 (2020)
* [28] C. 2023 (2023)
* [29] D. 2010 (2010)
* [30] Apa nama pasukan yang ada di foto ini? (What is the name of the squad in this photo?)
* [31] A. Paskibraka (Paskibraka)
* [32] B. Brimob (Brimob)
* [33] C. TNI (TNI)
* [34] D. ABRI (ABRI)
* [35] Apa tugas utama pasukan ini? (What is the main purpose of this squad?)

* [36] A. Mengibarkan bender (Hoisting the flag)
* [37] B. Mengawal president (Escorting president)
* [38] C. Menjuga kearanan (Matitaning security)
* [39] D. Mengirjnj pengantin (Accomparning the bride and groom)

* [41] Naon kaquunan ieu hiji iat? (What is the use of this tool?)

* [42] A. Alat musik (Musical instrument)
* [43] B. Alat pertahanam diri (Self defence tool)
* [44] C. Jermura (Clothes drying equipment)
* [45] D. Alat masak (Cocking tool)
* [46] leu hij alat teh asalina ti propnisi mana di Indonesia? (This tool comes from which province in Indonesia?)
* [47] A. Jawa Barat (West Java)
* [48] B. Bali (Bali)C. Bengkulu (Bengkulu)

D. Sumatra Barat (West Sumatra)

Apakath nama bunga dalam gambar ni?

(What is the name of the flower in this picture?)

A. Pakma (Rafflesia)

B. Bunga raya (Hikiscus)

C. Anggeri (Orchid)

D. Bunga kertas (Bougainvillea)

Di rantau Asia manakah bunga it boleh ditemui?

(In which region of Asia can the flower be found?)

A. Asia Tenggra (Southeast Asia)

B. Asia Timur (East Asia)

C. Asia Selatan (South Asia)

D. Asia Tengah (Central Asia)

Opo arane wong seng hang tengah embong ki?

(What is the term for the man in the middle of the road?)

A. Polisi cepek (Polisi cepek)

B. Tukang parktr (Parking assistance man)

C. Mili (Grocer) man

D. Tukang becak (Pedcap man)

Opo seng dilakukno wong seng hang tengah dalan iku?

(What does the man in the middle of the road do?)

A. Nagatur prapatan (Managing the intersection)

B. Njaluk donasi (Asking for donations)

C. Ngawasi pelanggaaran lalu intas (Looking out for traffic violations)

D. Nunjukino arah (Showing directions)

**Category: People and everyday life - Malay/Malaysian**

Roh manakah yang disembah dengan altar ni?

(Which deity is worshiped on this altar?)

A. Datuk Gong (Na Tuk Kong)

B. Buddha (Buddha)

C. Brahma (Brhma)

D. Vishnu (Vishnu)

Apakath agama yang diamakkan oleh pengguna altar in!?

(What religion do the users of these altars practice?)

A. Taolism (Taolism)

B. Buddha (Buddshine)

C. Islam (Islam)

D. Hindu (Hinduisme)

[MISSING_PAGE_EMPTY:21]

## Appendix B Annotation Platform

We use JotForm as our annotation platform. For question entry, contributors can upload and write questions in both languages in the form. The interface can be seen in Figure 4. During validation, contributors can see all the data submitted by other contributors (Figure 5). They can select the entry to see detailed preview of the entry (Figure 6). They can then either edit the data directly, provide comments, or confirm the data by starring the entry.

## Appendix C Most-Frequent Words in the Questions

Figure 7 shows word clouds for the most frequent words in CVQA per category. We exclude stopwords as well as 'picture', 'photo', and 'image' from the list, since most questions contain these words. In this VQA context, we can treat them as stopwords.

Figure 4: Annotation interface for inputting image and questions

Figure 5: Annotation interface for validation. Contributors can comment, edit, and star the entries

## Appendix D CVQA Annotor Demographic

Figure 8 illustrates the demographic statistics of the annotators, based on an anonymous questionnaire we provided. At the time of writing, we have information for 36 out of 76 annotators. As such, this breakdown is a rough representation of the annotation group.

## Appendix E Country-Language Pairs and Scripts

In Table 8, we provide information on the script used in each Country-Language pair.

Figure 8: Annotator demographic statistics

Figure 6: During validation, contributors can preview the submission from other contributors

Figure 7: Word Cloud in CVQA per category

[MISSING_PAGE_EMPTY:25]

\begin{table}
\begin{tabular}{l l|l l|l l} \hline \hline
**Author** & **Affiliation** & **Author** & **Affiliation** & **Author** & **Affiliation** \\ \hline David Romero & MBZUAI & Chenyang Lyu & MBZUAI & Haryo Ak- & MBZUAI \\  & & & & barianto & \\  & & & & Wibowo & \\ \hline Teresa Lynn & MBZUAI & Injy Hamed & MBZUAI & Aditya Nanda & IIT Madras \\  & & & & Kishore & \\ \hline Aishik Mandal & TU Darmstadt & Alina Drag- & Universidad de la Republica & Artem Abzaiev & University of Michigan \\ \hline Atnafu Lam- & Independent & Bontu Fufa & Independent & Chenxi White- & University of Cambridge \\ \hline Christian & Universidad & Dan John Velasco & Samsung & David Ife- & Independent \\ Salamea & Politecnica & lasco & Research & duwa Adelani & Researcher \\  & Salesiana & & Philippines & & \\ \hline David Le Meur & Bretagne & Emilio Villa- & MBZUAI & Fajri Koto & MBZUAI \\  & numérique & Cueva & & & \\ \hline Fauzan Farooqui & Independent & Frederico Bel- & Federal University of Juiz de & Ganzorig Bat- & United Arab Emirates \\  & & & & & University / MBZUAI \\ \hline Gisela Vallejo & The University & Grainne & Dublin City & Guido Ivetta & Universidad \\  & of Melbourne & Caulfield & University & Nacional de \\  & & & & Cordoba \\ \hline Haiyue Song & NICT & Henok Biadg- & EAII & Hernán Maina & Universidad \\  & & & & & \\  & & & & & \\  & & & & & \\ \hline Holy Lovenia & AI Singapore & Israel Abebe & Saarland University & Jan Christian & Samsung \\  & & Azime & & Blaise Cruz & Research \\ \hline Jay Gala & MBZUAI & Jesus-German & MBZUAI & Jiahui Geng & MBZUAI \\  & & Ortiz-Barajas & & & \\ \hline Jinheon Baek & KAIST & Jocelyn Dunstan Escudero & Pontificia & Kumaranage & MBZUAI \\  & & & Universidad & Katolica de & Naroinda Yasas \\  & & & Chile & & \\ \hline Laura Alonso & Universidad & Luciana & Universidad & Luis Fernando & Universidad \\ Alemany & Nacional & Benotti & Nacional de & D’Haro & Politecnica de \\  & & & & & Madrid \\ \hline Marcelo Viridi- & Federal University of Juiz de & Marcos & Universidad & Maria Camila & University of \\  & Fora & Estecha- & Politecnica de & Buitrago Cabr- & Stuttgart \\ \hline Mario & Universidad & Melanie Joutteau & IKER, CNRS & Mihail Mi- \\ Rodriguez- & Pantelar & Madrid & & & \\ \hline Mohamed Fa- & MBZUAI & Muhammad & MBZUAI & Munkh- & Crdene & United Arab Emirates \\  & & & & & University \\ \hline \hline \end{tabular}
\end{table}
Table 9: Author affiliations

\begin{table}
\begin{tabular}{l l|l l|l l} \hline \hline
**Author** & **Affiliation** & **Author** & **Affiliation** & **Author** & **Affiliation** \\ \hline Munkhjargal & United Arab & Naome A. & Independent & Olivier NIV- & Independent \\ Gochoo & Emirates & Etori & Researcher & OMUGISHA & Researcher \\ \hline Paula Monica & Millenium Institute Foundation Reseach on Data & Pranjal Chitale & Independent & Raj Dabre & IIT Madras \\ \hline Rendi Chevi & MBZUAI & Ruochen & Brown University & Ryandito Dian- & ITB \\ \hline Samuel & HKUST & Santiago Gongora & Universidad de la Republica & Soyeong Jeong & KAIST \\ \hline Sukannya & TU Darmstadt & Tatsuki Kuribayashi & MBZUAI & Thanmay & IIT Madras \\ \hline Tiago Timponi & Federal University of Juiz de Fora, CNPq & Toqeer Ehsan & MBZUAI & Vladimir & KU Leuven \\ \hline Yova Ken- & MBZUAI & Zara Burzo & Skyline High- & Zheng Wei & The University of Melbourne \\ jhieva & & & school & Lim & The University of Melbourne \\ \hline Zheng-Xin & Brown University & Oana Ignat & University of Michigan & Joan Nwatu & University of Michigan \\ \hline Rada Mihalcea & University of Michigan & Thamar & MBZUAI & Alham Fikri & MBZUAI \\ \hline \hline \end{tabular}
\end{table}
Table 9: continued from previous page