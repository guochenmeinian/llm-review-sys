# LEACE: Perfect linear concept erasure in closed form

 Nora Belrose\({}^{1}\) David Schneider-Joseph\({}^{1}\) Shauli Ravfogel\({}^{2}\) Ryan Cotterell\({}^{3}\)

Edward Raff\({}^{1}\) Stella Biderman\({}^{1,4}\)

\({}^{1}\)EleutherAI \({}^{2}\)Bar-Ilan University \({}^{3}\)ETH Zurich \({}^{4}\)Booz Allen Hamilton

{nora,stella}@eleuther.ai david@davidsj.com

###### Abstract

Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called concept scrubbing, which erases target concept information from _every_ layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Our code is available at https://github.com/EleutherAI/concept-erasure.

## 1 Introduction

The ability to prevent a machine learning system from using a specified concept is important for fairness and interpretability. Popular notions of fairness require that protected attributes should not causally affect predictions [22, 26], and interpretability research often estimates the causal effect of a concept by attempting to remove it from a model's internal representations [10, 30, 25, 5, 18].

What it means for a model \(\mathcal{M}\) to "use" a concept Z is often vague and application-specific, but a necessary condition is that its outputs--and therefore its inputs and hidden states--should have significant _mutual information_ with Z.1**Concept erasure** leverages this fact to limit \(\mathcal{M}\)'s use of Z _without_ finetuning or inspecting its parameters. Instead, we edit the input or hidden states X used by \(\mathcal{M}\) to minimize the predictive \(\mathcal{V}\)-information \(I_{\mathcal{V}}(\mathrm{X}\to\mathrm{Z})\)[43], a tractable lower bound on the mutual information \(I(\mathrm{X};\mathrm{Z})\) which measures the degree to which classifiers from the family \(\mathcal{V}\) can predict Z. Intuitively, if no classifier in \(\mathcal{V}\) can outperform a constant function at predicting Z--a condition known as **guardedness**--then \(\mathcal{M}\) can't use Z either, at least if \(\mathcal{V}\) is expressive enough relative to \(\mathcal{M}\).

Footnote 1: This follows from the fact that causal dependence is a special kind of statistical dependence [28]. By the data processing inequality, \(\mathcal{M}\)’s output can’t have any more information about Z than its input or hidden states.

In this work, we improve upon existing concept erasure techniques using a theory-driven approach. We focus on the case where \(\mathcal{V}\) is the set of linear classifiers, and prove a previously unnoticed equivalence: a classification task is linearly guarded _if and only if_ every class has exactly the same mean feature vector (SS 3). Leveraging this equivalence, we derive a simple necessary and sufficient condition for an affine transformation to produce linearly guarded features. We then identify the unique _surgical_ transformation in this family--the one that minimizes the mean squared distance from the original features with respect to _all_ norms induced by inner products, including the popular Euclidean and Mahalanobis norms. We name it **LEAst-squares Concept Erasure (LEACE)** (SS 4).

While prior work has focused on preventing linear models from leveraging Z, we aim to erase concepts from deep neural networks as well. Interpretability research has shown that networkscan be usefully described as encoding features in linear subspaces [11; 24; 41], suggesting that fundamentally nonlinear methods may not be necessary for successful erasure in DNNs. In light of this, we introduce a simple procedure called **concept scrubbing** (SS 6), which sequentially applies LEACE to the intermediate representations at each layer of a deep network.

We empirically validate our proposals, demonstrating the superiority of LEACE for erasing gender bias from BERT representations (SS 5.2), and using concept scrubbing to measure the extent to which large language models use part-of-speech information (SS 6).

## 2 Preliminaries

Consider a \(k\)-class classification task over jointly defined random vectors \(\mathrm{X}\) (the input data) and \(\mathrm{Z}\) (the one-hot labels), with \(\mathrm{X}\) of finite first moment and taking values in \(\mathbb{R}^{d}\), and \(\mathrm{Z}\) taking values in \(\mathcal{Z}=\{\mathbf{z}\in\{0,1\}^{k}\ \big{|}\ \|\mathbf{z}\|_{1}=1\}^{2}\) with each \(\mathbb{P}(\mathrm{Z}=j)>0\). Let \(\eta(\cdot;\bm{\theta}):\mathbb{R}^{d}\to\mathbb{R}^{k}\) be a predictor chosen from a function class \(\mathcal{V}=\{\eta(\cdot;\bm{\theta})\ |\ \bm{\theta}\in\Theta\}\) (presumed to contain all constant functions) so as to minimize the expectation \(\mathbb{E}\big{[}\mathcal{L}(\eta(\mathrm{X}),\mathrm{Z})\big{]}\) of some \(\mathcal{L}:\mathbb{R}^{k}\times\mathcal{Z}\to[0,\infty)\) in a class \(\mathfrak{L}\) of loss functions.

We borrow the concept of **guardedness** from Ravfogel et al. [31], who define it in terms of \(\mathcal{V}\)-information [43]. We opt for a slightly more general definition here, which is equivalent to theirs in the case of cross-entropy loss (see Appendix G).

**Definition 2.1** (Guardedness).: _Let \(\mathrm{X}\), \(\mathrm{Z}\), \(\mathcal{V}\), and \(\mathfrak{L}\) be as defined above, and let \(\chi\) be the set of all random vectors of finite first moment taking values in \(\mathbb{R}^{d}\), jointly defined with \(\mathrm{Z}\)._

_We say \(\mathrm{X}\ (\mathcal{V},\mathfrak{L})-\)**guards**\(\mathrm{Z}\) if, for all losses \(\mathcal{L}\in\mathfrak{L}\), it maximizes the minimum expected loss:_

\[\mathrm{X}\in\operatorname*{argmax}_{\mathrm{X}^{\prime}\in\chi}\ \inf_{\bm{\theta}\in\Theta}\ \mathbb{E}\Big{[}\mathcal{L}(\eta(\mathrm{X}^{\prime};\bm{\theta}),\mathrm{Z} )\Big{]}.\]

_In other words, its conditional distribution \(\mathbb{P}(\mathrm{X}\ |\ \mathrm{Z}=\cdot)\) is among the worst possible distributions for predicting \(\mathrm{Z}\) from \(\mathrm{X}\) using a predictor of the form \(\eta(\cdot;\bm{\theta})\in\mathcal{V}\) and a loss function in \(\mathfrak{L}\)._

**Definition 2.2** (Trivially Attainable Loss).: _The **trivially attainable loss** for labels \(\mathrm{Z}\) and loss \(\mathcal{L}\) is the lowest possible expected loss available to a constant predictor \(\eta(\mathbf{x})=\mathbf{b}\):_

\[L_{\tau}=\inf_{\mathbf{b}\in\mathbb{R}^{k}}\mathbb{E}[\mathcal{L}(\mathbf{b}, \mathrm{Z})]\]

_We will sometimes write it \(L_{\tau}^{(\mathcal{Z},\mathcal{L})}\) in cases of possible ambiguity. If there is a specific constant predictor actually achieving this loss, we call it the **trivial predictor**\(\eta_{\tau}=\eta_{\tau}^{(\mathcal{Z},\mathcal{L})}\)._

We examine this problem in the important case of loss functions \(\mathcal{L}:\mathbb{R}^{k}\times\mathcal{Z}\to[0,\infty)\) which are convex in the prediction \(\eta(\mathbf{x})\), and linear predictors that take the functional form \(\eta(\mathbf{x};\mathbf{b},\mathbf{W})=\mathbf{b}+\mathbf{W}\mathbf{x}\), for some bias \(\mathbf{b}\in\mathbb{R}^{k}\) and weight matrix \(\mathbf{W}\in\mathbb{R}^{k\times d}\).

**Definition 2.3** (Linear Guardedness).: _If \(\mathrm{X}\ (\mathcal{V},\mathfrak{L})\)-guards \(\mathrm{Z}\), where \(\mathfrak{L}\) is the class of nonnegative loss functions which are convex in their first argument, and \(\mathcal{V}\) is the class of linear predictors \(\eta(\mathbf{x})=\mathbf{b}+\mathbf{W}\mathbf{x}\), we say that \(\mathrm{X}\)**linearly guards**\(\mathrm{Z}\)._

## 3 Theoretical Results

Our primary theoretical result is that the following conditions are all equivalent:

1. The data \(\mathrm{X}\) linearly guards the labels \(\mathrm{Z}\). (Definition 2.3)
2. For all convex losses \(\mathcal{L}\), the trivially attainable loss is optimal on \((\mathrm{X},\mathrm{Z})\). (Definition 2.2)
3. The class-conditional mean vectors \(\mathbb{E}[\mathrm{X}\ |\ \mathrm{Z}=i]\) are equal to the unconditional mean \(\mathbb{E}[\mathrm{X}]\).
4. Every component of \(\mathrm{X}\) has zero covariance with every component of \(\mathrm{Z}\).
5. Every linear classifier evaluated on \(\mathrm{X}\) exhibits statistical parity w.r.t. \(\mathrm{Z}\). (App. C)

The equivalence of conditions 1, 2, and 5 is relatively straightforward to show, and the relevant theorems can be found in Appendices B and C. The other equivalences are proven below (cond. 3 \(\leftrightarrow\) cond. 2 in SS 3.1 and SS 3.2); cond. 3 \(\leftrightarrow\) 4 in SS 3.3).

### Equality of Class Centroids Implies Linear Guardedness

The following result establishes the implication from condition 3 to condition 2.

**Theorem 3.1**.: _Suppose \(\mathcal{L}\) is convex in the linear prediction \(\eta\). Then if each class-conditional mean \(\mathbb{E}\big{[}\mathrm{X}\mid\mathrm{Z}=i\big{]}\) is equal to \(\mathbb{E}\big{[}\mathrm{X}\big{]}\), the trivially attainable loss cannot be improved upon._

Proof.: Let \(\eta(\mathbf{x})=\mathbf{b}+\mathbf{W}\mathbf{x}\) be any linear predictor. By Jensen's inequality,3 the loss with \(\eta\) evaluated on \(\mathrm{X}\) is lower bounded by the loss with \(\eta\) evaluated on the unconditional mean of the data \(\mathbb{E}\big{[}\mathrm{X}\big{]}\):

Footnote 3: Specifically, its generalization to convex functions over \(\mathbb{R}^{k}\). See [12] p. 76.

\[\mathbb{E}\Big{[}\mathcal{L}(\eta,\mathrm{Z})\Big{]} =\mathbb{E}_{\mathrm{Z}}\Big{[}\mathbb{E}\Big{[}\mathcal{L}(\eta, \mathrm{Z})\big{|}\mathrm{Z}\Big{]}\Big{]}\] \[\geq\mathbb{E}_{\mathrm{Z}}\Big{[}\mathcal{L}\Big{(}\mathbb{E} \big{[}\eta\big{|}\mathrm{Z}\big{]},\mathrm{Z}\Big{)}\Big{]}\] (Jensen's inequality) \[=\mathbb{E}_{\mathrm{Z}}\Big{[}\mathcal{L}\Big{(}\mathbf{b}+ \mathbf{W}\mathbb{E}\big{[}\mathrm{X}\big{|}\mathrm{Z}\big{]},\mathrm{Z}\Big{)} \Big{]}\] (linearity of \[\eta\] ) \[=\mathbb{E}_{\mathrm{Z}}\Big{[}\mathcal{L}\Big{(}\mathbf{b}+ \mathbf{W}\mathbb{E}\big{[}\mathrm{X}\big{]},\mathrm{Z}\Big{)}\Big{]}.\] (by assumption)

This in turn is the loss of the constant predictor \(\eta^{\prime}(\mathbf{x})=\mathbf{b}+\mathbf{W}\mathbb{E}\big{[}\mathrm{X}\big{]}\). Since the trivially attainable loss is the best that can be achieved by a constant predictor, and _every_ predictor's loss is lower bounded by that of some constant predictor, we cannot improve upon the trivially attainable loss. 

Intuitively, this shows that the classifier's expected loss is lower-bounded by the loss it would receive if each data point were replaced with the centroid of its class. But, if these centroids are all equal, the loss can't be any lower than what we'd get if every data point were replaced with the _global_ mean \(\mathbb{E}[\mathrm{X}]\). In that case, the data points are indistinguishable and we can't do better than \(\mathbf{W}=\mathbf{0}\).

### Linear Guardedness Implies Equality of Class Centroids

We now prove the implication from condition 2 to condition 3. Condition 2 applies when the trivially attainable loss is optimal for _all_ convex losses, including cross-entropy loss in particular. And if it holds for cross-entropy loss, we now show that condition 3--the class centroids are equal--must follow. First a more general lemma:

**Lemma 3.2**.: _Suppose \(\mathcal{L}\) has bounded partial derivatives, which when off-category never vanish and do not depend on the category, i.e. \(\partial\mathcal{L}(\eta,z_{1})/\partial\eta_{i}=\partial\mathcal{L}(\eta,z_{2 })/\partial\eta_{i}\neq 0\) for all categories \(z_{1},z_{2}\neq i\). If \(\mathbb{E}\big{[}\mathcal{L}(\eta,\mathrm{Z})\big{]}\) is minimized among linear predictors by the constant predictor \(\eta(\mathbf{x})=\mathbf{b}^{*}+\mathbf{W}^{*}\mathbf{x}\) with \(\mathbf{W}^{*}=\mathbf{0}\), then each class-conditional mean \(\mathbb{E}\big{[}\mathrm{X}|\mathrm{Z}=i\big{]}\) is equal to \(\mathbb{E}\big{[}\mathrm{X}\big{]}\)._

Proof.: The first-order optimality condition on the \(i^{\text{th}}\) component of our parameters \(\mathbf{b}\) and \(\mathbf{W}\) yields the equations:

\[\mathbb{E}\Bigg{[}\frac{\partial\mathcal{L}(\eta,\mathrm{Z})}{\partial\eta_{i }}\cdot\frac{\partial\eta_{i}}{\partial b_{i}}\Bigg{]}=0\quad\text{and}\quad \mathbb{E}\Bigg{[}\frac{\partial\mathcal{L}(\eta,\mathrm{Z})}{\partial\eta_{i }}\cdot\frac{\partial\eta_{i}}{\partial\mathbf{W}_{1}}\Bigg{]}=\mathbf{0},\] (1)

where we have used the boundedness of \(\mathcal{L}\)'s partial derivative and the finite first moment of \(\frac{\partial\eta_{i}}{\partial b_{i}}=1\) and \(\frac{\partial\eta_{i}}{\partial\mathbf{W}_{1}}=\mathrm{X}\) to justify (via the Dominated Convergence Theorem) interchanging the derivative with the expectation.

Since \(\eta\) is constant over all values of \(\mathrm{X}\), and \(\frac{\partial\eta_{i}}{\partial b_{i}}=1\), the first equation in (1) reduces to:

\[\mathbb{P}(\mathrm{Z}=i)\frac{\partial\mathcal{L}(\eta,i)}{\partial\eta_{i}}+ \mathbb{P}(\mathrm{Z}\neq i)\frac{\partial\mathcal{L}(\eta,\neq i)}{\partial \eta_{i}}=0,\] (2)

where \(\frac{\partial\mathcal{L}(\eta,i\neq i)}{\partial\eta_{i}}\) is an abuse of notation denoting the off-category partial derivative, emphasizing its independence of the category \(\mathrm{Z}\).

Similarly, the constancy of \(\eta\) and the fact that \(\frac{\partial\eta_{i}}{\partial\mathbf{W}_{1}}=\mathrm{X}\) reduces the second equation in (1) to:

\[\mathbb{P}(\mathrm{Z}=i)\frac{\partial\mathcal{L}(\eta,i)}{\partial\eta_{i}} \cdot\mathbb{E}\big{[}\mathrm{X}\big{|}\mathrm{Z}=i\big{]}+\mathbb{P}(\mathrm{ Z}\neq i)\frac{\partial\mathcal{L}(\eta,\neq i)}{\partial\eta_{i}}\cdot \mathbb{E}\big{[}\mathrm{X}\big{|}\mathrm{Z}\neq i\big{]}=\mathbf{0}.\] (3)

Solving for \(\mathbb{P}(\mathrm{Z}=i)\frac{\partial\mathcal{L}(\eta,i)}{\partial\eta_{i}}\) in (2) and substituting in (3) gives us:

\[\mathbb{P}(\mathrm{Z}\neq i)\frac{\partial\mathcal{L}(\eta,\neq i)}{\partial \eta_{i}}\cdot\left(\mathbb{E}\big{[}\mathrm{X}\big{|}\mathrm{Z}\neq i\big{]}- \mathbb{E}\big{[}\mathrm{X}\big{|}\mathrm{Z}=i\big{]}\right)=\mathbf{0}.\]

If \(\mathbb{P}(\mathrm{Z}\neq i)=0\), then \(\mathbb{E}[\mathrm{X}]=\mathbb{E}[\mathrm{X}|\mathrm{Z}=i]\) is trivially true. Otherwise, using the non-vanishingness of the off-category partial derivative \(\frac{\partial\mathcal{L}(\eta,\neq i)}{\partial\eta_{i}}\), division yields the equivalence of \(\mathbb{E}\big{[}\mathrm{X}\big{|}\mathrm{Z}=i\big{]}\) to \(\mathbb{E}\big{[}\mathrm{X}\big{|}\mathrm{Z}\neq i\big{]}\), and hence to the unconditional mean \(\mathbb{E}\big{[}\mathrm{X}\big{]}\). 

We now show that Lemma 3.2 applies to the widely used cross entropy loss:

**Theorem 3.3**.: _If the class probabilities \(\mathbb{P}(\mathrm{Z}=j)\) are all nonzero, and the trivially obtainable loss is optimal when \(\mathcal{L}(\eta,z)=-\log\frac{\exp(\eta_{i})}{\sum_{i=1}^{k}\exp(\eta_{i})}\), then each class has the same mean \(\mathbb{E}\big{[}\mathrm{X}\big{|}\mathrm{Z}=z\big{]}\)._

Proof.: In this case, the trivial predictor \(\eta_{r}(\mathrm{Z})_{j}=\log(\mathbb{P}(\mathrm{Z}=j))\) exists, achieving the trivially obtainable loss, which we have assumed optimal. Furthermore, \(\mathcal{L}\) has on-category partial derivative \(\partial\mathcal{L}(\eta,i)/\partial\eta_{i}=\exp(\eta_{i})/\sum_{j=1}^{k}\exp (\eta_{j})-1\in(-1,0]\), and nonvanishing off-category partial derivative \(\partial\mathcal{L}(\eta,\neq i)/\partial\eta_{i}=\exp(\eta_{i})/\sum_{j=1}^{k} \exp(\eta_{j})\in(0,1)\), both bounded, so the conditions of Lemma 3.2 apply. 

### Linearly Guarded Labels Have Zero Covariance with the Features

The next theorem establishes the equivalence of conditions 3 and 4.

**Theorem 3.4**.: _Let \(\mathrm{X}\) be a random vector taking values in \(\mathbb{R}^{d}\) with finite first moment, and \(\mathrm{Z}\) a random vector taking values in \(\{0,1\}^{k}\) with one-hot encoding, with each class probability \(\mathbb{P}(\mathrm{Z}=j)\) being nonzero. Then the class-conditional means \(\mathbb{E}[\mathrm{X}|\mathrm{Z}=j]\) are all equal to the unconditional mean \(\mathbb{E}[\mathrm{X}]\) if and only if every component of \(\mathrm{X}\) has zero covariance with every component of \(\mathrm{Z}\), i.e. the cross-covariance matrix \(\mathbf{\Sigma}_{\mathrm{XZ}}\), whose \((i,j)^{\text{th}}\) entry is \(\mathrm{Cov}(\mathrm{X}_{i},\mathrm{Z}_{j})\), is the zero matrix._

Proof.: Since \(\mathrm{Z}\) is one-hot, we can rewrite the \((i,j)^{\text{th}}\) entry of \(\mathbf{\Sigma}_{\mathrm{XZ}}\) as:

\[\mathbb{E}[\mathrm{X}_{i}\mathrm{Z}_{j}]-\mathbb{E}[\mathrm{X}_{i}]\mathbb{E} [\mathrm{Z}_{j}]=\mathbb{P}(\mathrm{Z}=j)\Big{(}\mathbb{E}[\mathrm{X}_{i}| \mathrm{Z}=j]-\mathbb{E}[\mathrm{X}_{i}]\Big{)}.\]

As \(\mathbb{P}(\mathrm{Z}=j)>0\), it follows that \(\mathbb{E}[\mathrm{X}_{i}|\mathrm{Z}=j]=\mathbb{E}[\mathrm{X}_{i}]\) if and only if \(\mathrm{Cov}(\mathrm{X}_{i},\mathrm{Z}_{j})=0\). 

We have thus established the equivalence of the first four conditions stated earlier. See Appendix C for the last one, on statistical parity.

## 4 Least-Squares Concept Erasure

In Section 3 we saw that \(\mathrm{X}\) linearly guards \(\mathrm{Z}\) if and only if each component of \(\mathrm{X}\) has zero covariance with each component of \(\mathrm{Z}\). We will now characterize the set of affine transformations \(r(\mathbf{x})=\mathbf{P}\boldsymbol{x}+\mathbf{b}\) such that \(r(\mathrm{X})\) linearly guards \(\mathrm{Z}\).

**Theorem 4.1**.: _Let \(\mathrm{X}\) and \(\mathrm{Z}\) be random vectors taking values in \(\mathbb{R}^{d}\) and \(\mathbb{R}^{k}\) respectively, with \(\mathrm{X}\) of finite first moment. Then given some affine function \(r(\boldsymbol{x})=\mathbf{P}\boldsymbol{x}+\mathbf{b}\), the modified random vector \(r(\mathrm{X})\) linearly guards \(\mathrm{Z}\) if and only if the columns of the cross-covariance matrix \(\mathbf{\Sigma}_{\mathrm{XZ}}\) are contained in the null space of \(\mathbf{P}\)._

Proof.: From Theorem 3.4 we know that \(r(\mathrm{X})\) linearly guards \(\mathrm{Z}\) if and only if \(\mathrm{Cov}(r(\mathrm{X}),\mathrm{Z})\) is the zero matrix. By the linearity property of cross-covariance, we have:

\[\mathrm{Cov}(r(\mathrm{X}),\mathrm{Z})=\mathrm{Cov}(\mathbf{P}\mathrm{X}+ \mathbf{b},\mathrm{Z})=\mathbf{P}\mathrm{Cov}(\mathrm{X},\mathrm{Z})=\mathbf{P} \mathbf{\Sigma}_{\mathrm{XZ}}.\]

Therefore, \(r(\mathrm{X})\) linearly guards \(\mathrm{Z}\) if and only if \(\mathrm{ker}(\mathbf{P})\supseteq\mathrm{colsp}(\mathbf{\Sigma}_{\mathrm{XZ}})\).

**Implications for prior work.** Notably, the above theorems imply that three previously proposed methods in the literature, Spectral Attribute Removal (SAL) [37], Mean Projection [17], and Fair PCA [20], are guaranteed to achieve linear guardedness given suitable hyperparameters. See Appendix D for further discussion.

### Derivation of LEACE

Theorem 4.1 is a very weak condition, which is far from identifying unique values for \(\mathbf{P}\) and \(\mathbf{b}\). In most applications, however, we'd like to make a "small" edit to \(\mathrm{X}\) so that useful information contained in \(\mathrm{X}\) is maximally preserved. We operationalize the notion of a small edit in terms of the mean squared norm \(\mathbb{E}\|r(\mathrm{X})-\mathrm{X}\|_{\mathbf{M}}^{2}\) defined by some positive-definite inner product \(\mathbf{M}\),4 which can be thought of as a local quadratic approximation to _any_ measure of divergence between \(\mathrm{X}\) and \(r(\mathrm{X})\) (such as Kullback-Leibler divergence, for example). While we are primarily interested in the Euclidean (\(\mathbf{M}=\mathbf{I}\)) and Mahalanobis (\(\mathbf{M}=\mathbf{\Sigma}_{\mathrm{XX}}^{+}\)) norms, it will turn out that there is a _single_ erasure function that minimizes _all_ such norms simultaneously. We will see in Section 6 that ensuring edits are small in this sense provides substantial benefit to downstream task performance as compared to other methods which also guard the labels \(\mathrm{Z}\).

Footnote 4: Our proofs also include degenerate “inner products” where \(\mathbf{M}\) is singular, and the associated seminorms.

Below, we derive the optimal eraser under the assumption that \(\mathrm{X}\) and \(\mathrm{Z}\) are centered.

**Theorem 4.2**.: _Let \(\mathrm{X}\) and \(\mathrm{Z}\) be centered random vectors taking values in \(\mathbb{R}^{d}\) and \(\mathbb{R}^{k}\) respectively, each of finite second moment. Let \(\mathbf{M}\in\mathbb{R}^{d\times d}\) be a p.s.d. matrix defining a (possibly degenerate) inner product on \(\mathbb{R}^{d}\): \(\langle\mathbf{x},\mathbf{y}\rangle_{\mathbf{M}}=\mathbf{x}^{T}\mathbf{M} \mathbf{y}\). Let \(\mathbf{\Sigma}_{\mathrm{XX}}\in\mathbb{R}^{d\times d}\) be \(\mathrm{X}\)'s covariance matrix, and \(\mathbf{\Sigma}_{\mathrm{XZ}}\in\mathbb{R}^{d\times k}\) be the cross-covariance matrix of \(\mathrm{X}\) and \(\mathrm{Z}\). Let \(\mathbf{A}^{+}\) denote the Moore-Penrose pseudoinverse of a matrix \(\mathbf{A}\), and let \(\mathbf{A}^{1/2}\) be the p.s.d. square root of a p.s.d. matrix \(\mathbf{A}\). Then the objective_

\[\operatorname*{argmin}_{\mathbf{P}\in\mathbb{R}^{d\times d}}\mathbb{E}\left[ \left\|\mathbf{P}\mathrm{X}-\mathrm{X}\right\|_{\mathbf{M}}^{2}\right] \quad\mathrm{subject\ to\ \ Cov}(\mathbf{P}\mathrm{X},\mathrm{Z})=\mathbf{0}\]

_has the following solution:_

\[\mathbf{P}^{*}=\mathbf{I}-\mathbf{W}^{+}\mathbf{P}_{\mathbf{W}\mathbf{\Sigma }_{\mathrm{XZ}}}\mathbf{W},\]

_where \(\mathbf{W}\) is the whitening transformation \((\mathbf{\Sigma}_{\mathrm{XX}}^{1/2})^{+}\) and \(\mathbf{P}_{\mathbf{W}\mathbf{\Sigma}_{\mathrm{XZ}}}=(\mathbf{W}\mathbf{ \Sigma}_{\mathrm{XZ}})(\mathbf{W}\mathbf{\Sigma}_{\mathrm{XZ}})^{+}\) is the orthogonal projection matrix onto \(\mathrm{colsp}(\mathbf{W}\mathbf{\Sigma}_{\mathrm{XZ}})\)._

Proof.: See Appendices E.1 and E.2 for two independent proofs of Theorem 4.2. 

The above theorem assumes that the random vectors \(\mathrm{X}\) and \(\mathrm{Z}\) are centered, and does not include a bias term. Below we extend our results to the uncentered case, and derive the optimal bias \(\mathbf{b}^{*}\).

**Theorem 4.3**.: _Let \(\mathrm{X}\) and \(\mathrm{Z}\) be random vectors taking values in \(\mathbb{R}^{d}\) and \(\mathbb{R}^{k}\) respectively, each of finite second moment. Define \(\mathbf{M}\) and \(\mathbf{P}^{*}\) as in Theorem 4.2 and \(\mathbf{b}^{*}=\mathbb{E}[\mathrm{X}]-\mathbf{P}^{*}\mathbb{E}[\mathrm{X}]\). Then \((\mathbf{P}^{*},\mathbf{b}^{*})\) minimizes \(\mathbb{E}\big{\|}\mathbf{P}\mathrm{X}+\mathbf{b}-\mathrm{X}\big{\|}^{2}\), subject to \(\mathrm{Cov}(\mathbf{P}\mathrm{X}+\mathbf{b},\mathrm{Z})=\mathbf{0}\)._

Proof.: Let \(\mathbf{P}\in\mathbb{R}^{d\times d}\) and define \(\tilde{\mathrm{X}}=\mathrm{X}-\mathbb{E}[\mathrm{X}]\) and \(\mathbf{c}=\mathbf{P}\mathbb{E}[\mathrm{X}]+\mathbf{b}-\mathbb{E}[\mathrm{X}]\). Then,

\[\mathbb{E}\big{\|}\mathbf{P}\mathrm{X}+\mathbf{b}-\mathrm{X} \big{\|}_{\mathbf{M}}^{2} =\mathbb{E}\big{\|}(\mathbf{P}\tilde{\mathrm{X}}-\tilde{\mathrm{ X}})+\mathbf{c}\big{\|}_{\mathbf{M}}^{2}\] \[=\mathbb{E}\big{\|}\mathbf{P}\tilde{\mathrm{X}}-\tilde{\mathrm{ X}}\big{\|}_{\mathbf{M}}^{2}+2\mathbb{E}\big{[}\mathbf{P}\tilde{\mathrm{X}}- \tilde{\mathrm{X}}\big{]}^{T}\mathbf{M}\mathbf{c}+\mathbf{c}^{T}\mathbf{M} \mathbf{c}\] \[=\mathbb{E}\big{\|}\mathbf{P}\tilde{\mathrm{X}}-\tilde{\mathrm{ X}}\big{\|}_{\mathbf{M}}^{2}+\mathbf{c}^{T}\mathbf{M}\mathbf{c},\]

where we have eliminated the middle term because \(\mathbf{P}\) is linear and \(\mathbb{E}[\tilde{\mathrm{X}}]=0\). Since \(\mathbf{M}\) is p.s.d., our objective is minimized for \(\mathbf{c}=\mathbf{0}\), i.e. \(\mathbf{b}=\mathbb{E}[\mathrm{X}]-\mathbf{P}\mathbb{E}[\mathrm{X}]\). The problem thus reduces to choosing \(\mathbf{P}\) so as to minimize \(\mathbb{E}\big{\|}\mathbf{P}\tilde{\mathrm{X}}-\tilde{\mathrm{X}}\big{\|}_{ \mathbf{M}}^{2}\) subject to \(\mathrm{Cov}(\mathbf{P}\mathrm{X}+\mathbf{b},\mathrm{Z})=\mathrm{Cov}( \mathbf{P}\tilde{\mathrm{X}},\mathrm{Z})=\mathbf{0}\), which Theorem 4.2 shows occurs when \(\mathbf{P}=\mathbf{P}^{*}\). 

[MISSING_PAGE_FAIL:6]

compare our intervention with RLACE [32], which uses gradient-based optimization to solve a linear concept-erasure adversarial game.

Concept erasure results.First, we evaluate the ability of logistic regression classifiers to recover the removed information. The results, presented in Fig. 2, show that our method is the only to achieve random accuracy (perfect erasure) with a small edit, although RLACE (but not INLP) comes close. At the same time, our method is around 2 orders of magnitude faster, and does not require gradient-based optimization.

### Downstream Fairness

How does our intervention affect the behavior of the model on the main classification task of profession prediction? We fit a logistic regression profession-prediction classifier over the projected [CLS] representations.

To measure the bias in a classifier, we follow De-Arteaga et al. [6] and use the TPR-GAP measure, which quantifies the bias in a classifier by considering the difference (GAP) in the true positive rate (TPR) between individuals with different protected attributes (e.g. race or gender). We use the notation \(\mathrm{GAP}_{z,y}^{\mathrm{TPR}}\) to denote the TPR-gap in some main-class label \(y\) (e.g. "unverse prediction) for some protected group \(z\) (e.g. "female"), we also consider \(\mathrm{GAP}_{z}^{\mathrm{TPR,RMS}}\), the RMS of the TPR-gap across all professions for a protected group \(z\):

\[GAP_{z}^{\mathrm{TPR,RMS}}=\sqrt{\frac{1}{|C|}\sum_{y\in C}(GAP_{z,y}^{ \mathrm{TPR}})^{2}}\]

To calculate the relation between the bias the model exhibits and the bias in the data, we also calculate \(\sigma_{(\mathrm{GAP}^{\mathrm{TPR}},\%\mathrm{Women})}\), the correlation between the TPR gap in a given profession and the percentage of women in that profession.

Results.The main-task classifier achieves profession-prediction accuracy of 77.3% on the projected representations (compared with 79.3% over the original representations), indicating that the intervention minimally affects the ability to predict the profession of a person from the representation of their biography. At the same time, the TPR gap drops significantly from 0.198 to 0.084, indicating a sharp drop in the biased behavior of the profession classifier. Indeed, inspecting the correlation \(\sigma_{(\mathrm{GAP}^{\mathrm{TPR}},\%\mathrm{Women})}\) between the gap (per profession) and the representation of women in

Figure 3: The correlation between \(GAP_{female,y}^{TPR}\) and the relative proportion of women in profession \(y\), for BERT representation, before (left; R=0.867) and after (right; R=0.392) the projection.

Figure 2: Gender prediction accuracy after bias-removal projection versus the mean squared distance from the original representation for INLP, RLACE, and LEACE on BERT representations.

this profession, we see that this correlation plummets from 0.867 to 0.392 after erasure. Re-fitting the main-task logistic regression classifier over the projected representations yields a slightly higher main-task accuracy of 78.1%, at the price of significantly increasing the TPR gap to 0.158.5

Footnote 5: The softmax probabilities of a multiclass logistic regression classifier can leak the removed information if _another_ classifier is stacked on top of it [31], though this setup is not linear.

### Revisiting Amnesic Probing

Elazar et al. [10] have introduced the idea of _amnesic probing_ as a causal intervention that aims to test the importance of a given concept (e.g. part-of-speech tag) to some main task (e.g. language modeling). They applied Iterative Nullspace Projection (INLP) to remove different concepts from the hidden representations of the model, and assessed the degree to which its behavior changed when performing masked language modeling. Since INLP often requires dozens of iterations to completely erase the concept, its usage in this context raises concerns of collateral damage due to magnitude of the intervention and the non-exhaustive nature of INLP removal. Here, we replicate their experiments on the bert-base-uncased model with our interventions.

Experimental setup.We use part-of-speech (POS) tags as our concept of interest. We collect sentences and their coarse POS tags ("Noun", "Verb" etc.; 18 in total) from the English Universal Dependencies dataset [27]. We tokenize the sentences with the BERT tokenizer and map each wordpiece to the POS tag of the word to which it belongs. We collect the unmasked BERT representations for each layer, intervene to linearly erase the POS concept from that layer, and continue the forward pass until the last layer, from which we compute the distribution of the MLM over the vocabulary. Note that in each experiment we intervene on a single layer. We quantify the decrease in accuracy following the intervention, as well as the increase in the loss. We compare with a baseline intervention of a random orthogonal projection whose null space has the same rank as the label space (18). For INLP, we perform 20 iterations. This is needed because INLP does not effectively remove the concept; even after 20 iterations, classification accuracy is above majority accuracy. As a result, INLP reduces the rank of the representation by 360. By contrast, our method decreases the rank just by 17.

Results.The results are shown in Fig. 3(b). Our intervention only mildly changes BERT LM accuracy and loss until layer 8, with the highest drop recorded in layer 11. INLP, in contrast, shows maximum effect at layer 6. Since it removes hundreds of dimensions, it is difficult to attribute this effect to the erasure of the concept. These results suggest that the _causal_ effect of the POS concept on the language model is concentrated in layer 11. Interestingly, this stands in contrast with POS linear probing results, which are optimal at earlier layers [38]. As Elazar et al. [10] have noted, probing does not generally correlate with intervention-based analysis techniques.

Figure 4: Amnesic probing results on bert-base-uncased.

## 6 Concept Scrubbing

Unfortunately, Elazar et al. [10] were forced to limit their interventions to a single layer due to the limitations of INLP. INLP often requires the deletion of several dozen dimensions before linear guarding is achieved--as demonstrated in Figure 2. Kumar et al. [21] show empirically and theoretically that INLP causes needless "collateral damage" to useful parts of the representation that are orthogonal to the concept being erased. Because of this collateral damage, it's impossible to apply INLP to multiple layers of a transformer without causing its outputs to collapse into gibberish.

Instead, we would like to erase all linear information about a concept in _every_ intermediate representation, which we term **concept scrubbing**. LEACE makes concept scrubbing possible and eminently practical. It causes minimal collateral damage, induces little computational overhead, and the covariance statistics it relies on can be computed in a _streaming_ fashion, without ever storing all the hidden states in memory or on disk.

**Algorithm.** Any intervention on the model at layer \(\ell\) changes the distribution of hidden states at layers \(\ell^{\prime}>\ell\). Because of this, the naive approach of independently fitting LEACE parameters \((\mathbf{P},\mathbf{b})\) for all layers of the clean model, then applying them all at once, may fail to fully erase the target concept. Instead, we fit LEACE parameters _sequentially_, starting from the first layer and proceeding to the final layer. After we compute \((\mathbf{P},\mathbf{b})\) for a layer, we immediately use them to scrub the hidden states for that layer, then feed these scrubbed representations to the next layer (Algorithm 1).

### Experimental Details

**Dataset.** For each model family, we use a sample from the respective pretraining distribution: the validation split of the Pile [13] for the Pythia models [2], and the RedPajama replication of the LLaMA pretraining corpus for the LLaMA family [39]. sampling a slice of \(2^{22}\) tokens for fitting the LEACE parameters and another slice of \(2^{22}\) tokens for evaluation. Since neither corpus comes with part-of-speech tags, we use the model from the SpaCy library [19] to automatically generate Universal Dependency tags [23].

**Baseline method.** We also run concept scrubbing using full-rank SAL [37], which is similar to our method but lacks a bias term and does not adjust for correlations between features (Appendix D).

**Architecture.** We focus on autoregressive language models. We evaluate our method on EleutherAI's Pythia 160M, 1.4B, 6.9B, and 12B models [2], and Meta's LLaMA 7B, 13B, and 30B [39]. We apply concept erasure to the input of each transformer block, immediately after normalization is applied (LayerNorm or RMSNorm).

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{3}{c}{LLaMA} & \multicolumn{3}{c}{Pythia} \\ \cline{2-7} Condition & 7B & 13B & 30B & 160M & 1.4B & 6.9B & 12B \\ \hline No intervention & 0.69 & 0.66 & 0.62 & 0.90 & 0.70 & 0.64 & 0.62 \\ Random erasure & 0.69 & 0.66 & 0.62 & 0.99 & 0.72 & 0.66 & 0.63 \\ \hline LEACE & 1.73 & 1.84 & 1.96 & 2.79 & 2.25 & 3.57 & 3.20 \\ SAL & 3.24 & 3.26 & 3.16 & 3.53 & 3.44 & 4.17 & 4.69 \\ \hline unigram entropy & 2.90 & 2.90 & 2.90 & 2.66 & 2.66 & 2.66 & 2.66 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Perplexity in autoregressive language models when removing linearly available part-of-speech information from the input to each transformer layer. Units are bits per UTF-8 byte. The unigram baseline assigns probabilities to tokens based only on their frequency and not on the context.

**Randomized erasure.** Almost any intervention on a neural network will cause its performance to degrade to some extent. Following Elazar et al. [10], we isolate the effect of the concept erasure by comparing it to a control condition in which we orthogonally project onto a _random_ linear subspace of the same rank as the cross-covariance matrix. To reduce the variance of our results, we sample a fresh subspace for each minibatch, and erase that subspace at each layer, reporting the cross-entropy loss averaged over subspaces.

**Training efficiency.** Algorithm 1 avoids redundant computation by caching the layer \(i\) hidden states for _every_ data point, then using them to run layer \(i+1\). This approach has the downside of requiring a large amount of memory or disk space during training (up to 500GB in our experiments). It's possible to avoid caching any hidden states and instead recompute them as needed, at the expense of increasing the total compute cost from \(O(\ell)\) to \(O(\ell^{2})\).

### Results

We find strong evidence that autoregressive language models heavily rely on linearly encoded part-of-speech information. While erasing a randomly selected subspace has little to no effect on language modeling performance, scrubbing away part-of-speech information induces a large increase in perplexity across all models (Table 1).

The specific numbers, however, depend on the erasure method used: SAL induces significantly larger increases in perplexity for all models we tested. We take this to mean that SAL inflicts more collateral damage on other useful features in the representation than LEACE does. In other words, interventions made with LEACE are more _surgical_ than those made with prior work; they more closely approximate the ideal of a perfect intervention which only erases the target concept and keeps everything else fixed [40; 15]. If this experiment were conducted with SAL alone, we would have _overestimated_ the causal effect of part-of-speech.

## 7 Limitations and Future Work

Much work remains to be done to validate concept scrubbing. Specifically, we'd like to see experiments that target concepts much narrower than part-of-speech, and use behavioral metrics to determine whether scrubbing changes the network in the ways we'd intuitively expect. If these experiments succeed, an exciting next step would be the incorporation of concept scrubbing into the pretraining and/or finetuning process. This may make it possible to train deep neural networks subject to _conceptual constraints_. It remains to be seen if gradient-based optimizers will be able to "circumvent" such constraints by learning completely nonlinear representations of protected attributes.

In this work, we focused exclusively on _linear_ concept erasure due to its simplicity and tractability. Some authors have proposed nonlinear concept erasure techniques based on kernel methods, but have found that erasure functions fit using one kernel do not generalize well to other kernels [33; 37]. We conjecture that it is intractable to nondestructively edit X so as to prevent a general nonlinear adversary from recovering Z, unless the data generating process for X is known in detail.6

Footnote 6: We suspect erasing a concept is at least as hard as extracting it from the original representation. But in the worst case, information about Z could be encoded _cryptographically_ in X, which would be intractable to decode given standard computational complexity assumptions. If the data is generated by a known algorithm, however, it may be possible to efficiently eliminate mutual information between Z and X by simply breaking the links in the causal graph that connect them.

A major motivation of concept erasure is that it promises to prevent models from using a concept in a _post hoc_, model-agnostic fashion. But if our concept scrubbing procedure turns out to yield unsatisfactory results in practical use cases, the most promising research direction might then be to improve model-_specific_ techniques, such as those that modify the training procedure [8; 9; 14].

## 8 Acknowledgements

We are grateful to CoreWeave for providing the compute resources used in Section 6. Shauli Ravfogel is grateful to be supported by the Bloomberg Data Science PhD Fellowship.

## References

* [1] UC Berkeley. The Hilbert space of random variables. Lecture Notes Electrical Engineering 126, 2018. URL https://inst.eecs.berkeley.edu/~ee126/sp18/projection.pdf.
* [2] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Afah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. _arXiv preprint arXiv:2304.01373_, 2023.
* [3] Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. _Advances in Neural Information Processing Systems_, 29:4349-4357, 2016. URL https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf.
* [4] Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie, and Kilian Weinberger. Adversarial deep averaging networks for cross-lingual sentiment classification. _Transactions of the Association for Computational Linguistics_, 6:557-570, 2018. doi: 10.1162/tacl_a_00039. URL https://aclanthology.org/Q18-1039.
* [5] Verna Dankers, Christopher Lucas, and Ivan Titov. Can transformer be too compositional? Analysing idiom processing in neural machine translation. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3608-3626, 2022.
* [6] Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnamar Kenthapadi, and Adam Tauman Kalai. Bias in bios: A case study of semantic representation bias in a high-stakes setting. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_, FAT* '19, page 120-128, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.3287572. URL https://doi.org/10.1145/3287560.3287572.
* [7] Sunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Srikumar. OSCaR: Orthogonal subspace correction and rectification of biases in word embeddings. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 5034-5050, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.411. URL https://aclanthology.org/2021.emnlp-main.411.
* [8] Harrison Edwards and Amos Storkey. Censoring representations with an adversary. In _International Conference in Learning Representations_, pages 1-14, May 2016. URL https://arxiv.org/abs/1511.05897.
* [9] Yanai Elazar and Yoav Goldberg. Adversarial removal of demographic attributes from text data. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 11-21, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1002. URL https://aclanthology.org/D18-1002.
* [10] Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral explanation with amnesic counterfactuals. _Transactions of the Association for Computational Linguistics_, 9:160-175, 2021. doi: 10.1162/tacl_a_00359. URL https://aclanthology.org/2021.tacl-1.10.
* [11] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Hatfield Zac Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021.
* [12] Thomas S. Ferguson. _Mathematical Statistics_. Academic Press, Cambridge, MA, 1967.

* Gao et al. [2020] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_, 2020.
* Geiger et al. [2022] Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Goodman, and Christopher Potts. Inducing causal structure for interpretable neural networks. In _International Conference on Machine Learning_, pages 7324-7338. PMLR, 2022.
* Grimsley et al. [2020] Christopher Grimsley, Elijah Mayfield, and Julia Bursten. Why attention is not explanation: Surgical intervention and causal reasoning about neural models. 2020.
* Leibniz-Zentrum fur Informatik, August 2021. doi: 10.4230/LIPIcs.MFCS.2021.56.
* Haghighatkhah et al. [2022] Pantea Haghighatkhah, Antske Fokkens, Pia Sommerauer, Bettina Speckmann, and Kevin Verbeek. Better hit the nail on the head than beat around the bush: Removing protected attributes with a single projection. _arXiv preprint arXiv:2212.04273_, 2022.
* Hernandez and Andreas [2021] Evan Hernandez and Jacob Andreas. The low-dimensional linear geometry of contextualized word representations. In _Proceedings of the 25th Conference on Computational Natural Language Learning_, pages 82-93, 2021.
* Honnibal et al. [2020] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, Adriane Boyd, et al. spacy: Industrial-strength natural language processing in python. 2020.
* Kleindessner et al. [2023] Matthaus Kleindessner, Michele Donini, Chris Russell, and Muhammad Bilal Zafar. Efficient fair PCA for fair representation learning. In _International Conference on Artificial Intelligence and Statistics_, pages 5250-5270. PMLR, 2023.
* Kumar et al. [2022] Abhinav Kumar, Chenhao Tan, and Amit Sharma. Probing classifiers are unreliable for concept removal and detection. _arXiv preprint arXiv:2207.04153_, 2022.
* Kusner et al. [2017] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. _Advances in neural information processing systems_, 30, 2017.
* McDonald et al. [2013] Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Tackstrom, et al. Universal dependency annotation for multilingual parsing. In _Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 92-97, 2013.
* Nanda [2023] Neel Nanda. Actually, Othello-GPT has a linear emergent world model, Mar 2023. URL <https://neelnanda.io/mechanistic-interpretability/othello>.
* Nikoulina et al. [2021] Vassilina Nikoulina, Maxat Tezekbayev, Nuradil Kozhakhmet, Madina Babazhanova, Matthias Galle, and Zhenisbek Assylbekov. The rediscovery hypothesis: Language models need to meet linguistics. _Journal of Artificial Intelligence Research_, 72:1343-1384, 2021.
* Nilforoshan et al. [2022] Hamed Nilforoshan, Johann D Gaebler, Ravi Shroff, and Sharad Goel. Causal conceptions of fairness and their consequences. In _International Conference on Machine Learning_, pages 16848-16887. PMLR, 2022.
* Nivre et al. [2020] Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajic, Christopher D Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. Universal dependencies v2: An evergrowing multilingual treebank collection. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 4034-4043, 2020.
* Pearl [2009] Judea Pearl. _Causality_. Cambridge University Press, Cambridge, UK, 2 edition, 2009. ISBN 978-0-521-89560-6. doi: 10.1017/CBO9780511803161.

* Ravfogel et al. [2020] Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out: Guarding protected attributes by iterative nullspace projection. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7237-7256, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.647. URL https://aclanthology.org/2020.acl-main.647.
* Ravfogel et al. [2021] Shauli Ravfogel, Grusha Prasad, Tal Linzen, and Yoav Goldberg. Counterfactual interventions reveal the causal effect of relative clause representations on agreement prediction. In _Proceedings of the 25th Conference on Computational Natural Language Learning_, pages 194-209, 2021.
* Ravfogel et al. [2022] Shauli Ravfogel, Yoav Goldberg, and Ryan Cotterell. Linear guardedness and its implications. _arXiv preprint arXiv:2210.10012_, 2022.
* Ravfogel et al. [2022] Shauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan D Cotterell. Linear adversarial concept erasure. In _International Conference on Machine Learning_, pages 18400-18421. PMLR, 2022.
* Ravfogel et al. [2022] Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, and Ryan Cotterell. Adversarial concept erasure in kernel space. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 6034-6055, 2022.
* Sadeghi and Boddeti [2021] Bashir Sadeghi and Vishnu Boddeti. On the fundamental trade-offs in learning invariant representations. _arXiv preprint arXiv:2109.03386_, 2021. URL https://openreview.net/pdf?id=KOk7mUgspN9.
* Sadeghi et al. [2019] Bashir Sadeghi, Runyi Yu, and Vishnu Boddeti. On the global optima of kernelized adversarial representation learning. In _2019 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 7970-7978. IEEE, 2019. URL http://hal.cse.msu.edu/assets/pdfs/papers/2019-iccv-kernel-adversarial-representation-learning.pdf.
* Shao et al. [2023] Shun Shao, Yftah Ziser, and Shay Cohen. Erasure of unaligned attributes from neural representations. _arXiv preprint arXiv:2302.02997_, 2023.
* Shao et al. [2023] Shun Shao, Yftah Ziser, and Shay B. Cohen. Gold doesn't always glitter: Spectral removal of linear and nonlinear guarded attribute information. In _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics_, pages 1611-1622, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.eaacl-main.118.
* Tenney et al. [2019] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4593-4601, 2019.
* Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* Woodward [2005] James Francis Woodward. _Making Things Happen: A Theory of Causal Explanation explanation_. Oxford University Press, 2005.
* Wu et al. [2023] Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D Goodman. Interpretability at scale: Identifying causal mechanisms in Alpaca. _arXiv preprint arXiv:2305.08809_, 2023.
* Xie et al. [2017] Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable invariance through adversarial feature learning. In _Advances in Neural Information Processing Systems_, volume 30, pages 585-596, 2017. URL https://dl.acm.org/doi/10.5555/3294771.3294827.
* Xu et al. [2020] Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable information under computational constraints. _arXiv preprint arXiv:2002.10689_, 2020.
* Zhang et al. [2018] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In _Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society_, page 335-340, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450360128. doi: 10.1145/3278721.3278779. URL https://doi.org/10.1145/3278721.3278779.

Additional Related Work

The problem of linear concept erasure is an instance of the general problem of information removal. Information removal methods generally divide into adversarial methods, which are applied during training, and the post-hoc linear methods considered in this paper. Adversarial methods [8, 42, 4, 9, 44] use a gradient-reversal layer during training to induce representations that do not encode the protected attribute. However, Elazar and Goldberg [9] have shown that these methods fail in exhaustively removing all the information associated with the protected attribute: it is often possible to train new adversaries that successfully recover the removed information. Linear methods have been proposed as a tractable alternative, where one identifies a linear subspace that captures the concept of interest, and neutralizes it using algebraic techniques. Different methods have been proposed for the identification of the subspace, e.g. PCA and variants thereof [3, 20], orthogonal-rotation [7], classification-based [29], spectral [37, 36] and adversarial approaches [32].

Few works theoretically characterize the condition of linear guardedness. Haghighatkhah et al. [16] extensively analyzed the problem of preventing linear classification, with the focus on decreasing accuracy. They provide a constructive proof of an optimal intervention for an SVM classifier. Ravfogel et al. [31] have proposed a formal definition of linear guardedness based on \(\mathcal{V}\) information, and characterized the fairness implications of guardedness; we show the relations with our definition above. Ravfogel et al. [32] provide an adversarial formulation of the problem, derive a closed-formed solution to certain cases, and propose an SGD-based optimization for others. While they seek an _orthogonal_ projection, we empirically showed that their solution is very close to ours. Sadeghi et al. [35] and Sadeghi and Boddeti [34] both study an adversarial formulation of concept erasure for linear regression, and they trade-off with main-task performance. In contrast to Ravfogel et al. [32], they consider a general linear adversary, i.e. not necessarily a projection matrix. Closest to our work are Kleindessner et al. [20], Haghighatkhah et al. [17], Shao et al. [37]. As we showed above (SS 4), those methods do achieve the goal of linear guardedness though they are unable to prove this fact. At the same time, they are not optimal in terms of damage to the original representation space.

## Appendix B Equivalence of Guardedness with the Optimality of Constant Predictors

The following two theorems establish the equivalence of conditions 1 and 2 (indeed, they do so in the general setting, with no assumption of convex loss or linear predictors).

**Theorem B.1**.: _Suppose \(\mathrm{X}\ (\mathcal{V},\mathfrak{L})\)-guards \(\mathrm{Z}\). Then for every loss \(\mathcal{L}\in\mathfrak{L}\), the corresponding trivially attainable loss \(L_{\tau}^{(\mathcal{Z},\mathcal{L})}\) cannot be improved upon by any predictor \(\eta(\cdot;\bm{\theta})\in\mathcal{V}\), i.e. \(L_{\tau}=\inf_{\bm{\theta}}\mathbb{E}[\mathcal{L}(\eta(\mathrm{X};\bm{ \theta}),\mathrm{Z})]\)._

Proof.: Consider the null random vector \(\mathrm{X}^{\prime}(\omega)=\bm{0}\). Since all predictors are constant on \(\mathrm{X}^{\prime}\), and the trivially attainable loss gives the _best_ available expected loss among constant predictors, we must have:

\[L_{\tau}=\inf_{\bm{\theta}}\mathbb{E}[\mathcal{L}(\eta(\mathrm{X}^{\prime}; \bm{\theta}),\mathrm{Z})]\] (4)

The right side of equation (4) is the best possible loss achievable by a function \(\eta(\cdot;\bm{\theta})\) on the joint distribution of \((\mathrm{X}^{\prime},\mathrm{Z})\), which by the definition of guardedness is upper bounded by the best possible loss achievable on the joint distribution of \((\mathrm{X},\mathrm{Z})\):

\[\inf_{\bm{\theta}}\mathbb{E}[\mathcal{L}(\eta(\mathrm{X}^{\prime};\bm{\theta }),\mathrm{Z})]\leq\inf_{\bm{\theta}}\mathbb{E}[\mathcal{L}(\eta(\mathrm{X}; \bm{\theta}),\mathrm{Z})]\] (5)

Combining equations (4) and (5), and the fact that all constant functions exist in our function class \(\mathcal{V}=\{\eta(\cdot;\bm{\theta})\}\), we arrive at our desired result:

\[L_{\tau}=\inf_{\bm{\theta}}\mathbb{E}[\mathcal{L}(\eta(\mathrm{X};\bm{\theta }),\mathrm{Z})]\]

**Theorem B.2**.: _Suppose that for every loss \(\mathcal{L}\in\mathfrak{L}\), the corresponding trivially attainable loss \(L_{\tau}^{(\mathbb{Z},\mathcal{L})}\) cannot be improved upon by any predictor \(\eta(\cdot;\bm{\theta})\in\mathcal{V}\), i.e. \(L_{\tau}=\inf_{\bm{\theta}}\mathbb{E}[\mathcal{L}(\eta(\mathrm{X};\bm{\theta }),\mathrm{Z})]\). Then \(\mathrm{X}\)\((\mathcal{V},\mathfrak{L})\)-guards \(\mathrm{Z}\)._Proof.: Let \(\mathrm{X}^{\prime}:\Omega\to\mathbb{R}^{d}\) be any other random data vector with finite first moment.

Since all constant predictors exist in our predictor class \(\mathcal{V}=\{\eta(\cdot;\bm{\theta})\}\), the best loss achievable on \((\mathrm{X}^{\prime},\mathrm{Z})\) by functions in \(\mathcal{V}\) must be at least as good as the trivially attainable loss (the best loss available by such constant predictors):

\[\inf_{\bm{\theta}}\mathbb{E}[\mathcal{L}(\eta(\mathrm{X}^{\prime};\bm{\theta}),\mathrm{Z})]\leq L_{\tau}\]

By assumption, the trivially attainable loss cannot be improved upon over \((\mathrm{X},\mathrm{Z})\) by predictors in \(\mathcal{V}\):

\[L_{\tau}=\inf_{\bm{\theta}}\mathbb{E}[\mathcal{L}(\eta(\mathrm{X};\bm{\theta}),\mathrm{Z})]\]

Since our choice of \(\mathrm{X}^{\prime}\) was arbitrary, this shows that \(\mathrm{X}\) maximizes the minimal achievable loss, so \(\mathrm{X}\)\((\mathcal{V},\mathfrak{L})\)-guards \(\mathrm{Z}\). 

## Appendix C Linear Guardedness is Equivalent to Linear Statistical Parity

To measure the effect of linear guardedness on main-task classifiers, we use the following minimal definition of "fairness" with respect to an attribute, adapted from Edwards and Storkey [8].

**Definition C.1** (Statistical Parity).: _Let \(\mathrm{X}\) and \(\mathrm{Z}\) be defined as above, and let \(f\) be a function with domain \(\mathbb{R}^{d}\). Then \(f\) exhibits **statistical parity** with respect to \(\mathrm{Z}\) when evaluated on \(\mathrm{X}\) if_

\[\forall z\in\mathcal{Z}:\mathbb{E}[f(\mathrm{X})|\mathrm{Z}=z]=\mathbb{E}[f( \mathrm{X})].\]

We now prove the equivalence of conditions 3 and 5.

**Theorem C.2**.: _Let \(\mathrm{X}\) and \(\mathrm{Z}\) be defined as above. Then every linear predictor \(f(\mathbf{x})=\mathbf{b}+\mathbf{W}\mathbf{x}\) exhibits statistical parity w.r.t. \(\mathrm{Z}\) when evaluated on \(\mathrm{X}\) if and only if each class-conditional mean \(\mathbb{E}\big{[}\mathrm{X}|\mathrm{Z}=z\big{]}\) is equal to \(\mathbb{E}\big{[}\mathrm{X}\big{]}\)._

Proof.: Suppose each class-conditional mean \(\mathbb{E}\big{[}\mathrm{X}|\mathrm{Z}=z\big{]}\) is equal to \(\mathbb{E}\big{[}\mathrm{X}\big{]}\). Then by the linearity of expectation, we have for all \(z\in\mathcal{Z}\):

\[\mathbb{E}[f(\mathrm{X})|\mathrm{Z}=z]=\mathbb{E}[\mathbf{W}\mathrm{X}+ \mathbf{b}|\mathrm{Z}=z]=\mathbf{W}\mathbb{E}[\mathrm{X}|\mathrm{Z}=z]+ \mathbf{b}=\mathbf{W}\mathbb{E}[\mathrm{X}]+\mathbf{b}=\mathbb{E}[f(\mathrm{X })].\]

This matches the definition of statistical parity provided in Definition C.1.

Conversely, suppose every linear predictor \(f(\mathbf{x})=\mathbf{b}+\mathbf{W}\mathbf{x}\) exhibits statistical parity w.r.t. \(\mathrm{Z}\) when evaluated on \(\mathrm{X}\). Then this holds for the identity function \(\mathrm{id}(\mathbf{x})=\mathbf{x}\), and thus for all \(z\in\mathcal{Z}\):

\[\mathbb{E}[\mathrm{X}|\mathrm{Z}=z]=\mathbb{E}[\mathrm{id}(\mathrm{X})|\mathrm{ Z}=z]=\mathbb{E}[\mathrm{id}(\mathrm{X})]=\mathbb{E}[\mathrm{X}].\]

## Appendix D Implications for Prior Work

In this section we discuss the implications of Theorem 4.1, which characterizes the necessary and sufficient conditions for an affine erasure function to yield a perfectly linearly guarded dataset, for methods proposed in prior work.

Spectral Attribute RemovaL (SAL) [37] uses the top \(n\) left singular vectors of \(\bm{\Sigma}_{\chi\mathrm{Z}}\) to construct an orthogonal projection matrix \(\mathbf{Q}_{\mathrm{SAL}}=\mathbf{I}-\mathbf{U}_{:n}\mathbf{U}_{:n}^{T}\) which is then applied to \(\mathrm{X}\). Notably, while \(n\) is presented as a free parameter in their method, all of their experiments involve binary classification problems where \(\mathrm{Z}\) is a one-hot vector, and \(n\) is set to a value no greater than 2. We'll call the version of SAL where \(n=\mathrm{rank}(\bm{\Sigma}_{\chi\mathrm{Z}})\), "full-rank SAL." Since these left singular vectors are an orthonormal basis for \(\bm{\Sigma}_{\chi\mathrm{Z}}\)'s column space, Theorem 4.1 implies that full-rank SAL guarantees linear guardedness.

Mean Projection (MP) [17] orthogonally projects \(\mathrm{X}\) onto the orthogonal complement of the span of the difference in class centroids \(\mathbb{E}[\mathrm{X}|\mathrm{Z}=1]-\mathbb{E}[\mathrm{X}|\mathrm{Z}=0]\), where \(\mathrm{Z}\) is assumed to be binary. Since the centroids are equal after the projection, this method guarantees linear guardedness by Theorem 3.1. In fact, by Theorem 3.4, MP is mathematically equivalent to SAL when \(\mathrm{Z}\) is a one-dimensional random vector taking one of two possible values.

Derivation of LEACE

**Theorem 4.2**.: _Let \(\mathrm{X}\) and \(\mathrm{Z}\) be centered random vectors taking values in \(\mathbb{R}^{d}\) and \(\mathbb{R}^{k}\) respectively, each of finite second moment. Let \(\mathbf{M}\in\mathbb{R}^{d\times d}\) be a p.s.d. matrix defining a (possibly degenerate) inner product on \(\mathbb{R}^{d}\): \(\langle\mathbf{x},\mathbf{y}\rangle_{\mathbf{M}}=\mathbf{x}^{T}\mathbf{M} \mathbf{y}\). Let \(\mathbf{\Sigma}_{\mathrm{XX}}\in\mathbb{R}^{d\times d}\) be \(\mathrm{X}\)'s covariance matrix, and \(\mathbf{\Sigma}_{\mathrm{XZ}}\in\mathbb{R}^{d\times k}\) be the cross-covariance matrix of \(\mathrm{X}\) and \(\mathrm{Z}\). Let \(\mathbf{A}^{+}\) denote the Moore-Penrose pseudoinverse of a matrix \(\mathbf{A}\), and let \(\mathbf{A}^{1/2}\) be the p.s.d. square root of a p.s.d. matrix \(\mathbf{A}\). Then the objective_

\[\operatorname*{argmin}_{\mathbf{P}\in\mathbb{R}^{d\times d}}\mathbb{E}\Big{[} \big{\|}\mathbf{P}\mathrm{X}-\mathrm{X}\big{\|}_{\mathbf{M}}^{2}\Big{]} \quad\mathrm{subject\ to\ }\operatorname{Cov}(\mathbf{P}\mathrm{X},\mathrm{Z})= \mathbf{0}\]

_has the following solution:_

\[\mathbf{P}^{*}=\mathbf{I}-\mathbf{W}^{+}\mathbf{P}_{\mathbf{W}\mathbf{\Sigma} _{\mathrm{XZ}}}\mathbf{W},\]

_where \(\mathbf{W}\) is the whitening transformation \((\mathbf{\Sigma}_{\mathrm{XX}}^{1/2})^{+}\) and \(\mathbf{P}_{\mathbf{W}\mathbf{\Sigma}_{\mathrm{XZ}}}=(\mathbf{W}\mathbf{ \Sigma}_{\mathrm{XZ}})(\mathbf{W}\mathbf{\Sigma}_{\mathrm{XZ}})^{+}\) is the orthogonal projection matrix onto \(\mathrm{colp}(\mathbf{W}\mathbf{\Sigma}_{\mathrm{XZ}})\)._

Below are two independent proofs of Theorem 4.2.

### Algebraic Proof

Proof.: We shall first show that, in any orthonormal basis,7 each row \(\mathbf{P}_{\mathbf{i}}\) constitutes an independent optimization problem, and then select a basis in which we can easily show that the corresponding component \(\mathrm{X}_{i}\) of \(\mathrm{X}\) can be almost surely decomposed into a linear combination of mutually uncorrelated components in the whitened random vector \(\mathbf{W}\mathrm{X}\), some of which correlate with \(\mathrm{Z}\) and some of which do not. The solution \((\mathbf{P}\mathrm{X})_{i}\) is then that same linear combination, restricted to those components which do not correlate with \(\mathrm{Z}\).

Footnote 7: Throughout this proof, we abuse the notations \(\mathrm{X}_{i},\mathbf{P}_{\mathbf{i}}\), etc. to refer to the \(i^{\text{th}}\) component in the specified basis, not necessarily the standard one.

Consider first an orthonormal basis diagonalizing the inner product \(\mathbf{M}\), so that \(\langle\mathbf{x},\mathbf{y}\rangle_{\mathbf{M}}=\sum_{i=1}^{d}\alpha_{i}x_{i }y_{i}\) for fixed \(\alpha_{1},\ldots,\alpha_{d}\geq 0\). This allows us to treat each row \(\mathbf{P}_{\mathbf{i}}\in\mathbb{R}^{d}\) of \(\mathbf{P}\) as a separate optimization problem,

\[\operatorname*{argmin}_{\mathbf{P}_{\mathbf{i}}\in\mathbb{R}^{d}}\mathbb{E} \Big{[}\alpha_{i}\big{(}\mathbf{P}_{\mathbf{i}}{}^{T}\mathrm{X}-\mathrm{X}_{i }\big{)}^{2}\Big{]}\quad\mathrm{subject\ to\ }\operatorname{Cov}(\mathbf{P}_{ \mathbf{i}}{}^{T}\mathrm{X},\mathrm{Z})=\mathbf{0},\]

at which point the weights \(\alpha_{i}\) of each subproblem become irrelevant, and our objective may as well be Euclidean, allowing us to view each row as an independent optimization problem not just in this basis, but from any convenient one.

So now let \(\ell=\mathrm{rank}(\mathbf{\Sigma}_{\mathrm{XZ}})=\mathrm{rank}(\mathbf{ \Sigma}_{\mathbf{W}\mathrm{X},\mathrm{Z}})\) and \(m=\mathrm{rank}(\mathbf{\Sigma}_{\mathrm{XX}})=\mathrm{rank}(\mathbf{\Sigma}_ {\mathbf{W}\mathrm{X},\mathbf{W}\mathrm{X}})\), and consider a (new) orthonormal basis whose first \(m\) coordinates span the column (and row) space of \(\mathbf{W}\) (i.e. the subspace of \(\mathbb{R}^{d}\) in which \(\mathrm{X}\) and \(\mathbf{W}\mathrm{X}\) have nonzero variance), and whose first \(\ell\leq m\) coordinates span the column space of \(\mathbf{\Sigma}_{\mathbf{W}\mathrm{X},\mathrm{Z}}\) (i.e. the subspace of \(\mathbb{R}^{d}\) in which \(\mathbf{W}\mathrm{X}\) has nonzero covariance with \(\mathrm{Z}\)).

Any component of \(\mathrm{X}\) can be (almost surely) written as a fixed linear combination of the nontrivial components of its whitening \(\mathbf{W}\mathrm{X}\):

\[\mathrm{X}_{i}=(\mathbf{W}^{+}\mathbf{W}\mathrm{X})_{i}=\sum_{j=1}^{m}W_{ij}^ {+}(\mathbf{W}\mathrm{X})_{j}.\] (almost surely)

Meanwhile, any component of \(\mathbf{P}\mathrm{X}\) can be (always) written as a fixed linear combination of the nontrivial components of \(\mathbf{W}\mathrm{X}\) and the almost surely zero components of \(\mathrm{X}\):

\[(\mathbf{P}\mathrm{X})_{i}=\sum_{j=1}^{m}A_{ij}(\mathbf{W}\mathrm{X})_{j}+ \sum_{j=m+1}^{d}B_{ij}\mathrm{X}_{j},\]

i.e. \(\mathbf{P}=\mathbf{A}\mathbf{W}+\mathbf{B}\mathbf{V}\), where \(\mathbf{V}=\mathbf{I}-\mathbf{W}^{+}\mathbf{W}\) is the orthogonal projection onto \(\mathrm{X}\)'s almost surely zero components.

[MISSING_PAGE_FAIL:17]

The Optimality of Oblique Projections

As noted in subsection 4.2, the optimal affine erasure function \(r(\mathbf{x})=\mathbf{b}+\mathbf{P}\mathbf{x}\) does _not_ in general use an orthogonal projection for the matrix \(\mathbf{P}\). A simple example illustrates why. Let \(d=2,k=1\) so that \(\mathrm{X}\) takes values in \(\mathbb{R}^{2}\) and \(\mathrm{Z}\) takes values in \(\mathbb{R}\), with the first feature \(\mathrm{X}_{1}\) and the label \(\mathrm{Z}\) each independently and uniformly distributed in \(\{-1,+1\}\), and the second feature \(\mathrm{X}_{2}\) simply equal to the sum \(\mathrm{X}_{2}=\mathrm{X}_{1}+\mathrm{Z}\). A dataset reflecting such a distribution has four \((\mathbf{x},\mathbf{y})\) pairs:

\[([1,2]^{T},1),\quad([1,0]^{T},-1),\quad([-1,0]^{T},1),\quad([-1,-2]^{T},-1)\]

In this case, _all_ of the information \(\mathrm{X}\) has about \(\mathrm{Z}\) resides in \(\mathrm{X}_{2}\), so the minimally disruptive orthogonal projection which guards \(\mathrm{Z}\) will nullify that component:

\[\mathbf{P}_{\text{ortho}}=\begin{bmatrix}1&0\\ 0&0\end{bmatrix}\]

On the other hand, \(\mathrm{X}_{1}\) contains some information about \(\mathrm{X}_{2}\) (despite having no information about \(\mathrm{Z}\)), allowing a partial reconstruction of \(\mathrm{X}_{2}\) while preserving full concept erasure:

\[\mathbf{P}_{\text{oblique}}=\begin{bmatrix}1&0\\ 1&0\end{bmatrix}\]

Both methods fully erase the ability to predict \(\mathrm{Z}\) from the data, however a simple calculation shows the second, oblique method to perform better as measured by mean squared edit distance:

\[\mathbb{E}\|\mathbf{P}_{\text{ortho}}\mathrm{X}-\mathrm{X}\|^{2}=2,\quad \mathbb{E}\|\mathbf{P}_{\text{oblique}}\mathrm{X}-\mathrm{X}\|^{2}=1\]

## Appendix G Equivalence of Guardedness Definitions

Xu et al. [43] define the **conditional \(\mathcal{V}\)-entropy** of \(\mathrm{Z}\) given \(\mathrm{X}\) as the lowest achievable cross-entropy loss predicting \(\mathrm{Z}\) with a function of \(\mathrm{X}\) in the predictor class \(\mathcal{V}\). In our notation:

\[H_{\mathcal{V}}(\mathrm{Z}\:|\:\mathrm{X})=\inf_{\theta\in\Theta}\mathbb{E}[ \mathcal{L}(\eta(\mathrm{X};\boldsymbol{\theta}),\mathrm{Z})],\]

where \(\mathcal{L}(\eta,z)=-\log\frac{\exp(\eta_{z})}{\sum_{i=1}^{k}\exp(\eta_{i})}\) is the cross-entropy loss function.

They then define the (unconditional) \(\mathcal{V}\)**-entropy**\(H_{\mathcal{V}}(\mathrm{Z})=H_{\mathcal{V}}(\mathrm{Z}\:|\:\boldsymbol{0})\) to be the lowest achievable cross-entropy loss in the case of a constantly null random data variable. This is exactly our trivially attainable loss \(L_{\tau}\) (Definition 2.2).

Finally, they define the \(\mathcal{V}\)**-information** from \(\mathrm{X}\) to \(\mathrm{Z}\) as the reduction in \(\mathcal{V}\)-entropy as compared to using such a null random data variable:

\[I_{\mathcal{V}}(\mathrm{X}\rightarrow\mathrm{Z})=H_{\mathcal{V}}(\mathrm{Z})-H _{\mathcal{V}}(\mathrm{Z}\:|\:\mathrm{X}).\]

Using these notions, Ravfogel et al. [31] say that \(\mathrm{X}\) is \(\epsilon\)**-guarded** with respect to \(\mathcal{V}\) if \(I_{\mathcal{V}}(\mathrm{X}\ \rightarrow\ \mathrm{Z})<\epsilon\).

In Appendix B, we showed the equivalence of guardedness (as we have defined it in Definition 2.1) to the optimality of the trivially attainable loss. That is, \(\mathrm{X}\ (\mathcal{V},\mathfrak{L})\)-guards \(\mathrm{Z}\) when \(H_{\mathcal{V}}(\mathrm{Z}\:|\:\mathrm{X})=L_{\tau}=H_{\mathcal{V}}(\mathrm{Z})\), in the case where \(\mathfrak{L}\) is the singleton class consisting solely of the cross-entropy loss function. In the language of [31], \(\mathrm{X}\) is \(\epsilon\)-guarded with respect to \(\mathcal{V}\) for all \(\epsilon>0\).

## Appendix H Constraining Norm Growth

In early concept scrubbing experiments (Sec. 6), we found that at specific layers in some models, concept scrubbing with LEACE would cause the norm of the representation to diverge, leading to NaN outputs. By contrast, SAL never caused divergence, even though it causes a larger disruption to model performance on average (Table 1). This is because SAL uses an orthogonal projection \(\mathbf{Q}\), whose eigenvalues are thus all in \(\{0,1\}\), so the norm of the hidden state can never increase after erasure, while LEACE's oblique projection matrix \(\mathbf{P}\) does generally have singular values greater than 1. To combine the superior average-case MSE of LEACE with the stability of SAL, we adopt a simple regularization heuristic. After constructing \(\mathbf{P}\), we analytically compute the trace of the covariance matrix of the hidden states after applying \(\mathbf{P}\). If \(\operatorname{tr}(\mathbf{P}\mathbf{\Sigma}_{\mathrm{XX}}\mathbf{P}^{\mathsf{T}})> \operatorname{tr}(\mathbf{\Sigma}_{\mathrm{XX}})\), we solve a quadratic equation to find the convex combination \(\mathbf{P}^{\prime}=\alpha\mathbf{P}+(1-\alpha)\mathbf{Q}\) such that \(\operatorname{tr}(\mathbf{\Sigma}_{\mathrm{XX}})=\operatorname{tr}(\mathbf{P} ^{\prime}\mathbf{\Sigma}_{\mathrm{XX}}(\mathbf{P}^{\prime})^{\mathsf{T}})\). By Theorem 4.1, the set of matrices which ensure linear guardedness is convex,10 so \(\mathbf{P}^{\prime}\) is guaranteed to be in the feasible set. Furthermore, since our mean squared error objective is convex, \(\mathbf{P}^{\prime}\) is guaranteed to have no worse MSE than \(\mathbf{Q}\). We find this solves the divergence issue in practice.

Footnote 10: In fact, it is a subspace of \(\mathbb{R}^{d\times d}\). For any matrices \(\mathbf{A},\mathbf{B}\in\mathbb{R}^{d\times d}\) such that \(\mathbf{A}\mathbf{\Sigma}_{\mathrm{XZ}}=\mathbf{0}\) and \(\mathbf{B}\mathbf{\Sigma}_{\mathrm{XZ}}=\mathbf{0}\), we have by linearity \((\alpha\mathbf{A}+\beta\mathbf{B})\mathbf{\Sigma}_{\mathrm{XZ}}=\alpha\mathbf{ A}\mathbf{\Sigma}_{\mathrm{XZ}}+\beta\mathbf{B}\mathbf{\Sigma}_{\mathrm{XZ}}= \alpha\mathbf{0}+\beta\mathbf{0}=\mathbf{0}\) for any scalars \(\alpha\) and \(\beta\).

## Appendix I Oracle LEACE

The concept erasure method derived in Section 4 does not require access to concept labels at inference time. That is, we can fit an erasure function on a labeled training dataset, then apply the function to unlabeled datapoints. If we have oracle access to the label \(\bm{z}\) for each \(\bm{x}\), we can achieve an even more surgical edit. In Theorem I.1 below, we derive **Oracle LEACE**, a closed-form formula for the the nearest \(\mathrm{X}^{\prime}\) to any \(\mathrm{X}\) such that \(\operatorname{Cov}(\mathrm{X}^{\prime},\mathrm{Z})=\mathbf{0}\).

Like in Sec. 4, the resulting \(\mathrm{X}^{\prime}_{\mathrm{LEACE}}\) is "nearest" to \(\mathrm{X}\) with respect to all p.s.d. inner products \(\mathbf{a}^{T}\mathbf{M}\mathbf{b}\) defined on \(\mathbb{R}^{d}\) simultaneously. This is because, by expressing \(\mathrm{X}\) in a basis that diagonalizes \(\mathbf{M}\), we can decompose the problem into \(d\) independent subproblems, one for each component of \(\mathrm{X}_{i}\). Each subproblem can then be viewed as an orthogonal projection, not in \(\mathbb{R}^{d}\), but in an abstract vector space of real-valued random variables. For geometric intuition, see Figure 5.

Prior work has noted that computing an orthogonal projection in a random variable Hilbert space is equivalent to solving an ordinary least squares regression problem [1]. Our theorem is a natural extension of this work: we find that \(\mathrm{X}^{\prime}_{\mathrm{LEACE}}\) is equal to the OLS residual from regressing \(\mathrm{X}\) on \(\mathrm{Z}\), plus a constant shift needed to ensure that erasing \(\mathrm{Z}\) does not change the mean of \(\mathrm{X}\).

**Theorem I.1** (Oracle Concept Erasure).: _Let \(\mathcal{H}\) be the Hilbert space of square-integrable real-valued random variables equipped with the inner product \(\langle\xi,\zeta\rangle_{\mathcal{H}}:=\mathbb{E}[\xi\zeta]\). Let \((\mathrm{X},\mathrm{Z})\) be random vectors in \(\mathcal{H}^{d}\) and \(\mathcal{H}^{k}\) respectively. Then for every p.s.d. inner product \(\langle\mathbf{a},\mathbf{b}\rangle_{\mathbf{M}}=\mathbf{a}^{T}\mathbf{M} \mathbf{b}\) on \(\mathbb{R}^{d}\), the objective_

\[\operatorname*{argmin}_{\mathrm{X}^{\prime}\in\mathcal{H}^{d}}\,\mathbb{E} \big{\|}\mathrm{X}^{\prime}-\mathrm{X}\big{\|}_{\mathbf{M}}^{2}\quad\mathrm{ subject}\operatorname{to}\ \operatorname{Cov}(\mathrm{X}^{\prime},\mathrm{Z})=\mathbf{0}\]

_is minimized by the (appropriately shifted) ordinary least squares residuals from regressing \(\mathrm{X}\) on \(\mathrm{Z}\):_

\[\mathrm{X}^{\prime}_{\mathrm{LEACE}}=\mathrm{X}-\mathbf{\Sigma}_{\mathrm{XZ}} \mathbf{\Sigma}^{+}_{\mathrm{ZZ}}\big{(}\mathrm{Z}-\mathbb{E}[\mathrm{Z}]\big{)}.\]

Proof.: Assume w.l.o.g. that \(\mathrm{X}\) and \(\mathrm{X}^{\prime}\) are represented in a basis diagonalizing \(\mathbf{M}\), so we may write

\[\mathbb{E}\big{\|}\mathrm{X}^{\prime}-\mathrm{X}\big{\|}_{\mathbf{M}}^{2}=\sum_ {i=1}^{d}m_{i}\,\mathbb{E}\big{[}(\mathrm{X}^{\prime}_{i}-\mathrm{X}_{i})^{2} \big{]},\]

where \(m_{1},\ldots,m_{d}\geq 0\) are eigenvalues of \(\mathbf{M}\). Crucially, each term in this sum is independent from the others, allowing us to decompose the primal problem into \(d\) separate subproblems of the form \(\|\mathrm{X}^{\prime}_{i}-\mathrm{X}_{i}\|_{\mathcal{H}}^{2}\), one for each component \(i\) of \((\mathrm{X},\mathrm{X}^{\prime})\).

Factoring out constants.Now consider the subspace \(\mathcal{C}=\mathrm{span}(1)\subset\mathcal{H}\) consisting of all constant (i.e. zero variance) random variables. Orthogonally decomposing \(\mathrm{X}_{i}\) along \(\mathcal{C}\) yields \(\mathrm{X}_{i}=\tilde{\mathrm{X}}_{i}+\mu_{i}\), where \(\mu_{i}=\mathbb{E}[\mathrm{X}_{i}]\in\mathcal{C}\) and \(\tilde{\mathrm{X}}_{i}=\mathrm{X}-\mathbb{E}[\mathrm{X}_{i}]\in\mathcal{C}^{\perp}\), and likewise for \(\mathrm{X}^{\prime}_{i}\). Our objective is now

\[\big{\|}\mathrm{X}^{\prime}_{i}-\mathrm{X}_{i}\big{\|}_{\mathcal{H}}^{2}=\big{\|} \mu_{i}^{\prime}-\mu_{i}\big{\|}_{\mathcal{H}}^{2}+\big{\|}\tilde{\mathrm{X}}^{ \prime}_{i}-\tilde{\mathrm{X}}_{i}\big{\|}_{\mathcal{H}}^{2}.\] (7)

Figure 5: Orthogonal projection of \(i^{\text{th}}\) component of \(\mathrm{X}\), itself a vector in the random variable Hilbert space \(\mathcal{H}\), onto the span of the components of \(\mathrm{Z}\). The residual \(\mathrm{X}_{i}-\mathrm{proj}_{Z}\mathrm{X}_{i}\) is the closest vector to \(\mathrm{X}_{i}\) orthogonal to, and hence uncorrelated with, \(\mathcal{Z}=\mathrm{span}(\{\mathrm{Z}_{1},\mathrm{Z}_{2}\})\).

Since \(\mu^{\prime}_{i}\) and \(\mu_{i}\) are orthogonal to \(\tilde{\mathrm{X}}^{\prime}_{i}\) and \(\tilde{\mathrm{X}}_{i}\), and the constraint \(\mathrm{Cov}(\mathrm{X}^{\prime},\mathrm{Z})=\mathbf{0}\) is invariant to constant shifts, we can optimize the two terms in Eq. 7 independently. The first term is trivial: it is minimized when \(\mu^{\prime}_{i}=\mu_{i}\), and hence \(\mathrm{X}^{\prime}_{i}=\tilde{\mathrm{X}}^{\prime}_{i}+\mathbb{E}[\mathrm{X }_{i}]\).

Orthogonal projection.We can now rewrite the zero covariance condition as an orthogonality constraint on \(\tilde{\mathrm{X}}_{i}\). Specifically, for every \(i\in\{1\ldots d\}\) we have

\[\operatorname*{argmin}_{\tilde{\mathrm{X}}^{\prime}_{i}\in\mathcal{H}}\,\left\| \tilde{\mathrm{X}}^{\prime}_{i}-\tilde{\mathrm{X}}_{i}\right\|^{2}_{\mathcal{H }}\quad\mathrm{s.t.}\ \forall j\in\{1\ldots k\}:\langle\tilde{\mathrm{X}}^{\prime}_{i},\tilde{ \mathrm{Z}}_{j}\rangle_{\mathcal{H}}=0,\] (8)

where \(\tilde{\mathrm{Z}}=\mathrm{Z}-\mathbb{E}[\mathrm{Z}]\). In other words, we seek the nearest \(\tilde{\mathrm{X}}^{\prime}_{i}\) to \(\tilde{\mathrm{X}}_{i}\) orthogonal to \(\mathcal{Z}=\mathrm{span}(\{\tilde{\mathrm{Z}}_{1},\ldots,\tilde{\mathrm{Z}}_ {k}\})\), which is simply the orthogonal projection of \(\tilde{\mathrm{X}}_{i}\) onto \(\mathcal{Z}^{\perp}\). This in turn is equal to the ordinary least squares residual from regressing \(\tilde{\mathrm{X}}\) on \(\tilde{\mathrm{Z}}\):

\[\tilde{\mathrm{X}}^{\prime}_{i}=\tilde{\mathrm{X}}_{i}-\mathrm{proj}\big{(} \tilde{\mathrm{X}}_{i},\mathcal{Z}\big{)}=\mathrm{X}_{i}-(\mathbf{\Sigma}_{ \mathrm{XZ}})_{i}\mathbf{\Sigma}^{+}_{\mathrm{ZZ}}(\mathrm{Z}-\mathbb{E}[ \mathrm{Z}])-\mathbb{E}[\mathrm{X}_{i}].\] (9)

Putting it all together.Plugging Eq. 9 into \(\mathrm{X}^{\prime}_{i}=\tilde{\mathrm{X}}^{\prime}_{i}+\mathbb{E}[\mathrm{X }_{i}]\) and combining all components into vector form yields

\[\mathrm{X}^{\prime}_{\mathrm{LEACE}}=\mathbf{X}-\mathbf{\Sigma}_{\mathrm{XZ}} \mathbf{\Sigma}^{+}_{\mathrm{ZZ}}(\mathrm{Z}-\mathbb{E}[\mathrm{Z}]),\] (10)

which completes the proof. 

## Appendix J Notation Key

\begin{tabular}{l l} \(\mathcal{Z}\) & The space of one-hot labels \(\{(z_{1},\ldots z_{k})\in\{0,1\}^{k}\ \big{|}\ \sum_{j=1}^{k}z_{j}=1\}\}\) \\  & (treated interchangeably with the integers \(\{1,\ldots,k\}\) when convenient). \\ \(\mathrm{X},\mathrm{Z}\) & Integrable (i.e. finite first moment) random vectors taking values in \(\mathbb{R}^{d}\) and \(\mathbb{R}^{k}\) \\  & respectively (or their realized values inside an expectation, e.g. in \(\mathbb{E}[f(\mathrm{X})]\)). \\  & \(\mathrm{Z}\) is sometimes restricted to the one-hot labels \(\mathcal{Z}\), in which case we assume \\  & each \(\mathbb{P}(\mathrm{Z}=j)>0\). \\ \(\mathrm{X}_{i},\mathrm{Z}_{j}\) & The \(i^{\text{th}}\) and \(j^{\text{th}}\) components thereof, themselves scalar random variables (or \\  & their realized values inside an expectation). \\ \(\xi,\zeta\) & Scalar random variables taking values in \(\mathbb{R}\). \\ \(\eta\) & A predictor function \(\mathbb{R}^{d}\to\mathcal{Z}\) (or its value \(\eta(\mathrm{X})\) when inside an expectation). \\ \(\mathcal{V}\) & A space of predictor functions \(\{\eta(\cdot;\boldsymbol{\theta}):\mathbb{R}^{d}\to\mathbb{R}^{k}\ |\ \boldsymbol{\theta}\in\Theta\}\), parameterized \\  & by \(\boldsymbol{\theta}\) and containing all constant functions. \\ \(\mathfrak{L}\) & A space of loss functions \(\{\mathcal{L}:\mathbb{R}^{k}\times\mathcal{Z}\to[0,\infty)\}\). \\ \(r\) & An erasure function \(\mathbb{R}^{d}\to\mathbb{R}^{d}\), hopefully making a minimal edit to \(\mathrm{X}\) that \\  & eliminates the ability to predict labels \(\mathrm{Z}\) with predictors in \(\mathcal{V}\). \\ \(\mathbf{A}\) & A matrix with entries in \(\mathbb{R}\). \\ \(A_{ij}\) & The entry thereof at the \(i^{\text{th}}\) row and \(j^{\text{th}}\) column. \\ \(\mathbf{A}^{+}\) & The Moore-Penrose pseudoinverse of \(\mathbf{A}\). \\ \(\mathbf{v}\) & A column vector with entries in \(\mathbb{R}\). \\ \(v_{i}\) & The \(i^{\text{th}}\) component thereof. \\ \end{tabular}