# Recursive PAC-Bayes: A Frequentist Approach to Sequential Prior Updates with No Information Loss

 Yi-Shan Wu

University of South Denmark

yswu@imada.sdu.dk

&Yijie Zhang

University of Copenhagen & Novo Nordisk A/S

yizh@di.ku.dk

&Badr-Eddine Cherief-Abdellatif

CNRS, LPSM, Sorbonne Universite, Universite Paris Cite

badr-eddine.cherief-abdellatif@cnrs.fr

&Yevgeny Seldin

University of Copenhagen

seldin@di.ku.dk

###### Abstract

PAC-Bayesian analysis is a frequentist framework for incorporating prior knowledge into learning. It was inspired by Bayesian learning, which allows sequential data processing and naturally turns posteriors from one processing step into priors for the next. However, despite two and a half decades of research, the ability to update priors sequentially without losing confidence information along the way remained elusive for PAC-Bayes. While PAC-Bayes allows construction of data-informed priors, the final confidence intervals depend only on the number of points that were not used for the construction of the prior, whereas confidence information in the prior, which is related to the number of points used to construct the prior, is lost. This limits the possibility and benefit of sequential prior updates, because the final bounds depend only on the size of the final batch.

We present a novel and, in retrospect, surprisingly simple and powerful PAC-Bayesian procedure that allows sequential prior updates with no information loss. The procedure is based on a novel decomposition of the expected loss of randomized classifiers. The decomposition rewrites the loss of the posterior as an excess loss relative to a downscaled loss of the prior plus the downscaled loss of the prior, which is bounded recursively. As a side result, we also present a generalization of the split-kl and PAC-Bayes-split-kl inequalities to discrete random variables, which we use for bounding the excess losses, and which can be of independent interest. In empirical evaluation the new procedure significantly outperforms state-of-the-art.

## 1 Introduction

PAC-Bayesian analysis was born from an attempt to derive frequentist generalization guarantees for Bayesian-style prediction rules (Shawe-Taylor and Williamson, 1997; McAllester, 1998). The motivation was to provide a way to incorporate prior knowledge into the frequentist analysis of generalization. PAC-Bayesian bounds provide high-probability generalization guarantees for randomized classifiers. A randomized classifier is defined by a distribution \(\rho\) on a set of prediction rules \(\mathcal{H}\), which is used to sample a prediction rule each time a prediction is to be made. Bayesian posterior is an example of a randomized classifier, whereas PAC-Bayesian bounds hold generally for all randomized classifiers. Prior knowledge is encoded through a prior distribution \(\pi\) on \(\mathcal{H}\), and the complexity of a posterior distribution \(\rho\) is measured by the Kullback-Leibler (KL) divergence from the prior, \(\mathrm{KL}(\rho\|\pi)\). PAC-Bayesian generalization guarantees are optimized by posterior distributions \(\rho\) that optimize a trade-off between empirical data fit and divergence from the prior in the KL sense.

Selection of a "good" prior plays an important role in the PAC-Bayesian bounds. If one manages to foresee which prediction rules are likely to produce low prediction error and allocate a higher prior mass for them, then the bounds are tighter, because the posterior only needs to make a small deviation from the prior. But if the prior mass on well-performing prediction rules is small, the bounds are loose. A major technique to design good priors is to use part of the data to estimate a good prior and the rest of the data to compute a PAC-Bayes bound. It is known as data-dependent or data-informed priors (Ambroladze et al., 2007). However, all existing approaches to data-informed priors have three major disadvantages. The first is that the bounds are computed on "the rest of the data" that were not used in construction of the prior. Thus, the sample size in the bounds is only a fraction of the total sample size. Therefore, empirically data-informed priors are not always helpful. In many cases starting with an uninformed prior and using all the data to compute the posterior and the bound turns to be superior to sacrificing part of the data for prior construction (Ambroladze et al., 2007; Mhammedi et al., 2019). The second disadvantage is that all the confidence information about the prior is lost in the process. In particular, a prior trained on a few data points is treated in the same way as a prior trained on a lot of data. And a third related disadvantage is that sequential data processing provides no benefit, because the bounds only depend on the size of the last chunk and all the confidence information from processing earlier chunks is lost in the process.

Our main contribution is a new (and simple) way of decomposing the loss of a randomized classifier defined by the posterior. We write it as an excess loss relative to a downscaled loss of the randomized classifier defined by the prior plus the downscaled loss of the randomized classifier defined by the prior. The excess loss can be bounded using PAC-Bayes-Empirical-Bernstein-style inequalities (Tolstikhin and Seldin, 2013; Mhammedi et al., 2019; Wu et al., 2021; Wu and Seldin, 2022), whereas the loss of the randomized classifier defined by the prior can be bounded recursively. The recursive bound can both use the data used for construction of the prior and "the rest of the data", and thereby preserves confidence information on the prior. Our contribution stands out relative to all prior work on PAC-Bayes, and in fact all prior work on frequentist generalization bounds, because it makes sequential data processing and sequential prior updates meaningful and beneficial.

We note that while several recent papers experimented with sequential posterior updates by using martingale-style analysis, in all these works the prior remained fixed and only the posterior was changing (Chugg et al., 2023; Biggs and Guedj, 2023; Rodriguez-Galvez et al., 2024). The work on sequential posterior updates is orthogonal to our contribution and can be combined with it. Namely, it is possible to apply sequential posterior updates in-between sequential prior updates. Another line of work used tools from online learning to derive PAC-Bayesian bounds (Jang et al., 2023), and in this context Haddouche and Guedj (2023) have used sequential prior updates, but their bounds hold for a uniform aggregation of sequentially constructed posteriors, which is different from standard posteriors studied in our work. The confidence bounds in their work come primarily from aggregation rather than confidence in individual posteriors in the sequence (the denominator of their bounds depends on the number of aggregated posteriors). The need to construct and maintain a large number of posteriors has a negative impact on the computational efficiency. Our work is the first one allowing sequential prior updates without loss of confidence information.

An additional side contribution of independent interest is a generalization of the split-kl and PAC-Bayes-split-kl inequalities of Wu and Seldin (2022) from ternary to general discrete random variables. It is based on a novel representation of discrete random variables as a superposition of Bernoulli random variables.

The paper is organized in the following way. In Section 2 we briefly survey the evolution of data-informed priors in PAC-Bayes and present our main idea behind Recursive PAC-Bayes; in Section 3 we present our generalization of the split-kl and PAC-Bayes-split-kl inequalities, which are later used to bound the excess losses; in Section 4 we present the Recursive PAC-Bayes bound; in Section 5 we present an empirical evaluation; and in Section 6 we conclude with a discussion.

## 2 The evolution of data-informed priors and the idea of Recursive PAC-Bayes

In this section we briefly survey the evolution of data-informed priors, and then present our construction of Recursive PAC-Bayes. We consider the standard classification setting, with \(\mathcal{X}\) being a sample space, \(\mathcal{Y}\) a label space, \(\mathcal{H}\) a set of prediction rules \(h:\mathcal{X}\rightarrow\mathcal{Y}\), and \(\ell(h(X),Y)=\mathds{1}(h(X)\neq Y)\) the zero-one loss function, where \(\mathds{1}(\cdot)\) denotes the indicator function. We let \(\mathcal{D}\)denote a distribution on \(\mathcal{X}\times\mathcal{Y}\) and \(S=\{(X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\}\) an i.i.d. sample from \(\mathcal{D}\). Let \(L(h)=\mathbb{E}_{(X,Y)\sim\mathcal{D}}[\ell(h(X),Y)]\) be the expected and \(\hat{L}(h,S)=\frac{1}{n}\sum_{i=1}^{n}\ell(h(X_{i}),Y_{i})\) the empirical loss.

Let \(\rho\) be a distribution on \(\mathcal{H}\). A _randomized classifier_ associated with \(\rho\) samples a prediction rule \(h\) according to \(\rho\) for each sample \(X\in\mathcal{X}\), and applies it to make a prediction \(h(X)\). The expected loss of such randomized classifier, which we call \(\rho\), is \(\mathbb{E}_{h\sim\rho}[L(h)]\) and the empirical loss is \(\mathbb{E}_{h\sim\rho}[\hat{L}(h,S)]\). For brevity we use \(\mathbb{E}_{\rho}[\cdot]\) to denote \(\mathbb{E}_{h\sim\rho}[\cdot]\).

We use \(\mathrm{KL}(\rho\|\pi)\) to denote the Kullback-Leibler divergence between two probability distributions, \(\rho\) and \(\pi\)(Cover and Thomas, 2006). For \(p,q\in[0,1]\) we further use \(\mathrm{kl}(p\|q)=\mathrm{KL}((1-p,p)\|(1-q,q))\) to denote the Kullback-Leibler divergence between two Bernoulli distributions with biases \(p\) and \(q\).

The goal of PAC-Bayes is to bound \(\mathbb{E}_{\rho}[L(h)]\). Below we present how the bounds on \(\mathbb{E}_{\rho}[L(h)]\) have evolved. In Appendix A we also provide a graphical illustration of the evolution.

Uninformed priorsEarly work on PAC-Bayes used _uninformed priors_(McAllester, 1998). An uniformed prior \(\pi\) is a distribution on \(\mathcal{H}\) that is independent of the data \(S\). A classical, and still one of the tightest bounds, is the following.

**Theorem 1** (PAC-Bayes-\(\mathrm{kl}\) Inequality, Seeger, 2002, Maurer, 2004).: _For any probability distribution \(\pi\) on \(\mathcal{H}\) that is independent of \(S\) and any \(\delta\in(0,1)\):_

\[\mathbb{P}\bigg{(}\exists\rho\in\mathcal{P}:\mathrm{kl}\left(\mathbb{E}_{\rho }[\hat{L}(h,S)]\bigg{\|}\mathbb{E}_{\rho}\left[L(h)\right]\right)\geq\frac{ \mathrm{KL}(\rho\|\pi)+\ln(2\sqrt{n}/\delta)}{n}\bigg{)}\leq\delta,\]

_where \(\mathcal{P}\) is the set of all probability distributions on \(\mathcal{H}\), including those dependent on \(S\)._

A posterior \(\rho\) that minimizes \(\mathbb{E}_{\rho}[L(h)]\) has to balance between allocating higher mass to prediction rules \(h\) with small \(\hat{L}(h,S)\) and staying close to \(\pi\) in the \(\mathrm{KL}(\rho\|\pi)\) sense. Since \(\pi\) has to be independent of \(S\), typical uninformed priors aim "to leave maximal options open" for \(\rho\) by staying close to uniform.

Data-informed priorsAmbroladze et al. (2007) proposed to split the data \(S\) into two disjoint sets, \(S=S_{1}\cup S_{2}\), and use \(S_{1}\) to construct a _data-informed prior_\(\pi\) and compute a bound on \(\mathbb{E}_{\rho}[L(h)]\) using \(\pi\) and \(S_{2}\). Since in this approach \(\pi\) is independent of \(S_{2}\), Theorem 1 can be applied. The advantage is that \(\pi\) can use \(S_{1}\) to give higher mass to promising classifiers, thus relaxing the regularization pressure \(\mathrm{KL}(\rho\|\pi)\) and making it easier for \(\rho\) to allocate even higher mass to well-performing classifiers (those with small \(\hat{L}(h,S_{2})\)). The disadvantage is that the sample size in the bound (the \(n\) in the denominator) decreases from the size of \(S\) to the size of \(S_{2}\). Indeed, Ambroladze et al. observed that the sacrifice of \(S_{1}\) for prior construction does not always pay off.

Data-informed priors + excess lossMhammedi et al. (2019) observed that if we have already sacrificed \(S_{1}\) for the construction of \(\pi\), we could also use it to construct a reference prediction rule \(h^{*}\), typically an Empirical Risk Minimizer (ERM) on \(S_{1}\). They then employed the decomposition

\[\mathbb{E}_{\rho}[L(h)]=\mathbb{E}_{\rho}[L(h)-L(h^{*})]+L(h^{*})\]

and used \(S_{2}\) to give a PAC-Bayesian bound on \(\mathbb{E}_{\rho}[L(h)-L(h^{*})]\) and a single-hypothesis bound on \(L(h^{*})\). The quantity \(\mathbb{E}_{\rho}[L(h)-L(h^{*})]\) is known as _excess loss_. The advantage of this approach is that when \(L(h^{*})\) is a good approximation of \(\mathbb{E}_{\rho}[L(h)]\), the excess loss has lower variance than the plain loss \(\mathbb{E}_{\rho}[L(h)]\) and, therefore, is more efficient to bound, whereas the single-hypothesis bound on \(L(h^{*})\) does not involve the \(\mathrm{KL}(\rho\|\pi)\) term. Therefore, it is generally beneficial to use excess losses in combination with data-informed priors. However, as with the previous approach, sacrificing \(S_{1}\) to learn \(\pi\) and \(h^{*}\) means that the denominator in the bounds (\(n\) in Theorem 1) reduces to the size of \(S_{2}\), and it does not always pay off. (We note that the excess loss is not binary and not in the \([0,1]\) interval, and in order to exploit small variance it is actually necessary to apply a PAC-Bayes-Empirical-Bernstein-style inequality (Tolstikhin and Seldin, 2013, Mhammedi et al., 2019, Wu et al., 2021) or the PAC-Bayes-split-kl inequality (Wu and Seldin, 2022) rather than Theorem 1, but the point about reduced sample size still applies.)

Recursive PAC-Bayes (new)We introduce the following decomposition of the loss

\[\mathbb{E}_{\rho}[L(h)]=\mathbb{E}_{\rho}[L(h)-\gamma\mathbb{E}_{\pi}[L(h^{\prime })]]+\gamma\mathbb{E}_{\pi}[L(h^{\prime})].\] (1)As before, we decompose \(S\) into two disjoint sets \(S=S_{1}\cup S_{2}\). We make the following major observations:

* The quantity \(\mathbb{E}_{\pi}[L(h^{\prime})]\) on the right is "of the same kind" as \(\mathbb{E}_{\rho}[L(h)]\) on the left.
* We can take an uninformed prior \(\pi_{0}\) and apply Theorem 1 (or any other suitable PAC-Bayes bound) to bound \(\mathbb{E}_{\pi}[L(h^{\prime})]\). (The \(\mathrm{KL}\) term in the bound on \(\mathbb{E}_{\pi}[L(h^{\prime})]\) will be \(\mathrm{KL}(\pi\|\pi_{0})\).)
* We can restrict \(\pi\) to depend only on \(S_{1}\), but still use all the data \(S\) in calculation of the PAC-Bayes bound on \(\mathbb{E}_{\pi}[L(h^{\prime})]\), because \(\pi\) is a posterior relative to \(\pi_{0}\), and a posterior is allowed to depend on all the data, and in particular on any subset of the data. Therefore, the empirical loss \(\mathbb{E}_{\pi}[\hat{L}(h^{\prime},S)]\) can be computed on all the data \(S\), and the denominator of the bound in Theorem 1 can be the size of \(S\), and not the size of \(S_{2}\). This is what we call _preservation of confidence information on \(\pi\)_, because all the data \(S\) are used to construct a confidence bound on \(\mathbb{E}_{\pi}[L(h^{\prime})]\), and not just \(S_{2}\). This is in contrast to the bound on \(L(h^{*})\) in the approach of Mhammedi et al. (2019), which only allows to use \(S_{2}\) for bounding \(L(h^{*})\). Note that while we use all the data \(S\) in calculation of the bound, we only use \(S_{1}\) and \(\mathbb{E}_{\pi}[\hat{L}(h^{\prime},S_{1})]\) in the construction of \(\pi\). Nevertheless, we can still use the knowledge that we will have \(n\) samples when we reach the estimation phase, i.e., when constructing \(\pi\) we can leave the denominator of the bound at \(n\), allowing more aggressive deviation from \(\pi_{0}\).
* If we restrict \(\pi\) to depend only on \(S_{1}\), then it is a valid prior for estimation of any posterior quantity \(\mathbb{E}_{\rho}[\cdot]\) based on \(S_{2}\). Thus, if we also restrict \(\gamma\) to depend only on \(S_{1}\), we can use any PAC-Bayes-Empirical-Bernstein-style inequality or the PAC-Bayes-split-kl inequality to estimate the excess loss \(\mathbb{E}_{\rho}[L(h)-\gamma\mathbb{E}_{\pi}[L(h^{\prime})]]\) based on \(S_{2}\), i.e., based on \(\mathbb{E}_{\rho}[\hat{L}(h,S_{2})-\gamma\mathbb{E}_{\pi}[\hat{L}(h^{\prime},S_{2})]]\). If \(\gamma\mathbb{E}_{\pi}[L(h^{\prime})]\) is a good approximation of \(\mathbb{E}_{\rho}[L(h)]\) and \(\mathbb{E}_{\rho}[L(h)]\) is not close to zero, then the excess loss \(\mathbb{E}_{\rho}[L(h)-\gamma\mathbb{E}_{\pi}[L(h^{\prime})]]\) is more efficient to bound than the plain loss \(\mathbb{E}_{\rho}[L(h)]\).
* In general, since \(\mathbb{E}_{\rho}[L(h)]\) is expected to improve on \(\mathbb{E}_{\pi}[L(h^{\prime})]\), it is natural to set \(\gamma<1\). However, \(\gamma\) is not allowed to depend on \(S_{2}\), because otherwise \(\hat{L}(h,S_{2})-\gamma\mathbb{E}_{\pi}[\hat{L}(h^{\prime},S_{2})]\) becomes a biased estimate of \(L(h)-\gamma\mathbb{E}_{\pi}[\hat{L}(h^{\prime})]\). We discuss the choice of \(\gamma\) in more detail when we present the bound and the experiments.
* Biggs and Guedj (2023) have proposed a sequential martingale-style evaluation of a martingale version of \(\mathbb{E}_{\rho}[L(h)-L(h^{*})]\) and \(L(h^{*})\) in the approach of Mhammedi et al., but it has not been shown to yield significant improvements yet. The same "martingalization" can be directly applied to our decomposition, but to keep things simple we stay with the basic decomposition.
* Finally, we note that we can split \(S_{1}\) further and apply (1) recursively to bound \(\mathbb{E}_{\pi}[L(h^{\prime})]\).

To set notation for recursive decomposition, we use \(\pi_{0},\pi_{1},\ldots,\pi_{T}\) to denote a sequence of distributions on \(\mathcal{H}\), where \(\pi_{0}\) is an uninformed prior and \(\pi_{T}=\rho\) is the final posterior. We use \(\gamma_{2},\ldots,\gamma_{T}\) to denote a sequence of coefficients. For \(t\geq 2\) we then have the recursive decomposition

\[\mathbb{E}_{\pi_{t}}[L(h)]=\mathbb{E}_{\pi_{t}}[L(h)-\gamma_{t}\mathbb{E}_{ \pi_{t-1}}[L(h)]]+\gamma_{t}\mathbb{E}_{\pi_{t-1}}[L(h)].\] (2)

To construct \(\pi_{1},\ldots,\pi_{T}\) we split the data \(S\) into \(T\) non-overlapping subsets, \(S=S_{1}\cup\cdots\cup S_{T}\). We restrict \(\pi_{t}\) to depend on \(U_{t}^{\mathrm{train}}=\bigcup_{s=1}^{t}S_{s}\) only, and we use \(U_{t}^{\mathrm{val}}=\bigcup_{s=t}^{T}S_{s}\) to estimate (recursively) \(\mathbb{E}_{\pi_{t}}[L(h)]\) (see Figures 1 and 2 in Appendix A for a graphical illustration). Note that \(S_{t}\) is used both for construction of \(\pi_{t}\) and for estimation of \(\mathbb{E}_{\pi_{t}}[L(h)]\) (it is both in \(U_{t}^{\mathrm{train}}\) and \(U_{t}^{\mathrm{val}}\)), resulting in efficient use of the data. It is possible to use any standard PAC-Bayes bound, e.g., Theorem 1, to bound \(\mathbb{E}_{\pi_{1}}[L(h)]\), and any PAC-Bayes-Empirical-Bernstein-style bound or the PAC-Bayes-split-kl bound to bound the excess losses \(\mathbb{E}_{\pi_{t}}[L(h)-\gamma_{t}\mathbb{E}_{\pi_{t-1}}[L(h)]]\). The excess losses take more than three values, so in the next section we present a generalization of the PAC-Bayes-split-kl inequality to general discrete random variables, which may be of independent interest. The Recursive PAC-Bayes bound is presented in Section 4.

## 3 Split-kl and PAC-Bayes-split-kl inequalities for discrete random variables

The \(\mathrm{kl}\) inequality is one of the tightest concentration of measure inequalities for binary random variables. Letting \(\mathrm{kl}^{-1,+}(\hat{p},\varepsilon):=\max\left\{p:p\in[0,1]\text{ and }\mathrm{kl}(\hat{p}\|p)\leq\varepsilon\right\}\) denote the upper inverse of \(\mathrm{kl}\) and \(\mathrm{kl}^{-1,-}(\hat{p},\varepsilon):=\min\left\{p:p\in[0,1]\text{ and }\mathrm{kl}(\hat{p}\|p)\leq\varepsilon\right\}\) the lower inverse, it states the following.

**Theorem 2** (kl Inequality (Langford, 2005; Foong et al., 2021, 2022)).: _Let \(Z_{1},\cdots,Z_{n}\) be independent random variables bounded in the \([0,1]\) interval and with \(\mathbb{E}\left[Z_{i}\right]=p\) for all \(i\). Let \(\hat{p}=\frac{1}{n}\sum_{i=1}^{n}Z_{i}\) be the empirical mean. Then, for any \(\delta\in(0,1)\):_

\[\mathbb{P}\!\left(p\geq\mathrm{kl}^{-1,+}\left(\hat{p},\frac{1}{n}\ln\frac{1 }{\delta}\right)\right)\leq\delta\qquad;\qquad\mathbb{P}\!\left(p\leq\mathrm{ kl}^{-1,-}\left(\hat{p},\frac{1}{n}\ln\frac{1}{\delta}\right)\right)\leq\delta.\]

While the \(\mathrm{kl}\) inequality is tight for binary random variables, it is loose for random variables taking more than two values due to its inability to exploit small variance. To address this shortcoming Wu and Seldin (2022) have presented the split-kl and PAC-Bayes-split-kl inequalities for ternary random variables. Ternary random variables naturally appear in a variety of applications, including analysis of excess losses, certain ways of analysing majority votes, and in learning with abstention. The bound of Wu and Seldin is based on decomposition of a ternary random variable into a pair of binary random variables and application of the \(\mathrm{kl}\) inequality to each of them. Their decomposition yields a tight bound in the binary and ternary case, but loose otherwise. The same decomposition was used by Biggs and Guedj (2023) to derive a slight variation of the inequality, with the same limitations. We present a novel decomposition of discrete random variables into a superposition of binary random variables. Unlike the decomposition of Wu and Seldin, which only applies in the ternary case, our decomposition applies to general discrete random variables. By combining it with \(\mathrm{kl}\) bounds for the binary elements we obtain a tight bound. The decomposition is presented formally below and illustrated graphically in Figure 3 in Appendix A.

### Split-kl inequality

Let \(Z\in\{b_{0},\ldots,b_{K}\}\) be a \((K+1)\)-valued random variable with \(b_{0}<b_{1}<\cdots<b_{K}\). For \(j\in\{1,\ldots,K\}\) define \(Z_{|j}=\mathds{1}(Z\geq b_{j})\) and \(\alpha_{j}=b_{j}-b_{j-1}\). Then \(Z=b_{0}+\sum_{j=1}^{K}\alpha_{j}Z_{|j}\). For a sequence \(Z_{1},\ldots,Z_{n}\) of \((K+1)\)-valued random variables with the same support, let \(Z_{i|j}=\mathds{1}(Z_{i}\geq b_{j})\) denote the elements of binary decomposition of \(Z_{i}\).

**Theorem 3** (Split-kl inequality for discrete random variables).: _Let \(Z_{1},\ldots,Z_{n}\) be i.i.d. random variables taking values in \(\{b_{0},\ldots,b_{K}\}\) with \(\mathbb{E}\left[Z_{i}\right]=p\) for all \(i\). Let \(\hat{p}_{|j}=\frac{1}{n}\sum_{i=1}^{n}Z_{i|j}\). Then for any \(\delta\in(0,1)\):_

\[\mathbb{P}\!\left(p\geq b_{0}+\sum_{j=1}^{K}\alpha_{j}\,\mathrm{kl}^{-1,+} \left(\hat{p}_{|j},\frac{1}{n}\ln\frac{K}{\delta}\right)\right)\leq\delta.\]

Proof.: Let \(p_{|j}=\mathbb{E}\left[\hat{p}_{|j}\right]\), then \(p=b_{0}+\sum_{j=1}^{K}\alpha_{j}p_{|j}\) and

\[\mathbb{P}\!\left(p\geq b_{0}+\sum_{j=1}^{K}\alpha_{j}\,\mathrm{kl}^{-1,+} \left(\hat{p}_{|j},\frac{1}{n}\ln\frac{K}{\delta}\right)\right)\leq\mathbb{P} \!\left(\exists j:p_{|j}\geq\mathrm{kl}^{-1,+}\left(\hat{p}_{|j},\frac{1}{n} \ln\frac{K}{\delta}\right)\right)\leq\delta,\]

where the first inequality is by the decomposition of \(p\) and the second inequality is by the union bound and Theorem 2. 

### PAC-Bayes-Split-kl inequality

Let \(f:\mathcal{H}\times\mathcal{Z}\to\{b_{0},\ldots,b_{K}\}\) be a \((K+1)\)-valued loss function. (To connect it to the earlier examples, in the binary prediction case we would have \(\mathcal{Z}=\mathcal{X}\times\mathcal{Y}\) with elements \(Z=(X,Y)\) and \(f(h,Z)=\ell(h(X),Y)\), but we will need a more general space \(\mathcal{Z}\) later.) For \(j\in\{1,\ldots,K\}\) let \(f_{|j}(\cdot,\cdot)=\mathds{1}(f(\cdot,\cdot)\geq b_{j})\). Let \(\mathcal{D}_{Z}\) be an unknown distribution on \(\mathcal{Z}\). For \(h\in\mathcal{H}\) let \(F(h)=\mathbb{E}_{\mathcal{D}_{Z}}[f(h,Z)]\) and \(F_{|j}(h)=\mathbb{E}_{\mathcal{D}_{Z}}[f_{|j}(h,Z)]\). Let \(S=\{Z_{1},\ldots,Z_{n}\}\) be an i.i.d. sample according to \(\mathcal{D}_{Z}\) and \(\hat{F}_{|j}(h,S)=\frac{1}{n}\sum_{i=1}^{n}f_{|j}(h,Z_{i})\).

**Theorem 4** (PAC-Bayes-Split-kl Inequality).: _For any distribution \(\pi\) on \(\mathcal{H}\) that is independent of \(S\) and any \(\delta\in(0,1)\):_

\[\mathbb{P}\!\left(\exists\rho\in\mathcal{P}:\mathbb{E}_{\rho}[F(h)]\geq b_{0}+ \sum_{j=1}^{K}\alpha_{j}\,\mathrm{kl}^{-1,+}\left(\mathbb{E}_{\rho}[\hat{F}_{ |j}(h,S)],\frac{\mathrm{KL}(\rho\|\pi)+\ln\frac{2K\sqrt{n}}{\delta}}{n}\right) \right)\leq\delta,\]

_where \(\mathcal{P}\) is the set of all possible probability distributions on \(\mathcal{H}\) that can depend on \(S\)._Proof.: We have \(f(\cdot,\cdot)=b_{0}+\sum_{j=1}^{K}\alpha_{j}f_{[j}(\cdot,\cdot)\) and \(F(h)=b_{0}+\sum_{j=1}^{K}\alpha_{j}F_{[j}(h)\). Therefore,

\[\mathbb{P}\Bigg{(}\exists\rho\in\mathcal{P}:\mathbb{E}_{\rho}[F(h )]\geq b_{0}+\sum_{j=1}^{K}\alpha_{j}\operatorname{kl}^{-1,+}\left(\mathbb{E}_ {\rho}[\hat{F}_{]j}(h,S)],\frac{\operatorname{KL}(\rho\|\pi)+\ln\frac{2K\sqrt{ n}}{\delta}}{n}\right)\Bigg{)}\] \[\leq\mathbb{P}\Bigg{(}\exists\rho\in\mathcal{P}\text{ and }\exists j: \mathbb{E}_{\rho}[F_{]j}(h)]\geq\operatorname{kl}^{-1,+}\left(\mathbb{E}_{ \rho}[\hat{F}_{]j}(h,S)],\frac{\operatorname{KL}(\rho\|\pi)+\ln\frac{2K\sqrt{ n}}{\delta}}{n}\right)\Bigg{)}\leq\delta,\]

where the first inequality is by the decomposition of \(F\) and the second inequality is by the union bound and application of Theorem 1 to \(F_{]j}\) (note that \(f_{[j}\) is a zero-one loss function). 

## 4 Recursive PAC-Bayes bound

Now we derive a Recursive PAC-Bayes bound based on the loss decomposition in equation (2). We aim to bound \(\mathbb{E}_{\pi_{t}}[L(h)-\gamma_{t}\mathbb{E}_{\pi_{t-1}}[L(h^{\prime})]]\), which we denote by

\[F_{\gamma_{t},\pi_{t-1}}(h)=L(h)-\gamma_{t}\mathbb{E}_{\pi_{t-1}}[L(h^{\prime })]=\mathbb{E}_{\mathcal{D}\times\pi_{t-1}}[\ell(h(X),Y)-\gamma_{t}\ell(h^{ \prime}(X),Y)],\]

where \(\mathcal{D}\times\pi_{t-1}\) is a product distribution on \(\mathcal{X}\times\mathcal{Y}\times\mathcal{H}\) and \(h^{\prime}\in\mathcal{H}\) is sampled according to \(\pi_{t-1}\). We further define

\[f_{\gamma_{t}}(h,(X,Y,h^{\prime}))=\ell(h(X),Y)-\gamma_{t}\ell(h^{\prime}(X),Y )\in\left\{-\gamma_{t},0,1-\gamma_{t},1\right\},\]

then \(F_{\gamma_{t},\pi_{t-1}}(h)=\mathbb{E}_{\mathcal{D}\times\pi_{t-1}}[f_{\gamma_ {t}}(h,(X,Y,h^{\prime}))]\). In order to apply Theorem 4, we represent \(f_{\gamma_{t}}\) as a superposition of binary functions. For this purpose we let \(\big{\{}b_{t|0},b_{t|1},b_{t|2},b_{t|3}\big{\}}=\{-\gamma_{t},0,1-\gamma_{t},1\}\) and define \(f_{\gamma_{t}|j}(h,(X,Y,h^{\prime}))=\mathds{1}\big{(}f_{\gamma_{t}}(h,(X,Y,h^ {\prime}))\geq b_{t|j}\big{)}\). We let \(F_{\gamma_{t},\pi_{t-1}|j}(h)=\mathbb{E}_{\mathcal{D}\times\pi_{t-1}}[f_{\gamma _{t}|j}(h,(X,Y,h^{\prime}))]\), then \(F_{\gamma_{t},\pi_{t-1}}(h)=-\gamma_{t}+\sum_{j=1}^{3}(b_{t|j}-b_{t|j-1})F_{ \gamma_{t},\pi_{t-1}|j}(h)\).

Now we construct an empirical estimate of \(F_{\gamma_{t},\pi_{t-1}|j}(h)\). We first let \(\hat{\pi}_{t-1}=\big{\{}h_{1}^{\pi_{t-1}},h_{2}^{\pi_{t-1}},\dots\big{\}}\) be a sequence of prediction rules sampled independently according to \(\pi_{t-1}\). We define \(U_{t}^{\mathrm{val}}\circ\hat{\pi}_{t-1}=\big{\{}\big{(}X_{i},Y_{i},h_{i}^{ \pi_{t-1}}):(X_{i},Y_{i})\in U_{t}^{\mathrm{val}}\big{\}}\). In words, for every sample \((X_{i},Y_{i})\in U_{t}^{\mathrm{val}}\) we sample a prediction rule \(h_{i}^{\pi_{t-1}}\) according to \(\pi_{t-1}\) and put the triplet \((X_{i},Y_{i},h_{i}^{\pi_{t-1}})\) in \(U_{t}^{\mathrm{val}}\circ\hat{\pi}_{t-1}\). The triplets \((X_{i},Y_{i},h_{i}^{\pi_{t-1}})\) correspond to the random variables \(Z\) in Theorem 4. We note that \(|U_{t}^{\mathrm{val}}|=|U_{t}^{\mathrm{val}}\circ\hat{\pi}_{t-1}|\), and we let \(n_{t}^{\mathrm{val}}=|U_{t}^{\mathrm{val}}|\). We define the empirical estimate of \(F_{\gamma_{t},\pi_{t-1}|j}(h)\) as \(\hat{F}_{\gamma_{t}|j}(h,U_{t}^{\mathrm{val}}\circ\hat{\pi}_{t-1})=\frac{1}{n_ {t}^{\mathrm{val}}}\sum_{(X,Y,h^{\prime})\in U_{t}^{\mathrm{val}}\circ\hat{\pi} _{t-1}}f_{\gamma_{t}|j}(h,(X,Y,h^{\prime}))\). Note that \(\mathbb{E}_{\mathcal{D}\times\pi_{t-1}}[\hat{F}_{\gamma_{t}|j}(h,U_{t}^{ \mathrm{val}}\circ\hat{\pi}_{t-1})]=F_{\gamma_{t},\pi_{t-1}|j}(h)\), therefore, we can use Theorem 4 to bound \(\mathbb{E}_{\pi_{t}}[F_{\gamma_{t},\pi_{t-1}}(h)]\) using its empirical estimates. We are now ready to state the bound.

**Theorem 5** (Recursive PAC-Bayes Bound).: _Let \(S=S_{1}\cup\dots\cup S_{T}\) be an i.i.d. sample split in an arbitrary way into \(T\) non-overlapping subsamples, and let \(U_{t}^{\mathrm{train}}=\bigcup_{s=1}^{t}S_{s}\) and \(U_{t}^{\mathrm{val}}=\bigcup_{s=t}^{T}S_{s}\). Let \(n_{t}^{\mathrm{val}}=|U_{t}^{\mathrm{val}}|\). Let \(\pi_{0}^{*},\pi_{1}^{*},\dots,\pi_{T}^{*}\) be a sequence of distributions on \(\mathcal{H}\), where \(\pi_{t}^{*}\) is allowed to depend on \(U_{t}^{\mathrm{train}}\), but not the rest of the data. Let \(\gamma_{2},\dots,\gamma_{T}\) be a sequence of coefficients, where \(\gamma_{t}\) is allowed to depend on \(U_{t-1}^{\mathrm{train}}\), but not the rest of the data. For \(t\in\{1,\dots,T\}\) let \(\mathcal{P}_{t}\) be a set of distributions on \(\mathcal{H}\), which are allowed to depend on \(U_{t}^{\mathrm{train}}\). Then for any \(\delta\in(0,1)\):_

\[\mathbb{P}(\exists t\in\{1,\dots,T\}\text{ and }\pi_{t}\in\mathcal{P}_{t}: \mathbb{E}_{\pi_{t}}[L(h)]\geq\mathrm{B}_{t}(\pi_{t}))\leq\delta,\]

_where \(\mathrm{B}_{t}(\pi_{t})\) is a PAC-Bayes bound on \(\mathbb{E}_{\pi_{t}}[L(h)]\) defined recursively as follows. For \(t=1\)_

\[\mathrm{B}_{1}(\pi_{1})=\operatorname{kl}^{-1,+}\left(\mathbb{E}_{\pi_{1}}[ \hat{L}(h,S)],\frac{\operatorname{KL}(\pi_{1}\|\pi_{0}^{*})+\ln\frac{2T\sqrt{ n}}{\delta}}{n}\right).\]

_For \(t\geq 2\) we let \(\mathcal{E}_{t}(\pi_{t},\gamma_{t})\) denote a PAC-Bayes bound on \(\mathbb{E}_{\pi_{t}}[L(h)-\gamma_{t}\mathbb{E}_{\pi_{t-1}^{*}}[L(h^{\prime})]]\) given by_

\[\mathcal{E}_{t}(\pi_{t},\gamma_{t})=-\gamma_{t}+\sum_{j=1}^{3}(b_{t|j}-b_{t|j-1} )\operatorname{kl}^{-1,+}\left(\mathbb{E}_{\pi_{t}}\left[\hat{F}_{\gamma_{t}|j }(h,U_{t}^{\mathrm{val}}\circ\hat{\pi}_{t-1}^{*})\right],\frac{\operatorname{KL} (\pi_{t}\|\pi_{t-1}^{*})+\ln\frac{6T\sqrt{n_{t}^{\mathrm{val}}}}{\delta}}{n_ {t}^{\mathrm{val}}}\right)\]

_and then_

\[\mathrm{B}_{t}(\pi_{t})=\mathcal{E}_{t}(\pi_{t},\gamma_{t})+\gamma_{t}\, \mathrm{B}_{t-1}(\pi_{t-1}^{*}).\] (3)Proof.: By Theorem 1 we have \(\mathbb{P}(\exists\pi_{1}\in\mathcal{P}_{1}:\mathbb{E}_{\pi_{1}}[L(h)]\geq B_{1}( \pi_{1}))\leq\frac{\delta}{T}\). Further, by Theorem 4 for \(t\in\{2,\ldots,T\}\) we have \(\mathbb{P}\Big{(}\exists\pi_{t}\in\mathcal{P}_{t}:\mathbb{E}_{\pi_{t}}[L(h)- \gamma_{t}\mathbb{E}_{\pi_{t-1}^{*}}[L(h^{\prime})]]\geq\mathcal{E}_{t}(\pi_{ t},\gamma_{t})\Big{)}\leq\frac{\delta}{T}\). The theorem follows by a union bound and the recursive decomposition of the loss (2). 

**Discussion**

* Note that \(\pi_{1}^{*},\ldots,\pi_{T}^{*}\) can be constructed sequentially, but \(\pi_{t}^{*}\) can only be constructed based on the data in \(U_{t}^{\rm train}\), meaning that in the construction of \(\pi_{t}^{*}\) we can only rely on \(\mathbb{E}_{\pi_{t}}\left[\hat{F}_{\gamma_{t}|j}(B,S_{t}\circ\hat{\pi}_{t-1})\right]\), but not on \(\mathbb{E}_{\pi_{t}}\left[\hat{F}_{\gamma_{t}|j}(h,U_{t}^{\rm val}\circ\hat{ \pi}_{t-1})\right]\). Also note that \(S_{t}\) is part of both \(U_{t}^{\rm train}\) and \(U_{t}^{\rm val}\) (see Figure 1 in Appendix A for a graphical illustration). In other words, when we evaluate the bounds we can use additional data. And even though the additional data can only be used in the evaluation stage, we can still use the knowledge that we will get more data for evaluation when we construct \(\pi_{t}^{*}\). For example, we can take \[\pi_{1}^{*}=\arg\min_{\pi}\mathrm{kl}^{-1,+}\left(\mathbb{E}_{\pi}[\hat{L}(h,S _{1})],\frac{\mathrm{KL}(\pi\|\pi_{0}^{*})+\ln\frac{2T\sqrt{\pi}}{\delta}}{n}\right)\] (4) and for \(t\geq 2\) \[\pi_{t}^{*}=\arg\min_{\pi}\sum_{j=1}^{3}(b_{t|j}-b_{t|j-1})\,\mathrm{kl}^{-1,+ }\left(\mathbb{E}_{\pi}\left[\hat{F}_{\gamma_{t}|j}(h,S_{t}\circ\hat{\pi}_{t-1 }^{*})\right],\frac{\mathrm{KL}(\pi\|\pi_{t-1}^{*})+\ln\frac{6T\sqrt{n_{t}^{ \rm val}}}{\delta}}{n_{t}^{\rm val}}\right).\] (5) The empirical losses above are calculated on \(S_{t}\) corresponding to \(\pi_{t}^{*}\), but the sample sizes \(n_{t}^{\rm val}\) correspond to the size of the validation set \(U_{t}^{\rm val}\) rather than the size of \(S_{t}\). This allows to be more aggressive in deviating with \(\pi_{t}^{*}\) from \(\pi_{t-1}^{*}\) by sustaining larger \(\mathrm{KL}(\pi_{t}^{*}\|\pi_{t-1}^{*})\) terms.
* Similarly, \(\gamma_{2},\ldots,\gamma_{T}\) can also be constructed sequentially, as long as \(\gamma_{t}\) only depends on \(U_{t-1}^{\rm train}\) (otherwise \(\hat{F}_{\gamma_{t}|j}(h,S_{t}\circ\hat{\pi}_{t-1}^{*})\) becomes a biased estimate of \(F_{\gamma_{t},\pi_{t-1}^{*}|j}(h)\)).
* We naturally want to have improvement over recursion steps, meaning \(\mathrm{B}_{t}(\pi_{t}^{*})<\mathrm{B}_{t-1}(\pi_{t-1}^{*})\). Plugging this into (3), we obtain \(\mathcal{E}(\pi_{t}^{*},\gamma_{t})+\gamma_{t}B_{t-1}(\pi_{t-1}^{*})<B_{t-1}( \pi_{t-1}^{*})\), which implies that we want \(\gamma_{t}\) to be sufficiently small to satisfy \(\gamma_{t}<1-\frac{\mathcal{E}_{t}(\pi_{t}^{*},\gamma_{t})}{B_{t-1}(\pi_{t-1} ^{*})}\). At the same time, \(\gamma_{t}\) should be non-negative. Therefore, improvement over recursion steps can only be maintained as long as \(\mathcal{E}_{t}(\pi_{t}^{*},\gamma_{t})<B_{t-1}(\pi_{t-1}^{*})\). We note that \(\gamma_{t}\,\mathrm{B}_{t-1}(\pi_{t-1}^{*})\) term in (3) is linearly increasing in \(\gamma_{t}\), whereas \(\mathcal{E}(\pi_{t}^{*},\gamma_{t})\) is decreasing in \(\gamma_{t}\). The value of \(\gamma_{t}\) that minimizes the trade-off depends on the data. Even though it is not allowed to use \(U_{t}^{\rm val}\) for tuning \(\gamma_{t}\), it is possible to take a grid of values of \(\gamma_{t}\) and a union bound over the grid, and then select the best value from the grid based the value of the bound evaluated on \(U_{t}^{\rm val}\).

## 5 Experiments

In this section, we provide an empirical comparison of our Recursive PAC-Bayes (RPB) procedure to the following prior work: i) Uninformed priors (Uninformed), (Dziugaite and Roy, 2017); ii) Data-informed priors (Informed) (Ambroladze et al., 2007; Perez-Ortiz et al., 2021); iii) Data-informed prior + excess loss (Informed + Excess) (Mhammedi et al., 2019; Wu and Seldin, 2022). All the experiments were run on a laptop. The source code for replicating the experiments is available at Github1.

Footnote 1: https://github.com/pyijizezhang/rpb

We start with describing the details of the optimization procedure, and then present the results.

### Details of the optimization and evaluation procedure

We constructed \(\pi_{1}^{*},\ldots,\pi_{T}^{*}\) sequentially using the optimization objective, (4) for \(\pi_{1}^{*}\) and (5) for \(\pi_{2}^{*}\) to \(\pi_{T}^{*}\), and computed the bound using the recursive procedure in Theorem 5. There are a few technical details concerning convexity of the optimization procedure and infinite size of the set of prediction rules \(\mathcal{H}\) that we address next.

#### 5.1.1 Convexification of the loss functions

The functions \(f_{\gamma_{i}|j}(h,(X,Y,h^{\prime}))\) defined in Section 4 are non-convex and non-differentiable: \(f_{\gamma_{i}|j}(h,(X,Y,h^{\prime}))=\mathds{1}\big{(}f_{\gamma_{t}}(h,(X,Y,h^{ \prime}))\geq b_{i|j}\big{)}=\mathds{1}\big{(}\ell(h(X),Y)-\gamma_{t}\ell(h^{ \prime}(X),Y)\geq b_{i|j}\big{)}\). In order to facilitate optimization, we approximate the external indicator function \(\mathds{1}(z\geq z_{0})\) by a sigmoid function \(\omega(z;c_{1},z_{0})=(1+\exp(c_{1}(z-z_{0})))^{-1}\) with a fixed parameter \(c_{1}>0\) specified in Appendix B.3.

Furthermore, since the zero-one loss \(\ell(h(X),Y)\) is also non-differentiable, we adopt the cross-entropy loss, as in most modern training procedures (Perez-Ortiz et al., 2021). Specifically, for a \(k\)-class classification problem, let \(h:\mathcal{X}\rightarrow\mathbb{R}^{k}\) represent the function implemented by the neural network, assigning each class a real value. Let \(u=h(X)\) be the assignment, with \(u_{i}\) being the \(i\)-th value of the vector. To convert this real-valued vector into a probability distribution over classes, we apply the softmax function \(\sigma:\mathbb{R}^{k}\rightarrow\Delta^{k-1}\), where \(\sigma(u)_{i}=\exp(c_{2}u_{i})/\sum_{j}\exp(c_{2}u_{j})\) for some \(c_{2}>0\) for each entry. The cross-entropy loss \(\ell^{\text{ce}}:\mathbb{R}^{k}\times[k]\rightarrow\mathbb{R}\) is defined by \(\ell^{\text{ce}}(u,Y)=-\log(\sigma(u)_{Y})\). However, since this loss is unbounded, whereas the PAC-Bayes-kl bound requires losses within \([0,1]\), we enforce a \([0,1]\)-valued cross-entropy loss by mixing the output distribution with a uniform distribution \(\sigma(u)\), i.e., \(\tilde{\sigma}(u)_{i}=(1-p_{\min})\sigma(u)_{i}+p_{\min}/k\) for all \(i\in[k]\) and for some \(p_{\min}>0\), and then rescaling it to \([0,1]\) by taking \(\tilde{\ell}^{\text{ce}}(u,Y)=-\log(\tilde{\sigma}(u)_{Y})/\log(k/p_{\min})\).

We emphasize that in the evaluation of the bound (using Theorem 5), we directly compute the zero-one loss and the \(f_{\gamma_{t}|j}\) functions without employing the approximations.

#### 5.1.2 Relaxation of the PAC-Bayes-kl bound

The PAC-Bayes-kl bound is often criticized for being unfriendly to optimization (Rodriguez-Galvez et al., 2024). Therefore, several relaxations have been proposed, including the PAC-Bayes-classic bound (McAllester, 1999), the PAC-Bayes-\(\lambda\) bound (Thiemann et al., 2017), and the PAC-Bayes-quadratic bound (Rivasplata et al., 2019; Perez-Ortiz et al., 2021), among others. In our optimization we have adopted the bound of McAllester (1999) instead of the kl-based bounds in Equation (5).

We again emphasize that in the evaluation of the bound we used the kl-based bounds in Theorem 5.

#### 5.1.3 Estimation of \(\mathbb{E}_{\pi}[\cdot]\)

Due to the infinite size of \(\mathcal{H}\) and lack of a closed-form expression for \(\mathbb{E}_{\pi_{1}}[\hat{L}(h,S)]\) and \(\mathbb{E}_{\pi_{t}}[\hat{F}_{\gamma_{t}|j}(h,U_{t}^{\text{val}}\circ\hat{ \pi}_{t-1}^{*})]\) appearing in Theorem 5, we approximate them by sampling (Perez-Ortiz et al., 2021). For optimization, we sample one classifier for each mini-batch during stochastic gradient descent. For evaluation, we sample one classifier for each data in the corresponding evaluation dataset. Due to approximation of the empirical quantities the final bound in Theorem 5 requires an additional concentration bound. (We note that the extra bound is only required for computation of the final bound, but not for optimization of \(\hat{\pi}_{t}^{*}\).) Specifically, let \(\hat{\pi}_{t}^{*}=\{h_{1}^{\pi_{1}},h_{2}^{\pi_{1}},\ldots,h_{m}^{\pi_{t}}\}\) be \(m\) prediction rules sampled independently according to \(\pi_{t}\). Then for any function \(f(h)\) taking values in \([0,1]\) (which is the case for \(\hat{L}(h,S)\) and \(\hat{F}_{\gamma_{t}|j}(h,U_{t}^{\text{val}}\circ\hat{\pi}_{t-1}^{*})\)) and \(\delta^{\prime}\in(0,1)\) we have

\[\mathbb{P}\Bigg{(}\mathbb{E}_{\pi_{t}^{*}}[f(h)]\geq\mathrm{kl}^{-1,+}\left( \frac{1}{m}\sum_{i=1}^{m}f(h_{i}^{\pi_{i}^{*}}),\frac{1}{m}\log\frac{1}{\delta^{ \prime}}\right)\Bigg{)}\leq\delta^{\prime}.\]

It is worth noting that \(\mathbb{E}_{\pi_{t}^{*}}[f(h)]\) is evaluated for a fixed \(\pi_{t}^{*}\), meaning that there is no selection involved, and therefore no \(\mathrm{KL}\) term appears in the bound above. We, of course, take a union bound over all the quantities being estimated.

### Experimental results

We evaluated our approach and compared it to prior work using multi-class classification tasks on MNIST (LeCun and Cortes, 2010) and Fashion MNIST (Xiao et al., 2017) datasets, both with 60000 training data. The experimental setup was based on the work of Dziugaite and Roy (2017) and Perez-Ortiz et al. (2021). Similar to them we used Gaussian distributions for all the priors and posteriors, modeled by probabilistic neural networks. Technical details are provided in B.

The empirical evaluation is presented in Table 1. For the Uninformed approach, we trained and evaluated the bound using the entire training dataset directly. For the other two baseline methods, Informed and Informed + Excess Loss, we used half of the training data to train the informed prior and an ERM \(h^{*}\) for the excess loss, and the other half to learn the posterior. For our Recursive PAC-Bayes, we chose \(\gamma_{t}=1/2\) for all \(t\), and conducted experiments with \(T=2,4,6,8\) to study the impact of recursion depth. (Each value of \(T\) corresponded to a separate run of the algorithm and a separate evaluation of the bound, i.e., they should not be seen as successive refinements.) We applied a geometric data split. Specifically, for \(T=2\) the split was (30000, 30000) points; for \(T=4\), it was (7500, 7500, 15000, 30000); for \(T=6\) it was (1875, 1875, 3750, 7500, 15000, 30000); and for \(T=8\), it was (469, 469, 937, 1875, 3750, 7500, 15000, 30000). This approach allowed the early recursion steps, which had fewer data points, to efficiently learn the prior, while preserving enough data for fine-tuning in the later steps. Note that with this approach the value of \(n_{t}^{\rm val}=|U_{t}^{\rm val}|=\sum_{s=t}^{T}|S_{s}|\), which is in the denominator of the bounds in Theorem 5, is at least \(\frac{n}{2}\).

Table 1 shows that even with only \(T=2\), which corresponds to the data split used in the Informed and the Informed + Excess Loss approaches, RPB achieves better test performance than prior work. As the recursion deepens, further improvements in both the test error and the bound are observed. We note that while the bound for \(T=2\) is looser compared to the Informed + Excess Loss method, deeper recursion yields bounds that are tighter. Overall, deep recursion provides substantial improvements in the bound and the test error relative to prior work.

Tables 2 and 3 provide a glimpse into the training progress of RPB with \(T=8\) by showing the evolution of the key quantities along the recursive process. Similar tables for other values of \(T\) are provided in Appendix B.4, along with training details for other methods. The tables show an impressive reduction of the KL term and significant improvement of the bound as the recursion proceeds, demonstrating effectiveness of the approach.

\begin{table}
\begin{tabular}{l|c c c|c c c}  & \multicolumn{3}{c|}{MNIST} & \multicolumn{3}{c}{Fashion MNIST} \\  & Train 0-1 & Test 0-1 & Bound & Train 0-1 & Test 0-1 & Bound \\ \hline Uninf. &.343 (2e-3) &.335 (3e-3) &.457 (2e-3) &.382 (2e-3) &.384 (2e-3) &.464 (2e-3) \\ Inf. &.377 (8e-4) &.371 (6e-3) &.408 (9e-4) &.412 (1e-3) &.413 (6e-3) &.440 (1e-3) \\ Inf. + Ex. &.157 (2e-3) &.151 (3e-3) &.192 (2e-3) &.280 (4e-3) &.285 (5e-3) &.342 (6e-3) \\ RPB \(T=2\) &.143 (2e-3) &.139 (3e-3) &.321 (3e-3) &.257 (3e-3) &.266 (5e-3) &.404 (3e-3) \\ RPB \(T=4\) &.112 (1e-3) &.109 (1e-3) &.203 (8e-4) &.203 (2e-3) &.213 (3e-3) &.293 (1e-3) \\ RPB \(T=6\) &.103 (1e-3) &.101 (1e-3) &.166 (1e-3) &.186 (4e-4) &.198 (1e-3) &.255 (1e-3) \\ RPB \(T=8\) & **.101 (1e-3)** & **.097 (2e-3)** & **.158 (2e-3)** & **.181 (1e-3)** & **.192 (3e-3)** & **.242 (1e-3)** \\ \end{tabular}
\end{table}
Table 1: Comparison of the classification loss of the final posterior \(\rho\) on the entire training data, \(\mathbb{E}_{\rho}[\hat{L}(h,S)]\) (Train 0-1), and on the testing data, \(\mathbb{E}_{\rho}[\hat{L}(h,S^{\rm test})]\) (Test 0-1), and the corresponding bounds for each method on MNIST and Fashion MNIST. We report the mean and one standard deviation over 5 repetitions. “Unif.” abbreviates the Uniform approach, “Inf.” the Informed, “Inf. + Ex.” the Informed + Excess Loss, and “RPB” the Recursive PAC-Bayes.

\begin{table}
\begin{tabular}{l|c c c c|c|c} \(t\) & \(n_{t}^{\rm val}\) & \(\mathbb{E}_{\pi_{t}}[\hat{F}_{\gamma_{t}}(h,U_{t}^{\rm val}\circ\hat{\pi}_{t-1 })]\) & \(\frac{\mathrm{KL}(\pi_{t}^{*}|\pi_{t-1}^{*})}{n_{t}^{\rm val}}\) & \(\mathcal{E}_{t}(\pi_{t}^{*},\gamma_{t})\) & \(B_{t}(\pi_{t}^{*})\) & Test 0-1 \\ \hline
1 & 60000 &.009 (3e-4) &.612 (9e-3) &.532 (.011) \\
2 & 59532 & -0.046 (4e-3) &.031 (1e-3) &.114 (2e-3) &.421 (5e-3) &.215 (7e-3) \\
3 & 59063 &.040 (3e-3) &.013 (9e-4) &.125 (3e-3) &.336 (2e-3) &.146 (3e-3) \\
4 & 58125 &.049 (1e-3) &.005 (3e-4) &.099 (1e-3) &.267 (7e-4) &.120 (2e-3) \\
5 & 56250 &.052 (4e-4) &.002 (1e-4) &.083 (1e-3) &.217 (1e-3) &.111 (2e-3) \\
6 & 52500 &.051 (1e-3) &.001 (4e-5) &.076 (1e-3) &.185 (1e-3) &.104 (2e-3) \\
7 & 45000 &.050 (1e-3) & 8e-4 (6e-5) &.073 (1e-3) &.166 (1e-3) &.099 (1e-3) \\
8 & 30000 &.050 (1e-3) & 6e-4 (4e-5) &.074 (1e-3) &.158 (2e-3) &.097 (2e-3) \\ \end{tabular}
\end{table}
Table 2: Insight into the training process of the Recursive PAC-Bayes for \(T=8\) on MNIST. The table shows the evolution of \(\mathcal{E}_{t}(\pi_{t}^{*},\gamma_{t})\), \(B_{t}(\pi_{t}^{*})\), and other quantities as the training progresses with \(t\). We define \(\hat{F}_{\gamma_{t}}(h,U_{t}^{\rm val}\circ\hat{\pi}_{t-1})=-\gamma_{t}+\sum_{j=1}^ {3}(b_{t|j}-b_{t|j-1})\hat{F}_{\gamma_{t}|j}(h,U_{t}^{\rm val}\circ\hat{\pi}_{t- 1})\).

## 6 Discussion

We have presented the first PAC-Bayesian bound that supports sequential prior updates and preserves confidence information on the prior. The work closes a long-standing gap between Bayesian and Frequentist learning by making sequential data processing and sequential updates of prior knowledge meaningful and beneficial in the frequentist framework, as it has always been in the Bayesian framework. We have shown that apart from theoretical beauty the approach is highly beneficial in practice.

The Recursive PAC-Bayes framework is extremely rich and powerful, and leads to numerous directions for future research, some of which we briefly sketch next.

* The decomposition in (2) applies to any loss function, including unbounded losses. It would be interesting to find additional applications to it.
* While we have restricted ourselves to the zero-one loss function to illustrate the use of PAC-Bayes-split-kl, the results can be directly generalized to any bounded loss function by replacing PAC-Bayes-split-kl with PAC-Bayes-Empirical-Bernstein or PAC-Bayes-Unexpected-Bernstein, and deriving the corresponding analogue of Theorem 5 (which is straightforward).
* We have shown that the bound works well with geometric split of the data, but there are many other ways to split the data which could be studied.
* There is also a lot of space for experimentation with optimization of \(\gamma_{t}\).
* It would be interesting to study how the bound will perform in sequential learning settings, where the data arrives sequentially, and thus the partition is dictated externally.
* There are many interesting research directions from the computational perspective. We note that for base models with linear computational complexity (e.g., neural networks) the overhead of recursion is relatively small and optimization time of Recursive PAC-Bayes is comparable to processing all data at once or in two chunks (as in data-dependent priors). For base models with superlinear computational complexity (e.g., kernel SVMs) sequential training of several small models in the recursion may actually be cheaper than training a big model based on all the data. Moreover, since the bound in Theorem 5 holds for any sequence of distributions \(\pi_{0}^{*},\pi_{1}^{*},\ldots,\pi_{T}^{*}\), the optimization in equation (5) is allowed to be approximate. Considering that the improvement of the bounds and the test loss relative to prior work was very significant, there is space to look at the trade-off between statistical power and computational complexity. Namely, it may potentially be possible to relax the approximation of \(\arg\min\) in equation (5) to gain computational speed-up at the cost of only a small compromise on the bounds and test losses.
* We note that it is possible to start the recursion at \(\pi_{0}\). Namely, it is possible to use, for example, Theorem 2 to bound \(\mathbb{E}_{\pi_{0}}[L(h)]\) using all the data, and apply the recursive decomposition (2) starting from \(\pi_{1}\). Whether this would yield an advantage relative to starting the recursion at \(\pi_{1}\), as we did, remains to be studied.

## Acknowledgments and Disclosure of Funding

YW acknowledges support from the Novo Nordisk Foundation, grant number NNF21OC0070621. YZ acknowledge Ph.D. funding from Novo Nordisk A/S. BECA acknowledges funding from the ANR grant project BACKUP ANR-23-CE40-0018-01.

\begin{table}
\begin{tabular}{c|c c c c|c|c} \(t\) & \(n_{t}^{\rm val}\) & \(\mathbb{E}_{\pi_{1}}[\hat{F}_{\gamma_{t}}(h,U_{t}^{\rm val}\circ\hat{\pi}_{t-1 })]\) & \(\frac{\mathrm{KL}(\pi_{1}^{*}|\pi_{t-1}^{*})}{n_{t}^{\rm val}}\) & \(\mathcal{E}_{t}(\pi_{t}^{*},\gamma_{t})\) & \(B_{t}(\pi_{t}^{*})\) & Test 0-1 \\ \hline
1 & 60000 & &.003 (7e-5) & &.733 (7e-3) &.686 (8e-3) \\
2 & 59532 & -0.043 (8e-3) &.023 (6e-4) &.104 (9e-3) &.470 (8e-3) &.309 (7e-3) \\
3 & 59063 &.083 (4e-3) &.008 (3e-4) &.161 (3e-3) &.396 (4e-3) &.242 (1e-3) \\
4 & 58125 &.090 (3e-3) &.004 (5e-4) &.142 (4e-3) &.341 (5e-4) &.216 (5e-3) \\
5 & 56250 &.093 (3e-3) &.001 (2e-4) &.126 (3e-3) &.297 (4e-3) &.204 (4e-3) \\
6 & 52500 &.090 (1e-3) & 6e-4 (6e-5) &.117 (1e-3) &.265 (1e-3) &.195 (3e-3) \\
7 & 45000 &.090 (1e-3) & 4e-4 (2e-5) &.115 (1e-3) &.248 (1e-3) &.195 (5e-4) \\
8 & 30000 &.090 (1e-3) & 4e-4 (1e-5) &.117 (1e-3) &.242 (1e-3) &.192 (3e-3) \\ \end{tabular}
\end{table}
Table 3: Insight into the training process of the Recursive PAC-Bayes for \(T=8\) on Fashion MNIST.

## References

* Ambroladze et al. (2007) Amiran Ambroladze, Emilio Parrado-Hernandez, and John Shawe-Taylor. Tighter PAC-Bayes bounds. In _NeurIPS_, 2007.
* Biggs and Guedj (2023) Felix Biggs and Benjamin Guedj. Tighter PAC-Bayes generalisation bounds by leveraging example difficulty. In _Proceedings on the International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2023.
* Chugg et al. (2023) Ben Chugg, Hongjian Wang, and Aaditya Ramdas. A unified recipe for deriving (time-uniform) PAC-Bayes bounds. _Journal of Machine Learning Research_, 24(372), 2023.
* Cover and Thomas (2006) Thomas M. Cover and Joy A. Thomas. _Elements of Information Theory_. Wiley Series in Telecommunications and Signal Processing, 2nd edition, 2006.
* Dziugaite and Roy (2017) Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In _Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)_, 2017.
* Foong et al. (2021) Andrew Foong, Wessel Bruinsma, David Burt, and Richard Turner. How tight can pac-bayes be in the small data regime? In _NeurIPS_, 2021.
* Foong et al. (2022) Andrew Y. K. Foong, Wessel P. Bruinsma, and David R. Burt. A note on the chernoff bound for random variables in the unit interval. _arXiv preprint arXiv.2205.07880_, 2022.
* Haddouche and Guedj (2023) Maxime Haddouche and Benjamin Guedj. PAC-Bayes generalisation bounds for heavy-tailed losses through supermartingales. _Transactions on Machine Learning Research Journal_, 2023.
* Jang et al. (2023) Kyoungseok Jang, Kwang-Sung Jun, Ilja Kuzborskij, and Francesco Orabona. Tighter PAC-Bayes bounds through coin-betting. In _Proceedings of the Conference on Learning Theory (COLT)_, 2023.
* Langford (2005) John Langford. Tutorial on practical prediction theory for classification. _Journal of Machine Learning Research_, 6, 2005.
* LeCun and Cortes (2010) Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/exdb/mnist/.
* Maurer (2004) Andreas Maurer. A note on the PAC-Bayesian theorem. arXiv preprint cs/0411099, 2004.
* McAllester (1998) David McAllester. Some PAC-Bayesian theorems. In _Proceedings of the Conference on Learning Theory (COLT)_, 1998.
* McAllester (1999) David McAllester. Some PAC-Bayesian theorems. _Machine Learning_, 37, 1999.
* Mhammedi et al. (2019) Zakaria Mhammedi, Peter Grunwald, and Benjamin Guedj. PAC-Bayes un-expected Bernstein inequality. In _NeurIPS_, 2019.
* Perez-Ortiz et al. (2021) Maria Perez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesvari. Tighter risk certificates for neural networks. _Journal of Machine Learning Research_, 2021.
* Rivasplata et al. (2019) Omar Rivasplata, Vikram M Tankasali, and Csaba Szepesvari. Pac-bayes with backprop. _arXiv preprint arXiv:1908.07380_, 2019.
* Rodriguez-Galvez et al. (2024) Borja Rodriguez-Galvez, Ragnar Thobaben, and Mikael Skoglund. More PAC-Bayes bounds: From bounded losses, to losses with general tail behaviors, to anytime validity. _Journal of Machine Learning Research_, 25(110), 2024.
* Seeger (2002) Matthias Seeger. PAC-Bayesian generalization error bounds for Gaussian process classification. _Journal of Machine Learning Research_, 3, 2002.
* Shawe-Taylor and Williamson (1997) John Shawe-Taylor and Robert C. Williamson. A PAC analysis of a Bayesian estimator. In _Proceedings of the Conference on Learning Theory (COLT)_, 1997.
* Thiemann et al. (2017) Niklas Thiemann, Christian Igel, Olivier Wintenberger, and Yevgeny Seldin. A strongly quasiconvex PAC-Bayesian bound. In _Proceedings of the International Conference on Algorithmic Learning Theory (ALT)_, 2017.
* Tikhonov and Vedral (2017)Ilya Tolstikhin and Yevgeny Seldin. PAC-Bayes-Empirical-Bernstein inequality. In _NeurIPS_, 2013.
* Wu and Seldin (2022) Yi-Shan Wu and Yevgeny Seldin. Split-kl and PAC-Bayes-split-kl inequalities for ternary random variables. In _NeurIPS_, 2022.
* Wu et al. (2021) Yi-Shan Wu, Andres Masegosa, Stephan Sloth Lorenzen, Christian Igel, and Yevgeny Seldin. Chebyshev-cantelli pac-bayes-bennett inequality for the weighted majority vote. In _NeurIPS_, 2021.
* Xiao et al. (2017) Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.

Illustrations

In this appendix we provide graphical illustrations of the basic concepts presented in the paper.

Figure 1: **Evolution of PAC-Bayes.** The figure shows how data are used by different PAC-Bayes approaches. Dark yellow shows data used directly for optimization of the indicated quantities. Light yellow shows data involved indirectly through dependence on the prior. Light green shows data used for estimation of the indicated quantities. In Recursive PAC-Bayes data are released and used sequentially chunk-by-chunk, as indicated by the dashed lines. For example, in the \(T=4\) case \(\mathbb{E}_{\pi_{1}}[L(h)]\) is first evaluated on \(S_{1}\) to construct \(\pi_{1}\) and \(\gamma_{2}\), then in the first recursion step on \(S_{1}\cup S_{2}\), in the second step on \(S_{1}\cup S_{2}\cup S_{3}\), and in the last step on all \(S\).

Figure 3: **Decomposition of a discrete random variable into a superposition of binary random variables.** The figure illustrates a decomposition of a discrete random variable \(Z\) with domain of four values \(b_{0}<b_{1}<b_{2}<b_{3}\) into a superposition of three binary random variables, \(Z=b_{0}+\sum_{j=1}^{3}\alpha_{j}Z_{|j}\). A way to think about the decomposition is to compare it to a progress bar. In the illustration \(Z\) takes value \(b_{2}\), and so the random variables \(Z_{|1}\) and \(Z_{|2}\) corresponding to the first two segments “light up” (take value 1), whereas the random variable \(Z_{|3}\) corresponding to the last segment remains “turned off” (takes value 0). The value of \(Z\) equals the sum of the lengths \(\alpha_{j}\) of the “lighted up” segments.

Figure 2: **Recursive Decomposition into Three Terms.** The figure illustrates recursive decomposition of \(\mathbb{E}_{\pi_{3}}[L(h)]\) into three terms based on equation (2), and a geometric data split, as used in our experiments. The bottom line illustrates which data are used for construction of which distribution: \(S_{1}\) for \(\pi_{1}\); \(S_{2}\) for \(\pi_{2}\); and \(S_{3}\) for \(\pi_{3}\). The brackets above the data show which data are used for computing PAC-Bayes bounds for which term: \(S_{1}\cup S_{2}\cup S_{3}\) for \(\mathbb{E}_{\pi_{1}}[L(h)]\); \(S_{2}\cup S_{3}\) for \(\mathbb{E}_{\pi_{2}}[L(h)-\gamma_{2}\mathbb{E}_{\pi_{1}}[L(h^{\prime})]]\); and \(S_{3}\) for \(\mathbb{E}_{\pi_{3}}[L(h)-\gamma_{3}\mathbb{E}_{\pi_{2}}[L(h^{\prime})]]\). Note that a direct computation of a PAC-Bayes bound on \(\mathbb{E}_{\pi_{3}}[L(h)]\) would have only allowed to use the data in \(S_{3}\), as shown by the black dashed line. The figure illustrates that recursive decomposition provides more efficient use of the data. We also note that initially we start with poor priors, and so the \(\mathrm{KL}(\pi_{t}\|\pi_{t-1})\) term for small \(t\) is expected to be large, but this is compensated by a small multiplicative factor \(\prod_{i=t+1}^{t}\gamma_{i}\) and availability of a lot of data \(\bigcup_{i=t}^{T}S_{i}\) for computing the PAC-Bayes bound. For example, \(\mathbb{E}_{\pi_{1}}[L(h)]\) is multiplied by \(\gamma_{3}\gamma_{2}\) and we can use all the data for computing a PAC-Bayes bound on this term. By the time we reach higher \(t\), the priors \(\pi_{t-1}\) get better, and the \(\mathrm{KL}(\pi_{t}\|\pi_{t-1})\) term in the bounds gets much smaller, and additionally the bounds benefit from the small variance of the excess loss. With geometric split of the data, we use little data to quickly move \(\pi_{t}\) to a good region, and then we still have enough data for a good estimation of the later terms, like \(\mathbb{E}_{\pi_{3}}[L(h)-\gamma_{3}\mathbb{E}_{\pi_{2}}[L(h^{\prime})]]\).

Experimental details

In this section, we provide the details of the datasets in Appendix B.1, our neural network architectures in Appendix B.2, and other details in Appendix B.3. We provide further statistics for all the methods on both datasets in Appendix B.4.

### Datasets

We perform our evaluation on two datasets, MNIST (LeCun and Cortes, 2010) and Fashion MNIST (Xiao et al., 2017). We will introduce these two datasets in the following.

#### b.1.1 Mnist

The MNIST (Modified National Institute of Standards and Technology) dataset is one of the most renowned and widely used datasets in the field of machine learning, particularly for training and testing in the domain of image processing and computer vision. It consists of a large collection of handwritten digit images, spanning the numbers 0 through 9.

The MNIST dataset comprises a total of 70,000 grayscale images of handwritten digits, where the training set has 60,000 images and the test set has 10,000 images. Each image in the dataset is 28x28 pixels, resulting in a total of 784 pixels per image. The images are in grayscale, with pixel values ranging from 0 (black) to 255 (white). Each image is associated with a label from 0 to 9, indicating the digit that the image represents. The images are typically stored in a single flattened array of 784 elements, although they can also be represented in a 28x28 matrix format.

#### b.1.2 Fashion MNIST

The Fashion MNIST dataset is a contemporary alternative to the traditional MNIST dataset, created to provide a more challenging benchmark for machine learning algorithms. It consists of images of various clothing items and accessories, offering a more complex and varied dataset for image classification tasks.

The Fashion MNIST dataset contains a total of 70,000 grayscale images, where the training set has 60,000 images and the test set has 10,000 images. Each image in the dataset is 28x28 pixels, resulting in a total of 784 pixels per image. The images are in grayscale, with pixel values ranging from 0 (black) to 255 (white). Each image is associated with one of 10 categories, representing different types of fashion items. The categories are: 1. T-shirt/top 2. Trouser 3. Pullover 4. Dress 5. Coat 6. Sandal 7. Shirt 8. Baeker 9. Bag 10. Ankle boot. Similar to MNIST, the images are stored in a single flattened array of 784 elements but can also be represented in a 28x28 matrix format.

### Neural network architectures

For all methods, we adopt a family of factorized Gaussian distributions to model both priors and posteriors, characterized by the form \(\pi=\mathcal{N}(w,\sigma\mathbf{I})\) where \(w\in\mathbb{R}^{d}\) denotes the mean vector, and \(\sigma\) represents the scalar variance. We use feedforward neural networks for the MNIST dataset (LeCun and Cortes, 2010), while using convolutional neural networks for the Fashion MNIST dataset (Xiao et al., 2017).

Both our feedforward neural network and convolutional neural network are probabilistic, and each layer has a factorized (i.e. mean-field) Gaussian distribution.

Our feedforward neural network has the following architecture:

1. Input layer. Input size: \(28\times 28\) (flattened to 784 features).
2. Probabilistic linear layer 1. Input features: 784, output features: 600, activation: ReLU.
3. Probabilistic linear layer 2. Input features: 600, output features: 600, activation: ReLU.
4. Probabilistic linear layer 3. Input features: 600, output features: 600, activation: ReLU.
5. Probabilistic linear layer 4. Input features: 600, output features: 10, activation: Softmax.

Our convolutional neural network has the following architecture:1. Input layer. Input size: \(1\times 28\times 28\).
2. Probabilistic convolutional layer 1. Input channels: 1, output channels: 32, kernel size: 3x3, activation: ReLU.
3. Probabilistic convolutional layer 2. Input channels: 32, output channels: 64, kernel size: 3x3, activation: ReLU.
4. Max pooling layer. Pooling size: 2x2.
5. Flattening layer. Flattens the output from the previous layers into a single vector.
6. Probabilistic linear layer 1. Input features: 9216, output features: 128, activation: ReLU.
7. Probabilistic linear layer 2 (output layer). Input features: 128, output features: 10, activation: Softmax.

### Other details in the experiments

General for all methodsThe methods in comparisons are trained and evaluated using the procedure described in Section 2 and visually illustrated in Figure 1. We will provide some further details for each method later in the following. For all methods in comparison, we apply the optimization and evaluation method described in Section 5.1. For the approximation described in Section 5.1.1, we set the parameters \(c_{1}=c_{2}=5\). The lower bound for the prediction \(p_{\min}=1e-5\). The \(\delta\) in our bound and all the other methods is selected to be \(\delta=0.025\). As mentioned in Section 5.1.2, we use the PAC-Bayes-classic bound by McAllester in replacement of PAC-Bayes-\(\mathrm{kl}\) when doing optimization. Note that for all methods, we also have to estimate the empirical loss of the posterior \(\mathbb{E}_{\pi}[\cdot]\) described in Section 5.1.3. We also allocate the budget for the union bound for the estimation such that these estimations in the bound are controlled with probability at least \(1-\delta^{\prime}\), where we chose \(\delta^{\prime}=0.01\). Therefore, the ultimate bounds for all methods hold with probability at least \(1-\delta-\delta^{\prime}\). Note that we do not consider such bounds during optimization but only when estimating the bounds.

For all methods, we adopt a family of factorized Gaussian distributions to model both priors and posteriors of all the learnable parameters of the classifiers, characterized by the form \(\pi=\mathcal{N}(w,\sigma\mathbf{I})\) where \(w\in\mathbb{R}^{d}\) denotes the mean vector, and \(\sigma\) represents the scalar variance. For all methods, we initialize an uninformed prior \(\pi_{0}=\mathcal{N}(w_{0},\sigma_{0}\mathbf{I})\) that is independent of data, where the mean is randomly initialized, and the variance \(\sigma_{0}\) is initialized to 0.03 (Perez-Ortiz et al., 2021).

In the training process of all methods in our experiments, we set the batch size to 250, the number of training epochs to 200, and use stochastic gradient descent with a learning rate of 0.005 and a momentum of 0.95.

Uninformed priorsWe take \(\pi_{0}\) defined above as the uninformed prior. We then learn the posterior \(\rho\) from the prior using the entire training dataset \(S\), applying a PAC-Bayes bound. We evaluate the bound using, again, the entire training dataset \(S\).

Data-informed priorsWe start with the same \(\pi_{0}\) as the uninformed prior. We train the informed prior \(\pi_{1}\) using \(S_{1}\) with \(|S_{1}|=|S|/2\) by minimizing a PAC-Bayes bound. The posterior \(\rho\) is then learned using the informed prior \(\pi_{1}\) and the subset \(S_{2}\) with \(|S_{2}|=|S|/2\), again by minimizing a PAC-Bayes bound. The bound is evaluated using \(S_{2}\).

Data-informed priors + excess lossWe train the informed prior \(\pi_{1}\) and the reference classifier \(h^{*}\) using \(S_{1}\) that contains half of the training dataset. \(\pi_{1}\) is obtained by minimizing a PAC-Bayes bound with the uninformed prior \(\pi_{0}\), while the reference classifier \(h^{*}\) is obtained by an empirical risk minimizer (ERM). The posterior \(\rho\) is obtained by minimizing a PAC-Bayes bound on the excess loss between \(\rho\) and \(h^{*}\). The prior used in the bound for both training and evaluation is the data-informed prior \(\pi_{1}\). Therefore, the data for both training and evaluation of \(\rho\) must be the other half of data \(S_{2}\).

### Further results for the experiments

In this section, we report some more statistics for all methods.

[MISSING_PAGE_FAIL:17]

#### b.4.3 Data-informed priors

We report the additional results of data-informed priors (Ambroladze et al., 2007) on MNIST and Fashion MNIST in Table 11. As described earlier in Section 2, Section 5, and Section B.3, we evaluate the bound using \(S_{2}\) that is independent of the data-informed prior \(\pi_{1}\).

#### b.4.4 Data-informed priors + excess loss

We report the additional results of data-informed priors + excess loss (Mhammedi et al., 2019) on MNIST and Fashion MNIST in Table 12 and 13. As described earlier in Section 2, Section 5, and Section B.3, we evaluate the bound using \(S_{2}\) that is independent of the data-informed prior \(\pi_{1}\) and the reference prediction rule \(h^{*}\). The bound is composed of two parts: a bound on the excess loss of \(\rho\) with respect to \(h^{*}\) (Excess bound) and a single hypothesis bound on \(h^{*}\) (\(h^{*}\) bound). We report the two components of the bound in Table 12. We provide further details to compute these bounds from the losses of their corresponding quantities in Table 13.

\begin{table}
\begin{tabular}{l|c c c|c|c}  & \(\mathbb{E}_{\rho}[\hat{\Delta}(h,h^{*},S_{2})]\) & \(\frac{\mathrm{KL}(\rho\|\pi_{0})}{|S_{2}|}\) & E. Bound & \(\hat{L}(h^{*},S_{2})\) & \(h^{*}\) Bound & Bound \\ \hline MNIST & -0.011 (3e-3) &.035 (5e-4) &.162 (1e-3) &.026 (4e-4) &.029 (4e-4) &.192 (2e-3) \\ F-MNIST &.104 (6e-3) &.018 (5e-4) &.196 (5e-3) &.112 (1e-3) &.145 (1e-3) &.342 (6e-3) \\ \end{tabular}
\end{table}
Table 10: Further details to compute the bound for the uninformed prior approach on MNIST and Fashion MNIST.

\begin{table}
\begin{tabular}{l|c c c|c}  & \(\mathbb{E}_{\rho}[\hat{L}(h,S)]\) & \(\frac{\mathrm{KL}(\rho\|\pi_{0})}{|S_{2}|}\) & Bound & Test 0-1 \\ \hline MNIST &.343 (2e-3) &.023 (4e-5) &.457 (2e-3) &.335 (3e-3) \\ F-MNIST &.382 (2e-3) &.011 (8e-6) &.464 (2e-3) &.384 (5e-3) \\ \end{tabular}
\end{table}
Table 11: Further details to compute the bound for the data-informed prior on MNIST and Fashion MNIST.

\begin{table}
\begin{tabular}{l|c c c|c}  & \(\mathbb{E}_{\rho}[\hat{L}(h,S)]\) & \(\frac{\mathrm{KL}(\rho\|\pi_{0})}{|S_{2}|}\) & Bound & Test 0-1 \\ \hline MNIST &.376 (8e-4) & 8e-4 (9e-6) &.408 (9e-4) &.371 (6e-3) \\ F-MNIST &.412 (1e-3) & 4e-4 (7e-6) &.440 (1e-3) &.413 (6e-3) \\ \end{tabular}
\end{table}
Table 11: Further details to compute the bound for the data-informed prior on MNIST and Fashion MNIST.

\begin{table}
\begin{tabular}{l|c c c|c c|c}  & \(\mathbb{E}_{\rho}[\hat{L}(h,S)]\) & \(\frac{\mathrm{KL}(\rho\|\pi_{0})}{|S_{2}|}\) & Bound & Test 0-1 \\ \hline MNIST &.346 (8e-4) & 8e-4 (9e-6) &.408 (9e-4) &.371 (6e-3) \\ F-MNIST &.412 (1e-3) & 4e-4 (7e-6) &.440 (1e-3) &.413 (6e-3) \\ \end{tabular}
\end{table}
Table 12: Details to compute the bound for the data-informed prior and excess loss on MNIST and Fashion MNIST. The table shows the bound on the excess loss of \(\rho\) with respect to \(h^{*}\) (Excess bound) and a single hypothesis bound on \(h^{*}\) (\(h^{*}\) bound).

\begin{table}
\begin{tabular}{l|c c c|c c|c}  & \(\mathbb{E}_{\rho}[\hat{\Delta}(h,h^{*},S_{2})]\) & \(\frac{\mathrm{KL}(\rho\|\pi_{1})}{|S_{2}|}\) & Ex. Bound & \(\hat{L}(h^{*},S_{2})\) & \(h^{*}\) Bound & Bound \\ \hline MNIST & -0.011 (3e-3) &.035 (5e-4) &.162 (1e-3) &.026 (4e-4) &.029 (4e-4) &.192 (2e-3) \\ F-MNIST &.104 (6e-3) &.018 (5e-4) &.196 (5e-3) &.112 (1e-3) &.145 (1e-3) &.342 (6e-3) \\ \end{tabular}
\end{table}
Table 13: Further details to compute the bound for the data-informed prior and excess loss on MNIST and Fashion MNIST. The table shows the empirical excess loss \(\mathbb{E}_{\rho}[\hat{\Delta}(h,h^{*},S_{2})]\), where we define \(\Delta(h,h^{*},S_{2})=\hat{L}(h,S_{2})-\hat{L}(h^{*},S_{2})\), and its bound (Excess Bound). It also shows the empirical loss of the reference prediction rule \(\hat{L}(h^{*},S_{2})\) and its bound. The computation of such bound does not involve the \(\mathrm{KL}\) term.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We tried to write an abstract and an introduction that reflected the rest of the paper as accurately as possible. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have tried to be as honest as possible in setting the framework of this theoretical work and have explicitly mentioned the limiting assumptions of the paper, e.g. i.i.d. data. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We have clearly stated the assumptions made in the paper, and our theoretical results are accompanied by detailed proofs. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The datasets are available online and we have tried to provide as much detail as possible to make our experiments transparent and reproducible. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code has been uploaded, the datasets are open source. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have introduced many different notations to accurately specify the different data splits and parameters used in our framework, and have tried to detail the optimization procedure to make our results as understandable as possible. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provided for each experiment the corresponding standard deviations over different runs of the experiments. Please refer to the tables in the paper for more details. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: As mentioned in the paper, all the experiments were run on a personal laptop. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have not violated any of the points in the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a theoretical paper with no specific societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The open source datasets we used in the paper were properly cited, namely MNIST (LeCun and Cortes, 2010) and Fashion MNIST (Xiao et al., 2017). Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We have not released any new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper did not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper did not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.