# InversionView: A General-Purpose Method for

Reading Information from Neural Activations

 Xinting Huang

Saarland University

xhuang@lst.uni-saarland.de &Madhur Panwar

EPFL

madhur.panwar@epfl.ch &Navin Goyal

Microsoft Research India

navingo@microsoft.com &Michael Hahn

Saarland University

mhahn@lst.uni-saarland.de

###### Abstract

The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present four case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we show that InversionView can reveal clear information contained in activations, including basic information about tokens appearing in the context, as well as more complex information, such as the count of certain tokens, their relative positions, and abstract knowledge about the subject. We also provide causally verified circuits to confirm the decoded information.1

Footnote 1: Code is available at https://github.com/huangxt39/InversionView

## 1 Introduction

Despite their huge success, neural networks are still widely considered black boxes. One of the most important reasons is that the continuous vector representations in these models pose a significant challenge for interpretation. If we could understand what information is encoded in the activations of a neural model, significant progress might be achieved in fully deciphering the inner workings of neural networks, which would make modern AI systems safer and more controllable. Toward this goal, various methods have been proposed for understanding the inner activations of neural language models. They range from supervised probes [2, 5, 4, 55] to projecting to model's vocabulary space [42, 7] to causal intervention [21, 54, 26, 13] on model's inner states. However, to this date, decoding the information present in neural network activations in human-understandable form remains a major challenge. Supervised probing classifiers require the researcher to decide which specific information to probe for, and does not scale when the space of possible outputs is very large. Projecting to the vocabulary space is restricted in scope, as it only produces individual tokens. Causal interventions uncover information flow, but do not provide direct insight into the information present in activations.

Here, we introduce InversionView as a principled general-purpose method for generating hypotheses about the information present in activations in neural models on language and discrete sequences, which in turn helps us identify how the information flows through the model--crucial for obtaining the algorithm implemented by the model. InversionView aims at providing a direct way of reading outthe information encoded in an activation. The technique starts from the intuition that the information encoded in an activation can be formalized as its _preimage_, the set of inputs giving rise to this particular activation under the given model. In order to explore this preimage, given an activation, we train a decoder to sample from this preimage. Inspection of the preimage, across different inputs, makes it easy to identify which information is passed along, and which information is forgotten. It accounts for the geometry of the representation, and can identify which information is reinforced or downweighted at different model components. InversionView facilitates the interpretation workflow, and provides output that is in principle amenable to automated interpretation via LLMs (we present a proof of concept in Section 4).

We showcase the usefulness of the method in three case studies: a character counting task, Indirect Object Identification, and 3-digit addition. We also present preliminary results on the factual recall task, demonstrating the applicability of our method to larger models. The character counting task illustrates how the method uncovers how information is processed and forgotten in a small transformer. In Indirect Object Identification in GPT2-Small [54], we use InversionView to easily interpret the information encoded in the components identified by Wang et al. [54], substantially simplifying the interpretability workflow. For 3-digit addition, we use InversionView to provide for the first time a fully verified circuit. Across the case studies, InversionView allows us to rapidly generate hypotheses about the information encoded in each activation site. Coupled with attention patterns or patching methods, we reverse-engineer the flow of information, which we verify using causal interventions.

## 2 Methodology

Interpretation FrameworkWhat information does an activation in a neural network encode? InversionView answers this in terms of the inputs that give rise to this activation (Figure 1). For instance, if a certain activation encodes solely that "_the subject is John_" and nothing else (Figure 1, right), then it will remain unchanged when other parts in the sentence change while preserving this aspect (e.g., "_John is on leave today_." "_John has a cute dog_."). From another perspective, if all sentences where the subject is John are represented so similarly that the model cannot distinguish them, given one of these representations, the only information is the commonality "_the subject is John_" (assuming sentences are represented differently when it does not hold). Building on this intuition, given an activation, InversionView aims to find those inputs that give rise to the same activation, and examine what's common among them to infer what information it encodes. In realistic networks, different inputs will rarely give rise to exactly the same activation. Rather, different changes to an input will change the activation to different degrees. The sensitivity of an activation to different changes reflects the representational geometry: larger changes make it easier for downstream components to read out information than very small changes. This motivates a threshold-based definition of preimages, where we consider information as present in an activation when the activation is sufficiently sensitive to it. Formally speaking, given a space \(\mathcal{X}\) of valid inputs, a query input \(\mathbf{x}^{q}\in\mathcal{X}\), a function \(f\) that represents the activation of interest as a function of the input, and a query activation \(\mathbf{z}^{q}=f(\mathbf{x}^{q})\), define the _\(\epsilon\)-preimage_:2

Footnote 2: Strictly speaking, when \(\mathbf{x}^{q}\) is a sequence, we study the vector \(\mathbf{z}^{q}\) corresponding to a specific position \(t\) in this sequence, i.e. \(\mathbf{z}^{q}=f(\mathbf{x}^{q})_{t}\) where \(f(\mathbf{x}^{q})_{t}\) represents taking the activation from the site of interest (abstracted by \(f\)) at position \(t\) in input sequence \(\mathbf{x}^{q}\). In this case, the preimage can more rigorously be defined as \(B_{\mathbf{z}^{q},f,\epsilon}=\{\mathbf{x}:\mathbf{x}\in\mathcal{X},\exists t \in[1,|\mathbf{x}|]:D(f(\mathbf{x})_{t},\mathbf{z}^{q})\leq\epsilon\}\).

\[B_{\mathbf{z}^{q},f,\epsilon}=\{\mathbf{x}\in\mathcal{X}:D(f(\mathbf{x}), \mathbf{z}^{q})\leq\epsilon\},\] (1)

where \(\epsilon>0\) is a threshold and \(D(\cdot,\cdot)\) is a distance metric. Both \(\epsilon\) and \(D(\cdot,\cdot)\) are chosen by the researcher based on representation geometry; we will define these later in case studies. In practice,

Figure 1: Illustration of the geometry at two different activation sites, encoding different information about the input. Top: the semantics of being on leave are encoded. Bottom: the information that the subject of the input sentence is John is encoded.

in all our three case studies, we vary \(\epsilon\) and set it so we can read out coherent concepts from the \(\epsilon\)-preimage (Appendix A.4). With a threshold-based definition, we consider only those pieces of information that have substantial impact on the activation. See more discussion in Appendix A.1.

Conditional Decoder ModelIn this paper, we study the setting where \(\mathbf{x}^{q}\) is a sequence. Directly enumerating \(B_{\mathbf{z}^{q},f,\epsilon}\) is in general not scalable, as the input space grows exponentially with the sequence length. To efficiently inspect \(B_{\mathbf{z}^{q},f,\epsilon}\), we train a conditional decoder model that takes as input the activation \(\mathbf{z}^{q}\) and generates inputs giving rise to similar activations in the model under investigation. In the following, we refer to the original model that we are interpreting as the _probed model_, the conditional decoder as the _decoder_, the place in the probed model from which we take the activation as the _activation site_ (e.g., the output of \(i\)th layer), the inputs generated by the decoder as _samples_, and the index of a token in the sequence as _position_.

We implement the decoder as an autoregressive language model conditioned on \(\mathbf{z}^{q}\), decoding input samples \(\mathbf{x}\) (see Figure 2, and details in Appendix C). As the decoder's training objective corresponds to recovering \(\mathbf{x}\) exactly, sampling at temperature 1 will typically not cover the full \(\epsilon\)-preimage. Thus, for generating elements of the \(\epsilon\)-preimage, we increase diversity by drawing samples at higher temperatures and with noise added to \(\mathbf{z}^{q}\) (details in Appendix A.2). We then evaluate \(D(f(\mathbf{x}),\mathbf{z}^{q})\) at each position in each sample \(\mathbf{x}\), select the position minimizing \(D\),3 determine membership in \(B_{\mathbf{z}^{q},f,\epsilon}\), and subsample in-\(\epsilon\)-preimage and out-of-\(\epsilon\)-preimage samples for inspection.

Footnote 3: In some cases, including Figure 3b 4b, we fix a particular position for interpretation. See Appendix A.3

An important question is whether this method, relying on a black-box decoder, produces valid \(\epsilon\)-preimages. _Correctness_ (are all generated samples in the \(\epsilon\)-preimage?) is ensured by design, as we evaluate \(D(f(\mathbf{x}),\mathbf{z}^{q})\) for each generated sample. The other angle is _completeness_ (are the samples representative of the \(\epsilon\)-preimage?). If some groups of inputs in \(\epsilon\)-preimage are systematically missing from the generations, one may overestimate the information contained in activations. But this behavior would be punished by the training objective, since the loss on these examples would be high. We explicitly verify completeness by enumerating inputs in one of our case studies (Appendix B). Another approach is to design counter-examples \(\mathbf{x}\) not satisfying a hypothesis about the content of \(B_{\mathbf{z}^{q},f,\epsilon}\). In our experiments, we found that these examples were always outside of \(B_{\mathbf{z}^{q},f,\epsilon}\).

## 3 Discovering the Underlying Algorithm by InversionView

Notation.In the transformer architecture, outputs from each layer are added to their inputs due to residual connection. The representations of each token are only updated by additive updates,

Figure 2: **(a)** The probed model is trained on language modeling objective. **(b)** Given a trained probed model, we first cache the internal activations \(\mathbf{z}\) together with their corresponding inputs and activation site indices (omitted in the figure for brevity), then use them to train the decoder. The decoder is trained with language modeling objective, while being able to attend to \(\mathbf{z}\). **(c)** When interpreting a specific query activation \(\mathbf{z}^{\mathbf{q}}\), we give it to the decoder, which generates possible inputs auto-regressively. We then evaluate the distances on the original probed model.

forming a _residual stream_[17]. Using notation based on [17] and [40], we denote the residual stream as \(x^{i,\{\mathrm{pre,mid,post}\}}\in\mathbb{R}^{N\times d}\), where \(i\) is the layer (an attention (sub)layer + an MLP (sub)layer) index, \(N\) is the number of input tokens, \(d\) is the model dimension, \(\mathrm{pre}\), \(\mathrm{mid}\), \(\mathrm{post}\) stand for the residual stream before the attention layer, between attention and MLP layer, and after the MLP layer. For example, \(x^{0,\mathrm{pre}}\) is the sum of token and position embedding, \(x^{0,\mathrm{mid}}\) is the sum of the output of the first attention layer and \(x^{0,\mathrm{pre}}\), and \(x^{0,\mathrm{post}}\) is the sum of the output of the first MLP layer and \(x^{0,\mathrm{mid}}\). Note that \(x^{i,\mathrm{post}}=x^{i+1,\mathrm{pre}}\). We use subscript \(t\) to refer to the activation at token position \(t\), e.g., \(x^{i,\mathrm{mid}}_{t}\in\mathbb{R}^{d}\). The attention layer output decomposes into outputs of individual heads \(h^{i,j}(\cdot)\), i.e., \(x^{i,\mathrm{mid}}=x^{i,\mathrm{pre}}+\sum_{j}h^{i,j}(\text{LN}(x^{i,\mathrm{ pre}}))\), where \(\text{LN}(\cdot)\) represents layer normalization (GPT style/pre-layer-norm). We denote the attention head's output as \(a^{i,j}\), i.e., \(a^{i,j}=h^{i,j}(\text{LN}(x^{i,\mathrm{pre}}))\).

Decoder Architecture.We train a single two-layer transformer decoder across all activation sites of interest. The query activation \(\mathbf{z}^{q}\) is concatenated with an _activation site embedding_\(\mathbf{e}\), a learned embedding layer indicating where the activation comes from, passed through multiple MLP layers with residual connections, and then made available to the attention heads in each layer of the decoder, alongside the already present tokens from the input, so that each attention head can also attend to the post-processed query activation in addition to the context tokens. Each training example is a triple consisting of an activation vector \(\mathbf{z}^{q}\in\mathbb{R}^{d}\), the activation site index, and the input, on which the decoder is trained with a language modeling objective. Appendix C has technical details.

### Character Counting

We train a transformer (2 layers, 1 head) on inputs such as "vvzccvczvvzcvcv\(|\)v:8" to predict the last token "8", the frequency of the target character (here, "v") before the separator "\(|\)". For each input, three distinct characters are sampled from the set of lowercase characters, and each character's frequency is sampled uniformly from 1-9. The input length varies between 7 and 31. We created 1.56M instances and applied a 75%-25% train-test split; test set accuracy is 99.53% (Details in

Figure 3: **InversionView on Character Counting Task. The model counts how often the target character (after ‘1’) occurs in the prefix (before ‘1’). B and E denote beginning and end of sequence tokens. The query activation conditions the decoder to generate samples capturing its information content. We show non-cherrypicked samples inside and outside the \(\epsilon\)-preimage (\(\epsilon=0.1\)) at three activation sites on the same query input. Distance for each sample is calculated between activations corresponding to the parenthesized characters in the query input and the sample. “True count” indicates the correct count of the target character in the samples (decoder may generate incorrect counts). (**a**) _MLP layer amplifies count information_. Comparing the distances before (left) and after (right) the MLP, we see that samples with diverging counts become much more distant from the query activation. (**b**) In the next layer (“:” exclusively attends to target character – copying information from residual stream of target character to the residual stream of “:”), _the count is retained but the identity of the target character is no longer encoded_ (“c”, “m”, etc. instead of “g”), as it is no longer relevant for the predicting the count. Therefore, observing the generations informs us of the activations’ content and how it changes across activation sites.**

Appendix D). We use \(D(\mathbf{z},\mathbf{z}^{q})=\frac{\|\mathbf{z}-\mathbf{z}^{q}\|_{2}}{\|\mathbf{z} ^{q}\|_{2}}\) (i.e., normalized euclidean distance, as the magnitude of activations varies between layers), where \(\mathbf{z}\) denotes the aforementioned \(f(\mathbf{x})\), and set \(\epsilon=0.1\).

Interpreting via InversionView and attention.In layer 0, the target character consistently attends to the same character in the previous context, suggesting that counting happens here. In Figure 2(a), we show the \(\epsilon\)-preimage of \(x^{0,\mathrm{mid}}_{tc}\) and \(x^{0,\mathrm{post}}_{tc}\), where the subscript \(tc\) denotes the target character. We show \(\approx 10\) random samples at a single query input, but our hypotheses are based on--and easily confirmed by--rapid visual inspection of dozens of inputs across different query inputs.4 On the left (before the MLP), the activation encodes the target character, as all samples have "g" as the target character. Count information is not sharply encoded: while the closest activation corresponds to "g" occurring 3 times, two activations corresponding to a count-4 input ("g" occurring 4 times) are also close, even closer than a count-3 input. On the other hand, on the right (after the MLP), only count-3 inputs are inside the \(\epsilon\)-preimage, and count-4 inputs become much more distant than before. Comparing the \(\epsilon\)-preimage before and after the MLP in layer 0, we find that the MLP makes the count information more prominent in the representational geometry of the activation. The examples are not cherry-picked; count information is generally reinforced by the MLP across query inputs.

Footnote 4: The reader can check generations conveniently at https://inversion-view.streamlit.app

In the next layer, the colon consistently attends to the target character, and InversionView confirms that count information is moved to the colon's residual stream (Figure 2(b)). More importantly, this illustrates how information is abstracted: We previously found that \(x^{0,\mathrm{post}}_{tc}\) encodes identity and frequency of the target character. However, the colon obtains only an abstracted version of the information, in which count information remains while the target character is largely (though not completely) removed. InversionView makes this process visible, by showing that the target character becomes interchangeable with little change to the activation. See more examples in Appendix D.2. Overall, with InversionView, we have found a simple algorithm by which the model makes the right prediction: _In layer 0, the target character attends to all its occurrences and obtains the counts. In layer 1, the colon moves the results from the target character to its residual stream and then produces the correct prediction._ Accounting for other activation sites, we find that the model implements a somewhat more nuanced algorithm, investigated in Appendix D.4. Overall, InversionView shows how certain information is amplified, but also how information is abstracted or forgotten.

Quantitative verification.We causally verified our hypothesis using activation patching [53; 21] on (position, head output) pairs. As the attention head in layer 1 attends almost entirely to the target character, only head outputs \(a^{0,0}_{tc}\), \(a^{0,0}_{:,}\) and \(a^{1,0}_{:}\) can possibly play a role in routing count information. We patch their outputs with activations from a contrast example flipping a single character before "l". We patch activations cumulatively, starting either at the lowest or highest layer, with some fixed ordering within each layer. For example, we patch \(a^{0,0}_{:}\) and observe how final logits change compared to the clean run, then we patch both \(a^{0,0}_{:}\) and \(a^{0,0}_{tc}\) and do the same, and so forth. By the end of patching, the model prediction will be flipped. When adding an activation to the patched set, we attribute to it the increment in the difference of \(LD\) before and after patching, where \(LD\) denotes the logit difference between original count and the count in the contrast example. Cumulative patching allows us to observe dependencies: For instance, as we hypothesize that \(a^{1,0}_{:}\) is _completely_ dependent on \(a^{0,0}_{tc}\), we expect that, when \(a^{0,0}_{tc}\) is already patched, patching \(a^{1,0}_{:}\) will have no further effect, whereas when \(a^{0,0}_{tc}\) is not patched, patching \(a^{1,0}_{:}\) will have a significant effect. Results (Figure 3(a)) match our prediction: Patching either of the activation in the hypothesized path (\(a^{0,0}_{tc}\) and \(a^{1,0}_{:}\)) is sufficient to absorb the entire effect on logit differences, confirming the hypothesis. See Appendix D.3 for further details and D.4 for further experiments.

### IOI circuit in GPT-2 small

To test the applicability of InversionView to transformers pretrained on real-world data, we apply our method to the activations in the indirect object identification (IOI) circuit in GPT-2 small [48] discovered by Wang et al. [54]. We apply InversionView to the components of the circuit, read out the information, and compare it with the information or function that Wang et al. [54] had ingeniously inferred using a variety of tailored methods, such as patching and investigating effects on logits and attention. We show that InversionView unveils the information contained in the attention heads' outputs, with results agreeing with those of Wang et al. [54].

The IOI task consists of examples such as "When Mary and John went to the store, John gave a drink to", which should be completed with "Mary". We use S for the subject "John" in the main clause, IO for the indirect object "Mary" introduced in the initial subclause, S1 and S2 for the first and second occurrences of the subject, and END for the "to" after which IO should be predicted. To facilitate comparison, we denote attention heads as in Wang et al. [54] with i.j denoting \(h^{i,j}\). Wang et al. [54] discover a circuit of 26 attention heads in GPT-2 small and categorize them by their function. In short, GPT-2 small makes correct predictions by copying the name that occurs only once in the previous context. For InversionView, we train the decoder on the IOI examples (See details in E.1). Despite the size of the probed model, we find the same 2-layer decoder architecture as in Section 3.1 to be sufficient. We use \(D(\mathbf{z},\mathbf{z}^{q})=1-\frac{\mathbf{z}\cdot\mathbf{z}^{q}}{\|\mathbf{ z}\|\cdot\|\mathbf{z}^{q}\|}\) (i.e., cosine distance), and \(\epsilon=0.1\). Euclidean distance leads to similar results, but cosine distance is a better choice for this case (Appendix E.4).

We start with the Name Mover Head 9.9, which Wang et al. [54] found moves the IO name to the residual stream of END. 4b shows the \(\epsilon\)-preimage at "to". The samples in the \(\epsilon\)-preimage share the name "Justin" as the IO. The head also shows similar activity at some other positions (Appendix A.3). Results are consistent across query inputs. Therefore, InversionView agrees with the conclusions of Wang et al. [54] on head 9.9. Applying the same analysis to other heads (Table 2), we recovered information in high agreement with the information that Wang et al. [54] had inferred using multiple tailored methods. For example, Wang et al. [54] found S-Inhibition heads were outputting both token signals (value of S) and position signals (position of S1) by patching these heads' outputs from a series of counterfactual datasets. These datasets are designed to disentangle the two effects, in which token and/or position information are ablated or inverted. These two kinds of information can be directly read out by InversionView (Figure 19 shows an example for an S-Inhibition head that contains position information), and there is no need to guess the possible information to design patching experiments. Overall, among the 26 attention heads that Wang et al. [54] identified, InversionView indicates a different interpretation in only 3 cases; these (0.1, 0.10, 5.9) were challenging for the methods used before (Appendix E.3). In summary, InversionView scales to larger models.

### 3-Digit Addition

We next applied InversionView to the problem of adding 3-digit numbers, between 100 and 999. Input strings have the form "B362+405=767E" or "B824+692=1516E", and are tokenized at the

Figure 4: **(a) Character Counting. Activation patching results show that \(a_{tc}^{0,0}\) and \(a_{:}^{1,0}\) play crucial roles in prediction, as hypothesized based on Figure 3 and Sec. 3.3. In contrast examples, only one character differs. Top: We patch activations cumulatively from left to right. We can see patching \(a_{tc}^{0,0}\) accounts for the whole effect, and when \(a_{tc}^{0,0}\) is already patched, patching \(a_{:}^{1,0}\) has almost no effect. Bottom: On the other hand, if we patch cumulatively from right to left, \(a_{:}^{1,0}\) accounts for the whole effect while patching \(a_{tc}^{0,0}\) has no effect if \(a_{:}^{1,0}\) has been patched. So we verified that \(a_{:}^{1,0}\) solely relies on \(a_{tc}^{0,0}\) and this path is the one by which the model performs precise counting. The patching effect is averaged across the whole test set. (b) IOI. InversionView applied to Name Mover Head 9.9 at “to”; we fix the compared position to “to”. Throughout the \(\epsilon\)-preimage, “Justin” appears as the IO, revealing that the head encodes this name. This interpretation is confirmed across query inputs.**

character level. We use F1, F2, F3 to denote the three digits of the first operand and S1, S2, S3 for the digits of the second operand, and A1, A2, A3, A4 (if it exists) for the three or four digits of the answer, and C2, C3 for the carry from tens place and ones place (i.e., C2: whether F2+S2\(\geq\)10, C3: whether F3+S3\(\geq\)10). Unlike [47], we do not left-pad answers to have all the same length; hence, positional information is insufficient to determine the place value of each digit.

The probed model is a decoder-only transformer (2 layers, 4 attention heads, dimension 32). We set attention dropout to 0. Other aspects are identical to GPT-2. The model is trained for autoregressive next-token prediction on the full input, in analogy to real-world language models. In testing, the model receives the tokens up to and including "=", and greedily generates up to "E". The prediction counts as correct if all generated tokens match the ground truth. The same train-test ratio as in Section 3.1 is used. The test accuracy is 98.01%. For other training details see Appendix F.1.

Interpreting via InversionView and attention.As Section 3.1 we use normalized Euclidean distance for \(D(\cdot,\cdot)\) and the threshold \(\epsilon=0.1\). We first trace how the model generates the first answer digit, A1, by understanding the activations at the preceding token, "=". We first examine the attention heads at "=" in the \(0\)-th layer (Figure 5). As for the first head (\(a^{0,0}\)), only F1 and S1 matter in the samples - indeed, changing other digits, or swapping their order, has a negligible effect on the activation (Figure 5). Across different inputs, each of the three heads \(a^{0,0}\), \(a^{0,1}\), \(a^{0,2}\) encode either one or both of F1 and S1 (Figure 26); taken together, they always encode both. This is in agreement with attention focusing on these tokens. The fourth and remaining head in layer 0 (\(a^{0,3}\)) encodes F2 and S2, which provide the carry from the tens place to the hundreds place. Combining the information from these four heads, \(x^{0,\mathrm{mid}}\) consistently encodes F1 and S1; and approximately represents F2, S2--only the carry to A1 (whether F2+S2\(\geq\)10) matters here (Figure 5c). Other examples are in

Figure 5: **InversionView applied to 3-digit addition**: Visually inspecting sample inputs inside and outside the \(\epsilon\)-preimage of the query allows us to understand what information is contained in an activation. The color on each token in generated samples denotes the difference in the token’s likelihood between a conditional or unconditional decoder (Appendix G). The shade thus denotes how much the generation of the token is caused by the query activation (darker shade means a stronger dependence). In (a–c), the colored tokens are most relevant to the interpretation. We interpret two attention heads (a,b) and the output of the corresponding residual stream after attention (c). In **(a)**, what’s common throughout the \(\epsilon\)-preimage is that the digits in the hundreds places are 6 and 8. Inputs outside the \(\epsilon\)-preimage don’t have this property. In **(b)**, what’s common is that the digits in tens places are 1, 6, or numerically close. Hence, we can infer that _the activation sites \(a^{0,0}\) and \(a^{0,3}\) encode hundreds and tens place in the input operands respectively;_ the latter is needed to provide carry to A1. Also, the samples show that the activations encode commutativity since the digits at hundreds and tens place are swapped between the two operands. In **(c)**, the output of the attention layer after residual connection combining information from the sites in (a) and (b) _encodes “6” and “8” in hundreds place, and the carry from tens place_. Note that \(a^{0,1}\) and \(a^{0,2}\) contains similar information as \(a^{0,0}\). These observations are confirmed across inputs. Taken together, InversionView reveals how information is aggregated and passed on by different model components.

Figure 27. We can summarize the function of layer 0 at "=": _Three heads route F1 and S1 to the residual stream of "=" \(x_{=}\). The fourth head routes the carry resulting from F2 and S2_. Layer 1 mainly forwards information already obtained in layer 0, and does not consistently add further information for A1. See more examples in Appendix F.2.

Figure 5(a) shows the circuit predicting A1. InversionView allows us to diagnose an important deficiency of this circuit: Even though the ones place sometimes receives attention in layer 1, the circuit does not consistently provide the carry from the ones place to the hundreds place, which matters on certain inputs--we find that this deficiency in the circuit accounts for _all_ mistakes made by the model (Appendix F.3). Taken together, we have provided a circuit allowing the model to predict A1 while also understanding its occasional failure in doing so correctly. Corresponding findings for A2, A3, and A4 are in Table 3 and Figure 31. From A2 onwards, InversionView allows us to uncover how the model exhibits two different algorithms depending on whether the resulting output will have 3 or 4 digits. In particular, when predicting A3, the layer 0 circuit is the same across both cases, while the layer 1 circuit varies, since this determines whether A3 will be a tens place or ones place. Beyond figures in the Appendix, we encourage readers to verify our claims in our interactive web application.

Quantitative verification.We used causal interventions to verify that information about the digits in hundreds and tens place is routed to the prediction of A1 only through the paths determined in Figure 5(a), and none else. Like before, we cumulatively patch the head output on "=" preceding the target token A1, with an activation produced at the same activation site by a contrast example changing both digits in a certain place. Results shown in Figure 5(b) strongly support our previous conclusions. For example, \(a^{0,3}\) and \(a^{1,2}\) are not relevant to F1 and S1. Important heads detected by activation patching, \(a^{0,0}\), \(a^{0,1}\), \(a^{0,2}\), \(a^{1,1}\), all contain F1 and S1 according to Figure 5(a). Furthermore, we can also confirm that \(a^{1,1}\) relies on the output of layer 0 as depicted in sub-figure (a): When heads in layer 0 are already patched, patching \(a^{1.1}\) has no further effect (value corresponding to \(\rightarrow\) is zero), but it has an effect when patching in the opposite direction. On the contrary, \(a^{1,0}\) shows little dependence on layer 0, consistent with Figure 5(a). On the right of Figure 5(b), we can confirm that \(a^{0,3}\) is important for routing F2 and S2, and the downstream heads in layer 1 rely on it. Findings for other answer digits are similar (See Appendix F.5). Overall, the full algorithm obtained by InversionView is well-supported by causal interventions.

Figure 6: **3-Digit Addition Task: (a)** Information flow diagram for predicting A1 inferred via InversionView. The colors denote which places are routed; alternating colors indicate two places are routed. This is a subfigure of Figure 31. (b) Validation of (a) via activation patching for the prediction of A1. Like Figure 5(a), \(\rightarrow\) (\(\leftarrow\)) means cumulatively patching activation from left to right (right to left) on the horizontal axis. **Left:** Patching with activation containing modified F1 and S1 information. **Right:** Patching with activation containing modified F2 and S2 information. As we can see, components from (a) show a substantial increment if and only if they have a not-yet-patched connection to output (when patching right to left) or input (patching left to right), verifying that (a) causally describes the flow of information. Therefore, InversionView helps us uncover both information flow and content of activations.

InversionView reveals granularity of information.Heads often read from both digits of a place, but only the sum matters for addition. Are the digits represented separately, or only as their sum? Unlike traditional probing, InversionView answers this question without designing tailored probing tasks. In Figure 7 (left), \(a^{0,2}\) exactly represents F2 and S2 (here, 2 and 5). Other inputs where F1+S1=5+2 have high \(D\). In contrast, on the right, F2 and S2 are represented only by their sum: throughout the \(\epsilon\)-preimage, F2+S2=9. In fact, we find such sum-only encoding only when F2+S2=9--a special case where the ones place of operands affects the hundreds place of the answer via cascading carry. We hypothesize that the model encodes them similarly because these inputs require special treatment. Therefore, even though encoding number pairs by their sum is a good strategy for the addition task from a human perspective, the model only does it as needed. We also observe intermediate cases (Figure 29).

### Factual Recall

To test whether InversionView can be applied to larger language models, we explore how GPT-2 XL (1.5B parameters) performs the task of recalling factual associations. In this case study, our intention is not to provide a full interpretation of the computations performed to solve this task, which we deem out of scope for this paper. Instead, we show that InversionView produces interpretable results on larger models by focusing on a relatively small set of important attention heads in upper layers. The decoder model in this case study is based on GPT-2 Medium, because we expect a more complex inverse mapping from activation to inputs to be learned. We observe the resulting \(\epsilon\)-preimage can express high-level knowledge (Figure 39-44), and sometimes can predict the failure of the model (Appendix H.6). Using InversionView, we again shed light on the underlying mechanism of the model. We present detailed findings in Appendix H.

## 4 Discussion and Related Work

Comparison with other Interpretation MethodsSupervised **probing classifiers**, assessing how much information about a variable of interest is encoded in an activation site, are arguably the most common method for uncovering information from activations [e.g. 2, 6, 5, 4, 55, 52, 33, 34]. It requires a hypothesis in advance and is thus inherently limited to hypotheses conceived a priori by the researcher. InversionView, on the other hand, helps researchers form hypotheses without any need for prior guesses, and allows fine-grained per-activation interpretation. Inspecting attention patterns [e.g. 12] is a traditional approach to inferring **information flow**, and we have drawn on it in our analyses. More recently, path patching [54, 26, 13, 28, 35] causally identifies _paths_ along which information flows. While the information flow provides an _upper bound_ on the information passed along by tracing back to the input token, it is insufficient for determining how information is processed and abstracted. For instance, in Section 3.1, occurrences of the target character are causally connected to \(a_{te}^{0,0}\), which then connects to \(a_{:}^{1,0}\) (direct or mediated by MLP layer 0). Without looking at encoded information, we only know that the information in these paths is related to the occurrences of the target character, but not whether it is their identity, positions, count, etc. More generally, when a component reads a component that itself has read from multiple components, connectivity does not tell us which pieces of information are passed on. Similar considerations apply to other intervention methods.

Figure 7: **3-Digit Addition Task**: InversionView uncovers different ways in which digit representation is encoded in activations. **Left:** The digits in the hundreds place are encoded separately and hence generations denote them as separate entities. **Right:** The digits in the tens place are encoded as a sum (9 in this case) and the generations represent different 2-partitions (7+2, 6+3, 1+8, 5+4, etc.) of that sum.

Geva et al. [24] intervene on attention weights to study information flow. Activation patching, a causal intervention method, can be used to study the causal effect of an activation on the output, and can help localize where information is stored [37; 49], or find alignment between a high-level causal model and inner states of a neural model [21; 22; 57]. Many recent works obtain insights about information content by projecting representations or parameters into the **vocabulary space**[42; 7; 44; 31; 54; 23; 24; 16]. This technique is sometimes referred to as Direct Logit Attribution (DLA). We argue that DLA is only suitable for studying model components that _directly_ affect model's final output. For those components whose effect is mediated by other components, their output information is meant to be read by a downstream component, thus not necessarily visible when projecting to the vocabulary space. We provide further discussion in Appendix I. Generalizing this approach, Ghandeharioun et al. [25] patch activations into an LLM. Another line of recent research [10; 51; 14] decomposes activations into interpretable features using **sparse autoencoders**.

Some other interpretation methods also generate in input space, but differ from InversionView in goals and methods. This includes feature visualization [43; 41], adversarial or counterfactual example generation [27; 59; 46; 45], and GAN inversion methods [58]. We discuss the similarities and differences of these works compared to InversionView in Appendix I.4.

InversionView offers distinctive advantages and makes analyses feasible that are otherwise very hard to do with other methods. It can also improve the interpretability workflow in coordination with other methods. For example, one may first use methods such as path patching or attribution [50; 18] to localize activity to specific components, and then understand the function of these components using InversionView. In sum, InversionView is worth adding to the toolbox of interpretability research.

Transformer Circuits for ArithmeticRelated to Section 3.3, [47] interpret the algorithm implemented by a \(1\)-layer \(3\)-head transformer for \(n\)-digit addition (\(n\in\{5,10,15\}\)), finding that the model implements the usual addition algorithm with restrictions on carry propagation. In their one-layer setup, attention patterns are sufficient for generating hypotheses. Lengths of operands and results are fixed by prepending \(0\). Our results, in contrast, elucidate a more complex algorithm computed by a _two_-layer transformer on a more realistic version without padding, which requires the model to determine which place it is predicting. We also contribute by providing a detailed interpretation, including how digits are represented in activations.

Automated Interpretation for InversionViewRecent work has started using LLMs to generate interpretations [8; 10]. The samples produced by InversionView can be easily fed into LLMs for automated interpretation. We show a proof of concept by using Claude 3 to interpret the model trained for 3-digit addition. See results in Table 5. The LLM-provided interpretation reflects the main information in almost all cases of the addition task. Despite some flaws, the outcome is informative in general, suggesting this as a promising direction for further speeding up hypothesis generation.

LimitationsInversionView relies on a black-box decoder, which needs to be trained using relevant inputs and whose completeness needs to be validated by counter-examples. Also, InversionView, while easing the human's task, is still not automated, and interpretation can be laborious when there are many activation sites. We focus on models up to 1.5B parameters; scaling the technique to large models is an interesting problem for future work, which will likely require advances in localizing behavior to a tractable number of components of interest. Fourth, interpretation uses a metric \(D(\cdot,\cdot)\). The geometry, however, in general could be nonisotropic and treating each dimension equally could be sub-optimal. We leave the exploration of this to future work.

## 5 Conclusion

We present InversionView, an effective method for decoding information from neural activations. In four case studies--character counting, IOI, 3-digit addition, and factual recall--we showcase how it can reveal various types of information, thus facilitating reverse-engineering of algorithm implemented by neural networks. Moreover, we compare it with other interpretability methods and show its unique advantages. We also show that the results given by InversionView can in principle be interpreted automatically by LLMs, which opens up possibilities for a more automated workflow. This paper only explores a fraction of the opportunities this method offers. Future work could apply it to subspaces of residual stream, to larger models, or to different modalities such as vision.

## Acknowledgements

Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 232722074 - SFB 1102. We thank anonymous reviewers for their encouraging and constructive feedback.

## References

* [1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] G. Alain and Y. Bengio. Understanding intermediate layers using linear classifier probes. _arXiv preprint arXiv:1610.01644_, 2016.
* [3] Anthropic. Introducing the next generation of claude, 2024. https://www.anthropic.com/news/claude-3-family.
* [4] Y. Belinkov. Probing classifiers: Promises, shortcomings, and advances. _Comput. Linguistics_, 48(1):207-219, 2022. doi: 10.1162/COLI_A_00422. URL https://doi.org/10.1162/coli_a_00422.
* [5] Y. Belinkov and J. Glass. Analysis methods in neural language processing: A survey. _Transactions of the Association for Computational Linguistics_, 7:49-72, 2019.
* [6] Y. Belinkov, N. Durrani, F. Dalvi, H. Sajjad, and J. Glass. What do neural machine translation models learn about morphology? In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 861-872, 2017.
* [7] N. Belrose, Z. Furman, L. Smith, D. Halawi, I. Ostrovsky, L. McKinney, S. Biderman, and J. Steinhardt. Eliciting latent predictions from transformers with the tuned lens, 2023.
* [8] S. Bills, N. Cammarata, D. Mossing, H. Tillman, L. Gao, G. Goh, I. Sutskever, J. Leike, J. Wu, and W. Saunders. Language models can explain neurons in language models. _URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023)_, 2023.
* [9] S. Bird and E. Loper. NLTK: The natural language toolkit. In _Proceedings of the ACL Interactive Poster and Demonstration Sessions_, pages 214-217, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/P04-3031.
* [10] T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner, C. Anil, C. Denison, A. Askell, et al. Towards monosemanticity: Decomposing language models with dictionary learning. _Transformer Circuits Thread_, page 2, 2023.
* [11] B. Chughtai, A. Cooney, and N. Nanda. Summing up the facts: Additive mechanisms behind factual recall in lms. _arXiv preprint arXiv:2402.07321_, 2024.
* [12] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does bert look at? an analysis of bert's attention. In _Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pages 276-286, 2019.
* [13] A. Conmy, A. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. _Advances in Neural Information Processing Systems_, 36:16318-16352, 2023.
* [14] H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse autoencoders find highly interpretable features in language models. _arXiv preprint arXiv:2309.08600_, 2023.
* [15] J. Dao, Y.-T. Lao, C. Rager, and J. Janiak. An adversarial example for direct logit attribution: Memory management in gelu-4l. _arXiv preprint arXiv:2310.07325_, 2023.
* [16] G. Dar, M. Geva, A. Gupta, and J. Berant. Analyzing transformers in embedding space. In _The 61st Annual Meeting Of The Association For Computational Linguistics_, 2023.
* [17] N. Elhage, N. Nanda, C. O. T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical framework for transformer circuits, 2021. https://transformer-circuits.pub/2021/framework/index.html.

* Ferrando and Voita [2024] J. Ferrando and E. Voita. Information flow routes: Automatically interpreting language models at scale, 2024.
* Ferrando et al. [2023] J. Ferrando, G. I. Gallego, I. Tsiamas, and M. R. Costa-jussa. Explaining how transformers use context to build predictions. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5486-5513, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.301. URL https://aclanthology.org/2023.acl-long.301.
* Ferrando et al. [2024] J. Ferrando, G. Sarti, A. Bisazza, and M. R. Costa-jussa. A primer on the inner workings of transformer-based language models. _arXiv preprint arXiv:2405.00208_, 2024.
* Geiger et al. [2021] A. Geiger, H. Lu, T. Icard, and C. Potts. Causal abstractions of neural networks. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 9574-9586, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html.
* Geiger et al. [2023] A. Geiger, C. Potts, and T. Icard. Causal abstraction for faithful model interpretation. _arXiv preprint arXiv:2301.04709_, 2023.
* Geva et al. [2022] M. Geva, A. Caciularu, K. Wang, and Y. Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 30-45, 2022.
* Geva et al. [2023] M. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting recall of factual associations in auto-regressive language models. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 12216-12235, 2023.
* Ghandeharioun et al. [2024] A. Ghandeharioun, A. Caciularu, A. Pearce, L. Dixon, and M. Geva. Patchscope: A unifying framework for inspecting hidden representations of language models. _arXiv preprint arXiv:2401.06102_, 2024.
* Goldowsky-Dill et al. [2023] N. Goldowsky-Dill, C. MacLeod, L. Sato, and A. Arora. Localizing model behavior with path patching. _arXiv preprint arXiv:2304.05969_, 2023.
* Goodfellow et al. [2014] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. _arXiv preprint arXiv:1412.6572_, 2014.
* Hanna et al. [2024] M. Hanna, O. Liu, and A. Variengien. How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. _Advances in Neural Information Processing Systems_, 36, 2024.
* Hanna et al. [2024] M. Hanna, S. Pezzelle, and Y. Belinkov. Have faith in faithfulness: Going beyond circuit overlap when finding model mechanisms. _arXiv preprint arXiv:2403.17806_, 2024.
* Kaddour [2023] J. Kaddour. The minipile challenge for data-efficient language models. _arXiv preprint arXiv:2304.08442_, 2023.
* Katz and Belinkov [2023] S. Katz and Y. Belinkov. Visit: Visualizing and interpreting the semantic information flow of transformers. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 14094-14113, 2023.
* Kobayashi et al. [2020] G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui. Attention is not only a weight: Analyzing transformers with vector norms. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 7057-7075, 2020.
* Li et al. [2021] B. Z. Li, M. Nye, and J. Andreas. Implicit representations of meaning in neural language models. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 1813-1827, 2021.
* Li et al. [2022] K. Li, A. K. Hopkins, D. Bau, F. Viegas, H. Pfister, and M. Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. In _The Eleventh International Conference on Learning Representations_, 2022.
* Lieberum et al. [2023] T. Lieberum, M. Rahtz, J. Kramar, G. Irving, R. Shah, and V. Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. _arXiv preprint arXiv:2307.09458_, 2023.

* [36] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2018.
* [37] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt. _Advances in Neural Information Processing Systems_, 35:17359-17372, 2022.
* [38] P. Michel, O. Levy, and G. Neubig. Are sixteen heads really better than one? _Advances in neural information processing systems_, 32, 2019.
* [39] N. Nanda. Attribution patching: Activation patching at industrial scale, 2023. URL https://www.neelnanda.io/mechanistic-interpretability/attribution-patching.
* [40] N. Nanda and J. Bloom. Transformerlens. https://github.com/TransformerLensOrg/TransformerLens, 2022.
* [41] A. Nguyen, J. Yosinski, and J. Clune. Understanding neural networks via feature visualization: A survey. _Explainable AI: interpreting, explaining and visualizing deep learning_, pages 55-76, 2019.
* [42] nostalgebraist. interpreting gpt: the logit lens. _LESSWRONG_, 2020. https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.
* [43] C. Olah, A. Mordvintsev, and L. Schubert. Feature visualization. _Distill_, 2(11):e7, 2017.
* [44] K. Pal, J. Sun, A. Yuan, B. C. Wallace, and D. Bau. Future lens: Anticipating subsequent tokens from a single hidden state. In _Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)_, pages 548-560, 2023.
* [45] M. Pawelczyk, C. Agarwal, S. Joshi, S. Upadhyay, and H. Lakkaraju. Exploring counterfactual explanations through the lens of adversarial examples: A theoretical and empirical analysis. In _International Conference on Artificial Intelligence and Statistics_, pages 4574-4594. PMLR, 2022.
* [46] X. Qi, K. Huang, A. Panda, P. Henderson, M. Wang, and P. Mittal. Visual adversarial examples jailbreak aligned large language models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 21527-21536, 2024.
* [47] P. Quirke and F. Barez. Understanding addition in transformers. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=r1x1YXWZb.
* [48] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [49] A. Stolfo, Y. Belinkov, and M. Sachan. A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. In H. Bouamor, J. Pino, and K. Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 7035-7052, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.435. URL https://aclanthology.org/2023.emnlp-main.435.
* [50] A. Syed, C. Rager, and A. Conmy. Attribution patching outperforms automated circuit discovery. _arXiv preprint arXiv:2310.10348_, 2023.
* [51] A. Tamkin, M. Taufeeque, and N. D. Goodman. Codebook features: Sparse and discrete interpretability for neural networks. _arXiv preprint arXiv:2310.17230_, 2023.
* [52] I. Tenney, D. Das, and E. Pavlick. Bert rediscovers the classical nlp pipeline. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4593-4601, 2019.
* [53] J. Vig, S. Gehrmann, Y. Belinkov, S. Qian, D. Nevo, Y. Singer, and S. Shieber. Investigating gender bias in language models using causal mediation analysis. _Advances in neural information processing systems_, 33:12388-12401, 2020.
* [54] K. R. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. 2023. URL https://openreview.net/forum?id=NpsVSN6o4ul.
* [55] Z. Wang, A. Ku, J. M. Baldridge, T. L. Griffiths, and B. Kim. Gaussian process probes (gpp) for uncertainty-aware probing. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.

* [56] J. Wiland, M. Ploner, and A. Akbik. BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models, 2024. URL http://arxiv.org/abs/2404.04113.
* [57] Z. Wu, A. Geiger, T. Icard, C. Potts, and N. Goodman. Interpretability at scale: Identifying causal mechanisms in alpaca. _Advances in Neural Information Processing Systems_, 36, 2024.
* [58] W. Xia, Y. Zhang, Y. Yang, J.-H. Xue, B. Zhou, and M.-H. Yang. Gan inversion: A survey. _IEEE transactions on pattern analysis and machine intelligence_, 45(3):3121-3138, 2022.
* [59] C. Xiao, B. Li, J.-Y. Zhu, W. He, M. Liu, and D. Song. Generating adversarial examples with adversarial networks. _arXiv preprint arXiv:1801.02610_, 2018.
* [60] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. _Advances in neural information processing systems_, 32, 2019.
* [61] Y. Yao, N. Zhang, Z. Xi, M. Wang, Z. Xu, S. Deng, and H. Chen. Knowledge circuits in pretrained transformers. _arXiv preprint arXiv:2405.17969_, 2024.
* [62] Q. Yu, J. Merullo, and E. Pavlick. Characterizing mechanisms for factual recall in language models. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 9924-9959, 2023.

###### Contents

* 1 Introduction
* 2 Methodology
* 3 Discovering the Underlying Algorithm by InversionView
	* 3.1 Character Counting
	* 3.2 IOI circuit in GPT-2 small
	* 3.3 3-Digit Addition
	* 3.4 Factual Recall
* 4 Discussion and Related Work
* 5 Conclusion
* A Practical Guidelines
* A.1 Observing Larger Neighborhoods is Important
* A.2 Sampling with Decoder Model
* A.3 Selecting Position in Samples
* A.4 Threshold-Dependence of Claims about Activations
* B Experimental Verification of Completeness
* C Decoder Model
* D Character Counting: More Details and Examples
* D.1 Implementation Details
* D.2 More Examples of InversionView
* D.3 Causal Intervention Details
* D.4 Extended Algorithm with Positional Cues
* E IOI Task: Details and Qualitative Results
* E.1 Implementation Details
* E.2 More Examples of InversionView
* E.3 Qualitative Examination Results
* E.4 Choice of Distance Metric in IOI
* F 3-Digit Addition: More Details and Examples
* F.1 Implementation Details
* F.2 More Examples of InversionView
* F.3 Model Deficiency
* F.4 Qualitative Examination Results
* F.5 Causal Intervention: Details and Full Results

* **Decoder Likelihood Difference**
* **Factual Recall: Detailed Findings*
	* 1.1 Background
	* 2.2 Selecting Activation Sites
	* 2.3 Decoder Training
	* 2.4 Sampling from Decoder
	* 2.5 Observation
	* 2.6 More Examples of InversionView
* **Notes on Attention, Path Patching, DLA and others*
	* 1.1 InversionView Reveals Information from Un-attended Tokens
	* 1.2 Additional Discussion about Path Patching
	* 1.3 Additional Discussion about DLA
	* 1.4 Methods Generating in Input Space
* **Automated Interpretability**
* **Compute Resources**

## Appendix A Practical Guidelines

### Observing Larger Neighborhoods is Important

Here, we illustrate the importance of inspecting \(\epsilon\)-preimages up to the threshold \(\epsilon\), rather than just top-\(k\) nearest neighbors of the query activation. In Figure 8, an initial glance at the samples on the left may suggest that the residual stream of "+" encodes F1 and F2. However, observing a broader neighborhood (as depicted on the right) reveals that this conclusion is not even robust to tiny perturbations of the activation. Indeed, after a more comprehensive calculation over all possible \(x_{+}^{0,\mathrm{post}}\), we find that the maximum possible metric value between any pair of \(x_{+}^{0,\mathrm{post}}\) is 0.0184. So for any \(\epsilon\geq 0.0184\) the \(\epsilon\)-preimage covers the entire input space. Hence, the activation is unlikely to contain usable information.

We further prove this by causal intervention. We found that \(x_{+}^{0,\mathrm{post}}\) has no effect on the model's output. Concretely we patch \(x_{+}^{0,\mathrm{post}}\) with its mean on the test set (mean ablation [54]) and for each prediction target (A1, A2 etc.), we compare 1) the KL divergence between the distribution before and after patching. 2) logit decrement rate, which is the difference between the maximum logit value before patching and the logit value of the same target token after patching, divided by the former. E.g., 1.0 means the logit is reduced to zero (assuming it is originally positive). The results are shown in Table 1. We can see the effect of \(x_{+}^{0,\mathrm{post}}\) is negligible.

### Sampling with Decoder Model

In Section 2, we mentioned that the distribution \(p(\mathbf{x}|\mathbf{z}^{q})\) is modeled by the decoder. Strictly speaking, \(p(\mathbf{x}|\mathbf{z}^{q})\) represents the data distribution in the \(\epsilon\)-preimage defined by \(\epsilon=0\). For example, when the

\begin{table}
\begin{tabular}{c||c|c|c|c} \hline  & A1 & A2 & A3 & A4/E \\ \hline KL divergence & \(8.1\times 10^{-8}\) & \(5.4\times 10^{-7}\) & \(2.5\times 10^{-8}\) & \(3.6\times 10^{-9}\) \\ Logit decrement rate & \(-2.3\times 10^{-5}\) & \(8.7\times 10^{-7}\) & \(1.4\times 10^{-5}\) & \(2.8\times 10^{-7}\) \\ \hline \end{tabular}
\end{table}
Table 1: Activation patching results for \(x_{+}^{0,\mathrm{post}}\).

probed model is using causal masking, and a certain activation is relevant to all previous context (by non-zero attention weights), then \(p(\mathbf{x}|\mathbf{z}^{q})\) is the distribution over those inputs that share the same previous context (i.e., they have same prefix). This requires that the decoder can distinguish any tiny difference in activation and decode the full information (imagine a token attended with 0.0001 attention weight). Such a decoder must be very powerful and perhaps trained without any regularization. But in practice, the decoder is a continuous function of activation and tiny changes in activation are not perceivable by the decoder. We observe that the decoder rarely generates the sample that lies at the same point (producing the same activation) as the query input in vector space, instead it usually generates samples that are in the neighborhood of the query input. Because we need to observe the whole neighborhood of the query input and prevent samples from being too concentrated, we adjust the sampling temperature to control how concentrated they are. Importantly, even if the decoder is too powerful and can always recover the same activation, we can still obtain the neighborhood by adding random noise to the query activation before giving it to the decoder. This motivates decoding with temperature and noise, as described in the next paragraph.

Increasing Coverage by Temperature and Noise.In our experiments, we use both ways to control the generation, i.e., by adjusting the temperature and adding random noise to the query activation. We denote temperature as \(\tau\) and noise coefficient as \(\eta\). The noise vector consists of independent random variables sampled from the standard normal distribution and then multiplied by \(\text{std}(\mathbf{z}^{q})\cdot\eta\) where \(\text{std}(\cdot)\) stands for standard deviation. In our web application, we provide multiple sampling configurations: four configurations in which \(\tau=\{0.5,1.0,2.0,4.0\}\) and \(\eta=0.0\) (only for addition task); one figuration named "Auto" which is sampled by following procedure: we iterate over a few predefined \(\tau\) (ranging from 0.5 to 2.0) and \(\eta\) (0.0 or 0.1) and sample a certain amount of inputs (e.g., 250) for each parameter combination. We then calculate the metric value for all inputs collected from different sampling configurations. We then randomly choose a small part of them (100) with different probability for in-\(\epsilon\)-preimage inputs and out-of-\(\epsilon\)-preimage inputs. We dynamically adjust the probability such that the in-\(\epsilon\)-preimage inputs account for 60%-80% of the chosen set of inputs (when this is possible). Note that we use different noise in factual recall task, which will be described later in Appendix H.4.

When inspecting the samples, we choose a configuration for which the distances \(D(\cdot,\cdot)\) to the query activation best cover the interval \([0,\epsilon]\). The choice is usually specific to the activation site that we are inspecting and can be performed manually in the web application.

Figure 8: Addition Task: Inspecting \(\epsilon\)-preimage avoids pitfall of inspecting simple top-k similar activations. Generation based on query activation \(x_{+}^{0,\text{post}}\) of a random example. Contents after “+” is omitted since they do not affect the activation due to causal masking.

### Selecting Position in Samples

As the decoder outputs an input but not the position of the activation, we then assign the position minimizing \(D(\cdot,\cdot)\) to the query activation. Usually, there is only one position with a small \(D(\cdot,\cdot)\), matching the structural position (not necessarily the absolute position) of the position the query activation was taken from (e.g., the target character in Figure 2(a)). In certain cases, we visualize \(D(\cdot,\cdot)\) for an activation from a position not minimizing \(D(\cdot,\cdot)\) for expository purposes. For example, in Figure 2(b), because the target character exclusively attends to itself in layer 1, resulting \(a_{tc}^{1,0}\approx a_{1}^{1,0}\), so sometimes the metric value of \(a_{tc}^{1,0}\) is smaller than \(a_{1}^{1,0}\). Throughout the appendix and our web application, we use italic font and rounded bars to visualize \(D(\cdot,\cdot)\) in such cases.

We also find that selecting the position minimizing \(D(\cdot,\cdot)\) can reveal that components are active in similar ways at other positions than the one originally investigated. For example, in Figure 9, we can see sometimes "gave" is selected. This is reasonable, because the IO is also likely to appear right after "gave" and the head needs to move the IO name for this prediction. We can see that activation at the period "\(\cdot\)" can also be somewhat similar to the query activation, this is not surprising. Because the model needs to predict the subject for the next sentence and copying a name from the previous context is helpful. In summary, the copying mechanism can be triggered in circumstances different from IOI, selecting position minimizing \(D(\cdot,\cdot)\) reveals more information about this.

### Threshold-Dependence of Claims about Activations

One question people may have is whether our conclusion about the information in activation depends significantly on the threshold we choose. To address this potential concern, we show more details about the geometry of the vector space in Figure 10. On the one hand, we can see that with different thresholds \(\epsilon\) we can make different conclusions about the query activation. On the other hand, the conclusions made with different thresholds are "in alignment". In other words, the conclusions do not differ fundamentally, instead, the difference between them is about granularity or the amount of details being ignored.

Specifically, in Figure 9(a), \(\epsilon_{1}\) results in the conclusion that the count is 5, the target character is either 't' or'm', and also approximate sequence length is retained. \(\epsilon_{2}\) results in a conclusion only about the count and the sequence length. In Figure 9(b), if we set the threshold to \(\epsilon_{1}\) (i.e., a value between 0.000 and 0.009), the obtained information will be F1=5, S1=7. If we set the threshold to \(\epsilon_{2}\), the information will be 5 and 7 are in the hundreds place. If we set the threshold to \(\epsilon_{3}\) the information will be "5 is in the hundreds place". In Figure 9(c), \(\epsilon_{1}\) results in conclusion that 9, 8 are in hundreds place and 2, 7 are in tens place; \(\epsilon_{2}\) results in conclusion that 9, 8 are in hundreds place and F2+S2=9; \(\epsilon_{3}\) results in conclusion that F1+S1\(\approx\)17 and F2+S2\(\approx\)9. Therefore, changing the threshold value will not lead us in a different direction, because the \(\epsilon\)-preimage is based on the same underlying geometry.

Figure 9: IOI: InversionView applied to Name Mover Head 9.9 at “to”; Unlike Figure 3(b), here the position minimizing \(D(\cdot,\cdot)\) is in parentheses. The head also copies the name “Justin” in other circumstances, e.g., at “gave”. The name “Justin” is always containedFigure 10: **(a)** Activation site \(a^{1,0}\). **(b)** Activation site \(a^{0,2}\). **(c)** Activation site \(a^{1,3}\). In all three cases, we use normalized Euclidean distance as the distance metric. We use \(\epsilon_{1},\epsilon_{2},\cdots\) to mark varying threshold values by which different interpretations will be made.

In practice, rather than selecting a threshold first and treating inputs in a black-and-white manner, we first observe the geometry of the vector space and obtain a broad understanding of the encoded information, then choose a reasonable threshold that best _summarizes_ our findings. In other words, the threshold value is used to simplify our findings so that we can focus more on the big picture of the model's overall algorithm, and it should also be set according to the difference that is likely to be readable for the model. As the interpretation progresses, one can see if the chosen threshold leads to a plausible algorithm and can adjust it if necessary. Finally, verification experiments are conducted to verify the hypothesis.

## Appendix B Experimental Verification of Completeness

In Section 2, we described that an ideal strategy for obtaining samples in the \(\epsilon\)-preimage satisfies two desiderata: it only provides samples that are indeed within the \(\epsilon\)-preimage (Correctness), and it provides all such samples at reasonable probability (Completeness). As further described there, we can directly ensure Correctness by evaluating \(D(\cdot)\) for every sample. Ensuring completeness is more challenging, due to the exponential size of the input space; the most general approach is to design counterexamples not satisfying a hypothesis about the content of the \(\epsilon\)-preimage, and verifying that \(D(\cdot)\) is indeed large.

Here, we provide a direct test of completeness in one domain (3-digit addition). The primary concern with completeness is that, if some groups of inputs in \(B_{\mathbf{z}^{q},f,\epsilon}\) are systematically missing from the generated samples, one may overestimate the information contained in activations. To see if may happen in reality, we plotted log-probability against distance, each of which includes _all_ inputs in 3 digit addition task, as shown in Figure 11. We next evaluated the sampling probability for different sampling configurations, as described in Appendix A.2. When adding noise, we calculate probability of an input using a Monte Carlo estimate: Concretely, because the probability of inputs is conditioned on the noise vector added to the query activation, we randomly sample 500 noise vectors from the normal distribution (with the standard deviation described in Appendix A.2) and calculate input probability given these noise vectors, then average to obtain the estimated probability, and then compute the logarithm.

Across setups, we can see that there is a triangular blank area in the bottom left corner, i.e., the bottom left frontier stretches from the upper left towards the lower right. In all sub-figures, not a single input close to the query input is assigned disproportionately low probability. All inputs in the \(\epsilon\)-preimage (the dots on the left of the red vertical line) are reasonably likely to be sampled from the decoder, with probability decreasing as the input becomes more distant, alleviating concerns about completeness for these query activations. On the other hand, some inputs distant from the query input are also likely to appear in samples, but this is not a problem for our approach, as we can easily tell that they are not in the \(\epsilon\)-preimage by calculating the distance (correctness is ensured).

We can see that sometimes the distribution of the decoder itself (at temperature 1 and no noise) is quite sharp, and in-\(\epsilon\)-preimage inputs can have low probability as they are near the boundary of preimage. By comparing the sub-figures, we can see both increasing the temperature and adding noise substantially smooth the distribution within the \(\epsilon\)-preimage, lowering the difference of the probability of inputs that are at similar distance.

## Appendix C Decoder Model

The overall training and sampling pipelines are shown in Figure 2. In this section, we describe the architecture of the decoder model in detail.

The decoder model is basically a decoder-only transformer combined with some additional MLP layers. In order to condition the decoder on the query activation, the query activation is first passed through a stack of MLP layers to decode information depending on the activation sites and then made available to each attention layer of the transformer part of the decoder, as depicted in Figure 12.

Processing Query Activation.The query activation \(\mathbf{z}^{q}\in\mathbb{R}^{d}\) is first concatenated with a trainable activation site embedding \(\mathbf{e}_{act}\in\mathbb{R}^{d_{site}}\), producing the intermediate representation \(\mathbf{z}^{(0)}=[\mathbf{z}^{q};\mathbf{e}_{act}]\). We chose \(d_{site}\) to be the number of possible activation sites in the training set. The result \(\mathbf{z}^{(0)}\) is Figure 11: Addition Task: Exhaustive verification of the decoder’s completeness for 8 random query activation. Failure of completeness would mean that some inputs result in an activation very close to the query activation but nonetheless are assigned very small probability. Here, we show that this does not happen, by verifying that _all_ inputs within the \(\epsilon\)-preimage are assigned higher probability by the decoder than most other inputs. We also show that by increasing the temperature and adding random noise, we can increase the probability of inputs near the boundary of \(\epsilon\)-preimage. Each sub-figure – (a), (b), (c) – contains 8 scatter plots, each of which contains 810000 dots representing all input sequences in the 3-digit addition task. The y-axis of scatter plots is the log-probability of the input sequence given by the decoder (which reads the query activation), the x-axis is the distance between the query input and the input sequence. As before, distance is measured by the normalized Euclidean distance between the query activation (the activation site, query input, and selected position are shown in the scatter plot title) and the most similar activation along the sequence axis. In addition, the red vertical line represents the threshold \(\epsilon\), which is 0.1 in the case study. **(a)** Temperature \(\tau=1.0\), no noise is added. **(b)** Temperature \(\tau=2.0\), no noise is added. **(c)** Temperature \(\tau=1.0\), noise coefficient \(\eta=0.1\) (See Appendix A.2 for explanation of \(\eta\)).

then fed through multiple MLP layers (each layer indexed by \(p\in\{0,1,\cdots,P-1\}\)) with residual connections:

\[\mathbf{z}^{(p+1)}=\text{MLP}(\text{LN}(\mathbf{z}^{(p)}))+\mathbf{z}^{(p)}\] (2)

where LN represents layer normalization. For each MLP layer, the input, hidden and output dimensions are \(d+d_{site}\), \(d\), and \(d+d_{site}\), respectively. The activation function is ReLU. There is also a final layer normalization, \(\mathbf{z}^{(fn)}=\text{LN}(\mathbf{z}^{(P)})\).

Integrating Query Activation.As we want to make the query activation available to each attention layer of the decoder, we separately customize it to the needs of each layer using a linear layer. That is, for each layer of the transformer part of the decoder (indexed by \(\ell\), so \(\ell\in\{0,1,\cdots,L-1\}\)), we define a linear layer \(\text{Linear}^{(\ell)}:\mathbb{R}^{d+d_{site}}\rightarrow\mathbb{R}^{d_{decoder}}\) and a layer normalization \(\text{LN}^{(\ell)}\), where \(d_{decoder}\) is the model dimension of the decoder model:

\[\hat{\mathbf{z}}^{(\ell)}=\text{LN}^{(\ell)}(\text{Linear}^{(\ell)}(\mathbf{ z}^{(fn)}))\] (3)

We add superscript \((\ell)\) to the model components to emphasize they are layer-specific. In the \(\ell\)-th layer of the transformer part, \(\hat{\mathbf{z}}^{(\ell)}\) is concatenated with the input of the attention layer along the length axis before computing keys and values, so that each attention head can also attend to \(\hat{\mathbf{z}}^{(\ell)}\) in addition to the context tokens. This means that each head in the \(\ell\)-th layer, instead of attending to \(x_{1}^{(\ell)},x_{2}^{(\ell)},...\), now computes its attention weights over \(\hat{\mathbf{z}}^{(\ell)},x_{1}^{(\ell)},x_{2}^{(\ell)},...\). Here \(x_{t}^{(\ell)}\) is the residual stream corresponding to the \(t\)-th token input into the \(\ell\)-th layer.

Motivation for Architecture Design.At the beginning, we train separate decoders for each activation site, but this is not very scalable when there are many activation sites. In the architecture above mentioned, we use activation site embedding \(\mathbf{e}_{act}\) as a signal to trigger different processing, and functions or model components that are needed for all activation sites are shared. Similar reason applies to the linear layer \(\text{Linear}^{(\ell)}\), we expect the \(\mathbf{z}^{q}\) should be transformed differently for each transformer layer, but having separate MLP stacks to process \(\mathbf{z}^{q}\) for each layer would largely increase the number of parameters. In preliminary experiments, we also try "encoder-decoder" attention layer. That is, instead of providing query activation in self attention layer, we add new attention layers that analogous to the encoder-decoder attention layer in original transformer architecture, where each token can attend to the processed query activation as well as a blank representation (similar to the function of "BOS", so that "no-op" is possible). However, we do not find significant difference between this design and the aforementioned one. Therefore, other than adding components for processing \(\mathbf{z}^{q}\), we do not modify the decoder-only transformer architecture, so that we can also choose to use pretrained models. We note that there are other possible choices for conditioning the generation on the activation, and we didn't optimize this choice thoroughly.

Figure 12: The decoder model architecture used in this paper. The query activation is processed by a stack of MLP layers before being available as part of the context in attention layers. We use transparent blocks to represent model components inherited from original decoder-only transformer model.

Decoder Hyperparameters.Regarding processing query activation, the decoder has 6 MLP layers, i.e., \(P=6\). The decoder model has 2 transformer layers (\(L=2\)), 4 heads per layer, and a model dimension of 256. The attention dropout rate is 0. Other settings are the same as the GPT-2. We use the this architecture for the 3 tasks--character counting, IOI, and 3-digit addition-- in the paper. Regarding the factual recall task, we use the same architecture for processing query activation, i.e. \(P=6\), and use pretrained GPT-2 Medium (24 layers) as the transformer part of the decoder.

Training Details.We construct the training dataset by feeding in-domain inputs to the probed model, and collect activations from random activation sites and random position as query activations (the choice of activation sites and position is specific to each task and is described later), we also record the activation site they come from. For each input, we could obtain many possible training examples because of many choices of activation sites and position. So we do not iterate over all possible training examples. We sample certain amount of examples to train the decoder for 1 epoch, using constant learning rate of 0.0001 and AdamW optimizer with weight decay of 0.01. Other details (e.g., amount of examples, training steps) are task-specific, and can be found later in their own section.

During training, we regularly calculate the in-preimage rate, which serves as a proxy for generation quality. Concretely, for a fixed set of query activations used for testing, the decoder generates samples with temperature=1, we then compute the fraction of samples inside of \(\epsilon\)-preimage (with \(\epsilon=0.1\), \(D\) as normalized Euclidean distance). The rate is calculated for each activation site. We usually observe difference between the ratios for each activation site, indicating some inverse mappings are easier to learn (we also observe these activation sites tend to have clearer information). Overall, we usually see a continuous improvement on the average rate during training.

## Appendix D Character Counting: More Details and Examples

### Implementation Details

To construct the dataset, we enumerate all the 3-combinations from the set of lowercase characters defined in ASCII. For each combination, we generate 600 distinct data points by varying the occurrence of each character and the order of the string. The occurrences are sampled uniformly from 1-9 (both inclusive). So the length of the part before the pipe symbol ("l") lies in [3, 27] (not considering "B"). Like 3-digit addition, we split the dataset into train and test sets, which account for 75% and 25% of all data respectively. The input is tokenized on the character level.

The model is a two-layer transformer with one head in each layer, the model dimension is 64. All dropout rates are set to 0. The model is trained with cross-entropy loss on the last token, the answer of the counting task. The model is trained with a batch size of 128 for 100 epochs, using a constant learning rate of 0.0005, weight decay of 0.01, and AdamW [36] optimizer. The training loss is shown in Figure 13, we can see the stair-like pattern. An interesting future direction is to investigate what happens when the loss rapidly decreases using InversionView.

With regard to the decoder model, the architecture is described in C. We select \(x^{0,\mathrm{pre}},x^{i,\mathrm{mid}},x^{i,\mathrm{post}},a^{i,j},m^{i}\) as the set of activation sites we are interested in, where \(i\in\{0,1\},j\in\{0\}\), and \(m\) denotes MLP layer output. The query activation is sampled from those activations corresponding to only the target character and colon. We sample 100 million training examples (all activation sites are included) and train the decoder with batch size of 512, resulting in roughly 200K steps. During training, as we mentioned before in C, we test the generation quality by measuring in-preimage rate. For those activation sites for which the decoder has a low generation quality, we increase their probability of being sampled in the training data. The final in-preimage rate averaged across activation sites is 67.7%.

### More Examples of InversionView

See Figures 14 and 15. Here, we show results similar to Figure 3 for other query inputs.

Figure 14: \(\epsilon\)-preimage showing function of MLP layer \(0\)

Figure 13: Training loss of the Character Counting task. Each data point is the averaged loss over an epoch.

Figure 15: \(\epsilon\)-preimage of \(a_{i}^{1,0}\). As we mentioned, we hypothesize that the attention head is reading the subspace where the count information is stored. One can presumably find this “count subspace” by optimizing a projection matrix such that after projecting the activation there is only pure count information in the \(\epsilon\)-preimage, and compare it with the subspace read by the value matrix of the attention head. Therefore, InversionView can be potentially useful for subspace study.

### Causal Intervention Details

For an input example \(\mathsf{x}_{orig}\) with \(t_{orig}\) as the count (final token), we construct a contrast example \(\mathsf{x}_{con}\) with a different count \(t_{con}\) by changing a random character before "l". The contrast example is a valid input (the count token is the count of the target character). We also ensure that the contrast example is within the dataset distribution (the count is in the range [1-9] and there are 3 distinct characters in the input).

We run three forward passes. 1) The model takes as input \(\mathsf{x}_{orig}\) and produces logit values for count prediction, we record the logit difference \(LD_{orig}\) between \(t_{orig}\) and \(t_{con}\) (former minus latter). 2) We feed the model with \(\mathsf{x}_{con}\) and store all activations. 3) We run a forward pass using \(\mathsf{x}_{orig}\), replacing the interested activations (e.g, \(\{a_{:}^{0,0},a_{tc}^{0,0}\}\)) with the stored activation in the same position and activation sites, and record the new logit difference \(LD_{pch}\). Because the model can make the right prediction in most cases, we can see that average \(LD_{pch}\) changes from positive to negative values as we patch more and more activations. We do the same for all inputs in the test set and report the average results.

Figure 16 shows the \(LD_{orig}\) and \(LD_{pch}\). We cumulatively patch the activations we study. For example, on the top of the figure, we patch \(\{a_{:}^{0,0}\}\), \(\{a_{:}^{0,0},a_{tc}^{0,0}\}\), \(\{a_{:}^{0,0},a_{tc}^{0,0},a_{:}^{1,0}\}\) respectively. Patching more activation results in increases of \(LD_{orig}-LD_{pch}\), we attribute the increment to the newly patched activation. Hence, the causal effect of each activation is measured conditioned on some activations already being patched.

We sort the activations according to their layer indices and show the results of patching from bottom to top (\(\rightarrow\)) and from top to bottom (\(\leftarrow\)). In this way, we can verify the dependence between activations in the top and bottom layer. For example, at the top of Figure 16, when \(a_{tc}^{0,0}\) is already patched, patching \(a_{:}^{1,0}\) has almost no effect. On the bottom, we also see patching \(a_{tc}^{0,0}\) has no effect if \(a_{:}^{1,0}\) has been patched. So we verified that \(a_{tc}^{0,0}\) is the only upstream activation that \(a_{:}^{1,0}\) relies on, and \(a_{:}^{1,0}\) is the only downstream activation that reads \(a_{tc}^{0,0}\).

Figure 16: Results of activation patching for model trained on character counting task. Same figure as (a)a with intermediate steps of calculation shown using line plot. Note that the gray lines correspond to the y-axis on the right. In contrast examples, only one character differs. \(LD\) stands for logit difference between the original count and the count in the contrast example. \(LD_{pch}\) and \(LD_{orig}\) correspond to the \(LD\) with and without patching, respectively. Top: We patch activations cumulatively from left to right, flipping the sign of \(LD\). The “none” on the left end of x-axis denotes the starting point, i.e., nothing is patched. Bottom: We patch from right to left. Similarly, “none” on the right end of x-axis denotes the starting point.

### Extended Algorithm with Positional Cues

In Section 3.1, we verified the information flow by an activation patching experiment in which the contrast example only differs by one character. These experiments verified that the algorithm we described is complete in distinguishing between such minimally different contrast examples. We now show that the model implements a somewhat more complex algorithm that combines this algorithm with position-based cues, which become visible once we consider contrast example that differ in more than one character, in particular, those that differ in length.

To show this, we conduct another activation patching experiment in which the contrast example is a random example in the dataset with a different count. In other words, everything can be different in contrast examples, including the sequence length and the target character. Thus, we cumulatively patch four places:

1. \(e^{pos}_{tc}\) and \(e^{pos}_{c}\), where \(e^{pos}\) stands for position embedding, because the final count correlates with positional signal, so the model may utilize it. They are patched together because the attention pattern of the colon in layer 1 relies on their adjacency.
2. \(a^{0,0}_{tc}\) (as before) and \(e^{tkn}_{tc}\), where \(e^{tkn}\) stands for token embedding. They are patched together because patching only one of them would result in a conflict between character information in the patched and the un-patched activation;
3. 3) \(a^{0,0}_{:}\); (as before)
4. \(a^{1,0}_{:}\) (as before).

The result is shown on the left of Figure 17. We can see that, besides the components we had detected previously based on minimal contrast examples (\(a^{0,0}_{tc}\), \(a^{1,0}_{:}\)), some other signal also contributes notably to the final logits. We compare with patching experiments for the same set of activations on contrast examples that differ in one character, shown on the right of Figure 17.

Overall, besides the algorithm identified in Section 3.1, we find other 3 sources of information influencing the model's output. 1) The position embedding, \(e^{pos}_{tc}\) and \(e^{pos}_{:}\). This is observable on the left of Figure 17, from which we can also know \(a^{1,0}_{:}\) contains the position information (because the bars of \(e^{pos}_{tc}\) and \(e^{pos}_{:}\) are not symmetric). This is confirmed by InversionView. As shown in Figure 18, we see the inputs in \(\epsilon\)-preimage roughly follow the query input length, being independent of the count. Therefore, the model is also utilizing the correlation between the count and the sequence length. 2) Attention output of colon, \(a^{0,0}_{:}\), which attends to all previous token equally. From InversionView, we observe it contains fuzzy information about the length (same as position signal), and the characters that occur in the context, as well as their approximate count. Our causal experiment also shows that

Figure 17: Results of activation patching for model trained on character counting task. \(\rightarrow\) and \(\leftarrow\) means the same as previously. **Left**: Patching with activation from examples with different counts. **Right**: Patching with activation from examples in which only one character differs.

it does not contain a precise count. Therefore, it contributes to the model's prediction in manner similar to the position signal. 3) Attention output of the pipe sign, \(a_{|}^{0,0}\). From the attention pattern we observe sometimes in layer 0, pipe sign attends selectively to one type of character, e.g. "x", "k", or "j". InversionView shows that it indeed contains the approximate count in that case (though the decoder has not been trained on activation corresponding to pipe sign). In next layer, the colon also attends to the pipe sign if target character is the same as the character attended by pipe sign in layer 0. This explains why we can observe nonzero effect of patching \(a_{|}^{1,0}\) when other activation is already patched (the red bar corresponding to \(a_{|}^{1,0}\) on both sub-figures of Figure 17).

Whereas patching with minimally different contrast examples allowed us to extract an algorithm _sufficient_ for solving the task in Section 3.1, patching with arbitrarily different contrast examples allowed us to uncover that the model combines this algorithm with position-based cues. The model performs precise counting using the algorithm we found earlier in Section 3.1, while it also makes use of simple mechanisms such as correlation to obtain a coarse-grained distribution over counts. Overall, we have found the full algorithm by alternating between different methods - InversionView, traditional inspection of attention patterns, and causal interventions, and confirming results from one with others.

## Appendix E IOI Task: Details and Qualitative Results

### Implementation Details

In order to train a decoder model, we construct a dataset that consists of IOI examples. We used the templates of IOI examples from ACDC [13] implementation. For example, "Then, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A]", in which "[B]" and "[A]" will be replaced by two random names (one token name), "[PLACE]" and "[OBJECT]" will also be replaced by random item from the predefined set. Besides "BABA" template (i.e., S is before IO) we also use "ABBA" templates (S is after IO) by swapping the first "[B]" and "[A]". We generate 250k data points.

The architecture of the decoder model is the same as before, as described in Appendix C. The set of activation sites the decoder is trained on consists of the output of all attention heads and MLP layers (no residual stream). Note that when producing query activation using GPT-2, we always add the "\(<|\text{endoftext}|>\)" token as the BOS token. We do so as during the training of GPT-2 this or multiple such tokens that usually appear in the previous context can be used as a BOS token, which is possibly important to the model's functioning. We use a new token "[EOS]" as the EOS token when training the decoder. The query activation is sampled uniformly from all positions excluding EOS and padding tokens, and uniformly from all activation sites the decoder is trained for. We sample 20 million training examples (all activation sites are included) and train the decoder with batch size of

Figure 18: \(\epsilon\)-preimage of \(a_{|}^{1,0}\) to show the position information is also encoded and is independent of count information.

256, resulting in roughly 80K steps. The final average in-preimage rate is 58.0%, despite that the decoder is trained for 157 activation sites.

### More Examples of InversionView

See Figures 19, 20, 21.

### Qualitative Examination Results

The qualitative examination results is shown in Table 2. We summarize the description in [54] of each head category to facilitate comparison. Figure 22 shows the IOI circuit in GPT-2 small, which is taken from their paper. For more details, please refer to [54].

There are some heads for which InversionView indicates a different interpretation. First, Head 0.1 and 0.10: [54] only shows that they usually attend to the previous occurrence of a duplicate token and validates the attention pattern on different datasets. However, there is no evidence for the information moved by these heads. Thus they only hypothesize that the position of previous occurrence is copied. Second, Head 5.9: The path patching experiments in [54] show that head 5.9 influences the final logits notably via S-Inhibition Heads' keys. But there are no further experiments to explain the concrete function of this head. While the authors refer to it as a Fuzzy Induction Head, the induction score (measured by the attention weight from a token \(T\) to the token after \(T\)'s last occurrence) of this head shows a very weak induction pattern. Even if such pattern occurs, it cannot tell us what

Figure 19: \(\epsilon\)-preimage of S-Inhibition Head 7.3. The relative position of S1–but not its identity–is contained in the head output (together with some template information). That means, in the samples within the \(\epsilon\)-preimage, S1 always appears before the IO. While the relative position is encoded, the absolute position can vary, as can the identities of the names.

Figure 20: \(\epsilon\)-preimage of Duplicate Token Head 0.1. S name is contained in head output.

information is captured by this head. Interpretation with InversionView suggests that the head barely contains any information, within the input space of IOI-like patterns. One possibility is that head 5.9 recognizes the IOI pattern (i.e., there are two names and one is duplicated in the previous context), so that if an IOI-like pattern exists, S2 should be attended to by S-Inhibition heads. As the decoder model is trained on IOI examples and generates mostly IOI examples - that is, the input space \(\mathcal{X}\) in (1) consists of IOI-like inputs, this information is by definition not visible. Expanding the input space to arbitrary language modeling would allow capturing such information; we leave this to future work.

### Choice of Distance Metric in IOI

As described in Section 3.2, we used cosine distance for the IOI task, while for the other two tasks, we use normalized Euclidean distance. In this section, we show that both distance metrics produce similar interpretations while cosine distance makes the meaningful patterns easier to identify.

In Figure 23 and 24 we show two examples of using normalized Euclidean distance as the distance metric. Readers can see more examples on our web application. We see that the samples generated by the decoder tend to have larger distances under normalized Euclidean distance. Nonetheless, we can still see the top-ranked samples show meaningful commonalities - indeed if we set \(\epsilon=0.4\), we obtain the same interpretation as the one we obtain based on Figure 3(b) and 19.

While normalized Euclidean distance can produce a similar interpretation, we have to set a much larger \(\epsilon\), which we find less intuitive. We believe this to be because of the differences in the activation dimensionality, which is \(768\) in the IOI task, much larger than in character counting (64) and addition (32) tasks: Under Euclidean distance, the ratio of all close samples to all possible samples becomes lower and lower when the dimension becomes higher. In other words, the volume of the \(\epsilon\)-preimage accounts for a very tiny proportion of the whole space in the high-dimensional case. By using cosine

Figure 21: \(\epsilon\)-preimage of Induction Head 5.5. Position – but not identity – of the current token (token in parenthesis)’s last occurrence is contained in head output

Figure 22: IOI circuit in GPT-2 small. Figure 2 from [54]

\begin{table}
\begin{tabular}{|p{56.9pt}|p{85.4pt}|p{85.4pt}|p{85.4pt}|p{85.4pt}|} \hline Head Category & Function According to [54] & Position & Observation from InversionView & Whether Consistent \\ \hline \hline Name Mover Heads & Copy whatever it attends to (increase its logit). It will always attend to IO because of previous components. & END & 9.9 9.6 10.0: IO name & Yes \\ \hline Negative Name Mover Heads & Copy whatever it attends to (decrease its logit), it attends to IO & END & 10.7 11.10: IO name & Yes \\ \hline S-Inhibition Heads & Move info about S and cause Name Mover Heads to attend less to S. They attend to S2. They adds two signals to residual stream, one is token value of S, the other is position of S1, and position signal has a greater effect. & END & 7.3 8.6: Position of S1 (relative position to IO, same for the following part when we say position) & Yes \\ \hline Duplicate Token Heads & Attend from S2 to S1, more generally, attend to previous duplicate token, and copy the position of this previous occurrence. & S2 & 0.1 0.10: **S name (instead of position)** & Partly \\ \hline Previous Token Heads & Attend the previous token, move the token (name) to S1+1, then this information is used as key of S1+1 in Induction Heads & S1+1 & 2.2: S name (most of the time, sometimes attend to “and” when S1 is after IO and make S name less important) & Yes \\ \hline Induction Heads & Attend the previous token, move the token (name) to S1+1, then this information is used as key of S1+1 in Induction Heads & S2 & 5.5 6.9: Position of the current token’s last occurrence & \\ \hline \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & \begin{tabular}{} \end{tabular} & S2 & 5.5 6.9: Position of the current token’s last occurrence & Partly \\ \cline{1-1} \cline{3-4}  & \begin{tabular}{} \end{tabular} & S2 & 5.8: Position of the current token’s last occurrence (most of the time) & \\ \cline{1-1} \cline{3-4}  & \begin{tabular}{} \end{tabular} & S2 & 5.8: Position of the current token’s last occurrence (most of the time) & \\ \cline{1-1} \cline{3-4}  & \begin{tabular}{} \end{tabular} & S2 & 5.9: **Almost no info**, possibly some info about the template. & \\ \hline Backup Name Mover Heads & Do not move IO normally, but act as Name Mover Heads when they are knocked out. & & 9.0 10.1 10.10: IO name & \\ \cline{1-1} \cline{3-4}  & \begin{tabular}{} \end{tabular} & S10.6 : S and/or IO name (Most of the time: IO, sometimes: IO and S, occasionally: S) \\ \cline{1-1} \cline{3-4}  & \begin{tabular}{} \end{tabular} & S2 & 10.2: S and IO name (most of the time), S or IO name (sometimes) & Yes \\ \cline{1-1} \cline{3-4}  & \begin{tabular}{} \end{tabular} & S11.9: S and/or IO name (no obvious difference in frequency) & \\ \cline{1-1} \cline{3-4}  & \begin{tabular}{} \end{tabular} & S12.2: S and IO name (most of the time), S name (sometimes) & \\ \cline{1-1} \cline{3-4}  & 
\begin{tabular}{} \end{tabular} & S2 & 9.7: S name & \\ \hline \end{tabular}
\end{table}
Table 2: Column “Position” means the query activation is taken from that position. “S1+1” means the token right after S1. Rows are ordered according to the narration in the original paper. When we say “S name”, it means the the name of S in the query input, but the name is not necessarily S in the samples. This also applies to “IO name”. The information learned by InversionView which is different from the information suggested by Wang et al. [54] is in **bold**.

Figure 23: \(\epsilon\)-preimage of the same activation as Figure 4b using normalized Euclidean distance instead of cosine similarity (Appendix E.4). The line shown in the figure still represents threshold of 0.1. While at \(\epsilon=0.1\), some query inputs do result in a substantial number of in-\(\epsilon\)-preimage samples, many cases result in very small or even empty (as here) sample sets, suggesting that the \(\epsilon\)-preimage at 0.1 – at least as accessible to the decoder – is extremely small in this model, which we speculate is related to the models higher dimensionality compared to the other tasks (768 vs 32/64), which tends to make Euclidean distances large except for extremely similar vectors (see Appendix E.4 for more discussion). We find it more convenient and natural to use similar thresholds (\(\epsilon=0.1\)) across tasks and account for the different geometries using different distance metrics. What is key, however, is that for an appropriately higher \(\epsilon\) (e.g., \(\epsilon=0.4\)) we again always obtain a substantial number of samples, and – most importantly – these samples lead to the same interpretation as we obtained in our main experiments with the cosine similarity. Here, for example, we can obtain the same interpretation by setting \(\epsilon=0.4\), i.e., the IO name is encoded in the activation.

Figure 24: \(\epsilon\)-preimage of the same activation as Figure 19 using normalized Euclidean distance. The line shown in the figure still represents the threshold of 0.1. As explained in Figure 23, due to the representation geometry, normalized Euclidean distance tends to require a much higher threshold to obtain a sufficient sample size for interpretation. Importantly, as also explained there, we still obtain the same interpretation as in our main experiments if we use normalized Euclidean distance but take a higher threshold (e.g., \(\epsilon=0.4\)): here, the relative position of S1 is encoded in the activation.

similarity, we allow more input samples to lie in a close distance with the query input, as cosine similarity ignores the magnitude difference. This suggests cosine similarity may overall be more suitable when applying InversionView in high-dimensional activations.

## Appendix F 3-Digit Addition: More Details and Examples

### Implementation Details

We constructed 810,000 instances and applied a random 75%-25% train-test split. The probed model is trained with a constant learning rate of 0.0001, a batch size of 512, for 50 epochs (59350 steps). We save the last checkpoint as the trained model, and test it on the test set. We use the AdamW [36] optimizer with weight decay of 0.01. The training loss is shown in Figure 25.

With regard to the decoder model, we select \(x^{0,\mathrm{pre}},x^{i,\mathrm{mid}},x^{i,\mathrm{post}}a^{i,j}\) as the set of activation sites we are interested, where \(i\in\{0,1\},j\in\{0,1,2,3\}\). When sampling training data, we select an activation site and a token position uniformly at random. We sample 100 million training examples (all activation sites are included) and train the decoder with batch size of 512, resulting in roughly 200K steps. The final average in-preimage rate is 88.6%.

### More Examples of InversionView

See Figures 26 to 29.5

Footnote 5: We have also tried value-weighted attention pattern [32]; it makes a negligible difference, so we always show the original attention pattern in our paper.

### Model Deficiency

In Section 3.3, we mention that there is no firm and clear path of obtaining the carry from ones to tens, so the model may make wrong prediction. We examine those instances for which the model makes wrong prediction and find they all satisfy one condition: F2+S2=9. In other words, it fails to make the right prediction because the ones place matters. This is consistent with our interpretation of the model. Furthermore, we check the model's accuracy on this special subset where F2+S2=9, and find that it is significantly higher than chance level. The accuracy on training subset (training data where F2+S2=9) is 80.45%, and on test subset is 80.06%, while chance level is 50%. So, we can infer that the probed model obtains some information about the ones place by means other than memorization. Indeed, we observe fuzzy information about ones place in \(a^{1,0}\) and \(a^{1,1}\) occasionally (See Figure 30).

Figure 25: Training loss of the 3-digit addition task. Each data point is the averaged loss over an epoch. The final loss is still big since the two operands of the addition is unpredictable.

### Qualitative Examination Results

We present our overall qualitative results in Figure 31 and Table 3. We have found that the model obtains required digits by attention, and primary digits are assigned more heads than secondary digits, e.g., \(a^{0,0}\), \(a^{0,1}\), \(a^{0,2}\) for hundreds and \(a^{0,3}\) for tens when predicting A1. More importantly, the primary digits are encoded precisely while the secondary digits are encoded approximately in the residual stream. In addition, the model routes the information differently based on whether A1=1, i.e., the length of the answer is 3 or 4. When predicting A2, this information is known before layer 0, thus paths differ from the start. On the contrary, when predicting A3, the information is obtained in layer 0, thus paths differ only in layer 1. Furthermore, in Figure 31, sub-figure (c) and (d) are very similar, indicating model uses almost the same algorithm to predict digit in tens place. While sub-figure (e) shares the layer 0 with (d), its layer 1 is similar to (f).

### Causal Intervention: Details and Full Results

We use similar activation patching method as character counting task, described in D.3. In Figure 31, we split the overall algorithm into individual ones for digits in the answer, under different

Figure 27: The \(\epsilon\)-preimage of \(x^{0,\mathrm{mid}}_{=}\) of different examples.

\begin{table}
\begin{tabular}{|p{56.9pt}||p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \hline Pred Target & **A1** & **A2** & **A3** & **A4** / **E** \\ \hline \hline \(a^{0,0}\) & 1-2 digits from F1 and S1 & whether A1=1 & whether A1=1, so the model knows A3 is in tens or ones place & whether A1=1, so the model knows the next token is A4 or E \\ \hline \(a^{0,1}\) & & **If** A1=1: Almost no info & & 1-2 digits from F2 and S2 & 1-2 digits from F3 and S3 \\ \hline \(a^{0,2}\) & & **If** A=1: both F1 and S1 & & & \\  & & **Else**: 1-2 digits from F2 and S2 & & & \\ \hline \(a^{0,3}\) & 1-2 digits from F2 and S2; C2 & **If** A=1: I-2 digits from F1 and S3; C3 & 1 digit from F3 and S3; C3 & & \\ \hline \(x^{0,\mathrm{mid}}\) & F1 and S1; C2 & **If** A1=1: F1 and S1 & & & \\ \hline \(x^{0,\mathrm{post}}\) & same as \(x^{0,\mathrm{mid}}\) & same as \(x^{0,\mathrm{mid}}\) & same as \(x^{0,\mathrm{mid}}\) & same as \(x^{0,\mathrm{mid}}\) & same as \(x^{0,\mathrm{mid}}\) \\ \hline \(a^{1,0}\) & Fuzzy info about F1, S1 and C2; Fuzzy info about F3 and S3 (sometimes) & **If** A1=1: F1 and S1 (sometimes fuzzy); C2 (sometimes) & **If** A1=1: I-2 digits from F2 and S2; C3 (sometimes) & **If** A1=1: I-2 digits from F2 and S2; & 1-2 digits from F3 and S3 & 1-2 digits from F3 and S3; For \(a^{1,0}\) and \(a^{1,1}\), they also contain info about whether next token should be E \\ \hline \(a^{1,2}\) & Fuzzy info about F2 and S2 (sometimes) & 1-2 digits from F1 and S1 (sometimes fuzzy); 1-2 digits from F2 and S2 & & \\ \hline \(a^{1,3}\) & 1-2 digits from F1 and S1 (sometimes fuzzy) & **If** A1=1: F1 and S1 (sometimes F2 and S2 & & \\ \hline \(x^{1,\mathrm{mid}}\) & same as \(x^{0,\mathrm{mid}}\) & **If** A1=1: info from \(x^{0,\mathrm{mid}}\) + C2 & **If** A1=1: same as \(x^{0,\mathrm{mid}}\) + F3 and S3 & & same as \(x^{0,\mathrm{mid}}\) \\ \hline \(x^{1,\mathrm{post}}\) & same as \(x^{0,\mathrm{mid}}\) & same as \(x^{1,\mathrm{mid}}\) except that current input token A1 is less important & same as \(x^{1,\mathrm{mid}}\) except that current input token A2 is less important & same as \(x^{0,\mathrm{mid}}\) except that current input token A3 is blurred \\ \hline \end{tabular}
\end{table}
Table 3: Summary of our observations for each activation site and position. “same as” denotes that there is no obvious difference between the two sites for indicated position.

condition. Each algorithm predict target token based on multiple types of information (digits in hundreds/tens/ones place).

In order to verify the paths responsible for routing each type of information, we construct contrast examples as follows: Given a prediction target (e.g., A1), a set of tokens that can be changed \(T_{chg}\) (corresponds to a certain type information, e.g., F1 and S1), we construct a contrast example \(\mathsf{x}_{con}\) that contains a different token \(t_{con}\) as prediction target by changing tokens in \(T_{chg}\). Note that the contrast example still follows the rule that the answer is the sum of the two operands.

We now give a detailed explanation for Figure 6b shown in the main paper, in which prediction target is A1 and we patch head output corresponding to the preceding token "=". On the left of Figure 6b, \(T_{chg}=\{\text{F1, S1}\}\). So in the contrast examples the F1 and S1 are changed and other digits in operands remains the same. In the third run where we calculate \(LD_{pch}\), activations are replaced by new activations from contrast example, so the new activations contain modified F1 and S1 information. Therefore, for activations that contributes to routing F1 and S1 (e.g., \(a_{=}^{0,0}\)), patching them with new activations can effect model's prediction. On the contrary, patching \(a_{=}^{0,3}\) has no effect

Figure 28: \(\epsilon\)-preimage of more examples

Figure 29: Some examples where we can see intermediate states between representing digits separately and representing digits as their sum. In these examples, we see in the \(\epsilon\)-preimage the digits in hundreds place are either (2,8) or (3,7), while the digits in tens place are mostly encoded as their sum.

Figure 31: The information flow diagrams for predicting the digits in answer. F1 and S1 are aligned, F2 and S2 are aligned, and so forth. Color of the lines represents the information being routed, and alternating color represents a mixture of information. The computation is done from left to right (or simultaneously during training), and from bottom to top in each sub-figure. Note that the figure represents what information we find in activation, rather than the information being used by the model. Also note that the graphs are based on our qualitative examination using InversionView and attention pattern, and are an approximate representation of reality. We keep those stable paths that almost always occur. Inconsistently present paths such as routing the ones place when predicting A1 are not shown.

Figure 30: Some examples where information about ones place is also encoded.

because it contains information about F2 and S2, which are the same in contrast examples. On the right of Figure 6b, \(T_{chg}=\{\text{F2, S2}\}\), so we are verifying the activations that plays a role in routing F2 and S2. Note that we exclude those data in x\({}_{orig}\) where F1+S1\(\geq\)10, because changing F2 and S2 cannot change A1 in those cases.

Activation patching results for other cases are shown in Figures 32 to 36.

Overall, among all the intervention experiments and their corresponding information flow diagrams in Figure 31, the activation with the highest increment not included in the information flow diagram is \(a_{A1}^{1,1}\) in Figure 32 (right), accounting for only 5.92% of the cumulative increment. In this sense, the information flow diagrams coupled with interpretations present an almost exhaustive characterization of the algorithm used by the model to predict the answer digits.

Figure 33: Causal verification results for the information flow in sub-figure (c) in Figure 31: predicting A2 when \(A1\neq 1\). We exclude those data in x\({}_{orig}\) where A1=1. The constructed contrast data x\({}_{con}\) also satisfies this constraint. **Left**: \(T_{chg}=\{\text{F2, S2}\}\). **Right**: \(T_{chg}=\{\text{F3, S3}\}\). We further exclude those data in x\({}_{orig}\) where F1+S1=9 and F2+S2=9 because we cannot find a contrast example in those cases. The results confirm that information about the digits in hundreds and tens places is routed through the paths that we hypothesized based on InversionView in Figure 31c.

Figure 32: Causal verification results for the information flow in sub-figure (b) in Figure 31: predicting A2 when A1=1. We only consider data in x\({}_{orig}\) where A1=1. The constructed contrast data x\({}_{con}\) also satisfies this constraint. **Left**: \(T_{chg}=\{\text{F1, S1}\}\). **Right**: \(T_{chg}=\{\text{F2, S2}\}\). Note that the included data from x\({}_{orig}\) all satisfy F1+S1\(\geq\)10, because, if F1+S1=9 and A1=1, no contrast example obtained by changing F2 and S2 would satisfy the constraint. The results confirm that information about the digits in hundreds and tens places is routed through the paths that we hypothesized based on InversionView in Figure 31b.

## Appendix G Decoder Likelihood Difference

In figures of addition task, we also show the decoder likelihood difference for each token in generated samples. It indicates what _might_ be relevant to the activation. It is calculated as follows: During generation, we sample the next token from the distribution produced by the decoder model, and we record the probability of the sampled token in that distribution. We denote it as \(p_{act}\). Then, we run the decoder again with the same input tokens, but this time it is fed with a "blank" activation (the activation corresponding to BOS token from the same activation site: because of the causal masking of the probed model, this activation does not contain any information). Therefore, it produces a different distribution. The probability of the same token (the token already sampled in the normal run) in the new distribution is \(p_{blank}\). The difference \(p_{act}-p_{blank}\) indicates if the decoder model can be more confident about a token when it receives the information from the activation. If \(p_{act}-p_{blank}>0\), the color is blue, and if \(p_{act}-p_{blank}<0\) it is red. The depth of color is proportional to the magnitude of the value. Therefore, it highlights what can be learned from the query activation, in addition to what is in the context.

Figure 34: Causal verification results for the information flow in sub-figure (d) in Figure 31: predicting A3 when \(A1=1\). We exclude those data in \(\mathsf{x}_{orig}\) where A1\(\neq\)1. The constructed contrast data \(\mathsf{x}_{con}\) also satisfies this constraint. **Left**: \(T_{chg}=\{\text{F2},\text{S2}\}\). **Right**: \(T_{chg}=\{\text{F3},\text{S3}\}\). We further exclude those data in \(\mathsf{x}_{orig}\) where F1+S1=9 and F2+S2=9 because we cannot find a contrast example in those cases.

Figure 35: Causal verification results for the information flow in sub-figure (e) in Figure 31: predicting A3 when \(A1\neq 1\). \(T_{chg}=\{\text{F3},\text{S3}\}\). We exclude those data in \(\mathsf{x}_{orig}\) where A1=1. The constructed contrast data \(\mathsf{x}_{con}\) also satisfies this constraint.

Note that the decoder has learned how to handle the activation of BOS, since it also appears in the training set because we sample uniformly at random among all tokens in the input (as in IOI and addition task).

Importantly, unlike the distance metric value, the decoder likelihood difference depends on the capability of the decoder model, and we should bear in mind that it might be inaccurate. We caution that this difference does not necessarily highlight the directly relevant part, complicating its interpretation. As shown in Figure 37, the color highlights digits in the second operand, which does not reflect the actual flow of information. In these examples the query activation corresponds to digits in answer. For example, in the left most sub-figure, \(x^{0,\mathrm{pre}}\) contains the information "6 is at the position of A1", but the color does not highlight A1. This is because, on one hand, decoder predicts S1 conditioned on F1, so knowing their sum will significantly increase the confidence of S1, and S1 is highlighted. On the other hand, when predicting A1, the previous digits can already determine the answer. There is a high confidence even without knowing A1, thus it is not highlighted. In essence, the information contained in a query activation may manifest itself early, and the decoder likelihood difference does not necessarily align with the part of the input from which the information has actually been obtained.

An interesting direction for future work could be developing decoders that generate samples in a permuted order, and generate most confident tokens first, possibly based on architectures like XLNet [60].

## Appendix H Factual Recall: Detailed Findings

### Background

We use InversionView to investigate how GPT-2 XL (1.5B parameters) performs the task of recalling factual associations. In recent research [24, 11, 20], the task has been formalized in terms of triples \((s,r,a)\), where \(s\), \(r\), \(a\) are subject, relation, attribute respectively. The model should predict \(a\) based on input containing \(s\) and \(r\) in natural language. E.g., given the prompt: _"LGA 775 is created by"_, the model is expected to predict _"Intel"_. In this section of the paper, we refer the last token as END, and the last subject token as SUBJ. In the above example, END is _"by"_ and SUBJ is _"75"_. Geva et al. [24] found a high-level pattern that GPT-2 XL uses in solving this task: Early-middle MLP layers at SUBJ integrate information about the subject into its residual stream. Meanwhile, the relation

Figure 36: Causal verification results for the information flow in sub-figure (f) in Figure 31: predicting A4/E. **Left**: \(T_{chg}=\{\text{F3,S3}\}\). We exclude those data in x\({}_{orig}\) where A1\(\neq\)1, since in that case the prediction the target position is almost always E (end of the text). Changing F3 and S3 will not change E. Even when it does, i.e., when F1+S1=9 and F2+S2=9 and F3+S3\(<\)9, changing F3 and S3 will cause A1 to change. But we need to keep other variables the same. Based on the same reason, the contrast examples should also satisfy the constraint A1=1. **Right**: \(T_{chg}=\{\text{F1,S1}\}\). We change F1 and S1 in order to change A1, thus changing A4 to E or vice versa. There is no constraint in this case, since we can always find contrast examples.

information is incorporated into END's residual stream through early attention layers. In upper layers, the correct attribute is moved from SUBJ to END's residual stream by attention layers.

### Selecting Activation Sites

As we mentioned earlier, our goal here is not to provide a full interpretation of the computations performed to solve this task; rather, it is to test whether InversionView can successfully produce interpretable results on larger models. As the factual recall task is complex and involves many model components [61; 11], we decide to focus on 25 attention heads in the upper part of the model (layer 24-47) that contribute most frequently to the final prediction. We select these activation sites because the attribute retrieval tends to happen there [24] and we expect more abstract information in \(\epsilon\)-preimage. Here, we note that preliminary experiments revealed that it is necessary to restrict the number of activation sites that the decoder is trained for, given a limited compute budget, as more activation sites require the decoder to learn more a complex overall inverse mapping from activations to inputs. Scaling the approach to apply to many activation sites simultaneously is left to future work.

Concretely, we use the attribution method introduced in [18] to estimate the head importance at END position. Formally, given \(y=z_{1}+\cdots z_{m}\), the importance of each term \(z_{j}\) to the sum \(y\) is estimated as follows:

\[\text{importance}(z_{j},y)=\frac{\text{proximity}(z_{j},y)}{\sum_{k}\text{ proximity}(z_{k},y)},\] (4)

\[\text{proximity}(z_{j},y)=\max(-\|z_{j}-y\|_{1}+\|y\|_{1},0).\] (5)

Regarding the intuition and estimation quality of this method, please refer to [18]. In our case, \(x^{i,mid}_{\text{END}}=x^{i,pre}_{\text{END}}+\sum_{j}a^{i,j}_{\text{END}}\), we calculate \(\text{importance}(a^{i,j}_{\text{END}},x^{i,mid}_{\text{END}})\) on each example on a subset of CounterFact[37]. The subset contains around 1,000 factual prompts known by GPT-2 XL (we used the same subset as described in [37] Appendix B.1). We consider an attention head activated on a certain input prompt if its importance is higher than the threshold of 0.02, and calculate the frequency of being activated over the subset. Finally we select top 25 most frequent heads in model's upper layers.6

Footnote 6: \(h^{31,0}\), \(h^{33,0}\), \(h^{38,22}\), \(h^{29,9}\), \(h^{37,7}\), \(h^{3,2,12}\), \(h^{31,8}\), \(h^{24,24}\), \(h^{28,3}\), \(h^{25,7}\), \(h^{28,21}\), \(h^{27,16}\), \(h^{42,24}\), \(h^{31,4}\), \(h^{34,20}\), \(h^{30,23}\), \(h^{24,8}\), \(h^{30,8}\), \(h^{25,10}\), \(h^{33,9}\), \(h^{32,15}\), \(h^{30,1}\), \(h^{29,20}\), \(h^{36,17}\), \(h^{35,19}\), ordered by frequency, last one is the most frequent

There are multiple methods that can be used to find important attention heads [38; 39; 50; 29], because we do not have strict requirements for finding the most important heads, we choose this one because of its simplicity.

Figure 37: The \(\epsilon\)-preimage of \(x^{0,\text{pre}}\). Here the information contained is A1, A2, A3 respectively, while the decoder likelihood difference highlights digits in operands. Because given the first operand and the final sum, the digits in second operand can be inferred.

### Decoder Training

To train the decoder model, we collect text from 3 datasets, including the factual statements from CounterFact[37]7 and BEAR[56]8, as well as general text from _MiniPile_[30]. The factual statements we used are complete sentences containing the final attribute. Note that in CounterFact, for each attribute, there are always multiple different subjects associated with it. So if the information contained is solely the attribute, a well-trained decoder should generate different subjects. Regarding _MiniPile_, we randomly select 10% of it, and split the text into sentences [9] and remove sentences longer than 100 characters.

Footnote 7: We make use of all prompts of each data point. Besides the _”prompt”_, we also include _”paraphrase_prompts”_, “neighborhood_prompts”_, and “attribute_prompts”_ concatenated with their corresponding answer.

Importantly, for each head we selected, we extract a separate subset of sentences from the collected data on which the head is "activated". By being "activated" we mean the attention weight on BOS token is less than 0.69. We find this is important based on preliminary experiments, because in many cases attention heads in higher layers execute "no-op" by exclusively attending to BOS, resulting in attention output containing no information. Training decoder on these activations discourages it to learn to read information from the query activation. So each head correspond to a subset of text, on which the head output will be captured to create training data. The number of sentences in subsets ranges from 0.6 to 2.6 million.

Footnote 9: we do not experiment with other values

The training data for decoder consists of two parts, each of which accounts for 50%. In one part, the activations are taken from GPT-2 XL when processing factual statements from CounterFact and BEAR, and activations correspond to the END position (the sentence structure is provided in these datasets). In the other part, the activations correspond to text from all 3 datasets (thus mainly composed of _MiniPile_ text), and correspond to the position with least attention weight on BOS token. By doing so, we emphasize the importance of factual statements domain while covering a large variety of text.

We use GPT-2 Medium as the backbone model for decoder. Concretely, the newly added components in the decoder architecture (e.g. those parts responsible for processing query activation as shown in Figure 12) are trained from scratch, but the transformer layers in the decoder are equipped with pretrained weights. In this way, we also make use of the existing capacity of language models. Note that in other case studies, we use a small 2 layer transformer as decoder. The reason why we use a much larger decoder for this task is we expect a much more complex inverse mapping to be learned. Specifically, we expect \(\epsilon\)-preimage for some activation sites to contain multiple different subjects sharing a certain attribute, the decoder needs to generate these subjects given the attribute. So it needs enough capacity to memorize the knowledge.

Similar as before, we always add the "\(<|\)endoftext\(|>\)" token as the BOS token when capturing query activation from GPT-2 XL. We again use a new token "[EOS]" as the EOS token when training the decoder. We sample query activation with equal probability of activation sites. We sample 32 million training examples (all activation sites are included) and train the decoder with batch size of 512, resulting in roughly 60K steps. The final average in-preimage rate is 36.0%.

### Sampling from Decoder

Similar as before, when using decoder to generate samples, we perturb the query activation before feeding it to the decoder in order to cover the neighbourhood around the query. In this larger model with high-dimensional activations, we found it useful to craft this process more carefully and use different disturbance. In preliminary experiments, we found that cosine similarity usually produces more interpretable \(\epsilon\)-preimages, therefore, we randomly sample activations that have a certain angle \(\theta\) to the query activations, and then randomly scale it so that its magnitude ranges from \(e^{-1}\cdot\|\mathbf{z}^{q}\|_{2}\) to \(e\cdot\|\mathbf{z}^{q}\|_{2}\). We repeat this process for different \(\theta\) values with in the range \(\cos\theta\in[0.75,0.9]\). To further encourage diversity, we lower the probability of tokens that have already appeared too many times in the generated samples.10We use the first 200 examples of the aforementioned subset of CounterFact (the subset containing examples known by GPT-2 XL) as query inputs and generate samples. The results are also available in our web application.

### Observation

Choice of Threshold.We use the same distance metric as in IOI task, and we set the threshold \(\epsilon=0.25\). As we mentioned earlier, larger threshold produces more coarse-grained information. Because we observe that the trained decoder in many cases does not generate enough and diverse samples within a close distance (e.g., 0.1), we increase the threshold in order to draw more reliable conclusions.

A priori, the sparsity of sampled \(\epsilon\)-preimages at smaller thresholds may reflect multiple possibilities. One is that there indeed are no close neighbors for the query input. For example, if the subject's text is copied, then only samples containing the same subject or the same token will lie in \(\epsilon\)-preimage. Such a phenomenon may generally reflect the high-dimensional geometry of larger models. Another one is that the decoder is not complete; and that it would either require more training or more capacity, or more training data.11 Training larger decoders on more data would mitigate this problem, which can be done in future work. Nevertheless, in some cases, we do observe a substantial number of diverse samples even within \(\epsilon=0.1\), allowing us to infer information at a higher level of granularity in these cases.

Footnote 11: For example, if the query activation contains information “related to Amazon and related to audio”, there are only a few satisfactory subjects in the dataset (“Audible.com”, “Amazon Music”).

Some heads have fixed behaviorWe observe that some heads' outputs almost always contain only relation information, if they contain any information at all (being "activated").12 In Figure 38, we show two examples for one of these heads. We can see that samples in \(\epsilon\)-preimage share the same relation. The results are consistent across query inputs. We can infer that the these heads move the relation information to END's residual stream.

Footnote 12: \(h^{24,24}\), \(h^{25,7}\), \(h^{27,16}\), \(h^{28,21}\), \(h^{29,20}\), \(h^{33,9}\)

On the other hand, some other heads almost always move information about the subject, when they are "activated".13. Figure 39 shows one of these heads. We can see that the information is only about the subject - so the function of these heads can be summarized as moving information about subject. Interestingly, while a head can move certain attribute about the subject (e.g., nationality, or profession, etc), the attributes it moves for different subjects are diverse. For instance, while a head might, on a certain input, move information that the subject plays certain kind of sports, it may, on another input, move information that the subject is an electronic product. One head, 31.0 tends to usually show

Figure 38: \(\epsilon\)-preimage for head output \(a^{24,24}\), showing encoded information is the relation. On the left, the relation “developed by” appears throughout the \(\epsilon\)-preimage. On the right, the relation “domain of work is” is consistent \(\epsilon\)-preimage.

nationality or language information about the subject, but usually the heads we studied show no clear preference for a type of attribute. In addition, these is no obvious correlation between the subject attributes encoded by such heads and the attribute queried by the relation.

Other heads exhibit a mix of behaviorsThe other heads among the 25 heads we inspect, when "activated", move information about subject or relation (in some cases, both). Which behavior is exhibited varies between inputs. Figure 40 shows one of these heads. On the left of the figure, the head moves information about subject, while on the right it moves information about the relation.

Relation-agnostic retrievalIn the factual recall task, only one specific attribute is sought. Geva et al. [24] found evidence that the model "queries" the residual stream at SUBJ for the specific attribute asked by the relation part of the prompt, and the representation at END can be viewed as such a relation query. In other words, the attribute extracted from the subject representation depends on the relation. However, we do not observe reliable evidence for this phenomenon among the 25 heads we inspect. In the figures we have shown so far, we can see the commonality shared between samples in \(\epsilon\)-preimage is not the attribute requested in the prompt. This still holds if one reduces the

Figure 40: \(\epsilon\)-preimage for head output \(a^{35,19}\), which exhibit different information on different inputs. On the left, Philippines-related words are common throughout the \(\epsilon\)-preimage. For example, “Cebu”: a city in Philippines, “TV Patrol Southern Tagalog”: a TV program in Philippines, “Enrile”: A municipality of Philippines and a surname of many famous Filipinos, “GMA”: a Philippine television channel / broadcasting company. So the information contained is the subject is “Philippines-related”. Note that because the samples are generated, _the fact stated in sample is not true in many cases_. On the right, the relation “performs on” is encoded.

Figure 39: \(\epsilon\)-preimage for head output \(a^{29,9}\), which contains information about the subject. On the left, Czech-related words are common throughout the \(\epsilon\)-preimage, e.g., “Czech”, “Prague”, “Bohem...”, so the information contained is the subject is “Czech-related”. On the right, “Nokia” is shared in \(\epsilon\)-preimage, so the information is simply “Nokia”.

threshold. Figure 41 shows two more examples showing this characteristic. In general, we find that the information moved by the 25 attention heads tends to be the most important attribute or the "main" attribute about the subject. On the left of Figure 41, the most important attribute coincides with the requested attribute. We know this is a coincidence because the closest sample in \(\epsilon\)-preimage has a different relation. Note that other attributes about the subject could be moved by heads that we have not studied.

In addition, while heads that attend to subject usually move the most important attribute, we do observe sometimes different attributes are moved by different heads. In Figure 42, we show that information about profession and information about language/nationality of the subject are extracted by two heads. In fact, we observe that head \(h^{31,0}\) tends to extract language/nationality information in general. But we do not find other obvious pattern of attribute category extracted by other heads.

Our findings echo those from [11], who argue that the primary mechanism for the factual recall task is additive. With our running example _"LGA 775 is created by"_, simply speaking, some heads promote attributes associated with the subject (_chip_, _hardware_, _Intel_, etc.), some heads promote attributes associated with the relation (_Apple_, _Nintendo_, _Intel_, etc.). When the results from these independent mechanisms are added together, the intersection (_Intel_) will stand out. Therefore, the model can solve the task by adding two simple circuits, while humans find Q-composition [17] (i.e., relation information is used as queries in attention heads) more intuitive. From another perspective, this

Figure 41: Examples showing relation-agnostic retrieval. On the left, the information encoded is “soccer”, which is indeed the requested attribute. However, the first sample shows this is not dependent on the relation, since the “soccer” is still retrieved when relation is “speaks language”. On the right, the information “audio-related” is encoded, while the relation in the query input is “owned by”.

Figure 42: Examples showing different attributes of the same subject are extracted by different heads. In the query input, “Joseph Schumpeter” is an Austrian political economist. On the left, the information encoded is “economist”. On the right, the information is about language/nationality (areas around Austria). Again, we emphasize that the facts stated in the sample are not necessarily true.

mechanism implies vector arithmetic. Instead of vector addition in vocabulary space, we can think of it as first summing two vectors (e.g., the output of subject heads and the output of relation heads) and then projecting them to vocabulary space.

The evidence found by Geva et al. [24] can also be explained by this mechanism. They use DLA to inspect attention layers' updates to END's residual stream, and find that the token most strongly promoted by each update usually matches the attribute predicted at the final layer. In other words, after projecting attention layer's output to vocabulary space, the top-1 token is usually the exact requested attribute. Because their experiments study attention layer's output as a whole, instead of individual heads, an alternative explanation is the additive mechanism mentioned above. Importantly, because we only inspect 25 heads, other mechanisms including Q-composition are also possible. Moreover, different mechanism can exist in other models, readers should not draw strong conclusions.

### More Examples of InversionView

See Figure 43, Figure 44. Interestingly, when inspecting the \(\epsilon\)-preimage shown on the right of Figure 44, we also find an example showing InversionView can detect a flaw of the model. Our interpretation for this activation is "Canada-related", and we can see "York University" (which is in Canada) inside the preimage, while "University of York" (a different university, located in England) is outside of the preimage. However, we find that the "U of York" is _inside_ the preimage.

Checking the model's prediction about "U of York", we find that _the model indeed believes "U of York" is in Canada_. More specifically, given the prompt "U of York is located in", the top-12 predictions for the next token are: the, Toronto, York, downtown, a, one, New, central, Canada, North, Scarborough, London. On the contrary, with the prompt "University of York is located in" the model's top-12 predictions are: the, York, North, East, central, Yorkshire, north, a, northeast, London, England, northwest.

## Appendix I Notes on Attention, Path Patching, DLA and others

### InversionView Reveals Information from Un-attended Tokens

In Section 4, we mention that attention pattern is not sufficient to form hypothesis when the model has more than one layer. Because unlike in layer 0 each residual stream contains information only about the corresponding token, in higher layers each residual stream contains a mixture of tokens from the context, making it difficult to determine what information is routed by attention. Besides this point, we also find that sometimes attention pattern can be misleading even in layer 0.

InversionView reveals how components can know more than what they attend to. At the top of Figure 45, we show the attention weights of head \(h^{0.3}\). Here, "=" attends almost solely to S2, so the head output \(a^{0.3}\) should only contain information that there is an "8" in tens place. The generated \(\epsilon\)-preimage, however, shows that it contains information about F2: The number in tens place other

Figure 43: \(\epsilon\)-preimage showing information about the subject moved by the attention head. On the left, the information is “cpu/computer-hardware-related”. On the right, the information is “island country”. Note that some statements are not correct.

than "8" is always in a certain range (\(\geq 4\)), resulting in a carry to the hundreds place. To verify this, we manually constructed examples (rightmost column in Figure 45) where the other number is outside of the range, and found that, for these, the activation distance is indeed very large, confirming the information suggested by the decoder. In layer 0, how does the model obtain information about a token without attending it? At the bottom of Figure 45, we show the attention weights of those manually inserted examples. So the answer is: a different attention pattern would arise if F2 is not in that range. Information can be passed not only by values, but also by queries and keys. InversionView successfully shows this hidden information, even without comparing across different examples.

Figure 44: \(\epsilon\)-preimage showing different information of subject is moved by different head. On the left, the information is superficial text content “York” and/or “University”. Samples containing these words are in \(\epsilon\)-preimage, such as “University of York”, even though it is a different university located in England. This can be confirmed by the fact that “Wellington, Ontario” is far away. On the right, the information is “Canada-related”, which is more high-level. We can also see “University of York” is outside of the preimage.

Figure 45: For activation site \(a^{0,3}\), InversionView reveals how activations can encode information without an attention edge: (a) Even though, on this input, \(h^{0,3}\) attends only to one tens place digit, it also encodes the approximate identity (range 4–8) of another tens place digit. It encodes that the sum of tens places is greater than ten. (b) We verify our hypothesis by manually create some samples and calculate \(D\). (c) Attention patterns for manually created inputs outside of the \(\epsilon\)-preimage. The attention pattern differs from that of the query input. In the query input, the attention head infers information about the second tens place digit from the _absence_ of an attention edge.

### Additional Discussion about Path Patching

Besides our argument in Section 4, another important aspect of circuit discovery methods is that, in many tasks (including our character counting task), the computational nodes do not correspond to fixed positions, and directly applying path patching is problematic. It's not really clear how to apply path patching when varying input positions matter, as the literature on circuit discovery defines circuits in terms of components, not in terms of input positions. In the case of Character Counting Task, such an interpretation would just define a circuit linking the embeddings, attention heads, and MLPs, without capturing the role of different positions, and the fact that characters from varying positions feed into the computation. Such a view would not provide any nontrivial information about the mechanics of the implemented algorithm. This reflects a more general conceptual challenge of circuit discovery: When different input positions are relevant on different inputs, as in the Character Counting Task, one could either define a single circuit across inputs in which every input position is connected to a single node that performs a potentially complex computation, or define per-input circuits where the wiring is input-dependent; however, per-sample path patching is not very scalable, and resulting per-input circuits would require further interpretation to understand how they are formed across inputs.

### Additional Discussion about DLA

Direct Logit Attribution (DLA) extends the logit lens method to study individual model components. Specifically, it projects the output of a model component (thus an update on the residual stream) into vocabulary space, and interpret the component by inspecting the tokens it promotes or suppresses. This method has gained popularity in recent years [23, 54, 16, 19], especially in the research of interpreting factual recall in language models [24, 62, 11, 61]. However, in this section, we argue that DLA is only suitable for studying model components that _directly_ affect the model's final output, and is not well-suited for components whose effect is mediated by other components. In the circuits found by path patching [54, 13], we can see many components that do not connect to the final output directly, which suggests a substantial part of their effect on the final predictions is mediated by further components. DLA shows their direct effect, which may even be non-causal "side effects". Intuitively speaking, some information is meant to be read by a downstream component, e.g., the S-Inhibition Heads' output in IOI circuit is meant to be read by the query matrix of Name Mover Heads, and such information may not necessarily be visible when projecting to the vocabulary space. Dao et al. [15] also point out such limitations of DLA.

In Table 4 we show the results of applying DLA to attention heads' output in IOI circuit. We can see that the expected information is not visible in most cases. The best result comes from the S-Inhibition head 8.10, with only 7.5% of cases where the expected name is in the top-30 tokens and there is no other name being suppressed more than it. The rare cases where the expected name is visible can also be explained by the small direct effect on the final output as depicted by Figure 3(b) in [54].

Therefore, researchers should be cautious when using DLA and should be aware of its limitations. A good usage example is the IOI circuit [54], where the authors first identify those attention heads directly affecting the final logits, and only apply DLA to them and design other experiments to interpret other components. Importantly, in the context of factual recall task, we find that the information given by InversionView about the upper layer attention heads is often visible via DLA, indicating these heads contribute to model's output directly. Thus, our results can serve as confirmation that prior results relying on DLA in this task are generally reliable.

### Methods Generating in Input Space

Feature visualization [43, 41] generates inputs maximally activating a certain neural network unit, and interprets an individual neural network unit (e.g., neurons) to understand its general role across inputs, while InversionView interprets specific values of inner representations, by finding inputs that result in the same vector. When the input changes, the value and thus the interpretation may change. Adversarial or counterfactual example generation methods [27, 59, 46, 45] generate input that is similar to the original input but results in different outcome. Some of them are also used to explain the model. While similar in input space, the adversarial/counterfactual input is likely to be quite different in internal representation space, leading to a different output. In contrast, we are interested in how different inputs in input space are represented very similarly in internal representation space.

Similar to InversionView, GAN inversion methods [58] also study the mapping between input space and representation space, with a focus on interpreting the semantics of GAN's latent space and manipulating generation.

## Appendix J Automated Interpretability

We further explore whether the process of obtaining the common information from a collection of inputs can be automated by LLMs. We use Claude 314[3] In preliminary experiments we also try GPT-4[1] but we find Claude 3 works better in our case. In the prompt given to Claude 3, we first describe the task it needs to perform, the terminology we are using (e.g., F1, F2, etc.), the rules (e.g., the pattern it finds should be applicable to each inputs in the \(\epsilon\)-preimage), input and output form, and the crucial steps it should follow. In addition, we also provide it with 3 demonstrating examples in conjunction with the correct answers. Each example corresponds to a specific activation site and token position (e.g., \(a^{0,0}\), A1). In each example, there are 2-3 specific query inputs, each query input is accompanied with 20 (sometimes less) samples that are _inside_ the \(\epsilon\)-preimage. Claude 3 needs to find the pattern for each query input, and summarize its findings across several query inputs, which is the information contained in general in that activation. In addition to the content described above (the common part shared between prompts), we give it a questioning example, which is the content we would like it to interpret. The questioning example shares the same form as the demonstrating example, except that it contains 5 query inputs and their corresponding samples. In addition, when two separate interpretations are needed based on different A1 value, we run the generation twice with examples of different A1 value, instead of giving the model a mixture of two cases and resorting to its own capacity.

Footnote 14: Version: Claude-3-opus-20240229

We think the following findings from our experiments are worth mentioning: 1) It's hard for the model to align digits of the same place (e.g., comparing all F1 digits), because the samples are presented as a single flattened string instead of a 2-dimensional table. We find that explicitly adding the variable name can largely mitigate this problem, they may serve as certain kind of keys. For example, "7(F1)

\begin{table}
\begin{tabular}{|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|p{56.9pt}|} \hline \multirow{2}{*}{Attn Head, position} & Info by InversionView (See Table 2) & Top 30 promoted rate & Top 30 promoted 1st name rate & Top 30 suppressed rate & Top 30 suppressed 1st name rate \\ \hline \hline
9.9, END & IO name & 99.5\% & 98.8\% & 0\% & 0\% \\ \hline \multirow{2}{*}{7.9, END} & S name; Position of... & \multirow{2}{*}{0\%} & \multirow{2}{*}{3.2\%} & \multirow{2}{*}{3.2\%} & \multirow{2}{*}{3.2\%} \\ \cline{2-2} \cline{5-6}  & S name; Position of... & \multirow{2}{*}{0\%} & \multirow{2}{*}{27.4\%} & \multirow{2}{*}{7.5\%} \\ \cline{2-2} \cline{5-6}  & S name; Position of... & \multirow{2}{*}{0\%} & \multirow{2}{*}{27.4\%} & \multirow{2}{*}{7.5\%} \\ \cline{2-2} \cline{5-6}  & S name; Position of... & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} \\ \cline{2-2} \cline{5-6}  & S name & & & & \\ \hline
0.1, S2 & S name & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} \\ \cline{2-2} \cline{5-6}  & S name & & & & \\ \hline
2.2, S1+1 & S name (most of time) & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} \\ \cline{2-2} \cline{5-6}  & S name; Position of... & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} & \multirow{2}{*}{0\%} \\ \hline \end{tabular}
\end{table}
Table 4: Applying DLA to the heads in IOI circuit. Except the first row, all heads do not directly connect to final output according to the IOI circuit, the results show DLA cannot decode their information. We do not include those heads in which only position information is encoded. “Top 30 promoted (suppressed) rate” means the fraction of input examples where the expected name (IO name for the first row, S name for other rows) is inside the top 30 tokens promoted (suppressed) by the head’s output. “Top 30 promoted (suppressed) & 1st name rate” means the expected name is not only inside the top 30 promoted (suppressed) tokens, but also the most promoted (suppressed) name among a list of common and single-token names, so it does not count when another name is ranked higher. Note that a name can be associated with two tokens (with and without a space before it), when calculating the rate, either of them satisfying the condition will count. The rate is calculated over 1000 random IOI examples. As we can see, except for the first row, the expected name is not observable most of the time.

1(F2) 1(F3) + 9(S1) 9(S2) 4(S3)". 2) The generated interpretation is sometimes not consistent. The model may generate different conclusion even with the same prompt, but this usually only happens to less important information. 3) The model does not strictly follow the rule, i.e., the common pattern should match _all_ inputs, even though we state this repeatedly in different ways in the prompt. The model will say "always" even when there is a counterexample. We should keep 2) and 3) in mind when reading the results.

We run the generation for each entry in Table 3 once, using the samples generated from the corresponding activation. The results are shown in Table 5, accompanied with human interpretation for comparison. The interpretation given by Claude 3 reflects the main information in almost all cases. Even when the information becomes more complex in layer 1, the interpretation quality does not significantly decline. This implies that automated interpretation by LLM is promising. On the other hand, we can also see there are some problems: 1) Some of the model's claims are spurious, these claims are usually ranked low by the model, indicating they are not very obvious. 2) The model sometimes does not explain in a desirable manner. For example, for the entry "\(x^{0,\mathrm{mid}}\), A3", the information includes A2 and whether A1=1, which means when A1=1, the sum of F1 and S1 is known. Thus Claude 3 concludes that (F1, S1) is approximately represented. Though this is somewhat true, it is not a fundamental piece of information, and there is a more concise summary. 3) The interpretation tends to be verbose, even though we ask the model to be succinct and provide it with some examples. Despite the problems, we think the automatically generated interpretation is by and large satisfactory and informative, and we think some of the problems can probably be solved or mitigated by engineering better prompts.

\begin{tabular}{|p{56.9pt}||p{56.9pt}|p{56.9pt}|} \hline \multicolumn{3}{|c|}{Start of Table 5} \\ \hline Entry in Table 3 & Interpretation from Table 3 & Interpretation by Claude 3 \\ \hline \hline \(a^{0,0}\), A1 & 1-2 digits from F1 and S1 & *_used as example in the prompt_ \\ \hline \(a^{0,1}\), A1 & 1-2 digits from F1 and S1 & F1 and S1 (sometimes exactly and sometimes approximately. Their order can sometimes vary) \\ \hline \(a^{0,2}\), \(A1\) & 1-2 digits from F1 and S1 & the digits in the hundreds place (F1 and S1): - They tend to be large single digits (7, 8, 9) that add up to 10 or more - Sometimes one or both are fixed to a certain single digit value - Sometimes their values can swap positions between F1 and S1 There are no strong consistent patterns observed for the digits in the tens or ones places (F2, S2, F3, S3). No other types of patterns (e.g. on A1 or carries) are consistently observed. \\ \hline \(a^{0,3}\), A1 & 1-2 digits from F2 and S2; C2 & F2 and S2 (fuzzy) \\ \hline \(x^{0,\mathrm{mid}}\), A1 & 1) F1 and S1; C2 & 1) F1 and S1 are either the same digit, or a pair of digits that always appear together in the hundreds place (sometimes with order swapped). 2) Whether F2+S2 produces a carry (the value of C2) is often consistent in each example, either always 0 or always 1. 3) There is no consistent pattern found for the ones digits F3 and S3. \\ \hline \(x^{0,\mathrm{post}}\), A1 & F1 and S1; C2 & 1) F1 and S1, either exactly or an approximate version, with order sometimes varying 2) S2, usually 3) C2, occasionally F3, S3 and other parts of the addition rarely contain any mutual information between the query and generated strings. \\ \hline \(a^{1,0}\), A1 & Fuzzy info about F1, S1 and C2; Fuzzy info about F3 and S3 (sometimes) & 1) F1 and S1 digits (sometimes exact match, sometimes approximate match). 2) Whether the sum of F2 and S2 is greater than or equal to 10 (i.e. the value of C2). 3) F3 and/or S3 digits are often preserved from the query to the generated strings. \\ \hline \end{tabular}

[MISSING_PAGE_FAIL:50]

[MISSING_PAGE_FAIL:51]

\begin{tabular}{|p{56.9pt}||p{34.1pt}|p{34.1pt}|} \hline \multicolumn{3}{|c|}{Continuation of Table 5} \\ \hline Entry in Table 3 & Interpretation from Table 3 \\ \hline \hline \multirow{6}{*}{\(x^{1,\mathrm{mid}},\mathrm{A2}\)} & \multirow{6}{*}{\begin{tabular}{c} Interpretation \\ from Table 3 \\ \end{tabular} } & \multicolumn{1}{p{34.1pt}}{Interpretation by Claude 3} \\ \cline{3-3}  & & **If** A1=1: 1. F1 and S1 are always the same two digits but their order can vary. 2. The sum of F2 and S2 is always the same, implying the carry C2 is consistent. 3. A1 is always the same digit "1". There is no consistent pattern found for F3 and S3. **Else**: 1. F1 and S1: their specific values in query often show up in generated strings in an approximate way, and their order can sometimes vary. 2. F2 and S2: the specific values or a small range of values are often shared. 3. F3 and S3: a fuzzy version is often shared, meaning the specific values may vary but are close to those in query. 4. A1: often exactly the same between query and generated strings. **If** A1=1: - F1 and S1 have two possible values that are swapped between F1 and S1. In other words, (F1, S1) have two possible combinations that are the reverse of each other - A1 is always the same in the query and all generated strings in each example. Sometimes the carry C2 (whether F2+S2\(\geq\)10) is also shared between query and generated strings. **Else**: 1) Specific digits are often fixed or highly common in certain places (e.g. F2, S2) across query and equations. 2) The answer digit A1 is often the same or highly consistent across query and equations. 3) The carry C2 from tens place is sometimes consistent (0 or 1) across examples. 4) Other than above, the digits and relationships seem to vary substantially between examples. \\ \hline \hline \multirow{6}{*}{\(a^{0,0}\), A3} & \multirow{6}{*}{whether A1=1} & 1. Digits in the answer part (A1, A2, etc.), either the exact digit or a range. This is observed in all 5 examples. 2. The range that F1 and S1 can take, and sometimes they can swap positions. This is observed in 2 out of 5 examples. Digits in F2, S2, F3, S3 do not show consistent patterns across examples. \\ \hline \multirow{6}{*}{\(a^{0,1}\), A3} & \multirow{6}{*}{\begin{tabular}{c} 1-2 digits from \\ F2 and S2 \\ \end{tabular} } & - When the sum is a 2-digit number, A1 is sometimes shared between query and generated strings - When the sum is a 3-digit number, A2 is often (but not always) shared - If there is a shared A2, its value often matches the query - If both A1 and A2 are present, A1A2 is between 10-19 There are no consistent patterns in the operand digits F1/S1, F2/S2, F3/S3 or their sums. \\ \hline \multirow{6}{*}{\(a^{0,2}\), A3} & \multirow{6}{*}{\begin{tabular}{c} 1-2 digits from \\ F2 and S2 \\ \end{tabular} } & 1) S2 or F2 sometimes takes on a consistent value (like 0, 9, 7) in the query and many generated strings. 2) The equations are often incomplete, with A1 and/or A2 missing in many generated strings. 3) When present, A2 sometimes has a consistent value (like 1, 2, 6) in the query and generated strings, but not always. 4) No consistent patterns were found for the (F1, S1) and (F3, S3) digit pairs. \\ \hline \multirow{6}{*}{\(a^{0,3}\), A3} & \multirow{6}{*}{\begin{tabular}{c} 1 digit from F3 and S3 (2 when \\ F3=S3); C3 \\ \end{tabular} } & 1) Certain digits from F3 and/or S3, although the order of F3 and S3 may vary in generated strings compared to the query. 2) The existence of A1 and A2 in complete equations, but not always. Other patterns like digits from F1, S1, F2, S2 are rarely shared. \\ \hline \multirow{6}{*}{\(x^{0,\mathrm{mid}},\mathrm{A3}\)} & \multirow{6}{*}{\begin{tabular}{c} A2; F2 and S2; \\ C3; whether \\ A1=1 \\ \end{tabular} } & 1. An exact or approximate representation of (F1, S1), sometimes with order varying 2. Specific values or range of values for F2 and S2, sometimes along with their sum (C2) 3. Sometimes specific values for F3 and/or S3 4. Often an exact or approximate representation of (A1, A2) The amount and type of information shared varies between examples. But in general, there are usually some constraints on the digits in each place value shared by the query and generated strings. \\ \hline \multirow{6}{*}{\(x^{0,\mathrm{post}}\), A3} & \multirow{6}{*}{
\begin{tabular}{c} A2; F2 and S2; \\ C3; whether \\ A1=1 \\ \end{tabular} } & 1. F1 and S1: exact match or fuzzy match, order of F1 and S1 may vary 2. F2 and S2: exact match or fuzzy match, order of F2 and S2 may vary, sometimes have size relationship (like F2 \(\leq\) S2) 3. F3: exact match or fuzzy match 4. S3: exact match or fuzzy match 5. A1: exact match 6. A2: exact match Not every example has all the above common information. \\ \hline \end{tabular}

\begin{tabular}{|p{56.9pt}||p{142.3pt}|p{142.3pt}|} \hline \multicolumn{3}{|c|}{Continuation of Table 5} \\ \hline Entry in Table 3 & Interpretation from Table 3 \\ \hline \hline \multirow{4}{*}{\(a^{1,0}\), A3} & **If** A1=1: 1-2 digits from F2 and S2; C3 (sometimes); **Else**: 1-2 digits from F3 and S3 & **If** A1=1: 1. F2 and/or S2 (sometimes their roles as F2 or S2 can vary) 2. A pattern about A1 and A2: when A1 is a specific value (usually "1"), A2 is provided; otherwise A2 is not provided 3. Sometimes A1 4. Rarely A2 when A1 is a specific value **Else**: - Fuzzy/approximate matching of F3 and S3 values - Exact matching of F3, S2, S3 values in some cases - Partial matching of A2 (last digit) in some cases Overall, ones place digits (F3, S3) and occasionally other digits (S2, A2) tend to be shared in a fuzzy or exact manner. Hundreds place digits (F1, S1, A1) are rarely shared. \\ \hline \multirow{4}{*}{\(a^{1,1}\), A3} & **If** A1=1: 1-2 digits from F2 and S2; C3 (sometimes); **Else**: 1-2 digits from F3 and S3 & **If** A1=1: F2 and S2 are always certain fixed digits in each example, shared by the query and all generated strings in that example. Sometimes the first answer digit A1 is also shared between query and some generated strings, but this is not fully consistent. The last answer digit shows no consistency between query and generated strings. **Else**: 1) S3 (sometimes F3) 2) Fuzzy representation of (A1, A2) in some cases The ones place digit (F3 or S3) tends to be exactly preserved, while the answer (A1, A2) is sometimes preserved approximately but not exactly. Other digits and the carry do not show consistent patterns. **If** A1=1: 1. F1 and S1 (order may vary), values may be fuzzy or exact match) 2. F2 (may be exact or fuzzy match) 3. S2 (may be exact or fuzzy match, may appear in F2 or S2) 4. A1 (always exact match) 5. A2 (may be exact match or not present) The amount of shared information varies across examples. F3, S3 are not shared. **Else**: Across the examples, the most common pattern is that either F3 or S3 has a fixed single digit value shared by the query and all generated strings in that example. The specific digit varies between examples but is consistent within each example. \\ \hline \multirow{4}{*}{\(a^{1,3}\), A3} & **If** A1=1: 1-2 digits from F2 and S2; C3 (sometimes); **Else**: 1-2 digits from F3 and S3 & **If** A1=1: 1. F1 and S1 (sometimes fuzzy values or ranges); F2 and S2 (sometimes unordered); F3 (rarely); A1; A2 **Else**: F3 and S3, sometimes only appearing in an approximate manner and sometimes their relative order can vary; A2, always precisely shared between the query string and generated strings. \\ \cline{1-1}  & **If** A1=1: 1. Approximate or exact values of F1 and S1, their order can vary 2. Exact values or a couple possible values of F2 and S2, their order can vary 3. Exact or approximate values of A1 and A2 The information about F3 and S3 does not seem to be shared. **Else**: - Some or all digits in F2, F3, S2, S3 (F2 and S2, F3 and S3 can be swapped) - The last digit of the answer (A2) - Whether there is a carry from tens place to hundreds place (C2) \\ \hline \hline \multirow{4}{*}{\(a^{0,0}\), A4/E} & **If** A1=1: 1. Approximate or exact values of F1 and S1, their order can vary 2. Exact values or a couple possible values of F2 and S2, their order can vary 3. Exact or approximate values of A1 and A2 The information about F3 and S3 does not seem to be shared. **Else**: - Some or all digits in F2, F3, S2, S3 (F2 and S2, F3 and S3 can be swapped) - The last digit of the answer (A2) - Whether there is a carry from tens place to hundreds place (C2) \\ \cline{1-1}  & **If** A1=1: 1. Approximate or exact values of F1 and S1, their order can vary 2. Exact values or a couple possible values of F2 and S2, their order can vary 3. Exact or approximate values of A1 and A2 The information about F3 and S3 does not seem to be shared. **Else**: - Some or all digits in F2, F3, S2, S3 (F2 and S2, F3 and S3 can be swapped) - The last digit of the answer (A2) - Whether there is a carry from tens place to hundreds place (C2) \\ \hline \hline \multirow{4}{*}{\(a^{0,0}\), A4/E} & **If** A1=1: 1. Approximate or exact values of F1 and S1, their order can vary 2. Exact values or a couple possible values of F2 and S2, their order can vary 3. Exact or approximate values of A1 and A2 The information about F3 and S3 does not seem to be shared. **Else**: - Some or all digits in F2, F3, S2, S3 (F2 and S2, F3 and S3 can be swapped) - The last digit of the answer (A2) - Whether there is a carry from tens place to hundreds place (C2) \\ \cline{1-1}  & **If** A1=1: 1. Approximate or exact values of F1 and S1, their order can vary 2. Exact values or a couple possible values of F2 and S2, their order can vary 3. Exact or approximate values of A1 and A2 The information about F3 and S3 does not seem to be shared. **Else**: - Some or all digits in F2, F3, S2, S3 (F2 and S2, F3 and S3 can be swapped) - The last digit of the answer (A2) - Whether there is a carry from tens place to hundreds place (C2) \\ \cline{1-1}  & **If** A1=1: 1. Approximate or exact values of F1 and S1, their order can vary 2. Exact values or a couple possible values of F2 and S2, their order can vary 3. Exact or approximate values of A1 and A2 The information about F3 and S3 does not seem to be shared. **Else**: - Some or all digits in F2, F3, S2, S3 (F2 and S2, F3 and S3 can be swapped) - The last digit of the answer (A2) - Whether there is a carry from tens place to hundreds place (C2) \\ \hline \hline \multirow{4}{*}{\(a^{0,0}\), A4/E} & **If** A1=1: 1. Approximate or exact values of F1 and S1, their order can vary 2. Exact values or a couple possible values of F2 and S2, their order can vary 3. Exact or approximate values of A1 and A2 The information about F3 and S3 does not seem to be shared. **Else**: - Some or all digits in F2, F3, S2, S3 (F2 and S2, F3 and S3 can be swapped) - The last digit of the answer (A2) - Whether there is a carry from tens place to hundreds place (C2) \\ \cline{1-1}  & **If** A1=1: 1. Approximate or exact values of F1 and S1, their order can vary 2. Exact values or a couple possible values of F2 and S2, their order can vary 3. Exact or approximate values of A1 and A2 The information about F3 and S3 does not seem to be shared. **Else**: - Some or all digits in F2, F3, S2, S3 (F2 and S2, F3 and S3 can be swapped) - The last digit of the answer (A2) - Whether there is a carry from tens place to hundreds place (C2) \\ \cline{1-1}  & **If** A1=1: 1.

## Appendix K Compute Resources

We ran all experiments on NVIDIA A100 cards. The decoder is trained for 4-6 hours on 2 GPUs for the first three case studies, without exhaustively tuning efficiency of the implementation, which we believe could further speed up training. Regarding the factual recall task, we train the decoder

\begin{table}
\begin{tabular}{|p{56.9pt}||p{142.3pt}|p{142.3pt}|} \hline \multicolumn{3}{|c|}{Continuation of Table 5} \\ \hline Entry in Table 3 & Interpretation from Table 3 & Interpretation by Claude 3 \\ \hline \hline \multirow{4}{*}{\(a^{0,1}\), A4/E} & \multirow{4}{*}{1-2 digits from F3 and S3} & Across the examples, the most consistent commonality is in the ones place (F3 and S3). They often have the same or very similar values in the query and generated strings. \\  & & Sometimes a digit in a certain place of the final answer is also shared between query and many/all generated strings, like the first digit (A1) or last digit (A3). But this is not always the case. \\  & & The hundreds place (F1/S1) and tens place (F2/S2) digits usually vary without a clear mutual pattern between the query and generated strings. \\ \hline \multirow{4}{*}{\(a^{0,2}\), A4/E} & 1-2 digits from F3 and S3 & 1) F3 and S3 digits (sometimes approximately). Present in 4 out of 5 examples. 2) A1 and/or A2 digits (only in example 0). \\ \hline \multirow{4}{*}{\(a^{0,3}\), A4/E} & \multirow{4}{*}{1-2 digits from F3 and S3} & - Certain digit positions (often F3 or S3) having the same value in the query and all/most generated strings - When sums are shown, certain digits (often A1 or A3) being restricted to a small set of values or range The specific digit positions and values/ranges vary between examples, but the general pattern of certain positions being fixed or constrained is consistent. \\ \hline \multirow{4}{*}{\(x^{0,\mathrm{mid}}\), A4/E} & \multirow{4}{*}{A3; F3 and S3; whether A1=1} & - F1 and S1 sometimes appear in generated strings with their order swapped - F3 and S3 sometimes appear in generated strings with their order swapped - F3, S3 and A3 of the query string often appear unchanged in the same position in generated strings \\ \hline \multirow{4}{*}{\(x^{0,\mathrm{post}}\), A4/E} & \multirow{4}{*}{A3; F3 and S3; whether A1=1} & - F3 is often shared between the query string and generated strings - S3 is sometimes shared between the query string and generated strings - A3 is consistently the same value in the query string and all generated strings for each example There is no strong pattern for F1, S1, F2 or S2. \\ \hline \multirow{4}{*}{\(a^{1,0}\), A4/E} & 1-2 digits from F3 and S3; whether next token is E & 1. Specific digits or narrow ranges in F3 and S3 2. The value or a narrow range of A1 3. The ending digits or a narrow range of ending digits in A3 4. Sometimes the carry C1 or C2 The digits in F1, S1, F2, S2 and the full value of A2, A3 are usually not shared. \\ \hline \multirow{4}{*}{\(a^{1,1}\), A4/E} & 1-2 digits from F3 and S3; whether next token is E & 1) F3 (always the same or frequently appears) 2) S3 (always the same or frequently appears) 3) One or more answer digits, especially the last one A3 (always the same or frequently appears) \\ \hline \multirow{4}{*}{\(a^{1,2}\), A4/E} & \multirow{4}{*}{1-2 digits from F3 and S3} & The ones digit (F3) of the query string consistently shows up in the ones place (either F3 or S3) of the generated strings. Sometimes the other ones digit in generated strings has a specific value when query F3 is in a certain place. Occasionally, the carry (C3) from the ones place addition is also shared between the query and generated strings. \\ \hline \multirow{4}{*}{\(a^{1,3}\), A4/E} & 1-2 digits from F3 and S3 & only the ones digits (F3 and sometimes S3) are consistently shared, while the other parts of the addition problems vary between the query and generated strings in each example. \\ \hline \multirow{4}{*}{\(x^{1,\mathrm{mid}}\), A4/E} & \multirow{4}{*}{A3; F3 and S3; whether A1=1} & - F3 and S3 have some fixed values (varying by example) that always sum to the same total. The order of the two digits doesn’t matter. - As a result, A3 is always a fixed value for each example. - There is sometimes a consistent carry over amount from the tens to hundreds place, resulting in a fixed A1 value. \\ \hline \multirow{4}{*}{\(x^{1,\mathrm{post}}\), A4/E} & \multirow{4}{*}{F3 and S3; whether A1=1} & - Specific digits in the answer (A3) - Specific digits in the operands (F3, S3) - Whether there is a carry in a certain place (C2, C3) The shared information varies across examples, but usually relates to the ones or tens place digits and carries. \\ \hline \end{tabular}
\end{table}
Table 5: Interpretation for 3 digit addition produced by Claude 3, compared with human interpretation from Table 3. In general, the automated information is very informative, and the human interpretations 3 is contained in almost all cases, though the output tends to be more verbose. The LLM outputs, with some human post-checking, can thus further speed up interpretation.

for less than 1 day on 4 GPUs. Generation of \(\epsilon\)-preimage samples (including forward passes on the probed model to calculate distance metric) is fast for the first 3 tasks, and it takes around 9 hours on 4 GPUs for factual recall task (for 200 query inputs). Patching experiments run quickly, as they are done for small models.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We ensured that all claims made in abstract and introduction are well supported by our results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have a "Limitations" paragraph in the Discussion section, where we discuss limitations of this work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: The paper includes no theorems or other non-empirical claims. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Besides explaining all experimental details, we also provide the code and the probed models. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have attached the code with the submission. No new data were created. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify all relevant details, and also include the code for reproducibility, Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: All quantitative results have sample sizes of hundreds of thousands of data points, and error bars would be very small, thus not add substantial information. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We include an appendix section on Compute Resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is foundational research on neural networks, and we anticipate no societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not see any risk of misuse of our research results. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use the GPT-2 Small model and IOI templates, both of which we cite. We use no other existing models or data. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We document the new assets, which are models of character counting and addition. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper involves no crowdsourcing or human subjects experiments. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper involves no crowdsourcing or human subjects experiments. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.