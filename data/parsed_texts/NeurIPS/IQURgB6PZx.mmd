# RetroBridge: Modeling Retrosynthesis with Markov Bridges

 Ilia Igashov

Ecole Polytechnique Federale de Lausanne

ilia.igashov@epfl.ch

&Arne Schneuing

Ecole Polytechnique Federale de Lausanne

arne.schneuing@epfl.ch

&Marwin Segler

Microsoft Research

marwinsegler@microsoft.com

&Michael Bronstein

University of Oxford

michael.bronstein@cs.ox.ac.uk

These authors contributed equally

Bruno Correia

Ecole Polytechnique Federale de Lausanne

bruno.correia@epfl.ch

###### Abstract

Retrosynthesis planning is a fundamental challenge in chemistry which aims at designing multi-step reaction pathways from commercially available starting materials to a target molecule. Each step in multi-step retrosynthesis planning requires accurate prediction of possible precursor molecules given the target molecule and confidence estimates to guide heuristic search algorithms. We model single-step retrosynthesis as a distribution learning problem in a discrete state space. First, we introduce the Markov Bridge Model, a generative framework aimed to approximate the dependency between two intractable discrete distributions accessible via a finite sample of coupled data points. Our framework is based on the concept of a Markov bridge, a Markov process pinned at its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does not need a tractable noise distribution as a sampling proxy and directly operates on the input product molecules as samples from the intractable prior distribution. We then address the retrosynthesis planning problem with our novel framework and introduce RetroBridge, a template-free retrosynthesis modeling approach that achieves state-of-the-art results on standard evaluation benchmarks.

## 1 Introduction

Computational and machine learning methods for _de novo_ drug design show great promise as more cost-effective alternatives to experimental high-throughput screening approaches [14] to propose molecules with desirable properties. While _in silico_ results suggest high predicted target binding affinities and other favorable properties of the generated molecules, limited emphasis has so far been placed on their synthesizability [12]. For laboratory testing, synthetic pathways need to be developed for the newly designed molecules, which is an extremely challenging and time-consuming task.

Retrosynthesis planning [13, 14, 15] tools address this challenge by proposing reaction steps or entire pathways that can be validated and optimized in thelab. Single-step retrosynthesis models predict precursor molecules for a given target molecule (Segler and Waller, 2017; Coley et al., 2017; Liu et al., 2017; Strieth-Kalthoff et al., 2020; Tu et al., 2023). Applying these methods recursively allows to decompose the initial molecule in progressively simpler intermediates and eventually reach available starting molecules (Segler et al., 2018).

While most works have used a discriminative formulation for retrosynthesis modeling (Strieth-Kalthoff et al., 2020; Tu et al., 2023; Jiang et al., 2022), we propose to view the task as a conditional distribution learning problem, as shown in Figure 1. This approach has several advantages, including the ability to model uncertainty and to generate new and diverse retrosynthetic pathways. Furthermore, and most importantly, the probabilistic formulation reflects the fact that the same product molecule can often be synthesized with different sets of reactants and reagents.

Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) and other modern score-based and flow-based generative methods (Rezende and Mohamed, 2015; Song et al., 2020; Lipman et al., 2022; Albergo and Vanden-Eijnden, 2022; Albergo et al., 2023) may seem like good candidates for retrosynthesis modeling. However, as we show in this work, such models do not fit naturally to the formulation of the problem, as they are designed to approximate a single intractable data distribution. To do so, one typically samples initial noise from a simple prior distribution and then maps it to a data point that follows a complex target distribution. In contrast, we aim to learn the dependency between two intractable distributions rather than one intractable distribution itself. While this can be achieved by conditioning the sampling process on the relevant context and keep sampling from the prior noise, we show here that such use of the original unconditional generative idea is suboptimal for approximating the dependency between two discrete distributions.

In this work, we propose RetroBridge, a template-free probabilistic method for single-step retrosynthesis modeling. As shown in Figure 1, we model the dependency between the spaces of products and reactants as a stochastic process that is constrained to start and to end at specific data points. To this end, we introduce the Markov Bridge Model, a generative model that learns the dependency between two intractable discrete distributions through the finite sample of coupled data points. Taking a product molecule as input, our method models the trajectories of Markov bridges starting at the given product and ending at data points following the distribution of reactants. To score reactant graphs sampled in this way, we leverage the probabilistic nature of RetroBridge and measure its uncertainty at each sample. We demonstrate that RetroBridge achieves competitive results on standard retrosynthesis modeling benchmarks. Besides, we compare RetroBridge with the state-of-the-art graph diffusion model DiGress (Vignac et al., 2022), and demonstrate quantitatively and qualitatively that the proposed Markov Bridge Model is better suited to tasks where two intractable discrete distributions need to be mapped.

To summarise, the main contributions of this work are the following:

Figure 1: Markov bridges between the distribution of products and distribution of reactants.

* We introduce the Markov Bridge Model to approximate the probabilistic dependency between two intractable discrete distributions accessible via a finite sample of coupled data points.
* We demonstrate the superiority of the proposed formulation over diffusion models in the context of learning the dependency between two intractable discrete distributions.
* We propose RetroBridge, the first Markov Bridge Model for retrosynthesis modeling. RetroBridge is a template-free single-step retrosynthesis prediction method that achieves state-of-the-art results on standard benchmarks.

## 2 Related Work

Diffusion ModelsDiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) form a class of powerful and effective score-based generative methods that have recently achieved promising results in many different domains including protein design (Watson et al., 2023), small molecule generation (Hoogeboom et al., 2022; Igashov et al., 2022; Schneuing et al., 2022), molecular docking (Corso et al., 2022), and sampling of transition state molecular structures (Duan et al., 2023; Kim et al., 2023). While most models are designed for the continuous data domain, a few methods were proposed to operate on discrete data (Hoogeboom et al., 2021; Johnson et al., 2021; Austin et al., 2021; Yang et al., 2023) and, in particular, on discrete graphs (Vignac et al., 2022). To the best of our knowledge, however, no diffusion models have been applied to modeling chemical reactions and recovering retrosynthetic pathways.

Schrodinger Bridge ProblemGiven two distributions and a reference stochastic process between them, solving the Schrodinger bridge (SB) problem (Schrodinger, 1932; Leonard, 2013) amounts to finding a process closest to the reference in terms of Kullback-Leibler divergence on path spaces. While most recent methods employ the SB formalism in the context of unconditional generative modeling (Vargas et al., 2021; Wang et al., 2021; De Bortoli et al., 2021; Chen et al., 2021; Bunne et al., 2023; Liu et al., 2022), a few works aimed to approximate the reference stochastic process through training on coupled samples from two continuous distributions (Holdijk et al., 2022; Somnath et al., 2023). To the best of our knowledge, there are no methods operating on categorical distributions, which is the subject of the present work.

Retrosynthesis ModelingRecent retrosynthesis prediction methods can be divided into two main groups: template-based and template-free methods (Jiang et al., 2022). While template-based methods depend on predefined sets of specific reaction templates or leaving groups, template-free methods are less restricted and therefore are able to explore new reaction pathways. Two common data representations used for retrosynthesis prediction are symbolic representations SMILES (Weininger, 1988) and molecular graphs. A variety of language models have been recently proposed (Liu et al., 2017; Zheng et al., 2019; Tetko et al., 2020) to operate on SMILES. Due to the nature of the sequence-to-sequence translation problem, all these methods are template-free. Among the existing graph-based methods (Segler and Waller, 2017), the most recent template-based ones are GLN (Dai et al., 2019), GraphRetro (Somnath et al., 2021) and LocalRetro (Chen and Jung, 2021), and template-free approaches are G2G (Shi et al., 2020) and MEGAN (Sacha et al., 2021). In this work, we propose a novel template-free graph-based method.

## 3 RetroBridge

We frame the retrosynthesis prediction task as a generative problem of modeling a stochastic process between two discrete-valued distributions of products \(p_{\mathcal{X}}\) and reactants \(p_{\mathcal{Y}}\). These distributions are intractable and are represented by a finite collection of \(D\) coupled samples \(\{(\bm{x}_{i},\bm{y}_{i})\}_{i=1}^{D}\), where \(\bm{x}_{i}\sim p_{\mathcal{X}}(\bm{x}_{i})\) is a product molecule and \(\bm{y}_{i}\sim p_{\mathcal{Y}}(\bm{y}_{i})\) is a corresponding set of reactant molecules. While products and reactants follow distributions \(p_{\mathcal{X}}\) and \(p_{\mathcal{Y}}\) respectively, there is a dependency between these variables that can be expressed in the form of the joint distribution \(p_{\mathcal{X},\mathcal{Y}}\) such that \(\int p_{\mathcal{X},\mathcal{Y}}(\bm{x},\bm{y})d\bm{x}=p_{\mathcal{Y}}(\bm{y})\) and \(\int p_{\mathcal{X},\mathcal{Y}}(\bm{x},\bm{y})d\bm{y}=p_{\mathcal{X}}(\bm{x})\). The joint distribution \(p_{\mathcal{X},\mathcal{Y}}\) is also intractable and accessible only through the discrete sample of coupled data points \(\{(\bm{x}_{i},\bm{y}_{i})\}_{i=1}^{D}\).

First, we introduce the Markov Bridge Model, a general framework for learning the dependency between two intractable discrete-valued distributions. Next, we discuss a special case where random variables are molecular graphs. Upon this formulation, we introduce RetroBride, a Markov Bridge Model for single-step retrosynthesis modeling. Finally, we explain a simple but rather effective way of scoring RetroBridge samples based on the statistical uncertainty of the model.

### Markov Bridge Model

We model the dependency between two discrete spaces \(\mathcal{X}\) and \(\mathcal{Y}\) by a Markov bridge (Fitzsimmons et al., 1992; Cetin and Danilova, 2016), which is a Markov process pinned to specific data points in the beginning and in the end. For a pair of samples \((\bm{x},\bm{y})\sim p_{\mathcal{X},\mathcal{Y}}(\bm{x},\bm{y})\) and a sequence of time steps \(t=0,1,\ldots,T\), we define the corresponding Markov bridge as a sequence of random variables \((\bm{z}_{t})_{t=0}^{T}\), that starts at \(\bm{x}\), i.e., \(\bm{z}_{0}=\bm{x}\), and satisfies the Markov property,

\[p(\bm{z}_{t}|\bm{z}_{0},\bm{z}_{1},\ldots,\bm{z}_{t-1},\bm{y})=p(\bm{z}_{t}|\bm {z}_{t-1},\bm{y}).\] (1)

To pin the process at the data point \(\bm{y}\), we introduce an additional requirement,

\[p(\bm{z}_{T}=\bm{y}|\bm{z}_{T-1},\bm{y})=1.\] (2)

Assuming that both distributions \(p_{\mathcal{X}}\) and \(p_{\mathcal{Y}}\) are categorical with a finite sample space \(\{1,\ldots,K\}\), we can represent data points as \(K\)-dimensional one-hot vectors: \(\bm{x},\bm{y},\bm{z}_{t}\in\mathbb{R}^{K}\). To model a Markov bridge defined by equations (1-2), similar to Austin et al. (2021), we introduce a sequence of transition matrices \(\bm{Q}_{0},\bm{Q}_{1},\ldots,\bm{Q}_{T-1}\in\mathbb{R}^{K\times K}\), defined as

\[\bm{Q}_{t}\coloneqq\bm{Q}_{t}(\bm{y})=\alpha_{t}\bm{I}_{K}+(1-\alpha_{t})\bm{ y}\bm{1}_{K}^{\top},\] (3)

where \(\bm{I}_{K}\) is a \(K\times K\) identity matrix, \(\bm{1}_{K}\) is a \(K\)-dimensional all-one vector, and \(\alpha_{t}\) is a schedule parameter transitioning from \(\alpha_{0}=1\) to \(\alpha_{T-1}=0\). Transition probabilities (1) can be written as follows,

\[p(\bm{z}_{t+1}|\bm{z}_{t},\bm{y})=\text{Cat}\left(\bm{z}_{t+1};\bm{Q}_{t}\bm{ z}_{t}\right),\] (4)

where \(\text{Cat}(\cdot\ ;\bm{p})\) is a categorical distribution with probabilities given by \(\bm{p}\). We note that setting \(\alpha_{T-1}=0\) ensures the requirement (2).

Using the finite set of coupled samples \(\{(\bm{x}_{i},\bm{y}_{i})\}_{i=1}^{D}\sim p_{\mathcal{X},\mathcal{Y}}\), our goal is to learn a Markov bridge (1-2) to be able to sample \(\bm{y}\) when only \(\bm{x}\) is available. To do this, we replace \(\bm{y}\) with an approximation \(\hat{\bm{y}}\) computed with a neural network \(\varphi_{\theta}\):

\[\hat{\bm{y}}=\varphi_{\theta}(\bm{z}_{t},t),\] (5)

and define an approximated transition kernel,

\[q_{\theta}(\bm{z}_{t+1}|\bm{z}_{t})=\text{Cat}\left(\bm{z}_{t+1};\bm{Q}_{t}( \hat{\bm{y}})\bm{z}_{t}\right).\] (6)

Figure 2: The process of changing atom types along the trajectory of the Markov bridge. The trajectory starts at time step \(t=0\) with the product molecule and several disconnected “dummy” atoms that will be included in the final reactant molecule. The probability of sampling the target atom type increases as \(t\) grows. Five circles filled with different colors represent these probabilities. To make the illustration less bulky, we omitted a part of the product molecule and one of two reactant molecules.

We train \(\varphi_{\theta}\) by maximizing a lower bound of log-likelihood \(\log q_{\theta}(\bm{y}|\bm{x})\). As shown in Appendix A.1, it has the following closed-form expression,

\[\log q_{\theta}(\bm{y}|\bm{x})\geq-T\cdot\mathbb{E}_{t\sim\mathcal{U}(0,...,T-1 )}\mathbb{E}_{\bm{z}_{t}\sim p(\bm{z}_{t}|\bm{x},\bm{y})}D_{\text{KL}}\left(p( \bm{z}_{t+1}|\bm{z}_{t},\bm{y})\|q_{\theta}(\bm{z}_{t+1}|\bm{z}_{t})\right).\] (7)

For any \(\bm{x}\in\mathcal{X},\bm{y}\in\mathcal{Y}\), and \(t=1,\dots,T\), sampling of \(\bm{z}_{t}\) can be effectively performed using a cumulative product matrix \(\overline{\bm{Q}}_{t}=\bm{Q}_{i}\bm{Q}_{t-1}...\bm{Q}_{0}\). As shown in Appendix A.2, the cumulative matrix \(\overline{\bm{Q}}_{t}\) can be written in closed form,

\[\overline{\bm{Q}}_{t}=\overline{\alpha}_{t}\bm{I}_{K}+(1-\overline{\alpha}_{t} )\bm{y}\bm{1}_{K}^{\top},\] (8)

where \(\overline{\alpha}_{t}=\prod_{s=0}^{t}\alpha_{s}\). Therefore, \(p(\bm{z}_{t+1}|\bm{z}_{0},\bm{z}_{T})\) can be written as follows,

\[p(\bm{z}_{t+1}|\bm{z}_{0},\bm{z}_{T})=\text{Cat}\left(\bm{z}_{t+1};\overline{ \bm{Q}}_{t}\bm{z}_{0}\right).\] (9)

To sample a data point \(\bm{y}\equiv\bm{z}_{T}\) starting from a given \(\bm{z}_{0}\equiv\bm{x}\sim p_{\mathcal{X}}(\bm{x})\), one iteratively predicts \(\hat{\bm{y}}=\varphi_{\theta}(\bm{z}_{t},t)\) and then derives \(\bm{z}_{t+1}\sim q_{\theta}(\bm{z}_{t+1}|\bm{z}_{t})=\text{Cat}\left(\bm{z}_{t+ 1};\bm{Q}_{t}(\hat{\bm{y}})\bm{z}_{t}\right)\) for \(t=0,\dots,T-1\). Training and sampling procedures of the Markov Bridge Model are provided in Algorithms 1 and 2 respectively.

``` Input: coupled sample \((\bm{x},\bm{y})\sim p_{\mathcal{X},\mathcal{Y}}\), neural network \(\varphi_{\theta}\) \(t\sim\mathcal{U}(0,\dots,T-1),~{}\bm{z}_{t}\sim\text{Cat}\left(\bm{z}_{t}; \overline{\bm{Q}}_{t-1}\bm{x}\right)\)\(\triangleright\) Sample time step and intermediate state \(\hat{\bm{y}}\leftarrow\varphi_{\theta}(\bm{z}_{t},t)\)\(\triangleright\) Output of \(\varphi_{\theta}\) is a vector of probabilities \(p(\bm{z}_{t+1}|\bm{z}_{t},\bm{y})\leftarrow\text{Cat}\left(\bm{z}_{t+1};\bm{Q}_{ t}(\bm{y})\bm{z}_{t}\right)\)\(\triangleright\) Reference transition distribution \(q_{\theta}(\bm{z}_{t+1}|\bm{z}_{t})\leftarrow\text{Cat}\left(\bm{z}_{t+1};\bm{Q}_{t}(\hat{\bm{y}}) \bm{z}_{t}\right)\)\(\triangleright\) Approximated transition distribution \(\bm{z}_{t+1}\sim q_{\theta}(\bm{z}_{t+1}|\bm{z}_{t})\) Return \(\bm{z}_{T}\) ```

**Algorithm 1** Training of the Markov Bridge Model

### RetroBridge: Markov Bridge Model for Retrosynthesis Planning

In our setup, each data point is a molecular graph with nodes representing atoms and edges corresponding to covalent bonds. We represent the molecular graph with a matrix of node features \(\bm{H}\in\mathbb{R}^{N\times K_{i}}\) which are, for instance, one-hot encoded atom types, and a tensor of edge features \(\bm{E}\in\mathbb{R}^{N\times N\times K_{e}}\), which can be one-hot encoded bond types.

In the scope of our probabilistic framework, we consider such a molecular graph representation as a collection of independent categorical random variables. More formally, we denote product and reactants data points \(\bm{x}\) and \(\bm{y}\) as tuples of the corresponding node and edge feature tensors: \(\bm{x}=[\bm{H}_{x},\bm{E}_{x}]\) and \(\bm{y}=[\bm{H}_{y},\bm{E}_{y}]\). For such complex data points, we modify the definitions of transition matrices and probabilities accordingly:

\[[\bm{Q}_{t}^{H}]_{j}=\alpha_{t}\bm{I}_{K_{t}}+(1-\alpha_{t})\bm{1} _{K_{t}}[\bm{H}_{y}]_{j},\qquad\ p(\bm{H}_{t+1}|\bm{H}_{t},\bm{H}_{y})=\text{Cat }\left(\bm{H}_{t+1};\bm{H}_{t}\bm{Q}_{t}^{H}\right),\] (10) \[[\bm{Q}_{t}^{E}]_{j,k}=\alpha_{t}\bm{I}_{K_{t}}+(1-\alpha_{t}) \bm{1}_{K_{e}}[\bm{E}_{y}]_{j,k},\quad\ p(\bm{E}_{t+1}|\bm{E}_{t},\bm{E}_{y})= \text{Cat}\left(\bm{E}_{t+1};\bm{E}_{t}\bm{Q}_{t}^{E}\right),\] (11)

where \([\bm{H}]_{j}\in\mathbb{R}^{1\times K_{i}}\) is the \(j\)-th row of the feature matrix \(\bm{H}\) (i.e. transposed feature vector of the \(j\)-th node), and \([\bm{E}]_{j,k}\in\mathbb{R}^{1\times K_{e}}\) is the transposed feature vector of the edge between \(j\)-th and \(k\)-th nodes.

Because some atoms present in the reactant molecules can be absent in the corresponding product molecule, we add "dummy" nodes to the initial graph of the product. As shown in Figure 2, some "dummy" nodes are transformed into atoms of reactant molecules. In our experiments, we always add 10 "dummy" nodes to the initial product graphs.

### Confidence and Scoring

It is important to have a reliable scoring method that selects the most relevant sets of reactants out of all generated samples. In order to rank RetroBridge samples, we benefit from the probabilistic nature of the model and utilize its confidence in the generated samples as a scoring function. We estimate the confidence of the model by computing the likelihood \(q_{\theta}(\bm{y}|\bm{x})\) of a set of reactants \(\bm{y}\) for an input product molecule \(\bm{x}\). For a set of \(M\) samples \(\{\bm{z}_{T}^{(i)}\}_{i=1}^{M}\) generated by RetroBridge for an input product \(\bm{x}\), we compute the likelihood-based confidence score for the set of reactants \(\bm{y}\) as follows,

\[q_{\theta}(\bm{y}|\bm{x})=\mathbb{E}_{\bm{y}^{\prime}\sim q_{\theta}(\cdot|\bm {x})}\mathbbm{1}\{\bm{y}^{\prime}=\bm{y}\}\approx\frac{1}{M}\sum_{i=1}^{M} \mathbbm{1}\{\bm{z}_{T}^{(i)}=\bm{y}\}.\] (12)

## 4 Results

### Experimental Setup

DatasetFor all the experiments we use the USPTO-50k dataset (Schneider et al., 2016) which includes 50k reactions found in the US patent literature. We use standard train/validation/test splits provided by Dai et al. (2019). Somnath et al. (2021) report that the dataset contains a shortcut in that the product atom with atom-mapping 1 is part of the edit in almost 75% of the cases. Even though our model does not depend on the order of graph nodes, we utilize the dataset version with canonical SMILES provided by Somnath et al. (2021). Besides, we randomly permute graph nodes once SMILES are read and converted to graphs.

BaselinesWe compare RetroBridge with template-based methods GLN (Dai et al., 2019), LocalRetro (Chen and Jung, 2021), and GraphRetro (Somnath et al., 2021), and template-free methods MEGAN (Sacha et al., 2021), G2G (Shi et al., 2020), Augmented Transformer Tetko et al. (2020) and SCROP (Zheng et al., 2019). We note that SCROP, G2G and Augmented Transformer were trained and evaluated using different data splits provided by Liu et al. (2017). We obtained GLN predictions using the publicly available code and model weights2 and used the latest LocalRetro predictions provided by its authors. Additionally, MEGAN was originally trained and evaluated on random data splits, so we retrained and evaluated it ourselves. Finally, as described in Appendix A.4, we compare RetroBridge with the state-of-the-art discrete graph diffusion model DiGress Vignac et al. (2022) and a template-free baseline based on a graph transformer architecture (Dwivedi and Bresson, 2020; Vignac et al., 2022).

Footnote 2: Note that the top-\(k\) exact match accuracies differ from the one originally reported values because we deduplicate outputs for our evaluation.

EvaluationFor each input product, we sample 100 reactant sets and report top-\(k\) exact match accuracy (\(k=1,3,5,10\)) which is measured as the proportion of input products for which the method managed to produce the correct set of reactants in its top-\(k\) samples. Subsequently, for top-\(k\) samples produced for every input product, we run the forward reaction prediction model Molecular Transformer (Schwaller et al., 2019) and report round-trip accuracy and coverage (Schwaller et al., 2020). Round-trip accuracy is the percentage of correctly predicted reactants among all predictions, where reactants are considered correct either if they match the ground truth or if they lead back to the input product. Round-trip coverage, on the other hand, measures if there is at least one correct prediction in the top-\(k\) according to the definition above. These metrics reflect the fact that one product can be mapped to multiple different valid sets of reactants, as shown in Figure 1.

### Neural Network

We use a graph transformer network (Dwivedi and Bresson, 2020; Vignac et al., 2022) to approximate the final state of the Markov bridge process. We represent molecules as fully-connected graphs where node features are one-hot encoded atom types (sixteen atom types and additional "dummy" type) and edge features are covalent bond types (three bond types and additional "none" type). Similarly to Vignac et al. (2022) we compute several graph-related node and global features that include number of cycles and spectral graph features. Details on the network architecture, hyperparameters and training process are provided in Appendix A.3.

[MISSING_PAGE_FAIL:7]

(which are not correct) have scores 0.18 and 0.38 respectively (cf. 0.17 and 0.2 for the correct ones). More examples are provided in Figure 5.

## 5 Conclusion

In this work, we introduce the Markov Bridge Model, a new generative framework for tasks that involve learning dependencies between two intractable discrete-valued distributions. We furthermore apply the new methodology to the retrosynthesis prediction problem, which is an important challenge in medicinal chemistry and drug discovery. Our template-free method, RetroBridge, achieves state-of-the-art results on common evaluation benchmarks. Importantly, our experiments show that choosing a suitable probabilistic modeling framework positively affects the performance on this task compared to the straightforward adaptation of diffusion models.

While this work is focused on the retrosynthesis modeling task, we note that application of Markov Bridge Models is not limited to this problem. The proposed framework can be used in many other settings where two discrete distributions accessible via a finite sample of coupled data points need to be mapped. Such applications include but are not limited to image-to-image translation, inpainting, text translation and design of protein binders. We leave exploration of Markov Bridge Models in the scope of these and other possible challenges for future work.

#### Acknowledgments

We thank Max Welling, Philippe Schwaller, Rebecca Neeser, Clement Vignac and Anar Rzayev for helpful feedback and insightful discussions. Ilia Igashov has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 945363. Arne Schneuing is supported by Microsoft Research AI4Science.

## References

* Albergo and Vanden-Eijnden (2022) Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. _arXiv preprint arXiv:2209.15571_, 2022.
* Albergo et al. (2023) Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023.
* Austin et al. (2021) Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* Bunne et al. (2023) Charlotte Bunne, Ya-Ping Hsieh, Marco Cuturi, and Andreas Krause. The schrodinger bridge between gaussian measures has a closed form. In _International Conference on Artificial Intelligence and Statistics_, pages 5802-5833. PMLR, 2023.

Figure 3: Examples of modeled reactants. We selected three random inputs from the test set and for each of them we provide the top-3 RetroBridge predictions along with their confidence scores. Two check marks indicate that sampled reactants are the same as the ground truth, and one check mark means that reactants are different, but Molecular Transformer (Schwaller et al., 2019) predicts the product molecule used as input.

Umut Cetin and Albina Danilova. Markov bridges: Sde representation. _Stochastic Processes and their Applications_, 126(3):651-679, 2016.
* Chen and Jung (2021) Shuan Chen and Yousung Jung. Deep retrosynthetic reaction prediction using local reactivity and global attention. _JACS Au_, 1(10):1612-1620, 2021.
* Chen et al. (2021) Tianrong Chen, Guan-Horng Liu, and Evangelos A Theodorou. Likelihood training of schr\(\backslash\)" odinger bridge using forward-backward sdes theory. _arXiv preprint arXiv:2110.11291_, 2021.
* Chen et al. (2020) Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures? _Advances in neural information processing systems_, 33:10383-10395, 2020.
* Coley et al. (2017) Connor W Coley, Luke Rogers, William H Green, and Klavs F Jensen. Computer-assisted retrosynthesis based on molecular similarity. _ACS central science_, 3(12):1237-1245, 2017.
* Corey (1991) Elias James Corey. _The logic of chemical synthesis_. 1991.
* Corso et al. (2022) Gabriele Corso, Hannes Stark, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock: Diffusion steps, twists, and turns for molecular docking. _arXiv preprint arXiv:2210.01776_, 2022.
* Dai et al. (2019) Hanjun Dai, Chengtao Li, Connor Coley, Bo Dai, and Le Song. Retrosynthesis prediction with conditional graph logic network. _Advances in Neural Information Processing Systems_, 32, 2019.
* De Bortoli et al. (2021) Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709, 2021.
* Duan et al. (2023) Chenru Duan, Yuanqi Du, Haojun Jia, and Heather J Kulik. Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model. _arXiv preprint arXiv:2304.06174_, 2023.
* Dwivedi and Bresson (2020) Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. _arXiv preprint arXiv:2012.09699_, 2020.
* Fitzsimmons et al. (1992) Pat Fitzsimmons, Jim Pitman, and Marc Yor. Markovian bridges: construction, palm interpretation, and splicing. In _Seminar on Stochastic Processes, 1992_, pages 101-134. Springer, 1992.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Holdijk et al. (2022) Lars Holdijk, Yuanqi Du, Ferry Hooft, Priyank Jaini, Bernd Ensing, and Max Welling. Path integral stochastic optimal control for sampling transition paths. _arXiv preprint arXiv:2207.02149_, 2022.
* Hoogeboom et al. (2021) Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. _Advances in Neural Information Processing Systems_, 34:12454-12465, 2021.
* Hoogeboom et al. (2022) Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In _International conference on machine learning_, pages 8867-8887. PMLR, 2022.
* Igashov et al. (2022) Ilia Igashov, Hannes Stark, Clement Vignac, Victor Garcia Satorras, Pascal Frossard, Max Welling, Michael Bronstein, and Bruno Correia. Equivariant 3d-conditional diffusion models for molecular linker design. _arXiv preprint arXiv:2210.05274_, 2022.
* Jiang et al. (2022) Yinjie Jiang, Yemin Yu, Ming Kong, Yu Mei, Luotian Yuan, Zhengxing Huang, Kun Kuang, Zhihua Wang, Huaxiu Yao, James Zou, et al. Artificial intelligence for retrosynthesis prediction. _Engineering_, 2022.
* Johnson et al. (2021) Daniel D Johnson, Jacob Austin, Rianne van den Berg, and Daniel Tarlow. Beyond in-place corruption: Insertion and deletion in denoising probabilistic models. _arXiv preprint arXiv:2107.07675_, 2021.
* Kim et al. (2023) Seonghwan Kim, Jeheon Woo, and Woo Youn Kim. Diffusion-based generative ai for exploring transition states from 2d molecular graphs. 2023.
* Kim et al. (2021)Christian Leonard. A survey of the schr\(\backslash\)" odinger problem and some of its connections with optimal transport. _arXiv preprint arXiv:1308.0215_, 2013.
* Lipman et al. [2022] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. _arXiv preprint arXiv:2210.02747_, 2022.
* Liu et al. [2017] Bowen Liu, Bharath Ramsundar, Prasad Kawthekar, Jade Shi, Joseph Gomes, Quang Luu Nguyen, Stephen Ho, Jack Sloane, Paul Wender, and Vijay Pande. Retrosynthetic reaction prediction using neural sequence-to-sequence models. _ACS central science_, 3(10):1103-1113, 2017.
* Liu et al. [2022] Guan-Horng Liu, Tianrong Chen, Oswin So, and Evangelos Theodorou. Deep generalized schrodinger bridge. _Advances in Neural Information Processing Systems_, 35:9374-9388, 2022.
* Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* Nichol and Dhariwal [2021] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* Perez et al. [2018] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* Rezende and Mohamed [2015] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _International conference on machine learning_, pages 1530-1538. PMLR, 2015.
* Sacha et al. [2021] Mikolaj Sacha, Mikolaj Blaz, Piotr Byrski, Pawel Dabrowski-Tumanski, Mikolaj Chrominski, Rafal Loska, Pawel Wlodarczyk-Pruszynski, and Stanislaw Jastrzebski. Molecule edit graph attention network: modeling chemical reactions as sequences of graph edits. _Journal of Chemical Information and Modeling_, 61(7):3273-3284, 2021.
* Schneider et al. [2016] Nadine Schneider, Nikolaus Stiefl, and Gregory A Landrum. What's what: The (nearly) definitive guide to reaction role assignment. _Journal of chemical information and modeling_, 56(12):2336-2346, 2016.
* Schneuing et al. [2022] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Lio, Carla Gomes, Max Welling, et al. Structure-based drug design with equivariant diffusion models. _arXiv preprint arXiv:2210.13695_, 2022.
* Schrodinger [1932] Erwin Schrodinger. Sur la theorie relativiste de l'electron et l'interpretation de la mecanique quantique. In _Annales de l'institut Henri Poincare_, volume 2, pages 269-310, 1932.
* Schwaller et al. [2019] Philippe Schwaller, Teodoro Laino, Theophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. _ACS central science_, 5(9):1572-1583, 2019.
* Schwaller et al. [2020] Philippe Schwaller, Riccardo Petraglia, Valerio Zullo, Vishnu H Nair, Rico Andreas Haeuselmann, Riccardo Pisoni, Costas Bekas, Anna Iuliano, and Teodoro Laino. Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. _Chemical science_, 11(12):3316-3325, 2020.
* Segler and Waller [2017] Marwin HS Segler and Mark P Waller. Neural-symbolic machine learning for retrosynthesis and reaction prediction. _Chemistry-A European Journal_, 23(25):5966-5971, 2017.
* Segler et al. [2018] Marwin HS Segler, Mike Preuss, and Mark P Waller. Planning chemical syntheses with deep neural networks and symbolic ai. _Nature_, 555(7698):604-610, 2018.
* Shi et al. [2020] Chence Shi, Minkai Xu, Hongyu Guo, Ming Zhang, and Jian Tang. A graph to graphs framework for retrosynthesis prediction. In _International conference on machine learning_, pages 8818-8827. PMLR, 2020.
* Sohl-Dickstein et al. [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.
* Sohl-Dickstein et al. [2016]Vignesh Ram Somnath, Charlotte Bunne, Connor Coley, Andreas Krause, and Regina Barzilay. Learning graph models for retrosynthesis prediction. _Advances in Neural Information Processing Systems_, 34:9405-9415, 2021.
* Somnath et al. [2023] Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, and Charlotte Bunne. Aligned diffusion schr\(\backslash\)" _odinger bridges. arXiv preprint arXiv:2302.11419_, 2023.
* Song et al. [2020] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* Stanley and Segler [2023] Megan Stanley and Marwin Segler. Fake it until you make it? generative de novo design and virtual screening of synthesizable molecules. _Current Opinion in Structural Biology_, 82:102658, 2023.
* Strieth-Kalthoff et al. [2020] Felix Strieth-Kalthoff, Frederik Sandfort, Marwin HS Segler, and Frank Glorius. Machine learning the ropes: principles, applications and directions in synthetic chemistry. _Chemical Society Reviews_, 49(17):6154-6168, 2020.
* Tetko et al. [2020] Igor V Tetko, Pavel Karpov, Ruud Van Deursen, and Guillaume Godin. State-of-the-art augmented nlp transformer models for direct and single-step retrosynthesis. _Nature communications_, 11(1):5575, 2020.
* Thomas et al. [2023] Morgan Thomas, Andreas Bender, and Chris de Graaf. Integrating structure-based approaches in generative molecular design. _Current Opinion in Structural Biology_, 79:102559, 2023.
* Tu et al. [2023] Zhengkai Tu, Thijs Stuyver, and Connor W Coley. Predictive chemistry: machine learning for reaction deployment, reaction development, and reaction discovery. _Chemical Science_, 2023.
* Vargas et al. [2021] Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving schrodinger bridges via maximum likelihood. _Entropy_, 23(9):1134, 2021.
* Vignac et al. [2022] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. _arXiv preprint arXiv:2209.14734_, 2022.
* Wang et al. [2021] Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep generative learning via schrodinger bridge. In _International Conference on Machine Learning_, pages 10794-10804. PMLR, 2021.
* Watson et al. [2023] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. _Nature_, pages 1-3, 2023.
* Weininger [1988] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. _Journal of chemical information and computer sciences_, 28(1):31-36, 1988.
* Yang et al. [2023] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 2023.
* Zheng et al. [2019] Shuangjia Zheng, Jiahua Rao, Zhongyue Zhang, Jun Xu, and Yuedong Yang. Predicting retrosynthetic reactions using self-corrected transformer neural networks. _Journal of chemical information and modeling_, 60(1):47-55, 2019.

Appendix

### Variational Lower Bound

The log-likelihood of reactants \(\bm{y}\equiv\bm{z}_{T}\) given product \(\bm{x}\equiv\bm{z}_{0}\) can be written as follows,

\[\log q_{\theta}(\bm{y}|\bm{x}) =\log q_{\theta}(\bm{z}_{T}|\bm{z}_{0})\] (13) \[=\log\int d\bm{z}_{1:T-1}q_{\theta}(\bm{z}_{1:T}|\bm{z}_{0})\] (14) \[=\log\int d\bm{z}_{1:T-1}\prod_{t=1}^{T}q_{\theta}(\bm{z}_{t}|\bm {z}_{t-1})\] (15) \[=\log\int d\bm{z}_{1:T-1}\frac{p(\bm{z}_{1:T}|\bm{z}_{0},\bm{z}_{ T})}{p(\bm{z}_{1:T}|\bm{z}_{0},\bm{z}_{T})}\prod_{t=1}^{T}q_{\theta}(\bm{z}_{t}| \bm{z}_{t-1})\] (16) \[=\log\int d\bm{z}_{1:T-1}p(\bm{z}_{1:T}|\bm{z}_{0},\bm{z}_{T}) \prod_{t=1}^{T}\frac{q_{\theta}(\bm{z}_{t}|\bm{z}_{t-1})}{p(\bm{z}_{t}|\bm{z}_ {t-1},\bm{z}_{T})}.\] (17)

Using Jensen's inequality (JI) and the fact that \(p(\bm{z}_{1:T}|\bm{z}_{0},\bm{z}_{T})=p(\bm{z}_{1:T-1}|\bm{z}_{0},\bm{z}_{T})\) (\(*\)) we can derive a lower bound of this log-likelihood,

\[\log q_{\theta}(\bm{y}|\bm{x}) \stackrel{{\text{II}}}{{\geq}}\int d\bm{z}_{1:T-1}p (\bm{z}_{1:T}|\bm{z}_{0},\bm{z}_{T})\log\prod_{t=1}^{T}\frac{q_{\theta}(\bm{ z}_{t}|\bm{z}_{t-1})}{p(\bm{z}_{t}|\bm{z}_{t-1},\bm{z}_{T})}\] (18) \[\stackrel{{(*)}}{{=}}\int d\bm{z}_{1:T-1}p(\bm{z}_{1 :T-1}|\bm{z}_{0},\bm{z}_{T})\log\prod_{t=1}^{T}\frac{q_{\theta}(\bm{z}_{t}|\bm {z}_{t-1})}{p(\bm{z}_{t}|\bm{z}_{t-1},\bm{z}_{T})}\] (19) \[=\sum_{t=1}^{T}\int d\bm{z}_{1:T-1}p(\bm{z}_{1:T-1}|\bm{z}_{0}, \bm{z}_{T})\log\frac{q_{\theta}(\bm{z}_{t}|\bm{z}_{t-1})}{p(\bm{z}_{t}|\bm{z}_ {t-1},\bm{z}_{T})}\] (20) \[=\mathcal{L}_{1}(\bm{z}_{0},\bm{z}_{T})+\sum_{t=2}^{T}\mathcal{L} _{t}(\bm{z}_{0},\bm{z}_{T}).\] (21)

Here, the first term \(\mathcal{L}_{1}\) can be written as follows,

\[\mathcal{L}_{1}(\bm{z}_{0},\bm{z}_{T})=\int d\bm{z}_{1}p(\bm{z}_{1}|\bm{z}_{0},\bm{z}_{T})\log\frac{q_{\theta}(\bm{z}_{1}|\bm{z}_{0})}{p(\bm{z}_{1}|\bm{z}_ {0},\bm{z}_{T})}=-D_{\text{KL}}\left(p(\bm{z}_{1}|\bm{z}_{0},\bm{z}_{T})\|q_{ \theta}(\bm{z}_{1}|\bm{z}_{0})\right).\] (22)

Using Bayes' rule (BR) and the Markov property (MP) of the Markov bridge \(p\), we can derive a similar expression for all intermediate terms \(\mathcal{L}_{t}\),

\[\mathcal{L}_{t}(\bm{z}_{0},\bm{z}_{T}) =\int d\bm{z}_{t-1}d\bm{z}_{t}p(\bm{z}_{t-1},\bm{z}_{t}|\bm{z}_{0},\bm{z}_{T})\log\frac{q_{\theta}(\bm{z}_{t}|\bm{z}_{t-1})}{p(\bm{z}_{t}|\bm{z}_ {t-1},\bm{z}_{T})}\] (23) \[\stackrel{{\text{BR}}}{{=}}\int d\bm{z}_{t-1}d\bm{z}_ {t}p(\bm{z}_{t-1}|\bm{z}_{0},\bm{z}_{T})p(\bm{z}_{t}|\bm{z}_{t-1},\bm{z}_{0},\bm{z}_{T})\log\frac{q_{\theta}(\bm{z}_{t}|\bm{z}_{t-1})}{p(\bm{z}_{t}|\bm{z} _{t-1},\bm{z}_{T})}\] (24) \[\stackrel{{\text{MP}}}{{=}}\int d\bm{z}_{t-1}d\bm{z}_ {t}p(\bm{z}_{t-1}|\bm{z}_{0},\bm{z}_{T})p(\bm{z}_{t}|\bm{z}_{t-1},\bm{z}_{T}) \log\frac{q_{\theta}(\bm{z}_{t}|\bm{z}_{t-1})}{p(\bm{z}_{t}|\bm{z}_{t-1},\bm{z }_{T})}\] (25) \[=-\int d\bm{z}_{t-1}p(\bm{z}_{t-1}|\bm{z}_{0},\bm{z}_{T})D_{\text {KL}}\left(p(\bm{z}_{t}|\bm{z}_{t-1},\bm{z}_{T})\|q_{\theta}(\bm{z}_{t}|\bm{z}_ {t-1})\right).\] (26)

We combine \(\mathcal{L}_{1}\) and \(\mathcal{L}_{t}\) to obtain the final expression for the variational lower bound of the log-likelihood:

\[\log q_{\theta}(\bm{y}|\bm{x}) \geq-\sum_{t=1}^{T}\mathbb{E}_{\bm{z}_{t-1}\sim p(\bm{z}_{t-1}| \bm{x},\bm{y})}D_{\text{KL}}\left(p(\bm{z}_{t}|\bm{z}_{t-1},\bm{y})\|q_{\theta}( \bm{z}_{t}|\bm{z}_{t-1})\right)\] (27) \[=-\sum_{t=0}^{T-1}\mathbb{E}_{\bm{z}_{t}\sim p(\bm{z}_{t}|\bm{x},\bm{y})}D_{\text{KL}}\left(p(\bm{z}_{t+1}|\bm{z}_{t},\bm{y})\|q_{\theta}(\bm{z}_ {t+1}|\bm{z}_{t})\right).\] (28)Finally, we obtain the form provided in (7) by replacing the sum over all \(T\) terms with its unbiased estimator,

\[\log q_{\theta}(\bm{y}|\bm{x})\geq-T\cdot\mathbb{E}_{t\sim\mathcal{U}(0,\ldots,T- 1)}\mathbb{E}_{\bm{z}_{t}\sim p(\bm{z}_{t}|\bm{x},\bm{y})}D_{\text{KL}}\left(p( \bm{z}_{t+1}|\bm{z}_{t},\bm{y})\|q_{\theta}(\bm{z}_{t+1}|\bm{z}_{t})\right).\] (29)

### Cumulative Transition Matrix \(\bm{Q}_{t}\)

A proof by induction. For \(t=0\), by definition, \(\bm{\overline{Q}}_{0}=\bm{Q}_{0}\) and \(\overline{\alpha}_{0}=\alpha_{0}\).

Assume that for \(t>0\) Equation 8 holds, i.e. \(\bm{\overline{Q}}_{t}=\overline{\alpha}_{t}\bm{I}_{K}+(1-\overline{\alpha}_{ t})\bm{y}\bm{1}_{K}^{\top}\) and \(\overline{\alpha}_{t}=\prod_{s=0}^{t}\alpha_{s}\).

Then for \(t+1\) we have

\[\overline{\bm{Q}}_{t+1} =\bm{Q}_{t+1}\overline{\bm{Q}}_{t}\] (30) \[=\left[\alpha_{t+1}\bm{I}_{K}+(1-\alpha_{t+1})\bm{y}\bm{1}_{K}^{ \top}\right]\left[\overline{\alpha}_{t}\bm{I}_{K}+(1-\overline{\alpha}_{t}) \bm{y}\bm{1}_{K}^{\top}\right]\] (31) \[=\alpha_{t+1}\overline{\alpha}_{t}\bm{I}_{K}+(\alpha_{t+1}- \alpha_{t+1}\overline{\alpha}_{t}+\overline{\alpha}_{t}-\alpha_{t+1}\overline {\alpha}_{t})\bm{y}\bm{1}_{K}^{\top}\] (32) \[+(1-\alpha_{t+1}-\overline{\alpha}_{t}+\alpha_{t+1}\overline{ \alpha}_{t})\bm{y}\bm{1}_{K}^{\top}\bm{y}\bm{1}_{K}^{\top}.\] (33)

Note that \(\bm{y}\bm{1}_{K}^{\top}\bm{y}\bm{1}_{K}^{\top}=\bm{y}\bm{1}_{K}^{\top}\) and \(\alpha_{t+1}\overline{\alpha}_{t}=\overline{\alpha}_{t+1}\). Therefore, we get

\[\overline{\bm{Q}}_{t+1} =\overline{\alpha}_{t+1}\bm{I}_{K}+(\alpha_{t+1}-\overline{\alpha }_{t+1}+\overline{\alpha}_{t}-\overline{\alpha}_{t+1}+1-\alpha_{t+1}-\overline {\alpha}_{t}+\overline{\alpha}_{t+1})\bm{y}\bm{1}_{K}^{\top}\] (34) \[=\overline{\alpha}_{t+1}\bm{I}_{K}+(1-\overline{\alpha}_{t+1})\bm {y}\bm{1}_{K}^{\top}.\] (35)

### Implementation Details

#### a.3.1 Noise schedule

In all experiments, we use the cosine schedule (Nichol and Dhariwal, 2021)

\[\alpha_{t}=\cos\left(0.5\pi\frac{t/T+s}{1+s}\right)^{2}\] (36)

with \(s=0.008\) and number of time steps \(T=500\).

#### a.3.2 Additional Features

We represent molecules as fully-connected graphs where node features are one-hot encoded atom types (sixteen atom types and additional "dummy" type) and edge features are covalent bond types (three bond types and additional "none" type). Besides, we use a global graph feature \(\bm{y}\) which includes the normalized time step: \(\bm{y}=t/T\). Similarly to Vignac et al. (2022) we compute additional node features. For completeness, we provide the description of these features as in (Vignac et al., 2022) below.

CyclesRings of different sizes are crucial features of many bioactive molecules but graph neural networks are unable to detect them (Chen et al., 2020). We therefore add both global cycle counts \(\bm{y}_{k}\), that capture the overall number of cycles in the graph, as well as local cycle counts \(\bm{H}_{k}\), that measure how many cycles each node belongs to. These quantities are computed for \(k\)-cycles up to size \(k=5\) and \(k=6\) for local and global counts, respectively. As proposed by Vignac et al. (2022)3, we use the following equations that can be efficiently computed on GPUs

\[\bm{H}_{3}= \frac{1}{2}\operatorname{diag}(\bm{A}^{3})\] \[\bm{H}_{4}= \frac{1}{2}\left(\operatorname{diag}(\bm{A}^{4})-\bm{d}\odot(\bm{ d}-\bm{1}_{n})-\bm{A}\bm{d}\right)\] \[\bm{H}_{5}= \frac{1}{2}\left(\operatorname{diag}(\bm{A}^{5})-2\bm{T}\bm{d}-2 \bm{d}\operatorname{diag}(\bm{A}^{3})-\bm{A}\operatorname{diag}(\bm{A}^{3})+5 \operatorname{diag}(\bm{A}^{3})\right)\] \[\bm{y}_{3}= \frac{1}{3}\bm{H}_{3}^{\top}\bm{1}_{n}\] \[\bm{y}_{4}= \frac{1}{4}\bm{H}_{4}^{\top}\bm{1}_{n}\] \[\bm{y}_{5}= \frac{1}{5}\bm{H}_{5}^{\top}\bm{1}_{n}\] \[\bm{y}_{6}= \operatorname{Tr}(\bm{A}^{6})-3\operatorname{Tr}(\bm{A}^{3} \odot\bm{A}^{3})+9\|\bm{A}(\bm{A}^{2}\odot\bm{A}^{2})\|_{F}-6\operatorname{ diag}(\bm{A}^{2})^{\top}\operatorname{diag}(\bm{A}^{4})\] \[+6\operatorname{Tr}(\bm{A}^{4})-4\operatorname{Tr}(\bm{A}^{3})+4 \operatorname{Tr}(\bm{A}^{2}\bm{A}^{2}\odot\bm{A}^{2})+3\|\bm{A}^{3}\|_{F}-12 \operatorname{Tr}(\bm{A}^{2}\odot\bm{A}^{2})+4\operatorname{Tr}(\bm{A}^{2})\]

where \(\bm{A}\in\mathbb{R}^{n\times n}\) is the adjacency matrix and \(\bm{d}\in\mathbb{R}^{n}\) the node degree vector. Matrix \(\bm{T}=\bm{A}\odot\bm{A}^{2}\) indicates how many triangles each nodes shares with every other node.

Spectral featuresAgain following (Vignac et al., 2022), we include spectral features based on the eigenvalues and eigenvectors of the graph Laplacian. We use the multiplicity of eigenvalue 0 and the first five nonzero eigenvalues as graph-level features, and an indicator of the largest connected component (approximated based on the eigenvectors corresponding to zero eigenvalues) as well as two eigenvectors corresponding to the first nonzero eigenvalues as node-level features.

Molecular featuresWe also tried adding the molecular weight as a graph-level feature and each atom's valency as node-level features, following (Vignac et al., 2022). Models trained with these features were used everywhere except experiments reported in Tables 1 and 2. Later on, we found out that removing these features improves the performance on the validation set. Therefore, the final model from Tables 1 and 2 does not use molecular features.

#### a.3.3 Neural Network Architecture

We use a graph transformer network (Dwivedi and Bresson, 2020; Vignac et al., 2022) to approximate the final state of the Markov bridge process. The architecture of the network is provided in Figure 4A. First, node, edge and global features are passed through an encoder which is implemented as an MLP. Next, the encoded features are processed by a sequence of Graph Transformer Layers (GTL). As shown in Figure 4B, GTL first updates node features using self-attention and combines its output with edge features via FiLM (Perez et al., 2018):

\[\operatorname{FiLM}(\bm{X}_{1},\bm{X}_{2})=\bm{X}_{1}\bm{W}_{1}+(\bm{X}_{1} \bm{W}_{2})\odot\bm{X}_{2}+\bm{W}_{2},\] (37)

where \(\bm{W}_{1}\) and \(\bm{W}_{2}\) are learnable parameters. Then, edge features are updated using attention scores and global features. To update the global features, GTL combines encoded global features and node and edge features aggregated with PNA:

\[\operatorname{PNA}(\bm{X})=\text{concat}(\max(\bm{X}),\min(\bm{X}),\text{ mean}(\bm{X}),\text{std}(\bm{X}))\bm{W},\] (38)

where \(\bm{W}\) is a learnable parameter.

Finally, to obtain the final graph representation, updated node and edge features are passed through the decoder which is implemented as an MLP.

#### a.3.4 Training

We train our models on a single GPU Tesla V100-PCIE-32GB using AdamW optimizer (Loshchilov and Hutter, 2017) with learning rate \(0.0002\) and batch size \(64\). We trained models for up to 1000 epochs (which takes several days) and then selected the best checkpoints based on top-5 accuracy (that was computed on a subset of the USPTO-50k validation set).

### Additional Experiments

In this section, we compare RetroBridge with the naive adaptation of DiGress (Vignac et al., 2022) for retrosynthesis prediction, which can be considered the most comparable diffusion-based method for this task, and with a graph transformer network that predicts reactants in a one-shot fashion. Furthermore, we study the effect of adding an input product molecule as context to the neural network \(\varphi_{\theta}\) at each sampling step. More precisely, for models with no context we use the formulation (5), while models with context compute predictions as \(\hat{\bm{y}}=\varphi_{\theta}(\bm{z}_{t},\bm{x},t)\). Finally, we try a simpler cross-entropy (CE) loss function, as used in DiGress, that directly compares approximated reactants with the ground-truth,

\[\mathcal{L}_{\text{CE}}(\theta)=T\cdot\mathbb{E}_{t\sim\mathcal{U}(0,\dots,T-1 )}\mathbb{E}_{\bm{z}_{t},\bm{z}_{T}\sim p}\text{CrossEntropy}(\bm{z}_{T}, \varphi_{\theta}(\bm{z}_{t},t)).\] (39)

In all experiments we use the same neural network architectures and sets of hyperparameters. We perform our evaluation on the USPTO-50k validation set and report top-\(k\) accuracy of the generated samples in Table 3.

First of all, we observe that iterative sampling as performed by diffusion models or the Markov Bridge Model is essential for solving the problem of mapping between two graph distributions. Our one-shot graph transformer model trained for a comparable amount of time does not manage to recover any of the reactants. Indeed, it is an extremely challenging task as even a single incorrectly predicted bond or atom type is detrimental for the accuracy metric. To the best of our knowledge, all one-shot graph-based models proposed for retrosynthesis prediction fall into the category of template-based methods. In this case, the networks do not predict the entire set of reactants right away, but instead

Figure 4: Architecture of the network that approximates the final state of the Markov bridge process (A) and scheme of the Graph Transformer Layer (B).

aim to solve much simpler tasks such as prediction of graph edits. To obtain the final set of reactants, graph edit predictions should be further processed by an additional block that typically relies on the predefined reaction templates or leaving group dictionaries.

Next, we compare RetroBridge with DiGress to demonstrate that the Markov bridge formulation is more suitable than diffusion models when two discrete graph distributions are to be mapped. As shown in Table 3, RetroBridge outperforms DiGress with context in all metrics. We note that if we do not pass the context, DiGress predictably does not manage to recover any reactants. This result illustrates that the Markov bridge framework captures the underlying structure of the task much more naturally than diffusion models. A diffusion model maps sampled noise to the reactants having access to the input product molecule only through the additional context while the Markov bridge model starts each sampling trajectory with a product molecule from the intractable distribution \(p_{\mathcal{X}}(\bm{x})=\int p_{\mathcal{X},\mathcal{Y}}(\bm{x},\bm{y})d\bm{y}\).

Finally, we demonstrate that the variational lower bound loss (7) works better than a simplified cross-entropy loss (39) proposed by Vignac et al. (2022). Ultimately, we find it beneficial to include the input product as context at each sampling step. Unlike the diffusion approach, however, RetroBridge achieves reasonable accuracy values even without additional context as large parts of the product structure are retained throughout the sampling trajectory.

### Forward Reaction Prediction

We additionally trained and evaluated two models for forward reaction prediction using USPTO-50k and USPTO-MIT datasets. We used the same hyperparameters as in other experiments. As shown in Table 4, our models (ForwardBridge) demonstrate comparable performance with other state-of-the-art methods. However, we stress that the probabilistic formulation is less applicable to the forward reaction prediction task, and under certain assumptions this problem can be considered as completely deterministic. Therefore, we leave the study of capabilities of the Markov Bridge Model in the context of forward reaction prediction out of the scope of this work.

\begin{table}
\begin{tabular}{l l c c c} \hline \hline
**Dataset** & **Model** & \(k=1\) & \(k=3\) & \(k=5\) \\ \hline USPTO-50k & ForwardBridge (ours) & 89.9 & 93.9 & 94.0 \\ \hline \multirow{3}{*}{USPTO-MIT} & ForwardBridge (ours) & 81.6 & 88.5 & 89.8 \\  & MEGAN & 86.3 & 92.4 & 94.0 \\ \cline{1-1}  & Mol. Transformer & 88.7 & 93.1 & 94.2 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Top-\(k\) accuracy for forward reaction prediction.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Model** & \(k=1\) & \(k=3\) & \(k=5\) & \(k=10\) & \(k=50\) \\ \hline DiGress (context) & 47.32 & 68.56 & 73.93 & 78.45 & 80.88 \\ RetroBridge-CE (no context) & 48.71 & 66.84 & 72.33 & 76.08 & 79.38 \\ RetroBridge-CE (context) & **50.74** & 71.50 & 76.58 & 79.50 & 80.58 \\ RetroBridge-VLB (no context) & 47.42 & 69.46 & 75.21 & 79.40 & 83.82 \\ RetroBridge-VLB (context) & 48.92 & **73.04** & **79.44** & **83.74** & **86.31** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Additional experiments: top-\(k\) accuracy on the USPTO-50k validation set.

Figure 5: Examples of modeled reactants. We selected 10 random inputs from the USPTO-50k test set and for each of them we provide the top-3 RetroBridge predictions along with their confidence scores. Two check marks indicate that sampled reactants are the same as the ground truth, and one check mark means that reactants are different, but Molecular Transformer (Schwaller et al., 2019) predicts the product molecule used as input.