Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm

 Qinbo Bai

Purdue University

West Lafayette, IN 47906

bai113@purdue.edu &Washim Uddin Mondal

Indian Institute of Technology Kanpur

Kanpur, UP, India 208016

wmondal@iitk.ac.in &Vaneet Aggarwal

Purdue University

West Lafayette, IN 47906

vaneet@purdue.edu

Qinbo Bai

Purdue University

West Lafayette, IN 47906

bai113@purdue.edu &Washim Uddin Mondal

Indian Institute of Technology Kanpur

Kanpur, UP, India 208016

wmondal@iitk.ac.in &Vaneet Aggarwal

Purdue University

West Lafayette, IN 47906

vaneet@purdue.edu

###### Abstract

This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDPs). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual-based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, our proposed algorithm achieves \(\tilde{\mathcal{O}}(T^{4/5})\) objective regret and \(\tilde{\mathcal{O}}(T^{4/5})\) constraint violation bounds.

## 1 Introduction

The framework of Reinforcement Learning (RL) is concerned with a class of problems where an agent learns to yield the maximum cumulative reward in an unknown environment via repeated interaction. RL finds applications in diverse areas, such as wireless communication, transportation, and epidemic control [1; 2; 3]. RL problems are mainly categorized into three setups: episodic, infinite horizon discounted reward, and infinite horizon average reward. Among them, the infinite horizon average reward setup is particularly significant for real-world applications. It aligns with most of the practical scenarios and captures their long-term goals. Some applications in real life require the learning procedure to respect the boundaries of certain constraints. In an epidemic control setup, for example, vaccination policies must take the supply shortage (budget constraint) into account. Such restrictive decision-making routines are described by constrained Markov Decision Processes (CMDP) [4; 5; 6]. Existing papers on CMDPs utilize either a tabular or a linear MDP structure. This work provides the first algorithm for an infinite horizon average reward CMDP with general parametrization and proves its sub-linear regret and constraint violation bounds.

There are two primary ways to solve a CMDP problem in the infinite horizon average reward setting. The first one, known as the model-based approach, involves constructing estimates of the transition probabilities of the underlying CMDP, which are subsequently utilized to derive policies [6; 7; 5]. The caveat of this approach is the large memory requirement to store the estimated parameters, which effectively curtails its applicability to CMDPs with large state spaces. The alternative strategy, known as the model-free approach, either directly estimates the policy function or maintains an estimate of the \(Q\) function, which is subsequently used for policy generation [8]. Model-free algorithms typically demand lower memory and computational resources than their model-based counterparts. Although the CMDP has been solved in a model-free manner in the tabular [8] and linear [9] setups, its exploration with the general parameterization is still open and is the goal of this paper.

General parameterization indexes the policies by finite-dimensional parameters (e.g., weights of neural networks) to accommodate large state spaces. The learning is manifested by updating these parameters using policy gradient (PG)-type algorithms. Note that PG algorithms are primarily studied in discounted reward setups. For example, [10] characterizes the sample complexities of the PG and the Natural PG (NPG) algorithms with softmax and direct parameterization. Similar results for general parameterization are obtained by [11, 12]. The regret analysis of a PG algorithm with the general parameterization has been recently performed for an infinite horizon average reward MDP without constraints [13]. Similar regret and constraint violation analysis for the average reward CMDP is still missing in the literature. In this paper, we bridge this gap.

**Challenges and Contribution:** We propose a PG-based algorithm with general parameterized policies for the average reward CMDP and establish its sublinear regret and constraint violation bounds. In particular, assuming the underlying CMDP to be ergodic, we demonstrate that our PG algorithm achieves an average optimality rate of \(\tilde{\mathcal{O}}(T^{-\frac{1}{5}})\) and average constraint violation rate of \(\tilde{\mathcal{O}}(T^{-\frac{1}{5}})\). Invoking this convergence result, we establish that our algorithm achieves regret and constraint violation bounds of \(\tilde{\mathcal{O}}(T^{\frac{1}{5}})\). Apart from providing the first sublinear regret guarantee for the average reward CMDP with general parameterization, our work also improves the state-of-the-art regret guarantee, \(\tilde{\mathcal{O}}(T^{5/6})\) in the model-free tabular setup [8].

Despite the availability of sample complexity analysis of PG algorithms with constraints in the discounted reward setup [14, 4] and PG algorithms without constraint in average reward setup [13], obtaining sublinear regret and constraint violation bounds for their average reward counterpart is challenging.

\(\bullet\)[14, 4] solely needs an estimate of the value function \(V\) while we additionally need the estimate of the gain function, \(J\).

\(\bullet\)[14, 4] assume access to a simulator to generate unbiased value estimates. In contrast, our algorithm uses a sample trajectory of length \(H\) to estimate the values and gains and does not assume the availability of a simulator.

\(\bullet\)The first-order convergence analysis (Lemma 6) differs from that in [13]. Note that both of these papers use an ascent-like inequality. In [13], this bounds the term \(J(\theta_{k+1})-J(\theta_{k})\). The final result is obtained by calculating a sum over \(k\) which cancels the intermediate terms and leaves us with \(J(\theta_{K})-J(\theta_{1})\). We would like to emphasize that the cancellation of the intermediate terms is crucial to establishing the result. However, a similar effort in our case only leads to a bound of \(J_{\mathrm{L}}(\theta_{k+1},\lambda_{k})-J_{\mathrm{L}}(\theta_{k},\lambda_{k})\). Note that directly performing a sum over this difference does not lead to the cancellation of intermediate terms. We had to take a different route and apply the bounds of the Lagrange multipliers and the estimate of the constraint function to achieve that goal.

\(\bullet\)After solving the problems mentioned above, we prove \(\tilde{\mathcal{O}}(T^{-\frac{1}{5}})\) convergence rate of the Lagrange function. Unfortunately, the strong duality property, which is central to proving convergence results of CMDPs for tabular and softmax policies, does not hold under the general parameterization. As a result, the convergence result for the dual problem does not automatically translate to that for the primal problem, which is a main difference from [13]. We overcome this barrier by introducing a novel constraint violation analysis and a series of intermediate results (Lemma 16-18) that help disentangle the regret and constraint violation rates from the Lagrange convergence. It is important to mention that although the techniques applied are inspired by the [14], those techniques cannot be directly adopted for average reward MDPs. This is primarily because the estimate \(\hat{J}_{c}(\theta_{k})\) is biased in the average case. To the best of our knowledge, constraint violation analysis with a biased estimate of the cost value is not available in the literature and is performed for the first time in our paper.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Algorithm & Regret & Violation & Model-free & Setting \\ \hline Algorithm 1 in [6] & \(\tilde{\mathcal{O}}(\sqrt{T})\) & \(\tilde{\mathcal{O}}(\sqrt{T})\) & No & Tabular \\ \hline Algorithm 2 in [6] & \(\tilde{\mathcal{O}}(T^{2/3})\) & \(\tilde{\mathcal{O}}(T^{2/3})\) & No & Tabular \\ \hline UC-CURL and PS-CURL [5] & \(\tilde{\mathcal{O}}(\sqrt{T})\) & \(0\) & No & Tabular \\ \hline Algorithm 2 in [9] & \(\tilde{\mathcal{O}}((dT)^{3/4})\) & \(\tilde{\mathcal{O}}((dT)^{3/4})\) & No & Linear MDP \\ \hline Algorithm 3 in [9] & \(\tilde{\mathcal{O}}(\sqrt{T})\) & \(\tilde{\mathcal{O}}(\sqrt{T})\) & No & Linear MDP \\ \hline Triple-QA [8] & \(\tilde{\mathcal{O}}(T^{5/6})\) & \(0\) & Yes & Tabular \\ \hline This paper & \(\tilde{\mathcal{O}}(T^{\frac{1}{5}})\) & \(\tilde{\mathcal{O}}(T^{\frac{1}{5}})\) & Yes & General Parameterization \\ \hline \end{tabular}
\end{table}
Table 1: This table summarizes the different model-based and mode-free state-of-the-art algorithms available in the literature for average reward CMDPs. We note that our proposed algorithm is the first to analyze the regret and constraint violation for average reward CMDP with general parametrization. Here, the parameter \(d\) refers to the dimension of the feature map for linear MDPs.

* Due to the presence of the Lagrange multiplier, the convergence analysis of a CMDP is much more convoluted than its unconstrained counterpart. The learning rate of the Lagrange update, \(\beta\), turns out to be pivotal in determining the growth rate of regret and constraint violation. Low values of \(\beta\) push the regret down while simultaneously increasing the constraint violation. Finding the optimal value of \(\beta\) that judiciously balances these two competing goals is one of the cornerstones of our analysis.

**Related work for unconstrained average reward RL:** In the absence of constraints, both model-based and model-free tabular setups have been widely studied for infinite horizon average reward MDPs. For example, the model-based algorithms proposed by [15; 16] achieve the optimal regret bound of \(\tilde{\mathcal{O}}(\sqrt{T})\). Similarly, the model-free algorithm proposed by [17] for tabular MDP results in \(\tilde{\mathcal{O}}(\sqrt{T})\) regret. Regret analysis for average reward MDP with general parametrization has been recently studied in [13], where a regret bound of \(\tilde{\mathcal{O}}(T^{3/4})\) is derived.

**Related work for constrained RL:** The constrained reinforcement learning problem has been extensively studied both for infinite horizon discounted reward and episodic MDPs. For example, discounted reward CMDPs have been recently studied in the tabular setup [18], with both softmax [14; 19], and general policy parameterization [14; 19; 4; 12]. Moreover, [20; 21; 22] investigated episodic CMDPs in the tabular setting.

Recently, the infinite horizon average reward CMDPs have been investigated in model-based setups [5; 6; 7], tabular model-free setting [8] and linear CMDP setting [9]. For model-based CMDP setup, [6] proposed a model-based online mirror descent algorithm in the ergodic setting which achieves \(\tilde{\mathcal{O}}(\sqrt{T})\) for regret and violation at the same time. [7] proposed algorithms based on the posterior sampling and the optimism principle that achieve \(\tilde{\mathcal{O}}(\sqrt{T})\) regret with zero constraint violations in the ergodic setting. However, the above model-based algorithms cannot be extended to large state space. In the tabular model-free setup, the algorithm proposed by [8] achieves a regret of \(\tilde{\mathcal{O}}(T^{5/6})\) with zero constraint violations. Finally, in the linear CMDP setting, [9] achieves \(\tilde{\mathcal{O}}(\sqrt{T})\) regret bound with zero constraint violation. Note that the linear CMDP setting assumes that the transition probability has a certain linear structure with a known feature map which is not realistic. Table 1 summarizes all relevant works. Unfortunately, none of these papers study the infinite horizon average reward CMDPs with general parametrization which is the main focus of our article.

Additionally, for the weakly communicating setting, [6] proposed a model-based algorithm achieving \(\tilde{\mathcal{O}}(T^{2/3})\) for both regret and violation in tabular case. [9] further extends such result to linear MDP setting with \(\tilde{\mathcal{O}}(T^{3/4})\) regret and violation. In general, it is difficult to propose a model-free algorithm with provable guarantees for Constrained MDPs (CMDPs) without considering the ergodic model. [6] pointed out several extra challenges in Weakly communicating MDP compared to the ergodic case. For example, there is no uniform bound for the span of the value function for all stationary policies. It is also unclear how to estimate a policy's bias function accurately without the estimated model, which is an important step for estimating the policy gradient.

## 2 Formulation

This paper analyzes an infinite-horizon average reward constrained Markov Decision Process (CMDP) denoted as \(\mathcal{M}=(\mathcal{S},\mathcal{A},r,c,P,\rho)\) where \(\mathcal{S}\) denotes the state space, \(\mathcal{A}\) is the action space of size \(A\), \(r:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) is the reward function, \(c:\mathcal{S}\times\mathcal{A}\rightarrow[-1,1]\) is the constraint cost function, \(P:\mathcal{S}\times\mathcal{A}\rightarrow\Delta^{|\mathcal{S}|}\) is the state transition function where \(\Delta^{|\mathcal{S}|}\) denotes a probability simplex with dimension \(|\mathcal{S}|\), and \(\rho\in\Delta^{|\mathcal{S}|}\) is the initial distribution of states. A policy \(\pi\in\Pi:\mathcal{S}\rightarrow\Delta^{A}\) maps the current state to an action distribution. The average reward and cost of a policy, \(\pi\), is,

\[J^{\pi}_{g,\rho}\triangleq\lim_{T\rightarrow\infty}\frac{1}{T}\mathbf{E}\bigg{[} \sum_{t=0}^{T-1}g(s_{t},a_{t})\bigg{|}s_{0}\sim\rho,\pi\bigg{]}\] (1)

where \(g=r,c\) for average reward and cost respectively. The expectation is calculated over the distribution of all sampled trajectories \(\{(s_{t},a_{t})\}_{t=0}^{\infty}\) where \(a_{t}\sim\pi(s_{t})\), \(s_{t+1}\sim P(\cdot|s_{t},a_{t})\), \(\forall t\in\{0,1,\cdots\}\). For notational convenience, we shall drop the dependence on \(\rho\) whenever there is no confusion. Our goal is to maximize the average reward function while ensuring that the average cost is above a given threshold. Without loss of generality, we can mathematically write this problem as,

\[\max_{\pi\in\Pi}\;J^{\pi}_{r}\;\;\text{s.t.}\;J^{\pi}_{c}\geq 0\] (2)However, the above problem becomes difficult to handle when the underlying state space, \(\mathcal{S}\) is large. Therefore, we consider a class of parametrized policies, \(\{\pi_{\theta}|\theta\in\Theta\}\) whose elements are indexed by a \(\mathrm{d}\)-dimensional parameter, \(\theta\in\mathbb{R}^{\mathrm{d}}\) where \(\mathrm{d}\ll|\mathcal{S}||\mathcal{A}|\). Thus, the original problem in Eq (2) can be reformulated as the following parameterized problem.

\[\max_{\theta\in\Theta}\;J_{r}^{\pi_{\theta}}\;\;\text{s.t.}\;J_{c}^{\pi_{ \theta}}\geq 0\] (3)

We denote \(J_{g}^{\pi_{\theta}}=J_{g}(\theta)\), \(g\in\{r,c\}\) for notational convenience. Let, \(P^{\pi_{\theta}}:\mathcal{S}\rightarrow\Delta^{|\mathcal{S}|}\) be a transition function induced by \(\pi_{\theta}\) and defined as, \(P^{\pi_{\theta}}(s,s^{\prime})=\sum_{a\in\mathcal{A}}P(s^{\prime}|s,a)\pi_{ \theta}(a|s)\), \(\forall s,s^{\prime}\). If \(\mathcal{M}\) is such that for every policy \(\pi\), the function, \(P^{\pi}\) is irreducible and aperiodic, then \(\mathcal{M}\) is called ergodic.

**Assumption 1**.: The CMDP \(\mathcal{M}\) is ergodic.

Ergodicity is a common assumption in the literature [23, 24]. If \(\mathcal{M}\) is ergodic, then \(\forall\theta\), there exists a unique stationary distribution, \(d^{\pi_{\theta}}\in\Delta^{|\mathcal{S}|}\) given as follows.

\[d^{\pi_{\theta}}(s)=\lim_{T\rightarrow\infty}\frac{1}{T}\sum_{t=0}^{T-1}\Pr(s _{t}=s|s_{0}\sim\rho,\pi_{\theta})\] (4)

Ergodicity implies that \(d^{\pi_{\theta}}\) is independent of the initial distribution, \(\rho\), and obeys \(P^{\pi_{\theta}}d^{\pi_{\theta}}=d^{\pi_{\theta}}\). Hence, the average reward and cost functions can be expressed as,

\[J_{g}(\theta)=\mathbf{E}_{s\sim d^{\pi_{\theta}},a\sim\pi_{\theta}(s)}[g(s,a) ]=(d^{\pi_{\theta}})^{T}g^{\pi_{\theta}}\] (5)

where \(g^{\pi_{\theta}}(s)\triangleq\sum_{a\in\mathcal{A}}g(s,a)\pi_{\theta}(a|s),\; g\in\{r,c\}\). Note that the functions \(J_{g}(\theta)\), \(g\in\{r,c\}\) are also independent of the initial distribution, \(\rho\). Furthermore, \(\forall\theta\), there exist a function \(Q_{g}^{\pi_{\theta}}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) such that the following Bellman equation is satisfied \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\).

\[Q_{g}^{\pi_{\theta}}(s,a)=g(s,a)-J_{g}(\theta)+\mathbf{E}_{s^{\prime}\sim P( \cdot|s,a)}\left[V_{g}^{\pi_{\theta}}(s^{\prime})\right]\] (6)

where \(g\in\{r,c\}\) and \(V_{g}^{\pi_{\theta}}:\mathcal{S}\rightarrow\mathbb{R}\) is given as \(V_{g}^{\pi_{\theta}}(s)=\sum_{a\in\mathcal{A}}\pi_{\theta}(a|s)Q_{g}^{\pi_{ \theta}}(s,a),\;\forall s\in\mathcal{S}\). Note that if \(Q_{g}^{\pi_{\theta}}\) satisfies \((\ref{eq:d_g})\), then it is also satisfied by \(Q_{g}^{\pi_{\theta}}+c\) for any arbitrary, \(c\). To uniquely define the value functions, we assume that \(\sum_{s\in\mathcal{S}}d^{\pi_{\theta}}(s)V_{g}^{\pi_{\theta}}(s)=0\). In this case, \(V_{g}^{\pi_{\theta}}(s)\) is given by,

\[V_{g}^{\pi_{\theta}}(s)=\sum_{t=0}^{\infty}\sum_{s^{\prime}\in\mathcal{S}} \left[(P^{\pi_{\theta}})^{t}(s,s^{\prime})-d^{\pi_{\theta}}(s^{\prime}) \right]g^{\pi_{\theta}}(s^{\prime})=\sum_{t=0}^{\infty}\mathbf{E}\left[\{g(s_{ t},a_{t})-J_{g}(\theta)\}\left|s_{0}=s\right]\] (7)

where the expectation is computed over all \(\pi_{\theta}\)-induced trajectories. In a similar way, \(\forall(s,a)\), one can uniquely define \(Q_{g}^{\pi_{\theta}}(s,a)\), \(g\in\{r,c\}\) as follows.

\[Q_{g}^{\pi_{\theta}}(s,a)=\sum_{t=0}^{\infty}\mathbf{E}\left[\big{\{}g(s_{t}, a_{t})-J_{g}(\theta)\big{\}}\big{|}s_{0}=s,a_{0}=a\right]\] (8)

Moreover, the advantage function \(A_{g}^{\pi_{\theta}}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is defined such that \(A_{g}^{\pi_{\theta}}(s,a)\triangleq Q_{g}^{\pi_{\theta}}(s,a)-V_{g}^{\pi_{ \theta}}(s)\), \(\forall(s,a)\), \(\forall g\in\{r,c\}\). Assumption 1 also implies the existence of a finite mixing time. Specifically, for an ergodic MDP, \(\mathcal{M}\), the mixing time is defined as follows.

**Definition 1**.: The mixing time, \(t_{\mathrm{mix}}^{\theta}\), of the CMDP \(\mathcal{M}\) for a parameterized policy, \(\pi_{\theta}\), is defined as, \(t_{\mathrm{mix}}^{\theta}\triangleq\min\big{\{}t\geq 1\big{\|}(P^{\pi_{ \theta}})^{t}(s,\cdot)-d^{\pi_{\theta}}\big{\|}\leq\frac{1}{4},\forall s\big{\}}\). The overall mixing time is \(t_{\mathrm{mix}}\triangleq\sup_{\theta\in\Theta}t_{\mathrm{mix}}^{\theta}\). In this paper, \(t_{\mathrm{mix}}\) is finite due to ergodicity.

Mixing time characterizes how fast a CMDP converges to its stationary state distribution, \(d^{\pi_{\theta}}\), under a given policy, \(\pi_{\theta}\). We also define the hitting time as follows.

**Definition 2**.: The hitting time of an ergodic CMDP \(\mathcal{M}\) with respect to a policy, \(\pi_{\theta}\), is defined as \(t_{\mathrm{hit}}^{\theta}\triangleq\max_{s\in\mathcal{S}}[d^{\pi_{\theta}}(s) ]^{-1}\). The overall hitting time is defined as \(t_{\mathrm{hit}}\triangleq\sup_{\theta\in\Theta}t_{\mathrm{hit}}^{\theta}\). In this paper, \(t_{\mathrm{hit}}\) is finite due to ergodicity as well.

Define \(\pi^{*}\) as the optimal solution to the unparameterized problem (2). For a given CMDP \(\mathcal{M}\), and a time horizon \(T\), the regret and constraint violation of any algorithm \(\mathbb{A}\) is defined as follows.

\[\mathrm{Reg}_{T}(\mathbb{A},\mathcal{M})\triangleq\sum_{t=0}^{T-1}\left(J_{r}^{ \pi^{*}}-r(s_{t},a_{t})\right),\;\mathrm{Vi}_{T}(\mathbb{A},\mathcal{M}) \triangleq-\sum_{t=0}^{T-1}c(s_{t},a_{t})\] (9)where the algorithm, \(\mathbb{A}\), executes the actions, \(\{a_{t}\}\), \(t\in\{0,1,\cdots\}\) based on the trajectory observed up to time, \(t\), and the state, \(s_{t+1}\) is decided according to the state transition function, \(P\). For simplicity, we shall denote the regret and constraint violation as \(\mathrm{Reg}_{T}\) and \(\mathrm{Vi}_{T}\) respectively. Our goal is to design an algorithm \(\mathbb{A}\) that achieves low regret and constraint violation bounds.

## 3 Proposed Algorithm

We solve (3) via a primal-dual algorithm based on the following problem.

\[\max_{\theta\in\Theta}\min_{\lambda\geq 0}\ J_{\mathrm{L}}(\theta,\lambda),\] (10)

where \(J_{\mathrm{L}}(\theta,\lambda)\triangleq J_{r}(\theta)+\lambda J_{c}(\theta)\). The function, \(J_{\mathrm{L}}(\cdot,\cdot)\), is called the Lagrange function and \(\lambda\) the Lagrange multiplier. Our algorithm updates the pair \((\theta,\lambda)\) following the policy gradient iteration as shown below \(\forall k\in\{1,\cdots,K\}\) with an initial point \((\theta_{1},\lambda_{1})\), \(\lambda_{1}=0\).

\[\theta_{k+1}=\theta_{k}+\alpha\nabla_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda _{k}),\ \lambda_{k+1}=\mathcal{P}_{[0,\frac{2}{\delta}]}[\lambda_{k}-\beta J_{c}( \theta_{k})]\] (11)

where \(\alpha\) and \(\beta\) are learning parameters and \(\delta\) is the Slater parameter introduced in the following assumption. Finally, for any set, \(\Lambda\), \(\mathcal{P}_{\Lambda}[\cdot]\) denotes projection onto \(\Lambda\). The assumption stated below ensures that we have at least one feasible interior point solution to (2).

**Assumption 2** (Slater condition).: There exists a \(\delta\in(0,1)\) and \(\bar{\theta}\in\Theta\) such that \(J_{c}(\bar{\theta})\geq\delta\).

Note that in (11), the dual update is projected onto the set \([0,\frac{2}{\delta}]\) because the optimal dual variable for the parameterized problem is bounded in Lemma 16. The gradient of \(J_{\mathrm{L}}(\cdot,\lambda)\) can be computed by invoking a variant of the well-known policy gradient theorem [25].

**Lemma 1**.: _The gradient of \(J_{\mathrm{L}}(\cdot,\lambda)\) is computed as,_

\[\nabla_{\theta}J_{\mathrm{L}}(\theta,\lambda)=\mathbf{E}_{s\sim d^{\pi_{ \theta}},a\sim\pi_{\theta}(s)}\big{[}A^{\pi_{\theta}}_{\mathrm{L},\lambda}(s, a)\nabla_{\theta}\log\pi_{\theta}(a|s)\big{]}\]

```
1:Input: Episode length \(H\), learning rates \(\alpha,\beta\), initial parameters \(\theta_{1},\lambda_{1}\), initial state \(s_{0}\sim\rho(\cdot)\),
2:\(K=T/H\)
3:for\(k\in\{1,\cdots,K\}\)do
4:\(\mathcal{T}_{k}\leftarrow\phi\)
5:for\(t\in\{(k-1)H,\cdots,kH-1\}\)do
6: Execute \(a_{t}\sim\pi_{\theta_{k}}(\cdot|s_{t})\)
7: Observe \(r(s_{t},a_{t})\), \(c(s_{t},a_{t})\) and \(s_{t+1}\)
8:\(\mathcal{T}_{k}\leftarrow\mathcal{T}_{k}\cup\{(s_{t},a_{t})\}\)
9:endfor
10:for\(t\in\{(k-1)H,\cdots,kH-1\}\)do
11: Obtain \(\tilde{A}^{\pi_{\theta_{k}}}_{\mathrm{L},\lambda_{k}}(s_{t},a_{t})\) via Algorithm 2 and \(\mathcal{T}_{k}\)
12:endfor
13: Compute \(\omega_{k}\) using (15)
14: Update the parameters: \[\theta_{k+1}=\theta_{k}+\alpha\omega_{k},\] (12) \[\lambda_{k+1}=\mathcal{P}_{[0,\frac{2}{\delta}]}\big{[}\lambda_{k }-\beta\hat{J}_{c}(\theta_{k})\big{]}\] \[\text{where }\hat{J}_{c}(\theta_{k})=\frac{1}{H-N}\sum_{t=(k-1)H+N}^{ kH-1}c(s_{t},a_{t})\]
15:endfor ```

**Algorithm 1** Primal-Dual Parameterized Policy Gradient

**Lemma 1**.: _The gradient of \(J_{\mathrm{L}}(\cdot,\lambda)\) is computed as,_

\[\nabla_{\theta}J_{\mathrm{L}}(\theta,\lambda)=\mathbf{E}_{s\sim d^{\pi_{ \theta}},a\sim\pi_{\theta}(s)}\big{[}A^{\pi_{\theta}}_{\mathrm{L},\lambda}(s, a)\nabla_{\theta}\log\pi_{\theta}(a|s)\big{]}\]

_where \(\forall(s,a)\), \(A^{\pi_{\theta}}_{\mathrm{L},\lambda}(s,a)\triangleq A^{\pi_{\theta}}_{r}(s,a)+ \lambda A^{\pi_{\theta}}_{c}(s,a)\), and \(\{A^{\pi_{\theta}}_{g}\}_{g\in\{r,c\}}\) are the advantage functions corresponding to reward and cost. In typical RL scenarios, learners do not have access to the state transition function, \(P\), and thereby to the functions \(d^{\pi_{\theta}}\) and \(A^{\pi_{\theta}}_{\mathrm{L},\lambda}\). This makes computing the exact gradient a difficult task. In Algorithm 1, we demonstrate how one can still obtain good estimates of the gradient using sampled trajectories.

Algorithm 1 runs \(K\) epochs, each of duration \(H=16t_{\mathrm{hit}}t_{\mathrm{mix}}T^{\xi}(\log T)^{2}\) where \(\xi\in(0,1)\) defines a constant whose value is specified later. Clearly, \(K=T/H\). Note that the learner is assumed to know the horizon length, \(T\). This can be relaxed utilizing the well-known doubling trick [26]. Additionally, it is assumed that the algorithm is aware of the mixing time and the hitting time. This assumption is common in the literature [13; 17]. The first step in obtaining a gradient estimate is estimating the advantage value for a given pair \((s,a)\). This can be accomplished via Algorithm 2. At the \(k\)th epoch, a \(\pi_{\theta_{k}}\)-induced trajectory, \(\mathcal{T}_{k}=\{(s_{t},a_{t})\}_{t=(k-1)H}^{kH-1}\) is obtained and subsequently passed to Algorithm 2 that searches for subtrajectories within it that start with a given state \(s\), are of length \(N=4t_{\mathrm{mix}}(\log T)\), and are at least \(N\) distance apart from each other. Assume that there are \(M\) such subtrajectories. Let the total reward and cost of the \(i\)th subtrajectory be \(\{r_{i},c_{i}\}\) respectively and \(\tau_{i}\) be its starting time. The value function estimates for the \(k\)th epoch are

\[\hat{Q}_{g}^{\pi_{\theta_{k}}}(s,a)=\frac{1}{\pi_{\theta_{k}}(a|s)}\left[ \frac{1}{M}\sum_{i=1}^{M}g_{i}1(a_{\tau_{i}}=a)\right],\;\hat{V}_{g}^{\pi_{ \theta_{k}}}(s)=\frac{1}{M}\sum_{i=1}^{M}g_{i},\;\;\forall g\in\{r,c\}\] (13)

This leads to the following advantage estimator.

\[\hat{A}_{\mathrm{L},\lambda_{k}}^{\pi_{\theta_{k}}}(s,a)=\hat{A}_{r}^{\pi_{ \theta_{k}}}(s,a)+\lambda_{k}\hat{A}_{c}^{\pi_{\theta_{k}}}(s,a),\] (14)

where \(\hat{A}_{g}^{\pi_{\theta_{k}}}(s,a)=\hat{Q}_{g}^{\pi_{\theta_{k}}}(s,a)-\hat{ V}_{g}^{\pi_{\theta_{k}}}(s)\), \(g\in\{r,c\}\). Finally, the gradient estimator is,

\[\omega_{k}\triangleq\hat{\nabla}_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda_{ k})=\frac{1}{H}\sum_{t=t_{k}}^{t_{k+1}-1}\hat{A}_{\mathrm{L},\lambda_{k}}^{ \pi_{\theta_{k}}}(s_{t},a_{t})\nabla_{\theta}\log\pi_{\theta_{k}}(a_{t}|s_{t})\] (15)

where \(t_{k}=(k-1)H\) is the starting time of the \(k\)th epoch. The parameters are updated following (12). To update the Lagrange multiplier, we need an estimation of \(J_{c}(\theta_{k})\), which is obtained as the average cost of the \(k\)th epoch. It should be noted that we remove the first \(N\) samples from the \(k\)th epoch because we require the state distribution emanating from the remaining samples to be close enough to the stationary distribution \(d^{\pi_{\theta_{k}}}\), which is the key to make \(\hat{J}_{c}(\theta_{k})\) close to \(J_{c}(\theta_{k})\). The following lemma demonstrates that \(\hat{A}_{\mathrm{L},\lambda_{k}}^{\pi_{\theta_{k}}}(s,a)\) is a good estimator of \(A_{\mathrm{L},\lambda_{k}}^{\pi_{\theta_{k}}}(s,a)\).

```
1:Input: Trajectory \((s_{t_{1}},a_{t_{1}},\ldots,s_{t_{2}},a_{t_{2}})\), state \(s\), action \(a\), Lagrange multiplier \(\lambda\), and parameter \(\theta\)
2:Initialize:\(M\gets 0\), \(\tau\gets t_{1}\)
3:Define:\(N=4t_{\mathrm{mix}}\log_{2}T\).
4:while\(\tau\leq t_{2}-N\)do
5:if\(s_{\tau}=s\)then
6:\(M\gets M+1,\;\tau_{M}\leftarrow\tau\)
7:\(g_{M}\leftarrow\sum_{t=\tau}^{\tau+N-1}\hskip-14.226378ptg(s_{t},a_{t}),\; \forall g\in\{r,c\}\)
8:\(\tau\leftarrow\tau+2N\).
9:else
10:\(\tau\leftarrow\tau+1\).
11:endif
12:endwhile
13:if\(M>0\)then
14: Compute \(\hat{Q}_{g}(s,a)\), \(\hat{V}_{g}(s)\) via (13), \(\forall g\in\{r,c\}\)
15:else
16:\(\hat{V}_{g}(s)=0\), \(\hat{Q}_{g}(s,a)=0,\;\;\forall g\in\{r,c\}\)
17:endif
18:return\((\hat{Q}_{r}(s,a)-\hat{V}_{r}(s))+\lambda(\hat{Q}_{c}(s,a)-\hat{V}_{c}(s))\) ```

**Algorithm 2** Advantage Estimation

**Lemma 2**.: _The following inequality holds \(\forall k\), \(\forall(s,a)\) and sufficiently large \(T\)._

\[\mathbf{E}\bigg{[}\bigg{(}\hat{A}_{\mathrm{L},\lambda_{k}}^{\pi_{\theta_{k}}}(s, a)-A_{\mathrm{L},\lambda_{k}}^{\pi_{\theta_{k}}}(s,a)\bigg{)}^{2}\bigg{]}\leq\mathcal{O} \left(\frac{t_{\mathrm{hit}}N^{3}\log T}{\delta^{2}H\pi_{\theta_{k}}(a|s)} \right)=\mathcal{O}\left(\frac{t_{\mathrm{mix}}^{2}(\log T)^{2}}{\delta^{2}T ^{\xi}\pi_{\theta_{k}}(a|s)}\right)\] (16)

[MISSING_PAGE_FAIL:7]

**Lemma 4**.: _With a slight abuse of notation, let \(J_{\mathrm{L}}(\pi,\lambda)=J_{r}^{\pi}+\lambda J_{\pi}^{\pi}\). For any two policies \(\pi\), \(\pi^{\prime}\), the following result holds \(\forall\lambda>0\)._

\[J_{\mathrm{L}}(\pi,\lambda)-J_{\mathrm{L}}(\pi^{\prime},\lambda)=\mathbf{E}_{s \sim d^{*}}\mathbf{E}_{a\sim\pi(s)}\big{[}A_{\mathrm{L},\lambda}^{\pi^{\prime} }(s,a)\big{]}\]

We now present a general framework for the convergence analysis of Algorithm 1.

**Lemma 5**.: _If the policy parameters, \(\{\theta_{k},\lambda_{k}\}_{k=1}^{K}\) are updated via (12) and assumptions 3, 4,and 5 hold, then we have the following inequality for any \(K\),_

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{(}J_{\mathrm{L}}(\pi^{ *},\lambda_{k})-J_{\mathrm{L}}(\theta_{k},\lambda_{k})\bigg{)} \leq\sqrt{\epsilon_{\mathrm{bias}}}+\frac{G}{K}\sum_{k}^{K} \mathbf{E}\|(\omega_{k}-\omega_{k}^{*})\|+\frac{B\alpha}{2K}\sum_{k=1}^{K} \mathbf{E}\|\omega_{k}\|^{2}\] \[+\frac{1}{\alpha K}\mathbf{E}_{s\sim d^{*^{*}}}[KL(\pi^{*}(\cdot |s)\|\pi_{\theta_{1}}(\cdot|s))]\]

_where \(\omega_{k}^{*}:=\omega_{\theta_{k},\lambda_{k}}^{*}\), \(\omega_{\theta_{k},\lambda_{k}}^{*}\) is defined in (19), and \(\pi^{*}\) is the optimal solution to the problem (2)._

Lemma 5 proves that the optimality error of the Lagrange sequence can be bounded by the average first-order and second-order norms of the intermediate gradients. Note the presence of \(\epsilon_{\mathrm{bias}}\) in the result. If the policy class is severely restricted, the optimality bound loses its importance. Consider the expectation of the second term in (20). Note that,

\[\bigg{(}\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\|\omega_{k}-\omega_{k }^{*}\|\bigg{)}^{2}\leq\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{[}\|\omega_{k }-\omega_{k}^{*}\|^{2}\bigg{]}=\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{[}\| \omega_{k}-F_{\rho}(\theta_{k})^{\dagger}\nabla_{\theta}J_{\mathrm{L}}(\theta _{k},\lambda_{k})\|^{2}\bigg{]}\] \[\leq\frac{2}{K}\sum_{k=1}^{K}\Bigg{\{}\mathbf{E}\bigg{[}\|\omega_ {k}-\nabla_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2}\bigg{]}+ \mathbf{E}\bigg{[}\|\nabla_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda_{k})-F_ {\rho}(\theta_{k})^{\dagger}\nabla_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda_ {k})\|^{2}\bigg{]}\Bigg{\}}\] \[\overset{(a)}{\leq}\frac{2}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{[} \|\omega_{k}-\nabla_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2} \bigg{]}+\frac{2}{K}\sum_{k=1}^{K}\bigg{(}1+\frac{1}{\mu_{F}^{2}}\bigg{)} \,\mathbf{E}\bigg{[}\|\nabla_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^ {2}\bigg{]}\]

where \((a)\) follows from Assumption 5. The expectation of the third term in (20) can be bounded as

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{[}\|\omega_{k}\|^{2}\bigg{]}\leq\frac {1}{K}\sum_{k=1}^{K}\mathbf{E}\left[\|\nabla_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2}\right]+\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{[}\|\omega _{k}-\nabla_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2}\bigg{]}\]

In both (4) and (20), \(\mathbf{E}\|\omega_{k}-\nabla_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda_{k}) \|^{2}\) is bounded above by Lemma 3. To bound the term, \(\mathbf{E}\|\nabla_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2}\), the following lemma is applied.

**Lemma 6**.: _Let \(J_{g}(\cdot)\) be L-smooth, \(\forall g\in\{r,c\}\) and \(\alpha=\frac{1}{4L(1+\frac{3}{2})}\). Then the following holds._

\[\frac{1}{K}\sum_{k=1}^{K}\|\nabla_{\theta}J_{\mathrm{L}}(\theta_{k},\lambda_{k })\|^{2}\leq\frac{288L}{\delta^{2}K}+\frac{3}{K}\sum_{k=1}^{K}\|\nabla_{\theta }J_{\mathrm{L}}(\theta_{k},\lambda_{k})-\omega_{k}\|^{2}+\beta\] (20)

Note the presence of \(\beta\) in (20). To ensure convergence, \(\beta\) must be a function of \(T\). Invoking Lemma 3, we get the following relation under the same set of assumptions and the choice of parameters as in Lemma 6.

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\|\nabla_{\theta}J_{\mathrm{L}}(\theta_{k}, \lambda_{k})\|^{2}\leq\tilde{\mathcal{O}}\left(\frac{AG^{2}t_{\mathrm{mix}}^{ 2}}{\delta^{2}T^{\xi}}\right)+\tilde{\mathcal{O}}\left(\frac{Lt_{\mathrm{mix}} t_{\mathrm{hit}}}{\delta^{2}T^{1-\xi}}\right)+\beta\] (21)

Applying Lemma 3 and (21) in (20), we arrive at,

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{[}\|\omega_{k}\|^{2}\bigg{]}\leq \tilde{\mathcal{O}}\left(\frac{AG^{2}t_{\mathrm{mix}}^{2}}{\delta^{2}T^{\xi}}+ \frac{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}{\delta^{2}T^{1-\xi}}\right)+\beta\] (22)

Similarly, using \((\ref{eq:L-smooth})\), we deduce the following.

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\|\omega_{k}-\omega_{k}^{*}\|\leq\left(1+ \frac{1}{\mu_{F}}\right)\sqrt{\beta}+\left(1+\frac{1}{\mu_{F}}\right)\tilde{ \mathcal{O}}\left(\frac{\sqrt{AG}t_{\mathrm{mix}}}{\delta T^{\xi/2}}+\frac{ \sqrt{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}}{\delta T^{(1-\xi)/2}}\right)\] (23)

Inequalities (22) and (23) lead to the following global convergence of the Lagrange function.

**Lemma 7**.: _Let \(\{\theta_{k}\}_{k=1}^{K}\) be as described in Lemma 5. If assumptions 1\(-\)5 hold, \(\{J_{g}(\cdot)\}_{g\in\{r,c\}}\) are \(L\)-smooth functions, \(\alpha=\frac{1}{4L(1+\frac{\delta}{\delta})}\), \(K=\frac{T}{H}\), and \(H=16t_{\rm mix}t_{\rm hit}T^{\xi}(\log_{2}T)^{2}\), then_

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{(}J_{\rm L}(\pi^{*}, \lambda_{k})-J_{\rm L}(\theta_{k},\lambda_{k})\bigg{)}\leq G\left(1+\frac{1}{ \mu_{F}}\right)\tilde{\mathcal{O}}\left(\sqrt{\beta}+\frac{\sqrt{A}Gt_{\rm mix }}{\delta T^{\xi/2}}+\frac{\sqrt{Lt_{\rm mix}t_{\rm hit}}}{\delta T^{(1-\xi)/2 }}\right)\] \[+\frac{B}{L}\tilde{\mathcal{O}}\left(\frac{AG^{2}t_{\rm mix}^{2} }{\delta^{2}T^{\xi}}+\frac{Lt_{\rm mix}t_{\rm hit}}{\delta^{2}T^{1-\xi}}+ \beta\right)+\tilde{\mathcal{O}}\bigg{(}\frac{Lt_{\rm mix}t_{\rm hit}\mathbf{ E}_{s\sim d^{*}}[KL(\pi^{*}(\cdot|s))\|\pi_{\theta_{1}}(\cdot|s))]}{T^{1-\xi} \delta}\bigg{)}+\sqrt{\epsilon_{\rm bias}}\]

Lemma 7 establishes that the average difference between \(J_{\rm L}(\pi^{*},\lambda_{k})\) and \(J_{\rm L}(\theta_{k},\lambda_{k})\) is \(\tilde{\mathcal{O}}(\sqrt{\beta}+T^{-\xi/2}+T^{-(1-\xi)/2})\). Expanding the function, \(J_{\rm L}\), and utilizing the update rule of the Lagrange multiplier, we achieve the global convergence for the objective and the constraint in Theorem 1 (stated below). In its proof, Lemma 18 (stated in the appendix) serves as an important tool in disentangling the convergence rates of regret and constraint violation. Interestingly, Lemma 18 is built upon the strong duality property of the unparameterized optimization (2) and has no apparent direct connection with the parameterized setup.

**Theorem 1**.: _Consider the same parameters as in Lemma 7 and set \(\beta=T^{-2/5}\), \(\xi=2/5\). We have,_

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{(}J_{r}^{\pi^{*}}-J_{r} (\theta_{k})\bigg{)}\leq\sqrt{\epsilon_{\rm bias}}+\frac{\sqrt{A}G^{2}t_{\rm mix }}{\delta}\left(1+\frac{1}{\mu_{F}}\right)\tilde{\mathcal{O}}\left(T^{-1/5}\right)\] \[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{(}-J_{c}(\theta_{k}) \bigg{)}\leq\delta\sqrt{\epsilon_{\rm bias}}+\tilde{\mathcal{O}}\left(\frac{t_ {\rm mix}t_{\rm hit}}{\delta T^{1/5}}\right)+\sqrt{A}G^{2}t_{\rm mix}\left(1+ \frac{1}{\mu_{F}}\right)\tilde{\mathcal{O}}\left(T^{-1/5}\right)\]

_where \(\pi^{*}\) is a solution to (2). In the above bounds, we write only the dominating terms of \(T\)._

Theorem 1 establishes \(\tilde{\mathcal{O}}(T^{-1/5})\) convergence rates for both the objective and the constraint violation.

## 5 Regret and Violation Analysis

In this section, we use the convergence result of the previous section to bound the expected regret and constraint violation of Algorithm 1. Note that the regret and constraint violation decompose as,

\[\mathrm{Reg}_{T} =\sum_{t=0}^{T-1}\left(J_{r}^{\pi^{*}}-r(s_{t},a_{t})\right)=H \sum_{k=1}^{K}\left(J_{r}^{\pi^{*}}-J(\theta_{k})\right)+\sum_{k=1}^{K}\sum_{t \in\mathcal{I}_{k}}\left(J(\theta_{k})-r(s_{t},a_{t})\right)\] \[\mathrm{Vio}_{T} =\sum_{t=0}^{T-1}\left(-c(s_{t},a_{t})\right)=H\sum_{k=1}^{K} \left(-J_{c}(\theta_{k})\right)+\sum_{k=1}^{K}\sum_{t\in\mathcal{I}_{k}}\left( J_{c}(\theta_{k})-c(s_{t},a_{t})\right)\]

where \(\mathcal{I}_{k}\triangleq\{(k-1)H,\cdots,kH-1\}\). Observe that the expectation of the first terms in regret and violation can be bounded by Theorem 1. The expectation of the second term in regret and violation can be expanded as follows,

\[\mathbf{E}\left[\sum_{k=1}^{K}\sum_{t\in\mathcal{I}_{k}}\left(J_{ g}(\theta_{k})-g(s_{t},a_{t})\right)\right]\overset{(a)}{=}\mathbf{E}\left[ \sum_{k=1}^{K}\sum_{t\in\mathcal{I}_{k}}\mathbf{E}_{s^{\prime}\sim P(\cdot|s_{ t},a_{t})}[V_{g}^{\pi_{\theta_{k}}}(s^{\prime})]-Q_{g}^{\pi_{\theta_{k}}}(s_{t},a_{t})\right]\] \[\overset{(b)}{=}\mathbf{E}\left[\sum_{k=t}^{K}\sum_{t\in \mathcal{I}_{k}}V_{g}^{\pi_{\theta_{k}}}(s_{t+1})-V_{g}^{\pi_{\theta_{k}}}(s_{t })\right]=\mathbf{E}\left[\sum_{k=1}^{K}V_{g}^{\pi_{\theta_{k}}}(s_{kH})-V_{g}^ {\pi_{\theta_{k}}}(s_{(k-1)H})\right]\] (24) \[=\mathbf{E}\left[\sum_{k=1}^{K-1}V_{g}^{\pi_{\theta_{k+1}}}(s_{kH })-V_{g}^{\pi_{\theta_{k}}}(s_{kH})\right]+\mathbf{E}\left[V_{g}^{\pi_{\theta_{K} }}(s_{T})-V_{g}^{\pi_{\theta_{0}}}(s_{0})\right]\]

where \(g\in\{r,c\}\). Equality \((a)\) uses the Bellman equation and \((b)\) follows from the definition of \(Q_{g}\). The first term in the last line of Eq. (24) can be upper bounded by Lemma 8 (stated below). On the other hand, the second term can be upper bounded as \(\mathcal{O}(t_{\rm mix})\) using Lemma 9.

**Lemma 8**.: _If assumptions 1 and 3 hold, then for \(K=\frac{T}{H}\) where \(H=16t_{\mathrm{mix}}t_{\mathrm{hit}}T^{\frac{3}{5}}(\log_{2}T)^{2}\), the following inequalities hold \(\forall k\), \(\forall(s,a)\) and sufficiently large \(T\):_

\[(a)\left|\pi_{\theta_{k+1}}(a|s)-\pi_{\theta_{k}}(a|s)\right|\leq G \|\theta_{k+1}-\theta_{k}\|\] \[(b)\sum_{k=1}^{K}\mathbf{E}|J_{g}(\theta_{k+1})-J_{g}(\theta_{k}) |\leq\tilde{\mathcal{O}}\left(\frac{\alpha AG}{\delta t_{\mathrm{hit}}}\left[ \left(\sqrt{AG}t_{\mathrm{mix}}+\delta\right)T^{\frac{2}{5}}+\sqrt{Lt_{ \mathrm{mix}}t_{\mathrm{hit}}}T^{\frac{3}{10}}\right]\right)\] \[(c)\sum_{k=1}^{K}\mathbf{E}|V_{g}^{\pi_{\theta_{k+1}}}(s_{k})-V_ {g}^{\pi_{\theta_{k}}}(s_{k})|\leq\tilde{\mathcal{O}}\left(\frac{\alpha AG_{ \mathrm{mix}}}{\delta t_{\mathrm{hit}}}\left[\left(\sqrt{AG}t_{\mathrm{mix}}+ \delta\right)T^{\frac{2}{5}}+\sqrt{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}T^{\frac {3}{10}}\right]\right)\]

_where \(g\in\{r,c\}\), and \(\{s_{k}\}_{k=1}^{K}\) is an arbitrary sequence of states._

Lemma 8 states that the obtained policy parameters are such that the average consecutive difference in the sequence \(\{J_{g}(\theta_{k})\}_{k=1}^{K}\), \(g\in\{r,c\}\) decreases with time horizon, \(T\). We would like to emphasize that Lemma 8 works for both reward and constraint functions. Hence, we can prove our regret guarantee and constraint violation as shown below.

**Theorem 2**.: _If assumptions 1-5 hold, \(J_{g}(\cdot)\)'s are \(L\)-smooth, \(\forall g\in\{r,c\}\) and \(T\) are sufficiently large, then our proposed Algorithm 1 achieves the following expected regret and constraint violation bounds with learning rates \(\alpha=\frac{1}{4L(1+\frac{1}{\delta})}\) and \(\beta=T^{-2/5}\)._

\[\mathbf{E}\left[\mathrm{Reg}_{T}\right] \leq T\sqrt{\epsilon_{\mathrm{bias}}}+\tilde{\mathcal{O}}(T^{4/5} )+\mathcal{O}(t_{\mathrm{mix}})\] (25) \[\mathbf{E}\left[\mathrm{Vio}_{T}\right] \leq T\delta\sqrt{\epsilon_{\mathrm{bias}}}+\tilde{\mathcal{O}}(T ^{4/5})+\mathcal{O}(t_{\mathrm{mix}})\] (26)

The detailed expressions of these bounds are provided in the Appendix. Here, we keep only those terms that emphasize the order of \(T\). Note that our result outperforms the state-of-the-art model-free tabular result in average-reward CMDP [8]. However, our regret bound is worse than that achievable in average reward unconstrained MDP with general parameterization [13]. Interestingly, the gap between the convergence results of constrained and unconstrained setups is a common observation across the literature. For example, in the tabular model-free average reward MDP, the state-of-the-art regret bound for unconstrained setup, \(\tilde{\mathcal{O}}(T^{1/2})\)[17], is better than that in the constrained setup, \(\tilde{\mathcal{O}}(T^{5/6})\)[8].

## 6 Conclusions

This paper establishes the first sublinear regret and constraint violation bounds in the average reward CMDP setup with general parametrization (and do not assume the underlying constrained Markov Decision Process (CMDP) to be tabular or linear). We show that our proposed algorithm achieves \(\tilde{\mathcal{O}}(T^{4/5})\) regret and constraint violation bounds where \(T\) is the time horizon. Note that the state of the art in unconstrained counterpart is \(\tilde{\mathcal{O}}(T^{3/4})\). Closing this gap by designing more efficient algorithms is an open question in the average reward CMDP literature with the general parametrization. Moreover, our current algorithm requires the knowledge of mixing time. Relaxing such assumptions is another important future direction in realistic settings. For further discussions on future work directions, the readers are referred to [35].

## 7 Acknowledgement

This research was supported in part by the National Science Foundation under grant CCF-2149588 and Cisco, Inc.

## References

* [1] Liu, C., N. Geng, et al. Cmix: Deep multi-agent reinforcement learning with peak and average constraints. In _Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part I 21_, pages 157-173. Springer, 2021.

* [2] Al-Abbasi, A. O., A. Ghosh, V. Aggarwal. Deeppool: Distributed model-free algorithm for ride-sharing using deep reinforcement learning. _IEEE Transactions on Intelligent Transportation Systems_, 20(12):4714-4727, 2019.
* [3] Ling, L., W. U. Mondal, S. V. Ukkusuri. Cooperating graph neural networks with deep reinforcement learning for vaccine prioritization. _arXiv preprint arXiv:2305.05163_, 2023.
* [4] Bai, Q., A. S. Bedi, V. Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 6737-6744. 2023.
* [5] Agarwal, M., Q. Bai, V. Aggarwal. Concave utility reinforcement learning with zero-constraint violations. _Transactions on Machine Learning Research_, 2022.
* [6] Chen, L., R. Jain, H. Luo. Learning infinite-horizon average-reward markov decision process with constraints. In _International Conference on Machine Learning_, pages 3246-3270. PMLR, 2022.
* [7] Agarwal, M., Q. Bai, V. Aggarwal. Regret guarantees for model-based reinforcement learning with long-term average constraints. In _Uncertainty in Artificial Intelligence_, pages 22-31. PMLR, 2022.
* [8] Wei, H., X. Liu, L. Ying. A provably-efficient model-free algorithm for infinite-horizon average-reward constrained markov decision processes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 3868-3876. 2022.
* [9] Ghosh, A., X. Zhou, N. Shroff. Achieving sub-linear regret in infinite horizon average reward constrained mdp with linear function approximation. In _The Eleventh International Conference on Learning Representations_. 2023.
* [10] Agarwal, A., S. M. Kakade, J. D. Lee, G. Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. _The Journal of Machine Learning Research_, 22(1):4431-4506, 2021.
* [11] Mondal, W. U., V. Aggarwal. Improved sample complexity analysis of natural policy gradient algorithm with general parameterization for infinite horizon discounted reward markov decision processes. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_. 2024. 2024.
* [12] --. Sample-efficient constrained reinforcement learning with general parameterization. _arXiv preprint arXiv:2405.10624_, 2024.
* [13] Bai, Q., W. U. Mondal, V. Aggarwal. Regret analysis of policy gradient algorithm for infinite horizon average reward markov decision processes. In _Proceedings of the AAAI Conference on Artificial Intelligence_. 2024.
* [14] Ding, D., K. Zhang, T. Basar, M. Jovanovic. Natural policy gradient primal-dual method for constrained markov decision processes. _Advances in Neural Information Processing Systems_, 33:8378-8390, 2020.
* [15] Agrawal, S., R. Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. _Advances in Neural Information Processing Systems_, 30, 2017.
* [16] Auer, P., T. Jaksch, R. Ortner. Near-optimal regret bounds for reinforcement learning. _Advances in neural information processing systems_, 21, 2008.
* [17] Wei, C.-Y., M. J. Jahromi, H. Luo, H. Sharma, R. Jain. Model-free reinforcement learning in infinite-horizon average-reward markov decision processes. In _International conference on machine learning_, pages 10170-10180. PMLR, 2020.
* [18] Bai, Q., A. S. Bedi, M. Agarwal, A. Koppel, V. Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 3682-3689. 2022.
* [19] Xu, T., Y. Liang, G. Lan. Crpo: A new approach for safe reinforcement learning with convergence guarantee. In _International Conference on Machine Learning_, pages 11480-11491. PMLR, 2021.
* [20] Efroni, Y., S. Mannor, M. Pirotta. Exploration-exploitation in constrained mdps. _arXiv preprint arXiv:2003.02189_, 2020.

* [21] Qiu, S., X. Wei, Z. Yang, J. Ye, Z. Wang. Upper confidence primal-dual reinforcement learning for cmdp with adversarial loss. _Advances in Neural Information Processing Systems_, 33:15277-15287, 2020.
* [22] Germano, J., F. E. Stradi, G. Genalti, M. Castiglioni, A. Marchesi, N. Gatti. A best-of-both-worlds algorithm for constrained mdps with long-term constraints. _arXiv preprint arXiv:2304.14326_, 2023.
* [23] Pesquerel, F., O.-A. Maillard. Imed-rl: Regret optimal learning of ergodic markov decision processes. In _NeurIPS 2022-Thirty-sixth Conference on Neural Information Processing Systems_. 2022.
* [24] Gong, H., M. Wang. A duality approach for regret minimization in average-award ergodic markov decision processes. In _Learning for Dynamics and Control_, pages 862-883. PMLR, 2020.
* [25] Sutton, R. S., D. McAllester, S. Singh, Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. _Advances in neural information processing systems_, 12, 1999.
* [26] Lattimore, T., C. Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [27] Agarwal, A., S. M. Kakade, J. D. Lee, G. Mahajan. Optimality and approximation with policy gradient methods in markov decision processes. In _Conference on Learning Theory_, pages 64-66. PMLR, 2020.
* [28] Zhang, J., C. Ni, C. Szepesvari, M. Wang. On the convergence and sample efficiency of variance-reduced policy gradient method. _Advances in Neural Information Processing Systems_, 34:2228-2240, 2021.
* [29] Liu, Y., K. Zhang, T. Basar, W. Yin. An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods. _Advances in Neural Information Processing Systems_, 33:7624-7636, 2020.
* [30] Jin, C., Z. Yang, Z. Wang, M. I. Jordan. Provably efficient reinforcement learning with linear function approximation. In J. Abernethy, S. Agarwal, eds., _Proceedings of Thirty Third Conference on Learning Theory_, vol. 125 of _Proceedings of Machine Learning Research_, pages 2137-2143. PMLR, 2020.
* [31] Wang, L., Q. Cai, Z. Yang, Z. Wang. Neural policy gradient methods: Global optimality and rates of convergence. In _International Conference on Learning Representations_. 2019.
* [32] Yuan, R., R. M. Gower, A. Lazaric. A general sample complexity analysis of vanilla policy gradient. In _International Conference on Artificial Intelligence and Statistics_, pages 3332-3380. PMLR, 2022.
* [33] Fatkhullin, I., A. Barakat, A. Kireeva, N. He. Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies. In _International Conference on Machine Learning_, pages 9827-9869. PMLR, 2023.
* [34] Mondal, W. U., V. Aggarwal, S. V. Ukkusuri. Mean-field control based approximation of multi-agent reinforcement learning in presence of a non-decomposable shared global state. _Transactions on Machine Learning Research_, 2023.
* [35] Aggarwal, V., W. U. Mondal, Q. Bai. Constrained reinforcement learning with average reward objective: Model-based and model-free algorithms. _Found. Trends Optim._, 6(4):193-298, 2024.
* [36] Dorfman, R., K. Y. Levy. Adapting to mixing time in stochastic optimization with markovian data. In _International Conference on Machine Learning_, pages 5429-5446. PMLR, 2022.
* [37] Ding, D., K. Zhang, J. Duan, T. Basar, M. R. Jovanovic. Convergence and sample complexity of natural policy gradient primal-dual methods for constrained mdps. _arXiv preprint arXiv:2206.02346_, 2022.
* [38] Bai, Q., V. Aggarwal, A. Gattami. Provably sample-efficient model-free algorithm for mdps with peak constraints. _Journal of Machine Learning Research_, 24(60):1-25, 2023.

Proofs for Lemmas in Section 3

### Proof of Lemma 1

Since the first step of the proof works in the same way for functions \(J_{r}\) and \(J_{c}\), we use the generic notations \(J_{g},V_{g},Q_{g}\) where \(g=r,c\) and derive the following.

\[\nabla_{\theta}V_{g}^{\pi_{\theta_{k}}}(s)=\nabla_{\theta}\bigg{(} \sum_{a}\pi_{\theta}(a|s)Q_{g}^{\pi_{\theta}}(s,a)\bigg{)}\] (27) \[=\sum_{a}\bigg{(}\nabla_{\theta}\pi_{\theta}(a|s)\bigg{)}Q_{g}^{ \pi_{\theta}}(s,a)+\sum_{a}\pi_{\theta}(a|s)\nabla_{\theta}Q_{g}^{\pi_{\theta} }(s,a)\] \[\overset{(a)}{=}\sum_{a}\pi_{\theta}(a|s)\bigg{(}\nabla_{\theta} \log\pi_{\theta}(a|s)\bigg{)}Q_{g}^{\pi_{\theta}}(s,a)+\sum_{a}\pi_{\theta}(a| s)\nabla_{\theta}\bigg{(}g(s,a)-J_{g}(\theta)+\sum_{s^{\prime}}P(s^{\prime}|s,a)V_{g}^{ \pi_{\theta}}(s^{\prime})\bigg{)}\] \[=\sum_{a}\pi_{\theta}(a|s)\bigg{(}\nabla_{\theta}\log\pi_{\theta }(a|s)\bigg{)}Q_{g}^{\pi_{\theta}}(s,a)+\sum_{a}\pi_{\theta}(a|s)\bigg{(}\sum_ {s^{\prime}}P(s^{\prime}|s,a)\nabla_{\theta}V_{g}^{\pi_{\theta}}(s^{\prime}) \bigg{)}-\nabla_{\theta}J_{g}(\theta)\]

where the step (a) is a consequence of \(\nabla_{\theta}\log\pi_{\theta}=\frac{\nabla\pi_{\theta}}{\pi_{\theta}}\) and the Bellman equation. Multiplying both sides by \(d^{\pi_{\theta}}(s)\), taking a sum over \(s\in\mathcal{S}\), and rearranging the terms, we obtain the following.

\[\nabla_{\theta}J_{g}(\theta)=\sum_{s}d^{\pi_{\theta}}(s)\nabla_{ \theta}J_{g}(\theta)\] (28) \[=\sum_{s}d^{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(a|s)\bigg{(} \nabla_{\theta}\log\pi_{\theta}(a|s)\bigg{)}Q_{g}^{\pi_{\theta}}(s,a)+\sum_{s} d^{\pi_{\theta}}(s)\sum_{a}\pi_{\theta}(a|s)\bigg{(}\sum_{s^{\prime}}P(s^{\prime}|s,a) \nabla_{\theta}V_{g}^{\pi_{\theta}}(s^{\prime})\bigg{)}\] \[-\sum_{s}d^{\pi_{\theta}}(s)\nabla_{\theta}V_{g}^{\pi_{\theta}}(s)\] \[=\mathbf{E}_{s\sim d^{\pi_{\theta}},a\sim\pi_{\theta}(\cdot|s)} \bigg{[}Q_{g}^{\pi_{\theta}}(s,a)\nabla_{\theta}\log\pi_{\theta}(a|s)\bigg{]}+ \sum_{s}d^{\pi_{\theta}}(s)\sum_{s^{\prime}}P^{\pi_{\theta}}(s^{\prime}|s) \nabla_{\theta}V_{g}^{\pi_{\theta}}(s^{\prime})-\sum_{s}d^{\pi_{\theta}}(s) \nabla_{\theta}V_{g}^{\pi_{\theta}}(s)\] \[\overset{(a)}{=}\mathbf{E}_{s\sim d^{\pi_{\theta}},a\sim\pi_{ \theta}(\cdot|s)}\bigg{[}Q_{g}^{\pi_{\theta}}(s,a)\nabla_{\theta}\log\pi_{ \theta}(a|s)\bigg{]}+\sum_{s^{\prime}}d^{\pi_{\theta}}(s^{\prime})\nabla_{ \theta}V_{g}^{\pi_{\theta}}(s^{\prime})-\sum_{s}d^{\pi_{\theta}}(s)\nabla_{ \theta}V_{g}^{\pi_{\theta}}(s)\] \[=\mathbf{E}_{s\sim d^{\pi_{\theta}},a\sim\pi_{\theta}(\cdot|s)} \bigg{[}Q_{g}^{\pi_{\theta}}(s,a)\nabla_{\theta}\log\pi_{\theta}(a|s)\bigg{]}\]

where \((a)\) uses the fact that \(d^{\pi_{\theta}}\) is a stationary distribution. Note that,

\[\mathbf{E}_{s\sim d^{\pi_{\theta}},a\sim\pi_{\theta}(\cdot|s)} \bigg{[}V_{g}^{\pi_{\theta}}(s)\nabla\log\pi_{\theta}(a|s)\bigg{]}\] (29) \[=\mathbf{E}_{s\sim d^{\pi_{\theta}}}\left[\sum_{a\in\mathcal{A}} V_{g}^{\pi_{\theta}}(s)\nabla_{\theta}\pi_{\theta}(a|s)\right]\] \[=\mathbf{E}_{s\sim d^{\pi_{\theta}}}\Big{[}V_{g}^{\pi_{\theta}}(s )\nabla_{\theta}\left(\sum_{a\in\mathcal{A}}\pi_{\theta}(a|s)\right)\bigg{]}= \mathbf{E}_{s\sim d^{\pi_{\theta}}}\bigg{[}V_{g}^{\pi_{\theta}}(s)\nabla_{ \theta}(1)\bigg{]}=0\]

We can, therefore, replace the function \(Q_{g}^{\pi_{\theta}}\) in the policy gradient with the advantage function \(A_{g}^{\pi_{\theta}}(s,a)=Q_{g}^{\pi_{\theta}}(s,a)-V_{g}^{\pi_{\theta}}(s), \forall(s,a)\in\mathcal{S}\times\mathcal{A}\). Thus,

\[\nabla_{\theta}J_{g}(\theta)=\mathbf{E}_{s\sim d^{\pi_{\theta}},a\sim\pi_{ \theta}(\cdot|s)}\bigg{[}A_{g}^{\pi_{\theta}}(s,a)\nabla_{\theta}\log\pi_{ \theta}(a|s)\bigg{]}\] (30)

The proof is completed using the definitions of \(J_{\mathrm{L},\lambda}\) and \(A_{\mathrm{L},\lambda}\).

### Proof of Lemma 2

Proof.: The proof is similar to the proof of [17, Lemma 6]. Consider the \(k\)th epoch and assume that \(\pi_{\theta_{k}}\) is denoted as \(\pi\) for notational convenience. Let, \(M\) be the number of disjoint sub-trajectories of 

[MISSING_PAGE_EMPTY:14]

Note that (35) cannot be directly used to bound the bias of \(\hat{A}_{g}^{\pi}(s,a)\). This is because the random variable \(M\) is correlated with the variables \(\{g_{k,i}\}_{i=1}^{M}\). To decorrelate them, imagine a CMDP where the state distribution resets to the stationary distribution, \(d^{\pi}\) after exactly \(N\) time steps since the completion of a sub-trajectory. In other words, if a sub-trajectory starts at \(\tau_{i}\), and ends at \(\tau_{i}+N\), then the system'rests' for additional \(N\) steps before rejuvenating with the state distribution, \(d^{\pi}\) at \(\tau_{i}+2N\). Clearly, the wait time between the reset after the \((i-1)\)th sub-trajectory and the start of the \(i\)th sub-trajectory is, \(w_{i}=\tau_{i}-(\tau_{i-1}+2N)\), \(i>1\). Let \(w_{1}\) be the difference between the start time of the \(k\)th epoch and the start time of the first sub-trajectory. Note that,

\((a)\)\(w_{1}\) only depends on the initial state, \(s_{(k-1)H}\) and the induced transition function, \(P^{\pi}\),

\((b)\)\(w_{i}\), where \(i>1\), depends on the stationary distribution, \(d^{\pi}\), and the induced transition function, \(P^{\pi}\),

\((c)\)\(M\) only depends on \(\{w_{1},w_{2},\cdots\}\) as other segments of the epoch have fixed length, \(2N\).

Clearly, in this imaginary CMDP, the sequence, \(\{w_{1},w_{2},\cdots\}\), and hence, \(M\) is independent of \(\{g_{k,1},g_{k,2},\cdots\}\). Let, \(\mathbf{E}^{\prime}\) denote the expectation operation and \(\Pr^{\prime}\) denote the probability of events in this imaginary system. Define the following.

\[\Delta_{i}\triangleq\frac{g_{k,i}1(a_{\tau_{i}}=a)}{\pi(a|s)}-g_{k,i}-A_{g}^{ \pi}(s,a)+\Delta_{T}^{\pi}(s,a)\] (36)

where \(\Delta_{T}^{\pi}(s,a)\) is given in (34). Note that we have suppressed the dependence on \(T\), \(s,a\), and \(\pi\) while defining \(\Delta_{i}\) to remove clutter. Using (34), one can write \(\mathbf{E}^{\prime}\left[\Delta_{i}(s,a)|\{w_{i}\}\right]=0\). Moreover,

\[\begin{split}&\mathbf{E}^{\prime}\left[\left(\hat{A}_{g}^{\pi}(s,a )-A_{g}^{\pi}(s,a)\right)^{2}\right]\\ &=\mathbf{E}^{\prime}\left[\left(\hat{A}_{g}^{\pi}(s,a)-A_{g}^{ \pi}(s,a)\right)^{2}\left|M>0\right]\times\Pr^{\prime}(M>0)+\left(A_{g}^{\pi}(s,a)\right)^{2}\times\Pr^{\prime}(M=0)\\ &=\mathbf{E}^{\prime}\left[\left(\frac{1}{M}\sum_{i=1}^{M}\Delta _{i}-\Delta_{T}^{\pi}(s,a)\right)^{2}\left|M>0\right]\times\Pr^{\prime}(M>0)+ \left(A_{g}^{\pi}(s,a)\right)^{2}\times\Pr^{\prime}(M=0)\\ &\leq 2\mathbf{E}^{\prime}_{\{w_{i}\}}\left[\mathbf{E}^{\prime} \left[\left(\frac{1}{M}\sum_{i=1}^{M}\Delta_{i}\right)^{2}\left|\{w_{i}\} \right.\right]\left|w_{1}\leq H-N\right]\times\Pr^{\prime}(w_{1}\leq H-N)\\ &+2\left(\Delta_{T}^{\pi}(s,a)\right)^{2}+\left(A_{g}^{\pi}(s,a) \right)^{2}\times\Pr^{\prime}(M=0)\\ &\overset{(a)}{\leq}2\mathbf{E}^{\prime}_{\{w_{i}\}}\left[\frac{1 }{M^{2}}\sum_{i=1}^{M}\mathbf{E}^{\prime}\left[\Delta_{i}^{2}\middle|\{w_{i}\} \right.\right]\left|w_{1}\leq H-N\right]\times\Pr^{\prime}(w_{1}\leq H-N)\\ &+\frac{8}{T^{6}}+\left(A_{g}^{\pi}(s,a)\right)^{2}\times\Pr^{ \prime}(M=0)\end{split}\] (37)

where \((a)\) uses the bound \(|\Delta_{T}^{\pi}(s,a)|\leq\frac{2}{T^{3}}\) derived in (35), and the fact that \(\{\Delta_{i}\}\) are zero mean independent random variables conditioned on \(\{w_{i}\}\). Note that \(|g_{k,i}|\leq N\) almost surely, \(|A_{g}^{\pi}(s,a)|\leq\mathcal{O}(t_{\text{mix}})\) via Lemma 9, and \(|\Delta_{T}^{\pi}(s,a)|\leq\frac{2}{T^{3}}\) as shown in (35). Combining, we get, \(\mathbf{E}^{\prime}[|\Delta_{i}|^{2}|\{w_{i}\}]\leq\mathcal{O}(N^{2}/\pi(a|s))\) (see the definition of \(\Delta_{i}\) in (36)). Invoking this bound into (37), we get the following result.

\[\begin{split}&\mathbf{E}^{\prime}\left[\left(\hat{A}_{g}^{\pi}(s,a )-A_{g}^{\pi}(s,a)\right)^{2}\right]\leq 2\mathbf{E}^{\prime}\left[\frac{1}{M}\middle|w_{1}\leq H-N \right]\mathcal{O}\left(\frac{N^{2}}{\pi(a|s)}\right)+\frac{8}{T^{6}}\\ &+\mathcal{O}(t_{\text{mix}}^{2})\times\Pr^{\prime}(w_{1}>H-N) \end{split}\] (38)

Note that, one can use Lemma 11 to bound the following violation probability.

\[\Pr^{\prime}(w_{1}>H-N)\leq\left(1-\frac{3d^{\pi}(s)}{4}\right)^{4t_{\text{hit} }T^{\xi}(\log T)-1}\overset{(a)}{\leq}\left(1-\frac{3d^{\pi}(s)}{4}\right)^{ \frac{4}{d^{\pi}(s)}(\log T)}\leq\frac{1}{T^{3}}\] (39)where \((a)\) is a consequence of the fact that \(4t_{\rm hit}T^{\xi}(\log_{2}T)-1\geq\frac{4}{d^{\pi}(s)}\log_{2}T\) for sufficiently large \(T\). Finally, note that, if \(M<M_{0}\), where \(M_{0}\) is defined as,

\[M_{0}\triangleq\frac{H-N}{2N+\frac{4N\log T}{d^{\pi}(s)}}\] (40)

then there exists at least one \(w_{i}\) that exceeds \(4N\log_{2}T/d^{\pi}(s)\) which can happen with the following maximum probability according to Lemma 11.

\[\Pr^{\prime}\left(M<M_{0}\right)\leq\left(1-\frac{3d^{\pi}(s)}{4}\right)^{ \frac{4\log T}{d^{\pi}(s)}}\leq\frac{1}{T^{3}}\] (41)

The above probability bound can be used to obtain the following result,

\[\mathbf{E}^{\prime}\left[\frac{1}{M}\bigg{|}M>0\right]=\frac{ \sum_{m=1}^{\infty}\frac{1}{m}\Pr^{\prime}(M=m)}{\Pr^{\prime}(M>0)} \leq\frac{1\times\Pr^{\prime}(M\leq M_{0})+\frac{1}{M_{0}} \Pr^{\prime}(M>M_{0})}{\Pr^{\prime}(M>0)}\] (42) \[\leq\frac{\frac{1}{T^{3}}+\frac{2N+\frac{4N\log T}{d^{\pi}(s)}}{ H-N}}{1-\frac{1}{T^{3}}}\leq\mathcal{O}\left(\frac{N\log T}{Hd^{\pi}(s)}\right)\]

Injecting \((\ref{eq:11})\) and \((\ref{eq:12})\) into \((\ref{eq:13})\), we finally obtain the following.

\[\mathbf{E}^{\prime}\left[\left(\hat{A}_{g}^{\pi}(s,a)-A_{g}^{\pi} (s,a)\right)^{2}\right] \leq\mathcal{O}\left(\frac{N^{3}\log T}{Hd^{\pi}(s)\pi(a|s)}\right)\] (43) \[=\mathcal{O}\left(\frac{N^{3}t_{\rm hit}\log T}{H\pi(a|s)}\right) =\mathcal{O}\left(\frac{t_{\rm mix}^{2}(\log T)^{2}}{T^{\xi}\pi(a|s)}\right)\]

Eq. \((\ref{eq:12})\) demonstrates that our desired inequality is obeyed in the imaginary system. We now need a mechanism to translate this result to our actual CMDP. Note that \((\hat{A}_{g}^{\pi}(s,a)-A_{g}^{\pi}(s,a))^{2}=f(X)\) where \(X=(M,\tau_{1},\mathcal{T}_{1},\cdots,\tau_{M},\mathcal{T}_{M})\), and \(\mathcal{T}_{i}=(a_{\tau_{i}},s_{\tau_{i}+1},a_{\tau_{i}+1},\cdots,s_{\tau_{i} +N},a_{\tau_{i}+N})\). We have,

\[\frac{\mathbf{E}[f(X)]}{\mathbf{E}^{\prime}[f(X)]}=\frac{\sum_{X}f(X)\Pr(X)}{ \sum_{X}f(X)\Pr^{\prime}(X)}\leq\max_{X}\frac{\Pr(X)}{\Pr^{\prime}(X)}\] (44)

The last inequality uses the non-negativity of \(f(\cdot)\). Observe that, for a fixed sequence, \(X\), we have,

\[\Pr(X) =\Pr(\tau_{1})\times\Pr(\mathcal{T}_{1}|\tau_{1})\times\Pr(\tau_{ 2}|\tau_{1},\mathcal{T}_{1})\times\Pr(\mathcal{T}_{2}|\tau_{2})\times\cdots\] (45) \[\times\Pr(\tau_{M}|\tau_{M-1},\mathcal{T}_{M-1})\times\Pr(\mathcal{ T}_{M}|\tau_{M})\times\Pr(s_{t}\neq s,\forall t\in[\tau_{M}+2N,kH-N]|\tau_{M}, \mathcal{T}_{M}),\]

\[\Pr^{\prime}(X) =\Pr(\tau_{1})\times\Pr(\mathcal{T}_{1}|\tau_{1})\times\Pr^{ \prime}(\tau_{2}|\tau_{1},\mathcal{T}_{1})\times\Pr(\mathcal{T}_{2}|\tau_{2})\times\cdots\] (46) \[\times\Pr^{\prime}(\tau_{M}|\tau_{M-1},\mathcal{T}_{M-1})\times \Pr(\mathcal{T}_{M}|\tau_{M})\times\Pr(s_{t}\neq s,\forall t\in[\tau_{M}+2N,kH- N]|\tau_{M},\mathcal{T}_{M}),\]

The difference between \(\Pr(X)\) and \(\Pr^{\prime}(X)\) arises because \(\Pr(\tau_{i+1}|\tau_{i},\mathcal{T}_{i})\neq\Pr^{\prime}(\tau_{i+1}|\tau_{i}, \mathcal{T}_{i})\), \(\forall i\in\{1,\cdots,M-1\}\). Note that the ratio of these two terms can be bounded as follows,

\[\frac{\Pr(\tau_{i+1}|\tau_{i},\mathcal{T}_{i})}{\Pr^{\prime}(\tau _{i+1}|\tau_{i},\mathcal{T}_{i})}\] (47) \[=\frac{\sum_{s^{\prime}\neq s}\Pr(s_{\tau_{i}+2N}=s^{\prime}|\tau_ {i},\mathcal{T}_{i})\times\Pr(s_{t}\neq s,\forall t\in[\tau_{i}+2N,\tau_{i+1}- 1],s_{\tau_{i+1}}=s|s_{\tau_{i}+2N}=s^{\prime})}{\sum_{s^{\prime}\neq s}\Pr^{ \prime}(s_{\tau_{i}+2N}=s^{\prime}|\tau_{i},\mathcal{T}_{i})\times\Pr(s_{t} \neq s,\forall t\in[\tau_{i}+2N,\tau_{i+1}-1],s_{\tau_{i+1}}=s|s_{\tau_{i}+2N}= s^{\prime})}\] \[\leq\max_{s^{\prime}}\frac{\Pr(s_{\tau_{i}+2N}=s^{\prime}|\tau_{i},\mathcal{T}_{i})}{\Pr^{\prime}(s_{\tau_{i}+2N}=s^{\prime}|\tau_{i},\mathcal{T }_{i})}\] \[=\max_{s^{\prime}}1+\frac{\Pr(s_{\tau_{i}+2N}=s^{\prime}|\tau_{i},\mathcal{T}_{i})-d^{\pi}(s^{\prime})}{d^{\pi}(s^{\prime})}\stackrel{{ (a)}}{{\leq}}\max_{s^{\prime}}1+\frac{1}{T^{3}d^{\pi}(s^{\prime})}\leq 1+\frac{t_{\rm hit }}{T^{3}}\leq 1+\frac{1}{T^{2}}\]

[MISSING_PAGE_EMPTY:17]

where \((a)\) follows from Assumption 3 and Jensen's inequality whereas \((b)\) follows from Lemma 2. Combining, \((\ref{eq:1})\) and \((\ref{eq:2})\), we conclude the result. 

### Proof of Lemma 4

Proof.: Using the Lemma 12, it is obvious to see that

\[\begin{split} J_{g}^{\pi}-J_{g}^{\pi^{\prime}}&=\sum_ {s}\sum_{a}d^{\pi}(s)(\pi(a|s)-\pi^{\prime}(a|s))Q_{g}^{\pi^{\prime}}(s,a)\\ &=\sum_{s}\sum_{a}d^{\pi}(s)\pi(a|s)Q_{g}^{\pi^{\prime}}(s,a)- \sum_{s}d^{\pi}(s)V_{g}^{\pi^{\prime}}(s)\\ &=\sum_{s}\sum_{a}d^{\pi}(s)\pi(a|s)Q_{g}^{\pi^{\prime}}(s,a)- \sum_{s}\sum_{a}d^{\pi}(s)\pi(a|s)V_{g}^{\pi^{\prime}}(s)\\ &=\sum_{s}\sum_{a}d^{\pi}(s)\pi(a|s)[Q_{g}^{\pi^{\prime}}(s,a)-V _{g}^{\pi^{\prime}}(s)]=\mathbf{E}_{s\sim d^{\pi}}\mathbf{E}_{a\sim\pi(\cdot| s)}\big{[}A_{g}^{\pi^{\prime}}(s,a)\big{]}\end{split}\] (55)

We conclude the lemma using the definition of \(J_{\mathrm{L},\lambda}\) and \(A_{\mathrm{L},\lambda}\). 

### Proof of Lemma 5

Proof.: We start with the definition of KL divergence.

\[\begin{split}&\mathbf{E}_{s\sim d^{\pi^{*}}}[KL(\pi^{*}(\cdot|s) \|\pi_{\theta_{k}}(\cdot|s))-KL(\pi^{*}(\cdot|s)\|\pi_{\theta_{k+1}}(\cdot|s))] \\ &=\mathbf{E}_{s\sim d^{\pi^{*}}}\mathbf{E}_{a\sim\pi^{*}(\cdot|s )}\bigg{[}\log\frac{\pi_{\theta_{k+1}(a|s)}}{\pi_{\theta_{k}}(a|s)}\bigg{]}\\ &\stackrel{{(a)}}{{\geq}}\mathbf{E}_{s\sim d^{\pi^{*} }}\mathbf{E}_{a\sim\pi^{*}(\cdot|s)}[\nabla_{\theta}\log\pi_{\theta_{k}}(a|s) \cdot(\theta_{k+1}-\theta_{k})]-\frac{B}{2}\|\theta_{k+1}-\theta_{k}\|^{2}\\ &=\alpha\mathbf{E}_{s\sim d^{\pi^{*}}}\mathbf{E}_{a\sim\pi^{*}( \cdot|s)}[\nabla_{\theta}\log\pi_{\theta_{k}}(a|s)\cdot\omega_{k}]-\frac{B \alpha^{2}}{2}\|\omega_{k}\|^{2}\\ &=\alpha\mathbf{E}_{s\sim d^{\pi^{*}}}\mathbf{E}_{a\sim\pi^{*}( \cdot|s)}[\nabla_{\theta}\log\pi_{\theta_{k}}(a|s)\cdot\omega_{k}^{*}]+\alpha \mathbf{E}_{s\sim d^{\pi^{*}}}\mathbf{E}_{a\sim\pi^{*}(\cdot|s)}[\nabla_{ \theta}\log\pi_{\theta_{k}}(a|s)\cdot(\omega_{k}-\omega_{k}^{*})]-\frac{B \alpha^{2}}{2}\|\omega_{k}\|^{2}\\ &=\alpha[J_{\mathrm{L}}(\pi^{*},\lambda_{k})-J_{\mathrm{L}}( \theta_{k},\lambda_{k})]+\alpha\mathbf{E}_{s\sim d^{\pi^{*}}}\mathbf{E}_{a \sim\pi^{*}(\cdot|s)}[\nabla_{\theta}\log\pi_{\theta_{k}}(a|s)\cdot\omega_{k} ^{*}]-\alpha[J_{\mathrm{L}}(\pi^{*},\lambda_{k})-J_{\mathrm{L}}(\theta_{k}, \lambda_{k})]\\ &+\alpha\mathbf{E}_{s\sim d^{\pi^{*}}}\mathbf{E}_{a\sim\pi^{*}( \cdot|s)}[\nabla_{\theta}\log\pi_{\theta_{k}}(a|s)\cdot(\omega_{k}-\omega_{k}^ {*})]-\frac{B\alpha^{2}}{2}\|\omega_{k}\|^{2}\\ &\stackrel{{(b)}}{{=}}\alpha[J_{\mathrm{L}}(\pi^{*}, \lambda_{k})-J_{\mathrm{L}}(\theta_{k},\lambda_{k})]+\alpha\mathbf{E}_{s\sim d ^{\pi^{*}}}\mathbf{E}_{a\sim\pi^{*}(\cdot|s)}\bigg{[}\nabla_{\theta}\log\pi_{ \theta_{k}}(a|s)\cdot\omega_{k}^{*}-A_{\mathrm{L},\lambda_{k}}^{\pi_{\theta_{k} }}(s,a)\bigg{]}\\ &+\alpha\mathbf{E}_{s\sim d^{\pi^{*}}}\mathbf{E}_{a\sim\pi^{*}( \cdot|s)}[\nabla_{\theta}\log\pi_{\theta_{k}}(a|s)\cdot(\omega_{k}-\omega_{k}^ {*})]-\frac{B\alpha^{2}}{2}\|\omega_{k}\|^{2}\\ &\stackrel{{(c)}}{{\geq}}\alpha[J_{\mathrm{L}}(\pi^{*},\lambda_{k})-J_{\mathrm{L}}(\theta_{k},\lambda_{k})]-\alpha\sqrt{\mathbf{E}_{s \sim d^{\pi^{*}}}\mathbf{E}_{a\sim\pi^{*}(\cdot|s)}\bigg{[}\bigg{(}\nabla_{ \theta}\log\pi_{\theta_{k}}(a|s)\cdot\omega_{k}^{*}-A_{\mathrm{L},\lambda_{k}}^ {\pi_{\theta_{k}}}(s,a)\bigg{)}^{2}\bigg{]}}\\ &-\alpha\mathbf{E}_{s\sim d^{\pi^{*}}}\mathbf{E}_{a\sim\pi^{*}( \cdot|s)}\|\nabla_{\theta}\log\pi_{\theta_{k}}(a|s)\|_{2}\|(\omega_{k}-\omega_{ k}^{*})\|-\frac{B\alpha^{2}}{2}\|\omega_{k}\|^{2}\\ &\stackrel{{(d)}}{{\geq}}\alpha[J_{\mathrm{L}}(\pi^{*}, \lambda_{k})-J_{\mathrm{L}}(\theta_{k},\lambda_{k})]-\alpha\sqrt{\epsilon_{ \mathrm{bias}}}-\alpha G\|(\omega_{k}-\omega_{k}^{*})\|-\frac{B\alpha^{2}}{2} \|\omega_{k}\|^{2}\end{split}\] (56)

where the step (a) holds by Assumption 3 and step (b) holds by Lemma 4. Step (c) uses the convexity of the function \(f(x)=x^{2}\). Finally, step (d) comes from the Assumption 4. Rearranging items, we have

\[\begin{split} J_{\mathrm{L}}(\pi^{*},\lambda_{k})-J_{\mathrm{L}}( \theta_{k},\lambda_{k})&\leq\sqrt{\epsilon_{\mathrm{bias}}}+G\|( \omega_{k}-\omega_{k}^{*})\|+\frac{B\alpha}{2}\|\omega_{k}\|^{2}\\ &+\frac{1}{\alpha}\mathbf{E}_{s\sim d^{\pi^{*}}}[KL(\pi^{*}(\cdot|s) \|\pi_{\theta_{k}}(\cdot|s))-KL(\pi^{*}(\cdot|s)\|\pi_{\theta_{k+1}}(\cdot|s))] \end{split}\] (57)

Summing from \(k=1\) to \(K\), using the non-negativity of KL divergence and dividing the resulting expression by \(K\), we get the desired result.

### Proof of Lemma 6

Proof.: By the \(L\)-smooth property of the objective function and constraint function, we know that \(J_{\mathrm{L}}(\cdot,\lambda)\) is a \(L(1+\lambda)\)-smooth function. Thus,

\[J_{\mathrm{L}}(\theta_{k+1},\lambda_{k})\geq J_{\mathrm{L}}( \theta_{k},\lambda_{k})+\langle\nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k}), \theta_{k+1}-\theta_{k}\rangle-\frac{L(1+\lambda_{k})}{2}\|\theta_{k+1}- \theta_{k}\|^{2}\] (58) \[\overset{(a)}{=}J_{\mathrm{L}}(\theta_{k},\lambda_{k})+\alpha \nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})^{T}\omega_{k}-\frac{L(1+\lambda_ {k})\alpha^{2}}{2}\|\omega_{k}\|^{2}\] \[=J_{\mathrm{L}}(\theta_{k},\lambda_{k})+\alpha\|\nabla J_{ \mathrm{L}}(\theta_{k},\lambda_{k})\|^{2}-\alpha\langle\nabla J_{\mathrm{L}}( \theta_{k},\lambda_{k})-\omega_{k},\nabla J_{\mathrm{L}}(\theta_{k},\lambda_ {k})\rangle\] \[\quad-\frac{L(1+\lambda_{k})\alpha^{2}}{2}\|\nabla J_{\mathrm{L }}(\theta_{k},\lambda_{k})-\omega_{k}-\nabla J_{\mathrm{L}}(\theta_{k}, \lambda_{k})\|^{2}\] \[\overset{(b)}{\geq}J_{\mathrm{L}}(\theta_{k},\lambda_{k})+ \alpha\|\nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2}-\frac{\alpha}{2}\| \nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})-\omega_{k}\|^{2}-\frac{\alpha}{ 2}\|\nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2}\] \[\quad-L(1+\lambda_{k})\alpha^{2}\|\nabla J_{\mathrm{L}}(\theta_{k },\lambda_{k})-\omega_{k}\|^{2}-L(1+\lambda_{k})\alpha^{2}\|\nabla J_{\mathrm{ L}}(\theta_{k},\lambda_{k})\|^{2}\] \[=J_{\mathrm{L}}(\theta_{k},\lambda_{k})+\left(\frac{\alpha}{2}-L( 1+\lambda_{k})\alpha^{2}\right)\|\nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k} )\|^{2}-\left(\frac{\alpha}{2}+L(1+\lambda_{k})\alpha^{2}\right)\|\nabla J_{ \mathrm{L}}(\theta_{k},\lambda_{k})-\omega_{k}\|^{2}\]

where step (a) follows from the fact that \(\theta_{k+1}=\theta_{k}+\alpha\omega_{k}\) and inequality (b) holds due to the Cauchy-Schwarz inequality. Now, adding \(J_{\mathrm{L}}(\theta_{k+1},\lambda_{k+1})\) on both sides, we have

\[J_{\mathrm{L}}(\theta_{k+1},\lambda_{k+1}) \geq J_{\mathrm{L}}(\theta_{k+1},\lambda_{k+1})-J_{\mathrm{L}}( \theta_{k+1},\lambda_{k})+J_{\mathrm{L}}(\theta_{k},\lambda_{k})+\left(\frac{ \alpha}{2}-L(1+\lambda_{k})\alpha^{2}\right)\|\nabla J_{\mathrm{L}}(\theta_{ k},\lambda_{k})\|^{2}\] (59) \[\overset{(a)}{=}(\lambda_{k+1}-\lambda_{k})J_{c}(\theta_{k+1})+J _{\mathrm{L}}(\theta_{k},\lambda_{k})+\left(\frac{\alpha}{2}-L(1+\lambda_{k}) \alpha^{2}\right)\|\nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2}\] \[\quad-\left(\frac{\alpha}{2}+L(1+\lambda_{k})\alpha^{2}\right)\| \nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})-\omega_{k}\|^{2}\] \[\overset{(b)}{\geq}-\beta+J_{\mathrm{L}}(\theta_{k},\lambda_{k} )+\left(\frac{\alpha}{2}-L(1+\lambda_{k})\alpha^{2}\right)\|\nabla J_{\mathrm{ L}}(\theta_{k},\lambda_{k})\|^{2}\] \[\quad-\left(\frac{\alpha}{2}+L(1+\lambda_{k})\alpha^{2}\right)\| \nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})-\omega_{k}\|^{2}\]

where (a) holds by the definition of \(J_{\mathrm{L}}(\theta,\lambda)\) and step (b) is true because \(|J_{c}(\theta)|\leq 1,\forall\theta\) and \(|\lambda_{k+1}-\lambda_{k}|\leq\beta|\hat{J}_{c}(\theta_{k})|\leq\beta\) where the last inequality uses the fact that \(|\hat{J}_{c}(\theta_{k})|\leq 1\). Summing over \(k\in\{1,\cdots,K\}\), we have,

\[\sum_{k=1}^{K}\left[J_{\mathrm{L}}(\theta_{k+1},\lambda_{k+1})-J _{\mathrm{L}}(\theta_{k},\lambda_{k})\right] \geq-\beta K+\sum_{k=1}^{K}\left(\frac{\alpha}{2}-L(1+\lambda_{k} )\alpha^{2}\right)\|\nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2}\] (60) \[\quad-\sum_{k=1}^{K}\left(\frac{\alpha}{2}+L(1+\lambda_{k})\alpha^ {2}\right)\|\nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})-\omega_{k}\|^{2}\]

which leads to the following.

\[J_{\mathrm{L}}(\theta_{K+1},\lambda_{K+1})-J_{\mathrm{L}}(\theta_{ 1},\lambda_{1}) \geq-\beta K+\sum_{k=1}^{K}\left(\frac{\alpha}{2}-L(1+\lambda_{k}) \alpha^{2}\right)\|\nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2}\] (61) \[\quad-\sum_{k=1}^{K}\left(\frac{\alpha}{2}+L(1+\lambda_{k})\alpha^ {2}\right)\|\nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})-\omega_{k}\|^{2}\]

Rearranging the terms and using \(0\leq\lambda_{k}\leq\frac{2}{\delta}\) due to the dual update, we arrive at the following.

\[\sum_{k=1}^{K}\|\nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2}\leq\frac{J_{ \mathrm{L}}(\theta_{K+1},\lambda_{K+1})-J_{\mathrm{L}}(\theta_{1},\lambda_{1})+ \beta K+(\frac{\alpha}{2}+L(1+\frac{2}{\delta})\alpha^{2})\sum_{k=1}^{K}\| \nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})-\omega_{k}\|^{2}}{\frac{\alpha}{ 2}-L(1+\frac{2}{\delta})\alpha^{2}}\] (62)Choosing \(\alpha=\frac{1}{4L(1+\frac{2}{\delta})}\) and dividing both sides by \(K\), we conclude the result.

\[\frac{1}{K}\sum_{k=1}^{K}\|\nabla J_{\mathrm{L}}(\theta_{k},\lambda_ {k})\|^{2} \leq\frac{16L(1+\frac{2}{\delta})}{K}\left[J_{\mathrm{L}}(\theta_{K+1}, \lambda_{K+1})-J_{\mathrm{L}}(\theta_{1},\lambda_{1})\right]\] (63) \[+\frac{3}{K}\sum_{k=1}^{K}\|\nabla J_{\mathrm{L}}(\theta_{k}, \lambda_{k})-\omega_{k}\|^{2}+\beta\]

Recall that \(|J_{\mathrm{L}}(\theta,\lambda)|\leq 1+\lambda\leq 1+\frac{2}{\delta}\leq \frac{3}{\delta},\forall\theta\in\Theta,\forall\lambda\geq 0\). Thus,

\[\frac{1}{K}\sum_{k=1}^{K}\|\nabla J_{\mathrm{L}}(\theta_{k},\lambda_{k})\|^{2 }\leq\frac{288L}{\delta^{2}K}+\frac{3}{K}\sum_{k=1}^{K}\|\nabla J_{\mathrm{L}} (\theta_{k},\lambda_{k})-\omega_{k}\|^{2}+\beta\] (64)

This completes the proof. 

### Proof of Theorem 1

#### b.5.1 Rate of Convergence of the Objective

Recall the definition of \(J_{\mathrm{L}}(\theta,\lambda)=J_{r}(\theta)+\lambda J_{c}(\theta)\). Using Lemma 7, we arrive at the following.

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{(}J_{r}^{\pi^{*}}-J_{r}( \theta_{k})\bigg{)}\leq G\left(1+\frac{1}{\mu_{F}}\right)\tilde{\mathcal{O}} \left(\sqrt{\beta}+\frac{\sqrt{A}Gt_{\mathrm{mix}}}{\delta T^{\xi/2}}+\frac{ \sqrt{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}}{\delta T^{(1-\xi)/2}}\right)\] (65) \[+\frac{B}{L}\tilde{\mathcal{O}}\left(\frac{AG^{2}t_{\mathrm{mix} }^{2}}{\delta^{2}T^{\xi}}+\frac{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}{\delta^{2} T^{1-\xi}}+\beta\right)+\tilde{\mathcal{O}}\bigg{(}\frac{Lt_{\mathrm{mix}}t_{ \mathrm{hit}}\mathbf{E}_{s\sim d^{\pi^{*}}}[KL(\pi^{*}(\cdot|s)\|\pi_{\theta_{ 1}}(\cdot|s))]}{T^{1-\xi}\delta}\bigg{)}\] \[-\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{[}\lambda_{k}\bigg{(}J _{c}^{\pi^{*}}-J_{c}(\theta_{k})\bigg{)}\bigg{]}+\sqrt{\epsilon_{\mathrm{bias }}}\]

Thus, we need to find a bound for the last term in the above equation.

\[0 \leq(\lambda_{K+1})^{2}\] (66) \[\overset{(a)}{=}\sum_{k=1}^{K}\bigg{(}(\lambda_{k+1})^{2}-( \lambda_{k})^{2}\bigg{)}\] \[=\sum_{k=1}^{K}\bigg{(}\mathcal{P}_{[0,\frac{2}{\delta}]}\big{[} \lambda_{k}-\beta\hat{J}_{c}(\theta_{k})\big{]}^{2}-(\lambda_{k})^{2}\bigg{)}\] \[\leq\sum_{k=1}^{K}\bigg{(}\big{[}\lambda_{k}-\beta\hat{J}_{c}( \theta_{k})\big{]}^{2}-(\lambda_{k})^{2}\bigg{)}\] \[=-2\beta\sum_{k=1}^{K}\lambda_{k}\hat{J}_{c}(\theta_{k})+\beta^{ 2}\sum_{k=1}^{K}\hat{J}_{c}(\theta_{k})^{2}\] \[\overset{(b)}{\leq}2\beta\sum_{k=1}^{K}\lambda_{k}(J_{c}^{\pi^{ *}}-\hat{J}_{c}(\theta_{k}))+\beta^{2}\sum_{k=1}^{K}\hat{J}_{c}(\theta_{k})^{2}\] \[\leq 2\beta\sum_{k=1}^{K}\lambda_{k}(J_{c}^{\pi^{*}}-\hat{J}_{c}( \theta_{k}))+2\beta^{2}\sum_{k=1}^{K}\hat{J}_{c}(\theta_{k})^{2}\] \[=2\beta\sum_{k=1}^{K}\lambda_{k}(J_{c}^{\pi^{*}}-J_{c}(\theta_{k} ))+2\beta\sum_{k=1}^{K}\lambda_{k}(J_{c}(\theta_{k})-\hat{J}_{c}(\theta_{k}))+2 \beta^{2}\sum_{k=1}^{K}\hat{J}_{c}(\theta_{k})^{2}\]where (a) uses \(\lambda_{1}=0\) and inequality (b) holds because \(\theta^{*}\) is a feasible solution to the constrained optimization problem. Rearranging items and taking the expectation, we have,

\[-\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{[}\lambda_{k}(J_{c}^{n^{* }}-J_{c}(\theta_{k}))\bigg{]} \leq\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{[}\lambda_{k}(J_{c}( \theta_{k})-\hat{J}_{c}(\theta_{k}))\bigg{]}+\frac{\beta}{K}\sum_{k=1}^{K} \mathbf{E}[\hat{J}_{c}(\theta_{k})]^{2}\] (67) \[\overset{(a)}{\leq}\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\left[ \lambda_{k}\left(J_{c}(\theta_{k})-\hat{J}_{c}(\theta_{k})\right)\right]+\beta\] \[\overset{(b)}{=}\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{[} \lambda_{k}\left(J_{c}(\theta_{k})-\mathbf{E}\left[\hat{J}_{c}(\theta_{k}) \big{|}\theta_{k}\right]\right)\bigg{]}+\beta\] \[\leq\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{[}\lambda_{k}\left|J _{c}(\theta_{k})-\mathbf{E}\left[\hat{J}_{c}(\theta_{k})\big{|}\theta_{k} \right]\right|\bigg{]}+\beta\overset{(c)}{\leq}\frac{2}{\delta T^{2}}+\beta\]

where (a) results from \(|\hat{J}_{c,\rho}(\theta)|^{2}\leq 1\), \(\forall\theta\in\Theta\) and (b) uses the fact that \(\hat{J}_{c,\rho}(\theta_{k})\) and \(\lambda_{k}\) are conditionally independent given \(\theta_{k}\). Finally, (c) is a consequence of Lemma 13. Combining (67) with (65), we deduce,

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{(}J_{r}^{\pi^{*}}-J_{r}( \theta_{k})\bigg{)}\] (68) \[\leq\sqrt{\epsilon_{\mathrm{bias}}}+G\left(1+\frac{1}{\mu_{F}} \right)\tilde{\mathcal{O}}\left(\sqrt{\beta}+\frac{\sqrt{A}Gt_{\mathrm{mix}}}{ \delta T^{\xi/2}}+\frac{\sqrt{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}}{\delta T^{( 1-\xi)/2}}\right)+\mathcal{O}\left(\frac{1}{\delta T^{2}}+\beta\right)\] \[+\frac{B}{L}\tilde{\mathcal{O}}\left(\frac{AG^{2}t_{\mathrm{mix}}^ {2}}{\delta^{2}T^{\xi}}+\frac{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}{\delta^{2}T^ {1-\xi}}+\beta\right)+\tilde{\mathcal{O}}\bigg{(}\frac{Lt_{\mathrm{mix}}t_{ \mathrm{hit}}\mathbf{E}_{s\sim d^{\pi^{*}}}[KL(\pi^{*}(\cdot|s)\|\pi_{\theta_{ 1}}(\cdot|s))]}{T^{1-\xi}\delta}\bigg{)}\] \[\leq\sqrt{\epsilon_{\mathrm{bias}}}+G\left(1+\frac{1}{\mu_{F}} \right)\tilde{\mathcal{O}}\left(\sqrt{\beta}+\frac{\sqrt{A}Gt_{\mathrm{mix}}}{ \delta T^{\xi/2}}+\frac{\sqrt{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}}{\delta T^{( 1-\xi)/2}}\right)\]

The last inequality presents only the dominant terms of \(\beta\) and \(T\).

#### b.5.2 Rate of Constraint Violation

Since \(\{\lambda_{k}\}_{k=1}^{K}\) are derived by applying the dual update in Algorithm 1, we have,

\[\mathbf{E}\left|\lambda_{k+1}-\frac{2}{\delta}\right|^{2}\overset{ (a)}{\leq}\mathbf{E}\left|\lambda_{k}-\beta\hat{J}_{c}(\theta_{k})-\frac{2}{ \delta}\right|^{2}\] (69) \[=\mathbf{E}\left|\lambda_{k}-\frac{2}{\delta}\right|^{2}-2\beta \mathbf{E}\left[\hat{J}_{c}(\theta_{k})\left(\lambda_{k}-\frac{2}{\delta} \right)\right]+\beta^{2}\mathbf{E}\left[\hat{J}_{c}^{2}(\theta_{k})\right]\] \[\overset{(b)}{\leq}\mathbf{E}\left|\lambda_{k}-\frac{2}{\delta} \right|^{2}-2\beta\mathbf{E}\left[J_{c}(\theta_{k})\left(\lambda_{k}-\frac{2}{ \delta}\right)\right]-2\beta\mathbf{E}\left[\left(\hat{J}_{c}(\theta_{k})-J_{ c}(\theta_{k})\right)\left(\lambda_{k}-\frac{2}{\delta}\right)\right]+\beta^{2}\] \[\overset{(c)}{=}\mathbf{E}\left|\lambda_{k}-\frac{2}{\delta} \right|^{2}-2\beta\mathbf{E}\left[J_{c}(\theta_{k})\left(\lambda_{k}-\frac{2}{ \delta}\right)\right]-2\beta\mathbf{E}\left[\left(\mathbf{E}\left[\hat{J}_{c} (\theta_{k})\big{|}\theta_{k}\right]-J_{c}(\theta_{k})\right)\left(\lambda_{k}- \frac{2}{\delta}\right)\right]+\beta^{2}\] \[\leq\mathbf{E}\left|\lambda_{k}-\frac{2}{\delta}\right|^{2}-2\beta \mathbf{E}\left[J_{c}(\theta_{k})\left(\lambda_{k}-\frac{2}{\delta}\right) \right]+2\beta\mathbf{E}\left[\left|\mathbf{E}\left[\hat{J}_{c}(\theta_{k}) \big{|}\theta_{k}\right]-J_{c}(\theta_{k})\right|\left|\lambda_{k}-\frac{2}{ \delta}\right|\right]+\beta^{2}\] \[\overset{(d)}{\leq}\mathbf{E}\left|\lambda_{k}-\frac{2}{\delta} \right|^{2}-2\beta\mathbf{E}\left[J_{c}(\theta_{k})\left(\lambda_{k}-\frac{2}{ \delta}\right)\right]+\frac{4\beta}{\delta T^{2}}+\beta^{2}\]

where \((a)\) is due to the non-expansiveness of the projection \(\mathcal{P}_{[0,\frac{2}{\delta}]}\) and \((b)\) holds because \(\hat{J}_{c}(\theta)\in[0,1]\), \(\forall\theta\in\Theta\) according to its definition in Algorithm 1. Finally, \((c)\) is a consequence of the fact that \(\hat{J}_{c}(\theta_{k})\)and \(\lambda_{k}\) are conditionally independent given \(\theta_{k}\) whereas \((d)\) applies \(|\lambda_{k}-\frac{2}{\delta}|\leq\frac{2}{\delta}\) and Lemma 13. Averaging (69) over \(k\in\{1,\ldots,K\}\), we get,

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\left[J_{c}(\theta_{k})\left(\lambda_{k}- \frac{2}{\delta}\right)\right]\leq\frac{\left|\lambda_{1}-\frac{2}{\delta} \right|^{2}-\left|\lambda_{K+1}-\frac{2}{\delta}\right|^{2}}{2\beta K}+\frac{ 2}{\delta T^{2}}+\frac{\beta}{2}\stackrel{{(a)}}{{\leq}}\frac{2}{ \delta^{2}\beta K}+\frac{2}{\delta T^{2}}+\frac{\beta}{2}\] (70)

where (a) uses \(\lambda_{1}=0\). Note that \(\lambda_{k}J_{c}^{\pi^{*}}\geq 0\), \(\forall k\). Adding the above inequality to (65) at both sides, we, therefore, have,

\[\begin{split}&\mathbf{E}\bigg{[}J_{r}^{\pi^{*}}-\frac{1}{K}\sum_{k =1}^{K}J_{r}(\theta_{k})\bigg{]}+\frac{2}{\delta}\mathbf{E}\bigg{[}\frac{1}{K }\sum_{k=1}^{K}-J_{c}(\theta_{k})\bigg{]}\leq\sqrt{\epsilon_{\rm bias}}+\frac {2}{\delta^{2}\beta K}+\frac{2}{T^{2}\delta}+\frac{\beta}{2}\\ &+G\left(1+\frac{1}{\mu_{F}}\right)\tilde{\mathcal{O}}\left( \sqrt{\beta}+\frac{\sqrt{A}Gt_{\rm mix}}{\delta T^{\xi/2}}+\frac{\sqrt{Lt_{ \rm mix}t_{\rm hit}}}{\delta T^{(1-\xi)/2}}\right)+\frac{B}{L}\tilde{ \mathcal{O}}\left(\frac{AG^{2}t_{\rm mix}^{2}}{\delta^{2}T^{\xi}}+\frac{Lt_{ \rm mix}t_{\rm hit}}{\delta^{2}T^{1-\xi}}+\beta\right)\\ &+\tilde{\mathcal{O}}\bigg{(}\frac{Lt_{\rm mix}t_{\rm hit}\mathbf{ E}_{s\sim d^{*}}[KL(\pi^{*}(\cdot|s)\|\pi_{\theta_{1}}(\cdot|s))]}{T^{1-\xi}\delta} \bigg{)}\end{split}\] (71)

Since the functions \(\{J_{g}(\theta_{k})\},k\in\{0,\cdots,K-1\},g\in\{r,c\}\) are linear in occupancy measure, there exists a policy \(\bar{\pi}\) such that the following holds \(\forall g\in\{r,c\}\).

\[\frac{1}{K}\sum_{k=1}^{K}J_{g}(\theta_{k})=J_{g}^{\bar{\pi}}\] (72)

Injecting the above relation to (71), we have

\[\begin{split}&\mathbf{E}\bigg{[}J_{r}^{\pi^{*}}-J_{r}^{\bar{\pi}} \bigg{]}+\frac{2}{\delta}\mathbf{E}\bigg{[}-J_{c}^{\bar{\pi}}\bigg{]}\leq \sqrt{\epsilon_{\rm bias}}+\frac{2}{\delta^{2}\beta K}+\frac{2}{T^{2}\delta}+ \frac{\beta}{2}\\ &+G\left(1+\frac{1}{\mu_{F}}\right)\tilde{\mathcal{O}}\left( \sqrt{\beta}+\frac{\sqrt{A}Gt_{\rm mix}}{\delta T^{\xi/2}}+\frac{\sqrt{Lt_{ \rm mix}t_{\rm hit}}}{\delta T^{(1-\xi)/2}}\right)\\ &+\frac{B}{L}\tilde{\mathcal{O}}\left(\frac{AG^{2}t_{\rm mix}^{2} }{\delta^{2}T^{\xi}}+\frac{Lt_{\rm mix}t_{\rm hit}}{\delta T^{1-\xi}}+\beta \right)+\tilde{\mathcal{O}}\bigg{(}\frac{Lt_{\rm mix}t_{\rm hit}\mathbf{E}_{ s\sim d^{*}}[KL(\pi^{*}(\cdot|s)\|\pi_{\theta_{1}}(\cdot|s))]}{T^{1-\xi}\delta} \bigg{)}\end{split}\] (73)

By Lemma 18, we arrive at,

\[\begin{split}&\mathbf{E}\bigg{[}-J_{c}^{\bar{\pi}}\bigg{]}\\ &\leq\delta\sqrt{\epsilon_{\rm bias}}+\frac{2}{\delta\beta K}+ \frac{2}{T^{2}}+\frac{\delta\beta}{2}+G\left(1+\frac{1}{\mu_{F}}\right)\tilde{ \mathcal{O}}\left(\delta\sqrt{\beta}+\frac{\sqrt{A}Gt_{\rm mix}}{T^{\xi/2}}+ \frac{\sqrt{Lt_{\rm mix}t_{\rm hit}}}{T^{(1-\xi)/2}}\right)\\ &+\frac{B}{L}\tilde{\mathcal{O}}\left(\frac{AG^{2}t_{\rm mix}^{2} }{\delta T^{\xi}}+\frac{Lt_{\rm mix}t_{\rm hit}}{\delta T^{1-\xi}}+\delta \beta\right)+\tilde{\mathcal{O}}\bigg{(}\frac{Lt_{\rm mix}t_{\rm hit}\mathbf{ E}_{s\sim d^{*}}[KL(\pi^{*}(\cdot|s)\|\pi_{\theta_{1}}(\cdot|s))]}{T^{1-\xi}}\bigg{)} \end{split}\] (74)

The last inequality presents only the dominant terms of \(\beta\) and \(T\).

#### b.5.3 Optimal Choice of \(\beta\) and \(\xi\)

If we choose \(\beta=T^{-\eta}\) for some \(\eta\in(0,1)\), then following (68) and (74), we can write,

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{(}J_{r}^{\pi^{*}}-J_{r}( \theta_{k})\bigg{)} \leq\sqrt{\epsilon_{\rm bias}}+\tilde{\mathcal{O}}\left(T^{-\eta/2}+T^{- \xi/2}+T^{-(1-\xi)/2}\right),\] (75) \[\mathbf{E}\left[\frac{1}{K}\sum_{k=1}^{K}-J_{c}(\theta_{k})\right] \leq\delta\sqrt{\epsilon_{\rm bias}}+\tilde{\mathcal{O}}\left(T^{-(1- \xi-\eta)}+T^{-\eta/2}+T^{-\xi/2}+T^{-(1-\xi)/2}\right)\] (76)Clearly, the optimal values of \(\eta\) and \(\xi\) can be obtained by solving the following optimization.

\[\max_{(\eta,\xi)\in(0,1)^{2}}\min\left\{1-\xi-\eta,\frac{\eta}{2},\frac{\xi}{2}, \frac{1-\xi}{2}\right\}\] (77)

One can easily verify that \((\xi,\eta)=(2/5,2/5)\) is the solution of the above optimization. Therefore, the convergence rate of the objective function can be written as follows.

\[\frac{1}{K}\sum_{k=1}^{K}\mathbf{E}\bigg{(}J_{r}^{\pi^{*}}-J_{r}( \theta_{k})\bigg{)}\] (78) \[\leq\sqrt{\epsilon_{\mathrm{bias}}}+G\left(1+\frac{1}{\mu_{F}} \right)\tilde{\mathcal{O}}\left(\frac{1}{T^{1/5}}+\frac{\sqrt{A}Gt_{\mathrm{ mix}}}{\delta T^{1/5}}+\frac{\sqrt{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}}{\delta T^{3/ 10}}\right)+\mathcal{O}\left(\frac{1}{\delta T^{2}}+\frac{1}{T^{2/5}}\right)\] \[+\frac{B}{L}\tilde{\mathcal{O}}\left(\frac{\delta^{2}+AG^{2}t_{ \mathrm{mix}}^{2}}{\delta^{2}T^{2/5}}+\frac{Lt_{\mathrm{mix}}t_{\mathrm{hit}} }{\delta^{2}T^{3/5}}\right)+\tilde{\mathcal{O}}\bigg{(}\frac{Lt_{\mathrm{mix} }t_{\mathrm{hit}}\mathbf{E}_{s\sim d^{*}}[KL(\pi^{*}(\cdot|s)\|\pi_{\theta_{1} }(\cdot|s))]}{T^{3/5}\delta}\bigg{)}\] \[\leq\sqrt{\epsilon_{\mathrm{bias}}}+\frac{\sqrt{A}G^{2}t_{ \mathrm{mix}}}{\delta}\left(1+\frac{1}{\mu_{F}}\right)\tilde{\mathcal{O}} \left(T^{-1/5}\right)\]

The last expression only considers the dominant terms of \(T\). Similarly, the constraint violation rate can be computed as,

\[\mathbf{E}\bigg{[}\frac{1}{K}\sum_{k=1}^{K}-J_{c}(\theta_{k}) \bigg{]}\] (79) \[\leq\delta\sqrt{\epsilon_{\mathrm{bias}}}+\tilde{\mathcal{O}} \left(\frac{t_{\mathrm{mix}}t_{\mathrm{hit}}}{\delta T^{1/5}}+\frac{1}{T^{2}} +\frac{\delta}{T^{2/5}}\right)+G\left(1+\frac{1}{\mu_{F}}\right)\tilde{ \mathcal{O}}\left(\frac{\delta+\sqrt{A}Gt_{\mathrm{mix}}}{T^{1/5}}+\frac{ \sqrt{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}}{T^{3/10}}\right)\] \[+\frac{B}{L}\tilde{\mathcal{O}}\left(\frac{\delta^{2}+AG^{2}t_{ \mathrm{mix}}^{2}}{\delta T^{2/5}}+\frac{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}{ \delta T^{3/5}}\right)+\tilde{\mathcal{O}}\bigg{(}\frac{Lt_{\mathrm{mix}}t_{ \mathrm{hit}}\mathbf{E}_{s\sim d^{*}}[KL(\pi^{*}(\cdot|s)\|\pi_{\theta_{1}}( \cdot|s))]}{T^{3/5}}\bigg{)}\] \[\leq\delta\sqrt{\epsilon_{\mathrm{bias}}}+\tilde{\mathcal{O}} \left(\frac{t_{\mathrm{mix}}t_{\mathrm{hit}}}{\delta T^{1/5}}\right)+\sqrt{A} G^{2}t_{\mathrm{mix}}\left(1+\frac{1}{\mu_{F}}\right)\tilde{\mathcal{O}}\left(T^{-1/5}\right)\]

where the last expression contains only the dominant terms of \(T\). This concludes the theorem.

## Appendix C Proofs for the Regret and Violation Analysis

### Proof of Lemma 8

Proof.: Using Taylor's expansion, we can write the following \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\), \(\forall k\).

\[|\pi_{\theta_{k+1}}(a|s)-\pi_{\theta_{k}}(a|s)| =\left|(\theta_{k+1}-\theta_{k})^{T}\nabla_{\theta}\pi_{\theta}( a|s)\right|\] (80) \[=\pi_{\theta_{k}}(a|s)\left|(\theta_{k+1}-\theta_{k})^{T}\nabla_{ \theta}\log\pi_{\theta_{k}}(a|s)\right|\] \[\leq\pi_{\tilde{\theta}_{k}}(a|s)\|\theta_{k+1}-\theta_{k}\|\| \nabla_{\theta}\log\pi_{\tilde{\theta}_{k}}(a|s)\|\overset{(a)}{\leq}G\| \theta_{k+1}-\theta_{k}\|\]

[MISSING_PAGE_FAIL:24]

[MISSING_PAGE_FAIL:25]

### Proof of Theorem 2

Proof.: Recall the decomposition of the regret in section 5 and take the expectation.

\[\begin{split}&\mathbf{E}[\mathrm{Reg}_{T}]=\sum_{t=0}^{T-1}\left(J_{r }^{\pi^{*}}-r(s_{t},a_{t})\right)=H\sum_{k=1}^{K}\left(J_{r}^{\pi^{*}}-J_{r}( \theta_{k})\right)+\sum_{k=1}^{K}\sum_{t\in\mathcal{I}_{k}}\left(J_{r}(\theta_ {k})-r(s_{t},a_{t})\right)\\ &=H\sum_{k=1}^{K}\left(J_{r}^{\pi^{*}}-J_{r}(\theta_{k})\right)+ \mathbf{E}\left[\sum_{k=1}^{K-1}V_{r}^{\pi_{\theta_{k+1}}}(s_{kH})-V_{r}^{\pi_ {\theta_{k}}}(s_{kH})\right]+\mathbf{E}\left[V_{r}^{\pi_{\theta_{K}}}(s_{T})-V _{r}^{\pi_{\theta_{0}}}(s_{0})\right]\end{split}\] (88)

Using the result in (78), Lemma 8 and Lemma 9, we get,

\[\begin{split}&\mathbf{E}[\mathrm{Reg}_{T}]\leq T\sqrt{\epsilon_{ \mathrm{bias}}}+G\left(1+\frac{1}{\mu_{F}}\right)\tilde{\mathcal{O}}\left(T^{ \frac{4}{5}}+\frac{\sqrt{A}Gt_{\mathrm{mix}}}{\delta}T^{\frac{4}{5}}+\frac{ \sqrt{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}}{\delta}T^{\frac{7}{10}}\right)+ \mathcal{O}\left(\frac{1}{T}+T^{\frac{2}{5}}\right)\\ &+\frac{B}{L}\tilde{\mathcal{O}}\left(\frac{\delta^{2}+AG^{2}t_{ \mathrm{mix}}^{2}}{\delta^{2}}+\frac{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}{ \delta^{2}}T^{\frac{2}{5}}\right)+\tilde{\mathcal{O}}\bigg{(}\frac{Lt_{ \mathrm{mix}}t_{\mathrm{hit}}\mathbf{E}_{s\sim d^{\pi^{*}}}[KL(\pi^{*}(\cdot |s)\|\pi_{\theta_{1}}(\cdot|s))]}{\delta}T^{\frac{2}{5}}\bigg{)}\\ &+\tilde{\mathcal{O}}\left(\frac{\alpha AGt_{\mathrm{mix}}}{ \delta t_{\mathrm{hit}}}\left[\left(\sqrt{A}Gt_{\mathrm{mix}}+\delta\right)T^ {\frac{2}{5}}+\sqrt{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}T^{\frac{3}{10}}\right] \right)+\mathcal{O}(t_{\mathrm{mix}})\end{split}\] (89)

Similarly, for the constraint violation, we have

\[\begin{split}\mathbf{E}[\mathrm{Vio}_{T}]&=\sum_{t=0 }^{T-1}\left(-c(s_{t},a_{t})\right)=H\sum_{k=1}^{K}-J_{c}(\theta_{k})+\sum_{k=1 }^{K}\sum_{t\in\mathcal{I}_{k}}\left(J_{c}(\theta_{k})-c(s_{t},a_{t})\right)\\ &=-H\sum_{k=1}^{K}J_{c}(\theta_{k})+\mathbf{E}\left[\sum_{k=1}^{K -1}V_{c}^{\pi_{\theta_{k+1}}}(s_{kH})-V_{c}^{\pi_{\theta_{k}}}(s_{kH})\right] +\mathbf{E}\left[V_{c}^{\pi_{\theta_{K}}}(s_{T})-V_{c}^{\pi_{\theta_{0}}}(s_{ 0})\right]\end{split}\] (90)

Using the result in (79), Lemma 8 and Lemma 9, we get,

\[\begin{split}\mathbf{E}[\mathrm{Vio}_{T}]&\leq T \delta\sqrt{\epsilon_{\mathrm{bias}}}+G\left(1+\frac{1}{\mu_{F}}\right) \tilde{\mathcal{O}}\left(\left[\delta+\sqrt{A}Gt_{\mathrm{mix}}\right]T^{ \frac{4}{5}}+\sqrt{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}T^{\frac{7}{10}}\right) \\ &+\mathcal{O}\left(\frac{t_{\mathrm{mix}}t_{\mathrm{hit}}}{ \delta}T^{\frac{4}{5}}+\frac{1}{\delta T}+\delta T^{\frac{3}{5}}\right)+ \frac{B}{L}\tilde{\mathcal{O}}\left(\frac{\delta^{2}+AG^{2}t_{\mathrm{mix}}^{ 2}}{\delta}T^{\frac{3}{5}}+\frac{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}{\delta}T^ {\frac{2}{5}}\right)\\ &+\tilde{\mathcal{O}}\bigg{(}Lt_{\mathrm{mix}}t_{\mathrm{hit}} \mathbf{E}_{s\sim d^{\pi^{*}}}[KL(\pi^{*}(\cdot|s)\|\pi_{\theta_{1}}(\cdot|s) )]T^{\frac{7}{5}}\bigg{)}\\ &+\tilde{\mathcal{O}}\left(\frac{\alpha AGt_{\mathrm{mix}}}{ \delta t_{\mathrm{hit}}}\left[\left(\sqrt{A}Gt_{\mathrm{mix}}+\delta\right)T^ {\frac{7}{5}}+\sqrt{Lt_{\mathrm{mix}}t_{\mathrm{hit}}}T^{\frac{3}{10}}\right] \right)+\mathcal{O}(t_{\mathrm{mix}})\end{split}\] (91)

This concludes the theorem. 

## Appendix D Some Auxiliary Lemmas for the Proofs

**Lemma 9**.: _[_17_, Lemma 14]_ _For any ergodic MDP with mixing time \(t_{\mathrm{mix}}\), the following holds \(\forall(s,a)\in\mathcal{S}\times\mathcal{A}\), any policy \(\pi\) and \(\forall g\in\{r,c\}\)._

\[(a)|V_{g}^{\pi}(s)|\leq 5t_{\mathrm{mix}},\ \ (b)|Q_{g}^{\pi}(s,a)|\leq 6t_{ \mathrm{mix}}\]

**Lemma 10**.: _[_17_, Corollary 13.2]_ _Let \(\delta^{\pi}(\cdot,T)\) be defined as written below for an arbitrary policy \(\pi\)._

\[\delta^{\pi}(s,T)\triangleq\sum_{t=N}^{\infty}\left\|(P^{\pi})^{t}(s,\cdot)-d^{ \pi}\right\|_{1},\ \forall s\in\mathcal{S}\ \text{where}\ N=4t_{\mathrm{mix}}(\log_{2}T)\] (92)

_If \(t_{\mathrm{mix}}<T/4\), we have the following inequality \(\forall s\in\mathcal{S}\): \(\delta^{\pi}(s,T)\leq\frac{1}{T^{3}}\)._

**Lemma 11**.: _[_17_, Lemma 16]_ _Let \(\mathcal{I}=\{t_{1}+1,t_{1}+2,\cdots,t_{2}\}\) be a certain period of an epoch \(k\) of Algorithm 2 with length \(N\). Then for any \(s\), the probability that the algorithm never visits \(s\) in \(\mathcal{I}\) is upper bounded by_

\[\left(1-\frac{3d^{\pi_{\theta_{k}}}(s)}{4}\right)^{\left\lfloor\frac{|\mathcal{ I}|}{N}\right\rfloor}\] (93)

**Lemma 12**.: _[_17_, Lemma 15]_ _The difference of the values of the function \(J_{g}\), \(g\in\{r,c\}\) at policies \(\pi\) and \(\pi^{\prime}\), is_

\[J_{g}^{\pi}-J_{g}^{\pi^{\prime}}=\sum_{s}\sum_{a}d^{\pi}(s)(\pi(a|s)-\pi^{ \prime}(a|s))Q_{g}^{\pi^{\prime}}(s,a)\] (94)

**Lemma 13**.: _[_6_, Lemma 7]_ _The term \(\hat{J}_{c}(\theta)\) for any \(\theta\in\Theta\) is a good estimator of \(J_{c}(\theta)\), which means_

\[\left|\mathbf{E}[\hat{J}_{c}(\theta)]-J_{c}(\theta)\right|\leq\frac{1}{T^{2}}\] (95)

**Lemma 14**.: _[_36_, Lemma A.6]_ _Let \(\theta\in\Theta\) be a policy parameter. Fix a trajectory \(z=\{(s_{t},a_{t},r_{t},s_{t+1})\}_{t\in\mathbb{N}}\) generated by following the policy \(\pi_{\theta}\) starting from some initial state \(s_{0}\sim\rho\). Let, \(\nabla L(\theta)\) be the gradient that we wish to estimate over \(z\), and \(l(\theta,\cdot)\) is a function such that \(\mathbf{E}_{z\sim d^{\pi_{\theta}},\pi_{\theta}}l(\theta,z)=\nabla L(\theta)\). Assume that \(\|l(\theta,z)\|,\|\nabla L(\theta)\|\leq G_{L}\), \(\forall\theta\in\Theta\), \(\forall z\in\mathcal{S}\times\mathcal{A}\times\mathbb{R}\times\mathcal{S}\). Define \(l^{Q}=\frac{1}{Q}\sum_{i=1}^{Q}l(\theta,z_{i})\). If \(P=2t_{\max}\log T\), then the following holds as long as \(Q\leq T\),_

\[\mathbf{E}\left[\left\|l^{Q}-\nabla L(\theta)\right\|^{2}\right]\leq\mathcal{ O}\left(G_{L}^{2}\log\left(PQ\right)\frac{P}{Q}\right)\] (96)

**Lemma 15** (Strong duality).: _[_37_, Lemma 3]_ _For convenience, we rewrite the unparameterized problem (2)._

\[\max_{\pi\in\Pi}\;J_{r}^{\pi}\] (97) _s.t._ \[J_{c}^{\pi}\geq 0\]

_Define \(\pi^{*}\) as the optimal solution to the above problem. Define the associated dual function as_

\[J_{D}^{\lambda}\triangleq\max_{\pi\in\Pi}J_{r}^{\pi}+\lambda J_{c}^{\pi}\] (98)

_and denote \(\lambda^{*}=\arg\min_{\lambda\geq 0}J_{D}^{\lambda}\). We have the following strong duality property for the unparameterized problem whenever Assumption 2 holds._

\[J_{r}^{\pi^{*}}=J_{D}^{\lambda^{*}}\] (99)

Although the strong duality holds for the unparameterized problem, the same is not true for parameterized class \(\{\pi_{\theta}|\theta\in\Theta\}\). To formalize this statement, define the dual function associated with the parameterized problem as follows.

\[J_{D,\Theta}^{\lambda}\triangleq\max_{\theta\in\Theta}J_{r}(\theta)+\lambda J _{c}(\theta)\] (100)

and denote \(\lambda_{\Theta}^{*}=\arg\min_{\lambda\geq 0}J_{D,\Theta}^{\lambda}\). The lack of strong duality states that, in general, \(J_{D,\Theta}^{\lambda_{\Theta}^{*}}\neq J_{r}(\theta^{*})\) where \(\theta^{*}\) is a solution of the parameterized constrained optimization (3). However, the parameter \(\lambda_{\Theta}^{*}\), as we demonstrate below, must obey some restrictions.

**Lemma 16**.: _Under Assumption 2, the optimal dual variable for the parameterized problem is bounded as_

\[0\leq\lambda_{\Theta}^{*}\leq\frac{J_{r}^{\pi^{*}}-J_{r}(\bar{\theta})}{\delta }\leq\frac{1}{\delta}\] (101)

Proof.: The proof follows the approach in [37, Lemma 3], but is revised to the general parameterization setup. Let \(\Lambda_{a}\triangleq\{\lambda\geq 0\,|\,J_{D,\Theta}^{\lambda}\leq a\}\) be a sublevel set of the dual function for \(a\in\mathbb{R}\). If \(\Lambda_{a}\) is non-empty, then for any \(\lambda\in\Lambda_{a}\),

\[a\geq J_{D,\Theta}^{\lambda}\geq J_{r}(\bar{\theta})+\lambda J_{c}(\bar{ \theta})\geq J_{r}(\bar{\theta})+\lambda\delta\] (102)

where \(\bar{\theta}\) is a Slater point in Assumption 2. Thus, \(\lambda\leq(a-J_{r}(\bar{\theta}))/\delta\). If we take \(a=J_{D,\Theta}^{\lambda_{\Theta}^{*}}\leq J_{D,\Theta}^{\lambda^{*}}\leq J_{ D}^{\lambda^{*}}=J_{r}^{\pi^{*}}\), then we have \(\lambda_{\Theta}^{*}\in\Lambda_{a}\), which proves the Lemma. The last inequality holds since \(J_{r}^{\pi}\in[0,1]\) for any policy, \(\pi\)Since the above inequality holds for arbitrary \(\Theta\), we also have, \(0\leq\lambda^{*}\leq\frac{1}{\delta}\). Define \(v(\tau)\triangleq\max_{\pi\in\Pi}\{J_{r}^{\pi}|J_{c}^{\pi}\geq\tau\}.\) Using the strong duality property of the unparameterized problem (97), we establish the following property of the function, \(v(\cdot)\).

**Lemma 17**.: _Assume that the Assumption 2 holds, we have for any \(\tau\in\mathbb{R}\),_

\[v(0)-\tau\lambda^{*}\geq v(\tau)\] (103)

Proof.: By the definition of \(v(\tau)\), we have \(v(0)=J_{r}^{\pi^{*}}\). With a slight abuse of notation, denote \(J_{\mathrm{L}}(\pi,\lambda)=J_{r}^{\pi}+\lambda J_{c}^{\pi}\). By the strong duality stated in Lemma 15, we have the following for any \(\pi\in\Pi\).

\[J_{\mathrm{L}}(\pi,\lambda^{*})\leq\max_{\pi\in\Pi}J_{\mathrm{L}}(\pi, \lambda^{*})\stackrel{{ Def}}{{=}}J_{D}^{\lambda^{*}}\stackrel{{ \eqref{eq:v(0)}}}{{=}}J_{r}^{\pi^{*}}=v(0)\] (104)

Thus, for any \(\pi\in\{\pi\in\Pi\,|\,J_{c}^{\pi}\geq\tau\}\),

\[v(0)-\tau\lambda^{*} \geq J_{\mathrm{L}}(\pi,\lambda^{*})-\tau\lambda^{*}\] (105) \[=J_{r}^{\pi}+\lambda^{*}(J_{c}^{\pi}-\tau)\geq J_{r}^{\pi}\]

Maximizing the right-hand side of this inequality over \(\{\pi\in\Pi|J_{c}^{\pi}\geq\tau\}\) yields

\[v(0)-\tau\lambda^{*}\geq v(\tau)\] (106)

This completes the proof of the lemma. 

We note that a similar result was shown in [38, Lemma 15]. However, the setup of the stated paper is different from that of ours. Specifically, [38] considers a tabular setup with peak constraints. Note that Lemma 17 has no direct connection with the parameterized setup since its proof uses strong duality and the function, \(v(\cdot)\), is defined via a constrained optimization over the entire policy set, \(\Pi\), rather than the parameterized policy set. Interestingly, however, the relationship between \(v(\tau)\) and \(v(0)\) leads to the lemma stated below which turns out to be pivotal in establishing regret and constraint violation bounds in the parameterized setup.

**Lemma 18**.: _Let Assumption 2 hold. For any constant \(C\geq 2\lambda^{*}\), if there exists a \(\pi\in\Pi\) and \(\zeta>0\) such that \(J_{r}^{\pi^{*}}-J_{r}^{\pi}+C[-J_{c}^{\pi}]\leq\zeta\), then_

\[-J_{c}^{\pi}\leq 2\zeta/C\] (107)

Proof.: Let \(\tau=J_{c}^{\pi}\). Using the definition of \(v(\tau)\), one can write,

\[J_{r}^{\pi}\leq v(\tau)\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The contribution and challenges are clearly described at the end of the introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We add all the assumptions in the work, list the gap with lower bound in Table 1, and give future work direction. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: All the assumptions are clearly stated with a remark to discuss. All the proof are given in the appendix in details. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: the paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: the paper does not include experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: the paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: the paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: the paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: the research conducted in the paper satisfies the NeurIPS Code of Ethics Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: there is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: the paper poses no such risks Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: the paper does not use existing assets Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: the paper does not use release new assets Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: the paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.