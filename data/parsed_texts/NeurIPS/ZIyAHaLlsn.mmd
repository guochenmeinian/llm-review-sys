# ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting

Zongsheng Yue Jianyi Wang Chen Change Loy

S-Lab, Nanyang Technological University

{zongsheng.yue,jianyi001,ccloy}@ntu.edu.sg

###### Abstract

Diffusion-based image super-resolution (SR) methods are mainly limited by the low inference speed due to the requirements of hundreds or even thousands of sampling steps. Existing acceleration sampling techniques inevitably sacrifice performance to some extent, leading to over-blurry SR results. To address this issue, we propose a novel and efficient diffusion model for SR that significantly reduces the number of diffusion steps, thereby eliminating the need for post-acceleration during inference and its associated performance deterioration. Our method constructs a Markov chain that transfers between the high-resolution image and the low-resolution image by shifting the residual between them, substantially improving the transition efficiency. Additionally, an elaborate noise schedule is developed to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experiments demonstrate that the proposed method obtains superior or at least comparable performance to current state-of-the-art methods on both synthetic and real-world datasets, _even only with 15 sampling steps_. Our code and model are available at https://github.com/zsyOAOA/ResShift.

## 1 Introduction

Image super-resolution (SR) is a fundamental problem in low-level vision, aiming at recovering the high-resolution (HR) image given the low-resolution (LR) one. This problem is severely ill-posed due to the complexity and unknown nature of degradation models in real-world scenarios. Recently, diffusion model [1, 2], a newly emerged generative model, has achieved unprecedented success in image generation [3]. Furthermore, it has also demonstrated great potential in solving several downstream low-level vision tasks, including image editing [4, 5], image inpainting [6, 7], image colorization [8, 9]. There is also ongoing research exploring the potential of diffusion models to tackle the long-standing and challenging SR task.

One common approach [10, 11] involves inserting the LR image into the input of current diffusion model (e.g., DDPM [2]) and retraining the model from scratch on the training data for SR. Another popular way [7, 12, 13, 14] is to use an unconditional pre-trained diffusion model as a prior and modify its reverse path to generate the expected HR image. Unfortunately, both strategies inherit the Markov chain underlying DDPM, which can be inefficient in inference, often taking hundreds or even thousands of sampling steps. Although some acceleration techniques [15, 16, 17] have been developed to compress the sampling steps in inference, they inevitably lead to a significant drop in performance, resulting in over-smooth results as shown in Fig. 1, in which the DDIM [16] algorithm is employed to speed up the inference. Thus, there is a need to design a new diffusion model for SR that achieves both efficiency and performance, without sacrificing one for the other.

Let us revisit the diffusion model in the context of image generation. In the forward process, it builds up a Markov chain to gradually transform the observed data into a pre-specified prior distribution, typically a standard Gaussian distribution, over a large number of steps. Subsequently, imagegeneration can be achieved by sampling a noise map from the prior distribution and feeding it into the reverse path of the Markov chain. While the Gaussian prior is well-suited for the task of image generation, it may not be optimal for SR, where the LR image is available. In this paper, we argue that the reasonable diffusion model for SR should start from a prior distribution based on the LR image, enabling an iterative recovery of the HR image from its LR counterpart instead of Gaussian white noise. Additionally, such a design can reduce the number of diffusion steps required for sampling, thereby improving inference efficiency.

Following the aforementioned motivation, we propose an efficient diffusion model involving a shorter Markov chain for transitioning between the HR image and its corresponding LR one. The initial state of the Markov chain converges to an approximate distribution of the HR image, while the final state converges to an approximate distribution of the LR image. To achieve this, we carefully design a transition kernel that shifts the residual between them step by step. This approach is more efficient than existing diffusion-based SR methods since the residual information can be quickly transferred in dozens of steps. Moreover, our design also allows for an analytical and concise expression for the evidence lower bound, easing the induction of the optimization objective for training. Based on this constructed diffusion kernel, we further develop a highly flexible noise schedule that controls the shifting speed of the residual and the noise strength in each step. This schedule facilitates a fidelity-realism trade-off of the recovered results by tuning its hyper-parameters.

In summary, the main contributions of this work are as follows:

* We present an efficient diffusion model for SR, which renders an iterative sampling procedure from the LR image to the desirable HR one by shifting the residual between them during inference. Extensive experiments demonstrate the superiority of our approach in terms of efficiency, as it requires only 15 sampling steps to achieve appealing results, outperforming or at least being comparable to current diffusion-based SR methods that require a long sampling process. A preview of our recovered results compared with existing methods is shown in Fig. 1.
* We formulate a highly flexible noise schedule for the proposed diffusion model, enabling more precise control of the shifting of residual and noise levels during the transition.

## 2 Methodology

In this section, we present a diffusion model, _ResShift_, which is tailored for SR. For ease of presentation, the LR and HR images are denoted as \(\bm{y}_{0}\) and \(\bm{x}_{0}\), respectively. Furthermore, we assume \(\bm{y}_{0}\) and \(\bm{x}_{0}\) have identical spatial resolution, which can be easily achieved through pre-upsampling the LR image \(\bm{y}_{0}\) using nearest neighbor interpolation if necessary.

Figure 1: Qualitative comparisons on one typical real-world example of the proposed method and recent state of the arts, including BSRGAN [18], RealESRGAN [19], SwinIR [20], DASR [21], and LDM [11]. As for LDM and our method, we mark the number of sampling steps with the format of “LDM (or Ours)-A” for more intuitive visualization, where “A” is the number of sampling steps. Note that LDM contains 1000 diffusion steps in training and is accelerated to “A” steps using DDIM [16] during inference. Please zoom in for a better view.

### Model Design

The iterative generation paradigm of diffusion models has proven highly effective at capturing complex distributions, inspiring us to approach the SR problem iteratively as well. Our proposed method constructs a Markov chain that serves as a bridge between the HR and LR images as shown in Fig. 2. This way, the SR task can be accomplished by reverse sampling from this Markov chain given any LR image. Next, we will detail the process of building such a Markov chain specifically for SR.

**Forward Process**. Let's denote the residual between the LR and HR images as \(\bm{e}_{0}\), i.e., \(\bm{e}_{0}=\bm{y}_{0}-\bm{x}_{0}\). Our core idea is to transit from \(\bm{x}_{0}\) to \(\bm{y}_{0}\) by gradually shifting their residual \(\bm{e}_{0}\) through a Markov chain with length \(T\). A shifting sequence \(\{\eta_{t}\}_{t=1}^{T}\) is first introduced, which monotonically increases with the timestep \(t\) and satisfies \(\eta_{1}\to 0\) and \(\eta_{T}\to 1\). The transition distribution is then formulated based on this shifting sequence as follows:

\[q(\bm{x}_{t}|\bm{x}_{t-1},\bm{y}_{0})=\mathcal{N}(\bm{x}_{t};\bm{x}_{t-1}+ \alpha_{t}\bm{e}_{0},\kappa^{2}\alpha_{t}\bm{I}),\;t=1,2,\cdots,T,\] (1)

where \(\alpha_{t}=\eta_{t}-\eta_{t-1}\) for \(t>1\) and \(\alpha_{1}=\eta_{1}\), \(\kappa\) is a hyper-parameter controlling the noise variance, \(\bm{I}\) is the identity matrix. Notably, we show that the marginal distribution at any timestep \(t\) is analytically integrable, namely

\[q(\bm{x}_{t}|\bm{x}_{0},\bm{y}_{0})=\mathcal{N}(\bm{x}_{t};\bm{x}_{0}+\eta_{t }\bm{e}_{0},\kappa^{2}\eta_{t}\bm{I}),\;t=1,2,\cdots,T.\] (2)

The design of the transition distribution presented in Eq. (1) is based on two primary principles. The first principle concerns the standard deviation, i.e., \(\kappa\sqrt{\alpha_{t}}\), which aims to facilitate a smooth transition between \(\bm{x}_{t}\) and \(\bm{x}_{t-1}\). This is because the expected distance between \(\bm{x}_{t}\) and \(\bm{x}_{t-1}\) can be bounded by \(\sqrt{\alpha_{t}}\), given that the image data falls within the range of \([0,1]\), i.e.,

\[\text{max}[(\bm{x}_{0}+\eta_{t}\bm{e}_{0})-(\bm{x}_{0}+\eta_{t-1}\bm{e}_{0})]= \text{max}[\alpha_{t}\bm{e}_{0}]<\alpha_{t}<\sqrt{\alpha_{t}},\] (3)

where \(\text{max}[\cdot]\) represents the pixel-wise maximizing operation. The hyper-parameter \(\kappa\) is introduced to increase the flexibility of this design. The second principle pertains to the mean parameter, i.e., \(\bm{x}_{0}+\alpha_{t}\bm{e}_{0}\), which induces the marginal distribution in Eq. (2). Furthermore, the marginal distributions of \(\bm{x}_{1}\) and \(\bm{x}_{T}\) converges to \(\delta_{\bm{x}_{0}}(\cdot)\)1 and \(\mathcal{N}(\cdot;\bm{y}_{0},\kappa^{2}\bm{I})\), which act as two approximate distributions for the HR image and the LR image, respectively. By constructing the Markov chain in such a thoughtful way, it is possible to handle the SR task by inversely sampling from it given the LR image \(\bm{y}_{0}\).

Footnote 1: \(\delta_{\bm{\mu}}(\cdot)\) denotes the Dirac distribution centered at \(\bm{\mu}\).

**Reverse Process**. The reverse process aims to estimate the posterior distribution \(p(\bm{x}_{0}|\bm{y}_{0})\) via the following formulation:

\[p(\bm{x}_{0}|\bm{y}_{0})=\int p(\bm{x}_{T}|\bm{y}_{0})\prod_{t=1}^{T}p_{\bm{ \theta}}(\bm{x}_{t-1}|\bm{x}_{t},\bm{y}_{0})\text{d}\bm{x}_{1:T},\] (4)

where \(p(\bm{x}_{T}|\bm{y}_{0})\approx\mathcal{N}(\bm{x}_{T}|\bm{y}_{0},\kappa^{2} \bm{I})\), \(p_{\bm{\theta}}(\bm{x}_{t-1}|\bm{x}_{t},\bm{y}_{0})\) is the inverse transition kernel from \(\bm{x}_{t}\) to \(\bm{x}_{t-1}\) with a learnable parameter \(\bm{\theta}\). Following most of the literature in diffusion model [1, 2, 8], we adopt the assumption of \(p_{\bm{\theta}}(\bm{x}_{t-1}|\bm{x}_{t},\bm{y}_{0})=\mathcal{N}(\bm{x}_{t-1}; \bm{\mu}_{\bm{\theta}}(\bm{x}_{t},\bm{y}_{0},t),\bm{\Sigma}_{\bm{\theta}}(\bm{ x}_{t},\bm{y}_{0},t))\). The optimization for \(\bm{\theta}\) is achieved by minimizing the negative evidence lower bound, namely,

\[\min_{\bm{\theta}}\sum_{t}D_{\text{KL}}\left[q(\bm{x}_{t-1}|\bm{x}_{t},\bm{x}_ {0},\bm{y}_{0})\|p_{\bm{\theta}}(\bm{x}_{t-1}|\bm{x}_{t},\bm{y}_{0})\right],\] (5)

Figure 2: Overview of the proposed method. It builds up a Markov chain between the HR/LR image pair by shifting their residual.

where \(D_{\text{KL}}[\cdot\|\cdot]\) denotes the Kullback-Leibler (KL) divergence. More mathematical details can be found in Sohl-Dickstein et al. [1] or Ho et al. [2].

Combining Eq. (1) and Eq. (2), the targeted distribution \(q(\bm{x}_{t-1}|\bm{x}_{t},\bm{x}_{0},\bm{y}_{0})\) in Eq. (5) can be rendered tractable and expressed in an explicit form given below:

\[q(\bm{x}_{t-1}|\bm{x}_{t},\bm{x}_{0},\bm{y}_{0})=\mathcal{N}\left(\bm{x}_{t-1} \middle|\frac{\eta_{t-1}}{\eta_{t}}\bm{x}_{t}+\frac{\alpha_{t}}{\eta_{t}}\bm{x} _{0},\kappa^{2}\frac{\eta_{t-1}}{\eta_{t}}\alpha_{t}\bm{I}\right).\] (6)

The detailed calculation of this derivation is presented in the supplementary material. Considering that the variance parameter is independent of \(\bm{x}_{t}\) and \(\bm{y}_{0}\), we thus set \(\bm{\Sigma}_{\bm{\theta}}(\bm{x}_{t},\bm{y}_{0},t)=\kappa^{2}\frac{\eta_{t-1} }{\eta_{t}}\alpha_{t}\bm{I}\). As for the mean parameter \(\bm{\mu}_{\bm{\theta}}(\bm{x}_{t},\bm{y}_{0},t)\), it is reparameterized as follows:

\[\bm{\mu}_{\bm{\theta}}(\bm{x}_{t},\bm{y}_{0},t)=\frac{\eta_{t-1}}{\eta_{t}}\bm {x}_{t}+\frac{\alpha_{t}}{\eta_{t}}f_{\bm{\theta}}(\bm{x}_{t},\bm{y}_{0},t),\] (7)

where \(f_{\bm{\theta}}\) is a deep neural network with parameter \(\bm{\theta}\), aiming to predict \(\bm{x}_{0}\). We explored different parameterization forms on \(\bm{\mu}_{\theta}\) and found that Eq. (7) exhibits superior stability and performance.

Based on Eq. (7), we simplify the objective function in Eq. (5) as follows,

\[\min_{\bm{\theta}}\sum\nolimits_{t}w_{t}\|f_{\bm{\theta}}(\bm{x}_{t},\bm{y}_{ 0},t)-\bm{x}_{0}\|_{2}^{2},\] (8)

where \(w_{t}=\frac{\alpha_{t}}{2\kappa^{2}\eta_{t}\eta_{t-1}}\). In practice, we empirically find that the omission of weight \(w_{t}\) results in an evident improvement in performance, which aligns with the conclusion in Ho et al. [2].

**Extension to Latent Space**. To alleviate the computational overhead in training, we move the aforementioned model into the latent space of VQGAN [22], where the original image is compressed by a factor of four in spatial dimensions. This does not require any modifications on our model other than substituting \(\bm{x_{0}}\) and \(\bm{y}_{0}\) with their latent codes. An intuitive illustration is shown in Fig. 2.

### Noise Schedule

The proposed method employs a hyper-parameter \(\kappa\) and a shifting sequence \(\{\eta_{t}\}_{t=1}^{T}\) to determine the noise schedule in the diffusion process. Specifically, the hyper-parameter \(\kappa\) regulates the overall noise intensity during the transition, and its impact on performance is empirically discussed in Sec. 4.2. The subsequent exposition mainly revolves around the construction of the shifting sequence \(\{\eta_{t}\}_{t=1}^{T}\).

Equation (2) implies that the noise level in state \(\bm{x}_{t}\) is proportional to \(\sqrt{\eta_{t}}\) with a scaling factor \(\kappa\). This observation motivates us to focus on designing \(\sqrt{\eta_{t}}\) instead of \(\eta_{t}\). Song and Ermon [23] show that \(\kappa\sqrt{\eta_{1}}\) should be sufficiently small (e.g., 0.04 in LDM [11]) to ensure that \(q(\bm{x}_{1}|\bm{x}_{0},\bm{y}_{0})\approx q(\bm{x}_{0})\). Combining with the additional constraint of \(\eta_{1}\to 0\), we set \(\eta_{1}\) to be the minimum value between \(\nicefrac{{(0.04)}}{{\kappa}}^{2}\) and \(0.001\). For the final step \(T\), we set \(\eta_{T}\) as 0.999 ensuring \(\eta_{T}\to 1\). For the intermediate timesteps, i.e., \(t\in[2,T-1]\), we propose a non-uniform geometric schedule for \(\sqrt{\eta_{t}}\) as follows:

\[\sqrt{\eta_{t}}=\sqrt{\eta_{1}}\times b_{0}^{\beta_{t}},\ t=2,\cdots,T-1,\] (9)

where

\[\beta_{t}=\left(\frac{t-1}{T-1}\right)^{p}\times(T-1),\ b_{0}=\exp\left[\frac{ 1}{2(T-1)}\log\frac{\eta_{T}}{\eta_{1}}\right].\] (10)

Note that the choice of \(\beta_{t}\) and \(b_{0}\) is based on the assumption of \(\beta_{1}=0\), \(\beta_{T}=T-1\), and \(\sqrt{\eta_{T}}=\sqrt{\eta_{1}}\times b_{0}^{T-1}\). The hyper-parameter \(p\) controls the growth rate of \(\sqrt{\eta_{t}}\) as shown in Fig. 3(h).

The proposed noise schedule exhibits high flexibility in three key aspects. First, for small values of \(\kappa\), the final state \(\bm{x}_{T}\) converges to a perturbation around the LR image as depicted in Fig. 3(c)-(d). Compared to the corruption ended at Gaussian noise, this design considerably shortens the length of the Markov chain, thereby improving the inference efficiency. Second, the hyper-parameter \(p\) provides precise control over the shifting speed, enabling a fidelity-realism trade-off in the SR results as analyzed in Sec. 4.2. Third, by setting \(\kappa=40\) and \(p=0.8\), our method achieves a diffusion process remarkably similar to LDM [11]. This is clearly demonstrated by the visual results during the diffusion process presented in Fig. 3(e)-(f), and further supported by the comparisons on the relative noise strength as shown in Fig. 3(g).

## 3 Related Work

**Diffusion Model**. Inspired by the non-equilibrium statistical physics, Sohl-Dickstein et al. [1] firstly proposed the diffusion model to fit complex distributions. Ho et al. [2] established a novel connection between the diffusion model and the denoising scoring matching. Later, Song et al. [8] proposed a unified framework to formulate the diffusion model from the perspective of the stochastic differential equation (SDE). Attributed to its robust theoretical foundation, the diffusion model has achieved impressive success in the generation of images [3; 11], audio [24], graph [25] and shapes [26].

**Image Super-Resolution**. Traditional image SR methods primarily focus on designing more rational image priors based on our subjective knowledge, such as non-local similarity [27], low-rankness [28], sparsity [29; 30], and so on. With the development of deep learning (DL), Dong et al. [31] proposed the seminal work SRCNN to solve the SR task using a deep neural network. Then DL-based SR methods rapidly dominated the research field. Various SR technologies were explored from different perspectives, including network architecture [32; 33; 34; 35], image prior [36; 37; 38; 39], deep unfolding [40; 41; 42], degradation model [18; 19; 43; 44].

Recently, some works have investigated the application of diffusion models in SR. A prevalent approach is to concatenate the LR image with the noise in each step and retrain the diffusion model from scratch [10; 11; 45]. Another popular way is to utilize an unconditional pre-trained diffusion model as a prior and incorporate additional constraints to guide the reverse process [7; 12; 13; 46]. Both strategies often require hundreds or thousands of sampling steps to generate a realistic HR image. While several acceleration algorithms [15; 16; 17] have been proposed, they typically sacrifice the performance and result in blurry outputs. This work designs a more efficient diffusion model that overcomes this trade-off between efficiency and performance, as detailed in Sec. 2.

**Remark**. Several parallel works [47; 48; 49] also exploit such an iterative restoration paradigm in SR. Despite a similar motivation, our work and others have adopted different mathematical formulations to achieve this goal. Delbracio and Milanfar [47] employed the Inversion by Direct Iteration (InDI) to model this process, while Luo et al. [48] and Liu et al. [49] attempted to formulate it as a SDE. In this paper, we design a discrete Markov chain to depict the transition between the HR and LR images, offering a more intuitive and efficient solution to this problem.

Figure 3: Illustration of the proposed noise schedule. (a) HR image. (b) Zoomed LR image. (c)-(d) Diffused images of _ResShift_ in timesteps of 1, 3, 5, 7, 9, 12, and 15 under different values of \(\kappa\) by fixing \(p=0.3\) and \(T=15\). (e)-(f) Diffused images of _ResShift_ with a specified configuration of \(\kappa=40,p=0.8,T=1000\) and LDM [11] in timesteps of 100, 200, 400, 600, 800, 900, and 1000. (g) The relative noise intensity (vertical axes, measured by \(\sqrt{\nicefrac{{1}}{{\lambda_{\text{swr}}}}}\), where \(\lambda_{\text{snr}}\) denotes the signal-to-noise ratio) of the schedules in (d) and (e) w.r.t. the timesteps (horizontal axes). (h) The shifting speed \(\sqrt{\eta_{t}}\) (vertical axes) w.r.t. to the timesteps (horizontal axes) across various configurations of \(p\). Note that the diffusion processes in this figure are implemented in the latent space, but we display the intermediate results after decoding back to the image space for the purpose of easy visualization.

## 4 Experiments

This section presents an empirical analysis of the proposed _ResShift_ and provides extensive experimental results to verify its effectiveness on one synthetic dataset and three real-world datasets. Following [18; 19], our investigation specifically focuses on the more challenging \(\times 4\) SR task. Due to page limitation, some experimental results are put in the supplementary material.

### Experimental Setup

**Training Details**. HR images with a resolution of \(256\times 256\) in our training data are randomly cropped from the training set of ImageNet [50] following LDM [11]. We synthesize the LR images using the degradation pipeline of RealESRGAN [19]. The Adam [51] algorithm with the default settings of PyTorch [52] and a mini-batch size of 64 is used to train _ResShift_. During training, we use a fixed learning rate of \(5\)e-\(5\) and update the weight parameters for 500K iterations. As for the network architecture, we employ the UNet structure in DDPM [2]. To increase the robustness of

\begin{table}
\begin{tabular}{c|c|c|c c c c} \hline \multicolumn{3}{c|}{Configurations} & \multicolumn{4}{c}{Metrics} \\ \hline \(T\) & \(p\) & \(\kappa\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & CLIPIQAT & MUSIQ\(\uparrow\) \\ \hline
10 & & & & 25.20 & 0.6828 & 0.2517 & 0.5492 & 50.6617 \\
15 & & & & 25.01 & 0.6769 & 0.2312 & 0.5922 & 53.6596 \\
30 & 0.3 & 2.0 & 24.52 & 0.6585 & 0.2253 & 0.6273 & 55.7904 \\
40 & & & & 24.29 & 0.6513 & 0.2225 & 0.6468 & 56.8482 \\
50 & & & & 24.22 & 0.6483 & 0.2212 & 0.6489 & 56.8463 \\ \hline \hline \multirow{4}{*}{15} & 0.3 & & 25.01 & 0.6769 & 0.2312 & 0.5922 & 53.6596 \\  & 0.5 & & 25.05 & 0.6745 & 0.2387 & 0.5816 & 52.4475 \\ \cline{1-1}  & 1.0 & 2.0 & 25.12 & 0.6780 & 0.2613 & 0.5314 & 48.4964 \\ \cline{1-1}  & 2.0 & & 25.32 & 0.6827 & 0.3050 & 0.4601 & 43.3060 \\ \cline{1-1}  & 3.0 & & 25.39 & 0.5813 & 0.3432 & 0.4041 & 38.5324 \\ \hline \hline \multirow{4}{*}{15} & & 0.5 & 24.90 & 0.6709 & 0.2437 & 0.5700 & 50.6101 \\  & & 1.0 & 24.84 & 0.6699 & 0.2354 & 0.5914 & 52.9933 \\ \cline{1-1}  & 0.3 & 2.0 & 25.01 & 0.6769 & 0.2312 & 0.5922 & 53.6596 \\ \cline{1-1}  & & 8.0 & 25.31 & 0.6858 & 0.2592 & 0.5231 & 49.3182 \\ \cline{1-1}  & & 16.0 & 24.46 & 0.6891 & 0.2772 & 0.4898 & 46.9794 \\ \hline \end{tabular}
\end{table}
Table 1: Performance comparison of _ResShift_ on the _ImageNet-Test_ under different configurations.

Figure 4: Qualitative comparisons of _ResShift_ under different combinations of (\(T\), \(p\), \(\kappa\)). For example, “(15, 0.3, 2.0)” represents the recovered result with \(T=15\), \(p=0.3\), and \(\kappa=2.0\). Please zoom in for a better view.

_ResShift_ to arbitrary image resolution, we replace the self-attention layer in UNet with the Swin Transformer [53] block.

**Testing Datasets**. We synthesize a testing dataset that contains 3000 images randomly selected from the validation set of ImageNet [50] based on the commonly-used degradation model, i.e., \(\bm{y}=(\bm{x}*\bm{k})\downarrow+\bm{n}\), where \(k\) is the blurring kernel, \(n\) is the noise, \(\bm{y}\) and \(\bm{x}\) denote the LR image and HR image, respectively. To comprehensively evaluate the performance of _ResShift_, we consider more complicated types of blurring kernels, downsampling operators, and noise types. The detailed settings on them can be found in the supplementary material. It should be noted that we selected the HR images from ImageNet [50] instead of the prevailing datasets in SR such as _Set5_[54], _Set14_[55], and _Urban100_[56]. The rationale behind this setting is rooted in the fact that these datasets only contain very few source images, which fails to thoroughly evaluate the performance of various methods under different degradation types. We name this dataset as _ImageNet-Test_ for convenience.

Two real-world datasets are adopted to evaluate the efficacy of _ResShift_. The first is _RealSR_[57], containing 100 real images captured by Canon 5D3 and Nikon D810 cameras. Additionally, we collect another real-world dataset named _RealSet5_. It comprises 35 LR images widely used in recent literature [19; 58; 59; 60; 61]. The remaining 30 images were obtained from the internet by ourselves.

**Compared Methods**. We evaluate the effectiveness of _ResShift_ in comparison to seven recent SR methods, namely ESRGAN [62], RealSR-JPEG [63], BSRGAN [18], RealESRGAN [19], SwinIR [20], DASR [21], and LDM [11]. Note that LDM is a diffusion-based method with 1,000 diffusion steps. For a fair comparison, we accelerate LDM to the same number of steps with _ResShift_ using DDIM [16] and denote it as "LDM-A", where "A" indicates the number of inference steps. The hyper-parameter \(\eta\) in DDIM is set to be 1 as this value yields the most realistic recovered images.

**Metrics**. The performance of various methods was assessed using five metrics, including PSNR, SSIM [64], LPIPS [65], MUSIQ [66], and CLIPIQA [67]. It is worth noting that the latter two are non-reference metrics specifically designed to assess the realism of images. CLIPIQA, in particular, leverages the CLIP [68] model that is pre-trained on a massive dataset (i.e., Laion400M [69]) and thus demonstrates strong generalization ability. On the real-world datasets, we mainly rely on CLIPIQA and MUSIQ as evaluation metrics to compare the performance of different methods.

### Model Analysis

We analyze the performance of _ResShift_ under different settings on the number of diffusion steps \(T\) and the hyper-parameters \(p\) in Eq. (10) and \(\kappa\) in Eq. (1).

**Diffusion Steps \(T\) and Hyper-parameter \(p\)**. The proposed transition distribution in Eq. (1) significantly reduces the diffusion steps \(T\) in the Markov chain. The hyper-parameter \(p\) allows for flexible control over the speed of residual shifting during the transition. Table 1 summarizes the performance of _ResShift_ on _ImageNet-Test_ under different configurations of \(T\) and \(p\). We can see that both of \(T\) and \(p\) render a trade-off between the fidelity, measured by the reference metrics such as PSNR, SSIM, and LPIPS, and the realism, measured by the non-reference metrics, including CLIPIQA and MUSIQ, of the super-resolved results. Taking \(p\) as an example, when it increases, the reference metrics improve while the non-reference metrics deteriorate. Furthermore, the visual comparison in Fig. 4 shows that a large value of \(p\) will suppress the model's ability to hallucinate more image details and result in blurry outputs.

**Hyper-parameter \(\kappa\)**. Equation (2) reveals that \(\kappa\) dominates the noise strength in state \(\bm{x}_{t}\). We report the influence of \(\kappa\) to the performance of _ResShift_ in Table 1. Combining with the visualization in

\begin{table}
\begin{tabular}{c|c c c|c c c|c} \hline \hline \multirow{2}{*}{Metrics} & \multicolumn{6}{c}{Methods} \\ \cline{2-9}  & \multicolumn{1}{c}{BSRGAN} & RealESRGAN & SwinIR & LDM-15 & LDM-30 & LDM-100 & _ResShift_ \\ \hline PSNR\(\uparrow\) & 24.42 & 24.04 & 23.99 & 24.89 & 24.49 & 23.90 & 25.01 \\ LPIPS\(\downarrow\) & 0.259 & 0.254 & 0.238 & 0.269 & 0.248 & 0.244 & 0.231 \\ CLIPIQA\(\uparrow\) & 0.581 & 0.523 & 0.564 & 0.512 & 0.572 & 0.620 & 0.592 \\ Runtime (s) & 0.012 & 0.013 & 0.046 & 0.102 & 0.184 & 0.413 & 0.105 \\ \hline \# Parameters (M) & 16.70 & 16.70 & 28.01 & & 113.60 & & 118.59 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Efficiency and performance comparisons of _ResShift_ to other methods on the dataset of _ImageNet-Test_. “LDM-A” represents the results achieved by accelerated the sampling steps of LDM [11] to “A”. Running time is tested on NVIDIA Tesla V100 GPU on the x4 (64\(\rightarrow\) 256) SR task.

Fig. 4, we can find that excessively large or small values of \(\kappa\) will smooth the recovered results, regardless of their favorable metrics of PSNR and SSIM. When \(\kappa\) is in the range of \([1.0,2.0]\), our method achieves the most realistic quality indicated by CLIPIQA and MUSIQ, which is more desirable in real applications. We thus set \(\kappa\) to be \(2.0\) in this work.

**Efficiency Comparison**. To improve inference efficiency, it is desirable to limit the number of diffusion steps \(T\). However, this causes a decrease in the realism of the restored HR images. To compromise, the hyper-parameter \(p\) can be set to a relatively small value. Therefore, we set \(T=15\) and \(p=0.3\), and yield our model named _ResShift_. Table 2 presents the efficiency and performance comparisons of _ResShift_ to the state-of-the-art (SotA) approach LDM [11] and three other GAN-based methodologies on _ImageNet-Test_ dataset. It is evident from the results that the proposed _ResShift_ surpasses LDM [11] in terms of PSNR and LPIPS [65], and demonstrates a remarkable fourfold enhancement in computational efficiency when compared to LDM-100. Despite showing considerable potential in mitigating the efficiency bottleneck of the diffusion-based SR approaches, _ResShift_ still lags behind current GAN-based methods in speed due to its iterative sampling mechanism. Therefore, it remains imperative to explore further optimizations of the proposed method to address this limitation, which we leave in our future work.

**Perception-Distortion Trade-off.** There exists a well-known phenomenon called perception-distortion trade-off [70] in the field of SR. In particular, the augmentation of the generative capability of a restoration model, such as elevating the sampling steps for a diffusion-based method or amplifying the weight of the adversarial loss for a GAN-based method, will result in a deterioration in fidelity preservation while concurrently enhancing the authenticity of restored images. That is mainly because the restoration model with powerful generation capability tends to hallucinate more high-frequency image structures, thereby deviating from the underlying ground truth. To facilitate a comprehensive comparison between our _ResShift_ and current SotA diffusion-based method LDM, we plotted the perception-distortion curves of them in Fig. 7, wherein the perception and distortion are mea

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{3}{c}{Metrics} \\ \cline{2-6}  & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) & CLIPIQA\(\uparrow\) & MUSIQ\(\uparrow\) \\ \hline ESRGAN [62] & 20.67 & 0.448 & 0.485 & 0.451 & 43.615 \\ RealSR-JPEG [63] & 23.11 & 0.591 & 0.326 & 0.537 & 46.981 \\ BSRGAN [18] & 24.42 & 0.659 & 0.259 & 0.581 & **54.697** \\ SwinIR [20] & 23.99 & 0.667 & 0.238 & 0.564 & 53.790 \\ RealESRGAN [19] & 24.04 & 0.665 & 0.254 & 0.523 & 52.538 \\ DASR [21] & 24.75 & 0.675 & 0.250 & 0.536 & 48.337 \\ LDM-15 [11] & 24.89 & 0.670 & 0.269 & 0.512 & 46.419 \\ _ResShift_ & **25.01** & **0.677** & **0.231** & **0.592** & 53.660 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative results of different methods on the dataset of _ImageNet-Test_. The best and second best results are highlighted in **bold** and underline.

Figure 5: Qualitative comparisons of different methods on two synthetic examples of the _ImageNet-Test_ dataset. Please zoom in for a better view.

Figure 7: Perception-distortion trade-off of _ResShift_ and LDM. The vertical and horizontal axes represent the strength of the perception and distortion, measured by LPIPS and MSE, respectively.

sured by LPIPS and mean square-error (MSE), respectively. This plot reflects the perception quality and the reconstruction fidelity of _ResShift_ and LDM across varying numbers of diffusion steps, i.e., 10, 15, 20, 30, 40, and 50. As can be observed, the perception-distortion curve of our _ResShift_ consistently resides beneath that of the LDM, indicating its superior capacity in balancing perception and distortion.

### Evaluation on Synthetic Data

We present a comparative analysis of the proposed method with recent SotA approaches on the _ImageNet-Test_ dataset, as summarized in Table 3 and Fig. 5. Based on this evaluation, several significant conclusions can be drawn as follows: i) _ResShift_ exhibits superior or at least comparable performance across all five metrics, affirming the effectiveness and superiority of the proposed method. ii) The notably higher PSNR as SSIM values attained by _ResShift_ indicate its capacity to better preserve fidelity to ground truth images. This advantage primarily arises from our well-designed diffusion model, which starts from a subtle disturbance of the LR image, rather than the conventional assumption of white Gaussian noise in LDM. iii) Considering the metrics of LPIPS and CLIPIQA, which gauge the perceptual quality and realism of the recovered image, _ResShift_ also demonstrates evident superiority over existing methods. Furthermore, in terms of MUSIQ, our approach achieves comparable performance with recent SotA methods. In summary, the proposed _ResShift_ exhibits remarkable capabilities in generating more realistic results while preserving fidelity. This is of paramount importance for the task of SR.

### Evaluation on Real-World Data

Table 4 lists the comparative evaluation using CLIPIQA [67] and MUSIQ [66] of various methods on two real-world datasets. Note that CLIPIQA, benefiting from the powerful representative capability inherited from CLIP, performs stably and robustly in assessing the perceptional quality of natural images. The results in Table 4 show that the proposed _ResShift_ evidently surpasses existing methods in CLIPIQA, meaning that the restored outputs of _ResShift_ better align with human visual and perceptive systems. In the case of MUSIQ evaluation, _ResShift_ achieves the competitive performance when compared to current SotA methods, namely BSRGAN [18], SwinIR [20], and RealESRGAN [19]. Collectively, our method shows promising capability in addressing the real-world SR problem.

We display four real-world examples in Fig. 6. More examples can be found in the supplementary material. We consider diverse scenarios, including comic, text, face, and natural images to ensure a comprehensive evaluation. A noticeable observation is that _ResShift_ produces more naturalistic image structures, as evidenced by the patterns on the beam in the third example and the eyes of a person in the fourth example. We note that the recovered results of LDM are excessively smooth when compressing the inference steps to match with the proposed _ResShift_, specifically utilizing 15 steps, largely deviating from the training procedure's 1,000 steps. Even though other GAN-based methods may also succeed in hallucinating plausible structures to some extent, they are often accompanied by obvious artifacts.

## 5 Conclusion

In this work, we have introduced an efficient diffusion model named _ResShift_ for SR. Unlike existing diffusion-based SR methods that require a large number of iterations to achieve satisfactory results,

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{4}{c|}{Datasets} \\ \cline{2-5}  & \multicolumn{2}{c|}{_RealSR_} & \multicolumn{2}{c}{_RealSet65_} \\ \cline{2-5}  & CLIPIQA\(\dagger\) & MUSIQ\(\dagger\) & CLIPIQA\(\dagger\) & MUSIQ\(\dagger\) \\ \hline ESRGAN [62] & 0.2362 & 29.048 & 0.3739 & 42.369 \\ RealSR-JPEG [63] & 0.3615 & 36.076 & 0.5282 & 50.539 \\ BSRGAN [18] & 0.5439 & **63.586** & 0.6163 & **65.582** \\ SwinIR [20] & 0.4654 & 59.636 & 0.5782 & 63.822 \\ RealESRGAN [19] & 0.4898 & 59.678 & 0.5995 & 63.220 \\ DASR [21] & 0.3629 & 45.825 & 0.4965 & 55.708 \\ LDM-15 [11] & 0.3836 & 49.317 & 0.4274 & 47.488 \\ _ResShift_ & **0.5958** & 59.873 & **0.6537** & 61.330 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Quantitative results of different methods on two real-world datasets. The best and second best results are highlighted in **bold** and underline.

our proposed method constructs a diffusion model with only 15 sampling steps, thereby significantly improving inference efficiency. The core idea is to corrupt the HR image toward the LR image instead of the Gaussian white noise, which can effectively cut off the length of the diffusion model. Extensive experiments on both synthetic and real-world datasets have demonstrated the superiority of our proposed method. We believe that our work will pave the way for the development of more efficient and effective diffusion models to address the SR problem.

**Acknowledgement.** This study is supported under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).

Figure 6: Qualitative comparisons on four real-world examples. Please zoom in for a better view.

## References

* [1] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning (ICML)_, pages 2256-2265. PMLR, 2015.
* [2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, volume 33, pages 6840-6851, 2020.
* [3] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, volume 34, pages 8780-8794, 2021.
* [4] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2021.
* [5] Omri Avraham, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 18208-18218, 2022.
* [6] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11461-11471, June 2022.
* [7] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12413-12422, 2022.
* [8] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2021.
* [9] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In _Proceedings of ACM SIGGRAPH Conference_, pages 1-10, 2022.
* [10] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 2022.
* [11] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, 2022.
* [12] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 14367-14376, 2021.
* [13] Zongsheng Yue and Chen Change Loy. Diffuse: Blind face restoration with diffused error contraction. _arXiv preprint arXiv:2212.06512_, 2022.
* [14] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C.K. Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. _arXiv preprint arXiv:2305.07015_, 2023.
* [15] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning (ICML)_, pages 8162-8171. PMLR, 2021.
* [16] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2021.
* [17] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. DPM-solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [18] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model for deep blind image super-resolution. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 4791-4800, 2021.

* [19] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In _Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCV-W)_, pages 1905-1914, 2021.
* [20] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In _Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCV-W)_, pages 1833-1844, 2021.
* [21] Jie Liang, Hui Zeng, and Lei Zhang. Efficient and degradation-adaptive network for real-world image super-resolution. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 574-591, 2022.
* [22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12873-12883, 2021.
* [23] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, volume 32, 2019.
* [24] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. WaveGrad: estimating gradients for waveform generation. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2020.
* [25] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant graph generation via score-based generative modeling. In _Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 108, pages 4474-4484, 2020.
* [26] Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and Bharath Hariharan. Learning gradient fields for shape generation. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 364-381, 2020.
* [27] Weisheng Dong, Lei Zhang, Guangming Shi, and Xin Li. Nonlocally centralized sparse representation for image restoration. _IEEE Transactions on Image Processing (TIP)_, 22(4):1620-1630, 2012.
* [28] Shuhang Gu, Qi Xie, Deyu Meng, Wangmeng Zuo, Xiangchu Feng, and Lei Zhang. Weighted nuclear norm minimization and its applications to low level vision. _International Journal of Computer Vision (IJCV)_, 121:183-208, 2017.
* [29] Weisheng Dong, Lei Zhang, Guangming Shi, and Xiaolin Wu. Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization. _IEEE Transactions on Image Processing (TIP)_, 20(7):1838-1857, 2011.
* [30] Shuhang Gu, Wangmeng Zuo, Qi Xie, Deyu Meng, Xiangchu Feng, and Lei Zhang. Convolutional sparse coding for image super-resolution. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 1823-1831, 2015.
* [31] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 38(2):295-307, 2015.
* [32] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1874-1883, 2016.
* [33] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. _IEEE Transactions on Image Processing (TIP)_, 26(7):3142-3155, 2017.
* [34] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan Yang. Deep laplacian pyramid networks for fast and accurate super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 624-632, 2017.
* [35] Muhammad Haris, Gregory Shakhnarovich, and Norimichi Ukita. Deep back-projection networks for super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1664-1673, 2018.

* [36] Jingyun Liang, Kai Zhang, Shuhang Gu, Luc Van Gool, and Radu Timofte. Flow-based kernel prior with application to blind super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10601-10610, 2021.
* [37] Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. GLEAN: Generative latent bank for large-factor image super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 14245-14254, 2021.
* [38] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 44(11):7474-7489, 2021.
* [39] Zongsheng Yue, Qian Zhao, Jianwen Xie, Lei Zhang, Deyu Meng, and Kwan-Yee K. Wong. Blind image super-resolution with elaborate degradation modeling on noise and kernel. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2128-2138, 2022.
* [40] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Deep plug-and-play super-resolution for arbitrary blur kernels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1671-1681, 2019.
* [41] Kai Zhang, Luc Van Gool, and Radu Timofte. Deep unfolding network for image super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3217-3226, 2020.
* [42] Jiahong Fu, Hong Wang, Qi Xie, Qian Zhao, Deyu Meng, and Zongben Xu. Kxnet: A model-driven deep neural network for blind super-resolution. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 235-253, 2022.
* [43] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Learning a single convolutional super-resolution network for multiple degradations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 3262-3271, 2018.
* [44] Chong Mou, Yanze Wu, Xintao Wang, Chao Dong, Jian Zhang, and Ying Shan. Metric learning based interactive modulation for real-world super-resolution. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 723-740, 2022.
* [45] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single image super-resolution with diffusion probabilistic models. _Neurocomputing_, 479:47-59, 2022.
* [46] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [47] Mauricio Delbracio and Peyman Milanfar. Inversion by direct iteration: An alternative to denoising diffusion for image restoration. _arXiv preprint arXiv:2303.11435_, 2023.
* [48] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas B Schon. Image restoration with mean-reverting stochastic differential equations. _arXiv preprint arXiv:2301.11699_, 2023.
* [49] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar. I\({}^{2}\)SB: Image-to-image schrodinger bridge. In _International Conference on Machine Learning (ICML)_, 2023.
* [50] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 248-255, 2009.
* [51] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2015.
* [52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, volume 32, 2019.
* [53] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 10012-10022, 2021.

* [54] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. 2012.
* [55] Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In _International Conference on Curves and Surfaces_, pages 711-730. Springer, 2012.
* [56] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5197-5206, 2015.
* [57] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: A new benchmark and a new model. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 3086-3095, 2019.
* [58] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, volume 2, pages 416-423, 2001.
* [59] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. _Multimedia Tools and Applications_, 76:21811-21838, 2017.
* [60] Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc Van Gool. Dslr-quality photos on mobile devices with deep convolutional networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2017.
* [61] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. _IEEE Transactions on Image Processing (TIP)_, 27(9):4608-4622, 2018.
* [62] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In _Proceedings of the European Conference on Computer Vision Workshops (ECCV-W)_, 2018.
* [63] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. Real-world super-resolution via kernel estimation and noise injection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (CVPR-W)_, pages 466-467, 2020.
* [64] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE Transactions on Image Processing (TIP)_, 13(4):600-612, 2004.
* [65] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 586-595, 2018.
* [66] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 5148-5157, 2021.
* [67] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2023.
* [68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, pages 8748-8763, 2021.
* [69] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. In _Proceedings of Advances in Neural Information Processing Systems Workshops (NeurIPS-W)_, 2021.
* [70] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2018.