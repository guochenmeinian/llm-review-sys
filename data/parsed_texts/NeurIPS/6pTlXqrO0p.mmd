# xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token

 Xin Cheng\({}^{1}\)1 **Xun Wang\({}^{2}\)**Xingxing Zhang\({}^{2}\)**Tao Ge\({}^{2}\)

**Si-Qing Chen\({}^{2}\)**Furu Wei\({}^{2}\)**Huishuai Zhang\({}^{1,3}\)**Dongyan Zhao\({}^{1,3}\)

\({}^{1}\) Peking University \({}^{2}\) Microsoft

\({}^{3}\) National Key Laboratory of General Artificial Intelligence

chengxin1998@stu.pku.edu.cn

Work done during internship at Microsoft, corresponding to Xun Wang, Huishuai Zhang and Dongyan Zhao

###### Abstract

This paper introduces xRAG, a novel context compression method designed specifically for retrieval-augmented generation. xRAG redefines the use of document embeddings in dense retrieval--traditionally limited to retrieval purposes--by integrating them as features from the retrieval modality. Through a modality fusion approach, xRAG effectively merges these embeddings into the language model's representation space, eliminating the need for their textual counterparts and achieving an extreme compression rate. In xRAG, the modality bridge is the only trainable component, while the retriever and language model remain frozen. This design choice allows for the reuse of offline-constructed document embeddings and preserves the plug-and-play nature of retrieval augmentation. Experimental results demonstrate that xRAG achieves an average improvement of over 10% across six knowledge-intensive tasks, compatible with various language model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts configuration. xRAG not only significantly outperforms previous context compression methods but also matches the performance of uncompressed models on several benchmarks, while reducing overall FLOPs by a factor of 3.53. This work pioneers new avenues in retrieval-augmented generation through multimodal fusion, potentially setting a groundwork for future developments in efficient and scalable retrieval systems.

## 1 Introduction

Retrieval-Augmented Language Models (RALMs) [42, 22, 9, 14, 70] have shown exceptional performance in a variety of knowledge-intensive tasks. By retrieving domain-specific, long-tailed, and up-to-date knowledge from a non-parametric datastore, RALMs significantly extend the boundaries of parametric Large Language Models (LLMs). However, the integration of entire documents into prompts can significantly increase inference costs and may surpass the context limit of LLMs [29, 79]. As illustrated in Figure 1, while the inclusion of a relevant document enables the LLM to generate accurate responses, it does so at the expense of processing documents that expand the original query by more than tenfold.

How might we mitigate the costs associated with extended context while maintaining the benefits of retrieval augmentation? Recent research interest has converged on a promising direction: Context Compression. This concept is pursued through two primary strategies: soft-prompting methods, such as Gist [59], AutoCompressor [15], and ICAE [20], which compress the context into dense memory slots, and hard-prompting methods, such as LLMLingua [29] and RECOMP [79], wherecompression is applied on the surface form. These approaches, however, either require significant memory for storing LLM activations (e.g., 1.05 MB per token as reported by [59]) or suffer from relatively low compression rates. More critically, these methods overlook a crucial characteristic of RALMs: through large-scale contrastive learning with question-document pairs, modern dense retrieval systems already distill document content into a single high-dimensional embedding, and _this embedding reveals (almost) as much information as the text_[58, 44].

In this paper, we pioneer an innovative approach to retrieval augmentation and context compression through the lens of modality fusion. Drawing from multimodal research, where text-only language models are taught to "perceive" and "listen," a pretrained modality encoder like CLIP [64] is typically used to extract modality features. These features are then integrated into language models using a modality fusion bridge [45, 52, 34]. Building on the conceptual overlap between the retriever encoder and modality encoder, we introduce xRAG. This model redefines document embeddings from dense retrieval--traditionally solely for retrieval purposes--as retrieval modality features. xRAG employs a modality fusion methodology to seamlessly integrate these embeddings into the language model's representation space, thus obviating the need for textual counterparts and achieving significant context compression. In xRAG, the modality bridge is the only trainable component, while both the retriever and the LLM are kept frozen. This design decision facilitates the reuse of pre-constructed document embeddings and maintains the plug-and-play nature of retrieval augmentation--two essential factors for a functional RAG system.

To verify the effectiveness and versatility of our framework, we conducted comprehensive experiments with different LLM backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts model. Our results reveal that adding just one document token could lead to over a 10% improvement across six knowledge-intensive tasks, significantly surpassing previous compression methods. xRAG also delivers results comparable to uncompressed models on several benchmarks. This is remarkable considering that the only trainable component constitutes less than 0.1% of the LLM's parameters. In terms of efficiency, xRAG reduces total FLOPs by a factor of 3.53 compared to the uncompressed RAG model. We further provide detailed analyses of xRAG, examining various training strategies, data blends, and component selections for the retrieval system. We believe this research sets a strong foundation for the development of future efficient and scalable retrieval-augmented systems.

## 2 Related Work

Retrieval-augmented GenerationEquipping a parametric language model with a non-parametric datastore has proven effective for a range of NLP tasks, including language modeling [37, 57, 87],

Figure 1: xRAG enables efficient retrieval augmentation by adding one document token \([\texttt{X}]\).

open-domain question answering [25; 42; 70; 86], domain adaptation [7] and machine translation [36; 12], among others. Given the vast design space of this generation paradigm, numerous approaches with different focuses have been proposed. For instance, RETRO [9] and PlugLM [13] introduce architectural innovations for enhanced integration with the non-parametric datastore. REALM [22] pioneers an end-to-end approach for simultaneous optimization of the language model and retriever. REPLUG [70] and RA-DIT [51] improve retriever alignment using feedback from LLMs. DSP [38] and InteR [18] investigate complex interactions between the retriever and the language model. Selfmen [14] utilizes a reward model to refine retrieval and generation iteratively. Self-RAG [4] incorporates a self-reflection mechanism to enhance the quality and factuality of language model outputs. For a detailed overview, see [19; 5; 3]. Our contribution, xRAG, stands out by implementing a modality fusion approach to retrieval augmentation, creating an effective and efficient RAG system.

Context CompressionContext compression, aimed at reducing the input length for LLMs while retaining essential information, has recently attracted substantial interest [47]. Gist [59] achieves a compression rate of up to 26x by modifying the attention mask and caching soft gist token activations. ICAE [20]AutoCompressor [15], and 500xCompressor[48] condense lengthy contexts into succinct, compact memory slots, which are directly utilizable by LLMs for diverse functions. LLMLingua [29; 30; 62] and CompAct[82] introduces a coarse-to-fine prompt compression technique based on perplexity scores and distilled token-level score. While these methods are generally applicable, others are tailored specifically for RAG systems, such as FilCo [76] and RECOMP [79]. A concurrent work directly employs passage embeddings for efficient listwise reranking [53]. For an in-depth comparison of these compression methods regarding memory efficiency, compression rates, and adaptability, refer to Appendix A.

## 3 Methods

Problem FormulationIn retrieval-augmented generation, a non-parametric datastore \(\mathbb{D}=\{(\mathrm{E}_{i},\mathrm{D}_{i})\}_{i=1}^{|\mathbb{D}|}\) consists of pairs where each \(\mathrm{D}_{i}\) represents a document chunk as a sequence of \(L_{i}\) tokens \(\mathrm{D}_{i}=\{d_{1}^{i},\ldots,d_{L_{i}}^{i}\}\). Correspondingly, \(\mathrm{E}_{i}\) is the dense representation derived from a sentence embedding model \(\mathbf{SE}_{\theta}(\cdot)\) with input \(D_{i}\). For an input query \(q\), its dense representation \(\mathbf{SE}_{\theta}(q)\) is used to find the relevant documents by matching against the collection \(\{\mathrm{E}_{i}\}_{i=1}^{|\mathbb{D}|}\) with certain similarity search algorithm such as MIPS. After retrieval, the system selects a relevant pair \((\mathrm{E},\mathrm{D})\) from \(\mathbb{D}\), concatenates the chosen document D with \(q\), and processes the combined input with a language model

Figure 2: Overview of xRAG (a) and RAG (b). For a given query, RAG typically concatenates the retrieved document with the query, significantly extending the context length. In contrast, xRAG addresses this issue through modality fusion by directly projecting the document embedding into the LLMâ€™s representation space. This allows for efficient retrieval-augmentation with the addition of only one token.

\(\boldsymbol{\mathcal{F}}_{\phi}(\mathrm{D}\oplus q)\). Optionally, a context compression module \(\boldsymbol{\mathcal{C}}\) can be integrated to reduce the length of \(\mathrm{D}\) from \(L\) to a more concise \(l\), achieving a compression ratio of \(\frac{L}{l}\).

### xRAG Architecture

Traditional methods for document compression typically focus on surface form of the document [29; 76; 79]. In contrast xRAG tackle the problem from a modality fusion view. Concretely, we introduce a modality projector \(\mathbf{W}\), which is trained to directly project the retrieval features \(\mathrm{E}\) into the LLM representation space. Our proposed framework is visually contrasted with the traditional RAG system in Figure 2. In the standard RAG, the input to the LLM comprises the embeddings \(\mathtt{Emb}(\mathrm{D}\oplus q)\) of length \(|\mathrm{D}|+|q|\), where \(\mathtt{Emb}\) signifies the embedding layer of the LLM. Conversely, with xRAG, the modified input is represented as \(\mathbf{W}(\mathrm{E})\oplus\mathtt{Emb}(q)\), which yields a substantially reduced length of \(1+|q|\). In this framework, the challenges come from the modality fusion: How can a text-only language model understand features from retrieval modality? To achieve this, we explore a two-stage training strategy: Paraphrase Pretraining followed by Context-aware Instruction Tuning.

### Paraphrase Pretraining

Similar to the pretraining strategies employed in vision-language models that use image-captioning data to align two modalities [52; 16; 54], the primary objective of our paraphrase pretraining is to build a compatible representation between the extracted retrieval feature and the corresponding document. Illustrated in Figure 3(a), for each pair \((\mathrm{E},\mathrm{D})\) in a retrieval corpus \(\mathbb{D}\), we employ a natural language instruction \(\mathbf{X}_{\mathtt{instruct}}\) to prompt the LLM to undertake a paraphrasing task (e.g. "[X] The above text could be paraphrased as: [D]", where [X] and [D] are placeholders for \(\mathbf{W}(\mathrm{E})\) and document \(\mathrm{D}\))2. In this setup, the model learns to connect \(\mathbf{W}(\mathrm{E})\) and \(\mathrm{D}\) by recovering \(\mathrm{D}\) on the condition of \(\mathbf{W}(\mathrm{E})\) and the model is optimized by:

Footnote 2: To maintain diversity, we sample from an instruction pool, which could be found in Appendix B.

\[\mathcal{L}_{\mathtt{nll}}=-\sum_{i=1}\text{log }p_{\phi}(d_{i}|\mathbf{W}( \mathrm{E}),\mathbf{X}_{\mathtt{instruct}},d_{<i})\] (1)

where \(p_{\phi}\) is given by the softmax distribution of LLM \(\boldsymbol{\mathcal{F}}_{\phi}\), and \(d_{<i}\) denotes the document token before current prediction token \(d_{i}\), achieved by casual attention mask in auto-regressive LMs.

### Context-aware Instruction Tuning

After the pretraining phase, although the language model \(\boldsymbol{\mathcal{F}}_{\phi}\) has developed an internally compatible representation, it has never been explicitly trained to utilize these features for downstream tasks. To address this gap, we proceed to instruct the model in harnessing the fused feature \(\mathbf{W}(\mathrm{E})\) by continually training the model on data where the answer is closely associated with the given context, including reading comprehension, summarization, and open domain question answering data. We

Figure 3: Two-stage training strategy of xRAG including (a) Paraphrase Pre-training on unlabeled corpus and (b) Context-aware Instruction Tuning optimized with labeled data and self-distillation.

constructed an mixed dataset, containing approximately 1 million entries from open-source datasets, as detailed in Appendix C. For each triplet in the dataset, \((\mathbf{X}_{\texttt{context}},\mathbf{X}_{\texttt{question}},\mathbf{X}_{ \texttt{answer}})\), we initially obtain the sentence representation for \(\mathbf{X}_{\texttt{context}}\) via the embedding model \(\text{E}_{\texttt{context}}=\mathbf{SE}_{\theta}(\mathbf{X}_{\texttt{ context}})\). Subsequently, we refine the optimization of on two directions:

Optimization I: Language Modeling.Aligned with established instruction tuning methodologies [75; 24; 51; 43], our objective is to finetune the model so that it generates the correct output when provided with a specific instruction, conditioned upon the given context information. Unlike traditional models that utilize the textual context \(\mathbf{X}_{\texttt{context}}\), our method employs a dense feature \(\text{E}_{\texttt{context}}\) to encapsulate the context information:

\[\mathcal{L}_{\texttt{nll}}=-\sum_{i=1}\text{log}\;p_{\phi}(\mathbf{X}_{ \texttt{answer},i}|\mathbf{W}(\text{E}_{\texttt{context}}),\mathbf{X}_{ \texttt{question}},\mathbf{X}_{\texttt{answer},<i})\] (2)

Optimization II: Self-Distillation.The second trajectory of optimization aims to guide the xRAG in the effective utilization of contextual information, drawing from the principles of self-distillation [2; 71] and imitation learning [61; 23]. By considering the RAG model as a "teacher" and xRAG as a "student", we endeavor to distill the knowledge from RAG, thereby enabling xRAG to emulate the RAG model's proficiency in handling the full, uncompressed documents. This approach enhances xRAG's resilience in scenarios where it encounters noisy or irrelevant context that may not directly lead to the correct answer, detailedly discussed in SS 6.1. Concretely, for a language model \(\boldsymbol{\mathcal{F}}_{\phi}\) using either \(\mathbf{X}_{\texttt{context}}\) or \(\text{E}_{\texttt{context}}\) as the source of context, our objective is to minimize the divergence between the two resulting output distributions. This discrepancy is measured using the Kullback-Leibler (KL) divergence:

\[\mathcal{L}_{\texttt{kl}}=D_{\text{KL}}(p_{\phi}(\mathbf{X}_{\texttt{answer}}| \mathbf{X}_{\texttt{context}},\cdot)\parallel p_{\phi}(\mathbf{X}_{\texttt{answer}}| \mathbf{W}(\text{E}_{\texttt{context}}),\cdot))\] (3)

Here \(\mathbf{X}_{\texttt{Question}}\) is omitted for brevity and the final loss is the linear combination controlled by a hyperparameter: \(\mathcal{L}_{nll}+\alpha\mathcal{L}_{kl}\).

### Design Principle

In designing the projector \(\mathbf{W}\), our primary objective is to maintain the simplicity of the framework. We therefore opted for a two-layer MLP while other more sophisticated module such as Q-Former [45] could also be considered. Notice that the projector is the only trainable component, accounting for only 0.46% of the total parameters in the Mistral-7b model and 0.07% in the Mistral-8x7b model. Such a design choice departs from previous studies that necessitated full-parameter tuning to adapt LLMs for compressed contexts [59; 76; 15]. We believe this approach will likely be more accessible and practical because, fundamentally, the RAG itself functions as a plug-and-play module for LLMs, and so should its compressed version. This design also avoid the risk of compromising other core capabilities of LLM during full-parameter tuning, as observed in [55; 56].

Moreover, in contrast to other compression methods that necessitate storing LLM activations for each compressed token [59; 20; 15]--an impractical strategy in the RAG setting, given the millions of documents involved--our method introduces no additional memory overhead. Instead, it leverages offline-constructed document embeddings, originally designed for retrieval. To summarize, xRAG not only simplifies the integration process but also avoids unnecessary computational or memory expenses.

## 4 Experimental Setup

### Evaluation Dataset

We evaluated the performance of xRAG primarily on knowledge-intensive tasks which encompass a range of challenges: (1) three Open Domain Question Answering datasets that address questions on a wide array of topics: Natural Questions [41], TriviaQA [33], and Web Questions [8]. (2) one Multihop Question Answering dataset, HotpotQA [81], which necessitates multi-step reasoning to generate answers. (3) one Long-form Question Answering dataset, TruthfulQA [50], that requires the generation of long-form and truthful responses. (4) one fact-checking dataset, FactKG [39], which challenges the model to use complex reasoning to determine the factual accuracy of given claims.

In line with the KILT [63] and GenRead [84], we assessed three ODQA datasets and HotpotQA using the Exact Match (EM) metric, FactKG with Accuracy, and for the long-form QA, we used both the F1 score and Rouge-L (R-L) score. These tasks demand a broad spectrum of world knowledge and have been extensively explored in the retrieval-augmentation literature [42; 22; 35; 10; 25; 70].

### Implementation Details

To demonstrate the versatility of our framework, we choose two backbones, differing in scale and architecture: Mistral-7b [27] and Mistral-8x7b [28]. For the retrieval corpus, we utilized the Wikipedia dump from December 2021, which was pre-processed into passages following the methodology described in [26]. This resulted in approximately 37 million passages, each averaging 180 tokens in length. Our default retrieval model is the SFR [1], which, at the time of writing this paper, holds the leading position on the MTEB leaderboard [60]. We use _top-1_ ranked document for inclusion in our instruction-tuning dataset and for the evaluation of downstream tasks. More details are provided in Appendix D. Code is available at: https://github.com/Hannibal046/xRAG.

### Baselines

In determining appropriate baselines for comparison, we adhered to a fundamental principle: the selected compression methods must support the general plug-and-play capability of retrieval augmentation. This entails that they should function effectively without the need for dataset-specific tuning [79; 76], or any alteration to the parameters of LLMs [76; 59]. Furthermore, given the extensive volume of the retrieval corpus, it is essential that these compression methods demonstrate memory efficiency, specifically by not requiring the storage of LLM activations for each individual token [59; 20; 15]. With these criteria in mind, our evaluation compares xRAG to the following baselines: **(I)** Primarily, we consider two variants of LLMs: one that operates without retrieval augmentation and another that includes it. These serve as the lower and upper performance bounds for our study of compression techniques, respectively. **(II)** Additionally, our comparisons extend to LLMingua [29], a plug-and-play approach for context compression. **(III)** Taking inspiration from [59], we incorporate a method of discrete compression using TF-IDF. This approach yields compression rates comparable to those achieved by xRAG and serves as the lower bound for discrete compression.

## 5 Experimental Results

### Knowledge Intensive Tasks

In Table 1, we present our main results. Across both Mistral-7b and Mistral-8x7b configurations, we observe a consistent and significant improvement when retrieval augmentation is applied (p-value

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & **NQ** & **TriviaQA** & **WebQA** & **HotpotQA** & **TrutfulQA** & **FactKG** & **Average** & **\# Doc** \\ \multirow{2}{*}{**Task Type**} & \multicolumn{3}{c}{Open-Domain QA} & \multicolumn{3}{c}{Multiop QA} & \multicolumn{3}{c}{Long-form QA} & \multicolumn{3}{c}{Fact Checking} & \multirow{2}{*}{**Average**} & \multicolumn{1}{c}{\multirow{2}{*}{**Length**}} \\  & \multicolumn{3}{c}{(EM)} & \multicolumn{3}{c}{(EM)} & \multicolumn{3}{c}{(EM)} & \multicolumn{3}{c}{(F1-R-L)} & \multicolumn{3}{c}{(Acc)} & \\ \hline
**Mistral-7b** & & & & & & & & & \\ w/o retrieval & 30.25 & 57.08 & 34.89 & 27.02 & 26.23 & 25.51 & 54.78 & 36.54 (0.0\%) & 0 \\
**w retrieval & **24.71** & **65.88** & 37.84 & **38.79** & 26.50 & 25.92 & **67.76** & **43.63** (19.4\%) & 175.1 \\
**with Compression** & & & & & & & & & \\ LLMingua \({}^{\dagger}\) & 30.64 & 57.94 & 32.63 & 29.91 & 25.70 & 25.10 & 64.17 & 38.01 (4.0\%) & 98.6 \\ LLMingua \({}^{\dagger}\) & 28.81 & 57.09 & 32.33 & 29.13 & 26.10 & 25.39 & 63.57 & 37.48 (2.5\%) & 61.1 \\ TF-IDF & 30.25 & 58.49 & 35.43 & 26.62 & 26.33 & 25.83 & 59.56 & 37.49 (2.6\%) & 1 \\ xRAG & 39.10 & 65.77 & **39.40** & 34.05 & **28.10** & **27.71** & 63.08 & 42.46 (16.2\%) & 1 \\ \hline
**Mistral-8x7b** & & & & & & & & & \\ w/o retrieval & 41.99 & 71.10 & 40.31 & 32.87 & 25.60 & 24.90 & 62.64 & 42.76 (0.0\%) & 0 \\
**w** retrieval & 45.15 & 70.34 & 41.26 & **43.46** & 27.10 & 25.80 & **70.42** & 46.22 (8.0\%) & 175.1 \\
**with Compression** & & & & & & & & & \\ LLMingua\({}^{\dagger}\) & 37.65 & 67.70 & 36.02 & 35.66 & 25.99 & 25.39 & 67.98 & 42.32 (-1.0\%) & 96.6 \\ LLMingua\({}^{\dagger}\) & 37.81 & 67.81 & 35.78 & 35.27 & 25.68 & 25.00 & 68.03 & 44.17 (-1.3\%) & 61.1 \\ TF-IDF & 41.19 & 69.94 & 41.63 & 32.05 & 26.80 & 26.00 & 66.17 & 43.41 (1.4\%) & 1 \\ xRAG & **47.28** & **74.14** & **44.50** & 39.66 & **27.80** & **26.64** & 68.20 & **46.91** (9.7\%) & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Experimental results on six downstream tasks. The best results are in **bold** and the second best are with underscore. Percentage in the brackets denotes the relative improvement over non-retrieval setting. LLMs are frozen during the experiments and retrieved documents are set the same for different compression methods. \({}^{\ddagger}\) and \({}^{\dagger}\) denotes different compression ratio.

< 0.05), although the gains are more modest for the larger model configurations. This trend aligns with observations reported by [70; 51]. Further analysis on the efficacy of various compression techniques reveals that xRAG outperforms other approaches by a large margin. Remarkably, xRAG not only reduces the token count drastically--from 175.1 to a single token--but also maintains robust performance levels. In some instances, xRAG's performance is comparable to, or even exceeds, that of the uncompressed models. Specifically, in the Mistral-7b configuration, xRAG achieves nearly the same performance improvement as the uncompressed model (16.6% compared to 19.4%), and in the Mistral-8x7b configuration, it surpasses the uncompressed model (9.7% compared to 8.0%). One possible reason lies in the vulnerability of current RAG system when the irrelevant or misleading documents are presented, a topic detailed discussed in SS6.1. We also observe that xRAG performs well in tasks that require document understanding, such as TriviaQA. However, in tasks that demand reasoning over document, like HotpotQA and FactKG, xRAG lags behind by a considerable gap.

### Computational Efficiency

In this section, we conduct a thorough assessment of our framework's computational efficiency and memory management. To rigorously evaluate our model, we employed Torch Profiler3 to measure the CUDA Time (milliseconds) and Giga FLOPs of both the RAG and xRAG models across four real-world datasets. In these evaluations, the Mistral-7b, operating in bfloat16 inference mode, served as the base LLM. CUDA Time and GFLOPs were calculated on an average per batch basis with a fixed batch size, and GFLOPs were normalized by the number of generated tokens. These experiments were performed on the same computational hardware, specifically an Nvidia A100 and an AMD EPYC 7V12 64-Core Processor. As depicted in Table 2, despite variations in prompt and generation lengths across the datasets, xRAG significantly outpaced the RAG model, achieving a x1.64 increase in CUDA Time efficiency and a x3.53 reduction in GFLOPs.

Footnote 3: https://pytorch.org/docs/stable/profiler.html#module-torch.profiler

## 6 Analysis

### Evaluation Beyond the Overall Score

Although retrieval augmentation generally boosts performance as shown by aggregate metrics, it may not uniformly benefit all instances. In certain cases, the retrieval system might provide irrelevant or even misleading information, leading to incorrect answers that were previously correct [83; 76; 5]. To enable a more fine-grained evaluation, we introduce two novel metrics: the Resilience Rate and the Boost Rate. The resilience rate quantifies the percentage of instances in which the system's responses remain correct both before and after retrieval augmentation, highlighting the system's stability and robustness. Conversely, the boost rate measures the percentage of instances that were initially answered incorrectly but were rectified following the introduction of a retrieved document, thereby assessing the efficacy of retrieval augmentation. An ideal RAG system should have both high resilience rate and boost rate.

In Figure 4, we display these metrics for the uncompressed RAG and two compression methods: LLMlingua and xRAG. Surprisingly, although retrieval augmentation generally enhances performance, the resilience rate for RAG averages only 75.2%, indicating that retrieval can adversely affect about one-quarter of previously correct answers. In contrast, xRAG demonstrates considerable

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{**CUDA Time (ms)**} & \multicolumn{3}{c}{**GFLOPs**} \\  & **RAG** & **xRAG** & **Improvement** & **RAG** & **xRAG** & **Improvement** \\ \hline FactKG & 431.5 & 215.6 & x2.01 & 4683.8 & 1289.5 & x3.63 \\ NQ & 918.7 & 611.3 & x1.51 & 1338.6 & 384.0 & x3.48 \\ TriviaQA & 807.1 & 512.1 & x1.57 & 1667.2 & 492.3 & x3.38 \\ WebQA & 872.6 & 577.3 & x1.51 & 1405.1 & 386.8 & x3.63 \\ Average & & & **x1.64** & & & **x3.53** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of RAG and xRAG performance in CUDA Time and GFLOPS.

robustness across all evaluated datasets. This robustness largely stems from xRAG's ability to maintain an unbiased stance toward the internal knowledge representation of the LLM, especially when confronted with noisy retrieval content. Similar trends are noted in [51, 55], where search-augmented instruction learning is shown to bolster the robustness of language models. However, xRAG still lags behind RAG in boost rate, particularly in multi-hop reasoning tasks. It is crucial to note that a high resilience rate does not necessarily mean that the LLM disregards the provided information, which could potentially lead to a reduced boost rate. A comparative analysis with LLMlingua indicates that xRAG is not only more robust but also more effective.

### What makes xRAG effective?

This section delves into a thorough evaluation of various elements that contribute to xRAG's overall performance, focusing on its training strategy, the blend of datasets used and the effect of different embedding models. Due to the space limit, we present the last factor in Appendix E.

**1. Training Strategy** We carefully ablate four optimization choices: pretraining, instruction tuning, and two optimization objectives--language modeling (nll) and self-distillation (self-kd). We also train a Mistral-7b with LoRA on our instruction tuning dataset to rule out the possibility that our improvement simply comes from tuning on more data. The outcomes are presented in Table 3. Our analysis reveals that the interplay of different training strategies significantly contributes to the

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & **NQ** & **TriviaQA** & **WebQA** & **HotpotQA** & \multicolumn{3}{c}{**Averaged**} \\  & & & & & **Performance** & Resilience & **Boost** \\ \hline
**Mistral-7b** & & & & & & & & \\ xRAG & 39.10 & 65.77 & 39.40 & 34.05 & **44.58** & 82.3\% & 22.2\% \\ w/o finetune & 30.14 & 59.48 & 35.19 & 26.70 & 37.87 & 66.6\% & 20.8\% \\ w/o pretrain & 31.25 & 59.07 & 41.19 & 24.32 & 38.95 & 79.8\% & 14.1\% \\ w/o nll & 35.46 & 65.27 & 39.57 & 31.80 & 43.02 & 83.7\% & 19.4\% \\ w/o self-kd & 34.99 & 64.33 & 39.22 & 27.45 & 41.49 & 76.2\% & 20.8\% \\ w LoRA & 35.71 & 60.14 & 40.45 & 22.91 & 39.80 & 76.0\% & 18.0\% \\ \hline
**Mistral-8x7b** & & & & & & & \\ xRAG & 47.48 & 74.14 & 44.50 & 39.66 & **51.45** & 84.9\% & 20.0\% \\ w/o finetune & 34.46 & 64.08 & 34.89 & 30.43 & 40.96 & 65.9\% & 17.8\% \\ w/o pretrain & 42.54 & 71.17 & 47.44 & 31.23 & 48.09 & 85.0\% & 14.2\% \\ w/o nll & 45.10 & 72.85 & 45.03 & 37.11 & 50.02 & 84.8\% & 18.9\% \\ w/o self-kd & 42.38 & 72.26 & 44.73 & 32.41 & 47.94 & 79.8\% & 18.9\% \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation on different training strategy for xRAG.

Figure 4: Resilience rate and boost rate of three augmentation methods: LLMLinuga, xRAG and RAG over a Mistral-8x7b baseline without retrieval augmentation.

performance of our framework. In the case of Mistral-7b, pretraining and finetuning phases are of equal significance to the end results. However, for Mistral-8x7b, the impact of pretraining is notably diminished, likely due to the larger model's enhanced capability to incorporate multi-modality information. Furthermore, we find that during finetuning, self-distillation is more important than language modeling. The primary advantage of self-distillation lies in bolstering the resilience rate of the xRAG system. Optimization with hill loss tends to cause an overreliance on context information, rendering the system more vulnerable when the retriever fails to fetch relevant documents.

**II. Instruction-tuning Dataset Blend** As discussed in SS3.3, our instruction-tuning dataset primarily comprises three categories: reading comprehension, open-domain QA, and text summarization. To explore the effects of different data blends, we instruction-tune three xRAG model variants, each using data from these distinct categories. The results are shown in Table4. Our analysis reveals that among the dataset blends, reading comprehension data most significantly enhances the xRAG model's performance, as evidenced by both high resilience and boost rates. Intriguingly, when tuned solely with summarization data, xRAG still manages to deliver strong performance on QA datasets it has never been exposed to. This finding underscores that the advantages of instruction tuning for xRAG are not rooted in task-specific knowledge. Instead, they derive from the model's improved ability to utilize projected context information effectively.

### Case Study

In Figure 5, we show one interesting case about the robustness of xRAG. When retrieval system provide misleading content, standard RAG would overly rely on the document and generate answer that are faithful to the document while not factually true. Our xRAG model opt to rely on the internal

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & **\# Train** & **NQ** & **TriviaQA** & **WebQA** & **HotpotQA** & \multicolumn{3}{c}{
\begin{tabular}{c} **Average** \\ **Performance** \\ \end{tabular} } \\ \hline
**xRAG (Mistral-7b)** & 955k & 39.10 & 65.77 & 39.40 & 34.05 & **44.58** & 82.3\% & 22.2\% \\ w RC only & 488k & 36.98 & 65.77 & 41.39 & 32.82 & 44.24 & 81.9\% & 22.4\% \\ w QA only & 385k & 36.45 & 65.57 & 41.14 & 31.80 & 43.74 & 80.5\% & 22.1\% \\ w Summ only & 81k & 36.37 & 64.95 & 40.40 & 31.98 & 43.42 & 78.8\% & **22.8\%** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation results on different data selection strategy.

Figure 5: Given the misleading document, RAG model tend to generate a wrong answer based on the document, while xRAG demonstrate its robustness by leveraging the internal knowledge of LLM.

knowledge of LLM and being robust to the misleading content. In Appendix H, we include more cases about xRAG including several error analysis.

## 7 Conclusion

In this work, we present xRAG, an innovative context compression method tailored for retrieval-augmented generation. For knowledge-intensive tasks, xRAG can be significantly faster than RAG while maintaining comparable performance. We are excited about the future of this modality-based retrieval-augmented system and plan to further improve its performance in the areas of reasoning over embedding, handling multiple documents, and combining with multi-vector retrieval.

## 8 Acknowledgement

We would like to express our sincere gratitude to the anonymous reviewers for their thorough review, insightful comments, and constructive suggestions, which have significantly improved the quality of this manuscript. This work paper is supported (in part) by the State Key Laboratory of General Artificial Intelligence.

## References

* [1]S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J. Lespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang, L. Maggiore, C. Jones, A. Cassier, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. W. Rae, E. Elsen, and L. Sifre (2022) Improving language models by retrieving from trillions of tokens. External Links: 2204.03016 Cited by: SS1.
* [2]S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J. Lespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang, L. Maggiore, C. Jones, A. Cassier, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. W. Rae, E. Elsen, and L. Sifre (2022) Improving language models by retrieving from trillions of tokens. External Links: 2204.03016 Cited by: SS1.
* [3]A. Asai, Z. Zhong, D. Chen, P. Koh, L. Zettlemoyer, H. Hajishirzi, and W. v. h. (2024) Reliable, adaptable, and attributable language models with retrieval. External Links: 2204.03016 Cited by: SS1.
* [4]A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi (2023) Self-rag: learning to retrieve, generate, and critique through self-reflection. External Links: 2303.00166 Cited by: SS1.
* [5]A. Asai, Z. Zhong, D. Chen, P. Koh, L. Zettlemoyer, H. Hajishirzi, and W. v. h. (2024) Reliable, adaptable, and attributable language models with retrieval. External Links: 2204.03016 Cited by: SS1.
* [6]P. Bajaj, D. Campos, N. Craswell, L. Deng, J. Gao, X. Liu, R. Majumder, A. McNamara, B. Mitra, T. Nguyen, M. Rosenberg, X. Song, A. Stoica, S. Tiwary, and T. Wang (2018) Ms marco: a human generated machine reading comprehension dataset. External Links: 1803.00166 Cited by: SS1.
* [7]D. Beauchemin, Z. Gagnon, and R. Khoury (2024) Quebec automobile insurance question-answering with retrieval-augmented generation. arXiv preprint arXiv:2410.09623. Cited by: SS1.
* [8]J. Berant, A. Chou, R. Frostig, and P. Liang (2013) Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1533-1544. Cited by: SS1.

[MISSING_PAGE_POST]

* [11] Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. Dialogsum: A real-life scenario dialogue summarization dataset, 2021.
* [12] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation with contrastive translation memories, 2022.
* [13] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. Decouple knowledge from parameters for plug-and-play language modeling, 2023.
* [14] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself up: Retrieval-augmented text generation with self memory, 2023.
* [15] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts, 2023.
* [16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
* [17] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019.
* [18] Jiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen, Can Xu, Guodong Long, Dongyan Zhao, and Daxin Jiang. Synergistic interplay between search and large language models for information retrieval, 2023.
* [19] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey, 2024.
* [20] Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model, 2023.
* [21] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. In _Proceedings of the 2nd Workshop on New Frontiers in Summarization_. Association for Computational Linguistics, 2019.
* [22] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training, 2020.
* [23] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Christina Jayne. Imitation learning: A survey of learning methods. _ACM Computing Surveys (CSUR)_, 50(2):1-35, 2017.
* [24] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. Camels in a changing climate: Enhancing lm adaptation with tubl 2, 2023.
* [25] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering, 2021.
* [26] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models, 2022.
* [27] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023.
* [28] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral of experts, 2024.

* [29] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LImlingua: Compressing prompts for accelerated inference of large language models, 2023.
* [30] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression, 2023.
* [31] Kelvin Jiang, Dekun Wu, and Hui Jiang. FreebaseQA: A new factoid QA data set matching trivia-style question-answer pairs with Freebase. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 318-323, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
* [32] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A dataset for biomedical research question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2567-2577, Hong Kong, China, November 2019. Association for Computational Linguistics.
* [33] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, 2017.
* [34] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models, 2024.
* [35] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen taui Yih. Dense passage retrieval for open-domain question answering, 2020.
* [36] Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest neighbor machine translation, 2021.
* [37] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models, 2020.
* [38] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp, 2023.
* [39] Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, and Edward Choi. Factkg: Fact verification via reasoning on knowledge graphs. _arXiv preprint arXiv:2305.06590_, 2023.
* [40] Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge, 2017.
* [41] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7, 2019.
* [42] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen taui Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.
* [43] Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, Yuxian Gu, Xin Cheng, Xun Wang, Si-Qing Chen, Li Dong, Wei Lu, Zhifang Sui, Benyou Wang, Wai Lam, and Furu Wei. Synthetic data (almost) from scratch: Generalized instruction tuning for language models, 2024.

* [44] Haoran Li, Mingshi Xu, and Yangqiu Song. Sentence embedding leaks more information than you expect: Generative embedding inversion attack to recover the whole sentence, 2023.
* [45] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.
* [46] Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 6342-6353, Singapore, December 2023. Association for Computational Linguistics.
* [47] Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier. Prompt compression for large language models: A survey, 2024.
* [48] Zongqian Li, Yixuan Su, and Nigel Collier. 500xcompressor: Generalized prompt compression for large language models, 2024.
* [49] Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen tau Yih, and Xilun Chen. How to train your dragon: Diverse augmentation towards generalizable dense retrieval, 2023.
* [50] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2022.
* [51] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-augmented dual instruction tuning, 2023.
* [52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2024.
* [53] Qi Liu, Bo Wang, Nan Wang, and Jiaxin Mao. Leveraging passage embeddings for efficient listwise reranking with large language models, 2024.
* [54] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024.
* [55] Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, and James Glass. Sail: Search-augmented instruction learning, 2023.
* [56] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empirical study of catastrophic forgetting in large language models during continual fine-tuning, 2024.
* [57] Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. Nonparametric masked language modeling, 2023.
* [58] John X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander M. Rush. Text embeddings reveal (almost) as much as text, 2023.
* [59] Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens, 2024.
* [60] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark, 2023.
* [61] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In _International conference on machine learning_, pages 3878-3887. PMLR, 2018.
* [62] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression, 2024.

* [63] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktaschel, and Sebastian Riedel. Kilt: a benchmark for knowledge intensive language tasks, 2021.
* [64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.
* [65] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for squad, 2018.
* [66] Siva Reddy, Danqi Chen, and Christopher D. Manning. Coqa: A conversational question answering challenge, 2019.
* [67] Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. Getting closer to ai complete question answering: A set of prerequisite real tasks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 8722-8731, 2020.
* [68] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: Effective and efficient retrieval via lightweight late interaction, 2022.
* [69] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks, 2017.
* [70] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. Replug: Retrieval-augmented black-box language models, 2023.
* [71] Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context, 2022.
* [72] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge, 2019.
* [73] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2024.
* [74] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective method by retrieving from training data, 2022.
* [75] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources, 2023.
* [76] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. Learning to filter context for retrieval-augmented generation, 2023.
* [77] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew W. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022.
* [78] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources to advance general chinese embedding, 2023.
* [79] Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023.
* [80] Yi Yang, Wen-tau Yih, and Christopher Meek. WikiQA: A challenge dataset for open-domain question answering. In Lluis Marquez, Chris Callison-Burch, and Jian Su, editors, _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 2013-2018, Lisbon, Portugal, September 2015. Association for Computational Linguistics.

* [81] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.
* [82] Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. Compact: Compressing retrieved documents actively for question answering, 2024.
* [83] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust to irrelevant context, 2023.
* [84] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators, 2023.
* [85] Jungmin Yun, Mihyeon Kim, and Youngbin Kim. Focus on the core: Efficient attention via pruned token compression for document classification. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 13617-13628, Singapore, December 2023. Association for Computational Linguistics.
* [86] Xinping Zhao, Yan Zhong, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Dongfang Li, Baotian Hu, and Min Zhang. Funnelrag: A coarse-to-fine progressive retrieval paradigm for rag. _arXiv preprint arXiv:2410.10293_, 2024.
* [87] Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation, 2022.

Comparison between different Context Compression Models

In Table 5, we present a detailed comparison of various context compression models, emphasizing their real-world applicability. This comparison focuses on two key aspects: (1) Plug-and-play capability, which assesses whether dataset-specific tuning is necessary for new, unseen data; (2) Memory efficiency, which evaluates if additional memory space is required to store the compressed information, such as high-dimensional vectors typically used in soft prompting methods.

## Appendix B Instructions for Paraphrase Pretraining

The list of instructions for Paraphrase Pretraining is shown in Table 6. They present the same meaning with natural language variance.

## Appendix C

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Model** & \begin{tabular}{l} **Specifically** \\ **designed for RAG** \\ \end{tabular} & \begin{tabular}{l} **Maximum** \\ **Compression Rate** \\ \end{tabular} & \begin{tabular}{l} **Approach** \\ **Plug-and-Play** \\ \end{tabular} & 
\begin{tabular}{l} **Memory Efficient** \\ **Memory Efficient** \\ \end{tabular} \\ \hline AutoCompressor [15] & âœ— & x15 & Soft Prompting & âœ— & âœ— \\ Gist [59] & âœ— & x26 & Soft Prompting & âœ— & âœ— \\ ICAE [20] & âœ— & x8 & Soft Prompting & âœ“ & âœ— \\ LLMLingua [29] & âœ— & x20 & Prompt Editing & âœ“ & âœ“ \\ Selective Context [46] & âœ— & x5 & Prompt Editing & âœ“ & âœ“ \\ Token Elimination [85] & âœ“ & x10 & Attention Filtering & âœ“ & âœ“ \\ FiCo [74] & âœ“ & x2 & Prompt Editing & âœ— & âœ“ \\ RECOMP [79] & âœ“ & x16.6 & Prompt Editing & âœ“ & âœ“ \\ xMAG & âœ“ & x178 & Modality Fusion & âœ“ & âœ“ \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison between different compression methods from their setting to design principle.

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_EMPTY:20]

Limitations

We discuss the limitations of our framework as follows:

* In this work, we only consider the most commonly used retrieval system--single dense vector retrieval, while sparse retrieval methods such as BM25 or multi-vector retrieval methods like ColBERT are not included. We believe that combining these methods would be a promising direction for xRAG, as sparse vectors could complement dense vectors, and multi-vector retrieval would provide xRAG with more flexibility by not condensing all information into one token.
* Currently, xRAG delivers decent performance when a relevant document is fetched; however, it lags behind RAG by a considerable margin in tasks that require reasoning (such as HotpotQA and FactKG). One possible reason is that during the training phase of xRAG, reasoning-relevant data is not provided. How to make xRAG a better reasoner remains our future work.
* We only consider the _Top-1_ retrieval setting, while ensembling multiple relevant documents has been shown to be effective for RAG systems due to the complementary information contained in _Top-K_ documents. We believe there is potential advantage for xRAG to scale to multi-document settings, as the input length of xRAG for multi-documents scales by a factor of 1, while for RAG, it scales by the document length factor.

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_FAIL:23]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Abstract and Section 1 Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section 5.1 and Appendix G Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.

Figure 10: xRAG correctly locates the relevant part in a long document while RAG would still hallucinate the wrong answer.

* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: the paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: in Appendix D we provide full details about our training process Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: all the models and data we use are publicly available and we carefully cite each paper Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Section 4.1 Guidelines:* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: in Section 5.1 Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: in Appendix D Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: reviewed and confirmed Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: in section 5.2 Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: all the data and model we use is publicly available Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets**Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer:[NA] Justification: in Section C Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: n/a Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: n/a Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: n/a

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.