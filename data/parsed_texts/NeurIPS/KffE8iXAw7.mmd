# Towards Optimal Effective Resistance Estimation

 Rajat Vadiraj Dwaraknath

Stanford University

rajatvd@stanford.edu &Ishani Karmarkar

Stanford University

ishanik@stanford.edu &Aaron Sidford

Stanford University

sidford@stanford.edu

###### Abstract

We provide new algorithms and conditional hardness for the problem of estimating effective resistances in \(n\)-node \(m\)-edge undirected, expander graphs. We provide an \(\widetilde{O}(m\epsilon^{-1})\)-time algorithm that produces with high probability, an \(\widetilde{O}(n\epsilon^{-1})\)-bit sketch from which the effective resistance between any pair of nodes can be estimated, to \((1\pm\epsilon)\)-multiplicative accuracy, in \(\widetilde{O}(1)\)-time. Consequently, we obtain an \(\widetilde{O}(m\epsilon^{-1})\)-time algorithm for estimating the effective resistance of all edges in such graphs, improving (for sparse graphs) on the previous fastest runtimes of \(\widetilde{O}(m\epsilon^{-3/2})\)[Chu et. al. 2018] and \(\widetilde{O}(n^{2}\epsilon^{-1})\)[Jambulapati, Sidford 2018] for general graphs and \(\widetilde{O}(m+n\epsilon^{-2})\) for expanders [Li, Sachdeva 2022]. We complement this result by showing a conditional lower bound that a broad set of algorithms for computing such estimates of the effective resistances between all pairs of nodes require \(\widetilde{\Omega}(n^{2}\epsilon^{-1/2})\)-time, improving upon the previous best such lower bound of \(\widetilde{\Omega}(n^{2}\epsilon^{-1/13})\)[Musco et. al. 2017]. Further, we leverage the tools underlying these results to obtain improved algorithms and conditional hardness for more general problems of sketching the pseudoinverse of positive semidefinite matrices and estimating functions of their eigenvalues.

## 1 Introduction

In a weighted, undirected graph \(G\) the _effective resistance_ between a pair of vertices \(a\) and \(b\), denoted \(r_{G}(a,b)\), is defined as the energy of a unit of electric current sent from \(a\) to \(b\) in the natural resistor network induced by \(G\). Effective resistances arise for a broad set of graph processing tasks and have multiple equivalent definitions. For example, \(r_{G}(a,b)\) is proportional to the expected roundtrip commute time between \(a\) and \(b\) in the natural random walk induced on the graph and when \(\{a,b\}\) is an edge in the graph, it is proportional to the probability that the edge is in a random spanning tree.

Effective resistances also form a particular class of metrics on the vertices [1, 2] and are a key measure of proximity between vertex pairs. Correspondingly, effective resistances can arise in a variety of data analysis tasks. For example, effective resistances have been used in social network analysis for measuring edge centrality in social networks [3] as well as for measuring chemical distances [4].

Effective resistances have a broad range of algorithmic implications. Sampling edges of a graph using effective resistance is known to efficiently produce cut and spectral sparsifiers (sparse graphs which approximately preserve cuts, random walk properties, and more) [5, 6, 7]. Effective resistance-based graph sparsifiers have also been applied to develop fast graph attention neural networks [8], to design graph convolutional neural networks for action recognition [9], to sample from Gaussian graphical models [10], and beyond [11, 12]. Effective resistances have also been used in algorithms for maximum flow problems, [13, 14, 15, 16, 16, 17, 18], sampling random spanning trees [19, 20, 21], and graph partitioning [22, 23]. More recently, effective resistances have also been used to analyze the problem of oversquashing in GNNs and in designing algorithms to alleviate oversquashing [24, 25, 26] and have been applied to increase expressivity when incorporated as edge features into certain GNNs [27].

Algorithms.Given the broad utility of effective resistances, there have been many methods for estimating and approximately compressing them [28, 29, 30, 31, 5]. In this paper, our main focus is the following effective resistance estimation problem. (We use \(x\approx_{\epsilon}y\) as shorthand for \((1-\epsilon)y\leq x\leq(1+\epsilon)y\) and assume all edge weights in graphs are positive. See Section 2 for notation more broadly).

**Definition 1** (Effective Resistance Estimation Problem).: _In the effective resistance estimation problem we are given an undirected, weighted graph \(G=(V,E,w)\), a set of vertex pairs \(S\subseteq V\times V\), and \(\epsilon\in(0,1)\) and must output \(\tilde{r}\in\mathbb{R}^{S}\) such that with high probability (whp.), \(\tilde{r}_{(a,b)}\approx_{\epsilon}r_{G}(a,b)\) for all \((a,b)\in S\)._

The state-of-the-art runtimes for solving the effective resistance estimation problem on \(n\)-node, \(m\)-edge graphs are given in Table 2. To contextualize these results, consider the special case of estimating the effective resistance of a graph's edges, i.e., when \(S=E\). This special case appears in many of the aforementioned applications, e.g., [13, 14, 15, 19, 20]. The state-of-the-art runtimes for this problem are \(\widetilde{O}(n^{2}\epsilon^{-1})\) due to [28] and \(\widetilde{O}(m\epsilon^{-1.5})\) due to [30]. A major open problem is whether improved runtimes, e.g., \(\widetilde{O}(m\epsilon^{-1})\) (which would subsume prior work), are attainable.

One of the main results of this paper is resolving this open problem in the case of well-connected graphs, i.e., expanders. Expanders are a non-trivial, previously studied, special case that is often the first step or a key component for developing more general algorithms [32]. In particular we provide an \(\widetilde{O}(m\epsilon^{-1}\bar{\kappa}\left(G\right))\) time algorithm where \(\bar{\kappa}\) is a measure of the graph's connectivity. Previously, the only non-trivial improvement in this setting was an independently obtained runtime of \(\widetilde{O}(m+n\epsilon^{-2}(\bar{\kappa}\left(G\right))^{3/2})\) for graphs with \(\bar{\kappa}\left(G\right)=\widetilde{O}(1)\)[29]. We improve upon the result of [29] for sparse enough graphs and, as we explain in Section 1.1, we can almost match the result of [29] (up to an \(m^{o(1)}\) factor) in dense graphs. 1

Footnote 1: While our algorithms for the effective resistance estimation problem (Definition 1) were obtained independently, our writing and discussion of effective resistance sketch algorithms (Definition 1) was informed by their paper. We provide a more complete comparison of our work with prior results in Table 1.

Interestingly, we obtain our main result by providing new effective resistance sketch algorithms.

**Definition 2** (Effective Resistance Sketch).: _We call a randomized algorithm an \((\mathcal{T}_{\text{s}},\mathcal{T}_{\text{q}},s)\)-effective resistance sketch algorithm if given an input \(n\)-node, \(m\)-edge undirected, weighted graph \(G=(V,E,w)\) and \(\epsilon\in(0,1)\) in time \(O(\mathcal{T}_{\text{s}}(G,\epsilon))\) it creates a binary string of length \(O(s(G,\epsilon))\) from which when queried with any \(a,b\in V\), it outputs \(\tilde{r}_{a,b}\approx_{\epsilon}r_{G}(a,b)\) whp. in time \(O(\mathcal{T}_{\text{q}}(G,\epsilon))\)._

Effective resistance sketching algorithms immediately imply algorithms for the effective resistance estimation problem. We obtain our result by obtaining an \((\widetilde{O}(m\epsilon^{-1}),\widetilde{O}(1),\widetilde{O}(n\epsilon^{-1}))\)-effective resistance sketch algorithm for expanders (see Section 1.1 for a comparison to prior work).

Lower Bounds.Given the central role of effective resistance estimation and the challenging open-problem of determining its complexity, previous work has sought complexity theoretic lower bounds for the problem. [33] showed a conditional lower bound of \(\Omega(n^{2}\epsilon^{-1/13})\) for the problem by showing that an algorithm that computes effective resistances in \((n^{2}\epsilon^{-1/13+\delta})\) for some \(\delta>0\) time could be used to obtain a subcubic algorithm for the _triangle detection problem_, that we define below.

**Definition 3** (Triangle Detection Problem).: _Given an \(n\)-node undirected unweighted graph \(G=(V,E)\), determine whether there are distinct \(a,b,c\in V\) with \(\{a,b\}\in E\), \(\{b,c\}\in E\) and \(\{c,a\}\in E\)._

Currently, the only known subcubic algorithms for the triangle detection problem leverage fast matrix multiplication (FMM) and therefore their practical utility (in the worst case) is questionable.

**Theorem 1** (Informal, [34]).: _Given an algorithm which solves the triangle detection problem in subcubic time, we can produce a subcubic algorithm, which only performs combinatorial operations and uses the triangle detection algorithm, for Boolean matrix multiplication (BMM) and additional problems which currently are not known to be solvable subcubicly without FMM._

This theorem implies that any subcubic algorithm for triangle detection that doesn't use FMM implies a subcubic BMM algorithm that doesn't use FMM. Consequently, subcubic triangle detection is a common hardness assumption used to illustrate barriers towards improving non-FMM based methods, e.g., the effective resistance estimation algorithms of this paper.

In this paper we take a key step towards closing the gap between the best known running times for effective resistance estimation and lower bounds by improving the conditional lower bound of \(\Omega(n^{2}\epsilon^{-1/13})\) to \(\widetilde{\Omega}(n^{2}\epsilon^{-1/2})\) for randomized algorithms. We show this lower bound holds _even for expander graphs_, and hence our effective resistance estimation algorithm (as well as [28]) are optimal up to an \(\epsilon^{-1/2}\)-factor among non-FMM based algorithms, barring a major breakthrough in BMM.

Broader Linear Algebraic Tools.The effective resistance between vertex \(a\) and vertex \(b\) in a graph \(G\) has a natural linear algebraic formulation. For all \(a,b\in V\) it is known that \(r_{G}(a,b)=\bar{\delta}_{a,b}\mathbbm{E}_{G}^{\top}\bar{\delta}_{a,b}\), where \(\mathbbm{L}_{G}\in\mathbb{R}^{V\times V}\) is a natural matrix associated with \(G\) known as the _Laplacian matrix_ and \(\bar{\delta}_{a,b}=e_{a}-e_{b}\) (see Section 2 for notation). Thus, sketching effective resistances can be viewed as problems of preserving information about subsets of entries of the pseudoinverse of a Laplacian.

Both our algorithms and lower bounds develop more general tools for handling related problems for more general (not-necessarily Laplacian) matrices. On the algorithmic side, we show our techniques can also lead to algorithms and data structures for computing certain quadratic forms involving well-conditioned SDD and PSD matrices. On the hardness side, we show our techniques also improve triangle detection hardness bounds for estimating various properties of the singular values of a matrix.

Paper Organization.In the remainder of the introduction we provide a more precise statement and comparison of our results in Section 1.1. In the remainder of the paper we cover preliminaries in Section 2, present upper bounds in Section 3, and present lower bounds in Section 4. Omitted proofs and additional discussion of related proofs are deferred to the supplementary material.

### Our Results

Algorithms.Here we outline our main algorithmic results pertaining to effective resistance sketching and estimation, and in Section 3 we describe a extensions of our work to broader linear algebraic problems involving SDD and PSD matrices. Our main algorithmic result is a new efficient effective resistance sketch for _expanders_, a term which is used to refer to graphs with \(\widetilde{\Omega}(1)\)-expansion.

**Definition 4** (Expander).: _For \(\alpha>0\), we say that a graph \(G=(V,E,w)\) has \(\alpha\)-expansion if \(\alpha\leqslant\phi(G)\), where \(\phi(G)\) denotes the conductance of \(G\) and is defined as_

\[\phi(G):=\min_{S\subseteq V,S\notin\{0,V\}}\frac{\sum_{\{u,v\}:w\in S,v\in V \setminus S}w_{u,v}}{\min\left(\sum_{u\in S}d_{u},\sum_{v\in V\setminus S}d_{ u}\right)},\text{ where }d_{u}:=\sum_{\{u,v\}\in E}w_{u}.\]

**Theorem 2**.: _There is an \((\widetilde{\mathcal{O}}(m\epsilon^{-1}),\widetilde{\mathcal{O}}(1),\widetilde {\mathcal{O}}(n\epsilon^{-1}))\)-effective resistance sketch algorithm for graphs with \(\widetilde{\Omega}(1)\)-expansion._

Table 1 summarizes and compares our Theorem 2 to previous work on effective resistance sketches, including naive algorithms to explicitly compute the pseudoinverse of the Laplacian of \(G\), which can be computed in \(O(n^{\omega})\) time using FMM or \(\widetilde{\mathcal{O}}(mn)\) time using a Laplacian system solver (labeled Solver). 2 A \((\mathcal{T}_{\text{s}},\mathcal{T}_{\text{q}},s)\) effective-resistance sketch algorithm implies an \(O(\mathcal{T}_{\text{s}}+|S|\mathcal{T}_{\text{q}})\) algorithm for the effective resistance estimation problem. Hence, Theorem 2 implies the following.

Footnote 2: \(\omega\leqslant n^{2.37188}\)[35] denotes the fast matrix multiplication constant.

**Theorem 3** (Effective Resistance Estimation on Expanders).: _There is an \(\widetilde{\mathcal{O}}(m\epsilon^{-1}+|S|)\) time algorithm which solves the effective resistance estimation problem for graphs with \(\widetilde{\Omega}(1)\)-expansion._

Effective resistance sketches are a common approach to solving the effective resistance estimation problem; but there are also approaches to the problem that do not explicitly construct effective resistance sketches. Table 2 summarizes prior work on effective resistance estimation more broadly.

There has been a long line of research on the problem of computing sketches and sparsifiers of graph Laplacians [5, 28, 30, 36] (i.e., computing a sparse graph \(G^{\prime}\) such that quadratic forms in the Laplacian of \(G^{\prime}\) approximately preserves quadratic forms in the Laplacian of \(G\)). Building on this work, [30] showed there is an algorithm which processes a graph \(G\) on \(n\) nodes and \(m\) edges in \(O(m^{1+o(1)})\) time and produces a sparse sketch graph \(H\) with only \(\widetilde{\mathcal{O}}(n\epsilon^{-1})\) edges such that \(r_{G}(a,b)\approx r_{H}(a,b)\)for all \(a,b\in V\). Consequently, any algorithm which runs in \(\widetilde{O}(m\epsilon^{-c})\) on expanders can be improved to run in \(\widetilde{O}(m^{1+o(1)}+n\epsilon^{-(c+1)})\) on expanders simply by running the algorithm on \(H\) instead of \(G\).

Summary of prior work on Effective Resistance Sketch (Table 1) and Effective Resistance Estimation (Table 2) algorithms on \(n\)-node, \(m\)-edge expanders. All time and space complexities are reported up to to \(\widetilde{O}(\cdot)\). The methods of This Paper and [29] apply only to expanders; however, the remaining works apply to general graphs. As discussed, when \(m^{1+o(1)}+n\epsilon^{-(c+1)}=o(m\epsilon^{-c})\), any runtime dependence on \(m\epsilon^{-c}\) in the table can be improved to a dependence on \(m^{o(1)+1}+n\epsilon^{-(c+1)}\).

Lower BoundsFor the effective resistance estimation problem, [33] showed that any combinatorial algorithm (i.e., an algorithm that uses only combinatorial operations in the sense of Theorem 1) which solves the effective resistance estimation problem for \(S=V\times V\) in \(\widetilde{O}(n^{2}\epsilon^{-1/13+\delta})\) for some \(\delta>0\), would imply a combinatorial subcubic deterministic algorithm which detects a triangle in an \(n\)-node undirected unweighted graph. We improve on their result, as follows.

**Theorem 4**.: _Given a combinatorial algorithm which solves the effective resistance estimation problem for \(S=V\times V\) on graphs with \(\widetilde{\Omega}(1)\)-expansion in \(\widetilde{O}(n^{2}\epsilon^{-1/2+\delta})\) time for \(\delta>0\), we can produce a randomized combinatorial algorithm which solves the triangle detection problem on an \(n\)-node graph in \(\widetilde{O}(n^{3-2\delta})\) time whp._

Theorem 4 implies an \(\widetilde{\Omega}(n^{2}\epsilon^{-1/2})\) randomized conditional lower bound for the problem of estimating effective resistances of all pairs of nodes in an undirected unweighted expander graph, while [33] shows only an \(\Omega(n^{2}\epsilon^{-1/13})\) lower bound. As we show in Section 4, by a simple reduction we can extend any \(\widetilde{\Omega}(n^{2}\epsilon^{-c})\) lower bound for the all-pairs effective resistance problem to a \(\widetilde{\Omega}(m\epsilon^{-c})\) lower bound for the all-edges effective resistance problem. Consequently, our result also yields a \(\widetilde{\Omega}(m\epsilon^{-1/2})\) randomized lower bound for the problem of estimating effective resistances of all edges in an undirected expander graph.

In addition to conditional lower bounds for effective resistance estimation, we also improve on existing conditional lower bounds for the problem of estimating spectral sums that we define below.

**Definition 5** (Spectral Sum).: _For \(f:\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}\) and \(\mathbf{A}\in\mathbb{R}^{n\times n}\) with singular values \(\sigma_{1}(\mathbf{A})\leqslant\sigma_{2}(\mathbf{A})\leqslant\cdots\leqslant \sigma_{n}(\mathbf{A})\), we define the spectral sum \(\mathcal{S}_{f}:\mathbb{R}^{n\times n}\rightarrow\mathbb{R}^{+}\) as \(\mathcal{S}_{f}(\mathbf{A}):=\sum_{i=1}^{n}f(\sigma_{i}(\mathbf{A}))\)._

[33] showed that for several spectral sums \(\mathcal{S}_{f}\), any combinatorial algorithm that outputs \(Y\approx_{\epsilon}\mathcal{S}_{f}(\mathbf{A})\) in \((n^{\gamma}\epsilon^{-c})\) time on an \(n\times n\) PSD matrix would imply an \(O(n^{\gamma+\alpha c})\) time combinatorial algorithm which solves the triangle detection problem, where the scaling \(\alpha\) varies depending on the specific \(\mathcal{S}_{f}\) (see Table 3). We build on their results to show improved randomized conditional lower bounds for several spectral sum estimation problems, as presented in Theorem 5 below.

**Theorem 5**.: _Given a combinatorial algorithm which on input \(\mathbf{B}\in\mathbb{R}^{n\times n}\) outputs a spectral sum estimate \(Y\approx_{\epsilon}\mathcal{S}_{f}(\mathbf{B})\) in \(O\left(n^{\gamma}\epsilon^{-c}\right)\) time with \(\gamma\geqslant 2\) for the spectral sums in Table 3, we can produce a randomized combinatorial algorithm that can detect a triangle in an \(n\)-node graph whp. in \(\widetilde{O}\left(n^{\gamma+\alpha c}\right)\) time, where \(\alpha\) is a scaling that depends on properties of the function \(f\) (see Table 3 for values of \(\alpha\) for several spectral sums.)_

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Citation & \(\mathcal{T}_{p}\) & \(\mathcal{T}_{q}\) & \(s\) & Citation & Runtime & \(S\) Restrictions \\ \hline FMM & \(n^{\omega}\) & \(1\) & \(n^{2}\) & FMM & \(n^{\omega}+|S|\) & None \\ Solver & \(nm\) & \(1\) & \(n^{2}\) & Solver & \(nm+|S|\) & None \\
[5] & \(m\epsilon^{-2}\) & \(\epsilon^{-2}\) & \(n\epsilon^{-2}\) & [5] & \(n^{2}\epsilon^{-2}\) & \(S=V\times V\) \\
[28] & \(n^{2}\epsilon^{-1}\) & \(1\) & \(n\epsilon^{-1}\) & [28] & \(n^{2}\epsilon^{-1}\) & \(S=V\times V\) \\
[29] & \(m+n\epsilon^{-2}\) & \(1\) & \(n\epsilon^{-1}\) & [19] & \(m+(n+|S|)\epsilon^{-2}\) & None \\
**This Paper** & \(m\epsilon^{-1}\) & \(1\) & \(n\epsilon^{-1}\) & [37] & \(m+|S|\,\epsilon^{-2}\) & None \\  & & & & [30] & \(m\epsilon^{-1.5}\) & \(S=E\) \\  & & & & [29] & \(m+n\epsilon^{-2}+|S|\) & None \\  & & & & **This Paper** & \(m\epsilon^{-1}+|S|\) & None \\ \hline \hline \end{tabular}
\end{table}
Table 1: Effective Resistance Sketch

## 2 Preliminaries

General notation.We use \(\mathbf{A}_{i,j}\) to denote the \((i,j)\)-th entry of \(\mathbf{A}\). For \(\mathbf{A}\in\mathbb{R}^{n\times n}\) we use \(\lambda\left(\mathbf{A}\right)\) for its spectrum, \(\lambda_{i}(\mathbf{A})\) and \(\sigma_{i}(\mathbf{A})\) for its \(i\)-th smallest eigenvalue and singular value respectively, and \(\rho(\mathbf{A}):=|\lambda_{n}(\mathbf{A})|\) for its spectral radius. \(\|\cdot\|_{p}\) denotes the \(\ell_{p}\)-norm. When \(\mathbf{A}\) is PSD, \(\lambda_{\min}(\mathbf{A})\) denotes its smallest nonzero eigenvalue and \(\kappa(\mathbf{A}):=\lambda_{n}(\mathbf{A})/\lambda_{\min}(\mathbf{A})\) denotes its pseudo-condition number. We use \(\langle\cdot,\cdot\rangle\) for the Euclidean inner product, \(\mathbbm{1}\) for the all ones vector, and \(e_{i}\) for the \(i\)-th standard basis vector. We define \(\vec{\delta}_{i,j}:=e_{i}-e_{j}\) and \([k]:=\{1,...,k\}\). We use \(x\approx_{\epsilon}y\) as shorthand for \((1-\epsilon)y\lessdot x\lessdot(1+\epsilon)y\). For \(v\in\mathbb{R}^{n}\), we use \(v[i:j]\) for the sub-vector from index \(i\) to \(j\). We use \(v\perp w\) to indicate that \(v,w\in\mathbb{R}^{n}\) are orthogonal (i.e., \(v^{\top}w=0\)).

Graphs.We use \(G=(V,E,w)\) to denote a weighted undirected graph on \(V\) with edges \(E\) and edge weights \(w\in\mathbb{R}_{>0}^{E}\) (or \(G=(V,E)\) if unweighted). We use \(\mathbf{A}_{G}\) to denote its (weighted) adjacency matrix \((\mathbf{A}_{G})_{u,v}=w_{u,v}\) for \(u,v\in V\times V\) and \(\mathbf{D}_{G}\) to denote its diagonal (weighted) degree matrix \((\mathbf{D}_{G})_{u}=\sum_{\{u,v\}\in E}w_{u,v}\) for \(u\in V\) (treated as \(w_{u,v}=1\) if \(G\) is unweighted). We define \(\mathbf{L}_{G}:=\mathbf{D}_{G}-\mathbf{A}_{G}\) as its graph Laplacian. \(d_{\max}(G)\) and \(d_{\min}(G)\) refer to the max and min diagonal entry in \(\mathbf{D}_{G}\). We may drop the argument or subscript \(G\) if it is clear from context. The effective resistance between nodes \(i,j\in V\) is denoted \(r_{G}(i,j)=\vec{\delta}_{i,j}^{\top}\mathbf{L}_{G}^{\dagger}\vec{\delta}_{i,j}\). We assume all input graphs are connected, as effective resistances can be computed separately on connected components. We use \(\mathbf{B}_{G}\) to denote the \(E\times V\) edge-incidence matrix of \(G\), where \((\mathbf{B}_{G})_{e,u}=1\) and \((\mathbf{B}_{G})_{e,v}=-1\) for all \(e\in E\), and \(e=\{u,v\}\), with \(\mathbf{B}_{Ge,l}=0\) for all \(l\neq u\) and \(l\neq v\).

Symmetric Diagonally Dominant (SDD) MatricesA matrix \(\mathbf{M}\in\mathbb{R}^{n\times n}\) is SDD if it can be decomposed as \(\mathbf{M}=\mathbf{D}_{\mathbf{M}}-\mathbf{A}_{\mathbf{M}}\), where the \(\mathbf{D}_{\mathbf{M}}\) is a diagonal matrix with non-negative entries and \(\mathbf{A}_{M}\) is a matrix with zeros on the diagonal such that \(d_{i,i}>\sum_{j=1}^{n}|a_{i,j}|\). We define the normalization of \(\mathbf{M}\) as \(\mathbf{N}_{M}:=\mathbf{D}_{M}^{-1/2}\mathbf{M}\mathbf{D}_{M}^{-1/2}\). Throughout this paper, we assume, without loss of generality that \(\mathbf{D}_{M}\) has strictly positive entries on the diagonal (otherwise, we can simply remove an entire row and column of zeros). We use \(d_{\max}(\mathbf{M})\) and \(d_{\min}(\mathbf{M})\) to denote the max and min entry in the diagonal of \(\mathbf{D}_{M}\) respectively. We may drop the argument or subscript \(\mathbf{M}\) if it is clear from context.

Spectral Graph Theory.Our results leverage well-known spectral graph theory results. In particular, our algorithm complexities are parameterized by the normalized pseudo condition number of a graph (or SDD matrix).

**Definition 6** (Normalized (pseudo-)condition number).: _We define the normalized (pseudo-)condition number of an SDD matrix \(\mathbf{M}\in\mathbb{R}^{n}\) as \(\bar{\kappa}\left(\mathbf{M}\right):=\lambda_{n}(\mathbf{N}_{M})/\lambda_{ \min}(\mathbf{N}_{M})\)._

To connect the normalized condition number to expander graphs, we can apply Cheeger's inequality, which guarantees that if \(G\) has \(\alpha\)-expansion for some \(\alpha=\tilde{\Omega}(1)\), then \(\bar{\kappa}\left(\mathbf{L}_{G}\right)=\lambda_{n}(\mathbf{N}_{L_{G}})/ \lambda_{2}(\mathbf{N}_{L_{G}})\leq 4/\alpha^{2}=\tilde{O}(1)\).

**Theorem 6** (Cheeger's Inequality [38]).: _Let \(G=(V,E,w)\) be an undirected graph. Then, \(\frac{1}{2}\lambda_{2}(\mathbf{N}_{L})\leq\phi(G)\leq\sqrt{2\lambda_{2}( \mathbf{N}_{L})}\)._

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & & [33] & \multicolumn{2}{c}{This Paper} \\ \cline{2-5} Spectral Sum & TD Runtime & Lower Bound & TD Runtime & Lower Bound \\ \hline Schatten 3-norm & \(n^{\gamma+4c}\) & \(n^{2}\epsilon^{-1/4}\) & \(n^{\gamma+5c/2}\) & \(n^{2}\epsilon^{-2/5}\) \\ Schatten p-norm, \(p\neq 1,2\), & \(n^{\gamma+13c}\) & \(n^{2}\epsilon^{-1/13}\) & \(n^{\gamma+10c}\) & \(n^{2}\epsilon^{-1/10}\) \\ SVD Entropy & \(n^{\gamma+6c}\) & \(n^{2}\epsilon^{-1/6}\) & \(n^{\gamma+5c}\) & \(n^{2}\epsilon^{-1/5}\) \\ Log Determinant & \(n^{\gamma+6c}\) & \(n^{2}\epsilon^{-1/6}\) & \(n^{\gamma+5c}\) & \(n^{2}\epsilon^{-1/5}\) \\ Trace of Exponential & \(n^{\gamma+13c}\) & \(n^{2}\epsilon^{-1/13}\) & \(n^{\gamma+10c}\) & \(n^{2}\epsilon^{-1/10}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Runtimes for the triangle detection (TD) problem in an \(n\)-node graph using algorithms that produce \((1\pm\epsilon)\) multiplicative approximations to various spectral sums in \(O(n^{\gamma}\epsilon^{-c})\) time. The second columns contain the best achievable runtimes for \(\gamma=2\) that do not use FMM, barring a breakthrough in subcubic triangle detection. All runtimes are reported up to \(\tilde{O}(\cdot)\).

In addition, we leverage the fact that effective resistances can be expressed in several equivalent expressions. In particular, it is known that:

\[r_{G}(i,j)=\vec{\delta}_{i,j}^{\top}\mathbf{L}_{G}^{\dagger}\vec{\delta}_{i,j}= \vec{\delta}_{i,j}D^{-1/2}\mathbf{N}_{G}{}^{\top}D^{-1/2}\vec{\delta}_{i,j}=( \mathbf{W}_{G}^{1/2}\mathbf{B}_{G}\mathbf{L}_{G}^{\dagger}\vec{\delta}_{i,j})^ {\top}(\mathbf{W}_{G}^{1/2}\mathbf{B}_{G}\mathbf{L}_{G}^{\dagger}\vec{\delta}_{ i,j}),\]

where \(\mathbf{W}_{G}\in\mathbb{R}^{E\times E}\) is the diagonal matrix of weights in \(G\)[5].

Runtimes and Space Complexities.In our algorithmic results and analysis, when clear from context, we use \(\widetilde{O}(\cdot)\) (resp., \(\widetilde{\Omega}(\cdot)\)) notation to hide polylogarithmic factors (resp., inverse polylogarithmic factors) in the number of vertices, the number of edges, the size of the matrix, the number of nonzero entries in a matrix, the maximum and minimum diagonal element of a matrix, the maximum and minimum weighted degree of a graph, \(\epsilon\), the condition number, and the normalized psuedo-condition number of a matrix. We say event \(E\) occurs with high probability in \(t\) if \(\mathbb{P}\left[E\right]\geqslant 1-t^{-c}\), where \(c>0\) can be controlled by appropriately configuring the algorithm parameters. We may simply say that an event occurs "with high probability" or "whp." if it occurs with high probability in the size of a matrix or number of nodes in a graph.

## 3 Algorithmic Results

In this section, we present our main algorithmic results. Section 3.1 outlines our approach to effective resistance sketches and estimation; we defer discussion of our approach to SDD and PSD extensions to the supplementary material. Section 3.2, presents our original results on effective resistance sketching and estimation and generalizations to SDD matrices. Section 3.3 extends our techniques to yield an interesting data structure for estimating quadratic forms of PSD matrices.

### Our Approach

Approach in prior work: Johnson Lindenstrauss sketches.Our starting inspiration is a classic result of [5], which obtains an \((\widetilde{O}(m\epsilon^{-2}),\widetilde{O}(\epsilon^{-2}),\widetilde{O}(n \epsilon^{-2}))\)-effective resistance sketch by using the Johnson Lindenstrauss Lemma (JL) [39] and its algorithmic instantiations [40].

**Lemma 1** (Johnson-Lindenstrauss Lemma [40]).: _Given fixed vectors \(v_{1},...,v_{n}\in\mathbb{R}^{d}\) and \(\epsilon\in(0,1)\), let \(\mathbf{J}\) be an independently sampled random matrix in \(\{\pm 1/\sqrt{k}\}^{k\times d}\). For \(k=\widetilde{O}(\log(n)\epsilon^{-2})\), whp. in \(n\), \(\left\|\mathbf{J}v_{i}\right\|_{2}\approx_{\epsilon}\left\|v_{i}\right\|_{2}\) for all \(i\in[n]\)._

[5] observe that \(r_{G}(i,j)=(\mathbf{W}_{G}^{1/2}\mathbf{B}_{G}\mathbf{L}_{G}^{\dagger}\vec{ \delta}_{i,j})^{\top}(\mathbf{W}_{G}^{1/2}\mathbf{B}_{G}\mathbf{L}_{G}^{ \dagger}\vec{\delta}_{i,j})\). Consequently, w.h.p in \(n\), \(\left\|\mathbf{J}\mathbf{W}_{G}^{1/2}\mathbf{B}_{G}\mathbf{L}_{G}^{\dagger} \vec{\delta}_{i,j}\right\|_{2}\approx_{\epsilon}r_{G}(i,j)\). With SDD linear system solvers, \(\mathbf{J}\mathbf{W}_{G}^{1/2}\mathbf{B}_{G}\mathbf{L}_{G}^{\dagger}\) can be approximated in \(\widetilde{O}(m\epsilon^{-1})\) time, from which \(\|\mathbf{J}\mathbf{W}_{G}^{1/2}\mathbf{B}_{G}\mathbf{L}_{G}^{\dagger}\vec{ \delta}_{i,j}\|_{2}\) can be queried in \(\widetilde{O}(\epsilon^{-2})\) time.

Our approach: asymmetric CountSketch in \(\ell_{1}\).Towards improving upon JL sketches for effective resistance estimation, our key tool is to use other sketching algorithms. In particular we use that there are algorithms that achieve better than \(\widetilde{O}(\epsilon^{-2})\)-sketch dimension for vectors with small \(\ell_{1}\) norm with comparable guarantees, e.g., CountSketch. CountSketch is a classic memory-efficient algorithm for estimating the number of occurences of various datapoints in a data stream [41] and efficiently computing inner products [42]. Given \(v\in\mathbb{R}^{n}\) and integer parameters \(s,t>0\), CountSketch transforms \(v\) to a vector \(\mathbf{S}v\in\mathbb{R}^{3ts\times n}\), where \(\mathbf{S}\in\mathbb{R}^{3ts\times n}\) is a \(3t\)-column-sparse 0/1 matrix. Lemma 2 is a special case of Theorem 4 from [42], which provides accuracy guarantees for inner product estimation using CountSketch.

**Lemma 2** (Special Case of [42], Theorem 4).: _Let vectors \(v,w\in\mathbb{R}^{n}\). Let \(\mathbf{S}\) be a random CountSketch matrix. Let \(x_{i}=\langle(\mathbf{S}v)[(i-1)s+1:i\cdot s],(\mathbf{S}w)[(i-1)s+1:i\cdot s]\rangle\) for \(i\in[3t]\), and let \(X\) denote the median of \(\{x_{i}\}\). For \(s=O\left(\min\left(\frac{|v|_{1}|w|}{e|\langle v,w\rangle|},\frac{|v|_{2}^{2} |w|_{2}^{2}}{e|\langle v,w\rangle|^{2}}\right)\right)\), and \(t=\log(n^{c})\), \(\left|X-\langle v,w\rangle\right|\leqslant\epsilon\left|\langle v,w\rangle\right|\) with probability at least \(\Omega(1-n^{-c})\)._

To improve the guarantee in Lemma 2 to hold whp. _for all \(v,w\in S\)_ rather than for each fixed pair, one can simply choose \(t=\log(n^{c}|S|)\) and apply a union bound. Consequently, if we knew that \(\|\mathbf{W}_{G}^{1/2}\mathbf{B}_{G}\mathbf{L}_{G}^{\dagger}\vec{\delta}_{i,j} \|_{1}^{2}/r_{G}(i,j)=\widetilde{O}(1)\), then building a CountSketch with \(s=\widetilde{O}(\epsilon^{-1})\) would yield a \(\widetilde{O}(n\epsilon^{-1})\)-size sketch, improving over the \(\widetilde{O}(n\epsilon^{-2})\) sketch obtained using the \(\ell_{2}\) JL sketch in [5]. Unfortunately, it is unclear if and when such a bound holds, and so, it is unclear how the \(\ell_{1}\) CountSketches could be useful in this setting. This leads to the main insight that fuels our algorithms. Rather than seeking a symmetric factorization of \(r_{G}(i,j)\) as a quadratic form \(v^{\top}v\) and applying a sketching procedure to \(v\), we instead work with an _asymmetric factorization_. In particular, we observe

\[r_{G}(i,j)=\langle\mathbf{D}_{\mathbf{L}_{G}}^{-1}\vec{\delta}_{i,j},\mathbf{D }_{\mathbf{L}}^{1/2}(\mathbf{N}_{\mathbf{L}_{G}})^{\dagger}\mathbf{D}_{ \mathbf{L}_{G}}^{-1/2}\vec{\delta}_{i,j}\rangle. \tag{1}\]

At first glance, it may seem unclear why (1) is helpful. However, we show that indeed, for expanders

\[\left\|\mathbf{D}_{\mathbf{L}}^{-1}\vec{\delta}_{i,j}\right\|_{1}\left\| \mathbf{D}_{\mathbf{L}}^{1/2}(\mathbf{N}_{\mathbf{L}})^{\dagger}\mathbf{D}_{ \mathbf{L}}^{-1/2}\vec{\delta}_{i,j}\right\|_{1}/r_{G}(i,j)=\widetilde{O}(1). \tag{2}\]

Our main result essentially follows from (2). Using SDD linear system solvers, we can efficiently approximate \(\mathbf{SD}_{\mathbf{L}}^{1/2}(\mathbf{N}_{\mathbf{L}})^{\dagger}\mathbf{D}_{ \mathbf{L}}^{-1/2}\in\mathbb{R}^{\widetilde{O}(\epsilon^{-1})\times n}\) in \(\widetilde{O}(m\epsilon^{-1})\) time, yielding our \(\mathcal{T}_{\mathrm{q}}\) of \(\widetilde{O}(m\epsilon^{-1})\) and \(s\) of \(\widetilde{O}(n\epsilon^{-1})\) (see discussion in supplementary material). Moreover, \(\mathbf{SD}_{\mathbf{L}}^{-1}\) is \(\widetilde{O}(1)\)-sparse, due to the structure of Count-Sketch. So, using our (approximate) access to \(\mathbf{SD}_{\mathbf{L}}^{1/2}(\mathbf{N}_{\mathbf{L}})^{\dagger}\mathbf{D}_{ \mathbf{L}}^{-1/2}\), for any query \(i,j\in V\), we can efficiently approximate (1) using the recovery procedure of Lemma 2 in \(\widetilde{O}(1)\) time.

### Our Results

We use the approach in Section 3.1 to develop algorithms to compute spectral sketch data structures for SDD matrices \(\mathbf{M}\) with \(\widetilde{O}(1)\) normalized condition number, as defined in Definitions 6 and 7. So, as discussed in Section 2, this implies that our spectral sketch algorithms will automatically apply to normalized Laplacians of expanders and enable us to compute effective resistances.

**Definition 7** (Spectral Sketch Data Structure).: _We say an algorithm produces an \((\mathcal{T}_{\mathrm{s}},\mathcal{T}_{\mathrm{q}},s)\)-spectral sketch data structure for a PSD matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\) if given \(\mathbf{A}\in\mathbb{R}^{n\times n}\) and \(\epsilon\in(0,1)\), the algorithm creates a binary string of length \(O(s(A,\epsilon))\) in time \(O(\mathcal{T}_{\mathrm{s}}(A,\epsilon),\) from which, for any supported query \(b\in\mathbb{R}^{n}\), w.h.p it outputs \(q_{A}(b)\approx_{\epsilon}b^{\top}\mathbf{A}b\) in time \(O(\mathcal{T}_{\mathrm{q}}(A,\epsilon)(\mathrm{nnz}(b))^{2})\)._

Our spectral sketches of SDD matrices \(\mathbf{M}\) will only support \(\mathbf{D}_{M}\)-numerically sparse query vectors.

**Definition 8** (D-numerically sparse).: _For a diagonal matrix \(\mathbf{D}\), the \(\mathbf{D}\)-numerical sparsity of \(x\in\mathbb{R}^{n}\) is \(\mathrm{n}_{\mathrm{SD}}\left(x\right):=\left\|\mathbf{D}^{-1}x\right\|_{1} \left\|x\right\|_{1}/\left\|\mathbf{D}^{-1/2}x\right\|_{2}^{2}\). We say \(x\) is \((c,\mathbf{D})\)-numerically sparse if \(\mathrm{n}_{\mathbf{SD}}\left(x\right)\leqslant c\)._

Definition 8 is restrictive; however, several natural classes of vectors satisfy the requirements. For example, \(\mathbbm{1}_{i}\) are (1, \(\mathbf{D}\))-numerically sparse and \(\vec{\delta}_{i,j}\) is (2, \(\mathbf{D}_{M}\))-numerically sparse for any \(i,j\in[n]\) and invertible diagonal matrix \(\mathbf{D}\in\mathbb{R}^{n\times n}\) (see supplementary material for additional examples.)

The following asymmetric rearrangement of quadratic forms is crucial to our analysis.

**Lemma 3**.: _Let \(\mathbf{M}\in\mathbb{R}^{n\times n}\) be SDD and \(x\in\mathbb{R}^{n}\) be orthogonal to \(\ker\left(\mathbf{M}\right)\). Then,_

Proof.: For notational convenience, let \(\mathbf{N}_{\ell M}=\mathbf{N}_{M}/2\). Let \(v=\mathbf{M}^{\dagger}x=(\mathbf{D}_{M}-\mathbf{A}_{M})^{\dagger}x\). Note that \(\mathbf{D}_{M}^{-1/2}x\perp\ker\left(\mathbf{N}_{M}\right)\), and \(2\mathbf{D}_{M}^{1/2}\mathbf{N}_{\ell M}\mathbf{D}_{M}^{1/2}=\mathbf{M}\). Consequently, \(v=\frac{1}{2}\mathbf{D}_{M}^{-1/2}\mathbf{N}_{\ell M}^{\dagger}\mathbf{D}_{M} ^{-1/2}x\), and hence \(x^{\top}\mathbf{M}^{\dagger}x=\frac{1}{2}\langle\mathbf{D}_{M}^{-1}x,\mathbf{D }_{M}^{1/2}\mathbf{N}_{\ell M}^{\dagger}\mathbf{D}_{M}^{-1/2}x\rangle\). The second equality now follows immediately by rearranging terms. To obtain the inequality, note that, because \(\mathbf{M}\) is SDD and \(\mathbf{D}\) is invertible, \(\mathbf{N}_{M}\) is PSD. Furthermore, \(2\mathbf{I}-\mathbf{N}_{M}=\mathbf{I}-\mathbf{D}_{M}^{-1/2}\mathbf{A}_{M} \mathbf{D}_{M}^{-1/2}\), which is also PSD, as \(\lambda\left(\mathbf{A}_{M}\right)\subset[-d_{\max},d_{\max}]\). So, \(0\leqslant\lambda\left(\mathbf{N}_{M}\right)\leqslant 2\). So, \(\lambda_{\min}(\mathbf{N}_{M})\geqslant 1/2\) and the lemma follows. 

Lemma 4 bounds \(\left\|\mathbf{D}_{M}^{1/2}(\mathbf{N}_{M}/2)^{\dagger}\mathbf{D}_{M}^{-1/2}x \right\|_{1}\). The proof uses the power series expansion of \((\mathbf{N}_{M}/2)^{\dagger}\).

**Lemma 4**.: _Let \(\mathbf{M}\in\mathbb{R}^{n\times n}\) be an SDD matrix and \(x\in\mathbb{R}^{n}\) be a unit vector orthogonal to \(\ker\left(\mathbf{M}\right)\). Then_

\[\left\|\mathbf{D}_{M}^{1/2}(\mathbf{N}_{M}/2)^{\dagger}\mathbf{D}_{M}^{-1/2}x \right\|_{1}\leqslant\max\left(1,2\bar{\kappa}\left(\mathbf{M}\right)\log \left(\sqrt{nd_{\max}2\bar{\kappa}}\left(\mathbf{M}\right)/\sqrt{d_{\min}} \right)\right)\|x\|_{1}+1.\]Combining our Lemmas 3 and 4 with the guarantees of Lemma 2 and prior work on SDD linear system solvers (see supplementary material), we obtain the following theorem.

**Theorem 7**.: _For any SDD matrix \(\mathbf{M}\in\mathbb{R}^{n\times n}\), there is an algorithm to construct an \((\widetilde{\mathcal{O}}(\bar{\kappa}\left(\mathbf{M}\right)\operatorname{nnz}( \mathbf{M})\epsilon^{-1}),\widetilde{\mathcal{O}}(1),\widetilde{\mathcal{O}}( \bar{\kappa}\left(\mathbf{M}\right)n\epsilon^{-1}))\)-spectral sketch data structure of \(\mathbf{M}^{\dagger}\) supported over queries \(S\), where \(S\) is any set of \((\widetilde{\mathcal{O}}(1),\mathbf{D}_{M})\)-numerically sparse vectors orthogonal to \(\ker\left(\mathbf{M}\right)\)._

Because the \(\vec{\delta}_{i,j}\) queries appearing in effective resistance computations are 2-sparse and \((2,\mathbf{D}_{M})\)-numerically sparse for all SDD matrices \(\mathbf{M}\), taking \(\mathbf{M}=\mathbf{L}_{G}\) in Theorem 7 immediately implies Theorem 2 and Theorem 3 (see supplementary material for detailed discussion and pseudocode.)

### Extensions to PSD Matrices

Our approach of approximating quadratic forms via asymmetric inner products also yields a query-efficient sketching procedure for approximating quadratic forms of well-conditioned PSD matrices.

**Theorem 8**.: _There is an algorithm to construct an \((\widetilde{\mathcal{O}}(\kappa(\mathbf{A})\operatorname{nnz}(\mathbf{A}) \epsilon^{-2}),\widetilde{\mathcal{O}}(1),\widetilde{\mathcal{O}}(\kappa( \mathbf{A})n\epsilon^{-2}))\)-spectral sketch data structure of \(\mathbf{A}\) supported over \(S\), where \(S\) is any set of vectors orthogonal to \(\ker\left(\mathbf{A}\right)\) and \(\mathbf{A}\) is PSD._

In comparison, JL [39] gives an \((\widetilde{\mathcal{O}}(n^{\omega}),\widetilde{\mathcal{O}}(\epsilon^{-2}), \widetilde{\mathcal{O}}(n\epsilon^{-2}))\)-spectral sketch data structure using efficient square root algorithms [43]. JL achieves better compression than Theorem 8, while Theorem 8 achieves faster query time. When the matrix is well conditioned and error tolerance is sufficiently high, Theorem 8 may achieve better construction time and query time than JL while maintaining comparable compression.

## 4 Lower Bounds

In this section, we present our main hardness results. Section 4.1 outlines our approach. In Section 4.2, we present our lower bounds for the problem of estimating effective resistances for all pairs of nodes (case where \(S=V\times V\)), which we call the "all pairs effective resistance estimation problem." Section 4.3 shows our techniques also yield lower bounds for other spectral sum estimation problems.

### Our Approach

Approaches of Previous WorkThe approach of [33] begins with the fact that \(G\) has a triangle if and only if \(\left(\mathbf{A}_{G}^{3}\right)/6\geq 1\). They use the fact that various spectral sums \(\mathcal{S}_{f}\) of the of the SDD matrix \(\mathbf{I}-\delta\mathbf{A}_{G}\) (for \(\delta\) sufficiently small) can be expressed as a power series \(\mathcal{S}_{f}(\mathbf{I}-\delta\mathbf{A}_{G})=\sum_{k=0}^{\infty}c_{k} \delta^{k}\operatorname{tr}(\mathbf{A}_{G}^{k})\). The first two terms of this series can be computed directly. So given \(Y\approx_{\epsilon}\mathcal{S}_{f}(\mathbf{I}-\delta\mathbf{A}_{G})\), one can estimate \(\operatorname{tr}\left(\mathbf{A}_{G}^{3}\right)\), where the estimation error is controlled by the magnitude of the first two terms of the series and the tail error due to truncating at the third term. By bounding this estimation error, [33] show that, for appropriate choices of \(\delta\), \(Y\approx_{\epsilon}\mathcal{S}_{f}(\mathbf{I}-\delta\mathbf{A}_{G})\) yields an additive 1/2 approximation to \(\operatorname{tr}\left(\mathbf{A}_{G}^{3}\right)\), which is sufficient for triangle detection. They also reduce the problem of estimating the spectral sum \(\operatorname{tr}\left(\mathbf{B}^{-1}\right)\) for an SDD matrix \(\mathbf{B}\) to the all pairs effective resistance estimation problem.

Our ApproachWe use three key techniques to better bound the estimation errors incurred in the power-series-inspired approach of [33]. This yields faster reductions and better lower bounds for effective resistance estimation. Rather than obtaining effective resistance lower bounds by reducing the problem of computing \(\operatorname{tr}\left(\mathbf{A}_{G}^{3}\right)/6\) to computing the _trace_ of an SDD matrix as in [33], we use a reduction that closely resembles the structure of effective resistances. For \(\alpha>0\) sufficiently small,

\[\left(\mathbf{I}-\frac{\alpha}{n}\mathbf{A}_{G}\right)^{-1}=\sum_{k=0}^{ \infty}\frac{\alpha^{k}}{n^{k}}\mathbf{A}_{G}^{k}. \tag{3}\]

Since \(\mathbf{A}_{G}\) is known, given access to \(\vec{\delta}_{i,j}^{\top}(\mathbf{I}-\frac{\alpha}{n}\mathbf{A}_{G})^{-1}\vec{ \delta}_{i,j}\), we can estimate the entries of \(\mathbf{A}_{G}^{2}\), where the estimation error is controlled by \(\alpha\) and the tail error of truncating (3) at the third term. By bounding this estimation error, for appropriate choice of \(\alpha\), we can obtain additive 1/2 approximations to all entries of \(\mathbf{A}_{G}^{2}\), which is sufficient to identify all paths of length 2. We can then detect a triangle by simply scanning for an edge \(\{u,v\}\) such that \(u\) and \(v\) are connected by a path of length 2. Estimating the entries of \(\mathbf{A}_{G}^{2}\) leads to lower estimation error than estimating \(\operatorname{tr}\left(\mathbf{A}_{G}^{3}\right)\) as in [33]). Second, we use a standard randomized reduction that reduces the triangle detection problem to the triangle detection problem restricted to tripartite graphs. The reduction relies on the fact that a randomly sampled tripartition of the original graph preserves triangles with constant probability. To detect a triangle in a tripartite graph \(G=(V_{1}\sqcup V_{2}\sqcup V_{3},E)\), we construct a graph \(H\) by removing all edges \(E_{1,2}:=\{\{u,v\}\in E:u\in V_{1},v\in V_{2}\}\) between \(V_{1}\) and \(V_{2}\). \(G\) has a triangle if and only if there is an edge \(\{u,v\}\in E_{1,2}\) and a path of length 2 between \(u\) and \(v\) in \(H\). Crucially, we can show that the third term \(\bar{\mathbf{A}}_{H}^{3}\) does not contribute to the tail error when estimating the \(\{u,v\}\)-th entry of \(\left(\mathbf{A}_{H}^{2}\right)\) using (3). Third, to lower the spectral norm of \(\mathbf{A}_{H}\) (and consequently better bound the convergence of the power series (3)), we introduce the symmetric random signing of \(\mathbf{A}_{H}\) below.

**Definition 9** (Symmetric Random Signing).: _Given a symmetric matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\), its symmetric random signing \(\bar{\mathbf{A}}\) is the random matrix with \(\bar{\mathbf{A}}_{i,j}:=\xi_{i,j}\mathbf{A}_{i,j}\), where \(\xi_{i,j}\) are independent Rademacher random variables that satisfy \(\xi_{i,j}=\xi_{j,i}\)._

We show that this random signing preserves the non-seroness of entries of \(\mathbf{A}_{H}^{2}\) with constant probability, allowing us to detect whether \(G\) has a triangle even if we replace \(\mathbf{A}_{G}\) in (3) with \(\bar{\mathbf{A}}_{H}\) instead. This is beneficial, as matrix Chernoff guarantees \(\left\|\bar{\mathbf{A}}_{H}\right\|_{2}=\widehat{O}(\sqrt{n})\) whp. whereas \(\left\|\mathbf{A}_{H}\right\|_{2}\) may be as large as \(n\). So the tail error of truncating the power series is smaller. To compute entries of \(\bar{\mathbf{A}}_{H}^{2}\) efficiently using effective resistance estimates on expanders, we first show that we can use all pairs effective resistance estimates on expanders to estimate \(\delta_{i,j}^{\top}\mathbf{M}^{-1}\vec{\delta}_{i,j}\) for all \(i,j\in[n]\), where \(\mathbf{M}=(\mathbf{I}-\mathbf{Q})\) is an SDD matrix with \(\rho(\mathbf{Q})\leqslant 1/3\). Then, by choosing \(\mathbf{M}=\mathbf{I}-\frac{\alpha}{n}\bar{\mathbf{A}}_{H}\) as in (3) for an appropriate constant \(\alpha\), we can estimate \(\bar{\mathbf{A}}_{H}^{2}\) from estimates of \(\vec{\delta}_{i,j}^{\top}\mathbf{M}^{-1}\vec{\delta}_{i,j}\). This yields our lower bound on the all pairs effective resistance estimation problem.

Additionally, we show that random signing also preserves the non-seroness of \(\operatorname{tr}\left(\mathbf{A}_{G}^{3}\right)\) with constant probability, and leverage this to obtain improved randomized conditional lower bounds for various spectral sum estimation problems. We closely follow the trace estimation approach of [33], and again use the smaller spectral radius of \(\bar{\mathbf{A}}_{G}\) to improve bounds on the power series truncation error.

### Improved Lower Bounds for Effective Resistance Estimation

In this section we provide a series of reductions which yield our main result on lower bounds for the all pairs effective resistance estimation problem for all pairs of nodes (case where \(S=V\times V\)).

**Definition 10**.: _In the SDD effective resistance estimation problem, given an SDD matrix \(\mathbf{M}\) such that \(\mathbf{D}_{\mathbf{M}}=\mathbf{I}\), \(\mathbf{A}_{\mathbf{M}}=\mathbf{Q}\), with \(\rho(\mathbf{Q})\leqslant 1/3\) and \(\epsilon\in(0,1)\), we must output \(\widehat{r}\in\mathbb{R}^{n^{2}}\) such that \(\widetilde{r}_{a,b}\approx_{\epsilon}\vec{\delta}_{a,b}^{\top}\mathbf{M}^{-1} \vec{\delta}_{a,b}\ \forall a,b\in[n]\). We call \(\vec{\delta}_{a,b}^{\top}\mathbf{M}^{-1}\vec{\delta}_{a,b}\) the SDD effective resistance of \((a,b)\) in \(\mathbf{M}\)._

For brevity, we use \(\widehat{r}(\mathbf{M})\) to refer to the solution of the SDD effective resistance problem on input \(\mathbf{M}\). Our first step is to show that an algorithm for the all pairs effective resistance estimation problem on expanders implies an algorithm for the SDD effective resistance estimation problem.

**Lemma 5**.: _Given an algorithm to solve the all pairs effective resistance estimation problem on graphs with \(\widetilde{\Omega}(1)\)-expansion in \(\widetilde{O}(n^{2}\epsilon^{-c})\) time for some \(c>0\), we can produce an algorithm to solve the SDD effective resistance estimation problem in \(\widetilde{O}(n^{2}\epsilon^{-c})\) time._

To prove Lemma 5, we first prove the lemma for the case where \(\mathbf{Q}\) is entrywise non-negative by constructing an expander \(G\) with \(n+1\) vertices such that \(r_{G}(a,b)=\vec{\delta}_{a,b}^{\top}\mathbf{M}^{-1}\vec{\delta}_{a,b}\) for all \(a,b\in[n]\). We extend the reduction to arbitrary \(\mathbf{Q}\) by constructing \(\mathbf{Q}^{\prime}\) of size \(2n\) so that \(\mathbf{Q}^{\prime}\) is entrywise non-negative and \(\widetilde{r}(\mathbf{I}-\mathbf{Q})\) is a simple linear transform of \(\widetilde{r}(\mathbf{I}-\mathbf{Q}^{\prime})\).

We turn our attention to reducing the triangle detection problem to the SDD effective resistance problem. As discussed, a key aspect of our approach is to work with the random signing \(\bar{\mathbf{A}}_{G}\) of \(\mathbf{A}_{G}\). Lemma 6 shows that to determine whether \((\mathbf{A}_{G}^{2})_{i,j}>0\) with constant probability, it suffices to determine whether \((\bar{\mathbf{A}}_{G}^{2})_{i,j}>0\). Matrix Chernoff ensures whp. \(\rho(\bar{\mathbf{A}}_{G})=\widetilde{O}(\sqrt{n})\), while \(\rho(\mathbf{A}_{G})\) could be as large as \(n\)[44]. So, estimating entries of \(\bar{\mathbf{A}}_{G}\) leads to lower power series tail error.

**Lemma 6**.: _For \(i\neq j\), if \((\mathbf{A}_{G}^{2})_{i,j}=0\), then \((\bar{\mathbf{A}}_{G}^{2})_{i,j}=0\); if \((\mathbf{A}_{G}^{2})_{i,j}>0\), \(\mathbb{P}\left[|(\bar{\mathbf{A}}_{G}^{2})_{i,j}|>1\right]\geqslant 1/2\)._The idea of the proof is that if \(\{a,b\},\{b,c\}\) exist in \(G\), either \(\xi_{a,c}=1\) or \(\xi_{a,c}=-1\) results in \((\bar{\mathbf{A}}_{G}^{2})_{a,c}>0\). Finally, we use the power series approach in Section 4.1 to obtain Theorem 9.

**Theorem 9**.: _Given an algorithm which solves the SDD effective resistance estimation problem in \(\widetilde{O}(n^{2}\epsilon^{-c})\) time, we can produce a randomized algorithm that solves the triangle detection problem in \(\widetilde{O}(n^{2(1+c)})\) time whp._

Theorem 9 and Lemma 5 with \(c=1/2-\delta\) immediately imply our main result Theorem 4.

Additionally, we extend the lower bound of Theorem 4 to the all edges effective resistance estimation by the following reduction.

**Lemma 7**.: _Given an algorithm to solve the all edges effective resistance estimation problem (i.e., Definition 1 where \(S=\) E) in \(\widetilde{O}(m\epsilon^{-c})\) time, we can produce an algorithm to solve the all pairs effective resistance estimation problem in \(\widetilde{O}(n^{2}\epsilon^{-c})\) time for some \(c>0\)._

The rough idea behind the reduction is to add a complete graph of edges of sufficiently small weight that would not change the effective resistances much. Lemma 7 combined with Theorem 4 then imply a \(\widetilde{\Omega}(m\epsilon^{-1/2})\) randomized lower bound for the all edges effective resistance estimation problem on graphs with graphs with \(\widetilde{\Omega}(1)\)-expansion.

### Improved Lower Bounds for Spectral Sum Estimation

Finally, we discuss our improved lower bounds for various spectral sum estimation problems. Analogous to Lemma 6, in the following lemma we show that to determine whether a graph has a triangle (i.e., \(\operatorname{tr}\left(\mathbf{A}_{G}^{3}\right)>0\)) with constant probability, it suffices to determine whether \(\operatorname{tr}\left(\bar{\mathbf{A}}_{G}^{3}\right)>0\).

**Lemma 8**.: _If \(\operatorname{tr}\left(\mathbf{A}_{G}^{3}\right)=0\), then \(\operatorname{tr}\left(\bar{\mathbf{A}}_{G}^{3}\right)=0\), and if \(\operatorname{tr}\left(\mathbf{A}_{G}^{3}\right)>0\) then \(\mathbb{P}\left[\left|\operatorname{tr}\left(\bar{\mathbf{A}}_{G}^{3}\right) \right|>0\right]\geq 1/4\)._

The central idea of the proof is that if a triangle \(\{a,b\},\{b,c\},\{c,a\}\) exists in \(G\), then amongst the 4 possible configurations of the Rademacher random signing variables \(\xi_{a,b}\) and \(\xi_{b,c}\), at least one configuration must result in \(\left|\operatorname{tr}\left(\bar{\mathbf{A}}_{G}^{3}\right)\right|>0\). By following the proof of Theorem 15 from [33], and replacing their use of \(\mathbf{A}_{G}\) with a symmetric random signing \(\bar{\mathbf{A}}_{G}\), we obtain an improved randomized version of their result by leveraging the smaller spectral radius of \(\bar{\mathbf{A}}_{G}\). Theorem 5 follows by applying this result to the functions \(f\) that define the corresponding spectral sums (see supplementary material).

## 5 Conclusion

In this paper we provided improved upper and lower bounds on the problem of estimating and sketching effective resistances on expanders. On the algorithmic side we show how sketches tailored to \(\ell_{1}\) when carefully applied to asymmetric formulations of the quadratic form of the Laplacian pseudoinverse gave our results. On the lower bound side, we provided an alternative to the trace estimation approach of [33] for showing lower bounds and coupled it with techniques of randomly signing edges of the graph to obtain our results. Further, we showed that these techniques had broader implications for addressing algorithmic challenges in numerical linear algebra.

Beyond the natural open problem of improving both our upper and lower bounds towards bringing them together, there are interesting open problems in broadening the applicability of both our upper and lower bounds. For example, obtaining an \(\widetilde{O}(m\epsilon^{-1})\) time algorithm for estimating the effective resistance of all edges in a general (non-expander) graph and extending our \(\widetilde{\Omega}(n^{2}\epsilon^{-1/2})\) lower bounds to deterministic algorithms remain interesting open problems. We hope that the results of this paper provide useful tools for addressing each.

## Acknowledgments and Disclosure of Funding

We thank Hongyue Li for helpful discussions and work on this project at various stages. Aaron Sidford was supported in part by a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, a PayPal research award, and a Sloan Research Fellowship. Ishani Karmarkar was supported in part by an NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, a PayPal research grant, and a Stanford Institute for Computational and Mathematical Engineering (ICME) Fellowship. Rajat Vadiraj Dwaraknath was supported by a Stanford ICME Fellowship.

## References

* [1] Aleksander Madry, Damian Straszak, and Jakub Tarnawski. Fast generation of random spanning trees and the effective resistance metric. In _Symposium on Discrete Algorithms_, 2014.
* [2] Karel Devriendt. Effective resistance is more than distance: Laplacians, simplices and the schur complement. In _Linear Algebra and its Applications_, 2022.
* [3] Huan Li and Zhongzhi Zhang. Kirchhoff index as a measure of edge centrality in weighted networks: Nearly linear time algorithms. In _Symposium on Discrete Algorithms_. Society for Industrial and Applied Mathematics, 2018.
* [4] Douglas Klein and Milan Randic. Resistance distance. In _Journal of Mathematical Chemistry_, 1993.
* [5] Daniel A. Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. In _SIAM Journal of Computing_, 2009.
* [6] Yin Tat Lee and He Sun. Constructing linear-sized spectral sparsification in almost-linear time. In _SIAM Journal of Computing_, 2015.
* [7] David Durfee, Yu Gao, Gramoz Goranci, and Richard Peng. Fully dynamic spectral vertex sparsifiers and applications. In _Symposium on Theory of Computing_, 2019.
* [8] Rakshith S Srinivasa, Cao Xiao, Lucas Glass, and Jimeng Sun Justin Romberg. Fast graph attention networks using effective resistance based graph sparsification. In _arXiv preprint arXiv:2006.08796_, 2020.
* [9] Tasweer Ahmad, Lianwen Jin, Luojun Lin, and GuoZhi Tang. Skeleton-based action recognition using sparse spatio-temporal gcn with edge effective resistance. In _Neurocomputing_, 2021.
* [10] Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, and Shang-Hua Teng. Efficient sampling for gaussian graphical models via spectral sparsification. In _Proceedings of the Conference on Learning Theory_, 2015.
* [11] Daniele Calandriello, Alessandro Lazaric, Ioannis Koutis, and Michal Valko. Improved large-scale graph learning through ridge spectral sparsification. In _International Conference on Machine Learning_, 2018.
* [12] Alexander Mercier, Samuel Scarpino, and Cristopher Moore. Effective resistance against pandemics: Mobility network sparsification for high-fidelity epidemic simulations. In _PLOS Computational Biology_, 2022.
* [13] Paul Christiano, Jonathan A. Kelner, Aleksander Madry, Daniel A. Spielman, and Shang-Hua Teng. Electrical flows, laplacian systems, and faster approximation of maximum flow in undirected graphs. In _Symposium on Theory of Computing_, 2010.
* [14] Aleksander Madry. Navigating central path with electrical flows: from flows to matchings, and back. In _Symposium on Foundations of Computer Science_, 2013.
* [15] Aleksander Madry. Computing maximum flow with augmenting electrical flows. In _Computing Maximum Flow with Augmenting Electrical Flows_, 2016.
* [16] Jan van den Brand, Yu Gao, Arun Jambulapati, Yin Tat Lee, Yang P Liu, Richard Peng, and Aaron Sidford. Faster maxflow via improved dynamic spectral vertex sparsifiers. In _Symposium on Theory of Computing_, 2022.
* [17] Jan Van Den Brand, Yang P Liu Yin Tat Lee, Thatchaphol Saranurak, Aaron Sidford, Zhao Song, and Di Wang. Minimum cost flows, mdps, and \(\ell_{1}\)-regression in nearly linear time for dense instances. In _Symposium on Theory of Computing_, 2021.
* [18] Yin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solving linear programs in o (vrank) iterations and faster algorithms for maximum flow. In _Symposium on Foundations of Computer Science_, 2014.

* [19] David Durfee, Rasmus Kyng, John Peebles, Anup B. Rao, and Sushant Sachdeva. Sampling random spanning trees faster than matrix multiplication. In _Symposium on Theory of Computing_, 2017.
* [20] Aaron Schild. An almost-linear time algorithm for uniform random spanning tree generation. In _Symposium on Theory of Computing_, 2018.
* [21] Aleksander Madry, Damian Straszak, and Jakub Tarnawski. Fast generation of random spanning trees and the effective resistance metric. In _Symposium on Discrete Algorithms_, 2014.
* [22] Vedat Levi Alev, Nima Anari, Lap Chi Lau, and Shayan Oveis Gharan. Graph clustering using effective resistance. In _arXiv preprint arXiv:1711.06530_, 2017.
* [23] Zhiqiang Zhao and Zhuo Feng. Effective-resistance preserving spectral reduction of graphs. In _Proceedings of the Annual Design Automation Conference_, 2019.
* [24] Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio', and Michael Bronstein. On over-squashing in message passing neural networks: The impact of width, depth, and topology. In _International Conference on Machine Learning_, 2023.
* [25] Pradeep Banerjee, Kedar Karhadkar, and Guido Montufar Yu Guang Wang, Uri Alon. Oversquashing in gnns through the lens of information contraction and graph expansion. In _Allerton Conference on Communication, Control, and Computing_, 2022.
* [26] Mitchell Black, Amir Nayyeri, Zhengchao Wan, and Yusu Wang. Understanding oversquashing in gnns through the lens of effective resistance. In _International Conference on Machine Learning_, 2023.
* [27] Ameya Velingker, Ali Kemal Sinop, Ira Ktena, Petar Velivckovic, and Sreenivas Gollapudi. Affinity-aware graph networks. In _arXiv preprint arXiv:2206.11941_, 2022.
* [28] Arun Jambulapati and Aaron Sidford. Efficient \(\tilde{O}(n/\epsilon)\) spectral sketches for the laplacian and its pseudoinverse. In _Symposium on Discrete Algorithms_, 2018.
* [29] Lawrence Li and Sushant Sachdeva. A new approach to estimating effective resistances and counting spanning trees in expander graphs. In _Symposium on Discrete Algorithms_, 2022.
* [30] Timothy Chu, Yu Gao, Richard Peng, Sushant Sachdeva, Saurabh Sawlani, and Junxing Wang. Graph sparsification, spectral sketches, and faster resistance computation, via short cycle decompositions. In _Symposium on Foundations of Computer Science_, 2018.
* [31] Gramoz Goranci, Monika Henzinger, and Pan Peng. Dynamic effective resistances and approximate schur complement on separable graphs. In _Embedded Systems and Applications_, 2018.
* [32] Michael Dinitz, Robert Krauthgamer, and Tal Wagner. Towards resistance sparsifiers, 2015.
* [33] Cameron Musco, Praneeth Netrapalli, Aaron Sidford, Shashanka Ubara, and David P Woodruff. Spectrum approximation beyond fast matrix multiplication: Algorithms and hardness. In _arXiv preprint arXiv:1704.04163_, 2017.
* [34] Virginia Vassilevska Williams and R. Ryan Williams. Subcubic equivalences between path, matrix, and triangle problems. In _Symposium on Foundations of Computer Science_, 2018.
* [35] Ran Duan, Hongxun Wu, and Renfei Zhou. Faster matrix multiplication via asymmetric hashing. In _[https://arxiv.org/abs/2210.10173_](https://arxiv.org/abs/2210.10173_), 2023.
* [36] Alexandr Andoni, Jiecao Chen, Robert Krauthgamer, Bo Qin, David P. Woodruff, and Qin Zhang. On sketching quadratic forms. In _Proceedings of the Conference on Innovations in Theoretical Computer Science_, 2015.
* [37] Huan Li and Aaron Schild. Spectral subspace sparsification. In _Symposium on Foundations of Computer Science_, 2018.
* [38] Jeff Cheeger. A lower bound for the smallest eigenvalue of the laplacian. In _Problems in Analysis_, 1971.
* [39] William B. Johnson. Extensions of lipschitz mappings into hilbert space. In _Contemporary mathematics_, 1984.
* [40] Dimitris Achlioptas. Database-friendly random projections. In _Proceedings of the Symposium on Principles of Database Systems_, 2001.

* [41] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In _Theoretical Computer Science_, 2004.
* [42] Kasper Green Larsen, Rasmus Pagh, and Jakub Tetek. Countsketches, feature hashing and the median of three. In _International Conference on Machine Learning_, 2021.
* [43] Prateek Jain, Chi Jin, Sham M. Kakade, and Praneeth Netrapalli. Global convergence of non-convex gradient descent for computing matrix squareroot. In _International Conference on Artificial Intelligence and Statistics_, 2017.
* [44] Joel A Tropp. User-friendly tail bounds for sums of random matrices. In _Foundations of computational mathematics_, 2012.
* [45] Yu Gao, Yang Liu, and Richard Peng. Fully dynamic electrical flows: Sparse maxflow faster than goldberg-rao. In _SIAM Journal on Computing_, 2023.
* [46] Jan van den Brand, Yu Gao, Arun Jambulapati, Yin Tat Lee, Yang P. Liu, Richard Peng, and Aaron Sidford. Faster maxflow via improved dynamic spectral vertex sparsifiers. In _Symposium on Theory of Computing,_, 2022.
* [47] Ignacio Garcia-Marco, Pascal Koiran, Timothee Pecatte, and Stephan Thomasse. On the complexity of partial derivatives. In _Theoretical Computer Science_, 2017.
* [48] Jacques Morgenstern. Note on a lower bound on the linear complexity of the fast fourier transform. In _Journal of the Association for Computing Machinery_, 1973.
* [49] Amir Shpilka. Lower bounds for matrix product. In _SIAM Journal on Computing_, 2003.
* [50] Virginia Vassilevska Williams, Eyob Woldeghebriel, and Yinzhan Xu. Algorithms and lower bounds for replacement paths under multiple edge failure. In _Symposium on Foundations of Computer Science_, 2022.
* [51] Mahdi Boroujeni, Sina Dehghani, Soheil Ehsani, MohammadTaghi HajiAghayi, and Saeed Seddighin. Subcubic equivalences between graph centrality measures and complementary problems. In _arXiv preprint arXiv:1905.08127_, 2019.
* [52] Rasmus Kyng and Sushant Sachdeva. Approximate gaussian elimination for laplacians: Fast, sparse, and simple. In _Symposium on Foundations of Computer Science_, 2016.
* [53] Jonathan A. Kelner, Lorenzo Orecchia, Aaron Sidford, and Zeyuan Allen Zhu. A simple, combinatorial algorithm for solving sdd systems in nearly-linear time. In _Symposium on Theory of Computing_, 2014.
* [54] Daniel A Spielman and Shang-Hua Teng. A local clustering algorithm for massive graphs and its application to nearly linear time graph partitioning. In _SIAM Journal on Computing_, 2013.
* [55] Arun Jambulapati and Aaron Sidford. Ultrasparse ultrasparsifiers and faster laplacian system solvers. In _Symposium on Discrete Algorithms_, 2021.
* [56] Neha Gupta and Aaron Sidford. Exploiting numerical sparsity for efficient learning: Faster eigenvector computation and regression. In _Neural Information Processing Systems_, 2018.
* [57] Suk-Geun Hwang. Cauchy's interlace theorem for eigenvalues of hermitian matrices. In _The American mathematical monthly_, 2004.