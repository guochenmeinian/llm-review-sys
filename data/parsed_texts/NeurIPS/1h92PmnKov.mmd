# Momentum Provably Improves Error Feedback!

 Ilyas Fatkhullin

ETH AI Center & ETH Zurich & Alexander Tyurin

KAUST\({}^{*}\) &Peter Richtarik

KAUST

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia.

###### Abstract

Due to the high communication overhead when training machine learning models in a distributed environment, modern algorithms invariably rely on lossy communication compression. However, when untreated, the errors caused by compression propagate, and can lead to severely unstable behavior, including exponential divergence. Almost a decade ago, Seide et al. (2014) proposed an error feedback (EF) mechanism, which we refer to as EF14, as an immensely effective heuristic for mitigating this issue. However, despite steady algorithmic and theoretical advances in the EF field in the last decade, our understanding is far from complete. In this work we address one of the most pressing issues. In particular, in the canonical nonconvex setting, all known variants of EF rely on very large batch sizes to converge, which can be prohibitive in practice. We propose a surprisingly simple fix which removes this issue both theoretically, and in practice: the application of Polyak's momentum to the latest incarnation of EF due to Richtarik et al. (2021) known as EF21. Our algorithm, for which we coin the name EF21-SGDM, improves the communication and sample complexities of previous error feedback algorithms under standard smoothness and bounded variance assumptions, and does not require any further strong assumptions such as bounded gradient dissimilarity. Moreover, we propose a double momentum version of our method that improves the complexities even further. Our proof seems to be novel even when compression is removed from the method, and as such, our proof technique is of independent interest in the study of nonconvex stochastic optimization enriched with Polyak's momentum.

## 1 Introduction

Since the practical utility of modern machine learning models crucially depends on our ability to train them on large quantities of training data, it is imperative to perform the training in a distributed storage and compute environment. In federated learning (FL) (Konecny et al., 2016; Kairouz, 2019), for example, data is naturally stored in a distributed fashion across a large number of clients (who capture and own the data in the first place), and the goal is to train a single machine learning model from the wealth of all this distributed data, in a private fashion, directly on their devices.

**1.1 Formalism.** We consider the problem of collaborative training of a single model by several clients in a data-parallel fashion. In particular, we aim to solve the _distributed nonconvex stochastic optimization problem_

\[\min_{x\in\mathbb{R}^{d}}\left[f(x):=\tfrac{1}{n}\sum\limits_{i=1}^{n}f_{i}(x) \right],\qquad f_{i}(x):=\mathbb{E}_{\xi_{i}\sim\mathcal{D}_{i}}\left[f_{i}(x, \xi_{i})\right],\qquad i=1,\dots,n,\] (1)

where \(n\) is the number of clients, \(x\in\mathbb{R}^{d}\) represents the parameters of the model we wish to train, and \(f_{i}(x)\) is the (typically nonconvex) loss of model parameterized by the vector \(x\) on the data \(\mathcal{D}_{i}\) owned by client \(i\). Unlike most works in federated learning, we do not assume the datasets to be similar, i.e., we allow the distributions \(\mathcal{D}_{1},\dots,\mathcal{D}_{n}\) to be arbitrarily different.

We are interested in the fundamental problem of finding an approximately stationary point of \(f\) in expectation, i.e., we wish to find a (possibly random) vector \(\hat{x}\in\mathbb{R}^{d}\) such that \(\mathbb{E}\left[\left\|\nabla f(\hat{x})\right\|\right]\leq\varepsilon\). In order to solve this problem, we assume that the \(n\) clients communicate via an orchestrating server. Typically, the role of the server is to first perform aggregation of the messages obtained from the workers, and to subsequently broadcast the aggregated information back to the workers. Following an implicit assumption made in virtually all theoretically-focused papers on communication-efficient training, we also assume that the speed of client-to-workers broadcast is so fast (compared to speed of workers-to-client communication) that the cost associated with broadcast can be neglected2.

Footnote 2: While this is a reasonable assumption in many practical situations [Mishchenko et al., 2019, Kairouz, 2019], some works consider the regime when the server-to-workers broadcast cannot be neglected [Horvath et al., 2019a, Tang et al., 2020, Philippenko and Dieuleveut, 2020, Kovalev et al., 2021, Fatkullin et al., 2021, Gruntkowska et al., 2022].

**1.2 Aiming for communication and computation efficiency at the same time.** In our work, we pay attention to two key aspects of efficient distributed training--_communication cost_ and _computation cost_ for finding an approximate stationary point \(\hat{x}\). The former refers to the number of bits that need to be communicated by the workers to the server, and the latter refers to the number of stochastic gradients that need to be sampled by each client. The rest of the paper can be summarized as follows: _We pick one of the most popular communication-efficient gradient-type methods (the \(\mathsf{EF}21\) method of Richtarik et al. [2021] - the latest variant of error feedback pioneered by Seide et al. [2014]) and modify it in a way which provably preserves its communication complexity, but massively improves its computation/sample complexity, both theoretically and in practice._

## 2 Communication Compression, Error Feedback, and Sample Complexity

Communication compression techniques such as _quantization_[Alistarh et al., 2017, Horvath et al., 2019a] and _sparsification_[Seide et al., 2014, Beznosikov et al., 2020] are known to be immensely powerful for reducing the communication footprint of gradient-type3 methods. Arguably the most studied, versatile and practically useful class of compression mappings are _contractive_ compressors.

Footnote 3: For Newton-type methods, see [Islamov et al., 2022] and references therein.

**Definition 1** (Contractive compressors).: _We say that a (possibly randomized) mapping \(\mathcal{C}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is a contractive compression operator if there exists a constant \(0<\alpha\leq 1\) such that_

\[\mathbb{E}\left[\left\|\mathcal{C}(x)-x\right\|^{2}\right]\leq(1-\alpha) \left\|x\right\|^{2},\qquad\forall x\in\mathbb{R}^{d}.\] (2)

Inequality (2) is satisfied by a vast array of compressors considered in the literature, including numerous variants of sparsification operators [Alistarh et al., 2018, Stich et al., 2018], quantization operators [Alistarh et al., 2017, Horvath et al., 2019a], and low-rank approximation [Vogels et al., 2019, Safaryan et al., 2022] and more [Beznosikov et al., 2020, Safaryan et al., 2021]. The canonical examples are i) the \(\mathrm{Top}K\) sparsifier, which preserves the \(K\) largest components of \(x\) in magnitude and sets all remaining coordinates to zero [Stich et al., 2018], and ii) the (scaled) \(\mathrm{Rand}K\) sparsifier, which preserves a subset of \(K\) components of \(x\) chosen uniformly at random and sets all remaining coordinates to zero [Beznosikov et al., 2020]. In both cases, (2) is satisfied with \(\alpha=\nicefrac{{K}}{{d}}\).

### Brief history of error-feedback

When greedy contractive compressors, such as \(\mathrm{Top}K\), are used in a direct way to compress the local gradients in distributed gradient descent (GD), the resulting method may diverge exponentially, even on strongly convex quadratics [Beznosikov et al., 2020]. Empirically, instability caused by such a naive application of greedy compressors was observed much earlier, and a fix was proposed in the form of the _error feedback_ (EF) mechanism by Seide et al. [2014], which we henceforth call \(\mathsf{EF}14\) or \(\mathsf{EF}14\)-\(\mathsf{SGD}\) (in the stochastic case).4 To the best of our knowledge, the best _sample complexity_ of \(\mathsf{EF}14\)-\(\mathsf{SGD}\) for finding a stationary point in the distributed nonconvex setting is given by Koloskova et al. [2020]: after \(\mathcal{O}(G\alpha^{-1}\varepsilon^{-3}+\sigma^{2}n^{-1}\varepsilon^{-4})\) samples5, \(\mathsf{EF}14\)-\(\mathsf{SGD}\) finds a point \(x\) such that \(\mathbb{E}[\left\|\nabla f(x)\right\|]\leq\varepsilon\), where \(\alpha\) is the contraction parameter (see Definition 1). However, such an analysis has two important deficiencies. First, in the deterministic case (when exact gradients are computable by each node), the analysis only gives the suboptimal \(\mathcal{O}(\varepsilon^{-3})\)_iteration complexity_, which is suboptimal compared to vanilla (i.e., non-compressed) gradient descent, whose iteration complexity is \(\mathcal{O}(\varepsilon^{-2})\). Second, their analysis relies heavily on additional strong assumptions, such as the _bounded gradient_ (BG) assumption, \(\mathbb{E}[\|\nabla f_{i}(x,\xi_{i})\|^{2}]\leq G^{2}\) for all \(x\in\mathbb{R}^{d}\), \(i\in[n]\), \(\xi_{i}\sim\mathcal{D}_{i}\), or the bounded gradient similarity (BGS) assumption, \(\frac{1}{n}\sum_{i=1}^{n}\|\nabla f_{i}(x)-\nabla f(x)\|^{2}\leq G^{2}\) for all \(x\in\mathbb{R}^{d}\). Such assumptions are restrictive and sometimes even unrealistic. In particular, both BG and BGS might not hold even in the case of convex quadratic functions.6 Moreover, it was recently shown that nonconvex analysis of stochastic gradient methods using a BG assumption may hide an exponential dependence on the smoothness constant in the complexity (Yang et al., 2023).

Footnote 6: For example, one can consider \(f_{i}(x)=x^{\top}A_{i}x\) with \(A_{i}\in\mathbb{R}^{d\times d}\), for which BG or BGS assumptions hold only in the trivial cases: matrices \(A_{i}\) are all zero or all equal to each other (homogeneous data regime).

In 2021, these issues were _partially_ resolved by Richtarik et al. (2021), who propose a modification of the EF mechanism, which they call EF21. They address both deficiencies of the original EF14 method: i) they removed the BG/BGS assumptions, and improved the iteration complexity to \(\mathcal{O}(\varepsilon^{-2})\) in the full gradient regime. Subsequently, the EF21 method was modified in several directions, e.g., extended to bidirectional compression, variance reduction and proximal setup (Fatkhullin et al., 2021), generalized from contractive to three-point compressors (Richtarik et al., 2022) and adaptive compressors (Makarenko et al., 2022), modified from dual (gradient) to primal (model) compression (Gruntkowska et al., 2022) and from centralized to decentralized setting (Zhao et al., 2022). For further work, we refer to (Wang et al., 2022; Dorfman et al., 2023; Islamov et al., 2022).

### Key issue: error feedback has an unhealthy appetite for samples!

Unfortunately, the current theory of EF21 with _stochastic gradients_ has weak sample complexity guarantees. In particular, Fatkhullin et al. (2021) extended the EF21-GD method, which is the basic variant of EF21 using full gradient at the clients, to EF21-SGD, which uses a "large minibatch" of stochastic gradients instead. They obtained \(\mathcal{O}(\frac{1}{\alpha\varepsilon^{2}}+\frac{\sigma^{2}}{\alpha^{2} \varepsilon^{4}})\) sample complexity for their method. Later, Zhao et al. (2022) improved this result slightly7 to \(\mathcal{O}(\frac{1}{\alpha\varepsilon^{2}}+\frac{\sigma^{2}}{\alpha^{2} \varepsilon^{4}})\), shaving off one \(\alpha\) in the stochastic term. However, it is easy to notice several issues in these results, which generally feature the fundamental challenge of combining biased gradient methods with stochastic gradients.

Footnote 7: The result was obtained under a more general setting of decentralized optimization over a network.

\(\bullet\)**Mega-batches.** These works require all clients to sample "mega-batches" of stochastic gradients/datapoints in each iteration, of order \(\mathcal{O}(\varepsilon^{-2}),\) in order to control the variance coming from stochastic gradients. In Figure 1, we find that, in fact, a batch-free (i.e., with mini-batch size \(B=1\))

Figure 1: Divergence of EF21-SGD on the quadratic function \(f(x)=\frac{1}{2}\left\|x\right\|^{2},\,x\in\mathbb{R}^{2}\), using the Top1 compressor. See the proof of Theorem 1 for details on the construction of the noise \(\xi\); we use \(\sigma=1,B=1\). The starting point is \(x^{0}=(0,-\,0.01)^{\top}\). Unlike EF21-SGD, our method EF21-SGD does not suffer from divergence and is stable near optimum. Figure 0(b) shows that when increasing the number of nodes \(n\), EF21-SGD applied with \(B=1\) does not improve, and, moreover, diverges from the optimum even faster. All experiments use constant parameters \(\gamma=\eta=\nicefrac{{0.1}}{{\sqrt{\tau}}}=10^{-3}\); see Figure 4 for diminishing parameters. Each method is run \(10\) times and the plot shows the median performance alongside the \(25\%\) and \(75\%\) quantiles.

version of EF21-SGD diverges even on a very simple quadratic function. We also observe a similar behavior when a small batch \(B>1\) is applied. This implies that there is a fundamental flaw in the EF21-SGD method itself, rather "just" a problem of the theoretical analysis. While mega-batch methods are common in optimization literature, smaller batches are often preferred whenever they "work". For example, the time/cost required to obtain such a large number of samples at each iteration might be unreasonably large compared to the communication time, which is already reduced using compression. Moreover, when dealing with medical data, large batches might simply be unavailable (Rieke et al., 2020). In certain applications, such as federated reinforcement learning (RL) or multi-agent RL, it is often intractable to sample more than one trajectory of the environment in order to form a gradient estimator (Mitra et al., 2023; Doan et al., 2019; Jin et al., 2022; Khodadadian et al., 2022). Further, a method using a mega-batch at each iteration effectively follows the gradient descent (GD) dynamics instead of the dynamics of (mini-batch) SGD, which may hinder the training and generalization performance of such algorithms since it is both empirically (Keskar et al., 2017; Kleinberg et al., 2018) and theoretically (Kale et al., 2021) observed that mini-batch SGD is superior to mega-batch SGD or GD in a number of machine learning tasks.

\(\bullet\)**Dependence on \(\alpha\).** The total sample complexity results derived by Fatkhullin et al. (2021); Zhao et al. (2022) suffer from poor dependence on the contraction parameter \(\alpha\). Typically, EF methods are used with the Top\(K\) sparsifier, which only communicates \(K\) largest entries in magnitude. In this case, \(\alpha=\nicefrac{{K}}{{d}}\), and the stochastic part of sample complexity scales quadratically with dimension.

\(\bullet\)**No improvement with \(n\).** The stochastic term in the sample complexity of EF21-SGD does _not_ improve when increasing the number of nodes. However, the opposite behavior is typically desired, and is present in several latest non-EF methods based on _unbiased_ compressors, such as MARINA (Gorbunov et al., 2021) and DASHA(Tyurin and Richtarik, 2022). We are not aware of any distributed algorithms utilizing the Top\(K\) compressor achieving linear speedup in \(n\) in the stochastic term without relying on restrictive BG or BGS assumptions.

These observations motivate our work with the following central questions:

_Can we design a batch-free distributed SGD method utilizing contractive communication compression (such as Top\(K\)) without relying on restrictive BG/BGS assumptions? Is it possible to improve over the current state-of-the-art \(\mathcal{O}\left(\alpha^{-1}\varepsilon^{-2}+\sigma^{2}\alpha^{-2}\varepsilon^ {-4}\right)\) sample complexity under the standard smoothness and bounded variance assumptions?_

We answer both questions in the affirmative by incorporating a momentum update into EF21-SGD.

### Mysterious effectiveness of momentum in nonconvex optimization

An immensely popular modification of SGD (and its distributed variants) is the use of _momentum_. This technique, initially inspired by the developments in convex optimization (Polyak, 1964), is often applied in machine learning for stabilizing convergence and speeding up the training. In particular, momentum is an important part of an immensely popular and empirically successful line of adaptive methods for deep learning, including ADAM(Kingma and Ba, 2015) and a plethora of variants. The classical SGD method with Polyak (i.e., heavy ball) momentum (SGD) reads:

\[x^{t+1}=x^{t}-\gamma v^{t},\qquad v^{t+1}=(1-\eta)v^{t}+\eta\nabla f(x^{t+1}, \xi^{t+1}),\] (3)

where \(\gamma>0\) is a learning rate and \(\eta>0\) is the momentum parameter.

We provide a concise walk through the key theoretical developments in the analysis of SGDM in stochastic nonconvex optimization in Appendix A; and only mention the most relevant works here. The most closely related works to ours are (Mishchenko et al., 2019), (Xie et al., 2020), and (Fatkhullin et al., 2021), which analyze momentum together with communication compression. The analysis in (Mishchenko et al., 2019; Xie et al., 2020) requires BG/BGS assumption, and does not provide any theoretical improvement over the variants without momentum. Finally, the analysis of Fatkhullin et al. (2021) is only established for deterministic case, and it is unclear if its extension to stochastic case can bring any convergence improvement over EF21-SGD. Recently, several other works attempt to explain the benefit of momentum (Plattner, 2022); some consider structured nonconvex problems (Wang and Abernethy, 2021), and others focus on generalization (Jelassi and Li, 2022).

[MISSING_PAGE_FAIL:5]

_i.e., \(\left\|\nabla f_{i}(x)-\nabla f_{i}(y)\right\|\leq L_{i}\left\|x-y\right\|\) for all \(i\in[n],\,x,y\in\mathbb{R}^{d}\). We denote \(\widetilde{L}^{2}:=\frac{1}{n}\sum_{i=1}^{n}L_{i}^{2}\). Moreover, we assume that \(f\) is lower bounded, i.e., \(f^{*}:=\inf_{x\in\mathbb{R}^{d}}f(x)>-\infty\)._

**Assumption 2** (Bounded variance (BV)).: _There exists \(\sigma>0\) such that_

\[\mathbb{E}\left[\left\|\nabla f_{i}(x,\xi_{i})-\nabla f_{i}(x)\right\|^{2} \right]\leq\sigma^{2},\qquad\forall x\in\mathbb{R}^{d},\qquad\forall i\in[n],\] (4)

_where \(\xi_{i}\sim\mathcal{D}_{i}\) are i.i.d. random samples for each \(i\in[n]\)._

### A deeper dive into the issues \(\mathtt{EF21}\) has with stochastic gradients

As remarked before, the current analysis of \(\mathtt{EF21}\) in the stochastic setting requires each client to sample a mega-batch in each iteration, and it is not clear how to avoid this. In order to understand this phenomenon, we propose to step back and examine an "idealized" version of \(\mathtt{EF21}\)-\(\mathtt{SGD}\), which we call \(\mathtt{EF21}\)-\(\mathtt{SGD}\)-ideal, defined by the update rules (5a) + (5aa):

\[x^{t+1} =x^{t}-\gamma g^{t},\quad g^{t} =\tfrac{1}{n}\sum_{i=1}^{n}g_{i}^{t}\] (5a) \[\mathtt{EF21}\)-\(\mathtt{SGD}\)-ideal: \[g_{i}^{t+1} =\nabla f_{i}(x^{t+1}) +\mathcal{C}\left(\nabla f_{i}(x^{t+1},\xi_{i}^{t+1})-\nabla f_{i }(x^{t+1})\right),\] (5aa) \[\mathtt{EF21}\)-\(\mathtt{SGD}\): \[g_{i}^{t+1} =\qquad g_{i}^{t} +\mathcal{C}\left(\nabla f_{i}(x^{t+1},\xi_{i}^{t+1})-\qquad g_ {i}^{t}\right).\] (5ab)

Compared to \(\mathtt{EF21}\)-\(\mathtt{SGD}\), given by (5a) + (5ab), we replace the previous state \(g_{i}^{t}\) by the _exact gradient_ at the current iteration. Since \(\mathtt{EF21}\)-\(\mathtt{SGD}\) heavily relies on the approximation \(g_{i}^{t}\approx\nabla f_{i}(x^{t+1})\), and according to the proof of convergence of \(\mathtt{EF21}\)-\(\mathtt{SGD}\), such discrepancy tends to zero as \(t\to\infty\), this change can only improve the method. While we admit this is a conceptual algorithm only (it does not lead to any communication or sample complexity reduction in practice)8, it serves us well to illustrate the drawbacks of \(\mathtt{EF21}\)-\(\mathtt{SGD}\). We now establish the following negative result for \(\mathtt{EF21}\)-\(\mathtt{SGD}\)-ideal.

Footnote 8: This is because full gradients would need to be computed and communicated for its implementation. Notice also that if \(\sigma=0\), this method becomes the exact distributed gradient descent.

**Theorem 1**.: _Let \(L\), \(\sigma>0\), \(0<\gamma\leq\nicefrac{{1}}{{L}}\) and \(n=1\). There exists a convex, \(L\)-smooth function \(f:\mathbb{R}^{2}\to\mathbb{R}\), a contractive compressor \(\mathcal{C}(\cdot)\) satisfying Definition 1, and an unbiased stochastic gradient with bounded variance \(\sigma^{2}\) such that if the method \(\mathtt{EF21}\)-\(\mathtt{SGD}\)-\(\mathtt{ideal}\) ((5a) + (5aa)) is run with step-size \(\gamma\), then for all \(T\geq 0\) and for all \(x^{0}\in\{(0,x^{0}_{(2)})^{\top}\in\mathbb{R}^{2}\,|\,x^{0}_{(2)}<0\},\) we have_

\[\mathbb{E}\left[\left\|\nabla f(x^{T})\right\|^{2}\right]\geq\tfrac{1}{60}\min \left\{\sigma^{2},\left\|\nabla f(x^{0})\right\|^{2}\right\}.\]

_Fix \(0<\varepsilon\leq\nicefrac{{L}}{{\sqrt{60}}}\) and \(x^{0}=(0,-1)^{\top}\). Additionally assume that \(n\geq 1\) and the variance of unbiased stochastic gradient is controlled by \(\nicefrac{{\sigma^{2}}}{{B}}\) for some \(B\geq 1\). If \(B<\frac{\sigma^{2}}{60\varepsilon^{2}}\), then we have \(\mathbb{E}\left[\left\|\nabla f(x^{T})\right\|\right]>\varepsilon\) for all \(T\geq 0\)._

The above theorem implies that the method (5a), (5aa), does not converge with small batch-size (e.g., equal to one) for any fixed step-size choice.9 Moreover, in distributed setting with \(n\) nodes, a mini-batch of order \(B=\Omega\left(\nicefrac{{\sigma^{2}}}{{\varepsilon^{2}}}\right)\) is required for convergence. Notice that this batch-size is independent of \(n\), which further implies that a linear speedup in the number of nodes \(n\) cannot be achieved for this method. While we only prove these negative results for an "idealized" version of \(\mathtt{EF21}\)-\(\mathtt{SGD}\) rather than for the method itself, in Figures 0(a) and 3(a), we empirically verify that \(\mathtt{EF21}\)-\(\mathtt{SGD}\) also suffers from a similar divergence on the same problem instance provided in the proof of Theorem 1. Additionally, Figures 0(b) and 3(b) illustrate that the situation does not improve for \(\mathtt{EF21}\)-\(\mathtt{SGD}\) when increasing \(n\).

Footnote 9: In fact, the example can be easily extended to the case of polynomially decaying step-size.

### Momentum for avoiding mega-batches

Let us now focus on the single node setting10 and try to fix the divergence issue shown above. As we can learn from Theorem 1, the key reason for non-convergence of \(\mathtt{EF21}\)-\(\mathtt{SGD}\) is that even if the state vector \(g^{t}\) sufficiently approximates the current gradient, i.e., \(g^{t}\approx\nabla f(x^{t+1})\), the design of this method cannot guarantee that the quantity \(\left\|g^{t}-\nabla f(x^{t})\right\|^{2}\approx\left\|\mathcal{C}\left(\nabla f(x^{ t},\xi^{t})-\nabla f(x^{t})\right)\right\|^{2}\) is small enough. Indeed, the last term above can be bounded by \(2(2-\alpha)\sigma^{2}\) in expectation, but it is not sufficient as formally illustrated in Theorem 1. To fix this problem, we propose to modify our "idealized" EF21-SGD-ideal method so that the compressed difference can be controlled and made arbitrarily small, which leads us to another (more advanced) conceptual algorithm,

\[\begin{split}\texttt{EF21-SGDM-ideal:}& v^{t+1}=\nabla f(x^{t+1})+\eta(\nabla f(x^{t+1},\xi^{t+1})-\nabla f (x^{t+1})),\\ & g^{t+1}=\nabla f(x^{t+1})+\mathcal{C}\left(v^{t+1}-\nabla f(x^ {t+1})\right).\end{split}\] (6)

In this method, instead of using \(v^{t+1}=\nabla f(x^{t+1},\xi^{t+1})\) as in EF21-SGD-ideal, we introduce a correction, which allows to control variance of the difference \(\nabla f(x^{t+1},\xi^{t+1})-\nabla f(x^{t+1})\). This allows us to derive the following convergence result. Let \(\delta_{0}:=f(x^{0})-f^{*}\).

**Proposition 1**.: _Let Assumptions 1, 2 hold, and let \(\mathcal{C}\) satisfy Definition 1. Let \(g^{0}=0\) and the step-size in method (5a), (6) be set as \(\gamma\leq\nicefrac{{1}}{{L}}\). Let \(\hat{x}^{T}\) be sampled uniformly at random from the iterates of the method. Then for any \(\eta>0\) after \(T\) iterations, we have \(\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right]\leq\frac{2 \delta_{0}}{\gamma T}+4\eta^{2}\sigma^{2}\)._

Notice that if \(\eta=1\), then algorithm EF21-SGD-ideal (5a), (6) reduces to EF21-SGD-ideal method (5a), (5aa), and this result shows that the lower bound for the batch-size established in Theorem 1 is tight, i.e., \(B=\Theta(\nicefrac{{\sigma^{2}}}{{\varepsilon^{2}}})\) is necessary and sufficient11 for convergence. For \(\eta<1\), the above theorem suggests that using a small enough parameter \(\eta\), the variance term can be completely eliminated. This observation motivates us to design a practical variant of this method. Similarly to the design of EF21 mechanism (from EF21-SGD-ideal), we propose to do this by replacing the exact gradients \(\nabla f(x^{t+1})\) by state vectors \(v^{t}\) and \(g^{t}\) as follows:

Footnote 11: This follows by replacing \(\sigma^{2}\) in the batch free algorithm by \(\nicefrac{{\sigma^{2}}}{{\beta}}\) if the batch-size of size \(B>1\) is used.

\[\begin{split} v^{t+1}&=v^{t}+\eta(\nabla f(x^{t+1},\xi^{t+1})-v^{t}),\\ g^{t+1}&=g^{t}+\mathcal{C}\left(v^{t+1}-g^{t}\right) \end{split}\] (7)

**Theorem 2**.: _Let Assumptions 1, 2 hold, and let \(\mathcal{C}\) satisfy Definition 1. Let method (5a), (7) be run with \(g^{0}=v^{0}=\nabla f(x^{0})\), and \(\hat{x}^{T}\) be sampled uniformly at random from the iterates of the method. Then for all \(\eta\in(0,1]\) with \(\gamma\leq\gamma_{0}=\min\left\{\frac{\alpha}{20L},\frac{\eta}{TL}\right\},\) we have \(\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right]\leq\mathcal{O} (\frac{\delta_{0}}{\gamma T}+\eta\sigma^{2})\). The choice \(\eta=\min\left\{1,\left(\frac{L\delta_{0}}{\sigma^{2}T}\right)^{1/2}\right\}\), \(\gamma=\gamma_{0}\) results in \(\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right]\leq\mathcal{O} \big{(}\frac{L\delta_{0}}{\alpha T}+\big{(}\frac{L\delta_{0}\sigma^{2}}{T} \big{)}^{1/2}\big{)}\)._

Compared to Proposition 1, where \(\eta\) can be made arbitrarily small, Theorem 2 suggests that there is a trade-off for the choice of \(\eta\in(0,1]\) in algorithm (5a), (7). The above theorem implies that in single node setting EF21-SGD has \(\mathcal{O}(\frac{L}{\alpha\sigma^{2}}+\frac{L\sigma^{2}}{\varepsilon^{4}})\) sample complexity. For \(\alpha=1\), this result matches with the sample complexity of SGD and is known to be unimprovable under Assumptions 1 and 2 (Arjevani et al., 2019). Moreover, when \(\alpha=1\), our sample complexity matches with previous analysis of momentum methods in (Liu et al., 2020) and (Defazio, 2021). However, even in this single node (\(n=1\)), uncompressed (\(\alpha=1\)) setting our analysis is different from the previous work, in particular, our choice of momentum parameter and the Lyapunov function are different, see Appendix A and J. For \(\alpha<1\), the above result matches with sample complexity of EF14-SGD (single node setting) (Stich and Karimireddy, 2019), which was recently shown to be optimal (Huang et al., 2022) for biased compressors satisfying Definition 1. However, notice that the extension of the analysis by Stich and Karimireddy (2019) for EF14-SGD to distributed setting meets additional challenges and it is unclear whether it is possible without imposing additional BG or BGS assumptions as in (Koloskova et al., 2020). We revisit this analysis in Appendix K to showcase the difficulty of removing BG/BGS. In the following we will demonstrate the benefit of our EF21-SGD method by extending it to distributed setting without imposing any additional assumptions.

### Distributed stochastic error feedback with momentum

Now we are ready to present a distributed variant of EF21-SGDM, see Algorithm 1. Letting \(\delta_{t}:=f(x^{t})-f^{*}\), our convergence analysis of this method relies on the monotonicity of the following Lyapunov function:

\[\Lambda_{t}:=\delta_{t}+\tfrac{\gamma}{\alpha n}\sum\limits_{i=1}^{n}\left\|g_{ i}^{t}-v_{i}^{t}\right\|^{2}+\tfrac{\gamma n}{\alpha^{2}n}\sum\limits_{i=1}^{n} \left\|v_{i}^{t}-\nabla f_{i}(x^{t})\right\|^{2}+\tfrac{\gamma}{\eta}\left\| \sum\limits_{i=1}^{n}(v_{i}^{t}-\nabla f_{i}(x^{t}))\right\|^{2}.\] (8)\(\bullet\) **Convergence of \(\mathsf{EF21\text{-}SGDM}\) with contractive compressors.** We obtain the following result:

**Theorem 3**.: _Let Assumptions 1 and 2 hold. Let \(\hat{x}^{T}\) be sampled uniformly at random from the \(T\) iterates of the method. Let \(\mathsf{EF21\text{-}SGDM}\) (Algorithm 1) be run with a contractive compressor. For all \(\eta\in(0,1]\) and \(B_{\text{init}}\geq 1\), with \(\gamma\leq\min\left\{\frac{\alpha}{20C},\frac{\eta}{\gamma L}\right\},\) we have_

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right]\leq\mathcal{O} \left(\frac{\Lambda_{0}}{\gamma T}+\frac{\eta^{3}\sigma^{2}}{\alpha^{2}}+\frac {\eta^{2}\sigma^{2}}{\alpha}+\frac{\eta\sigma^{2}}{n}\right),\] (9)

_where \(\Lambda_{0}\) is given by (8). Choosing the batch size \(B_{\text{init}}=\left\lceil\frac{\sigma^{2}}{L\delta_{0}}\right\rceil\), and stepsize \(\gamma=\min\left\{\frac{\alpha}{20L},\frac{\eta}{\gamma L}\right\}\), and momentum \(\eta=\min\left\{1,\left(\frac{L\delta_{0}\sigma^{2}}{\sigma^{2}T}\right)^{1/4},\left(\frac{L\delta_{0}\alpha}{\sigma^{2}T}\right)^{1/3},\left(\frac{L\delta _{0}n}{\sigma^{2}T}\right)^{1/2},\frac{\alpha\sqrt{L\delta_{0}B_{\text{init}}} }{\sigma}\right\},\)12 we get_

Footnote 12: In Appendix J, we show how to deal with time varying \(\gamma_{t}\) and \(\eta_{t}\).

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] \leq \mathcal{O}\left(\frac{\bar{L}\delta_{0}}{\alpha T}+\left(\frac{L \delta_{0}\sigma^{2/3}}{\alpha^{2/3}T}\right)^{3/4}+\left(\frac{L\delta_{0} \sigma}{\sqrt{\alpha T}}\right)^{2/3}+\left(\frac{L\delta_{0}\sigma^{2}}{nT} \right)^{1/2}\right).\]

**Remark 1**.: _Note that using large initial batch size \(B_{\text{init}}>1\) is not necessary for convergence of \(\mathsf{EF21\text{-}SGDM}\). If we set \(B_{\text{init}}=1\), the above theorem still holds by replacing \(\delta_{0}\) with \(\Lambda_{0}\)._

**Remark 2**.: _In the single node setting (\(n=1\)), the above result recovers the statement of Theorem 2 (with the same choice of parameters) since by Young's inequality \(\left(\frac{L\delta_{0}\sigma^{2/3}}{\alpha^{2/3}T}\right)^{3/4}\leq\frac{1}{ 2}\frac{L\delta_{0}}{\alpha T}+\frac{1}{2}\left(\frac{L\delta_{0}\sigma^{2}} {T}\right)^{1/2}\), \(\left(\frac{L\delta_{0}}{\sqrt{\alpha T}}\right)^{2/3}\leq\frac{1}{3}\frac{L \delta_{0}}{\alpha T}+\frac{2}{3}\left(\frac{L\delta_{0}\sigma^{2}}{T}\right) ^{1/2}\), and \(\widetilde{L}=L\)._

\(\bullet\) **Recovering previous rates in case of full gradients.** Compared to the iteration complexity \(\mathcal{O}(\frac{L_{\text{init}}G}{\alpha c^{3}})\) of \(\mathsf{EF14}\)[10], our result, summarized in

**Corollary 1**.: _If \(\sigma=0\), then \(\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|\right]\leq\varepsilon\) after \(T=\mathcal{O}\left(\frac{\bar{L}}{\alpha c^{2}}\right)\) iterations._

is better by an order of magnitude, and does not require the BG assumption. The result of Corollary 1 is the same as for \(\mathsf{EF21}\) method [16], and \(\mathsf{EF21\text{-}HB}\) method [17]. Notice, however, that even in this deterministic setting (\(\sigma=0\)) \(\mathsf{EF21\text{-}SGDM}\) method is different from \(\mathsf{EF21}\) and \(\mathsf{EF21\text{-}HB}\): while the original \(\mathsf{EF21}\) does not use momentum, \(\mathsf{EF21\text{-}HB}\) method incorporates momentum on the server side to update \(x^{t}\), which is different from our Algorithm 1, where momentum is applied by each node. This iteration complexity \(\mathcal{O}\left(\frac{1}{\alpha c^{2}}\right)\) is optimal in both \(\alpha\) and \(\varepsilon\). The matching lower bound was recently established by Huang et al. [2022] for smooth nonconvex optimization in the class of centralized, zero-respecting algorithms with contractive compressors.

\(\bullet\) **Comparison to previous work.** Our sample complexity13 in

Footnote 13: Note that the initial batch size contributes to the sample complexity only an additive constant independent of \(\varepsilon\). Moreover, \(B_{\text{init}}=\left\lceil\frac{\sigma^{2}}{L\delta_{0}}\right\rceil\leq \left\lceil\frac{2\sigma^{2}}{\varepsilon^{2}}\right\rceil\) since, otherwise, \(\left\|\nabla f(x^{0})\right\|^{2}\leq 2L\delta_{0}\leq\varepsilon^{2}\), and \(x^{0}\) is a solution. In the following, we ignore the dependece on \(B_{\text{init}}\) for a fair comparison with other works.

\(\bullet\) **Comparison to previous work.** Our sample complexity13 in

Footnote 13: Note that the initial batch size contributes to the sample complexity only an additive constant independent of \(\varepsilon\). Moreover, \(B_{\text{init}}=\left\lceil\frac{\sigma^{2}}{L\delta_{0}}\right\rceil\leq \left\lceil\frac{2\sigma^{2}}{\varepsilon^{2}}\right\rceil\) since, otherwise, \(\left\|\nabla f(x^{0})\right\|^{2}\leq 2L\delta_{0}\leq\varepsilon^{2}\), and \(x^{0}\) is a solution. In the following, we ignore the dependece on \(B_{\text{init}}\) for a fair comparison with other works.

\(\bullet\) **Comparison to previous work.** Our sample complexity13 in

Footnote 13: Note that the initial batch size contributes to the sample complexity only an additive constant independent of \(\varepsilon\). Moreover, \(B_{\text{init}}=\left\lceil\frac{\sigma^{2}}{L\delta_{0}}\right\rceil\leq \left\lceil\frac{2\sigma^{2}}{\varepsilon^{2}}\right\rceil\) since, otherwise, \(\left\|\nabla f(x^{0})\right\|^{2}\leq 2L\delta_{0}\leq\varepsilon^{2}\), and \(x^{0}\) is a solution. In the following, we ignore the dependece on \(B_{\text{init}}\) for a fair comparison with other works.

\(\bullet\) **Comparison to previous work.** Our sample complexity13 in

Footnote 13: Note that the initial batch size contributes to the sample complexity only an additive constant independent of \(\varepsilon\). Moreover, \(B_{\text{init}}=\left\lceil\frac{\sigma^{2}}{L\delta_{0}}\right\rceil\leq \left\lceil\frac{2\sigma^{2}}{\varepsilon^{2}}\right\rceil\) since, otherwise, \(\left\|\nabla f(x^{0})\right\|^{2}\leq 2L\delta_{0}\leq\varepsilon^{2}\), and \(x^{0}\) is a solution. In the following, we ignore the dependece on \(B_{\text{init}}\) for a fair comparison with other works.

\(\bullet\) **Comparison to previous work.** Our sample complexity13 in

**Corollary 2**.: \(\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|\right]\leq\varepsilon\) _after \(T=\mathcal{O}\left(\frac{\bar{L}}{\alpha\varepsilon^{2}}+\frac{L\sigma^{2/3}}{ \alpha^{2/3}c^{8/3}}+\frac{L\sigma}{\alpha^{1/2}c^{3}}+\frac{L\sigma^{2}}{n \varepsilon^{4}}\right)\) iterations._strictly improves over the complexity \(\mathcal{O}(\frac{GL_{\text{max}}}{\alpha\varepsilon^{2}}+\frac{L_{\text{max}} \sigma^{2}}{n\varepsilon^{4}})\) of EF14-SGD by Koloskova et al. (2020), even in case when \(G<+\infty\). Notice that it always holds that \(\sigma\leq G\). If we assume that \(G\approx\sigma\), our three first terms in the complexity improve the first term from Koloskova et al. (2020) by the factor of \(\nicefrac{{\varepsilon}}{{\sigma}}\), \((\nicefrac{{\varepsilon}}{{\alpha}}\!/\!\sigma)^{1/3}\), or \(\alpha^{1/2}\). Compared to the BEER algorithm of Zhao et al. (2022), with sample complexity \(\mathcal{O}(\frac{L_{\text{max}}}{\alpha\varepsilon}+\frac{L_{\text{max}} \sigma^{2}}{\alpha^{2}\varepsilon^{4}})\), the result of Corollary 2 is strictly better in terms of \(\alpha\), \(n\), and the smoothness constants.14 In addition, we remove the large batch requirement for convergence compared to (Fatkullin et al., 2021; Zhao et al., 2022). Moreover, notice that Corollary 2 implies that EF21-SGD achieves asymptotically optimal sample complexity \(\mathcal{O}(\frac{L\sigma^{2}}{n\varepsilon^{4}})\) in the regime \(\varepsilon\to 0\).

Footnote 14: \(L_{\text{max}}:=\max_{i\in[n]}L_{i}\). Notice that \(L\lesssim\tilde{L}\leq L_{\text{max}}\) and the inequalities are strict in heterogeneous setting.

### Further improvement using _double_ momentum!

Unfortunately, in the non-asymptotic regime, our sample complexity does not match with the lower bound in all problem parameters simultanously due to the middle term \(\frac{L\sigma^{2/3}}{\alpha^{2/3}\varepsilon^{8/3}}+\frac{L\sigma}{\alpha^{1/2 }\varepsilon^{3}}\), which can potentially dominate over \(\frac{L\sigma^{2}}{n\varepsilon^{4}}\) term for large enough \(n\) and \(\varepsilon\), and small enough \(\alpha\) and \(\sigma\). We propose a _double-momentum_ method, which can further improve the middle term in the sample complexity of EF21-SGD. We replace the momentum estimator \(v_{i}^{t}\) in line 6 of Algorithm 1 by the following two-step momentum update

\[\texttt{EF21-SGD2M:}\quad v_{i}^{t+1}=(1-\eta)v_{i}^{t}+\eta\nabla f_{i}(x^ {t+1},\xi_{i}^{t+1}),\quad u_{i}^{t+1}=(1-\eta)u_{i}^{t}+\eta v_{i}^{t+1}.\] (10)

We formally present this method in Algorithm 3 in Appendix G. Compared to EF21-SGDM (Algorithm 1), the only change is that instead of compressing \(v_{i}^{t}-g_{i}^{t}\), in EF21-SGD2M, we compress \(u_{i}^{t}-g_{i}^{t}\), where \(u_{i}\) is a two step (double) momentum estimator. The intuition behind this modification is that a double momentum estimator \(u_{i}^{t}\) has richer "memory" of the past gradients compared to \(v_{i}^{t}\). Notice that for each node, EF21-SGD2M requires to save \(3\) vectors (\(v_{i}^{t}\), \(u_{i}^{t}\), \(g_{i}^{t}\)) instead of \(2\) in EF21-SGDM (\(v_{i}^{t}\), \(g_{i}^{t}\)) and EF14-SGD (\(e_{i}^{t}\), \(g_{i}^{t}\)).15 When interacting with biased compression operator \(\mathcal{C}(\cdot)\), such effect becomes crucial in improving the sample complexity. For EF21-SGD2M, we derive

Footnote 15: See Appendix K for details on EF14-SGD. In contrast, EF21-SGD needs to save only one vector (\(g_{i}^{t}\)).

**Corollary 3**.: _Let \(v_{i}^{t}\) in Algorithm 1 be replaced by \(u_{i}^{t}\) given by (10) (Algorithm 3 in Appendix G). Then with appropriate choice of \(\gamma\) and \(\eta\) (given in Theorem 5), we have \(\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|\right]\leq\varepsilon\) after \(T=\mathcal{O}\left(\frac{\tilde{L}\delta_{9}}{\alpha\varepsilon^{2}}+\frac{L \delta_{9}\sigma^{2/3}}{\alpha^{7/3}\varepsilon^{8/3}}+\frac{L\delta_{9} \sigma^{2}}{n\varepsilon^{4}}\right)\) iterations._

## 4 Experiments

We consider a nonconvex logistic regression problem: \(f_{i}(x_{1},\ldots,x_{c})=-\frac{1}{m}\sum_{j=1}^{m}\log(\exp(a_{ij}^{\top}x_ {y_{ij}})/\sum_{y=1}^{c}\exp(a_{ij}^{\top}x_{y}))\) with a nonconvex regularizer \(h(x_{1},\ldots,x_{c})=\lambda\sum_{y=1}^{c}\sum_{k=1}^{l}[x_{y}]_{k}^{2}/(1+[x_ {y}]_{k}^{2})\) with \(\lambda=10^{-3},\) where \(x_{1},\ldots,x_{c}\in\mathbb{R}^{l}\), \([\cdot]_{k}\) is an indexing operation of a vector, \(c\geq 2\) is the number of classes, \(l\) is the number of features, \(m\) is the size of a dataset, \(a_{ij}\in\mathbb{R}^{l}\) and \(y_{ij}\in\{1,\ldots,c\}\) are features and labels. The datasets used are _MNIST_ (with \(l=784\), \(m=60\,000\), \(c=10\)) and _real-sim_ (with \(l=20\,958\), \(m=72\,309\), \(c=2\)) (LeCun et al., 2010; Chang and Lin, 2011). The dimension of the problem is \(d=(l+1)c\), i.e., \(d=7\,850\) for _MNIST_ and \(d=41\,918\) for _real-sim_. In each experiment, we show relations between the total number of transmitted coordinates and gradient/function values. The stochastic gradients in each algorithm are replaced by a mini-batch estimator \(\frac{1}{B}\sum_{j=1}^{B}\nabla f_{i}(x,\xi_{ij})\) with the same \(B\geq 1\) in each plot. Notice that all methods (except for NEOLITHIC)16 calculate the same number of samples at each communication round, thus the dependence on the number of samples used will be qualitatively the same. In all algorithms, the step sizes are fine-tuned from a set \(\{2^{k}\,|\,k\in[-20,20]\}\) and the Top\(K\) compressor is used to compress information from the nodes to the master. For EF21-SGDM, we fix momentum parameter \(\eta=0.1\) in all experiments. Prior to that, we tuned \(\eta\in\{0.01,0.1\}\) on the independent dataset _w8a_ (with \(l=300\), \(m=49\,749\), \(c=2\)). We omit BEER method from the plots since it showed worse performance than EF21-SGD in all runs.

Footnote 16: For NEOLITHIC, we use the parameter \(R=\lceil d/K\rceil\) following the requirement in their Theorem 3. Experiments in Huang et al. (2022) use a heuristic choice \(R=4\), and thus can show faster convergence.

### Experiment 1: increasing batch-size

In this experiment, we use _MNIST_ dataset and fix the number of transmitted coordinates to \(K=10\) (thus \(\alpha\geq\nicefrac{{K}}{{d}}\approx 10^{-3}\)), and set \(n=10\). Figure 2 shows convergence plots for \(B\in\{1\),\(32\),\(128\}\). EF21-SGDM and its double momentum version EF21-SGD2M have fast convergence and show a significant improvement when increasing batch-size compared to EF14-SGD. In contrast, EF21-SGD suffers from poor performance for small \(B\), which confirms our observations in previous sections. NEOLITHHC has order times slower convergence rate to the fact that it sends \(\left\lceil\nicefrac{{d}}{{K}}\right\rceil\) compressed vectors in each iteration, while other methods send only one.

### Experiment 2: improving convergence with \(n\)

This experiment uses _real-sim_ dataset, \(K=100\) (thus \(\alpha\geq\nicefrac{{K}}{{d}}\approx 2\cdot 10^{-3}\)), and with \(B=128\ll m\). We vary the number of nodes within \(n\in\{1,10,100\}\), see Figure 3. In this case, EF21-SGDM and EF21-SGD2M have much faster convergence compared to other methods for all \(n\). Moreover, the proposed algorithms show a significant improvement when \(n\) increases. We also observe that on this task, EF21-SGD2M performs slightly worse than EF21-SGDM for \(n=10,100\), but it is still much faster than other other methods.

In Section C, we present extra simulations with different parameters for above experiments. Additionally, we inlclude experiemnts on simple quadratic problems and perform training of larger image recognition models. In all cases, EF21-SGDM and EF21-SGD2M outperform other algorithms.

## Acknowledgments and Disclosure of Funding

This work of I. Fatkhullin was supported by ETH AI Center doctoral fellowship. The work of P. Richtarik and A. Tyurin was supported by the KAUST Baseline Research Scheme (KAUST BRF) and the KAUST Extreme Computing Research Center (KAUST ECRC); P. Richtarik was also supported by the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence (SDAIA-KAUST AI).

Figure 3: Experiment on _real-sim_ dataset with batch-size \(B=128\), and Top\(100\) compressor.

Figure 2: Experiment on _MNIST_ dataset with \(n=10\), and Top\(10\) compressor.

## References

* Alistarh et al. (2017) Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-efficient SGD via gradient quantization and encoding. In _Advances in Neural Information Processing Systems_, pages 1709-1720, 2017.
* Alistarh et al. (2018) Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola Konstantinov, and Cedric Renggli. The convergence of sparsified gradient methods. In _Advances in Neural Information Processing Systems_, 2018.
* Arjevani et al. (2019) Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. _arXiv preprint arXiv:1912.02365_, 2019.
* Beznosikov et al. (2020) Aleksandr Beznosikov, Samuel Horvath, Peter Richtarik, and Mher Safaryan. On biased compression for distributed learning. _arXiv preprint arXiv:2002.12410_, 2020.
* Chang and Lin (2011) Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 2(3):1-27, 2011.
* Chen et al. (2022) Xin Chen, Niao He, Yifan Hu, and Zikun Ye. Efficient algorithms for minimizing compositions of convex functions and random functions and its applications in network revenue management. _arXiv preprint arXiv:2205.01774_, 2022.
* Cutkosky and Mehta (2020) Ashok Cutkosky and Harsh Mehta. Momentum improves normalized SGD. In _International Conference on Machine Learning_, page 2260-2268. PMLR, 2020.
* Cutkosky and Mehta (2021) Ashok Cutkosky and Harsh Mehta. High-probability bounds for non-convex stochastic optimization with heavy tails. pages 4883-4895, 2021.
* Cutkosky and Orabona (2019) Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex SGD. In _Advances in Neural Information Processing Systems_, 2019.
* Defazio (2021) Aaron Defazio. Momentum via primal averaging: Theoretical insights and learning rate schedules for non-convex optimization. _arXiv preprint arXiv:2010.00406_, 2021.
* Doan et al. (2019) Thinh Doan, Siva Maguluri, and Justin Romberg. Finite-time analysis of distributed TD(0) with linear function approximation on multi-agent reinforcement learning. In _International Conference on Machine Learning_, pages 1626-1635. PMLR, 2019.
* Dorfman et al. (2023) Ron Dorfman, Shay Vargaftik, Yaniv Ben-Itzhak, and Kfir Y. Levy. DoCoFL: Downlink compression for cross-device federated learning. _arXiv preprint arXiv:2302.00543_, 2023.
* Fatkhullin et al. (2021) Ilyas Fatkhullin, Igor Sokolov, Eduard Gorbunov, Zhize Li, and Peter Richtarik. EF21 with bells & whistles: Practical algorithmic extensions of modern error feedback. _arXiv preprint arXiv:2110.03294_, 2021.
* Gao et al. (2023) Juan Gao, Xin-Wei Liu, Yu-Hong Dai, Yakui Huang, and Junhua Gu. Distributed stochastic gradient tracking methods with momentum acceleration for non-convex optimization. _Computational Optimization and Applications_, 84(2):531-572, 2023.
* Ghadimi et al. (2015) Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence of the heavy-ball method for convex optimization. In _2015 European control conference (ECC)_, pages 310-315. IEEE, 2015.
* Gorbunov et al. (2020) Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richtarik. Linearly converging error compensated SGD. In _34th Conference on Neural Information Processing Systems_, 2020.
* Gorbunov et al. (2021) Eduard Gorbunov, Konstantin Burlachenko, Zhize Li, and Peter Richtarik. MARINA: Faster non-convex distributed learning with compression. In _International Conference on Machine Learning_, pages 3788-3798. PMLR, 2021.
* Gruntkowska et al. (2022) Kaja Gruntkowska, Alexander Tyurin, and Peter Richtarik. Ef21-p and friends: Improved theoretical communication complexity for distributed optimization with bidirectional compression. _arXiv preprint arXiv:2209.15218_, Sep 2022.
* Grundt et al. (2020)Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In _International Conference on Machine Learning_, page 1737-1746, Feb 2015.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2016.
* Horvath et al. (2019a) Samuel Horvath, Chen-Yu Ho, L'udovit Horvath, Atal Narayan Sahu, Marco Canini, and Peter Richtarik. Natural compression for distributed deep learning. _arXiv preprint arXiv:1905.10988_, 2019a.
* Horvath et al. (2019b) Samuel Horvath, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtarik. Stochastic distributed learning with gradient quantization and variance reduction. _arXiv preprint arXiv:1904.05115_, 2019b.
* Huang et al. (2022) Xinmeng Huang, Yiming Chen, Wotao Yin, and Kun Yuan. Lower bounds and nearly optimal algorithms in distributed learning with communication compression. In _Advances in Neural Information Processing Systems_, 2022.
* Islamov et al. (2022) Rustem Islamov, Xun Qian, Slavomir Hanzely, Mher Safaryan, and Peter Richtarik. Distributed newton-type methods with communication compression and bernoulli aggregation. _arXiv preprint arXiv:2206.03588_, 2022.
* Jelassi and Li (2022) Samy Jelassi and Yuanzhi Li. Towards understanding how momentum improves generalization in deep learning. In _International Conference on Machine Learning_, pages 9965-10040. PMLR, 2022.
* Jin et al. (2022) Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Federated reinforcement learning with environment heterogeneity. In _International Conference on Artificial Intelligence and Statistics_, volume 151 of _Proceedings of Machine Learning Research_, pages 18-37. PMLR, 28-30 Mar 2022.
* Kairouz (2019) Peter et al Kairouz. Advances and open problems in federated learning. _arXiv preprint arXiv:1912.04977_, 2019.
* Kale et al. (2021) Satyen Kale, Ayush Sekhari, and Karthik Sridharan. Sgd: The role of implicit regularization, batch-size and multiple-epochs. _arXiv preprint arXiv:2107.05074_, 2021.
* Karimireddy et al. (2019) Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback fixes SignSGD and other gradient compression schemes. In _International Conference on Machine Learning_, 2019.
* Karimireddy et al. (2021) Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in federated learning. _arXiv preprint arXiv:2008.03606_, 2021.
* Keskar et al. (2017) Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2017.
* Khodadadian et al. (2022) Sajad Khodadadian, Pranay Sharma, Gauri Joshi, and Siva Theja Maguluri. Federated reinforcement learning: Linear speedup under markovian sampling. In _International Conference on Machine Learning_, page 10997-11057. PMLR, Jun 2022.
* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR (Poster)_, 2015.
* Kleinberg et al. (2018) Bobby Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does sgd escape local minima? In _International Conference on Machine Learning_, page 2698-2707. PMLR, 2018.
* Koloskova et al. (2020) Anastasia Koloskova, Tao Lin, S. Stich, and Martin Jaggi. Decentralized deep learning with arbitrary communication compression. In _International Conference on Learning Representations_, 2020.
* Krizhevsky et al. (2014)Jakub Konecny, H. Brendan McMahan, Felix Yu, Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: strategies for improving communication efficiency. In _NIPS Private Multi-Party Machine Learning Workshop_, 2016.
* Kovalev et al. (2021) Dmitry Kovalev, Anastasia Koloskova, Martin Jaggi, Peter Richtarik, and Sebastian Stich. A linearly convergent algorithm for decentralized optimization: Sending less bits for free! In _The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)_, 2021.
* Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, University of Toronto, Toronto, 2009.
* LeCun et al. (2010) Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2, 2010.
* Li and Orabona (2020) Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive SGD with momentum. _arXiv preprint arXiv:2007.14294_, 2020.
* Li et al. (2022a) Xiaoyu Li, Mingrui Liu, and Francesco Orabona. On the last iterate convergence of momentum methods. In _International Conference on Algorithmic Learning Theory_, pages 699-717. PMLR, 2022a.
* Li et al. (2022b) Xiaoyun Li, Belhal Karimi, and Ping Li. On distributed adaptive optimization with gradient compression. In _International Conference on Learning Representations_, 2022b.
* Li et al. (2020) Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richtarik. Acceleration for compressed gradient descent in distributed and federated optimization. In _International Conference on Machine Learning_, pages 5895-5904. PMLR, 2020.
* Li et al. (2021) Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtarik. PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In _International Conference on Machine Learning_, pages 6286-6295. PMLR, 2021.
* Lin et al. (2018) Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. In _International Conference on Learning Representations_, 2018.
* Liu et al. (2020) Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with momentum. In _Advances in Neural Information Processing Systems_, 2020.
* Makarenko et al. (2022) Maksim Makarenko, Elnur Gasanov, Rustem Islamov, Abdurakhmon Sadiev, and Peter Richtarik. Adaptive compression for communication-efficient distributed training. _arXiv preprint arXiv:2211.00188_, Oct 2022.
* Mishchenko et al. (2019) Konstantin Mishchenko, Eduard Gorbunov, Martin Takac, and Peter Richtarik. Distributed learning with compressed gradient differences. _arXiv preprint arXiv:1901.09269_, 2019.
* Mishchenko et al. (2022) Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtarik. ProxSkip: Yes! Local gradient steps provably lead to communication acceleration! Finally! In _International Conference on Machine Learning_, pages 15750-15769. PMLR, 2022.
* Mitra et al. (2023) Aritra Mitra, George J. Pappas, and Hamed Hassani. Temporal difference learning with compressed updates: Error-feedback meets reinforcement learning. _arXiv preprint arXiv:2301.00944_, 2023.
* Philippenko and Dieuleveut (2020) Constantin Philippenko and Aymerie Dieuleveut. Bidirectional compression in heterogeneous settings for distributed or federated learning with partial participation: tight convergence guarantees. _arXiv preprint arXiv:2006.14591_, 2020.
* Plattner (2022) Maximilian Plattner. On SGD with momentum. _Master's Thesis_, 2022.
* Polyak (1964) Boris T Polyak. Some methods of speeding up the convergence of iteration methods. _Ussr computational mathematics and mathematical physics_, 4(5):1-17, 1964.
* Pappas et al. (2019)Xun Qian, Peter Richtarik, and Tong Zhang. Error compensated distributed SGD can be accelerated. _arXiv preprint arXiv:2010.00091_, 2020.
* Richtarik et al. (2021) Peter Richtarik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A new, simpler, theoretically better, and practically faster error feedback. In _Advances in Neural Information Processing Systems_, 2021.
* Richtarik et al. (2022) Peter Richtarik, Igor Sokolov, Ilyas Fatkhullin, Elnur Gasanov, Zhize Li, and Eduard Gorbunov. 3PC: Three point compressors for communication-efficient distributed training and a better theory for lazy aggregation. _arXiv preprint arXiv:2202.00998_, 2022.
* Rieke et al. (2020) Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R. Roth, Shadi Albarqouni, Spyridon Bakas, Mathieu N. Galtier, Bennett A. Landman, Klaus Maier-Hein, Sebastien Ourselin, Micah Sheller, Ronald M. Summers, Andrew Trask, Daguang Xu, Maximilian Baust, and M. Jorge Cardoso. The future of digital health with federated learning. _npj Digital Medicine_, 3(11):1-7, 2020.
* Safaryan et al. (2021) Mher Safaryan, Egor Shulgin, and Peter Richtarik. Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor. _Information and Inference: A Journal of the IMA_, 2021.
* Safaryan et al. (2022) Mher Safaryan, Rustem Islamov, Xun Qian, and Peter Richtarik. FedNL: Making Newton-type methods applicable to federated learning. In _Internatioanl Conference on Machine Learning_, 2022.
* Sahu et al. (2021) Atal Sahu, Aritra Dutta, Ahmed M. Abdelmoniem, Trambak Banerjee, Marco Canini, and Panos Kalnis. Rethinking gradient sparsification as total error minimization. In _Advances in Neural Information Processing Systems_, page 8133-8146, 2021.
* Sapio et al. (2021) Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, and Masoud Moshref. Scaling distributed machine learning with in-network aggregation. In _18th USENIX Symposium on Networked Systems Design and Implementation_, page 785-808, 2021.
* Sebbouh et al. (2019) Othmane Sebbouh, Charles Dossal, and Aude Rondepierre. Nesterov's acceleration and Polyak's heavy ball method in continuous time: convergence rate analysis under geometric conditions and perturbations. _arXiv preprint arXiv:1907.02710_, 2019.
* Sebbouh et al. (2021) Othmane Sebbouh, Robert M Gower, and Aaron Defazio. Almost sure convergence rates for stochastic gradient descent and stochastic heavy ball. In _Conference on Learning Theory_, pages 3935-3971. PMLR, 2021.
* Seide et al. (2014) Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. In _Fifteenth Annual Conference of the International Speech Communication Association_, 2014.
* Singh et al. (2021) Navjot Singh, Deepesh Data, Jemin George, and Suhas Diggavi. Squarm-sgd: Communication-efficient momentum sgd for decentralized optimization. _IEEE Journal on Selected Areas in Information Theory_, 2(3):954-969, 2021.
* Stich and Karimireddy (2019) Sebastian Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for SGD with delayed gradients and compressed communication. _arXiv preprint arXiv:1909.05350_, 2019.
* Stich and Karimireddy (2021) Sebastian U. Stich and Sai Praneeth Karimireddy. The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication. _arXiv preprint arXiv:1909.05350_, 2021.
* Stich et al. (2018) Sebastian U. Stich, J.-B. Cordonnier, and Martin Jaggi. Sparsified SGD with memory. In _Advances in Neural Information Processing Systems_, 2018.
* Szlendak et al. (2021) Rafal Szlendak, Alexander Tyurin, and Peter Richtarik. Permutation compressors for provably faster distributed nonconvex optimization. _arXiv preprint arXiv:2110.03300_, 2021.
* Stich et al. (2020)Yuki Takezawa, Han Bao, Kenta Niwa, Ryoma Sato, and Makoto Yamada. Momentum tracking: Momentum acceleration for decentralized deep learning on heterogeneous data. _arXiv preprint arXiv:2209.15505_, 2022.
* Tang et al. (2020) Hanlin Tang, Xiangru Lian, Chen Yu, Tong Zhang, and Ji Liu. DoubleSqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression. In _International Conference on Machine Learning_, 2020.
* Tyurin and Richtarik (2022) Alexander Tyurin and Peter Richtarik. Dasha: Distributed nonconvex optimization with communication compression, optimal oracle complexity, and no client synchronization. _arXiv preprint arXiv:2202.01268_, 2022.
* Vogels et al. (2019) Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Powersgd: Practical low-rank gradient compression for distributed optimization. _Advances in Neural Information Processing Systems_, 2019.
* Wang and Abernethy (2021) Jun-Kun Wang and Jacob Abernethy. Quickly finding a benign region via heavy ball momentum in non-convex optimization. _arXiv preprint arXiv:2010.01449_, 2021.
* Wang et al. (2022) Yujia Wang, Lu Lin, and Jinghui Chen. Communication-compressed adaptive gradient method for distributed nonconvex optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 6292-6320. PMLR, 2022.
* Xiao and Yang (2022) Tiannan Xiao and Guoguo Yang. A convergence study of sgd-type methods for stochastic optimization. _arXiv preprint arXiv:2211.06197_, 2022.
* Xie et al. (2020) Cong Xie, Shuai Zheng, Oluwasanmi Koyejo, Indranil Gupta, Mu Li, and Haibin Lin. CSER: Communication-efficient SGD with error reset. In _Advances in Neural Information Processing Systems_, pages 12593-12603, 2020.
* Xu and Huang (2022) An Xu and Heng Huang. Detached error feedback for distributed sgd with random sparsification. In _International Conference on Machine Learning_, pages 24550-24575. PMLR, 2022.
* Yang et al. (2023) Junchi Yang, Xiang Li, Ilyas Fatkhullin, and Niao He. Two sides of one coin: the limits of untuned SGD and the power of adaptive methods. In _Advances in Neural Information Processing Systems_, 2023.
* Yang et al. (2016) Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization. _arXiv preprint arXiv:1604.03257_, 2016.
* Yau and Wai (2022) Chung-Yiu Yau and Hoi-To Wai. DoCoM-SGT: Doubly compressed momentum-assisted stochastic gradient tracking algorithm for communication efficient decentralized learning. _arXiv preprint arXiv:2202.00255_, 2022.
* Yu et al. (2019) Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization. In _International Conference on Machine Learning_, page 7184-7193. PMLR, 2019.
* Zavriev and Kostyuk (1993) SK Zavriev and FV Kostyuk. Heavy-ball method in nonconvex optimization problems. _Computational Mathematics and Modeling_, 4(4):336-341, 1993.
* Zhao et al. (2022) Haoyu Zhao, Boyue Li, Zhize Li, Peter Richtarik, and Yuejie Chi. BEER: Fast O(1/T) rate for decentralized nonconvex optimization with communication compression. In _Advances in Neural Information Processing Systems_, 2022.
* Zheng et al. (2019) Shuai Zheng, Ziyue Huang, and James Kwok. Communication-efficient distributed blockwise momentum SGD with error-feedback. In _Advances in Neural Information Processing Systems_, 2019.

###### Contents

* 1 Introduction
* 2 Communication Compression, Error Feedback, and Sample Complexity
	* 2.1 Brief history of error-feedback
	* 2.2 Key issue: error feedback has an unhealthy appetite for samples!
	* 2.3 Mysterious effectiveness of momentum in nonconvex optimization
* 3 Main Results
	* 3.1 A deeper dive into the issues EF21 has with stochastic gradients
	* 3.2 Momentum for avoiding mega-batches
	* 3.3 Distributed stochastic error feedback with momentum
	* 3.4 Further improvement using _double_ momentum!
* 4 Experiments
	* 4.1 Experiment 1: increasing batch-size
	* 4.2 Experiment 2: improving convergence with \(n\)
* A More on Contractive Compressors, Error Feedback and Momentum
* B Variance Reduction Effect of SGDM and Comparison to STORM
* C Additional Experiments and Details of Experimental Setup
* C.1 Extra plots for experiments 1 and 2
* C.2 Experiment 3: stochastic quadratic optimization
* C.3 Experiment 4: training neural network
* D Descent Lemma
* E EF21-SGDM-ideal (Proof of Theorem 1 and Proposition 1)
* F EF21-SGDM (Proof of Theorems 2 and 3)
* F.1 Controlling the error of momentum estimator
* F.2 Controlling the error of contractive compression and momentum estimator
* G Further Improvement Using Double Momentum (Proof of Corollary 3)
* G.1 Controlling the error of second momentum estimator
* G.2 Controlling the error of contractive compression and double momentum estimator
* H EF21-SGDM with Absolute Compressor
* H.1 Controlling the error of absolute compression
* I EF21-Storm/MVR

	* 1.1 Controlling the variance of STORM/MVR estimator
	* 1.2 Controlling the variance of contractive compression and STORM/MVR estimator
* 1.3 Simplified Proof of SGDM: Time Varying Parameters and No Tuning for Momentum Sequence
* 1.4 Revisiting EF14-SGD Analysis under BG and BGS Assumptions

## Appendix A More on Contractive Compressors, Error Feedback and Momentum

Greedys us uniform.In our work, we specifically focus on the class of contractive compressors satisfying Definition 1, which contains a greedy Top\(K\) compressor as a special case. Note that Top\(K\) is greedy in that it minimizes the error \(\|\mathrm{Top}K(x)-x\|^{2}\) subject to the sparsity constraint \(\|\mathcal{C}(x)\|_{0}\leq K\), where \(\|u\|_{0}\) counts the number of nonzero entries in \(u\). In practice, greediness is almost always17 very useful, translating into excellent empirical performance, especially when compared to the performance of the Rand\(K\) sparsifier. On the other hand, it appears to be very hard to formalize these practical gains theoretically18. In fact, while greedy compressors such as Top\(K\) outperform their randomized cousins such as Rand\(K\) in practice, and often by huge margins (Lin et al., 2018), the theoretical picture is exactly reversed, and the theoretical communication complexity of gradient-type methods based on randomized compressors (Alistarh et al., 2017; Mishchenko et al., 2019; Horvath et al., 2019; Li et al., 2020; Gorbunov et al., 2021) is much better than of those based on greedy compressors (Koloskova et al., 2020; Richtarik et al., 2021; Fatkullin et al., 2021; Richtarik et al., 2022). The key reason behind this is the fact that popular randomized compressors such as Rand\(K\) become _unbiased_ mappings after appropriate scaling (e.g., \(\mathbb{E}[\frac{d}{K}\mathrm{Rand}K(x)]\equiv x\)), and that the inherent randomness is typically drawn _independently_ by all clients. This leads to several key simplifications in the analysis, and consequently, to theoretical gains over methods that do not compress, and over methods that compress greedily. Further improvements are possible when the randomness is _correlated_ in an appropriate way (Szelndak et al., 2021).

Footnote 17: Greediness is not useful, for example, when \(\mathcal{D}_{i}=\mathcal{D}_{j}\) for all \(i\),\(j\) and when Top\(K\) is applied to the full-batch gradient \(\nabla f_{i}(x)\) by each client. However, situations of this type arise rarely in practice.

Footnote 18: No theoretical results of this type exist for \(n>1\).

Due to the superior empirical properties of greedy contractive compressors, and our desire to push this very potent line of work further, in this paper we work with the general class of compressors

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \multirow{2}{*}{**Method**} & **Comm.** & **Batch-size for** & **Asymp.** & **Batch** & **No extra** \\  & **compl.** & **comm. compl.** & **sample** & **free?** & **assump.?** \\ \hline \multicolumn{2}{|c|}{EF14-SGD} & \multicolumn{2}{c|}{\(\frac{KGL_{\text{max}}}{\alpha\varepsilon^{2}}\)} & \(\frac{\alpha\sigma^{2}}{n\sigma G}^{(\epsilon)}\) & \(\frac{L_{\text{max}}\sigma^{2}}{n\varepsilon^{4}}\) & \multirow{2}{*}{} & \multirow{2}{*}{} (a) \\ \hline \multicolumn{2}{|c|}{NEGUTHIC} & \multicolumn{2}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\ \cline{1-1} (Huang et al., 2022) & \(\frac{KL_{\text{max}}}{\alpha\varepsilon^{2}}\log\left(\frac{G}{\varepsilon} \right)\) (b) & \(\frac{\sigma^{2}}{n\varepsilon^{2}}\vee\frac{1}{\alpha}\log\left(\frac{G}{ \varepsilon}\right)^{(\epsilon)}\) & \(\frac{L_{\text{max}}\sigma^{2}}{n\varepsilon^{4}}\) & \multirow{2}{*}{} & \multirow{2}{*}{} (c) \\ \hline \multicolumn{2}{|c|}{EF21-SGD} & \multicolumn{2}{c|}{\(\frac{K\widetilde{L}}{\alpha\varepsilon^{2}}\)} & \(\frac{\sigma^{2}}{\alpha^{2}\varepsilon^{2}}\) & \(\frac{\widetilde{L}\sigma^{2}}{\alpha^{2}\varepsilon^{4}}\) (d) & \multirow{2}{*}{} & \multirow{2}{*}{} \\ \hline \multicolumn{2}{|c|}{BEF} & \multicolumn{2}{c|}{\(\frac{K_{\text{max}}}{\alpha\varepsilon^{2}}\)} & \(\frac{\sigma^{2}}{\alpha^{2}\varepsilon^{2}}\) & \(\frac{L_{\text{max}}\sigma^{2}}{\alpha^{2}\varepsilon^{4}}\) (d) & \multirow{2}{*}{} & \multirow{2}{*}{} \\ \hline \multicolumn{2}{|c|}{EF21-SGD} & \multicolumn{2}{c|}{\(\frac{K\widetilde{L}}{\alpha\varepsilon^{2}}\)} & \(\frac{\alpha L}{n\varepsilon^{2}}\vee\frac{\alpha L^{2}}{L^{2}}\frac{\sigma^{2}}{ \alpha^{2}\varepsilon^{2}}\) & \(\frac{L\sigma^{2}}{n\varepsilon^{4}}\) & \multirow{2}{*}{} & \multirow{2}{*}{} \\ \hline \multicolumn{2}{|c|}{EF21-SGD2M} & \multicolumn{2}{c|}{\(\frac{K\widetilde{L}}{\alpha\varepsilon^{2}}\)} & \(\frac{\alpha L}{n\varepsilon^{2}}\vee\frac{\alpha L^{3}}{L^{3}}\frac{\sigma^{2}}{ \alpha^{2}}\) & \(\frac{L\sigma^{2}}{n\varepsilon^{4}}\) & \multirow{2}{*}{} & \multirow{2}{*}{} \\ \hline \multicolumn{2}{|c|}{Corollary 3} & \multicolumn{2}{c|}{\(\frac{K\widetilde{L}}{\alpha\varepsilon^{2}}\)} & \(\frac{\alpha L}{n\varepsilon^{2}}\vee\frac{\alpha L^{3}}{L^{3}}\frac{\sigma^{2}}{ \alpha^{2}}\) & \(\frac{L\sigma^{2}}{n\varepsilon^{4}}\) & \multirow{2}{*}{} & \multirow{2}{*}{} \\ \hline \multicolumn{2}{|c|}{F21-SGD} & \multicolumn{2}{c|}{\(\frac{K\widetilde{L}}{\alpha\varepsilon^{2}}\)} & \(\frac{\alpha L}{n\varepsilon^{2}}\vee\frac{\alpha L^{3}}{L^{3}}\frac{\sigma^{2}}{ \alpha^{2}}\) & \(\frac{L\sigma^{2}}{n\varepsilon^{4}}\) & \multirow{2}{*}{} & \multirow{2}{*}{} \\ \hline \multicolumn{2}{|c|}{Corollary 4} & \multicolumn{2}{c|}{\(\frac{K\widetilde{L}}{\alpha\varepsilon^{2}}\)} & \(\frac{\alpha L}{n\varepsilon^{2}}\vee\frac{\alpha L^{3}}{L^{3}}\frac{\sigma^{2}}{ \alpha^{2}}\) & \(\frac{L\sigma^{2}}{n\varepsilon^{4}}\) & \multirow{2}{*}{} & \multirow{2}{*}{} \\ \hline \multicolumn{2}{|c|}{F21-SGD} & \multicolumn{2}{c|}{\(\frac{K\widetilde{L}}{\alpha\varepsilon^{2}}\)} & \(\frac{\alpha L}{n\varepsilon^{2}}\vee\frac{\alpha L^{3}}{L^{3}}\frac{\sigma^{2}}{ \alpha^{2}}\) & \(\frac{L\sigma^{2}}{n\varepsilon^{4}}\) & \multirow{2}{*}{} & \multirow{2}{*}{} \\ \hline \multicolumn{2}{|c|}{Corollary 4} & \multicolumn{2}{c|}{\(\frac{K\widetilde{L}}{\alpha\varepsilon^{2}}\)} & \(\frac{\alpha L}{n\satisfying Definition 1, and do not invoke any additional restrictive assumptions. For example, we do not assume \(\mathcal{C}\) can be made unbiased after scaling.

Error Feedback.The first theoretical analysis of \(\mathsf{EF14}\) was presented in the works of Stich et al. (2018), Alistarh et al. (2018) and further revisited in convex case in (Karimireddy et al., 2019, Beznosikov et al., 2020, Gorbunov et al., 2020, Qian et al., 2020) and analysis was extended to nonconvex setting in (Stich and Karimireddy, 2019). Later, in nonconvex case, various extensions and combinations of \(\mathsf{EF14}\) with other optimization techniques were considered and analyzed, which include bidirectional compression (Tang et al., 2020), decentralized training (Koloskova et al., 2020, Singh et al., 2021), server level momentum (Xie et al., 2020), client level momentum (Zheng et al., 2019), combination with adaptive methods (Li et al., 2022). To our knowledge, the best sample complexity for finding a stationary point for this method (including its momentum and adaptive variants) in the distributed nonconvex setting is given by Koloskova et al. (2020), which is \(\mathcal{O}(\frac{G}{\alpha\varepsilon^{3}}+\frac{\sigma^{2}}{n\varepsilon^{4 }})\). More recently, Huang et al. (2022) propose a modification of \(\mathsf{EF14}\) method achieving \(\mathcal{O}\left(\frac{1}{\alpha\varepsilon^{2}}\log(\frac{G}{\varepsilon})+ \frac{\sigma^{2}}{n\varepsilon^{4}}\right)\) sample complexity by using the BGS assumption. When applied with \(\text{Top}K\) compressor, this method requires to communicate \(\widetilde{\mathcal{O}}\left(\nicefrac{{K}}{{\alpha}}\right)\) coordinates at every iteration. This makes it impractical since when the effective \(\alpha\) is unknown and is set to \(\alpha=K/d\), it means that their method communicates all \(d\) coordinates at every iteration, mimicking vanilla (\(\mathsf{S}\mathsf{G}\mathsf{D}\) method. Moreover, their algorithm uses an additional subroutine and applies updates with a large batch-size of samples of order \(\mathcal{O}(\frac{1}{\alpha}\log\left(\frac{G}{\varepsilon}\right))\), making the algorithm less practical and difficult to implement. It is worth to mention, that error feedback was also analyzed for other classes of compressors such as absolute (see Definition 2) or additive compressors (i.e., \(\mathcal{C}(x+y)=\mathcal{C}(x)+\mathcal{C}(y)\) for all \(x,y\in\mathbb{R}^{d}\)) (Tang et al., 2020, Xu and Huang, 2022), which do not include \(\text{Top}K\) sparsifier.

Momentum.The first convergence analysis of gradient descent with momentum was proposed by B.T. Polyak in his seminal work (Polyak, 1964) studying the benefit of multi-step methods. The proof technique proposed in this work is based on the analysis of the spectral norm of a certain matrix arising from the dynamics of a multi-step process on a quadratic function. Unfortunately, such technique is restricted to the case of strongly convex quadratic objective and the setting of full gradients. Later Zavriev and Kostyuk (1993) prove an asymptotic convergence of this method in nonconvex deterministic case without specifying the rate of convergence.

To our knowledge, the first non-asymptotic analysis of \(\mathsf{SGDM}\) in the smooth nonconvex setting is due to Yu et al. (2019). Their analysis, however, heavily relies on BG assumption. Later, Liu et al. (2020) provide a refined analysis of \(\mathsf{SGDM}\), removing the BG assumption and improving the dependence on the momentum parameter \(\eta\). Notice that the analysis of Liu et al. (2020) and the majority of other works relies on some variant of the following Lyapunov function:

\[\Lambda_{t}:=f(z^{t})-f^{*}+\sum_{\tau=0}^{t}c_{\tau}\left\|x^{t-\tau}-x^{t-1- \tau}\right\|^{2},\] (11)

where \(\left\{z^{t}\right\}_{t\geq 0}\) is some auxiliary sequence (often) different from the iterates \(\left\{x^{t}\right\}_{t\geq 0}\), and \(\left\{c_{\tau}\right\}_{\tau\geq 0}\) is a diminishing non-negative sequence. This approach is motivated by the dynamical system point of view at Polyak's heavy ball momentum, where the two terms in (11) are interpreted as the potential and kinetic energy of the system (Sebbouh et al., 2019). In contrast, the Lyapunov function used in this work is conceptually different even in the single node (\(n=1\)), uncompressed (\(\alpha=1\)) setting. Later, Defazio (2021) revisit the analysis in (Liu et al., 2020) through the lens of primal averaging and provide insights on why momentum helps in practice. The momentum is also used for stabilizing adaptive algorithms such as normalized SGD (Cutkosky and Mehta, 2020). In particular, it was shown that by using momentum, one can ensure convergence without large batches for normalized SGD (while keeping the same sample complexity as a large batch normalized SGD). However, their analysis is specific to the normalized method, which allows using the function value as a Lyapunov function. High probability analysis of momentum methods was investigated in (Cutkosky and Mehta, 2021, Li and Orabona, 2020). In the distributed setting, (Yu et al., 2019, Karimireddy et al., 2021) extend the analysis of \(\mathsf{SGDM}\) under BGS assumption. Later (Takezawa et al., 2022, Gao et al., 2023) remove this assumption providing a refined analysis based on the techniques developed in (Liu et al., 2020). However, the algorithms in these works do not apply any bandwidth reduction technique such as communication compression.

We would like to mention that understanding the behavior of SGDM in convex case also remains an active area of research (Ghadimi et al., 2015; Yang et al., 2016; Sebbouh et al., 2021; Li et al., 2022; Xiao and Yang, 2022).

Variance Reduction Effect of SGDM and Comparison to STORM

Notice that the choice of our Lyapunov function \(\Lambda_{t}\) (8), which is used in the analysis of EF21-SGDM implies that the gradient estimators \(g_{i}^{t}\) and \(v_{i}^{t}\) improve over the iterations, i.e.,

\[g_{i}^{t}\to\nabla f_{i}(x^{t}),\qquad v_{i}^{t}\to\nabla f_{i}(x^{t})\qquad \text{for }t\to\infty.\]

This comes in contrast with the behavior of SGD, for which the gradient estimator \(v_{i}^{t}=\nabla f_{i}(x^{t},\xi_{i}^{t})\) does not necessarily tend to zero over iterations. Such effect of asymptotic improvement of the estimation error of the gradient estimator is reminiscent to the analogous effect known in the literature on variance reduction (VR) methods. In particular, the classical momentum step 6 of Algorithm 1 may be contrasted with a STORM variance reduced estimator proposed by Cutkosky and Orabona (2019), which updates the gradient estimator via

\[w_{i}^{t+1}=\nabla f_{i}(x^{t+1},\xi_{i}^{t+1})+(1-\eta)(w_{i}^{t}-\nabla f_{i} (x^{t},\xi_{i}^{t+1})),\quad w_{i}^{0}=\nabla f_{i}(x^{0},\xi_{i}^{0})\] (12)

It is known that the class of VR methods (and STORM, in particular) can show faster asymptotic convergence in terms of \(T\) (or \(\varepsilon\)) compared to SGD and SGDM under some additional assumptions. However, we would like to point out the important differences (and limitations) of (12) compared to the classical Polyak's momentum used on line 6 of Algorithm 1. First, the estimator \(w_{i}^{t+1}\) is different from the momentum update rule \(v_{i}^{t+1}\) in that it is unbiased for any \(t\geq 0\), i.e., \(\mathbb{E}\left[w_{i}^{t+1}-\nabla f_{i}(x^{t+1})\right]=0\),19

Footnote 19: Notice that \(\mathbb{E}\left[w_{i}^{0}-\nabla f_{i}(x^{0})\right]=0\). Let \(\mathbb{E}\left[w_{i}^{t}-\nabla f_{i}(x^{t})\right]=0\) hold, then

\[\mathbb{E}\left[w_{i}^{t+1}-\nabla f_{i}(x^{t+1})\right]=(1-\eta)\mathbb{E} \left[w_{i}^{t}-\nabla f_{i}(x^{t},\xi_{i}^{t+1})\right]=0.\]

 which greatly facilitates the analysis of this method. Notice that, in particular, in the deterministic case (\(\sigma=0\), \(\alpha=1\)), the method with estimator (12) reduces to vanilla gradient descent with \(w_{i}^{t+1}=\nabla f_{i}(x^{t+1})\). Second, the computation of \(w_{i}^{t+1}\) requires access to two stochastic gradients \(\nabla f_{i}(x^{t+1},\xi_{i}^{t+1})\) and \(\nabla f_{i}(x^{t},\xi_{i}^{t+1})\) under the same realization of noise \(\xi_{i}^{t+1}\) at each iteration, and requires the additional storage of control variate \(x^{t}\). This is a serious limitation, which can make the method impractical or even not implementable for certain applications such as federated RL (Mitra et al., 2023), multi-agent RL (Doan et al., 2019) or operations research problems (Chen et al., 2022). Third, the analysis of variance reduced methods such as STORM requires an additional assumptions such as individual smoothness of stochastic functions (or its averaged variants) (Assumption 3), i.e., \(\|\nabla f_{i}(x,\xi_{i})-\nabla f_{i}(y,\xi_{i})\|\leq\ell_{i}\left\|x-y\right\|\) for all \(x,y\in\mathbb{R}^{d}\), \(\xi_{i}\sim\mathcal{D}_{i}\), \(i\in[n]\), while our EF21-SGDM only needs smoothness of (deterministic) local functions \(f_{i}(x)\). While this assumption is satisfied for some loss functions in supervised learning, it can also be very limiting. Even if Assumption 3 is satisfied, the constant \(\widetilde{\ell}\) (which always satisfies \(\widetilde{\ell}\geq\widetilde{L}\)) can be much larger than \(\widetilde{L}\) canceling the speed-up in terms of \(T\) (or \(\varepsilon\)). For completeness, we provide the sample complexity analysis of our error compensated method combined with estimator (12), which is deferred to Appendix I.

## Appendix C Additional Experiments and Details of Experimental Setup

Divergence of EF21-SGD with time-varying step-sizes.We complement our Figure 1 in the main part of the paper, which shows divergence of EF21-SGD (Fatkullin et al., 2021) with small (constant) step-size. Here, in Figure 4, we see that the similar divergence is observed when using time varying step-sizes \(\gamma_{t}=\frac{0.1}{\sqrt{t+1}}\). Also, EF21-SGD with time-varying step-size does not improve convergence when \(n\) is increased.

Implementation Details.The experiments were implemented in Python 3.7.9. The distributed environment was emulated on machines with Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz. In all experiments with _MNIST_, we split the dataset across nodes by labels to simulate the heterogeneous setting.

### Extra plots for experiments 1 and 2

In Figures 5 and 6, we provide extra experiments for the setup from Section 4.

### Experiment 3: stochastic quadratic optimization

We now consider a synthetic \(\lambda\)-strongly convex quadratic function problem \(f(x)=\frac{1}{n}\sum_{i=1}^{n}f_{i}(x)\), where the functions \(f_{i}(x)=\frac{1}{2}x^{\top}\mathbf{Q}_{i}x-x^{\top}b_{i}\) are (not necessarily convex) quadratic functions for all \(i\in[n]\) and \(x\in\mathbb{R}^{d}\). The matrices \(\mathbf{Q}_{1},\cdots,\mathbf{Q}_{n}\), vectors \(b_{1},\cdots,b_{n},\) and a starting point \(x^{0}\) are generated by Algorithm 2 with the number of nodes \(n=100,\) dimension \(d=1000,\) regularizer \(\lambda=0.01,\) and scale \(s=1\). For all \(i\in[n]\) and \(x\in\mathbb{R}^{d}\), we consider stochastic gradients \(\nabla f_{i}(x,\xi)=\nabla f_{i}(x)+\xi_{i},\) where \(\xi_{i}\) are i.i.d. samples from \(\mathcal{N}(0,\sigma)\) with \(\sigma\in\{0.001,0.01\}\). In

Figure 4: Divergence of EF21-SGD on a quadratic function \(\frac{1}{2}\left\|x\right\|^{2}\) with Top1 compressor. See the proof of Therem 1 for details on the construction of noise \(\xi\), we use \(\sigma=1\), \(B=1\). The starting point for all methods is \(x^{0}=(0,-0.01)^{\top}\). Unlike Figure 1, these experiments use time varying step-sizes and momentum parameters \(\gamma_{t}=\eta_{t}=\frac{0.1}{\sqrt{t+1}}\). Each method is run \(10\) times and the plot shows the median performance alongside the \(25\%\) and \(75\%\) quantiles.

Figure 5: Performance of algorithms on _MNIST_ dataset with \(n=100\), and Top1\(0\) compressor.

Figure 7, we present the comparison of EF21-SGD and EF14-SGD with three different step sizes. The behavior of methods for other step sizes from the set \(\{2^{k}\,|\,k\in[-20,20]\}\) follows a similar trend. For every step size, we observe that at the beginning, the methods have almost the same linear rates, but then EF14-SGD gets stuck at high accuracies, while EF21-SGD continues converging to the lower accuracies.

```
1:Parameters: number nodes \(n\), dimension \(d\), regularizer \(\lambda\), and scale \(s\).
2:for\(i=1,\ldots,n\)do
3: Calculate Guassian noises \(\mu_{i}^{s}=1+s\xi_{i}^{s}\) and \(\mu_{i}^{b}=s\xi_{i}^{b},\) i.i.d. \(\xi_{i}^{s},\xi_{i}^{b}\sim\mathcal{N}(0,1)\)
4:\(b_{i}=\frac{\mu_{i}^{s}}{4}(-1+\mu_{i}^{b},0,\cdots,0)\in\mathbb{R}^{d}\)
5: Scale the predefined tridiagonal matrix \[\mathbf{Q}_{i}=\frac{\mu_{i}^{s}}{4}\left(\begin{array}{cccc}2&-1&&0\\ -1&\ddots&\ddots&\\ &\ddots&\ddots&-1\\ 0&&-1&2\end{array}\right)\in\mathbb{R}^{d\times d}\]
6:endfor
7: Find the mean of matrices \(\mathbf{Q}=\frac{1}{n}\sum_{i=1}^{n}\mathbf{Q}_{i}\)
8: Find the minimum eigenvalue \(\lambda_{\min}(\mathbf{Q})\)
9:for\(i=1,\ldots,n\)do
10: Normalize matrix \(\mathbf{Q}_{i}=\mathbf{Q}_{i}+(\lambda-\lambda_{\min}(\mathbf{Q}))\mathbf{I}\)
11:endfor
12: Find a starting point \(x^{0}=(\sqrt{d},0,\cdots,0)\)
13:Output a new problem: matrices \(\mathbf{Q}_{1},\cdots,\mathbf{Q}_{n}\), vectors \(b_{1},\cdots,b_{n}\), starting point \(x^{0}\) ```

**Algorithm 2** Quadratic Optimization Task Generation Procedure

A procedure to generate stochastic quadratic optimization problems.In this section, we present an algorithm that generates quadratic optimization tasks. The formal description is provided in Algorithm 2. The idea is to take a predefined tridiagonal matrix and add noises to simulate the heterogeneous setting. Algorithm 2 returns matrices \(\mathbf{Q}_{1},\cdots,\mathbf{Q}_{n}\), vectors \(b_{1},\cdots,b_{n},\) and a starting

Figure 6: Performance of algorithms on _real-sim_ dataset with batch-size \(B=1\), and Top\(100\) compressor.

Figure 7: Stochastic Quadratic Optimization Problem with \(\sigma=0.001\) (left figure) and \(\sigma=0.01\) (right figure)

point \(x^{0}\) such that the matrix \(\mathbf{Q}=\frac{1}{n}\sum_{i=1}^{n}\mathbf{Q}_{i}\) has the minimum eigenvalue \(\lambda_{\min}(\mathbf{Q})=\lambda,\) where \(\lambda\geq 0\) is a parameter. Next, we define the functions \(f_{i}\) and stochastic gradients in the following way:

\[f_{i}(x):=\frac{1}{2}x^{\top}\mathbf{Q}_{i}x-x^{\top}b_{i}\]

and

\[\nabla f_{i}(x,\xi):=\nabla f_{i}(x)+\xi_{i},\]

for all \(x\in\mathbb{R}^{d}\) and \(i\in[n].\) The noises \(\xi_{i}\) are i.i.d. samples from \(\mathcal{N}(0,\sigma),\) where \(\sigma\) is a parameter.

### Experiment 4: training neural network

We test algorithms on an image recognition task, _CIFAR10_(Krizhevsky et al., 2009), with the _ResNet18_(He et al., 2016) deep neural network (the number of parameters \(d\approx 10^{7}\)). We split _CIFAR10_ among 5 nodes, and take \(K=2\times 10^{6}\) in Top\(K.\) In all methods we finetune the step sizes. One can see that our findings in the low-scale experiments translate into large-scale experiments in Figure 8. With different batch sizes, EF21-SGD converges slower than EF21-SGD and EF14-SGD, and our new method EF21-SGD improves over EF14-SGD in Figure 7(b). We checked the accuracies on the test dataset (see Table 9) and observed the same relations between algorithms (note that accuracies are far from the real SOTA because we turned off all augmentations and regularizations in training).

Figure 8: _ResNet-18_ on _CIFAR10_ dataset with \(n=5\).

Figure 9: Accuracy on the _CIFAR10_ test split.

Descent Lemma

Let us state the following lemma that is used in the analysis of nonconvex optimization methods.

**Lemma 1** ([11]).: _Let the function \(f(\cdot)\) be L-smooth and let \(x^{t+1}=x^{t}-\gamma g^{t}\) for some vector \(g^{t}\in\mathbb{R}^{d}\) and a step-size \(\gamma>0\). Then we have_

\[f(x^{t+1})\leq f(x^{t})-\frac{\gamma}{2}\left\|\nabla f(x^{t})\right\|^{2}- \left(\frac{1}{2\gamma}-\frac{L}{2}\right)\left\|x^{t+1}-x^{t}\right\|^{2}+ \frac{\gamma}{2}\left\|g^{t}-\nabla f(x^{t})\right\|^{2}.\] (13)

## Appendix E EF21-SGDM-ideal (Proof of Theorem 1 and Proposition 1)

We now state a slighly more general result than Theorem 1, which holds for EF21-SGDM-ideal method with any \(\eta\in(0,1]\). The statement of Theorem 1 follows by setting \(\eta=1\), since in that case EF21-SGDM-ideal coincides with EF21-SGD-ideal (5a), (5aa). Recall that EF21-SGDM-ideal (distributed variant) has the following update rule:

\[x^{t+1}=x^{t}-\gamma g^{t},\qquad g^{t}=\frac{1}{n}\sum_{i=1}^{n}g_{i}^{t},\] (14)

\[\begin{array}{ll}\text{EF21-SGDM-ideal:}&\begin{array}{l}v_{i}^{t+1}= \nabla f_{i}(x^{t+1})+\eta(\nabla f_{i}(x^{t+1},\xi_{i}^{t+1})-\nabla f_{i}(x^ {t+1})),\\ g_{i}^{t+1}=\nabla f_{i}(x^{t+1})+\mathcal{C}\left(v_{i}^{t+1}-\nabla f_{i}(x^ {t+1})\right).\end{array}\end{array}\] (15)

**Theorem 4**.: _Let \(L,\sigma>0\), \(0<\gamma\leq 1/L\), \(0<\eta\leq 1\) and \(n=1.\) There exists a convex, \(L\)-smooth function \(f(\cdot)\), a contractive compressor \(\mathcal{C}(\cdot)\) satisfying Definition 1, and an unbiased stochastic gradient with bounded variance \(\sigma^{2}\) such that if the method (14), (15) is run with a step-size \(\gamma\), then for all \(T\geq 0\) and for all \(x^{0}\in\{(0,x^{0}_{(2)})^{\top}\in\mathbb{R}^{2}\left\|\,x^{0}_{(2)}<0\},\) we have_

\[\mathbb{E}\left[\left\|\nabla f(x^{T})\right\|^{2}\right]\geq\frac{1}{60}\min \left\{\eta^{2}\sigma^{2},\left\|\nabla f(x^{0})\right\|^{2}\right\}.\]

_Fix \(0<\varepsilon\leq L/\sqrt{60}\) and \(x^{0}=(0,-1)^{\top}.\) Additionally assume that \(n\geq 1\) and the variance of unbiased stochastic gradient is controlled by \(\nicefrac{{\sigma^{2}}}{{B}}\) for some \(B\geq 1\). If \(B<\frac{\eta^{2}\sigma^{2}}{60\varepsilon^{2}}\), then we have \(\mathbb{E}\left[\left\|\nabla f(x^{T})\right\|\right]>\varepsilon\) for all \(T\geq 0\)._

Proof of Theorem 1.: **Part I.** Consider \(f(x)=\frac{L}{2}\left\|x\right\|^{2}\), \(x\in\mathbb{R}^{2}\). For each iteration \(t\geq 0\), let the random vector \(\xi^{t+1}\) be sampled uniformly at random from the set of vectors:

\[z_{1}=\begin{pmatrix}2\\ 0\end{pmatrix}\sqrt{\frac{3\sigma^{2}}{10}},\quad z_{2}=\begin{pmatrix}0\\ 1\end{pmatrix}\sqrt{\frac{3\sigma^{2}}{10}},\quad z_{3}=\begin{pmatrix}-2\\ -1\end{pmatrix}\sqrt{\frac{3\sigma^{2}}{10}}.\]

Define the stochastic gradient as \(\nabla f(x^{t},\xi^{t}):=\nabla f(x^{t})+\xi^{t}=Lx^{t}+\xi^{t}\). Notice that \(\mathbb{E}\left[\nabla f(x^{t},\xi^{t})\right]=\nabla f(x^{t})\), and \(\mathbb{E}\left[\left\|\nabla f(x^{t},\xi^{t})-\nabla f(x^{t})\right\|^{2} \right]=\sigma^{2}\). The update rule of method (14), (15) with such estimator is

\[x^{t+1}=x^{t}-\gamma g^{t}=x^{t}-L\gamma x^{t}-\gamma\mathcal{C}\left(\eta\, \xi^{t}\right),\]

where we choose \(\mathcal{C}(\cdot)\) as a Top1 compressor. Notice that \(\mathbb{E}\left[\xi^{t}\right]=(0,0)^{\top}\), but

\[\mathbb{E}\left[\mathcal{C}(\xi^{t})\right]=\eta\,\sqrt{\frac{3\sigma^{2}}{10} }(0,1/3)^{\top}\neq(0,0)^{\top}.\]

By setting the initial iterate to \(x^{0}=(0,x^{0}_{(2)})^{\top}\) for any \(x^{0}_{(2)}<0\), we can derive

\[\mathbb{E}\left[x^{T}\right] = (1-L\gamma)^{T}x^{0}-\eta\,\sqrt{\frac{3\sigma^{2}}{10}}\begin{pmatrix} 0\\ \frac{1}{3}\end{pmatrix}\gamma\sum_{t=0}^{T-1}(1-L\gamma)^{t}\] (16) \[= (1-L\gamma)^{T}\begin{pmatrix}0\\ x^{0}_{(2)}\end{pmatrix}+\frac{\eta}{L}\sqrt{\frac{\sigma^{2}}{30}}\begin{pmatrix} 0\\ -1\end{pmatrix}(1-(1-L\gamma)^{T})\neq\begin{pmatrix}0\\ 0\end{pmatrix}\]for any \(0\leq\gamma\leq 1/L\) and any \(x_{(2)}^{0}<0\). The inequality in (16) is because the first vector has strictly negative component \(x_{(2)}^{0}\), and the second vector has non-positive second component when \(\gamma>0\) and \(\sigma^{2}>0\). Therefore, since \(\left\|\nabla f(x)\right\|^{2}=\left\|Lx\right\|^{2}\), we have

\[\mathbb{E}\left[\left\|\nabla f(x^{T})\right\|^{2}\right] = \mathbb{E}\left[\left\|Lx^{T}\right\|^{2}\right]\] \[= \left\|\mathbb{E}\left[Lx^{T}\right]\right\|^{2}+\mathbb{E}\left[ \left\|Lx^{T}-\mathbb{E}\left[Lx^{T}\right]\right\|^{2}\right]\] \[\stackrel{{(i)}}{{=}} \left((1-L\gamma)^{T}\left\|Lx^{0}\right\|+\eta\,\sqrt{\frac{ \sigma^{2}}{30}}(1-(1-L\gamma)^{T})\right)^{2}\] \[\stackrel{{(ii)}}{{\geq}} (1-L\gamma)^{2T}\left\|\nabla f(x^{0})\right\|^{2}+\frac{\eta^{2 }\sigma^{2}}{30}(1-(1-L\gamma)^{T})^{2}\] \[\geq \frac{\left\|\nabla f(x^{0})\right\|^{2}\eta^{2}\sigma^{2}}{30 \left\|\nabla f(x^{0})\right\|^{2}+\eta^{2}\sigma^{2}}\]

for all \(T\geq 1\), where in \((i)\) we used the form of vector \(\mathbb{E}\left[x^{T}\right]\) in (16), in \((ii)\) we drop a non-negative cross term, and use \(\nabla f(x^{0})=Lx^{0}\). The last inequality follows by lower bounding a univariate quadratic function with respect to \(z:=(1-L\gamma)^{T}\) for \(0\leq z\leq 1\), where optimal choice is \(z=\eta^{2}\sigma^{2}/(30\left\|\nabla f(x^{0})\right\|^{2}+\eta^{2}\sigma^{2})\). It is left to note that \(\frac{xy}{x+y}\geq\frac{1}{2}\min\{x,y\}\) for all \(x,y>0\).

**Part II.** Fix \(n\geq 1\) and \(B\geq 1\). Let at each node \(i=1,\ldots,n\), the random vectors \(\xi_{i}^{t}\) be sampled independently and uniformly form the set of vectors:

\[z_{1}=\begin{pmatrix}2\\ 0\end{pmatrix}\sqrt{\frac{3\sigma^{2}}{10B}},\quad z_{2}=\begin{pmatrix}0\\ 1\end{pmatrix}\sqrt{\frac{3\sigma^{2}}{10B}},\quad z_{3}=\begin{pmatrix}-2\\ -1\end{pmatrix}\sqrt{\frac{3\sigma^{2}}{10B}}.\]

Define a random matrix \(\xi^{t}:=(\xi_{1}^{t},\ldots,\xi_{n}^{t})^{\top}\). Then \(\mathbb{E}\left[\left\|\nabla f(x^{t},\xi^{t})-\nabla f(x^{t})\right\|^{2} \right]=\frac{\sigma^{2}}{B}\). The update of the method (14), (15) on the same function instance will take the form

\[x^{t+1}=x^{t}-\gamma\frac{1}{n}\sum_{i=1}^{n}g_{i}^{t}=x^{t}-L\gamma x^{t}- \gamma\frac{1}{n}\sum_{i=1}^{n}\mathcal{C}\left(\eta\,\xi_{i}^{t}\right).\]

Notice that in this case, we still have

\[\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^{n}\mathcal{C}(\eta\,\xi_{i}^{t})\right] =\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\mathcal{C}(\eta\,\xi_{i}^{t}) \right]=\eta\,\sqrt{\frac{3\sigma^{2}}{10}}(0,1/3)^{\top}\neq(0,0)^{\top},\]

which is independent (!) of \(n\). Therefore, by similar derivations, we can conclude that

\[\mathbb{E}\left[\left\|\nabla f(x^{T})\right\|^{2}\right] \geq \frac{1}{60}\min\left\{\frac{\eta^{2}\sigma^{2}}{B},\left\|\nabla f (x^{0})\right\|^{2}\right\}>\varepsilon^{2},\]

where we use that \(B<\frac{\eta^{2}\sigma^{2}}{60\varepsilon^{2}}\), \(\varepsilon\leq L/\sqrt{60}\), and \(x^{0}=(0,-1)^{\top}\).

_Proof of Proposition 1._ By smoothness (Assumption 1) of \(f(\cdot)\) it follows from Lemma 1 that for \(\gamma\leq 1/L\) we have

\[f(x^{t+1})\leq f(x^{t})-\frac{\gamma}{2}\left\|\nabla f(x^{t})\right\|^{2}+ \frac{\gamma}{2}\left\|g^{t}-\nabla f(x^{t})\right\|^{2}.\] (17)

Now it remains to control the last term, which is due to the error introduced by a contractive compressor and stochastic gradients. We have

\[\mathbb{E}\left[\left\|g^{t}-\nabla f(x^{t})\right\|^{2}\right] \stackrel{{(i)}}{{=}} \mathbb{E}\left[\left\|\mathcal{C}\left(v^{t}-\nabla f(x^{t}) \right)\right\|^{2}\right]\stackrel{{(ii)}}{{=}}\mathbb{E}\left[ \left\|\mathcal{C}\left(\eta\left(\nabla f(x^{t},\xi^{t})-\nabla f(x^{t}) \right)\right)\right\|^{2}\right]\] \[\stackrel{{(iii)}}{{\leq}} 2\mathbb{E}\left[\left\|\mathcal{C}\left(\eta\left(\nabla f(x^{t}, \xi^{t})-\nabla f(x^{t})\right)\right)-\eta(\nabla f(x^{t},\xi^{t})-\nabla f( x^{t}))\right\|^{2}\right]\]\[\qquad\qquad+2\eta^{2}\mathbb{E}\left[\left\|\nabla f(x^{t},\xi^{t})- \nabla f(x^{t})\right\|^{2}\right]\] \[\leq 2(2-\alpha)\eta^{2}\mathbb{E}\left[\left\|\nabla f(x^{t},\xi^{t})- \nabla f(x^{t})\right\|^{2}\right]\] \[\leq 4\eta^{2}\sigma^{2},\]

where \((i)\) and \((ii)\) use the update rule (6), \((iii)\) holds by Young's inequality, and the last two steps hold by Definition 1 and Assumption 2.

Subtracting \(f^{*}\) from both sides of (17), taking expectation and defining \(\delta_{t}:=\mathbb{E}\left[f(x^{t})-f^{*}\right]\), we derive

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right]=\frac{1}{T} \sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla f(x^{t})\right\|^{2}\right]\leq \frac{2\delta_{0}}{\gamma T}+4\eta^{2}\sigma^{2}.\]EF21-SGDM (Proof of Theorems 2 and 3)

The statement of Theorem 2 follows directly from Theorem 3 and Remark 2. Let us prove Theorem 3.

_Proof of Theorem 3._ In order to control the error between \(g^{t}\) and \(\nabla f(x^{t})\), we decompose it into two terms

\[\left\|g^{t}-\nabla f(x^{t})\right\|^{2}\leq 2\left\|g^{t}-v^{t}\right\|^{2}+2 \left\|v^{t}-\nabla f(x^{t})\right\|^{2}\leq 2\frac{1}{n}\sum_{i=1}^{n}\left\|g^{t} _{i}-v^{t}_{i}\right\|^{2}+2\left\|v^{t}-\nabla f(x^{t})\right\|^{2},\]

and develop a recursion for each term above separately.

**Part I (a). Controlling the error of momentum estimator for each \(v^{t}_{i}\).** Recall that by Lemma 2-(24), we have for each \(i=1,\ldots,n\), and any \(0<\eta\leq 1\) and \(t\geq 0\)

\[\mathbb{E}\left[\left\|v^{t+1}_{i}-\nabla f_{i}(x^{t+1})\right\|^{2}\right] \leq(1-\eta)\mathbb{E}\left[\left\|v^{t}_{i}-\nabla f_{i}(x^{t})\right\|^{2} \right]+\frac{3L_{i}^{2}}{\eta}\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^ {2}\right]+\eta^{2}\sigma^{2}.\] (18)

Averaging inequalities (18) over \(i=1,\ldots,n\) and denoting \(\widetilde{P}_{t}:=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|v^{t}_{i} -\nabla f_{i}(x^{t})\right\|^{2}\right]\), \(R_{t}:=\mathbb{E}\left[\left\|x^{t}-x^{t+1}\right\|^{2}\right]\) we have

\[\widetilde{P}_{t+1}\leq(1-\eta)\widetilde{P}_{t}+\frac{3\widetilde{L}^{2}}{ \eta}R_{t}+\eta^{2}\sigma^{2}.\]

Summing up the above inequality for \(t=0,\ldots,T-1\), we derive

\[\frac{1}{T}\sum_{t=0}^{T-1}\widetilde{P}_{t}\leq\frac{3\widetilde{L}^{2}}{ \eta^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+\eta\sigma^{2}+\frac{1}{\eta T} \widetilde{P}_{0}.\] (19)

**Part I (b). Controlling the error of momentum estimator for \(v^{t}\) (on average).** Similarly by Lemma 2-(25), we have for any \(0<\eta\leq 1\) and \(t\geq 0\)

\[\mathbb{E}\left[\left\|v^{t+1}-\nabla f(x^{t+1})\right\|^{2}\right]\leq(1- \eta)\mathbb{E}\left[\left\|v^{t}-\nabla f(x^{t})\right\|^{2}\right]+\frac{3 L^{2}}{\eta}\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+\frac{\eta^{2} \sigma^{2}}{n},\]

where \(v^{t}:=\frac{1}{n}\sum_{i=1}^{n}v^{t}_{i}\) is an auxiliary sequence.

Summing up the above inequality for \(t=0,\ldots,T-1\), and denoting \(P_{t}:=\mathbb{E}\left[\left\|v^{t}-\nabla f(x^{t})\right\|^{2}\right]\), we derive

\[\frac{1}{T}\sum_{t=0}^{T-1}P_{t}\leq\frac{3L^{2}}{\eta^{2}}\frac{1}{T}\sum_{t= 0}^{T-1}R_{t}+\frac{\eta\sigma^{2}}{n}+\frac{1}{\eta T}P_{0}.\] (20)

**Part II. Controlling the error of contractive compressor and momentum estimator.** By Lemma 3 we have for each \(i=1,\ldots,n\), and any \(0<\eta\leq 1\) and \(t\geq 0\)

\[\mathbb{E}\left[\left\|g^{t+1}_{i}-v^{t+1}_{i}\right\|^{2}\right] \leq \left(1-\frac{\alpha}{2}\right)\mathbb{E}\left[\left\|g^{t}_{i}-v ^{t}_{i}\right\|^{2}\right]+\frac{4\eta^{2}}{\alpha}\mathbb{E}\left[\left\|v ^{t}_{i}-\nabla f_{i}(x^{t})\right\|^{2}\right]\] (21) \[\quad+\frac{4L_{i}^{2}\eta^{2}}{\alpha}\mathbb{E}\left[\left\|x^ {t+1}-x^{t}\right\|^{2}\right]+\eta^{2}\sigma^{2}.\]

Averaging inequalities (21) over \(i=1,\ldots,n\), denoting \(\widetilde{V}_{t}:=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|g^{t}_{i}- v^{t}_{i}\right\|^{2}\right]\), and summing up the resulting inequality for \(t=0,\ldots,T-1\), we obtain

\[\frac{1}{T}\sum_{t=0}^{T-1}\widetilde{V}_{t} \leq \frac{8\eta^{2}}{\alpha^{2}}\frac{1}{T}\sum_{t=0}^{T-1}\widetilde{ P}_{t}+\frac{8\widetilde{L}^{2}\eta^{2}}{\alpha^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+ \frac{2\eta^{2}\sigma^{2}}{\alpha}+\frac{2}{\alpha T}\widetilde{V}_{0}.\] (22)

[MISSING_PAGE_FAIL:29]

\(\frac{\eta\sigma^{2}}{\alpha^{2}B_{\text{in}}T}\leq\frac{L\delta_{0}}{\eta T}\). Therefore, we have

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] \leq \mathcal{O}\left(\frac{\widetilde{L}\delta_{0}}{\alpha T}+\left( \frac{L\delta_{0}\sigma^{2/3}}{\alpha^{2/3}T}\right)^{3/4}+\left(\frac{L \delta_{0}\sigma}{\sqrt{\alpha T}}\right)^{2/3}+\left(\frac{L\delta_{0} \sigma^{2}}{nT}\right)^{1/2}+\frac{\sigma\sqrt{L\delta_{0}}}{\alpha\sqrt{B_{ \text{init}}}T}\right).\]

Using \(B_{\text{init}}\geq\min\left\{\frac{\sigma^{2}L}{L^{2}\delta_{0}},\frac{\sigma }{\alpha\sqrt{L\delta_{0}}T},\frac{\sigma^{2/3}}{\alpha^{4/3}T^{2/3}(L\delta_{ 0})^{1/3}},\frac{n}{\alpha^{2}T}\right\},\) we obtain

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] \leq \mathcal{O}\left(\frac{\widetilde{L}\delta_{0}}{\alpha T}+\left( \frac{L\delta_{0}\sigma^{2/3}}{\alpha^{2/3}T}\right)^{3/4}+\left(\frac{L \delta_{0}\sigma}{\sqrt{\alpha T}}\right)^{2/3}+\left(\frac{L\delta_{0} \sigma^{2}}{nT}\right)^{1/2}\right).\]

It remains to notice that \(\left\lceil\max\left\{\min\left\{\frac{\sigma^{2}L}{L^{2}\delta_{0}},\frac{ \sigma}{\alpha\sqrt{L\delta_{0}}T},\frac{\sigma^{2/3}}{\alpha^{4/3}T^{2/3}(L \delta_{0})^{1/3}},\frac{n}{\alpha^{2}T}\right\},\frac{\sigma^{2}}{L\delta_{0} n}\right\}\right\rceil\leq\left\lceil\frac{\sigma^{2}}{L\delta_{0}}\right\rceil\).

### Controlling the error of momentum estimator

**Lemma 2**.: _Let Assumption 1 be satisfied, and suppose \(0<\eta\leq 1\). For every \(i=1,\ldots,n\), let the sequence \(\left\{v_{i}^{t}\right\}_{t\geq 0}\) be updated via_

\[v_{i}^{t}=v_{i}^{t-1}+\eta\left(\nabla f_{i}(x^{t},\xi_{i}^{t})-v_{i}^{t-1} \right),\]

_Define the sequence \(v^{t}:=\frac{1}{n}\sum_{i=1}^{n}v_{i}^{t}\). Then for every \(i=1,\ldots,n\) and \(t\geq 0\) it holds_

\[\mathbb{E}\left[\left\|v_{i}^{t}-\nabla f_{i}(x^{t})\right\|^{2}\right]\leq(1 -\eta)\mathbb{E}\left[\left\|v_{i}^{t-1}-\nabla f_{i}(x^{t-1})\right\|^{2} \right]+\frac{3L_{i}^{2}}{\eta}\mathbb{E}\left[\left\|x^{t}-x^{t-1}\right\|^{2 }\right]+\eta^{2}\sigma^{2},\] (24)

\[\mathbb{E}\left[\left\|v^{t}-\nabla f(x^{t})\right\|^{2}\right]\leq(1-\eta) \mathbb{E}\left[\left\|v^{t-1}-\nabla f(x^{t-1})\right\|^{2}\right]+\frac{3L ^{2}}{\eta}\mathbb{E}\left[\left\|x^{t}-x^{t-1}\right\|^{2}\right]+\frac{\eta ^{2}\sigma^{2}}{n}.\] (25)

Proof.: By the update rule of \(v_{i}^{t}\), we have

\[\mathbb{E}\left[\left\|v_{i}^{t}-\nabla f_{i}(x^{t})\right\|^{2}\right] = \mathbb{E}\left[\left\|v_{i}^{t-1}-\nabla f_{i}(x^{t})+\eta( \nabla f_{i}(x^{t},\xi_{i}^{t})-v_{i}^{t-1})\right\|^{2}\right]\] \[= \mathbb{E}\left[\mathbb{E}_{\xi_{1}^{t}}\left[\left\|(1-\eta)(v _{i}^{t-1}-\nabla f_{i}(x^{t}))+\eta(\nabla f_{i}(x^{t},\xi_{i}^{t})-\nabla f_ {i}(x^{t}))\right\|^{2}\right]\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\left\|v_{i}^{t-1}-\nabla f_{i}(x^{t} )\right\|^{2}\right]+\eta^{2}\mathbb{E}\left[\left\|\nabla f_{i}(x^{t},\xi_{i} ^{t})-\nabla f_{i}(x^{t})\right\|^{2}\right]\] \[\leq (1-\eta)^{2}\left(1+\frac{\eta}{2}\right)\mathbb{E}\left[\left\| v_{i}^{t-1}-\nabla f_{i}(x^{t-1})\right\|^{2}\right]\] \[\qquad+\left(1+\frac{2}{\eta}\right)\mathbb{E}\left[\left\|\nabla f _{i}(x^{t-1})-\nabla f_{i}(x^{t})\right\|^{2}\right]+\eta^{2}\sigma^{2}\] \[\leq (1-\eta)\mathbb{E}\left[\left\|v_{i}^{t-1}-\nabla f_{i}(x^{t-1}) \right\|^{2}\right]+\frac{3L_{i}^{2}}{\eta}\mathbb{E}\left[\left\|x^{t}-x^{t+1 }\right\|^{2}\right]+\eta^{2}\sigma^{2},\]

where the first inequality holds by Young's inequality, and the last step uses smoothness of \(f_{i}(\cdot)\) (Assumption 1), which concludes the proof of (24).

For each \(t=0,\ldots,T-1\), define a random vector \(\xi^{t}:=(\xi_{1}^{t},\ldots,\xi_{n}^{t})\) and denote by \(\nabla f(x^{t},\xi^{t}):=\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(x^{t},\xi_{i}^{t})\). Note that the entries of the random vector \(\xi^{t}\) are independent and \(\mathbb{E}_{\xi^{t}}\left[\nabla f(x^{t},\xi^{t})\right]=\nabla f(x^{t})\), then we have

\[v^{t}=v^{t-1}+\eta\left(\nabla f(x^{t},\xi^{t})-v^{t-1}\right),\]

where \(v^{t}:=\frac{1}{n}\sum_{i=1}^{n}v_{i}^{t}\) is an auxiliary sequence. Therefore, we can similarly derive

\[\mathbb{E}\left[\left\|v^{t}-\nabla f(x^{t})\right\|^{2}\right] = \mathbb{E}\left[\left\|v^{t-1}-\nabla f(x^{t})+\eta\left(\nabla f (x^{t},\xi^{t})-v^{t-1}\right)\right\|^{2}\right]\] \[= \mathbb{E}\left[\mathbb{E}_{\xi^{t}}\left[\left\|(1-\eta)(v^{t-1}- \nabla f(x^{t}))+\eta\left(\nabla f(x^{t},\xi^{t})-\nabla f(x^{t})\right)\right\| ^{2}\right]\right]\]\[= (1-\eta)^{2}\mathbb{E}\left[\left\|v^{t-1}-\nabla f(x^{t})\right\|^{2 }\right]+\eta^{2}\mathbb{E}\left[\left\|\nabla f(x^{t},\xi^{t})-\nabla f(x^{t}) \right\|^{2}\right]\] \[\leq (1-\eta)^{2}\left(1+\frac{\eta}{2}\right)\mathbb{E}\left[\left\|v ^{t-1}-\nabla f(x^{t-1})\right\|^{2}\right]\] \[\qquad+\left(1+\frac{2}{\eta}\right)\mathbb{E}\left[\left\|\nabla f (x^{t-1})-\nabla f(x^{t})\right\|^{2}\right]+\frac{\eta^{2}\sigma^{2}}{n}\] \[\leq (1-\eta)\mathbb{E}\left[\left\|v^{t-1}-\nabla f(x^{t-1})\right\| ^{2}\right]+\frac{3L^{2}}{\eta}\mathbb{E}\left[\left\|x^{t}-x^{t-1}\right\|^{ 2}\right]+\frac{\eta^{2}\sigma^{2}}{n},\]

where the last step uses smoothness of \(f(\cdot)\) (Assumption 1), which concludes the proof of (25). 

### Controlling the error of contractive compression and momentum estimator

**Lemma 3**.: _Let Assumption 1 be satisfied, and suppose \(\mathcal{C}\) is a contractive compressor with \(\alpha\leq\frac{1}{2}\). For every \(i=1,\ldots,n\), let the sequences \(\left\{v_{i}^{t}\right\}_{t\geq 0}\) and \(\left\{g_{i}^{t}\right\}_{t\geq 0}\) be updated via_

\[v_{i}^{t}=v_{i}^{t-1}+\eta\left(\nabla f_{i}(x^{t},\xi_{i}^{t})-v_{i}^{t-1} \right),\]

\[g_{i}^{t}=g_{i}^{t-1}+\mathcal{C}\left(v_{i}^{t}-g_{i}^{t-1}\right),\]

_Then for every \(i=1,\ldots,n\) and \(t\geq 0\) it holds_

\[\mathbb{E}\left[\left\|g_{i}^{t}-v_{i}^{t}\right\|^{2}\right] \leq \left(1-\frac{\alpha}{2}\right)\mathbb{E}\left[\left\|g_{i}^{t-1} -v_{i}^{t-1}\right\|^{2}\right]+\frac{4\eta^{2}}{\alpha}\mathbb{E}\left[ \left\|v_{i}^{t-1}-\nabla f_{i}(x^{t-1})\right\|^{2}\right]\] (26) \[\qquad+\frac{4L_{i}^{2}\eta^{2}}{\alpha}\mathbb{E}\left[\left\|x ^{t}-x^{t-1}\right\|^{2}\right]+\eta^{2}\sigma^{2}.\]

Proof.: By the update rules of \(g_{i}^{t}\) and \(v_{i}^{t}\), we derive

\[\mathbb{E}\left[\left\|g_{i}^{t}-v_{i}^{t}\right\|^{2}\right] = \mathbb{E}\left[\left\|g_{i}^{t-1}-v_{i}^{t}+\mathcal{C}(v_{i}^{ t}-g_{i}^{t-1})\right\|^{2}\right]\] \[= \mathbb{E}\left[\mathbb{E}_{\mathcal{C}}\left[\left\|\mathcal{C} (v_{i}^{t}-g_{i}^{t-1})-(v_{i}^{t}-g_{i}^{t-1})\right\|^{2}\right]\right]\] \[\overset{(i)}{\leq} (1-\alpha)\mathbb{E}\left[\left\|v_{i}^{t}-g_{i}^{t-1}\right\|^{2}\right]\] \[\overset{(ii)}{=} (1-\alpha)\mathbb{E}\left[\left\|v_{i}^{t-1}-g_{i}^{t-1}+\eta( \nabla f_{i}(x^{t},\xi_{i}^{t})-v_{i}^{t-1})\right\|^{2}\right]\] \[= (1-\alpha)\mathbb{E}\left[\mathbb{E}_{\xi_{i}^{t}}\left[\left\|v _{i}^{t-1}-g_{i}^{t-1}+\eta(\nabla f_{i}(x^{t},\xi_{i}^{t})-v_{i}^{t-1})\right\| ^{2}\right]\right]\] \[= (1-\alpha)\mathbb{E}\left[\left\|v_{i}^{t-1}-g_{i}^{t-1}+\eta( \nabla f_{i}(x^{t})-v_{i}^{t-1})\right\|^{2}\right]\] \[\qquad+(1-\alpha)\eta^{2}\mathbb{E}\left[\left\|\nabla f_{i}(x^{t },\xi_{i}^{t})-\nabla f_{i}(x^{t})\right\|^{2}\right]\] \[\overset{(iii)}{\leq} (1-\alpha)\left(1+\rho\right)\mathbb{E}\left[\left\|v_{i}^{t-1}-g _{i}^{t-1}\right\|^{2}\right]+(1-\alpha)\left(1+\rho^{-1})\eta^{2}\mathbb{E} \left[\left\|v_{i}^{t-1}-\nabla f_{i}(x^{t})\right\|^{2}\right]\] \[\qquad+(1-\alpha)\eta^{2}\sigma^{2}\] \[\overset{(iv)}{=} (1-\theta)\mathbb{E}\left[\left\|g_{i}^{t-1}-v_{i}^{t-1}\right\| ^{2}\right]+\beta\eta^{2}\mathbb{E}\left[\left\|v_{i}^{t-1}-\nabla f_{i}(x^{t}) \right\|^{2}\right]+(1-\alpha)\eta^{2}\sigma^{2}\] \[\overset{(v)}{\leq} (1-\theta)\mathbb{E}\left[\left\|g_{i}^{t-1}-v_{i}^{t-1}\right\| ^{2}\right]+2\beta\eta^{2}\mathbb{E}\left[\left\|v_{i}^{t-1}-\nabla f_{i}(x^{t-1} )\right\|^{2}\right]\] \[\qquad+2\beta L_{i}^{2}\eta^{2}\mathbb{E}\left[\left\|x^{t}-x^{t-1 }\right\|^{2}\right]+\eta^{2}\sigma^{2},\]

where \((i)\) is due to the definition of a contractive compressor (Definition 1), \((ii)\) follows by the update rule of \(v_{i}^{t}\), \((iii)\) and \((v)\) hold by Young's inequality for any \(\rho>0\). In \((iv)\), we introduced the notation \(\theta:=1-(1-\alpha)(1+\rho)\), and \(\beta:=(1-\alpha)(1+\rho^{-1})\). The last step follows by smoothness of \(f_{i}(\cdot)\) (Assumption 1). The proof is complete by the choice \(\rho=\alpha/2\), which guarantees \(1-\theta\leq 1-\alpha/2\), and \(2\beta\leq 4/\alpha\).

## Appendix G Further Improvement Using Double Momentum (Proof of Corollary 3)

```
1:Input: starting point \(x^{0}\), step-size \(\gamma>0\), parameter \(\eta\in(0,1]\), initial batch size \(B_{\text{init}}\)
2:Initialize \(u_{i}^{0}=v_{i}^{0}=g_{i}^{0}=\frac{1}{B_{\text{init}}}\sum_{j=1}^{B_{\text{ init}}}\nabla f_{i}(x^{0},\xi_{i,j}^{0})\) for \(i=1,\ldots,n\); \(g^{0}=\frac{1}{n}\sum_{i=1}^{n}g_{i}^{0}\)
3:for\(t=0,1,2,\ldots,T-1\)do
4: Master computes \(x^{t+1}=x^{t}-\gamma g^{t}\) and broadcasts \(x^{t+1}\) to all nodes
5:for all nodes \(i=1,\ldots,n\)in paralleldo
6: Compute the first momentum estimator \(v_{i}^{t+1}=(1-\eta)v_{i}^{t}+\eta\nabla f_{i}(x^{t+1},\xi_{i}^{t+1})\)
7: Compute the second momentum estimator \(u_{i}^{t+1}=(1-\eta)u_{i}^{t}+\eta v_{i}^{t+1}\)
8: Compress \(c_{i}^{t+1}=\mathcal{C}(u_{i}^{t+1}-g_{i}^{t})\) and send \(c_{i}^{t+1}\) to the master
9: Update local state \(g_{i}^{t+1}=g_{i}^{t}+c_{i}^{t+1}\)
10:endfor
11: Master computes \(g^{t+1}=\frac{1}{n}\sum_{i=1}^{n}g_{i}^{t+1}\) via \(g^{t+1}=g^{t}+\frac{1}{n}\sum_{i=1}^{n}c_{i}^{t+1}\)
12:endfor ```

**Algorithm 3** EF21-SGD2M (Error Feedback 2021 Enhanced with _Double Momentum_)

In this section, we state the detailed version of Corollary 3 in Theorem 5, followed by its formal proof. Notice that the key reason for the sample complexity improvement of the double momentum variant compared to EF21-SGDM (Theorem 3) is that in (27), one of the terms has better dependence on \(\eta\) compared to (9) in Theorem 3, i.e., \(\eta^{4}\sigma^{2}/\alpha\) instead of \(\eta^{2}\sigma^{2}/\alpha\). As a result, this term is dominated by other terms and vanishes in Corollary 3.

**Theorem 5**.: _Let Assumptions 1 and 2 hold. Let \(\hat{x}^{T}\) be sampled uniformly at random from the iterates of the method. Let Algorithm 3 run with a contractive compressor. For all \(\eta\in(0,1]\) and \(B_{\text{init}}\geq 1\), with \(\gamma\leq\min\left\{\frac{\alpha}{60L},\frac{\eta}{16L}\right\}\), we have_

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] \leq\mathcal{O}\left(\frac{\Psi_{0}}{\gamma T}+\frac{\eta^{3}\sigma^{2}}{ \alpha^{2}}+\frac{\eta^{4}\sigma^{2}}{\alpha}+\frac{\eta\sigma^{2}}{n}\right),\] (27)

_where \(\Psi_{0}:=\delta_{0}+\frac{\gamma}{\eta}\mathbb{E}\left[\left\|v^{0}-\nabla f( x^{0})\right\|^{2}\right]+\frac{\gamma\mu^{4}}{\alpha^{2}}\frac{1}{n}\sum_{i=1}^{n} \mathbb{E}\left[\left\|v_{i}^{0}-\nabla f_{i}(x^{0})\right\|^{2}\right]\). Setting initial batch size \(B_{\text{init}}=\left\lceil\frac{\sigma^{2}}{L\delta_{0}}\right\rceil\), step-size and momentum parameters_

\[\gamma=\min\left\{\frac{\alpha}{60\widetilde{L}},\frac{\eta}{16L} \right\},\qquad\eta=\min\left\{1,\left(\frac{L\delta_{0}\alpha^{2}}{\sigma^{2 }T}\right)^{\nicefrac{{1}}{{4}}},\left(\frac{L\delta_{0}n}{\sigma^{2}T}\right) ^{\nicefrac{{1}}{{2}}},\frac{\alpha\sqrt{L\delta_{0}B_{\text{init}}}}{\sigma} \right\},\] (28)

_we get_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla f(x^{t}) \right\|^{2}\right] \leq \mathcal{O}\left(\frac{\widetilde{L}\delta_{0}}{\alpha T}+\left( \frac{L\delta_{0}\sigma^{2/3}}{\alpha^{2/3}T}\right)^{\nicefrac{{3}}{{4}}}+ \left(\frac{L\delta_{0}\sigma^{2}}{nT}\right)^{\nicefrac{{1}}{{2}}}\right).\]

Proof.: In order to control the error between \(g^{t}\) and \(\nabla f(x^{t})\), we decompose it into three terms

\[\left\|g^{t}-\nabla f(x^{t})\right\|^{2} \leq 3\left\|g^{t}-u^{t}\right\|^{2}+3\left\|u^{t}-v^{t}\right\|^{2}+3 \left\|v^{t}-\nabla f(x^{t})\right\|^{2}\] \[\leq 3\frac{1}{n}\sum_{i=1}^{n}\left\|g_{i}^{t}-u_{i}^{t}\right\|^{2} +3\left\|u^{t}-v^{t}\right\|^{2}+3\left\|v^{t}-\nabla f(x^{t})\right\|^{2},\]

where we define the sequences \(v^{t}:=\frac{1}{n}\sum_{i=1}^{n}v_{i}^{t}\) and \(u^{t}:=\frac{1}{n}\sum_{i=1}^{n}u_{i}^{t}\). In the following, we develop a recursion for each term above separately.

**Part I. Controlling the error of momentum estimator for each \(v_{i}^{t}\) and on average for \(v^{t}\).** Denote \(P_{t}:=\mathbb{E}\left[\left\|v^{t}-\nabla f(x^{t})\right\|^{2}\right]\), \(\widetilde{P}_{t}:=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|v_{i}^{t}- \nabla f_{i}(x^{t})\right\|^{2}\right]\), \(R_{t}:=\mathbb{E}\left[\left\|x^{t}-x^{t+1}\right\|^{2}\right]\). Similarly to Part I of the proof of Theorem 3, we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\widetilde{P}_{t}\leq\frac{3\widetilde{L}^{2}}{\eta^ {2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+\eta\sigma^{2}+\frac{1}{\eta T}\widetilde{P }_{0},\] (29)\[\frac{1}{T}\sum_{t=0}^{T-1}P_{t}\leq\frac{3L^{2}}{\eta^{2}}\frac{1}{T}\sum_{t=0}^ {T-1}R_{t}+\frac{\eta\sigma^{2}}{n}+\frac{1}{\eta T}P_{0}.\] (30)

**Part II (a). Controlling the error of the second momentum estimator for each \(u_{i}^{t}\).** Recall that by Lemma 4-(37), we have for each \(i=1,\ldots,n\), and any \(0<\eta\leq 1\) and \(t\geq 0\)

\[\mathbb{E}\left[\left\|u_{i}^{t+1}-v_{i}^{t+1}\right\|^{2}\right] \leq (1-\eta)\mathbb{E}\left[\left\|u_{i}^{t}-v_{i}^{t}\right\|^{2} \right]+6\eta\mathbb{E}\left[\left\|v_{i}^{t}-\nabla f_{i}(x^{t})\right\|^{2}\right]\] (31) \[\quad+6L_{i}^{2}\eta\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\| ^{2}\right]+\eta^{2}\sigma^{2},\]

Averaging inequalities (31) over \(i=1,\ldots,n\) and denoting \(\widetilde{Q}_{t}:=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|u_{i}^{t}- v_{i}^{t}\right\|^{2}\right]\), we have

\[\widetilde{Q}_{t+1} \leq (1-\eta)\widetilde{Q}_{t}+6\eta\widetilde{P}_{t}+6\widetilde{L} ^{2}\eta R_{t}+\eta^{2}\sigma^{2}.\]

Summing up the above inequalities for \(t=0,\ldots,T-1\), we derive

\[\frac{1}{T}\sum_{t=0}^{T-1}\widetilde{Q}_{t} \leq \frac{6}{T}\sum_{t=0}^{T-1}\widetilde{P}_{t}+6\widetilde{L}^{2} \frac{1}{T}\sum_{t=0}^{T-1}R_{t}+\eta\sigma^{2}+\frac{1}{\eta T}\widetilde{Q} _{0}\] (32) \[\leq \left(\frac{6\cdot 3\widetilde{L}^{2}}{\eta^{2}}+6\widetilde{L} ^{2}\right)\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+7\eta\sigma^{2}+\frac{1}{\eta T} \widetilde{Q}_{0}+\frac{6}{\eta T}\widetilde{P}_{0}\] \[\leq \frac{19\widetilde{L}^{2}}{\eta^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_ {t}+7\eta\sigma^{2}+\frac{6}{\eta T}\widetilde{P}_{0},\]

where we used (29), the bound \(\eta\leq 1\), and \(u_{i}^{0}=v_{i}^{0}\) for \(i=1,\ldots,n\).

**Part II (b). Controlling the error of the second momentum estimator \(u^{t}\) (on average)**. Similarly by Lemma 4-(38), we have for any \(0<\eta\leq 1\) and \(t\geq 0\)

\[\mathbb{E}\left[\left\|u^{t+1}-v^{t+1}\right\|^{2}\right] \leq (1-\eta)\mathbb{E}\left[\left\|u^{t}-v^{t}\right\|^{2}\right]+6 \eta\mathbb{E}\left[\left\|v^{t}-\nabla f(x^{t})\right\|^{2}\right]\] \[\quad+6L^{2}\eta\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2} \right]+\frac{\eta^{2}\sigma^{2}}{n},\]

Summing up the above inequalities for \(t=0,\ldots,T-1\), and denoting \(Q_{t}:=\mathbb{E}\left[\left\|u^{t}-v^{t}\right\|^{2}\right]\), we derive

\[\frac{1}{T}\sum_{t=0}^{T-1}Q_{t} \leq \frac{6}{T}\sum_{t=0}^{T-1}P_{t}+6L^{2}\frac{1}{T}\sum_{t=0}^{T-1 }R_{t}+\eta\sigma^{2}+\frac{1}{\eta T}Q_{0}\] (33) \[\leq \left(\frac{6\cdot 3L^{2}}{\eta^{2}}+6L^{2}\right)\frac{1}{T}\sum_{t =0}^{T-1}R_{t}+\frac{7\eta\sigma^{2}}{n}+\frac{1}{\eta T}Q_{0}+\frac{6}{\eta T }P_{0}\] \[\leq \frac{19L^{2}}{\eta^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+\frac{7 \eta\sigma^{2}}{n}+\frac{6}{\eta T}P_{0},\]

where we used (30), the bound \(\eta\leq 1\), and \(u^{0}=v^{0}\).

**Part III. Controlling the error of contractive compressor and the double momentum estimator.** By Lemma 5 we have for each \(i=1,\ldots,n\), and any \(0<\eta\leq 1\) and \(t\geq 0\)

\[\mathbb{E}\left[\left\|g_{i}^{t+1}-u_{i}^{t+1}\right\|^{2}\right] \leq \left(1-\frac{\alpha}{2}\right)\mathbb{E}\left[\left\|g_{i}^{t}-u _{i}^{t}\right\|^{2}\right]+\frac{6\eta^{2}}{\alpha}\mathbb{E}\left[\left\|u_ {i}^{t}-v_{i}^{t}\right\|^{2}\right]\] \[\quad+\frac{6\eta^{4}}{\alpha}\mathbb{E}\left[\left\|v_{i}^{t}- \nabla f_{i}(x^{t},\xi_{i}^{t})\right\|^{2}\right]+\frac{6L_{i}^{2}\eta^{4}}{ \alpha}\mathbb{E}\left[\left\|x^{t}-x^{t+1}\right\|^{2}\right]+\eta^{4}\sigma^ {2}.\]

Averaging inequalities (34) over \(i=1,\ldots,n\), denoting \(\widetilde{V}_{t}:=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|g_{i}^{t}-u _{i}^{t}\right\|^{2}\right]\), and summing up the resulting inequality for \(t=0,\ldots,T-1\), we obtain

\[\frac{1}{T}\sum_{t=0}^{T-1}\widetilde{V}_{t} \leq \frac{12\eta^{2}}{\alpha^{2}}\frac{1}{T}\sum_{t=0}^{T-1}\widetilde{Q }_{t}+\frac{12\eta^{4}}{\alpha^{2}}\frac{1}{T}\sum_{t=0}^{T-1}\widetilde{P}_{t }+\frac{12\widetilde{L}^{2}\eta^{4}}{\alpha^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t} +\frac{2\eta^{4}\sigma^{2}}{\alpha}\]\[\leq \frac{12\eta^{2}}{\alpha^{2}}\left(\frac{19\widetilde{L}^{2}}{\eta^{2 }}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+7\eta\sigma^{2}\right)+\frac{12\eta^{4}}{ \alpha^{2}}\left(\frac{3\widetilde{L}^{2}}{\eta^{2}}\frac{1}{T}\sum_{t=0}^{T-1 }R_{t}+\eta\sigma^{2}\right)\] \[\quad+\frac{12\widetilde{L}^{2}\eta^{4}}{\alpha^{2}}\frac{1}{T} \sum_{t=0}^{T-1}R_{t}+\frac{2\eta^{4}\sigma^{2}}{\alpha}+\frac{12\eta^{4}}{ \alpha^{2}T}\widetilde{P}_{0}\] \[\leq \frac{12\cdot 19\widetilde{L}^{2}}{\alpha^{2}}\frac{1}{T}\sum_{t=0}^{T-1 }R_{t}+\frac{12\cdot 7\eta^{3}\sigma^{2}}{\alpha^{2}}+\frac{12\cdot 3\widetilde{L}^{2} \eta^{2}}{\alpha^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+\frac{12\eta^{5}\sigma ^{2}}{\alpha^{2}}\] \[\quad+\frac{12\widetilde{L}^{2}\eta^{4}}{\alpha^{2}}\frac{1}{T} \sum_{t=0}^{T-1}R_{t}+\frac{2\eta^{4}\sigma^{2}}{\alpha}+\frac{12\eta^{4}}{ \alpha^{2}T}\widetilde{P}_{0}\] \[\leq \frac{276\widetilde{L}^{2}}{\alpha^{2}}\frac{1}{T}\sum_{t=0}^{T-1 }R_{t}+\frac{84\eta^{3}\sigma^{2}}{\alpha^{2}}+\frac{12\eta^{5}\sigma^{2}}{ \alpha^{2}}+\frac{2\eta^{4}\sigma^{2}}{\alpha}+\frac{12\eta^{4}}{\alpha^{2}T} \widetilde{P}_{0}\]

**Part IV. Combining steps I, II and III with descent lemma.** By smoothness (Assumption 1) of \(f(\cdot)\) it follows from Lemma 1 that for any \(\gamma\leq 1/(2L)\) we have

\[f(x^{t+1}) \leq f(x^{t})-\frac{\gamma}{2}\left\|\nabla f(x^{t})\right\|^{2}- \frac{1}{4\gamma}\left\|x^{t+1}-x^{t}\right\|^{2}+\frac{\gamma}{2}\left\|g^{t }-\nabla f(x^{t})\right\|^{2}\] \[\leq f(x^{t})-\frac{\gamma}{2}\left\|\nabla f(x^{t})\right\|^{2}- \frac{1}{4\gamma}\left\|x^{t+1}-x^{t}\right\|^{2}\] \[\quad+\frac{3\gamma}{2}\frac{1}{n}\sum_{i=1}^{n}\left\|g_{i}^{t}- u_{i}^{t}\right\|^{2}+\frac{3\gamma}{2}\left\|u^{t}-v^{t}\right\|^{2}+\frac{3 \gamma}{2}\left\|v^{t}-\nabla f(x^{t})\right\|^{2}.\]

Subtracting \(f^{*}\) from both sides of (3), taking expectation and defining \(\delta_{t}:=\mathbb{E}\left[f(x^{t})-f^{*}\right]\), we derive

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] = \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla f(x^{t}) \right\|^{2}\right]\] \[\leq \frac{2\delta_{0}}{\gamma T}+3\frac{1}{T}\sum_{t=0}^{T-1} \widetilde{V}_{t}+3\frac{1}{T}\sum_{t=0}^{T-1}Q_{t}+3\frac{1}{T}\sum_{t=0}^{T- 1}P_{t}-\frac{1}{2\gamma^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}\] \[\stackrel{{(i)}}{{\leq}} \frac{2\delta_{0}}{\gamma T}+\frac{3\cdot 276\widetilde{L}^{2}}{ \alpha^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+\frac{3\cdot 84\eta^{3}\sigma^{2}}{ \alpha^{2}}+\frac{3\cdot 12\eta^{5}\sigma^{2}}{\alpha^{2}}+\frac{3\cdot 2\eta^{4} \sigma^{2}}{\alpha}\] \[\quad+\frac{3\cdot 19L^{2}}{\eta^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+ \frac{3\cdot 7\eta\sigma^{2}}{n}\] \[\quad+\frac{3L^{2}}{\eta^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+ \frac{\eta\sigma^{2}}{n}-\frac{1}{2\gamma^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}\] \[\quad+\frac{36\eta^{4}}{\alpha^{2}T}\widetilde{P}_{0}+\frac{18}{ \eta T}P_{0}+\frac{3}{\eta T}P_{0}\] \[= \frac{2\delta_{0}}{\gamma T}+\frac{3\cdot 84\eta^{3}\sigma^{2}}{ \alpha^{2}}+\frac{3\cdot 12\eta^{5}\sigma^{2}}{\alpha^{2}}+\frac{3\cdot 2\eta^{4} \sigma^{2}}{\alpha}+\frac{22\eta\sigma^{2}}{n}\] \[\quad+\left(\frac{60L^{2}}{\eta^{2}}+\frac{3\cdot 276\widetilde{L}^{2} }{\alpha^{2}}-\frac{1}{2\gamma^{2}}\right)\frac{1}{T}\sum_{t=0}^{T-1}R_{t}\] \[\quad+\frac{36\eta^{4}}{\alpha^{2}T}\widetilde{P}_{0}+\frac{21}{ \eta T}P_{0}\] \[= \frac{2\delta_{0}}{\gamma T}+\frac{288\eta^{3}\sigma^{2}}{\alpha^{2 }}+\frac{6\eta^{4}\sigma^{2}}{\alpha}+\frac{22\eta\sigma^{2}}{n}+\frac{36\eta^{4 }}{\alpha^{2}T}\widetilde{P}_{0}+\frac{21}{\eta T}P_{0}.\]where \((i)\) holds due to (30), (35) and (33), the last two steps hold because of the assumption on the step-size, and \(\eta\leq 1\), which completes the proof of the first part of Theorem.

Notice that it suffices to take the same initial batch-size as in the proof of the Theorem 3 in order to "remove" \(\widetilde{P}_{0}\) and \(P_{0}\) terms, since the power of \(\eta\) in front of \(\widetilde{P}_{0}\) is larger here compared to the proof of Theorem 3. The choice of the momentum parameter such that \(\eta\leq\left(\frac{L\delta_{0}\alpha^{2}}{\sigma^{2}T}\right)^{\nicefrac{{1}}{ {4}}}\), \(\eta\leq\left(\frac{L\delta_{0}n}{\sigma^{2}T}\right)^{\nicefrac{{1}}{{2}}}\) ensures that \(\frac{\eta^{3}\sigma^{2}}{\alpha^{2}}\leq\frac{L\delta_{0}}{\eta T}\), and \(\frac{\eta\sigma^{2}}{n}\leq\frac{L\delta_{0}}{\eta T}\). Therefore, we can guarantee that the choice \(\eta=\min\left\{\frac{\alpha\sqrt{L\delta_{0}B_{\text{out}}}}{\sigma},\left( \frac{L\delta_{0}\alpha^{2}}{\sigma^{2}T}\right)^{\nicefrac{{1}}{{4}}},\left( \frac{L\delta_{0}n}{\sigma^{2}T}\right)^{\nicefrac{{1}}{{2}}}\right\}\) ensures that

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] \leq \mathcal{O}\left(\frac{\widetilde{L}\delta_{0}}{\alpha T}+\left( \frac{L\delta_{0}\sigma^{2/3}}{\alpha^{2/3}T}\right)^{\nicefrac{{3}}{{4}}}+ \left(\frac{L\delta_{0}\sigma^{2}}{nT}\right)^{\nicefrac{{1}}{{2}}}\right).\]

### Controlling the error of second momentum estimator

**Lemma 4**.: _Let Assumption 1 be satisfied, and suppose \(0<\eta\leq 1\). For every \(i=1,\ldots,n\), let the sequences \(\left\{v_{i}^{t}\right\}_{t\geq 0}\) and \(\left\{u_{i}^{t}\right\}_{t\geq 0}\) be updated via_

\[v_{i}^{t}=v_{i}^{t-1}+\eta\left(\nabla f_{i}(x^{t},\xi_{i}^{t})-v_{i}^{t-1} \right),\]

\[u_{i}^{t}=u_{i}^{t-1}+\eta\left(v_{i}^{t}-u_{i}^{t-1}\right).\]

_Define the sequences \(v^{t}:=\frac{1}{n}\sum_{i=1}^{n}v_{i}^{t}\) and \(u^{t}:=\frac{1}{n}\sum_{i=1}^{n}u_{i}^{t}\). Then for every \(i=1,\ldots,n\) and \(t\geq 0\) it holds_

\[\mathbb{E}\left[\left\|u_{i}^{t}-v_{i}^{t}\right\|^{2}\right]\leq (1-\eta)\mathbb{E}\left[\left\|u_{i}^{t-1}-v_{i}^{t-1}\right\|^{2}\right]+6 \eta\mathbb{E}\left[\left\|v_{i}^{t-1}-\nabla f_{i}(x^{t-1})\right\|^{2}\right]\] \[+6L_{i}^{2}\eta\mathbb{E}\left[\left\|x^{t}-x^{t-1}\right\|^{2} \right]+\eta^{2}\sigma^{2},\] (37)

\[\mathbb{E}\left[\left\|u^{t}-v^{t}\right\|^{2}\right]\leq(1-\eta) \mathbb{E}\left[\left\|u^{t-1}-v^{t-1}\right\|^{2}\right]+6\eta\mathbb{E} \left[\left\|v^{t-1}-\nabla f(x^{t-1})\right\|^{2}\right]\] \[+6L^{2}\eta\mathbb{E}\left[\left\|x^{t}-x^{t-1}\right\|^{2} \right]+\frac{\eta^{2}\sigma^{2}}{n}.\] (38)

Proof.: By the update rule of \(v_{i}^{t}\), we have

\[\mathbb{E}\left[\left\|u_{i}^{t}-v_{i}^{t}\right\|^{2}\right] = \mathbb{E}\left[\left\|u_{i}^{t-1}-v_{i}^{t}+\eta(v_{i}^{t}-u_{i }^{t-1})\right\|^{2}\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\left\|v_{i}^{t}-u_{i}^{t-1}\right\|^ {2}\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\left\|(1-\eta)v_{i}^{t-1}+\eta \nabla f_{i}(x^{t},\xi_{i}^{t})-u_{i}^{t-1}\right\|^{2}\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\left\|(u_{i}^{t-1}-v_{i}^{t-1})+\eta (v_{i}^{t-1}-\nabla f_{i}(x^{t},\xi_{i}^{t}))\right\|^{2}\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\left\|(u_{i}^{t-1}-v_{i}^{t-1})+\eta (v_{i}^{t-1}-\nabla f_{i}(x^{t}))+\eta(\nabla f_{i}(x^{t})-\nabla f_{i}(x^{t}, \xi_{i}^{t}))\right\|^{2}\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\mathbb{E}_{\xi_{i}^{t}}\left[\left\|( u_{i}^{t-1}-v_{i}^{t-1})+\eta(v_{i}^{t-1}-\nabla f_{i}(x^{t}))+\eta(\nabla f_{i}(x^{t})- \nabla f_{i}(x^{t},\xi_{i}^{t}))\right\|^{2}\right]\right]\] \[= (1-\eta)^{2}\left(\mathbb{E}\left[\left\|u_{i}^{t-1}-v_{i}^{t-1}+ \eta(v_{i}^{t-1}-\nabla f_{i}(x^{t}))\right\|^{2}\right]+\eta^{2}\mathbb{E} \left[\left\|\nabla f_{i}(x^{t},\xi_{i}^{t})-\nabla f_{i}(x^{t})\right\|^{2} \right]\right)\] \[\leq (1-\eta)^{2}\mathbb{E}\left[\left\|u_{i}^{t-1}-v_{i}^{t-1}+\eta (v_{i}^{t-1}-\nabla f_{i}(x^{t}))\right\|^{2}\right]+\eta^{2}\sigma^{2}\] \[\leq (1-\eta)^{2}\left(1+\frac{\eta}{2}\right)\mathbb{E}\left[\left\|u_ {i}^{t-1}-v_{i}^{t-1}\right\|^{2}\right]\] \[+\left(1+\frac{2}{\eta}\right)\eta^{2}\mathbb{E}\left[\left\|v_{i} ^{t-1}-\nabla f_{i}(x^{t})\right\|^{2}\right]+\eta^{2}\sigma^{2}\]\[\leq (1-\eta)\mathbb{E}\left[\left\|u^{t-1}-v^{t-1}\right\|^{2}\right]+3 \eta\mathbb{E}\left[\left\|v^{t-1}_{i}-\nabla f_{i}(x^{t})\right\|^{2}\right]+ \eta^{2}\sigma^{2}\] \[\leq (1-\eta)\mathbb{E}\left[\left\|u^{t-1}-v^{t-1}\right\|^{2}\right] +6\eta\mathbb{E}\left[\left\|v^{t-1}_{i}-\nabla f_{i}(x^{t-1})\right\|^{2}\right]\] \[\qquad+6\eta\mathbb{E}\left[\left\|\nabla f_{i}(x^{t})-\nabla f_{ i}(x^{t-1})\right\|^{2}\right]+\eta^{2}\sigma^{2}\] \[\leq (1-\eta)\mathbb{E}\left[\left\|u^{t-1}-v^{t-1}\right\|^{2}\right] +6\eta\mathbb{E}\left[\left\|v^{t-1}_{i}-\nabla f_{i}(x^{t-1})\right\|^{2}\right]\] \[\qquad+6L_{i}^{2}\eta\mathbb{E}\left[\left\|x^{t}-x^{t-1}\right\|^ {2}\right]+\eta^{2}\sigma^{2},\]

where the first inequality holds Assumption 2, the second inequality holds by Young's inequality, and the last step uses smoothness of \(f_{i}(\cdot)\) (Assumption 1), which concludes the proof of (37).

For each \(t=0,\ldots,T-1\), define a random vector \(\xi^{t}:=(\xi^{t}_{1},\ldots,\xi^{t}_{n})\) and denote by \(\nabla f(x^{t},\xi^{t+1}):=\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(x^{t},\xi^{t} _{i})\). Note that the entries of the random vector \(\xi^{t}\) are independent and \(\mathbb{E}_{\xi^{t}}\left[\nabla f(x^{t},\xi^{t})\right]=\nabla f(x^{t})\), then we have

\[v^{t}=v^{t-1}+\eta\left(\nabla f(x^{t},\xi^{t})-v^{t-1}\right),\]

\[u^{t}_{i}=u^{t-1}_{i}+\eta\left(v^{t}_{i}-u^{t-1}_{i}\right),\]

where \(v^{t}:=\frac{1}{n}\sum_{i=1}^{n}v^{t}_{i}\), \(u^{t}:=\frac{1}{n}\sum_{i=1}^{n}u^{t}_{i}\) are auxiliary sequences. Therefore, we can similarly derive

\[\mathbb{E}\left[\left\|u^{t}-v^{t}\right\|^{2}\right] = \mathbb{E}\left[\left\|u^{t-1}-v^{t}+\eta(v^{t}-u^{t-1})\right\| ^{2}\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\left\|v^{t}-u^{t-1}\right\|^{2}\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\left\|(1-\eta)v^{t-1}+\eta\nabla f(x ^{t},\xi^{t})-u^{t-1}\right\|^{2}\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\left\|(u^{t-1}-v^{t-1})+\eta(v^{t-1 }-\nabla f(x^{t}))\right\|^{2}\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\left\|(u^{t-1}-v^{t-1})+\eta(v^{t-1 }-\nabla f(x^{t}))+\eta(\nabla f(x^{t})-\nabla f(x^{t},\xi^{t}))\right\|^{2}\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\left\|u^{t-1}-v^{t-1}+\eta(v^{t-1}- \nabla f(x^{t}))\right\|^{2}\right]+\eta^{2}\mathbb{E}\left[\left\|\nabla f( x^{t},\xi^{t})-\nabla f(x^{t})\right\|^{2}\right]\right)\] \[\leq (1-\eta)^{2}\mathbb{E}\left[\left\|u^{t-1}-v^{t-1}+\eta(v^{t-1}- \nabla f(x^{t}))\right\|^{2}\right]+\frac{\eta^{2}\sigma^{2}}{n}\] \[\leq (1-\eta)^{2}\left(1+\frac{\eta}{2}\right)\mathbb{E}\left[\left\|u ^{t-1}-v^{t-1}\right\|^{2}\right]\] \[\qquad+\left(1+\frac{2}{\eta}\right)\eta^{2}\mathbb{E}\left[ \left\|v^{t-1}-\nabla f(x^{t})\right\|^{2}\right]+\frac{\eta^{2}\sigma^{2}}{n}\] \[\leq (1-\eta)\mathbb{E}\left[\left\|u^{t-1}-v^{t-1}\right\|^{2}\right] +3\eta\mathbb{E}\left[\left\|v^{t-1}-\nabla f(x^{t})\right\|^{2}\right]+\frac {\eta^{2}\sigma^{2}}{n}\] \[\leq (1-\eta)\mathbb{E}\left[\left\|u^{t-1}-v^{t-1}\right\|^{2}\right] +6\eta\mathbb{E}\left[\left\|v^{t-1}-\nabla f(x^{t-1})\right\|^{2}\right]\] \[\qquad+6\eta\mathbb{E}\left[\left\|\nabla f(x^{t})-\nabla f(x^{t-1 })\right\|^{2}\right]+\frac{\eta^{2}\sigma^{2}}{n}\] \[\leq (1-\eta)\mathbb{E}\left[\left\|u^{t-1}-v^{t-1}\right\|^{2}\right] +6\eta\mathbb{E}\left[\left\|v^{t-1}-\nabla f(x^{t-1})\right\|^{2}\right]\] \[\qquad+6L^{2}\eta\mathbb{E}\left[\left\|x^{t}-x^{t-1}\right\|^{2} \right]+\frac{\eta^{2}\sigma^{2}}{n},\]

where the first inequality holds Assumption 2, the second inequality holds by Young's inequality, and the last step uses smoothness of \(f(\cdot)\) (Assumption 1), which concludes the proof of (38).

### Controlling the error of contractive compression and double momentum estimator

**Lemma 5**.: _Let Assumption 1 be satisfied, and suppose \(\mathcal{C}\) is a contractive compressor. For every \(i=1,\ldots,n\), let the sequences \(\left\{v_{i}^{t}\right\}_{t\geq 0}\), \(\left\{u_{i}^{t}\right\}_{t\geq 0}\), and \(\left\{g_{i}^{t}\right\}_{t\geq 0}\) be updated via_

\[v_{i}^{t} = v_{i}^{t-1}+\eta\left(\nabla f_{i}(x^{t},\xi_{i}^{t})-v_{i}^{t-1} \right),\] \[u_{i}^{t} = u_{i}^{t-1}+\eta\left(v_{i}^{t}-u_{i}^{t-1}\right),\] \[g_{i}^{t} = g_{i}^{t-1}+\mathcal{C}\left(u_{i}^{t}-g_{i}^{t-1}\right).\]

_Then for every \(i=1,\ldots,n\) and \(t\geq 0\) it holds_

\[\mathbb{E}\left[\left\|g_{i}^{t}-u_{i}^{t}\right\|^{2}\right] \leq \left(1-\frac{\alpha}{2}\right)\mathbb{E}\left[\left\|g_{i}^{t-1} -u_{i}^{t-1}\right\|^{2}\right]+\frac{6\eta^{2}}{\alpha}\mathbb{E}\left[ \left\|u_{i}^{t-1}-v_{i}^{t-1}\right\|^{2}\right]\] \[\quad+\frac{6\eta^{4}}{\alpha}\mathbb{E}\left[\left\|v_{i}^{t-1}- \nabla f_{i}(x^{t-1},\xi_{i}^{t-1})\right\|^{2}\right]+\frac{6L_{i}^{2}\eta^{ 4}}{\alpha}\mathbb{E}\left[\left\|x^{t}-x^{t-1}\right\|^{2}\right]+\eta^{4} \sigma^{2}.\]

Proof.: By the update rules of \(g_{i}^{t}\), \(u_{i}^{t}\) and \(v_{i}^{t}\), we derive

\[\mathbb{E}\left[\left\|g_{i}^{t}-u_{i}^{t}\right\|^{2}\right] = \mathbb{E}\left[\left\|g_{i}^{t-1}-u_{i}^{t}+\mathcal{C}(u_{i}^{ t}-g_{i}^{t-1})\right\|^{2}\right]\] \[\overset{(i)}{\leq} (1-\alpha)\mathbb{E}\left[\left\|u_{i}^{t}-g_{i}^{t-1}\right\|^{2}\right]\] \[\overset{(ii)}{=} (1-\alpha)\mathbb{E}\left[\left\|u_{i}^{t-1}-g_{i}^{t-1}+\eta(v_{ i}^{t-1}-u_{i}^{t-1})+\eta^{2}(\nabla f_{i}(x^{t},\xi_{i}^{t})-v_{i}^{t-1}) \right\|^{2}\right]\] \[= (1-\alpha)\mathbb{E}\big{[}\|u_{i}^{t-1}-g_{i}^{t-1}+\eta(v_{i}^{ t-1}-u_{i}^{t-1})+\eta^{2}(\nabla f_{i}(x^{t})-v_{i}^{t-1})\] \[\quad+\eta^{2}(\nabla f_{i}(x^{t},\xi_{i}^{t})-\nabla f_{i}(x^{t }))\|^{2}\big{]}\] \[= (1-\alpha)\mathbb{E}\big{[}\|\mathbb{E}_{\xi_{i}^{t}}\big{[}\|u_ {i}^{t-1}-g_{i}^{t-1}+\eta(v_{i}^{t-1}-u_{i}^{t-1})+\eta^{2}(\nabla f_{i}(x^{t })-v_{i}^{t-1})\] \[\quad+\eta^{2}(\nabla f_{i}(x^{t},\xi_{i}^{t})-\nabla f_{i}(x^{t }))\|^{2}\big{]}\] \[= (1-\alpha)\mathbb{E}\left[\left\|u_{i}^{t-1}-g_{i}^{t-1}+\eta(v_{ i}^{t-1}-u_{i}^{t-1})+\eta^{2}(\nabla f_{i}(x^{t})-v_{i}^{t-1})\right\|^{2}\right]\] \[\quad+(1-\alpha)\eta^{4}\mathbb{E}\left[\left\|\nabla f_{i}(x^{t },\xi_{i}^{t})-\nabla f_{i}(x^{t})\right\|^{2}\right]\] \[\overset{(iii)}{\leq} (1-\alpha)(1+\rho)\mathbb{E}\left[\left\|u_{i}^{t-1}-g_{i}^{t-1} \right\|^{2}\right]\] \[\quad+(1-\alpha)(1+\rho^{-1})\mathbb{E}\left[\left\|\eta(v_{i}^ {t-1}-u_{i}^{t-1})+\eta^{2}(\nabla f_{i}(x^{t})-v_{i}^{t-1})\right\|^{2}\right]\] \[\quad+\eta^{4}\sigma^{2}\] \[\overset{(iv)}{=} (1-\theta)\mathbb{E}\left[\left\|u_{i}^{t-1}-g_{i}^{t-1}\right\|^ {2}\right]+\eta^{4}\sigma^{2}\] \[\quad+\beta\mathbb{E}\left[\left\|\eta(v_{i}^{t-1}-u_{i}^{t-1})+ \eta^{2}(\nabla f_{i}(x^{t-1})-v_{i}^{t-1})+\eta^{2}(\nabla f_{i}(x^{t})- \nabla f_{i}(x^{t-1})\right\|^{2}\right]\] \[\overset{(v)}{\leq} (1-\theta)\mathbb{E}\left[\left\|u_{i}^{t-1}-g_{i}^{t-1}\right\|^ {2}\right]+3\beta\eta^{2}\mathbb{E}\left[\left\|v_{i}^{t-1}-u_{i}^{t-1}\right\|^ {2}\right]\] \[\quad+3\beta\eta^{4}\mathbb{E}\left[\left\|v_{i}^{t-1}-\nabla f_{ i}(x^{t-1})\right\|^{2}\right]\] \[\quad+3\beta\eta^{4}\mathbb{E}\left[\left\|v_{i}^{t-1}-\nabla f _{i}(x^{t-1})\right\|^{2}\right]+\eta^{4}\sigma^{2}\] \[\leq (1-\theta)\mathbb{E}\left[\left\|u_{i}^{t-1}-g_{i}^{t-1}\right\|^ {2}\right]+3\beta\eta^{2}\mathbb{E}\left[\left\|v_{i}^{t-1}-u_{i}^{t-1}\right\|^ {2}\right]\] \[\quad+3\beta\eta^{4}\mathbb{E}\left[\left\|v_{i}^{t-1}-\nabla f_{ i}(x^{t-1})\right\|^{2}\right]\] \[\quad+3\beta L_{i}^{2}\eta^{4}\mathbb{E}\left[\left\|x^{t}-x^{t-1} \right\|^{2}\right]+\eta^{4}\sigma^{2}\]

where \((i)\) is due to definition of a contractive compressor (Definition 1), \((ii)\) follows by the update rule of \(v_{i}^{t}\) and \(u_{i}^{t}\), \((iii)\) and \((v)\) hold by Young's inequality for any \(\rho>0\). In \((iv)\), we introducedthe notation \(\theta:=1-(1-\alpha)(1+\rho)\), and \(\beta:=(1-\alpha)(1+\rho^{-1})\). The last step follows by smoothness of \(f_{i}(\cdot)\) (Assumption 1). The proof is complete by the choice \(\rho=\alpha/2\), which guarantees \(1-\theta\leq 1-\alpha/2\), and \(3\beta\leq 6/\alpha\).

EF21-Sgdm with Absolute Compressor

In this section, we complement our theory by analyzing \(\mathsf{EF21\text{-}SGDM}\) under a different class of widely used biased compressors, namely, absolute compressors, which are defined as follows.

**Definition 2** (Absolute compressors).: _We say that a (possibly randomized) map \(\mathcal{C}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) is an absolute compression operator if there exists a constant \(\Delta>0\) such that_

\[\mathbb{E}\left[\left\|\mathcal{C}(x)-x\right\|^{2}\right]\leq \Delta^{2},\qquad\forall x\in\mathbb{R}^{d}.\] (40)

This class includes important examples of compressors such as hard-threshold sparsifier [15], (stochastic) rounding schemes with bounded error [12] and scaled integer rounding [12].

```
1:Input: starting point \(x^{0}\), step-size \(\gamma>0\), momentum \(\eta\in(0,1]\), initial batch size \(B_{\text{init}}\)
2: Initialize \(v_{i}^{0}=g_{i}^{0}=\frac{1}{B_{\text{init}}}\sum_{j=1}^{B_{\text{init}}} \nabla f_{i}(x^{0},\xi_{i,j}^{0})\) for \(i=1,\ldots,n\); \(g^{0}=\frac{1}{n}\sum_{i=1}^{n}g_{i}^{0}\)
3:for\(t=0,1,2,\ldots,T-1\)do
4: Master computes \(x^{t+1}=x^{t}-\gamma g^{t}\) and broadcasts \(x^{t+1}\) to all nodes
5:for all nodes \(i=1,\ldots,n\)in paralleldo
6: Compute momentum estimator \(v_{i}^{t+1}=(1-\eta)v_{i}^{t}+\eta\nabla f_{i}(x^{t+1},\xi_{i}^{t+1})\)
7: Compress \(c_{i}^{t+1}=\mathcal{C}\left(\frac{v_{i}^{t+1}-g_{i}^{t}}{\gamma}\right)\) and send \(c_{i}^{t+1}\) to the master
8: Update local state \(g_{i}^{t+1}=g_{i}^{t}+\gamma c_{i}^{t+1}\)
9:endfor
10: Master computes \(g^{t+1}=\frac{1}{n}\sum_{i=1}^{n}g_{i}^{t+1}\) via \(g^{t+1}=g^{t}+\frac{1}{n}\sum_{i=1}^{n}\gamma c_{i}^{t+1}\)
11:endfor ```

**Algorithm 4**\(\mathsf{EF21\text{-}SGDM}\) (abs)

To accomodate absolute compressors into our \(\mathsf{EF21\text{-}SGDM}\) method, we need to make a slight modification to our algorithm, see Algorithm 4. At each iteration, before compressing the difference \(v_{i}^{t+1}-g_{i}^{t}\), we divide it by the step-size \(\gamma\). Later, we multiply the compressed vector \(c_{i}^{t+1}\) by \(\gamma\), i.e., have

\[g_{i}^{t+1}=g_{i}^{t}+\gamma\,\mathcal{C}\left(\frac{v_{i}^{t+1}-g_{i}^{t}}{ \gamma}\right).\]

Such modification is necessary for absolute compressors because by Definition 2 the compression error is not proportional to \(\|x\|^{2}\), but merely an absolute constant \(\Delta^{2}\). In fact, Algorithm 4 is somewhat more universal in the sense that it can be also applied for contractive compressors.20 We derive the following result for \(\mathsf{EF21\text{-}SGDM}\) (abs).

Footnote 20: It is straightforward to modify the proof of our Theorem 3 for the case when Algorithm 4 is applied with a contractive compressor.

**Theorem 6**.: _Let Assumptions 1 and 2 hold. Let \(\hat{x}^{T}\) be sampled uniformly at random from the iterates of the method. Let Algorithm 4 run with an absolute compressor (Definition 2). For all \(\eta\in(0,1]\) and \(B_{\text{init}}\geq 1\), with \(\gamma\leq\frac{\eta}{4L},\) we have_

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] \leq\mathcal{O}\left(\frac{\Psi_{0}}{\gamma T}+\gamma^{2}\Delta^{2}+\frac{ \eta\sigma^{2}}{n}\right),\] (41)

_where \(\Psi_{0}:=\delta_{0}+\frac{\gamma}{\eta}\mathbb{E}\left[\left\|v^{0}-\nabla f (x^{0})\right\|^{2}\right]\) is a Lyapunov function. With the following step-size, momentum parameter, and initial batch size_

\[\gamma=\frac{\eta}{4L},\qquad\eta=\min\left\{1,\left(\frac{L^{3} \delta_{0}}{\Delta^{2}T}\right)^{\nicefrac{{1}}{{3}}},\left(\frac{L\delta_{0}n }{\sigma^{2}T}\right)^{\nicefrac{{1}}{{2}}}\right\},\qquad B_{init}=\frac{ \sigma^{2}}{L\delta_{0}n}\] (42)

_we have_

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] \leq \mathcal{O}\left(\frac{L\delta_{0}}{T}+\left(\frac{\delta_{0} \Delta}{T}\right)^{\nicefrac{{2}}{{3}}}+\left(\frac{L\delta_{0}\sigma^{2}}{ nT}\right)^{\nicefrac{{1}}{{2}}}\right).\]

**Corollary 4**.: _Under the setting of Theorem 6, we have \(\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|\right]\leq\varepsilon\) after \(T=\mathcal{O}\left(\frac{L\delta_{0}}{\varepsilon^{2}}+\frac{\Delta\delta_{0}} {\varepsilon^{3}}+\frac{\sigma^{2}L\delta_{0}}{\pi e^{4}}\right)\) iterations._

**Remark 3**.: _The sample complexity result in Corollary 4 matches the one derived for DoubleSqueeze algorithm [13], which is different from Algorithm 4._

Proof.: Similarly to the proof of Theorem 3, we control the error between \(g^{t}\) and \(\nabla f(x^{t})\) by decomposing it into two terms

\[\left\|g^{t}-\nabla f(x^{t})\right\|^{2}\leq 2\left\|g^{t}-v^{t}\right\|^{2}+2 \left\|v^{t}-\nabla f(x^{t})\right\|^{2}\leq 2\frac{1}{n}\sum_{i=1}^{n}\left\|g _{i}^{t}-v_{i}^{t}\right\|^{2}+2\left\|v^{t}-\nabla f(x^{t})\right\|^{2}.\]

Again, for the second term above we can use the recursion developed for momentum estimator Lemma 2. However, since we use a different compressor here, we need to bound \(\left\|g_{i}^{t}-v_{i}^{t}\right\|^{2}\) term differently, thus we invoke Lemma 6 for absolute compressor.

**Part I. Controlling the error of momentum estimator on average for \(v^{t}\).** Denote \(P_{t}:=\mathbb{E}\left[\left\|v^{t}-\nabla f(x^{t})\right\|^{2}\right]\), \(R_{t}:=\mathbb{E}\left[\left\|x^{t}-x^{t+1}\right\|^{2}\right]\). Similarly to Part I of the proof of Theorem 3, we have by Lemma 2

\[\frac{1}{T}\sum_{t=0}^{T-1}P_{t}\leq\frac{3L^{2}}{\eta^{2}}\frac{1}{T}\sum_{t= 0}^{T-1}R_{t}+\frac{\eta\sigma^{2}}{n}+\frac{1}{\eta T}P_{0}.\] (43)

**Part II. Controlling the error of absolute compressor and momentum estimator.** By Lemma 6 we have for any \(0<\eta\leq 1\) and \(t\geq 0\)

\[\widetilde{V}_{t}:=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|g_{i}^{t}- v_{i}^{t}\right\|^{2}\right]\leq\gamma^{2}\Delta^{2}.\] (44)

**Part III. Combining steps I and II with descent lemma.** By smoothness (Assumption 1) of \(f(\cdot)\) it follows from Lemma 1 that for any \(\gamma\leq 1/(2L)\) we have

\[f(x^{t+1}) \leq f(x^{t})-\frac{\gamma}{2}\left\|\nabla f(x^{t})\right\|^{2}- \frac{1}{4\gamma}\left\|x^{t+1}-x^{t}\right\|^{2}+\frac{\gamma}{2}\left\|g^{t }-\nabla f(x^{t})\right\|^{2}\] \[\leq f(x^{t})-\frac{\gamma}{2}\left\|\nabla f(x^{t})\right\|^{2}- \frac{1}{4\gamma}\left\|x^{t+1}-x^{t}\right\|^{2}+\gamma\widetilde{V}_{t}+ \gamma P_{t}.\]

Subtracting \(f^{*}\) from both sides of (45), taking expectation and defining \(\delta_{t}:=\mathbb{E}\left[f(x^{t})-f^{*}\right]\), we derive

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] = \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla f(x^{t} )\right\|^{2}\right]\] \[\leq \frac{2\delta_{0}}{\gamma T}+2\frac{1}{T}\sum_{t=0}^{T-1} \widetilde{V}_{t}+2\frac{1}{T}\sum_{t=0}^{T-1}P_{t}-\frac{1}{2\gamma^{2}} \frac{1}{T}\sum_{t=0}^{T-1}R_{t}\] \[\stackrel{{(i)}}{{\leq}} \frac{2\delta_{0}}{\gamma T}+2\gamma^{2}\Delta^{2}+2\frac{1}{T} \sum_{t=0}^{T-1}P_{t}-\frac{1}{2\gamma^{2}}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}\] \[\stackrel{{(ii)}}{{\leq}} \frac{2\delta_{0}}{\gamma T}+2\gamma^{2}\Delta^{2}+\left(\frac{6 L^{2}}{\eta^{2}}-\frac{1}{2\gamma^{2}}\right)\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+ \frac{2\eta\sigma^{2}}{n}+\frac{1}{\eta T}P_{0}\] \[\leq \frac{2\delta_{0}}{\gamma T}+2\gamma^{2}\Delta^{2}+\frac{2\eta \sigma^{2}}{n}+\frac{1}{\eta T}P_{0}\]

where in \((i)\) and \((ii)\) we apply (43), (44), and in the last step we use the assumption on the step-size \(\gamma\leq\eta/(4L)\).

Setting \(\gamma=\frac{\eta}{4L}\), and taking \(\eta\leq\left(\frac{L^{3}\delta_{0}}{\Delta^{2}T}\right)^{1/\delta_{0}}\) we can ensure that \(\frac{\eta^{2}\Delta^{2}}{L^{2}}\leq\frac{L\delta_{0}}{\eta T}\), since \(\eta\leq\left(\frac{L\delta_{0}\eta}{\sigma^{2}T}\right)^{1/2}\) we have \(\frac{\eta\sigma^{2}}{n}\leq\frac{L\delta_{0}}{\eta T}\). Finally, by setting the initial batch-size to \(B_{init}=\frac{\sigma^{2}}{L\delta_{0}n}\), we have \(\frac{1}{\eta T}P_{0}=\frac{\sigma^{2}}{\eta TnB_{init}}\leq\frac{L\delta_{0}} {\eta T}\). Therefore, we derive

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla f(x^{t}) \right\|^{2}\right] \leq \frac{2\delta_{0}}{\gamma T}+2\gamma^{2}\Delta^{2}+\frac{2\eta \sigma^{2}}{n}+\frac{1}{\eta T}P_{0}\] \[= \frac{8L\delta_{0}}{\eta T}+\frac{\eta^{2}\Delta^{2}}{8L^{2}}+ \frac{2\eta\sigma^{2}}{n}+\frac{\sigma^{2}}{\eta TB_{init}}\] \[= \mathcal{O}\left(\frac{L\delta_{0}}{T}+\frac{\delta_{0}^{2/3} \Delta^{2/3}}{T^{2/3}}+\frac{\sigma(L\delta_{0})^{1/2}}{(nT)^{1/2}}\right).\]

### Controlling the error of absolute compression

**Lemma 6**.: _Let \(\mathcal{C}\) be an absolute compressor and \(g_{i}^{t+1}\) be updated according to Algorithm 4, then for \(t\geq 0\), we have \(\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|g_{i}^{t}-v_{i}^{t}\right\|^{2 }\right]\leq\gamma^{2}\Delta^{2}\)._

Proof.: By the update rule for \(g_{i}^{t+1}\) in Algorithm 4 and Definition 2, we can bound

\[\mathbb{E}\left[\left\|g_{i}^{t+1}-v_{i}^{t+1}\right\|^{2}\right] = \mathbb{E}\left[\left\|\gamma\mathcal{C}\left(\frac{v_{i}^{t+1}- g_{i}^{t}}{\gamma}\right)-(v_{i}^{t+1}-g_{i}^{t})\right\|^{2}\right]\] \[= \gamma^{2}\mathbb{E}\left[\left\|\mathcal{C}\left(\frac{v_{i}^{t +1}-g_{i}^{t}}{\gamma}\right)-\frac{v_{i}^{t+1}-g_{i}^{t}}{\gamma}\right\|^{2 }\right]\leq\gamma^{2}\Delta^{2}.\]

## Appendix I EF21-Storm/Mvr

```
1:Input:\(x^{0}\), step-size \(\gamma>0\), parameter \(\eta\in(0,1]\), \(B_{\text{init}}\geq 1\)
2: Initialize \(w_{i}^{0}=g_{i}^{0}=\frac{1}{B_{\text{init}}}\sum_{j=1}^{B_{\text{init}}}\nabla f _{i}(x^{0},\xi_{i,j}^{0})\) for \(i=1,\dots,n\); \(g^{0}=\frac{1}{n}\sum_{i=1}^{n}g_{i}^{0}\)
3:for\(t=0,1,2,\dots,T-1\)do
4: Master computes \(x^{t+1}=x^{t}-\gamma g^{t}\) and broadcasts \(x^{t+1}\) to all nodes
5:for all nodes \(i=1,\dots,n\)in parallel do
6: Draw \(\xi_{i}^{t+1}\) and compute two (stochastic) gradients \(\nabla f_{i}(x^{t},\xi_{i}^{t+1})\) and \(\nabla f_{i}(x^{t+1},\xi_{i}^{t+1})\)
7: Compute variance reduced STORM/MVR estimator
8:\(w_{i}^{t+1}=\nabla f_{i}(x^{t+1},\xi_{i}^{t+1})+(1-\eta)(w_{i}^{t}-\nabla f_{ i}(x^{t},\xi_{i}^{t+1}))\)
9: Compress \(c_{i}^{t+1}=\mathcal{C}(w_{i}^{t+1}-g_{i}^{t})\) and send \(c_{i}^{t+1}\) to the master
10: Update local state \(g_{i}^{t+1}=g_{i}^{t}+c_{i}^{t+1}\)
11:endfor
12: Master computes \(g^{t+1}=\frac{1}{n}\sum_{i=1}^{n}g_{i}^{t+1}\) via \(g^{t+1}=g^{t}+\frac{1}{n}\sum_{i=1}^{n}c_{i}^{t+1}\)
13:endfor ```

**Algorithm 5** EF21-STORM/MVR

**Assumption 3** (Individual smoothness21).: _For each \(i=1,\dots,n\), every realization of \(\xi_{i}\sim\mathcal{D}_{i}\), the stochastic gradient \(\nabla f_{i}(x,\!\xi_{i})\) is \(\ell_{i}\)-Lipschitz, i.e., for all \(x,y\in\mathbb{R}^{d}\)_

Footnote 21: This assumption can be also relaxed to so-called expected smoothness.

\[\left\|\nabla f_{i}(x,\!\xi_{i})-\nabla f_{i}(y,\xi_{i})\right\|\leq\ell_{i} \left\|x-y\right\|.\]

_We denote \(\widetilde{\ell}^{2}:=\frac{1}{n}\sum_{i=1}^{n}\ell_{i}^{2}\)_

**Theorem 7**.: _Let Assumptions 1, 2 and 3 hold. Let \(\hat{x}^{T}\) be sampled uniformly at random from the iterates of the method. Let Algorithm 5 run with a contractive compressor. For all \(\eta\in(0,1]\) and \(B_{\text{init}}\geq 1\), with \(\gamma\leq\min\left\{\frac{\alpha}{8L},\frac{\sqrt{\alpha}}{6\ell},\frac{\sqrt {n\eta}}{8\widetilde{\ell}}\right\},\) we have_

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right]\leq\mathcal{O} \left(\frac{\Psi_{0}}{\gamma T}+\frac{\eta^{3}\sigma^{2}}{\alpha^{2}}+\frac{ \eta^{2}\sigma^{2}}{\alpha}+\frac{\eta\sigma^{2}}{n}\right),\] (48)

_where \(\Psi_{0}:=\delta_{0}+\frac{\gamma}{\eta}\mathbb{E}\left[\left\|v^{0}-\nabla f (x^{0})\right\|^{2}+\frac{\gamma\eta}{\alpha^{2}}\frac{1}{n}\sum_{i=1}^{n} \mathbb{E}\left[\left\|v_{i}^{0}-\nabla f_{i}(x^{0})\right\|^{2}\right]\). With the following step-size, momentum parameter, and initial batch size_

\[\gamma=\min\left\{\frac{\alpha}{8\widetilde{L}},\frac{\sqrt{\alpha}}{6 \widetilde{\ell}},\frac{\sqrt{n\eta}}{8\widetilde{\ell}}\right\},\quad\eta= \min\left\{\alpha,\left(\frac{\widetilde{\ell}\delta_{0}\alpha^{2}}{\sigma^{2} \sqrt{n}T}\right)^{\!2/\!\gamma},\left(\frac{\widetilde{\ell}\delta_{0}\alpha }{\sigma^{2}\sqrt{n}T}\right)^{\!2/\!\gamma},\left(\frac{\widetilde{\ell}\delta _{0}\sqrt{n}}{\sigma^{2}T}\right)^{\!2/\!3}\right\},\]

_and \(B_{\text{init}}=\max\left\{\frac{\sigma^{2}}{L\delta_{0}n},\frac{\alpha n}{T}\right\}\), we have_

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] \leq \mathcal{O}\left(\frac{\widetilde{L}\delta_{0}}{\alpha T}+\frac{ \widetilde{\ell}\delta_{0}}{\sqrt{\alpha}T}+\left(\frac{\widetilde{\ell} \delta_{0}\sigma^{1/3}}{\alpha^{1/3}\sqrt{n}T}\right)^{\!6/\!\gamma}+\left( \frac{\widetilde{\ell}\delta_{0}\sigma^{1/2}}{\alpha^{1/4}\sqrt{n}T}\right)^{ \!4/\!5}+\left(\frac{\widetilde{\ell}\delta_{0}\sigma}{nT}\right)^{\!2/\!3} \right).\]

**Corollary 5**.: _Under the setting of Theorem 7. we have \(\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|\right]\leq\varepsilon\) after \(T=\mathcal{O}\left(\frac{\widetilde{\ell}\delta_{0}}{\alpha c^{2}}+\frac{ \widetilde{\ell}\delta_{0}\sigma^{1/3}}{\alpha^{1/3}\sqrt{n}e^{\gamma/3}}+ \frac{\widetilde{\ell}\delta_{0}\sigma^{1/2}}{\alpha^{1/4}\sqrt{n}e^{\gamma/2} }+\frac{\widetilde{\ell}\delta_{0}\sigma}{ne^{3}}\right)\) iterations._

Recently, Yau and Wai [2022] propose and analyze a DoCoM-SGT algorithm for decentralized optimization with contractive compressor under the above Assumption 3. When their method is specialized to centralized setting (with mixing constant \(\rho=1\)), their total sample complexity becomes \(\mathcal{O}\left(\frac{\widetilde{\ell}}{\alpha c^{2}}+\frac{n^{4/5}\sigma^{3/2 }}{\alpha^{9/4}\varepsilon^{3/2}}+\frac{\sigma^{3}}{ne^{3}}\right)\) (see Table 1 or Theorem 4.1 in [202]). In contrast, the sample complexity given in our Corollary 5 improves the dependence on \(\sigma\) in the last term and, moreover, achieves the linear speedup in terms of \(n\) for all stochastic terms in the sample complexity.

Proof.: Similarly to the proof of Theorem 3, we control the error between \(g^{t}\) and \(\nabla f(x^{t})\) by decomposing it into two terms

\[\left\|g^{t}-\nabla f(x^{t})\right\|^{2}\leq 2\left\|g^{t}-w^{t}\right\|^{2}+2 \left\|w^{t}-\nabla f(x^{t})\right\|^{2}\leq 2\frac{1}{n}\sum_{i=1}^{n}\left\|g ^{t}_{i}-w^{t}_{i}\right\|^{2}+2\left\|w^{t}-\nabla f(x^{t})\right\|^{2}.\]

In the following, we develop a recursive bound for each term above separately.

**Part I. Controlling the variance of STORM/MVR estimator for each \(w^{t}_{i}\) and on average \(w^{t}\).**

Recall that by Lemma 7-(55), we have for each \(i=1,\ldots,n\), and any \(0<\eta\leq 1\) and \(t\geq 0\)

\[\mathbb{E}\left[\left\|w^{t+1}_{i}-\nabla f_{i}(x^{t+1})\right\|^{2}\right] \leq(1-\eta)\mathbb{E}\left[\left\|w^{t}_{i}-\nabla f_{i}(x^{t})\right\|^{2} \right]+2\ell_{i}^{2}\mathbb{E}\left[\left\|x^{t}-x^{t+1}\right\|^{2}\right]+ 2\eta^{2}\sigma^{2}.\] (49)

Averaging inequalities (49) over \(i=1,\ldots,n\) and denoting \(\widetilde{P}_{t}:=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|w^{t}_{i}- \nabla f_{i}(x^{t})\right\|^{2}\right]\), \(R_{t}:=\mathbb{E}\left[\left\|x^{t}-x^{t+1}\right\|^{2}\right]\) we have

\[\widetilde{P}_{t+1}\leq(1-\eta)\widetilde{P}_{t}+2\widetilde{\ell}^{2}R_{t}+2 \eta^{2}\sigma^{2}.\]

Summing up the above inequality for \(t=0,\ldots,T-1\), we derive

\[\frac{1}{T}\sum_{t=0}^{T-1}\widetilde{P}_{t}\leq\frac{2\widetilde{\ell}^{2}}{ \eta}\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+2\eta\sigma^{2}+\frac{1}{\eta T} \widetilde{P}_{0}.\] (50)

Similarly by Lemma 7-(56) denoting \(P_{t}:=\mathbb{E}\left[\left\|w^{t}-\nabla f(x^{t})\right\|^{2}\right]\), we have

\[\frac{1}{T}\sum_{t=0}^{T-1}P_{t}\leq\frac{2\widetilde{\ell}^{2}}{\eta n}\frac {1}{T}\sum_{t=0}^{T-1}R_{t}+\frac{2\eta\sigma^{2}}{n}+\frac{1}{\eta T}P_{0}.\] (51)

**Part II. Controlling the variance of contractive compressor and STORM/MVR estimator.** By Lemma 8 we have for each \(i=1,\ldots,n\), and any \(0<\eta\leq 1\) and \(t\geq 0\)

\[\mathbb{E}\left[\left\|g^{t+1}_{i}-w^{t+1}_{i}\right\|^{2}\right] \leq \left(1-\frac{\alpha}{2}\right)\mathbb{E}\left[\left\|g^{t}_{i}-w^ {t}_{i}\right\|^{2}\right]+\frac{4\eta^{2}}{\alpha}\mathbb{E}\left[\left\|w^ {t}_{i}-\nabla f_{i}(x^{t})\right\|^{2}\right]\] (52) \[\qquad+\left(\frac{4L_{i}^{2}}{\alpha}+\ell_{i}^{2}\right) \mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+2\eta^{2}\sigma^{2}.\]

Averaging inequalities (52) over \(i=1,\ldots,n\), denoting \(\widetilde{V}_{t}:=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|g^{t}_{i}- w^{t}_{i}\right\|^{2}\right]\), and summing up the resulting inequality for \(t=0,\ldots,T-1\), we obtain

\[\frac{1}{T}\sum_{t=0}^{T-1}\widetilde{V}_{t} \leq \frac{8\eta^{2}}{\alpha^{2}}\frac{1}{T}\sum_{t=0}^{T-1} \widetilde{P}_{t}+\left(\frac{8\widetilde{L}^{2}}{\alpha^{2}}+\frac{2 \widetilde{\ell}^{2}}{\alpha}\right)\frac{1}{T}\sum_{t=0}^{T-1}R_{t}+\frac{2 \eta^{2}\sigma^{2}}{\alpha}\] (53) \[\leq \left(\frac{8\widetilde{L}^{2}}{\alpha^{2}}+\frac{2\widetilde{ \ell}^{2}}{\alpha}+\frac{16\eta\widetilde{\ell}^{2}}{\alpha^{2}}\right)\frac{ 1}{T}\sum_{t=0}^{T-1}R_{t}\] \[\qquad+\frac{16\eta^{3}\sigma^{2}}{\alpha^{2}}+\frac{2\eta^{2} \sigma^{2}}{\alpha}+\frac{8\eta}{\alpha^{2}T}\widetilde{P}_{0}.\]

**Part III. Combining steps I and II with descent lemma.** By smoothness (Assumption 1) of \(f(\cdot)\) it follows from Lemma 1 that for any \(\gamma\leq 1/(2L)\) we have

\[f(x^{t+1}) \leq f(x^{t})-\frac{\gamma}{2}\left\|\nabla f(x^{t})\right\|^{2}- \frac{1}{4\gamma}\left\|x^{t+1}-x^{t}\right\|^{2}+\frac{\gamma}{2}\left\|g^{t }-\nabla f(x^{t})\right\|^{2}\] \[\leq f(x^{t})-\frac{\gamma}{2}\left\|\nabla f(x^{t})\right\|^{2}- \frac{1}{4\gamma}\left\|x^{t+1}-x^{t}\right\|^{2}+\gamma\frac{1}{n}\sum_{i=1}^{ n}\left\|g^{t}_{i}-w^{t}_{i}\right\|^{2}+\gamma\left\|w^{t}-\nabla f(x^{t})\right\|^{2}.\]

[MISSING_PAGE_EMPTY:45]

### Controlling the variance of STORM/MVR estimator

**Lemma 7**.: _Let Assumptions 2 and 3 be satisfied, and suppose \(0<\eta\leq 1\). For every \(i=1,\ldots,n\), let the sequence \(\left\{w_{i}^{t}\right\}_{t\geq 0}\) be updated via \(w_{i}^{t+1}=\nabla f_{i}(x^{t+1},\xi_{i}^{t+1})+(1-\eta)(w_{i}^{t}-\nabla f_{i} (x^{t},\xi_{i}^{t+1}))\) Define the sequence \(w^{t}:=\frac{1}{n}\sum_{i=1}^{n}w_{i}^{t}\). Then for every \(i=1,\ldots,n\) and \(t\geq 0\) it holds_

\[\mathbb{E}\left[\left\|w_{i}^{t+1}-\nabla f_{i}(x^{t+1})\right\|^{2}\right] \leq(1-\eta)\mathbb{E}\left[\left\|w_{i}^{t}-\nabla f_{i}(x^{t})\right\|^{2} \right]+2\ell_{i}^{2}\mathbb{E}\left[\left\|x^{t}-x^{t+1}\right\|^{2}\right]+ 2\eta^{2}\sigma^{2},\] (55)

\[\mathbb{E}\left[\left\|w^{t+1}-\nabla f(x^{t+1})\right\|^{2}\right]\leq(1- \eta)\mathbb{E}\left[\left\|w^{t}-\nabla f(x^{t})\right\|^{2}\right]+\frac{2 \widetilde{\ell}^{2}}{n}\mathbb{E}\left[\left\|x^{t}-x^{t+1}\right\|^{2} \right]+\frac{2\eta^{2}\sigma^{2}}{n}.\] (56)

Proof.: For each \(t=0,\ldots,T-1\), define a random vector \(\xi^{t}:=(\xi_{1}^{t},\ldots,\xi_{n}^{t})\) and denote by \(\nabla f(x^{t},\xi^{t}):=\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(x^{t},\xi_{i}^{ t})\). Note that the entries of the random vector \(\xi^{t}\) are independent and \(\mathbb{E}_{\xi^{t}}\left[\nabla f(x^{t},\xi^{t})\right]=\nabla f(x^{t})\), then we have

\[w^{t+1}=\nabla f(x^{t+1},\xi^{t+1})+(1-\eta)\left(w^{t}-\nabla f(x^{t},\xi^{t+ 1})\right),\]

where \(w^{t}=\frac{1}{n}\sum_{i=1}^{n}w_{i}^{t}\) is an auxiliary sequence.

We define

\[\mathcal{V}_{i}^{t}:=\nabla f_{i}(x^{t},\xi_{i}^{t})-\nabla f_{i}(x^{t}), \qquad\mathcal{V}^{t}:=\frac{1}{n}\sum_{i=1}^{n}\mathcal{V}_{i}^{t},\]

\[\mathcal{W}_{i}^{t}:=\nabla f_{i}(x^{t})-\nabla f_{i}(x^{t},\xi_{i}^{t+1})+ \nabla f_{i}(x^{t+1},\xi_{i}^{t+1})-\nabla f_{i}(x^{t+1}),\qquad\mathcal{W}^{ t}:=\frac{1}{n}\sum_{i=1}^{n}\mathcal{W}_{i}^{t}.\]

Then by Assumptions 2, we have

\[\mathbb{E}\left[\mathcal{V}_{i}^{t}\right]=\mathbb{E}\left[\mathcal{W}_{i}^{ t}\right]=\mathbb{E}\left[\mathcal{V}^{t}\right]=\mathbb{E}\left[\mathcal{W}^{t} \right]=0,\] (57)

\[\mathbb{E}\left[\left\|\mathcal{V}_{i}^{t}\right\|^{2}\right]\leq\sigma^{2}, \qquad\mathbb{E}\left[\left\|\mathcal{V}^{t}\right\|^{2}\right]\leq\frac{ \sigma^{2}}{n}.\] (58)

Furthermore, we can derive

\[\mathbb{E}\left[\left\|\mathcal{W}^{t}\right\|^{2}\right] = \mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}\mathcal{W}_{i}^{ t}\right\|^{2}\right]\] \[= \frac{1}{n^{2}}\mathbb{E}\left[\left\|\sum_{i=1}^{n}\mathcal{W}_{i }^{t}\right\|^{2}\right]\] \[= \frac{1}{n^{2}}\sum_{i=1}^{n}\mathbb{E}\left[\left\|\mathcal{W}_{i }^{t}\right\|^{2}\right]+\frac{1}{n^{2}}\sum_{i\neq j}\mathbb{E}\left[ \left\langle\mathcal{W}_{i}^{t},\mathcal{W}_{j}^{t}\right\rangle\right]\] \[\stackrel{{(i)}}{{=}} \frac{1}{n^{2}}\sum_{i=1}^{n}\mathbb{E}\left[\left\|\mathcal{W}_{i }^{t}\right\|^{2}\right]+\frac{1}{n^{2}}\sum_{i\neq j}\langle\mathbb{E}\left[ \mathcal{W}_{i}^{t}\right],\mathbb{E}\left[\mathcal{W}_{j}^{t}\right]\rangle\] \[= \frac{1}{n^{2}}\sum_{i=1}^{n}\mathbb{E}\left[\left\|\mathcal{W}_{ i}^{t}\right\|^{2}\right]\] \[\leq \frac{1}{n^{2}}\sum_{i=1}^{n}\mathbb{E}\left[\left\|\nabla f_{i}( x^{t+1},\xi_{i}^{t+1})-\nabla f_{i}(x^{t},\xi_{i}^{t+1})\right\|^{2}\right]\] \[\leq \frac{1}{n^{2}}\sum_{i=1}^{n}\ell_{i}^{2}\mathbb{E}\left[\left\|x ^{t+1}-x^{t}\right\|^{2}\right]=\frac{\widetilde{\ell}^{2}}{n}\mathbb{E} \left[\left\|x^{t+1}-x^{t}\right\|^{2}\right],\]where \((i)\) holds by the conditional independence of \(\mathcal{W}_{i}^{t}\) and \(\mathcal{W}_{j}^{t}\), and the last inequality follows by the individual smoothness of stochastic functions (Assumption 3). Therefore, we have

\[\mathbb{E}\left[\left\|\mathcal{W}_{i}^{t}\right\|^{2}\right]\leq \ell_{i}^{2}\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right],\qquad \mathbb{E}\left[\left\|\mathcal{W}^{t}\right\|^{2}\right]\leq\frac{\widetilde {\ell^{2}}}{\alpha}\mathbb{E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right],\] (59)

where the first inequality is obtained by using a similar derivation.

By the update rule for \(w^{t}\), we can also derive

\[w^{t+1}-\nabla f(x^{t+1}) = (1-\eta)\left(w^{t}-\nabla f(x^{t},\xi^{t+1})\right)+\left(\nabla f (x^{t+1},\xi^{t+1})-\nabla f(x^{t+1})\right)\] \[= (1-\eta)\left(w^{t}-\nabla f(x^{t})\right)+\eta\left(\nabla f(x^{ t+1},\xi^{t+1})-\nabla f(x^{t+1})\right)\] \[\quad+(1-\eta)\left(\left(\nabla f(x^{t})-\nabla f(x^{t},\xi^{t+ 1})+\nabla f(x^{t+1},\xi^{t+1})-\nabla f(x^{t+1})\right)\right)\] \[= (1-\eta)\left(w^{t}-\nabla f(x^{t})\right)+\eta\mathcal{V}^{t+1} +(1-\eta)\mathcal{W}^{t}.\]

Therefore, we have

\[\mathbb{E}\left[\left\|w^{t+1}-\nabla f(x^{t+1})\right\|^{2}\right] \leq \mathbb{E}\left[\mathbb{E}_{\xi^{t+1}}\left[\left\|(1-\eta) \left(w^{t}-\nabla f(x^{t})\right)+\eta\mathcal{V}_{t+1}+(1-\eta)\mathcal{W}_ {t}\right\|^{2}\right]\right]\] \[= (1-\eta)^{2}\mathbb{E}\left[\left\|w^{t}-\nabla f(x^{t})\right\| ^{2}\right]+\mathbb{E}\left[\left\|\eta\mathcal{V}^{t+1}+(1-\eta)\mathcal{W}^ {t}\right\|^{2}\right]\] \[\leq (1-\eta)\mathbb{E}\left[\left\|w^{t}-\nabla f(x^{t})\right\|^{2} \right]+\frac{2\sigma^{2}\eta^{2}}{n}+\frac{2\widetilde{\ell^{2}}}{n}\mathbb{ E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right],\]

where the last inequality holds by (58) and (59). Similarly for each \(i=1,\ldots,n\), we have

\[w_{i}^{t+1}-\nabla f_{i}(x^{t+1}) = (1-\eta)\left(w_{i}^{t}-\nabla f_{i}(x^{t})\right)+\eta\mathcal{ V}_{i}^{t+1}+(1-\eta)\mathcal{W}_{i}^{t}.\] (60)

Thus,

\[\mathbb{E}\left[\left\|w_{i}^{t+1}-\nabla f_{i}(x^{t+1})\right\| ^{2}\right] \leq (1-\eta)\mathbb{E}\left[\left\|w_{i}^{t}-\nabla f_{i}(x^{t}) \right\|^{2}\right]+2\sigma^{2}\eta^{2}+2\ell_{i}^{2}R_{t}.\]

### Controlling the variance of contractive compression and STORM/MVR estimator

**Lemma 8**.: _Let Assumptions 1, 2 and 3 be satisfied, and suppose \(0<\eta\leq 1\). For every \(i=1,\ldots,n\), let the sequences \(\left\{w_{i}^{t}\right\}_{t\geq 0}\) and \(\left\{g_{i}^{t}\right\}_{t\geq 0}\) be updated via_

\[w_{i}^{t+1} = \nabla f_{i}(x^{t+1},\xi_{i}^{t+1})+(1-\eta)(w_{i}^{t}-\nabla f_{ i}(x^{t},\xi_{i}^{t+1})),\] \[g_{i}^{t+1} = g_{i}^{t}+\mathcal{C}\left(w_{i}^{t+1}-g_{i}^{t}\right).\]

_Then for every \(i=1,\ldots,n\) and \(t\geq 0\) it holds_

\[\mathbb{E}\left[\left\|g_{i}^{t+1}-w_{i}^{t+1}\right\|^{2}\right] \leq \left(1-\frac{\alpha}{2}\right)\mathbb{E}\left[\left\|g_{i}^{t}- w_{i}^{t}\right\|^{2}\right]+\frac{4\eta^{2}}{\alpha}\mathbb{E}\left[\left\|w_{i}^{t}- \nabla f_{i}(x^{t})\right\|^{2}\right]\] (61) \[\quad+\left(\frac{4L_{i}^{2}}{\alpha}+\ell_{i}^{2}\right)\mathbb{ E}\left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+2\eta^{2}\sigma^{2}.\]

Proof.: By the update rule of \(w_{i}^{t}\), \(g_{i}^{t}\), and definition of \(\mathcal{V}_{i}^{t}\), \(\mathcal{W}_{i}^{t}\) given in the proof of Lemma 7, we can derive

\[\mathbb{E}\left[\left\|g_{i}^{t+1}-w_{i}^{t+1}\right\|^{2}\right] = \mathbb{E}\left[\left\|\mathcal{C}(w_{i}^{t+1}-g_{i}^{t})-(w_{i}^{ t+1}-g_{i}^{t})\right\|^{2}\right]\] \[\stackrel{{(i)}}{{\leq}} (1-\alpha)\mathbb{E}\left[\left\|w_{i}^{t+1}-g_{i}^{t}\right\|^{2}\right]\] \[\stackrel{{(ii)}}{{=}} (1-\alpha)\mathbb{E}\left[\left\|(1-\eta)\left(w_{i}^{t}-\nabla f _{i}(x^{t})\right)+\eta\mathcal{V}_{i}^{t+1}+(1-\eta)\mathcal{W}_{i}^{t}+\nabla f _{i}(x^{t+1})-g_{i}^{t}\right\|^{2}\right]\] \[= (1-\alpha)\mathbb{E}\left[\mathbb{E}_{\xi_{i}^{t+1}}\left[\left\|( 1-\eta)\left(w_{i}^{t}-\nabla f_{i}(x^{t})\right)+\eta\mathcal{V}_{i}^{t+1}+(1- \eta)\mathcal{W}_{i}^{t}+\nabla f_{i}(x^{t+1})-g_{i}^{t}\right\|^{2}\right]\]\[\stackrel{{(iii)}}{{=}} (1-\alpha)\mathbb{E}\left[\left\|(1-\eta)\left(w_{i}^{t}-\nabla f_{i}( x^{t})\right)+\nabla f_{i}(x^{t+1})-g_{i}^{t}\right\|^{2}\right]\] \[\qquad+(1-\alpha)\mathbb{E}\left[\left\|\eta\mathcal{V}_{i}^{t+1} +(1-\eta)\mathcal{W}_{i}^{t}\right\|^{2}\right]\] \[= (1-\alpha)\mathbb{E}\left[\left\|\left(w_{i}^{t}-g_{i}^{t} \right)+\left(\nabla f_{i}(x^{t+1})-\nabla f_{i}(x^{t})\right)-\eta\left(w_{i} ^{t}-\nabla f_{i}(x^{t})\right)\right\|^{2}\right]\] \[\qquad+(1-\alpha)\mathbb{E}\left[\left\|\eta\mathcal{V}_{i}^{t+1 }+(1-\eta)\mathcal{W}_{i}^{t}\right\|^{2}\right]\] \[\stackrel{{(iv)}}{{\leq}} (1-\alpha)\left(1+\rho\right)\mathbb{E}\left[\left\|w_{i}^{t}-g_ {i}^{t}\right\|^{2}\right]\] \[\qquad+(1-\alpha)\left(1+\rho^{-1}\right)\mathbb{E}\left[\left\| \left(\nabla f_{i}(x^{t+1})-\nabla f_{i}(x^{t})\right)-\eta\left(w_{i}^{t}- \nabla f_{i}(x^{t})\right)\right\|^{2}\right]\] \[\qquad+2(1-\alpha)\eta^{2}\mathbb{E}\left[\left\|\mathcal{V}_{i}^ {t+1}\right\|^{2}\right]+2(1-\alpha)(1-\eta)^{2}\mathbb{E}\left[\left\| \mathcal{W}_{i}^{t}\right\|^{2}\right]\] \[\stackrel{{(vi)}}{{\leq}} (1-\theta)\mathbb{E}\left[\left\|w_{i}^{t}-g_{i}^{t}\right\|^{2} \right]+2\beta\eta^{2}\mathbb{E}\left[\left\|w_{i}^{t}-\nabla f_{i}(x^{t}) \right\|^{2}\right]\] \[\qquad+2\beta\mathbb{E}\left[\left\|\nabla f_{i}(x^{t+1})-\nabla f _{i}(x^{t})\right\|^{2}\right]+2\ell_{i}^{2}\mathbb{E}\left[\left\|x^{t+1}-x^{ t}\right\|^{2}\right]+2\eta^{2}\sigma^{2}\] \[\leq (1-\theta)\mathbb{E}\left[\left\|w_{i}^{t}-g_{i}^{t}\right\|^{2} \right]+2\beta\eta^{2}\mathbb{E}\left[\left\|w_{i}^{t}-\nabla f_{i}(x^{t}) \right\|^{2}\right]\] \[\qquad+\left(2\beta L_{i}^{2}+\ell_{i}^{2}\right)\mathbb{E} \left[\left\|x^{t+1}-x^{t}\right\|^{2}\right]+2\eta^{2}\sigma^{2},\]

where \((i)\) holds by Definition 1, \((ii)\) follows from (60), \((iii)\) holds by unbiasedness of \(\mathcal{V}_{i}^{t+1}\) and \(\mathcal{W}_{i}^{t}\) (57). In \((iv)\) we use Young's inequality twice, in \((v)\) we introduce the notation \(\theta:=1-(1-\alpha)(1+\rho)\) and \(\beta:=(1-\alpha)(1+\rho^{-1})\), in \((vi)\) we again use Young's inequality and the bound (58) and (59). The last step holds by smoothness of \(f_{i}(\cdot)\) (Assumption 1). The proof is complete by the choice \(\rho=\alpha/2\), which guarantees \(1-\theta\leq 1-\alpha/2\), and \(2\beta\leq 4/\alpha\).

Simplified Proof of SGDM: Time Varying Parameters and No Tuning for Momentum Sequence

In this section, we give a simplified proof of SGDM in the single node setting (\(n=1\)) without compression (\(\alpha=1\)). The following theorem shows that the momentum parameter can be chosen in a parameter agnostic22 way as \(\eta_{t}=1/\sqrt{t+1}\) (or \(\eta_{t}=1/\sqrt{T+1}\)), instead of being a constant depending on problem parameters as it is suggested in our main Theorem 3. In other words, using SGDM with time varying momentum does not introduce any additional tuning of hyper-parameters.

Footnote 22: That is, independently of the problem specific parameters

**Theorem 8**.: _Let Assumptions 1, 2 hold. Let \(n=1\) and Algorithm 1 run with identity compressor \(\mathcal{C}\), i.e., \(\alpha=1\), and (possibly) time varying momentum \(\eta_{t}\in(0,1]\) and step-size paramters \(\gamma_{t}=\gamma\eta_{t}\) with \(\gamma\in(0,\nicefrac{{1}}{{(3L)}}]\). Let \(\hat{x}^{T}\) be sampled from the iterates of the algorithm with probabilities \(p_{t}=\eta_{t}/(\sum_{t=0}^{T-1}\eta_{t})\), then_

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] \leq \frac{2\Lambda_{0}\gamma^{-1}+2\sigma^{2}\sum_{t=0}^{T-1}\eta_{t }^{2}}{\sum_{t=0}^{T-1}\eta_{t}},\]

_where \(\Lambda_{0}:=f(x^{0})-f^{*}+\gamma\mathbb{E}\left[\left\|v^{0}-\nabla f(x^{0} )\right\|^{2}\right]\) is the Lyapunov function._

Proof.: By Lemma 2 denoting \(P_{t}:=\mathbb{E}\left[\left\|v^{t}-\nabla f(x^{t})\right\|^{2}\right]\), \(R_{t}:=\mathbb{E}\left[\left\|x^{t}-x^{t+1}\right\|^{2}\right]\), we have

\[P_{t+1}\leq P_{t}-\eta_{t}P_{t}+\frac{3L^{2}}{\eta_{t}}R_{t}+\eta_{t}^{2} \sigma^{2}.\] (62)

By descent Lemma 1, we have for any \(\gamma_{t}>0\)

\[\delta_{t+1}\leq\delta_{t}-\frac{\gamma_{t}}{2}\mathbb{E}\left[\left\|\nabla f (x^{t})\right\|^{2}\right]-\frac{1}{2\gamma_{t}}\left(1-\gamma_{t}L\right)R_{t }+\frac{\gamma_{t}}{2}P_{t},\] (63)

where \(\delta_{0}:=\mathbb{E}\left[f(x^{t})-f^{*}\right]\). Define the Lyapunov function as \(\Lambda_{t}=\delta_{0}+\gamma P_{t}\). Then summing up (63) with a \(\gamma\) multiple of (62) and noticing that \(\gamma_{t}\leq\gamma\), we get

\[\Lambda_{t+1}\leq\Lambda_{t}-\frac{\gamma_{t}}{2}\mathbb{E}\left[\left\|\nabla f (x^{t})\right\|^{2}\right]-\frac{1}{2\gamma_{t}}\left(1-\gamma L-6\gamma^{2}L^ {2}\right)R_{t}+\gamma\eta_{t}^{2}\sigma^{2}.\]

Since \(\gamma\leq 1/(3L)\), we have \(1-\gamma L-6\gamma^{2}L^{2}\leq 0\), and, therefore, by telescoping we can derive

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right] = \left(\sum_{t=0}^{T-1}\eta_{t}\right)^{-1}\sum_{t=0}^{T-1}\eta_{t }\mathbb{E}\left[\left\|\nabla f(x^{t})\right\|^{2}\right]\] \[\leq \frac{2\Lambda_{0}\gamma^{-1}+2\sigma^{2}\sum_{t=0}^{T-1}\eta_{t }^{2}}{\sum_{t=0}^{T-1}\eta_{t}}.\]

The above theorem suggests that to ensure convergence, we can select any momentum sequence such that \(\sigma^{2}\sum_{t=0}^{\infty}\eta_{t}^{2}<\infty\), and \(\sum_{t=0}^{\infty}\eta_{t}^{2}\rightarrow\infty\) for \(t\rightarrow\infty\). The parameter \(\gamma\), which determines the step-size \(\gamma_{t}=\gamma\eta_{t}\), should be set to \(\gamma=1/(3L)\) (to minimize the upper bound). Let us now consider some special cases.

Deterministic case.If \(\sigma=0\), we can set it to be any constant \(\eta_{t}=\eta\in(0,1]\) and derive

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right]\leq\frac{2 \delta_{0}}{\gamma\eta T}=\mathcal{O}\left(\frac{L\delta_{0}}{\eta T}\right).\]Stochastic case.For \(\sigma^{2}>0\), we can select time-varying \(\eta_{t}=\frac{1}{\sqrt{t+1}}\) or constant \(\eta_{t}=\frac{1}{\sqrt{T+1}}\), which gives \(\sum_{t=0}^{T-1}\eta_{t}^{2}=\mathcal{O}\left(\log(T)\right)\), and \(\sum_{t=0}^{T-1}\eta_{t}=\Omega\left(\sqrt{T}\right)\). Thus

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right]=\widetilde{ \mathcal{O}}\left(\frac{L\Lambda_{0}+\sigma^{2}}{\sqrt{T}}\right).\]

Notice that if we set \(\eta_{t}\) as above, we do not need any tuning of momentum parameter. Only tuning of paramter \(\gamma\) is required to ensure convergence with optimal dependence on \(T\), as in SGD without momentum. Of course, this rate is not yet optimal in other parameters, e.g., \(\sigma^{2}\) and \(L\). To make it optimal in all problem parameters, we can set \(\eta=\max\left\{1,\left(\frac{L\Lambda_{0}}{\sigma^{2}T}\right)^{1/2}\right\}\) similarly to the statement of Theorem 2.

Revisiting EF14-SGD Analysis under BG and BGS Assumptions

In this section, we revisit the analysis of the original variant of error feedback (EF14-SGD) to showcase the difficulty in avoiding BG/BGS assumptions commonly used in the nonconvex analysis of this variant. In summary, the key reason for BG/BGS assumption is to bound the second term in (67) or (68).

Recall that EF14-SGD has the update rule [Stich et al., 2018]

\[x^{t+1}=x^{t}-g^{t},\qquad g^{t}=\frac{1}{n}\sum_{i=1}^{n}g_{i}^{t},\] (64)

\[\mbox{\sf EF14-SGD:}\] (65)

where \(\left\{e_{i}^{t}\right\}_{t\geq 0}\) are error/memory sequences with \(e_{i}^{0}=0\) for each \(i=1,\ldots,n\). Let \(e^{t}:=\frac{1}{n}\sum_{i=1}^{n}e_{i}^{t}\). The proof of this method relies on so called perturbed iterate analysis, for which one defines a "virtual sequence": \(\tilde{x}^{t}:=x^{t}-e^{t}\). Then it is verified by direct substitution that for any \(t\geq 0\)

\[\tilde{x}^{t+1}=\tilde{x}^{t}-\gamma\frac{1}{n}\sum_{i=1}^{n}\nabla f_{i}(x^{t },\xi_{i}^{t}).\]

If follows from Lemma 9 in [Stich and Karimireddy, 2021] that for any \(\gamma\leq\nicefrac{{1}}{{2L}}\) and \(t\geq 0\)

\[\mathbb{E}\left[f(\tilde{x}^{t+1})\right]\leq\mathbb{E}\left[f(\tilde{x}^{t}) \right]-\frac{\gamma}{4}\mathbb{E}\left[\left\|\nabla f(x^{t})\right\|^{2} \right]+\frac{\gamma L\sigma^{2}}{2n}+\frac{L^{2}}{2}\mathbb{E}\left[\left\|e ^{t}\right\|^{2}\right].\]

Telescoping the recursion above and setting \(\delta_{0}:=f(x^{0})-f^{*}\), we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla f(x^{t})\right\|^{2} \right]\leq\frac{4\delta_{0}}{\gamma T}+\frac{2\gamma L\sigma^{2}}{n}+2L^{2} \frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|e^{t}\right\|^{2}\right].\] (66)

Now it remains to bound efficiently the average error term \(\mathbb{E}\left[\left\|e^{t}\right\|^{2}\right]=\mathbb{E}\left[\left\|\frac{ 1}{n}\sum_{i=1}^{n}e_{i}^{t}\right\|^{2}\right].\) By Jensen's inequality, we have

\[\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}e_{i}^{t}\right\|^{2}\right] \leq\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|e_{i}^{t}\right\|^{2} \right],\]

and develop a bound for each \(\mathbb{E}\left[\left\|e_{i}^{t}\right\|^{2}\right]\) individually. Denote by \(z:=e_{i}^{t}+\gamma\nabla f_{i}(x^{t},\xi_{i}^{t})\), then

\[\mathbb{E}\left[\left\|e_{i}^{t+1}\right\|^{2}\right] \leq \mathbb{E}\left[\left\|\mathcal{C}(z)-z\right\|^{2}\right]\] (67) \[\leq (1-\alpha)\mathbb{E}\left[\left\|e_{i}^{t}+\gamma\nabla f_{i}(x^ {t},\xi_{i}^{t})\right\|^{2}\right]\] \[\leq (1-\alpha)\left(1+\frac{\alpha}{2}\right)\mathbb{E}\left[\left\|e _{i}^{t}\right\|^{2}\right]+\left(1+\frac{2}{\alpha}\right)\mathbb{E}\left[ \left\|\gamma\nabla f_{i}(x^{t},\xi_{i}^{t})\right\|^{2}\right]\] \[\leq \left(1-\frac{\alpha}{2}\right)\mathbb{E}\left[\left\|e_{i}^{t} \right\|^{2}\right]+\frac{3\gamma^{2}}{\alpha}\mathbb{E}\left[\left\|\nabla f_ {i}(x^{t},\xi_{i}^{t})\right\|^{2}\right],\]

where we used Definition 1 and Young's inequality.

BG asssumption.If we assume bounded (stochastic) gradients (BG), i.e., \(\mathbb{E}\left[\left\|\nabla f_{i}(x,\xi_{i})\right\|^{2}\right]\leq G^{2}\) for all \(i=1,\ldots,n\), then using (67) we can derive

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|e^{t}\right\|^{2}\right] \leq\frac{6\gamma^{2}G^{2}}{\alpha^{2}}.\]Combining this bound with (66), we have

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|\nabla f(x^{t})\right\|^{2} \right]\leq\frac{4\delta_{0}}{\gamma T}+\frac{2\gamma L\sigma^{2}}{n}+\frac{12 L^{2}\gamma^{2}G^{2}}{\alpha^{2}}.\]

The step-size choice \(\gamma=\min\left\{\frac{1}{L},\left(\frac{\delta_{0}\alpha^{2}}{TL^{2}\sigma^{ 2}}\right)^{\nicefrac{{1}}{{3}}},\left(\frac{n\delta_{0}}{TL\sigma^{2}}\right)^ {\nicefrac{{1}}{{2}}}\right\}\), allows us to bound the RHS by \(\frac{12\delta_{0}}{\gamma T}\), and guarantees

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right]=\mathcal{O} \left(\frac{L\delta_{0}}{T}+\left(\frac{L\delta_{0}G}{\alpha T}\right)^{ \nicefrac{{2}}{{3}}}+\left(\frac{L\delta_{0}}{nT}\right)^{\nicefrac{{1}}{{2} }}\right),\]

or, equivalently, \(T=\mathcal{O}\left(\frac{L\delta_{0}}{\varepsilon^{2}}+\frac{L\delta_{0}G}{ \sigma^{3}}+\frac{L\delta_{0}}{n\varepsilon^{4}}\right)\) sample complexity to find a stationary point. This analysis using BG assumption and derived sample complexity is essentially a simplified version of the one by Koloskova et al. (2020).23

Footnote 23: Up to a smoothness constant and the fact that Koloskova et al. (2020) works in a more general decentralized setting.

Bgs asssumption.If we assume bounded gradient similarity (BGS), i.e., \(\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|\nabla f_{i}(x)-\nabla f(x) \right\|^{2}\right]\leq G^{2}\), we can slightly modify the derivation in (67) as follows

\[\mathbb{E}\left[\left\|e_{i}^{t+1}\right\|^{2}\right] \leq (1-\alpha)\mathbb{E}\left[\left\|e_{i}^{t}+\gamma\nabla f_{i}(x^ {t},\xi_{i}^{t})\right\|^{2}\right]\] (68) \[= (1-\alpha)\mathbb{E}\left[\left\|e_{i}^{t}+\gamma\nabla f_{i}(x^ {t})\right\|^{2}\right]+(1-\alpha)\gamma^{2}\mathbb{E}\left[\left\|\nabla f_{ i}(x^{t},\xi_{i}^{t})-\nabla f_{i}(x^{t})\right\|^{2}\right]\] \[\leq (1-\alpha)\left(1+\frac{\alpha}{2}\right)\mathbb{E}\left[\left\| e_{i}^{t}\right\|^{2}\right]+\left(1+\frac{2}{\alpha}\right)\mathbb{E}\left[ \left\|\gamma\nabla f_{i}(x^{t})\right\|^{2}\right]+\gamma^{2}\sigma^{2}\] \[\leq \left(1-\frac{\alpha}{2}\right)\mathbb{E}\left[\left\|e_{i}^{t} \right\|^{2}\right]+\frac{3\gamma^{2}}{\alpha}\mathbb{E}\left[\left\|\nabla f _{i}(x^{t})\right\|^{2}\right]+\gamma^{2}\sigma^{2}.\]

Averaging the above inequalities over \(i=1,\ldots,n\) and using BGS assumption, i.e., \(\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\left\|\nabla f_{i}(x)\right\|^{2} \right]\leq\left\|\nabla f(x)\right\|^{2}+G^{2}\), we can derive via averaging over \(t=0,\ldots,T-1\)

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left[\left\|e^{t}\right\|^{2}\right] \leq \frac{6\gamma^{2}}{\alpha^{2}}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{ E}\left[\left\|\nabla f(x^{t})\right\|^{2}\right]+\frac{6\gamma^{2}G^{2}}{ \alpha^{2}}+\frac{2\gamma^{2}\sigma^{2}}{\alpha},\]

Combining the above inequality with (66), we have

\[\left(1-\frac{12L^{2}\gamma^{2}}{\alpha^{2}}\right)\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}\left[\left\|\nabla f(x^{t})\right\|^{2}\right]\leq\frac{4\delta_{0 }}{\gamma T}+\frac{2\gamma L\sigma^{2}}{n}+\frac{12\gamma^{2}L^{2}G^{2}}{ \alpha^{2}}+\frac{4\gamma^{2}L^{2}\sigma^{2}}{\alpha}.\]

By setting \(\gamma=\min\left\{\frac{\alpha}{4L},\left(\frac{n\delta_{0}}{L\sigma^{2}T} \right)^{\nicefrac{{1}}{{2}}},\left(\frac{\alpha^{2}\delta_{0}}{L^{2}G^{2}T} \right)^{\nicefrac{{1}}{{3}}},\left(\frac{\alpha\delta_{0}}{L^{2}\sigma^{2}T} \right)^{\nicefrac{{1}}{{3}}}\right\}\), we have \(\left(1-\frac{12L^{2}\gamma^{2}}{\alpha^{2}}\right)\geq\frac{1}{4}\), and the RHS is at most \(\frac{16\delta_{0}}{\gamma T}\). Therefore,

\[\mathbb{E}\left[\left\|\nabla f(\hat{x}^{T})\right\|^{2}\right]=\mathcal{O} \left(\frac{L\delta_{0}}{\alpha T}+\left(\frac{L\delta_{0}G}{\alpha T}\right)^ {\nicefrac{{2}}{{3}}}+\left(\frac{L\delta_{0}\sigma}{\sqrt{\alpha}T}\right)^ {\nicefrac{{2}}{{3}}}+\left(\frac{L\delta_{0}}{nT}\right)^{\nicefrac{{1}}{{2} }}\right),\]

or, equivalently, \(T=\mathcal{O}\left(\frac{L\delta_{0}}{\alpha\varepsilon^{2}}+\frac{L\delta_{0}G }{\alpha\varepsilon^{3}}+\frac{L\delta_{0}\sigma}{\sqrt{\alpha}\varepsilon^{3} }+\frac{L\delta_{0}}{n\varepsilon^{4}}\right)\) sample complexity. Notice that in the single node case (\(n=1\)), we have \(G=0\), and by Young's inequality \(\left(\frac{L\delta_{0}\sigma}{\sqrt{\alpha T}}\right)^{\nicefrac{{2}}{{3}}}\leq \frac{1}{3}\frac{L\delta_{0}}{\alpha T}+\frac{2}{3}\left(\frac{L\delta_{0} \sigma^{2}}{T}\right)^{\nicefrac{{1}}{{2}}}\). Therefore, the above rate recovers the one by Stich and Karimireddy (2021) in the single node setting.