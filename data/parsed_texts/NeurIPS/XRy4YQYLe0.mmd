# Aleatoric and Epistemic Discrimination:

Fundamental Limits of Fairness Interventions

 Hao Wang

MIT-IBM Watson AI Lab

hao@ibm.com

&Luxi (Lucy) He

Harvard College

luxihe@college.harvard.edu

&Rui Gao

The University of Texas at Austin

rui.gao@mccombs.utexas.edu

&Flavio P. Calmon

Harvard University

flavio@seas.harvard.edu

###### Abstract

Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: _aleatoric discrimination_, which is inherent in the data distribution, and _epistemic discrimination_, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate that state-of-the-art fairness interventions are effective at removing epistemic discrimination on standard (overused) tabular datasets. However, when data has missing values, there is still significant room for improvement in handling aleatoric discrimination.

## 1 Introduction

Algorithmic discrimination may occur in different stages of the machine learning (ML) pipeline. For example, historical biases in the data-generating process can propagate to downstream tasks; human biases can influence a ML model through inductive bias; optimizing solely for accuracy can lead to disparate model performance across groups in the data (Suresh and Guttag, 2019; Mayson, 2019). The past years have seen a rapid increase in algorithmic interventions that aim to mitigate biases in ML models (see e.g., Zemel et al., 2013; Feldman et al., 2015; Calmon et al., 2017; Menon and Williamson, 2018; Zhang et al., 2018; Zafar et al., 2019; Friedler et al., 2019; Bellamy et al., 2019; Kim et al., 2019; Celis et al., 2019; Yang et al., 2020; Jiang and Nachum, 2020; Jiang et al., 2020; Martinez et al., 2020; Lowy et al., 2021; Alghamdi et al., 2022). A recent survey (Hort et al., 2022) found _nearly 400_ fairness-intervention algorithms, including 123 pre-processing, 212 in-processing, and 56 post-processing algorithms introduced in the past decade.

Which sources of biases are (the hundreds of) existing fairness interventions trying to control? In order to create effective strategies for reducing algorithmic discrimination, it is critical to disentangle where biases in model performance originate. For instance, if a certain population group has significantly more missing features in training data, then it is more beneficial to collect data than selecting a more complex model class or training strategy. Conversely, if the model class does not accurately representthe underlying distribution of a specific population group, then collecting more data for that group will not resolve performance disparities.

We divide algorithmic discrimination1 into two categories: aleatoric and epistemic discrimination.2 Aleatoric discrimination captures inherent biases in the data distribution that can lead to unfair decisions in downstream tasks. Epistemic discrimination, in turn, is due to algorithmic choices made during model development and lack of knowledge about the optimal "fair" predictive model.

Footnote 1: There are various measures to quantify algorithmic discrimination, and the choice should be based on the specific application of interest (see Blodgett et al., 2020; Varshney, 2021; Katzman et al., 2023, for a more detailed discussion). In this paper, we focus on group fairness measures (see Table 1 for some examples), which are crucial in contexts like hiring and recidivism prediction.

In this paper, we provide methods for measuring aleatoric and epistemic discrimination in classification tasks for group fairness metrics. Since aleatoric discrimination only depends on properties of the data distribution and the fairness measure of choice, we quantify it by asking a fundamental question:

_For a given data distribution, what is the best achievable performance (e.g., accuracy)_

_under a set of group fairness constraints?_

We refer to the answer as the _fairness Pareto frontier_. This frontier delineates the optimal performance achievable by a classifier when unlimited data and computing power are available. For a fixed data distribution, the fairness Pareto frontier represents the ultimate, information-theoretic limit for accuracy and group fairness beyond which no model can achieve. Characterizing this limit enables us to (i) separate sources of discrimination and create strategies to control them accordingly; (ii) evaluate the effectiveness of existing fairness interventions for reducing epistemic discrimination; and (iii) inform the development of data collection methods that promote fairness in downstream tasks.

At first, computing the fairness Pareto frontier can appear to be an intractable problem since it requires searching over all possible classifiers--even if the data distribution is known exactly. Our main technical contribution is to provide an upper bound estimate for this frontier by solving a sequence of optimization problems. The proof technique is based on Blackwell's seminal results (Blackwell, 1953), which proposed the notion of comparisons of statistical experiments and inspired a line of works introducing alternative comparison criteria (see e.g., Shannon, 1958; Cam, 1964; Torgersen, 1991; Cohen et al., 1998; Raginsky, 2011). Here, we apply these results to develop an algorithm that iteratively refines the achievable fairness Pareto frontier. We also prove convergence guarantees for our algorithm and demonstrate how it can be used to benchmark existing fairness interventions.

We quantify epistemic discrimination by comparing a classifier's performance with the information-theoretic optimal given by the fairness Pareto frontier. Our experiments indicate that given sufficient data, state-of-the-art (SOTA) group fairness interventions are effective at reducing epistemic discrimination as their gap to the information-theoretic limit is small (see Figure 1 and 2). Consequently, there are diminishing returns in benchmarking new fairness interventions on standard (overused) tabular datasets (e.g., UCI Adult and ProPublica COMPAS datasets). However, existing interventions _do not_ eliminate aleatoric discrimination as this type of discrimination is not caused by choice of learning algorithm or model class, and is due to the data distribution. Factors such as data missing values can significantly contribute to aleatoric discrimination. We observe that when population groups have disparate missing patterns, aleatoric discrimination escalates, leading to a sharp decline in the effectiveness of fairness intervention algorithms (see Figure 3).

#### Related Work

There is significant work analyzing the tension between group fairness measures and model performance metrics (see e.g., Kleinberg et al., 2016; Chouldechova, 2017; Corbett-Davies et al., 2017; Chen et al., 2018; Wick et al., 2019; Dutta et al., 2020; Wang et al., 2021). For example, there is a growing body of work on omnipredictors (Gopalan et al., 2021; Hu et al., 2023; Globus-Harris et al., 2023) discussing how, and under which conditions, the fair Bayes optimal classifier can be derived using post-processing techniques from multicibrated regressors. While previous studies (Hardt et al., 2016; Corbett-Davies et al., 2017; Menon and Williamson, 2018; Chzhen et al., 2019; Yang et al., 2020; Zeng et al., 2022, 2022) have investigated the fairness Pareto frontier and fair Bayesoptimal classifier, our approach differs from this prior work in the following aspects: our approach is applicable to _multiclass_ classification problems with _multiple_ protected groups; it _avoids disparate treatment_ by not requiring the classifier to use group attributes as an input variable; and it can handle _multiple_ fairness constraints simultaneously and produce fairness-accuracy trade-off curves (instead of a single point). Additionally, our proof techniques based on Blackwell's results on comparing statistical experiments are unique and may be of particular interest to fair ML and information theory communities. We present a detailed comparison with this line of work in Table 2 of Appendix E.

We recast the fairness Pareto frontier in terms of the conditional distribution \(P_{\hat{\mathrm{Y}}|\mathrm{Y,S}}\) of predicted outcome \(\hat{\mathrm{Y}}\) given true label \(\mathrm{Y}\) and group attributes \(\mathrm{S}\). This conditional distribution is related to confusion matrices conditioned on each subgroup. In this regard, our work is related to Verma and Rubin (2018); Alghamdi et al. (2020); Kim et al. (2020); Yang et al. (2020); Berk et al. (2021), which observed that many group fairness metrics can be written in terms of the confusion matrices for each subgroup. Among them, the closest work to ours is Kim et al. (2020), which optimized accuracy and fairness objectives over these confusion matrices and proposed a post-processing technique for training fair classifiers. However, they only imposed marginal sum constraints for the confusion matrices. We demonstrate that the feasible region of confusion matrices can be much smaller (see Remark 2 for an example), leading to a tighter approximation of the fairness Pareto frontier.

Recently, many strategies have been proposed to reduce the tension between group fairness and model performance by investigating properties of the data distribution. For example, Blum and Stangl (2019); Suresh and Guttag (2019); Fogliato et al. (2020); Wang et al. (2020); Mehrotra and Celis (2021); Fernando et al. (2021); Wang and Singh (2021); Zhang and Long (2021); Tomasev et al. (2021); Jacobs and Wallach (2021); Kallus et al. (2022); Jeong et al. (2022) studied how noisy or missing data affect fairness and model accuracy. Dwork et al. (2018); Ustun et al. (2019); Wang et al. (2021) considered training a separate classifier for each subgroup when their data distributions are different. Another line of research introduces data pre-processing techniques that manipulate data distribution for reducing its bias (e.g., Calmon et al., 2017; Kamiran and Calders, 2012). Among all these works, the closest one to ours is Chen et al. (2018), which decomposed group fairness measures into bias, variance, and noise (see their Theorem 1) and proposed strategies for reducing each term. Compared with Chen et al. (2018), the main difference is that we characterize a fairness Pareto frontier that depends on fairness metrics _and_ a performance measure, giving a complete picture of how the data distribution influences fairness and accuracy.

## 2 Preliminaries

Next, we introduce notation, overview the key results in Blackwell (1953) on comparisons of experiments, and outline the fair classification setup considered in this paper.

Notation.For a positive integer \(n\), let \([n]\triangleq\{1,\cdots,n\}\). We denote all probability distributions on the set \(\mathcal{X}\) by \(\mathcal{P}(\mathcal{X})\). Moreover, we define the probability simplex \(\Delta_{m}\triangleq\mathcal{P}([m])\). When random variables \(\mathrm{A}\), \(\mathrm{X}\), \(\mathrm{Z}\) form a Markov chain, we write \(\mathrm{A}-\mathrm{X}-\mathrm{Z}\). We write the mutual information between \(\mathrm{A}\), \(\mathrm{X}\) as \(I(\mathrm{A};\mathrm{X})\triangleq\mathbb{E}_{P_{\mathrm{A},\mathrm{X}}} \left[\log\frac{P_{\mathrm{A},\mathrm{X}}(\mathrm{X})}{P_{\mathrm{A}}(\mathrm{ A})P_{\mathrm{X}}(\mathrm{X})}\right]\). Since \(I(\mathrm{A};\mathrm{X})\) is determined by the marginal distribution \(P_{\mathrm{A}}\) and the conditional distribution \(P_{\mathrm{X}|\mathrm{A}}\), we also write \(I(\mathrm{A};\mathrm{X})\) as \(I(P_{\mathrm{A}};P_{\mathrm{X}|\mathrm{A}})\). When \(\mathrm{A}\), \(\mathrm{X}\) are independent, we write \(\mathrm{A}\_\|\mathrm{X}\).

If a random variable \(\mathrm{A}\in[n]\) has finite support, the conditional distribution \(P_{\mathrm{X}|\mathrm{A}}:[n]\rightarrow\mathcal{P}(\mathcal{X})\) can be equivalently written as \(\bm{P}\triangleq(P_{1},\cdots,P_{n})\) where each \(P_{i}=P_{\mathrm{X}|\mathrm{A}=i}\in\mathcal{P}(\mathcal{X})\). Additionally, if \(\mathcal{X}\) is a finite set \([m]\), then \(P_{\mathrm{X}|\mathrm{A}}\) can be fully characterized by a transition matrix. We use \(\mathcal{T}(m|n)\) to denote all transition matrices from \([n]\) to \([m]\): \(\Big{\{}\bm{P}\in\mathbb{R}^{n\times m}\Bigm{|}0\leq P_{i,j}\leq 1,\sum_{j=1}^{m}P_{i,j }=1,\forall i\in[n]\Big{\}}\).

Comparisons of ExperimentsGiven two statistical experiments (i.e., conditional distributions) \(\bm{P}\) and \(\bm{Q}\), is there a way to decide which one is more informative? Here \(\bm{P}\) and \(\bm{Q}\) have the common input alphabet \([n]\) and potentially different output spaces. Blackwell gave an answer in his seminal work (Blackwell, 1953) from a decision-theoretic perspective. We review these results next.

Let \(\mathcal{A}\) be a closed, bounded, convex subset of \(\mathbb{R}^{n}\). A decision function \(\bm{f}(x)=(a_{1}(x),\cdots,a_{n}(x))\) is any mapping from \(\mathcal{X}\) to \(\mathcal{A}\). It is associated with a loss vector:

\[\bm{v}(\bm{f})=\left(\int a_{1}(x)\mathrm{d}P_{1}(x),\cdots,\int a_{n}(x) \mathrm{d}P_{n}(x)\right).\] (1)

The collection of all \(\bm{v}(\bm{f})\) is denoted by \(\mathcal{B}(\bm{P},\mathcal{A})\). Blackwell defined that \(\bm{P}\) is more informative than \(\bm{Q}\) if for every \(\mathcal{A}\), \(\mathcal{B}(\bm{P},\mathcal{A})\supseteq\mathcal{B}(\mathcal{Q},\mathcal{A})\). Intuitively, this result means any risk achievable with \(\bm{Q}\) is also achievable with \(\bm{P}\). Moreover, Blackwell considered the standard measure \(P^{*}\) which is the probability distribution of \(\bm{p}(\bar{\mathbf{X}})\) where \(\bm{p}(x):\mathcal{X}\rightarrow\Delta_{n}\) is a function defined as

\[\left(\frac{\mathrm{d}P_{1}}{\mathrm{d}P_{1}+\cdots+\mathrm{d}P_{n}},\cdots, \frac{\mathrm{d}P_{n}}{\mathrm{d}P_{1}+\cdots+\mathrm{d}P_{n}}\right).\] (2)

and \(\bar{\mathbf{X}}\) follows the probability distribution \(\frac{P_{1}+\cdots+P_{n}}{n}\). One of the most important findings by Blackwell in his paper is to discover the following equivalent conditions.

**Lemma 1** (Blackwell (1951, 1953)).: _The following three conditions are equivalent:_

* \(\bm{P}\) _is more informative than_ \(\bm{Q}\)_;_
* _for any continuous and convex function_ \(\phi:\Delta_{n}\rightarrow\mathbb{R}\)_,_ \(\int\phi(\bm{p})dP^{*}(\bm{p})\geq\int\phi(\bm{p})dQ^{*}(\bm{p})\)_;_
* _there is a stochastic transformation_ \(\mathsf{T}\) _s.t._ \(\mathsf{T}P_{i}=Q_{i}\)_. In other words, there exists a Markov chain_ \(\mathbf{A}-\mathbf{X}-\mathsf{Z}\) _for any distributions on_ \(\mathbf{A}\) _such that_ \(\bm{P}=P_{\mathrm{X}|\mathbf{A}}\) _and_ \(\bm{Q}=P_{\mathrm{Z}|\mathbf{A}}\)_._

If \(\bm{P}=P_{\mathrm{X}|\mathbf{A}}\) is more informative than \(\bm{Q}=P_{\mathrm{Z}|\mathbf{A}}\), by the third condition of Lemma 1 and the data processing inequality, \(I(P_{\mathrm{A}};P_{\mathrm{X}|\mathbf{A}})\geq I(P_{\mathrm{A}};P_{\mathrm{Z }|\mathbf{A}})\) holds for _any_ marginal distribution \(P_{\mathrm{A}}\). However, the converse does not hold in general--even if the above inequality holds for any \(P_{\mathrm{A}}\), \(\bm{P}\) is not necessarily more informative than \(\bm{Q}\)(Rauh et al., 2017). In this regard, Blackwell's conditions are "stronger" than the mutual information based data processing inequality.

#### Group Fair Classification

Consider a multi-class classification task, where the goal is to train a probabilistic classifier \(h:\mathcal{X}\rightarrow\Delta_{C}\) that uses input features \(\mathbf{X}\) to predict their true label \(\mathrm{Y}\in[C]\). Additionally, assume the classifier produces a predicted outcome \(\hat{\mathrm{Y}}\in[C]\) and let \(\mathbf{S}\in[A]\) represent group attributes (e.g., race and sex). Depending on the domain of interest, \(\mathbf{X}\) can either include or exclude \(\mathbf{S}\) as an input to the classifier. Our framework can be easily extended to the setting where multiple subgroups overlap (Kearns et al., 2018). Throughout this paper, we focus on three standard group fairness measures: statistical parity (SP) (Feldman et al., 2015), equalized odds (EO) (Hardt et al., 2016; Pleiss et al., 2017), and overall accuracy equality (OAE) (Berk et al., 2021) (see Table 1 for their definitions) but our analysis can be extended to many other group fairness metrics, including the ones in Table 1 of Kim et al. (2020), as well as alternative performance measures beyond accuracy.

\begin{table}
\begin{tabular}{l l l} \hline \hline Fairness Metric & Abbr. & Definition \\  & & Expression w.r.t. \(\bm{P}\) \\ \hline Statistical Parity & \(\mathsf{SP}\leq\alpha_{\text{tr}}\) & \(|\Pr(\hat{\mathrm{Y}}=\hat{y}|\mathbf{S}=s)-\Pr(\hat{\mathrm{Y}}=\hat{y}| \mathbf{S}=s^{\prime})|\leq\alpha_{\text{tr}}\) \\  & \(\left|\sum_{y=1}^{C}\left(\frac{\mu_{s,y}}{\mu_{s}}P_{(y,y),\hat{\theta}}-\frac{ \mu_{s^{\prime},y}}{\mu_{s^{\prime}}}P_{(s^{\prime},y),\hat{y}}\right)\right|\leq \alpha_{\text{tr}}\) \\ \hline Equalized Odds & \(\mathsf{EO}\leq\alpha_{\text{tr}}\) & \(|\Pr(\hat{\mathrm{Y}}=\hat{y}|\mathbf{S}=s,\mathbf{Y}=y)-\Pr(\hat{\mathrm{Y}}= \hat{y}|\mathbf{S}=s^{\prime},\mathrm{Y}=y)|\leq\alpha_{\text{tr}}\) \\  & \(P_{(s,y),\hat{y}}-P_{(s^{\prime},y),\hat{y}}|\leq\alpha_{\text{tr}}\) \\ \hline Overall Accuracy Equality & \(\mathsf{OAE}\leq\alpha_{\text{aux}}\) & \(|\Pr(\hat{\mathrm{Y}}=\mathrm{Y}|\mathbf{S}=s)-\Pr(\hat{\mathrm{Y}}=\mathrm{Y}| \mathbf{S}=s^{\prime})|\leq\alpha_{\text{aux}}\) \\  & \(\left|\sum_{y=1}^{C}\left(\frac{\mu_{s,y}}{\mu_{s}}P_{(s,y),y}-\frac{\mu_{s^{ \prime},y}}{\mu_{s^{\prime}}}P_{(s^{\prime},y),y}\right)\right|\leq\alpha_{ \text{aux}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Standard group fairness metrics under multi-group and multi-class classification tasks. Here \(\alpha_{\text{SP}},\alpha_{\text{tr}},\alpha_{\text{tr}},\alpha_{\text{tr}}, \in[0,1]\) are threshold parameters, \(\hat{y},y\in[C]\), \(s,s^{\prime}\in[A]\), and \(\mu_{s,y}\), \(\mu_{s}\) are defined in Proposition 1. Our analysis can be extended to many other group fairness metrics (see e.g., Table 1 in Kim et al., 2020).

## 3 Fairness Pareto Frontier

In this section, we introduce our main concept--fairness Pareto frontier (\(\mathsf{FairFront}\)). We use it to measure aleatoric discrimination and quantify epistemic discrimination by comparing a classifier's performance to the \(\mathsf{FairFront}\). We recast \(\mathsf{FairFront}\) in terms of the conditional distribution \(P_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}\) and apply Blackwell's conditions to characterize the feasible region of this conditional distribution. This effort converts a functional optimization problem into a convex program with a small number of variables. However, this convex program may involve infinitely many constraints. Hence, we introduce a greedy improvement algorithm that iteratively refines the approximation of \(\mathsf{FairFront}\) and tightens the feasible region of \(P_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}\). Finally, we establish a convergence guarantee for our algorithm.

Recall that we refer to aleatoric discrimination as the inherent biases of the data distribution that can lead to an unfair or inaccurate classifier. As its definition suggests, aleatoric discrimination only relies on properties of the data distribution and fairness metric of choice--it does not depend on the hypothesis class nor optimization method. Below we introduce \(\mathsf{FairFront}\) that delineates a curve of optimal accuracy over all probabilistic classifiers under certain fairness constraints for a given data distribution \(P_{\mathsf{S},\mathsf{X},\mathsf{Y}}\). We use \(\mathsf{FairFront}\) to quantify aleatoric discrimination.

**Definition 1**.: For \(\alpha_{\textsc{sp}},\alpha_{\textsc{EO}},\alpha_{\textsc{OE}}\geq 0\) and a given \(P_{\mathsf{S},\mathsf{X},\mathsf{Y}}\), we define \(\mathsf{FairFront}(\alpha_{\textsc{sp}},\alpha_{\textsc{EO}},\alpha_{\textsc{OE}})\) by

\[\mathsf{FairFront}(\alpha_{\textsc{sp}},\alpha_{\textsc{EO}}, \alpha_{\textsc{OE}})\triangleq\max_{h} \mathbb{E}\left[\mathbb{I}_{\hat{\mathsf{Y}}=\mathsf{Y}}\right]\] (3a) \[\text{s.t.} \mathsf{SP}\leq\alpha_{\textsc{sp}},\mathsf{EO}\leq\alpha_{ \textsc{EO}},\mathsf{OAE}\leq\alpha_{\textsc{OE}}\] (3b)

where \(\mathbb{I}\) is the indicator function; \(\hat{\mathsf{Y}}\) is produced by applying the classifier \(h\) to \(\mathsf{X}\); the maximum is taken over all measurable \(h\); and the definitions of \(\mathsf{SP}\), \(\mathsf{EO}\), and \(\mathsf{OAE}\) are in Table 1. As a special case, if \(\alpha_{\textsc{sp}},\alpha_{\textsc{EO}},\alpha_{\textsc{OE}}\geq 1\), then \(\mathsf{FairFront}(\alpha_{\textsc{sp}},\alpha_{\textsc{EO}},\alpha_{\textsc{OE}})\) is the accuracy of the Bayes optimal classifier.

Solving this functional optimization problem is difficult since it optimizes over all measurable classifiers. There is a line of works that proposed different fairness-intervention algorithms for training group-fair classifiers (see e.g., Menon and Williamson, 2018; Zhang et al., 2018; Zafar et al., 2019; Celis et al., 2019; Yang et al., 2020; Wei et al., 2021; Alghamdi et al., 2022). They restrict the model class and vary loss functions and optimizers to find classifiers that approach \(\mathsf{FairFront}\) as close as possible. However, these algorithms only describe a lower bound for \(\mathsf{FairFront}\). They do not determine what is the _best_ achievable accuracy for a given set of fairness constraints.

We circumvent the above-mentioned challenges by rewriting \(\mathsf{FairFront}\) in terms of the conditional distribution \(P_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}\). The caveat is that although each classifier yields a \(P_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}\), not every conditional distribution corresponds to a valid classifier. Hence, we introduce the following definition which characterizes all feasible \(P_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}\).

**Definition 2**.: Given \(P_{\mathsf{X}|\mathsf{S},\mathsf{Y}}\), we define \(\mathcal{C}\) as the set of all conditional distributions \(P_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}\) where \(\hat{\mathsf{Y}}\) is produced by some probabilistic classifier \(h\). In other words,

\[\mathcal{C}\triangleq\{P_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}\mid(\mathsf{ S},\mathsf{Y})-\mathsf{X}-\hat{\mathsf{Y}}\}.\] (4)

Throughout this paper, we write \(P_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}\) or its corresponding transition matrix \(\bm{P}\in\mathcal{T}(C|AC)\) interchangeably. Specifically, the \((C(s-1)+y)\)-th row, \(\hat{y}\)-th column of \(\bm{P}\) represents \(P_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}(\hat{y}|s,y)\) and we denote it by \(P_{(s,y),\hat{y}}\).

**Remark 1**.: We demonstrate the connection between the conditional distribution \(P_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}\) and confusion matrices in the setting of binary classification with binary groups. We define \(\hat{\mathcal{C}}\) as the counterpart of \(\mathcal{C}\) when we replace \(P_{\mathsf{X}|\mathsf{S},\mathsf{Y}}\) with an empirical distribution \(\hat{P}_{\mathsf{X}|\mathsf{S},\mathsf{Y}}\) computed from a dataset. The confusion matrix for group \(s\in\{0,1\}\) consists of four numbers: True Positive (\(\mathsf{TP}_{s}\)), False Positive (\(\mathsf{FP}_{s}\)), False Negative (\(\mathsf{FN}_{s}\)), True Negative (\(\mathsf{TN}_{s}\)). Assume that the number of positive-label data \(n_{s}^{+}=\mathsf{TP}_{s}+\mathsf{FN}_{s}\) and negative-label data \(n_{s}^{-}=\mathsf{TN}_{s}+\mathsf{FP}_{s}\) are given--these numbers do not depend on the classifier. Then there is a one-to-one mapping from each element in \(\hat{\mathcal{C}}\) to a confusion matrix:

\[\hat{P}_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}(1|s,1) =\frac{1}{n_{s}^{+}}\mathsf{TP}_{s},\quad\hat{P}_{\hat{\mathsf{Y} }|\mathsf{S},\mathsf{Y}}(1|s,0)=\frac{1}{n_{s}^{-}}\mathsf{FP}_{s},\] \[\hat{P}_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}(0|s,1) =\frac{1}{n_{s}^{+}}\mathsf{FN}_{s},\quad\hat{P}_{\hat{\mathsf{Y}}| \mathsf{S},\mathsf{Y}}(0|s,0)=\frac{1}{n_{s}^{-}}\mathsf{TN}_{s}.\]Hence, \(\hat{\mathcal{C}}\) essentially characterizes all feasible confusion matrices and \(\mathcal{C}\) is the population counterpart of \(\hat{\mathcal{C}}\). Note that \(\mathcal{C}\) is determined by the underlying data distribution while \(\hat{\mathcal{C}}\) (and confusion matrices) are tailored to a specific dataset.

**Proposition 1**.: \(\mathsf{FairFront}(\alpha_{\text{sr}},\alpha_{\text{io}},\alpha_{\text{o}\text{ u}})\) _in (3) is equal to the solution of the following convex optimization:_

\[\max_{\bm{P}\in\mathbb{R}^{AC\times C}} \sum_{s=1}^{A}\sum_{y=1}^{C}\mu_{s,y}P_{(s,y),y}\] (5a) \[\text{s.t.} \mathsf{SP}\leq\alpha_{\text{sr}},\mathsf{EO}\leq\alpha_{\text{ Eo}},\mathsf{OAE}\leq\alpha_{\text{OAE}}\] (5b) \[\bm{P}\in\mathcal{C}.\] (5c)

_Here the constants \(\mu_{s,y}\triangleq\Pr(\mathsf{S}=s,\mathsf{Y}=y)\) and \(\mu_{s}\triangleq\Pr(\mathsf{S}=s)\) for \(s\in[A]\), \(y\in[A]\) and \(P_{(s,y),\hat{y}}\) denotes the \((C(s-1)+y)\)-th row, \(\hat{y}\)-th column of the transition matrix \(\bm{P}\), which is \(P_{\hat{\mathsf{Y}}|\mathsf{S},\mathsf{Y}}(\hat{y}|s,y)\)._

For example, in binary classification with a binary group attribute, the above optimization only has \(8\) variables, \(14\) linear constraints + a single convex constraint \(\bm{P}\in\mathcal{C}\). Hence, standard convex optimization solvers can directly compute its optimal value as long as we know how to characterize \(\mathcal{C}\).

**Remark 2**.: Note that Kim et al. (2020) investigated fairness Pareto frontiers via confusion matrices. The main difference is that Definition 1 in Kim et al. (2020) relaxed the constraint (5c) to \(\bm{P}\in\mathcal{T}(C|AC)\) where \(\mathcal{T}(C|AC)\) represents _all_ transition matrices from \([AC]\) to \([C]\). This leads to a loose approximation of the frontier because \(\mathcal{C}\) is often a strict subset of \(\mathcal{T}(C|AC)\). To demonstrate this point, consider the scenario where \(\mathsf{X}\_\|\left(\mathsf{S},\mathsf{Y}\right)\). Then \(\bar{\mathsf{Y}}\_\|\left(\mathsf{S},\mathsf{Y}\right)\) by data processing inequality so

\[\mathcal{C}=\left\{\bm{P}\in\mathcal{T}(C|AC)\mid\text{each row of }\bm{P}\text{ is the same}\right\}.\] (6)

Optimizing over \(\mathcal{C}\) rather than \(\mathcal{T}(C|AC)\) can significantly tighten the fairness Pareto frontier.

Before diving into the analysis, we first introduce a function \(\bm{g}:\mathcal{X}\rightarrow\Delta_{AC}\) defined as \(\bm{g}(x)=\left(P_{\mathsf{S},\mathsf{Y}|\mathsf{X}}(1,1|x),\cdots,P_{\mathsf{ S},\mathsf{Y}|\mathsf{X}}(A,C|x)\right).\) To obtain this function in practice, a common strategy among various post-processing fairness interventions (see e.g., Menon and Williamson, 2018; Alghamdi et al., 2022) is to train a probabilistic classifier that uses input features \(\mathsf{X}\) to predict \((\mathsf{S},\mathsf{Y})\). The output probability generated by this classifier is then utilized as an approximation of the function \(\bm{g}\).

The following theorem is the main theoretical result in this paper. It provides a precise characterization of the set \(\mathcal{C}\) through a series of convex constraints.

**Theorem 1**.: _The set \(\mathcal{C}\) is the collection of all transition matrices \(\bm{P}\in\mathcal{T}(C|AC)\) such that the following condition holds: For any \(k\in\mathcal{N}\) and any \(\{\bm{a}_{i}\mid\bm{a}_{i}\in[-1,1]^{AC},i\in[k]\}\),_

\[\sum_{\hat{y}=1}^{C}\max_{i\in[k]}\left\{\bm{a}_{i}^{T}\bm{\Lambda}_{\bm{\mu} }\bm{p}_{\hat{y}}\right\}\leq\mathbb{E}\left[\max_{i\in[k]}\{\bm{a}_{i}^{T} \bm{g}(\mathsf{X})\}\right],\] (7)

_where \(\bm{p}_{\hat{y}}\) is the \(\hat{y}\)-th column of \(\bm{P}\) and \(\bm{\Lambda}_{\mu}=\mathsf{diag}(\mu_{1,1},\cdots,\mu_{A,C})\)._

Intuitively, (7) uses piecewise linear functions to approximate the boundary of the convex set \(\mathcal{C}\) where \(k\) represents the number of linear pieces. Unfortunately, replacing \(\bm{P}\in\mathcal{C}\) with this series of constraints in (5) may result in an intractable problem since standard duality-based approaches will lead to infinitely many dual variables. To resolve this issue, we first fix \(k\) and let \(\mathcal{C}_{k}\) be the set of \(\bm{P}\) such that (7) holds under this fixed \(k\). Accordingly, we define \(\mathsf{FairFront}_{k}(\alpha_{\text{sr}},\alpha_{\text{Eo}},\alpha_{\text{o} \text{o}\text{o}\text{o}})\) as the optimal value of (5) when replacing \(\mathcal{C}\) with \(\mathcal{C}_{k}\). Since \(\mathcal{C}_{1}\supseteq\mathcal{C}_{2}\supseteq\cdots\supseteq\mathcal{C}\), we have \(\mathsf{FairFront}_{1}(\alpha_{\text{sr}},\alpha_{\text{Eo}},\alpha_{\text{ o}\text{o}\text{o}\text{o}})\geq\mathsf{FairFront}_{2}(\alpha_{\text{sr}}, \alpha_{\text{Eo}},\alpha_{\text{o}\text{o}\text{o}\text{o}})\geq\cdots \geq\mathsf{FairFront}(\alpha_{\text{sr}},\alpha_{\text{Eo}},\alpha_{\text{ o}\text{o}\text{o}\text{o}}).\) However, computing \(\mathsf{FairFront}_{k}(\alpha_{\text{sr}},\alpha_{\text{Eo}},\alpha_{\text{ o}\text{o}\text{o}\text{o}})\) still involves infinitely many constraints.

Next, we introduce a greedy improvement algorithm that consists of solving a sequence of tractable optimization problems for approximating \(\mathsf{FairFront}_{k}(\alpha_{\text{sr}},\alpha_{\text{Eo}},\alpha_{\text{o} \text{o}\text{o}})\). We use \(\mathcal{A}\) to collect the constraints of \(\bm{P}\) and set \(\mathcal{A}=\emptyset\) initially. At each iteration, our algorithm solves a convex program to find an optimal \(\bm{P}\) that maximizes the accuracy while satisfying the desired group fairness constraints and the constraints in \(\mathcal{A}\); then we verify if this \(\bm{P}\) is within the set \(\mathcal{C}_{k}\) by solving a DC (difference of convex) program (Shen et al., 2016; Horst and Thoai, 1999). If \(\bm{P}\in\mathcal{C}_{k}\), then the algorithm stops. Otherwise, the algorithm will find the constraint that is mostly violated by \(\bm{P}\) and add this constraint to \(\mathcal{A}\). Specifically, we determine a piecewise linear function that divides the space into two distinct regions: one containing \(\bm{P}\) and the other containing \(\mathcal{C}_{k}\). By "mostly violated", we mean the function is constructed to maximize the distance between \(\bm{P}\) and the boundary defined by the function. We describe our algorithm in Algorithm 1 and establish a convergence guarantee below.

``` Input:\(\mathcal{D}=\{(x_{i},y_{i},s_{i})\}_{i=1}^{N}\), max number of iterations \(T\); max pieces \(k\); classifier \(g(x)\); \(\alpha_{\textsc{sp}}\), \(\alpha_{\textsc{lo}}\), \(\alpha_{\textsc{oae}}\). Initialize: set \(\mathcal{A}=\emptyset\); \(\mu_{s,y}=\frac{\{|i|s_{i}=s,y_{i}=y\}|}{N}\); \(t=1\). Repeat:  Solve a convex program: \[\max_{\bm{P}} \sum_{s=1}^{A}\sum_{y=1}^{C}\mu_{s,y}P_{(s,y),y}\] \[\text{s.t.} \bm{P}\in\mathcal{T}(|AC),\mathsf{SP}\leq\alpha_{\textsc{sp}}, \mathsf{EO}\leq\alpha_{\textsc{lo}},\mathsf{OAE}\leq\alpha_{\textsc{oav}}\] \[\sum_{\hat{y}=1}^{C}\max_{i\in[k]}\left\{\bm{a}_{i}^{T}\bm{ \Lambda}_{\mu}\bm{p}_{\hat{y}}\right\}\leq\mathbb{E}\left[\max_{i\in[k]}\{\bm{a }_{i}^{T}\bm{g}(\mathbf{X})\}\right]\quad\forall(\bm{a}_{1},\cdots,\bm{a}_{k}) \in\mathcal{A}.\] Let \(v^{t}\) and \(\bm{P}^{t}\) be the optimal value and optimal solution.  Solve a DC program: \[\min_{\begin{subarray}{c}\bm{a}_{i}\in[-1,1]^{AC}\\ i\in[k]\end{subarray}}\;\mathbb{E}\left[\max_{i\in[k]}\{\bm{a}_{i}^{T}\bm{g}( \mathbf{X})\}\right]-\sum_{\hat{y}=1}^{C}\max_{i\in[k]}\left\{\bm{a}_{i}^{T} \bm{\Lambda}_{\mu}\bm{p}_{\hat{y}}^{t}\right\}.\] If the optimal value is \(\geq 0\) or \(t=T\), stop; otherwise,  add the optimal \((\bm{a}_{1},\cdots,\bm{a}_{k})\) to \(\mathcal{A}\) and \(t=t+1\). return:\(v^{t}\), \(\bm{P}^{t}\), \(\mathcal{A}\). ```

**Algorithm 1** Approximate the fairness Pareto frontier.

**Theorem 2**.: _Let \(T=\infty\). If Algorithm 1 stops, its output \(\bm{P}^{t}\) is an optimal solution of \(\mathsf{FairFront}_{k}(\alpha_{\textsc{sp}},\alpha_{\textsc{lo}},\alpha_{\textsc {oae}})\). Otherwise, any convergent sub-sequence of \(\{\bm{P}^{t}\}_{t=1}^{\infty}\) converges to an optimal solution of \(\mathsf{FairFront}_{k}(\alpha_{\textsc{sp}},\alpha_{\textsc{lo}},\alpha_{ \textsc{oae}})\)._

Note that the output \(v^{t}\) from Algorithm 1 is always an _upper bound_ for \(\mathsf{FairFront}(\alpha_{\textsc{sp}},\alpha_{\textsc{lo}},\alpha_{\textsc {oae}})\), assuming the estimation error is sufficiently small. The tightness of this upper bound is determined by \(k\) (i.e., how well the piecewise linear functions approximate the boundary of \(\mathcal{C}\)), \(T\) (i.e., the total number of iterations). On the other hand, running off-the-shelf in-processing and post-processing fairness interventions can only yield _lower bounds_ for \(\mathsf{FairFront}(\alpha_{\textsc{sp}},\alpha_{\textsc{lo}},\alpha_{\textsc {oae}})\).

## 4 Numerical Experiments

In this section, we demonstrate the tightness of our upper bound approximation of \(\mathsf{FairFront}\), apply it to benchmark existing group fairness interventions, and show how data biases, specifically missing values, impact their effectiveness. We find that given sufficient data, SOTA fairness interventions are successful at reducing epistemic discrimination as their gap to (our upper bound estimate of) \(\mathsf{FairFront}\) is small. However, we also discover that when different population groups have varying missing data patterns, aleatoric discrimination increases, which diminishes the performance of fairness intervention algorithms. Our numerical experiments are semi-synthetic since we apply fairness interventions to train classifiers using the _entire_ dataset and resample from it as the test set. This setup enables us to eliminate the estimation error associated with Algorithm 1 (see Appendix E for a discussion). We provide additional experimental results and details in Appendix C.

### Benchmark Fairness Interventions

Setup.We evaluate our results on the UCI Adult dataset (Bache and Lichman, 2013), the ProPublica COMPAS dataset (Angwin et al., 2016), the German Credit dataset (Bache and Lichman, 2013), and HLSLS (High School Longitudinal Study) dataset (Ingels et al., 2011; Jeong et al., 2022). We recognize that Adult, COMPAS, and German Credit datasets are overused and acknowledge the recent calls to move away from them (see e.g., Ding et al., 2021). We adopt these datasets for benchmarking purposes only since most fairness interventions have available code for these datasets. The HLS dataset is a new dataset that first appeared in the fair ML literature last year and captures a common use-case of ML in education (student performance prediction, see Jeong et al., 2022). It has multi-class labels and multiple protected groups. We apply existing (group) fairness interventions to these datasets and measure their fairness violations via _Max equalized odds_:

\[\max\,|\Pr(\hat{\mathrm{Y}}=\hat{y}|\mathrm{S}=s,\mathrm{Y}=y)-\Pr(\hat{ \mathrm{Y}}=\hat{y}|\mathrm{S}=s^{\prime},\mathrm{Y}=y)|\]

where the max is taken over \(y,\hat{y},s,s^{\prime}\). We run Algorithm 1 with \(k=6\) pieces, \(20\) iterations, and varying \(\alpha_{\mathrm{EO}}\) to estimate FairFront on each dataset. We compute the expectations and the \(g\) function from the empirical distributions and solve the DC program by using the package in Shen et al. (2016). The details about how we pre-process these datasets and additional experimental results on the German Credit and HLSLS datasets are deferred to Appendix C.

Group fairness interventions.We consider five existing fairness-intervention algorithms: Reduction (Agarwal et al., 2018), EqQdds (Hardt et al., 2016), CalEqQdds (Pleiss et al., 2017), LevEqQpp (Chzhen et al., 2019), and FairProjection Alghamdi et al. (2022). Among them, Reduction is an in-processing method and the rest are all post-processing methods. For the first three benchmarks, we use the implementations from IBM AIF360 library (Bellamy et al., 2018); for LevEqQpp and FairProjection, we use the Python implementations from the Github repo in Alghamdi et al. (2022). For Reduction and FairProjection, we can vary their tolerance of fairness violations to produce a fairness-accuracy curve; for EqQdds, CalEqQdds, and LevEqQpp, each of them produces a single point since they only allow hard equality constraint. We note that FairProjection is optimized for transforming probabilistic classifier outputs (see also Wei et al., 2021), but here we threshold the probabilistic outputs to generate binary predictions which may limit its performance. Finally, we train a random forest as the Baseline classifier.

Results.We observe that if we run Algorithm 1 for a single iteration, which is equivalent to solving Proposition 1 without (5c), its solution is very close to \(1\) for all \(\alpha_{\mathrm{EO}}\). This demonstrates the benefits of incorporating Blackwell's conditions into the fairness Pareto frontier.

Figure 1: We compare Reduction and FairProjection with (our upper bound estimate of) FairFront on the Adult (Left) and COMPAS (Right) datasets. We train a classifier that approximates the Bayes optimal and use it as a basis for Reduction and FairProjection. This result not only demonstrates the tightness of our approximation but also shows that SOTA fairness interventions have already achieved near-optimal fairness-accuracy curves.

We train a classifier that approximates the Bayes optimal and use it as a basis for both Reduction and FairProjection, which are SOTA fairness interventions. We then apply these two fairness interventions to the entire dataset and evaluate their performance on the same dataset. Figure 1 shows that in this infinite sample regime, the fairness-accuracy curves produced by Reduction and FairProjection can approach our upper bound estimate of FairFront. This result not only demonstrates the tightness of our approximation (recall that Algorithm 1 gives an upper bound of FairFront and existing fairness interventions give lower bounds) but also shows that SOTA fairness interventions have already achieved near-optimal fairness-accuracy curves.

Recall that we use FairFront to quantify aleatoric discrimination since it characterizes the highest achievable accuracy among all classifiers satisfying the desired fairness constraints. Additionally, we measure epistemic discrimination by comparing a classifier's accuracy and fairness violation with FairFront. Given that our Algorithm 1 provides a tight approximation of FairFront, we use it to benchmark existing fairness interventions. Specifically, we first train a base classifier which may not achieve Bayes optimal accuracy. Then we use it as a basis for all existing fairness interventions. The results in Figure 2 show that SOTA fairness interventions remain effective at reducing epistemic discrimination. In what follows, we demonstrate how missing values in data can increase aleatoric discrimination and dramatically reduce the effectiveness of SOTA fairness interventions.

### Fairness Risks in Missing Values

Real-world data often have missing values and the missing patterns can be different across different protected groups (see Jeong et al., 2022, for some examples). There is a growing line of research (see e.g., Jeong et al., 2022; Fernando et al., 2021; Wang and Singh, 2021; Subramonian et al., 2022; Caton et al., 2022; Zhang and Long, 2021; Schelter et al., 2019) studying the fairness risks of data with missing values. In this section, we apply our result to demonstrate how disparate missing patterns influence the fairness-accuracy curves.

Setup.We choose sex (group 0: female, group 1: male) as the group attribute for the Adult dataset, and race (group 0: African-American, group 1: Caucasian) for the COMPAS dataset. To investigate the impact of disparate missing patterns on aleatoric discrimination, we artificially generate missing values in both datasets. This is necessary as the datasets do not contain sufficient missing data. The missing values are generated according to different probabilities for different population groups. For each data point from group 0, we erase each input feature with a varying probability \(p_{0}\in\{10\%,50\%,70\%\}\), while for group 1, we erase each input feature with a fixed probability \(p_{1}=10\%\). We then apply mode imputation to the missing values, replacing them with the mode of non-missing values for each feature. Finally, we apply Algorithm 1 along with Reduction and Baseline to the imputed data. The experimental results are shown in Figure 3.

Figure 2: We benchmark existing fairness interventions using (our upper bound estimate of) FairFront. We use FairFront to quantify aleatoric discrimination and measure epistemic discrimination by comparing a classifierâ€™s accuracy and fairness violation with FairFront. The results show that SOTA fairness interventions are effective at reducing epistemic discrimination.

Results.As we increase the missing probability of group 0, (our upper bound estimate of) FairFront decreases since it becomes more difficult to accurately predict outcomes for group 0. This in turn affects the overall model performance, since the fairness constraint requires that the model performs similarly for both groups. We also observe the fairness-accuracy curves of Reduction decrease as the missing data for group 0 become more prevalent. In other words, as the missing data for group 0 increase, it becomes more difficult to maintain both high accuracy and fairness in the model's prediction.

## 5 Final Remarks

The past years have witnessed a growing line of research introducing various group fairness-intervention algorithms. Most of these interventions focus on optimizing model performance subject to group fairness constraints. Though comparing and benchmarking these methods on various datasets is valuable (e.g., see benchmarks in Friedler et al., 2019; Bellamy et al., 2019; Wei et al., 2021), this does not reveal if there is still room for improvement in their fairness-accuracy curves, or if existing methods approach the information-theoretic optimal limit when infinite data is available. Our results address this gap by introducing the fairness Pareto frontier, which measures the highest possible accuracy under a set of group fairness constraints. We precisely characterize the fairness Pareto frontier using Blackwell's conditions and present a greedy improvement algorithm that approximates it from data. Our results show that the fairness-accuracy curves produced by SOTA fairness interventions are very close to the fairness Pareto frontier on standard datasets.

Additionally, we demonstrate that when data are biased due to missing values, the fairness Pareto frontier degrades. Although existing fairness interventions can still reduce performance disparities, they come at the cost of significantly lowering overall model accuracy. The methods we present for computing the fairness Pareto frontier can also be applied to analyze other sources of aleatoric discrimination, such as when individuals may misreport their data or when there are measurement errors. Overall, the fairness Pareto frontier can serve as a valuable framework for guiding data collection and cleaning. Our results indicate that existing fairness interventions can be effective in reducing epistemic discrimination, and there are diminishing returns in developing new fairness interventions focused solely on optimizing accuracy for a given group fairness constraint on pristine data. However, existing fairness interventions have yet to effectively provide both fair and accurate classification when additional sources of aleatoric discrimination are present (such as missing values in data). This suggests that there is still significant need for research on handling aleatoric sources of discrimination that appear throughout the data collection process.

We provide an in-depth discussion on future work in Appendix E.

Figure 3: Fairness risks of disparate missing patterns. The missing probabilities of group 0 (female in Adult/African-American in COMPAS) and group 1 (male in Adult/Caucasian in COMPAS) are varying among \(\{(10\%,10\%),(50\%,10\%),(70\%,10\%)\}\). We apply Reduction and Baseline to the imputed data and plot their fairness-accuracy curves against FairFront. As shown, the effectiveness of fairness interventions substantially decrease with increasing disparate missing patterns in data.

## Acknowledgement

This material is based upon work supported by the National Science Foundation under grants CAREER 1845852, CIF 2312667, FAI 2040880, CIF 1900750.

## References

* Agarwal et al. (2018) Agarwal, A., Beygelzimer, A., Dudik, M., Langford, J., and Wallach, H. (2018). A reductions approach to fair classification. In _International Conference on Machine Learning_, pages 60-69. PMLR.
* Alghamdi et al. (2020) Alghamdi, W., Asoodeh, S., Wang, H., Calmon, F. P., Wei, D., and Ramamurthy, K. N. (2020). Model projection: Theory and applications to fair machine learning. In _2020 IEEE International Symposium on Information Theory (ISIT)_, pages 2711-2716. IEEE.
* Alghamdi et al. (2022) Alghamdi, W., Hsu, H., Jeong, H., Wang, H., Michalak, P. W., Asoodeh, S., and Calmon, F. P. (2022). Beyond Adult and COMPAS: Fair multi-class prediction via information projection. In _Advances in Neural Information Processing Systems_.
* Angwin et al. (2016) Angwin, J., Larson, J., Mattu, S., and Kirchner, L. (2016). Machine bias. _ProPublica_.
* Bache and Lichman (2013) Bache, K. and Lichman, M. (2013). UCI Machine Learning Repository.
* Bellamy et al. (2018) Bellamy, R. K., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., Lohia, P., Martino, J., Mehta, S., Mojsilovic, A., et al. (2018). Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. _arXiv preprint arXiv:1810.01943_.
* Bellamy et al. (2019) Bellamy, R. K., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., Lohia, P., Martino, J., Mehta, S., Mojsilovic, A., et al. (2019). Ai fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. _IBM Journal of Research and Development_, 63(4/5):4-1.
* Berk et al. (2021) Berk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A. (2021). Fairness in criminal justice risk assessments: The state of the art. _Sociological Methods & Research_, 50(1):3-44.
* Blackwell (1951) Blackwell, D. (1951). Comparison of experiments. _Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability_, pages 93-102.
* Blackwell (1953) Blackwell, D. (1953). Equivalent comparisons of experiments. _The annals of mathematical statistics_, pages 265-272.
* Blodgett et al. (2020) Blodgett, S. L., Barocas, S., Daume III, H., and Wallach, H. (2020). Language (technology) is power: A critical survey of" bias" in nlp. _arXiv preprint arXiv:2005.14050_.
* Blum and Stangl (2019) Blum, A. and Stangl, K. (2019). Recovering from biased data: Can fairness constraints improve accuracy? _arXiv preprint arXiv:1912.01094_.
* Calmon et al. (2017) Calmon, F., Wei, D., Vinzamuri, B., Natesan Ramamurthy, K., and Varshney, K. R. (2017). Optimized pre-processing for discrimination prevention. _Advances in neural information processing systems_, 30.
* Cam (1964) Cam, L. L. (1964). Sufficiency and approximate sufficiency. _The Annals of Mathematical Statistics_, pages 1419-1455.
* Caton et al. (2022) Caton, S., Malisetty, S., and Haas, C. (2022). Impact of imputation strategies on fairness in machine learning. _Journal of Artificial Intelligence Research_, 74:1011-1035.
* Celis et al. (2019) Celis, L. E., Huang, L., Keswani, V., and Vishnoi, N. K. (2019). Classification with fairness constraints: A meta-algorithm with provable guarantees. In _Proceedings of the conference on fairness, accountability, and transparency_, pages 319-328.
* Chen et al. (2018) Chen, I., Johansson, F. D., and Sontag, D. (2018). Why is my classifier discriminatory? _Advances in neural information processing systems_, 31.
* Chouldechova (2017) Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. _Big data_, 5(2):153-163.
* Chouldechova (2018)Chzhen, E., Denis, C., Hebiri, M., Oneto, L., and Pontil, M. (2019). Leveraging labeled and unlabeled data for consistent fair binary classification. _Advances in Neural Information Processing Systems_, 32.
* Cohen et al. (1998) Cohen, J., Kempermann, J. H., and Zbaganu, G. (1998). _Comparisons of stochastic matrices with applications in information theory, statistics, economics and population_. Springer Science & Business Media.
* Corbett-Davies et al. (2017) Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., and Huq, A. (2017). Algorithmic decision making and the cost of fairness. In _Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining_, pages 797-806.
* Ding et al. (2021) Ding, F., Hardt, M., Miller, J., and Schmidt, L. (2021). Retiring adult: New datasets for fair machine learning. _Advances in neural information processing systems_, 34:6478-6490.
* Dutta et al. (2020) Dutta, S., Wei, D., Yueksel, H., Chen, P.-Y., Liu, S., and Varshney, K. (2020). Is there a trade-off between fairness and accuracy? a perspective using mismatched hypothesis testing. In _International Conference on Machine Learning_, pages 2803-2813. PMLR.
* Dwork et al. (2018) Dwork, C., Immorlica, N., Kalai, A. T., and Leiserson, M. (2018). Decoupled classifiers for group-fair and efficient machine learning. In _Conference on fairness, accountability and transparency_, pages 119-133. PMLR.
* Feldman et al. (2015) Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., and Venkatasubramanian, S. (2015). Certifying and removing disparate impact. In _proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining_, pages 259-268.
* Fernando et al. (2021) Fernando, M.-P., Cesar, F., David, N., and Jose, H.-O. (2021). Missing the missing values: The ugly duckling of fairness in machine learning. _International Journal of Intelligent Systems_, 36(7):3217-3258.
* Fogliato et al. (2020) Fogliato, R., Chouldechova, A., and G'Sell, M. (2020). Fairness evaluation in presence of biased noisy labels. In _International Conference on Artificial Intelligence and Statistics_, pages 2325-2336. PMLR.
* Friedler et al. (2019) Friedler, S. A., Scheidegger, C., Venkatasubramanian, S., Choudhary, S., Hamilton, E. P., and Roth, D. (2019). A comparative study of fairness-enhancing interventions in machine learning. In _Proceedings of the conference on fairness, accountability, and transparency_, pages 329-338.
* Globus-Harris et al. (2023) Globus-Harris, I., Gupta, V., Jung, C., Kearns, M., Morgenstern, J., and Roth, A. (2023). Multicalibrated regression for downstream fairness. In _Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society_, pages 259-286.
* Gopalan et al. (2021) Gopalan, P., Kalai, A. T., Reingold, O., Sharan, V., and Wieder, U. (2021). Omnipredictors. _arXiv preprint arXiv:2109.05389_.
* Hardt et al. (2016) Hardt, M., Price, E., and Srebro, N. (2016). Equality of opportunity in supervised learning. In _Advances in Neural Information Processing Systems_, volume 29.
* Horst and Thoai (1999) Horst, R. and Thoai, N. V. (1999). Dc programming: overview. _Journal of Optimization Theory and Applications_, 103(1):1-43.
* Hort et al. (2022) Hort, M., Chen, Z., Zhang, J. M., Sarro, F., and Harman, M. (2022). Bia mitigation for machine learning classifiers: A comprehensive survey. _arXiv preprint arXiv:2207.07068_.
* Hu et al. (2023) Hu, L., Navon, I. R. L., Reingold, O., and Yang, C. (2023). Omnipredictors for constrained optimization. In _International Conference on Machine Learning_, pages 13497-13527. PMLR.
* Hullermeier and Waegeman (2021) Hullermeier, E. and Waegeman, W. (2021). Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. _Machine Learning_, 110(3):457-506.
* Ingels et al. (2011) Ingels, S. J., Pratt, D. J., Herget, D. R., Burns, L. J., Dever, J. A., Ottem, R., Rogers, J. E., Jin, Y., and Leinwand, S. (2011). High school longitudinal study of 2009 (hsls: 09): Base-year data file documentation. neces 2011-328. _National Center for Education Statistics_.
* Held et al. (2018)Jacobs, A. Z. and Wallach, H. (2021). Measurement and fairness. In _Proceedings of the 2021 ACM conference on fairness, accountability, and transparency_, pages 375-385.
* Jeong et al. (2022) Jeong, H., Wang, H., and Calmon, F. P. (2022). Fairness without imputation: A decision tree approach for fair prediction with missing values. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 9558-9566.
* Jiang and Nachum (2020) Jiang, H. and Nachum, O. (2020). Identifying and correcting label bias in machine learning. In _International Conference on Artificial Intelligence and Statistics_, pages 702-712. PMLR.
* Jiang et al. (2020) Jiang, R., Pacchiano, A., Stepleton, T., Jiang, H., and Chiappa, S. (2020). Wasserstein fair classification. In _Uncertainty in Artificial Intelligence_, pages 862-872. PMLR.
* Kallus et al. (2022) Kallus, N., Mao, X., and Zhou, A. (2022). Assessing algorithmic fairness with unobserved protected class using data combination. _Management Science_, 68(3):1959-1981.
* Kamiran and Calders (2012) Kamiran, F. and Calders, T. (2012). Data preprocessing techniques for classification without discrimination. _Knowledge and information systems_, 33(1):1-33.
* Katzman et al. (2023) Katzman, J., Wang, A., Scheuerman, M., Blodgett, S. L., Laird, K., Wallach, H., and Barocas, S. (2023). Taxonomizing and measuring representational harms: A look at image tagging. _arXiv preprint arXiv:2305.01776_.
* Kearns et al. (2018) Kearns, M., Neel, S., Roth, A., and Wu, Z. S. (2018). Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. In _International Conference on Machine Learning_, pages 2564-2572. PMLR.
* Kim et al. (2020) Kim, J. S., Chen, J., and Talwalkar, A. (2020). Fact: A diagnostic for group fairness trade-offs. In _International Conference on Machine Learning_, pages 5264-5274. PMLR.
* Kim et al. (2019) Kim, M. P., Ghorbani, A., and Zou, J. (2019). Multiaccuracy: Black-box post-processing for fairness in classification. In _Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society_, pages 247-254.
* Kleinberg et al. (2016) Kleinberg, J., Mullainathan, S., and Raghavan, M. (2016). Inherent trade-offs in the fair determination of risk scores. _arXiv preprint arXiv:1609.05807_.
* Lowy et al. (2021) Lowy, A., Pavan, R., Baharlouei, S., Razaviyayn, M., and Beirami, A. (2021). Fermi: Fair empirical risk minimization via exponential Renyi mutual information. _arXiv preprint arXiv:2102.12586_.
* Martinez et al. (2020) Martinez, N., Bertran, M., and Sapiro, G. (2020). Minimax pareto fairness: A multi objective perspective. In _International Conference on Machine Learning_, pages 6755-6764. PMLR.
* Mayson (2019) Mayson, S. G. (2019). Bias in, bias out. _The Yale Law Journal_, 128(8):2218-2300.
* Mehrotra and Celis (2021) Mehrotra, A. and Celis, L. E. (2021). Mitigating bias in set selection with noisy protected attributes. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, pages 237-248.
* Menon and Williamson (2018) Menon, A. K. and Williamson, R. C. (2018). The cost of fairness in binary classification. In _Conference on Fairness, Accountability and Transparency_, pages 107-118. PMLR.
* Pedregosa et al. (2011) Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830.
* Pleiss et al. (2017) Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., and Weinberger, K. Q. (2017). On fairness and calibration. _Advances in neural information processing systems_, 30.
* Raginsky (2011) Raginsky, M. (2011). Shannon meets blackwell and le cam: Channels, codes, and statistical experiments. In _2011 IEEE International Symposium on Information Theory Proceedings_, pages 1220-1224. IEEE.
* Raghavan et al. (2018)Rauh, J., Banerjee, P. K., Olbrich, E., Jost, J., Bertschinger, N., and Wolpert, D. (2017). Coarse-graining and the blackwell order. _Entropy_, 19(10):527.
* Schelter et al. (2019) Schelter, S., He, Y., Khilnani, J., and Stoyanovich, J. (2019). Fairprep: Promoting data to a first-class citizen in studies on fairness-enhancing interventions. _arXiv preprint arXiv:1911.12587_.
* Shannon (1958) Shannon, C. E. (1958). A note on a partial ordering for communication channels. _Information and control_, 1(4):390-397.
* Shen et al. (2016) Shen, X., Diamond, S., Gu, Y., and Boyd, S. (2016). Disciplined convex-concave programming. In _2016 IEEE 55th Conference on Decision and Control (CDC)_, pages 1009-1014. IEEE.
* Subramonian et al. (2022) Subramonian, A., Chang, K.-W., and Sun, Y. (2022). On the discrimination risk of mean aggregation feature imputation in graphs. _Advances in Neural Information Processing Systems_.
* Suresh and Guttag (2019) Suresh, H. and Guttag, J. V. (2019). A framework for understanding unintended consequences of machine learning. _arXiv preprint arXiv:1901.10002_, 2:8.
* Tomasev et al. (2021) Tomasev, N., McKee, K. R., Kay, J., and Mohamed, S. (2021). Fairness for unobserved characteristics: Insights from technological impacts on queer communities. In _Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society_, pages 254-265.
* Torgersen (1991) Torgersen, E. (1991). _Comparison of statistical experiments_, volume 36. Cambridge University Press.
* Ustun et al. (2019) Ustun, B., Liu, Y., and Parkes, D. (2019). Fairness without harm: Decoupled classifiers with preference guarantees. In _International Conference on Machine Learning_, pages 6373-6382. PMLR.
* Varshney (2021) Varshney, K. R. (2021). Trustworthy machine learning. _Chappaqua, NY_.
* Verma and Rubin (2018) Verma, S. and Rubin, J. (2018). Fairness definitions explained. In _2018 ieee/acm international workshop on software fairness (fairware)_, pages 1-7. IEEE.
* Wang et al. (2021) Wang, H., Hsu, H., Diaz, M., and Calmon, F. P. (2021). To split or not to split: The impact of disparate treatment in classification. _IEEE Transactions on Information Theory_, 67(10):6733-6757.
* Wang et al. (2020) Wang, S., Guo, W., Narasimhan, H., Cotter, A., Gupta, M., and Jordan, M. (2020). Robust optimization for fairness with noisy protected groups. _Advances in Neural Information Processing Systems_, 33:5190-5203.
* Wang and Singh (2021) Wang, Y. and Singh, L. (2021). Analyzing the impact of missing values and selection bias on fairness. _International Journal of Data Science and Analytics_, 12(2):101-119.
* Wei et al. (2021) Wei, D., Ramamurthy, K. N., and Calmon, F. P. (2021). Optimized score transformation for consistent fair classification. _J. Mach. Learn. Res._, 22:258-1.
* Wick et al. (2019) Wick, M., Tristan, J.-B., et al. (2019). Unlocking fairness: a trade-off revisited. _Advances in neural information processing systems_, 32.
* Yang et al. (2020) Yang, F., Cisse, M., and Koyejo, S. (2020). Fairness with overlapping groups; a probabilistic perspective. In _Advances in Neural Information Processing Systems_, volume 33, pages 4067-4078.
* Zafar et al. (2019) Zafar, M. B., Valera, I., Gomez-Rodriguez, M., and Gummadi, K. P. (2019). Fairness constraints: A flexible approach for fair classification. _The Journal of Machine Learning Research_, 20(1):2737-2778.
* Zemel et al. (2013) Zemel, R., Wu, Y., Swersky, K., Pitassi, T., and Dwork, C. (2013). Learning fair representations. In _International conference on machine learning_, pages 325-333. PMLR.
* Zeng et al. (2022a) Zeng, X., Dobriban, E., and Cheng, G. (2022a). Bayes-optimal classifiers under group fairness. _arXiv preprint arXiv:2202.09724_.
* Zeng et al. (2022b) Zeng, X., Dobriban, E., and Cheng, G. (2022b). Fair Bayes-optimal classifiers under predictive parity. In _Advances in Neural Information Processing Systems_.

Zhang, B. H., Lemoine, B., and Mitchell, M. (2018). Mitigating unwanted biases with adversarial learning. In _Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society_, pages 335-340.
* Zhang and Long (2021) Zhang, Y. and Long, Q. (2021). Assessing fairness in the presence of missing data. _Advances in neural information processing systems_, 34:16007-16019.

Technical Background

In this section, we extend some results in Blackwell (1951, 1953) to our setting. For a random variable \(\mathrm{X}\), we denote its probability distribution by \(\mathcal{L}(\mathrm{X})\). A conditional distribution \(P_{\mathrm{X|A}}:[n]\to\mathcal{P}(\mathcal{X})\) can be equivalently written as \(\bm{P}\triangleq(P_{1},\cdots,P_{n})\) where each \(P_{i}=P_{\mathrm{X|A=i}}\in\mathcal{P}(\mathcal{X})\). Let \(\mathcal{A}\) be a closed, bounded, convex subset of \(\mathbb{R}^{n}\). A decision function is a mapping \(\bm{f}:\mathcal{X}\to\mathcal{A}\), which can also be written as \(\bm{f}(x)=(a_{1}(x),\cdots,a_{n}(x))\). A decision function is associated a loss vector:

\[\bm{v}(\bm{f})=\left(\int a_{1}(x)\mathrm{d}P_{1}(x),\cdots,\int a_{n}(x) \mathrm{d}P_{n}(x)\right).\] (9)

The collection of all \(\bm{v}(\bm{f})\) is denoted by \(\mathcal{B}(P_{\mathrm{X|A}},\mathcal{A})\) or \(\mathcal{B}(\bm{P},\mathcal{A})\).

For a vector \(\bm{\lambda}\in\Delta_{n}\) such that \(\bm{\lambda}>0\), we define a function \(\bm{p_{\lambda}}(x):\mathcal{X}\to\Delta_{n}\):

\[\bm{p_{\lambda}}(x)=\left(\frac{\lambda_{1}\mathrm{d}P_{1}}{\lambda_{1} \mathrm{d}P_{1}+\cdots+\lambda_{n}\mathrm{d}P_{n}},\cdots,\frac{\lambda_{n} \mathrm{d}P_{n}}{\lambda_{1}\mathrm{d}P_{1}+\cdots+\lambda_{n}\mathrm{d}P_{n} }\right).\] (10)

Note that \(\bm{p_{\lambda}}(\mathrm{X})\) is a sufficient statistic for \(\mathrm{X}\), considering \(\mathrm{A}\) as the parameter (it can be proved by Fisher-Neyman factorization theorem). In other words, two Markov chains hold: \(\mathrm{A}-\bm{p_{\lambda}}(\mathrm{X})-\mathrm{X}\) and \(\mathrm{A}-\mathrm{X}-\bm{p_{\lambda}}(\mathrm{X})\) for any distribution on \(\mathrm{A}\).

Consider a new set of probability distributions \(\bm{P_{\lambda}^{*}}\triangleq(\mathcal{L}(\bm{p_{\lambda}}(\mathrm{X}_{1})), \cdots,\mathcal{L}(\bm{p_{\lambda}}(\mathrm{X}_{n})))\) where \(\mathcal{L}(\mathrm{X}_{i})=P_{i}\). Here \(\bm{P_{\lambda}^{*}}\) can be viewed as a conditional distribution from \([n]\) to \(\mathcal{P}(\Delta_{n})\) since each \(\mathcal{L}(\bm{p_{\lambda}}(\mathrm{X}_{i}))\) is a probability distribution over \(\Delta_{n}\). The following lemma follows from the sufficiency of \(\bm{p_{\lambda}}(\mathrm{X})\).

**Lemma 2** (Adaptation of Theorem 3 in Blackwell (1951)).: _For any \(\mathcal{A}\), \(\mathcal{B}(\bm{P},\mathcal{A})=\mathcal{B}(\bm{P_{\lambda}^{*}},\mathcal{A})\)._

Proof.: Suppose that \(\bm{f}^{*}(\bm{p})=(a_{1}^{*}(\bm{p}),\cdots,a_{n}^{*}(\bm{p}))\) is a decision function for \((\bm{P_{\lambda}^{*}},\mathcal{A})\). Accordingly, we define \(\bm{f}(x)=(a_{1}^{*}(\bm{p_{\lambda}}(x)),\cdots,a_{n}(\bm{p_{\lambda}}(x)))\) where the function \(\bm{p_{\lambda}}\) is defined in (10). Then it is clear that \(\bm{f}\) is a decision function for \((\bm{P},\mathcal{A})\). By the law of unconscious statistician, we have

\[\int a_{i}^{*}(\bm{p})\mathrm{d}P_{\bm{\lambda},i}^{*}(\bm{p})=\mathbb{E}\left[ a_{i}^{*}(\bm{p_{\lambda}}(\mathrm{X}_{i}))\right]=\int a_{i}^{*}(\bm{p_{ \lambda}}(x))\mathrm{d}P_{i}(x).\] (11)

Hence, \(\bm{v}(\bm{f}^{*})=\bm{v}(\bm{f})\), which implies \(\mathcal{B}(\bm{P_{\lambda}^{*}},\mathcal{A})\subseteq\mathcal{B}(\bm{P}, \mathcal{A})\). For the other direction, suppose \(\bm{f}(x)=(a_{1}(x),\cdots,a_{n}(x))\) is a decision function for \((\bm{P},\mathcal{A})\). Let \(\bm{f}^{*}(\bm{p})=(a_{1}^{*}(\bm{p}),\cdots,a_{n}^{*}(\bm{p}))\) where \(a_{i}^{*}(\bm{p})\triangleq\mathbb{E}\left[a_{i}(\mathrm{X}_{i})\mid\bm{p_{ \lambda}}(\mathrm{X}_{i})=\bm{p}\right]\). Since \(\bm{p_{\lambda}}(\mathrm{X})\) is a sufficient statistics, for any \(i\in[n]\)

\[\mathcal{L}(\mathrm{X}_{i}|\bm{p_{\lambda}}(\mathrm{X}_{i})=\bm{p})=\mathcal{ L}(\mathrm{X}_{1}|\bm{p_{\lambda}}(\mathrm{X}_{1})=\bm{p}).\] (12)

Therefore, \(\bm{f}^{*}(\bm{p})=\mathbb{E}\left[\bm{f}(\mathrm{X}_{1})|\bm{p_{\lambda}}( \mathrm{X}_{1})=\bm{p}\right]\). Since \(\mathcal{A}\) is a convex set, \(\bm{f}^{*}\) is a decision function for \((\bm{P}^{*},\mathcal{A})\). By the law of total expectation, we have

\[\int a_{i}^{*}(\bm{p})\mathrm{d}P_{\bm{\lambda},i}^{*}(\bm{p})=\int a_{i}(x) \mathrm{d}P_{i}(x).\] (13)

Hence, \(\bm{v}(\bm{f})=\bm{v}(\bm{f}^{*})\), which implies \(\mathcal{B}(\bm{P},\mathcal{A})\subseteq\mathcal{B}(\bm{P_{\lambda}^{*}}, \mathcal{A})\). 

For a vector \(\bm{\lambda}\in\Delta_{n}\) such that \(\bm{\lambda}>0\), the condition distribution \(P_{\mathrm{X|A}}\) induces a weighted standard measure \(P_{\bm{\lambda}}^{*}\triangleq\mathcal{L}\left(\bm{p_{\lambda}}(\bar{\mathrm{X} })\right)\) where \(\mathcal{L}(\bar{\mathrm{X}})=\lambda_{1}P_{1}+\cdots+\lambda_{n}P_{n}\).

**Theorem 3** (Adaptation of Theorem 4 in Blackwell (1951)).: _For any two conditional distributions \(P_{\mathrm{X|A}}\) and \(Q_{\mathrm{Y|A}}\), let \(P_{\mathrm{X}}^{*}\) and \(Q_{\mathrm{X}}^{*}\) be their weighted standard measures, respectively. Then \(\mathcal{B}(P_{\mathrm{X|A}},\mathcal{A})\supseteq\mathcal{B}(Q_{\mathrm{Y|A}},\mathcal{A})\) for any closed, bounded, convex set \(\mathcal{A}\) if and only if for any continuous convex \(\phi:\Delta_{n}\to\mathbb{R}\), \(\int\phi(\bm{p})\mathrm{d}P_{\bm{\lambda}}^{*}(\bm{p})\geq\int\phi(\bm{p}) \mathrm{d}Q_{\bm{\lambda}}^{*}(\bm{p})\)_

Proof.: First, by Lemma 2, we know \(\mathcal{B}(P_{\mathrm{X|A}},\mathcal{A})=\mathcal{B}(\bm{P_{\lambda}^{*}}, \mathcal{A})\) and \(\mathcal{B}(Q_{\mathrm{Y|A}},\mathcal{A})=\mathcal{B}(\bm{Q_{\lambda}^{*}}, \mathcal{A})\). We denote \(\bm{\Lambda}=\mathsf{diag}(\lambda_{1},\cdots,\lambda_{n})\). Consider any \(\mathcal{A}=\mathsf{conv}(\bm{a}_{1},\cdots,\bm{a}_{k})\). Let

\[\bm{f}^{*}(\bm{p})=\operatorname*{argmin}_{\bm{a}\in\mathcal{A}}\bm{p}^{T} \bm{\Lambda}^{-1}\bm{a}.\] (14)Note that \(\bm{f}^{*}(\bm{p})\in\{\bm{a}_{1},\cdots,\bm{a}_{k}\}\) since this set contains all the extreme points of \(\mathcal{A}\).3 By definition, for any decision function w.r.t. \((\bm{P}^{*}_{\bm{\lambda}},\mathcal{A})\), we have

Footnote 3: If (14) has multiple optimal solutions, we always choose the one from \(\{\bm{a}_{1},\cdots,\bm{a}_{k}\}\).

\[\bm{p}^{T}\bm{\Lambda}^{-1}\bm{f}(\bm{p})\geq\bm{p}^{T}\bm{\Lambda}^{-1}\bm{f}^ {*}(\bm{p}),\quad\forall\bm{p}.\] (15)

Let \(\bm{v}=\bm{v}(\bm{f})\). By the same reason with (11), we have

\[v_{j} =\int a_{j}(\bm{p}_{\bm{\lambda}}(x))\mathrm{d}P_{j}(x)\] (16) \[=\frac{1}{\lambda_{j}}\int a_{j}(\bm{p}_{\bm{\lambda}}(x))\frac{ \lambda_{j}\mathrm{d}P_{j}}{\lambda_{1}\mathrm{d}P_{1}+\cdots+\lambda_{n} \mathrm{d}P_{n}}(x)(\lambda_{1}\mathrm{d}P_{1}+\cdots+\lambda_{n}\mathrm{d}P_ {n})(x)\] (17) \[=\frac{1}{\lambda_{j}}\int a_{j}(\bm{p}_{\bm{\lambda}}(x))[p_{\bm {\lambda}}(x)]_{j}(\lambda_{1}\mathrm{d}P_{1}+\cdots+\lambda_{n}\mathrm{d}P_ {n})(x)\] (18) \[=\frac{1}{\lambda_{j}}\operatorname{\mathbb{E}}\left[a_{j}(\bm{p }_{\bm{\lambda}}(\bar{\mathrm{X}}))[p_{\bm{\lambda}}(\bar{\mathrm{X}})]_{j}\right]\] (19) \[=\frac{1}{\lambda_{j}}\int a_{j}(\bm{p})p_{j}\mathrm{d}P^{*}_{\bm {\lambda}}(\bm{p}),\] (20)

where the last step is due to the law of unconscious statistician. Therefore,

\[\sum_{j=1}^{n}v_{j} =\int\bm{p}^{T}\bm{\Lambda}^{-1}\bm{f}(\bm{p})\mathrm{d}P^{*}_{ \bm{\lambda}}(\bm{p})\] (21) \[\geq\int\bm{p}^{T}\bm{\Lambda}^{-1}\bm{f}^{*}(\bm{p})\mathrm{d}P^ {*}_{\bm{\lambda}}(\bm{p})\] (22) \[=\int\min_{i}\{\bm{p}^{T}\bm{\Lambda}^{-1}\bm{a}_{i}\}\mathrm{d}P^ {*}_{\bm{\lambda}}(\bm{p}).\] (23)

The equality is achieved by \(\bm{v}(\bm{f}^{*})\). Hence, for any \(\mathcal{A}=\mathsf{conv}(\bm{a}_{1},\cdots,\bm{a}_{k})\)

\[\min_{\bm{v}\in\mathcal{B}(P_{\mathrm{X}|\Lambda},\mathcal{A})}\sum_{j=1}^{n}v _{j}=\int\min_{i}\{\bm{a}_{i}^{T}\bm{\Lambda}^{-1}\bm{p}\}\mathrm{d}P^{*}_{\bm {\lambda}}(\bm{p}).\] (24)

Recall that Theorem 2.(3) in Blackwell (1951) states

\[\mathcal{B}(P_{\mathrm{X}|\mathrm{A}},\mathcal{A})\supseteq \mathcal{B}(P_{\mathrm{Y}|\mathrm{A}},\mathcal{A})\quad\text{for every closed, bounded, convex $\mathcal{A}$}\] \[\Leftrightarrow \min_{\bm{v}\in\mathcal{B}(P_{\mathrm{X}|\mathrm{A}},\mathcal{A} )}\sum_{j=1}^{n}v_{j}\leq\min_{\bm{v}\in\mathcal{B}(P_{\mathrm{Y}|\mathrm{A}}, \mathcal{A})}\sum_{j=1}^{n}v_{j}\quad\text{for every closed, bounded, convex $\mathcal{A}$}.\]

By approximation theory, the second condition can be relaxed to any \(\mathcal{A}\) that is a convex hull of a finite set. By (24), this relaxed condition is equivalent to

\[\int\phi(\bm{p})\mathrm{d}P^{*}_{\bm{\lambda}}(\bm{p})\geq\int\phi(\bm{p}) \mathrm{d}Q^{*}_{\bm{\lambda}}(\bm{p})\] (25)

for all \(\phi(\bm{p})\) that are the **maximum** of finitely many linear functions. By approximation theory again, the above condition is equivalent to the one holding for any continuous convex function \(\phi\). 

## Appendix B Omitted Proofs

### Proof of Lemma 3

Proof.: Clearly, \(\mathcal{C}\) is a subset of \(\mathcal{T}(C|AC)\). Let \(\lambda\in(0,1)\) and \(P_{\hat{\mathrm{Y}}_{0}|\mathrm{S},\mathrm{Y}},P_{\hat{\mathrm{Y}}_{1}|\mathrm{ S},\mathrm{Y}}\in\mathcal{C}\). Now we introduce a Bernoulli random variable \(\mathrm{B}\) such that \(\mathrm{Pr}(\mathrm{B}=0)=\lambda\). Finally, we define \(\hat{\mathrm{Y}}_{\lambda}=\mathrm{B}\hat{\mathrm{Y}}_{1}+(1-\mathrm{B})\hat{ \mathrm{Y}}_{0}\). By definition, we have \((\mathrm{S},\mathrm{Y})-\mathrm{X}-\hat{\mathrm{Y}}_{\lambda}\) so \(P_{\hat{\mathrm{Y}}_{\lambda}|\mathrm{S},\mathrm{Y}}\in\mathcal{C}\). Moreover,

\[P_{\hat{\mathrm{Y}}_{\lambda}|\mathrm{S},\mathrm{Y}}=\lambda P_{\hat{\mathrm{Y}}_ {0}|\mathrm{S},\mathrm{Y}}+(1-\lambda)P_{\hat{\mathrm{Y}}_{1}|\mathrm{S}, \mathrm{Y}}.\]

[MISSING_PAGE_FAIL:18]

As a result, \(f(\bm{P}^{t})=\max_{\bm{P}\in\mathcal{F}}f(\bm{P})\) so \(\bm{P}^{t}\) is an optimal solution of \(\mathsf{FairFront}_{k}(\alpha_{\text{sp}},\alpha_{\text{co}},\alpha_{\text{co}})\).

If the algorithm never stops, consider any convergent sub-sequence of \(\bm{P}^{t}\) that converges to a limit point \(\bm{P}^{*}\in\mathcal{T}(C|AC)\). To simplify our notation, we assume \(\bm{P}^{t}\to\bm{P}^{*}\) as \(t\to\infty\). Since \(\{\mathcal{F}^{t}\}_{t\geq 1}\) is non-increasing and they all contain \(\mathcal{F}\), there exists a set \(\mathcal{F}^{*}\) such that

\[\lim_{t\to\infty}\ \mathcal{F}^{t}=\mathcal{F}^{*},\quad\mathcal{F}\subseteq \mathcal{F}^{*}.\]

Therefore, we have

\[f(\bm{P}^{*})=\lim_{t\to\infty}f(\bm{P}^{t})=\lim_{t\to\infty}\max_{\bm{P}\in \mathcal{F}^{t}}\ f(\bm{P})=\max_{\bm{P}\in\mathcal{F}^{*}}\ f(\bm{P}).\]

Since \(\mathcal{F}\subseteq\mathcal{F}^{*}\), we have

\[f(\bm{P}^{*})=\max_{\bm{P}\in\mathcal{F}^{*}}\ f(\bm{P})\geq\max_{\bm{P}\in \mathcal{F}}\ f(\bm{P}).\]

If \(\bm{P}^{*}\not\in\mathcal{F}\), then there exists a \((\bar{\bm{a}}_{1},\cdots,\bar{\bm{a}}_{k})\), such that \(g(\bm{P}^{*};\bar{\bm{a}}_{1},\cdots,\bar{\bm{a}}_{k})>0\). Let \((\bm{a}_{1,t},\cdots,\bm{a}_{k,t})\) be the output of Step 2 at \(t\)-th iteration. Since \(\bm{P}^{*}\in\mathcal{F}^{t}\) for all \(t\), we have

\[g(\bm{P}^{*};\bm{a}_{1,t},\cdots,\bm{a}_{k,t})\leq 0.\] (31)

By the optimality of \((\bm{a}_{1,t},\cdots,\bm{a}_{k,t})\), we have

\[g(\bm{P}^{t};\bm{a}_{1,t},\cdots,\bm{a}_{k,t})\geq g(\bm{P}^{t};\bar{\bm{a}}_{ 1},\cdots,\bar{\bm{a}}_{k}).\] (32)

Suppose that some sub-sequence of \((\bm{a}_{1,t},\cdots,\bm{a}_{k,t})\) converges to a vector \((\bm{a}_{1}^{*},\cdots,\bm{a}_{k}^{*})\). For the sake of simplicity, we assume \((\bm{a}_{1,t},\cdots,\bm{a}_{k,t})\to(\bm{a}_{1}^{*},\cdots,\bm{a}_{k}^{*})\) as \(t\to\infty\). On the one hand, taking limit of \(t\to\infty\) on both sides of (32) leads to

\[g(\bm{P}^{*};\bm{a}_{1}^{*},\cdots,\bm{a}_{k}^{*})\geq g(\bm{P}^{*};\bar{\bm{a }}_{1},\cdots,\bar{\bm{a}}_{k}).\]

On the other hand, taking limit of \(t\to\infty\) on both sides of (31) leads to

\[g(\bm{P}^{*};\bm{a}_{1}^{*},\cdots,\bm{a}_{k}^{*})\leq 0.\]

Therefore,

\[0\geq g(\bm{P}^{*};\bm{a}_{1}^{*},\cdots,\bm{a}_{k}^{*})\geq g(\bm{P}^{*}; \bar{\bm{a}}_{1},\cdots,\bar{\bm{a}}_{k})>0,\]

which is impossible. Therefore, \(\bm{P}^{*}\in\mathcal{F}\) and, as a result, we have

\[f(\bm{P}^{*})=\max_{\bm{P}\in\mathcal{F}^{*}}\ f(\bm{P})\geq\max_{\bm{P}\in \mathcal{F}}\ f(\bm{P})\geq f(\bm{P}^{*})\implies\max_{\bm{P}\in\mathcal{F}} \ f(\bm{P})=f(\bm{P}^{*}).\]

### Additional Results

We establish basic properties of \(\mathcal{C}\) and \(\mathsf{FairFront}(\alpha_{\text{sp}},\alpha_{\text{co}},\alpha_{\text{co}})\) in the following lemma.

**Lemma 3**.: \(\mathcal{C}\) _is a convex subset of \(\mathcal{T}(C|AC)\) and \(\mathsf{FairFront}(\alpha_{\text{sp}},\alpha_{\text{co}},\alpha_{\text{co}})\) is a concave function w.r.t. \(\alpha_{\text{sp}},\alpha_{\text{co}},\alpha_{\text{co}}\). Here the constants \(A\) and \(C\) denote the number of protected groups and the number of classes._

Next, we discuss a special case--X is discrete--under which \(\mathcal{C}\) has a simple characterization.

**Remark 3**.: If X is a _discrete_ variable with a _finite support_\([D]\), we can write \(P_{\text{X}|\text{S},\text{Y}}\) as a transition matrix \(\bm{\Phi}\in\mathcal{T}(D|AC)\). By introducing an auxiliary variable \(\bm{M}\in\mathcal{T}(C|D)\), we can write \(\bm{P}\in\mathcal{C}\) equivalently as linear constraints: \(\bm{P}=\bm{\Phi}\bm{M}\) by using the last condition of Lemma 1. Consequently, Proposition 1 boils down to a linear program. However, this characterization fails to generalize to continuous data because \(\bm{\Phi}\) and \(\bm{M}\) will have an infinite dimension; for categorical data, this characterization suffers from the curse of dimensionality since the support size of X grows exponentially fast w.r.t. the number of features.

## Appendix C Details on the Experimental Results

### Additional Experiments

In this section, we present additional experimental results to further support our findings. We reproduce our experimental results on the German Credit dataset (Bache and Lichman, 2013) and HSLS (High School Longitudinal Study) dataset (Ingels et al., 2011; Jeong et al., 2022) in Figure 4 and Figure 5. In particular, the HSLS dataset experiment is a multi-group, multi-label experiment. Our observation is consistent with those on the previous two datasets--the fairness-accuracy curves given by SOTA fairness interventions, such as Reduction and FairProjection, are close to the information-theoretic limit.

[MISSING_PAGE_EMPTY:20]

loan duration in month, credit amount, age, number of existing credits at this bank, sex, credit history, savings, and length of present employment as input features. We include the group attribute age as an input feature. We group credit amount into three disjoint intervals: [0, 5000), [5000, 10000), [100000,\(\infty\)). We group duration of loan into two categories: under 36 months and over 36 months.

Hsls.We use race as the group attribute and mathematics test score (number of questions answered correctly out of 72) as the target for prediction. This is a multi-group and multi-label dataset. The entire population is grouped by 4 categories: White, Asian, African American, and Others. We seek to predict the mathematics test performance from a set of attributes, including the scale of student's mathematical identity, scale of student's mathematics utility, scale of one's mathematics self-efficacy, parent's education, parent's income, scale of student's sense of school belonging, race, and sex. Note that we include the group attribute as an input feature. We group the target column (estimated number of questions answered correctly) into a total of 5 disjoint intervals: \([0,30)\), \([30,40),[40,50),[50,60),[60,\infty)\); we group the scale of student's mathematical identity, mathematics self-efficacy, and sense of school belonging into a total of 4 disjoint intervals, characterized by standard deviations away from the mean: \((-\infty,-1),[-1,0),[0,1),[1,\infty)\).

### Benchmark

Each benchmark method's hyper-parameter values are provided below. Each point in Figure 2 for Baseline, EqQdds, CalEqQdds, Reduction, LevEqOpp, and FairProjection is obtained by applying the obtained classifier to 10 different test sets. For the Adult dataset, we use Random Forest with n_estimators=15, min_samples_leaf=3, criterion = log_loss, bootstrap = False as our baseline classifier; for the COMPAS dataset, we use Random Forest with n_estimators = 17 as our baseline classifier. For the German Credit dataset, we use Random Forest with n_estimators=100,min_samples_split =2,min_samples_leaf=1 as our baseline classifier. They are all implemented by Scikit-learn (Pedregosa et al., 2011).

EqQdds (Hardt et al., 2016).We use AIF360 implementation of EqQddsPostprocessing and the default hyper-parameter setup.

CalEqOdds (Pleiss et al., 2017).We use AIF360 implementation of CalibratedEqQddsPostprocessing and the default hyper-parameter setup.

Reduction (Agarwal et al., 2018).We use AIF360 implementation of ExponentiatedGradientReduction. We vary the allowed fairness constraint violation \(\epsilon\in\{0.001,0.01,0.2,0.5,1,2,5,10,15\}\) for Adult dataset and \(\epsilon\in\{0.001,0.01,0.2,0.5,1,2,5,10,15\}\) for Adult with missing values. We vary \(\epsilon\in\{0.001,2,5,10,15,20,25,30,35,40\}\) for COMPAS to obtain a fairness-accuracy curve, and \(\epsilon\in\{0.001,0.1,0.5,1,2,7,8,10,15,20,25,30\}\) for COMPAS with 50% missing values in the minority group. We use \(\epsilon\in\{20,50,80,95\}\) for German Credit dataset and \(\epsilon\in\{5,8,10,20,23\}\) when using Bayes Optimal classifier.

LevEqOpp (Chzhen et al., 2019).We use the Python implementation of LevEqopp from the Github repo in Alghamdi et al. (2022). We follow the same hyperparameters setup as in the original method.

FairProjection Alghamdi et al. (2022).We use the implementation from the Github repo in Alghamdi et al. (2022) and set use_protected = True. We use Random Forest with n_estimators = 17 as the baseline classifier to predict S from \((\text{X},\text{Y})\). We set the list of fairness violation tolerance to be \(\{0.07,0.075,0.08,0.085,0.09,0.095,0.1,0.5,0.75,1.0\}\) for Adult dataset and \(\{0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.1,0.5,1.0\}\) for COMPAS dataset to obtain a fairness-accuracy curve. We set the list of fairness violation tolerance to be \(\{0.005,0.01,0.02,0.07,0.1,0.15\}\) on the German Credit dataset experiment, and \(\{0.0001,0.001,0.005,0.01,0.015,0.02,0.05\}\) when using a Bayes optimal baseline classifier. For the HSLS dataset, the list of tolerance is \(\{0.06,0.065,0.07,0.08,0.1,0.09,0.15,0.2,0.3\}\).

## Appendix D More on Related Work

We provide a detailed comparison with existing work on fairness Pareto frontier and ML uncertainty in this section.

### Fairness Pareto Frontier

We present in Table 2 a detailed comparison of our approach with previous studies that have investigated the fairness Pareto frontier and fair Bayes optimal classifier. In short, our approach is different from this line of research as it simultaneously combines several important aspects: it is applicable to _multiclass_ classification problems with _multiple_ protected groups; it _avoids disparate treatment_ by not requiring the classifier to use group attributes as an input variable; and it can handle _multiple_ fairness constraints simultaneously and produce fairness-accuracy trade-off curves (instead of a single point).

### Aleatoric and Epistemic Uncertainty

In this paper, we divide algorithmic discrimination into aleatoric and epistemic discrimination. We borrow this notion from ML uncertainty literature (see Hullermeier and Waegeman, 2021, for a survey). Here we provide a detailed comparison between them.

In terms of their definitions, epistemic uncertainty arises from a lack of knowledge about the best model, such as the Bayes predictor, while epistemic discrimination results from a lack of knowledge about the optimal "fair" predictive model. On the other hand, aleatoric uncertainty is the irreducible part of uncertainty caused by the random relationship between input features and label, while aleatoric discrimination is due to inherent biases in the data-generating distribution.

In terms of their characterization, epistemic uncertainty can in principle be reduced by including additional information (e.g., more data); epistemic discrimination can be reduced in a similar approach since a data scientist can choose a more effective fairness-intervention algorithm with access to more information.

Finally, in the infinite sample regime, a consistent learner will be able to remove all epistemic uncertainty, assuming the model class is large enough and there are no computational constraints. Analogously, we demonstrate in Figure 1 that when the underlying distribution is known, SOTA fairness interventions are able to eliminate epistemic discrimination as their fairness-accuracy curves are close to the fair front.

## Appendix E More on Future Work

In this paper, we present an upper bound estimate for \(\mathsf{FairFront}\) in Algorithm 1. It is important to note that this estimate may be subjected to errors originating from various sources. These include (i)

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & Multiclass & Multigroup & 
\begin{tabular}{c} Avoid \\ disparate treatment \\ \end{tabular} & Multi-constraint & Curve \\ \hline Hard et al. (2016) & \(\bigtimes\) & âœ“ & \(\bigtimes\) & \(\bigtimes\) & \(\bigtimes\) \\ \hline Corbett-Davies et al. (2017) & \(\bigtimes\) & âœ“ & \(\bigtimes\) & \(\bigtimes\) & \(\bigtimes\) \\ \hline Menon and Williamson (2018) & \(\bigtimes\) & \(\bigtimes\) & âœ“ & \(\bigtimes\) & âœ“ \\ \hline Chrlen et al. (2019) & \(\bigtimes\) & \(\bigtimes\) & âœ“ & \(\bigtimes\) & \(\bigtimes\) \\ \hline Yang et al. (2020) & âœ“ & âœ“ & \(\bigtimes\) & âœ“ & âœ“ \\ \hline Zeng et al. (2022) & \(\bigtimes\) & âœ“ & \(\bigtimes\) & \(\bigtimes\) & âœ“ \\ \hline Zeng et al. (2022) & \(\bigtimes\) & âœ“ & \(\bigtimes\) & \(\bigtimes\) & \(\bigtimes\) \\ \hline Our approach & âœ“ & âœ“ & âœ“ & âœ“ & âœ“ \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison with existing work that investigate the fairness Pareto frontier. **Multi-class/multigroup**: can handle multiclass classification problems with multiple protected groups; **Avoid disparate treatment**: not require the classifier to use group attributes as an input variable; **Multi-constraint**: can handle multiple (group) fairness constraints simultaneously; **Curve**: produce fairness-accuracy trade-off curves (instead of a single point).

the approximation error of the function \(g\), (ii) estimation errors from computing the expectation in (7) with a finite dataset, and (iii) the influence of hyperparameters, \(T\) (number of running iterations of Algorithm 1) and \(k\) (number of segments in the piece-wise linear functions). Regarding the dependence on \(T\), our Theorem 2 ensures the algorithm's asymptotic convergence as \(T\to\infty\). However, we have not established a proof for its behavior at a finite \(T\). Regarding the dependence on \(k\), we conjecture that \(k=A*C\) should suffice, where \(A\) is the number of protected groups and \(C\) is the number of labels. While Blackwell proved this result for \(k=2\) in Theorem 10 of Blackwell (1953), an extension of this proof to a general value of \(k\) appears to remain an open problem.

We define aleatoric and epistemic discrimination with respect to the entire population. Investigating their per-instance counterparts and the relationship to individual fairness would be a compelling area of future research. Additionally, a more nuanced analysis of aleatoric and epistemic discrimination is desirable, further breaking them down into fine-grained components. For instance, epistemic discrimination may be attributed to various factors including limited training data, noisy observations of labels or sensitive attributes, and limitations of learning algorithms. Finally, investigating other criteria, such as scalability, generalization, and robustness in evaluating existing fairness interventions is a significant topic for future exploration.