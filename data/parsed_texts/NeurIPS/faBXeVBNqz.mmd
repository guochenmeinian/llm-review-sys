# Higher-Rank Irreducible Cartesian Tensors for

Equivariant Message Passing

 Viktor Zaverkin\({}^{1,}\)1  Francesco Alesiani\({}^{1,}\)2  Takashi Maruyama\({}^{1,}\)3  Federico Errica\({}^{2}\)

Henrik Christiansen\({}^{1}\)  Makoto Takamoto\({}^{1}\)  Nicolas Weber\({}^{1}\)  Matthias Niepert\({}^{1,3}\)

\({}^{1}\)NEC Laboratories Europe \({}^{2}\)NEC Italy \({}^{3}\)University of Stuttgart

Footnote 1: Corresponding author: viktor.zaverkin@neclab.eu

Footnote 2: These authors contributed equally.

###### Abstract

The ability to perform fast and accurate atomistic simulations is crucial for advancing the chemical sciences. By learning from high-quality data, machine-learned interatomic potentials achieve accuracy on par with ab initio and first-principles methods at a fraction of their computational cost. The success of machine-learned interatomic potentials arises from integrating inductive biases such as equivariance to group actions on an atomic system, e.g., equivariance to rotations and reflections. In particular, the field has notably advanced with the emergence of equivariant message passing. Most of these models represent an atomic system using spherical tensors, tensor products of which require complicated numerical coefficients and can be computationally demanding. Cartesian tensors offer a promising alternative, though state-of-the-art methods lack flexibility in message-passing mechanisms, restricting their architectures and expressive power. This work explores higher-rank irreducible Cartesian tensors to address these limitations. We integrate irreducible Cartesian tensor products into message-passing neural networks and prove the equivariance and traceless property of the resulting layers. Through empirical evaluations on various benchmark data sets, we consistently observe on-par or better performance than that of state-of-the-art spherical and Cartesian models.

## 1 Introduction

The ability to perform sufficiently fast and accurate atomistic simulations for large molecular and material systems holds the potential to revolutionize molecular and materials science [1, 2, 3, 4, 5, 6, 7, 8]. Conventionally, computational chemistry and materials science rely on ab initio or first-principles approaches--e.g., coupled cluster [9, 10, 11] or density functional theory (DFT) [12, 13], respectively--that are accurate but computationally demanding, thus limiting the accessible simulation time and system sizes. However, the ability to generate high-quality, first-principles-based data sets has prompted the development of machine-learned interatomic potentials (MLIPs). These potentials enable atomistic simulations with accuracy that is on par with the reference first-principles method but at a fraction of the computational cost. Message-passing neural networks (MPNNs) [14, 15, 16, 17, 18, 19, 20, 21] have been employed in chemical and materials sciences, including the development of MLIPs, due to their efficient processing of the graph representation of the atomic system [22, 23, 24, 25, 26]. Achieving the desired MLIPs' performance, however, requires the inclusion of inductive biases, like the invariance of total energy to translations, reflections, and rotations in the three-dimensional space, and an effective encoding of the atomic system into a learnable representation [27]. Designing equivariant MPNNs [28, 29, 25, 26, 27, 30, 31, 32, 33, 34, 35, 36], which preserve the directional information of the local atomic environment, has been one of the most active research directions of the last years. Previous work often achieves equivariance by representing an atomicsystem as a graph, with node features expressed in spherical harmonics basis or using lower-rank Cartesian tensors and designing ad-hoc message-passing layers [25; 28; 29; 30; 31; 32; 33; 34; 35; 36; 37].

MLIPs based on spherical harmonics [25; 33; 34], which are the basis for irreducible representations of the three-dimensional rotation group, often demonstrate a better performance compared to those that use lower-rank Cartesian representations (scalars and vectors) [28; 37]. Spherical tensors, however, require the definition of a particular rotational axis, often chosen as the \(z\)-axis, resulting in inherent bias [38]. Furthermore, coupling spherical tensors via tensor products, involved in designing equivariant convolutions and constructing many-body features [33; 39; 40], requires the definition of complicated numerical coefficients, e.g., Wigner \(3\)-\(j\) symbols defined in terms of the Clebsch-Gordan coefficients [41], and can be computationally demanding. In contrast, irreducible Cartesian tensors have no preferential directions; their tensor products are simpler and have a better computational complexity--up to a certain tensor rank--than the tensor products of spherical tensors [42; 43; 44; 45; 38]. Recent work improved results of Cartesian MPNNs by using rank-two tensors and decomposing them into irreducible representations of the three-dimensional rotation group [30], i.e., into representations of dimension 1 (trace), 3 (anti-symmetric part), and 5 (traceless symmetric part) [46]. Higher-rank reducible Cartesian tensors have also been integrated into many-body MPNNs [31]. These state-of-the-art Cartesian approaches, however, lack the flexibility of their message-passing mechanisms. They rely exclusively on convolutions with invariant filters and restrict the construction of many-body features, limiting the range of possible architectures and their expressive power.

**Contributions.** In this work, we address the limitations of state-of-the-art Cartesian models and demonstrate that operating with irreducible Cartesian tensors leads to on-par or, sometimes, even better performance than that of spherical counterparts. Particularly, this work goes beyond scalars, vectors, and rank-two tensors and explores the integration of higher-rank irreducible Cartesian tensors and their products into equivariant MPNNs: i) We demonstrate how irreducible Cartesian tensors that are symmetric and traceless can be constructed from a unit vector and a product of two irreducible Cartesian tensors, essential for equivariant convolutions and constructing many-body features; ii) We prove that the resulting tensors are traceless and equivariant under the action of the three-dimensional rotation and reflection group; iii) We demonstrate that higher-rank irreducible Cartesian tensors can be used to design cost-efficient--up to a certain tensor rank--and accurate equivariant models of many-body interactions; iv) We conduct different experiments to assess the effectiveness of equivariant message passing based on irreducible Cartesian tensors, and achieve state-of-the-art performance on benchmark data sets such as rMD17 [47], MD22 [48], 3BPA [49], acetylacetone [32], and Ta-V-Cr-W [50]. We hope our contributions will offer new insights into the use of Cartesian tensors for constructing accurate and cost-efficient MLIPs and beyond.

## 2 Background

**Group representations and equivariance.** A group \((G,\cdot)\) is defined by a set of elements \(G\) and a group product \(\cdot\). A representation \(D\) of a group is a function from \(G\) to square matrices such that \(D[g]D[g^{\prime}]=D[g\cdot g^{\prime}],\,\forall\,g,g^{\prime}\in G\). This representation defines an action of \(G\) to any vector space \(\mathcal{X}\) (of the same dimension as the dimension of square matrices) through the matrix-vector multiplication, i.e., \((g,\mathbf{x})\mapsto D_{\mathcal{X}}[g]\mathbf{x}\) for \(\forall\,g\in G\) and \(\forall\,\mathbf{x}\in\mathcal{X}\). For vector spaces \(\mathcal{X}\) and \(\mathcal{Y}\), a function \(f:\mathcal{X}\rightarrow\mathcal{Y}\) is called equivariant to the action of a group \(G\) to \(\mathcal{X}\) and \(\mathcal{Y}\) iff

\[f(D_{\mathcal{X}}[g]\mathbf{x})=D_{\mathcal{Y}}[g]f(\mathbf{x})\,,\forall\,g \in G.\]

Invariance can be seen as a special type of equivariance with \(D_{\mathcal{Y}}[g]\) being the identity for all \(g\). An important class of equivariant models focuses on equivariance to the action of the Euclidean group \(\mathrm{E}(3)\), comprising translations and the orthogonal group \(\mathrm{O}(3)\), i.e., rotation group \(\mathrm{SO}(3)\) and reflections, in \(\mathbb{R}^{3}\). MLIPs based on equivariant models usually focus on equivariance to the action of the orthogonal group \(\mathrm{O}(3)\) and are invariant to translations.

**Cartesian tensors.** A vector \(\mathbf{x}\in\mathbb{R}^{3}\) transforms under the action of the rotation group \(\mathrm{SO}(3)\) as \(\mathbf{x}^{\prime}=R\mathbf{x}\), i.e., each component of it transforms as \(\left(\mathbf{x}\right)^{\prime}_{i}=\sum_{j}R_{ij}\left(\mathbf{x}\right)_{j}\). Here, \(R=D_{\mathcal{X}}\left[g\right]\in\mathbb{R}^{3\times 3}\) denotes the rotation matrix representation of \(g\in\mathrm{SO}(3)\). Cartesian tensors of rank \(n\) are described by \(3^{n}\) numbers and generalize the concept of vectors. A rank-\(n\) Cartesian tensor \(\mathbf{T}\) can be viewed as a multidimensional array with \(n\) indices, i.e., \(\left(\mathbf{T}\right)_{i_{1}i_{2}\cdots i_{n}}\) with \(i_{k}\in\{1,2,3\}\) for \(\forall\,k\in\{1,\cdots,n\}\). Furthermore, each index of \(\left(\mathbf{T}\right)_{i_{1}i_{2}\cdots i_{n}}\) transforms independently as a vector under rotation. For example, a rank-two tensor transforms under rotation as \(\mathbf{T}^{\prime}=R\mathbf{T}R^{\top}\), i.e., each component of it transforms as \(\left(\mathbf{T}\right)_{i_{1}i_{2}}^{l}=\sum_{j_{1},j_{2}}R_{i_{1}j_{1}}R_{i_{2}j _{2}}\left(\mathbf{T}\right)_{j_{2}j_{2}}\). For Cartesian tensors, one defines an \(r\)-fold tensor contraction and an outer product, i.e., \(\left(\mathbf{T}\right)_{j_{1}\cdots j_{s}}=\sum_{i_{1},\cdots,i_{r}}\left( \mathbf{U}\right)_{i_{1}\cdots i_{r}j_{1}\cdots j_{s}}\left(\mathbf{S}\right)_{ i_{1}\cdots i_{r}}\) and \(\left(\mathbf{T}\right)_{i_{1}\cdots i_{r}j_{1}\cdots j_{s}}=\left(\mathbf{U} \right)_{i_{1}\cdots i_{r}}\left(\mathbf{S}\right)_{j_{1}\cdots j_{s}}\), respectively. Cartesian tensors are generally reducible and can, thus, be decomposed into smaller representations that transform independently within their linear subspaces under rotation. For example, a rank-two reducible Cartesian tensor contains representations of dimension 1 (trace), 3 (anti-symmetric part), and 5 (traceless symmetric part): \(\sum_{i_{1}}(\mathbf{T})_{i_{1}i_{1}}\), \(\left(\mathbf{T}\right)_{i_{1}i_{2}}-\left(\mathbf{T}\right)_{i_{2}i_{1}}\), and \(\left(\mathbf{T}\right)_{i_{1}i_{2}}+\left(\mathbf{T}\right)_{i_{2}i_{1}}-2/ 3\sum_{i_{1}}(\mathbf{T})_{i_{1}i_{1}}\), respectively. Under rotation, the traceless symmetric part remains within its irreducible subspace, with a similar behavior for other irreducible components of \(\left(\mathbf{T}\right)_{i_{1}i_{2}}\). Furthermore, for a reducible Cartesian tensor of rank \(n\), an action of the rotation group \(\mathrm{SO}(3)\) can be represented with a \(3^{n}\times 3^{n}\)-dimensional rotation matrix. This rotation matrix is also reducible, meaning that an appropriate change of basis can block diagonalize it into smaller subsets, the irreducible representations of the rotation group \(\mathrm{SO}(3)\).

**Spherical tensors.** Spherical harmonics (or spherical tensors) \(Y_{m}^{l}\) are functions from the points on a sphere to complex or real numbers, with degree \(l\geq 0\) and components \(-l\leq m\leq l\). Collecting all components for a given \(l\) we obtain a \((2l+1)\)-dimensional object \(\mathbf{Y}^{l}=(Y_{-l}^{l},\ldots,Y_{l-1}^{l},Y_{l}^{l})\). Spherical tensors transform under rotation as \(Y_{m}^{l}(R\hat{\mathbf{x}})=\sum_{m^{\prime}}(\mathbf{D})_{mm^{\prime}}^{l}Y_ {m^{\prime}}^{l}(\hat{\mathbf{x}})\), where \((\mathbf{D})_{mm^{\prime}}^{l}\) is the irreducible representation of \(\mathrm{SO}(3)\)--the Wigner \(D\)-matrix--and \(R\) denotes the rotation matrix. Spherical tensors are irreducible and can be combined using the Clebsch-Gordan tensor product

\[\left(Y_{m_{1}}^{l_{1}}\otimes_{\mathrm{CG}}Y_{m_{2}}^{l_{2}}\right)_{m_{3}}^{ l_{3}}=\sum_{m_{1}=-l_{1}}^{l_{1}}\sum_{m_{2}=-l_{2}}^{l_{2}}C_{l_{1}m_{1},l_{2}m_{ 2}}^{l_{3}m_{3}}Y_{m_{1}}^{l_{1}}Y_{m_{2}}^{l_{2}},\]

with Clebsch-Gordan coefficients \(C_{l_{1}m_{1},l_{2}m_{2}}^{l_{3}m_{3}}\) and \(l_{3}\in\left\{\left|l_{1}-l_{2}\right|,\cdots,l_{1}+l_{2}\right\}\). Furthermore, any reducible Cartesian tensor of rank \(n\) can be written in spherical harmonics as \(\left(\hat{\mathbf{x}}\right)_{i_{1}}\left(\hat{\mathbf{x}}\right)_{i_{2}} \cdots\left(\hat{\mathbf{x}}\right)_{i_{n}}=\sum_{l=0}^{n}\sum_{m=-l}^{l}X_{i_ {1}i_{2}\cdots i_{n}}^{lm}Y_{m}^{l}\), with coefficients \(X_{i_{1}i_{2}\cdots i_{n}}^{lm}\) defined elsewhere [51].

## 3 Related work

**Equivariant message-passing potentials.** Equivariant MPNNs [28; 29; 30; 31; 32; 33; 34; 35; 52; 53; 54] often outperform more traditional invariant models [55; 56; 57; 58; 59; 60]. While many invariant and equivariant MPNNs rely on two-body features within a single message-passing layer, there is a growing interest in constructing higher-body-order features, such as angles and dihedrals, to model many-body interactions in atomic systems [57; 59; 60]. Note that equivariant models with two-body features in a single message-passing layer build many-body ones through repeated message-passing iterations, increasing the receptive field and, thus, computational cost. Recent work recognized the importance of higher-body-order features but faced challenges due to explicit summation over triplets or quadruplets [57; 59]. In contrast, MACE advances the current state-of-the-art by combining ACE and equivariant message passing, introducing cost-efficient many-body message-passing potentials [33]. With just two message-passing layers, MACE yields accurate potentials for interacting many-body systems [61].

**Beyond Clebsch-Gordan tensor product.** Despite the success of equivariant MPNNs based on spherical tensors, the high computational cost of the Clebsch-Gordan tensor product limits their computational efficiency [28; 30; 33; 34; 53; 54; 62; 63; 64]. Thus, current research focuses on alternatives to the Clebsch-Gordan tensor product, which has a \(\mathcal{O}(L^{5})\) complexity for tensors up to degree \(L\). Recently, the relation of Clebsch-Gordan coefficients to the integral of products of three spherical harmonics, known as the Gaunt coefficients [41], has been exploited to reduce the computational cost of the tensor product of spherical tensors to \(\mathcal{O}(L^{3})\), compared to \(\mathcal{O}(L^{6})\) of the full Clebsch-Gordan tensor product including all \((l_{1},l_{2})\to l_{3}\)[36]. However, this approach excludes odd tensor products and, thus, restricts the expressive power and the range of possible architectures.

Cartesian tensors and their products offer another promising alternative to the Clebsch-Gordan tensor product, enabling the efficient construction of message-passing layers with two- and many-body features. Recent work has explored decomposing rank-two tensors into their irreducible representations and using higher-rank reducible tensors [30; 31]. These approaches, however, are limited to convolutions with invariant filters, restricting the range of possible architectures and, thus, the expressive power of resulting Cartesian models. Furthermore, they provide limited mechanisms for constructing higher-body-order features, restricted to three-body features obtained through matrix-matrix multiplication and invariant many-body features obtained through full tensor contractions, similar to moment tensor potentials and Gaussian moments [65; 66; 67]. Finally, using reducible Cartesian tensors during message-passing leads to mixing different irreducible representations, which can hinder the performance of resulting models compared to state-of-the-art spherical models [31].

## 4 Methods

We define an atomic configuration \(\mathcal{S}=\{\mathbf{r}_{u},Z_{u}\}_{u=1}^{N_{\mathrm{at}}}\), where \(\mathbf{r}_{u}\in\mathbb{R}^{3}\) denotes Cartesian coordinates and \(Z_{u}\in\mathbb{N}\) represents the atomic number of atom \(u\), with a total of \(N_{\mathrm{at}}\) atoms. Our focus lies on message-passing MLIPs, parameterized by \(\bm{\theta}\), that learn a mapping from a configuration \(\mathcal{S}\) to the total energy \(E\), i.e., \(f_{\bm{\theta}}:\mathcal{S}\mapsto E\in\mathbb{R}\). Thus, we represent molecular and material systems as graphs in a three-dimensional Euclidean space. An edge \(\{u,v\}\) exists if atoms \(u\) and \(v\) are within a cutoff distance \(r_{c}\), i.e., \(\|\mathbf{r}_{u}-\mathbf{r}_{v}\|_{2}\leq r_{c}\). For more details on MPNNs, see Appendix A. The total energy of an atomic configuration is defined by the sum of individual atomic energy contributions [68], i.e., \(E=\sum_{u=1}^{N_{\mathrm{at}}}E_{u}\). Atomic forces are computed as negative gradients of the total energy with respect to atomic coordinates, i.e., \(\mathbf{F}_{u}=-\nabla_{\mathbf{r}_{u}}E\).

### Irreducible Cartesian tensor product

In the following, we explore irreducible Cartesian tensors and their products based on the three-dimensional vector space and the three-dimensional orthogonal group \(\mathrm{O}(3)\), which comprises rotations and reflections. The respective tensors and tensor products are schematically illustrated in Fig. 1 (a) and (b), respectively, and are further employed in constructing MPNNs for atomic systems equivariant under actions of the orthogonal group. An irreducible Cartesian tensor \(\mathbf{T}_{n}\in(\mathbb{R}^{3})^{\otimes n}\) of rank \(n\) and weight \(l\leq n\) (related to the degree \(l\) of spherical tensors) can be represented by a tensor with \(3^{n}\) components in a three-dimensional vector space. These \(3^{n}\) components form a basis for a \((2l+1)\)-dimensional irreducible representation of the three-dimensional rotation group \(\mathrm{SO}(3)\)[69; 70]. Thus, only \(2l+1\) of the \(3^{n}\) components are independent.

The rotation in the space of all tensors of rank \(n\) is induced through the \(n\)-fold outer product of a rotation matrix \(R\in\mathbb{R}^{3\times 3}\), i.e., \(R^{\otimes n}=R\otimes\cdots\otimes R\). The obtained representation of the

Figure 1: **Schematic illustration of (a) the construction of an irreducible Cartesian tensor for a local atomic environment and (b) the tensor product of two irreducible Cartesian tensors of rank \(l_{1}\) and \(l_{2}\).** The construction of an irreducible Cartesian tensor from a unit vector \(\hat{\mathbf{r}}\) is defined in Eq. (1). In this work, we use tensors with the same rank \(n\) and weight \(l\), i.e., \(n=l\), avoiding the need for embedding tensors with \(l<n\) in a higher-dimensional tensor space. Therefore, we use \(l\) to identify the rank and the weight of an irreducible Cartesian tensor. The tensor product is defined in Eqs. (2) and (3), resulting in a new tensor \(\mathbf{T}_{l_{3}}=(\mathbf{T}_{l_{1}}\otimes_{\mathrm{Cart}}\mathbf{T}_{l_{2 }})_{l_{3}}\) of rank \(l_{3}=\{|l_{1}-l_{2}|,\cdots,l_{1}+l_{2}\}\). Transparent boxes denote the linearly dependent elements of symmetric and traceless tensors. The tensor product can be even or odd, defined by \(l_{1}+l_{2}-l_{3}\).

rotation group \(\mathrm{SO}(3)\) is reducible for all \(n\) except \(n=\{0,1\}\). Reducing a tensor of rank \(n\) yields a unique irreducible tensor with the same weight and rank (\(n=l\)), which is characterized by being symmetric, i.e., \((\mathbf{T}_{n})_{\cdots i\cdots j\cdots}=(\mathbf{T}_{n})_{\cdots j\cdots i \cdots}\), \(\forall\;i\neq j\in\{i_{1}\cdots,i_{n}\}\), and traceless, i.e., \(\sum_{i}(\mathbf{T}_{n})_{\cdots i\cdots i\cdots}=0\;,\forall\,i\in\{i_{1}, \cdots,i_{n}\}\). An irreducible tensor of rank \(n\) and weight \(l\) with \(l<n\) can be viewed as a \(l\)-rank tensor embedded in the \(n\)-rank tensor space, e.g., by computing an outer product with the identity matrix. However, the embedding is often not unique. Thus, we construct tensors with the same weight and rank (\(n=l\)) in the following.

**Irreducible Cartesian tensors from unit vectors.** Hereafter, we denote the rank and the weight of irreducible Cartesian tensors by \(l\) to distinguish them from reducible counterparts. An irreducible Cartesian tensor of an arbitrary rank \(l\) can be constructed from a unit vector \(\hat{\mathbf{r}}\) in the form [45]

\[\mathbf{T}_{l}(\hat{\mathbf{r}})=C\sum\nolimits_{m=0}^{\lfloor l/2\rfloor}(-1 )^{m}\frac{(2l-2m-1)!!}{(2l-1)!!}\big{\{}\hat{\mathbf{r}}^{\otimes(l-2m)} \otimes\hat{\mathbf{I}}^{\otimes m}\big{\}},\] (1)

resulting in a symmetric and traceless tensor of rank \(l\). Here, \(\mathbf{I}\) denotes the \(3\times 3\) identity matrix, \(\hat{\mathbf{r}}^{\otimes(l-2m)}=\hat{\mathbf{r}}\otimes\cdots\otimes\hat{ \mathbf{r}}\) and \(\mathbf{I}^{\otimes m}=\mathbf{I}\otimes\cdots\otimes\mathbf{I}\) are the corresponding \((l-2m)\)- and \(m\)-fold outer products. The curly brackets indicate the summation over all permutations of the \(l\) unsymmetrized indices [45], i.e., \(\{\mathbf{T}_{l}\}_{i_{1}\cdots i_{l}}=\sum_{\pi\in S_{l}}(\mathbf{T}_{l})_{i _{\pi(1)}\cdots i_{\pi(l)}}\), with \(S_{l}\) being the corresponding set of permutations. The expression in Eq. (1) involves three distinct outer products \((\hat{\mathbf{r}}\otimes\hat{\mathbf{r}})_{i_{1}i_{2}}=\hat{r}_{i_{1}}\hat{r} _{i_{2}}\), \((\mathbf{I}\otimes\mathbf{I})_{i_{1}i_{2}i_{3}i_{4}}=\delta_{i_{1}i_{2}} \delta_{i_{3}i_{4}}\), and \((\hat{\mathbf{r}}\otimes\mathbf{I})_{i_{1}i_{2}i_{3}}=\hat{r}_{i_{1}}\delta_{ i_{2}i_{3}}\), where \(\delta_{i_{2}i_{3}}\) denotes the Kronecker delta. The normalization constant \(C=(2l-1)!!/l!\) is chosen such that an \(l\)-fold contraction of \(\mathbf{T}_{l}\) with the unit vector \(\hat{\mathbf{r}}\) yields unity. An example of an irreducible Cartesian tensor with rank \(l=3\) is \((\mathbf{T}_{l=3})_{i_{1}i_{2}i_{3}}=\frac{5}{2}\big{(}\hat{r}_{i_{1}}\hat{r} _{i_{2}}\hat{r}_{i_{3}}-\frac{1}{5}(\hat{r}_{i_{1}}\delta_{i_{2}i_{3}}+\hat{r} _{i_{2}}\delta_{i_{3}i_{1}}+\hat{r}_{i_{3}}\delta_{i_{1}i_{2}})\big{)}\).

**Irreducible Cartesian tensor product.** The following defines the product of two irreducible Cartesian tensors \(\mathbf{x}_{l_{1}}\in(\mathbb{R}^{3})^{\otimes l_{1}}\) and \(\mathbf{y}_{l_{2}}\in(\mathbb{R}^{3})^{\otimes l_{2}}\), yielding an irreducible Cartesian tensor of rank \(l_{3}\), i.e., \(\mathbf{z}_{l_{3}}=(\mathbf{x}_{l_{1}}\otimes_{\mathrm{Cart}}\mathbf{y}_{l_{2} })_{l_{3}}\in(\mathbb{R}^{3})^{\otimes l_{3}}\,\forall\,l_{3}\in\{|l_{1}-l_{2} |,\cdots,l_{1}+l_{2}\}\), that is symmetric and traceless. The irreducible Cartesian tensor product is crucial for designing equivariant MPNNs and is used for equivariant convolutions and constructing many-body features in Section 4.2. For an even \(l_{1}+l_{2}-l_{3}=2k\), the general form of an irreducible Cartesian tensor of rank \(l_{3}\) reads [45]

\[\begin{split}&(\mathbf{x}_{l_{1}}\otimes_{\mathrm{Cart}}\mathbf{y}_{l _{2}})_{l_{3}}\\ =& C_{l_{1}l_{2}l_{3}}\sum\nolimits_{m=0}^{\min(l_{1},l_{2})-k}(-1)^{m}2^{m}\frac{(2l_{3}-2m-1)!!}{(2l_{3}-1)!!}\big{\{}\,(\mathbf{x }_{l_{1}}\cdot(k+m)\cdot\mathbf{y}_{l_{2}})\otimes\mathbf{I}^{\otimes m} \big{\}},\end{split}\] (2)

where \((\mathbf{x}_{l_{1}}\cdot(k+m)\cdot\mathbf{y}_{l_{2}})=\sum_{i_{1},\cdots,i_{k +m}}(\mathbf{x}_{l_{1}})_{i_{1}\cdots i_{k+m}}(\mathbf{y}_{l_{2}})_{i_{1} \cdots i_{k+m}}\) denotes an \((k+m)\)-fold tensor contraction, which results in a tensor of rank \(l_{1}+l_{2}-2(k+m)\). For simplicity, we skip the uncontracted indices in the above definition. For example, for \(l_{1}=4\) and \(l_{2}=3\) and a three-fold tensor contraction we obtain \((\mathbf{x}_{l_{1}}\cdot 3\cdot\mathbf{y}_{l_{2}})_{i_{4}}=\sum_{i_{1},i_{2},i_{3}}( \mathbf{x}_{l_{1}})_{i_{1}i_{2}i_{3}i_{4}}(\mathbf{y}_{l_{2}})_{i_{1}i_{2}i_{3}}\), i.e., the corresponding tensors are contracted along \(i_{1}\), \(i_{2}\), and \(i_{3}\). Note that the final result is independent of the index permutation, as the contracted tensors are symmetric. For an odd \(l_{1}+l_{2}-l_{3}=2k+1\), we define [45]

\[\begin{split}(\mathbf{x}_{l_{1}}\otimes_{\mathrm{Cart}}\mathbf{y}_{l _{2}})_{l_{3}}\\ =& D_{l_{1}l_{2}l_{3}}\sum\nolimits_{m=0}^{\min(l_{1},l_{2})-k-1}(-1)^{m}2^{m}\frac{(2l_{3}-2m-1)!!}{(2l_{3}-1)!!}\big{\{}\,( \boldsymbol{\varepsilon}:\mathbf{x}_{l_{1}}\cdot(k+m)\cdot\mathbf{y}_{l_{2}}) \otimes\mathbf{I}^{\otimes m}\big{\}},\end{split}\] (3)

with \(\boldsymbol{\varepsilon}\) denoting the Levi-Civita symbol (\(\varepsilon_{i_{1}i_{2}i_{3}}=-\varepsilon_{i_{3}i_{2}i_{4}}\) and \(\varepsilon_{i_{1}i_{1}i_{1}}=0\)). The double contraction with the Levi-Civita symbol reads \((\boldsymbol{\varepsilon}:\mathbf{x}_{l_{1}}\cdot(k+m)\cdot\mathbf{y}_{l_{2}})_{ i_{1}}=\sum_{i_{2},i_{3}}\varepsilon_{i_{1}i_{2}i_{3}}\,(\mathbf{x}_{l_{1}} \cdot(k+m)\cdot\mathbf{y}_{l_{2}})_{i_{2}i_{3}}\), and yields a tensor of rank \(l_{1}+l_{2}-2(k+m)-1\). Details on the normalization constants \(C_{l_{1}l_{2}l_{3}}\) and \(D_{l_{1}l_{2}l_{3}}\) are provided in Appendix B.1.

### Equivariant message-passing based on irreducible Cartesian tensors

The following section introduces the basic operations for constructing equivariant MPNNs based on irreducible Cartesian tensors. Using their irreducible tensor products, we demonstrate how to build equivariant two- and many-body features, crucial for modeling many-body interactions in molecular and materials systems. Particularly, we focus on MLIPs based on equivariant MPNNs and extend the state-of-the-art MACE architecture [33] to the Cartesian basis. Following the MACE architecture, we use only even tensor products. We split vectors \(\mathbf{r}_{uv}=\mathbf{r}_{u}-\mathbf{r}_{v}\in\mathbb{R}^{3}\) from atom \(u\) to atom \(v\), schematically shown in Fig. 1 (a), into their radial and angular components (unit vectors), i.e.,\(r_{uv}=\|\mathbf{r}_{uv}\|_{2}\in\mathbb{R}\) and \(\hat{\mathbf{r}}_{uv}=\mathbf{r}_{uv}/r_{uv}\in\mathbb{R}^{3}\), respectively. In the \(t\)-th message-passing layer, edges \(\{u,v\}\) are embedded using a fully connected neural network \(R_{kl_{1}l_{2}l_{3}}^{(t)}:\mathbb{R}\rightarrow\mathbb{R}\) with \(k\) output feature channels. The radial function \(R_{kl_{1}l_{2}l_{3}}^{(t)}\) takes as an input radial distances \(r_{uv}\), which are embedded through Bessel functions and multiplied by a smooth polynomial cutoff function [60]. Finally, we use irreducible Cartesian tensors \(\mathbf{T}_{1}(\hat{\mathbf{r}})\), similar to spherical tensors \(\mathbf{Y}^{l}(\hat{\mathbf{r}})\), to embed unit vectors into the tensor space of maximal rank \(l_{\max}\).

**Equivariant convolutions and two-body features.** Rotation equivariance in MPNNs is typically achieved by constraining convolution filters to be the products between learnable radial functions and spherical tensors, i.e., \(R_{kl_{1}l_{2}l_{3}}^{(t)}(r_{uv})Y_{m_{1}}^{l_{1}}(\hat{\mathbf{r}}_{uv})\). The two-body features \(A_{ukl_{3}m_{3}}^{(t)}\) are further obtained through the tensor product--the point-wise convolution [39]--between the respective filters and neighbors' equivariant features \(h_{ukl_{2}m_{2}}^{(t)}\). The permutational invariance is enforced by pooling over the neighbors \(v\in\mathcal{N}(u)\). Here, we use irreducible Cartesian tensors, with the rotation-equivariant filters given by \(R_{kl_{1}l_{2}l_{3}}^{(t)}(r_{uv})\big{(}\mathbf{T}_{l_{1}}(\hat{\mathbf{r}}_{ uv})\big{)}_{i_{1}i_{2}\cdots i_{l_{3}}}\). Thus, two-body features \(\big{(}\mathbf{A}_{ukl_{3}}^{(t)}\big{)}_{i_{1}i_{2}\cdots i_{l_{3}}}\) are obtained using the irreducible Cartesian tensor product and are represented by rank-\(l_{3}\) irreducible Cartesian tensors. The Cartesian two-body features are defined by

\[\big{(}\mathbf{A}_{ukl_{3}}^{(t)}\big{)}_{i_{1}i_{2}\cdots i_{l_{3}}}=\sum \nolimits_{v\in\mathcal{N}(u)}\Big{(}R_{kl_{1}l_{2}l_{3}}^{(t)}(r_{uv}) \mathbf{T}_{l_{1}}(\hat{\mathbf{r}}_{uv})\otimes_{\mathrm{Cart}}\frac{1}{ \sqrt{d_{t}}}\sum\limits_{k^{\prime}}W_{kk^{\prime}l_{2}}^{(t)}\mathbf{n}_{ vk^{\prime}l_{2}}^{(t)}\Big{)}_{i_{1}i_{2}\cdots i_{l_{3}}},\] (4)

where \(d_{t}\) represents the number of feature channels in the node embeddings \(\mathbf{h}_{vk^{\prime}l_{2}}^{(t)}\) of the \(t\)-th message-passing layer. In the first message-passing layer, node embeddings are initialized as learnable weights \(W_{kZ_{v}}\) that are invariant to actions of the orthogonal group, i.e., are scalars or tensors of rank \(l_{2}=0\), and embed the atom type \(Z_{v}\). Thus, constructing equivariant two-body features simplifies to

\[\big{(}\mathbf{A}_{ukl_{1}}^{(1)}\big{)}_{i_{1}i_{2}\cdots i_{l_{1}}}=\sum \nolimits_{v\in\mathcal{N}(u)}R_{kl_{1}}^{(1)}(r_{uv})\big{(}\mathbf{T}_{l_{1} }(\hat{\mathbf{r}}_{uv})\big{)}_{i_{1}i_{2}\cdots i_{l_{1}}}W_{kZ_{v}}.\] (5)

**Equivariant many-body features.** The importance of many-body terms arises from the fact that the interaction between atoms changes when additional atoms are present; see Appendix A. Furthermore, many-body terms are often required to ensure the generalization of interatomic potentials, i.e., their ability to accurately predict energies and forces for temperatures and stoichiometries on which they were not trained [71]. Here, we construct \((\nu+1)\)-body equivariant features from \(\big{(}\mathbf{A}_{ukl_{\xi}}^{(t)}\big{)}_{i_{1}i_{2}\cdots i_{l_{\xi}}}\) obtained using Eqs. (4) or (5). The \(\nu\)-fold Cartesian tensor product, which yields \((\nu+1)\)-body features represented by an irreducible Cartesian tensor of rank \(L\), reads

\[\big{(}\mathbf{B}_{u\eta_{\nu},kL}^{(t)}\big{)}_{i_{1}i_{2}\cdots i_{L}}=\big{(} \underbrace{\tilde{\mathbf{A}}_{ukl_{1}}^{(t)}\otimes_{\mathrm{Cart}}\cdots \otimes_{\mathrm{Cart}}\tilde{\mathbf{A}}_{ukl_{\nu}}^{(t)}}_{\nu\text{-fold}} \big{)}_{i_{1}i_{2}\cdots i_{L}},\] (6)

where \(\eta_{\nu}\) counts all possible \(\nu\)-fold products of \(\{l_{1},\cdots,l_{\nu}\}\)-rank tensors, yielding rank-\(L\) irreducible Cartesian tensors, and \(\tilde{\mathbf{A}}_{ukl_{\xi}}^{(t)}=\frac{1}{\sqrt{d_{t}}}\sum_{k^{\prime}}W_{ kk^{\prime}l_{\xi}}^{(t)}\mathbf{A}_{uk^{\prime}l_{\xi}}^{(t)}\) with \(d_{t}\) feature channels.

The irreducible Cartesian tensor product does not allow pre-computing the coefficients of the \(\nu\)-fold tensor product, differing from spherical tensors that use the generalized Clebsch-Gordan coefficients, contracted with weights from Eq. (A1) and summed over the possible paths \(\mathrm{len}(\eta_{\nu})\), for this purpose [33, 40, 51]. Thus, we obtain the result of Eq. (6) by iteratively applying the irreducible Cartesian tensor product \((\nu-1)\) times and refer to the respective models as irreducible Cartesian tensor potentials (ICTPs) with the full product basis or ICTPfull. However, two-fold tensor products in Eq. (6) are symmetric to permutations of involved tensors. Thus, the number of the \(\nu\)-fold tensor products, \(\mathrm{len}(\eta_{\nu})\), can be significantly reduced; we refer to the corresponding models as ICTPsym. Furthermore, we can reduce the computational cost of the Cartesian product basis by performing the calculations in a latent feature space. We use learnable weights \(W_{kk^{\prime}l_{\xi}}\) to reduce the number of feature channels for the product basis calculation and then increase it again for subsequent steps; we refer to the corresponding models as ICTPlt. For more details on the model architecture, such as the construction of updated many-body node embeddings, readout functions, and different options for the Cartesian product basis, see Appendix B.2.

**Runtime considerations.** When choosing an architecture to implement MLIPs, the runtime per energy and force evaluation is crucial. The computational complexity as a function of rank \(L\) is \(\mathcal{O}\left(9^{L}L!/\left(2^{L/2}\left(L/2\right)!\right)\right)\) for the irreducible Cartesian tensor product and \(\mathcal{O}\left(L^{5}\right)\) for the Clebsch-Gordan one; see Appendix B.3 for more details. Thus, equivariant convolutions based on spherical tensors are more computationally efficient for \(L\to\infty\) than those based on irreducible Cartesian tensors. However, state-of-the-art models and physical properties typically require \(L\leq 4\)[33; 72], making sub-leading terms and implementation-dependent amplitudes crucial. Based on our analysis, for \(L\leq 4\), we can expect equivariant convolutions based on irreducible Cartesian tensors to be more computationally and memory efficient than their spherical counterparts. For \(L\leq 4\), the \(\nu\)-fold tensor product in the Cartesian basis can also offer computational advantages. Its cost, as a function of rank \(L\) and correlation order \(\nu\), is \(\mathcal{K}(9^{L}L!/(2^{L/2}(L/2)!))^{\nu-1}\) and \(\mathcal{K}L^{\frac{1}{2}\nu(\nu+3)}\) for ICTP and MACE, respectively. The pre-factor \(\mathcal{K}\), which counts all possible \(\nu\)-fold tensor products, is removed in MACE through generalized Clebsch-Gordan coefficients, though these coefficients increase the memory spherical models use. Therefore, it is essential to consider the inference times for a fair comparison, which we provide in Section 5.

**Equivariance and tracels property of message-passing layers.** We conclude this section by giving a theoretical result that ensures the equivariance of message-passing layers based on irreducible Cartesian tensors and their irreducible tensor products to actions of the orthogonal group. The proof is provided in Appendix C. We also prove in Appendix D that these message-passing layers preserve the traceless property of irreducible Cartesian tensors.

**Proposition 4.1**.: _The message-passing layers based on irreducible Cartesian tensors and their irreducible tensor products are equivariant to actions of the orthogonal group._

**Proposition 4.2**.: _The message-passing layers based on irreducible Cartesian tensors and their irreducible tensor products preserve the traceless property of irreducible Cartesian tensors._

## 5 Experiments and results

This section presents the results for the five benchmark data sets: rMD17, MD22, 3BPA, acetylacetone, and Ta-V-Cr-W. We describe data sets and training details in Appendices E.1 and E.2, respectively.

**Scaling and computational cost**. The expressive power and computational efficiency of equivariant many-body message-passing potentials depend on the tensor ranks employed in equivariant message passing and embedding local atomic environments, i.e., \(L\) and \(l_{\max}\), respectively, as well as the correlation order \(\nu\). Recent work has shown that for identifying environments with \(L\)-fold symmetries, at least rank-\(L\) tensors are required [73]. These symmetries are typically lifted in atomistic simulations, motivating the use of \(L\leq 4\). Higher body-order correlations \(\nu\), in turn, are required if atomic environments are degenerate to a lower body-order correlation \(\nu-1\)[73; 74]. Figure 2 demonstrates inference times and memory consumption of models based on irreducible Cartesian and spherical tensors, i.e., ICTP and MACE, respectively, as a function of the tensor rank and the correlation order. Table A1 presents the corresponding numerical results. We find that irreducible Cartesian tensors

Figure 2: **Inference times and memory consumption as a function of the tensor rank \(L\) (a)–(b) and the correlation order \(\nu\) (c)–(d).** All results are obtained for the 3BPA data set and \(l_{\max}=L\). We used eight feature channels to allow experiments with larger \(\nu\) values. MACE models use intermediate tensors with \(l>l_{\max}\) for their product basis, which we fixed to \(l=l_{\max}\). Otherwise, pre-computing generalized Clebsch–Gordan coefficients for \(\nu>4\) would require more than 2 TB of RAM. For ICTP, we used the full product basis to compute the same number of \(\nu\)-fold tensor products as in MACE.

outperform spherical ones for most parameter values. In particular, irreducible Cartesian tensors allow spanning the \(\nu\)-space more efficiently, in line with our theoretical results in Section 4.2.

**Molecular dynamics trajectories.** We assess the performance of ICTP models using the revised MD17 (rMD17) data set, which includes structures, total energies, and atomic forces for ten small organic molecules obtained from ab initio molecular dynamics simulations [47]. Table 1 shows that ICTP\({}_{\text{sym}}\) achieves accuracy on par with state-of-the-art spherical and Cartesian models. Notably, several methods exhibit similar accuracy when trained with 950 configurations. However, the achieved accuracy is much lower than the desired accuracy of \(43.37\) meV \(\approx 1\) kcal/mol, making a model comparison less meaningful. Therefore, we also compare ICTP\({}_{\text{sym}}\) with MACE and NequIP, trained using 50 configurations, making learning accurate MLIPs more challenging. From Table 1, we see that ICTP\({}_{\text{sym}}\) outperforms MACE and NequIP for most molecules in this scenario.

We further evaluate the performance of ICTP using the MD22 data set, which contains seven molecular systems with 42-370 atoms [48]. This data set spans four major classes of biomolecules and supramolecules and was designed to challenge short-range models. Table 2 shows that ICTP achieves an accuracy on par with or better than state-of-the-art models, including long-range ones.

**Extrapolation to out-of-domain data.** We continue to assess the performance of ICTP models using the 3BPA data set [49]. The training data set comprises 500 configurations, total energies, and atomic forces acquired from molecular dynamics at 300 K. The test data set is obtained from simulations at 300 K, 600 K, and 1200 K. We also test our models using energies and forces along dihedral rotations of the molecule. Table 2 shows that ICTP models trained using 450 configurations perform on par with state-of-the-art spherical models, similar to the results for rMD17. However, we were not able to reproduce the original results using the current MACE source code and the described training setup [33]. Therefore, for a fair comparison, we unified the training setup for ICTP and MACE (see Appendix E.2) and Table 2 reports the corresponding results for 450 training configurations. In Table 3, we present the results obtained for ICTP and MACE trained using 50 configurations.

From Table 2, we observe that ICTP\({}_{\text{full}}\) slightly outperforms MACE in total energy and atomic force RMSEs but is \(\sim 1.4\) times less computationally efficient. This difference arises from MACE using the generalized Clebsch-Gordan coefficients and pre-computing their product with the weights in the linear expansion in Eq. (A1) [33], which reduces the effective number of evaluated tensor products. Thus, we may attribute the lower computational efficiency of ICTP\({}_{\text{full}}\) to the use of the

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & & & \(N_{\text{sym}}=950\) & & & & \(N_{\text{sym}}=50\) & \\  & & ICTP\({}_{\text{sym}}\) & TensorNet [30] & MACE [33] & AllEGro [34] & NequIP [25] & NequIP [25] & MACE [33] & ICTP\({}_{\text{sym}}\) \\ \hline Aspirin & E & \(\bm{2.27\pm 0.11}\) & 2.4 & **2.2** & 2.3 & 2.3 & 19.5 & 17.0 & \(\bm{14.84\pm 0.98}\) \\  & F & \(\bm{6.67\pm 0.19}\) & \(8.9\pm 0.1\) & **6.6** & 7.3 & 8.2 & 52.0 & 43.9 & \(\bm{40.19\pm 0.95}\) \\ \hline Axobenzene & E & \(1.20\pm 0.01\) & **0.7** & 1.2 & 1.2 & **0.7** & 6.0 & **5.4** & \(\bm{5.47\pm 0.63}\) \\  & F & \(2.92\pm 0.03\) & 3.1 & 3.0 & **2.6** & 2.9 & 20.0 & **17.7** & \(\bm{17.25\pm 0.53}\) \\ \hline Benzene & E & \(0.26\pm 0.00\) & **0.02** & 0.4 & 0.3 & 0.04 & 0.6 & 0.7 & \(\bm{0.38\pm 0.02}\) \\  & F & \(0.34\pm 0.02\) & 0.3 & 0.3 & **0.2** & 0.3 & 2.9 & 2.7 & \(\bm{2.45\pm 0.13}\) \\ \hline Ethanol & E & \(\bm{0.43\pm 0.02}\) & 0.5 & **0.4** & **0.4** & **0.4** & 8.7 & 6.7 & \(\bm{6.15\pm 0.26}\) \\  & F & \(2.63\pm 0.10\) & 3.5 & **2.1** & **2.1** & 2.8 & 40.2 & 32.6 & \(\bm{9.53\pm 1.14}\) \\ \hline Malonaldehyde & E & \(0.82\pm 0.03\) & 0.8 & 0.8 & **0.6** & 0.8 & 12.7 & **10.0** & \(\bm{9.72\pm 0.42}\) \\  & F & \(4.96\pm 0.21\) & 5.4 & 4.1 & **3.6** & 5.1 & 52.5 & **43.3** & \(\bm{42.88\pm 3.08}\) \\ \hline Naphthalene & E & \(0.56\pm 0.00\) & **0.2** & 0.5 & **0.2** & 0.9 & **2.1** & **2.1** & \(\bm{2.06\pm 0.10}\) \\  & F & \(1.45\pm 0.05\) & 1.6 & 1.6 & **0.9** & 1.3 & 10.0 & **9.2** & \(\bm{9.43\pm 0.46}\) \\ \hline Panacetamol & E & \(1.44\pm 0.03\) & **1.3** & 1.5 & 1.4 & 14.3 & 9.7 & \(\bm{8.94\pm 0.66}\) \\  & F & \(\bm{4.89\pm 0.11}\) & \(5.9\pm 0.1\) & **4.8** & 4.9 & 5.9 & 39.7 & \(\bm{31.5}\) & \(\bm{30.13\pm 1.51}\) \\ \hline Salicylic acid & E & \(0.97\pm 0.01\) & 0.8 & 0.9 & 0.9 & **0.7** & 8.0 & 6.5 & \(\bm{5.95\pm 0.43}\) \\  & F & \(3.66\pm 0.06\) & \(4.6\pm 0.1\) & 3.1 & **2.9** & 4.0 & 35.9 & \(\bm{28.4}\) & \(\bm{27.78\pm 1.39}\) \\ \hline Toluene & E & \(0.46\pm 0.00\) & **0.3** & 0.5 & 0.4 & **0.3** & 3.3 & 3.1 & \(\bm{2.45\pm 0.13}\) \\  & F & \(1.61\pm 0.02\) & 1.7 & **1.5** & 1.8 & 1.6 & 15.1 & 12.1 & \(\bm{11.24\pm 0.55}\) \\ \hline Uracil & E & \(0.57\pm 0.01\) & **0.4** & 0.5 & 0.6 & **0.4** & 7.3 & **4.4** & \(4.66\pm 0.16\) \\  & F & \(2.64\pm 0.08\) & 3.1 & 2.1 & **1.8** & 3.1 & 40.1 & **25.9** & \(\bm{25.97\pm 0.78}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Energy (E) and force (F) mean absolute errors (MAEs) for the rMD17 data set. E- and F-MAE are given in meV and meV/Å, respectively. Results are shown for models trained using \(N_{\text{train}}=\{950,50\}\) configurations randomly drawn from the data set, with further 50 used for early stopping. All values are obtained by averaging over five independent runs, with the standard deviation provided if available. Best performances, considering the standard deviation, are highlighted in bold.**MACE architecture, which results in a larger pre-factor \(\mathcal{K}\) for Cartesian models but facilitates a fair comparison between irreducible Cartesian and spherical tensors. Using the symmetric Cartesian product basis and that in the latent space, for example, we further improve the runtime of our models while maintaining accuracy on par with MACE.

Table A4 presents additional results obtained for \(\nu=1\), i.e., focusing on models that rely exclusively on two-body interactions. We observe that Cartesian models exhibit shorter inference times than spherical ones, with MACE and ICTP\({}_{\text{full}}\) achieving \(2.96\pm 0.06\) ms and \(2.63\pm 0.02\) ms, respectively. Regarding memory consumption, MACE and ICTP perform similarly despite the larger number of tensor products required for the Cartesian product basis in Eq. (6). This observation can be attributed to, for example, the Clebsch-Gordan tensor product requiring the computation of intermediate tensors with \((2L+1)^{2}\) elements, whereas irreducible Cartesian tensors contain \(3^{L}\) elements.

Figure 3 compares potential energy profiles obtained with ICTP and MACE trained using 50 configurations. For the potential energy cut at \(\beta=120^{\circ}\) (left panel), ICTP and MACE perform similarly, except for the energy barrier at \(\gamma\approx 143^{\circ}\), which ICTP tends to underestimate stronger than MACE.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & & ICTP\({}_{\text{full}}\) & ICTP\({}_{\text{full}}\) & ICTP\({}_{\text{spin}+1}\) & MACE\({}^{\text{a}}\) & CACE [31] & MACE [33] & NequIP [34] \\ \hline \multirow{2}{*}{300 K} & E & **2.70 \(\pm\) 0.22** & **2.70 \(\pm\) 0.08** & **2.98 \(\pm\) 0.34** & **2.81 \(\pm\) 0.18** & 6.3 & **3.0 \(\pm\) 0.2** & 3.28 \(\pm\) 0.10 \\  & F & **9.45 \(\pm\) 0.29** & **9.39 \(\pm\) 0.31** & **9.57 \(\pm\) 0.20** & **9.47 \(\pm\) 0.42** & 21.4 & **8.8 \(\pm\) 0.3** & 10.77 \(\pm\) 0.19 \\ \hline \multirow{2}{*}{600 K} & E & **10.74 \(\pm\) 0.31** & **10.38 \(\pm\) 0.80** & **10.29 \(\pm\) 0.90** & **11.11 \(\pm\) 1.41** & 18.0 & **9.7 \(\pm\) 0.5** & 11.16 \(\pm\) 0.14 \\  & F & **22.99 \(\pm\) 0.64** & **22.87 \(\pm\) 0.91** & **23.03 \(\pm\) 0.76** & **23.27 \(\pm\) 1.45** & 45.2 & **21.8 \(\pm\) 0.6** & 26.37 \(\pm\) 0.09 \\ \hline \multirow{2}{*}{1200 K} & E & **29.80 \(\pm\) 0.92** & **30.84 \(\pm\) 1.87** & **31.32 \(\pm\) 1.80** & **31.15 \(\pm\) 1.58** & 58.0 & **29.8 \(\pm\) 1.0** & 38.52 \(\pm\) 1.63 \\  & F & **62.82 \(\pm\) 1.23** & **64.54 \(\pm\) 3.88** & **65.36 \(\pm\) 3.47** & **62.32 \(\pm\) 3.52** & 113.3 & **62.0 \(\pm\) 0.7** & 76.18 \(\pm\) 1.11 \\ \hline \multirow{2}{*}{Diebradal slices} & E & **9.82 \(\pm\) 0.79** & 10.64 \(\pm\) 1.07 & 13.03 \(\pm\) 3.44 & **8.56 \(\pm\) 1.53** & – & **7.8 \(\pm\) 0.6** & 23.23 [33] \\  & F & **17.52 \(\pm\) 0.54** & **17.18 \(\pm\) 0.81** & 19.31 \(\pm\) 0.83 & **17.69 \(\pm\) 1.29** & – & **16.5 \(\pm\) 1.7** & 23.1 [33] \\ \hline \multirow{2}{*}{Inference time} & & 6.45 \(\pm\) 0.50 & 5.31 \(\pm\) 0.02 & **3.51 \(\pm\) 0.22** & 4.66 \(\pm\) 0.05 & – & **24.3\({}^{\text{b}}\)** & 103.5\({}^{\text{b}}\)[33] \\ \cline{2-10}  & Memory consumption & 49.66 \(\pm\) 0.00 & 42.01 \(\pm\) 0.11 & 39.08 \(\pm\) 0.00 & **36.26 \(\pm\) 0.00** & – & – & – \\ \hline \hline \end{tabular} \({}^{a}\) During inference time measurements with the MACE source code, we were not able to reproduce the original results [33]. Thus, we re-run MACE experiments using a training setup similar to that of ICTP; see Appendix E.2. \({}^{b}\) The original publication did not report the batch size used to measure inference time [33]. Therefore, the values provided are used solely to demonstrate the relative computational cost of MACE and NequIP.

\end{table}
Table 2: **Energy (E) and force (F) root-mean-square errors (RMSEs) for the 3BPA data set.** E- and F-RMSE are given in meV and meV/Å, respectively. Results are shown for models trained using 450 configurations randomly drawn from the training data set collected at 300 K, with further 50 used for early stopping. All ICTP results are obtained by averaging over five independent runs. For MACE and NequIP, the results are reported for three runs. The standard deviation is provided if it is available. Best performances, considering the standard deviation, are highlighted in bold. Inference time and memory consumption are measured for a batch size of 100. Inference time is reported per structure in ms, while memory consumption is provided for the entire batch in GB.

Figure 3: **Potential energy profiles for three cuts through the 3BPA molecule’s potential energy surface.** All models are trained using 50 configurations, and additional 50 are used for early stopping. The 3BPA molecule, including the three dihedral angles (\(\alpha\), \(\beta\), and \(\gamma\)), provided in degrees \({}^{\circ}\), is shown as an inset. The color code of the inset molecule is C grey, O red, N blue, and H white. The reference potential energy profile (DFT) is shown in black. Each profile is shifted such that each model’s lowest energy is zero. Shaded areas denote standard deviations across five independent runs.

For \(\beta=150^{\circ}\) (middle panel), however, ICTP\({}_{\text{full}}\) and ICTP\({}_{\text{sym}}\) outperform MACE across nearly the entire range of the dihedral angle \(\gamma\). For \(\beta=180^{\circ}\) (right panel), all models perform similarly. Figure A1 shows the corresponding potential energy profiles for models trained with 450 configurations. All models perform similarly in this scenario, with energy profiles close to the reference (DFT).

**Flexibility and reactivity.** We further use the acetylacetone data set to assess the ICTP models' extrapolation capabilities to higher temperatures (similar to 3BPA), bond breaking, and bond torsions [32]. Table 3 shows that ICTP models achieve state-of-the-art results while employing fewer parameters than spherical counterparts. Appendix E.3 includes additional results for the acetylacetone data set, such as total energy and atomic force RMSEs for models trained with 50 configurations and details on the potential energy profiles for hydrogen transfer and C-C bond rotation. Overall, ICTP and MACE perform similarly, demonstrating excellent generalization capability. However, when trained using 50 configurations, ICTP\({}_{\text{full}}\) is the only MLIP consistently producing the potential energy profile for hydrogen transfer close to the reference (DFT).

**Multicomponent alloys.** We finally evaluate the ICTP and MACE models using the Ta-V-Cr-W data set, designed to assess the performance of state-of-the-art MLIPs in modeling chemically complex multicomponent systems. In this evaluation, we attempt to predict energies and forces for Ta-V-Cr-W subsystems under two scenarios: The 0 K energies and forces in binary, ternary, and quaternary systems and near-melting temperature energies and forces in 4-component disordered alloys. Table A6 shows that ICTP\({}_{\text{sym}}\) outperforms MACE in nearly all subsystems, particularly in energy prediction. ICTP achieves an overall accuracy of 1.38 \(\pm\) 0.09 meV/atom for energies and 0.028 \(\pm\) 0.001 eV/A for forces, compared to 2.19 \(\pm\) 0.31 meV/atom and 0.029 \(\pm\) 0.001 eV/A by MACE. However, the \(\mathcal{K}\) pre-factor from the \(\nu\)-fold tensor product results in longer inference times for ICTP than MACE, in line with the discussion for 3BPA.

## 6 Conclusions and limitations

This work introduces many-body equivariant MPNNs based on higher-rank irreducible Cartesian tensors, offering an alternative to spherical models and addressing the limitations of state-of-the-art Cartesian models. We assess the performance of resulting MPNNs using five benchmark data sets, such as rMD17, MD22, 3BPA, acetylacetone, and Ta-V-Cr-W. In these experiments, MPNNs based on irreducible Cartesian tensors show a lower computational cost of individual operations compared to spherical counterparts. Furthermore, we demonstrate that these Cartesian models achieve accuracy and generalization capability on par with or better than state-of-the-art spherical models while memory consumption is comparable. Our results hold across the typical range of tensor ranks used in modeling many-body interactions and relevant physical properties, i.e., \(L\leq 4\).

**Limitations.** We emphasize our focus on introducing MPNNs based on irreducible Cartesian tensors and prove their equivariance and traceless property. We adapted the MACE architecture, which uses only even tensor products, to enable a fair comparison with state-of-the-art spherical models. Further modifications to the architecture are possible and necessary, e.g., to reduce the pre-factor arising from the Cartesian product basis, before we can fully exploit the potential of irreducible Cartesian tensors.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & & ICTP\({}_{\text{full}}\) & ICTP\({}_{\text{sym}}\) & ICTP\({}_{\text{sym}+\text{H}}\) & MACE\({}^{\text{a}}\) & MACE [33] & NequIP [32] \\ \hline \multirow{3}{*}{300 K} & E & **0.75 \(\pm\) 0.04** & **0.76 \(\pm\) 0.03** & **0.77 \(\pm\) 0.04** & **0.75 \(\pm\) 0.05** & 0.9 \(\pm\) 0.03 & **0.81 \(\pm\) 0.05** \\  & F & **5.08 \(\pm\) 0.11** & **5.17 \(\pm\) 0.10** & **5.18 \(\pm\) 0.16** & **5.00 \(\pm\) 0.17** & **5.1 \(\pm\) 0.1** & 5.90 \(\pm\) 0.46 \\ \hline \multirow{3}{*}{600 K} & E & **5.39 \(\pm\) 1.22** & **4.43 \(\pm\) 0.34** & **5.12 \(\pm\) 0.29** & **4.96 \(\pm\) 0.64** & **4.6 \(\pm\) 0.3** & 6.04 \(\pm\) 1.54 \\  & F & **23.21 \(\pm\) 1.96** & **22.90 \(\pm\) 1.62** & **24.05 \(\pm\) 1.71** & **23.25 \(\pm\) 1.82** & **22.4 \(\pm\) 0.9** & 27.80 \(\pm\) 4.03 \\ \hline \multicolumn{2}{l}{Number of parameters} & \multicolumn{2}{c}{2,774,800} & \multicolumn{2}{c}{2,736,400} & \multicolumn{2}{c}{2,648,080} & \multicolumn{2}{c}{2,803,984} & \multicolumn{2}{c}{2,803,984} & \multicolumn{2}{c}{3,190,488} \\ \hline \hline \end{tabular}

* Similar to Table 2, we re-run MACE experiments using the similar training setup as for ICTP; see Appendix E.2.

\end{table}
Table 3: **Energy (E) and force (F) root-mean-square errors (RMSEs) for the acetylacetone data set. E- and F-RMSE are given in meV and meV/A, respectively. Results are shown for models trained using 450 configurations randomly drawn from the training data set collected at 300 K, with further 50 used for early stopping. All ICTP results are obtained by averaging over five independent runs. For MACE and NequIP, the results are reported for three runs. The standard deviation is provided if it is available. Best performances, considering the standard deviation, are highlighted in bold.**

## Data availability

All data sets used in this study are publicly available: rMD17 (https://doi.org/10.6084/m9.figshare.12672038.v3), MD22 (http://www.sgdml.org), 3BPA (https://github.com/davkovacs/BOTNet-datasets), acetylacetone (https://github.com/davkovacs/BOTNet-datasets), and Ta-V-Cr-W (https://doi.org/10.18419/darus-3516).

## Code availability

The source code is available on GitHub and can be accessed via this link: https://github.com/nec-research/ictp.

## Acknowledgements

MN acknowledges support from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC 2075 - 390740016 and the Stuttgart Center for Simulation Science (SimTech).

## References

* [1] K. T. Butler, D. W. Davies, H. Cartwright, O. Isayev, and A. Walsh: _Machine learning for molecular and materials science_. Nature **559**, 547-555 (2018)
* [2] J. Vamathevan, D. Clark, P. Czodrowski, I. Dunham, E. Ferran et al.: _Applications of machine learning in drug discovery and development_. Nat. Rev. Drug Discov. **18**, 463-477 (2019)
* [3] J. A. Keith, V. Vassilev-Galindo, B. Cheng, S. Chmiela, M. Gastegger et al.: _Combining Machine Learning and Computational Chemistry for Predictive Insights Into Chemical Systems_. Chem. Rev. **121**, 9816-9872 (2021)
* [4] O. T. Unke, S. Chmiela, H. E. Sauceda, M. Gastegger, I. Poltavsky et al.: _Machine Learning Force Fields_. Chem. Rev. **121**, 10142-10186 (2021)
* [5] N. Fedik, R. Zubatyuk, M. Kulichenko, N. Lubbers, J. S. Smith et al.: _Extending machine learning beyond interatomic potentials for predicting molecular properties_. Nat. Rev. Chem. **6**, 653-672 (2022)
* [6] A. Merchant, S. Batzner, S. S. Schoenholz, M. Aykol, G. Cheon et al.: _Scaling deep learning for materials discovery_. Nature **624**, 80-85 (2023)
* [7] D. P. Kovacs, J. H. Moore, N. J. Browning, I. Batatia, J. T. Horton et al.: _MACE-OFF23: Transferable Machine Learning Force Fields for Organic Molecules_. https://arxiv.org/abs/2312.15211 (2023)
* [8] I. Batatia, P. Benner, Y. Chiang, A. M. Elena, D. P. Kovacs et al.: _A foundation model for atomistic materials chemistry_. https://arxiv.org/abs/2401.00096 (2023)
* [9] G. D. Purvis and R. J. Bartlett: _A full coupled-cluster singles and doubles model: The inclusion of disconnected triples_. J. Chem. Phys. **76**, 1910-1918 (1982)
* [10] T. D. Crawford and H. F. Schaefer III: _An Introduction to Coupled Cluster Theory for Computational Chemists_, pp. 33-136. John Wiley & Sons, Ltd (2000)
* [11] R. J. Bartlett and M. Musial: _Coupled-cluster theory in quantum chemistry_. Rev. Mod. Phys. **79**, 291-352 (2007)
* [12] P. Hohenberg and W. Kohn: _Inhomogeneous Electron Gas_. Phys. Rev. **136**, B864-B871 (1964)
* [13] W. Kohn and L. J. Sham: _Self-Consistent Equations Including Exchange and Correlation Effects_. Phys. Rev. **140**, A1133-A1138 (1965)* [14] A. Micheli: _Neural network for graphs: A contextual constructive approach_. IEEE Trans. Neural Netw. **20**, 498-511 (2009)
* [15] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini: _The graph neural network model_. IEEE Trans. Neural Netw. **20**, 61-80 (2009)
* [16] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl: _Neural Message Passing for Quantum Chemistry_. Int. Conf. Mach. Learn. **70**, 1263-1272 (2017)
* [17] W. L. Hamilton, R. Ying, and J. Leskovec: _Representation learning on graphs: Methods and applications_. IEEE Data Eng. Bull. **40**, 52-74 (2017)
* [18] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst: _Geometric deep learning: going beyond Euclidean data_. IEEE Signal Process. Mag. **34**, 18-42 (2017)
* [19] Z. Zhang, P. Cui, and W. Zhu: _Deep Learning on Graphs: A Survey_. IEEE Trans. Knowl. Data Eng. **34**, 249-270 (2022)
* [20] S. Zhang, H. Tong, J. Xu, and R. Maciejewski: _Graph convolutional networks: a comprehensive review_. Comput. Soc. Netw. **6**, 11 (2019)
* [21] D. Bacciu, F. Errica, A. Micheli, and M. Podda: _A Gentle Introduction to Deep Learning for Graphs_. Neural Netw. **129**, 203-221 (2020)
* [22] C. L. Zitnick, L. Chanussot, A. Das, S. Goyal, J. Heras-Domingo et al.: _An Introduction to Electrocatalyst Design using Machine Learning for Renewable Energy Storage_. https://arxiv.org/abs/2010.09435 (2020)
* [23] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov et al.: _Highly accurate protein structure prediction with AlphaFold_. Nature **596**, 583-589 (2021)
* [24] J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte et al.: _Robust deep learning-based protein sequence design using ProteinMPNN_. Science **378**, 49-56 (2022)
* [25] S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa et al.: _E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials_. Nat. Commun. **13**, 2453 (2022)
* [26] A. Duval, S. V. Mathis, C. K. Joshi, V. Schmidt, S. Miret et al.: _A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems_. https://arxiv.org/abs/2312.07511 (2024)
* [27] M. F. Langer, A. Goessmann, and M. Rupp: _Representations of molecules and materials for interpolation of quantum-mechanical simulations via machine learning_. npj Comput. Mater. **8**, 41 (2022)
* [28] K. T. Schutt, O. T. Unke, and M. Gastegger: _Equivariant message passing for the prediction of tensorial properties and molecular spectra_. Int. Conf. Mach. Learn. **139**, 9377-9388 (2021)
* [29] M. Haghighatlari, J. Li, X. Guan, O. Zhang, A. Das et al.: _NewtonNet: a Newtonian message passing network for deep learning of interatomic potentials and forces_. Digital Discovery **1**, 333-343 (2022)
* [30] G. Simeon and G. D. Fabritiis: _TensorNet: Cartesian Tensor Representations for Efficient Learning of Molecular Potentials_. Adv. Neural Inf. Process. Syst. **36**, 37334-37353 (2023)
* [31] B. Cheng: _Cartesian atomic cluster expansion for machine learning interatomic potentials_. npj Comput. Mater. **10**, 157 (2024)
* [32] I. Batatia, S. Batzner, D. P. Kovacs, A. Musaelian, G. N. C. Simm et al.: _The Design Space of E(3)-Equivariant Atom-Centered Interatomic Potentials_. https://arxiv.org/abs/2205.06643 (2022)
* [33] I. Batatia, D. P. Kovacs, G. N. C. Simm, C. Ortner, and G. Csanyi: _MACE: Higher Order Equivariant Message Passing Neural Networks for Fast and Accurate Force Fields_. Adv. Neural Inf. Process. Syst. **35**, 11423-11436 (2022)* [34] A. Musaelian, S. Batzner, A. Johansson, L. Sun, C. J. Owen et al.: _Learning local equivariant representations for large-scale atomistic dynamics_. Nat. Commun. **14**, 579 (2023)
* [35] S. Passaro and C. L. Zitnick: _Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs_. Int. Conf. Mach. Learn. **202**, 27420-27438 (2023)
* [36] S. Luo, T. Chen, and A. S. Krishnapriyan: _Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products_. Int. Conf. Learn. Represent. https://arxiv.org/abs/2401.10216 (2024)
* [37] P. Tholke and G. D. Fabritiis: _Equivariant Transformers for Neural Network based Molecular Potentials_. Int. Conf. Learn. Represent. https://arxiv.org/abs/2202.02541 (2022)
* [38] R. F. Snider: _Irreducible Cartesian Tensors_. De Gruyter, Berlin, Boston (2018)
* [39] N. Thomas, T. Smidt, S. Kearnes, L. Yang, L. Li et al.: _Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds_. https://arxiv.org/abs/1802.08219 (2018)
* [40] R. Drautz: _Atomic cluster expansion for accurate and transferable interatomic potentials_. Phys. Rev. B **99**, 014104 (2019)
* [41] E. Wigner: _Group Theory and its Application to the Quantum Mechanics of Atomic Spectra_, volume 5. Elsevier (2012)
* [42] J. A. R. Coope, R. F. Snider, and F. R. McCourt: _Irreducible Cartesian Tensors_. J. Chem. Phys. **43**, 2269-2275 (1965)
* [43] J. A. R. Coope and R. F. Snider: _Irreducible Cartesian Tensors. II. General Formulation_. J. Math. Phys. **11**, 1003-1017 (1970)
* [44] J. A. R. Coope: _Irreducible Cartesian Tensors. III. Clebsch-Gordan Reduction_. J. Math. Phys. **11**, 1591-1612 (1970)
* [45] D. R. Lehman and W. C. Parke: _Angular reduction in multiparticle matrix elements_. J. Math. Phys. **30**, 2797-2806 (1989)
* [46] M. Weiler, M. Geiger, M. Welling, W. Boomsma, and T. S. Cohen: _3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data_. Adv. Neural Inf. Process. Syst. **31** (2018)
* [47] A. S. Christensen and O. A. von Lilienfeld: _On the role of gradients for machine learning of molecular energies and forces_. Mach. Learn.: Sci. Technol. **1**, 045018 (2020)
* [48] S. Chmiela, V. Vassilev-Galindo, O. T. Unke, A. Kabylda, H. E. Sauceda et al.: _Accurate global machine learning force fields for molecules with hundreds of atoms_. Sci. Adv. **9**, eadf0873 (2023)
* [49] D. P. Kovacs, C. v. d. Oord, J. Kucera, A. E. A. Allen, D. J. Cole et al.: _Linear Atomic Cluster Expansion Force Fields for Organic Molecules: Beyond RMSE_. J. Chem. Theory Comput. **17**, 7696-7711 (2021)
* [50] K. Gubaev, V. Zaverkin, P. Srinivasan, A. I. Duff, J. Kastner et al.: _Performance of two complementary machine-learned potentials in modelling chemically complex systems_. npj Comput. Mater. **9**, 129 (2023)
* [51] R. Drautz: _Atomic cluster expansion of scalar, vectorial, and tensorial properties including magnetism and charge transfer_. Phys. Rev. B **102**, 024104 (2020)
* [52] B. Anderson, T. S. Hy, and R. Kondor: _Cormorant: Covariant Molecular Neural Networks_. Adv. Neural Inf. Process. Syst. **32**, 14537-14546 (2019)
* [53] V. G. Satorras, E. Hoogeboom, and M. Welling: _E(n) Equivariant Graph Neural Networks_. Int. Conf. Mach. Learn. **139**, 9323-9332 (2021)* [54] J. Brandstetter, R. Hesselink, E. van der Pol, E. J. Bekkers, and M. Welling: _Geometric and Physical Quantities improve E(3) Equivariant Message Passing_. Int. Conf. Learn. Represent. https://arxiv.org/abs/2110.02905 (2022)
* [55] K. Schutt, P.-J. Kindermans, H. E. Sauceda Felix, S. Chmiela, A. Tkatchenko et al.: _SchNet: A continuous-filter convolutional neural network for modeling quantum interactions_. Adv. Neural Inf. Process. Syst. **30**, 991-1001 (2017)
* [56] O. T. Unke and M. Meuwly: _PhysNet: A Neural Network for Predicting Energies, Forces, Dipole Moments, and Partial Charges_. J. Chem. Theory Comput. **15** (6), 3678-3693 (2019)
* [57] Y. Liu, L. Wang, M. Liu, Y. Lin, X. Zhang et al.: _Spherical Message Passing for 3D Molecular Graphs_. Int. Conf. Learn. Represent. https://arxiv.org/abs/2102.05013 (2022)
* [58] J. Gasteiger, S. Giri, J. T. Margraf, and S. Gunnemann: _Fast and Uncertainty-Aware Directional Message Passing for Non-Equilibrium Molecules_. Machine Learning for Molecules Workshop, Adv. Neural Inf. Process. Syst. https://arxiv.org/abs/2011.14115 (2020)
* [59] J. Gasteiger, F. Becker, and S. Gunnemann: _GemNet: Universal Directional Graph Neural Networks for Molecules_. Adv. Neural Inf. Process. Syst. **34**, 6790-6802 (2021)
* [60] J. Gasteiger, J. Gross, and S. Gunnemann: _Directional Message Passing for Molecular Graphs_. Int. Conf. Learn. Represent. https://arxiv.org/abs/2003.03123 (2020)
* [61] D. P. Kovacs, I. Batatia, E. S. Arany, and G. Csanyi: _Evaluation of the MACE force field architecture: From medicinal chemistry to materials science_. J. Chem. Phys. **159**, 044118 (2023)
* [62] F. Fuchs, D. Worrall, V. Fischer, and M. Welling: _SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks_. Adv. Neural Inf. Process. Syst. **33**, 1970-1981 (2020)
* [63] T. Frank, O. Unke, and K.-R. Muller: _So3krates: Equivariant attention for interactions on arbitrary length-scales in molecular systems_. Adv. Neural Inf. Process. Syst. **35**, 29400-29413 (2022)
* [64] Y.-L. Liao, B. Wood, A. Das, and T. Smidt: _EquformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations_. Int. Conf. Learn. Represent. https://arxiv.org/abs/2306.12059 (2023)
* [65] A. V. Shapeev: _Moment Tensor Potentials: A Class of Systematically Improvable Interatomic Potentials_. Multiscale Model. Simul. **14**, 1153-1173 (2016)
* [66] V. Zaverkin and J. Kastner: _Gaussian Moments as Physically Inspired Molecular Descriptors for Accurate and Scalable Machine Learning Potentials_. J. Chem. Theory Comput. **16**, 5410-5421 (2020)
* [67] V. Zaverkin, D. Holzmuller, I. Steinwart, and J. Kastner: _Fast and Sample-Efficient Interatomic Neural Network Potentials for Molecules and Materials Based on Gaussian Moments_. J. Chem. Theory Comput. **17**, 6658-6670 (2021)
* [68] J. Behler and M. Parrinello: _Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces_. Phys. Rev. Lett. **98**, 146401 (2007)
* [69] U. Fano and G. Racah: _Irreducible Tensorial Sets_. Academic Press Inc., New York (1959)
* [70] I. M. Gel'fand, R. A. Minlos, and Z. Y. Shapiro: _Representations of the Rotation and Lorentz Groups and Their Applications_. Pergamon Press, Inc. (1963)
* [71] S. V. S. Martin H. Muser and L. Pastewka: _Interatomic potentials: achievements and challenges_. Adv. Phys. X **8**, 2093129 (2023)
* [72] I. Grega, I. Batatia, G. Csanyi, S. Karlapati, and V. Deshpande: _Energy-conserving equivariant GNN for elasticity of lattice architected metamaterials_. Int. Conf. Learn. Represent. https://arxiv.org/abs/2401.16914 (2024)* [73] C. K. Joshi, C. Bodnar, S. V. Mathis, T. Cohen, and P. Lio: _On the Expressive Power of Geometric Graph Neural Networks_. Int. Conf. Learn. Represent. https://arxiv.org/abs/2301.09308 (2023)
* [74] S. N. Pozdnyakov, M. J. Willatt, A. P. Bartok, C. Ortner, G. Csanyi et al.: _Incompleteness of Atomic Structure Representations_. Phys. Rev. Lett. **125**, 166001 (2020)
* [75] J. Behler: _Atom-centered symmetry functions for constructing high-dimensional neural network potentials_. J. Chem. Phys. **134**, 074106 (2011)
* [76] A. P. Bartok, M. C. Payne, R. Kondor, and G. Csanyi: _Gaussian Approximation Potentials: The Accuracy of Quantum Mechanics, without the Electrons_. Phys. Rev. Lett. **104**, 136403 (2010)
* [77] A. P. Bartok, R. Kondor, and G. Csanyi: _On representing chemical environments_. Phys. Rev. B **87**, 184115 (2013)
* [78] M. S. Daw and M. I. Baskes: _Embedded-atom method: Derivation and application to impurities, surfaces, and other defects in metals_. Phys. Rev. B **29**, 6443-6453 (1984)
* [79] Y.-M. Kim, B.-J. Lee, and M. I. Baskes: _Modified embedded-atom method interatomic potentials for Ti and Zr_. Phys. Rev. B **74**, 014101-014112 (2006)
* [80] A. Stone: _Transformation between cartesian and spherical tensors_. Mol. Phys. **29**, 1461-1471 (1975)
* [81] A. J. Stone: _Properties of Cartesian-spherical transformation coefficients_. J. Phys. A **9**, 485 (1976)
* [82] J. M. Normand and J. Raynal: _Relations between Cartesian and spherical components of irreducible Cartesian tensors_. J. Phys. A **15**, 1437 (1982)
* [83] K. He, X. Zhang, S. Ren, and J. Sun: _Deep Residual Learning for Image Recognition_. IEEE Conf. Comput. Vis. Pattern Recognit. https://doi.org/10.1109/CVPR.2016.90 (2016)
* [84] S. Chmiela, A. Tkatchenko, H. E. Sauceda, I. Poltavsky, K. T. Schutt et al.: _Machine learning of accurate energy-conserving molecular force fields_. Sci. Adv. **3**, e1603015 (2017)
* [85] K. T. Schutt, F. Arbabzadah, S. Chmiela, K. R. Muller, and A. Tkatchenko: _Quantum-chemical insights from deep tensor neural networks_. Nat. Commun. **8**, 13890 (2017)
* [86] S. Chmiela, H. E. Sauceda, K.-R. Muller, and A. Tkatchenko: _Towards exact molecular dynamics simulations with machine-learned force fields_. Nat. Commun. **9**, 3887 (2018)
* [87] S. Elfwing, E. Uchibe, and K. Doya: _Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning_. Neural Netw. **107**, 3-11 (2018)
* [88] P. Ramachandran, B. Zoph, and Q. V. Le: _Searching for activation functions_. Int. Conf. Learn. Represent. https://arxiv.org/abs/1710.05941 (2018)
* [89] S. J. Reddi, S. Kale, and S. Kumar: _On the Convergence of Adam and Beyond_. Int. Conf. Learn. Represent. https://arxiv.org/abs/1904.09237 (2018)
* [90] Y. Li, Y. Wang, L. Huang, H. Yang, X. Wei et al.: _Long-Short-Range Message-Passing: A Physics-Informed Framework to Capture Non-Local Interaction for Scalable Molecular Dynamics Simulation_. Int. Conf. Learn. Represent. https://arxiv.org/abs/2304.13542 (2024)
* [91] J. T. Frank, O. T. Unke, K.-R. Muller, and S. Chmiela: _A Euclidean transformer for fast and stable machine learned force fields_. Nat. Commun. **15**, 6539 (2024)

## Appendix

* [1] A Background
* B Methods
* B.1 Normalization constants
* B.2 Further details on the equivariant message passing
* B.3 Computational complexity and runtime analysis
* C Proof of Cartesian message-passing equivariance to actions of the orthogonal group
* D Proof of the traceless property for irreducible Cartesian tensors
* E Experiments and results
* E.1 Description of the data sets
* E.2 Training details
* E.3 Additional results
* F Broader social impact
Background

**Message-passing neural networks.** Message-passing neural networks (MPNNs) learn node representations in a graph by iteratively processing local information sent by the nodes' neighbors. The initial features of node \(u\) are represented as the vector \(\mathbf{x}_{u}\), and undirected edges \(\{u,v\}\) connect pairs of nodes \(u,v\). A node \(v\) belongs to the neighborhood of node \(u\), denoted as \(\mathcal{N}(u)\), if there exists an edge \(\{u,v\}\) in the graph. Typically, the \((t+1)\)-th message-passing layer computes a new node \(u\)'s representation \(\mathbf{h}_{u}^{(t+1)}\) by applying a permutation invariant aggregation function over the neighbors [16; 21]

\[\mathbf{h}_{u}^{(t+1)}=\phi^{(t)}\Big{(}\mathbf{h}_{u}^{(t)},\sum\nolimits_{ v\in\mathcal{N}(u)}\psi^{(t)}\big{(}\mathbf{h}_{u}^{(t)},\mathbf{h}_{v}^{(t)} \big{)}\Big{)},\]

where \(\phi^{(t)},\psi^{(t)}\) are often implemented as learnable fully-connected neural networks (NNs), and \(\mathbf{h}_{u}^{(0)}=\mathbf{x}_{u}\). To learn a mapping from a learned representation \(\mathbf{h}_{u}^{(t)}\) to the atoms' energies, we can couple \(T\) message-passing layers with corresponding readout functions \(\mathcal{R}_{t},\,t\in\{1,\dots,T\}\) such that \(E_{u}=\sum_{t=1}^{T}\mathcal{R}_{t}\big{(}\mathbf{h}_{u}^{(t)}\big{)}\).

**Many-body interatomic potentials.** Interatomic potentials approximate the potential energy of atoms--the electronic ground state energy--as a function of their coordinates [71]. Many-body potentials naturally arise because the interaction between two atoms is influenced by the presence of additional atoms, changing their electronic structure. This concept is formalized by expanding the atomic energy \(E_{u}\) of a many-atom system into a series of two-body, three-body, and higher-body-order contributions

\[E_{u}=E^{(1)}(\mathbf{r}_{u})+\sum_{v_{1}}E^{(2)}(\mathbf{r}_{u},\mathbf{r}_{ v_{1}})+\sum_{v_{1}<v_{2}}E^{(3)}(\mathbf{r}_{u},\mathbf{r}_{v_{1}},\mathbf{r}_{ v_{2}})+\cdots,\]

where \(\mathbf{r}_{u}\) represents the position of atom \(u\) and the superscript \(\nu\) in \(E^{(\nu)}\) indicates the order of the many-body interaction. In the absence of external fields, \(E^{(\nu)}\) contributions to the atomic energy are invariant to rotations, and the two-body potential depends only on distances \(r_{uv}=\|\mathbf{r}_{u}-\mathbf{r}_{v}\|_{2}\)

\[E_{u}=\sum_{v_{1}}E^{(2)}(r_{uv_{1}})+\sum_{v_{1}<v_{2}}E^{(3)}(r_{uv_{1}},r_ {uv_{2}},r_{v_{1}v_{2}})+\cdots.\]

Expansions of this form have found a broad application in constructing machine-learned interatomic potentials (MLIPs), significantly advancing the field [40; 65].

**Higher-body-order local descriptors.** Recent advances in MLIPs have been influenced by moment tensor potentials (MTPs) [65] and atomic cluster expansion (ACE) [40]. These approaches enable systematic construction of higher-body-order polynomial basis functions, encompassing representations like atom-centered symmetry functions (ACSFs) [68; 75], smooth overlap of atomic positions (SOAP) [76; 77], Gaussian moments [66; 67], and embedded atom/multi-scale embedded atom method (EAM/MEAM) potentials [78; 79]. Furthermore, a reducible Cartesian tensor can be represented as a linear combination of irreducible spherical counterparts, and vice versa [40; 51]. More general expressions, including tensor contractions, have also been provided, hinting at the relationship between Cartesian and spherical models [80; 81; 82]. Despite the success of MTP and ACE, defining smaller cutoff radii and rigid architecture can result in limited accuracy compared to MPNNs.

**Many-body message passing.** Designing accurate and computationally efficient interatomic potential models for interacting many-body systems requires including higher-body-order energy contributions and, thus, higher-body-order learnable features. Recently, a new message construction mechanism has been proposed by expanding the messages \(\mathbf{m}_{u}^{(t)}\) to include many-body contributions [33]

\[\mathbf{m}_{u}^{(t)}=\sum_{v_{1}}\psi_{2}^{(t)}\big{(}\mathbf{h}_{u}^{(t)}, \mathbf{h}_{v_{1}}^{(t)}\big{)}+\sum_{v_{1},v_{2}}\psi_{3}^{(t)}\big{(} \mathbf{h}_{u}^{(t)},\mathbf{h}_{v_{1}}^{(t)},\mathbf{h}_{v_{2}}^{(t)}\big{)} +\cdots+\sum_{v_{1},\cdots,v_{\nu}}\psi_{\nu+1}^{(t)}\big{(}\mathbf{h}_{u}^{(t )},\mathbf{h}_{v_{1}}^{(t)},\cdots,\mathbf{h}_{v_{\nu}}^{(t)}\big{)},\]

where \((\nu+1)\) denotes the order of many-body interactions, defining the number of contracted tensors. Using \(\sum_{v_{1},\cdots,v_{\nu}}\) instead of \(\sum_{v_{1}<\cdots<v_{\nu}}\) circumvents the exponential increase in computational cost with \(\nu\). It allows exploiting the product structure of many-body features, which differs from other approaches [58; 59; 60].

Methods

### Normalization constants

The following provides the normalization constants \(C_{l_{1}l_{2}l_{3}}\) and \(D_{l_{1}l_{2}l_{3}}\) for even and odd irreducible Cartesian tensor products in Eqs. (2) and (3), respectively. The respective normalization constants read [45]

\[C_{l_{1}l_{2}l_{3}} =\frac{l_{1}!l_{2}!(2l_{3}-1)!!((L_{1}+1)/2)!((L_{2}+1)/2)!}{l_{3}!L _{1}!!L_{2}!!L_{3}!!(L/2)!},\] \[D_{l_{1}l_{2}l_{3}} =\frac{2l_{1}!l_{2}!(2l_{3}-1)!!(L_{1}/2)!(L_{2}/2)!}{(l_{3}-1)!(L _{1}+1)!!(L_{2}+1)!!(L_{3}+1)!!((L+1)/2)!},\]

with \(L=l_{1}+l_{2}+l_{3}\) and \(L_{i}=L-2l_{i}-1\). Here, \(C_{l_{1}l_{2}l_{3}}\) is defined such that an \(l_{3}\)-fold contraction of the tensor \(\mathbf{z}_{l_{3}}\), obtained through the irreducible Cartesian tensor product between \(\mathbf{x}_{l_{1}}\) and \(\mathbf{y}_{l_{2}}\) (\(\mathbf{x}_{l_{1}}\) and \(\mathbf{y}_{l_{2}}\) are obtained using Eq. (1)), with the unit vector \(\hat{\mathbf{r}}\) yields unity. We refer to the original publication for the motivation behind \(D_{l_{1}l_{2}l_{3}}\)[45].

### Further details on the equivariant message passing

The following provides additional details on the employed equivariant message-passing architecture. Learnable weights employed in Eqs. (4), (5), and (6) (i.e., \(W^{(t)}_{kk^{\prime}l_{2}}\), \(W^{(t)}_{kZ_{v}}\), and \(W^{(t)}_{kk^{\prime}l_{\xi}}\) with \(k,k^{\prime}\) running over feature channels) are initialized by picking the respective entries from a normal distribution with zero mean and unit variance.

**Many-body message-passing.** The many-body equivariant features, represented by an irreducible Cartesian tensor of rank \(L\) and obtained through the \(\nu\)-fold tensor product in Eq. (6), are combined using the linear expansion

\[\big{(}\mathbf{m}^{(t)}_{ukL}\big{)}_{i_{1}i_{2}\cdots i_{L}}=\Big{(}\sum_{ \nu}\sum_{\eta_{\nu}}W^{(t)}_{Z_{u}\eta_{\nu}kL}\mathbf{B}^{(t)}_{u\eta_{\nu} kL}\Big{)}_{i_{1}i_{2}\cdots i_{L}},\] (A1)

where \(W^{(t)}_{Z_{u}\eta_{\nu}kL}\) denotes a learnable weight matrix which depends on the chemical element \(Z_{u}\) and rank \(L\) and which elements are initialized by picking the respective entries from a normal distribution with zero mean and a standard deviation of \(1/\mathrm{len}(\eta_{\nu})\). The updated node embeddings are further obtained as a linear function of \(\big{(}\mathbf{m}^{(t)}_{ukL}\big{)}_{i_{1}i_{2}\cdots i_{L}}\) and the residual connection [33; 83]

\[\big{(}\mathbf{h}^{(t+1)}_{ukL}\big{)}_{i_{1}i_{2}\cdots i_{L}}=\frac{1}{ \sqrt{d_{t}}}\sum_{k^{\prime}}W^{(t)}_{kk^{\prime}L}\big{(}\mathbf{m}^{(t)}_{ uk^{\prime}L}\big{)}_{i_{1}i_{2}\cdots i_{L}}+\frac{1}{\sqrt{d_{t}N_{Z}}}\sum_{k^{ \prime}}W^{(t)}_{Z_{u}kk^{\prime}L}\big{(}\mathbf{h}^{(t)}_{uk^{\prime}L} \big{)}_{i_{1}i_{2}\cdots i_{L}},\] (A2)

where \(N_{Z}\) denotes the number of atom types and learnable weights \(W^{(t)}_{kk^{\prime}L}\) and \(W^{(t)}_{Z_{u}kk^{\prime}L}\) are initialized by picking the respective entries from a normal distribution with zero mean and unit variance.

**Full and symmetric product basis.** In the MACE architecture, the authors pre-compute products between the generalized Clebsch-Gordan coefficients, which define the interactions of \(\{l_{1},\cdots,l_{\nu}\}\)-rank spherical tensors, and the learnable weight \(W^{(t)}_{Z_{u}\eta_{\nu}kL}\) of the linear expansion in Eq. (A1) [33]. This approach reduces the computational cost of constructing the product basis with spherical tensors as the effective number of evaluated tensor products is smaller by \(\mathcal{K}=\mathrm{len}\left(\eta_{\nu}\right)\). For irreducible Cartesian tensors, operations like matrix-vector and matrix-matrix products define the interactions between \(\{l_{1},\cdots,l_{\nu}\}\)-rank tensors. Thus, an equivalent operation to those proposed for spherical tensors may be generally impossible for irreducible Cartesian tensors or would lead to an architecture different from MACE.

Note that one of our main goals is to demonstrate that a many-body equivariant message-passing architecture defined using higher-rank irreducible Cartesian tensors can be as expressive as the one using spherical tensors. Therefore, we compute \(\mathbf{m}^{(t)}_{ukL}\) by iteratively evaluating all possible \(\nu\)-fold tensor products, which effectively leads to a larger number of operations than for MACE. We refer to the models based on this architecture as irreducible Cartesian tensor potentials (ICTPs) with the full product basis, or \(\mathrm{ICTP}_{\mathrm{full}}\). We also noticed that the number of tensor products \(\mathrm{len}(\eta_{\nu})\) leading to the tensor of rank \(L\), can be reduced by the symmetry of the two-fold tensor product in Eq. (6), i.e., \(\mathbf{A}_{l_{1}}\otimes\mathbf{A}_{l_{2}}=\mathbf{A}_{l_{2}}\otimes\mathbf{A}_ {l_{1}}\) if \(\mathbf{A}_{l_{1}}\) and \(\mathbf{A}_{l_{2}}\) are symmetric. We refer to this design choice as ICTPs with the symmetric product basis or ICTP\({}_{\mathrm{sym}}\).

**Coupled feature channels.** Using coupled feature channels instead of uncoupled ones in Eq. (A1) can improve the performance of the final model. Additionally, the number of feature channels and, thus, the overall computational cost can be reduced when constructing the product basis. However, the number of parameters and, thus, the expressive power of the model can be preserved. Specifically, we can define a linear expansion for combining many-body equivariant features as

\[\left(\mathbf{m}_{ukL}^{(t)}\right)_{i_{1}i_{2}\cdots i_{L}}=\Big{(}\sum_{\nu} \sum_{\eta_{\nu}}\sum_{k^{\prime}}W_{Z_{u}\eta_{\nu}kk^{\prime}L}^{(t)}\mathbf{ B}_{u\eta_{\nu}k^{\prime}L}^{(t)}\Big{)}_{i_{1}i_{2}\cdots i_{L}},\] (A3)

where \(W_{Z_{u}\eta_{\nu}kk^{\prime}L}^{(t)}\) denotes a learnable weight matrix which depends on the chemical element \(Z_{u}\) and rank \(L\) and which elements are initialized by picking the respective entries from a normal distribution with zero mean and a standard deviation of \(1/(\sqrt{d_{i}}\times\mathrm{len}(\eta_{\nu}))\). The construction of the product basis in the latent feature space requires encoding the two-body features \(\mathbf{A}_{ukl_{\xi}}^{(t)}\) using the learnable weight matrices in Eq. (6). The linear expansion is then constructed in the latent feature space and decoded using the learnable weight matrices of the linear function in Eq. (A2). We refer to this design choice as ICTPs with the product basis constructed in the latent feature space of ICTP\({}_{\mathrm{lt}}\). Combining it with the symmetric product basis, we obtain ICTP\({}_{\mathrm{sym}+\mathrm{lt}}\).

**Readout.** Atomic energies expanded into a series of many-body contributions, \(E_{u}=E_{u}^{(0)}+E_{u}^{(1)}+\cdots+E_{u}^{(T)}\), are obtained by applying readout functions \(\mathcal{R}_{t}\) to node features with \(L=0\), which are invariant to rotations

\[E_{u}^{(t)}=\mathcal{R}_{t}\Big{(}\big{\{}\mathbf{h}_{uk(L=0)}^{(t)}\big{\}}_ {k}\Big{)}=\begin{cases}\frac{1}{\sqrt{d_{t}}}\sum_{k^{\prime}}W_{k^{\prime} }^{(t)}h_{uk^{\prime}(L=0)}^{(t)}&\text{if $t<T$},\\ \mathrm{NN}^{(t)}\Big{(}\big{\{}\mathbf{h}_{uk(L=0)}^{(t)}\big{\}}_{k}\Big{)}& \text{if $t=T$},\end{cases}\] (A4)

with learnable weights \(W_{k^{\prime}}^{(t)}\) or those of \(\mathrm{NN}^{(t)}\) initialized by picking the respective entries from a normal distribution with zero mean and unit variance. Linear readout functions for \(t<T\) preserve the many-body orders in \(\mathbf{h}_{uk(L=0)}^{(t)}\), while a one-layer fully-connected NN is used for the last message-passing layer and accounts for the residual higher-order terms in the expansion [32].

### Computational complexity and runtime analysis

We analyze the computational complexity of the irreducible Cartesian tensor product with respect to the maximum rank \(L\) used. For each tuple of \((i_{1},\cdots,i_{L})\) of a rank-\(L\) tensor in Eq. (2), the single contraction term \((\mathbf{x}_{L}\cdot(k+m)\cdot\mathbf{y}_{L})\) has a cost of \(3^{k+m}\). The total computational cost is \(3^{L}\) for even tensor products and \(3^{L+1}\) for odd ones--due to the additional double contraction with the Levi-Civita symbol. The computation of the set of permutations over the \(L\) unsymmetrized indices scales as \(L!/\left(2^{L/2}\left(L/2\right)!\right)\) for even tensor products and \(L!/\left(2^{\left(L-1\right)/2}\left(\left(L-1\right)/2\right)!\right)\) for odd ones [45]. Because each final rank-\(L\) tensor has \(3^{L}\) elements, the complexity for computing an irreducible Cartesian tensor of rank \(L\) is \(\mathcal{O}\left(9^{L}L!/\left(2^{L/2}\left(L/2\right)!\right)\right)\). This expression is used throughout this work as it captures contributions from tensor contractions and index permutations, though it simplifies asymptotically to \(\mathcal{O}\left(L^{L}\right)\) using Stirling's approximation for factorials.

The number of calculations required to obtain a rank-\(L\) spherical tensor through the Clebsch-Gordan tensor product is \(\left(2L+1\right)^{5}\). Thus, the computational complexity of the Clebsch-Gordan tensor product is \(\mathcal{O}\left(L^{5}\right)\). While the Clebsch-Gordan tensor product is more computationally efficient than the irreducible Cartesian tensor product for \(L\rightarrow\infty\), the latter is expected to be advantageous for smaller tensor ranks, i.e., \(L\leq 4\), assuming similar multiplicative factors and negligible sub-leading terms. Tensors of rank \(L\leq 4\) are particularly relevant for equivariant message-passing architectures and representing physical properties [32; 33; 72]. Compared to the Gaunt tensor product with the computational complexity of \(\mathcal{O}\left(L^{3}\right)\), including all \((l_{1},l_{2})\to l_{3}\)[36], the irreducible Cartesian tensor product may be less advantageous for \(L\geq 3\). However, the Gaunt-coefficients-based approach excludes odd tensor products, limiting the expressive power and the range of possible architectures.

As regards the cost of performing message passing at each layer, the general neighbors' aggregation of Eq. (4) has a cost of \(\mathcal{E}N_{\mathrm{ch}}9^{L}L!/\left(2^{L/2}\left(L/2\right)!\right)\), where \(\mathcal{E}\) is the number of edges in the atomic system and \(N_{\mathrm{ch}}\) is the number of feature channels. Computing many-body features via Eq. (6) requires to iteratively perform \(\nu-1\) Cartesian tensor products for all \(\mathcal{K}=\mathrm{len}(\eta_{\nu})\) possible \(\nu\)-fold tensor products, resulting in a computational cost of \(\mathcal{MN}_{\mathrm{ch}}\mathcal{K}(9^{LD}l/(2^{L/2}(L/2)!))^{\nu-1}\) with \(\mathcal{M}\) denoting the number of nodes. For the Clebsch-Gordan tensor product, the corresponding number of calculations is \(\mathcal{EN}_{\mathrm{ch}}L^{5}\) and \(\mathcal{MN}_{\mathrm{ch}}\mathcal{K}L^{5(\nu-1)}\). Spherical models, however, can use generalized Clebsch-Gordan coefficients, resulting in \(\mathcal{MN}_{\mathrm{ch}}\mathcal{K}L^{\frac{1}{2}\nu(\nu+3)}\) for the product basis. We can remove the factor \(\mathcal{K}\) from the above expression by restricting the parameterization to uncoupled feature channels as in Eq. (A1) and obtain \(\mathcal{MN}_{\mathrm{ch}}L^{\frac{1}{2}\nu(\nu+3)}\). Thus, MACE with this specific choice of the product basis can be more computationally efficient than ICTP only for large \(N_{\mathrm{ch}}\) and small \(\nu\), given \(L\leq 4\). In this work, we have shown that leveraging the symmetry of the irreducible Cartesian tensor product and coupled feature channels can improve the computational cost of ICTP. The former, in particular, reduces the effective number of \(\nu\)-fold tensor products \(\mathcal{K}\). Further optimization of the architecture, however, is possible and is expected to fully exploit the advantage of Cartesian tensors.

Finally, memory consumption for Cartesian tensors is often believed to be less advantageous than that of spherical tensors. However, for contracting two rank-\(L\) spherical tensors, the memory requirement is about \((2L+1)^{2}\), resulting from the definition of the Clebsch-Gordan tensor product in Section 2. For a Cartesian tensor, the memory requirement is about \(3^{L}\). Thus, the irreducible Cartesian tensor product can be expected to require the same or less memory for \(L\leq 4\) than the Clebsch-Gordan counterpart. The memory consumption of models based on spherical tensors is further increased by the use of generalized Clebsch-Gordan coefficients, as we demonstrate in Section 5.

## Appendix C Proof of Cartesian message-passing equivariance to actions of the orthogonal group

We first recap the standard action of the \(\mathrm{O}(3)\) group onto \(\left(\mathbb{R}^{3}\right)^{\otimes l}\). Let \(D_{\mathcal{X}}\) be a representation of the orthogonal group \(\mathrm{O}(3)\) (also in line with Section 2), i.e.,

\[D_{\mathcal{X}}:\mathrm{O}(3)\to\mathbb{R}^{3\times 3},\quad g\mapsto D_{ \mathcal{X}}[g]=R.\]

We define the action of the \(\mathrm{O}(3)\) group onto \(\left(\mathbb{R}^{3}\right)^{\otimes l}\) as follows

\[\phi:\mathrm{O}(3)\times\left(\mathbb{R}^{3}\right)^{\otimes l}\to\left( \mathbb{R}^{3}\right)^{\otimes l},\quad\phi\left(g,\mathbf{T}_{l}\right)_{i_ {1}i_{2}\cdots i_{l}}:=\sum_{j_{1}}\cdots\sum_{j_{l}}R_{i_{1}j_{1}}\cdots R_{ i_{l}j_{l}}\left(\mathbf{T}_{l}\right)_{j_{1}\cdots j_{l}},\]

where \(\mathbf{T}_{l}\in\left(\mathbb{R}^{3}\right)^{\otimes l}\). We hereinafter denote \(\phi\left(g,\mathbf{T}_{l}\right)\) also by \(R\mathbf{T}_{l}\). The outer product \(\otimes\) is equivariant to this action, and \(R\) acts trivially on the \(3\times 3\)-identity matrix.

**Lemma C.1**.: _Let \(R\) be a representation of an element of the orthogonal group \(\mathrm{O}(3)\). Then,_

1. \(\forall\,\mathbf{T}_{l_{1}}\in\left(\mathbb{R}^{3}\right)^{\otimes l_{1}}\) _and_ \(\forall\,\mathbf{T}_{l_{2}}\in\left(\mathbb{R}^{3}\right)^{\otimes l_{2}}\)__ \[R\left(\mathbf{T}_{l_{1}}\otimes\mathbf{T}_{l_{2}}\right)=\left(R\mathbf{T}_{l_ {1}}\right)\otimes\left(R\mathbf{T}_{l_{2}}\right).\]
2. _For the_ \(3\times 3\)_-identity matrix_ \(\mathbf{I}\)_, we have_ \[R\mathbf{I}=\mathbf{I}.\]

Proof.: (i) We first show that \(R\left(\mathbf{T}_{l_{1}}\otimes\mathbf{T}_{l_{2}}\right)=\left(R\mathbf{T}_{l _{1}}\right)\otimes\left(R\mathbf{T}_{l_{2}}\right)\)

\[\left(\left(R\mathbf{T}_{l_{1}}\otimes\left(R\mathbf{T}_{l_{2}} \right)\right)_{i_{1}\cdots i_{l_{1}}i_{l_{1}+1}\cdots i_{l_{1}+l_{2}}}\right.\] \[\quad=\left(R\mathbf{T}_{l_{1}}\right)_{i_{1}\cdots i_{l_{1}}} \left(R\mathbf{T}_{l_{2}}\right)_{i_{1}\cdots i_{l_{1}+1}\cdots i_{l_{1}+l_{2}}}\] \[\quad=\left(\sum_{j_{1},\ldots,j_{l_{1}}}R_{i_{1}j_{1}}\cdots R_{ i_{l}j_{1}}\left(\mathbf{T}_{l_{1}}\right)_{j_{1}\cdots j_{l_{1}}}\right) \left(\sum_{j_{l_{1}+1}\cdots,j_{l_{1}+l_{2}}}R_{i_{l_{1}+1}j_{1}+1}\cdots R_{ i_{l_{1}+l_{2}}j_{1}+l_{2}}\left(\mathbf{T}_{l_{2}}\right)_{j_{1}+1 \cdots j_{l_{1}+l_{2}}}\right)\] \[\quad=\sum_{j_{1}}\cdots\sum_{j_{l_{1}}}\sum_{j_{l_{1}+1}}\cdots \sum_{j_{l_{1}+l_{2}}}R_{i_{1}j_{1}}\cdots R_{i_{l_{1}}j_{1}}R_{i_{1}+j_{1}+1 }\cdots R_{i_{l_{1}+l_{2}}j_{1}+l_{2}}\left(\mathbf{T}_{l_{1}}\right)_{j_{1} \cdots j_{l_{1}}}\left(\mathbf{T}_{l_{2}}\right)_{j_{1}+1\cdots j_{l_{1}+l_{2}}}\] \[\quad=\left(R\left(\mathbf{T}_{l_{1}}\otimes\mathbf{T}_{l_{2}} \right)\right)_{i_{1}\cdots i_{l_{1}}i_{l_{1}+1}\cdots i_{l_{1}+l_{2}}}.\](ii) With \(\sum_{i}R_{ij}R_{ik}=\delta_{jk}\), or equivalently \(R^{T}R=\mathbf{I}\), we get

\[(R\mathbf{I})_{i_{1}i_{2}}=\sum_{j_{1},j_{2}}R_{i_{1}j_{1}}R_{i_{2}j_{2}}\delta_ {j_{1}j_{2}}=\sum_{j_{1}}R_{i_{1}j_{1}}R_{i_{2}j_{1}}=\delta_{i_{1}i_{2}}.\]

Using Lemma C.1, we can show that the irreducible Cartesian tensor operator \(\mathbf{T}_{l}:\mathbb{R}^{3}\to(\mathbb{R}^{3})^{\otimes l}\) is equivariant to actions of the \(\mathrm{O}(3)\) group.

**Proposition C.2**.: _Let \(l\geq 0\) be a positive integer. Then, the irreducible Cartesian tensor operator \(\mathbf{T}_{l}:\mathbb{R}^{3}\to(\mathbb{R}^{3})^{\otimes l}\) is equivariant to actions of the \(\mathrm{O}(3)\) group._

Proof.: It suffices to show that the map \(\hat{\mathbf{r}}\mapsto\hat{\mathbf{r}}^{\otimes(l-2m)}\otimes\mathbf{I}^{ \otimes m}\) is equivariant to actions of the \(\mathrm{O}(3)\) group \(\forall\,l\geq 1\) and \(\forall\,m\leq\lfloor l/2\rfloor\) (\(m\geq 0\)).

\[(R\hat{\mathbf{r}})^{\otimes(l-2m)}\otimes\mathbf{I}^{\otimes m} \stackrel{{\text{Lemma C.1 (ii)}}}{{=}}(R\hat{\mathbf{r}})^{\otimes(l-2m)}\otimes(R\mathbf{I})^{\otimes m}\] \[\stackrel{{\text{Lemma C.1 (i)}}}{{=}}R\left(\hat{\mathbf{r}}^{\otimes(l-2m)}\otimes\mathbf{I}^{\otimes m }\right).\]

Our next claim is that for \(l_{3}\in\{|l_{1}-l_{2}|,|l_{1}-l_{2}|+1,\cdots,l_{1}+l_{2}\}\) the irreducible Cartesian tensor product \(\otimes_{\mathrm{Cart}}\) defined in Eqs. (2) and (3) is equivariant to actions of the \(\mathrm{O}(3)\) group, i.e., the following diagram commutes

The proof for this claim is very similar to Proposition C.2. The only difference is to show the equivariance for the \((k+m)\)-fold tensor contraction.

**Proposition C.3**.: _The irreducible Cartesian tensor product \(\otimes_{\mathrm{Cart}}:\left(\mathbb{R}^{3}\right)^{\otimes l_{1}}\times \left(\mathbb{R}^{3}\right)^{\otimes l_{2}}\to\left(\mathbb{R}^{3}\right)^{ \otimes l_{3}}\) makes the above diagram commute._

Proof.: It suffices to show that the \((k+m)\)-fold tensor contraction is equivariant to actions of the \(\mathrm{O}(3)\) group. For \(\mathbf{x}_{l_{1}}\in(\mathbb{R}^{3})^{\otimes l_{1}}\), \(\mathbf{y}_{l_{2}}\in(\mathbb{R}^{3})^{\otimes l_{2}}\), and \(R\) being a representation of an element of the orthogonal group \(\mathrm{O}(3)\), we can write

\[((R\mathbf{x}_{l_{1}})\cdot(s)\cdot(R\mathbf{y}_{l_{2}}))_{\beta _{1}\cdots\beta_{l_{1}-s}\delta_{1}\cdots\delta_{l_{2}-s}}\] \[\qquad=\sum_{\alpha_{1},\cdots,\alpha_{s}}\left(\sum_{\begin{subarray} {c}\gamma_{1},\cdots,\gamma_{s}\\ m_{1},\cdots,\eta_{1-s}\end{subarray}}R_{\alpha_{1}\gamma_{1}}\cdots R_{\alpha _{s}\gamma_{s}}R_{\beta_{1}\eta_{1}}\cdots R_{\beta_{l_{1}-s}\eta_{1-s}} \left(\mathbf{x}_{l_{1}}\right)_{\gamma_{1}\cdots\gamma_{s}\eta_{1}\cdots \eta_{1-s}}\right)\] \[\qquad\qquad\qquad\times\left(\sum_{\begin{subarray}{c}\tilde{ \gamma}_{1},\cdots,\tilde{\gamma}_{s}\\ \tilde{\eta}_{1},\cdots,\tilde{\eta}_{2-s}\end{subarray}}R_{\alpha_{1}\tilde{ \gamma}_{1}}\cdots R_{\alpha_{s}\tilde{\gamma}_{s}}R_{\delta_{1}\tilde{\eta}_{ 1}}\cdots R_{\delta_{l_{2}-s}\tilde{\eta}_{2-s}}\left(\mathbf{y}_{l_{2}} \right)_{\tilde{\gamma}_{1}\cdots\tilde{\gamma}_{s}\tilde{\eta}_{1}\cdots \tilde{\eta}_{2-s}}\right)\] \[= \left(\sum_{\eta_{1}\cdots\eta_{1-s}}R_{\beta_{1}\eta_{1}}\cdots R _{\beta_{l_{1}-s}\eta_{1-s}}\right)\left(\sum_{\tilde{\eta}_{1}\cdots\tilde{ \eta}_{2-s}}R_{\delta_{1}\tilde{\eta}_{1}}\cdots R_{\delta_{l_{2}-s}\tilde{ \eta}_{2-s}}\right)\] \[\qquad\qquad\times\sum_{\gamma_{1}\cdots\gamma_{s}}\left(\mathbf{ x}_{l_{1}}\right)_{\gamma_{1}\cdots\gamma_{s}\eta_{1}\cdots\eta_{1-s}}\left( \mathbf{y}_{l_{2}}\right)_{\gamma_{1}\cdots\gamma_{s}\tilde{\eta}_{1}\cdots \tilde{\eta}_{2-s}}\] \[= \left(R\left(\mathbf{x}_{l_{1}}\cdot(s)\cdot\mathbf{y}_{l_{2}} \right)\right)_{\beta_{1}\cdots\beta_{l_{1}-s}\delta_{1}\cdots\delta_{l_{2}-s}}.\]A similar derivation applies to the odd case (3), which completes the proof. 

We finally give a proof for Proposition 4.1, i.e., the equivariance of message-passing layers based on irreducible Cartesian tensors to actions of the \(\mathrm{O}(3)\) group.

Proof of Proposition 4.1.: Since a message-passing layer in Section 4.2 and Appendix B.2 is defined by stacking layers corresponding to Eqs. (4), (5), (6), (A1), (A2), and (A3), we show the equivariance for each of these equations. We show the equivariance by induction.

* Case \(t=1\): Equivariance of Eq. (5): Since learnable weights \(W_{kZ_{v}}\) and learnable (invariant) radial basis \(R^{(1)}_{kl_{1}}(r_{uv})\) are defined for each feature channel \(k\), the weights are multiplied with \(\big{(}\mathbf{T}_{l_{1}}(\hat{\mathbf{r}}_{uv})\big{)}_{i_{1}i_{2}\cdots i_{l _{1}}}\) as scalars. Therefore, \[\big{(}\mathbf{A}^{(1)}_{ukl_{1}}\big{)}_{i_{1}i_{2}\cdots i_{l _{1}}}=\sum\nolimits_{v\in\mathcal{N}(u)}R^{(1)}_{kl_{1}}(r_{uv})\big{(} \mathbf{T}_{l_{1}}(\hat{\mathbf{r}}_{uv})\big{)}_{i_{1}i_{2}\cdots i_{l_{1}}} W_{kZ_{v}}\] is equivariant to actions of the \(\mathrm{O}(3)\) group. Equivariance of Eq. (6): The proof is similar to the case of Eq. (5), since we again have learnable parameter \(W^{(1)}_{kk^{\prime}l_{\nu}}\) for each \(\mathbf{A}^{(1)}_{uk^{\prime}l_{\nu}}\). Therefore, noting that the Cartesian tensor product is equivariant to actions of the \(\mathrm{O}(3)\) group, the following function is also equivariant to actions of the \(\mathrm{O}(3)\) group. Equivariance of Eq. (A1): The equation defined by \[\big{(}\mathbf{m}^{(1)}_{ukL}\big{)}_{i_{1}i_{2}\cdots i_{L}}=\Big{(}\sum \limits_{\nu}\sum\limits_{\eta_{\nu}}W^{(1)}_{Z_{u}\eta_{\nu}kL}\mathbf{B}^{( 1)}_{u\eta_{\nu}kL}\Big{)}_{i_{1}i_{2}\cdots i_{L}},\] is equivariant to actions of the \(\mathrm{O}(3)\) group, since for each set of indices \(u\), \(\eta_{\nu}\), \(k\), and \(L\) we have a scalar learnable parameter \(W^{(1)}_{Z_{u}\eta_{\nu}kL}\). Equivariance of Eq. (A2): The respective parameters \(W^{(1)}_{kk^{\prime}L}\) and \(W^{(1)}_{Z_{u}kk^{\prime}L}\) in Eq. (A2) \[\big{(}\mathbf{h}^{(2)}_{ukL}\big{)}_{i_{1}i_{2}\cdots i_{L}}=\frac{1}{\sqrt{ d_{t}}}\sum\limits_{k^{\prime}}W^{(1)}_{kk^{\prime}L}\big{(}\mathbf{m}^{(1)}_{uk ^{\prime}L}\big{)}_{i_{1}i_{2}\cdots i_{L}}+\frac{1}{\sqrt{d_{t}N_{Z}}}\sum \limits_{k^{\prime}}W^{(1)}_{Z_{u}kk^{\prime}L}\big{(}\mathbf{h}^{(1)}_{uk^{ \prime}L}\big{)}_{i_{1}i_{2}\cdots i_{L}}.\] are multiplied as scalar with \(\mathbf{h}^{(1)}_{uk^{\prime}L}\) and \(\mathbf{m}^{(1)}_{uk^{\prime}L}\), which are shown to be equivariant to actions of the \(\mathrm{O}(3)\) group. Thus, \(\mathbf{h}^{(2)}_{ukL}\) is equivariant to actions of the \(\mathrm{O}(3)\) group as a function of \(\mathbf{h}^{(1)}_{uk^{\prime}L}\) and \(\mathbf{m}^{(1)}_{uk^{\prime}L}\). Equivariance of Eq. (A3): The equivariance of Eq. (A3) to actions of the \(\mathrm{O}(3)\) group is also immediate since learnable parameters apply to each tuple of \((i_{1},i_{2},\cdots,i_{L})\) as a scalar.
* General case \(t>1\): The equivariance of Eqs. (4), (5), (6), (A1), (A2), and (A3) to actions of the \(\mathrm{O}(3)\) group for arbitrary \(t>1\) follows in a similar way to those obtained for the \(t=1\) case. However, we need to show the equivariance of Eq. (4). Suppose that Eqs. (5), (6), (A1), (A2), and (A3) are equivariant to actions of the \(\mathrm{O}(3)\) group for \(t>1\). Then, for the \((t+1)\)-th message-passing layer, \[\big{(}\mathbf{A}^{(t)}_{ukl_{3}}\big{)}_{i_{1}i_{2}\cdots i_{l_{3}}}=\sum \nolimits_{v\in\mathcal{N}(u)}\Big{(}R^{(t)}_{kl_{1}l_{2}l_{3}}(r_{uv})\mathbf{T} _{l_{1}}(\hat{\mathbf{r}}_{uv})\otimes_{\mathrm{Cart}}\frac{1}{\sqrt{d_{t}}} \sum\limits_{k^{\prime}}W^{(t)}_{kk^{\prime}l_{2}}\mathbf{h}^{(t)}_{vk^{\prime} l_{2}}\Big{)}_{i_{1}i_{2}\cdots i_{l_{3}}},\] is equivariant to actions of the \(\mathrm{O}(3)\) group. Here, the irreducible Cartesian tensor product \(\otimes_{\mathrm{Cart}}\) and \(\mathbf{T}_{l_{1}}\) are equivariant to actions of the \(\mathrm{O}(3)\) group, \(\mathbf{h}^{(t)}_{vk^{\prime}l_{2}}\) is equivariant by the assumption, and \(W^{(t)}_{kk^{\prime}l_{2}}\) applies to \(\mathbf{h}^{(t)}_{vk^{\prime}l_{2}}\) as a scalar.

Proof of the traceless property for irreducible Cartesian tensors

This section provides a proof for Proposition 4.2 in the main text. The corresponding proposition states the following

**Proposition D.1**.: _The message-passing layers based on irreducible Cartesian tensors and their irreducible tensor products preserve the traceless property of irreducible Cartesian tensors._

Our proof of Proposition 4.2 comprises two main parts. First, we show that given two irreducible Cartesian tensors, the irreducible Cartesian tensor product yields again an irreducible Cartesian tensor. We then can use this result in the second part, where we show that expressions defined by Eqs. (4), (5), (6), (7), (8), and (8) preserve the traceless property of irreducible Cartesian tensors. The proof of the second part is straightforward since the message-passing layers are defined by multiplications with scalars, summations, and the irreducible Cartesian tensor product of tensors defined in Eqs. (1), (2), and (3). Therefore, we provide the proof only for the first part in the rest of this section.

**Irreducible Cartesian tensors from unit vectors.** Recall that an irreducible Cartesian tensor of arbitrary rank \(l\) for a unit vector \(\hat{\mathbf{r}}\in\mathbb{R}^{3}\) is defined as

\[\mathbf{T}_{l}\left(\hat{\mathbf{r}}\right)=C\sum\nolimits_{m=0}^{\lfloor l/2 \rfloor}(-1)^{m}\frac{(2l-2m-1)!!}{(2l-1)!!}\big{\{}\hat{\mathbf{r}}^{\otimes \left(l-2m\right)}\otimes\mathbf{I}^{\otimes m}\big{\}}.\]

The trace of this tensor reads

\[\operatorname{Tr}\left(\mathbf{T}_{l}\left(\hat{\mathbf{r}}\right)\right) =\sum\nolimits_{i_{1}=i_{2}}\left(\mathbf{T}_{l}\left(\hat{ \mathbf{r}}\right)\right)_{i_{1}i_{2}}\] \[=\sum\nolimits_{i_{1}=i_{2}}C\sum\nolimits_{m=0}^{\lfloor l/2 \rfloor}(-1)^{m}\,\frac{(2l-2m-1)!!}{(2l-1)!!}\left(\big{\{}\hat{\mathbf{r}}^{ \otimes\left(l-2m\right)}\otimes\mathbf{I}^{\otimes m}\big{\}}\right)_{i_{1}i_ {2}}\] \[=C\sum\nolimits_{m=0}^{\lfloor l/2\rfloor}(-1)^{m}\,\frac{(2l-2m- 1)!!}{(2l-1)!!}\sum\nolimits_{i_{1}=i_{2}}\left(\big{\{}\hat{\mathbf{r}}^{ \otimes\left(l-2m\right)}\otimes\mathbf{I}^{\otimes m}\big{\}}\right)_{i_{1}i_ {2}}.\]

Let \(\big{\{}k_{1},k_{2},\ldots,k_{l-2m}\big{\}}\) be a subset of \(\{1,2,\ldots,l\}\) with \(l-2m\) distinct elements, and a subset \(I_{l-2m}\) with \(l-2m\) distinct elements in \(\big{\{}i_{1},\ldots,i_{l}\big{\}}\) is written as \(I_{l-2m}=\big{\{}i_{k_{1}},i_{k_{2}},\ldots,i_{k_{l-2m}}\big{\}}\). We also let \(J_{l-2m}\) be a set of \(m\) disjoint subsets with \(2\) elements in \(\big{\{}i_{1},\ldots,i_{l}\big{\}}\backslash I_{l-2m}\), i.e., \(J_{l-2m}=\{\{i_{k_{l-2m+1}},i_{k_{l-2m+2}}\},\ldots,\{i_{k_{l-1}},i_{k_{l}}\}\}\). While we introduce \(I_{l-2m}\) and \(J_{l-2m}\) as sets, we assume the order of elements in those sets if the order is necessary and there is no confusion. Furthermore, we introduce two notations used throughout this section

\[\hat{\mathbf{r}}_{I_{l-2m}}^{\otimes\left(l-2m\right)} =\hat{r}_{i_{k_{1}}}\hat{r}_{i_{k_{2}}}\cdots\hat{r}_{i_{k_{l-2m}}}\] \[\mathbf{I}_{J_{l-2m}}^{\otimes m} =\delta_{i_{k_{l-2m+1}}i_{k_{l-2m+2}}}\cdots\delta_{i_{k_{l-1}}i_ {k_{l}}}.\]

Then, we can write

\[\big{\{}\hat{\mathbf{r}}^{\otimes\left(l-2m\right)}\otimes \mathbf{I}^{\otimes m}\big{\}} =\sum\nolimits_{I_{l-2m}}\sum\nolimits_{J_{l-2m}}\hat{\mathbf{r} }_{I_{l-2m}}^{\otimes\left(l-2m\right)}\otimes\mathbf{I}_{J_{l-2m}}^{\otimes m}\] \[=\sum\nolimits_{I_{l-2m}}\sum\nolimits_{J_{l-2m}}\hat{r}_{i_{k_{ 1}}}\cdots\hat{r}_{i_{k_{l-2m}}}\delta_{i_{k_{l-2m+1}}i_{k_{l-2m+2}}}\cdots \delta_{i_{k_{l-1}}i_{k_{l}}},\]

where \(I_{l-2m}\) runs over all subsets of \(\big{\{}i_{1},\ldots,i_{l}\big{\}}\) with \(l-2m\) elements and \(J_{l-2m}\) comprises all \(\left(\prod_{p=m}^{1}\binom{2p}{2}\right)/m!\) combinations of \(m\) subsets with \(2\) elements of \(\big{\{}i_{1},\ldots,i_{l}\big{\}}\backslash I_{l-2m}\). Depending on whether \(i_{1}\) and/or \(i_{2}\) belong to \(I_{l-2m}\), the above expression may be further reduced to

\[\left(\sum\limits_{i_{1},i_{2}\in I_{l-2m}}^{\left(a\right)_{m}} +\sum\limits_{\begin{subarray}{c}i_{1}\in I_{l-2m}\\ i_{2}\in J_{l-2m}\end{subarray}}^{\left(b\right)_{m}}+\sum\limits_{ \begin{subarray}{c}i_{2}\in I_{l-2m}\\ i_{1}\in J_{l-2m}\end{subarray}}^{\left(c\right)_{m}}+\sum\limits_{ \begin{subarray}{c}i_{1},i_{2}\notin I_{l-2m}\\ \delta_{i_{1}i_{2}}\end{subarray}}^{\left(c\right)_{m}}\right)\hat{r}_{i_{k_{1}}} \cdots\hat{r}_{i_{k_{l-2m}}}\] \[\times\delta_{i_{k_{l-2m+1}}i_{k_{l-2m+2}}}\cdots\delta_{i_{k_{l-1 }}i_{k_{l}}}.\]We get the following traces for each pair of \(I_{l-2m}\) and \(J_{l-2m}\)

\[\operatorname{Tr}\left(\left(a\right)_{m}\right) =\hat{\mathbf{r}}_{I_{l-2m}\setminus\left\{i_{1},i_{2}\right\}}^{ \otimes\left(l-2m\right)}\otimes\mathbf{I}_{I_{l-2m}}^{\otimes m},\] \[\operatorname{Tr}\left(\left(b\right)_{m}\right) =\sum\nolimits_{i_{1}=i_{2}}\hat{r}_{i_{k_{1}}}\cdots\hat{r}_{i_ {1}}\cdots\hat{r}_{i_{l-2m}}\delta_{i_{k_{l-2m+1}}i_{k_{l-2m+2}}}\cdots\delta_ {i_{k_{l-1}}i_{k_{l}}}\delta_{i_{k_{x}}i_{2}}\] \[=\hat{r}_{i_{k_{1}}}\cdots\hat{r}_{i_{k_{x}}}\cdots\hat{r}_{i_{k_ {l-2m}}}\delta_{i_{k_{l-2m+1}}i_{k_{l-2m+2}}}\cdots\delta_{i_{k_{l-1}}i_{k_{l}}}\] \[=\hat{\mathbf{r}}_{\left\{i_{k_{x}}\right\}\cup I_{l-2m}\setminus \left\{i_{1}\right\}}^{\otimes\left(m-1\right)}\otimes\mathbf{I}_{J_{l-2m} \setminus\left\{i_{k_{x}},i_{2}\right\}}^{\otimes\left(m-1\right)},\] \[\operatorname{Tr}\left(\left(c\right)_{m}\right) =\sum\nolimits_{i_{1}=i_{2}}\hat{r}_{i_{k_{1}}}\cdots\hat{r}_{i_ {2}}\cdots\hat{r}_{i_{k_{l-2m}}}\delta_{i_{k_{l-2m+1}}i_{k_{l-2m+2}}}\cdots \delta_{i_{k_{l-1}}i_{k_{l}}}\delta_{i_{k_{x}}i_{1}}\] \[=\hat{r}_{i_{k_{1}}}\cdots\hat{r}_{i_{k_{x}}}\cdots\hat{r}_{i_{k_ {l-2m}}}\delta_{i_{k_{l-2m+1}}i_{k_{l-2m+2}}}\cdots\delta_{i_{k_{l-1}}i_{k_{l}}}\] \[=\hat{\mathbf{r}}_{\left\{i_{k_{x}}\right\}\cup I_{l-2m}\setminus \left\{i_{2}\right\}}^{\otimes\left(m-1\right)}\otimes\mathbf{I}_{J_{l-2m} \setminus\left\{i_{k_{x}},i_{1}\right\}}^{\otimes\left(m-1\right)},\] \[\operatorname{Tr}\left(\left(d\right)_{m}\right) =\sum\nolimits_{i_{1}=i_{2}}\hat{r}_{i_{k_{1}}}\cdots\hat{r}_{i_ {k_{l-2m}}}\delta_{i_{k_{l-2m+1}}i_{k_{l-2m+1}}i_{k_{l-2m+2}}}\cdots\delta_{i_ {k_{l-1}}i_{k_{l}}}\delta_{i_{1}i_{2}}\] \[=3\left(\hat{\mathbf{r}}_{I_{l-2m}}^{\otimes\left(l-2m\right)} \otimes\mathbf{I}_{J_{l-2m}\setminus\left\{i_{1},i_{2}\right\}}^{\otimes\left( m-1\right)}\right),\] \[\operatorname{Tr}\left(\left(e\right)_{m}\right) =\sum\nolimits_{i_{1}=i_{2}}\hat{r}_{i_{k_{1}}}\cdots\hat{r}_{i_ {k_{l-2m}}}\delta_{i_{k_{l-2m+1}}i_{k_{l-2m+2}}}\cdots\delta_{i_{k}i_{1}} \delta_{i_{2}i_{k}}\cdots\delta_{i_{k_{l-1}}i_{k_{l}}}\] \[=\hat{\mathbf{r}}_{I_{l-2m}}^{\otimes\left(l-2m\right)}\otimes \mathbf{I}_{J_{l-2m}\setminus\left\{i_{1},i_{2}\right\}}^{\otimes\left(m-1 \right)}.\]

**Lemma D.2**.: _Let \(l\geq 2\) and \(0\leq m\leq\lfloor l/2\rfloor-1\). For each pair \(\left(I_{l-2m},J_{l-2m}\right)\), there exist \(2l-2m-1\) pairs of \(\left(I_{l-2(m+1)},J_{l-2(m+1)}\right)\) whose trace equals to the trace for the pair \(\left(I_{l-2m},J_{l-2m}\right)\), i.e.,_

\[\left(2l-2m-1\right)\operatorname{Tr}\left(\hat{\mathbf{r}}_{I_{l-2m}}^{ \otimes\left(l-2m\right)}\otimes\mathbf{I}_{J_{l-2m}}^{\otimes m}\right)= \operatorname{Tr}\left(\sum\nolimits_{\left(I_{l-2(m+1)},J_{l-2(m+1)}\right) }\hat{\mathbf{r}}_{I_{l-2(m+1)}}^{\otimes\left(l-2(m+1)\right)}\otimes \mathbf{I}_{J_{l-2(m+1)}}^{\otimes\left(m+1\right)}\right).\]

Proof.: Without loss of generality, we may assume \(I_{l-2m}=\left(i_{1},i_{2},i_{k_{3}},\ldots,i_{k_{l-2m}}\right)\) and \(J_{l-2m}=\left\{\left\{i_{k_{l-2m+1}},i_{k_{l-2m+2}}\right\},\ldots,\left\{i _{k_{l-1}},i_{k_{l}}\right\}\right\}\). For \(\forall\)\(m\), \(\operatorname{Tr}\left(\left(a\right)_{m}\right)\) has the following expression

\[\operatorname{Tr}\left(\left(a\right)_{m}\right) =\sum\nolimits_{i_{1}=i_{2}}\hat{r}_{i_{1}}\hat{r}_{i_{2}}\hat{r}_{ i_{k_{3}}}\cdots\hat{r}_{i_{k_{l-2m}}}\delta_{i_{k_{l-2m+1}}i_{k_{l-2m+2}}} \cdots\delta_{i_{k_{l-1}}i_{k_{l}}}\] \[=\hat{r}_{i_{k_{3}}}\cdots\hat{r}_{i_{k_{l-2m}}}\delta_{i_{k_{l-2m+ 1}}i_{k_{l-2m+2}}}\cdots\delta_{i_{k_{l-1}}i_{k_{l}}}.\]

For \(\operatorname{Tr}\left(\left(b\right)_{m+1}\right)\) and \(m+1\), we have a set of \(\left(I_{l-2(m+1)},J_{l-2(m+1)}\right)\) with the cardinality of \(l-2m-2\) and the corresponding trace equals to \(\operatorname{Tr}\left(\left(a\right)_{m}\right)\). For \(3\leq\forall\)\(s\leq l-2m\), let

\[I_{l-2(m+1)}^{\left(s\right)} =\left\{i_{k_{3}},\ldots,\underbrace{i_{1}}_{\text{s-th index}}, \ldots,i_{k_{l-2m}}\right\},\] \[J_{l-2(m+1)}^{\left(s\right)} =\left\{\left\{i_{k_{l-2m+1}},i_{k_{l-2m+2}}\right\},\ldots, \left\{i_{k_{l-1}},i_{k_{l}}\right\},\left\{i_{k_{x}},i_{2}\right\}\right\}.\]

Here, we note that we assume the order of elements in \(I_{l-2(m+1)}^{\left(s\right)}\). Then, we obtain

\[\operatorname{Tr}\left(\left(b\right)_{m+1}\right) =\sum\nolimits_{i_{1}=i_{2}}\sum\nolimits_{s=3}^{l-2m}\hat{r} _{i_{k_{3}}}\cdots\underbrace{\hat{r}_{i_{k_{1}}}}_{\text{s-th vector}}\cdots\hat{r}_{i_{k_{l-2m}}} \delta_{i_{k_{l-2m+1}}i_{k_{l-2m+2}}}\cdots\delta_{i_{k_{l-1}}i_{k_{l}}}\delta_{ i_{k_{x}}i_{2}}\] \[=\sum\nolimits_{s=3}^{l-2m}\hat{r}_{i_{k_{3}}}\cdots\hat{r}_{i_{k_ {x}}}\cdots\hat{r}_{i_{k_{l-2m}}}\delta_{i_{k_{l-2m+1}}i_{k_{l-2m+2}}}\cdots \delta_{i_{k_{l-1}}i_{k_{l}}}\] \[=\left(l-2m-2\right)\operatorname{Tr}\left(\left(a\right)_{m} \right).\]

The same derivation as above holds for \(\operatorname{Tr}\left(\left(c\right)_{m+1}\right)\). For \(\operatorname{Tr}\left(\left(d\right)_{m+1}\right)\), let

\[I_{l-2(m+1)} =\left\{i_{k_{3}},\ldots,i_{k_{l-2m}}\right\},\] \[J_{l-2(m+1)} =\left\{\left\{i_{k_{l-2m+1}},i_{k_{l-2m+2}}\right\},\ldots, \left\{i_{k_{l-1}},i_{k_{l}}\right\},\left\{i_{1},i_{2}\right\}\right\}.\]

[MISSING_PAGE_FAIL:25]

In the following, we let

\[I_{l_{1}-(k+m)} =\big{\{}i_{k_{1}},\ldots,i_{k_{l_{1}-(k+m)}}\big{\}}\subset\big{\{}i _{1},\ldots,i_{l_{3}}\big{\}},\] \[J_{l_{2}-(k+m)} =\big{\{}i_{k_{l_{1}-(k+m)+1}},\ldots,i_{k_{l_{3}-2m}}\big{\}} \subset\big{\{}i_{1},\ldots,i_{l_{3}}\big{\}}\backslash I_{l_{1}-(k+m)},\] \[K_{l_{3}-2m} =\big{\{}\big{\{}i_{k_{l_{3}-2m+1}},i_{k_{l_{3}-2m+2}}\big{\}}, \ldots,\big{\{}i_{k_{l_{3}-1}},i_{k_{l_{3}}}\big{\}}\big{\}}.\]

For simplicity, we omit the subscripts \(l_{1}-(k+m)\), \(l_{2}-(k+m)\), and \(l_{3}-2m\) for \(I\), \(J\), and \(K\), if there is no confusion. We also introduce a new notation \(\mathbf{x}_{j_{1}\cdots j_{p},Q}=\mathbf{x}_{j_{1}\cdots j_{p},q_{1}\cdots q _{u}}\), where \(\mathbf{x}\) is an irreducible Cartesian tensor and \(Q=\big{\{}q_{1},\ldots,q_{u}\big{\}}\), which is used throughout this section. Then, non-trivial terms in \((*)_{\text{even}}\) may be written as

\[\sum_{i_{1}=i_{2}}\left(\sum_{\begin{subarray}{c}i_{1}\in I\\ i_{2}\in J\end{subarray}}^{(a)_{m}}+\sum_{\begin{subarray}{c}i_{2}\in I\\ i_{1}\in J\end{subarray}}^{(b)_{m}}+\sum_{\begin{subarray}{c}i_{1}\in I,J \\ i_{2}\in K\end{subarray}}^{(c)_{m}}+\sum_{\begin{subarray}{c}i_{2}\in I,U \\ i_{1}\in K\end{subarray}}^{(d)_{m}}+\sum_{\begin{subarray}{c}i_{1},i_{2}\in K \end{subarray}}^{(e)_{m}}\right)\sum_{j_{1},\ldots,j_{k+m}}\left(\mathbf{x}_{ l_{1}}\right)_{j_{1}\cdots j_{k+m},I}\left(\mathbf{y}_{l_{2}}\right)_{j_{1} \cdots j_{k+m},J}\mathbf{I}_{K}^{\otimes m}.\]

We note that the trace is trivial when both of \(i_{1}\) and \(i_{2}\) belong to either of \(I\) or \(J\), because \(\mathbf{x}_{l_{1}}\) and \(\mathbf{y}_{l_{2}}\) are traceless by assumption.

**Lemma D.3**.: _Let \(0\leq m\leq\min(l_{1},l_{2})-k-1\). For each triplet \(\big{(}I_{l_{1}-(k+m)},J_{l_{2}-(k+m)},K_{l_{3}-2m}\big{)}\), there exist \(2l_{3}-2m-1\) triplets of \(\big{(}I_{l_{1}-(k+m+1)},J_{l_{2}-(k+m+1)},K_{l_{3}-2(m+1)}\big{)}\) such that the trace for each triplet equals to the trace for \(\big{(}I_{l_{1}-(k+m)},J_{l_{2}-(k+m)},K_{l_{3}-2m}\big{)}\)._

Proof.: Without loss of generality, we may assume

\[I_{l_{1}-(k+m)} =\big{\{}i_{1},i_{k_{2}},\ldots,i_{k_{l_{1}-(k+m)}}\big{\}} \subset\big{\{}i_{1},\ldots,i_{l_{3}}\big{\}},\] \[J_{l_{2}-(k+m)} =\big{\{}i_{2},i_{k_{l_{1}-(k+m)+2}},\ldots,i_{k_{l_{3}-2m}} \big{\}}\subset\big{\{}i_{1},\ldots,i_{l_{3}}\big{\}}\backslash I_{l_{1}-(k+m )},\] \[K_{l_{3}-2m} =\big{\{}\big{\{}i_{k_{l_{3}-2m+1}},i_{k_{l_{3}-2m+2}}\big{\}}, \ldots,\big{\{}i_{k_{l_{3}-1}},i_{k_{l_{3}}}\big{\}}\big{\}}.\]

Then, \(\operatorname{Tr}\left((a)_{m}\right)\) has the following expression

\[\operatorname{Tr}\left((a)_{m}\right)\] \[=\sum_{\begin{subarray}{c}i_{1}=i_{2}\,j_{1},\ldots,j_{k+m}\\ j_{k+m+1}\end{subarray}}\left(\mathbf{x}_{l_{1}}\right)_{j_{1}\cdots j_{k+m+1}, i_{k_{2}}\cdots i_{k_{l_{1}-(k+m)}}}\left(\mathbf{y}_{l_{2}}\right)_{j_{1} \cdots j_{k+m+1},i_{k_{l_{1}-(k+m)+2}}\cdots i_{k_{l_{3}-2m}}}\mathbf{I}_{K}^{ \otimes m}.\]

The same derivation applies to \(\operatorname{Tr}\left((b)_{m}\right)\), and it has the same result as above.

For \(\operatorname{Tr}((c)_{m+1})\) and \(m+1\), we have \(l_{3}-2m-2\) of \(\big{(}I_{l_{1}-(k+m+1)},J_{l_{2}-(k+m+1)},K_{l_{3}-2(k+m+1)}\big{)}\) triplets whose trace equals to \(\operatorname{Tr}\left((a)_{m}\right)\). Indeed, like in the proof of Lemma D.2, we let

\[I_{l_{1}-(k+m+1)}\cup J_{l_{2}-(k+m+1)} =\big{\{}i_{k_{2}},\ldots,i_{1},\ldots,i_{k_{l_{1}-(k+m)}},i_{k_ {l_{1}-(k+m)+2}},\ldots,i_{k_{l_{3}-2m}}\big{\}},\] \[K_{l_{3}-2(m+1)} =\big{\{}\big{\{}i_{k_{l_{3}-2m+1}},i_{k_{l_{3}-2m+2}}\big{\}}, \ldots,\big{\{}i_{k_{l_{3}-1}},i_{k_{l_{3}}}\big{\}},\big{\{}i_{k_{s}},i_{2} \big{\}}\big{\}}.\]

Then, by letting \(\tilde{J}=\big{\{}j_{1},\ldots,j_{k+m},j_{k+m+1}\big{\}}\), we obtain

\[\operatorname{Tr}((c)_{m+1})\] \[=\sum_{i_{1}=i_{2}}\,\sum_{s}\sum_{J}\left(\mathbf{x}_{l_{1}} \right)_{J_{i_{k_{2}}}\cdots i_{k_{l_{1}-(k+m)}}}\left(\mathbf{y}_{l_{2}} \right)_{J_{i_{k_{l_{1}-(k+m)+2}}\cdots i_{k_{l_{3}-2m}}}}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\times\delta_{i_{k_{l_{3}-2m+1}},i_{k_ {l_{3}-2m+2}}}\cdots\delta_{i_{k_{l_{3}-1}},i_{k_{l_{3}}}}\delta_{i_{k_{s}}i_{2}}\] \[=\sum_{s}\sum_{J}\left(\mathbf{x}_{l_{1}}\right)_{J_{i_{k_{2}}} \cdots i_{k_{s}}\cdots i_{k_{l_{1}-(k+m)}}}\left(\mathbf{y}_{l_{2}}\right)_{J,i_{k_{l_{1}-(k+m)+2}}\cdots i_{k_{l_{3}-2m}}}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\times\delta_{i_{k_{l_{3}-2m+1}},i_{k_{l_{3}-2m+2}}} \cdots\delta_{i_{k_{l_{3}-1}},i_{k_{l_{3}}}}\] \[=\left(l_{3}-2m-2\right)\operatorname{Tr}\left((a)_{m}\right).\]

[MISSING_PAGE_FAIL:27]

Note that the trace is trivial when \(i_{1}\) or \(i_{2}\) belongs to \(E\) and the remaining index belongs to \(I\cup J\). Indeed, let \(\tilde{J}=\left\{j_{1},\ldots,j_{k+m}\right\}\) and suppose \(i_{1}\in E\) and \(i_{2}\in I\). Then, and we have

\[\left(\operatorname{Tr}\left(\left(\mathbf{x}_{l_{1}}\otimes_{ \operatorname{Cart}}\mathbf{y}_{l_{2}}\right)_{l_{3}}\right)\right)_{(I\cup J )\setminus\{i_{2}\}}\] \[\quad=\sum_{i_{1}=i_{2}}\left(\sum_{\tilde{J}}\sum_{a_{2},a_{3}} \boldsymbol{\varepsilon}_{i_{1}a_{2}a_{3}}\left(\mathbf{x}_{l_{1}}\right)_{ \tilde{J},a_{2},i_{2},I\setminus\{i_{2}\}}\left(\mathbf{y}_{l_{2}}\right)_{ \tilde{J},a_{3},J}\right)\] \[\quad=\sum_{\tilde{J}}\left(\sum_{i_{1}=i_{2}}\sum_{a_{2}<a_{3}} \boldsymbol{\varepsilon}_{i_{1}a_{2}a_{3}}\left(\left(\mathbf{x}_{l_{1}} \right)_{\tilde{J},a_{2},i_{2},I\setminus\{i_{2}\}}\left(\mathbf{y}_{l_{2}} \right)_{\tilde{J},a_{3},J}-\left(\mathbf{x}_{l_{1}}\right)_{\tilde{J},a_{3},i _{2},I\setminus\{i_{2}\}}\left(\mathbf{y}_{l_{2}}\right)_{\tilde{J},a_{2},J} \right)\right)\] \[\quad=\sum_{\tilde{J}}\left(\sum_{\begin{subarray}{c}(i_{1},a_{2},a_{3})=\\ (1,2,3)\\ (2,1,3)\\ (2,1,3)\end{subarray}}\boldsymbol{\varepsilon}_{i_{1}a_{2}a_{3}}\left(\left( \mathbf{x}_{l_{1}}\right)_{\tilde{J},a_{2},i_{1},I\setminus\{i_{1}\}}\left( \mathbf{y}_{l_{2}}\right)_{\tilde{J},a_{3},J}-\left(\mathbf{x}_{l_{1}}\right)_ {\tilde{J},a_{3},i_{1},I\setminus\{i_{1}\}}\left(\mathbf{y}_{l_{2}}\right)_{ \tilde{J},a_{2},J}\right)\right)\] \[\quad=0.\]

For the non-trivial terms inside \((*)_{\text{odd}}\) we have the lemma below, whose proof is identical to that of Lemma D.3.

**Lemma D.4**.: _Let \(0\leq m\leq\min(l_{1},l_{2})-k-2\). For each tuple \(\left(E_{l_{3}},I_{l_{1}-(k+m)},J_{l_{2}-(k+m)},K_{l_{3}-2m}\right)\), there exist \(2l_{3}-2m-1\) tuples of \(\left(E_{l_{3}},I_{l_{1}-(k+m+1)},J_{l_{2}-(k+m+1)},K_{l_{3}-2(m+1)}\right)\) such that the trace for each of the tuples equals to the trace for \(\left(E_{l_{3}},I_{l_{1}-(k+m)},J_{l_{2}-(k+m)},K_{l_{3}-2m}\right)\)._

Proof.: Without loss of generality, we may assume

\[E_{l_{3}} =\left\{i_{k_{0}}\right\}\subset\left\{i_{1},\ldots,i_{l_{3}}\right\}\] \[I_{l_{1}-(k+m)} =\left\{i_{1},i_{k_{2}},\ldots,i_{k_{l_{1}-(k+m)}}\right\}\subset \left\{i_{1},\ldots,i_{l_{3}}\right\}\backslash E_{l_{3}},\] \[J_{l_{2}-(k+m)} =\left\{i_{2},i_{k_{l_{1}-(k+m)+2}},\ldots,i_{k_{l_{3}-2m}}\right\} \subset\left\{i_{1},\ldots,i_{l_{3}}\right\}\backslash\left(I_{l_{1}-(k+m)} \cup E_{l_{3}}\right),\] \[K_{l_{3}-2m} =\left\{\left\{i_{k_{l_{3}-2m+1}},i_{k_{l_{3}-2m+2}}\right\}, \ldots,\left\{i_{k_{l_{3}-1}},i_{k_{l_{3}}}\right\}\right\}.\]

Then, \(\operatorname{Tr}\left(\left(a\right)_{m}\right)\) has the following expression

\[\operatorname{Tr}\left(\left(a\right)_{m}\right)\] \[=\sum_{i_{1}=i_{2}}\sum_{a_{2},a_{3},j_{1},\ldots,j_{k+m}} \boldsymbol{\varepsilon}_{i_{k_{0}}a_{2}a_{3}}\left(\mathbf{x}_{l_{1}}\right) _{j_{1}\cdots j_{k+m},a_{2}i_{1}i_{k_{2}}\cdots i_{l_{1}-(k+m)}}\left(\mathbf{ y}_{l_{2}}\right)_{j_{1}\cdots j_{k+m},a_{3}i_{2}i_{k_{l_{1}-(k+m)+3}}\cdots i _{l_{3}-2m}}\mathbf{T}_{K}^{\otimes m}\] \[=\sum_{a_{2},a_{3},j_{1},\ldots,j_{k+m+1}}\boldsymbol{ \varepsilon}_{i_{k_{0}}a_{2}a_{3}}\left(\mathbf{x}_{l_{1}}\right)_{j_{1} \cdots j_{k+m+1},a_{2}i_{k_{3}}\cdots i_{l_{1}-(k+m)}}\left(\mathbf{y}_{l_{2} }\right)_{j_{1}\cdots j_{k+m+1},a_{3}i_{k_{l_{1}-(k+m)+3}}\cdots i_{l_{3}-2m}} \mathbf{T}_{K}^{\otimes m}.\]

Here, we assume that \(a_{2}\in I_{l_{1}-(k+m)}\) and \(a_{3}\in J_{l_{2}-(k+m)}\). The same derivation holds for \(\operatorname{Tr}\left(\left(b\right)_{m}\right)\).

For \(\operatorname{Tr}((c)_{m+1})\) and \(m+1\), we have \(l_{3}-2m-2\) of \(\left(E_{l_{3}},I_{l_{1}-(k+m+1)},J_{l_{2}-(k+m+1)},K_{l_{3}-2(m+1)}\right)\) triplets whose trace equals to \(\operatorname{Tr}\left((a)_{m}\right)\). Indeed, like in the proof of Lemma D.2, we let

\[E_{l_{3}}\cup I_{l_{1}-(k+m+1)}\cup J_{l_{2}-(k+m+1)} =\left\{i_{k_{0}},i_{k_{2}},\ldots,i_{1},\ldots,i_{k_{l_{1}-(k+m)} },i_{k_{l_{1}-(k+m)+2}},\ldots,i_{k_{l_{3}-2m}}\right\},\] \[K_{l_{3}-2(m+1)} =\left\{\left\{i_{k_{l_{3}-2m+1}},i_{k_{l_{3}-2m+2}}\right\}, \ldots,\left\{i_{k_{l_{3}-1}},i_{k_{l_{3}}}\right\},\left\{i_{k_{l_{s}}},i_{2} \right\}\right\}.\]Then, by letting \(\bar{J}=\left\{j_{1},\ldots,j_{k+m},j_{k+m+1}\right\}\), we obtain

\[\operatorname{Tr}((c)_{m+1})\] \[=\sum_{i_{1}=i_{2}}\sum_{s}\sum_{a_{2},a_{3}}\sum_{\bar{J}} \boldsymbol{\varepsilon}_{i_{k_{0}}a_{2}a_{3}}\left(\mathbf{x}_{l_{1}}\right) _{\bar{J}a_{2}i_{k_{2}}\cdots i_{k_{1}}\cdots i_{k_{1}-(k+m)}}\left(\mathbf{y }_{l_{2}}\right)_{\bar{J}a_{3}i_{k_{1}-(k+m)+3}\cdots i_{k_{1}-2m}}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\times\delta_{i_{k_{1}}_{3-2m+1}i_{k_{1}-2m+2}}\cdots\delta_{i_{k_{1}}_{ 3-1}i_{k_{1}}}\delta_{i_{k_{2}}i_{2}}\] \[=\sum_{s}\sum_{a_{2},a_{3}}\sum_{\bar{J}}\boldsymbol{\varepsilon }_{i_{k_{0}}a_{2}a_{3}}\left(\mathbf{x}_{l_{1}}\right)_{\bar{J}a_{2}i_{k_{2}} \cdots i_{k_{s}}\cdots i_{k_{1}-(k+m)}}\left(\mathbf{y}_{l_{2}}\right)_{\bar{ J}a_{3}i_{k_{1}-(k+m)+3}\cdots i_{k_{1}-2m}}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\times\delta_{i_{k_{1}}_{3-2m+1 }i_{k_{1}-2m+2}}\cdots\delta_{i_{k_{1}}_{3-1}i_{k_{1}}}\] \[=\left(l_{3}-2m-2\right)\operatorname{Tr}\left((a)_{m}\right).\]

The same derivation applies to \(\operatorname{Tr}((d)_{m+1})\), and we get \(\operatorname{Tr}((d)_{m+1})=\left(l_{3}-2m-2\right)\operatorname{Tr}\left(( a)_{m}\right)\).

For \(\operatorname{Tr}((e)_{m+1})\), the same derivation as for \(\operatorname{Tr}((d)_{m+1})\) and \(\operatorname{Tr}((e)_{m+1})\) in the proof of Lemma D.2 applies. Therefore, we obtain

\[\operatorname{Tr}((e)_{m+1})=\left(2m+3\right)\operatorname{Tr}\left((a)_{m} \right).\]

Finally, we can write

\[\operatorname{Tr}((c)_{m+1})+\operatorname{Tr}((d)_{m+1})+ \operatorname{Tr}((e)_{m+1})\] \[=\left(l_{3}-2m-2\right)\operatorname{Tr}((a)_{m})+\left(l_{3}-2 m-2\right)\operatorname{Tr}((a)_{m})+\left(2m+3\right)\operatorname{Tr}((a)_{m})\] \[=\left(2l_{3}-2m-1\right)\operatorname{Tr}((a)_{m})=\left(2l_{3}- 2m-1\right)\operatorname{Tr}((b)_{m}),\]

which completes the proof of the Lemma. 

Noting that \(\operatorname{Tr}\left((a)_{m}\right)\) is doubled for every \(m\) due to the factor \(2^{m}\), the proof is completed similarly to the proof of Lemma D.3. This result completes the proof of the traceless property for Eq. (3).

## Appendix E Experiments and results

### Description of the data sets

**rMD17 data set.** The revised MD17 (rMD17) data set is a collection of structures, energies, and atomic forces of ten small organic molecules obtained from ab initio molecular dynamics (AIMD) [47]. These molecules are derived from the original MD17 data set [84, 85, 47, 86], with 100,000 structures sampled for each. Our models are trained using 950 and 50 configurations for each molecule randomly sampled from the original data set using five random seeds, with 50 additional configurations randomly sampled for early stopping. We use the remaining configurations to test the final models. Table 1 reports the mean absolute errors (MAEs) in total energies and atomic forces averaged over five independent runs, including the standard deviation between them.

**MD22 data set.** The MD22 data set includes structures, energies, and atomic forces of seven molecular systems derived from AIMD simulations at elevated temperatures, spanning four major classes of biomolecules and supramolecules [48]. These molecular systems range from a small peptide containing 42 atoms to a double-walled nanotube comprising 370 atoms. Characterized by complex intermolecular interactions, this data set was designed to challenge short-range models. The training set sizes used in this study are consistent with those in the original publication [48], selected to ensure that the sGDML model stays within a target accuracy of 1 kcal/mol (\(\approx 43.37\) meV). We randomly selected an additional set of 500 structures for each molecule in the data set for early stopping, while the remaining configurations were reserved for testing the final models. The corresponding training and validation data sets were randomly selected using three random seeds. Table A2 reports MAEs in total energies per atom and atomic forces averaged over three independent runs, including the standard deviation between them.

**3BPA data set.** The 3BPA data set comprises structures, energies, and atomic forces of a flexible drug-like organic molecule obtained from AIMD at various temperatures [49]. The training data set consists of 500 configurations sampled at 300 K, while three separate test data sets are obtained from AIMD simulations at 300 K, 600 K, and 1200 K. An additional test data set provides energy values along dihedral rotations of the molecule. This test directly assesses the smoothness and accuracy of the potential energy surface, influencing properties such as binding free energies to protein targets. Our models are trained using 450 and 50 configurations randomly sampled from the training data set using five random seeds, with further 50 configurations reserved for early stopping. Table 2 reports the root-mean-square errors (RMSEs) in total energies and atomic forces averaged over five independent runs for a training set size of 450 structures, including the standard deviation between them. Table A3 presents the corresponding results obtained with a training set size of 50 structures.

**Acetylacetone data set.** The acetylacetone data set includes structures, energies, and atomic forces of a small reactive molecule obtained from AIMD at various temperatures [32]. The training data set comprises configurations sampled at 300 K, while the test data sets are sampled at 300 K and 600 K. The generalization ability of final models is evaluated using an elevated temperature of 600 K and along two internal coordinates of the molecule: The hydrogen transfer path and a partially conjugated double bond rotation featuring a high rotation barrier. Our models are trained using 450 and 50 configurations randomly sampled from the training dataset using five random seeds, with further 50 configurations reserved for early stopping. Table 3 reports RMSEs in total energies and atomic forces averaged over five independent runs for a training set size of 450 structures, including the standard deviation between them. Table A5 presents the corresponding results obtained with a training set size of 50 structures.

**Ta-V-Cr-W data set.** The Ta-V-Cr-W data set includes 0 K energies, atomic forces, and stresses for binaries, ternaries, and quaternary and near-melting temperature properties in four-component disordered high-entropy alloys [50]. In total, this benchmark data set comprises 6711 configurations, with energies, atomic forces, and stresses computed at the density functional theory (DFT) level. More precisely, there are 5680 0 K structures: 4491 binary, 595 ternary, and 594 quaternary structures, along with 1031 structures sampled from MD at 2500 K. Structure sizes range from 2 to 432 atoms in the periodic cell. All models are trained using 5373 configurations, with 4873 used for training and 500 for early stopping. The remaining configurations are reserved for testing the models' performance. The performance is evaluated separately using 0 K binaries, ternaries, quaternaries, and near-melting temperature four-component disordered alloys. The original data set provides ten different training-test splits. In our experiments, each training set is further split into training and validation subsets using a random seed. Furthermore, we use additional binary structures strained along the \([100]\) direction as a part of the test data set. Note that the final models, in this case, were obtained by training using the whole data set of 6711 configurations (training + test), 500 of which were reserved as a validation data set. Table A6 reports RMSEs in total energies per atom and atomic forces averaged over ten independent runs, including the standard deviation between them.

### Training details

All ICTP and MACE models employed in this work were trained on a single NVIDIA A100 GPU with 80 GB of RAM. Training times for ICTP and MACE models typically ranged from 30 minutes to a few days, depending on the data set, the data set size, and the employed precision. We used double precision for 3BPA and acetylacetone data sets and single precision for rMD17, in line with the original experiments [33]. Double precision was also used for MD22, while single precision was employed in our Ta-V-Cr-W experiments. Unless stated otherwise, we used two message-passing layers and irreducible Cartesian tensors or spherical tensors of a maximal rank of \(l_{\mathrm{max}}=3\) to embed the directional information of atomic distance vectors.

For ICTP models with the full (ICTP\({}_{\mathrm{full}}\)) and symmetric (ICTP\({}_{\mathrm{sym}}\)) product basis and MACE, we employ 256 uncoupled feature channels. Exceptions include our experiments with the 3BPA data set, aimed at investigating scaling and computational cost, and the Ta-V-Cr-W experiments, where we used eight and 32 feature channels, respectively. For ICTP models with the symmetric product basis evaluated in the latent feature space (ICTP\({}_{\mathrm{sym+lt}}\)), we use 64 coupled feature channels for the Cartesian product basis and 256 for two-body features. Radial features are derived from eight Bessel basis functions with polynomial envelope for the cutoff with \(p=5\)[60]. These features are fed into a fully connected NN of size \([64,64,64]\). We apply SiLU non-linearities to the outputs of the hidden layers [87; 88]. The readout function of the first message-passing layer is implemented as a linear layer. The readout function of the second layer is a single-layer fully connected NN with 16 hidden neurons. A cutoff radius of 5.0 A is used across all data sets except MD22, where we used a cutoff radius of 5.5 A for the double-walled nanotube and 6.0 A for the other molecules in the data set.

All parameters of ICTP and MACE models were optimized by minimizing the combined squared loss on training data \(\mathcal{D}_{\mathrm{train}}=(\mathcal{X}_{\mathrm{train}},\mathcal{Y}_{ \mathrm{train}})\), where \(\mathcal{X}_{\mathrm{train}}=\{\mathcal{S}^{(k)}\}_{k=1}^{N_{\mathrm{train}}}\) and \(\mathcal{Y}_{\mathrm{train}}=\{E_{k}^{\mathrm{ref}},\{\mathbf{F}_{u,k}^{ \mathrm{ref}}\}_{u=1}^{N_{\mathrm{att}}},\boldsymbol{\sigma}_{k}^{\mathrm{ref} }\}_{k=1}^{N_{\mathrm{train}}}\)

\[\mathcal{L}\left(\boldsymbol{\theta},\mathcal{D}_{\mathrm{train}} \right)=\sum_{k=1}^{N_{\mathrm{train}}}\Bigg{[}C_{\mathrm{e}}\Big{\|}E_{k}^{ \mathrm{ref}}-E(S^{(k)},\boldsymbol{\theta})\Big{\|}_{2}^{2} +C_{\mathrm{f}}\sum_{u=1}^{N_{\mathrm{att}}^{(k)}}\Big{\|}\mathbf{ F}_{u,k}^{\mathrm{ref}}-\mathbf{F}_{u}\left(S^{(k)},\boldsymbol{\theta}\right) \Big{\|}_{2}^{2}\] (A5)

Here, \(\boldsymbol{\sigma}_{k}^{\mathrm{ref}}\) is the stress tensor defined as \(\boldsymbol{\sigma}=\frac{1}{V}\ \nabla_{\boldsymbol{\epsilon}}E_{\big{|} \boldsymbol{\epsilon}=\boldsymbol{0}}\), where \(E\) denotes total energy after a strain deformation with symmetric tensor \(\boldsymbol{\epsilon}\in\mathbb{R}^{3\times 3}\) and \(V\) is the volume of the periodic box.

When training ICTP models on rMD17, 3BPA, and acetylacetone data sets, we neglected the stress loss and set \(C_{\mathrm{e}}=1/N_{\mathrm{att}}^{(k)}\) and \(C_{\mathrm{f}}=10\ \text{\AA}^{2}\) to balance the relative contributions of total energies and atomic forces, respectively. For MACE and 3BPA/acetylacetone, \(C_{\mathrm{e}}=1/(B\times N_{\mathrm{att}}^{(k)})\) and \(C_{\mathrm{f}}=1000/(B\times 3\times N_{\mathrm{att}}^{(k)})\ \text{\AA}^{2}\) were used with \(B\) denoting the batch size. For ICTP models trained on the MD22 data set, we set \(C_{\mathrm{e}}=10/N_{\mathrm{att}}^{(k)}\) and \(C_{\mathrm{f}}=1\ \text{\AA}^{2}\), using energies and forces in eV and eV/A, respectively. For the Ta-V-Cr-W dataset, the stress loss was incorporated into the combined loss in Eq. (A5), along with the energy and force losses. For ICTP, we used \(C_{\mathrm{e}}=1/N_{\mathrm{att}}^{(k)}\), \(C_{\mathrm{f}}=0.01\ \text{\AA}^{2}\), and \(C_{\mathrm{s}}=0.001/N_{\mathrm{att}}^{(k)}\) to balance the relative contributions of total energies, atomic forces, and stresses, respectively. For MACE, we chose \(C_{\mathrm{e}}=1/(B\times N_{\mathrm{att}}^{(k)}\times N_{\mathrm{att}}^{(k)})\), \(C_{\mathrm{f}}=1/(B\times 3\times N_{\mathrm{att}}^{(k)})\ \text{\AA}^{2}\), and \(C_{\mathrm{s}}=0.05/(B\times 9\times N_{\mathrm{att}}^{(k)}\times N_{\mathrm{att}}^{(k)})\). Here, \(E(\mathcal{S}^{(k)},\boldsymbol{\theta})\), \(\mathbf{F}_{u}\left(\mathcal{S}^{(k)},\boldsymbol{\theta}\right)\), and \(\boldsymbol{\sigma}\left(S^{(k)},\boldsymbol{\theta}\right)\) are total energies, atomic forces, and stresses predicted by ICTP or MACE.

All models for rMD17, 3BPA, and acetylacetone were trained for 2000 epochs using the AMSGrad variant of Adam [89], with default parameters of \(\beta_{1}=0.9\), \(\beta_{2}=0.999\), and \(\varepsilon=10^{-8}\). For MD22 and Ta-V-Cr-W, all models were trained for 1000 epochs. For rMD17, 3BPA, and acetylacetone data sets, we used a learning rate of 0.01 and a batch size of 5. For MD22 and Ta-V-Cr-W, we again chose a learning rate of 0.01 but a mini-batch of 2 and 32, respectively. For evaluations on the validation and test data sets, we used a batch size of 10 for rMD17, 3BPA, and acetylacetone, while mini-batches of 2 and 32 were used for MD22 and Ta-V-Cr-W, respectively.

The learning rate was reduced using an on-plateau scheduler based on the validation loss with a patience of 50 and a decay factor of 0.8 for all data sets except for MD22, for which we used a patience of 10. We utilize an exponential moving average with a weight of 0.99 for evaluation on the validation set and for the final model. Additionally, in line with MACE [33], we apply exponential weight decay of \(5\times 10^{-7}\) on the weights of Eqs. (6), (A1), and (A3). Furthermore, we incorporate a per-atom shift of total energies via the average per-atom energy over all the training configurations, including the energies of individual atoms for 3BPA and acetylacetone datasets. If no atomic energies are provided, as for rMD17, MD22, and Ta-V-Cr-W, the per-atom shift is obtained by solving a linear regression problem [67]. Additionally, a per-atom scale is determined as the root-mean-square of the components of the forces over the training configurations.

### Additional results

**Scaling and computational cost.** Table A1 provides the numerical results complementing Fig. 2 in the main text. All results in Table A1 and Fig. 2 were obtained using irreducible Cartesian tensors with a maximal rank of \(l_{\mathrm{max}}=L\) to represent local atomic environments. Here, \(L\) denotes the tensor rank of employed equivariant messages. We facilitated the exploration of larger \(\nu\) values by setting the number of uncoupled feature channels to eight. In the MACE model, intermediate spherical tensors with a rank of \(l>l_{\mathrm{max}}\) are used to construct the product basis. However, the pre-computation of generalized Clebsch-Gordan coefficients for \(\nu>4\), in some cases, would require more than 2 TB of RAM. Therefore, in our experiments, we fixed the maximum rank of intermediate tensors to \(l=l_{\mathrm{max}}\). We also used the full product basis for ICTP to calculate the same number of \(\nu\)-fold tensor products, i.e., \(\mathcal{K}=\mathrm{len}\left(\eta_{\nu}\right)\), as used in MACE with \(l=l_{\mathrm{max}}\). Finally, we note that ICTP and MACE use different approaches to optimize their runtimes; however, the scaling with respect to the tensor rank and the correlation order is independent of these optimization methods.

**Molecular dynamics trajectories.** Table A2 presents the energy and force mean absolute errors (MAEs) for the ICTP models trained on the MD22 data set. This data set was designed to challenge short-range potential models--i.e., those based on local or semi-local atomic representations--and includes large molecular systems with complex intermolecular interactions. We compare errors obtained for ICTP models with those of state-of-the-art approaches that incorporate global, short- and long-range information. From Table A2, we see that ICTP achieves accuracy in predicted energies and atomic forces on par with or better than state-of-the-art methods. The ICTP model uses a cutoff of 6.0 A (5.5 A for the double-walled nanotube), resulting in a receptive field of 12.0 A (11.0 A), considering the two message-passing layers. This receptive field is, in most cases, smaller than the diameter of molecular systems in the data set. Thus, ICTP is, at most, a semi-local potential model--similar to MACE, which used a 5.0 A cutoff and two message-passing layers.

We chose a larger cutoff for ICTP than the one used by MACE motivated by the results in the recent work [7]. In particular, the authors compared MACE to VisNet-LSRM--the best model reported to date for MD22, which employs mixed short- and long-range information in their message passing--and reported that MACE can achieve lower force errors. However, VisNet-LSRM typically had lower energy errors, attributed to the improvement from considering atomic interactions beyond 10.0 A. Using a larger cutoff radius for ICTP compared to MACE, we expected results closer to those of VisNet-LSRM. Table A2 shows that using a receptive field of 12.0 A is, in most cases, sufficient to achieve accuracy in predicted energies close to the one obtained by VisNet-LSRM.

**Extrapolation to out-of-domain data.** Table A3 demonstrates total energy and atomic force RMSEs obtained for ICTP and MACE models trained using 50 configurations randomly drawn from the original data set. ICTP and MACE models perform similarly, considering the standard deviation obtained across five independent runs. However, ICTP models often have lower mean RMSE values compared to MACE. Furthermore, Table A4 presents the total energy and atomic force RMSEs for ICTP and MACE models that use \(\nu=1\) to compute the product basis. Thus, we provide results for models which rely exclusively on two-body interactions. We note that for \(\nu=1\), ICTP fulfill and ICTP sym are identical; though, we include both results for completeness.

Figure A1 compares potential energy profiles obtained with ICTP and MACE models trained using 450 configurations. Potential energy cuts at \(\beta=120^{\circ}\) and \(\beta=180^{\circ}\) are easier tasks for MLIPs, as there are training points in the data set with similar combinations of dihedral angles [32]. In contrast, the potential energy cut at \(\beta=150^{\circ}\) is more challenging, with no training points close to it. Notably, Fig. A1 shows that all models produce smooth potential energy profiles close to the reference ones (DFT) for all values of \(\beta\). These results again demonstrate excellent extrapolation capabilities of irreducible Cartesian models that are on par with the spherical MACE model.

**Flexibility and reactivity.** Table A5 demonstrates total energy and atomic force RMSEs obtained for ICTP and MACE models trained using 50 configurations randomly drawn from the original data set.

Similar to the 3BPA data set, ICTP and MACE models demonstrate comparable accuracy in predicted energies and forces, considering the standard deviation obtained across five independent runs.

Figure A2 further investigates the generalization capabilities of ICTP models trained using 450 configurations, demonstrating potential energy profiles for the rotation around the C-C bond, i.e., the C-C-C-O dihedral angle (\(\alpha\)), and for the hydrogen transfer (i.e., the O-H distance \(d_{\mathrm{OH}}\) in Fig. A2). The training data set encompasses dihedral angles less than \(30^{\circ}\). Furthermore, the energy barrier of 1 eV is outside the energy range of the training data set obtained at 300 K. As for the hydrogen transfer, the training data does not contain transition geometries, but the reaction still occurs in a region that is not too far from the training data. Overall, ICTP models perform on par with MACE for predicting potential energy profiles for the rotation around the corresponding C-C bond and for the hydrogen transfer.

Figure A3 shows potential energy profiles for MLIPs trained with 50 configurations, similar to Fig. A2. Notably, Fig. A2 (b) demonstrates that ICTP\({}_{\text{full}}\) is the only MLIP consistently producing the potential energy profile for the hydrogen transfer close to the reference (DFT). This task is particularly challenging, as most data splits do not include configurations sufficiently close to the transition structure as in Fig. A2.

**Multicomponent alloys.** The Ta-V-Cr-W data set is designed to evaluate the performance of MLIPs across atomic systems with varying numbers of atom types/components, comprising both relaxed (0 K) and high-temperature structures [50]. In particular, this data set includes 0 K energies, forces, and stresses for 2-, 3-, and 4-component systems and 2500 K properties in 4-component disordered alloys. It contains 6711 configurations with sizes ranging from two to 432 atoms in the periodic cell. Table A6 demonstrates the energy and force RMSEs for the ICTP and MACE models, evaluated separately on 0 K binaries, ternaries, quaternaries, and near-melting temperature four-component disordered alloys. ICTP consistently outperforms state-of-the-art models for the Ta-V-Cr-W data set, i.e., MTP and GM-NN. In contrast, for MACE, we were not able to identify a set of relative weights for energy, forces, and virial losses that consistently yield results better than those of MTP and GM-NN in both energies and forces. Table A6 shows that MACE often matches the accuracy of ICTP on forces but is typically outperformed by a factor of \(\leq\) 2.0 on energies.

We further compare the ICTP, MACE, MTP, and GM-NN models using the separate test data set containing binary structures strained along the \([100]\) direction. We find that neither ICTP nor MACE consistently outperforms MTP and GM-NN in this case. However, because this test data set contains only a single configuration of 432 atoms per binary, it may not serve as a valuable benchmark in this study and is included merely for completeness. Additionally, we trained a Cartesian model with \(\nu=2\) and \(L=l_{\text{max}}=2\), which resembles a TensorNet-like architecture [30], though it incorporates equivariant convolution filters. The corresponding results are provided in Table A6. We found that model configurations with \(\nu=3\) and \(L=2\) (and often with \(L=1\)) outperform the \(\nu=2\) and \(L=l_{\text{max}}=2\) configuration by factors of \(\leq\) 1.4 and \(\leq\) 1.2 in energy and force RMSEs, respectively.

Finally, similar to our results for the 3BPA data set, we observe that MACE can be more computationally efficient than ICTP. We attribute longer inference times of ICTP to the pre-factor \(\mathcal{K}\) arising from the Cartesian product basis in Eq. (6). Thus, we attribute ICTP's lower computational efficiency to the use of the MACE architecture, which is optimized for spherical tensors. We expect that further modifications to the architecture can facilitate more efficient use of the advantages of operations based on irreducible Cartesian tensors.

## Appendix F Broader social impact

This section discusses the broader social impact of the presented work. Our work has important implications for the chemical sciences and engineering, as many problems in these fields require atomistic simulations; we also discuss it in Section 1. Although this work focuses on standard benchmark data sets, our experiments demonstrate the scalability of our method to larger atomic systems. Beyond constructing machine-learned interatomic potentials, equivariant models based on irreducible Cartesian tensors can be applied for molecular property prediction, protein structure prediction, protein generation, ribonucleic acid structure ranking, and many more.

Our work has no obvious negative social impact. As long as it is applied to the chemical sciences and engineering in a way that benefits society, it will have positive effects.

[MISSING_PAGE_FAIL:37]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction include the presented work's main contributions, assumptions, and results. Each of these claims are backed up through theoretical and empirical results in the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of the presented work in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide the proof of the equivariance and traceless property of MPNNs based on irreducible Cartesian tensors in Appendices C and D. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe the entire experimental setting and all details on the performed experimens in Section 5 and Appendix E. We also provide a detailed description of MPNNs employed in this work, including the weight initialization; see Section 4.2 and Appendix B.2. Furthermore, we describe the construction of irreducible Cartesian tensor products and introduce the irreducible Cartesian tensor products in Section 4.1. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The source code is available on GitHub and can be accessed via this link: https://github.com/nec-research/ictp. All data sets used in this study are publicly available: rMD17 (https://doi.org/10.6084/m9.figshare.12672038.v3), MD22 (http://www.sgdml.org), 3BPA (https://github.com/davkovacs/BOTNet-datasets), acetylacetone (https://github.com/davkovacs/BOTNet-datasets), and Ta-V-Cr-W (https://doi.org/10.18419/darus-3516). Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The details on the experimental setting are provided in Section 5 and Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We run all experiments using several random seeds and provide the corresponding standard deviations and error bars in all tables and figures.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details on the computer resources in Section 5 and Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The presented work follows the NeurIPS Code of Ethics. We also include Appendix F, which discusses the broader social impact. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [Yes] Justification: We include Appendix F, which discusses the broader social impact. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not release any data or models with a high risk for misuse. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We reference all employed data sets and source codes in the main text. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide a detailed description of the construction of irreducible Cartesian tensors, computation of their irreducible tensor products, and the resulting equivariant message-passing layers; see Section 4 and Section B. We also provide training details in Appendix E.2 and describe the employed data sets in greater detail in Appendix E.1. The source code includes comprehensive documentation as well.

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We performed neither crowdsourcing nor research with human subjects. * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We performed neither crowdsourcing nor research with human subjects. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.