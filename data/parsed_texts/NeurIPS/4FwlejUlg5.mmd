# Causal Effect Estimation with Mixed Latent Confounders and Post-treatment Variables

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Causal inference from observational data has attracted considerable attention among researchers. One main obstacle is the handling of confounders. As direct measurement of confounders may not be feasible, recent methods seek to address the confounding bias via proxy variables, i.e., covariates postulated to be conducive to the inference of latent confounders. However, the selected proxies may scramble both confounders and post-treatment variables in practice, which risks biasing the estimation by controlling for variables affected by the treatment. In this paper, we systematically investigate the bias due to latent post-treatment variables, i.e., _latent post-treatment bias_, in causal effect estimation. Specifically, we first derive the bias when selected proxies scramble both confounders and post-treatment variables, which we demonstrate can be arbitrarily bad. We then propose a novel Confounder-identifiable VAE (CiVAE) to address the bias. Based on a mild assumption that the prior of latent variables that generate the proxy belongs to a general exponential family with at least one invertible sufficient statistic in the factorized part, CiVAE _individually_ identifies latent confounders and latent post-treatment variables up to bijective transformations. We then prove that with individual identification, the intractable disentanglement problem of latent confounders and post-treatment variables can be transformed into a tractable independence test problem. Finally, we prove that the true causal effects can be unbiasedly estimated with transformed confounders inferred by CiVAE. Experiments on both simulated and real-world datasets demonstrate significantly improved robustness of CiVAE.

## 1 Introduction

Causal inference, which aims to infer cause-and-effect relations from data, has gained increasing prominence in various fields, such as social science, economics, and public health [10; 17; 34]. Traditional methods rely on the golden standard of randomized control trials (RCT) to draw valid causal conclusions via experimentation [6]. Recently, more attention has been dedicated to causal inference from observational data, where treatments, outcomes, and unit features are passively observed, and researchers have no control over the treatment assignment mechanism [36; 37; 40].

One main obstacle to inferring valid causal relations from observational data is the confounding bias, which occurs when we fail to account for the systematic difference between the treatment and non-treatment group due to variables that causally influence the past treatments and the outcome, i.e., unobserved confounders [16]. If the confounders can be measured, a simple strategy to address the bias is to control them via covariate adjustment [33] or propensity score re-weighting [24]. However, confounders are not always measurable [23]. Therefore, recent methods seek to adjust for the influence of unobserved confounders based on their proxies, which are easily acquirable covariates postulated to be causally related with the unobserved confounders [29; 42; 28]. One exemplar workis the causal effect variational auto-encoder (CEVAE) [25], which has demonstrated that confounding bias can be mitigated by controlling latent variables inferred from the proxies of confounders.

Although proxy-based methods have achieved substantial progress in recent years, they may risk controlling latent post-treatment variables scrambled in the proxies, where **latent post-treatment bias** can be introduced. Here, we note that the negative effects of controlling _observed_ post-treatment variables have been investigated in prior research [1; 9; 21]. For example, Montgomery et al. [30] found that more than 50% of the papers published in top journals of politics _inadvertently control post-treatment variables_ in the experimental setting, even though researchers have complete control over which covariates to control for. On this basis, we postulate that the post-treatment bias could be even worse for proxy-based methods in the setting of observational study where variables are passively recorded. In addition, the post-treatment variables can be **latent** and scrambled into the observed covariates together with the latent confounders, which makes them difficult to disentangle.

Consider a real-world example from the Company1. We found that _changing_ a job from onsite to online mode causes applicants to make different decisions, and we want to estimate the causal effects of _switching a job from onsite to online mode_ to _the decisions of the applicants_ (reflected by statistics of applicants that apply for the job). In this case, the Company collected two groups of online (treated) and onsite (control) jobs, where the statistics of the applicants (e.g., the average age) are calculated as the surrogate outcome. Clearly, job seniority is a confounder, since less senior jobs are more likely to permit online work, and applicants for these jobs tend to be younger. However, the seniority level of a job can be difficult to measure. Therefore, the required skills of the job can be used as the proxy of the confounder "seniority", as senior jobs tend to require more advanced skills. However, **a caveat** is that switching to an online work mode may also alter the required skills of a job, thereby affecting the qualification and, therefore, the decision of the applicants. Consequently, directly using the skills as the proxy of the confounder "seniority" for adjustment could unintentionally control latent mediators (changed skills), which introduces latent post-treatment bias in the causal effect estimation.

Footnote 1: Anonymized due to double-blind review policy.

Addressing the **latent post-treatment bias** faces multi-faceted challenges. First, there lacks a theoretical formulation of the bias when selected proxies scramble latent post-treatment variables for existing proxy-based methods. In addition, it is difficult to distinguish confounders and post-treatment variables in the latent space due to their similar observed behaviors. Existing covariate disentanglement-based methods, e.g., TEDVAE [44], focus on an easier task of disentangling latent confounders with latent adjusters and instrumental variables, which can be achieved by leveraging their different predictive abilities w.r.t. the treatment and outcome. However, since both latent confounders and post-treatment variables correlate with the treatment and the outcome, they cannot be disentangled by these methods. Finally, even if latent confounders can be distinguished from post-treatment variables, since most existing latent variable models have no identifiability guarantee [19], it is unclear whether controlling the inferred latent variables, which may be arbitrary transformations of the true confounders, can provide unbiased estimations of true causal effects.

To address the aforementioned challenges, we first analyze existing proxy-based methods when selected proxies scramble both latent confounders and post-treatment variables and show the estimation can be arbitrarily biased. We then propose a novel Confounder-identifiable VAE (CiVAE) to address the latent post-treatment bias. Specifically, we prove that based on a mild assumption that the prior of latent variables that generate the observed proxy (i.e., the latent confounders and post-treatment variables) belong to a general exponential family with at least one invertible sufficient statistic in the factorized part, latent confounders and latent post-treatment variables can be _individually_ identified up to _simple bijective transformations_. With such identifiability guarantee, based on the causal relations among confounders, mediators, and treatment, we further demonstrate that the inferred confounders

Figure 1: Comparison between the causal models assumed by CEVAE, TEDVAE, and CiVAE.

(which are actually transformed proxies of the true confounders) could be properly distinguished from the latent post-treatment variables with pair-wise conditional independence tests. Finally, we prove that the true causal effects can be unbiasedly estimated based on transformed confounders inferred by CiVAE. Experiments on both simulated and real-world datasets demonstrate that CiVAE shows more robustness to latent post-treatment bias than existing methods.

## 2 Problem Formulation

In this paper, we assume the causal model in Fig. 1-(c). We use a binary random variable \(T\) to denote the treatment, a random vector \(\bm{X}\in\mathbb{R}^{K_{X}}\) to denote the observed covariates (i.e., the proxy), and a random scalar \(Y\in\mathbb{R}\) to denote the outcome. Furthermore, the observed covariates \(X\) are assumed to be generated from \(K_{C}\) independent latent confounders \(\bm{C}\triangleq[C_{1},C_{2}...,C_{K_{C}}]\) causally influencing both \(T\) and \(Y\), and \(K_{M}\) latent post-treatment variables \(\bm{M}\triangleq[M_{1},M_{2}...,M_{K_{M}}]\) under the causal influence of the treatment (where the relation between \(\bm{M}\) and \(Y\) can be arbitrary). We use the random vector \(\bm{Z}\triangleq[\bm{C}]|\bm{M}|\in\mathbb{R}^{K_{Z}=K_{C}+K_{M}}\) to denote all latent factors. **Our aim** is to estimate the average causal effects of treatment \(T\) on outcome \(Y\) with auxiliary confounder information in \(\bm{X}\), where the estimation should be devoid of both confounding bias and post-treatment bias.

## 3 Theoretical Analysis of Latent Post-Treatment Bias

### Preliminaries and Assumptions

To achieve such a purpose, we first define the (conditional) average treatment effects (C/ATE) when covariates \(\bm{X}\) scramble both latent confounders \(\bm{C}\) and post-treatment variables \(\bm{M}\). We then define the post-treatment bias when covariates \(\bm{X}\) are directly used as the proxy of confounders. To facilitate the analysis, we make the following assumption regarding the causal generative process.

**Assumption 1**.: _(**Noisy-Injectivity**). We assume \(\bm{X}=f(\bm{C},\bm{M})+\bm{\epsilon}\), where \(f\) is a deterministic function that combines latent confounders \(\bm{C}\) and latent post-treatment variables \(\bm{M}\) into observations \(\bm{X}\), and \(\bm{\epsilon}\) is random noise. In addition, we assume that the function \(f\) is **injective**; beyond injectivity**, \(f\) can be arbitrarily nonlinear. We use \(f^{\dagger}:\bm{X}\rightarrow[\bm{C}||\bm{M}]\) to denote its left inverse. We use \(f^{\dagger}_{C}:\bm{X}\rightarrow\bm{C}\) and \(f^{\dagger}_{M}:\bm{X}\rightarrow\bm{M}\) to denote the mapping from \(\bm{X}\) to \(\bm{C}\), \(\bm{M}\), respectively._

_Noisy-Injectivity_ is a common assumption made either explicitly or implicitly in most existing proxy-of-confounder-based causal inference algorithms. For example, if both \(\bm{X}\) and \(\bm{C}\) are categorical, [31] assumes that \(\bm{X}\) has at least the same number of categories as \(\bm{C}\), whereas the effect restoration algorithm [35] assumes that the matrix of \(p(\bm{C},\bm{X})\) to be full-rank. Although CEVAE [25] makes no explicit injectivity assumption between \(\bm{C}\) and \(\bm{X}\), it requires that the joint distribution \(p(\bm{C},\bm{X},T,Y)\) can be fully recovered from the observations \((\bm{X},T,Y)\). [2] show that some of the possible identification criteria for the recovery include **1)** having multiple independent views of \(\bm{C}\) in \(\bm{X}\)[8], and **2)**\(\bm{C}\) is categorical and \(\bm{X}\) is a mixture of Gaussian components determined by \(\bm{C}\) (that is, \(\bm{X}\) is generated by bijective mapping of \(\bm{C}\) to the mean of the corresponding component with added Gaussian noise).

In the following part of this section, we omit the noise \(\bm{\epsilon}\) to gain better intuition of latent post-treatment bias (but all the exact conclusions will still hold in the posterior sense [19]). In Section 4, we assume noise exists and demonstrate that our method can still properly identify the latent confounders.

### Causal Estimand and the True ATE

Based on Assumption 1, we are ready to define the estimand of average treatment effect (ATE) through controlling the covariates \(\bm{X}^{\prime}\), as well the as the true (conditional) average treatment effects.

**Definition 1**.: _(**DCEV & DEV**). We define the Difference in Conditional Expected Values (DCEV) as:_

\[DCEV(\bm{x}^{\prime})=\mathbb{E}[Y|T=1,\bm{X}^{\prime}=\bm{x}^{\prime}]-\mathbb{ E}[Y|T=0,\bm{X}^{\prime}=\bm{x}^{\prime}],\] (1)

_which is the difference of the expected value of \(Y\) for units with variable \(\bm{X}^{\prime}=\bm{x}^{\prime}\) in the treatment group and the non-treatment group. Based on \(DCEV(\bm{x}^{\prime})\), we define the Difference in Expected Value (DEV) as \(DEV(\bm{X}^{\prime})=\mathbb{E}_{p(\bm{X}^{\prime})}[DCEV(\bm{X}^{\prime})]\) as the expectation of \(DCEV\) w.r.t. \(p(X^{\prime})\)._\(DEV(\bm{X}^{\prime})\) denotes the estimand of ATE when \(\bm{X}^{\prime}\) is the covariates that we choose to control (i.e., calculate the expected difference in each stratum of \(\bm{X}^{\prime}=\bm{x}^{\prime}\)). If \(\bm{X}^{\prime}=\emptyset\), \(DEV(\emptyset)\) represents the _naive estimator_ that directly calculates the expected difference of the outcome \(Y\) between the treatment group and the non-treatment group. With the causal estimand \(DEV(\bm{X}^{\prime})\) defined, we then derive the true causal effects with the covariates \(\bm{X}^{\prime}\) when it scrambles both latent confounders and post-treatment variables according to the generative process described in Assumption 1:

**Definition 2**.: _Under Assumption 1, we define the Conditional Average Treatment Effect (CATE) for individuals with observed covariates \(\bm{X}=\bm{x}\) by controlling only the confounder part in \(\bm{X}\) as:_

\[CATE(\bm{x})=\mathbb{E}[Y|T=1,\bm{C}=f_{C}^{\dagger}(\bm{x})]-\mathbb{E}[Y|T=0, \bm{C}=f_{C}^{\dagger}(\bm{x})],\] (2)

_with the Average Treatment Effect (ATE) of treatment \(T\) defined as:_

\[ATE=\mathbb{E}[Y|do(T=1)]-\mathbb{E}[Y|do(T=0)]=\mathbb{E}_{p(\bm{C})}[ \mathbb{E}[Y|T=1,\bm{C}]-\mathbb{E}[Y|T=0,\bm{C}]].\] (3)

Please note that we only consider the latent confounder component of the observed features \(\bm{X}\) in the definition of CATE in Eq. (2). This is because the causal relationship between the post-treatment variables \(\bm{M}\) and the outcome \(Y\) is indeterminate. However, if the specific relationship between \(\bm{M}\) and \(Y\) can be further established by the researcher (e.g., all elements of \(\bm{M}\) are latent mediators), more precise forms of CATE can be derived with path-specific counterfactual analysis [5; 14].

### Latent Post-Treatment Bias

With \(DEV(\bm{X}^{\prime})\) (the ATE estimator that control for the covariates \(\bm{X}^{\prime}\)), CATE, and ATE defined in Section 3.2, in this section, we analyze the _latent post-treatment bias_ of existing proxy-of-confounder-based causal inference methods, such as CEVAE, that control for latent variables inferred from the covariates \(\bm{X}\) to estimate the ATE of \(T\) on \(Y\), when \(\bm{X}\) scrambles both latent confounders and post-treatment variables as Assumption 1. In our analysis, Lemma 3.1 will be frequently used.

**Lemma 3.1**.: _For an injective function \(g\), \(\mathbb{E}[Y|\bm{X}^{\prime}=\bm{x}^{\prime}]=\mathbb{E}[Y|g(\bm{X}^{\prime}) =g(\bm{x}^{\prime})]\) holds._

The proof when \(g\) is differentiable _a.e._ can be referred to in Appendix C.1. Since the latent variable models used in existing methods (such as VAE with factorized Gaussian prior in CEVAE) lack identifiability guarantee (i.e., the recovery of the exact latent variables), we assume that these models can recover the true latent space \(\bm{Z}=[\bm{C},\bm{M}]\) up to invertible transformations \(\tilde{f}\), where the inference process can be represented as \(\hat{\bm{Z}}=f(\bm{X})=\bar{f}\circ f^{\dagger}(\bm{X})\). With such an assumption, we have the following theorem regarding the latent post-treatment bias when \(\bm{X}\) mixes post-treatment variables.

**Theorem 3.2**.: _If the observed covariates \(\bm{X}\) are generated from latent confounders \(\bm{C}\) and latent post-treatment variables \(\bm{M}\) according to Assumption 1, the latent post-treatment bias of a proxy-based causal inference algorithm that controls latent variables \(\hat{\bm{Z}}\) inferred from \(\bm{X}\) via \(\tilde{f}=\bar{f}\circ f^{\dagger}:\mathbb{R}^{K_{X}}\rightarrow\mathbb{R}^{K _{C}+K_{M}}\) to estimate the ATE can be formulated as follows:_

\[\begin{split}\text{Bias}(\bm{X})&=ATE-DEV(\tilde{ f}(\bm{X}))=ATE-\mathbb{E}[\mathbb{E}[Y|T=1,\tilde{f}(\bm{X})]-\mathbb{E}[Y|T=0, \tilde{f}(\bm{X})]]\\ &=ATE-\mathbb{E}[\mathbb{E}[Y|1,\bar{f}\circ f^{\dagger}(f(\bm{C },\bm{M}))]-\mathbb{E}[Y|0,\bar{f}\circ f^{\dagger}(f(\bm{C},\bm{M}))]]\\ &=\mathbb{E}[\mathbb{E}[Y|1,\bm{C}]-\mathbb{E}[Y|0,\bm{C}]]- \mathbb{E}[\mathbb{E}[Y|1,\bm{C},\bm{M}]-\mathbb{E}[Y|0,\bm{C},\bm{M}]],\end{split}\] (4)

_which can be arbitrarily bad. Therefore, the estimator of existing proxy-of-confounder-based methods, i.e., \(DEV(\tilde{f}(\bm{X}))\), is an arbitrarily biased estimator of the ATE, when the selected proxy of confounders \(\bm{X}\) accidentally mixes in latent post-treatment variables \(\bm{M}\)._

The final step of Eq. (4) can be proved since \(f\) is injective and \(\bar{f}\) bijective, the composite \(\bar{f}\circ f^{\dagger}\circ f:[\bm{C},\bm{M}]\rightarrow\hat{\bm{Z}}\) is bijective, so we can use Lemma 3.1 to remove \(\bar{f}\circ f^{\dagger}\circ f\) in the condition.

### Examples in the Linear Case

Generally, the latent post-treatment bias defined in Eq. (4) cannot be simplified, because _(i)_ the causal relationship between \(\bm{M}\) and \(Y\) are indeterminate, and _(ii)_ the causal influence of \(\bm{C}\), \(\bm{M}\), and \(T\) on \(Y\) can be arbitrary. However, for linear structural causal models with determined causal relationships between \(\bm{M}\) and \(Y\) (e.g., \(\bm{M}\) are mediators, which are post-treatment variables that have causal influences on the outcomes), stronger conclusions can be drawn as follows:

**Corollary 3.3**.: _(**Mixed Latent Mediator**). For the linear Structural Causal Model (SCM) defined as:_

\[\begin{split}&(i)\;T\leftarrow\mathds{1}(\alpha_{T}+\sum\beta_{i} \cdot C_{i}>a),\;(ii)\;M_{j}\leftarrow\alpha_{M}+\gamma_{j}\cdot T\\ &(iii)\;\bm{X}\leftarrow\bm{\alpha}_{X}+\mathbf{A}[\bm{M}||\bm{C }],\;(iv)\;Y\leftarrow\alpha_{Y}+\tau\cdot T+\sum\theta_{j}\cdot M_{j}+\sum \kappa_{i}\cdot C_{i},\end{split}\] (5)

_where the mixture function \(f=\mathbf{A}\in\mathbb{R}^{K_{X}\times(K_{C}+K_{M})}\) is a full column-rank matrix, the CATE, ATE, and the bias of proxy-of-confounder-based causal inference model that controls the latent variables \(\hat{\bm{Z}}\) inferred via \(\hat{\bm{Z}}=\hat{f}(\bm{X})=\mathbf{B}^{T}\bm{X}\) can be formulated as follows:_

\[\begin{split}& ATE=CATE=\tau+\sum\gamma_{j}\cdot\theta_{j},\; \mathrm{and}\;DEV(\hat{\bm{Z}})=\mathbb{E}[DCEV(\hat{\bm{Z}})]=DCEV(\hat{\bm{ Z}})=\tau\\ & Bias(\hat{\bm{Z}})=ATE-DEV(\hat{\bm{Z}})=\sum\gamma_{j}\cdot \theta_{j},\end{split}\] (6)

_where \(\mathbf{B}\in\mathbb{R}^{K_{X}\times(K_{C}+K_{M})}\) is another full column-rank matrix. Since \(\sum\gamma_{j}\cdot\theta_{j}\) is arbitrary, the estimator \(DEV(\hat{\bm{Z}})=\mathbb{E}[DCEV(\mathbf{B}^{T}\bm{X})]\) is arbitrarily biased for ATE estimation._

The proof of Eq. (6) is provided in Appendix C.2. In addition, we show that post-treatment variables \(\bm{M}\) DO NOT necessarily need to have direct causal effects on the outcome \(Y\) to incur arbitrary bias in ATE estimation. In Appendix C.3, we provide another example (i.e., Mixed Latent Correlator) in the linear case where \(\bm{M}\) is correlated with \(Y\) through unobserved confounders \(\bm{U}\) in Corollary C.1.

## 4 Methodology

In this section, we introduce the proposed Confounder-identifiable Variational Auto-Encoder (**CiVAE**) in detail. Specifically, we first prove that if the prior distribution of the true latent variables \(\bm{Z}=[\bm{C},\bm{M}]\) satisfies certain weak assumptions, CiVAE _individually_ identify \([\bm{C},\bm{M}]\) up to bijective transformations. Then, utilizing the causal relations between \(\bm{C}\), \(\bm{M}\), and \(T\), we novelly transform the challenging confounder-identifiability problem into a tractable pair-wise conditional independence test problem, which can be effectively solved with kernel-based methods. The generalization of CiVAE to address the interactions among \([\bm{C},\bm{M}]\) are discussed in Section D of the Appendix.

### Generative Process

The fundamental work on the identifiability of deep variational inference, i.e., the identifiable VAE (iVAE) [19], makes a strict assumption that the prior of true latent variables \(\mathbf{Z}\) (i.e., \([\bm{C},\bm{M}]\) in our case) is conditionally factorized given the available covariates. However, since both \(\bm{C}\) and \(\bm{M}\) form fork structures with the outcome \(Y\) (see Fig. 1-(c)) [22], \(C_{i}\), \(C_{j}\), \(M_{i}\), and \(M_{j}\) are not independent given \(Y\). Recently, Non-Factorized iVAE (NF-iVAE) [26] was proposed that allows arbitrary dependence among the true latent variables \(\bm{Z}\) in the conditional priors, where \(\bm{Z}\) can be identified up to arbitrary non-linear transformations. However, the transformation is not necessarily invertible, which is risky as multiple values of the confounders may collapse, leading to bias when estimating the ATE by averaging the \(DCEV\) calculated in each stratum of the inferred confounders.

In contrast to NF-iVAE, CiVAE guarantees the individual and bijective identifiability of \(\bm{Z}\) by putting a general exponential family _with at least one invertible sufficient statistic in the factorized part_ as its prior when conditioning on treatment \(T\) and outcome \(Y\), which can be formulated as follows.

**Assumption 2**.: _Let \(\bm{Z}=[\bm{C}||\bm{M}]\) be the random vector for latent variables that causally generate the observed covariates \(\bm{X}\) according to Assumption 1. We assume that the conditional prior of \(\bm{Z}\) given the outcome \(Y\) and the treatment \(T\) belongs to a general exponential family with parameter vector \(\bm{\lambda}(Y,T)\) and sufficient statistics \(\bm{S}(\bm{Z})=[\bm{S}_{f}(\bm{Z})^{T},\bm{S}_{nf}(\bm{Z})^{T}]^{T}\). Specifically, \(\bm{S}(\bm{Z})\) is composed of (i) the sufficient statistics of a factorized exponential family, i.e., \(\bm{S}_{f}(\bm{Z})=[\bm{S}_{1}(Z_{1})^{T},\cdots,\bm{S}_{K_{Z}}(Z_{K_{Z}})^{T} ]^{T}\), where all components \(\bm{S}_{i}(Z_{i})\) have dimension larger than or equal to 2 and **each \(\bm{S}_{i}\) has at least one invertible dimension**, and (ii) \(\bm{S}_{nf}(\bm{Z})\), where \(\bm{S}_{nf}\) is a neural network with ReLU activation. The density of the conditional prior can be formulated as:_

\[p_{\bm{S},\bm{\lambda}}(\bm{Z}|Y,T)=\mathcal{Q}(\bm{Z})/\mathcal{C}(Y,T)\exp[ \bm{S}(\bm{Z})^{T}\bm{\lambda}(Y,T)],\] (7)

_where \(\mathcal{Q}(\bm{Z})\) is the base measure, and \(\mathcal{C}(Y,T)\) is the normalizing constant independent of \(\bm{Z}\)._We justify that assumption 2 is weak and practical as follows. _(i)_ Neural networks with ReLU activation have **universal approximation ability** of distributions [27]. Therefore, Eq. (7) can model arbitrary dependence between true latent confounders \(\bm{C}\) and post-treatment variables \(\bm{M}\) conditional on \(T\) and \(Y\). _(ii)_ Although CiVAE makes an extra assumption that \(\forall i\), at least one dimension of \(\bm{S}_{i}\) is invertible, this can be easily satisfied as most commonly used exponential family distributions, such as Gaussian, Bernoulli, etc., has at least one invertible sufficient statistics2.

Footnote 2: There are a few exponential family dist. with no invertible sufficient statistics, e.g., Weibull with even shape parameter \(k\). However, these distributions are not commonly used in statistics or machine learning.

The reason why we use ReLU as the activation is that, the identifiability of iVAE relies on the condition that the sufficient statistics \(\bm{S}\) have zero second-order cross-derivative. The factorized part, i.e., \(\bm{S}_{f}\), satisfies it trivially as all cross-derivatives of \(\bm{S}_{f}\) are zero. In addition, since the ReLU neural networks are linear _a.e._, all second-order derivatives of \(\bm{S}_{nf}\) are zero. Therefore, identifiability holds after adding \(\bm{S}_{nf}\) in the prior that allows the capturing of arbitrary dependence among \(\bm{Z}\).

### Optimization Objective

Combining Assumptions 1 and 2, the generative process assumed by CiVAE can be formulated as:

\[(i)\;p_{\bm{\theta}}(\bm{X},\bm{Z}\mid Y,T)=p_{f}(\bm{X}\mid\bm{Z}),(ii)\;\;p _{\bm{S},\bm{\lambda}}(\bm{Z}\mid Y,T),\;(iii)\;p_{f}(\bm{X}\mid\bm{Z})=p_{\bm {\epsilon}}(\bm{X}-f(\bm{Z})).\] (8)

where \(\bm{\theta}=(f,\bm{\lambda},\bm{S})\in\Theta\) are the parameters of the generative distribution. Since the generative process of CiVAE is parameterized by deep neural networks, the posterior distribution of \(\bm{Z}\), i.e., \(p_{\bm{\theta}}(\bm{Z}\mid\bm{X},Y,T)\), is intractable. Therefore, we resort to variational inference [4], where we introduce an approximate posterior \(q_{\bm{\phi}}(\bm{Z}\mid\bm{X},Y,T)\) parameterized by a deep neural network with a trainable parameter \(\bm{\phi}\), and in \(q_{\bm{\phi}}(\bm{Z}|\cdot)\) finds the one closest to \(p_{\bm{\theta}}(\bm{Z}|\cdot)\) measured by KL divergence. The minimization of KL is equivalent to maximization of the evidence lower bound (ELBO):

\[\mathcal{L}(\bm{\theta},\bm{\phi}):=\mathbb{E}_{q_{\bm{\phi}}}\big{[}\log p_{ f}(\bm{X}\mid\bm{Z})+\underbrace{\log p_{\bm{S},\bm{\lambda}}(\bm{Z}\mid Y,T)-\log q_{\bm{\phi}}(\bm{Z}\mid\cdot)}_{\text{KL of posterior with prior}}\big{]}.\] (9)

Since the normalization constant \(\mathcal{C}\) in Eq. (7) is generally intractable, it is infeasible to directly learn \(\bm{S}\), \(\bm{\lambda}\) by optimizing Eq. (9). Therefore, we substitute the KL term in Eq. (9) with the widely-used score matching [13] to learn unnormalized densities instead as follows:

\[\mathcal{L}(\bm{S},\bm{\lambda},\bm{\phi}):=\mathbb{E}_{q_{\bm{ \phi}}(\bm{Z}|\cdot)}\left[\|\nabla_{\bm{Z}}\log q_{\bm{\phi}}(\bm{Z}\mid \cdot)-\nabla_{\bm{Z}}\log p_{\bm{S},\bm{\lambda}}(\bm{Z}\mid Y,T)\|^{2}\right]\] (10) \[=\mathbb{E}_{q_{\bm{\phi}}(\bm{Z}|\cdot)}\left[\sum_{j=1}^{K_{Z} }\left[\frac{\partial^{2}p_{\bm{S},\bm{\lambda}}(\bm{Z}\mid Y,T)}{\partial Z _{j}^{2}}+\frac{1}{2}\left(\frac{\partial p_{\bm{S},\bm{\lambda}}(\bm{Z}\mid Y,T)}{\partial Z_{j}}\right)^{2}\right]\right]+\text{ const},\]

### Identifiability of CiVAE

With the generative process and optimization objective of CiVAE discussed in previous sub-sections, we are ready to introduce the final assumption of CiVAE, which, combined with Assumptions 1 and 2, leads to the main Theorem of this paper, which states the identifiability of CiVAE.

**Assumption 3**.: _Assume the following: (i) The set \(\{\bm{X}\in\mathcal{X}|\phi(\bm{X})=0\}\) has measure zero, where \(\phi\) is the characteristic function of the density \(p_{f}\) in Eq. (8). (ii) The sufficient statistics, \(\bm{S}_{i}\) in \(\bm{S}_{f}\) are all twice differentiable. (iii) The mixture function \(f\) in Eq. (8) has all second-order cross derivatives. (iv) There exist \(k+1\) distinct points \((Y,T)_{0}\), \(\cdots,(Y,T)_{k}\) s.t. the matrix \(\bm{\mathrm{L}}=[\bm{\lambda}((Y,T)_{1})-\bm{\lambda}((Y,T)_{0}),\cdots,\bm{ \lambda}((Y,T)_{k})-\bm{\lambda}((Y,T)_{0})]\) of size \(k\times k\) is invertible, where \(k=Dim(\bm{S})\)._

Here, we note that Assumptions _(i) - (iii)_ are trivial for differentiable neural networks. The Assumption _(iv)_ can be intuitively understood as independent samples of \((Y,T)\) are required to identify \(\bm{C}\) and \(\bm{M}\). The identifiability theorem of CiVAE can be formulated as follows.

**Theorem 4.1**.: _If Assumptions 1, 2, and 3 hold, and if \(\bm{\theta},\tilde{\bm{\theta}}\in\Theta\to p_{\bm{\theta}}(\bm{X}|Y,T)=p_{\tilde {\bm{\theta}}}(\bm{X}|Y,T)\), the true latent variables \(\bm{Z}\) are identifiable up to **permutation** and **element-wise bijective transformation**. Furthermore, in the case of **variational inference**, if we denote the true parameter that generates the data as \(\bm{\theta}^{*}\), if (i) the distribution family \(q_{\bm{\phi}}(\bm{Z}|\bm{X},Y,T)\) contains the posterior \(p_{\bm{\theta}}(\bm{Z}|\bm{X},Y,T)\), and \(q_{\bm{\phi}}(\bm{Z}|\bm{X},Y,T)>0\), (ii) we optimize Eq. (4) w.r.t. both \(\bm{\theta},\bm{\phi}\), then in the limit of infinite data, true parameters \(\bm{\theta}^{*}\) can be learned up to a permutation and bijective transformation of \(\bm{Z}\)._The proof of Theorem 4.1 non trivially extends the NF-iVAE paper [26] by incorporating the new assumption introduced in CiVAE (i.e., each \(\bm{S}_{i}\) has at least one invertible dimension) to ensure that the transformation of each \(Z_{i}\) is bijective. The detailed proof is provided in Appendix C.4 for reference.

### Identification of Latent Confounders

Theorem 4.1 ensures that the latent variables \(\hat{\bm{Z}}\) inferred by CiVAE cannot _(i)_ mix confounders and post-treatment variables in each dimension, or _(ii)_ collapsing of different values of the latent confounders into the same value. To further determine the dimensions of confounder and post-treatment variable in \(\hat{\bm{Z}}\), we rely on the causal relations between latent variables \(\hat{\bm{Z}}\) and the treatment \(T\) and the associated marginal/conditional dependence properties, which are discussed as follows.

* _Case 1. Intra-Confounders._ Latent confounders \(C_{i}\), \(C_{j}\) and the treatment \(T\) form the _V structure_\(C_{i}\to T\gets C_{j}\). Therefore, \(C_{i}\) and \(C_{j}\) are marginally **independent**, whereas they become **dependent** when conditioning on the assigned treatment \(T\).
* _Case 2. Intra-Post Treatment Variables._ Latent post-treatment variables \(M_{i}\), \(M_{j}\) and the treatment \(T\) form a _Fork-structure_\(M_{i}\gets T\to M_{j}\), where \(M_{i}\), \(M_{j}\) are marginally **dependent**, but they become **independent** after conditioning on the assigned treatment \(T\).
* _Case 3. Cross-Confounder and Post-Treatment Variables._ Latent confounder \(C_{i}\), latent post-treatment variable \(M_{j}\), and the treatment \(T\) forms a Chain structure \(C_{i}\to T\to M_{j}\), where \(C_{i}\), \(M_{j}\) are marginally dependent, and they become **independent** after conditioning on \(T\).

From the above analysis we can find that, the dependence between two latent variables \(\hat{Z}_{i}\) and \(\hat{Z}_{j}\)**increases** after conditioning on the treatment \(T\) ONLY in the case of _intra-confounders_. Therefore, if more than one latent confounder exists, which is highly probable when covariates \(\bm{X}\) are high-dimensional, we can conduct independence test \(\texttt{Ind}(\hat{Z}_{i},\hat{Z}_{j})\) and \(\texttt{CInd}(\hat{Z}_{i},\hat{Z}_{j}|T)\) for all pairs of inferred latent variables, which can be implemented via kernel-based methods as [43], and select the pairs where the p-value of CInd is larger than that of Ind as latent confounders. Here, we note that the kernel-based (conditional) independence test incurs \(N^{2}\times K_{Z}^{2}\) complexity in the training phase. However, once the dimensions of the confounders in \(\hat{\bm{Z}}\) are determined, CiVAE **has the same complexity as CEVAE** for the estimation of CATE and ATE in the test phase.

### ATE Estimator with Transformed Confounders

Finally, we demonstrate that controlling the transformed confounders \(\hat{\bm{C}}\) inferred by CiVAE provides an unbiased estimation of ATE. Specifically, we have the final Theorem show the unbiasedness.

**Theorem 4.2**.: _Controlling bijective of confounders is equivalent to original confounders in ATE estimation, i.e., \(DEV(\hat{\bm{C}})=DEV(g(\bm{C}))=ATE\), if the transformation function \(g\) is bijective._

The proof of Theorem 4.2 for discrete \(\bm{C}\) is trivial (where \(\hat{\bm{C}}=g(\bm{C})\) represents a simple relabeling of the stratum that we calculate the \(DCEV\) and take the expectation). The proof in the continuous case where \(g\) is differentiable is provided in Appendix C.5. With Theorem 4.2, we can control the identified latent confounders as true confounders, providing an unbiased estimate of ATE.

## 5 Empirical Study

In this section, we provide and analyze the experiments we conduct on both simulated and real-world datasets, where a code demo written in PyTorch and Pyro is provided in this anonymous URL.

### Datasets

**Simulated Datasets.** We first establish two simulated datasets, i.e., LatentMediator and LatentCorrelator, that consider two types of post-treatment variables, i.e., _(i)_ mediators and _(ii)_ correlators, i.e., variables that are correlated with the outcome \(Y\) via latent confounders \(\bm{U}\), where the causal generative process is under the full control of the experimenter. The generative process of the two datasets can be referred to in Corollary 3.3 and Corollary C.1 in the Appendix, respectively. In our experiments, \(\bm{C}\) are generated from Gaussian distribution as \(\bm{C}\sim Gaussian(0,\mathbf{I}_{K_{C}})\). For 
**LatentMediator, \(\bm{\gamma}\)** is set as \([-1,-1,-1]\), \(\bm{\theta}\) is set as \([1,1,1]\), and \(\tau\) is set as 2, which results in \(ATE=-1\). For the LatentCorrelator dataset, we set the same \(\bm{\gamma}\) and \(\bm{\theta}\) as the LatentMediator dataset, where parameters \(\bm{\phi}\) and \(\tau\) are set to 1, which results in an overall \(ATE\) of \(1\).

**Real-world Datasets.** In addition, we build real-world datasets from the Company to estimate the ATE of _switching a job from **onsite to _online work mode_ to _the statistics of the applicants_. The average age and the variance of gender of the applicants are two outcomes of interest. Covariates \(\bm{X}\in\{0,1\}^{K_{X}}\) include the required skills of the job. Specifically, we establish a cohort of 3,228 jobs from the Bay Area in the US, where a preliminary study shows that \(DEV(\emptyset)\approx 2\) years3 (i.e., online job applicants are two years younger than onsite job applicants in the collected data), and \(DEV(\emptyset)\approx-0.015\) (i.e., online jobs exhibit 0.015 more gender variance than onsite jobs in the collected data). To simulate \(\bm{C}\) and \(\bm{M}\), we first learn a generative model as follows:

Footnote 3: which leads to 0.178 and -0.105 after standardization of the outcome.

\[\bm{Z}\sim Gaussian(\bm{0},\mathbf{I}_{K_{Z}}),\bm{X}\sim Multi(NN_{f}(\bm{ Z})),Y\sim Gaussian(\bm{w}\odot\bm{Z},1),\] (11)

where \(Multi\) represents multinomial distribution, \(NN_{f}\) is a neural network with softmax activation, \(\bm{Z},\bm{w}\in\mathbb{R}^{K_{Z}}\), \(K_{Z}=8\), and \(\odot\) represents the element-wise product operator, respectively. We then treat the first \(K_{C}=5\) dimensions of \(\bm{Z}\) as the latent confounders \(\bm{C}\) and the remaining \(K_{M}=K_{Z}-K_{C}\) dimensions as the latent mediators \(\bm{M}\). After learning \(NN_{f}\) and \(\bm{w}\) according to Eq. (11), we draw latent confounders \(\bm{C}\in Gaussian(0,\mathbf{I})\), latent mediators \(\bm{M}=T\cdot\bm{\gamma}\), and set the outcome \(Y=\bm{w}\odot[\bm{C}||\bm{M}]+\tau\cdot T\), where the true ATE can be calculated as \(sum(\bm{\gamma}\odot\bm{w}_{-K_{M}:})+\tau\).

#### 5.1.1 Disentangle Confounders and Post-treatment Variables

We first show the \(p\)-value of the kernel-based pairwise independence test of the true latent variables before and after conditioning on the assigned treatment \(T\). From Fig. 2, we can find that the distinction of the intra-confounder case from the other two cases discussed in Subsection 4.4 is significant. Here, we should note this relies on the assumption that latent confounders are independent. If the latent confounders are correlated, we can first use causal discovery techniques such as the PC algorithm [39] to find direct parents of \(T\), and use our algorithm as the refinement to determine the true confounders \(C\) from the misidentified post-treatment variables (Experiments see Section D) in Appendix.

### Baselines

The baselines we include for comparisons can be categorized into three classes. _(i)_**Unawareness**, where no information in \(\bm{X}\) is used for ATE estimation. We implement the naive LR0 estimator, which regresses \(Y\) on \(T\) and uses the coefficient to estimate the ATE [15] (LR0 equals to \(DEV(\emptyset)\), i.e., the difference of the average outcome between the treatment and non-treatment group). _(ii)_**Control-\(\bm{X}\)**, which directly controls the covariates \(\bm{X}\). In this class, LR1 regresses \(Y\) on \(T\) and \(\bm{X}\), whereas TarNet uses a two-branch neural network to estimate the \(DEV(\bm{X})\)_**(iii) Control-\(\bm{Z}\)**, which controls latent variables \(\bm{Z}\) learned from the covariates \(\bm{X}\). Methods from this class include the CEVAE [25] and covariate disentanglement methods, such as DR-CFR [12], TEDVAE [44], NICE [38], and AFS [41].

#### 5.2.1 Results and Analysis

From Table 1, we can find that for all four datasets, CEVAE is worse than the naive LR0 estimator. In addition, for the LatentMediator and Company (Age) dataset, all methods except CiVAE fail to predict the negativity of the ATE. Covariates disentanglement-based methods, i.e., DR-CFR and TEDVAE, inherit the latent post-treatment bias of CEVAE. The reason is that, these methods disentangle latent confounders \(\bm{C}\) from latent instrumental variables \(\bm{I}\) and latent adjusters \(\bm{A}\) by

Figure 2: Visualization of \(p\)-value of independence test before and after conditioning on treatment \(T\).

utilizing their causal relations with \(T\) and \(Y\), i.e., \(\bm{I}\) is predictive only for \(T\), \(\bm{A}\) is predictive only for \(Y\), whereas \(\bm{C}\) is predictive for both \(T\) and \(Y\). For example, TEDVAE includes three encoders to infer three sets of latent variables \(\hat{\bm{I}}\), \(\hat{\bm{A}}\), \(\hat{\bm{C}}\) from \(\bm{X}\) and adds classification losses \(p(T|\hat{\bm{I}},\hat{\bm{C}})\) and \(p(Y|T,\hat{\bm{C}},\hat{\bm{A}})\) on the CEVAE loss. However, since both latent confounders \(\bm{C}\) and latent post-treatment variables \(\bm{M}\) are correlated with both \(T\) and \(Y\), these methods cannot disentangle \(\bm{C}\) from \(\bm{M}\). An exception is NICE [38], which uses invariant risk minimization (IRM) [3] to find all causal parents of the outcome \(Y\) as the confounders, which makes it more robust in the LatentCorrelator case. However, since mediators \(\bm{M}\) are also the causal parent of \(Y\), the performance degrades substantially on the LatentMediator dataset. Although AFS [41] considers the existence of post-treatment variables \(\bm{M}\) in the proxy \(\bm{X}\), it assumes that they can be separated from other variables in \(\bm{X}\) in the observational space, and no relationship exists between the post-treatment variables and the outcome, so it still has poor performance in our setting since both assumptions are violated.

### Sensitivity Analysis

In this part, we vary the number of confounders and post-treatment variables that generate proxy \(\bm{X}\) in the Company (Age) and Company (Gender) datasets and compare CiVAE with the baseline TEDVAE in Fig. 3. Fig. 3 shows that the error is consistently lower for CiVAE. In addition, the error is comparatively higher when the number of confounders is low since the misidentification of latent post-treatment variables as confounders can have a comparatively larger influence on the ATE estimation. In addition, when the number of confounders becomes larger, the performance gap between CiVAE and TEDVAE gracefully shrinks.

## 6 Conclusions

In this paper, we systematically investigate the latent post-treatment bias in causal inference from observational data. We first prove that unresolved latent post-treatment variables scrambled in the proxy of confounders can arbitrarily bias the ATE estimation. To address the bias, we proposed the Confounder-identifiable VAE (CiVAE), which, utilizing a mild assumption regarding the prior of latent factors, guarantees the identifiability of latent confounders up to bijective transformations. Finally, we show that controlling the latent confounders inferred by CiVAE can provide an unbiased estimation of the ATE. Experiments on both simulated and real-world datasets demonstrate that CiVAE has superior robustness to latent post-treatment bias compared to state-of-the-art methods.

\begin{table}
\begin{tabular}{l||c c|c c||c c|c c} \hline Dataset & \multicolumn{2}{c|}{LatentMediator} & \multicolumn{2}{c||}{LatentCorrelator} & \multicolumn{2}{c|}{Company (Age)} & \multicolumn{2}{c}{Company (Gender)} \\ \hline Method & ATE. & Err. & ATE. & Err. & ATE. & Err. & ATE. & Err. \\ \hline \hline LR0 & 0.975 \(\pm\) 0.032 & 1.975 & 2.977 \(\pm\) 0.032 & 1.977 & 0.131 \(\pm\) 0.015 & 0.399 & -0.105 \(\pm\) 0.009 & -0.213 \\ \hline LR1 & 1.457 \(\pm\) 0.167 & 2.457 & 3.400 \(\pm\) 0.130 & 2.400 & 0.093 \(\pm\) 0.029 & 0.361 & -0.175 \(\pm\) 0.014 & -0.256 \\ TarNet & 1.461 \(\pm\) 0.172 & 2.461 & 3.414 \(\pm\) 0.146 & 2.414 & 0.112 \(\pm\) 0.085 & 0.380 & -0.167 \(\pm\) 0.021 & -0.248 \\ CEVAE & 1.550 \(\pm\) 0.292 & 2.550 & 3.323 \(\pm\) 0.167 & 2.323 & 0.106 \(\pm\) 0.078 & 0.374 & -0.180 \(\pm\) 0.028 & -0.261 \\ DR-CFR & 1.239 \(\pm\) 0.324 & 2.239 & 3.185 \(\pm\) 0.319 & 2.185 & 0.094 \(\pm\) 0.089 & 0.362 & -0.159 \(\pm\) 0.030 & -0.240 \\ NICE & 1.868 \(\pm\) 0.530 & 2.868 & 1.942 \(\pm\) 0.524 & 0.942 & 0.149 \(\pm\) 0.126 & 0.417 & -0.186 \(\pm\) 0.041 & -0.267 \\ TEDVAE & 1.042 \(\pm\) 0.315 & 2.042 & 3.138 \(\pm\) 0.281 & 2.138 & 0.097 \(\pm\) 0.093 & 0.365 & -0.143 \(\pm\) 0.027 & -0.224 \\ AFS & 1.496 \(\pm\) 0.825 & 2.496 & 3.251 \(\pm\) 0.398 & 2.251 & 0.105 \(\pm\) 0.102 & 0.373 & -0.163 \(\pm\) 0.045 & -0.244 \\ \hline CiVAE & **-0.822** \(\pm\) 0.753 & **0.178** & **1.199** \(\pm\) 0.765 & **0.199** & **-0.140**\(\pm\) 0.137 & **0.128** & **-0.106**\(\pm\) 0.064 & **-0.187** \\ \hline True ATE & -1.000 \(\pm\) 0.000 & 0.000 & 1.000 \(\pm\) 0.000 & 0.000 & -0.268 \(\pm\) 0.000 & 0.000 & -0.081 \(\pm\) 0.000 & 0.000 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of CiVAE with baselines under latent post-treatment bias on various datasets.

Figure 3: Error with different ratio of latent confounders and latent post-treatment variable in the latent space.

## References

* Acharya et al. [2016] A. Acharya, M. Blackwell, and M. Sen. Explaining causal findings without bias: Detecting and assessing direct effects. _American Political Science Review_, 110(3):512-529, 2016.
* Anandkumar et al. [2014] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning latent variable models. _Journal of Machine Learning Research_, 15:2773-2832, 2014.
* Arjovsky et al. [2019] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* Blei et al. [2017] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. _Journal of the American Statistical Association_, 112(518):859-877, 2017.
* Cheng et al. [2022] L. Cheng, R. Guo, and H. Liu. Causal mediation analysis with hidden confounders. In _Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining_, pages 113-122, 2022.
* Cook et al. [2002] T. D. Cook, D. T. Campbell, and W. Shadish. _Experimental and quasi-experimental designs for generalized causal inference_. Houghton Mifflin Boston, MA, 2002.
* Ding and Miratrix [2015] P. Ding and L. W. Miratrix. To adjust or not to adjust? sensitivity analysis of m-bias and butterfly-bias. _Journal of Causal Inference_, 3(1):41-57, 2015.
* Edwards et al. [2015] J. K. Edwards, S. R. Cole, and D. Westreich. All your data are always missing: incorporating bias due to measurement error into the potential outcomes framework. _International Journal of Epidemiology_, 44(4):1452-1459, 2015.
* Elwert and Winship [2014] F. Elwert and C. Winship. Endogenous selection bias: The problem of conditioning on a collider variable. _Annual review of sociology_, 40:31-53, 2014.
* Glass et al. [2013] T. A. Glass, S. N. Goodman, M. A. Hernan, and J. M. Samet. Causal inference in public health. _Annual Review of Public Health_, 34:61-75, 2013.
* Hassanpour and Greiner [2019] N. Hassanpour and R. Greiner. Counterfactual regression with importance sampling weights. In _IJCAI_, pages 5880-5887, 2019.
* Hassanpour and Greiner [2020] N. Hassanpour and R. Greiner. Learning disentangled representations for counterfactual regression. In _International Conference on Learning Representations_, 2020.
* Hyvarinen and Dayan [2005] A. Hyvarinen and P. Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* Imai et al. [2010] K. Imai, L. Keele, and D. Tingley. A general approach to causal mediation analysis. _Psychological Methods_, 15(4):309, 2010.
* Imbens and Rubin [2015] G. W. Imbens and D. B. Rubin. _Causal inference in statistics, social, and biomedical sciences_. Cambridge University Press, 2015.
* Jager et al. [2008] K. Jager, C. Zoccali, A. Macleod, and F. Dekker. Confounding: what it is and how to deal with it. _Kidney international_, 73(3):256-260, 2008.
* Johansson et al. [2016] F. Johansson, U. Shalit, and D. Sontag. Learning representations for counterfactual inference. In _International Conference on Machine Learning_, pages 3020-3029, 2016.
* Kalisch and Buhlman [2007] M. Kalisch and P. Buhlman. Estimating high-dimensional directed acyclic graphs with the pc-algorithm. _Journal of Machine Learning Research_, 8(3), 2007.
* Khemakhem et al. [2020] I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational autoencoders and nonlinear ICA: A unifying framework. In _International Conference on Artificial Intelligence and Statistics_, pages 2207-2217. PMLR, 2020.
* King [2010] G. King. A hard unsolved problem? post-treatment bias in big social science questions. In _Hard Problems in Social Science" Symposium, April_, volume 10, 2010.

* [21] G. King and L. Zeng. The dangers of extreme counterfactuals. _Political Analysis_, 14(2):131-159, 2006.
* [22] D. Koller and N. Friedman. _Probabilistic graphical models: principles and techniques_. MIT press, 2009.
* [23] M. Kuroki and J. Pearl. Measurement bias and effect restoration in causal inference. _Biometrika_, 101(2):423-437, 2014.
* [24] F. Li, K. L. Morgan, and A. M. Zaslavsky. Balancing covariates via propensity score weighting. _Journal of the American Statistical Association_, 113(521):390-400, 2018.
* [25] C. Louizos, U. Shalit, J. M. Mooij, D. Sontag, R. Zemel, and M. Welling. Causal effect inference with deep latent-variable models. _Advances in Neural Information Processing Systems_, 30, 2017.
* [26] C. Lu, Y. Wu, J. M. Hernandez-Lobato, and B. Scholkopf. Invariant causal representation learning for out-of-distribution generalization. In _International Conference on Learning Representations_, 2021.
* [27] Y. Lu and J. Lu. A universal approximation theorem of deep neural networks for expressing probability distributions. In _Advances in Neural Information Processing Systems_, pages 3094-3105, 2020.
* [28] D. Madras, E. Creager, T. Pitassi, and R. Zemel. Fairness through causal awareness: Learning causal latent-variable models for biased data. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_, pages 349-358, 2019.
* [29] W. Miao, Z. Geng, and E. J. Tchetgen Tchetgen. Identifying causal effects with proxy variables of an unmeasured confounder. _Biometrika_, 105(4):987-993, 2018.
* [30] J. M. Montgomery, B. Nyhan, and M. Torres. How conditioning on posttreatment variables can ruin your experiment and what to do about it. _American Journal of Political Science_, 62(3):760-775, 2018.
* [31] J. Pearl. On measurement bias in causal inference. _arXiv preprint arXiv:1203.3504_, 2012.
* [32] J. Pearl. Conditioning on post-treatment variables. _Journal of Causal Inference_, 3(1):131-137, 2015.
* [33] S. J. Pocock, S. E. Assmann, L. E. Enos, and L. E. Kasten. Subgroup analysis, covariate adjustment and baseline comparisons in clinical trial reporting: current practiceand problems. _Statistics in Medicine_, 21(19):2917-2930, 2002.
* [34] M. Prosperi, Y. Guo, M. Sperrin, J. S. Koopman, J. S. Min, X. He, S. Rich, M. Wang, I. E. Buchan, and J. Bian. Causal inference and counterfactual prediction in machine learning for actionable healthcare. _Nature Machine Intelligence_, 2(7):369-375, 2020.
* [35] K. J. Rothman, S. Greenland, T. L. Lash, et al. _Modern epidemiology_, volume 3. Wolters Kluwer Health/Lippincott Williams & Wilkins Philadelphia, 2008.
* [36] U. Shalit, F. D. Johansson, and D. Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In _International Conference on Machine Learning_, pages 3076-3085, 2017.
* [37] C. Shi, D. Blei, and V. Veitch. Adapting neural networks for the estimation of treatment effects. In _Advances in Neural Information Processing Systems_, 2019.
* [38] C. Shi, V. Veitch, and D. M. Blei. Invariant representation learning for treatment effect estimation. In _Uncertainty in Artificial Intelligence_, pages 1546-1555. PMLR, 2021.
* [39] P. Spirtes, C. N. Glymour, R. Scheines, and D. Heckerman. _Causation, prediction, and search_. MIT press, 2000.

* [40] S. Wager and S. Athey. Estimation and inference of heterogeneous treatment effects using random forests. _Journal of the American Statistical Association_, 113(523):1228-1242, 2018.
* [41] H. Wang, K. Kuang, H. Chi, L. Yang, M. Geng, W. Huang, and W. Yang. Treatment effect estimation with adjustment feature selection. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 2290-2301, 2023.
* [42] L. Yao, S. Li, Y. Li, M. Huai, J. Gao, and A. Zhang. Representation learning for treatment effect estimation from observational data. In _Advances in Neural Information Processing Systems_, volume 31, 2018.
* [43] K. Zhang, J. Peters, D. Janzing, and B. Scholkopf. Kernel-based conditional independence test and application in causal discovery. _arXiv preprint arXiv:1202.3775_, 2012.
* [44] W. Zhang, L. Liu, and J. Li. Treatment effect estimation with disentangled latent factors. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 10923-10930, 2021.

## Appendix A Broader Impact

The proposed CiVAE is a universal model for causal effect estimation with observational data. Although we use the Company job data that estimate the causal effects of _online working mode_ to _applicant statistics_ as a real-world example, proxy-of-confounder-based methods have been heavily used in other observational studies, which may be susceptible to latent post-treatment bias. Therefore, we speculate that the proposed CiVAE will have a broader impact on causal inference community.

## Appendix B Related Work

### Post-Treatment Bias in Causal Inference

Bias due to accidentally controlling post-treatment variables, i.e., _post-treatment bias_, has long been recognized as dangerous in causal effect estimation [20]. Back at 2005, Pearl [32] cautioned that controlling more is not better, and uses the collider bias [9] and M-Bias [7] as two examples to show that bias can be increased when controlling the post-treatment variables. Furthermore, [30] show that indirect correlations between post-treatment variable \(\bm{M}\) and outcome \(Y\) can still cause bias. Recent works prove that even if \(\bm{M}\) has no causal relationship with \(Y\), controlling it can still increase the variance of estimand [12]. However, most of these works study the post-treatment bias in the observational space, where latent post-treatment variables that are mixed with confounders to generate the observed covariates can be easily ignored by the researcher. Therefore, it motivates us to develop CiVAE, which is robust to the latent post-treatment bias under mild assumptions.

### Covariate Disentanglement

Recently, researchers have realized that directly controlling proxy of confounders \(\mathbf{X}\) may not be safe, as variables other than confounders could lurk in the proxy and ruin the ATE estimation [12]. Traditional methods assume that the variables that generate \(\mathbf{X}\) are a mixture of confounders, adjusters, and influencers [36], where adjusters should not be controlled as it can increase the estimation variance [11]. Most methods rely on the fact that adjusters are correlated only with the treatment to separate them from other variables [12, 44] (see Fig. (1)). This can also be used to remove post-treatment variables that are not correlated with the outcome, which have similar statistics properties with adjustors [41]. Here, a different work is NICE [38], which uses the fact that confounders and influencers are direct causal parents of the outcome to find these variables with invariant learning as the control set [3]. However, since mediators are also direct parents of the outcome, NICE is still not robust to general post-treatment bias. Given that all above methods cannot satisfactorily address the latent post-treatment in general cases, it is imperative to design the CiVAE, where confounders can be identified and distinguished with latent post-treatment variables for unbiased adjustment.

## Appendix C Theoretical Analysis

### Proof of Lemma 3.1.

Proof.: Let \(\bm{Z}=f(\bm{X})\) and \(\bm{z}=f(\bm{x})\). If \(f\) is injective and differentiable _a.e._, and \(f^{\dagger}\) is the left-inverse, we have:

\[f_{Y|f(\bm{X})}(y|f(\bm{x}))=f_{Y|\bm{Z}}(y|\bm{z})=\frac{f_{Y,\bm{Z}}(y,\bm{ z})}{f_{\bm{Z}}(\bm{z})}=\frac{f_{Y,\bm{X}}(y,f^{\dagger}(\bm{z}))|\mathbf{J}_{f ^{\dagger}}(\bm{z})|}{f_{\bm{X}}(f^{\dagger}(\bm{z}))|\mathbf{J}_{f^{\dagger}} (\bm{z})|}=\frac{f_{Y,\bm{X}}(y,\bm{x})}{f_{\bm{X}}(\bm{x})}=f_{Y|\bm{X}}(y| \bm{x}),\] (12)

where \(f.\) and \(f_{\cdot|\cdot}\) represent the marginal and conditional density function, respectively, and \(\mathbf{J}_{f^{\dagger}}(\bm{z})\) is the Jacobian matrix of function \(f^{\dagger}\) evaluated at \(\bm{z}\). Based on Eq. (12), we have:

\[\mathbb{E}[Y|\bm{X}]=\int\bm{y}\!\cdot\!f_{Y|\bm{X}}(\bm{y}|\bm{x})dy=\int y\! \cdot\!f_{Y|\bm{Z}}(\bm{y}|\bm{z})dy=\mathbb{E}[Y|\bm{Z}=\bm{z}]=\mathbb{E}[Y| f(\bm{X})=f(\bm{x})].\] (13)

### Proof of Corollary 3.3.

Proof.: For \(\bm{X}=\bm{x}\), let \([\bm{c}||\bm{m}]\doteq[f^{\dagger}_{C}(\bm{x})||f^{\dagger}_{M}(\bm{x})]\doteq f^ {\dagger}(\bm{x})=\mathbf{A}^{\dagger}(\bm{x}-\bm{\alpha}_{X})\), where \(\mathbf{A}^{\dagger}\) is the left inverse of the full column-rank matrix \(\mathbf{A}\) in Eq. (2), we have:

\[CATE(\bm{x}) =\mathbb{E}[Y|T=1,\bm{C}=f^{\dagger}_{C}(\bm{x})]-\mathbb{E}[Y|T=0,\bm{C}=f^{\dagger}_{C}(\bm{x})]\] (14) \[=\mathbb{E}[Y|T=1,\bm{C}=\bm{c}]-\mathbb{E}[Y|T=0,\bm{C}=\bm{c}]\] \[=\mathbb{E}[\alpha_{Y}+\tau\cdot T+\sum\theta_{j}\cdot M_{j}+\sum \kappa_{i}\cdot C_{i}|T=1,\bm{C}=\bm{c}]\] \[-\mathbb{E}[\alpha_{Y}+\tau\cdot T+\sum\theta_{j}\cdot M_{j}+\sum \kappa_{i}\cdot C_{i}|T=0,\bm{C}=\bm{c}]\] \[=\alpha_{Y}+\tau\cdot\mathbb{E}[T|T=1,\bm{C}=\bm{c}]+\sum\theta_ {j}\cdot\mathbb{E}[M_{j}|T=1,\bm{C}=\bm{c}]+\sum\kappa_{i}\cdot\mathbb{E}[C_{ i}|T=1,\bm{C}=\bm{c}]\] \[-\alpha_{Y}+\tau\cdot\mathbb{E}[T|T=0,\bm{C}=\bm{c}]+\sum\theta_ {j}\cdot\mathbb{E}[M_{j}|T=0,\bm{C}=\bm{c}]+\sum\kappa_{i}\cdot\mathbb{E}[C_{ i}|T=0,\bm{C}=\bm{c}]\] \[=\tau\cdot(1-0)+\sum\theta_{j}\cdot(\gamma_{j}\cdot(1-0))+\sum \kappa_{i}\cdot(c_{i}-c_{i})\] \[=\tau+\sum\theta_{j}\cdot\gamma_{j}=\mathbb{E}[\tau+\sum\theta_ {j}\cdot\gamma_{j}]=ATE,\]

where the first equality is due to the definition of CATE in Eq. (2). In addition, the causal estimand and bias of a proxy-of-confounder-based causal inference model that controls the latent variable \(\bm{Z}\) inferred via \(\bm{Z}=\bar{f}(\bm{X})=\mathbf{B}^{T}\bm{X}\) (where \(\mathbf{B}\) is also a full column-rank matrix) can be formulated as:

\[DCEV(\mathbf{B}^{T}\bm{x}) =\mathbb{E}[Y|T=1,\bm{Z}=\mathbf{B}^{T}\bm{x}]-\mathbb{E}[Y|T=0, \bm{Z}=\mathbf{B}^{T}\bm{x}]\] (15) \[=\mathbb{E}[Y|T=1,\bm{Z}=\mathbf{B}^{T}\bm{\alpha}_{X}+\mathbf{B} ^{T}\mathbf{A}[\bm{c}||\bm{m}]]-\mathbb{E}[Y|T=0,\bm{Z}=\mathbf{B}^{T}\bm{ \alpha}_{X}+\mathbf{B}^{T}\mathbf{A}[\bm{c}||\bm{m}]]\] \[\overset{(a)}{=}\mathbb{E}[Y|T=1,\bm{C}=\bm{c},\bm{M}=\bm{m}]- \mathbb{E}[Y|T=0,\bm{C}=\bm{c},\bm{M}=\bm{m}]\] \[=\alpha_{Y}+\tau\cdot 1+\sum\theta_{j}\cdot\mathbb{E}[M_{j}|T=1, \bm{C}=\bm{c},\bm{M}=\bm{m}]+\sum\kappa_{i}\cdot\mathbb{E}[C_{i}|T=1,\bm{C}= \bm{c},\bm{M}=\bm{m}]\] \[-\alpha_{Y}+\tau\cdot 0+\sum\theta_{j}\cdot\mathbb{E}[M_{j}|T=0,\bm{C} =\bm{c},\bm{M}=\bm{m}]+\sum\kappa_{i}\cdot\mathbb{E}[C_{i}|T=0,\bm{C}=\bm{c}, \bm{M}=\bm{m}]\] \[=\tau\cdot(1-0)+\sum\theta_{j}\cdot(m_{j}-m_{j})+\sum\kappa_{i} \cdot(c_{i}-c_{i})\] \[=\tau=\mathbb{E}[\tau]=\mathbb{E}[DCEV(\mathbf{B}^{T}\bm{X})],\]

where step (a) is due to the fact that, since both \(\mathbf{A}\) and \(\mathbf{B}\) are full column-rank matrices, \(\mathbf{B}^{T}\mathbf{A}\) is an invertible matrix, and the mapping \(f=\mathbf{B}^{T}\bm{\alpha}_{X}+\mathbf{B}^{T}\mathbf{A}\) is bijective. Therefore, we can invoke Lemma 3.1 and apply the left-inverse of \(f\), i.e., \(f^{\dagger}=(\mathbf{B}^{T}\mathbf{A})^{-1}-\mathbf{B}^{T}\bm{\alpha}_{X}\), to the condition of the expectation. The rest steps are based on the structural causal equations defined in Eq. (2). 

### Another Case of Linear SCM with Latent Correlators

**Corollary C.1**.: _For another Linear Structural Causal Model defined as follows_

\[T \leftarrow\mathds{1}(\alpha_{T}+\sum\beta_{i}\cdot C_{i}>a)\] (16) \[M_{j} \leftarrow\alpha_{M}+\gamma_{j}\cdot T+\phi_{j}\cdot U_{j}\] \[\bm{X} \leftarrow\bm{\alpha}_{X}+\mathbf{A}[\bm{M}||\bm{C}]\] \[Y \leftarrow\alpha_{Y}+\tau\cdot T+\sum\theta_{j}\cdot U_{j}+\sum \kappa_{i}\cdot C_{i},\]

_where \(f=\mathbf{A}\in\mathbb{R}^{K_{X}\times(K_{C}+K_{M})}\) is a full column-rank matrix, the CATE, ATE, and the bias of proxy-of-confounder-based causal inference model that controls the latent variable \(\bm{Z}\) inferred via \(\bm{Z}=\bar{f}(\bm{X})=\mathbf{B}^{T}\bm{X}\) can be formulated as follows:_

\[ATE=CATE=\tau\] (17) \[\mathbb{E}[DCEV(\bm{Z}=\mathbf{B}^{T}\bm{X})]=DCEV(\bm{Z}=\mathbf{ B}^{T}\bm{X})=\tau-\sum\frac{\theta_{j}\cdot\gamma_{j}}{\phi_{j}}\] \[Bias=ATE-\mathbb{E}[DCEV(\mathbf{B}^{T}\bm{X})]=\sum\frac{\theta_ {j}\cdot\gamma_{j}}{\phi_{j}},\]_where \(\mathbf{B}\in\mathbb{R}^{K_{X}\times(K_{C}+K_{M})}\) is another full column-rank matrix. Since \(\sum\frac{\theta_{j}\cdot\gamma_{j}}{\phi_{j}}\) is arbitrary, the estimator \(\mathbb{E}[DCEV(\mathbf{B}^{T}\bm{X})]\) is arbitrarily biased for the estimation of ATE._

Proof.: The proof of the CATE and ATE is trivial. The causal estimand and the bias of a proxy-of-confounder-based causal inference model that controls the latent variables \(\bm{Z}\) inferred via \(\bm{Z}=\bar{f}(\bm{X})=\mathbf{B}^{T}\bm{X}\) (where \(\mathbf{B}\) is also a full column-rank matrix) can be formulated as follows:

\[DCEV(\mathbf{B}^{T}\bm{x}) =\mathbb{E}[Y|T=1,\bm{Z}=\mathbf{B}^{T}\bm{x}]-\mathbb{E}[Y|T=0, \bm{Z}=\mathbf{B}^{T}\bm{x}]\] (18) \[=\mathbb{E}[Y|T=1,\bm{Z}=\bm{\alpha}_{X}+\mathbf{B}^{T}\mathbf{A} [\bm{c}||\bm{m}]]-\mathbb{E}[Y|T=0,\bm{Z}=\bm{\alpha}_{X}+\mathbf{B}^{T} \mathbf{A}[\bm{c}||\bm{m}]]\] \[\overset{(a)}{=}\mathbb{E}[Y|T=1,\bm{C}=\bm{c},\bm{M}=\bm{m}]- \mathbb{E}[Y|T=0,\bm{C}=\bm{c},\bm{M}=\bm{m}]\] \[=\alpha_{Y}+\tau\cdot 1+\sum\theta_{j}\cdot\mathbb{E}[U_{j}|T=1, \bm{C}=\bm{c},\bm{M}=\bm{m}]+\sum\kappa_{i}\cdot\mathbb{E}[C_{i}|T=1,\bm{C}= \bm{c},\bm{M}=\bm{m}]\] \[-\alpha_{Y}+\tau\cdot 0+\sum\theta_{j}\cdot\mathbb{E}[U_{j}|T=0, \bm{C}=\bm{c},\bm{M}=\bm{m}]+\sum\kappa_{i}\cdot\mathbb{E}[C_{i}|T=0,\bm{C}= \bm{c},\bm{M}=\bm{m}]\] \[=\tau\cdot(1-0)+\sum\theta_{j}\cdot(\phi_{j}^{-1}\cdot(m_{j}- \alpha_{M}-\gamma_{j})-\phi_{j}^{-1}\cdot(m_{j}-\alpha_{M}))+\sum\kappa_{i} \cdot(c_{i}-c_{i})\] \[=\tau-\sum\frac{\theta_{j}\cdot\gamma_{j}}{\phi_{j}}=\mathbb{E} \left[\tau-\sum\frac{\theta_{j}\cdot\gamma_{j}}{\phi_{j}}\right]=\mathbb{E}[ DCEV(\mathbf{B}^{T}\bm{X})],\]

where step (a) and the rest of the proof follow the same logic as the proof in Section 3.3.

### Proof of Theorem 4.1

The strict definitions of the exponential family, strong exponential (which is assumed for the factorized part of the conditional prior), and identifiability follow [19, 26], and can be referred to in Appendix E, F of [26], which we omit to avoid redundancy. The proof of Theorem 4.1 is largely based on the NF-iVAE paper [26], where most of the details can be found, with the new assumption introduced in CiVAE that each \(\bm{S}_{f,i}\) has at least one invertible dimension incorporated to ensure that each dimension of the inferred latent variables is a bijective transformation of the corresponding true latent variable.

#### c.4.1 Part I

**Step I**. In this step, we transform the equality of noisy conditional marginal distribution of \(\bm{X}\) given \(Y,T\) of two models with parameter \(\bm{\theta},\tilde{\bm{\theta}}\in\Theta\) into the equality of noise-free distributions.

\[\begin{split}& p_{\bm{\theta}}(\bm{X}\mid Y,T)=p_{\tilde{\bm{ \theta}}}(\bm{X}\mid Y,T)\\ &\implies\int_{\mathcal{Z}}p_{f}(\bm{X}\mid\bm{Z})p_{\bm{S},\bm{ \lambda}}(\bm{Z}\mid Y,T)d\bm{Z}=\int_{\mathcal{Z}}p_{\tilde{f}}(\bm{X}\mid \bm{Z})p_{\tilde{\bm{S}},\tilde{\bm{\lambda}}}(\bm{Z}\mid Y,T)d\bm{Z}\\ &\implies\int_{\mathcal{Z}}p_{\bm{e}}(\bm{X}-f(\bm{Z}))p_{\bm{S},\bm{\lambda}}(\bm{Z}\mid Y,T)d\bm{Z}=\int_{\mathcal{Z}}p_{\bm{e}}(\bm{X}- \tilde{f}(\bm{Z}))p_{\tilde{\bm{S}},\tilde{\bm{\lambda}}}(\bm{Z}\mid Y,T)d\bm {Z}\\ &\overset{(a)}{\implies}\int_{\mathcal{X}}p_{\bm{e}}(\bm{X}-\bm{ \overline{X}})p_{\bm{S},\bm{\lambda}}\left(f^{\dagger}(\bm{\overline{X}}) \mid Y,T\right)\operatorname{vol}\left(\mathbf{J}_{f^{\dagger}}(\bm{\overline{X}} )\right)d\bm{\overline{X}}=\\ &\qquad\int_{\mathcal{X}}p_{\bm{e}}(\bm{X}-\bm{\overline{X}})p_{ \tilde{\bm{S}},\tilde{\bm{\lambda}}}\left(\tilde{f}^{\dagger}(\bm{\overline{X}} )\mid Y,T\right)\operatorname{vol}\left(\mathbf{J}_{\tilde{f}^{\dagger}}(\bm{ \overline{X}})\right)d\bm{\overline{X}}\\ &\overset{(b)}{\implies}\int_{\mathbb{R}^{d}}p_{\bm{e}}(\bm{X}- \bm{\overline{X}})\tilde{p}_{f,\bm{S},\bm{\lambda},Y,T}(\bm{\overline{X}})d \bm{\overline{X}}=\int_{\mathbb{R}^{d}}p_{\bm{e}}(\bm{X}-\bm{\overline{X}}) \tilde{p}_{\tilde{f},\tilde{\bm{S}},\tilde{\bm{\lambda}},\tilde{Y},\tilde{T}}( \bm{\overline{X}})d\bm{\overline{X}}\\ &\implies\left(\tilde{p}_{f,\bm{S},\bm{\lambda},Y,T}\ast p_{\bm{e}} \right)(\bm{X})=\left(\tilde{p}_{\tilde{f},\tilde{\bm{S}},\tilde{\bm{\lambda} },\tilde{Y},\tilde{T}}\ast p_{\bm{\epsilon}}\right)(\bm{X})\\ &\overset{(c)}{\implies}F\left[\tilde{p}_{f,\bm{S},\bm{\lambda},Y,T}\right](\bm{\omega})\varphi_{\bm{\epsilon}}(\bm{\omega})=F\left[\tilde{p}_{ \tilde{f},\tilde{\bm{S}},\tilde{\bm{\lambda}},\tilde{Y},\tilde{T}}\right](\bm{ \omega})\varphi_{\bm{\epsilon}}(\bm{\omega})\\ &\overset{(d)}{\implies}F\left[\tilde{p}_{f,\bm{S},\bm{\lambda},Y,T}\right](\bm{\omega})=F\left[\tilde{p}_{\tilde{f},\tilde{\bm{S}},\tilde{\bm{ \lambda}},\tilde{Y},\tilde{T}}\right](\bm{\omega})\\ &\implies\tilde{p}_{f,\bm{S},\bm{\lambda},Y,T}(\bm{X})=\tilde{p}_{ \tilde{f},\tilde{\bm{S}},\tilde{\bm{\lambda}},\tilde{Y},\tilde{T}}(\bm{X}).\end{split}\] (19)Step (a) is based on the rule of change-of-variable, where \(\operatorname{vol}(\mathbf{A})=\sqrt{\det\left(\mathbf{A}^{T}\mathbf{A}\right)}\). In step (b), we define \(\tilde{p}_{f,\mathcal{S},\mathbf{A},Y,T}(\bm{X})\triangleq p_{\mathcal{S}, \mathbf{A}}\left(f^{\dagger}(\bm{X})\mid Y,T\right)\operatorname{vol}\left( \mathbf{J}_{f^{\dagger}}(\bm{X})\right)\mathbb{I}_{\mathcal{X}}(\bm{X})\). In step (c), we use \(F[\cdot]\) to denote the Fourier transform. In step (d), we drop \(\varphi_{\bm{\varepsilon}}(\bm{\omega})\) as it is non-zero _a.e._ (see Assumption 3).

**Step II**. In this step, we transform the equality of the noise-free distributions into the relationship of the sufficient statistics \(\bm{S}\) and \(\bm{\tilde{S}}\). By taking logarithm of both sides of Eq. (19), we have:

\[\begin{split}&\log\operatorname{vol}\left(J_{f^{\dagger}}(\bm{X}) \right)+\log\mathcal{Q}\left(f^{\dagger}(\bm{X})\right)-\log\mathcal{C}(Y,T) +\left\langle\bm{S}\left(f^{\dagger}(\bm{X})\right),\bm{\lambda}(Y,T)\right\rangle \\ &=\log\operatorname{vol}\left(J_{\tilde{f}^{\dagger}}(\bm{X}) \right)+\log\tilde{\mathcal{Q}}\left(\tilde{f}^{\dagger}(\bm{X})\right)- \log\tilde{\mathcal{C}}(Y,T)+\left\langle\tilde{\bm{S}}\left(\tilde{f}^{ \dagger}(\bm{X})\right),\tilde{\bm{\lambda}}(Y,T)\right\rangle.\end{split}\] (20)

Let \((Y,T)_{0},\cdots,(Y,T)_{k}\) be the \(k+1\) distinct points defined in Assumption 3 - (iv). We obtain \(k+1\) equations by evaluating the Eq. (20) at these points, where the first equation is subtracted from the remaining ones, which leads to the following equation system:

\[\begin{split}&\left\langle\bm{S}\left(f^{\dagger}(\bm{X}) \right),\bm{\lambda}\left((Y,T)_{l}\right)\;-\bm{\lambda}\left((Y,T)_{0} \right)\right\rangle+\log\frac{\mathcal{C}\left((Y,T)_{0}\right)}{\mathcal{C} \left((Y,T)_{l}\right)}\\ &=\left\langle\tilde{\bm{S}}\left(\tilde{f}^{\dagger}(\bm{X}) \right),\tilde{\bm{\lambda}}\left((Y,T)_{l}\right)-\tilde{\bm{\lambda}}\left( (Y,T)_{0}\right)\right\rangle+\log\frac{\tilde{\mathcal{C}}\left((Y,T)_{0} \right)}{\tilde{\mathcal{C}}\left((Y,T)_{l}\right)},\quad l=1,\cdots,k.\end{split}\] (21)

Let \(\mathbf{L}\) be the invertible matrix defined in Assumption 3 - (iv) and \(\mathbf{\tilde{L}}\) be the counterpart for \(\bm{\tilde{\lambda}}\), if we summarize all terms irrelevant to \(\bm{X}\) into a constant \(\bm{b}\),we have:

\[\begin{split}&\mathbf{L}^{T}\bm{S}\left(f^{\dagger}(\bm{X}) \right)=\tilde{\mathbf{L}}^{T}\tilde{\bm{S}}\left(\tilde{f}^{\dagger}(\bm{X}) \right)+\bm{b}\\ &\Longrightarrow\bm{S}\left(f^{\dagger}(\bm{X})\right)=\mathbf{A }\tilde{\bm{S}}\left(\tilde{f}^{\dagger}(\bm{X})\right)+\bm{c},\end{split}\] (22)

where \(\mathbf{A}=\mathbf{L}^{-T}\tilde{\mathbf{L}}\in\mathbb{R}^{k\times k}\), and \(\bm{c}=\mathbf{L}^{-T}\bm{b}\in\mathbb{R}^{k}\).

**Step III**. Ideally, to prove the element-wise bijective identifiability of the latent variables \(\bm{Z}\), the transformation of the sufficient statistics \(\bm{S}\) derived in Eq. (22) should be bijective. We claim that if the conditional prior \(p_{\mathcal{S},\mathbf{A}}(\bm{Z}\mid Y,T)\) is strongly exponential and \(\mathbf{L}\) is invertible, \(\tilde{\mathbf{L}}\) and \(\mathbf{A}\) must also be invertible. The proof is omitted, and can be referred to in Appendix H.1.1 of [26].

#### c.4.2 Part II

In this part, we prove that, if Assumptions 1, 2 and 3 hold, we can identify the factorized part of the sufficient statistics \(\bm{S}(\bm{Z})\), i.e., \(\bm{S}_{f}(\bm{Z})\), up to permutation and element-wise transformation. Specifically, if we use \(\bm{v}\) to denote the composite map \(\tilde{f}^{\dagger}\circ f:\mathcal{Z}\to\mathcal{Z}\), Eq. (22) can be rewritten into:

\[\bm{S}(\bm{Z})=\mathbf{A}\tilde{\bm{S}}(\bm{v}(\bm{Z}))+\bm{c}.\] (23)

We aim to prove that \(\mathbf{A}\) in Eq. (23) is a block permutation matrix.

**Step I**. We start by showing that \(\bm{v}\) is a component-wise function. If we differentiate both sides of Eq. (23) with respect to \(Z_{s}\) and \(Z_{t}\), where \(s\neq t\), we have:

\[\begin{split}&\frac{\partial\bm{S}(\bm{Z})}{\partial Z_{s}}=\mathbf{A} \sum_{i=1}^{K_{\mathcal{Z}}}\frac{\partial\tilde{\bm{S}}(\bm{v}(\bm{Z}))}{ \partial v_{i}(\bm{Z})}\cdot\frac{\partial v_{i}(\bm{Z})}{\partial Z_{s}}\\ &\frac{\partial^{2}\bm{S}(\bm{Z})}{\partial Z_{s}\partial Z_{t}}= \mathbf{A}\sum_{i=1}^{K_{\mathcal{Z}}}\sum_{i=1}^{K_{\mathcal{Z}}}\frac{ \partial^{2}\tilde{\bm{S}}(\bm{v}(\bm{Z}))}{\partial v_{i}(\bm{Z})\partial v_{ j}(\bm{Z})}\cdot\frac{\partial v_{j}(\bm{Z})}{\partial Z_{t}}\cdot\frac{ \partial v_{i}(\bm{Z})}{\partial z_{s}}+\mathbf{A}\sum_{i=1}^{K_{\mathcal{Z}}} \frac{\partial\tilde{\bm{S}}(\bm{v}(\bm{Z}))}{\partial v_{i}(\bm{Z})}\cdot \frac{\partial^{2}v_{i}(\bm{Z})}{\partial Z_{s}\partial Z_{t}}.\end{split}\] (24)

Note that for the factorized part of the sufficient statistics \(\bm{S}\), i.e., \(\bm{S}_{f}\), all _cross-derivatives_ are zero, and for the non-factorized part of \(\bm{S}\), i.e., \(\bm{S}_{nf}\), which is a neural network with ReLU activation (i.e., linear _a.e._), all _second-order derivatives_ are zero. Therefore, the _second order cross-derivatives_ on the LHS. of Eq. (24) are zero, which leads to the following equality:

\[\bm{0}=\mathbf{A}\sum_{i=1}^{K_{\mathcal{Z}}}\frac{\partial^{2}\tilde{\bm{S}}(\bm {v}(\bm{Z}))}{\partial v_{i}(\bm{Z})^{2}}\cdot\frac{\partial v_{i}(\bm{Z})}{ \partial Z_{t}}\cdot\frac{\partial v_{i}(\bm{Z})}{\partial Z_{s}}+\mathbf{A} \sum_{i=1}^{K_{\mathcal{Z}}}\frac{\partial\tilde{\bm{S}}(\bm{v}(\bm{Z}))}{ \partial v_{i}(\bm{Z})}\cdot\frac{\partial^{2}v_{i}(\bm{Z})}{\partial Z_{s} \partial Z_{t}}.\] (25)Eq. (25) can be written into the matrix-vector product form as follows:

\[\bm{0}=\mathbf{A}\tilde{\bm{S}}^{\prime\prime}(\bm{Z})\bm{v}_{s,t}^{\prime}(\bm{Z })+\mathbf{A}\tilde{\bm{S}}^{\prime}(\bm{Z})\bm{v}_{s,t}^{\prime\prime}(\bm{Z}),\] (26)

where

\[\tilde{\bm{S}}^{\prime\prime}(\bm{Z})=\left[\frac{\partial^{2}\tilde{\bm{S}}( \bm{v}(\bm{Z}))}{\partial v_{1}(\bm{Z})^{2}},\cdots,\frac{\partial^{2}\tilde{ \bm{S}}(\bm{v}(\bm{Z}))}{\partial v_{K_{Z}}(\bm{Z})^{2}}\right]\in\mathbb{R}^{ k\times K_{Z}},\]

\[\bm{v}_{s,t}^{\prime}(\bm{Z})=\left[\frac{\partial v_{1}(\bm{Z})}{\partial Z _{t}}\cdot\frac{\partial v_{1}(\bm{Z})}{\partial Z_{s}},\cdots,\frac{\partial v _{K_{Z}}(\bm{Z})}{\partial Z_{t}}\cdot\frac{\partial v_{K_{Z}}(\bm{Z})}{ \partial Z_{s}}\right]^{T}\in\mathbb{R}^{K_{Z}},\]

and

\[\tilde{\bm{S}}^{\prime}(\bm{Z})=\left[\frac{\partial\tilde{\bm{S}}(\bm{v}( \bm{Z}))}{\partial v_{1}(\bm{Z})},\cdots,\frac{\partial\tilde{\bm{S}}(\bm{v}( \bm{Z}))}{\partial v_{K_{Z}}(\bm{Z})}\right]\in\mathbb{R}^{k\times K_{Z}},\]

\[\bm{v}_{s,t}^{\prime\prime}(\bm{Z})=\left[\frac{\partial^{2}v_{1}(\bm{Z})}{ \partial Z_{s}\partial Z_{t}},\cdots,\frac{\partial^{2}v_{K_{Z}}(\bm{Z})}{ \partial Z_{s}\partial Z_{t}}\right]^{T}\in\mathbb{R}^{K_{Z}}.\]

If we denote the concatenation as \(\tilde{\bm{S}}^{\prime\prime\prime}(\bm{Z})=\left[\tilde{\bm{S}}^{\prime \prime}(\bm{Z}),\tilde{\bm{S}}^{\prime}(\bm{Z})\right]\in\mathbb{R}^{k\times 2K _{Z}}\) and \(\bm{v}_{s,t}^{\prime\prime}(\bm{Z})=\left[\bm{v}_{s,t}^{\prime}(\bm{Z})^{T}, \bm{v}_{s,t}^{\prime\prime}(\bm{Z})^{T}\right]^{T}\in\mathbb{R}^{2K_{z}}\), we have:

\[\bm{0}=\mathbf{A}\tilde{\bm{S}}^{\prime\prime\prime}(\bm{Z})\bm{v}_{s,t}^{ \prime\prime\prime}(\bm{Z}).\] (27)

Finally, if we denote the rows of \(\tilde{\bm{S}}^{\prime\prime\prime}(\bm{Z})\) that correspond to the factorized part of \(\bm{S}\) by \(\tilde{\bm{S}}_{f}^{\prime\prime\prime}(\bm{Z})\), according to Lemma 5 of the iVAE paper [19] and the assumption that \(k\geq 2K_{Z}\), we have that the rank of \(\tilde{\bm{S}}_{f}^{\prime\prime\prime}(\bm{Z})\) is \(2K_{Z}\). Since \(k\geq 2K_{Z}\), the rank of \(\tilde{\bm{S}}_{f}^{\prime\prime\prime}(\bm{Z})\) is also \(2K_{Z}\). Since the rank of \(\mathbf{A}\) is \(k\), the rank of \(\mathbf{A}\tilde{\bm{S}}^{\prime\prime\prime}(\bm{Z})\) is \(2K_{Z}\), which implies that \(\bm{v}_{s,t}^{\prime\prime\prime}(\bm{Z})\in\mathbb{R}^{2K_{Z}}\) is a zero vector. Therefore, we have \(\bm{v}_{s,t}^{\prime}(\bm{Z})=\bm{0},\forall s\neq t\), and we have demonstrated that \(\bm{v}\) is a component-wise function.

**Step II.** Based on **Step I**, we demonstrate that \(\mathbf{A}\) is a block permutation matrix. Without loss of generality, we assume that the permutation in \(\bm{v}\) is Identity, where \(\bm{v}(\bm{Z})=\left[v_{1}\left(Z_{1}\right),\cdots,v_{K_{Z}}\left(Z_{K_{Z}} \right)\right]^{T}\) and each \(v_{i}\) is a nonlinear univariate scalar function. Since \(f\) and \(\tilde{f}\) are injective, \(\bm{v}\) is bijective and \(\bm{v}^{-1}(\bm{Z})=\left[v_{1}^{-1}\left(Z_{1}\right),\cdots,v_{K_{Z}}^{-1} \left(Z_{K_{Z}}\right)\right]^{T}\). If we denote \(\widetilde{\bm{S}}(\bm{v}(\bm{Z}))=\tilde{\bm{S}}(\bm{v}(\bm{Z}))+\mathbf{A}^{- 1}\bm{c}\), Eq. (23) can be reformulated as \(\bm{S}(\bm{Z})=\mathbf{A}\widetilde{\bm{S}}(\bm{v}(\bm{Z}))\). We then apply \(\bm{v}^{-1}\) to \(\bm{Z}\) on both sides, which gives

\[\bm{S}\left(\bm{v}^{-1}(\bm{Z})\right)=\mathbf{A}\widetilde{\bm{S}}(\bm{Z}).\] (28)

Let \(t\) be the index of an entry in \(\bm{S}\) that corresponds to the factorized part \(\bm{S}_{f}\). For all \(s\neq t\), we have:

\[0=\frac{\partial\bm{S}\left(\bm{v}^{-1}(\bm{Z})\right)_{t}}{\partial Z_{s}}= \sum_{j=1}^{k}a_{tj}\frac{\partial\widetilde{\bm{S}}(\bm{Z})_{j}}{\partial Z_{s }}.\] (29)

Since the entries of \(\tilde{\bm{S}}\) are linearly independent, \(a_{tj}\) is zero for any \(j\) such that \(\frac{\partial\widetilde{\bm{S}}(\bm{Z})_{j}}{\partial Z_{s}}\neq 0\). This includes the entries \(S_{j}\) that correspond to (1) the factorized part that does not depend on \(Z_{t}\); and (2) the non-factorized part \(\bm{S}_{nf}\). Therefore, when \(t\) is the index of an entry in the sufficient statistics \(\bm{S}\) that corresponds to factor \(i\) in the factorized part \(\bm{S}_{f}\), i.e., \(\bm{S}_{f,i}\), the only non-zero \(a_{tj}\) are the ones that map between \(\bm{S}_{f,i}\left(Z_{i}\right)\) and \(\overline{\bm{S}}_{f,i}\left(v_{i}\left(Z_{i}\right)\right)\). Therefore, we can construct an invertible submatrix \(\mathbf{A}_{i}^{\prime}\) with all non-zero elements \(a_{tj}\) for all \(t\) that corresponds to factor \(i\), such that

\[\bm{S}_{f,i}\left(Z_{i}\right)=\mathbf{A}_{i}^{\prime}\overline{\bm{S}}_{f,i} \left(v_{i}\left(Z_{i}\right)\right)=\mathbf{A}_{i}^{\prime}\tilde{\bm{S}}_{f,i} \left(v_{i}\left(Z_{i}\right)\right)+\bm{c}_{i},\quad i=1,\cdots,K_{Z},\] (30)

where \(\bm{c}_{i}\) denotes the corresponding elements of \(\bm{c}\). Eq. (30) means that for each \(i=1,\cdots,K_{Z}\), the matrix block \(\mathbf{A}_{i}^{\prime}\) of \(\mathbf{A}\) affinely transforms the \(i\)-specific sufficient statistics vector \(\bm{S}_{f,i}\left(Z_{i}\right)\) into \(\tilde{\bm{S}}_{f,i}\left(v_{i}\left(Z_{i}\right)\right)\). In addition, there is also an additional block \(\mathbf{A}^{\prime}\) that affinely transforms \(\bm{S}_{nf}(\bm{Z})\) in into \(\bm{S}_{nf}(v(\bm{Z}))\). This completes the proof that \(\mathbf{A}\) is a block permutation matrix.

#### c.4.3 Part III

Let \(\tilde{Z}_{i}=v_{i}\left(Z_{i}\right)=\tilde{f}^{\dagger}(\bm{X})_{i}\) be the \(i\)th inferred latent variable. Assume again that the permutation in \(\bm{v}\) is Identity. In this part, we prove that if Assumption 2 holds, each inferred latent variable \(\tilde{Z}_{i}\) is the bijective transformation of the true latent variable. The proof is as follows.

Proof.: Plugging \(\tilde{Z}_{i}\) into Eq. (30), we have:

\[\bm{S}_{f,i}(Z_{i})=\bm{A}_{i}^{\prime}\bar{\bm{S}}_{f,i}(\tilde{Z}_{i}).\] (31)

According to Assumption 2, there exists one dimension of \(\bm{S}_{f,i}\), i.e., \(j\), such that \(S_{f,ij}\) is bijective. This implies that \(\bm{S}_{f,i}\) is injective, and therefore it has a left-inverse \(\bm{S}_{f,i}^{\dagger}\). we apply \(\bm{S}_{f,i}^{\dagger}\) to both sides of Eq. (31), which gives:

\[Z_{i}=\bm{S}_{f,i}^{\dagger}\bm{A}_{i}^{\prime}\bar{\bm{S}}_{f,i}(\tilde{Z}_{ i}).\] (32)

Since \(\bm{A}_{i}^{\prime}\) is a block of an invertible block permutation matrix, \(\bm{A}_{i}\) is also an invertible matrix, and therefore \(\bm{A}_{i}^{\prime}\) is a bijective mapping. In addition, since \(\tilde{\bm{S}}_{f,i}\) is injective, \(\tilde{\bm{S}}_{f,i}\) is also injective, and therefore the composite map \(\bm{S}_{f,i}^{\dagger}\bm{A}_{i}^{\prime}\bar{\bm{S}}_{f,i}:\mathbb{R}\to \mathbb{R}\) that applies on \(\tilde{Z}_{i}\) is a bijective. This completes the proof that each inferred latent variable \(\tilde{Z}_{i}\) is the bijective transformation of the true latent variable in the case of no noise, where \(\bm{Z}=f^{\dagger}(\bm{X})\) are the true latent variables. If noise \(\bm{\varepsilon}\) exists, the posterior distribution of the latent variables can be identified up to an analogous bijective indeterminacy. 

#### c.4.4 Consistency

Proof.: If the family of the variational posterior \(q_{\bm{\phi}}(\bm{Z}|\bm{X},Y,T)\) contains the true posterior \(p_{\bm{\theta}}(\bm{Z}|\bm{X},Y,T)\), then by optimizing the loss of Eq. (9) (with the KL term replaced by the score matching loss defined in Eq. (10)) over its parameter \(\bm{\phi}\), the score matching term will eventually vanish. Therefore, the ELBO term in Eq. (9) will be equal to the log-likelihood. Under this circumstance, CiVAE inherits all the properties of maximum likelihood estimation (MLE). Since the identifiability of CiVAE is guaranteed up to permutation and component-wise bijective transformation of the latent variables, the consistency property of MLE means that the model will converge to the true parameter \(\bm{\theta}^{*}\) up to such mild indeterminacy of the latent variables in the limit of infinite data. 

### Proof of Theorem 4.2

Proof.: Let \(\bm{C}\) be the true latent confounders and \(\tilde{\bm{C}}\) be the transformed confounders, where the transformation function \(f\) is bijective and differentiable _a.e._ Let \(f^{-1}\) denote its inverse. The ATE estimator that controls transformed confounders \(\tilde{\bm{C}}\) can be formulated as:

\[DEV(\tilde{\bm{C}})=\mathbb{E}_{p(\tilde{\bm{C}})}[\mathbb{E}[Y|T=1,\tilde{ \bm{C}}=\tilde{\bm{c}}]-\mathbb{E}[Y|T=0,\tilde{\bm{C}}=\tilde{\bm{c}}]].\] (33)

Specifically, for the continuous case where density functions exist, for each term, we have:

\[\mathbb{E}_{p(\tilde{\bm{C}})}[\mathbb{E}[Y|T=t,\tilde{\bm{C}}=\tilde{\bm{c}}]] =\int f_{\tilde{\bm{C}}}(\tilde{\bm{c}})\int y\cdot f_{Y|T,\tilde{\bm{C}}}(y| t,\tilde{\bm{c}})dyd\tilde{\bm{c}}.\] (34)

For the marginal density \(f_{\tilde{\bm{C}}}(\tilde{\bm{c}})\), the following equality holds:

\[f_{\tilde{\bm{C}}}(\tilde{\bm{c}})=f_{\bm{C}}(f^{-1}(\tilde{\bm{c}}))|J_{f^{- 1}}(\tilde{\bm{c}})|=f_{\bm{C}}(\bm{c})|J_{f^{-1}}(\tilde{\bm{c}})|.\] (35)

As for the conditional density \(f_{Y|T,\tilde{\bm{C}}}(y|t,\tilde{\bm{c}})\), since \(f\) is bijective, according to Eq. (12), we have:

\[f_{Y|T,\tilde{\bm{C}}}(y|t,\tilde{\bm{c}})=f_{Y|T,\bm{C}}(y|t,\bm{c}).\] (36)

Combining Eqs. (35) and (36), and given that \(d\tilde{\bm{c}}=|J_{f}(\bm{c})|d\bm{c}\), we have:

\[\begin{split}(\ref{eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eqwhere the term \(|J_{f^{-1}}(\tilde{\bm{c}})|\cdot|J_{f}(\bm{c})|\) vanishes in step (a) as the two factors have the product of one. Therefore, if we plug Eq. (37) into Eq. (33), it leads to the following equality:

\[\begin{split} DEV(\bm{\tilde{C}})&=\mathbb{E}_{p( \bm{\tilde{C}})}[\mathbb{E}[Y|T=1,\bm{\tilde{C}}=\bm{\tilde{c}}]-\mathbb{E}[Y|T =0,\bm{\tilde{C}}=\bm{\tilde{c}}]]\\ &=\mathbb{E}_{p(\bm{C})}[\mathbb{E}[Y|T=1,\bm{C}=\bm{c}]-\mathbb{E }[Y|T=0,\bm{C}=\bm{c}]]=DEV(\bm{C})=ATE,\end{split}\] (38)

where the last step is due to Eq. (2) in Definition 2, which completes our proof that controlling bijectively transformed confounders provides an unbiased estimation of ATE. 

## Appendix D Extending CiVAE to address Latent Interactions

In this section, we extend CiVAE to more general cases where interactions exist among the latent confounders \(\bm{C}\) and the latent post-treatment variables \(\bm{M}\). Here, we note that the identification of latent confounders \(\bm{C}\) in CiVAE is achieved in two steps. _(i)_ CiVAE _individually_ identifies latent variables \([\bm{C},\bm{M}]\) that generate \(\bm{X}\) in inferred \(\bm{Z}\) (but which dims of \(\bm{Z}\) correspond to \(\bm{C}\) or \(\bm{M}\) is unknown). _(ii)_ pairwise independence test to identify \(\bm{C}\). Since Assumption 2 allows arbitrary dependence among \(\bm{C}\) and \(\bm{M}\), step _(i)_ still holds when interactions among \([\bm{C},\bm{M}]\) exist. To distinguish \(\bm{C}\) in these cases, we can use more general causal discovery algorithms, e.g., the PC algorithm [18] in the second step. In this section, we consider two cases of interaction: _(i)_ Intra-Interaction among mediators, and _(ii)_ Inter-Interaction among mediators and confounders.

### Intra-Interactions among Latent Mediators

In this subsection, we discuss the case where latent post-treatment variables \(\bm{M}\) interact with each other. Since in this case, \(\bm{M}\) cannot causally influence the latent confounders \(\bm{C}\) (otherwise \(\bm{C}\) will be post-treatment), and the PC algorithm orients edges in causal graphs via colliders, latent confounders can still be identified from the inferred \(\bm{Z}\) as they form colliders with the treatment \(T\).

To empirically verify the claim, we extend the simulated datasets described in Section 5.1, where we make _(i)_\(T\) directly affects \(M_{1}\), _(ii)_\(M_{1}\) affects \(M_{2}\), and _(iii)_\(M_{1}\), \(M_{2}\) affect \(M_{3}\). The coefficients are randomly sampled from \(\mathcal{N}(0,1/3)\). In step _(ii)_, we use the PC algorithm [18] to identify \(\bm{C}\) from the inferred \(\bm{Z}\). The results in Table 2 demonstrate that the adapted CiVAE is still significantly more robust to latent post-treatment bias compared to CEVAE and TEDVAE, which empirically verify our claim that PC-adapted CiVAE can address the interaction among post-treatment variables.

### Inter-Interactions between Latent Mediators and Latent Confounders

In this subsection, we discuss another case where inter-interactions exist between latent confounders \(\bm{C}\) and latent post-treatment variables \(\bm{M}\). Since in this case, \(\bm{M}\) still cannot causally influence \(\bm{C}\) (otherwise \(\bm{C}\) will be post-treatment), and the PC algorithm orients edges in causal graph via colliders, latent confounders \(\bm{C}\) can still be identified from \(\bm{Z}\) as they form colliders with the treatment \(T\).

\begin{table}
\begin{tabular}{l||c c|c c||c c|c c} \hline Dataset & LatentMediator & LatentCorrelator & Company (Age) & Company (Gender) \\ \hline Method & ATE. & Err. & ATE. & Err. & ATE. & Err. & ATE. & Err. \\ \hline \hline CEVAE & 1.627 \(\pm\) 0.549 & 2.627 & 2.659 \(\pm\) 0.302 & 1.353 & 0.152 \(\pm\) 0.027 & 0.420 & -0.225 \(\pm\) 0.044 & -0.144 \\ TEDVAE & 1.653 \(\pm\) 0.511 & 2.042 & 2.827 \(\pm\) 0.259 & 1.521 & 0.180 \(\pm\) 0.047 & 0.448 & -0.189 \(\pm\) 0.012 & -0.108 \\ \hline CiVAE & **-0.350**\(\pm\) 0.695 & **1.785**\(\pm\) 0.481 & **0.479** & **-0.073**\(\pm\)0.101 & **0.195** & **-0.136**\(\pm\) 0.087 & **-0.055** \\ \hline True ATE & -1.000 \(\pm\) 0.000 & 0.000 & 1.306 \(\pm\) 0.000 & 0.000 & -0.268 \(\pm\) 0.000 & 0.000 & -0.081 \(\pm\) 0.000 & 0.000 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of CiVAE with baselines when intra-interactions among \(\bm{M}\) exist.

\begin{table}
\begin{tabular}{l||c c|c c||c c|c c} \hline Dataset & LatentMediator & LatentCorrelator & Company (Age) & Company (Gender) \\ \hline Method & ATE. & Err. & ATE. & Err. & ATE. & Err. & ATE. & Err. \\ \hline \hline CEVAE & 2.070 \(\pm\) 0.279 & 3.070 & 2.831 \(\pm\) 0.398 & 1.831 & 0.094 \(\pm\) 0.061 & 0.362 & -0.192 \(\pm\) 0.015 & -0.111 \\ TEDVAE & 1.743 \(\pm\) 0.307 & 2.743 & 2.954 \(\pm\) 0.763 & 1.954 & 0.109 \(\pm\) 0.116 & 0.377 & -0.212 \(\pm\) 0.019 & -0.131 \\ \hline CiVAE & **-0.716**\(\pm\) 0.523 & **0.284** & **1.385**\(\pm\) 0.660 & **0.385** & **-0.041**\(\pm\)0.144 & **0.227** & **-0.129**\(\pm\) 0.064 & **-0.048** \\ \hline True ATE & -1.000 \(\pm\) 0.000 & 0.000 & 1.000 \(\pm\) 0.000 & 0.000 & -0.268 \(\pm\) 0.000 & 0.000 & -0.081 \(\pm\) 0.000 & 0.000 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of CiVAE with baselines when inter-interactions between \(\bm{C}\) and \(\bm{M}\) exist.

To verify the claim, we extend the simulated datasets described in Section 5.1 to allow each latent confounder \(C_{i}\in\mathbb{R}^{3}\) to determine \(\bm{M}\in\mathbb{R}^{3}\). The coefficients are randomly sampled from \(\mathcal{N}(0,1/3)\). In step _(ii)_, we use the PC algorithm to identify \(\bm{C}\) from the inferred \(\bm{Z}\). The results in Table 3 demonstrate that the PC-adapted CiVAE is still significantly more robust to latent post-treatment bias compared to CEVAE and TEDVAE, which empirically verify our claim that PC-adapted CiVAE can address the case where inter-interactions exist among latent confounders and post-treatment variables.

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The contribution of this paper can be summarized as: We study a critical but easily overlooked problem in causal effect estimation: latent post-treatment bias, and we propose a novel framework, i.e., CiVAE, to address the bias. The details are in Section 4.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have discussed the potential issue of the vanilla when interactions among the latent variables exists. However, in Section D we have addressed the issue by extendeding our framework.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have introduced the three mild assumptions required for the identification of causal effects under latent post-treatment bias. In addition, we have provided the proof for all the theorems in the Appendix.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided implementation details in Section 5.1. In addition, we have provided a code demo in an anonymous URL.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: See Checklist 4.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Checklist 4.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We have reported the error of five independent run for both the proposed CiVAE and all the baselines in the main paper.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Checklist 4.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have carefully read the code of ethics and behaved strictly according to it.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Section A of the Appendix.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our model does not have a high risk for misuse.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the papers of our baselines and honor their license of code.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide the Readme file along side the codes.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No human subjects are involved in our experiments.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No human subjects are involved in our experiments.