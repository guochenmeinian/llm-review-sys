# Learning with Fitzpatrick Losses

Seta Rakotomandimby

Ecole des Ponts

seta.rakotomandimby@enpc.fr

&Jean-Philippe Chancelier

Ecole des Ponts

jean-philippe.chancelier@enpc.fr

Michel De Lara

Ecole des Ponts

michel.delara@enpc.fr

&Mathieu Blondel

Google DeepMind

mblondel@google.com

###### Abstract

Fenchel-Young losses are a family of loss functions, encompassing the squared, logistic and sparsemax losses, among others. They are convex w.r.t. the model output and the target, separately. Each Fenchel-Young loss is implicitly associated with a link function, that maps model outputs to predictions. For instance, the logistic loss is associated with the soft argmax link function. Can we build new loss functions associated with the same link function as Fenchel-Young losses? In this paper, we introduce Fitzpatrick losses, a new family of separately convex loss functions based on the Fitzpatrick function. A well-known theoretical tool in maximal monotone operator theory, the Fitzpatrick function naturally leads to a refined Fenchel-Young inequality, making Fitzpatrick losses tighter than Fenchel-Young losses, while maintaining the same link function for prediction. As an example, we introduce the Fitzpatrick logistic loss and the Fitzpatrick sparsemax loss, counterparts of the logistic and the sparsemax losses. This yields two new tighter losses associated with the soft argmax and the sparse argmax, two of the most ubiquitous output layers used in machine learning. We study in details the properties of Fitzpatrick losses and, in particular, we show that they can be seen as Fenchel-Young losses using a modified, target-dependent generating function. We demonstrate the effectiveness of Fitzpatrick losses for label proportion estimation.

## 1 Introduction

Loss functions are a cornerstone of statistics and machine learning: they measure the difference, or "loss," between a ground-truth target and a model prediction. As such, they have attracted a wealth of research. Proper losses (a.k.a. proper scoring rules) [17; 16] measure the discrepancy between a target distribution and a probability forecast. They are essentially primal-primal Bregman divergences, with both the target and the prediction belonging to the same primal space. They are typically explicitly composed with a link function [26; 30], in order to map the model output to a prediction. A disadvantage of this explicit composition is that it often makes the resulting composite loss function non-convex. A related family of loss functions are Fenchel-Young losses [7; 8], which encompass many commonly-used loss functions in machine learning including the squared, logistic, sparsemax and perceptron losses. Fenchel-Young losses can be seen as primal-dual Bregman divergences [1], with the target belonging to the primal space and the model output belonging to the dual space. In contrast to proper losses, each Fenchel-Young loss is implicitly associated with a given link function, mapping the dual-space model output to a primal-space prediction (for instance, the soft argmax is the link function associated with the logistic loss). This crucial difference makes Fenchel-Young losses always convex w.r.t. the model output and w.r.t. the target, separately. Can we build new (separately) convex losses associated with the same link function as Fenchel-Young losses?In this paper, we introduce Fitzpatrick losses, a new family of primal-dual (separately) convex loss functions. Our proposal builds upon the Fitzpatrick function, a well-known theoretical object in maximal monotone operator theory [15; 11; 2]. So far, the Fitzpatrick function had been used as a theoretical tool to represent maximal monotone operators [28] and to construct Bregman-like primal-primal divergences [10], but it had not been used to construct primal-dual loss functions for machine learning, as we do. Crucially, the Fitzpatrick function naturally leads to a refined Fenchel-Young inequality, making Fitzpatrick losses tighter than Fenchel-Young losses. Yet, their predictions are produced using the same link function, suggesting that we can use Fitzpatrick losses as a tighter replacement for the corresponding Fenchel-Young losses (Figure 1). We make the following contributions.

* After reviewing some background, we introduce Fitzpatrick losses. They can be thought as a tighter version of Fenchel-Young losses, that use the same link function.
* We instantiate two new loss functions in this family: the Fitzpatrick logistic loss and the Fitzpatrick sparsemax loss. They are the counterparts of the logistic and sparsemax losses, two instances of Fenchel-Young losses. We therefore obtain two new tighter losses for the soft argmax and the sparse argmax, two of the most popular output layers in machine learning.
* We study in detail the properties of Fitzpatrick losses. We show that Fitzpatrick losses are equivalent to Fenchel-Young losses with a modified, target-dependent generating function.
* We demonstrate the effectiveness of Fitzpatrick losses for probabilistic classification on \(11\) datasets.

## 2 Background

### Convex analysis

We denote the non-negative real numbers by \(\mathbb{R}_{+}\coloneqq[0,+\infty)\), the positive real numbers by \(\mathbb{R}_{++}\coloneqq(0,+\infty)\), and the extended real numbers by \(\overline{\mathbb{R}}\coloneqq\mathbb{R}\cup\{-\infty,+\infty\}\). We suppose given a nonzero natural number \(k\). We denote the probability simplex by \(\triangle^{k}\coloneqq\{p\in\mathbb{R}_{+}^{k}\ \big{|}\ \sum_{i=1}^{k}p_{i}=1\}\). We denote the indicator function of a set \(\mathcal{C}\subset\mathbb{R}^{k}\) by \(\iota_{\mathcal{C}}(y)=0\) if \(y\in\mathcal{C}\), \(+\infty\) otherwise. We denote the effective domain of a function \(\Omega\colon\mathbb{R}^{k}\to\overline{\mathbb{R}}\) by \(\ \operatorname{dom}\Omega:=\{y\in\mathbb{R}^{k}\ \big{|}\ \Omega(y)<+\infty\}\). A function \(\Omega\colon\mathbb{R}^{k}\to\overline{\mathbb{R}}\) is said to be **proper** if it never takes the value \(-\infty\) and if \(\operatorname{dom}\Omega\neq\emptyset\). We denote the Euclidean projection onto a nonempty closed convex set \(\mathcal{C}\subset\mathbb{R}^{k}\) by \(P_{\mathcal{C}}(\theta)\), the unique solution \(y^{\prime}\in\mathcal{C}\) of the minimization problem \(\min_{y^{\prime}\in\mathcal{C}}\|y^{\prime}-\theta\|_{2}^{2}\).

Figure 1: We introduce **Fitzpatrick losses**, a new family of loss functions \(L(y,\theta)\) generated by a convex regularization function \(\Omega\), that are **lower-bounds** of the Fenchel-Young losses generated by the same function \(\Omega\), while maintaining the **same** link function \(\widehat{y}_{\Omega}=\nabla\Omega^{*}\). In particular, we use our framework to instantiate the counterparts of the **logistic** and **sparsemax** losses, two instances of Fenchel-Young losses, associated with the **soft argmax** and the **sparse argmax**. In the figures above, we plot \(L(y,\theta)\), where \(y=e_{1}\), \(\theta=(s,0)\) and \(L\in\{L_{F[\partial\Omega]},L_{\Omega\oplus\Omega^{*}}\}\), confirming the lower-bound property.

For a function \(\Omega:\mathbb{R}^{k}\to\overline{\mathbb{R}}\), its **subdifferential**\(\partial\Omega\subset\mathbb{R}^{k}\times\mathbb{R}^{k}\) is defined by

\[(y^{\prime},\theta^{\prime})\in\partial\Omega\iff\theta^{\prime}\in\partial \Omega(y^{\prime})\iff\Omega(y)\geq\Omega(y^{\prime})+\langle y-y^{\prime}, \theta^{\prime}\rangle,\ \forall y\in\mathbb{R}^{k}.\]

When \(\Omega\) is convex and differentiable, the subdifferential is a singleton and we have \(\partial\Omega(y^{\prime})=\{\nabla\Omega(y^{\prime})\}\). The **normal cone** to a nonempty closed convex set \(\mathcal{C}\subset\mathbb{R}^{k}\) at \(y^{\prime}\) is defined by

\[\theta^{\prime}\in N_{\mathcal{C}}(y^{\prime})\iff\langle y-y^{\prime},\theta ^{\prime}\rangle\leq 0,\quad\forall y\in\mathcal{C}\]

if \(y^{\prime}\in\mathcal{C}\) and \(N_{\mathcal{C}}(y^{\prime})=\emptyset\) if \(y^{\prime}\not\in\mathcal{C}\). The **Fenchel conjugate**\(\Omega^{*}:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) of a function \(\Omega:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) is defined by

\[\Omega^{*}(\theta)\coloneqq\sup_{y^{\prime}\in\mathbb{R}^{k}}\langle y^{ \prime},\theta\rangle-\Omega(y^{\prime}).\]

From standard results in convex analysis [27, Proposition 11.3], when \(\Omega:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) is a proper convex l.s.c. (lower semicontinuous) function,

\[\partial\Omega^{*}(\theta)=\operatorname*{argmax}_{y^{\prime}\in\mathbb{R}^{k} }\langle y^{\prime},\theta\rangle-\Omega(y^{\prime}).\]

When the argmax is unique, it is equal to \(\nabla\Omega^{*}(\theta)\). We define the **generalized Bregman divergence**[18]\(D_{\Omega}:\mathbb{R}^{k}\times\mathbb{R}^{k}\to\overline{\mathbb{R}}_{+}\) generated by a proper convex l.s.c. function \(\Omega:\mathbb{R}^{k}\to\overline{\mathbb{R}}\), by

\[D_{\Omega}(y,y^{\prime})\coloneqq\Omega(y)-\Omega(y^{\prime})-\sup_{\theta^{ \prime}\in\partial\Omega(y^{\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle,\] (1)

with the convention \(+\infty+(-\infty)=+\infty\) (we comment on this convention in Appendix C). When \(\Omega\) is differentiable, Equation (1) gives the classical **Bregman divergence**

\[D_{\Omega}(y,y^{\prime})\coloneqq\Omega(y)-\Omega(y^{\prime})-\langle y-y^{ \prime},\nabla\Omega(y^{\prime})\rangle.\]

Both \(y\) and \(y^{\prime}\) belong to the **primal space**.

### Fenchel-Young losses

**Definition and properties**

The **Fenchel-Young loss**[8]\(L_{\Omega\oplus\Omega^{*}}:\mathbb{R}^{k}\times\mathbb{R}^{k}\to \overline{\mathbb{R}}\) generated by a proper convex l.s.c. function \(\Omega\) is

\[L_{\Omega\oplus\Omega^{*}}(y,\theta)\coloneqq\big{(}\Omega\oplus\Omega^{*} \big{)}(y,\theta)-\langle y,\theta\rangle\coloneqq\Omega(y)+\Omega^{*}(\theta )-\langle y,\theta\rangle.\]

As its name indicates, it is grounded in the Fenchel-Young inequality

\[\langle y,\theta\rangle\leq\Omega(y)+\Omega^{*}(\theta)\quad\forall y,\theta \in\mathbb{R}^{k}.\]

The Fenchel-Young loss enjoys many desirable properties, notably it is **non-negative** and it is **separately convex** in \(y\) and \(\theta\). The Fenchel-Young loss can be seen as a **primal-dual** Bregman divergence [1, 8], where \(y\) belongs to the primal space and \(\theta\) belongs to the dual space.

#### Link functions

To map an element \(\theta\) of a dual space to an element \(y\) of a primal space, we define the link function (potentially set-valued) associated with the loss \(L\) by

\[\theta\mapsto\{y\in\mathbb{R}^{k}\ \big{|}\ L(y,\theta)=0\}.\]

Given a proper convex function \(\Omega\), the associated Fenchel-Young loss \(L_{\Omega\oplus\Omega^{*}}\) produces the canonical link function \(\partial\Omega^{*}\), since

\[L_{\Omega\oplus\Omega^{*}}(y,\theta)=0\iff y\in\partial\Omega^{*}(\theta).\]

In particular when \(\Omega\) is strictly convex, and thus \(\Omega^{*}\) is differentiable according to [27, Theorem 11.13], the Fenchel-Young loss satisfies the identity of indiscernibles

\[L_{\Omega\oplus\Omega^{*}}(y,\theta)=0\iff y=\nabla\Omega^{*}(\theta).\]

In the remainder of this paper, we will use the notation \(\widehat{y}_{\Omega}(\theta)\) for the canonical link function \(\partial\Omega^{*}(\theta)\). When the function \(\Omega^{*}\) is differentiable, \(\widehat{y}_{\Omega}(\theta)\) will denote \(\nabla\Omega^{*}(\theta)\). Since \(\Omega^{*}\) is convex, \(\widehat{y}_{\Omega}\) is monotone (see SS2.3). As shown in [8], the monotonicity implies that \(\theta\) and \(\widehat{y}_{\Omega}(\theta)\) are sorted the same way, i.e., \(\theta_{i}>\theta_{j}\Longrightarrow\widehat{y}_{\Omega}(\theta)_{i}\geq \widehat{y}_{\Omega}(\theta)_{j}\). Link functions also play an important role in the loss subgradients (and in the loss gradient when it is differentiable), as we have

\[\partial_{\theta}L_{\Omega\oplus\Omega^{*}}(y,\theta)=\widehat{y}_{\Omega}( \theta)-y.\] (2)

#### Examples of Fenchel-Young loss instances and their associated link function

We give a few examples of Fenchel-Young losses. With the squared \(2\)-norm, \(\Omega(y^{\prime})=\frac{1}{2}\|y^{\prime}\|_{2}^{2}\), we obtain the **squared loss**

\[L_{\Omega\oplus\Omega^{*}}(y,\theta)=L_{\mathrm{squared}}(y,\theta)\coloneqq \frac{1}{2}\|y-\theta\|_{2}^{2}\]

and the **identity link**

\[\widehat{y}_{\Omega}(\theta)=\theta.\]

With the indicator of a nonempty closed convex set \(\mathcal{C}\), \(\Omega(y^{\prime})=\iota_{\mathcal{C}}(y^{\prime})\), we obtain the **perceptron loss**

\[L_{\Omega\oplus\Omega^{*}}(y,\theta)=L_{\mathrm{perceptron}}(y,\theta)\coloneqq \max_{y^{\prime}\in\mathcal{C}}\ \langle y^{\prime},\theta\rangle-\langle y,\theta\rangle\]

and the **argmax link**

\[\widehat{y}_{\Omega}(\theta)=\operatorname*{argmax}_{y\in\mathcal{C}}\ \langle y,\theta\rangle.\]

With the squared \(2\)-norm restricted to some nonempty closed convex set \(\mathcal{C}\), \(\Omega(y^{\prime})=\frac{1}{2}\|y^{\prime}\|_{2}^{2}+\iota_{\mathcal{C}}(y^{ \prime})\), we obtain the **sparseMAP loss**[24]

\[L_{\Omega\oplus\Omega^{*}}(y,\theta)=L_{\mathrm{sparseMAP}}(y,\theta) \coloneqq\frac{1}{2}\|y-\theta\|_{2}^{2}-\frac{1}{2}\|P_{\mathcal{C}}(\theta )-\theta\|_{2}^{2},\]

and the link is the **Euclidean projection** onto \(\mathcal{C}\),

\[\widehat{y}_{\Omega}(\theta)=P_{\mathcal{C}}(\theta).\]

When the set is \(\mathcal{C}=\triangle^{k}\), we obtain the **sparsemax loss**[21] and the **sparse argmax link**\(\widehat{y}_{\Omega}(\theta)=P_{\triangle^{k}}(\theta)\) (also known as sparsemax), which is known to produce sparse probability distributions.

With the Shannon negentropy restricted to the probability simplex, \(\Omega(y^{\prime})\coloneqq\langle y^{\prime},\log y^{\prime}\rangle+\iota_{ \triangle^{k}}(y^{\prime})\), we obtain the **logistic loss**

\[L_{\Omega\oplus\Omega^{*}}(y,\theta)=L_{\mathrm{logistic}}(y,\theta)\coloneqq \log\sum_{i=1}^{k}\exp(\theta_{i})+\langle y,\log y\rangle-\langle y,\theta\rangle,\]

and the **soft argmax link** (also known as softmax)

\[\widehat{y}_{\Omega}(\theta)=\mathrm{softmax}(\theta)\coloneqq\exp(\theta) /\sum_{i=1}^{k}\exp(\theta_{i}).\]

### Maximal monotone operators and the Fitzpatrick function

An operator \(A\), that is, a subset \(A\subset\mathbb{R}^{k}\times\mathbb{R}^{k}\), is called **monotone** if for all \((y,\theta)\in A\) and all \((y^{\prime},\theta^{\prime})\in A\), we have

\[\langle y^{\prime}-y,\theta^{\prime}-\theta\rangle\geq 0.\]

We overload the notation to denote \(A(y)\coloneqq\{\theta\in\mathbb{R}^{k}\ \big{|}\ (y,\theta)\in A\}\). A monotone operator \(A\) is said to be **maximal** if there does not exist \((y,\theta)\not\in A\) such that \(A\cup\{(y,\theta)\}\) is still monotone. It is well-known that the subdifferential \(\partial\Omega\) of a proper convex l.s.c. function \(\Omega\) is maximal monotone. For more details on monotone operators, see [3, 28].

A well-known object in monotone operator theory, the **Fitzpatrick function** associated with a maximal monotone operator \(A\)[15, 11, 2], denoted \(F[A]:\mathbb{R}^{k}\times\mathbb{R}^{k}\to\overline{\mathbb{R}}\), is defined by

\[F[A](y,\theta)\coloneqq\sup_{(y^{\prime},\theta^{\prime})\in A}\langle y-y^{ \prime},\theta^{\prime}\rangle+\langle y^{\prime},\theta\rangle.\]

In particular, with \(A=\partial\Omega\), we have

\[F[\partial\Omega](y,\theta)=\sup_{(y^{\prime},\theta^{\prime})\in\partial \Omega}\langle y-y^{\prime},\theta^{\prime}\rangle+\langle y^{\prime},\theta \rangle=\sup_{y^{\prime}\in\operatorname*{dom}\Omega}\left[\langle y^{\prime}, \theta\rangle+\sup_{\theta^{\prime}\in\partial\Omega(y^{\prime})}\langle y-y^{ \prime},\theta^{\prime}\rangle\right].\]The Fitzpatrick function was studied in depth in [2]. In particular, it is (jointly) convex and satisfies

\[\langle y,\theta\rangle\leq F[\partial\Omega](y,\theta)\leq\big{(}\Omega\oplus \Omega^{\star}\big{)}(y,\theta)=\Omega(y)+\Omega^{\star}(\theta)\quad\forall y,\theta\in\mathbb{R}^{k}.\] (3)

We introduce the operator \(y_{F[\partial\Omega]}^{\star}\subset(\mathbb{R}^{k}\times\mathbb{R}^{k})\times \mathbb{R}^{k}\), associated to the Fitzpatrick function \(F[\partial\Omega]\):

\[y_{F[\partial\Omega]}^{\star}(y,\theta)\coloneqq\operatorname*{ argmax}_{y^{\prime}\in\operatorname*{dom}\Omega}\ \left[\langle y^{\prime},\theta\rangle+\sup_{\theta^{\prime}\in\partial\Omega(y ^{\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle\right].\] (4)

As proven for Item 4 in Proposition 1, we have \(y_{F[\partial\Omega]}^{\star}(y,\theta)\subset\partial_{\theta}F[\partial \Omega](y,\theta)\). For the rest of the paper, in case that the \(\operatorname*{argmax}\) in (4) is a singleton \(\{y^{\star}\}\), we will write \(y_{F[\partial\Omega]}^{\star}(y,\theta)\coloneqq y^{\star}\). The Fitzpatrick function \(F[\partial\Omega](y,\theta)\) and \(\big{(}\Omega\oplus\Omega^{\star}\big{)}(y,\theta)=\Omega(y)+\Omega^{\star} (\theta)\) play a similar role but the latter function is **separable** in \(y\) and \(\theta\), while the former is **not**. In particular this makes the subdifferential \(\partial_{\theta}F[\partial\Omega](y,\theta)\) depend on both \(y\) and \(\theta\), while \(\partial_{\theta}(\Omega\oplus\Omega^{\star})(y,\theta)=\partial\Omega^{ \star}(\theta)\) depends only on \(\theta\).

The Fitzpatrick function was used in [10] to theoretically study primal-primal Bregman-like divergences. As discussed in more detail in Section 3.4, using these divergences for machine learning would require us to compose them with an explicit link function, which would typically break convexity. In the next section, we introduce new primal-dual losses based on the Fitzpatrick function.

## 3 Fitzpatrick losses

### Definition and properties

Inspired by the inequality in (3), which we can view as a refined Fenchel-Young inequality, we introduce Fitzpatrick losses, a new family of loss functions generated by a convex l.s.c. function \(\Omega\).

**Definition 1**: _Fitzpatrick loss_

_Let \(\Omega:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) be a proper convex l.s.c. function. When \(y\in\operatorname*{dom}\Omega\) and \(\theta\in\mathbb{R}^{k}\), we define the Fitzpatrick loss \(L_{F[\partial\Omega]}:\mathbb{R}^{k}\times\mathbb{R}^{k}\to\overline{\mathbb{R}}\) generated by \(\Omega\) as_

\[L_{F[\partial\Omega]}(y,\theta) \coloneqq F[\partial\Omega](y,\theta)-\langle y,\theta\rangle\] \[=\sup_{(y^{\prime},\theta^{\prime})\in\partial\Omega}\langle y-y^ {\prime},\theta^{\prime}\rangle+\langle y^{\prime},\theta\rangle-\langle y,\theta\rangle\] \[=\sup_{(y^{\prime},\theta^{\prime})\in\partial\Omega}\langle y^ {\prime}-y,\theta-\theta^{\prime}\rangle.\]

_When \(y\not\in\operatorname*{dom}\Omega\), \(L_{F[\partial\Omega]}(y,\theta)\coloneqq+\infty\)._

Fitzpatrick losses enjoy similar properties as Fenchel-Young losses, while being **tighter** than Fenchel-Young losses.

**Proposition 1**: _Properties of Fitzpatrick losses_

1. _Non-negativity:_ _for all_ \((y,\theta)\in\mathbb{R}^{k}\times\mathbb{R}^{k}\)_,_ \(L_{F[\partial\Omega]}(y,\theta)\geq 0\)_._
2. _Same link function:_ \(L_{\Omega\oplus\Omega^{\star}}(y,\theta)=L_{F[\partial\Omega]}(y,\theta)=0 \iff y\in\widehat{y}_{\Omega}(\theta)\)_._
3. _Separable convexity:_ \(L_{F[\partial\Omega]}(y,\theta)\) _is separately convex._
4. _(Sub-)Gradient:_ \(\partial_{\theta}L_{F[\partial\Omega]}(y,\theta)\supset y_{F[\partial\Omega]} ^{\star}(y,\theta)-y\) _where_ \(y_{F[\partial\Omega]}^{\star}(y,\theta)\) _is given by (_4_)._
5. _Tighter inequality:_ _for all_ \((y,\theta)\in\mathbb{R}^{k}\)_,_ \(0\leq L_{F[\partial\Omega]}(y,\theta)\leq L_{\Omega\oplus\Omega^{\star}}(y,\theta)\)_._

A proof is given in Appendix B.2. Because the Fitzpatrick loss and the Fenchel-Young loss generated by the same \(\Omega\) have the same link function \(\widehat{y}_{\Omega}\), they share the same minimizers w.r.t. \(\theta\) for \(y\) fixed. However, the Fitzpatrick loss is always a **lower bound** of the corresponding Fenchel-Young loss. Moreover, they have different gradients w.r.t. \(\theta\): \(\partial_{\theta}L_{\Omega\oplus\Omega^{\star}}(y,\theta)=\widehat{y}_{\Omega} (\theta)-y\) vs. \(\partial_{\theta}L_{F[\partial\Omega]}(y,\theta)\supset y_{F[\partial\Omega]} ^{\star}(y,\theta)-y\). It is worth noticing that \(y_{F[\partial\Omega]}^{\star}(y,\theta)\) depends on both \(y\) and \(\theta\), contrary to \(\widehat{y}_{\Omega}(\theta)\).

When \(\Omega\) is a twice differentiable function on its domain (which is for instance the case of the squared \(2\)-norm or the negentropy), we next show that Fitzpatrick losses enjoy a particularly simple expression and become a squared Mahalanobis-like distance.

**Proposition 2**: _Expressions of \(F[\partial\Omega](y,\theta)\) and \(L_{F[\partial\Omega]}(y,\theta)\) when \(\Omega\) is twice differentiable Let \(\Omega:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) be a convex function such that \(\mathrm{dom}\;\;\Omega\) is an open set. Let us assume that \(\Omega\) is twice differentiable. Then, for any \(y\in\mathrm{dom}\;\Omega\) and for any \(y^{\star}\in y^{\star}_{F[\partial\Omega]}(y,\theta)\), as defined in (4), we have that_

\[F[\partial\Omega](y,\theta) =\langle y,\nabla\Omega(y^{\star})\rangle+\langle y^{\star},\theta \rangle-\langle y^{\star},\nabla\Omega(y^{\star})\rangle\] \[L_{F[\partial\Omega]}(y,\theta) =\langle y^{\star}-y,\theta-\nabla\Omega(y^{\star})\rangle\] \[=\langle y^{\star}-y,\nabla^{2}\Omega(y^{\star})(y^{\star}-y)\rangle\]

_and_

\[\nabla^{2}\Omega(y^{\star})(y^{\star}-y)=\theta-\nabla\Omega(y^{\star}).\]

A proof is given in B.3. When \(\Omega\) is constrained (i.e., when it contains an indicator function), we show in SS3.5 that the above expression becomes a lower bound.

### Examples

We now present the Fitzpatrick loss counterparts of various Fenchel-Young losses.

**Squared loss.**

**Proposition 3**: _Squared loss as a Fitzpatrick loss When \(\Omega(y^{\prime})=\frac{1}{2}\left\|y^{\prime}\right\|_{2}^{2}\), we have for all \(y\in\mathbb{R}^{k}\) and \(\theta\in\mathbb{R}^{k}\)_

\[L_{F[\partial\Omega]}(y,\theta)=\frac{1}{4}\left\|y-\theta\right\|_{2}^{2}= \frac{1}{2}L_{\mathrm{squared}}(y,\theta).\]

A proof is given in Appendix B.4. Therefore, the Fenchel-Young and Fitzpatrick losses generated by \(\Omega\) coincide, but up to a factor \(\frac{1}{2}\).

**Perceptron loss.**

**Proposition 4**: _Perceptron loss as a Fitzpatrick loss When \(\Omega(y^{\prime})=\iota_{\mathcal{C}}(y^{\prime})\), where \(\mathcal{C}\) is a nonempty closed convex set, we have for all \(y\in\mathcal{C}\) and \(\theta\in\mathbb{R}^{k}\)_

\[L_{F[\partial\Omega]}(y,\theta)=L_{\mathrm{perceptron}}(y,\theta)=\max_{y^{ \prime}\in\mathcal{C}}\;\langle y^{\prime},\theta\rangle-\langle y,\theta\rangle.\]

A proof is given in Appendix B.5. Therefore, the Fenchel-Young and Fitzpatrick losses generated by \(\Omega\) exactly coincide in this case.

Fitzpatrick sparseMAP and Fitzpatrick sparsemax losses.As our first example where Fenchel-Young and Fitzpatrick losses substantially differ, we introduce the **Fitzpatrick sparseMAP** loss, which is the Fitzpatrick counterpart of the sparseMAP loss [24].

**Proposition 5**: _Fitzpatrick sparseMAP loss When \(\Omega(y^{\prime})=\frac{1}{2}\|y^{\prime}\|_{2}^{2}+\iota_{\mathcal{C}}(y^{ \prime})\), where \(\mathcal{C}\) is a nonempty closed convex set, we have for all \(y\in\mathcal{C}\) and \(\theta\in\mathbb{R}^{k}\)_

\[L_{F[\partial\Omega]}(y,\theta)=2\Omega^{*}\left((y+\theta)/2\right)-\langle y,\theta\rangle=\langle y^{\star}-y,\theta-y^{\star}\rangle\]

_where we used \(y^{\star}\) as a shorthand for_

\[y^{\star}_{F[\partial\Omega]}(y,\theta)=\nabla\Omega^{*}((y+\theta)/2)=P_{ \mathcal{C}}((y+\theta)/2).\]A proof is given in Appendix B.6. As a special case, when \(\mathcal{C}=\triangle^{k}\), we call the obtained loss the **Fitzpatrick sparsemax loss**, as it is the counterpart of the sparsemax loss [21]. Like the sparseMAP and sparsemax losses, these new losses rely on the Euclidean projection as a core building block. The Euclidean projection onto the probability simplex \(\triangle^{k}\) can be computed exactly in \(O(k)\) expected time and \(O(k\log k)\) worst-case time [9, 22, 14, 12].

Fitzpatrick logistic loss.We now derive the Fitzpatrick counterpart of the logistic loss. Before stating the next proposition, we recall the definition of the Lambert function [13]\(W:\mathbb{R}_{+}\to\mathbb{R}_{+}\). The function \(W\) is the inverse of the function \(f:\mathbb{R}_{+}\to\mathbb{R}_{+}\) where \(f(w)=w\exp(w)\) for all \(w\in\mathbb{R}_{+}\).

**Proposition 6**: _Fitzpatrick logistic loss_

_When \(\Omega(y^{\prime})=\langle y^{\prime},\log y^{\prime}\rangle+{}_{\iota_{ \triangle^{k}}}(y^{\prime})\), we have for all \(y\in\triangle^{k}\) and \(\theta\in\mathbb{R}^{k}\)_

\[L_{F[\partial\Omega]}(y,\theta)=\langle y^{\star}-y,\theta-\log y^{\star}-1\rangle\]

_where we used \(y^{\star}\) as a shorthand for \(y^{\star}_{F[\partial\Omega]}(y,\theta)\) defined by_

\[y^{\star}_{F[\partial\Omega]}(y,\theta)_{i}=\left\{\begin{array}{c}\mathrm{e }^{-\lambda^{\star}}\mathrm{e}^{\theta_{i}},\text{ if }y_{i}=0,\\ \frac{y_{i}}{W(y_{i}\mathrm{e}^{\lambda^{\star}}-\theta_{i})},\text{ if }y_{i}>0.\end{array}\right.\]

A proof and the value of \(\lambda^{\star}=\lambda^{\star}(y,\theta)\in\mathbb{R}\) are given in Appendix B.7. To obtain \(\lambda^{\star}(y,\theta)\), we need to solve a one-dimensional root equation, which can be done using, for instance, a bisection.

### Relation with Fenchel-Young losses

On first sight, Fitzpatrick losses and Fenchel-Young losses appear quite different. In the next proposition, we show that the Fitzpatrick loss generated by \(\Omega\) is in fact equal to the Fenchel-Young loss generated by the modified, target-dependent function

\[\Omega_{y}(y^{\prime})\coloneqq\Omega(y^{\prime})+D_{\Omega}(y,y^{\prime}),\]

where \(D_{\Omega}\) is the generalized Bregman divergence defined in (1). In particular, Lemma 1 in the appendix shows that if \(\Omega=\Psi+\iota_{\mathcal{C}}\), where \(\mathcal{C}\) is a nonempty closed convex set, then \(\Omega_{y}(y^{\prime})=\Psi_{y}(y^{\prime})+\iota_{\mathcal{C}}(y^{\prime})\), where \(\Psi_{y}(y^{\prime})\coloneqq\Psi(y^{\prime})+D_{\Psi}(y,y^{\prime})\).

**Proposition 7**: _Characterization of \(F[\partial\Omega]\), \(L_{F[\partial\Omega]}\) and \(y^{\star}_{F[\partial\Omega]}\) using \(\Omega_{y}\)_

_Let \(\Omega:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) be a proper convex l.s.c. function. Then, for all \(y\in\mathrm{dom}\,\Omega\) and all \(\theta\in\mathbb{R}^{k}\),_

\[F[\partial\Omega](y,\theta) =\Omega_{y}(y)+\Omega^{\star}_{y}(\theta)\] \[L_{F[\partial\Omega]}(y,\theta) =L_{\Omega_{y}\oplus\Omega^{\star}_{y}}(y,\theta)\] \[y^{\star}_{F[\partial\Omega]}(y,\theta) =\widehat{y}_{\Omega_{y}}(\theta).\]

This characterization of the Fitzpatrick function \(F[\partial\Omega]\) is also new to our knowledge. A proof is given in Appendix B.8. Proposition 7 is very useful, as it means that Fitzpatrick losses inherit from all the known properties of Fenchel-Young losses, analyzed in prior works [8, 6]. In particular, Fenchel-Young losses are smooth (i.e., with Lipschitz gradients) when \(\Omega\) is strongly convex. We therefore immediately obtain that Fitzpatrick losses are smooth in their second argument \(\theta\) if \(\Omega\) is strongly convex and \(D_{\Omega}\) is convex in its second argument, which is the case when \(\Omega(y^{\prime})=\frac{1}{2}\|y^{\prime}\|_{2}^{2}\) and \(\Omega(y^{\prime})=\langle y^{\prime},\log y^{\prime}\rangle\). Therefore, the Fitzpatrick sparsemax and logistic losses are smooth. However in the general case, this does not hold, as \(D_{\Omega}\) is usually not convex in its second argument. Proposition 7 also provides a mean to compute Fitzpatrick losses and their gradient. Finally, it suggests a very natural geometric interpretation of Fitzpatrick losses, as presented in Figure 2.

### Relation with generalized Bregman divergences

As we stated before, the generalized Bregman divergence \(D_{\Omega}(y,y^{\prime})\) in (1) is a primal-primal divergence, as both \(y\) and \(y^{\prime}\) belong to the same primal space. In contrast, Fenchel-Young losses\(L_{\Omega\oplus\Omega^{*}}(y,\theta)\) are primal-dual, since \(y\) belongs to the primal space and \(\theta\) belongs to the dual space. Both can however be related for any proper l.s.c. convex function \(\Omega\), since, for any \(y^{\prime}\) such that \(\partial\Omega(y^{\prime})\neq\emptyset\), we have

\[\inf_{\theta^{\prime}\in\partial\Omega(y^{\prime})}L_{\Omega\oplus \Omega^{*}}(y,\theta^{\prime}) =\inf_{\theta^{\prime}\in\partial\Omega(y^{\prime})}\Omega(y)+ \Omega^{*}(\theta^{\prime})-\langle y,\theta^{\prime}\rangle\] \[=\Omega(y)+\inf_{\theta^{\prime}\in\partial\Omega(y^{\prime})} \Omega^{*}(\theta^{\prime})-\langle y,\theta^{\prime}\rangle\] \[=\Omega(y)-\sup_{\theta^{\prime}\in\partial\Omega(y^{\prime})}- \Omega^{*}(\theta^{\prime})+\langle y,\theta^{\prime}\rangle\] \[=\Omega(y)-\Omega(y^{\prime})-\sup_{\theta^{\prime}\in\partial \Omega(y^{\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle\] \[=D_{\Omega}(y,y^{\prime})\]

where, in the penultimate line, we have used that \(\Omega^{*}(\theta^{\prime})=\langle y^{\prime},\theta^{\prime}\rangle-\Omega (y^{\prime})\), as \(\theta^{\prime}\in\partial\Omega(y^{\prime})\). This equality remains true when \(\partial\Omega(y^{\prime})=\emptyset\), by convention \(\inf\emptyset=+\infty\) and by definition of \(D_{\Omega}(y,y^{\prime})\) in (1). This identity suggests that we can create Bregman-like primal-primal divergences by replacing \(\Omega\oplus\Omega^{*}\) with \(F[\partial\Omega]\),

\[\mathcal{D}_{F[\partial\Omega]}(y,y^{\prime})\coloneqq\inf_{\theta^{\prime} \in\partial\Omega(y^{\prime})}L_{F[\partial\Omega]}(y,\theta^{\prime})=\inf_{ \theta^{\prime}\in\partial\Omega(y^{\prime})}F[\partial\Omega](y,\theta^{ \prime})-\langle y,\theta^{\prime}\rangle.\]

This recovers one of the two Bregman-like divergences proposed in [10], the other one replacing the \(\inf\) above by a \(\sup\). As stated in [10], \(F[\partial\Omega]\) and \(\Omega\oplus\Omega^{*}\) are **representations** of \(\partial\Omega\).

In order to use a primal-primal divergence as a loss, we need to explicitly compose it with a link function, such as \(\widehat{y}_{\Omega}(\theta)=\nabla\Omega^{*}(\theta)\). Unfortunately, \(D_{\Omega}(y,\widehat{y}_{\Omega}(\theta))\) or \(\mathcal{D}_{F[\partial\Omega]}(y,\widehat{y}_{\Omega}(\theta))\) are typically **non convex** functions of \(\theta\), while Fenchel-Young and Fitzpatrick losses are always **convex** in \(\theta\). In addition, differentiating through \(\widehat{y}_{\Omega}(\theta)\) typically requires implicit differentiation [19, 5], while Fenchel-Young and Fitzpatrick losses enjoy easy-to-compute gradients, thanks to Danskin's theorem.

### Lower bound on Fitzpatrick losses

If \(\Omega=\Psi+\iota_{\mathcal{C}}\), where \(\Psi:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) is a proper convex Legendre-type function and \(\mathcal{C}\subseteq\operatorname{dom}\Psi\) is a nonempty closed convex set, then it was shown in [8, Proposition 3] that Fenchel-Young lossessatisfy the lower bound

\[D_{\Psi}\big{(}y,\widehat{y}_{\Omega}(\theta)\big{)}\leq L_{\Omega\oplus\Omega^{ *}}(y,\theta),\]

with equality if \(\mathcal{C}=\operatorname{dom}\Psi\). We now show that a similar result holds for Fitzpatrick losses. Similarly as before, we define \(\Psi_{y}(y^{\prime})\coloneqq\Psi(y^{\prime})+D_{\Psi}(y,y^{\prime})\).

**Proposition 8**: _Lower bound on Fitzpatrick losses Let \(\Omega=\Psi+\iota_{\mathcal{C}}\), where \(\Psi:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) is a proper convex Legendre-type function, as defined in [8, Definition 3], and \(\mathcal{C}\subseteq\operatorname{dom}\Psi\) is a nonempty closed convex set. We remind that \(\Omega_{y}(y^{\prime})\coloneqq\Omega(y^{\prime})+D_{\Omega}(y,y^{\prime})\). Let us assume that \(\Omega_{y}^{*}\) is differentiable. Then,_

\[D_{\Psi_{y}}(y,y^{*})=\langle y-y^{*},\nabla^{2}\Psi(y^{*})(y-y^{*})\rangle \leq L_{F[\partial\Omega]}(y,\theta),\]

_with equality if \(\operatorname{dom}\Psi=\mathcal{C}\), where we used \(y^{*}\) as a shorthand for \(\nabla\Omega_{y}^{*}(\theta)\)._

A proof is given in Appendix B.9. If \(\Psi_{y}\) is \(\mu\)-strongly convex, we obtain \(\frac{\mu}{2}\|y-y^{*}\|_{2}^{2}\leq D_{\Psi_{y}}(y,y^{*})\).

## 4 Experiments

Experimental setup.We follow exactly the same experimental setup as in [7, 8]. We consider a dataset of \(n\) pairs \((x_{i},y_{i})\) of feature vectors \(x_{i}\in\mathbb{R}^{d}\) and label proportions \(y_{i}\in\triangle^{k}\), where \(d\) is the number of features and \(k\) is the number of classes. At inference time, given an unknown input vector \(x\in\mathbb{R}^{d}\), our goal is to estimate a vector of label proportions \(\widehat{y}\in\triangle^{k}\). A model is specified by a matrix \(W\in\mathbb{R}^{k\times d}\) and a convex l.s.c. function \(\Omega:\mathbb{R}^{k}\to\overline{\mathbb{R}}\). Predictions are then produced by the generalized linear model \(x\mapsto\widehat{y}_{\Omega}(Wx)\). At training time, we estimate the matrix \(W\in\mathbb{R}^{k\times d}\) by minimizing the convex objective

\[R_{L,\lambda}(W)\coloneqq\sum_{i=1}^{n}L(y_{i},Wx_{i})+\frac{\lambda}{2}\left\| W\right\|_{2}^{2},\] (5)

where \(L\in\big{\{}L_{\Omega\oplus\Omega^{*}},L_{F[\partial\Omega]}\big{\}}\). We focus on the (Fitzpatrick) sparsemax and the (Fitzpatrick) logistic losses. We optimize (5) using the L-BFGS algorithm [20]. The gradient of the Fenchel-Young loss is given in (2), while the gradient of the Fitzpatrick loss is given in Proposition 1, Item 4. Experiments were conducted on a Intel Xeon E5-2667 clocked at 3.30GHz with 192 GB of RAM running on Linux. Our implementation relies on the SciPy [29] and scikit-learn [25] libraries.

We ran experiments on \(11\) standard multi-label benchmark datasets1 (see Table 2 in Appendix A for statistics on the datasets). For all datasets, we removed samples with no label, normalized samples to have zero mean unit variance, and normalized labels to lie in the probability simplex. In (5), we chose the hyperparameter \(\lambda\in\{10^{-4},10^{-3},\ldots,10^{4}\}\) against the validation set. We report test set mean squared error (also known as Brier score in a probabilistic forecasting context) in Table 1.

Footnote 1: The datasets can be downloaded from http://mulan.sourceforge.net/datasets-mlc.html and https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.

Results.We found that the logistic loss and the Fitzpatrick logistic loss are comparable on most datasets, with the logistic loss significantly winning on \(2\) datasets and the Fitzpatrick logistic loss significantly winning on \(2\) datasets, out of \(11\). Since the Fitzpatrick logistic loss is slightly more computationally demanding, requiring to solve a root equation while the logistic loss does not, we believe that the logistic loss remains the best choice when we wish to use the sofargmax as link function \(\widehat{y}_{\Omega}\).

Similarly, we found that the sparsemax loss and the Fitzpatrick sparsemax loss are comparable on most datasets, with the sparsemax loss significantly winning on only \(1\) dataset out of \(11\) and the Fitzpatrick loss significantly winning on \(2\) datasets out of \(11\). Since the two losses both use the Euclidean projection onto the simplex \(P_{\triangle^{k}}\) as their link function \(\widehat{y}_{\Omega}\), we conclude that the Fitzpatrick sparsemax loss is a serious contender to the sparsemax loss, especially when predicting sparse label proportions is important.

## 5 Conclusion

We proposed to leverage the Fitzpatrick function, a theoretical tool from monotone operator theory, to build a new family of primal-dual separately convex loss functions for machine learning. We reinterpreted Fitzpatrick losses as lower bounds of Fenchel-Young losses that maintains the same link function. Our paper therefore challenges the idea that there can only be one loss function, convex in each argument, associated with a certain link function. For instance, we created the Fitzpatrick logistic and sparsemax losses, that are associated with the soft argmax and sparse argmax links, traditionally associated with the logistic and sparsemax losses, respectively. We believe that even more loss functions with the same link can be created, which calls for a systematic study of their properties and respective benefits. In future work, we intend to study calibration guarantees for Fitzpatrick losses, to test new link functions from maximal monotone operator theory and to implement more efficient training for the Fitzpatrick logistic loss.

## References

* Amari [2016] S.-i. Amari. _Information geometry and its applications_, volume 194. Springer, 2016.
* Bauschke et al. [2005] H. Bauschke, D. McLaren, and H. Sendov. Fitzpatrick functions: Inequalities, examples, and remarks on a problem by S. Fitzpatrick. _Journal of Convex Analysis_, 13, 07 2005.
* Bauschke and Combettes [2017] H. H. Bauschke and P. L. Combettes. _Convex analysis and monotone operator theory in Hilbert spaces_. CMS Books in Mathematics/Ouvrages de Mathematiques de la SMC. Springer-Verlag, New York, second edition, 2017.
* Bertsekas [1997] D. P. Bertsekas. Nonlinear programming. _Journal of the Operational Research Society_, 48(3):334-334, 1997.
* Blondel et al. [2022] M. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-Lopez, F. Pedregosa, and J.-P. Vert. Efficient and modular implicit differentiation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 5230-5242. Curran Associates, Inc., 2022.
* Blondel et al. [2022] M. Blondel, F. Llinares-Lopez, R. Dadashi, L. Hussenot, and M. Geist. Learning energy networks with generalized Fenchel-Young losses. _Advances in Neural Information Processing Systems_, 35:12516-12528, 2022.
* Blondel et al. [2019] M. Blondel, A. Martins, and V. Nicolae. Learning classifiers with Fenchel-Young losses: Generalized entropies, margins, and algorithms. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 606-615. PMLR, 2019.
* Blondel et al. [2020] M. Blondel, A. F. Martins, and V. Nicolae. Learning with Fenchel-Young losses. _Journal of Machine Learning Research_, 21(35):1-69, 2020.

\begin{table}
\begin{tabular}{r||c c|c c} \hline Dataset & Sparsemax & Fitzpatrick-sparsemax & Logistic & Fitzpatrick-logistic \\ \hline Birds & 0.531 & **0.513** & 0.519 & 0.522 \\ Cal500 & 0.035 & 0.035 & 0.034 & 0.034 \\ Delicious & 0.051 & 0.052 & 0.056 & 0.055 \\ Echtr A & 0.514 & 0.514 & 0.431 & **0.423** \\ Emotions & 0.317 & 0.318 & 0.327 & **0.320** \\ Flags & 0.186 & 0.188 & 0.184 & 0.187 \\ Mediamill & **0.191** & 0.203 & **0.207** & 0.220 \\ Scene & 0.363 & **0.355** & **0.344** & 0.368 \\ Tmc & 0.151 & 0.152 & 0.161 & 0.160 \\ Unfair & 0.149 & 0.148 & 0.157 & 0.158 \\ Yeast & 0.186 & 0.187 & 0.183 & 0.185 \\ \hline \end{tabular}
\end{table}
Table 1: Test performance comparison between the sparsemax loss, the logistic loss and their Fitzpatrick counterparts on the task of label proportion estimation, with regularization parameter \(\lambda\) in (5) tuned against the validation set. For each dataset, label proportion errors are measured using the mean squared error (MSE). We use bold if the error is at least 0.005 lower than its counterpart.

* [9] P. Brucker. An \(O(n)\) algorithm for quadratic knapsack problems. _Operations Research Letters_, 3(3):163-166, 1984.
* [10] R. S. Burachik and J. E. Martinez-Legaz. On Bregman-type distances for convex functions and maximally monotone operators. _Set-Valued and Variational Analysis_, 26:369-384, 2018.
* [11] R. S. Burachik and B. F. Svaiter. Maximal monotone operators, convex functions and a special family of enlargements. _Set-Valued Analysis_, 10:297-316, 2002.
* [12] L. Condat. Fast projection onto the simplex and the \(\ell_{1}\) ball. _Mathematical Programming_, 158(1-2):575-585, 2016.
* [13] R. M. Corless, G. H. Gonnet, D. E. G. Hare, D. J. Jeffrey, and D. E. Knuth. On the Lambert W function. _Advances in Computational Mathematics_, 5(1):329-359, Dec 1996.
* [14] J. C. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efficient projections onto the \(\ell_{1}\)-ball for learning in high dimensions. In _Proc. of ICML_, 2008.
* [15] S. Fitzpatrick. Representing monotone operators by convex functions. In _Workshop/Miniconference on Functional Analysis and Optimization_, volume 20, pages 59-66. Australian National University, Mathematical Sciences Institute, 1988.
* [16] T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and estimation. _Journal of the American statistical Association_, 102(477):359-378, 2007.
* [17] P. D. Grunwald and A. P. Dawid. Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory. _The Annals of Statistics_, 32(4):1367-1433, 2004.
* [18] K. C. Kiwiel. Proximal minimization methods with generalized Bregman functions. _SIAM journal on control and optimization_, 35(4):1142-1168, 1997.
* [19] S. G. Krantz and H. R. Parks. _The implicit function theorem: history, theory, and applications_. Springer Science & Business Media, 2002.
* [20] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. _Mathematical programming_, 45(1):503-528, 1989.
* [21] A. Martins and R. Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. In _International conference on machine learning_, pages 1614-1623. PMLR, 2016.
* [22] C. Michelot. A finite algorithm for finding the projection of a point onto the canonical simplex of \(\mathbb{R}^{n}\). _Journal of Optimization Theory and Applications_, 50(1):195-200, 1986.
* [23] J. J. Moreau. Inf-convolution, sous-additivite, convexite des fonctions numeriques. _J. Math. Pures Appl. (9)_, 49:109-154, 1970.
* [24] V. Niculae, A. Martins, M. Blondel, and C. Cardie. Sparsemap: Differentiable sparse structured inference. In _International Conference on Machine Learning_, pages 3799-3808. PMLR, 2018.
* [25] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in Python. _the Journal of machine Learning research_, 12:2825-2830, 2011.
* [26] M. D. Reid and R. C. Williamson. Composite binary losses. _The Journal of Machine Learning Research_, 11:2387-2422, 2010.
* [27] R. T. Rockafellar and R. J.-B. Wets. _Variational Analysis_. sv, Berlin, 1998.
* [28] E. K. Ryu and W. Yin. _Large-scale convex optimization: algorithms & analyses via monotone operators_. Cambridge University Press, 2022.
* [29] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, et al. Scipy 1.0: fundamental algorithms for scientific computing in Python. _Nature methods_, 17(3):261-272, 2020.
* [30] R. C. Williamson, E. Vernet, and M. D. Reid. Composite multiclass losses. _Journal of Machine Learning Research_, 17(222):1-52, 2016.

## Appendix A Datasets statistics

## Appendix B Proofs

### Lemmas

**Lemma 1**: _Generalized Bregman divergence for constrained \(\Omega\)_

_Let \(\Omega=\Psi+\iota_{\mathcal{C}}\), where \(\Psi:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) is proper convex l.s.c. and \(\mathcal{C}\subseteq\operatorname{dom}\Psi\) is a nonempty closed convex set such that \(\operatorname{ri}\mathcal{C}\cap\operatorname{ri}\operatorname{dom}\Psi\neq\emptyset\), where \(\operatorname{ri}\mathcal{C}\) is the relative interior of \(\mathcal{C}\). Then, for all \(y,y^{\prime}\in\mathbb{R}^{k}\)_

\[D_{\Omega}(y,y^{\prime})=D_{\Psi}(y,y^{\prime})+D_{\iota_{\mathcal{C}}}(y,y^{ \prime}).\]

**Proof.**

As \(\mathcal{C}\), \(\operatorname{dom}\Psi\subset\mathbb{R}^{k}\) and \(\operatorname{ri}\mathcal{C}\cap\operatorname{ri}\operatorname{dom}\Psi\neq\emptyset\), we can apply [3, Proposition 6.19] and [3, Theorem 16.47] to write \(\partial\Omega(y^{\prime})=\partial\Psi(y^{\prime})+\partial\iota_{\mathcal{C }}(y^{\prime})\).

Thus, we have

\[D_{\Omega}(y,y^{\prime}) \coloneqq\Omega(y)-\Omega(y^{\prime})-\sup_{\theta^{\prime}\in \partial\Omega(y^{\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle\] \[=\Psi(y)+\iota_{\mathcal{C}}(y)-\Psi(y^{\prime})-\iota_{\mathcal{ C}}(y^{\prime})-\sup_{\theta^{\prime}\in\partial\Psi(y^{\prime})+\partial \iota_{\mathcal{C}}(y^{\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle\] \[=\Psi(y)-\Psi(y^{\prime})-\sup_{\theta^{\prime}\in\partial\Psi(y ^{\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle+\iota_{\mathcal{C}}(y) -\iota_{\mathcal{C}}(y^{\prime})-\sup_{\theta^{\prime}\partial\iota_{ \mathcal{C}}(y^{\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle\] \[=D_{\Psi}(y,y^{\prime})+D_{\iota_{\mathcal{C}}}(y,y^{\prime}).\]

**Lemma 2**: _Generalized Bregman divergence of indicator function of some nonempty closed convex set set \(\mathcal{C}\subset\mathbb{R}^{k}\)_

\[D_{\iota_{\mathcal{C}}}(y,y^{\prime})=\begin{cases}\iota_{\mathcal{C}}(y)& \text{if }y^{\prime}\in\mathcal{C}=\iota_{\mathcal{C}}(y)+\iota_{\mathcal{C}}(y^{ \prime}).\end{cases}\]

**Proof.** Using (1) and the fact that \(\partial\iota_{\mathcal{C}}=N_{\mathcal{C}}\)[3, Example 16.13], we obtain that

\[D_{\iota_{\mathcal{C}}}(y,y^{\prime})\coloneqq\iota_{\mathcal{C}}(y)-\iota_{ \mathcal{C}}(y^{\prime})-\sup_{\theta^{\prime}\in N_{\mathcal{C}}(y^{\prime}) }\langle y-y^{\prime},\theta^{\prime}\rangle.\]

When \(y^{\prime}\in\mathcal{C}\) and \(y\in\mathcal{C}\),

\[\sup_{\theta^{\prime}\in N_{\mathcal{C}}(y^{\prime})}\langle y-y^{\prime}, \theta^{\prime}\rangle=\sup_{\begin{subarray}{c}\theta^{\prime}\in\mathbb{R}^ {k}\\ \langle z-y^{\prime},\theta^{\prime}\rangle\leq 0\end{subarray}}\langle y-y^{\prime}, \theta^{\prime}\rangle=0.\]

When \(y^{\prime}\in\mathcal{C}\) and \(y\notin\mathcal{C}\), \(D_{\iota_{\mathcal{C}}}(y,y^{\prime})=+\infty\), as \(+\infty+(-\infty)=+\infty\) in the definition of the Bregman divergence. Therefore, when \(y^{\prime}\in\mathcal{C}\), we get that \(D_{\iota_{\mathcal{C}}}(y,y^{\prime})=\iota_{\mathcal{C}}(y)\).

\begin{table}
\begin{tabular}{r||c c c c c c c} \hline Dataset & Type & Train & Dev & Test & Features & Classes & Avg.labels \\ \hline Birds & Audio & 134 & 45 & 172 & 260 & 19 & 2 \\ Cal500 & Music & 376 & 126 & 101 & 68 & 174 & 26 \\ Delicious & Text & 9682 & 3228 & 3181 & 500 & 983 & 19 \\ Echtr A & Text & 6683 & 228 & 847 & 92401 & 10 & 1 \\ Emotions & Music & 293 & 98 & 202 & 72 & 6 & 2 \\ Flags & Images & 96 & 33 & 65 & 19 & 7 & 3 \\ Mediamill & Video & 22353 & 7451 & 12373 & 120 & 101 & 5 \\ Scene & Images & 908 & 303 & 1196 & 294 & 6 & 1 \\ Tmc & Text & 16139 & 5380 & 7077 & 48099 & 896 & 6 \\ Unfair & Text & 645 & 215 & 172 & 6290 & 8 & 1 \\ Yeast & Micro-array & 1125 & 375 & 917 & 103 & 14 & 4 \\ \hline \end{tabular}
\end{table}
Table 2: Datasets statisticsWhen \(y^{\prime}\not\in{\cal C}\), \(N_{\cal C}(y^{\prime})=\emptyset\). Again, in the definition of the Bregman divergence, \(+\infty+(-\infty)=+\infty\) and we use the convention \(\sup_{\emptyset}=-\infty\).

**Lemma 3**: _Bregman divergence of \(\Psi_{y}\) Let \(\Psi:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) be proper, convex and twice differentiable on the interior of its domain. For all \(y\in\mathbb{R}^{k}\), let \(\Psi_{y}:=\Psi+D_{\Psi}(y,\cdot)\). Then, for all \(y,y^{\prime},y^{\prime\prime}\in\operatorname{int}\operatorname{dom}\Psi\),_

\[D_{\Psi_{y}}(y^{\prime\prime},y^{\prime}) =D_{\Psi}(y,y^{\prime\prime})-D_{\Psi}(y,y^{\prime})+D_{\Psi}(y^{ \prime\prime},y^{\prime})+\langle y^{\prime\prime}-y^{\prime},\nabla^{2}\Psi(y ^{\prime})(y-y^{\prime})\rangle\] \[=\langle y^{\prime\prime}-y,\nabla\Psi(y^{\prime\prime})\rangle- \langle y^{\prime\prime}-y,\nabla\Psi(y^{\prime})\rangle+\langle y^{\prime \prime}-y^{\prime},\nabla^{2}\Psi(y^{\prime})(y-y^{\prime})\rangle\] _and, in particular, for all \(y,y^{\prime}\in\operatorname{int}\operatorname{dom}\Psi\)_

\[D_{\Psi_{y}}(y,y^{\prime})=\langle y-y^{\prime},\nabla^{2}\Psi(y^{\prime})(y-y ^{\prime})\rangle.\]

**Proof.** For all \(y,y^{\prime}\in\operatorname{int}\operatorname{dom}\Psi\), as \(\Psi\) is convex and differentiable on \(\operatorname{int}\operatorname{dom}\Psi\), we have that

\[\Psi_{y}(y^{\prime}) =\Psi(y^{\prime})+D_{\Psi}(y,y^{\prime})\] \[=\Psi(y^{\prime})+\Psi(y)-\Psi(y^{\prime})-\langle y-y^{\prime}, \nabla\Psi(y^{\prime})\rangle\] \[=\Psi(y)-\langle y-y^{\prime},\nabla\Psi(y^{\prime})\rangle,\]

and therefore, as \(\Psi\) is twice differentiable on \(\operatorname{int}\operatorname{dom}\Psi\), we get that

\[\nabla\Psi_{y}(y^{\prime})=\nabla^{2}\Psi(y^{\prime})(y^{\prime}-y)+\nabla \Psi(y^{\prime}).\]

Therefore, for all \(y,y^{\prime},y^{\prime\prime}\in\operatorname{dom}\Psi\),

\[D_{\Psi_{y}}(y^{\prime\prime},y^{\prime}) =\Psi_{y}(y^{\prime\prime})-\Psi_{y}(y^{\prime})-\langle y^{ \prime\prime}-y^{\prime},\nabla\Psi_{y}(y^{\prime})\rangle\] \[=\Psi(y^{\prime\prime})+D_{\Psi}(y,y^{\prime\prime})-\Psi(y^{ \prime})-D_{\Psi}(y,y^{\prime})-\langle y^{\prime\prime}-y^{\prime},\nabla^{2 }\Psi(y^{\prime})(y^{\prime}-y)\rangle\] \[\qquad\qquad\qquad\qquad-\langle y^{\prime\prime}-y^{\prime}, \nabla\Psi(y^{\prime})\rangle\] \[=D_{\Psi}(y,y^{\prime\prime})-D_{\Psi}(y,y^{\prime})+D_{\Psi}(y^{ \prime\prime},y^{\prime})+\langle y^{\prime\prime}-y^{\prime},\nabla^{2}\Psi (y^{\prime})(y-y^{\prime})\rangle\] \[=\langle y^{\prime\prime}-y,\nabla\Psi(y^{\prime\prime})\rangle- \langle y^{\prime\prime}-y,\nabla\Psi(y^{\prime})\rangle+\langle y^{\prime \prime}-y^{\prime},\nabla^{2}\Psi(y^{\prime})(y-y^{\prime})\rangle\]

and in particular using the just last established equality with the triplet \((y,y,y^{\prime})\), we have for all \(y,y^{\prime}\in\operatorname{dom}\Psi\),

\[D_{\Psi_{y}}(y,y^{\prime}) =\langle y-y,\nabla\Psi(y)\rangle-\langle y-y,\nabla\Psi(y^{ \prime})\rangle+\langle y-y^{\prime},\nabla^{2}\Psi(y^{\prime})(y-y^{\prime})\rangle\] \[=\langle y-y^{\prime},\nabla^{2}\Psi(y^{\prime})(y-y^{\prime})\rangle.\]

**Lemma 4**: _Generalized Bregman divergence of negentropy Let \(\alpha\in\mathbb{R}\). Let \(\Psi(y^{\prime}):=\sum_{i=1}^{k}y^{\prime}_{i}\log y^{\prime}_{i}-\alpha\sum_ {i=1}^{k}y^{\prime}_{i}\) be defined for \(y^{\prime}\in\mathbb{R}^{k}_{+}\). Then, for \(y,y^{\prime}\in\mathbb{R}^{k}_{+}\),_

\[D_{\Psi}(y,y^{\prime})=\sum_{i=1}^{k}y_{i}\log\frac{y_{i}}{y^{\prime}_{i}}-\sum _{i=1}^{k}(y_{i}-y^{\prime}_{i})+\iota_{\mathbb{R}^{k}_{++}}(y^{\prime}),\]

_which does not depend on \(\alpha\)._

**Proof.** First, if \(y^{\prime}\in\mathbb{R}^{k}_{++}\), \(\Psi\) is differentiable at \(y^{\prime}\) and \(\nabla\Psi(y^{\prime})_{i}=\log y_{i}+1-\alpha\). Thus, \(\partial\Psi(y^{\prime})=\{\nabla\Psi(y^{\prime})\}\) and

\[D_{\Psi}(y,y^{\prime}) =\Psi(y)-\Psi(y^{\prime})-\sup_{\theta^{\prime}\in\partial\Psi(y^ {\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle,\] \[=\Psi(y)-\Psi(y^{\prime})-\langle y-y^{\prime},\nabla\Psi(y^{ \prime})\rangle,\] \[=\sum_{i=1}^{k}y_{i}\log\frac{y_{i}}{y^{\prime}_{i}}-\sum_{i=1}^{k} (y_{i}-y^{\prime}_{i}).\]Second, if we prove that \(\partial\Psi(y^{\prime})=\emptyset\) when there exists a component \(i\) such that \(y^{\prime}_{i}=0\), we can conclude the proof, as \(\sup_{\emptyset}=-\infty\) by convention. Indeed, Let us assume that \(y^{\prime}_{i}=0\). Suppose that \(\theta^{\prime}\in\partial\Psi(y^{\prime})\). Then, by definition of subgradients,

\[\langle y^{\prime\prime}-y^{\prime},\theta^{\prime}\rangle+\Psi(y^{\prime}) \leq\Psi(y^{\prime\prime}),\;\forall y^{\prime\prime}\in\mathbb{R}^{k}_{++}.\]

We consider \(\varepsilon>0\) and choose \(y^{\prime\prime}=y^{\prime}+\varepsilon e_{i}\), where \(e_{i}\) is the \(i\)-th canonical base vector. Thus, we obtain

\[\varepsilon\theta_{i} \leq\Psi(y^{\prime}+\varepsilon e_{i})-\Psi(y^{\prime}),\] \[=\sum_{j=1}^{k}y^{\prime}_{j}\log y^{\prime}_{j}+\varepsilon\log \varepsilon-\alpha\sum_{j=1}^{k}y^{\prime}_{j}-\alpha\varepsilon-\big{(}\sum _{j=1}^{k}y^{\prime}_{j}\log y^{\prime}_{j}-\alpha\sum_{j=1}^{k}y^{\prime}_{ j}\big{)},\] \[=\varepsilon\log\varepsilon-\alpha\varepsilon,\]

as \(y_{i}=0\) and \(0\log 0=0\) by convention. By noticing that \(\lim_{\varepsilon\to 0^{+}}\big{(}\varepsilon\log\varepsilon-\alpha \varepsilon\big{)}/\varepsilon=-\infty\), we get a contradiction, which concludes the proof.

**Lemma 5**: _Subdifferential inclusion for \(\sup\) function_

_Let_ \(Y\subset\mathbb{R}^{k}\) _be some set. Let_ \(\{f_{y}\}_{y\in Y}\) _be a family of finite functions_ \(f_{y}:\mathbb{R}^{k}\to\mathbb{R}\)_. We define the function_ \(g:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) _by_ \(g(\theta)=\sup_{y\in Y}f_{y}(\theta)\)_, for any_ \(\theta\in\mathbb{R}^{k}\)_. Then, we have_

\[\partial f_{\tilde{y}}(\theta)\subset\partial g(\theta),\]

_for all_ \(\theta\in\mathbb{R}^{k}\) _and for all_ \(\tilde{y}\in\operatorname*{argmax}_{y\in Y}f_{y}(\theta)\)_._

**Proof.** Left as an exercise.

**Lemma 6**: _Value and gradient of \(\Psi^{*}_{y}\)_

_Let_ \(\Psi:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) _be a proper strictly convex function. Let us assume that_ \(D_{\Psi}(y,y^{\prime})\) _is convex w.r.t_ \(y^{\prime}\)_, for all_ \(y\in\operatorname*{dom}\Psi\)_. We remind that_ \(\Psi_{y}(y^{\prime})=\Psi(y^{\prime})+D_{\Psi}(y,y^{\prime})\)_. Then, for all_ \(\theta\in\mathbb{R}^{k}\) _and for all_ \(\tilde{y}\in\operatorname*{argmax}_{y^{\prime}\in\operatorname*{dom}\Psi} \langle y^{\prime},\theta\rangle+\langle y-y^{\prime},\nabla\Psi(y^{\prime})\rangle\)_,_

\[\Psi^{*}_{y}(\theta) =\langle\tilde{y},\theta\rangle-\Psi(y)+\langle y-\tilde{y}, \nabla\Psi(\tilde{y})\rangle\] \[\nabla\Psi^{*}_{y}(\theta) =\tilde{y}.\]

_Furthermore, if the function_ \(\Psi\) _is twice differentiable, we have_

\[\operatorname*{argmax}_{y^{\prime}\in\operatorname*{dom}\Psi}\langle y^{ \prime},\theta\rangle+\langle y-y^{\prime},\nabla\Psi(y^{\prime})\rangle\subset \big{\{}y^{\prime}\in\operatorname*{dom}\Psi\;\big{|}\;\nabla^{2}\Psi(y^{ \prime})(y^{\prime}-y)=\theta-\nabla\Psi(y^{\prime})\big{\}}.\]

**Proof.** As \(\Psi\leq\Psi_{y}\), we have \(\operatorname*{dom}\Psi_{y}\subset\operatorname*{dom}\Psi\). Thus, we get

\[\Psi^{*}_{y}(\theta) =\sup_{y^{\prime}\in\operatorname*{dom}\Psi}\langle y^{\prime}, \theta\rangle-\Psi_{y}(y^{\prime})\] \[=\sup_{y^{\prime}\in\operatorname*{dom}\Psi}\langle y^{\prime}, \theta\rangle-(\Psi(y^{\prime})+\Psi(y)-\Psi(y^{\prime})-\langle y-y^{\prime}, \nabla\Psi(y^{\prime})\rangle)\] \[=\sup_{y^{\prime}\in\operatorname*{dom}\Psi}\langle y^{\prime}, \theta\rangle-\Psi(y)+\langle y-y^{\prime},\nabla\Psi(y^{\prime})\rangle.\]

As \(\Psi\) is strictly convex and \(D_{\Psi}(y,y^{\prime})\) is convex w.r.t. \(y^{\prime}\), we have that \(\Psi_{y}\) is strictly convex. Thus, according to [27, Theorem 11.13], \(\Psi^{*}_{y}\) is differentiable. Thus,

\[\{\nabla\Psi^{*}_{y}(\theta)\} =\partial\Psi^{*}_{y}(\theta)\] \[\supset\operatorname*{argmax}_{y^{\prime}\in\operatorname*{dom}\Psi }\langle y^{\prime},\theta\rangle-\Psi(y)+\langle y-y^{\prime},\nabla\Psi(y^{ \prime})\rangle\] \[=\operatorname*{argmax}_{y^{\prime}\in\operatorname*{dom}\Psi} \langle y^{\prime},\theta\rangle+\langle y-y^{\prime},\nabla\Psi(y^{\prime})\rangle,\]

using Lemma 5 for the inclusion.

In the case that \(\Psi\) is twice differentiable, setting the gradient of the inner function to zero concludes the proof.

**Lemma 7**: _Gradient of \(\Psi_{y}^{*}\), squared norm case_

_Let \(\Psi(y^{\prime})\coloneqq\frac{1}{2}\|y^{\prime}\|_{2}^{2}\) and \(\Psi_{y}\) defined as in Lemma 6. Then, the gradient of \(\Psi_{y}^{*}\) is given by_

\[\nabla\Psi_{y}^{*}(\theta)=\frac{y+\theta}{2}.\]

**Proof.** Let us notice that

\[\sup_{y^{\prime}\in\operatorname{dom}\Psi}\langle y^{\prime},\theta\rangle+ \langle y-y^{\prime},\nabla\Psi(y^{\prime})\rangle=\sup_{y^{\prime}\in \operatorname{dom}\Psi}\langle y^{\prime},\theta+y^{\prime}\rangle-\left\|y^ {\prime}\right\|^{2},\]

and that \(\Psi\) is strictly convex function. By strong convexity of \(y^{\prime}\mapsto\left\|y^{\prime}\right\|^{2}\), we deduce that \(\operatorname{argmax}_{y^{\prime}\in\operatorname{dom}\Psi}\langle y^{\prime },\theta\rangle+\langle y-y^{\prime},\nabla\Psi(y^{\prime})\rangle^{\prime}\neq\emptyset\). Furthermore, as \(\Psi\) is twice differentiable and \(\nabla^{2}\Psi(y^{\prime})=I\) and \(\operatorname{dom}\Psi=\mathbb{R}^{k}\), we have that \(\left\{y^{\prime}\in\operatorname{dom}\Psi\,\left|\,\nabla^{2}\Psi(y^{\prime })(y^{\prime}-y)=\theta-\nabla\Psi(y^{\prime})\right.\right\}\) is a singleton. Thus, we use Lemma 6 with \(\nabla\Psi(y^{\prime})=y^{\prime}\) and \(\nabla^{2}\Psi(y^{\prime})=I\), we obtain that \(\nabla\Psi_{y}^{*}(\theta)\) is the solution w.r.t. \(y^{\prime}\) of the equation \(y^{\prime}-y=\theta-y^{\prime}\). Rearranging the terms concludes the proof.

Before stating the next lemma, we recall the definition of the Lambert function [13]\(W:\mathbb{R}_{+}\to\mathbb{R}_{+}\). The function \(W\) is the inverse of the function \(f:\mathbb{R}_{+}\to\mathbb{R}_{+}\) where \(f(w)=w\exp(w)\) for all \(w\in\mathbb{R}_{+}\).

**Lemma 8**: _Gradient of \(\Psi_{y}^{*}\), negentropy case_

_Let \(\Psi(y^{\prime})\coloneqq\sum_{i=1}^{k}y^{\prime}_{i}\log y^{\prime}_{i}- \alpha\sum_{i=1}^{k}y^{\prime}_{i}\) be defined for \(y^{\prime}\in\mathbb{R}_{+}^{k}\). Then,_

\[\nabla\Psi_{y}^{*}(\theta)_{i}=\left\{\begin{array}{ll}\frac{\mathrm{e}^{ \theta_{i}-2+\alpha}}{\frac{y_{i}}{W(y,\mathrm{e}^{-(\theta_{i}-2+\alpha)})}},& \mbox{if }y_{i}=0\\ \end{array}\right.\]

**Proof.** Following the same arguments as in the proof of Lemma 7, we use Lemma 6. We know that \(\tilde{y}\) is the solution of \(\nabla^{2}\Psi(\tilde{y})(\tilde{y}-y)=\theta-\nabla\Psi(\tilde{y})\). Using \(\nabla\Psi(\tilde{y})=\log\tilde{y}+1-\alpha\) and \(\nabla^{2}\Psi(\tilde{y})=1/\tilde{y}\) (where logarithm and division are performed element-wise), we obtain for all \(i\in\{1,\ldots,k\}\)

\[(\tilde{y}_{i}-y_{i})/\tilde{y}_{i}=\theta_{i}-\log\tilde{y}_{i}-1+\alpha \iff 1-y_{i}/\tilde{y}_{i}=\theta_{i}-\log\tilde{y}_{i}-1+\alpha.\]

When \(y_{i}=0\), we immediately have \(\tilde{y}_{i}=\exp(\theta_{i}-2+\alpha)\). When \(y_{i}>0\), after rearranging, we obtain

\[\frac{y_{i}}{\tilde{y}_{i}}\exp\left(\frac{y_{i}}{\tilde{y}_{i}}\right)=y_{i} \exp(-(\theta_{i}-2+\alpha))\iff\frac{y_{i}}{\tilde{y}_{i}}=W(y_{i}\exp(-( \theta_{i}-2+\alpha))),\]

hence the result.

**Lemma 9**: _Gradient of \(\Omega_{y}^{*}\)_

_Let \(\Psi:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) be a proper strictly convex l.s.c. function and assume that, for all \(y\in\mathbb{R}^{k}\), the function \(D_{\Psi}(y,\cdot)\) is convex. Let \(\Omega_{y}=\Psi_{y}+\iota_{\mathcal{C}}\) with \(\Psi_{y}\) defined as in Lemma 6 and assume that \(\mathcal{C}\) is a nonempty compact convex set included in \(\mathrm{dom}\,\Psi\). Then \(\nabla\Omega_{y}^{*}(\theta)\) is the unique element of the set_

\[\operatorname*{argmax}_{y^{\prime}\in\mathcal{C}}\ \langle y^{\prime},\theta \rangle+\langle y-y^{\prime},\nabla\Psi(y^{\prime})\rangle.\]

**Proof.** The result follows from Danskin's theorem [4, Proposition B.25].

**Lemma 10**: _Dual of simplex-constrained conjugate_

_Let \(\Psi:\mathbb{R}^{k}\to\overline{\mathbb{R}}\) be a proper strictly convex function. Then,_

\[(\Psi+\iota_{\triangle^{k}})^{*}(\theta)=\min_{\tau\in\mathbb{R}}\tau+(\Psi+ \iota_{\mathbb{R}_{+}^{k}})^{*}(\theta-\tau\mathbf{1}).\]

_and for any \(\tau^{\star}\in\operatorname*{argmin}_{\tau\in\mathbb{R}}\tau+(\Psi+\iota_{ \mathbb{R}_{+}^{k}})^{*}(\theta-\tau\mathbf{1})\), we get_

\[\nabla(\Psi+\iota_{\triangle^{k}})^{*}(\theta)=\nabla(\Psi+\iota_{\mathbb{R}_{ +}^{k}})^{*}(\theta-\tau^{\star}\mathbf{1}).\]

**Proof.** We have by the definition of the Fenchel conjugate that \((\Psi+\iota_{\triangle^{k}})^{*}(\theta)=\max_{y^{\prime}\in\triangle^{k}} \big{(}\langle y^{\prime},\theta\rangle-\Psi(y^{\prime})\big{)}\). As the unit simplex \(\triangle^{k}\) is a compact set and as \((\Psi+\iota_{\triangle^{k}})^{*}\) is differentiable (by [27, Theorem 11.13] given that \(\Psi\) is strictly convex and \(\triangle^{k}\) is convex ), we apply Danskin's theorem [4, Proposition B.25] and get \(\{\nabla(\Psi+\iota_{\triangle^{k}})^{*}(\theta)\}=\operatorname*{argmax}_{y^ {\prime}\in\triangle^{k}}\big{(}\langle y^{\prime},\theta\rangle-\Psi(y^{ \prime})\big{)}\).

Furthermore, we rewrite \((\Psi+\iota_{\triangle^{k}})^{*}\) in the following dual minimization problem

\[(\Psi+\iota_{\triangle^{k}})^{*}(\theta) =\max_{y^{\prime}\in\triangle^{k}}\langle y^{\prime},\theta \rangle-\Psi(y^{\prime})\] \[=\max_{y^{\prime}\in\mathbb{R}_{+}^{k}}\min_{\tau\in\mathbb{R}} \langle y^{\prime},\theta\rangle-\Psi(y^{\prime})-\tau(\langle y^{\prime}, \mathbf{1}\rangle-1)\] \[=\min_{\tau\in\mathbb{R}}\tau+\max_{y^{\prime}\in\mathbb{R}_{+}^ {k}}\langle y^{\prime},\theta-\tau\mathbf{1}\rangle-\Psi(y^{\prime})\] \[=\min_{\tau\in\mathbb{R}}\tau+(\Psi+\iota_{\mathbb{R}_{+}^{k}})^{* }(\theta-\tau\mathbf{1}).\]

Again, as \(\Psi\) is strictly convex, \((\Psi+\iota_{\mathbb{R}_{+}^{k}})^{*}\) is differentiable [27, Theorem 11.13]. After some elementary calculus on subdifferentials, we conclude by applying primal and dual forms of Fermat's rule [27, Theorem 11.39, Item (d)]: for any \(\tau^{\star}\in\operatorname*{argmin}_{\tau\in\mathbb{R}}\tau+(\Psi+\iota_{ \mathbb{R}_{+}^{k}})^{*}(\theta-\tau\mathbf{1})\), we have \(\nabla(\Psi+\iota_{\mathbb{R}_{+}^{k}})^{*}(\theta-\tau^{\star}\mathbf{1})\in \operatorname*{argmax}_{y^{\prime}\in\triangle^{k}}\big{(}\langle y^{\prime}, \theta\rangle-\Psi(y^{\prime})\big{)}\).

**Lemma 11**: _Gradient of \(\Omega^{*}_{y}\), negentropy, constrained to the simplex_

_Let \(\Psi(y^{\prime})=\langle y^{\prime},\log y^{\prime}\rangle\), if \(y^{\prime}\in\mathbb{R}^{k}_{+}\), \(+\infty\) otherwise. Then, for all \(y\in\mathbb{R}^{k}_{+}\), the gradient of \(\Omega_{y}=\Psi_{y}+\iota_{\triangle^{k}}\) (as defined in Lemma 9) is given by_

\[\nabla\Omega^{*}_{y}(\theta)_{i}=\begin{cases}\mathrm{e}^{-\lambda^{*}}\mathrm{ e}^{\theta_{i}}&\text{if }y_{i}=0,\\ \frac{y_{i}}{W(y,\mathrm{e}^{\lambda^{*}-\theta_{i}})}&\text{if }y_{i}>0.\end{cases}\]

_where \(\lambda^{\star}\) is the unique solution of the equation_

\[\mathrm{e}^{-\lambda^{*}}\sum_{i:y_{i}=0}\mathrm{e}^{\theta_{i}}+\sum_{i:y_{i }>0}\frac{y_{i}}{W(y_{i}\mathrm{e}^{-(\theta_{i}-\lambda^{*})})}=1.\]

**Proof.** From Lemma 9 and Lemma 10, since \(\mathrm{dom}\,\Psi_{y}=\mathbb{R}^{k}_{+}\), we have

\[y^{\star}=\nabla\Omega^{*}_{y}(\theta)=\nabla\Psi^{*}_{y}(\theta-\tau^{\star} \mathbf{1})\]

for any solution \(\tau^{\star}\) of

\[\min_{\tau\in\mathbb{R}}\tau+\Psi^{*}_{y}(\theta-\tau\mathbf{1}).\]

As \(\tau\mapsto\tau+\Psi^{*}_{y}(\theta-\tau\mathbf{1})\) is a convex function, Setting the gradient of the inner function to zero, we get

\[\tau^{\star}\in\operatorname*{argmin}_{\tau\in\mathbb{R}}\tau+\Psi^{*}_{y}( \theta-\tau\mathbf{1})\iff\langle\nabla\Psi^{*}_{y}(\theta-\tau^{\star} \mathbf{1}),\mathbf{1}\rangle=1.\]

Using Lemma 8, we obtain that \(\tau^{\star}\) satisfies

\[\mathrm{e}^{-\tau^{\star}-2}\sum_{i:y_{i}=0}\mathrm{e}^{\theta_{i}}+\sum_{i:y _{i}>0}\frac{y_{i}}{W(y_{i}\mathrm{e}^{-(\theta_{i}-\tau^{\star}-2)})}=1.\]

By monotonicity in \(\tau^{\star}\), we conclude that \(\tau^{\star}\) exists and is unique. Using the change of variable \(\tau^{\star}=\lambda^{\star}+2\) concludes the proof.

### Proof of Proposition 1 (Properties of Fitzpatrick losses)

Apart from differentiability, the proofs follow from the study of Fitzpatrick functions found in [15, 2, 28]. We include the proofs for completeness.

Link function and non-negativity.We recall that

\[L_{F[\partial\Omega]}(y,\theta) =\sup_{(y^{\prime},\theta^{\prime})\in\partial\Omega}\langle y^{ \prime}-y,\theta-\theta^{\prime}\rangle\] \[=-\inf_{(y^{\prime},\theta^{\prime})\in\partial\Omega}\langle y^{ \prime}-y,\theta^{\prime}-\theta\rangle.\]

From the monotonicity of \(\partial\Omega\), we have that if \((y,\theta)\in\partial\Omega\) and \((y^{\prime},\theta^{\prime})\in\partial\Omega\), then \(\langle y^{\prime}-y,\theta^{\prime}-\theta\rangle\geq 0\). Therefore, for all \((y,\theta)\in\partial\Omega\),

\[\inf_{(y^{\prime},\theta^{\prime})\in\partial\Omega}\langle y^{\prime}-y, \theta^{\prime}-\theta\rangle=0,\]

with the infimum being attained at \((y^{\prime},\theta^{\prime})=(y,\theta)\). This proves the link function.

From the maximality of \(\partial\Omega\), if \((y,\theta)\not\in\partial\Omega\), there exists \((y^{\prime},\theta^{\prime})\in\partial\Omega\) such that \(\langle y^{\prime}-y,\theta^{\prime}-\theta\rangle<0\). Therefore, for all \((y,\theta)\not\in\partial\Omega\),

\[\inf_{(y^{\prime},\theta^{\prime})\in\partial\Omega}\langle y^{\prime}-y, \theta^{\prime}-\theta\rangle<0.\]

This proves the non-negativity.

Separable convexity.We recall that

\[L_{F[\partial\Omega]}(y,\theta)=F[\partial\Omega](y,\theta)-\langle y,\theta\rangle\]

where

\[F[\partial\Omega](y,\theta)=\sup_{(y^{\prime},\theta^{\prime})\in\partial\Omega} \langle y-y^{\prime},\theta^{\prime}\rangle+\langle y^{\prime},\theta\rangle= \sup_{(y^{\prime},\theta^{\prime})\in\partial\Omega}\langle y^{\prime},\theta \rangle+\langle y,\theta^{\prime}\rangle-\langle y^{\prime},\theta^{\prime}\rangle.\]

The function \((y,\theta)\mapsto\langle y^{\prime},\theta\rangle+\langle y,\theta^{\prime} \rangle-\langle y^{\prime},\theta^{\prime}\rangle\) is (jointly) convex in \((y,\theta)\) for all \((y^{\prime},\theta^{\prime})\). Since the supremum preserves convexity, \(F[\partial\Omega](y,\theta)\) is (jointly) convex in \((y,\theta)\). The function \(\langle y,\theta\rangle\) is convex in \(y\) and \(\theta\) but not (jointly) convex in \((y,\theta)\). Therefore, \(L_{F[\partial\Omega]}(y,\theta)\) is separately convex in \(y\) and \(\theta\).

SubgradientWe are going to prove that, for any \(y,\theta\in\mathbb{R}^{k}\) and for any \(y^{\star}\in y^{\star}_{F[\partial\Omega]}(y,\theta)\) as defined in (4), we have \(y^{\star}-y\in\partial_{\theta}L_{F[\partial\Omega]}(y,\theta)\).

Indeed, by Definition 1, we have that

\[L_{F[\partial\Omega]}(y,\theta)=\sup_{y^{\prime}\in\operatorname{dom}\Omega} \ \left[\langle y^{\prime}-y,\theta\rangle+\sup_{\theta^{\prime}\in\partial\Omega (y^{\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle\right].\]

Furthermore, \(\partial f_{y^{\prime}}(\theta)=\{y^{\prime}-y\}\), where \(f_{y^{\prime}}(\theta)=\langle y^{\prime},\theta\rangle+\sup_{\theta^{\prime }\in\partial\Omega(y^{\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle.\) Thus, by applying Lemma 5, we get

\[y^{\star}-y\in\partial_{\theta}L_{F[\partial\Omega]}(y,\theta)\]

for any \(y^{\star}\in\operatorname{argmax}_{y^{\prime}\in\operatorname{dom}\Omega}f_{ y^{\prime}}(\theta)=\operatorname{argmax}_{y^{\prime}\in\operatorname{dom}\Omega}\ \left[\langle y^{\prime}-y,\theta\rangle+\sup_{\theta^{\prime}\in\partial \Omega(y^{\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle\right],\) which yields the result by definition of \(y^{\star}_{F[\partial\Omega]}(y,\theta)\) in (4).

Differentiability.Since the function \(\Omega\) is strictly convex and \(y^{\prime}\mapsto D_{\Omega}(y,y^{\prime})\) is convex, then \(\Omega_{y}(y^{\prime})=\Omega(y^{\prime})+D_{\Omega}(y,y^{\prime})\) is strictly convex in \(y^{\prime}\). From the duality between strict convexity and differentiability, \(\Omega^{*}_{y}(\theta)\) is differentiable in \(\theta\).

Tighter inequality.Using

\[\partial\Omega=\{(y^{\prime},\theta^{\prime})\in\mathbb{R}^{k}\times\mathbb{R }^{k}\ \big{|}\ \Omega(y)\geq\Omega(y^{\prime})+\langle y-y^{\prime},\theta^{\prime}\rangle\ \forall y\}\]

and

\[\Omega^{*}(\theta)=\sup_{y^{\prime}\in\mathbb{R}^{k}}\langle y^{\prime},\theta \rangle-\Omega(y^{\prime}),\]

we get for any \((y^{\prime},\theta^{\prime})\in\partial\Omega\),

\[\langle y-y^{\prime},\theta^{\prime}\rangle+\langle y^{\prime},\theta\rangle \leq\Omega(y)-\Omega(y^{\prime})+\langle y^{\prime},\theta\rangle\] \[\leq\Omega(y)+\Omega^{*}(\theta).\]

Therefore

\[F[\partial\Omega](y,\theta)=\sup_{(y^{\prime},\theta^{\prime})\in\partial \Omega}\langle y-y^{\prime},\theta^{\prime}\rangle+\langle y^{\prime},\theta \rangle\leq\Omega(y)+\Omega^{*}(\theta).\]

### Proof of Proposition 2 (Expression of Fitzpatrick loss when \(\Omega\) is twice differentiable)

We recall that

\[F[\partial\Omega](y,\theta)=\sup_{(y^{\prime},\theta^{\prime})\in\partial \Omega}\langle y,\theta^{\prime}\rangle+\langle y^{\prime},\theta\rangle- \langle y^{\prime},\theta^{\prime}\rangle=\sup_{y^{\prime}\in\operatorname{ dom}\Omega}\langle y^{\prime},\theta\rangle+\sup_{\theta^{\prime}\in\partial \Omega(y^{\prime})}\langle y,\theta^{\prime}\rangle-\langle y^{\prime},\theta^{ \prime}\rangle.\]

Since \(\Omega\) is differentiable, we have \(\partial\Omega(y^{\prime})=\{\nabla\Omega(y^{\prime})\}\) and therefore \(\theta^{\prime}=\nabla\Omega(y^{\prime})\), which gives

\[F[\partial\Omega](y,\theta)=\sup_{y^{\prime}\in\operatorname{dom}\Omega} \langle y,\nabla\Omega(y^{\prime})\rangle+\langle y^{\prime},\theta\rangle- \langle y^{\prime},\nabla\Omega(y^{\prime})\rangle.\]

Setting the gradient of the inner function w.r.t. \(y^{\prime}\) to zero, we get, for any \(y^{\star}\in y^{\star}_{F[\partial\Omega]}(y,\theta)\) as defined in (4),

\[\nabla^{2}\Omega(y^{\star})y+\theta-\nabla\Omega(y^{\prime})-\nabla^{2}\Omega(y ^{\star})y^{\star}=0.\]

Using the \(y^{\prime}=y^{\star}\) and \(\theta^{\prime}=\nabla\Omega(y^{\star})\) in Definition 1, we then obtain

\[L_{F[\partial\Omega]}(y,\theta) =\langle y^{\prime}-y,\theta-\theta^{\prime}\rangle\] \[=\langle y^{\star}-y,\theta-\nabla\Omega(y^{\star})\rangle\] \[=\langle y^{\star}-y,\nabla^{2}\Omega(y^{\star})(y^{\star}-y)\rangle.\]

### Proof of Proposition 3 (squared loss)

Using Proposition 2 with \(\nabla\Omega(y^{\prime})=y^{\prime}\) and \(\nabla^{2}\Omega(y^{\prime})=I\), we obtain

\[y+\theta-2y^{\prime}=0\iff y^{\prime}=\frac{y+\theta}{2}.\]

We therefore obtain

\[L_{F[\partial\Omega]}(y,\theta) =\left\langle\frac{y+\theta}{2}-y,\theta-\frac{y+\theta}{2}\right\rangle\] \[=\left\langle\frac{\theta-y}{2},\frac{\theta-y}{2}\right\rangle\] \[=\frac{1}{4}\|y-\theta\|_{2}^{2}.\]

### Proof of Proposition 4 (perceptron loss)

A proof of the Fitzpatrick function for this case was given in [2, Example 3.1]. We include a proof for completeness. Since \(\Omega=\iota_{\mathcal{C}}\), we have \(\operatorname{dom}\Omega=\mathcal{C}\). Therefore, for all \(y\in\mathcal{C}\) and \(\theta\in\mathbb{R}^{k}\),

\[F[\partial\Omega](y,\theta) =\sup_{y^{\prime}\in\operatorname{dom}\Omega}\langle y^{\prime}, \theta\rangle+\sup_{\theta^{\prime}\in\partial\Omega(y^{\prime})}\langle y-y^ {\prime},\theta^{\prime}\rangle\] \[=\sup_{y^{\prime}\in\mathcal{C}}\langle y^{\prime},\theta\rangle -\left(\iota_{\mathcal{C}}(y)-\iota_{\mathcal{C}}(y^{\prime})-\sup_{\theta^{ \prime}\in\partial\iota_{\mathcal{C}}(y^{\prime})}\langle y-y^{\prime},\theta ^{\prime}\rangle\right)\] \[=\sup_{y^{\prime}\in\mathcal{C}}\langle y^{\prime},\theta\rangle -D_{\iota_{\mathcal{C}}}(y,y^{\prime})\] \[=\sup_{y^{\prime}\in\mathcal{C}}\langle y^{\prime},\theta\rangle,\]

where in the third line we used that \(\iota_{\mathcal{C}}(y)=\iota_{\mathcal{C}}(y^{\prime})=0\) and where in the last line we used Lemma 2. Therefore, for all \(y\in\mathbb{R}^{k}\) and \(\theta\in\mathbb{R}^{k}\),

\[F[\partial\Omega](y,\theta)=\sup_{y^{\prime}\in\mathcal{C}}\langle y^{\prime}, \theta\rangle+\iota_{\mathcal{C}}(y)=\iota_{\mathcal{C}}(y)+\iota_{\mathcal{C }}^{*}(\theta).\]

### Proof of Proposition 5 (Fitzpatrick sparseMAP loss)

A proof of the Fitzpatrick function for this case was given in [2, Example 3.13]. We provide an alternative proof.

From Proposition 7, we know that for any \(y\in\operatorname{dom}\Omega(=\mathcal{C}\) here)

\[F[\partial\Omega](y,\theta)=\Omega_{y}(y)+\Omega_{y}^{*}(\theta)=\Omega(y)+ \Omega_{y}^{*}(\theta),\]

where

\[\Omega_{y}(y^{\prime}) =\frac{1}{2}\|y^{\prime}\|_{2}^{2}+\frac{1}{2}\|y-y^{\prime}\|_{ 2}^{2}+\iota_{\mathcal{C}}(y^{\prime})\] \[=\|y^{\prime}\|_{2}^{2}+\frac{1}{2}\|y\|_{2}^{2}-\langle y,y^{ \prime}\rangle+\iota_{\mathcal{C}}(y^{\prime})\] \[=2\Omega(y^{\prime})+\Omega(y)-\langle y,y^{\prime}\rangle.\]

Using conjugate calculus, we obtain

\[\Omega_{y}^{*}(\theta)=2\Omega^{*}\left(\frac{y+\theta}{2}\right)-\Omega(y).\]

Therefore,

\[F[\partial\Omega](y,\theta)=2\Omega^{*}\left(\frac{y+\theta}{2}\right).\]

From Proposition 7, the supremum w.r.t. \(y^{\prime}\) is achieved at \(y^{\star}=\nabla\Omega^{*}((y+\theta)/2)=P_{\mathcal{C}}((y+\theta)/2)\). We therefore obtain

\[L_{F[\partial\Omega]}(y,\theta)=\langle y^{\star}-y,\theta-y^{\star}\rangle.\]

[MISSING_PAGE_FAIL:20]

then

\[g(\lambda)=\underbrace{\mathrm{e}^{-\lambda}\sum_{i:y_{i}=0}\mathrm{e}^{\theta_{i} }}_{\leq 1/2}\mathrm{e}^{\theta_{i}}+\sum_{i:y_{i}>0}\underbrace{\frac{y_{i}}{ W(y_{i}\mathrm{e}^{\lambda^{-\theta_{i}}})}}_{\leq 1/\left(2\ell_{0}(y)\right)}\leq 1.\]

Thus, all \(\lambda\) satisfying the following inequalities are upper bounds of \(\lambda^{\star}\)

\[2\sum_{i:y_{i}=0}\mathrm{e}^{\theta_{i}}\leq\mathrm{e}^{\lambda}\] \[2\ell_{0}(y)y_{i}\leq W(y_{i}\mathrm{e}^{\lambda-\theta_{i}}), \forall i:y_{i}>0.\]

As \(W\) is monotone and \(W^{-1}(t)=t\mathrm{e}^{t}\), we get

\[\log 2+\log\sum_{i:y_{i}=0}\mathrm{e}^{\theta_{i}} \leq\lambda\] \[2\ell_{0}(y)\mathrm{e}^{2\ell_{0}(y)y_{i}} \leq\mathrm{e}^{\lambda^{\star}-\theta_{i}},\forall i:y_{i}>0.\]

Thus taking \(\lambda=\max\Big{\{}\log 2+\log\sum_{i:y_{i}=0}^{k}\mathrm{e}^{\theta_{i}}, \max_{i:y_{i}>0}\log 2+\log\ell_{0}(y)+\theta_{i}+2\ell_{0}(y)y_{i} \Big{\}}\) yields an upper bound of \(\lambda^{\star}\).

### Proof of Proposition 7 (characterization of \(F[\partial\Omega]\) using \(D_{\Omega}\))

Let \((y,\theta)\in\mathrm{dom}\,\Omega\times\mathbb{R}^{k}\). We have

\[F[\partial\Omega](y,\theta) =\sup_{(y^{\prime},\theta^{\prime})\in\partial\Omega}\langle y-y^ {\prime},\theta^{\prime}\rangle+\langle y^{\prime},\theta\rangle\] \[=\sup_{y^{\prime}\in\mathrm{dom}\,\Omega}\left\{\langle y^{\prime },\theta\rangle+\sup_{\theta^{\prime}\in\partial\Omega(y^{\prime})}\langle y- y^{\prime},\theta^{\prime}\rangle\right\}\] \[=\sup_{y^{\prime}\in\mathrm{dom}\,\Omega}\left\{\langle y^{\prime },\theta\rangle-\Omega(y^{\prime})+\Omega(y^{\prime})+\sup_{\theta^{\prime}\in \partial\Omega(y^{\prime})}\langle y-y^{\prime},\theta^{\prime}\rangle\right\}\] \[=\Omega(y)+\sup_{y^{\prime}\in\mathrm{dom}\,\Omega}\langle y^{ \prime},\theta\rangle-\left(\Omega(y^{\prime})+D_{\Omega}(y,y^{\prime})\right)\] \[=\Omega(y)+(\Omega+D_{\Omega}(y,\cdot))^{\ast}\left(\theta\right)\] \[=\Omega_{y}(y)+\Omega_{y}^{\ast}(\theta).\]

The supremum above is achieved at some \(y^{\prime}\in\partial\Omega_{y}^{\ast}(\theta)\).

When \(\Omega=\Psi+\iota_{\mathcal{C}}\), where \(\mathcal{C}\subseteq\mathrm{dom}\,\Psi\) is a nonempty closed convex set, using Lemmas 1 and 2, we have for all \(y\in\mathcal{C}\)

\[\Omega_{y}(y^{\prime})=\Psi(y^{\prime})+D_{\Psi}(y,y^{\prime})+\iota_{ \mathcal{C}}(y^{\prime}).\]

### Proof of Proposition 8 (lower bound)

It was shown in [8, Proposition 3] that if \(f=g+\iota_{\mathcal{C}}\), where the function \(g\) is Legendre-type with \(\mathcal{C}\subseteq\mathrm{dom}\,\Psi\) a nonempty closed convex set, then for all \(y\in\mathcal{C}\) and \(\theta\in\mathbb{R}^{k}\),

\[0\leq D_{g}(y,\nabla f^{\ast}(\theta))\leq L_{f\oplus f^{\ast}}(y,\theta),\]

with equality if \(\mathcal{C}=\mathrm{dom}\,g\). Using \(g=\Psi_{y}\), \(f=\Omega_{y}=\Psi_{y}+\iota_{\mathcal{C}}\), \(y^{\star}=\nabla\Omega_{y}^{\ast}(\theta)=y_{F[\partial\Omega]}^{\ast}(y,\theta)\), and Lemma 3, we therefore obtain

\[D_{\Psi_{y}}(y,y^{\star})=\langle y-y^{\star},\nabla^{2}\Psi(y^{\star})(y-y^{ \star})\rangle\leq L_{\Omega_{y}\oplus\Omega_{y}^{\ast}}(y,\theta)=L_{F[\partial \Omega]}(y,\theta).\]

If \(\Psi_{y}\) is \(\mu\)-strongly convex and \(D_{\Psi}\) is convex in its second argument, then \(\Psi_{y}\) is \(\mu\)-strongly convex as well. Therefore, we also have

\[\frac{\mu}{2}\|y-y^{\star}\|_{2}^{2}\leq D_{\Psi_{y}}(y,y^{\star}).\]Comment on the inf-addition convention \(+\infty+(-\infty)=+\infty\)

The question of adding conflicting infinities has been thoroughly studied by Moreau in [23]. In this paper, Moreau introduced two additions for \(\mathbb{R}\cup\{-\infty,+\infty\}\). One of Moreau's two conventions for summing infinities can be found under the name of inf-addition in [27, Chapter 1]. The other convention is named sup-addition.

Using the inf-addition on \(\mathbb{R}\cup\{-\infty,+\infty\}\) allows, for instance, the equivalence of \(\min_{x\in C}f(x)\) and \(\min_{x\in\mathbb{R}^{n}}f(x)+\iota_{C}(x)\). The same goes for the sup-addition but for maximization problems.

We used the inf-addition convention for the definition of the generalized Bregman divergence in (1). As the generalized Bregman divergence is to be thought of as a generalization of a distance, it is generally a quantity that will be minimized. Therefore, the inf-addition is a good fit for the definition of the generalized Bregman divergence.

In the proof of Proposition 7 in B.8, the choice of inf-addition is corroborated by the fact that we want to calculate, for some fixed \(y\in\mathbb{R}^{k}\), \(\big{(}\Omega(\cdot)+D_{\Omega}(y,\cdot)\big{)}(\theta)=\sup_{y^{\prime}} \langle y^{\prime},\theta\rangle-\big{(}\Omega(y^{\prime})+D_{\Omega}(y,y^{ \prime})\big{)},\forall y^{\prime}\in\mathbb{R}^{k}\).

The minus sign transforms the inf-addition into the sup-addition when distributed. If we had chosen the sup-addition in the definition of the Bregman divergence \(D_{\Omega}\), we would here calculate a supremum of an inf-addition, thus entering unknown territory.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: As stated in the abstract and in the introduction, we have defined Fitzpatrick losses in Definition 1 and studied their properties in Proposition 1. We instantiated the Fitzpatrick sparseMAP and the Fitzpatrick logistic loss in Proposition 5 and Proposition 6; we have studied the properties of Fitzpatrick losses in Proposition 1 and Proposition 7; we have gathered the results of the label proportion estimation in Table 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the computational limitations of the Fitzpatrick logistic loss. As stated in Proposition 6 and in Section 4, the computation of the loss value and gradient involves solving a root equation. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All results in the paper are proved in the appendix. We strived to make the proofs self-contained. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In Section 4, we describe the training setup, the linear model we use for predictions and give a link to access the datasets we use. We also indicate that we use the L-BFGS algorithm for training. Furthermore, Proposition 5 and Proposition 6 yield formulae for the gradients of the new losses we introduce. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provided a link for the used datasets in the experiments. The code will be opened upon release. For now, we provide instructions to reproduce the main experimental results. These instructions involved basic supervised learning tools such as L-BFGS algorithm, linear prediction model and standard cross-validation for the hyperparameter. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The selection of hyperparameter are discussed in section 4. We use predetermined train-test splits that come with the datasets. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]Justification: The split between training sets and test sets are fixed in advance so there is no variability when conducting the label proportion estimation tests. Furthermore, as we minimize convex losses, the convergence of the minimization algorithm is independent of the starting point. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The characteristics of the computer we used to conduct the tests. On the largest dataset, our experiment only takes a couple of hours to run. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted does not involve human subjects. We did not create new datasets and used standard open access datasets for multiclassification. After reviewing the "societal impact and potential harmful consequences" from the Code of Ethics, we conclude that the research conduct in the paper does not pose a risk of harmful consequences. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The research conducted in this paper is theoretical and has no societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The datasets used in the tests are publicly available. No new prediction model has been released in this paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **License for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]Justification: In Section 4, the libraries used for the implementation of the numerical experiments are credited and cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.