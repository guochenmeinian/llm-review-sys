# Parallel Submodular Function Minimization

 Deeparnab Chakrabarty

Dartmouth College

Hanover, USA

deeparnab@dartmouth.edu

&Andrei Graur

Stanford University

Stanford, USA

agraur@stanford.edu

&Haotian Jiang

Microsoft Research

Redmond, USA

jhtdavid96@gmail.com

Supported in part by NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, and Stanford Management Science & Engineering Department Nakagawa Fellowship

Supported in part by a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, a PayPal research award, and a Sloan Research Fellowship.

Aaron Sidford

Stanford University

Stanford, USA

sidford@stanford.edu

Supported in part by NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, and Stanford Management Science & Engineering Department Nakagawa Fellowship

Supported in part by a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, a PayPal research award, and a Sloan Research Fellowship.

###### Abstract

We consider the parallel complexity of submodular function minimization (SFM). We provide a pair of methods which obtain two new query versus depth tradeoffs a submodular function defined on subsets of \(n\) elements that has integer values between \(-M\) and \(M\). The first method has depth \(2\) and query complexity \(n^{O(M)}\) and the second method has depth \(\widetilde{O}(n^{1/3}M^{2/3})\) and query complexity \(O(\operatorname{poly}(n,M))\). Despite a line of work on improved parallel lower bounds for SFM, prior to our work the only known algorithms for parallel SFM either followed from more general methods for sequential SFM or highly-parallel minimization of convex \(\ell_{2}\)-Lipschitz functions. Interestingly, to obtain our second result we provide the first highly-parallel algorithm for minimizing \(\ell_{\infty}\)-Lipschitz function over the hypercube which obtains near-optimal depth for obtaining constant accuracy.

## 1 Introduction

A function \(f:2^{[n]}\to\mathbb{Z}\) is _submodular_ if it has the diminishing marginal return property that \(f(S\cup\{i\})-f(S)\geq f(T\cup\{i\})-f(T)\) for all elements \(i\in[n]\) and subsets \(S\subseteq T\subseteq[n]\setminus\{i\}\), i.e., the function value increase of adding \(i\) to a set \(S\) is at least that of adding \(i\) to a superset \(T\) that does not contain \(i\). Submodular functions are used to model a range of problems arising machine learning [10, 14, 15], operations research [11, 12], and economics [10]. The _submodular function minimization (SFM) problem_ consists of finding the subset \(S\) with the smallest \(f(S)\), given evaluation oracle access to the submodular function \(f\), using as few queries as possible. SFM has numerous applications. For example, in natural language processing SFM has played a key role in speech analysis where [1, 2] modeled the task of optimally selecting terse, yet diverse, training data out of a large speech dataset as a SFM problem; in computer vision, the task of energy minimization was reduced to SFM [1, 14, 15, 16, 17].

Seminal work of [1] established that SFM can be solved with a polynomial number of queries and in polynomial time. In the decades since, there has been extensive research characterizing the query and computational complexity of SFM. When \(f\) is integer valued with values between\(-M\) and \(M\), the state-of-the-art query complexities are \(\widetilde{O}(nM^{2})^{4}\) due to [10] and [1], \(\widetilde{O}(n^{2}\log M)\) and \(\widetilde{O}(n^{3})\), both due to [10] (the latter query complexity being improved by a factor of \(\log^{2}n\) in [11]), and \(O(n^{2}\log n)\) (with an exponential runtime) due to [11]. The largest query complexity lower bound is \(\Omega(n\log n)\) for deterministic algorithms due to [12].

The algorithms underlying the above results are highly _sequential_. State-of-the-art SFM algorithms use at least a linear number of rounds of queries to the evaluation oracle (\(\widetilde{O}(nM^{2})\) rounds in [1] and \(O(n\log n)\) rounds in [1]). Although there is a trivial \(1\)-round algorithm that queries all \(2^{n}\) input of \(f\) in parallel, all known polynomial time SFM algorithms use \(\Omega(n)\) rounds.

With the prevalence of parallel computation in practice and applications of SFM to problems involving massive data sets, recently there has been an increasing number of works which study the _parallel_ complexity of SFM, i.e., the smallest number of rounds a SFM algorithm must take. Starting with [1], significant progress has been made in proving _lower bounds_ on the parallel complexity of SFM. It is now known that any SFM algorithm making polynomially many queries needs \(\widetilde{\Omega}(n^{1/3})\) rounds when \(M=\Theta(n)\)[13] and \(\widetilde{\Omega}(n)\) rounds when \(M=\Theta(n^{n})\)[12].

In contrast to the significant progress on lower bounds, less progress has been made on designing SFM algorithms with small parallel complexity. This is due in part to the established lower bounds; as noted above, if we desire _range-independent_ parallel complexity bounds, i.e., independent of the range \(M\), then it is impossible to obtain an \(O(n^{1-\varepsilon})\)-round SFM algorithm, for any constant \(\varepsilon>0\). The central question motivating this paper is what non-trivial parallel speedups for SFM are possible if we allow methods with a _range-dependent_ parallel complexity.

_Is it possible to obtain query-efficient \(o(n)\)-round \(M\)-dependent algorithms for SFM?_

In this paper we provide positive answers to the above question. Our first result is an algorithm that runs in \(\widetilde{O}(n^{1/3}M^{2/3})\) rounds with query complexity \(\widetilde{O}(n^{2}M^{2})\). To achieve this result, we first provide a generic reduction from parallel SFM to parallel convex optimization (a well studied problem discussed below). While naively this approach yields a sublinear bound of \(\widetilde{O}(n^{2/3}M^{2/3})\) rounds, we show how to further improve these convex optimization methods in our setting.

Our second result is a simple \(2\)-round SFM algorithm (Algorithm 1) with query complexity \(n^{O(M)}\). For constant \(M\), the parallel complexity of \(2\) is optimal among the class of query-efficient algorithms that query the minimizer [10]. It is instructive to contrast our second result to the lower bound in [13], where it is proved that when \(M=n\), any algorithm with query complexity \(n^{M^{1-\delta}}\) for any constant \(\delta>0\) must proceed in \(n^{\Omega(\delta)}\) rounds.

**Highly Parallel Convex Optimization.** Motivated by applications to distributed and large scale optimization [1, 1], the question of highly parallel convex optimization has received significant attention in the last decade. Formally, the task is to find an (approximate) minimizer of a convex function this is Lipschitz in some norm, using as few rounds of \(O(\mathsf{poly}(n))\) parallel queries to a subgradient oracle as possible. Over the past few years, there has been progress on both upper [1, 1, 2] and lower bounds [14, 1, 2] for this problem.

This line of work is particularly relevant as SFM reduces to minimizing the _Lovasz extension_ (see Fact 2.4) [15] over \([0,1]^{n}\), which is convex and \(O(M)\)-Lipschitz in \(\ell_{\infty}\) (see Definition 2.1). In Section 2.1 we provide a straightforward reduction from this problem to unconstrained minimization of a \(O(L)\)-Lipschitz function in \(\ell_{\infty}\) where the minimizer has \(\ell_{\infty}\) norm \(O(1)\). Consequently, improved parallel \(\ell_{\infty}\)-Lipschitz convex optimization algorithms can imply improved parallel SFM algorithms.

Many prior algorithms for highly parallel convex optimization focused on convex functions that are \(\ell_{2}\)-Lipschitz and some were written only for unconstrained optimization problems. Naively using these algorithms for \(\ell_{\infty}\) yields parallel complexities of \(\widetilde{O}(n^{3/4}/\varepsilon)\)[1] and \(\widetilde{O}(n^{2/3}/\varepsilon^{2/3})\)[1, 1, 1] for convex functions that are \(\ell_{\infty}\)-Lipschitz.

Our first SFM result follows from an improved algorithm that we develop that finds \(\varepsilon\)-approximate minimizers for convex \(\ell_{\infty}\)-Lipschitz functions in \(\widetilde{O}(n^{1/3}/\varepsilon^{2/3})\) rounds. Interestingly, for constant \(\varepsilon>0\), the dependence on \(n\) in this improved parallel complexity is optimal up to logarithmic factors; [15] proved a lower bound of \(\widetilde{\Omega}(n^{1/3}\ln(1/\varepsilon))\) on the round complexity of any algorithm obtaining an \(\varepsilon\)-approximate minimizer over \([0,1]^{n}\) for \(\ell_{\infty}\)-Lipschitz convex functions. For constant \(\varepsilon>0\), this lower bound also applies to unconstrained \(\ell_{\infty}\)-optimization [11].

**Notation.** We let \([n]:=\{1,\ldots,n\}\) for any \(n\in\mathbb{Z}_{>0}\). We let \(\mathbf{I}_{n}\) denote the identity matrix in \(\mathbb{R}^{n\times n}\).

### Problems, Results, and Approach

Here we formally define the problems we consider, present our results, and discuss some of the key insights of our approach. This section is organized as follows. We begin by presenting the parallel computation model. Then, we introduce the parallel \(\ell_{p}\)-Lipschitz convex optimization problem, which is closely tied to parallel SFM. We present our improvement for the \(\ell_{\infty}\)-Lipschitz setting and offer a brief overview of our techniques in obtaining this result. Finally, we formally introduce the SFM setup, along with the new results we obtain (Theorem 1.3 and Theorem 1.4).

**Parallel Complexity Model.** We consider the standard black-box query model for optimization, where we are given a convex function \(f:\mathcal{D}\to\mathbb{R}\), with domain \(\mathcal{D}\subset\mathbb{R}^{n}\), accessed through an oracle. Parallel algorithms for minimizing \(f\) proceed in rounds, where in each round the algorithm can submit a set of queries to the oracle in parallel. The _parallel complexity_ of an algorithm is the total number of rounds it uses and it captures how "sequential" the algorithm is. Additionally, we consider the _query complexity_ of an algorithm which is the total number of queries it makes.

**Parallel \(\ell_{p}\)-Lipschitz Convex Optimization.** In Section 2 we consider the problem of minimizing a convex function \(f:\mathbb{R}^{n}\to\mathbb{R}\) given access to a _subgradient oracle_\(g:\mathbb{R}^{n}\to\mathbb{R}^{n}\), which when queried with point \(x\in\mathbb{R}^{n}\) returns a vector \(g(x)\) that is a subgradient, denote \(g(x)\in\partial(f(x))\) where \(\partial(f(x))\) is the set of all subgradients of \(f\) at \(x\), i.e., \(v\in\partial f(x)\) if and only if \(f(y)\geq f(x)+v^{\top}(y-x)\) for all \(y\in\mathbb{R}^{n}\). Furthermore, we assume that \(f\) is \(L\)-Lipschitz with respect to a norm \(\left\lVert\cdot\right\rVert\).

**Definition 1.1** (\(L\)-Lipschitzness with respect to a given norm).: _We say that \(f:\mathbb{R}^{n}\to\mathbb{R}\) is \(L\)-Lipschitz with respect to \(\left\lVert\cdot\right\rVert\) for norm \(\left\lVert\cdot\right\rVert\) if \(\left\lvert f(x)-f(y)\right\rvert\leq L\left\lVert x-y\right\rVert\) for all \(x,y\in\mathbb{R}^{n}\). If \(f\) is \(O(1)\)-Lipschitz with respect to a norm \(\left\lVert\cdot\right\rVert\), we say that \(f\) is \(\left\lVert\cdot\right\rVert\)-Lipschitz. When the norm is \(\left\lVert\cdot\right\rVert_{p}\) we alternatively say that \(f\) is \(L\)-Lipschitz in \(\ell_{p}\) and that \(f\) is \(\ell_{p}\)-Lipschitz respectively._

There is a broad line of work on studying the parallel complexity of \(\ell_{p}\)_-Lipschitz convex optimization_ in which the goal is to efficiently compute an \(\varepsilon\)-approximate minimizer (i.e., a point \(y\) with \(f(y)\leq\inf_{x}f(x)\)) of a \(\ell_{p}\)-Lipschitz convex function where the minimizer either has \(\ell_{p}\)-norm \(O(1)\) or the problem is constrained to the \(\ell_{p}\)-norm ball of of radius \(O(1)\). The case when \(p=2\) is perhaps the most well studied and our new result regarding \(\ell_{\infty}\)-Lipschitz convex optimization builds upon a result in [11], Theorem 1, which considers this setting. The statement of our result is below:

**Theorem 1.2** (Parallel Convex Optimization in \(\ell_{\infty}\)).: _There is an algorithm that when given a subgradient oracle for convex \(f:\mathbb{R}^{n}\to\mathbb{R}\) that is \(1\)-Lipschitz in \(\ell_{\infty}\) and has a minimizer \(x_{\star}\) with \(\left\lVert x_{\star}\right\rVert_{\infty}\leq 1\) computes an \(\varepsilon\)-approximate minimizer of \(f\) in \(\widetilde{O}(n^{1/3}\varepsilon^{-2/3})\) rounds and \(\widetilde{O}(n\varepsilon^{-2})\) queries._

In fact, we obtain a more general result that solves stochastic variants of parallel \(\ell_{\infty}\)-optimization (see Theorem 2.2) and Theorem 1.2 is an important corollary of this more general result.

Our parallel convex optimization algorithms build on machinery developed for highly-parallel algorithms for minimizing convex functions that are \(\ell_{2}\)-Lipschitz. These methods consider a convolution of \(f\) with a centered Gaussian with covariance \(\rho^{2}\mathbf{I}_{n}\) (also referred to as _Gaussian smoothing_), and then apply optimization methods [11, 12, 13] to this smooth function. By leveraging

\begin{table}
\begin{tabular}{|c|c|c|} \hline
**Paper** & **Year** & **Parallel Rounds** \\ \hline
[1] & 2020 & \(\widetilde{O}(nM^{2})\) \\ \hline
[1] & 2020 & \(\widetilde{O}(n\log M)\) \\ \hline
[10] & 2021 & \(\widetilde{O}(n)\) \\ \hline
**This paper** & 2023 & \(\widetilde{O}(n^{1/3}M^{2/3})\) \\ \hline \end{tabular} 
\begin{tabular}{|c|c|c|} \hline
**Paper** & **Year** & **Parallel Rounds** \\ \hline Subgradient Descent & 1960s & \(\widetilde{O}(n/\varepsilon^{2})\) \\ \hline Cutting Plane & 1965 & \(\widetilde{O}(n\log(1/\varepsilon))\) \\ \hline
[1] & 2019 & \(\widetilde{O}(n^{2/3}/\varepsilon^{2/3})\) \\ \hline
**This paper** & 2023 & \(\widetilde{O}(n^{1/3}/\varepsilon^{2/3})\) \\ \hline \end{tabular}
\end{table}
Table 1: State-of-the-art parallel complexity for SFM and \(\ell_{\infty}\)-optimization. See Section 1.2 for references on cutting plane methods. *The \(\widetilde{O}(n^{2/3}/\varepsilon^{2/3})\) result uses Lemma 2.3 from this paper.

the properties of this smoothing and the convergence rate of the associated optimization methods, they obtain their parallel complexities. Since functions which are \(\ell_{\infty}\)-Lipschitz are also \(\ell_{2}\)-Lipschitz, the above algorithms also apply to \(\ell_{\infty}\)-Lipschitz functions but they give us a suboptimal dependence of \(n^{2/3}\) on the dimension. We improve the dependence on dimension by utilizing the \(\ell_{\infty}\)-Lipschitzness of our function to add more Gaussian smoothing. This allows us to obtain a \(n^{1/3}\) dependence on the dimension, which is optimal up to polylogarithmic factors for constant \(\varepsilon\)[15, 10]. The key observation is that convolving an \(\ell_{\infty}\)-Lipschitz function with a Gaussian of covariance \(\rho^{2}\mathbf{I}_{n}\) changes the function value by no more than \(O(\rho\sqrt{\log n})\) (see Lemma 2.6), whereas for \(\ell_{2}\)-Lipschitz functions it could change the function value by \(O(\rho\sqrt{n})\).

**Submodular Function Minimization.** In SFM, we assume the submodular function \(f:2^{[n]}\to\mathbb{Z}\) is given by an _evaluation oracle_, which when queried with \(S\subseteq[n]\), returns the value of \(f(S)\). Throughout the paper, we assume that \(f(S)\in[-M,M]\) for some \(M\in\mathbb{Z}_{>0}\), and \(f(\emptyset)=0\). The assumption that \(f(\emptyset)=0\) can be made without loss of generality by instead minimizing \(\hat{f}(S):=f(S)-f(\emptyset)\); this transformation can be implemented with one query and moves the range of \(f\) by at most \(\pm M\), turning any dependence on \(M\) in an algorithms' complexity to \(2M\).

Note that this submodular function minimization setup is different from the setup of parallel convex optimization, as \(f\) only defined on the vertices of the unit hypercube. Nonetheless, it is known that there is a convex function \(f_{\mathsf{Lov}}\) defined on \([0,1]^{n}\), known as the Lovasz Extension, such that optimizing \(f_{\mathsf{Lov}}\) suffices for optimizing \(f\). Additionally, it is known how to compute a subgradient of \(f_{\mathsf{Lov}}\) at any point \(x\) in 1 round using at most \(n\) evaluation queries to \(f\) (as highlighted in Fact 2.4).

Now, we are ready to present our first result on parallel SFM. Later, in Section 2.1, we provide a more general version of this theorem, Theorem 2.5, which gives improved parallel complexities for approximately minimizing bounded, real-valued submodular functions.

**Theorem 1.3** (Sublinear Parallel SFM).: _There is an algorithm that, when given an evaluation oracle for submodular \(f:2^{[n]}\to\mathbb{Z}\) with \(f(\emptyset)=0\) and \(|f(S)|\leq M\) for all \(S\subseteq[n]\), finds a minimizer of \(f\) in \(\widetilde{O}(n^{1/3}M^{2/3})\) rounds and \(\widetilde{O}(n^{2}M^{2})\) queries._

As discussed, Theorem 1.3, is obtained by using and enhancing tools for optimizing Lipschitz convex functions with a subgradient oracle. We in fact prove a more general result, namely that \(\widetilde{O}(n^{1/3}/\varepsilon^{2/3})\) rounds and \(\widetilde{O}(n^{2}/\varepsilon^{2})\) queries are sufficient to find an \(\varepsilon M\)-approximate minimizer (Theorem 2.5). Since the function is integer valued, approximating the scaled Lovasz extension to \(\varepsilon\approx\Theta(1/M)\) gives the exact minimizer to the submodular function. Our proof of Theorem 2.5 follows from our new result on \(\ell_{\infty}\)-convex optimization (Theorem 1.2). By applying it to a scaled version of the Lovasz extension; it is known that if \(f\) is a submodular function with range \(\mathbb{Z}\cap[-M,+M]\), then the Lovasz extension scaled by \(O(1/M)\) is a convex function which is \(\ell_{\infty}\)-Lipschitz. However, it is important to note that SFM is only equivalent to _constrained_ minimization of the Lovasz extension in \([0,1]^{n}\), while Theorem 1.2 below is _unconstrained_ (e.g. applies for minimizing over \(\mathbb{R}^{n}\)). To apply Theorem 1.2 in the context of SFM, we give a general reduction from constrained to unconstrained optimization by adding a regularizer that restricts the minimizer of the regularized function to the constrainted set (see Lemma 2.3). This reduction is a generic technique and might be of independent utility.

**Parallel SFM in Two Rounds.** Our second SFM result is a simple combinatorial \(2\)-round algorithm which is efficient for functions of constant range.

**Theorem 1.4** (Two-round Parallel SFM).: _There is an algorithm (Algorithm 1) that when given an evaluation oracle for submodular \(f:2^{[n]}\to\mathbb{Z}\) with \(f(\emptyset)=0\) and \(|f(S)|\leq M\) for all \(S\subseteq[n]\) finds a minimizer of \(f\) in 2 rounds and \(O(n^{M+1})\) queries._

The algorithm proving Theorem 1.4 relies on two key observations. First, if \(S_{*}\) is the minimizer of maximum size, for every subset \(T\subseteq S_{*}\) and \(i\in[n]\) with \(f(T\cup\{i\})\leq f(T)\), we have \(i\in S_{*}\). In other words, every element with a non-positive marginal at a subset \(T\subseteq S^{*}\) is also contained in \(S_{*}\). This leads to the idea of _augmenting_ a set \(T\) by the set \(T^{\prime}=T\cup\{i:f(T\cup\{i\})\leq f(T)\}\). Secondly, every subgradient \(g\in\partial f(x)\) has at most \(M\) entries that are strictly positive. This ensures that there exists an \(M\)-sparse subset \(T\subseteq S_{*}\) with the property that \(f(T\cup\{i\})\leq f(T)\) forall \(i\in S_{*}\backslash T\). Consequently, our algorithm proceeds by augmenting all \(M\)-sparse sets, as it is guaranteed that one of these augmented sets is the maximal minimizer (see Section 3 for more details).

### Related Work

**Parallel Convex Optimization.** As mentioned in Section 1.1, there are a number of works studying the parallel complexity of \(\ell_{p}\)_-Lipschitz convex optimization_. Perhaps, the most well-studied case is that of \(p=2\). In this case, the classic subgradient descent algorithm achieves a parallel complexity of \(O(\varepsilon^{-2})\)-rounds and the standard cutting-plane methods achieve a parallel complexity of \(O(n\log(1/\varepsilon))\)-rounds. [1, 19, 23] improved upon this rate, achieving parallel complexities of \(\widetilde{O}(n^{1/4}\varepsilon^{-1})\)[10] and \(\widetilde{O}(n^{1/3}\epsilon^{-2/3})\)[19, 23] respectively. The implications of these results for the \(p=\infty\) case, which is the object of study in our paper, were discussed earlier and we are unaware of works on alternative upper bounds for \(p=\infty\).

In terms of lower bounds, the \(p=\infty\) case was studied in the prescient paper of [14] which obtains a \(\widetilde{\Omega}(n^{1/3}\ln(1/\varepsilon))\) lower bound for minimizing \(\ell_{\infty}\)-Lipschitz functions over the \(\ell_{\infty}\)-ball (see also [1]). When \(\varepsilon\) is a constant, our upper bound matches this lower bound, though our dependence on \(\varepsilon\) is polynomial instead of logarithmic. For the \(p=2\) case, [14, 1] proved a tight lower bound of \(\Omega(1/\varepsilon^{2})\) on the parallel complexity when \(1/\varepsilon^{2}\leq\widetilde{O}(n^{1/3})\), i.e., the parallel complexity of subgradient descent is optimal up to \(\widetilde{O}(n^{1/3})\) rounds of queries. This was later improved by [19], which showed that subgradient descent is optimal up to \(\widetilde{O}(n^{1/2})\) rounds. [1] considered the general \(p\) case (and other non-Euclidean settings) and proved a lower bound of \(\Omega(\varepsilon^{-p})\) lower bound on the parallel complexity for \(2\leq p<\infty\), \(\Omega(\varepsilon^{-2})\) for \(1<p\leq 2\), and \(\Omega(\varepsilon^{-2/3})\) for \(p=1\). This paper also has lower bounds for smooth convex functions.

**Submodular Function Minimization.** We now expand on the history of SFM upper and lower bounds for parallel and sequential algorithms touched upon earlier. Since the seminal work of Edmonds in 1970 [10], there has been extensive work [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22] on developing query-efficient algorithms for SFM. [11, 12] gave the first polynomial time algorithms using the ellipsoid method. The state-of-the-art SFM algorithms include a \(\widetilde{O}(n^{2})\)-query exponential time algorithm due to [22], \(\widetilde{O}(n^{3})\)-query polynomial time algorithms due to [12]; a \(\widetilde{O}(n^{2}\log M)\)-query polynomial time algorithm due to [1], and a \(\widetilde{O}(nM^{2})\)-query polynomial time algorithm due to [1]. Despite these algorithmic improvements, limited progress has been made on lower bounding the query complexity of SFM and the best known lower bound has been \(\Omega(n)\) for decades [1, 13, 15]. Very recently, [10] proved an \(\Omega(n\log n)\)-lower bound for deterministic SFM algorithms.

All the algorithms above are highly sequential and proceed in at least \(n\) rounds. The question of parallel complexity for SFM was first studied in [1] where an \(\Omega(\log n/\log\log n)\)-lower bound on the number of rounds required by any query-efficient SFM algorithm was given. The range \(M\) in their construction is \(M=n^{\Theta(n)}\). Subsequently, [14] proved a \(\widetilde{\Omega}(n^{1/3})\) lower bound on the round-complexity and the range is \(M=n\) for their functions. Recently, [10] described a \(\widetilde{\Omega}(n)\) lower bound with functions of range \(M=n^{\Theta(n)}\).

**Cutting plane methods.** Cutting plane methods are a class of optimization methods that minimize a convex function by iteratively refining a convex set containing the minimizer. Since the center of gravity method was developed independently in [14, 15], there have been many developments of faster cutting plane methods over the last six decades [16, 17, 18, 19, 20, 21], with the state-of-the-art due to [19].

## 2 Minimizing \(\ell_{\infty}\)-Lipschitz Functions and Submodular Functions

In this section we provide a new, improved parallel algorithm for convex optimization in \(\ell_{\infty}\) and show how to use these algorithms to obtain an improved parallel algorithm for SFM. In much of this section, we consider the following optimization problem which we term _stochastic convex optimization in \(\ell_{\infty}\)_. As we discussed, this problem generalizes parallel convex optimization in \(\ell_{\infty}\). The problem is more general in terms of the norms it considers and how it allows for stochastic gradients; we consider it as it could be useful more broadly and as it perhaps more tightly captures the performance of our optimization algorithm.

**Definition 2.1** (Stochastic Convex Optimization in \(\ell_{\infty}\)).: _In the stochastic convex optimization in \(\ell_{\infty}\) problem we have a (stochastic) subgradient oracle \(g:\mathbb{R}^{n}\to\mathbb{R}^{n}\) such that \(\mathbb{E}g(x)\in\partial f(x)\) and \(\mathbb{E}\left\|g(x)\right\|_{2}^{2}\leq\sigma^{2}\) for a convex function \(f:\mathbb{R}^{n}\to\mathbb{R}\) that is \(L\)-Lipschitz in \(\ell_{\infty}\). Given the guarantee that \(f\) has a minimizer \(x_{*}\in\mathbb{R}^{n}\) with \(\left\|x_{*}\right\|_{2}\leq R\) our goal is to compute an \(\varepsilon\)-approximate minimizer of \(f\), i.e., \(x\in\mathbb{R}^{n}\) with \(f(x)\leq f(x_{*})+\varepsilon\)._

Note that in Definition 2.1, \(\ell_{\infty}\) appears only to determine the norm in which \(f\) is Lipschitz. However, the bound on \(x_{*}\) in \(\ell_{2}\) that can be easily converted to one in terms of \(\ell_{\infty}\) by using that \(\left\|x_{*}\right\|_{2}\leq\sqrt{n}\left\|x_{*}\right\|_{\infty}\). Furthermore, a convex function \(f:\mathbb{R}^{n}\to\mathbb{R}\) is \(L\)-Lipschitz in \(\ell_{\infty}\) if and only if \(\left\|g\right\|_{1}\leq L\) for all \(g\in\partial(x)\) for \(x\in\mathbb{R}^{n}\). Since \(\left\|g\right\|_{2}\leq\left\|g\right\|_{1}\) we see that this stochastic convex optimization problem subsumes the (non-stochastic) problem of computing an \(\varepsilon\)-approximate minimizer to a convex function that is \(L\)-Lipschitz in \(\ell_{\infty}\) given a (deterministic) subgradient oracle. We define this more general problem as, interestingly, our algorithm tolerates this weaker stochastic oracle without any loss (as we discussed).

Our main result regarding stochastic convex optimization in \(\ell_{\infty}\) is given in the following theorem.

**Theorem 2.2** (Stochastic Convex Optimization in \(\ell_{\infty}\)).: _There is an algorithm that solves the stochastic convex optimization problem (Definition 2.1) in \(\ell_{\infty}\) (Definition 2.1) in \(\widetilde{O}((LR/\varepsilon)^{2/3})\) rounds and \(\widetilde{O}((\sigma R/\varepsilon)^{2})\) queries._

Due to the aforementioned connection between \(\ell_{\infty}\)-Lipschitz continuity and bounds on the subgradient, and the fact that \(\left\|x_{*}\right\|_{\infty}\leq 1\) implies \(\left\|x_{*}\right\|_{2}\leq\sqrt{n}\), Theorem 2.2 immediately yields a \(\widetilde{O}(n^{1/3}\varepsilon^{-2/3})\)-round, \(\widetilde{O}(n\varepsilon^{-2})\)-query algorithm for the problem of minimizing a convex function that is \(1\)-Lipschitz in \(\ell_{\infty}\) and minimized at a point with \(\ell_{\infty}\)-norm at most 1. As discussed in the introduction, the parallel complexity of this algorithm is near-optimal for constant \(\varepsilon\)[12].

**Theorem 1.2** (Parallel Convex Optimization in \(\ell_{\infty}\)).: _There is an algorithm that when given a subgradient oracle for convex \(f:\mathbb{R}^{n}\to\mathbb{R}\) that is \(1\)-Lipschitz in \(\ell_{\infty}\) and has a minimizer \(x_{*}\) with \(\left\|x_{*}\right\|_{\infty}\leq 1\) computes an \(\varepsilon\)-approximate minimizer of \(f\) in \(\widetilde{O}(n^{1/3}\varepsilon^{-2/3})\) rounds and \(\widetilde{O}(n\varepsilon^{-2})\) queries._

In Section 2.1, we show how to use Theorem 2.2 to obtain our results for SFM. We then present the ingredients in the proof of Theorem 2.2 (which we defer to Appendix A) in Section 2.2. As part of our reduction from SFM to Stochastic Convex Optimization in \(\ell_{\infty}\) in Theorem 2.2, we provide a general tool for reducing constrained to unconstrained minimization (Lemma 2.3); we use this lemma to facilitate our results in both sections.

### From Unconstrained Convex Optimization in \(\ell_{\infty}\) to SFM

Here we show how to use Theorem 1.2 to prove the following theorem regarding SFM.

**Theorem 1.3** (Sublinear Parallel SFM).: _There is an algorithm that, when given an evaluation oracle for submodular \(f:2^{[n]}\to\mathbb{Z}\) with \(f(\emptyset)=0\) and \(\left|f(S)\right|\leq M\) for all \(S\subseteq[n]\), finds a minimizer of \(f\) in \(\widetilde{O}(n^{1/3}M^{2/3})\) rounds and \(\widetilde{O}(n^{2}M^{2})\) queries._

A key ingredient of our proof is the following general, simple technical tool which allows one to reduce constrained Lipschitz optimization over a ball in any norm to unconstrained minimization with only a very mild increase in parameters.

**Lemma 2.3**.: _Let \(f:\mathbb{R}^{n}\to\mathbb{R}\) be convex and \(L\)-Lipschitz with respect to norm \(\left\|\cdot\right\|:\mathbb{R}^{n}\to\mathbb{R}\). For any \(c,x\in\mathbb{R}^{n}\) and \(r\in\mathbb{R}\) let_

\[f^{c,r}_{\mathsf{reg}}(x):=f(x)+2L\cdot\max\{0,\left\|x-c\right\|-r\}\,.\] (1)

_Then \(f^{c,r}_{\mathsf{reg}}(x)\) is convex and \(3L\)-Lipschitz with respect to \(\left\|\cdot\right\|\). Additionally, for any \(y\in\mathbb{R}^{n}\) for which \(\left\|y-c\right\|\geq r\), define \(y^{c,r}:=c+\frac{r}{\left\|y-c\right\|}(y-c)\). Then,_

\[\left\|y^{c,r}-c\right\|=r\text{ and }f^{c,r}_{\mathsf{reg}}(y^{c,r})=f(y^{c,r}) \leq f^{c,r}_{\mathsf{reg}}(y)-L(\left\|y-c\right\|-r)\,.\] (2)

_Consequently, \(f^{c,r}_{\mathsf{reg}}(x)\) has an unconstrained minimizer \(x^{c,r}_{*}\) and all such minimizers satisfy_

\[\left\|x^{c,r}_{*}-c\right\|\leq r\text{ and }f^{c,r}_{\mathsf{reg}}(x^{c,r}_{*})=f(x^ {c,r}_{*})=\min_{x\in\mathbb{R}^{n}\left\|x-c\right\|\leq r}f(x)\,.\] (3)Lemma 2.3 implies that minimizing \(f\) subject to a distance constraint \(\|x-c\|\leq r\) reduces to unconstrained minimization of \(f_{\mathsf{reg}}^{c,r}\). More formally, to compute an \(\epsilon\)-optimal solution to the constrained minimization problem, \(\min_{x\in\mathbb{R}^{n}:\ \|x-c\|\leq r}f(x)\), it suffices to instead compute an \(\varepsilon\)-optimal solution, \(x_{\varepsilon}\), to the unconstrained minimization problem \(\min_{x}f_{\mathsf{reg}}^{c,r}(x)\), and then output that point \(x_{\varepsilon}\) if \(\|x_{\varepsilon}-c\|\leq r\) and output \(x_{\varepsilon}^{r,c}\)-otherwise. From (2), we get \(f(x_{\varepsilon}^{r,c})\leq f_{\mathsf{reg}}^{c,r}(x_{\varepsilon})\leq\mathsf{ opt}+\varepsilon\), where \(\mathsf{opt}:=\min_{x}f_{\mathsf{reg}}^{c,r}(x)=\min_{x\in\mathbb{R}^{n}\|x-c\| \leq r}f(x)\) and the last equality follows from (3).

We remark that the \(2L\) in the definition of \(f_{\mathsf{reg}}^{c,r}\) can be changed to \(L+\delta\) for any \(\delta\geq 0\) with the only effect of turning the \(L\) in (2) to \(\delta\) and causing (3) to only hold for some minimizer (rather than all) if \(\delta=0\). The proof of Lemma 2.3 is deferred to Appendix A.

Next, we obtain Theorem 1.3 by applying Theorem 2.2 to the Lovasz extension of the submodular function \(f\) extended to an unconstrained minimization problem by Lemma 2.3.

Given a submodular function \(f\) defined over subsets of an \(n\) element universe, the Lovasz extension \(f_{\mathsf{Low}}:\mathbb{R}^{n}\to\mathbb{R}\) is defined as follows: \(f_{\mathsf{Low}}(x):=\sum_{i\in[n]}x_{\pi_{x}(i)}(f(S_{\pi_{x},i})-f(S_{\pi_{ x},i-1}))\), where \(\pi_{x}:[n]\to[n]\) is the permutation such that \(x_{\pi_{x}(1)}\geq x_{\pi_{x}(2)}\geq\cdots\geq x_{\pi_{x}(n)}\) (ties broken in an arbitrary but consistent manner), and \(S_{\pi_{x},j}\) is the subset \(\{\pi_{x}(1),\ldots,\pi_{x}(j)\}\).

Next we give standard properties of the Lovasz extension and use them to prove Theorem 1.3.

**Fact 2.4** (e.g., [10, 11]).: _The following are true about the Lovasz extension \(f_{\mathsf{Low}}\):_

1. \(f_{\mathsf{Low}}\) _is convex with_ \(\min_{x\in[0,1]^{n}}f_{\mathsf{Low}}(x)=\min_{S\subseteq V}f(S)\)_. Indeed, given any_ \(x\in[0,1]^{n}\)_, in_ \(n\) _queries one can find a subset_ \(S\) _with_ \(f(S)\leq f_{\mathsf{Low}}(x)\)_._
2. _Given any_ \(x\in\mathbb{R}^{n}\) _and corresponding permutation_ \(\pi_{x}\)_, the vector_ \(g\in\mathbb{R}^{n}\) _where_ \(g(x)_{(\pi(i))}:=f(S_{\pi_{x},i})-f(S_{\pi_{x},i-1})\) _is a subgradient of_ \(f_{\mathsf{Low}}\) _at_ \(x\)_. Furthermore,_ \(g(x)\) _can be computed in_ \(1\) _round of_ \(n\) _queries to an evaluation oracle for_ \(f\)_._
3. _If_ \(f\) _has range in_ \([-M,+M]\)_, then the_ \(\ell_{1}\)_-norm of the subgradient is bounded, in particular,_ \(\left\|g(x)\right\|_{1}\leq 3M\)_. Equivalently,_ \(f_{\mathsf{Low}}\) _is_ \(3M\)_-Lipschitz with respect to the_ \(\ell_{\infty}\)_-norm._

As discussed in Section 1.1, to prove Theorem 1.3, it suffices to prove the following more general result regarding approximately minimizing a submodular function.

**Theorem 2.5** (\(\epsilon\)-approximate minimizer for SFM).: _There is an algorithm that, when given an evaluation oracle for submodular \(f:2^{[n]}\to\mathbb{R}\) with minimizer \(x_{*}\), \(f(\emptyset)=0\) and \(|f(S)|\leq M,\forall S\subseteq[n]\), finds a set \(S\) with \(f(S)\leq f(x_{*})+\epsilon M\) in \(\widetilde{O}(n^{1/3}/\epsilon^{2/3})\) rounds and a total of \(\widetilde{O}(n^{2}/\epsilon^{2})\) queries._

Proof of Theorem 2.5.: By Fact 2.4, SFM reduces to minimizing \(f_{\mathsf{Low}}\) over \(x\in[0,1]^{n}\) which is the same set \(\{x\in\mathbb{R}^{n}:\left\|x-c\right\|_{\infty}\leq 0.5\}\) where \(c\) is the \(n\)-dimensional vector with all entries \(0.5\). By Lemma 2.3, we can do so by applying Theorem 2.2 to the regularized version \(f_{\mathsf{reg}}^{c,0.5}\) of \(f_{\mathsf{Low}}\) with respect to the \(\ell_{\infty}\)-norm. This regularized function is guaranteed to have a minimizer \(x_{*}\) with \(\left\|x_{*}\right\|_{2}\leq\sqrt{n}\) which also minimizes \(f_{\mathsf{Low}}\) in \([0,1]^{n}\). The subgradient of this regularized function at any point can be computed from the subgradient of \(f_{\mathsf{Low}}\) at the same point which takes \(n\) evaluation oracle queries to the submodular function. Hence, we obtain an \(\varepsilon M\)-approximate minimizer in \(\widetilde{O}(n^{1/3}/\varepsilon^{2/3})\) rounds and with a total of \(\widetilde{O}(n^{2}/\varepsilon^{2})\) queries to the evaluation oracle of \(f\). 

Now, we are ready to prove Theorem 1.3.

Proof of Theorem 1.3.: Set \(\epsilon=\frac{1}{2M}\) and apply Theorem 2.5 to obtain, in \(\widetilde{O}(n^{1/3}M^{2/3})\) rounds and with a total of \(\widetilde{O}(n^{2}M^{2})\) queries to the evaluation oracle of \(f\), a \(x\in[0,1]^{n}\) with \(f_{\mathsf{Low}}(x)\leq\min_{z\in[0,1]^{n}}f_{\mathsf{Low}}(z)+\frac{1}{2}\). Then by property 1 in Fact 2.4, one can get a subset \(A\subseteq V\) with \(f(A)\leq\min_{S\subseteq V}f(S)+\frac{1}{2}\). As \(f\) is assumed to be integer valued, \(A\) must be the minimizer of the submodular function. 

### Parallel Stochastic Convex Optimization in \(\ell_{\infty}\)

Here we present the key steps in proving Theorem 2.2 regarding our new parallel results for the stochastic convex optimization problem in \(\ell_{\infty}\) (Definition 2.1). Throughout this subsection, in our exposition, lemma statements, and proofs we assume that we are in the setting of Definition 2.1.

To prove Theorem 2.2 we apply the approach of [13] with two modifications. First, we consider the convolution of \(f\) with a centered Gaussian density function with covariance \(\rho^{2}\mathbf{I}_{n}\). However we show that in our setting, it is possible to use a larger value of \(\rho\) without perturbing the function value too much, due to the \(\ell_{\infty}\) geometry. Unfortunately, the minimizer of the convolved function may move outside the box of radius \(R\). Thus, the second modification we make is working with a regularized function, \(f_{\mathsf{reg}}\), which is pointwise close to \(f\), still \(L\)-Lipschitz in the \(\ell_{\infty}\) norm, and keeps the minimizer in the ball or radius \(R\) even after applying the convolution with a Gaussian.

In the remainder of this subsection we first present the ingredients going into the proof of Theorem 2.2, and then give a brief explanation for how they fit into the proof of Theorem 2.2, deferring the complete proof to the appendix. We start with our bound on function perturbation after adding a Gaussian with covariance \(\rho^{2}\mathbf{I}_{n}\) to an \(\ell_{\infty}\) Lipschitz function (Lemma 2.6). We then introduce the concept of a ball optimization oracle, along with a result on how to implement it in low depth for the special case of a function that is the result of Gaussian convolution (Proposition 2.8). Lastly, we present the result which allows us to use a ball optimization oracle black-box to obtain the desired depth (Proposition 2.9).

Now, we are ready to present the lemma which allows us to obtain a better dependence of depth on the dimension \(n\), compared to the naive \(n^{2/3}\) obtained by directly applying the \(\ell_{2}\)-Lipschitz optimization result. We start with the definition of Gaussian convolution.

Gaussian Convolution.Let \(\gamma_{\rho}:=(2\pi\rho)^{-n/2}\exp(-\frac{1}{2\rho^{2}}\|x\|_{2}^{2})\) be the probability density function of \(\mathcal{N}(0,\rho^{2}\mathbf{I}_{n})\). Given a function \(f:\mathbb{R}^{n}\to\mathbb{R}\), we define its convolution with a Gaussian of covariance \(\rho^{2}\mathbf{I}_{n}\) by \(\widehat{f}_{\rho}:=f*\gamma_{\rho}\), i.e.

\[\widehat{f}_{\rho}(x):=\mathbb{E}_{y\sim\mathcal{N}(0,\rho^{2}\mathbf{I}_{n}) }[f(x+y)]=\int_{\mathbb{R}^{n}}f(x-y)\gamma_{\rho}(y)\mathrm{d}y.\] (4)

Next, we present a lemma which allows us to obtain a better dependence on the dimension \(n\) in depth, compared to the naive \(n^{2/3}\) obtained by directly applying the \(\ell_{2}\)-Lipschitz optimization result. This lemma shows that we can perform more Gaussian smoothing (as compared to the \(\ell_{2}\)-setting) without perturbing the function too much (as mentioned in Section 1.1).

**Lemma 2.6** (Gaussian Convolution Distortion Bound for \(\ell_{\infty}\)).: _Let \(f:\mathbb{R}^{n}\to\mathbb{R}\) be \(L\)-Lipschitz with respect to the \(\ell_{\infty}\)-norm. Then for any point \(x\in\mathbb{R}^{n}\), we have \(\left|\widehat{f}_{\rho}(x)-f(x)\right|\leq L\rho\cdot\sqrt{2\log n}\)._

Proof.: Note that \(\left|\widehat{f}_{\rho}(x)-f(x)\right|\leq\int_{\mathbb{R}^{n}}\left|f(z)-f( x)\right|\gamma_{\rho}(x-z)\mathrm{d}z\leq L\int_{\mathbb{R}^{n}}\left\|x-z \right\|_{\infty}\gamma_{\rho}(x-z)\mathrm{d}z\) where the first inequality follows from the definition of \(\widehat{f}_{\rho}\) and the second follows as \(f\) is \(L\)-Lipschitz in \(\ell_{\infty}\)-norm. The RHS is simply the expected \(\ell_{\infty}\)-norm of a zero-mean random Gaussian vector with covariance \(\rho^{2}\mathbf{I}_{n}\), and this is \(\Theta(\rho\sqrt{\log n})\) (e.g., [18]). More precisely, we get

\[\left|\widehat{f}_{\rho}(x)-f(x)\right|\leq L\cdot\mathbb{E}_{y\sim\mathcal{N }(0,\rho^{2}\mathbf{I}_{n})}\left\|y\right\|_{\infty}\leq L\rho\cdot\sqrt{2 \log n}.\qed\]

As mentioned in Section 1.1, by contrast, convolving a function \(f\) that is \(L\)-Lipschitz in \(\ell_{2}\) with a Gaussian of covariance \(\rho^{2}\mathbf{I}_{n}\) could change the function value by \(O(\rho\sqrt{n})\). Hence, the \(\ell_{\infty}\) geometry allows us to add a larger amount of Gaussian smoothing without changing the function value by more than \(\varepsilon\), which in turn allows for better rates.

Ball Optimization.A subroutine that we use for minimizing \(\widehat{f}_{\rho}\) is called a _ball optimization oracle_. As suggested by [13], the concept of ball optimization oracle is related to the notion of trust regions, explored in several papers, such as [13]. The particular ball optimization procedure we employ takes a function \(F:\mathbb{R}^{n}\to\mathbb{R}\) and a point \(\bar{x}\in\mathbb{R}^{n}\), which is an approximate solution to \(F\) in a small ball of \(\bar{x}\). More formally, we work with the following definition:

**Definition 2.7** (Ball Optimization Oracle [13]).: _Let \(F:\mathbb{R}^{n}\to\mathbb{R}\) be a convex function. \(\mathcal{O}_{\mathsf{bo}}\) is an \((\phi,\lambda,r)\)-ball optimization oracle for \(F\) if given any \(\bar{x}\in\mathbb{R}^{n}\), it returns an \(x\in\mathbb{R}^{n}\) with the property_

\[\mathbb{E}\left[F(x)+\frac{\lambda}{2}\cdot\left\|x-\bar{x}\right\|_{2}^{2} \right]\ \leq\ F(x_{\mathsf{loc}}^{\star})+\frac{\lambda}{2}\left\|x_{\mathsf{ loc}}^{\star}-\bar{x}\right\|_{2}^{2}+\phi\,\]

_where \(x_{\mathsf{loc}}^{\star}=\arg\min_{x\in B_{\bar{x}}(r)}(F(x)+\frac{\lambda}{2} \left\|x-\bar{x}\right\|_{2}^{2})\)._From [3] it is known is that for any Lipschitz convex function \(f\), any stochastic subgradient oracle \(g\) as above, and any \(\rho\), if we set \(r=\rho\), then efficient ball-optimization oracles exist. More formally, we use the following proposition from [3] which is in turn inspired from [1].

**Proposition 2.8** (Proposition 3, [3]).: _Let \(f:\mathbb{R}^{n}\to\mathbb{R}\) be convex and \(g:\mathbb{R}^{n}\to\mathbb{R}^{n}\) be a stochastic subgradient oracle satisfying \(\mathbb{E}[g(x)]\in\partial f(x)\) and \(\mathbb{E}\left\|g(x)\right\|_{2}^{2}\leq\sigma^{2}\) for all \(x\in\mathbb{R}^{n}\). Let \(\widehat{f}_{\rho}:=f*\gamma_{\rho}\), i.e.,_

\[\widehat{f}_{\rho}(x):=\mathbb{E}_{y\sim\mathcal{N}(0,\rho^{2}\mathbf{I}_{n})} [f(x-y)]=\int_{\mathbb{R}^{n}}f(x-y)\gamma_{\rho}(y)\mathrm{d}y.\] (5)

_Then there is a \((\phi,\lambda,\rho)\)-ball optimization oracle for \(\widehat{f}_{\rho}\) which makes \(O(\frac{\sigma^{2}}{\phi\lambda})\) total queries to \(g\) in a constant number of rounds._

Highly Parallel Optimization.As shown in [3], the ball optimization oracle above can be used for highly-parallel optimization as follows.

**Proposition 2.9** (Proposition 2 in [3]).: _Fix a function \(F:\mathbb{R}^{n}\to\mathbb{R}\) which is \(L\)-Lipschitz with respect to the \(\ell_{2}\)-norm and convex. Suppose \(R\geq 0\) is a parameter such that \(x^{\star}\in\arg\min_{x}F(x)\) satisfies \(\left\|x^{\star}\right\|_{2}\leq R\). Let \(r\in(0,R]\) and \(\epsilon_{\text{opt}}\in(0,LR]\) be two parameters. Define the following quantities_

\[\kappa:=\frac{LR}{\epsilon_{\text{opt}}},\quad K:=\left(\frac{R}{r}\right)^{ \frac{2}{3}},\quad\text{and}\quad\lambda_{\star}:=\frac{\epsilon_{\text{opt} }K^{2}}{R^{2}}\log^{2}\kappa.\] (6)

_Then, there exists a universal constant \(C>0\) and an algorithm BallAccel which runs in \(CK\log\kappa\) iterations and produces a point \(x\) such that \(\mathbb{E}F(x)\leq F(x^{\star})+\epsilon_{\text{opt}}\). Moreover,_

1. _Each iteration makes at most_ \(C\log^{2}(\frac{R\kappa}{r})\) _calls to_ \((\frac{\lambda r^{2}}{C},\lambda,r)\)_-ball optimization oracle with values of_ \(\lambda\in[\frac{\lambda_{\star}}{C},\frac{CL}{\epsilon_{\text{opt}}}]\)_._
2. _For each_ \(j\in[\lceil\log_{2}K+C\rceil]\)_, at most_ \(C^{2}\cdot 2^{-j}K\log(\frac{R\kappa}{r})\) _iterations query a_ \((\frac{\lambda r^{2}}{C^{2j}}\cdot\log^{-2}(\frac{R\kappa}{r}),\lambda,r)\)_-ball optimization oracle for some_ \(\lambda\in[\frac{\lambda_{\star}}{C},\frac{CL}{\epsilon_{\text{opt}}}]\)_._

While the proof of Theorem 2.2 is deferred to Appendix A, we provide some intuition for how to use the stated components to obtain the result. A natural approach would be to apply Proposition 2.9 to \(F:=\widehat{f}_{\rho}\). However, to ensure that the minimizer of \(F\) has \(\ell_{2}\)-norm at most \(R\), we will instead work with \(F:=\widehat{f}_{\text{reg}_{\rho}}^{c,R}\) as defined in Lemma 2.3. The idea is to invoke Lemma 2.3 and apply BallAccel on the function \(F:=\widehat{f}_{\text{reg}_{\rho}}^{c,R}\) with respect to \(\left\|\cdot\right\|_{2}\), \(c\) being the origin, and \(\rho:=\frac{\epsilon_{\text{opt}}}{L\sqrt{2\log n}}\). With this choice of \(\rho\), by Lemma 2.6, we know \(|F(x)-f_{\text{reg}}^{c,R}(x)|\leq\epsilon_{\text{opt}}\) everywhere. Noting that \(F\) has a minimizer \(x_{\star}\) with \(\left\|x_{\star}\right\|_{2}\leq 3R\), this enables us to apply Proposition 2.9 to \(F\), obtaining the stated bounds on the number of rounds and query complexity.

## 3 2-Round \(O(n^{M+1})\)-Query Algorithm for SFM

Here we present our 2-round, \(O(n^{M+1})\)-query algorithm for SFM. The algorithm AugmentingSets, given in Algorithm 1, iterates over every \(M\)-sparse \(S\subseteq[n]\) (i.e. \(|S|\leq\bar{M}\)) (denoted \(\mathcal{F}\)). For every such \(S\) the algorithm then builds the augmented set \(A(S)\), consisted of the union of \(S\) and all elements \(i\) that have non-positive marginal with respect to \(S\), i.e., \(f(S\cup\{i\})\leq f(S)\). The algorithm then outputs the set \(A(S)\) that has the smallest value.

As we show below, computing all the \(A(S)\) for \(M\)-sparse sets \(S\) can be done in \(1\) round and \(O(n^{M+1})\)-queries, and then computing an element of \(A(S)\) with the smallest value can be done in another round and \(O(n^{M+1})\)-queries. The correctness of the algorithm is guaranteed by the fact that \(A(S)\), for some \(|S|\leq M\), is the maximal minimizer of \(f\), and therefore the algorithm outputs a set with the optimum value6.

Footnote 6: Note, however, that the algorithm doesn’t necessarily output the maximal minimizer itself.

Our main result of this section is the following theorem.

**Theorem 1.4** (Two-round Parallel SFM).: _There is an algorithm (Algorithm 1) that when given an evaluation oracle for submodular \(f:2^{[n]}\rightarrow\mathbb{Z}\) with \(f(\emptyset)=0\) and \(|f(S)|\leq M\) for all \(S\subseteq[n]\) finds a minimizer of \(f\) in 2 rounds and \(O(n^{M+1})\) queries._

Proof.: First, we bound the parallel and query complexity of the algorithm. Line 2 to Line 5 can be implemented in 1 round as they simply evaluate \(f\) on all subsets of \([n]\) of size \(\leq M+1\). Line 6 can be implemented in 1 round by evaluating \(f(A(S))\) for each \(S\in\mathcal{F}\) in parallel. Consequently, the algorithm is implementable in 2 rounds. To bound the query complexity, note that \(|\mathcal{F}|\leq\sum_{k=0}^{M}\binom{n}{k}=O(n^{M})\) and the algorithm only makes \(O(n)\) queries for each \(S\in\mathcal{F}\) (\(O(n)\) in Line 4 and 1 in Line 6). Consequently, the algorithm makes \(O(n^{M+1})\) total queries.

It only remains to show that the algorithm outputs a minimizer of \(f\). We prove this by showing that for some \(S\in\mathcal{F}\), its augmented set \(A(S)\) is the maximal minimizer of \(f\), i.e., the union of all minimizers, which is a minimizer itself by submodularity. This suffices as the algorithm outputs the \(A(S)\) for \(S\in\mathcal{F}\) of smallest value. Let \(S_{*}\) be the maximal minimizer of \(f\). We build a subset \(T\subseteq S_{*}\) of size \(|T|\leq M\), which we call _anchor_, as follows. Start with \(T=\emptyset\) and an arbitrary ordering of elements of \(S_{*}\). For each element \(i\in S_{*}\) in this order, we add it to the current \(T\) if \(f(T\cup\{i\})>f(T)\). Since \(f\) only takes integer values, this means that whenever we add an element \(i\) to \(T\), the value of \(f(T)\) goes up by at least \(1\). At the end of the process we have \(f(T)\geq|T|\). Since \(f(T)\leq M\), it follows that \(|T|\leq M\) and therefore \(T\in\mathcal{F}\).

We now claim that \(A(T)=S_{*}\). For any element \(i\in S_{*}\setminus T\), we didn't add \(i\) to \(T\) because \(f(T_{i}\cup\{i\})-f(T_{i})\leq 0\), where \(T_{i}\subseteq T\) is the value of \(T\) when element \(i\) is visited in the procedure above. By submodularity, \(f(T\cup\{i\})-f(T)\leq f(T_{i}\cup\{i\})-f(T_{i})\leq 0\). This implies that \(S_{*}\subseteq A(T)\). Also note that for any \(j\notin S_{*}\), we have \(f(S_{*}\cup\{j\})>f(S_{*})\) by the _maximality_ of \(S_{*}\). It again follows from submodularity and \(T\subseteq S_{*}\) that \(f(T\cup\{j\})>f(T)\), which implies \(j\notin A(T)\). This proves \(A(T)=S^{*}\) and completes the proof of the theorem. 

## 4 Conclusion

In this paper we designed two new parallel algorithms for minimizing submodular functions \(f:2^{[n]}\rightarrow\mathbb{Z}\cap[-M,+M]\) with round complexities \(\widetilde{O}(n^{1/3}M^{2/3})\) and \(2\), and query complexities \(\widetilde{O}(n^{2}M^{2})\) and \(O(n^{M+1})\), respectively. These \(M\)-dependent sublinear dependence on \(n\) in the round complexities stand in contrast to the \(\widetilde{\Omega}(n)\)-lower bound on the number of rounds required for SFM when \(M=n^{\Theta(n)}\). On the way to the first result, we obtain a new efficient parallel algorithm for \(\varepsilon\)-approximate minimization of \(\ell_{\infty}\)-Lipschitz convex functions over \([0,1]^{n}\) with round-complexity \(\widetilde{O}(n^{1/3}\varepsilon^{-2/3})\). Given results of [11, 1] the dependence on \(n\) is optimal for constant \(\varepsilon\).

Two related open questions are whether one can obtain \(o(n)\)-round SFM algorithms with polylogarithmic dependence on \(M\), and whether one can obtain algorithms for \(\varepsilon\)-approximate minimization of \(\ell_{\infty}\)-Lipschitz convex functions over \([0,1]^{n}\) in \(\widetilde{O}(n^{1/3}\mathsf{poly}\log(1/\varepsilon))\)-rounds, or can one prove lower bounds ruling them out. Another related open question is whether one can perform SFM in \(O(\mathsf{poly}(M))\)-rounds with query complexity \(\mathsf{poly}(n)\).

## Acknowledgements

We thank the anonymous reviewers of NeurIPS 2023 for helpful comments. Part of this work was done while Deeparnab Chakrabarty, Haotian Jiang, and Aaron Sidford were attending the Discrete Optimization trimester program at the Hausdorff Research Institute for Mathematics.

## References

* [ACJ\({}^{+}\)21] Hilal Asi, Yair Carmon, Arun Jambulapati, Yujia Jin, and Aaron Sidford. Stochastic bias-reduced gradient methods. _Advances in Neural Information Processing Systems_, 34:10810-10822, 2021.
* [ALS20] Brian Axelrod, Yang P Liu, and Aaron Sidford. Near-optimal approximate discrete and continuous submodular function minimization. In _Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 837-853. SIAM, 2020.
* [BJL\({}^{+}\)19] Sebastien Bubeck, Qijia Jiang, Yin Tat Lee, Yuanzhi Li, and Aaron Sidford. Complexity of highly parallel non-smooth convex optimization. _Advances in Neural Information Processing Systems_, 2019.
* [BPC\({}^{+}\)11] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization and statistical learning via the alternating direction method of multipliers. _Foundations and Trends\(\otimes\) in Machine learning_, 3(1):1-122, 2011.
* [BS18] Eric Balkanski and Yaron Singer. Parallelization does not accelerate convex optimization: Adaptivity lower bounds for non-smooth convex minimization. _arXiv preprint arXiv:1808.03880_, 2018.
* [BS20] Eric Balkanski and Yaron Singer. A lower bound for parallel submodular minimization. In _Proceedings of the 52nd annual ACM SIGACT Symposium on Theory of Computing (STOC)_, pages 130-139, 2020.
* [BV04] Dimitris Bertsimas and Santosh Vempala. Solving convex programs by random walks. _Journal of the ACM (JACM)_, 51(4):540-556, 2004.
* [BVZ01] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph cuts. _IEEE Transactions on pattern analysis and machine intelligence_, 23(11):1222-1239, 2001.
* [CCK21] Deeparnab Chakrabarty, Yu Chen, and Sanjeev Khanna. A polynomial lower bound on the number of rounds for parallel submodular function minimization and matroid intersection. In _2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)_. IEEE, 2021.
* [CGJS22] Deeparnab Chakrabarty, Andrei Graur, Haotian Jiang, and Aaron Sidford. Improved lower bounds for submodular function minimization. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 245-254. IEEE, 2022.
* [CGT00] AR Conn, NIM Gould, and Ph L Toint. Trust region methods, mps/siam ser. _Optim., SIAM, Philadelphia, PA_, 2000.
* [CJJ\({}^{+}\)20] Yair Carmon, Arun Jambulapati, Qijia Jiang, Yujia Jin, Yin Tat Lee, Aaron Sidford, and Kevin Tian. Acceleration with a ball optimization oracle. _Advances in Neural Information Processing Systems_, 33:19052-19063, 2020.
* [CJJ\({}^{+}\)23] Yair Carmon, Arun Jambulapati, Yujia Jin, Yin Tat Lee, Daogao Liu, Aaron Sidford, and Kevin Tian. Resqueing parallel and private stochastic convex optimization. _arXiv preprint arXiv:2301.00457_, 2023.
* [CLSW17] Deeparnab Chakrabarty, Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. Subquadratic submodular function minimization. In _Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing (STOC)_, pages 1220-1231, 2017.

* [DBW12] John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic optimization. _SIAM Journal on Optimization_, 22(2):674-701, 2012.
* [DG19] Jelena Diakonikolas and Cristobal Guzman. Lower bounds for parallel and randomized convex optimization. In _Conference on Learning Theory_, pages 1132-1157. PMLR, 2019.
* [DVZ21] Daniel Dadush, Laszlo A. Vegh, and Giacomo Zambelli. Geometric rescaling algorithms for submodular function minimization. _Mathematics of Operations Research_, 46(3):1081-1108, 2021.
* [Edm70] Jack Edmonds. Submodular functions, matroids, and certain polyhedra. _Edited by G. Goos, J. Hartmanis, and J. van Leeuwen_, page 11, 1970.
* [FG88] Awi Federgruen and Henry Groenevelt. Characterization and optimization of achievable performance in general queueing systems. _Operations Research_, 36(5):733-741, 1988.
* [FI00] Lisa Fleischer and Satoru Iwata. Improved algorithms for submodular function minimization and submodular flow. In _Proceedings of the thirty-second annual ACM symposium on Theory of computing_, pages 107-116, 2000.
* [GLS81] Martin Grotschel, Laszlo Lovasz, and Alexander Schrijver. The ellipsoid method and its consequences in combinatorial optimization. _Combinatorica_, 1(2):169-197, 1981.
* [GLS88] Martin Grotschel, Laszlo Lovasz, and Alexander Schrijver. _Geometric algorithms and combinatorial optimization_. Springer, 1988.
* [GPRW20] Andrei Graur, Tristan Pollner, Vidhya Ramaswamy, and S. Matthew Weinberg. New query lower bounds for submodular function minimization. _11th Innovations in Theoretical Computer Science Conference, ITCS_, pages 64:1-64:16, 2020.
* [GR18] Pontus Giselsson and Anders Rantzer. _Large-scale and distributed optimization_, volume 2227. Springer, 2018.
* [Har08] Nicholas James Alexander Harvey. _Matchings, matroids and submodular functions_. PhD thesis, Massachusetts Institute of Technology, 2008.
* [IFF01] Satoru Iwata, Lisa Fleischer, and Satoru Fujishige. A combinatorial strongly polynomial algorithm for minimizing submodular functions. _Journal of the ACM (JACM)_, 48(4):761-777, 2001.
* [IO09] Satoru Iwata and James B Orlin. A simple combinatorial algorithm for submodular function minimization. In _Proceedings of the twentieth annual ACM-SIAM symposium on Discrete algorithms_, pages 1230-1237. SIAM, 2009.
* [Iwa03] Satoru Iwata. A faster scaling algorithm for minimizing submodular functions. _SIAM Journal on Computing_, 32(4):833-840, 2003.
* [JB11] Stefanie Jegelka and Jeff Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In _CVPR 2011_, pages 1897-1904. IEEE, 2011.
* [JBS13] Stefanie Jegelka, Francis Bach, and Suvrit Sra. Reflection methods for user-friendly submodular optimization. _Advances in Neural Information Processing Systems_, 26, 2013.
* [Jia21] Haotian Jiang. Minimizing convex functions with integral minimizers. In _Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 976-985. SIAM, 2021.
* [Jia22] Haotian Jiang. Minimizing convex functions with rational minimizers. _Journal of the ACM_, 70(1):1-27, 2022.

* [JLSW20] Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting plane method for convex optimization, convex-concave games, and its applications. In _Proceedings of the 52nd annual ACM SIGACT Symposium on Theory of Computing (STOC)_, pages 944-953, 2020.
* [JLSZ23] Haotian Jiang, Yin Tat Lee, Zhao Song, and Lichen Zhang. Convex minimization with integer minima in \(\widetilde{O}(n^{4})\) time. _arXiv preprint arXiv:2304.03426_, 2023.
* [KBJ\({}^{+}\)15] KS Kumar, Alvaro Barbero, Stefanie Jegelka, Suvrit Sra, and Francis Bach. Convex optimization for parallel energy minimization. _arXiv preprint arXiv:1503.01563_, 2015.
* [KC10] Andreas Krause and Volkan Cevher. Submodular dictionary selection for sparse representation. In _International Conference on Machine Learning (ICML)_, number CONF, 2010.
* [KG05] Andreas Krause and Carlos Guestrin. _Optimal nonmyopic value of information in graphical models: efficient algorithms and theoretical limits_. Carnegie Mellon University. Center for Automated Learning and Discovery, 2005.
* [Kha80] Leonid G Khachiyan. Polynomial algorithms in linear programming. _USSR Computational Mathematics and Mathematical Physics_, 20(1):53-72, 1980.
* [KKT08] Pushmeet Kohli, M Pawan Kumar, and Philip HS Torr. \(p^{3}\) & beyond: Move making algorithms for solving higher order functions. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 31(9):1645-1656, 2008.
* [KNTB09] Yoshinobu Kawahara, Kiyohito Nagano, Koji Tsuda, and Jeff A Bilmes. Submodularity cuts and applications. _Advances in Neural Information Processing Systems_, 22, 2009.
* [KT10] Pushmeet Kohli and Philip HS Torr. Dynamic graph cuts and their applications in computer vision. In _Computer Vision_, pages 51-108. Springer, 2010.
* [KTE88] Leonid G Khachiyan, Sergei Pavlovich Tarasov, and I. I. Erlikh. The method of inscribed ellipsoids. In _Soviet Math. Dokl_, volume 37, pages 226-230, 1988.
* [LB11] Hui Lin and Jeff Bilmes. Optimal selection of limited vocabulary speech corpora. In _Twelfth Annual Conference of the International Speech Communication Association_, 2011.
* [Lev65] Anatoly Yur'evich Levin. An algorithm for minimizing convex functions. In _Doklady Akademii Nauk_, volume 160, pages 1244-1247. Russian Academy of Sciences, 1965.
* [Lov83] Laszlo Lovasz. Submodular functions and convexity. _Mathematical Programming The State of the Art: Bonn 1982_, pages 235-257, 1983.
* [LSW15] Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its implications for combinatorial and convex optimization. In _2015 IEEE 56th Annual Symposium on Foundations of Computer Science_, pages 1049-1065. IEEE, 2015.
* [Nem94] Arkadi Nemirovski. On parallel complexity of nonsmooth convex optimization. _Journal of Complexity_, 10(4):451-463, 1994.
* [New65] Donald J Newman. Location of the maximum on unimodal surfaces. _Journal of the ACM (JACM)_, 12(3):395-398, 1965.
* [NN89] YE Nesterov and AS Nemirovskii. Self-concordant functions and polynomial time methods in convex programming. preprint, central economic & mathematical institute, ussr acad. _Sci. Moscow, USSR_, 1989.
* [Orl09] James B Orlin. A faster strongly polynomial time algorithm for submodular function minimization. _Mathematical Programming_, 118(2):237-251, 2009.

* [QS95] Maurice Queyranne and Andreas S Schulz. Scheduling unit jobs with compatible release dates on parallel machines with nonstationary speeds. In _Integer Programming and Combinatorial Optimization: 4th International IPCO Conference Copenhagen, Denmark, May 29-31, 1995 Proceedings 4_, pages 307-320. Springer, 1995.
* [SBB\({}^{+}\)18] Kevin Scaman, Francis Bach, Sebastien Bubeck, Laurent Massoulie, and Yin Tat Lee. Optimal algorithms for non-smooth distributed optimization in networks. _Advances in Neural Information Processing Systems_, 31, 2018.
* [Sch00] Alexander Schrijver. A combinatorial algorithm minimizing submodular functions in strongly polynomial time. _Journal of Combinatorial Theory, Series B_, 80(2):346-355, 2000.
* [Sho77] Naum Z Shor. Cut-off method with space extension in convex programming problems. _Cybernetics_, 13(1):94-96, 1977.
* [Top98] Donald M Topkis. _Supermodularity and complementarity_. Princeton university press, 1998.
* [Vai89] Pravin M Vaidya. A new algorithm for minimizing convex functions over convex sets. In _30th Annual IEEE Symposium on Foundations of Computer Science (FOCS)_, pages 338-343, 1989.
* [Ver18] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [Vyg03] Jens Vygen. A note on Schrijver's submodular function minimization algorithm. _Journal of Combinatorial Theory, Series B_, 88(2):399-402, 2003.
* [YN76] David B Yudin and Arkadii S Nemirovski. Evaluation of the information complexity of mathematical programming problems. _Ekonomika i Matematicheskie Metody_, 12:128-142, 1976.

Supplement

In this supplement, we provide the complete proofs of Lemma 2.3 and Theorem 2.2.

Proof of Lemma 2.3.: First we show that \(f_{\mathsf{reg}}^{c,r}(x)\) is convex and \(3L\)-Lipschitz. Let \(h(x):=2L(\left\lVert x-c\right\rVert-r)\). By triangle inequality and homogenity of norms, \(2L(\left\lVert x-c\right\rVert-r)\) is convex and \(2L\)-Lipschitz (with respect to \(\left\lVert\cdot\right\rVert\)). Further, it is straightforward to check that the maximum of two convex, \(L\)-Lipschitz functions is convex and \(L\)-Lipschitz and that the sum of a \(L\)-Lipschitz and \(2L\)-Lipschitz function is convex and \(3L\)-Lipschitz. Since \(f_{\mathsf{reg}}^{c,r}(x)=f(x)+\max\{0,h(x)\}\) and constant functions are convex and \(0\)-Lipschitz the result follows.

Next, consider \(y\in\mathbb{R}^{n}\) with \(\left\lVert y-c\right\rVert\geq r\). Since \(\left\lVert\cdot\right\rVert\) is a norm we see that \(\left\lVert y^{c,r}-c\right\rVert=r\) and therefore \(f_{\mathsf{reg}}^{c,r}(y^{c,r})=f(y^{c,r})\). Furthermore, \(\left\lVert y^{c,r}-y\right\rVert=\left\lVert y-c\right\rVert-r\) and so,

\[f_{\mathsf{reg}}^{c,r}(y) =f(y)+2L(\left\lVert y-c\right\rVert-r)\geq f(y^{c,r})-L\left \lVert y^{c,r}-y\right\rVert+2L(\left\lVert y-c\right\rVert-r)\] \[=f(y^{c,r})+L(\left\lVert y-c\right\rVert-r)\,.\]

where the inequality follows since \(f\) is \(L\)-Lipschitz with respect to \(\left\lVert\cdot\right\rVert\). This yields the desired result as it implies that values of \(f_{\mathsf{reg}}^{c,r}\) are all larger than those of points where \(\left\lVert x-c\right\rVert\leq r\) for which \(f_{\mathsf{reg}}^{c,r}(x)=f(x)\). Further, since \(\left\{x\in\mathbb{R}^{n}|\left\lVert x-c\right\rVert\leq r\right\}\) is closed and \(f\) is Lipschitz, there exists a minimizer of \(\min_{x\in\mathbb{R}^{n}\left\lVert x-c\right\rVert\leq r}f(x)\) and the result follows. 

Proof of Theorem 2.2.: We invoke Lemma 2.3 and apply BallAccel on the function \(F:=\widehat{f}_{\mathsf{reg}_{\rho}}^{c,R}\) as defined in Lemma 2.3 with respect to \(\left\lVert\cdot\right\rVert_{2}\), \(c\) being the origin, and \(\rho:=\frac{\epsilon_{\mathsf{opt}}}{L\sqrt{2\log n}}\). With this choice of \(\rho\), by Lemma 2.6, we know \(\left\lvert F(x)-f_{\mathsf{reg}}^{c,R}(x)\right\rvert\leq\epsilon_{\mathsf{ opt}}\) everywhere. For brevity, we remove the superscript \((c,R)\) from \(f_{\mathsf{reg}}\) for the remainder of the proof.

We now claim that \(F\) has a minimizer \(x_{\star}\) with \(\left\lVert x_{\star}\right\rVert_{2}\leq 3R\). To see this, let \(y\in\mathbb{R}^{n}\) be a point with \(\left\lVert y\right\rVert_{2}>3R\) and consider \(\tilde{y}=\frac{y}{\left\lVert y\right\rVert_{2}}\cdot R\). Therefore,

\[F(y)\geq f_{\mathsf{reg}}(y)-\epsilon_{\mathsf{opt}}=f(y)+2L\cdot(\left\lVert y \right\rVert_{2}-R)-\epsilon_{\mathsf{opt}}\geq f_{\mathsf{reg}}(\tilde{y})+L \cdot(\left\lVert y\right\rVert_{2}-R)-\epsilon_{\mathsf{opt}}\]

where the first inequality follows from pointwise approximation of \(F\) and \(f_{\mathsf{reg}}\), and the second follows since \(f\) is also \(L\)-Lipschitz in the \(\left\lVert\cdot\right\rVert_{2}\) norm, and so \(f_{\mathsf{reg}}(\tilde{y})=f(\tilde{y})\leq f(y)+L\cdot(\left\lVert y\right\rVert _{2}-R)\). Again using the pointwise approximation of \(F\) and \(f_{\mathsf{reg}}\) we get \(F(y)\geq F(\tilde{y})-2\epsilon_{\mathsf{opt}}+L\cdot(\left\lVert y\right\rVert _{2}-R)\), and if \(\left\lVert y\right\rVert_{2}>3R\) using that \(\epsilon_{\mathsf{opt}}\in(0,LR]\) we get a contradiction to minimality of \(F(y)\).

Note that the stochastic subgradient \(g^{\prime}\) of \(f_{\mathsf{reg}}\) is given by \(g^{\prime}=g+2L\cdot v\) for \(v\in\partial h(x)\) where \(h(x)=\max(0,\left\lVert x\right\rVert_{2}-R)\). Note that \(\left\lVert v\right\rVert_{2}^{2}\leq 1\), so the stochastic gradient \(g^{\prime}\) satisfies \(\mathbb{E}\|g^{\prime}(x)\|_{2}^{2}\leq 2\sigma^{2}+8L^{2}\). It follows that by setting \(r=\rho\), Proposition 2.8 implies that for any \(\phi,\lambda>0\), there exists a \((\phi,\lambda,\rho)\)-ball optimization oracle for \(F\) which makes \(O(\frac{\sigma^{2}}{\phi\lambda})\) total queries to \(g\) in \(O(1)\) parallel rounds (as \(\sigma^{2}\geq L^{2}\) by definition of \(L\)-Lipschitzness).

Next we apply ball acceleration in Proposition 2.9 to \(F\). We have already argued above that the minimizer \(x_{\star}\) of \(F\) satisfies \(\left\lVert x_{\star}\right\rVert_{2}\leq 3R\). We set the parameter \(r=\rho=\frac{\epsilon_{\mathsf{opt}}}{L\sqrt{2\log n}}\) and the \(R\) and \(L\) multiplied by factor \(3\). Using these, the parameters of (6) in Proposition 2.9 become

\[\kappa=\frac{LR}{\epsilon_{\mathsf{opt}}},\ \ K=\left(\frac{LR\sqrt{2\log n}}{ \epsilon_{\mathsf{opt}}}\right)^{2/3},\ \text{ and }\lambda_{\star}=\frac{\epsilon_{\mathsf{opt}}K^{2}}{R^{2}}\cdot\log^{2}\kappa.\]

Using Proposition 2.9 and Proposition 2.8, we get that the number of rounds of queries is \((CK\log\kappa)\cdot(C\log^{2}(R\kappa/\rho))\) which is \(\widetilde{O}((LR/\epsilon_{\mathsf{opt}})^{2/3})\).

Next, we bound the query complexity. Adding up the queries made by the ball-optimization oracle calls made in all iterations per part 1 of Proposition 2.9 is

\[(CK\log\kappa)\cdot\left(C\log^{2}(R\kappa/\rho)\right)\cdot\frac{\sigma^{2}}{ \lambda_{\star}^{2}\rho^{2}}=\widetilde{O}\left(K\sigma^{2}\lambda_{\star}^{-2} \rho^{-2}\right)\.\]Adding up the queries made by the ball-optimization oracle calls made in all iterations per part 2 of Proposition 2.9 is

\[\sum_{j\in[\lceil\log_{2}K+C\rceil]}\left(C^{2}2^{-j}K\log(R\kappa/\rho)\right) \cdot\frac{2^{j}C\sigma^{2}\log^{2}(R\kappa/\rho)}{\lambda_{\star}^{2}\rho^{2}} \text{ which is also }\widetilde{O}\left(K\sigma^{2}\lambda_{\star}^{-2}\rho^{-2}\right).\]

Substituting the values from above and \(\rho=\frac{\epsilon_{\text{opt}}}{L\sqrt{2\log n}}\), we get that the total query complexity is \(\widetilde{O}((\sigma R/\epsilon_{\text{opt}})^{2})\).

Finally note that Proposition 2.9 applied to \(F\) computes an \(\epsilon_{\text{opt}}\)-approximate minimizer which is a \(2\epsilon_{\text{opt}}\)-approximate minimizer of \(f_{\text{reg}}\). By setting \(\epsilon_{\text{opt}}=\varepsilon/2\), Lemma 2.3 implies that we can find an \(\varepsilon\)-approximate minimizer \(x\) of \(f\) with \(\left\|x\right\|_{2}\leq R\). This completes the proof of the theorem.