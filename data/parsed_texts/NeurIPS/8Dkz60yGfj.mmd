# Adjust Pearson's \(r\) to Measure Arbitrary Monotone Dependence

 Xinbo Ai

School of Intelligent Engineering and Automation

Beijing University of Posts and Telecommunications

Beijing 100876, China

axb@bupt.edu.cn

###### Abstract

Pearson's \(r\), the most widely-used correlation coefficient, is traditionally regarded as exclusively capturing linear dependence, leading to its discouragement in contexts involving nonlinear relationships. However, recent research challenges this notion, suggesting that Pearson's \(r\) should not be ruled out _a priori_ for measuring nonlinear monotone relationships. Pearson's \(r\) is essentially a scaled covariance, rooted in the renowned Cauchy-Schwarz Inequality. Our findings reveal that different scaling bounds yield coefficients with different capture ranges, and interestingly, tighter bounds actually expand these ranges. We derive a tighter inequality than Cauchy-Schwarz Inequality, leverage it to refine Pearson's \(r\), and propose a new correlation coefficient, i.e., rearrangement correlation. This coefficient is able to capture arbitrary monotone relationships, both linear and nonlinear ones. It reverts to Pearson's \(r\) in linear scenarios. Simulation experiments and real-life investigations show that the rearrangement correlation is more accurate in measuring nonlinear monotone dependence than the three classical correlation coefficients, and other recently proposed dependence measures.

## 1 Introduction

Proposed in the late 19th century, Pearson's \(r\)(Pearson, 1896) has been one of the main tools for scientists and engineers to study bivariate dependence during the 20th century. It is remarkably unaffected by the passage of time (Lee Rodgers and Alan Nice Wander, 1988) and still goes strong in the 21st century (Puccetti, 2022). It has been, and probably still is, the most used measure for statistical associations, and generally accepted as _the_ measure of dependence, not only in statistics, but also in most applications of natural and social sciences (Tjostheim, Otneim, and Stove, 2022).

Despite its popularity, Pearson's \(r\) has a number of shortcomings, and the most serious issue might be that it can only capture linear dependence, as stated in classical textbooks (Wasserman, 2004) and contemporary literatures (Armstrong, 2019; Tjostheim, Otneim, and Stove, 2022). The use of Pearson's \(r\) has been strongly discouraged for forms of associations other than linear ones (Speed, 2011).

Numerous nonlinear alternative coefficients have been proposed to address this deficiency, such as Spearman's \(\rho\)(Spearman, 1904), Kendall's \(\tau\)(Kendall, 1938), Hilbert-Schmidt Independence Criterion(HSIC) (Gretton et al., 2005), distance correlation(dCor) (Szekely, Rizzo, and Bakirov, 2007), Maximal Information Coefficient(MIC) (Reshef et al., 2011), and Chatterjee's \(\xi\)(Chatterjee, 2021). Their capture ranges are extending from linear dependence to monotone dependence, and then to non-monotone dependence. Without exception, all these coefficients adopt radically different approaches for nonlinear dependence, rather than following the original way of Pearson's \(r\) for a breakthrough.

In their recent paper titled "_Myths About Linear and Monotonic Associations: Pearson's \(r\), Spearman's \(\rho\), and Kendall's \(\tau\)_", van den Heuvel and Zhan (2022) challenged the widespread belief that Pearson's \(r\) is only a measure for linear dependence, proving this notion to be false. Their findings indicate that Pearson's \(r\) should not be ruled out _a priori_ for measuring nonlinear monotone dependence. Although this potential has been recognized, the specific approach to using Pearson's \(r\) for accurate measurement of nonlinear monotone dependence remains unresolved.

Pearson's \(r\) is essentially a scaled covariance, with the renowned Cauchy-Schwarz Inequality as its mathematical foundation. We find that different scaling bounds yield coefficients with different capture ranges, and interestingly, tighter bounds actually expand these ranges. We derive a tighter inequality than Cauchy-Schwarz Inequality, leverage it to adjust Pearson's \(r\) to measure nonlinear monotone dependence. The adjusted version of Pearson's \(r\) is more accurate in measuring nonlinear monotone dependence than the three classical correlation coefficients, and other recently proposed dependence measures.

## 2 Methods

### Definitions and notations

Consider real-valued random variables \(X\) and \(Y\) with cdf's (cumulative distribution functions) \(F\) and \(G\) respectively. We denote the covariance of \(X\) and \(Y\) as \(\operatorname{cov}\left(X,Y\right)\); the variance of \(X\) as \(\operatorname{var}\left(X\right)\), and the variance of \(Y\) as \(\operatorname{var}\left(Y\right)\). We assume that \(0<\operatorname{var}\left(X\right)<\infty\), \(0<\operatorname{var}\left(Y\right)<\infty\). We define \(X^{\uparrow}=F^{-1}\left(U\right)\), \(X^{\downarrow}=F^{-1}\left(1-U\right)\). Here, \(U\) is a random variable with the uniform distribution on \(\left(0,1\right)\), and \(F^{-1}\) is the inverse cdf or quantile function defined as \(F^{-1}\left(u\right)=\inf\left\{x\in\mathbb{R}:F\left(x\right)\geqslant u \right\},u\in\left(0,1\right)\). Similarly, \(Y^{\uparrow}=G^{-1}\left(U\right)\), \(Y^{\downarrow}=G^{-1}\left(1-U\right)\).

Let \(x=\left(x_{1},\cdots,x_{n}\right)\) and \(y=\left(y_{1},\cdots,y_{n}\right)\) be samples of \(X\) and \(Y\), each with \(n\) elements. Neither \(x\) nor \(y\) is constant. We denote the sample covariance of \(x\) and \(y\) as \(s_{x,y}\); the sample variance of \(x\) as \(s_{x}^{2}\); the sample variance of \(y\) as \(s_{y}^{2}\). We define the increasing and decreasing rearrangement of \(x\) as \(x^{\uparrow}=\left(x_{(1)},x_{(2)},\cdots,x_{(n)}\right)\) and \(x^{\downarrow}=\left(x_{(n)},x_{(n-1)},\cdots,x_{(1)}\right)\) respectively, with \(x_{(1)}\leqslant x_{(2)}\leqslant\cdots\leqslant x_{(n)}\). Similarly, we define \(y^{\uparrow}=\left(y_{(1)},y_{(2)},\cdots,y_{(n)}\right)\), \(y^{\downarrow}=\left(y_{(n)},y_{(n-1)},\cdots,y_{(1)}\right)\).

**Definition 1**.: _A subset \(S\) of \(\mathbb{R}^{2}\) is non-decreasing (resp. non-increasing) if and only if for all \(\left(x_{1},y_{1}\right)\), \(\left(x_{2},y_{2}\right)\) in \(S\), \(x_{1}<x_{2}\) implies \(y_{1}\leqslant y_{2}\) (resp. \(x_{1}<x_{2}\) implies \(y_{1}\geqslant y_{2}\)). Random variables \(X\) and \(Y\) are called increasing (resp. decreasing) monotone dependent if \(\left(X,Y\right)\) lies almost surely in a non-decreasing (resp. non-increasing) subset of \(\mathbb{R}^{2}\). Samples \(x\) and \(y\) are called increasing (resp. decreasing) monotone dependent if \(\left\{\left(x,y\right)\right\}\) is a non-decreasing (resp. non-increasing) subset of \(\mathbb{R}^{2}\)._

Definition 1 is sourced from (Mikusinski, Sherwood, and Taylor, 1991). Clearly Definition 1 is symmetrical with respect to \(X\) and \(Y\). The monotone dependence outlined here encompasses a broader scope than definitions like the one in (Kimeldorf and Sampson, 1978), where "_each of \(X\) and \(Y\) is almost surely a monotone function of the other_". This is primarily because it doesn't necessitate a one-to-one mapping. Also, linear dependence, i.e., \(P\left(Y=\alpha X+\beta\right)=1\) at the population level or \(y=ax+b\) at the sample level, is special case of monotone dependence, and we will refer to dependence which is monotone but not linear as "_nonlinear monotone dependence_".

### Different bounds lead to different capture ranges

With Cauchy-Schwarz Inequality, the well-known covariance inequality can be directly derived as

\[\left|\operatorname{cov}\left(X,Y\right)\right|\leqslant\sqrt{\operatorname{ var}\left(X\right)\operatorname{var}\left(Y\right)},\]

thus the geometric mean of \(\operatorname{var}\left(X\right)\) and \(\operatorname{var}\left(Y\right)\), i.e., \(\sqrt{\operatorname{var}\left(X\right)\operatorname{var}\left(Y\right)}\), mathematically provides a bound for covariance \(\operatorname{cov}\left(X,Y\right)\), which ensures that Pearson's Correlation Coefficient

\[r\left(X,Y\right)=\frac{\operatorname{cov}\left(X,Y\right)}{\sqrt{ \operatorname{var}\left(X\right)\operatorname{var}\left(Y\right)}}\]

always falls into the range \(\left[-1,+1\right]\). Scaled by \(\sqrt{\operatorname{var}\left(X\right)\operatorname{var}\left(Y\right)}\), Pearson's \(r\) turns into a _normalized covariance_, which is dimensionless and bounded. It possesses significant advantage over the original _covariance_ in the sense that its value will not be affected by the change in the units of \(X\) and \(Y\).

A crucial issue that has been neglected so far is that boundedness doesn't ensure optimum. Scaling \(\operatorname{cov}\left(X,Y\right)\) to the range \([-1,+1]\) is not the only thing that matters. In fact, different bounds can be utilized to scale covariance to be bounded coefficients, as reported in previous works (Lin, 1989; Zegers, 1986).

For example, with the Mean Inequality Series (Hardy, Littlewood, and Polya, 1952), it is immediate that

\[\left|\operatorname{cov}\left(X,Y\right)\right|\leqslant\sqrt{\operatorname{ var}\left(X\right)\operatorname{var}\left(Y\right)}\leqslant\frac{1}{2}\left( \operatorname{var}\left(X\right)+\operatorname{var}\left(Y\right)\right)\]

in the sense that geometric mean \(\sqrt{\operatorname{var}\left(X\right)\operatorname{var}\left(Y\right)}\) is always less than or equal to arithmetic mean \(\frac{1}{2}\left(\operatorname{var}\left(X\right)+\operatorname{var}\left(Y \right)\right)\) for nonnegative values \(\operatorname{var}\left(X\right)\) and \(\operatorname{var}\left(Y\right)\). Then we get a looser bound for covariance, i.e., \(\frac{1}{2}\left(\operatorname{var}\left(X\right)+\operatorname{var}\left(Y \right)\right)\), with which another measure can be defined as follows(Zegers, 1986):

\[r^{+}\left(X,Y\right)=\frac{\operatorname{cov}\left(X,Y\right)}{\frac{1}{2} \left(\operatorname{var}\left(X\right)+\operatorname{var}\left(Y\right) \right)}\]

\(r^{+}\left(X,Y\right)\) is named early as _Additivity Coefficient_ (Zegers, 1986) and later as _Standardized Covariance_ (Andraszewicz and Rieskamp, 2014). It is proved that the capture range of \(r^{+}\left(X,Y\right)\) is limited to additive relationships, i.e., \(Y=\pm X+\beta\), which are special cases of linear relationships, i.e., \(Y=\alpha X+\beta\), with \(\alpha\) being fixed to \(\pm 1\)(Zegers, 1986).

Further, we can find an even looser bound for covariance, in the sense that

\[\frac{1}{2}\left(\operatorname{var}\left(X\right)+\operatorname{var}\left(Y \right)\right)\leqslant\frac{1}{2}\left(\operatorname{var}\left(X\right)+ \operatorname{var}\left(Y\right)+\left|\bar{X}-\bar{Y}\right|^{2}\right),\]

and define a new measure as follows:

\[r^{=}\left(X,Y\right)=\frac{\operatorname{cov}\left(X,Y\right)}{\frac{1}{2} \left(\operatorname{var}\left(X\right)+\operatorname{var}\left(Y\right)+ \left|\bar{X}-\bar{Y}\right|^{2}\right)}\]

\(r^{=}\) is named as _Concordance Correlation Coefficient_ (Lin, 1989), and it is designed to measure identical relationship, i.e., \(Y=\pm X\). When \(X\) and \(Y\) are both positive, it can be utilized to evaluate their agreement by measuring the variation from the 45\({}^{\circ}\) line through the origin (Lin, 1989).

As for the abovementioned three measures, \(r\), \(r^{+}\), and \(r^{=}\), they share the same numerator, \(\operatorname{cov}\left(X,Y\right)\), the differences lie in their denominators. These denominators serve as bounds for \(\operatorname{cov}\left(X,Y\right)\). Different bounds lead to different capture ranges. With the bounds being looser, their capture ranges are shrinking from linear (\(Y=\alpha X+\beta\)) towards additive (\(Y=\pm X+\beta\)) and ultimately to identical (\(Y=\pm X\)) relationships. The looser the bound, the narrower the capture range.

Up until now, all the efforts have only led to looser bounds and measures with narrower capture ranges. _Could we possibly explore breakthroughs by approaching the problem from the opposite direction, aiming to achieve a tighter bound and consequently, devise a new measure with a broader capture range?_

The bound in Pearson's \(r\) is intrinsically provided by Cauchy-Schwarz Inequality, which is one of the most widespread and useful inequalities in mathematics. Cauchy-Schwartz Inequality is so classic and reliable that one seldom tries to improve it. Both bounds in \(r^{+}\) and \(r^{=}\) are looser than that provided by Cauchy-Schwartz Inequality. To loosen Cauchy-Schwartz Inequality might be easy while to tighten such a classic inequality might be relatively difficult. However, we find that it is not impossible to improve the tightness of Cauchy-Schwarz Inequality. In other words, there exists sharper bound for covariance, which will be depicted in the next section.

### New inequality tighter than Cauchy-Schwarz Inequality

Before deriving the new inequality, we will briefly review the classic Cauchy-Schwarz inequality, which is common in textbooks. The Cauchy-Schwarz inequality states that for \(x\) and \(y\), we have

\[\left|\left\langle x,y\right\rangle\right|\leqslant\left\|x\right\|\left\|y \right\|,\]

where \(\left\langle\cdot,\cdot\right\rangle\) is the inner product. and \(\left\|\cdot\right\|\) is the norm. The equality holds if and only if \(x\) and \(y\) are linearly dependent, i.e., \(y=ax\) for some constant \(a\).

After defining an inner product on the set of random variables using the expectation of their product, i.e., \(\left\langle X,Y\right\rangle=EXY\), the Cauchy-Schwarz inequality becomes

\[\left|EXY\right|\leqslant\sqrt{EX^{2}EY^{2}}.\]

Now, we will sharpen the Cauchy-Schwarz inequality. On the basis of the rearrangement theorems (Hardy, Littlewood, and Polya, 1952), we derive 6 theorems(corollaries/propositions) as follows.

**Theorem 1**.: _For random variables \(X\) and \(Y\), we have_

\[\left|EXY\right|\leqslant\left|EX^{\updownarrow}Y^{\updownarrow}\right| \leqslant\sqrt{EX^{2}EY^{2}}.\]

_The equality on the left holds if and only if \(X\) and \(Y\) are monotone dependent, and the equality on the right holds if and only if \(Y\mathop{=}\limits^{d}\alpha X\), with \(\mathop{\mathrm{sgn}}\left(EXY\right)=\mathop{\mathrm{sgn}}\left(\alpha\right)\)._

_Here, \(\mathop{=}\limits^{d}\) denotes equality in distribution, and \(EX^{\updownarrow}Y^{\updownarrow}\) is defined as:_

\[EX^{\updownarrow}Y^{\updownarrow}=\left\{\begin{array}{cc}EX^{\updownarrow} Y^{\updownarrow},if&EXY\geqslant 0\\ EX^{\updownarrow}Y^{\updownarrow},if&EXY<0\end{array}\right.\]

For the sake of conciseness, the proofs of Theorem 1 and theorems undermined are all included in Appendix A.1.

**Theorem 2**.: _For samples \(x\) and \(y\) we have_

\[\left|\left\langle x,y\right\rangle\right|\leqslant\left|\left\langle x^{ \updownarrow},y^{\updownarrow}\right\rangle\right|\leqslant\left\|x\right\| \left\|y\right\|.\]

_The equality on the left holds if and only if \(x\) and \(y\) are monotone dependent, and the equality on the right holds if and only if \(y\) is arbitrary permutation of \(ax\), with \(\mathop{\mathrm{sgn}}\left(\left\langle x,y\right\rangle\right)=\mathop{ \mathrm{sgn}}\left(a\right)\)._

_Here, \(\left\langle x^{\updownarrow},y^{\updownarrow}\right\rangle\) is defined as:_

**Corollary 1**.: _For random variables \(X\) and \(Y\), we have covariance inequality series as:_

\[\left|\mathop{\mathrm{cov}}\left(X,Y\right)\right|\leqslant\left| \mathop{\mathrm{cov}}\left(X^{\updownarrow},Y^{\updownarrow}\right)\right| \leqslant\sqrt{\mathop{\mathrm{var}}\left(X\right)\mathop{ \mathrm{var}}\left(Y\right)}\] \[\leqslant\tfrac{1}{2}\left(\mathop{\mathrm{var}}\left(X\right)+ \mathop{\mathrm{var}}\left(Y\right)\right)\] \[\leqslant\tfrac{1}{2}\left(\mathop{\mathrm{var}}\left(X\right)+ \mathop{\mathrm{var}}\left(Y\right)+\left|\bar{X}-\bar{Y}\right|^{2}\right)\]

_The first equality holds if and only if \(X\) and \(Y\) are monotone dependent, and the second equality holds if and only if \(Y\mathop{=}\limits^{d}\alpha X+\beta\), with \(\mathop{\mathrm{sgn}}\left(\mathop{\mathrm{cov}}\left(X,Y\right)\right)= \mathop{\mathrm{sgn}}\left(\alpha\right)\)._

_Here, \(\mathop{\mathrm{cov}}\left(X^{\upuparrow},Y^{\updownarrow}\right)\) is defined as:_

\[\mathop{\mathrm{cov}}\left(X^{\updownarrow},Y^{\updownarrow}\right)=\left\{ \begin{array}{cc}\mathop{\mathrm{cov}}\left(X^{\updownarrow},Y^{\updownarrow} \right),&if&\mathop{\mathrm{cov}}\left(X,Y\right)\geqslant 0\\ \mathop{\mathrm{cov}}\left(X^{\updownarrow},Y^{\updownarrow}\right)&if& \mathop{\mathrm{cov}}\left(X,Y\right)<0\end{array}\right.\]

**Corollary 2**.: _For samples \(x\) and \(y\), we have covariance inequality series as_

\[\left|s_{x,y}\right|\leqslant\left|s_{x^{\upuparrow},y^{\updownarrow}}\right| \leqslant\sqrt{s_{x}^{2}s_{y}^{2}}\] \[\leqslant\frac{1}{2}\left(s_{x}^{2}+s_{y}^{2}\right)\] \[\leqslant\frac{1}{2}\left(s_{x}^{2}+s_{y}^{2}+\left|\bar{x}-\bar{ y}\right|^{2}\right)\]

_The first equality holds if and only if \(x\) and \(y\) are monotone dependent, and the second equality holds if and only if \(y\) is arbitrary permutation of \(ax+b\), with \(\mathop{\mathrm{sgn}}\left(s_{x,y}\right)=\mathop{\mathrm{sgn}}\left(a\right)\)._

_Here, \(s_{x^{\upuparrow},y^{\updownarrow}}\) is defined as:_

\[s_{x^{\upuparrow},y^{\updownarrow}}=\left\{\begin{array}{cc}s_{x^{\upuparrow},y^{\updownarrow}},if&s_{x,y}\geqslant 0\\ s_{x^{\upuparrow},y^{\downarrow}},if&s_{x,y}<0\end{array}\right.\]

### The proposed Rearrangement Correlation

The inequality series in Corollary 1 and Corollary 2 provides sharper bounds for covariance at the population level and the sample level respectively. We will leverage them to define the so-called _Rearrangement Correlation_, which is the adjusted version of Pearson's \(r\) proposed here.

**Definition 2**.: _The Rearrangement Correlation of random variables \(X\) and \(Y\) is defined as:_

\[r^{\#}\left(X,Y\right)=\frac{\operatorname{cov}\left(X,Y\right)}{\left| \operatorname{cov}\left(X^{\uparrow},Y^{\downarrow}\right)\right|}\]

**Definition 3**.: _The Rearrangement Correlation of samples \(x\) and \(y\) is defined as:_

\[r^{\#}\left(x,y\right)=\frac{s_{x,y}}{\left|s_{x^{\uparrow},y^{\downarrow}}\right|}\]

The new measure is named "_Rearrangement Correlation_" because its theoretical foundation is the rearrangement inequality, as shown in Theorem 1 and Theorem 2. We adopt the musical sharp symbol # to denote rearrangement correlation, signifying that this measure has sharp values because of its sharp bounds. Analogous to how C# is pronounced as _C-sharp_, \(r^{\#}\) is pronounced as _r-sharp_.

As for the relationship between \(r^{\#}\left(x,y\right)\) and \(r^{\#}\left(X,Y\right)\), it is clear that \(r^{\#}\left(x,y\right)\) converges to \(r^{\#}\left(X,Y\right)\) when \(n\rightarrow\infty\) according to their definitions.

The capture range of rearrangement correlation is no longer limited to linear dependence but monotone dependence, which is revealed by the next proposition.

**Proposition 1**.: _For random variables \(X\), \(Y\), and samples \(x,\)\(y\), the following hold:_

* \(\left|r^{\#}\left(X,Y\right)\right|\leqslant 1\) _and the equality holds if and only if_ \(X\) _and_ \(Y\) _are monotone dependent._
* \(\left|r^{\#}\left(x,y\right)\right|\leqslant 1\) _and the equality holds if and only if_ \(x\) _and_ \(y\) _are monotone dependent._

An interesting question might arise in one's mind: how can the simple adjustment, replacing \(\sqrt{\operatorname{var}\left(X\right)\operatorname{var}\left(Y\right)}\) with \(\left|\operatorname{cov}\left(X^{\uparrow},Y^{\downarrow}\right)\right|\), leads to the capture range expanding from linear dependence to (nonlinear) monotone dependence? The capture range is inherited from _covariance_ itself. The capture range of covariance is limited neither to identical dependence as that of _Concordance Correlation Coefficient_, additive dependence as that of _Additivity Coefficient_, nor to linear dependence as that of Pearson's \(r\). In fact, it can potentially detect and measure arbitrary monotone dependence, if scaled properly. In other words, Pearson's \(r\) is also measuring nonlinear monotone dependence to some extent. The adjustment is nothing more than compensating for underestimation.

The relationships among the above-mentioned _Concordance Correlation Coefficient_ (\(r^{=}\)), _Additivity Coefficient_ (\(r^{+}\)), _Pearson's \(r\)_, and the new proposed _Rearrangement Correlation_ (\(r^{\#}\)) are depicted in Figure 1.

Figure 1: Covariance inequality series, correlation coefficients and their capture rangesThe following proposition reveals the relationship between Pearson's \(r\) and its adjusted version, i.e., Rearrangement Correlation:

**Proposition 2**.: _For random variables \(X\), \(Y\), and samples \(x\), \(y\), the following hold:_

* \(\left|r^{\#}\left(X,Y\right)\right|\geqslant\left|r\left(X,Y\right)\right|\) _and the equality holds if and only if_ \(Y\,\raisebox{-1.72pt}{$\stackrel{{ d}}{{=}}$}\,\alpha X+\beta\)_, with_ \(\operatorname{sgn}\left(r\left(X,Y\right)\right)=\operatorname{sgn}\left( \alpha\right)\)_._
* \(\left|r^{\#}\left(x,y\right)\right|\geqslant\left|r\left(x,y\right)\right|\) _and the equality holds if and only if_ \(y\) _is arbitrary permutation of_ \(ax+b\)_, with_ \(\operatorname{sgn}\left(r\left(x,y\right)\right)=\operatorname{sgn}\left(a\right)\)_._

Proposition 2 shows that \(r^{\#}\left(X,Y\right)\) will revert to \(r\left(X,Y\right)\) if and only if \(Y\,\raisebox{-1.72pt}{$\stackrel{{ d}}{{=}}$}\,\alpha X+\beta\), \(\operatorname{sgn}\left(r\left(X,Y\right)\right)=\operatorname{sgn}\left( \alpha\right)\), and \(r^{\#}\left(x,y\right)\) to \(r\left(x,y\right)\) if and only if \(y\) is arbitrary permutation of \(ax+b\), with \(\operatorname{sgn}\left(r\left(x,y\right)\right)=\operatorname{sgn}\left(a\right)\). It is clear that linear dependence is special case of these conditions. Thus, \(r^{\#}\) reverts to \(r\) in linear scenarios.

Another question to be asked is, do we need a new monotone measure given that rank-based measures such as Spearman's \(\rho\) can already measure monotone dependence? The answer is twofold:

On the one hand, \(r^{\#}\) has a higher _resolution_ and is more accurate. Without exception, all measures designed for monotone dependence are utilizing the order information. However, what we utilize here is the original information, rather than the ranks. Mapping numerical values to their ranks does of course produce a certain loss of information. A small difference between two values may no longer be distinguished from a large difference. With sample size \(n\), there are totally \(\frac{n^{3}-n}{6}\) possible \(\rho\) values between \(-1\) and \(+1\), whatever raw values are and however correlated patterns differ. The _resolution_ of Spearman's \(\rho\) might be inadequate. To take a simple example, let \(x=\left(4,3,2,1\right)\) and

* \(y_{1}=\left(5,4,3,2.00\right)\)
* \(y_{2}=\left(5,4,3,3.25\right)\)
* \(y_{3}=\left(5,4,3,3.50\right)\)
* \(y_{4}=\left(5,4,3,3.75\right)\)
* \(y_{5}=\left(5,4,3,4.50\right)\)

Obviously, \(y_{1}\) and \(x\) behaves exactly in the same way, with their values getting small and small step by step. The behavior of \(y_{2}\), \(y_{3}\), \(y_{4}\), and \(y_{5}\) are becoming more and more different from that of \(x\). However, the \(\rho\) values are all the same for \(y_{2}\), \(y_{3}\) and \(y_{4}\). In contrast, the \(r^{\#}\) values can reveal all these differences exactly.

* \(r^{\#}\left(x,y_{1}\right)=1.00\), \(\rho\left(x,y_{1}\right)=1.00\)
* \(r^{\#}\left(x,y_{2}\right)=0.93\), \(\rho\left(x,y_{2}\right)=0.80\)
* \(r^{\#}\left(x,y_{3}\right)=0.85\), \(\rho\left(x,y_{3}\right)=0.80\)
* \(r^{\#}\left(x,y_{4}\right)=0.76\), \(\rho\left(x,y_{4}\right)=0.80\)
* \(r^{\#}\left(x,y_{5}\right)=0.38\), \(\rho\left(x,y_{5}\right)=0.40\)

On the other hand, \(r^{\#}\) is comparable with Pearson's \(r\), while the latter is not. For nonlinear monotone dependence, the value of Spearman's \(\rho\) might be remarkably greater than the value of Pearson's \(r\). One may attempt to search for nonlinear relationships in data by checking whether the value of \(\rho\) far exceeds that of \(r\). However, it might be meaningless and even impossible to compare their values directly. In cases, \(\rho\) can be either greater or less than \(r\), and their sign can even be different. Thus the difference \(\left|\rho\right|-\left|r\right|\) is confusing. On the contrary, the signs of \(r^{\#}\) and \(r\) are always the same, and \(\left|r^{\#}\right|\) is always greater than or equal to \(\left|r\right|\). \(\left|r^{\#}\right|-\left|r\right|\) equals to 0 if and only if \(y\) is arbitrary permutation of \(ax+b\). Its value increases with the degree of nonlinearity.

However, Spearman's \(\rho\) can also be superior to \(r^{\#}\) in the sense that the former is robust to outliers while the latter is not. Rearrangement correlation is a scaled covariance, and the limitation of being non-robust to outliers is inherited from covariance itself. In fact, concordance correlation coefficient, additivity coefficient, and Pearson's \(r\) are also scaled covariance measures, and none of them are robust to outliers.

To be more robust, we can also transform the raw data into their ranks before calculating \(r^{\#}\). Interestingly, \(r^{\#}\) becomes equivalent to Spearman's \(\rho\) when calculated on ranks. Let \(P\) and \(Q\) be the ranks of \(x\) and \(y\) respectively. Then, in the sense that \(sd\left(P,Q\right)=sd\left(P^{\dagger},Q^{\ddagger}\right)=\frac{n\left(n+1 \right)}{12}\), \(r^{\#}\left(P,Q\right)=\rho\left(P,Q\right)\). This explains why \(\rho\) can measure nonlinear monotone relationships while \(r\) only measures linear ones, despite them sharing a similar formula. The key is not just the ranking but achieving a sharp bound. Since \(\rho\) and \(r^{\#}\) are equivalent when applied to ranks, and \(r^{\#}\) can measure arbitrary monotone dependence (as proven in our manuscript), \(\rho\) can do the same.

## 3 Experiments

### Performance metrics

The main purpose of proposing Rearrangement Correlation is to provide a measure of dependence strength for nonlinear monotone relationships, rather than to simply serve as a test statistic for testing independence. Thus, our performance metrics focus on strength measurement.

The basic question to be asked when measuring any attribute is how accurate is this measurement, and there should be no exception for dependence measurement. ISO 5725 uses two terms _"trueness"_ and _"precision"_ to describe the accuracy of a measurement method. _Trueness_ refers to the closeness of agreement between the mean or median of measured results and the true or accepted reference value. _Precision_ refers to the closeness of agreement between measured values (ISO, 1994). The comprehensive performance of _trueness_ and _precision_ can be represented as the mean absolute error (_MAE_ for short), and calculated as the mean of the absolute values of the difference between the measured value and the conventional true value. On the whole, a measure with lower MAE value is better.

We evaluate the performance of different measures in a supervised way. We employ the coefficient of determination, \(R^{2}\), which is defined as the proportion of variance for one variable explained by the other variable, as the ground truth of strength of dependence, which is common in practices (Reshef et al., 2011). Further, we take its square root, \(R\), as the conventional true value of the relationship strength. A simple evidence is that Pearson's \(r\), as the golden standard for measuring linear dependence, is equivalent to \(R\), as long as the relationship is linear. Thus, it is reasonable to adopt \(R\) as the reference value.

### Simulation procedure

We investigate the accuracy performance of \(r^{\#}\), along with \(r^{+}\), Pearson's \(r\), Spearman's \(\rho\), Kendall's \(\tau\) and four other leading dependence measures, i.e., HSIC, dCor, MIC and Chatterjee's \(\xi\) in the following way: for each scenario \(y=f\left(x\right)\), we generated 512 pairs of \(\left(x,y\right)\) from the regression model \(y=f\left(x\right)+\varepsilon\), and computed the values of different measures between \(x\) and \(y\) at different \(R\) levels. In the regression model, the \(x\) sample is uniformly distributed on the unit interval \(\left(0,1\right)\), and the noise is normally distributed as \(\varepsilon\sim N\left(0,\sigma\right)\), with \(\sigma\) controlling \(R\) to a certain level.

\[R=\sqrt{1-\frac{\sigma^{2}}{\text{var}\left(Y\right)}}\]

For the sake of robustness, the computation process is repeated 10 times for each measure at each \(R\) level, and the mean value is adopted.

Simulation procedure is implemented in the _recor_ R package, which is available in supplemental materials. The workflow is to call _accuracy_db_() firstly, _accuracy_results_frm_db_() secondly and _accuracy_plot_lite_() finally. To reproduce the results, just keep all the parameters as default. More details are available in the package help files.

### Performance in simulated scenarios

The simulation is conducted in up to 50 types of different monotone scenarios, including all basic elementary functions, lots of composite functions and several special functions. To our knowledge, our research explores the most extensive and representative range of scenarios. Detailed descriptions of these scenarios can be found in Appendix A.3, and the results are shown in Figure 2.

Figure 2 shows the scatter plots of conventional true value versus measured values in different scenarios. The nine investigated measures are located in nine panels respectively. In each panel, there are totally 50 transparent green lines, representing the measured value (\(Y\)-axis) with respect to conventional true value (\(X\)-axis) for 50 scenarios. The prefixes \(I\)- and \(H\)- refer to _Inverse_ and _Hyperbolic_ respectively. For example, _I-H-Tangent_ stands for _Inverse Hyperbolic Tangent_ function. In all these panels, the dashed diagonal lines represent an ideal measure, the score of which is exactly the same as the conventional true value. Apparently, the closeness to this reference line reflects the performance of a measure for a certain scenario. The median values of each measure among scenarios at different \(R\) levels are also calculated and denoted by the non-transparent red line.

We first look at the extreme values on both sides. It is expected that a measure will score nearly zero when \(X\) and \(Y\) are randomly generated, i.e., \(R\approx 0\), and score one when there is perfect monotone relationship, i.e., \(R=1\). Only Spearman's \(\rho\), Kendall's \(\tau\) and the adjusted \(r^{\#}\) meet this requirement. MIC also scores \(+1\) when \(R=1\), however, it tends to overestimate the strength when \(R\) is near zero, as also reported in other literature before (Chatterjee, 2021). The remaining five measures, i.e., \(r^{+}\), \(r\), HSIC, dCor and \(\xi\), underestimate the strength of nonlinear relationships, and never converge to \(+1\) even when \(R\) approaches to 1.

Now let's take a close look at the intermediate values. It can be seen in Figure 2 that the non-transparent red line of \(r^{\#}\) is the closest one to the dashed line, which means the measured values by \(r^{\#}\) possess the minimum error. To further quantify the accuracy, we add four boxplots at four representative \(R\) levels (approximately, 0.25, 0.50, 0.75, and 1.00) for each measure. \(r^{\#}\) has the highest trueness in all these representative levels. As for precision, the \(r^{\#}\) also outperforms all other measures except HSIC and MIC. Although HSIC and MIC possesses the best precision, they suffer from lower trueness. HSIC tends to underestimate the strength severely, and MIC is also a biased measure, tending to overestimate the strength when the signal is weak, and underestimate it when the signal is strong, as shown in Figure 2.

The overall performance in terms of MAE is ordered as \(\ r^{\#}\left(0.060\right)\succ\rho\left(0.102\right)\succ dCor\left(0.127 \right)\succ r\left(0.150\right)\succ\tau\left(0.157\right)\succ MIC\left(0.1 96\right)\succ\xi\left(0.206\right)\succ r^{+}\left(0.263\right)\succ HSIC \left(0.518\right)\). \(r^{\#}\) possesses significant accuracy advantage over all other measures.

### Performance in real-life scenarios

In addition to simulated scenarios, we also investigate the performance of these measures on real life scenarios provided by NIST (National Institute of Standards and Technology, 2003). There are five available monotone scenarios: Chwirut1, Hahn1, Rat43, Roszman1, and Thurber. Details for these scenarios are available in Appendix A.4.

The performance of nine measures in these five scenarios are depicted in Figure 3. Bar plots illustrate the measured values, the conventional true value verified by NIST is annotated on the top of each

Figure 2: Performance of different measures in 50 simulated scenarios

scenario group. And the differences between the measured value and the true value are mapped as error bars.

It can be seen from Figure 3 that \(r^{\#}\) possesses minimum error, or best accuracy performance among all these five scenarios, with its MAE value as 0.00141, followed by \(\rho\)(0.0159), MIC(0.0249), dCor(0.0575), \(\tau\)(0.0779), \(r\)(0.0916), \(\xi\)(0.166), HSIC(0.891) and \(r^{+}\)(0.956).

### Performance in non-monotone scenarios

It's worth noting that the aforementioned scenarios only cover monotone cases. To evaluate performance in typical non-monotone contexts, we conducted experiments in 16 scenarios, encompassing those outlined in (Reshef et al., 2011) and (Simon and Tibshirani, 2014). Details for these scenarios are available in Appendix A.5.

As anticipated, the MAE value of \(r^{\#}\) reaches a significant 0.418, notably inferior to those of \(\xi\)(0.141), MIC (0.157) and dCor (0.364). In essence, \(r^{\#}\) struggles to accurately measure non-monotone dependence. This limitation stems from its reliance on _covariance_, which inherently fails to detect non-monotone relationships. To illustrate, consider a standard introductory text book example, i.e., \(\mathrm{cov}\left(X,Y\right)=0\) despite \(Y\) being totally dependent on \(X\) via \(Y=X^{2}\). Attempts to tighten its bound proves futile.

However, the performance of \(r^{\#}\) is also superior to those of Spearman's \(\rho\)(0.431), Pearson's \(r\)(0.437) and Kendall's \(\tau\)(0.461). As for accuracy performance, \(r^{\#}\) outperforms the three classical correlation coefficients in not only monotone, but also non-monotone scenarios.

## 4 Conclusion and discussion

We proposed here an adjusted version of Pearson's \(r\), i.e., _rearrangement correlation_, which can be treated as counterpart of Pearson's \(r\) for nonlinear monotone dependence.

The basic idea of rearrangement correlation is simple and straightforward. Its mathematical foundation is a sharpened version of the famous Cauchy-Schwarz Inequality. Tighter bound leads to wider capture range. With the adjustment, the capture range of Pearson's \(r\) is extended from linear dependence to (nonlinear) monotone dependence. Simulated and real-life investigations demonstrate that the rearrangement correlation is more accurate in measuring nonlinear monotone dependence than the three classical correlation coefficients and other more recently proposed dependence measures.

We may draw the conclusion that: Pearson's \(r\) is undoubtedly the gold measure for linear dependence. Now, _it might be the gold measure also for nonlinear monotone dependence, if adjusted_.

Figure 3: Performance of Different Measures in 5 Real-life Scenarios

## References

* Andraszewicz and Rieskamp (2014) Andraszewicz, Sandra and Jorg Rieskamp (2014). "Standardized Covariance--A Measure of Association, Similarity and Co-riskiness between Choice Options". In: _Journal of Mathematical Psychology_ 61, pp. 25-37.
* Armstrong and Riesham (2019) Armstrong, Richard A (2019). "Should Pearson's Correlation Coefficient Be Avoided?" In: _Ophthalmic and Physiological Optics_ 39.5, pp. 316-327.
* Chatterjee and Sourav (2021) Chatterjee, Sourav (2021). "A New Coefficient of Correlation". In: _Journal of the American Statistical Association_ 116.536, pp. 2009-2022.
* Gretton et al. (2005) Gretton, Arthur et al. (2005). "Measuring Statistical Dependence with Hilbert-Schmidt Norms". In: _Algorithmic Learning Theory_. Ed. by David Hutchison et al. Vol. 3734. Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 63-77.
* Hardy et al. (1952) Hardy, G.H., J.E. Littlewood, and G. Polya (1952). _Inequalities_. London: Cambridge University Press.
* ISO (1994) ISO (1994). _Accuracy (Trueness and Precision) of Measurement Methods and Results -- Part 1: General Principles and Definitions_. International Standard.
* Kendall (1938) Kendall, M. G. (1938). "A New Measure of Rank Correlation". In: _Biometrika_ 30.1/2, pp. 81-93.
* Kimeldorf and Sampson (1978) Kimeldorf, George and Allan R. Sampson (July 1978). "Monotone Dependence". In: _The Annals of Statistics_ 6.4. (Visited on 09/21/2022).
* Lee Rodgers and Wander (1988) Lee Rodgers, Joseph and W. Alan Nice Wander (1988). "Thirteen Ways to Look at the Correlation Coefficient". In: _American Statistician_ 42.1, pp. 59-66.
* Lin and Kuei (1989) Lin, Lawrence I-Kuei (1989). "A Concordance Correlation Coefficient to Evaluate Reproducibility". In: _Biometrics_ 45.1, pp. 255-268.
* Mikusinski and Taylor (1991) Mikusinski, Sherwood, and Taylor (1991). "The Frechet Bounds Revisited". In: _Real Analysis Exchange_ 17.2, p. 759.
* Statistical Reference Datasets_. Tech. rep.
* Pearson (1896) Pearson, Karl (1896). "Mathematical Contributions to the Theory of Evolution, III. Regression, Heredity, and Panmixia". In: _Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character_ 187, pp. 253-318.
* Puccetti and Piccetti (2022) Puccetti, Giovanni (2022). "Measuring Linear Correlation between Random Vectors". In: _Information Sciences_ 607, pp. 1328-1347.
* R Core Team (2024) R Core Team (2024). _R: A Language and Environment for Statistical Computing_. Manual. Vienna, Austria.
* Reshef et al. (2011) Reshef, David N. et al. (2011). "Detecting Novel Associations in Large Data Sets". In: _Science_ 334.6062, pp. 1518-1524.
* Simon and Tibshirani (2014) Simon, Noah and Robert Tibshirani (2014). _Comment on "Detecting Novel Associations In Large Data Sets" by Reshef Et Al, Science Dec 16, 2011_. arXiv: 1401.7645.
* Spearman (1904) Spearman, C. (1904). "The Proof and Measurement of Association between Two Things". In: _The American Journal of Psychology_ 15.1, pp. 72-101.
* Speed (2011) Speed, T. (2011). "A Correlation for the 21st Century". In: _Science_ 334.6062, pp. 1502-1503.
* Szekely et al. (2007) Szekely, Gabor J., Maria L. Rizzo, and Nail K. Bakirov (2007). "Measuring and Testing Dependence by Correlation of Distances". In: _Annals of Statistics_ 35.6, pp. 2769-2794.
* Tibshirani et al. (2022) Tibshirani, Dag, Hakon Otnein, and Bard Stoye (2022). "Statistical Dependence: Beyond Pearson's \(\rho\)". In: _Statistical Science_ 37.1, pp. 90-109.
* van den Heuvel et al. (2022) van den Heuvel, Edwin and Zhuozhao Zhan (2022). "Myths About Linear and Monotonic Associations: Pearson's r, Spearman's \(\rho\), and Kendall's \(\tau\)". In: _The American Statistician_ 76.1, pp. 44-52.
* Wasserman (2004) Wasserman, Larry (2004). _All of Statistics: A Concise Course in Statistical Inference_. New York: Springer.
* Whitt (1976) Whitt, Ward (1976). "Bivariate Distributions with Given Marginals". In: _The Annals of Statistics_ 4.6, pp. 1280-1289.
* Zegers (1986) Zegers, Frits E. (1986). "A Family of Chance-corrected Association Coefficients for Metric Scales". In: _Psychometrika_ 51.4, pp. 559-562.

Appendix

### Proofs of theorems, corollaries and propositions

**Theorem 1**.: _For random variables \(X\) and \(Y\), we have_

\[\left|EXY\right|\leqslant\left|EX^{\dagger}Y^{\ddagger}\right|\leqslant\sqrt{EX^ {2}EY^{2}}.\]

_The equality on the left holds if and only if \(X\) and \(Y\) are monotone dependent, and the equality on the right holds if and only if \(Y\mathop{=}\limits^{d}\alpha X\), with \(\mathop{\mathrm{sgn}}\nolimits\left(EXY\right)=\mathop{\mathrm{sgn}} \nolimits\left(\alpha\right)\)._

_Here, \(\mathop{=}\limits^{d}\) denotes equality in distribution, and \(EX^{\dagger}Y^{\ddagger}\) is defined as:_

\[EX^{\dagger}Y^{\ddagger}=\left\{\begin{array}{ll}EX^{\dagger}Y^{\ddagger}, if&EXY\geqslant 0\\ EX^{\dagger}Y^{\ddagger},if&EXY<0\end{array}\right.\]

Proof.: The proof will be completed in two parts.

* The proof of \(\left|EXY\right|\leqslant\left|EX^{\dagger}Y^{\ddagger}\right|\) is mainly based on the rearrangement theorem for functions, i.e., Theorem 378 on page 278 of [1]: Let \(f^{\dagger}\), \(g^{\dagger}\) denote increasing rearrangements and \(f^{\ddagger}\), \(g^{\ddagger}\) decreasing rearrangements of \(f\) and \(g\) on \(\left[0,1\right]\) as defined on page 276 of [1]. Then we have \[\int_{0}^{1}f^{\dagger}\left(u\right)g^{\ddagger}\left(u\right)du\leqslant \int_{0}^{1}f\left(u\right)g\left(u\right)du\leqslant\int_{0}^{1}f^{\dagger} \left(u\right)g^{\dagger}\left(u\right)du.\] Let \(\prod\left(F,G\right)\) be the set of all joint cdf's on \(\mathbb{R}^{2}\) having \(F\) and \(G\) as marginal cdf's. For arbitrary cdf \(H\in\prod\) there exists \(\left(X,Y\right)\): \(\left[0,1\right]\rightarrow\mathbb{R}^{2}\) such that \(\left[X\left(U\right),Y\left(U\right)\right]\) has cdf \(H\). We can let \(f\left(u\right)=X\left(u\right)\) and \(g\left(u\right)=Y\left(u\right)\) so that \(EXY=\int_{0}^{1}f\left(u\right)g\left(u\right)du\). The increasing and decreasing rearrangements of \(f\) and \(g\) are just \(f^{\dagger}\left(u\right)=F^{-1}\left(u\right)\), \(f^{\dagger}\left(u\right)=F^{-1}\left(1-u\right)\), \(g^{\dagger}\left(u\right)=G^{-1}\left(u\right)\), and \(g^{\ddagger}\left(u\right)=G^{-1}\left(1-u\right)\), as stated in [19]. Thus, we have \[EX^{\dagger}Y^{\ddagger}\leqslant EXY\leqslant EX^{\dagger}Y^{\dagger}.\] The right-hand (_resp._ left-hand) equality holds if and only if \(\left(X,Y\right)\mathop{=}\limits^{d}\left(F^{-1}\left(U\right),G^{-1}\left( U\right)\right)\) (_resp._\(\left(X,Y\right)\mathop{=}\limits^{d}\left(F^{-1}\left(U\right),G^{-1} \left(1-U\right)\right)\)). The equality conditions can be equivalently expressed as \(X\) and \(Y\) are increasing (_resp._ decreasing) monotone dependent [19]. If \(EXY\geqslant 0\), we have \(EX^{\dagger}Y^{\ddagger}=EX^{\dagger}Y^{\dagger}Y^{\dagger}\geqslant EXY\geqslant 0\), which implies that \(\left|EX^{\dagger}Y^{\ddagger}\right|=EX^{\dagger}Y^{\dagger}\geqslant EXY= \left|EXY\right|\), and the equality holds if and only if \(X\) and \(Y\) are increasing monotone dependent. If \(EXY<0\), we have \(EX^{\dagger}Y^{\ddagger}=EX^{\dagger}Y^{\ddagger}\leqslant EXY<0\), which implies that \(\left|EX^{\dagger}Y^{\ddagger}\right|=-EX^{\dagger}Y^{\ddagger}\geqslant- EXY=\left|EXY\right|\), and the equality holds if and only if \(X\) and \(Y\) are decreasing monotone dependent. Either way, we have \(\left|EX^{\dagger}Y^{\ddagger}\right|\geqslant\left|EXY\right|\), and the equality holds if and only if \(X\) and \(Y\) are monotone dependent.
* The proof of \(\left|EX^{\dagger}Y^{\ddagger}\right|\leq\sqrt{EX^{2}EY^{2}}\) is mainly based on Cauchy-Schwarz Inequality: If \(EXY\geqslant 0\), \(\left|EX^{\dagger}Y^{\ddagger}\right|=\left|EX^{\dagger}Y^{\dagger}\right| \leqslant\sqrt{E\left(X^{\dagger}\right)^{2}E\left(Y^{\dagger}\right)^{2}}= \sqrt{EX^{2}EY^{2}}\) in the sense that \(X^{\dagger}\mathop{=}\limits^{d}X\), and \(Y^{\dagger}\mathop{=}\limits^{d}Y\). And the equality holds if and only if \(Y^{\dagger}=\alpha X^{\dagger}\), equivalently, \(Y\mathop{=}\limits^{d}\alpha X\), for some constant \(\alpha\geq 0\). Similarly, If \(E\left(XY\right)<0\), \(\left|EX^{\dagger}Y^{\ddagger}\right|=\left|EX^{\dagger}Y^{\ddagger}\right| \leqslant\sqrt{E\left(X^{\dagger}\right)^{2}E\left(Y^{\ddagger}\right)^{2}}= \sqrt{EX^{2}EY^{2}}\) in the sense that \(X^{\dagger}\mathop{=}\limits^{d}X\), and \(Y^{\ddagger}\mathop{=}\limits^{d}Y\). And the equality holds if and only if \(Y^{\ddagger}=\alpha X^{\dagger}\), equivalently, \(Y\mathop{=}\limits^{d}\alpha X\), for some constant \(\alpha<0\). Either way, we have \(\left|EX^{\dagger}Y^{\ddagger}\right|\leqslant\sqrt{EX^{2}EY^{2}}\) and the equality holds if and only if \(Y\mathop{=}\limits^{d}\alpha X\), with \(\mathop{\mathrm{sgn}}\nolimits\left(EXY\right)=\mathop{\mathrm{sgn}}\nolimits \left(\alpha\right)\).

**Theorem 2**.: _For samples \(x\) and \(y\) we have_

\[\left|\left\langle x,y\right\rangle\right|\leqslant\left|\left\langle x^{\uparrow}, y^{\ddagger}\right\rangle\right|\leqslant\left\|x\right\|\left\|y\right\|.\]

_The equality on the left holds if and only if \(x\) and \(y\) are monotone dependent, and the equality on the right holds if and only if \(y\) is arbitrary permutation of \(ax\), with \(\operatorname{sgn}\left(\left\langle x,y\right\rangle\right)=\operatorname{ sgn}\left(a\right)\)._

_Here, \(\left\langle x^{\uparrow},y^{\ddagger}\right\rangle\) is defined as:_

Proof.: The proof will also be completed in two parts.

* The proof of \(\left|\left\langle x,y\right\rangle\right|\leqslant\left|\left\langle x^{ \uparrow},y^{\ddagger}\right\rangle\right|\) is mainly based on another rearrangement theorem for finite sets, i.e., Theorem 368 on page 261 of (Hardy, Littlewood, and Polya, 1952): With \(x^{\uparrow}=\left(x_{(1)},x_{(2)},\cdots,x_{(n)}\right)\), \(y^{\uparrow}=\left(y_{(1)},y_{(2)},\cdots,y_{(n)}\right)\), and \(y^{\downarrow}=\left(y_{(n)},y_{(n-1)},\cdots,y_{(1)}\right)\), we have \[\sum_{i=1}^{n}x_{(i)}y_{(n-i+1)}\leqslant\sum_{i=1}^{n}x_{i}y_{i}\leqslant \sum_{i=1}^{n}x_{(i)}y_{(i)}.\] That is, \[\left\langle x^{\uparrow},y^{\downarrow}\right\rangle\leqslant\left\langle x,y\right\rangle\leqslant\left\langle x^{\uparrow},y^{\uparrow}\right\rangle,\] and the right-hand (_resp._ left-hand) equality holds if and only if \(x\) and \(y\) are similarly(_resp._ oppositely) ordered. According to the definitions of "_similarly(resp. oppositely) ordered_" on page 43 in (Hardy, Littlewood, and Polya, 1952), the equality conditions can be equivalently expressed as \(x\)_and \(y\) are increasing(resp. decreasing) monotone dependent_. If \(\left\langle x,y\right\rangle\geqslant 0\), we have \(\left\langle x^{\uparrow},y^{\ddagger}\right\rangle=\left\langle x^{\uparrow},y^{\uparrow}\right\rangle\geqslant\left\langle x,y\right\rangle\geqslant 0\), which implies \(\left|\left\langle x^{\uparrow},y^{\ddagger}\right\rangle\right|=\left\langle x ^{\uparrow},y^{\uparrow}\right\rangle\geqslant\left\langle x,y\right\rangle= \left|\left\langle x,y\right\rangle\right|\), and the equality holds if and only if \(x\) and \(y\) are increasing monotone dependent.If \(\left\langle x,y\right\rangle<0\), we have \(\left\langle x^{\uparrow},y^{\ddagger}\right\rangle=\left\langle x^{\uparrow},y^{\downarrow}\right\rangle\leqslant\left\langle x,y\right\rangle<0\), which implies \(\left|\left\langle x^{\uparrow},y^{\ddagger}\right\rangle\right|=-\left\langle x ^{\uparrow},y^{\downarrow}\right\rangle\geqslant-\left\langle x,y\right\rangle =\left|\left\langle x,y\right\rangle\right|\), and the equality holds if and only if \(x\) and \(y\) are decreasing monotone dependent. Either way, we have \(\left|\left\langle x^{\uparrow},y^{\ddagger}\right\rangle\right|\geqslant \left|\left\langle x,y\right\rangle\right|\) and the equality holds if and only if \(x\) and \(y\) are monotone dependent.
* The proof of \(\left|\left\langle x^{\uparrow},y^{\ddagger}\right\rangle\right|\leqslant \left\|x\right\|\left\|y\right\|\) is mainly based on Cauchy-Schwarz Inequality: in the sense that norm \(\left\|\cdot\right\|\) is permutation invariant, we have \(\left\|x^{\uparrow}\right\|=\left\|x\right\|\) and \(\left\|y^{\uparrow}\right\|=\left\|y^{\downarrow}\right\|=\left\|y\right\|\). If \(\left\langle x,y\right\rangle\geqslant 0\), we have \(\left|\left\langle x^{\uparrow},y^{\ddagger}\right\rangle\right|=\left|\left\langle x ^{\uparrow},y^{\uparrow}\right\rangle\right|\leqslant\left\|x^{\uparrow} \right\|\left\|y^{\uparrow}\right\|=\left\|x\right\|\left\|y\right\|\), and the equality holds if and only if \(y^{\uparrow}=ax^{\uparrow}\), or equivalently, \(y\) is arbitrary permutation of \(ax\) for some constant \(a\geq 0\). If \(\left\langle x,y\right\rangle<0\), we have \(\left|\left\langle x^{\uparrow},y^{\ddagger}\right\rangle\right|=\left|\left \langle x^{\uparrow},y^{\downarrow}\right\rangle\right|\leqslant\left\|x^{ \uparrow}\right\|\left\|y^{\downarrow}\right\|=\left\|x\right\|\left\|y\right\|\), and the equality holds if and only if \(y^{\downarrow}=ax^{\uparrow}\), or equivalently, \(y\) is arbitrary permutation of \(ax\) for some constant \(a<0\). Either way, we have \(\left|\left\langle x^{\uparrow},y^{\ddagger}\right\rangle\right|\leqslant\left\| x\right\|\left\|y\right\|\), and the equality holds if and only if \(y\) is arbitrary permutation of \(ax\),, with \(\operatorname{sgn}\left(\left\langle x,y\right\rangle\right)=\operatorname{ sgn}\left(a\right)\).

**Corollary 1**.: _For random variables \(X\) and \(Y\), we have covariance inequality series as:_

\[\left|\operatorname{cov}\left(X,Y\right)\right|\leqslant\left| \operatorname{cov}\left(X^{\uparrow},Y^{\ddagger}\right)\right| \leqslant\sqrt{\operatorname{var}\left(X\right)\operatorname{ var}\left(Y\right)}\] \[\leqslant\tfrac{1}{2}\left(\operatorname{var}\left(X\right)+ \operatorname{var}\left(Y\right)\right)\] \[\leqslant\tfrac{1}{2}\left(\operatorname{var}\left(X\right)+ \operatorname{var}\left(Y\right)+\left|\bar{X}-\bar{Y}\right|^{2}\right)\]

_The first equality holds if and only if \(X\) and \(Y\) are monotone dependent, and the second equality holds if and only if \(Y\overset{d}{=}\alpha X+\beta\), with \(\operatorname{sgn}\left(\operatorname{cov}\left(X,Y\right)\right)=\operatorname{ sgn}\left(\alpha\right)\).__Here, \(\operatorname{cov}\left(X^{\dagger},Y^{\ddagger}\right)\) is defined as:_

\[\text{cov}\left(X^{\dagger},Y^{\ddagger}\right):=\left\{\begin{array}{ccc} \text{cov}\left(X^{\dagger},Y^{\dagger}\right),&if&\text{cov}\left(X,Y\right) \geqslant 0\\ \text{cov}\left(X^{\dagger},Y^{\ddagger}\right)&if&\text{cov}\left(X,Y\right) <0\end{array}\right.\]

Proof.: \(\left|\operatorname{cov}\left(X,Y\right)\right|\leqslant\left|\operatorname{ cov}\left(X^{\dagger},Y^{\ddagger}\right)\right|\leqslant\sqrt{\operatorname{ var}\left(X\right)\operatorname{var}\left(Y\right)}\), and the equality conditions are immediate from Theorem 1. The remaining parts of the inequality series are obvious. 

**Corollary 2**.: _For samples \(x\) and \(y\), we have covariance inequality series as_

\[\left|s_{x,y}\right|\leqslant\left|s_{x^{\ddagger},y^{\ddagger}}\right| \leqslant\sqrt{s_{x}^{2}s_{y}^{2}}\] \[\leqslant\frac{1}{2}\left(s_{x}^{2}+s_{y}^{2}\right)\] \[\leqslant\frac{1}{2}\left(s_{x}^{2}+s_{y}^{2}+\left|\bar{x}-\bar {y}\right|^{2}\right)\]

_The first equality holds if and only if \(x\) and \(y\) are monotone dependent, and the second equality holds if and only if \(y\) is arbitrary permutation of \(ax+b\), with \(\operatorname{sgn}\left(s_{x,y}\right)=\operatorname{sgn}\left(a\right)\)._

_Here, \(s_{x^{\ddagger},y^{\ddagger}}\) is defined as:_

\[s_{x^{\ddagger},y^{\ddagger}}=\left\{\begin{array}{ccc}s_{x^{\ddagger},y^{ \ddagger}},if&s_{x,y}\geqslant 0\\ s_{x^{\ddagger},y^{\ddagger}},if&s_{x,y}<0\end{array}\right.\]

Proof.: \(\left|s_{x,y}\right|\leqslant\left|s_{x^{\ddagger},y^{\ddagger}}\right| \leqslant\sqrt{s_{x}^{2}s_{y}^{2}}\), and the equality conditions are immediate from Theorem 2. The remaining parts of the inequality series are obvious. 

**Proposition 1**.: _For random variables \(X\), \(Y\), and samples \(x\), \(y\), the following hold:_

* \(\left|r^{\#}\left(X,Y\right)\right|\leqslant 1\) _and the equality holds if and only if_ \(X\) _and_ \(Y\) _are monotone dependent._
* \(\left|r^{\#}\left(x,y\right)\right|\leqslant 1\) _and the equality holds if and only if_ \(x\) _and_ \(y\) _are monotone dependent._

Proof.: The proposition is immediate from Corollary 1 and Corollary 2. 

**Proposition 2**.: _For random variables \(X\), \(Y\), and samples \(x\), \(y\), the following hold:_

* \(\left|r^{\#}\left(X,Y\right)\right|\geqslant\left|r\left(X,Y\right)\right|\) _and the equality holds if and only if_ \(Y\mathop{=}^{d}\alpha X+\beta\)_, with_ \(\operatorname{sgn}\left(r\left(X,Y\right)\right)=\operatorname{sgn}\left( \alpha\right)\)_._
* \(\left|r^{\#}\left(x,y\right)\right|\geqslant\left|r\left(x,y\right)\right|\) _and the equality holds if and only if_ \(y\) _is arbitrary permutation of_ \(ax+b\)_, with_ \(\operatorname{sgn}\left(r\left(x,y\right)\right)=\operatorname{sgn}\left(a\right)\)_._

Proof.: The proposition is immediate from Corollary 1 and Corollary 2. 

### Experiments settings

All the experiments are implemented with the R language (R Core Team, 2024), along with several add-on packages. The following are lists of packages and functions for the implementation of the nine measures involved in our study:

* \(r^{+}\), recor::loose_pearson()
* \(r\), stats::cor()
* \(r^{\#}\), recor::sharp_pearson()
* \(\rho\), stats::cor(), with the argument _method_ set as "spearman"
* \(\tau\), stats::cor(), with the argument _method_ set as "kendall"
* HSIC, dHSIC::dhsic()
* dCor, energy::dcor()* MIC, minerva::mine_stat()
* \(\xi\), XICOR::calculateXI()

For convenience, we developed an R package _recor_, which encapsulates all these measures as _cor_XXX_() functions. The _recor_ package is available as _recor_1.0.2.targz_ in supplemental materials. For a latest version, please visit https://github.com/byaxb/recor.

Hardware environment configuration for this study was: DELL OptiPlex 7070 Tower, equipped with 8-core CPU Core i7-9700 @ 3.00GHz, 24G DDR4 2666MHz RAM. Under this configuration, it took about 5 days to complete all the experiments.

### Simulated scenarios

We carry out our experiments on 50 simulated scenarios, including all basic elementary functions, lots of composite functions and several typical special functions.

1. Linear: \(y=2x+1,x\in\left[0,1\right]\)
2. Quadratic [asymmetry]: \(y=x^{2},x\in\left[0,1\right]\)
3. Square Root: \(y=\sqrt{x},x\in\left[0,1\right]\)
4. Cubic: \(y=x^{3},x\in\left[0,1\right]\)
5. Reciprocal: \(y=\frac{1}{x},x\in\left[0,1\right]\)
6. Exponential: \(y=e^{x}\), with \(x\in\left[0,1\right]\)
7. Logarithm: \(y=\ln x,x\in\left[0,1\right]\)
8. Sine [quarter period]: \(y=\sin\left(x\right),x\in\left[0,\frac{\pi}{2}\right]\)
9. Cosine [quarter period]: \(y=\cos\left(x\right),x\in\left[0,\frac{\pi}{2}\right]\)
10. Tangent [half period]: \(y=\tan\left(x\right),x\in\left[0,\frac{\pi}{2}\right]\)
11. Cotangent [half period]: \(y=\cot\left(x\right),x\in\left[0,\frac{\pi}{2}\right]\)
12. Inverse Sine: \(y=\arcsin\left(x\right),x\in\left[0,1\right]\)
13. Inverse Cosine: \(y=\arccos\left(x\right),x\in\left[0,1\right]\)
14. Inverse Tangent: \(y=\arctan\left(x\right),x\in\left[0,1\right]\)
15. Inverse Cotangent: \(y=\arccos\left(x\right),x\in\left[0,1\right]\)
16. Secant [quarter period]: \(y=\sec\left(x\right),x\in\left[0,\frac{\pi}{2}\right]\)
17. Cosecant [quarter period]: \(y=\csc\left(x\right),x\in\left[0,\frac{\pi}{2}\right]\)
18. Hyperbolic Sine: \(y=\sinh x=\frac{e^{x}-e^{-x}}{2},x\in\left[0,1\right]\)
19. Hyperbolic Cosine: \(y=\cosh x=\frac{e^{x}+e^{-x}}{e^{2x}+1},x\in\left[0,1\right]\)
20. Hyperbolic Tangent: \(y=\tanh x=\frac{e^{2x}-1}{e^{2x}+1},x\in\left[0,1\right]\)
21. Hyperbolic Cotangent: \(y=\coth x=\frac{e^{2x}+1}{e^{2x}-1},x\in\left[0,1\right]\)
22. Hyperbolic Secant: \(y=\sec\left(x\right)=\frac{2}{e^{x}+e^{-x}},x\in\left[0,100\right]\)
23. Hyperbolic Coscant: \(y=csch\left(x\right)=\frac{2}{e^{x}-e^{-x}},x\in\left[0,100\right]\)
24. Inverse Hyperbolic Sine: \(y=arcsinh\left(x\right)=\ln\left(x+\sqrt{x^{2}+1}\right),x\in\left[0,1\right]\)
25. Inverse Hyperbolic Cosine: \(y=arccosh\left(x\right)=\ln\left(x+\sqrt{x^{2}-1}\right),x\in\left[1,2\right]\)
26. Inverse Hyperbolic Tangent: \(y=arctanh\left(x\right)=\frac{1}{2}\ln\left(\frac{1+x}{1-x}\right)\),x\in\left[0,1\right]\)
27. Inverse Hyperbolic Cotangent: \(y=arccoth\left(x\right)=\frac{1}{2}\ln\left(\frac{x+1}{x-1}\right)\),x\in\left[1,2\right]\)
28. Inverse Hyperbolic Secant: \(y=arccoth\left(x\right)=\ln\left(\frac{1}{x}+\sqrt{\frac{1}{x^{2}}-1}\right),x \in\left[0,1\right]\)29. Inverse Hyperbolic Cosecant: \(y=arccsch\left(x\right)=\ln\left(\frac{1}{x}+\sqrt{\frac{1}{x^{2}}+1}\right),x\in \left[0,1\right]\)
30. Hook: \(y=ax+\frac{b}{x},a=1,b=1,x\in\left[0,1\right]\)
31. Rational: \(y=\frac{x+1}{x-1},x\in\left[0,1\right]\)
32. Hoerl: \(y=x^{a}e^{x},a=-1,x\in\left[0,1\right]\)
33. Sigmoid: \(y=\frac{1}{1+e^{-x}},x\in\left[-0.5,0.5\right]\)
34. Logit: \(y=\ln\frac{x}{1-x},x\in\left[0,1\right]\)
35. Step: \(y=\left\{\begin{array}{cc}0,if\ 0\leqslant x<\frac{1}{2}\\ 1,if\ \frac{1}{2}\leqslant x\leqslant 1\end{array}\right.\)
36. Piecewise [Sigmoid]: \(y=\left\{\begin{array}{cc}0,&if\ 0\leqslant x\leqslant\frac{49}{100}\\ 50\left(x-\frac{1}{2}\right)+\frac{1}{2},if\ \frac{49}{100}<x<\frac{51}{100}\\ 1,&if\ \frac{51}{100}\leqslant x\leqslant 1\end{array}\right.\)
37. Linear + Periodic, High Freq: \(y=\frac{1}{10}\sin\left(10.6\left(2x-1\right)\right)+\frac{11}{10}\left(2x-1 \right),x\in\left[0,1\right]\)
38. Sinc Function: \(S_{k,h}\left(x\right)=\frac{\sin\left(\pi\left(x-kh\right)/h\right)}{\pi \left(x-kh\right)/h},k=0,h=1,x\in\left[0,1\right]\)
39. Einstein Function: \(\mathrm{Einstein}_{1}\left(x\right)=\frac{x^{2}e^{x}}{\left(e^{x}-1\right)^{2 }},x\in\left[0,1\right]\)
40. Exponential Integral: \(E_{1}\left(x\right)=\int_{x}^{\infty}\frac{e^{-t}}{t}dt,x\in\left[0,1\right]\)
41. Hyperbolic Sine Integral: \(Shi\left(x\right)=\int_{0}^{x}\frac{\sinh t}{t}dt,x\in\left[0,1\right]\)
42. Hyperbolic Cosine Integral: \(Chi\left(x\right)=\gamma+\ln x+\int_{0}^{x}\frac{\cosh t-1}{t}dt,x\in\left[0,1\right]\). Here \(\gamma\) is Euler's Constant
43. Error Function: \(erf\left(x\right)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}{e^{-t^{2}}dt},x\in\left[0,1\right]\)
44. Inverse Error Function: \(inverf\left(x\right)=t+\frac{1}{3}t^{3}+\frac{7}{30}t^{5}+\cdots,t=\frac{1}{2} \sqrt{\pi}x,x\in\left[0,1\right]\)
45. Gamma Function: \(\Gamma\left(x\right)=\int_{0}^{\infty}{t^{x-1}e^{-t}dt},x\in\left[0,1\right]\)
46. Psi Function: \(\psi\left(x\right)=\frac{d^{k+1}}{dx^{k+1}}\ln\Gamma\left(x\right)=\frac{ \Gamma^{\prime}\left(x\right)}{\Gamma\left(x\right)},x\in\left[0,1\right],k=0\)
47. Riemann Zeta Function: \(\zeta\left(x\right)=\sum\limits_{n=1}^{\infty}{\frac{1}{n^{x}}},x\in\left[0,1\right]\)
48. Bessel Function: \(Y_{v}\left(x\right)=\frac{J_{v}\left(x\right)\cos\left(v\pi\right)-J_{-v} \left(x\right)}{\sin\left(v\pi\right)}\), \(J_{v}\left(x\right)=\left(\frac{1}{2}x\right)^{v}\sum\limits_{k=0}^{\infty}{ \left(-1\right)}^{k}\frac{\left(\frac{1}{4}x^{2}\right)^{k}}{k\Gamma\left(v+k+1 \right)}\), \(v=0\), \(x\in\left[0,1\right]\)
49. Beta Function: \(B\left(x,w\right)=\frac{\Gamma\left(x\right)\Gamma\left(w\right)}{\Gamma\left(x +w\right)},w=1,x\in\left[0,1\right]\)
50. Dirichlet Eta Function: \(\eta\left(x\right)=\sum\limits_{n=1}^{\infty}{\frac{-1^{n-1}}{n^{x}}},x\in \left[0,1\right]\)

### Real-life scenarios

All the five real life scenarios are provided by NIST (National Institute of Standards and Technology, 2003) as follows:

* Chwirut1: ultrasonic calibration, with \(Y\) as ultrasonic response, and \(X\) as metal distance.
* Hahn1: thermal expansion of copper, with \(Y\) as the coefficient of thermal expansion, and \(X\) as temperature in degrees kelvin.
* Rat43: sigmoid growth, with \(Y\) as dry weight of onion bulbs and tops, and \(X\) as growing time.
* Roszman1: quantum defects in iodine atoms, with \(Y\) as the number of quantum defects, and \(X\) as the excited energy state.

* Thurber: semiconductor electron mobility, with \(Y\) as a measure of electron mobility, and \(X\) as the natural log of the density.

Data and details about these scenarios are available publicly at:

https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml

### Non-monotone scenarios

We conducted our experiments on 16 non-monotone scenarios, comprehensively covering all the scenarios from (Reshef et al., 2011) and (Simon and Tibshirani, 2014).

1. Quadratic [symmetry]: \(y=4x^{2},x\in\left[-\frac{1}{2},\frac{1}{2}\right]\)
2. Cubic 2: \(y=128\big{(}x-\frac{1}{3}\big{)}^{3}-48\big{(}x-\frac{1}{3}\big{)}^{2}-12\left( x-\frac{1}{3}\right),x\in\left[0,1\right]\)
3. Sine, High Freq: \(y=\sin\left(16\pi x\right),x\in\left[0,1\right]\)
4. Cosine [High Freq]: \(y=\cos\left(14\pi x\right),x\in\left[0,1\right]\)
5. Lopsided L-shaped: \(y=\left\{\begin{array}{c}200x,\ if\ 0\leqslant x<\frac{1}{200}\\ -198x+\frac{199}{100},if\ \frac{1}{200}\leqslant x<\frac{1}{100}\\ -\frac{x}{99}+\frac{1}{99},\ if\ \frac{1}{100}\leqslant x\leqslant 1\end{array}\right.\)
6. Circle: \(y=\sqrt{1-\left(2x-1\right)^{2}},x\in\left[0,1\right]\)
7. Linear + Periodic, Medium Freq: \(y=\sin\left(10\pi x\right)+x,x\in\left[0,1\right]\)
8. Cubic 3: \(y=4x^{3}+x^{2}-4x,x\in\left[-1.1,1.3\right]\)
9. Cubic, Y-stretched: \(y=41\left(4x^{3}+x^{2}-4x\right),x\in\left[-1.1,1.3\right]\)
10. Sine [Two periods]: \(y=\sin\left(4\pi x\right),x\in\left[0,1\right]\)
11. Sine [Low Freq]: \(y=\sin\left(8\pi x\right),x\in\left[0,1\right]\)
12. Sine, Non-Fourier Freq [Low]: \(y=\sin\left(9\pi x\right),x\in\left[0,1\right]\)
13. Cosine, Non-Fourier Freq [Low]: \(y=\cos\left(7\pi x\right),x\in\left[0,1\right]\)
14. Sine, Varying Freq [Medium]: \(y=\sin\left(6\pi x\left(1+x\right)\right),x\in\left[0,1\right]\)
15. Cosine, Varying Freq [Medium]: \(y=\cos\left(5\pi x\left(1+x\right)\right),x\in\left[0,1\right]\)
16. Linear + Periodic, High Freq 2: \(y=\frac{1}{6}\sin\left(10.6\left(2x-1\right)\right)+\frac{11}{10}\left(2x-1 \right),x\in\left[0,1\right]\)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The contributions are summarized in _Abstract_ and the last paragraph of _Introduction_. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The proposed \(r^{\#}\) measures linear and nonlinear monotone relationships accurately. However, it may fail to measure non-monotone dependence. The limitations are discussed in _3.5 Performance in non-monotone scenarios_. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Full set of assumptions and complete and correct proofs are provided in _2.3 New inequality tighter than Cauchy-Schwarz Inequality_, _2.4 The proposed Rearrangement Correlation_ and _A.1 Proofs of theorems, corollaries and propositions_. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We fully disclosed all the information needed to reproduce the experimental results, as depicted in _3.2 Simulation procedure_. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The data and code are available as an attached zip file, _Code and data.zip_. We developed an R package _recor_, which implemented all the experiments. The _recor_ package is available as _recor_1.0.2.targz_, included in the zip file. Sufficient instructions to faithfully reproduce the experiments are available in the package help files. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experimental settings are provided in _3 Experiments_, and _A.2 Experiments Settings_. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We evaluate the performance of proposed method and others according to ISO 5725. Please see _3 Experiments_ for details. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details about the hardware configuration and the time of execution are shown in section "_A.2 Experiments Settings_". Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have thoroughly reviewed the NeurIPS Code of Ethics. And we confirm that our research fully complies with all of its provisions. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: What we proposed here is an adjusted version of Pearson's \(r\). This study simply provides theoretical results for measuring dependence and does not involve societal impacts. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Neither the proposed method nor the released data has risk for misuse. The applicability range of this method has undergone comprehensive discussion in _3.5 Performance in non-monotone scenarios_. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All code and data used in our paper are properly credited and explicitly mentioned, as shown in _A.2 Experiments Settings_. Guidelines:* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We developed an R package to reproduce the experiments. And all the functions in this package are well documented, which can be accessed by the command "?function_name" after installation. For more details, see _A.2 Experiments Settings_. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our research does not involve crowdsourcing or research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: Our research does not involve crowdsourcing or research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.